{
    "hands_on_practices": [
        {
            "introduction": "Theoretical knowledge of adaptive step-size control becomes truly concrete when you build a solver yourself. This first practice guides you through the implementation of a complete adaptive integrator from the ground up. By coding the Bogacki-Shampine 2(3) embedded Runge-Kutta pair , you will directly engage with the core logic of estimating local error and dynamically adjusting the step size to meet a specified tolerance.",
            "id": "3259704",
            "problem": "Consider the Initial Value Problem (IVP) defined by an Ordinary Differential Equation (ODE) of the form $y'(t) = f(t,y)$ with initial condition $y(t_0) = y_0$. A one-step method advances the solution from $(t,y)$ to $(t+h, y_{\\text{new}})$ using only information from the current step. In an explicit Runge-Kutta (RK) method, the increment is constructed from a weighted combination of stage derivatives. An embedded RK pair provides two approximations of different orders computed from the same stages, enabling a local error estimate for adaptive step-size control.\n\nYour task is to implement an embedded Runge-Kutta pair that is consistent with the Bogacki–Shampine $2(3)$ construction and use it to drive an adaptive step-size selection algorithm. The algorithm must:\n\n- Compute stage derivatives and produce a higher-order approximation $y^{[p]}$ and a lower-order approximation $y^{[q]}$ at each step, with $pq$.\n- Estimate the local truncation error from the difference $y^{[p]} - y^{[q]}$.\n- Accept a step when a scaled error norm is less than or equal to $1$ and reject otherwise.\n- Adjust the step size $h$ using the asymptotic scaling relation between the error and the step size, ensuring numerical stability by applying a safety factor and bounding the growth and decay of $h$.\n- Respect the final time $t_f$ by reducing the last step as necessary so that $t$ reaches $t_f$ exactly.\n\nYou must implement the adaptive integrator for scalar ODEs with the following scaled error norm per step:\n$$\nE = \\frac{|y^{[p]} - y^{[q]}|}{\\mathrm{atol} + \\mathrm{rtol}\\,\\max(|y|,|y^{[p]}|)},\n$$\nwhere $\\mathrm{atol}$ is the absolute tolerance and $\\mathrm{rtol}$ is the relative tolerance. The step is accepted if $E \\leq 1$. The step-size controller must be based on the principle that an error of order $h^{m}$ implies $h_{\\text{new}} \\propto h\\,E^{-1/m}$ for some integer $m$ that matches the order of the error estimator. Include a multiplicative safety factor and cap the growth and decay by prescribed bounds.\n\nAngle quantities must be interpreted in radians whenever trigonometric functions appear.\n\nImplement the adaptive solver and apply it to the following test suite of IVPs. For each case, return the numerical approximation of $y(t_f)$ as a floating-point number.\n\n- Case $1$ (happy path, exponentially stable):\n  - $f(t,y) = -y$, $y(0) = 1$, $t_0 = 0$, $t_f = 5$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests general performance and stability on a smooth problem.\n\n- Case $2$ (time-dependent growth with trigonometric forcing, radians):\n  - $f(t,y) = y\\sin(t)$, $y(0) = 1$, $t_0 = 0$, $t_f = 3$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-7}$, $\\mathrm{atol} = 10^{-10}$.\n  - Rationale: tests handling of time-dependent coefficients and requires angle in radians.\n\n- Case $3$ (moderately stiff-like linear decay):\n  - $f(t,y) = -15\\,y$, $y(0) = 1$, $t_0 = 0$, $t_f = 1$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests step-size adaptation under faster decay.\n\n- Case $4$ (nonlinear growth near a singularity):\n  - $f(t,y) = y^2$, $y(0) = 1$, $t_0 = 0$, $t_f = 0.9$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-9}$.\n  - Rationale: tests robustness near a blow-up at $t = 1$.\n\n- Case $5$ (zero dynamics, edge case):\n  - $f(t,y) = 0$, $y(0) = 5$, $t_0 = 0$, $t_f = 10$.\n  - Tolerances: $\\mathrm{rtol} = 10^{-8}$, $\\mathrm{atol} = 10^{-12}$.\n  - Rationale: tests the controller when the estimated error is identically zero.\n\nController parameters common to all cases:\n- Safety factor $s = 0.9$,\n- Minimum growth factor $g_{\\min} = 0.2$,\n- Maximum growth factor $g_{\\max} = 5.0$,\n- Initial step size $h_0 = 10^{-3}$,\n- Minimum step size $h_{\\min} = 10^{-12}$,\n- Maximum step size $h_{\\max} = (t_f - t_0)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $5$, for example $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is the floating-point approximation of $y(t_f)$ for Case $i$.",
            "solution": "The core of the task is to construct an adaptive one-step integrator from first principles, grounded in the definitions of an Initial Value Problem (IVP), Runge-Kutta (RK) methods, and local truncation error.\n\nAn IVP specifies $y'(t) = f(t,y)$ with $y(t_0) = y_0$. A one-step method computes $y_{n+1}$ from $(t_n,y_n)$ using a single step size $h_n$ without reference to earlier history. In an explicit Runge-Kutta (RK) method, the new value is a weighted combination of stage derivatives. The general explicit RK construction is:\n$$\nk_1 = f(t_n, y_n),\\quad\nk_2 = f(t_n + c_2 h, y_n + h a_{21} k_1),\\quad \\dots,\\quad\nk_s = f(t_n + c_s h, y_n + h \\sum_{j=1}^{s-1} a_{sj} k_j),\n$$\nand an order-$p$ approximation is\n$$\ny^{[p]}_{n+1} = y_n + h \\sum_{j=1}^s b_j k_j.\n$$\nAn embedded pair provides two sets of weights $(b_j)$ and $(\\hat b_j)$ evaluated with the same stages to deliver $y^{[p]}$ and $y^{[q]}$ with $pq$. The difference\n$$\n\\Delta = y^{[p]}_{n+1} - y^{[q]}_{n+1}\n$$\nis computable with negligible extra cost and scales with a known power of $h$ determined by the construction.\n\nFor the Bogacki–Shampine $2(3)$ pair, the method uses $s=4$ stages with nodes $c_2 = \\tfrac{1}{2}$, $c_3 = \\tfrac{3}{4}$, $c_4 = 1$. The internal coefficients are $a_{21} = \\tfrac{1}{2}$, $a_{32} = \\tfrac{3}{4}$ with $a_{31} = 0$, and $a_{41} = \\tfrac{2}{9}$, $a_{42} = \\tfrac{1}{3}$, $a_{43} = \\tfrac{4}{9}$. The higher-order weights (order $p=3$) are\n$$\nb_1 = \\tfrac{2}{9},\\quad b_2 = \\tfrac{1}{3},\\quad b_3 = \\tfrac{4}{9},\\quad b_4 = 0,\n$$\nand the lower-order weights (order $q=2$) are\n$$\n\\hat b_1 = \\tfrac{7}{24},\\quad \\hat b_2 = \\tfrac{1}{4},\\quad \\hat b_3 = \\tfrac{1}{3},\\quad \\hat b_4 = \\tfrac{1}{8}.\n$$\nThe stages and approximations are computed as\n$$\n\\begin{aligned}\nk_1 = f(t, y),\\\\\nk_2 = f\\Big(t + \\tfrac{1}{2}h,\\, y + h\\,\\tfrac{1}{2}\\,k_1\\Big),\\\\\nk_3 = f\\Big(t + \\tfrac{3}{4}h,\\, y + h\\,\\tfrac{3}{4}\\,k_2\\Big),\\\\\ny^{[3]} = y + h\\Big(\\tfrac{2}{9}k_1 + \\tfrac{1}{3}k_2 + \\tfrac{4}{9}k_3\\Big),\\\\\nk_4 = f\\Big(t + h,\\, y^{[3]}\\Big),\\\\\ny^{[2]} = y + h\\Big(\\tfrac{7}{24}k_1 + \\tfrac{1}{4}k_2 + \\tfrac{1}{3}k_3 + \\tfrac{1}{8}k_4\\Big).\n\\end{aligned}\n$$\nThe embedded difference $\\Delta = y^{[3]} - y^{[2]}$ provides an error estimator with leading behavior $O(h^3)$; that is, it scales like $C h^3$ for smooth $f$. This scaling justifies a controller of the form\n$$\nh_{\\text{new}} = h \\cdot s \\cdot E^{-1/3},\n$$\nwhere $s$ is a safety factor and $E$ is a scaled error norm. To ensure numerical robustness, we bound the multiplicative growth/decay factor by $g_{\\min} \\leq s E^{-1/3} \\leq g_{\\max}$, and we also clip $h_{\\text{new}}$ to $[h_{\\min}, h_{\\max}]$. When $E \\leq 1$, the step is accepted and $y$ advances to $y^{[3]}$; otherwise, the step is rejected and recomputed with a smaller $h$.\n\nThe scaled error norm for scalar problems is chosen to balance absolute and relative tolerances:\n$$\nE = \\frac{|\\Delta|}{\\mathrm{atol} + \\mathrm{rtol}\\,\\max(|y|,|y^{[3]}|)}.\n$$\nThis ensures that the acceptance criterion $E \\leq 1$ controls the error relative to the magnitude of the solution while preventing division by very small numbers when $y$ is near zero.\n\nAlgorithmic steps for integrating from $t_0$ to $t_f$:\n- Initialize $t = t_0$, $y = y_0$, choose $h$ in $[h_{\\min}, h_{\\max}]$ (for example $h_0$ as given), and set controller parameters $s$, $g_{\\min}$, $g_{\\max}$.\n- While $t  t_f$:\n  - If $t + h  t_f$, set $h = t_f - t$ to land exactly at $t_f$.\n  - Compute $k_1$, $k_2$, $k_3$, $y^{[3]}$, $k_4$, and $y^{[2]}$.\n  - Compute $E$ from $y$, $y^{[3]}$, $y^{[2]}$, $\\mathrm{atol}$, and $\\mathrm{rtol}$.\n  - If $E \\leq 1$, accept the step: set $t \\gets t + h$, $y \\gets y^{[3]}$.\n  - Compute the candidate growth factor $g = s \\cdot E^{-1/3}$; if $E=0$, set $g = g_{\\max}$.\n  - Bound $g$ to $[g_{\\min}, g_{\\max}]$, then update $h \\gets \\mathrm{clip}(h \\cdot g, h_{\\min}, h_{\\max})$.\n  - If a step is rejected ($E  1$), update $h$ as above and recompute without advancing $t$ or $y$.\n- Return $y(t_f)$.\n\nWe now briefly analyze each test case and its expected behavior:\n- Case $1$: $y'(t) = -y$, exact solution $y(t) = e^{-t}$, so $y(5) = e^{-5}$. The method should take moderate steps and converge rapidly.\n- Case $2$: $y'(t) = y\\sin(t)$ in radians with exact solution $y(t) = \\exp(1 - \\cos t)$, giving $y(3) = \\exp(1 - \\cos 3)$. The algorithm must handle time-dependent forcing smoothly.\n- Case $3$: $y'(t) = -15y$ with exact $y(1) = e^{-15}$, requiring smaller steps initially due to rapid decay, but the controller will increase $h$ as $y$ shrinks.\n- Case $4$: $y'(t) = y^2$ with exact $y(t) = \\frac{1}{1 - t}$ for $t1$, so $y(0.9) = 10$. The method must adaptively reduce $h$ as $t$ approaches the blow-up at $t=1$.\n- Case $5$: $y'(t) = 0$ yields constant solution $y(t) = 5$; the error estimator is identically zero, and the controller will enlarge $h$ up to $h_{\\max}$.\n\nBy implementing the Bogacki–Shampine $2(3)$ stages and the controller derived from the $O(h^3)$ error estimate, the adaptive solver will provide $y(t_f)$ for all cases, printed in the specified format.",
            "answer": "```python\n# Python 3.12, numpy 1.23.5 allowed; no other libraries.\nimport numpy as np\n\ndef rk23_bogacki_shampine_step(f, t, y, h):\n    \"\"\"\n    Perform one Bogacki-Shampine 2(3) step for a scalar ODE y' = f(t,y).\n    Returns (y_high, y_low) where y_high is the 3rd-order solution, y_low is the 2nd-order embedded solution.\n    \"\"\"\n    k1 = f(t, y)\n    k2 = f(t + 0.5 * h, y + h * 0.5 * k1)\n    k3 = f(t + 0.75 * h, y + h * 0.75 * k2)\n    # 3rd-order solution\n    y3 = y + h * ( (2.0/9.0) * k1 + (1.0/3.0) * k2 + (4.0/9.0) * k3 )\n    # Stage 4 evaluated at t+h, y3\n    k4 = f(t + h, y3)\n    # 2nd-order embedded solution\n    y2 = y + h * ( (7.0/24.0) * k1 + (1.0/4.0) * k2 + (1.0/3.0) * k3 + (1.0/8.0) * k4 )\n    return y3, y2\n\ndef integrate_adaptive(f, t0, tf, y0, rtol, atol,\n                       h0=1e-3, hmin=1e-12, hmax=None,\n                       safety=0.9, growth_min=0.2, growth_max=5.0):\n    \"\"\"\n    Adaptive integrator using Bogacki-Shampine 2(3) pair for scalar ODEs.\n    \"\"\"\n    t = float(t0)\n    y = float(y0)\n    if hmax is None:\n        hmax = abs(tf - t0)\n    h = max(hmin, min(h0, hmax))\n    # Direction of integration\n    direction = 1.0 if tf >= t0 else -1.0\n    h *= direction\n    hmin_signed = hmin * direction\n    hmax_signed = hmax * direction\n\n    # Main integration loop\n    # Guard for max iterations to prevent infinite loops in pathological cases\n    max_steps = 10_000_000\n    steps = 0\n    while (direction > 0 and t  tf) or (direction  0 and t > tf):\n        steps += 1\n        if steps > max_steps:\n            # Fallback: give current y\n            break\n\n        # Adjust step to not overshoot tf\n        remaining = tf - t\n        if direction * h > direction * remaining:\n            h = remaining\n\n        # Take one RK23 step\n        y3, y2 = rk23_bogacki_shampine_step(f, t, y, h)\n\n        # Scaled error norm (scalar)\n        scale = atol + rtol * max(abs(y), abs(y3))\n        # Prevent zero scale\n        if scale == 0.0:\n            scale = atol\n        err = abs(y3 - y2) / scale\n\n        # Accept or reject\n        if err = 1.0:\n            # Accept the step\n            t = t + h\n            y = y3\n            # Compute growth factor; estimator scales ~ h^3\n            if err == 0.0:\n                g = growth_max\n            else:\n                g = safety * err ** (-1.0 / 3.0)\n            # Bound growth factor\n            g = max(growth_min, min(g, growth_max))\n            # Update h and clip\n            h = h * g\n            # Clip to [hmin, hmax] with sign\n            if direction > 0:\n                h = min(max(h, hmin_signed), hmax_signed)\n            else:\n                h = max(min(h, hmin_signed), hmax_signed)\n        else:\n            # Reject step; decrease h\n            g = safety * err ** (-1.0 / 3.0)\n            g = max(growth_min, min(g, growth_max))\n            h = h * g\n            # Ensure not below minimum\n            if direction > 0:\n                h = max(h, hmin_signed)\n            else:\n                h = min(h, hmin_signed)\n            # If h becomes too small, break to avoid infinite loop\n            if abs(h)  hmin:\n                # Cannot reduce further; accept and break\n                # This is a conservative fallback\n                t = t + h\n                y = y3\n                break\n\n    return y\n\ndef solve():\n    # Define the test cases\n    # Case 1: y' = -y, y(0) = 1, tf = 5\n    def f1(t, y): return -y\n\n    # Case 2: y' = y*sin(t), radians, y(0) = 1, tf = 3\n    def f2(t, y): return y * np.sin(t)\n\n    # Case 3: y' = -15 y, y(0) = 1, tf = 1\n    def f3(t, y): return -15.0 * y\n\n    # Case 4: y' = y^2, y(0) = 1, tf = 0.9\n    def f4(t, y): return y * y\n\n    # Case 5: y' = 0, y(0) = 5, tf = 10\n    def f5(t, y): return 0.0\n\n    test_cases = [\n        # (f, t0, tf, y0, rtol, atol)\n        (f1, 0.0, 5.0, 1.0, 1e-6, 1e-9),\n        (f2, 0.0, 3.0, 1.0, 1e-7, 1e-10),\n        (f3, 0.0, 1.0, 1.0, 1e-6, 1e-9),\n        (f4, 0.0, 0.9, 1.0, 1e-6, 1e-9),\n        (f5, 0.0, 10.0, 5.0, 1e-8, 1e-12),\n    ]\n\n    results = []\n    for f, t0, tf, y0, rtol, atol in test_cases:\n        ytf = integrate_adaptive(\n            f=f, t0=t0, tf=tf, y0=y0, rtol=rtol, atol=atol,\n            h0=1e-3, hmin=1e-12, hmax=abs(tf - t0),\n            safety=0.9, growth_min=0.2, growth_max=5.0\n        )\n        results.append(ytf)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Now that you understand how an adaptive solver works, a critical practical question arises: how should you choose the tolerance values? In scientific and engineering applications, these parameters are not arbitrary; they are dictated by the physical constraints and measurement precision of the system being modeled. This exercise  challenges you to develop a principled methodology for translating allowable physical measurement error in a thermal cooling model into the absolute and relative tolerances ($\\epsilon_{\\text{abs}}$ and $\\epsilon_{\\text{rel}}$) for your solver, a crucial skill for ensuring your simulations are both efficient and meaningful.",
            "id": "3095862",
            "problem": "You are given the linear thermal model expressed as the ordinary differential equation (ODE) $T'(t) = -k\\,(T(t) - T_{\\infty})$, where $T(t)$ is the temperature in degrees Celsius at time $t$ in seconds, $k$ is the cooling coefficient in $\\mathrm{s}^{-1}$, and $T_{\\infty}$ is the ambient temperature in degrees Celsius. You must design and implement a methodology that tunes the adaptive step-size controller tolerances $\\epsilon_{\\text{abs}}$ (absolute tolerance) and $\\epsilon_{\\text{rel}}$ (relative tolerance) based on the physical units and allowable measurement error of the temperature, and then validate the methodology against known exact solutions.\n\nThe derivation must start from a valid base appropriate to adaptive step-size control for ODEs: the definition of an ordinary differential equation, the existence of an exact solution for the linear cooling model, the concept of a local error estimator in embedded Runge–Kutta methods, and the widely used adaptive error acceptance criterion that compares the estimated local error to a combination of absolute and relative tolerances. You must not assume any shortcut formulas; instead, justify the mapping from physical measurement limits to solver tolerances by dimensional analysis and the structure of the error acceptance criterion.\n\nTask requirements:\n- Use temperature in degrees Celsius and time in seconds. Any measurement error limits must be respected in degrees Celsius for absolute error and as a unitless decimal fraction for relative error (for example, a relative error of $\\delta$ means the error must be bounded by $\\delta \\cdot |T|$, and you must write $\\delta$ as a decimal such as $0.002$ and not as a percentage).\n- Develop a principled mapping from the allowable measurement error to the solver tolerances $\\epsilon_{\\text{abs}}$ and $\\epsilon_{\\text{rel}}$ that is consistent with units and accounts for a safety margin so that the solver’s numerical error remains strictly below the allowable measurement error across the entire integration interval.\n- Implement an adaptive numerical integration of the ODE from $t = 0$ to $t = t_{\\text{end}}$ using a method with embedded local error estimation and tolerances $\\epsilon_{\\text{abs}}$ and $\\epsilon_{\\text{rel}}$ as tuned by your methodology.\n- Use the exact analytical solution to compute the numerical error across the integration grid and check compliance with the measurement limits. The exact solution is $T(t) = T_{\\infty} + (T(0) - T_{\\infty})\\, e^{-k t}$, which follows from solving the linear first-order ODE.\n- For each test case, compute the maximum over the integration interval of the quantity $q(t) = |T_{\\text{num}}(t) - T_{\\text{exact}}(t)| - \\left(A_{\\text{abs}} + R_{\\text{rel}}\\,|T_{\\text{exact}}(t)|\\right)$, where $A_{\\text{abs}}$ is the allowable absolute measurement error in degrees Celsius and $R_{\\text{rel}}$ is the allowable relative measurement error expressed as a decimal fraction. Report a boolean indicating whether $\\max_{t \\in [0, t_{\\text{end}}]} q(t) \\le 0$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{True},\\text{False},\\text{True}]$).\n\nTest suite:\n- Case $1$ (happy path): $k = 0.05\\,\\mathrm{s}^{-1}$, $T_{\\infty} = 20\\,^{\\circ}\\mathrm{C}$, $T(0) = 100\\,^{\\circ}\\mathrm{C}$, $t_{\\text{end}} = 100\\,\\mathrm{s}$, $A_{\\text{abs}} = 0.05\\,^{\\circ}\\mathrm{C}$, $R_{\\text{rel}} = 0.002$ (decimal), safety factor $s = 0.1$ (decimal).\n- Case $2$ (fast cooling, small temperature difference): $k = 0.5\\,\\mathrm{s}^{-1}$, $T_{\\infty} = 20\\,^{\\circ}\\mathrm{C}$, $T(0) = 25\\,^{\\circ}\\mathrm{C}$, $t_{\\text{end}} = 10\\,\\mathrm{s}$, $A_{\\text{abs}} = 0.01\\,^{\\circ}\\mathrm{C}$, $R_{\\text{rel}} = 0.001$ (decimal), safety factor $s = 0.1$ (decimal).\n- Case $3$ (near equilibrium, slow cooling): $k = 0.02\\,\\mathrm{s}^{-1}$, $T_{\\infty} = 30\\,^{\\circ}\\mathrm{C}$, $T(0) = 31\\,^{\\circ}\\mathrm{C}$, $t_{\\text{end}} = 60\\,\\mathrm{s}$, $A_{\\text{abs}} = 0.02\\,^{\\circ}\\mathrm{C}$, $R_{\\text{rel}} = 0.002$ (decimal), safety factor $s = 0.1$ (decimal).\n- Case $4$ (large dynamic range): $k = 0.2\\,\\mathrm{s}^{-1}$, $T_{\\infty} = 25\\,^{\\circ}\\mathrm{C}$, $T(0) = 1000\\,^{\\circ}\\mathrm{C}$, $t_{\\text{end}} = 30\\,\\mathrm{s}$, $A_{\\text{abs}} = 0.1\\,^{\\circ}\\mathrm{C}$, $R_{\\text{rel}} = 0.001$ (decimal), safety factor $s = 0.1$ (decimal).\n\nOutput specification:\n- Your program must implement the methodology, run the four cases, and output a single line containing a list of four booleans $[\\text{b}_{1},\\text{b}_{2},\\text{b}_{3},\\text{b}_{4}]$, where $\\text{b}_{i}$ is $\\text{True}$ if and only if the numerical solution satisfies $|T_{\\text{num}}(t) - T_{\\text{exact}}(t)| \\le A_{\\text{abs}} + R_{\\text{rel}}\\,|T_{\\text{exact}}(t)|$ for all $t$ in $[0, t_{\\text{end}}]$ for the $i$-th test case, and $\\text{False}$ otherwise. The units for $A_{\\text{abs}}$ are degrees Celsius, the units for time are seconds, and $R_{\\text{rel}}$ is a unitless decimal fraction.",
            "solution": "The problem requires the design and validation of a methodology to set adaptive ODE solver tolerances, $\\epsilon_{\\text{abs}}$ and $\\epsilon_{\\text{rel}}$, based on specified physical measurement error allowances, $A_{\\text{abs}}$ and $R_{\\text{rel}}$. The context is the linear thermal model described by the ordinary differential equation (ODE):\n$$\nT'(t) = -k(T(t) - T_{\\infty})\n$$\nwhere $T(t)$ is the temperature at time $t$, $k$ is the cooling coefficient, and $T_{\\infty}$ is the ambient temperature. The initial condition is given as $T(0)$. This is a standard initial value problem (IVP).\n\nThe derivation begins with the fundamental principles of adaptive step-size control in numerical methods for ODEs. An adaptive solver, such as an embedded Runge-Kutta method, estimates the local error, $E_{\\text{local}}$, in each integration step. This estimated error is compared against a tolerance, $\\tau$, to decide whether to accept the step and to adjust the next step size. The standard tolerance criterion is a weighted combination of an absolute tolerance, $\\epsilon_{\\text{abs}}$, and a relative tolerance, $\\epsilon_{\\text{rel}}$:\n$$\n\\text{criterion: } E_{\\text{local}} \\le \\tau = \\epsilon_{\\text{abs}} + \\epsilon_{\\text{rel}} |y_{\\text{current}}|\n$$\nwhere $|y_{\\text{current}}|$ is the magnitude of the numerical solution at the beginning of the step. The units of $\\epsilon_{\\text{abs}}$ must match the units of the solution variable (degrees Celsius in this case), while $\\epsilon_{\\text{rel}}$ is a dimensionless fraction.\n\nThe problem's core requirement is to ensure that the *global numerical error*, defined as $E_{\\text{global}}(t) = |T_{\\text{num}}(t) - T_{\\text{exact}}(t)|$, remains below a physically meaningful bound for the entire integration interval $t \\in [0, t_{\\text{end}}]$. This bound is defined by the allowable measurement errors:\n$$\nE_{\\text{global}}(t) \\le A_{\\text{abs}} + R_{\\text{rel}}|T_{\\text{exact}}(t)|\n$$\nHere, $A_{\\text{abs}}$ is the allowable absolute error in degrees Celsius, and $R_{\\text{rel}}$ is the allowable relative error, given as a unitless decimal.\n\nA critical distinction must be made: adaptive solvers control the *local error* per step, not the *global error* accumulated over the entire interval. The global error is the cumulative result of the local errors from all steps. For a stable problem, the global error is related to the local error, but it is generally larger. To guarantee that the global error stays within its prescribed bound, the local error must be controlled much more tightly.\n\nThis leads to the principled mapping from physical allowances $(A_{\\text{abs}}, R_{\\text{rel}})$ to solver tolerances $(\\epsilon_{\\text{abs}}, \\epsilon_{\\text{rel}})$. We introduce a safety factor, $s$, where $0  s  1$. This factor represents the fraction of the allowable global error budget that we allocate to the local error control at each step. By setting solver tolerances to be a fraction $s$ of the physical allowances, we create a stringent local error target, with the expectation that the accumulation of these small local errors will not exceed the larger global error allowance.\n\nThe structural similarity between the solver's tolerance formula and the required global error bound justifies a component-wise scaling. This is also dimensionally consistent. The absolute tolerance $\\epsilon_{\\text{abs}}$ has the same units as the allowable absolute error $A_{\\text{abs}}$, and the relative tolerance $\\epsilon_{\\text{rel}}$ is dimensionless, like the allowable relative error $R_{\\text{rel}}$. The mapping is therefore defined as:\n$$\n\\epsilon_{\\text{abs}} = s \\cdot A_{\\text{abs}}\n$$\n$$\n\\epsilon_{\\text{rel}} = s \\cdot R_{\\text{rel}}\n$$\nWith the given safety factor of $s = 0.1$, the solver will be instructed to maintain a local error that is ten times smaller than the target global error bound.\n\nThe validation procedure involves the following steps for each test case:\n$1$. Calculate the solver tolerances $\\epsilon_{\\text{abs}}$ and $\\epsilon_{\\text{rel}}$ using the specified $A_{\\text{abs}}$, $R_{\\text{rel}}$, and $s$.\n$2$. Numerically integrate the ODE $T'(t) = -k(T - T_{\\infty})$ from $t=0$ to $t=t_{\\text{end}}$ with the initial condition $T(0)$, using an adaptive solver (such as `scipy.integrate.solve_ivp`) configured with these tolerances.\n$3$. At each time point $t_i$ returned by the solver, compute the exact solution: $T_{\\text{exact}}(t_i) = T_{\\infty} + (T(0) - T_{\\infty}) e^{-kt_i}$.\n$4$. For each point, calculate the validation quantity: $q(t_i) = |T_{\\text{num}}(t_i) - T_{\\text{exact}}(t_i)| - (A_{\\text{abs}} + R_{\\text{rel}}|T_{\\text{exact}}(t_i)|)$.\n$5$. Determine the maximum value of $q(t)$ across all time points. The methodology is successful for that case if and only if $\\max_i q(t_i) \\le 0$. A positive value indicates that the numerical error exceeded the allowable physical error bound at some point.\n$6$. Report the boolean outcome for each case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\n# Make sure to use the specified versions:\n# numpy>=1.23.5\n# scipy>=1.11.4\n# Python>=3.12\n\ndef solve():\n    \"\"\"\n    Implements and validates a methodology for tuning adaptive ODE solver \n    tolerances based on physical measurement error limits.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: k, T_inf, T0, t_end, A_abs, R_rel, s\n        (0.05, 20.0, 100.0, 100.0, 0.05, 0.002, 0.1),\n        # Case 2\n        (0.5, 20.0, 25.0, 10.0, 0.01, 0.001, 0.1),\n        # Case 3\n        (0.02, 30.0, 31.0, 60.0, 0.02, 0.002, 0.1),\n        # Case 4\n        (0.2, 25.0, 1000.0, 30.0, 0.1, 0.001, 0.1),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        k, T_inf, T0, t_end, A_abs, R_rel, s = case\n\n        # Step 1: Develop principled mapping from physical error to solver tolerances\n        # The solver's local error tolerances are set to be a fraction `s` of the\n        # global physical error allowances to provide a safety margin.\n        eps_abs = s * A_abs\n        eps_rel = s * R_rel\n\n        # Step 2: Define the ODE right-hand side function\n        def thermal_model(t, T):\n            return -k * (T - T_inf)\n\n        # Step 3: Implement adaptive numerical integration\n        # Use solve_ivp, which employs an adaptive step-size method (default 'RK45').\n        # dense_output=True is not strictly needed here as we check against the\n        # output grid, which is sufficient for this problem's validation.\n        sol = solve_ivp(\n            fun=thermal_model,\n            t_span=(0, t_end),\n            y0=[T0],\n            method='RK45',\n            rtol=eps_rel,\n            atol=eps_abs\n        )\n        \n        t_num = sol.t\n        T_num = sol.y[0]\n\n        # Step 4: Use the exact solution to compute numerical error and check compliance\n        def exact_solution(t, T0, T_inf, k):\n            return T_inf + (T0 - T_inf) * np.exp(-k * t)\n\n        T_exact_vals = exact_solution(t_num, T0, T_inf, k)\n\n        # Step 5: Compute the maximum of the validation quantity q(t)\n        # q(t) = |T_num(t) - T_exact(t)| - (A_abs + R_rel * |T_exact(t)|)\n        # The condition q(t) = 0 must hold for all t.\n        \n        # Absolute numerical error at each time step\n        numerical_error = np.abs(T_num - T_exact_vals)\n        \n        # Allowable physical error bound at each time step\n        allowed_error = A_abs + R_rel * np.abs(T_exact_vals)\n        \n        # The quantity q(t) measures by how much the numerical error exceeds the allowance\n        q_values = numerical_error - allowed_error\n        \n        # Check if the maximum of q(t) is non-positive.\n        is_compliant = np.max(q_values) = 0.0\n        \n        results.append(is_compliant)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a suite of numerical methods at your disposal, the final step is learning to choose the most efficient one for a given problem. This advanced practice moves from implementing a single method to quantitatively comparing different ones. You will construct a cost model  to analyze the trade-offs between a low-order and a high-order method when tackling both nonstiff and stiff ODEs, revealing the intricate relationship between method order, accuracy requirements, and stability constraints.",
            "id": "3095889",
            "problem": "You must write a complete and runnable program that, from first principles of adaptive step-size control for ordinary differential equations (ODEs), constructs a simple, quantitative cost model to choose between two explicit embedded Runge–Kutta (RK) methods under a fixed absolute local error tolerance. The model must balance the local work per step against the total number of steps by explicitly simulating an adaptive controller and counting the total number of right-hand-side function evaluations required to integrate a scalar linear ODE over a fixed time interval. The two methods must be evaluated on both a stiff linear test equation and a nonstiff linear test equation.\n\nFundamental base. Use the following foundations only: the definition of an ordinary differential equation (ODE) $y'(t)=f(t,y(t))$, the concept of local truncation error per step for a method of order $p$ behaving like $O(h^{p+1})$, and the notion of absolute stability for explicit RK methods on the linear test equation $y'=-\\lambda y$. For the adaptive controller, you must derive the step-size update from the asymptotic scaling of the local error. For stiffness, you must incorporate an absolute stability constraint that enforces $h \\le r/\\lambda$, where $r$ is the approximate length of the stability interval along the negative real axis for the given method.\n\nTarget ODEs. Consider the scalar linear ODEs $y'=-\\lambda y$ with $y(0)=1$ on a finite horizon $t \\in [0,T]$:\n- Nonstiff: $\\lambda=1$.\n- Stiff: $\\lambda=1000$.\n\nEmbedded methods to compare. Implement two explicit embedded RK pairs with absolute local error control based on the difference between the higher- and lower-order solutions in the pair.\n- Method A (low order): an embedded pair of orders $\\{2,1\\}$ based on the second-order Heun method (explicit trapezoidal) with the first-order forward Euler as the embedded estimator. Count $2$ right-hand-side evaluations per attempted step. Use an approximate stability radius $r_A=2.0$ on the negative real axis for the linear test equation.\n- Method B (high order): the Cash–Karp embedded pair $\\{5,4\\}$ with $6$ stages. Count $6$ right-hand-side evaluations per attempted step. Use an approximate stability radius $r_B=2.8$ on the negative real axis for the linear test equation.\n\nAdaptive controller specification. For each attempted step of size $h$, compute the absolute local error estimate $e = |y_{\\text{high}}-y_{\\text{low}}|$. Accept the step if $e \\le \\text{tol}$ and reject otherwise. After each attempt, propose a new step size via\n$$h_{\\text{new}} = h \\cdot \\mathrm{clip}\\left(s \\cdot \\left(\\frac{\\text{tol}}{\\max(e,\\varepsilon)}\\right)^{1/p}, f_{\\min}, f_{\\max}\\right)$$\nwhere $p$ is the order of the higher-order member of the embedded pair ($p=2$ for Method A and $p=5$ for Method B), $s$ is a safety factor, $\\varepsilon$ is a very small positive number to avoid division by zero, and $\\mathrm{clip}(x,f_{\\min},f_{\\max})=\\min(\\max(x,f_{\\min}),f_{\\max})$. You must enforce the absolute stability cap $h \\le 0.95 \\cdot r/\\lambda$ for every attempted step, and the last step must not overshoot $T$. Use the fixed controller constants $s=0.9$, $f_{\\min}=0.2$, $f_{\\max}=5.0$, and $\\varepsilon=10^{-30}$. Initialize with $t=0$, $y=1$, and a positive initial step size that respects both the stability cap and the interval length; one reasonable choice is $h_0=\\min\\left(T, 0.1 \\cdot \\text{tol}^{1/p}, 0.8 \\cdot r/\\lambda\\right)$ for the corresponding method.\n\nCost model and decision rule. For each run, the cost is the total number of right-hand-side evaluations, that is, the number of attempted steps (accepted or rejected) multiplied by the per-step evaluations ($2$ for Method A and $6$ for Method B). For each test case, choose the method with the strictly smaller total cost. If the costs are exactly equal, choose Method A.\n\nAngle units and physical units. No physical units or angle units are involved. All quantities are dimensionless real numbers.\n\nTest suite. Your program must evaluate the following four parameter sets:\n- Case $1$: $\\lambda=1$, $T=5$, $\\text{tol}=10^{-6}$ (nonstiff, tight tolerance).\n- Case $2$: $\\lambda=1$, $T=5$, $\\text{tol}=10^{-1}$ (nonstiff, loose tolerance).\n- Case $3$: $\\lambda=1000$, $T=1$, $\\text{tol}=10^{-6}$ (stiff, tight tolerance).\n- Case $4$: $\\lambda=1000$, $T=1$, $\\text{tol}=10^{-2}$ (stiff, moderate tolerance).\n\nRequired final output format. Your program should produce a single line of output containing the decisions for the four cases, in order, as a comma-separated list enclosed in square brackets, where each entry is an integer method identifier: $0$ for Method A (low order) or $1$ for Method B (high order). For example, a valid output looks like $[1,0,0,0]$.",
            "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded in the principles of numerical analysis for ordinary differential equations (ODEs), well-posed with a clear and deterministic algorithm, and self-contained with all necessary parameters and definitions provided. We may therefore proceed with a solution.\n\nThe task is to construct a cost model to decide between two explicit embedded Runge-Kutta (RK) methods for integrating a scalar linear ODE. The decision is based on the total number of function evaluations required by an adaptive step-size controller to integrate the ODE over a fixed interval to a specified tolerance. The cost model must be simulated for both a nonstiff and a stiff instance of the ODE.\n\n### 1. Theoretical Foundation\n\n**ODE and Test Equation:**\nThe problem considers the scalar linear test equation, a fundamental model for analyzing the stability and accuracy of numerical ODE solvers:\n$$\ny'(t) = -\\lambda y(t), \\quad y(0) = 1, \\quad t \\in [0, T]\n$$\nThe exact solution is $y(t) = e^{-\\lambda t}$. The parameter $\\lambda  0$ determines the stiffness of the equation; a large $\\lambda$ corresponds to a stiff problem where the solution decays rapidly.\n\n**Embedded Runge-Kutta Methods:**\nAn embedded RK pair provides two solutions at each step, one of a higher order $p$ ($y_{\\text{high}}$) and one of a lower order $q$ ($y_{\\text{low}}$). The difference between these solutions provides an estimate of the local truncation error (LTE) of the lower-order method:\n$$\ne = |y_{\\text{high}} - y_{\\text{low}}|\n$$\nThis error estimate is used to control the step size $h$. The two methods under consideration are:\n- **Method A:** A $\\{2,1\\}$ pair using the second-order Heun method ($p=2$) and the first-order forward Euler method ($q=1$). It requires $2$ evaluations of the right-hand-side function $f(t,y)$ per step. Its stability radius on the negative real axis is given as $r_A=2.0$.\n- **Method B:** The Cash-Karp $\\{5,4\\}$ pair ($p=5$, $q=4$). It requires $6$ function evaluations per step. Its stability radius is given as $r_B=2.8$.\n\n**Error Estimation from First Principles:**\nFor an RK method of order $q$ applied to $y'=-\\lambda y$, the LTE after one step from a point $y_n$ on the exact solution is approximately the first non-vanishing term in the Taylor series difference. The error estimate $e$ from an embedded pair is designed to approximate this LTE.\n$$\ne \\approx |\\text{LTE}_{q}| \\approx \\left| \\frac{d^{q+1}y}{dt^{q+1}} \\frac{h^{q+1}}{(q+1)!} \\right|\n$$\nFor $y(t)=e^{-\\lambda t}$, the $(q+1)$-th derivative is $y^{(q+1)}(t) = (-\\lambda)^{q+1} e^{-\\lambda t} = (-\\lambda)^{q+1} y(t)$. Thus, the error estimate can be expressed as:\n$$\ne \\approx \\frac{(h\\lambda)^{q+1}}{(q+1)!} |y(t)|\n$$\nFor Method A, with $q=1$, the error estimate is $e \\approx \\frac{1}{2}(h\\lambda)^2 |y|$. For Method B, with $q=4$, it is $e \\approx \\frac{1}{120}(h\\lambda)^5 |y|$.\n\n**State Propagation:**\nWhen a step is accepted, the numerical solution is advanced using the higher-order approximation. For any explicit RK method applied to $y'=-\\lambda y$, the update has the form $y_{n+1} = R(-h\\lambda) y_n$, where $R(z)$ is the method's stability function. For a method of order $p$, $R(z)$ is a polynomial that matches the Taylor series of $e^z$ up to the term $z^p$:\n$$\nR_p(z) = \\sum_{k=0}^{p} \\frac{z^k}{k!}\n$$\n- For Method A ($p=2$): $y_{n+1} = y_n \\left(1 - h\\lambda + \\frac{1}{2}(h\\lambda)^2\\right)$.\n- For Method B ($p=5$): $y_{n+1} = y_n \\sum_{k=0}^{5} \\frac{(-h\\lambda)^k}{k!}$.\n\n### 2. Adaptive Control Algorithm\n\nThe simulation of the adaptive integrator proceeds iteratively from $t=0$ to $T$.\n\n**Initialization:**\n- The simulation starts at $t=0$ with $y=1$.\n- The initial step size $h_0$ is chosen conservatively to respect the integration interval $T$, the tolerance $\\text{tol}$, and the stability limit of the method:\n$$\nh_0 = \\min\\left(T, 0.1 \\cdot \\text{tol}^{1/p}, 0.8 \\cdot \\frac{r}{\\lambda}\\right)\n$$\nwhere $p$ is the order of the higher-order method and $r$ is its stability radius.\n\n**Main Loop:**\nAt each iteration, with a proposed step size $h$, the following steps are performed:\n1.  **Step-size Constraints:** The actual attempted step, $h_{\\text{current}}$, is capped by the stability requirement and the remaining interval length:\n    $$\n    h_{\\text{current}} = \\min\\left(h, 0.95 \\frac{r}{\\lambda}, T-t\\right)\n    $$\n2.  **Attempt Step and Estimate Error:** The total count of function evaluations is incremented. The local error estimate $e$ is calculated using the formula derived above.\n3.  **Accept/Reject:**\n    - If $e \\le \\text{tol}$, the step is accepted. The time $t$ and solution $y$ are advanced using $h_{\\text{current}}$ and the higher-order update rule.\n    - If $e  \\text{tol}$, the step is rejected. The state $(t, y)$ remains unchanged.\n4.  **Propose Next Step:** A new step size for the next attempt is calculated using the specified PI controller formula:\n    $$\n    h_{\\text{new}} = h_{\\text{current}} \\cdot \\mathrm{clip}\\left(s \\cdot \\left(\\frac{\\text{tol}}{\\max(e, \\varepsilon)}\\right)^{1/p}, f_{\\min}, f_{\\max}\\right)\n    $$\n    The parameters are given as safety factor $s=0.9$, clipping factors $f_{\\min}=0.2$ and $f_{\\max}=5.0$, and a small regularizer $\\varepsilon=10^{-30}$. The order $p$ is that of the higher-order method in the pair.\n\n### 3. Cost Model and Decision\n\nThe total cost for a given method and test case is the total number of function evaluations accumulated throughout the simulation. The decision rule is to choose the method with the strictly lower cost. If costs are identical, Method A is chosen. This procedure is repeated for all four test cases specified in the problem statement.",
            "answer": "```python\nimport math\nimport numpy as np\n\ndef run_simulation(lambda_val, T, tol, p, q, evals_per_step, r):\n    \"\"\"\n    Simulates an adaptive step-size ODE solver to calculate the total cost.\n\n    Args:\n        lambda_val (float): The lambda parameter of the ODE y' = -lambda*y.\n        T (float): The final integration time.\n        tol (float): The absolute local error tolerance.\n        p (int): The order of the higher-order method in the embedded pair.\n        q (int): The order of the lower-order method in the embedded pair.\n        evals_per_step (int): The number of RHS evaluations per step.\n        r (float): The stability radius of the higher-order method.\n\n    Returns:\n        int: The total number of RHS evaluations (cost).\n    \"\"\"\n    # Controller constants\n    s = 0.9\n    f_min = 0.2\n    f_max = 5.0\n    epsilon = 1.0e-30\n\n    # Initial conditions\n    t = 0.0\n    y = 1.0\n    total_evals = 0\n\n    # Stability cap\n    h_stab_max = (0.95 * r / lambda_val) if lambda_val > 0 else float('inf')\n\n    # Initial step size h\n    h0_acc = 0.1 * (tol**(1.0/p))\n    h0_stab = (0.8 * r / lambda_val) if lambda_val > 0 else float('inf')\n    h = min(T, h0_acc, h0_stab)\n    \n    # Pre-calculate factorials for the stability function\n    factorials = [math.factorial(i) for i in range(p + 1)]\n    \n    q_factorial = math.factorial(q + 1)\n\n    while t  T:\n        # Guard against floating-point issues near the end of the interval\n        if T - t  1e-14 * T:\n            break\n\n        # Apply stability and interval-end constraints to the step size\n        h_current = min(h, h_stab_max, T - t)\n        \n        # An attempt costs function evaluations regardless of acceptance\n        total_evals += evals_per_step\n\n        # Calculate error estimate based on first principles for y'=-lambda*y\n        # e ~ |y| * |(-h*lambda)^(q+1) / (q+1)!|\n        e = abs(y) * (h_current * lambda_val)**(q + 1) / q_factorial\n\n        # Accept or reject the step\n        if e = tol:\n            # Step accepted: update time and solution\n            t += h_current\n            # Update y using the stability function R_p(z) of the higher-order method\n            z = -h_current * lambda_val\n            R_p_z = sum((z**k) / factorials[k] for k in range(p + 1))\n            y *= R_p_z\n        # If rejected, t and y do not change.\n\n        # Propose the next step size based on the current attempt\n        # Use a small number epsilon to avoid division by zero\n        ratio = tol / max(e, epsilon)\n        scale_factor = s * (ratio**(1.0/p))\n        \n        # Clip the scaling factor to prevent overly aggressive changes\n        clipped_scale_factor = min(max(scale_factor, f_min), f_max)\n        \n        h = h_current * clipped_scale_factor\n        \n    return total_evals\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and determine the optimal method for each case.\n    \"\"\"\n    test_cases = [\n        # (lambda, T, tol)\n        (1.0, 5.0, 1.0e-6),      # Case 1: Nonstiff, tight tolerance\n        (1.0, 5.0, 1.0e-1),      # Case 2: Nonstiff, loose tolerance\n        (1000.0, 1.0, 1.0e-6),   # Case 3: Stiff, tight tolerance\n        (1000.0, 1.0, 1.0e-2),   # Case 4: Stiff, moderate tolerance\n    ]\n\n    # Method A: Heun(2)/Euler(1) embedded pair\n    method_A_params = {'p': 2, 'q': 1, 'evals_per_step': 2, 'r': 2.0}\n    # Method B: Cash-Karp 5(4) embedded pair\n    method_B_params = {'p': 5, 'q': 4, 'evals_per_step': 6, 'r': 2.8}\n\n    results = []\n    for lambda_val, T, tol in test_cases:\n        cost_A = run_simulation(lambda_val, T, tol, **method_A_params)\n        cost_B = run_simulation(lambda_val, T, tol, **method_B_params)\n\n        # Decision rule: Choose method with strictly smaller cost.\n        # If costs are equal, choose Method A.\n        # 0 for Method A, 1 for Method B.\n        if cost_B  cost_A:\n            decision = 1\n        else:\n            decision = 0\n        results.append(decision)\n    \n    # Print the final output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}