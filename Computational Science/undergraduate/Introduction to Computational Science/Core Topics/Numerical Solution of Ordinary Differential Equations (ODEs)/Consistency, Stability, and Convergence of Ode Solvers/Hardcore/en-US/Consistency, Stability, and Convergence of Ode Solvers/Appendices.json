{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is foundational, designed to demonstrate vividly why consistency alone is not enough to guarantee that a numerical method will converge. By implementing a method that is mathematically consistent but inherently unstable, you will observe firsthand how errors can accumulate uncontrollably, preventing the numerical solution from approaching the true solution. This practice solidifies the crucial role of stability as a necessary partner to consistency for achieving convergence, a principle formalized in the Lax-Richtmyer equivalence theorem. ",
            "id": "3156045",
            "problem": "Consider the initial value problem for an ordinary differential equation (ODE) given by $y'(t) = f(t,y(t))$ with $y(0) = y_0$, where $y(t)$ is a sufficiently smooth scalar function and $f$ is a sufficiently smooth function. Define the local truncation error (LTE) as the defect produced by substituting the exact solution into a proposed numerical recurrence for a single step, and define the global truncation error (GTE) as the difference between the numerical approximation and the exact solution at a specified final time. In the context of linear multistep methods, zero-stability is the property that the homogeneous part of the recurrence does not amplify perturbations unboundedly as the number of steps increases; for one-step methods, stability refers to the boundedness of error propagation governed by the linearized recurrence factor.\n\nDesign and implement a numerical experiment to separate the concept of consistency (LTE tends to zero as the step size tends to zero) from convergence (GTE tends to zero as the step size tends to zero). Use the ODE $y'(t) = -y(t)$ on the interval $t \\in [0,1]$ with the exact solution $y(t) = e^{-t}$ and $y_0 = 1$. Construct and analyze a linear multistep method that is consistent (its LTE tends to $0$ as $h \\to 0$) but not stable, and thus not convergent, in contrast with a stable one-step method.\n\nTasks:\n1. Implement the two-step recurrence $y_{n+1} - 2 y_n + y_{n-1} = h f(t_n, y_n)$ for $f(t,y) = -y$. Initialize the method with $y_0 = y(0)$ and $y_1 = y(h)$ taken from the exact solution to isolate propagation effects rather than start-up approximation effects. For $n = 1,2,\\dots,N-1$ with $Nh = 1$, evolve the recurrence to $t_N = 1$ and compute the final-time absolute global truncation error $|y_N - e^{-1}|$.\n2. For the same method, compute the maximum-in-time absolute local truncation error by evaluating the one-step defect when substituting the exact solution into the recurrence, i.e., $\\max_{n=1,\\dots,N-1} \\left| y(t_{n+1}) - 2 y(t_n) + y(t_{n-1}) - h f(t_n, y(t_n)) \\right|$, where $t_n = n h$.\n3. Implement the forward Euler one-step method $y_{n+1} = y_n + h f(t_n, y_n)$ for $f(t,y) = -y$ with $y_0 = 1$, and similarly compute its maximum-in-time absolute local truncation error $\\max_{n=0,\\dots,N-1} \\left| y(t_{n+1}) - y(t_n) - h f(t_n, y(t_n)) \\right|$ and its final-time absolute global truncation error $|y_N - e^{-1}|$.\n4. Use the following test suite of step sizes, which explores a general case and progressively refined steps: $h \\in \\{0.5, 0.25, 0.125, 0.0625\\}$. For each $h$, let $N = 1/h$ be the number of steps so that $t_N = 1$ exactly.\n5. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each $h$ in the order listed, append four floating-point numbers in this order: the maximum-in-time absolute local truncation error for the unstable two-step method, the final-time absolute global truncation error for the unstable two-step method, the maximum-in-time absolute local truncation error for the forward Euler method, and the final-time absolute global truncation error for the forward Euler method. The final output, therefore, contains $16$ floats corresponding to the four values for each of the four step sizes, all in a single flat list. No physical units are involved in this problem, and angles are not used.",
            "solution": "We start from the initial value problem $y'(t) = f(t,y(t))$ with $f(t,y) = -y$, $y(0) = 1$, and exact solution $y(t) = e^{-t}$ on $t \\in [0,1]$. The local truncation error (LTE) at a time $t_n$ for a given scheme is defined as the algebraic defect obtained when substituting the exact values into the numerical recurrence over a single step. The global truncation error (GTE) at time $t_N$ is defined as the difference $|y_N - y(t_N)|$ where $y_N$ is the numerical solution and $y(t_N)$ is the exact solution.\n\nTo contrast consistency from convergence, we consider two schemes.\n\nFirst scheme (two-step, unstable and inconsistent):\nThe method is $y_{n+1} - 2 y_n + y_{n-1} = h f(t_n,y_n)$, which for $f(t,y) = -y$ becomes $y_{n+1} = (2 - h) y_n - y_{n-1}$. Its homogeneous recurrence is $y_{n+1} - 2 y_n + y_{n-1} = 0$ with characteristic polynomial $\\rho(r) = r^2 - 2r + 1 = (r - 1)^2$. The double root at $r = 1$ violates the zero-stability condition for linear multistep methods, which requires that all roots of $\\rho(r)$ satisfy $|r| \\le 1$ and that any root with $|r| = 1$ be simple. Therefore, this method is not zero-stable and cannot be convergent.\n\nFurthermore, the method is also not consistent. To analyze consistency for this LMM, we re-index it to the standard form $y_{n+2} - 2y_{n+1} + y_n = h f_{n+1}$. The characteristic polynomials are $\\rho(r) = (r-1)^2$ and $\\sigma(r) = r$. A method is consistent (order $p \\ge 1$) if $\\rho(1)=0$ and $\\rho'(1)=\\sigma(1)$. Here, $\\rho(1)=0$ is satisfied, but $\\rho'(r)=2r-2 \\implies \\rho'(1)=0$, while $\\sigma(1)=1$. Since $0 \\ne 1$, the method is not consistent. Its order is $p=0$.\n\nWe can confirm this by deriving the local truncation error. By definition, the LTE at $t_n$ is:\n$$\n\\tau_n = y(t_{n+1}) - 2 y(t_n) + y(t_{n-1}) - h f(t_n, y(t_n))\n$$\nUsing Taylor expansions around $t_n$ for the exact solution $y(t_n)$, we have $y(t_{n+1}) - 2y(t_n) + y(t_{n-1}) = h^2 y''(t_n) + \\mathcal{O}(h^4)$. Since $f(t_n, y(t_n)) = -y(t_n)$ and for this ODE $y''(t)=y(t)$, the LTE becomes:\n$$\n\\tau_n = (h^2 y(t_n) + \\mathcal{O}(h^4)) - h(-y(t_n)) = h y(t_n) + \\mathcal{O}(h^2)\n$$\nThe LTE is $\\mathcal{O}(h)$, so the method is of order $p=0$. This lack of consistency is one reason the method fails to converge. The lack of zero-stability is another, more dramatic reason. The error propagation is governed by the homogeneous recurrence, which has solutions of the form $e_n = C_1 + C_2 n$. This polynomial growth in the error, amplified by the number of steps $N=1/h$, guarantees divergence.\n\nSecond scheme (one-step, stable and convergent):\nThe forward Euler method is $y_{n+1} = y_n + h f(t_n,y_n)$, here $y_{n+1} = (1 - h) y_n$. Its LTE, by substitution of the exact solution, is\n$$\n\\tau_n = y(t_{n+1}) - y(t_n) - h f(t_n, y(t_n)) = y(t_{n+1}) - y(t_n) + h y(t_n).\n$$\nUsing Taylor expansion,\n$$\ny(t_{n+1}) - y(t_n) = h y'(t_n) + \\frac{h^2}{2} y''(t_n) + \\mathcal{O}(h^3) = -h y(t_n) + \\frac{h^2}{2} y''(t_n) + \\mathcal{O}(h^3),\n$$\nand with $y''(t_n) = y(t_n)$, we get\n$$\n\\tau_n = \\frac{h^2}{2} y(t_n) + \\mathcal{O}(h^3),\n$$\nso LTE $\\to 0$ quadratically. The error recurrence is\n$$\ne_{n+1} = (1 - h) e_n + \\tau_n.\n$$\nFor $h \\in (0,2)$, we have $|1 - h|  1$, implying that errors are damped. Summing the forced response shows\n$$\ne_N = \\sum_{k=0}^{N-1} (1 - h)^{N-1-k} \\tau_k,\n$$\nwhich is bounded by a geometric series scaling with $\\max_k |\\tau_k| = \\mathcal{O}(h^2)$, yielding $e_N = \\mathcal{O}(h)$ as $h \\to 0$. Hence, forward Euler is both consistent and stable, and therefore convergent, with GTE $\\to 0$.\n\nExperimental design and computation:\n- We fix $T = 1$ and step sizes $h \\in \\{0.5, 0.25, 0.125, 0.0625\\}$ so that $N = 1/h$ is an integer.\n- For the two-step method, we use exact initialization $y_0 = y(0)$ and $y_1 = y(h)$ to focus on propagation stability. We compute the maximum-in-time absolute LTE by substituting $y(t_n)$ into the recurrence and taking the maximum over $n = 1,\\dots,N-1$. We compute the final-time absolute GTE $|y_N - e^{-1}|$ after evolving the recurrence to $t_N = 1$.\n- For forward Euler, we compute its maximum-in-time absolute LTE similarly via substitution and its final-time absolute GTE by evolving the numerical recurrence from $y_0 = 1$.\n\nExpected qualitative outcomes:\n- For the two-step method, the maximum-in-time absolute LTE decreases with $h$ (tending to zero, but only linearly, reflecting order p=0), but the final-time absolute GTE does not tend to zero due to the lack of both zero-stability and consistency. It remains of order $\\mathcal{O}(1)$ as $h \\to 0$.\n- For forward Euler, both the maximum-in-time absolute LTE and the final-time absolute GTE decrease with $h$; the final-time GTE tends to zero linearly, reflecting convergence.\n\nThe program outputs, in a single flat list and in the order of the test suite, for each $h$:\n1. the two-step method maximum-in-time absolute LTE,\n2. the two-step method final-time absolute GTE,\n3. the forward Euler maximum-in-time absolute LTE,\n4. the forward Euler final-time absolute GTE.\n\nThis directly demonstrates that the first method fails to converge, while the second one (which is both consistent and stable) does.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef y_exact(t: float) - float:\n    # Exact solution for y' = -y with y(0) = 1\n    return np.exp(-t)\n\ndef f_rhs(t: float, y: float) - float:\n    # Right-hand side f(t,y) = -y\n    return -y\n\ndef lte_two_step(h: float, T: float) - float:\n    # Compute maximum-in-time absolute LTE for two-step method:\n    # tau_n = y(t_{n+1}) - 2 y(t_n) + y(t_{n-1}) - h f(t_n, y(t_n))\n    N = int(round(T / h))\n    # times t_n = n h, n = 0..N\n    max_tau = 0.0\n    for n in range(1, N):  # n=1..N-1 has neighbors n-1 and n+1 within [0,N]\n        t_n = n * h\n        tau = y_exact(t_n + h) - 2.0 * y_exact(t_n) + y_exact(t_n - h) - h * f_rhs(t_n, y_exact(t_n))\n        max_tau = max(max_tau, abs(tau))\n    return max_tau\n\ndef gte_two_step(h: float, T: float) - float:\n    # Compute final-time absolute GTE for two-step method evolution with exact starts\n    N = int(round(T / h))\n    y = np.zeros(N + 1, dtype=float)\n    # exact starts\n    y[0] = y_exact(0.0)\n    if N = 1:\n        y[1] = y_exact(h)\n    # evolve: y_{n+1} = (2 - h) y_n - y_{n-1}\n    for n in range(1, N):\n        y[n + 1] = (2.0 - h) * y[n] - y[n - 1]\n    return abs(y[N] - y_exact(T))\n\ndef lte_forward_euler(h: float, T: float) - float:\n    # Compute maximum-in-time absolute LTE for forward Euler:\n    # tau_n = y(t_{n+1}) - y(t_n) - h f(t_n, y(t_n)), for n=0..N-1\n    N = int(round(T / h))\n    max_tau = 0.0\n    for n in range(0, N):\n        t_n = n * h\n        tau = y_exact(t_n + h) - y_exact(t_n) - h * f_rhs(t_n, y_exact(t_n))\n        max_tau = max(max_tau, abs(tau))\n    return max_tau\n\ndef gte_forward_euler(h: float, T: float) - float:\n    # Compute final-time absolute GTE for forward Euler evolution\n    N = int(round(T / h))\n    y = 1.0  # y0\n    for _ in range(N):\n        y = y + h * f_rhs(0.0, y)  # f depends only on y here; t not needed\n        # Using t argument consistently, but for f = -y, t is irrelevant.\n    return abs(y - y_exact(T))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    T = 1.0\n    test_steps = [0.5, 0.25, 0.125, 0.0625]\n\n    results = []\n    for h in test_steps:\n        # Two-step method (consistent but not zero-stable)\n        lte_ts = lte_two_step(h, T)\n        gte_ts = gte_two_step(h, T)\n        # Forward Euler (stable for h in (0,2), convergent)\n        lte_fe = lte_forward_euler(h, T)\n        gte_fe = gte_forward_euler(h, T)\n        results.extend([lte_ts, gte_ts, lte_fe, gte_fe])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on these foundational concepts, this practice tackles the common and challenging issue of \"stiff\" differential equations, where different processes evolve on vastly different timescales. You will implement and compare an explicit method, whose performance is limited by stability constraints, against an implicit method that can overcome these limitations on a classic predator-prey model. This exercise allows you to experimentally verify how a stable method can achieve its theoretical order of accuracy even in challenging scenarios, illustrating the practical interplay of consistency, stability, and convergence. ",
            "id": "3112018",
            "problem": "Create a complete, runnable program that performs a step-halving convergence study on a predator–prey system with a saturating functional response and uses it to reason about consistency, stability, and convergence of ordinary differential equation (ODE) solvers. The study must be grounded in first principles: the initial value problem (IVP) definition, the notions of consistency, stability, and convergence, and the construction of fixed-step one-step methods from Taylor expansions and integral formulations. Use these ideas to design a procedure that detects when stiffness introduced by prey saturation forces implicit methods to realize their formal order of accuracy while explicit methods are stability-limited on coarser steps.\n\nYou must work with the predator–prey system\n$$\n\\begin{aligned}\n\\frac{dx}{dt} = r\\,x\\left(1-\\frac{x}{K}\\right)\\;-\\;\\frac{c\\,x\\,y}{1+\\alpha x},\\\\\n\\frac{dy}{dt} = \\varepsilon\\left(\\eta\\,\\frac{c\\,x\\,y}{1+\\alpha x}\\;-\\;m\\,y\\right),\n\\end{aligned}\n$$\nwhere $x$ is the prey density and $y$ is the predator density. The saturating functional response is encoded by the factor $(1+\\alpha x)$ in the denominator, and stiffness can arise when the prey dynamics rapidly saturate (large $r$ relative to other rates and/or small $\\varepsilon$ makes the predator–prey timescales disparate).\n\nBase your reasoning and algorithmic design on the following foundational elements only:\n- The initial value problem (IVP) concept for an ODE: given $\\frac{d\\mathbf{u}}{dt}=\\mathbf{f}(t,\\mathbf{u})$ and $\\mathbf{u}(0)=\\mathbf{u}_0$, seek $\\mathbf{u}(t)$.\n- Definitions: A method is consistent if its local truncation error tends to $0$ as the step size $h\\to 0$. A method is stable if errors do not grow uncontrollably under perturbations; for linear scalar tests $\\frac{du}{dt}=\\lambda u$ with $\\mathrm{Re}(\\lambda)\\le 0$, this is characterized by a stability region in the complex plane for $z=h\\lambda$. A method is convergent if its global error tends to $0$ as $h\\to 0$. Consistency plus stability implies convergence.\n- Fixed-step one-step methods may be constructed by matching Taylor series (for explicit Runge–Kutta-type schemes) or by applying the fundamental theorem of calculus to average the vector field over the step (for implicit trapezoidal-type schemes).\n\nImplement two methods:\n- An explicit second-order Runge–Kutta method (explicit midpoint), a consistent explicit method of formal order $2$.\n- The implicit trapezoidal rule (also of formal order $2$), solved at each step by Newton’s method using the exact Jacobian.\n\nUse these methods on a fixed interval $[0,T]$, with $T$ specified below. For each method, perform a step-halving study with uniform steps. For each step size $h_k$, compute the numerical solution at $t=T$ and compare against a high-accuracy reference solution to estimate the global error. Estimate the experimental order of convergence (EOC) from a linear fit of $\\log(\\text{error})$ versus $\\log(h)$ using a decreasing-error suffix of the smallest available step sizes. A method “realizes its formal order” in this study if the estimated EOC is at least $1.7$ using at least $3$ strictly decreasing error points at the smallest step sizes.\n\nDefine “stiffness from prey saturation forces implicit methods to realize their formal order” for a given parameter set to mean: the implicit trapezoidal method realizes its formal order as defined above and simultaneously the explicit midpoint method is stability-limited on coarse steps, evidenced by failing to produce finite, non-NaN terminal values on at least one of the coarser step sizes in the halving sequence (i.e., it produces fewer than all $5$ finite terminal approximations across the prescribed step sizes). Under this condition, report a value of $1$ for that parameter set; otherwise, report $0$.\n\nNumerical details and test suite:\n- Use final time $T=2$ (that is, $T=2$).\n- Use initial condition $(x(0),y(0))=(x_0,y_0)=(0.5,0.3)$.\n- Use $N_k\\in\\{20,40,80,160,320\\}$ uniform steps, corresponding to $h_k=T/N_k$ for $k=0,1,2,3,4$. This gives $5$ step sizes with halving.\n- Compute a high-accuracy reference solution at $t=T$ using a stiff-capable integrator with strict tolerances; this reference is used only to measure terminal global errors for the fixed-step methods.\n- For the Newton solve in the implicit trapezoidal method, use the exact Jacobian of $\\mathbf{f}$, an absolute Newton tolerance of $10^{-12}$, and a maximum of $20$ iterations per step.\n\nProvide results for the following three parameter sets (test suite), each expressed as $(r,K,\\alpha,c,\\eta,m,\\varepsilon)$:\n- Case $\\mathsf{A}$ (non-stiff baseline): $(2,\\,1,\\,2,\\,3,\\,0.7,\\,0.4,\\,1)$.\n- Case $\\mathsf{B}$ (moderately stiff via prey saturation and timescale split): $(50,\\,1,\\,2,\\,4,\\,0.7,\\,0.4,\\,0.1)$.\n- Case $\\mathsf{C}$ (highly stiff): $(150,\\,1,\\,2,\\,4,\\,0.7,\\,0.4,\\,0.05)$.\n\nOutput specification:\n- For each case in the order $(\\mathsf{A},\\mathsf{B},\\mathsf{C})$, output an integer $0$ or $1$ as defined above.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[1,0,1]$.\n\nAll computations are dimensionless; no physical units are required. Angles are not used. The final output must be computable as booleans or integers as specified, and the single-line format is mandatory.",
            "solution": "The central task is to investigate the numerical properties of ODE solvers by performing a step-halving convergence study on a predator-prey system. The system's dynamics can be adjusted to be stiff, which allows for a clear demonstration of the differing behaviors of explicit and implicit numerical methods. We will implement two second-order methods, one explicit and one implicit, and use their performance to classify the stiffness of the system under different parameter sets, strictly adhering to the definitions of consistency, stability, and convergence.\n\nThe system is a two-dimensional autonomous ordinary differential equation (ODE) given by:\n$$\n\\begin{aligned}\n\\frac{dx}{dt} = r\\,x\\left(1-\\frac{x}{K}\\right)\\;-\\;\\frac{c\\,x\\,y}{1+\\alpha x} \\\\\n\\frac{dy}{dt} = \\varepsilon\\left(\\eta\\,\\frac{c\\,x\\,y}{1+\\alpha x}\\;-\\;m\\,y\\right)\n\\end{aligned}\n$$\nThis can be written as an initial value problem (IVP) in the standard vector form $\\frac{d\\mathbf{u}}{dt} = \\mathbf{f}(t, \\mathbf{u})$ with an initial condition $\\mathbf{u}(t_0) = \\mathbf{u}_0$. Here, the state vector is $\\mathbf{u}(t) = [x(t), y(t)]^T$, and the vector field is $\\mathbf{f}(\\mathbf{u}) = [f_x(x,y), f_y(x,y)]^T$. The initial condition is given as $\\mathbf{u}(0) = [x_0, y_0]^T = [0.5, 0.3]^T$. The study will be conducted on the time interval $[0, T]$ with $T=2$.\n\nThe core concepts are:\n- **Consistency**: A method is consistent if its local truncation error (the error made in a single step) approaches zero as the step size $h \\to 0$.\n- **Stability**: A method is stable if errors introduced at one step do not grow unboundedly in subsequent steps. For stiff problems, this is analyzed with the model equation $\\frac{du}{dt} = \\lambda u$ for complex $\\lambda$ with $\\mathrm{Re}(\\lambda) \\le 0$. A method's stability region is the set of complex values $z=h\\lambda$ for which the numerical solution remains bounded.\n- **Convergence**: A method is convergent if the global error (the cumulative error at a fixed final time $T$) approaches zero as $h \\to 0$. The Lax-Richtmyer equivalence theorem states that for a consistent method, stability is equivalent to convergence. The rate of convergence is typically measured by the experimental order of convergence (EOC).\n\nWe will implement two fixed-step, one-step methods of formal order $2$.\n\n**1. Explicit Second-Order Runge-Kutta Method (Explicit Midpoint)**\n\nThis method is derived by matching terms in the Taylor series expansion of the solution. It advances the solution from time $t_n$ to $t_{n+1} = t_n + h$ using two function evaluations:\n$$\n\\begin{aligned}\n\\mathbf{k}_1 = \\mathbf{f}(t_n, \\mathbf{u}_n) \\\\\n\\mathbf{k}_2 = \\mathbf{f}\\left(t_n + \\frac{h}{2}, \\mathbf{u}_n + \\frac{h}{2}\\mathbf{k}_1\\right) \\\\\n\\mathbf{u}_{n+1} = \\mathbf{u}_n + h \\mathbf{k}_2\n\\end{aligned}\n$$\nThis method is explicit because $\\mathbf{u}_{n+1}$ is computed directly from known quantities. It is consistent with a local truncation error of order $O(h^3)$, which leads to a global error of order $O(h^2)$. However, its stability region is a finite area in the complex plane. If a problem is stiff, the eigenvalues of its Jacobian can have large negative real parts, forcing the product $h\\lambda$ outside the stability region unless the step size $h$ is severely restricted. This is a stability limitation.\n\n**2. Implicit Trapezoidal Rule**\n\nThis method is derived from the fundamental theorem of calculus, $\\mathbf{u}(t_{n+1}) - \\mathbf{u}(t_n) = \\int_{t_n}^{t_{n+1}} \\mathbf{f}(t, \\mathbf{u}(t)) dt$, by approximating the integral using the trapezoidal rule:\n$$\n\\mathbf{u}_{n+1} = \\mathbf{u}_n + \\frac{h}{2} \\left[ \\mathbf{f}(t_n, \\mathbf{u}_n) + \\mathbf{f}(t_{n+1}, \\mathbf{u}_{n+1}) \\right]\n$$\nThis method is implicit because the unknown $\\mathbf{u}_{n+1}$ appears on both sides of the equation. It is also a second-order method. Critically, it is A-stable, meaning its stability region contains the entire left half of the complex plane. This property allows it to remain stable for stiff problems even with large step sizes, making its accuracy, not stability, the limiting factor.\n\nTo use the trapezoidal rule, we must solve a nonlinear system of equations for $\\mathbf{u}_{n+1}$ at each step. We define a function $\\mathbf{G}(\\mathbf{w}) = \\mathbf{0}$ where we seek the root $\\mathbf{w} = \\mathbf{u}_{n+1}$:\n$$\n\\mathbf{G}(\\mathbf{w}) = \\mathbf{w} - \\mathbf{u}_n - \\frac{h}{2} \\left[ \\mathbf{f}(t_n, \\mathbf{u}_n) + \\mathbf{f}(t_{n+1}, \\mathbf{w}) \\right] = \\mathbf{0}\n$$\nThis is solved using Newton's method. Starting with an initial guess $\\mathbf{w}^{(0)}$ (e.g., $\\mathbf{w}^{(0)} = \\mathbf{u}_n$), we iterate:\n$$\n\\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - [J_G(\\mathbf{w}^{(k)})]^{-1} \\mathbf{G}(\\mathbf{w}^{(k)})\n$$\nThe Jacobian of $\\mathbf{G}$ with respect to $\\mathbf{w}$ is $J_G(\\mathbf{w}) = I - \\frac{h}{2} J_f(t_{n+1}, \\mathbf{w})$, where $I$ is the identity matrix and $J_f$ is the Jacobian of the ODE vector field $\\mathbf{f}$. For our system, the components of $J_f$ are:\n$$\nJ_f(x, y) = \\begin{pmatrix} \\frac{\\partial f_x}{\\partial x}  \\frac{\\partial f_x}{\\partial y} \\\\ \\frac{\\partial f_y}{\\partial x}  \\frac{\\partial f_y}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} r\\left(1-\\frac{2x}{K}\\right) - \\frac{cy}{(1+\\alpha x)^2}  -\\frac{cx}{1+\\alpha x} \\\\ \\frac{\\varepsilon\\eta cy}{(1+\\alpha x)^2}  \\varepsilon\\left(\\frac{\\eta cx}{1+\\alpha x} - m\\right) \\end{pmatrix}\n$$\nThe Newton iteration involves solving the linear system $J_G(\\mathbf{w}^{(k)}) \\Delta\\mathbf{w}^{(k)} = -\\mathbf{G}(\\mathbf{w}^{(k)})$ for the update $\\Delta\\mathbf{w}^{(k)}$, then setting $\\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} + \\Delta\\mathbf{w}^{(k)}$. We stop when the norm of the residual $\\mathbf{G}(\\mathbf{w}^{(k)})$ is below a tolerance of $10^{-12}$.\n\n**Algorithmic Design for the Study**\n\nFor each parameter set $(\\mathsf{A}, \\mathsf{B}, \\mathsf{C})$, the following procedure is executed:\n1.  **Reference Solution**: A high-accuracy reference solution $\\mathbf{u}_{\\text{ref}}$ at $t=T=2$ is computed using a robust stiff solver (`Radau` from `SciPy`) with very strict tolerances (`atol=10^{-13}`, `rtol=10^{-13}`).\n2.  **Step-Halving Loop**: A loop is run over the step counts $N_k \\in \\{20, 40, 80, 160, 320\\}$, corresponding to step sizes $h_k = T/N_k$.\n3.  **Method Evaluation**: For each $h_k$:\n    *   The explicit midpoint method is used to integrate the IVP from $t=0$ to $t=T$. The final state $\\mathbf{u}_{\\text{EM}}(T)$ is recorded. We check if the result is finite. The $L_2$ norm of the global error, $\\|\\mathbf{u}_{\\text{EM}}(T) - \\mathbf{u}_{\\text{ref}}\\|_2$, is stored.\n    *   The implicit trapezoidal method is used similarly. The final state $\\mathbf{u}_{\\text{IT}}(T)$ and its global error norm, $\\|\\mathbf{u}_{\\text{IT}}(T) - \\mathbf{u}_{\\text{ref}}\\|_2$, are stored.\n4.  **Analysis and Classification**:\n    *   **Explicit Method Stability**: We check if the explicit method produced any non-finite (e.g., `inf` or `NaN`) results for $\\mathbf{u}_{\\text{EM}}(T)$ across the $5$ step sizes. If so, a flag `explicit_is_unstable` is set to true.\n    *   **Implicit Method Convergence**: We analyze the errors from the implicit method. We find the longest suffix of the error list (corresponding to the smallest step sizes) that is strictly decreasing. If this suffix contains at least $3$ points, we perform a linear regression on $\\log(\\text{error})$ vs. $\\log(h)$ for these points. The slope of this fit is the estimated EOC. If EOC is at least $1.7$, a flag `implicit_realizes_order` is set to true.\n    *   **Final Verdict**: For the given parameter set, the result is $1$ if `implicit_realizes_order` is true AND `explicit_is_unstable` is true. Otherwise, the result is $0$.\n\nThis procedure effectively contrasts the methods. In non-stiff cases, both methods should converge with an EOC near $2$. In stiff cases, the explicit method will fail for larger step sizes due to stability constraints, while the A-stable implicit method will remain stable and demonstrate its theoretical convergence order, thus satisfying the conditions for a result of $1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.linalg import solve as solve_linear_system\n\ndef solve():\n    \"\"\"\n    Performs a step-halving convergence study on a predator-prey system to \n    detect stiffness based on the performance of explicit and implicit solvers.\n    \"\"\"\n\n    # --- Problem Definition ---\n    # Initial Value Problem (IVP) settings\n    T_FINAL = 2.0\n    U0 = np.array([0.5, 0.3])\n\n    # Step-halving study parameters\n    N_STEPS_LIST = [20, 40, 80, 160, 320]\n    H_LIST = [T_FINAL / n for n in N_STEPS_LIST]\n\n    # Newton's method parameters for the implicit solver\n    NEWTON_TOL = 1e-12\n    NEWTON_MAX_ITER = 20\n\n    # Test cases: (r, K, alpha, c, eta, m, epsilon)\n    test_cases = [\n        # Case A: Non-stiff baseline\n        (2.0, 1.0, 2.0, 3.0, 0.7, 0.4, 1.0),\n        # Case B: Moderately stiff\n        (50.0, 1.0, 2.0, 4.0, 0.7, 0.4, 0.1),\n        # Case C: Highly stiff\n        (150.0, 1.0, 2.0, 4.0, 0.7, 0.4, 0.05),\n    ]\n\n    # --- ODE System Definition ---\n    def f_ode(t, u, params):\n        r, K, alpha, c, eta, m, epsilon = params\n        x, y = u\n        \n        # Avoid potential division by zero or negative populations, though unlikely with the chosen IC\n        if x  0: x = 0\n        if y  0: y = 0\n            \n        common_term = (c * x * y) / (1.0 + alpha * x)\n        \n        dxdt = r * x * (1.0 - x / K) - common_term\n        dydt = epsilon * (eta * common_term - m * y)\n        \n        return np.array([dxdt, dydt])\n\n    def jacobian_f(t, u, params):\n        r, K, alpha, c, eta, m, epsilon = params\n        x, y = u\n        \n        # Avoid negative populations in Jacobian calculation\n        if x  0: x = 0\n        if y  0: y = 0\n\n        denom = 1.0 + alpha * x\n        denom_sq = denom * denom\n\n        df1_dx = r * (1.0 - 2.0 * x / K) - (c * y) / denom_sq\n        df1_dy = - (c * x) / denom\n        df2_dx = epsilon * eta * (c * y) / denom_sq\n        df2_dy = epsilon * (eta * (c * x) / denom - m)\n        \n        return np.array([[df1_dx, df1_dy], [df2_dx, df2_dy]])\n\n    # --- Numerical Methods ---\n    def explicit_midpoint_solver(f, u0, t_final, n_steps, params):\n        h = t_final / n_steps\n        u = u0.copy()\n        t = 0.0\n        for _ in range(n_steps):\n            k1 = f(t, u, params)\n            k2 = f(t + h / 2.0, u + h / 2.0 * k1, params)\n            u += h * k2\n            t += h\n            if not np.all(np.isfinite(u)):\n                return u # Propagate non-finite value immediately\n        return u\n\n    def implicit_trapezoidal_solver(f, jac, u0, t_final, n_steps, params):\n        h = t_final / n_steps\n        u_n = u0.copy()\n        t_n = 0.0\n        I = np.identity(len(u0))\n\n        for _ in range(n_steps):\n            f_n = f(t_n, u_n, params)\n            t_np1 = t_n + h\n            \n            # Newton's method to solve for u_{n+1}\n            w = u_n.copy() # Initial guess for u_{n+1}\n            for _ in range(NEWTON_MAX_ITER):\n                f_np1 = f(t_np1, w, params)\n                G = w - u_n - (h / 2.0) * (f_n + f_np1)\n                \n                if np.linalg.norm(G)  NEWTON_TOL:\n                    break\n\n                J_G = I - (h / 2.0) * jac(t_np1, w, params)\n                delta_w = solve_linear_system(J_G, -G)\n                w += delta_w\n            \n            u_n = w\n            t_n = t_np1\n            \n            if not np.all(np.isfinite(u_n)):\n                return u_n\n\n        return u_n\n\n    results = []\n    for params in test_cases:\n        # 1. Compute high-accuracy reference solution\n        ref_sol = solve_ivp(\n            f_ode, [0, T_FINAL], U0, method='Radau', \n            args=(params,), atol=1e-13, rtol=1e-13\n        )\n        u_ref = ref_sol.y[:, -1]\n\n        em_errors = []\n        it_errors = []\n        em_unstable = False\n\n        # 2. Run step-halving study for both methods\n        for i, N in enumerate(N_STEPS_LIST):\n            h = H_LIST[i]\n\n            # Explicit Midpoint\n            u_em = explicit_midpoint_solver(f_ode, U0, T_FINAL, N, params)\n            if not np.all(np.isfinite(u_em)):\n                em_unstable = True\n                em_errors.append(np.inf)\n            else:\n                em_errors.append(np.linalg.norm(u_em - u_ref))\n\n            # Implicit Trapezoidal\n            u_it = implicit_trapezoidal_solver(f_ode, jacobian_f, U0, T_FINAL, N, params)\n            if not np.all(np.isfinite(u_it)):\n                 it_errors.append(np.inf)\n            else:\n                it_errors.append(np.linalg.norm(u_it - u_ref))\n        \n        # 3. Analyze results and classify\n        \n        # Check if implicit method realizes its formal order\n        it_realizes_order = False\n        \n        # Find the longest strictly decreasing suffix of errors\n        fit_indices = []\n        if len(it_errors)  0:\n            fit_indices.append(len(it_errors) - 1)\n            for i in range(len(it_errors) - 2, -1, -1):\n                if it_errors[i+1]  it_errors[i] and it_errors[i+1]  0:\n                    fit_indices.insert(0, i)\n                else:\n                    break\n        \n        if len(fit_indices) = 3:\n            h_fit = np.log([H_LIST[i] for i in fit_indices])\n            err_fit = np.log([it_errors[i] for i in fit_indices])\n            \n            # Use polyfit to find the slope (Experimental Order of Convergence)\n            eoc = np.polyfit(h_fit, err_fit, 1)[0]\n            \n            if eoc = 1.7:\n                it_realizes_order = True\n        \n        # Final classification\n        if it_realizes_order and em_unstable:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many physical systems are defined by conserved quantities or invariants, such as energy or momentum, which standard numerical solvers often fail to preserve. This final practice explores how \"geometric integration\" techniques can remedy this by implementing a projection method that forces the numerical solution onto a constrained manifold. By comparing this projected solver to its standard counterpart, you will investigate the nuanced effects of enforcing invariants on numerical stability, local truncation error, and the global order of convergence. ",
            "id": "3111955",
            "problem": "You are to write a complete, runnable program that implements and evaluates a projection-based solver for constrained Ordinary Differential Equation (ODE) dynamics. The context is the Initial Value Problem (IVP) for a smooth ODE on a constraint manifold that preserves invariants. The scientific base must start from fundamental definitions: an Ordinary Differential Equation (ODE) is $dx/dt = f(x)$, an Initial Value Problem (IVP) specifies $x(0) = x_0$, and an invariant manifold is a set defined by $g(x) = 0$ that is preserved by the exact flow. Consistency is defined via the local truncation error, stability is assessed by boundedness of numerical solutions under perturbations or large step sizes (for linear problems, through the absolute stability analysis), and convergence is the property that the global error tends to zero with step size under consistency and stability. All angles in this problem must be expressed in radians.\n\nYou will evaluate the effect of enforcing invariants by a projection step on both stability and observed convergence order. Consider the constrained dynamics on the unit circle manifold $\\mathcal{M} = \\{ x \\in \\mathbb{R}^2 : \\|x\\| = 1 \\}$ with the rotational dynamics\n$$\n\\frac{dx}{dt} = f(x) = \\omega J x,\n$$\nwhere $J = \\begin{bmatrix} 0  -1 \\\\ 1  0 \\end{bmatrix}$ and $\\omega  0$ is a constant angular speed. The exact solution from $x(0) = (1,0)^\\top$ is\n$$\nx(t) = \\begin{bmatrix} \\cos(\\omega t) \\\\ \\sin(\\omega t) \\end{bmatrix},\n$$\nwhich preserves the invariant $\\|x(t)\\| = 1$ for all $t \\ge 0$.\n\nImplement two one-step numerical methods:\n- Explicit Euler (EE): $x_{n+1} = x_n + h f(x_n)$.\n- Projected Explicit Euler (PEE): compute the tentative step $x_{n+1}^\\ast = x_n + h f(x_n)$, then enforce the invariant by projection onto $\\mathcal{M}$ via normalization:\n$$\nx_{n+1} = \\frac{x_{n+1}^\\ast}{\\|x_{n+1}^\\ast\\|}.\n$$\n\nYou must quantify the following notions:\n- Consistency via local truncation error: starting from the exact initial state $x(0)$, take a single step of size $h$ and compare to the exact solution $x(h)$. The local truncation error is defined as $e_{\\text{loc}}(h) = \\|x(h) - \\Phi_h(x(0))\\|$, where $\\Phi_h$ is the one-step numerical map. The observed local order $p_{\\text{loc}}$ is the slope of $\\log(e_{\\text{loc}}(h))$ versus $\\log(h)$ over a sequence of decreasing $h$.\n- Convergence via global error: integrate to final time $T$ with step size $h$ (with $T/h$ an integer) and compare the numerical state to the exact solution $x(T)$. The global error is $e_{\\text{glob}}(h,T) = \\|x(T) - x_N\\|$, where $x_N$ is the numerical state after $N = T/h$ steps. The observed global order $p_{\\text{glob}}$ is the slope of $\\log(e_{\\text{glob}}(h,T))$ versus $\\log(h)$.\n- Stability: for a large step size and finite horizon $T$, declare a method “stable” if the numerical trajectory remains bounded close to the invariant, formalized here as $\\max_{0 \\le n \\le N} \\|x_n\\| \\le 2$. If $\\|x_n\\|$ becomes non-finite or exceeds $2$, declare “unstable.”\n\nYour program must carry out the following test suite, using the specified parameters and producing the required outputs:\n\n- Test Suite A (Consistency / Local Truncation Error):\n  - Parameters: $\\omega = 1$, initial state $x(0) = (1,0)^\\top$, step sizes $h \\in \\{0.1, 0.05, 0.025, 0.0125\\}$.\n  - Tasks:\n    1. Compute $e_{\\text{loc}}(h)$ for Explicit Euler (EE).\n    2. Compute $e_{\\text{loc}}(h)$ for Projected Explicit Euler (PEE).\n    3. Estimate $p_{\\text{loc}}$ for EE and PEE by linear regression of $\\log(e_{\\text{loc}}(h))$ versus $\\log(h)$.\n\n- Test Suite B (Convergence / Global Error at Final Time):\n  - Parameters: $\\omega = 1$, final time $T = 1$, initial state $x(0) = (1,0)^\\top$, step sizes $h \\in \\{T/20, T/40, T/80, T/160\\} = \\{0.05, 0.025, 0.0125, 0.00625\\}$ with $T/h \\in \\mathbb{N}$.\n  - Tasks:\n    1. Integrate to $T$ using EE and compute $e_{\\text{glob}}(h,T)$ for each $h$.\n    2. Integrate to $T$ using PEE and compute $e_{\\text{glob}}(h,T)$ for each $h$.\n    3. Estimate $p_{\\text{glob}}$ for EE and PEE by linear regression of $\\log(e_{\\text{glob}}(h,T))$ versus $\\log(h)$.\n\n- Test Suite C (Stability under Large Step Size):\n  - Parameters: $\\omega = 5$, final time $T = 4$, step size $h = 0.5$ (so $T/h = 8 \\in \\mathbb{N}$), initial state $x(0) = (1,0)^\\top$.\n  - Tasks:\n    1. Integrate using EE and compute $\\max_{0 \\le n \\le N} \\|x_n\\|$; return a boolean “stable” if this maximum is finite and $\\le 2$, otherwise “unstable.”\n    2. Integrate using PEE and compute the same boolean.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[p_{\\text{loc,EE}}, p_{\\text{loc,PEE}}, p_{\\text{glob,EE}}, p_{\\text{glob,PEE}}, \\text{stable}_{\\text{EE}}, \\text{stable}_{\\text{PEE}}].\n$$\n- The first four entries are floats, and the last two entries are booleans. Angles are in radians, and no physical units are involved. The floats should be rounded to three decimal places in the output string.",
            "solution": "The problem requires an implementation and analysis of two numerical methods for a constrained Ordinary Differential Equation (ODE). The governing ODE is $\\frac{dx}{dt} = f(x)$, with the initial condition $x(0) = x_0$. This constitutes an Initial Value Problem (IVP). The dynamics are constrained to an invariant manifold, a set $\\mathcal{M}$ defined by an algebraic equation $g(x) = 0$ which is preserved by the exact flow of the ODE, meaning if $x_0 \\in \\mathcal{M}$, then $x(t) \\in \\mathcal{M}$ for all time $t$.\n\nThe specific problem considers a rotational dynamics in $\\mathbb{R}^2$ given by:\n$$\n\\frac{dx}{dt} = \\omega J x\n$$\nwhere $x \\in \\mathbb{R}^2$, $\\omega  0$ is a constant angular speed, and $J = \\begin{bmatrix} 0  -1 \\\\ 1  0 \\end{bmatrix}$ is the skew-symmetric matrix for a $\\pi/2$ rotation. The initial condition is $x(0) = (1, 0)^\\top$. The exact solution is $x(t) = (\\cos(\\omega t), \\sin(\\omega t))^\\top$. This solution trajectory is confined to the unit circle, which is the invariant manifold $\\mathcal{M} = \\{x \\in \\mathbb{R}^2 : \\|x\\| = 1 \\}$. The corresponding invariant is $g(x) = \\|x\\|^2 - 1 = 0$.\n\nWe will analyze two numerical methods for this IVP.\n\nThe first method is the standard Explicit Euler (EE) method. It is a one-step method defined by the update rule:\n$$\nx_{n+1} = x_n + h f(x_n)\n$$\nwhere $h$ is the step size. For the given rotational system, the EE update is $x_{n+1} = (I + h\\omega J)x_n$. The norm of the state evolves according to $\\|x_{n+1}\\|^2 = \\|(I + h\\omega J)x_n\\|^2 = x_n^\\top (I - h\\omega J)(I + h\\omega J) x_n = x_n^\\top (I + (h\\omega)^2 J^\\top J) x_n = (1+(h\\omega)^2)\\|x_n\\|^2$. Since the amplification factor $\\sqrt{1+(h\\omega)^2}$ is always greater than $1$ for $h\\omega  0$, the numerical solution spirals outwards, systematically violating the invariant $\\|x\\|=1$. This demonstrates a major weakness of the EE method for conservative systems.\n\nThe second method is the Projected Explicit Euler (PEE) method. This is a geometric integration technique designed to enforce the invariant. It consists of two stages:\n$1$. A tentative step using Explicit Euler: $x_{n+1}^\\ast = x_n + h f(x_n)$.\n$2$. A projection step back to the manifold $\\mathcal{M}$: $x_{n+1} = \\frac{x_{n+1}^\\ast}{\\|x_{n+1}^\\ast\\|}$.\nThis projection by normalization ensures that $\\|x_{n+1}\\| = 1$ for all steps $n \\ge 0$, thus preserving the invariant by construction.\n\nWe evaluate these methods based on consistency, convergence, and stability.\n\nConsistency is evaluated via the local truncation error (LTE), $e_{\\text{loc}}(h) = \\|x(h) - \\Phi_h(x(0))\\|$, where $\\Phi_h$ is the one-step numerical map. The observed order of accuracy $p_{\\text{loc}}$ is estimated from the slope of $\\log(e_{\\text{loc}})$ versus $\\log(h)$. For EE, a Taylor expansion of the exact and numerical solutions reveals an LTE of $O(h^2)$, corresponding to a local order of $p_{\\text{loc,EE}}=2$. For PEE, the projection step surprisingly improves the local accuracy. For systems on spheres where the vector field is tangent to the sphere (as is the case here since $x \\cdot f(x) = \\omega x^\\top Jx = 0$), the projection corrects for first-order terms in the error perpendicular to the tangent space of the manifold, leading to an LTE of $O(h^3)$. Thus, we expect a local order of $p_{\\text{loc,PEE}}=3$.\n\nConvergence is evaluated via the global error at a fixed time $T$, $e_{\\text{glob}}(h,T) = \\|x(T) - x_N\\|$, where $N=T/h$. The global order of convergence $p_{\\text{glob}}$ is found from the slope of $\\log(e_{\\text{glob}})$ versus $\\log(h)$. For a method with local order $p_{\\text{loc}}$, the global order is typically $p_{\\text{glob}} = p_{\\text{loc}}-1$. Therefore, we expect EE to have a global order of $p_{\\text{glob,EE}} = 2-1 = 1$. For PEE, while the local order is $3$, this simple post-step projection scheme does not typically elevate the global order of the underlying method. The accumulation of phase error, which is first-order, dominates the global error. We thus expect the global order to remain $p_{\\text{glob,PEE}}=1$, though with a potentially smaller error constant compared to EE.\n\nStability is assessed by integrating over a finite horizon $T=4$ with a large step size $h=0.5$. A method is deemed stable if the numerical trajectory remains bounded, specifically $\\max_{n}\\|x_n\\| \\le 2$. For EE, with $\\omega=5$ and $h=0.5$, the norm amplification factor at each step is $\\sqrt{1+(5 \\cdot 0.5)^2} = \\sqrt{1+2.5^2} = \\sqrt{7.25} \\approx 2.69$. Since the initial norm is $\\|x_0\\|=1$, the norm after one step is already $\\|x_1\\| \\approx 2.69$, which exceeds the bound of $2$. Thus, EE is expected to be unstable. For PEE, the projection ensures $\\|x_n\\|=1$ for $n \\ge 1$. The maximum norm over the trajectory will be $1$, so the method will be stable by this definition.\n\nThe program implements these three test suites. Test A computes the local errors for a series of decreasing step sizes $h$ and uses linear regression on the log-log data to find $p_{\\text{loc}}$. Test B computes the global errors at $T=1$ for a series of $h$ to find $p_{\\text{glob}}$. Test C integrates with a large step size and checks the stability criterion. All calculations are performed using `numpy` for efficient vector and matrix operations, and `numpy.polyfit` is used to perform the linear regression to find the orders of accuracy.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates Explicit Euler (EE) and Projected Explicit Euler (PEE)\n    methods for a constrained ODE system.\n    \"\"\"\n\n    # Define the ODE function dx/dt = omega * J * x\n    def f(x, omega):\n        # f(x) = omega * [-x[1], x[0]]\n        return omega * np.array([-x[1], x[0]])\n\n    # Define the exact solution x(t) = [cos(omega*t), sin(omega*t)]\n    def exact_solution(t, omega):\n        return np.array([np.cos(omega * t), np.sin(omega * t)])\n\n    # --- Test Suite A: Consistency (Local Truncation Error) ---\n    omega_A = 1.0\n    x0_A = np.array([1.0, 0.0])\n    h_vals_A = np.array([0.1, 0.05, 0.025, 0.0125])\n    \n    errors_loc_ee = []\n    errors_loc_pee = []\n\n    for h in h_vals_A:\n        x_exact_h = exact_solution(h, omega_A)\n        \n        # Tentative step (same for both methods)\n        x_star = x0_A + h * f(x0_A, omega_A)\n        \n        # EE state after one step\n        x1_ee = x_star\n        \n        # PEE state after one step\n        x1_pee = x_star / np.linalg.norm(x_star)\n        \n        # Compute local errors\n        errors_loc_ee.append(np.linalg.norm(x_exact_h - x1_ee))\n        errors_loc_pee.append(np.linalg.norm(x_exact_h - x1_pee))\n\n    # Estimate local orders using linear regression on log-log plot\n    log_h_A = np.log(h_vals_A)\n    log_err_loc_ee = np.log(np.array(errors_loc_ee))\n    log_err_loc_pee = np.log(np.array(errors_loc_pee))\n\n    p_loc_ee = np.polyfit(log_h_A, log_err_loc_ee, 1)[0]\n    p_loc_pee = np.polyfit(log_h_A, log_err_loc_pee, 1)[0]\n\n    # --- Test Suite B: Convergence (Global Error) ---\n    omega_B = 1.0\n    T_B = 1.0\n    x0_B = np.array([1.0, 0.0])\n    h_vals_B = np.array([T_B/20, T_B/40, T_B/80, T_B/160])\n    \n    errors_glob_ee = []\n    errors_glob_pee = []\n\n    x_exact_T = exact_solution(T_B, omega_B)\n\n    for h in h_vals_B:\n        N = int(round(T_B / h))\n        \n        # EE integration\n        x_n_ee = x0_B.copy()\n        for _ in range(N):\n            x_n_ee = x_n_ee + h * f(x_n_ee, omega_B)\n        \n        # PEE integration\n        x_n_pee = x0_B.copy()\n        for _ in range(N):\n            x_star_pee = x_n_pee + h * f(x_n_pee, omega_B)\n            norm_star = np.linalg.norm(x_star_pee)\n            if norm_star  0:\n                x_n_pee = x_star_pee / norm_star\n            else: # In case of zero vector, avoid division by zero\n                x_n_pee = x_star_pee\n\n        # Compute global errors\n        errors_glob_ee.append(np.linalg.norm(x_exact_T - x_n_ee))\n        errors_glob_pee.append(np.linalg.norm(x_exact_T - x_n_pee))\n\n    # Estimate global orders using linear regression\n    log_h_B = np.log(h_vals_B)\n    log_err_glob_ee = np.log(np.array(errors_glob_ee))\n    log_err_glob_pee = np.log(np.array(errors_glob_pee))\n\n    p_glob_ee = np.polyfit(log_h_B, log_err_glob_ee, 1)[0]\n    p_glob_pee = np.polyfit(log_h_B, log_err_glob_pee, 1)[0]\n\n    # --- Test Suite C: Stability ---\n    omega_C = 5.0\n    T_C = 4.0\n    h_C = 0.5\n    x0_C = np.array([1.0, 0.0])\n    N_C = int(round(T_C / h_C))\n    \n    # EE stability check\n    x_n_ee_C = x0_C.copy()\n    max_norm_ee = np.linalg.norm(x_n_ee_C)\n    for _ in range(N_C):\n        x_n_ee_C += h_C * f(x_n_ee_C, omega_C)\n        norm = np.linalg.norm(x_n_ee_C)\n        if not np.isfinite(norm):\n            max_norm_ee = np.inf\n            break\n        max_norm_ee = max(max_norm_ee, norm)\n    \n    stable_ee = max_norm_ee = 2.0\n\n    # PEE stability check\n    x_n_pee_C = x0_C.copy()\n    max_norm_pee = np.linalg.norm(x_n_pee_C)\n    for _ in range(N_C):\n        x_star_C = x_n_pee_C + h_C * f(x_n_pee_C, omega_C)\n        norm_star = np.linalg.norm(x_star_C)\n        if norm_star == 0:\n            max_norm_pee = 0 # norm becomes 0, still = 2\n            x_n_pee_C = x_star_C\n        else:\n             x_n_pee_C = x_star_C / norm_star\n        max_norm_pee = max(max_norm_pee, np.linalg.norm(x_n_pee_C))\n\n    stable_pee = max_norm_pee = 2.0\n\n    # Format and print the final output\n    print(f\"[{p_loc_ee:.3f},{p_loc_pee:.3f},{p_glob_ee:.3f},{p_glob_pee:.3f},{str(stable_ee)},{str(stable_pee)}]\")\n\nsolve()\n```"
        }
    ]
}