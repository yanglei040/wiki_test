{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental concept in analyzing iterative methods is the rate of convergence. This practice introduces quotient-linear (Q-linear) convergence, where the error decreases by a roughly constant factor at each step. You will solidify this concept by first deriving it for a simple fixed-point iteration and then using two practical numerical techniques—the ratio of successive errors and the slope of a log-error plot—to measure the convergence rate directly from generated data. Mastering these diagnostic tools is a foundational skill for analyzing the performance of any iterative algorithm .",
            "id": "3113939",
            "problem": "Consider a fixed-point iteration on the real line defined by $x_{k+1} = g(x_k)$, where $g$ is a function from $\\mathbb{R}$ to $\\mathbb{R}$. Assume that $g$ is a contraction with respect to the standard absolute-value metric, meaning there exists a constant $L$ with $0  L  1$ such that for all $x,y \\in \\mathbb{R}$, the inequality $|g(x) - g(y)| \\le L |x - y|$ holds. Let $x_\\star \\in \\mathbb{R}$ denote the fixed point of $g$, which satisfies $g(x_\\star) = x_\\star$. Define the error at iteration $k$ by $e_k = x_k - x_\\star$ and its magnitude by $|e_k|$.\n\nTasks:\n- Derive, starting solely from the contraction property and the definition of fixed-point iteration, an inequality relating $|e_{k+1}|$ and $|e_k|$. Use this to argue that the iteration exhibits quotient-linear (Q-linear) convergence, where quotient-linear (Q-linear) means that the sequence of ratios $|e_{k+1}| / |e_k|$ converges to a constant as $k \\to \\infty$. Then, for the specific affine mapping $g(x) = a x + b$ with $|a|  1$, identify the quotient-linear factor in terms of $a$ and $b$, and state the condition under which equality holds in the one-step error inequality.\n- Connect the contraction factor to the slope of the plot of $\\log(|e_k|)$ versus $k$ (natural logarithm). Explain, using basic properties of logarithms and sequences, how the slope is related to the quotient-linear factor.\n- Implement a program that, for a set of affine contractions $g(x) = a x + b$, generates the error sequence and numerically estimates the contraction factor in two ways: (i) by using the ratio $|e_{k+1}| / |e_k|$ from the tail of the sequence, and (ii) by performing a least-squares fit of $\\log(|e_k|)$ versus $k$ and exponentiating the fitted slope to recover a factor. Compare both numerical estimates to the theoretical Lipschitz constant $L$.\n\nUse the following test suite of parameter sets $(a,b,x_0,N)$, where $x_0$ is the initial iterate and $N$ is the number of iterations:\n- Test $1$: $(a,b,x_0,N) = (0.5, 1.0, 10.0, 30)$\n- Test $2$: $(a,b,x_0,N) = (0.9, -2.0, 0.0, 60)$\n- Test $3$: $(a,b,x_0,N) = (-0.75, 3.0, -5.0, 50)$\n- Test $4$: $(a,b,x_0,N) = (0.2, 0.0, 100.0, 40)$\n\nFor each test, compute the fixed point $x_\\star$ analytically, generate the sequence $(x_k)_{k=0}^{N}$ via $x_{k+1} = a x_k + b$, compute the error magnitudes $|e_k| = |x_k - x_\\star|$, estimate the ratio-based factor from the last valid pair of successive nonzero errors, estimate the slope-based factor by performing a least-squares linear fit of $\\log(|e_k|)$ versus the iteration index $k$ over all iterations with strictly positive errors (to avoid taking $\\log(0)$), and then exponentiate the fitted slope. The theoretical Lipschitz constant for the affine map is $L = |a|$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of three floating-point numbers in the order $[L,\\widehat{L}_{\\mathrm{ratio}},\\widehat{L}_{\\mathrm{slope}}]$, where $L$ is the theoretical Lipschitz constant, $\\widehat{L}_{\\mathrm{ratio}}$ is the empirical tail ratio estimate, and $\\widehat{L}_{\\mathrm{slope}}$ is the exponential of the fitted slope. For example, the output should look like $[[L_1,\\widehat{L}_{\\mathrm{ratio},1},\\widehat{L}_{\\mathrm{slope},1}],[L_2,\\widehat{L}_{\\mathrm{ratio},2},\\widehat{L}_{\\mathrm{slope},2}],\\ldots]$. No physical units or angles are involved; all reported values are dimensionless real numbers.",
            "solution": "The problem requires an analysis of the convergence properties of a one-dimensional fixed-point iteration, $x_{k+1} = g(x_k)$, focusing on the special case of an affine map, $g(x) = ax+b$. We will first derive the fundamental error inequality, then connect the convergence rate to a logarithmic plot of the error, and finally outline a numerical verification procedure.\n\n### Part 1: One-Step Error Inequality and Q-Linear Convergence\n\nThe fixed-point iteration is defined by the sequence $x_{k+1} = g(x_k)$ for $k = 0, 1, 2, \\ldots$. The fixed point, denoted by $x_\\star$, satisfies the equation $g(x_\\star) = x_\\star$. The error at iteration $k$ is defined as $e_k = x_k - x_\\star$. We are interested in the behavior of the error magnitude, $|e_k|$.\n\nThe error magnitude at step $k+1$ is given by $|e_{k+1}| = |x_{k+1} - x_\\star|$. By substituting the definition of the iteration for $x_{k+1}$ and the equivalent definition of the fixed point, $x_\\star = g(x_\\star)$, we obtain:\n$$|e_{k+1}| = |g(x_k) - g(x_\\star)|$$\nThe problem states that $g$ is a contraction mapping with respect to the standard absolute-value metric on $\\mathbb{R}$. This implies the existence of a Lipschitz constant $L$ such that $0  L  1$ and for all $x, y \\in \\mathbb{R}$:\n$$|g(x) - g(y)| \\le L |x - y|$$\nApplying this inequality with $x = x_k$ and $y = x_\\star$, we get:\n$$|g(x_k) - g(x_\\star)| \\le L |x_k - x_\\star|$$\nRecognizing that $|x_k - x_\\star| = |e_k|$, we arrive at the one-step error inequality:\n$$|e_{k+1}| \\le L |e_k|$$\nThis inequality demonstrates that the error magnitude is guaranteed to decrease by a factor of at least $L$ at each iteration. Since $0  L  1$, this ensures that $\\lim_{k \\to \\infty} |e_k| = 0$, and thus the sequence of iterates $(x_k)$ converges to the fixed point $x_\\star$.\n\nQuotient-linear (Q-linear) convergence is established if the sequence of error ratios converges to a constant $C \\in [0, 1)$:\n$$\\lim_{k \\to \\infty} \\frac{|e_{k+1}|}{|e_k|} = C$$\nThe constant $C$ is known as the quotient-linear factor or the rate of convergence.\n\nFor the specific case of an affine mapping, $g(x) = ax + b$ with $|a|  1$, we can establish an exact relationship for the error propagation. The fixed point $x_\\star$ solves $x_\\star = ax_\\star + b$, yielding $x_\\star = b/(1-a)$.\nLet's re-examine the expression for $|e_{k+1}|$:\n$$|e_{k+1}| = |g(x_k) - g(x_\\star)| = |(ax_k + b) - (ax_\\star + b)| = |ax_k - ax_\\star| = |a(x_k - x_\\star)|$$\nThis simplifies to:\n$$|e_{k+1}| = |a| |x_k - x_\\star| = |a| |e_k|$$\nFor this affine function, the general inequality becomes a precise equality. If the initial error $e_0$ is non-zero, then all subsequent errors $e_k$ will be non-zero (since $|a|  0$ in the relevant test cases). We can therefore divide by $|e_k|$ to find the ratio:\n$$\\frac{|e_{k+1}|}{|e_k|} = |a|$$\nSince this ratio is constant for all $k$, its limit as $k \\to \\infty$ is trivially $|a|$. Thus, the iteration exhibits Q-linear convergence, and the quotient-linear factor is exactly $|a|$. The factor depends on $a$ but not on $b$. The condition under which equality holds in the one-step error inequality is that the function $g$ is affine, as demonstrated. For such a function, the tightest possible Lipschitz constant is $L = |a|$.\n\n### Part 2: Connection to the Slope of the Log-Error Plot\n\nThe exact relation $|e_{k+1}| = |a| |e_k|$ for the affine map allows us to find a closed-form expression for $|e_k|$ by recursive substitution:\n$$|e_k| = |a| |e_{k-1}| = |a|^2 |e_{k-2}| = \\dots = |a|^k |e_0|$$\nwhere $|e_0| = |x_0 - x_\\star|$ is the initial error magnitude. This relation is valid for all $k \\ge 0$.\n\nTo reveal a linear structure, we take the natural logarithm of both sides (assuming $|e_0|  0$):\n$$\\ln(|e_k|) = \\ln(|a|^k |e_0|)$$\nUsing the properties of logarithms, specifically $\\ln(xy) = \\ln(x) + \\ln(y)$ and $\\ln(x^p) = p \\ln(x)$, we can expand the expression:\n$$\\ln(|e_k|) = \\ln(|a|^k) + \\ln(|e_0|) = k \\ln(|a|) + \\ln(|e_0|)$$\nThis equation is in the form of a straight line, $y = mx + c$, where:\n- The dependent variable is $y = \\ln(|e_k|)$.\n- The independent variable is the iteration index, $x = k$.\n- The slope of the line is $m = \\ln(|a|)$.\n- The y-intercept is $c = \\ln(|e_0|)$.\n\nThis shows that a plot of the natural logarithm of the error magnitude versus the iteration index will yield a straight line. The slope of this line, $m$, is the natural logarithm of the Q-linear factor, $|a|$. Consequently, the factor can be recovered from the numerically estimated slope by exponentiation:\n$$|a| = e^m$$\nThis provides a second method for numerically estimating the convergence factor by performing a linear least-squares fit on the data points $(k, \\ln(|e_k|))$.\n\n### Part 3: Numerical Implementation Plan\n\nThe program will execute the following steps for each provided test case $(a, b, x_0, N)$:\n$1$. The theoretical Lipschitz constant $L$ is determined as $|a|$.\n$2$. The fixed point $x_\\star$ is computed analytically as $x_\\star = b / (1-a)$.\n$3$. The sequence of iterates $(x_k)_{k=0}^{N}$ is generated, starting from $x_0$, using the relation $x_{k+1} = a x_k + b$. Simultaneously, the sequence of error magnitudes $|e_k| = |x_k - x_\\star|$ is computed and stored.\n$4$. The ratio-based estimate, $\\widehat{L}_{\\mathrm{ratio}}$, is calculated using the last two error magnitudes in the sequence: $\\widehat{L}_{\\mathrm{ratio}} = |e_N| / |e_{N-1}|$. This is a valid interpretation of \"tail ratio,\" and for an affine map, this ratio is theoretically constant.\n$5$. The slope-based estimate, $\\widehat{L}_{\\mathrm{slope}}$, is obtained via a linear least-squares regression.\n    a. The set of data points $(k, \\ln(|e_k|))$ is constructed for all $k \\in \\{0, 1, \\dots, N\\}$ where $|e_k|  0$.\n    b. A linear fit to these points provides an estimate for the slope, $m$.\n    c. The estimate is then calculated as $\\widehat{L}_{\\mathrm{slope}} = e^m$.\nThe final output for each test case will be the triplet $[L, \\widehat{L}_{\\mathrm{ratio}}, \\widehat{L}_{\\mathrm{slope}}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating convergence factors for affine fixed-point iterations.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, b, x_0, N)\n        (0.5, 1.0, 10.0, 30),\n        (0.9, -2.0, 0.0, 60),\n        (-0.75, 3.0, -5.0, 50),\n        (0.2, 0.0, 100.0, 40),\n    ]\n\n    results = []\n    for a, b, x_0, N in test_cases:\n        # Theoretical Lipschitz constant\n        L_theoretical = np.abs(a)\n\n        # Analytical fixed point\n        # x_star = a * x_star + b  =  x_star * (1 - a) = b  =  x_star = b / (1 - a)\n        # Since |a|  1 is given, 1 - a is never zero.\n        if 1 - a == 0:\n            # This case should not be reached with the given |a|  1.\n            # However, for robustness, handle it.\n            # If a=1 and b=0, any x is a fixed point. If b!=0, no fixed point.\n            # Problem constraints make this moot.\n            x_star = np.nan\n        else:\n            x_star = b / (1.0 - a)\n\n        # Generate sequence and error magnitudes\n        x_k = x_0\n        e_magnitudes = np.zeros(N + 1)\n        for k in range(N + 1):\n            e_k = x_k - x_star\n            e_magnitudes[k] = np.abs(e_k)\n            # Update for next iteration\n            x_k = a * x_k + b\n\n        # (i) Estimate factor using the ratio from the tail of the sequence\n        # The problem states \"last valid pair of successive nonzero errors\".\n        # For the given affine maps with |a|0 and e_0 != 0, errors are never zero.\n        # So we can safely use the last two elements.\n        L_ratio_estimate = e_magnitudes[N] / e_magnitudes[N-1]\n\n        # (ii) Estimate factor using a least-squares fit of log(|e_k|) vs k\n        # Filter for strictly positive errors to avoid log(0)\n        k_indices = np.arange(N + 1)\n        positive_error_mask = e_magnitudes  0\n        \n        k_filtered = k_indices[positive_error_mask]\n        log_e_filtered = np.log(e_magnitudes[positive_error_mask])\n        \n        # Perform linear least-squares fit: log(|e_k|) = m*k + c\n        # np.polyfit returns [slope, intercept] for a degree 1 polynomial\n        if len(k_filtered)  2:\n            # Not enough data points for a fit\n            L_slope_estimate = np.nan\n        else:\n            slope, _ = np.polyfit(k_filtered, log_e_filtered, 1)\n            # The factor is the exponential of the slope\n            L_slope_estimate = np.exp(slope)\n\n        results.append([L_theoretical, L_ratio_estimate, L_slope_estimate])\n\n    # Final print statement in the exact required format.\n    # e.g., [[L1,L_ratio1,L_slope1],[L2,L_ratio2,L_slope2],...]\n    output_str = f\"[{','.join([f'[{l1},{l2},{l3}]' for l1, l2, l3 in results])}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond simply analyzing convergence, we often want to actively control or improve it. Some iterative methods converge by oscillating around the solution, a behavior caused by a negative derivative of the iteration function at the fixed point. This hands-on exercise demonstrates how a powerful technique called damping, or relaxation, can be used to transform this inefficient oscillatory behavior into smooth, monotone convergence. Through this practice, you will learn to theoretically predict and numerically verify how damping alters the convergence factor and stabilizes the iteration .",
            "id": "3113950",
            "problem": "You will study how damping changes oscillatory convergence into monotone convergence and how it alters the local linear convergence rate of a nonlinear fixed-point iteration. Work entirely in the scalar setting. Use the following foundational definitions only: a fixed point is any $x^{\\star}$ such that $g(x^{\\star}) = x^{\\star}$; a Fixed-Point Iteration (FPI) is the recurrence $x_{k+1} = g(x_k)$; the error is $e_k = x_k - x^{\\star}$; and an iteration converges linearly with factor $\\rho$ if $\\lim_{k \\to \\infty} \\lvert e_{k+1} \\rvert / \\lvert e_k \\rvert = \\rho$ with $0 \\le \\rho  1$. To introduce damping, consider the relaxed iteration $x_{k+1} = (1-\\alpha) x_k + \\alpha g(x_k)$ with relaxation parameter $\\alpha \\in (0,1)$.\n\nConstruct a nonlinear iteration function using a quadratic model about a known fixed point:\n$$\ng(x) = c + q \\,(x - c) + r \\,(x - c)^2,\n$$\nwhere $c$ is the fixed point $x^{\\star}$ because $g(c)=c$, $q$ is the first-derivative coefficient at $x^{\\star}$, and $r$ controls nonlinearity. Assume $\\lvert q \\rvert  1$ so that the linearized map is locally contractive, and use initial guesses $x_0$ sufficiently close to $c$.\n\nYour tasks:\n1. Using only fundamental definitions (fixed point, error, and linearization via a first-order Taylor expansion), explain why the undamped iteration $x_{k+1} = g(x_k)$ has asymptotic error dynamics governed by a first-order term and how the sign of $q$ determines whether the errors $e_k$ oscillate in sign. Then, for the damped iteration $x_{k+1} = (1-\\alpha) x_k + \\alpha g(x_k)$, derive the first-order error recurrence and obtain the corresponding local linear convergence factor and the condition on $\\alpha$ under which previously oscillatory errors become monotone near $x^{\\star}$. Do not invoke any prepackaged formulas beyond the definitions provided.\n2. Implement both undamped and damped iterations for the nonlinear map $g$ defined above, for each parameter set in the test suite given below. Use $N = 1000$ total iterations and record errors $e_k = x_k - c$.\n3. For each run (undamped and damped), compute:\n   - An oscillation flag: Over the last $W = 50$ errors, declare the sequence oscillatory if the number of sign flips between consecutive errors is at least $W - 5$, and monotone if it is at most $2$. For sign, use $\\operatorname{sign}(e) = 1$ if $e \\ge 0$ and $-1$ if $e  0$.\n   - An estimated linear convergence factor: compute the ratios $\\lvert e_{k+1} \\rvert / \\lvert e_k \\rvert$ over the last $M = 100$ steps where the denominator is nonzero, and report the median of those ratios. Round each reported factor to six decimal places.\n4. For each parameter set, output a list of four items in this order: oscillation flag for the undamped run (a boolean), estimated factor for the undamped run (a float), oscillation flag for the damped run (a boolean), estimated factor for the damped run (a float). Aggregate the results for all parameter sets into a single outer list.\n\nUse the following test suite (each tuple lists $(c, q, r, x_0, \\alpha)$):\n- Test A (happy path, clear oscillation turned monotone by damping): $(c, q, r, x_0, \\alpha) = (1.2, -0.6, 0.2, 1.3, 0.5)$.\n- Test B (slow oscillation near the stability boundary, damping chosen to remove oscillation): $(c, q, r, x_0, \\alpha) = (-0.7, -0.95, 0.05, -0.72, 0.35)$.\n- Test C (weakly negative slope with stronger nonlinearity, damping chosen to remove oscillation): $(c, q, r, x_0, \\alpha) = (0.0, -0.2, 1.0, 0.4, 0.4)$.\n- Test D (very close to critical oscillatory case, gentle damping): $(c, q, r, x_0, \\alpha) = (2.0, -0.999, 0.01, 2.001, 0.01)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, for example: [[True,0.123456,False,0.012345],[\\dots]].\n- Each inner list corresponds to one test case in the same order as above: $[$undamped\\_osc, undamped\\_factor, damped\\_osc, damped\\_factor$]$.\n- All floats must be rounded to six decimal places. There are no physical units involved in this problem, and no angles are used.",
            "solution": "### Part 1: Theoretical Analysis of Convergence\n\nLet $x^{\\star}$ be a fixed point of the function $g(x)$, such that $g(x^{\\star}) = x^{\\star}$. For the given problem, the fixed point is $c$. A Fixed-Point Iteration (FPI) is defined by the an undamped recurrence relation $x_{k+1} = g(x_k)$. The error at iteration $k$ is $e_k = x_k - x^{\\star}$.\n\n**Undamped Iteration Analysis**\n\nThe error at iteration $k+1$ is $e_{k+1} = x_{k+1} - x^{\\star}$. Substituting the iteration scheme yields $e_{k+1} = g(x_k) - x^{\\star}$. Since $x^{\\star} = g(x^{\\star})$, we can write $e_{k+1} = g(x_k) - g(x^{\\star})$.\n\nWe are given that $x_k$ is sufficiently close to $x^{\\star}$, which means $e_k$ is small. We can perform a first-order Taylor expansion of $g(x_k)$ around the fixed point $x^{\\star}$. Recalling that $x_k = x^{\\star} + e_k$, we have:\n$$\ng(x_k) = g(x^{\\star} + e_k) = g(x^{\\star}) + g'(x^{\\star})e_k + O(e_k^2)\n$$\nSubstituting this back into the expression for $e_{k+1}$:\n$$\ne_{k+1} = \\left( g(x^{\\star}) + g'(x^{\\star})e_k + O(e_k^2) \\right) - g(x^{\\star}) = g'(x^{\\star})e_k + O(e_k^2)\n$$\nThe problem specifies that for the function $g(x) = c + q(x-c) + r(x-c)^2$, the fixed point is $x^{\\star} = c$ and the first derivative at the fixed point is $g'(c) = q$. Thus, the asymptotic error dynamics are governed by the first-order approximation:\n$$\ne_{k+1} \\approx q e_k\n$$\nFrom this relationship, we can determine the local linear convergence factor and the nature of the convergence. The definition of linear convergence is $\\lim_{k \\to \\infty} \\lvert e_{k+1} \\rvert / \\lvert e_k \\rvert = \\rho$. From our approximation, this ratio is $\\lvert q e_k \\rvert / \\lvert e_k \\rvert = \\lvert q \\rvert$. Therefore, the local linear convergence factor is $\\rho = \\lvert q \\rvert$. For convergence, we require $\\rho  1$, which is given by the condition $\\lvert q \\rvert  1$.\n\nThe sign of the errors is also determined by $e_{k+1} \\approx q e_k$.\n- If $q  0$, then $e_{k+1}$ has the same sign as $e_k$. This means if the initial guess $x_0$ is greater than $x^{\\star}$, all subsequent iterates $x_k$ will also be greater than $x^{\\star}$ (and similarly for an initial guess smaller than $x^{\\star}$). This is termed **monotone convergence**.\n- If $q  0$, then $e_{k+1}$ has the opposite sign of $e_k$. The iterates will successively alternate being above and below the fixed point $x^{\\star}$. This is termed **oscillatory convergence**.\n\n**Damped Iteration Analysis**\n\nThe damped iteration is given by $x_{k+1} = (1-\\alpha) x_k + \\alpha g(x_k)$, with a relaxation parameter $\\alpha \\in (0,1)$. We can define a new iteration function, $g_{\\alpha}(x)$, for this scheme:\n$$\ng_{\\alpha}(x) = (1-\\alpha) x + \\alpha g(x)\n$$\nFirst, we verify that $x^{\\star}=c$ is also a fixed point of $g_{\\alpha}(x)$:\n$$\ng_{\\alpha}(c) = (1-\\alpha) c + \\alpha g(c) = (1-\\alpha) c + \\alpha c = c\n$$\nThe fixed point is preserved. The error dynamics are now governed by $x_{k+1} = g_{\\alpha}(x_k)$. Following the same Taylor expansion procedure as before:\n$$\ne_{k+1} = x_{k+1} - x^{\\star} = g_{\\alpha}(x_k) - g_{\\alpha}(x^{\\star}) \\approx g'_{\\alpha}(x^{\\star}) e_k\n$$\nTo find the new convergence factor, we compute the derivative of $g_{\\alpha}(x)$ and evaluate it at $x^{\\star}=c$:\n$$\ng'_{\\alpha}(x) = \\frac{d}{dx} \\left( (1-\\alpha) x + \\alpha g(x) \\right) = 1-\\alpha + \\alpha g'(x)\n$$\nAt the fixed point $x^{\\star}=c$, where $g'(c) = q$:\n$$\ng'_{\\alpha}(c) = 1-\\alpha + \\alpha g'(c) = 1-\\alpha + \\alpha q\n$$\nLet this new effective derivative be $q_{\\alpha} = 1-\\alpha + \\alpha q$. The error recurrence for the damped iteration is $e_{k+1} \\approx q_{\\alpha} e_k$. The corresponding local linear convergence factor is $\\rho_{\\alpha} = \\lvert q_{\\alpha} \\rvert = \\lvert 1-\\alpha + \\alpha q \\rvert$.\n\nFor a previously oscillatory case where $q  0$, the errors $e_k$ become monotone if the new effective derivative $q_{\\alpha}$ is non-negative. The condition for monotone convergence is therefore:\n$$\nq_{\\alpha} = 1-\\alpha + \\alpha q \\ge 0\n$$\nSolving for $\\alpha$:\n$$\n1 \\ge \\alpha - \\alpha q\n$$\n$$\n1 \\ge \\alpha(1-q)\n$$\nSince the original undamped iteration is assumed to converge, we have $\\lvert q \\rvert  1$. For the oscillatory case, this means $-1  q  0$. This implies that $1-q$ is positive (specifically $1  1-q  2$). We can therefore divide by $(1-q)$ without changing the inequality's direction:\n$$\n\\alpha \\le \\frac{1}{1-q}\n$$\nGiven that $\\alpha \\in (0,1)$ and $1/(1-q)  1/2$, this condition can be satisfied for a suitable choice of $\\alpha$, thereby transforming an oscillatory convergence into a monotone one.\n\n### Part 2: Implementation Plan\n\nThe implementation will follow the theoretical analysis. For each set of parameters $(c, q, r, x_0, \\alpha)$, two simulations will be run:\n1.  **Undamped:** Iterate $x_{k+1} = g(x_k)$ for $N=1000$ steps, where $g(x) = c + q(x-c) + r(x-c)^2$. Store the errors $e_k = x_k - c$.\n2.  **Damped:** Iterate $x_{k+1} = (1-\\alpha)x_k + \\alpha g(x_k)$ for $N=1000$ steps. Store the errors $e_k = x_k - c$.\n\nFor each simulation's recorded error history, two metrics are computed:\n- **Oscillation Flag:** The number of sign changes is counted over the last $W=50$ errors. A custom sign function is used, where $\\operatorname{sign}(e) = 1$ if $e \\ge 0$ and $-1$ if $e  0$. The sequence is flagged as oscillatory (boolean `True`) if the number of sign flips is at least $W-5 = 45$.\n- **Estimated Factor:** The ratios $\\lvert e_{k+1} \\rvert / \\lvert e_k \\rvert$ are computed over the last $M=100$ steps, ignoring cases where $e_k = 0$. The median of these ratios is taken as the estimated linear convergence factor and rounded to six decimal places.\n\nThe final output will be a list of lists, with each inner list containing the four computed metrics for a given parameter set.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing damped and undamped fixed-point iterations.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple contains (c, q, r, x0, alpha)\n    test_cases = [\n        (1.2, -0.6, 0.2, 1.3, 0.5),    # Test A\n        (-0.7, -0.95, 0.05, -0.72, 0.35), # Test B\n        (0.0, -0.2, 1.0, 0.4, 0.4),      # Test C\n        (2.0, -0.999, 0.01, 2.001, 0.01), # Test D\n    ]\n\n    # Global parameters for simulation and analysis\n    N = 1000  # Total iterations\n    W = 50    # Window for oscillation check\n    M = 100   # Window for factor estimation\n\n    def custom_sign(e):\n        \"\"\"Custom sign function as per problem spec: sign(e) = 1 if e = 0, -1 if e  0.\"\"\"\n        return 1 if e = 0 else -1\n\n    def run_simulation(c, q, r, x0, alpha, damped):\n        \"\"\"\n        Runs a single fixed-point iteration simulation (damped or undamped).\n\n        Returns:\n            A list of errors [e_0, e_1, ..., e_N].\n        \"\"\"\n        \n        def g(x_val):\n            \"\"\"The nonlinear iteration function g(x).\"\"\"\n            return c + q * (x_val - c) + r * (x_val - c)**2\n\n        x = x0\n        errors = np.zeros(N + 1)\n        errors[0] = x - c\n\n        for k in range(N):\n            if damped:\n                x = (1 - alpha) * x + alpha * g(x)\n            else:\n                x = g(x)\n            errors[k + 1] = x - c\n        \n        return errors\n\n    def analyze_results(errors):\n        \"\"\"\n        Analyzes the error history to compute oscillation flag and convergence factor.\n\n        Returns:\n            A tuple (is_oscillatory, est_factor).\n        \"\"\"\n        # 1. Compute oscillation flag\n        last_W_errors = errors[N - W + 1 : N + 1]\n        sign_flips = 0\n        for i in range(len(last_W_errors) - 1):\n            if custom_sign(last_W_errors[i]) != custom_sign(last_W_errors[i+1]):\n                sign_flips += 1\n        \n        is_oscillatory = (sign_flips = W - 5)\n\n        # 2. Compute estimated linear convergence factor\n        ratios = []\n        # We need errors from e_{N-M} to e_N to compute M ratios\n        # |e_{k+1}|/|e_k| for k from N-M to N-1\n        for k in range(N - M, N):\n            # Denominator check as per problem\n            if errors[k] != 0:\n                ratio = np.abs(errors[k+1] / errors[k])\n                ratios.append(ratio)\n        \n        if not ratios:\n            # Handle case where all denominators are zero (e.g., convergence in few steps)\n            est_factor = 0.0\n        else:\n            est_factor = np.median(ratios)\n\n        return is_oscillatory, round(est_factor, 6)\n\n    all_results = []\n    for case in test_cases:\n        c, q, r, x0, alpha = case\n        \n        # Run and analyze undamped case\n        undamped_errors = run_simulation(c, q, r, x0, alpha, damped=False)\n        und_osc, und_factor = analyze_results(undamped_errors)\n        \n        # Run and analyze damped case\n        damped_errors = run_simulation(c, q, r, x0, alpha, damped=True)\n        damp_osc, damp_factor = analyze_results(damped_errors)\n        \n        all_results.append(\n            f\"[{str(und_osc)},{und_factor:.6f},{str(damp_osc)},{damp_factor:.6f}]\"\n        )\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world scientific problems rarely involve only a single variable. This practice extends our analysis from one-dimensional iterations to multivariable optimization by exploring the coordinate descent method. You will learn that in higher dimensions, the scalar convergence factor is replaced by an iteration matrix, and the rate of convergence is governed by its spectral radius, $\\rho(M)$. This exercise guides you through a comparison of two fundamental update strategies: a deterministic cyclic approach and a modern randomized one, highlighting the theoretical tools needed to analyze and contrast the performance of complex, high-dimensional algorithms .",
            "id": "3113892",
            "problem": "Consider minimizing the strictly convex quadratic function $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$, where $A \\in \\mathbb{R}^{n \\times n}$ is a real, symmetric, strictly diagonally dominant matrix with positive diagonal entries, and $\\mathbf{b} \\in \\mathbb{R}^{n}$. Such an $A$ is Symmetric Positive Definite (SPD), which guarantees a unique minimizer $\\mathbf{x}^{\\star}$ satisfying $A \\mathbf{x}^{\\star} = \\mathbf{b}$. Coordinate descent iteratively updates one coordinate of $\\mathbf{x}$ at a time by performing exact one-dimensional minimization of $f$ along that coordinate, keeping the others fixed. Two coordinate sampling schemes are considered: cyclic (deterministic order) and randomized (stochastic selection according to a distribution on coordinates).\n\nStarting only from the fundamental definition of the gradient of the quadratic function $f$ and the concept of exact one-dimensional minimization, derive the linear error-update representation for a single coordinate update as an iteration operator acting on the current error $\\mathbf{e} = \\mathbf{x} - \\mathbf{x}^{\\star}$. Use this to define the iteration matrix that represents:\n- One full cyclic epoch that visits coordinates in a fixed order once.\n- One randomized step, in terms of the expected operator under a given coordinate sampling distribution.\nThen interpret their convergence rates through the spectral radius, defined as $\\rho(M) = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } M \\}$, of the corresponding iteration matrices acting on the expected error. Explain why the expected error after $k$ independent randomized steps follows a linear iteration in expectation, and construct the operator corresponding to one randomized epoch of $n$ independent steps.\n\nYour program must implement the following tasks for the provided test suite, without requiring any external input:\n1. For each test case, construct the per-coordinate iteration operators from the derived formula and form:\n   - The cyclic-epoch iteration matrix (product over one pass in a fixed order).\n   - The expected one-step randomized iteration matrix for the given sampling distribution.\n2. Compute the spectral radius for the cyclic-epoch matrix and for the expected one-step randomized matrix. Also compute the randomized-epoch spectral radius corresponding to $n$ independent randomized steps.\n3. For each test case, output a sequence of four values:\n   - The cyclic-epoch spectral radius (a float).\n   - The randomized one-step spectral radius (a float).\n   - The randomized-epoch spectral radius (a float).\n   - A boolean indicating whether the randomized epoch has strictly smaller spectral radius than the cyclic epoch (i.e., whether it is predicted to be faster under this spectral-radius interpretation).\n\nTest suite:\n- Case $1$ (edge case, purely diagonal): $n = 3$, \n  $$A = \\begin{bmatrix}\n  4  0  0 \\\\\n  0  5  0 \\\\\n  0  0  6\n  \\end{bmatrix}, \\quad \\text{uniform sampling } \\mathbf{p} = \\left[\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}\\right].$$\n- Case $2$ (mildly coupled, strictly diagonally dominant): $n = 3$, \n  $$A = \\begin{bmatrix}\n  4  -0.9  0.2 \\\\\n  -0.9  3.5  -0.4 \\\\\n  0.2  -0.4  2.8\n  \\end{bmatrix}, \\quad \\text{uniform sampling } \\mathbf{p} = \\left[\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}\\right].$$\n- Case $3$ (larger, non-uniform sampling proportional to diagonal entries): $n = 5$, \n  $$A = \\begin{bmatrix}\n  8  -0.5  0.2  0  0.1 \\\\\n  -0.5  7  0.3  -0.4  0 \\\\\n  0.2  0.3  9  -0.6  0.5 \\\\\n  0  -0.4  -0.6  6  -0.3 \\\\\n  0.1  0  0.5  -0.3  5\n  \\end{bmatrix}, \\quad \\mathbf{p} \\text{ given by } p_j = \\frac{A_{jj}}{\\sum_{i=1}^{n} A_{ii}} \\text{ for } j = 1,\\dots,n.$$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Concatenate the four values for Case $1$, followed by the four values for Case $2$, followed by the four values for Case $3$, all in order. For example, the format is $[\\text{c1\\_cyc},\\text{c1\\_rand\\_step},\\text{c1\\_rand\\_epoch},\\text{c1\\_better},\\text{c2\\_cyc},\\dots,\\text{c3\\_better}]$, where each $\\text{c\\*}$ value is a float and each $\\text{better}$ value is a boolean. No physical units or angle units are involved; all numerical answers must be provided as plain decimals or booleans in the single specified output line.",
            "solution": "The objective function to minimize is $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$. Since the matrix $A$ is symmetric, the gradient of $f(\\mathbf{x})$ is given by:\n$$\n\\nabla f(\\mathbf{x}) = A \\mathbf{x} - \\mathbf{b}\n$$\nThe unique minimizer $\\mathbf{x}^{\\star}$ is the solution to the linear system $\\nabla f(\\mathbf{x}^{\\star}) = \\mathbf{0}$, which implies $A \\mathbf{x}^{\\star} = \\mathbf{b}$.\n\n**1. Single Coordinate Update Derivation**\n\nCoordinate descent updates one coordinate at a time. Let $\\mathbf{x}^{(k)}$ be the current iterate. To find the next iterate $\\mathbf{x}^{(k+1)}$ by updating the $i$-th coordinate, we keep all other coordinates fixed ($x_j^{(k+1)} = x_j^{(k)}$ for $j \\neq i$) and perform an exact one-dimensional minimization of $f$ along the $i$-th coordinate axis. This is equivalent to solving for $x_i^{(k+1)}$ such that the $i$-th component of the gradient at the new point is zero:\n$$\n(\\nabla f(\\mathbf{x}^{(k+1)}))_i = (A \\mathbf{x}^{(k+1)} - \\mathbf{b})_i = 0\n$$\nExpanding the $i$-th row of this matrix equation gives:\n$$\n\\sum_{j=1}^{n} A_{ij} x_j^{(k+1)} - b_i = 0\n$$\nSeparating the term with $x_i^{(k+1)}$ and substituting $x_j^{(k+1)} = x_j^{(k)}$ for $j \\neq i$:\n$$\nA_{ii} x_i^{(k+1)} + \\sum_{j \\neq i} A_{ij} x_j^{(k)} - b_i = 0\n$$\nSolving for $x_i^{(k+1)}$ yields the update rule:\n$$\nx_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j \\neq i} A_{ij} x_j^{(k)} \\right)\n$$\nNote that since $A$ is strictly diagonally dominant with positive diagonals, $A_{ii} \\neq 0$.\n\n**2. Error Propagation and Per-Coordinate Operator ($M_i$)**\n\nWe now express this update in terms of the error vector $\\mathbf{e}^{(k)} = \\mathbf{x}^{(k)} - \\mathbf{x}^{\\star}$. Substituting $\\mathbf{x}^{(k)} = \\mathbf{e}^{(k)} + \\mathbf{x}^{\\star}$ into the update rule:\n$$\ne_i^{(k+1)} + x_i^{\\star} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j \\neq i} A_{ij} (e_j^{(k)} + x_j^{\\star}) \\right)\n$$\nFrom the optimality condition $A \\mathbf{x}^{\\star} = \\mathbf{b}$, we have $b_i = \\sum_{j=1}^{n} A_{ij} x_j^{\\star} = A_{ii} x_i^{\\star} + \\sum_{j \\neq i} A_{ij} x_j^{\\star}$. Substituting this for $b_i$:\n$$\ne_i^{(k+1)} + x_i^{\\star} = \\frac{1}{A_{ii}} \\left( \\left( A_{ii} x_i^{\\star} + \\sum_{j \\neq i} A_{ij} x_j^{\\star} \\right) - \\sum_{j \\neq i} A_{ij} e_j^{(k)} - \\sum_{j \\neq i} A_{ij} x_j^{\\star} \\right)\n$$\nSimplifying the expression:\n$$\ne_i^{(k+1)} + x_i^{\\star} = \\frac{1}{A_{ii}} \\left( A_{ii} x_i^{\\star} - \\sum_{j \\neq i} A_{ij} e_j^{(k)} \\right) = x_i^{\\star} - \\frac{1}{A_{ii}} \\sum_{j \\neq i} A_{ij} e_j^{(k)}\n$$\nThis gives the update for the $i$-th component of the error vector:\n$$\ne_i^{(k+1)} = - \\sum_{j \\neq i} \\frac{A_{ij}}{A_{ii}} e_j^{(k)}\n$$\nThe other components of the error remain unchanged in this step: $e_j^{(k+1)} = e_j^{(k)}$ for $j \\neq i$. This linear transformation on the error vector $\\mathbf{e}^{(k)}$ can be represented by a matrix-vector product $\\mathbf{e}^{(k+1)} = M_i \\mathbf{e}^{(k)}$. The matrix $M_i$ is the identity matrix $I$ except for its $i$-th row. The elements of $M_i$ are:\n$$\n(M_i)_{jk} =\n\\begin{cases}\n\\delta_{jk}  \\text{if } j \\neq i \\\\\n-A_{ik}/A_{ii}  \\text{if } j = i, k \\neq i \\\\\n0  \\text{if } j = i, k = i\n\\end{cases}\n$$\nwhere $\\delta_{jk}$ is the Kronecker delta.\n\n**3. Cyclic Epoch Iteration Operator ($M_{\\text{cyc}}$)**\n\nA full cyclic epoch involves updating coordinates in a fixed order, say $i=1, 2, \\dots, n$. If $\\mathbf{e}^{(k)}$ is the error at the beginning of an epoch, the error after updating coordinate $1$ is $M_1 \\mathbf{e}^{(k)}$. The error after subsequently updating coordinate $2$ is $M_2 (M_1 \\mathbf{e}^{(k)})$, and so on. After one full epoch, the final error $\\mathbf{e}^{(k+1)}$ is:\n$$\n\\mathbf{e}^{(k+1)} = M_n M_{n-1} \\cdots M_2 M_1 \\mathbf{e}^{(k)}\n$$\nThe iteration matrix for one full cyclic epoch is the product of the individual operators:\n$$\nM_{\\text{cyc}} = M_n M_{n-1} \\cdots M_1\n$$\nThe asymptotic rate of convergence is determined by the spectral radius $\\rho(M_{\\text{cyc}})$.\n\n**4. Randomized Step and Epoch Operators ($M_{\\text{rand\\_step}}$, $M_{\\text{rand\\_epoch}}$)**\n\nIn randomized coordinate descent, at each step $k$, a coordinate $i_k$ is chosen from $\\{1, \\dots, n\\}$ with probability $p_{i_k}$, where $\\mathbf{p} = [p_1, \\dots, p_n]^{\\top}$ is a probability distribution. The error update is then $\\mathbf{e}^{(k+1)} = M_{i_k} \\mathbf{e}^{(k)}$.\n\nThe expected error after one step, conditioned on the current error $\\mathbf{e}^{(k)}$, is:\n$$\n\\mathbb{E}[\\mathbf{e}^{(k+1)} | \\mathbf{e}^{(k)}] = \\sum_{j=1}^{n} p_j (M_j \\mathbf{e}^{(k)}) = \\left( \\sum_{j=1}^{n} p_j M_j \\right) \\mathbf{e}^{(k)}\n$$\nThis demonstrates that the expected error follows a linear iteration. The corresponding operator is the expected one-step randomized iteration matrix:\n$$\nM_{\\text{rand\\_step}} = \\mathbb{E}[M_{i_k}] = \\sum_{j=1}^{n} p_j M_j\n$$\nThe convergence of the expected error is governed by $\\rho(M_{\\text{rand\\_step}})$.\n\nA randomized epoch is defined as $n$ independent randomized steps. Let the sequence of chosen indices be $i_1, i_2, \\dots, i_n$. The error after $n$ steps is $\\mathbf{e}^{(k+n)} = M_{i_n} \\cdots M_{i_1} \\mathbf{e}^{(k)}$. The expected error after this epoch is:\n$$\n\\mathbb{E}[\\mathbf{e}^{(k+n)} | \\mathbf{e}^{(k)}] = \\mathbb{E}[M_{i_n} \\cdots M_{i_1}] \\mathbf{e}^{(k)}\n$$\nDue to the independence of the coordinate choices, the expectation of the product is the product of expectations:\n$$\n\\mathbb{E}[\\mathbf{e}^{(k+n)} | \\mathbf{e}^{(k)}] = \\mathbb{E}[M_{i_n}] \\cdots \\mathbb{E}[M_{i_1}] \\mathbf{e}^{(k)} = (M_{\\text{rand\\_step}})^n \\mathbf{e}^{(k)}\n$$\nThus, the operator for a randomized epoch is $M_{\\text{rand\\_epoch}} = (M_{\\text{rand\\_step}})^n$.\n\nThe spectral radius of this epoch operator can be related to the one-step operator's spectral radius using the property $\\rho(M^k) = (\\rho(M))^k$ for any square matrix $M$ and integer $k \\ge 1$. Therefore:\n$$\n\\rho(M_{\\text{rand\\_epoch}}) = \\rho((M_{\\text{rand\\_step}})^n) = (\\rho(M_{\\text{rand\\_step}}))^n\n$$\nThis allows us to compare the per-epoch convergence rates of the cyclic and randomized schemes by comparing $\\rho(M_{\\text{cyc}})$ and $\\rho(M_{\\text{rand\\_epoch}})$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_metrics(A, p):\n    \"\"\"\n    Computes the spectral radii for cyclic and randomized coordinate descent.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix of the quadratic form.\n        p (np.ndarray): The probability distribution for randomized coordinate selection.\n\n    Returns:\n        tuple: A tuple containing:\n            - rho_cyc (float): Spectral radius of the cyclic epoch operator.\n            - rho_rand_step (float): Spectral radius of the one-step randomized operator.\n            - rho_rand_epoch (float): Spectral radius of the n-step randomized epoch operator.\n            - is_faster (bool): True if randomized epoch has a strictly smaller spectral radius.\n    \"\"\"\n    n = A.shape[0]\n\n    # 1. Construct the per-coordinate iteration operators M_i\n    m_coords = []\n    for i in range(n):\n        Mi = np.identity(n)\n        # The i-th row is updated based on the derivation\n        for j in range(n):\n            if i == j:\n                Mi[i, j] = 0.0\n            else:\n                Mi[i, j] = -A[i, j] / A[i, i]\n        m_coords.append(Mi)\n\n    # 2. Construct the cyclic-epoch iteration matrix M_cyc = M_{n-1} ... M_1 M_0\n    # The order of updates is 0, 1, ..., n-1\n    M_cyc = np.identity(n)\n    for Mi in m_coords:\n        M_cyc = Mi @ M_cyc\n\n    # 3. Construct the expected one-step randomized iteration matrix M_rand_step\n    M_rand_step = np.zeros((n, n))\n    for i in range(n):\n        M_rand_step += p[i] * m_coords[i]\n\n    # 4. Compute spectral radii\n    # For a complex number z = x + iy, np.abs(z) computes sqrt(x^2 + y^2)\n    rho_cyc = np.max(np.abs(np.linalg.eigvals(M_cyc)))\n    rho_rand_step = np.max(np.abs(np.linalg.eigvals(M_rand_step)))\n    \n    # The spectral radius of the randomized epoch operator is (rho_rand_step)^n\n    rho_rand_epoch = rho_rand_step ** n\n\n    # 5. Compare the epoch spectral radii\n    is_faster = rho_rand_epoch  rho_cyc\n\n    return rho_cyc, rho_rand_step, rho_rand_epoch, is_faster\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 3,\n            \"A\": np.array([\n                [4.0, 0.0, 0.0],\n                [0.0, 5.0, 0.0],\n                [0.0, 0.0, 6.0]\n            ]),\n            \"p_type\": \"uniform\"\n        },\n        {\n            \"n\": 3,\n            \"A\": np.array([\n                [4.0, -0.9, 0.2],\n                [-0.9, 3.5, -0.4],\n                [0.2, -0.4, 2.8]\n            ]),\n            \"p_type\": \"uniform\"\n        },\n        {\n            \"n\": 5,\n            \"A\": np.array([\n                [8.0, -0.5, 0.2, 0.0, 0.1],\n                [-0.5, 7.0, 0.3, -0.4, 0.0],\n                [0.2, 0.3, 9.0, -0.6, 0.5],\n                [0.0, -0.4, -0.6, 6.0, -0.3],\n                [0.1, 0.0, 0.5, -0.3, 5.0]\n            ]),\n            \"p_type\": \"proportional\"\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        n = case[\"n\"]\n        \n        if case[\"p_type\"] == \"uniform\":\n            p = np.full(n, 1.0 / n)\n        elif case[\"p_type\"] == \"proportional\":\n            diagonals = np.diag(A)\n            p = diagonals / np.sum(diagonals)\n        \n        metrics = compute_metrics(A, p)\n        results.extend(metrics)\n\n    # Format the final output string\n    # Booleans will be converted to \"True\" or \"False\" by str()\n    # Floats will be in standard decimal representation\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}