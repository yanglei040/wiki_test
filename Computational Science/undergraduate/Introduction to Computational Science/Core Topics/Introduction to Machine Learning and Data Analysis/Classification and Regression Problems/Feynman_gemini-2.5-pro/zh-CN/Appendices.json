{
    "hands_on_practices": [
        {
            "introduction": "在机器学习中，一个核心挑战是在模型的复杂性与泛化能力之间找到最佳平衡点，以避免欠拟合和过拟合。本练习提供了一种量化方法来分析这一权衡，您将通过回归分析对泛化差距进行建模，并利用该模型来“分类”出最优的模型复杂度。这个实践将帮助您理解验证集误差曲线出现“抖动”时，如何做出更稳健的模型选择 。",
            "id": "3107026",
            "problem": "给定在同一数据集上拟合的 $d$ 次多项式回归模型的观测值。对于每个测试用例，会为您提供一组次数 $d \\in \\mathbb{N}$，以及对应的经验训练均方误差（Mean Squared Error (MSE)）$E_{\\text{train}}(d)$ 和经验交叉验证均方误差 $E_{\\text{val}}(d)$。您的任务是通过回归泛化差距 $g(d)$ 与 $d$ 的关系来预测过拟合风险，并分类出使预期泛化误差估计值最小的最优次数。\n\n基本原理和定义：\n- 在复杂度为 $d$ 时的泛化误差可以表示为 $E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$，其中 $g(d)$ 是泛化差距，定义为 $g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$。在实践中，$E_{\\text{test}}(d)$ 是未知的，交叉验证被用作一种经过充分检验的替代方法，因此我们估计差距为 $g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$。\n- 过拟合风险随模型复杂度的增加而增加，这反映为 $g(d)$ 作为 $d$ 的函数的正斜率。\n\n算法要求：\n- 对于每个测试用例，计算每个观测次数 $d_k$ 处的 $g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$。\n- 使用普通最小二乘法（Ordinary Least Squares (OLS)）将直线 $g(d) \\approx a + b\\,d$ 拟合到观测对 $\\{(d_k, g(d_k))\\}$。\n- 将每个观测次数处的预期泛化误差估计值构建为 $\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$。\n- 将最优次数 $\\widehat{d}^{\\star}$ 分类为给定集合中使 $\\widehat{E}_{\\text{gen}}(d_k)$ 最小化的整数次数。如果在数值公差范围内有多个次数达到相同的最小值，则选择其中最小的次数（偏好较低的复杂度）。\n- 每个测试用例报告两个量：差距与次数回归的估计斜率 $\\widehat{b}$（四舍五入到4位小数），以及分类出的最优次数 $\\widehat{d}^{\\star}$（作为整数）。\n\n测试套件：\n- 测试用例 $1$：$d = [1,2,3,4,5]$，$E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$，$E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$。\n- 测试用例 $2$：$d = [1,2,3,4]$，$E_{\\text{train}} = [5.0,4.2,3.6,3.3]$，$E_{\\text{val}} = [5.0,4.2,3.6,3.3]$。\n- 测试用例 $3$：$d = [1,2,3,4,5,6]$，$E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$，$E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$。\n- 测试用例 $4$：$d = [1,10]$，$E_{\\text{train}} = [5.0,0.1]$，$E_{\\text{val}} = [5.1,6.0]$。\n- 测试用例 $5$：$d = [2,3,4]$，$E_{\\text{train}} = [8.0,6.0,5.0]$，$E_{\\text{val}} = [8.7,6.6,5.4]$。\n\n输出规格：\n- 对于每个测试用例，输出一个包含两个元素的列表 $[\\widehat{b}, \\widehat{d}^{\\star}]$，其中 $\\widehat{b}$ 是四舍五入到4位小数的斜率，$\\widehat{d}^{\\star}$ 是一个整数。\n- 您的程序应生成单行输出，其中包含一个列表，该列表按顺序汇总了每个测试用例的输出，且不含空格。每个元素本身必须是如上所述的双元素列表。最终打印输出必须只有一行。\n\n不涉及物理单位。此处不涉及任何角度（如有）。不需要百分比。\n\n您的程序必须是一个完整的、可运行的程序，其内部定义了上述测试用例，并产生指定的单行输出。",
            "solution": "### 步骤 1：提取已知信息\n- **模型**：次数为 $d \\in \\mathbb{N}$ 的多项式回归模型。\n- **每个测试用例的数据**：一组次数 $d_k$，对应的训练均方误差（MSE）$E_{\\text{train}}(d_k)$ 和交叉验证均方误差 $E_{\\text{val}}(d_k)$。\n- **定义**：\n    - 泛化误差：$E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$。\n    - 泛化差距：$g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$。\n    - 经验泛化差距估计：$g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$。\n- **算法要求**：\n    1.  计算每个次数的经验差距：$g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$。\n    2.  使用普通最小二乘法（OLS）将直线 $g(d) \\approx a + b\\,d$ 拟合到数据点 $\\{(d_k, g(d_k))\\}$，以找到估计的截距 $\\widehat{a}$ 和斜率 $\\widehat{b}$。\n    3.  估计每个次数处的预期泛化误差：$\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$。\n    4.  将最优次数 $\\widehat{d}^{\\star}$ 分类为给定集合中使 $\\widehat{E}_{\\text{gen}}(d_k)$ 最小化的整数次数。平局规则是，如果多个次数产生相同的最小值，则选择最小的次数。\n    5.  报告四舍五入到4位小数的斜率 $\\widehat{b}$ 和整数最优次数 $\\widehat{d}^{\\star}$。\n- **测试套件**：\n    - **测试用例 1**：$d = [1,2,3,4,5]$，$E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$，$E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$。\n    - **测试用例 2**：$d = [1,2,3,4]$，$E_{\\text{train}} = [5.0,4.2,3.6,3.3]$，$E_{\\text{val}} = [5.0,4.2,3.6,3.3]$。\n    - **测试用例 3**：$d = [1,2,3,4,5,6]$，$E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$，$E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$。\n    - **测试用例 4**：$d = [1,10]$，$E_{\\text{train}} = [5.0,0.1]$，$E_{\\text{val}} = [5.1,6.0]$。\n    - **测试用例 5**：$d = [2,3,4]$，$E_{\\text{train}} = [8.0,6.0,5.0]$，$E_{\\text{val}} = [8.7,6.6,5.4]$。\n- **输出规格**：一个单行列表，其中包含每个测试用例的双元素列表 $[\\widehat{b}, \\widehat{d}^{\\star}]$，最终输出字符串中没有空格。\n\n### 步骤 2：使用提取的已知信息进行验证\n根据验证标准评估问题陈述。\n- **科学依据**：该问题基于统计学习理论的基本概念，包括偏差-方差权衡、过拟合、训练误差、验证误差和泛化误差。使用普通最小二乘法来建模模型复杂性与泛化差距之间的关系是一种标准且可靠的分析技术。\n- **良构性**：该问题是良构的。对于每个测试用例，用于 OLS 回归的不同数据点的数量至少为2，确保了线参数的唯一解。对最优次数的搜索是在有限集上的最小化，保证有解。平局打破规则确保了此解的唯一性。\n- **客观性**：该问题使用精确的数学定义和清晰、明确的算法过程进行陈述。输入是数值，要求的输出是明确定义的。\n- **缺陷清单**：该问题没有违反任何指定的缺陷。它在科学上是合理的、可形式化的、完整的、现实的并且是良构的。它需要相当程度的计算和推理。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整的解决方案。\n\n该问题要求分析模型性能作为复杂度（特别是多项式回归模型的次数 $d$）的函数。分析的核心在于理解和建模泛化差距 $g(d)$，它表示模型在未见数据上的性能（由验证误差 $E_{\\text{val}}$ 近似）与其在训练数据上的性能（$E_{\\text{train}}$）之间的差异。随复杂度增加而增大的差距是过拟合的标志。\n\n指定的算法旨在创建泛化误差的平滑估计，以做出比简单地选择具有最低验证误差的模型更稳健的模型选择决策。验证误差本身可能存在噪声，而对泛化差距的趋势进行建模有助于滤除这种噪声。\n\n每个测试用例的步骤如下：\n\n1.  **计算经验泛化差距**：对于每个给定的多项式次数 $d_k$，我们使用提供的训练和验证误差计算经验泛化差距 $g(d_k)$：\n    $$g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$$\n\n2.  **通过 OLS 建模泛化差距**：我们假设模型复杂度 $d$ 和泛化差距 $g(d)$ 之间存在线性关系。我们使用普通最小二乘法（OLS）将一条直线 $\\widehat{g}(d) = \\widehat{a} + \\widehat{b}d$ 拟合到观测点集 $\\{(d_k, g(d_k))\\}$。选择斜率 $\\widehat{b}$ 和截距 $\\widehat{a}$ 以最小化平方差之和 $\\sum_k (g(d_k) - (\\widehat{a} + \\widehat{b}d_k))^2$。对于一组 $n$ 个点，OLS 估计量由以下公式给出：\n    $$ \\widehat{b} = \\frac{\\sum_{k=1}^{n} (d_k - \\bar{d})(g_k - \\bar{g})}{\\sum_{k=1}^{n} (d_k - \\bar{d})^2} $$\n    $$ \\widehat{a} = \\bar{g} - \\widehat{b}\\bar{d} $$\n    其中 $\\bar{d} = \\frac{1}{n}\\sum_k d_k$ 和 $\\bar{g} = \\frac{1}{n}\\sum_k g(d_k)$ 是样本均值。斜率 $\\widehat{b}$ 可作为过拟合风险的直接度量；正的 $\\widehat{b}$ 表示训练和验证性能之间的差距随着模型复杂度的增加而扩大。\n\n3.  **估计泛化误差**：使用差距的线性模型，我们构建真实泛化误差的平滑估计 $\\widehat{E}_{\\text{gen}}(d_k)$：\n    $$ \\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{g}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}d_k $$\n    该估计将直接观测到的训练误差（反映模型拟合数据的程度）与泛化惩罚的正则化估计（考虑了复杂度）相结合。\n\n4.  **确定最优次数**：最优次数 $\\widehat{d}^{\\star}$ 从给定集合 $\\{d_k\\}$ 中选择，使得我们的估计泛化误差 $\\widehat{E}_{\\text{gen}}(d_k)$ 最小化。\n    $$ \\widehat{d}^{\\star} = \\underset{d_k}{\\arg\\min} \\{ \\widehat{E}_{\\text{gen}}(d_k) \\} $$\n    问题规定，如果出现平局，应选择达到最小误差的次数中最小的那个。这反映了简约原则（奥卡姆剃刀）：当所有其他条件相同时，偏好更简单的模型。\n\n最后，对于每个测试用例，我们报告计算出的斜率 $\\widehat{b}$（过拟合风险的度量）和确定的最优次数 $\\widehat{d}^{\\star}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {'d': [1, 2, 3, 4, 5], \n         'E_train': [9.0, 6.0, 4.5, 4.0, 3.8], \n         'E_val': [10.0, 7.0, 5.0, 5.2, 6.0]},\n        # Test case 2\n        {'d': [1, 2, 3, 4], \n         'E_train': [5.0, 4.2, 3.6, 3.3], \n         'E_val': [5.0, 4.2, 3.6, 3.3]},\n        # Test case 3\n        {'d': [1, 2, 3, 4, 5, 6], \n         'E_train': [12.0, 8.0, 6.0, 5.2, 5.0, 4.9], \n         'E_val': [11.5, 7.3, 5.1, 4.1, 3.7, 3.4]},\n        # Test case 4\n        {'d': [1, 10], \n         'E_train': [5.0, 0.1], \n         'E_val': [5.1, 6.0]},\n        # Test case 5\n        {'d': [2, 3, 4], \n         'E_train': [8.0, 6.0, 5.0], \n         'E_val': [8.7, 6.6, 5.4]},\n    ]\n\n    results_as_strings = []\n    \n    for case in test_cases:\n        # Extract and convert data to numpy arrays for vectorized operations\n        d = np.array(case['d'], dtype=float)\n        E_train = np.array(case['E_train'], dtype=float)\n        E_val = np.array(case['E_val'], dtype=float)\n        \n        # Step 1: Compute the empirical generalization gap\n        g = E_val - E_train\n        \n        # Step 2: Fit a straight line g(d) = a + b*d using OLS.\n        # np.polyfit with degree 1 returns the coefficients [b, a] for slope and intercept.\n        b_hat, a_hat = np.polyfit(d, g, 1)\n        \n        # Step 3: Form the estimated expected generalization error\n        # E_gen_hat(d) = E_train(d) + (a_hat + b_hat*d)\n        E_gen_hat = E_train + a_hat + b_hat * d\n        \n        # Step 4: Classify the optimal degree d_star\n        # np.argmin finds the index of the first occurrence of the minimum value.\n        # Since the input degrees 'd' are sorted, this satisfies the tie-breaking rule\n        # of choosing the smallest degree.\n        min_error_index = np.argmin(E_gen_hat)\n        d_star = int(d[min_error_index])\n        \n        # Format the result for this case as a string \"[b,d_star]\" with no spaces\n        # and b rounded to 4 decimal places.\n        case_result_str = f\"[{b_hat:.4f},{d_star}]\"\n        results_as_strings.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    # The output is a single list containing the results for all test cases.\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "建立一个优秀的回归模型不仅仅是拟合一条直线，更需要检验其基本假设是否成立。本练习将引导您完成一个专业的工作流程：首先，通过正式的统计检验（一个分类任务）来诊断“异方差性”——即误差方差不恒定的常见问题。接着，您将通过对误差方差本身建立一个回归模型，来估计权重并应用加权最小二乘法（WLS）以修正原始模型，从而体验一个完整的模型诊断与优化过程 。",
            "id": "3107027",
            "problem": "要求您编写一个完整的、可运行的程序，该程序针对多个合成数据集，执行基于经典线性模型的三个任务：使用由拉格朗日乘子原理形式化的残差诊断方法，对误差项条件方差 $\\operatorname{Var}(\\epsilon \\mid x)$ 的异方差性进行分类，通过回归估计参数化方差函数 $\\sigma^{2}(x)$，并通过加权最小二乘法重新拟合回归。您的实现必须遵循基于基本定义和经过充分检验的结果的原理性方法，具体如下。\n\n基本模型是简单线性回归\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i=1,\\dots,n,\n$$\n其中误差满足 $\\mathbb{E}[\\epsilon_i \\mid x_i] = 0$。在同方差性下，条件方差是恒定的，$\\operatorname{Var}(\\epsilon_i \\mid x_i) = \\sigma^2$，而在异方差性下，它随 $x_i$ 变化，$\\operatorname{Var}(\\epsilon_i \\mid x_i) = \\sigma^2(x_i)$。普通最小二乘法 (OLS) 选择使残差平方和最小化的系数，\n$$\n(\\widehat{\\beta}_0,\\widehat{\\beta}_1) = \\arg\\min_{\\beta_0,\\beta_1} \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2.\n$$\n定义残差 $r_i = y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 x_i$。\n\n任务 A (通过形式化为检验的残差图进行异方差性分类)：使用 Breusch–Pagan 拉格朗日乘子方法作为残差模式诊断的正式代理。将残差平方对回归量和常数项进行回归，\n$$\nr_i^2 = \\delta_0 + \\delta_1 x_i + u_i,\n$$\n并计算此辅助回归的决定系数 $R^2$。Breusch–Pagan 检验统计量是\n$$\n\\mathrm{LM} = n R^2,\n$$\n在同方差性的零假设下，该统计量近似服从自由度为 $k$ 的卡方分布，其中 $k$ 是辅助回归中非常数回归量的数量。对于本问题，$k = 1$。如果卡方生存概率满足\n$$\np\\text{-value} = \\Pr\\left(\\chi^2_k \\ge \\mathrm{LM}\\right)  0.05,\n$$\n则将数据集分类为异方差。\n\n任务 B (方差函数回归)：使用 OLS 残差，通过将对数残差平方对 $x$ 的一个固定特征进行回归来拟合一个正方差模型：\n$$\n\\log\\left(r_i^2 + \\varepsilon\\right) = \\gamma_0 + \\gamma_1 \\,\\log\\!\\left(1 + x_i^2\\right) + \\eta_i,\n$$\n其中 $\\varepsilon$ 是一个小的正常数，以避免对零取对数。然后将每个 $x_i$ 处的拟合方差定义为\n$$\n\\widehat{\\sigma}_i^2 = \\exp\\!\\left(\\widehat{\\gamma}_0 + \\widehat{\\gamma}_1 \\,\\log\\!\\left(1 + x_i^2\\right)\\right).\n$$\n这确保了对所有 $i$ 都有 $\\widehat{\\sigma}_i^2 > 0$。\n\n任务 C (加权最小二乘法重新拟合)：使用权重 $w_i = 1/\\widehat{\\sigma}_i^2$，计算最小化\n$$\n\\sum_{i=1}^{n} w_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n$$\n的加权最小二乘 (WLS) 估计量，并报告重新拟合的系数。同时计算 OLS 和 WLS 拟合的均方误差 (MSE)，定义为\n$$\n\\mathrm{MSE}_{\\mathrm{OLS}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2,\\quad\n\\mathrm{MSE}_{\\mathrm{WLS}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\widehat{\\beta}^{\\mathrm{WLS}}_0 - \\widehat{\\beta}^{\\mathrm{WLS}}_1 x_i\\right)^2.\n$$\n\n您必须遵循的实现细节：\n- 使用以下测试套件，并使用固定的随机种子以保证可复现性。对于每种情况，按指定独立均匀地抽取 $x_i$，生成均值为 0、标准差函数指定的独立高斯噪声 $\\epsilon_i$，并设置 $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$。\n- 随机种子：对所有随机生成使用种子值 $1337$。\n- 对每个数据集，使用确切的参数：\n\n1. 情况 1 (同方差，“理想情况”)：$n = 200$，$\\beta_0 = 1.25$，$\\beta_1 = -0.75$，$x_i \\sim \\mathrm{Uniform}[-2,2]$，噪声标准差 $\\sigma(x) \\equiv 0.5$ (常数)。\n2. 情况 2 (明显异方差，随 $|x|$ 增加)：$n = 200$，$\\beta_0 = 0.5$，$\\beta_1 = 1.5$，$x_i \\sim \\mathrm{Uniform}[-3,3]$，噪声标准差 $\\sigma(x) = 0.3 + 0.7|x|$。\n3. 情况 3 (样本量适中且方差呈曲线变化的边缘情况)：$n = 60$，$\\beta_0 = 0.0$，$\\beta_1 = 2.0$，$x_i \\sim \\mathrm{Uniform}[-2,2]$，噪声标准差 $\\sigma(x) = 0.2 + 0.4(x+1)^2$。\n\n- 在方差回归步骤中，使用 $\\varepsilon = 10^{-8}$。\n- 在 Breusch–Pagan 辅助回归中，包括一个截距和回归量 $x_i$。对卡方参考分布使用 $k=1$ 的自由度。\n\n每个数据集的必需输出：\n- 一个形式如下的列表\n$$\n[\\text{hetero\\_boolean},\\; p\\_value,\\; \\widehat{\\beta}^{\\mathrm{OLS}}_0,\\; \\widehat{\\beta}^{\\mathrm{OLS}}_1,\\; \\widehat{\\beta}^{\\mathrm{WLS}}_0,\\; \\widehat{\\beta}^{\\mathrm{WLS}}_1,\\; \\mathrm{MSE}_{\\mathrm{OLS}},\\; \\mathrm{MSE}_{\\mathrm{WLS}}].\n$$\n- 布尔值必须像 Python 中那样大写 ($\\mathrm{True}$ 或 $\\mathrm{False}$)。所有浮点数必须四舍五入到恰好 $6$ 位小数。\n\n最终程序要求：\n- 您的程序必须使用上述测试套件在内部构建所有数据集，为每个数据集执行任务 A–C，并生成一行输出，其中包含一个有三个元素（每个测试用例一个）的列表，每个元素都是上面描述的列表。格式必须为单行：“[$\\dots$]”，条目以逗号分隔，无额外文本。例如：“[[True,0.012345,1.234000, ...],[...],[...]]”。\n- 不允许用户输入、外部文件和网络访问。唯一允许的库是 Numerical Python (NumPy) 和 Scientific Python (SciPy)。",
            "solution": "用户提供的问题被评估为**有效**。这是一个在计算统计学领域中定义明确、有科学依据且客观的问题。它概述了在简单线性回归背景下识别和校正异方差性的标准流程。所有必要的参数、模型和评估标准都得到了明确的规定。\n\n解决方案将通过为三个合成数据集系统地实现指定的三个任务来开发。该方法论依赖于线性回归、统计检验和加权最小二乘估计的基本原理。\n\n### 基于原理的方法\n\n问题的核心是处理异方差性，这是一种误差项 $\\epsilon_i$ 的方差在不同观测值之间不恒定的情况。标准的普通最小二乘 (OLS) 估计量在异方差性下虽然仍然是无偏的，但不再是最佳线性无偏估计量 (BLUE)。所提供的任务概述了一个诊断和缓解此问题的常用工作流程。\n\n#### 1. 数据生成和初始 OLS 拟合\n\n对于每个测试用例，我们首先根据指定的模型生成一个合成数据集：\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i=1,\\dots,n\n$$\n自变量 $x_i$ 从均匀分布中抽取，误差项 $\\epsilon_i$ 从正态分布 $\\mathcal{N}(0, \\sigma^2(x_i))$ 中抽取，其中标准差 $\\sigma(x_i)$ 对每种情况都是特定的。\n\n我们首先使用普通最小二乘法 (OLS) 拟合此模型。OLS 估计量 $(\\widehat{\\beta}^{\\mathrm{OLS}}_0, \\widehat{\\beta}^{\\mathrm{OLS}}_1)$ 是通过最小化残差平方和找到的。在矩阵形式下，设设计矩阵为 $\\mathbf{X}$ (其中第一列全为 1，第二列为 $x_i$ 值的向量)，响应向量为 $\\mathbf{y}$，则 OLS 系数向量 $\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}$ 由以下公式给出：\n$$\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\n根据此拟合，我们计算 OLS 残差 $r_i = y_i - (\\widehat{\\beta}^{\\mathrm{OLS}}_0 + \\widehat{\\beta}^{\\mathrm{OLS}}_1 x_i)$。这些残差对于后续的诊断和建模步骤至关重要。\n\n#### 2. 任务 A：异方差性分类 (Breusch-Pagan 检验)\n\n第一个任务是正式检验异方差性的存在。Breusch-Pagan 检验基于这样一个思想：如果存在异方差性，则残差平方 $r_i^2$ (作为真实误差方差 $\\sigma_i^2$ 的代理) 应与自变量系统相关。\n\n我们执行一个辅助 OLS 回归：\n$$\nr_i^2 = \\delta_0 + \\delta_1 x_i + u_i\n$$\n检验统计量是拉格朗日乘子 (LM) 统计量，使用此辅助回归的决定系数 $R^2$ 计算得出：\n$$\n\\mathrm{LM} = n R^2\n$$\n在同方差性的零假设下，该统计量服从自由度为 $k$ 的卡方分布，其中 $k$ 是辅助回归中自变量的数量（不包括常数项）。在本问题中，$k=1$。\n\n然后我们计算 $p$ 值：\n$$\np\\text{-value} = \\Pr\\left(\\chi^2_1 \\ge \\mathrm{LM}\\right)\n$$\n一个小的 $p$ 值（根据问题要求，具体为 $p  0.05$）提供了反对零假设的证据，从而使我们将数据集分类为异方差。\n\n#### 3. 任务 B：参数化方差函数回归\n\n如果怀疑存在异方差性（或者即使没有，为了本练习的目的），我们继续对方法函数 $\\sigma^2(x_i)$ 进行建模。该问题指定了一个灵活且稳健的对数线性模型，以确保估计的方差始终为正。我们将残差平方的对数对 $x_i$ 的一个函数进行回归：\n$$\n\\log\\left(r_i^2 + \\varepsilon\\right) = \\gamma_0 + \\gamma_1 \\,\\log\\!\\left(1 + x_i^2\\right) + \\eta_i\n$$\n这里，$\\varepsilon = 10^{-8}$ 是一个小的正常数，以防止在残差恰好为零的情况下取零的对数。通过 OLS 估计参数 $\\widehat{\\gamma}_0$ 和 $\\widehat{\\gamma}_1$ 后，我们可以为每个观测值构建方差的估计值：\n$$\n\\widehat{\\sigma}_i^2 = \\exp\\!\\left(\\widehat{\\gamma}_0 + \\widehat{\\gamma}_1 \\,\\log\\!\\left(1 + x_i^2\\right)\\right)\n$$\n\n#### 4. 任务 C：加权最小二乘 (WLS) 重新拟合\n\n有了估计的方差 $\\widehat{\\sigma}_i^2$，我们可以通过使用加权最小二乘法 (WLS) 来改进初始的 OLS 拟合。WLS 通过为每个观测值分配一个与其误差方差成反比的权重来解释异方差性。权重定义为 $w_i = 1/\\widehat{\\sigma}_i^2$。方差较小（因此信息量更大）的观测值会获得更高的权重。\n\nWLS 估计量是最小化加权残差平方和的向量 $\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{WLS}}$：\n$$\n\\sum_{i=1}^{n} w_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n$$\n在矩阵形式中，$\\mathbf{W}$ 是权重 $w_i$ 的对角矩阵，WLS 解为：\n$$\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\n在假定的异方差性形式下，该估计量是有效的。\n\n最后，我们计算 OLS 和 WLS 拟合的均方误差 (MSE)，以比较它们在原始尺度上的性能：\n$$\n\\mathrm{MSE}_{\\mathrm{OLS}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 \\quad \\text{和} \\quad \\mathrm{MSE}_{\\mathrm{WLS}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\widehat{\\beta}^{\\mathrm{WLS}}_0 - \\widehat{\\beta}^{\\mathrm{WLS}}_1 x_i\\right)^2\n$$\n\n这就完成了整个流程。实现将为每个测试用例精确地遵循这些步骤，收集所需的八个输出值：布尔分类、p 值、两个 OLS 系数、两个 WLS 系数以及两个 MSE 值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing OLS, Breusch-Pagan test, variance modeling,\n    and WLS for three synthetic datasets.\n    \"\"\"\n    \n    # Define test cases as per the problem description.\n    test_cases = [\n        # Case 1: Homoscedastic\n        {'n': 200, 'beta0': 1.25, 'beta1': -0.75, 'x_range': [-2, 2], 'sigma_func': lambda x: 0.5},\n        # Case 2: Clearly heteroscedastic\n        {'n': 200, 'beta0': 0.5, 'beta1': 1.5, 'x_range': [-3, 3], 'sigma_func': lambda x: 0.3 + 0.7 * np.abs(x)},\n        # Case 3: Edge case, curved variance\n        {'n': 60, 'beta0': 0.0, 'beta1': 2.0, 'x_range': [-2, 2], 'sigma_func': lambda x: 0.2 + 0.4 * (x + 1)**2}\n    ]\n\n    # Global constants and random number generator\n    RANDOM_SEED = 1337\n    EPSILON_LOG = 1e-8\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    all_results = []\n\n    for case in test_cases:\n        # --- Data Generation ---\n        n = case['n']\n        beta0_true, beta1_true = case['beta0'], case['beta1']\n        x = rng.uniform(case['x_range'][0], case['x_range'][1], size=n)\n        sigma = case['sigma_func'](x)\n        epsilon = rng.normal(0, sigma, size=n)\n        y = beta0_true + beta1_true * x + epsilon\n\n        # --- Task A: OLS and Breusch-Pagan Test ---\n        \n        # 1. Initial OLS fit\n        X_ols = np.c_[np.ones(n), x]\n        try:\n            beta_ols = np.linalg.inv(X_ols.T @ X_ols) @ X_ols.T @ y\n        except np.linalg.LinAlgError:\n            # Fallback to lstsq if matrix is singular, though unlikely for this problem.\n            beta_ols, _, _, _ = np.linalg.lstsq(X_ols, y, rcond=None)\n\n        res_ols = y - X_ols @ beta_ols\n        squared_res = res_ols**2\n        \n        # 2. Auxiliary regression for Breusch-Pagan test: r_i^2 on x_i\n        X_aux = np.c_[np.ones(n), x]\n        try:\n            delta = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ squared_res\n        except np.linalg.LinAlgError:\n            delta, _, _, _ = np.linalg.lstsq(X_aux, squared_res, rcond=None)\n            \n        y_pred_aux = X_aux @ delta\n        ss_res_aux = np.sum((squared_res - y_pred_aux)**2)\n        ss_tot_aux = np.sum((squared_res - np.mean(squared_res))**2)\n        \n        # Guard against ss_tot_aux being zero\n        r_squared_aux = 1 - (ss_res_aux / ss_tot_aux) if ss_tot_aux > 0 else 0.0\n\n        # 3. LM statistic and p-value\n        lm_stat = n * r_squared_aux\n        p_value = chi2.sf(lm_stat, df=1)\n        is_heteroscedastic = p_value  0.05\n        \n        # --- Task B: Variance Function Regression ---\n        log_squared_res = np.log(squared_res + EPSILON_LOG)\n        log_x_feature = np.log(1 + x**2)\n        \n        X_var = np.c_[np.ones(n), log_x_feature]\n        try:\n            gamma = np.linalg.inv(X_var.T @ X_var) @ X_var.T @ log_squared_res\n        except np.linalg.LinAlgError:\n            gamma, _, _, _ = np.linalg.lstsq(X_var, log_squared_res, rcond=None)\n\n        sigma2_hat = np.exp(X_var @ gamma)\n        \n        # --- Task C: Weighted Least Squares Refit ---\n        weights = 1.0 / sigma2_hat\n        W = np.diag(weights)\n        \n        try:\n            # Direct WLS formula\n            X_wls = X_ols\n            beta_wls = np.linalg.inv(X_wls.T @ W @ X_wls) @ X_wls.T @ W @ y\n        except np.linalg.LinAlgError:\n            # Fallback using transformed OLS, which is more stable\n            sqrt_w = np.sqrt(weights)\n            y_prime = y * sqrt_w\n            X_prime = X_wls * sqrt_w[:, np.newaxis]\n            beta_wls, _, _, _ = np.linalg.lstsq(X_prime, y_prime, rcond=None)\n\n        # --- MSE Calculation ---\n        mse_ols = np.mean(res_ols**2)\n        res_wls = y - X_wls @ beta_wls\n        mse_wls = np.mean(res_wls**2)\n        \n        # --- Collect and Format Results ---\n        result_list = [\n            is_heteroscedastic,\n            p_value,\n            beta_ols[0],\n            beta_ols[1],\n            beta_wls[0],\n            beta_wls[1],\n            mse_ols,\n            mse_wls,\n        ]\n        all_results.append(result_list)\n\n    # Final print statement in the exact required format.\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        formatted_res = (\n            f\"[{res[0]},\"\n            f\"{res[1]:.6f},\"\n            f\"{res[2]:.6f},\"\n            f\"{res[3]:.6f},\"\n            f\"{res[4]:.6f},\"\n            f\"{res[5]:.6f},\"\n            f\"{res[6]:.6f},\"\n            f\"{res[7]:.6f}]\"\n        )\n        output_str += formatted_res\n        if i  len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "每个模型都有其适用范围，在训练数据范围之外使用模型（即“外推”）存在巨大风险。本练习将介绍一个形式化的框架，通过检验模型的单调性和是否满足物理边界来分类判断一次外推是否“安全”。此外，您还将通过回归分析来量化预测不确定性是如何随着与训练数据距离的增加而增大的，从而对模型外推的风险建立直观且深刻的认识 。",
            "id": "3107029",
            "problem": "给定一个已学习的单变量回归模型，表示为多项式函数 $\\hat{f}(x) = \\sum_{k=0}^{K} c_k x^k$ 和一个训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$。您必须编写一个完整的、可运行的程序，该程序针对几个指定的测试用例执行两项任务：(1) 根据逻辑单调性和物理边界准则，分类将 $\\hat{f}(x)$ 外推到训练域外的指定区间是否安全；以及 (2) 通过线性回归估计对称高斯置信区间宽度随超出训练域的距离而变宽的速率。程序必须为每个测试用例输出一个结果，并将所有结果汇总到单行中，格式需严格符合下文规定。\n\n用作起点的基础定义：\n- 如果对于区间内的每一对 $x_1 \\le x_2$，都有 $\\hat{f}(x_1) \\le \\hat{f}(x_2)$，则称函数在该区间上是单调非递减的。如果对于所有此类配对，都有 $\\hat{f}(x_1) \\ge \\hat{f}(x_2)$，则称其为单调非递增的。常数函数既是单调非递减的，也是单调非递增的。\n- 响应的物理边界由一个闭区间 $[y_{\\min}, y_{\\max}]$ 给出，模型输出不得超过该区间。\n- 对于名义覆盖率为 $q \\in (0,1)$、高斯噪声标准差为 $\\sigma > 0$ 的双侧高斯置信区间，对称区间的宽度 $w$ 与标准正态分布 (SND) 的分位数相关，这是一个经过充分检验的事实：水平为 $q$ 的对称双侧区间使用由 $z_q = \\Phi^{-1}\\!\\left(1 - \\frac{1-q}{2}\\right)$ 定义的SND分位数 $z_q$，其中 $\\Phi$ 是SND的累积分布函数。来自已知标准差 $\\sigma$ 的相应宽度贡献为 $2 z_q \\sigma$。\n- 对于带截距的响应 $W$ 对标量预测变量 $D$ 的线性回归，在通常假设下且 $\\mathrm{Var}(D) > 0$ 时，普通最小二乘斜率等于 $\\mathrm{Cov}(D,W)/\\mathrm{Var}(D)$。\n\n您的程序必须为每个测试用例实现以下逻辑：\n\n1) 基于单调性和边界的安全分类规则：\n- 通过检查函数在训练区间 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$上是单调非递减、单调非递增还是（在数值容差内）恒定，来确定其单调性类型。为此检查使用一个包含 $N = 1000$ 个点的均匀网格，容差为 $\\varepsilon = 10^{-8}$。您可以通过检查整个网格上离散导数的符号或任何符合上述定义的等效数值标准来评估单调性。将训练单调性类型声明为以下之一：递增 (increasing)、递减 (decreasing) 或平坦 (flat)。如果在容差范围内没有适用的单调性类型，则分类必须为不安全。\n- 使用相同的 $N$ 和 $\\varepsilon$ 检查 $\\hat{f}$ 在外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 上是否与在 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ 上具有相同的单调性类型。如果训练类型是平坦的，那么外推也必须在容差内是平坦的。\n- 确认在 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 上的一个包含 $N$ 个点的均匀网格（包括端点）的每个点上，$\\hat{f}(x) \\in [y_{\\min}, y_{\\max}]$ 都在容差 $\\varepsilon$ 内成立。\n- 当且仅当单调性条件和边界条件都满足时，外推才被分类为安全。\n\n2) 置信区间扩展速率的回归：\n- 对于给定的覆盖水平 $q$ 和噪声标准差 $\\sigma$，将在训练域外一点 $x$ 处的合成对称置信区间宽度定义为\n$$\nw(x) = 2 z_q \\sigma \\left(1 + \\lambda \\, d(x)\\right),\n$$\n其中 $d(x)$ 是 $x$ 到训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ 最近点的非负距离，即\n$$\nd(x) = \\begin{cases}\nx_{\\mathrm{L}} - x,  \\text{如果 } x  x_{\\mathrm{L}},\\\\\n0,  \\text{如果 } x \\in [x_{\\mathrm{L}}, x_{\\mathrm{R}}],\\\\\nx - x_{\\mathrm{R}},  \\text{如果 } x > x_{\\mathrm{R}}.\n\\end{cases}\n$$\n参数 $\\lambda \\ge 0$ 控制宽度每单位距离增加的速度。对于每个测试用例，在外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 中构建 $M=5$ 个均匀间隔的点，计算这些点上的 $d(x)$ 和 $w(x)$，并拟合普通最小二乘线性模型 $w = \\alpha + r \\, d$。将估计的斜率 $r$ 报告为扩展速率。将报告的 $r$ 四舍五入到六位小数。\n\n测试套件：\n为以下四个测试用例提供结果。在每个案例中，多项式系数从最低次幂到最高次幂列出，因此 $c_0$ 是常数项。\n\n- 案例 1：\n  - $c = \\{1, 2\\}$，所以 $\\hat{f}(x) = 1 + 2 x$。\n  - 训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-1, 1]$。\n  - 外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [1, 2]$。\n  - 物理边界 $[y_{\\min}, y_{\\max}] = [0, 5]$。\n  - 噪声标准差 $\\sigma = 0.5$。\n  - 覆盖水平 $q = 0.95$。\n  - 扩展参数 $\\lambda = 0.1$。\n\n- 案例 2：\n  - $c = \\{1, 4\\}$，所以 $\\hat{f}(x) = 1 + 4 x$。\n  - 训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-1, 1]$。\n  - 外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [1, 3]$。\n  - 物理边界 $[y_{\\min}, y_{\\max}] = [0, 10]$。\n  - 噪声标准差 $\\sigma = 0.3$。\n  - 覆盖水平 $q = 0.90$。\n  - 扩展参数 $\\lambda = 0.2$。\n\n- 案例 3：\n  - $c = \\{0, -1, 0, 1\\}$，所以 $\\hat{f}(x) = x^3 - x$。\n  - 训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-0.5, 0.5]$。\n  - 外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [0.5, 1.0]$。\n  - 物理边界 $[y_{\\min}, y_{\\max}] = [-10, 10]$。\n  - 噪声标准差 $\\sigma = 0.4$。\n  - 覆盖水平 $q = 0.95$。\n  - 扩展参数 $\\lambda = 0.15$。\n\n- 案例 4：\n  - $c = \\{3\\}$，所以 $\\hat{f}(x) = 3$。\n  - 训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-2, 2]$。\n  - 外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [2, 3]$。\n  - 物理边界 $[y_{\\min}, y_{\\max}] = [0, 5]$。\n  - 噪声标准差 $\\sigma = 0.2$。\n  - 覆盖水平 $q = 0.99$。\n  - 扩展参数 $\\lambda = 0.0$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有测试用例的结果，形式为一个逗号分隔的列表，并用方括号括起来。每个测试用例的结果必须是一个形式为 $[s, r]$ 的双元素列表，其中 $s$ 是一个布尔值（True 表示安全，False 表示不安全），$r$ 是四舍五入到六位小数的斜率。例如，包含两个假设结果的输出应如下所示：“[[True,0.123456],[False,0.000000]]”。您的程序必须且仅能打印这样一行，不得包含任何其他内容。",
            "solution": "用户提供了一个计算问题，要求对给定的多项式回归模型 $\\hat{f}(x)$ 实现一个包含两部分的分析。第一项任务是基于逻辑准则对模型在其训练域之外进行外推的安全性进行分类。第二项任务是估计一个合成的置信区间宽度作为其与训练域距离的函数的扩展速率。该问题定义明确，科学上基于标准的数值和统计原理，并为获得唯一的、可验证的解提供了所有必要的数据和定义。\n\n解决方案通过为每个测试用例实现指定的逻辑来推进。\n\n**第 1 部分：安全分类**\n\n外推的安全性取决于两个条件：保持单调性和遵守物理边界。\n\n1.  **单调性分析**：如果对于该区间内的任何 $x_1 \\le x_2$，都有 $\\hat{f}(x_1) \\le \\hat{f}(x_2)$，则函数在该区间上被定义为单调非递减。如果 $\\hat{f}(x_1) \\ge \\hat{f}(x_2)$，则为单调非递增。常数函数同时满足这两个条件。为了在数值上评估这一点，我们在给定区间内一个精细的、包含 $N=1000$ 个点的均匀网格上评估函数 $\\hat{f}(x)$。设这些评估点为 $y_i = \\hat{f}(x_i)$。然后我们检查一阶差分序列 $\\Delta y_i = y_{i+1} - y_i$。\n    -   对于一个小的容差 $\\varepsilon=10^{-8}$，如果所有的 $\\Delta y_i \\ge -\\varepsilon$，则该函数被认为是数值非递减的。\n    -   如果所有的 $\\Delta y_i \\le \\varepsilon$，则为数值非递增的。\n    -   如果两个条件都成立，则函数被分类为 `flat`（平坦）。如果只有第一个条件成立，则为 `increasing`（递增）。如果只有第二个条件成立，则为 `decreasing`（递减）。如果两者都不成立，则为非单调的，或 `none`。\n\n    安全规则要求在训练区间 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ 上观察到的单调性类型必须在外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 上得以保持。如果函数在训练区间上不是单调的，则外推立即被视为不安全。\n\n2.  **物理边界检查**：对于外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 中的所有 $x$，模型的输出 $\\hat{f}(x)$ 必须保持在指定的物理边界 $[y_{\\min}, y_{\\max}]$ 之内。在数值上，通过检查在 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 上的评估网格上的所有点 $y_i$ 是否满足条件 $y_{\\min} - \\varepsilon \\le y_i \\le y_{\\max} + \\varepsilon$ 来验证这一点。\n\n当且仅当单调性得以保持且物理边界得到遵守时，外推被分类为 `safe` (True)。否则，它就是 `unsafe` (False)。\n\n**第 2 部分：置信区间扩展速率**\n\n这部分涉及对一个合成生成的数据集进行回归分析。\n\n1.  **合成数据生成**：问题定义了一个模型，该模型描述了在某点 $x$ 处的对称置信区间宽度作为其与训练域距离的函数：\n    $$\n    w(x) = 2 z_q \\sigma \\left(1 + \\lambda \\, d(x)\\right)\n    $$\n    这里，$\\sigma$ 是噪声标准差，$\\lambda$ 是一个扩展参数，$d(x)$ 是从 $x$ 到区间 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ 的距离。术语 $z_q$ 是对应于双侧置信水平 $q$ 的标准正态分布 (SND) 的分位数，由 $z_q = \\Phi^{-1}((1+q)/2)$ 给出，其中 $\\Phi$ 是 SND 的累积分布函数。为了进行分析，我们在外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 中生成 $M=5$ 个均匀间隔的点 $\\{x_i\\}$，并计算相应的配对 $(d_i, w_i)$，其中 $d_i = d(x_i)$ 和 $w_i = w(x_i)$。\n\n2.  **线性回归**：我们使用普通最小二乘法 (OLS) 对生成的 $(d_i, w_i)$ 对拟合一个形式为 $w = \\alpha + r d$ 的简单线性回归模型。该回归的斜率 $r$ 代表了置信区间宽度随与训练域距离每单位增加而扩展的估计速率。斜率 $r$ 的 OLS 估计由以下公式给出：\n    $$\n    r = \\frac{\\mathrm{Cov}(d, w)}{\\mathrm{Var}(d)} = \\frac{\\sum_{i=1}^{M} (d_i - \\bar{d})(w_i - \\bar{w})}{\\sum_{i=1}^{M} (d_i - \\bar{d})^2}\n    $$\n    其中 $\\bar{d}$ 和 $\\bar{w}$ 分别是 $d_i$ 和 $w_i$ 值的样本均值。为每个测试用例计算该值。请注意，由于合成数据 $w_i$ 是 $d_i$ 的精确线性函数，即 $w_i = (2z_q \\sigma) + (2z_q \\sigma \\lambda) d_i$，因此只要 $\\mathrm{Var}(d) > 0$，OLS回归将精确地恢复理论斜率 $r = 2z_q \\sigma \\lambda$。\n\n最终程序遍历指定的测试用例，应用上述逻辑，并将结果汇总成所需的输出格式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It encapsulates all logic and helper functions for self-contained execution.\n    \"\"\"\n\n    def check_monotonicity(c_coeffs, interval, N, eps):\n        \"\"\"\n        Numerically determines the monotonicity of a polynomial on an interval.\n        Returns 'increasing', 'decreasing', 'flat', or 'none'.\n        \"\"\"\n        # np.polyval expects coefficients for highest power first.\n        c_rev = c_coeffs[::-1]\n        \n        x_grid = np.linspace(interval[0], interval[1], N)\n        y_vals = np.polyval(c_rev, x_grid)\n        \n        diffs = np.diff(y_vals)\n        \n        is_non_decreasing = np.all(diffs >= -eps)\n        is_non_increasing = np.all(diffs = eps)\n        \n        if is_non_decreasing and is_non_increasing:\n            return 'flat'\n        elif is_non_decreasing:\n            return 'increasing'\n        elif is_non_increasing:\n            return 'decreasing'\n        else:\n            return 'none'\n\n    def check_bounds(c_coeffs, interval, N, y_min, y_max, eps):\n        \"\"\"\n        Checks if the polynomial's output stays within physical bounds on an interval.\n        \"\"\"\n        c_rev = c_coeffs[::-1]\n        \n        x_grid = np.linspace(interval[0], interval[1], N)\n        y_vals = np.polyval(c_rev, x_grid)\n        \n        # Check if all y_vals are within [y_min, y_max] with tolerance\n        return np.all((y_vals >= y_min - eps) and (y_vals = y_max + eps))\n\n    def calculate_distance(x_points, x_L, x_R):\n        \"\"\"\n        Vectorized calculation of the distance from points 'x' to the interval [x_L, x_R].\n        \"\"\"\n        return np.maximum(0, x_points - x_R) + np.maximum(0, x_L - x_points)\n\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: c, [x_L, x_R], [x_A, x_B], [y_min, y_max], sigma, q, lambda\n        (np.array([1, 2]), [-1, 1], [1, 2], [0, 5], 0.5, 0.95, 0.1),\n        # Case 2\n        (np.array([1, 4]), [-1, 1], [1, 3], [0, 10], 0.3, 0.90, 0.2),\n        # Case 3\n        (np.array([0, -1, 0, 1]), [-0.5, 0.5], [0.5, 1.0], [-10, 10], 0.4, 0.95, 0.15),\n        # Case 4\n        (np.array([3]), [-2, 2], [2, 3], [0, 5], 0.2, 0.99, 0.0),\n    ]\n\n    results = []\n    \n    # Constants for numerical checks\n    N = 1000\n    M = 5\n    eps = 1e-8\n\n    for case in test_cases:\n        c, train_domain, extrap_interval, phys_bounds, sigma, q, lam = case\n        x_L, x_R = train_domain\n        x_A, x_B = extrap_interval\n        y_min, y_max = phys_bounds\n\n        # --- Part 1: Safety Classification ---\n        train_mono_type = check_monotonicity(c, train_domain, N, eps)\n        \n        is_safe = False\n        if train_mono_type != 'none':\n            extrap_mono_type = check_monotonicity(c, extrap_interval, N, eps)\n            mono_ok = (train_mono_type == extrap_mono_type)\n            \n            if mono_ok:\n                bounds_ok = check_bounds(c, extrap_interval, N, y_min, y_max, eps)\n                if bounds_ok:\n                    is_safe = True\n\n        # --- Part 2: Widening Rate Regression ---\n        z_q = norm.ppf(1 - (1 - q) / 2.0)\n        \n        x_points_regr = np.linspace(x_A, x_B, M)\n        d_vals = calculate_distance(x_points_regr, x_L, x_R)\n        w_vals = 2 * z_q * sigma * (1 + lam * d_vals)\n        \n        var_d = np.var(d_vals, ddof=1)\n        \n        r = 0.0\n        # OLS slope r = Cov(d, w) / Var(d). Check Var(d) > 0.\n        if var_d > 1e-12:\n            cov_dw = np.cov(d_vals, w_vals, ddof=1)[0, 1]\n            r = cov_dw / var_d\n        \n        results.append([is_safe, r])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{s},{r:.6f}]\" for s, r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}