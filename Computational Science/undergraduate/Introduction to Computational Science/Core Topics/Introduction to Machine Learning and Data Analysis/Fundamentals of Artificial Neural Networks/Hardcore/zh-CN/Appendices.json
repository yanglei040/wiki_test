{
    "hands_on_practices": [
        {
            "introduction": "梯度下降是驱动神经网络学习的核心引擎，而损失函数的梯度则为参数更新提供了方向和动力。这项实践将带领你深入剖析最常用的分类损失函数——Softmax交叉熵——的梯度计算 ，并亲手实现和验证梯度与模型置信度之间的动态关系。通过这个练习，你将直观地理解为何一个自信但错误的模型会收到强烈的修正信号，以及标签平滑等正则化技术是如何防止模型变得“过度自信”的。",
            "id": "3134219",
            "problem": "您将分析在人工神经网络 (ANN) 背景下的 Softmax 交叉熵 (SCE) 损失的梯度。您的任务包括：从第一性原理出发，推导梯度如何依赖于 logits；将梯度大小与 logit 差异 (裕量) 相关联；并研究标签平滑对梯度饱和的影响。然后，您必须实现一个程序，为提供的测试套件计算定量的梯度指标。\n\n基础知识 (仅定义)：\n- 给定一个 logits 向量 $\\mathbf{z} \\in \\mathbb{R}^K$，softmax 函数会生成类别概率 $\\mathbf{p} \\in \\mathbb{R}^K$，其中 $p_i \\ge 0$ 且 $\\sum_{i=1}^K p_i = 1$。\n- 对于一个目标分布 $\\mathbf{y} \\in \\mathbb{R}^K$，其中 $y_i \\ge 0$ 且 $\\sum_{i=1}^K y_i = 1$，交叉熵损失是 $\\mathbf{p}$ 的一个标量函数。\n- 标签平滑通过为正确类别索引 $c$ 分配 $y_c = 1 - \\varepsilon$ 并为所有 $j \\ne c$ 分配 $y_j = \\varepsilon/(K-1)$ 来修改 one-hot 目标，其中 $\\varepsilon \\in [0,1)$。\n\n要求的推导与分析：\n1) 严格从以上定义和基础微积分（商法则、链式法则以及指数和对数的性质）出发，推导 Softmax 交叉熵损失关于 logits $\\mathbf{z}$ 的梯度。给出针对任意目标分布 $\\mathbf{y}$ 的通用结果，然后将其特化为 one-hot 目标和标签平滑目标。不要假设任何已知的梯度公式；必须进行推导。\n2) 使用您的梯度表达式以及 softmax 概率仅依赖于 logits 差异（平移不变性）这一事实，分析梯度大小与 logit 差异的关系。形式化定义正确类别 logit 裕量 $\\Delta = z_c - \\max_{j \\ne c} z_j$，并使用您的公式解释为什么对于 one-hot 目标，较大的正 $\\Delta$ 会导致小梯度（饱和），为什么较大的负 $\\Delta$ 会导致大梯度，以及当 $\\varepsilon  0$ 时，标签平滑如何改变这些梯度大小，即使模型非常确信时也是如此。\n3) 实现一个程序，为每个测试用例计算以下量：\n   - 使用数值稳定方法计算的 softmax 概率 $\\mathbf{p}$。\n   - 在标签平滑参数 $\\varepsilon$ 下，关于 logits 的梯度向量 $\\nabla_{\\mathbf{z}} L$。\n   - 正确类别裕量 $\\Delta = z_c - \\max_{j \\ne c} z_j$。\n   - 欧几里得范数（也称为 $\\ell_2$ 范数）$\\lVert \\nabla_{\\mathbf{z}} L \\rVert_2$。\n   - 正确类别上的绝对梯度 $\\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert$。\n   - 所有类别中的最大绝对梯度 $\\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert$。\n\n实现要求：\n- 使用自然对数。\n- 使用数值稳定的 softmax 实现（您可以在进行指数运算前，从所有 logits 中减去 $\\max_i z_i$）。\n- 对于每个测试用例，返回一个包含四个浮点数的列表 $[\\Delta, \\lVert \\nabla_{\\mathbf{z}} L \\rVert_2, \\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert, \\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert]$，每个浮点数四舍五入到 6 位小数。\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个逗号分隔的列表的列表，不含空格，并用方括号括起来。例如，输出格式必须类似于 $[[a\\_1,b\\_1,c\\_1,d\\_1],[a\\_2,b\\_2,c\\_2,d\\_2],\\dots]$，其中每个 $a_i,b_i,c_i,d_i$ 是小数点后恰好有 6 位数字的小数。\n\n测试套件（每个测试用例是一个三元组 $(\\mathbf{z}, c, \\varepsilon)$）：\n- 用例 1：$\\mathbf{z} = [0.1, 0.2, 0.15, 0.05]$，$c = 1$，$\\varepsilon = 0.0$。\n- 用例 2：$\\mathbf{z} = [8.0, 1.0, 0.0, -2.0]$，$c = 0$，$\\varepsilon = 0.0$。\n- 用例 3：$\\mathbf{z} = [-2.0, 5.0, -1.0, 0.0]$，$c = 0$，$\\varepsilon = 0.0$。\n- 用例 4：$\\mathbf{z} = [1000.0, 1000.0, 1000.0, 1000.0]$，$c = 2$，$\\varepsilon = 0.0$。\n- 用例 5：$\\mathbf{z} = [8.0, 1.0, 0.0, -2.0]$，$c = 0$，$\\varepsilon = 0.1$。\n- 用例 6：$\\mathbf{z} = [-2.0, 5.0, -1.0, 0.0]$，$c = 0$，$\\varepsilon = 0.1$。\n\n最终输出规范：\n- 程序必须精确打印一行：一个包含 6 个内部列表的顶层列表，每个内部列表对应一个测试用例，并按上述顺序列出。\n- 每个内部列表必须是 $[\\Delta, \\lVert \\nabla_{\\mathbf{z}} L \\rVert_2, \\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert, \\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert]$，其中每个浮点数四舍五入到恰好 6 位小数。\n- 输出行中任何位置都不能有空格。",
            "solution": "该问题是有效的，因为它具有科学依据、定义明确且客观。它包含一个标准的、非平凡的推导，一个植根于该推导的概念性分析，以及一个具体的实现任务，所有这些都是人工神经网络基础知识的核心。\n\n### 1. Softmax 交叉熵损失的梯度推导\n\n我们的任务是推导 Softmax 交叉熵 (SCE) 损失关于输入 logits $\\mathbf{z}$ 的梯度。\n\n**定义：**\n- Logits: $\\mathbf{z} = [z_1, z_2, \\dots, z_K]^T \\in \\mathbb{R}^K$。\n- 类别 $i$ 的 Softmax 概率：$p_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$。\n- 目标概率分布：$\\mathbf{y} = [y_1, y_2, \\dots, y_K]^T$，其中 $y_i \\ge 0$ 且 $\\sum_{i=1}^K y_i = 1$。\n- 交叉熵损失：$L(\\mathbf{p}, \\mathbf{y}) = -\\sum_{i=1}^K y_i \\log p_i$。注意，此处的对数是自然对数 $\\ln$。\n\n我们的目标是计算梯度向量 $\\nabla_{\\mathbf{z}} L$，其分量为 $\\frac{\\partial L}{\\partial z_k}$，其中 $k \\in \\{1, \\dots, K\\}$。我们应用链式法则：\n$$\n\\frac{\\partial L}{\\partial z_k} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k}\n$$\n\n**步骤 1.1：计算 $\\frac{\\partial L}{\\partial p_i}$**\n损失为 $L = -\\sum_{j=1}^K y_j \\log p_j$。关于特定概率 $p_i$ 的偏导数很简单：\n$$\n\\frac{\\partial L}{\\partial p_i} = \\frac{\\partial}{\\partial p_i} \\left( -y_i \\log p_i - \\sum_{j \\ne i} y_j \\log p_j \\right) = -\\frac{y_i}{p_i}\n$$\n\n**步骤 1.2：计算 $\\frac{\\partial p_i}{\\partial z_k}$（Softmax 函数的雅可比矩阵）**\n我们必须考虑 $p_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$ 关于 $z_k$ 的导数的两种情况。令 $D = \\sum_{j=1}^K e^{z_j}$。\n\n情况 A：$i = k$。我们使用商法则 $\\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2}$。\n$$\n\\frac{\\partial p_k}{\\partial z_k} = \\frac{\\frac{\\partial(e^{z_k})}{\\partial z_k} D - e^{z_k} \\frac{\\partial D}{\\partial z_k}}{D^2} = \\frac{e^{z_k} D - e^{z_k} e^{z_k}}{D^2} = \\frac{e^{z_k}}{D} \\frac{D - e^{z_k}}{D} = p_k (1 - p_k)\n$$\n\n情况 B：$i \\ne k$。\n$$\n\\frac{\\partial p_i}{\\partial z_k} = \\frac{\\frac{\\partial(e^{z_i})}{\\partial z_k} D - e^{z_i} \\frac{\\partial D}{\\partial z_k}}{D^2} = \\frac{0 \\cdot D - e^{z_i} e^{z_k}}{D^2} = -\\frac{e^{z_i}}{D} \\frac{e^{z_k}}{D} = -p_i p_k\n$$\n\n**步骤 1.3：组合导数**\n现在我们将这些结果代回到 $\\frac{\\partial L}{\\partial z_k}$ 的链式法则表达式中：\n$$\n\\frac{\\partial L}{\\partial z_k} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k} = \\left(\\frac{\\partial L}{\\partial p_k} \\frac{\\partial p_k}{\\partial z_k}\\right) + \\sum_{i \\ne k} \\left(\\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k}\\right)\n$$\n$$\n\\frac{\\partial L}{\\partial z_k} = \\left(-\\frac{y_k}{p_k}\\right) (p_k(1 - p_k)) + \\sum_{i \\ne k} \\left(-\\frac{y_i}{p_i}\\right) (-p_i p_k)\n$$\n$$\n\\frac{\\partial L}{\\partial z_k} = -y_k(1 - p_k) + \\sum_{i \\ne k} y_i p_k = -y_k + y_k p_k + p_k \\sum_{i \\ne k} y_i\n$$\n提取公因式 $p_k$：\n$$\n\\frac{\\partial L}{\\partial z_k} = p_k \\left(y_k + \\sum_{i \\ne k} y_i \\right) - y_k\n$$\n由于 $\\mathbf{y}$ 是一个概率分布，$\\sum_{i=1}^K y_i = y_k + \\sum_{i \\ne k} y_i = 1$。这可将表达式简化为：\n$$\n\\frac{\\partial L}{\\partial z_k} = p_k(1) - y_k = p_k - y_k\n$$\n这个简洁优美的结果表明，Softmax 交叉熵损失关于 logit $z_k$ 的梯度是预测概率 $p_k$ 和目标概率 $y_k$ 之间的差。以向量形式表示，梯度为：\n$$\n\\nabla_{\\mathbf{z}} L = \\mathbf{p} - \\mathbf{y}\n$$\n\n**特化情况：**\n1.  **One-Hot 目标：** 对于一个正确类别索引 $c$，目标向量 $\\mathbf{y}$ 满足 $y_c = 1$ 且对于所有 $j \\ne c$ 有 $y_j = 0$。梯度分量为：\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1$\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - 0 = p_j \\quad (\\text{对于 } j \\ne c)\n\n2.  **标签平滑目标：** 对于平滑参数 $\\varepsilon \\in [0, 1)$，目标向量 $\\mathbf{y}$ 定义为 $y_c = 1 - \\varepsilon$ 且对于所有 $j \\ne c$ 有 $y_j = \\frac{\\varepsilon}{K-1}$。梯度分量为：\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - (1 - \\varepsilon)$\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - \\frac{\\varepsilon}{K-1} \\quad (\\text{对于 } j \\ne c)\n\n### 2. 梯度大小与 Logit 裕量的分析\n\nSoftmax 函数对于所有 logits 的常数平移具有不变性，即对于任意标量 $C$，$\\text{softmax}(\\mathbf{z} + C) = \\text{softmax}(\\mathbf{z})$。这意味着概率 $\\mathbf{p}$ 仅依赖于 logits 之间的*差异*。\n\n我们将正确类别 logit 裕量定义为 $\\Delta = z_c - \\max_{j \\ne c} z_j$。这个裕量衡量了模型对正确类别 $c$ 的“确信”程度比对最可能的错误类别高出多少。\n\n**对 One-Hot 目标的分析 ($\\varepsilon = 0$)：**\n- **较大的正裕量 ($\\Delta \\to \\infty$)：** 如果 $z_c$ 远大于所有其他的 $z_j$，则模型对正确类别非常确信。\n    - $p_c = \\frac{e^{z_c}}{e^{z_c} + \\sum_{j \\ne c} e^{z_j}} = \\frac{1}{1 + \\sum_{j \\ne c} e^{z_j - z_c}} \\to 1$ 因为 $z_j - z_c \\to -\\infty$。\n    - 因此，对于 $j \\ne c$，$p_j \\to 0$。\n    - 梯度分量变为：\n        - $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1 \\to 1 - 1 = 0$。\n        - $(\\nabla_{\\mathbf{z}} L)_j = p_j \\to 0$。\n    - 整个梯度向量 $\\nabla_{\\mathbf{z}} L$ 趋近于零。这种现象被称为**梯度饱和**。随着模型变得更加确信，它接收到的学习信号会不断减弱，从而有效地停止了对该样本的学习。\n\n- **较大的负裕量 ($\\Delta \\to -\\infty$)：** 如果 $z_c$ 远小于最大的错误 logit（比如 $z_m = \\max_{j \\ne c} z_j$），则模型对一个错误的类别非常确信。\n    - $p_c \\to 0$。\n    - 正确类别 logit 的梯度为 $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1 \\to -1$。这是一个强烈的信号，要求增加 $z_c$。\n    - “获胜”的错误类别 $m$ 的概率趋近于 1，即 $p_m \\to 1$。\n    - 这个错误 logit 的梯度为 $(\\nabla_{\\mathbf{z}} L)_m = p_m \\to 1$。这是一个强烈的信号，要求减小 $z_m$。\n    - 梯度幅度很大，为模型提供了强烈的修正信号。\n\n**标签平滑的影响 ($\\varepsilon  0$)：**\n标签平滑改变了目标分布，这反过来又改变了梯度行为，特别是在高置信度的情况下。\n- **较大的正裕量 ($\\Delta \\to \\infty$)：** 如前所述，$p_c \\to 1$ 且对于 $j \\ne c$ 有 $p_j \\to 0$。梯度分量变为：\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - (1 - \\varepsilon) \\to 1 - (1 - \\varepsilon) = \\varepsilon$。\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - \\frac{\\varepsilon}{K-1} \\to 0 - \\frac{\\varepsilon}{K-1} = -\\frac{\\varepsilon}{K-1}$。\n- 与 one-hot 的情况不同，梯度分量不会消失。\n    - 正确 logit 的梯度 $\\varepsilon$ 是一个小的正值。这通过鼓励 $z_c$ 不要无限地大于其他 logits 来惩罚过度自信。$p_c$ 的目标现在是 $1-\\varepsilon$，而不是 1。\n    - 错误 logits 的梯度是小的负值，鼓励它们的 logits $z_j$ 略微增加。\n- 这种持续的、非零的梯度阻止了模型变得过度确信，起到了正则化器的作用，可以改善模型的泛化能力和校准水平。\n\n### 3. 实现细节\n\n该实现为每个测试用例计算所需的指标。\n1.  **稳定的 Softmax：** 为了防止大 logits 导致的数值上溢/下溢，我们利用平移不变性，在进行指数运算前从所有 logits 中减去最大 logit 值：$p_i = \\frac{e^{z_i - \\max(\\mathbf{z})}}{\\sum_j e^{z_j - \\max(\\mathbf{z})}}$。\n2.  **目标向量构建：** 根据正确类别索引 $c$、类别数 $K$ 以及平滑参数 $\\varepsilon$ 创建目标向量 $\\mathbf{y}$。\n3.  **梯度计算：** 梯度 $\\nabla_{\\mathbf{z}} L$ 简单地计算为 $\\mathbf{p} - \\mathbf{y}$。\n4.  **指标计算：**\n    - 通过找到错误类别中的最大 logit 并从正确类别的 logit 中减去它来计算裕量 $\\Delta$。\n    - 梯度向量的欧几里得 ($\\ell_2$) 范数使用 `np.linalg.norm` 计算。\n    - 正确类别索引处的绝对梯度和所有类别中的最大绝对梯度分别使用 `np.abs` 和 `np.max` 找到。\n5.  **输出格式化：** 结果被格式化为一个表示列表的列表的字符串，其中每个浮点数都被格式化为恰好六位小数，并且不含任何空白字符。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and computes gradient metrics for the Softmax Cross-Entropy loss\n    for a given suite of test cases.\n    \"\"\"\n    # Test suite: each case is a tuple (z, c, epsilon)\n    # z: logits vector (list of floats)\n    # c: correct class index (int)\n    # epsilon: label smoothing parameter (float)\n    test_cases = [\n        ([0.1, 0.2, 0.15, 0.05], 1, 0.0),\n        ([8.0, 1.0, 0.0, -2.0], 0, 0.0),\n        ([-2.0, 5.0, -1.0, 0.0], 0, 0.0),\n        ([1000.0, 1000.0, 1000.0, 1000.0], 2, 0.0),\n        ([8.0, 1.0, 0.0, -2.0], 0, 0.1),\n        ([-2.0, 5.0, -1.0, 0.0], 0, 0.1),\n    ]\n\n    all_results = []\n    \n    for z_list, c, epsilon in test_cases:\n        z = np.array(z_list, dtype=np.float64)\n        K = len(z)\n\n        # 1. Compute softmax probabilities (numerically stable)\n        z_max = np.max(z)\n        exp_z = np.exp(z - z_max)\n        p = exp_z / np.sum(exp_z)\n\n        # 2. Compute the gradient vector (grad_L = p - y)\n        if K > 1:\n            y = np.full(K, epsilon / (K - 1))\n        else: # Handle edge case of K=1, though not in tests\n            y = np.array([1.0])\n        y[c] = 1.0 - epsilon\n        \n        grad_L = p - y\n\n        # 3. Compute the correct-class margin Delta\n        if K > 1:\n            mask = np.ones(K, dtype=bool)\n            mask[c] = False\n            max_z_incorrect = np.max(z[mask])\n            delta = z[c] - max_z_incorrect\n        else: # Only one class, so margin is ill-defined, use 0\n            delta = 0.0\n\n        # 4. Compute the required gradient metrics\n        norm_grad_L = np.linalg.norm(grad_L)\n        abs_grad_c = np.abs(grad_L[c])\n        max_abs_grad = np.max(np.abs(grad_L))\n\n        # Store the four required floats\n        case_result = [delta, norm_grad_L, abs_grad_c, max_abs_grad]\n        all_results.append(case_result)\n\n    # Format the final output string exactly as required.\n    # e.g., [[val1,val2,...],[...],...] with no spaces and 6 decimal places.\n    inner_results_str = []\n    for result_vector in all_results:\n        formatted_values = [f'{v:.6f}' for v in result_vector]\n        inner_results_str.append(f\"[{','.join(formatted_values)}]\")\n\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "神经网络的力量源于其隐藏层神经元能够学习到数据中多样化的特征。但这引出了一个根本问题：如果所有神经元在初始化时都完全相同，它们是如何学会执行不同功能的？这项实践聚焦于神经网络中的“置换对称性”概念 ，通过一个具体的数值实验，你将揭示为何确定性的全批量梯度下降无法打破这种对称性，以及随机性（如随机梯度下降或噪声）对于促使神经元分化、学习丰富特征的关键作用。",
            "id": "3134207",
            "problem": "您将研究单隐藏层人工神经网络中隐藏单元间的置换对称性，并量化不同的训练动态如何保持或打破该对称性。考虑一个全连接网络，其含有一个宽度为 $n$ 的隐藏层，输入维度为 $d$，以及一个标量输出，定义如下：\n$$\nf_{\\theta}(x) \\;=\\; \\sum_{k=1}^{n} v_k \\,\\phi\\!\\left(w_k^{\\top} x + b_k\\right) + c,\n$$\n其中 $\\theta = \\{(w_k,b_k,v_k)_{k=1}^{n}, c\\}$ 是参数，$w_k \\in \\mathbb{R}^{d}$，$b_k \\in \\mathbb{R}$，$v_k \\in \\mathbb{R}$，$c \\in \\mathbb{R}$，激活函数为 $\\phi(z) = \\tanh(z)$。在数据集 $\\{(x_i,y_i)\\}_{i=1}^{m}$ 上的经验风险是均方误差 (MSE)，\n$$\n\\mathcal{L}(\\theta) \\;=\\; \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta}(x_i) - y_i\\right)^2.\n$$\n\n基本基础与定义：\n- 置换群 $S_n$ 通过置换隐藏单元的索引来作用于它们。一个置换 $\\pi \\in S_n$ 将 $(w_k,b_k,v_k)$ 变为 $(w_{\\pi(k)}, b_{\\pi(k)}, v_{\\pi(k)})$，并保持 $c$ 不变。\n- 梯度下降通过 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$ 来更新参数，其中学习率 $\\eta  0$。随机梯度下降 (SGD) 使用在小批量（mini-batches）而非整个数据集上计算的梯度的无偏估计量。\n- 丢弃（Dropout）将隐藏单元的激活值乘以独立的伯努利掩码。在反向丢弃（inverted dropout）中，每个隐藏单元的激活值乘以一个保留概率为 $q \\in (0,1]$ 的随机掩码，然后通过 $1/q$ 进行重缩放，以保持其期望不变。\n\n任务：\n1) 仅使用上述核心定义，从第一性原理出发论证对于任意 $\\pi \\in S_n$，都有 $\\mathcal{L}(\\theta) = \\mathcal{L}(\\theta^{\\pi})$，其中 $\\theta^{\\pi}$ 是通过 $\\pi$ 置换隐藏单元后的参数元组。解释为什么如果所有隐藏单元被相同地初始化，即对于所有 $k$ 都有 $(w_k,b_k,v_k) = (w_1,b_1,v_1)$，那么无噪声的全批量梯度下降会在每一步都保持此等式成立。您的论证必须依赖于损失函数在 $S_n$ 下的对称性和链式法则，而不是任何快捷公式。\n\n2) 根据定义，实现一个程序，该程序构建一个数据集并在四种不同的训练方案下训练网络，以通过一个多样性度量来量化特征多样化。请使用以下精确设置，为保证可复现性和可测试性，必须严格遵守。\n\n- 数据集：设置 $m=64$, $d=2$，通过 $x_i \\sim \\mathcal{N}(0,I_2)$ 生成输入 $x_i \\in \\mathbb{R}^2$，随机种子为 $s_{\\text{data}}=2025$。通过以下方式定义目标：\n$$\ny_i \\;=\\; \\sin(x_{i1}) + 0.5 \\cos(2 x_{i2}),\n$$\n其中 $x_{i1}$ 和 $x_{i2}$ 是 $x_i$ 的两个坐标。\n\n- 网络初始化：设置宽度 $n=3$。初始化 $W \\in \\mathbb{R}^{n \\times d}$，使每一行都相同，对于所有 $k \\in \\{1,2,3\\}$，$w_k^{\\top} = [0.1,\\,-0.2]$，设置对所有 $k$ 都有 $b_k = 0$，对所有 $k$ 都有 $v_k = 1/n$，并设置 $c = 0$。\n\n- 训练目标和梯度：使用上述确切的 MSE 损失，并通过链式法则计算所有梯度，\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial v_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right),\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial c} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right),\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, v_k \\, \\left(1 - \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right)^2\\right) x_i,\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, v_k \\, \\left(1 - \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right)^2\\right),\n$$\n其中 $B$ 是批量大小。对于保留概率为 $q$ 的 dropout，将 $\\phi(z)$ 替换为 $\\tilde{\\phi}(z) = \\frac{m}{q}\\phi(z)$，其中 $m \\sim \\mathrm{Bernoulli}(q)$ 对于每个隐藏单元和每个样本独立生成，并在反向传播中使用相同的掩码激活值。\n\n- 特征多样性度量：训练后，计算隐藏层权重向量之间的平均成对角，\n$$\n\\Delta(W) \\;=\\; \\frac{1}{\\binom{n}{2}}\\sum_{1 \\leq i  j \\leq n} \\arccos\\!\\left(\\frac{w_i^{\\top} w_j}{\\|w_i\\|\\,\\|w_j\\|}\\right),\n$$\n约定如果 $\\|w_i\\|\\,\\|w_j\\|$ 低于一个数值阈值（例如，低于 $10^{-12}$），则将相应的角度视为 $0$。所有角度必须以弧度表示。\n\n- 训练方案（测试套件）：运行以下四种情况，每种情况的学习率均为 $\\eta = 0.05$，使用相同的数据集，但随机性来源不同。对于每种情况内部的任何随机操作（例如，dropout 掩码、梯度噪声、数据洗牌），请使用指定的随机种子 $s_{\\text{case}}$。\n  - 情况 1（保持对称性）：全批量梯度下降，批量大小 $B = m = 64$，无 dropout（保留概率 $q=1$），无噪声，轮次 $T=200$，随机种子 $s_{\\text{case}}=101$。\n  - 情况 2（加性梯度噪声）：与情况 1 相同，但在每次更新前，向参数梯度的每个分量添加独立的零均值高斯噪声，标准差为 $\\sigma=10^{-3}$，轮次 $T=200$，随机种子 $s_{\\text{case}}=102$。\n  - 情况 3（带 dropout 的随机梯度下降）：小批量大小 $B=1$（即纯随机梯度下降 (SGD)），对隐藏单元激活值应用反向 dropout，保留概率 $q=0.5$，对每个样本和每个隐藏单元独立进行，轮次 $T=200$，随机种子 $s_{\\text{case}}=103$。\n  - 情况 4（显式无穷小对称性破缺）：与情况 1 相同，但在初始化时通过向第一个隐藏单元权重向量的第一个坐标添加 $\\varepsilon$ 来扰动它，其中 $\\varepsilon = 10^{-3}$，轮次 $T=200$，随机种子 $s_{\\text{case}}=104$。\n\n您的程序必须：\n- 完全按照规定（包括设定种子）实现上述训练动态。\n- 在每种情况后，以弧度计算 $\\Delta(W)$。\n- 产生单行输出，包含四个结果，形式为逗号分隔的列表并用方括号括起，例如 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$，其中每个 $\\Delta_k$ 都四舍五入到恰好 $6$ 位小数。角度必须是弧度。\n\n最终输出格式：\n- 单行列表 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$，其中每个条目都是一个恰好有 $6$ 位小数的浮点数，按顺序代表情况 1 到 4 的平均成对角（以弧度为单位）。",
            "solution": "该问题陈述清晰，在神经网络和优化的基础上具有科学依据，并为数值实验提供了完整、明确的规范。因此，该问题被认定为有效。\n\n根据要求，解决方案包括两部分：关于置换对称性的理论论证，以及一个数值实验的实现。\n\n### 第1部分：关于置换对称性的理论论证\n\n**1.1. 损失函数在置换下的不变性**\n\n网络输出定义为 $f_{\\theta}(x) = \\sum_{k=1}^{n} v_k \\phi(w_k^{\\top} x + b_k) + c$，其中 $\\theta$ 代表全套参数 $\\{(w_k, b_k, v_k)_{k=1}^{n}, c\\}$。激活函数为 $\\phi(z) = \\tanh(z)$。\n\n一个置换 $\\pi \\in S_n$ 通过重新索引其参数来作用于隐藏单元。置换后的参数集为 $\\theta^{\\pi} = \\{(w_{\\pi(k)}, b_{\\pi(k)}, v_{\\pi(k)})_{k=1}^{n}, c\\}$。使用这些置换后参数的网络函数为：\n$$ f_{\\theta^{\\pi}}(x) = \\sum_{k=1}^{n} v_{\\pi(k)} \\phi\\left(w_{\\pi(k)}^{\\top} x + b_{\\pi(k)}\\right) + c $$\n令 $j = \\pi(k)$。当 $k$ 从 $1$ 遍历到 $n$ 时，$j$ 也会覆盖从 $1$ 到 $n$ 的所有索引，因为 $\\pi$ 是一个置换。因此，该求和仅仅是其项的重新排序，由于加法的交换律，这不会改变和的值：\n$$ f_{\\theta^{\\pi}}(x) = \\sum_{j \\in \\{\\pi(1), \\dots, \\pi(n)\\}} v_j \\phi\\left(w_j^{\\top} x + b_j\\right) + c = \\sum_{j=1}^{n} v_j \\phi\\left(w_j^{\\top} x + b_j\\right) + c = f_{\\theta}(x) $$\n由于对于任何 $\\theta$ 及其置换 $\\theta^{\\pi}$，函数的输出 $f_{\\theta}(x)$ 都是相同的，因此均方误差 (MSE) 损失函数 $\\mathcal{L}(\\theta)$ 在此置换下也是不变的：\n$$ \\mathcal{L}(\\theta^{\\pi}) = \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta^{\\pi}}(x_i) - y_i\\right)^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta}(x_i) - y_i\\right)^2 = \\mathcal{L}(\\theta) $$\n这证实了损失函数具有 $S_n$ 置换对称性。\n\n**1.2. 全批量梯度下降对对称性的保持**\n\n我们现在论证，如果所有隐藏单元被相同地初始化，那么在全批量梯度下降的每一步中，这种对称性都会被保持。我们使用归纳法进行论证。\n\n*基本情况（初始化）：* 参数被初始化，使得对于所有 $k \\in \\{1, \\dots, n\\}$，都有 $(w_k^{(0)}, b_k^{(0)}, v_k^{(0)}) = (\\mathbf{w}_0, \\mathbf{b}_0, \\mathbf{v}_0)$。在步骤 $t=0$ 时，对称性成立。\n\n*归纳步骤：* 假设在某个步骤 $t$，所有隐藏单元的参数都是相同的：对于所有 $k$，都有 $(w_k^{(t)}, b_k^{(t)}, v_k^{(t)}) = (\\mathbf{w}_t, \\mathbf{b}_t, \\mathbf{v}_t)$。我们必须证明，经过一个梯度下降步骤后，参数对于所有 $k$ 仍然保持相同，即 $(w_k^{(t+1)}, b_k^{(t+1)}, v_k^{(t+1)})$ 对于所有 $k$ 都相同。\n\n参数 $\\vartheta$ 的梯度下降更新规则是 $\\vartheta^{(t+1)} = \\vartheta^{(t)} - \\eta \\nabla_{\\vartheta} \\mathcal{L}(\\theta^{(t)})$。我们需要证明，关于每个隐藏单元参数的梯度是相同的。\n\n让我们检查任意一个隐藏单元 $k$ 的梯度分量。对于全批量梯度下降，$B=m$。\n关于输出权重 $v_k$ 的梯度是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial v_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right) $$\n根据归纳假设，对所有 $k$，都有 $w_k^{(t)} = \\mathbf{w}_t$ 和 $b_k^{(t)} = \\mathbf{b}_t$。这意味着项 $\\phi(\\dots)$ 对所有单元都是相同的。此外，函数值 $f_{\\theta^{(t)}}(x_i) = \\sum_{j=1}^n v_j^{(t)} \\phi((w_j^{(t)})^{\\top} x_i + b_j^{(t)}) + c^{(t)}$ 也与单元索引 $k$ 的选择无关，因为所有单元对总和的贡献都相同。因此，$\\frac{\\partial \\mathcal{L}}{\\partial v_k}$ 的整个表达式对所有 $k$ 都是相同的。\n\n关于输入权重 $w_k$ 的梯度是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) v_k^{(t)} \\left(1 - \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right)^2\\right) x_i $$\n根据归纳假设，$v_k^{(t)} = \\mathbf{v}_t$，$w_k^{(t)} = \\mathbf{w}_t$ 和 $b_k^{(t)} = \\mathbf{b}_t$。如前所述，$f_{\\theta^{(t)}}(x_i)$ 是公共项。因此，求和中的所有项对于任何 $k$ 的选择都是相同的。因此，$\\frac{\\partial \\mathcal{L}}{\\partial w_k}$ 对所有 $k$ 都是相同的。\n\n类似的论证也适用于偏置项的梯度：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) v_k^{(t)} \\left(1 - \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right)^2\\right) $$\n这个梯度对所有 $k$ 也都是相同的。\n\n由于梯度 $(\\nabla_{w_k}\\mathcal{L}, \\nabla_{b_k}\\mathcal{L}, \\nabla_{v_k}\\mathcal{L})$ 对所有单元 $k$ 都是相同的，并且参数 $(w_k^{(t)}, b_k^{(t)}, v_k^{(t)})$ 也是相同的，因此更新步骤\n$$ (w_k^{(t+1)}, b_k^{(t+1)}, v_k^{(t+1)}) = (w_k^{(t)}, b_k^{(t)}, v_k^{(t)}) - \\eta (\\nabla_{w_k}\\mathcal{L}, \\nabla_{b_k}\\mathcal{L}, \\nabla_{v_k}\\mathcal{L}) $$\n产生的新参数集对所有单元也都是相同的。\n\n通过归纳法，从相同的参数开始，全批量梯度下降在所有训练步骤中都保持了这种对称性。这导致所有隐藏单元学习到完全相同的特征，权重向量 $w_k$ 保持共线（在这种情况下是相同的），从而得到多样性度量 $\\Delta(W) = 0$。任何对这种对称设置的偏离（例如，随机梯度、梯度噪声、dropout 或非相同初始化）都将打破对称性，并导致隐藏单元多样化。\n\n### 第2部分：实现\n\n以下程序实现了四个指定的训练方案，并为每个方案计算特征多样性度量 $\\Delta(W)$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants from problem\nM = 64\nD = 2\nN = 3\nETA = 0.05\nEPOCHS = 200\nNUM_THRESHOLD = 1e-12\n\ndef phi(z):\n    \"\"\"Tanh activation function.\"\"\"\n    return np.tanh(z)\n\ndef phi_prime(z):\n    \"\"\"Derivative of the tanh activation function.\"\"\"\n    return 1 - np.tanh(z)**2\n\ndef calculate_diversity(w_matrix):\n    \"\"\"\n    Computes the mean pairwise angle between rows of the weight matrix W.\n    \"\"\"\n    total_angle = 0.0\n    num_pairs = 0\n    for i in range(N):\n        for j in range(i + 1, N):\n            w_i = w_matrix[i]\n            w_j = w_matrix[j]\n            \n            norm_i = np.linalg.norm(w_i)\n            norm_j = np.linalg.norm(w_j)\n            \n            denominator = norm_i * norm_j\n            if denominator  NUM_THRESHOLD:\n                angle = 0.0\n            else:\n                dot_product = np.dot(w_i, w_j)\n                # Clip for numerical stability of arccos\n                cosine_sim = np.clip(dot_product / denominator, -1.0, 1.0)\n                angle = np.arccos(cosine_sim)\n            \n            total_angle += angle\n            num_pairs += 1\n            \n    return total_angle / num_pairs if num_pairs > 0 else 0.0\n\ndef run_case(case_num, seed, batch_size, q, noise_std, initial_perturbation, X, Y):\n    \"\"\"\n    Runs a single training experiment as specified by the case parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Initialize parameters\n    W = np.array([[0.1, -0.2]] * N, dtype=np.float64)\n    b = np.zeros(N, dtype=np.float64)\n    v = np.ones(N, dtype=np.float64) / N\n    c = 0.0\n\n    # Apply perturbation for Case 4\n    if initial_perturbation > 0:\n        W[0, 0] += initial_perturbation\n    \n    indices = np.arange(M)\n\n    for epoch in range(EPOCHS):\n        if batch_size  M:  # For SGD, shuffle data each epoch\n            rng.shuffle(indices)\n\n        for i in range(0, M, batch_size):\n            batch_indices = indices[i:i + batch_size]\n            X_batch = X[batch_indices]\n            Y_batch = Y[batch_indices]\n            \n            B = X_batch.shape[0]\n\n            # Forward pass\n            Z = X_batch @ W.T + b\n            A_pre = phi(Z)\n            \n            dropout_mask = None\n            if q  1.0:\n                dropout_mask = rng.binomial(1, q, size=A_pre.shape)\n                A_post = A_pre * dropout_mask / q\n            else:\n                A_post = A_pre\n\n            Y_pred = A_post @ v + c\n            \n            # Backward pass (compute gradients)\n            error_term = (2 / B) * (Y_pred - Y_batch)  # shape (B,)\n            \n            grad_c = np.sum(error_term)\n            grad_v = A_post.T @ error_term  # shape (n,)\n            \n            delta_out = error_term[:, np.newaxis] * v  # shape (B, n)\n            \n            dZ = delta_out * (1 - A_pre**2) # phi_prime(Z) is 1-phi(Z)^2 = 1-A_pre^2\n            \n            if q  1.0:\n                dZ = dZ * dropout_mask / q\n            \n            grad_b = np.sum(dZ, axis=0)  # shape (n,)\n            grad_W = dZ.T @ X_batch      # shape (n, d)\n\n            # Add gradient noise for Case 2\n            if noise_std > 0:\n                grad_W += rng.normal(0, noise_std, size=grad_W.shape)\n                grad_b += rng.normal(0, noise_std, size=grad_b.shape)\n                grad_v += rng.normal(0, noise_std, size=grad_v.shape)\n                grad_c += rng.normal(0, noise_std)\n\n            # Update parameters\n            W -= ETA * grad_W\n            b -= ETA * grad_b\n            v -= ETA * grad_v\n            c -= ETA * grad_c\n\n    return calculate_diversity(W)\n\ndef solve():\n    # Dataset generation\n    data_rng = np.random.default_rng(2025)\n    X = data_rng.normal(0, 1, size=(M, D))\n    Y = np.sin(X[:, 0]) + 0.5 * np.cos(2 * X[:, 1])\n\n    # Test suite definition\n    test_cases = [\n        # (case_num, seed, batch_size, q, noise_std, initial_perturbation)\n        (1, 101, M, 1.0, 0.0, 0.0),            # Case 1: Full-batch GD\n        (2, 102, M, 1.0, 1e-3, 0.0),           # Case 2: Gradient noise\n        (3, 103, 1, 0.5, 0.0, 0.0),            # Case 3: SGD with dropout\n        (4, 104, M, 1.0, 0.0, 1e-3),           # Case 4: Initial perturbation\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_case(*case_params, X, Y)\n        results.append(result)\n\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们训练神经网络，希望它能学习到生成数据的真实潜在函数。但在实践中，我们如何能确定学习是否成功？这项实践引入了一个受控的“教师-学生”实验框架 ，让你扮演一个上帝的角色：首先定义一个“教师”网络作为“真理”，然后训练一个“学生”网络来拟合教师生成的数据。通过这个过程，你将定量地探索模型容量、样本数量和噪声水平如何共同影响学生网络成功“恢复”教师网络参数的能力，从而深刻理解泛化与过拟合的本质。",
            "id": "3134222",
            "problem": "给定一个用于两层人工神经网络（ANN）的教师-学生（teacher-student）设置，其中教师网络是已知的，学生网络试图从教师网络生成的数据中恢复其参数。教师网络有一个隐藏层，使用双曲正切激活函数。学生网络具有相同的架构，但可能有不同的隐藏层宽度，以模拟过参数化（overparameterization）。您的任务是实现一个完整的程序，该程序从教师网络生成数据，通过最小化经验均方误差来训练学生网络，然后评估在不同样本量和宽度下，学生网络是否在置换（permutation）意义下恢复了教师网络的隐藏单元。\n\n从以下基本概念开始：\n- 一个参数向量为 $w \\in \\mathbb{R}^d$、输入为 $x \\in \\mathbb{R}^d$ 的人工神经元，对于一个平滑的非线性函数 $\\sigma$，其输出为 $\\sigma(w^\\top x)$。此处，使用双曲正切激活函数 $\\sigma(z) = \\tanh(z)$。\n- 一个隐藏层宽度为 $H$ 且标量输出的两层网络计算 $f(x) = \\sum_{j=1}^{H} a_j \\sigma(w_j^\\top x)$，其中 $\\{w_j\\}_{j=1}^{H}$ 是隐藏层权重，$\\{a_j\\}_{j=1}^{H}$ 是输出层权重。\n- 使用均方误差的经验风险最小化定义了目标函数 $L(W,a) = \\frac{1}{m} \\sum_{k=1}^{m} \\left(f_S(x_k;W,a) - y_k\\right)^2$，其中 $\\{(x_k,y_k)\\}_{k=1}^{m}$ 是训练集，$W = [w_1,\\dots,w_{H_s}]^\\top$ 汇集了学生网络中的隐藏层权重，$a = [a_1,\\dots,a_{H_s}]^\\top$ 汇集了输出层权重。\n- 全批量梯度下降（Full-batch gradient descent）通过沿 $L(W,a)$ 相对于 $W$ 和 $a$ 的负梯度方向来更新参数。\n\n教师网络和数据生成：\n- 固定输入维度 $d \\in \\mathbb{N}$ 和教师网络的隐藏层宽度 $H_t \\in \\mathbb{N}$。\n- 教师函数为 $f_T(x) = \\sum_{j=1}^{H_t} v_j \\tanh(u_j^\\top x)$，其中教师隐藏层权重为 $\\{u_j \\in \\mathbb{R}^d\\}_{j=1}^{H_t}$，输出层权重为 $\\{v_j \\in \\mathbb{R}\\}_{j=1}^{H_t}$。训练输入 $x_k$ 是独立从 $\\mathcal{N}(0,I_d)$ 中抽取的，对于 $k=1,\\dots,m$。输出为 $y_k = f_T(x_k) + \\epsilon_k$，其中 $\\epsilon_k \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立噪声。\n- 学生函数为 $f_S(x;W,a) = \\sum_{i=1}^{H_s} a_i \\tanh(w_i^\\top x)$，其中 $H_s \\in \\mathbb{N}$ 是学生网络的隐藏层宽度。\n\n训练：\n- 用较小的随机值初始化学生网络的参数 $(W,a)$。\n- 在训练数据 $\\{(x_k,y_k)\\}_{k=1}^{m}$ 上，使用学习率为 $\\eta$ 的全批量梯度下降执行 $T$ 步，以最小化 $L(W,a)$。\n- 使用链式法则推导关于 $W$ 和 $a$ 的梯度。\n\n恢复准则：\n- 由于隐藏单元仅在置换意义下是可识别的，因此使用一个从 $\\mathcal{N}(0,I_d)$ 中独立同分布（i.i.d.）抽取的验证集 $\\{x^\\text{(val)}_\\ell\\}_{\\ell=1}^{m_\\text{val}}$，来定义一个在教师和学生隐藏单元之间的置换不变匹配。\n- 对于每个教师单元 $j \\in \\{1,\\dots,H_t\\}$ 和学生单元 $i \\in \\{1,\\dots,H_s\\}$，在验证输入上定义贡献向量：\n  $$c_{T,j} = \\left[v_j \\tanh\\left(u_j^\\top x^\\text{(val)}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}}, \\quad c_{S,i} = \\left[a_i \\tanh\\left(w_i^\\top x^\\text{(val)}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}}.$$\n- 定义成对成本\n  $$C_{j,i} = \\frac{1}{m_\\text{val}} \\sum_{\\ell=1}^{m_\\text{val}} \\left(c_{S,i}(\\ell) - c_{T,j}(\\ell)\\right)^2.$$\n- 使用一个线性分配过程找到教师单元到学生单元的最小成本分配，该过程最小化所选配对的 $C_{j,i}$ 之和。如果 $H_s  H_t$，一些教师单元将保持未匹配状态；为每个未匹配的教师单元增加一个固定的成本 $P  0$作为惩罚。\n- 令 $J^\\star$ 为已匹配的教师-学生对的集合。定义每个教师单元的平均匹配成本\n  $$\\overline{C} = \\frac{1}{H_t} \\left( \\sum_{(j,i)\\in J^\\star} C_{j,i} + P \\cdot \\left(H_t - |J^\\star|\\right) \\right).$$\n- 如果 $\\overline{C} \\leq \\tau$，其中 $\\tau  0$ 是一个固定的阈值，则宣布恢复成功。\n\n研究样本复杂度的影响：\n- 改变样本数量 $m$ 和相对于教师网络宽度 $H_t$ 的学生网络宽度 $H_s$，并评估是否发生恢复。过参数化对应于 $H_s  H_t$。\n\n程序行为要求：\n- 实现完整流程：教师网络生成、数据生成、通过梯度下降训练学生网络、验证集生成、通过分配实现的置换不变匹配以及恢复决策。\n- 使用 $m_\\text{val} = 512$ 个验证输入。\n- 使用超参数：$T = 2000$ 步训练、学习率 $\\eta = 0.05$、未匹配惩罚项 $P = 2.0$ 以及恢复阈值 $\\tau = 0.12$。在训练期间对 $W$ 施加一个强度为 $\\lambda_w = 10^{-4}$ 的小 $\\ell_2$ 正则化，以稳定优化过程。\n- 为保证可复现性，将所有随机数生成器设置为以下测试套件中指定的固定种子。\n\n测试套件：\n在以下五个案例上评估程序。每个案例是一个元组 $(d,H_t,H_s,m,\\sigma,\\text{seed})$：\n- 案例 1：$(5,3,3,2000,0.0,12345)$，一个拥有大量样本且规格匹配的学生网络。\n- 案例 2：$(5,3,2,2000,0.0,22345)$，一个拥有大量样本但欠参数化的学生网络。\n- 案例 3：$(5,3,6,800,0.0,32345)$，一个拥有中等样本量且过参数化的学生网络。\n- 案例 4：$(5,3,3,50,0.1,42345)$，一个小样本、有噪声的场景。\n- 案例 5：$(5,3,3,200,0.02,52345)$，一个中等样本量且有轻微噪声的场景。\n\n答案规范：\n- 对每个测试用例，输出一个布尔值，指示是否根据准则 $\\overline{C} \\leq \\tau$ 实现了恢复。\n- 您的程序应生成单行输出，其中包含一个按上述测试套件顺序排列、由逗号分隔并用方括号括起来的列表。例如，输出格式必须类似于 $[b_1,b_2,b_3,b_4,b_5]$，其中每个 $b_k$ 是 $True$ 或 $False$。",
            "solution": "该问题要求实现并评估一个用于两层人工神经网络的教师-学生框架。目标是确定一个通过梯度下降训练的学生网络，在模型大小、样本量和噪声等不同条件下，是否能恢复一个已知教师网络的功能组件。\n\n问题的核心涉及以下一系列步骤：\n1.  定义教师和学生网络架构。\n2.  从教师网络生成合成数据集。\n3.  通过最小化正则化的均方误差损失，在该数据集上训练学生网络。\n4.  使用置换不变的匹配准则，评估学生网络隐藏单元对教师网络隐藏单元的恢复情况。\n\n让我们正式说明每个组件。\n\n**1. 网络架构和数据生成**\n\n教师和学生网络是具有单个隐藏层和标量输出的两层神经网络。隐藏层神经元的激活函数是双曲正切函数 $\\sigma(z) = \\tanh(z)$。\n\n输入维度为 $d$、隐藏层宽度为 $H_t$ 的教师网络函数 $f_T(x)$ 由下式给出：\n$$ f_T(x) = \\sum_{j=1}^{H_t} v_j \\tanh(u_j^\\top x) $$\n其中 $\\{u_j \\in \\mathbb{R}^d\\}_{j=1}^{H_t}$ 是教师网络的隐藏层权重向量，$\\{v_j \\in \\mathbb{R}\\}_{j=1}^{H_t}$ 是教师网络的输出层权重。这些参数是固定的，被视为真实值。\n\n训练数据集 $\\{(x_k, y_k)\\}_{k=1}^{m}$ 包含 $m$ 个样本。输入 $x_k$ 独立地从标准正态分布中抽取，即 $x_k \\sim \\mathcal{N}(0, I_d)$。对应的输出 $y_k$ 由教师网络生成，并加入了加性高斯噪声：\n$$ y_k = f_T(x_k) + \\epsilon_k, \\quad \\text{其中} \\quad \\epsilon_k \\sim \\mathcal{N}(0, \\sigma^2) $$\n\n学生网络具有相同的架构，但可能有不同的隐藏层宽度 $H_s$。其函数 $f_S(x)$ 为：\n$$ f_S(x; W, a) = \\sum_{i=1}^{H_s} a_i \\tanh(w_i^\\top x) $$\n这里，$W = [w_1, \\dots, w_{H_s}]^\\top$ 是学生网络的隐藏层权重矩阵（$W \\in \\mathbb{R}^{H_s \\times d}$），$a = [a_1, \\dots, a_{H_s}]^\\top$ 是学生网络的输出层权重向量（$a \\in \\mathbb{R}^{H_s}$）。这些是在训练过程中要学习的参数。\n\n**2. 通过梯度下降进行训练**\n\n学生网络通过最小化经验均方误差进行训练，并增加了一个关于隐藏层权重的 $\\ell_2$ 正则化项以提高稳定性。损失函数 $L(W, a)$ 为：\n$$ L(W, a) = \\frac{1}{m} \\sum_{k=1}^{m} \\left(f_S(x_k; W, a) - y_k\\right)^2 + \\frac{\\lambda_w}{2} \\|W\\|_F^2 $$\n其中 $\\|W\\|_F^2 = \\sum_{i=1}^{H_s} \\|w_i\\|_2^2$ 是权重矩阵 $W$ 的弗罗贝尼乌斯范数的平方，$\\lambda_w$ 是正则化强度。\n\n训练使用全批量梯度下降进行 $T$ 次迭代。参数根据以下规则更新：\n$$ a^{(t+1)} = a^{(t)} - \\eta \\nabla_a L_t $$\n$$ W^{(t+1)} = W^{(t)} - \\eta \\nabla_W L_t $$\n其中 $\\eta$ 是学习率，梯度是在当前参数值 $(W^{(t)}, a^{(t)})$ 处计算的。\n\n梯度是使用链式法则推导出来的。激活函数的导数是 $\\sigma'(z) = \\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$。\n令 $\\delta_k = f_S(x_k) - y_k$ 为样本 $k$ 的预测误差。\n\n关于输出权重 $a_i$ 的梯度是：\n$$ \\frac{\\partial L}{\\partial a_i} = \\frac{1}{m} \\sum_{k=1}^{m} 2 \\delta_k \\frac{\\partial f_S(x_k)}{\\partial a_i} = \\frac{2}{m} \\sum_{k=1}^{m} \\delta_k \\tanh(w_i^\\top x_k) $$\n\n关于隐藏层权重向量 $w_i$ 的梯度是：\n$$ \\frac{\\partial L}{\\partial w_i} = \\left( \\frac{1}{m} \\sum_{k=1}^{m} 2 \\delta_k \\frac{\\partial f_S(x_k)}{\\partial w_i} \\right) + \\frac{\\partial}{\\partial w_i}\\left(\\frac{\\lambda_w}{2} \\sum_{j=1}^{H_s} w_j^\\top w_j\\right) $$\n$$ \\frac{\\partial L}{\\partial w_i} = \\left( \\frac{2}{m} \\sum_{k=1}^{m} \\delta_k a_i \\left(1 - \\tanh^2(w_i^\\top x_k)\\right) x_k \\right) + \\lambda_w w_i $$\n\n这些表达式可以通过对整个数据集进行向量化操作来高效计算。\n\n**3. 恢复评估准则**\n\n神经网络的隐藏单元仅在置换和符号翻转的意义下是可识别的。例如，将 $(a_i, w_i)$ 更改为 $(-a_i, -w_i)$ 不会改变项 $a_i \\tanh(w_i^\\top x)$ 的值，因为 $\\tanh$ 是一个奇函数。同样，重新排序隐藏单元也不会改变网络的输出。为了解决这个问题，恢复情况的评估采用了一个置换不变的匹配过程。\n\n从 $\\mathcal{N}(0, I_d)$ 中抽取一个大小为 $m_\\text{val}$ 的验证集 $\\{x^{(\\text{val})}_\\ell\\}_{\\ell=1}^{m_\\text{val}}$。对于每个教师单元 $j$ 和学生单元 $i$，我们计算它们在该集合上的功能贡献：\n$$ c_{T,j} = \\left[v_j \\tanh\\left(u_j^\\top x^{(\\text{val})}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}} \\in \\mathbb{R}^{m_\\text{val}} $$\n$$ c_{S,i} = \\left[a_i \\tanh\\left(w_i^\\top x^{(\\text{val})}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}} \\in \\mathbb{R}^{m_\\text{val}} $$\n\n定义一个成对成本矩阵 $C \\in \\mathbb{R}^{H_t \\times H_s}$，其中每个条目 $C_{j,i}$ 是教师单元 $j$ 和学生单元 $i$ 的贡献向量之间的均方误差：\n$$ C_{j,i} = \\frac{1}{m_\\text{val}} \\sum_{\\ell=1}^{m_\\text{val}} \\left(c_{S,i}(\\ell) - c_{T,j}(\\ell)\\right)^2 $$\n\n寻找教师和学生单元之间最佳匹配的问题对应于一个线性分配问题（或最小权重二分图匹配）。目标是找到一组配对 $(j,i)$，以最小化总成本 $\\sum C_{j,i}$。这个问题可以使用诸如匈牙利算法之类的高效算法来解决，该算法在 `scipy.optimize.linear_sum_assignment` 中可用。\n\n令 $J^\\star$ 为找到的最优成本配对集合。匹配的配对数量为 $|J^\\star| = \\min(H_t, H_s)$。如果学生网络是欠参数化的（$H_s  H_t$），一些教师单元将无法匹配。对于 $H_t - |J^\\star|$ 个未匹配的教师单元，每个都会增加一个惩罚 $P$。\n\n然后，每个教师单元的平均匹配成本 $\\overline{C}$ 计算如下：\n$$ \\overline{C} = \\frac{1}{H_t} \\left( \\sum_{(j,i)\\in J^\\star} C_{j,i} + P \\cdot \\left(H_t - |J^\\star|\\right) \\right) $$\n\n最后，如果这个平均成本低于给定的阈值 $\\tau$，则宣布恢复成功：\n$$ \\text{恢复} = (\\overline{C} \\leq \\tau) $$\n\n完整的实现将为每个指定的测试用例执行这整个流程，使用固定的随机种子以保证可复现性，并报告二元恢复结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to run the teacher-student ANN simulation for all test cases.\n    \"\"\"\n    # Test cases: (d, H_t, H_s, m, sigma, seed)\n    test_cases = [\n        (5, 3, 3, 2000, 0.0, 12345),\n        (5, 3, 2, 2000, 0.0, 22345),\n        (5, 3, 6, 800, 0.0, 32345),\n        (5, 3, 3, 50, 0.1, 42345),\n        (5, 3, 3, 200, 0.02, 52345),\n    ]\n\n    # Hyperparameters\n    m_val = 512\n    T = 2000\n    eta = 0.05\n    P = 2.0\n    tau = 0.12\n    lambda_w = 1e-4\n\n    results = []\n\n    for d, H_t, H_s, m, sigma, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Teacher network and data generation\n        # Teacher parameters\n        U_teacher = rng.standard_normal(size=(H_t, d))\n        v_teacher = rng.standard_normal(size=H_t)\n\n        # Training data\n        X_train = rng.standard_normal(size=(m, d))\n        teacher_activations = np.tanh(X_train @ U_teacher.T)\n        y_true = teacher_activations @ v_teacher\n        noise = rng.normal(scale=sigma, size=m)\n        y_train = y_true + noise\n\n        # 2. Student network training\n        # Student parameters initialization\n        # Initialize with small random values to break symmetry but stay in a reasonable regime\n        W_student = rng.normal(scale=0.1, size=(H_s, d))\n        a_student = rng.normal(scale=0.1, size=H_s)\n\n        # Full-batch gradient descent loop\n        for _ in range(T):\n            # Forward pass\n            student_pre_activations = X_train @ W_student.T\n            student_activations = np.tanh(student_pre_activations)\n            y_pred = student_activations @ a_student\n\n            # Compute error\n            error = y_pred - y_train\n\n            # Compute gradients\n            grad_a = (2 / m) * (student_activations.T @ error)\n            \n            # The gradient for W requires careful vectorization\n            # term_for_w_grad has shape (m, H_s)\n            term_for_w_grad = (2 / m) * (error[:, np.newaxis] * a_student[np.newaxis, :]) * (1 - student_activations**2)\n            grad_W = term_for_w_grad.T @ X_train + lambda_w * W_student\n\n            # Update parameters\n            a_student -= eta * grad_a\n            W_student -= eta * grad_W\n\n        # 3. Recovery criterion evaluation\n        # Generate validation data\n        X_val = rng.standard_normal(size=(m_val, d))\n        \n        # Compute contribution vectors for teacher and student units\n        contrib_teacher = np.tanh(X_val @ U_teacher.T) * v_teacher[np.newaxis, :]\n        contrib_student = np.tanh(X_val @ W_student.T) * a_student[np.newaxis, :]\n        \n        # Compute pairwise cost matrix C\n        # Using broadcasting to avoid loops\n        # C_si shape: (m_val, 1, H_s)\n        # C_tj shape: (m_val, H_t, 1)\n        # diff shape: (m_val, H_t, H_s) after broadcasting\n        diff_sq = (contrib_student[:, np.newaxis, :] - contrib_teacher[:, :, np.newaxis])**2\n        cost_matrix = np.mean(diff_sq, axis=0) # shape (H_t, H_s)\n\n        # Find minimum-cost assignment\n        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n        \n        # Calculate total and average cost\n        matched_cost = cost_matrix[row_ind, col_ind].sum()\n        num_matched = len(row_ind)\n        num_unmatched_teachers = H_t - num_matched\n        \n        penalty_cost = P * num_unmatched_teachers\n        total_cost = matched_cost + penalty_cost\n        avg_cost = total_cost / H_t\n        \n        # Check for recovery\n        recovery_success = avg_cost = tau\n        results.append(recovery_success)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}