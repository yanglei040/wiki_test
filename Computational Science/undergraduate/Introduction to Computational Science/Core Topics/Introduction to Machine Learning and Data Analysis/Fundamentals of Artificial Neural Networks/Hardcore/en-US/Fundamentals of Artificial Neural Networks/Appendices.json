{
    "hands_on_practices": [
        {
            "introduction": "In modern deep learning, classification is a primary task, and the Softmax Cross-Entropy loss function is the standard for training models. The gradient of this loss, which has the elegant form $\\mathbf{p} - \\mathbf{y}$, acts as the engine of learning by providing a corrective signal that tells the model how to adjust its predictions. By deriving this gradient from first principles and exploring its properties numerically , you will build a foundational understanding of gradient saturation, a phenomenon where learning slows as the model becomes overly confident, and see how regularization techniques like label smoothing can ensure more stable and effective training.",
            "id": "3134219",
            "problem": "You will analyze gradients of the Softmax Cross-Entropy (SCE) loss in the context of Artificial Neural Networks (ANN). Your tasks are to derive from first principles how the gradient depends on the logits, to relate the gradient magnitude to logit differences (margins), and to examine the impact of label smoothing on gradient saturation. You must then implement a program that computes quantitative gradient metrics for a provided test suite.\n\nFoundational base (definitions only):\n- Given a vector of logits $\\mathbf{z} \\in \\mathbb{R}^K$, the softmax function produces class probabilities $\\mathbf{p} \\in \\mathbb{R}^K$ with $p_i \\ge 0$ and $\\sum_{i=1}^K p_i = 1$.\n- For a target distribution $\\mathbf{y} \\in \\mathbb{R}^K$ with $y_i \\ge 0$ and $\\sum_{i=1}^K y_i = 1$, the cross-entropy loss is a scalar function of $\\mathbf{p}$.\n- Label smoothing modifies a one-hot target for the correct class index $c$ by assigning $y_c = 1 - \\varepsilon$ and $y_j = \\varepsilon/(K-1)$ for all $j \\ne c$, where $\\varepsilon \\in [0,1)$.\n\nRequired derivations and analysis:\n1) Starting strictly from the above definitions and fundamental calculus (quotient rule, chain rule, and properties of the exponential and logarithm), derive the gradient of the Softmax Cross-Entropy loss with respect to the logits $\\mathbf{z}$. Present the general result for an arbitrary target distribution $\\mathbf{y}$ and then specialize it to one-hot targets and to label-smoothed targets. Do not assume any pre-known gradient formula; derive it.\n2) Using your gradient expression and the fact that softmax probabilities depend only on differences of logits (shift invariance), analyze how the gradient magnitude relates to logit differences. Formalize the notion of the correct-class logit margin $\\Delta = z_c - \\max_{j \\ne c} z_j$ and explain, using your formulas, why large positive $\\Delta$ leads to small gradients (saturation) for one-hot targets, why large negative $\\Delta$ leads to large gradients, and how label smoothing with $\\varepsilon > 0$ alters these magnitudes even when the model is very confident.\n3) Implement a program that, for each test case, computes the following quantities:\n   - The softmax probabilities $\\mathbf{p}$ using a numerically stable method.\n   - The gradient vector $\\nabla_{\\mathbf{z}} L$ with respect to logits under label smoothing parameter $\\varepsilon$.\n   - The correct-class margin $\\Delta = z_c - \\max_{j \\ne c} z_j$.\n   - The Euclidean (also called $\\ell_2$) norm $\\lVert \\nabla_{\\mathbf{z}} L \\rVert_2$.\n   - The absolute gradient at the correct class $\\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert$.\n   - The maximum absolute gradient across classes $\\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert$.\n\nImplementation requirements:\n- Use natural logarithms.\n- Use a numerically stable softmax implementation (you may subtract $\\max_i z_i$ from all logits before exponentiation).\n- For each test case, return a list of four floats $[\\Delta, \\lVert \\nabla_{\\mathbf{z}} L \\rVert_2, \\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert, \\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert]$, each rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of lists, with no spaces, enclosed in square brackets. For example, the output format must look like $[[a\\_1,b\\_1,c\\_1,d\\_1],[a\\_2,b\\_2,c\\_2,d\\_2],\\dots]$ where each $a_i,b_i,c_i,d_i$ are decimal numbers shown with exactly $6$ digits after the decimal point.\n\nTest suite (each test case is a triple $(\\mathbf{z}, c, \\varepsilon)$):\n- Case $1$: $\\mathbf{z} = [0.1, 0.2, 0.15, 0.05]$, $c = 1$, $\\varepsilon = 0.0$.\n- Case $2$: $\\mathbf{z} = [8.0, 1.0, 0.0, -2.0]$, $c = 0$, $\\varepsilon = 0.0$.\n- Case $3$: $\\mathbf{z} = [-2.0, 5.0, -1.0, 0.0]$, $c = 0$, $\\varepsilon = 0.0$.\n- Case $4$: $\\mathbf{z} = [1000.0, 1000.0, 1000.0, 1000.0]$, $c = 2$, $\\varepsilon = 0.0$.\n- Case $5$: $\\mathbf{z} = [8.0, 1.0, 0.0, -2.0]$, $c = 0$, $\\varepsilon = 0.1$.\n- Case $6$: $\\mathbf{z} = [-2.0, 5.0, -1.0, 0.0]$, $c = 0$, $\\varepsilon = 0.1$.\n\nFinal output specification:\n- The program must print exactly one line: a single top-level list containing $6$ inner lists, one per test case, in the order listed above.\n- Each inner list must be $[\\Delta, \\lVert \\nabla_{\\mathbf{z}} L \\rVert_2, \\lvert (\\nabla_{\\mathbf{z}} L)_c \\rvert, \\max_i \\lvert (\\nabla_{\\mathbf{z}} L)_i \\rvert]$, with each float rounded to exactly $6$ decimal places.\n- There must be no spaces anywhere in the output line.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It consists of a standard, non-trivial derivation, a conceptual analysis rooted in that derivation, and a concrete implementation task, all central to the fundamentals of artificial neural networks.\n\n### 1. Derivation of the Gradient of Softmax Cross-Entropy Loss\n\nWe are tasked with deriving the gradient of the Softmax Cross-Entropy (SCE) loss with respect to the input logits, $\\mathbf{z}$.\n\n**Definitions:**\n- Logits: $\\mathbf{z} = [z_1, z_2, \\dots, z_K]^T \\in \\mathbb{R}^K$.\n- Softmax probability for class $i$: $p_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$.\n- Target probability distribution: $\\mathbf{y} = [y_1, y_2, \\dots, y_K]^T$, where $y_i \\ge 0$ and $\\sum_{i=1}^K y_i = 1$.\n- Cross-Entropy Loss: $L(\\mathbf{p}, \\mathbf{y}) = -\\sum_{i=1}^K y_i \\log p_i$. Note that the logarithm is the natural logarithm, $\\ln$.\n\nOur goal is to compute the gradient vector $\\nabla_{\\mathbf{z}} L$, whose components are $\\frac{\\partial L}{\\partial z_k}$ for $k \\in \\{1, \\dots, K\\}$. We apply the chain rule:\n$$\n\\frac{\\partial L}{\\partial z_k} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k}\n$$\n\n**Step 1.1: Compute $\\frac{\\partial L}{\\partial p_i}$**\nThe loss is $L = -\\sum_{j=1}^K y_j \\log p_j$. The partial derivative with respect to a specific probability $p_i$ is straightforward:\n$$\n\\frac{\\partial L}{\\partial p_i} = \\frac{\\partial}{\\partial p_i} \\left( -y_i \\log p_i - \\sum_{j \\ne i} y_j \\log p_j \\right) = -\\frac{y_i}{p_i}\n$$\n\n**Step 1.2: Compute $\\frac{\\partial p_i}{\\partial z_k}$ (Jacobian of the Softmax function)**\nWe must consider two cases for the derivative of $p_i = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$ with respect to $z_k$. Let $D = \\sum_{j=1}^K e^{z_j}$.\n\nCase A: $i = k$. We use the quotient rule $\\left(\\frac{f}{g}\\right)' = \\frac{f'g - fg'}{g^2}$.\n$$\n\\frac{\\partial p_k}{\\partial z_k} = \\frac{\\frac{\\partial(e^{z_k})}{\\partial z_k} D - e^{z_k} \\frac{\\partial D}{\\partial z_k}}{D^2} = \\frac{e^{z_k} D - e^{z_k} e^{z_k}}{D^2} = \\frac{e^{z_k}}{D} \\frac{D - e^{z_k}}{D} = p_k (1 - p_k)\n$$\n\nCase B: $i \\ne k$.\n$$\n\\frac{\\partial p_i}{\\partial z_k} = \\frac{\\frac{\\partial(e^{z_i})}{\\partial z_k} D - e^{z_i} \\frac{\\partial D}{\\partial z_k}}{D^2} = \\frac{0 \\cdot D - e^{z_i} e^{z_k}}{D^2} = -\\frac{e^{z_i}}{D} \\frac{e^{z_k}}{D} = -p_i p_k\n$$\n\n**Step 1.3: Combine the derivatives**\nNow we substitute these results back into the chain rule expression for $\\frac{\\partial L}{\\partial z_k}$:\n$$\n\\frac{\\partial L}{\\partial z_k} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k} = \\left(\\frac{\\partial L}{\\partial p_k} \\frac{\\partial p_k}{\\partial z_k}\\right) + \\sum_{i \\ne k} \\left(\\frac{\\partial L}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_k}\\right)\n$$\n$$\n\\frac{\\partial L}{\\partial z_k} = \\left(-\\frac{y_k}{p_k}\\right) (p_k(1 - p_k)) + \\sum_{i \\ne k} \\left(-\\frac{y_i}{p_i}\\right) (-p_i p_k)\n$$\n$$\n\\frac{\\partial L}{\\partial z_k} = -y_k(1 - p_k) + \\sum_{i \\ne k} y_i p_k = -y_k + y_k p_k + p_k \\sum_{i \\ne k} y_i\n$$\nFactoring out $p_k$:\n$$\n\\frac{\\partial L}{\\partial z_k} = p_k \\left(y_k + \\sum_{i \\ne k} y_i \\right) - y_k\n$$\nSince $\\mathbf{y}$ is a probability distribution, $\\sum_{i=1}^K y_i = y_k + \\sum_{i \\ne k} y_i = 1$. This simplifies the expression to:\n$$\n\\frac{\\partial L}{\\partial z_k} = p_k(1) - y_k = p_k - y_k\n$$\nThis elegant result states that the gradient of the Softmax Cross-Entropy loss with respect to a logit $z_k$ is the difference between the predicted probability $p_k$ and the target probability $y_k$. In vector form, the gradient is:\n$$\n\\nabla_{\\mathbf{z}} L = \\mathbf{p} - \\mathbf{y}\n$$\n\n**Specializations:**\n1.  **One-Hot Targets:** For a correct class index $c$, the target vector is $\\mathbf{y}$ with $y_c = 1$ and $y_j = 0$ for all $j \\ne c$. The gradient components are:\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1$\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - 0 = p_j \\quad (\\text{for } j \\ne c)$\n\n2.  **Label-Smoothed Targets:** With smoothing parameter $\\varepsilon \\in [0, 1)$, the target vector $\\mathbf{y}$ is defined as $y_c = 1 - \\varepsilon$ and $y_j = \\frac{\\varepsilon}{K-1}$ for all $j \\ne c$. The gradient components are:\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - (1 - \\varepsilon)$\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - \\frac{\\varepsilon}{K-1} \\quad (\\text{for } j \\ne c)$\n\n### 2. Analysis of Gradient Magnitude and Logit Margin\n\nThe softmax function is invariant to a constant shift in all logits, i.e., $\\text{softmax}(\\mathbf{z} + C) = \\text{softmax}(\\mathbf{z})$ for any scalar $C$. This means the probabilities $\\mathbf{p}$ depend only on the *differences* between logits.\n\nWe define the correct-class logit margin as $\\Delta = z_c - \\max_{j \\ne c} z_j$. This margin measures how much more \"confident\" the model is in the correct class $c$ compared to the most likely incorrect class.\n\n**Analysis for One-Hot Targets ($\\varepsilon = 0$):**\n- **Large Positive Margin ($\\Delta \\to \\infty$):** If $z_c$ is much larger than all other $z_j$, the model is very confident in the correct class.\n    - $p_c = \\frac{e^{z_c}}{e^{z_c} + \\sum_{j \\ne c} e^{z_j}} = \\frac{1}{1 + \\sum_{j \\ne c} e^{z_j - z_c}} \\to 1$ since $z_j - z_c \\to -\\infty$.\n    - Consequently, $p_j \\to 0$ for $j \\ne c$.\n    - The gradient components become:\n        - $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1 \\to 1 - 1 = 0$.\n        - $(\\nabla_{\\mathbf{z}} L)_j = p_j \\to 0$.\n    - The entire gradient vector $\\nabla_{\\mathbf{z}} L$ approaches zero. This phenomenon is known as **gradient saturation**. The model receives a diminishing learning signal as it becomes more confident, effectively stopping learning for this example.\n\n- **Large Negative Margin ($\\Delta \\to -\\infty$):** If $z_c$ is much smaller than the largest incorrect logit, say $z_m = \\max_{j \\ne c} z_j$, the model is very confident in a wrong class.\n    - $p_c \\to 0$.\n    - The gradient for the correct class logit is $(\\nabla_{\\mathbf{z}} L)_c = p_c - 1 \\to -1$. This is a strong signal to increase $z_c$.\n    - The probability for the \"winning\" incorrect class $m$ approaches $1$, i.e., $p_m \\to 1$.\n    - The gradient for this incorrect logit is $(\\nabla_{\\mathbf{z}} L)_m = p_m \\to 1$. This is a strong signal to decrease $z_m$.\n    - The gradient magnitude is large, providing a strong corrective signal to the model.\n\n**Impact of Label Smoothing ($\\varepsilon > 0$):**\nLabel smoothing changes the target distribution, which in turn alters the gradient behavior, particularly in the high-confidence regime.\n- **Large Positive Margin ($\\Delta \\to \\infty$):** As before, $p_c \\to 1$ and $p_j \\to 0$ for $j \\ne c$. The gradient components become:\n    - $(\\nabla_{\\mathbf{z}} L)_c = p_c - (1 - \\varepsilon) \\to 1 - (1 - \\varepsilon) = \\varepsilon$.\n    - $(\\nabla_{\\mathbf{z}} L)_j = p_j - \\frac{\\varepsilon}{K-1} \\to 0 - \\frac{\\varepsilon}{K-1} = -\\frac{\\varepsilon}{K-1}$.\n- Unlike the one-hot case, the gradient components do not vanish.\n    - The gradient for the correct logit, $\\varepsilon$, is a small positive value. This penalizes overconfidence by encouraging $z_c$ to not grow infinitely larger than other logits. The target for $p_c$ is now $1-\\varepsilon$, not $1$.\n    - The gradients for incorrect logits are small negative values, encouraging their logits $z_j$ to increase slightly.\n- This persistent, non-zero gradient prevents the model from becoming excessively certain, acting as a regularizer that can improve generalization and model calibration.\n\n### 3. Implementation Details\n\nThe implementation computes the required metrics for each test case.\n1.  **Stable Softmax:** To prevent numerical overflow/underflow with large logits, we use the shift-invariance property by subtracting the maximum logit value from all logits before exponentiation: $p_i = \\frac{e^{z_i - \\max(\\mathbf{z})}}{\\sum_j e^{z_j - \\max(\\mathbf{z})}}$.\n2.  **Target Vector Construction:** A target vector $\\mathbf{y}$ is created based on the correct class index $c$, the number of classes $K$, and the smoothing parameter $\\varepsilon$.\n3.  **Gradient Calculation:** The gradient $\\nabla_{\\mathbf{z}} L$ is computed simply as $\\mathbf{p} - \\mathbf{y}$.\n4.  **Metric Computation:**\n    - The margin $\\Delta$ is calculated by finding the maximum logit among incorrect classes and subtracting it from the correct class logit.\n    - The Euclidean ($\\ell_2$) norm of the gradient vector is computed using `np.linalg.norm`.\n    - The absolute gradient at the correct class index and the maximum absolute gradient across all classes are found using `np.abs` and `np.max`.\n5.  **Output Formatting:** The results are formatted into a string that represents a list of lists, with each floating-point number formatted to exactly six decimal places, and no whitespace.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and computes gradient metrics for the Softmax Cross-Entropy loss\n    for a given suite of test cases.\n    \"\"\"\n    # Test suite: each case is a tuple (z, c, epsilon)\n    # z: logits vector (list of floats)\n    # c: correct class index (int)\n    # epsilon: label smoothing parameter (float)\n    test_cases = [\n        ([0.1, 0.2, 0.15, 0.05], 1, 0.0),\n        ([8.0, 1.0, 0.0, -2.0], 0, 0.0),\n        ([-2.0, 5.0, -1.0, 0.0], 0, 0.0),\n        ([1000.0, 1000.0, 1000.0, 1000.0], 2, 0.0),\n        ([8.0, 1.0, 0.0, -2.0], 0, 0.1),\n        ([-2.0, 5.0, -1.0, 0.0], 0, 0.1),\n    ]\n\n    all_results = []\n    \n    for z_list, c, epsilon in test_cases:\n        z = np.array(z_list, dtype=np.float64)\n        K = len(z)\n\n        # 1. Compute softmax probabilities (numerically stable)\n        z_max = np.max(z)\n        exp_z = np.exp(z - z_max)\n        p = exp_z / np.sum(exp_z)\n\n        # 2. Compute the gradient vector (grad_L = p - y)\n        if K > 1:\n            y = np.full(K, epsilon / (K - 1))\n        else: # Handle edge case of K=1, though not in tests\n            y = np.array([1.0])\n        y[c] = 1.0 - epsilon\n        \n        grad_L = p - y\n\n        # 3. Compute the correct-class margin Delta\n        if K > 1:\n            mask = np.ones(K, dtype=bool)\n            mask[c] = False\n            max_z_incorrect = np.max(z[mask])\n            delta = z[c] - max_z_incorrect\n        else: # Only one class, so margin is ill-defined, use 0\n            delta = 0.0\n\n        # 4. Compute the required gradient metrics\n        norm_grad_L = np.linalg.norm(grad_L)\n        abs_grad_c = np.abs(grad_L[c])\n        max_abs_grad = np.max(np.abs(grad_L))\n\n        # Store the four required floats\n        case_result = [delta, norm_grad_L, abs_grad_c, max_abs_grad]\n        all_results.append(case_result)\n\n    # Format the final output string exactly as required.\n    # e.g., [[val1,val2,...],[...],...] with no spaces and 6 decimal places.\n    inner_results_str = []\n    for result_vector in all_results:\n        formatted_values = [f'{v:.6f}' for v in result_vector]\n        inner_results_str.append(f\"[{','.join(formatted_values)}]\")\n\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A neural network's remarkable ability to model complex functions stems from its hidden units learning to extract diverse and hierarchical features from data. This practice explores the concept of permutation symmetry, a property of fully-connected layers where hidden neurons are interchangeable. You will demonstrate that without a symmetry-breaking mechanism—such as the randomness from mini-batch sampling or explicit noise—identically initialized hidden units will forever learn identical features, crippling the network's capacity. This exercise reveals the subtle yet critical role of stochasticity in training, explaining how a group of identical neurons can differentiate into a powerful ensemble of specialized feature detectors.",
            "id": "3134207",
            "problem": "You will study permutation symmetry among hidden units in a one-hidden-layer artificial neural network and quantify how different training dynamics preserve or break that symmetry. Consider a fully connected network with one hidden layer of width $n$, input dimension $d$, and a scalar output, defined by\n$$\nf_{\\theta}(x) \\;=\\; \\sum_{k=1}^{n} v_k \\,\\phi\\!\\left(w_k^{\\top} x + b_k\\right) + c,\n$$\nwhere $\\theta = \\{(w_k,b_k,v_k)_{k=1}^{n}, c\\}$ are the parameters, $w_k \\in \\mathbb{R}^{d}$, $b_k \\in \\mathbb{R}$, $v_k \\in \\mathbb{R}$, $c \\in \\mathbb{R}$, and the activation function is $\\phi(z) = \\tanh(z)$. The empirical risk on a dataset $\\{(x_i,y_i)\\}_{i=1}^{m}$ is the Mean Squared Error (MSE),\n$$\n\\mathcal{L}(\\theta) \\;=\\; \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta}(x_i) - y_i\\right)^2.\n$$\n\nFundamental base and definitions:\n- The permutation group $S_n$ acts on the hidden units by permuting their indices. A permutation $\\pi \\in S_n$ sends $(w_k,b_k,v_k)$ to $(w_{\\pi(k)}, b_{\\pi(k)}, v_{\\pi(k)})$ and leaves $c$ unchanged.\n- Gradient descent updates parameters by $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$ for a learning rate $\\eta > 0$. Stochastic Gradient Descent (SGD) uses unbiased estimators of the gradient computed on mini-batches rather than the full dataset.\n- Dropout multiplies hidden activations by independent Bernoulli masks. In inverted dropout, each hidden unit activation is multiplied by a random mask with keep probability $q \\in (0,1]$ and then rescaled by $1/q$ to keep its expectation unchanged.\n\nTasks:\n1) Using only the above core definitions, argue from first principles that for any $\\pi \\in S_n$ one has $\\mathcal{L}(\\theta) = \\mathcal{L}(\\theta^{\\pi})$ where $\\theta^{\\pi}$ is the parameter tuple after permuting hidden units by $\\pi$. Explain why if all hidden units are initialized identically, i.e., $(w_k,b_k,v_k) = (w_1,b_1,v_1)$ for all $k$, then full-batch gradient descent without noise preserves this equality at every step. Your argument must rely on the symmetry of the loss under $S_n$ and the chain rule, not on any shortcut formulas.\n\n2) Implement, from the definitions, a program that constructs a dataset and trains the network under four distinct training regimes to quantify feature diversification via a diversity metric. Use the following precise setup, which must be adhered to verbatim for reproducibility and testability.\n\n- Dataset: set $m=64$, $d=2$, generate inputs $x_i \\in \\mathbb{R}^2$ by $x_i \\sim \\mathcal{N}(0,I_2)$ with random seed $s_{\\text{data}}=2025$. Define targets by\n$$\ny_i \\;=\\; \\sin(x_{i1}) + 0.5 \\cos(2 x_{i2}),\n$$\nwhere $x_{i1}$ and $x_{i2}$ are the two coordinates of $x_i$.\n\n- Network initialization: set width $n=3$. Initialize $W \\in \\mathbb{R}^{n \\times d}$ so that each row is identical, $w_k^{\\top} = [0.1,\\,-0.2]$ for all $k \\in \\{1,2,3\\}$, set $b_k = 0$ for all $k$, set $v_k = 1/n$ for all $k$, and set $c = 0$.\n\n- Training objective and gradients: use the exact MSE loss above and compute all gradients via the chain rule,\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial v_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right),\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial c} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right),\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, v_k \\, \\left(1 - \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right)^2\\right) x_i,\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b_k} = \\frac{2}{B}\\sum_{i=1}^{B} \\left(f_{\\theta}(x_i) - y_i\\right) \\, v_k \\, \\left(1 - \\phi\\!\\left(w_k^{\\top} x_i + b_k\\right)^2\\right),\n$$\nwhere $B$ is the batch size. For dropout with keep probability $q$, replace $\\phi(z)$ by $\\tilde{\\phi}(z) = \\frac{m}{q}\\phi(z)$ where $m \\sim \\mathrm{Bernoulli}(q)$ independently per hidden unit and per sample, and use the same masked activations in backpropagation.\n\n- Feature diversity metric: after training, compute the mean pairwise angle among hidden weight vectors,\n$$\n\\Delta(W) \\;=\\; \\frac{1}{\\binom{n}{2}}\\sum_{1 \\leq i < j \\leq n} \\arccos\\!\\left(\\frac{w_i^{\\top} w_j}{\\|w_i\\|\\,\\|w_j\\|}\\right),\n$$\nwith the convention that if $\\|w_i\\|\\,\\|w_j\\|$ is below a numerical threshold (e.g., below $10^{-12}$), treat the corresponding angle as $0$. All angles must be expressed in radians.\n\n- Training regimes (test suite): run the following four cases, each with learning rate $\\eta = 0.05$ and the same dataset, but with different sources of stochasticity. Use the specified random seed $s_{\\text{case}}$ for any stochastic operation internal to each case (e.g., dropout masks, gradient noise, data shuffling).\n  - Case $1$ (symmetry preserved): full-batch gradient descent with batch size $B = m = 64$, no dropout (keep probability $q=1$), no noise, epochs $T=200$, random seed $s_{\\text{case}}=101$.\n  - Case $2$ (additive gradient noise): same as Case $1$ but at each update add independent zero-mean Gaussian noise with standard deviation $\\sigma=10^{-3}$ to every component of the parameter gradients before the update, epochs $T=200$, random seed $s_{\\text{case}}=102$.\n  - Case $3$ (stochastic gradient descent with dropout): mini-batch size $B=1$ (that is, pure Stochastic Gradient Descent (SGD)), apply inverted dropout on hidden activations with keep probability $q=0.5$ independently per sample and per hidden unit, epochs $T=200$, random seed $s_{\\text{case}}=103$.\n  - Case $4$ (explicit infinitesimal symmetry breaking): same as Case $1$ except perturb the first hidden weight vector at initialization by adding $\\varepsilon$ to its first coordinate, with $\\varepsilon = 10^{-3}$, epochs $T=200$, random seed $s_{\\text{case}}=104$.\n\nYour program must:\n- Implement the above training dynamics exactly as specified, including seeding.\n- After each case, compute $\\Delta(W)$ in radians.\n- Produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, for example, $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$, where each $\\Delta_k$ is rounded to exactly $6$ decimal places. Angles must be in radians.\n\nFinal output format:\n- A single line with the list $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$, where each entry is a float with exactly $6$ digits after the decimal point, representing the mean pairwise angle in radians for Cases $1$ through $4$, in that order.",
            "solution": "The problem is well-posed, scientifically grounded in the fundamentals of neural networks and optimization, and provides a complete, unambiguous specification for a numerical experiment. It is therefore deemed valid.\n\nThe solution consists of two parts as requested: a theoretical argument regarding permutation symmetry, and the implementation of a numerical experiment.\n\n### Part 1: Theoretical Argument on Permutation Symmetry\n\n**1.1. Invariance of the Loss Function under Permutation**\n\nThe network's output is defined as $f_{\\theta}(x) = \\sum_{k=1}^{n} v_k \\phi(w_k^{\\top} x + b_k) + c$, where $\\theta$ represents the full set of parameters $\\{(w_k, b_k, v_k)_{k=1}^{n}, c\\}$. The activation function is $\\phi(z) = \\tanh(z)$.\n\nA permutation $\\pi \\in S_n$ acts on the hidden units by re-indexing their parameters. The permuted parameter set is $\\theta^{\\pi} = \\{(w_{\\pi(k)}, b_{\\pi(k)}, v_{\\pi(k)})_{k=1}^{n}, c\\}$. The network function with these permuted parameters is:\n$$ f_{\\theta^{\\pi}}(x) = \\sum_{k=1}^{n} v_{\\pi(k)} \\phi\\left(w_{\\pi(k)}^{\\top} x + b_{\\pi(k)}\\right) + c $$\nLet $j = \\pi(k)$. As $k$ iterates from $1$ to $n$, $j$ also covers all indices from $1$ to $n$ because $\\pi$ is a permutation. Therefore, the summation is merely a reordering of its terms, which does not change the sum's value due to the commutativity of addition:\n$$ f_{\\theta^{\\pi}}(x) = \\sum_{j \\in \\{\\pi(1), \\dots, \\pi(n)\\}} v_j \\phi\\left(w_j^{\\top} x + b_j\\right) + c = \\sum_{j=1}^{n} v_j \\phi\\left(w_j^{\\top} x + b_j\\right) + c = f_{\\theta}(x) $$\nSince the function's output $f_{\\theta}(x)$ is identical for any $\\theta$ and its permutation $\\theta^{\\pi}$, the Mean Squared Error (MSE) loss function $\\mathcal{L}(\\theta)$ is also invariant under this permutation:\n$$ \\mathcal{L}(\\theta^{\\pi}) = \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta^{\\pi}}(x_i) - y_i\\right)^2 = \\frac{1}{m} \\sum_{i=1}^{m} \\left(f_{\\theta}(x_i) - y_i\\right)^2 = \\mathcal{L}(\\theta) $$\nThis confirms that the loss function possesses $S_n$ permutation symmetry.\n\n**1.2. Preservation of Symmetry by Full-Batch Gradient Descent**\n\nWe now argue that if all hidden units are initialized identically, this symmetry is preserved at every step of full-batch gradient descent. We use an inductive argument.\n\n*Base Case (Initialization):* The parameters are initialized such that $(w_k^{(0)}, b_k^{(0)}, v_k^{(0)}) = (\\mathbf{w}_0, \\mathbf{b}_0, \\mathbf{v}_0)$ for all $k \\in \\{1, \\dots, n\\}$. The symmetry holds at step $t=0$.\n\n*Inductive Step:* Assume at some step $t$, the parameters for all hidden units are identical: $(w_k^{(t)}, b_k^{(t)}, v_k^{(t)}) = (\\mathbf{w}_t, \\mathbf{b}_t, \\mathbf{v}_t)$ for all $k$. We must show that the parameters remain identical after one gradient descent step, i.e., $(w_k^{(t+1)}, b_k^{(t+1)}, v_k^{(t+1)})$ is the same for all $k$.\n\nThe gradient descent update rule for a parameter $\\vartheta$ is $\\vartheta^{(t+1)} = \\vartheta^{(t)} - \\eta \\nabla_{\\vartheta} \\mathcal{L}(\\theta^{(t)})$. We need to show that the gradients with respect to the parameters of each hidden unit are identical.\n\nLet's examine the gradient components for an arbitrary hidden unit $k$. For full-batch gradient descent, $B=m$.\nThe gradient with respect to the output weight $v_k$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial v_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right) $$\nUnder the inductive hypothesis, $w_k^{(t)} = \\mathbf{w}_t$ and $b_k^{(t)} = \\mathbf{b}_t$ for all $k$. This means the term $\\phi(\\dots)$ is identical for all units. Furthermore, the function value $f_{\\theta^{(t)}}(x_i) = \\sum_{j=1}^n v_j^{(t)} \\phi((w_j^{(t)})^{\\top} x_i + b_j^{(t)}) + c^{(t)}$ is also independent of the choice of unit index $k$, as all units contribute identically to the sum. Thus, the entire expression for $\\frac{\\partial \\mathcal{L}}{\\partial v_k}$ is the same for all $k$.\n\nThe gradient with respect to the input weights $w_k$ is:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial w_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) v_k^{(t)} \\left(1 - \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right)^2\\right) x_i $$\nBy the inductive hypothesis, $v_k^{(t)} = \\mathbf{v}_t$, $w_k^{(t)} = \\mathbf{w}_t$, and $b_k^{(t)} = \\mathbf{b}_t$. As before, $f_{\\theta^{(t)}}(x_i)$ is common. All terms within the summation are therefore identical for any choice of $k$. Thus, $\\frac{\\partial \\mathcal{L}}{\\partial w_k}$ is the same for all $k$.\n\nA similar argument holds for the bias term gradient:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_k} = \\frac{2}{m}\\sum_{i=1}^{m} \\left(f_{\\theta^{(t)}}(x_i) - y_i\\right) v_k^{(t)} \\left(1 - \\phi\\left((w_k^{(t)})^{\\top} x_i + b_k^{(t)}\\right)^2\\right) $$\nThis gradient is also identical for all $k$.\n\nSince the gradients $(\\nabla_{w_k}\\mathcal{L}, \\nabla_{b_k}\\mathcal{L}, \\nabla_{v_k}\\mathcal{L})$ are identical for all units $k$, and the parameters $(w_k^{(t)}, b_k^{(t)}, v_k^{(t)})$ are identical, the update step\n$$ (w_k^{(t+1)}, b_k^{(t+1)}, v_k^{(t+1)}) = (w_k^{(t)}, b_k^{(t)}, v_k^{(t)}) - \\eta (\\nabla_{w_k}\\mathcal{L}, \\nabla_{b_k}\\mathcal{L}, \\nabla_{v_k}\\mathcal{L}) $$\nproduces a new set of parameters that is also identical for all units.\n\nBy induction, starting with identical parameters, full-batch gradient descent maintains this symmetry across all training steps. This leads to all hidden units learning the exact same feature, and the weight vectors $w_k$ remain collinear (in this case, identical), resulting in a diversity metric $\\Delta(W) = 0$. Any deviation from this symmetric setup (e.g., stochastic gradients, gradient noise, dropout, or non-identical initialization) will break the symmetry and cause the hidden units to diversify.\n\n### Part 2: Implementation\n\nThe following program implements the four specified training regimes and calculates the feature diversity metric $\\Delta(W)$ for each.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants from problem\nM = 64\nD = 2\nN = 3\nETA = 0.05\nEPOCHS = 200\nNUM_THRESHOLD = 1e-12\n\ndef phi(z):\n    \"\"\"Tanh activation function.\"\"\"\n    return np.tanh(z)\n\ndef phi_prime(z):\n    \"\"\"Derivative of the tanh activation function.\"\"\"\n    return 1 - np.tanh(z)**2\n\ndef calculate_diversity(w_matrix):\n    \"\"\"\n    Computes the mean pairwise angle between rows of the weight matrix W.\n    \"\"\"\n    total_angle = 0.0\n    num_pairs = 0\n    for i in range(N):\n        for j in range(i + 1, N):\n            w_i = w_matrix[i]\n            w_j = w_matrix[j]\n            \n            norm_i = np.linalg.norm(w_i)\n            norm_j = np.linalg.norm(w_j)\n            \n            denominator = norm_i * norm_j\n            if denominator < NUM_THRESHOLD:\n                angle = 0.0\n            else:\n                dot_product = np.dot(w_i, w_j)\n                # Clip for numerical stability of arccos\n                cosine_sim = np.clip(dot_product / denominator, -1.0, 1.0)\n                angle = np.arccos(cosine_sim)\n            \n            total_angle += angle\n            num_pairs += 1\n            \n    return total_angle / num_pairs if num_pairs > 0 else 0.0\n\ndef run_case(case_num, seed, batch_size, q, noise_std, initial_perturbation, X, Y):\n    \"\"\"\n    Runs a single training experiment as specified by the case parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Initialize parameters\n    W = np.array([[0.1, -0.2]] * N, dtype=np.float64)\n    b = np.zeros(N, dtype=np.float64)\n    v = np.ones(N, dtype=np.float64) / N\n    c = 0.0\n\n    # Apply perturbation for Case 4\n    if initial_perturbation > 0:\n        W[0, 0] += initial_perturbation\n    \n    indices = np.arange(M)\n\n    for epoch in range(EPOCHS):\n        if batch_size < M:  # For SGD, shuffle data each epoch\n            rng.shuffle(indices)\n\n        for i in range(0, M, batch_size):\n            batch_indices = indices[i:i + batch_size]\n            X_batch = X[batch_indices]\n            Y_batch = Y[batch_indices]\n            \n            B = X_batch.shape[0]\n\n            # Forward pass\n            Z = X_batch @ W.T + b\n            A_pre = phi(Z)\n            \n            dropout_mask = None\n            if q < 1.0:\n                dropout_mask = rng.binomial(1, q, size=A_pre.shape)\n                A_post = A_pre * dropout_mask / q\n            else:\n                A_post = A_pre\n\n            Y_pred = A_post @ v + c\n            \n            # Backward pass (compute gradients)\n            error_term = (2 / B) * (Y_pred - Y_batch)  # shape (B,)\n            \n            grad_c = np.sum(error_term)\n            grad_v = A_post.T @ error_term  # shape (n,)\n            \n            delta_out = error_term[:, np.newaxis] * v  # shape (B, n)\n            \n            dZ = delta_out * (1 - A_pre**2) # phi_prime(Z) is 1-phi(Z)^2 = 1-A_pre^2\n            \n            if q < 1.0:\n                dZ = dZ * dropout_mask / q\n            \n            grad_b = np.sum(dZ, axis=0)  # shape (n,)\n            grad_W = dZ.T @ X_batch      # shape (n, d)\n\n            # Add gradient noise for Case 2\n            if noise_std > 0:\n                grad_W += rng.normal(0, noise_std, size=grad_W.shape)\n                grad_b += rng.normal(0, noise_std, size=grad_b.shape)\n                grad_v += rng.normal(0, noise_std, size=grad_v.shape)\n                grad_c += rng.normal(0, noise_std)\n\n            # Update parameters\n            W -= ETA * grad_W\n            b -= ETA * grad_b\n            v -= ETA * grad_v\n            c -= ETA * grad_c\n\n    return calculate_diversity(W)\n\ndef solve():\n    # Dataset generation\n    data_rng = np.random.default_rng(2025)\n    X = data_rng.normal(0, 1, size=(M, D))\n    Y = np.sin(X[:, 0]) + 0.5 * np.cos(2 * X[:, 1])\n\n    # Test suite definition\n    test_cases = [\n        # (case_num, seed, batch_size, q, noise_std, initial_perturbation)\n        (1, 101, M, 1.0, 0.0, 0.0),            # Case 1: Full-batch GD\n        (2, 102, M, 1.0, 1e-3, 0.0),           # Case 2: Gradient noise\n        (3, 103, 1, 0.5, 0.0, 0.0),            # Case 3: SGD with dropout\n        (4, 104, M, 1.0, 0.0, 1e-3),           # Case 4: Initial perturbation\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_case(*case_params, X, Y)\n        results.append(result)\n\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A central question in the theory of machine learning is whether a model is truly learning the underlying patterns in a dataset or simply memorizing the examples it has seen. The teacher-student framework provides a controlled environment to investigate this, where a \"student\" network is trained on data generated by a known \"teacher\" network. By implementing this simulation , you will explore fundamental questions of learnability, including the impact of sample size ($m$), data noise, and model overparameterization on the student's ability to successfully recover the teacher's hidden functional components.",
            "id": "3134222",
            "problem": "You are given a teacher–student setup for a two-layer Artificial Neural Network (ANN), where the teacher is known and the student attempts to recover the teacher’s parameters from data generated by the teacher. The teacher has one hidden layer with the hyperbolic tangent activation function. The student has the same architecture but possibly a different hidden width to model overparameterization. Your task is to implement a complete program that generates data from the teacher, trains the student by minimizing the empirical mean squared error, and then evaluates whether the student has recovered the teacher’s hidden units up to permutation under varying sample sizes and widths.\n\nStart from the following fundamental bases:\n- An artificial neuron with parameter vector $w \\in \\mathbb{R}^d$ and input $x \\in \\mathbb{R}^d$ produces output $\\sigma(w^\\top x)$ for a smooth nonlinearity $\\sigma$. Here, use the hyperbolic tangent activation $\\sigma(z) = \\tanh(z)$.\n- A two-layer network with hidden width $H$ and scalar output computes $f(x) = \\sum_{j=1}^{H} a_j \\sigma(w_j^\\top x)$, where $\\{w_j\\}_{j=1}^{H}$ are hidden-layer weights and $\\{a_j\\}_{j=1}^{H}$ are output-layer weights.\n- Empirical risk minimization with mean squared error defines the objective $L(W,a) = \\frac{1}{m} \\sum_{k=1}^{m} \\left(f_S(x_k;W,a) - y_k\\right)^2$, where $\\{(x_k,y_k)\\}_{k=1}^{m}$ is the training set, $W = [w_1,\\dots,w_{H_s}]^\\top$ collects hidden-layer weights in the student network, and $a = [a_1,\\dots,a_{H_s}]^\\top$ collects output-layer weights.\n- Full-batch gradient descent updates parameters by following the negative gradient of $L(W,a)$ with respect to $W$ and $a$.\n\nTeacher network and data generation:\n- Fix input dimension $d \\in \\mathbb{N}$ and teacher hidden width $H_t \\in \\mathbb{N}$.\n- The teacher function is $f_T(x) = \\sum_{j=1}^{H_t} v_j \\tanh(u_j^\\top x)$, with teacher hidden weights $\\{u_j \\in \\mathbb{R}^d\\}_{j=1}^{H_t}$ and output weights $\\{v_j \\in \\mathbb{R}\\}_{j=1}^{H_t}$. The training inputs are independently drawn as $x_k \\sim \\mathcal{N}(0,I_d)$ for $k=1,\\dots,m$. The outputs are $y_k = f_T(x_k) + \\epsilon_k$, with $\\epsilon_k \\sim \\mathcal{N}(0,\\sigma^2)$ independent noise.\n- The student function is $f_S(x;W,a) = \\sum_{i=1}^{H_s} a_i \\tanh(w_i^\\top x)$, where $H_s \\in \\mathbb{N}$ is the student hidden width.\n\nTraining:\n- Initialize the student parameters $(W,a)$ randomly with small values.\n- Perform $T$ steps of full-batch gradient descent with learning rate $\\eta$ to minimize $L(W,a)$ on the training data $\\{(x_k,y_k)\\}_{k=1}^{m}$.\n- Use the chain rule to derive the gradients with respect to $W$ and $a$.\n\nRecovery criterion:\n- Because hidden units are only identifiable up to permutation, define a permutation-invariant matching between teacher and student hidden units using a validation set $\\{x^\\text{(val)}_\\ell\\}_{\\ell=1}^{m_\\text{val}}$ drawn i.i.d. from $\\mathcal{N}(0,I_d)$.\n- For each teacher unit $j \\in \\{1,\\dots,H_t\\}$ and student unit $i \\in \\{1,\\dots,H_s\\}$, define the contribution vectors on validation inputs:\n  $$c_{T,j} = \\left[v_j \\tanh\\left(u_j^\\top x^\\text{(val)}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}}, \\quad c_{S,i} = \\left[a_i \\tanh\\left(w_i^\\top x^\\text{(val)}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}}.$$\n- Define the pairwise cost\n  $$C_{j,i} = \\frac{1}{m_\\text{val}} \\sum_{\\ell=1}^{m_\\text{val}} \\left(c_{S,i}(\\ell) - c_{T,j}(\\ell)\\right)^2.$$\n- Find a minimum-cost assignment of teacher units to student units using a linear assignment procedure that minimizes the sum of $C_{j,i}$ over chosen pairs. If $H_s < H_t$, some teacher units will remain unmatched; penalize each unmatched teacher unit by adding a fixed cost $P > 0$.\n- Let $J^\\star$ be the set of matched teacher–student pairs. Define the average per-teacher-unit matching cost\n  $$\\overline{C} = \\frac{1}{H_t} \\left( \\sum_{(j,i)\\in J^\\star} C_{j,i} + P \\cdot \\left(H_t - |J^\\star|\\right) \\right).$$\n- Declare successful recovery if $\\overline{C} \\leq \\tau$ for a fixed threshold $\\tau > 0$.\n\nStudy the effect of sample complexity:\n- Vary the number of samples $m$ and the student width $H_s$ relative to the teacher width $H_t$ and evaluate whether recovery occurs. Overparameterization corresponds to $H_s > H_t$.\n\nRequired program behavior:\n- Implement the full pipeline: teacher generation, data generation, student training by gradient descent, validation set generation, permutation-invariant matching via assignment, and recovery decision.\n- Use $m_\\text{val} = 512$ validation inputs.\n- Use the hyperparameters $T = 2000$ training steps, learning rate $\\eta = 0.05$, unmatched penalty $P = 2.0$, and recovery threshold $\\tau = 0.12$. Include a small $\\ell_2$ regularization on $W$ of strength $\\lambda_w = 10^{-4}$ during training to stabilize optimization.\n- Set all random number generators to fixed seeds for reproducibility as specified in the test suite below.\n\nTest suite:\nEvaluate the program on the following five cases. Each case is a tuple $(d,H_t,H_s,m,\\sigma,\\text{seed})$:\n- Case $1$: $(5,3,3,2000,0.0,12345)$, a well-specified student with many samples.\n- Case $2$: $(5,3,2,2000,0.0,22345)$, an underparameterized student with many samples.\n- Case $3$: $(5,3,6,800,0.0,32345)$, an overparameterized student with a moderate sample size.\n- Case $4$: $(5,3,3,50,0.1,42345)$, a small-sample, noisy scenario.\n- Case $5$: $(5,3,3,200,0.02,52345)$, a moderate sample size with slight noise.\n\nAnswer specification:\n- For each test case, output a boolean indicating whether recovery occurs according to the criterion $\\overline{C} \\leq \\tau$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above. For example, the output format must be like $[b_1,b_2,b_3,b_4,b_5]$, where each $b_k$ is either $True$ or $False$.",
            "solution": "The problem requires the implementation and evaluation of a teacher-student framework for a two-layer artificial neural network. The objective is to determine whether a student network, trained via gradient descent, can recover the functional components of a known teacher network under various conditions of model size, sample size, and noise.\n\nThe core of the problem involves the following sequence of steps:\n1.  Defining the teacher and student network architectures.\n2.  Generating a synthetic dataset from the teacher network.\n3.  Training the student network on this dataset by minimizing a regularized mean squared error loss.\n4.  Evaluating the recovery of the teacher's hidden units by the student's hidden units using a permutation-invariant matching criterion.\n\nLet us formally specify each component.\n\n**1. Network Architecture and Data Generation**\n\nThe teacher and student networks are two-layer neural networks with a single hidden layer and a scalar output. The activation function for the hidden neurons is the hyperbolic tangent, $\\sigma(z) = \\tanh(z)$.\n\nThe teacher network function, $f_T(x)$, with input dimension $d$ and hidden width $H_t$, is given by:\n$$ f_T(x) = \\sum_{j=1}^{H_t} v_j \\tanh(u_j^\\top x) $$\nwhere $\\{u_j \\in \\mathbb{R}^d\\}_{j=1}^{H_t}$ are the teacher's hidden-layer weight vectors and $\\{v_j \\in \\mathbb{R}\\}_{j=1}^{H_t}$ are the teacher's output-layer weights. These parameters are fixed and considered the ground truth.\n\nThe training dataset $\\{(x_k, y_k)\\}_{k=1}^{m}$ consists of $m$ samples. The inputs $x_k$ are drawn independently from a standard normal distribution, $x_k \\sim \\mathcal{N}(0, I_d)$. The corresponding outputs $y_k$ are generated by the teacher network with additive Gaussian noise:\n$$ y_k = f_T(x_k) + \\epsilon_k, \\quad \\text{where} \\quad \\epsilon_k \\sim \\mathcal{N}(0, \\sigma^2) $$\n\nThe student network has the same architecture but a potentially different hidden width, $H_s$. Its function, $f_S(x)$, is:\n$$ f_S(x; W, a) = \\sum_{i=1}^{H_s} a_i \\tanh(w_i^\\top x) $$\nHere, $W = [w_1, \\dots, w_{H_s}]^\\top$ is the matrix of student hidden-layer weights ($W \\in \\mathbb{R}^{H_s \\times d}$), and $a = [a_1, \\dots, a_{H_s}]^\\top$ is the vector of student output-layer weights ($a \\in \\mathbb{R}^{H_s}$). These are the parameters that will be learned during training.\n\n**2. Training via Gradient Descent**\n\nThe student network is trained by minimizing the empirical mean squared error, with an added $\\ell_2$ regularization term on the hidden weights to improve stability. The loss function, $L(W, a)$, is:\n$$ L(W, a) = \\frac{1}{m} \\sum_{k=1}^{m} \\left(f_S(x_k; W, a) - y_k\\right)^2 + \\frac{\\lambda_w}{2} \\|W\\|_F^2 $$\nwhere $\\|W\\|_F^2 = \\sum_{i=1}^{H_s} \\|w_i\\|_2^2$ is the squared Frobenius norm of the weight matrix $W$, and $\\lambda_w$ is the regularization strength.\n\nTraining is performed using full-batch gradient descent for $T$ iterations. The parameters are updated according to the rules:\n$$ a^{(t+1)} = a^{(t)} - \\eta \\nabla_a L_t $$\n$$ W^{(t+1)} = W^{(t)} - \\eta \\nabla_W L_t $$\nwhere $\\eta$ is the learning rate and the gradients are evaluated at the current parameter values $(W^{(t)}, a^{(t)})$.\n\nThe gradients are derived using the chain rule. The derivative of the activation function is $\\sigma'(z) = \\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$.\nLet $\\delta_k = f_S(x_k) - y_k$ be the prediction error for sample $k$.\n\nThe gradient with respect to an output weight $a_i$ is:\n$$ \\frac{\\partial L}{\\partial a_i} = \\frac{1}{m} \\sum_{k=1}^{m} 2 \\delta_k \\frac{\\partial f_S(x_k)}{\\partial a_i} = \\frac{2}{m} \\sum_{k=1}^{m} \\delta_k \\tanh(w_i^\\top x_k) $$\n\nThe gradient with respect to a hidden weight vector $w_i$ is:\n$$ \\frac{\\partial L}{\\partial w_i} = \\left( \\frac{1}{m} \\sum_{k=1}^{m} 2 \\delta_k \\frac{\\partial f_S(x_k)}{\\partial w_i} \\right) + \\frac{\\partial}{\\partial w_i}\\left(\\frac{\\lambda_w}{2} \\sum_{j=1}^{H_s} w_j^\\top w_j\\right) $$\n$$ \\frac{\\partial L}{\\partial w_i} = \\left( \\frac{2}{m} \\sum_{k=1}^{m} \\delta_k a_i \\left(1 - \\tanh^2(w_i^\\top x_k)\\right) x_k \\right) + \\lambda_w w_i $$\n\nThese expressions can be efficiently computed using vectorized operations on the entire dataset.\n\n**3. Recovery Evaluation Criterion**\n\nThe hidden units of a neural network are identifiable only up to permutation and sign flips. For example, changing $(a_i, w_i)$ to $(-a_i, -w_i)$ leaves the term $a_i \\tanh(w_i^\\top x)$ unchanged, as $\\tanh$ is an odd function. Also, reordering the hidden units does not change the network's output. To account for this, recovery is assessed using a permutation-invariant matching procedure.\n\nA validation set $\\{x^{(\\text{val})}_\\ell\\}_{\\ell=1}^{m_\\text{val}}$ of size $m_\\text{val}$ is drawn from $\\mathcal{N}(0, I_d)$. For each teacher unit $j$ and student unit $i$, we compute their functional contributions on this set:\n$$ c_{T,j} = \\left[v_j \\tanh\\left(u_j^\\top x^{(\\text{val})}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}} \\in \\mathbb{R}^{m_\\text{val}} $$\n$$ c_{S,i} = \\left[a_i \\tanh\\left(w_i^\\top x^{(\\text{val})}_\\ell\\right)\\right]_{\\ell=1}^{m_\\text{val}} \\in \\mathbb{R}^{m_\\text{val}} $$\n\nA pairwise cost matrix $C \\in \\mathbb{R}^{H_t \\times H_s}$ is defined, where each entry $C_{j,i}$ is the mean squared error between the contribution vectors of teacher unit $j$ and student unit $i$:\n$$ C_{j,i} = \\frac{1}{m_\\text{val}} \\sum_{\\ell=1}^{m_\\text{val}} \\left(c_{S,i}(\\ell) - c_{T,j}(\\ell)\\right)^2 $$\n\nThe problem of finding the best matching between teacher and student units corresponds to a linear assignment problem (or minimum weight bipartite matching). The goal is to find a set of pairs $(j,i)$ that minimizes the total cost $\\sum C_{j,i}$. This is solved efficiently using algorithms like the Hungarian method, available in `scipy.optimize.linear_sum_assignment`.\n\nLet $J^\\star$ be the set of optimal-cost pairs found. The number of matched pairs is $|J^\\star| = \\min(H_t, H_s)$. If the student is underparameterized ($H_s < H_t$), some teacher units will be unmatched. A penalty $P$ is added for each of the $H_t - |J^\\star|$ unmatched teacher units.\n\nThe average per-teacher-unit matching cost, $\\overline{C}$, is then calculated as:\n$$ \\overline{C} = \\frac{1}{H_t} \\left( \\sum_{(j,i)\\in J^\\star} C_{j,i} + P \\cdot \\left(H_t - |J^\\star|\\right) \\right) $$\n\nFinally, successful recovery is declared if this average cost is below a given threshold $\\tau$:\n$$ \\text{Recovery} = (\\overline{C} \\leq \\tau) $$\n\nThe full implementation will execute this entire pipeline for each specified test case, using fixed random seeds for reproducibility, and report the binary recovery outcome.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to run the teacher-student ANN simulation for all test cases.\n    \"\"\"\n    # Test cases: (d, H_t, H_s, m, sigma, seed)\n    test_cases = [\n        (5, 3, 3, 2000, 0.0, 12345),\n        (5, 3, 2, 2000, 0.0, 22345),\n        (5, 3, 6, 800, 0.0, 32345),\n        (5, 3, 3, 50, 0.1, 42345),\n        (5, 3, 3, 200, 0.02, 52345),\n    ]\n\n    # Hyperparameters\n    m_val = 512\n    T = 2000\n    eta = 0.05\n    P = 2.0\n    tau = 0.12\n    lambda_w = 1e-4\n\n    results = []\n\n    for d, H_t, H_s, m, sigma, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Teacher network and data generation\n        # Teacher parameters\n        U_teacher = rng.standard_normal(size=(H_t, d))\n        v_teacher = rng.standard_normal(size=H_t)\n\n        # Training data\n        X_train = rng.standard_normal(size=(m, d))\n        teacher_activations = np.tanh(X_train @ U_teacher.T)\n        y_true = teacher_activations @ v_teacher\n        noise = rng.normal(scale=sigma, size=m)\n        y_train = y_true + noise\n\n        # 2. Student network training\n        # Student parameters initialization\n        # Initialize with small random values to break symmetry but stay in a reasonable regime\n        W_student = rng.normal(scale=0.1, size=(H_s, d))\n        a_student = rng.normal(scale=0.1, size=H_s)\n\n        # Full-batch gradient descent loop\n        for _ in range(T):\n            # Forward pass\n            student_pre_activations = X_train @ W_student.T\n            student_activations = np.tanh(student_pre_activations)\n            y_pred = student_activations @ a_student\n\n            # Compute error\n            error = y_pred - y_train\n\n            # Compute gradients\n            grad_a = (2 / m) * (student_activations.T @ error)\n            \n            # The gradient for W requires careful vectorization\n            # term_for_w_grad has shape (m, H_s)\n            term_for_w_grad = (2 / m) * (error[:, np.newaxis] * a_student[np.newaxis, :]) * (1 - student_activations**2)\n            grad_W = term_for_w_grad.T @ X_train + lambda_w * W_student\n\n            # Update parameters\n            a_student -= eta * grad_a\n            W_student -= eta * grad_W\n\n        # 3. Recovery criterion evaluation\n        # Generate validation data\n        X_val = rng.standard_normal(size=(m_val, d))\n        \n        # Compute contribution vectors for teacher and student units\n        contrib_teacher = np.tanh(X_val @ U_teacher.T) * v_teacher[np.newaxis, :]\n        contrib_student = np.tanh(X_val @ W_student.T) * a_student[np.newaxis, :]\n        \n        # Compute pairwise cost matrix C\n        # Using broadcasting to avoid loops\n        # C_si shape: (m_val, 1, H_s)\n        # C_tj shape: (m_val, H_t, 1)\n        # diff shape: (m_val, H_t, H_s) after broadcasting\n        diff_sq = (contrib_student[:, np.newaxis, :] - contrib_teacher[:, :, np.newaxis])**2\n        cost_matrix = np.mean(diff_sq, axis=0) # shape (H_t, H_s)\n\n        # Find minimum-cost assignment\n        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n        \n        # Calculate total and average cost\n        matched_cost = cost_matrix[row_ind, col_ind].sum()\n        num_matched = len(row_ind)\n        num_unmatched_teachers = H_t - num_matched\n        \n        penalty_cost = P * num_unmatched_teachers\n        total_cost = matched_cost + penalty_cost\n        avg_cost = total_cost / H_t\n        \n        # Check for recovery\n        recovery_success = avg_cost = tau\n        results.append(recovery_success)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\".replace(\"True\", \"true\").replace(\"False\", \"false\"))\n\nsolve()\n```"
        }
    ]
}