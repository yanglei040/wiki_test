{
    "hands_on_practices": [
        {
            "introduction": "Regularization is fundamentally about managing a trade-off between how well a model fits the training data and how complex the model is. This exercise moves beyond viewing the regularization parameter $\\lambda$ as just a hyperparameter to tune. Instead, you will treat model fitting and regularization as two separate objectives to visualize the inherent tension between them . By implementing Ridge regression and plotting its performance in a two-dimensional objective space, you will construct a Pareto front that clearly illustrates this foundational trade-off, offering a more principled view of model selection.",
            "id": "3168619",
            "problem": "You will study the biasâ€“variance trade-off through a two-objective view of linear models with quadratic regularization. Work within the following mathematical framework grounded in the core definition of Empirical Risk Minimization: a linear predictor with parameters $w \\in \\mathbb{R}^p$ is trained by minimizing the sum of the empirical squared error and a quadratic regularization penalty. For each regularization strength, consider the two objectives separately: training mean squared error and the value of the regularization penalty term itself. The goal is to construct a discrete approximation of the trade-off curve across a set of regularization strengths and identify Pareto-optimal models.\n\nDefinitions you must use:\n- The training data matrix is $X \\in \\mathbb{R}^{n \\times p}$ and the response vector is $y \\in \\mathbb{R}^n$.\n- A linear model predicts $\\hat{y} = X w$ for $w \\in \\mathbb{R}^p$.\n- The training Mean Squared Error (MSE) is $E_{\\text{train}}(w) = \\frac{1}{n}\\lVert X w - y \\rVert_2^2$.\n- The quadratic (ridge) regularization penalty term value for a given regularization strength $\\lambda > 0$ is $P_\\lambda(w) = \\lambda \\lVert w \\rVert_2^2$.\n- For a fixed dataset and a fixed list of $\\lambda$ values, training a separate model for each $\\lambda$ yields a set of pairs $(E_{\\text{train}}(w_\\lambda), P_\\lambda(w_\\lambda))$. Consider these as points in the objective space.\n- A model $i$ with objective pair $(E_i, P_i)$ is Pareto dominated by a model $j$ with objective pair $(E_j, P_j)$ if and only if $E_j \\le E_i$ and $P_j \\le P_i$ with at least one of the two inequalities being strict. A model is Pareto optimal if it is not dominated by any other model in the set.\n\nYour program must, for each test case defined below, do the following:\n1. Generate a synthetic dataset using the stated generative model.\n2. For each given $\\lambda$ in the test case, train the regularized linear model and compute the pair $(E_{\\text{train}}(w_\\lambda), P_\\lambda(w_\\lambda))$.\n3. Determine the set of indices (zero-based, aligned with the order of $\\lambda$ values listed) that are Pareto optimal.\n4. Count how many models are dominated.\n\nData generation protocol (deterministic and scientifically sound):\n- For a given seed $s$, dimension $p$, and sample size $n$, draw the design matrix $X \\in \\mathbb{R}^{n \\times p}$ with entries independently from the Normal distribution with mean $0$ and variance $1$, denoted $\\mathcal{N}(0,1)$. The draws must be Independent and Identically Distributed (i.i.d.).\n- Draw the ground-truth parameter vector $w^\\star \\in \\mathbb{R}^p$ with entries i.i.d. from $\\mathcal{N}(0,1)$ using the same seed $s$ but advanced deterministically so that $X$ and $w^\\star$ are not identical draws.\n- Draw noise $\\epsilon \\in \\mathbb{R}^n$ with entries i.i.d. from $\\mathcal{N}(0,\\sigma^2)$, for the specified $\\sigma$.\n- Form the response as $y = X w^\\star + \\epsilon$.\n\nImportant implementation notes:\n- All regularization strengths $\\lambda$ will be strictly positive. Do not include $\\lambda = 0$.\n- Your algorithm for identifying Pareto-optimal points must adhere to the definition above. In floating-point arithmetic, use a reasonable small tolerance when comparing real numbers to mitigate spurious dominance caused by numerical roundoff, but preserve strict improvement in at least one coordinate.\n\nTest suite to cover typical and edge scenarios:\n- Test case A (well-posed, low noise):\n  - Seed $s = 7$\n  - Samples $n = 40$\n  - Features $p = 6$\n  - Noise standard deviation $\\sigma = 10^{-1}$\n  - Regularization strengths (in order): $\\lambda \\in \\{10^{-6}, 10^{-4}, 10^{-2}, 10^{-1}, 1, 10\\}$\n- Test case B (underdetermined, very low noise):\n  - Seed $s = 101$\n  - Samples $n = 20$\n  - Features $p = 50$\n  - Noise standard deviation $\\sigma = 5 \\times 10^{-2}$\n  - Regularization strengths (in order): $\\lambda \\in \\{10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1\\}$\n- Test case C (well-posed, high noise):\n  - Seed $s = 2023$\n  - Samples $n = 50$\n  - Features $p = 10$\n  - Noise standard deviation $\\sigma = 1$\n  - Regularization strengths (in order): $\\lambda \\in \\{10^{-5}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10^2\\}$\n\nRequired final output format:\n- Your program should produce a single line of output containing a list with one element per test case, in the same order A, B, C.\n- For each test case, output a list with two entries:\n  1. A list of the zero-based indices (integers) of the Pareto-optimal models, sorted in ascending order.\n  2. An integer equal to the number of dominated models.\n- Concretely, the output must be a single line in the form\n  - $[ [\\text{pareto\\_A}, \\text{dominated\\_count\\_A}], [\\text{pareto\\_B}, \\text{dominated\\_count\\_B}], [\\text{pareto\\_C}, \\text{dominated\\_count\\_C}] ]$\n  where each $\\text{pareto\\_X}$ is a list of integers as specified.\n- The output must contain no units and no additional text, and the elements must be comma-separated and enclosed in square brackets exactly as shown.",
            "solution": "The user-provided problem has been analyzed and found to be valid. It is scientifically sound, well-posed, objective, and contains all necessary information for a unique solution. The problem asks for an analysis of Ridge regression from a bi-objective optimization perspective, where the two objectives are the training mean squared error and the value of the regularization penalty. The goal is to identify Pareto-optimal models from a set of models trained with different regularization strengths.\n\nThe solution proceeds in three main steps:\n1.  Derivation of the analytical solution for the regularized linear model's parameters.\n2.  Specification of the data generation process and the two objective functions.\n3.  Description of the algorithm for identifying the Pareto-optimal set among the trained models.\n\n**1. Regularized Linear Regression (Ridge Regression)**\n\nThe problem involves a linear model where the predicted response $\\hat{y} \\in \\mathbb{R}^n$ is given by $\\hat{y} = Xw$, with a data matrix $X \\in \\mathbb{R}^{n \\times p}$ and a parameter vector $w \\in \\mathbb{R}^p$. The parameters $w$ are determined by minimizing an objective function that consists of the sum of squared errors and a quadratic regularization term (also known as an $\\ell_2$-norm penalty). This is the defining characteristic of Ridge regression.\n\nThe objective function to be minimized is:\n$$\nL(w) = \\sum_{i=1}^n (x_i^T w - y_i)^2 + \\lambda \\sum_{j=1}^p w_j^2\n$$\nIn matrix notation, this is expressed as:\n$$\nL(w) = \\lVert Xw - y \\rVert_2^2 + \\lambda \\lVert w \\rVert_2^2\n$$\nwhere $y \\in \\mathbb{R}^n$ is the vector of true responses and $\\lambda > 0$ is the regularization strength parameter.\n\nTo find the optimal parameter vector $w_\\lambda$ that minimizes $L(w)$, we compute the gradient of $L(w)$ with respect to $w$ and set it to zero.\n$$\n\\nabla_w L(w) = \\nabla_w \\left( (Xw - y)^T(Xw - y) + \\lambda w^T w \\right)\n$$\n$$\n\\nabla_w L(w) = \\nabla_w \\left( w^T X^T X w - 2y^T X w + y^T y + \\lambda w^T w \\right)\n$$\nUsing standard rules of matrix calculus, the gradient is:\n$$\n\\nabla_w L(w) = 2 X^T X w - 2 X^T y + 2 \\lambda I w\n$$\nwhere $I$ is the $p \\times p$ identity matrix. Setting the gradient to zero to find the minimum:\n$$\n2 X^T X w - 2 X^T y + 2 \\lambda I w = 0\n$$\n$$\n(X^T X + \\lambda I) w = X^T y\n$$\nSince $X^T X$ is a positive semi-definite matrix and $\\lambda$ is strictly positive ($\\lambda > 0$), the matrix $(X^T X + \\lambda I)$ is positive definite and therefore always invertible. This ensures a unique solution for $w$ for any $\\lambda > 0$, even in cases where $n  p$ (the underdetermined system where $X^T X$ is singular).\n\nThe solution for the parameter vector for a given $\\lambda$ is:\n$$\nw_\\lambda = (X^T X + \\lambda I)^{-1} X^T y\n$$\n\n**2. Bi-Objective Framework**\n\nThe problem reframes this single-objective minimization into a bi-objective analysis. For each model trained with a specific $\\lambda$, we evaluate two distinct objective functions:\n1.  **Training Mean Squared Error ($E_{\\text{train}}$)**: This measures the model's fit to the training data.\n    $$\n    E_{\\text{train}}(w_\\lambda) = \\frac{1}{n} \\lVert Xw_\\lambda - y \\rVert_2^2\n    $$\n2.  **Regularization Penalty Value ($P_\\lambda$)**: This is the value of the penalty term itself.\n    $$\n    P_\\lambda(w_\\lambda) = \\lambda \\lVert w_\\lambda \\rVert_2^2\n    $$\nFor each $\\lambda_k$ in a given set of regularization strengths, we compute the pair of objective values $(E_k, P_k) = (E_{\\text{train}}(w_{\\lambda_k}), P_{\\lambda_k}(w_{\\lambda_k}))$.\n\nThe data $(X, y)$ are generated synthetically based on a ground-truth model. The process is deterministic given a seed $s$, sample size $n$, feature dimension $p$, and noise level $\\sigma$.\n- A random number generator is initialized with seed $s$.\n- The data matrix $X \\in \\mathbb{R}^{n \\times p}$ is drawn with entries i.i.d. from $\\mathcal{N}(0,1)$.\n- The true parameter vector $w^\\star \\in \\mathbb{R}^p$ is drawn with entries i.i.d. from $\\mathcal{N}(0,1)$.\n- A noise vector $\\epsilon \\in \\mathbb{R}^n$ is drawn with entries i.i.d. from $\\mathcal{N}(0, \\sigma^2)$.\n- The response vector is constructed as $y = Xw^\\star + \\epsilon$.\n\n**3. Pareto Optimality Analysis**\n\nWith the set of objective pairs $\\{(E_k, P_k)\\}$, we identify the Pareto-optimal models. In this bi-objective context (where both objectives are to be minimized), a model is considered superior to another if it is better or equal in both objectives, and strictly better in at least one.\n\n- **Dominance**: A model $j$ with objectives $(E_j, P_j)$ *dominates* a model $i$ with objectives $(E_i, P_i)$ if and only if:\n  $E_j \\le E_i$ and $P_j \\le P_i$, with at least one inequality being strict (i.e., $E_j  E_i$ or $P_j  P_i$).\n\n- **Pareto Optimality**: A model $i$ is *Pareto optimal* if there is no other model $j$ in the set that dominates it. The set of all Pareto-optimal points is called the Pareto front.\n\nThe algorithm to identify the Pareto-optimal models and count the dominated ones is as follows:\n1.  For each test case, generate the data $(X, y)$ as specified.\n2.  For each provided regularization strength $\\lambda_k$, compute the corresponding weight vector $w_{\\lambda_k}$ using the analytical solution.\n3.  Calculate the objective pair $(E_k, P_k)$ for each model $k$.\n4.  Initialize a boolean array `is_dominated` of the same size as the number of models, with all entries set to `False`.\n5.  Iterate through each model $i$:\n    a. Iterate through every other model $j$ (where $j \\neq i$).\n    b. Check if model $j$ dominates model $i$ according to the definition above. To handle floating-point arithmetic, comparisons use a small tolerance, $\\epsilon_{tol}$. Model $j$ dominates $i$ if $(E_j \\le E_i + \\epsilon_{tol} \\text{ and } P_j \\le P_i + \\epsilon_{tol})$ and $(E_j  E_i - \\epsilon_{tol} \\text{ or } P_j  P_i - \\epsilon_{tol})$.\n    c. If a dominating model $j$ is found for model $i$, set `is_dominated[i]` to `True` and break the inner loop (over $j$) for efficiency.\n6.  After the loops complete, the indices $k$ for which `is_dominated[k]` is `False` correspond to the Pareto-optimal models. These indices are collected.\n7.  The total number of dominated models is the sum of `True` values in the `is_dominated` array.\n8.  The results for each test case are the sorted list of Pareto-optimal indices and the count of dominated models.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(s, n, p, sigma, lambdas):\n    \"\"\"\n    Generates data, trains models, and performs Pareto analysis for one test case.\n\n    Args:\n        s (int): Seed for the random number generator.\n        n (int): Number of samples.\n        p (int): Number of features.\n        sigma (float): Standard deviation of the noise.\n        lambdas (list[float]): List of regularization strengths.\n\n    Returns:\n        list: A list containing two elements:\n              1. A list of zero-based indices of Pareto-optimal models.\n              2. The integer count of dominated models.\n    \"\"\"\n    # 1. Data Generation using a deterministic protocol\n    rng = np.random.default_rng(seed=s)\n    X = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n    # The RNG state advances, so w_star is drawn from a different state than X\n    w_star = rng.normal(loc=0.0, scale=1.0, size=p)\n    epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = X @ w_star + epsilon\n\n    # 2. Model Training and Objective Calculation for each lambda\n    objectives = []\n    XTX = X.T @ X\n    XTy = X.T @ y\n    I = np.identity(p)\n    \n    for lmbda in lambdas:\n        # Solve (X'X + lambda*I)w = X'y for w\n        w_lambda = np.linalg.solve(XTX + lmbda * I, XTy)\n\n        # Calculate the two objectives: training MSE and penalty value\n        pred_error = X @ w_lambda - y\n        E_train = (1.0 / n) * np.sum(pred_error**2)\n        P_lambda_val = lmbda * np.sum(w_lambda**2)\n        \n        objectives.append((E_train, P_lambda_val))\n\n    # 3. Pareto Optimality Analysis\n    num_models = len(lambdas)\n    is_dominated = [False] * num_models\n    tol = 1e-12  # Tolerance for floating-point comparisons\n\n    for i in range(num_models):\n        for j in range(num_models):\n            if i == j:\n                continue\n            \n            # Check if model j dominates model i\n            E_i, P_i = objectives[i]\n            E_j, P_j = objectives[j]\n            \n            # Dominance condition: E_j = E_i and P_j = P_i, with one being strict\n            is_le_in_both = (E_j = E_i + tol) and (P_j = P_i + tol)\n            is_lt_in_one = (E_j  E_i - tol) or (P_j  P_i - tol)\n            \n            if is_le_in_both and is_lt_in_one:\n                is_dominated[i] = True\n                break  # Model i is dominated, no need to check other j's\n\n    pareto_indices = [i for i, dominated in enumerate(is_dominated) if not dominated]\n    dominated_count = sum(is_dominated)\n\n    return [pareto_indices, dominated_count]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Test case A (well-posed, low noise)\n        {'s': 7, 'n': 40, 'p': 6, 'sigma': 1e-1, 'lambdas': [1e-6, 1e-4, 1e-2, 1e-1, 1.0, 10.0]},\n        \n        # Test case B (underdetermined, very low noise)\n        {'s': 101, 'n': 20, 'p': 50, 'sigma': 5e-2, 'lambdas': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]},\n        \n        # Test case C (well-posed, high noise)\n        {'s': 2023, 'n': 50, 'p': 10, 'sigma': 1.0, 'lambdas': [1e-5, 1e-3, 1e-2, 1e-1, 1.0, 1e2]}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case['s'], case['n'], case['p'], case['sigma'], case['lambdas'])\n        results.append(result)\n\n    # Format the output string exactly as required, without extra spaces.\n    outer_list_parts = []\n    for res_pair in results:\n        pareto_indices, dominated_count = res_pair\n        indices_str = '[' + ','.join(map(str, pareto_indices)) + ']'\n        pair_str = f\"[{indices_str},{dominated_count}]\"\n        outer_list_parts.append(pair_str)\n    \n    final_output = '[' + ','.join(outer_list_parts) + ']'\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "While the previous practice explored the regularization trade-off in linear models, many real-world problems require non-linear solutions. This exercise extends the concept of regularization to Kernel Ridge Regression (KRR), a powerful non-linear method. Here you will learn to quantify a model's complexity using the concept of \"effective degrees of freedom\" ($d_{\\mathrm{eff}}$), a measure derived from the trace of the model's smoother matrix . By implementing this calculation, you will gain a tangible understanding of how the regularization parameter $\\lambda$ and kernel hyperparameters, like the length-scale $\\ell$, work together to control a model's capacity and prevent overfitting.",
            "id": "3168571",
            "problem": "You are given a one-dimensional training design with inputs $\\mathbf{x} \\in \\mathbb{R}^{n}$ defined by $n=20$ equispaced points on the closed interval $\\left[0,1\\right]$, specifically $x_i = \\dfrac{i-1}{n-1}$ for $i \\in \\{1,2,\\dots,n\\}$. Consider Kernel Ridge Regression (KRR) with a Radial Basis Function (RBF) kernel. The RBF kernel with length-scale $\\ell0$ is defined by $k_{\\ell}(x,x') = \\exp\\left(-\\dfrac{\\|x-x'\\|^2}{2\\ell^2}\\right)$, and the associated Gram matrix $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ has entries $K_{ij} = k_{\\ell}(x_i,x_j)$. KRR estimates the response by minimizing a Tikhonov-regularized squared loss over the Reproducing Kernel Hilbert Space, with regularization parameter $\\lambda \\ge 0$.\n\nStarting from fundamental definitions, derive the linear smoother that maps training outputs $\\mathbf{y} \\in \\mathbb{R}^n$ to fitted values $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ for KRR and use this to define the effective degrees of freedom $d_{\\mathrm{eff}}$ as the trace of the smoother. Your program must implement a numerically stable computation of $d_{\\mathrm{eff}}$ for given $\\lambda$ and $\\ell$ for the fixed design $\\mathbf{x}$ described above, without performing an explicit matrix inversion.\n\nDemonstrate overfitting control by reporting $d_{\\mathrm{eff}}$ across the following test suite of $(\\lambda,\\ell)$ pairs:\n$(10^{-6}, \\, 0.05)$,\n$(10^{-6}, \\, 5.0)$,\n$(10^{-1}, \\, 0.2)$,\n$(10^{-1}, \\, 1.0)$,\n$(10^{1}, \\, 0.05)$,\n$(10^{1}, \\, 5.0)$.\n\nYour program should compute $d_{\\mathrm{eff}}$ for each test case using the fixed $\\mathbf{x}$ and the RBF kernel specified, and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$. Each $d_{\\mathrm{eff}}$ must be printed as a decimal rounded to $6$ places. No physical units are involved, and angles are not used.",
            "solution": "The problem of Kernel Ridge Regression (KRR) is to find a function $f$ from a Reproducing Kernel Hilbert Space (RKHS) $\\mathcal{H}_k$ that minimizes the regularized squared loss. For a training set $\\{ (x_i, y_i) \\}_{i=1}^n$, the objective function is:\n$$\n\\min_{f \\in \\mathcal{H}_k} \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\|f\\|_{\\mathcal{H}_k}^2\n$$\nHere, $\\mathbf{y} = (y_1, \\dots, y_n)^T$ is the vector of observed responses, and $\\lambda \\ge 0$ is the regularization parameter.\n\nAccording to the Representer Theorem, the minimizer $f$ can be expressed as a linear combination of the kernel function evaluated at the training points:\n$$\nf(x) = \\sum_{j=1}^n \\alpha_j k(x, x_j)\n$$\nwhere $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_n)^T$ is a vector of coefficients. The squared RKHS norm is given by $\\|f\\|_{\\mathcal{H}_k}^2 = \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}$, where $\\mathbf{K}$ is the Gram matrix with entries $K_{ij} = k(x_i, x_j)$.\n\nThe vector of function evaluations at the training points, $\\mathbf{f} = (f(x_1), \\dots, f(x_n))^T$, can be written as $\\mathbf{f} = \\mathbf{K}\\boldsymbol{\\alpha}$. Substituting these into the objective function, we transform the problem into one of finding the optimal coefficients $\\boldsymbol{\\alpha}$:\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^n} \\|\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}\\|_2^2 + \\lambda \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}\n$$\nThis is a quadratic optimization problem. To find the minimum, we differentiate the objective function with respect to $\\boldsymbol{\\alpha}$ and set the gradient to zero. The objective is $L(\\boldsymbol{\\alpha}) = (\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha})^T(\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}) + \\lambda \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}$.\n$$\n\\nabla_{\\boldsymbol{\\alpha}} L = -2\\mathbf{K}^T(\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}) + 2\\lambda \\mathbf{K}\\boldsymbol{\\alpha} = 0\n$$\nSince the kernel is symmetric, $\\mathbf{K}^T = \\mathbf{K}$.\n$$\n-\\mathbf{K}\\mathbf{y} + \\mathbf{K}\\mathbf{K}\\boldsymbol{\\alpha} + \\lambda \\mathbf{K}\\boldsymbol{\\alpha} = 0\n$$\n$$\n\\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})\\boldsymbol{\\alpha} = \\mathbf{K}\\mathbf{y}\n$$\nFor distinct points and a valid RBF kernel ($\\ell0$), the Gram matrix $\\mathbf{K}$ is positive definite and thus invertible. We can multiply by $\\mathbf{K}^{-1}$:\n$$\n(\\mathbf{K} + \\lambda\\mathbf{I})\\boldsymbol{\\alpha} = \\mathbf{y}\n$$\nSolving for $\\boldsymbol{\\alpha}$, we get:\n$$\n\\boldsymbol{\\alpha} = (\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\mathbf{y}\n$$\nThe fitted values, $\\hat{\\mathbf{y}}$, are the predictions at the training points, which are given by $\\hat{\\mathbf{y}} = \\mathbf{f} = \\mathbf{K}\\boldsymbol{\\alpha}$. Substituting the expression for $\\boldsymbol{\\alpha}$:\n$$\n\\hat{\\mathbf{y}} = \\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\mathbf{y}\n$$\nThis equation shows the linear relationship between the observed outputs $\\mathbf{y}$ and the fitted values $\\hat{\\mathbf{y}}$. The matrix that performs this mapping is called the smoother matrix, $\\mathbf{S}$:\n$$\n\\mathbf{S} = \\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\n$$\nThe effective degrees of freedom, $d_{\\mathrm{eff}}$, which measures the complexity of the fitted model, is defined as the trace of the smoother matrix:\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}(\\mathbf{S}) = \\mathrm{Tr}\\left(\\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\right)\n$$\nTo compute this quantity without explicit matrix inversion, we perform a spectral decomposition of the Gram matrix $\\mathbf{K}$. Since $\\mathbf{K}$ is real and symmetric, it can be diagonalized as $\\mathbf{K} = \\mathbf{U\\Sigma U}^T$, where $\\mathbf{U}$ is an orthogonal matrix of eigenvectors ($\\mathbf{U}\\mathbf{U}^T = \\mathbf{U}^T\\mathbf{U} = \\mathbf{I}$) and $\\mathbf{\\Sigma}$ is a diagonal matrix of the corresponding non-negative eigenvalues $\\sigma_i$.\nSubstituting this into the expression for $d_{\\mathrm{eff}}$:\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}\\left( (\\mathbf{U\\Sigma U}^T) (\\mathbf{U\\Sigma U}^T + \\lambda\\mathbf{I})^{-1} \\right) \\\\\n= \\mathrm{Tr}\\left( \\mathbf{U\\Sigma U}^T \\left(\\mathbf{U}(\\mathbf{\\Sigma} + \\lambda\\mathbf{I})\\mathbf{U}^T\\right)^{-1} \\right) \\\\\n= \\mathrm{Tr}\\left( \\mathbf{U\\Sigma U}^T (\\mathbf{U}^T)^{-1} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\mathbf{U}^{-1} \\right)\n$$\nUsing $\\mathbf{U}^{-1} = \\mathbf{U}^T$ and $(\\mathbf{U}^T)^{-1} = \\mathbf{U}$:\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}\\left( \\mathbf{U\\Sigma} (\\mathbf{U}^T \\mathbf{U}) (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\mathbf{U}^T \\right) = \\mathrm{Tr}\\left( \\mathbf{U}\\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\mathbf{U}^T \\right)\n$$\nUsing the cyclic property of the trace, $\\mathrm{Tr}(\\mathbf{ABC}) = \\mathrm{Tr}(\\mathbf{CAB})$:\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}\\left( \\mathbf{U}^T \\mathbf{U}\\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\right) = \\mathrm{Tr}\\left( \\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\right)\n$$\nThe matrix $\\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1}$ is diagonal with entries $\\frac{\\sigma_i}{\\sigma_i + \\lambda}$. The trace is the sum of these diagonal elements:\n$$\nd_{\\mathrm{eff}} = \\sum_{i=1}^n \\frac{\\sigma_i}{\\sigma_i + \\lambda}\n$$\nThis formula provides a numerically stable way to compute $d_{\\mathrm{eff}}$ by first finding the eigenvalues of $\\mathbf{K}$. The implementation will construct the Gram matrix $\\mathbf{K}$ for each $(\\lambda, \\ell)$ pair, compute its eigenvalues, and then apply this formula.\n\nA high value of $d_{\\mathrm{eff}}$ (approaching $n=20$) signifies an overly complex model that may be overfitting, as it uses many parameters to fit the data. This occurs when $\\lambda$ is small. A low value of $d_{\\mathrm{eff}}$ (approaching $0$) signifies a very simple, highly regularized model that may be underfitting. This occurs when $\\lambda$ is large or when $\\ell$ is very large (forcing a near-constant function, which has complexity $1$).\n\n- For $(\\lambda=10^{-6}, \\ell=0.05)$: Small $\\lambda$ and a length-scale $\\ell$ comparable to inter-point distance suggests low regularization and a complex model. We expect $d_{\\mathrm{eff}} \\approx 20$.\n- For $(\\lambda=10^{-6}, \\ell=5.0)$: Small $\\lambda$ but large $\\ell$ forces a very smooth function. The Gram matrix approaches a rank-1 matrix, leading to few effective parameters. We expect $d_{\\mathrm{eff}} \\approx 1$.\n- For $(\\lambda=10^{1}, \\ell=0.05)$: Large $\\lambda$ imposes strong regularization, reducing complexity despite the small $\\ell$. We expect a small $d_{\\mathrm{eff}}$.\n- For $(\\lambda=10^{1}, \\ell=5.0)$: Both large $\\lambda$ and large $\\ell$ promote simplicity. We expect a very small $d_{\\mathrm{eff}}$.\n\nThe code below implements this logic.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the effective degrees of freedom for Kernel Ridge Regression\n    with an RBF kernel for a set of (lambda, ell) parameters.\n    \"\"\"\n    \n    # Define fixed problem parameters\n    n = 20\n    x = np.linspace(0, 1, n)\n\n    # Define the test suite of (lambda, ell) pairs\n    test_cases = [\n        (1e-6, 0.05),\n        (1e-6, 5.0),\n        (1e-1, 0.2),\n        (1e-1, 1.0),\n        (1e+1, 0.05),\n        (1e+1, 5.0)\n    ]\n\n    results = []\n    \n    # Use broadcasting to create a matrix of squared distances\n    # x_col has shape (n, 1), x has shape (n,). Broadcasting creates an (n, n) matrix.\n    x_col = x.reshape(-1, 1)\n    sq_dist_matrix = (x_col - x)**2\n\n    for lmbda, ell in test_cases:\n        # Construct the RBF Gram matrix K\n        # K_ij = exp(-||x_i - x_j||^2 / (2 * ell^2))\n        K = np.exp(-sq_dist_matrix / (2 * ell**2))\n\n        # Compute the eigenvalues of the symmetric matrix K.\n        # np.linalg.eigvalsh is numerically stable and optimized for Hermitian matrices.\n        eigenvalues = np.linalg.eigvalsh(K)\n\n        # Compute the effective degrees of freedom using the derived formula.\n        # d_eff = sum(sigma_i / (sigma_i + lambda))\n        d_eff = np.sum(eigenvalues / (eigenvalues + lmbda))\n        \n        # Round the result to 6 decimal places and store as a string\n        results.append(f\"{d_eff:.6f}\")\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "So far, we have focused on explicit regularization, where a penalty term is added directly to the loss function. This practice introduces two new concepts: the sparsity-inducing $\\ell_1$ penalty of the LASSO model and the idea of *implicit* regularization through optimization. You will implement the Iterative Shrinkage-Thresholding Algorithm (ISTA) and discover that simply stopping the algorithm early can act as a powerful regularizer, even without an explicit penalty term . This exercise highlights the deep and often surprising connection between the process of optimization and the generalization properties of a model.",
            "id": "3168592",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) objective as a composite function with a smooth data-fit term and a non-smooth sparsity-promoting penalty. Let the training design matrix be $A_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$, the validation design matrix be $A_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$, and the corresponding response vectors be $y_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ and $y_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}}}$. The empirical risk minimization problem with an $\\ell_1$ penalty is\n$$\n\\min_{x \\in \\mathbb{R}^d} \\; \\frac{1}{2 n_{\\text{train}}} \\left\\| A_{\\text{train}} x - y_{\\text{train}} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| x \\right\\|_1,\n$$\nwhere $\\lambda \\ge 0$ balances the fit to data and sparsity. The Iterative Shrinkage-Thresholding Algorithm (ISTA) is obtained by applying proximal gradient descent to the composite objective, which alternates a gradient step on the smooth term with an application of the proximal operator of the $\\ell_1$ norm. In this problem, you will unroll a fixed number of ISTA iterations $T$ as a layer-wise computation graph (interpreting $T$ as early stopping), and compare its generalization behavior to a long-run ISTA with explicit $\\lambda$ regularization.\n\nYour program must:\n- Construct synthetic linear regression datasets with sparse ground truth using the following fundamental and well-tested specifications:\n  1. The ground-truth parameter $x^\\star \\in \\mathbb{R}^d$ is $k$-sparse, with exactly $k$ nonzero entries drawn independently from a zero-mean unit-variance distribution. The support (locations of nonzero entries) is chosen uniformly at random.\n  2. The training and validation matrices are generated to have controllable column-wise correlation. For a specified correlation parameter $\\rho \\in [0,1)$, let $Z_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$ and $Z_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$ have independent and identically distributed (i.i.d.) standard normal entries. For each dataset, draw a latent vector $u_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ and $u_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}}}$ with i.i.d. standard normal entries. Define\n  $$\n  A_{\\text{train}}[:,j] \\;=\\; \\sqrt{1-\\rho}\\; Z_{\\text{train}}[:,j] \\;+\\; \\sqrt{\\rho}\\; u_{\\text{train}}, \\quad\n  A_{\\text{val}}[:,j] \\;=\\; \\sqrt{1-\\rho}\\; Z_{\\text{val}}[:,j] \\;+\\; \\sqrt{\\rho}\\; u_{\\text{val}},\n  $$\n  for each column index $j \\in \\{1,\\dots,d\\}$. This construction yields feature columns with correlation controlled by $\\rho$ and is scientifically sound for studying the effects of collinearity.\n  3. The training and validation responses satisfy\n  $$\n  y_{\\text{train}} \\;=\\; A_{\\text{train}} x^\\star \\;+\\; \\varepsilon_{\\text{train}}, \\quad\n  y_{\\text{val}} \\;=\\; A_{\\text{val}} x^\\star \\;+\\; \\varepsilon_{\\text{val}},\n  $$\n  where $\\varepsilon_{\\text{train}}$ and $\\varepsilon_{\\text{val}}$ have i.i.d. Gaussian entries with zero mean and standard deviation $\\sigma$ (noise level).\n- Starting from the core definitions of the squared loss and the $\\ell_1$ penalty, implement ISTA by:\n  1. Computing the gradient of the smooth term using first principles from multivariate calculus.\n  2. Choosing a constant step size $s$ that is at most the reciprocal of the Lipschitz constant of the gradient of the smooth term, where the Lipschitz constant equals the largest eigenvalue of $\\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top A_{\\text{train}}$.\n  3. Applying the proximal operator of the $\\ell_1$ norm to the gradient-updated iterate.\n- Unroll exactly $T$ iterations from the zero vector $x^{(0)} = 0$ to obtain $x^{(T)}$ and interpret finite $T$ as early stopping, an implicit form of regularization arising from halting the optimization before convergence.\n\nFor each dataset, construct two estimators:\n- Early-stopped estimator with no explicit $\\ell_1$ penalty: set $\\lambda = 0$ and run ISTA for exactly $T_{\\text{small}}$ iterations to obtain $x^{(T_{\\text{small}})}$.\n- Explicitly regularized estimator: set a prescribed $\\lambda  0$ and run ISTA for $T_{\\text{large}}$ iterations to approximate convergence and obtain $x^{(T_{\\text{large}})}$.\n\nEvaluate generalization by computing the Mean Squared Error (MSE) on the validation set for both estimators:\n$$\n\\text{MSE}_{\\text{val}}(x) \\;=\\; \\frac{1}{n_{\\text{val}}} \\left\\| A_{\\text{val}} x - y_{\\text{val}} \\right\\|_2^2.\n$$\nReport, for each dataset, whether early stopping achieves strictly lower validation error than explicit $\\lambda$ regularization. Represent this comparison as an integer: $1$ if $\\text{MSE}_{\\text{val}}(x^{(T_{\\text{small}})})  \\text{MSE}_{\\text{val}}(x^{(T_{\\text{large}})})$, otherwise $0$.\n\nUse the following test suite to ensure coverage of distinct regimes:\n- Case $1$ (happy path, moderate noise, mild correlation): $d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 0.2$, $\\rho = 0.2$, $T_{\\text{small}} = 10$, $T_{\\text{large}} = 500$, $\\lambda = 0.05$.\n- Case $2$ (boundary condition with $T=0$ and higher noise): $d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 1.0$, $\\rho = 0.0$, $T_{\\text{small}} = 0$, $T_{\\text{large}} = 500$, $\\lambda = 0.1$.\n- Case $3$ (strong feature correlation, low noise): $d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 0.05$, $\\rho = 0.9$, $T_{\\text{small}} = 3$, $T_{\\text{large}} = 500$, $\\lambda = 0.01$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3]$), where each $r_i$ is the integer comparison result for Case $i$. No physical units or angle units are involved. Ensure all random choices are reproducible by fixing seeds per case inside the program.",
            "solution": "The user-provided problem is valid. It is a well-posed, scientifically grounded, and objective problem in computational science, specifically focusing on regularization techniques in machine learning. It provides a complete and consistent set of definitions, parameters, and evaluation criteria, allowing for a unique and meaningful solution.\n\nThe problem asks for a comparison between two forms of regularization for the LASSO (Least Absolute Shrinkage and Selection Operator) model: explicit $\\ell_1$ penalization and implicit regularization through early stopping of the optimization algorithm. The chosen algorithm is the Iterative Shrinkage-Thresholding Algorithm (ISTA), a standard proximal gradient method for this class of problems.\n\nThe core of the problem lies in solving the following optimization objective:\n$$\n\\min_{x \\in \\mathbb{R}^d} \\; F(x) = \\min_{x \\in \\mathbb{R}^d} \\left( \\frac{1}{2 n_{\\text{train}}} \\left\\| A_{\\text{train}} x - y_{\\text{train}} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| x \\right\\|_1 \\right)\n$$\nwhere the objective $F(x)$ is a composite function $F(x) = f(x) + g(x)$. The term $f(x) = \\frac{1}{2 n_{\\text{train}}} \\| A_{\\text{train}} x - y_{\\text{train}} \\|_2^2$ is the smooth, differentiable data-fit term (empirical risk from a squared loss), and $g(x) = \\lambda \\|x\\|_1$ is the non-smooth, convex regularization term.\n\n### Data Generation\nA synthetic dataset is constructed according to rigorous specifications to ensure controllability and reproducibility.\n1.  A $k$-sparse ground-truth parameter vector $x^\\star \\in \\mathbb{R}^d$ is generated. Its support (the set of indices of its $k$ non-zero elements) is chosen uniformly at random. The values at these non-zero positions are drawn from a standard normal distribution, $N(0, 1)$.\n2.  The design matrices, $A_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$ and $A_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$, are constructed to have a specified feature-wise correlation $\\rho$. Each column $A[:,j]$ is a linear combination of an independent random vector $Z[:,j]$ and a common latent vector $u$:\n    $$\n    A[:,j] = \\sqrt{1-\\rho} Z[:,j] + \\sqrt{\\rho} u\n    $$\n    where entries of $Z$ and $u$ are i.i.d. standard normal. This construction induces a theoretical correlation of $\\text{Cov}(A[:,i], A[:,j]) = \\rho$ for $i \\neq j$.\n3.  The response vectors, $y_{\\text{train}}$ and $y_{\\text{val}}$, are generated from the linear model $y = Ax^\\star + \\varepsilon$, where $\\varepsilon$ is additive Gaussian noise with mean $0$ and standard deviation $\\sigma$.\n\n### Iterative Shrinkage-Thresholding Algorithm (ISTA)\nISTA is an instance of a proximal gradient method, which is suitable for minimizing composite objectives of the form $f(x) + g(x)$. Each iteration consists of two steps:\n1.  A gradient descent step on the smooth term $f(x)$.\n2.  An application of the proximal operator of the non-smooth term $g(x)$.\n\nThe gradient of the smooth term $f(x)$ is given by:\n$$\n\\nabla f(x) = \\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top (A_{\\text{train}} x - y_{\\text{train}})\n$$\nThe proximal operator of $g(x) = \\lambda \\|x\\|_1$ with step size $s$ is the soft-thresholding operator, $S_{s\\lambda}(\\cdot)$:\n$$\n\\text{prox}_{s g}(v)_i = S_{s\\lambda}(v_i) = \\text{sign}(v_i) \\max(|v_i| - s\\lambda, 0)\n$$\nCombining these, the ISTA update rule, starting from an initial guess $x^{(0)}$, is:\n$$\nx^{(t+1)} = S_{s\\lambda} \\left( x^{(t)} - s \\nabla f(x^{(t)}) \\right)\n$$\nConvergence of this algorithm is guaranteed if the step size $s$ satisfies $0  s \\le 1/L$, where $L$ is the Lipschitz constant of $\\nabla f(x)$. The Hessian of $f(x)$ is $\\nabla^2 f(x) = \\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top A_{\\text{train}}$. The Lipschitz constant $L$ is the largest eigenvalue (i.e., the spectral norm) of this Hessian matrix. A safe and standard choice is $s = 1/L$.\n\n### Regularization Comparison\nTwo estimators are constructed and compared:\n1.  **Early-Stopped Estimator**: This model uses implicit regularization. We set the explicit regularization parameter $\\lambda = 0$ and run the ISTA algorithm for a small, fixed number of iterations, $T_{\\text{small}}$. When $\\lambda=0$, the soft-thresholding operator becomes the identity, $S_0(v) = v$, and ISTA reduces to standard gradient descent on the least-squares objective. Starting from $x^{(0)} = 0$, a few iterations move the parameters towards the minimum of the empirical risk, but stopping early prevents the parameters from growing too large and overfitting the training data.\n2.  **Explicitly Regularized Estimator**: This model uses the standard LASSO formulation with a prescribed penalty $\\lambda  0$. The ISTA algorithm is run for a large number of iterations, $T_{\\text{large}}$, to approximate convergence to the unique minimizer of the explicitly regularized objective.\n\n### Evaluation\nThe generalization performance of both estimators, $x^{(T_{\\text{small}})}$ and $x^{(T_{\\text{large}})}$, is evaluated on the unseen validation set. The metric is the Mean Squared Error (MSE):\n$$\n\\text{MSE}_{\\text{val}}(x) = \\frac{1}{n_{\\text{val}}} \\left\\| A_{\\text{val}} x - y_{\\text{val}} \\right\\|_2^2\n$$\nThe final output for each test case is an integer $1$ if the early-stopped estimator achieves a strictly lower validation MSE than the explicitly regularized one, and $0$ otherwise. Reproducibility is ensured by using a fixed random seed for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(d, n_train, n_val, k, sigma, rho, rng):\n    \"\"\"\n    Generates synthetic data for a sparse linear regression problem.\n    \n    Args:\n        d (int): Number of features.\n        n_train (int): Number of training samples.\n        n_val (int): Number of validation samples.\n        k (int): Sparsity level of the true parameter vector.\n        sigma (float): Standard deviation of the noise.\n        rho (float): Correlation parameter for the design matrix columns.\n        rng (numpy.random.Generator): NumPy random number generator instance.\n        \n    Returns:\n        tuple: A_train, y_train, A_val, y_val\n    \"\"\"\n    # 1. Generate k-sparse ground-truth parameter vector x_star\n    x_star = np.zeros(d)\n    support = rng.choice(d, k, replace=False)\n    x_star[support] = rng.standard_normal(k)\n\n    # 2. Generate training and validation design matrices A\n    # Training data\n    Z_train = rng.standard_normal((n_train, d))\n    u_train = rng.standard_normal(n_train)\n    A_train = np.sqrt(1 - rho) * Z_train + np.sqrt(rho) * u_train[:, np.newaxis]\n    \n    # Validation data\n    Z_val = rng.standard_normal((n_val, d))\n    u_val = rng.standard_normal(n_val)\n    A_val = np.sqrt(1 - rho) * Z_val + np.sqrt(rho) * u_val[:, np.newaxis]\n\n    # 3. Generate response vectors y\n    eps_train = sigma * rng.standard_normal(n_train)\n    y_train = A_train @ x_star + eps_train\n    \n    eps_val = sigma * rng.standard_normal(n_val)\n    y_val = A_val @ x_star + eps_val\n\n    return A_train, y_train, A_val, y_val\n\ndef ista(A, y, lambda_reg, T, step_size):\n    \"\"\"\n    Implements the Iterative Shrinkage-Thresholding Algorithm (ISTA).\n    \n    Args:\n        A (np.ndarray): Design matrix.\n        y (np.ndarray): Response vector.\n        lambda_reg (float): L1 regularization parameter.\n        T (int): Number of iterations.\n        step_size (float): Step size for the gradient descent step.\n        \n    Returns:\n        np.ndarray: The estimated parameter vector x after T iterations.\n    \"\"\"\n    n, d = A.shape\n    x = np.zeros(d)\n    \n    if T == 0:\n        return x\n\n    for _ in range(T):\n        # Gradient of the smooth term\n        grad = (1 / n) * A.T @ (A @ x - y)\n        \n        # Gradient update step\n        z = x - step_size * grad\n        \n        # Proximal operator (soft-thresholding)\n        x = np.sign(z) * np.maximum(np.abs(z) - step_size * lambda_reg, 0)\n        \n    return x\n\ndef calculate_mse(A, y, x):\n    \"\"\"Calculates the Mean Squared Error.\"\"\"\n    n = A.shape[0]\n    error = A @ x - y\n    return (1 / n) * np.sum(error**2)\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        # d, n_train, n_val, k, sigma, rho, T_small, T_large, lambda\n        (50, 80, 80, 5, 0.2, 0.2, 10, 500, 0.05),\n        (50, 80, 80, 5, 1.0, 0.0, 0, 500, 0.1),\n        (50, 80, 80, 5, 0.05, 0.9, 3, 500, 0.01),\n    ]\n\n    results = []\n    \n    for i, params in enumerate(test_cases):\n        d, n_train, n_val, k, sigma, rho, T_small, T_large, lambda_val = params\n        \n        # Ensure reproducibility for each case by fixing the seed\n        rng = np.random.default_rng(seed=i)\n\n        # Generate data\n        A_train, y_train, A_val, y_val = generate_data(d, n_train, n_val, k, sigma, rho, rng)\n        \n        # Calculate step size\n        C = (1 / n_train) * (A_train.T @ A_train)\n        L = np.max(np.linalg.eigvalsh(C))\n        step_size = 1.0 / L\n\n        # Early-stopped estimator (lambda = 0)\n        x_early = ista(A_train, y_train, lambda_reg=0.0, T=T_small, step_size=step_size)\n        \n        # Explicitly regularized estimator (lambda  0)\n        x_reg = ista(A_train, y_train, lambda_reg=lambda_val, T=T_large, step_size=step_size)\n        \n        # Evaluate validation MSE for both\n        mse_early = calculate_mse(A_val, y_val, x_early)\n        mse_reg = calculate_mse(A_val, y_val, x_reg)\n        \n        # Compare and store result\n        result = 1 if mse_early  mse_reg else 0\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}