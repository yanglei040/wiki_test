{
    "hands_on_practices": [
        {
            "introduction": "The power of importance sampling lies not just in its ability to compute challenging integrals, but in its potential for variance reduction. A well-chosen proposal distribution $q(x)$ can dramatically speed up the convergence of a Monte Carlo estimate compared to naive sampling. This first exercise  provides a concrete example of this optimization process, asking you to analytically derive the optimal parameter for a proposal distribution to minimize the estimator's variance. By working through this problem, you will gain direct insight into how the similarity between the proposal distribution and the target integrand governs the efficiency of importance sampling.",
            "id": "767870",
            "problem": "Importance sampling is a variance reduction technique used in Monte Carlo methods to estimate properties of a particular distribution, while drawing samples from a different distribution. Suppose we wish to estimate the expectation $I = E_p[f(X)] = \\int f(x) p(x) dx$, where $p(x)$ is the target probability density function. Instead of sampling from $p(x)$, we can draw samples $\\{X_i\\}_{i=1}^N$ from a proposal distribution $q(x)$ and construct the estimator:\n$$ \\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^N f(X_i) w(X_i) $$\nwhere $w(x) = \\frac{p(x)}{q(x)}$ are the importance weights. This estimator is unbiased, i.e., $E_q[\\hat{I}_N] = I$. The efficiency of this method depends on the variance of the estimator, which is given by $\\text{Var}_q(\\hat{I}_N) = \\frac{1}{N} \\text{Var}_q(f(X)w(X))$. To minimize the variance of the estimator for a fixed sample size $N$, one must minimize $\\text{Var}_q(f(X)w(X))$. Since $\\text{Var}_q(Y) = E_q[Y^2] - (E_q[Y])^2$ and $E_q[f(X)w(X)] = I$ is a constant with respect to the choice of $q(x)$, minimizing the variance is equivalent to minimizing the second moment, $E_q[(f(X)w(X))^2]$.\n\nConsider a scenario where the target distribution $p(x)$ is a Rayleigh distribution with scale parameter $\\sigma_p$. The probability density function of a Rayleigh distribution with scale parameter $\\sigma$ is given by:\n$$ g(x; \\sigma) = \\frac{x}{\\sigma^2} e^{-x^2 / (2\\sigma^2)} \\quad \\text{for } x \\ge 0 $$\nWe wish to estimate the mean of this target distribution, so $f(x)=x$. As our proposal distribution $q(x)$, we choose another Rayleigh distribution with a tunable scale parameter $\\sigma_q$.\n\nDerive the optimal value of the proposal scale parameter $\\sigma_q$ that minimizes the variance of the importance sampling estimator. Express your answer in terms of $\\sigma_p$.",
            "solution": "We wish to minimize the second moment\n$$M_2(\\sigma_q)=E_q[(f(X)w(X))^2]\n=\\int_0^\\infty x^2\\Bigl(\\frac{p(x)}{q(x)}\\Bigr)^2q(x)\\,dx\n=\\int_0^\\infty\\frac{x^2p(x)^2}{q(x)}\\,dx$$\nwith \n$$p(x)=\\frac{x}{\\sigma_p^2}e^{-x^2/(2\\sigma_p^2)},\\quad\nq(x)=\\frac{x}{\\sigma_q^2}e^{-x^2/(2\\sigma_q^2)}.$$\nSubstitute to get\n$$M_2\n=\\int_0^\\infty\\frac{x^2\\,(x/\\sigma_p^2)^2e^{-x^2/\\sigma_p^2}}\n{(x/\\sigma_q^2)e^{-x^2/(2\\sigma_q^2)}}\\,dx\n=\\frac{\\sigma_q^2}{\\sigma_p^4}\\int_0^\\infty x^3e^{-ax^2}\\,dx,$$\nwhere \n$$a=\\frac1{\\sigma_p^2}-\\frac1{2\\sigma_q^2}\n=\\frac{2\\sigma_q^2-\\sigma_p^2}{2\\sigma_p^2\\sigma_q^2}.$$\nUsing \n$$\\int_0^\\infty x^3e^{-ax^2}dx=\\frac1{2a^2},$$\nwe obtain\n$$M_2\n=\\frac{\\sigma_q^2}{\\sigma_p^4}\\,\\frac1{2a^2}\n=2\\,\\frac{\\sigma_q^6}{(2\\sigma_q^2-\\sigma_p^2)^2}.$$\nSet $x=\\sigma_q^2$ and minimize\n$$F(x)=\\frac{x^3}{(2x-\\sigma_p^2)^2}$$\nby solving $dF/dx=0$.  One finds the admissible critical point\n$$\\sigma_q^2=\\tfrac32\\,\\sigma_p^2,\n\\quad\\sigma_q=\\sigma_p\\sqrt{\\tfrac32}.$$",
            "answer": "$$\\boxed{\\sigma_p\\sqrt{\\frac{3}{2}}}$$"
        },
        {
            "introduction": "While choosing an efficient proposal distribution is key, there is a fundamental constraint that must always be satisfied for an importance sampling estimator to be valid. The support of the proposal distribution must cover the support of the target integrand, a condition sometimes written as $p \\ll q$. This practice problem  sets up a hypothetical scenario to demonstrate what goes wrong when this rule is violated, leading to biased results. By analyzing this pitfall, you will develop a deeper understanding of the theoretical guarantees underpinning importance sampling and why they are not mere technicalities.",
            "id": "3143026",
            "problem": "You are tasked with estimating the quantity $\\mu = \\mathbb{E}_{p}[g(X)]$ for a bounded measurable function $g$, where $X$ is a real-valued random variable with probability density function $p$. Consider the following scientifically plausible setting: the target density $p$ is uniform on the interval $[0, 2]$, and the proposal density $q$ used for importance sampling is uniform on the interval $[0, 1.5]$. Concretely,\n$$\np(x) = \\frac{1}{2}\\,\\mathbf{1}_{[0, 2]}(x), \\qquad q(x) = \\frac{2}{3}\\,\\mathbf{1}_{[0, 1.5]}(x),\n$$\nwhere $\\mathbf{1}_{A}(x)$ is the indicator function of the set $A$. The standard importance sampling method uses samples $X_{1}, \\dots, X_{n} \\overset{\\text{i.i.d.}}{\\sim} q$ and weights $w(x) = \\frac{p(x)}{q(x)}$ to form an estimator of $\\mu$. In this setting, the interval $[1.5, 2]$ has positive mass under $p$ but zero mass under $q$, creating a gap in proposal support.\n\nTo remedy support gaps, one common strategy is to replace $q$ with a mixture proposal that guarantees support coverage. Let $r$ be the uniform density on $[0, 2]$, i.e.,\n$$\nr(x) = \\frac{1}{2}\\,\\mathbf{1}_{[0, 2]}(x),\n$$\nand consider the mixture proposal\n$$\nq_{\\varepsilon}(x) = (1 - \\varepsilon)\\,q(x) + \\varepsilon\\,r(x),\n$$\nwith $\\varepsilon \\in (0, 1)$.\n\nSelect all statements that are correct in this setting.\n\nA. For $g(x) = 1$, the standard importance sampling estimator using $q$ is unbiased for $\\mu$.\n\nB. The condition that $p$ is absolutely continuous with respect to $q$ (i.e., $p \\ll q$) fails here. Consequently, the standard importance sampling estimator using $q$ is biased unless $g(x) = 0$ on $[1.5, 2]$.\n\nC. Using the mixture proposal $q_{\\varepsilon}$ restores unbiasedness for any bounded $g$ and yields a finite variance importance sampling estimator; however, as $\\varepsilon \\to 0$, the variance can become arbitrarily large.\n\nD. Multiplying the weights $\\frac{p(x)}{q(x)}$ by a fixed constant to cap their magnitude prevents bias and ensures finite variance even when $q$ has zero density on parts of the support of $p$.\n\nE. Switching to the self-normalized importance sampling (SNIS) estimator removes the need for support coverage; unbiasedness holds even if $q$ assigns zero density where $p(x)  0$.",
            "solution": "The user wants me to validate the problem statement and, if valid, provide a detailed solution evaluating each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Quantity to estimate: $\\mu = \\mathbb{E}_{p}[g(X)]$.\n- $g$: a bounded measurable function.\n- $X$: a real-valued random variable.\n- Target probability density function (PDF): $p(x) = \\frac{1}{2}\\,\\mathbf{1}_{[0, 2]}(x)$.\n- Proposal PDF for standard importance sampling: $q(x) = \\frac{2}{3}\\,\\mathbf{1}_{[0, 1.5]}(x)$.\n- Standard importance sampling uses i.i.d. samples $X_{1}, \\dots, X_{n} \\sim q$.\n- Importance weights: $w(x) = \\frac{p(x)}{q(x)}$.\n- A secondary density for mixture modeling: $r(x) = \\frac{1}{2}\\,\\mathbf{1}_{[0, 2]}(x)$.\n- Mixture proposal PDF: $q_{\\varepsilon}(x) = (1 - \\varepsilon)\\,q(x) + \\varepsilon\\,r(x)$, with $\\varepsilon \\in (0, 1)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard exercise in computational statistics, specifically Monte Carlo methods. The concepts of importance sampling, probability densities, expectation, bias, and variance are all well-established mathematical and statistical principles. The setup is sound.\n- **Well-Posed:** The problem provides specific functional forms for the densities $p$, $q$, and $r$, and defines the estimators and quantities of interest. It asks for an evaluation of several precise statements, which is a well-defined task.\n- **Objective:** The problem is stated in precise, objective mathematical language.\n- **Incomplete or Contradictory Setup:** All necessary information is provided. The densities are well-defined and integrate to $1$. There are no contradictions.\n- **Unrealistic or Infeasible:** The setup is a simplified, theoretical case designed to illustrate a key concept in importance sampling. It is not physically unrealistic.\n- **Ill-Posed or Poorly Structured:** The terminology is standard and unambiguous within the field of computational science and statistics.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It presents a clear, consistent, and solvable problem rooted in established statistical theory. I will proceed to the solution.\n\n### Solution Derivation\n\nThe quantity to be estimated is the expectation of $g(X)$ under the distribution $p$:\n$$\n\\mu = \\mathbb{E}_{p}[g(X)] = \\int_{-\\infty}^{\\infty} g(x)p(x)dx\n$$\nImportance sampling aims to estimate this by drawing samples from a different distribution $q$ and re-weighting. The fundamental identity is:\n$$\n\\mu = \\int_{-\\infty}^{\\infty} g(x) \\frac{p(x)}{q(x)} q(x) dx = \\mathbb{E}_{q}\\left[g(X)\\frac{p(X)}{q(X)}\\right]\n$$\nThis identity holds if and only if the support of $p$ is a subset of the support of $q$, i.e., $p(x)  0 \\implies q(x)  0$. This condition is also written as $p$ being absolutely continuous with respect to $q$ ($p \\ll q$).\n\nThe standard (or \"plain\") importance sampling estimator for a sample of size $n$ is:\n$$\n\\hat{\\mu}_{n} = \\frac{1}{n} \\sum_{i=1}^n g(X_i) w(X_i), \\quad \\text{where } X_i \\overset{\\text{i.i.d.}}{\\sim} q \\text{ and } w(x) = \\frac{p(x)}{q(x)}.\n$$\nThe expectation of this estimator is:\n$$\n\\mathbb{E}[\\hat{\\mu}_{n}] = \\mathbb{E}_{q}\\left[g(X)w(X)\\right] = \\int_{\\{x: q(x)  0\\}} g(x)\\frac{p(x)}{q(x)} q(x) dx = \\int_{\\{x: q(x)  0\\}} g(x)p(x)dx.\n$$\nThe estimator is unbiased if $\\mathbb{E}[\\hat{\\mu}_{n}] = \\mu$. This requires $\\int_{\\{x: q(x)  0\\}} g(x)p(x)dx = \\int_{\\{x: p(x)  0\\}} g(x)p(x)dx$, which is true if and only if $\\int_{\\{x: p(x)0, q(x)=0\\}} g(x)p(x)dx = 0$.\n\nIn this problem:\n- The support of $p$ is $\\text{supp}(p) = [0, 2]$.\n- The support of $q$ is $\\text{supp}(q) = [0, 1.5]$.\nThe set where $p(x)  0$ and $q(x) = 0$ is the interval $(1.5, 2]$. Since $p(x) = 1/2  0$ on this interval, the estimator is biased unless $\\int_{1.5}^{2} g(x) dx = 0$.\n\n### Option-by-Option Analysis\n\n**A. For $g(x) = 1$, the standard importance sampling estimator using $q$ is unbiased for $\\mu$.**\n\nFirst, let's find the true value $\\mu$. For $g(x) = 1$:\n$$\n\\mu = \\mathbb{E}_{p}[1] = \\int_{0}^{2} 1 \\cdot p(x) dx = \\int_{0}^{2} \\frac{1}{2} dx = \\frac{1}{2} [x]_{0}^{2} = 1.\n$$\nNext, let's find the expectation of the estimator $\\hat{\\mu}_{n}$. The samples $X_i$ are drawn from $q$, which has support $[0, 1.5]$.\n$$\n\\mathbb{E}[\\hat{\\mu}_{n}] = \\int_{\\text{supp}(q)} g(x) \\frac{p(x)}{q(x)} q(x) dx = \\int_{0}^{1.5} 1 \\cdot p(x) dx = \\int_{0}^{1.5} \\frac{1}{2} dx = \\frac{1}{2} [x]_{0}^{1.5} = \\frac{1.5}{2} = 0.75.\n$$\nSince $\\mathbb{E}[\\hat{\\mu}_{n}] = 0.75$ and $\\mu = 1$, the estimator is biased.\n\nVerdict: **Incorrect**.\n\n**B. The condition that $p$ is absolutely continuous with respect to $q$ (i.e., $p \\ll q$) fails here. Consequently, the standard importance sampling estimator using $q$ is biased unless $g(x) = 0$ on $[1.5, 2]$.**\n\nThe condition for absolute continuity, $p \\ll q$, requires that $\\text{supp}(p) \\subseteq \\text{supp}(q)$. Here, $\\text{supp}(p) = [0, 2]$ and $\\text{supp}(q) = [0, 1.5]$. Since $[0, 2] \\not\\subseteq [0, 1.5]$, the condition fails. This part of the statement is correct.\n\nThe bias of the estimator is given by:\n$$\n\\text{Bias} = \\mathbb{E}[\\hat{\\mu}_{n}] - \\mu = \\int_{\\text{supp}(q)} g(x)p(x)dx - \\int_{\\text{supp}(p)} g(x)p(x)dx = -\\int_{\\text{supp}(p) \\setminus \\text{supp}(q)} g(x)p(x)dx.\n$$\nIn this case, this becomes:\n$$\n\\text{Bias} = -\\int_{1.5}^{2} g(x)p(x)dx = -\\frac{1}{2} \\int_{1.5}^{2} g(x)dx.\n$$\nThe estimator is unbiased if and only if this bias term is zero. Since $p(x)  0$ on $(1.5, 2]$, this requires $\\int_{1.5}^{2} g(x)dx = 0$. The condition \"$g(x) = 0$ on $[1.5, 2]$\" is a sufficient condition to make this integral zero. For a general non-negative function $g$, it is also a necessary condition (almost everywhere). Thus, the statement accurately identifies the failure of the support condition and its direct consequence on bias, along with the condition on $g$ that would eliminate this bias.\n\nVerdict: **Correct**.\n\n**C. Using the mixture proposal $q_{\\varepsilon}$ restores unbiasedness for any bounded $g$ and yields a finite variance importance sampling estimator; however, as $\\varepsilon \\to 0$, the variance can become arbitrarily large.**\n\nThe mixture proposal is $q_{\\varepsilon}(x) = (1 - \\varepsilon)\\,q(x) + \\varepsilon\\,r(x)$. Since $\\text{supp}(q) = [0, 1.5]$ and $\\text{supp}(r) = [0, 2]$, the support of the mixture is $\\text{supp}(q_{\\varepsilon}) = [0, 2]$. This matches the support of $p(x)$, so $p \\ll q_{\\varepsilon}$. This guarantees that the importance sampling estimator using $q_{\\varepsilon}$ is unbiased for any bounded function $g$.\n\nThe variance of the estimator is related to the second moment of the weighted samples, which involves the integral $\\int g(x)^2 \\frac{p(x)^2}{q_{\\varepsilon}(x)} dx$. For variance to be finite, this integral must converge.\nLet's analyze the integrand on the problematic interval $(1.5, 2]$. On this interval, $q(x) = 0$ and $p(x)=r(x)=1/2$. So, $q_{\\varepsilon}(x) = \\varepsilon r(x) = \\varepsilon/2$.\nThe contribution to the variance integral from this interval is:\n$$\n\\int_{1.5}^{2} g(x)^2 \\frac{p(x)^2}{q_{\\varepsilon}(x)} dx = \\int_{1.5}^{2} g(x)^2 \\frac{(1/2)^2}{\\varepsilon/2} dx = \\frac{1}{2\\varepsilon} \\int_{1.5}^{2} g(x)^2 dx.\n$$\nSince $g$ is bounded, this integral is finite for any fixed $\\varepsilon  0$. The integral over $[0, 1.5]$ is also finite because $q_{\\varepsilon}(x)$ is bounded below by a positive constant. Thus, for any $\\varepsilon \\in (0, 1)$, the variance is finite.\n\nHowever, as $\\varepsilon \\to 0$, the term $\\frac{1}{2\\varepsilon} \\int_{1.5}^{2} g(x)^2 dx$ will diverge to infinity, unless $g(x)=0$ on $(1.5, 2]$. Thus, the variance can become arbitrarily large as $\\varepsilon \\to 0$. The statement is correct on all three points.\n\nVerdict: **Correct**.\n\n**D. Multiplying the weights $\\frac{p(x)}{q(x)}$ by a fixed constant to cap their magnitude prevents bias and ensures finite variance even when $q$ has zero density on parts of the support of $p$.**\n\nThis technique is known as weight truncation. One defines a new weight $w'(x) = \\min(w(x), C)$ for some cap $C$. The resulting estimator is $\\hat{\\mu}'_n = \\frac{1}{n} \\sum g(X_i) w'(X_i)$. The expectation is $\\mathbb{E}[\\hat{\\mu}'_n] = \\mathbb{E}_q[g(X) \\min(w(X), C)]$. This is not equal to $\\mathbb{E}_q[g(X)w(X)]$, so this procedure *introduces* bias in order to control variance. The claim that it \"prevents bias\" is false.\nFurthermore, in the present problem, the issue is not large weights but a complete lack of samples in the region $(1.5, 2]$. The weights on the sampled region $[0, 1.5]$ are constant: $w(x) = \\frac{p(x)}{q(x)} = \\frac{1/2}{2/3} = 3/4$. Capping them has no effect unless $C  3/4$. More importantly, this technique does nothing to gather information from the region $(1.5, 2]$, so the bias due to the support mismatch remains. The statement is fundamentally flawed.\n\nVerdict: **Incorrect**.\n\n**E. Switching to the self-normalized importance sampling (SNIS) estimator removes the need for support coverage; unbiasedness holds even if $q$ assigns zero density where $p(x)  0$.**\n\nThe self-normalized importance sampling (SNIS) estimator is $\\hat{\\mu}_{SNIS} = \\frac{\\sum_{i=1}^n g(X_i) w(X_i)}{\\sum_{i=1}^n w(X_i)}$. This estimator is a ratio of random variables and is generally biased for finite sample sizes $n$, even when the support condition holds. Its main advantage is that it works when $p$ is known only up to a normalization constant. The claim that \"unbiasedness holds\" is false from the outset for finite $n$.\n\nLet's check for consistency (whether it converges to $\\mu$ as $n \\to \\infty$). By the Law of Large Numbers:\n- Numerator average: $\\frac{1}{n}\\sum g(X_i)w(X_i) \\to \\mathbb{E}_q[g(X)w(X)] = \\int_{0}^{1.5} g(x)p(x)dx$.\n- Denominator average: $\\frac{1}{n}\\sum w(X_i) \\to \\mathbb{E}_q[w(X)] = \\int_{0}^{1.5} p(x)dx$.\n\nSo, as $n \\to \\infty$, $\\hat{\\mu}_{SNIS} \\to \\frac{\\int_{0}^{1.5} g(x)p(x)dx}{\\int_{0}^{1.5} p(x)dx}$. This is the expected value of $g(X)$ with respect to a version of $p(x)$ that has been truncated to $[0, 1.5]$ and re-normalized. This is not, in general, equal to $\\mu = \\int_{0}^{2} g(x)p(x)dx$. For example, if $g(x) = x$, we found in thought process that the limit is $0.75$ while the true value is $1$. The estimator is not consistent, let alone unbiased. The support condition $p \\ll q$ is essential for the consistency of SNIS as well.\n\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{BC}$$"
        },
        {
            "introduction": "Having explored the principles of variance reduction and support conditions, it is time to apply these concepts to a practical, computational problem. Many real-world applications in science and engineering involve estimating the probability of rare events, where naive simulation is computationally infeasible. This hands-on exercise  tasks you with implementing an importance sampling algorithm to solve such a problem, guiding you through the process of deriving the estimator, handling numerical stability, and seeing its power in action. This is where theory meets practice, turning an intractable problem into a solvable one.",
            "id": "3241899",
            "problem": "Consider a combinatorial search space defined as the set $\\mathcal{X} = \\{1,2,\\dots,m\\}^n$ of all sequences $x = (x_1,x_2,\\dots,x_n)$ where each coordinate $x_i$ is an integer between $1$ and $m$. A randomized search algorithm that samples uniformly from $\\mathcal{X}$ induces a target distribution $p(x)$ over $\\mathcal{X}$ where $p(x) = m^{-n}$ for every $x \\in \\mathcal{X}$. Define a real-valued cost function $C(x)$ on $\\mathcal{X}$ of the form\n$$\nC(x) = \\sum_{i=1}^{n} a_i x_i,\n$$\nwhere $a_i  0$ are fixed coefficients. A solution $x$ is called satisfactory if $C(x) \\le T$ for a given threshold $T  0$. The quantity of interest is the probability\n$$\n\\pi = \\mathbb{P}_p\\big(C(X) \\le T\\big),\n$$\nwhere $X \\sim p$.\n\nDerive, from the definitions of expectation and probability and the change of measure principle, a computable importance sampling estimator for $\\pi$ that uses a tractable proposal distribution $q(x)$ over $\\mathcal{X}$, constructed as a product of independent, coordinate-wise distributions. In particular, let $q(x)$ be defined by independent coordinates with\n$$\nq(x) = \\prod_{i=1}^{n} q_i(x_i),\n$$\nand for a given parameter $\\beta \\ge 0$,\n$$\nq_i(k) \\propto \\exp\\big(-\\beta\\, a_i\\, k\\big), \\quad k \\in \\{1,2,\\dots,m\\}.\n$$\nThe normalization constants for each coordinate must be included to make $q_i$ a valid probability mass function on $\\{1,2,\\dots,m\\}$. Using this proposal, implement a program that:\n- Samples $N$ independent points $X^{(1)}, X^{(2)}, \\dots, X^{(N)}$ from $q$.\n- Computes a theoretically justified importance weight $w(x)$ for each sampled point.\n- Produces an estimator $\\hat{\\pi}$ for $\\pi$ based on the sampled points and their weights.\n\nYou must ensure numerical stability for large $n$ and $m$ by working in logarithmic space where appropriate. Use a fixed pseudo-random seed so that the results are reproducible. All sampling and estimation should be purely mathematical and unit-free.\n\nYour program must solve the following test suite of parameter settings. For each test case, return the single importance sampling estimate $\\hat{\\pi}$ as a floating-point number:\n\n- Test case $1$ (general case): $n = 12$, $m = 10$, $a_i = i$ for $i = 1,2,\\dots,12$, $T = 300$, $\\beta = 0.2$, $N = 30000$, seed $= 42$.\n- Test case $2$ (uniform baseline via $\\beta = 0$): $n = 12$, $m = 10$, $a_i = i$ for $i = 1,2,\\dots,12$, $T = 300$, $\\beta = 0.0$, $N = 30000$, seed $= 43$.\n- Test case $3$ (rare-event emphasis): $n = 20$, $m = 20$, $a_i = ((i-1) \\bmod 5) + 1$ for $i = 1,2,\\dots,20$, $T = 200$, $\\beta = 0.3$, $N = 40000$, seed $= 44$.\n- Test case $4$ (boundary where every sample is satisfactory): $n = 8$, $m = 15$, $a_i = 2$ for all $i$, $T = 240$, $\\beta = 0.5$, $N = 10000$, seed $= 45$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each floating-point number rounded to six decimal places, for example, $[r_1,r_2,r_3,r_4]$ where each $r_j$ corresponds to the estimate for test case $j$ in the order given above.",
            "solution": "The user wants to find an importance sampling estimator for a probability and implement it.\n\n### Step 1: Problem Validation\n\nThe problem is to estimate the probability $\\pi = \\mathbb{P}_p(C(X) \\le T)$, where the search space is $\\mathcal{X} = \\{1,2,\\dots,m\\}^n$, the target distribution is uniform $p(x) = m^{-n}$, and the cost function is $C(x) = \\sum_{i=1}^{n} a_i x_i$. The estimation must be done using importance sampling with a specific proposal distribution $q(x)$.\n\n**Extracted Givens:**\n- **Search Space**: $\\mathcal{X} = \\{1,2,\\dots,m\\}^n$.\n- **Target Distribution**: $p(x) = m^{-n}$ for all $x \\in \\mathcal{X}$.\n- **Cost Function**: $C(x) = \\sum_{i=1}^{n} a_i x_i$ with $a_i  0$.\n- **Event of Interest**: $C(x) \\le T$ for a threshold $T  0$.\n- **Quantity of Interest**: $\\pi = \\mathbb{P}_p\\big(C(X) \\le T\\big)$.\n- **Proposal Distribution**: $q(x) = \\prod_{i=1}^{n} q_i(x_i)$, where $q_i(k) \\propto \\exp(-\\beta\\, a_i\\, k)$ for $k \\in \\{1,2,\\dots,m\\}$ and $\\beta \\ge 0$.\n- **Implementation Details**: Use $N$ samples, ensure numerical stability using log-space, use fixed seeds, and solve for four specified test cases.\n\n**Validation using Extracted Givens:**\n1.  **Scientifically Grounded**: The problem is a standard application of importance sampling, a fundamental technique in Monte Carlo methods used widely in physics, statistics, and scientific computing. The mathematical formulation is based on established probability theory. The problem is valid.\n2.  **Well-Posed**: All parameters ($n, m, a_i, T, \\beta, N$, seed) are specified for each test case. The distributions and the quantity to be estimated are defined unambiguously. A unique form for the estimator can be derived. The problem is valid.\n3.  **Objective**: The problem is stated using formal mathematical language, with all terms explicitly defined. It is free from subjectivity. The problem is valid.\n4.  **Completeness and Consistency**: The problem is self-contained and provides all necessary information. The requirement to normalize the proposal distribution is consistent with the definition of a probability mass function. The problem is valid.\n5.  **Feasibility**: The specified parameters are within a range where computation is feasible. The prompt correctly anticipates numerical stability issues and suggests using log-space, which is a standard practice. The problem is valid.\n\n**Verdict**: The problem is valid and well-defined. I will proceed with deriving and implementing the solution.\n\n### Step 2: Derivation of the Importance Sampling Estimator\n\nThe probability $\\pi$ can be expressed as an expectation of an indicator function $\\mathbb{I}(\\cdot)$ with respect to the target distribution $p(x)$:\n$$\n\\pi = \\mathbb{E}_p[\\mathbb{I}(C(X) \\le T)] = \\sum_{x \\in \\mathcal{X}} p(x) \\mathbb{I}(C(x) \\le T)\n$$\nUsing the change of measure principle, we introduce the proposal distribution $q(x)$ to rewrite the expectation with respect to $q$:\n$$\n\\pi = \\sum_{x \\in \\mathcal{X}} \\frac{p(x)}{q(x)} q(x) \\mathbb{I}(C(x) \\le T) = \\mathbb{E}_q\\left[\\frac{p(X)}{q(X)} \\mathbb{I}(C(X) \\le T)\\right]\n$$\nThe term $w(x) = \\frac{p(x)}{q(x)}$ is the importance weight. The importance sampling estimator, $\\hat{\\pi}$, is the sample mean of this new quantity, based on $N$ samples $X^{(j)} \\sim q(x)$:\n$$\n\\hat{\\pi} = \\frac{1}{N} \\sum_{j=1}^{N} w(X^{(j)}) \\mathbb{I}(C(X^{(j)}) \\le T)\n$$\n\nTo implement this, we need the explicit forms of $p(x)$, $q(x)$, and $w(x)$.\nThe target distribution is $p(x) = m^{-n}$.\nThe proposal distribution is $q(x) = \\prod_{i=1}^{n} q_i(x_i)$. Each coordinate-wise distribution $q_i$ must be normalized:\n$$\nq_i(k) = \\frac{\\exp(-\\beta a_i k)}{Z_i}, \\quad \\text{for } k \\in \\{1, 2, \\dots, m\\}\n$$\nwhere $Z_i$ is the normalization constant (partition function):\n$$\nZ_i = \\sum_{k=1}^{m} \\exp(-\\beta a_i k)\n$$\nThe full proposal distribution is $q(x) = \\prod_{i=1}^{n} \\frac{\\exp(-\\beta a_i x_i)}{Z_i} = \\frac{\\exp(-\\beta \\sum_{i=1}^n a_i x_i)}{\\prod_{i=1}^n Z_i} = \\frac{\\exp(-\\beta C(x))}{\\prod_{i=1}^n Z_i}$.\n\nThe importance weight $w(x)$ is:\n$$\nw(x) = \\frac{p(x)}{q(x)} = \\frac{m^{-n}}{\\frac{\\exp(-\\beta C(x))}{\\prod_{i=1}^n Z_i}} = m^{-n} \\exp(\\beta C(x)) \\prod_{i=1}^n Z_i\n$$\n\n### Step 3: Numerical Implementation and Log-Space Computation\n\nDirect computation of $m^{-n}$ and products of many terms is numerically unstable. We must work in logarithmic space.\n\nThe log of the importance weight is:\n$$\n\\log w(x) = \\log(m^{-n}) + \\log(\\exp(\\beta C(x))) + \\log\\left(\\prod_{i=1}^n Z_i\\right)\n$$\n$$\n\\log w(x) = -n \\log m + \\beta C(x) + \\sum_{i=1}^n \\log Z_i\n$$\nThe normalization constant $\\log Z_i$ is computed using the log-sum-exp trick for stability:\n$$\n\\log Z_i = \\log\\left(\\sum_{k=1}^{m} \\exp(-\\beta a_i k)\\right)\n$$\nLet $v_k = -\\beta a_i k$. Let $v_{\\max} = \\max_{k} \\{v_k\\} = -\\beta a_i$ (since $a_i  0, \\beta \\ge 0, k \\ge 1$).\n$$\n\\log Z_i = v_{\\max} + \\log\\left(\\sum_{k=1}^{m} \\exp(v_k - v_{\\max})\\right)\n$$\nIn the special case $\\beta=0$, $q_i(k)$ becomes a uniform distribution on $\\{1,\\dots,m\\}$, so $q_i(k) = 1/m$. In this case, $q(x) = p(x)$, and all weights $w(x)=1$. Our formulas must correctly handle this: if $\\beta=0$, then $\\exp(-\\beta a_i k)=1$, so $Z_i = \\sum_{k=1}^m 1 = m$, and $\\log Z_i = \\log m$. The log-weight becomes $\\log w(x) = -n \\log m + 0 \\cdot C(x) + \\sum_{i=1}^n \\log m = -n \\log m + n \\log m = 0$, so $w(x)=1$, as expected.\n\nThe estimator can then be computed as:\n$$\n\\hat{\\pi} = \\frac{1}{N} \\sum_{j=1}^{N} \\mathbb{I}(C(X^{(j)}) \\le T) \\exp(\\log w(X^{(j)}))\n$$\n\n### Algorithm\nFor each test case:\n1.  **Setup**: Initialize parameters $n, m, a, T, \\beta, N$, and the random number generator with the given seed.\n2.  **Pre-computation**:\n    a. For each coordinate $i \\in \\{1, \\dots, n\\}$, calculate $\\log Z_i$. Handle the $\\beta=0$ case separately.\n    b. For each coordinate $i$, compute the PMF vector $P_i = [q_i(1), \\dots, q_i(m)]$. The probabilities are $q_i(k) = \\exp(-\\beta a_i k - \\log Z_i)$.\n3.  **Sampling**:\n    a. Create an $N \\times n$ matrix to store samples.\n    b. For each coordinate $i$, draw $N$ samples from the discrete distribution defined by the PMF $P_i$ and store them in the $i$-th column of the matrix. This is done using inverse transform sampling or a library function like `numpy.random.choice`.\n4.  **Estimation**:\n    a. For each of the $N$ sample vectors $X^{(j)}$, calculate the cost $C(X^{(j)}) = \\sum_i a_i X_i^{(j)}$. This can be vectorized as a matrix-vector product.\n    b. Identify the samples for which $C(X^{(j)}) \\le T$.\n    c. For these satisfactory samples only, calculate the log-weights $\\log w(X^{(j)})$.\n    d. Convert the log-weights to weights via exponentiation and sum them up.\n    e. Divide the sum by $N$ to get the final estimate $\\hat{\\pi}$.\n\nThis procedure provides a numerically stable and theoretically sound method to compute the desired estimate.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a set of importance sampling problems.\n    \"\"\"\n    test_cases = [\n        # (n, m, a_rule, T, beta, N, seed)\n        (12, 10, 'i',      300, 0.2, 30000, 42),\n        (12, 10, 'i',      300, 0.0, 30000, 43),\n        (20, 20, 'mod',    200, 0.3, 40000, 44),\n        ( 8, 15, 'const',  240, 0.5, 10000, 45),\n    ]\n\n    results = []\n    \n    for n, m, a_rule, T, beta, N, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate 'a' vector based on rule\n        if a_rule == 'i':\n            a = np.arange(1, n + 1, dtype=np.float64)\n        elif a_rule == 'mod':\n            a = ((np.arange(n)) % 5) + 1.0\n        elif a_rule == 'const':\n            a = np.full(n, 2.0)\n\n        # 2. Pre-compute log_Z and sampling distributions (PMFs)\n        log_Z = np.zeros(n)\n        all_pmfs = []\n        k_vals = np.arange(1, m + 1, dtype=np.float64)\n\n        for i in range(n):\n            if beta == 0.0:\n                log_Z[i] = np.log(m)\n                pmf = np.full(m, 1.0 / m)\n            else:\n                # Use log-sum-exp trick for numerical stability\n                v = -beta * a[i] * k_vals\n                # v_max is the first element since k_vals is increasing and arg is negative\n                v_max = v[0] \n                log_Z[i] = v_max + np.log(np.sum(np.exp(v - v_max)))\n                \n                # Compute Probability Mass Function (PMF)\n                log_pmf = v - log_Z[i]\n                pmf = np.exp(log_pmf)\n            \n            # Normalize to correct for minute floating-point inaccuracies\n            pmf /= np.sum(pmf)\n            all_pmfs.append(pmf)\n\n        # 3. Generate N samples from the proposal distribution q(x)\n        samples = np.zeros((N, n), dtype=int)\n        for i in range(n):\n            samples[:, i] = rng.choice(k_vals, size=N, p=all_pmfs[i])\n            \n        # 4. Compute costs for all N samples (vectorized)\n        costs = samples @ a\n        \n        # 5. Filter for satisfactory samples\n        satisfactory_mask = costs = T\n        satisfactory_costs = costs[satisfactory_mask]\n        \n        pi_hat = 0.0\n        if satisfactory_costs.size  0:\n            # 6. Compute importance weights for satisfactory samples\n            log_w_const = -n * np.log(m) + np.sum(log_Z)\n            log_weights = beta * satisfactory_costs + log_w_const\n            \n            # 7. Sum weights and compute the final estimator\n            # The sum is only over the satisfactory samples as per the estimator formula\n            total_sum_of_weights = np.sum(np.exp(log_weights))\n            pi_hat = total_sum_of_weights / N\n            \n        results.append(f\"{pi_hat:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}