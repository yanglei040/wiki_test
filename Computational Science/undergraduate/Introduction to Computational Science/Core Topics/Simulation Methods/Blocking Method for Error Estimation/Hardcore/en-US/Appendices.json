{
    "hands_on_practices": [
        {
            "introduction": "Many scientific simulations, such as those in statistical mechanics or quantum chemistry, produce time-correlated data. Naively calculating the standard error as if the data were independent leads to a severe underestimation of the true uncertainty. This first exercise guides you through implementing the blocking method from scratch and comparing its result to the naive (and incorrect) estimate, providing a concrete understanding of why this technique is essential for reliable error analysis.",
            "id": "2885597",
            "problem": "Consider a diffusion Monte Carlo (DMC) estimation of a fixed-node ground-state energy in quantum chemistry, where the measured local energy constitutes a correlated time series. The fixed-node approximation constrains the nodal surface and shifts the asymptotic mean energy by a constant bias, but does not alter the stationarity or the autocorrelation structure of the fluctuations around that mean. Consequently, the central statistical task is to estimate an unbiased standard error of the sample mean from correlated observations.\n\nStart from the following foundational base:\n- The sample mean of a stationary time series with finite integrated autocorrelation time obeys a central limit theorem for Markov chains: the scaled deviation of the sample mean tends to a normal distribution as the number of samples grows, and the variance of the sample mean depends on the autocorrelation function.\n- An autoregressive process of order one (AR(1)) with stationary mean and variance is a well-tested model for exponentially decaying time correlations.\n\nYour program must:\n- Generate synthetic DMC-like local-energy traces using an AR(1) Gaussian process with specified parameters.\n- Compute the naive standard error of the mean that ignores correlations.\n- Compute a blocked standard error using binary blocking (also called reblocking), which coarse-grains the series by contiguous averaging with block sizes that are powers of two, and chooses the largest block size subject to a minimum number of blocks constraint, thereby reducing the bias from time correlations.\n\nFormally, let the synthetic local-energy time series be defined by an AR(1) recursion\n$$\nX_{t} = \\mu + \\rho \\left( X_{t-1} - \\mu \\right) + \\varepsilon_{t}, \\quad \\varepsilon_{t} \\sim \\mathcal{N}\\!\\left(0, \\sigma_{\\varepsilon}^{2}\\right),\n$$\ninitialized in its stationary distribution with\n$$\nX_{0} \\sim \\mathcal{N}\\!\\left(\\mu, \\sigma^{2}\\right), \\quad \\sigma_{\\varepsilon}^{2} = \\sigma^{2}\\left(1-\\rho^{2}\\right),\n$$\nwhere $t \\in \\{1,2,\\dots,N-1\\}$, $N$ is the total number of samples, $\\mu$ is the fixed-node energy offset (a constant mean), $\\sigma$ is the stationary standard deviation of the local energy, and $\\rho \\in (-1,1)$ controls the correlation strength.\n\nGiven a realization $\\{X_{t}\\}_{t=0}^{N-1}$:\n- The naive standard error is the sample standard deviation divided by $\\sqrt{N}$, where the sample variance uses the unbiased divisor.\n- For blocking, define block sizes $b \\in \\{2^{0}, 2^{1}, 2^{2}, \\dots\\}$ and, for each block size $b$, form $n_{b} = \\lfloor N/b \\rfloor$ contiguous blocks of size $b$, take their means, and compute the sample variance of these block means (with unbiased divisor). The blocked standard error at block size $b$ is the square root of the variance of the block means divided by $n_{b}$. Choose the largest block size $b^{\\star}$ such that $n_{b^{\\star}} \\geq B_{\\min}$, where $B_{\\min}$ is a specified minimum number of blocks. Use this blocked standard error as the reported blocking-based error bar.\n\nYour program must implement the above without using any shortcut formulas for the answer and must output, for each test case, the following list of four floats:\n- The naive standard error of the mean.\n- The blocked standard error of the mean determined at $b^{\\star}$ with the specified $B_{\\min}$.\n- The ratio of the naive to the blocked standard error.\n- The estimated effective sample size defined as the ratio of the unbiased sample variance of the original series to the squared blocked standard error.\n\nAll floating-point outputs must be rounded to six decimal places.\n\nTest suite:\nUse the following four parameter sets, each given as a tuple $(N, \\mu, \\sigma, \\rho, \\text{seed}, B_{\\min})$:\n- Case $1$: $(N=\\;65536,\\; \\mu=\\;-0.5,\\; \\sigma=\\;1.0,\\; \\rho=\\;0.0,\\; \\text{seed}=\\;12345,\\; B_{\\min}=\\;16)$.\n- Case $2$: $(N=\\;65536,\\; \\mu=\\;-0.5,\\; \\sigma=\\;1.0,\\; \\rho=\\;0.8,\\; \\text{seed}=\\;24680,\\; B_{\\min}=\\;16)$.\n- Case $3$: $(N=\\;65536,\\; \\mu=\\;-0.5,\\; \\sigma=\\;1.0,\\; \\rho=\\;0.98,\\; \\text{seed}=\\;13579,\\; B_{\\min}=\\;16)$.\n- Case $4$: $(N=\\;50000,\\; \\mu=\\;-1.0,\\; \\sigma=\\;1.5,\\; \\rho=\\;0.9,\\; \\text{seed}=\\;98765,\\; B_{\\min}=\\;16)$.\n\nFinal output format:\nYour program should produce a single line of output containing a Python-style list with one entry per test case, where each entry is itself a list of the four floats in the order specified above. The list must be printed as a comma-separated list enclosed in square brackets, for example,\n$$\n\\left[\\left[\\text{naive}_{1},\\text{blocked}_{1},\\text{ratio}_{1},\\text{effN}_{1}\\right],\\left[\\text{naive}_{2},\\text{blocked}_{2},\\text{ratio}_{2},\\text{effN}_{2}\\right],\\dots\\right].\n$$\nNo physical units are required. Angles are not involved. Percentages must not be used; any fraction must be expressed as a decimal number. The program must be completely self-contained and require no input. Deterministic pseudorandom number generation must be ensured by the provided seeds. The computation must adhere to the definitions above for all steps.",
            "solution": "The problem presented is a well-defined exercise in the statistical analysis of correlated time series, a fundamental task in the processing of data from Monte Carlo simulations in computational physics and chemistry. The use of a first-order autoregressive, or AR($1$), process to model the local energy fluctuations from a Diffusion Monte Carlo (DMC) calculation is a standard and physically justified approximation. The objective is to correctly estimate the statistical uncertainty of the mean energy in the presence of temporal correlations. This requires moving beyond naive statistical estimators that assume independent data.\n\nValidation of the problem statement confirms that it is scientifically grounded, mathematically well-posed, and contains all necessary information to proceed. It is neither trivial nor ill-posed. Therefore, a rigorous solution can be constructed.\n\nThe solution will be developed in three stages, following the principles of statistical mechanics and time-series analysis.\n\nFirst, we must generate the synthetic data. The local-energy time series, denoted by a sequence of random variables $\\{X_t\\}_{t=0}^{N-1}$, is modeled as a stationary Gaussian AR($1$) process. Its evolution is given by the recursion relation:\n$$\nX_{t} = \\mu + \\rho \\left( X_{t-1} - \\mu \\right) + \\varepsilon_{t}\n$$\nwhere $\\mu$ is the constant mean energy, $\\rho$ is the autocorrelation coefficient between successive steps, and $\\varepsilon_t$ are independent and identically distributed Gaussian noise terms with mean zero and variance $\\sigma_{\\varepsilon}^2$. To ensure the process is stationary with a constant variance $\\sigma^2 = \\text{Var}(X_t)$, the noise variance must be set to $\\sigma_{\\varepsilon}^2 = \\sigma^2(1-\\rho^2)$. The simulation begins by drawing the initial state $X_0$ from the stationary distribution itself, which is a normal distribution with mean $\\mu$ and variance $\\sigma^2$, i.e., $X_0 \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Subsequent points $X_t$ for $t \\in \\{1, 2, \\dots, N-1\\}$ are generated via the recursion. This procedure guarantees that the entire generated series is a true sample from the specified stationary process.\n\nSecond, we compute the naive standard error of the mean. This estimator is predicated on the false assumption that the $N$ data points are uncorrelated. It is calculated as:\n$$\n\\text{SE}_{\\text{naive}} = \\frac{s}{\\sqrt{N}}\n$$\nwhere $s^2$ is the unbiased sample variance of the time series $\\{X_t\\}$:\n$$\ns^2 = \\frac{1}{N-1} \\sum_{t=0}^{N-1} (X_t - \\bar{X})^2\n$$\nHere, $\\bar{X}$ is the sample mean of the series. For any positive correlation ($\\rho  0$), this estimator will systematically underestimate the true uncertainty.\n\nThird, we implement the blocking method, a robust technique to account for serial correlation. The core principle is to coarse-grain the data into blocks that are sufficiently large such that the block averages are approximately uncorrelated. For a chosen block size $b$, the original series is partitioned into $n_b = \\lfloor N/b \\rfloor$ non-overlapping blocks. The mean of each block is computed, yielding a new, shorter time series of $n_b$ block averages.\nThe variance of the grand mean can then be estimated from the sample variance of these block means. The standard error for a given block size $b$ is:\n$$\n\\text{SE}_{\\text{blocked}}(b) = \\sqrt{\\frac{\\text{var}(\\text{block means})}{n_b}}\n$$\nwhere $\\text{var}(\\text{block means})$ is the unbiased sample variance of the $n_b$ block averages. As the block size $b$ increases, the block means become less correlated, and $\\text{SE}_{\\text{blocked}}(b)$ converges towards the true standard error of the mean. However, as $b$ increases, $n_b$ decreases, leading to a poorer statistical estimate of the variance. We must therefore select an optimal block size, $b^{\\star}$. The problem specifies a practical criterion: $b^{\\star}$ is the largest block size of the form $2^k$ for integer $k \\ge 0$ such that the number of blocks $n_{b^{\\star}}$ is at least a specified minimum, $B_{\\min}$. This procedure provides a balance between reducing bias from correlation and maintaining statistical stability.\n\nFinally, we calculate two derived quantities to characterize the impact of correlation. The ratio of the naive to the blocked standard error, $\\text{SE}_{\\text{naive}} / \\text{SE}_{\\text{blocked}}(b^{\\star})$, quantifies the degree of underestimation by the naive formula. The effective sample size, defined as:\n$$\nN_{\\text{eff}} = \\frac{s^2}{(\\text{SE}_{\\text{blocked}}(b^{\\star}))^2}\n$$\nrepresents the number of independent samples that would yield an equivalent statistical error. For positively correlated data, we expect $N_{\\text{eff}}  N$.\n\nThe implementation will execute this complete procedure for each parameter set provided in the test suite, ensuring deterministic output by using the specified pseudorandom number generator seeds. All numerical results will be rounded to six decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Generates synthetic DMC local-energy traces using an AR(1) process and\n    computes naive and blocked standard errors of the mean.\n    \"\"\"\n    \n    # Test cases are given as (N, mu, sigma, rho, seed, B_min).\n    test_cases = [\n        (65536, -0.5, 1.0, 0.0, 12345, 16),\n        (65536, -0.5, 1.0, 0.8, 24680, 16),\n        (65536, -0.5, 1.0, 0.98, 13579, 16),\n        (50000, -1.0, 1.5, 0.9, 98765, 16)\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N, mu, sigma, rho, seed, B_min = case\n\n        # Initialize the pseudorandom number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate the AR(1) time series.\n        # This series emulates correlated local energy data from a DMC simulation.\n        X = np.zeros(N)\n        sigma_eps = sigma * np.sqrt(1.0 - rho**2)\n\n        # Initialize from the stationary distribution.\n        X[0] = rng.normal(loc=mu, scale=sigma)\n        \n        # Generate the rest of the series via the AR(1) recursion.\n        for t in range(1, N):\n            epsilon_t = rng.normal(loc=0.0, scale=sigma_eps)\n            X[t] = mu + rho * (X[t-1] - mu) + epsilon_t\n\n        # Calculate the unbiased sample variance of the original series.\n        # This will be used for both naive error and effective sample size.\n        sample_variance = np.var(X, ddof=1)\n\n        # 1. Compute the naive standard error of the mean.\n        # This estimator ignores correlations and is expected to be inaccurate for rho != 0.\n        naive_se = np.sqrt(sample_variance / N)\n\n        # 2. Compute the blocked standard error of the mean.\n        # This involves reblocking the data with increasing block sizes.\n        blocked_se = -1.0  # Placeholder, will be updated in the loop.\n        k = 0\n        while True:\n            # Block sizes are powers of two.\n            b = 2**k\n            n_b = N // b\n            \n            # Stop if the number of blocks is less than the required minimum.\n            # The result from the previous iteration is the correct one.\n            if n_b  B_min:\n                break\n            \n            # If n_b becomes 1, variance calculation is impossible.\n            # B_min  1 ensures this path is not taken for the final result.\n            if n_b = 1:\n                # If this is the first iteration (k=0), it means N  B_min\n                # and no valid blocking is possible. Set SE to NaN.\n                if k == 0:\n                    blocked_se = np.nan\n                break\n\n            # Reshape the data into blocks. Leftover data at the end of the series is discarded.\n            num_elements_to_block = n_b * b\n            data_to_block = X[:num_elements_to_block]\n            blocks = data_to_block.reshape((n_b, b))\n            \n            # Compute the means of the blocks.\n            block_means = np.mean(blocks, axis=1)\n            \n            # Compute the unbiased variance of the block means.\n            var_block_means = np.var(block_means, ddof=1)\n            \n            # The standard error of the grand mean, estimated from this blocking level.\n            # This value is updated and stored. The last valid one is used.\n            blocked_se = np.sqrt(var_block_means / n_b)\n            \n            k += 1\n\n        # 3. Compute the ratio of naive to blocked standard error.\n        ratio = naive_se / blocked_se\n        \n        # 4. Compute the effective sample size.\n        effective_n = sample_variance / (blocked_se**2)\n\n        # Collect and round results to six decimal places.\n        result_list = [\n            round(naive_se, 6),\n            round(blocked_se, 6),\n            round(ratio, 6),\n            round(effective_n, 6)\n        ]\n        all_results.append(result_list)\n\n    # Format the final output string to be a compact list of lists.\n    # e.g., [[item1,item2],[item3,item4]]\n    # The map(str, ...) converts each inner list to its string representation.\n    # The ','.join(...) combines them into a single string.\n    # The outer f-string adds the enclosing brackets.\n    final_output_str = f\"[{','.join(map(str, all_results))}]\"\n    final_output_str = final_output_str.replace(\" \", \"\")\n\n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A robust way to understand an algorithm is to test its behavior in a controlled experiment. This practice solidifies your understanding of the blocking method  by having you computationally verify its core purpose. You will randomly permute a correlated time series, an action that destroys its temporal structure, and observe how the blocking estimator's result changes, confirming that it is the data's *order* that the method corrects for.",
            "id": "3102616",
            "problem": "You are given the task of validating the blocking method for error estimation of a sample mean under serial dependence. The blocking method partitions a time-ordered sequence into non-overlapping groups of equal size to form block means and estimates the variance of the sample mean from the variability across blocks. The core principle to be validated is that destroying temporal dependence by a random permutation of time indices should make the blocking-based variance estimator collapse to the independent and identically distributed (IID) variance formula.\n\nBase definitions and facts to be used:\n- A discrete time series $\\{X_t\\}_{t=1}^n$ has sample mean $\\bar{X} = \\frac{1}{n} \\sum_{t=1}^n X_t$.\n- For IID data with variance $\\sigma^2$, the variance of the sample mean is $\\operatorname{Var}(\\bar{X}) = \\sigma^2 / n$.\n- In the blocking method with block size $b$, the first $m = \\lfloor n/b \\rfloor$ non-overlapping blocks produce block means $Y_i = \\frac{1}{b} \\sum_{t=(i-1)b+1}^{ib} X_t$ for $i=1,\\dots,m$. When blocks are sufficiently large so that block means are approximately independent, the variance of $\\bar{X}$ is approximated using the variability among $\\{Y_i\\}$.\n- A random permutation of indices $\\pi$ applied to the sequence, giving $X'_t = X_{\\pi(t)}$, preserves the marginal distribution of $\\{X_t\\}$ but destroys time ordering and therefore serial dependence.\n\nTask:\n1. For each specified test case, generate a time series by an Autoregressive (AR) model of order one. The series satisfies $X_t = \\phi X_{t-1} + \\epsilon_t$ with $|\\phi|  1$, where $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$ are independent Gaussian innovations, and $X_0$ is drawn from the stationary distribution with variance $\\sigma_\\epsilon^2 / (1 - \\phi^2)$ for stationarity. Use a fixed random seed per test case.\n2. For a list of block sizes $b$, compute the blocking-based variance estimator of the sample mean as follows: form $m = \\lfloor n/b \\rfloor$ non-overlapping block means $Y_1,\\dots,Y_m$, compute the unbiased sample variance $s_Y^2$ of $\\{Y_i\\}$ with Bessel’s correction, and estimate the variance of the sample mean by $\\hat{\\sigma}^2(b) = s_Y^2 / m$. This uses the identity $\\operatorname{Var}\\left(\\frac{1}{m}\\sum_{i=1}^m Y_i\\right) = \\operatorname{Var}(Y_i)/m$ under independence of $\\{Y_i\\}$.\n3. Compute the naive IID variance estimator of the sample mean $\\hat{\\sigma}_{\\text{iid}}^2 = s_X^2 / n$, where $s_X^2$ is the unbiased sample variance of $\\{X_t\\}_{t=1}^n$.\n4. Create a randomly permuted version of the series by applying a random permutation to indices, giving $\\{X'_t\\}_{t=1}^n$. For each $b$, compute $\\hat{\\sigma}'^2(b)$ from $\\{X'_t\\}$ via blocking and compare it to $\\hat{\\sigma}_{\\text{iid}}^2$.\n5. For each test case, produce a boolean that is true if, for all listed block sizes $b$, the relative error $\\left|\\hat{\\sigma}'^2(b) - \\hat{\\sigma}_{\\text{iid}}^2\\right| / \\hat{\\sigma}_{\\text{iid}}^2$ is less than the specified tolerance $\\varepsilon$; otherwise false. This validates that the blocking estimator collapses to the IID variance when order is scrambled.\n\nConstraints and implementation details:\n- Use only the first $m b$ points if $b$ does not divide $n$; in the test suite below, each $b$ divides $n$ so that $m$ is an integer.\n- Use unbiased sample variances with Bessel’s correction.\n- Use a fixed pseudorandom number generator seed per test case to ensure reproducibility and independent randomness across test cases.\n\nTest suite:\n- Test case $1$ (general happy path with strong autocorrelation): $n = 8192$, $\\phi = 0.8$, $\\sigma_\\epsilon = 1$, block sizes $b \\in \\{1,2,4,8,16,32,64,128,256\\}$, tolerance $\\varepsilon = 0.08$.\n- Test case $2$ (weak autocorrelation): $n = 4096$, $\\phi = 0.2$, $\\sigma_\\epsilon = 1$, block sizes $b \\in \\{1,2,4,8,16,32,64,128,256,512\\}$, tolerance $\\varepsilon = 0.08$.\n- Test case $3$ (IID boundary, $\\phi = 0$): $n = 4096$, $\\phi = 0$, $\\sigma_\\epsilon = 1$, block sizes $b \\in \\{1,2,4,8,16,32,64,128,256,512\\}$, tolerance $\\varepsilon = 0.08$.\n- Test case $4$ (edge case with very strong autocorrelation and wider $b$ coverage): $n = 8192$, $\\phi = 0.95$, $\\sigma_\\epsilon = 1$, block sizes $b \\in \\{1,2,4,8,16,32,64,128,256,512\\}$, tolerance $\\varepsilon = 0.08$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for all four test cases as a comma-separated list enclosed in square brackets, for example, $[\\text{result1},\\text{result2},\\text{result3},\\text{result4}]$, where each $\\text{result}$ is a boolean indicating whether the collapse condition holds across the listed block sizes in that test case.",
            "solution": "The task is to validate a core principle of the blocking method for error estimation in time series analysis. Specifically, we will verify that for a time series with temporal dependence, randomly permuting the data points destroys this dependence, causing the blocking method's estimate for the variance of the sample mean to collapse to the simpler formula used for independent and identically distributed (IID) data.\n\nThe validation will be performed numerically for several test cases, each defined by an Autoregressive model of order one (AR($1$)). For each case, we follow a precise algorithmic procedure.\n\nFirst, we generate a stationary time series $\\{X_t\\}_{t=1}^n$ of length $n$ from an AR($1$) process given by the equation:\n$$X_t = \\phi X_{t-1} + \\epsilon_t$$\nwhere $\\phi$ is the autoregressive coefficient satisfying $|\\phi|  1$, and $\\{\\epsilon_t\\}$ are IID Gaussian random variables (innovations) drawn from a normal distribution with mean $0$ and variance $\\sigma_\\epsilon^2$, i.e., $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$. To ensure stationarity, the initial value $X_0$ is drawn from the stationary distribution of the process, which is $\\mathcal{N}(0, \\sigma_X^2)$ with variance $\\sigma_X^2 = \\sigma_\\epsilon^2 / (1 - \\phi^2)$. For each test case, a fixed pseudorandom number generator seed is used to guarantee reproducibility.\n\nSecond, we compute the standard IID-based estimator for the variance of the sample mean, $\\bar{X} = \\frac{1}{n} \\sum_{t=1}^n X_t$. This estimator, which we denote $\\hat{\\sigma}_{\\text{iid}}^2$, is valid only if the data points are uncorrelated. It is calculated as:\n$$\\hat{\\sigma}_{\\text{iid}}^2 = \\frac{s_X^2}{n}$$\nwhere $s_X^2$ is the unbiased sample variance of the time series $\\{X_t\\}_{t=1}^n$, computed with Bessel's correction:\n$$s_X^2 = \\frac{1}{n-1} \\sum_{t=1}^n (X_t - \\bar{X})^2$$\n\nThird, we create a permuted time series $\\{X'_t\\}_{t=1}^n$ by applying a random permutation to the indices of the original series $\\{X_t\\}_{t=1}^n$. This operation preserves the set of values and thus the sample mean and variance ($s_{X'}^2 = s_X^2$), but it destroys the temporal correlation structure inherent in the AR($1$) process. The resulting sequence $\\{X'_t\\}$ is an exchangeable sequence.\n\nFourth, for the permuted series $\\{X'_t\\}$, we apply the blocking method for a list of block sizes $b$. The series is partitioned into $m = \\lfloor n/b \\rfloor$ non-overlapping blocks. For the given test cases, $n$ is always divisible by $b$, so $m=n/b$. We calculate the mean of each block:\n$$Y'_i = \\frac{1}{b} \\sum_{t=(i-1)b+1}^{ib} X'_t \\quad \\text{for } i = 1, \\dots, m$$\nThe blocking method estimates the variance of the sample mean by treating these block means $\\{Y'_i\\}_{i=1}^m$ as approximately independent data points. The estimator, which we denote $\\hat{\\sigma}'^2(b)$, is given by:\n$$\\hat{\\sigma}'^2(b) = \\frac{s_{Y'}^2}{m}$$\nwhere $s_{Y'}^2$ is the unbiased sample variance of the block means $\\{Y'_i\\}$:\n$$s_{Y'}^2 = \\frac{1}{m-1} \\sum_{i=1}^m (Y'_i - \\bar{Y'})^2$$\nHere, $\\bar{Y'}$ is the mean of the block means, which is numerically identical to the overall sample mean $\\bar{X'}$.\n\nFinally, we perform the validation. The core hypothesis is that for the permuted (temporally unstructured) data, the blocking estimate $\\hat{\\sigma}'^2(b)$ should be consistent with the IID estimate $\\hat{\\sigma}_{\\text{iid}}^2$, regardless of the block size $b$. We test this by checking if the relative error between the two estimates is smaller than a given tolerance $\\varepsilon$ for all specified block sizes $b$. For each test case, the final result is a boolean value, which is `True` if the following condition holds for all $b$ in its list of block sizes, and `False` otherwise:\n$$\\frac{\\left| \\hat{\\sigma}'^2(b) - \\hat{\\sigma}_{\\text{iid}}^2 \\right|}{\\hat{\\sigma}_{\\text{iid}}^2}  \\varepsilon$$\n\nTheoretically, this collapse is expected because for an exchangeable sequence, the expected value of the sample variance of block means, $E[s_{Y'}^2]$, can be shown to approximate $b \\cdot \\operatorname{Var}(\\bar{X'})$ where $\\bar{X'}$ is the sample mean of $n$ correlated draws. In our case of a random permutation of a fixed set of numbers, we can show through an Analysis of Variance (ANOVA) argument that the expected value of the mean square between blocks equals the overall sample variance of the data, $E[MSB] = s_X^2$. Since $\\hat{\\sigma}'^2(b) = MSB/n$ and $\\hat{\\sigma}_{\\text{iid}}^2 = s_X^2/n$, we expect their values to be close. The tolerance $\\varepsilon$ accounts for the statistical fluctuation of $MSB$ around its expected value, which is particularly relevant when the number of blocks $m$ is small.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the validation for all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'n': 8192, 'phi': 0.8, 'sigma_eps': 1.0, 'block_sizes': [1, 2, 4, 8, 16, 32, 64, 128, 256], 'tolerance': 0.08},\n        {'n': 4096, 'phi': 0.2, 'sigma_eps': 1.0, 'block_sizes': [1, 2, 4, 8, 16, 32, 64, 128, 256, 512], 'tolerance': 0.08},\n        {'n': 4096, 'phi': 0.0, 'sigma_eps': 1.0, 'block_sizes': [1, 2, 4, 8, 16, 32, 64, 128, 256, 512], 'tolerance': 0.08},\n        {'n': 8192, 'phi': 0.95, 'sigma_eps': 1.0, 'block_sizes': [1, 2, 4, 8, 16, 32, 64, 128, 256, 512], 'tolerance': 0.08},\n    ]\n\n    results = []\n    # Use a different seed for each test case for independent experiments.\n    # The seeds are fixed to ensure the overall result is reproducible.\n    for i, case in enumerate(test_cases):\n        result = validate_blocking_collapse(**case, seed=i)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef validate_blocking_collapse(n, phi, sigma_eps, block_sizes, tolerance, seed):\n    \"\"\"\n    Performs the validation for a single test case.\n\n    Args:\n        n (int): Length of the time series.\n        phi (float): Autoregressive coefficient.\n        sigma_eps (float): Standard deviation of innovations.\n        block_sizes (list): List of block sizes to test.\n        tolerance (float): Relative error tolerance.\n        seed (int): Seed for the pseudorandom number generator.\n\n    Returns:\n        bool: True if the collapse condition holds for all block sizes, False otherwise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate AR(1) time series\n    if abs(phi)  1.0:\n        var_x_stationary = sigma_eps**2 / (1 - phi**2)\n    else:\n        # This case is not expected based on problem constraints but included for robustness.\n        var_x_stationary = 1.0 \n    \n    x0 = rng.normal(loc=0, scale=np.sqrt(var_x_stationary))\n    innovations = rng.normal(loc=0, scale=sigma_eps, size=n)\n    \n    x = np.zeros(n)\n    x[0] = phi * x0 + innovations[0]\n    for t in range(1, n):\n        x[t] = phi * x[t-1] + innovations[t]\n        \n    # 2. Compute naive IID variance estimator for the sample mean\n    s_x_sq = np.var(x, ddof=1)\n    sigma_iid_sq = s_x_sq / n\n\n    # 3. Create a randomly permuted version of the series\n    x_prime = rng.permutation(x)\n\n    # 4. For each block size, compute the blocking estimator and check the condition\n    for b in block_sizes:\n        m = n // b\n        \n        # Ensure there are at least 2 blocks to compute variance\n        if m  2:\n            # According to problem specification, m is always = 8.\n            # If for some reason m  2, the variance s_Y^2 is undefined.\n            # We treat this as a failure of the condition.\n            return False\n\n        # Reshape the permuted series into blocks\n        blocks = x_prime.reshape((m, b))\n        \n        # Compute means of the blocks\n        y_prime = np.mean(blocks, axis=1)\n        \n        # Compute the unbiased sample variance of the block means\n        s_y_prime_sq = np.var(y_prime, ddof=1)\n        \n        # Compute the blocking-based variance estimator for the sample mean\n        sigma_prime_sq_b = s_y_prime_sq / m\n\n        # 5. Check if the relative error is within the specified tolerance\n        if sigma_iid_sq == 0:\n            # This is extremely unlikely but would occur if all x_t are identical.\n            # If both estimators are 0, the error is 0. If not, it's infinite.\n            if sigma_prime_sq_b != 0:\n                return False\n        else:\n            relative_error = np.abs(sigma_prime_sq_b - sigma_iid_sq) / sigma_iid_sq\n            if relative_error = tolerance:\n                # If the condition fails for any block size, the entire test case fails.\n                return False\n\n    # If the loop completes, the condition held for all block sizes.\n    return True\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Beyond analyzing past data, a key skill in computational science is planning future experiments. This final exercise moves from analysis to prediction, demonstrating how the blocking method can answer the practical question: \"How long do I need to run my simulation?\". By using the blocking analysis from a pilot run to estimate the effective variance, you will learn to project the total computational effort needed to meet a specific goal for statistical accuracy.",
            "id": "3102661",
            "problem": "You are given a univariate time series and asked to estimate the minimum sample size needed to achieve a prescribed relative error under confidence level control, using the blocking method to account for temporal autocorrelation. The derivation must start from the Central Limit Theorem (CLT) for weakly dependent stationary sequences and the definition of integrated autocorrelation time. The algorithm must avoid shortcut formulas by explicitly constructing blocked averages and extrapolating a stabilized blocked variance. Your program will generate synthetic time series for a provided test suite, perform the blocking analysis, and output the required minimum sample sizes.\n\nStarting point and definitions. Consider a strictly stationary time series $\\{X_t\\}_{t=1}^N$ with mean $\\mu$, variance $\\sigma_X^2$, and autocorrelation function $\\rho_k = \\operatorname{Corr}(X_t,X_{t+k})$. By the Central Limit Theorem for weakly dependent sequences, the sample mean $\\bar{X} = \\frac{1}{N}\\sum_{t=1}^N X_t$ is approximately normal with mean $\\mu$ and variance\n$$\n\\operatorname{Var}(\\bar{X}) \\approx \\frac{\\sigma_X^2}{N}\\left(1 + 2\\sum_{k=1}^{\\infty}\\rho_k\\right) = \\frac{\\sigma_{\\mathrm{eff}}^2}{N},\n$$\nwhere $\\sigma_{\\mathrm{eff}}^2 = \\sigma_X^2 \\left(1 + 2\\sum_{k=1}^{\\infty}\\rho_k\\right)$ is the effective variance per independent sample, accounting for autocorrelation. In practice, $\\sigma_{\\mathrm{eff}}^2$ is unknown and must be estimated from the data.\n\nBlocking estimator. For a chosen block size $b \\in \\mathbb{N}$, define $N_b = \\left\\lfloor \\frac{N}{b} \\right\\rfloor$ non-overlapping blocks and block means\n$$\nY_j = \\frac{1}{b}\\sum_{i=1}^{b} X_{(j-1)b + i}, \\quad j = 1,\\dots,N_b.\n$$\nLet $s_Y^2(b)$ denote the unbiased sample variance of $\\{Y_j\\}_{j=1}^{N_b}$. A standard blocked estimator for the effective variance is\n$$\n\\hat{\\sigma}^2(b) = b \\, s_Y^2(b),\n$$\nwhich ideally stabilizes (approaches a plateau) as $b \\to \\infty$, reflecting $\\sigma_{\\mathrm{eff}}^2$. To both regularize finite $b$ effects and enforce enough averaging for the Central Limit Theorem at the block level, use only block sizes with $N_b \\geq 30$, and linearly extrapolate $\\hat{\\sigma}^2(b)$ as a function of $1/b$ to $1/b \\to 0$.\n\nTarget error criterion. For a target relative error $\\epsilon  0$ and a two-sided confidence level encoded by the standard normal quantile $z  0$, require\n$$\nz \\, \\frac{\\hat{\\sigma}}{\\sqrt{N}} \\le \\epsilon \\, |\\bar{X}|,\n$$\nwhere $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$ is the extrapolated effective standard deviation and $\\bar{X}$ is the sample mean of the entire series. Solve for the minimal integer $N$ satisfying this inequality.\n\nAlgorithm requirements.\n- Construct the sequence of block sizes $b$ as powers of two, namely $b \\in \\{2^k: k \\in \\mathbb{N}_0\\}$, restricted to those for which $N_b = \\left\\lfloor \\frac{N}{b} \\right\\rfloor \\ge 30$. For each such $b$, compute $\\hat{\\sigma}^2(b) = b \\, s_Y^2(b)$, where $s_Y^2(b)$ is computed with the unbiased divisor $N_b - 1$. Use only the first $M = b N_b$ samples to form complete blocks.\n- Perform a linear regression of $y = \\hat{\\sigma}^2(b)$ against $x = 1/b$ over the selected $b$ values to obtain an intercept at $x = 0$, which defines the extrapolated $\\hat{\\sigma}^2$. If the extrapolated intercept is negative due to sampling variability, clamp it to $0$ before taking the square root.\n- Compute $\\bar{X}$ using all $N$ available samples.\n- Compute the minimal integer\n$$\nN_{\\min} = \\left\\lceil \\left(\\frac{z \\, \\hat{\\sigma}}{\\epsilon \\, |\\bar{X}|}\\right)^2 \\right\\rceil.\n$$\nIf $|\\bar{X}|$ is numerically extremely small, use a small positive floor such as $10^{-15}$ to avoid division by zero. Assume $\\bar{X} \\ne 0$ for the given test suite.\n\nTest suite. Your program must implement the following four deterministic test cases by generating the time series internally, each of length $N_0 = 65536$:\n\n- Case $1$ (Independent and Identically Distributed (IID) normal): $\\mu = 1.0$, $\\sigma_X = 2.0$, seed $= 20231102$, $\\epsilon = 0.05$, $z = 1.96$.\n- Case $2$ (Autoregressive of order one (AR(1))): $\\phi = 0.7$, $\\mu = 1.0$, stationary standard deviation $\\sigma_X = 2.0$, seed $= 20231103$, $\\epsilon = 0.05$, $z = 1.96$. Generate the process by $X_t = \\mu + \\phi (X_{t-1} - \\mu) + \\eta_t$ with $\\eta_t \\sim \\mathcal{N}(0,\\sigma_\\eta^2)$ and $\\sigma_\\eta = \\sigma_X \\sqrt{1 - \\phi^2}$, and initialize $X_1$ from the stationary distribution $\\mathcal{N}(\\mu,\\sigma_X^2)$.\n- Case $3$ (AR(1), stronger correlation): $\\phi = 0.95$, $\\mu = 1.0$, $\\sigma_X = 2.0$, seed $= 20231104$, $\\epsilon = 0.05$, $z = 1.96$, generated as in Case $2$.\n- Case $4$ (IID normal with small mean): $\\mu = 0.1$, $\\sigma_X = 1.0$, seed $= 20231105$, $\\epsilon = 0.02$, $z = 1.96$.\n\nOutput specification. Your program must produce a single line containing the list of the four integers $[N_{\\min}^{(1)}, N_{\\min}^{(2)}, N_{\\min}^{(3)}, N_{\\min}^{(4)}]$ in that order, with no extra whitespace except commas. For example, the output format must be exactly like $[123,456,789,1011]$.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and provides a complete and consistent set of instructions. The task is to estimate the minimum sample size $N_{\\min}$ required to achieve a specified relative error $\\epsilon$ on the sample mean of a stationary time series, accounting for autocorrelation. The methodology is based on the blocking method for estimating the effective variance of the sample mean, combined with linear extrapolation. The solution involves generating synthetic time series, performing the blocking analysis, and calculating $N_{\\min}$ for four distinct test cases.\n\nThe methodological foundation is the Central Limit Theorem for weakly dependent stationary sequences. For a time series $\\{X_t\\}_{t=1}^N$ with mean $\\mu$ and variance $\\sigma_X^2$, the sample mean $\\bar{X} = \\frac{1}{N}\\sum_{t=1}^N X_t$ is approximately normally distributed with mean $\\mu$ and variance given by:\n$$\n\\operatorname{Var}(\\bar{X}) \\approx \\frac{\\sigma_X^2}{N}\\left(1 + 2\\sum_{k=1}^{\\infty}\\rho_k\\right) = \\frac{\\sigma_{\\mathrm{eff}}^2}{N}\n$$\nwhere $\\rho_k$ is the autocorrelation at lag $k$, and $\\sigma_{\\mathrm{eff}}^2$ is the effective variance. This effective variance captures the impact of correlations on the uncertainty of the sample mean. Our goal is to estimate $\\sigma_{\\mathrm{eff}}^2$ from the data.\n\nThe problem specifies a detailed algorithm to be implemented for each test case.\n\n**Part 1: Time Series Generation**\nFor each test case, a time series of length $N_0 = 65536$ is generated based on the provided parameters. A deterministic outcome is ensured by using a specified random number generator seed for each case.\n\n- **Independent and Identically Distributed (IID) Normal Series (Cases 1 and 4)**: The series $\\{X_t\\}$ is generated by drawing $N_0$ samples from a normal distribution $\\mathcal{N}(\\mu, \\sigma_X^2)$, where the mean $\\mu$ and standard deviation $\\sigma_X$ are given for the specific case.\n\n- **Autoregressive of Order 1 (AR(1)) Series (Cases 2 and 3)**: The AR($1$) process is defined by the recurrence relation:\n$$\nX_t = \\mu + \\phi (X_{t-1} - \\mu) + \\eta_t\n$$\nwhere $\\phi$ is the autoregressive coefficient and $\\{\\eta_t\\}$ is a white noise process with $\\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$. To ensure the process has the desired stationary standard deviation $\\sigma_X$, the variance of the innovation term $\\eta_t$ is set to $\\sigma_\\eta^2 = \\sigma_X^2(1 - \\phi^2)$. The generation begins by drawing the first point $X_1$ from the stationary distribution $\\mathcal{N}(\\mu, \\sigma_X^2)$. Subsequent points $X_t$ for $t=2, \\dots, N_0$ are generated iteratively using the recurrence relation.\n\n**Part 2: Blocking Analysis and Variance Extrapolation**\nOnce the time series is generated, the effective variance $\\sigma_{\\mathrm{eff}}^2$ is estimated using the blocking method with extrapolation.\n\n1.  **Selection of Block Sizes**: The analysis uses block sizes $b$ that are powers of two, $b = 2^k$ for $k = 0, 1, 2, \\dots$. A block size $b$ is considered valid only if it results in at least $N_b = \\lfloor N_0/b \\rfloor \\ge 30$ blocks. This condition ensures that the Central Limit Theorem can be reasonably applied to the block means themselves. For $N_0=65536$, the valid block sizes are $b \\in \\{1, 2, 4, \\dots, 2048\\}$.\n\n2.  **Calculation of Blocked Variance**: For each valid block size $b$, the following steps are performed:\n    a. The time series is partitioned into $N_b = \\lfloor N_0/b \\rfloor$ non-overlapping blocks of size $b$. Since $N_0 = 2^{16}$, for all valid block sizes $b=2^k$, the division is exact, and all $N_0$ data points are used.\n    b. The mean of each block, $Y_j = \\frac{1}{b}\\sum_{i=1}^{b} X_{(j-1)b + i}$, is calculated for $j = 1, \\dots, N_b$.\n    c. The unbiased sample variance of these block means, $s_Y^2(b)$, is computed using a divisor of $N_b - 1$.\n    d. The blocked variance estimator for that block size is calculated as $\\hat{\\sigma}^2(b) = b \\cdot s_Y^2(b)$. As $b$ increases, correlations within blocks are averaged out, and the block means $Y_j$ become progressively less correlated. In the limit $b \\to \\infty$, the $Y_j$ become effectively independent, and $\\hat{\\sigma}^2(b)$ converges to $\\sigma_{\\mathrm{eff}}^2$.\n\n3.  **Linear Extrapolation**: The estimator $\\hat{\\sigma}^2(b)$ suffers from finite-size bias, which, for large $b$, is often proportional to $1/b$. To correct for this, we extrapolate to the limit $b \\to \\infty$ (i.e., $1/b \\to 0$). The pairs $(x, y) = (1/b, \\hat{\\sigma}^2(b))$ for all valid block sizes are collected. A simple linear regression of $y$ on $x$ is performed. The intercept of the resulting regression line provides the extrapolated estimate for the effective variance, denoted as $\\hat{\\sigma}^2$. If sampling fluctuations lead to a negative intercept, it is physically meaningless and is clamped to $0$. The final estimate for the effective standard deviation is $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$.\n\n**Part 3: Calculation of Minimum Sample Size**\nWith the estimated effective standard deviation $\\hat{\\sigma}$ and the full sample mean $\\bar{X} = \\frac{1}{N_0}\\sum_{t=1}^{N_0} X_t$, the problem is to find the minimum sample size $N_{\\min}$ that satisfies the target error criterion:\n$$\nz \\, \\frac{\\hat{\\sigma}}{\\sqrt{N_{\\min}}} \\le \\epsilon \\, |\\bar{X}|\n$$\nHere, $z$ is the standard normal quantile corresponding to the desired confidence level (e.g., $z=1.96$ for $95\\%$ confidence) and $\\epsilon$ is the target relative error. Solving for $N_{\\min}$ gives:\n$$\nN_{\\min} \\ge \\left(\\frac{z \\, \\hat{\\sigma}}{\\epsilon \\, |\\bar{X}|}\\right)^2\n$$\nSince $N_{\\min}$ must be an integer, we take the ceiling of the right-hand side:\n$$\nN_{\\min} = \\left\\lceil \\left(\\frac{z \\, \\hat{\\sigma}}{\\epsilon \\, |\\bar{X}|}\\right)^2 \\right\\rceil\n$$\nThis calculation is performed for each of the four test cases to produce the final output. The problem statement guarantees that $\\bar{X}$ is not pathologically close to zero, so no special handling for division by zero is needed for this specific test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef generate_series(params):\n    \"\"\"\n    Generates a time series based on the provided parameters.\n    \"\"\"\n    N0 = params['N0']\n    mu = params['mu']\n    sigma_x = params['sigma_x']\n    seed = params['seed']\n    series_type = params['type']\n    \n    rng = np.random.default_rng(seed)\n\n    if series_type == 'iid':\n        return rng.normal(loc=mu, scale=sigma_x, size=N0)\n    elif series_type == 'ar1':\n        phi = params['phi']\n        sigma_eta = sigma_x * np.sqrt(1 - phi**2)\n        \n        x = np.zeros(N0)\n        # Initialize with a draw from the stationary distribution\n        x[0] = rng.normal(loc=mu, scale=sigma_x)\n        \n        # Generate the rest of the series\n        for t in range(1, N0):\n            eta_t = rng.normal(loc=0, scale=sigma_eta)\n            x[t] = mu + phi * (x[t-1] - mu) + eta_t\n        return x\n    else:\n        raise ValueError(f\"Unknown series type: {series_type}\")\n\ndef estimate_effective_variance(series, N0):\n    \"\"\"\n    Estimates the effective variance using the blocking method with extrapolation.\n    \"\"\"\n    # 1. Determine valid block sizes\n    block_sizes = []\n    k = 0\n    while True:\n        b = 2**k\n        if N0 // b  30:\n            break\n        block_sizes.append(b)\n        k += 1\n\n    # 2. Calculate blocked variance for each block size\n    blocked_variances = []\n    inv_block_sizes = []\n\n    for b in block_sizes:\n        Nb = N0 // b\n        # Since N0 is a power of 2, N0 is divisible by all valid b, so M=N0\n        M = Nb * b\n        \n        # Reshape data into blocks\n        block_data = series[:M].reshape(Nb, b)\n        \n        # Calculate block means\n        block_means = block_data.mean(axis=1)\n        \n        # Calculate unbiased variance of block means (ddof=1)\n        s_Y_sq = np.var(block_means, ddof=1)\n        \n        # Calculate the blocked variance estimator\n        sigma_hat_sq_b = b * s_Y_sq\n        \n        blocked_variances.append(sigma_hat_sq_b)\n        inv_block_sizes.append(1/b)\n\n    # 3. Perform linear extrapolation\n    # regression of y = sigma_hat_sq_b on x = 1/b\n    regression_result = linregress(x=inv_block_sizes, y=blocked_variances)\n    \n    # The intercept is the extrapolated value at 1/b = 0\n    sigma_sq_extrapolated = regression_result.intercept\n    \n    # Clamp to 0 if negative\n    sigma_sq_extrapolated = max(0, sigma_sq_extrapolated)\n    \n    return sigma_sq_extrapolated\n\ndef calculate_n_min(series, sigma_hat_sq, params):\n    \"\"\"\n    Calculates the minimum required sample size N_min.\n    \"\"\"\n    z = params['z']\n    epsilon = params['epsilon']\n    \n    sigma_hat = np.sqrt(sigma_hat_sq)\n    x_bar = np.mean(series)\n    \n    # Per problem statement, abs(x_bar) is not pathologically small for test cases.\n    # The minimum positive floor is not strictly necessary but good practice.\n    abs_x_bar = abs(x_bar)\n    if abs_x_bar  1e-15:\n        abs_x_bar = 1e-15\n\n    # Calculate N_min\n    n_min_float = (z * sigma_hat / (epsilon * abs_x_bar))**2\n    n_min = int(np.ceil(n_min_float))\n    \n    return n_min\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    N0 = 65536\n    test_cases = [\n        # Case 1 (IID normal)\n        {'type': 'iid', 'mu': 1.0, 'sigma_x': 2.0, 'seed': 20231102, 'epsilon': 0.05, 'z': 1.96, 'N0': N0},\n        # Case 2 (AR(1))\n        {'type': 'ar1', 'phi': 0.7, 'mu': 1.0, 'sigma_x': 2.0, 'seed': 20231103, 'epsilon': 0.05, 'z': 1.96, 'N0': N0},\n        # Case 3 (AR(1), stronger correlation)\n        {'type': 'ar1', 'phi': 0.95, 'mu': 1.0, 'sigma_x': 2.0, 'seed': 20231104, 'epsilon': 0.05, 'z': 1.96, 'N0': N0},\n        # Case 4 (IID normal with small mean)\n        {'type': 'iid', 'mu': 0.1, 'sigma_x': 1.0, 'seed': 20231105, 'epsilon': 0.02, 'z': 1.96, 'N0': N0},\n    ]\n\n    results = []\n    for params in test_cases:\n        # 1. Generate the time series\n        series = generate_series(params)\n        \n        # 2. Estimate the effective variance\n        sigma_hat_sq = estimate_effective_variance(series, N0)\n        \n        # 3. Calculate the minimum sample size\n        n_min = calculate_n_min(series, sigma_hat_sq, params)\n        \n        results.append(n_min)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}