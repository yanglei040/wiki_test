{
    "hands_on_practices": [
        {
            "introduction": "马尔可夫链蒙特卡洛（MCMC）方法的核心在于其独特的接受-拒绝机制，该机制确保了马尔可夫链最终能收敛到目标分布。这个基础练习将带你亲手计算Metropolis算法中的接受概率，这是理解MCMC如何“智能地”在状态空间中移动、从而探索目标概率分布的第一步。通过这个具体的计算，你将对算法如何权衡接受新状态和维持现有状态有一个直观的认识。",
            "id": "1371728",
            "problem": "一位数据科学家正在实现一个马尔可夫链蒙特卡洛 (MCMC) 模拟，以从参数 $x$ 的后验概率分布中抽取样本。目标分布 $\\pi(x)$ 与参数负绝对值的指数成正比，即 $\\pi(x) \\propto \\exp(-|x|)$。\n\n该科学家使用 Metropolis 算法和一个对称提议分布 $q(x'|x)$，其中在给定当前状态 $x$ 的情况下提议一个新状态 $x'$ 的概率等于在给定 $x'$ 的情况下提议 $x$ 的概率（即 $q(x'|x) = q(x|x')$）。\n\n假设在模拟的某一步中，链的当前状态为 $x = 1.5$。然后，算法提议转移到一个新的候选状态 $x' = 2.0$。\n\n计算这个特定转移的接受概率。你的答案应该是一个无量纲实数。将最终答案四舍五入到四位有效数字。",
            "solution": "对于从 $x$ 到 $x'$ 的转移，在使用对称提议 $q(x'|x)=q(x|x')$ 的情况下的 Metropolis 接受概率是\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\n给定目标分布 $\\pi(x)\\propto \\exp(-|x|)$，该比率简化为\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\n当 $x=1.5$ 且 $x'=2.0$ 时，我们有 $|x|=1.5$ 和 $|x'|=2.0$，所以\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\n因此，\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\n数值上，$\\exp(-0.5)\\approx 0.6065$（四舍五入到四位有效数字）。",
            "answer": "$$\\boxed{0.6065}$$"
        },
        {
            "introduction": "成功实现一个MCMC采样器仅仅是第一步，更关键的是要诊断其运行效率。一个看似良好但实际上效率低下的采样器会产生高度自相关的样本，导致对后验分布的估计不准确。这个练习探讨了一个MCMC诊断中的经典“悖论”：极高的接受率并不意味着高效的探索。通过分析这种情况，你将学会如何解读MCMC的关键诊断指标之一，并理解调整提议分布的重要性。",
            "id": "2408757",
            "problem": "考虑一个计算金融中的贝叶斯后验抽样问题，其中标量参数 $\\theta$ 代表了在一个正态-正态共轭模型中某项资产的预期超额收益。为具体起见，假设后验密度为高斯分布，其密度正比于\n$$\n\\pi(\\theta) \\propto \\exp\\!\\left(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2}\\right),\n$$\n其中 $\\mu$ 和 $\\sigma^2$ 已知。您实现了一个带有对称提议的随机游走 Metropolis-Hastings 算法\n$$\nq(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, \\tau^2),\n$$\n以及接受概率\n$$\n\\alpha(\\theta, \\theta') = \\min\\!\\left\\{1,\\ \\frac{\\pi(\\theta')}{\\pi(\\theta)}\\right\\}.\n$$\n在长时间运行后，您观察到平均接受率 $\\hat{a} \\approx 0.97$（即 $\\hat{a} \\approx 97\\%$）。关于这对状态空间探索的影响以及应采取的适当行动，以下哪些陈述是正确的？\n\nA. 非常高的接受率意味着低自相关和良好的混合性，因此当前的提议尺度 $\\tau$ 应保持不变。\n\nB. 非常高的接受率通常表明 $\\tau$ 过小；尽管大多数提议都被接受，但每次迭代的期望跳跃平方非常小，这导致了缓慢的探索（高自相关）。增加 $\\tau$ 以将接受率降低到中等水平（例如，在一维情况下约为 $0.44$），可以改善探索效果。\n\nC. 为进一步改善探索并减少有限样本偏差，应该进一步减小 $\\tau$ 以将接受率推向接近 $1$。\n\nD. 对于一个平滑的、单峰的目标分布和一个对称的随机游走提议，当 $\\tau \\to 0$ 时，接受率趋向于 $1$，但积分自相关时间趋向于 $\\infty$，这意味着尽管接受几乎是确定的，探索效果却很差。\n\nE. 马尔可夫链的平稳分布依赖于 $\\tau$；当接受率非常高时，链的目标分布会比 $\\pi(\\theta)$ 更集中，这解释了探索效果差的原因。",
            "solution": "必须首先验证问题陈述的科学性和逻辑完整性。\n\n### 步骤 1：提取已知信息\n问题提供了以下信息：\n-   **模型**：一个来自正态-正态共轭模型的贝叶斯后验抽样问题。\n-   **参数**：一个标量参数 $\\theta$。\n-   **目标后验密度**：$\\pi(\\theta) \\propto \\exp\\!\\left(-\\frac{(\\theta - \\mu)^2}{2\\sigma^2}\\right)$，其中 $\\mu$ 和 $\\sigma^2$ 是已知常数。这是一个高斯分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的密度。\n-   **算法**：随机游走 Metropolis-Hastings (RWMH)。\n-   **提议分布**：一个对称提议，$q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, \\tau^2)$。参数 $\\tau$ 是提议尺度或步长。\n-   **接受概率**：$\\alpha(\\theta, \\theta') = \\min\\!\\left\\{1,\\ \\frac{\\pi(\\theta')}{\\pi(\\theta)}\\right\\}$。\n-   **经验观察**：长时间运行后的平均接受率为 $\\hat{a} \\approx 0.97$。\n\n### 步骤 2：使用提取的已知信息进行验证\n对问题进行严格验证。\n1.  **科学依据**：该问题描述了 Metropolis-Hastings 算法的一个典型应用，该算法是计算统计学和计量经济学中的一个基本工具。模型（高斯后验）和提议机制（高斯随机游走）构成了一个标准的教科书示例。该问题在科学上是合理的。\n2.  **良定性**：问题要求对算法的一个特定、明确定义的结果（$\\hat{a} \\approx 0.97$）进行解释。这引出了对抽样器性能的诊断分析，这是 MCMC 实践中的一项标准任务。该问题是良定的。\n3.  **客观性**：问题以精确的数学语言陈述，没有歧义或主观内容。\n4.  **完整性与一致性**：问题提供了分析 MCMC 抽样器性能所需的所有必要信息。没有矛盾之处。\n\n### 步骤 3：结论与行动\n问题陈述是有效的。这是一个计算统计学领域中良定的、有科学依据的问题。我将进行完整的推导和分析。\n\n### 解题推导\nMetropolis-Hastings 算法的目标是从一个难以直接抽样的概率分布 $\\pi(\\theta)$ 中生成一个样本序列 $\\{\\theta_t\\}_{t=1}^N$。该算法构建了一个马尔可夫链，其平稳分布是目标分布 $\\pi(\\theta)$。对于任何有效的提议分布，包括指定的尺度 $\\tau > 0$ 的分布，该算法都保证以 $\\pi(\\theta)$ 作为其平稳分布。然而，$\\tau$ 的选择严重影响算法的效率，即链探索状态空间并收敛到该平稳分布的速率。\n\n效率取决于提议步长和接受概率之间的权衡。\n-   如果提议尺度 $\\tau$ 非常小，新的提议 $\\theta' = \\theta_t + \\epsilon$（其中 $\\epsilon \\sim \\mathcal{N}(0, \\tau^2)$）将非常接近当前状态 $\\theta_t$。因此，$\\pi(\\theta')$ 将非常接近 $\\pi(\\theta_t)$，比率 $\\frac{\\pi(\\theta')}{\\pi(\\theta_t)}$ 将接近 $1$，接受概率 $\\alpha(\\theta_t, \\theta')$ 也将接近 $1$。链几乎接受所有提议。然而，由于每一步都非常小，链移动得非常慢。这导致样本序列高度相关，从而对状态空间的探索效果很差。这被称为慢混合。\n-   如果提议尺度 $\\tau$ 非常大，新的提议 $\\theta'$ 很可能远离当前状态 $\\theta_t$。如果 $\\theta_t$ 位于高概率区域，$\\theta'$ 很可能落在一个概率低得多的区域（分布的尾部）。因此，比率 $\\frac{\\pi(\\theta')}{\\pi(\\theta_t)}$ 将非常小，导致接受概率很低。链会拒绝大多数提议，并在许多迭代中停留在同一状态 $\\theta_t$。这也导致高样本自相关和差的探索效果。\n\n观察到的平均接受率为 $\\hat{a} \\approx 0.97$。这个值非常高，表明算法正处于第一种情况：提议尺度 $\\tau$ 过小。虽然链在不断移动，但其步长太小以至于效率低下。\n\n最优调整旨在最大化抽样器的效率，通常通过最小化样本的自相关来实现。理论研究（例如，Roberts, Gelman, 和 Gilks, 1997）已经为 RWMH 算法确立了最优接受率。对于像本问题中的一维目标分布，最大化探索效率的最优接受率约为 $0.44$。观察到的 $0.97$ 的接受率远非这个最优值，证实了抽样器调整不当。正确的做法是增加 $\\tau$ 以将接受率降低到最优范围。\n\n### 逐项分析\n\n**A. 非常高的接受率意味着低自相关和良好的混合性，因此当前的提议尺度 $\\tau$ 应保持不变。**\n这个陈述有根本性错误。非常高的接受率是提议步长过小的一个典型症状，这导致了微小、保守的移动。这使得链中连续的状态几乎相同，导致*高*自相关和*差*的混合性。因此，保持 $\\tau$ 不变的建议是错误的。\n结论：**错误**。\n\n**B. 非常高的接受率通常表明 $\\tau$ 过小；尽管大多数提议都被接受，但每次迭代的期望跳跃平方非常小，这导致了缓慢的探索（高自相关）。增加 $\\tau$ 以将接受率降低到中等水平（例如，在一维情况下约为 $0.44$），可以改善探索效果。**\n这个陈述完全正确。它准确地诊断了情况：$\\approx 0.97$ 的接受率表明 $\\tau$ 过小。它正确地指出了其后果：链的跳跃非常小，导致探索缓慢，表现为高自相关。它正确地给出了纠正措施：增加 $\\tau$。最后，它正确地引用了一维问题中理论上的最优接受率约为 $0.44$。这是标准的、教科书式的诊断和应对方法。\n结论：**正确**。\n\n**C. 为进一步改善探索并减少有限样本偏差，应该进一步减小 $\\tau$ 以将接受率推向接近 $1$。**\n这个陈述建议了与正确行动完全相反的做法。如前所述，接近 $1$ 的接受率是效率低下的标志。进一步减小 $\\tau$ 只会加剧问题，使步长更小，探索更慢。这将增加而不是减少从链输出计算的任何统计估计量的自相关和方差。\n结论：**错误**。\n\n**D. 对于一个平滑的、单峰的目标分布和一个对称的随机游走提议，当 $\\tau \\to 0$ 时，接受率趋向于 $1$，但积分自相关时间趋向于 $\\infty$，这意味着尽管接受几乎是确定的，探索效果却很差。**\n这是一个精确的理论陈述，它形式化了 $\\tau$ 过小时的病态情况。当 $\\tau \\to 0$ 时，任何提议的步长 $\\theta' = \\theta + \\epsilon$ 将无限接近于 $\\theta$。对于一个平滑的密度 $\\pi$，$\\pi(\\theta') \\approx \\pi(\\theta)$，所以接受比率 $\\frac{\\pi(\\theta')}{\\pi(\\theta)} \\to 1$，因此接受率趋向于 $1$。然而，此时链是一个随机游走的离散近似。为了探索一个固定大小的区域，所需的步数与 $1/\\tau^2$ 成比例。积分自相关时间（IACT），衡量的是与一个独立样本等价的相关样本数量，可以证明其与 $\\text{IACT} \\propto 1/\\tau^2$ 成比例。因此，当 $\\tau \\to 0$ 时，IACT $\\to \\infty$。这标志着高效探索的完全崩溃。这一陈述是 MCMC 最优尺度理论的基石。\n结论：**正确**。\n\n**E. 马尔可夫链的平稳分布依赖于 $\\tau$；当接受率非常高时，链的目标分布会比 $\\pi(\\theta)$ 更集中，这解释了探索效果差的原因。**\n这个陈述包含一个严重的观念错误。Metropolis-Hastings 算法的构建是为了满足细致平衡条件 $\\pi(\\theta) K(\\theta'|\\theta) = \\pi(\\theta') K(\\theta|\\theta')$，其中 $K$ 是转移核。这个条件保证了马尔可夫链的平稳分布*恰好是*目标分布 $\\pi(\\theta)$。对于任何有效的提议尺度 $\\tau > 0$，此属性都成立。参数 $\\tau$ 影响的是收敛到平稳分布的*速率*和抽样器的*效率*，但它*不会*改变目标分布本身。探索效果差是由于动力学缓慢，而不是因为目标是错误的分布。\n结论：**错误**。",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "MCMC方法的理论基石之一是马尔可夫链的不可约性（irreducibility），即从状态空间的任意一点出发，都有可能在有限步内到达其他任何一点。如果这个条件不满足，采样器可能会被困在目标分布的某个局部区域，从而无法收敛到真实的全局分布。这个动手编程练习将引导你构建一个不可约性失效的案例，让你亲眼见证采样器如何“失败”，并学会通过诊断工具识别这种根本性的收敛问题。",
            "id": "3157911",
            "problem": "您将实现并分析一个 Metropolis–Hastings 马尔可夫链蒙特卡洛 (MCMC) 采样器，以演示当提议核在具有不连通支撑集的目标分布上不可约时出现的收敛失败情况。整个工作将在一个维度上进行。目标密度 $\\pi(x)$ 在不连通集合 $S = [-3,-1] \\cup [1,3]$ 上与均匀密度成正比，而在其他地方为零。也就是说，对于 $x \\in S$，$\\pi(x) \\propto 1$；对于 $x \\notin S$，$\\pi(x) = 0$。Metropolis–Hastings 链使用形式为 $x' \\sim q(\\cdot \\mid x)$ 的提议，其接受概率为 $\\alpha(x,x') = \\min\\left(1, \\dfrac{\\pi(x')\\,q(x \\mid x')}{\\pi(x)\\,q(x' \\mid x)}\\right)$。您必须将每条链的起始点设为 $x_0 = -2$，该点位于左侧分量 $[-3,-1]$ 中。\n\n基本原理：使用马尔可夫链、不可约性、提议核、Metropolis–Hastings 接受概率的标准定义，以及以下概念：一个马尔可夫链蒙特卡洛采样器若以 $\\pi(x)$ 为目标分布，则它必须是不可约、非周期的，并且满足关于 $\\pi(x)$ 的细致平衡条件，而一个正确实现的 Metropolis–Hastings 算法通过构造即可满足这些条件。\n\n您将考虑两种提议族：\n- 步长为 $\\Delta > 0$ 的有界均匀随机游走：$q_{\\Delta}(x' \\mid x)$ 在 $[x-\\Delta, x+\\Delta]$ 上均匀分布。\n- 标准差为 $\\sigma > 0$ 的高斯随机游走：$q_{\\sigma}(x' \\mid x)$ 服从 $\\mathcal{N}(x, \\sigma^2)$ 分布。\n\n观察：由于 $\\pi(x)$ 在 $S$ 上为常数，在 $S$ 外为零，对于对称提议（有界均匀随机游走和高斯随机游走都是对称的），Metropolis–Hastings 的接受规则简化为：接受所有落在 $S$ 内的提议，并拒绝所有落在 $S$外的提议。\n\n定义分量标签函数为：如果 $x \\in [-3,-1]$，则 $c(x) = 0$；如果 $x \\in [1,3]$，则 $c(x) = 1$。因为链从 $S$ 内开始，并拒绝任何 $x' \\notin S$ 的提议状态，所以在初始化之后所有状态都保持在 $S$ 内。为了进行分析，使用 $B = 1000$ 步的预烧期，然后对剩余的样本计算诊断指标。\n\n您必须为每条链在预烧期后实现以下诊断指標：\n1. 访问过的不同分量的数量，即集合 $\\{c(x_t) : t \\ge B\\}$ 的基数；将其表示为 $\\{1,2\\}$ 中的一个整数。\n2. 预烧期后樣本中位于右侧分量的比例，计算公式为 $\\frac{1}{T-B}\\sum_{t=B}^{T-1} \\mathbf{1}\\{c(x_t)=1\\}$，其中 $T$ 是总步数；表示为小数点后四位四舍五入的小数。\n3. 根据连续的预烧期后标签估计的、分量间的经验二状态转移矩阵。设 $N_{ij}$ 为在时间 $t$ 从分量 $i$ 转移到时间 $t+1$ 的分量 $j$ 的次数计数，其中 $t=B,\\dots,T-2$。定义 $\\widehat{P}_{ij} = \\frac{N_{ij}}{\\sum_{k \\in \\{0,1\\}} N_{ik}}$（如果分母为正），如果分母为零则定义 $\\widehat{P}_{ij} = 0$。报告非对角线元素 $\\widehat{P}_{0,1}$ 和 $\\widehat{P}_{1,0}$（每个都四舍五入到小数点后四位）。\n4. 双分量聚合链的估计不可约性标志，定义为布尔值 $(\\widehat{P}_{0,1} > 0)$ and $(\\widehat{P}_{1,0} > 0)$。\n\n您必须实现该采样器，并为下面的测试套件计算这些诊断指标。按照规定使用独立的随机数生成器种子以保证可复现性。\n\n测试套件：\n- 情况 1 (可约)：有界均匀提议，$\\Delta = 1.5$，总步数 $T = 5000$，随机种子 $12345$。\n- 情况 2 (边界可约)：有界均匀提议，$\\Delta = 2.0$，总步数 $T = 5000$，随机种子 $12345$。\n- 情况 3 (不可约)：有界均匀提议，$\\Delta = 2.5$，总步数 $T = 5000$, 随机种子 $12345$。\n- 情况 4 (不可约)：高斯提议，$\\sigma = 1.0$，总步数 $T = 6000$，随机种子 $67890$。\n\n基于第一性原理的预期行为指导：分量之间的间隙宽度为 $2$。对于有界均匀提议，当 $\\Delta  2$ 时，链无法在一步内跨越间隙，因此在 $S$ 上是**可约的**；当 $\\Delta = 2$ 时，唯一的跨越方式需要提议一个恰好为 $2$ 的跳跃，而这在连续提议下的概率为 $0$，因此链实际上仍然是**可约的**；当 $\\Delta > 2$ 时，跨越成为可能，链才是**不可约的**。对于高斯提议，其支撑集是无界的，因此对于任何 $\\sigma > 0$，跨越都具有正概率，链总是不可约的。\n\n最终输出格式：您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔的结果列表。每个测试用例贡献一个形式为 [visited_components,positive_fraction,p01,p10,is_irreducible_estimated] 的子列表，其中 visited_components 是一个整数，positive_fraction、p01 和 p10 是四舍五入到四位小数的小数，is_irreducible_estimated 是一个布尔值。因此，总输出必须是这四个子列表组成的单个列表，不含空格，例如：[[1,0.0000,0.0000,0.0000,False],[...],[...],[...]]。不允许使用外部文件或输入；所有内容必须自包含运行。\n\n角度单位不适用。物理单位不适用。百分比必须表示为小数。",
            "solution": "这个问题的目标是演示 Metropolis-Hastings 马尔可夫链蒙特卡洛 (MCMC) 算法的一个关键失败模式：当其底层的马尔可夫链不是不可约时，无法收敛到真实的目标分布。如果一条马尔可夫链可以在有限步数内从任何状态到达任何其他状态，那么它就是不可约的。为了让一个 MCMC 采样器能从目标分布 $\\pi(x)$ 正确采样，其对应的马尔可夫链必须在 $\\pi(x)$ 的整个支撑集上是不可约的。如果提议核使得支撑集的某些区域无法从其他区域到达，采样器将无法探索整个分布，从而导致有偏和不正确的估计。\n\n我们将使用一个一维目标密度 $\\pi(x)$ 来研究这一现象，该密度在一个不连通的支撑集 $S = [-3, -1] \\cup [1, 3]$ 上是均匀的，而在其他地方为零。因此，概率密度为：\n$$\n\\pi(x) = \\begin{cases}\n    1/4  \\text{if } x \\in [-3, -1] \\cup [1, 3] \\\\\n    0  \\text{otherwise}\n\\end{cases}\n$$\n支撑集 $S$ 的总长度为 $( -1 - (-3)) + (3 - 1) = 2 + 2 = 4$。\n\nMetropolis-Hastings 算法通过从一个提议分布 $q(x' \\mid x_t)$ 中提议一个新状态 $x'$，并以以下概率接受它，来生成一个状态序列 $\\{x_t\\}$：\n$$\n\\alpha(x_t, x') = \\min\\left(1, \\frac{\\pi(x') q(x_t \\mid x')}{\\pi(x') q(x' \\mid x_t)}\\right)\n$$\n我们将使用两种类型的对称提议核，对于这些核，$q(x \\mid x') = q(x' \\mid x)$。接受概率简化为 $\\alpha(x_t, x') = \\min(1, \\pi(x') / \\pi(x_t))$。给定我们特定的 $\\pi(x)$，如果当前状态 $x_t$ 在支撑集 $S$ 中，那么 $\\pi(x_t) = 1/4$。\n- 如果提议的状态 $x'$ 也在 $S$ 中，那么 $\\pi(x') = 1/4$，比值为 $1$，接受概率为 $\\alpha(x_t, x') = 1$。该提议总是被接受。\n- 如果提议的状态 $x'$ 不在 $S$ 中，那么 $\\pi(x') = 0$，比值为 $0$，接受概率为 $\\alpha(x_t, x') = 0$。该提议总是被拒绝，链保持在 $x_{t+1} = x_t$。\n\n链在 $x_0 = -2$ 处初始化，该点位于支撑集的左分量 $[-3,-1]$ 中。\n\n两种提议核是：\n1.  有界均匀随机游走，其中 $x' \\sim U(x - \\Delta, x + \\Delta)$。为了让链能够从左分量 $[-3, -1]$ 转移到右分量 $[1, 3]$，提议区间必须能够跨越它们之间宽度为 $2$ 的间隙。左分量的最右点是 $x = -1$。从此点出发的提议位于 $[-1-\\Delta, -1+\\Delta]$ 内。为了使该区间与 $[1, 3]$ 重叠，我们必须有 $-1+\\Delta  1$，这意味着 $\\Delta  2$。如果 $\\Delta \\le 2$，链就不可能从左分量跳到右分量，从而使链在 $S$ 上是**可约的**（非不可约）。\n2.  高斯随机游走，其中 $x' \\sim \\mathcal{N}(x, \\sigma^2)$。该分布具有无界支撑集 $(-\\infty, \\infty)$。因此，对于任何当前状态 $x \\in S$ 和任何 $\\sigma  0$，提议一个位于支撑集任何其他部分的新状态 $x'$ 都具有非零的概率密度。这确保了链是不可约的。\n\n为了分析其行为，我们将模拟链 $T$ 步，并丢棄前 $B = 1000$ 步作为预烧期。我们定义一个分量标签函数 $c(x)$，其中对于 $x \\in [-3,-1]$，$c(x) = 0$；对于 $x \\in [1,3]$，$c(x) = 1$。我们在预烧期后的样本上计算以下诊断指标：\n1.  访问过的不同分量的数量，对于已经混合的不可约链，该值必须为 $2$。\n2.  右侧分量（$c(x)=1$）中的样本比例，对于一个对对称分布进行采样的不可约链，该值应接近 $0.5$。\n3.  分量间经验转移矩阵的非对角线元素 $\\widehat{P}_{0,1}$ 和 $\\widehat{P}_{1,0}$。它们分别代表从分量 $0$ 跳到 $1$ 和从 $1$ 跳到 $0$ 的估计概率。\n4.  一个经验性的不可约性标志，$(\\widehat{P}_{0,1}  0) \\land (\\widehat{P}_{1,0}  0)$，当且仅当观察到两个分量之间的转移时，该标志应为真。\n\n该算法按规定针对四个测试用例进行实现和评估，以展示基于此理论分析的预期行为。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the MCMC simulations and print the final results.\n    \"\"\"\n\n    def run_mcmc(proposal_type, param, T, x0, seed):\n        \"\"\"\n        Generates a Markov chain using the Metropolis-Hastings algorithm.\n\n        Args:\n            proposal_type (str): 'uniform' or 'gaussian'.\n            param (float): Delta for uniform or sigma for Gaussian.\n            T (int): Total number of steps.\n            x0 (float): Initial state.\n            seed (int): Random number generator seed.\n\n        Returns:\n            np.ndarray: The generated Markov chain of length T.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        chain = np.zeros(T)\n        chain[0] = x0\n        current_x = x0\n\n        for t in range(T - 1):\n            if proposal_type == 'uniform':\n                proposal = rng.uniform(current_x - param, current_x + param)\n            elif proposal_type == 'gaussian':\n                proposal = rng.normal(current_x, param)\n            \n            # Check if the proposal is in the support S = [-3,-1] U [1,3]\n            in_S = (-3.0 = proposal = -1.0) or (1.0 = proposal = 3.0)\n            \n            # Simplified Metropolis-Hastings acceptance rule\n            if in_S:\n                current_x = proposal\n            \n            chain[t + 1] = current_x\n        \n        return chain\n\n    def analyze_chain(chain, B):\n        \"\"\"\n        Computes diagnostics for a given Markov chain after burn-in.\n\n        Args:\n            chain (np.ndarray): The Markov chain.\n            B (int): The number of burn-in steps.\n\n        Returns:\n            list: A list containing the computed diagnostics.\n        \"\"\"\n        post_burn_in = chain[B:]\n        \n        def component_label(x):\n            return 0 if -3.0 = x = -1.0 else 1\n\n        labels = [component_label(x) for x in post_burn_in]\n        \n        # 1. Number of distinct components visited\n        visited_components = len(set(labels))\n        \n        # 2. Fraction of post–burn-in samples in the right component\n        if not labels:\n            positive_fraction = 0.0\n        else:\n            num_in_right = sum(1 for label in labels if label == 1)\n            positive_fraction = num_in_right / len(labels)\n        \n        # 3. Empirical two-state transition matrix\n        N00, N01, N10, N11 = 0, 0, 0, 0\n        for i in range(len(labels) - 1):\n            if labels[i] == 0:\n                if labels[i+1] == 0: N00 += 1\n                else: N01 += 1\n            else: # labels[i] == 1\n                if labels[i+1] == 0: N10 += 1\n                else: N11 += 1\n                \n        N0_total = N00 + N01\n        N1_total = N10 + N11\n        \n        p01 = float(N01) / N0_total if N0_total  0 else 0.0\n        p10 = float(N10) / N1_total if N1_total  0 else 0.0\n        \n        # 4. Estimated irreducibility flag\n        is_irreducible_estimated = (p01  0) and (p10  0)\n        \n        return [visited_components, positive_fraction, p01, p10, is_irreducible_estimated]\n\n    test_cases = [\n        {'type': 'uniform', 'param': 1.5, 'T': 5000, 'seed': 12345},\n        {'type': 'uniform', 'param': 2.0, 'T': 5000, 'seed': 12345},\n        {'type': 'uniform', 'param': 2.5, 'T': 5000, 'seed': 12345},\n        {'type': 'gaussian', 'param': 1.0, 'T': 6000, 'seed': 67890}\n    ]\n    \n    B = 1000\n    x0 = -2.0\n    \n    all_results = []\n    for case in test_cases:\n        chain = run_mcmc(case['type'], case['param'], case['T'], x0, case['seed'])\n        diagnostics = analyze_chain(chain, B)\n        all_results.append(diagnostics)\n\n    result_strings = []\n    for diag in all_results:\n        # Format: [visited_components, positive_fraction, p01, p10, is_irreducible_estimated]\n        # Decimals rounded to 4 places.\n        s = (f\"[{diag[0]},{diag[1]:.4f},{diag[2]:.4f},\"\n             f\"{diag[3]:.4f},{diag[4]}]\")\n        result_strings.append(s)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}