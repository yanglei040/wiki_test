## 引言
[计算模拟](@entry_id:146373)已成为科学发现和工程设计的第三大支柱，与理论和实验并驾齐驱。然而，模拟本身仅仅是第一步；其真正的价值蕴藏在对海量、复杂的输出数据进行严谨而富有洞察力的分析之中。如何从充满噪声和伪影的原始数据中提取可靠的科学结论？如何量化结果的不确定性并建立对其的信任？这些问题构成了计算科学中的一个核心挑战，也是本篇文章旨在解决的知识鸿沟。

本文将系统地引导您掌握分析模拟输出的艺术与科学。在第一部分 **“原理与机制”** 中，我们将奠定分析的理论基石，学习如何通过[验证与确认](@entry_id:173817)（[V&V](@entry_id:173817)）来建立对模拟的信任，掌握[处理时间](@entry_id:196496)序列以识别[稳态](@entry_id:182458)的核心技术，并探索用于理解系集数据变异性与模式的高级方法。随后，在 **“应用与跨学科联系”** 部分，我们将理论付诸实践，展示这些分析技术如何应用于计算物理、[材料科学](@entry_id:152226)、生物学等多个前沿领域，从微观动力学中提取宏观性质并验证物理模型。最后，一系列 **“动手实践”** 练习将提供具体编码的机会，让您亲手实现关键分析算法，从而将概念性知识转化为实用技能。让我们从建立信任的第一步开始，深入了解分析模拟输出的基本原理。

## 原理与机制

在对[计算模拟](@entry_id:146373)的输出进行分析时，我们面临一系列核心挑战：我们如何确信模拟结果是正确的？如何从充斥着瞬态行为和随机噪声的时间序列中提取有意义的[稳态](@entry_id:182458)特征？当面对大量高维度的系集（ensemble）数据时，我们又该如何发现主导模式并有效传达不确定性？本章将系统地探讨应对这些挑战的关键原理和机制，从建立对模拟结果信任的[验证与确认](@entry_id:173817)（Verification and Validation, [V&V](@entry_id:173817)）开始。

### [验证与确认](@entry_id:173817)：建立对模拟结果的信任

在从模拟输出中提取科学见解之前，首要任务是建立对模拟本身的信任。[验证与确认](@entry_id:173817)（[V&V](@entry_id:173817)）旨在回答两个基本问题：“我们是否正确地求解了方程？”（验证）以及“我们是否求解了正确的方程？”（确认）。本节重点介绍几种核心的验证技术，它们帮助我们评估模拟的数值准确性和物理保真度。

#### [代码验证](@entry_id:146541)：确认精确度阶

[代码验证](@entry_id:146541)的核心任务之一是确认数值算法的收敛速度是否与其理论[精确度](@entry_id:143382)阶相符。一种标准的做法是，将数值解与一个已知精确解的问题进行比较。通过在一系列逐步加密的网格上运行模拟，并[测量误差](@entry_id:270998)的减小速率，我们可以凭经验估计出方法的精确度阶 。

考虑一个定义在周期性区间 $[0, 2\pi]$ 上的均匀网格，其包含 $N$ 个点，网格间距为 $h = \frac{2\pi}{N}$。对于一个已知函数，如 $u(x) = \sin(x)$，其精确导数为 $u'(x) = \cos(x)$。我们可以使用不同的[有限差分格式](@entry_id:749361)来近似计算其导数，例如一阶[前向差分](@entry_id:173829)、[二阶中心差分](@entry_id:170774)或四阶[中心差分](@entry_id:173198)。

对于每种格式和每个网格分辨率 $h$，我们可以计算出数值解与精确解之间的点态误差向量 $e_i$。为了将这个误差向量的大小量化为一个单一的数值，我们通常使用两种离散范数：

- **离散 $L^2$ 范数**：$\lVert e \rVert_{2,h} = \sqrt{h \sum_{i=0}^{N-1} e_i^2}$，它衡量了误差的[均方根](@entry_id:263605)大小，是对连续 $L^2$ 范数 $\left(\int_0^{2\pi} e(x)^2 dx\right)^{1/2}$ 的一致近似。
- **$L^\infty$ 范数**（或[最大范数](@entry_id:268962)）：$\lVert e \rVert_{\infty} = \max_{0 \le i \le N-1} |e_i|$，它衡量了网格上最坏情况下的点态误差。

如果一个数值方法的[精确度](@entry_id:143382)阶为 $p$，其误差 $E(h)$（可以用任一范数度量）应随网格间距 $h$ 的减小而按 $E(h) \approx C h^p$ 的规律缩放，其中 $C$ 是一个常数。对该关系式两边取对数，我们得到一个[线性关系](@entry_id:267880)：
$$
\ln(E(h)) \approx \ln(C) + p \ln(h)
$$
这意味着，在对数-对数[坐标系](@entry_id:156346)（log-log plot）中绘制误差 $E$ 对网格间距 $h$ 的关系图，这些数据点应近似地落在一根直线上，而该直线的斜率就是我们凭经验观察到的精确度阶 $p$。通过对一系列 $(\ln(h_j), \ln(E_j))$ 数据点进行线性[最小二乘拟合](@entry_id:751226)，我们可以精确地估计出 $p$ 值。如果这个经验值与方法的理论阶数（例如，对于[二阶中心差分](@entry_id:170774)，$p$ 应接近 2）相符，我们就获得了代码正确实现该算法的有力证据。

#### 验证守恒律

许多物理系统都遵循基本的守恒律，如质量守恒、[动量守恒](@entry_id:149964)和[能量守恒](@entry_id:140514)。一个物理上真实的模拟必须在离散层面尊重这些定律。预算闭合分析（budget-closure analysis）是一种强大的诊断技术，用于检验模拟是否做到了这一点 。

其核心思想是将一个守恒量（如一个[控制体积](@entry_id:143882)内的总质量 $M$）的变化率与所有进出该体积的通量（源、汇、边界流入/流出）的净和进行比较。根据守恒原理，这两者应相等：
$$
\frac{dM}{dt} = \dot{M}_{\text{源}} - \dot{M}_{\text{汇}} + \dot{M}_{\text{流入}} - \dot{M}_{\text{流出}}
$$
在离散的模拟输出中，我们可以在每个时间间隔 $[t_k, t_{k+1})$ 上独立地计算等式的两边。左边，即“储存趋势率”（storage tendency rate），可以根据该时间段内储存量的变化来计算：
$$
\left(\frac{dM}{dt}\right)_{\text{储存}, k} = \frac{M_{k+1} - M_k}{t_{k+1} - t_k}
$$
右边，即“净驱动率”（net driving rate），是通过将在该时间间隔内所有已知的源、汇和边界通量速率相加得到的。

理论上，这两项应该相等。然而，由于数值误差（例如，时间积分方案的误差、空间离散误差或边界条件处理不当），它们之间可能存在差异。这个差异被称为“残差率”（residual rate）：
$$
R_k = \left(\frac{dM}{dt}\right)_{\text{储存}, k} - \left(\frac{dM}{dt}\right)_{\text{驱动}, k}
$$
一个非零的残差意味着质量在数值上被凭空创造或销毁了，这直接指向了模拟中的一个问题。为了量化这个不平衡的严重程度，我们可以计算一些聚合指标，如在整个模拟时长[内积](@entry_id:158127)分得到的“累积残差质量”，以及无量纲的“归一化闭合误差”，后者将总残差的大小与系统总通量的大小进行比较。持续的、系统性的残差通常是需要调查和修复的严重错误的信号。

#### 验证长期定性行为

对于描述保守动力系统（如[行星轨道](@entry_id:179004)或分子动力学）的模拟，短期精度并非唯一重要的衡量标准。模拟能否在长[时间积分](@entry_id:267413)后依然保持系统的关键定性特征，往往更为重要。一个典型的例子是检验能量等运动[不变量](@entry_id:148850)的守恒情况 。

考虑一个双体[引力](@entry_id:175476)[轨道](@entry_id:137151)问题，其总能量和角动量在理论上是守恒的。然而，当使用[数值积分器](@entry_id:752799)求解时，这些量可能会随时间变化。变化的模式揭示了[积分器](@entry_id:261578)的深层性质：

- **非辛积分器（Non-symplectic integrators）**，如[显式欧拉法](@entry_id:141307)和经典的四阶[龙格-库塔](@entry_id:140452)（RK4）方法，通常会表现出**[长期漂移](@entry_id:172399)（secular drift）**。这意味着即使每一步的误差很小，这些误差也会系统性地累积，导致数值计算出的能量随时间单调增加或减少。例如，即使是高精度的 RK4 方法，在长时间模拟[轨道](@entry_id:137151)时也会显示出一个微小但持续的[能量漂移](@entry_id:748982)率。这个漂移率的大小通常与其方法的阶数有关，例如，对于 RK4，[能量漂移](@entry_id:748982)率与步长的四次方 $(\Delta t)^4$ 成正比。

- **[辛积分器](@entry_id:146553)（Symplectic integrators）**，如[速度-Verlet](@entry_id:160498)方法，被设计用于求解[哈密顿系统](@entry_id:143533)。它们虽然不精确守恒原始系统的能量，但会精确守恒一个“影子”[哈密顿量](@entry_id:172864)，该[哈密顿量](@entry_id:172864)与原始[哈密顿量](@entry_id:172864)非常接近。其结果是，数值能量不会出现[长期漂移](@entry_id:172399)，而是在真实能量值附近进行**有界[振荡](@entry_id:267781)（bounded oscillations）**。在长时间模拟中，能量误差保持在一个范围内。[振荡](@entry_id:267781)的幅度取决于步长和方法的阶数，例如，对于[速度-Verlet](@entry_id:160498)方法，能量[振荡](@entry_id:267781)的幅度与步长的平方 $(\Delta t)^2$ 成正比。

通过追踪模拟输出中这些[不变量](@entry_id:148850)的偏差，我们可以判断所选的积分器是否适合长期模拟。对于需要保持[能量守恒](@entry_id:140514)的系统，一个显示有界[振荡](@entry_id:267781)的低阶[辛积分器](@entry_id:146553)，通常远胜于一个显示[能量漂移](@entry_id:748982)的高阶非辛积分器。这强调了一个关键点：分析模拟输出不仅要关注定量误差，还要关注其是否再现了正确的定性物理行为。

#### [数值稳定性分析](@entry_id:201462)

[数值稳定性](@entry_id:146550)是任何时间演化模拟的先决条件。一个不稳定格式会导致误差指数级增长，最终产生无意义的、溢出的结果。理解稳定性的来源并区分数值不稳定性与物理不稳定性是至关重要的技能。

最简单的稳定性概念可以通过一个线性测试方程 $y' = -\lambda y$（其中 $\lambda > 0$）与[显式欧拉法](@entry_id:141307)的结合来阐明 。该方法的更新规则为 $y_{n+1} = (1 - \lambda \Delta t) y_n$。为了防止解发散，[放大因子](@entry_id:144315) $|1 - \lambda \Delta t|$ 必须小于1，这导出了一个严格的稳定性约束：$\Delta t  2/\lambda$。这个条件，通常以 Courant–Friedrichs–Lewy (CFL) 条件的形式出现，揭示了精度、成本和稳定性之间的[基本权](@entry_id:200855)衡。选择一个远小于稳定性极限的步长 $c \cdot (2/\lambda)$（其中 $c \ll 1$）会得到一个稳定且偏差较小的解，但计算成本（步数）很高。随着 $c$ 接近 1，成本降低，但偏差增大。当 $c \ge 1$ 时，格式变得不稳定，误差会[振荡](@entry_id:267781)并增长，导致结果毫无价值。

更微妙的挑战在于区分模拟中观察到的指数增长是源于真实的**物理不稳定性**，还是源于**[数值不稳定性](@entry_id:137058)**的假象 。这里的裁决原则是**收敛性**。一个物理量，比如一个不稳定模式的增长率 $g$，是连续介质模型的内禀属性，它不应依赖于离散化的参数。因此，一个设计良好的模拟在网格被加密（即 $\Delta t \to 0, \Delta x \to 0$）时，其测得的增长率 $g_{\text{num}}$ 应该收敛到一个确定的、有限的极限值 $g$。相反，如果测得的“增长率”随着离散化参数的变化而变化，甚至在网格加密时发散（例如，观察到 $g_{\text{num}} \propto 1/\Delta t$），这便是一个强烈的信号，表明我们所见的是数值假象，而非物理现实。这种行为是[数值格式](@entry_id:752822)本身不稳定的典型特征，因为每一步的放大都会引入一个依赖于 $\Delta t$ 的误差。

### [时间序列分析](@entry_id:178930)：表征[稳态](@entry_id:182458)

许多模拟在启动后需要一段时间才能达到一个统计上的稳定状态，即“平衡态”。在此之前的初始阶段被称为“瞬态”或“预热期”（burn-in）。分析通常只应在系统[达到平衡](@entry_id:170346)后进行，因此，识别并丢弃瞬态数据是分析过程中的一个关键步骤。

#### 识别瞬态行为的终点

我们可以采用多种方法来确定[平衡点](@entry_id:272705)的到来。方法的选择取决于所需的严谨程度和对数据模型的假设。

一种实用的启发式方法是基于滑动窗口统计 。该方法持续监控输出时间序列的两个关键统计特性：

1.  **估计的精度**：在一个固定大小为 $W$ 的滑动窗口内，我们可以计算样本均值 $\hat{\mu}_t$ 和样本标准差 $s_t$。基于这些值，我们可以构造一个置信区间，其半宽 $h_t$（例如，使用学生t分布计算，$h_t \propto s_t/\sqrt{W}$）反映了均值估计的不确定性。当系统稳定时，我们期望其固有变异性是恒定的，因此 $h_t$ 应稳定在一个较小的值。我们可以设定一个精度阈值 $\tau$，并要求 $h_t \le \tau$。

2.  **估计的稳定性**：除了窗口内的变异性，我们还需要确保均值本身不再系统性地漂移。这可以通过比较连续滑动窗口的均值来实现。我们可以设定一个漂移阈值 $\delta$，并要求 $|\hat{\mu}_t - \hat{\mu}_{t-1}| \le \delta$。

为了避免因偶然的波动而过早地判断[达到平衡](@entry_id:170346)，一个稳健的停止规则会要求这两个条件在 $K$ 个连续的窗口中都得到满足。满足条件的第一个窗口的结束点就可以被认为是平衡期的开始。

与上述[启发式方法](@entry_id:637904)相比，一个更正式的方法是采用**[变点检测](@entry_id:634570)（change-point detection）** 。这种方法将问题框架化为一个统计模型选择问题。其核心假设是，时间序列的均值在某个未知的变点 $\tau$ 处发生了一次突变。整个过程包括三个步骤：

1.  **变[点估计](@entry_id:174544)**：对于每一个可能的变点 $k$，我们将数据分为两段（$[0, k-1]$ 和 $[k, n-1]$），并计算两段内数据点与其各自均值之间的总平方误差和 (SSE)。最小化 SSE 的 $k$ 值，即 $k^\star$，就是最大似然估计下的最佳变点候选。为了高效地对所有可能的 $k$ 进行计算，我们可以利用[累积和](@entry_id:748124)技巧，将[算法复杂度](@entry_id:137716)从 $O(n^2)$ 降低到 $O(n)$。

2.  **[模型选择](@entry_id:155601)**：找到了最佳变点 $k^\star$ 后，我们必须判断引入这个变点是否真的有统计学意义。一个包含变点的模型比一个均值恒定的模型更复杂。我们可以使用**[贝叶斯信息准则](@entry_id:142416)（Bayesian Information Criterion, BIC）**来进行[模型选择](@entry_id:155601)。BIC 通过一个惩罚项来权衡模型的[拟合优度](@entry_id:637026)（由 SSE 体现）和模型的复杂度（由自由参数数量体现）。我们计算“无变点”模型（$p_0$ 个参数）和“有单一变点”模型（$p_1$ 个参数）的 BIC 值。只有当后者的 BIC 值显著更低时，我们才接受存在变点的结论。

3.  **后变点稳定性检验**：即使 BIC 确认了一个变点，系统在变点之后可能不会立即进入完美的[稳态](@entry_id:182458)，而是经历一个短暂的弛豫过程。因此，我们可以从 $k^\star$ 开始，进一步向后搜索，直到找到一个起始点 $j \ge k^\star$，从该点开始的后续数据段（例如，通过比较其前后两半的均值）表现出统计稳定性。这个点 $j$ 最终被确定为[预热](@entry_id:159073)期的结束点。

### 系集分析：从原始数据到科学洞见

当模拟本质上是随机的，或者当我们需要探索参数空间时，单次运行的模拟是不足够的。在这种情况下，我们会运行一个包含多次模拟的**系集（ensemble）**。分析系集数据需要一套独特的统计和可视化工具。

#### 充分性原理：数据简化的理论依据

在分析大量数据时，我们几乎总是会计算一些摘要统计量，如均值和[方差](@entry_id:200758)，而不是处理整个原始数据集。这种数据简化操作的合法性源于一个深刻的统计学概念：**充分统计量（sufficient statistic）** 。

一个统计量 $T(X_1, \dots, X_n)$ 如果包含了样本中关于模型未知参数（例如 $\theta$）的全部信息，那么它就被称为 $\theta$ 的充分统计量。更正式地，根据 **Fisher-Neyman [因子分解定理](@entry_id:749213)**，如果[似然函数](@entry_id:141927) $L(\theta | \mathbf{x})$ 可以被分解为两部分的乘积：一部分只通过 $T(\mathbf{x})$ 依赖于数据，另一部分则完全不依赖于参数 $\theta$，那么 $T(\mathbf{x})$ 就是一个充分统计量。
$$
L(\theta | \mathbf{x}) = g(T(\mathbf{x}), \theta) \cdot h(\mathbf{x})
$$
这个定理意味着，一旦我们计算出了充分统计量 $T(\mathbf{x})$ 的值，原始数据集 $\mathbf{x}$ 本身对于推断 $\theta$ 就不再提供任何额外信息。从某种意义上说，充分统计量是对数据进行了关于参数的“[无损压缩](@entry_id:271202)”。

对于一个来自[正态分布](@entry_id:154414) $N(\mu, \sigma^2)$ 的[独立同分布](@entry_id:169067)样本，我们可以证明样本均值 $\bar{X}$ 和样本[方差](@entry_id:200758) $s^2$（或与之等价的离差平方和 $\sum(X_i - \bar{X})^2$）构成的二维统计量 $(\bar{X}, s^2)$ 是参数对 $(\mu, \sigma^2)$ 的一个充分统计量。这为我们广泛使用样本均值和[方差](@entry_id:200758)来总结[正态分布](@entry_id:154414)数据提供了坚实的理论基础。相比之下，像中位数或超过某个阈值的样本比例这样的统计量则不是充分的，因为它们丢弃了对估计参数至关重要的信息。

#### 量化逐次运行间的变异性

对于[随机模拟](@entry_id:168869)，每次使用不同的随机种子运行时，输出都会有所不同。为了得出一个可信的结论，我们必须量化并报告这种逐次运行间的变异性。这要求我们建立一个系统的框架来评估结果的**再现性（reproducibility）** 。

该过程通常包括以下步骤：
1.  用一组不同的随机种子运行模拟，生成一个结果系集。
2.  对每次运行的输出，计算其内部的关键统计量，如均值 $m_s$ 和[标准差](@entry_id:153618) $s_s$（其中 $s$ 是种子）。
3.  对这些单次运行的统计量，再进行跨系集的统计分析。例如，计算均值的均值 $\bar{m}$ 和均值的标准差 $sd_m$。后者量化了我们的平均结果在不同运行之间的波动程度。
4.  定义**相对变异性**指标。例如，均值的相对变异性可以定义为 $rv_m = sd_m / |\bar{m}|$。这个无量纲的指标衡量了结果的波动相对于其自身大小的程度。
5.  评估**成对一致性**。我们可以计算系集中任意两次运行的结果（例如均值 $m_s$ 和 $m_t$）之间差异小于某个预设容差 $\Delta_m$ 的配对所占的比例 $f_m$。一个高的 $f_m$ 值表示大多数运行都给出了彼此接近的结果。
6.  最后，将这些量化指标与用户定义的再现性阈值进行比较。例如，我们可以要求“均值是可再现的”当且仅当其相对变异性 $rv_m$ 小于一个阈值 $\tau_m$ 并且其成对一致性比例 $f_m$ 大于一个阈值 $p_m$。

这个多方面的框架提供了一种客观和可量化的方法来评估和报告[随机模拟](@entry_id:168869)的稳定性。

#### 使用PCA发现多变量输出中的主导模式

当每次模拟运行都产生一个高维的输出向量时，手动分析变得不可行。我们需要一种方法来自动地降低数据的维度，并识别出其中最重要的变化模式。**主成分分析（Principal Component Analysis, PCA）** 就是为此而生的强大工具 。

PCA 的核心思想是在高维输出空间中找到一组新的[正交坐标](@entry_id:166074)轴，即**主成分（principal components）**。第一个主成分被定义为数据投影后[方差](@entry_id:200758)最大的方向；第二个主成分是在与第一个主成分正交的[子空间](@entry_id:150286)中[方差](@entry_id:200758)最大的方向，以此类推。这些主成分方向揭示了系集数据中变异最大的“模式”或“模态”。

在实践中，PCA 通常通过对中心化后的数据矩阵 $X_c$（即每列减去其均值）进行**奇异值分解（Singular Value Decomposition, SVD）**来实现。$X_c = U S V^\top$。分解得到的[右奇异向量](@entry_id:754365)（$V$ 的列）就是主成分方向。相关的[奇异值](@entry_id:152907)的平方则与每个主成分解释的[方差](@entry_id:200758)成正比，这使我们能够计算出每个主成分的**[方差](@entry_id:200758)解释率（explained variance ratio）**，从而判断哪些模式是主导的。

PCA 的威力不仅在于[降维](@entry_id:142982)，更在于其[可解释性](@entry_id:637759)。一旦我们识别出主导的主成分（例如，第一个主成分 $v_1$），我们就可以：
- **解释模式的物理意义**：通过计算主成分向量 $v_1$ 与已知的物理[基向量](@entry_id:199546)（例如，模拟器中预设的潜在输出模式）之间的方向对齐程度（如余弦相似度），我们可以将抽象的数学模式与具体的物理过程联系起来。
- **识别驱动因素**：主成分**得分**（$U S$ 的列）是一个向量，它表示每个模拟运行在相应主成分方向上的投影值。通过计算这些得分与模拟输入参数之间的相关性（如[皮尔逊相关系数](@entry_id:270276)），我们可以找出是哪个输入参数主要驱动了所观察到的主导变化模式。

#### 可视化系集不确定性

将系集数据中的不确定性有效地传达给读者，是分析的最后但同样关键的一步。不同的可视化方法有其各自的优缺点，选择不当可能会产生误导 。

- **意大利面图（Spaghetti plot）**：这是最直接的方法，即将系集中的每一条轨迹都绘制出来。它能直观地展示所有可能的结果和轨迹的形态。然而，当轨迹数量较多时，该图会因**过度绘制（overplotting）**而变得混乱不堪。线条的重叠使得观者难以准确判断轨迹的密度[分布](@entry_id:182848)，尤其是在密集区域，视觉上的“墨水密度”会很快饱和，无法线性地反映真实的[概率密度](@entry_id:175496)。

- **扇形图（Fan chart）**：这种图通过在每个时间点绘制嵌套的**分位数（quantile）**区间来展示不确定性。例如，一个95%的中心区间会显示从2.5%分位数到97.5%[分位数](@entry_id:178417)之间的范围。对于单峰、对称的[分布](@entry_id:182848)，扇形图非常直观。但它的一个主要缺陷在于，对于**多峰[分布](@entry_id:182848)**，它会创建一个连续的区间，该区间必然会覆盖峰与峰之间的低概率密度“山谷”，从而给用户造成这些中间值也很可能的错觉。此外，必须警惕一个常见的误解：一个逐点的95%置信区间并不意味着95%的**完整轨迹**会落入其中；实际上，由于需要同时在所有时间点都满足条件，完全包含在内的轨迹比例会远低于95%。

- **最高密度区域图（Highest Density Region, HDR）**：与基于分位数的扇形图不同，HDR图旨在显示在每个时间点上“最可能”的值所在的区域。一个 $(1-\gamma)$ 的HDR被定义为包含总概率为 $(1-\gamma)$ 的最小可能的值域。这等价于找到一个[概率密度](@entry_id:175496)阈值，并绘制出所有密度高于该阈值的区域。HDR 的一个巨大优势在于，对于多峰[分布](@entry_id:182848)，它可以正确地显示为**两个或多个不相交的区间**，从而准确地反映出[分布](@entry_id:182848)的真实结构。对于有偏的单峰[分布](@entry_id:182848)，HDR 也会呈现出非对称的区间，正确地反映其[偏度](@entry_id:178163)。

最后，当处理来自[重要性采样](@entry_id:145704)等方法的**加权样本**时，所有统计摘要（分位数、[密度估计](@entry_id:634063)）的计算都必须使用这些权重。忽略权重将导致对错误的[概率分布](@entry_id:146404)进行可视化，从而得出无效的结论。