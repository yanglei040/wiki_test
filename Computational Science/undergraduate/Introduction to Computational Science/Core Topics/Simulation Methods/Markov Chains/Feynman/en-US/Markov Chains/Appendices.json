{
    "hands_on_practices": [
        {
            "introduction": "The first step in working with Markov chains is often translating a real-world process into a mathematical model. This practice guides you through this crucial skill using a familiar example from video game design. By setting up a transition matrix for a character's behavior, you will learn to calculate the probability of future states, providing a concrete foundation for understanding how these systems evolve over time. ",
            "id": "1639029",
            "problem": "An engineer is programming the behavior of a non-player character (NPC) guard for a video game. The NPC's state is updated in discrete time steps, or \"update cycles\". The NPC can be in one of three states: 'Patrol', 'Alert', or 'Attack'. The transitions between these states are probabilistic and depend only on the current state.\n\n- If the NPC is in the 'Patrol' state, it is following a predefined route. In any given update cycle, there is an 80% chance it remains in the 'Patrol' state and a 20% chance it hears a suspicious sound and transitions to the 'Alert' state. It cannot transition directly from 'Patrol' to 'Attack'.\n\n- If the NPC is in the 'Alert' state, it is actively investigating a disturbance. In any given update cycle, there is a 30% chance it finds nothing and returns to the 'Patrol' state, a 40% chance it continues investigating and remains in the 'Alert' state, and a 30% chance it spots the player and transitions to the 'Attack' state.\n\n- If the NPC is in the 'Attack' state, it is engaged in combat with the player. In any given update cycle, there is a 10% chance it loses track of the player and returns to the 'Patrol' state. Otherwise, it remains in the 'Attack' state. It cannot transition from 'Attack' back to 'Alert'.\n\nThe NPC is initialized and begins its first update cycle in the 'Patrol' state. What is the probability that the NPC is in the 'Attack' state at the end of the third update cycle? Round your final answer to four significant figures.",
            "solution": "Model the NPC's state as a time-homogeneous Markov chain with states ordered as $[P, A, T]$ for Patrol, Alert, Attack. The one-step transition matrix $M$ is\n$$\nM=\\begin{pmatrix}\n0.8 & 0.2 & 0 \\\\\n0.3 & 0.4 & 0.3 \\\\\n0.1 & 0 & 0.9\n\\end{pmatrix}.\n$$\nLet the row vector $p_{n}$ denote the state distribution at the end of the $n$th update cycle. The evolution is given by the Markov property and matrix multiplication:\n$$\np_{n+1}=p_{n}M.\n$$\nThe NPC starts in Patrol, so before the first update cycle $p_{0}=(1,0,0)$. After the first update cycle,\n$$\np_{1}=p_{0}M=(1,0,0)M=(0.8,\\,0.2,\\,0).\n$$\nAfter the second update cycle,\n$$\np_{2}=p_{1}M=(0.8,\\,0.2,\\,0)M,\n$$\nwhose components are\n$$\n\\begin{aligned}\np_{2}(P)&=0.8\\cdot 0.8+0.2\\cdot 0.3+0\\cdot 0.1=0.64+0.06=0.70,\\\\\np_{2}(A)&=0.8\\cdot 0.2+0.2\\cdot 0.4+0\\cdot 0=0.16+0.08=0.24,\\\\\np_{2}(T)&=0.8\\cdot 0+0.2\\cdot 0.3+0\\cdot 0.9=0.06.\n\\end{aligned}\n$$\nAfter the third update cycle,\n$$\np_{3}=p_{2}M,\n$$\nand the probability of being in Attack is, by the law of total probability over the current state,\n$$\np_{3}(T)=p_{2}(P)\\cdot 0+p_{2}(A)\\cdot 0.3+p_{2}(T)\\cdot 0.9=0.24\\cdot 0.3+0.06\\cdot 0.9=0.072+0.054=0.126.\n$$\nRounded to four significant figures, this is $0.1260$.",
            "answer": "$$\\boxed{0.1260}$$"
        },
        {
            "introduction": "While predicting the next few steps is useful, the true power of Markov chains often lies in understanding their long-term behavior. This exercise introduces the concept of the stationary distribution, which describes the probabilities of being in each state after the system has run for a long time. By analyzing a simple model of website user navigation, you will practice solving for this equilibrium state algebraically, a fundamental technique in many applications. ",
            "id": "1639030",
            "problem": "A simplified model for user navigation on a small commercial website is described by a discrete-time Markov chain. The website consists of three distinct pages: a Home page (State 1), a Blog page (State 2), and a Contact page (State 3). The browsing behavior is modeled by the following transition probabilities:\n\n- A user on the Home page (State 1) will navigate to the Blog page (State 2) with probability $p_1$ or to the Contact page (State 3) with probability $1-p_1$.\n- A user on the Blog page (State 2) will return to the Home page (State 1) with probability $p_2$, navigate to the Contact page (State 3) with probability $p_3$, or remain on the Blog page with the remaining probability.\n- A user on the Contact page (State 3) will always return to the Home page (State 1).\n\nAll parameters $p_1, p_2, p_3$ are constants in the interval $(0, 1)$, and it is given that $p_2 + p_3 < 1$. This set of conditions ensures the Markov chain is ergodic, meaning it has a unique stationary distribution.\n\nAssuming a user has been browsing the website for a very long time, the process reaches a stationary state. Determine the stationary distribution $\\pi = (\\pi_1, \\pi_2, \\pi_3)$, where $\\pi_i$ is the long-term probability of finding the user on page $i$. Express your answer as a row vector in terms of the parameters $p_1$, $p_2$, and $p_3$.",
            "solution": "Let the transition matrix, with states ordered as Home (1), Blog (2), Contact (3), be\n$$\nP=\\begin{pmatrix}\n0 & p_{1} & 1-p_{1} \\\\\np_{2} & 1-p_{2}-p_{3} & p_{3} \\\\\n1 & 0 & 0\n\\end{pmatrix}.\n$$\nA stationary distribution $\\pi=(\\pi_{1},\\pi_{2},\\pi_{3})$ satisfies $\\pi=\\pi P$ and $\\pi_{1}+\\pi_{2}+\\pi_{3}=1$. Writing the stationarity equations componentwise gives\n$$\n\\pi_{1}=p_{2}\\pi_{2}+\\pi_{3},\\quad\n\\pi_{2}=p_{1}\\pi_{1}+(1-p_{2}-p_{3})\\pi_{2},\\quad\n\\pi_{3}=(1-p_{1})\\pi_{1}+p_{3}\\pi_{2}.\n$$\nFrom the second equation,\n$$\n(p_{2}+p_{3})\\pi_{2}=p_{1}\\pi_{1}\\quad\\Rightarrow\\quad \\pi_{2}=\\frac{p_{1}}{p_{2}+p_{3}}\\pi_{1}.\n$$\nSubstituting this into the third equation yields\n$$\n\\pi_{3}=(1-p_{1})\\pi_{1}+p_{3}\\cdot\\frac{p_{1}}{p_{2}+p_{3}}\\pi_{1}\n=\\frac{(1-p_{1})p_{2}+p_{3}}{p_{2}+p_{3}}\\pi_{1}.\n$$\nApply the normalization condition:\n$$\n\\pi_{1}+\\pi_{2}+\\pi_{3}\n=\\pi_{1}\\left[1+\\frac{p_{1}}{p_{2}+p_{3}}+\\frac{(1-p_{1})p_{2}+p_{3}}{p_{2}+p_{3}}\\right]\n=1.\n$$\nCombining terms in the bracket gives\n$$\n1+\\frac{p_{1}+(1-p_{1})p_{2}+p_{3}}{p_{2}+p_{3}}\n=\\frac{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}{p_{2}+p_{3}},\n$$\nso\n$$\n\\pi_{1}=\\frac{p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}.\n$$\nTherefore,\n$$\n\\pi_{2}=\\frac{p_{1}}{p_{2}+p_{3}}\\pi_{1}=\\frac{p_{1}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}},\\quad\n\\pi_{3}=\\frac{(1-p_{1})p_{2}+p_{3}}{p_{2}+p_{3}}\\pi_{1}=\\frac{(1-p_{1})p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}.\n$$\nThus the stationary distribution, as a row vector, is\n$$\n\\left(\\frac{p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}},\\ \\frac{p_{1}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}},\\ \\frac{(1-p_{1})p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}\\right).\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}&\\frac{p_{1}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}&\\frac{(1-p_{1})p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving from pen-and-paper calculations to computational implementation is a key step for any computational scientist. This final practice challenges you to build a complete computational model of a dynamic system, synthesizing the concepts of state evolution and stationary distributions. You will not only simulate the system but also use your model to evaluate hypothetical 'interventions,' demonstrating how Markov chains can serve as powerful tools for analysis and decision-making in practical scenarios. ",
            "id": "3158351",
            "problem": "Consider a Discrete-Time Markov Chain (DTMC) with a finite state space $\\mathcal{S} = \\{\\text{Attentive}, \\text{Distracted}, \\text{Absent}\\}$ indexed as $0, 1, 2$ respectively. Let $P$ be the state transition matrix where $P_{ij}$ is the probability of transitioning from state $i$ to state $j$ in one time step. Let $\\pi_0$ be an initial distribution over states written as a row vector, and let $H$ be a positive integer horizon representing the number of time steps to evaluate. We define the expected number of time steps in which the process is in the Attentive state over the horizon $H$ as the sum of the probabilities of being in the Attentive state at each time step.\n\nAn intervention is modeled as a modification to $P$ by shifting probability mass within a specified row from a source column to a target column by a nonnegative amount $\\delta$, while preserving row stochasticity. This is represented as a quadruple $(r, s, t, \\delta)$ meaning: in row $r$, decrease $P_{r s}$ by $\\delta$ (capped at the available $P_{r s}$ if necessary) and increase $P_{r t}$ by the same $\\delta$. Multiple such shifts may be applied sequentially for a given intervention. All indices are $0$-based. After modification, any small numerical deviations should be corrected so each row sums to $1$.\n\nYour tasks for each test case are:\n1. Compute the expected number of Attentive time steps over the horizon $H$ using the baseline matrix $P$ and initial distribution $\\pi_0$.\n2. For a given set of candidate interventions, select the single intervention that maximizes the expected number in item $1$. Break ties by selecting the intervention with the larger long-run Attentive fraction (defined in item $3$), and if still tied, select the smallest index intervention.\n3. Compute the long-run Attentive fraction for both the baseline $P$ and the selected intervention. The long-run fraction is the Attentive component of a stationary distribution $\\pi_\\infty$ satisfying $\\pi_\\infty P = \\pi_\\infty$ and $\\sum_i (\\pi_\\infty)_i = 1$.\n\nYou must implement this using the foundational definitions of Markov chains and linearity of expectation. No heuristic shortcuts are permitted. The final output must be a single line containing a list of per-test-case results, where each test-case result is a list in the format $[E_{\\text{base}}, E_{\\text{best}}, \\pi_{\\infty,\\text{att}}^{\\text{base}}, \\pi_{\\infty,\\text{att}}^{\\text{best}}, \\text{best\\_index}]$. Round all floating-point values to $6$ decimal places. The final line must be exactly this list format, with no additional text.\n\nUse the following test suite. For each test case, the states are ordered as $[0=\\text{Attentive}, 1=\\text{Distracted}, 2=\\text{Absent}]$.\n\nTest case $1$ (general classroom dynamics):\n- $P = \\begin{bmatrix} 0.6 & 0.3 & 0.1 \\\\ 0.4 & 0.4 & 0.2 \\\\ 0.2 & 0.3 & 0.5 \\end{bmatrix}$\n- $\\pi_0 = \\begin{bmatrix} 1.0 & 0.0 & 0.0 \\end{bmatrix}$\n- $H = 10$\n- Candidate interventions:\n  - Index $0$: $(1,1,0,0.1)$\n  - Index $1$: $(2,2,0,0.1)$\n  - Index $2$: $(0,2,0,0.05)$ and $(0,1,0,0.05)$\n\nTest case $2$ (near-attentive stability from distracted start):\n- $P = \\begin{bmatrix} 0.95 & 0.05 & 0.0 \\\\ 0.7 & 0.3 & 0.0 \\\\ 0.3 & 0.2 & 0.5 \\end{bmatrix}$\n- $\\pi_0 = \\begin{bmatrix} 0.0 & 1.0 & 0.0 \\end{bmatrix}$\n- $H = 5$\n- Candidate interventions:\n  - Index $0$: $(1,1,0,0.15)$\n  - Index $1$: $(2,2,0,0.2)$\n  - Index $2$: $(0,1,0,0.04)$\n\nTest case $3$ (absent state absorbing in baseline):\n- $P = \\begin{bmatrix} 0.5 & 0.4 & 0.1 \\\\ 0.3 & 0.3 & 0.4 \\\\ 0.0 & 0.0 & 1.0 \\end{bmatrix}$\n- $\\pi_0 = \\begin{bmatrix} 0.5 & 0.5 & 0.0 \\end{bmatrix}$\n- $H = 8$\n- Candidate interventions:\n  - Index $0$: $(1,2,0,0.2)$\n  - Index $1$: $(0,2,0,0.1)$\n  - Index $2$: $(2,2,0,0.05)$\n\nTest case $4$ (short horizon, mixed initial):\n- $P = \\begin{bmatrix} 0.7 & 0.2 & 0.1 \\\\ 0.2 & 0.7 & 0.1 \\\\ 0.1 & 0.2 & 0.7 \\end{bmatrix}$\n- $\\pi_0 = \\begin{bmatrix} \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{bmatrix}$\n- $H = 1$\n- Candidate interventions:\n  - Index $0$: $(0,1,0,0.1)$\n  - Index $1$: $(1,2,0,0.1)$\n  - Index $2$: $(2,2,0,0.2)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case, and each such element is itself a comma-separated list $[E_{\\text{base}}, E_{\\text{best}}, \\pi_{\\infty,\\text{att}}^{\\text{base}}, \\pi_{\\infty,\\text{att}}^{\\text{best}}, \\text{best\\_index}]$ with floating-point values rounded to $6$ decimal places.",
            "solution": "The user wants a solution to a problem involving Discrete-Time Markov Chains (DTMCs). I will first validate the problem statement and then, if valid, provide a reasoned solution.\n\n### Step 1: Extract Givens\n- **Model**: A Discrete-Time Markov Chain (DTMC) with a state space $\\mathcal{S} = \\{\\text{Attentive}, \\text{Distracted}, \\text{Absent}\\}$ indexed by $\\{0, 1, 2\\}$.\n- **Parameters**: A state transition matrix $P$, an initial distribution row vector $\\pi_0$, and a positive integer horizon $H$.\n- **Metric of Interest**: The expected number of time steps in the Attentive state over horizon $H$, defined as the sum of the probabilities of being in the Attentive state at each time step.\n- **Intervention**: A modification to $P$ defined by a quadruple $(r, s, t, \\delta)$, where probability mass $\\delta$ is shifted from $P_{rs}$ to $P_{rt}$ in row $r$. The shift is capped by the available probability $P_{rs}$. Rows are re-normalized to sum to $1$ after modification.\n- **Tasks**:\n  1.  Compute the expected number of Attentive steps ($E_{\\text{base}}$) for the baseline $P$.\n  2.  Select the best intervention from a candidate set, which maximizes this expectation. Ties are broken first by the long-run Attentive fraction, then by the smallest intervention index.\n  3.  Compute the long-run Attentive fraction for the baseline ($\\pi_{\\infty,\\text{att}}^{\\text{base}}$) and the best intervention's matrix ($\\pi_{\\infty,\\text{att}}^{\\text{best}}$). This is the Attentive-state component of the stationary distribution $\\pi_\\infty$ satisfying $\\pi_\\infty P = \\pi_\\infty$.\n- **Output**: For each test case, a list $[E_{\\text{base}}, E_{\\text{best}}, \\pi_{\\infty,\\text{att}}^{\\text{base}}, \\pi_{\\infty,\\text{att}}^{\\text{best}}, \\text{best\\_index}]$, with all floating-point values rounded to $6$ decimal places.\n- **Test Cases**: Four test cases are provided with specific values for $P$, $\\pi_0$, $H$, and candidate interventions.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded (Critical)**: The problem is rooted in the standard theory of DTMCs. The concepts of transition matrices, state distributions over time, expected values, and stationary distributions are fundamental to this field. The problem is scientifically sound.\n2.  **Well-Posed**: The problem is well-defined. The objectives are quantitative and clear. The metric for expectation and the criteria for selecting the best intervention, including tie-breaking rules, are unambiguously specified, ensuring a unique solution can be determined.\n3.  **Objective (Critical)**: The problem statement uses precise, formal mathematical language, devoid of any subjectivity or ambiguity.\n4.  **Completeness and Consistency**: All required data for each test case ($P, \\pi_0, H$, and interventions) are provided. The definitions are consistent with standard literature on Markov chains. The provided transition matrices are valid stochastic matrices (rows sum to $1$).\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed to design and implement a solution.\n\n### Solution Design\n\nThe solution will be structured into three core computational functions, which will be applied to each test case.\n\n**1. Expected Number of Attentive Time Steps**\nLet $\\pi_k$ be the state distribution at time step $k$. It is a row vector where $(\\pi_k)_j = P(X_k = j)$. The evolution of the distribution is given by $\\pi_k = \\pi_{k-1} P = \\pi_0 P^k$.\nThe problem defines the expected number of Attentive steps over a horizon $H$ as the sum of probabilities of being in the Attentive state (state $0$) at each step. Assuming time steps $k \\in \\{0, 1, \\dots, H-1\\}$, this is:\n$$E_{\\text{att}} = \\sum_{k=0}^{H-1} P(X_k = 0) = \\sum_{k=0}^{H-1} (\\pi_k)_0 = \\sum_{k=0}^{H-1} (\\pi_0 P^k)_0$$\nThis can be computed iteratively: initialize $\\pi_{\\text{current}} = \\pi_0$ and a running sum. In a loop for $H$ steps, add $(\\pi_{\\text{current}})_0$ to the sum and update $\\pi_{\\text{current}} \\leftarrow \\pi_{\\text{current}} P$.\n\n**2. Stationary Distribution and Long-Run Fraction**\nThe stationary distribution $\\pi_\\infty$ is a probability distribution that is invariant under the transition matrix $P$, i.e., $\\pi_\\infty P = \\pi_\\infty$. This can be rewritten as a system of homogeneous linear equations:\n$$\\pi_\\infty (P - I) = \\mathbf{0}$$\nwhere $I$ is the identity matrix and $\\mathbf{0}$ is a zero vector. To obtain a unique solution, we add the constraint that the elements of $\\pi_\\infty$ must sum to $1$: $\\sum_i (\\pi_\\infty)_i = 1$.\nThis system can be solved using standard linear algebra. Transposing the equation gives $(P^T - I^T)\\pi_\\infty^T = \\mathbf{0}^T$. We can form a matrix $A = P^T - I$, replace one of its rows (e.g., the last one) with a row of all ones to enforce the summation constraint, and form a corresponding right-hand-side vector $\\mathbf{b}$ (e.g., $[0, 0, \\dots, 1]^T$). The system $A \\pi_\\infty^T = \\mathbf{b}$ can then be solved for $\\pi_\\infty^T$. The long-run Attentive fraction is simply the first component, $(\\pi_\\infty)_0$. This method works for both irreducible chains and reducible chains with transient states leading to absorbing states, as seen in the test cases.\n\n**3. Intervention Application and Selection**\nAn intervention consists of one or more shifts $(r, s, t, \\delta)$. For each shift, the probability mass is moved within row $r$. The amount of mass moved, $\\Delta$, is $\\min(\\delta, P_{rs})$. The matrix is updated as $P'_{rs} = P_{rs} - \\Delta$ and $P'_{rt} = P_{rt} + \\Delta$. After all shifts prescribed for a candidate intervention are applied, the modified rows must be re-normalized to ensure they sum to $1$.\n\nFor each test case, the overall procedure is:\n1.  Calculate $E_{\\text{base}}$ and $\\pi_{\\infty,\\text{att}}^{\\text{base}}$ using the baseline matrix $P$.\n2.  For each candidate intervention (indexed $0, 1, \\dots$):\n    a. Apply the specified shifts to a copy of $P$ to create a new matrix, $P_{\\text{new}}$.\n    b. Calculate $E_{\\text{new}}$ and $\\pi_{\\infty,\\text{att}}^{\\text{new}}$ using $P_{\\text{new}}$.\n    c. Store these results along with the intervention index.\n3.  Compare the results for all candidates based on the specified criteria:\n    a. Primary: Maximize the expected Attentive steps, $E$.\n    b. Tie-breaker 1: Maximize the long-run Attentive fraction, $\\pi_{\\infty,\\text{att}}$.\n    c. Tie-breaker 2: Select the smallest intervention index.\n4.  This is efficiently accomplished by sorting the candidates' results in descending order of $E$, then descending order of $\\pi_{\\infty,\\text{att}}$, and finally ascending order of the index. The top entry after sorting is the best intervention.\n5.  Extract the statistics for the best intervention ($E_{\\text{best}}, \\pi_{\\infty,\\text{att}}^{\\text{best}}, \\text{best\\_index}$).\n6.  Assemble the five required values and format them as specified. This process is repeated for all test cases.",
            "answer": "```python\nimport numpy as np\n\ndef apply_interventions(p_matrix, interventions):\n    \"\"\"Applies a list of sequential interventions to a transition matrix.\"\"\"\n    p_new = p_matrix.copy()\n    modified_rows = set()\n    for r, s, t, delta in interventions:\n        if r < 0 or r >= p_new.shape[0] or s < 0 or s >= p_new.shape[1] or t < 0 or t >= p_new.shape[1]:\n            continue # Invalid index\n        \n        actual_delta = min(delta, p_new[r, s])\n        p_new[r, s] -= actual_delta\n        p_new[r, t] += actual_delta\n        modified_rows.add(r)\n    \n    # Re-normalize modified rows to correct for any floating point deviations\n    for r in modified_rows:\n        row_sum = np.sum(p_new[r, :])\n        if row_sum > 0:\n            p_new[r, :] /= row_sum\n    return p_new\n\ndef calculate_expected_attentive_steps(p_matrix, pi0, h):\n    \"\"\"Calculates the expected number of attentive steps over a horizon H.\"\"\"\n    pi_current = pi0.copy()\n    expected_steps = 0.0\n    for _ in range(h):\n        # The probability of being in the attentive state (index 0) at the current step\n        expected_steps += pi_current[0, 0]\n        # Evolve the distribution to the next step\n        pi_current = pi_current @ p_matrix\n    return expected_steps\n\ndef calculate_stationary_distribution(p_matrix):\n    \"\"\"Calculates the stationary distribution of a transition matrix P.\"\"\"\n    n_states = p_matrix.shape[0]\n    # We solve the system pi * (P - I) = 0 with constraint sum(pi) = 1.\n    # This is equivalent to (P^T - I^T) * pi^T = 0^T.\n    A = p_matrix.T - np.eye(n_states)\n    \n    # Replace the last row with the constraint sum(pi) = 1\n    A[-1, :] = 1.0\n    \n    # The right-hand side vector for the linear system\n    b = np.zeros(n_states)\n    b[-1] = 1.0\n    \n    # Solve A * pi^T = b for pi^T\n    try:\n        pi_T = np.linalg.solve(A, b)\n        return pi_T.T # Return as a row vector\n    except np.linalg.LinAlgError:\n        # Fallback for singular matrices if standard solve fails\n        pi_T, _, _, _ = np.linalg.lstsq(A, b, rcond=None)\n        return pi_T.T\n\ndef solve_case(p_matrix, pi0, h, candidate_interventions):\n    \"\"\"Solves a single test case.\"\"\"\n    # 1. Baseline calculations\n    e_base = calculate_expected_attentive_steps(p_matrix, pi0, h)\n    pi_inf_base = calculate_stationary_distribution(p_matrix)\n    pi_inf_att_base = pi_inf_base[0]\n\n    # 2. Evaluate candidate interventions\n    candidate_results = []\n    # Use sorted items to ensure consistent processing order\n    for index, intervention_spec in sorted(candidate_interventions.items()):\n        p_candidate = apply_interventions(p_matrix, intervention_spec)\n        e_candidate = calculate_expected_attentive_steps(p_candidate, pi0, h)\n        pi_inf_candidate = calculate_stationary_distribution(p_candidate)\n        pi_inf_att_candidate = pi_inf_candidate[0]\n        \n        candidate_results.append({\n            'index': index,\n            'E': e_candidate,\n            'pi_inf_att': pi_inf_att_candidate,\n        })\n    \n    # 3. Sort to find the best intervention based on the specified criteria\n    # Primary: Maximize E (desc). Secondary: Maximize pi_inf_att (desc). Tertiary: Smallest index (asc).\n    # Sorting by -index with reverse=True achieves ascending order for the index tie-breaker.\n    best_intervention_stats = sorted(\n        candidate_results, \n        key=lambda x: (x['E'], x['pi_inf_att'], -x['index']), \n        reverse=True\n    )[0]\n    \n    e_best = best_intervention_stats['E']\n    pi_inf_att_best = best_intervention_stats['pi_inf_att']\n    best_index = best_intervention_stats['index']\n    \n    return [e_base, e_best, pi_inf_att_base, pi_inf_att_best, best_index]\n\ndef solve():\n    \"\"\"Main function to run all test cases and print the final result.\"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            'P': np.array([[0.6, 0.3, 0.1], [0.4, 0.4, 0.2], [0.2, 0.3, 0.5]]),\n            'pi0': np.array([[1.0, 0.0, 0.0]]),\n            'H': 10,\n            'interventions': {\n                0: [(1, 1, 0, 0.1)],\n                1: [(2, 2, 0, 0.1)],\n                2: [(0, 2, 0, 0.05), (0, 1, 0, 0.05)]\n            }\n        },\n        # Test case 2\n        {\n            'P': np.array([[0.95, 0.05, 0.0], [0.7, 0.3, 0.0], [0.3, 0.2, 0.5]]),\n            'pi0': np.array([[0.0, 1.0, 0.0]]),\n            'H': 5,\n            'interventions': {\n                0: [(1, 1, 0, 0.15)],\n                1: [(2, 2, 0, 0.2)],\n                2: [(0, 1, 0, 0.04)]\n            }\n        },\n        # Test case 3\n        {\n            'P': np.array([[0.5, 0.4, 0.1], [0.3, 0.3, 0.4], [0.0, 0.0, 1.0]]),\n            'pi0': np.array([[0.5, 0.5, 0.0]]),\n            'H': 8,\n            'interventions': {\n                0: [(1, 2, 0, 0.2)],\n                1: [(0, 2, 0, 0.1)],\n                2: [(2, 2, 0, 0.05)]\n            }\n        },\n        # Test case 4\n        {\n            'P': np.array([[0.7, 0.2, 0.1], [0.2, 0.7, 0.1], [0.1, 0.2, 0.7]]),\n            'pi0': np.array([[1/3, 1/3, 1/3]]),\n            'H': 1,\n            'interventions': {\n                0: [(0, 1, 0, 0.1)],\n                1: [(1, 2, 0, 0.1)],\n                2: [(2, 2, 0, 0.2)]\n            }\n        }\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        result = solve_case(case['P'], case['pi0'], case['H'], case['interventions'])\n        # Format the result list into the required string format\n        r_str = f\"[{result[0]:.6f},{result[1]:.6f},{result[2]:.6f},{result[3]:.6f},{int(result[4])}]\"\n        all_results_str.append(r_str)\n\n    # Combine all case results into a single line string representation of a list of lists.\n    final_output = f\"[{','.join(all_results_str)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}