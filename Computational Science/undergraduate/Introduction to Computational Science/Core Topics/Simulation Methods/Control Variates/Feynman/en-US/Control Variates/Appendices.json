{
    "hands_on_practices": [
        {
            "introduction": "To effectively apply the control variates method, it's essential to first understand its theoretical underpinnings. This practice guides you through a first-principles derivation of the optimal control variate coefficient, $\\beta^{\\star}$, in the elegant context of estimating $\\pi$ using a geometric Monte Carlo simulation . By working through this problem, you will solidify your grasp of the core formula $\\beta^{\\star} = \\frac{\\operatorname{Cov}(Y, X)}{\\operatorname{Var}(X)}$ and see how abstract statistical concepts connect to a tangible application, achieving a significant reduction in variance.",
            "id": "3218869",
            "problem": "A common Monte Carlo approach to estimate the constant $\\pi$ is to sample $N$ independent and identically distributed (i.i.d.) points uniformly from the square $[-1,1] \\times [-1,1]$ and use the fraction that fall inside the unit circle centered at the origin. Let $Y_i$ be the indicator random variable $Y_i = \\mathbf{1}\\{(U_i,V_i) \\in \\{(x,y): x^2 + y^2 \\leq 1\\}\\}$, where $(U_i,V_i)$ is the $i$-th sample, and define the estimator $\\hat{\\pi} = 4 \\,\\bar{Y}_N$, where $\\bar{Y}_N = \\frac{1}{N} \\sum_{i=1}^{N} Y_i$.\n\nTo reduce the variance using the method of control variates, introduce the indicator random variable $X_i = \\mathbf{1}\\{(U_i,V_i) \\in P_{12}\\}$, where $P_{12}$ is the regular dodecagon (regular $12$-gon) inscribed in the same unit circle. The expectation $\\mathbb{E}[X_i]$ is the area of $P_{12}$ divided by the area of the square $[-1,1] \\times [-1,1]$. Consider the adjusted estimator\n$$\n\\hat{\\pi}_{\\beta} = 4 \\left( \\bar{Y}_N - \\beta \\left( \\bar{X}_N - \\mathbb{E}[X_i] \\right) \\right),\n$$\nwhere $\\bar{X}_N = \\frac{1}{N} \\sum_{i=1}^{N} X_i$ and $\\beta \\in \\mathbb{R}$ is a coefficient to be chosen.\n\nStarting only from the fundamental definitions of expectation, variance, covariance, and the area of a regular polygon inscribed in a circle, derive the expression for the optimal control variate coefficient $\\beta^{\\star}$ that minimizes the variance of $\\hat{\\pi}_{\\beta}$, and then compute its exact value for the regular dodecagon $P_{12}$. Express your final answer as a single closed-form analytic expression. No numerical rounding is required.",
            "solution": "The objective is to find the optimal control variate coefficient $\\beta^{\\star}$ that minimizes the variance of the estimator $\\hat{\\pi}_{\\beta}$. The estimator is given by\n$$ \\hat{\\pi}_{\\beta} = 4 \\left( \\bar{Y}_N - \\beta \\left( \\bar{X}_N - \\mathbb{E}[X_i] \\right) \\right) $$\nwhere $\\bar{Y}_N = \\frac{1}{N} \\sum_{i=1}^{N} Y_i$ and $\\bar{X}_N = \\frac{1}{N} \\sum_{i=1}^{N} X_i$. The random variables $(Y_i, X_i)$ are independent and identically distributed for $i=1, \\dots, N$. For simplicity, we denote a generic pair as $(Y, X)$.\n\nFirst, we derive a general expression for the optimal coefficient $\\beta^{\\star}$. We start by computing the variance of $\\hat{\\pi}_{\\beta}$.\n$$ \\text{Var}(\\hat{\\pi}_{\\beta}) = \\text{Var}\\left( 4 \\left( \\bar{Y}_N - \\beta \\bar{X}_N + \\beta \\mathbb{E}[X] \\right) \\right) $$\nThe term $\\beta \\mathbb{E}[X]$ is a constant, which does not affect the variance. Thus,\n$$ \\text{Var}(\\hat{\\pi}_{\\beta}) = \\text{Var}(4(\\bar{Y}_N - \\beta \\bar{X}_N)) = 16 \\, \\text{Var}(\\bar{Y}_N - \\beta \\bar{X}_N) $$\nUsing the properties of variance,\n$$ \\text{Var}(\\bar{Y}_N - \\beta \\bar{X}_N) = \\text{Var}(\\bar{Y}_N) + \\beta^2 \\text{Var}(\\bar{X}_N) - 2\\beta \\text{Cov}(\\bar{Y}_N, \\bar{X}_N) $$\nSince the samples are i.i.d., the variance of the sample mean is $\\frac{1}{N}$ times the variance of a single observation, and similarly for the covariance of sample means:\n$$ \\text{Var}(\\bar{Y}_N) = \\frac{1}{N} \\text{Var}(Y), \\quad \\text{Var}(\\bar{X}_N) = \\frac{1}{N} \\text{Var}(X), \\quad \\text{Cov}(\\bar{Y}_N, \\bar{X}_N) = \\frac{1}{N} \\text{Cov}(Y, X) $$\nSubstituting these into the expression for the variance of $\\hat{\\pi}_{\\beta}$:\n$$ \\text{Var}(\\hat{\\pi}_{\\beta}) = \\frac{16}{N} \\left( \\text{Var}(Y) + \\beta^2 \\text{Var}(X) - 2\\beta \\text{Cov}(Y, X) \\right) $$\nTo find the value of $\\beta$ that minimizes this variance, we differentiate with respect to $\\beta$ and set the result to zero.\n$$ \\frac{d}{d\\beta} \\text{Var}(\\hat{\\pi}_{\\beta}) = \\frac{16}{N} \\left( 2\\beta \\text{Var}(X) - 2 \\text{Cov}(Y, X) \\right) $$\nSetting this to zero yields:\n$$ 2\\beta^{\\star} \\text{Var}(X) - 2 \\text{Cov}(Y, X) = 0 $$\nSolving for the optimal coefficient $\\beta^{\\star}$, we get the standard result:\n$$ \\beta^{\\star} = \\frac{\\text{Cov}(Y, X)}{\\text{Var}(X)} $$\nNow, we must compute $\\text{Cov}(Y, X)$ and $\\text{Var}(X)$ for the specific problem. The sampling domain is the square $S = [-1,1] \\times [-1,1]$, which has an area of $A_S = 2 \\times 2 = 4$.\nThe random variables are defined as indicators:\n$Y = \\mathbf{1}\\{C_1\\}$, where $C_1$ is the unit circle, $\\{(x,y) : x^2 + y^2 \\leq 1\\}$.\n$X = \\mathbf{1}\\{P_{12}\\}$, where $P_{12}$ is the regular dodecagon inscribed in the unit circle.\n\nFor an indicator variable $Z = \\mathbf{1}\\{R\\}$ corresponding to a region $R$, its expectation is the probability of a uniformly sampled point falling in $R$, which is $\\mathbb{E}[Z] = \\frac{\\text{Area}(R)}{\\text{Area}(S)}$.\n\nThe area of the unit circle is $A_C = \\pi (1)^2 = \\pi$. Therefore,\n$$ \\mathbb{E}[Y] = \\frac{A_C}{A_S} = \\frac{\\pi}{4} $$\nThe area of a regular $n$-gon inscribed in a circle of radius $R$ is given by the formula $A_n = \\frac{1}{2} n R^2 \\sin(\\frac{2\\pi}{n})$. For the dodecagon $P_{12}$, we have $n=12$ and $R=1$:\n$$ A_{12} = \\frac{1}{2} (12) (1)^2 \\sin\\left(\\frac{2\\pi}{12}\\right) = 6 \\sin\\left(\\frac{\\pi}{6}\\right) = 6 \\cdot \\frac{1}{2} = 3 $$\nThe expectation of $X$ is therefore:\n$$ \\mathbb{E}[X] = \\frac{A_{12}}{A_S} = \\frac{3}{4} $$\nNext, we compute the variance of $X$. Since $X$ is an indicator variable (a Bernoulli random variable), its variance is given by $\\text{Var}(X) = \\mathbb{E}[X](1-\\mathbb{E}[X])$.\n$$ \\text{Var}(X) = \\frac{3}{4} \\left(1 - \\frac{3}{4}\\right) = \\frac{3}{4} \\cdot \\frac{1}{4} = \\frac{3}{16} $$\nNow we compute the covariance, $\\text{Cov}(Y, X) = \\mathbb{E}[YX] - \\mathbb{E}[Y]\\mathbb{E}[X]$.\nThe product of the indicator variables is $YX = \\mathbf{1}\\{C_1\\} \\cdot \\mathbf{1}\\{P_{12}\\} = \\mathbf{1}\\{C_1 \\cap P_{12}\\}$.\nSince the dodecagon $P_{12}$ is inscribed in the unit circle $C_1$, any point in $P_{12}$ is also in $C_1$. This means the region $P_{12}$ is a subset of the region $C_1$, i.e., $P_{12} \\subseteq C_1$.\nTherefore, their intersection is just the dodecagon itself: $C_1 \\cap P_{12} = P_{12}$.\nThis implies that $YX = \\mathbf{1}\\{P_{12}\\} = X$.\nSo, the expectation of the product is:\n$$ \\mathbb{E}[YX] = \\mathbb{E}[X] = \\frac{3}{4} $$\nNow we can calculate the covariance:\n$$ \\text{Cov}(Y, X) = \\mathbb{E}[YX] - \\mathbb{E}[Y]\\mathbb{E}[X] = \\frac{3}{4} - \\left(\\frac{\\pi}{4}\\right)\\left(\\frac{3}{4}\\right) = \\frac{3}{4} - \\frac{3\\pi}{16} = \\frac{12 - 3\\pi}{16} = \\frac{3(4-\\pi)}{16} $$\nFinally, we substitute the covariance and variance into the formula for $\\beta^{\\star}$:\n$$ \\beta^{\\star} = \\frac{\\text{Cov}(Y, X)}{\\text{Var}(X)} = \\frac{\\frac{3(4-\\pi)}{16}}{\\frac{3}{16}} $$\nThe common factor of $\\frac{3}{16}$ in the numerator and denominator cancels out, leaving:\n$$ \\beta^{\\star} = 4 - \\pi $$\nThis is the optimal control variate coefficient.",
            "answer": "$$\n\\boxed{4-\\pi}\n$$"
        },
        {
            "introduction": "While theoretical derivations provide crucial insight, real-world applications of control variates require us to work directly with sample data. This hands-on coding exercise  challenges you to estimate a definite integral by empirically determining the optimal coefficient $\\beta$ from your Monte Carlo samples. You will not only implement the variance reduction technique in a practical setting but also compare its performance against other numerical methods, highlighting the trade-offs involved in computational science.",
            "id": "3218755",
            "problem": "You are to implement and analyze Monte Carlo (MC) integration with control variates for the integral $$I=\\int_{0}^{1} e^{-x^2}\\,dx,$$ using two different control variates and a hybrid comparison with Gauss–Legendre (GL) quadrature on the residual. The target is to demonstrate principled variance reduction by choosing the control variate coefficient empirically from sampled data, and to compare the outcome with deterministic quadrature applied to the residual function.\n\nUse the following fundamental base:\n- The expectation of a function under a probability density integrates that function over its domain: if $X \\sim \\mathrm{Uniform}(0,1)$, then $$\\mathbb{E}[f(X)] = \\int_{0}^{1} f(x)\\,dx.$$\n- For any integrable function $g$ with known mean $\\mu_g = \\int_{0}^{1} g(x)\\,dx$, the control variate adjusted random variable $$Z_\\beta = f(X) - \\beta \\big(g(X) - \\mu_g\\big)$$ satisfies $$\\mathbb{E}[Z_\\beta] = \\mathbb{E}[f(X)],$$ for any scalar $\\beta \\in \\mathbb{R}$, because $\\mathbb{E}[g(X)] = \\mu_g$.\n- Variance is minimized by an appropriate choice of $\\beta$ obtained by minimizing the variance of $Z_\\beta$ in a data-driven manner.\n\nYour task is to:\n1. Implement MC estimators for the integral $I$ using $X_i \\sim \\mathrm{Uniform}(0,1)$, independent and identically distributed, for $i=1,\\dots,N$, where $N$ is the number of samples. The function is $f(x)=e^{-x^2}$.\n2. Use the control variates $g_1(x)=1-x$ and $g_2(x)=x(1-x)$, whose exact means over $[0,1]$ are $\\mu_{g_1}=\\int_{0}^{1} (1-x)\\,dx = \\tfrac{1}{2}$ and $\\mu_{g_2}=\\int_{0}^{1} x(1-x)\\,dx = \\tfrac{1}{6}$.\n3. Empirically select $\\beta$ from random samples by minimizing the sample variance of the adjusted values $Z_\\beta$ over $\\beta \\in \\mathbb{R}$. You must do this strictly from sampled data with no closed-form formula provided in the problem statement. If the empirical variance of the control variate is numerically zero, set $\\beta=0$ to avoid instability.\n4. Construct the following three estimators and compute their absolute errors with respect to the exact value of $I$:\n   - Plain MC: $$\\hat{I}_{\\mathrm{MC}} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i).$$\n   - MC with control variates (empirical $\\beta$): $$\\hat{I}_{\\mathrm{CV}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[f(X_i) - \\beta \\big(g(X_i) - \\mu_g\\big)\\right].$$\n   - GL quadrature on the residual using the empirically selected $\\beta$: define the residual function $$r_\\beta(x) = f(x) - \\beta \\big(g(x) - \\mu_g\\big),$$ then approximate $$\\hat{I}_{\\mathrm{GL,res}} \\approx \\int_{0}^{1} r_\\beta(x)\\,dx$$ using $m$-point Gauss–Legendre quadrature on $[0,1]$. Use nodes and weights from standard $m$-point Gauss–Legendre quadrature on $[-1,1]$ mapped to $[0,1]$ via $t=\\tfrac{1}{2}(u+1)$ with weights scaled by $\\tfrac{1}{2}$. Note that if $g$ is a polynomial of low degree, and $m$ is sufficiently large, GL quadrature integrates $g$ exactly, and thus applying GL to $r_\\beta$ can coincide with applying GL directly to $f$; nevertheless, perform the residual quadrature as specified.\n\nThe exact value of $I$ can be expressed using the Gaussian error function as $$I = \\frac{\\sqrt{\\pi}}{2}\\,\\mathrm{erf}(1).$$ Use this to compute absolute errors.\n\nTest Suite:\nProvide a program that evaluates the three absolute errors for each of the following parameter sets:\n- Case $1$ (happy path, quadratic control variate): $N=100000$, seed $=2024$, control variate $g_2$, $m=16$.\n- Case $2$ (happy path, linear control variate): $N=100000$, seed $=99$, control variate $g_1$, $m=16$.\n- Case $3$ (small $N$ boundary, coarse quadrature not exact for quadratic): $N=10$, seed $=7$, control variate $g_2$, $m=1$.\n- Case $4$ (extreme $N=1$ edge, linear control variate, coarse quadrature): $N=1$, seed $=42$, control variate $g_1$, $m=1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the absolute errors in the order of cases $1$ through $4$, and within each case, in the order plain MC, MC with control variates, GL residual. That is, the output format is $$[\\mathrm{err}_{1,\\mathrm{MC}},\\mathrm{err}_{1,\\mathrm{CV}},\\mathrm{err}_{1,\\mathrm{GLres}},\\mathrm{err}_{2,\\mathrm{MC}},\\mathrm{err}_{2,\\mathrm{CV}},\\mathrm{err}_{2,\\mathrm{GLres}},\\mathrm{err}_{3,\\mathrm{MC}},\\mathrm{err}_{3,\\mathrm{CV}},\\mathrm{err}_{3,\\mathrm{GLres}},\\mathrm{err}_{4,\\mathrm{MC}},\\mathrm{err}_{4,\\mathrm{CV}},\\mathrm{err}_{4,\\mathrm{GLres}}].$$",
            "solution": "The problem requires the implementation and analysis of three numerical methods for approximating the definite integral $I=\\int_{0}^{1} e^{-x^2}\\,dx$. The methods are: standard Monte Carlo (MC) integration, MC integration with a control variate (CV), and Gauss-Legendre (GL) quadrature applied to a residual function derived from the control variate method. The analysis will compare the absolute errors of these three estimators against the known exact value of the integral for a set of specified test cases.\n\nThe function to be integrated is $f(x) = e^{-x^2}$ over the domain $[0, 1]$. The basis for the stochastic methods is the generation of $N$ independent and identically distributed random samples $X_i \\sim \\mathrm{Uniform}(0,1)$. The integral $I$ is then equivalent to the expectation $\\mathbb{E}[f(X)]$.\n\nThe exact value of the integral is required for error computation and is given by $I = \\frac{\\sqrt{\\pi}}{2}\\,\\mathrm{erf}(1)$, where $\\mathrm{erf}$ is the Gaussian error function.\n\nThe problem specifies two potential control variates:\n1.  A linear function $g_1(x) = 1-x$, with known mean over $[0,1]$:\n    $$\\mu_{g_1} = \\int_{0}^{1} (1-x)\\,dx = \\left[x - \\frac{x^2}{2}\\right]_{0}^{1} = 1 - \\frac{1}{2} = \\frac{1}{2}.$$\n2.  A quadratic function $g_2(x) = x(1-x)$, with known mean over $[0,1]$:\n    $$\\mu_{g_2} = \\int_{0}^{1} (x-x^2)\\,dx = \\left[\\frac{x^2}{2} - \\frac{x^3}{3}\\right]_{0}^{1} = \\frac{1}{2} - \\frac{1}{3} = \\frac{1}{6}.$$\n\nThe process for determining each of the three estimators is as follows:\n\n1.  **Plain Monte Carlo (MC) Estimator**\n    The standard MC estimator, $\\hat{I}_{\\mathrm{MC}}$, is the sample mean of the function $f(x)$ evaluated at the random points $X_i$:\n    $$\\hat{I}_{\\mathrm{MC}} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i).$$\n    By the Law of Large Numbers, $\\hat{I}_{\\mathrm{MC}} \\to I$ as $N \\to \\infty$.\n\n2.  **Control Variate (CV) Estimator**\n    This method aims to reduce the variance of the MC estimator. It introduces an adjusted random variable $Z_\\beta = f(X) - \\beta(g(X) - \\mu_g)$, where $g(x)$ is a control variate with a known mean $\\mu_g$. The expectation of $Z_\\beta$ is $\\mathbb{E}[f(X)] = I$, for any scalar coefficient $\\beta \\in \\mathbb{R}$. The estimator is the mean of the samples of this new variable:\n    $$\\hat{I}_{\\mathrm{CV}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left[f(X_i) - \\beta \\big(g(X_i) - \\mu_g\\big)\\right].$$\n    The optimal coefficient, $\\beta^*$, minimizes the variance $\\mathrm{Var}(Z_\\beta)$. This variance is a quadratic function of $\\beta$:\n    $$\\mathrm{Var}(Z_\\beta) = \\mathrm{Var}(f(X) - \\beta g(X)) = \\mathrm{Var}(f(X)) - 2\\beta \\mathrm{Cov}(f(X), g(X)) + \\beta^2 \\mathrm{Var}(g(X)).$$\n    Minimizing with respect to $\\beta$ yields the optimal value:\n    $$\\beta^* = \\frac{\\mathrm{Cov}(f(X), g(X))}{\\mathrm{Var}(g(X))}.$$\n    The problem mandates that $\\beta$ be estimated empirically from the generated samples. Let $f_i = f(X_i)$ and $g_i = g(X_i)$. The sample-based estimate $\\hat{\\beta}$ is calculated as:\n    $$\\hat{\\beta} = \\frac{\\widehat{\\mathrm{Cov}}(f, g)}{\\widehat{\\mathrm{Var}}(g)} = \\frac{\\sum_{i=1}^{N} (f_i - \\bar{f})(g_i - \\bar{g})}{\\sum_{i=1}^{N} (g_i - \\bar{g})^2},$$\n    where $\\bar{f}$ and $\\bar{g}$ are the respective sample means. If the denominator, which is proportional to the sample variance of $g$, is numerically zero (which occurs if $N \\le 1$ or if all $g_i$ are coincidentally equal), $\\hat{\\beta}$ is set to $0$ as per the problem instructions to prevent numerical instability. The CV estimator can then be computed efficiently as:\n    $$\\hat{I}_{\\mathrm{CV}} = \\bar{f} - \\hat{\\beta}(\\bar{g} - \\mu_g).$$\n\n3.  **Gauss-Legendre Quadrature on the Residual (GL,res) Estimator**\n    This method combines the control variate concept with deterministic quadrature. First, the empirical coefficient $\\hat{\\beta}$ is determined from the $N$ MC samples as described above. Then, a residual function $r_{\\hat{\\beta}}(x)$ is defined:\n    $$r_{\\hat{\\beta}}(x) = f(x) - \\hat{\\beta}(g(x) - \\mu_g).$$\n    The integral of this residual function is then approximated using $m$-point Gauss-Legendre quadrature. The standard GL quadrature rule is defined for the interval $[-1, 1]$:\n    $$\\int_{-1}^{1} h(u)\\,du \\approx \\sum_{j=1}^{m} w_j h(u_j),$$\n    where $u_j$ are the nodes (roots of the $m$-th degree Legendre polynomial) and $w_j$ are the corresponding weights. To apply this to the interval $[0, 1]$, a linear change of variables $x = \\frac{1}{2}(u+1)$ is used, with $dx = \\frac{1}{2}du$. The quadrature rule becomes:\n    $$\\int_{0}^{1} r_{\\hat{\\beta}}(x)\\,dx \\approx \\sum_{j=1}^{m} \\frac{w_j}{2} r_{\\hat{\\beta}}\\left(\\frac{u_j+1}{2}\\right).$$\n    This sum provides the estimator $\\hat{I}_{\\mathrm{GL,res}}$. The principle here is that if $g(x)$ is a good approximation to $f(x)$, the residual $r_{\\hat{\\beta}}(x)$ will be a \"smoother\" or smaller-magnitude function than $f(x)$ itself, potentially leading to a more accurate result from a fixed-order quadrature rule.\n\nFor each of the four test cases, these three estimators are computed. Finally, their absolute errors are calculated with respect to the exact value $I = \\frac{\\sqrt{\\pi}}{2}\\,\\mathrm{erf}(1)$.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and analyzes Monte Carlo integration with control variates\n    and a hybrid Gauss-Legendre quadrature method for the integral of exp(-x^2) from 0 to 1.\n    \"\"\"\n\n    # Define the primary function and the control variates\n    def f(x):\n        return np.exp(-x**2)\n\n    def g1(x):\n        return 1.0 - x\n\n    def g2(x):\n        return x * (1.0 - x)\n\n    # Dictionary to map control variate names to functions and their exact means\n    g_map = {\n        'g1': {'func': g1, 'mean': 0.5},\n        'g2': {'func': g2, 'mean': 1.0 / 6.0}\n    }\n\n    # Test cases as specified in the problem statement\n    test_cases = [\n        {'N': 100000, 'seed': 2024, 'g_name': 'g2', 'm': 16},\n        {'N': 100000, 'seed': 99, 'g_name': 'g1', 'm': 16},\n        {'N': 10, 'seed': 7, 'g_name': 'g2', 'm': 1},\n        {'N': 1, 'seed': 42, 'g_name': 'g1', 'm': 1}\n    ]\n\n    # Exact value of the integral for error calculation\n    i_exact = (math.sqrt(math.pi) / 2.0) * math.erf(1.0)\n\n    results = []\n\n    for case in test_cases:\n        N = case['N']\n        seed = case['seed']\n        g_name = case['g_name']\n        m = case['m']\n\n        g_func = g_map[g_name]['func']\n        mu_g = g_map[g_name]['mean']\n\n        # --- Monte Carlo Sampling ---\n        rng = np.random.default_rng(seed)\n        x_samples = rng.uniform(0.0, 1.0, N)\n\n        f_samples = f(x_samples)\n        g_samples = g_func(x_samples)\n\n        # --- Estimator 1: Plain Monte Carlo ---\n        i_mc = np.mean(f_samples)\n\n        # --- Estimator 2: Monte Carlo with Control Variates ---\n        # Empirically determine beta\n        if N = 1:\n            beta = 0.0\n        else:\n            # Using the direct formula for beta to avoid numerical issues with np.var at N=1\n            # The scaling factors (like 1/(N-1)) cancel in the ratio.\n            g_samples_mean = np.mean(g_samples)\n            # Sum of squared deviations for variance term\n            var_g_numerator = np.sum((g_samples - g_samples_mean)**2)\n\n            if var_g_numerator  1e-15:  # Check for numerically zero variance\n                beta = 0.0\n            else:\n                f_samples_mean = np.mean(f_samples)\n                # Sum of products of deviations for covariance term\n                cov_fg_numerator = np.sum((f_samples - f_samples_mean) * (g_samples - g_samples_mean))\n                beta = cov_fg_numerator / var_g_numerator\n\n        f_mean = np.mean(f_samples)\n        g_mean = np.mean(g_samples)\n        i_cv = f_mean - beta * (g_mean - mu_g)\n\n        # --- Estimator 3: Gauss-Legendre Quadrature on the Residual ---\n        # Get standard GL nodes and weights for [-1, 1]\n        gl_nodes_std, gl_weights_std = np.polynomial.legendre.leggauss(m)\n\n        # Transform nodes and weights for [0, 1]\n        gl_nodes = 0.5 * (gl_nodes_std + 1.0)\n        gl_weights = 0.5 * gl_weights_std\n        \n        # Define the residual function using the empirically found beta\n        def r_beta(x, beta_val, g_function, g_mean_val):\n            return f(x) - beta_val * (g_function(x) - g_mean_val)\n\n        # Evaluate residual at the transformed GL nodes\n        r_vals_at_nodes = r_beta(gl_nodes, beta, g_func, mu_g)\n        \n        # Compute the integral estimator\n        i_gl_res = np.sum(gl_weights * r_vals_at_nodes)\n\n        # --- Calculate Absolute Errors ---\n        err_mc = abs(i_mc - i_exact)\n        err_cv = abs(i_cv - i_exact)\n        err_gl_res = abs(i_gl_res - i_exact)\n        \n        results.extend([err_mc, err_cv, err_gl_res])\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The true power of control variates is unlocked when we look beyond simple linear relationships. This advanced problem  presents a carefully constructed scenario where the linear correlation between two variables is zero, a situation that might naively suggest no benefit from using one to control the other. By solving it, you will discover how a well-chosen non-linear control variate can still yield a dramatic reduction in variance, demonstrating a more profound level of statistical dependence that is key to sophisticated modeling.",
            "id": "3218904",
            "problem": "Consider a Monte Carlo estimation problem for the mean $\\mu = \\mathbb{E}[X]$ where you have access to samples of two random variables $X$ and $Y$. You will construct a joint distribution of $(X,Y)$ for which the Pearson correlation coefficient $\\rho(X,Y)$ is zero, yet a non-linear control variate $Z = g(Y)$ leads to a very large variance reduction. Use only foundational definitions of expectation, variance, covariance, and properties of the normal distribution.\n\nLet $Y \\sim \\mathcal{N}(0,1)$ and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$, independent of $Y$, with $\\sigma^{2} = 0.01$. Define\n$$\nX = (Y^{2} - 1) + \\varepsilon\n$$\nand consider the non-linear control variate\n$$\nZ = g(Y) = Y^{2} - 1,\n$$\nwhose mean $\\mathbb{E}[Z]$ is known from the distribution of $Y$.\n\nTasks:\n- Using the core definitions of covariance and properties of moments of the standard normal distribution, verify that the covariance $\\operatorname{Cov}(X,Y)$ equals $0$, and hence the correlation $\\rho(X,Y)$ equals $0$.\n- Starting from the definition of variance and covariance and without assuming any particular formula, derive the value of the coefficient $\\beta$ that minimizes the variance of the adjusted estimator for $\\mu$ given by $X - \\beta(Z - \\mathbb{E}[Z])$.\n- Compute the per-sample variance of the adjusted estimator at the optimal choice of $\\beta$, and the variance reduction factor\n$$\nR = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big)}.\n$$\n\nProvide your final answer as the ordered pair $(\\beta^{\\star}, R)$ in exact form. No rounding is required. The final answer must be a calculation as specified.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It constitutes a formal exercise in probability theory and numerical methods. All necessary information is provided, and the tasks are well-defined. We may proceed with the solution.\n\nThe problem asks for three tasks related to a Monte Carlo estimation scenario. We are given the random variables $Y \\sim \\mathcal{N}(0,1)$ and $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma^{2} = 0.01$. The random variables $Y$ and $\\varepsilon$ are independent. The variable of interest is $X = (Y^{2} - 1) + \\varepsilon$, and the control variate is $Z = g(Y) = Y^{2} - 1$.\n\nFirst, we establish the necessary moments of the standard normal variable $Y$.\nThe probability density function of $Y$ is symmetric about $0$. Therefore, all odd moments of $Y$ are zero.\n$\\mathbb{E}[Y] = 0$\n$\\mathbb{E}[Y^3] = 0$\nThe even moments are well-known. The second moment is the variance, since the mean is zero:\n$\\mathbb{E}[Y^2] = \\operatorname{Var}(Y) = 1$\nThe fourth moment of a standard normal distribution is:\n$\\mathbb{E}[Y^4] = 3$\n\nWith these, we can determine the properties of $X$ and $Z$.\nThe mean of $Z$ is:\n$\\mathbb{E}[Z] = \\mathbb{E}[Y^2 - 1] = \\mathbb{E}[Y^2] - 1 = 1 - 1 = 0$.\nThe mean of $X$, which is the quantity $\\mu$ to be estimated, is:\n$\\mu = \\mathbb{E}[X] = \\mathbb{E}[(Y^2 - 1) + \\varepsilon] = \\mathbb{E}[Y^2 - 1] + \\mathbb{E}[\\varepsilon] = \\mathbb{E}[Z] + 0 = 0$.\n\n**Task 1: Verify $\\operatorname{Cov}(X,Y) = 0$**\n\nThe covariance is defined as $\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]$.\nWe have $\\mathbb{E}[Y] = 0$, so the second term vanishes:\n$\\operatorname{Cov}(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X] \\cdot 0 = \\mathbb{E}[XY]$.\nWe compute $\\mathbb{E}[XY]$ by substituting the definition of $X$:\n$\\mathbb{E}[XY] = \\mathbb{E}[((Y^2 - 1) + \\varepsilon)Y] = \\mathbb{E}[Y^3 - Y + \\varepsilon Y]$.\nBy linearity of expectation:\n$\\mathbb{E}[XY] = \\mathbb{E}[Y^3] - \\mathbb{E}[Y] + \\mathbb{E}[\\varepsilon Y]$.\nAs established, $\\mathbb{E}[Y^3] = 0$ and $\\mathbb{E}[Y] = 0$. For the last term, since $\\varepsilon$ and $Y$ are independent, $\\mathbb{E}[\\varepsilon Y] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[Y]$. We know $\\mathbb{E}[\\varepsilon]=0$ and $\\mathbb{E}[Y]=0$, so $\\mathbb{E}[\\varepsilon Y] = 0 \\cdot 0 = 0$.\nTherefore, $\\mathbb{E}[XY] = 0 - 0 + 0 = 0$.\nThis confirms that $\\operatorname{Cov}(X,Y) = 0$.\nThe Pearson correlation coefficient is $\\rho(X,Y) = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$. Since the numerator is $0$ and the variances are non-zero (as will be shown), $\\rho(X,Y) = 0$.\n\n**Task 2: Derive the optimal coefficient $\\beta^{\\star}$**\n\nWe are tasked to find the value of $\\beta$, denoted $\\beta^{\\star}$, that minimizes the variance of the adjusted estimator $X_{\\beta} = X - \\beta(Z - \\mathbb{E}[Z])$. Let $V(\\beta) = \\operatorname{Var}(X_{\\beta})$.\nSince $\\mathbb{E}[Z] = 0$, the estimator is $X_{\\beta} = X - \\beta Z$.\nThe variance to be minimized is:\n$V(\\beta) = \\operatorname{Var}(X - \\beta Z)$.\nUsing the properties of variance, where $\\beta$ is a constant:\n$V(\\beta) = \\operatorname{Var}(X) + \\operatorname{Var}(-\\beta Z) + 2\\operatorname{Cov}(X, -\\beta Z)$.\n$V(\\beta) = \\operatorname{Var}(X) + \\beta^{2}\\operatorname{Var}(Z) - 2\\beta\\operatorname{Cov}(X, Z)$.\nThis is a quadratic function of $\\beta$. To find the minimum, we compute the derivative with respect to $\\beta$ and set it to zero:\n$\\frac{d V}{d\\beta} = \\frac{d}{d\\beta} \\left( \\operatorname{Var}(X) + \\beta^{2}\\operatorname{Var}(Z) - 2\\beta\\operatorname{Cov}(X, Z) \\right) = 2\\beta\\operatorname{Var}(Z) - 2\\operatorname{Cov}(X, Z)$.\nSetting the derivative to zero:\n$2\\beta\\operatorname{Var}(Z) - 2\\operatorname{Cov}(X, Z) = 0$.\nSolving for the optimal coefficient $\\beta^{\\star}$:\n$\\beta^{\\star} = \\frac{\\operatorname{Cov}(X, Z)}{\\operatorname{Var}(Z)}$.\nThe second derivative $\\frac{d^2 V}{d\\beta^2} = 2\\operatorname{Var}(Z)$ is positive since variance is non-negative (and non-zero in this case), confirming that this is a minimum.\n\n**Task 3: Compute $\\beta^{\\star}$ and the variance reduction factor $R$**\n\nTo compute $\\beta^{\\star}$, we need to calculate $\\operatorname{Var}(Z)$ and $\\operatorname{Cov}(X,Z)$.\nFirst, we find the variance of the control variate $Z$:\n$\\operatorname{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$. Since $\\mathbb{E}[Z]=0$, $\\operatorname{Var}(Z) = \\mathbb{E}[Z^2]$.\n$\\operatorname{Var(Z)} = \\mathbb{E}[(Y^2-1)^2] = \\mathbb{E}[Y^4 - 2Y^2 + 1]$.\nUsing linearity of expectation and the moments of $Y$:\n$\\operatorname{Var}(Z) = \\mathbb{E}[Y^4] - 2\\mathbb{E}[Y^2] + 1 = 3 - 2(1) + 1 = 2$.\n\nNext, we calculate the covariance between $X$ and $Z$:\n$\\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ] - \\mathbb{E}[X]\\mathbb{E}[Z]$.\nSince $\\mathbb{E}[X] = 0$ and $\\mathbb{E}[Z] = 0$, we have $\\operatorname{Cov}(X,Z) = \\mathbb{E}[XZ]$.\nSubstitute $X = Z + \\varepsilon$:\n$\\operatorname{Cov}(X,Z) = \\mathbb{E}[(Z+\\varepsilon)Z] = \\mathbb{E}[Z^2 + \\varepsilon Z] = \\mathbb{E}[Z^2] + \\mathbb{E}[\\varepsilon Z]$.\nWe know $\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) = 2$.\nFor the term $\\mathbb{E}[\\varepsilon Z]$, we use the independence of $\\varepsilon$ and $Y$. Since $Z = Y^2-1$ is a function of $Y$, $Z$ and $\\varepsilon$ are also independent.\nTherefore, $\\mathbb{E}[\\varepsilon Z] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[Z] = 0 \\cdot 0 = 0$.\nSo, $\\operatorname{Cov}(X,Z) = 2 + 0 = 2$.\n\nNow we can compute the optimal coefficient $\\beta^{\\star}$:\n$\\beta^{\\star} = \\frac{\\operatorname{Cov}(X, Z)}{\\operatorname{Var}(Z)} = \\frac{2}{2} = 1$.\n\nFinally, we compute the variance reduction factor $R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big)}$.\nFirst, the numerator $\\operatorname{Var}(X)$:\n$X = Z + \\varepsilon$. Since $Z$ and $\\varepsilon$ are independent, the variance of their sum is the sum of their variances:\n$\\operatorname{Var}(X) = \\operatorname{Var}(Z) + \\operatorname{Var}(\\varepsilon) = 2 + \\sigma^2 = 2 + 0.01 = 2.01$.\n\nNext, the denominator, which is the variance of the adjusted estimator with the optimal $\\beta^{\\star}=1$ and $\\mathbb{E}[Z]=0$:\n$\\operatorname{Var}\\big(X - \\beta^{\\star}(Z - \\mathbb{E}[Z])\\big) = \\operatorname{Var}(X - 1 \\cdot (Z - 0)) = \\operatorname{Var}(X-Z)$.\nSubstitute $X = Z + \\varepsilon$:\n$\\operatorname{Var}(X-Z) = \\operatorname{Var}((Z+\\varepsilon) - Z) = \\operatorname{Var}(\\varepsilon)$.\nWe are given $\\operatorname{Var}(\\varepsilon) = \\sigma^2 = 0.01$.\n\nNow, we can compute the variance reduction factor $R$:\n$R = \\frac{\\operatorname{Var}(X)}{\\operatorname{Var}(X-Z)} = \\frac{2.01}{0.01} = 201$.\n\nThe final answer is the ordered pair $(\\beta^{\\star}, R)$.\n$\\beta^{\\star} = 1$\n$R = 201$\nThe final pair is $(1, 201)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  201 \\end{pmatrix}}\n$$"
        }
    ]
}