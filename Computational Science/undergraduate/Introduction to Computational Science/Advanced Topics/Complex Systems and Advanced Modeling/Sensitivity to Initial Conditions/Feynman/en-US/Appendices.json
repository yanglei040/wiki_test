{
    "hands_on_practices": [
        {
            "introduction": "This first exercise introduces a fundamental tool for analyzing chaotic systems: the Jacobian matrix. By calculating how a small separation vector evolves after a few steps of the discrete-time Hénon map, you will gain hands-on experience with linearizing a nonlinear system to understand its local stretching and contracting properties. This practice  provides the mathematical foundation for quantifying sensitivity to initial conditions.",
            "id": "1705927",
            "problem": "Consider the Hénon map, a discrete-time dynamical system described by the equations:\n$$x_{n+1} = 1 - a x_n^2 + y_n$$\n$$y_{n+1} = b x_n$$\n\nThis map describes the evolution of a point $(x_n, y_n)$ in the plane. For this problem, use the classical chaotic parameters $a = 1.4$ and $b = 0.3$.\n\nA trajectory is generated by starting with an initial point $P_0 = (x_0, y_0)$ and repeatedly applying the map to find $P_1, P_2, P_3, \\dots$. The local evolution of an infinitesimal separation vector $\\delta\\mathbf{v}_n = (\\delta x_n, \\delta y_n)$ between two nearby trajectories is governed by the Jacobian matrix of the map.\n\nSuppose a system starts at the initial point $P_0 = (0, 0)$. Two trajectories are initiated, one exactly at $P_0$ and another at an infinitesimally close point separated by the initial vector $\\delta\\mathbf{v}_0 = (0, \\epsilon)$, where $\\epsilon$ is a very small positive constant. After two iterations of the map, the separation vector between the two trajectories is $\\delta\\mathbf{v}_2$.\n\nCalculate the amplification factor of the separation, defined as the ratio of the final separation magnitude to the initial separation magnitude, $||\\delta\\mathbf{v}_2|| / ||\\delta\\mathbf{v}_0||$.\n\nExpress your final answer as a single real number, rounded to three significant figures.",
            "solution": "The Hénon map is given by\n$$x_{n+1}=1-a x_{n}^{2}+y_{n}, \\quad y_{n+1}=b x_{n}.$$\nThe linearized evolution of an infinitesimal separation vector $\\delta \\mathbf{v}_{n}=(\\delta x_{n},\\delta y_{n})$ is governed by\n$$\\delta \\mathbf{v}_{n+1}=J(P_{n})\\,\\delta \\mathbf{v}_{n},$$\nwhere $P_{n}=(x_{n},y_{n})$ is the reference trajectory and the Jacobian matrix is\n$$J(x,y)=\\begin{pmatrix}\\frac{\\partial x_{n+1}}{\\partial x_{n}}  \\frac{\\partial x_{n+1}}{\\partial y_{n}}\\\\ \\frac{\\partial y_{n+1}}{\\partial x_{n}}  \\frac{\\partial y_{n+1}}{\\partial y_{n}}\\end{pmatrix}\n=\\begin{pmatrix}-2 a x  1\\\\ b  0\\end{pmatrix}.$$\n\nWith parameters $a=1.4$ and $b=0.3$, start from $P_{0}=(0,0)$. The reference trajectory evolves as\n$$x_{1}=1-a x_{0}^{2}+y_{0}=1,\\quad y_{1}=b x_{0}=0,$$\nso $P_{1}=(1,0)$.\n\nEvaluate the Jacobians along the reference trajectory:\n$$J(P_{0})=J(0,0)=\\begin{pmatrix}0  1\\\\ b  0\\end{pmatrix}=\\begin{pmatrix}0  1\\\\ 0.3  0\\end{pmatrix},$$\n$$J(P_{1})=J(1,0)=\\begin{pmatrix}-2 a  1\\\\ b  0\\end{pmatrix}=\\begin{pmatrix}-2.8  1\\\\ 0.3  0\\end{pmatrix}.$$\n\nThe initial separation is $\\delta \\mathbf{v}_{0}=(0,\\epsilon)$. After one iteration,\n$$\\delta \\mathbf{v}_{1}=J(P_{0})\\,\\delta \\mathbf{v}_{0}=\\begin{pmatrix}0  1\\\\ 0.3  0\\end{pmatrix}\\begin{pmatrix}0\\\\ \\epsilon\\end{pmatrix}=\\begin{pmatrix}\\epsilon\\\\ 0\\end{pmatrix}.$$\nAfter the second iteration,\n$$\\delta \\mathbf{v}_{2}=J(P_{1})\\,\\delta \\mathbf{v}_{1}=\\begin{pmatrix}-2.8  1\\\\ 0.3  0\\end{pmatrix}\\begin{pmatrix}\\epsilon\\\\ 0\\end{pmatrix}=\\begin{pmatrix}-2.8\\,\\epsilon\\\\ 0.3\\,\\epsilon\\end{pmatrix}.$$\n\nThe amplification factor is the ratio of magnitudes:\n$$\\frac{\\|\\delta \\mathbf{v}_{2}\\|}{\\|\\delta \\mathbf{v}_{0}\\|}=\\frac{\\sqrt{(-2.8\\,\\epsilon)^{2}+(0.3\\,\\epsilon)^{2}}}{\\sqrt{0^{2}+\\epsilon^{2}}}=\\sqrt{2.8^{2}+0.3^{2}}=\\sqrt{7.84+0.09}=\\sqrt{7.93}.$$\nNumerically,\n$$\\sqrt{7.93}\\approx 2.82 \\quad \\text{(to three significant figures)}.$$",
            "answer": "$$\\boxed{2.82}$$"
        },
        {
            "introduction": "Moving from a discrete map to a continuous flow, this problem explores the famous Lorenz system, a simplified model of atmospheric convection. You will use a given model of exponential divergence to calculate the time it takes for an infinitesimally small difference in starting points to grow to a macroscopic scale. This exercise  provides a tangible understanding of the \"predictability horizon\" and the practical implications of the butterfly effect.",
            "id": "1705925",
            "problem": "The Lorenz system is a system of three coupled, nonlinear ordinary differential equations that provides a simplified model for atmospheric convection. The equations are given by:\n$$\n\\frac{dx}{dt} = \\sigma (y - x)\n$$\n$$\n\\frac{dy}{dt} = x (\\rho - z) - y\n$$\n$$\n\\frac{dz}{dt} = xy - \\beta z\n$$\nFor this problem, consider the standard parameter values: $\\sigma = 10$, $\\rho = 28$, and $\\beta = 8/3$.\n\nA key feature of this system is its extreme sensitivity to initial conditions, a hallmark of chaotic systems. To investigate this, two simulations are run. Simulation A is initiated from the point $(x_A(0), y_A(0), z_A(0)) = (1.0, 1.0, 1.0)$. Simulation B is initiated from a nearly identical point, $(x_B(0), y_B(0), z_B(0)) = (1.0 + 10^{-5}, 1.0, 1.0)$.\n\nFor a short period, the separation between the trajectories grows exponentially. A detailed numerical study finds that the magnitude of the difference between the $z$-coordinates of the two trajectories, $\\Delta z(t) = |z_A(t) - z_B(t)|$, can be well-approximated by the model:\n$$\n\\Delta z(t) = C \\exp(\\lambda t)\n$$\nwhere $t$ is the dimensionless time. The constants in this model are empirically determined from the simulation data to be $C = 5.0 \\times 10^{-5}$ and $\\lambda = 0.91$ (in units of inverse dimensionless time).\n\nAssuming this exponential growth model holds, calculate the time at which the difference $\\Delta z(t)$ grows to a magnitude of $10.0$, a value which is a significant fraction of the overall range of the $z$-coordinate in the Lorenz attractor. Express your answer in dimensionless time units, rounded to three significant figures.",
            "solution": "We model the separation by $\\Delta z(t) = C \\exp(\\lambda t)$ and seek the time $t$ when $\\Delta z(t) = 10.0$. Set\n$$\n10 = C \\exp(\\lambda t).\n$$\nSolve for $t$ by isolating the exponential and taking the natural logarithm:\n$$\n\\exp(\\lambda t) = \\frac{10}{C}, \\quad \\lambda t = \\ln\\!\\left(\\frac{10}{C}\\right), \\quad t = \\frac{1}{\\lambda}\\,\\ln\\!\\left(\\frac{10}{C}\\right).\n$$\nWith $C = 5.0 \\times 10^{-5}$ and $\\lambda = 0.91$,\n$$\n\\frac{10}{C} = \\frac{10}{5.0 \\times 10^{-5}} = 2.0 \\times 10^{5},\n$$\nso\n$$\nt = \\frac{1}{0.91}\\,\\ln\\!\\left(2.0 \\times 10^{5}\\right).\n$$\nCompute the logarithm using $\\ln\\!\\left(2.0 \\times 10^{5}\\right) = \\ln 2.0 + 5 \\ln 10 \\approx 0.69314718056 + 5 \\times 2.302585093 \\approx 12.206072646$.\nThus,\n$$\nt \\approx \\frac{12.206072646}{0.91} \\approx 13.41326664,\n$$\nwhich, rounded to three significant figures, is $13.4$ in dimensionless time units.",
            "answer": "$$\\boxed{13.4}$$"
        },
        {
            "introduction": "Our final practice synthesizes the previous concepts by formalizing the connection between local stretching and the long-term rate of divergence, quantified by the maximal Lyapunov exponent. For the classic Arnold's cat map, you will first derive the exact Lyapunov exponent from the eigenvalues of its constant Jacobian matrix. You will then verify this analytical result by computationally implementing numerical algorithms , a task that directly reflects the daily work of a computational scientist.",
            "id": "3191529",
            "problem": "Consider the discrete-time Arnold cat map on the two-dimensional torus, defined by the transformation $f : \\mathbb{T}^2 \\to \\mathbb{T}^2$ with $f(\\mathbf{x}) = M \\mathbf{x} \\bmod 1$, where $M = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}$ and the modulus is applied component-wise with respect to the unit square $[0,1)^2$. The Jacobian of this map is constant and equal to $M$ at every point on $\\mathbb{T}^2$.\n\nThe maximal Lyapunov exponent is defined, from first principles, as the long-time average of the logarithmic expansion rate of infinitesimal perturbations transported by the Jacobian along an orbit. For a differentiable map $f$ and an initial state $\\mathbf{x}_0$ with an initial perturbation $\\mathbf{v}_0 \\neq \\mathbf{0}$, this is\n$$\n\\lambda(\\mathbf{x}_0, \\mathbf{v}_0) = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{k=0}^{n-1} \\ln \\left( \\frac{\\left\\| Df(\\mathbf{x}_k)\\, \\mathbf{v}_k \\right\\|}{\\left\\| \\mathbf{v}_k \\right\\|} \\right),\n$$\nwhere $\\mathbf{x}_{k+1} = f(\\mathbf{x}_k)$ and $\\mathbf{v}_{k+1} = \\frac{Df(\\mathbf{x}_k)\\, \\mathbf{v}_k}{\\left\\| Df(\\mathbf{x}_k)\\, \\mathbf{v}_k \\right\\|}$, with $\\|\\cdot\\|$ denoting the Euclidean norm.\n\nTasks:\n- Derive the exact maximal Lyapunov exponent $\\lambda_{\\text{exact}}$ for the given $M$ starting from the fundamental definition above and the eigenstructure of $M$. Your program must compute a closed-form numerical value for $\\lambda_{\\text{exact}}$.\n- Numerically estimate the Lyapunov exponent using two independent approaches:\n  1. A tangent-space Benettin algorithm: iterate a normalized perturbation vector under the constant Jacobian $M$ and accumulate the average logarithmic stretch per step.\n  2. A two-trajectory separation method on the torus: iterate two nearby points under $f$, measure the minimal torus distance after each step, accumulate the average of $\\ln(\\text{distance ratio})$, and renormalize the separation to maintain it at a small fixed magnitude.\n\nFor the two-trajectory method, the minimal torus displacement should be computed per coordinate by wrapping differences into $(-\\tfrac{1}{2}, \\tfrac{1}{2}]$ using nearest-integer shifts. The renormalization step must rescale the displacement back to a fixed initial magnitude $d_0$ after each iteration. There are no physical units involved. Angles are not used. All outputs must be real numbers in plain decimal format.\n\nImplement a single program that computes the absolute error $|\\lambda_{\\text{num}} - \\lambda_{\\text{exact}}|$ for each of the following test cases, where $n$ denotes the number of iterations and $\\text{seed}$ sets a deterministic random number generator for reproducibility; the two-trajectory method uses $d_0 = 10^{-8}$:\n\n- Test case $1$: tangent-space method, $n = 1$, $\\text{seed} = 0$.\n- Test case $2$: tangent-space method, $n = 10$, $\\text{seed} = 1$.\n- Test case $3$: tangent-space method, $n = 1000$, $\\text{seed} = 2$.\n- Test case $4$: two-trajectory method, $n = 50$, $\\text{seed} = 3$, $d_0 = 10^{-8}$.\n- Test case $5$: two-trajectory method, $n = 200$, $\\text{seed} = 4$, $d_0 = 10^{-8}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order of the test cases listed above, for example $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$, where each $\\text{result}_i$ is the absolute error for the corresponding test case.",
            "solution": "The problem requires the calculation of the maximal Lyapunov exponent for the Arnold cat map, both analytically and through two distinct numerical methods, followed by a comparison of the numerical estimates against the exact value.\n\n### Part 1: Analytical Derivation of the Exact Maximal Lyapunov Exponent\n\nThe Arnold cat map is given by the transformation $f(\\mathbf{x}) = M \\mathbf{x} \\bmod 1$ on the two-dimensional torus $\\mathbb{T}^2$, with the matrix $M = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}$. The Jacobian of this map is constant for all $\\mathbf{x} \\in \\mathbb{T}^2$ and is equal to the matrix $M$ itself, i.e., $Df(\\mathbf{x}) = M$.\n\nThe maximal Lyapunov exponent $\\lambda$ quantifies the average exponential rate of separation of infinitesimally close trajectories. For a system with a constant Jacobian $M$, the evolution of an infinitesimal perturbation vector $\\mathbf{v}$ over $n$ iterations is governed by the matrix power $M^n$. The Lyapunov exponent is defined as:\n$$\n\\lambda = \\lim_{n \\to \\infty} \\frac{1}{n} \\ln \\left( \\frac{\\| M^n \\mathbf{v}_0 \\|}{\\| \\mathbf{v}_0 \\|} \\right)\n$$\nfor a generic initial perturbation $\\mathbf{v}_0$.\n\nTo evaluate this limit, we analyze the eigenstructure of $M$. Let the eigenvalues of $M$ be $\\mu_i$ and the corresponding eigenvectors be $\\mathbf{e}_i$. Any initial vector $\\mathbf{v}_0$ can be expressed as a linear combination of these eigenvectors (assuming they form a basis), $\\mathbf{v}_0 = \\sum_i c_i \\mathbf{e}_i$. Applying $M^n$ yields:\n$$\nM^n \\mathbf{v}_0 = M^n \\left(\\sum_i c_i \\mathbf{e}_i\\right) = \\sum_i c_i (M^n \\mathbf{e}_i) = \\sum_i c_i \\mu_i^n \\mathbf{e}_i\n$$\nLet's denote the eigenvalue with the largest magnitude as $\\mu_{\\max}$, so $|\\mu_{\\max}|  |\\mu_i|$ for all other $i$. For large $n$, the term corresponding to $\\mu_{\\max}$ will dominate the sum:\n$$\nM^n \\mathbf{v}_0 \\approx c_{\\max} \\mu_{\\max}^n \\mathbf{e}_{\\max}\n$$\nwhere $c_{\\max}$ and $\\mathbf{e}_{\\max}$ are the coefficient and eigenvector associated with $\\mu_{\\max}$, assuming the initial vector $\\mathbf{v}_0$ has a non-zero component along $\\mathbf{e}_{\\max}$ (which is true for a generic, i.e., non-special, choice of $\\mathbf{v}_0$).\n\nThe norm is then $\\|M^n \\mathbf{v}_0\\| \\approx |c_{\\max}| |\\mu_{\\max}|^n \\|\\mathbf{e}_{\\max}\\|$. Substituting this into the definition of $\\lambda$:\n$$\n\\lambda = \\lim_{n \\to \\infty} \\frac{1}{n} \\ln (\\|M^n \\mathbf{v}_0\\|) = \\lim_{n \\to \\infty} \\frac{1}{n} \\ln(|c_{\\max}| |\\mu_{\\max}|^n \\|\\mathbf{e}_{\\max}\\|) = \\lim_{n \\to \\infty} \\frac{1}{n} \\left(\\ln|c_{\\max}| + n \\ln|\\mu_{\\max}| + \\ln\\|\\mathbf{e}_{\\max}\\|\\right)\n$$\nAs $n \\to \\infty$, the terms not multiplied by $n$ become negligible, leaving:\n$$\n\\lambda_{\\text{exact}} = \\ln|\\mu_{\\max}|\n$$\nThus, the maximal Lyapunov exponent is the natural logarithm of the magnitude of the largest eigenvalue of the Jacobian matrix $M$.\n\nWe find the eigenvalues by solving the characteristic equation $\\det(M - \\mu I) = 0$:\n$$\n\\det \\begin{pmatrix} 1 - \\mu  1 \\\\ 1  2 - \\mu \\end{pmatrix} = (1 - \\mu)(2 - \\mu) - (1)(1) = 0\n$$\n$$\n\\mu^2 - 3\\mu + 2 - 1 = 0 \\implies \\mu^2 - 3\\mu + 1 = 0\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\mu = \\frac{-(-3) \\pm \\sqrt{(-3)^2 - 4(1)(1)}}{2} = \\frac{3 \\pm \\sqrt{5}}{2}\n$$\nThe two eigenvalues are $\\mu_1 = \\frac{3 + \\sqrt{5}}{2}$ and $\\mu_2 = \\frac{3 - \\sqrt{5}}{2}$. Both are positive real numbers. The largest eigenvalue is $\\mu_{\\max} = \\mu_1 = \\frac{3 + \\sqrt{5}}{2}$, which is the square of the golden ratio, $\\phi^2$.\n\nTherefore, the exact maximal Lyapunov exponent is:\n$$\n\\lambda_{\\text{exact}} = \\ln\\left(\\frac{3 + \\sqrt{5}}{2}\\right)\n$$\n\n### Part 2: Numerical Estimation Algorithms\n\n#### Method 1: Tangent-Space (Benettin) Algorithm\nThis algorithm directly implements the definition of the Lyapunov exponent by tracking the stretching of a tangent vector. Since the Jacobian $M$ is constant, the procedure simplifies considerably as it is independent of the orbit's position $\\mathbf{x}_k$.\n1. Initialize a random 2D unit vector $\\mathbf{v}_0$.\n2. For $k = 0, 1, \\dots, n-1$:\n   a. Apply the Jacobian to find the stretched vector: $\\mathbf{w}_k = M \\mathbf{v}_k$.\n   b. The stretch factor for this step is $s_k = \\|\\mathbf{w}_k\\|$.\n   c. Accumulate the logarithm of the stretch factor: $\\sum \\ln(s_k)$.\n   d. Renormalize the vector for the next iteration: $\\mathbf{v}_{k+1} = \\mathbf{w}_k / s_k$.\n3. The estimated Lyapunov exponent is the average of the logarithmic stretches:\n$$\n\\lambda_{\\text{num}} = \\frac{1}{n} \\sum_{k=0}^{n-1} \\ln(s_k)\n$$\n\n#### Method 2: Two-Trajectory Separation Method\nThis method approximates the evolution of an infinitesimal perturbation with the separation of two nearby trajectories.\n1. Initialize a random point $\\mathbf{x}_0 \\in [0,1)^2$ and a perturbed point $\\mathbf{y}_0 = \\mathbf{x}_0 + \\mathbf{\\delta}_0$, where $\\mathbf{\\delta}_0$ is a small displacement vector of magnitude $\\|\\mathbf{\\delta}_0\\| = d_0 = 10^{-8}$.\n2. For $k = 0, 1, \\dots, n-1$:\n   a. Evolve both points under the map: $\\mathbf{x}_{k+1} = M\\mathbf{x}_k \\pmod 1$ and $\\mathbf{y}_{k+1} = M\\mathbf{y}_k \\pmod 1$.\n   b. Calculate the separation vector $\\Delta\\mathbf{y} = \\mathbf{y}_{k+1} - \\mathbf{x}_{k+1}$.\n   c. To find the minimal distance on the torus, wrap each component of the separation vector into the interval $(-\\frac{1}{2}, \\frac{1}{2}]$. This is achieved by $\\delta'_i = \\Delta y_i - \\text{round}(\\Delta y_i)$. Let the resulting vector be $\\mathbf{\\delta}'$.\n   d. The separation distance is $d_{k+1} = \\|\\mathbf{\\delta}'\\|$. The ratio of separation is $d_{k+1}/d_0$.\n   e. Accumulate the logarithm of this ratio: $\\sum \\ln(d_{k+1}/d_0)$.\n   f. Rescale the separation to its original magnitude $d_0$ for the next iteration. This is done by adjusting the perturbed point: $\\mathbf{y}_{k+1} \\leftarrow \\mathbf{x}_{k+1} + \\mathbf{\\delta}' \\frac{d_0}{d_{k+1}}$.\n3. The estimated Lyapunov exponent is the average of the logarithmic ratios:\n$$\n\\lambda_{\\text{num}} = \\frac{1}{n} \\sum_{k=0}^{n-1} \\ln\\left(\\frac{d_{k+1}}{d_0}\\right)\n$$\n\n### Part 3: Implementation for Error Computation\nThe program will first compute the high-precision value of $\\lambda_{\\text{exact}}$. Then, for each test case, it will execute the specified numerical algorithm and compute $\\lambda_{\\text{num}}$. Finally, it will calculate the absolute error $|\\lambda_{\\text{num}} - \\lambda_{\\text{exact}}|$ and report the results as a list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef tangent_space_method(n, seed):\n    \"\"\"\n    Estimates the maximal Lyapunov exponent using the tangent-space Benettin algorithm.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    M = np.array([[1., 1.], [1., 2.]])\n    \n    # Initialize a random 2D unit vector\n    v = rng.standard_normal(2)\n    v /= np.linalg.norm(v)\n    \n    total_log_stretch = 0.0\n    for _ in range(n):\n        # Evolve the tangent vector by applying the Jacobian\n        w = M @ v\n        \n        # Calculate the stretch factor (norm of the new vector)\n        stretch = np.linalg.norm(w)\n        \n        # Accumulate the log of the stretch\n        if stretch > 0:\n            total_log_stretch += np.log(stretch)\n        \n        # Renormalize the vector for the next iteration\n        v = w / stretch\n        \n    return total_log_stretch / n if n > 0 else 0.0\n\ndef two_trajectory_method(n, seed, d0):\n    \"\"\"\n    Estimates the maximal Lyapunov exponent using the two-trajectory separation method.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    M = np.array([[1., 1.], [1., 2.]])\n    \n    # Initialize a random starting point on the torus\n    x = rng.random(2)\n    \n    # Create a perturbed point with a small initial separation d0\n    u = rng.standard_normal(2)\n    u /= np.linalg.norm(u)\n    y = x + d0 * u\n    \n    total_log_ratio = 0.0\n    for _ in range(n):\n        # Evolve both trajectories\n        x = (M @ x) % 1.0\n        y = (M @ y) % 1.0\n        \n        # Calculate the separation vector on the torus\n        delta = y - x\n        # Wrap the separation into the interval [-0.5, 0.5) for each component\n        delta -= np.round(delta)\n        \n        # Calculate the new separation distance\n        d_next = np.linalg.norm(delta)\n        \n        # Accumulate the log of the separation ratio\n        if d_next > 0 and d0 > 0:\n            total_log_ratio += np.log(d_next / d0)\n        \n        # Renormalize the separation for the next iteration\n        y = x + delta * (d0 / d_next)\n\n    return total_log_ratio / n if n > 0 else 0.0\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'method': 'tangent', 'n': 1, 'seed': 0},\n        {'method': 'tangent', 'n': 10, 'seed': 1},\n        {'method': 'tangent', 'n': 1000, 'seed': 2},\n        {'method': 'two_traj', 'n': 50, 'seed': 3, 'd0': 1e-8},\n        {'method': 'two_traj', 'n': 200, 'seed': 4, 'd0': 1e-8},\n    ]\n\n    # Calculate the exact Lyapunov exponent: log((3 + sqrt(5))/2)\n    lambda_exact = np.log((3.0 + np.sqrt(5.0)) / 2.0)\n    \n    results = []\n    for case in test_cases:\n        lambda_num = 0.0\n        if case['method'] == 'tangent':\n            lambda_num = tangent_space_method(case['n'], case['seed'])\n        elif case['method'] == 'two_traj':\n            lambda_num = two_trajectory_method(case['n'], case['seed'], case['d0'])\n        \n        # Calculate the absolute error\n        error = np.abs(lambda_num - lambda_exact)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}