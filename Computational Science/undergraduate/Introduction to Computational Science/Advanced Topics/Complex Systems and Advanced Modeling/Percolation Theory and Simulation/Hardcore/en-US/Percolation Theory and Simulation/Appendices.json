{
    "hands_on_practices": [
        {
            "introduction": "Percolation on two-dimensional lattices possesses a remarkable property known as duality, which provides an elegant connection between a system and its 'negative' image. This practice allows you to computationally verify this principle by showing that for any configuration, a horizontal crossing path of open bonds precludes a vertical crossing path of closed bonds . By implementing this check, you will gain a deeper appreciation for the geometric constraints in planar graphs and understand why the critical point for bond percolation on a square lattice is exactly $p=1/2$.",
            "id": "3171640",
            "problem": "Consider independent bond percolation on the square lattice: each edge is independently declared open with probability $p \\in [0,1]$ and closed otherwise. Let the primal graph be the usual grid graph on a rectangular array of $H \\times W$ vertices with nearest-neighbor edges (horizontal and vertical). The dual graph has vertices corresponding to faces of the primal, and its dual edges cross primal edges. In the percolation overlay, declare a dual edge open if and only if the corresponding primal edge is closed, so the dual percolation parameter is $1-p$. A left-right primal crossing is the existence of an open path connecting the left side (all vertices with column index $0$) to the right side (all vertices with column index $W-1$). A top-bottom dual crossing is the existence of an open path in the dual graph connecting the dual top boundary to the dual bottom boundary, where the dual boundary vertices are defined by adjacency to primal boundary edges.\n\nYour task is to write a complete, runnable program that empirically verifies planar duality by overlaying primal and dual configurations at $p$ and $1-p$ and checking that the events \"primal left-right crossing\" and \"dual top-bottom crossing\" are mutually exclusive and collectively exhaustive. In addition, for self-dual points (square domains with $H=W$ and parameter $p=1/2$), numerically examine the crossing probability of the primal left-right event.\n\nStart from the following foundational base:\n- Independence of bonds: each edge state is an independent Bernoulli random variable with parameter $p$.\n- Planarity and duality: in a planar rectangular grid, for any fixed configuration of open/closed primal edges, the dual graph’s open edges are exactly the primal graph’s closed edges. It is a well-tested fact that for a rectangle, either there exists an open left-right crossing in the primal or there exists an open top-bottom crossing in the dual, and these events are disjoint.\n- Algorithmic primitives: Breadth-First Search (BFS) for connectivity in the primal graph and Disjoint Set Union (DSU) for connectivity in the dual graph.\n\nImplement the following algorithmic specifications:\n- Generate primal edge states by sampling horizontal and vertical edges independently as open with probability $p$, using a fixed pseudorandom number generator seed $42$ for reproducibility. Dual edge states are defined deterministically as the complement of the corresponding primal edge states.\n- Detect a primal left-right open crossing using Breadth-First Search (BFS), starting from the left boundary vertices and exploring only along open edges; declare success if any right boundary vertex is reached.\n- Detect a dual top-bottom open crossing by building dual connectivity over faces using Disjoint Set Union (DSU). Add two special dual nodes representing the top and bottom boundaries. For each closed primal edge:\n  - If it is a horizontal edge on the top boundary, union the top boundary node with the adjacent face.\n  - If it is a horizontal edge in the interior, union the faces above and below the edge.\n  - If it is a horizontal edge on the bottom boundary, union the bottom boundary node with the adjacent face.\n  - If it is a vertical edge in the interior (not on the extreme left or right boundary), union the faces to the left and right of the edge.\nDeclare a dual top-bottom crossing if the top and bottom dual boundary nodes are connected in the DSU structure.\n- For each configuration, record whether exactly one of the two events occurs. Aggregate results over multiple independent trials for each test case.\n\nUse the following test suite with specified parameters $(H,W,p,\\text{trials})$:\n- Test $1$: $(20,20,0.5,500)$ is a self-dual square case ($H=W$ and $p=1/2$).\n- Test $2$: $(20,20,0.3,400)$ is a subcritical-like case.\n- Test $3$: $(20,20,0.7,400)$ is a supercritical-like case.\n- Test $4$: $(10,30,0.5,500)$ is a rectangular self-dual parameter case ($p=1/2$) but not square ($H \\ne W$).\n- Test $5$: $(3,3,0.5,1000)$ is a small square self-dual case.\n\nFor each test case, your program must compute:\n- A boolean indicating exclusivity across all trials, defined as true if and only if for every trial exactly one of the events occurs.\n- The empirical primal left-right crossing probability, expressed as a decimal in $[0,1]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as:\n  $[$exclusivity\\_1, primal\\_prob\\_1, exclusivity\\_2, primal\\_prob\\_2, exclusivity\\_3, primal\\_prob\\_3, exclusivity\\_4, primal\\_prob\\_4, exclusivity\\_5, primal\\_prob\\_5$]$.\n- All probabilities must be decimals (not percentages).\n\nNo external input is allowed. The program must be self-contained and runnable as specified.",
            "solution": "We describe the principle-based design that connects planar duality in bond percolation to algorithmic verification via simulation.\n\nFoundational definitions. In independent bond percolation on a rectangular $H \\times W$ grid, each edge is open with probability $p$ and closed with probability $1-p$, independently of all others. The dual graph has vertices corresponding to the faces of the primal. Each dual edge crosses exactly one primal edge. Declaring dual edges open when primal edges are closed overlays configurations at parameters $p$ and $1-p$.\n\nCore duality property. In planar rectangles, it is a well-tested fact that, for any fixed configuration, exactly one of the events occurs: an open left-right crossing in the primal, or an open top-bottom crossing in the dual. The reason is geometric. Any primal left-right open path, if it existed simultaneously with a dual top-bottom open path, would force an intersection of an open primal edge with an open dual edge that crosses it, which is impossible because a dual edge crossing a primal edge implies the primal edge is closed. Conversely, if there were no primal left-right open path, then the set of closed primal edges contains a dual top-bottom open path that separates the left from the right; in planar graphs, this separation corresponds to the existence of a dual path connecting the top and bottom boundaries through faces adjacent to closed edges. Therefore the two events are disjoint and collectively exhaustive: for every configuration, the exclusive-or (one but not both) holds.\n\nSelf-dual symmetry at $p=1/2$. When $p=1/2$ and the domain is a square ($H=W$), the percolation model is symmetric under $90^\\circ$ rotation combined with swapping open and closed (which preserves the distribution at $p=1/2$). Consequently, the probability of a primal left-right open crossing equals the probability of a dual top-bottom open crossing, and by the exclusivity property, each is $1/2$. In finite simulations, sampling error leads to an estimate that is close to $1/2$.\n\nAlgorithmic design. We implement two detectors:\n- Primal left-right crossing via Breadth-First Search (BFS). Initialize a queue with all left boundary vertices $(r,0)$ for $r \\in \\{0,\\dots,H-1\\}$. While the queue is not empty, pop a vertex and push all neighbors reachable by open primal edges (horizontal or vertical). If a vertex with column index $W-1$ is reached, declare a crossing. This explores the connected component of left boundary sites in the open subgraph, with time complexity $O(HW)$ per trial.\n- Dual top-bottom crossing via Disjoint Set Union (DSU). Construct dual nodes for all faces $(H-1)\\cdot (W-1)$ plus two boundary nodes for top and bottom. For each closed primal edge:\n  - Horizontal edges at row $r=0$ (top boundary) connect the top boundary node to the face below.\n  - Horizontal edges at $1 \\le r \\le H-2$ (interior) connect the face above to the face below.\n  - Horizontal edges at $r=H-1$ (bottom boundary) connect the bottom boundary node to the face above.\n  - Vertical edges at $1 \\le c \\le W-2$ (interior) connect the face to the left to the face to the right.\nWe do not need to represent left or right dual boundary nodes because they are irrelevant for top-bottom connectivity. After unions, if the DSU finds the top and bottom boundary nodes in the same set, we declare a dual top-bottom crossing. This runs in near-linear time in the number of faces and edges.\n\nSimulation protocol. For each test case $(H,W,p,\\text{trials})$:\n- Initialize a pseudorandom number generator with seed $42$ for reproducibility.\n- For each trial, sample open primal edges (horizontal shape $H \\times (W-1)$ and vertical shape $(H-1) \\times W$) by comparing uniform random numbers to $p$.\n- Determine primal left-right crossing via BFS and dual top-bottom crossing via DSU on the complement (closed primal edges).\n- Record a violation if the two events are equal (both true or both false). By duality, violations should be zero.\n- Count primal crossings to estimate the empirical crossing probability.\n\nOutput design. For each test case $i$, output two values:\n- A boolean exclusivity flag indicating whether all trials were exclusive (zero violations).\n- The empirical primal left-right crossing probability in decimal form.\n\nAggregate all test case results in the prescribed single-line format:\n$[$exclusivity\\_1, primal\\_prob\\_1, exclusivity\\_2, primal\\_prob\\_2, exclusivity\\_3, primal\\_prob\\_3, exclusivity\\_4, primal\\_prob\\_4, exclusivity\\_5, primal\\_prob\\_5$]$.\n\nCorrectness and expectations. The exclusivity flags should be true for all tests due to planar duality. For the self-dual square cases ($H=W$ and $p=1/2$), the empirical primal crossing probabilities should be close to $1/2$, with deviations consistent with sampling error. For $p$ significantly below or above $1/2$, the crossing probabilities should reflect subcritical-like or supercritical-like behavior, respectively, on finite grids.\n\nComputational considerations. The BFS and DSU operations per trial are $O(HW)$, and the total number of trials across the test suite is moderate, ensuring fast execution. The fixed seed guarantees reproducible outputs.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import deque\n\nclass DSU:\n    def __init__(self, n: int):\n        self.parent = list(range(n))\n        self.rank = [0] * n\n\n    def find(self, x: int) -> int:\n        # Path compression\n        while self.parent[x] != x:\n            self.parent[x] = self.parent[self.parent[x]]\n            x = self.parent[x]\n        return x\n\n    def union(self, a: int, b: int):\n        ra = self.find(a)\n        rb = self.find(b)\n        if ra == rb:\n            return\n        # Union by rank\n        if self.rank[ra] < self.rank[rb]:\n            self.parent[ra] = rb\n        elif self.rank[ra] > self.rank[rb]:\n            self.parent[rb] = ra\n        else:\n            self.parent[rb] = ra\n            self.rank[ra] += 1\n\ndef sample_edges(H: int, W: int, p: float, rng: np.random.Generator):\n    # Horizontal edges: H rows, W-1 columns\n    h_open = rng.random((H, W - 1)) < p\n    # Vertical edges: H-1 rows, W columns\n    v_open = rng.random((H - 1, W)) < p\n    return h_open, v_open\n\ndef primal_lr_crossing(H: int, W: int, h_open: np.ndarray, v_open: np.ndarray) -> bool:\n    # BFS from all left boundary vertices (r, 0)\n    visited = np.zeros((H, W), dtype=bool)\n    q = deque()\n    for r in range(H):\n        visited[r, 0] = True\n        q.append((r, 0))\n    while q:\n        r, c = q.popleft()\n        if c == W - 1:\n            return True\n        # Left neighbor via horizontal edge\n        if c > 0 and h_open[r, c - 1] and not visited[r, c - 1]:\n            visited[r, c - 1] = True\n            q.append((r, c - 1))\n        # Right neighbor via horizontal edge\n        if c < W - 1 and h_open[r, c] and not visited[r, c + 1]:\n            visited[r, c + 1] = True\n            q.append((r, c + 1))\n        # Up neighbor via vertical edge\n        if r > 0 and v_open[r - 1, c] and not visited[r - 1, c]:\n            visited[r - 1, c] = True\n            q.append((r - 1, c))\n        # Down neighbor via vertical edge\n        if r < H - 1 and v_open[r, c] and not visited[r + 1, c]:\n            visited[r + 1, c] = True\n            q.append((r + 1, c))\n    return False\n\ndef dual_tb_crossing(H: int, W: int, h_open: np.ndarray, v_open: np.ndarray) -> bool:\n    # Dual graph over faces: (H-1) x (W-1) faces, plus top and bottom boundary nodes\n    F_rows = H - 1\n    F_cols = W - 1\n    num_faces = F_rows * F_cols\n    TOP = num_faces\n    BOTTOM = num_faces + 1\n    dsu = DSU(num_faces + 2)\n\n    def face_id(fr: int, fc: int) -> int:\n        return fr * F_cols + fc\n\n    # Process horizontal primal edges (rows r=0..H-1, cols c=0..W-2)\n    # Closed primal edges become open dual edges\n    for r in range(H):\n        for c in range(W - 1):\n            if not h_open[r, c]:\n                if r == 0:\n                    # Top boundary: connect TOP with face below (0, c)\n                    dsu.union(TOP, face_id(0, c))\n                elif r == H - 1:\n                    # Bottom boundary: connect BOTTOM with face above (H-2, c)\n                    dsu.union(BOTTOM, face_id(H - 2, c))\n                else:\n                    # Interior: connect face above (r-1, c) with face below (r, c)\n                    dsu.union(face_id(r - 1, c), face_id(r, c))\n\n    # Process vertical primal edges (rows r=0..H-2, cols c=0..W-1)\n    # Only interior vertical edges contribute to dual face connectivity between left/right faces\n    for r in range(H - 1):\n        for c in range(1, W - 1):\n            if not v_open[r, c]:\n                dsu.union(face_id(r, c - 1), face_id(r, c))\n\n    # Check if TOP and BOTTOM are connected in the dual\n    return dsu.find(TOP) == dsu.find(BOTTOM)\n\ndef run_trials(H: int, W: int, p: float, trials: int, rng: np.random.Generator):\n    primal_count = 0\n    dual_count = 0\n    violations = 0\n    for _ in range(trials):\n        h_open, v_open = sample_edges(H, W, p, rng)\n        primal_lr = primal_lr_crossing(H, W, h_open, v_open)\n        dual_tb = dual_tb_crossing(H, W, h_open, v_open)\n        if primal_lr:\n            primal_count += 1\n        if dual_tb:\n            dual_count += 1\n        # Exclusivity requires exactly one to be true\n        if primal_lr == dual_tb:\n            violations += 1\n    return primal_count, dual_count, violations\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (H, W, p, trials)\n        (20, 20, 0.5, 500),   # Self-dual square\n        (20, 20, 0.3, 400),   # Subcritical-like\n        (20, 20, 0.7, 400),   # Supercritical-like\n        (10, 30, 0.5, 500),   # Rectangular, self-dual parameter\n        (3, 3, 0.5, 1000),    # Small square self-dual\n    ]\n\n    rng = np.random.default_rng(42)\n    results = []\n    for H, W, p, trials in test_cases:\n        primal_count, dual_count, violations = run_trials(H, W, p, trials, rng)\n        exclusivity_ok = (violations == 0)\n        primal_prob = primal_count / trials\n        results.append(exclusivity_ok)\n        results.append(primal_prob)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "At the percolation threshold, the emerging infinite cluster is not a simple one- or two-dimensional object; it is a fractal, exhibiting intricate patterns at all scales of observation. This exercise provides a hands-on method to characterize this complex geometry by estimating the cluster's fractal dimension, $d_f$, using both mass-scaling and box-counting techniques . Completing this practice will solidify your understanding of scale invariance and provide you with practical tools for analyzing the geometry of complex systems.",
            "id": "3171745",
            "problem": "You will implement a computational experiment to estimate the fractal dimension $d_f$ of the incipient infinite cluster in site percolation on a two-dimensional square lattice at the percolation threshold $p_c$. Start from the fundamental base that scale invariance at criticality implies power-law relations between counts measured at different scales, and that the fractal dimension $d_f$ quantifies the scaling of mass with spatial scale for clusters that are statistically self-similar. Use nearest-neighbor connectivity (von Neumann, $4$-connected) on an $L \\times L$ lattice with open boundaries.\n\nDefinitions and constraints:\n- Site percolation on a square lattice occupies each site independently with probability $p$. The percolation threshold for two-dimensional site percolation on the square lattice is $p_c \\approx 0.592746$.\n- A cluster is a set of occupied sites connected through nearest-neighbor bonds (up, down, left, right).\n- A cluster spans if it touches both the top and bottom boundaries or both the left and right boundaries of the lattice.\n- The incipient infinite cluster at finite size is operationally approximated by selecting the spanning cluster if one exists; otherwise, select the largest cluster by mass (the number of sites it contains).\n- Estimate $d_f$ by two independent methods derived from scale invariance:\n  1. Box counting: count how the number of occupied boxes at scale $s$ changes as the scale varies.\n  2. Mass scaling: count how the mass within Euclidean radius $R$ from a reference point changes with $R$.\n\nAlgorithmic requirements:\n1. Generate an $L \\times L$ Boolean lattice where each site is occupied with probability $p$ using a fixed pseudorandom seed to ensure reproducibility. Use open boundaries.\n2. Identify connected clusters using Breadth-First Search (BFS). Record, for each cluster, its mass, whether it touches the top, bottom, left, and right boundaries, and its site coordinates.\n3. Select the target cluster as the spanning cluster with the largest mass if at least one spanning cluster exists; otherwise, select the largest cluster by mass.\n4. Box counting dimension estimate:\n   - For scales $s$ chosen as powers of $2$ within the lattice size (e.g., $s \\in \\{2,4,8,16,\\dots\\}$ and $s \\le \\lfloor L/2 \\rfloor$), partition the lattice into non-overlapping boxes of side $s$.\n   - Count the number of boxes that contain at least one site from the selected cluster. Use only the selected cluster’s sites for counting coverage.\n   - Perform a linear regression of the appropriate logarithmic variables implied by scale invariance to obtain $d_f$ from the slope, using natural logarithms.\n5. Mass scaling dimension estimate:\n   - Choose the reference point as the geometric center of the selected cluster (the arithmetic mean of row and column indices of the cluster’s sites).\n   - For radii $R$ chosen as powers of $2$ up to a fraction of the lattice size (e.g., $R \\in \\{2,4,8,16,\\dots\\}$ and $R \\le \\lfloor L/4 \\rfloor$), count the number of cluster sites whose Euclidean distance from the reference point is less than or equal to $R$.\n   - Perform a linear regression of the appropriate logarithmic variables implied by scale invariance to obtain $d_f$ from the slope, using natural logarithms.\n6. If the number of usable scales for either method is less than $2$, you should return a not-a-number float for that estimate.\n\nNumerical choices:\n- Use natural logarithms.\n- Use Ordinary Least Squares (OLS) linear regression to estimate the slope that yields $d_f$ for each method.\n\nTest suite:\nRun your program on the following test cases, each specified as $(L, p, \\text{seed}, \\text{method})$, where $\\text{method} \\in \\{\\text{\"box\"}, \\text{\"mass\"}\\}$:\n- Case $1$: $(64, 0.592746, 1234, \\text{\"box\"})$.\n- Case $2$: $(64, 0.592746, 1234, \\text{\"mass\"})$.\n- Case $3$: $(128, 0.592746, 42, \\text{\"box\"})$.\n- Case $4$: $(128, 0.592746, 42, \\text{\"mass\"})$.\n- Case $5$: $(32, 0.592746, 7, \\text{\"box\"})$.\n- Case $6$: $(32, 0.58, 7, \\text{\"mass\"})$.\n\nAnswer specification:\n- For each case, output a single float equal to the estimated $d_f$ for the selected method, rounded to $3$ decimal places.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[d_1,d_2,d_3,d_4,d_5,d_6]$), where each $d_i$ is the float for case $i$.",
            "solution": "The problem requires the estimation of the fractal dimension $d_f$ of the incipient infinite cluster in two-dimensional site percolation. This is a classic problem in computational statistical physics. The solution involves generating a percolating system, identifying the critical cluster, and then applying two distinct methods derived from the principle of scale invariance at the critical point: box counting and mass scaling.\n\nThe fundamental principle is that at the percolation threshold, $p=p_c$, the system is statistically self-similar. This means its geometric properties are independent of the length scale at which they are observed. This self-similarity or scale invariance is characteristic of a fractal. The fractal dimension, $d_f$, is an exponent that quantifies how the 'mass' or 'content' of the object scales with its linear size. For a $d$-dimensional Euclidean object, its mass scales with its radius $R$ as $M \\propto R^d$. For a fractal object, this relationship is generalized to $M(R) \\propto R^{d_f}$, where $d_f$ is typically non-integer. For the incipient infinite cluster in $2$D percolation, theory predicts $d_f = 91/48 \\approx 1.8958$.\n\nThe overall computational procedure is as follows:\n1.  Generate a random $L \\times L$ lattice where each site is occupied with probability $p$.\n2.  Use a graph traversal algorithm, Breadth-First Search (BFS), to identify all connected clusters of occupied sites.\n3.  Select a single 'target cluster' that serves as the finite-size approximation of the incipient infinite cluster. This is chosen as the largest spanning cluster if one exists, or the largest cluster by mass otherwise.\n4.  Apply two different methods to estimate $d_f$ for this target cluster.\n\n**Method 1: Mass Scaling Dimension**\n\nThis method directly uses the defining relationship for mass fractals. The mass $M(R)$ of the cluster contained within a Euclidean radius $R$ from a central point is expected to scale as a power law:\n$$M(R) \\propto R^{d_f}$$\nTo extract the exponent $d_f$, we can linearize this relationship by taking the natural logarithm of both sides:\n$$\\ln(M(R)) = d_f \\ln(R) + C$$\nwhere $C$ is a constant. This equation has the form of a line, $y = m x + c$, where the dependent variable is $y = \\ln(M(R))$, the independent variable is $x = \\ln(R)$, and the slope is $m = d_f$.\n\nThe algorithm is therefore:\n1.  Calculate the geometric center of the target cluster by taking the arithmetic mean of the coordinates of all its sites.\n2.  For a sequence of increasing radii $R$ (chosen as powers of $2$ up to $L/4$), count the number of cluster sites $M(R)$ that lie within a Euclidean distance $R$ of the center.\n3.  Perform an Ordinary Least Squares (OLS) linear regression on the set of points $(\\ln(R), \\ln(M(R)))$.\n4.  The slope of the resulting regression line is the estimate for the fractal dimension, $d_f$.\n\n**Method 2: Box Counting Dimension**\n\nThe box counting method provides an alternative way to measure fractal dimension. It quantifies how the number of 'boxes' of a given size $s$ needed to cover the object changes as $s$ changes. For a fractal object, this number $N(s)$ scales as a power law:\n$$N(s) \\propto \\left(\\frac{1}{s}\\right)^{d_f} = s^{-d_f}$$\nAs $s$ decreases, the number of boxes needed to cover the object increases. Again, we linearize this by taking the natural logarithm:\n$$\\ln(N(s)) = -d_f \\ln(s) + C'$$\nThis is also a linear equation, $y = m x + c$, where $y = \\ln(N(s))$, $x = \\ln(s)$, and the slope is $m = -d_f$. Therefore, the fractal dimension can be estimated as $d_f = -m$.\n\nThe algorithm is:\n1.  For a sequence of box sizes $s$ (chosen as powers of $2$ up to $L/2$), partition the $L \\times L$ lattice into a grid of non-overlapping $s \\times s$ boxes.\n2.  Count the number of boxes, $N(s)$, that contain at least one site belonging to the target cluster.\n3.  Perform an OLS linear regression on the set of points $(\\ln(s), \\ln(N(s)))$.\n4.  The estimate for the fractal dimension is the negative of the slope of the regression line, $d_f = -m$.\n\n**Implementation Details**\n\nThe lattice is generated using `numpy.random.default_rng` with a specified seed for reproducibility. Cluster identification is performed by iterating through all sites of the lattice. If an occupied site has not yet been visited, a Breadth-First Search (BFS) is initiated from that site to find all connected sites belonging to that cluster, marking them as visited. During the BFS, the cluster's properties (mass, site coordinates, and whether it touches each of the four boundaries) are collected. After all clusters are found, the target cluster is selected according to the specified rules. The linear regression is performed using the `scipy.stats.linregress` function, which provides a robust implementation of OLS. If a method yields fewer than two data points for regression, a not-a-number value (`nan`) is returned as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\nfrom collections import deque\n\ndef _bfs_cluster_find(grid, start_node, visited):\n    \"\"\"\n    Performs a Breadth-First Search to find a single cluster.\n    \"\"\"\n    L = grid.shape[0]\n    q = deque([start_node])\n    visited[start_node] = True\n    \n    cluster_sites = []\n    mass = 0\n    touches_top = False\n    touches_bottom = False\n    touches_left = False\n    touches_right = False\n    \n    while q:\n        r, c = q.popleft()\n        \n        cluster_sites.append((r, c))\n        mass += 1\n        \n        if r == 0: touches_top = True\n        if r == L - 1: touches_bottom = True\n        if c == 0: touches_left = True\n        if c == L - 1: touches_right = True\n        \n        # Von Neumann neighbors\n        for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            nr, nc = r + dr, c + dc\n            \n            if 0 <= nr < L and 0 <= nc < L and grid[nr, nc] and not visited[nr, nc]:\n                visited[nr, nc] = True\n                q.append((nr, nc))\n                \n    return {\n        \"mass\": mass,\n        \"sites\": cluster_sites,\n        \"touches_top\": touches_top,\n        \"touches_bottom\": touches_bottom,\n        \"touches_left\": touches_left,\n        \"touches_right\": touches_right,\n    }\n\ndef find_clusters(grid):\n    \"\"\"\n    Identifies all connected clusters in the grid using BFS.\n    \"\"\"\n    L = grid.shape[0]\n    visited = np.zeros_like(grid, dtype=bool)\n    clusters = []\n    \n    for r in range(L):\n        for c in range(L):\n            if grid[r, c] and not visited[r, c]:\n                cluster = _bfs_cluster_find(grid, (r, c), visited)\n                clusters.append(cluster)\n    return clusters\n\ndef select_target_cluster(clusters, L):\n    \"\"\"\n    Selects the target cluster based on spanning/mass criteria.\n    \"\"\"\n    if not clusters:\n        return None\n        \n    spanning_clusters = []\n    for c in clusters:\n        is_spanning_v = c['touches_top'] and c['touches_bottom']\n        is_spanning_h = c['touches_left'] and c['touches_right']\n        if is_spanning_v or is_spanning_h:\n            spanning_clusters.append(c)\n            \n    if spanning_clusters:\n        return max(spanning_clusters, key=lambda c: c['mass'])\n    else:\n        return max(clusters, key=lambda c: c['mass'])\n\ndef estimate_df_box(cluster, L):\n    \"\"\"\n    Estimates fractal dimension using the box counting method.\n    \"\"\"\n    sites = cluster['sites']\n    \n    log_s_vals = []\n    log_N_vals = []\n    \n    s = 2\n    while s <= L // 2:\n        if s > 0:\n            occupied_boxes = set()\n            for r, c in sites:\n                occupied_boxes.add((r // s, c // s))\n            \n            N_s = len(occupied_boxes)\n            if N_s > 0:\n                log_s_vals.append(np.log(s))\n                log_N_vals.append(np.log(N_s))\n        s *= 2\n        \n    if len(log_s_vals) < 2:\n        return np.nan\n        \n    slope, _, _, _, _ = stats.linregress(log_s_vals, log_N_vals)\n    return -slope\n\ndef estimate_df_mass(cluster, L):\n    \"\"\"\n    Estimates fractal dimension using the mass scaling method.\n    \"\"\"\n    sites_arr = np.array(cluster['sites'])\n    if sites_arr.shape[0] == 0:\n        return np.nan\n        \n    center = np.mean(sites_arr, axis=0)\n    distances = np.sqrt(np.sum((sites_arr - center)**2, axis=1))\n    \n    log_R_vals = []\n    log_M_vals = []\n    \n    R = 2\n    while R <= L // 4:\n        if R > 0:\n            mass_in_radius = np.sum(distances <= R)\n            if mass_in_radius > 0:\n                log_R_vals.append(np.log(R))\n                log_M_vals.append(np.log(mass_in_radius))\n        R *= 2\n        \n    if len(log_R_vals) < 2:\n        return np.nan\n\n    slope, _, _, _, _ = stats.linregress(log_R_vals, log_M_vals)\n    return slope\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute fractal dimensions.\n    \"\"\"\n    test_cases = [\n        (64, 0.592746, 1234, \"box\"),\n        (64, 0.592746, 1234, \"mass\"),\n        (128, 0.592746, 42, \"box\"),\n        (128, 0.592746, 42, \"mass\"),\n        (32, 0.592746, 7, \"box\"),\n        (32, 0.58, 7, \"mass\"),\n    ]\n\n    results = []\n    lattice_cache = {}\n\n    for L, p, seed, method in test_cases:\n        cache_key = (L, p, seed)\n        \n        if cache_key in lattice_cache:\n            target_cluster = lattice_cache[cache_key]\n        else:\n            # 1. Generate grid\n            rng = np.random.default_rng(seed)\n            grid = rng.random((L, L)) < p\n            \n            # 2. Find clusters\n            clusters = find_clusters(grid)\n            \n            # 3. Select target cluster\n            target_cluster = select_target_cluster(clusters, L)\n            lattice_cache[cache_key] = target_cluster\n        \n        df_estimate = np.nan\n        if target_cluster is not None:\n            if method == \"box\":\n                df_estimate = estimate_df_box(target_cluster, L)\n            elif method == \"mass\":\n                df_estimate = estimate_df_mass(target_cluster, L)\n        \n        results.append(df_estimate)\n\n    formatted_results = []\n    for r in results:\n        if np.isnan(r):\n            formatted_results.append(\"nan\")\n        else:\n            formatted_results.append(f\"{r:.3f}\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "How can we measure properties of an infinite system, like the emergence of a phase transition, using finite computer simulations? This practice introduces finite-size scaling, a cornerstone of computational statistical physics that provides the answer. You will simulate percolation at its critical point on lattices of different sizes to see how the largest cluster size scales, allowing you to extract the universal critical exponent $\\beta$ that governs the strength of the percolating cluster . This exercise is a capstone in learning how to connect finite data to the universal laws of critical phenomena.",
            "id": "3171679",
            "problem": "Consider site percolation on a finite two-dimensional square lattice of linear size $L$ with open boundary conditions. Each site is independently occupied with probability $p$ and empty with probability $1-p$. Define the order parameter $P_{\\infty}(L,p)$ for a finite system as the expected fraction of sites that belong to the largest connected occupied cluster under nearest-neighbor connectivity (four neighbors). According to the finite-size scaling hypothesis in percolation, near the critical occupation probability $p_c$, the order parameter obeys the scaling form\n$$\nP_{\\infty}(L,p) \\sim L^{-\\beta/\\nu} \\, \\tilde{P}\\!\\left((p-p_c) L^{1/\\nu}\\right),\n$$\nwhere $\\beta$ is the order parameter critical exponent, $\\nu$ is the correlation length exponent, and $\\tilde{P}(\\cdot)$ is a scaling function that is regular near the origin. For two-dimensional site percolation, it is a well-tested fact that the correlation length exponent is $\\nu = 4/3$.\n\nYour task is to write a complete, runnable program that estimates $\\beta$ by exploiting the above scaling form at $p \\approx p_c$. At $p = p_c$, the argument of $\\tilde{P}(\\cdot)$ tends to $0$ as $L \\to \\infty$, so that $P_{\\infty}(L,p_c)$ behaves as\n$$\nP_{\\infty}(L,p_c) \\propto L^{-\\beta/\\nu},\n$$\nwhich implies that a linear fit of $\\log P_{\\infty}(L,p_c)$ as a function of $\\log L$ has slope $-\\beta/\\nu$. Therefore, an estimate of $\\beta$ can be obtained from the fitted slope $s$ via $\\beta \\approx -s \\, \\nu$.\n\nImplement the following algorithmic steps in your program:\n- For a given tuple $(L,p)$, generate independent realizations of site percolation and, for each realization, compute the largest-cluster fraction $S_{\\max}/N$, where $S_{\\max}$ is the size (number of occupied sites) of the largest connected occupied cluster and $N = L^2$ is the total number of sites. Use nearest-neighbor connectivity on the square lattice.\n- Estimate $P_{\\infty}(L,p)$ by averaging $S_{\\max}/N$ over a specified number of independent realizations.\n- For a collection of system sizes $\\{L_k\\}$ at a fixed $p$, compute $\\log P_{\\infty}(L_k,p)$ versus $\\log L_k$ and perform a linear regression to obtain the slope $s$.\n- Use $\\nu = 4/3$ to estimate $\\beta$ via $\\beta \\approx -s \\, \\nu$.\n\nScientific realism requirements:\n- Use open boundary conditions and nearest-neighbor connections on a square lattice.\n- Use independent site occupation events with probability $p$.\n- Ensure that all random trials are reproducible by seeding the random number generator.\n\nYour program must implement the above methodology and evaluate the following test suite. For each test case, use the provided list of lattice sizes, occupation probability, number of realizations per lattice size, and random seed. The tests are designed to cover a general case at $p = p_c$, small-system boundaries at $p = p_c$, and near-critical offsets $p = p_c \\pm \\Delta p$.\n\nTest suite:\n- Case $1$ (happy path): $L$ values $\\{\\,32,48,64,96,128\\,\\}$, $p = 0.592746$, trials per $L$ equal to $40$, seed $= 123$, with $\\nu = 4/3$.\n- Case $2$ (slightly above critical): $L$ values $\\{\\,24,32,40,48,64\\,\\}$, $p = 0.602746$, trials per $L$ equal to $35$, seed $= 456$, with $\\nu = 4/3$.\n- Case $3$ (slightly below critical): $L$ values $\\{\\,16,24,32,48\\,\\}$, $p = 0.582746$, trials per $L$ equal to $35$, seed $= 789$, with $\\nu = 4/3$.\n- Case $4$ (small-size boundary): $L$ values $\\{\\,8,12,16,20,24\\,\\}$, $p = 0.592746$, trials per $L$ equal to $40$, seed $= 1011$, with $\\nu = 4/3$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the estimated $\\beta$ for each test case in the same order as listed above, each reported as a float rounded to three decimal places. For example, your program should print an output such as $[0.139,0.142,0.135,0.140]$ (the numerical values are illustrative).\n\nNo physical units are involved in this problem. Angles are not required. All reported numeric answers must be floats in the specified output format.",
            "solution": "The problem requires the estimation of the critical exponent $\\beta$ for two-dimensional site percolation on a square lattice. This is accomplished through a numerical simulation and finite-size scaling analysis. The procedure is based on established principles of statistical physics, specifically the theory of critical phenomena and phase transitions.\n\nThe fundamental premise is the finite-size scaling hypothesis for the order parameter, $P_{\\infty}(L,p)$. For a finite system of linear size $L$, the order parameter is defined as the expected fraction of sites belonging to the largest connected cluster of occupied sites, $P_{\\infty}(L,p) = \\mathbb{E}[S_{\\max} / L^2]$, where $S_{\\max}$ is the size of the largest cluster. Near the critical occupation probability $p_c$, the order parameter is postulated to obey the scaling relation:\n$$\nP_{\\infty}(L,p) \\sim L^{-\\beta/\\nu} \\, \\tilde{P}\\!\\left((p-p_c) L^{1/\\nu}\\right)\n$$\nHere, $\\beta$ is the critical exponent for the order parameter (the probability of belonging to the infinite cluster in an infinite system), and $\\nu$ is the critical exponent for the correlation length. The function $\\tilde{P}(x)$ is a universal scaling function, which is regular for small arguments $x$.\n\nThe proposed methodology exploits this scaling form precisely at the critical point, $p=p_c$. At this point, the argument of the scaling function, $(p-p_c)L^{1/\\nu}$, is zero. Assuming that $\\tilde{P}(0)$ is a non-zero finite constant, the scaling relation simplifies to a power law:\n$$\nP_{\\infty}(L,p_c) \\propto L^{-\\beta/\\nu}\n$$\nThis power-law relationship can be linearized by taking the natural logarithm of both sides:\n$$\n\\log P_{\\infty}(L,p_c) = -\\frac{\\beta}{\\nu} \\log L + \\text{constant}\n$$\nThis equation has the form of a straight line, $y=s x+c$, where $y = \\log P_{\\infty}(L,p_c)$, $x = \\log L$, and the slope is $s = -\\beta/\\nu$. By performing a linear regression on numerically obtained data for $\\log P_{\\infty}(L,p_c)$ versus $\\log L$, we can determine the slope $s$. Given the well-established theoretical value for the correlation length exponent in two dimensions, $\\nu = 4/3$, we can then extract an estimate for $\\beta$ using the formula:\n$$\n\\beta = -s \\nu\n$$\nThe overall algorithm to implement this procedure for each test case is as follows:\n\n1.  **Initialization**: For a given test case specified by a set of lattice sizes $\\{L_k\\}$, an occupation probability $p$, a number of trials, and a random seed, initialize data structures to store the results. The random number generator is seeded to ensure reproducibility.\n\n2.  **Outer Loop (over lattice sizes)**: Iterate through each linear size $L_k$ provided in the test case.\n\n3.  **Inner Loop (Monte Carlo Simulation)**: For each $L_k$, perform a specified number of independent `trials` to estimate $P_{\\infty}(L_k,p)$.\n    a. **Lattice Generation**: An $L_k \\times L_k$ grid is created. Each site is marked as occupied with probability $p$ or empty with probability $1-p$.\n    b. **Cluster Identification**: The largest connected cluster of occupied sites must be found. This standard graph traversal problem is solved using a Breadth-First Search (BFS) algorithm. We maintain an $L_k \\times L_k$ boolean grid, `visited`, initialized to `False`. We iterate through every site $(i,j)$ of the lattice. If site $(i,j)$ is occupied and has not yet been visited, it signifies the start of a new, unexplored cluster. A BFS is initiated from $(i,j)$:\n        i. A queue is initialized with the starting site $(i,j)$, which is then marked as visited.\n        ii. A counter for the current cluster's size is initialized.\n        iii. While the queue is not empty, a site is dequeued. Its four nearest neighbors (up, down, left, right) are examined.\n        iv. For each neighbor, if it is within the lattice boundaries (implementing open boundary conditions), is occupied, and has not been visited, it is marked as visited and enqueued.\n        v. The process continues until the queue is empty, at which point all sites in the current connected component have been visited and counted.\n    c. **Largest Cluster Fraction**: The size of the largest cluster found in the grid, $S_{\\max}$, is recorded for the current trial. The fraction $S_{\\max}/(L_k^2)$ is computed and stored.\n    d. **Averaging**: After completing all `trials` for the size $L_k$, the stored fractions are averaged to produce the estimate for $P_{\\infty}(L_k,p)$.\n\n4.  **Data Preparation for Regression**: After computing $P_{\\infty}(L_k,p)$ for all $L_k$ in the test case, two arrays are created: one containing the logarithms of the lattice sizes, $x_k = \\log L_k$, and the other containing the logarithms of the corresponding order parameter estimates, $y_k = \\log P_{\\infty}(L_k,p)$. Data points where $P_{\\infty}(L_k,p) = 0$ are excluded, as $\\log(0)$ is undefined.\n\n5.  **Linear Regression**: A simple linear regression is performed on the $(x_k, y_k)$ data points to find the slope $s$ of the best-fit line.\n\n6.  **Exponent Calculation**: The estimate for the critical exponent $\\beta$ is calculated using the determined slope $s$ and the given value $\\nu=4/3$, according to $\\beta \\approx -s \\nu$.\n\nThis entire process is repeated for each test case provided in the problem statement, and the resulting estimates for $\\beta$ are collected for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\nimport collections\n\ndef solve():\n    \"\"\"\n    Main function to run the percolation simulations and estimate the critical exponent beta.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"L_values\": [32, 48, 64, 96, 128],\n            \"p\": 0.592746,\n            \"trials\": 40,\n            \"seed\": 123\n        },\n        {\n            \"L_values\": [24, 32, 40, 48, 64],\n            \"p\": 0.602746,\n            \"trials\": 35,\n            \"seed\": 456\n        },\n        {\n            \"L_values\": [16, 24, 32, 48],\n            \"p\": 0.582746,\n            \"trials\": 35,\n            \"seed\": 789\n        },\n        {\n            \"L_values\": [8, 12, 16, 20, 24],\n            \"p\": 0.592746,\n            \"trials\": 40,\n            \"seed\": 1011\n        }\n    ]\n\n    nu = 4.0 / 3.0\n    results = []\n\n    for case in test_cases:\n        beta_estimate = calculate_beta_for_case(case, nu)\n        results.append(round(beta_estimate, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_beta_for_case(case_params, nu):\n    \"\"\"\n    Calculates the beta exponent for a single test case.\n    \"\"\"\n    L_values = case_params[\"L_values\"]\n    p = case_params[\"p\"]\n    trials = case_params[\"trials\"]\n    seed = case_params[\"seed\"]\n\n    rng = np.random.default_rng(seed)\n    \n    log_L_values = []\n    log_P_inf_values = []\n\n    for L in L_values:\n        total_sites = L * L\n        largest_cluster_fractions = []\n\n        for _ in range(trials):\n            grid = rng.random((L, L)) < p\n            \n            # Simple check to skip empty lattices, though unlikely for these p values\n            if not np.any(grid):\n                largest_cluster_fractions.append(0.0)\n                continue\n\n            max_cluster_size = 0\n            visited = np.zeros((L, L), dtype=bool)\n\n            for r in range(L):\n                for c in range(L):\n                    if grid[r, c] and not visited[r, c]:\n                        current_cluster_size = 0\n                        q = collections.deque([(r, c)])\n                        visited[r, c] = True\n                        \n                        while q:\n                            row, col = q.popleft()\n                            current_cluster_size += 1\n                            \n                            # Check 4 nearest neighbors\n                            for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n                                nr, nc = row + dr, col + dc\n                                \n                                # Check boundaries (open boundary conditions)\n                                if 0 <= nr < L and 0 <= nc < L:\n                                    if grid[nr, nc] and not visited[nr, nc]:\n                                        visited[nr, nc] = True\n                                        q.append((nr, nc))\n                        \n                        if current_cluster_size > max_cluster_size:\n                            max_cluster_size = current_cluster_size\n            \n            largest_cluster_fractions.append(max_cluster_size / total_sites)\n\n        P_inf = np.mean(largest_cluster_fractions)\n        \n        # Avoid log(0) error if a run results in no spanning cluster\n        if P_inf > 0:\n            log_L_values.append(np.log(L))\n            log_P_inf_values.append(np.log(P_inf))\n\n    # Perform linear regression to find the slope\n    # y = log(P_inf), x = log(L)\n    if len(log_L_values) < 2:\n        # Not enough data points for a fit.\n        # This should not happen with the given test cases.\n        return np.nan\n\n    slope, _, _, _, _ = stats.linregress(log_L_values, log_P_inf_values)\n    \n    # Calculate beta from the slope\n    beta = -slope * nu\n    return beta\n\nsolve()\n```"
        }
    ]
}