{
    "hands_on_practices": [
        {
            "introduction": "The Generalized Minimal Residual Method (GMRES) is often presented from two different but equivalent perspectives. One view frames it as an orthogonal projection problem, which leads to the standard Arnoldi-based algorithm, while the other, more abstract view, defines it as a search for an optimal polynomial that minimizes the residual. This exercise  guides you to connect these two faces of GMRES by implementing both viewpoints and verifying their numerical equivalence, providing a deep, first-principles understanding of what the method truly accomplishes.",
            "id": "3136956",
            "problem": "You are given the task of connecting the residual-polynomial viewpoint to the orthogonal-projection viewpoint of the Generalized Minimal Residual Method (GMRES). The goal is to show what the residual polynomial is, why its minimization characterizes GMRES, and how to compute it algorithmically from first principles.\n\nFundamental starting point and definitions:\n- For a linear system $A x = b$ with a square real matrix $A \\in \\mathbb{R}^{n \\times n}$, an initial guess $x_0 \\in \\mathbb{R}^n$, and the Euclidean norm $\\|\\cdot\\|_2$, the initial residual is $r_0 = b - A x_0$.\n- The $k$-th Krylov subspace is $\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0\\}$.\n- A residual polynomial of degree at most $k$ with $p_k(0) = 1$ is a polynomial $p_k(t) = 1 + c_1 t + c_2 t^2 + \\cdots + c_k t^k$ satisfying the constraint $p_k(0) = 1$; the residual-polynomial viewpoint seeks coefficients $c_1, \\dots, c_k$ that minimize $\\|p_k(A) r_0\\|_2$.\n- The Generalized Minimal Residual Method (GMRES) is defined as choosing $x_k \\in x_0 + \\mathcal{K}_k(A,r_0)$ to minimize $\\|b - A x_k\\|_2$, with the orthogonal projection interpretation that the GMRES residual $r_k = b - A x_k$ is orthogonal to $A \\mathcal{K}_k(A,r_0)$.\n\nYour program must, for each test case below, perform the following tasks purely in mathematical terms:\n1. Construct a residual polynomial $p_k(t)$ of degree at most $k$ with $p_k(0) = 1$ and compute the minimal attainable value of $\\|p_k(A) r_0\\|_2$, denoted $N_{\\text{poly}}$.\n2. Independently compute $N_{\\text{gmres}}$, the norm of the residual obtained after $k$ steps of the Generalized Minimal Residual Method (GMRES) started from $x_0$.\n3. Verify the orthogonal projection interpretation by checking if the GMRES residual $r_k$ is orthogonal to the set $A \\mathcal{K}_k(A,r_0)$, within numerical tolerance $10^{-9}$ in the sense that for every basis vector $w \\in \\mathcal{K}_k(A,r_0)$, the inner product $r_k^\\top (A w)$ has magnitude at most $10^{-9} \\|r_k\\|_2 \\|A w\\|_2$.\n4. For each test case, output two items: the absolute difference $|N_{\\text{poly}} - N_{\\text{gmres}}|$ (a float) and a boolean reporting whether the orthogonality condition holds (true or false). Aggregate all cases’ results into a single line list by alternating the float and the boolean.\n\nTest suite specification:\n- All computations use real numbers without any physical units.\n- Angles used to define entries must be in radians.\n- The right-hand side vector is defined deterministically as $b \\in \\mathbb{R}^8$ with entries $b_i = \\sin(i)$ for $i = 1, 2, \\dots, 8$ (with $\\sin(\\cdot)$ in radians). The initial guess is $x_0 = 0$ in $\\mathbb{R}^8$.\n- Matrices:\n  1. Case 1 (symmetric with widely spread eigenvalues): $A_1 \\in \\mathbb{R}^{8 \\times 8}$ is diagonal with diagonal entries $\\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 5000.0\\}$. Degree $k = 3$.\n  2. Case 2 (boundary degree): Same $A_1$ as Case 1. Degree $k = 0$.\n  3. Case 3 (non-normal with widely spread eigenvalues): $A_3 \\in \\mathbb{R}^{8 \\times 8}$ is upper-triangular with diagonal entries $\\{0.001, 0.01, 1.0, 10.0, 100.0, 500.0, 1000.0, 5000.0\\}$ and ones on the first superdiagonal; all other off-diagonals are zero. Degree $k = 7$.\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-style list of six elements that alternates the float and boolean for the three test cases, in order: Case 1, Case 2, Case 3. For example, the format must be exactly like\n$[r_1, b_1, r_2, b_2, r_3, b_3]$\nwhere $r_j$ are floats and $b_j$ are booleans.",
            "solution": "The problem requires a verification of the equivalence between two fundamental viewpoints of the Generalized Minimal Residual Method (GMRES) for solving a linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$. The first viewpoint is the minimization of a residual polynomial, and the second is the orthogonal projection method that defines the standard algorithm.\n\nLet the initial guess be $x_0$, and the initial residual be $r_0 = b - A x_0$. The $k$-th iterate of GMRES, $x_k$, is sought in the affine Krylov subspace $x_0 + \\mathcal{K}_k(A,r_0)$, where $\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$.\n\n**1. The Residual-Polynomial Viewpoint**\n\nThe GMRES iterate $x_k$ can be expressed as $x_k = x_0 + z_{k-1}$ for some vector $z_{k-1} \\in \\mathcal{K}_k(A,r_0)$. By definition of the Krylov subspace, $z_{k-1}$ can be written as a polynomial in $A$ of degree at most $k-1$ acting on $r_0$. Let us write $z_{k-1} = q_{k-1}(A)r_0$ for some polynomial $q_{k-1}$. The corresponding residual is $r_k = b - A x_k = b - A(x_0 + q_{k-1}(A)r_0) = (b - A x_0) - A q_{k-1}(A)r_0 = r_0 - A q_{k-1}(A)r_0$.\n\nLet's define a new polynomial $p_k(t) = 1 - t q_{k-1}(t)$. This polynomial $p_k(t)$ has a degree of at most $k$ and satisfies the crucial constraint $p_k(0) = 1$. The residual can now be expressed as $r_k = p_k(A)r_0$. GMRES finds the iterate $x_k$ that minimizes $\\|r_k\\|_2 = \\|p_k(A)r_0\\|_2$ over all possible choices of $z_{k-1} \\in \\mathcal{K}_k(A,r_0)$. This is equivalent to minimizing over all polynomials $p_k$ of degree at most $k$ with $p_k(0)=1$.\n\nTo compute the minimum norm from this viewpoint, denoted $N_{\\text{poly}}$, we set up a linear least-squares problem. A generic such polynomial is $p_k(t) = 1 + c_1 t + c_2 t^2 + \\dots + c_k t^k$. We seek coefficients $c = [c_1, \\dots, c_k]^\\top$ that minimize:\n$$ \\|p_k(A)r_0\\|_2 = \\left\\| \\left(I + \\sum_{j=1}^k c_j A^j\\right) r_0 \\right\\|_2 = \\left\\| r_0 + \\sum_{j=1}^k c_j (A^j r_0) \\right\\|_2 $$\nLet $M$ be a matrix whose columns are the vectors $A^j r_0$ for $j=1, \\dots, k$. The problem is to find $c$ that minimizes $\\|r_0 + Mc\\|_2$, or equivalently, $\\|Mc - (-r_0)\\|_2$. This is a standard linear least-squares problem. The minimal norm $N_{\\text{poly}}$ is the norm of the residual of this problem, i.e., $N_{\\text{poly}} = \\|r_0 + Mc^*\\|_2$, where $c^*$ is the optimal coefficient vector.\n\n**2. The GMRES Algorithm and Orthogonal Projection**\n\nThe standard GMRES algorithm operationalizes the minimization of $\\|r_k\\|_2 = \\|b-Ax_k\\|_2$ for $x_k \\in x_0+\\mathcal{K}_k(A,r_0)$. This is a least-squares problem over a $k$-dimensional subspace. The key is to construct an orthonormal basis for $\\mathcal{K}_{k+1}(A, r_0)$, which is done via the Arnoldi iteration.\n\nThe Arnoldi iteration generates a set of orthonormal vectors $\\{q_1, q_2, \\dots, q_{k+1}\\}$ that span $\\mathcal{K}_{k+1}(A, r_0)$, with $q_1 = r_0 / \\|r_0\\|_2$. It also produces an upper Hessenberg matrix $\\bar{H}_k \\in \\mathbb{R}^{(k+1) \\times k}$ such that $A V_k = V_{k+1} \\bar{H}_k$, where $V_j = [q_1, \\dots, q_j]$.\n\nThe iterate is expressed as $x_k = x_0 + V_k y_k$ for some coordinate vector $y_k \\in \\mathbb{R}^k$. The minimization problem becomes:\n$$ \\min_{y_k \\in \\mathbb{R}^k} \\|r_0 - A V_k y_k\\|_2 = \\min_{y_k \\in \\mathbb{R}^k} \\| \\|r_0\\|_2 q_1 - V_{k+1} \\bar{H}_k y_k \\|_2 $$\nUsing $q_1 = V_{k+1} e_1$ (where $e_1=[1,0,\\dots,0]^\\top \\in \\mathbb{R}^{k+1}$) and the isometry of $V_{k+1}$, the problem simplifies to a small $(k+1) \\times k$ least-squares problem for $y_k$:\n$$ \\min_{y_k \\in \\mathbb{R}^k} \\| \\|r_0\\|_2 e_1 - \\bar{H}_k y_k \\|_2 $$\nThe norm of the GMRES residual, $N_{\\text{gmres}}$, is the minimum value (the norm of the residual vector) of this small problem. Let $y_k^*$ be the solution. Then $N_{\\text{gmres}} = \\| \\|r_0\\|_2 e_1 - \\bar{H}_k y_k^* \\|_2$.\n\n**3. Verification of the Orthogonality Condition**\n\nThe first-order optimality condition for the GMRES minimization problem is that the residual $r_k$ must be orthogonal to the search space translated to the origin, which is $A \\mathcal{K}_k(A, r_0)$. This means $r_k^\\top v = 0$ for all $v \\in A \\mathcal{K}_k(A, r_0)$.\n\nTo verify this numerically, we check the condition against a basis for $A \\mathcal{K}_k(A, r_0)$. A natural basis for this space is $\\{A r_0, A^2 r_0, \\dots, A^k r_0\\}$. For each basis vector $v_j = A^j r_0$ where $j \\in \\{1, \\dots, k\\}$, we must verify that the inner product with the GMRES residual $r_k$ is close to zero, within a specified tolerance $\\epsilon = 10^{-9}$. The condition is:\n$$ |r_k^\\top v_j| \\leq \\epsilon \\|r_k\\|_2 \\|v_j\\|_2 $$\nThe GMRES residual vector $r_k$ is computed as $r_k = V_{k+1} (\\|r_0\\|_2 e_1 - \\bar{H}_k y_k^*)$.\n\nThe implementation below computes $N_{\\text{poly}}$ and $N_{\\text{gmres}}$ independently and then verifies the orthogonality property, demonstrating the theoretical equivalence of the two viewpoints. The absolute difference $|N_{\\text{poly}} - N_{\\text{gmres}}|$ should be close to floating-point machine precision.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GMRES verification for all test cases.\n    \"\"\"\n\n    def run_case(A, b, x0, k):\n        \"\"\"\n        Performs the required computations for a single test case.\n        1. Computes N_poly from the residual-polynomial viewpoint.\n        2. Computes N_gmres and the residual vector r_k via GMRES/Arnoldi.\n        3. Checks the orthogonality of r_k against A*K_k(A,r0).\n        Returns the absolute difference between N_poly and N_gmres, and a\n        boolean indicating if the orthogonality condition holds.\n        \"\"\"\n        n = A.shape[0]\n        r0 = b - A @ x0\n        norm_r0 = np.linalg.norm(r0)\n        \n        # Handle the trivial case k=0\n        if k == 0:\n            N_poly = norm_r0\n            N_gmres = norm_r0\n            # The space A*K_0 is {0}, so any vector is orthogonal to it.\n            is_orthogonal = True\n            return np.abs(N_poly - N_gmres), is_orthogonal\n\n        # --- 1. Residual-Polynomial Viewpoint ---\n        # Minimize ||r0 + c1*A*r0 + ... + ck*A^k*r0|| wrt cj's.\n        # This is the least-squares problem ||M*c - (-r0)||^2.\n        M = np.zeros((n, k))\n        power_vec = r0\n        for j in range(k):\n            power_vec = A @ power_vec\n            M[:, j] = power_vec\n        \n        c = np.linalg.lstsq(M, -r0, rcond=None)[0]\n        res_poly_vec = r0 + M @ c\n        N_poly = np.linalg.norm(res_poly_vec)\n\n        # --- 2. GMRES/Arnoldi Viewpoint ---\n        Q = np.zeros((n, k + 1))\n        H = np.zeros((k + 1, k))\n        \n        if norm_r0 == 0:\n            # x0 is the exact solution.\n            N_gmres = 0.0\n            r_k = np.zeros(n)\n        else:\n            Q[:, 0] = r0 / norm_r0\n    \n            for j in range(k):\n                w = A @ Q[:, j]\n                for i in range(j + 1):\n                    H[i, j] = Q[:, i].T @ w\n                    w = w - H[i, j] * Q[:, i]\n                \n                h_next = np.linalg.norm(w)\n                if h_next  1e-12: # Check for breakdown\n                    # In case of breakdown, the space is smaller.\n                    # This requires truncating H and Q matrices.\n                    # For a general-purpose solver this is crucial; here we assume no breakdown.\n                    # For this problem's setup, this path is not taken.\n                    k_eff = j + 1\n                    H = H[:k_eff+1, :k_eff]\n                    Q = Q[:, :k_eff+1]\n                    break\n                H[j + 1, j] = h_next\n                Q[:, j + 1] = w / h_next\n\n            # Solve the small least-squares problem: min ||norm(r0)*e1 - H*y||\n            e1 = np.zeros(k + 1)\n            e1[0] = 1.0\n            rhs = norm_r0 * e1\n            \n            y = np.linalg.lstsq(H, rhs, rcond=None)[0]\n            \n            # Compute N_gmres and the full residual vector r_k\n            small_res_vec = rhs - H @ y\n            N_gmres = np.linalg.norm(small_res_vec)\n            r_k = Q @ small_res_vec\n\n        # --- 3. Orthogonality Check ---\n        # Verify r_k is orthogonal to the basis {A*r0, A^2*r0, ..., A^k*r0}\n        # which spans A*K_k(A,r0).\n        is_orthogonal = True\n        norm_rk = np.linalg.norm(r_k)\n        check_vec = r0\n        \n        for j in range(1, k + 1):\n            check_vec = A @ check_vec   # This is A^j * r0\n            norm_check_vec = np.linalg.norm(check_vec)\n            \n            ip = r_k.T @ check_vec\n            \n            # Use relative tolerance check\n            if norm_rk  0 and norm_check_vec  0:\n                condition = (np.abs(ip) = 1e-9 * norm_rk * norm_check_vec)\n            else:\n                # If either vector is zero, their dot product must be (near) zero\n                condition = (np.abs(ip)  1e-9)\n\n            if not condition:\n                is_orthogonal = False\n                break\n\n        # --- 4. Return results for the case ---\n        diff = np.abs(N_poly - N_gmres)\n        return diff, is_orthogonal\n\n    # Define common parameters\n    n = 8\n    b = np.sin(np.arange(1, n + 1))\n    x0 = np.zeros(n)\n    \n    # Define test cases\n    A1_diag = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 5000.0]\n    A1 = np.diag(A1_diag)\n    \n    A3_diag = [0.001, 0.01, 1.0, 10.0, 100.0, 500.0, 1000.0, 5000.0]\n    A3 = np.diag(A3_diag) + np.diag(np.ones(n - 1), 1)\n\n    test_cases = [\n        (A1, b, x0, 3),  # Case 1\n        (A1, b, x0, 0),  # Case 2\n        (A3, b, x0, 7),  # Case 3\n    ]\n\n    # Run all test cases and collect results\n    results = []\n    for case in test_cases:\n        diff, ortho = run_case(*case)\n        results.append(diff)\n        results.append(ortho)\n\n    # Format and print the final output\n    # Using a custom formatter for booleans to satisfy potential ambiguity,\n    # though standard str(bool) -> 'True'/'False' would also be Python-style.\n    # The problem example `true`/`false` is non-standard in Python output. \n    # Sticking to standard Python representation 'True'/'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A remarkable theoretical property of GMRES is its guarantee of finding the exact solution for any solvable linear system in at most $n$ iterations, where $n$ is the size of the matrix. In many cases, it terminates much sooner. This practice exercise  delves into this finite termination property by asking you to demonstrate computationally that the exact number of steps needed is equal to the degree of the minimal polynomial of the matrix with respect to the initial residual vector.",
            "id": "3137005",
            "problem": "You are to design and implement a program that demonstrates the exact termination behavior of the Generalized Minimal Residual method (GMRES) for solving a linear system $A x = b$. The theoretical base that you must use is limited to core linear algebra definitions and facts: the concept of a Krylov subspace, the Arnoldi process, and the definition of the minimal polynomial of a matrix relative to a vector. The fundamental definitions you may assume are: for a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and an initial residual $r_0 = b - A x_0$, the Krylov subspace of order $k$ is $ \\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{ r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0 \\}$; and the minimal polynomial of $A$ relative to $r_0$ is the nonzero polynomial $p$ of least degree such that $p(A) r_0 = 0$, whose degree equals the dimension at which the Krylov subspace stops increasing. You must start from these definitions and no others, and you must not rely on any shortcut formulas that skip the derivation of GMRES from these bases.\n\nTask. Implement a non-restarted GMRES solver from first principles using the Arnoldi process and a least-squares solve restricted to the Krylov subspace, starting from the zero initial guess $x_0 = 0$. Use a relative residual stopping rule: stop at the first iteration $k$ such that $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$, where $\\varepsilon$ is the given tolerance. Also implement a routine that computes the degree $g$ of the minimal polynomial of $A$ relative to $r_0$ by detecting linear dependence in the sequence $\\{r_0, A r_0, A^2 r_0, \\dots\\}$ via orthonormalization.\n\nScientific requirement. In exact arithmetic, GMRES terminates in at most the degree $g$ of the minimal polynomial relative to $r_0$. Your program must numerically demonstrate that the first iteration $k^\\star$ at which GMRES achieves the stopping rule equals that degree $g$ for a collection of carefully chosen test matrices $A$ and corresponding right-hand sides $b$.\n\nAngle units are not applicable. No physical units are involved. All computations are purely mathematical over $\\mathbb{R}$.\n\nTolerance. Use the numerical tolerance $\\varepsilon = 10^{-12}$.\n\nTest suite. Your program must apply both routines (GMRES termination counter and minimal-polynomial degree detector) to the following four cases, all with $x_0 = 0$:\n\n- Case $1$ (degree $1$ boundary case): \n  - $A_1 = 5 I_3$,\n  - $b_1 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}$.\n- Case $2$ (diagonalizable with two distinct eigenvalues): \n  - $A_2 = \\operatorname{diag}(1, 2, 2)$,\n  - $b_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n- Case $3$ (defective Jordan block, full chain): \n  - $A_3 = \\begin{bmatrix} 2  1  0 \\\\ 0  2  1 \\\\ 0  0  2 \\end{bmatrix}$,\n  - $b_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n- Case $4$ (block diagonal with selective excitation; global minimal polynomial larger than relative one):\n  - $A_4 = \\operatorname{diag}\\!\\Big( \\begin{bmatrix} 3  1 \\\\ 0  3 \\end{bmatrix}, \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\Big)$,\n  - $b_4 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n\nFor each case, let $k^\\star$ be the first GMRES iteration satisfying the stopping rule, and let $g$ be the detected degree of the minimal polynomial of $A$ relative to $r_0 = b - A x_0 = b$. For scientific realism and numerical robustness, determine $g$ by building an orthonormal basis of $\\{ r_0, A r_0, A^2 r_0, \\dots \\}$ via modified Gram–Schmidt and stopping when the next vector is detected linearly dependent with tolerance proportional to $\\|r_0\\|_2$.\n\nRequired output. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output the boolean value of the statement “$k^\\star = g$”. Thus the final output should be a single line of the form, for example, “[True,True,True,True]”. No other text should be printed.\n\nNotes.\n- All numbers used above, such as $5$, $1$, $2$, $3$, and $10^{-12}$, are exact specifications of the test instances and tolerance.\n- You must not read input; you must hardcode the test suite exactly as stated and print precisely one line as specified.\n- Your implementation must adhere to standard double-precision arithmetic and the definitions provided above.",
            "solution": "The problem requires a numerical demonstration of a fundamental property of the Generalized Minimal Residual method (GMRES) for solving linear systems of the form $Ax=b$. Specifically, it posits that in exact arithmetic, the method terminates with the exact solution at an iteration count equal to the degree of the minimal polynomial of the matrix $A$ with respect to the initial residual vector $r_0$. Our task is to implement both a GMRES solver and a function to compute this degree, and then verify their equivalence across a suite of test cases. We begin from first principles as dictated.\n\nThe initial guess is specified as $x_0=0$, which implies the initial residual is $r_0 = b - Ax_0 = b$.\n\n**1. Degree of the Minimal Polynomial**\n\nThe minimal polynomial of a matrix $A \\in \\mathbb{R}^{n \\times n}$ with respect to a vector $r_0 \\in \\mathbb{R}^n$ is the monic polynomial $p(z)$ of least degree $g$ such that $p(A)r_0 = 0$. This degree $g$ is also the dimension of the Krylov subspace $\\mathcal{K}(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, A^2 r_0, \\dots\\}$. The dimension ceases to increase when a vector in the sequence $\\{A^k r_0\\}_{k \\ge 0}$ becomes linearly dependent on its predecessors. The first index $k$ for which this occurs defines the degree, $g=k$.\n\nWe can determine $g$ computationally by constructing an orthonormal basis for the growing Krylov subspace, using the modified Gram-Schmidt (MGS) algorithm for its superior numerical stability. Let the sequence of Krylov vectors be $u_k = A^k r_0$ for $k=0, 1, 2, \\dots, n$. We generate an orthonormal basis $\\{v_0, v_1, \\dots, v_{g-1}\\}$ for $\\mathcal{K}_g(A, r_0)$.\n\nThe algorithm proceeds as follows:\n1. Initialize an empty list of orthonormal basis vectors, $V$.\n2. For $k=0, 1, 2, \\dots, n$:\n   a. Take the next Krylov vector, $u_k = A^k r_0$. (This is computed iteratively: $u_0=r_0$, $u_{k+1}=Au_k$).\n   b. Orthogonalize $u_k$ against the current basis vectors $\\{v_0, \\dots, v_{k-1}\\}$ in $V$:\n      $$ w = u_k - \\sum_{j=0}^{k-1} (v_j^T u_k) v_j $$\n   c. The norm $\\|w\\|_2$ represents the component of $u_k$ that is orthogonal to the subspace $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{v_0, \\dots, v_{k-1}\\}$. If this norm is numerically zero (i.e., $\\|w\\|_2 \\le \\tau \\|r_0\\|_2$ for some small tolerance $\\tau$), it signifies that $u_k$ is linearly dependent on the preceding vectors $\\{u_0, \\dots, u_{k-1}\\}$. The dimension of the Krylov subspace is $k$, so the degree of the minimal polynomial is $g=k$. The process terminates, returning $k$.\n   d. If $\\|w\\|_2$ is not negligible, we normalize it to obtain the next basis vector $v_k = w / \\|w\\|_2$ and add it to our orthonormal set $V$.\n\nThis procedure directly determines the dimension of the largest reachable Krylov subspace, which is by definition the degree $g$.\n\n**2. GMRES Algorithm from First Principles**\n\nThe GMRES method finds an approximate solution $x_k$ at iteration $k$ from the affine subspace $x_0 + \\mathcal{K}_k(A, r_0)$. The solution $x_k$ is chosen to minimize the $2$-norm of the residual, $\\|r_k\\|_2 = \\|b - Ax_k\\|_2$. With the initial guess $x_0=0$, the approximation $x_k$ lies in the Krylov subspace $\\mathcal{K}_k(A, r_0)$.\n\nThe core of GMRES is the Arnoldi process, which builds an orthonormal basis $V_{k+1} = [v_1, \\dots, v_{k+1}]$ for the Krylov subspace $\\mathcal{K}_{k+1}(A, r_0)$ and simultaneously produces an upper Hessenberg matrix $H_{k+1,k} \\in \\mathbb{R}^{(k+1) \\times k}$. These matrices are linked by the Arnoldi relation:\n$$ A V_k = V_{k+1} H_{k+1,k} $$\nwhere $V_k = [v_1, \\dots, v_k]$. The process is initialized with the first basis vector $v_1 = r_0 / \\|r_0\\|_2$.\n\nAn iterate $x_k$ can be expressed as a linear combination of the basis vectors, $x_k = V_k y$, for some coordinate vector $y \\in \\mathbb{R}^k$. The corresponding residual is then:\n$$ r_k = b - Ax_k = r_0 - A V_k y $$\nSubstituting $r_0 = \\|r_0\\|_2 v_1$ and the Arnoldi relation, we get:\n$$ r_k = \\|r_0\\|_2 v_1 - V_{k+1} H_{k+1,k} y = V_{k+1} (\\|r_0\\|_2 e_1 - H_{k+1,k} y) $$\nwhere $e_1$ is the first standard basis vector in $\\mathbb{R}^{k+1}$. Since the columns of $V_{k+1}$ are orthonormal, minimizing $\\|r_k\\|_2$ is equivalent to minimizing $\\|\\,\\|r_0\\|_2 e_1 - H_{k+1,k} y\\,\\|_2$.\n\nThis is a small-scale linear least-squares problem for the coordinate vector $y$. It can be solved at each iteration $k$. The overall GMRES algorithm is:\n1. Initialize: $r_0 = b$, $\\beta = \\|r_0\\|_2$, $v_1 = r_0 / \\beta$.\n2. For $k=1, 2, \\dots, n$:\n   a. **Arnoldi Step**: Generate $v_{k+1}$ and the $(k-1)$-th column of the Hessenberg matrix using the Arnoldi process. This involves computing $w = Av_k$ and orthogonalizing it against $\\{v_1, \\dots, v_k\\}$. Let the resulting coefficients be $h_{i,k}$ for $i=1, \\dots, k$ and the norm of the orthogonalized vector be $h_{k+1,k}$.\n   b. **Least-Squares Solve**: Form the matrix $H_{k+1,k}$ and the vector $g = \\beta e_1 \\in \\mathbb{R}^{k+1}$. Find $y_k \\in \\mathbb{R}^k$ that minimizes $\\|H_{k+1,k} y - g\\|_2$.\n   c. **Check Convergence**: The norm of the residual for the $k$-th iterate is the minimum value from the least-squares problem: $\\|r_k\\|_2 = \\min_y \\|H_{k+1,k} y - g\\|_2$. We check if the relative residual satisfies $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$. If so, the termination iteration is $k^\\star = k$, and we stop.\n\n**3. Verification of $k^\\star = g$**\n\nThe theoretical foundation connecting the two algorithms is that if the degree of the minimal polynomial is $g$, then the Krylov subspace $\\mathcal{K}_g(A, r_0)$ contains the exact solution $x = A^{-1}b$. Furthermore, for any vector $u \\in \\mathcal{K}_g(A, r_0)$, the vector $Au$ also lies in $\\mathcal{K}_g(A, r_0)$.\nWhen the Arnoldi process reaches iteration $k=g$, the vector $A v_g$ is already in the space spanned by $\\{v_1, \\dots, v_g\\}$. Consequently, after orthogonalization, the remaining vector is zero, leading to a zero (or numerically tiny) value for the Hessenberg entry $h_{g+1,g}$. This event is known as a \"lucky breakdown\". The implication for the least-squares problem is that a vector $y_g$ exists such that $H_{g+1,g} y_g = \\beta e_1$, which makes the residual norm $\\|r_g\\|_2$ drop to zero. The GMRES approximation $x_g$ becomes the exact solution.\nTherefore, the first iteration $k^\\star$ at which GMRES satisfies the stopping criterion with a sufficiently small tolerance $\\varepsilon=10^{-12}$ will be precisely the degree $g$ of the minimal polynomial relative to $r_0$. Our program will compute both $g$ and $k^\\star$ for each test case and verify this equality.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_min_poly_degree(A, b):\n    \"\"\"\n    Computes the degree of the minimal polynomial of A with respect to b.\n    This is done by finding the dimension of the Krylov subspace K(A, b)\n    by detecting linear dependence in the sequence {b, Ab, A^2b, ...}\n    using modified Gram-Schmidt.\n    \"\"\"\n    n = A.shape[0]\n    r0_norm = np.linalg.norm(b)\n    \n    if r0_norm == 0:\n        return 0\n\n    # Tolerance for detecting linear dependence, proportional to the initial norm.\n    dep_tol = 1e-12 * r0_norm\n\n    orthonormal_basis = []\n    current_krylov_vec = b.copy().astype(np.float64)\n\n    for k in range(n + 1):\n        # Orthogonalize the current Krylov vector against the basis so far.\n        orth_vec = current_krylov_vec.copy()\n        for v in orthonormal_basis:\n            proj = np.dot(v.conj(), orth_vec)\n            orth_vec -= proj * v\n            \n        norm_orth_vec = np.linalg.norm(orth_vec)\n        \n        # If the norm of the orthogonalized vector is close to zero,\n        # it means the current Krylov vector is linearly dependent on the previous ones.\n        if norm_orth_vec  dep_tol:\n            return k # The degree of the minimal polynomial is k.\n            \n        # Add the new orthonormal vector to the basis.\n        orthonormal_basis.append(orth_vec / norm_orth_vec)\n        \n        # Generate the next Krylov vector for the next iteration.\n        current_krylov_vec = A @ current_krylov_vec\n            \n    return n\n\ndef gmres_solver(A, b, tol):\n    \"\"\"\n    Implements a non-restarted GMRES solver from first principles.\n    Returns the number of iterations k* at which the relative residual\n    ||r_k||/||b|| falls below the tolerance.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Initial guess x0 = 0, so initial residual r0 = b.\n    r0 = b.copy().astype(np.float64)\n    b_norm = np.linalg.norm(b)\n    \n    if b_norm == 0:\n        return 0 # Trivial case: solution is x=0, 0 iterations.\n\n    r0_norm = b_norm\n    \n    # V stores the orthonormal basis vectors for the Krylov subspace.\n    V = [r0 / r0_norm]\n    # H will be the (k+1) x k upper Hessenberg matrix.\n    H = np.zeros((n + 1, n), dtype=np.float64)\n    \n    # e1 is used to form the right-hand side of the least-squares problem.\n    e1 = np.zeros(n + 1, dtype=np.float64)\n    e1[0] = 1.0\n\n    # Main GMRES loop.\n    for k in range(n): # k from 0 to n-1, corresponding to iterations 1 to n.\n        # Arnoldi process to generate v_{k+1} and the k-th column of H.\n        # Note on indices: V[k] is the (k+1)-th basis vector, v_{k+1} in 1-based indexing.\n        w = A @ V[k]\n        \n        for j in range(k + 1):\n            H[j, k] = np.dot(w, V[j])\n            w = w - H[j, k] * V[j]\n        \n        H[k + 1, k] = np.linalg.norm(w)\n        \n        # Form and solve the least-squares problem for iteration k+1.\n        # The Hessenberg matrix for this iteration is of size (k+2) x (k+1).\n        H_sub = H[:k+2, :k+1]\n        \n        # The right-hand side is beta * e1, where beta = ||r0||.\n        target = r0_norm * e1[:k+2]\n        \n        # Solve the (k+2)x(k+1) least-squares problem.\n        y, residuals, _, _ = np.linalg.lstsq(H_sub, target, rcond=None)\n        \n        # The residual norm ||r_k|| is the residual of the LS problem.\n        # np.linalg.lstsq returns the sum of squared residuals, so take the square root.\n        res_norm = np.sqrt(residuals[0]) if residuals.size  0 else np.linalg.norm(H_sub @ y - target)\n\n        # Check stopping criterion based on relative residual.\n        if res_norm / b_norm = tol:\n            return k + 1 # GMRES has converged in k+1 iterations.\n        \n        # Check for Arnoldi breakdown. If h_{k+1,k} is numerically zero, the subspace\n        # is invariant. GMRES finds the exact solution. The residual check above\n        # inherently captures this, but this is a fail-safe.\n        if H[k + 1, k]  1e-16:\n            return k + 1\n            \n        V.append(w / H[k + 1, k])\n        \n    return n # Maximum number of iterations reached.\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the algorithms, and print the results.\n    \"\"\"\n    TOL = 1e-12\n\n    test_cases = [\n        # Case 1: Identity matrix scaled, degree g=1\n        (\n            5.0 * np.identity(3, dtype=np.float64),\n            np.array([1.0, -2.0, 3.0], dtype=np.float64)\n        ),\n        # Case 2: Diagonalizable with repeated eigenvalue, g=2\n        (\n            np.diag(np.array([1.0, 2.0, 2.0], dtype=np.float64)),\n            np.array([1.0, 1.0, 1.0], dtype=np.float64)\n        ),\n        # Case 3: Defective Jordan block, g=3\n        (\n            np.array([[2.0, 1.0, 0.0], [0.0, 2.0, 1.0], [0.0, 0.0, 2.0]], dtype=np.float64),\n            np.array([1.0, 1.0, 1.0], dtype=np.float64)\n        ),\n        # Case 4: Block diagonal with selective excitation, g=2\n        (\n            np.array([\n                [3.0, 1.0, 0.0, 0.0], \n                [0.0, 3.0, 0.0, 0.0], \n                [0.0, 0.0, 1.0, 1.0], \n                [0.0, 0.0, 0.0, 1.0]\n            ], dtype=np.float64),\n            np.array([1.0, 1.0, 0.0, 0.0], dtype=np.float64)\n        )\n    ]\n\n    results = []\n    for A, b in test_cases:\n        g = find_min_poly_degree(A, b)\n        k_star = gmres_solver(A, b, TOL)\n        results.append(g == k_star)\n\n    # Convert list of booleans to the required string format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from theory to practice, an efficient implementation of GMRES avoids forming and solving the internal least-squares problem from scratch at each step. Instead, it uses techniques like Givens rotations to update the solution incrementally. This hands-on problem  will have you implement this efficient update scheme, focusing on how it allows for a cheap and accurate way to monitor the residual norm and detect the exact moment of convergence.",
            "id": "3136916",
            "problem": "You are to implement the Generalized Minimal Residual method (GMRES) with Givens rotations to solve linear systems of the form $A x = b$. Your program must explicitly construct a small matrix $A$ and carefully chosen right-hand sides $b$ (and initial guesses $x_0$) so that the Krylov subspace $\\mathcal{K}_k(A, r_0)$ contains the exact solution for some small $k$, and demonstrate how the least-squares problem over the upper Hessenberg matrix detects that the $k$-th residual is zero via Givens rotations. You must derive your algorithm from the following base definitions and facts.\n\nFundamental definitions and facts:\n- Let $A \\in \\mathbb{R}^{n \\times n}$ be nonsingular, $b \\in \\mathbb{R}^n$, and an initial guess $x_0 \\in \\mathbb{R}^n$. The initial residual is $r_0 = b - A x_0$.\n- The Krylov subspace of order $k$ is $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0\\}$.\n- GMRES seeks $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$ that minimizes the Euclidean norm $\\|b - A x_k\\|_2$.\n- The Arnoldi process with normalized vectors produces an orthonormal basis $V_{k+1} = [v_1, \\dots, v_{k+1}]$ for $\\mathcal{K}_{k+1}(A, r_0)$ and an upper Hessenberg matrix $H_{k+1,k} \\in \\mathbb{R}^{(k+1)\\times k}$ such that $A V_k = V_{k+1} H_{k+1,k}$ and $v_1 = r_0 / \\|r_0\\|_2$.\n- The GMRES residual minimization at step $k$ reduces to a least-squares problem in the small dimension $k$: find $y_k \\in \\mathbb{R}^k$ minimizing $\\|\\beta e_1 - H_{k+1,k} y_k\\|_2$, where $\\beta = \\|r_0\\|_2$ and $e_1$ is the first coordinate vector in $\\mathbb{R}^{k+1}$. The solution update is $x_k = x_0 + V_k y_k$.\n- Applying Givens rotations incrementally to $H_{k+1,k}$ during Arnoldi produces a triangular factorization and updates the right-hand side vector. The updated right-hand side entries reveal the current GMRES residual norm: specifically, after $k$ steps, the residual norm equals the absolute value of the $(k+1)$-st entry of the rotated right-hand side. When that entry becomes zero (up to a specified tolerance), GMRES has found an exact solution in $k$ steps.\n\nConstruct $A$ and test cases:\nYou must use a fixed diagonal matrix $A \\in \\mathbb{R}^{4 \\times 4}$ and four test cases that together demonstrate the happy-path case, a minimal $k$ boundary case, a non-convergence within a prescribed iteration limit, and the zero-initial-residual boundary. Diagonal matrices with distinct diagonal entries have invariant subspaces aligned with coordinate axes, allowing explicit construction of $b$ and $x_0$ so that $\\mathcal{K}_k(A,r_0)$ contains the exact solution. Diagonal $A$ also ensures numerical stability of this small demonstration.\n\nLet\n$$\nA = \\operatorname{diag}(4, 7, 9, 10) \\in \\mathbb{R}^{4\\times 4}.\n$$\n\nFor each test case $i \\in \\{1,2,3,4\\}$, you are given $b^{(i)}$, a maximum Arnoldi step count $m^{(i)}$, and an initial guess $x_0^{(i)}$. For all cases, define $r_0^{(i)} = b^{(i)} - A x_0^{(i)}$ and run GMRES with at most $m^{(i)}$ steps, applying Givens rotations to the evolving upper Hessenberg system to update the small least-squares problem. At each step $k$, use the updated right-hand side to compute the current residual norm. Declare that an exact solution has been detected at iteration $k$ if and only if the updated residual norm is less than or equal to a tolerance $\\tau = 10^{-12}$. If the initial residual is already zero, declare detection at iteration $0$ without performing Arnoldi.\n\nTest suite:\n- Case 1 (happy-path, exact solution in two steps): $b^{(1)} = [1, 2, 0, 0]^\\top$, $x_0^{(1)} = [0, 0, 0, 0]^\\top$, $m^{(1)} = 4$. Here, $b^{(1)}$ has components only on two eigendirections of $A$, so $x_\\ast^{(1)} = A^{-1} b^{(1)} \\in \\mathcal{K}_2(A, r_0^{(1)})$, hence GMRES must detect an exact solution in $k \\leq 2$.\n- Case 2 (boundary $k=1$): $b^{(2)} = [0, 0, 5, 0]^\\top$, $x_0^{(2)} = [0,0,0,0]^\\top$, $m^{(2)} = 4$. Here, $b^{(2)}$ is an eigenvector of $A$, so the exact solution lies in $\\mathcal{K}_1(A, r_0^{(2)})$, and GMRES must detect an exact solution in $k \\leq 1$.\n- Case 3 (non-convergence within limit): $b^{(3)} = [1, 1, 1, 0]^\\top$, $x_0^{(3)} = [0,0,0,0]^\\top$, $m^{(3)} = 2$. Here, three distinct eigendirections are present in $b^{(3)}$, so the invariant subspace spanned by the excited directions has dimension three. Restricting to $m^{(3)} = 2$ steps prevents exact convergence; GMRES must not detect an exact solution within two steps.\n- Case 4 (zero initial residual, boundary $k=0$): $b^{(4)} = [1, 2, 0, 0]^\\top$, $x_0^{(4)} = A^{-1} b^{(4)} = [1/4, 2/7, 0, 0]^\\top$, $m^{(4)} = 4$. Here, $r_0^{(4)} = 0$ by construction, so GMRES must declare detection at iteration $0$.\n\nProgram requirements:\n- Implement the Arnoldi process to construct the orthonormal basis and the upper Hessenberg matrix incrementally.\n- Implement incremental Givens rotations to maintain an upper-triangular form of the evolving small least-squares problem and update the right-hand side vector. Use these updates to compute the residual norms at each iteration and decide when $r_k = 0$ (numerically, when the residual norm is less than or equal to $\\tau = 10^{-12}$).\n- For each test case $i$, output a single integer:\n  - If $r_0^{(i)} = 0$, output $0$.\n  - Else if GMRES detects an exact solution at step $k \\leq m^{(i)}$, output $k$.\n  - Else output $-1$ to indicate that exact convergence was not detected within the iteration limit.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the four test cases, for example, \"[result1,result2,result3,result4]\". All numbers must be integers. There are no physical units or angles in this problem, so no unit conversions are required.",
            "solution": "We begin from the definitions of Krylov subspaces and the Generalized Minimal Residual method (GMRES) and derive a computational procedure that uses the Arnoldi process and Givens rotations to identify the iteration $k$ at which the residual becomes exactly zero in exact arithmetic and effectively zero up to a tolerance in floating-point arithmetic.\n\n1. From the initial guess $x_0 \\in \\mathbb{R}^n$, the initial residual is $r_0 = b - A x_0$. If $\\|r_0\\|_2 = 0$, then $x_0$ is already a solution and GMRES terminates at iteration $0$.\n\n2. For $k \\ge 1$, GMRES searches within the affine space $x_0 + \\mathcal{K}_k(A, r_0)$ for the iterate $x_k$ that minimizes the Euclidean norm of the residual. The Krylov subspace is defined as\n$$\n\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0\\}.\n$$\n\n3. The Arnoldi process constructs an orthonormal basis $V_{k+1} = [v_1, \\dots, v_{k+1}]$ with $v_1 = r_0 / \\|r_0\\|_2$ and an upper Hessenberg matrix $H_{k+1,k}$ such that\n$$\nA V_k = V_{k+1} H_{k+1,k},\n$$\nwhere $V_k = [v_1, \\dots, v_k]$ is the first $k$ columns of $V_{k+1}$. This is achieved by iteratively computing $w = A v_j$, orthogonalizing $w$ against $\\{v_i\\}_{i=1}^j$ to obtain $h_{i,j} = v_i^\\top w$ and updating $w \\leftarrow w - \\sum_{i=1}^j h_{i,j} v_i$, and then defining $h_{j+1,j} = \\|w\\|_2$ and $v_{j+1} = w / h_{j+1,j}$ when $h_{j+1,j} \\ne 0$.\n\n4. The GMRES iterate has the form $x_k = x_0 + V_k y_k$ for some $y_k \\in \\mathbb{R}^k$. The residual $r_k = b - A x_k$ can be expressed using the Arnoldi relation:\n$$\nr_k = b - A(x_0 + V_k y_k) = r_0 - A V_k y_k = \\|r_0\\|_2 v_1 - V_{k+1} H_{k+1,k} y_k = V_{k+1}(\\beta e_1 - H_{k+1,k} y_k),\n$$\nwhere $\\beta = \\|r_0\\|_2$ and $e_1 \\in \\mathbb{R}^{k+1}$ is the first coordinate vector. Because $V_{k+1}$ has orthonormal columns, minimizing $\\|r_k\\|_2$ over $y_k$ is equivalent to solving the least-squares problem\n$$\ny_k = \\operatorname*{argmin}_{y \\in \\mathbb{R}^k} \\|\\beta e_1 - H_{k+1,k} y\\|_2.\n$$\n\n5. To solve the evolving least-squares problem efficiently and to monitor the residual norm at each iteration without re-solving from scratch, we apply Givens rotations to maintain an upper-triangular form. At step $j$ of Arnoldi, we have introduced a new subdiagonal element $h_{j+1,j}$. We apply the previously computed Givens rotations to the column $h_{:,j}$ to preserve triangularity, and then compute a new Givens rotation $G_j$ that zeroes the subdiagonal entry $h_{j+1,j}$:\nLet $h_{j,j}$ and $h_{j+1,j}$ be the relevant elements after applying all prior rotations. Define\n$$\nr = \\sqrt{h_{j,j}^2 + h_{j+1,j}^2}, \\quad c_j = \\frac{h_{j,j}}{r}, \\quad s_j = \\frac{h_{j+1,j}}{r}.\n$$\nSet\n$$\n\\begin{bmatrix} h_{j,j}^\\prime \\\\ h_{j+1,j}^\\prime \\end{bmatrix}\n=\n\\begin{bmatrix} c_j  s_j \\\\ -s_j  c_j \\end{bmatrix}\n\\begin{bmatrix} h_{j,j} \\\\ h_{j+1,j} \\end{bmatrix}\n=\n\\begin{bmatrix} r \\\\ 0 \\end{bmatrix}.\n$$\n\n6. Apply the same new Givens rotation to the right-hand side vector for the least-squares problem. Initially, $g^{(0)} = [\\beta, 0, \\dots, 0]^\\top \\in \\mathbb{R}^{m+1}$. Each new Givens rotation $G_j$ is also applied to entries $g_j$ and $g_{j+1}$:\n$$\n\\begin{bmatrix} g_j^\\prime \\\\ g_{j+1}^\\prime \\end{bmatrix}\n=\n\\begin{bmatrix} c_j  s_j \\\\ -s_j  c_j \\end{bmatrix}\n\\begin{bmatrix} g_j \\\\ g_{j+1} \\end{bmatrix}.\n$$\nAfter this update, the current GMRES residual norm at step $j$ equals $|g_{j+1}^\\prime|$ because the transformed least-squares problem has an upper-triangular coefficient matrix in the first $j$ rows:\n$$\n\\|r_j\\|_2 = \\left\\|\\beta e_1 - H_{j+1,j} y_j\\right\\|_2 = |g_{j+1}^\\prime|.\n$$\nTherefore, if $|g_{j+1}^\\prime| \\le \\tau$ for a small tolerance $\\tau$, we have detected that $r_j \\approx 0$ and GMRES has found an exact solution in $j$ steps (to numerical precision). The corresponding update $x_j = x_0 + V_j y_j$ can be obtained by back-substitution if needed, but for detection it is sufficient to monitor $|g_{j+1}^\\prime|$.\n\n7. Construction ensuring that $\\mathcal{K}_k(A, r_0)$ contains the exact solution: If $A$ is diagonal with distinct diagonal entries and $b$ has nonzero entries only in the first $k$ coordinates, then the invariant subspace excited by $r_0 = b - A x_0$ has dimension at most $k$. Specifically, let $A = \\operatorname{diag}(\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4)$ with distinct $\\lambda_i$, and let $x_0 = 0$ so that $r_0 = b$. If $b$ has nonzero components only along the first $k$ coordinate axes, then $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{e_1, \\dots, e_k\\}$ and the exact solution $x_\\ast = A^{-1} b$ lies in this $k$-dimensional invariant subspace, hence in $\\mathcal{K}_k(A, r_0)$. In the special case $k=1$, taking $b$ to be an eigenvector yields $x_\\ast \\in \\mathcal{K}_1(A, r_0)$. If three or more coordinates are excited but the iteration cap $m$ is less than the number of distinct excited eigendirections, GMRES cannot reach the exact solution within $m$ steps.\n\n8. Test cases:\n- Case 1: $A = \\operatorname{diag}(4,7,9,10)$, $b^{(1)} = [1,2,0,0]^\\top$, $x_0^{(1)} = 0$, $m^{(1)} = 4$. The excited eigendirections are two, so $\\mathcal{K}_2(A,r_0^{(1)})$ contains $x_\\ast^{(1)} = A^{-1} b^{(1)}$, and GMRES must detect $r_k = 0$ at $k \\le 2$ via $|g_{k+1}^\\prime|$ reaching zero.\n- Case 2: $b^{(2)} = [0,0,5,0]^\\top$, $x_0^{(2)} = 0$, $m^{(2)} = 4$. This is a single eigendirection; detection must occur at $k \\le 1$.\n- Case 3: $b^{(3)} = [1,1,1,0]^\\top$, $x_0^{(3)} = 0$, $m^{(3)} = 2$. Three distinct eigendirections are excited, but $m^{(3)} = 2$ disallows exact convergence; detection must fail within the limit.\n- Case 4: $b^{(4)} = [1,2,0,0]^\\top$, $x_0^{(4)} = A^{-1} b^{(4)} = [1/4, 2/7, 0, 0]^\\top$, $m^{(4)} = 4$. Here $r_0^{(4)} = 0$ and detection must yield $k=0$.\n\nAlgorithmic design:\n- Initialize with $r_0$, compute $\\beta = \\|r_0\\|_2$. If $\\beta \\le \\tau$, return $0$.\n- Set $v_1 = r_0 / \\beta$, $V = [v_1]$, and initialize $H$ as a zero matrix of size $(m+1) \\times m$.\n- Initialize $g = [\\beta, 0, 0, \\dots, 0]^\\top \\in \\mathbb{R}^{m+1}$ and arrays for cosines $c_j$ and sines $s_j$.\n- For $j=0,1,\\dots,m-1$:\n  - Compute $w = A v_j$.\n  - For $i=0,\\dots,j$: set $h_{i,j} = v_i^\\top w$, update $w \\leftarrow w - h_{i,j} v_i$.\n  - Set $h_{j+1,j} = \\|w\\|_2$, and if nonzero, append $v_{j+1} = w / h_{j+1,j}$.\n  - Apply all prior Givens rotations to the vector $[h_{0,j},\\dots,h_{j+1,j}]^\\top$.\n  - Construct a new Givens rotation to zero $h_{j+1,j}$ and apply it to $h_{j,j}$ and $h_{j+1,j}$, and to $g_j$ and $g_{j+1}$.\n  - The current residual norm is $|g_{j+1}|$. If this is $\\le \\tau$, declare detection at iteration $k = j+1$ and stop.\n- If the loop completes without detection, return $-1$.\n\nWhy the construction works:\n- In Cases 1 and 2, the chosen $b$ restricts the excited invariant subspace of $A$ to dimension $2$ and $1$, respectively. Because $A$ is diagonal and the initial guess is $x_0 = 0$, the residual $r_0 = b$ lies in that invariant subspace. The Krylov subspace $\\mathcal{K}_k(A, r_0)$ spans this subspace once $k$ reaches its dimension. The exact solution $x_\\ast = A^{-1} b$ belongs to the same invariant subspace, hence $x_\\ast \\in \\mathcal{K}_k(A, r_0)$ with $k=2$ in Case 1 and $k=1$ in Case 2. The least-squares residual after applying Givens rotations must reach zero at those $k$.\n- In Case 3, three eigendirections are excited but the iteration cap $m=2$ is insufficient, so the least-squares residual cannot reach zero; $|g_{k+1}|$ remains above tolerance and the algorithm returns $-1$.\n- In Case 4, $r_0 = 0$ and GMRES terminates immediately with detection at iteration $0$.\n\nQuantifiable outputs:\n- For each case, the program outputs an integer indicating the first iteration at which the rotated right-hand side yields zero residual within the tolerance, or $-1$ if not within the iteration limit.\n\nThe final program implements this procedure with the specified test suite, and prints a single line with the list of four integers in order. No physical units or angle measures are involved.",
            "answer": "```python\nimport numpy as np\n\ndef gmres_givens_detect_iteration(A, b, x0, m, tol=1e-12):\n    \"\"\"\n    Run at most m steps of GMRES with Givens rotations to detect the\n    first iteration at which the residual is (numerically) zero.\n    Returns:\n        k (int): 0 if initial residual is zero,\n                 else the smallest positive iteration k = m with residual = tol,\n                 else -1 if not detected within m.\n    \"\"\"\n    n = A.shape[0]\n    r0 = b - A @ x0\n    beta = np.linalg.norm(r0)\n    if beta = tol:\n        return 0\n\n    # Storage for Arnoldi basis and Hessenberg\n    V = np.zeros((n, m + 1), dtype=float)\n    H = np.zeros((m + 1, m), dtype=float)\n\n    # Initialize Arnoldi\n    V[:, 0] = r0 / beta\n\n    # Givens rotation parameters\n    cs = np.zeros(m, dtype=float)  # cosines\n    sn = np.zeros(m, dtype=float)  # sines\n\n    # Right-hand side for the small least-squares problem\n    g = np.zeros(m + 1, dtype=float)\n    g[0] = beta\n\n    for j in range(m):\n        # Arnoldi step: w = A v_j\n        w = A @ V[:, j]\n        # Orthogonalize against existing V\n        for i in range(j + 1):\n            H[i, j] = np.dot(V[:, i], w)\n            w = w - H[i, j] * V[:, i]\n        H[j + 1, j] = np.linalg.norm(w)\n        if H[j + 1, j]  0:\n            V[:, j + 1] = w / H[j + 1, j]\n\n        # Apply all previous Givens rotations to the new column of H\n        for i in range(j):\n            temp = cs[i] * H[i, j] + sn[i] * H[i + 1, j]\n            H[i + 1, j] = -sn[i] * H[i, j] + cs[i] * H[i + 1, j]\n            H[i, j] = temp\n\n        # Compute new Givens rotation to zero H[j+1, j]\n        h_ij = H[j, j]\n        h_ip1j = H[j + 1, j]\n        r = np.hypot(h_ij, h_ip1j)\n        if r == 0:\n            # No rotation needed; degenerate, but set to identity\n            cs[j] = 1.0\n            sn[j] = 0.0\n        else:\n            cs[j] = h_ij / r\n            sn[j] = h_ip1j / r\n\n        # Apply new Givens to H entries\n        H[j, j] = cs[j] * h_ij + sn[j] * h_ip1j\n        H[j + 1, j] = 0.0\n\n        # Apply Givens to g\n        g_j = g[j]\n        g_jp1 = g[j + 1]\n        g[j] = cs[j] * g_j + sn[j] * g_jp1\n        g[j + 1] = -sn[j] * g_j + cs[j] * g_jp1\n\n        # Residual norm is |g[j+1]|\n        resnorm = abs(g[j + 1])\n\n        if resnorm = tol:\n            # Detected exact convergence at iteration k = j+1\n            return j + 1\n\n    # Not detected within m steps\n    return -1\n\n\ndef solve():\n    # Define the fixed diagonal matrix A\n    A = np.diag([4.0, 7.0, 9.0, 10.0])\n\n    # Test suite as specified in the problem statement\n    # Case 1: happy-path, exact solution in at most two steps\n    b1 = np.array([1.0, 2.0, 0.0, 0.0])\n    x01 = np.zeros(4)\n    m1 = 4\n\n    # Case 2: boundary k=1 (b is an eigenvector)\n    b2 = np.array([0.0, 0.0, 5.0, 0.0])\n    x02 = np.zeros(4)\n    m2 = 4\n\n    # Case 3: non-convergence within limit m=2\n    b3 = np.array([1.0, 1.0, 1.0, 0.0])\n    x03 = np.zeros(4)\n    m3 = 2\n\n    # Case 4: zero initial residual (x0 is exact solution)\n    b4 = np.array([1.0, 2.0, 0.0, 0.0])\n    x04 = np.array([1.0/4.0, 2.0/7.0, 0.0, 0.0])  # A^{-1} b4\n    m4 = 4\n\n    test_cases = [\n        (A, b1, x01, m1),\n        (A, b2, x02, m2),\n        (A, b3, x03, m3),\n        (A, b4, x04, m4),\n    ]\n\n    results = []\n    for A_i, b_i, x0_i, m_i in test_cases:\n        k_detect = gmres_givens_detect_iteration(A_i, b_i, x0_i, m_i, tol=1e-12)\n        results.append(int(k_detect))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}