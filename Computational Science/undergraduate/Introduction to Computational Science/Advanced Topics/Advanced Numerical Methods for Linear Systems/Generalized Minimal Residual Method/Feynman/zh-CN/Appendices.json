{
    "hands_on_practices": [
        {
            "introduction": "GMRES方法的核心在于其巧妙的双重特性：它既可以被看作一个将残差投影到克雷洛夫子空间上的几何过程，也可以被理解为寻找一个最优的“残差多项式”的代数问题。本练习将引导你通过编程实践，亲手验证这两个视角是等价的。通过直接实现这两种方法并比较它们的结果，你将深刻理解GMRES在每次迭代中究竟在做什么，从而建立对该算法的直观而坚实的认识。",
            "id": "3136956",
            "problem": "您的任务是将广义最小残差方法 (GMRES) 的残差多项式观点与正交投影观点联系起来。目标是展示什么是残差多项式，为什么其最小化是 GMRES 的特征，以及如何从第一性原理出发通过算法计算它。\n\n基本出发点和定义：\n- 对于一个线性系统 $A x = b$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个实数方阵，$x_0 \\in \\mathbb{R}^n$ 是一个初始猜测，以及欧几里得范数 $\\|\\cdot\\|_2$，初始残差为 $r_0 = b - A x_0$。\n- 第 $k$ 个 Krylov 子空间是 $\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0\\}$。\n- 一个次数最多为 $k$ 且满足 $p_k(0) = 1$ 的残差多项式是一个形如 $p_k(t) = 1 + c_1 t + c_2 t^2 + \\cdots + c_k t^k$ 并满足约束 $p_k(0) = 1$ 的多项式；残差多项式观点旨在寻找系数 $c_1, \\dots, c_k$ 以最小化 $\\|p_k(A) r_0\\|_2$。\n- 广义最小残差方法 (GMRES) 定义为选择 $x_k \\in x_0 + \\mathcal{K}_k(A,r_0)$ 以最小化 $\\|b - A x_k\\|_2$，其正交投影解释为 GMRES 残差 $r_k = b - A x_k$ 与 $A \\mathcal{K}_k(A,r_0)$ 正交。\n\n对于下方的每个测试用例，您的程序必须纯粹以数学方式执行以下任务：\n1. 构造一个次数最多为 $k$ 且满足 $p_k(0) = 1$ 的残差多项式 $p_k(t)$，并计算 $\\|p_k(A) r_0\\|_2$ 的可达到最小值，记为 $N_{\\text{poly}}$。\n2. 独立地计算 $N_{\\text{gmres}}$，即从 $x_0$ 开始执行 $k$ 步广义最小残差方法 (GMRES) 后得到的残差范数。\n3. 通过检查 GMRES 残差 $r_k$ 是否与集合 $A \\mathcal{K}_k(A,r_0)$ 正交来验证正交投影的解释，数值容差为 $10^{-9}$，即对于每个基向量 $w \\in \\mathcal{K}_k(A,r_0)$，内积 $r_k^\\top (A w)$ 的大小最多为 $10^{-9} \\|r_k\\|_2 \\|A w\\|_2$。\n4. 对于每个测试用例，输出两项：绝对差 $|N_{\\text{poly}} - N_{\\text{gmres}}|$（一个浮点数）和一个报告正交性条件是否成立的布尔值（true 或 false）。将所有用例的结果交替排列成一个单行列表。\n\n测试套件规范：\n- 所有计算都使用实数，没有任何物理单位。\n- 用于定义条目的角度必须以弧度为单位。\n- 右端向量确定性地定义为 $b \\in \\mathbb{R}^8$，其条目为 $b_i = \\sin(i)$，其中 $i = 1, 2, \\dots, 8$（$\\sin(\\cdot)$ 以弧度计算）。初始猜测是 $\\mathbb{R}^8$ 中的 $x_0 = 0$。\n- 矩阵：\n  1. 用例 1（对称且特征值分布广泛）：$A_1 \\in \\mathbb{R}^{8 \\times 8}$ 是对角矩阵，对角线元素为 $\\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 5000.0\\}$。次数 $k = 3$。\n  2. 用例 2（边界次数）：与用例 1 相同的 $A_1$。次数 $k = 0$。\n  3. 用例 3（非正规且特征值分布广泛）：$A_3 \\in \\mathbb{R}^{8 \\times 8}$ 是上三角矩阵，对角线元素为 $\\{0.001, 0.01, 1.0, 10.0, 100.0, 500.0, 1000.0, 5000.0\\}$，第一超对角线元素为 1；所有其他非对角线元素为零。次数 $k = 7$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个 Python 风格的列表，该列表包含六个元素，按顺序（用例 1、用例 2、用例 3）交替排列三个测试用例的浮点数和布尔值。例如，格式必须与以下完全一样\n$[r_1, b_1, r_2, b_2, r_3, b_3]$\n其中 $r_j$ 是浮点数，$b_j$ 是布尔值。",
            "solution": "该问题要求验证广义最小残差方法 (GMRES) 在求解线性系统 $A x = b$（其中 $A \\in \\mathbb{R}^{n \\times n}$）时的两个基本观点之间的等价性。第一个观点是残差多项式的最小化，第二个是定义标准算法的正交投影方法。\n\n设初始猜测为 $x_0$，初始残差为 $r_0 = b - A x_0$。GMRES 的第 $k$ 次迭代 $x_k$ 在仿射 Krylov 子空间 $x_0 + \\mathcal{K}_k(A,r_0)$ 中寻找，其中 $\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$。\n\n**1. 残差多项式观点**\n\nGMRES 迭代 $x_k$ 可以表示为 $x_k = x_0 + z_{k-1}$，其中向量 $z_{k-1} \\in \\mathcal{K}_k(A,r_0)$。根据 Krylov 子空间的定义，$z_{k-1}$ 可以写成一个作用于 $r_0$ 的、次数最多为 $k-1$ 的 $A$ 的多项式。让我们将 $z_{k-1}$ 写为 $z_{k-1} = q_{k-1}(A)r_0$，其中 $q_{k-1}$ 是某个多项式。相应的残差是 $r_k = b - A x_k = b - A(x_0 + q_{k-1}(A)r_0) = (b - A x_0) - A q_{k-1}(A)r_0 = r_0 - A q_{k-1}(A)r_0$。\n\n让我们定义一个新多项式 $p_k(t) = 1 - t q_{k-1}(t)$。这个多项式 $p_k(t)$ 的次数最多为 $k$，并满足关键约束 $p_k(0) = 1$。现在，残差可以表示为 $r_k = p_k(A)r_0$。GMRES 寻找迭代 $x_k$，使得在所有可能的 $z_{k-1} \\in \\mathcal{K}_k(A,r_0)$ 选择中，$\\|r_k\\|_2 = \\|p_k(A)r_0\\|_2$ 最小化。这等价于在所有次数最多为 $k$ 且满足 $p_k(0)=1$ 的多项式 $p_k$ 上进行最小化。\n\n为了从这个观点计算最小范数（记为 $N_{\\text{poly}}$），我们建立一个线性最小二乘问题。一个通用的此类多项式是 $p_k(t) = 1 + c_1 t + c_2 t^2 + \\dots + c_k t^k$。我们寻求系数 $c = [c_1, \\dots, c_k]^\\top$ 来最小化：\n$$ \\|p_k(A)r_0\\|_2 = \\left\\| \\left(I + \\sum_{j=1}^k c_j A^j\\right) r_0 \\right\\|_2 = \\left\\| r_0 + \\sum_{j=1}^k c_j (A^j r_0) \\right\\|_2 $$\n令 $M$ 为一个矩阵，其列是向量 $A^j r_0$，其中 $j=1, \\dots, k$。问题是找到 $c$ 来最小化 $\\|r_0 + Mc\\|_2$，或者等价地，$\\|Mc - (-r_0)\\|_2$。这是一个标准的线性最小二乘问题。最小范数 $N_{\\text{poly}}$ 是该问题的残差范数，即 $N_{\\text{poly}} = \\|r_0 + Mc^*\\|_2$，其中 $c^*$ 是最优系数向量。\n\n**2. GMRES 算法和正交投影**\n\n标准的 GMRES 算法实现了对 $\\|r_k\\|_2 = \\|b-Ax_k\\|_2$ 的最小化，其中 $x_k \\in x_0+\\mathcal{K}_k(A,r_0)$。这是一个在 $k$ 维子空间上的最小二乘问题。关键是通过 Arnoldi 迭代为 $\\mathcal{K}_{k+1}(A, r_0)$ 构造一个标准正交基。\n\nArnoldi 迭代生成一组标准正交向量 $\\{q_1, q_2, \\dots, q_{k+1}\\}$，它们张成 $\\mathcal{K}_{k+1}(A, r_0)$，其中 $q_1 = r_0 / \\|r_0\\|_2$。它还产生一个上 Hessenberg 矩阵 $\\bar{H}_k \\in \\mathbb{R}^{(k+1) \\times k}$，使得 $A V_k = V_{k+1} \\bar{H}_k$，其中 $V_j = [q_1, \\dots, q_j]$。\n\n迭代可以表示为 $x_k = x_0 + V_k y_k$，其中 $y_k \\in \\mathbb{R}^k$ 是某个坐标向量。最小化问题变为：\n$$ \\min_{y_k \\in \\mathbb{R}^k} \\|r_0 - A V_k y_k\\|_2 = \\min_{y_k \\in \\mathbb{R}^k} \\| \\|r_0\\|_2 q_1 - V_{k+1} \\bar{H}_k y_k \\|_2 $$\n利用 $q_1 = V_{k+1} e_1$（其中 $e_1=[1,0,\\dots,0]^\\top \\in \\mathbb{R}^{k+1}$）和 $V_{k+1}$ 的等距性，问题简化为关于 $y_k$ 的一个小的 $(k+1) \\times k$ 最小二乘问题：\n$$ \\min_{y_k \\in \\mathbb{R}^k} \\| \\|r_0\\|_2 e_1 - \\bar{H}_k y_k \\|_2 $$\nGMRES 残差的范数 $N_{\\text{gmres}}$ 是这个小问题的最小值（残差向量的范数）。设 $y_k^*$ 是解。那么 $N_{\\text{gmres}} = \\| \\|r_0\\|_2 e_1 - \\bar{H}_k y_k^* \\|_2$。\n\n**3. 正交性条件的验证**\n\nGMRES 最小化问题的一阶最优性条件是残差 $r_k$ 必须与平移到原点的搜索空间 $A \\mathcal{K}_k(A, r_0)$ 正交。这意味着对于所有 $v \\in A \\mathcal{K}_k(A, r_0)$，都有 $r_k^\\top v = 0$。\n\n为了在数值上验证这一点，我们针对 $A \\mathcal{K}_k(A, r_0)$ 的一个基来检查该条件。该空间的一个自然基是 $\\{A r_0, A^2 r_0, \\dots, A^k r_0\\}$。对于每个基向量 $v_j = A^j r_0$（其中 $j \\in \\{1, \\dots, k\\}$），我们必须验证其与 GMRES 残差 $r_k$ 的内积在指定的容差 $\\epsilon = 10^{-9}$ 内接近于零。条件是：\n$$ |r_k^\\top v_j| \\leq \\epsilon \\|r_k\\|_2 \\|v_j\\|_2 $$\nGMRES 残差向量 $r_k$ 计算为 $r_k = V_{k+1} (\\|r_0\\|_2 e_1 - \\bar{H}_k y_k^*)$。\n\n下面的实现独立地计算 $N_{\\text{poly}}$ 和 $N_{\\text{gmres}}$，然后验证正交性属性，从而展示了两个观点的理论等价性。绝对差 $|N_{\\text{poly}} - N_{\\text{gmres}}|$ 应接近浮点机器精度。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GMRES verification for all test cases.\n    \"\"\"\n\n    def run_case(A, b, x0, k):\n        \"\"\"\n        Performs the required computations for a single test case.\n        1. Computes N_poly from the residual-polynomial viewpoint.\n        2. Computes N_gmres and the residual vector r_k via GMRES/Arnoldi.\n        3. Checks the orthogonality of r_k against A*K_k(A,r0).\n        Returns the absolute difference between N_poly and N_gmres, and a\n        boolean indicating if the orthogonality condition holds.\n        \"\"\"\n        n = A.shape[0]\n        r0 = b - A @ x0\n        norm_r0 = np.linalg.norm(r0)\n        \n        # Handle the trivial case k=0\n        if k == 0:\n            N_poly = norm_r0\n            N_gmres = norm_r0\n            # The space A*K_0 is {0}, so any vector is orthogonal to it.\n            is_orthogonal = True\n            return np.abs(N_poly - N_gmres), is_orthogonal\n\n        # --- 1. Residual-Polynomial Viewpoint ---\n        # Minimize ||r0 + c1*A*r0 + ... + ck*A^k*r0|| wrt cj's.\n        # This is the least-squares problem ||M*c - (-r0)||^2.\n        M = np.zeros((n, k))\n        power_vec = r0\n        for j in range(k):\n            power_vec = A @ power_vec\n            M[:, j] = power_vec\n        \n        c = np.linalg.lstsq(M, -r0, rcond=None)[0]\n        res_poly_vec = r0 + M @ c\n        N_poly = np.linalg.norm(res_poly_vec)\n\n        # --- 2. GMRES/Arnoldi Viewpoint ---\n        Q = np.zeros((n, k + 1))\n        H = np.zeros((k + 1, k))\n        \n        if norm_r0 == 0:\n            # x0 is the exact solution.\n            N_gmres = 0.0\n            r_k = np.zeros(n)\n        else:\n            Q[:, 0] = r0 / norm_r0\n    \n            for j in range(k):\n                w = A @ Q[:, j]\n                for i in range(j + 1):\n                    H[i, j] = Q[:, i].T @ w\n                    w = w - H[i, j] * Q[:, i]\n                \n                h_next = np.linalg.norm(w)\n                if h_next < 1e-12: # Check for breakdown\n                    # In case of breakdown, the space is smaller.\n                    # This requires truncating H and Q matrices.\n                    # For a general-purpose solver this is crucial; here we assume no breakdown.\n                    # For this problem's setup, this path is not taken.\n                    k_eff = j + 1\n                    H = H[:k_eff+1, :k_eff]\n                    Q = Q[:, :k_eff+1]\n                    break\n                H[j + 1, j] = h_next\n                Q[:, j + 1] = w / h_next\n\n            # Solve the small least-squares problem: min ||norm(r0)*e1 - H*y||\n            e1 = np.zeros(k + 1)\n            e1[0] = 1.0\n            rhs = norm_r0 * e1\n            \n            y = np.linalg.lstsq(H, rhs, rcond=None)[0]\n            \n            # Compute N_gmres and the full residual vector r_k\n            small_res_vec = rhs - H @ y\n            N_gmres = np.linalg.norm(small_res_vec)\n            r_k = Q @ small_res_vec\n\n        # --- 3. Orthogonality Check ---\n        # Verify r_k is orthogonal to the basis {A*r0, A^2*r0, ..., A^k*r0}\n        # which spans A*K_k(A,r0).\n        is_orthogonal = True\n        norm_rk = np.linalg.norm(r_k)\n        check_vec = r0\n        \n        for j in range(1, k + 1):\n            check_vec = A @ check_vec   # This is A^j * r0\n            norm_check_vec = np.linalg.norm(check_vec)\n            \n            ip = r_k.T @ check_vec\n            \n            # Use relative tolerance check\n            if norm_rk > 0 and norm_check_vec > 0:\n                condition = (np.abs(ip) <= 1e-9 * norm_rk * norm_check_vec)\n            else:\n                # If either vector is zero, their dot product must be (near) zero\n                condition = (np.abs(ip) < 1e-9)\n\n            if not condition:\n                is_orthogonal = False\n                break\n\n        # --- 4. Return results for the case ---\n        diff = np.abs(N_poly - N_gmres)\n        return diff, is_orthogonal\n\n    # Define common parameters\n    n = 8\n    b = np.sin(np.arange(1, n + 1))\n    x0 = np.zeros(n)\n    \n    # Define test cases\n    A1_diag = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 5000.0]\n    A1 = np.diag(A1_diag)\n    \n    A3_diag = [0.001, 0.01, 1.0, 10.0, 100.0, 500.0, 1000.0, 5000.0]\n    A3 = np.diag(A3_diag) + np.diag(np.ones(n - 1), 1)\n\n    test_cases = [\n        (A1, b, x0, 3),  # Case 1\n        (A1, b, x0, 0),  # Case 2\n        (A3, b, x0, 7),  # Case 3\n    ]\n\n    # Run all test cases and collect results\n    results = []\n    for case in test_cases:\n        diff, ortho = run_case(*case)\n        results.append(diff)\n        results.append(ortho)\n\n    # Format and print the final output\n    # Using a custom formatter for booleans to satisfy potential ambiguity,\n    # though standard str(bool) -> 'True'/'False' would also be Python-style.\n    # The problem example `true`/`false` is non-standard in Python output. \n    # Sticking to standard Python representation 'True'/'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "理解了GMRES单步迭代的本质后，一个自然的问题是：这个迭代过程会持续多久？理论上，GMRES的收敛与一个深刻的代数概念——矩阵关于初始残差的最小多项式——紧密相关。此练习将让你通过编写代码来验证一个关键理论：在精确计算下，GMRES的收敛步数恰好等于该最小多项式的次数。这个实践将揭示GMRES收敛行为的“终极”秘密，并解释为何它能在有限步内找到精确解。",
            "id": "3137005",
            "problem": "您需要设计并实现一个程序，以演示用于求解线性系统 $A x = b$ 的广义最小残差方法 (GMRES) 的精确终止行为。您必须使用的理论基础仅限于核心线性代数定义和事实：克雷洛夫子空间的概念、阿诺迪过程以及矩阵相对于向量的最小多项式的定义。您可以假定的基本定义是：对于一个方阵 $A \\in \\mathbb{R}^{n \\times n}$ 和初始残差 $r_0 = b - A x_0$，其 $k$ 阶克雷洛夫子空间为 $ \\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{ r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0 \\}$；$A$ 相对于 $r_0$ 的最小多项式是使得 $p(A) r_0 = 0$ 成立的最低次的非零多项式 $p$，其次数等于克雷洛夫子空间停止增长时的维数。您必须从这些定义出发，不能使用其他定义，也不得依赖任何跳过从这些基础推导 GMRES 的快捷公式。\n\n任务。从零初始猜测 $x_0 = 0$ 开始，使用阿诺迪过程和限制在克雷洛夫子空间内的最小二乘解法，从第一性原理实现一个非重启动的 GMRES 求解器。使用相对残差停止准则：在第一个满足 $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$ 的迭代次数 $k$ 处停止，其中 $\\varepsilon$ 是给定的容差。同时，实现一个例程，通过正交规范化检测序列 $\\{r_0, A r_0, A^2 r_0, \\dots\\}$ 中的线性相关性，计算 $A$ 相对于 $r_0$ 的最小多项式的次数 $g$。\n\n科学要求。在精确算术中，GMRES 最多在相对于 $r_0$ 的最小多项式次数 $g$ 次迭代后终止。您的程序必须通过一系列精心选择的测试矩阵 $A$ 和相应的右端项 $b$，数值上证明 GMRES 达到停止准则的首次迭代次数 $k^\\star$ 等于该次数 $g$。\n\n角度单位不适用。不涉及物理单位。所有计算都是在 $\\mathbb{R}$ 上的纯数学计算。\n\n容差。使用数值容差 $\\varepsilon = 10^{-12}$。\n\n测试套件。您的程序必须将这两个例程（GMRES 终止计数器和最小多项式次数检测器）应用于以下四种情况，所有情况均设置 $x_0 = 0$：\n\n- 情况 1（次数为 1 的边界情况）：\n  - $A_1 = 5 I_3$，\n  - $b_1 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}$。\n- 情况 2（可对角化，具有两个不同特征值）：\n  - $A_2 = \\operatorname{diag}(1, 2, 2)$，\n  - $b_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$。\n- 情况 3（亏损若尔当块，完整链）：\n  - $A_3 = \\begin{bmatrix} 2  1  0 \\\\ 0  2  1 \\\\ 0  0  2 \\end{bmatrix}$，\n  - $b_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$。\n- 情况 4（块对角，选择性激励；全局最小多项式大于相对最小多项式）：\n  - $A_4 = \\operatorname{diag}\\!\\Big( \\begin{bmatrix} 3  1 \\\\ 0  3 \\end{bmatrix}, \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\Big)$，\n  - $b_4 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n\n对于每种情况，设 $k^\\star$ 为满足停止准则的首次 GMRES 迭代次数，设 $g$ 为检测到的 $A$ 相对于 $r_0 = b - A x_0 = b$ 的最小多项式的次数。为保证科学真实性和数值鲁棒性，通过修正的格拉姆-施密特方法构建 $\\{ r_0, A r_0, A^2 r_0, \\dots \\}$ 的正交规范基来确定 $g$，并在检测到下一个向量与之前向量线性相关（容差与 $\\|r_0\\|_2$ 成正比）时停止。\n\n要求输出。您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。对于每种情况，输出语句“$k^\\star = g$”的布尔值。因此，最终输出应为单行形式，例如“[True,True,True,True]”。不应打印任何其他文本。\n\n注意。\n- 上述使用的所有数字，如 $5$、$1$、$2$、$3$ 和 $10^{-12}$，都是测试实例和容差的精确规范。\n- 您不得读取输入；您必须按照说明精确地硬编码测试套件，并按规定精确打印一行输出。\n- 您的实现必须遵循标准的双精度算术和上述定义。",
            "solution": "该问题要求对广义最小残差方法 (GMRES) 求解形如 $Ax=b$ 的线性系统的一个基本性质进行数值演示。具体来说，该性质假设在精确算术中，此方法在迭代次数等于矩阵 $A$ 相对于初始残差向量 $r_0$ 的最小多项式次数时，会以精确解终止。我们的任务是实现一个 GMRES 求解器和一个计算该次数的函数，然后在一系列测试用例中验证它们是等价的。我们按照要求从第一性原理开始。\n\n初始猜测指定为 $x_0=0$，这意味着初始残差为 $r_0 = b - Ax_0 = b$。\n\n**1. 最小多项式的次数**\n\n一个矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 相对于一个向量 $r_0 \\in \\mathbb{R}^n$ 的最小多项式是使得 $p(A)r_0 = 0$ 成立的最低次数 $g$ 的首一多项式 $p(z)$。该次数 $g$ 也是克雷洛夫子空间 $\\mathcal{K}(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, A^2 r_0, \\dots\\}$ 的维数。当序列 $\\{A^k r_0\\}_{k \\ge 0}$ 中的某个向量与它前面的向量线性相关时，维数停止增加。第一个发生这种情况的索引 $k$ 定义了次数，即 $g=k$。\n\n我们可以通过为增长的克雷洛夫子空间构造一个正交规范基来计算确定 $g$，这里使用修正的格拉姆-施密特 (MGS) 算法以获得更好的数值稳定性。设克雷洛夫向量序列为 $u_k = A^k r_0$，其中 $k=0, 1, 2, \\dots, n$。我们为 $\\mathcal{K}_g(A, r_0)$ 生成一个正交规范基 $\\{v_0, v_1, \\dots, v_{g-1}\\}$。\n\n算法过程如下：\n1. 初始化一个空的正交规范基向量列表 $V$。\n2. 对于 $k=0, 1, 2, \\dots, n$：\n   a. 取下一个克雷洛夫向量 $u_k = A^k r_0$。（这通过迭代计算：$u_0=r_0$, $u_{k+1}=Au_k$）。\n   b. 将 $u_k$ 与 $V$ 中当前基向量 $\\{v_0, \\dots, v_{k-1}\\}$ 进行正交化：\n      $$ w = u_k - \\sum_{j=0}^{k-1} (v_j^T u_k) v_j $$\n   c. 范数 $\\|w\\|_2$ 代表了 $u_k$ 中与子空间 $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{v_0, \\dots, v_{k-1}\\}$ 正交的分量。如果该范数在数值上为零（即 $\\|w\\|_2 \\le \\tau \\|r_0\\|_2$，其中 $\\tau$ 为某个小容差），则表示 $u_k$ 与前面的向量 $\\{u_0, \\dots, u_{k-1}\\}$ 线性相关。克雷洛夫子空间的维数为 $k$，因此最小多项式的次数为 $g=k$。过程终止，返回 $k$。\n   d. 如果 $\\|w\\|_2$ 不可忽略，我们将其归一化以获得下一个基向量 $v_k = w / \\|w\\|_2$，并将其添加到我们的正交规范集 $V$ 中。\n\n此过程直接确定了可达到的最大克雷洛夫子空间的维数，根据定义，这即是次数 $g$。\n\n**2. 从第一性原理出发的 GMRES 算法**\n\nGMRES 方法在第 $k$ 次迭代时，从仿射子空间 $x_0 + \\mathcal{K}_k(A, r_0)$ 中寻找一个近似解 $x_k$。选择 $x_k$ 是为了最小化残差的 $2$-范数，即 $\\|r_k\\|_2 = \\|b - Ax_k\\|_2$。在初始猜测 $x_0=0$ 的情况下，近似解 $x_k$ 位于克雷洛夫子空间 $\\mathcal{K}_k(A, r_0)$ 内。\n\nGMRES 的核心是阿诺迪过程，它为克雷洛夫子空间 $\\mathcal{K}_{k+1}(A, r_0)$ 构建一个正交规范基 $V_{k+1} = [v_1, \\dots, v_{k+1}]$，同时生成一个上Hessenberg矩阵 $H_{k+1,k} \\in \\mathbb{R}^{(k+1) \\times k}$。这些矩阵通过阿诺迪关系式联系在一起：\n$$ A V_k = V_{k+1} H_{k+1,k} $$\n其中 $V_k = [v_1, \\dots, v_k]$。该过程以第一个基向量 $v_1 = r_0 / \\|r_0\\|_2$ 初始化。\n\n迭代解 $x_k$ 可以表示为基向量的线性组合，$x_k = V_k y$，其中 $y \\in \\mathbb{R}^k$ 是某个坐标向量。相应的残差则为：\n$$ r_k = b - Ax_k = r_0 - A V_k y $$\n代入 $r_0 = \\|r_0\\|_2 v_1$ 和阿诺迪关系式，我们得到：\n$$ r_k = \\|r_0\\|_2 v_1 - V_{k+1} H_{k+1,k} y = V_{k+1} (\\|r_0\\|_2 e_1 - H_{k+1,k} y) $$\n其中 $e_1$ 是 $\\mathbb{R}^{k+1}$ 中的第一个标准基向量。由于 $V_{k+1}$ 的列是正交规范的，最小化 $\\|r_k\\|_2$ 等价于最小化 $\\|\\,\\|r_0\\|_2 e_1 - H_{k+1,k} y\\,\\|_2$。\n\n这是一个关于坐标向量 $y$ 的小规模线性最小二乘问题。它可以在每次迭代 $k$ 时求解。整个 GMRES 算法如下：\n1. 初始化：$r_0 = b$，$\\beta = \\|r_0\\|_2$， $v_1 = r_0 / \\beta$。\n2. 对于 $k=1, 2, \\dots, n$：\n   a. **阿诺迪步骤**：使用阿诺迪过程生成 $v_{k+1}$ 和Hessenberg矩阵的第 $(k-1)$ 列。这涉及计算 $w = Av_k$ 并将其与 $\\{v_1, \\dots, v_k\\}$ 正交化。设得到的系数为 $h_{i,k}$ (对于 $i=1, \\dots, k$)，正交化后向量的范数为 $h_{k+1,k}$。\n   b. **最小二乘求解**：构建矩阵 $H_{k+1,k}$ 和向量 $g = \\beta e_1 \\in \\mathbb{R}^{k+1}$。找到最小化 $\\|H_{k+1,k} y - g\\|_2$ 的 $y_k \\in \\mathbb{R}^k$。\n   c. **检查收敛性**：第 $k$ 次迭代的残差范数是最小二乘问题的最小值：$\\|r_k\\|_2 = \\min_y \\|H_{k+1,k} y - g\\|_2$。我们检查相对残差是否满足 $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$。如果满足，终止迭代次数为 $k^\\star = k$，然后停止。\n\n**3. 验证 $k^\\star = g$**\n\n连接这两种算法的理论基础是，如果最小多项式的次数为 $g$，那么克雷洛夫子空间 $\\mathcal{K}_g(A, r_0)$ 包含精确解 $x = A^{-1}b$。此外，对于任何向量 $u \\in \\mathcal{K}_g(A, r_0)$，向量 $Au$ 也位于 $\\mathcal{K}_g(A, r_0)$ 中。\n当阿诺迪过程达到迭代 $k=g$ 时，向量 $A v_g$ 已经位于由 $\\{v_1, \\dots, v_g\\}$ 张成的空间中。因此，在正交化之后，剩余的向量为零，导致Hessenberg矩阵的元素 $h_{g+1,g}$ 的值为零（或在数值上极小）。这一事件被称为“幸运分解”。这对最小二乘问题的影响是，存在一个向量 $y_g$ 使得 $H_{g+1,g} y_g = \\beta e_1$，这使得残差范数 $\\|r_g\\|_2$ 降至零。GMRES 的近似解 $x_g$ 成为精确解。\n因此，对于足够小的容差 $\\varepsilon=10^{-12}$，GMRES 满足停止准则的首次迭代次数 $k^\\star$ 将恰好是相对于 $r_0$ 的最小多项式的次数 $g$。我们的程序将为每个测试用例计算 $g$ 和 $k^\\star$，并验证此等式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_min_poly_degree(A, b):\n    \"\"\"\n    Computes the degree of the minimal polynomial of A with respect to b.\n    This is done by finding the dimension of the Krylov subspace K(A, b)\n    by detecting linear dependence in the sequence {b, Ab, A^2b, ...}\n    using modified Gram-Schmidt.\n    \"\"\"\n    n = A.shape[0]\n    r0_norm = np.linalg.norm(b)\n    \n    if r0_norm == 0:\n        return 0\n\n    # Tolerance for detecting linear dependence, proportional to the initial norm.\n    dep_tol = 1e-12 * r0_norm\n\n    orthonormal_basis = []\n    current_krylov_vec = b.copy().astype(np.float64)\n\n    for k in range(n + 1):\n        # Orthogonalize the current Krylov vector against the basis so far.\n        orth_vec = current_krylov_vec.copy()\n        for v in orthonormal_basis:\n            proj = np.dot(v.conj(), orth_vec)\n            orth_vec -= proj * v\n            \n        norm_orth_vec = np.linalg.norm(orth_vec)\n        \n        # If the norm of the orthogonalized vector is close to zero,\n        # it means the current Krylov vector is linearly dependent on the previous ones.\n        if norm_orth_vec < dep_tol:\n            return k # The degree of the minimal polynomial is k.\n            \n        # Add the new orthonormal vector to the basis.\n        orthonormal_basis.append(orth_vec / norm_orth_vec)\n        \n        # Generate the next Krylov vector for the next iteration.\n        current_krylov_vec = A @ current_krylov_vec\n            \n    return n\n\ndef gmres_solver(A, b, tol):\n    \"\"\"\n    Implements a non-restarted GMRES solver from first principles.\n    Returns the number of iterations k* at which the relative residual\n    ||r_k||/||b|| falls below the tolerance.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Initial guess x0 = 0, so initial residual r0 = b.\n    r0 = b.copy().astype(np.float64)\n    b_norm = np.linalg.norm(b)\n    \n    if b_norm == 0:\n        return 0 # Trivial case: solution is x=0, 0 iterations.\n\n    r0_norm = b_norm\n    \n    # V stores the orthonormal basis vectors for the Krylov subspace.\n    V = [r0 / r0_norm]\n    # H will be the (k+1) x k upper Hessenberg matrix.\n    H = np.zeros((n + 1, n), dtype=np.float64)\n    \n    # e1 is used to form the right-hand side of the least-squares problem.\n    e1 = np.zeros(n + 1, dtype=np.float64)\n    e1[0] = 1.0\n\n    # Main GMRES loop.\n    for k in range(n): # k from 0 to n-1, corresponding to iterations 1 to n.\n        # Arnoldi process to generate v_{k+1} and the k-th column of H.\n        # Note on indices: V[k] is the (k+1)-th basis vector, v_{k+1} in 1-based indexing.\n        w = A @ V[k]\n        \n        for j in range(k + 1):\n            H[j, k] = np.dot(w, V[j])\n            w = w - H[j, k] * V[j]\n        \n        H[k + 1, k] = np.linalg.norm(w)\n        \n        # Form and solve the least-squares problem for iteration k+1.\n        # The Hessenberg matrix for this iteration is of size (k+2) x (k+1).\n        H_sub = H[:k+2, :k+1]\n        \n        # The right-hand side is beta * e1, where beta = ||r0||.\n        target = r0_norm * e1[:k+2]\n        \n        # Solve the (k+2)x(k+1) least-squares problem.\n        y, residuals, _, _ = np.linalg.lstsq(H_sub, target, rcond=None)\n        \n        # The residual norm ||r_k|| is the residual of the LS problem.\n        # np.linalg.lstsq returns the sum of squared residuals, so take the square root.\n        res_norm = np.sqrt(residuals[0]) if residuals.size > 0 else np.linalg.norm(H_sub @ y - target)\n\n        # Check stopping criterion based on relative residual.\n        if res_norm / b_norm <= tol:\n            return k + 1 # GMRES has converged in k+1 iterations.\n        \n        # Check for Arnoldi breakdown. If h_{k+1,k} is numerically zero, the subspace\n        # is invariant. GMRES finds the exact solution. The residual check above\n        # inherently captures this, but this is a fail-safe.\n        if H[k + 1, k] < 1e-16:\n            return k + 1\n            \n        V.append(w / H[k + 1, k])\n        \n    return n # Maximum number of iterations reached.\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the algorithms, and print the results.\n    \"\"\"\n    TOL = 1e-12\n\n    test_cases = [\n        # Case 1: Identity matrix scaled, degree g=1\n        (\n            5.0 * np.identity(3, dtype=np.float64),\n            np.array([1.0, -2.0, 3.0], dtype=np.float64)\n        ),\n        # Case 2: Diagonalizable with repeated eigenvalue, g=2\n        (\n            np.diag(np.array([1.0, 2.0, 2.0], dtype=np.float64)),\n            np.array([1.0, 1.0, 1.0], dtype=np.float64)\n        ),\n        # Case 3: Defective Jordan block, g=3\n        (\n            np.array([[2.0, 1.0, 0.0], [0.0, 2.0, 1.0], [0.0, 0.0, 2.0]], dtype=np.float64),\n            np.array([1.0, 1.0, 1.0], dtype=np.float64)\n        ),\n        # Case 4: Block diagonal with selective excitation, g=2\n        (\n            np.array([\n                [3.0, 1.0, 0.0, 0.0], \n                [0.0, 3.0, 0.0, 0.0], \n                [0.0, 0.0, 1.0, 1.0], \n                [0.0, 0.0, 0.0, 1.0]\n            ], dtype=np.float64),\n            np.array([1.0, 1.0, 0.0, 0.0], dtype=np.float64)\n        )\n    ]\n\n    results = []\n    for A, b in test_cases:\n        g = find_min_poly_degree(A, b)\n        k_star = gmres_solver(A, b, TOL)\n        results.append(g == k_star)\n\n    # Convert list of booleans to the required string format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理论是理想的，但现实世界中的科学计算问题往往充满挑战，其中一个常见难题就是求解病态（或接近奇异）的线性系统。在这种情况下，标准的GMRES方法可能会收敛缓慢甚至失效。本练习将带你直面这一挑战，通过构建一个病态系统来观察GMRES的行为，并引入一种强大的技术——Tikhonov regularization——来改善其性能。通过对比正则化前后的效果，你将学习到如何在实际应用中诊断并处理这类棘手问题。",
            "id": "3136913",
            "problem": "你需要研究广义最小残差法（GMRES）在求解近奇异线性系统时的行为，并将其与通过正规方程构建的吉洪诺夫正则化替代方法进行比较。仅从核心定义开始：线性系统 $Ax = b$ 的残差概念 $r(x) = b - Ax$，克雷洛夫子空间 $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, \\dots, A^{k-1} r_0\\}$ 的定义，以及 GMRES 在 $x_0 + \\mathcal{K}_k(A, r_0)$ 中寻找具有最小残差范数的近似解 $x_k$ 的思想。不要假定任何预先推导出的 GMRES 公式；相反，应从阿诺尔迪过程推导出相关的最小二乘问题，并基于这些基底解释残差多项式的解释。\n\n通过固定正交因子并仅改变奇异值，构造一个可复现的近奇异矩阵 $A \\in \\mathbb{R}^{n \\times n}$。具体步骤如下：\n- 固定一个维度 $n \\in \\mathbb{N}$，其中 $n \\ge 2$。\n- 令 $U, V \\in \\mathbb{R}^{n \\times n}$ 为正交矩阵，它们通过对以固定随机种子生成的高斯随机矩阵应用 $\\mathrm{QR}$ 分解得到。具体来说，使用一个以种子 42 初始化的伪随机数生成器，生成两个具有独立同分布标准正态条目的矩阵，并从它们的 $\\mathrm{QR}$ 分解中获得正交因子 $U$ 和 $V$。\n- 给定参数 $\\varepsilon \\in (0, 1]$，定义一个对角矩阵 $\\Sigma(\\varepsilon) = \\operatorname{diag}(s_1, \\dots, s_n)$，其中 $s_i = \\varepsilon^{\\frac{i-1}{n-1}}$（对于 $i = 1, \\dots, n$），因此 $s_1 = 1$ 且 $s_n = \\varepsilon$。设置 $A(\\varepsilon) = U \\Sigma(\\varepsilon) V^\\top$。\n- 将 $b \\in \\mathbb{R}^{n}$ 定义为一个以固定随机种子 7 生成的标准正态随机向量。\n\n你的任务是：\n- 实现一个使用改进的格拉姆-施密特法的 $k$ 步阿诺尔迪过程，以构造 $\\mathcal{K}_k(A, r_0)$ 的一个标准正交基，其中 $r_0 = b - A x_0$ 且 $x_0 = 0$。\n- 根据阿诺尔迪关系式，构建相关的小型最小二乘问题，并计算 $k$ 步 GMRES 近似解 $x_k$，该解在 $x_0 + \\mathcal{K}_k(A, r_0)$ 上最小化 $\\|b - A x\\|_2$。\n- 报告在恰好 $k$ 步阿诺尔迪步骤后（或者如果发生真正的分解则提前终止）的最终残差范数 $\\|b - A x_k\\|_2$，记为 $R_{\\mathrm{GMRES}}$。\n- 通过正规方程实现吉洪诺夫正则化：对于给定的 $\\lambda \\ge 0$，构建 $B(\\lambda) = A^\\top A + \\lambda I$ 和 $c = A^\\top b$，并对系统 $B(\\lambda) x = c$ 运行相同的 $k$ 步 GMRES 过程以获得迭代解 $x_k^{(\\lambda)}$。为便于比较，评估原始残差范数 $R_{\\mathrm{Tik}} = \\|b - A x_k^{(\\lambda)}\\|_2$。\n- 从第一性原理出发，解释在 GMRES 中残差如何能表示为 $r_k = p_k(A) r_0$，其中 $p_k$ 是一个次数至多为 $k$ 且满足 $p_k(0) = 1$ 的多项式。并解释应用于正规方程的吉洪诺夫正则化如何将其修改为 $r_k^{\\mathrm{NE}} = q_k(A^\\top A + \\lambda I) r_0^{\\mathrm{NE}}$，其中 $r_0^{\\mathrm{NE}} = c - B(\\lambda) x_0$；然后将 $\\lambda$ 对谱的影响与对容许残差多项式的影响联系起来。\n\n使用以下固定的测试套件。对于所有情况，取 $n = 12$，$x_0 = 0$，按上述方式构造 $A(\\varepsilon)$ 和 $b$，并运行恰好 $k$ 步阿诺尔迪步骤，除非发生分解。\n\n测试套件参数 $(\\varepsilon, \\lambda, k)$:\n- 情况 1：$(\\varepsilon, \\lambda, k) = (10^{-3}, 0, 4)$。\n- 情况 2：$(\\varepsilon, \\lambda, k) = (10^{-3}, 10^{-4}, 4)$。\n- 情况 3：$(\\varepsilon, \\lambda, k) = (10^{-8}, 10^{-6}, 8)$。\n- 情况 4：$(\\varepsilon, \\lambda, k) = (10^{-8}, 10^{-2}, 8)$。\n- 情况 5：$(\\varepsilon, \\lambda, k) = (10^{-2}, 0, 2)$。\n\n对于每种情况，计算并返回一个包含两个浮点数的列表 $[R_{\\mathrm{GMRES}}, R_{\\mathrm{Tik}}]$，其中 $R_{\\mathrm{GMRES}}$ 是求解 $A x = b$ 的 $k$ 步 GMRES 迭代解的残差范数，而 $R_{\\mathrm{Tik}}$ 是将 $k$ 步 GMRES 迭代应用于吉洪诺夫正则化的正规方程后，得到的解代入原始问题 $A x = b$ 计算出的残差范数。\n\n要求的最终输出格式：你的程序应生成单行输出，其中包含一个 Python 风格的列表，按顺序包含这五个结果。每个结果本身都是一个 Python 风格的列表，内含两个使用科学记数法表示且小数点后恰好有六位数字的浮点数。例如，两个假设情况的输出可能如下所示：\n\"[[1.234567e-03,9.876543e-04],[...],...]\"\n确保你的程序只打印这一行。",
            "solution": "所提出的问题要求对广义最小残差（GMRES）方法在一个近奇异线性系统 $Ax=b$ 上的表现，与一个基于正规方程的吉洪诺夫正则化变体进行比较研究。我们将从第一性原理出发，推导这些方法的核心组成部分，并为其行为提供理论解释。\n\n**广义最小残差（GMRES）方法**\n\nGMRES 方法是一种迭代过程，旨在求解一般的非奇异线性方程组 $Ax = b$，其中矩阵 $A \\in \\mathbb{R}^{n \\times n}$，向量 $x, b \\in \\mathbb{R}^{n}$。从一个初始猜测 $x_0$ 开始，GMRES 构建一系列近似解。在第 $k$ 步，它找到一个迭代解 $x_k$，该解最小化残差的欧几里得范数，即 $\\|r_k\\|_2 = \\|b - Ax_k\\|_2$。对这个最优 $x_k$ 的搜索被限制在仿射克雷洛夫子空间 $x_0 + \\mathcal{K}_k(A, r_0)$ 内，其中 $r_0 = b - Ax_0$ 是初始残差，且\n$$\n\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, \\dots, A^{k-1} r_0\\}\n$$\n是由 $A$ 和 $r_0$ 生成的第 $k$ 个克雷洛夫子空间。任何候选解都可以表示为 $x_k = x_0 + z_k$，其中 $z_k \\in \\mathcal{K}_k(A, r_0)$。因此，GMRES 求解的最小化问题是：\n$$\n\\min_{z_k \\in \\mathcal{K}_k(A, r_0)} \\|b - A(x_0 + z_k)\\|_2 = \\min_{z_k \\in \\mathcal{K}_k(A, r_0)} \\|r_0 - Az_k\\|_2.\n$$\n\n为了解决这个问题，我们首先使用阿诺尔迪迭代为 $\\mathcal{K}_k(A, r_0)$ 构建一个标准正交基。按照要求，我们使用改进的格拉姆-施密特（MGS）变体以增强数值稳定性。我们首先将初始残差归一化，$q_1 = r_0 / \\|r_0\\|_2$。然后，算法迭代地生成一组标准正交向量 $\\{q_1, q_2, \\dots, q_k\\}$。第 $j$ 步（从 $j=1$ 到 $k$）的过程如下：\n1. 计算下一个克雷洛夫候选向量：$v_j = A q_j$。\n2. 将 $v_j$ 与已有的基向量 $\\{q_1, \\dots, q_j\\}$ 正交化。对于 $i = 1, \\dots, j$，计算投影系数 $h_{i,j} = q_i^\\top v_j$ 并减去该投影：$v_j \\leftarrow v_j - h_{i,j} q_i$。\n3. 计算所得向量的范数：$h_{j+1,j} = \\|v_j\\|_2$。\n4. 如果 $h_{j+1,j} = 0$，算法因“幸运分解”而终止，这表明在当前子空间内已找到精确解。\n5. 否则，进行归一化以获得下一个基向量：$q_{j+1} = v_j / h_{j+1,j}$。\n\n经过 $k$ 次成功步骤后，该过程产生阿诺尔迪关系式 $A Q_k = Q_{k+1} \\tilde{H}_k$。这里，$Q_k = [q_1, \\dots, q_k] \\in \\mathbb{R}^{n \\times k}$ 和 $Q_{k+1} = [q_1, \\dots, q_{k+1}] \\in \\mathbb{R}^{n \\times (k+1)}$ 是列向量标准正交的矩阵，而 $\\tilde{H}_k \\in \\mathbb{R}^{(k+1) \\times k}$ 是包含系数 $h_{i,j}$ 的上Hessenberg矩阵。\n\n一个向量 $z_k \\in \\mathcal{K}_k(A, r_0)$ 可以写为 $z_k = Q_k y_k$，其中 $y_k \\in \\mathbb{R}^k$ 是唯一的系数向量。将此代入最小化问题并使用阿诺尔迪关系式，我们得到：\n$$\n\\min_{y_k \\in \\mathbb{R}^k} \\|r_0 - A Q_k y_k\\|_2 = \\min_{y_k \\in \\mathbb{R}^k} \\|\\|r_0\\|_2 q_1 - Q_{k+1} \\tilde{H}_k y_k\\|_2.\n$$\n令 $\\beta = \\|r_0\\|_2$ 并注意到 $q_1 = Q_{k+1} e_1$（其中 $e_1 = [1, 0, \\dots, 0]^\\top \\in \\mathbb{R}^{k+1}$），目标函数变为 $\\min_{y_k \\in \\mathbb{R}^k} \\|Q_{k+1} (\\beta e_1 - \\tilde{H}_k y_k)\\|_2$。由于 $Q_{k+1}$ 是等距同构的，它保持欧几里得范数不变。这将原始的 $n$ 维问题简化为一个小的 $(k+1) \\times k$ 维的最小二乘问题：\n$$\ny_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\|\\beta e_1 - \\tilde{H}_k y\\|_2.\n$$\n该系统通常通过对 $\\tilde{H}_k$ 进行QR分解来求解 $y_k$。最终的 GMRES 迭代解为 $x_k = x_0 + Q_k y_k$。残差的范数 $R_{\\mathrm{GMRES}} = \\|b - A x_k\\|_2$ 等于该小型最小二乘问题的最小值。\n\n**GMRES 的多项式解释**\n\n由于 $x_k = x_0 + z_k$ 且 $z_k \\in \\mathcal{K}_k(A, r_0)$，我们可以将 $z_k$ 表示为一个作用于 $r_0$ 的 $A$ 的多项式：$z_k = \\psi_{k-1}(A) r_0$，其中 $\\psi_{k-1}$ 是一个次数至多为 $k-1$ 的多项式。残差 $r_k$ 则可以写成：\n$$\nr_k = r_0 - A z_k = r_0 - A \\psi_{k-1}(A) r_0 = (I - A \\psi_{k-1}(A)) r_0.\n$$\n定义一个新多项式 $p_k(z) = 1 - z \\psi_{k-1}(z)$，我们看到 $p_k$ 的次数至多为 $k$ 并满足约束 $p_k(0) = 1$。残差即为 $r_k = p_k(A) r_0$。因此，GMRES 执行的最小化等价于在所有次数至多为 $k$ 且满足 $p_k(0)=1$ 的多项式集合中，找到那个最小化 $\\|p_k(A) r_0\\|_2$ 范数的多项式 $p_k$。对于特征值分布广泛且接近零的矩阵（即病态矩阵），一个低次多项式很难在整个谱上都很小同时又满足 $p_k(0)=1$ 的条件，这导致收敛缓慢。\n\n**通过正规方程进行吉洪诺夫正则化**\n\n对于病态或近奇异矩阵，直接求解 $Ax=b$ 对误差高度敏感。吉洪诺夫正则化通过求解一个邻近的、更稳定的问题来解决此问题：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\left( \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2 \\right),\n$$\n其中 $\\lambda > 0$ 是一个正则化参数，用于权衡解对数据的保真度与解的稳定性。这个优化问题的解由一个称为正则化正规方程的线性系统给出：\n$$\n(A^\\top A + \\lambda I) x = A^\\top b.\n$$\n我们定义 $B(\\lambda) = A^\\top A + \\lambda I$ 和 $c = A^\\top b$，将问题转化为求解 $B(\\lambda) x = c$。矩阵 $B(\\lambda)$ 是对称的，并且对于 $\\lambda > 0$ 是正定的。如果 $A = U \\Sigma V^\\top$ 是 $A$ 的奇异值分解，其奇异值为 $\\sigma_i$，那么 $A^\\top A$ 的特征值为 $\\sigma_i^2$。因此，$B(\\lambda)$ 的特征值为 $\\sigma_i^2 + \\lambda$。对于一个近奇异的 $A$，其最小奇异值 $\\sigma_n$ 接近于 0。参数 $\\lambda$ 有效地将 $A^\\top A$ 的整个谱移动了 $\\lambda$，使得最小的特征值 $\\sigma_n^2+\\lambda$ 远离零。这极大地改善了系统矩阵的条件数，其变为 $\\kappa_2(B(\\lambda)) = (\\sigma_{\\max}^2 + \\lambda) / (\\sigma_{\\min}^2 + \\lambda)$。\n\n我们将 GMRES 应用于这个条件良好的系统 $B(\\lambda)x = c$。使用初始猜测 $x_0 = 0$，这个新系统的初始残差为 $r_0^{\\mathrm{NE}} = c - B(\\lambda) x_0 = c$。GMRES 在克雷洛夫子空间 $\\mathcal{K}_k(B(\\lambda), c)$ 中找到一个迭代解 $x_k^{(\\lambda)}$，以最小化 $\\|c - B(\\lambda) x_k^{(\\lambda)}\\|_2$。此过程的残差为 $r_k^{\\mathrm{NE}} = q_k(B(\\lambda)) r_0^{\\mathrm{NE}}$，其中 $q_k$ 是一个次数至多为 $k$ 且满足 $q_k(0)=1$ 的多项式。由于 $B(\\lambda)$ 的谱更聚集且远离零，GMRES 收敛迅速。得到的迭代解 $x_k^{(\\lambda)}$ 是正则化问题的一个近似解。为了评估其相对于原始问题的质量，我们必须计算原始残差范数 $R_{\\mathrm{Tik}} = \\|b - A x_k^{(\\lambda)}\\|_2$。",
            "answer": "```python\nimport numpy as np\n\ndef arnoldi_mgs(A, v, k):\n    \"\"\"\n    Performs k steps of the Arnoldi iteration with Modified Gram-Schmidt.\n    \n    Args:\n        A (np.ndarray): The matrix (n x n).\n        v (np.ndarray): The starting vector (n,).\n        k (int): The number of iterations.\n        \n    Returns:\n        Q (np.ndarray): Orthonormal basis vectors (n x (m+1)).\n        H (np.ndarray): Upper Hessenberg matrix ((m+1) x m).\n        m (int): The actual number of iterations performed before breakdown.\n    \"\"\"\n    n = A.shape[0]\n    Q = np.zeros((n, k + 1), dtype=np.float64)\n    H = np.zeros((k + 1, k), dtype=np.float64)\n    \n    norm_v = np.linalg.norm(v)\n    if norm_v == 0:\n        return Q[:, :1], H[:1, :0], 0\n        \n    Q[:, 0] = v / norm_v\n    \n    for j in range(k):\n        w = A @ Q[:, j]\n        \n        # Modified Gram-Schmidt\n        for i in range(j + 1):\n            H[i, j] = np.dot(Q[:, i].T, w)\n            w = w - H[i, j] * Q[:, i]\n            \n        H[j + 1, j] = np.linalg.norm(w)\n        \n        # Breakdown condition\n        if H[j + 1, j] < 1e-12:\n            m = j + 1\n            return Q[:, :m+1], H[:m+1, :m], m\n            \n        Q[:, j + 1] = w / H[j + 1, j]\n        \n    return Q, H, k\n\ndef gmres_k_steps(A, b, k, x0):\n    \"\"\"\n    Runs up to k steps of GMRES.\n    \n    Args:\n        A (np.ndarray): The matrix.\n        b (np.ndarray): The right-hand side vector.\n        k (int): The maximum number of iterations.\n        x0 (np.ndarray): The initial guess.\n        \n    Returns:\n        xk (np.ndarray): The approximate solution.\n    \"\"\"\n    r0 = b - A @ x0\n    beta = np.linalg.norm(r0)\n    \n    if beta == 0:\n        return x0\n    \n    Q, H, actual_k = arnoldi_mgs(A, r0, k)\n    \n    if actual_k == 0:\n        return x0\n        \n    e1 = np.zeros(actual_k + 1)\n    e1[0] = 1.0\n    rhs = beta * e1\n    \n    y, _, _, _ = np.linalg.lstsq(H, rhs, rcond=None)\n    \n    Qk = Q[:, :actual_k]\n    xk = x0 + Qk @ y\n        \n    return xk\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print results.\n    \"\"\"\n    # Define problem constants\n    n = 12\n    seed_A = 42\n    seed_b = 7\n    x0 = np.zeros(n)\n\n    # Generate fixed U and V matrices\n    rng_A = np.random.default_rng(seed_A)\n    Z1 = rng_A.standard_normal((n, n))\n    U, _ = np.linalg.qr(Z1)\n    Z2 = rng_A.standard_normal((n, n))\n    V, _ = np.linalg.qr(Z2)\n\n    # Generate fixed b vector\n    rng_b = np.random.default_rng(seed_b)\n    b = rng_b.standard_normal(n)\n\n    test_cases = [\n        (1e-3, 0.0, 4),      # Case 1\n        (1e-3, 1e-4, 4),     # Case 2\n        (1e-8, 1e-6, 8),     # Case 3\n        (1e-8, 1e-2, 8),     # Case 4\n        (1e-2, 0.0, 2)       # Case 5\n    ]\n\n    results = []\n\n    for epsilon, lambda_reg, k in test_cases:\n        # Construct A(epsilon)\n        s = np.array([epsilon**(i / (n - 1)) for i in range(n)])\n        Sigma = np.diag(s)\n        A = U @ Sigma @ V.T\n\n        # --- GMRES on original system ---\n        xk_gmres = gmres_k_steps(A, b, k, x0)\n        R_gmres = np.linalg.norm(b - A @ xk_gmres)\n\n        # --- Tikhonov Regularization with GMRES ---\n        if lambda_reg == 0.0:\n            # Special case for Normal Equations without regularization\n            B = A.T @ A\n        else:\n            B = A.T @ A + lambda_reg * np.eye(n)\n        \n        c = A.T @ b\n        \n        xk_tik = gmres_k_steps(B, c, k, x0)\n        \n        R_tik = np.linalg.norm(b - A @ xk_tik)\n\n        results.append([R_gmres, R_tik])\n\n    # Format the output as specified\n    list_of_strings = [f\"[{res[0]:.6e},{res[1]:.6e}]\" for res in results]\n    output_str = f\"[{','.join(list_of_strings)}]\"\n    \n    print(output_str)\n\nsolve()\n```"
        }
    ]
}