{
    "hands_on_practices": [
        {
            "introduction": "When choosing a preconditioner, it is critical to understand the foundational requirements of each method. Incomplete Cholesky (IC) factorization is powerful for symmetric positive definite systems, but what happens when a matrix is even slightly non-symmetric? This exercise  challenges you to analyze the strict requirements of IC factorization versus the more general Incomplete LU (ILU) factorization, revealing how a loss of symmetry immediately impacts their applicability.",
            "id": "3143579",
            "problem": "Consider the $3 \\times 3$ sparse matrix family\n$$\nA(\\alpha)=\\begin{bmatrix}\n2  -1+\\alpha  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{bmatrix},\n$$\nwhich is obtained by perturbing the symmetric tridiagonal matrix associated with the one-dimensional Poisson operator by a small asymmetric term controlled by the real parameter $\\alpha$. Suppose you wish to build a preconditioner with either the Incomplete Lower-Upper (LU) factorization (ILU) with no fill, denoted ILU$(0)$, or the Incomplete Cholesky (IC) factorization with no fill, denoted IC$(0)$. Assume no pivoting is used, and the sparsity pattern of the factors is restricted to that of $A(\\alpha)$.\n\nFrom fundamental properties of triangular factorizations, analyze the feasibility of these two incomplete factorizations for $A(\\alpha)$ and determine how asymmetry affects their viability. Which of the following statements is correct?\n\nA. Because Cholesky requires exact symmetry, any nonzero asymmetry ($\\alpha \\neq 0$) makes Incomplete Cholesky (IC) inapplicable to $A(\\alpha)$; Incomplete Lower-Upper (LU) factorization (ILU) without pivoting succeeds for all $\\alpha$ with $3+\\alpha \\neq 0$ and $\\alpha \\neq -2$, so small asymmetry does not break ILU$(0)$.\n\nB. Incomplete Cholesky (IC) remains applicable for all $\\alpha$ with $|\\alpha|1$ because the antisymmetric perturbation is small, while Incomplete Lower-Upper (LU) factorization (ILU) fails for any $\\alpha \\neq 0$ due to loss of diagonal dominance.\n\nC. Both Incomplete Cholesky (IC) and Incomplete Lower-Upper (LU) factorization (ILU) succeed for any real $\\alpha$ because $A(\\alpha)$ is strictly diagonally dominant for all $\\alpha$.\n\nD. Incomplete Cholesky (IC) fails only when $\\alpha-1$ because then $A(\\alpha)$ ceases to be positive definite; Incomplete Lower-Upper (LU) factorization (ILU) fails only when $\\alpha1$ due to growth in the antisymmetric part.",
            "solution": "### Step 1: Analyze the Incomplete Cholesky (IC(0)) Factorization\n\nThe standard Cholesky factorization, $A = LL^T$ where $L$ is a lower triangular matrix, is defined only for matrices that are symmetric and positive definite. The given matrix is:\n$$\nA(\\alpha)=\\begin{bmatrix}\n2  -1+\\alpha  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{bmatrix}\n$$\nFor this matrix to be symmetric, the condition $A_{ij} = A_{ji}$ must hold for all $i, j$. Comparing the entries $A_{12}$ and $A_{21}$, we must have:\n$$\n-1 + \\alpha = -1 \\implies \\alpha = 0\n$$\nIf $\\alpha \\neq 0$, the matrix $A(\\alpha)$ is non-symmetric. Consequently, the standard Cholesky factorization and its incomplete variant, IC(0), are not applicable. While some numerical libraries might proceed by factoring the symmetric part of the matrix, the method is, by definition, not well-defined for a non-symmetric matrix.\n\n### Step 2: Analyze the Incomplete LU (ILU(0)) Factorization\n\nThe ILU(0) factorization without pivoting can be performed on general non-symmetric matrices. The process fails (breaks down) if a zero pivot is encountered during the elimination procedure. A pivot is a diagonal entry of the upper triangular factor $U$. Let's compute the pivots for $A(\\alpha)$ assuming a unit lower triangular factor $L$.\n\n1.  **First Pivot:** The first row of $U$ is the same as the first row of $A(\\alpha)$. So, the first pivot is $u_{11} = A_{11} = 2$. This is non-zero.\n\n2.  **Second Pivot:** The first step of elimination modifies the entry $A_{22}$. The multiplier is $l_{21} = A_{21}/u_{11} = -1/2$. The new diagonal entry, which becomes the second pivot, is:\n    $$\n    u_{22} = A_{22} - l_{21} A_{12} = 2 - \\left(-\\frac{1}{2}\\right)(-1+\\alpha) = 2 - \\frac{1-\\alpha}{2} = \\frac{4 - (1-\\alpha)}{2} = \\frac{3+\\alpha}{2}\n    $$\n    Breakdown occurs if $u_{22} = 0$, which happens when $3+\\alpha = 0$, or $\\alpha = -3$.\n\n3.  **Third Pivot:** The next step of elimination modifies $A_{33}$. Since $A_{31}=0$, the first elimination step does not affect row 3. The multiplier for the second step is $l_{32} = A_{32}/u_{22} = -1 / \\left(\\frac{3+\\alpha}{2}\\right) = -\\frac{2}{3+\\alpha}$. The third pivot is:\n    $$\n    u_{33} = A_{33} - l_{32} A_{23} = 2 - \\left(-\\frac{2}{3+\\alpha}\\right)(-1) = 2 - \\frac{2}{3+\\alpha} = \\frac{2(3+\\alpha) - 2}{3+\\alpha} = \\frac{6+2\\alpha-2}{3+\\alpha} = \\frac{4+2\\alpha}{3+\\alpha}\n    $$\n    Breakdown occurs if $u_{33} = 0$, which happens when $4+2\\alpha = 0$, or $\\alpha = -2$.\n\nIn summary, the ILU(0) factorization fails if $\\alpha = -3$ or $\\alpha = -2$. For other values of $\\alpha$, including small non-zero values, the factorization succeeds.\n\n### Step 3: Evaluate the Options\n\n-   **A:** States that IC is inapplicable for $\\alpha \\neq 0$ and ILU succeeds for all $\\alpha$ such that $3+\\alpha \\neq 0$ (i.e., $\\alpha \\neq -3$) and $\\alpha \\neq -2$. This matches our analysis.\n-   **B:** Incorrect. IC is inapplicable for $\\alpha \\neq 0$. ILU does not fail for any $\\alpha \\neq 0$.\n-   **C:** Incorrect. IC is inapplicable, and ILU has breakdown points. The matrix is also not strictly diagonally dominant.\n-   **D:** Incorrect. The conditions for failure are wrong.\n\nTherefore, option A provides the correct analysis.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Building on the knowledge that ILU factorization is applicable to a wider class of matrices, we now ask a more nuanced question: when is it actually the *better* choice? This practice  presents a hypothetical case of a nearly symmetric matrix, where intuition might suggest that an IC-based method would be effective. By comparing the preconditioning quality of ILU(0) applied to the non-symmetric matrix versus IC(0) applied to its symmetric part, you will uncover a key principle about how each method captures the underlying operator.",
            "id": "3143610",
            "problem": "Consider the $n \\times n$ tridiagonal matrix $T \\in \\mathbb{R}^{4 \\times 4}$ with entries $T_{ii} = 2$ for $i \\in \\{1,2,3,4\\}$, $T_{i,i+1} = -1$ for $i \\in \\{1,2,3\\}$, and $T_{i+1,i} = -1$ for $i \\in \\{1,2,3\\}$. Let $E \\in \\mathbb{R}^{4 \\times 4}$ be the skew-symmetric perturbation defined by $E_{1,2} = 1$, $E_{2,1} = -1$, and $E_{ij} = 0$ otherwise. For a small parameter $\\varepsilon  0$, define the nearly symmetric positive definite matrix\n$$\nA = T + \\varepsilon E,\n$$\nso that $A$ is nonsymmetric but has symmetric part $S = \\frac{1}{2}(A + A^\\top) = T$, which is symmetric positive definite by construction.\n\nZero-fill incomplete LU factorization (ILU(0)) constructs lower and upper triangular matrices $L$ and $U$ whose sparsity patterns match those of the strictly lower and strictly upper triangular parts of $A$, and seeks $L U \\approx A$ without introducing any fill-in. Zero-fill incomplete Cholesky (IC(0)) constructs a lower triangular matrix $L$ whose sparsity pattern matches that of the strictly lower triangular part of a symmetric matrix and seeks $L L^\\top \\approx$ that symmetric matrix, which in this context must be taken as the symmetric part $S$ of $A$ for the factorization to be well-defined.\n\nStarting from the foundational facts that (i) symmetric positive definiteness means $x^\\top S x  0$ for all nonzero $x \\in \\mathbb{R}^4$ and $S = S^\\top$, (ii) exact LU factorization of a tridiagonal matrix without pivoting introduces no fill beyond the tridiagonal pattern provided the pivots remain nonzero, and (iii) Cholesky factorization of a symmetric positive definite tridiagonal matrix introduces no fill beyond the tridiagonal pattern, analyze the preconditioning quality of ILU(0) applied to $A$ versus IC(0) applied to $S$ for small $\\varepsilon$. In particular, reason about the preconditioned operators $M_{\\text{ILU}}^{-1} A$ and $M_{\\text{IC}}^{-1} A$, where $M_{\\text{ILU}} = L U$ is the ILU(0) preconditioner for $A$, and $M_{\\text{IC}} = L L^\\top$ is the IC(0) preconditioner for $S$, and how their spectra and deviation from the identity relate to iterative solver convergence.\n\nWhich option correctly identifies a concrete mechanism by which ILU(0) yields strictly better preconditioning than IC(0) in this constructed case?\n\nA. Because $A$ is tridiagonal, ILU(0) with natural ordering reproduces the exact LU factors of $A$ without any fill, so $M_{\\text{ILU}}^{-1} A = I$; in contrast, IC(0) must be applied to the symmetric part $S$, giving $M_{\\text{IC}} = S$, so $M_{\\text{IC}}^{-1} A = S^{-1} A = I + \\varepsilon S^{-1} E \\neq I$, with a skew-symmetric $\\mathcal{O}(\\varepsilon)$ deviation that degrades spectral clustering and convergence.\n\nB. IC(0) always reproduces the exact factorization for any tridiagonal matrix (symmetric or not), so $M_{\\text{IC}}^{-1} A = I$, and ILU(0) offers no advantage here.\n\nC. IC(0) is guaranteed to be superior to ILU(0) whenever the matrix is diagonally dominant, so in this case IC(0) strictly outperforms ILU(0).\n\nD. ILU(0) introduces additional fill beyond the original sparsity, making its approximation more accurate than IC(0); this extra fill is the reason ILU(0) preconditions better in this case.",
            "solution": "### Step 1: Extract Givens\n\n-   $T \\in \\mathbb{R}^{4 \\times 4}$ is a tridiagonal matrix.\n-   $T_{ii} = 2$ for $i \\in \\{1,2,3,4\\}$.\n-   $T_{i,i+1} = -1$ for $i \\in \\{1,2,3\\}$.\n-   $T_{i+1,i} = -1$ for $i \\in \\{1,2,3\\}$.\n-   $E \\in \\mathbb{R}^{4 \\times 4}$ is a skew-symmetric perturbation.\n-   $E_{1,2} = 1$, $E_{2,1} = -1$.\n-   $E_{ij} = 0$ otherwise.\n-   $\\varepsilon  0$ is a small parameter.\n-   $A = T + \\varepsilon E$.\n-   Symmetric part of $A$ is $S = \\frac{1}{2}(A + A^\\top) = T$.\n-   $S$ is stated to be symmetric positive definite.\n-   ILU($0$) constructs factors $L, U$ with the same sparsity pattern as the strictly lower/upper parts of $A$, such that $M_{\\text{ILU}} = LU \\approx A$.\n-   IC($0$) constructs a factor $L$ with the same sparsity pattern as the strictly lower part of a symmetric matrix $S$, such that $M_{\\text{IC}} = LL^\\top \\approx S$.\n-   Foundational fact (i): Symmetric positive definiteness (SPD) means $x^\\top S x  0$ for all nonzero $x \\in \\mathbb{R}^4$ and $S = S^\\top$.\n-   Foundational fact (ii): Exact LU factorization of a tridiagonal matrix without pivoting introduces no fill-in (elements that were zero becoming non-zero), provided pivots are non-zero.\n-   Foundational fact (iii): Cholesky factorization of an SPD tridiagonal matrix introduces no fill-in.\n-   The question asks for the mechanism by which ILU($0$) provides strictly better preconditioning than IC($0$) for the matrix $A$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is scientifically grounded within the field of numerical linear algebra. All terms like `tridiagonal matrix`, `skew-symmetric`, `symmetric positive definite`, `ILU(0)`, `IC(0)`, `preconditioning`, and `spectral clustering` are well-defined and standard. The construction of the matrices $T$, $E$, and $A$ is mathematically sound. The matrix $T$ is a classic example of an SPD matrix (a scaled discrete one-dimensional Laplacian). The foundational facts provided are correct statements about matrix factorizations. The problem is well-posed, objective, and self-contained, providing sufficient information to derive a unique solution by analyzing the specified preconditioning methods. The setup is a canonical example used to illustrate the behavior of preconditioners on nearly symmetric systems.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. I will proceed with the derivation and analysis.\n\n### Derivation\n\nThe matrix $T$ is given by:\n$$\nT = \\begin{pmatrix} 2  -1  0  0 \\\\ -1  2  -1  0 \\\\ 0  -1  2  -1 \\\\ 0  0  -1  2 \\end{pmatrix}\n$$\nThe perturbation matrix $E$ is:\n$$\nE = \\begin{pmatrix} 0  1  0  0 \\\\ -1  0  0  0 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix}\n$$\nThe matrix $A$ is $A = T + \\varepsilon E$:\n$$\nA = \\begin{pmatrix} 2  -1+\\varepsilon  0  0 \\\\ -1-\\varepsilon  2  -1  0 \\\\ 0  -1  2  -1 \\\\ 0  0  -1  2 \\end{pmatrix}\n$$\nThe symmetric part of $A$ is correctly identified as $S = T$, since $A^\\top = T^\\top + \\varepsilon E^\\top = T - \\varepsilon E$, and thus $S = \\frac{1}{2}(A+A^\\top) = \\frac{1}{2}((T+\\varepsilon E) + (T-\\varepsilon E)) = T$.\n\n**Analysis of the ILU(0) Preconditioner for $A$**\n\nThe ILU($0$) preconditioner, $M_{\\text{ILU}}$, is constructed by performing a LU factorization of $A$ and discarding any \"fill-in\"—that is, any new non-zero entries in the factors $L$ and $U$ at positions $(i,j)$ where $A_{ij}$ was zero.\n\nThe matrix $A$ is a tridiagonal matrix. As stated in foundational fact (ii), the exact LU factorization of a tridiagonal matrix does not produce any fill-in, provided the pivots (the diagonal elements of the upper triangular factor $U$) are non-zero. For this specific matrix $A$, which is a small perturbation of the SPD matrix $T$, the pivots will be positive and thus non-zero. For instance, the first pivot is $A_{11}=2$. After the first step of Gaussian elimination, the new $(2,2)$ entry is $A_{22} - \\frac{A_{21}A_{12}}{A_{11}} = 2 - \\frac{(-1-\\varepsilon)(-1+\\varepsilon)}{2} = 2 - \\frac{1-\\varepsilon^2}{2} = \\frac{3+\\varepsilon^2}{2}  0$. This pattern continues, and all pivots remain positive.\n\nSince the exact LU factorization of $A$ generates no fill-in, the ILU($0$) algorithm, which is designed to discard fill-in, will produce the exact factors of $A$.\nTherefore, the ILU($0$) preconditioner is identical to the matrix $A$ itself:\n$$\nM_{\\text{ILU}} = LU = A\n$$\nThe preconditioned operator is then:\n$$\nM_{\\text{ILU}}^{-1} A = A^{-1} A = I\n$$\nwhere $I$ is the $4 \\times 4$ identity matrix. The spectrum of this preconditioned operator consists of a single eigenvalue, $1$, with multiplicity $4$. This represents perfect preconditioning, as an iterative solver like GMRES would converge in a single iteration (in exact arithmetic).\n\n**Analysis of the IC(0) Preconditioner for $A$**\n\nThe incomplete Cholesky factorization is defined for symmetric matrices. As specified, it must be applied to the symmetric part of $A$, which is $S=T$. The IC($0$) preconditioner, $M_{\\text{IC}}$, is constructed by performing a Cholesky factorization of $S$ and discarding any fill-in.\n\nThe matrix $S=T$ is a symmetric positive definite tridiagonal matrix. As stated in foundational fact (iii), the exact Cholesky factorization of such a matrix produces no fill-in. The Cholesky factor $L$ is a lower bidiagonal matrix.\nTherefore, the IC($0$) algorithm computes the exact Cholesky factorization of $S$.\nThis means the IC($0$) preconditioner is identical to the matrix $S$ itself:\n$$\nM_{\\text{IC}} = LL^\\top = S = T\n$$\nWhen this preconditioner is applied to the original non-symmetric matrix $A$, the preconditioned operator is:\n$$\nM_{\\text{IC}}^{-1} A = S^{-1} A = S^{-1}(S + \\varepsilon E) = I + \\varepsilon S^{-1} E\n$$\nThis preconditioned operator is not the identity matrix. It deviates from $I$ by the term $\\varepsilon S^{-1} E$, which is of order $\\mathcal{O}(\\varepsilon)$. The eigenvalues of this operator are not all equal to $1$. The eigenvalues of $S^{-1}E$ are known to be purely imaginary. Thus, the eigenvalues of $M_{\\text{IC}}^{-1} A$ are of the form $1 + i\\beta_j$ for some real numbers $\\beta_j$ of order $\\mathcal{O}(\\varepsilon)$. The spectrum is clustered around $1$, but spread along the vertical line $\\text{Re}(z)=1$ in the complex plane. This is a less favorable spectral distribution for iterative solvers compared to a single eigenvalue at $1$.\n\n**Comparison and Conclusion**\n\nThe ILU($0$) preconditioner results in the operator $M_{\\text{ILU}}^{-1} A = I$, which is ideal. The IC($0$) preconditioner results in the operator $M_{\\text{IC}}^{-1} A = I + \\varepsilon S^{-1} E \\neq I$. Consequently, an iterative method applied to the ILU($0$)-preconditioned system will converge much faster (ideally, in one step) than for the IC($0$)-preconditioned system. Thus, ILU($0$) yields strictly better preconditioning in this case. The mechanism is the fact that ILU($0$) exactly factorizes the tridiagonal matrix $A$, while IC($0$) exactly factorizes only its symmetric part $S$.\n\n### Option-by-Option Analysis\n\n**A. Because $A$ is tridiagonal, ILU(0) with natural ordering reproduces the exact LU factors of $A$ without any fill, so $M_{\\text{ILU}}^{-1} A = I$; in contrast, IC(0) must be applied to the symmetric part $S$, giving $M_{\\text{IC}} = S$, so $M_{\\text{IC}}^{-1} A = S^{-1} A = I + \\varepsilon S^{-1} E \\neq I$, with a skew-symmetric $\\mathcal{O}(\\varepsilon)$ deviation that degrades spectral clustering and convergence.**\n\nThis option correctly identifies the core mechanism. Its assertion that ILU($0$) reproduces the exact LU factors of the tridiagonal matrix $A$ is correct, leading to the conclusion that $M_{\\text{ILU}}^{-1} A = I$. Its assertion that IC($0$) must be applied to $S$ and that for the tridiagonal matrix $S=T$, this gives $M_{\\text{IC}}=S$ is also correct. The resulting preconditioned operator $M_{\\text{IC}}^{-1} A = I + \\varepsilon S^{-1} E \\neq I$ is also correctly derived. The final conclusion that this deviation degrades spectral clustering and convergence compared to the perfect clustering of ILU($0$) is sound. The description of the deviation term $\\varepsilon S^{-1} E$ as \"skew-symmetric\" is technically inaccurate, as $S^{-1}$ and $E$ do not commute in general. However, this minor point does not invalidate the overwhelmingly correct description of the primary mechanism at play. Among the given choices, this option provides the most accurate and complete explanation.\n**Verdict: Correct**\n\n**B. IC(0) always reproduces the exact factorization for any tridiagonal matrix (symmetric or not), so $M_{\\text{IC}}^{-1} A = I$, and ILU(0) offers no advantage here.**\n\nThis statement is false. The Cholesky factorization, and thus IC($0$), is only defined for symmetric matrices. It cannot be applied to the non-symmetric matrix $A$. The problem statement correctly directs its application to the symmetric part $S$. The premise of the option is fundamentally incorrect.\n**Verdict: Incorrect**\n\n**C. IC(0) is guaranteed to be superior to ILU(0) whenever the matrix is diagonally dominant, so in this case IC(0) strictly outperforms ILU(0).**\n\nThis option makes a false general claim. There is no theorem guaranteeing IC($0$) is always superior for diagonally dominant matrices, particularly when the matrix is non-symmetric. Furthermore, the matrix $A$ is not strictly diagonally dominant, as for row $2$, the diagonal entry is $|A_{22}|=2$ and the sum of the absolute values of off-diagonal entries is $|A_{21}|+|A_{23}| = |-1-\\varepsilon| + |-1| = 1+\\varepsilon+1 = 2+\\varepsilon$, and $2 \\not 2+\\varepsilon$. Most importantly, our direct analysis has already proven that ILU($0$) is perfect in this case, while IC($0$) is not. Therefore, ILU($0$) outperforms IC($0$).\n**Verdict: Incorrect**\n\n**D. ILU(0) introduces additional fill beyond the original sparsity, making its approximation more accurate than IC(0); this extra fill is the reason ILU(0) preconditions better in this case.**\n\nThis statement directly contradicts the definition of ILU($0$). The \"($0$)\" denotes zero-level fill, meaning no new non-zero entries are allowed in the factors. The very premise of the option is false. The reason for the superior performance of ILU($0$) in this specific case is that the exact factorization of the tridiagonal matrix $A$ requires no fill-in, not because ILU($0$) introduces it.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Moving from theory to practice, we must confront the numerical realities of implementing factorization algorithms. The ILU process can be unstable and fail entirely if it encounters a zero or near-zero pivot—a phenomenon known as breakdown. This hands-on coding problem  guides you through implementing the ILU(0) algorithm and empirically exploring a common stabilization technique: diagonal perturbation. By determining the smallest perturbation needed to prevent breakdown, you will gain practical insight into making ILU factorizations more robust.",
            "id": "3143608",
            "problem": "You must write a complete, runnable program that empirically determines how a small diagonal perturbation stabilizes an incomplete lower-upper factorization of order zero (incomplete lower-upper factorization (ILU(0))) without pivoting, by scanning a prescribed set of perturbation values and reporting the smallest perturbation that prevents breakdown. Use only the mathematical reasoning and algorithmic constraints stated here. All computations are over the real numbers.\n\nFundamental base and core definitions:\n- Start from the standard Gaussian elimination framework: for a square matrix $A \\in \\mathbb{R}^{n \\times n}$, an ideal lower-upper factorization (without pivoting) seeks lower-triangular $L$ with unit diagonal and upper-triangular $U$ such that $A = L U$. The incomplete lower-upper factorization of order zero (ILU(0)) applies the same elimination logic but enforces that the nonzero pattern of $L$ and $U$ does not introduce any additional fill entries beyond the pattern of $A$ (with the diagonal always included). Specifically, if $P \\subset \\{(i,j) : 1 \\le i,j \\le n\\}$ denotes the set of structural nonzeros of $A$ (including all diagonal positions $(i,i)$), then ILU(0) computes $L$ and $U$ such that $L$ is unit lower-triangular, $U$ is upper-triangular, and for all $(i,j) \\notin P$, the corresponding entries in $L$ (for $ij$) and in $U$ (for $i \\le j$) are set to zero and never created by the algorithm.\n- Define a diagonal perturbation by $A_{\\epsilon} = A + \\epsilon I$, where $I$ is the identity matrix of size $n$ and $\\epsilon \\in \\mathbb{R}$ is a nonnegative scalar.\n- Define breakdown as the occurrence during the ILU(0) process of a pivot $|U_{jj}|$ that is strictly less than a given positive tolerance $\\tau$ at the moment it is required in a division, i.e., a division by $U_{jj}$ is needed with $|U_{jj}|  \\tau$, or equivalently if after completing the updates of row $i$ the computed diagonal $U_{ii}$ satisfies $|U_{ii}|  \\tau$. If this happens at any step, the ILU(0) factorization is deemed to have broken down for that $\\epsilon$.\n\nTask:\n- For each test case below, you are given a concrete matrix $A$, a finite set $S$ of candidate perturbations $\\epsilon$, and a tolerance $\\tau  0$.\n- For each $\\epsilon \\in S$, attempt to perform ILU(0) on $A_{\\epsilon}$ without pivoting and with zero-fill enforcement as defined above. If a pivot $|U_{jj}|$ is found with $|U_{jj}|  \\tau$ at any step, declare breakdown for that $\\epsilon$.\n- Determine, for each test case, the smallest $\\epsilon \\in S$ (in the usual order on $\\mathbb{R}$) for which the ILU(0) process completes without breakdown. If no $\\epsilon \\in S$ avoids breakdown, report a not-a-number float.\n\nAlgorithmic constraints to enforce ILU(0):\n- Let $P$ be the structural nonzero pattern of $A$ with all diagonal entries $(i,i)$ included, even if $A_{ii} = 0$.\n- Construct $L$ and $U$ as follows, processing rows in increasing order $i = 1, 2, \\dots, n$:\n  - For each $j$ with $1 \\le j  i$ and $(i,j) \\in P$, compute\n    $$\n    L_{ij} = \\frac{A_{\\epsilon,ij} - \\sum_{k=1}^{j-1} L_{ik} \\, U_{kj}}{U_{jj}},\n    $$\n    but only include products $L_{ik} \\, U_{kj}$ in the sum if $(i,k) \\in P$ and $(k,j) \\in P$; if $|U_{jj}|  \\tau$ at this point, declare breakdown.\n  - For each $j$ with $i \\le j \\le n$ and $(i,j) \\in P$, compute\n    $$\n    U_{ij} = A_{\\epsilon,ij} - \\sum_{k=1}^{i-1} L_{ik} \\, U_{kj},\n    $$\n    but only include products $L_{ik} \\, U_{kj}$ in the sum if $(i,k) \\in P$ and $(k,j) \\in P$.\n  - After finishing row $i$, if $|U_{ii}|  \\tau$, declare breakdown.\n- For all $(i,j) \\notin P$, enforce $L_{ij} = 0$ for $ij$ and $U_{ij} = 0$ for $i \\le j$; set $L_{ii} = 1$ for all $i$.\n\nTest suite:\nFor each test, use the given $A$, candidate set $S$, and tolerance $\\tau$, in the order listed. The matrices are written explicitly.\n\n1. Test $1$ (near-zero main diagonal tridiagonal):\n   - Size $n = 5$.\n   - Matrix\n     $$\n     A = \\begin{bmatrix}\n     0  1  0  0  0 \\\\\n     1  0  1  0  0 \\\\\n     0  1  0  1  0 \\\\\n     0  0  1  0  1 \\\\\n     0  0  0  1  0\n     \\end{bmatrix}.\n     $$\n   - Candidate set $S = \\{\\, 0,\\; 10^{-16},\\; 10^{-12},\\; 10^{-10},\\; 10^{-8} \\,\\}$.\n   - Tolerance $\\tau = 10^{-12}$.\n\n2. Test $2$ (pivot created small at second step):\n   - Size $n = 3$.\n   - Matrix\n     $$\n     A = \\begin{bmatrix}\n     1  1  0 \\\\\n     1  1  1 \\\\\n     0  1  1\n     \\end{bmatrix}.\n     $$\n   - Candidate set $S = \\{\\, 0,\\; 10^{-16},\\; 5 \\cdot 10^{-13},\\; 10^{-12},\\; 10^{-9} \\,\\}$.\n   - Tolerance $\\tau = 10^{-12}$.\n\n3. Test $3$ (strictly diagonally dominant symmetric positive definite case):\n   - Size $n = 5$.\n   - Matrix\n     $$\n     A = \\begin{bmatrix}\n     2  -1  0  0  0 \\\\\n     -1  2  -1  0  0 \\\\\n     0  -1  2  -1  0 \\\\\n     0  0  -1  2  -1 \\\\\n     0  0  0  -1  2\n     \\end{bmatrix}.\n     $$\n   - Candidate set $S = \\{\\, 0,\\; 10^{-16},\\; 10^{-12} \\,\\}$.\n   - Tolerance $\\tau = 10^{-12}$.\n\n4. Test $4$ (isolated tiny diagonal entry):\n   - Size $n = 3$.\n   - Matrix\n     $$\n     A = \\begin{bmatrix}\n     10^{-16}  0  0 \\\\\n     0  1  0 \\\\\n     0  0  1\n     \\end{bmatrix}.\n     $$\n   - Candidate set $S = \\{\\, 0,\\; 10^{-16},\\; 5 \\cdot 10^{-13},\\; 10^{-12},\\; 10^{-9} \\,\\}$.\n   - Tolerance $\\tau = 10^{-12}$.\n\nRequired output format:\n- Your program should produce a single line of output containing the minimal stabilizing perturbations found for Tests $1$ through $4$, in order, as a comma-separated list enclosed in square brackets. If no $\\epsilon \\in S$ avoids breakdown, output a not-a-number floating-point value in that position.\n- Example format (not the actual answer): $[0.0,1e-12,{\\dots}]$.\n\nAngles, physical units, and percentages are not applicable in this task; all numeric quantities are dimensionless real numbers. The final answers must be raw floating-point numbers without units.",
            "solution": "The problem asks for an empirical determination of the smallest diagonal perturbation, $\\epsilon$, from a given set $S$ that stabilizes the incomplete LU factorization of order zero, ILU(0), for a given matrix $A$ and breakdown tolerance $\\tau$.\n\nAn LU factorization of a square matrix $A$ decomposes it into a product of a unit lower-triangular matrix $L$ and an upper-triangular matrix $U$, such that $A = LU$. The incomplete LU factorization, ILU($p$), is an approximation to this decomposition where a certain amount of \"fill-in\" (creation of nonzeros in $L$ and $U$ at positions that are zero in $A$) is allowed, controlled by the level $p$. The specific variant in this problem is ILU(0), where no fill-in is permitted. The sparsity patterns of $L$ and $U$ must be subsets of the sparsity pattern of $A$. That is, if an entry $(i,j)$ is zero in the original matrix $A$, it must remain zero in $L$ (if $ij$) or $U$ (if $i \\le j$).\n\nThe standard algorithm for LU factorization can fail (break down) if it encounters a zero or very small pivot element on the diagonal of $U$. This is because the algorithm requires division by these pivots. The ILU(0) algorithm is similarly susceptible to breakdown. A common technique to prevent this is to add a small positive value to the diagonal entries of $A$. This is known as diagonal perturbation or diagonal shifting. We form the matrix $A_{\\epsilon} = A + \\epsilon I$, where $I$ is the identity matrix and $\\epsilon$ is a small nonnegative scalar. This directly increases the magnitude of the diagonal entries, which in turn tends to increase the magnitude of the pivots $U_{ii}$, thus stabilizing the factorization. The task is to find the minimum $\\epsilon$ from a candidate set $S$ that is sufficient to prevent the absolute value of any pivot from falling below a given tolerance $\\tau$.\n\nThe procedure to solve this problem involves implementing the specified ILU(0) algorithm and then, for each test case, iterating through the candidate perturbations $\\epsilon \\in S$ in ascending order. The first $\\epsilon$ for which the ILU(0) of $A_\\epsilon$ completes without breakdown is the desired result.\n\nThe ILU(0) algorithm is implemented as follows, following the problem statement's constraints. Let $P$ be the set of indices $(i,j)$ corresponding to structural nonzeros in $A$, with all diagonal positions $(i,i)$ included.\nThe matrices $L$ and $U$ are constructed row by row, for $i = 1, \\dots, n$.\n\nFor each row $i$:\n1.  Compute the relevant entries of the $i$-th row of $L$. For each $j$ from $1$ to $i-1$, if $(i,j) \\in P$:\n    Before computing $L_{ij}$, a breakdown check is performed on the pivot $U_{jj}$: if $|U_{jj}|  \\tau$, the factorization fails. Otherwise, we compute:\n    $$\n    L_{ij} = \\frac{A_{\\epsilon,ij} - \\sum_{k=1}^{j-1} L_{ik} U_{kj}}{U_{jj}}\n    $$\n    The sum is restricted to include only terms where $(i,k) \\in P$ and $(k,j) \\in P$. This enforces the zero fill-in constraint. In practice, since $L$ and $U$ are built to respect the pattern $P$, any term where the indices are not in $P$ would be zero anyway.\n\n2.  Set the diagonal of $L$: $L_{ii} = 1$.\n\n3.  Compute the relevant entries of the $i$-th row of $U$. For each $j$ from $i$ to $n$, if $(i,j) \\in P$:\n    $$\n    U_{ij} = A_{\\epsilon,ij} - \\sum_{k=1}^{i-1} L_{ik} U_{kj}\n    $$\n    Again, the sum is restricted by the sparsity pattern $P$.\n\n4.  After computing all entries for row $i$, a final breakdown check is performed on the newly computed pivot $U_{ii}$: if $|U_{ii}|  \\tau$, the factorization fails.\n\nIf this process completes for all rows $i=1, \\dots, n$ without any breakdown check failing, the factorization is deemed successful for the chosen $\\epsilon$.\n\nFor each test case provided:\n- The set of candidate perturbations $S$ is already sorted.\n- We iterate through $\\epsilon \\in S$.\n- For a given $\\epsilon$, we run the ILU(0) algorithm on $A_{\\epsilon} = A + \\epsilon I$ with tolerance $\\tau$.\n- If the algorithm succeeds, we record this $\\epsilon$ as the minimal stabilizing perturbation for that test case and proceed to the next test case.\n- If we exhaust all $\\epsilon \\in S$ and all of them lead to breakdown, we record a not-a-number value (`NaN`) for that test case.\n\nThe final output is a list of these minimal $\\epsilon$ values for each test case in the specified order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef perform_ilu0(A, epsilon, tau):\n    \"\"\"\n    Performs ILU(0) factorization on a given matrix A with diagonal perturbation.\n\n    Args:\n        A (np.ndarray): The input square matrix.\n        epsilon (float): The non-negative scalar for diagonal perturbation.\n        tau (float): The positive tolerance for pivot breakdown.\n\n    Returns:\n        bool: True if the factorization is successful, False if breakdown occurs.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Create the perturbed matrix A_epsilon = A + epsilon * I\n    A_eps = A.copy()\n    if epsilon > 0.0:\n        for i in range(n):\n            A_eps[i, i] += epsilon\n\n    # Determine the sparsity pattern P. This includes all structurally\n    # nonzero entries of A, plus all diagonal entries by definition.\n    rows, cols = np.nonzero(A)\n    P = set(zip(rows, cols))\n    for i in range(n):\n        P.add((i, i))\n\n    # Initialize L and U matrices. L will have a unit diagonal.\n    L = np.zeros((n, n), dtype=float)\n    U = np.zeros((n, n), dtype=float)\n\n    for i in range(n):\n        # Compute the i-th row of L (off-diagonal elements, j  i)\n        for j in range(i):\n            if (i, j) in P:\n                # Calculate the sum term in the formula for L_ij\n                s = 0.0\n                # The sum is over k  j. The sparsity constraint on the sum is\n                # implicitly handled because L and U are built with this sparsity.\n                # However, an explicit check follows the problem spec exactly.\n                for k in range(j):\n                    if (i, k) in P and (k, j) in P:\n                        s += L[i, k] * U[k, j]\n                \n                # Breakdown check 1: before division by U_jj\n                if abs(U[j, j])  tau:\n                    return False  # Breakdown\n\n                L[i, j] = (A_eps[i, j] - s) / U[j, j]\n        \n        # Set the diagonal of L to 1\n        L[i, i] = 1.0\n\n        # Compute the i-th row of U (j = i)\n        for j in range(i, n):\n            if (i, j) in P:\n                # Calculate the sum term in the formula for U_ij\n                s = 0.0\n                # The sum is over k  i.\n                for k in range(i):\n                    if (i, k) in P and (k, j) in P:\n                        s += L[i, k] * U[k, j]\n                \n                U[i, j] = A_eps[i, j] - s\n        \n        # Breakdown check 2: after computing the new diagonal pivot U_ii\n        if abs(U[i, i])  tau:\n            return False  # Breakdown\n    \n    return True  # Success\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and find the minimal stabilizing perturbation.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([\n                [0, 1, 0, 0, 0],\n                [1, 0, 1, 0, 0],\n                [0, 1, 0, 1, 0],\n                [0, 0, 1, 0, 1],\n                [0, 0, 0, 1, 0]\n            ], dtype=float),\n            \"S\": [0.0, 1e-16, 1e-12, 1e-10, 1e-8],\n            \"tau\": 1e-12\n        },\n        {\n            \"A\": np.array([\n                [1, 1, 0],\n                [1, 1, 1],\n                [0, 1, 1]\n            ], dtype=float),\n            \"S\": [0.0, 1e-16, 5e-13, 1e-12, 1e-9],\n            \"tau\": 1e-12\n        },\n        {\n            \"A\": np.array([\n                [ 2, -1,  0,  0,  0],\n                [-1,  2, -1,  0,  0],\n                [ 0, -1,  2, -1,  0],\n                [ 0,  0, -1,  2, -1],\n                [ 0,  0,  0, -1,  2]\n            ], dtype=float),\n            \"S\": [0.0, 1e-16, 1e-12],\n            \"tau\": 1e-12\n        },\n        {\n            \"A\": np.array([\n                [1e-16, 0, 0],\n                [0, 1, 0],\n                [0, 0, 1]\n            ], dtype=float),\n            \"S\": [0.0, 1e-16, 5e-13, 1e-12, 1e-9],\n            \"tau\": 1e-12\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A, S, tau = case[\"A\"], case[\"S\"], case[\"tau\"]\n        minimal_epsilon = np.nan\n        \n        # The sets S are pre-sorted, so we check epsilon in increasing order.\n        for epsilon in S:\n            if perform_ilu0(A, epsilon, tau):\n                minimal_epsilon = epsilon\n                break  # Found the smallest epsilon, move to the next test case\n        \n        results.append(minimal_epsilon)\n\n    # Format the final output string exactly as required.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}