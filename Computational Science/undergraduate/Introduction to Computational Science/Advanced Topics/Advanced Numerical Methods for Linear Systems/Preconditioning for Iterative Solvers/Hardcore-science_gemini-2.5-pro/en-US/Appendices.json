{
    "hands_on_practices": [
        {
            "introduction": "Discretizing partial differential equations, such as the Poisson equation, is a cornerstone of computational science, but it often yields large, sparse linear systems whose condition numbers degrade as the grid is refined. This practice explores the most fundamental preconditioning technique, diagonal scaling, to see how it can counteract this degradation by altering the spectral properties of the system matrix. By implementing and analyzing this for a 2D Poisson problem, you will gain a concrete understanding of how preconditioning can cluster the eigenvalues of an operator, a key mechanism for accelerating iterative solvers. ",
            "id": "3176221",
            "problem": "You are given the task of analyzing and implementing a diagonal scaling preconditioner for the two-dimensional (2D) Poisson equation discretized on a unit square with homogeneous Dirichlet boundary conditions. Consider the standard $5$-point finite difference discretization that produces a Symmetric Positive Definite (SPD) linear system $\\mathbf{A}\\mathbf{u}=\\mathbf{b}$ with $\\mathbf{A}\\in\\mathbb{R}^{N\\times N}$ and $N=n^2$, where $n$ is the number of interior grid points per spatial dimension and the grid spacing is $h=1/(n+1)$. Define the diagonal scaling preconditioner $\\mathbf{M}=\\operatorname{diag}(\\mathbf{A})$. Your goals are:\n- Starting from the definitions of the discrete Laplacian and basic linear algebra facts about similarity transformations and eigenvalues, derive a prediction for how the spectrum of $\\mathbf{M}^{-1}\\mathbf{A}$ clusters compared to the spectrum of $\\mathbf{A}$ as $n$ increases. Your prediction must be reasoned from first principles, explicitly referencing how the discrete operator scales with $h$ and how diagonal scaling alters that dependence.\n- Implement a complete, runnable program that:\n  1. Constructs $\\mathbf{A}$ for given $n$ using the canonical $5$-point stencil on the unit square with zero Dirichlet boundary conditions.\n  2. Forms $\\mathbf{M}=\\operatorname{diag}(\\mathbf{A})$ and evaluates the spectrum of the preconditioned operator via the similar symmetric matrix $\\mathbf{M}^{-1/2}\\mathbf{A}\\mathbf{M}^{-1/2}$.\n  3. Computes, for each matrix, the spectral interval width defined by $\\max\\lambda-\\min\\lambda$, where $\\lambda$ ranges over eigenvalues. Let $w(\\mathbf{A})$ denote the width for $\\mathbf{A}$ and $w(\\mathbf{M}^{-1}\\mathbf{A})$ denote the width for the preconditioned operator.\n  4. Reports, for each test case, the triple of floats $\\left[w(\\mathbf{A}),\\,w(\\mathbf{M}^{-1}\\mathbf{A}),\\,w(\\mathbf{A})/w(\\mathbf{M}^{-1}\\mathbf{A})\\right]$.\n\nUse the following test suite of grid sizes to ensure coverage of a small edge case and progressively larger instances: $n\\in\\{2,4,8,16\\}$. These cases capture a boundary condition for very small $n$, a typical moderate case, and a larger case where scaling effects are visible. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case itself printed as a nested list of three floats with six digits after the decimal point, and with no spaces. For example, the output format must be of the form $\\left[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots\\right]$ where each $a_i$, $b_i$, and $c_i$ is a float. No physical units or angle units are involved, and all outputs are pure numbers. The solution must justify the prediction about spectral clustering using only valid foundational definitions from numerical linear algebra and the finite difference discretization of the Poisson equation, avoiding any shortcut formulas given directly in the problem statement.",
            "solution": "We begin from the definition of the discrete Laplacian in two dimensions on the unit square with homogeneous Dirichlet boundary conditions. Let $n$ denote the number of interior grid points per spatial dimension and $h=1/(n+1)$ the uniform grid spacing. The standard $5$-point finite difference discretization yields, for an interior node indexed by $(i,j)$, the relation\n$$\n-\\frac{1}{h^2}\\,u_{i-1,j} - \\frac{1}{h^2}\\,u_{i,j-1} + \\frac{4}{h^2}\\,u_{i,j} - \\frac{1}{h^2}\\,u_{i+1,j} - \\frac{1}{h^2}\\,u_{i,j+1} \\;=\\; f_{i,j},\n$$\nwhich produces an $N\\times N$ sparse matrix $\\mathbf{A}$ with $N=n^2$, a diagonal equal to $4/h^2$, and off-diagonal entries equal to $-1/h^2$ corresponding to nearest neighbor couplings in the $x$ and $y$ directions. This matrix is Symmetric Positive Definite (SPD) because it arises from a coercive bilinear form and a conforming discretization.\n\nFundamental facts from numerical linear algebra tell us that eigenvalues scale under scalar multiplication and that similar matrices share the same eigenvalues. Specifically, if $\\mathbf{D}=\\operatorname{diag}(\\mathbf{A})$ and $\\mathbf{M}=\\mathbf{D}$, then $\\mathbf{M}^{-1}\\mathbf{A}$ is similar to the symmetric matrix\n$$\n\\mathbf{S} \\;=\\; \\mathbf{M}^{-1/2}\\mathbf{A}\\mathbf{M}^{-1/2},\n$$\nso $\\mathbf{M}^{-1}\\mathbf{A}$ and $\\mathbf{S}$ have identical spectra. Because $\\mathbf{S}$ is symmetric, its eigenvalues are real and can be analyzed and computed with high numerical reliability.\n\nTo predict spectral clustering from first principles, we reason about the scaling behavior of $\\mathbf{A}$. The finite difference stencil shows that all entries of $\\mathbf{A}$ scale like $1/h^2$. Consequently, the eigenvalues of $\\mathbf{A}$ scale like $1/h^2$; that is, there exist mode-dependent dimensionless constants $\\alpha_{\\min}(n)$ and $\\alpha_{\\max}(n)$ such that\n$$\n\\lambda_{\\min}(\\mathbf{A}) \\approx \\frac{\\alpha_{\\min}(n)}{h^2},\\qquad\n\\lambda_{\\max}(\\mathbf{A}) \\approx \\frac{\\alpha_{\\max}(n)}{h^2}.\n$$\nAs $n$ increases (and $h$ decreases), both $\\lambda_{\\min}(\\mathbf{A})$ and $\\lambda_{\\max}(\\mathbf{A})$ grow like $1/h^2$, and the spectral interval width $w(\\mathbf{A})=\\lambda_{\\max}(\\mathbf{A})-\\lambda_{\\min}(\\mathbf{A})$ also grows like $1/h^2$. Thus the spectrum of $\\mathbf{A}$ expands unboundedly with refinement.\n\nDiagonal scaling with $\\mathbf{M}=\\operatorname{diag}(\\mathbf{A})$ counteracts this uniform $1/h^2$ factor. In the present discretization, the diagonal entries are $4/h^2$ throughout, so\n$$\n\\mathbf{M} \\;=\\; \\frac{4}{h^2}\\,\\mathbf{I},\n\\qquad\n\\mathbf{M}^{-1/2} \\;=\\; \\sqrt{\\frac{h^2}{4}}\\,\\mathbf{I}.\n$$\nTherefore\n$$\n\\mathbf{S} \\;=\\; \\mathbf{M}^{-1/2}\\mathbf{A}\\mathbf{M}^{-1/2}\n\\;=\\; \\left(\\sqrt{\\frac{h^2}{4}}\\,\\mathbf{I}\\right)\\,\\mathbf{A}\\,\\left(\\sqrt{\\frac{h^2}{4}}\\,\\mathbf{I}\\right)\n\\;=\\; \\frac{h^2}{4}\\,\\mathbf{A}.\n$$\nBy scalar scaling of eigenvalues, we then have\n$$\n\\lambda(\\mathbf{S}) \\;=\\; \\frac{h^2}{4}\\,\\lambda(\\mathbf{A}),\n\\qquad\n\\lambda(\\mathbf{M}^{-1}\\mathbf{A}) \\;=\\; \\lambda(\\mathbf{S}).\n$$\nThis shows that the entire spectrum of $\\mathbf{M}^{-1}\\mathbf{A}$ is obtained by multiplying the spectrum of $\\mathbf{A}$ by $h^2/4$. Because the eigenvalues of $\\mathbf{A}$ scale like $1/h^2$, the spectrum of $\\mathbf{M}^{-1}\\mathbf{A}$ becomes $\\mathcal{O}(1)$, i.e., bounded independently of $h$ and $n$. In the classical two-dimensional Dirichlet case, the mode structure implies that the preconditioned eigenvalues lie within an interval that approaches $(0,2)$ as $n$ grows. Intuitively, this happens because $\\mathbf{M}^{-1}\\mathbf{A}$ has a unit diagonal and nearest-neighbor couplings of magnitude $1/4$, so most eigenvalues cluster around $1$ with a bounded spread. In contrast, the unpreconditioned spectrum widens without bound as $h\\to 0$.\n\nHowever, the smallest preconditioned eigenvalue still vanishes like $\\mathcal{O}(h^2)$ when mapped back to the original scale of $\\mathbf{A}$, and in the preconditioned scale it behaves like the sum of two lowest-mode squared sines, which for large $n$ behaves like $\\mathcal{O}((n+1)^{-2})$. Thus the condition number of $\\mathbf{M}^{-1}\\mathbf{A}$ grows like $\\mathcal{O}(n^2)$, just as the condition number of $\\mathbf{A}$ does. The primary benefit of this diagonal scaling, as reflected in iterative convergence for many methods, is the clustering of the bulk of eigenvalues into a bounded interval near $1$ and the removal of the global $1/h^2$ scaling, not the elimination of all conditioning issues.\n\nAlgorithmic design for the program:\n- For each $n$ in the test suite, form the one-dimensional tridiagonal matrix $\\mathbf{T}\\in\\mathbb{R}^{n\\times n}$ with diagonal entries $2/h^2$ and off-diagonal entries $-1/h^2$. The two-dimensional operator is assembled as the Kronecker sum $\\mathbf{A}=\\mathbf{I}\\otimes\\mathbf{T}+\\mathbf{T}\\otimes\\mathbf{I}$, where $\\mathbf{I}$ is the $n\\times n$ identity.\n- Extract $\\mathbf{D}=\\operatorname{diag}(\\mathbf{A})$ and construct $\\mathbf{S}=\\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}$ implicitly by scaling rows and columns of a dense copy of $\\mathbf{A}$ by the inverse square roots of $\\mathbf{D}$.\n- Compute eigenvalues of $\\mathbf{A}$ and $\\mathbf{S}$ using symmetric eigenvalue routines (real Hermitian), then compute the spectral widths $w(\\mathbf{A})$ and $w(\\mathbf{S})=w(\\mathbf{M}^{-1}\\mathbf{A})$ and the ratio $w(\\mathbf{A})/w(\\mathbf{M}^{-1}\\mathbf{A})$.\n- Aggregate the results into the required single-line output format, with each test case reported as $\\left[w(\\mathbf{A}),\\,w(\\mathbf{M}^{-1}\\mathbf{A}),\\,w(\\mathbf{A})/w(\\mathbf{M}^{-1}\\mathbf{A})\\right]$.\n\nThis procedure directly demonstrates the theoretical prediction: $w(\\mathbf{A})$ grows like $1/h^2$ while $w(\\mathbf{M}^{-1}\\mathbf{A})$ remains bounded and near a constant, causing the ratio $w(\\mathbf{A})/w(\\mathbf{M}^{-1}\\mathbf{A})$ to grow like $1/h^2$ and thereby quantitatively illustrating stronger spectral clustering under diagonal scaling.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse as sp\n\ndef build_poisson_2d(n: int) - sp.csr_matrix:\n    \"\"\"\n    Build the 2D Poisson finite difference matrix A on the unit square\n    with zero Dirichlet boundary conditions using the 5-point stencil.\n    Size is N = n^2, where n is the number of interior points per dimension.\n    \"\"\"\n    h = 1.0 / (n + 1)\n    e = np.ones(n)\n    # 1D tridiagonal: diag = 2/h^2, off-diag = -1/h^2\n    T = sp.diags([-e, 2.0 * e, -e], offsets=[-1, 0, 1], shape=(n, n), format='csr') / (h**2)\n    I = sp.eye(n, format='csr')\n    # 2D operator: Kronecker sum\n    A = sp.kron(I, T, format='csr') + sp.kron(T, I, format='csr')\n    return A\n\ndef spectral_width(evals: np.ndarray) - float:\n    \"\"\"Compute spectral interval width: max(lambda) - min(lambda).\"\"\"\n    return float(np.max(evals) - np.min(evals))\n\ndef compute_metrics_for_n(n: int):\n    \"\"\"\n    For given n, construct A, preconditioner M=diag(A),\n    and compute spectral widths for A and M^{-1}A via the similar symmetric matrix.\n    Return [wA, wPre, wA_over_wPre] as floats.\n    \"\"\"\n    A = build_poisson_2d(n)\n    # Dense copy for eigenvalue computations (SPD, modest sizes)\n    A_dense = A.toarray()\n    # Eigenvalues of A (symmetric)\n    evals_A = np.linalg.eigvalsh(A_dense)\n\n    # Diagonal preconditioner M = diag(A), construct S = M^{-1/2} A M^{-1/2}\n    d = A.diagonal()\n    inv_sqrt_d = 1.0 / np.sqrt(d)\n    # Scale rows and columns: S = D^{-1/2} * A_dense * D^{-1/2}\n    S = (inv_sqrt_d[:, None] * A_dense) * inv_sqrt_d[None, :]\n    evals_S = np.linalg.eigvalsh(S)\n\n    wA = spectral_width(evals_A)\n    wPre = spectral_width(evals_S)\n    ratio = wA / wPre\n    return [wA, wPre, ratio]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_ns = [2, 4, 8, 16]\n\n    results = []\n    for n in test_ns:\n        wA, wPre, ratio = compute_metrics_for_n(n)\n        # Round to 6 decimal places as specified for output formatting\n        metrics = [round(wA, 6), round(wPre, 6), round(ratio, 6)]\n        results.append(metrics)\n\n    # Build exact required single-line output: no spaces, nested lists\n    formatted = \"[\" + \",\".join(\"[\" + \",\".join(f\"{x:.6f}\" for x in triple) + \"]\" for triple in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While preconditioning is a powerful tool for accelerating iterative solvers, it is not a universal remedy; a poorly chosen preconditioner can sometimes be worse than none. This exercise serves as a critical counterexample, demonstrating with a simple $2 \\times 2$ non-symmetric matrix that a standard Jacobi (diagonal) preconditioner can actually increase the matrix 2-norm condition number, $\\kappa_2(A)$. Working through this problem will underscore the importance of careful analysis and highlight that for non-symmetric systems, the intuition that preconditioning \"makes the matrix closer to the identity\" does not always translate to a better-conditioned system. ",
            "id": "2429417",
            "problem": "In many computational physics applications, such as upwind discretizations of steady convection-diffusion operators, the resulting linear systems can be nonsymmetric with strongly unbalanced diagonal entries. A common diagonal (Jacobi) preconditioner scales the rows by the inverse of the diagonal entries. While preconditioning is often intended to improve convergence of iterative solvers, it does not universally reduce the matrix condition number in the matrix $2$-norm.\n\nStarting from the core definitions:\n- The matrix $2$-norm condition number of a nonsingular matrix $X$ is $\\kappa_{2}(X) = \\sigma_{\\max}(X)/\\sigma_{\\min}(X)$, where $\\sigma_{\\max}(X)$ and $\\sigma_{\\min}(X)$ are the largest and smallest singular values of $X$, respectively.\n- The singular values of $X$ are the square roots of the eigenvalues of $X^{\\mathsf{T}}X$.\n- The Jacobi preconditioner $P$ is the diagonal matrix formed from the diagonal of $A$, and the left-preconditioned operator is $P^{-1}A$.\n\nConsider the explicit $2 \\times 2$ matrix\n$$\nA = \\begin{pmatrix}\n1  10 \\\\\n0.19  2\n\\end{pmatrix}\n$$,\nwhich is a simple model for a locally upwinded transport operator with disparate diagonal scaling. Let $P = \\operatorname{diag}(A) = \\operatorname{diag}(1, 2)$ be the Jacobi preconditioner, and define $B = P^{-1} A$.\n\nUsing only the definitions above (no other formulas may be assumed), compute the ratio\n$$\nR \\equiv \\frac{\\kappa_{2}(P^{-1}A)}{\\kappa_{2}(A)} = \\frac{\\kappa_{2}(B)}{\\kappa_{2}(A)}.\n$$\nReport the final numerical value of $R$ rounded to four significant figures. The answer is dimensionless; do not include any units.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in numerical linear algebra, well-posed with all necessary information provided, and objective in its formulation. No inconsistencies, ambiguities, or factual errors are present. We proceed with the computation as requested.\n\nThe task is to compute the ratio $R = \\frac{\\kappa_{2}(B)}{\\kappa_{2}(A)}$, where $A = \\begin{pmatrix} 1  10 \\\\ 0.19  2 \\end{pmatrix}$ and $B = P^{-1}A$ with $P = \\operatorname{diag}(A)$. The computation will be performed in two stages: first for the matrix $A$, then for the matrix $B$.\n\nFirst, we determine the condition number $\\kappa_2(A)$. According to the provided definition, the singular values of $A$ are the square roots of the eigenvalues of $A^{\\mathsf{T}}A$.\nThe transpose of $A$ is $A^{\\mathsf{T}} = \\begin{pmatrix} 1  0.19 \\\\ 10  2 \\end{pmatrix}$.\nWe compute the product $A^{\\mathsf{T}}A$:\n$$\nA^{\\mathsf{T}}A = \\begin{pmatrix} 1  0.19 \\\\ 10  2 \\end{pmatrix} \\begin{pmatrix} 1  10 \\\\ 0.19  2 \\end{pmatrix} = \\begin{pmatrix} 1^2 + 0.19^2  1 \\cdot 10 + 0.19 \\cdot 2 \\\\ 10 \\cdot 1 + 2 \\cdot 0.19  10^2 + 2^2 \\end{pmatrix} = \\begin{pmatrix} 1.0361  10.38 \\\\ 10.38  104 \\end{pmatrix}\n$$\nThe eigenvalues, denoted $\\lambda$, of $A^{\\mathsf{T}}A$ are the roots of the characteristic equation $\\det(A^{\\mathsf{T}}A - \\lambda I) = 0$:\n$$\n(1.0361 - \\lambda)(104 - \\lambda) - (10.38)^2 = 0\n$$\n$$\n\\lambda^2 - (1.0361 + 104)\\lambda + (1.0361 \\cdot 104 - 10.38^2) = 0\n$$\n$$\n\\lambda^2 - 105.0361\\lambda + (107.7544 - 107.7444) = 0\n$$\n$$\n\\lambda^2 - 105.0361\\lambda + 0.01 = 0\n$$\nUsing the quadratic formula, the eigenvalues are $\\lambda = \\frac{105.0361 \\pm \\sqrt{105.0361^2 - 4(0.01)}}{2}$.\nThe two eigenvalues are $\\lambda_{\\max, A} \\approx 105.0360048$ and $\\lambda_{\\min, A} \\approx 9.52054 \\times 10^{-5}$.\nThe singular values of $A$ are $\\sigma_{\\max}(A) = \\sqrt{\\lambda_{\\max, A}}$ and $\\sigma_{\\min}(A) = \\sqrt{\\lambda_{\\min, A}}$.\nThe condition number of $A$ is therefore:\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\sqrt{\\frac{\\lambda_{\\max, A}}{\\lambda_{\\min, A}}} \\approx \\sqrt{\\frac{105.0360048}{9.52054 \\times 10^{-5}}} \\approx \\sqrt{1103256} \\approx 1050.36\n$$\n\nNext, we determine the condition number $\\kappa_2(B)$. The preconditioner is $P = \\operatorname{diag}(A) = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}$.\nIts inverse is $P^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  0.5 \\end{pmatrix}$.\nThe preconditioned matrix $B$ is:\n$$\nB = P^{-1}A = \\begin{pmatrix} 1  0 \\\\ 0  0.5 \\end{pmatrix} \\begin{pmatrix} 1  10 \\\\ 0.19  2 \\end{pmatrix} = \\begin{pmatrix} 1  10 \\\\ 0.095  1 \\end{pmatrix}\n$$\nWe follow the same procedure for $B$. The transpose is $B^{\\mathsf{T}} = \\begin{pmatrix} 1  0.095 \\\\ 10  1 \\end{pmatrix}$.\nThe product $B^{\\mathsf{T}}B$ is:\n$$\nB^{\\mathsf{T}}B = \\begin{pmatrix} 1  0.095 \\\\ 10  1 \\end{pmatrix} \\begin{pmatrix} 1  10 \\\\ 0.095  1 \\end{pmatrix} = \\begin{pmatrix} 1^2 + 0.095^2  1 \\cdot 10 + 0.095 \\cdot 1 \\\\ 10 \\cdot 1 + 1 \\cdot 0.095  10^2 + 1^2 \\end{pmatrix} = \\begin{pmatrix} 1.009025  10.095 \\\\ 10.095  101 \\end{pmatrix}\n$$\nThe eigenvalues, denoted $\\mu$, of $B^{\\mathsf{T}}B$ are the roots of the characteristic equation $\\det(B^{\\mathsf{T}}B - \\mu I) = 0$:\n$$\n(1.009025 - \\mu)(101 - \\mu) - (10.095)^2 = 0\n$$\n$$\n\\mu^2 - (1.009025 + 101)\\mu + (1.009025 \\cdot 101 - 10.095^2) = 0\n$$\n$$\n\\mu^2 - 102.009025\\mu + (101.911525 - 101.909025) = 0\n$$\n$$\n\\mu^2 - 102.009025\\mu + 0.0025 = 0\n$$\nThe eigenvalues are $\\mu = \\frac{102.009025 \\pm \\sqrt{102.009025^2 - 4(0.0025)}}{2}$.\nThe two eigenvalues are $\\mu_{\\max, B} \\approx 102.0090005$ and $\\mu_{\\min, B} \\approx 2.45075 \\times 10^{-5}$.\nThe condition number of $B$ is:\n$$\n\\kappa_2(B) = \\frac{\\sigma_{\\max}(B)}{\\sigma_{\\min}(B)} = \\sqrt{\\frac{\\mu_{\\max, B}}{\\mu_{\\min, B}}} \\approx \\sqrt{\\frac{102.0090005}{2.45075 \\times 10^{-5}}} \\approx \\sqrt{4162166.5} \\approx 2040.14\n$$\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{\\kappa_2(B)}{\\kappa_2(A)} \\approx \\frac{2040.14}{1050.36} \\approx 1.94233\n$$\nRounding the final result to four significant figures gives $1.942$.",
            "answer": "$$\n\\boxed{1.942}\n$$"
        },
        {
            "introduction": "Beyond simple scaling, preconditioners based on incomplete factorizations, such as Incomplete Cholesky (IC), offer a more powerful approach by approximating a direct inverse. However, their power comes with a challenge: numerical instability, which often manifests as dangerously small or even negative pivots during the factorization process. This hands-on problem delves into this practical issue, tasking you with implementing and comparing strategies like diagonal shifting and pivoting to stabilize the IC factorization and ensure a robust preconditioner is constructed. ",
            "id": "3176187",
            "problem": "You will implement and study the stability of incomplete factorizations when small pivots cause breakdown, and test two stabilization strategies and their effect on preserving Symmetric Positive Definite (SPD) in the resulting preconditioner matrix. Work in purely mathematical terms and implement a complete, runnable program that performs the following tasks.\n\nStart from the base definitions:\n- A real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is Symmetric Positive Definite (SPD) if $x^{\\top} A x \\gt 0$ for all nonzero $x \\in \\mathbb{R}^{n}$.\n- If $A$ is SPD, its (exact) Cholesky factorization is $A = L L^{\\top}$ with $L$ lower triangular and strictly positive diagonal.\n- An incomplete Cholesky factorization with zero fill, denoted $\\mathrm{IC}(0)$, attempts to compute $A \\approx \\tilde{L} \\tilde{L}^{\\top}$ while restricting $\\tilde{L}$ to the sparsity pattern of $A$ (no fill outside the nonzero pattern of $A$). During the algorithm, at step $j$, the pivot is computed as\n  $$ s_j \\equiv A_{j j} - \\sum_{k=0}^{j-1} \\tilde{L}_{j k}^2 $$\n  . A breakdown is declared if $s_j$ is not safe to use numerically.\n- In practice, to avoid numerical instability, the algorithm may treat any pivot $s_j$ smaller than a specified threshold $\\tau \\gt 0$ as a breakdown.\n\nYou will implement three strategies for handling small pivots $s_j \\lt \\tau$ while computing $\\mathrm{IC}(0)$:\n- Strategy $S_0$ (no stabilization): If $s_j \\lt \\tau$, declare breakdown and stop. No preconditioner is produced.\n- Strategy $S_1$ (diagonal shift): If $s_j \\lt \\tau$, replace $A_{j j}$ by $A_{j j} + \\delta$ with the minimal $\\delta \\ge 0$ such that the new pivot equals $\\tau$ up to a numerical safety increment, and continue. This is a modified incomplete Cholesky that preserves symmetry and enforces positive pivots.\n- Strategy $S_2$ (symmetric diagonal pivoting): If $s_j \\lt \\tau$, search $p \\in \\{j, j+1, \\dots, n-1\\}$ that maximizes the tentative pivot\n  $$ \\hat{s}_p \\equiv A_{p p} - \\sum_{k=0}^{j-1} \\tilde{L}_{p k}^2 $$\n  , then apply a symmetric permutation swapping indices $j$ and $p$ (swap rows and columns $j$ and $p$ in $A$; maintain previously computed columns of $\\tilde{L}$ by swapping rows $j$ and $p$ for columns $0$ through $j-1$). After the swap, recompute $s_j$ and proceed if $s_j \\ge \\tau$; otherwise, declare breakdown. This preserves symmetry but generally changes the factorization order and the sparsity pattern through permutation.\n\nFor any strategy that completes the factorization, define the preconditioner as $M \\equiv \\tilde{L} \\tilde{L}^{\\top}$. Because $M$ is formed as a product of a lower triangular matrix and its transpose with nonzero diagonal entries, it should be SPD if all pivots were positive. Numerically, you will test whether $M$ is SPD by attempting to compute a Cholesky factorization of $M$ and declaring success if and only if the factorization succeeds without error.\n\nImplement the above for dimension $n = 6$, threshold $\\tau = 0.05$, and the following three real symmetric sparse SPD test matrices that define the test suite. All unspecified entries are zero; all matrices are symmetric.\n- Matrix $A^{(1)}$ (a well-conditioned tridiagonal SPD):\n  - Main diagonal: $\\{2, 2, 2, 2, 2, 2\\}$.\n  - First sub- and super-diagonal entries: $-1$ between consecutive indices $\\{(0,1), (1,2), (2,3), (3,4), (4,5)\\}$.\n- Matrix $A^{(2)}$ (SPD with deliberately small safe pivots under $\\mathrm{IC}(0)$):\n  - Main diagonal: $\\{1.01, 1.0001, 1.01, 1.02, 1.1, 1.2\\}$.\n  - First sub- and super-diagonal entries: $-1$ between consecutive indices $\\{(0,1), (1,2), (2,3)\\}$; no other off-diagonal nonzeros.\n- Matrix $A^{(3)}$ (SPD with extremely small safe pivots early in the factorization):\n  - Main diagonal: $\\{1.000001, 1.00000001, 1.0000011, 1.5, 1.6, 1.7\\}$.\n  - First sub- and super-diagonal entries: $-1$ between consecutive indices $\\{(0,1), (1,2)\\}$; no other off-diagonal nonzeros.\n\nFor each matrix $A^{(i)}$ with $i \\in \\{1, 2, 3\\}$, and each strategy $S_0$, $S_1$, $S_2$ in that order, run $\\mathrm{IC}(0)$ with threshold $\\tau$, and record a binary indicator\n$$ b_{i,s} = \\begin{cases}\n1,  \\text{if the algorithm finishes and the resulting } M = \\tilde{L} \\tilde{L}^{\\top} \\text{ is numerically SPD (Cholesky succeeds);} \\\\\n0,  \\text{otherwise.}\n\\end{cases} $$\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered as\n$$ [\\, b_{1,S_0}, b_{1,S_1}, b_{1,S_2}, b_{2,S_0}, b_{2,S_1}, b_{2,S_2}, b_{3,S_0}, b_{3,S_1}, b_{3,S_2} \\,]. $$\nEach $b_{i,s}$ must be an integer $0$ or $1$. No other output is permitted.\n\nAngles do not appear and no physical units are involved, so no unit specification is required. The program must be self-contained and must not read any input.",
            "solution": "The task is to implement and evaluate three strategies for handling small pivots during the Incomplete Cholesky factorization with zero fill-in, denoted $\\mathrm{IC}(0)$, for a set of given symmetric positive definite (SPD) matrices. The stability of the factorization and the symmetric positive definiteness of the resulting preconditioner matrix are the primary criteria for evaluation.\n\nFirst, we formalize the $\\mathrm{IC}(0)$ algorithm. Given a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, we seek a lower triangular matrix $\\tilde{L}$ such that $A \\approx \\tilde{L} \\tilde{L}^{\\top}$. The key constraint of $\\mathrm{IC}(0)$ is that the sparsity pattern of $\\tilde{L}$ is a subset of the sparsity pattern of the lower triangular part of $A$. That is, $\\tilde{L}_{ij} = 0$ if $A_{ij} = 0$ for $i  j$.\n\nThe elements of $\\tilde{L}$ are computed column by column, for $j = 0, 1, \\dots, n-1$. For each column $j$, the diagonal element $\\tilde{L}_{jj}$ and the sub-diagonal elements $\\tilde{L}_{ij}$ ($i  j$) are computed as follows:\nFirst, the diagonal pivot element, $s_j$, is calculated:\n$$ s_j = A_{jj} - \\sum_{k=0}^{j-1} \\tilde{L}_{jk}^2 $$\nThe diagonal element of $\\tilde{L}$ is then:\n$$ \\tilde{L}_{jj} = \\sqrt{s_j} $$\nFor a successful factorization, all pivots $s_j$ must be strictly positive. In numerical practice, to avoid instability from division by small numbers and to mitigate the effects of floating-point errors, we require the pivot to be greater than a small positive threshold, $s_j \\ge \\tau  0$. If this condition is violated, a \"breakdown\" occurs.\n\nThe sub-diagonal elements in column $j$ are then computed for $i = j+1, \\dots, n-1$:\n$$ \\tilde{L}_{ij} = \\frac{1}{\\tilde{L}_{jj}} \\left( A_{ij} - \\sum_{k=0}^{j-1} \\tilde{L}_{ik} \\tilde{L}_{jk} \\right), \\quad \\text{if } A_{ij} \\ne 0 $$\nIf $A_{ij}=0$, then $\\tilde{L}_{ij}=0$ is enforced by the $\\mathrm{IC}(0)$ sparsity constraint.\n\nWe examine three strategies for handling a pivot $s_j  \\tau$:\n\nStrategy $S_0$ (No Stabilization): This is the baseline approach. If any $s_j  \\tau$, the factorization process is immediately terminated, and no preconditioner is generated. This strategy fails for any matrix that naturally produces small pivots during $\\mathrm{IC}(0)$.\n\nStrategy $S_1$ (Diagonal Shift): If $s_j  \\tau$, this strategy modifies the matrix $A$ to ensure the pivot is safe. The diagonal element $A_{jj}$ is increased by a minimal non-negative value $\\delta$ such that the new pivot equals $\\tau$. The original pivot is $s_j = A_{jj} - \\sum_{k=0}^{j-1} \\tilde{L}_{jk}^2$. The modified pivot is $s'_j = (A_{jj}+\\delta) - \\sum_{k=0}^{j-1} \\tilde{L}_{jk}^2 = s_j + \\delta$. Setting $s'_j = \\tau$ gives $\\delta = \\tau - s_j$. The algorithm then proceeds by setting $\\tilde{L}_{jj} = \\sqrt{\\tau}$. While this locally resolves the small pivot issue, it can lead to numerical instability. The small value of $\\tilde{L}_{jj} = \\sqrt{\\tau}$ can cause subsequent elements $\\tilde{L}_{ij}$ for $i  j$ to become very large, as $\\tilde{L}_{jj}$ appears in the denominator. A large $\\tilde{L}_{ij}$ can, in turn, cause a future pivot $s_i$ to become large and negative due to the term $-\\tilde{L}_{ij}^2$, leading to a breakdown later in the factorization.\n\nStrategy $S_2$ (Symmetric Diagonal Pivoting): If $s_j  \\tau$, this strategy attempts to reorder the factorization. It searches among the remaining rows $p \\in \\{j, \\dots, n-1\\}$ for the one that would yield the largest tentative pivot, $\\hat{s}_p = A_{pp} - \\sum_{k=0}^{j-1} \\tilde{L}_{pk}^2$. If this maximal tentative pivot is at least $\\tau$, a symmetric permutation is performed, swapping rows and columns $j$ and $p$. This brings a larger diagonal element into the current pivot position. The factorization then continues from step $j$. This reordering effectively produces an incomplete factorization of a permuted matrix $P A P^{\\top}$, where $P$ is a permutation matrix. This strategy is more robust as it avoids artificially creating small pivots and instead leverages the existing structure of the matrix to maintain stability. The sparsity pattern of $\\tilde{L}$ will conform to that of the permuted matrix.\n\nFor each test matrix $A^{(i)}$ and strategy $S_s$, we compute the indicator $b_{i,s}$. If the factorization completes, we form the preconditioner $M = \\tilde{L} \\tilde{L}^{\\top}$. We then test if $M$ is numerically SPD by attempting its Cholesky factorization. $b_{i,s}=1$ if the factorization of $A$ completes and the Cholesky factorization of $M$ succeeds; otherwise, $b_{i,s}=0$. The provided parameters are $n=6$ and $\\tau=0.05$.\n\nAnalysis of Test Cases:\n- Matrix $A^{(1)}$: This is a well-conditioned discrete Laplacian matrix. Its standard $\\mathrm{IC}(0)$ factorization is known to be very stable. The pivots are $s_0=2$, $s_1=1.5$, $s_2 \\approx 1.33$, etc., all of which are significantly larger than $\\tau=0.05$. Consequently, no stabilization is needed. All three strategies ($S_0, S_1, S_2$) will execute identically, complete successfully, and produce an SPD preconditioner $M \\approx A^{(1)}$. We expect $b_{1,S_0} = b_{1,S_1} = b_{1,S_2} = 1$.\n\n- Matrix $A^{(2)}$: This matrix is designed to have a small pivot.\n  - $j=0$: $s_0 = 1.01  \\tau$.\n  - $j=1$: $s_1 = A_{11} - \\tilde{L}_{10}^2 = 1.0001 - (-1/\\sqrt{1.01})^2 \\approx 0.010001  \\tau$.\n  - For $S_0$, this causes an immediate breakdown. $b_{2,S_0}=0$.\n  - For $S_1$, the pivot is forced to $s_1 = \\tau = 0.05$, so $\\tilde{L}_{11} = \\sqrt{0.05}$. Then, for $j=2$, we compute $\\tilde{L}_{21} = (A_{21} - 0) / \\tilde{L}_{11} = -1/\\sqrt{0.05} \\approx -4.47$. The next pivot is $s_2 = A_{22} - \\tilde{L}_{21}^2 = 1.01 - (-1/\\sqrt{0.05})^2 = 1.01 - 20 = -18.99$. This negative pivot causes a breakdown. $b_{2,S_1}=0$.\n  - For $S_2$, at $j=1$, it will search for a new pivot. The tentative pivots for $p \\ge 1$ are $\\hat{s}_p = A_{pp} - \\tilde{L}_{p0}^2$. Since only $\\tilde{L}_{10}$ is non-zero, this search effectively finds the largest diagonal element $A_{pp}$ for $p \\ge 2$. $A_{55}=1.2$ is the largest. After swapping rows/columns $1$ and $5$, the algorithm proceeds with a large, stable pivot. This swapping strategy will likely allow the factorization to complete successfully, yielding an SPD matrix $M$. Thus, $b_{2,S_2}=1$.\n\n- Matrix $A^{(3)}$: This case is even more extreme than $A^{(2)}$.\n  - $j=0$: $s_0 = 1.000001  \\tau$.\n  - $j=1$: $s_1 = A_{11} - \\tilde{L}_{10}^2 = 1.00000001 - (-1/\\sqrt{1.000001})^2 \\approx 1.01 \\times 10^{-8} \\ll \\tau$.\n  - The behavior follows the same pattern as for $A^{(2)}$. $S_0$ fails immediately ($b_{3,S_0}=0$). $S_1$ creates an even larger element $\\tilde{L}_{21} = -1/\\sqrt{0.05}$ leading to a subsequent negative pivot $s_2 = A_{22} - \\tilde{L}_{21}^2 = 1.0000011 - 20  0$, causing breakdown ($b_{3,S_1}=0$). $S_2$ will again find a large diagonal element (e.g., $A_{55}=1.7$), pivot it into position, and complete the factorization successfully ($b_{3,S_2}=1$).\n\nThe expected final result array is $[1, 1, 1, 0, 0, 1, 0, 0, 1]$. The following program implements this logic to verify the analysis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates three strategies for incomplete Cholesky factorization.\n    \"\"\"\n\n    def ic0(A, strategy, tau):\n        \"\"\"\n        Computes the Incomplete Cholesky Factorization with zero fill-in (IC(0)).\n        \n        Args:\n            A (np.ndarray): The input symmetric matrix.\n            strategy (str): The stabilization strategy ('S0', 'S1', 'S2').\n            tau (float): The pivot threshold.\n            \n        Returns:\n            np.ndarray or None: The lower triangular factor L, or None if breakdown occurs.\n        \"\"\"\n        n = A.shape[0]\n        # Work on a copy of A as it might be modified\n        A_mod = A.copy()\n        L = np.zeros((n, n))\n\n        for j in range(n):\n            # Compute pivot s_j\n            sum_sq = np.dot(L[j, :j], L[j, :j])\n            s_j = A_mod[j, j] - sum_sq\n\n            if s_j  tau:\n                if strategy == 'S0':\n                    return None  # Breakdown\n                \n                elif strategy == 'S1':\n                    # Diagonal shift strategy\n                    A_mod[j, j] += (tau - s_j)\n                    s_j = tau\n                \n                elif strategy == 'S2':\n                    # Symmetric pivoting strategy\n                    best_p = j\n                    max_pivot = s_j\n                    \n                    for p_candidate in range(j + 1, n):\n                        p_sum_sq = np.dot(L[p_candidate, :j], L[p_candidate, :j])\n                        p_pivot = A_mod[p_candidate, p_candidate] - p_sum_sq\n                        if p_pivot  max_pivot:\n                            max_pivot = p_pivot\n                            best_p = p_candidate\n                    \n                    if max_pivot  tau:\n                        return None  # Breakdown even after search\n\n                    p = best_p\n                    if p != j:\n                        # Apply symmetric permutation to A_mod\n                        A_mod[[j, p], :] = A_mod[[p, j], :]\n                        A_mod[:, [j, p]] = A_mod[:, [p, j]]\n                        # Permute already computed parts of L\n                        L[[j, p], :j] = L[[p, j], :j]\n                    \n                    s_j = max_pivot\n\n            # A non-positive pivot after stabilization means breakdown\n            if s_j = 0:\n                return None\n\n            L[j, j] = np.sqrt(s_j)\n\n            # Compute column j of L based on sparsity of A_mod\n            for i in range(j + 1, n):\n                if A_mod[i, j] != 0:\n                    sum_prod = np.dot(L[i, :j], L[j, :j])\n                    L[i, j] = (A_mod[i, j] - sum_prod) / L[j, j]\n                    \n        return L\n\n    # Define constants and test cases\n    n = 6\n    tau = 0.05\n    strategies = ['S0', 'S1', 'S2']\n\n    # Matrix A^(1)\n    A1 = np.diag([2.0] * n) + np.diag([-1.0] * (n - 1), k=1) + np.diag([-1.0] * (n - 1), k=-1)\n\n    # Matrix A^(2)\n    A2 = np.zeros((n, n))\n    np.fill_diagonal(A2, [1.01, 1.0001, 1.01, 1.02, 1.1, 1.2])\n    A2[0, 1] = A2[1, 0] = -1.0\n    A2[1, 2] = A2[2, 1] = -1.0\n    A2[2, 3] = A2[3, 2] = -1.0\n\n    # Matrix A^(3)\n    A3 = np.zeros((n, n))\n    np.fill_diagonal(A3, [1.000001, 1.00000001, 1.0000011, 1.5, 1.6, 1.7])\n    A3[0, 1] = A3[1, 0] = -1.0\n    A3[1, 2] = A3[2, 1] = -1.0\n\n    test_matrices = [A1, A2, A3]\n    results = []\n\n    for A in test_matrices:\n        for strategy in strategies:\n            b_is = 0\n            L = ic0(A, strategy, tau)\n            \n            if L is not None:\n                M = L @ L.T\n                try:\n                    # Test if M is numerically SPD\n                    np.linalg.cholesky(M)\n                    b_is = 1\n                except np.linalg.LinAlgError:\n                    b_is = 0\n            \n            results.append(b_is)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}