## 引言
在科学与工程的众多领域中，寻找“最佳”解是一个永恒的主题。无论是调整模型参数以达到最高预测精度，还是设计部件以实现最优性能，其核心都归结为[优化问题](@entry_id:266749)。一维优化，即寻找单变量函数的最值，是这一宏大领域中最基础、却也至关重要的一环。然而，在许多实际场景中，描述系统行为的函数可能极其复杂，其导数难以解析求解，甚至根本不存在。这就引出了一个关键的知识缺口：当微积分的利器失效时，我们该如何系统、高效地找到最优解？

本文旨在系统性地解答这一问题。我们将带领读者深入探索一维优化中功能强大且应用广泛的“无导数”方法。文章将分为三个章节，层层递进：

- 在“**原理与机制**”中，我们将从第一性原理出发，揭示[无导数优化](@entry_id:137673)的核心思想——区间收缩法，并详细推导[黄金分割搜索](@entry_id:146661)（Golden-Section Search）算法的精妙机制，分析其鲁棒性和理论性质。
- 在“**应用与跨学科联系**”中，我们将展示该方法如何在机器学习、信号处理、工程设计等多个学科中作为直接工具解决参数整定问题，并探讨其作为“线搜索”子程序在驱动复杂[多维优化](@entry_id:147413)算法中的基础性作用。
- 最后，在“**动手实践**”部分，你将有机会通过一系列编码练习，将理论知识转化为实际技能，亲手实现并扩展[黄金分割搜索](@entry_id:146661)算法。

通过本文的学习，你将不仅掌握一种具体的优化算法，更能建立起从理论分析到应用实践的完整知识体系。让我们首先从一维优化的基本原理开始。

## 原理与机制

在单变量[优化问题](@entry_id:266749)中，我们的目标是找到一个函数 $f(x)$ 在给定区间上的最小值。当函数的导数信息难以获取或不存在时，我们必须依赖一种不依赖于梯度或更高阶导数的策略。本章将深入探讨这类“无导数”[优化方法](@entry_id:164468)背后的核心原理，重点介绍[黄金分割搜索](@entry_id:146661)（Golden-Section Search, GSS）算法，分析其机制、性质，并将其与其他经典优化算法进行比较。

### [无导数优化](@entry_id:137673)的基础：区间收缩原理

[无导数优化](@entry_id:137673)算法的核心思想是通过一系列函数求值来系统地缩小包含最小值的搜索区间。这种策略的理论基石是一个关键的函数属性：**单峰性 (unimodality)**。

一个函数 $f(x)$ 在区间 $[a, b]$ 上被称为**[单峰函数](@entry_id:143107)**，如果存在唯一的[最小值点](@entry_id:634980) $x^{\star} \in [a, b]$，使得当 $x \in [a, x^{\star}]$ 时函数是单调递减的，而当 $x \in [x^{\star}, b]$ 时函数是单调递增的。这个定义也包含了函数在整个区间上单调的情况，此时[最小值点](@entry_id:634980)会出现在区间的端点。

单峰性是区间[收缩方法](@entry_id:167472)得以成立的保证。如果我们只在区间内的一个点进行采样，我们将无法判断最小值位于该点的左侧还是右侧。因此，我们至少需要在区间内部选取两个不同的点，比如 $c$ 和 $d$，且满足 $a < c < d < b$。通过比较这两个点上的函数值 $f(c)$ 和 $f(d)$，我们就可以排除一部分不包含最小值的区间：

1.  如果 $f(c) < f(d)$，由于函数的单峰性，最小值点 $x^{\star}$ 不可能位于区间 $[d, b]$ 内。因为如果 $x^{\star} \in [d, b]$，那么 $c$ 和 $d$ 都将处于函数的单调递减部分，这将导致 $f(c) > f(d)$，与我们的观察相矛盾。因此，我们可以安全地将新的搜索区间缩小为 $[a, d]$。

2.  如果 $f(c) \ge f(d)$，同理，[最小值点](@entry_id:634980) $x^{\star}$ 不可能位于区间 $[a, c]$ 内。因此，新的搜索区间可以缩小为 $[c, b]$。

这个简单的逻辑构成了所有基于区间收缩的搜索算法的基础。然而，这种方法的效率关键在于如何策略性地选择内部点 $c$ 和 $d$。一个直接的想法是**二分搜索 (dichotomous search)**，即在区间中点 $(a+b)/2$ 的两侧各取一个非常接近的点。例如，在 $[(a+b)/2 - \delta, (a+b)/2 + \delta]$ 进行采样，其中 $\delta$ 是一个很小的正数。这种方法在每次迭代中几乎将区间长度减半，但它的一个主要缺点是，每次迭代都需要进行两次全新的函数求值，因为上一轮的采样点在新的、缩小的区间内没有特殊的位置关系可以被再次利用 。当函数求值成本高昂时，这会大大降低算法的效率。

因此，一个更深刻的问题是：我们能否设计一种[采样策略](@entry_id:188482)，使得每次迭代只需要进行一次新的函数求值？[黄金分割搜索](@entry_id:146661)正是对这个问题的一个优雅回答。

### [黄金分割搜索](@entry_id:146661) (GSS) 算法

[黄金分割搜索](@entry_id:146661)通过一种巧妙的几何布局，实现了每次迭代仅需一次新增函数求值的目标，从而在[无导数优化](@entry_id:137673)方法中达到了理论上的最优效率。

#### 从第一性原理推导

让我们来推导这种高效的[采样策略](@entry_id:188482)。假设在归一化区间 $[0, 1]$ 上，我们对称地选择两个内部点 $x_1$ 和 $x_2 = 1 - x_1$。为了方便，我们设 $x_1 < x_2$。

假设一次比较后，$f(x_1) < f(x_2)$。根据单峰原理，新的搜索区间是 $[0, x_2]$。在这个新区间里，原来的点 $x_1$ 仍然存在。为了实现求值重用，我们希望这个旧点 $x_1$ 能恰好成为新区间 $[0, x_2]$ 中的两个标准采样点之一。如果我们保持采样点的相对位置比例不变，那么在新区间 $[0, x_2]$（长度为 $x_2$）内，两个新的采样点应该位于 $x_1 \cdot x_2$ 和 $x_2 \cdot x_2$。为了重用 $f(x_1)$，我们必须让 $x_1$ 的位置与这两个新采样点之一重合。

由于 $x_1$ 是原区间中靠左的点，它在新区间中也相对靠左。因此，一个合理的假设是，它将成为新区间中那个靠右的采样点（因为另一个旧点被丢弃了）。用数学语言来说，我们将旧点 $x_1$ 在新区间 $[0, x_2]$ 中重新归一化的坐标 $\frac{x_1 - 0}{x_2 - 0} = \frac{x_1}{x_2}$ 设为原先的靠右坐标 $x_2$。

这给出了关系 $\frac{x_1}{x_2} = x_2$。结合对称性条件 $x_1 = 1 - x_2$，我们得到：
$$
\frac{1 - x_2}{x_2} = x_2 \implies 1 - x_2 = x_2^2
$$
整理后得到一个[二次方程](@entry_id:163234)：
$$
x_2^2 + x_2 - 1 = 0
$$
求解这个方程并取[正根](@entry_id:199264)（因为 $x_2$ 是区间内的坐标），我们得到：
$$
x_2 = \frac{-1 + \sqrt{1^2 - 4(1)(-1)}}{2} = \frac{\sqrt{5} - 1}{2} \approx 0.618034
$$
这个值正是[黄金比例](@entry_id:139097) $\phi = \frac{1+\sqrt{5}}{2}$ 的倒数，通常用 $\tau$ 或 $\rho$ 表示。相应地，$x_1 = 1 - x_2 = \frac{3-\sqrt{5}}{2} \approx 0.381966$，可以验证 $x_1 = x_2^2$。

这个推导表明，如果我们在每次迭代中都将采样点放置在当前区间长度的 $0.382$ 和 $0.618$ 比例处，那么在区间收缩后，总有一个旧的采样点会完美地落在新区间的一个采样位置上，从而其函数值可以被直接重用  。这正是[黄金分割搜索](@entry_id:146661)名称的由来和其高效性的根本原因。

#### 算法步骤

[黄金分割搜索](@entry_id:146661)的完整算法如下：

1.  **初始化**：给定一个包含最小值的单峰区间 $[a, b]$ 和一个[收敛容差](@entry_id:635614) $\tau_{tol}$。计算黄金分割常数 $\tau = \frac{\sqrt{5} - 1}{2}$。
2.  **首次采样**：计算两个内部点：
    $c = b - \tau (b - a)$
    $d = a + \tau (b - a)$
    并求其函数值 $f(c)$ 和 $f(d)$。这需要两次函数求值。
3.  **迭代收缩**：进入循环，直到 $b - a \le \tau_{tol}$：
    a.  **比较**：
        - 如果 $f(c) < f(d)$，则新区间为 $[a, d]$。更新 $b \leftarrow d$，并将旧点 $c$ 作为新区间中的“右侧”点，即 $d \leftarrow c, f(d) \leftarrow f(c)$。然后，只需计算新的“左侧”点 $c = b - \tau(b-a)$ 及其函数值 $f(c)$。
        - 如果 $f(c) \ge f(d)$，则新区间为 $[c, b]$。更新 $a \leftarrow c$，并将旧点 $d$ 作为新区间中的“左侧”点，即 $c \leftarrow d, f(c) \leftarrow f(d)$。然后，只需计算新的“右侧”点 $d = a + \tau(b-a)$ 及其函数值 $f(d)$。
    b.  每次循环仅需**一次**新的函数求值。
4.  **终止**：当区间长度小于容差时，循环结束。最终的[最小值点](@entry_id:634980)可以近似为最终区间的终点 $(a+b)/2$。

#### 内存需求

从算法机制中我们可以分析其对内存的最低要求。在每次迭代的比较步骤中，我们都需要同时知道 $f(c)$ 和 $f(d)$ 的值。这意味着我们至少需要两个内存单元来存储函数值。当[区间更新](@entry_id:634829)时，例如在 $f(c) < f(d)$ 的情况下， $f(d)$ 的值变得不再需要，而 $f(c)$ 的值被保留下来。此时，一个内存单元被“释放”，用于存储下一次新计算的函数值。因此，在整个迭代过程中，我们最多只需要同时存储两个函数值。这个从第一性原理出发的思考实验表明，实现[黄金分割搜索](@entry_id:146661)所需的最小函数值存储容量为 $K=2$ 。

### GSS的实际实现与性质

#### 区间初始化：从单点猜测开始

[黄金分割搜索](@entry_id:146661)的一个前提是需要一个已知的包含最小值的单峰区间。但在实际应用中，我们往往只有一个对最小值点的初步猜测 $x_0$。此时，我们需要一个系统性的方法来从这个单点猜测出发，生成一个有效的初始区间。

一种常见的策略是**扩展搜索 (expansion search)** 。该方法从 $x_0$ 开始，沿着函数值下降的方向以指数级增长的步长进行探索，直到函数值出现回升，从而形成一个三点模式 $(a, b, c)$，满足 $a < b < c$ 且 $f(b) < f(a)$ 和 $f(b) < f(c)$。这个三点模式就构成了一个有效的初始括号区间 $[a, c]$。具体步骤如下：
1. 从 $x_0$ 和一个初始小步长 $s_0$ 开始。
2. 比较 $f(x_0)$ 和 $f(x_0+s_0)$。
3. 如果 $f(x_0+s_0) < f(x_0)$，说明下降方向在右侧。我们设置前两个点为 $x_{k-1}=x_0$ 和 $x_k=x_0+s_0$，然后以一个增长因子（如[黄金比例](@entry_id:139097) $\phi$）生成下一个点 $x_{k+1} = x_k + \phi(x_k - x_{k-1})$。
4. 重复此过程，直到找到一个点 $x_{k+1}$ 使得 $f(x_{k+1}) > f(x_k)$。此时，点集 $\{x_{k-1}, x_k, x_{k+1}\}$ 就构成了我们需要的括号。
5. 如果在探索过程中碰到定义域边界，则以[边界点](@entry_id:176493)作为括号的端点。

#### 算法的鲁棒性

[黄金分割搜索](@entry_id:146661)的一个显著优点是其强大的鲁棒性。由于其决策完全基于函数值的[序数](@entry_id:150084)比较（即哪个更大），而不依赖于函数值的具体大小或其导数，GSS能够稳健地处理各种复杂的函数形态。

- **处理[非光滑函数](@entry_id:175189)**：对于连续但在某些点上导数不存在的函数（例如带有“[尖点](@entry_id:636792)”或“拐角”的函数），GSS依然有效。例如，函数 $f(x) = |x-1|$ 在 $x=1$ 处不可导，但它在任何包含 $1$ 的区间上都是单峰的。GSS可以毫无困难地收敛到其最小值点 $x=1$ 。同样，对于像  中定义的分段[光滑函数](@entry_id:267124)，只要整体保持单峰性，GSS就能跨越导数不连续点并准确找到最小值。

- **处理边界最小值**：如果最小值恰好位于区间的端点，例如在一个单调函数上，GSS仍然能够正确收敛。它会不断地将区间向包含最小值的端点收缩。然而，标准GSS在这种情况下可能效率不高，因为它会执行完整的迭代次数来将区间缩小到容差范围内。一种改进方法是引入**[边界检查](@entry_id:746954)启发式** 。在每次迭代开始时，检查当前区间的端点函数值是否已经小于内部采样点的函数值。如果是，就可以提前终止搜索并返回该端点作为最小值，从而在边界最小值的情况下节省大量函数求值次数。

#### [停止准则](@entry_id:136282)的考量

选择合适的[停止准则](@entry_id:136282)是保证算法性能和准确性的关键。
- **区间长度准则**：最常用和最稳健的[停止准则](@entry_id:136282)是当搜索区间长度 $|b-a|$ 小于一个预设的绝对容差 $\tau_x$ 时停止。这种方法直接控制了最小值点位置的不确定性。
- **函数值改进准则**：另一种选择是基于函数值的改进。当连续两次迭代中最佳函数值的下降量小于某个容差 $\varepsilon_f$ 时，算法停止。这种准则在某些情况下很有用，但存在一个严重的陷阱：对于在最小值附近特别平坦的函数（例如 $f(x) = |x-c|^{2m}$ 当 $m$ 很大时），函数值的变化可能在区间还很宽的时候就因[浮点精度](@entry_id:138433)限制而变得极小甚至为零。这会导致算法过早终止，远未达到所需的位置精度 。因此，在使用函数值改进准则时必须格外小心，通常建议与区间长度准则结合使用。

#### [标度不变性](@entry_id:180291)

[黄金分割搜索](@entry_id:146661)具有**标度不变性 (scale-invariance)** 。这意味着如果对自变量进行线性变换 $y = sx$（其中 $s>0$），例如从米到厘米的[单位换算](@entry_id:136593)，GSS在变换后的[坐标系](@entry_id:156346)中执行的归一化决策序列与原[坐标系](@entry_id:156346)完全相同。这是因为GSS的每一步决策都基于区间长度的**比率**和函数值的**[序数](@entry_id:150084)**关系，这两者都不会因为[线性标度](@entry_id:197235)变换而改变。这使得GSS成为一种在物理和工程问题中特别有吸[引力](@entry_id:175476)的方法，因为其性能与参数的物理单位选择无关。

### 比较分析：GSS在[优化算法](@entry_id:147840)体系中的位置

为了更全面地理解GSS，有必要将其与一维优化中的其他经典方法进行比较。

#### GSS vs. 基于导数的方法

- **GSS vs. 基于 $f'(x)$ 的二分法**：如果函数 $f(x)$ 是严格凸的且可微，那么其最小值点 $x^{\star}$ 是其导数 $f'(x)$ 的唯一零点。因此，我们可以通过在 $f'(x)$ 上使用[二分法](@entry_id:140816)来求解 $f'(x)=0$。由于 $f(x)$ 严格凸，$f'(x)$ 是严格单调递增的，这保证了[二分法](@entry_id:140816)的适用性 。

    - *收敛速度*：[二分法](@entry_id:140816)每次迭代将区间长度减半（收敛因子为 $0.5$），而GSS的收敛因子约为 $0.618$。因此，在迭代次数上，二分法收敛更快。
    - *鲁棒性*：GSS的优势在于其不依赖导数。当 $f'(x)$ 不存在、计算成本高或存在噪声时，GSS是唯一可行的选择。例如，如果对 $f'(x)$ 的求值有符号错误，二分法可能会丢弃包含根的区间，导致灾难性失败；而GSS只要 $f(x)$ 的求值准确，就不会受 $f'(x)$ 噪声的影响。

- **GSS vs. 牛顿法**：牛顿法是一种二阶方法，它使用[泰勒展开](@entry_id:145057)来构建一个二次模型，并直接跳到该模型的最小值点。其迭代公式为 $x_{k+1} = x_k - f'(x_k)/f''(x_k)$。

    - *收敛速度*：对于表现良好的函数（例如在[最小值点](@entry_id:634980)附近 $f''(x^{\star}) > 0$），[牛顿法](@entry_id:140116)具有二次收敛性，速度远超GSS的[线性收敛](@entry_id:163614)。对于二次函数 $f(x)=(x-c)^2$，牛顿法从任何起点出发都可以在一次迭代中精确找到最小值 。
    - *鲁棒性*：[牛顿法](@entry_id:140116)的强大威力伴随着脆弱性。它需要一阶和[二阶导数](@entry_id:144508)，这在很多实际问题中是不可行的。它对初始点敏感，并且当 $f''(x_k)$ 接近零（即函数在某点附近很平坦）时，更新步长会变得非常大，导致不稳定。此外，与二分法类似，导数中的噪声会严重破坏其收敛性，可能导致解在真实最小值附近“[抖动](@entry_id:200248)”而无法收敛到高精度 。

总结而言，GSS代表了一种**“慢而稳”**的策略。它不追求最快的[收敛速度](@entry_id:636873)，但它以极低的假设要求（仅需单峰性）和对函数形态、噪声及不[可微性](@entry_id:140863)的高度容忍，提供了无与伦比的可靠性。在许多复杂的科学与工程计算中，当函数求值本身就是一个复杂的模拟过程时，保证收敛的GSS往往是比那些看似更快但可能失败的更高阶方法更明智的选择。