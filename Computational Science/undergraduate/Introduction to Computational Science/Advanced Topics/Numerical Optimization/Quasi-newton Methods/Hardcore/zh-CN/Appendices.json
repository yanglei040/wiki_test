{
    "hands_on_practices": [
        {
            "introduction": "理解复杂方法通常始于将其简化。本练习将揭示我们熟悉的用于求根的割线法与用于优化的拟牛顿法之间的根本联系。通过推导一维情况下的更新公式，你将亲眼看到拟牛顿法的核心——割线条件——是如何自然而然地产生的 。",
            "id": "2195876",
            "problem": "考虑一个最小化二阶连续可微函数 $f(x)$ 的一维无约束优化问题，其中 $x$ 是一个标量变量。我们采用拟牛顿法来寻找最小值。对于一般多维情况，迭代更新规则由下式给出：\n$$x_{k+1} = x_k - (B_k)^{-1} \\nabla f(x_k)$$\n其中 $x_k$ 是最小值的当前估计值，$\\nabla f(x_k)$ 是函数在 $x_k$ 处的梯度，而 $B_k$ 是在 $x_k$ 处的海森矩阵的近似。\n\n在我们的一维情况下，梯度 $\\nabla f(x)$ 变为一阶导数 $f'(x)$，海森矩阵变为二阶导数 $f''(x)$。因此，近似值 $B_k$ 是一个标量，我们将其记为 $b_k$。这个标量 $b_k$ 在每一步都会更新。\n\n假设我们处于第 $k$ 次迭代，已知上一个迭代点 $x_{k-1}$ 和当前迭代点 $x_k$。$b_k$ 的值通过一维割线条件利用这两点的信息来确定：\n$$b_k (x_k - x_{k-1}) = f'(x_k) - f'(x_{k-1})$$\n\n推导下一个迭代点 $x_{k+1}$ 的表达式。你的最终答案应表示为 $x_k$、$x_{k-1}$ 以及函数在这些点上的一阶导数 $f'$ 的形式。",
            "solution": "我们考虑一维拟牛顿更新，在标量情况下可写为\n$$\nx_{k+1} = x_k - b_k^{-1} f'(x_k),\n$$\n其中 $b_k$ 是第 $k$ 次迭代时二阶导数的标量近似。一维的割线条件是\n$$\nb_k (x_k - x_{k-1}) = f'(x_k) - f'(x_{k-1}).\n$$\n对此求解 $b_k$ 可得\n$$\nb_k = \\frac{f'(x_k) - f'(x_{k-1})}{x_k - x_{k-1}},\n$$\n条件是 $x_k \\neq x_{k-1}$ 且 $f'(x_k) \\neq f'(x_{k-1})$。因此，\n$$\nb_k^{-1} = \\frac{x_k - x_{k-1}}{f'(x_k) - f'(x_{k-1})}.\n$$\n将此代入更新公式可得\n$$\nx_{k+1} = x_k - \\frac{x_k - x_{k-1}}{f'(x_k) - f'(x_{k-1})} \\, f'(x_k)\n= x_k - \\frac{f'(x_k)\\,(x_k - x_{k-1})}{f'(x_k) - f'(x_{k-1})}.\n$$\n这正是应用于求根问题 $f'(x)=0$ 的割线法更新公式。",
            "answer": "$$\\boxed{x_k - \\frac{f'(x_k)\\,(x_k - x_{k-1})}{f'(x_k) - f'(x_{k-1})}}$$"
        },
        {
            "introduction": "每个迭代优化算法都需要一个起点和第一步。本练习将带你计算一个典型的拟牛顿算法的初始搜索方向 。你会发现，当初始Hessian矩阵近似被选为单位矩阵时，第一步实际上就是最速下降方向，这为后续更复杂的迭代更新奠定了基础。",
            "id": "2195903",
            "problem": "采用一种迭代优化算法来寻找二元函数 $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$ 的一个局部最小值。该算法在点 $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$ 处初始化。在第一步（迭代 $k=0$）中，通过求解线性方程组 $B_0 p_0 = -\\nabla f(x_0)$ 来计算搜索方向 $p_0$，其中 $\\nabla f(x_0)$ 是 $f$ 在 $x_0$ 处的梯度，而 $B_0$ 是Hessian矩阵的一个近似。对于此过程，初始近似被选为 $2 \\times 2$ 的单位矩阵 $I$。确定初始搜索方向向量 $p_0$ 的分量。",
            "solution": "问题要求解初始搜索方向向量 $p_0$，它是线性系统 $B_0 p_0 = -\\nabla f(x_0)$ 的解。我们可以通过以下三个主要步骤来解决这个问题：首先，计算函数 $f(x_1, x_2)$ 的梯度；其次，在初始点 $x_0$ 处计算该梯度；第三，求解给定的线性系统以得到 $p_0$。\n\n步骤1：计算函数的梯度。\n函数由 $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$ 给出。\n$f$ 的梯度，记作 $\\nabla f$，是其偏导数的向量：\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix}\n$$\n关于 $x_1$ 的偏导数是：\n$$\n\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (\\sin(x_1) + \\cosh(x_2)) = \\cos(x_1)\n$$\n关于 $x_2$ 的偏导数是：\n$$\n\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (\\sin(x_1) + \\cosh(x_2)) = \\sinh(x_2)\n$$\n因此，梯度向量是：\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\cos(x_1) \\\\ \\sinh(x_2) \\end{pmatrix}\n$$\n\n步骤2：在初始点 $x_0$ 处计算梯度。\n初始点给定为 $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$。我们将这些值代入梯度表达式中：\n$$\n\\nabla f(x_0) = \\nabla f(0, \\ln(2)) = \\begin{pmatrix} \\cos(0) \\\\ \\sinh(\\ln(2)) \\end{pmatrix}\n$$\n我们计算每个分量。0的余弦是：\n$$\n\\cos(0) = 1\n$$\n双曲正弦函数的定义为 $\\sinh(y) = \\frac{\\exp(y) - \\exp(-y)}{2}$。对于 $y = \\ln(2)$：\n$$\n\\sinh(\\ln(2)) = \\frac{\\exp(\\ln(2)) - \\exp(-\\ln(2))}{2} = \\frac{2 - \\exp(\\ln(2^{-1}))}{2} = \\frac{2 - \\frac{1}{2}}{2} = \\frac{\\frac{3}{2}}{2} = \\frac{3}{4}\n$$\n所以，在初始点的梯度向量是：\n$$\n\\nabla f(x_0) = \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix}\n$$\n\n步骤3：求解线性系统以得到 $p_0$。\n需要求解的系统是 $B_0 p_0 = -\\nabla f(x_0)$。我们已知 $B_0$ 是 $2 \\times 2$ 的单位矩阵 $I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。\n该系统变为：\n$$\nI p_0 = -\\nabla f(x_0)\n$$\n任何向量乘以单位矩阵都保持向量不变，所以 $p_0 = -\\nabla f(x_0)$。\n代入我们求得的梯度值：\n$$\np_0 = - \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}\n$$\n因此，初始搜索方向向量 $p_0$ 的分量是 $-1$ 和 $-\\frac{3}{4}$。",
            "answer": "$$\\boxed{\\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}}$$"
        },
        {
            "introduction": "从理论走向大规模应用，我们必须考虑计算资源的限制，这催生了限制内存的BFGS（L-BFGS）算法。本练习通过一个精心设计的场景，让你具体地看到L-BFGS的有限内存是如何“遗忘”早期迭代中捕获的关键曲率信息的 。通过将L-BFGS的搜索方向与精确的牛顿方向进行比较，你将能更深刻地理解大规模优化中的性能权衡。",
            "id": "3264848",
            "problem": "你需要构建一个严格凸二次目标函数，以揭示有限内存Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) 方法因遗忘早期迭代的曲率信息而产生的影响，然后计算L-BFGS搜索方向与牛顿搜索方向之间的量化差异。\n\n考虑由下式定义的函数 $f : \\mathbb{R}^{2} \\to \\mathbb{R}$\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} Q x, \n\\quad \\text{其中 } Q \\;=\\; \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}.\n$$\n因此，$f$ 是严格凸的，并且在标准方向 $e_{1}$ 和 $e_{2}$ 上具有各向异性的曲率。设起始点为 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，并假设有限内存拟牛顿法的两次精确线搜索迭代产生以下位移：\n$$\ns_{0} \\;=\\; x_{1} - x_{0} \\;=\\; e_{1}, \n\\qquad\ns_{1} \\;=\\; x_{2} - x_{1} \\;=\\; e_{2}.\n$$\n对于这个二次目标函数，其梯度为 $\\nabla f(x) = Q x$，因此相应的曲率对为\n$$\ny_{0} \\;=\\; \\nabla f(x_{1}) - \\nabla f(x_{0}) \\;=\\; Q s_{0},\n\\qquad\ny_{1} \\;=\\; \\nabla f(x_{2}) - \\nabla f(x_{1}) \\;=\\; Q s_{1}.\n$$\n\n现在，在迭代 $k = 2$ 时应用内存参数为 $m = 1$ 的L-BFGS方法，仅使用最新的曲率对 $(s_{1}, y_{1})$，并结合L-BFGS常用的与最新曲率对一致的标量初始逆Hessian矩阵 $H_{2}^{(0)} = \\gamma_{2} I$。对于上述数据，这意味着 $H_{2}^{(0)} = I$。计算以下两个方向之间夹角的余弦值：\n- L-BFGS搜索方向 $p_{\\mathrm{LB}} = - H_{2} \\nabla f(x_{2})$，其中 $H_{2}$ 是通过对 $H_{2}^{(0)} = I$ 使用曲率对 $(s_{1}, y_{1})$ 进行单次 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 更新得到的逆Hessian近似矩阵，以及\n- 在 $x_{2}$ 处的精确牛顿方向 $p_{\\mathrm{N}} = - Q^{-1} \\nabla f(x_{2})$。\n\n请以单个精确表达式的形式给出你的答案。无需四舍五入。",
            "solution": "我们的目标是计算在迭代 $k=2$ 时，L-BFGS搜索方向 $p_{\\mathrm{LB}}$ 与牛顿搜索方向 $p_{\\mathrm{N}}$ 之间夹角的余弦值。\n\n首先，我们确定点 $x_2$ 以及该点处的梯度 $\\nabla f(x_2)$。\n起始点为 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n第一次迭代后的点是 $x_1 = x_0 + s_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n第二次迭代后的点是 $x_2 = x_1 + s_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n函数 $f(x) = \\frac{1}{2} x^{\\top} Q x$ 的梯度是 $\\nabla f(x) = Qx$。\n在 $x_2$ 处，梯度为 $\\nabla f(x_2) = Q x_2 = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$。\n\n接下来，我们计算精确牛顿方向 $p_{\\mathrm{N}}$。\n牛顿方向由 $p_{\\mathrm{N}} = -(\\nabla^2 f(x_2))^{-1} \\nabla f(x_2)$ 给出。对于此二次函数，Hessian矩阵是常数，$\\nabla^2 f(x) = Q$。\n$Q$ 的逆矩阵是 $Q^{-1} = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1/3  0 \\\\ 0  1 \\end{pmatrix}$。\n所以，牛顿方向是：\n$$\np_{\\mathrm{N}} = -Q^{-1} \\nabla f(x_2) = - \\begin{pmatrix} 1/3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}\n$$\n\n现在，我们计算L-BFGS方向 $p_{\\mathrm{LB}}$。\n在迭代 $k=2$ 且内存为 $m=1$ 时，L-BFGS方法仅使用最新的曲率对 $(s_1, y_1)$ 来构造逆Hessian近似矩阵 $H_2$。该过程从一个初始矩阵 $H_2^{(0)}$ 开始，其缩放因子 $\\gamma_2 = \\frac{s_1^{\\top} y_1}{y_1^{\\top} y_1}$。\n我们有 $s_1 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 和 $y_1 = Q s_1 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n因此，$s_1^{\\top} y_1 = 1$ 且 $y_1^{\\top} y_1 = 1$，所以 $\\gamma_2 = 1$。初始矩阵为 $H_2^{(0)} = I$。\n逆Hessian矩阵的BFGS更新公式为：\n$$\nH_2 = \\left(I - \\rho_1 s_1 y_1^{\\top}\\right) H_2^{(0)} \\left(I - \\rho_1 y_1 s_1^{\\top}\\right) + \\rho_1 s_1 s_1^{\\top}\n$$\n其中 $\\rho_1 = \\frac{1}{y_1^{\\top} s_1} = 1$。\n因为初始矩阵 $H_2^{(0)}=I$ 已经满足了曲率对 $(s_1, y_1)$ 的割线条件（即 $H_2^{(0)} y_1 = I y_1 = y_1 = s_1$），BFGS更新不会改变该矩阵。因此，$H_2 = H_2^{(0)} = I$。\nL-BFGS方向是：\n$$\np_{\\mathrm{LB}} = -H_2 \\nabla f(x_2) = -I \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -1 \\end{pmatrix}\n$$\n这表明L-BFGS算法由于内存限制遗忘了在 $e_1$ 方向上捕获的曲率信息，其搜索方向退化为最速下降方向。\n\n最后，我们使用公式 $\\cos(\\theta) = \\frac{p_{\\mathrm{LB}}^{\\top} p_{\\mathrm{N}}}{\\|p_{\\mathrm{LB}}\\| \\|p_{\\mathrm{N}}\\|}$ 计算两个方向之间夹角的余弦值。\n$$\np_{\\mathrm{LB}} = \\begin{pmatrix} -3 \\\\ -1 \\end{pmatrix}, \\quad p_{\\mathrm{N}} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}\n$$\n点积为：\n$$\np_{\\mathrm{LB}}^{\\top} p_{\\mathrm{N}} = (-3)(-1) + (-1)(-1) = 3 + 1 = 4\n$$\n范数为：\n$$\n\\|p_{\\mathrm{LB}}\\| = \\sqrt{(-3)^2 + (-1)^2} = \\sqrt{9 + 1} = \\sqrt{10}\n$$\n$$\n\\|p_{\\mathrm{N}}\\| = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{1 + 1} = \\sqrt{2}\n$$\n夹角的余弦值为：\n$$\n\\cos(\\theta) = \\frac{4}{\\sqrt{10} \\sqrt{2}} = \\frac{4}{\\sqrt{20}} = \\frac{4}{\\sqrt{4 \\cdot 5}} = \\frac{4}{2\\sqrt{5}} = \\frac{2}{\\sqrt{5}}\n$$",
            "answer": "$$\n\\boxed{\\frac{2}{\\sqrt{5}}}\n$$"
        }
    ]
}