{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how Genetic Algorithms work, we begin by isolating their primary engine: selection. This practice strips away the complexities of crossover and mutation to focus solely on how a fitter individual propagates through a population. By simulating this 'takeover' process and comparing it to a classic mathematical approximation, you will develop a foundational intuition for selection pressure and the inherent stochasticity of evolutionary processes. ",
            "id": "3137411",
            "problem": "Consider a selection-only model of a Genetic Algorithm (GA), defined here as a Genetic Algorithm (GA) that generates each new generation solely by fitness-proportionate selection without crossover, mutation, or other operators. The population consists of two types: a single best individual and all other individuals. Let the population size be $N$, and let the selection intensity of the best individual be $s$, meaning the best individual has fitness $s$ while all other individuals have fitness $1$. The initial population contains $1$ best individual and $N-1$ others.\n\nAt generation $t$, let $k_t$ denote the number of best individuals. Under fitness-proportionate selection, the probability $p_t$ that a randomly selected offspring comes from the best type is given by the total fitness contribution of the best type divided by the total population fitness. The next generation has $N$ individuals sampled with replacement from the previous generation, so the number of best individuals $k_{t+1}$ is a random variable with a binomial distribution with $N$ trials and success probability $p_t$.\n\nDefine the takeover time $T$ as the smallest nonnegative integer $t$ such that $k_t = N$ (that is, the entire population consists of the best individual). Due to stochastic sampling, the best individual can go extinct, which implies $k_t = 0$ for all subsequent generations; in that case, the takeover time is undefined. For this problem, define the estimated takeover time as the conditional expectation of $T$ given that takeover occurs, approximated by Monte Carlo simulation.\n\nYour task is to:\n- Implement a simulation that, for given $(N,s)$, performs $R$ independent runs. Each run starts with $k_0 = 1$ and evolves by repeated fitness-proportionate sampling until either $k_t = N$ (takeover), $k_t = 0$ (extinction), or a preset generation cap is reached. For each run that reaches takeover, record the takeover time. Estimate the conditional expected takeover time by averaging the takeover times across successful runs.\n- Compute a deterministic approximation for the takeover time that arises from assuming multiplicative growth of the expected count of best individuals while the fraction of best individuals is small.\n- For each test case, report the absolute difference between the Monte Carlo estimate and the deterministic approximation, rounded to three decimal places.\n\nUse the following test suite:\n1. $(N,s,R,\\text{max\\_gen},\\text{seed}) = (100, 1.5, 500, 1000, 42)$\n2. $(N,s,R,\\text{max\\_gen},\\text{seed}) = (100, 1.01, 500, 10000, 43)$\n3. $(N,s,R,\\text{max\\_gen},\\text{seed}) = (50, 2.0, 300, 200, 44)$\n4. $(N,s,R,\\text{max\\_gen},\\text{seed}) = (10, 1.5, 1000, 200, 45)$\n5. $(N,s,R,\\text{max\\_gen},\\text{seed}) = (200, 1.2, 300, 3000, 46)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the absolute error for the corresponding test case. If no runs in a test case achieve takeover within the generation cap, output the floating-point value $\\mathrm{nan}$ for that case. For example, the final output should look like $[e_1,e_2,e_3,e_4,e_5]$ where each $e_i$ is a floating-point number rounded to three decimal places.",
            "solution": "The user's problem statement has been validated and is determined to be sound. It is scientifically grounded in the principles of population genetics and computational science, specifically the Wright-Fisher model with selection. The problem is well-posed, with all necessary parameters and definitions provided for both the Monte Carlo simulation and the deterministic approximation. The objective is clear and formal. Therefore, I will now provide a complete solution.\n\nThe problem requires a comparison between a stochastic simulation of an evolutionary process and a simplified deterministic model of the same process. The goal is to compute the absolute difference between the takeover time estimated from a Monte-Carlo simulation and the takeover time predicted by a deterministic approximation.\n\nLet us define the parameters and variables as given:\n- $N$: The total population size.\n- $s$: The fitness of the \"best\" individual type, where $s > 1$. The fitness of all other individuals is $1$.\n- $k_t$: The number of best individuals in the population at generation $t$. The initial state is $k_0 = 1$.\n- $p_t$: The probability of selecting an offspring from the best type in generation $t$.\n\nThe core of the model lies in the transition from generation $t$ to $t+1$. The probability $p_t$ is determined by the proportion of total fitness contributed by the $k_t$ best individuals:\n$$\np_t = \\frac{k_t \\cdot s}{k_t \\cdot s + (N - k_t) \\cdot 1} = \\frac{s k_t}{s k_t + N - k_t}\n$$\nThe next generation is formed by $N$ independent samples with replacement from the current generation. Therefore, the number of best individuals in the next generation, $k_{t+1}$, follows a binomial distribution with $N$ trials and success probability $p_t$:\n$$\nk_{t+1} \\sim \\text{Binomial}(N, p_t)\n$$\n\nThe solution involves three main steps: deriving the deterministic approximation, outlining the Monte Carlo simulation, and calculating the final error.\n\n**1. Deterministic Approximation ($T_{det}$)**\n\nThe deterministic model approximates the stochastic process by tracking the expected number of individuals, $E[k_t]$. The expected value of $k_{t+1}$ given $k_t$ is $E[k_{t+1} | k_t] = N p_t$.\n$$\nE[k_{t+1} | k_t] = N \\left( \\frac{s k_t}{s k_t + N - k_t} \\right)\n$$\nThe problem specifies that the approximation should assume multiplicative growth while the fraction of best individuals, $x_t = k_t/N$, is small. When $x_t \\ll 1$, we can approximate the denominator:\n$$\ns k_t + N - k_t = N(1 + (s-1)\\frac{k_t}{N}) \\approx N\n$$\nSubstituting this into the expectation gives a simplified recurrence relation for the expected count, which we denote $\\bar{k}_t$:\n$$\n\\bar{k}_{t+1} \\approx N \\left( \\frac{s \\bar{k}_t}{N} \\right) = s \\bar{k}_t\n$$\nThis is a geometric progression. Starting with the initial condition $k_0 = 1$, the expected number of best individuals at generation $t$ grows exponentially:\n$$\n\\bar{k}_t \\approx k_0 s^t = s^t\n$$\nTakeover is defined as the point where the entire population consists of best individuals, i.e., when the count reaches $N$. We solve for the time $t$ required to satisfy this condition in our continuous approximation:\n$$\ns^t = N\n$$\nTaking the natural logarithm of both sides yields:\n$$\nt \\ln(s) = \\ln(N)\n$$\nThis gives the deterministic takeover time, $T_{det}$:\n$$\nT_{det} = \\frac{\\ln(N)}{\\ln(s)}\n$$\nThis approximation neglects the stochastic effects of random sampling (genetic drift), which can lead to extinction, especially when $k_t$ is small, or otherwise cause deviations from smooth exponential growth.\n\n**2. Monte Carlo Simulation Estimate ($T_{MC}$)**\n\nTo capture the stochastic nature of the process, we perform a Monte Carlo simulation. For each set of parameters $(N, s, R, \\text{max\\_gen}, \\text{seed})$, we conduct $R$ independent runs.\nEach run proceeds as follows:\n1.  Initialize the state: the generation count $t=0$ and the number of best individuals $k_0=1$.\n2.  Iterate through generations: for each generation, increment $t$.\n3.  Calculate the selection probability $p_t$ based on the current count $k_t$.\n4.  Sample the number of best individuals for the next generation, $k_{t+1}$, from a binomial distribution: $k_{t+1} = \\text{rng.binomial}(N, p_t)$, where `rng` is a seeded random number generator.\n5.  Check for termination conditions:\n    - If $k_{t+1} = N$ (takeover), record the current generation count $t+1$ as the takeover time for this run and stop the run.\n    - If $k_{t+1} = 0$ (extinction), stop the run without recording a time.\n    - If $t$ reaches the generation cap, `max_gen`, stop the run. This is a safeguard against excessively long runs.\nAfter all $R$ runs are completed, we will have a list of takeover times from the successful runs. The estimated conditional expected takeover time, $T_{MC}$, is the arithmetic mean of these recorded times. If no runs result in a takeover, $T_{MC}$ is considered undefined, represented as $\\mathrm{nan}$.\n\n**3. Error Calculation**\n\nFor each test case, the final result is the absolute difference between the Monte Carlo estimate and the deterministic approximation, rounded to three decimal places:\n$$\n\\Delta T = |T_{MC} - T_{det}|\n$$\nIf $T_{MC}$ is $\\mathrm{nan}$, the difference $\\Delta T$ is also $\\mathrm{nan}$. The implementation will calculate this value for each of the five provided test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the absolute difference between a Monte Carlo estimate and a\n    deterministic approximation for the takeover time in a selection-only\n    genetic algorithm model.\n    \"\"\"\n    test_cases = [\n        # (N, s, R, max_gen, seed)\n        (100, 1.5, 500, 1000, 42),\n        (100, 1.01, 500, 10000, 43),\n        (50, 2.0, 300, 200, 44),\n        (10, 1.5, 1000, 200, 45),\n        (200, 1.2, 300, 3000, 46),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, s, R, max_gen, seed = case\n\n        # Initialize random number generator for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # --- Monte Carlo Simulation ---\n        takeover_times = []\n        for _ in range(R):\n            k = 1  # Initial number of best individuals\n            for t in range(1, max_gen + 1):\n                # Check for termination conditions from the previous state\n                if k == N:\n                    takeover_times.append(t - 1)\n                    break\n                if k == 0:\n                    break\n\n                # Calculate selection probability\n                total_fitness_best = k * s\n                total_fitness_other = (N - k) * 1.0\n                total_fitness = total_fitness_best + total_fitness_other\n                \n                # Handle potential division by zero if population empties, though k=0 check prevents this\n                if total_fitness == 0:\n                    p = 0.0\n                else:\n                    p = total_fitness_best / total_fitness\n\n                # Sample the next generation\n                k = rng.binomial(N, p)\n            \n            # Check for takeover at the end of the loop (if max_gen is reached exactly)\n            else: # This 'else' belongs to the 'for t' loop\n                if k == N:\n                    takeover_times.append(max_gen)\n\n\n        # Calculate the MC estimate\n        if not takeover_times:\n            mc_estimate = np.nan\n        else:\n            mc_estimate = np.mean(takeover_times)\n\n        # --- Deterministic Approximation ---\n        # T_det = log(N) / log(s)\n        det_approx = np.log(N) / np.log(s)\n\n        # --- Calculate Absolute Difference ---\n        if np.isnan(mc_estimate):\n            diff = np.nan\n            results.append('nan')\n        else:\n            diff = np.abs(mc_estimate - det_approx)\n            results.append(f\"{diff:.3f}\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Having explored the force of selection, we now turn to a critical design choice: how we represent a solution. The 'genetic code' or encoding scheme can dramatically alter the effectiveness of a GA's search, particularly how mutation explores the solution space. This exercise provides a quantitative comparison between standard binary and Gray coding, revealing how the choice of representation creates a different 'search landscape' for the algorithm to navigate. You will calculate key metrics that expose the concept of locality and the infamous 'Hamming cliff' problem. ",
            "id": "3132789",
            "problem": "You are to write a complete, runnable program that compares two genotype encodings used in Genetic Algorithms (Genetic Algorithm (GA)): standard binary encoding and Gray coding, when optimizing the sphere function. The objective function is the sphere function $f(\\mathbf{x}) = \\sum_{i=1}^{n} x_i^2$ defined on a bounded hyper-rectangle $[L, U]^n$ with $L < 0 < U$. Each decision variable $x_i$ is represented by $b$ bits and decoded into a real value by uniform linear quantization.\n\nFundamental base and definitions to use:\n- The sphere function $f(\\mathbf{x}) = \\sum_{i=1}^{n} x_i^2$ is separable by coordinates.\n- The genotype-to-phenotype mapping is a linear quantizer: for each coordinate, a $b$-bit code word represents an integer $k \\in \\{0,1,\\dots,2^b - 1\\}$ which maps to a real value $x = L + \\dfrac{U - L}{2^b - 1}\\,k$. This induces a uniform grid of $2^b$ points per coordinate including the endpoints $L$ and $U$.\n- For binary encoding, the code word is the standard base-$2$ representation of $k$. For Gray coding, the code word is the Gray encoding of $k$, and decoding applies Gray-to-binary conversion to recover the integer $k$ before the linear map to $x$.\n- A single-bit mutation is defined as flipping exactly one bit chosen uniformly at random among all $n b$ genotype bits.\n\nCompute and report the following for each test case:\n1) Discretization error. Define the discretization error as the minimum attainable value of $f(\\mathbf{x})$ over the discretized grid induced by the chosen $b$ (for either encoding; both induce the same set of phenotype grid points) minus the continuous optimum of $f(\\mathbf{x})$ over $[L, U]^n$. You must compute this quantity exactly, not by sampling.\n2) Search bias via one-step improvement probability. Define the one-step improvement probability $P_{\\mathrm{improve}}$ for an encoding as the probability (with respect to the uniform distribution over genotypes and uniform choice among $n b$ bit positions) that a single-bit flip strictly decreases $f(\\mathbf{x})$. You must compute this probability exactly by enumeration, not by simulation. Formally,\n$$\nP_{\\mathrm{improve}} = \\frac{1}{(2^b)^n \\cdot n b} \\sum_{\\text{genotypes } g} \\sum_{p=1}^{n b} \\mathbf{1}\\big(f(g^{(p)}) < f(g)\\big),\n$$\nwhere $g^{(p)}$ is $g$ with bit $p$ flipped, and $\\mathbf{1}(\\cdot)$ is the indicator function. Use the separability of $f$ and the fact that the bit flip affects exactly one coordinate to reduce this to an equivalent exact enumeration over a single coordinate of $b$ bits.\n\nYour program must:\n- Implement both encodings (binary and Gray) exactly for any integer $b \\ge 1$.\n- Use exact logic instead of floating-point comparisons to test whether a bit flip improves a single coordinate’s contribution $x^2$, by observing that for symmetric bounds with $L = -U$ and uniform quantization with step size $\\Delta = \\dfrac{U - L}{2^b - 1}$, the per-coordinate value equals $x = \\Delta\\,(k - m)$ with $m = \\dfrac{2^b - 1}{2}$. Therefore, a flip improves the objective if and only if $\\lvert k' - m \\rvert < \\lvert k - m \\rvert$, which is equivalent to $\\lvert 2k' - (2^b - 1) \\rvert < \\lvert 2k - (2^b - 1) \\rvert$.\n- Compute the discretization error exactly from first principles, without numerically searching the grid. If $0 \\in [L, U]$, the continuous minimum is $0$ achieved at $\\mathbf{x} = \\mathbf{0}$; otherwise it is the closest boundary point to $0$ in each coordinate.\n- Output, for each test case, a list with four numbers: the discretization error under binary encoding, the discretization error under Gray coding, the one-step improvement probability under binary encoding, and the one-step improvement probability under Gray coding. All numbers must be printed as decimals rounded to $12$ places.\n\nTest suite:\nUse the following five test cases, each specified as a tuple $(n, b, L, U)$, where $n$ is the dimension, $b$ is the bit-length per coordinate, and $[L, U]$ is the bound interval:\n- $(\\,1,\\,3,\\,-1.0,\\,1.0\\,)$\n- $(\\,5,\\,4,\\,-1.0,\\,1.0\\,)$\n- $(\\,10,\\,8,\\,-1.0,\\,1.0\\,)$\n- $(\\,3,\\,1,\\,-1.0,\\,1.0\\,)$\n- $(\\,2,\\,12,\\,-1.0,\\,1.0\\,)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the five test cases as a comma-separated list of length five, where each element is itself a comma-separated list of four decimal numbers in the order described above, and every decimal is rounded to $12$ places. For example, a valid shape is\n$$\n[\\,[d_1^{\\text{bin}}, d_1^{\\text{gray}}, p_1^{\\text{bin}}, p_1^{\\text{gray}}],\\dots,[d_5^{\\text{bin}}, d_5^{\\text{gray}}, p_5^{\\text{bin}}, p_5^{\\text{gray}}]\\,].\n$$\nNo extra text should be printed.",
            "solution": "The user-provided problem has been analyzed and validated. It is scientifically grounded, well-posed, and all definitions and constraints are self-contained and consistent. The task is to compute two metrics, discretization error and one-step improvement probability, for binary and Gray encodings used in a genetic algorithm optimizing the sphere function.\n\n### 1. Discretization Error\n\nThe objective function is the sphere function $f(\\mathbf{x}) = \\sum_{i=1}^{n} x_i^2$ over the domain $\\mathbf{x} \\in [L, U]^n$. For all test cases, $L = -1.0$ and $U = 1.0$, so $L < 0 < U$. The continuous minimum of $f(\\mathbf{x})$ is $f(\\mathbf{0}) = 0$, achieved at $\\mathbf{x} = \\mathbf{0}$.\n\nThe problem describes a uniform linear quantization scheme where each coordinate $x_i$ is mapped from a $b$-bit codeword. The codeword corresponds to an integer $k \\in \\{0, 1, \\dots, 2^b-1\\}$, which in turn maps to a real value $x_k = L + \\frac{U-L}{2^b-1}k$. The set of possible real values for each coordinate, the grid points, is determined by the range of $k$.\n\nBoth binary and Gray encodings provide a bijective mapping from the set of $b$-bit strings to the set of integers $\\{0, 1, \\dots, 2^b-1\\}$. Consequently, the set of achievable phenotype values (the grid points) is identical for both encodings. The discretization error, defined as the difference between the minimum value on the grid and the continuous minimum, is therefore independent of the choice between binary and Gray encoding.\n\nTo find the minimum value of $f(\\mathbf{x})$ on the grid, we must find the grid point $x_k$ closest to $0$ for each coordinate. The value of $k$ that makes $x_k$ closest to $0$ is the integer nearest to the real value $k_{\\text{real}} = -L \\frac{2^b-1}{U-L}$.\nGiven $L=-1.0$ and $U=1.0$, this becomes $k_{\\text{real}} = -(-1.0) \\frac{2^b-1}{1.0 - (-1.0)} = \\frac{2^b-1}{2}$.\nSince $b \\ge 1$, $2^b-1$ is always odd, so $k_{\\text{real}}$ is never an integer. The two integers closest to $k_{\\text{real}}$ are $k_1 = \\lfloor \\frac{2^b-1}{2} \\rfloor = 2^{b-1}-1$ and $k_2 = \\lceil \\frac{2^b-1}{2} \\rceil = 2^{b-1}$.\nThe corresponding phenotype values are:\n$x_{k_1} = -1.0 + \\frac{2.0}{2^b-1}(2^{b-1}-1) = \\frac{-(2^b-1) + 2^b-2}{2^b-1} = \\frac{-1}{2^b-1}$.\n$x_{k_2} = -1.0 + \\frac{2.0}{2^b-1}(2^{b-1}) = \\frac{-(2^b-1) + 2^b}{2^b-1} = \\frac{1}{2^b-1}$.\nThe minimum possible absolute value for a coordinate is $|x^*| = \\frac{1}{2^b-1}$.\nThe minimum value of $f(\\mathbf{x})$ on the discrete grid is achieved when each $|x_i| = |x^*|$, so $\\min_{\\text{grid}} f(\\mathbf{x}) = \\sum_{i=1}^{n} (x^*)^2 = n \\left(\\frac{1}{2^b-1}\\right)^2$.\nThe discretization error $D$ is thus:\n$$ D = \\min_{\\text{grid}} f(\\mathbf{x}) - \\min_{\\text{continuous}} f(\\mathbf{x}) = \\frac{n}{(2^b-1)^2} - 0 = \\frac{n}{(2^b-1)^2} $$\nThis value is the same for both binary and Gray encoding, so $D_{\\text{binary}} = D_{\\text{Gray}}$.\n\n### 2. One-Step Improvement Probability ($P_{\\mathrm{improve}}$)\n\nThe one-step improvement probability is defined as:\n$$ P_{\\mathrm{improve}} = \\frac{1}{(2^b)^n \\cdot nb} \\sum_{\\text{genotypes } g} \\sum_{p=1}^{nb} \\mathbf{1}\\big(f(g^{(p)}) < f(g)\\big) $$\nDue to the separability of the sphere function $f(\\mathbf{x}) = \\sum_{i=1}^n x_i^2$, a single-bit flip in the genotype affects only one coordinate, say $x_i \\to x_i'$. The condition for improvement $f(g^{(p)}) < f(g)$ simplifies to $(x_i')^2 < x_i^2$, which is equivalent to $|x_i'| < |x_i|$.\nThe total number of improving moves can be summed over each coordinate independently. Let $S$ be the total number of single-bit flips that result in an improvement for a single $b$-bit coordinate, summed over all $2^b$ possible states of that coordinate.\n$$ S = \\sum_{c \\in \\{0,1\\}^b} \\sum_{j=1}^{b} \\mathbf{1}\\big( |x'(c^{(j)})| < |x(c)| \\big) $$\nwhere $c$ is a $b$-bit codeword, $c^{(j)}$ is $c$ with bit $j$ flipped, and $x(c)$ is the phenotype value corresponding to $c$.\nThe total number of improving moves across all $n$ coordinates and all $(2^b)^n$ genotypes is $n \\cdot (2^b)^{n-1} \\cdot S$.\nSubstituting this into the probability formula simplifies it significantly:\n$$ P_{\\mathrm{improve}} = \\frac{n \\cdot (2^b)^{n-1} \\cdot S}{(2^b)^n \\cdot nb} = \\frac{S}{2^b \\cdot b} $$\nThis means we only need to analyze a single $b$-bit coordinate to find the probability. The value of $S$ will differ between binary and Gray encodings because the mapping from a codeword $c$ to an integer $k$ (and thus to a phenotype value $x$) is different.\n\nTo avoid floating-point inaccuracies, we use the suggested integer-based comparison. For $L=-U$, the condition $|x'| < |x|$ is equivalent to $|k' - m| < |k - m|$ where $m = (2^b-1)/2$. This can be rewritten by multiplying by $2$ as $|2k' - (2^b-1)| < |2k - (2^b-1)|$.\n\nThe algorithm to compute $S$ for each encoding is as follows:\n1.  Initialize a counter for improving moves, `improvements`, to $0$. Let $M = 2^b - 1$.\n2.  Iterate through each possible $b$-bit codeword $c$, represented by an integer from $0$ to $2^b-1$.\n3.  For each $c$, determine its integer value $k$ for the given encoding (binary or Gray).\n    - For binary encoding, $k_{\\text{bin}}$ is the integer value of $c$.\n    - For Gray encoding, $k_{\\text{gray}}$ is obtained by a Gray-to-binary conversion of $c$.\n4.  Calculate the fitness-related term for the current state: $V_{\\text{bin}} = |2k_{\\text{bin}} - M|$ and $V_{\\text{gray}} = |2k_{\\text{gray}} - M|$.\n5.  Iterate through each bit position $j$ from $0$ to $b-1$.\n    a. Determine the new codeword $c'$ by flipping bit $j$ of $c$.\n    b. Decode $c'$ to get the new integer values $k'_{\\text{bin}}$ and $k'_{\\text{gray}}$.\n    c. Calculate the new fitness terms $V'_{\\text{bin}} = |2k'_{\\text{bin}} - M|$ and $V'_{\\text{gray}} = |2k'_{\\text{gray}} - M|$.\n    d. If $V'_{\\text{bin}} < V_{\\text{bin}}$, increment the counter for binary encoding.\n    e. If $V'_{\\text{gray}} < V_{\\text{gray}}$, increment the counter for Gray encoding.\n6.  After iterating through all codewords and bit flips, the total counts $S_{\\text{bin}}$ and $S_{\\text{gray}}$ are obtained.\n7.  The probabilities are then $P_{\\mathrm{improve}}^{\\text{bin}} = S_{\\text{bin}} / (2^b \\cdot b)$ and $P_{\\mathrm{improve}}^{\\text{gray}} = S_{\\text{gray}} / (2^b \\cdot b)$.\n\nThis procedure is implemented exactly in the provided code.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes discretization error and one-step improvement probability for\n    binary and Gray encodings on the sphere function.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, b, L, U)\n        (1, 3, -1.0, 1.0),\n        (5, 4, -1.0, 1.0),\n        (10, 8, -1.0, 1.0),\n        (3, 1, -1.0, 1.0),\n        (2, 12, -1.0, 1.0),\n    ]\n\n    def gray_to_bin_int(g: int) -> int:\n        \"\"\"\n        Converts a Gray-coded integer to its corresponding binary integer value.\n        \"\"\"\n        b = g\n        mask = g >> 1\n        while mask > 0:\n            b ^= mask\n            mask >>= 1\n        return b\n\n    # Store results as formatted strings to match output specification.\n    formatted_results = []\n\n    for n, b, L, U in test_cases:\n        # 1. Discretization Error Calculation\n        # For L = -U, the continuous minimum is 0. The discrete grid point closest to 0\n        # for a coordinate is at |U|/(2^b - 1). The minimum of sum(x_i^2) is n * (|U|/(2^b - 1))^2.\n        # Since all test cases have U=1.0, this simplifies.\n        if (2**b - 1) == 0: # a_val is denominator\n             disc_error = float('inf') if n > 0 else 0.0 # handle b=0 case, though not in tests\n        else:\n             disc_error = n * (U / (2**b - 1))**2\n        \n        # Discretization error is independent of the encoding strategy, as both\n        # cover the same set of phenotype points.\n        d_bin = disc_error\n        d_gray = disc_error\n\n        # 2. One-Step Improvement Probability Calculation\n        improvements_bin = 0\n        improvements_gray = 0\n        num_states = 2**b\n        # Precompute a map from Gray coded integers to binary integers for efficiency.\n        gray_map = {g: gray_to_bin_int(g) for g in range(num_states)}\n        \n        # This value is used for the integer-based fitness comparison\n        # |2k' - M| < |2k - M| to avoid floating point issues.\n        M = num_states - 1\n\n        # Iterate over all possible b-bit codewords (represented as integers 0..2^b-1)\n        for c_int in range(num_states):\n            # For binary encoding, the integer value k is the codeword itself.\n            k_bin = c_int\n            # For Gray encoding, the codeword c_int must be converted to find k.\n            k_gray = gray_map[c_int]\n\n            # Objective value proxy for the current state for each encoding\n            val_bin = abs(2 * k_bin - M)\n            val_gray = abs(2 * k_gray - M)\n\n            # Iterate over all possible single-bit flips\n            for j in range(b):\n                # Flipping the j-th bit is equivalent to XOR with 2^j\n                c_prime_int = c_int ^ (1 << j)\n\n                # --- Binary Encoding ---\n                k_prime_bin = c_prime_int\n                val_prime_bin = abs(2 * k_prime_bin - M)\n                if val_prime_bin < val_bin:\n                    improvements_bin += 1\n\n                # --- Gray Encoding ---\n                k_prime_gray = gray_map[c_prime_int]\n                val_prime_gray = abs(2 * k_prime_gray - M)\n                if val_prime_gray < val_gray:\n                    improvements_gray += 1\n\n        # The total number of possible one-bit mutations for a single coordinate\n        # is the number of states times the number of bits.\n        total_mutations = num_states * b\n        \n        p_bin = improvements_bin / total_mutations if total_mutations > 0 else 0.0\n        p_gray = improvements_gray / total_mutations if total_mutations > 0 else 0.0\n\n        sub_result = [d_bin, d_gray, p_bin, p_gray]\n        result_str = f\"[{','.join(f'{x:.12f}' for x in sub_result)}]\"\n        formatted_results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many real-world optimization problems are multimodal, meaning they have multiple good solutions or 'peaks'. A simple GA can suffer from premature convergence, where the entire population clusters around a single peak, missing other valuable solutions. This advanced practice introduces fitness sharing, a powerful niching technique designed to maintain population diversity and enable the GA to discover and sustain multiple peaks simultaneously. By implementing and testing this mechanism, you will learn how to adapt GAs to solve more complex, multimodal problems. ",
            "id": "3132784",
            "problem": "You are asked to design and implement a Genetic Algorithm (GA) for a one-dimensional continuous optimization problem with explicit fitness sharing. The aim is to quantify the effect of fitness sharing on preserving multiple niches when optimizing a multimodal objective function. The following definitions and facts constitute the fundamental base for this task, from which your solution must be derived.\n\nA Genetic Algorithm (GA) maintains a population of candidate solutions and iteratively applies selection, recombination (crossover), and mutation to evolve the population toward higher fitness. A standard GA uses raw fitness values to determine selection probabilities or tournament outcomes, which can lead to premature convergence at a single mode in multimodal landscapes. Fitness sharing is a diversification method that modifies the effective fitness to penalize crowded regions, enabling maintenance of multiple niches corresponding to different modes.\n\nConsider a real-coded GA on a scalar decision variable $x \\in [L, U] = [-5, 5]$. The multimodal objective is defined as a sum of Gaussian peaks,\n$$\nf(x) = \\sum_{k=1}^{K} a_k \\exp\\left(-\\frac{(x - c_k)^2}{2 w_k^2}\\right),\n$$\nwhere $K$ denotes the number of peaks, $a_k$ denotes the peak amplitude, $c_k$ denotes the peak center, and $w_k$ denotes the peak width. For this problem, use $K = 4$ with parameters\n$$\nc = [-3.5, -0.5, 1.8, 3.2], \\quad a = [1.0, 0.8, 0.9, 1.2], \\quad w = [0.28, 0.35, 0.25, 0.30].\n$$\n\nFitness sharing adjusts the effective fitness $f'_i$ of an individual $x_i$ based on its distances to all other individuals $x_j$ in the population:\n$$\nf'_i = \\frac{f(x_i)}{\\sum_{j=1}^{N} s\\left(d_{ij}\\right)},\n$$\nwhere $N$ is the population size, $d_{ij} = |x_i - x_j|$ is the Euclidean distance in decision space, and the sharing function is\n$$\ns(d) = \\frac{1}{1 + \\left(\\frac{d}{\\sigma}\\right)^{\\alpha}},\n$$\nwith niche radius $\\sigma > 0$ and shape parameter $\\alpha > 0$. When fitness sharing is disabled, selection must use the raw fitness $f(x)$ directly.\n\nYour GA must employ the following general components, designed from standard GA principles:\n- Initialization: sample $x$ uniformly in $[L, U]$.\n- Selection: apply tournament selection of size $\\kappa$ using the relevant fitness (shared or raw).\n- Crossover: given two parents $x_p$ and $x_q$, produce a child by arithmetic recombination $x_c = \\lambda x_p + (1 - \\lambda) x_q$ with $\\lambda \\in [0, 1]$ uniformly at random, applied with probability $\\rho$.\n- Mutation: add Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\mu^2)$ and clip to $[L, U]$.\n- Elitism: carry forward the top $e$ individuals based on raw fitness.\n- Termination: stop after $G$ generations.\n\nTo quantify multimodal preservation, define a mode discovery criterion: a peak centered at $c_k$ is considered discovered at the end of the run if there are at least $n_{\\min}$ individuals in the final population whose distance to $c_k$ is less than $r_{\\mathrm{detect}}$. Use $r_{\\mathrm{detect}} = 0.3$ and $n_{\\min} = 3$. The discovered peak count is the number of distinct $k \\in \\{1, 2, 3, 4\\}$ that meet this criterion.\n\nImplement two versions for each test configuration: one using fitness sharing (with given $\\sigma$ and $\\alpha$) and one baseline without fitness sharing. For each configuration, report the pair of integers $[\\text{with\\_sharing}, \\text{without\\_sharing}]$ indicating the number of discovered peaks at termination.\n\nTest Suite:\nRun the GA for the following parameter sets (each configuration is an independent test case). For reproducibility, fix the random seed per case to $s_i$, where $s_1 = 123$, $s_2 = 124$, $s_3 = 125$, $s_4 = 126$, $s_5 = 127$; use the same seed for both the shared and baseline runs within a case.\n\n- Case $1$: $N = 60$, $G = 120$, $\\sigma = 0.6$, $\\alpha = 2$, $\\mu = 0.15$, $\\kappa = 3$, $\\rho = 0.9$, $e = \\lfloor 0.1 N \\rfloor$.\n- Case $2$: $N = 60$, $G = 120$, $\\sigma = 10^{-9}$, $\\alpha = 2$, $\\mu = 0.15$, $\\kappa = 3$, $\\rho = 0.9$, $e = \\lfloor 0.1 N \\rfloor$.\n- Case $3$: $N = 60$, $G = 120$, $\\sigma = 3.0$, $\\alpha = 2$, $\\mu = 0.15$, $\\kappa = 3$, $\\rho = 0.9$, $e = \\lfloor 0.1 N \\rfloor$.\n- Case $4$: $N = 30$, $G = 150$, $\\sigma = 0.6$, $\\alpha = 2$, $\\mu = 0.20$, $\\kappa = 3$, $\\rho = 0.9$, $e = \\lfloor 0.1 N \\rfloor$.\n- Case $5$: $N = 60$, $G = 120$, $\\sigma = 0.6$, $\\alpha = 4$, $\\mu = 0.15$, $\\kappa = 3$, $\\rho = 0.9$, $e = \\lfloor 0.1 N \\rfloor$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list $[\\text{with\\_sharing}, \\text{without\\_sharing}]$. For example, $[[a_1,b_1],[a_2,b_2],\\dots]$ with integers $a_i$ and $b_i$ for $i \\in \\{1,2,3,4,5\\}$.",
            "solution": "The design proceeds from standard Genetic Algorithm (GA) principles and the fitness sharing mechanism. A GA operates by maintaining a population of candidate solutions, evaluating fitness, and applying selection, crossover, and mutation over generations to explore and exploit the search space. Selection amplifies individuals with higher fitness, which in multimodal objectives tends to cause most individuals to cluster at the highest peak, reducing diversity and suppressing discovery of other modes.\n\nFitness sharing modifies effective fitness to account for population density. For an individual at position $x_i$ with raw fitness $f(x_i)$, shared fitness is defined by\n$$\nf'_i = \\frac{f(x_i)}{\\sum_{j=1}^{N} s\\left(d_{ij}\\right)}, \\quad d_{ij} = |x_i - x_j|,\n$$\nwhere the sharing function $s(d)$ smoothly decays with distance according to\n$$\ns(d) = \\frac{1}{1 + \\left(\\frac{d}{\\sigma}\\right)^{\\alpha}}.\n$$\nThis denominator grows when many individuals are located within distance scale $\\sigma$ of $x_i$, thereby reducing the effective fitness of individuals in crowded regions. In selection, using $f'_i$ instead of $f(x_i)$ penalizes overcrowding, making less-populated niches more competitive. Two limiting cases reveal behavior consistent with GA fundamentals:\n- As $\\sigma \\to 0^+$, $\\left(\\frac{d}{\\sigma}\\right)^{\\alpha} \\to \\infty$ for $d > 0$ and $s(d > 0) \\to 0$, while $s(0) = 1$. Then $\\sum_j s(d_{ij}) \\approx 1$ and $f'_i \\approx f(x_i)$, recovering the baseline GA.\n- As $\\sigma$ becomes very large relative to the domain diameter, $s(d) \\approx \\frac{1}{1 + 0} = 1$ for all $d$, so $\\sum_j s(d_{ij}) \\approx N$, yielding $f'_i \\approx \\frac{f(x_i)}{N}$. This flattens selection pressure across the entire population, potentially slowing convergence and reducing the ability to form concentrated clusters around multiple peaks if mutation and crossover do not compensate.\n\nThe multimodal objective is defined as a sum of Gaussian peaks,\n$$\nf(x) = \\sum_{k=1}^{K} a_k \\exp\\left(-\\frac{(x - c_k)^2}{2 w_k^2}\\right),\n$$\nwith $K = 4$, centers $c = [-3.5, -0.5, 1.8, 3.2]$, amplitudes $a = [1.0, 0.8, 0.9, 1.2]$, and widths $w = [0.28, 0.35, 0.25, 0.30]$. These parameters yield distinct modes within $[L, U] = [-5, 5]$, creating a realistic and scientifically sound multimodal landscape.\n\nThe GA components are specified to ensure a principled implementation:\n- Initialization samples $x$ uniformly in $[L, U]$, providing initial diversity without bias.\n- Selection via tournament of size $\\kappa$ uses the chosen fitness (shared or raw). Tournament selection is grounded in the principle that higher fitness increases the probability of winning tournaments, thus propagating advantageous traits.\n- Arithmetic crossover $x_c = \\lambda x_p + (1 - \\lambda) x_q$ with $\\lambda \\sim \\mathcal{U}[0, 1]$ is appropriate for real-coded GAs and respects continuity of the search space.\n- Mutation adds Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\mu^2)$ and clips to bounds, ensuring bounded exploration.\n- Elitism preserves the top $e$ individuals based on raw fitness to avoid losing high-quality solutions abruptly, consistent with the exploitation aspect of GA.\n\nTo quantify multimodal preservation, define the discovered peak count as the number of peaks $c_k$ for which at least $n_{\\min}$ individuals are within distance $r_{\\mathrm{detect}}$ at termination. Using $r_{\\mathrm{detect}} = 0.3$ and $n_{\\min} = 3$ ensures detection requires a nontrivial cluster, not a single incidental individual, aligning with the goal of maintaining niches.\n\nThe effect of fitness sharing is assessed across a test suite with varying $\\sigma$, $\\alpha$, $N$, and $G$:\n- Moderate $\\sigma$ (for example $\\sigma = 0.6$) and $\\alpha = 2$ promotes multiple niches by penalizing overcrowded peaks at roughly the inter-peak spacing scale, enabling several peaks to persist.\n- Near-zero $\\sigma$ approximates the baseline, illustrating that sharing has negligible impact when niches are infinitesimal.\n- Large $\\sigma$ (for example $\\sigma = 3.0$) over-penalizes all individuals similarly, flattening effective fitness and reducing selective pressure to consolidate around any peak, which may reduce the number of sustained clusters if mutation does not produce sufficient convergence.\n- Increased $\\alpha$ sharpens the sharing function, making penalties drop faster with distance and modulating how sharply niches are separated.\n\nBy running both shared and baseline versions with identical seeds per case, we isolate the effect of fitness sharing on the final number of discovered peaks. The final output contains pairs $[\\text{with\\_sharing}, \\text{without\\_sharing}]$ per case. This directly quantifies how fitness sharing changes the GA’s ability to maintain multiple niches in a multimodal landscape, derived consistently from GA fundamentals and the specified sharing function.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Multimodal function: sum of Gaussians with specified parameters.\ndef f(x):\n    centers = np.array([-3.5, -0.5, 1.8, 3.2])\n    amplitudes = np.array([1.0, 0.8, 0.9, 1.2])\n    widths = np.array([0.28, 0.35, 0.25, 0.30])\n    # Ensure x is array-like\n    xv = np.atleast_1d(x)\n    total = np.zeros_like(xv, dtype=float)\n    for c, a, w in zip(centers, amplitudes, widths):\n        total += a * np.exp(-((xv - c) ** 2) / (2.0 * (w ** 2)))\n    return total\n\ndef sharing_denominator(pop, sigma_share, alpha):\n    \"\"\"\n    Compute the sharing denominator for each individual:\n    denom[i] = sum_j s(d_ij), where s(d) = 1 / (1 + (d/sigma)^alpha).\n    If sigma_share is None or <= 0, return ones (no sharing).\n    \"\"\"\n    n = pop.shape[0]\n    if sigma_share is None or sigma_share <= 0.0:\n        return np.ones(n, dtype=float)\n    # Compute pairwise distances in 1D\n    dists = np.abs(pop.reshape(-1, 1) - pop.reshape(1, -1))\n    # Avoid numerical issues when sigma is extremely small by safe division\n    scaled = (dists / sigma_share) ** alpha\n    s = 1.0 / (1.0 + scaled)\n    denom = np.sum(s, axis=1)\n    # Ensure denom is at least 1 (it includes self s(0)=1)\n    return denom\n\ndef tournament_select(pop, fitness, k, rng):\n    \"\"\"\n    Tournament selection: returns index of selected individual.\n    \"\"\"\n    n = pop.shape[0]\n    # Sample k competitors without replacement\n    idxs = rng.choice(n, size=k, replace=False)\n    # Winner is the one with highest fitness\n    winner_idx = idxs[np.argmax(fitness[idxs])]\n    return winner_idx\n\ndef make_child(p1, p2, crossover_rate, mutation_sigma, bounds, rng):\n    \"\"\"\n    Produce one child from parents p1, p2 with arithmetic crossover and Gaussian mutation.\n    \"\"\"\n    L, U = bounds\n    if rng.random() < crossover_rate:\n        lam = rng.random()\n        child = lam * p1 + (1.0 - lam) * p2\n    else:\n        # Clone one parent randomly\n        child = p1 if rng.random() < 0.5 else p2\n    # Mutation\n    child += rng.normal(loc=0.0, scale=mutation_sigma)\n    # Clip to bounds\n    child = np.clip(child, L, U)\n    return child\n\ndef count_discovered_peaks(pop, r_detect=0.3, n_min=3):\n    centers = np.array([-3.5, -0.5, 1.8, 3.2])\n    counts = []\n    for c in centers:\n        num_near = np.sum(np.abs(pop - c) < r_detect)\n        counts.append(num_near >= n_min)\n    return int(np.sum(counts))\n\ndef run_ga_case(seed, pop_size, generations, sigma_share, alpha, mutation_sigma, tournament_k, crossover_rate, elite_count):\n    rng = np.random.default_rng(seed)\n    L, U = -5.0, 5.0\n    # Initialize population uniformly\n    pop = rng.uniform(L, U, size=pop_size)\n    # Run GA loop\n    for _ in range(generations):\n        raw_fit = f(pop)\n        \n        # Determine fitness for selection\n        if sigma_share is not None:\n            denom = sharing_denominator(pop, sigma_share, alpha)\n            selection_fit = raw_fit / denom\n        else:\n            selection_fit = raw_fit\n\n        # Elites based on raw fitness\n        elite_idx = np.argsort(raw_fit)[-elite_count:]\n        elites = pop[elite_idx]\n        \n        # Create offspring\n        offspring = []\n        needed = pop_size - elite_count\n        for _ in range(needed):\n            i1 = tournament_select(pop, selection_fit, tournament_k, rng)\n            i2 = tournament_select(pop, selection_fit, tournament_k, rng)\n            child = make_child(pop[i1], pop[i2], crossover_rate, mutation_sigma, (L, U), rng)\n            offspring.append(child)\n        \n        pop = np.concatenate([elites, np.array(offspring)])\n        \n    discovered = count_discovered_peaks(pop, r_detect=0.3, n_min=3)\n    return discovered\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, N, G, sigma, alpha, mu, k, rho)\n        (123, 60, 120, 0.6, 2.0, 0.15, 3, 0.9),\n        (124, 60, 120, 1e-9, 2.0, 0.15, 3, 0.9),\n        (125, 60, 120, 3.0, 2.0, 0.15, 3, 0.9),\n        (126, 30, 150, 0.6, 2.0, 0.20, 3, 0.9),\n        (127, 60, 120, 0.6, 4.0, 0.15, 3, 0.9),\n    ]\n\n    results = []\n    for seed, N, G, sigma, alpha, mu, k, rho in test_cases:\n        elite_count = int(0.1 * N)\n        # With sharing\n        with_sharing = run_ga_case(seed, N, G, sigma, alpha, mu, k, rho, elite_count)\n        # Without sharing: same seed, sigma_share = None\n        without_sharing = run_ga_case(seed, N, G, None, alpha, mu, k, rho, elite_count)\n        results.append([with_sharing, without_sharing])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'[{a},{b}]' for a, b in results)}]\")\n\nsolve()\n```"
        }
    ]
}