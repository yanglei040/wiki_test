{
    "hands_on_practices": [
        {
            "introduction": "Before tackling complex, multi-dimensional problems, it is essential to master the fundamental mechanics of Newton's method in a single dimension. This exercise focuses on applying the core iterative formula, $x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}$, to find the minimum of a simple function. By working through the calculations, you will gain a hands-on feel for how the method uses information about both the slope (first derivative) and curvature (second derivative) to rapidly converge towards an optimum. ",
            "id": "2190708",
            "problem": "In various fields, from engineering to economics, it is often necessary to find the minimum of a function that represents cost, energy, or error. Consider a simplified one-dimensional cost function given by $f(x) = \\exp(x) + \\exp(-2x)$. To find the value of $x$ that minimizes this function, we can employ Newton's method for unconstrained optimization.\n\nStarting with an initial guess of $x_0 = 0$, perform two complete iterations of Newton's method. Let the results of these iterations be $x_1$ and $x_2$. Determine the value of $x_2$.\n\nRound your final answer to four significant figures.",
            "solution": "We seek a minimizer of $f(x)=\\exp(x)+\\exp(-2x)$ using Newton’s method for unconstrained optimization, which updates\n$$\nx_{k+1}=x_{k}-\\frac{f'(x_{k})}{f''(x_{k})}.\n$$\nCompute the derivatives:\n$$\nf'(x)=\\exp(x)-2\\exp(-2x), \\quad f''(x)=\\exp(x)+4\\exp(-2x).\n$$\nIteration 1 (from $x_{0}=0$):\n$$\nf'(0)=\\exp(0)-2\\exp(0)=1-2=-1,\\quad f''(0)=\\exp(0)+4\\exp(0)=1+4=5,\n$$\n$$\nx_{1}=0-\\frac{-1}{5}=\\frac{1}{5}.\n$$\nIteration 2 (from $x_{1}=\\frac{1}{5}$):\n$$\nf'\\!\\left(\\tfrac{1}{5}\\right)=\\exp\\!\\left(\\tfrac{1}{5}\\right)-2\\exp\\!\\left(-\\tfrac{2}{5}\\right), \\quad\nf''\\!\\left(\\tfrac{1}{5}\\right)=\\exp\\!\\left(\\tfrac{1}{5}\\right)+4\\exp\\!\\left(-\\tfrac{2}{5}\\right),\n$$\nso\n$$\nx_{2}=\\frac{1}{5}-\\frac{\\exp\\!\\left(\\tfrac{1}{5}\\right)-2\\exp\\!\\left(-\\tfrac{2}{5}\\right)}{\\exp\\!\\left(\\tfrac{1}{5}\\right)+4\\exp\\!\\left(-\\tfrac{2}{5}\\right)}.\n$$\nNumerically,\n$$\n\\exp\\!\\left(\\tfrac{1}{5}\\right)\\approx 1.221402758,\\quad \\exp\\!\\left(-\\tfrac{2}{5}\\right)\\approx 0.670320046,\n$$\nwhich gives\n$$\nx_{2}\\approx 0.230552657.\n$$\nRounding to four significant figures yields $x_{2}\\approx 0.2306$.",
            "answer": "$$\\boxed{0.2306}$$"
        },
        {
            "introduction": "Real-world optimization rarely involves just one variable. This practice extends the core idea of Newton's method to a multi-dimensional scenario, a crucial step towards practical applications. Here, you will replace the first and second derivatives with their multi-variable counterparts—the gradient vector ($\\nabla f$) and the Hessian matrix ($H_f$)—and solve the Newton system $H_f(\\mathbf{x}_k) \\mathbf{s}_k = -\\nabla f(\\mathbf{x}_k)$ to find the update step. ",
            "id": "2190727",
            "problem": "A team of engineers is working to minimize the operational cost of a new robotic arm. The cost is modeled by a function $f(x, y)$ of two dimensionless design parameters $x$ and $y$. The function is given by:\n$$f(x, y) = (y - x^2)^2 + (1-x)^2$$\nTo find the optimal parameters that minimize this cost, the team decides to use Newton's method for unconstrained optimization. They start from an initial design guess of $(x_0, y_0) = (2, 3)$.\n\nCalculate the coordinates of the next iterate, $(x_1, y_1)$, that results from applying one step of Newton's method. Express your answer as an ordered pair of exact fractions.",
            "solution": "For unconstrained optimization in two variables, one step of Newton's method updates the iterate $\\mathbf{z} = (x, y)$ by\n$$\n\\mathbf{z}_{k+1} = \\mathbf{z}_{k} - H(\\mathbf{z}_{k})^{-1} \\nabla f(\\mathbf{z}_{k}),\n$$\nwhere $\\nabla f$ is the gradient and $H$ is the Hessian of $f$.\n\nGiven $f(x, y) = (y - x^{2})^{2} + (1 - x)^{2}$, compute the gradient:\n$$\n\\frac{\\partial f}{\\partial x} = 2(y - x^{2})(-2x) + 2(1 - x)(-1) = -4xy + 4x^{3} - 2 + 2x,\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 2(y - x^{2}) = 2y - 2x^{2}.\n$$\nThus,\n$$\n\\nabla f(x, y) = \\begin{pmatrix} -4xy + 4x^{3} - 2 + 2x \\\\ 2y - 2x^{2} \\end{pmatrix}.\n$$\n\nCompute the Hessian:\n$$\n\\frac{\\partial^{2} f}{\\partial x^{2}} = \\frac{\\partial}{\\partial x}\\left(-4xy + 4x^{3} - 2 + 2x\\right) = -4y + 12x^{2} + 2,\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}\\left(-4xy + 4x^{3} - 2 + 2x\\right) = -4x,\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial y^{2}} = \\frac{\\partial}{\\partial y}\\left(2y - 2x^{2}\\right) = 2.\n$$\nTherefore,\n$$\nH(x, y) = \\begin{pmatrix} 12x^{2} - 4y + 2 & -4x \\\\ -4x & 2 \\end{pmatrix}.\n$$\n\nEvaluate at $(x_{0}, y_{0}) = (2, 3)$:\n$$\n\\nabla f(2, 3) = \\begin{pmatrix} -4\\cdot 2 \\cdot 3 + 4\\cdot 2^{3} - 2 + 2\\cdot 2 \\\\ 2\\cdot 3 - 2\\cdot 2^{2} \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ -2 \\end{pmatrix},\n$$\n$$\nH(2, 3) = \\begin{pmatrix} 12\\cdot 2^{2} - 4\\cdot 3 + 2 & -4\\cdot 2 \\\\ -4\\cdot 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 38 & -8 \\\\ -8 & 2 \\end{pmatrix}.\n$$\n\nThe Newton step $\\mathbf{s}$ solves $H(2, 3)\\,\\mathbf{s} = -\\nabla f(2, 3)$:\n$$\n\\begin{pmatrix} 38 & -8 \\\\ -8 & 2 \\end{pmatrix} \\begin{pmatrix} s_{1} \\\\ s_{2} \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ 2 \\end{pmatrix}.\n$$\nFrom the system\n$$\n38 s_{1} - 8 s_{2} = -10, \\quad -8 s_{1} + 2 s_{2} = 2,\n$$\nmultiply the second equation by $4$ to get $-32 s_{1} + 8 s_{2} = 8$ and add to the first to obtain $6 s_{1} = -2$, hence $s_{1} = -\\frac{1}{3}$. Substituting into $-8 s_{1} + 2 s_{2} = 2$ gives $8/3 + 2 s_{2} = 2$, so $2 s_{2} = -2/3$ and $s_{2} = -\\frac{1}{3}$.\n\nThus,\n$$\n(x_{1}, y_{1}) = (x_{0}, y_{0}) + \\mathbf{s} = \\left(2 - \\frac{1}{3},\\, 3 - \\frac{1}{3}\\right) = \\left(\\frac{5}{3},\\, \\frac{8}{3}\\right).\n$$",
            "answer": "$$\\boxed{\\left(\\frac{5}{3}, \\frac{8}{3}\\right)}$$"
        },
        {
            "introduction": "While powerful, the pure form of Newton's method has limitations and can fail in practice, particularly when the Hessian matrix is singular. This advanced problem challenges you to investigate such a scenario using a function whose Hessian becomes singular at the minimum. By comparing the naive implementation with robust alternatives like regularization, you will understand the critical need for safeguards in designing practical optimization algorithms. ",
            "id": "3164396",
            "problem": "You are asked to implement and analyze Newton’s method for unconstrained optimization in a scenario where the Hessian matrix is singular at the minimum. The analysis and implementation must be grounded in the second-order Taylor expansion of a scalar function of multiple variables and the definition of the Newton step as the minimizer of that quadratic model, subject to regularization when the Hessian is not invertible.\n\nStart from the following fundamental base:\n- The second-order Taylor expansion of a function $f : \\mathbb{R}^n \\to \\mathbb{R}$ around a point $x \\in \\mathbb{R}^n$ is given by\n$$\nf(x + s) \\approx f(x) + \\nabla f(x)^\\top s + \\frac{1}{2} s^\\top \\nabla^2 f(x) s,\n$$\nwhere $\\nabla f(x)$ is the gradient and $\\nabla^2 f(x)$ is the Hessian.\n- Minimizing the quadratic model with respect to $s$ yields a linear system of the form\n$$\n\\nabla^2 f(x) \\, s = - \\nabla f(x).\n$$\nWhen the Hessian is singular or indefinite, the system may be unsolvable or the step may not be a descent direction.\n\nTask:\n- Construct and use the function\n$$\nf(x,y) = x^4 + y^2,\n$$\nwhose Hessian is singular at the minimizer $(x^\\star,y^\\star) = (0,0)$. Implement three algorithms:\n    $1.$ Vanilla Newton: attempt to compute the Newton step by solving $\\nabla^2 f(x_k) s_k = -\\nabla f(x_k)$ and update $x_{k+1} = x_k + s_k$. If the linear system is singular at any iteration, declare failure and return $+\\infty$ as the final objective value.\n    $2.$ Damped Newton (Tikhonov regularization): compute $s_k$ from $(\\nabla^2 f(x_k) + \\lambda I) s_k = -\\nabla f(x_k)$ with a fixed damping parameter $\\lambda > 0$, and update $x_{k+1} = x_k + s_k$.\n    $3.$ Hybrid fallback: first attempt the vanilla Newton step. If the linear system is singular or the computed step $s_k$ is not a descent direction (that is, $\\nabla f(x_k)^\\top s_k \\ge 0$), instead take a steepest descent step $p_k = -\\nabla f(x_k)$ with a backtracking line search satisfying the Armijo decrease condition.\n\nSpecifications that must be implemented:\n- Use the function $f(x,y) = x^4 + y^2$ with gradient and Hessian\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 4 x^3 \\\\ 2 y \\end{bmatrix}, \\qquad\n\\nabla^2 f(x,y) = \\begin{bmatrix} 12 x^2 & 0 \\\\ 0 & 2 \\end{bmatrix}.\n$$\n- Run each algorithm for at most $N$ iterations with $N = 10$, with early stopping if $\\|\\nabla f(x_k,y_k)\\|_2 \\le \\varepsilon$ where $\\varepsilon = 10^{-12}$.\n- For the damped Newton method, use fixed $\\lambda = 10^{-3}$.\n- For the hybrid fallback method, use backtracking parameters $c = 10^{-4}$ and $\\rho = 1/2$, starting with step length $\\alpha = 1$, and reduce $\\alpha \\leftarrow \\rho \\alpha$ until the Armijo condition\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c \\alpha \\nabla f(x_k)^\\top p_k\n$$\nholds or until $\\alpha$ becomes numerically negligible.\n- If the vanilla Newton method encounters a singular linear system at any iteration, it must return $+\\infty$ as the final objective value for that test case. The other two methods must remain robust and return a finite objective value.\n\nTest suite:\nApply all three methods to the following initial conditions for the same function $f(x,y)$ as above:\n- Case $1$: $(x_0,y_0) = (1, 1)$.\n- Case $2$: $(x_0,y_0) = (0, 1)$.\n- Case $3$: $(x_0,y_0) = (10^{-8}, 10^{-8})$.\n\nAnswer specification:\n- For each case, report the final objective values $f(x_{\\text{final}},y_{\\text{final}})$ after running each of the three methods with the rules above, in the following fixed order per case:\n    $[\\,$vanilla Newton final value, damped Newton final value, hybrid fallback final value$\\,]$.\n- Aggregate all results across the three cases into a single flat list in the order Case $1$ then Case $2$ then Case $3$, resulting in a list of length $9$.\n- If a method fails under the specified rule, output $+\\infty$ for that position.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots,r_9]$). No units are involved. All numbers must be printed as standard real numbers; $+\\infty$ is acceptable as a floating-point infinity if encountered.",
            "solution": "The objective is to find the minimum of the function $f(x,y) = x^4 + y^2$ using three variations of Newton's method. The global minimum is at $(x^\\star, y^\\star) = (0,0)$, where $f(0,0)=0$. The gradient $\\nabla f(x,y)$ and Hessian $\\nabla^2 f(x,y)$ are given by:\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 4 x^3 \\\\ 2 y \\end{bmatrix}, \\qquad\n\\nabla^2 f(x,y) = \\begin{bmatrix} 12 x^2 & 0 \\\\ 0 & 2 \\end{bmatrix}.\n$$\nAt the minimizer $(0,0)$, the Hessian is $\\nabla^2 f(0,0) = \\begin{bmatrix} 0 & 0 \\\\ 0 & 2 \\end{bmatrix}$, which is singular. This singularity is the central challenge the problem is designed to test.\n\nNewton's method for optimization is an iterative algorithm that approximates a function $f$ with a quadratic model at the current iterate $x_k$ and then steps to the minimizer of that model. The second-order Taylor expansion around $x_k$ provides this model for a step $s$:\n$$\nf(x_k + s) \\approx m_k(s) = f(x_k) + \\nabla f(x_k)^\\top s + \\frac{1}{2} s^\\top \\nabla^2 f(x_k) s.\n$$\nTo find the minimum of $m_k(s)$, we set its gradient with respect to $s$ to zero:\n$$\n\\nabla_s m_k(s) = \\nabla f(x_k) + \\nabla^2 f(x_k) s = 0.\n$$\nThis yields the Newton system, a system of linear equations for the step $s_k$:\n$$\n\\nabla^2 f(x_k) s_k = - \\nabla f(x_k).\n$$\nThe next iterate is then $x_{k+1} = x_k + s_k$. The three specified algorithms handle the potential singularity of the Hessian matrix $\\nabla^2 f(x_k)$ differently.\n\n### Method 1: Vanilla Newton\nThis is the pure form of the method. The update step $s_k = [s_x, s_y]^\\top$ is computed by directly solving the Newton system:\n$$\n\\begin{bmatrix} 12 x_k^2 & 0 \\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} s_x \\\\ s_y \\end{bmatrix} = - \\begin{bmatrix} 4 x_k^3 \\\\ 2 y_k \\end{bmatrix}.\n$$\nThe Hessian is singular if and only if its determinant is zero, which for this diagonal matrix means $24x_k^2 = 0$, i.e., $x_k=0$. If $x_k \\neq 0$, the system has a unique solution:\n$s_x = - \\frac{4x_k^3}{12x_k^2} = -\\frac{1}{3}x_k$\n$s_y = - \\frac{2y_k}{2} = -y_k$\nThe update is $x_{k+1} = x_k - \\frac{1}{3}x_k = \\frac{2}{3}x_k$ and $y_{k+1} = y_k - y_k = 0$.\nThe method progresses by reducing the $x$ component by a factor of $2/3$ at each step and setting the $y$ component to zero in the first step. If any iterate has $x_k=0$, the system is singular, and the algorithm fails as specified, returning $+\\infty$.\n\n### Method 2: Damped Newton (Tikhonov Regularization)\nThis method modifies the Hessian to ensure it is always invertible and positive definite. It solves a regularized system:\n$$\n(\\nabla^2 f(x_k) + \\lambda I) s_k = - \\nabla f(x_k),\n$$\nwhere $\\lambda > 0$ is a damping parameter and $I$ is the identity matrix. The new matrix, $\\nabla^2 f(x_k) + \\lambda I$, has eigenvalues $\\mu_i + \\lambda$, where $\\mu_i$ are the eigenvalues of $\\nabla^2 f(x_k)$. For our problem, the eigenvalues of the regularized Hessian are $12x_k^2 + \\lambda$ and $2+\\lambda$. Since $\\lambda=10^{-3}>0$, both are strictly positive, guaranteeing a unique solution for $s_k$.\n$$\n\\begin{bmatrix} 12 x_k^2 + \\lambda & 0 \\\\ 0 & 2 + \\lambda \\end{bmatrix} \\begin{bmatrix} s_x \\\\ s_y \\end{bmatrix} = - \\begin{bmatrix} 4 x_k^3 \\\\ 2 y_k \\end{bmatrix}.\n$$\nThe solution is:\n$s_x = - \\frac{4x_k^3}{12x_k^2 + \\lambda}$ and $s_y = - \\frac{2y_k}{2 + \\lambda}$.\nThe update is $x_{k+1} = x_k + s_x$ and $y_{k+1} = y_k + s_y = y_k \\left(1 - \\frac{2}{2+\\lambda}\\right) = y_k \\frac{\\lambda}{2+\\lambda}$.\nThis method is robust against singularity. When $x_k$ is large, the step is nearly identical to the vanilla Newton step. When $x_k$ is small, the update for $x$ becomes $x_{k+1} \\approx x_k - (4/\\lambda)x_k^3$, which indicates slower (linear) convergence near the solution.\n\n### Method 3: Hybrid Fallback\nThis method attempts to use the fast-converging vanilla Newton step but falls back to a more robust method if needed.\n1.  **Attempt Newton Step**: Compute $s_k$ by solving $\\nabla^2 f(x_k) s_k = -\\nabla f(x_k)$. This is possible only if $x_k \\neq 0$. If a solution $s_k$ is found, a check is performed to ensure it is a descent direction, i.e., $\\nabla f(x_k)^\\top s_k < 0$. For our function, if $x_k \\neq 0$, the Newton step $s_k = [-x_k/3, -y_k]^\\top$ gives $\\nabla f(x_k)^\\top s_k = [4x_k^3, 2y_k] \\cdot [-x_k/3, -y_k] = -\\frac{4}{3}x_k^4 - 2y_k^2$. This is strictly negative for any $(x_k, y_k) \\neq (0,0)$. Thus, the fallback is triggered only by the singularity of the Hessian, which occurs when $x_k=0$.\n2.  **Fallback**: If $x_k=0$, the algorithm takes a steepest descent step, $p_k = -\\nabla f(x_k)$. The step length $\\alpha$ is determined by a backtracking line search to satisfy the Armijo condition:\n    $$\n    f(x_k + \\alpha p_k) \\le f(x_k) + c \\alpha \\nabla f(x_k)^\\top p_k.\n    $$\n    If an iterate is $(0, y_k)$ with $y_k \\neq 0$, the search direction is $p_k = -[0, 2y_k]^\\top = [0, -2y_k]^\\top$. The Armijo condition becomes $f(0, y_k - 2\\alpha y_k) \\le f(0, y_k) + c \\alpha (-4y_k^2)$, which simplifies to $\\alpha \\le 1-c$. With $c = 10^{-4}$, the initial step length $\\alpha=1$ fails, but the next trial $\\alpha = \\rho \\alpha = 1/2$ succeeds. The update is $(0, y_k) + \\frac{1}{2}(0, -2y_k) = (0,0)$, causing convergence to the exact minimum in a single step from any point on the $y$-axis (other than the origin).\n\n### Analysis Summary for Test Cases\n- **Case 1: $(1, 1)$** - Since $x_0 \\neq 0$, all three methods will behave similarly for the first few steps. Vanilla and Hybrid will be identical. Damped will be slightly different but follow a similar trajectory. All will converge.\n- **Case 2: $(0, 1)$** - Vanilla Newton will fail immediately because $x_0=0$ leads to a singular Hessian. Damped Newton will proceed, keeping $x_k=0$ and converging $y_k$ to $0$. Hybrid will detect the singular Hessian, switch to steepest descent, and converge to $(0,0)$ in one step.\n- **Case 3: $(10^{-8}, 10^{-8})$** - Since $x_0 \\neq 0$, Vanilla and Hybrid will take a Newton step. The gradient will become smaller than the tolerance $\\varepsilon=10^{-12}$ after one step, so they will terminate quickly. Damped Newton will also take a step, but the smallness of $x_0$ relative to $\\sqrt{\\lambda}$ will slow its convergence for the $x$-component, while the $y$-component converges rapidly. It will likely take more iterations than the other two to meet the gradient tolerance.",
            "answer": "[9.117290132338992e-08,1.487042893635311e-28,9.117290132338992e-08,inf,4.012012012011993e-07,0.0,1.930989939083323e-33,1.0000000000000001e-32,1.930989939083323e-33]"
        }
    ]
}