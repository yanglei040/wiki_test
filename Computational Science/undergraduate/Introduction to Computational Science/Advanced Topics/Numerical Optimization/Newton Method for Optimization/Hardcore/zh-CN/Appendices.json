{
    "hands_on_practices": [
        {
            "introduction": "理解牛顿法的第一步是亲手实践其核心计算过程。这个练习将引导你对一个二维函数执行单步牛顿迭代，重点在于掌握梯度向量和海森矩阵的计算，以及如何求解牛顿步长，这是构建更复杂优化算法的基础。",
            "id": "2190727",
            "problem": "一个工程师团队正在努力最小化一个新型机械臂的运营成本。该成本由一个关于两个无量纲设计参数 $x$ 和 $y$ 的函数 $f(x, y)$ 来建模。该函数为：\n$$f(x, y) = (y - x^2)^2 + (1-x)^2$$\n为了找到最小化此成本的最优参数，该团队决定使用牛顿法进行无约束优化。他们从一个初始设计猜测 $(x_0, y_0) = (2, 3)$ 开始。\n\n计算应用一步牛顿法后得到的下一个迭代点 $(x_1, y_1)$ 的坐标。将您的答案表示为一个由精确分数组成的有序对。",
            "solution": "对于两个变量的无约束优化，一步牛顿法通过以下方式更新迭代点 $\\mathbf{z} = (x, y)$：\n$$\n\\mathbf{z}_{k+1} = \\mathbf{z}_{k} - H(\\mathbf{z}_{k})^{-1} \\nabla f(\\mathbf{z}_{k}),\n$$\n其中 $\\nabla f$ 是 $f$ 的梯度，而 $H$ 是 $f$ 的海森矩阵。\n\n给定 $f(x, y) = (y - x^{2})^{2} + (1 - x)^{2}$，计算梯度：\n$$\n\\frac{\\partial f}{\\partial x} = 2(y - x^{2})(-2x) + 2(1 - x)(-1) = -4xy + 4x^{3} - 2 + 2x,\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 2(y - x^{2}) = 2y - 2x^{2}.\n$$\n因此，\n$$\n\\nabla f(x, y) = \\begin{pmatrix} -4xy + 4x^{3} - 2 + 2x \\\\ 2y - 2x^{2} \\end{pmatrix}.\n$$\n\n计算海森矩阵：\n$$\n\\frac{\\partial^{2} f}{\\partial x^{2}} = \\frac{\\partial}{\\partial x}\\left(-4xy + 4x^{3} - 2 + 2x\\right) = -4y + 12x^{2} + 2,\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}\\left(-4xy + 4x^{3} - 2 + 2x\\right) = -4x,\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial y^{2}} = \\frac{\\partial}{\\partial y}\\left(2y - 2x^{2}\\right) = 2.\n$$\n所以，\n$$\nH(x, y) = \\begin{pmatrix} 12x^{2} - 4y + 2  -4x \\\\ -4x  2 \\end{pmatrix}.\n$$\n\n在点 $(x_{0}, y_{0}) = (2, 3)$ 处求值：\n$$\n\\nabla f(2, 3) = \\begin{pmatrix} -4\\cdot 2 \\cdot 3 + 4\\cdot 2^{3} - 2 + 2\\cdot 2 \\\\ 2\\cdot 3 - 2\\cdot 2^{2} \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ -2 \\end{pmatrix},\n$$\n$$\nH(2, 3) = \\begin{pmatrix} 12\\cdot 2^{2} - 4\\cdot 3 + 2  -4\\cdot 2 \\\\ -4\\cdot 2  2 \\end{pmatrix} = \\begin{pmatrix} 38  -8 \\\\ -8  2 \\end{pmatrix}.\n$$\n\n牛顿步长 $\\mathbf{s}$ 求解 $H(2, 3)\\,\\mathbf{s} = -\\nabla f(2, 3)$：\n$$\n\\begin{pmatrix} 38  -8 \\\\ -8  2 \\end{pmatrix} \\begin{pmatrix} s_{1} \\\\ s_{2} \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ 2 \\end{pmatrix}.\n$$\n由方程组\n$$\n38 s_{1} - 8 s_{2} = -10, \\quad -8 s_{1} + 2 s_{2} = 2,\n$$\n将第二个方程乘以 $4$ 得到 $-32 s_{1} + 8 s_{2} = 8$，并与第一个方程相加得到 $6 s_{1} = -2$，因此 $s_{1} = -\\frac{1}{3}$。代入 $-8 s_{1} + 2 s_{2} = 2$ 中得到 $8/3 + 2 s_{2} = 2$，所以 $2 s_{2} = -2/3$ 且 $s_{2} = -\\frac{1}{3}$。\n\n因此，\n$$\n(x_{1}, y_{1}) = (x_{0}, y_{0}) + \\mathbf{s} = \\left(2 - \\frac{1}{3},\\, 3 - \\frac{1}{3}\\right) = \\left(\\frac{5}{3},\\, \\frac{8}{3}\\right).\n$$",
            "answer": "$$\\boxed{(\\frac{5}{3}, \\frac{8}{3})}$$"
        },
        {
            "introduction": "虽然牛顿法收敛速度快，但其“纯粹”形式在海森矩阵奇异时会失效。这个编程练习将探讨一个此类失效情况，并将原始牛顿法与正则化（阻尼）和混合回退等更稳健的策略进行比较，从而揭示为何在实际应用中必须为牛顿法增加保护机制。",
            "id": "3164396",
            "problem": "要求您在一个海森矩阵在最小值点处是奇异的场景下，实现并分析用于无约束优化的牛顿法。分析和实现必须基于多变量标量函数的二阶泰勒展开，以及将牛顿步长定义为该二次模型的最小化子，并在海森矩阵不可逆时进行正则化。\n\n从以下基本原理开始：\n- 函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$ 在点 $x \\in \\mathbb{R}^n$ 附近的二阶泰勒展开由下式给出：\n$$\nf(x + s) \\approx f(x) + \\nabla f(x)^\\top s + \\frac{1}{2} s^\\top \\nabla^2 f(x) s,\n$$\n其中 $\\nabla f(x)$ 是梯度，$\\nabla^2 f(x)$ 是海森矩阵。\n- 对二次模型关于 $s$ 进行最小化，可得到以下形式的线性系统：\n$$\n\\nabla^2 f(x) \\, s = - \\nabla f(x).\n$$\n当海森矩阵是奇异或不定时，该系统可能无解，或者该步长可能不是一个下降方向。\n\n任务：\n- 构建并使用函数\n$$\nf(x,y) = x^4 + y^2,\n$$\n其海森矩阵在最小值点 $(x^\\star,y^\\star) = (0,0)$ 处是奇异的。实现以下三种算法：\n    $1.$ 标准牛顿法：尝试通过求解 $\\nabla^2 f(x_k) s_k = -\\nabla f(x_k)$ 来计算牛顿步长，并更新 $x_{k+1} = x_k + s_k$。如果在任何迭代中线性系统是奇异的，则声明失败并返回 $+\\infty$ 作为最终目标值。\n    $2.$ 阻尼牛顿法 (Tikhonov 正则化)：使用固定的阻尼参数 $\\lambda  0$，从 $(\\nabla^2 f(x_k) + \\lambda I) s_k = -\\nabla f(x_k)$ 计算 $s_k$，并更新 $x_{k+1} = x_k + s_k$。\n    $3.$ 混合回退法：首先尝试标准牛顿步。如果线性系统是奇异的，或者计算出的步长 $s_k$ 不是一个下降方向 (即 $\\nabla f(x_k)^\\top s_k \\ge 0$)，则改为采用满足 Armijo 下降条件的最速下降步 $p_k = -\\nabla f(x_k)$ 并进行回溯线搜索。\n\n必须实现的具体要求：\n- 使用函数 $f(x,y) = x^4 + y^2$，其梯度和海森矩阵为\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 4 x^3 \\\\ 2 y \\end{bmatrix}, \\qquad\n\\nabla^2 f(x,y) = \\begin{bmatrix} 12 x^2  0 \\\\ 0  2 \\end{bmatrix}.\n$$\n- 每种算法最多运行 $N$ 次迭代，其中 $N = 10$，如果 $\\|\\nabla f(x_k,y_k)\\|_2 \\le \\varepsilon$ (其中 $\\varepsilon = 10^{-12}$)，则提前停止。\n- 对于阻尼牛顿法，使用固定的 $\\lambda = 10^{-3}$。\n- 对于混合回退法，使用回溯参数 $c = 10^{-4}$ 和 $\\rho = 1/2$，初始步长为 $\\alpha = 1$，然后不断减小 $\\alpha \\leftarrow \\rho \\alpha$，直到 Armijo 条件\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c \\alpha \\nabla f(x_k)^\\top p_k\n$$\n成立，或者直到 $\\alpha$ 在数值上可以忽略不计。\n- 如果标准牛顿法在任何迭代中遇到奇异线性系统，它必须为该测试用例返回 $+\\infty$ 作为最终目标值。其他两种方法必须保持稳健并返回一个有限的目标值。\n\n测试套件：\n将所有三种方法应用于上述相同函数 $f(x,y)$ 的以下初始条件：\n- 用例 1： $(x_0,y_0) = (1, 1)$。\n- 用例 2： $(x_0,y_0) = (0, 1)$。\n- 用例 3： $(x_0,y_0) = (10^{-8}, 10^{-8})$。\n\n答案规范：\n- 对于每个用例，按照以下每个用例的固定顺序，报告在上述规则下运行三种方法后得到的最终目标值 $f(x_{\\text{final}},y_{\\text{final}})$：\n    $[\\,$标准牛顿法最终值, 阻尼牛顿法最终值, 混合回退法最终值$\\,]$。\n- 将三个用例的所有结果按用例 1、用例 2、用例 3 的顺序汇总到一个长度为 9 的扁平列表中。\n- 如果某个方法在指定规则下失败，则在该位置输出 $+\\infty$。\n- 最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如 $[r_1,r_2,\\dots,r_9]$）。不涉及单位。所有数字必须以标准实数形式打印；如果遇到，可接受浮点无穷大形式的 $+\\infty$。",
            "solution": "用户提供的问题是有效的。这是一个在数值优化（计算科学的一个子领域）中适定的、有科学依据的问题。所有必需的参数、函数和初始条件都已指定，任务是客观且可验证的。\n\n目标是使用牛顿法的三种变体来找到函数 $f(x,y) = x^4 + y^2$ 的最小值。全局最小值位于 $(x^\\star, y^\\star) = (0,0)$，其中 $f(0,0)=0$。梯度 $\\nabla f(x,y)$ 和海森矩阵 $\\nabla^2 f(x,y)$ 由下式给出：\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 4 x^3 \\\\ 2 y \\end{bmatrix}, \\qquad\n\\nabla^2 f(x,y) = \\begin{bmatrix} 12 x^2  0 \\\\ 0  2 \\end{bmatrix}.\n$$\n在最小值点 $(0,0)$ 处，海森矩阵为 $\\nabla^2 f(0,0) = \\begin{bmatrix} 0  0 \\\\ 0  2 \\end{bmatrix}$，这是奇异的。这种奇异性是该问题旨在测试的核心挑战。\n\n用于优化的牛顿法是一种迭代算法，它在当前迭代点 $x_k$ 处用一个二次模型来近似函数 $f$，然后移动到该模型的最小值点。围绕 $x_k$ 的二阶泰勒展开为步长 $s$ 提供了这个模型：\n$$\nf(x_k + s) \\approx m_k(s) = f(x_k) + \\nabla f(x_k)^\\top s + \\frac{1}{2} s^\\top \\nabla^2 f(x_k) s.\n$$\n为了找到 $m_k(s)$ 的最小值，我们将其关于 $s$ 的梯度设为零：\n$$\n\\nabla_s m_k(s) = \\nabla f(x_k) + \\nabla^2 f(x_k) s = 0.\n$$\n这就得到了牛顿系统，这是一个关于步长 $s_k$ 的线性方程组：\n$$\n\\nabla^2 f(x_k) s_k = - \\nabla f(x_k).\n$$\n那么下一个迭代点是 $x_{k+1} = x_k + s_k$。三种指定的算法以不同方式处理海森矩阵 $\\nabla^2 f(x_k)$ 的潜在奇异性。\n\n### 方法 1：标准牛顿法\n这是该方法的纯粹形式。更新步长 $s_k = [s_x, s_y]^\\top$ 通过直接求解牛顿系统来计算：\n$$\n\\begin{bmatrix} 12 x_k^2  0 \\\\ 0  2 \\end{bmatrix} \\begin{bmatrix} s_x \\\\ s_y \\end{bmatrix} = - \\begin{bmatrix} 4 x_k^3 \\\\ 2 y_k \\end{bmatrix}.\n$$\n海森矩阵是奇异的当且仅当其行列式为零，对于这个对角矩阵，这意味着 $24x_k^2 = 0$，即 $x_k=0$。如果 $x_k \\neq 0$，系统有唯一解：\n$s_x = - \\frac{4x_k^3}{12x_k^2} = -\\frac{1}{3}x_k$\n$s_y = - \\frac{2y_k}{2} = -y_k$\n更新为 $x_{k+1} = x_k - \\frac{1}{3}x_k = \\frac{2}{3}x_k$ 和 $y_{k+1} = y_k - y_k = 0$。\n该方法通过在每一步将 $x$ 分量减少 $2/3$ 的因子，并在第一步将 $y$ 分量设为零来推进。如果任何迭代点有 $x_k=0$，系统就是奇异的，算法按规定失败，返回 $+\\infty$。\n\n### 方法 2：阻尼牛顿法 (Tikhonov 正则化)\n该方法修改海森矩阵以确保其始终是可逆且正定的。它求解一个正则化系统：\n$$\n(\\nabla^2 f(x_k) + \\lambda I) s_k = - \\nabla f(x_k),\n$$\n其中 $\\lambda  0$ 是一个阻尼参数，$I$ 是单位矩阵。新矩阵 $\\nabla^2 f(x_k) + \\lambda I$ 的特征值为 $\\mu_i + \\lambda$，其中 $\\mu_i$ 是 $\\nabla^2 f(x_k)$ 的特征值。对于我们的问题，正则化后海森矩阵的特征值是 $12x_k^2 + \\lambda$ 和 $2+\\lambda$。由于 $\\lambda=10^{-3}0$，两者都严格为正，从而保证了 $s_k$ 有唯一解。\n$$\n\\begin{bmatrix} 12 x_k^2 + \\lambda  0 \\\\ 0  2 + \\lambda \\end{bmatrix} \\begin{bmatrix} s_x \\\\ s_y \\end{bmatrix} = - \\begin{bmatrix} 4 x_k^3 \\\\ 2 y_k \\end{bmatrix}.\n$$\n解为：\n$s_x = - \\frac{4x_k^3}{12x_k^2 + \\lambda}$ 和 $s_y = - \\frac{2y_k}{2 + \\lambda}$。\n更新为 $x_{k+1} = x_k + s_x$ 和 $y_{k+1} = y_k + s_y = y_k \\left(1 - \\frac{2}{2+\\lambda}\\right) = y_k \\frac{\\lambda}{2+\\lambda}$。\n该方法对奇异性具有稳健性。当 $x_k$ 较大时，步长与标准牛顿步几乎相同。当 $x_k$ 较小时，对 $x$ 的更新变为 $x_{k+1} \\approx x_k - (4/\\lambda)x_k^3$，这表明在解附近收敛速度较慢（线性收敛）。\n\n### 方法 3：混合回退法\n该方法尝试使用快速收敛的标准牛顿步，但在需要时回退到更稳健的方法。\n1.  **尝试牛顿步**：通过求解 $\\nabla^2 f(x_k) s_k = -\\nabla f(x_k)$ 来计算 $s_k$。这仅在 $x_k \\neq 0$ 时才可能。如果找到了解 $s_k$，则会进行检查以确保它是一个下降方向，即 $\\nabla f(x_k)^\\top s_k  0$。对于我们的函数，如果 $x_k \\neq 0$，牛顿步 $s_k = [-x_k/3, -y_k]^\\top$ 给出 $\\nabla f(x_k)^\\top s_k = [4x_k^3, 2y_k] \\cdot [-x_k/3, -y_k] = -\\frac{4}{3}x_k^4 - 2y_k^2$。对于任何 $(x_k, y_k) \\neq (0,0)$，该值都严格为负。因此，回退仅由海森矩阵的奇异性触发，而这发生在 $x_k=0$ 时。\n2.  **回退**：如果 $x_k=0$，算法将采用最速下降步 $p_k = -\\nabla f(x_k)$。步长 $\\alpha$ 由回溯线搜索确定，以满足 Armijo 条件：\n    $$\n    f(x_k + \\alpha p_k) \\le f(x_k) + c \\alpha \\nabla f(x_k)^\\top p_k.\n    $$\n    如果一个迭代点是 $(0, y_k)$ 且 $y_k \\neq 0$，则搜索方向是 $p_k = -[0, 2y_k]^\\top = [0, -2y_k]^\\top$。Armijo 条件变为 $f(0, y_k - 2\\alpha y_k) \\le f(0, y_k) + c \\alpha (-4y_k^2)$，可简化为 $\\alpha \\le 1-c$。当 $c = 10^{-4}$ 时，初始步长 $\\alpha=1$ 会失败，但下一次尝试 $\\alpha = \\rho \\alpha = 1/2$ 会成功。更新为 $(0, y_k) + \\frac{1}{2}(0, -2y_k) = (0,0)$，这导致从 $y$ 轴上的任何点（原点除外）出发，都能在单步内收敛到精确最小值。\n\n### 测试用例分析总结\n- **用例 1: $(1, 1)$** - 由于 $x_0 \\neq 0$，所有三种方法在前几步的表现会很相似。标准牛顿法和混合法将完全相同。阻尼法会略有不同，但遵循相似的轨迹。所有方法都会收敛。\n- **用例 2: $(0, 1)$** - 标准牛顿法将立即失败，因为 $x_0=0$ 导致海森矩阵奇异。阻尼牛顿法将继续进行，保持 $x_k=0$ 并使 $y_k$ 收敛到 $0$。混合法将检测到奇异的海森矩阵，切换到最速下降法，并在一步内收敛到 $(0,0)$。\n- **用例 3: $(10^{-8}, 10^{-8})$** - 由于 $x_0 \\neq 0$，标准牛顿法和混合法将采取一个牛顿步。梯度将在一步之后变得小于容差 $\\varepsilon=10^{-12}$，因此它们会很快终止。阻尼牛顿法也会进行一步，但 $x_0$ 相对于 $\\sqrt{\\lambda}$ 的微小值会减慢其 $x$ 分量的收敛速度，而 $y$ 分量则快速收敛。它可能比另外两种方法需要更多的迭代才能满足梯度容差。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares three variants of Newton's method for unconstrained optimization on\n    the function f(x,y) = x^4 + y^2, which has a singular Hessian at its minimum.\n    \"\"\"\n\n    # --- Problem Specifications ---\n    N = 10\n    eps = 1e-12\n    lam = 1e-3  # Damping parameter for Method 2\n    c = 1e-4    # Armijo condition parameter for Method 3\n    rho = 0.5   # Backtracking reduction factor for Method 3\n\n    # --- Test Cases ---\n    test_cases = [\n        np.array([1.0, 1.0]),\n        np.array([0.0, 1.0]),\n        np.array([1e-8, 1e-8])\n    ]\n\n    # --- Objective Function and its Derivatives ---\n    def f(x_vec):\n        \"\"\"The objective function f(x, y) = x^4 + y^2.\"\"\"\n        x, y = x_vec\n        return x**4 + y**2\n\n    def grad_f(x_vec):\n        \"\"\"The gradient of f(x, y).\"\"\"\n        x, y = x_vec\n        return np.array([4 * x**3, 2 * y])\n\n    def hess_f(x_vec):\n        \"\"\"The Hessian of f(x, y).\"\"\"\n        x, y = x_vec\n        return np.array([[12 * x**2, 0.0], [0.0, 2.0]])\n\n    # --- Algorithm Implementations ---\n\n    def vanilla_newton(x0, max_iter, tol):\n        \"\"\"Algorithm 1: Vanilla Newton's method.\"\"\"\n        x_k = np.copy(x0).astype(float)\n        for _ in range(max_iter):\n            g = grad_f(x_k)\n            if np.linalg.norm(g) = tol:\n                break\n            \n            H = hess_f(x_k)\n            # Check for singularity using a tolerance. It's singular if x=0.\n            if abs(H[0, 0])  1e-30:\n                return np.inf\n\n            try:\n                s_k = np.linalg.solve(H, -g)\n                x_k += s_k\n            except np.linalg.LinAlgError:\n                return np.inf\n                \n        return f(x_k)\n\n    def damped_newton(x0, max_iter, tol, lambda_val):\n        \"\"\"Algorithm 2: Damped Newton's method (Tikhonov regularization).\"\"\"\n        x_k = np.copy(x0).astype(float)\n        I = np.identity(2)\n        for _ in range(max_iter):\n            g = grad_f(x_k)\n            if np.linalg.norm(g) = tol:\n                break\n            \n            H = hess_f(x_k)\n            H_reg = H + lambda_val * I\n            \n            s_k = np.linalg.solve(H_reg, -g)\n            x_k += s_k\n            \n        return f(x_k)\n\n    def hybrid_fallback(x0, max_iter, tol, c_bt, rho_bt):\n        \"\"\"Algorithm 3: Hybrid Newton with steepest descent fallback.\"\"\"\n        x_k = np.copy(x0).astype(float)\n        for _ in range(max_iter):\n            g = grad_f(x_k)\n            if np.linalg.norm(g) = tol:\n                break\n            \n            H = hess_f(x_k)\n            \n            use_fallback = False\n            # Check for singularity\n            if abs(H[0, 0])  1e-30:\n                use_fallback = True\n            else:\n                try:\n                    s_k = np.linalg.solve(H, -g)\n                    # Check for descent direction\n                    if g.T @ s_k >= 0:\n                        use_fallback = True\n                except np.linalg.LinAlgError:\n                    use_fallback = True\n\n            if not use_fallback:\n                # Use Newton step\n                x_k += s_k\n            else:\n                # Fallback to steepest descent with backtracking line search\n                p_k = -g\n                alpha = 1.0\n                fk = f(x_k)\n                gTp = g.T @ p_k  # This is -norm(g)**2\n                \n                # Armijo condition backtracking loop\n                while f(x_k + alpha * p_k) > fk + c_bt * alpha * gTp:\n                    alpha *= rho_bt\n                    if alpha  1e-16: # Safeguard for numerical precision\n                        break\n                \n                x_k += alpha * p_k\n                \n        return f(x_k)\n\n    # --- Main Execution Logic ---\n    results = []\n    for x_initial in test_cases:\n        # Vanilla Newton\n        res1 = vanilla_newton(x_initial, N, eps)\n        results.append(res1)\n        \n        # Damped Newton\n        res2 = damped_newton(x_initial, N, eps, lam)\n        results.append(res2)\n        \n        # Hybrid Fallback\n        res3 = hybrid_fallback(x_initial, N, eps, c, rho)\n        results.append(res3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "为了构建一个真正适用于实际问题的算法，我们需要确保每一步迭代都能取得有效进展。这个高级实践将实现一个带有Armijo线搜索和梯度下降回退机制的牛顿法，并将其应用于逻辑回归和非凸优化等问题。通过这个练习，你将学会如何将牛顿法的速度与一阶方法的稳健性结合起来。",
            "id": "2414720",
            "problem": "考虑光滑实值函数的无约束最小化问题，其更新规则在每次迭代中强制执行充分下降条件。设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是一个二次连续可微函数。在迭代点 $x_k \\in \\mathbb{R}^n$ 处，定义梯度 $\\nabla f(x_k)$ 和海森矩阵 $\\nabla^2 f(x_k)$。一个候选更新 $x_{k+1} = x_k + \\alpha_k p_k$ 必须满足带有常数 $c_1 \\in (0,1)$ 和 $\\rho \\in (0,1)$ 的 Armijo 充分下降条件，即\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n$$\n其中 $\\alpha_k$ 通过回溯法从 $\\alpha_k=1$ 开始选择，形式为 $\\alpha_k \\in \\{ \\rho^j : j \\in \\{0,1,2,\\ldots\\} \\}$。对于给定的 $x_k$，首先尝试选择 $p_k$ 作为线性系统\n$$\n\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k).\n$$\n的解。如果这个候选方向未能在回溯序列中产生一个满足充分下降条件的步长 $\\alpha_k$，则拒绝该方向，转而选择 $p_k = -\\nabla f(x_k)$，并通过相同的回溯充分下降条件来确定 $\\alpha_k$。当 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ 或达到最大迭代次数 $K$ 时，迭代终止。\n\n使用以下固定常数实现此迭代方案：$c_1 = 10^{-4}$，$\\rho = 0.5$，容差 $\\varepsilon = 10^{-8}$，每次迭代的回溯限制为 $M = 50$ 次缩减，以及最大迭代次数 $K = 50$。如果在某个 $x_k$ 处线性系统是奇异的或病态的，则将由 $\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k)$ 定义的方向视为失败，并如上所述继续使用 $p_k = -\\nabla f(x_k)$。所有计算都应在实数算术中执行，无需任何外部数据输入。\n\n将此方案应用于以下测试套件。在所有情况下，使用欧几里得范数 $\\|\\cdot\\|_2$ 作为终止准则，并在每次迭代时从步长 $\\alpha_k = 1$ 开始回溯。报告每种情况下由终止条件返回的极小点估计值。将报告的每个数字表示为四舍五入到小数点后六位的小数。\n\n测试用例 A（两个参数的二元选择负对数似然）。设参数为 $x = (b_0, b_1)^\\top \\in \\mathbb{R}^2$。设数据为 $\\{(x_i,y_i)\\}_{i=1}^5$，其中\n$$\n(x_1,y_1) = (-2,0),\\quad (x_2,y_2) = (-1,0),\\quad (x_3,y_3) = (0,1),\\quad (x_4,y_4) = (1,1),\\quad (x_5,y_5) = (2,0).\n$$\n定义负对数似然\n$$\nf(x) = \\sum_{i=1}^5 \\left[ \\log\\!\\left(1 + e^{b_0 + b_1 x_i}\\right) - y_i\\,(b_0 + b_1 x_i) \\right].\n$$\n使用初始点 $x_0 = (0,0)^\\top$。\n\n测试用例 B（病态二次型）。设 $x \\in \\mathbb{R}^2$。定义\n$$\nf(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x,\\quad Q = \\begin{bmatrix} 10^{-6}  0 \\\\ 0  1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 10^{-6} \\\\ 1 \\end{bmatrix}.\n$$\n使用初始点 $x_0 = (10,-10)^\\top$。\n\n测试用例 C（非凸一维四次函数）。设 $x \\in \\mathbb{R}$。定义\n$$\nf(x) = x^4 - 3 x^2 + x.\n$$\n使用初始点 $x_0 = 0.2$。\n\n输出规范。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果按顺序显示如下：\n- 对于测试用例 A：包含两个数字的列表 $[b_0^\\star,b_1^\\star]$，\n- 对于测试用例 B：包含两个数字的列表 $[x_1^\\star,x_2^\\star]$，\n- 对于测试用例 C：单个数字 $x^\\star$，\n每个数字都四舍五入到小数点后六位。例如，格式必须是\n$$\n\\big[ [b_0^\\star,b_1^\\star], [x_1^\\star,x_2^\\star], x^\\star \\big].\n$$\n打印行中不允许有空格。所有数字都应表示为普通小数。",
            "solution": "所提出的问题是数值优化中一个明确定义的任务。它要求实现一个混合迭代算法，该算法结合了牛顿法和最速下降法，并辅以回溯线搜索以确保每一步都有充分的下降。该问题在科学上是合理的，基于非线性优化的既定原则，并为三个不同的测试用例提供了所有必要的参数、初始条件和目标函数。因此，该问题被认为是有效的。\n\n待实现的算法是针对二次连续可微函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的无约束最小化过程。给定一个迭代点 $x_k$，下一个迭代点 $x_{k+1}$ 通过 $x_{k+1} = x_k + \\alpha_k p_k$ 找到，其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。\n\n算法的核心是搜索方向 $p_k$ 的选择。主要选择是牛顿方向，通过求解线性系统获得：\n$$\n\\nabla^2 f(x_k) p_k = - \\nabla f(x_k)\n$$\n其中 $\\nabla f(x_k)$ 是梯度，$\\nabla^2 f(x_k)$ 是 $f$ 在 $x_k$ 处的黑森矩阵。对于二次函数，该方向是最优的，并在海森矩阵为正定的极小点附近提供快速的局部收敛（二次收敛）。然而，如果海森矩阵是奇异的，牛顿方向可能没有定义；如果海森矩阵非正定，它也可能不是一个下降方向。如果 $\\nabla f(x_k)^\\top p_k  0$，则方向 $p_k$ 是一个下降方向。对于牛顿方向，$\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top (\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$，仅当 $\\nabla^2 f(x_k)$ 是正定的时候，该值才为负。如果牛顿方向不是下降方向或未能产生合适的步长，算法必须切换到一个稳健的替代方案。\n\n备用方向是最速下降方向，$p_k = -\\nabla f(x_k)$。只要梯度非零，这个方向保证是下降方向，因为 $\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top \\nabla f(x_k) = -\\|\\nabla f(x_k)\\|_2^2  0$。虽然其收敛速度通常较慢（线性收敛），但它能确保从任何 $\\nabla f(x_k) \\neq 0$ 的点取得进展。\n\n步长 $\\alpha_k$ 由回溯线搜索确定。从试探步长 $\\alpha = 1$ 开始，步长被连续乘以一个因子 $\\rho \\in (0,1)$，直到满足 Armijo 充分下降条件：\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^\\top p_k\n$$\n对于一个常数 $c_1 \\in (0,1)$。问题设定 $c_1 = 10^{-4}$ 和 $\\rho = 0.5$。施加了 $M=50$ 步回溯的限制。\n\n总的迭代过程如下：\n对于 $k = 0, 1, 2, \\ldots, K-1$：\n1. 计算梯度 $g_k = \\nabla f(x_k)$。如果 $\\|g_k\\|_2 \\le \\varepsilon$，则终止。\n2. 尝试计算牛顿方向 $p_{\\text{Newton}}$。如果 $\\nabla^2f(x_k)$ 是奇异的，或者所得方向不是下降方向（$\\nabla f(x_k)^\\top p_{\\text{Newton}} \\ge 0$），则此尝试失败。\n3. 如果牛顿方向有效，执行回溯线搜索。如果在 $M$ 次缩减内找到步长 $\\alpha_k$，则使用 $p_k = p_{\\text{Newton}}$ 和 $\\alpha_k$ 执行更新。\n4. 如果牛顿方向步骤因任何原因失败（奇异性、非下降方向或回溯失败），算法将退回到最速下降方向 $p_k = -\\nabla f(x_k)$，并执行另一次回溯线搜索以找到 $\\alpha_k$。\n5. 更新迭代点：$x_{k+1} = x_k + \\alpha_k p_k$。\n如果梯度范数低于容差 $\\varepsilon=10^{-8}$ 或达到最大迭代次数 $K=50$，则过程终止。\n\n该实现将把此逻辑应用于三个测试用例。对于每个用例，我们必须提供目标函数 $f(x)$、其梯度 $\\nabla f(x)$ 及其海森矩阵 $\\nabla^2 f(x)$ 的解析形式。\n\n**测试用例 A：二元选择负对数似然**\n目标函数是逻辑回归模型的负对数似然。\n$f(b_0, b_1) = \\sum_{i=1}^5 \\left[ \\log(1 + e^{b_0 + b_1 x_i}) - y_i(b_0 + b_1 x_i) \\right]$。\n设 $u_i(b) = b_0 + b_1 x_i$。Sigmoid 函数为 $\\sigma(u) = 1/(1+e^{-u})$。\n梯度分量为：\n$$\n\\frac{\\partial f}{\\partial b_0} = \\sum_{i=1}^5 (\\sigma(u_i) - y_i)\n$$\n$$\n\\frac{\\partial f}{\\partial b_1} = \\sum_{i=1}^5 x_i (\\sigma(u_i) - y_i)\n$$\n海森矩阵分量为（使用 $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$）：\n$$\n\\frac{\\partial^2 f}{\\partial b_0^2} = \\sum_{i=1}^5 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1 \\partial b_0} = \\sum_{i=1}^5 x_i \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1^2} = \\sum_{i=1}^5 x_i^2 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n该海森矩阵总是半正定的，确保了牛顿方向（如果已定义）是一个下降方向。\n\n**测试用例 B：病态二次型**\n目标函数是 $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$。\n梯度为 $\\nabla f(x) = Qx - b$。\n海森矩阵是常数：$\\nabla^2 f(x) = Q$。\n由于 $Q$ 是一个常数正定矩阵，使用完整步长（$\\alpha=1$）的牛顿法将在单次迭代中找到精确的极小点 $x^*=Q^{-1}b$。\n\n**测试用例 C：非凸一维四次函数**\n目标函数是 $f(x) = x^4 - 3x^2 + x$。\n梯度（导数）为 $f'(x) = 4x^3 - 6x + 1$。\n海森矩阵（二阶导数）为 $f''(x) = 12x^2 - 6$。\n在初始点 $x_0 = 0.2$ 处，海森矩阵为 $f''(0.2) = 12(0.04) - 6 = -5.52$，是负值。因此，$x_0$ 处的牛顿方向将是一个上升方向，算法在第一次迭代时必须退回到最速下降法。\n\n下面的 Python 代码实现了所描述的算法，并将其应用于三个测试用例，按规定格式化输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization for all test cases and print results.\n    \"\"\"\n    # Fixed constants for the algorithm\n    C1 = 1e-4\n    RHO = 0.5\n    EPS = 1e-8\n    MAX_ITER = 50\n    BT_LIMIT = 50\n\n    def optimizer(f, grad_f, hess_f, x0):\n        \"\"\"\n        Implements the hybrid Newton/Gradient-Descent optimization algorithm.\n        \"\"\"\n        x = np.array(x0, dtype=float)\n\n        for _ in range(MAX_ITER):\n            g = grad_f(x)\n            if np.linalg.norm(g) = EPS:\n                return x\n\n            p = None\n            alpha = None\n            newton_step_succeeded = False\n\n            # --- Attempt Newton's Method ---\n            try:\n                H = hess_f(x)\n                p_newton = np.linalg.solve(H, -g)\n\n                # Newton direction must be a descent direction\n                if np.dot(g, p_newton)  0:\n                    # Backtracking line search for Newton's direction\n                    alpha_bt = 1.0\n                    fk = f(x)\n                    g_dot_p = np.dot(g, p_newton)\n                    for _ in range(BT_LIMIT):\n                        if f(x + alpha_bt * p_newton) = fk + C1 * alpha_bt * g_dot_p:\n                            p = p_newton\n                            alpha = alpha_bt\n                            newton_step_succeeded = True\n                            break\n                        alpha_bt *= RHO\n            except np.linalg.LinAlgError:\n                # Hessian is singular or ill-posed\n                pass\n\n            # --- Fallback to Steepest Descent ---\n            if not newton_step_succeeded:\n                p_sd = -g\n                \n                # Backtracking line search for steepest descent\n                alpha_bt = 1.0\n                fk = f(x)\n                g_dot_p = np.dot(g, p_sd)\n                for _ in range(BT_LIMIT):\n                    if f(x + alpha_bt * p_sd) = fk + C1 * alpha_bt * g_dot_p:\n                        p = p_sd\n                        alpha = alpha_bt\n                        break\n                    alpha_bt *= RHO\n            \n            # If no step could be found (highly unlikely for steepest descent)\n            if p is None or alpha is None:\n                return x\n\n            # Update x\n            x = x + alpha * p\n        \n        return x\n\n    # --- Test Case A: Logistic Regression ---\n    x_data_A = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data_A = np.array([0.0, 0.0, 1.0, 1.0, 0.0])\n    x0_A = np.array([0.0, 0.0])\n\n    def f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Use np.logaddexp for numerical stability: log(1+exp(x)) = logaddexp(0,x)\n        return np.sum(np.logaddexp(0, u) - y_data_A * u)\n\n    def grad_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Sigmoid function: 1 / (1 + exp(-u))\n        sigma = 1 / (1 + np.exp(-u))\n        diff = sigma - y_data_A\n        return np.array([np.sum(diff), np.sum(diff * x_data_A)])\n\n    def hess_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        sigma = 1 / (1 + np.exp(-u))\n        weights = sigma * (1 - sigma)\n        h00 = np.sum(weights)\n        h01 = np.sum(weights * x_data_A)\n        h11 = np.sum(weights * x_data_A**2)\n        return np.array([[h00, h01], [h01, h11]])\n\n    result_A = optimizer(f_A, grad_f_A, hess_f_A, x0_A)\n\n    # --- Test Case B: Ill-Conditioned Quadratic ---\n    Q_B = np.array([[1e-6, 0.0], [0.0, 1.0]])\n    b_vec_B = np.array([1e-6, 1.0])\n    x0_B = np.array([10.0, -10.0])\n\n    def f_B(x):\n        return 0.5 * x.T @ Q_B @ x - b_vec_B.T @ x\n    \n    def grad_f_B(x):\n        return Q_B @ x - b_vec_B\n    \n    def hess_f_B(x):\n        return Q_B\n\n    result_B = optimizer(f_B, grad_f_B, hess_f_B, x0_B)\n\n    # --- Test Case C: Nonconvex Quartic ---\n    x0_C = np.array([0.2])\n\n    def f_C(x):\n        xv = x[0]\n        return xv**4 - 3 * xv**2 + xv\n    \n    def grad_f_C(x):\n        xv = x[0]\n        return np.array([4 * xv**3 - 6 * xv + 1])\n    \n    def hess_f_C(x):\n        xv = x[0]\n        return np.array([[12 * xv**2 - 6]])\n    \n    result_C = optimizer(f_C, grad_f_C, hess_f_C, x0_C)\n\n    # --- Format Output ---\n    results_list = [result_A, result_B, result_C]\n    formatted_parts = []\n    for res in results_list:\n        if res.size > 1:\n            num_strs = [f\"{val:.6f}\" for val in res]\n            formatted_parts.append(f\"[{','.join(num_strs)}]\")\n        else:\n            formatted_parts.append(f\"{res.item():.6f}\")\n\n    final_output = f\"[[{','.join(formatted_parts)}]]\"\n    print(final_output.replace(\"],[\", \"],[\").replace(\"[[\",\"[\").replace(\"]]\",\"]\"))\n\nsolve()\n```"
        }
    ]
}