{
    "hands_on_practices": [
        {
            "introduction": "最速下降法（在实数积分中常称为拉普拉斯方法）的核心思想是，当参数 $\\lambda$ 很大时，形式为 $\\int g(t) e^{-\\lambda \\phi(t)} dt$ 的积分值主要由函数 $\\phi(t)$ 的全局最小值点附近的区域贡献。本练习将通过一个典型的一维积分问题，引导你实践这一核心思想。你需要找出积分指数上的函数 $\\phi(t)$ 的所有驻点，确定哪个（或哪些）是全局最小值点，并计算它们对积分的领导阶渐近贡献。",
            "id": "920264",
            "problem": "考虑由下式给出的积分\n$$I(\\lambda) = \\int_{-\\infty}^\\infty \\exp\\left[-\\lambda(t^2-a^2)^2\\right] dt$$\n其中 $\\lambda$ 是一个大的正参数，而 $a$ 是一个正实数常数。在 $\\lambda$ 很大时对这类积分进行求值是物理学和工程学等多个领域中的一个常见任务，通常可以使用最速下降法（也称为鞍点法）来解决。\n\n你的任务是找出当 $\\lambda \\to \\infty$ 时 $I(\\lambda)$ 的主阶渐近表达式。",
            "solution": "问题是找出当 $\\lambda \\to \\infty$ 时积分 $I(\\lambda) = \\int_{-\\infty}^\\infty \\exp\\left[-\\lambda(t^2-a^2)^2\\right] dt$ 的渐近行为。该积分是适用于最速下降法的标准形式 $\\int e^{-\\lambda \\phi(t)} dt$，其中相位函数为 $\\phi(t) = (t^2-a^2)^2$。\n\n最速下降法通过被积函数鞍点的贡献来近似该积分，这些鞍点对应于相位函数 $\\phi(t)$ 的最小值点。\n\n1.  **寻找鞍点：**\n    鞍点是 $\\phi(t)$ 的驻点，通过令其一阶导数为零来找到。\n    $$\\phi'(t) = \\frac{d}{dt}(t^2-a^2)^2 = 2(t^2-a^2)(2t) = 4t(t^2-a^2)$$\n    令 $\\phi'(t) = 0$ 得到 $4t(t-a)(t+a) = 0$。鞍点位于 $t_1 = 0$，$t_2 = a$ 和 $t_3 = -a$。\n\n2.  **计算鞍点处的相位函数值：**\n    为了确定哪些鞍点提供主要贡献，我们计算每个鞍点处的 $\\phi(t)$ 值。积分由 $\\phi(t)$ 取最小值处的点主导，因为在这些点指数项 $e^{-\\lambda \\phi(t)}$ 最大。\n    $$\\phi(t_1) = \\phi(0) = (0^2 - a^2)^2 = a^4$$\n    $$\\phi(t_2) = \\phi(a) = (a^2 - a^2)^2 = 0$$\n    $$\\phi(t_3) = \\phi(-a) = ((-a)^2 - a^2)^2 = 0$$\n    因为 $a$ 是一个正常数，所以 $a^4 > 0$。$\\phi(t)$ 的最小值为 $0$，出现在两个鞍点 $t = a$ 和 $t = -a$ 处。因此，积分的主阶渐近行为由这两个点邻域的贡献之和给出。来自 $t=0$ 的贡献将被因子 $e^{-\\lambda a^4}$ 抑制，因此对于大的 $\\lambda$ 可以忽略不计。\n\n3.  **分析主导鞍点附近的行为：**\n    鞍点 $t_k$ 对积分的贡献由其邻域内的高斯积分近似给出。对于一个简单鞍点 $t_k$，其主阶贡献的标准公式为：\n    $$I_k(\\lambda) \\sim \\sqrt{\\frac{2\\pi}{\\lambda \\phi''(t_k)}} e^{-\\lambda \\phi(t_k)}$$\n    我们需要 $\\phi(t)$ 的二阶导数：\n    $$\\phi''(t) = \\frac{d}{dt}(4t^3 - 4a^2t) = 12t^2 - 4a^2$$\n    现在，我们计算两个主导鞍点处的 $\\phi''(t)$：\n    对于 $t=a$：\n    $$\\phi''(a) = 12a^2 - 4a^2 = 8a^2$$\n    对于 $t=-a$：\n    $$\\phi''(-a) = 12(-a)^2 - 4a^2 = 12a^2 - 4a^2 = 8a^2$$\n    由于 $\\phi''(a)$ 和 $\\phi''(-a)$ 均为正，这些点确实是沿实轴的极小值点，并且积分路径（实轴）是通过这些鞍点的最速下降路径。\n\n4.  **计算贡献并求和：**\n    来自鞍点 $t=a$ 的贡献为：\n    $$I_a(\\lambda) \\sim \\sqrt{\\frac{2\\pi}{\\lambda \\phi''(a)}} e^{-\\lambda \\phi(a)} = \\sqrt{\\frac{2\\pi}{\\lambda (8a^2)}} e^{-\\lambda(0)} = \\sqrt{\\frac{\\pi}{4\\lambda a^2}} = \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}}$$\n    来自鞍点 $t=-a$ 的贡献为：\n    $$I_{-a}(\\lambda) \\sim \\sqrt{\\frac{2\\pi}{\\lambda \\phi''(-a)}} e^{-\\lambda \\phi(-a)} = \\sqrt{\\frac{2\\pi}{\\lambda (8a^2)}} e^{-\\lambda(0)} = \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}}$$\n    积分的总主阶渐近近似是这两个贡献的和：\n    $$I(\\lambda) \\sim I_a(\\lambda) + I_{-a}(\\lambda) = \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}} + \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}} = \\frac{2\\sqrt{\\pi}}{2a\\sqrt{\\lambda}} = \\frac{\\sqrt{\\pi}}{a\\sqrt{\\lambda}}$$",
            "answer": "$$\\boxed{\\frac{\\sqrt{\\pi}}{a\\sqrt{\\lambda}}}$$"
        },
        {
            "introduction": "在应用拉普拉斯方法时，我们不仅要关注函数在积分区间内部的驻点，还必须考虑其在边界上的行为。全局极值点完全可能出现在积分区间的端点上，这在实际问题中是常见的情况。这个练习  将挑战你处理一个定义在有限区间上的积分，你需要比较来自内部鞍点和边界端点的贡献，从而确定积分在 $\\lambda \\to \\infty$ 时的主要行为。",
            "id": "920333",
            "problem": "考虑积分 $I(\\lambda)$，其定义为\n$$\nI(\\lambda) = \\int_{-1/2}^{2} \\exp\\left[\\lambda\\left(3t^2 - 2t^3\\right)\\right] dt\n$$\n其中 $\\lambda$ 是一个大的正实数参数。\n\n求当 $\\lambda \\to \\infty$ 时，$I(\\lambda)$ 的主阶渐近行为。你的答案应为一个以 $\\lambda$ 和标准数学常数表示的闭式解析表达式。",
            "solution": "我们应用拉普拉斯方法于\n$$I(\\lambda)=\\int_{-1/2}^{2}e^{\\lambda f(t)}dt,\\qquad f(t)=3t^2-2t^3.$$\n1. 临界点：\n$$f'(t)=6t-6t^2=6t(1-t)=0\\implies t=0,1.$$\n端点为 $t=-1/2,\\,2$。\n2. $f$ 的值：\n$$f(-\\tfrac12)=\\tfrac34+ \\tfrac14=1,\\quad f(0)=0,\\quad f(1)=1,\\quad f(2)=-4.$$\n因此全局最大值出现在 $t=-\\tfrac12$ 和 $t=1$ 处，两处均使得 $f=1$。\n3. 来自内部最大值点 $t=1$ 的贡献：\n$$f''(1)=6-12=-6,\\quad \n\\hbox{so }\n\\int_{\\,\\text{near }1}e^{\\lambda f(t)}dt\\sim e^{\\lambda}\\sqrt{\\frac{2\\pi}{\\lambda\\,|f''(1)|}}\n=e^{\\lambda}\\sqrt{\\frac{2\\pi}{6\\lambda}}\n=e^{\\lambda}\\sqrt{\\frac{\\pi}{3\\lambda}}\\,. $$\n4. 来自端点最大值点 $t=-\\tfrac12$ 的贡献：因为 $f'(-\\tfrac12)=-\\tfrac92$，\n$$\\int_{-1/2}^{\\,\\text{near }-1/2}e^{\\lambda f(t)}dt\\sim\n\\frac{e^{\\lambda f(-1/2)}}{\\lambda|f'(-1/2)|}\n=\\frac{e^{\\lambda}}{\\lambda\\,(9/2)}\n=\\frac{2}{9\\lambda}e^{\\lambda}\\,. $$\n5. 将各项相加并保留当 $\\lambda\\to\\infty$ 时的主导项，得到\n$$I(\\lambda)\\sim e^{\\lambda}\\sqrt{\\frac{\\pi}{3\\lambda}}+\\frac{2}{9\\lambda}e^{\\lambda}\n\\sim e^{\\lambda}\\sqrt{\\frac{\\pi}{3\\lambda}}\\,. $$",
            "answer": "$$\\boxed{e^{\\lambda}\\sqrt{\\frac{\\pi}{3\\lambda}}}$$"
        },
        {
            "introduction": "最速下降原理不仅是分析积分的强大工具，更是现代计算科学中一类重要优化算法的基石。其核心思想——沿着函数下降最快的方向进行迭代搜索——被广泛用于求解复杂的优化问题。这个动手编程练习  旨在让你将理论付诸实践，通过为一个非线性函数实现最速下降算法，并与牛顿法进行比较，你将直观地理解其收敛特性、稳健性，以及它在何种情况下优于高阶方法。",
            "id": "3159949",
            "problem": "您需要在一个光滑标量场上，严格从第一性原理出发，实现并比较两种迭代优化方法：最速下降法和牛顿法。目标函数是二维实值函数，定义为 $f(\\mathbf{x}) = f(x,y) = x^{2} + y^{4}$。目的是通过显式计算，展示这两种方法的内在原理和数值行为，包括一种由于 Hessian 矩阵在对称正定（Symmetric Positive Definite (SPD)）意义下非正定而导致牛顿法失效的情况。\n\n您的任务是：\n\n- 仅使用多变量微积分的基本定义：可微性、方向导数、梯度作为欧几里得范数下的最速上升方向，以及二阶泰勒近似，来推导最速下降法和牛顿法所选择的搜索方向。不要假设任何预先封装好的迭代公式。\n- 实现带有回溯线搜索的最速下降法，该线搜索强制执行基于一阶模型的标准充分下降条件。对于此问题，当从任何给定的初始点开始时，该方法必须是稳定的，并能产生单调递减的函数值序列。\n- 将牛顿法实现为对从二阶泰勒展开得到的二次模型的最小化；在每次迭代中，通过求解一个线性系统来计算步长。如果在某次迭代中 Hessian 矩阵不可逆（例如，非对称正定以致无法求解线性系统），导致牛顿步未定义，则声明牛顿法在该迭代点失败。\n- 在 $\\mathbb{R}^{2}$ 中使用欧几里得范数。对两种方法使用以下通用停止准则：当 $\\lVert \\nabla f(\\mathbf{x}_{k}) \\rVert_{2} \\le \\varepsilon$ 且容差 $\\varepsilon = 10^{-10}$ 时成功停止。牛顿法最多使用 $50$ 次迭代，最速下降法最多使用 $10000$ 次迭代。对于最速下降法中的回溯线搜索，使用初始步长 $\\alpha_{0} = 1$、充分下降参数 $c_{1} = 10^{-4}$ 和收缩因子 $\\tau = \\tfrac{1}{2}$。如果步长在未满足充分下降条件的情况下降至 $10^{-16}$ 以下，则声明最速下降法在该次迭代失败。\n- 程序必须显式处理 Hessian 矩阵的奇异性：如果因为 Hessian 矩阵奇异（或因其他原因导致无法得到唯一解）而无法求解牛顿步的线性系统，则报告牛顿法在该测试用例中失败。\n\n测试套件：\n实现您的程序以运行以下初始点 $(x_{0},y_{0})$：\n- 情况 A（常规“成功路径”）：$(x_{0},y_{0}) = (1,1)$。\n- 情况 B（Hessian 矩阵沿一轴奇异）：$(x_{0},y_{0}) = (1,0)$。\n- 情况 C（起始点即为驻点最优解）：$(x_{0},y_{0}) = (0,0)$。\n- 情况 D（负坐标，四次项非对称性测试）：$(x_{0},y_{0}) = (5,-1)$。\n\n可量化的输出：\n对于按 A、B、C、D 顺序排列的每个测试用例 $i$，按顺序计算并报告以下六个值：\n- $n\\_status\\_i$：一个整数，其中 $1$ 表示牛顿法在迭代次数限制内收敛，$0$ 表示它未能收敛或因 Hessian 矩阵不可逆而无法计算步长。\n- $n\\_iters\\_i$：牛顿法停止时（无论是收敛还是失败）实际执行的整数迭代次数。\n- $n\\_f\\_i$：牛顿法终止时的最终函数值 $f$，以浮点数表示。\n- $sd\\_status\\_i$：一个整数，其中 $1$ 表示最速下降法在迭代次数限制内收敛，$0$ 表示它在给定的线搜索规则下未能收敛。\n- $sd\\_iters\\_i$：最速下降法停止时实际执行的整数迭代次数。\n- $sd\\_f\\_i$：最速下降法终止时的最终函数值 $f$，以浮点数表示。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有四种情况的串联结果，形式为用方括号括起来的逗号分隔列表。即，输出必须是\n$[n\\_status\\_A,n\\_iters\\_A,n\\_f\\_A,sd\\_status\\_A,sd\\_iters\\_A,sd\\_f\\_A,n\\_status\\_B,n\\_iters\\_B,n\\_f\\_B,sd\\_status\\_B,sd\\_iters\\_B,sd\\_f\\_B,n\\_status\\_C,n\\_iters\\_C,n\\_f\\_C,sd\\_status\\_C,sd\\_iters\\_C,sd\\_f\\_C,n\\_status\\_D,n\\_iters\\_D,n\\_f\\_D,sd\\_status\\_D,sd\\_iters\\_D,sd\\_f\\_D]$。",
            "solution": "该问题要求实现并比较分析两种基本的迭代优化算法——最速下降法和牛顿法，应用于多元函数 $f(\\mathbf{x}) = f(x, y) = x^2 + y^4$ 的最小化问题。迭代步骤的推导必须从多变量微积分的第一性原理出发，并且实现必须遵守指定的参数，并处理牛顿法中涉及奇异 Hessian 矩阵的特定失败情况。\n\n首先，我们为函数 $f(\\mathbf{x})$（其中 $\\mathbf{x} = [x, y]^T \\in \\mathbb{R}^2$）建立必要的数学对象。该函数为 $f(x, y) = x^2 + y^4$。\n$f$ 的梯度是其偏导数组成的向量，两种方法都需要它。梯度指向局部最速上升的方向。\n$$\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2x \\\\ 4y^3 \\end{bmatrix}\n$$\n一个点 $\\mathbf{x}^*$ 成为最小化点的必要条件是梯度为零，即 $\\nabla f(\\mathbf{x}^*) = \\mathbf{0}$。对于给定的函数，此条件为 $2x = 0$ 和 $4y^3 = 0$，唯一解为点 $(0, 0)$。因此，$\\mathbf{x}^* = (0, 0)^T$ 是唯一的驻点，并且由于对所有 $\\mathbf{x} \\neq \\mathbf{0}$ 都有 $f(\\mathbf{x}) > 0$ 且 $f(\\mathbf{0}) = 0$，所以该点是全局最小值点。\n\n对于牛顿法，还需要 Hessian 矩阵，即二阶偏导数矩阵。\n$$\nH_f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2}  \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x}  \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  12y^2 \\end{bmatrix}\n$$\nHessian 矩阵描述了函数的局部曲率。\n\n**最速下降法**\n\n最速下降法是一种迭代算法，每一步都沿着与梯度相反的方向移动。这个方向 $-\\nabla f(\\mathbf{x})$ 是局部最速下降的方向。\n其推导始于函数 $f$ 在点 $\\mathbf{x}_k$ 周围的一阶泰勒近似：\n$$\nf(\\mathbf{x}_k + \\mathbf{p}) \\approx f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^T \\mathbf{p}\n$$\n为了在给定一个小的固定步长 $\\lVert \\mathbf{p} \\rVert_2 = \\delta$ 的情况下，使 $f$ 的值下降最大，我们必须最小化 $\\nabla f(\\mathbf{x}_k)^T \\mathbf{p}$。根据柯西-施瓦茨不等式，当 $\\mathbf{p}$ 与 $\\nabla f(\\mathbf{x}_k)$ 反平行时，该点积最小。因此，搜索方向 $\\mathbf{p}_k$ 选择为：\n$$\n\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)\n$$\n因此，迭代更新规则为：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)\n$$\n其中 $\\alpha_k > 0$ 是步长。选择一个合适的 $\\alpha_k$ 值对收敛至关重要。我们采用回溯线搜索来寻找合适的 $\\alpha_k$。从一个初始猜测 $\\alpha = \\alpha_0$ 开始，步长通过收缩因子 $\\tau$ 不断减小，直到满足 Armijo 充分下降条件：\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^T \\mathbf{p}_k\n$$\n对于我们的选择 $\\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k)$，该条件简化为：\n$$\nf(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)) \\le f(\\mathbf{x}_k) - c_1 \\alpha \\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2^2\n$$\n线搜索的参数指定为 $\\alpha_0 = 1$，$c_1 = 10^{-4}$，以及 $\\tau = 1/2$。当 $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\le \\varepsilon = 10^{-10}$ 或达到最大迭代次数 $10000$ 时，方法终止。如果在回溯线搜索过程中，步长 $\\alpha$ 在未满足条件的情况下变得小于 $10^{-16}$，则声明方法失败。\n\n**牛顿法**\n\n牛顿法通过在当前迭代点 $\\mathbf{x}_k$ 周围使用二阶泰勒展开，来构建一个更复杂的函数模型：\n$$\nf(\\mathbf{x}_k + \\mathbf{p}) \\approx m_k(\\mathbf{p}) = f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^T \\mathbf{p} + \\frac{1}{2}\\mathbf{p}^T H_f(\\mathbf{x}_k) \\mathbf{p}\n$$\n该方法通过寻找这个二次模型 $m_k(\\mathbf{p})$ 的最小化点来确定下一步的步长 $\\mathbf{p}_k$。$\\mathbf{p}_k$ 最小化 $m_k(\\mathbf{p})$ 的一个必要条件是模型关于 $\\mathbf{p}$ 的梯度为零：\n$$\n\\nabla m_k(\\mathbf{p}) = \\nabla f(\\mathbf{x}_k) + H_f(\\mathbf{x}_k) \\mathbf{p} = \\mathbf{0}\n$$\n这就得到了牛顿系统，一个关于步长 $\\mathbf{p}_k$ 的线性方程组：\n$$\nH_f(\\mathbf{x}_k) \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)\n$$\n接着，下一个迭代点为 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{p}_k$。在这种纯粹形式的方法中，不需要线搜索（步长 $\\alpha_k=1$ 总是被使用）。\n\n当且仅当 Hessian 矩阵 $H_f(\\mathbf{x}_k)$ 可逆时，$\\mathbf{p}_k$ 才存在唯一解。如果 $H_f(\\mathbf{x}_k)$ 是奇异的（不可逆），牛顿步就没有被良好定义，方法失败。对于函数 $f(x,y) = x^2 + y^4$，其 Hessian 矩阵为 $H_f(\\mathbf{x}) = \\text{diag}(2, 12y^2)$，行列式为 $\\det(H_f(\\mathbf{x})) = 24y^2$。Hessian 矩阵奇异当且仅当 $y=0$。因此，如果某个迭代点 $\\mathbf{x}_k$ 的 y 分量为零，牛顿法将会失败，除非该点的梯度已经为零。\n\n当 $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\le \\varepsilon = 10^{-10}$ 或达到最大迭代次数 $50$ 时，方法终止。如果由于 Hessian 矩阵奇异而无法求解牛顿系统，则报告失败。\n\n**测试用例分析**\n\n- **情况 A: $\\mathbf{x}_0 = (1, 1)^T$。**此处 $y_0 \\ne 0$，因此初始 Hessian 矩阵 $H_f(1, 1) = \\text{diag}(2, 12)$ 是正定的。两种方法都预期会收敛到最小值点 $(0, 0)^T$。牛顿法预期会表现出二次收敛速度，并且比最速下降法快得多。\n\n- **情况 B: $\\mathbf{x}_0 = (1, 0)^T$。**在该点，$y_0 = 0$。梯度为 $\\nabla f(1, 0) = [2, 0]^T \\neq \\mathbf{0}$，因此我们不在最小值点。Hessian 矩阵为 $H_f(1, 0) = \\text{diag}(2, 0)$，是奇异的。牛顿系统 $H_f(\\mathbf{x}_0) \\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$ 是不相容的，没有唯一解。因此，牛顿法预期在第一次迭代时就会失败。最速下降法将使用搜索方向 $\\mathbf{p}_0 = -[2, 0]^T$ 继续进行，沿着 x 轴向原点移动，并预期会收敛。\n\n- **情况 C: $\\mathbf{x}_0 = (0, 0)^T$。**这是最优解。初始梯度为 $\\nabla f(0, 0) = [0, 0]^T$，其范数为 $0$，满足停止准则。两种方法都应该立即终止，迭代次数为 $0$。\n\n- **情况 D: $\\mathbf{x}_0 = (5, -1)^T$。**与情况 A 类似，$y_0 \\ne 0$，因此初始 Hessian 矩阵是正定的。两种方法都预期会收敛，其中牛顿法会快得多。由于函数依赖于 $y^4$，负的 y 坐标没有定性影响。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization algorithms on the specified test cases\n    and print the results in the required format.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def f(x_vec):\n        \"\"\"Target function f(x, y) = x^2 + y^4.\"\"\"\n        x, y = x_vec\n        return x**2 + y**4\n\n    def grad_f(x_vec):\n        \"\"\"Gradient of the target function.\"\"\"\n        x, y = x_vec\n        return np.array([2 * x, 4 * y**3], dtype=float)\n\n    def hess_f(x_vec):\n        \"\"\"Hessian of the target function.\"\"\"\n        x, y = x_vec\n        return np.array([[2.0, 0.0], [0.0, 12.0 * y**2]], dtype=float)\n\n    # --- Newton's Method Implementation ---\n    def newtons_method(x0, tol=1e-10, max_iter=50):\n        \"\"\"\n        Implements Newton's method for optimization.\n\n        Returns:\n            (status, iterations, final_f_val)\n            status: 1 for convergence, 0 for failure.\n            iterations: Number of iterations performed.\n            final_f_val: Function value at the final point.\n        \"\"\"\n        xk = np.array(x0, dtype=float)\n        \n        for k in range(max_iter):\n            gk = grad_f(xk)\n            \n            # Check for convergence\n            if np.linalg.norm(gk) = tol:\n                return 1, k, f(xk)\n            \n            Hk = hess_f(xk)\n            \n            try:\n                # Solve the Newton system: Hk * p = -gk\n                pk = np.linalg.solve(Hk, -gk)\n            except np.linalg.LinAlgError:\n                # Failure due to singular Hessian\n                return 0, k, f(xk)\n                \n            # Update step\n            xk = xk + pk\n            \n        # Failure due to exceeding max iterations\n        gk = grad_f(xk)\n        if np.linalg.norm(gk) = tol:\n             return 1, max_iter, f(xk)\n        return 0, max_iter, f(xk)\n\n    # --- Steepest Descent Implementation ---\n    def steepest_descent(x0, tol=1e-10, max_iter=10000):\n        \"\"\"\n        Implements the method of steepest descent with backtracking line search.\n\n        Returns:\n            (status, iterations, final_f_val)\n            status: 1 for convergence, 0 for failure.\n            iterations: Number of iterations performed.\n            final_f_val: Function value at the final point.\n        \"\"\"\n        xk = np.array(x0, dtype=float)\n        \n        # Line search parameters\n        alpha0 = 1.0\n        c1 = 1e-4\n        tau = 0.5\n        alpha_min = 1e-16\n\n        for k in range(max_iter):\n            fk = f(xk)\n            gk = grad_f(xk)\n            \n            # Check for convergence\n            if np.linalg.norm(gk) = tol:\n                return 1, k, fk\n            \n            # Search direction\n            pk = -gk\n            \n            # Backtracking line search\n            alpha = alpha0\n            gk_pk_dot = np.dot(gk, pk)\n            \n            while f(xk + alpha * pk) > fk + c1 * alpha * gk_pk_dot:\n                alpha *= tau\n                if alpha  alpha_min:\n                    return 0, k, fk # Line search failure\n            \n            # Update step\n            xk = xk + alpha * pk\n\n        # Failure due to exceeding max iterations\n        fk = f(xk)\n        gk = grad_f(xk)\n        if np.linalg.norm(gk) = tol:\n             return 1, max_iter, fk\n        return 0, max_iter, fk\n\n    # --- Test Suite ---\n    test_cases = [\n        (1.0, 1.0),   # Case A\n        (1.0, 0.0),   # Case B\n        (0.0, 0.0),   # Case C\n        (5.0, -1.0),  # Case D\n    ]\n\n    results = []\n    for x0 in test_cases:\n        # Run Newton's method\n        n_status, n_iters, n_f_val = newtons_method(x0)\n        results.extend([n_status, n_iters, n_f_val])\n        \n        # Run Steepest Descent\n        sd_status, sd_iters, sd_f_val = steepest_descent(x0)\n        results.extend([sd_status, sd_iters, sd_f_val])\n\n    # --- Final Output Formatting ---\n    # Python's default float representation is sufficient here.\n    output_str = ','.join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}