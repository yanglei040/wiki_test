{
    "hands_on_practices": [
        {
            "introduction": "梯度下降法的收敛速度在很大程度上取决于目标函数的几何形状。当目标函数的等高线呈“拉伸”的椭圆形（即病态问题）时，标准的梯度下降会沿着“之”字形路径缓慢地向最小值移动。本练习  将通过一个具体的二次函数优化问题，让你亲手构建并观察这种现象，并探索如何通过坐标旋转来对齐梯度方向与等高线主轴，从而显著加速收敛。",
            "id": "3139501",
            "problem": "您将实现、分析并比较梯度下降法的两种变体在一个二维凸二次函数上的表现。其目标是构建一个数学上可控的案例，在该案例中，基本方法由于各向异性（病态条件）而表现出“之”字形轨迹，然后演示基于旋转的变量变换如何结合轴对齐的逐坐标步长，将更新方向与等值线的主轴对齐，并加速收敛。\n\n从数值优化和线性代数中的以下核心定义和经过充分检验的事实出发：\n- 一个具有对称正定（Positive Definite, PD）Hessian 矩阵的二次连续可微函数是强凸的。特别地，对于对称正定矩阵 $$\\mathbf{Q} \\in \\mathbb{R}^{2\\times 2}$$，二次函数 $$f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\top} \\mathbf{Q}\\,\\mathbf{x}$$ 在 $$\\mathbf{x}^{\\star} = \\mathbf{0}$$ 处有唯一最小值点，其梯度为 $$\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\,\\mathbf{x}.$$\n- 用于最小化 $$f$$ 的梯度下降更新（采用固定步长 $$\\alpha \\in (0, 2/L)$$）为 $$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\nabla f(\\mathbf{x}_{k}).$$ 对于上述二次函数，这变为 $$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k}.$$\n- 对于一个旋转角度为 $$\\theta$$（以弧度为单位）的正交旋转矩阵 $$\\mathbf{R}(\\theta) = \\begin{bmatrix}\\cos\\theta  -\\sin\\theta\\\\ \\sin\\theta  \\cos\\theta\\end{bmatrix}$$，变换 $$\\mathbf{y} = \\mathbf{R}^{\\top}\\mathbf{x}$$ 会改变坐标，但不会改变欧几里得范数，即 $$\\|\\mathbf{y}\\|_{2} = \\|\\mathbf{x}\\|_{2}.$$\n\n您将考虑一类凸二次函数，其 Hessian 矩阵是通过旋转一个对角特征值矩阵来构造的。设 $$\\lambda_{1}  \\lambda_{2}  0$$ 并设置 $$\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2}), \\quad \\mathbf{Q} = \\mathbf{R}(\\theta)\\,\\mathbf{\\Lambda}\\,\\mathbf{R}(\\theta)^{\\top}.$$ 较大的比率 $$\\lambda_{1}/\\lambda_{2}$$ 会引起各向异性（病态条件），这导致当 $$\\theta \\neq 0$$ 时，基本的梯度下降法在原始坐标系中会走出“之”字形路径。\n\n您的任务是：\n- 在原始坐标系中实现基准梯度下降法，使用固定步长 $$\\alpha = 1/\\lambda_{\\max} = 1/\\lambda_{1}$$，因此 $$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k}.$$ 使用初始条件 $$\\mathbf{x}_{0} = (1,\\,1)^{\\top}$$。当欧几里得范数满足 $$\\|\\mathbf{x}_{k}\\|_{2} \\le \\varepsilon$$ 时终止。\n- 实现一个旋转坐标、轴对齐、逐坐标步长的变体。定义旋转后的变量 $$\\mathbf{y} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}$$，并使用逐轴学习率 $$\\alpha_{1} = c/\\lambda_{1}, \\ \\alpha_{2} = c/\\lambda_{2}$$（其中 $$c \\in (0,2)$$ 是一个共享标量）在旋转坐标系中进行更新，即 $$\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - \\mathrm{diag}(\\alpha_{1}, \\alpha_{2})\\,\\nabla_{\\mathbf{y}} f(\\mathbf{y}_{k}), \\quad \\nabla_{\\mathbf{y}} f(\\mathbf{y}) = \\mathbf{\\Lambda}\\,\\mathbf{y}.$$ 使用在旋转坐标系中表示的相同初始条件 $$\\mathbf{y}_{0} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{0}$$，并在 $$\\|\\mathbf{y}_{k}\\|_{2} \\le \\varepsilon$$ 时终止。请注意，这种选择将更新方向与主轴对齐，并解耦了沿每个轴的动力学。您必须使用与基准方法相同的 $$\\varepsilon$$。\n- 对于下面的每个测试用例，计算基准方法所需的迭代次数 $$N_{\\mathrm{plain}}$$ 和旋转坐标、逐轴变体所需的迭代次数 $$N_{\\mathrm{rot}}$$，并报告定义为浮点数 $$S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}$$ 的加速比。\n\n角度单位说明：所有角度 $$\\theta$$ 均以弧度为单位提供，并且必须以弧度进行解释。\n\n数值终止与安全：如果任一方法在 $$10^{6}$$ 次迭代内未满足停止准则，则停止并使用已达到的迭代次数；但是，所有提供的测试用例都在此限制范围内。\n\n测试套件（理想情况，显著各向异性；中等各向异性；近各向同性边缘情况）：\n- 测试 $$1$$: $$(\\lambda_{1}, \\lambda_{2}, \\theta, \\varepsilon, c) = (200.0,\\,1.0,\\,\\pi/4,\\,10^{-8},\\,1.6).$$\n- 测试 $$2$$: $$(\\lambda_{1}, \\lambda_{2}, \\theta, \\varepsilon, c) = (10.0,\\,1.0,\\,0.4,\\,10^{-8},\\,1.6).$$\n- 测试 $$3$$: $$(\\lambda_{1}, \\lambda_{2}, \\theta, \\varepsilon, c) = (1.0,\\,1.0,\\,0.7,\\,10^{-8},\\,1.6).$$\n\n您的程序必须：\n- 根据指定的 $$(\\lambda_{1}, \\lambda_{2}, \\theta)$$ 构建 $$\\mathbf{Q}$$。\n- 从 $$\\mathbf{x}_{0} = (1,\\,1)^{\\top}$$ 开始，使用相应的步长规则和停止条件运行两种方法。\n- 对于每个测试，计算 $$S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}$$ 并四舍五入到 $$3$$ 位小数。\n\n最终输出格式：您的程序应生成单行输出，其中包含三个四舍五入后的结果，格式为方括号内的逗号分隔列表，例如 $$[S_{1},S_{2},S_{3}]$$，其中每个 $$S_{i}$$ 是一个四舍五入到 $$3$$ 位小数的浮点数。不应打印任何其他文本。",
            "solution": "用户提供的问题陈述是有效的。它在科学上基于数值优化和线性代数的原理，问题设定良好，定义了所有必要的参数和条件，并且其表述是客观的。任务是分析两种梯度下降变体在二维凸二次函数上的性能，这是计算科学中一个经典且具有指导意义的问题。\n\n解决方案的步骤是，首先实现基准梯度下降法并分析其行为，然后实现坐标旋转变体并分析其在面对各向异性时改进的收敛特性。最后，针对每个测试用例，计算两种方法的迭代次数以确定加速比。\n\n这个问题的目标函数是凸二次型 $f(\\mathbf{x}) = \\frac{1}{2}\\,\\mathbf{x}^{\\top} \\mathbf{Q}\\,\\mathbf{x}$。唯一的最小值点在 $\\mathbf{x}^{\\star} = \\mathbf{0}$，梯度为 $\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\,\\mathbf{x}$。Hessian 矩阵 $\\mathbf{Q}$ 被构造为 $\\mathbf{Q} = \\mathbf{R}(\\theta)\\,\\mathbf{\\Lambda}\\,\\mathbf{R}(\\theta)^{\\top}$，其中 $\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2})$ 的特征值为 $\\lambda_{1}  \\lambda_{2}  0$，而 $\\mathbf{R}(\\theta)$ 是旋转角度为 $\\theta$ 的标准 $2\\times 2$ 旋转矩阵。这种构造创建了一个函数，其等值线是主轴与 $\\mathbf{R}(\\theta)$ 的列向量对齐的椭圆。比率 $\\kappa = \\lambda_{1}/\\lambda_{2}$ 被称为条件数，它衡量了这些椭圆的各向异性程度。一个大的 $\\kappa$ 意味着一个病态问题。\n\n### 方法 1：基准梯度下降法\n\n标准的梯度下降算法通过沿负梯度方向移动一步来更新当前迭代点 $\\mathbf{x}_{k}$：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\nabla f(\\mathbf{x}_{k})\n$$\n其中 $\\alpha  0$ 是步长或学习率。对于给定的二次函数，这变为：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k} = (\\mathbf{I} - \\alpha\\,\\mathbf{Q})\\,\\mathbf{x}_{k}\n$$\n问题指定了一个固定的步长 $\\alpha = 1/\\lambda_{\\max}(\\mathbf{Q}) = 1/\\lambda_{1}$。这个迭代过程的收敛性由迭代矩阵 $\\mathbf{M} = \\mathbf{I} - \\alpha\\,\\mathbf{Q}$ 的谱半径决定。$\\mathbf{M}$ 的特征值为 $1-\\alpha\\lambda_{i}$。当 $\\alpha=1/\\lambda_1$ 时，特征值为 $1 - \\lambda_1/\\lambda_1 = 0$ 和 $1 - \\lambda_2/\\lambda_1$。因此，谱半径为 $\\rho(\\mathbf{M}) = |1 - \\lambda_2/\\lambda_1| = 1 - 1/\\kappa$。当条件数 $\\kappa$ 很大时，$\\rho(\\mathbf{M})$ 非常接近于 $1$，导致收敛缓慢。该算法从 $\\mathbf{x}_{0} = (1,\\,1)^{\\top}$ 开始，当欧几里得范数 $\\|\\mathbf{x}_{k}\\|_{2} \\le \\varepsilon$ 时终止，得到迭代次数 $N_{\\mathrm{plain}}$。\n\n### 方法 2：旋转坐标梯度下降法\n\n该方法首先执行变量变换，将坐标系与函数等值线的主轴对齐。新的坐标向量定义为 $\\mathbf{y} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}$。逆变换为 $\\mathbf{x} = \\mathbf{R}(\\theta)\\mathbf{y}$。将此代入目标函数可得：\n$$\nf(\\mathbf{y}) = \\frac{1}{2}\\,\\left(\\mathbf{R}(\\theta)\\mathbf{y}\\right)^{\\top} \\left(\\mathbf{R}(\\theta)\\mathbf{\\Lambda}\\mathbf{R}(\\theta)^{\\top}\\right) \\left(\\mathbf{R}(\\theta)\\mathbf{y}\\right)\n$$\n由于 $\\mathbf{R}(\\theta)$ 是正交的，$\\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta) = \\mathbf{I}$。表达式简化为：\n$$\nf(\\mathbf{y}) = \\frac{1}{2}\\,\\mathbf{y}^{\\top} \\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta)\\mathbf{\\Lambda}\\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta)\\mathbf{y} = \\frac{1}{2}\\,\\mathbf{y}^{\\top} \\mathbf{\\Lambda}\\,\\mathbf{y}\n$$\n在这个新的坐标系中，Hessian 矩阵是对角矩阵 $\\mathbf{\\Lambda}$，这意味着问题解耦为两个独立的一维优化问题。在 $\\mathbf{y}$ 坐标系中的梯度是 $\\nabla_{\\mathbf{y}} f(\\mathbf{y}) = \\mathbf{\\Lambda}\\mathbf{y}$。\n\n更新规则使用逐坐标步长，$\\alpha_{1} = c/\\lambda_{1}$ 和 $\\alpha_{2} = c/\\lambda_{2}$，其中 $c \\in (0, 2)$ 是一个共享标量。更新过程为：\n$$\n\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - \\mathrm{diag}(\\alpha_{1}, \\alpha_{2})\\,\\nabla_{\\mathbf{y}} f(\\mathbf{y}_{k}) = \\mathbf{y}_{k} - \\mathrm{diag}(c/\\lambda_{1}, c/\\lambda_{2})\\,(\\mathbf{\\Lambda}\\mathbf{y}_{k})\n$$\n由于 $\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_1, \\lambda_2)$，对角矩阵的乘积简化为：\n$$\n\\mathrm{diag}(c/\\lambda_{1}, c/\\lambda_{2})\\,\\mathrm{diag}(\\lambda_{1}, \\lambda_{2}) = \\mathrm{diag}(c, c) = c\\mathbf{I}\n$$\n因此，更新规则变得非常简单：\n$$\n\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - c\\mathbf{I}\\mathbf{y}_{k} = (1-c)\\mathbf{y}_{k}\n$$\n这是一个简单的几何收缩。收敛速率为 $|1-c|$，它与特征值 $\\lambda_1, \\lambda_2$无关，因此也与原始问题的条件数无关。该算法从变换后的初始条件 $\\mathbf{y}_{0} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{0}$ 开始，当 $\\|\\mathbf{y}_{k}\\|_{2} \\le \\varepsilon$ 时终止，得到迭代次数 $N_{\\mathrm{rot}}$。请注意，$\\|\\mathbf{y}_{k}\\|_{2} = \\|\\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{k}\\|_{2} = \\|\\mathbf{x}_{k}\\|_{2}$，因为旋转保留了欧几里得范数。\n\n### 加速比计算\n\n旋转坐标方法相对于基准方法的性能提升由加速比 $S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}$ 来量化。对于每个测试用例，我们实现这两种算法，计算它们的迭代次数，并计算这个比率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares two gradient descent methods on a 2D convex quadratic,\n    calculates the speedup factor for three test cases.\n    \"\"\"\n    test_cases = [\n        # (lambda_1, lambda_2, theta, epsilon, c)\n        (200.0, 1.0, np.pi/4, 1e-8, 1.6),\n        (10.0, 1.0, 0.4, 1e-8, 1.6),\n        (1.0, 1.0, 0.7, 1e-8, 1.6),\n    ]\n\n    results = []\n    max_iterations = 1_000_000\n\n    for case in test_cases:\n        lambda_1, lambda_2, theta, epsilon, c = case\n\n        # Common initial condition\n        x0 = np.array([1.0, 1.0])\n\n        # --- Task 1: Baseline Gradient Descent ---\n        \n        # Construct the Hessian matrix Q\n        cos_th, sin_th = np.cos(theta), np.sin(theta)\n        R = np.array([[cos_th, -sin_th], [sin_th, cos_th]])\n        Lambda = np.diag([lambda_1, lambda_2])\n        Q = R @ Lambda @ R.T\n\n        # Set up and run the iteration\n        alpha = 1.0 / lambda_1\n        x_k = np.copy(x0)\n        N_plain = 0\n        while np.linalg.norm(x_k) > epsilon and N_plain  max_iterations:\n            gradient = Q @ x_k\n            x_k = x_k - alpha * gradient\n            N_plain += 1\n\n        # --- Task 2: Rotated-Coordinate Gradient Descent ---\n\n        # Initial condition in rotated coordinates\n        y0 = R.T @ x0\n        \n        # Rate of contraction is (1-c)\n        contraction_factor = 1.0 - c\n        \n        y_k = np.copy(y0)\n        N_rot = 0\n        while np.linalg.norm(y_k) > epsilon and N_rot  max_iterations:\n            # The update rule y_{k+1} = (1-c) * y_k is computationally simpler\n            # than re-calculating the gradient at each step, but we implement\n            # the specified update rule for formal correctness.\n            # grad_y = Lambda @ y_k\n            # step_sizes = np.diag([c / lambda_1, c / lambda_2])\n            # y_k = y_k - step_sizes @ grad_y\n            # The above is equivalent to:\n            y_k = contraction_factor * y_k\n            N_rot += 1\n\n        # --- Task 3: Compute Speedup Factor ---\n        if N_rot > 0:\n            speedup = N_plain / N_rot\n        else:\n            # Handle the case where N_rot could be zero, though not expected\n            # for the given test cases and epsilon > 0.\n            speedup = float('inf') if N_plain > 0 else 1.0\n\n        results.append(round(speedup, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在处理了简单的二次型问题后，我们将挑战一个更复杂的非二次目标函数——著名的 Rosenbrock 函数。该函数以其狭窄、弯曲的“山谷”而闻名，对许多优化算法构成了严峻考验。这个练习  旨在展示如何通过一个巧妙的非线性坐标变换，“拉直”这个弯曲的山谷，从而使得梯度下降法能够更直接、更高效地走向最小值。",
            "id": "3139529",
            "problem": "要求您研究坐标变换如何重塑目标函数的几何形状，从而加速梯度下降 (GD) 算法。研究对象为一个双变量函数，其等值线呈现出狭窄弯曲的山谷形状。研究必须纯粹以数学和算法的术语进行。\n\n定义 Rosenbrock 函数 $$f(x,y) = (1 - x)^2 + a\\,(y - x^2)^2$$ 其中 $a$ 是一个正实数参数，控制山谷的曲率和狭窄程度。对于任何 $a  0$，唯一的最小值点位于 $(x,y) = (1,1)$。实现两个优化过程：\n\n- 在原始坐标 $(x,y)$ 下的标准梯度下降 (GD)：从一个初始点 $(x_0,y_0)$ 开始，沿着 $f$ 的负梯度方向，以步长 $\\alpha  0$ 进行迭代更新，直到满足停止条件。\n- 在拉直山谷的坐标变换下的变换梯度下降 (GD)：使用非线性双射 $$T:\\mathbb{R}^2 \\to \\mathbb{R}^2,\\quad (x,y)\\mapsto (u,v) = (x,\\; y - x^2).$$ 在这些坐标下，最小值点满足 $(u,v) = (1,0)$，目标函数变为 $$g(u,v) = f\\big(u,\\,v + u^2\\big) = (1 - u)^2 + a\\,v^2.$$ 从 $(u_0,v_0) = T(x_0,y_0)$ 开始，在 $(u,v)$ 空间中使用 $g$ 的梯度执行 GD 更新，并在每次迭代时通过 $T^{-1}(u,v) = \\big(u,\\, v + u^2\\big)$ 映射回 $(x,y)$ 坐标，以评估在原始坐标中与最小值点的接近程度。为进行公平比较，使用与标准 GD 相同的步长 $\\alpha$ 和停止规则。\n\n设计科学依据：\n- 可微标量场的梯度指向最陡峭的上升方向；沿梯度反方向走小步会局部降低函数值。一阶泰勒展开提供了局部线性近似，这是最速下降步骤的动机。\n- 坐标变换可以改变优化景观的条件数。在这个问题中，变换 $T$ 将 $f$ 沿 $y = x^2$ 的弯曲山谷拉直为 $g$ 沿 $v = 0$ 的直线，这可以在 $(u,v)$ 空间应用 GD 时改善收敛特性。\n\n实现要求：\n- 解析计算 $f$ 和 $g$ 的梯度；不要使用数值有限差分。\n- 使用固定的步长 $\\alpha$ 和基于与最小值点接近程度的停止准则：当欧几里得距离 $$\\sqrt{(x - 1)^2 + (y - 1)^2}$$ 小于或等于容差 $\\varepsilon$ 时，或当迭代次数达到最大值 $N_{\\max}$ 时停止。\n- 为保证稳健性，如果任何迭代值变为非有限数（不是实数），则将该次运行视为未收敛，并为该情况返回最大迭代次数 $N_{\\max}$。\n\n测试套件：\n对于每个测试用例，参数为 $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max})$。请精确使用以下测试用例集，以覆盖典型情况、不同曲率、边界条件和具有挑战性的步长：\n\n1. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (100,\\; -1.2,\\; 1.0,\\; 0.001,\\; 10^{-6},\\; 50000)$\n2. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (1000,\\; -1.2,\\; 1.0,\\; 0.0003,\\; 10^{-6},\\; 50000)$\n3. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (10,\\; -1.2,\\; 1.0,\\; 0.002,\\; 10^{-6},\\; 50000)$\n4. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (100,\\; 1.0,\\; 1.0,\\; 0.001,\\; 10^{-12},\\; 1)$\n5. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (100,\\; -1.2,\\; 1.0,\\; 0.005,\\; 10^{-6},\\; 50000)$\n6. $(a, x_0, y_0, \\alpha, \\varepsilon, N_{\\max}) = (100,\\; 2.0,\\; 2.0,\\; 0.001,\\; 10^{-6},\\; 50000)$\n\n所需输出：\n- 对每个测试用例，运行两种方法并返回一对整数 $[n_{\\mathrm{std}}, n_{\\mathrm{trans}}]$，其中 $n_{\\mathrm{std}}$ 是标准 GD 使用的迭代次数，$n_{\\mathrm{trans}}$ 是变换 GD 使用的迭代次数，每个值都以 $N_{\\max}$ 为上限。\n- 您的程序应生成单行输出，其中包含一个逗号分隔的整数对列表，并用方括号括起来。例如：$$\\texttt{[[n\\_std\\_1,n\\_trans\\_1],[n\\_std\\_2,n\\_trans\\_2],...]}$$\n\n此问题不涉及角度和物理单位；所有量均为无量纲实数。确保所有数值输出均为整数，并严格遵守指定格式。",
            "solution": "问题陈述已经过验证，被认为是定义明确、科学合理且计算上可行的。它代表了数值优化中一个标准且富有洞察力的练习，适合用来展示坐标变换对基于梯度方法收敛性的影响。\n\n任务是实现并比较两种梯度下降 (GD) 算法，用于最小化 Rosenbrock 函数，这是一个以其狭窄的抛物线形山谷而闻名的常用优化算法基准。第一种算法是原始欧几里得坐标下的标准 GD。第二种算法采用一种坐标变换，旨在简化函数等值线的几何形状，从而加速收敛。\n\nRosenbrock 函数由下式给出：\n$$f(x,y) = (1 - x)^2 + a(y - x^2)^2$$\n其中 $a  0$ 是一个参数。唯一的全局最小值点位于 $(x,y)=(1,1)$，此时 $f(1,1)=0$。\n\n**1. 标准梯度下降 (GD)**\n\n该方法通过在与梯度 $\\nabla f(x_k, y_k)$ 相反的方向上迈出大小为 $\\alpha$ 的一步来迭代更新当前位置 $(x_k, y_k)$。更新规则是：\n$$(x_{k+1}, y_{k+1}) = (x_k, y_k) - \\alpha \\nabla f(x_k, y_k)$$\n\n为实现此方法，我们必须首先计算 $f(x,y)$ 的解析梯度。偏导数是：\n$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} \\left[ (1 - x)^2 + a(y - x^2)^2 \\right] = 2(1 - x)(-1) + a \\cdot 2(y - x^2)(-2x) = -2(1 - x) - 4ax(y - x^2)$$\n$$\\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} \\left[ (1 - x)^2 + a(y - x^2)^2 \\right] = a \\cdot 2(y - x^2)(1) = 2a(y - x^2)$$\n\n因此，梯度向量为：\n$$\\nabla f(x,y) = \\begin{pmatrix} -2(1 - x) - 4ax(y - x^2) \\\\ 2a(y - x^2) \\end{pmatrix}$$\n算法从一个初始点 $(x_0, y_0)$ 开始，并迭代地应用更新规则。当与最小值点的欧几里得距离 $\\sqrt{(x - 1)^2 + (y - 1)^2}$ 小于容差 $\\varepsilon$，或迭代次数达到最大值 $N_{\\max}$ 时，过程终止。Rosenbrock 山谷陡峭弯曲的谷壁常常导致标准 GD 算法采取许多小的、呈之字形的步骤，从而导致收敛缓慢，尤其是在 $a$ 值较大时。\n\n**2. 变换梯度下降 (GD)**\n\n该方法首先应用一个非线性坐标变换 $T(x,y) = (u,v)$，该变换能够“拉直”Rosenbrock 函数的山谷。变换如下：\n$$(u, v) = T(x,y) = (x, y - x^2)$$\n逆变换将新坐标映射回原始坐标，其形式为：\n$$(x, y) = T^{-1}(u,v) = (u, v + u^2)$$\n\n将逆变换代入 $f(x,y)$，得到新坐标下的目标函数 $g(u,v)$：\n$$g(u,v) = f(u, v+u^2) = (1 - u)^2 + a((v+u^2) - u^2)^2 = (1 - u)^2 + av^2$$\n这个变换后的函数 $g(u,v)$ 要简单得多。其等值线是以最小值点 $(u,v)=(1,0)$ 为中心的椭圆，其轴与坐标轴对齐。这种结构更适合梯度下降法。\n\n优化在 $(u,v)$ 空间中进行。更新规则是：\n$$(u_{k+1}, v_{k+1}) = (u_k, v_k) - \\alpha \\nabla g(u_k, v_k)$$\n$g(u,v)$ 的梯度计算如下：\n$$\\frac{\\partial g}{\\partial u} = \\frac{\\partial}{\\partial u} \\left[ (1 - u)^2 + av^2 \\right] = -2(1 - u)$$\n$$\\frac{\\partial g}{\\partial v} = \\frac{\\partial}{\\partial v} \\left[ (1 - u)^2 + av^2 \\right] = 2av$$\n变换空间中的梯度向量为：\n$$\\nabla g(u,v) = \\begin{pmatrix} -2(1 - u) \\\\ 2av \\end{pmatrix}$$\n算法首先将初始点 $(x_0, y_0)$ 变换为 $(u_0, v_0) = T(x_0, y_0)$。然后，它在 $(u,v)$ 空间中迭代地应用 GD 更新。对于停止准则，在每一步 $k$，当前点 $(u_k, v_k)$ 被映射回原始空间，即 $(x_k, y_k) = T^{-1}(u_k, v_k)$，以评估与真实最小值点 $(1,1)$ 的距离。梯度 $\\nabla g$ 中变量 $u$ 和 $v$ 的解耦（即 $\\frac{\\partial g}{\\partial u}$ 仅依赖于 $u$，而 $\\frac{\\partial g}{\\partial v}$ 仅依赖于 $v$）使得算法能够更有效地收敛到最小值。\n\n实现将包括两个函数，每种方法一个，对每个测试用例进行调用。通过在每次更新步骤后检查非有限数来处理稳健性。如果迭代发散，则认为运行未收敛，并返回最大迭代次数 $N_{\\max}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are permitted.\n\ndef run_gd_standard(a, x0, y0, alpha, eps, n_max):\n    \"\"\"\n    Performs standard Gradient Descent on the Rosenbrock function.\n    \"\"\"\n    x, y = float(x0), float(y0)\n    minimizer = np.array([1.0, 1.0])\n\n    for k in range(n_max):\n        current_pos = np.array([x, y])\n        # Check for convergence before the update\n        if np.linalg.norm(current_pos - minimizer) = eps:\n            return k\n\n        # Calculate gradient analytically\n        grad_x = -2.0 * (1.0 - x) - 4.0 * a * x * (y - x**2)\n        grad_y = 2.0 * a * (y - x**2)\n\n        # Update step\n        x -= alpha * grad_x\n        y -= alpha * grad_y\n\n        # Check for non-finite values\n        if not np.isfinite([x, y]).all():\n            return n_max\n    \n    # After n_max iterations, check one last time. If it just converged,\n    # it took n_max iterations. Otherwise, it didn't converge.\n    if np.linalg.norm(np.array([x, y]) - minimizer) = eps:\n        return n_max\n\n    return n_max\n\ndef run_gd_transformed(a, x0, y0, alpha, eps, n_max):\n    \"\"\"\n    Performs Gradient Descent on the transformed Rosenbrock function.\n    \"\"\"\n    # Initial transform to (u, v) space\n    u = float(x0)\n    v = float(y0) - float(x0)**2\n    \n    # Store original coordinates for distance check\n    x, y = float(x0), float(y0)\n    \n    minimizer = np.array([1.0, 1.0])\n\n    for k in range(n_max):\n        # Check for convergence in (x, y) space before the update\n        current_pos = np.array([x, y])\n        if np.linalg.norm(current_pos - minimizer) = eps:\n            return k\n\n        # Calculate gradient analytically in (u, v) space\n        grad_u = -2.0 * (1.0 - u)\n        grad_v = 2.0 * a * v\n\n        # Update step in (u, v) space\n        u -= alpha * grad_u\n        v -= alpha * grad_v\n        \n        # Check for non-finite values in the transformed space\n        if not np.isfinite([u, v]).all():\n            return n_max\n\n        # Map back to (x, y) space for the next distance check\n        x = u\n        y = v + u**2\n\n    # After n_max iterations, check one last time.\n    if np.linalg.norm(np.array([x, y]) - minimizer) = eps:\n        return n_max\n\n    return n_max\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the results.\n    \"\"\"\n    test_cases = [\n        (100, -1.2, 1.0, 0.001, 1e-6, 50000),\n        (1000, -1.2, 1.0, 0.0003, 1e-6, 50000),\n        (10, -1.2, 1.0, 0.002, 1e-6, 50000),\n        (100, 1.0, 1.0, 0.001, 1e-12, 1),\n        (100, -1.2, 1.0, 0.005, 1e-6, 50000),\n        (100, 2.0, 2.0, 0.001, 1e-6, 50000)\n    ]\n\n    results = []\n    for case in test_cases:\n        a, x0, y0, alpha, eps, n_max = case\n        \n        n_std = run_gd_standard(a, x0, y0, alpha, eps, n_max)\n        n_trans = run_gd_transformed(a, x0, y0, alpha, eps, n_max)\n        \n        results.append([n_std, n_trans])\n\n    # Format the final output string exactly as specified\n    formatted_results = [f\"[{r[0]},{r[1]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "许多现实世界中的优化问题都带有约束，例如，机器学习模型中的概率分布参数其总和必须为1。本练习  将引导你探索在概率单纯形上进行约束优化的两种核心策略。你将亲手实现投影梯度法，即在每次迭代后将参数投影回可行域，并与另一种更优雅的方法——使用 softmax 函数进行重参数化——进行比较，从而深入理解处理约束梯度的不同思路。",
            "id": "3139517",
            "problem": "您将实现并比较两种梯度下降的变体，用于在概率向量上最小化光滑目标函数。概率向量是一个向量 $\\mathbf{p} \\in \\mathbb{R}^n$，满足对所有 $i$ 都有 $p_i \\ge 0$ 且 $\\sum_{i=1}^n p_i = 1$。可行集是概率单纯形 $\\Delta^n := \\{\\mathbf{p} \\in \\mathbb{R}^n \\mid p_i \\ge 0, \\sum_i p_i = 1\\}$。您必须仅使用第一性原理以及梯度和约束优化的核心定义，而不使用此处提供的任何快捷公式。您的程序必须是一个独立的脚本，不接受任何输入，并按下方指定格式精确打印一行输出。\n\n基本依据和目标。从以下定义出发：对于一个可微目标函数 $f(\\mathbf{p})$，在无约束空间中的基本梯度下降步骤通过 $\\mathbf{z}^{(t+1)} = \\mathbf{z}^{(t)} - \\eta_t \\nabla f(\\mathbf{z}^{(t)})$ 来更新 $\\mathbf{z}$，其中 $\\eta_t$ 是一个正步长，$\\nabla f$ 是梯度。当存在约束时，一种有原则的方法是将其重新参数化为一个无约束变量并使用链式法则，另一种方法是在原始变量上进行下降，然后根据欧几里得范数投影回可行集。您必须从这些基本原理出发推导出必要的表达式。\n\n需要实现和比较的算法：\n- 方法 A（单纯形上的投影梯度）：直接对 $\\mathbf{p} \\in \\Delta^n$ 进行操作。在每次迭代中，对 $\\mathbf{p}$ 进行一次无约束梯度下降步骤，然后计算其到 $\\Delta^n$ 上的欧几里得投影。您必须通过使用一阶最优性条件求解相应的约束最小二乘问题来推导并实现到 $\\Delta^n$ 上的欧几里得投影；不要使用任何预先封装好的投影例程。\n- 方法 B（Softmax 重新参数化）：引入一个无约束参数 $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ 并定义 $\\mathbf{p} = \\mathrm{softmax}(\\boldsymbol{\\theta})$，其中 softmax 函数由 $p_i = \\exp(\\theta_i) / \\sum_{j=1}^n \\exp(\\theta_j)$ 给出。对复合目标函数 $f(\\mathrm{softmax}(\\boldsymbol{\\theta}))$ 在 $\\boldsymbol{\\theta}$ 上应用梯度下降。您必须从链式法则和 softmax 函数的雅可比矩阵推导出 $\\nabla_{\\boldsymbol{\\theta}} f(\\mathrm{softmax}(\\boldsymbol{\\theta}))$ 的显式表达式；不要假定该表达式是已知的。\n\n步长策略与初始化。对于两种方法，均使用相同的递减步长策略 $\\eta_t = \\eta_0 / \\sqrt{t+1}$，其中 $\\eta_0 = 0.3$，以及固定的迭代预算 $T = 4000$。对于投影方法，将 $\\mathbf{p}^{(0)}$ 初始化为均匀向量，即 $p_i^{(0)} = 1/n$。将 $\\boldsymbol{\\theta}^{(0)}$ 初始化为零向量，这样 $\\mathrm{softmax}(\\boldsymbol{\\theta}^{(0)})$ 也是均匀的。在整个过程中使用 $n = 4$。\n\n目标函数与梯度。您必须在 $\\Delta^4$ 上实现以下目标函数 $f(\\mathbf{p})$，并从定义出发实现它们的梯度 $\\nabla_{\\mathbf{p}} f(\\mathbf{p})$。在所有情况下，$\\|\\cdot\\|_2$ 表示欧几里得范数。所有计算都是无量纲的，不涉及物理单位。\n- 情况1（朝向目标向量的二次型）：$f(\\mathbf{p}) = \\tfrac{1}{2} \\|\\mathbf{p} - \\mathbf{q}\\|_2^2$，其中 $\\mathbf{q} = [0.7, 0.1, 0.1, 0.1]$。\n- 情况2（线性目标函数）：$f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$，其中 $\\mathbf{c} = [0.5, -1.0, 0.2, 0.2]$。\n- 情况3（二次型数据拟合）：$f(\\mathbf{p}) = \\tfrac{1}{2} \\|A \\mathbf{p} - \\mathbf{b}\\|_2^2$，其中\n  $$\n  A = \\begin{bmatrix}\n  2  -1  0  0 \\\\\n  -1  2  -1  0 \\\\\n  0  -1  2  -1 \\\\\n  0  0  -1  2\n  \\end{bmatrix}, \\quad \\mathbf{b} = [1.0, 0.0, 0.0, 0.0].\n  $$\n- 情况4（朝向均匀分布的二次型）：$f(\\mathbf{p}) = \\tfrac{1}{2} \\|\\mathbf{p} - \\mathbf{u}\\|_2^2$，其中 $\\mathbf{u} = [0.25, 0.25, 0.25, 0.25]$。\n\n对于每种情况，从第一性原理实现 $\\nabla_{\\mathbf{p}} f(\\mathbf{p})$。对于线性目标函数，回想一下梯度是恒定的。对于二次型目标函数，使用标准法则，即对于 $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{M}\\mathbf{p} - \\mathbf{r}\\|_2^2$，有 $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{M}^\\top (\\mathbf{M}\\mathbf{p} - \\mathbf{r})$，这可以从链式法则和微分的线性性质推导得出。\n\n比较指标。对于每种情况，在使用指定的策略和初始化运行两种方法恰好 $T$ 次迭代后，计算所得概率向量 $\\mathbf{p}_{\\mathrm{softmax}}$ 和 $\\mathbf{p}_{\\mathrm{proj}}$ 之间的 $\\ell_1$ 距离，即 $\\|\\mathbf{p}_{\\mathrm{softmax}} - \\mathbf{p}_{\\mathrm{proj}}\\|_1 = \\sum_{i=1}^4 |p_{\\mathrm{softmax},i} - p_{\\mathrm{proj},i}|$。\n\n测试套件。使用上述四种情况作为测试套件。这些情况涵盖：一个类内部目标（情况1）、一个边界最优解（情况2）、一个带耦合的结构化二次型（情况3），以及一个已经是最后解的起始点（情况4）。\n\n要求的最终输出格式。您的程序应生成单行输出，其中包含一个由四个浮点数组成的列表，每个浮点数等于相应情况下的 $\\ell_1$ 距离，四舍五入到六位小数，形式为用方括号括起来的逗号分隔列表，例如，“[0.000123,0.045678,0.001000,0.000000]”。",
            "solution": "我们提出基于梯度、链式法则和通过投影进行约束优化的核心定义，并以此为基础进行有原则的推导和算法设计。\n\n可行集与目标函数。可行集是概率单纯形 $\\Delta^n = \\{\\mathbf{p} \\in \\mathbb{R}^n \\mid p_i \\ge 0,\\ \\sum_{i=1}^n p_i = 1\\}$。我们考虑 $n=4$ 和指定的四个目标函数。对于线性和二次函数，关于 $\\mathbf{p}$ 的梯度遵循基本法则：\n- 对于 $f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$，梯度为 $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{c}$，因为微分是线性的，并且 $\\partial (\\mathbf{c}^\\top \\mathbf{p}) / \\partial \\mathbf{p} = \\mathbf{c}$。\n- 对于 $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{M} \\mathbf{p} - \\mathbf{r}\\|_2^2$，定义 $\\mathbf{y}(\\mathbf{p}) = \\mathbf{M}\\mathbf{p} - \\mathbf{r}$ 和 $g(\\mathbf{y}) = \\tfrac{1}{2} \\|\\mathbf{y}\\|_2^2$。然后根据链式法则，\n$$\n\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{M}^\\top \\nabla_{\\mathbf{y}} g(\\mathbf{y}(\\mathbf{p})) = \\mathbf{M}^\\top \\mathbf{y}(\\mathbf{p}) = \\mathbf{M}^\\top (\\mathbf{M}\\mathbf{p} - \\mathbf{r}).\n$$\n取 $\\mathbf{M} = \\mathbf{I}$ 和 $\\mathbf{r} = \\mathbf{q}$ 可得 $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{p} - \\mathbf{q}$。取 $\\mathbf{M} = A$ 和 $\\mathbf{r} = \\mathbf{b}$ 可得 $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = A^\\top( A\\mathbf{p} - \\mathbf{b})$。\n\n方法 A（单纯形上的投影梯度）。从无约束梯度下降更新开始\n$$\n\\tilde{\\mathbf{p}}^{(t+1)} = \\mathbf{p}^{(t)} - \\eta_t \\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)}),\n$$\n然后通过计算到 $\\Delta^n$ 上的欧几里得投影来强制满足可行性：\n$$\n\\mathbf{p}^{(t+1)} = \\operatorname{Proj}_{\\Delta^n}(\\tilde{\\mathbf{p}}^{(t+1)}) := \\arg\\min_{\\mathbf{x} \\in \\Delta^n} \\|\\mathbf{x} - \\tilde{\\mathbf{p}}^{(t+1)}\\|_2^2.\n$$\n我们使用拉格朗日乘子法和最优性条件来推导投影。考虑优化问题\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\tfrac{1}{2}\\|\\mathbf{x} - \\mathbf{v}\\|_2^2 \\quad \\text{subject to} \\quad \\sum_{i=1}^n x_i = 1,\\ \\ x_i \\ge 0 \\ \\forall i,\n$$\n其中给定 $\\mathbf{v} \\in \\mathbb{R}^n$。构造拉格朗日函数\n$$\n\\mathcal{L}(\\mathbf{x}, \\tau, \\boldsymbol{\\mu}) = \\tfrac{1}{2}\\|\\mathbf{x} - \\mathbf{v}\\|_2^2 + \\tau \\left(\\sum_{i=1}^n x_i - 1\\right) - \\sum_{i=1}^n \\mu_i x_i,\n$$\n其中 $\\tau \\in \\mathbb{R}$ 对应等式约束，$\\mu_i \\ge 0$ 对应不等式约束。Karush-Kuhn-Tucker (KKT) 条件（稳定性、原始可行性、对偶可行性、互补松弛性）是：\n- 稳定性：\n$$\n\\nabla_{\\mathbf{x}} \\mathcal{L} = \\mathbf{x} - \\mathbf{v} + \\tau \\mathbf{1} - \\boldsymbol{\\mu} = \\mathbf{0}.\n$$\n- 原始可行性：$\\sum_i x_i = 1$，对所有 $i$ 都有 $x_i \\ge 0$。\n- 对偶可行性：对所有 $i$ 都有 $\\mu_i \\ge 0$。\n- 互补松弛性：对所有 $i$ 都有 $\\mu_i x_i = 0$。\n\n从稳定性条件可知，对每个 $i$：\n$$\nx_i = v_i - \\tau + \\mu_i.\n$$\n如果 $x_i  0$，则互补松弛性意味着 $\\mu_i = 0$，因此 $x_i = v_i - \\tau$。如果 $x_i = 0$，则 $\\mu_i \\ge 0$ 且 $0 = v_i - \\tau + \\mu_i$ 意味着 $v_i - \\tau \\le 0$。因此解具有阈值形式\n$$\nx_i^\\star = \\max\\{v_i - \\tau, 0\\}\n$$\n其中标量 $\\tau$ 的选择需满足 $\\sum_{i=1}^n x_i^\\star = 1$。标量 $\\tau$ 是唯一确定的，因为映射 $\\tau \\mapsto \\sum_i \\max\\{v_i - \\tau, 0\\}$ 是连续且严格递减的。一种计算 $\\tau$ 的高效方法是将 $\\mathbf{v}$ 的条目按降序排序，对于排序后的向量 $\\mathbf{u}$，找到最大的索引 $\\rho$，使得 $u_\\rho - \\frac{1}{\\rho+1}\\left(\\sum_{j=0}^{\\rho} u_j - 1\\right)  0$，然后设置\n$$\n\\tau = \\frac{1}{\\rho+1}\\left(\\sum_{j=0}^{\\rho} u_j - 1\\right),\n$$\n并输出逐元素应用的 $\\mathbf{x}^\\star = \\max\\{\\mathbf{v} - \\tau, 0\\}$。这就实现了到 $\\Delta^n$ 上的欧几里得投影。\n\n方法 B（Softmax 重新参数化）。定义无约束参数 $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ 并设置\n$$\np_i(\\boldsymbol{\\theta}) = \\frac{\\exp(\\theta_i)}{\\sum_{j=1}^n \\exp(\\theta_j)}.\n$$\n我们在复合函数 $F(\\boldsymbol{\\theta}) = f(\\mathbf{p}(\\boldsymbol{\\theta}))$ 上执行梯度下降。根据链式法则，\n$$\n\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta}) = J(\\boldsymbol{\\theta})^\\top \\nabla_{\\mathbf{p}} f(\\mathbf{p}(\\boldsymbol{\\theta})),\n$$\n其中 $J(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{n \\times n}$ 是 softmax 的雅可比矩阵。softmax 的导数是一个标准结果，通过对 $p_i = \\exp(\\theta_i)/\\sum_k \\exp(\\theta_k)$ 求导得到。对于每个 $i,k$，\n$$\n\\frac{\\partial p_i}{\\partial \\theta_k} = p_i(\\delta_{ik} - p_k),\n$$\n其中 $\\delta_{ik}$ 是克罗内克 $\\delta$。因此，如果我们记 $\\mathbf{g} = \\nabla_{\\mathbf{p}} f(\\mathbf{p}(\\boldsymbol{\\theta}))$，$\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta})$ 的第 $k$ 个分量是\n$$\n\\left[\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta})\\right]_k\n= \\sum_{i=1}^n g_i \\frac{\\partial p_i}{\\partial \\theta_k}\n= \\sum_{i=1}^n g_i p_i(\\delta_{ik} - p_k)\n= p_k \\left(g_k - \\sum_{i=1}^n g_i p_i\\right).\n$$\n用向量形式表示，\n$$\n\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta}) = \\mathbf{p} \\odot \\left(\\mathbf{g} - (\\mathbf{g}^\\top \\mathbf{p}) \\mathbf{1}\\right),\n$$\n其中 $\\odot$ 表示逐元素乘法。梯度下降更新变为\n$$\n\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta_t \\left[ \\mathbf{p}^{(t)} \\odot \\left(\\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)}) - \\left(\\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)})\\right)^\\top \\mathbf{p}^{(t)} \\cdot \\mathbf{1} \\right) \\right],\n$$\n其中 $\\mathbf{p}^{(t)} = \\mathrm{softmax}(\\boldsymbol{\\theta}^{(t)})$。通过构造，此更新确保了相应的 $\\mathbf{p}^{(t)}$ 保持在 $\\Delta^n$ 中。\n\n步长与初始化。我们使用递减策略 $\\eta_t = \\eta_0/\\sqrt{t+1}$，其中 $\\eta_0 = 0.3$，这符合为促进稳定性而减小步长的常用启发式方法。对于两种方法，初始化都是均匀分布：方法A直接设置 $p_i^{(0)} = 1/n$，方法B设置 $\\boldsymbol{\\theta}^{(0)} = \\mathbf{0}$，使得softmax产生均匀分布。\n\n测试用例和预期。我们实现四个测试用例：\n- 情况1：$f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{p} - \\mathbf{q}\\|_2^2$ 其中 $\\mathbf{q} = [0.7, 0.1, 0.1, 0.1]$。无约束最小化子是 $\\mathbf{q}$，它位于单纯形内，因此约束最小化子也是 $\\mathbf{q}$。两种方法都应该收敛到接近 $\\mathbf{q}$。\n- 情况2：$f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$ 其中 $\\mathbf{c} = [0.5, -1.0, 0.2, 0.2]$。约束最小化子是对应于 $\\mathbf{c}$ 最小分量的顶点，即第二个坐标；两种方法都应该逼近一个集中在该坐标上的分布。\n- 情况3：$f(\\mathbf{p}) = \\tfrac{1}{2}\\|A\\mathbf{p} - \\mathbf{b}\\|_2^2$ 其中 $A$ 为三对角矩阵，$\\mathbf{b}$ 已给出。最优解可能在内部或边界上；两种方法都应逼近一个相似的解。\n- 情况4：$f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{p} - \\mathbf{u}\\|_2^2$ 其中 $\\mathbf{u}$ 是均匀的。均匀初始化已经是最后解，因此两种方法都应该保持接近均匀分布。\n\n比较指标。在每种情况下迭代 $T = 4000$ 次后，我们计算 $\\ell_1$ 距离 $\\|\\mathbf{p}_{\\mathrm{softmax}} - \\mathbf{p}_{\\mathrm{proj}}\\|_1 = \\sum_{i=1}^4 |p_{\\mathrm{softmax},i} - p_{\\mathrm{proj},i}|$。该指标是非负的，当且仅当两个概率向量匹配时为零，并且对各坐标间的差异敏感。\n\n程序设计。程序将：\n- 使用上述源自KKT的阈值法实现到 $\\Delta^n$ 的投影。\n- 使用共同的步长策略和初始化实现方法A和方法B。\n- 为四种情况定义目标函数和梯度函数。\n- 在每种情况下运行两种方法 $T = 4000$ 次迭代，并收集 $\\ell_1$ 距离。\n- 以指定格式打印一行包含四个距离的列表，四舍五入到六位小数。\n\n这种方法整合了微分的基本规则、链式法则和 Karush-Kuhn-Tucker 条件，以推导出用于概率单纯形上的约束梯度下降和重新参数化梯度下降的可实现算法。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax(theta: np.ndarray) -> np.ndarray:\n    # Numerically stable softmax\n    z = theta - np.max(theta)\n    e = np.exp(z)\n    return e / np.sum(e)\n\ndef grad_softmax_chain(p: np.ndarray, grad_p: np.ndarray) -> np.ndarray:\n    # Computes grad wrt theta: p * (grad_p - (p.T @ grad_p))\n    return p * (grad_p - np.dot(p, grad_p))\n\ndef project_to_simplex(v: np.ndarray) -> np.ndarray:\n    \"\"\"Projects a vector v onto the probability simplex.\"\"\"\n    n_features = v.shape[0]\n    # Sort v in descending order\n    u = np.sort(v)[::-1]\n    # Cumulative sum of sorted vector\n    cssv = np.cumsum(u)\n    # Find the largest index rho such that u[rho] > (cssv[rho] - 1) / (rho + 1)\n    # This is equivalent to finding the number of positive elements in the projected vector.\n    rho_candidates = np.where(u > (cssv - 1) / np.arange(1, n_features + 1))[0]\n    if len(rho_candidates) == 0:\n        # This case should ideally not happen for this problem's setup,\n        # but as a fallback, handle it gracefully.\n        return np.ones(n_features) / n_features\n    rho = rho_candidates[-1]\n    # The threshold lambda is (cssv[rho] - 1) / (rho + 1)\n    theta = (cssv[rho] - 1) / (rho + 1)\n    # Project by thresholding\n    w = np.maximum(v - theta, 0)\n    return w\n\ndef solve():\n    n = 4\n    T = 4000\n    eta0 = 0.3\n\n    # --- Test Cases Setup ---\n    q1 = np.array([0.7, 0.1, 0.1, 0.1])\n    c2 = np.array([0.5, -1.0, 0.2, 0.2])\n    A3 = np.array([[2, -1, 0, 0], [-1, 2, -1, 0], [0, -1, 2, -1], [0, 0, -1, 2]])\n    b3 = np.array([1.0, 0.0, 0.0, 0.0])\n    u4 = np.array([0.25, 0.25, 0.25, 0.25])\n    \n    # Gradients wrt p\n    grad_f1 = lambda p: p - q1\n    grad_f2 = lambda p: c2\n    grad_f3 = lambda p: A3.T @ (A3 @ p - b3)\n    grad_f4 = lambda p: p - u4\n    \n    test_grads = [grad_f1, grad_f2, grad_f3, grad_f4]\n    \n    l1_distances = []\n\n    for grad_f in test_grads:\n        # --- Method A: Projected Gradient Descent ---\n        p_proj = np.ones(n) / n\n        for t in range(T):\n            eta_t = eta0 / np.sqrt(t + 1)\n            grad_p = grad_f(p_proj)\n            p_unconstrained = p_proj - eta_t * grad_p\n            p_proj = project_to_simplex(p_unconstrained)\n\n        # --- Method B: Softmax Reparameterization ---\n        theta_softmax = np.zeros(n)\n        for t in range(T):\n            eta_t = eta0 / np.sqrt(t + 1)\n            p_softmax_curr = softmax(theta_softmax)\n            grad_p = grad_f(p_softmax_curr)\n            grad_theta = grad_softmax_chain(p_softmax_curr, grad_p)\n            theta_softmax -= eta_t * grad_theta\n        \n        p_softmax_final = softmax(theta_softmax)\n        \n        # --- Compare and Store ---\n        l1_dist = np.sum(np.abs(p_softmax_final - p_proj))\n        l1_distances.append(round(l1_dist, 6))\n\n    print(f\"[{','.join(map(str, l1_distances))}]\")\n\nsolve()\n```"
        }
    ]
}