{
    "hands_on_practices": [
        {
            "introduction": "梯度下降法是优化的基石，但其收敛速度对目标函数的几何形状高度敏感。本练习将通过一个精心设计的二次函数，让你亲手构建一个“病态”问题，直观地观察梯度下降为何会产生效率低下的“之”字形路径，并探索如何通过坐标变换来加速收敛 。这个实践旨在揭示预处理技术背后的核心思想，即通过对齐梯度方向与等高线主轴来简化优化路径。",
            "id": "3139501",
            "problem": "您将实现、分析并比较梯度下降法的两种变体在一个二维凸二次型上的表现。目标是构建一个数学上可控的案例，其中基本方法由于各向异性（病态条件）而表现出锯齿形轨迹，然后演示基于旋转的变量变换结合坐标轴对齐的按坐标步长如何将更新方向与等高线的主轴对齐并加速收敛。\n\n从数值优化和线性代数中的以下核心定义和经过充分检验的事实开始：\n- 一个具有对称正定（Positive Definite (PD)）海森矩阵的二阶连续可微函数是强凸的。特别地，对于对称正定矩阵 $$\\mathbf{Q} \\in \\mathbb{R}^{2\\times 2}$$，二次函数 $$f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\top} \\mathbf{Q}\\,\\mathbf{x}$$ 在 $$\\mathbf{x}^{\\star} = \\mathbf{0}$$ 处有唯一最小值点，其梯度为 $$\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\,\\mathbf{x}$$。\n- 对于最小化 $$f$$，使用固定步长 $$\\alpha \\in (0, 2/L)$$ 的梯度下降更新是 $$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\nabla f(\\mathbf{x}_{k})$$。对于上述二次函数，这变为 $$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k}$$。\n- 对于一个旋转角度为 $$\\theta$$（以弧度为单位）的正交旋转矩阵 $$\\mathbf{R}(\\theta) = \\begin{bmatrix}\\cos\\theta  -\\sin\\theta\\\\ \\sin\\theta  \\cos\\theta\\end{bmatrix}$$，变换 $$\\mathbf{y} = \\mathbf{R}^{\\top}\\mathbf{x}$$ 改变坐标而不改变欧几里得范数，即 $$\\|\\mathbf{y}\\|_{2} = \\|\\mathbf{x}\\|_{2}$$。\n\n您将考虑一类凸二次函数，其海森矩阵是通过旋转一个对角特征值矩阵来构造的。设 $$\\lambda_{1}  \\lambda_{2}  0$$ 并设置 $$\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2}), \\quad \\mathbf{Q} = \\mathbf{R}(\\theta)\\,\\mathbf{\\Lambda}\\,\\mathbf{R}(\\theta)^{\\top}$$。大的比率 $$\\lambda_{1}/\\lambda_{2}$$ 会引起各向异性（病态条件），这导致基本梯度下降法在 $$\\theta \\neq 0$$ 时在原始坐标系中走出一条锯齿形路径。\n\n您的任务是：\n- 在原始坐标中实现使用固定步长 $$\\alpha = 1/\\lambda_{\\max} = 1/\\lambda_{1}$$ 的基准梯度下降，即 $$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k}$$。使用初始条件 $$\\mathbf{x}_{0} = (1,\\,1)^{\\top}$$。当欧几里得范数满足 $$\\|\\mathbf{x}_{k}\\|_{2} \\le \\varepsilon$$ 时终止。\n- 实现一个旋转坐标、坐标轴对齐、按坐标步长的变体。定义旋转变量 $$\\mathbf{y} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}$$ 并在旋转坐标中使用按轴学习率 $$\\alpha_{1} = c/\\lambda_{1}, \\ \\alpha_{2} = c/\\lambda_{2}$$（其中 $$c \\in (0,2)$$ 是一个共享标量）进行更新，即 $$\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - \\mathrm{diag}(\\alpha_{1}, \\alpha_{2})\\,\\nabla_{\\mathbf{y}} f(\\mathbf{y}_{k}), \\quad \\nabla_{\\mathbf{y}} f(\\mathbf{y}) = \\mathbf{\\Lambda}\\,\\mathbf{y}$$。使用在旋转坐标中表示的相同初始条件 $$\\mathbf{y}_{0} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{0}$$，并在 $$\\|\\mathbf{y}_{k}\\|_{2} \\le \\varepsilon$$ 时终止。请注意，此选择将更新与主轴对齐，并解耦了每个轴上的动态。您必须使用与基准方法相同的 $$\\varepsilon$$。\n- 对于下面的每个测试用例，计算基准方法所需的迭代次数 $$N_{\\mathrm{plain}}$$ 和旋转坐标、按轴变体所需的迭代次数 $$N_{\\mathrm{rot}}$$，并报告定义为浮点数 $$S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}$$ 的加速比。\n\n角度单位说明：所有角度 $$\\theta$$ 均以弧度为单位提供，并且必须以弧度为单位进行解释。\n\n数值终止和安全性：如果任一方法在 $$10^{6}$$ 次迭代内未满足停止准则，则停止并使用已达到的计数值；但是，所有提供的测试用例都在此限制之内。\n\n测试套件（理想情况，显著各向异性；中等各向异性；近各向同性边缘情况）：\n- 测试 $$1$$: $$(\\lambda_{1}, \\lambda_{2}, \\theta, \\varepsilon, c) = (200.0,\\,1.0,\\,\\pi/4,\\,10^{-8},\\,1.6)$$。\n- 测试 $$2$$: $$(\\lambda_{1}, \\lambda_{2}, \\theta, \\varepsilon, c) = (10.0,\\,1.0,\\,0.4,\\,10^{-8},\\,1.6)$$。\n- 测试 $$3$$: $$(\\lambda_{1}, \\lambda_{2}, \\theta, \\varepsilon, c) = (1.0,\\,1.0,\\,0.7,\\,10^{-8},\\,1.6)$$。\n\n您的程序必须：\n- 根据指定的 $$(\\lambda_{1}, \\lambda_{2}, \\theta)$$ 构造 $$\\mathbf{Q}$$。\n- 从 $$\\mathbf{x}_{0} = (1,\\,1)^{\\top}$$ 开始，使用相应的步长规则和停止条件运行两种方法。\n- 对于每个测试，计算 $$S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}$$ 并四舍五入到 $$3$$ 位小数。\n\n最终输出格式：您的程序应生成一行输出，其中包含三个四舍五入后的结果，形式为逗号分隔的列表并用方括号括起，例如 $$[S_{1},S_{2},S_{3}]$$，其中每个 $$S_{i}$$ 都是一个四舍五入到 $$3$$ 位小数的浮点数。不应打印其他任何文本。",
            "solution": "用户提供的问题陈述是有效的。它在科学上基于数值优化和线性代数的原理，定义了所有必要的参数和条件，问题阐述清晰，并且其表述是客观的。任务是分析两种梯度下降变体在二维凸二次函数上的性能，这是计算科学中一个经典且富有启发性的问题。\n\n解决方案首先实现基准梯度下降法并分析其行为，然后实现坐标旋转变体并分析其在面对各向异性时改进的收敛特性。最后，对每个测试用例，计算两种方法的迭代次数以确定加速比。\n\n该问题的目标函数是凸二次型 $f(\\mathbf{x}) = \\frac{1}{2}\\,\\mathbf{x}^{\\top} \\mathbf{Q}\\,\\mathbf{x}$。唯一的最小值点在 $\\mathbf{x}^{\\star} = \\mathbf{0}$，梯度为 $\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\,\\mathbf{x}$。海森矩阵 $\\mathbf{Q}$ 的构造方式为 $\\mathbf{Q} = \\mathbf{R}(\\theta)\\,\\mathbf{\\Lambda}\\,\\mathbf{R}(\\theta)^{\\top}$，其中 $\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2})$，特征值为 $\\lambda_{1}  \\lambda_{2}  0$，而 $\\mathbf{R}(\\theta)$ 是对应角度 $\\theta$ 的标准 $2\\times 2$ 旋转矩阵。这种构造创建了一个函数，其等值线是主轴与 $\\mathbf{R}(\\theta)$ 的列对齐的椭圆。比率 $\\kappa = \\lambda_{1}/\\lambda_{2}$，即条件数，衡量了这些椭圆的各向异性程度。大的 $\\kappa$ 意味着一个病态条件问题。\n\n### 方法1：基准梯度下降\n\n标准梯度下降算法通过在负梯度方向上迈出一步来更新当前迭代值 $\\mathbf{x}_{k}$：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\nabla f(\\mathbf{x}_{k})\n$$\n其中 $\\alpha  0$ 是步长或学习率。对于给定的二次函数，这变为：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k} = (\\mathbf{I} - \\alpha\\,\\mathbf{Q})\\,\\mathbf{x}_{k}\n$$\n问题指定了固定步长 $\\alpha = 1/\\lambda_{\\max}(\\mathbf{Q}) = 1/\\lambda_{1}$。此迭代过程的收敛性由迭代矩阵 $\\mathbf{M} = \\mathbf{I} - \\alpha\\,\\mathbf{Q}$ 的谱半径决定。$\\mathbf{M}$ 的特征值为 $1-\\alpha\\lambda_{i}$。当 $\\alpha=1/\\lambda_1$ 时，特征值为 $1 - \\lambda_1/\\lambda_1 = 0$ 和 $1 - \\lambda_2/\\lambda_1$。因此，谱半径为 $\\rho(\\mathbf{M}) = |1 - \\lambda_2/\\lambda_1| = 1 - 1/\\kappa$。当条件数 $\\kappa$ 很大时，$\\rho(\\mathbf{M})$ 非常接近 $1$，导致收敛缓慢。算法从 $\\mathbf{x}_{0} = (1,\\,1)^{\\top}$ 开始，并在欧几里得范数 $\\|\\mathbf{x}_{k}\\|_{2} \\le \\varepsilon$ 时终止，得到迭代次数 $N_{\\mathrm{plain}}$。\n\n### 方法2：旋转坐标梯度下降\n\n该方法首先执行变量变换，以使坐标系与函数等值线的主轴对齐。新的坐标向量定义为 $\\mathbf{y} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}$。逆变换为 $\\mathbf{x} = \\mathbf{R}(\\theta)\\mathbf{y}$。将此代入目标函数可得：\n$$\nf(\\mathbf{y}) = \\frac{1}{2}\\,\\left(\\mathbf{R}(\\theta)\\mathbf{y}\\right)^{\\top} \\left(\\mathbf{R}(\\theta)\\mathbf{\\Lambda}\\mathbf{R}(\\theta)^{\\top}\\right) \\left(\\mathbf{R}(\\theta)\\mathbf{y}\\right)\n$$\n由于 $\\mathbf{R}(\\theta)$ 是正交的，$\\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta) = \\mathbf{I}$。表达式简化为：\n$$\nf(\\mathbf{y}) = \\frac{1}{2}\\,\\mathbf{y}^{\\top} \\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta)\\mathbf{\\Lambda}\\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta)\\mathbf{y} = \\frac{1}{2}\\,\\mathbf{y}^{\\top} \\mathbf{\\Lambda}\\,\\mathbf{y}\n$$\n在这个新的坐标系中，海森矩阵是对角矩阵 $\\mathbf{\\Lambda}$，这意味着问题解耦为两个独立的一维优化问题。在 $\\mathbf{y}$-坐标系中的梯度为 $\\nabla_{\\mathbf{y}} f(\\mathbf{y}) = \\mathbf{\\Lambda}\\mathbf{y}$。\n\n更新规则使用按坐标的步长，$\\alpha_{1} = c/\\lambda_{1}$ 和 $\\alpha_{2} = c/\\lambda_{2}$，其中 $c \\in (0, 2)$ 是一个共享标量。更新过程为：\n$$\n\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - \\mathrm{diag}(\\alpha_{1}, \\alpha_{2})\\,\\nabla_{\\mathbf{y}} f(\\mathbf{y}_{k}) = \\mathbf{y}_{k} - \\mathrm{diag}(c/\\lambda_{1}, c/\\lambda_{2})\\,(\\mathbf{\\Lambda}\\mathbf{y}_{k})\n$$\n因为 $\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_1, \\lambda_2)$，对角矩阵的乘积简化为：\n$$\n\\mathrm{diag}(c/\\lambda_{1}, c/\\lambda_{2})\\,\\mathrm{diag}(\\lambda_{1}, \\lambda_{2}) = \\mathrm{diag}(c, c) = c\\mathbf{I}\n$$\n因此，更新规则变得异常简单：\n$$\n\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - c\\mathbf{I}\\mathbf{y}_{k} = (1-c)\\mathbf{y}_{k}\n$$\n这是一个简单的几何收缩。收敛速率为 $|1-c|$，它与特征值 $\\lambda_1, \\lambda_2$ 无关，因此也与原始问题的条件数无关。算法从变换后的初始条件 $\\mathbf{y}_{0} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{0}$ 开始，并在 $\\|\\mathbf{y}_{k}\\|_{2} \\le \\varepsilon$ 时终止，得到迭代次数 $N_{\\mathrm{rot}}$。请注意，$\\|\\mathbf{y}_{k}\\|_{2} = \\|\\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{k}\\|_{2} = \\|\\mathbf{x}_{k}\\|_{2}$，因为旋转保持欧几里得范数不变。\n\n### 加速比计算\n\n旋转坐标方法相对于基准方法的性能提升通过加速比 $S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}$ 来量化。对于每个测试用例，我们实现这两种算法，计算它们的迭代次数，并计算这个比率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares two gradient descent methods on a 2D convex quadratic,\n    calculates the speedup factor for three test cases.\n    \"\"\"\n    test_cases = [\n        # (lambda_1, lambda_2, theta, epsilon, c)\n        (200.0, 1.0, np.pi/4, 1e-8, 1.6),\n        (10.0, 1.0, 0.4, 1e-8, 1.6),\n        (1.0, 1.0, 0.7, 1e-8, 1.6),\n    ]\n\n    results = []\n    max_iterations = 1_000_000\n\n    for case in test_cases:\n        lambda_1, lambda_2, theta, epsilon, c = case\n\n        # Common initial condition\n        x0 = np.array([1.0, 1.0])\n\n        # --- Task 1: Baseline Gradient Descent ---\n        \n        # Construct the Hessian matrix Q\n        cos_th, sin_th = np.cos(theta), np.sin(theta)\n        R = np.array([[cos_th, -sin_th], [sin_th, cos_th]])\n        Lambda = np.diag([lambda_1, lambda_2])\n        Q = R @ Lambda @ R.T\n\n        # Set up and run the iteration\n        alpha = 1.0 / lambda_1\n        x_k = np.copy(x0)\n        N_plain = 0\n        while np.linalg.norm(x_k) > epsilon and N_plain  max_iterations:\n            gradient = Q @ x_k\n            x_k = x_k - alpha * gradient\n            N_plain += 1\n\n        # --- Task 2: Rotated-Coordinate Gradient Descent ---\n\n        # Initial condition in rotated coordinates\n        y0 = R.T @ x0\n        \n        # Rate of contraction is (1-c)\n        contraction_factor = 1.0 - c\n        \n        y_k = np.copy(y0)\n        N_rot = 0\n        while np.linalg.norm(y_k) > epsilon and N_rot  max_iterations:\n            # The update rule y_{k+1} = (1-c) * y_k is computationally simpler\n            # than re-calculating the gradient at each step, but we implement\n            # the specified update rule for formal correctness.\n            # grad_y = Lambda @ y_k\n            # step_sizes = np.diag([c / lambda_1, c / lambda_2])\n            # y_k = y_k - step_sizes @ grad_y\n            # The above is equivalent to:\n            y_k = contraction_factor * y_k\n            N_rot += 1\n\n        # --- Task 3: Compute Speedup Factor ---\n        if N_rot > 0:\n            speedup = N_plain / N_rot\n        else:\n            # Handle the case where N_rot could be zero, though not expected\n            # for the given test cases and epsilon > 0.\n            speedup = float('inf') if N_plain > 0 else 1.0\n\n        results.append(round(speedup, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界中的许多优化问题并非无约束的，例如，当参数必须代表概率分布时，它们需要满足非负且和为一的条件。本练习将带你探索两种处理此类约束的强大技术：投影梯度法和通过Softmax函数进行的重参数化 。通过在概率单纯形上最小化目标函数，你将学习如何将约束优化问题转化为可应用梯度下降的形式，并比较这两种主流方法的轨迹和结果。",
            "id": "3139517",
            "problem": "您将实现并比较两种梯度下降的变体，用于在概率向量上最小化平滑目标函数。概率向量是一个向量 $\\mathbf{p} \\in \\mathbb{R}^n$，满足对所有 $i$ 都有 $p_i \\ge 0$ 且 $\\sum_{i=1}^n p_i = 1$。可行集是概率单纯形 $\\Delta^n := \\{\\mathbf{p} \\in \\mathbb{R}^n \\mid p_i \\ge 0, \\sum_i p_i = 1\\}$。您必须仅使用梯度和约束优化的第一性原理与核心定义，而不使用此处提供的任何快捷公式。您的程序必须是一个单一的、独立的脚本，不接受任何输入，并按照下文指定的格式精确打印一行输出。\n\n基本原理和目标。从以下定义出发：对于一个可微目标函数 $f(\\mathbf{p})$，在无约束空间中的一个基本梯度下降步骤通过 $\\mathbf{z}^{(t+1)} = \\mathbf{z}^{(t)} - \\eta_t \\nabla f(\\mathbf{z}^{(t)})$ 来更新 $\\mathbf{z}$，其中 $\\eta_t$ 是一个正步长，$\\nabla f$ 是梯度。当存在约束时，一种有原则的方法是将其重新参数化为一个无约束变量并使用链式法则，另一种方法是在原始变量上进行下降，然后根据欧几里得范数投影回可行集。您必须从这些基本原理出发，推导出必要的表达式。\n\n需要实现和比较的算法：\n- 方法 A (单纯形上的投影梯度)：直接在 $\\mathbf{p} \\in \\Delta^n$ 上操作。在每次迭代中，对 $\\mathbf{p}$ 进行一次无约束梯度下降，然后计算其到 $\\Delta^n$ 上的欧几里得投影。您必须通过使用一阶最优性条件求解相应的约束最小二乘问题，来推导并实现到 $\\Delta^n$ 上的欧几里得投影；不要使用任何预封装的投影例程。\n- 方法 B (Softmax 重新参数化)：引入一个无约束参数 $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$，并定义 $\\mathbf{p} = \\mathrm{softmax}(\\boldsymbol{\\theta})$，其中 softmax 函数由 $p_i = \\exp(\\theta_i) / \\sum_{j=1}^n \\exp(\\theta_j)$ 给出。对复合目标函数 $f(\\mathrm{softmax}(\\boldsymbol{\\theta}))$ 在 $\\boldsymbol{\\theta}$ 上应用梯度下降。您必须通过链式法则和 softmax 函数的雅可比矩阵，推导出 $\\nabla_{\\boldsymbol{\\theta}} f(\\mathrm{softmax}(\\boldsymbol{\\theta}))$ 的显式表达式；不要假设该表达式是给定的。\n\n步长策略和初始化。两种方法都使用相同的递减步长策略 $\\eta_t = \\eta_0 / \\sqrt{t+1}$（其中 $\\eta_0 = 0.3$），以及固定的迭代预算 $T = 4000$。对于投影方法，将 $\\mathbf{p}^{(0)}$ 初始化为均匀向量，即 $p_i^{(0)} = 1/n$。将 $\\boldsymbol{\\theta}^{(0)}$ 初始化为零向量，以使 $\\mathrm{softmax}(\\boldsymbol{\\theta}^{(0)})$ 也是均匀的。在整个过程中使用 $n = 4$。\n\n目标函数和梯度。您必须根据定义实现以下在 $\\Delta^4$ 上的目标函数 $f(\\mathbf{p})$ 及其梯度 $\\nabla_{\\mathbf{p}} f(\\mathbf{p})$。在所有情况下，$\\|\\cdot\\|_2$ 表示欧几里得范数。所有计算都是无量纲的；不涉及物理单位。\n- 情况 1 (朝向目标向量的二次函数)：$f(\\mathbf{p}) = \\tfrac{1}{2} \\|\\mathbf{p} - \\mathbf{q}\\|_2^2$，其中 $\\mathbf{q} = [0.7, 0.1, 0.1, 0.1]$。\n- 情况 2 (线性目标函数)：$f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$，其中 $\\mathbf{c} = [0.5, -1.0, 0.2, 0.2]$。\n- 情况 3 (二次数据拟合)：$f(\\mathbf{p}) = \\tfrac{1}{2} \\|A \\mathbf{p} - \\mathbf{b}\\|_2^2$，其中\n  $$\n  A = \\begin{bmatrix}\n  2  -1  0  0 \\\\\n  -1  2  -1  0 \\\\\n  0  -1  2  -1 \\\\\n  0  0  -1  2\n  \\end{bmatrix}, \\quad \\mathbf{b} = [1.0, 0.0, 0.0, 0.0].\n  $$\n- 情况 4 (朝向均匀分布的二次函数)：$f(\\mathbf{p}) = \\tfrac{1}{2} \\|\\mathbf{p} - \\mathbf{u}\\|_2^2$，其中 $\\mathbf{u} = [0.25, 0.25, 0.25, 0.25]$。\n\n对于每种情况，从第一性原理实现 $\\nabla_{\\mathbf{p}} f(\\mathbf{p})$。对于线性目标函数，回想一下梯度是恒定的。对于二次目标函数，使用标准法则，即对于 $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{M}\\mathbf{p} - \\mathbf{r}\\|_2^2$，有 $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{M}^\\top (\\mathbf{M}\\mathbf{p} - \\mathbf{r})$，这可以从链式法则和微分的线性性质推导出来。\n\n比较指标。对于每种情况，在使用指定的策略和初始化运行两种方法恰好 $T$ 次迭代后，计算得到的概率向量 $\\mathbf{p}_{\\mathrm{softmax}}$ 和 $\\mathbf{p}_{\\mathrm{proj}}$ 之间的 $\\ell_1$ 距离，即 $\\|\\mathbf{p}_{\\mathrm{softmax}} - \\mathbf{p}_{\\mathrm{proj}}\\|_1 = \\sum_{i=1}^4 |p_{\\mathrm{softmax},i} - p_{\\mathrm{proj},i}|$。\n\n测试套件。使用上述四种情况作为测试套件。这些情况涵盖：一个类内部目标（情况 1）、一个边界最优解（情况 2）、一个带有耦合的结构化二次函数（情况 3）以及一个已经是最后的最优起始点（情况 4）。\n\n要求的最终输出格式。您的程序应生成单行输出，其中包含一个包含四个浮点数的列表，每个浮点数等于相应情况下的 $\\ell_1$ 距离，四舍五入到六位小数，形式为用方括号括起来的逗号分隔列表，例如，“[0.000123,0.045678,0.001000,0.000000]”。",
            "solution": "我们提出基于梯度、链式法则和通过投影的约束优化的核心定义，所进行的有原则的推导和算法设计。\n\n可行集和目标函数。可行集是概率单纯形 $\\Delta^n = \\{\\mathbf{p} \\in \\mathbb{R}^n \\mid p_i \\ge 0,\\ \\sum_{i=1}^n p_i = 1\\}$。我们考虑 $n = 4$ 和指定的四个目标函数。对于线性和二次函数，关于 $\\mathbf{p}$ 的梯度遵循基本规则：\n- 对于 $f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$，梯度为 $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{c}$，因为微分是线性的，并且 $\\partial (\\mathbf{c}^\\top \\mathbf{p}) / \\partial \\mathbf{p} = \\mathbf{c}$。\n- 对于 $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{M} \\mathbf{p} - \\mathbf{r}\\|_2^2$，定义 $\\mathbf{y}(\\mathbf{p}) = \\mathbf{M}\\mathbf{p} - \\mathbf{r}$ 和 $g(\\mathbf{y}) = \\tfrac{1}{2} \\|\\mathbf{y}\\|_2^2$。根据链式法则，\n$$\n\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{M}^\\top \\nabla_{\\mathbf{y}} g(\\mathbf{y}(\\mathbf{p})) = \\mathbf{M}^\\top \\mathbf{y}(\\mathbf{p}) = \\mathbf{M}^\\top (\\mathbf{M}\\mathbf{p} - \\mathbf{r}).\n$$\n令 $\\mathbf{M} = \\mathbf{I}$ 和 $\\mathbf{r} = \\mathbf{q}$ 得到 $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{p} - \\mathbf{q}$。令 $\\mathbf{M} = A$ 和 $\\mathbf{r} = \\mathbf{b}$ 得到 $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = A^\\top( A\\mathbf{p} - \\mathbf{b})$。\n\n方法 A (单纯形上的投影梯度)。从无约束梯度下降更新开始\n$$\n\\tilde{\\mathbf{p}}^{(t+1)} = \\mathbf{p}^{(t)} - \\eta_t \\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)}),\n$$\n然后通过计算到 $\\Delta^n$ 上的欧几里得投影来强制满足可行性：\n$$\n\\mathbf{p}^{(t+1)} = \\operatorname{Proj}_{\\Delta^n}(\\tilde{\\mathbf{p}}^{(t+1)}) := \\arg\\min_{\\mathbf{x} \\in \\Delta^n} \\|\\mathbf{x} - \\tilde{\\mathbf{p}}^{(t+1)}\\|_2^2.\n$$\n我们使用拉格朗日乘子法和最优性条件来推导投影。考虑优化问题\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\tfrac{1}{2}\\|\\mathbf{x} - \\mathbf{v}\\|_2^2 \\quad \\text{subject to} \\quad \\sum_{i=1}^n x_i = 1,\\ \\ x_i \\ge 0 \\ \\forall i,\n$$\n其中 $\\mathbf{v} \\in \\mathbb{R}^n$ 是给定的。构造拉格朗日函数\n$$\n\\mathcal{L}(\\mathbf{x}, \\tau, \\boldsymbol{\\mu}) = \\tfrac{1}{2}\\|\\mathbf{x} - \\mathbf{v}\\|_2^2 + \\tau \\left(\\sum_{i=1}^n x_i - 1\\right) - \\sum_{i=1}^n \\mu_i x_i,\n$$\n其中 $\\tau \\in \\mathbb{R}$ 用于等式约束，$\\mu_i \\ge 0$ 用于不等式约束。Karush-Kuhn-Tucker (KKT) 条件（平稳性、原始可行性、对偶可行性、互补松弛性）是：\n- 平稳性：\n$$\n\\nabla_{\\mathbf{x}} \\mathcal{L} = \\mathbf{x} - \\mathbf{v} + \\tau \\mathbf{1} - \\boldsymbol{\\mu} = \\mathbf{0}.\n$$\n- 原始可行性：$\\sum_i x_i = 1$，$x_i \\ge 0$ 对所有 $i$ 成立。\n- 对偶可行性：$\\mu_i \\ge 0$ 对所有 $i$ 成立。\n- 互补松弛性：$\\mu_i x_i = 0$ 对所有 $i$ 成立。\n\n从平稳性条件可知，对于每个 $i$，\n$$\nx_i = v_i - \\tau + \\mu_i.\n$$\n如果 $x_i  0$，则互补松弛性意味着 $\\mu_i = 0$，因此 $x_i = v_i - \\tau$。如果 $x_i = 0$，则 $\\mu_i \\ge 0$ 且 $0 = v_i - \\tau + \\mu_i$ 意味着 $v_i - \\tau \\le 0$。因此，解具有阈值形式\n$$\nx_i^\\star = \\max\\{v_i - \\tau, 0\\}\n$$\n其中标量 $\\tau$ 的选择要满足 $\\sum_{i=1}^n x_i^\\star = 1$。标量 $\\tau$ 是唯一确定的，因为映射 $\\tau \\mapsto \\sum_i \\max\\{v_i - \\tau, 0\\}$ 是连续且严格递减的。一种计算 $\\tau$ 的有效方法是对 $\\mathbf{v}$ 的条目进行降序排序，找到对于排序后的向量 $\\mathbf{u}$ 满足 $u_\\rho - \\frac{1}{\\rho+1}\\left(\\sum_{j=0}^{\\rho} u_j - 1\\right)  0$ 的最大索引 $\\rho$，然后设置\n$$\n\\tau = \\frac{1}{\\rho+1}\\left(\\sum_{j=0}^{\\rho} u_j - 1\\right),\n$$\n并输出逐元素应用的 $\\mathbf{x}^\\star = \\max\\{\\mathbf{v} - \\tau, 0\\}$。这就实现了到 $\\Delta^n$ 上的欧几里得投影。\n\n方法 B (Softmax 重新参数化)。定义无约束参数 $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ 并设置\n$$\np_i(\\boldsymbol{\\theta}) = \\frac{\\exp(\\theta_i)}{\\sum_{j=1}^n \\exp(\\theta_j)}.\n$$\n我们对复合函数 $F(\\boldsymbol{\\theta}) = f(\\mathbf{p}(\\boldsymbol{\\theta}))$ 执行梯度下降。根据链式法则，\n$$\n\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta}) = J(\\boldsymbol{\\theta})^\\top \\nabla_{\\mathbf{p}} f(\\mathbf{p}(\\boldsymbol{\\theta})),\n$$\n其中 $J(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{n \\times n}$ 是 softmax 的雅可比矩阵。Softmax 的导数是一个标准结果，通过对 $p_i = \\exp(\\theta_i)/\\sum_k \\exp(\\theta_k)$ 求导得到。对于每个 $i,k$，\n$$\n\\frac{\\partial p_i}{\\partial \\theta_k} = p_i(\\delta_{ik} - p_k),\n$$\n其中 $\\delta_{ik}$ 是克罗内克 δ。因此，如果我们表示 $\\mathbf{g} = \\nabla_{\\mathbf{p}} f(\\mathbf{p}(\\boldsymbol{\\theta}))$，那么 $\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta})$ 的第 $k$ 个分量是\n$$\n\\left[\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta})\\right]_k\n= \\sum_{i=1}^n g_i \\frac{\\partial p_i}{\\partial \\theta_k}\n= \\sum_{i=1}^n g_i p_i(\\delta_{ik} - p_k)\n= p_k \\left(g_k - \\sum_{i=1}^n g_i p_i\\right).\n$$\n以向量形式表示，\n$$\n\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta}) = \\mathbf{p} \\odot \\left(\\mathbf{g} - (\\mathbf{g}^\\top \\mathbf{p}) \\mathbf{1}\\right),\n$$\n其中 $\\odot$ 表示逐元素乘法。梯度下降的更新步骤变为\n$$\n\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta_t \\left[ \\mathbf{p}^{(t)} \\odot \\left(\\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)}) - \\left(\\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)})\\right)^\\top \\mathbf{p}^{(t)} \\cdot \\mathbf{1} \\right) \\right],\n$$\n其中 $\\mathbf{p}^{(t)} = \\mathrm{softmax}(\\boldsymbol{\\theta}^{(t)})$。通过构造，此更新确保了相应的 $\\mathbf{p}^{(t)}$ 保持在 $\\Delta^n$ 内。\n\n步长和初始化。我们使用递减策略 $\\eta_t = \\eta_0/\\sqrt{t+1}$，其中 $\\eta_0 = 0.3$，这满足了减小步长以促进稳定性的常见启发式方法。对于这两种方法，初始化都是均匀分布：对于方法 A，直接设置 $p_i^{(0)} = 1/n$；对于方法 B，设置 $\\boldsymbol{\\theta}^{(0)} = \\mathbf{0}$，这样 softmax 会产生均匀分布。\n\n测试用例和预期。我们实现四个测试用例：\n- 情况 1：$f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{p} - \\mathbf{q}\\|_2^2$ 其中 $\\mathbf{q} = [0.7, 0.1, 0.1, 0.1]$。无约束最小化器是 $\\mathbf{q}$，它位于单纯形内，因此约束最小化器也是 $\\mathbf{q}$。两种方法都应该收敛到接近 $\\mathbf{q}$。\n- 情况 2：$f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$ 其中 $\\mathbf{c} = [0.5, -1.0, 0.2, 0.2]$。约束最小化器是对应于 $\\mathbf{c}$ 最小分量的顶点，即第二个坐标；两种方法都应该逼近一个集中在该坐标上的分布。\n- 情况 3：$f(\\mathbf{p}) = \\tfrac{1}{2}\\|A\\mathbf{p} - \\mathbf{b}\\|_2^2$ 其中给定三对角矩阵 $A$ 和 $\\mathbf{b}$。最优解可能在内部或边界上；两种方法都应该逼近一个相似的解。\n- 情况 4：$f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{p} - \\mathbf{u}\\|_2^2$ 其中 $\\mathbf{u}$ 是均匀的。均匀初始化已经是最后的最优解，所以两种方法都应该保持接近均匀分布。\n\n比较指标。在每种情况下迭代 $T=4000$ 次后，我们计算 $\\ell_1$ 距离 $\\|\\mathbf{p}_{\\mathrm{softmax}} - \\mathbf{p}_{\\mathrm{proj}}\\|_1 = \\sum_{i=1}^4 |p_{\\mathrm{softmax},i} - p_{\\mathrm{proj},i}|$。该指标是非负的，当且仅当两个概率向量匹配时才为零，并且对坐标间的差异敏感。\n\n程序设计。该程序将：\n- 使用上述由 KKT 推导出的阈值法实现到 $\\Delta^n$ 上的投影。\n- 使用共同的步长策略和初始化实现方法 A 和方法 B。\n- 为四种情况定义目标函数和梯度函数。\n- 在每种情况下对两种方法运行 $T = 4000$ 次迭代，并收集 $\\ell_1$ 距离。\n- 以指定格式打印一行包含四个距离（四舍五入到六位小数）的列表。\n\n该方法整合了微分的基本规则、链式法则和 Karush-Kuhn-Tucker 条件，以推导出用于在概率单纯形上进行约束梯度下降和重新参数化梯度下降的可实现算法。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax(theta: np.ndarray) -> np.ndarray:\n    \"\"\"Numerically stable softmax function.\"\"\"\n    z = theta - np.max(theta)\n    e = np.exp(z)\n    return e / np.sum(e)\n\ndef grad_softmax_chain(p: np.ndarray, grad_p: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes gradient of f(softmax(theta)) w.r.t. theta using the chain rule.\n    ∇_θ f = p ⊙ (∇_p f - ( (∇_p f)ᵀ p ))\n    \"\"\"\n    return p * (grad_p - np.dot(grad_p, p))\n\ndef project_simplex(v: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Projects a vector v onto the probability simplex using the method\n    derived from KKT conditions.\n    \"\"\"\n    n = len(v)\n    # Check if already on simplex\n    if np.isclose(np.sum(v), 1) and np.all(v >= 0):\n        return v\n    # Sort v into u in descending order\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # Find the largest rho such that u_rho - (1/(rho+1)) * (sum_i u_i - 1) > 0\n    rho_candidates = np.where(u * np.arange(1, n + 1) > (cssv - 1))[0]\n    if len(rho_candidates) == 0:\n        # This case can happen if v has large negative values.\n        # It implies tau is large, making all projections 0 except one.\n        # Project to the closest vertex.\n        proj = np.zeros(n)\n        proj[np.argmax(v)] = 1.0\n        return proj\n    rho = rho_candidates[-1]\n    # Compute tau = (1/(rho+1)) * (sum_i u_i - 1)\n    tau = (cssv[rho] - 1) / (rho + 1.0)\n    # x_i = max(v_i - tau, 0)\n    return np.maximum(v - tau, 0)\n\ndef solve():\n    \"\"\"Main function to run the comparison for all four cases.\"\"\"\n    n = 4\n    eta0 = 0.3\n    T = 4000\n    \n    q1 = np.array([0.7, 0.1, 0.1, 0.1])\n    c2 = np.array([0.5, -1.0, 0.2, 0.2])\n    A3 = np.array([[2, -1, 0, 0], [-1, 2, -1, 0], [0, -1, 2, -1], [0, 0, -1, 2]], dtype=float)\n    b3 = np.array([1.0, 0.0, 0.0, 0.0], dtype=float)\n    u4 = np.full(n, 1.0 / n)\n\n    cases = [\n        {\"grad_f\": lambda p: p - q1},\n        {\"grad_f\": lambda p: c2},\n        {\"grad_f\": lambda p: A3.T @ (A3 @ p - b3)},\n        {\"grad_f\": lambda p: p - u4},\n    ]\n\n    results = []\n\n    for case in cases:\n        grad_f = case[\"grad_f\"]\n\n        # Method A: Projected Gradient Descent\n        p_proj = np.full(n, 1.0 / n)\n        for t in range(T):\n            eta_t = eta0 / np.sqrt(t + 1)\n            grad_p = grad_f(p_proj)\n            p_tilde = p_proj - eta_t * grad_p\n            p_proj = project_simplex(p_tilde)\n\n        # Method B: Softmax Reparameterization\n        theta_softmax = np.zeros(n)\n        for t in range(T):\n            eta_t = eta0 / np.sqrt(t + 1)\n            p_softmax_iter = softmax(theta_softmax)\n            grad_p = grad_f(p_softmax_iter)\n            grad_theta = grad_softmax_chain(p_softmax_iter, grad_p)\n            theta_softmax -= eta_t * grad_theta\n        \n        p_softmax_final = softmax(theta_softmax)\n\n        l1_dist = np.sum(np.abs(p_softmax_final - p_proj))\n        results.append(l1_dist)\n    \n    # Format output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "为了在现代硬件上实现极致的计算效率，研究人员常常使用低精度浮点数（如16位浮点）进行计算，但这可能导致数值不稳定和精度损失。本练习将指导你设计并实现一种混合精度梯度下降方案，其中计算主要在低精度下进行，而参数的“主副本”则维持在较高精度以保证收敛的稳定性 。你将亲手实践损失缩放（loss scaling）这一关键技术来防止数值下溢，并量化混合精度策略在不同条件下对最终解精度的影响。",
            "id": "3139464",
            "problem": "您需要为光滑凸目标设计、实现并评估一种混合精度梯度下降方法，该方法需从第一性原理出发进行原则性推导。从可微性和凸性的定义，以及可微函数的一阶泰勒模型开始。以此为基础，推导出一个下降方向的更新规则，确保在适当的步长下目标函数值会减小。您的实现必须包含两个求解器：一个全精度基准求解器和一个混合精度变体求解器。混合精度变体在半精度下执行算术运算，同时以单精度维护参数的主副本。您将在一系列凸二次目标上测试这两个求解器，并量化由混合精度引入的精度损失。\n\n使用的定义和约束：\n- 设 $f:\\mathbb{R}^d \\to \\mathbb{R}$ 是一个可微的凸函数，其梯度利普希茨连续，常数为 $L \\gt 0$。关于点 $x \\in \\mathbb{R}^d$ 的一阶泰勒模型为 $f(x + \\Delta) \\approx f(x) + \\nabla f(x)^\\top \\Delta$。对于足够小的步长，沿负梯度方向的最速下降为减小 $f$ 的值提供了一种原则性方法。\n- 凸二次目标函数的形式为 $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x + c$，其中 $A \\in \\mathbb{R}^{d \\times d}$ 是对称正定矩阵，$b \\in \\mathbb{R}^d$，且 $c \\in \\mathbb{R}$。\n\n算法要求：\n- 基准求解器（全精度）：参数和所有计算均保持在单精度（$32$ 位浮点数）。从 $x_0 \\in \\mathbb{R}^d$ 开始，迭代 $x_{k+1} = x_k - \\alpha \\, g_k$，$k = 0,1,\\dots,T-1$，其中 $g_k$ 是 $f$在 $x_k$ 处的梯度，$\\alpha \\gt 0$ 是一个固定步长。\n- 混合精度求解器：以 $32$ 位浮点数维护参数 $x_k$ 的主副本。在每次迭代中：\n  1. 将当前参数转换为 $16$ 位浮点数以获得 $x_k^{(16)}$。\n  2. 使用 $16$ 位浮点数算术计算梯度，并使用一个显式的损失缩放因子 $s \\in \\mathbb{R}_{\\gt 0}$ 来缓解下溢。对于二次目标函数，真实梯度为 $\\nabla f(x) = A x - b$。在 $16$ 位精度下计算缩放后的梯度 $g_k^{\\mathrm{scaled},(16)} = (s A^{(16)}) x_k^{(16)} - (s b^{(16)})$，其中 $A^{(16)}$ 和 $b^{(16)}$ 是 $A$ 和 $b$ 转换为 $16$ 位浮点数的结果。然后转换回 $32$ 位浮点数并取消缩放：$g_k^{(32)} = \\mathrm{cast32}(g_k^{\\mathrm{scaled},(16)}) / s$。\n  3. 在 $32$ 位浮点数精度下更新主参数：$x_{k+1} = x_k - \\alpha \\, g_k^{(32)}$。\n- 在迭代结束后进行目标函数评估和误差测量时，始终使用 $64$ 位浮点数算术以高精度计算 $f(x)$，以避免污染精度评估。\n\n测试套件：\n在以下四个凸二次测试用例上实现并运行两个求解器。在每个用例中，两个求解器都使用相同的固定步长 $\\alpha$ 和迭代次数 $T$，从相同的初始点 $x_0$ 开始，并比较最终的目标函数值和参数。\n\n对于每个用例 $i \\in \\{1,2,3,4\\}$，报告混合精度结果是否在指定的绝对阈值 $\\tau_f$（最终目标值差异）和 $\\tau_x$（均方根（RMS）参数差异）内与基准结果匹配。通过/失败条件是：\n- 设 $\\Delta f_i = \\left| f(x^{\\mathrm{mixed}}_T) - f(x^{\\mathrm{base}}_T) \\right|$ 和 $\\Delta x_i = \\sqrt{\\tfrac{1}{d} \\| x^{\\mathrm{mixed}}_T - x^{\\mathrm{base}}_T \\|_2^2}$。用例 $i$ 通过当且仅当 $\\Delta f_i \\le \\tau_f$ 且 $\\Delta x_i \\le \\tau_x$。\n\n四个用例如下：\n\n- 用例 1（良态二维二次函数，理想路径）：\n  - $d = 2$,\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  2 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$,\n  - $c = 0$,\n  - $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\alpha = 0.45$,\n  - $T = 200$,\n  - 损失缩放因子 $s = 1$,\n  - 阈值：$\\tau_f = 10^{-4}$，$\\tau_x = 10^{-4}$。\n\n- 用例 2（病态二维二次函数，接近稳定性边界）：\n  - $d = 2$,\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  1000 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$,\n  - $c = 0$,\n  - $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\alpha = 0.0009$,\n  - $T = 5000$,\n  - $s = 1$,\n  - 阈值：$\\tau_f = 10^{-3}$，$\\tau_x = 10^{-3}$。\n\n- 用例 3（中等条件数的十维对角二次函数）：\n  - $d = 10$,\n  - $A = \\mathrm{diag}\\left( 1, 2, 3, 5, 8, 13, 21, 34, 55, 89 \\right)$,\n  - $b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0.5 \\\\ -0.5 \\\\ 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\\\ 0.0625 \\\\ -0.0625 \\end{bmatrix}$,\n  - $c = 0$,\n  - $x_0 = \\mathbf{0}_{10}$,\n  - $\\alpha = 0.009$,\n  - $T = 8000$,\n  - $s = 1$,\n  - 阈值：$\\tau_f = 5 \\times 10^{-3}$，$\\tau_x = 5 \\times 10^{-3}$。\n\n- 用例 4（用于测试下溢和损失缩放的微小尺度五维二次函数）：\n  - $d = 5$,\n  - $A = 10^{-6} \\, I_5$,\n  - $b = 10^{-6} \\, \\mathbf{1}_5$,\n  - $c = 0$,\n  - $x_0 = \\mathbf{0}_5$,\n  - $\\alpha = 5 \\times 10^{5}$,\n  - $T = 20$,\n  - $s = 4096$,\n  - 阈值：$\\tau_f = 5 \\times 10^{-7}$，$\\tau_x = 2 \\times 10^{-5}$。\n\n实现细节：\n- 对于二次目标函数，使用从二次型微分定义推导出的基本梯度恒等式 $\\nabla f(x) = A x - b$。\n- 仅在进行最终精度测量时，以 $64$ 位浮点数计算 $f(x)$，即 $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x + c$。\n\n要求的最终输出格式：\n- 您的程序应生成一行输出，其中包含一个 Python 风格的布尔值列表，对应于从用例 1 到用例 4 的四个用例，每个布尔值为 $\\mathrm{True}$ 当且仅当该用例通过阈值测试，否则为 $\\mathrm{False}$。例如：“[True,False,True,True]”。\n- 不应打印任何其他文本。\n\n不涉及角度单位。没有物理单位。所有数值结果都是无量纲的实数，表示为标准浮点值。输出应根据指定数据确定性地计算，并且不得依赖于任何外部输入。",
            "solution": "我们从光滑凸优化的第一性原理开始。设 $f:\\mathbb{R}^d \\to \\mathbb{R}$ 是一个可微的凸函数，其梯度是利普希茨连续的，常数为 $L \\gt 0$，即对所有 $x,y \\in \\mathbb{R}^d$ 都有 $\\| \\nabla f(x) - \\nabla f(y) \\|_2 \\le L \\|x - y\\|_2$。对于小的 $\\Delta \\in \\mathbb{R}^d$，一阶泰勒展开给出\n$$\nf(x + \\Delta) \\approx f(x) + \\nabla f(x)^\\top \\Delta.\n$$\n为减小 $f$ 的值，选择一个使 $\\nabla f(x)^\\top \\Delta$ 为负的方向 $\\Delta$。在欧几里得范数下，最速下降方向是负梯度方向，这可由 Cauchy–Schwarz 不等式得出：\n$$\n\\min_{\\|\\Delta\\|_2 = \\eta} \\nabla f(x)^\\top \\Delta = - \\eta \\, \\|\\nabla f(x)\\|_2,\n$$\n在 $\\Delta = - \\eta \\, \\tfrac{\\nabla f(x)}{\\|\\nabla f(x)\\|_2}$ 处取得。一个实用的迭代方案是在负梯度方向上使用大小为 $\\alpha \\gt 0$ 的步长，\n$$\nx_{k+1} = x_k - \\alpha \\, \\nabla f(x_k).\n$$\n对于 $L$-利普希茨梯度，标准的下降引理表明，对于 $\\alpha \\in (0, 2/L)$，该方法会产生一个非递增的目标函数值序列，并且对于凸函数 $f$，在适当条件下它会收敛到一个最小值点。\n\n对于凸二次函数 $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x + c$，其中 $A \\in \\mathbb{R}^{d \\times d}$ 是对称正定矩阵，$b \\in \\mathbb{R}^d$，其梯度可以通过逐项微分得到。二次项的梯度是\n$$\n\\nabla \\left( \\tfrac{1}{2} x^\\top A x \\right) = \\tfrac{1}{2} \\left( A + A^\\top \\right) x = A x,\n$$\n因为 $A$ 是对称的，线性项的梯度是 $-b$。因此，\n$$\n\\nabla f(x) = A x - b.\n$$\n这个恒等式是我们实现的核心。\n\n混合精度设计。$16$ 位浮点数算术（半精度）比 $32$ 位浮点数（单精度）具有更有限的动态范围和更粗的离散化。为了保持稳定性，我们以 $32$ 位浮点数维护参数 $x_k$ 的主副本，并在 $16$ 位浮点数下执行梯度计算以降低计算成本。为了缓解在小数值范围内的下溢问题，我们使用损失缩放。定义一个缩放因子 $s \\gt 0$，并考虑缩放后的目标函数 $f_s(x) = s \\, f(x)$。其梯度满足 $\\nabla f_s(x) = s \\, \\nabla f(x)$。如果我们在 $16$ 位浮点数下计算 $\\nabla f_s(x)$，然后在应用更新之前在 $32$ 位浮点数下除以 $s$，我们将恢复相同的更新方向，但中间值具有更大的量级，从而在 $16$ 位下不太可能发生下溢。\n\n对于二次函数的情况，在 $16$ 位浮点数下稳定地计算 $\\nabla f_s(x)$ 意味着在易于下溢的操作之前执行缩放。具体来说，应该在 $16$ 位浮点数算术中计算\n$$\n\\nabla f_s(x) = s \\, (A x - b) = (s A) x - s b\n$$\n，其中缩放因子 $s$ 在矩阵向量乘法和减法之前应用于 $A$ 和 $b$。这确保了 $A$ 和 $b$ 的小元素在算术运算前被放大，减少了在 $16$ 位下被舍入为零的机会。在 $16$ 位浮点数下获得缩放后的梯度后，我们将其转换为 $32$ 位浮点数并取消缩放：\n$$\ng^{(32)} = \\frac{\\mathrm{cast}_{32} \\left( \\nabla f_s(x)^{(16)} \\right)}{s}.\n$$\n然后在 $32$ 位浮点数下应用参数更新：\n$$\nx_{k+1} = x_k - \\alpha \\, g^{(32)}.\n$$\n\n精度评估。为了量化混合精度的精度损失，我们与使用相同步长、相同迭代次数和相同初始条件计算的全单精度基准进行比较。我们定义两个指标：\n- 最终目标函数差距 $\\Delta f = \\left| f(x_T^{\\mathrm{mixed}}) - f(x_T^{\\mathrm{base}}) \\right|$。\n- 参数均方根差异 $\\Delta x = \\sqrt{ \\tfrac{1}{d} \\| x_T^{\\mathrm{mixed}} - x_T^{\\mathrm{base}} \\|_2^2 }$。\n为了避免算术噪声污染这些评估，$f(x)$ 在 $64$ 位浮点数下进行评估。\n\n测试套件设计理念：\n- 用例 1 是良态的，其中 $A = \\mathrm{diag}(1,2)$，步长 $\\alpha = 0.45$ 接近但低于 $1/L = 1/2$。它模拟了一个典型的“理想路径”，两个求解器应该非常接近；阈值 $\\tau_f = 10^{-4}$ 和 $\\tau_x = 10^{-4}$ 很严格但可以达到。\n- 用例 2 是病态的，其中 $A = \\mathrm{diag}(1,1000)$，步长 $\\alpha = 9 \\times 10^{-4}$ 略低于 $1/L = 10^{-3}$，强调了对舍入误差的敏感性以及在刚性方向上的缓慢收敛。阈值放宽到 $\\tau_f = 10^{-3}$ 和 $\\tau_x = 10^{-3}$。\n- 用例 3 是一个 10 维对角二次函数，其特征值分布高达 89，并且 $b$ 中有不同符号和量级的值，使用 $\\alpha = 0.009$，这满足 $\\alpha \\lt 2/L \\approx 0.02247$。阈值 $\\tau_f = 5 \\times 10^{-3}$ 和 $\\tau_x = 5 \\times 10^{-3}$ 反映了更高的维度和舍入累积。\n- 用例 4 使用一个微小尺度问题，其中 $A = 10^{-6} I_5$，$b = 10^{-6} \\mathbf{1}$，以及一个大但稳定的步长 $\\alpha = 5 \\times 10^5$（远低于 $2/L = 2 \\times 10^6$），以故意将梯度推入 $16$ 位浮点数的下溢区域。使用 $s = 4096$ 的损失缩放将中间量重新缩放到 $16$ 位浮点数的正常范围内。阈值 $\\tau_f = 5 \\times 10^{-7}$ 和 $\\tau_x = 2 \\times 10^{-5}$ 在小绝对尺度和预期舍入之间取得了平衡。\n\n实现计划：\n- 实现一个函数，以 $64$ 位浮点数计算 $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x + c$。\n- 在 $32$ 位浮点数下实现基准梯度下降，使用 $g = A x - b$ 和更新规则 $x \\leftarrow x - \\alpha g$ 进行 $T$ 次迭代。\n- 实现混合精度梯度下降，使用 $32$ 位浮点数的主参数向量，在 $16$ 位浮点数下计算缩放梯度为 $(s A) x - s b$，在 $32$ 位浮点数下取消缩放，并在 $32$ 位浮点数下更新主参数。\n- 对于每个用例，计算 $(\\Delta f, \\Delta x)$ 并检查两者是否都低于 $(\\tau_f, \\tau_x)$，如果是则返回 $\\mathrm{True}$，否则返回 $\\mathrm{False}$。\n- 输出一个包含四个用例的布尔值的单行 Python 风格列表。\n\n此方法遵循从一阶模型推导出的基本下降原则，并通过在半精度算术运算前应用损失缩放来谨慎地利用混合精度，以在如用例 4 等具有挑战性的情况下保持数值保真度。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef objective_value(A64, b64, c64, x64):\n    # Compute f(x) = 0.5 * x^T A x - b^T x + c in float64\n    return 0.5 * float(x64.T @ (A64 @ x64)) - float(b64.T @ x64) + float(c64)\n\ndef gradient_descent_baseline(A, b, x0, alpha, iters):\n    # Baseline GD in float32 for compute and state\n    A32 = A.astype(np.float32)\n    b32 = b.astype(np.float32)\n    x = x0.astype(np.float32).copy()\n    for _ in range(iters):\n        g = A32 @ x - b32  # grad in float32\n        x = x - np.float32(alpha) * g\n    return x.astype(np.float32)\n\ndef gradient_descent_mixed(A, b, x0, alpha, iters, loss_scale):\n    # Mixed precision: FP32 master weights, FP16 compute with loss scaling\n    # Master\n    x_master = x0.astype(np.float32).copy()\n    # Constants in both dtypes\n    A16 = A.astype(np.float16)\n    b16 = b.astype(np.float16)\n    s16 = np.float16(loss_scale)\n    s32 = np.float32(loss_scale)\n    alpha32 = np.float32(alpha)\n    for _ in range(iters):\n        # Cast master params to FP16 for compute\n        x16 = x_master.astype(np.float16)\n        # Compute scaled gradient entirely in FP16: (s*A) x - s*b\n        # Do scaling before the matvec/subtraction to mitigate underflow\n        # The problem asks for (s * A_16) * x_16 - (s * b_16).\n        # We perform scaling on the FP16 types to be precise.\n        g_scaled_16 = (s16 * A16) @ x16 - (s16 * b16)\n        # Unscale in FP32\n        g32 = g_scaled_16.astype(np.float32) / s32\n        # Update master weights in FP32\n        x_master = x_master - alpha32 * g32\n    return x_master.astype(np.float32)\n\ndef run_case(A, b, c, x0, alpha, iters, loss_scale, tau_f, tau_x):\n    # Solve with baseline and mixed precision\n    x_base = gradient_descent_baseline(A, b, x0, alpha, iters)\n    x_mix = gradient_descent_mixed(A, b, x0, alpha, iters, loss_scale)\n\n    # Evaluate objective and differences in float64 for assessment\n    A64 = A.astype(np.float64)\n    b64 = b.astype(np.float64)\n    c64 = np.float64(c)\n    x_base_64 = x_base.astype(np.float64)\n    x_mix_64 = x_mix.astype(np.float64)\n\n    f_base = objective_value(A64, b64, c64, x_base_64)\n    f_mix = objective_value(A64, b64, c64, x_mix_64)\n    df = abs(f_mix - f_base)\n\n    dx_rms = float(np.sqrt(np.mean((x_mix_64 - x_base_64) ** 2)))\n\n    ok = (df = tau_f) and (dx_rms = tau_x)\n    return ok\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"A\": np.array([[1.0, 0.0],\n                           [0.0, 2.0]], dtype=np.float64),\n            \"b\": np.array([1.0, -2.0], dtype=np.float64),\n            \"c\": 0.0,\n            \"x0\": np.zeros(2, dtype=np.float64),\n            \"alpha\": 0.45,\n            \"iters\": 200,\n            \"loss_scale\": 1.0,\n            \"tau_f\": 1e-4,\n            \"tau_x\": 1e-4,\n        },\n        # Case 2\n        {\n            \"A\": np.array([[1.0, 0.0],\n                           [0.0, 1000.0]], dtype=np.float64),\n            \"b\": np.array([1.0, 1.0], dtype=np.float64),\n            \"c\": 0.0,\n            \"x0\": np.zeros(2, dtype=np.float64),\n            \"alpha\": 0.0009,\n            \"iters\": 5000,\n            \"loss_scale\": 1.0,\n            \"tau_f\": 1e-3,\n            \"tau_x\": 1e-3,\n        },\n        # Case 3\n        {\n            \"A\": np.diag([1.0, 2.0, 3.0, 5.0, 8.0, 13.0, 21.0, 34.0, 55.0, 89.0]).astype(np.float64),\n            \"b\": np.array([1.0, -1.0, 0.5, -0.5, 0.25, -0.25, 0.125, -0.125, 0.0625, -0.0625], dtype=np.float64),\n            \"c\": 0.0,\n            \"x0\": np.zeros(10, dtype=np.float64),\n            \"alpha\": 0.009,\n            \"iters\": 8000,\n            \"loss_scale\": 1.0,\n            \"tau_f\": 5e-3,\n            \"tau_x\": 5e-3,\n        },\n        # Case 4\n        {\n            \"A\": (1e-6) * np.eye(5, dtype=np.float64),\n            \"b\": (1e-6) * np.ones(5, dtype=np.float64),\n            \"c\": 0.0,\n            \"x0\": np.zeros(5, dtype=np.float64),\n            \"alpha\": 5e5,\n            \"iters\": 20,\n            \"loss_scale\": 4096.0,\n            \"tau_f\": 5e-7,\n            \"tau_x\": 2e-5,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        ok = run_case(\n            case[\"A\"], case[\"b\"], case[\"c\"], case[\"x0\"],\n            case[\"alpha\"], case[\"iters\"], case[\"loss_scale\"],\n            case[\"tau_f\"], case[\"tau_x\"]\n        )\n        results.append(ok)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}