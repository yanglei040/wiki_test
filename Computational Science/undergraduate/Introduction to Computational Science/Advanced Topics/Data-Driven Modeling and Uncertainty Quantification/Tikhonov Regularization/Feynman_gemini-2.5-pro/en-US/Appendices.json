{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration of Tikhonov regularization, we will start with a concrete, low-dimensional example. By working through the calculations for a simple $2 \\times 2$ ill-conditioned system, you will gain a tangible understanding of how the regularization parameter, $\\lambda$, directly influences the solution vector. This exercise  makes the abstract formula $x_\\lambda = (A^T A + \\lambda I)^{-1} A^T b$ tangible and allows you to visualize the \"regularization path\"—the trajectory of the solution as $\\lambda$ changes—revealing the fundamental trade-off between fitting the data and controlling the magnitude of the solution.",
            "id": "2223157",
            "problem": "In many scientific and engineering applications, one encounters the need to solve a linear system of equations $Ax=b$ where the matrix $A$ is ill-conditioned. A common approach to stabilize the solution is Tikhonov regularization, which seeks to find a vector $x$ that minimizes the composite objective function $\\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2$, where $\\| \\cdot \\|_2$ denotes the Euclidean norm and $\\lambda > 0$ is a regularization parameter. The unique solution to this minimization problem, denoted as $x_\\lambda$, is given by the formula $x_\\lambda = (A^T A + \\lambda I)^{-1} A^T b$, where $I$ is the identity matrix.\n\nConsider the ill-conditioned linear system with the matrix $A$ and vector $b$ defined as:\n$$\nA = \\begin{pmatrix} 1 & 1.01 \\\\ 1 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\nThe solution vector $x_\\lambda = \\begin{pmatrix} x_1(\\lambda) \\\\ x_2(\\lambda) \\end{pmatrix}$ traces a path in the plane as the parameter $\\lambda$ varies. There exists a unique positive value of $\\lambda$, let's call it $\\lambda^*$, for which the solution path intersects the $x_2$-axis. This corresponds to the first component of the solution vector being zero, i.e., $x_1(\\lambda^*) = 0$.\n\nDetermine the value of the second component, $x_2(\\lambda^*)$, at this specific point of intersection. Round your final answer to four significant figures.",
            "solution": "The Tikhonov regularized solution to the system $Ax=b$ is given by $x_\\lambda = (A^T A + \\lambda I)^{-1} A^T b$. We are given the matrix $A$ and vector $b$:\n$$\nA = \\begin{pmatrix} 1 & 1.01 \\\\ 1 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\n\nFirst, we compute the required matrix and vector products.\nThe transpose of $A$ is:\n$$\nA^T = \\begin{pmatrix} 1 & 1 \\\\ 1.01 & 1 \\end{pmatrix}\n$$\nNext, we compute $A^T A$:\n$$\nA^T A = \\begin{pmatrix} 1 & 1 \\\\ 1.01 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1.01 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 1 & 1 \\cdot 1.01 + 1 \\cdot 1 \\\\ 1.01 \\cdot 1 + 1 \\cdot 1 & 1.01 \\cdot 1.01 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2.01 \\\\ 2.01 & 1.0201 + 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2.01 \\\\ 2.01 & 2.0201 \\end{pmatrix}\n$$\nNow, we compute $A^T b$:\n$$\nA^T b = \\begin{pmatrix} 1 & 1 \\\\ 1.01 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 2 + 1 \\cdot 1 \\\\ 1.01 \\cdot 2 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2.02 + 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3.02 \\end{pmatrix}\n$$\nThe expression for $x_\\lambda$ involves the matrix $(A^T A + \\lambda I)$:\n$$\nA^T A + \\lambda I = \\begin{pmatrix} 2 & 2.01 \\\\ 2.01 & 2.0201 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2+\\lambda & 2.01 \\\\ 2.01 & 2.0201+\\lambda \\end{pmatrix}\n$$\nTo find the inverse of this matrix, we first compute its determinant:\n$$\n\\det(A^T A + \\lambda I) = (2+\\lambda)(2.0201+\\lambda) - (2.01)^2\n$$\n$$\n= (4.0402 + 2\\lambda + 2.0201\\lambda + \\lambda^2) - 4.0401\n$$\n$$\n= \\lambda^2 + 4.0201\\lambda + 0.0001\n$$\nThe inverse matrix is:\n$$\n(A^T A + \\lambda I)^{-1} = \\frac{1}{\\lambda^2 + 4.0201\\lambda + 0.0001} \\begin{pmatrix} 2.0201+\\lambda & -2.01 \\\\ -2.01 & 2+\\lambda \\end{pmatrix}\n$$\nNow we can write the full expression for $x_\\lambda$:\n$$\nx_\\lambda = \\begin{pmatrix} x_1(\\lambda) \\\\ x_2(\\lambda) \\end{pmatrix} = \\frac{1}{\\lambda^2 + 4.0201\\lambda + 0.0001} \\begin{pmatrix} 2.0201+\\lambda & -2.01 \\\\ -2.01 & 2+\\lambda \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 3.02 \\end{pmatrix}\n$$\nLet's compute the matrix-vector product in the numerator:\n$$\n\\begin{pmatrix} (2.0201+\\lambda) \\cdot 3 - 2.01 \\cdot 3.02 \\\\ -2.01 \\cdot 3 + (2+\\lambda) \\cdot 3.02 \\end{pmatrix} = \\begin{pmatrix} 6.0603 + 3\\lambda - 6.0702 \\\\ -6.03 + 6.04 + 3.02\\lambda \\end{pmatrix} = \\begin{pmatrix} 3\\lambda - 0.0099 \\\\ 3.02\\lambda + 0.01 \\end{pmatrix}\n$$\nSo, the solution vector is:\n$$\nx_\\lambda = \\begin{pmatrix} x_1(\\lambda) \\\\ x_2(\\lambda) \\end{pmatrix} = \\frac{1}{\\lambda^2 + 4.0201\\lambda + 0.0001} \\begin{pmatrix} 3\\lambda - 0.0099 \\\\ 3.02\\lambda + 0.01 \\end{pmatrix}\n$$\nThe problem asks for the value of $x_2(\\lambda^*)$ where $\\lambda^*$ is the positive value of $\\lambda$ that makes $x_1(\\lambda) = 0$.\nWe set the first component's numerator to zero to find $\\lambda^*$:\n$$\n3\\lambda^* - 0.0099 = 0\n$$\n$$\n3\\lambda^* = 0.0099 \\implies \\lambda^* = 0.0033\n$$\nThis value is positive, as required. Now we substitute $\\lambda^*=0.0033$ into the expression for $x_2(\\lambda)$:\n$$\nx_2(\\lambda^*) = \\frac{3.02(0.0033) + 0.01}{(0.0033)^2 + 4.0201(0.0033) + 0.0001}\n$$\nCalculate the numerator:\n$$\n3.02 \\times 0.0033 + 0.01 = 0.009966 + 0.01 = 0.019966\n$$\nCalculate the denominator:\n$$\n(0.0033)^2 = 0.00001089\n$$\n$$\n4.0201 \\times 0.0033 = 0.01326633\n$$\n$$\n\\text{Denominator} = 0.00001089 + 0.01326633 + 0.0001 = 0.01337722\n$$\nFinally, we compute the value of $x_2(\\lambda^*)$:\n$$\nx_2(\\lambda^*) = \\frac{0.019966}{0.01337722} \\approx 1.4925343\n$$\nRounding the result to four significant figures, we get $1.493$.",
            "answer": "$$\\boxed{1.493}$$"
        },
        {
            "introduction": "While manual calculation is insightful, real-world applications require robust and scalable computational methods. The naive formula involving $(A^T A + \\lambda^2 I)^{-1}$ can be numerically unstable for the very ill-conditioned problems where regularization is most needed. This practice  introduces the industry-standard approach based on the Singular Value Decomposition (SVD), which provides a numerically stable way to compute the solution $x_\\lambda$ for any given $\\lambda$. You will implement this SVD-based algorithm to compute the entire regularization path for various challenging scenarios, including underdetermined and rank-deficient systems, and verify key theoretical properties of the path.",
            "id": "3200591",
            "problem": "You are given linear systems with noisy observations where a vector of observations $b \\in \\mathbb{R}^m$ is approximately modeled by a matrix $A \\in \\mathbb{R}^{m \\times n}$ and an unknown coefficient vector $x^\\star \\in \\mathbb{R}^n$ via $b \\approx A x^\\star$. To stabilize estimation when $A$ is ill-conditioned or rank-deficient, consider the Tikhonov-regularized objective\n$$\nJ_\\lambda(x) = \\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. The statistical ridge regression path is obtained by varying $\\lambda$ and observing how the coefficients shrink.\n\nStarting from the fundamental definition above and well-tested facts about linear algebra (including the existence and properties of the Singular Value Decomposition (SVD) and the Moore–Penrose pseudoinverse), derive a numerically stable algorithm to compute, for a logarithmically spaced grid of regularization strengths $\\lambda$, the corresponding minimizers $x_\\lambda$ of $J_\\lambda(x)$, and track coefficient shrinkage along this path. Do not rely on any shortcut formulas presented in this problem statement.\n\nYour program must implement the algorithm and apply it to the following test suite. For reproducibility, all random draws must use the specified seeds and standard normal distributions as indicated.\n\n- Test case $1$ (well-conditioned, overdetermined):\n  - Dimensions: $m = 80$, $n = 20$.\n  - Random seed: $42$.\n  - Construction:\n    - Draw $A \\in \\mathbb{R}^{80 \\times 20}$ with independent standard normal entries.\n    - Draw $x^\\text{true} \\in \\mathbb{R}^{20}$ with independent standard normal entries.\n    - Draw $\\epsilon \\in \\mathbb{R}^{80}$ with independent standard normal entries, then scale by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n- Test case $2$ (underdetermined, wide design):\n  - Dimensions: $m = 50$, $n = 100$.\n  - Random seed: $123$.\n  - Construction:\n    - Draw $A \\in \\mathbb{R}^{50 \\times 100}$, $x^\\text{true} \\in \\mathbb{R}^{100}$, and $\\epsilon \\in \\mathbb{R}^{50}$ as independent standard normal, with $\\epsilon$ scaled by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n- Test case $3$ (rank-deficient design):\n  - Dimensions: $m = 40$, $n = 40$.\n  - Random seed: $7$.\n  - Construction:\n    - Draw $A \\in \\mathbb{R}^{40 \\times 40}$ with independent standard normal entries.\n    - Impose exact column duplication to force rank deficiency: set the column at index $10$ equal to the column at index $5$, and the column at index $15$ equal to the column at index $5$.\n    - Draw $x^\\text{true} \\in \\mathbb{R}^{40}$ and $\\epsilon \\in \\mathbb{R}^{40}$ as independent standard normal, with $\\epsilon$ scaled by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n- Test case $4$ (highly ill-conditioned design with correlated columns):\n  - Dimensions: $m = 60$, $n = 20$.\n  - Random seed: $314$.\n  - Construction:\n    - Draw $G \\in \\mathbb{R}^{60 \\times 20}$ with independent standard normal entries.\n    - Create correlated and ill-conditioned columns by scaling: for column index $j \\in \\{0,1,\\dots,19\\}$, set the $j$-th column of $A$ to be the $j$-th column of $G$ multiplied by $10^{-\\frac{j}{3}}$.\n    - Draw $x^\\text{true} \\in \\mathbb{R}^{20}$ and $\\epsilon \\in \\mathbb{R}^{60}$ as independent standard normal, with $\\epsilon$ scaled by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n\nFor each test case, use a logarithmically spaced grid of $60$ values\n$$\n\\lambda \\in \\{\\lambda_k \\mid \\lambda_k = 10^{\\ell_k},\\ \\ell_k \\text{ equispaced in } [-10,6]\\},\n$$\nthat is, from $10^{-10}$ up to $10^{6}$.\n\nFor each test case, compute the regularization path $\\{x_{\\lambda_k}\\}$ and evaluate the following three boolean properties:\n\n1. Monotone shrinkage of the coefficient Euclidean norm: the sequence $\\{\\lVert x_{\\lambda_k} \\rVert_2\\}_{k=1}^{60}$ is nonincreasing within a small numerical tolerance, meaning $\\lVert x_{\\lambda_{k}} \\rVert_2 \\le \\lVert x_{\\lambda_{k-1}} \\rVert_2 + \\tau$ for all $k$, where $\\tau = 10^{-12} \\cdot \\max(1, \\lVert x_{\\lambda_{k-1}} \\rVert_2)$.\n2. Near-agreement at small regularization with the minimum-norm least squares solution: let $x_{\\mathrm{lsq}}$ be the Moore–Penrose pseudoinverse solution minimizing $\\lVert A x - b \\rVert_2$ with minimum $\\lVert x \\rVert_2$. Check that the relative difference between $x_{\\lambda_{\\min}}$ and $x_{\\mathrm{lsq}}$ satisfies\n$$\n\\frac{\\lVert x_{\\lambda_{\\min}} - x_{\\mathrm{lsq}} \\rVert_2}{\\max(1,\\lVert x_{\\mathrm{lsq}} \\rVert_2)} < 10^{-6}.\n$$\n3. Near-zero coefficients at very large regularization: check that\n$$\n\\frac{\\lVert x_{\\lambda_{\\max}} \\rVert_2}{\\max(1,\\lVert x_{\\mathrm{lsq}} \\rVert_2)} < 10^{-6}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries grouped per test case in the order described above and flattened across all test cases. Concretely, the output must be a list of $12$ boolean values\n$$\n[\\text{m1\\_t1},\\text{m2\\_t1},\\text{m3\\_t1},\\ \\text{m1\\_t2},\\text{m2\\_t2},\\text{m3\\_t2},\\ \\text{m1\\_t3},\\text{m2\\_t3},\\text{m3\\_t3},\\ \\text{m1\\_t4},\\text{m2\\_t4},\\text{m3\\_t4}],\n$$\nwhere, for test case $i$, $\\text{m1\\_t}i$ is the monotone-shrinkage check, $\\text{m2\\_t}i$ is the small-$\\lambda$ agreement check, and $\\text{m3\\_t}i$ is the large-$\\lambda$ near-zero check.",
            "solution": "The user-provided problem is a valid exercise in computational science, specifically on the topic of Tikhonov regularization. It requires the derivation and implementation of a numerically stable algorithm to compute the regularization path for a set of well-defined test cases and to verify key theoretical properties of the solutions. The problem is scientifically grounded, well-posed, objective, and contains all necessary information for its resolution.\n\n### Derivation of the Numerically Stable Algorithm\n\nThe Tikhonov-regularized objective function is given by:\n$$\nJ_\\lambda(x) = \\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$, and $\\lambda \\ge 0$ is the regularization parameter. The function $J_\\lambda(x)$ is convex in $x$. For $\\lambda > 0$, it is strictly convex, guaranteeing a unique minimizer. The minimizer $x_\\lambda$ can be found by setting the gradient of $J_\\lambda(x)$ with respect to $x$ to zero.\n\nFirst, we expand the objective function:\n$$\nJ_\\lambda(x) = (A x - b)^T (A x - b) + \\lambda^2 x^T x = x^T A^T A x - 2 b^T A x + b^T b + \\lambda^2 x^T I x\n$$\nThe gradient with respect to $x$ is:\n$$\n\\nabla_x J_\\lambda(x) = 2 A^T A x - 2 A^T b + 2 \\lambda^2 I x\n$$\nSetting the gradient to zero, $\\nabla_x J_\\lambda(x) = 0$, gives the Tikhonov normal equations:\n$$\n(A^T A + \\lambda^2 I) x = A^T b\n$$\nThe solution is formally $x_\\lambda = (A^T A + \\lambda^2 I)^{-1} A^T b$. However, directly forming the matrix $A^T A$ is numerically unstable, especially if $A$ is ill-conditioned. The condition number of $A^T A$ is the square of the condition number of $A$, which can lead to significant loss of precision.\n\nA numerically stable algorithm is derived using the Singular Value Decomposition (SVD) of $A$. Let the SVD of $A$ be:\n$$\nA = U \\Sigma V^T\n$$\nHere, $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices ($U^T U = I_m$, $V^T V = I_n$), and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix containing the non-negative singular values $\\sigma_i$ in decreasing order. Let $k = \\min(m, n)$. The singular values are $(\\Sigma)_{ii} = \\sigma_i$ for $i=1, \\dots, k$.\n\nWe substitute the SVD into the first term of the objective function. Using the property that the Euclidean norm is invariant under orthogonal transformations (i.e., $\\lVert U z \\rVert_2 = \\lVert z \\rVert_2$), we have:\n$$\n\\lVert A x - b \\rVert_2^2 = \\lVert U \\Sigma V^T x - b \\rVert_2^2 = \\lVert U^T (U \\Sigma V^T x - b) \\rVert_2^2 = \\lVert \\Sigma V^T x - U^T b \\rVert_2^2\n$$\nLet's introduce a change of variables. Define $y = V^T x$ and $c = U^T b$. Since $V$ is orthogonal, $x = V y$, and the norm is preserved: $\\lVert x \\rVert_2^2 = \\lVert V y \\rVert_2^2 = \\lVert y \\rVert_2^2$. The objective function transforms into a simpler form in terms of $y$:\n$$\nJ_\\lambda(y) = \\lVert \\Sigma y - c \\rVert_2^2 + \\lambda^2 \\lVert y \\rVert_2^2\n$$\nThis form is separable. We can write it as a sum over the components of $y$ and $c$:\n$$\nJ_\\lambda(y) = \\sum_{i=1}^m \\left( (\\Sigma y)_i - c_i \\right)^2 + \\lambda^2 \\sum_{j=1}^n y_j^2\n$$\nTaking into account the structure of $\\Sigma$:\n- For $i \\le k = \\min(m, n)$, $(\\Sigma y)_i = \\sigma_i y_i$.\n- For $i > k$, $(\\Sigma y)_i = 0$.\nThe objective function becomes:\n$$\nJ_\\lambda(y) = \\sum_{i=1}^k (\\sigma_i y_i - c_i)^2 + \\sum_{i=k+1}^m c_i^2 + \\lambda^2 \\left( \\sum_{i=1}^k y_i^2 + \\sum_{i=k+1}^n y_i^2 \\right)\n$$\nThe term $\\sum_{i=k+1}^m c_i^2$ is a constant with respect to $y$. To minimize $J_\\lambda(y)$, we can minimize the remaining parts independently for each component $y_i$.\nFor $i \\in \\{1, \\dots, k\\}$: we minimize $(\\sigma_i y_i - c_i)^2 + \\lambda^2 y_i^2$. Setting the derivative with respect to $y_i$ to zero gives:\n$$\n2(\\sigma_i y_i - c_i)\\sigma_i + 2\\lambda^2 y_i = 0 \\implies (\\sigma_i^2 + \\lambda^2) y_i = \\sigma_i c_i\n$$\nThe solution for $y_i$ is:\n$$\ny_i = \\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}\n$$\nThis formula is well-defined even if $\\sigma_i = 0$, in which case $y_i=0$ (for $\\lambda > 0$).\nFor $i \\in \\{k+1, \\dots, n\\}$ (this case only occurs if $n>m$, so $k=m$): we minimize $\\lambda^2 y_i^2$. For $\\lambda > 0$, the minimum is achieved at $y_i = 0$.\n\nThus, the solution vector $y_\\lambda \\in \\mathbb{R}^n$ has components:\n$$\n(y_\\lambda)_i =\n\\begin{cases}\n\\frac{\\sigma_i (U^T b)_i}{\\sigma_i^2 + \\lambda^2} & \\text{for } 1 \\le i \\le k \\\\\n0 & \\text{for } k < i \\le n\n\\end{cases}\n$$\nFinally, we transform the solution $y_\\lambda$ back to the original variable $x_\\lambda$ using $x = V y$:\n$$\nx_\\lambda = V y_\\lambda = \\sum_{i=1}^k (y_\\lambda)_i v_i\n$$\nwhere $v_i$ are the columns of $V$ (or the rows of $V^T$).\n\nThis leads to the following numerically stable algorithm:\n1.  Compute the full SVD of $A = U \\Sigma V^T$.\n2.  Compute the transformed vector $c = U^T b$.\n3.  For each desired $\\lambda$ in the grid:\n    a. Initialize an $n$-dimensional zero vector $y_\\lambda$.\n    b. For $i=1, \\dots, k=\\min(m,n)$, compute $(y_\\lambda)_i = \\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}$.\n    c. Compute the solution $x_\\lambda = V y_\\lambda$.\nThis algorithm avoids the formation of $A^T A$ and relies on the robust SVD computation, making it numerically superior. It correctly handles rank-deficient and ill-conditioned matrices.\n\n### Verification of Properties\n\nThe problem requires checking three properties:\n1.  **Monotone shrinkage of norm**: We must verify that $\\lVert x_\\lambda \\rVert_2$ is a non-increasing function of $\\lambda$. Analytically, $\\lVert x_\\lambda \\rVert_2^2 = \\lVert y_\\lambda \\rVert_2^2 = \\sum_{i=1}^k \\left(\\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}\\right)^2$. Each term in the sum is a non-increasing function of $\\lambda \\ge 0$, so their sum is also non-increasing. Thus, the property is expected to hold up to numerical precision.\n\n2.  **Small $\\lambda$ limit**: As $\\lambda \\to 0$, $x_\\lambda$ should approach the minimum-norm least-squares solution, $x_{\\mathrm{lsq}} = A^+ b$, where $A^+$ is the Moore-Penrose pseudoinverse of $A$. From our derivation, as $\\lambda \\to 0$:\n    $$\n    (y_\\lambda)_i \\to\n    \\begin{cases}\n    c_i / \\sigma_i & \\text{if } \\sigma_i > 0 \\\\\n    0 & \\text{if } \\sigma_i = 0\n    \\end{cases}\n    $$\n    This limiting vector is precisely $\\Sigma^+ c$. Thus, $x_0 = V \\Sigma^+ c = V \\Sigma^+ U^T b = A^+ b$. The property is expected to hold.\n\n3.  **Large $\\lambda$ limit**: As $\\lambda \\to \\infty$, the denominator $\\sigma_i^2 + \\lambda^2$ grows, driving each $(y_\\lambda)_i \\to 0$. Consequently, $y_\\lambda \\to 0$ and $x_\\lambda = V y_\\lambda \\to 0$. The coefficients are expected to shrink to zero.\n\nThe implementation will generate the test cases, apply the SVD-based algorithm, and verify these three properties empirically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization problem for four test cases\n    and verifies theoretical properties of the regularization path.\n    \"\"\"\n    test_cases_params = [\n        # (m, n, seed)\n        (80, 20, 42),\n        (50, 100, 123),\n        (40, 40, 7),\n        (60, 20, 314),\n    ]\n\n    all_results = []\n\n    for i, params in enumerate(test_cases_params):\n        m, n, seed = params\n        rng = np.random.default_rng(seed)\n\n        if i == 0:  # Case 1: well-conditioned, overdetermined\n            A = rng.standard_normal((m, n))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 1:  # Case 2: underdetermined\n            A = rng.standard_normal((m, n))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 2:  # Case 3: rank-deficient\n            A = rng.standard_normal((m, n))\n            A[:, 10] = A[:, 5]\n            A[:, 15] = A[:, 5]\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 3:  # Case 4: ill-conditioned\n            G = rng.standard_normal((m, n))\n            A = np.zeros_like(G)\n            for j in range(n):\n                A[:, j] = G[:, j] * (10**(-j / 3.0))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n\n        # Regularization path calculation\n        lambda_grid = np.logspace(-10, 6, 60)\n        \n        # SVD-based solution\n        U, s, Vt = svd(A, full_matrices=True)\n        k = len(s)\n        V = Vt.T\n        c = U.T @ b\n\n        x_path = []\n        for lam in lambda_grid:\n            y = np.zeros(n)\n            y[:k] = (s * c[:k]) / (s**2 + lam**2)\n            x_lam = V @ y\n            x_path.append(x_lam)\n\n        x_path_norms = [np.linalg.norm(x) for x in x_path]\n\n        # Verification checks\n        \n        # 1. Monotone shrinkage of the coefficient Euclidean norm\n        is_monotone = True\n        for k_idx in range(1, len(x_path_norms)):\n            norm_prev = x_path_norms[k_idx - 1]\n            norm_curr = x_path_norms[k_idx]\n            # Since lambda grid is increasing, norm should be non-increasing\n            tolerance = 1e-12 * max(1, norm_prev)\n            if norm_curr > norm_prev + tolerance:\n                is_monotone = False\n                break\n        \n        # 2. Near-agreement with least squares at small lambda\n        x_lsq = np.linalg.pinv(A) @ b\n        norm_x_lsq = np.linalg.norm(x_lsq)\n        x_lambda_min = x_path[0]\n        \n        rel_diff_small_lambda = np.linalg.norm(x_lambda_min - x_lsq) / max(1, norm_x_lsq)\n        agrees_at_small_lambda = rel_diff_small_lambda  1e-6\n        \n        # 3. Near-zero coefficients at very large lambda\n        x_lambda_max = x_path[-1]\n        norm_x_lambda_max = np.linalg.norm(x_lambda_max)\n        \n        ratio_large_lambda = norm_x_lambda_max / max(1, norm_x_lsq)\n        zero_at_large_lambda = ratio_large_lambda  1e-6\n\n        all_results.extend([is_monotone, agrees_at_small_lambda, zero_at_large_lambda])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The SVD provides a powerful, general-purpose solver, but in computational science, we always seek advantages by exploiting special problem structures. This advanced practice  explores such a case, where the system matrix is a low-rank update to the identity, i.e., $A = I + u v^{\\top}$. By applying the Sherman–Morrison–Woodbury identity, you will derive and implement a highly efficient algorithm that avoids forming and inverting large matrices, replacing them with small-scale vector and matrix operations. This exercise highlights how deep knowledge of linear algebra can lead to significant gains in computational speed and stability, a crucial skill for tackling large-scale inverse problems.",
            "id": "3283854",
            "problem": "You are given a square system matrix of the form $A = I + u v^{\\top}$ with vectors $u, v \\in \\mathbb{R}^{n}$. When the inner product $u^{\\top} v$ is close to $-1$, the matrix $A$ becomes nearly singular, which can cause numerical instability when inverting or solving linear systems with $A$. To handle this, consider the Tikhonov regularized least-squares problem: find $x_{\\alpha} \\in \\mathbb{R}^{n}$ that minimizes $\\lVert A x - b \\rVert_{2}^{2} + \\alpha \\lVert x \\rVert_{2}^{2}$ for a regularization parameter $\\alpha \\ge 0$. This solution satisfies the normal equations $(A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b$.\n\nStarting only from core definitions and well-tested facts, derive a computationally stable formula for $x_{\\alpha}$ that exploits the low-rank structure of $A^{\\top} A + \\alpha I$ and avoids explicitly inverting large matrices:\n\n- Use the normal equations $(A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b$ as the defining relation for the Tikhonov regularized solution.\n- Use the matrix inversion lemma (also known as the Sherman–Morrison–Woodbury identity) as a foundational tool: for an invertible matrix $C \\in \\mathbb{R}^{n \\times n}$ and matrices $U, V \\in \\mathbb{R}^{n \\times k}$, $(C + U V^{\\top})^{-1} = C^{-1} - C^{-1} U (I_{k} + V^{\\top} C^{-1} U)^{-1} V^{\\top} C^{-1}$. Do not assume any other specialized formulas.\n- Express your final algorithm solely in terms of scalar inner products, small fixed-size matrix inversions, and matrix–vector products with $u$ and $v$. Avoid forming dense $n \\times n$ matrices whenever a lower-dimensional computation suffices.\n\nThen implement a program that, for each test case in the suite below, computes $x_{\\alpha}$ in two ways:\n- Method $1$ (direct): Solve $(A^{\\top} A + \\alpha I) x = A^{\\top} b$ using a dense linear solver.\n- Method $2$ (structure-exploiting): Apply your derived low-rank, matrix-inversion-lemma-based formula to compute $(A^{\\top} A + \\alpha I)^{-1} A^{\\top} b$ without forming large inverses.\n\nFor each test case, report the maximum absolute difference between the two computed vectors $x_{\\alpha}$, that is, $\\max_{i} |(x_{\\alpha}^{(1)})_{i} - (x_{\\alpha}^{(2)})_{i}|$, as a single floating-point number.\n\nNo physical units or angle units are involved. All outputs must be real numbers.\n\nTest suite (use exactly these values):\n- Dimension: $n = 5$.\n- Fixed vector $u = [1, -2, 3, -4, 5]^{\\top}$.\n- Base right-hand side $b_{0} = [2, -1, 0.5, 1.5, -3]^{\\top}$.\n- For each test case, construct $v$ from a specified $\\delta$ via $v = c u$ with $c = \\dfrac{-1 + \\delta}{\\lVert u \\rVert_{2}^{2}}$. This ensures $u^{\\top} v = -1 + \\delta$.\n- Define the five cases as tuples $(\\delta, \\alpha, b)$:\n  1. $(0.2, 0, b_{0})$,\n  2. $(10^{-8}, 10^{-8}, b_{0})$,\n  3. $(10^{-12}, 10^{-4}, b_{0})$,\n  4. $(0, 1, b_{0})$,\n  5. $(10^{-14}, 10^{-6}, 0 \\cdot b_{0})$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above, for example, $[r_{1}, r_{2}, r_{3}, r_{4}, r_{5}]$, where each $r_{i}$ is the maximum absolute difference for test case $i$ computed as described.",
            "solution": "The user wants me to solve a problem on Tikhonov regularization.\n\n### Step 1: Extract Givens\n- **Matrix form**: $A = I + u v^{\\top}$ where $u, v \\in \\mathbb{R}^{n}$ are vectors and $I$ is the $n \\times n$ identity matrix.\n- **Problem condition**: The inner product $u^{\\top} v$ is close to $-1$.\n- **Problem to solve**: Minimize $\\lVert A x - b \\rVert_{2}^{2} + \\alpha \\lVert x \\rVert_{2}^{2}$ for $\\alpha \\ge 0$.\n- **Defining relation**: The solution $x_{\\alpha}$ satisfies the normal equations $(A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b$.\n- **Tool**: The Sherman–Morrison–Woodbury (SMW) identity: $(C + U V^{\\top})^{-1} = C^{-1} - C^{-1} U (I_{k} + V^{\\top} C^{-1} U)^{-1} V^{\\top} C^{-1}$ for an invertible matrix $C \\in \\mathbb{R}^{n \\times n}$ and matrices $U, V \\in \\mathbb{R}^{n \\times k}$.\n- **Final expression requirement**: The formula for $x_{\\alpha}$ must be expressed in terms of scalar inner products, small fixed-size matrix inversions, and matrix–vector products with $u$ and $v$.\n- **Test cases**:\n    - Dimension: $n = 5$.\n    - Fixed vector $u = [1, -2, 3, -4, 5]^{\\top}$.\n    - Base right-hand side $b_{0} = [2, -1, 0.5, 1.5, -3]^{\\top}$.\n    - Vector $v$ is constructed from a parameter $\\delta$ as $v = c u$ where $c = \\dfrac{-1 + \\delta}{\\lVert u \\rVert_{2}^{2}}$. This ensures $u^{\\top} v = -1 + \\delta$.\n    - Five test cases $(\\delta, \\alpha, b)$:\n      1. $(0.2, 0, b_{0})$\n      2. $(10^{-8}, 10^{-8}, b_{0})$\n      3. $(10^{-12}, 10^{-4}, b_{0})$\n      4. $(0, 1, b_{0})$\n      5. $(10^{-14}, 10^{-6}, 0 \\cdot b_{0})$\n- **Task**: Implement two methods to find $x_{\\alpha}$:\n    1. A direct method solving the dense system $(A^{\\top} A + \\alpha I) x = A^{\\top} b$.\n    2. A structure-exploiting method using the derived formula.\n- **Output**: For each test case, report the maximum absolute difference between the solutions from the two methods.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is validated against the specified criteria:\n- **Scientifically Grounded**: The problem is rooted in numerical linear algebra, specifically concerning the solution of ill-conditioned linear systems using Tikhonov regularization. The use of the Sherman-Morrison-Woodbury identity is a standard and well-established technique for rank-structured matrices. All concepts are mathematically sound.\n- **Well-Posed**: The problem asks for a specific derivation and its implementation. The existence and uniqueness of the Tikhonov regularized solution $x_{\\alpha}$ are guaranteed for $\\alpha  0$ or if $A^{\\top}A$ is invertible. The problem structure is clear and leads to a definite solution.\n- **Objective**: The problem is stated using precise mathematical language and definitions. All quantities and objectives are unambiguous.\n- **Completeness**: The problem provides all necessary information, including the matrix structure, the equation to solve, the tool to use (SMW), and a complete set of test data.\n- **No other flaws detected**: The problem is not metaphorical, trivial, or unverifiable. It's a standard exercise in numerical methods.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Derivation of the Structure-Exploiting Formula\n\nThe objective is to derive a computationally efficient formula for the Tikhonov-regularized solution $x_{\\alpha}$, which is defined by the normal equations:\n$$ (A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b $$\nThis can be written as $x_{\\alpha} = (A^{\\top} A + \\alpha I)^{-1} A^{\\top} b$. Let the matrix to be inverted be $M = A^{\\top} A + \\alpha I$. We will use the Sherman-Morrison-Woodbury (SMW) identity to find $M^{-1}$ by exploiting the low-rank structure of $A$.\n\nThe matrix $A$ is given as $A = I + u v^{\\top}$. Its transpose is $A^{\\top} = I + v u^{\\top}$. We first expand the term $A^{\\top} A$:\n$$ A^{\\top} A = (I + v u^{\\top})(I + u v^{\\top}) = I \\cdot I + I u v^{\\top} + v u^{\\top} I + v u^{\\top} u v^{\\top} $$\n$$ A^{\\top} A = I + u v^{\\top} + v u^{\\top} + v (u^{\\top} u) v^{\\top} $$\nLet $s_{uu} = u^{\\top} u$ be the scalar result of the inner product. Then:\n$$ A^{\\top} A = I + u v^{\\top} + v u^{\\top} + s_{uu} v v^{\\top} $$\nNow, we substitute this into the expression for $M$:\n$$ M = A^{\\top} A + \\alpha I = (1 + \\alpha)I + u v^{\\top} + v u^{\\top} + s_{uu} v v^{\\top} $$\nTo apply the SMW formula, we need to express the update part, $u v^{\\top} + v u^{\\top} + s_{uu} v v^{\\top}$, in the form $UV^{\\top}$. We can group the terms as follows:\n$$ u v^{\\top} + v u^{\\top} + s_{uu} v v^{\\top} = u v^{\\top} + v(u^{\\top} + s_{uu}v^{\\top}) = u v^{\\top} + v(u + s_{uu}v)^{\\top} $$\nThis expression is a sum of two outer products. Let us define two column vectors $u_1=u$ and $u_2=v$, and two other column vectors $v_1=v$ and $v_2=u+s_{uu}v$. Then the expression is $u_1 v_1^{\\top} + u_2 v_2^{\\top}$. This can be written in matrix form as $[u_1, u_2] [v_1, v_2]^{\\top}$.\nSo, we can write $M$ as:\n$$ M = C + U_{smw} V_{smw}^{\\top} $$\nwhere:\n- $C = (1 + \\alpha)I$, which is an invertible $n \\times n$ matrix for $\\alpha \\ge 0$.\n- $U_{smw} = [u, v]$, an $n \\times 2$ matrix.\n- $V_{smw} = [v, u + s_{uu}v]$, an $n \\times 2$ matrix.\n\nWe apply the SMW formula, $(C + U_{smw}V_{smw}^{\\top})^{-1} = C^{-1} - C^{-1}U_{smw}(I_2 + V_{smw}^{\\top}C^{-1}U_{smw})^{-1}V_{smw}^{\\top}C^{-1}$, with $k=2$.\nSubstituting $C^{-1} = \\frac{1}{1+\\alpha}I$:\n$$ M^{-1} = \\frac{1}{1+\\alpha}I - \\left(\\frac{1}{1+\\alpha}I\\right) U_{smw} \\left(I_2 + V_{smw}^{\\top}\\left(\\frac{1}{1+\\alpha}I\\right)U_{smw}\\right)^{-1} V_{smw}^{\\top} \\left(\\frac{1}{1+\\alpha}I\\right) $$\n$$ M^{-1} = \\frac{1}{1+\\alpha}I - \\frac{1}{(1+\\alpha)^2} U_{smw} \\left(I_2 + \\frac{1}{1+\\alpha}V_{smw}^{\\top}U_{smw}\\right)^{-1} V_{smw}^{\\top} $$\nLet $K = V_{smw}^{\\top}U_{smw}$. This is a $2 \\times 2$ matrix whose elements are inner products:\n$$ K = \\begin{pmatrix} v^{\\top}u  v^{\\top}v \\\\ (u+s_{uu}v)^{\\top}u  (u+s_{uu}v)^{\\top}v \\end{pmatrix} = \\begin{pmatrix} u^{\\top}v  v^{\\top}v \\\\ u^{\\top}u+s_{uu}(v^{\\top}u)  u^{\\top}v+s_{uu}(v^{\\top}v) \\end{pmatrix} $$\nThe solution $x_{\\alpha}$ is given by $x_{\\alpha} = M^{-1} (A^{\\top} b)$:\n$$ x_{\\alpha} = \\left( \\frac{1}{1+\\alpha}I - \\frac{1}{(1+\\alpha)^2} U_{smw} \\left(I_2 + \\frac{1}{1+\\alpha}K\\right)^{-1} V_{smw}^{\\top} \\right) (A^{\\top}b) $$\nWe can distribute $A^{\\top}b$ to obtain the final algorithmic formula:\n$$ x_{\\alpha} = \\frac{1}{1+\\alpha}A^{\\top}b - \\frac{1}{(1+\\alpha)^2} U_{smw} \\left(I_2 + \\frac{1}{1+\\alpha}K\\right)^{-1} (V_{smw}^{\\top}A^{\\top}b) $$\nThis formulation avoids forming any large $n \\times n$ matrices other than the identity. The main computational tasks are:\n1.  Compute the vector $A^{\\top}b = (I + v u^{\\top})b = b + (u^{\\top}b)v$. This requires one inner product ($u^{\\top}b$), one scalar-vector multiplication, and one vector addition.\n2.  Compute the scalar inner products needed for the $2 \\times 2$ matrix $K$.\n3.  Form and invert the $2 \\times 2$ matrix $M_{2\\times2} = I_2 + \\frac{1}{1+\\alpha}K$.\n4.  Compute the $2 \\times 1$ vector $z=V_{smw}^{\\top}(A^{\\top}b)$. This involves two inner products with the already computed vector $A^{\\top}b$.\n5.  Assemble the final solution $x_{\\alpha}$ using vector scaling and addition. Let $\\gamma = M_{2\\times2}^{-1}z$. The term $U_{smw}\\gamma$ is a linear combination of the vectors $u$ and $v$: $U_{smw}\\gamma = \\gamma_1 u + \\gamma_2 v$.\n\nThis procedure is computationally efficient, with a complexity dominated by vector operations (scaling as $O(n)$), in contrast to the direct solution of the $n \\times n$ system which scales as $O(n^3)$. It is also numerically more stable as it avoids the explicit formation of $A^{\\top}A$, which can have a much larger condition number than $A$.",
            "answer": "```python\nimport numpy as np\n\ndef solve_direct(A, b, alpha):\n    \"\"\"\n    Method 1: Solve (A^T A + alpha*I)x = A^T b using a dense linear solver.\n    \"\"\"\n    n = A.shape[0]\n    I = np.identity(n)\n    \n    A_T_A = A.T @ A\n    \n    # System matrix and right-hand side\n    system_matrix = A_T_A + alpha * I\n    rhs_vector = A.T @ b\n    \n    # Solve the linear system\n    try:\n        x1 = np.linalg.solve(system_matrix, rhs_vector)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if solve fails (e.g., for alpha=0 and singular matrix)\n        x1 = np.linalg.pinv(system_matrix) @ rhs_vector\n        \n    return x1\n\ndef solve_smw(u, v, b, alpha):\n    \"\"\"\n    Method 2: Apply the derived low-rank, SMW-based formula.\n    \"\"\"\n    n = u.shape[0]\n    I = np.identity(n)\n    I2 = np.identity(2)\n\n    # 1. Compute scalar products and related values\n    s_uu = u.T @ u\n    s_uv = u.T @ v\n    s_vv = v.T @ v\n    u_b = u.T @ b\n\n    # 2. Compute vectors needed for the formula\n    A_T_b = b + v * u_b\n    w = u + s_uu * v\n\n    # 3. Form the 2x2 matrix K\n    # K = V_smw.T @ U_smw, where U_smw=[u,v], V_smw=[v,w]\n    K = np.array([\n        [s_uv, s_vv],\n        [w.T @ u, w.T @ v]\n    ])\n\n    # 4. Invert the 2x2 matrix M_2x2 = I2 + (1/(1+alpha)) * K\n    inv_1_plus_alpha = 1.0 / (1.0 + alpha)\n    M_2x2 = I2 + inv_1_plus_alpha * K\n    M_2x2_inv = np.linalg.inv(M_2x2)\n\n    # 5. Compute the 2x1 vector z = V_smw.T @ A_T_b\n    z = np.array([v.T @ A_T_b, w.T @ A_T_b])\n\n    # 6. Compute gamma = M_2x2_inv @ z\n    gamma = M_2x2_inv @ z\n\n    # 7. Assemble the final solution x_alpha\n    # U_smw @ gamma = gamma[0]*u + gamma[1]*v\n    U_smw_gamma = gamma[0] * u + gamma[1] * v\n    \n    term1 = inv_1_plus_alpha * A_T_b\n    term2 = (inv_1_plus_alpha**2) * U_smw_gamma\n    \n    x2 = term1 - term2\n    \n    return x2\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute differences.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    n = 5\n    u = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n    b0 = np.array([2.0, -1.0, 0.5, 1.5, -3.0])\n    \n    test_cases = [\n        (0.2, 0.0, b0),\n        (1e-8, 1e-8, b0),\n        (1e-12, 1e-4, b0),\n        (0.0, 1.0, b0),\n        (1e-14, 1e-6, 0.0 * b0),\n    ]\n\n    results = []\n\n    s_uu = u.T @ u\n\n    for delta, alpha, b in test_cases:\n        # Construct v based on delta\n        c = (-1.0 + delta) / s_uu\n        v = c * u\n        \n        # Form matrix A\n        A = np.identity(n) + np.outer(u, v)\n\n        # Method 1: Direct solver\n        x1 = solve_direct(A, b, alpha)\n        \n        # Method 2: SMW-based formula\n        x2 = solve_smw(u, v, b, alpha)\n\n        # Calculate the maximum absolute difference\n        max_abs_diff = np.max(np.abs(x1 - x2))\n        results.append(max_abs_diff)\n    \n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}