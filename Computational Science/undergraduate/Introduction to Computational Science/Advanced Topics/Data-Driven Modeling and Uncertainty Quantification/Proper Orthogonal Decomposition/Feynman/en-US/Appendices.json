{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp Proper Orthogonal Decomposition, we begin with a foundational 'pen-and-paper' exercise. This practice bridges the gap between abstract theory and a concrete solution by having you derive the POD modes analytically for a simple, continuous space-time field. By working through the definition of the spatial correlation operator and solving its eigenproblem, you will gain a deeper intuition for what POD modes represent before we move to numerical implementations .",
            "id": "3265890",
            "problem": "Consider the spatial domain $x \\in [0,1]$ with the spatial inner product given by $\\langle f, g \\rangle = \\int_{0}^{1} f(x) g(x) \\, dx$, and the temporal ensemble average defined by the long-time mean $\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} (\\cdot) \\, dt$. Let the space-time field be\n$$\nu(x,t) = \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t),\n$$\nwhere $\\omega_{1} > 0$ and $\\omega_{2} > 0$ are distinct real constants. Using the foundational definition of Proper Orthogonal Decomposition (POD), where the spatial POD modes are the eigenfunctions of the spatial correlation operator\n$$\n\\mathcal{C}[\\phi](x) = \\int_{0}^{1} C(x,y) \\, \\phi(y) \\, dy,\n$$\nconstructed from the correlation kernel\n$$\nC(x,y) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} u(x,t) \\, u(y,t) \\, dt,\n$$\nderive the analytical spatial POD modes associated with $u(x,t)$. Express the modes as $L^{2}([0,1])$-normalized functions. Your final answer must list the two normalized spatial POD modes in a single row matrix. No units are required, and no rounding is needed.",
            "solution": "The problem requires the derivation of the spatial Proper Orthogonal Decomposition (POD) modes for a given space-time field $u(x,t)$. By definition, the spatial POD modes, denoted by $\\phi(x)$, are the eigenfunctions of the spatial two-point correlation operator $\\mathcal{C}$, which is an integral operator with kernel $C(x,y)$. The eigenproblem is given by:\n$$\n\\mathcal{C}[\\phi](x) = \\int_{0}^{1} C(x,y) \\, \\phi(y) \\, dy = \\lambda \\phi(x)\n$$\nThe kernel $C(x,y)$ is defined as the time-average of the product of the field at two spatial locations, $x$ and $y$.\n$$\nC(x,y) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} u(x,t) \\, u(y,t) \\, dt\n$$\nThe first step is to compute this kernel for the given field $u(x,t) = \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t)$.\n\nLet's expand the product $u(x,t) \\, u(y,t)$:\n\\begin{align*}\nu(x,t) \\, u(y,t) = & \\left[ \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t) \\right] \\left[ \\sin(2\\pi y) \\cos(\\omega_{1} t) + \\sin(3\\pi y) \\cos(\\omega_{2} t) \\right] \\\\\n= & \\sin(2\\pi x) \\sin(2\\pi y) \\cos^2(\\omega_{1} t) \\\\\n& + \\sin(3\\pi x) \\sin(3\\pi y) \\cos^2(\\omega_{2} t) \\\\\n& + \\sin(2\\pi x) \\sin(3\\pi y) \\cos(\\omega_{1} t) \\cos(\\omega_{2} t) \\\\\n& + \\sin(3\\pi x) \\sin(2\\pi y) \\cos(\\omega_{1} t) \\cos(\\omega_{2} t)\n\\end{align*}\nTo find $C(x,y)$, we must compute the long-time average of the temporal components. We need the following standard time-average results for harmonic functions:\n$1$. For any non-zero frequency $\\omega > 0$,\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega t) \\, dt = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\frac{1 + \\cos(2\\omega t)}{2} \\, dt = \\lim_{T \\to \\infty} \\frac{1}{T} \\left[ \\frac{t}{2} + \\frac{\\sin(2\\omega t)}{4\\omega} \\right]_{0}^{T} = \\frac{1}{2}\n$$\n$2$. For two distinct positive frequencies $\\omega_1 \\neq \\omega_2$,\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos(\\omega_1 t) \\cos(\\omega_2 t) \\, dt = \\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{0}^{T} \\left[ \\cos((\\omega_1 - \\omega_2)t) + \\cos((\\omega_1 + \\omega_2)t) \\right] dt = 0\n$$\nThe second result holds because the integral of a cosine function over a period is zero, and its indefinite integral is a sine function, which is bounded. Dividing by $T \\to \\infty$ makes the average tend to $0$.\n\nApplying these time averages to the expanded product, the cross-terms involving $\\cos(\\omega_{1} t) \\cos(\\omega_{2} t)$ average to zero. We are left with:\n\\begin{align*}\nC(x,y) &= \\sin(2\\pi x) \\sin(2\\pi y) \\left( \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega_{1} t) \\, dt \\right) + \\sin(3\\pi x) \\sin(3\\pi y) \\left( \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega_{2} t) \\, dt \\right) \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y)\n\\end{align*}\nThis is a separable kernel of rank $2$. The eigenfunctions of an operator with such a kernel must lie in the span of the functions that constitute the kernel, i.e., $\\text{span}\\{\\sin(2\\pi x), \\sin(3\\pi x)\\}$.\n\nLet's check if the basis functions $\\psi_1(x) = \\sin(2\\pi x)$ and $\\psi_2(x) = \\sin(3\\pi x)$ are orthogonal under the given inner product $\\langle f, g \\rangle = \\int_{0}^{1} f(x) g(x) \\, dx$:\n$$\n\\langle \\psi_1, \\psi_2 \\rangle = \\int_{0}^{1} \\sin(2\\pi x) \\sin(3\\pi x) \\, dx = \\frac{1}{2} \\int_{0}^{1} \\left[ \\cos(\\pi x) - \\cos(5\\pi x) \\right] dx = \\frac{1}{2} \\left[ \\frac{\\sin(\\pi x)}{\\pi} - \\frac{\\sin(5\\pi x)}{5\\pi} \\right]_{0}^{1} = 0\n$$\nSince the spatial functions are orthogonal, they are indeed the eigenfunctions of the correlation operator $\\mathcal{C}$. We can verify this by substituting them into the eigenproblem.\n\nFor the first eigenfunction candidate $\\phi(x) = \\psi_1(x) = \\sin(2\\pi x)$:\n\\begin{align*}\n\\mathcal{C}[\\psi_1](x) &= \\int_{0}^{1} \\left[ \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y) \\right] \\sin(2\\pi y) \\, dy \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\int_{0}^{1} \\sin^2(2\\pi y) \\, dy + \\frac{1}{2} \\sin(3\\pi x) \\int_{0}^{1} \\sin(3\\pi y) \\sin(2\\pi y) \\, dy\n\\end{align*}\nThe second integral is zero due to orthogonality. The first integral is:\n$$\n\\int_{0}^{1} \\sin^2(2\\pi y) \\, dy = \\int_{0}^{1} \\frac{1 - \\cos(4\\pi y)}{2} \\, dy = \\left[ \\frac{y}{2} - \\frac{\\sin(4\\pi y)}{8\\pi} \\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, $\\mathcal{C}[\\psi_1](x) = \\frac{1}{2} \\sin(2\\pi x) \\cdot \\frac{1}{2} = \\frac{1}{4} \\sin(2\\pi x)$.\nSo, $\\phi_1(x) = \\sin(2\\pi x)$ is an eigenfunction with eigenvalue $\\lambda_1 = \\frac{1}{4}$.\n\nFor the second eigenfunction candidate $\\phi(x) = \\psi_2(x) = \\sin(3\\pi x)$:\n\\begin{align*}\n\\mathcal{C}[\\psi_2](x) &= \\int_{0}^{1} \\left[ \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y) \\right] \\sin(3\\pi y) \\, dy \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\int_{0}^{1} \\sin(2\\pi y) \\sin(3\\pi y) \\, dy + \\frac{1}{2} \\sin(3\\pi x) \\int_{0}^{1} \\sin^2(3\\pi y) \\, dy\n\\end{align*}\nThe first integral is zero. The second integral is:\n$$\n\\int_{0}^{1} \\sin^2(3\\pi y) \\, dy = \\int_{0}^{1} \\frac{1 - \\cos(6\\pi y)}{2} \\, dy = \\left[ \\frac{y}{2} - \\frac{\\sin(6\\pi y)}{12\\pi} \\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, $\\mathcal{C}[\\psi_2](x) = \\frac{1}{2} \\sin(3\\pi x) \\cdot \\frac{1}{2} = \\frac{1}{4} \\sin(3\\pi x)$.\nSo, $\\phi_2(x) = \\sin(3\\pi x)$ is an eigenfunction with eigenvalue $\\lambda_2 = \\frac{1}{4}$.\n\nThe final step is to normalize these eigenfunctions to have unit $L^2$-norm. The squared norm of an eigenfunction $\\phi$ is $\\|\\phi\\|^2 = \\langle \\phi, \\phi \\rangle = \\int_{0}^{1} \\phi(x)^2 \\, dx$.\nFor the first mode, $\\sin(2\\pi x)$:\n$$\n\\|\\sin(2\\pi x)\\|^2 = \\int_{0}^{1} \\sin^2(2\\pi x) \\, dx = \\frac{1}{2}\n$$\nThe normalization constant is $1 / \\sqrt{1/2} = \\sqrt{2}$. The first normalized POD mode is $\\hat{\\phi_1}(x) = \\sqrt{2} \\sin(2\\pi x)$.\n\nFor the second mode, $\\sin(3\\pi x)$:\n$$\n\\|\\sin(3\\pi x)\\|^2 = \\int_{0}^{1} \\sin^2(3\\pi x) \\, dx = \\frac{1}{2}\n$$\nThe normalization constant is also $\\sqrt{2}$. The second normalized POD mode is $\\hat{\\phi_2}(x) = \\sqrt{2} \\sin(3\\pi x)$.\n\nThe two non-trivial spatial POD modes are therefore $\\sqrt{2} \\sin(2\\pi x)$ and $\\sqrt{2} \\sin(3\\pi x)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{2}\\sin(2\\pi x) & \\sqrt{2}\\sin(3\\pi x)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having established the theoretical foundation, we now transition to a computational setting. This hands-on coding practice tasks you with implementing POD for a series of data snapshots generated from a translating Gaussian pulse. This classic example is not only an excellent exercise in applying the Singular Value Decomposition (SVD) for data compression but also powerfully illustrates a key limitation of POD in efficiently representing advection-dominated systems .",
            "id": "3265968",
            "problem": "Consider the family of snapshot functions of a translating Gaussian pulse defined by $u(x,t) = \\exp\\!\\left(-\\big(x - c t\\big)^{2}\\right)$ on the spatial interval $x \\in [-L,L]$ and discrete times $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$. From first principles, Proper Orthogonal Decomposition (POD) is the procedure that, for a given rank $r$, selects an $r$-dimensional orthonormal basis in space that minimizes the total squared projection error of the snapshot set. Equivalently, it produces the best rank-$r$ approximation of the snapshot data (in the Euclidean least-squares sense across all grid points and times).\n\nYour task is to implement a program that:\n- Constructs the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$ whose $k$-th column is the sampled snapshot $u(x,t_k)$ at $N_x$ uniformly spaced grid points in $[-L,L]$, for a given speed $c$, number of snapshots $m$, and final time $T$ with $t_k$ equally spaced in $[0,T]$.\n- Computes, for ranks $r \\in \\{1,2,5,10\\}$, the best rank-$r$ approximation $X_r$ (as defined by POD) and the corresponding relative reconstruction error\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F},$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n- Reports the errors $E_r$ for each test case as floating-point numbers rounded to six decimal places.\n\nFundamental base to use:\n- Definitions of Euclidean inner product and Frobenius norm.\n- The defining optimization property of Proper Orthogonal Decomposition (POD): among all $r$-dimensional orthonormal bases, POD minimizes the total squared projection error of the snapshots. This yields the best rank-$r$ approximation of the snapshot matrix in the least-squares sense.\n\nTest suite:\nUse $L = 10$ and $N_x = 401$ for all cases. The four test cases are:\n1. Case A (stationary pulse): $c = 0$, $T = 5$, $m = 50$.\n2. Case B (slow translation): $c = 0.5$, $T = 10$, $m = 100$.\n3. Case C (fast translation): $c = 2.0$, $T = 4$, $m = 80$.\n4. Case D (few snapshots): $c = 0.5$, $T = 10$, $m = 5$.\n\nAnswer specification:\n- For each test case, output a list $[E_{1},E_{2},E_{5},E_{10}]$ of four floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed list of four errors for one test case, in the order of Cases A, B, C, D. For example, an output of the correct format would look like\n$[[e_{A,1},e_{A,2},e_{A,5},e_{A,10}],[e_{B,1},e_{B,2},e_{B,5},e_{B,10}],[e_{C,1},e_{C,2},e_{C,5},e_{C,10}],[e_{D,1},e_{D,2},e_{D,5},e_{D,10}]]$,\nwith no spaces anywhere in the line.\n\nUnits:\n- There are no physical units required in this problem.\n\nAngle units:\n- Not applicable.\n\nPercentages:\n- Not applicable; express all quantities as decimals.\n\nYour implementation must be self-contained and require no user input, external files, or network access. It must run in a modern programming language and produce the exact final output format described above in a single line.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Snapshot function**: $u(x,t) = \\exp(-(x - c t)^2)$\n- **Spatial domain**: $x \\in [-L,L]$\n- **Time domain**: $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$, which are $m$ equally spaced points in $[0,T]$.\n- **Spatial discretization**: $N_x$ uniformly spaced grid points in $[-L,L]$.\n- **Snapshot matrix**: $X \\in \\mathbb{R}^{N_x \\times m}$, where the $k$-th column is the sampled snapshot $u(x,t_k)$.\n- **Task**: Compute the best rank-$r$ approximation $X_r$ using Proper Orthogonal Decomposition (POD) for ranks $r \\in \\{1, 2, 5, 10\\}$.\n- **Error metric**: Relative reconstruction error $E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F}$, where $\\lVert \\cdot \\rVert_F$ is the Frobenius norm.\n- **Constants**: $L = 10$, $N_x = 401$.\n- **Test cases**:\n    1. Case A: $c = 0$, $T = 5$, $m = 50$.\n    2. Case B: $c = 0.5$, $T = 10$, $m = 100$.\n    3. Case C: $c = 2.0$, $T = 4$, $m = 80$.\n    4. Case D: $c = 0.5$, $T = 10$, $m = 5$.\n- **Output specification**: A single-line comma-separated list of lists, e.g., `[[e_{A,1},...,e_{A,10}],[e_{B,1},...,e_{B,10}],...]`, with numbers rounded to six decimal places and no spaces.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard application of Proper Orthogonal Decomposition (POD), a cornerstone of reduced-order modeling in scientific computing. The mathematical foundation is the Singular Value Decomposition (SVD), which is a principal result in linear algebra. The physics is a simple translating Gaussian pulse, which is a common and valid test case. The problem is scientifically sound.\n- **Well-Posed**: All necessary parameters ($L, N_x, c, T, m$) are provided for each case. The function $u(x,t)$, the procedure for constructing the snapshot matrix $X$, and the error metric $E_r$ are all defined unambiguously. The SVD provides a unique construction for the best rank-$r$ approximation, ensuring a unique solution exists.\n- **Objective**: The problem is expressed in precise mathematical terms, devoid of any subjectivity, ambiguity, or opinion-based statements.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined, self-contained, and scientifically sound problem in the field of numerical methods. A complete solution will be provided.\n\n### Principle-Based Solution\nThe objective is to compute the relative reconstruction error for a rank-$r$ approximation of a set of data snapshots. The foundational principle is that the optimal rank-$r$ approximation, in the least-squares sense defined by the Frobenius norm, is obtained via the Singular Value Decomposition (SVD). This result is formally stated by the Eckart-Young-Mirsky theorem.\n\n**1. Snapshot Matrix Construction**\nFirst, we discretize the problem domain. The spatial domain $x \\in [-L, L]$ is sampled at $N_x$ uniformly spaced points, forming the grid $\\{x_j\\}_{j=0}^{N_x-1}$. The time interval $t \\in [0, T]$ is sampled at $m$ discrete, equally spaced points $\\{t_k\\}_{k=0}^{m-1}$. The snapshot data at each time point $t_k$ is a vector in $\\mathbb{R}^{N_x}$ whose entries are given by the function $u(x_j, t_k)$. The collection of these snapshots forms the columns of the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$. An element $X_{jk}$ of this matrix is given by:\n$$X_{jk} = u(x_j, t_k) = \\exp\\!\\left(-\\big(x_j - c t_k\\big)^{2}\\right)$$\n\n**2. Singular Value Decomposition and Optimal Approximation**\nThe SVD of the snapshot matrix $X$ is given by:\n$$X = U \\Sigma V^T$$\nwhere $U \\in \\mathbb{R}^{N_x \\times N_x}$ is an orthogonal matrix whose columns $u_i$ are the left-singular vectors (POD modes), $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns $v_i$ are the right-singular vectors, and $\\Sigma \\in \\mathbb{R}^{N_x \\times m}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$. The singular values are non-negative and ordered by convention: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, where $k = \\min(N_x, m)$ is the rank of the matrix.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of $X$ that minimizes the Frobenius norm of the difference, $\\lVert X - X_r \\rVert_F$, is the truncated SVD:\n$$X_r = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\nThis approximation is constructed using the first $r$ singular values and their corresponding left and right singular vectors.\n\n**3. Error Calculation**\nThe relative reconstruction error $E_r$ is defined as the ratio of the Frobenius norm of the error matrix $(X - X_r)$ to the Frobenius norm of the original matrix $X$. The Frobenius norm is related to the singular values by the identity $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2$.\nApplying this property, the squared norm of the original matrix is the sum of the squares of all its singular values:\n$$\\lVert X \\rVert_F^2 = \\sum_{i=1}^{k} \\sigma_i^2$$\nThe error matrix is $X - X_r = \\sum_{i=r+1}^{k} \\sigma_i u_i v_i^T$. Due to the orthogonality of the singular vectors, the squared Frobenius norm of the error matrix is the sum of the squares of the discarded singular values:\n$$\\lVert X - X_r \\rVert_F^2 = \\sum_{i=r+1}^{k} \\sigma_i^2$$\nCombining these results, the relative reconstruction error is given by:\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\sqrt{\\sum_{i=r+1}^{k} \\sigma_i^2}}{\\sqrt{\\sum_{i=1}^{k} \\sigma_i^2}}$$\nNote that if the requested rank $r$ is greater than or equal to the actual rank of the matrix, $k$, the sum in the numerator is empty and evaluates to $0$, correctly yielding an error $E_r = 0$.\n\n**4. Computational Procedure**\nThe algorithm proceeds as follows for each test case:\n1.  Define the parameters $c$, $T$, and $m$, along with the fixed constants $L=10$ and $N_x=401$.\n2.  Construct the spatial grid $x$ and temporal grid $t$.\n3.  Assemble the $N_x \\times m$ snapshot matrix $X$ using the given function $u(x,t)$.\n4.  Compute the singular values $\\sigma_i$ of $X$ using a standard numerical library function for SVD. It is most efficient to compute only the singular values, not the full $U$ and $V$ matrices.\n5.  Calculate the total energy, represented by the squared Frobenius norm, $S_{total} = \\sum_{i=1}^{k} \\sigma_i^2$.\n6.  For each required rank $r \\in \\{1, 2, 5, 10\\}$, calculate the error energy, $S_{error} = \\sum_{i=r+1}^{k} \\sigma_i^2$.\n7.  The relative error is then $E_r = \\sqrt{S_{error} / S_{total}}$.\n8.  The calculated errors for each test case are collected and formatted according to the output specification.\nThis procedure provides a direct and numerically stable method for determining the required reconstruction errors based on fundamental principles of linear algebra.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Proper Orthogonal Decomposition problem for a translating Gaussian pulse.\n    \"\"\"\n    # Global parameters for all test cases\n    L = 10.0\n    Nx = 401\n    ranks_to_compute = [1, 2, 5, 10]\n\n    # Test suite: (c, T, m)\n    # c: speed, T: final time, m: number of snapshots\n    test_cases = [\n        (0.0, 5.0, 50),   # Case A: stationary pulse\n        (0.5, 10.0, 100), # Case B: slow translation\n        (2.0, 4.0, 80),   # Case C: fast translation\n        (0.5, 10.0, 5),   # Case D: few snapshots\n    ]\n\n    all_results = []\n\n    for c, T, m in test_cases:\n        # 1. Create spatial and temporal grids\n        x = np.linspace(-L, L, Nx)\n        t = np.linspace(0.0, T, m)\n\n        # 2. Construct the snapshot matrix X using broadcasting\n        # x_col has shape (Nx, 1) and t_row has shape (1, m)\n        # Broadcasting expands them to (Nx, m) for element-wise operations\n        x_col = x[:, np.newaxis]\n        t_row = t[np.newaxis, :]\n        X = np.exp(-((x_col - c * t_row) ** 2))\n\n        # 3. Compute the singular values of X\n        # We only need the singular values, so compute_uv=False is most efficient.\n        s = np.linalg.svd(X, compute_uv=False)\n        num_singular_values = s.shape[0]\n\n        # 4. Calculate the total energy (squared Frobenius norm of X)\n        # This is the sum of the squares of all singular values.\n        norm_X_sq = np.sum(s**2)\n\n        case_errors = []\n        for r in ranks_to_compute:\n            # 5. Calculate the reconstruction error for rank r\n            \n            # If norm_X_sq is zero, all errors are zero.\n            if norm_X_sq == 0.0:\n                 error = 0.0\n            # If rank r is >= number of singular values, the approximation is perfect.\n            elif r >= num_singular_values:\n                error = 0.0\n            else:\n                # The error norm is based on the truncated singular values (from r to end).\n                # s[r:] corresponds to sigma_{r+1}, sigma_{r+2}, ...\n                norm_err_sq = np.sum(s[r:]**2)\n                error = np.sqrt(norm_err_sq / norm_X_sq)\n            \n            case_errors.append(error)\n\n        all_results.append(case_errors)\n\n    # 6. Format the output string exactly as specified.\n    # e.g., [[err1,err2,...],[err1,err2,...]] with no spaces.\n    formatted_sublists = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places.\n        formatted_numbers = [f\"{err:.6f}\" for err in res_list]\n        # Join numbers with commas and enclose in brackets.\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the sublists with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(formatted_sublists)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice addresses a crucial aspect of real-world applications: computational efficiency. When dealing with high-resolution simulations, the number of spatial grid points often vastly exceeds the number of snapshots, making the direct approach to POD computationally infeasible. This exercise introduces the 'method of snapshots,' an elegant and efficient algorithm that solves a much smaller eigenvalue problem, and has you apply it to compute key diagnostic metrics used to assess the quality of a reduced-order model .",
            "id": "3266011",
            "problem": "You are tasked with implementing Proper Orthogonal Decomposition (POD) using the method of snapshots for a sequence of spatial fields represented on a grid, in the regime where the number of grid points is much larger than the number of snapshots. The method of snapshots proceeds from the fundamental principle that POD seeks an orthonormal basis in the spatial domain that minimizes the mean-squared reconstruction error of the snapshot ensemble under the standard Euclidean inner product. The data are organized into a matrix whose columns are snapshots of the field at different times. The sample mean field across snapshots should be removed prior to constructing the decomposition. The method reduces the dimensionality of the eigenvalue problem by solving it in the snapshot space. All angles used in trigonometric functions must be in radians.\n\nConstruct a program that builds the following test suite of synthetic datasets and computes quantities derived from the POD using the method of snapshots. For each dataset, let $X \\in \\mathbb{R}^{m \\times n}$ denote the data matrix whose columns are snapshots $x^{(j)} \\in \\mathbb{R}^{m}$ sampled on a uniform grid $x_i$ over the interval $[0,1]$. For each dataset, subtract the sample mean field across snapshots, i.e., for each spatial coordinate $i$, subtract the average across $j$ from $X_{i,j}$, yielding a mean-removed matrix $X_c$. Then construct the snapshot-space matrix $S = X_c^{\\top} X_c$. Implement POD by solving the symmetric eigenvalue problem for $S$, ordering eigenvalues from largest to smallest, and forming the corresponding spatial modes. All computations must be deterministic and use only the specified environment. The following datasets define $X$ via explicit formulas; all trigonometric arguments are in radians.\n\n- Test case $1$ (general tall-skinny case): $m = 200$, $n = 5$. Let $x_i = \\frac{i}{m-1}$ for $i \\in \\{0,1,\\dots,m-1\\}$ and $t_j = j$ for $j \\in \\{0,1,\\dots,n-1\\}$. Define spatial patterns $\\phi_1(x) = \\sin(\\pi x)$ and $\\phi_2(x) = \\cos(2\\pi x)$. For each snapshot $j$, set\n$$\nx^{(j)}(x) = 3\\,\\phi_1(x)\\left(1 + 0.5\\cos(0.8 t_j)\\right) + 1.5\\,\\phi_2(x)\\left(0.5\\sin(1.2 t_j)\\right) + 0.05\\,\\sin(5\\pi x)\\cos(2.3 t_j).\n$$\nCompute the minimal number of POD modes required so that the cumulative captured energy (the sum of the leading eigenvalues of $S$ divided by the sum of all eigenvalues of $S$) is at least $0.95$. Output this number as an integer.\n\n- Test case $2$ (dominant first mode): $m = 300$, $n = 3$. Let $x_i = \\frac{i}{m-1}$ and $t_j = j$. Define $\\phi_d(x) = \\sin(\\pi x)$ and $\\phi_s(x) = \\cos(3\\pi x)$. For each $j$, set\n$$\nx^{(j)}(x) = 4\\,\\phi_d(x)\\left(1.1 + 0.9\\sin(0.5 t_j)\\right) + 0.6\\,\\phi_s(x)\\left(\\cos(0.8 t_j)\\right).\n$$\nCompute the fraction of energy captured by the first POD mode as a decimal, defined as the largest eigenvalue of $S$ divided by the sum of all eigenvalues of $S$. Output this value as a float.\n\n- Test case $3$ (reconstruction error with two modes): $m = 50$, $n = 10$. Let $x_i = \\frac{i}{m-1}$ and $t_j = j$. Define $\\phi_1(x) = \\sin(2\\pi x)$, $\\phi_2(x) = \\cos(\\pi x)$, and $\\phi_3(x) = \\sin(4\\pi x)$. For each $j$, set\n$$\nx^{(j)}(x) = 2\\,\\phi_1(x)\\sin(0.7 t_j) + 1.5\\,\\phi_2(x)\\cos(1.1 t_j) + 0.3\\,\\phi_3(x)\\sin(0.9 t_j).\n$$\nUsing the first $2$ POD modes computed from $S$, form the rank-$2$ reconstruction of the mean-removed data by projecting $X_c$ onto the span of the first two spatial modes and compute the relative reconstruction error defined as\n$$\n\\frac{\\lVert X_c - X_{c,2} \\rVert_F}{\\lVert X_c \\rVert_F},\n$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm and $X_{c,2}$ is the projection of $X_c$ onto the first two modes. Output this value as a float.\n\n- Test case $4$ (degenerate snapshots, rank test): $m = 400$, $n = 6$. Let $x_i = \\frac{i}{m-1}$. Construct snapshots explicitly as columns:\n$$\nx^{(0)}(x) = \\sin(\\pi x) + \\cos(2\\pi x), \\quad\nx^{(1)}(x) = 2\\sin(2\\pi x) - 0.5\\cos(3\\pi x), \\quad\nx^{(2)}(x) = x^{(1)}(x), \\\\\nx^{(3)}(x) = 0.7\\sin(3\\pi x) + 0.1\\cos(\\pi x), \\quad\nx^{(4)}(x) = x^{(0)}(x), \\quad\nx^{(5)}(x) = 1.3\\sin(4\\pi x) - 0.9\\cos(2\\pi x).\n$$\nAfter mean removal across snapshots, compute the numerical rank of $X_c$ by counting the number of strictly positive eigenvalues of $S$ using a relative tolerance defined by labeling an eigenvalue as nonzero if it is larger than $10^{-10}$ times the largest eigenvalue of $S$. Output this rank as an integer.\n\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$). The four results must be, in order, the integer number of modes for test case $1$, the float energy fraction for the first mode in test case $2$, the float relative reconstruction error for test case $3$, and the integer rank for test case $4$.",
            "solution": "The problem requires the implementation of Proper Orthogonal Decomposition (POD) via the method of snapshots for several synthetic datasets. The core of this method is to analyze a sequence of snapshots (spatial data fields at different time instances) to find a low-dimensional basis that optimally captures the variance of the data.\n\nLet the data be organized in a matrix $X \\in \\mathbb{R}^{m \\times n}$, where $m$ is the number of spatial grid points and $n$ is the number of snapshots. The $j$-th column of $X$ is the snapshot vector $x^{(j)} \\in \\mathbb{R}^{m}$ at time $t_j$. The problem specifies that the number of grid points is much larger than the number of snapshots, i.e., $m \\gg n$.\n\nThe POD procedure as specified is as follows:\n$1$.  **Data Centering**: The first step is to remove the temporal mean from the snapshots. The mean snapshot is calculated as $\\bar{x} = \\frac{1}{n} \\sum_{j=0}^{n-1} x^{(j)}$. A mean-removed data matrix $X_c$ is formed by subtracting $\\bar{x}$ from each column of $X$. The columns of $X_c$ are $x_c^{(j)} = x^{(j)} - \\bar{x}$.\n$2$.  **Method of Snapshots**: The direct POD approach would require solving an eigenvalue problem for the $m \\times m$ spatial covariance matrix $C = X_c X_c^\\top$. When $m \\gg n$, this is computationally prohibitive. The method of snapshots circumvents this by solving an eigenvalue problem in the smaller snapshot space. We construct the $n \\times n$ snapshot matrix (or Gram matrix) $S = X_c^\\top X_c$.\n$3$.  **Eigenvalue Problem**: We solve the symmetric eigenvalue problem for $S$:\n$$\nS v_k = \\lambda_k v_k, \\quad \\text{for } k = 1, \\dots, n.\n$$\nThe eigenvalues $\\lambda_k$ of $S$ are the non-zero eigenvalues of the larger covariance matrix $C$. These eigenvalues, sorted in descending order $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$, represent the energy or variance captured by each corresponding mode. The total energy (variance) of the mean-centered data is given by the trace of $S$, which equals the sum of its eigenvalues: $\\sum_{k=1}^n \\lambda_k = \\text{tr}(S) = \\lVert X_c \\rVert_F^2$.\n$4$.  **POD Modes**: The spatial POD modes $\\Psi_k \\in \\mathbb{R}^m$ can be constructed from the eigenvectors $v_k$ of $S$ via the relation $\\Psi_k = \\frac{1}{\\sqrt{\\lambda_k}} X_c v_k$. These modes form an orthonormal basis for the spatial data.\n\nWe now apply this framework to the four test cases provided.\n\n**Test Case 1: Cumulative Energy**\nThe dataset is defined with $m = 200$ and $n = 5$. The snapshots are generated on a spatial grid $x_i = \\frac{i}{m-1}$ for $i \\in \\{0, \\dots, 199\\}$ and at times $t_j = j$ for $j \\in \\{0, \\dots, 4\\}$. The snapshot formula is:\n$$\nx^{(j)}(x) = 3 \\sin(\\pi x) \\left(1 + 0.5\\cos(0.8 t_j)\\right) + 0.75 \\cos(2\\pi x) \\sin(1.2 t_j) + 0.05 \\sin(5\\pi x)\\cos(2.3 t_j).\n$$\nAfter constructing the data matrix $X$ and its centered version $X_c$, we compute the snapshot matrix $S = X_c^\\top X_c$ and its eigenvalues $\\lambda_1, \\dots, \\lambda_5$, sorted in descending order. The cumulative energy captured by the first $k$ modes is the ratio of the partial sum of eigenvalues to the total sum:\n$$\nE_k = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^5 \\lambda_i}.\n$$\nWe need to find the smallest integer $k$ for which $E_k \\ge 0.95$. This is done by computing the cumulative sum of the sorted eigenvalues and checking the condition for $k = 1, 2, \\dots, 5$.\n\n**Test Case 2: Dominant Mode Energy**\nThe dataset has $m = 300$ and $n = 3$. The snapshots are generated on a grid $x_i = \\frac{i}{m-1}$ at times $t_j=j$ for $j \\in \\{0, 1, 2\\}$, using the formula:\n$$\nx^{(j)}(x) = 4 \\sin(\\pi x) \\left(1.1 + 0.9\\sin(0.5 t_j)\\right) + 0.6 \\cos(3\\pi x) \\cos(0.8 t_j).\n$$\nFollowing the POD procedure, we compute the eigenvalues $\\lambda_1, \\lambda_2, \\lambda_3$ of the $3 \\times 3$ snapshot matrix $S$. The fraction of energy captured by the first (most dominant) POD mode is given by:\n$$\n\\frac{\\lambda_1}{\\sum_{i=1}^3 \\lambda_i}.\n$$\nThis value will be computed and output as a float.\n\n**Test Case 3: Reconstruction Error**\nThis case uses $m = 50$ and $n = 10$. The snapshots at times $t_j=j$ for $j \\in \\{0, \\dots, 9\\}$ are given by:\n$$\nx^{(j)}(x) = 2 \\sin(2\\pi x) \\sin(0.7 t_j) + 1.5 \\cos(\\pi x) \\cos(1.1 t_j) + 0.3 \\sin(4\\pi x) \\sin(0.9 t_j).\n$$\nWe compute the eigenvalues $\\lambda_1, \\dots, \\lambda_{10}$ of the $10 \\times 10$ matrix $S$. The rank-$k$ POD approximation of $X_c$ is denoted $X_{c,k}$. The squared Frobenius norm of the reconstruction error is the sum of the neglected eigenvalues:\n$$\n\\lVert X_c - X_{c,k} \\rVert_F^2 = \\sum_{i=k+1}^n \\lambda_i.\n$$\nThe total energy, or the squared Frobenius norm of $X_c$, is $\\lVert X_c \\rVert_F^2 = \\sum_{i=1}^n \\lambda_i$. The relative reconstruction error for a rank-$2$ approximation ($k=2$) is therefore:\n$$\n\\text{Error} = \\frac{\\lVert X_c - X_{c,2} \\rVert_F}{\\lVert X_c \\rVert_F} = \\sqrt{\\frac{\\sum_{i=3}^{10} \\lambda_i}{\\sum_{i=1}^{10} \\lambda_i}}.\n$$\nThis value will be computed and output as a float.\n\n**Test Case 4: Numerical Rank**\nThis case has $m = 400$, $n = 6$. The snapshots are defined explicitly, with two pairs of identical snapshots: $x^{(2)} = x^{(1)}$ and $x^{(4)} = x^{(0)}$. These linear dependencies in the columns of $X$ reduce its rank. The mean-centering operation preserves these dependencies: $x_c^{(2)} = x_c^{(1)}$ and $x_c^{(4)} = x_c^{(0)}$. Furthermore, the columns of $X_c$ always sum to the zero vector, introducing another linear dependency. The rank of $X_c$ is therefore at most $n-1-2 = 3$. The rank of $X_c$ is equal to the rank of $S=X_c^\\top X_c$, which is the number of its non-zero eigenvalues. We compute the eigenvalues $\\lambda_1, \\dots, \\lambda_6$ of $S$. The numerical rank is determined by counting how many eigenvalues are strictly positive, using a relative tolerance. An eigenvalue $\\lambda_k$ is considered non-zero if $\\lambda_k > 10^{-10} \\lambda_1$, where $\\lambda_1$ is the largest eigenvalue. We will count the number of eigenvalues satisfying this condition.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef perform_pod_analysis(X):\n    \"\"\"\n    Performs POD analysis on a snapshot matrix X.\n\n    Args:\n        X (np.ndarray): The data matrix of shape (m, n), where m is the number of\n                        spatial points and n is the number of snapshots.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The mean-centered data matrix X_c.\n            - np.ndarray: A 1D array of eigenvalues of the snapshot matrix S,\n                          sorted in descending order.\n    \"\"\"\n    # Step 1: Mean removal\n    mean_field = X.mean(axis=1, keepdims=True)\n    X_c = X - mean_field\n\n    # Step 2: Form the snapshot matrix S\n    S = X_c.T @ X_c\n\n    # Step 3: Solve the eigenvalue problem for S\n    # np.linalg.eigh is used for symmetric matrices and returns eigenvalues in ascending order\n    eigenvalues = np.linalg.eigh(S)[0]\n\n    # Sort eigenvalues in descending order\n    eigenvalues = np.sort(eigenvalues)[::-1]\n    \n    # Ensure eigenvalues are non-negative due to potential floating point inaccuracies\n    eigenvalues[eigenvalues < 0] = 0\n\n    return X_c, eigenvalues\n\ndef solve_case1():\n    \"\"\"Calculates the minimal number of POD modes for >= 95% energy.\"\"\"\n    m = 200\n    n = 5\n    x = np.linspace(0, 1, m)\n    t = np.arange(n)\n\n    phi1 = np.sin(np.pi * x)\n    phi2 = np.cos(2 * np.pi * x)\n    noise_phi = np.sin(5 * np.pi * x)\n\n    time_dep1 = 3 * (1 + 0.5 * np.cos(0.8 * t))\n    time_dep2 = 1.5 * (0.5 * np.sin(1.2 * t))\n    noise_time_dep = 0.05 * np.cos(2.3 * t)\n\n    X = (phi1[:, np.newaxis] * time_dep1[np.newaxis, :] +\n         phi2[:, np.newaxis] * time_dep2[np.newaxis, :] +\n         noise_phi[:, np.newaxis] * noise_time_dep[np.newaxis, :])\n    \n    _, eigenvalues = perform_pod_analysis(X)\n    \n    total_energy = np.sum(eigenvalues)\n    if total_energy == 0:\n        return n\n        \n    cumulative_energy = np.cumsum(eigenvalues) / total_energy\n    \n    num_modes = np.searchsorted(cumulative_energy, 0.95, side='left') + 1\n    return int(num_modes)\n\ndef solve_case2():\n    \"\"\"Calculates the energy fraction of the first POD mode.\"\"\"\n    m = 300\n    n = 3\n    x = np.linspace(0, 1, m)\n    t = np.arange(n)\n\n    phi_d = np.sin(np.pi * x)\n    phi_s = np.cos(3 * np.pi * x)\n\n    time_dep_d = 4 * (1.1 + 0.9 * np.sin(0.5 * t))\n    time_dep_s = 0.6 * np.cos(0.8 * t)\n\n    X = (phi_d[:, np.newaxis] * time_dep_d[np.newaxis, :] +\n         phi_s[:, np.newaxis] * time_dep_s[np.newaxis, :])\n    \n    _, eigenvalues = perform_pod_analysis(X)\n    \n    total_energy = np.sum(eigenvalues)\n    if total_energy == 0:\n        return 0.0\n\n    energy_fraction = eigenvalues[0] / total_energy\n    return float(energy_fraction)\n\ndef solve_case3():\n    \"\"\"Calculates the relative reconstruction error with 2 modes.\"\"\"\n    m = 50\n    n = 10\n    x = np.linspace(0, 1, m)\n    t = np.arange(n)\n\n    phi1 = np.sin(2 * np.pi * x)\n    phi2 = np.cos(np.pi * x)\n    phi3 = np.sin(4 * np.pi * x)\n\n    time_dep1 = 2 * np.sin(0.7 * t)\n    time_dep2 = 1.5 * np.cos(1.1 * t)\n    time_dep3 = 0.3 * np.sin(0.9 * t)\n\n    X = (phi1[:, np.newaxis] * time_dep1[np.newaxis, :] +\n         phi2[:, np.newaxis] * time_dep2[np.newaxis, :] +\n         phi3[:, np.newaxis] * time_dep3[np.newaxis, :])\n\n    _, eigenvalues = perform_pod_analysis(X)\n    \n    num_modes_for_reconstruction = 2\n    \n    total_energy = np.sum(eigenvalues)\n    if total_energy == 0:\n        return 0.0\n        \n    neglected_energy = np.sum(eigenvalues[num_modes_for_reconstruction:])\n    \n    relative_error = np.sqrt(neglected_energy / total_energy)\n    return float(relative_error)\n\ndef solve_case4():\n    \"\"\"Calculates the numerical rank of the mean-centered data.\"\"\"\n    m = 400\n    n = 6\n    x = np.linspace(0, 1, m)\n    X = np.zeros((m, n))\n\n    X[:, 0] = np.sin(np.pi * x) + np.cos(2 * np.pi * x)\n    X[:, 1] = 2 * np.sin(2 * np.pi * x) - 0.5 * np.cos(3 * np.pi * x)\n    X[:, 2] = X[:, 1]\n    X[:, 3] = 0.7 * np.sin(3 * np.pi * x) + 0.1 * np.cos(np.pi * x)\n    X[:, 4] = X[:, 0]\n    X[:, 5] = 1.3 * np.sin(4 * np.pi * x) - 0.9 * np.cos(2 * np.pi * x)\n    \n    _, eigenvalues = perform_pod_analysis(X)\n    \n    if len(eigenvalues) == 0 or eigenvalues[0] == 0:\n        return 0\n        \n    tolerance = 1e-10 * eigenvalues[0]\n    numerical_rank = np.sum(eigenvalues > tolerance)\n    \n    return int(numerical_rank)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    result1 = solve_case1()\n    result2 = solve_case2()\n    result3 = solve_case3()\n    result4 = solve_case4()\n\n    results = [result1, result2, result3, result4]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}