{
    "hands_on_practices": [
        {
            "introduction": "While Proper Orthogonal Decomposition (POD) is typically computed numerically using Singular Value Decomposition, working through an analytical example is invaluable for building intuition. This practice returns to the foundational definition of POD, where modes are derived as the eigenfunctions of the two-point correlation operator. By solving this problem for a simple, synthesized data field , you will gain a deeper appreciation for the mathematical structure that underpins this powerful data reduction technique.",
            "id": "3265890",
            "problem": "Consider the spatial domain $x \\in [0,1]$ with the spatial inner product given by $\\langle f, g \\rangle = \\int_{0}^{1} f(x) g(x) \\, dx$, and the temporal ensemble average defined by the long-time mean $\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} (\\cdot) \\, dt$. Let the space-time field be\n$$\nu(x,t) = \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t),\n$$\nwhere $\\omega_{1} > 0$ and $\\omega_{2} > 0$ are distinct real constants. Using the foundational definition of Proper Orthogonal Decomposition (POD), where the spatial POD modes are the eigenfunctions of the spatial correlation operator\n$$\n\\mathcal{C}[\\phi](x) = \\int_{0}^{1} C(x,y) \\, \\phi(y) \\, dy,\n$$\nconstructed from the correlation kernel\n$$\nC(x,y) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} u(x,t) \\, u(y,t) \\, dt,\n$$\nderive the analytical spatial POD modes associated with $u(x,t)$. Express the modes as $L^{2}([0,1])$-normalized functions. Your final answer must list the two normalized spatial POD modes in a single row matrix. No units are required, and no rounding is needed.",
            "solution": "The problem requires the derivation of the spatial Proper Orthogonal Decomposition (POD) modes for a given space-time field $u(x,t)$. By definition, the spatial POD modes, denoted by $\\phi(x)$, are the eigenfunctions of the spatial two-point correlation operator $\\mathcal{C}$, which is an integral operator with kernel $C(x,y)$. The eigenproblem is given by:\n$$\n\\mathcal{C}[\\phi](x) = \\int_{0}^{1} C(x,y) \\, \\phi(y) \\, dy = \\lambda \\phi(x)\n$$\nThe kernel $C(x,y)$ is defined as the time-average of the product of the field at two spatial locations, $x$ and $y$.\n$$\nC(x,y) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} u(x,t) \\, u(y,t) \\, dt\n$$\nThe first step is to compute this kernel for the given field $u(x,t) = \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t)$.\n\nLet's expand the product $u(x,t) \\, u(y,t)$:\n\\begin{align*}\nu(x,t) \\, u(y,t) = & \\left[ \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t) \\right] \\left[ \\sin(2\\pi y) \\cos(\\omega_{1} t) + \\sin(3\\pi y) \\cos(\\omega_{2} t) \\right] \\\\\n= & \\sin(2\\pi x) \\sin(2\\pi y) \\cos^2(\\omega_{1} t) \\\\\n& + \\sin(3\\pi x) \\sin(3\\pi y) \\cos^2(\\omega_{2} t) \\\\\n& + \\sin(2\\pi x) \\sin(3\\pi y) \\cos(\\omega_{1} t) \\cos(\\omega_{2} t) \\\\\n& + \\sin(3\\pi x) \\sin(2\\pi y) \\cos(\\omega_{1} t) \\cos(\\omega_{2} t)\n\\end{align*}\nTo find $C(x,y)$, we must compute the long-time average of the temporal components. We need the following standard time-average results for harmonic functions:\n$1$. For any non-zero frequency $\\omega > 0$,\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega t) \\, dt = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\frac{1 + \\cos(2\\omega t)}{2} \\, dt = \\lim_{T \\to \\infty} \\frac{1}{T} \\left[ \\frac{t}{2} + \\frac{\\sin(2\\omega t)}{4\\omega} \\right]_{0}^{T} = \\frac{1}{2}\n$$\n$2$. For two distinct positive frequencies $\\omega_1 \\neq \\omega_2$,\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos(\\omega_1 t) \\cos(\\omega_2 t) \\, dt = \\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{0}^{T} \\left[ \\cos((\\omega_1 - \\omega_2)t) + \\cos((\\omega_1 + \\omega_2)t) \\right] dt = 0\n$$\nThe second result holds because the integral of a cosine function over a period is zero, and its indefinite integral is a sine function, which is bounded. Dividing by $T \\to \\infty$ makes the average tend to $0$.\n\nApplying these time averages to the expanded product, the cross-terms involving $\\cos(\\omega_{1} t) \\cos(\\omega_{2} t)$ average to zero. We are left with:\n\\begin{align*}\nC(x,y) &= \\sin(2\\pi x) \\sin(2\\pi y) \\left( \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega_{1} t) \\, dt \\right) + \\sin(3\\pi x) \\sin(3\\pi y) \\left( \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega_{2} t) \\, dt \\right) \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y)\n\\end{align*}\nThis is a separable kernel of rank $2$. The eigenfunctions of an operator with such a kernel must lie in the span of the functions that constitute the kernel, i.e., $\\text{span}\\{\\sin(2\\pi x), \\sin(3\\pi x)\\}$.\n\nLet's check if the basis functions $\\psi_1(x) = \\sin(2\\pi x)$ and $\\psi_2(x) = \\sin(3\\pi x)$ are orthogonal under the given inner product $\\langle f, g \\rangle = \\int_{0}^{1} f(x) g(x) \\, dx$:\n$$\n\\langle \\psi_1, \\psi_2 \\rangle = \\int_{0}^{1} \\sin(2\\pi x) \\sin(3\\pi x) \\, dx = \\frac{1}{2} \\int_{0}^{1} \\left[ \\cos(\\pi x) - \\cos(5\\pi x) \\right] dx = \\frac{1}{2} \\left[ \\frac{\\sin(\\pi x)}{\\pi} - \\frac{\\sin(5\\pi x)}{5\\pi} \\right]_{0}^{1} = 0\n$$\nSince the spatial functions are orthogonal, they are indeed the eigenfunctions of the correlation operator $\\mathcal{C}$. We can verify this by substituting them into the eigenproblem.\n\nFor the first eigenfunction candidate $\\phi(x) = \\psi_1(x) = \\sin(2\\pi x)$:\n\\begin{align*}\n\\mathcal{C}[\\psi_1](x) &= \\int_{0}^{1} \\left[ \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y) \\right] \\sin(2\\pi y) \\, dy \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\int_{0}^{1} \\sin^2(2\\pi y) \\, dy + \\frac{1}{2} \\sin(3\\pi x) \\int_{0}^{1} \\sin(3\\pi y) \\sin(2\\pi y) \\, dy\n\\end{align*}\nThe second integral is zero due to orthogonality. The first integral is:\n$$\n\\int_{0}^{1} \\sin^2(2\\pi y) \\, dy = \\int_{0}^{1} \\frac{1 - \\cos(4\\pi y)}{2} \\, dy = \\left[ \\frac{y}{2} - \\frac{\\sin(4\\pi y)}{8\\pi} \\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, $\\mathcal{C}[\\psi_1](x) = \\frac{1}{2} \\sin(2\\pi x) \\cdot \\frac{1}{2} = \\frac{1}{4} \\sin(2\\pi x)$.\nSo, $\\phi_1(x) = \\sin(2\\pi x)$ is an eigenfunction with eigenvalue $\\lambda_1 = \\frac{1}{4}$.\n\nFor the second eigenfunction candidate $\\phi(x) = \\psi_2(x) = \\sin(3\\pi x)$:\n\\begin{align*}\n\\mathcal{C}[\\psi_2](x) &= \\int_{0}^{1} \\left[ \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y) \\right] \\sin(3\\pi y) \\, dy \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\int_{0}^{1} \\sin(2\\pi y) \\sin(3\\pi y) \\, dy + \\frac{1}{2} \\sin(3\\pi x) \\int_{0}^{1} \\sin^2(3\\pi y) \\, dy\n\\end{align*}\nThe first integral is zero. The second integral is:\n$$\n\\int_{0}^{1} \\sin^2(3\\pi y) \\, dy = \\int_{0}^{1} \\frac{1 - \\cos(6\\pi y)}{2} \\, dy = \\left[ \\frac{y}{2} - \\frac{\\sin(6\\pi y)}{12\\pi} \\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, $\\mathcal{C}[\\psi_2](x) = \\frac{1}{2} \\sin(3\\pi x) \\cdot \\frac{1}{2} = \\frac{1}{4} \\sin(3\\pi x)$.\nSo, $\\phi_2(x) = \\sin(3\\pi x)$ is an eigenfunction with eigenvalue $\\lambda_2 = \\frac{1}{4}$.\n\nThe final step is to normalize these eigenfunctions to have unit $L^2$-norm. The squared norm of an eigenfunction $\\phi$ is $\\|\\phi\\|^2 = \\langle \\phi, \\phi \\rangle = \\int_{0}^{1} \\phi(x)^2 \\, dx$.\nFor the first mode, $\\sin(2\\pi x)$:\n$$\n\\|\\sin(2\\pi x)\\|^2 = \\int_{0}^{1} \\sin^2(2\\pi x) \\, dx = \\frac{1}{2}\n$$\nThe normalization constant is $1 / \\sqrt{1/2} = \\sqrt{2}$. The first normalized POD mode is $\\hat{\\phi_1}(x) = \\sqrt{2} \\sin(2\\pi x)$.\n\nFor the second mode, $\\sin(3\\pi x)$:\n$$\n\\|\\sin(3\\pi x)\\|^2 = \\int_{0}^{1} \\sin^2(3\\pi x) \\, dx = \\frac{1}{2}\n$$\nThe normalization constant is also $\\sqrt{2}$. The second normalized POD mode is $\\hat{\\phi_2}(x) = \\sqrt{2} \\sin(3\\pi x)$.\n\nThe two non-trivial spatial POD modes are therefore $\\sqrt{2} \\sin(2\\pi x)$ and $\\sqrt{2} \\sin(3\\pi x)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{2}\\sin(2\\pi x) & \\sqrt{2}\\sin(3\\pi x)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A key goal of reduced-order modeling is to compress complex data into a few representative patterns, but how effective is this in practice? This exercise  puts that goal to the test with a seemingly simple case: a translating Gaussian pulse. By implementing POD and measuring the reconstruction error, you will discover a fundamental limitation of the method, learning that its effectiveness is highly dependent on the nature of the underlying dynamics and that not all phenomena are easily compressed.",
            "id": "3265968",
            "problem": "Consider the family of snapshot functions of a translating Gaussian pulse defined by $u(x,t) = \\exp\\!\\left(-\\big(x - c t\\big)^{2}\\right)$ on the spatial interval $x \\in [-L,L]$ and discrete times $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$. From first principles, Proper Orthogonal Decomposition (POD) is the procedure that, for a given rank $r$, selects an $r$-dimensional orthonormal basis in space that minimizes the total squared projection error of the snapshot set. Equivalently, it produces the best rank-$r$ approximation of the snapshot data (in the Euclidean least-squares sense across all grid points and times).\n\nYour task is to implement a program that:\n- Constructs the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$ whose $k$-th column is the sampled snapshot $u(x,t_k)$ at $N_x$ uniformly spaced grid points in $[-L,L]$, for a given speed $c$, number of snapshots $m$, and final time $T$ with $t_k$ equally spaced in $[0,T]$.\n- Computes, for ranks $r \\in \\{1,2,5,10\\}$, the best rank-$r$ approximation $X_r$ (as defined by POD) and the corresponding relative reconstruction error\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F},$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n- Reports the errors $E_r$ for each test case as floating-point numbers rounded to six decimal places.\n\nFundamental base to use:\n- Definitions of Euclidean inner product and Frobenius norm.\n- The defining optimization property of Proper Orthogonal Decomposition (POD): among all $r$-dimensional orthonormal bases, POD minimizes the total squared projection error of the snapshots. This yields the best rank-$r$ approximation of the snapshot matrix in the least-squares sense.\n\nTest suite:\nUse $L = 10$ and $N_x = 401$ for all cases. The four test cases are:\n1. Case A (stationary pulse): $c = 0$, $T = 5$, $m = 50$.\n2. Case B (slow translation): $c = 0.5$, $T = 10$, $m = 100$.\n3. Case C (fast translation): $c = 2.0$, $T = 4$, $m = 80$.\n4. Case D (few snapshots): $c = 0.5$, $T = 10$, $m = 5$.\n\nAnswer specification:\n- For each test case, output a list $[E_{1},E_{2},E_{5},E_{10}]$ of four floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed list of four errors for one test case, in the order of Cases A, B, C, D. For example, an output of the correct format would look like\n$[[e_{A,1},e_{A,2},e_{A,5},e_{A,10}],[e_{B,1},e_{B,2},e_{B,5},e_{B,10}],[e_{C,1},e_{C,2},e_{C,5},e_{C,10}],[e_{D,1},e_{D,2},e_{D,5},e_{D,10}]]$,\nwith no spaces anywhere in the line.\n\nUnits:\n- There are no physical units required in this problem.\n\nAngle units:\n- Not applicable.\n\nPercentages:\n- Not applicable; express all quantities as decimals.\n\nYour implementation must be self-contained and require no user input, external files, or network access. It must run in a modern programming language and produce the exact final output format described above in a single line.",
            "solution": "The objective is to compute the relative reconstruction error for a rank-$r$ approximation of a set of data snapshots. The foundational principle is that the optimal rank-$r$ approximation, in the least-squares sense defined by the Frobenius norm, is obtained via the Singular Value Decomposition (SVD). This result is formally stated by the Eckart-Young-Mirsky theorem.\n\n**1. Snapshot Matrix Construction**\nFirst, we discretize the problem domain. The spatial domain $x \\in [-L, L]$ is sampled at $N_x$ uniformly spaced points, forming the grid $\\{x_j\\}_{j=0}^{N_x-1}$. The time interval $t \\in [0, T]$ is sampled at $m$ discrete, equally spaced points $\\{t_k\\}_{k=0}^{m-1}$. The snapshot data at each time point $t_k$ is a vector in $\\mathbb{R}^{N_x}$ whose entries are given by the function $u(x_j, t_k)$. The collection of these snapshots forms the columns of the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$. An element $X_{jk}$ of this matrix is given by:\n$$X_{jk} = u(x_j, t_k) = \\exp\\!\\left(-\\big(x_j - c t_k\\big)^{2}\\right)$$\n\n**2. Singular Value Decomposition and Optimal Approximation**\nThe SVD of the snapshot matrix $X$ is given by:\n$$X = U \\Sigma V^T$$\nwhere $U \\in \\mathbb{R}^{N_x \\times N_x}$ is an orthogonal matrix whose columns $u_i$ are the left-singular vectors (POD modes), $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns $v_i$ are the right-singular vectors, and $\\Sigma \\in \\mathbb{R}^{N_x \\times m}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$. The singular values are non-negative and ordered by convention: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, where $k = \\min(N_x, m)$ is the rank of the matrix.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of $X$, denoted $X_r$, that minimizes the Frobenius norm of the difference, $\\lVert X - X_r \\rVert_F$, is the truncated SVD:\n$$X_r = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\nThis approximation is constructed using the first $r$ singular values and their corresponding left and right singular vectors.\n\n**3. Error Calculation**\nThe relative reconstruction error $E_r$ is defined as the ratio of the Frobenius norm of the error matrix $(X - X_r)$ to the Frobenius norm of the original matrix $X$. The Frobenius norm is related to the singular values by the identity $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2$.\nApplying this property, the squared norm of the original matrix is the sum of the squares of all its singular values:\n$$\\lVert X \\rVert_F^2 = \\sum_{i=1}^{k} \\sigma_i^2$$\nThe error matrix is $X - X_r = \\sum_{i=r+1}^{k} \\sigma_i u_i v_i^T$. Due to the orthogonality of the singular vectors, the squared Frobenius norm of the error matrix is the sum of the squares of the discarded singular values:\n$$\\lVert X - X_r \\rVert_F^2 = \\sum_{i=r+1}^{k} \\sigma_i^2$$\nCombining these results, the relative reconstruction error is given by:\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\sqrt{\\sum_{i=r+1}^{k} \\sigma_i^2}}{\\sqrt{\\sum_{i=1}^{k} \\sigma_i^2}}$$\nNote that if the requested rank $r$ is greater than or equal to the actual rank of the matrix, $k$, the sum in the numerator is empty and evaluates to $0$, correctly yielding an error $E_r = 0$.\n\n**4. Computational Procedure**\nThe algorithm proceeds as follows for each test case:\n1.  Define the parameters $c$, $T$, and $m$, along with the fixed constants $L=10$ and $N_x=401$.\n2.  Construct the spatial grid $x$ and temporal grid $t$.\n3.  Assemble the $N_x \\times m$ snapshot matrix $X$ using the given function $u(x,t)$.\n4.  Compute the singular values $\\sigma_i$ of $X$ using a standard numerical library function for SVD. It is most efficient to compute only the singular values, not the full $U$ and $V$ matrices.\n5.  Calculate the total energy, represented by the squared Frobenius norm, $S_{total} = \\sum_{i=1}^{k} \\sigma_i^2$.\n6.  For each required rank $r \\in \\{1, 2, 5, 10\\}$, calculate the error energy, $S_{error} = \\sum_{i=r+1}^{k} \\sigma_i^2$.\n7.  The relative error is then $E_r = \\sqrt{S_{error} / S_{total}}$.\n8.  The calculated errors for each test case are collected and formatted according to the output specification.\nThis procedure provides a direct and numerically stable method for determining the required reconstruction errors based on fundamental principles of linear algebra.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Proper Orthogonal Decomposition problem for a translating Gaussian pulse.\n    \"\"\"\n    # Global parameters for all test cases\n    L = 10.0\n    Nx = 401\n    ranks_to_compute = [1, 2, 5, 10]\n\n    # Test suite: (c, T, m)\n    # c: speed, T: final time, m: number of snapshots\n    test_cases = [\n        (0.0, 5.0, 50),   # Case A: stationary pulse\n        (0.5, 10.0, 100), # Case B: slow translation\n        (2.0, 4.0, 80),   # Case C: fast translation\n        (0.5, 10.0, 5),   # Case D: few snapshots\n    ]\n\n    all_results = []\n\n    for c, T, m in test_cases:\n        # 1. Create spatial and temporal grids\n        x = np.linspace(-L, L, Nx)\n        t = np.linspace(0.0, T, m)\n\n        # 2. Construct the snapshot matrix X using broadcasting\n        # x_col has shape (Nx, 1) and t_row has shape (1, m)\n        # Broadcasting expands them to (Nx, m) for element-wise operations\n        x_col = x[:, np.newaxis]\n        t_row = t[np.newaxis, :]\n        X = np.exp(-((x_col - c * t_row) ** 2))\n\n        # 3. Compute the singular values of X\n        # We only need the singular values, so compute_uv=False is most efficient.\n        s = np.linalg.svd(X, compute_uv=False)\n        num_singular_values = s.shape[0]\n\n        # 4. Calculate the total energy (squared Frobenius norm of X)\n        # This is the sum of the squares of all singular values.\n        norm_X_sq = np.sum(s**2)\n\n        case_errors = []\n        for r in ranks_to_compute:\n            # 5. Calculate the reconstruction error for rank r\n            \n            # If norm_X_sq is zero, all errors are zero.\n            if norm_X_sq == 0.0:\n                 error = 0.0\n            # If rank r is >= number of singular values, the approximation is perfect.\n            elif r >= num_singular_values:\n                error = 0.0\n            else:\n                # The error norm is based on the truncated singular values (from r to end).\n                # s[r:] corresponds to sigma_{r+1}, sigma_{r+2}, ...\n                norm_err_sq = np.sum(s[r:]**2)\n                error = np.sqrt(norm_err_sq / norm_X_sq)\n            \n            case_errors.append(error)\n\n        all_results.append(case_errors)\n\n    # 6. Format the output string exactly as specified.\n    # e.g., [[err1,err2,...],[err1,err2,...]] with no spaces.\n    formatted_sublists = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places.\n        formatted_numbers = [f\"{err:.6f}\" for err in res_list]\n        # Join numbers with commas and enclose in brackets.\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the sublists with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(formatted_sublists)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A good basis for data reconstruction does not automatically guarantee a stable predictive model. This exercise  confronts a critical and advanced challenge in reduced-order modeling: the potential for Galerkin projection to create an unstable reduced-order model (ROM) from a perfectly stable full-order system. By constructing and simulating such a case, you will explore the subtle but crucial distinction between data-fitting and creating reliable dynamic predictions, a key consideration when modeling non-normal systems.",
            "id": "2432128",
            "problem": "You are asked to implement a complete numerical experiment in reduced-order modeling that demonstrates the following phenomenon: a Proper Orthogonal Decomposition (POD) basis can be excellent for reconstructing training snapshots of a stable full-order linear time-invariant system, yet the Galerkin-projected reduced-order model (ROM) can produce unstable dynamics that blow up when integrated in time.\n\nYour implementation must start from the full-order ordinary differential equation\n$$\n\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b},\n$$\nwhere $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ and $\\mathbf{b}\\in\\mathbb{R}^{n}$ are constant, and $\\mathbf{x}(t)\\in\\mathbb{R}^{n}$ is the state. All computations are over the real numbers with the standard Euclidean inner product. You will use $n=2$ throughout.\n\nFundamental definitions and requirements:\n- Proper Orthogonal Decomposition (POD) basis: Given a snapshot matrix\n$$\n\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}(t_1) & \\mathbf{x}(t_2) & \\cdots & \\mathbf{x}(t_m)\\end{bmatrix}\\in\\mathbb{R}^{n\\times m},\n$$\ncompute its singular value decomposition (SVD) $\\mathbf{X}=\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$. The rank-$r$ POD basis $\\mathbf{Q}\\in\\mathbb{R}^{n\\times r}$ is taken as the first $r$ columns of $\\mathbf{U}$.\n- Galerkin projection: The reduced operator and reduced forcing are\n$$\n\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}\\in\\mathbb{R}^{r\\times r},\\qquad \\mathbf{b}_r=\\mathbf{Q}^\\top\\mathbf{b}\\in\\mathbb{R}^{r}.\n$$\nThe reduced state $\\mathbf{z}(t)\\in\\mathbb{R}^{r}$ evolves as\n$$\n\\frac{d\\mathbf{z}}{dt} = \\mathbf{A}_r \\mathbf{z} + \\mathbf{b}_r,\\qquad \\mathbf{x}_r(t)=\\mathbf{Q}\\mathbf{z}(t).\n$$\n- Time integration: Use the classical fourth-order Runge–Kutta method with a fixed time step $h>0$ for both the full-order model and the ROM. Set the initial condition to $\\mathbf{x}(0)=\\mathbf{0}$ and $\\mathbf{z}(0)=\\mathbf{Q}^\\top\\mathbf{x}(0)=\\mathbf{0}$.\n- Snapshot collection: Integrate the full-order model over a training horizon $[0,T_{\\text{train}}]$ with a constant time step $h$, sampling the state at every step to form $\\mathbf{X}$.\n- Reconstruction error: Measure the relative POD reconstruction error of the training snapshots as\n$$\n\\varepsilon_{\\text{rec}} = \\frac{\\lVert \\mathbf{X} - \\mathbf{Q}\\mathbf{Q}^\\top \\mathbf{X}\\rVert_F}{\\lVert \\mathbf{X}\\rVert_F},\n$$\nwhere $\\lVert\\cdot\\rVert_F$ denotes the Frobenius norm.\n- Blow-up detection: Evolve both the full-order model and the ROM over a test horizon $[0,T_{\\text{test}}]$ with the same $h$. Declare a solution “blown up” if at any time step the Euclidean norm of the current state exceeds a threshold $M$, or if any component becomes not-a-number or infinite. Use the threshold $M=10^6$.\n\nConstructed forcing to target instability under ROM:\n- For each test, you must construct the constant forcing $\\mathbf{b}$ as follows. Compute the symmetric part $\\mathbf{S}=\\frac{1}{2}(\\mathbf{A}+\\mathbf{A}^\\top)$ and its dominant unit eigenvector $\\mathbf{q}\\in\\mathbb{R}^{n}$ associated with the largest eigenvalue of $\\mathbf{S}$ (break ties arbitrarily but deterministically). Set\n$$\n\\mathbf{b}=-\\mathbf{A}\\mathbf{q}.\n$$\nThis construction ensures that the full-order steady state is $\\mathbf{x}_\\infty = -\\mathbf{A}^{-1}\\mathbf{b}=\\mathbf{q}$. When $\\mathbf{A}$ is highly non-normal and the largest eigenvalue of $\\mathbf{S}$ is positive, the scalar ROM obtained with $r=1$ and $\\mathbf{Q}=\\mathbf{q}$ has reduced dynamics $\\frac{dz}{dt} = a_r z + b_r$ with $a_r=\\mathbf{q}^\\top\\mathbf{A}\\mathbf{q}>0$ and $b_r=-a_r$, which is unstable and diverges from $z(0)=0$.\n\nNumerical specification common to all tests:\n- Use $n=2$.\n- Use $h=10^{-3}$.\n- Use classical fourth-order Runge–Kutta.\n- Use the Euclidean norm for all vector norms.\n- Use $\\mathbf{x}(0)=\\mathbf{0}$.\n\nTest suite:\nImplement the above for the following parameter sets. In each case, define $\\mathbf{A}$, compute $\\mathbf{q}$ and $\\mathbf{b}$ as specified, collect snapshots over $[0,T_{\\text{train}}]$ to form $\\mathbf{Q}$, then form the ROM and run both models over $[0,T_{\\text{test}}]$.\n\n- Test $1$ (highly non-normal, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=50.0$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $2$ (highly non-normal, rank-$2$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=50.0$,\n  - $r=2$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $3$ (symmetric negative definite, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-1.0 & 0.0 \\\\ 0.0 & -2.0\\end{bmatrix}$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n- Test $4$ (more highly non-normal, rank-$1$ POD):\n  - $\\mathbf{A}=\\begin{bmatrix}-0.1 & \\alpha \\\\ 0 & -1.0\\end{bmatrix}$ with $\\alpha=120.0$,\n  - $r=1$,\n  - $T_{\\text{train}}=4.0$,\n  - $T_{\\text{test}}=1.0$.\n\nRequired outputs:\n- For each test, output a list of three entries:\n  - the scalar $\\varepsilon_{\\text{rec}}$ rounded to six decimal places,\n  - a boolean indicating whether the ROM blew up on $[0,T_{\\text{test}}]$,\n  - a boolean indicating whether the full-order model blew up on $[0,T_{\\text{test}}]$.\n- Aggregate the results from all tests into a single line as a comma-separated list enclosed in square brackets, in the same order as the tests. Example format:\n$[\\,[\\varepsilon_{\\text{rec}}^{(1)},\\,\\text{ROM}^{(1)}\\_\\text{blowup},\\,\\text{FOM}^{(1)}\\_\\text{blowup}],\\,[\\varepsilon_{\\text{rec}}^{(2)},\\,\\text{ROM}^{(2)}\\_\\text{blowup},\\,\\text{FOM}^{(2)}\\_\\text{blowup}],\\,\\ldots\\,]$.",
            "solution": "The core of the problem lies in the distinction between the spectrum of a matrix $\\mathbf{A}$ and its numerical range (or field of values), defined as $W(\\mathbf{A}) = \\{\\mathbf{v}^\\dagger\\mathbf{A}\\mathbf{v} : \\mathbf{v} \\in \\mathbb{C}^n, \\lVert\\mathbf{v}\\rVert_2 = 1\\}$. For a linear time-invariant system $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x}$, stability is determined by the eigenvalues of $\\mathbf{A}$ (the spectrum, $\\sigma(\\mathbf{A})$). If all eigenvalues have negative real parts, the system is stable. However, transient growth is possible if $\\mathbf{A}$ is non-normal (i.e., $\\mathbf{A}\\mathbf{A}^\\top \\neq \\mathbf{A}^\\top\\mathbf{A}$). The numerical range provides insight into this transient behavior. The real part of the numerical range is governed by the symmetric part of the matrix, $\\mathbf{S} = \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^\\top)$, since $\\text{Re}(\\mathbf{v}^\\top\\mathbf{A}\\mathbf{v}) = \\mathbf{v}^\\top\\mathbf{S}\\mathbf{v}$. A positive eigenvalue of $\\mathbf{S}$ implies that the numerical range of $\\mathbf{A}$ extends into the right half-plane, indicating potential for transient energy growth.\n\nA Galerkin projection with a rank-$r$ POD basis $\\mathbf{Q}$ transforms the full-order model (FOM) into the reduced-order model (ROM) $\\frac{d\\mathbf{z}}{dt} = \\mathbf{A}_r\\mathbf{z} + \\mathbf{b}_r$, where $\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$. The stability of the ROM is determined by the eigenvalues of the reduced matrix $\\mathbf{A}_r$. Crucially, the eigenvalues of $\\mathbf{A}_r$ are contained within the numerical range of $\\mathbf{A}$, but not necessarily within the convex hull of its spectrum. If the numerical range $W(\\mathbf{A})$ crosses into the right half-plane, it is possible to find a projection subspace (basis $\\mathbf{Q}$) such that $\\mathbf{A}_r$ has eigenvalues with positive real parts, rendering the ROM unstable.\n\nThe problem's construction is designed to expose this pathology. The FOM is stable (eigenvalues of $\\mathbf{A}$ are $\\{-0.1, -1.0\\}$). The forcing term $\\mathbf{b} = -\\mathbf{A}\\mathbf{q}$ is chosen such that the FOM steady state is $\\mathbf{x}_{\\infty} = \\mathbf{q}$, where $\\mathbf{q}$ is the eigenvector corresponding to the largest eigenvalue of $\\mathbf{S}$. This drives the system dynamics towards the direction of maximum transient growth. The resulting snapshots will be dominated by this direction, causing the primary POD mode (the first column of $\\mathbf{Q}$) to align with $\\mathbf{q}$. For a rank-1 ROM ($r=1$), the reduced matrix $\\mathbf{A}_r$ becomes a scalar $a_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$. If $\\mathbf{Q} \\approx \\mathbf{q}$, then $a_r \\approx \\mathbf{q}^\\top\\mathbf{A}\\mathbf{q} = \\mathbf{q}^\\top\\mathbf{S}\\mathbf{q} = \\lambda_{\\max}(\\mathbf{S})$. For the non-normal matrices in Tests 1 and 4, $\\lambda_{\\max}(\\mathbf{S}) > 0$, leading to an unstable ROM.\n\nThe computational procedure for each test case is as follows:\n1.  Define system parameters: matrix $\\mathbf{A}$, ROM rank $r$, and time horizons $T_{\\text{train}}$ and $T_{\\text{test}}$. The dimension is $n=2$ and the time step is $h=10^{-3}$.\n2.  Construct the forcing term: Compute the symmetric part $\\mathbf{S} = \\frac{1}{2}(\\mathbf{A} + \\mathbf{A}^\\top)$. Find its eigenvalues and eigenvectors. Let $\\mathbf{q}$ be the normalized eigenvector corresponding to the largest eigenvalue. Set $\\mathbf{b} = -\\mathbf{A}\\mathbf{q}$.\n3.  Generate training data: Integrate the FOM, $\\frac{d\\mathbf{x}}{dt} = \\mathbf{A}\\mathbf{x} + \\mathbf{b}$, from $\\mathbf{x}(0)=\\mathbf{0}$ over the time interval $[0, T_{\\text{train}}]$ using the classical fourth-order Runge-Kutta method. The states at each time step are collected into the snapshot matrix $\\mathbf{X}$.\n4.  Compute the POD basis: Perform a singular value decomposition (SVD) on the snapshot matrix, $\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top$. The POD basis $\\mathbf{Q}$ of rank $r$ is formed by the first $r$ columns of $\\mathbf{U}$.\n5.  Calculate reconstruction error: The relative Frobenius norm error is computed as $\\varepsilon_{\\text{rec}} = \\lVert \\mathbf{X} - \\mathbf{Q}\\mathbf{Q}^\\top \\mathbf{X}\\rVert_F / \\lVert \\mathbf{X}\\rVert_F$.\n6.  Construct the ROM: The reduced system matrices are $\\mathbf{A}_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$ and $\\mathbf{b}_r = \\mathbf{Q}^\\top\\mathbf{b}$.\n7.  Perform time integration for testing: Both the FOM and the ROM are integrated from zero initial conditions ($\\mathbf{x}(0)=\\mathbf{0}$, $\\mathbf{z}(0)=\\mathbf{0}$) over the interval $[0, T_{\\text{test}}]$. During integration, at each step, the Euclidean norm of the state vector is checked against the blow-up threshold $M=10^6$.\n8.  Record results: The final outputs for the test are the computed $\\varepsilon_{\\text{rec}}$, a boolean indicating if the ROM blew up, and a boolean indicating if the FOM blew up.\n\nExpected outcomes for the tests:\n- **Test 1** ($\\mathbf{A}$ non-normal, $r=1$): $\\mathbf{A}$ is stable. The construction of $\\mathbf{b}$ and the choice of $r=1$ are designed to produce an unstable ROM. We expect a small $\\varepsilon_{\\text{rec}}$, ROM blow-up, and no FOM blow-up.\n- **Test 2** ($\\mathbf{A}$ non-normal, $r=2$): Here, $r=n=2$. The POD basis $\\mathbf{Q}$ will be a complete orthonormal basis for $\\mathbb{R}^2$. Thus, $\\mathbf{Q}\\mathbf{Q}^\\top = \\mathbf{I}$, meaning the reconstruction error $\\varepsilon_{\\text{rec}}$ will be zero (or of the order of machine precision). The ROM is dynamically equivalent to the FOM, simply expressed in a different basis. Since the FOM is stable, the ROM will also be stable. We expect $\\varepsilon_{\\text{rec}} \\approx 0$, no ROM blow-up, and no FOM blow-up.\n- **Test 3** ($\\mathbf{A}$ symmetric, $r=1$): $\\mathbf{A}$ is a normal matrix. Its numerical range is the convex hull of its eigenvalues, which are $\\{-1.0, -2.0\\}$. Thus, the numerical range is the interval $[-2.0, -1.0]$ on the real axis. The reduced operator $a_r = \\mathbf{Q}^\\top\\mathbf{A}\\mathbf{Q}$ must be negative. The ROM will be stable. We expect no blow-up for either model.\n- **Test 4** ($\\mathbf{A}$ more non-normal, $r=1$): Similar to Test 1, but with a larger off-diagonal term $\\alpha=120.0$. This increases the non-normality, leading to a larger positive eigenvalue for $\\mathbf{S}$. The ROM instability should be even more pronounced. We expect a small $\\varepsilon_{\\text{rec}}$, ROM blow-up, and no FOM blow-up.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Implements the full numerical experiment to demonstrate ROM instability\n    for a stable FOM.\n    \"\"\"\n\n    def rk4_step(f, y, h, A, b):\n        \"\"\"A single step of the classical fourth-order Runge-Kutta method.\"\"\"\n        k1 = f(y, A, b)\n        k2 = f(y + h / 2 * k1, A, b)\n        k3 = f(y + h / 2 * k2, A, b)\n        k4 = f(y + h * k3, A, b)\n        return y + h / 6 * (k1 + 2 * k2 + 2 * k3 + k4)\n\n    def lti_rhs(y, A, b):\n        \"\"\"RHS of the LTI system dy/dt = Ay + b.\"\"\"\n        return A @ y + b\n\n    def simulate(A, b, y0, T, h, M):\n        \"\"\"\n        Simulates an LTI system and returns snapshots and blow-up status.\n        \"\"\"\n        num_steps = int(T / h)\n        y = y0.copy()\n        snapshots = [y0.copy()]\n        blew_up = False\n        \n        for _ in range(num_steps):\n            y = rk4_step(lti_rhs, y, h, A, b)\n            if not blew_up and (np.linalg.norm(y) > M or not np.all(np.isfinite(y))):\n                blew_up = True\n            snapshots.append(y.copy())\n            \n        return np.array(snapshots).T, blew_up\n\n    # General parameters\n    n = 2\n    h = 1e-3\n    M = 1e6\n    x0 = np.zeros(n)\n\n    # Test cases from the problem statement.\n    test_cases = [\n        # (A_params, r, T_train, T_test)\n        ({\"alpha\": 50.0}, 1, 4.0, 1.0),\n        ({\"alpha\": 50.0}, 2, 4.0, 1.0),\n        ({\"alpha\": None}, 1, 4.0, 1.0), # Symmetric case\n        ({\"alpha\": 120.0}, 1, 4.0, 1.0),\n    ]\n\n    results = []\n    \n    for i, (params, r, T_train, T_test) in enumerate(test_cases):\n        # 1. Define A\n        if i == 2: # Test 3: Symmetric case\n            A = np.array([[-1.0, 0.0], [0.0, -2.0]])\n        else: # Tests 1, 2, 4: Non-normal case\n            alpha = params[\"alpha\"]\n            A = np.array([[-0.1, alpha], [0.0, -1.0]])\n\n        # 2. Construct b\n        S = 0.5 * (A + A.T)\n        eigvals, eigvecs = eigh(S)\n        q = eigvecs[:, -1] # Dominant eigenvector (eigh sorts eigenvalues)\n        b = -A @ q\n\n        # 3. Generate FOM snapshots for training\n        X, _ = simulate(A, b, x0, T_train, h, M)\n\n        # 4. Compute POD basis Q\n        U, s, _ = np.linalg.svd(X, full_matrices=False)\n        Q = U[:, :r]\n\n        # 5. Calculate reconstruction error\n        # eps_rec = norm(X - Q @ Q.T @ X) / norm(X)\n        # Using singular values is more direct: sqrt(sum(s_i^2 for i>r)) / sqrt(sum(s_i^2))\n        if X.shape[1] > 1:\n            norm_X_sq = np.sum(s**2)\n            if norm_X_sq > 0:\n                norm_err_sq = np.sum(s[r:]**2)\n                eps_rec = np.sqrt(norm_err_sq / norm_X_sq)\n            else:\n                eps_rec = 0.0\n        else:\n            eps_rec = 0.0\n\n\n        # 6. Form the ROM\n        Ar = Q.T @ A @ Q\n        br = Q.T @ b\n        z0 = np.zeros(r)\n\n        # 7. Simulate FOM and ROM for testing, check blow-up\n        _, fom_blew_up = simulate(A, b, x0, T_test, h, M)\n        _, rom_blew_up = simulate(Ar, br, z0, T_test, h, M)\n\n        # 8. Record results\n        results.append([round(eps_rec, 6), rom_blew_up, fom_blew_up])\n\n    # Final print statement in the exact required format.\n    # Convert bools to lowercase 'true'/'false' for JS-like format\n    formatted_results = []\n    for res in results:\n        eps_str = f\"{res[0]:.6f}\"\n        rom_bool_str = str(res[1]).lower()\n        fom_bool_str = str(res[2]).lower()\n        formatted_results.append(f\"[{eps_str},{rom_bool_str},{fom_bool_str}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}