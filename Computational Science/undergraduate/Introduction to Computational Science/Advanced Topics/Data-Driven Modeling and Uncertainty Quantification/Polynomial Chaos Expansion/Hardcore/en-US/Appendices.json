{
    "hands_on_practices": [
        {
            "introduction": "In computational science, verifying your code against a known solution is a critical first step. This exercise guides you through building a synthetic benchmark where a Polynomial Chaos Expansion provides an exact representation of a quadratic model, allowing you to rigorously validate your implementation of coefficient extraction and moment calculations . Mastering this validation is essential before tackling more complex problems where the true solution is unknown.",
            "id": "3174279",
            "problem": "You are asked to construct and validate a synthetic benchmark for generalized Polynomial Chaos (gPC) that yields exact mean and variance for quadratic response functions. The benchmark shall be designed so that the truncated orthonormal polynomial chaos of total degree two represents the response exactly. You will implement coefficient extraction by orthonormal projection using exact numerical quadrature and verify that the mean and variance computed from the gPC coefficients match the analytical values derived from distribution moments.\n\nFundamental base and assumptions: consider independent random variables with known orthonormal polynomial families and exact quadrature rules. For a standard normal random variable $X$ with probability density function $\\phi(x)$ and a uniform random variable $U$ on $[-1,1]$, use their associated orthonormal polynomial bases and Gaussian quadrature rules. The generalized Polynomial Chaos (gPC) basis functions are tensor products of orthonormal univariate polynomials appropriate to each marginal distribution. Coefficients are defined by orthonormal projection with respect to the underlying probability measure.\n\nYour tasks:\n1. Construct orthonormal univariate bases:\n   - For $X \\sim \\mathcal{N}(0,1)$, use the orthonormalized probabilists' Hermite polynomials $\\{\\psi_n(x)\\}_{n \\ge 0}$ such that $E[\\psi_m(X)\\psi_n(X)] = \\delta_{mn}$, where $\\delta_{mn}$ is the Kronecker delta.\n   - For $U \\sim \\text{Uniform}([-1,1])$, use the orthonormalized Legendre polynomials $\\{\\ell_n(u)\\}_{n \\ge 0}$ such that $E[\\ell_m(U)\\ell_n(U)] = \\delta_{mn}$.\n   - In multiple dimensions, use tensor products of the univariate orthonormal bases, indexed by multi-indices of total degree up to two.\n2. Extract gPC coefficients by orthonormal projection. For a response $f(\\boldsymbol{\\xi})$ and an orthonormal basis function $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$, the coefficient $c_{\\boldsymbol{\\alpha}}$ is the inner product\n   $$c_{\\boldsymbol{\\alpha}} = E\\left[f(\\boldsymbol{\\xi}) \\, \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})\\right],$$\n   where $E[\\cdot]$ denotes expectation with respect to the joint distribution of $\\boldsymbol{\\xi}$.\n3. Compute the mean and variance from the extracted coefficients using orthonormality of the basis. Then, compute the exact analytical mean and variance using well-tested distribution moment formulas for quadratic polynomials and compare the two.\n4. Use exact numerical quadrature rules suitable for the corresponding measures and polynomial degrees:\n   - For $X \\sim \\mathcal{N}(0,1)$, use three-point Gauss-Hermite quadrature with the change of variables $x = \\sqrt{2}\\,t$ to evaluate expectations of polynomials of degree up to four exactly.\n   - For $U \\sim \\text{Uniform}([-1,1])$, use three-point Gauss-Legendre quadrature scaled by the probability density to evaluate expectations of polynomials of degree up to four exactly.\n5. Implement the above for one-dimensional and two-dimensional independent random inputs and quadratic responses. For two dimensions, use tensor-product quadrature to maintain exactness for bivariate polynomials of appropriate degree.\n\nDefine the response functions and their parameters as follows. For one-dimensional cases, let\n$$f(x) = a_0 + a_1 x + a_2 x^2,$$\nand for two-dimensional cases with independent inputs $(x,y)$, let\n$$f(x,y) = c_0 + c_1 x + c_2 y + c_3 x^2 + c_4 y^2 + c_5 x y.$$\n\nUse the following test suite of parameter values:\n- Case 1 (standard normal, one-dimensional): $a_0 = 1.3$, $a_1 = -0.7$, $a_2 = 0.5$.\n- Case 2 (uniform on $[-1,1]$, one-dimensional): $a_0 = 0.2$, $a_1 = 1.1$, $a_2 = -0.5$.\n- Case 3 (standard normal, two-dimensional): $c_0 = 0.9$, $c_1 = 0.2$, $c_2 = -0.4$, $c_3 = 0.3$, $c_4 = 0.1$, $c_5 = 0.25$.\n- Case 4 (uniform on $[-1,1]$, two-dimensional): $c_0 = -0.5$, $c_1 = 0.6$, $c_2 = 0.1$, $c_3 = -0.2$, $c_4 = 0.7$, $c_5 = 0.3$.\n- Case 5 (standard normal, one-dimensional, constant edge case): $a_0 = 2.0$, $a_1 = 0.0$, $a_2 = 0.0$.\n- Case 6 (uniform on $[-1,1]$, one-dimensional, linear edge case): $a_0 = 0.0$, $a_1 = 1.0$, $a_2 = 0.0$.\n\nAnalytical reference moments to be used:\n- For $X \\sim \\mathcal{N}(0,1)$: $E[X] = 0$, $E[X^2] = 1$, $E[X^4] = 3$.\n- For $U \\sim \\text{Uniform}([-1,1])$: $E[U] = 0$, $E[U^2] = \\frac{1}{3}$, $E[U^4] = \\frac{1}{5}$.\n\nYour program must:\n- Implement orthonormal polynomial evaluation for both distributions.\n- Implement three-point Gaussian quadrature rules with appropriate scaling to compute expectations exactly for the required polynomial degrees.\n- Extract gPC coefficients for total degree two.\n- Compute gPC-based mean and variance and compare them to analytical results for each case.\n- Produce a single line of output containing the validation results as a comma-separated list enclosed in square brackets. For each case, output a boolean indicating whether both mean and variance match the analytical values within an absolute tolerance of $10^{-12}$.\n\nThe final output format must be exactly:\n\"Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., [True,False,True,False,True,True]).\"",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the theory of generalized Polynomial Chaos (gPC) expansions, a standard method in uncertainty quantification. The problem is well-posed, with all necessary parameters, distributions, and response functions explicitly defined. The objective is clear and falsifiable: to verify the correctness of a numerical implementation against analytical results. The underlying mathematical assumptions, such as the exactness of a degree-$2$ gPC expansion for a quadratic response and the sufficiency of $3$-point Gaussian quadrature, are correct.\n\nThe solution proceeds as follows. First, we define the necessary orthonormal polynomial bases for the standard normal and uniform distributions. Second, we derive the analytical expressions for the mean and variance of the quadratic response functions. Third, we express the response function in the gPC basis to find the theoretical coefficients and the corresponding mean and variance. The results are shown to be identical to the analytical ones. Fourth, we describe the numerical procedure for extracting the gPC coefficients using orthonormal projection, evaluated via exact numerical quadrature. Finally, we implement this procedure for each test case and verify that the numerically obtained mean and variance match the analytical values within the specified tolerance.\n\nThe gPC expansion of a response function $f(\\boldsymbol{\\xi})$ with random inputs $\\boldsymbol{\\xi}$ is given by\n$$f(\\boldsymbol{\\xi}) = \\sum_{\\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$$\nwhere $\\{\\Psi_{\\boldsymbol{\\alpha}}\\}$ is a basis of polynomials orthonormal with respect to the probability measure of $\\boldsymbol{\\xi}$, and $c_{\\boldsymbol{\\alpha}}$ are the gPC coefficients. For a quadratic response function, the expansion is exact when truncated at total polynomial degree $P=2$.\n\nThe coefficients are found by orthonormal projection:\n$$c_{\\boldsymbol{\\alpha}} = E\\left[f(\\boldsymbol{\\xi}) \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})\\right]$$\nwhere $E[\\cdot]$ denotes the expectation. Due to orthonormality, $E[\\Psi_{\\boldsymbol{\\alpha}}] = \\delta_{\\boldsymbol{\\alpha}\\mathbf{0}}$ and $E[\\Psi_{\\boldsymbol{\\alpha}}\\Psi_{\\boldsymbol{\\beta}}] = \\delta_{\\boldsymbol{\\alpha}\\boldsymbol{\\beta}}$.\nThe mean and variance of $f$ are then computed directly from the coefficients:\n$$\\mu_f = E[f] = E\\left[\\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}\\right] = \\sum_{\\boldsymbol{\\alpha}} c_{\\boldsymbol{\\alpha}} E[\\Psi_{\\boldsymbol{\\alpha}}] = c_{\\mathbf{0}}$$\n$$\\sigma_f^2 = E[(f - \\mu_f)^2] = E\\left[\\left(\\sum_{\\boldsymbol{\\alpha} \\ne \\mathbf{0}} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}\\right)^2\\right] = \\sum_{\\boldsymbol{\\alpha} \\ne \\mathbf{0}} \\sum_{\\boldsymbol{\\beta} \\ne \\mathbf{0}} c_{\\boldsymbol{\\alpha}}c_{\\boldsymbol{\\beta}} E[\\Psi_{\\boldsymbol{\\alpha}}\\Psi_{\\boldsymbol{\\beta}}] = \\sum_{\\boldsymbol{\\alpha} \\ne \\mathbf{0}} c_{\\boldsymbol{\\alpha}}^2$$\n\nThe univariate orthonormal polynomial bases for total degree up to $2$ are:\n1.  For $X \\sim \\mathcal{N}(0,1)$: Orthonormal probabilists' Hermite polynomials.\n    $$\\psi_0(x) = 1, \\quad \\psi_1(x) = x, \\quad \\psi_2(x) = \\frac{1}{\\sqrt{2}}(x^2 - 1)$$\n    The inverse relations are: $1 = \\psi_0(x)$, $x = \\psi_1(x)$, and $x^2 = \\sqrt{2}\\psi_2(x) + \\psi_0(x)$.\n\n2.  For $U \\sim \\text{Uniform}([-1,1])$: Orthonormal Legendre polynomials.\n    $$\\ell_0(u) = 1, \\quad \\ell_1(u) = \\sqrt{3}u, \\quad \\ell_2(u) = \\frac{\\sqrt{5}}{2}(3u^2 - 1)$$\n    The inverse relations are: $1 = \\ell_0(u)$, $u = \\frac{1}{\\sqrt{3}}\\ell_1(u)$, and $u^2 = \\frac{2}{3\\sqrt{5}}\\ell_2(u) + \\frac{1}{3} = \\frac{2\\sqrt{5}}{15}\\ell_2(u) + \\frac{1}{3}\\ell_0(u)$.\n\nThe analytical mean and variance are derived from the moments of the distributions.\nFor a one-dimensional response $f(z) = a_0 + a_1 z + a_2 z^2$, where $z$ is $X$ or $U$:\n$$\\mu_f = E[f(z)] = a_0 + a_1 E[z] + a_2 E[z^2]$$\nSince $E[X]=0$ and $E[U]=0$, this simplifies to $\\mu_f = a_0 + a_2 E[z^2]$.\n$$\\sigma_f^2 = E[(f(z) - \\mu_f)^2] = E[(a_1 z + a_2 (z^2-E[z^2]))^2]$$\nSince odd moments are zero, the cross term vanishes:\n$$\\sigma_f^2 = a_1^2 E[z^2] + a_2^2 E[(z^2-E[z^2])^2] = a_1^2 E[z^2] + a_2^2 (E[z^4] - (E[z^2])^2)$$\n\nFor a two-dimensional response $f(x,y) = c_0 + c_1 x + c_2 y + c_3 x^2 + c_4 y^2 + c_5 xy$, where $x, y$ are independent variables:\n$$\\mu_f = E[f(x,y)] = c_0 + c_3 E[x^2] + c_4 E[y^2]$$\n$$\\sigma_f^2 = \\text{Var}(f(x,y)) = c_1^2 \\text{Var}(x) + c_2^2 \\text{Var}(y) + c_3^2 \\text{Var}(x^2) + c_4^2 \\text{Var}(y^2) + c_5^2 \\text{Var}(xy)$$\nUsing $\\text{Var}(z) = E[z^2]$ for zero-mean $z$, and independence of $x,y$, we get:\n$$\\sigma_f^2 = c_1^2 E[x^2] + c_2^2 E[y^2] + c_3^2(E[x^4] - (E[x^2])^2) + c_4^2(E[y^4] - (E[y^2])^2) + c_5^2 E[x^2]E[y^2]$$\n\nThe coefficient extraction integrals $c_{\\boldsymbol{\\alpha}} = E[f(\\boldsymbol{\\xi}) \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})]$ are evaluated using numerical quadrature. Since the integrand $f(\\boldsymbol{\\xi})\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\xi})$ for $P=2$ is a polynomial of at most degree $4=2+2$, a $3$-point Gaussian quadrature rule, which is exact for polynomials of degree up to $2 \\times 3 - 1 = 5$, yields the exact coefficient values.\n\nFor $X \\sim \\mathcal{N}(0,1)$, we use Gauss-Hermite quadrature. The expectation $E[g(X)]$ is computed as:\n$$E[g(X)] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} g(x) e^{-x^2/2} dx = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} g(\\sqrt{2}t) e^{-t^2} dt \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{3} w_i g(\\sqrt{2}t_i)$$\nwhere $(t_i, w_i)$ are the standard $3$-point Gauss-Hermite quadrature points and weights.\n\nFor $U \\sim \\text{Uniform}([-1,1])$, we use Gauss-Legendre quadrature. The expectation $E[g(U)]$ is computed as:\n$$E[g(U)] = \\frac{1}{2} \\int_{-1}^{1} g(u) du \\approx \\frac{1}{2} \\sum_{i=1}^{3} w_i g(u_i)$$\nwhere $(u_i, w_i)$ are the standard $3$-point Gauss-Legendre quadrature points and weights.\n\nIn two dimensions, a tensor product of the univariate quadrature rules is used to maintain exactness. The implementation will calculate the analytical and gPC-based statistics for each case and verify their agreement.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and validates a synthetic benchmark for generalized Polynomial Chaos (gPC)\n    for quadratic response functions. It verifies that mean and variance from gPC\n    coefficients (extracted via exact quadrature) match analytical values.\n    \"\"\"\n\n    TOLERANCE = 1e-12\n\n    # --- Orthonormal Polynomial Definitions ---\n    def hermite_psi(n, x):\n        \"\"\"Orthonormal probabilists' Hermite polynomials psi_n(x).\"\"\"\n        if n == 0:\n            return np.ones_like(x)\n        if n == 1:\n            return x\n        if n == 2:\n            return (x**2 - 1) / np.sqrt(2)\n        raise ValueError(\"Only degrees 0, 1, 2 are implemented.\")\n\n    def legendre_ell(n, u):\n        \"\"\"Orthonormal Legendre polynomials ell_n(u).\"\"\"\n        if n == 0:\n            return np.ones_like(u)\n        if n == 1:\n            return np.sqrt(3) * u\n        if n == 2:\n            return np.sqrt(5) / 2 * (3 * u**2 - 1)\n        raise ValueError(\"Only degrees 0, 1, 2 are implemented.\")\n\n    # --- Response Function Definitions ---\n    def response_1d(x, coeffs):\n        return coeffs['a0'] + coeffs['a1'] * x + coeffs['a2'] * x**2\n\n    def response_2d(xy, coeffs):\n        x, y = xy[..., 0], xy[..., 1]\n        return (coeffs['c0'] + coeffs['c1'] * x + coeffs['c2'] * y +\n                coeffs['c3'] * x**2 + coeffs['c4'] * y**2 + coeffs['c5'] * x * y)\n\n    # --- Analytical Statistics ---\n    def get_analytical_stats(case):\n        coeffs = case['coeffs']\n        dist_type = case['dist']\n        dim = case['dim']\n\n        # Moments\n        if dist_type == 'normal':\n            moments = {'E_z1': 0, 'E_z2': 1, 'E_z4': 3}\n        else: # uniform\n            moments = {'E_z1': 0, 'E_z2': 1/3, 'E_z4': 1/5}\n\n        if dim == 1:\n            mean = coeffs['a0'] + coeffs['a2'] * moments['E_z2']\n            var_z2 = moments['E_z4'] - moments['E_z2']**2\n            variance = coeffs['a1']**2 * moments['E_z2'] + coeffs['a2']**2 * var_z2\n        else: # dim == 2\n            mean = coeffs['c0'] + coeffs['c3'] * moments['E_z2'] + coeffs['c4'] * moments['E_z2']\n            var_z2 = moments['E_z4'] - moments['E_z2']**2\n            variance = (coeffs['c1']**2 * moments['E_z2'] +\n                        coeffs['c2']**2 * moments['E_z2'] +\n                        coeffs['c3']**2 * var_z2 +\n                        coeffs['c4']**2 * var_z2 +\n                        coeffs['c5']**2 * moments['E_z2'] * moments['E_z2'])\n        return mean, variance\n\n    # --- gPC Coefficient Extraction and Statistics ---\n    def get_gpc_stats(case):\n        dist_type = case['dist']\n        dim = case['dim']\n        coeffs = case['coeffs']\n\n        if dist_type == 'normal':\n            poly_func = hermite_psi\n            # 3-point Gauss-Hermite quadrature\n            gh_pts, gh_w = np.polynomial.hermite.hermgauss(3)\n            quad_pts = gh_pts * np.sqrt(2)\n            quad_w = gh_w / np.sqrt(np.pi)\n        else: # uniform\n            poly_func = legendre_ell\n            # 3-point Gauss-Legendre quadrature\n            gl_pts, gl_w = np.polynomial.legendre.leggauss(3)\n            quad_pts = gl_pts\n            quad_w = gl_w / 2.0\n\n        if dim == 1:\n            f_vals = response_1d(quad_pts, coeffs)\n            \n            multi_indices = [0, 1, 2]\n            pce_coeffs = []\n            for alpha in multi_indices:\n                psi_vals = poly_func(alpha, quad_pts)\n                integrand = f_vals * psi_vals\n                c_alpha = np.sum(integrand * quad_w)\n                pce_coeffs.append(c_alpha)\n            \n            mean_gpc = pce_coeffs[0]\n            var_gpc = np.sum(np.array(pce_coeffs[1:])**2)\n\n        else: # dim == 2\n            quad_pts_x, quad_pts_y = np.meshgrid(quad_pts, quad_pts)\n            quad_pts_2d = np.stack([quad_pts_x.ravel(), quad_pts_y.ravel()], axis=-1)\n            f_vals = response_2d(quad_pts_2d, coeffs)\n            \n            quad_w_x, quad_w_y = np.meshgrid(quad_w, quad_w)\n            quad_w_2d = (quad_w_x * quad_w_y).ravel()\n\n            multi_indices = [(0, 0), (1, 0), (0, 1), (2, 0), (0, 2), (1, 1)]\n            pce_coeffs = {}\n            for alpha in multi_indices:\n                psi_vals = poly_func(alpha[0], quad_pts_2d[:, 0]) * poly_func(alpha[1], quad_pts_2d[:, 1])\n                integrand = f_vals * psi_vals\n                c_alpha = np.sum(integrand * quad_w_2d)\n                pce_coeffs[alpha] = c_alpha\n\n            mean_gpc = pce_coeffs[(0, 0)]\n            var_gpc = sum(c**2 for k, c in pce_coeffs.items() if k != (0, 0))\n        \n        return mean_gpc, var_gpc\n\n    # --- Main Loop ---\n    test_cases = [\n        {'id': 1, 'dim': 1, 'dist': 'normal', 'coeffs': {'a0': 1.3, 'a1': -0.7, 'a2': 0.5}},\n        {'id': 2, 'dim': 1, 'dist': 'uniform', 'coeffs': {'a0': 0.2, 'a1': 1.1, 'a2': -0.5}},\n        {'id': 3, 'dim': 2, 'dist': 'normal', 'coeffs': {'c0': 0.9, 'c1': 0.2, 'c2': -0.4, 'c3': 0.3, 'c4': 0.1, 'c5': 0.25}},\n        {'id': 4, 'dim': 2, 'dist': 'uniform', 'coeffs': {'c0': -0.5, 'c1': 0.6, 'c2': 0.1, 'c3': -0.2, 'c4': 0.7, 'c5': 0.3}},\n        {'id': 5, 'dim': 1, 'dist': 'normal', 'coeffs': {'a0': 2.0, 'a1': 0.0, 'a2': 0.0}},\n        {'id': 6, 'dim': 1, 'dist': 'uniform', 'coeffs': {'a0': 0.0, 'a1': 1.0, 'a2': 0.0}},\n    ]\n\n    results = []\n    for case in test_cases:\n        mean_analytical, var_analytical = get_analytical_stats(case)\n        mean_gpc, var_gpc = get_gpc_stats(case)\n\n        mean_match = np.isclose(mean_analytical, mean_gpc, atol=TOLERANCE, rtol=0)\n        var_match = np.isclose(var_analytical, var_gpc, atol=TOLERANCE, rtol=0)\n        \n        results.append(mean_match and var_match)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Having validated your tools, we now move to a more challenging and realistic scenario: modeling a highly nonlinear system. This practice involves approximating a log-normal random variable, which arises from an exponential transformation of a Gaussian input, and assessing how well the PCE captures its pronounced asymmetry by calculating the skewness . This exercise demonstrates the power of PCE in representing non-Gaussian outputs and introduces a key analytical method for deriving coefficients.",
            "id": "3174337",
            "problem": "You are given a scalar random input $\\,\\xi \\sim \\mathcal{N}(0,1)\\,$ and a log-normal output defined by $\\,u(\\xi) = \\exp(\\sigma \\,\\xi)\\,$, where $\\,\\sigma > 0\\,$ is a fixed parameter. Consider a Polynomial Chaos Expansion (PCE) using the probabilists’ Hermite polynomials $\\,\\mathrm{He}_n(\\xi)\\,$ as the orthogonal basis with respect to the standard normal weight, and define the orthonormal basis functions $\\,\\psi_n(\\xi) = \\mathrm{He}_n(\\xi)/\\sqrt{n!}\\,$. The PCE truncated at total degree $\\,p \\in \\mathbb{N}\\,$ is\n$$\nU_p(\\xi) \\;=\\; \\sum_{n=0}^{p} c_n \\, \\psi_n(\\xi)\n$$.\nYour tasks are:\n- Starting only from core definitions and well-tested facts, derive expressions for the coefficients $\\,c_n\\,$ of the expansion of $\\,u(\\xi)\\,$ in this orthonormal basis. Allowed starting points include the orthogonality of $\\,\\mathrm{He}_n(\\xi)\\,$ with respect to the Gaussian measure and the generating-function characterization of $\\,\\mathrm{He}_n(\\xi)\\,$. Do not assume any specialized PCE formula as a given.\n- Using the truncated expansion $\\,U_p(\\xi)\\,$, estimate its skewness, defined as\n$$\n\\gamma(U_p) \\;=\\; \\frac{\\mathbb{E}\\!\\left[\\left(U_p - \\mathbb{E}[U_p]\\right)^3\\right]}{\\left(\\mathbb{V}\\mathrm{ar}[U_p]\\right)^{3/2}},\n$$\nwhere $\\,\\mathbb{E}[\\cdot]\\,$ denotes expectation and $\\,\\mathbb{V}\\mathrm{ar}[\\cdot]\\,$ denotes variance.\n- For reference, the exact skewness of the true log-normal output $\\,u(\\xi)\\,$ with parameters $\\,\\mu = 0\\,$ and $\\,\\sigma^2\\,$ is to be computed from first principles of the log-normal distribution, without using any PCE-specific shortcuts.\n\nImplementation requirements:\n- To compute expectations $\\,\\mathbb{E}[f(\\xi)]\\,$ under the standard normal law, use Gauss–Hermite quadrature with sufficient nodes to exactly integrate polynomials multiplied by the Gaussian weight. You must use the standard mapping from Gauss–Hermite nodes and weights for the weight $\\,\\exp(-x^2)\\,$ to the standard normal expectation:\n$$\n\\mathbb{E}[f(\\xi)] \\;=\\; \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{N} w_i \\, f\\!\\left(\\sqrt{2}\\,x_i\\right),\n$$\nwhere $\\,(x_i,w_i)\\,$ are the nodes and weights of the $\\,N$-point Gauss–Hermite rule for the weight $\\,\\exp(-x^2)\\,$. Choose $\\,N\\,$ large enough so that moments of $\\,U_p(\\xi)\\,$ up to order $\\,3\\,$ are integrated exactly when $\\,U_p(\\xi)\\,$ is a polynomial of degree $\\,p\\,$.\n- Implement $\\,\\mathrm{He}_n(\\xi)\\,$ via the recurrence $\\,\\mathrm{He}_0(\\xi)=1\\,$, $\\,\\mathrm{He}_1(\\xi)=\\xi\\,$, $\\,\\mathrm{He}_{n+1}(\\xi)=\\xi\\,\\mathrm{He}_n(\\xi)-n\\,\\mathrm{He}_{n-1}(\\xi)\\,$, and then form $\\,\\psi_n(\\xi) = \\mathrm{He}_n(\\xi)/\\sqrt{n!}\\,$.\n- Compute and report the absolute error between the skewness of the truncated PCE approximation $\\,\\gamma(U_p)\\,$ and the exact skewness of $\\,u(\\xi)\\,$.\n\nTest suite:\n- Use the following five test cases, each specified by a pair $\\;(\\sigma,p)\\;$:\n  - Case $\\,1$: $\\;\\sigma = 0.5,\\; p = 1$\n  - Case $\\,2$: $\\;\\sigma = 0.5,\\; p = 3$\n  - Case $\\,3$: $\\;\\sigma = 1.0,\\; p = 3$\n  - Case $\\,4$: $\\;\\sigma = 1.0,\\; p = 7$\n  - Case $\\,5$: $\\;\\sigma = 0.25,\\; p = 1$\n- For each case, compute a single float: the absolute error $\\,\\left|\\gamma(U_p) - \\gamma\\!\\left(u(\\xi)\\right)\\right|\\,$.\n\nFinal output format:\n- Your program must produce a single line containing the list of results for the five cases in order, as a comma-separated list enclosed in square brackets, for example $\\,\\big[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\big]\\,$.\n- Each result must be rounded to exactly $\\,8\\,$ decimal places.\n- No other text must be printed. There are no physical units in this problem, and angles are not involved; all outputs are dimensionless real numbers.",
            "solution": "The problem is valid. It is scientifically grounded in the theory of polynomial chaos expansions and numerical analysis, well-posed with a clear objective, and provides all necessary definitions, parameters, and constraints for a unique and verifiable solution.\n\nThe solution is developed in three main parts: 1) derivation of the analytical expression for the Polynomial Chaos Expansion (PCE) coefficients, 2) derivation of the analytical expression for the exact skewness of the log-normal distribution, and 3) specification of the numerical algorithm to compute the skewness of the truncated PCE approximation and the final absolute error.\n\n**1. Derivation of PCE Coefficients**\n\nThe model output is $u(\\xi) = \\exp(\\sigma \\xi)$, where $\\xi \\sim \\mathcal{N}(0,1)$ is a standard normal random variable. The PCE is performed using the orthonormal basis $\\psi_n(\\xi) = \\mathrm{He}_n(\\xi)/\\sqrt{n!}$, where $\\mathrm{He}_n(\\xi)$ are the probabilists' Hermite polynomials. The truncated expansion is $U_p(\\xi) = \\sum_{n=0}^{p} c_n \\psi_n(\\xi)$.\n\nThe coefficients $c_n$ are determined by projecting the function $u(\\xi)$ onto the basis functions $\\psi_n(\\xi)$. The inner product is defined with respect to the standard normal probability measure, which corresponds to the expectation operator $\\mathbb{E}[\\cdot]$.\n$$\nc_n = \\mathbb{E}[u(\\xi) \\psi_n(\\xi)] = \\mathbb{E}\\left[\\exp(\\sigma \\xi) \\frac{\\mathrm{He}_n(\\xi)}{\\sqrt{n!}}\\right] = \\frac{1}{\\sqrt{n!}} \\mathbb{E}[\\exp(\\sigma \\xi) \\mathrm{He}_n(\\xi)]\n$$\nTo evaluate the expectation, we use the generating function for the probabilists' Hermite polynomials:\n$$\nG(\\xi, t) = \\sum_{n=0}^{\\infty} \\frac{\\mathrm{He}_n(\\xi)}{n!} t^n = \\exp\\left(\\xi t - \\frac{t^2}{2}\\right)\n$$\nWe compute the expectation of the product of $u(\\xi)$ and the generating function:\n$$\n\\mathbb{E}[u(\\xi) G(\\xi, t)] = \\mathbb{E}\\left[\\exp(\\sigma \\xi) \\sum_{n=0}^{\\infty} \\frac{\\mathrm{He}_n(\\xi)}{n!} t^n\\right] = \\sum_{n=0}^{\\infty} \\frac{\\mathbb{E}[\\exp(\\sigma \\xi) \\mathrm{He}_n(\\xi)]}{n!} t^n\n$$\nThe left side can be evaluated directly:\n$$\n\\mathbb{E}[\\exp(\\sigma \\xi) G(\\xi, t)] = \\mathbb{E}\\left[\\exp(\\sigma \\xi) \\exp\\left(\\xi t - \\frac{t^2}{2}\\right)\\right] = \\exp\\left(-\\frac{t^2}{2}\\right) \\mathbb{E}\\left[\\exp((\\sigma+t)\\xi)\\right]\n$$\nThe expectation term $\\mathbb{E}[\\exp(s\\xi)]$ is the moment-generating function (MGF) of a standard normal random variable $\\xi$, which is $M_{\\xi}(s) = \\exp(s^2/2)$. Setting $s = \\sigma+t$:\n$$\n\\mathbb{E}\\left[\\exp((\\sigma+t)\\xi)\\right] = M_{\\xi}(\\sigma+t) = \\exp\\left(\\frac{(\\sigma+t)^2}{2}\\right) = \\exp\\left(\\frac{\\sigma^2 + 2\\sigma t + t^2}{2}\\right)\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}[u(\\xi) G(\\xi, t)] = \\exp\\left(-\\frac{t^2}{2}\\right) \\exp\\left(\\frac{\\sigma^2}{2} + \\sigma t + \\frac{t^2}{2}\\right) = \\exp\\left(\\frac{\\sigma^2}{2} + \\sigma t\\right)\n$$\nWe expand this result as a Taylor series in $t$:\n$$\n\\exp\\left(\\frac{\\sigma^2}{2}\\right) \\exp(\\sigma t) = \\exp\\left(\\frac{\\sigma^2}{2}\\right) \\sum_{n=0}^{\\infty} \\frac{(\\sigma t)^n}{n!} = \\sum_{n=0}^{\\infty} \\left(\\exp\\left(\\frac{\\sigma^2}{2}\\right) \\frac{\\sigma^n}{n!}\\right) t^n\n$$\nBy equating the coefficients of $t^n$ in the two series expansions for $\\mathbb{E}[u(\\xi) G(\\xi, t)]$, we find:\n$$\n\\frac{\\mathbb{E}[\\exp(\\sigma \\xi) \\mathrm{He}_n(\\xi)]}{n!} = \\exp\\left(\\frac{\\sigma^2}{2}\\right) \\frac{\\sigma^n}{n!} \\implies \\mathbb{E}[\\exp(\\sigma \\xi) \\mathrm{He}_n(\\xi)] = \\sigma^n \\exp\\left(\\frac{\\sigma^2}{2}\\right)\n$$\nFinally, substituting this into the formula for $c_n$:\n$$\nc_n = \\frac{1}{\\sqrt{n!}} \\left(\\sigma^n \\exp\\left(\\frac{\\sigma^2}{2}\\right)\\right) = \\frac{\\sigma^n \\exp(\\sigma^2/2)}{\\sqrt{n!}}\n$$\n\n**2. Exact Skewness of the Log-Normal Output**\n\nThe output $u(\\xi) = \\exp(\\sigma \\xi)$ follows a log-normal distribution, as it is the exponential of a normal random variable $\\sigma\\xi \\sim \\mathcal{N}(0, \\sigma^2)$. The skewness is computed from the first three moments of $u$. The $k$-th raw moment of $u$ is found using the MGF of $\\xi$:\n$$\nM_k = \\mathbb{E}[u^k] = \\mathbb{E}[\\exp(k\\sigma\\xi)] = M_{\\xi}(k\\sigma) = \\exp\\left(\\frac{(k\\sigma)^2}{2}\\right) = \\exp\\left(\\frac{k^2\\sigma^2}{2}\\right)\n$$\nThe first three raw moments are:\n- $M_1 = \\mathbb{E}[u] = \\exp(\\sigma^2/2)$\n- $M_2 = \\mathbb{E}[u^2] = \\exp(2\\sigma^2)$\n- $M_3 = \\mathbb{E}[u^3] = \\exp(9\\sigma^2/2)$\n\nThe mean, variance, and third central moment are:\n- Mean: $\\mathbb{E}[u] = M_1$\n- Variance: $\\mathbb{V}\\mathrm{ar}[u] = M_2 - M_1^2 = \\exp(2\\sigma^2) - (\\exp(\\sigma^2/2))^2 = \\exp(2\\sigma^2) - \\exp(\\sigma^2) = \\exp(\\sigma^2)(\\exp(\\sigma^2) - 1)$\n- Third central moment: $\\mu_3(u) = M_3 - 3M_2M_1 + 2M_1^3 = \\exp(9\\sigma^2/2) - 3\\exp(2\\sigma^2)\\exp(\\sigma^2/2) + 2(\\exp(\\sigma^2/2))^3 = \\exp(9\\sigma^2/2) - 3\\exp(5\\sigma^2/2) + 2\\exp(3\\sigma^2/2)$.\nFactoring $\\mu_3(u)$ by letting $K = \\exp(\\sigma^2)$ yields: $K^{3/2}(K^3 - 3K + 2) = K^{3/2}(K-1)^2(K+2)$.\n\nThe exact skewness $\\gamma(u)$ is:\n$$\n\\gamma(u) = \\frac{\\mu_3(u)}{(\\mathbb{V}\\mathrm{ar}[u])^{3/2}} = \\frac{\\exp(3\\sigma^2/2)(\\exp(\\sigma^2)-1)^2(\\exp(\\sigma^2)+2)}{(\\exp(\\sigma^2)(\\exp(\\sigma^2)-1))^{3/2}} = (\\exp(\\sigma^2)+2)\\sqrt{\\exp(\\sigma^2)-1}\n$$\n\n**3. Skewness of the PCE Approximation**\n\nThe skewness of the PCE approximation, $\\gamma(U_p)$, is defined as:\n$$\n\\gamma(U_p) = \\frac{\\mathbb{E}[(U_p - \\mathbb{E}[U_p])^3]}{(\\mathbb{V}\\mathrm{ar}[U_p])^{3/2}}\n$$\nThe moments of $U_p(\\xi)$ are computed using Gauss-Hermite quadrature as specified. The expectation of a function $f(\\xi)$ is approximated by:\n$$\n\\mathbb{E}[f(\\xi)] \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^{N} w_i f(\\sqrt{2}x_i)\n$$\nwhere $(x_i, w_i)$ are the nodes and weights for the weight function $\\exp(-x^2)$. An $N$-point Gauss-Hermite rule exactly integrates polynomials of degree up to $2N-1$. To compute the third central moment of $U_p(\\xi)$, we need its first three raw moments, which involve integrating polynomials of degree up to $3p$. Thus, we need $2N-1 \\ge 3p$, which implies choosing $N \\ge (3p+1)/2$. The smallest integer $N$ satisfying this is $N = \\lceil (3p+1)/2 \\rceil$.\n\nThe algorithmic procedure is as follows:\n1.  For a given pair $(\\sigma, p)$, calculate the coefficients $c_n$ for $n = 0, \\ldots, p$ using the derived formula.\n2.  Determine the required number of quadrature points $N = \\lceil (3p+1)/2 \\rceil$.\n3.  Obtain the $N$ Gauss-Hermite nodes $x_i$ and weights $w_i$.\n4.  Transform the nodes to correspond to the standard normal measure: $\\xi_i = \\sqrt{2}x_i$. The quadrature weights are scaled by $1/\\sqrt{\\pi}$.\n5.  At each node $\\xi_i$, evaluate the truncated expansion $U_p(\\xi_i) = \\sum_{n=0}^{p} c_n \\psi_n(\\xi_i)$. The orthonormal basis functions $\\psi_n(\\xi_i) = \\mathrm{He}_n(\\xi_i)/\\sqrt{n!}$ are computed by first generating the Hermite polynomials $\\mathrm{He}_n(\\xi_i)$ up to order $p$ using the recurrence relation: $\\mathrm{He}_0(\\xi)=1$, $\\mathrm{He}_1(\\xi)=\\xi$, and $\\mathrm{He}_{n+1}(\\xi)=\\xi\\,\\mathrm{He}_n(\\xi)-n\\,\\mathrm{He}_{n-1}(\\xi)$.\n6.  Compute the first three raw moments of $U_p$ using the quadrature rule:\n    - $M_{1,p} = \\mathbb{E}[U_p] = \\frac{1}{\\sqrt{\\pi}} \\sum_i w_i U_p(\\xi_i)$\n    - $M_{2,p} = \\mathbb{E}[U_p^2] = \\frac{1}{\\sqrt{\\pi}} \\sum_i w_i (U_p(\\xi_i))^2$\n    - $M_{3,p} = \\mathbb{E}[U_p^3] = \\frac{1}{\\sqrt{\\pi}} \\sum_i w_i (U_p(\\xi_i))^3$\n7.  Calculate the variance and third central moment of $U_p$:\n    - $\\mathbb{V}\\mathrm{ar}[U_p] = M_{2,p} - M_{1,p}^2$\n    - $\\mu_3(U_p) = M_{3,p} - 3M_{2,p}M_{1,p} + 2M_{1,p}^3$\n8.  Compute the skewness of the approximation: $\\gamma(U_p) = \\mu_3(U_p) / (\\mathbb{V}\\mathrm{ar}[U_p])^{3/2}$.\n\n**4. Absolute Error**\nThe final result for each test case is the absolute error between the approximated skewness and the exact skewness: $|\\gamma(U_p) - \\gamma(u)|$.",
            "answer": "```python\nimport numpy as np\nfrom numpy.polynomial.hermite import hermgauss\nfrom math import factorial, sqrt, exp, ceil\n\ndef solve():\n    \"\"\"\n    Computes the absolute error between the exact skewness of a log-normal\n    variable and the skewness of its Polynomial Chaos Expansion (PCE) approximation.\n    \"\"\"\n    test_cases = [\n        (0.5, 1),\n        (0.5, 3),\n        (1.0, 3),\n        (1.0, 7),\n        (0.25, 1),\n    ]\n\n    results = []\n    for sigma, p in test_cases:\n        # 1. Compute the exact skewness of the log-normal distribution u(xi) = exp(sigma * xi)\n        # where xi ~ N(0,1). The underlying normal distribution for u has mean 0 and\n        # variance sigma^2.\n        exp_sigma_sq = exp(sigma**2)\n        exact_skewness = (exp_sigma_sq + 2.0) * sqrt(exp_sigma_sq - 1.0)\n        \n        # 2. Compute the skewness of the PCE approximation U_p(xi)\n        \n        # 2.1 Calculate PCE coefficients c_n = sigma^n * exp(sigma^2/2) / sqrt(n!)\n        coeffs = np.zeros(p + 1)\n        exp_sigma_sq_half = exp(sigma**2 / 2.0)\n        for n in range(p + 1):\n            coeffs[n] = (sigma**n * exp_sigma_sq_half) / sqrt(factorial(n))\n\n        # 2.2 Determine quadrature order and get nodes/weights.\n        # An N-point Gauss-Hermite rule is exact for polynomials up to degree 2N-1.\n        # To compute E[U_p^3], a polynomial of degree 3p, we need 2N-1 >= 3p.\n        # So, N >= (3p+1)/2.\n        N = ceil((3 * p + 1) / 2.0)\n        nodes_x, weights_w = hermgauss(N)\n\n        # 2.3 Map nodes/weights for standard normal expectation E[f(xi)].\n        # E[f(xi)] = (1/sqrt(pi)) * integral(f(sqrt(2)x) * exp(-x^2) dx)\n        nodes_xi = sqrt(2.0) * nodes_x\n        quad_weights = weights_w / sqrt(np.pi)\n\n        # 2.4 Evaluate U_p at each quadrature node.\n        # U_p(xi) = sum_{n=0 to p} c_n * psi_n(xi), where psi_n = He_n / sqrt(n!)\n        # We pre-compute Hermite polynomial values He_n(xi) for all nodes.\n        He_vals = []\n        He_vals.append(np.ones_like(nodes_xi)) # He_0(xi) = 1\n        if p > 0:\n            He_vals.append(np.copy(nodes_xi)) # He_1(xi) = xi\n        for n in range(1, p):\n            # Recurrence: He_{n+1}(xi) = xi * He_n(xi) - n * He_{n-1}(xi)\n            He_next = nodes_xi * He_vals[n] - n * He_vals[n-1]\n            He_vals.append(He_next)\n        \n        # Sum the series expansion to get U_p values at nodes\n        Up_values = np.zeros_like(nodes_xi)\n        for n in range(p + 1):\n            psi_n_vals = He_vals[n] / sqrt(factorial(n))\n            Up_values += coeffs[n] * psi_n_vals\n\n        # 2.5 Compute first three raw moments of U_p using quadrature\n        M1 = np.sum(quad_weights * Up_values)\n        M2 = np.sum(quad_weights * Up_values**2)\n        M3 = np.sum(quad_weights * Up_values**3)\n\n        # 2.6 Compute skewness from moments\n        var_Up = M2 - M1**2\n        \n        pce_skewness = 0.0\n        # Check for non-positive variance due to floating-point errors\n        if var_Up > 1e-15:\n            mu3_Up = M3 - 3.0 * M2 * M1 + 2.0 * M1**3\n            pce_skewness = mu3_Up / (var_Up**1.5)\n\n        # 3. Compute the absolute error\n        abs_error = abs(pce_skewness - exact_skewness)\n        results.append(abs_error)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The power of PCE rests on a key assumption: the orthogonality of the polynomial basis with respect to the input's probability measure. This exercise confronts a common practical pitfall where this assumption is violated, particularly when coefficients are estimated from data using regression . By comparing coefficients from a 'matched' projection with those from a 'mismatched' regression, you will gain a crucial, first-hand understanding of the systematic bias that can arise and the importance of choosing an appropriate basis.",
            "id": "3174362",
            "problem": "You will write a complete program to empirically demonstrate how an orthogonality weight mismatch affects the coefficients of a Polynomial Chaos Expansion (PCE). Consider the Legendre polynomial basis, which is orthogonal on the interval $[-1,1]$ with respect to the uniform weight. You will compare two coefficient vectors for a truncated expansion of order $p$: one obtained by orthogonal projection under the uniform weight and one obtained by unweighted least squares regression using samples drawn from a non-uniform input distribution.\n\nUse the following fundamental base:\n- The Legendre polynomials $\\{P_n(x)\\}_{n=0}^{\\infty}$ are orthogonal on $[-1,1]$ with respect to the inner product $\\langle f,g\\rangle = \\int_{-1}^{1} f(x) g(x)\\,dx$, that is,\n$$\\int_{-1}^{1} P_m(x)\\,P_n(x)\\,dx = \\frac{2}{2n+1}\\,\\delta_{mn}.$$\n- A truncated Polynomial Chaos Expansion (PCE) of order $p$ approximates a scalar response $y(x)$ as\n$$y(x)\\approx \\sum_{n=0}^{p} c_n\\,P_n(x),$$\nwhere the coefficients $\\{c_n\\}_{n=0}^{p}$ are determined by an appropriate optimality criterion.\n\nThe task is to quantify the systematic coefficient bias introduced when the data are generated from a non-uniform input distribution but the Legendre basis (appropriate for the uniform weight) is still used with unweighted least squares. The program must implement the following steps for each test case:\n1. Define the function $y(x)$ as specified by the test case.\n2. Compute the coefficient vector $\\boldsymbol{c}^{\\star} = (c_0^{\\star},\\ldots,c_p^{\\star})$ corresponding to the orthogonal projection of $y(x)$ onto $\\{P_n(x)\\}_{n=0}^{p}$ under the uniform weight on $[-1,1]$. To do this, approximate the required integrals numerically using Gaussian–Legendre quadrature with at least $M=500$ nodes over $[-1,1]$.\n3. Independently, generate $N$ independent and identically distributed samples from a Beta distribution on $[0,1]$ with shape parameters $(a,b)$, then transform them to $[-1,1]$ via $X = 2U - 1$. Use these inputs to compute output samples $Y = y(X)$, form the design matrix with columns $P_n(X)$ for $n=0,\\ldots,p$, and compute the unweighted least squares estimate $\\widehat{\\boldsymbol{c}}$ solving the normal equations implicitly via a numerically stable solver. Use a fixed random seed $s=12345$ for reproducibility.\n4. Report the scalar bias magnitude defined as the Euclidean norm\n$$\\|\\widehat{\\boldsymbol{c}} - \\boldsymbol{c}^{\\star}\\|_2 = \\left(\\sum_{n=0}^{p} \\left(\\widehat{c}_n - c_n^{\\star}\\right)^2\\right)^{1/2}.$$\n\nImplement the following test suite of parameter values to cover different facets:\n- Case $1$ (happy path, pronounced mismatch): $y(x)=\\exp(x)$, $p=3$, $N=400$, $(a,b)=(2,5)$.\n- Case $2$ (no mismatch in distribution): $y(x)=\\exp(x)$, $p=3$, $N=400$, $(a,b)=(1,1)$ (uniform on $[-1,1]$ after transformation).\n- Case $3$ (different nonlinearity and mismatch): $y(x)=\\sin(x)$ with $x$ in radians, $p=5$, $N=800$, $(a,b)=(5,2)$.\n- Case $4$ (model well-specified within span): $y(x)=1 + 0.2\\,x + 0.3\\,x^2$, $p=5$, $N=200$, $(a,b)=(2,5)$.\n\nYour program should produce a single line of output containing the bias magnitudes for the four cases as a comma-separated list of floats rounded to six decimal places, enclosed in square brackets, in the same order as above, for example\n$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$.\nNo user input is required. All angles are in radians. No physical units are involved.",
            "solution": "The problem requires an empirical investigation into the bias of Polynomial Chaos Expansion (PCE) coefficients that arises when the sampling distribution of the input data does not match the orthogonality weight function of the chosen polynomial basis. We will compare coefficients derived from two distinct methods: a theoretically optimal orthogonal projection and a data-driven least squares regression.\n\nThe basis functions are the Legendre polynomials $\\{P_n(x)\\}_{n=0}^{\\infty}$, which are orthogonal on the interval $[-1, 1]$ with respect to the uniform weight function, $w(x) = 1$. The orthogonality condition is given by:\n$$\n\\int_{-1}^{1} P_m(x) P_n(x) \\,dx = \\frac{2}{2n+1}\\delta_{mn}\n$$\nwhere $\\delta_{mn}$ is the Kronecker delta.\n\nA function $y(x)$ can be approximated by a truncated PCE of order $p$:\n$$\ny(x) \\approx y_p(x) = \\sum_{n=0}^{p} c_n P_n(x)\n$$\n\nWe will compute two sets of coefficients for this expansion, $\\boldsymbol{c}^{\\star}$ and $\\widehat{\\boldsymbol{c}}$.\n\n**1. Orthogonal Projection Coefficients ($\\boldsymbol{c}^{\\star}$)**\n\nThe coefficients $\\boldsymbol{c}^{\\star} = (c_0^{\\star}, \\dots, c_p^{\\star})$ are determined by projecting the function $y(x)$ onto the basis $\\{P_n(x)\\}_{n=0}^{p}$ in the function space $L^2([-1,1])$. The coefficients that minimize the weighted squared error $\\int_{-1}^{1} [y(x) - y_p(x)]^2 w(x) \\,dx$ with $w(x)=1$ are given by the orthogonal projection formula:\n$$\nc_n^{\\star} = \\frac{\\langle y, P_n \\rangle}{\\langle P_n, P_n \\rangle} = \\frac{\\int_{-1}^{1} y(x) P_n(x) \\,dx}{\\int_{-1}^{1} P_n(x)^2 \\,dx}\n$$\nSubstituting the known value of the inner product $\\langle P_n, P_n \\rangle = \\frac{2}{2n+1}$, we obtain the precise formula for each coefficient:\n$$\nc_n^{\\star} = \\frac{2n+1}{2} \\int_{-1}^{1} y(x) P_n(x) \\,dx\n$$\nTo compute these coefficients, we must numerically approximate the integral. The problem specifies using Gaussian-Legendre quadrature with $M \\geq 500$ nodes. Let $\\{x_i\\}_{i=1}^M$ be the quadrature nodes and $\\{w_i\\}_{i=1}^M$ be the corresponding weights on the interval $[-1, 1]$. The integral is then approximated as a weighted sum:\n$$\n\\int_{-1}^{1} f(x) \\,dx \\approx \\sum_{i=1}^{M} w_i f(x_i)\n$$\nApplying this to our coefficient formula, we get the numerical estimate:\n$$\nc_n^{\\star} \\approx \\frac{2n+1}{2} \\sum_{i=1}^{M} w_i y(x_i) P_n(x_i)\n$$\nThese coefficients serve as the \"true\" or ideal coefficients for a PCE based on Legendre polynomials.\n\n**2. Least Squares Regression Coefficients ($\\widehat{\\boldsymbol{c}}$)**\n\nThe second set of coefficients, $\\widehat{\\boldsymbol{c}} = (\\widehat{c}_0, \\dots, \\widehat{c}_p)$, is estimated from a finite set of $N$ input-output samples, $\\{(X_j, Y_j)\\}_{j=1}^N$. The input samples $X_j$ are not drawn from a uniform distribution on $[-1, 1]$. Instead, they are generated by drawing $U_j$ from a Beta distribution with parameters $(a,b)$ on $[0,1]$ and then applying an affine transformation $X_j = 2U_j - 1$. The output samples are $Y_j = y(X_j)$.\n\nThe coefficients $\\widehat{\\boldsymbol{c}}$ are found by solving an unweighted linear least squares problem. We seek to minimize the sum of squared residuals:\n$$\n\\min_{\\widehat{\\boldsymbol{c}}} \\sum_{j=1}^{N} \\left( Y_j - \\sum_{n=0}^{p} \\widehat{c}_n P_n(X_j) \\right)^2\n$$\nThis can be expressed in matrix form as minimizing $\\|\\boldsymbol{Y} - \\boldsymbol{\\Psi}\\widehat{\\boldsymbol{c}}\\|_2^2$, where:\n- $\\boldsymbol{Y} = [Y_1, Y_2, \\dots, Y_N]^T$ is the vector of output samples.\n- $\\boldsymbol{\\Psi}$ is the $N \\times (p+1)$ design matrix with entries $\\Psi_{jn} = P_n(X_j)$.\n- $\\widehat{\\boldsymbol{c}} = [\\widehat{c}_0, \\widehat{c}_1, \\dots, \\widehat{c}_p]^T$ is the vector of coefficients to be determined.\n\nThe solution is found by solving the normal equations, $(\\boldsymbol{\\Psi}^T \\boldsymbol{\\Psi}) \\widehat{\\boldsymbol{c}} = \\boldsymbol{\\Psi}^T \\boldsymbol{Y}$. For numerical stability, this is best solved using methods based on QR decomposition or singular value decomposition, as implemented in standard numerical libraries.\n\n**3. The Source of Bias**\n\nThe least squares estimate $\\widehat{\\boldsymbol{c}}$ converges to the coefficients that are optimal with respect to the actual sampling distribution of $X$. If the probability density function of $X$ is $f_X(x)$, the least squares procedure is a discrete approximation of minimizing the error in a weighted $L^2$-norm with weight $f_X(x)$.\n\nA systematic bias $\\widehat{\\boldsymbol{c}} - \\boldsymbol{c}^{\\star}$ arises precisely because the weight function of the sampling distribution, $f_X(x)$, does not match the weight function for which the Legendre basis is orthogonal, $w(x) = 1$. The only exception is Case $2$, where the Beta distribution parameters are $(a,b)=(1,1)$, which corresponds to a uniform distribution on $[0,1]$. After transformation, this yields a uniform distribution on $[-1,1]$, so $f_X(x) \\propto w(x)$, and the bias is expected to be minimal, attributable only to finite sample variance. For other cases where $(a,b) \\neq (1,1)$, a non-uniform sampling density $f_X(x)$ is produced, leading to a demonstrable systematic bias.\n\nThe magnitude of this bias is quantified by the Euclidean norm of the difference vector:\n$$\n\\|\\widehat{\\boldsymbol{c}} - \\boldsymbol{c}^{\\star}\\|_2 = \\sqrt{\\sum_{n=0}^{p} (\\widehat{c}_n - c_n^{\\star})^2}\n$$\n\n**Algorithm Summary**\nThe program will execute the following steps for each of the four test cases:\n1.  Define the target function $y(x)$ and parameters $p, N, a, b$.\n2.  Compute the vector $\\boldsymbol{c}^{\\star}$ by applying the Gaussian-Legendre quadrature formula with $M=500$ nodes.\n3.  Set the random seed to $s=12345$ for reproducibility.\n4.  Generate $N$ samples from the Beta$(a,b)$ distribution, transform them to the interval $[-1,1]$, and evaluate $y(x)$ to get the sample pairs.\n5.  Construct the design matrix $\\boldsymbol{\\Psi}$ using the generated input samples and evaluations of Legendre polynomials up to order $p$.\n6.  Solve the linear least squares problem $\\boldsymbol{Y} \\approx \\boldsymbol{\\Psi}\\widehat{\\boldsymbol{c}}$ to find the coefficient vector $\\widehat{\\boldsymbol{c}}$.\n7.  Calculate the Euclidean norm $\\|\\widehat{\\boldsymbol{c}} - \\boldsymbol{c}^{\\star}\\|_2$.\n8.  Collect the results and format the output as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import eval_legendre\nfrom numpy.polynomial.legendre import leggauss\n\ndef compute_c_star(y_func, p: int, M: int) -> np.ndarray:\n    \"\"\"\n    Computes PCE coefficients via orthogonal projection using Gaussian-Legendre quadrature.\n\n    Args:\n        y_func: The function y(x) to be expanded.\n        p: The maximum order of the PCE.\n        M: The number of quadrature nodes to use.\n\n    Returns:\n        A numpy array containing the coefficients c_star.\n    \"\"\"\n    nodes, weights = leggauss(M)\n    c_star = np.zeros(p + 1)\n    \n    # Evaluate the function y at all quadrature nodes once\n    y_vals = y_func(nodes)\n    \n    for n in range(p + 1):\n        # Evaluate the nth Legendre polynomial at the nodes\n        p_n_vals = eval_legendre(n, nodes)\n        \n        # Approximate the integral using quadrature\n        integrand_values = y_vals * p_n_vals\n        integral = np.sum(weights * integrand_values)\n        \n        # Apply the projection formula\n        c_star[n] = (2 * n + 1) / 2.0 * integral\n        \n    return c_star\n\ndef compute_c_hat(y_func, p: int, N: int, a: float, b: float, seed: int) -> np.ndarray:\n    \"\"\"\n    Computes PCE coefficients via unweighted least squares from samples.\n\n    Args:\n        y_func: The function y(x) to generate samples.\n        p: The maximum order of the PCE.\n        N: The number of samples.\n        a: The alpha parameter of the Beta distribution.\n        b: The beta parameter of the Beta distribution.\n        seed: The random seed for reproducibility.\n\n    Returns:\n        A numpy array containing the estimated coefficients c_hat.\n    \"\"\"\n    # Generate samples from the specified distribution\n    rng = np.random.default_rng(seed)\n    U = rng.beta(a, b, size=N)\n    X = 2.0 * U - 1.0  # Transform samples from [0, 1] to [-1, 1]\n    Y = y_func(X)\n    \n    # Construct the design matrix Psi\n    Psi = np.zeros((N, p + 1))\n    for n in range(p + 1):\n        Psi[:, n] = eval_legendre(n, X)\n        \n    # Solve the least squares problem\n    c_hat, _, _, _ = np.linalg.lstsq(Psi, Y, rcond=None)\n    \n    return c_hat\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define constants\n    QUADRATURE_NODES = 500\n    RANDOM_SEED = 12345\n    \n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (y_function, order_p, num_samples_N, beta_a, beta_b)\n    test_cases = [\n        (lambda x: np.exp(x), 3, 400, 2, 5),\n        (lambda x: np.exp(x), 3, 400, 1, 1),\n        (lambda x: np.sin(x), 5, 800, 5, 2),\n        (lambda x: 1.0 + 0.2 * x + 0.3 * x**2, 5, 200, 2, 5)\n    ]\n\n    results = []\n    for case in test_cases:\n        y_func, p, N, a, b = case\n        \n        # 1. Compute the theoretical coefficients c_star via quadrature\n        c_star = compute_c_star(y_func, p, QUADRATURE_NODES)\n        \n        # 2. Compute the estimated coefficients c_hat via least squares\n        c_hat = compute_c_hat(y_func, p, N, a, b, RANDOM_SEED)\n        \n        # 3. Calculate the bias magnitude (Euclidean norm of the difference)\n        bias = np.linalg.norm(c_hat - c_star)\n        \n        results.append(bias)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}