{
    "hands_on_practices": [
        {
            "introduction": "To ground our understanding of sensitivity, we begin with a concrete, analytical example from electrical engineering. The performance of electronic circuits, such as radios and filters, depends critically on their resonance frequency. This practice explores how manufacturing tolerances in a component like a capacitor affect this frequency, introducing the powerful and practical concept of normalized sensitivity. By working through this problem, you will see how a straightforward application of calculus can reveal a simple, constant relationship governing the circuit's behavior .",
            "id": "3272518",
            "problem": "A Resistor–Inductor–Capacitor (RLC) series circuit is driven in sinusoidal steady state by a source at angular frequency $\\omega$. The impedance of the inductor is $j \\omega L$, the impedance of the capacitor is $\\frac{1}{j \\omega C}$, and the impedance of the resistor is $R$, where $j$ denotes the imaginary unit, $L$ is the inductance, $C$ is the capacitance, and $R$ is the resistance. In the usual engineering sense of resonance for a series RLC network, the resonance angular frequency $\\omega_{0}$ is defined as the frequency at which the net reactance is zero, so that the impedance is purely real. \n\nManufacturing tolerance in the capacitor means that the realized capacitance is $C + \\delta C$ with $|\\delta C| \\ll C$. Using first principles (the impedance definition and the resonance condition), derive the resonance angular frequency $\\omega_{0}$ as a function of $L$ and $C$, and then from the total differential define the normalized sensitivity of $\\omega_{0}$ with respect to $C$ as\n$$\nS_{C} \\equiv \\frac{C}{\\omega_{0}} \\frac{\\partial \\omega_{0}}{\\partial C}.\n$$\nCompute $S_{C}$, expressing your final answer as a single real number. No rounding is required, and no units are to be provided with the final answer.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in circuit analysis and sensitivity theory. We may proceed with the solution.\n\nThe total impedance $Z$ of a series RLC circuit is the sum of the individual impedances of the resistor ($R$), inductor ($L$), and capacitor ($C$). The impedances are given as $Z_R = R$, $Z_L = j \\omega L$, and $Z_C = \\frac{1}{j \\omega C}$, where $j$ is the imaginary unit satisfying $j^2 = -1$.\n\nThe total impedance as a function of angular frequency $\\omega$ is:\n$$\nZ(\\omega) = Z_R + Z_L + Z_C = R + j \\omega L + \\frac{1}{j \\omega C}\n$$\nUsing the property $\\frac{1}{j} = -j$, we can rewrite the impedance as:\n$$\nZ(\\omega) = R + j \\omega L - \\frac{j}{\\omega C} = R + j \\left(\\omega L - \\frac{1}{\\omega C}\\right)\n$$\nThe impedance $Z(\\omega)$ is a complex number with a real part (resistance) and an imaginary part (reactance). The real part is $\\text{Re}(Z) = R$, and the imaginary part, or net reactance, is $\\text{Im}(Z) = X(\\omega) = \\omega L - \\frac{1}{\\omega C}$.\n\nThe problem defines the resonance angular frequency, $\\omega_0$, as the frequency at which the net reactance is zero. This condition implies that the imaginary part of the total impedance vanishes:\n$$\n\\text{Im}(Z(\\omega_0)) = 0\n$$\nSubstituting the expression for the reactance, we get:\n$$\n\\omega_0 L - \\frac{1}{\\omega_0 C} = 0\n$$\nTo solve for $\\omega_0$, we rearrange the equation:\n$$\n\\omega_0 L = \\frac{1}{\\omega_0 C}\n$$\n$$\n\\omega_0^2 = \\frac{1}{LC}\n$$\nSince angular frequency must be a positive quantity, we take the positive square root:\n$$\n\\omega_0 = \\frac{1}{\\sqrt{LC}} = (LC)^{-1/2}\n$$\nThis is the expression for the resonance angular frequency as a function of inductance $L$ and capacitance $C$.\n\nNext, we must compute the normalized sensitivity of $\\omega_0$ with respect to $C$, which is defined as:\n$$\nS_{C} \\equiv \\frac{C}{\\omega_{0}} \\frac{\\partial \\omega_{0}}{\\partial C}\n$$\nTo compute this, we first need to find the partial derivative of $\\omega_0$ with respect to $C$. We treat $L$ as a constant.\n$$\n\\omega_0(C) = L^{-1/2} C^{-1/2}\n$$\nUsing the power rule for differentiation, $\\frac{d}{dx}(x^n) = nx^{n-1}$, we get:\n$$\n\\frac{\\partial \\omega_0}{\\partial C} = L^{-1/2} \\left(-\\frac{1}{2} C^{-1/2 - 1}\\right) = L^{-1/2} \\left(-\\frac{1}{2} C^{-3/2}\\right)\n$$\n$$\n\\frac{\\partial \\omega_0}{\\partial C} = -\\frac{1}{2} L^{-1/2} C^{-3/2}\n$$\nNow, we substitute the expressions for $\\omega_0$ and $\\frac{\\partial \\omega_0}{\\partial C}$ into the sensitivity formula:\n$$\nS_C = \\frac{C}{\\omega_0} \\left( -\\frac{1}{2} L^{-1/2} C^{-3/2} \\right)\n$$\nSubstitute $\\omega_0 = (LC)^{-1/2} = L^{-1/2}C^{-1/2}$:\n$$\nS_C = \\frac{C}{L^{-1/2}C^{-1/2}} \\left( -\\frac{1}{2} L^{-1/2} C^{-3/2} \\right)\n$$\nWe can simplify this expression by combining the terms:\n$$\nS_C = -\\frac{1}{2} \\cdot \\frac{C}{L^{-1/2}C^{-1/2}} \\cdot L^{-1/2} C^{-3/2}\n$$\nThe terms involving $L$ cancel out: $L^{-1/2}$ in the numerator and $L^{-1/2}$ in the denominator.\n$$\nS_C = -\\frac{1}{2} \\cdot \\frac{C}{C^{-1/2}} \\cdot C^{-3/2}\n$$\nNow, combine the powers of $C$:\n$$\nS_C = -\\frac{1}{2} \\cdot C^{1 - (-1/2) - 3/2} = -\\frac{1}{2} \\cdot C^{1 + 1/2 - 3/2} = -\\frac{1}{2} \\cdot C^{3/2 - 3/2} = -\\frac{1}{2} \\cdot C^0\n$$\nSince any non-zero quantity raised to the power of $0$ is $1$, we have:\n$$\nS_C = -\\frac{1}{2}\n$$\nThe normalized sensitivity of the resonance angular frequency with respect to the capacitance is a constant value of $-\\frac{1}{2}$. This signifies that a small fractional increase in capacitance leads to a fractional decrease in the resonance frequency of half that magnitude. For instance, a $1\\%$ increase in $C$ results in approximately a $0.5\\%$ decrease in $\\omega_0$.",
            "answer": "$$\\boxed{-\\frac{1}{2}}$$"
        },
        {
            "introduction": "Sensitivity analysis is not just for physical systems; it is also a vital tool for understanding the behavior of the numerical algorithms that form the bedrock of computational science. This exercise illuminates a crucial distinction: the sensitivity of the underlying mathematical problem (its \"conditioning\") versus the sensitivity of the algorithm used to solve it. By examining the well-known Newton's method for root-finding, you will discover how the local characteristics of a function's root dramatically impact both the problem's stability and the algorithm's convergence speed, a foundational concept in numerical analysis .",
            "id": "3272508",
            "problem": "A scalar nonlinear equation is to be solved with Newton’s method. Let $f:\\mathbb{R}\\to\\mathbb{R}$ be twice continuously differentiable near a root $x=r$ with $f(r)=0$. The Newton iteration is defined by $x_{k+1}=x_k-\\dfrac{f(x_k)}{f'(x_k)}$. A root $x=r$ is called simple if $f'(r)\\neq 0$, and is called a double root if $f'(r)=0$ and $f''(r)\\neq 0$. Consider two aspects of sensitivity in this context:\n\n- Sensitivity of the solution to a small additive perturbation in the equation: solve $f(x)+\\varepsilon=0$ with small $\\varepsilon\\in\\mathbb{R}$. The sensitivity is quantified by how the perturbed root $x=r(\\varepsilon)$ changes in magnitude as a function of $\\varepsilon$.\n- Sensitivity of Newton’s method to perturbations in the initial guess: start from $x_0=r+e_0$ with small $e_0$, and quantify how the one-step error $e_1=x_{1}-r$ depends on $e_0$.\n\nAssume the setting is locally well-posed so that $f$ is sufficiently smooth and $f''(r)\\neq 0$ when $f'(r)=0$. Which option best describes the change in sensitivity when the method is applied near a double root versus a simple root?\n\nA. Near a simple root, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\varepsilon)$, and Newton’s one-step error satisfies $e_1=\\mathcal{O}(e_0^2)$; near a double root, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\sqrt{|\\varepsilon|})$, and Newton’s one-step error satisfies $e_1=\\mathcal{O}(e_0)$ (specifically, $e_1\\approx \\dfrac{1}{2}e_0$).\n\nB. Near a simple root, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\sqrt{|\\varepsilon|})$, and Newton’s one-step error satisfies $e_1=\\mathcal{O}(e_0)$; near a double root, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\varepsilon)$, and Newton’s one-step error satisfies $e_1=\\mathcal{O}(e_0^2)$.\n\nC. In both the simple root and double root cases, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\varepsilon)$, and Newton’s one-step error satisfies $e_1=\\mathcal{O}(e_0^2)$.\n\nD. Near a double root, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\varepsilon)$, and Newton’s method removes the initial error exactly in one step, i.e., $e_1=0$ for all sufficiently small $e_0$.",
            "solution": "The validity of the problem statement is first assessed.\n\n### Step 1: Extract Givens\n- A scalar nonlinear equation is to be solved using Newton's method.\n- The function $f:\\mathbb{R}\\to\\mathbb{R}$ is twice continuously differentiable near a root $x=r$.\n- The root condition is $f(r)=0$.\n- The Newton iteration is $x_{k+1}=x_k-\\dfrac{f(x_k)}{f'(x_k)}$.\n- A simple root is defined by $f'(r)\\neq 0$.\n- A double root is defined by $f'(r)=0$ and $f''(r)\\neq 0$.\n- Sensitivity of the root is analyzed by solving the perturbed equation $f(x)+\\varepsilon=0$ for small $\\varepsilon\\in\\mathbb{R}$ and quantifying the change in the root $r(\\varepsilon)$.\n- Sensitivity of Newton's method is analyzed by starting from $x_0=r+e_0$ with small $e_0$ and quantifying the one-step error $e_1=x_1-r$.\n- An assumption is made that the problem is locally well-posed and $f''(r)\\neq 0$ when $f'(r)=0$.\n- The question asks to describe the change in sensitivity when applying the method near a double root versus a simple root.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, well-posed, and objective. It is a standard problem in introductory numerical analysis, used to illustrate the concepts of problem conditioning and algorithm convergence rate. The definitions for simple and double roots are standard. The two types of sensitivity analysis (problem sensitivity vs. method sensitivity) are well-defined. All required assumptions, such as the non-vanishing second derivative for a double root, are provided. The problem is formalizable and asks for a verifiable mathematical derivation based on Taylor series expansions, which is a core technique in this field.\n\n1.  **Scientific/Factual Unsoundness**: None. The problem is based on fundamental principles of calculus and numerical analysis.\n2.  **Non-Formalizable or Irrelevant**: None. The problem is directly relevant to sensitivity analysis in numerical methods.\n3.  **Incomplete or Contradictory Setup**: None. All necessary definitions and conditions are provided and are consistent.\n4.  **Unrealistic or Infeasible**: None. The problem operates within a standard mathematical framework.\n5.  **Ill-Posed or Poorly Structured**: None. The problem is well-structured and leads to a unique, derivable conclusion.\n6.  **Pseudo-Profound, Trivial, or Tautological**: None. The problem requires a non-trivial derivation that highlights key concepts in numerical analysis.\n7.  **Outside Scientific Verifiability**: None. The results can be proven mathematically.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full solution will be derived.\n\n### Derivation\n\nWe analyze the two types of sensitivity for both simple and double roots. This requires the use of Taylor series expansions for the function $f(x)$ around the root $x=r$.\n\n**Part 1: Analysis for a Simple Root**\nA simple root $x=r$ is characterized by $f(r)=0$ and $f'(r)\\neq 0$.\n\n**1.1. Sensitivity of the Solution**\nWe analyze the perturbed equation $f(x) + \\varepsilon = 0$. Let the new root be $x(\\varepsilon) = r + \\delta r$. We assume $\\delta r$ is small for small $\\varepsilon$.\nWe substitute $x=r+\\delta r$ into the equation:\n$$f(r+\\delta r) + \\varepsilon = 0$$\nWe perform a Taylor expansion of $f(r+\\delta r)$ around $r$:\n$$f(r) + f'(r)\\delta r + \\mathcal{O}((\\delta r)^2) + \\varepsilon = 0$$\nSince $f(r)=0$, this simplifies to:\n$$f'(r)\\delta r + \\mathcal{O}((\\delta r)^2) + \\varepsilon = 0$$\nFor small $\\varepsilon$, $\\delta r$ is also small, so we can neglect the higher-order terms:\n$$f'(r)\\delta r \\approx -\\varepsilon$$\nSolving for the root perturbation $\\delta r$:\n$$\\delta r \\approx -\\frac{\\varepsilon}{f'(r)}$$\nSince $f'(r)$ is a non-zero constant, the perturbation in the root, $\\delta r$, is directly proportional to $\\varepsilon$. Thus, the root shift is of order $\\mathcal{O}(\\varepsilon)$. This indicates the problem is well-conditioned.\n\n**1.2. Sensitivity of Newton's Method (One-Step Error)**\nLet the initial guess be $x_0 = r + e_0$, where $e_0$ is the initial error. The first iteration gives $x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$. The new error is $e_1 = x_1 - r$.\n$$e_1 = (x_0 - r) - \\frac{f(x_0)}{f'(x_0)} = e_0 - \\frac{f(r+e_0)}{f'(r+e_0)}$$\nWe use Taylor expansions for the numerator and denominator around $r$:\n$$f(r+e_0) = f(r) + f'(r)e_0 + \\frac{f''(r)}{2!}e_0^2 + \\mathcal{O}(e_0^3) = f'(r)e_0 + \\frac{f''(r)}{2}e_0^2 + \\mathcal{O}(e_0^3)$$\n$$f'(r+e_0) = f'(r) + f''(r)e_0 + \\mathcal{O}(e_0^2)$$\nSubstituting these into the expression for $e_1$:\n$$e_1 = e_0 - \\frac{f'(r)e_0 + \\frac{1}{2}f''(r)e_0^2 + \\mathcal{O}(e_0^3)}{f'(r) + f''(r)e_0 + \\mathcal{O}(e_0^2)}$$\nFactor out $f'(r)$ from the denominator:\n$$e_1 = e_0 - \\frac{f'(r)e_0 + \\frac{1}{2}f''(r)e_0^2 + \\dots}{f'(r) \\left(1 + \\frac{f''(r)}{f'(r)}e_0 + \\dots\\right)}$$\nUsing the geometric series approximation $\\frac{1}{1+z} \\approx 1-z$ for small $z = \\frac{f''(r)}{f'(r)}e_0$:\n$$e_1 \\approx e_0 - \\frac{1}{f'(r)} \\left(f'(r)e_0 + \\frac{1}{2}f''(r)e_0^2\\right) \\left(1 - \\frac{f''(r)}{f'(r)}e_0\\right)$$\nExpanding and keeping terms up to order $e_0^2$:\n$$e_1 \\approx e_0 - \\frac{1}{f'(r)} \\left(f'(r)e_0 + \\frac{1}{2}f''(r)e_0^2 - f'(r)\\frac{f''(r)}{f'(r)}e_0^2\\right)$$\n$$e_1 \\approx e_0 - \\frac{1}{f'(r)} \\left(f'(r)e_0 - \\frac{1}{2}f''(r)e_0^2\\right)$$\n$$e_1 \\approx e_0 - \\left(e_0 - \\frac{f''(r)}{2f'(r)}e_0^2\\right) = \\frac{f''(r)}{2f'(r)}e_0^2$$\nThe error $e_1$ is proportional to the square of the previous error $e_0$. Therefore, $e_1 = \\mathcal{O}(e_0^2)$, which is the signature of quadratic convergence.\n\n**Part 2: Analysis for a Double Root**\nA double root $x=r$ is characterized by $f(r)=0$, $f'(r)=0$, and $f''(r)\\neq 0$.\n\n**2.1. Sensitivity of the Solution**\nWe again analyze the perturbed equation $f(x) + \\varepsilon = 0$ with $x(\\varepsilon) = r + \\delta r$.\n$$f(r+\\delta r) + \\varepsilon = 0$$\nThe Taylor expansion of $f(r+\\delta r)$ around $r$ is:\n$$f(r) + f'(r)\\delta r + \\frac{f''(r)}{2!}(\\delta r)^2 + \\mathcal{O}((\\delta r)^3) + \\varepsilon = 0$$\nUsing the conditions for a double root, this becomes:\n$$0 + 0 \\cdot \\delta r + \\frac{f''(r)}{2}(\\delta r)^2 + \\mathcal{O}((\\delta r)^3) + \\varepsilon = 0$$\nFor small $\\varepsilon$ and $\\delta r$, we neglect the higher-order term:\n$$\\frac{f''(r)}{2}(\\delta r)^2 \\approx -\\varepsilon$$\nSolving for $\\delta r$:\n$$(\\delta r)^2 \\approx -\\frac{2\\varepsilon}{f''(r)} \\implies |\\delta r| \\approx \\sqrt{\\left|-\\frac{2\\varepsilon}{f''(r)}\\right|}$$\nThe magnitude of the root perturbation $|\\delta r|$ is proportional to $\\sqrt{|\\varepsilon|}$. Thus, the root shift is of order $\\mathcal{O}(\\sqrt{|\\varepsilon|})$. The square root dependence indicates that the problem of finding a double root is ill-conditioned.\n\n**2.2. Sensitivity of Newton's Method (One-Step Error)**\nAs before, $e_1 = e_0 - \\frac{f(r+e_0)}{f'(r+e_0)}$. We use Taylor expansions appropriate for a double root:\n$$f(r+e_0) = f(r) + f'(r)e_0 + \\frac{f''(r)}{2}e_0^2 + \\mathcal{O}(e_0^3) = \\frac{f''(r)}{2}e_0^2 + \\mathcal{O}(e_0^3)$$\n$$f'(r+e_0) = f'(r) + f''(r)e_0 + \\frac{f'''(r)}{2}e_0^2 + \\mathcal{O}(e_0^3) = f''(r)e_0 + \\mathcal{O}(e_0^2)$$\nSubstitute these into the expression for $e_1$:\n$$e_1 = e_0 - \\frac{\\frac{1}{2}f''(r)e_0^2 + \\mathcal{O}(e_0^3)}{f''(r)e_0 + \\mathcal{O}(e_0^2)}$$\nTo find the dominant behavior for small $e_0$, we consider the ratio of the leading terms:\n$$e_1 \\approx e_0 - \\frac{\\frac{1}{2}f''(r)e_0^2}{f''(r)e_0}$$\nSince $f''(r)\\neq 0$ and we assume $e_0\\neq 0$, we can simplify:\n$$e_1 \\approx e_0 - \\frac{1}{2}e_0 = \\frac{1}{2}e_0$$\nThe error $e_1$ is linearly proportional to the previous error $e_0$, with a constant factor of approximately $\\frac{1}{2}$. Therefore, $e_1 = \\mathcal{O}(e_0)$, which indicates linear convergence.\n\n### Summary of Results and Option Evaluation\n- **Simple Root**:\n    - Solution sensitivity: $\\delta r = \\mathcal{O}(\\varepsilon)$.\n    - Method sensitivity: $e_1 = \\mathcal{O}(e_0^2)$.\n- **Double Root**:\n    - Solution sensitivity: $|\\delta r| = \\mathcal{O}(\\sqrt{|\\varepsilon|})$.\n    - Method sensitivity: $e_1 \\approx \\frac{1}{2}e_0$, so $e_1 = \\mathcal{O}(e_0)$.\n\nNow we evaluate the given options based on this summary.\n\n**A. Near a simple root, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\varepsilon)$, and Newton’s one-step error satisfies $e_1=\\mathcal{O}(e_0^2)$; near a double root, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\sqrt{|\\varepsilon|})$, and Newton’s one-step error satisfies $e_1=\\mathcal{O}(e_0)$ (specifically, $e_1\\approx \\dfrac{1}{2}e_0$).**\nThis option perfectly matches all four parts of our derived results.\n**Verdict: Correct.**\n\n**B. Near a simple root, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\sqrt{|\\varepsilon|})$, and Newton’s one-step error satisfies $e_1=\\mathcal{O}(e_0)$; near a double root, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\varepsilon)$, and Newton’s one-step error satisfies $e_1=\\mathcal{O}(e_0^2)$.**\nThis option swaps the results for simple and double roots.\n**Verdict: Incorrect.**\n\n**C. In both the simple root and double root cases, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\varepsilon)$, and Newton’s one-step error satisfies $e_1=\\mathcal{O}(e_0^2)$.**\nThis option incorrectly claims that the behavior for a double root is the same as for a simple root. Our derivation shows significant differences in both solution sensitivity and method convergence.\n**Verdict: Incorrect.**\n\n**D. Near a double root, the solution to $f(x)+\\varepsilon=0$ has a root shift of order $\\mathcal{O}(\\varepsilon)$, and Newton’s method removes the initial error exactly in one step, i.e., $e_1=0$ for all sufficiently small $e_0$.**\nThis option makes two incorrect claims about the double root case. The root shift is $\\mathcal{O}(\\sqrt{|\\varepsilon|})$, not $\\mathcal{O}(\\varepsilon)$. Newton's method converges linearly ($e_1 \\approx \\frac{1}{2}e_0$), it does not converge in one step ($e_1=0$).\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "For many complex systems in science and engineering, analytical formulas are not available, and we must rely on computational simulation to probe their dynamics. This final practice provides a hands-on challenge in numerical sensitivity analysis using the classic Lotka-Volterra model of predator-prey interaction. You will write a program to investigate how an emergent, dynamic property of the ecosystem—the period of its population cycles—is affected by a fundamental biological parameter. This exercise integrates numerical methods for differential equations with finite difference approximations to quantify sensitivity, mirroring the approach used in modern scientific research .",
            "id": "3272403",
            "problem": "Consider the two-dimensional Lotka–Volterra predator–prey system given by the initial value problem\n$$\n\\frac{d x}{d t} = a\\, x - b\\, x\\, y,\\qquad \\frac{d y}{d t} = c\\, x\\, y - d\\, y,\\qquad x(0)=x_0>0,\\quad y(0)=y_0>0,\n$$\nwhere $x(t)$ denotes the prey population, $y(t)$ denotes the predator population, and the parameters $a>0$, $b>0$, $c>0$, $d>0$ denote the prey intrinsic growth rate, the predation rate coefficient, the predator reproduction efficiency, and the predator natural death rate, respectively. Time is measured in arbitrary time units (tu), and the parameter $d$ is in inverse time units (per tu). For positive parameters and initial conditions, solutions evolve on closed orbits in the positive quadrant.\n\nDefine the cycle period $T(d)$ as the elapsed time between two successive local maxima of the predator population $y(t)$ occurring on the trajectory that starts at $(x_0,y_0)$. A local maximum of $y(t)$ is identified by a zero of its time derivative with a sign change from positive to negative, that is, when $g(t)=\\frac{d y}{d t}$ satisfies $g(t^-)>0$ and $g(t^+)<0$ for times $t^-$ and $t^+$ bracketing the event time.\n\nYour task is to write a complete, runnable program that, for each test case specified below, numerically estimates the sensitivity of the period with respect to the predator natural death rate at the given value $d$, denoted by\n$$\nS(d) = \\frac{\\partial T}{\\partial d}.\n$$\nYou must approximate $S(d)$ by perturbing $d$ symmetrically by a small scalar $\\varepsilon>0$, computing the corresponding periods, and forming a numerical derivative that is consistent with fundamental definitions. All numerical computations must be carried out in dimensionless form using a fixed time step, and each event time (local maximum) must be refined from the bracketing time step endpoints by interpolation to reduce discretization error. The reported sensitivity values must be expressed in time units squared ($\\text{tu}^2$) and returned as floating-point numbers.\n\nImplement a consistent single-step method for integrating the initial value problem with a fixed step size that is sufficiently accurate to resolve the oscillations; ensure the method respects the structure of the system and produces stable estimates of the period under small perturbations of $d$. For each period estimate, integrate forward in time until two successive local maxima of $y(t)$ are detected; if this does not occur before reaching the specified maximum integration time, treat the period as undefined for that parameter set and propagate an undefined numerical result for the sensitivity.\n\nUse the following test suite, where each case is given as $(a,b,c,d,x_0,y_0,\\Delta t,\\varepsilon,t_{\\max})$:\n\n- Case 1 (general case): $(1.0,\\,0.6,\\,0.5,\\,0.4,\\,1.5,\\,1.0,\\,$10^{-3}$$,\\,$10^{-3}$$,\\,200.0)$.\n- Case 2 (small predator death rate boundary): $(1.2,\\,0.4,\\,0.3,\\,0.1,\\,3.0,\\,0.5,\\,$10^{-3}$$,\\,$$5 \\times 10^{-4}$$,\\,400.0)$.\n- Case 3 (higher predator death rate with stronger coupling): $(0.9,\\,0.7,\\,0.6,\\,0.8,\\,4.0,\\,2.5,\\,$10^{-3}$$,\\,$10^{-3}$$,\\,200.0)$.\n\nYour program must produce a single line of output containing the sensitivity estimates for all three cases, in order, as a comma-separated list enclosed in square brackets (for example, $[s_1,s_2,s_3]$). The values $s_1$, $s_2$, and $s_3$ must be floating-point numbers in time units squared ($\\text{tu}^2$).",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n\nThe system of ordinary differential equations (ODEs) is the Lotka-Volterra model:\n$$\n\\frac{d x}{d t} = a\\, x - b\\, x\\, y\n$$\n$$\n\\frac{d y}{d t} = c\\, x\\, y - d\\, y\n$$\nInitial conditions are given as $x(0) = x_0 > 0$ and $y(0) = y_0 > 0$. The parameters $a, b, c, d$ are all positive constants. The variables $x(t)$ and $y(t)$ represent prey and predator populations, respectively. The parameter $d$ has units of inverse time units (tu$^{-1}$).\n\nThe cycle period, $T(d)$, is defined as the time elapsed between two successive local maxima of the predator population $y(t)$. A local maximum is identified where $\\frac{dy}{dt}=0$ with the derivative changing from positive to negative.\n\nThe objective is to numerically estimate the sensitivity of the period with respect to the parameter $d$, defined as:\n$$\nS(d) = \\frac{\\partial T}{\\partial d}\n$$\nThis sensitivity is to be approximated using a symmetric perturbation $\\varepsilon > 0$:\n$$\nS(d) \\approx \\frac{T(d+\\varepsilon) - T(d-\\varepsilon)}{2\\varepsilon}\n$$\nNumerical implementation details specified are:\n- Use a consistent single-step method for ODE integration with a fixed time step, $\\Delta t$.\n- Refine event times (local maxima) using interpolation.\n- If two successive maxima are not found before a maximum integration time, $t_{\\max}$, the period is considered undefined.\n- The final sensitivity is to be expressed in units of tu$^2$.\n\nThe test suite provides specific values for each case in the format $(a, b, c, d, x_0, y_0, \\Delta t, \\varepsilon, t_{\\max})$:\n- Case 1: $(1.0, 0.6, 0.5, 0.4, 1.5, 1.0, 10^{-3}, 10^{-3}, 200.0)$.\n- Case 2: $(1.2, 0.4, 0.3, 0.1, 3.0, 0.5, 10^{-3}, 5\\times 10^{-4}, 400.0)$.\n- Case 3: $(0.9, 0.7, 0.6, 0.8, 4.0, 2.5, 10^{-3}, 10^{-3}, 200.0)$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded:** The problem is based on the Lotka-Volterra equations, a fundamental and well-established model in mathematical biology. The concept of sensitivity analysis is a standard and critical topic in scientific computing and engineering. The problem is scientifically sound.\n- **Well-Posed:** The problem provides a complete set of information. For each test case, all parameters for the ODEs, initial conditions, and numerical procedure ($\\Delta t$, $\\varepsilon$, $t_{\\max}$) are specified. The definition of the period $T(d)$ is unambiguous. The task to compute a numerical approximation of the sensitivity $S(d)$ is clearly defined. The initial value problem is well-posed.\n- **Objective:** The problem is stated in precise, formal mathematical language, devoid of any subjectivity or ambiguity.\n\nThe problem does not violate any of the invalidity criteria:\n1.  **Scientific/Factual Unsoundness:** None. The model and methods are standard.\n2.  **Non-Formalizable/Irrelevant:** None. The problem is a direct and formalizable application of numerical methods to sensitivity analysis.\n3.  **Incomplete/Contradictory Setup:** None. All required information is provided and is internally consistent.\n4.  **Unrealistic/Infeasible:** None. The parameter values are appropriate for a numerical study, and the computations are feasible. The required unit of the result, tu$^2$, is dimensionally consistent with the definition $\\frac{\\partial T}{\\partial d}$, as $T$ has units of tu and $d$ has units of tu$^{-1}$.\n5.  **Ill-Posed/Poorly Structured:** None. The problem is clearly structured, and the quantities to be computed are well-defined.\n6.  **Pseudo-Profound/Trivial:** None. The problem requires a non-trivial implementation involving ODE integration, event detection, and numerical differentiation, representing a standard problem in computational science.\n7.  **Outside Scientific Verifiability:** None. The results are numerically verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Design\n\nThe solution requires the numerical estimation of the sensitivity $S(d) = \\frac{\\partial T}{\\partial d}$, which is approached using a second-order central difference formula:\n$$\nS(d) \\approx \\frac{T(d+\\varepsilon) - T(d-\\varepsilon)}{2\\varepsilon}\n$$\nThis requires the computation of the period $T$ for two perturbed values of the parameter $d$: $d_{+} = d+\\varepsilon$ and $d_{-} = d-\\varepsilon$. The core of the problem is therefore to develop a robust function for calculating the period $T$.\n\nThe period $T$ is found by integrating the system of ODEs forward in time and identifying the times of two consecutive local maxima of the predator population, $y(t)$.\n\n**1. Numerical Integration:**\nThe system is an initial value problem (IVP). A suitable single-step integrator is the fourth-order Runge-Kutta (RK$4$) method. It offers a good balance of accuracy and stability for a fixed step size, $\\Delta t$, and is consistent with the problem's requirements. Given a state vector $\\mathbf{z}(t) = [x(t), y(t)]^T$ and the ODE $\\frac{d\\mathbf{z}}{dt} = \\mathbf{f}(\\mathbf{z})$, a single RK$4$ step from time $t_n$ to $t_{n+1} = t_n + \\Delta t$ is computed as:\n$$\n\\mathbf{k}_1 = \\mathbf{f}(\\mathbf{z}_n)\n$$\n$$\n\\mathbf{k}_2 = \\mathbf{f}(\\mathbf{z}_n + \\frac{\\Delta t}{2} \\mathbf{k}_1)\n$$\n$$\n\\mathbf{k}_3 = \\mathbf{f}(\\mathbf{z}_n + \\frac{\\Delta t}{2} \\mathbf{k}_2)\n$$\n$$\n\\mathbf{k}_4 = \\mathbf{f}(\\mathbf{z}_n + \\Delta t \\mathbf{k}_3)\n$$\n$$\n\\mathbf{z}_{n+1} = \\mathbf{z}_n + \\frac{\\Delta t}{6}(\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4)\n$$\nwhere $\\mathbf{f}(\\mathbf{z}) = [ax - bxy, cxy - dy]^T$.\n\n**2. Event Detection and Refinement:**\nA local maximum of $y(t)$ occurs when its derivative, $\\frac{dy}{dt}$, passes through zero from positive to negative. The derivative is given by $\\frac{dy}{dt} = y(t)(c x(t) - d)$. Since solutions with $y_0>0$ remain in the positive quadrant ($y(t)>0$), the condition for an extremum simplifies to finding roots of the function $g(t) = c x(t) - d$.\n\nWe can detect a root crossing within a time step $[t_{n}, t_{n+1}]$ by checking for a sign change in $g(t)$. Specifically, a local maximum is bracketed if $g(t_n) > 0$ and $g(t_{n+1}) \\le 0$.\n\nOnce an event is bracketed between $t_n$ and $t_{n+1}$, the precise event time, $t_{\\text{event}}$, is estimated using linear interpolation (secant method) on $g(t)$. Assuming $g(t)$ is linear over the small interval $\\Delta t$, the time $t_{\\text{event}}$ at which $g(t_{\\text{event}})=0$ is given by:\n$$\nt_{\\text{event}} = t_n + \\Delta t \\frac{g(t_n)}{g(t_n) - g(t_{n+1})}\n$$\nThis refined time provides a more accurate estimate of the maxima locations than simply using the discrete time step endpoints.\n\n**3. Period Calculation:**\nThe integration proceeds from $t=0$ up to $t_{\\max}$. The refined times of the first two detected local maxima, $t_{\\text{max},1}$ and $t_{\\text{max},2}$, are stored. The period is then calculated as their difference: $T = t_{\\text{max},2} - t_{\\text{max},1}$. If fewer than two maxima are found before the integration time reaches $t_{\\max}$, the period is considered undefined, and a `not-a-number` ($NaN$) value is returned.\n\n**4. Implementation Structure:**\nThe overall algorithm is implemented in a modular fashion:\n- A function implements the right-hand side, $\\mathbf{f}(\\mathbf{z})$, of the Lotka-Volterra equations.\n- A function executes a single RK$4$ step.\n- A primary function, `find_period`, orchestrates the time integration and event detection/refinement logic to calculate a single period $T(d)$.\n- A function, `calculate_sensitivity`, calls `find_period` for $d+\\varepsilon$ and $d-\\varepsilon$ and computes the central difference for $S(d)$.\n- A main `solve` function iterates through the test cases, calls `calculate_sensitivity` for each, and formats the final output as specified. This structure ensures clarity, correctness, and adherence to the problem specification.",
            "answer": "```python\nimport numpy as np\n\ndef lotka_volterra_rhs(state, a, b, c, d):\n    \"\"\"\n    Computes the right-hand side of the Lotka-Volterra equations.\n    d(state)/dt = f(state)\n    \"\"\"\n    x, y = state\n    dxdt = a * x - b * x * y\n    dydt = c * x * y - d * y\n    return np.array([dxdt, dydt])\n\ndef rk4_step(state, dt, rhs_func, a, b, c, d):\n    \"\"\"\n    Performs a single step of the fourth-order Runge-Kutta method.\n    The ODE is autonomous, so time t is not explicitly used by the RHS function.\n    \"\"\"\n    k1 = rhs_func(state, a, b, c, d)\n    k2 = rhs_func(state + 0.5 * dt * k1, a, b, c, d)\n    k3 = rhs_func(state + 0.5 * dt * k2, a, b, c, d)\n    k4 = rhs_func(state + dt * k3, a, b, c, d)\n    new_state = state + (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n    return new_state\n\ndef find_period(a, b, c, d, x0, y0, dt, t_max):\n    \"\"\"\n    Numerically integrates the LV system to find the period between two\n    successive local maxima of the predator population y(t).\n    \"\"\"\n    state = np.array([x0, y0], dtype=float)\n    maxima_times = []\n\n    # Initial conditions must be positive for biologically meaningful orbits.\n    if x0 <= 0 or y0 <= 0:\n        return np.nan\n\n    num_steps = int(t_max / dt)\n    for step in range(num_steps):\n        state_prev = state\n        t_prev = step * dt\n\n        state = rk4_step(state_prev, dt, lotka_volterra_rhs, a, b, c, d)\n        t_curr = (step + 1) * dt\n        \n        # In the LV model, positivity is preserved. This is a safeguard.\n        if state[0] <= 0 or state[1] <= 0:\n            return np.nan\n\n        # The derivative of y is dy/dt = y(cx - d).\n        # We look for roots of g(t) = cx(t) - d.\n        g_prev = c * state_prev[0] - d\n        g_curr = c * state[0] - d\n        \n        # A local maximum is identified when g(t) crosses zero from positive to negative.\n        if g_prev > 0 and g_curr <= 0:\n            #Denominator g_prev - g_curr cannot be zero due to the condition g_prev > 0 and g_curr <= 0,\n            #unless g_prev and g_curr are both zero, which is numerically improbable.\n            denominator = g_prev - g_curr\n            t_event = t_prev + dt * g_prev / denominator\n            maxima_times.append(t_event)\n            \n            # Once two maxima are found, calculate the period and return.\n            if len(maxima_times) == 2:\n                period = maxima_times[1] - maxima_times[0]\n                return period\n\n    # If the loop completes without finding two maxima, the period is undefined.\n    return np.nan\n\ndef calculate_sensitivity(case):\n    \"\"\"\n    Calculates the sensitivity S(d) = dT/dd using a central difference scheme.\n    \"\"\"\n    a, b, c, d, x0, y0, dt, eps, t_max = case\n    \n    # Parameters must be positive; ensure d-epsilon is positive.\n    if d - eps <= 0:\n        return np.nan\n\n    # Calculate period for d perturbed up and down.\n    T_plus = find_period(a, b, c, d + eps, x0, y0, dt, t_max)\n    T_minus = find_period(a, b, c, d - eps, x0, y0, dt, t_max)\n    \n    # If either period calculation failed, sensitivity is undefined.\n    if np.isnan(T_plus) or np.isnan(T_minus):\n        return np.nan\n        \n    # Central difference formula for the derivative.\n    sensitivity = (T_plus - T_minus) / (2.0 * eps)\n    return sensitivity\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Test cases: (a, b, c, d, x0, y0, Δt, ε, t_max)\n    test_cases = [\n        (1.0, 0.6, 0.5, 0.4, 1.5, 1.0, 1e-3, 1e-3, 200.0),\n        (1.2, 0.4, 0.3, 0.1, 3.0, 0.5, 1e-3, 5e-4, 400.0),\n        (0.9, 0.7, 0.6, 0.8, 4.0, 2.5, 1e-3, 1e-3, 200.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        s = calculate_sensitivity(case)\n        results.append(s)\n        \n    # Format the output as a comma-separated list of floats in brackets.\n    # str() of np.nan produces 'nan', which is an acceptable representation of an undefined float.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the main function.\nsolve()\n\n```"
        }
    ]
}