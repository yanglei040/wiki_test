{
    "hands_on_practices": [
        {
            "introduction": "寻找控制物理系统的数学法则是科学的基石。本练习将让你扮演一名计算科学家的角色，任务是直接从数据中发现一个偏微分方程。你将实现一种基于稀疏回归的强大技术，从大型候选函数库中自动选择最重要的项，该方法是现代数据驱动动力系统发现算法的核心。",
            "id": "3157268",
            "problem": "您将实现一个稀疏回归方法，从一个专为发现偏微分方程结构而设计的多项式库中选择相关项。您将使用一个基于稀疏组正则化的凸复合优化模型。请从以下基本概念出发：监督学习中的线性经验风险最小化原则（该原则旨在最小化经验最小二乘风险）、平方损失的凸性，以及结构化惩罚项的近端算子定义。具体来说，设特征矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，目标向量为 $y \\in \\mathbb{R}^{n}$。考虑以下结合了最小二乘法和稀疏组惩罚项的凸目标函数形式\n$$\n\\min_{w \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\lVert X w - y \\rVert_2^2 \\;+\\; \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 \\;+\\; \\alpha \\lVert w \\rVert_1 \\right),\n$$\n其中 $n$ 是样本数量，$p$ 是特征数量，$w$ 是系数向量，$\\lambda \\in \\mathbb{R}_{>0}$ 是正则化强度，$\\alpha \\in (0,1)$ 用于在组稀疏和元素稀疏之间进行插值，$\\mathcal{G}$ 是特征索引的一个非重叠分组划分，其中 $w_g$ 表示由组 $g$ 索引的 $w$ 的子向量。您将使用的分组 $\\mathcal{G}$ 对应于一个一维标量场的多项式库中按空间导数阶数分组的项。您必须使用一种基于原则的一阶近端梯度方法来求解此优化问题，其步长需要从梯度的利普希茨常数导出，以确保其有效性。请勿使用任何预打包的机器学习求解器。\n\n请按如下方式构建一个合成数据集，使用解析表达式以避免数值微分误差。设 $x$ 和 $t$ 是在矩形网格上采样的自变量，该网格在 $[0,1]$ 上有 $N_x$ 个等距点，在 $[0,1]$ 上有 $N_t$ 个等距点；令 $n = N_x \\cdot N_t$。定义标量场\n$$\nu(x,t) = \\sin(2\\pi x)\\, e^{-t} + \\frac{1}{2} \\cos(4\\pi x)\\, e^{-0.2\\, t},\n$$\n及其一阶和二阶空间导数\n$$\nu_x(x,t) = 2\\pi \\cos(2\\pi x)\\, e^{-t} - 2\\pi \\sin(4\\pi x)\\, e^{-0.2\\, t},\n$$\n$$\nu_{xx}(x,t) = - (2\\pi)^2 \\sin(2\\pi x)\\, e^{-t} - 8\\pi^2 \\cos(4\\pi x)\\, e^{-0.2\\, t}.\n$$\n构建一个多项式库 $X \\in \\mathbb{R}^{n \\times p}$，其包含 $p = 9$ 列，顺序如下：\n$[\\, 1,\\; u,\\; u^2,\\; u_x,\\; u\\,u_x,\\; u^2 u_x,\\; u_{xx},\\; u\\,u_{xx},\\; (u_x)^2 \\,]$。\n将响应的真实生成机制定义为恰好两个项的线性组合：\n$$\ny = \\beta_{u u_x} \\cdot (u\\,u_x) + \\beta_{u_{xx}} \\cdot u_{xx} + \\varepsilon,\n$$\n其中 $\\beta_{u u_x} = 2.0$，$\\beta_{u_{xx}} = 0.1$，并加入了标准差为 $\\sigma$ 的加性独立高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。因此，系数的真实非零支撑集精确地由指定顺序中对应于 $u\\,u_x$ 和 $u_{xx}$ 的索引组成。\n\n将 $p=9$ 个特征按空间导数阶数划分为 3 个非重叠的组：\n- 第 0 组（0 阶）：索引 $[0,1,2]$ 对应于 $[1,\\; u,\\; u^2]$。\n- 第 1 组（1 阶）：索引 $[3,4,5]$ 对应于 $[u_x,\\; u\\,u_x,\\; u^2 u_x]$。\n- 第 2 组（2 阶）：索引 $[6,7,8]$ 对应于 $[u_{xx},\\; u\\,u_{xx},\\; (u_x)^2]$。\n\n实现一个近端梯度算法来最小化给定的目标函数。从梯度利普希茨常数的上界推导出有效的恒定步长，并为稀疏组惩罚项实现正确的近端算子。在优化之前，将 $X$ 的每一列标准化为零均值和单位方差，并通过减去其均值来中心化 $y$。优化后，通过对系数的绝对值使用用户指定的阈值 $\\tau > 0$ 进行阈值处理来确定预测的支撑集，如果 $|w_j| \\ge \\tau$，则该索引被视为选中。按如下方式计算精确率（precision）和召回率（recall）：精确率是真阳性（true positives）数量除以预测阳性（predicted positives）总数，召回率是真阳性数量除以真实非零项总数。将结果表示为小数（而非百分比）。\n\n您的程序必须实现以下测试套件，运行所有测试用例，并汇总结果。使用 $N_x = 64$ 和 $N_t = 16$，因此 $n = 1024$。对于每个用例，使用指定的标准差 $\\sigma$ 构建一个噪声向量。正则化参数应设置为 $\\lambda = \\kappa \\cdot \\Lambda(\\alpha)$，其中 $\\kappa$ 是一个给定的正标量，$\\Lambda(\\alpha)$ 是一个依赖于数据的尺度因子，由标准化后的 $(X,y)$ 计算得出：\n$$\n\\Lambda(\\alpha) = \\max\\left( \\frac{1}{n}\\lVert X^\\top y \\rVert_{\\infty} \\cdot \\frac{1}{\\alpha}, \\; \\max_{g \\in \\mathcal{G}} \\frac{1}{n}\\lVert X_g^\\top y \\rVert_2 \\cdot \\frac{1}{1 - \\alpha} \\right),\n$$\n约定当 $\\alpha$ 等于 0 或 1 时，通过省略相应项来忽略除以零的情况（在测试套件中不会出现这种情况）。这种缩放为 $\\lambda$ 提供了一个相对于观测数据的稳健参考量级。\n\n测试套件参数，每个参数指定为一个元组 $(\\alpha, \\kappa, \\sigma, \\tau)$：\n- 用例 1：$(0.5, 0.10, 0.0, 10^{-3})$。\n- 用例 2：$(0.5, 0.20, 0.01, 10^{-3})$。\n- 用例 3：$(0.8, 0.30, 0.02, 10^{-3})$。\n- 用例 4：$(0.2, 0.60, 0.05, 10^{-3})$。\n\n对于每个用例，使用具有基于原则的步长的近端梯度下降法拟合模型，然后计算恢复的支撑集相对于真实情况的精确率和召回率。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\, \\text{precision}_1, \\text{recall}_1, \\text{precision}_2, \\text{recall}_2, \\text{precision}_3, \\text{recall}_3, \\text{precision}_4, \\text{recall}_4 \\,]$，每个值都四舍五入到恰好三位小数。不应打印任何其他输出。本问题不使用角度，因此不需要角度单位。本问题不涉及物理单位。所有计算都应使用实数进行。最终输出必须严格遵守指定格式。",
            "solution": "The user has provided a valid problem statement.\n\n### Step 1: Extract Givens\n\n- **Objective Function**: Minimize the convex objective function for $w \\in \\mathbb{R}^{p}$:\n$$\n\\frac{1}{2n}\\lVert X w - y \\rVert_2^2 \\;+\\; \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 \\;+\\; \\alpha \\lVert w \\rVert_1 \\right)\n$$\n- **Data**: Feature matrix $X \\in \\mathbb{R}^{n \\times p}$ and target vector $y \\in \\mathbb{R}^{n}$, with $n$ samples and $p$ features.\n- **Parameters**: Regularization strength $\\lambda \\in \\mathbb{R}_{>0}$, interpolation parameter $\\alpha \\in (0,1)$.\n- **Groups**: $\\mathcal{G}$ is a partition of feature indices $\\{0, 1, \\dots, p-1\\}$ into non-overlapping groups. $w_g$ is the subvector of $w$ for group $g$.\n- **Synthetic Dataset**:\n    - Grid: $N_x = 64$ points on $[0,1]$ and $N_t = 16$ points on $[0,1]$, giving $n = N_x \\cdot N_t = 1024$ total sample points.\n    - Scalar field: $u(x,t) = \\sin(2\\pi x)\\, e^{-t} + \\frac{1}{2} \\cos(4\\pi x)\\, e^{-0.2\\, t}$.\n    - Spatial derivatives: $u_x(x,t) = 2\\pi \\cos(2\\pi x)\\, e^{-t} - 2\\pi \\sin(4\\pi x)\\, e^{-0.2\\, t}$ and $u_{xx}(x,t) = - (2\\pi)^2 \\sin(2\\pi x)\\, e^{-t} - 8\\pi^2 \\cos(4\\pi x)\\, e^{-0.2\\, t}$.\n    - Feature library ($p=9$): $X$ has columns in the order $[\\, 1,\\; u,\\; u^2,\\; u_x,\\; u\\,u_x,\\; u^2 u_x,\\; u_{xx},\\; u\\,u_{xx},\\; (u_x)^2 \\,]$.\n    - Target vector: $y = \\beta_{u u_x} \\cdot (u\\,u_x) + \\beta_{u_{xx}} \\cdot u_{xx} + \\varepsilon$, with coefficients $\\beta_{u u_x} = 2.0$, $\\beta_{u_{xx}} = 0.1$, and noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n- **Ground Truth**: The true non-zero coefficients correspond to terms $u\\,u_x$ (index $4$) and $u_{xx}$ (index $6$).\n- **Grouping Structure**: $p=9$ features partitioned into $3$ groups by derivative order:\n    - Group $0$: indices $[0, 1, 2]$ for $[1, u, u^2]$.\n    - Group $1$: indices $[3, 4, 5]$ for $[u_x, u\\,u_x, u^2 u_x]$.\n    - Group $2$: indices $[6, 7, 8]$ for $[u_{xx}, u\\,u_{xx}, (u_x)^2]$.\n- **Method**: Proximal gradient descent with a principled constant stepsize.\n- **Preprocessing**: Standardize columns of $X$ to zero mean and unit variance. Center $y$ by subtracting its mean.\n- **Regularization Scaling**: $\\lambda = \\kappa \\cdot \\Lambda(\\alpha)$, where $\\Lambda(\\alpha) = \\max\\left( \\frac{1}{n}\\lVert X^\\top y \\rVert_{\\infty} \\cdot \\frac{1}{\\alpha}, \\; \\max_{g \\in \\mathcal{G}} \\frac{1}{n}\\lVert X_g^\\top y \\rVert_2 \\cdot \\frac{1}{1 - \\alpha} \\right)$, computed on standardized/centered data.\n- **Evaluation**:\n    - Predicted support: Indices $j$ where $|w_j| \\ge \\tau$.\n    - Metrics: Precision and Recall, defined as $\\text{precision} = \\frac{\\text{TP}}{\\text{predicted positives}}$ and $\\text{recall} = \\frac{\\text{TP}}{\\text{true nonzeros}}$.\n- **Test Suite**: $4$ cases with parameters $(\\alpha, \\kappa, \\sigma, \\tau)$:\n    1. $(0.5, 0.10, 0.0, 10^{-3})$\n    2. $(0.5, 0.20, 0.01, 10^{-3})$\n    3. $(0.8, 0.30, 0.02, 10^{-3})$\n    4. $(0.2, 0.60, 0.05, 10^{-3})$\n- **Output Format**: Single line `[precision_1, recall_1, ..., precision_4, recall_4]` with values rounded to $3$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded, well-posed, and objective. It is based on established principles of convex optimization and sparse regression, standard in machine learning and computational science. The problem is self-contained, providing all necessary formulas, parameters, and procedures for constructing the dataset and implementing the algorithm. The setup is consistent, and the task is computationally feasible. No flaws according to the validation checklist were found.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Design of the Solution\n\nThe problem requires solving a convex composite optimization problem using a proximal gradient method. The objective function is of the form $\\min_w F(w) = f(w) + g(w)$, where $f(w)$ is a smooth, convex function and $g(w)$ is a convex, possibly non-smooth, regularizer.\n\n1.  **Decomposition of the Objective Function**:\n    The objective function is decomposed as:\n    - Smooth part (least-squares loss): $f(w) = \\frac{1}{2n}\\lVert X w - y \\rVert_2^2$.\n    - Non-smooth part (sparse group penalty): $g(w) = \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 + \\alpha \\lVert w \\rVert_1 \\right)$.\n    For the optimization procedure, we use the standardized feature matrix $\\tilde{X}$ and centered target vector $\\tilde{y}$. Thus, we minimize $f(w) = \\frac{1}{2n}\\lVert \\tilde{X} w - \\tilde{y} \\rVert_2^2$ plus the same regularizer $g(w)$.\n\n2.  **Proximal Gradient Method**:\n    The proximal gradient algorithm generates a sequence of iterates via the update rule:\n    $$\n    w^{(k+1)} = \\text{prox}_{\\gamma g}\\left(w^{(k)} - \\gamma \\nabla f(w^{(k)})\\right)\n    $$\n    where $\\gamma > 0$ is the stepsize and $\\text{prox}_{\\gamma g}$ is the proximal operator of the function $\\gamma g$.\n\n3.  **Gradient of the Smooth Part**:\n    The gradient of the least-squares term $f(w)$ is:\n    $$\n    \\nabla f(w) = \\frac{1}{n} \\tilde{X}^\\top (\\tilde{X} w - \\tilde{y})\n    $$\n\n4.  **Stepsize Selection**:\n    The convergence of the proximal gradient method is guaranteed if the stepsize $\\gamma$ satisfies $0  \\gamma \\le 1/L$, where $L$ is the Lipschitz constant of $\\nabla f(w)$. The gradient is Lipschitz continuous with constant $L = \\frac{1}{n} \\lVert \\tilde{X}^\\top \\tilde{X} \\rVert_2 = \\frac{1}{n} \\sigma_{\\max}(\\tilde{X}^\\top \\tilde{X})$, where $\\sigma_{\\max}$ denotes the maximum eigenvalue (since $\\tilde{X}^\\top \\tilde{X}$ is positive semi-definite). A principled choice is to set the stepsize to its upper bound, $\\gamma = 1/L$, which we compute from the data.\n\n5.  **Proximal Operator for Sparse Group Penalty**:\n    The proximal operator is defined as $\\text{prox}_{\\eta h}(v) = \\arg\\min_z \\left\\{ \\frac{1}{2}\\lVert z - v \\rVert_2^2 + \\eta h(z) \\right\\}$. The penalty $g(w)$ is separable across the predefined groups $\\mathcal{G}$. This means we can compute the proximal operator for each group subvector $w_g$ independently. For a single group $g$, we must solve:\n    $$\n    \\text{prox}_{\\gamma g_g}(v_g) = \\arg\\min_{w_g} \\left\\{ \\frac{1}{2}\\lVert w_g - v_g \\rVert_2^2 + \\gamma \\lambda(1-\\alpha)\\lVert w_g \\rVert_2 + \\gamma \\lambda\\alpha\\lVert w_g \\rVert_1 \\right\\}\n    $$\n    The solution to this subproblem, known as the sparse group lasso proximal operator, is a two-stage procedure:\n    a. Apply the element-wise soft-thresholding operator $S_{\\eta}(\\cdot)$ with threshold $\\eta_1 = \\gamma \\lambda \\alpha$ to the input vector $v_g$:\n    $$\n    u_g = S_{\\eta_1}(v_g) \\quad \\text{where } (S_{\\eta_1}(v_g))_j = \\text{sign}((v_g)_j) \\max(|\\,(v_g)_j| - \\eta_1, 0)\n    $$\n    b. Apply group-wise soft-thresholding (or block soft-thresholding) to the result $u_g$ with threshold $\\eta_2 = \\gamma \\lambda (1-\\alpha)$:\n    $$\n    w_g^* = u_g \\cdot \\max\\left(0, 1 - \\frac{\\eta_2}{\\lVert u_g \\rVert_2}\\right)\n    $$\n    This operation is performed for all groups $g \\in \\mathcal{G}$ to obtain the full updated vector $w^{(k+1)}$.\n\n6.  **Algorithm Summary**:\n    The implemented algorithm will proceed as follows for each test case:\n    a. Generate the data matrix $X$ and target vector $y$ according to the specified analytic functions and noise level $\\sigma$.\n    b. Standardize $X$ to get $\\tilde{X}$ and center $y$ to get $\\tilde{y}$.\n    c. Calculate the data-dependent regularization scale $\\Lambda(\\alpha)$ and the final regularization parameter $\\lambda = \\kappa \\Lambda(\\alpha)$.\n    d. Compute the Lipschitz constant $L$ and the stepsize $\\gamma = 1/L$.\n    e. Initialize the weight vector $w = \\mathbf{0}$.\n    f. Iterate the proximal gradient update rule until the change in $w$ is below a tolerance or a maximum number of iterations is reached.\n    g. On the final coefficient vector $w$, identify the non-zero support by thresholding with $\\tau$.\n    h. Compute the precision and recall of the recovered support against the known true support (indices $\\{4, 6\\}$).\n    i. Store and format the results as required.\n\nThis principled approach ensures a correct and robust implementation for solving the given scientific machine learning problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for sparse group regression.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, kappa, sigma, tau)\n        (0.5, 0.10, 0.0, 1e-3),\n        (0.5, 0.20, 0.01, 1e-3),\n        (0.8, 0.30, 0.02, 1e-3),\n        (0.2, 0.60, 0.05, 1e-3),\n    ]\n\n    results = []\n    \n    # Fixed parameters for data generation\n    Nx = 64\n    Nt = 16\n    n = Nx * Nt\n    p = 9\n    \n    # Grid generation\n    x_grid = np.linspace(0, 1, Nx)\n    t_grid = np.linspace(0, 1, Nt)\n    xx, tt = np.meshgrid(x_grid, t_grid)\n    x = xx.ravel()\n    t = tt.ravel()\n\n    # Analytic field and derivatives\n    u = np.sin(2 * np.pi * x) * np.exp(-t) + 0.5 * np.cos(4 * np.pi * x) * np.exp(-0.2 * t)\n    u_x = 2 * np.pi * np.cos(2 * np.pi * x) * np.exp(-t) - 2 * np.pi * np.sin(4 * np.pi * x) * np.exp(-0.2 * t)\n    u_xx = -(2 * np.pi)**2 * np.sin(2 * np.pi * x) * np.exp(-t) - 8 * np.pi**2 * np.cos(4 * np.pi * x) * np.exp(-0.2 * t)\n\n    # Polynomial library\n    X = np.zeros((n, p))\n    X[:, 0] = 1.0\n    X[:, 1] = u\n    X[:, 2] = u**2\n    X[:, 3] = u_x\n    X[:, 4] = u * u_x\n    X[:, 5] = u**2 * u_x\n    X[:, 6] = u_xx\n    X[:, 7] = u * u_xx\n    X[:, 8] = u_x**2\n\n    # Group definitions\n    groups = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    true_support = {4, 6}\n\n    for i, case in enumerate(test_cases):\n        alpha, kappa, sigma, tau = case\n        \n        # Set seed for reproducible noise generation for each case\n        np.random.seed(i)\n        \n        # Generate target vector y\n        y_true = 2.0 * X[:, 4] + 0.1 * X[:, 6]\n        noise = np.random.normal(0, sigma, size=n)\n        y = y_true + noise\n\n        # Preprocessing: Standardize X and center y\n        y_mean = y.mean()\n        y_c = y - y_mean\n        \n        X_mean = X.mean(axis=0)\n        X_std = X.std(axis=0)\n        # Avoid division by zero for columns with zero variance (e.g., constant column)\n        X_std[X_std == 0] = 1.0\n        X_s = (X - X_mean) / X_std\n\n        # Calculate data-dependent regularization scale Lambda(alpha)\n        X_s_T_y_c = X_s.T @ y_c\n        \n        term1 = np.linalg.norm(X_s_T_y_c, ord=np.inf) / (n * alpha)\n        \n        term2_vals = []\n        for g_indices in groups:\n            norm_Xg_T_y_c = np.linalg.norm(X_s_T_y_c[g_indices], ord=2)\n            term2_vals.append(norm_Xg_T_y_c / (n * (1 - alpha)))\n        term2 = np.max(term2_vals)\n        \n        lambda_scale = max(term1, term2)\n        lambda_val = kappa * lambda_scale\n\n        # Proximal Gradient Descent\n        # Compute Lipschitz constant and stepsize\n        L = np.max(np.linalg.eigvalsh(X_s.T @ X_s / n))\n        if L == 0:\n            gamma = 1.0\n        else:\n            gamma = 1.0 / L\n\n        # Algorithm parameters\n        w = np.zeros(p)\n        max_iter = 20000\n        tol = 1e-7\n        lambda1 = lambda_val * (1 - alpha)\n        lambda2 = lambda_val * alpha\n\n        for _ in range(max_iter):\n            w_old = w.copy()\n            grad = X_s.T @ (X_s @ w - y_c) / n\n            v = w - gamma * grad\n            \n            w_new = np.zeros(p)\n            for g_indices in groups:\n                v_g = v[g_indices]\n                # Soft-thresholding for L1 penalty\n                u_g = np.sign(v_g) * np.maximum(np.abs(v_g) - gamma * lambda2, 0)\n                norm_u_g = np.linalg.norm(u_g)\n                # Group soft-thresholding for group L2 penalty\n                if norm_u_g > 0:\n                    scale_factor = np.maximum(0, 1 - (gamma * lambda1) / norm_u_g)\n                    w_new[g_indices] = scale_factor * u_g\n            \n            w = w_new\n            if np.linalg.norm(w - w_old)  tol:\n                break\n        \n        # Evaluation\n        predicted_support = {j for j, val in enumerate(w) if np.abs(val) >= tau}\n        \n        tp = len(true_support.intersection(predicted_support))\n        num_predicted_positives = len(predicted_support)\n        num_true_nonzeros = len(true_support)\n        \n        precision = tp / num_predicted_positives if num_predicted_positives > 0 else 0.0\n        recall = tp / num_true_nonzeros\n        \n        results.append(precision)\n        results.append(recall)\n\n    # Format the final output string\n    output_str = \",\".join([f\"{r:.3f}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "除了寻找控制方程，科学研究常常致力于揭示潜在的因果关系网络，例如在基因调控网络中。本练习将挑战你超越简单的相关性分析，通过实现一种基于约束的因果发现算法来完成这一任务。通过检验数据中的条件独立性，你将学会重建因果影响的有向图，并探索靶向干预如何帮助揭示系统的真实结构。",
            "id": "3157243",
            "problem": "考虑一个被建模为有向无环图（DAG）的小型基因调控网络，其中变量 $x_1, x_2, x_3, x_4$ 服从线性高斯结构方程模型。数据生成过程遵循在线性高斯噪声下的因果马尔可夫条件和忠实性假设。具体来说，每个变量 $x_i$ 由一个结构方程给出，形式为 $x_i = \\sum_{j \\in \\mathrm{Pa}(i)} w_{ji} x_j + \\varepsilon_i$，其中 $\\mathrm{Pa}(i)$ 表示 DAG 中 $x_i$ 的父节点集合，$w_{ji} \\in \\mathbb{R}$ 是固定系数，$\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_i^2)$ 是相互独立的噪声项。\n\n给定一个代表最小基因调控基序的 $4$ 变量网络的真实机制：\n- $x_1 = \\varepsilon_1$，\n- $x_2 = \\varepsilon_2$，\n- $x_3 = w_{13} x_1 + w_{23} x_2 + \\varepsilon_3$，\n- $x_4 = w_{34} x_3 + \\varepsilon_4$，\n其系数为 $w_{13} = 0.9$，$w_{23} = 0.8$，$w_{34} = 0.7$，噪声标准差为 $\\sigma_1 = 1.0$，$\\sigma_2 = 1.0$，$\\sigma_3 = 0.5$，$\\sigma_4 = 0.5$。根据构造，所有变量都以 $0$ 为中心。这定义了一个具有边 $x_1 \\rightarrow x_3$，$x_2 \\rightarrow x_3$ 和 $x_3 \\rightarrow x_4$ 的真实 DAG。\n\n您的任务是实现一个基于约束的因果发现算法，从样本中推断出 DAG，并测试在人工干预 $do(x_i = \\tilde{x})$ 下推断结构的稳定性。这种干预会切断所有指向 $x_i$ 的传入边，并在所有样本中将 $x_i$ 设置为常量值 $\\tilde{x}$。为避免在 $do(x_i=\\tilde{x})$ 下由零方差引起的退化问题，在干预测试案例中，仅在未被干预的变量中推断 DAG。\n\n算法要求：\n- 使用基于 Peter-Clark (PC) 算法的约束方法，通过条件独立性检验学习骨架，然后使用对撞结构识别和单个 Meek 定向规则来确定边的方向。\n- 条件独立性检验必须基于一个经过充分检验的事实：对于联合高斯变量，条件独立性 $x_i \\perp x_j \\mid S$ 等价于偏相关为零 $\\rho_{ij\\cdot S} = 0$。通过 Fisher z变换实现检验：如果残差的样本相关性为 $r$，则检验统计量为 $z = \\frac{1}{2} \\ln \\left( \\frac{1+r}{1-r} \\right) \\sqrt{n - |S| - 3}$，其中 $n$ 是样本量， $|S|$ 是条件集的大小。在显著性水平 $\\alpha$ 下使用双侧检验。\n- 通过在条件集大小最大为 $2$ 的情况下检测到条件独立性时移除边，来学习无向骨架。\n- 定向v-结构（对撞结构）：对于三元组 $(i,k,j)$，若存在边 $i - k$ 和 $j - k$，且 $i$ 和 $j$ 不相邻，则当 $k$ 不在骨架发现阶段为 $(i,j)$ 找到的任何分离集中时，将边定向为 $i \\rightarrow k \\leftarrow j$。\n- 应用一条 Meek 定向规则来传播方向：如果 $i \\rightarrow j$ 和 $j - k$ 存在，且 $i$ 和 $k$ 不相邻，则将 $j - k$ 定向为 $j \\rightarrow k$。\n\n评估指标：\n- 计算推断出的有向图与真实有向图之间的结构汉明距离（SHD）。对于干预案例，此计算仅限于未被干预的变量集。对于受限集合中的每一对无序对 $\\{i,j\\}$，如果推断的邻接关系与真实的邻接关系不匹配，则计数 $1$：真实图中存在边而推断图中缺失，真实图中没有边而推断图中多出，或者方向错误。如果推断图中存在无向边，而真实图中为有向边，则计数 $1$。\n\n合成数据生成：\n- 观测样本是从上述结构方程中抽取的。\n- 干预样本 $do(x_i = \\tilde{x})$ 是通过将 $x_i$ 的结构方程替换为 $x_i := \\tilde{x}$（在所有样本中相同）来生成的；所有从 $x_i$ 出发的边保持不变，而所有指向 $x_i$ 的传入边在数据生成目的上被有效切断。推断应仅在由未干预变量诱导的子图上执行。\n\n测试套件：\n提供以下 $4$ 个测试案例的 SHD 结果。在所有情况下，除非另有说明，均使用上述指定的真实权重和噪声参数。独立性检验的显著性水平为每个案例指定的 $\\alpha$，条件集的大小最多为 $2$。\n\n- 测试案例 1（理想路径）：观测数据， $n=1000$，$\\alpha = 0.01$，在变量 $\\{x_1, x_2, x_3, x_4\\}$ 上进行推断。\n- 测试案例 2（对父节点进行干预）：干预数据 $do(x_1 = \\tilde{x})$，其中 $\\tilde{x} = 0$，$n=1000$，$\\alpha = 0.01$，在变量 $\\{x_2, x_3, x_4\\}$ 上进行推断。\n- 测试案例 3（边界样本量）：观测数据， $n=80$，$\\alpha = 0.05$，在变量 $\\{x_1, x_2, x_3, x_4\\}$ 上进行推断。\n- 测试案例 4（对对撞节点进行干预）：干预数据 $do(x_3 = \\tilde{x})$，其中 $\\tilde{x} = 0$，$n=1000$，$\\alpha = 0.01$，在变量 $\\{x_1, x_2, x_4\\}$ 上进行推断。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个测试案例的 SHD 结果，格式为逗号分隔的列表并用方括号括起（例如，$[h_1,h_2,h_3,h_4]$），其中每个 $h_i$ 是一个整数。",
            "solution": "该问题要求实现一个基于约束的因果发现算法，来推断一个被建模为有向无环图（DAG）的小型基因调控网络的结构。底层系统是一个线性高斯结构方程模型（SEM）。解决方案涉及数据生成、因果结构推断，以及针对几个测试案例（包括观测数据和干预数据）与真实模型进行评估。\n\n首先，我们对问题进行验证。所有给定的信息，包括结构方程、参数（$w_{ij}, \\sigma_i$）、算法要求（类PC的骨架构建、v-结构定向、一条Meek规则）和评估指标（结构汉明距离，SHD），都已明确说明。该问题在科学上基于成熟的因果推断理论，特别是因果马尔可夫条件和忠实性假设，这两个假设为数据中的统计独立性与因果DAG中的d-分离这一图属性之间提供了理论联系。该问题的表述是适定的、客观的，并且不包含任何科学或逻辑上的矛盾。因此，该问题被认为是有效的。\n\n解决方案按以下原则性步骤进行：\n\n**1. 从结构方程模型生成数据**\n\n真实网络由一组带有高斯噪声的线性方程定义，代表因果机制：\n$$x_1 = \\varepsilon_1$$\n$$x_2 = \\varepsilon_2$$\n$$x_3 = w_{13} x_1 + w_{23} x_2 + \\varepsilon_3$$\n$$x_4 = w_{34} x_3 + \\varepsilon_4$$\n其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$ 是独立的噪声项。通过首先从噪声分布中抽样，然后按拓扑顺序计算 $x_i$ 的值来生成样本。对于干预数据 $do(x_i = \\tilde{x})$，在计算其后代的值之前，将 $x_i$ 的结构方程替换为赋值 $x_i := \\tilde{x}$。这正确地模拟了理想干预的效果，即打破所有指向被干预变量的因果影响。\n\n**2. 条件独立性（CI）检验**\n\n基于约束的因果发现的基石是能够检验条件独立性。对于联合高斯变量，条件 $x_i \\perp x_j \\mid S$ 等价于偏相关为零，即 $\\rho_{ij \\cdot S} = 0$。我们使用样本数据来检验这个假设。样本偏相关 $r$ 是通过计算 $x_i$ 和 $x_j$ 在分别对条件集 $S$ 中的变量进行回归后得到的残差之间的相关性来计算的。\n\n样本偏相关的统计显著性通过 Fisher z变换来评估。检验统计量由下式给出：\n$$z = \\frac{1}{2} \\ln \\left( \\frac{1+r}{1-r} \\right) \\sqrt{n - |S| - 3}$$\n其中 $n$ 是样本量， $|S|$ 是条件集的大小。在零偏相关的零假设下，$z$ 服从标准正态分布 $\\mathcal{N}(0, 1)$。我们根据此 $z$ 统计量计算双侧p值。如果p值大于预定的显著性水平 $\\alpha$，我们则无法拒绝零假设，并断定变量是条件独立的。\n\n**3. 因果图推断算法**\n\n我们实现了一个简化版的 Peter-Clark (PC) 算法，该算法包括三个主要阶段：\n\n*   **骨架发现**：我们从一个变量集合上的全连接无向图开始。然后，我们系统地检验每对相邻变量 $(i,j)$ 的条件独立性，条件集 $S$ 的大小递增（这里是 $|S|=0, 1, 2$）。如果发现一个独立性 $x_i \\perp x_j \\mid S$，则移除 $i$ 和 $j$ 之间的边，并将集合 $S$ 记录为对 $(i, j)$ 的分离集。这个过程将图简化为一个无向骨架，该骨架代表了数据中存在的条件独立关系。\n\n*   **V-结构定向**：在建立骨架之后，我们对一种称为v-结构或对撞结构的特定子结构进行定向。对于任何形成非耦合链 $i - k - j$（即 $i$ 和 $j$ 不相邻）的变量三元组 $(i, k, j)$，当且仅当节点 $k$ *不在* 使 $i$ 和 $j$ 独立的那个分离集中时，我们将边定向为 $i \\rightarrow k \\leftarrow j$。此规则源于d-分离的原理：路径上 $i$ 和 $j$ 之间的对撞节点 $k$ 不会阻断路径，但以它为条件会打开路径。因此，如果 $i$ 和 $j$ 被一个*不包含* $k$ 的集合 $S$ 变得独立，那么 $k$ 必定是它们之间路径上的一个对撞节点。\n\n*   **边定向传播（Meek规则）**：最后，我们应用进一步的定向规则，将从已识别的v-结构开始的方向传播到整个图中，同时避免产生新的v-结构或有向环。按照规定，我们使用第一条Meek规则：如果存在一条边 $i \\rightarrow j$ 和一条无向边 $j - k$，并且节点 $i$ 和 $k$ 不相邻，则将边 $j - k$ 定向为 $j \\rightarrow k$。这个过程被迭代应用，直到没有更多的边可以被定向为止。\n\n**4. 通过结构汉明距离（SHD）进行评估**\n\n推断图的准确性通过与真实DAG的结构汉明距离（SHD）来量化。SHD计算两个图之间边差异的数量：\n1.  **多余的边**：推断图中存在但真实图中不存在的边。\n2.  **缺失的边**：真实图中存在但推断图中不存在的边。\n3.  **方向错误**：两个图中都存在但方向不同的边（例如，真实图中为 $i \\rightarrow j$，而推断图中为 $j \\rightarrow i$ 或 $i - j$）。\n\n该算法被应用于四个不同的测试案例，以评估其在不同样本量下以及在存在干预的情况下的性能。计算并报告每个案例得到的SHD。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import combinations, permutations\n\ndef solve():\n    \"\"\"\n    Main function to run the four test cases and print the results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of results.\n    np.random.seed(42)\n\n    # Ground-truth parameters\n    W_13 = 0.9\n    W_23 = 0.8\n    W_34 = 0.7\n    SIGMAS = [1.0, 1.0, 0.5, 0.5]\n\n    test_cases = [\n        # Case 1: Observational, n=1000, alpha=0.01\n        {'n': 1000, 'alpha': 0.01, 'intervention': None, 'vars': [0, 1, 2, 3],\n         'true_adj': np.array([[0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 0]])},\n        # Case 2: Intervention do(x1=0), n=1000, alpha=0.01, on vars {x2, x3, x4}\n        {'n': 1000, 'alpha': 0.01, 'intervention': (0, 0.0), 'vars': [1, 2, 3],\n         'true_adj': np.array([[0, 1, 0], [0, 0, 1], [0, 0, 0]])},\n        # Case 3: Observational, n=80, alpha=0.05\n        {'n': 80, 'alpha': 0.05, 'intervention': None, 'vars': [0, 1, 2, 3],\n         'true_adj': np.array([[0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 0]])},\n        # Case 4: Intervention do(x3=0), n=1000, alpha=0.01, on vars {x1, x2, x4}\n        {'n': 1000, 'alpha': 0.01, 'intervention': (2, 0.0), 'vars': [0, 1, 3],\n         'true_adj': np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])}\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Generate Data\n        data_full = _generate_data(case['n'], SIGMAS, W_13, W_23, W_34, case['intervention'])\n        data_subset = data_full[:, case['vars']]\n\n        # 2. Discover DAG\n        inferred_adj = _discover_dag(data_subset, case['alpha'])\n\n        # 3. Calculate SHD\n        shd = _calculate_shd(inferred_adj, case['true_adj'])\n        results.append(shd)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _generate_data(n, sigmas, w13, w23, w34, intervention=None):\n    \"\"\"Generates synthetic data from the linear Gaussian SEM.\"\"\"\n    eps = [np.random.normal(0, s, n) for s in sigmas]\n    x = [np.zeros(n) for _ in range(4)]\n\n    # Equations are solved in topological order, applying interventions\n    x[0] = eps[0]\n    if intervention and intervention[0] == 0:\n        x[0] = np.full(n, intervention[1])\n    \n    x[1] = eps[1]\n    if intervention and intervention[0] == 1:\n        x[1] = np.full(n, intervention[1])\n    \n    x[2] = w13 * x[0] + w23 * x[1] + eps[2]\n    if intervention and intervention[0] == 2:\n        x[2] = np.full(n, intervention[1])\n        \n    x[3] = w34 * x[2] + eps[3]\n    if intervention and intervention[0] == 3:\n        x[3] = np.full(n, intervention[1])\n        \n    return np.stack(x, axis=1)\n\ndef _ci_test(data, i, j, S, alpha):\n    \"\"\"Performs a conditional independence test using Fisher's z-transform.\"\"\"\n    n = data.shape[0]\n    k = len(S)\n    \n    if k == 0:\n        r = np.corrcoef(data[:, i], data[:, j])[0, 1]\n    else:\n        y_i = data[:, i]\n        y_j = data[:, j]\n        X_S = data[:, S]\n        \n        X_S_intercept = np.hstack([np.ones((n, 1)), X_S.reshape(n, k)])\n        \n        try:\n            beta_i = np.linalg.lstsq(X_S_intercept, y_i, rcond=None)[0]\n            res_i = y_i - X_S_intercept @ beta_i\n            beta_j = np.linalg.lstsq(X_S_intercept, y_j, rcond=None)[0]\n            res_j = y_j - X_S_intercept @ beta_j\n        except np.linalg.LinAlgError:\n            return False\n\n        with np.errstate(invalid='ignore'):\n            r_matrix = np.corrcoef(res_i, res_j)\n        \n        if np.any(np.isnan(r_matrix)) or np.any(np.isinf(r_matrix)):\n             return False # Treat as dependent if correlation is undefined\n        r = r_matrix[0, 1]\n\n    # Clip for numerical stability with arctanh\n    r = np.clip(r, -1.0 + 1e-12, 1.0 - 1e-12)\n    # Fisher's z-transform\n    z = 0.5 * np.log((1 + r) / (1 - r)) * np.sqrt(n - k - 3)\n    p_val = 2 * (1 - norm.cdf(np.abs(z)))\n    \n    return p_val > alpha\n\ndef _discover_dag(data, alpha):\n    \"\"\"Infers the DAG structure using a simplified PC algorithm.\"\"\"\n    num_vars = data.shape[1]\n    \n    # 1. Skeleton Discovery\n    adj = np.ones((num_vars, num_vars), dtype=int) - np.eye(num_vars)\n    separating_sets = {}\n    \n    for d in range(3): # Conditioning sets up to size 2\n        for i in range(num_vars):\n            for j in range(i + 1, num_vars):\n                if adj[i, j] == 1:\n                    neighbors_i = [k for k in range(num_vars) if adj[i, k] == 1 and k != j]\n                    if len(neighbors_i) >= d:\n                        for S in combinations(neighbors_i, d):\n                            if _ci_test(data, i, j, list(S), alpha):\n                                adj[i, j] = adj[j, i] = 0\n                                separating_sets[(i, j)] = list(S)\n                                separating_sets[(j, i)] = list(S)\n                                break\n\n    # 2. V-structure orientation\n    pdag = adj.copy() # Partially Directed Acyclic Graph\n    for i, j, k in permutations(range(num_vars), 3):\n        # Find uncoupled colliders i-k-j where i and j are not adjacent\n        if pdag[i, k] == 1 and pdag[k, j] == 1 and pdag[i, j] == 0 and pdag[j, i] == 0:\n            sep_set = separating_sets.get((i, j))\n            if sep_set is not None and k not in sep_set:\n                pdag[k, i] = 0\n                pdag[k, j] = 0\n\n    # 3. Meek Rule R1 Application\n    changed = True\n    while changed:\n        changed = False\n        for i, j, k in permutations(range(num_vars), 3):\n            is_i_to_j = (pdag[i, j] == 1 and pdag[j, i] == 0)\n            is_j_undir_k = (pdag[j, k] == 1 and pdag[k, j] == 1)\n            is_i_k_nonadj = (pdag[i, k] == 0 and pdag[k, i] == 0)\n            \n            if is_i_to_j and is_j_undir_k and is_i_k_nonadj:\n                pdag[k, j] = 0\n                changed = True\n                \n    return pdag\n\ndef _calculate_shd(inferred_adj, true_adj):\n    \"\"\"Calculates the Structural Hamming Distance.\"\"\"\n    shd = 0\n    num_vars = inferred_adj.shape[0]\n    for i in range(num_vars):\n        for j in range(i + 1, num_vars):\n            true_edge_exists = (true_adj[i, j] != 0 or true_adj[j, i] != 0)\n            inferred_edge_exists = (inferred_adj[i, j] != 0 or inferred_adj[j, i] != 0)\n            \n            if true_edge_exists != inferred_edge_exists:\n                shd += 1 # Extra or missing edge\n            elif true_edge_exists: # Both have an edge, check orientation\n                if (inferred_adj[i, j] != true_adj[i, j] or inferred_adj[j, i] != true_adj[j, i]):\n                    shd += 1 # Wrong orientation or undirected vs directed\n    return shd\n\nsolve()\n```"
        },
        {
            "introduction": "许多科学模型计算成本高昂且涉及众多参数，难以分析。本练习介绍“活动子空间”这一强大的方法，用以在一个高维参数空间中“发现”一个低维结构。你将实现一个算法，利用梯度信息来识别出控制模型输出大部分方差的少数几个关键方向，这是模型简化和不确定性量化的关键技能。",
            "id": "3157324",
            "problem": "考虑一个在计算科学中使用的参数到输出的映射 $f:\\mathbb{R}^d \\to \\mathbb{R}$，该映射用于近似一个由偏微分方程（PDE）产生的关注量（QOI）。在活性子空间方法中，我们寻求参数空间中的一些方向，沿着这些方向，$f$ 相对于输入分布的平均变化最大。从一阶泰勒展开出发，局部变化由梯度 $\\nabla_\\theta f(\\theta)$ 控制。沿单位向量 $u$ 的全局平均方向敏感度由二次型 $u^\\top C u$ 量化，其中平均梯度外积矩阵定义为\n$$\nC \\equiv \\mathbb{E}\\left[\\nabla_\\theta f(\\theta)\\,\\nabla_\\theta f(\\theta)^\\top\\right],\n$$\n其中期望是关于 $\\theta$ 的分布计算的。活性子空间由 $C$ 的主特征向量构成；较大的特征值表示具有更高均方方向导数的方向。\n\n你的任务是实现一个完整的程序，对于给定的标量函数 $f$、维度 $d$、样本量 $N$、阈值 $\\tau$ 和随机种子 $s$，该程序将：\n(1) 对 $i=1,\\dots,N$，独立地采样输入参数 $\\theta_i \\sim \\mathcal{N}(0, I_d)$，\n(2) 仅使用多元微积分的基本法则（特别是链式法则和线性性质）计算精确的解析梯度 $\\nabla_\\theta f(\\theta_i)$，而不使用任何有限差分近似，\n(3) 通过蒙特卡洛平均估计矩阵 $C$\n$$\n\\widehat{C} = \\frac{1}{N}\\sum_{i=1}^N \\left(\\nabla_\\theta f(\\theta_i)\\right)\\left(\\nabla_\\theta f(\\theta_i)\\right)^\\top,\n$$\n(4) 计算 $\\widehat{C}$ 的特征值，将它们按降序排序，并确定满足以下条件的最小非负整数 $m \\in \\{0,1,\\dots,d\\}$\n$$\n\\frac{\\sum_{j=1}^m \\lambda_j}{\\sum_{j=1}^d \\lambda_j} \\ge \\tau,\n$$\n其中 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_d \\ge 0$ 是 $\\widehat{C}$ 的特征值。如果分母为零，则按照约定将 $m$ 设置为 $0$。返回每个测试用例的结果 $m$。\n\n你的推导和设计必须基于梯度的基本定义及其在一阶敏感度中的作用。程序不得使用任何外部数据或用户输入。\n\n使用以下三个科学上合理的函数 $f$，它们能分离不同的变化机制，并且适合进行解析梯度计算：\n(1) 一个由线性泛函与三次非线性复合生成的秩为1的函数：\n$$\nf_{\\text{rank1}}(\\theta) = \\left(w^\\top \\theta\\right)^3,\n$$\n其中 $w\\in\\mathbb{R}^d$ 是一个从标准正态分布中抽取然后归一化为单位长度的单位向量。\n(2) 一个各向同性的二次函数：\n$$\nf_{\\text{iso}}(\\theta) = \\frac{1}{2}\\left\\|\\theta\\right\\|_2^2,\n$$\n在标准正态输入下，该函数在所有方向上产生相同的敏感度。\n(3) 一个由两个正交三次脊函数之和构成的秩为2的函数：\n$$\nf_{\\text{rank2}}(\\theta) = \\left(v_1^\\top \\theta\\right)^3 + \\left(v_2^\\top \\theta\\right)^3,\n$$\n其中 $v_1, v_2 \\in \\mathbb{R}^d$ 是标准正交单位向量，通过抽取两个独立的标准正态随机向量然后将它们标准正交化（例如，通过 Gram–Schmidt 方法）来构建。\n\n所有的随机向量（$w$, $v_1$, $v_2$）都必须使用标准的伪随机数生成器从指定的种子 $s$ 确定性地生成，然后归一化为单位长度（对于向量对则进行标准正交化）。输入参数必须使用相同的种子 $s$ 从 $\\theta_i \\sim \\mathcal{N}(0, I_d)$ 中采样，以保证可复现性。\n\n设计一个测试套件，覆盖理想路径情况、各向同性边界行为和多方向活性：\n测试用例 1：函数 $f_{\\text{rank1}}$，$d = 7$，$N = 4096$，$\\tau = 0.9$，种子 $s = 11$。\n测试用例 2：函数 $f_{\\text{iso}}$，$d = 5$，$N = 2048$，$\\tau = 0.68$，种子 $s = 13$。\n测试用例 3：函数 $f_{\\text{rank2}}$，$d = 10$，$N = 4096$，$\\tau = 0.9$，种子 $s = 17$。\n测试用例 4：函数 $f_{\\text{iso}}$，$d = 4$，$N = 1024$，$\\tau = 0.99$，种子 $s = 19$。\n\n所有量都是无量纲的；不涉及物理单位或角度。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[m_1,m_2,m_3,m_4]$），其中每个 $m_k$ 是根据上述阈值定义计算出的第 $k$ 个测试用例的整数活性子空间维度。",
            "solution": "所提出的问题是有效的。它在科学上基于敏感性分析和不确定性量化的原理，特别是活性子空间方法。该问题是适定的，所有必要的函数、参数和过程都已明确定义，以确保一个唯一的、确定性的解。它是客观的，不含任何非科学或模棱两可的语言。因此，我们可以进行形式化的求解。\n\n核心目标是确定给定函数 $f(\\theta)$ 的活性子空间的维度 $m$。这个维度代表了参数空间 $\\mathbb{R}^d$ 中主导方向的数量，沿着这些方向，函数的输出平均表现出最显著的变化。该方法基于矩阵 $C = \\mathbb{E}\\left[\\nabla_\\theta f(\\theta)\\,\\nabla_\\theta f(\\theta)^\\top\\right]$ 的谱分析，该矩阵聚合了整个输入参数分布上的局部、由梯度驱动的敏感度信息。我们将使用蒙特卡洛方法估计 $C$，然后通过分析其特征值的累积能量来找到 $m$。\n\n解法结构如下：首先，我们为指定的函数推导解析梯度。其次，我们形式化确定性地生成所需随机向量和输入样本的过程。第三，我们详细说明估计矩阵 $C$ 并计算其特征值的算法。最后，我们描述基于指定的能量阈值 $\\tau$ 计算活性子空间维度 $m$ 的方法。\n\n**1. 解析梯度推导**\n\n问题要求使用精确的解析梯度。我们使用多元微积分的基本法则为三个函数族中的每一个推导这些梯度。设 $\\theta = [\\theta_1, \\dots, \\theta_d]^\\top \\in \\mathbb{R}^d$。\n\n*   **函数 1: $f_{\\text{rank1}}(\\theta) = (w^\\top \\theta)^3$**\n    该函数是线性泛函 $u(\\theta) = w^\\top \\theta = \\sum_{k=1}^d w_k \\theta_k$ 和三次函数 $g(u) = u^3$ 的复合。应用链式法则，梯度为 $\\nabla_\\theta f = \\frac{dg}{du} \\nabla_\\theta u$。\n    $g(u)$ 的导数是 $\\frac{dg}{du} = 3u^2 = 3(w^\\top \\theta)^2$。\n    $u(\\theta)$ 相对于 $\\theta$ 的梯度是 $\\nabla_\\theta (w^\\top \\theta) = w$。\n    因此，$f_{\\text{rank1}}$ 的梯度是：\n    $$\n    \\nabla_\\theta f_{\\text{rank1}}(\\theta) = 3(w^\\top \\theta)^2 w\n    $$\n\n*   **函数 2: $f_{\\text{iso}}(\\theta) = \\frac{1}{2}\\left\\|\\theta\\right\\|_2^2$**\n    这个各向同性的二次函数可以写作 $f_{\\text{iso}}(\\theta) = \\frac{1}{2} \\sum_{k=1}^d \\theta_k^2$。关于任意分量 $\\theta_j$ 的偏导数是：\n    $$\n    \\frac{\\partial f_{\\text{iso}}}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\left( \\frac{1}{2} \\sum_{k=1}^d \\theta_k^2 \\right) = \\frac{1}{2} (2\\theta_j) = \\theta_j\n    $$\n    由于这对所有 $j=1, \\dots, d$ 都成立，梯度向量就是输入向量本身：\n    $$\n    \\nabla_\\theta f_{\\text{iso}}(\\theta) = \\theta\n    $$\n\n*   **函数 3: $f_{\\text{rank2}}(\\theta) = (v_1^\\top \\theta)^3 + (v_2^\\top \\theta)^3$**\n    梯度算子是线性的。因此，我们可以分别计算每一项的梯度，然后将结果相加。每一项的形式都与 $f_{\\text{rank1}}$ 相同。应用我们之前的结果，得到：\n    $$\n    \\nabla_\\theta \\left( (v_1^\\top \\theta)^3 \\right) = 3(v_1^\\top \\theta)^2 v_1\n    $$\n    $$\n    \\nabla_\\theta \\left( (v_2^\\top \\theta)^3 \\right) = 3(v_2^\\top \\theta)^2 v_2\n    $$\n    $f_{\\text{rank2}}$ 的梯度是这两个向量的和：\n    $$\n    \\nabla_\\theta f_{\\text{rank2}}(\\theta) = 3(v_1^\\top \\theta)^2 v_1 + 3(v_2^\\top \\theta)^2 v_2\n    $$\n\n**2. 随机向量和样本的确定性生成**\n\n为确保可复现性，所有随机数必须从给定的种子 $s$ 确定性地生成。一个伪随机数生成器（PRNG）用 $s$ 初始化。所有需要的随机向量都使用这同一个 PRNG 实例生成。\n\n*   **参数向量 ($w, v_1, v_2$):**\n    对于 $f_{\\text{rank1}}$，从标准正态分布 $\\mathcal{N}(0, I_d)$ 中抽取一个向量，然后将其欧几里得范数归一化为 $1$，以产生单位向量 $w$。\n    对于 $f_{\\text{rank2}}$，从 $\\mathcal{N}(0, I_d)$ 中抽取两个独立的向量，记为 $u_1$ 和 $u_2$。然后使用 Gram-Schmidt 过程将它们标准正交化。\n    首先，通过归一化 $u_1$ 得到 $v_1$：$v_1 = u_1 / \\|u_1\\|_2$。\n    其次，移除 $u_2$ 中平行于 $v_1$ 的分量：$u_2' = u_2 - (v_1^\\top u_2) v_1$。\n    最后，将得到的向量 $u_2'$ 归一化以产生 $v_2 = u_2' / \\|u_2'\\|_2$。对于连续分布，$u_1$ 和 $u_2$ 线性相关（使得 $\\|u_2'\\|_2 = 0$）的事件概率为零，但鲁棒的代码应该处理这种不太可能发生的情况。\n\n*   **输入样本 ($\\theta_i$):**\n    对于每个测试用例，从多元标准正态分布 $\\mathcal{N}(0, I_d)$ 中抽取 $N$ 个独立样本 $\\theta_i$（$i=1, \\dots, N$），其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n\n**3. 蒙特卡洛估计与特征分解**\n\n矩阵 $C$ 通过梯度外积的样本均值来估计，记为 $\\widehat{C}$：\n$$\n\\widehat{C} = \\frac{1}{N}\\sum_{i=1}^N g_i g_i^\\top\n$$\n其中 $g_i = \\nabla_\\theta f(\\theta_i)$。算法流程如下：\n1. 初始化一个 $d \\times d$ 矩阵 $\\widehat{C}$，其所有元素均为零。\n2. 对每个样本 $i=1, \\dots, N$：\n   a. 抽取一个样本 $\\theta_i \\sim \\mathcal{N}(0, I_d)$。\n   b. 使用相应的推导公式计算解析梯度向量 $g_i = \\nabla_\\theta f(\\theta_i)$。\n   c. 计算外积 $g_i g_i^\\top$，这是一个 $d \\times d$ 矩阵。\n   d. 将此矩阵加到累加器 $\\widehat{C}$ 中。\n3. 循环结束后，将累加的矩阵除以样本量 $N$。\n\n根据其构造，矩阵 $\\widehat{C}$ 是对称和半正定的。然后我们计算它的特征值。由于 $\\widehat{C}$ 是实对称的，其所有特征值都是实数且非负。为此，我们使用一个高效的数值算法，例如在 `numpy.linalg.eigh` 中实现的算法。得到的特征值 $\\{\\lambda_j\\}_{j=1}^d$ 按降序排序：$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_d \\ge 0$。\n\n**4. 活性子空间维度计算**\n\n最后一步是找到最小的非负整数 $m$，使得前 $m$ 个特征值捕获的方差比例达到或超过阈值 $\\tau$。\n总方差是所有特征值的和，$S = \\sum_{j=1}^d \\lambda_j$。前 $k$ 个特征值捕获的累积方差是 $\\sum_{j=1}^k \\lambda_j$。我们寻求满足以下条件的最小 $m \\in \\{0, 1, \\dots, d\\}$：\n$$\n\\frac{\\sum_{j=1}^m \\lambda_j}{S} \\ge \\tau\n$$\n流程如下：\n1. 计算总和 $S$。如果 $S$ 实际上为零（例如，小于机器精度容差），这表明函数为常数，其梯度处处为零。在这种情况下，按照规定，我们设置 $m=0$。\n2. 我们将一个累积和初始化为零。我们从 $j=1$ 到 $d$ 遍历排序后的特征值，将每个 $\\lambda_j$ 加到累积和中。\n3. 在迭代的每一步中，我们检查当前累积和与总和 $S$ 的比率是否大于或等于 $\\tau$。\n4. 第一个满足此条件的索引 $j$ 即为活性子空间维度，即 $m=j$。然后终止当前测试用例的计算过程。\n5. 如果循环完成而条件仍未满足（例如，当 $\\tau=1.0$ 时由于浮点数不精确性可能发生），则维度取为 $d$。\n\n这就完成了解决该问题的理论和算法框架。实现将直接遵循为每个测试用例提供的这些步骤。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for active subspace dimension calculation.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'f_type': 'rank1', 'd': 7, 'N': 4096, 'tau': 0.9, 's': 11},\n        {'f_type': 'iso', 'd': 5, 'N': 2048, 'tau': 0.68, 's': 13},\n        {'f_type': 'rank2', 'd': 10, 'N': 4096, 'tau': 0.9, 's': 17},\n        {'f_type': 'iso', 'd': 4, 'N': 1024, 'tau': 0.99, 's': 19},\n    ]\n\n    results = []\n    for case in test_cases:\n        m = compute_active_subspace_dimension(**case)\n        results.append(m)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_active_subspace_dimension(f_type, d, N, tau, s):\n    \"\"\"\n    Computes the active subspace dimension for a single test case.\n\n    Args:\n        f_type (str): The type of function ('rank1', 'iso', 'rank2').\n        d (int): The dimension of the parameter space.\n        N (int): The number of Monte Carlo samples.\n        tau (float): The energy threshold for the active subspace.\n        s (int): The random seed for reproducibility.\n\n    Returns:\n        int: The computed active subspace dimension 'm'.\n    \"\"\"\n    \n    # 1. Initialize deterministic random number generator\n    rng = np.random.default_rng(s)\n\n    # 2. Define gradient function based on f_type\n    grad_func = None\n    if f_type == 'rank1':\n        # Generate the fixed random vector w\n        w_raw = rng.normal(size=d)\n        w = w_raw / np.linalg.norm(w_raw)\n        \n        def grad_rank1(theta):\n            # Gradient of f(theta) = (w^T * theta)^3 is 3 * (w^T * theta)^2 * w\n            return 3 * (w.T @ theta)**2 * w\n        grad_func = grad_rank1\n\n    elif f_type == 'iso':\n        # Gradient of f(theta) = 0.5 * ||theta||^2 is theta\n        grad_func = lambda theta: theta\n\n    elif f_type == 'rank2':\n        # Generate two orthonormal vectors v1, v2 using Gram-Schmidt\n        u1_raw = rng.normal(size=d)\n        v1 = u1_raw / np.linalg.norm(u1_raw)\n        \n        u2_raw = rng.normal(size=d)\n        proj_u2_on_v1 = (v1.T @ u2_raw) * v1\n        u2_prime = u2_raw - proj_u2_on_v1\n        norm_u2_prime = np.linalg.norm(u2_prime)\n        \n        # This case is highly improbable with continuous distributions\n        if norm_u2_prime  1e-15:\n            # Regenerate until we get a linearly independent vector\n            while norm_u2_prime  1e-15:\n                u2_raw = rng.normal(size=d)\n                proj_u2_on_v1 = (v1.T @ u2_raw) * v1\n                u2_prime = u2_raw - proj_u2_on_v1\n                norm_u2_prime = np.linalg.norm(u2_prime)\n\n        v2 = u2_prime / norm_u2_prime\n        \n        def grad_rank2(theta):\n            # Gradient of f(theta) = (v1^T*theta)^3 + (v2^T*theta)^3\n            # is 3*(v1^T*theta)^2*v1 + 3*(v2^T*theta)^2*v2\n            term1 = 3 * (v1.T @ theta)**2 * v1\n            term2 = 3 * (v2.T @ theta)**2 * v2\n            return term1 + term2\n        grad_func = grad_rank2\n\n    # 3. Estimate the matrix C using Monte Carlo\n    C_hat = np.zeros((d, d))\n    \n    # Generate all samples at once for efficiency\n    thetas = rng.normal(size=(N, d))\n    \n    for i in range(N):\n        theta_i = thetas[i, :]\n        grad = grad_func(theta_i)\n        C_hat += np.outer(grad, grad)\n        \n    C_hat /= N\n    \n    # 4. Compute eigenvalues and determine the active subspace dimension m\n    \n    # C_hat is real and symmetric, so use eigh for efficiency and stability.\n    # It returns eigenvalues in ascending order.\n    eigenvalues = np.linalg.eigh(C_hat)[0]\n    \n    # Sort eigenvalues in descending order and ensure non-negativity\n    eigenvalues = np.sort(eigenvalues)[::-1]\n    eigenvalues = np.maximum(0, eigenvalues)\n\n    total_eig_sum = np.sum(eigenvalues)\n\n    # Handle the case where the function is constant (zero gradient)\n    if total_eig_sum  1e-15:\n        return 0\n    \n    # Find the smallest m such that the cumulative energy exceeds tau\n    cumulative_eig_sum = 0\n    for i in range(d):\n        cumulative_eig_sum += eigenvalues[i]\n        if cumulative_eig_sum / total_eig_sum >= tau:\n            return i + 1\n\n    # If threshold is not met even with all eigenvalues (e.g., tau=1.0 and rounding)\n    return d\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}