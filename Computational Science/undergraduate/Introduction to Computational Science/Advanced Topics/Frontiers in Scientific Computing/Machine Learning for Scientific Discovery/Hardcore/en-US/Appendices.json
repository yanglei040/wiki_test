{
    "hands_on_practices": [
        {
            "introduction": "A primary goal of science is to move beyond correlation and uncover the underlying cause-and-effect relationships that structure a system. This practice provides direct, hands-on experience with computational causal discovery, a cornerstone of modern scientific machine learning. You will implement a foundational algorithm to infer the \"wiring diagram\" of a hypothetical gene regulatory network from raw data, learning to distinguish mere statistical association from genuine causal influence by leveraging the logic of conditional independence .",
            "id": "3157243",
            "problem": "Consider a small gene regulatory network modeled as a Directed Acyclic Graph (DAG), where variables $x_1, x_2, x_3, x_4$ follow a linear Gaussian Structural Equation Model. The data generation follows the assumptions of the Causal Markov Condition and Faithfulness under linear Gaussian noise. Specifically, each variable $x_i$ is given by a structural equation of the form $x_i = \\sum_{j \\in \\mathrm{Pa}(i)} w_{ji} x_j + \\varepsilon_i$, where $\\mathrm{Pa}(i)$ denotes the parent set of $x_i$ in the DAG, $w_{ji} \\in \\mathbb{R}$ are fixed coefficients, and $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_i^2)$ are mutually independent noise terms.\n\nYou are given the ground-truth mechanism for a $4$-variable network representing a minimal gene regulatory motif:\n- $x_1 = \\varepsilon_1$,\n- $x_2 = \\varepsilon_2$,\n- $x_3 = w_{13} x_1 + w_{23} x_2 + \\varepsilon_3$,\n- $x_4 = w_{34} x_3 + \\varepsilon_4$,\nwith coefficients $w_{13} = 0.9$, $w_{23} = 0.8$, $w_{34} = 0.7$, and noise standard deviations $\\sigma_1 = 1.0$, $\\sigma_2 = 1.0$, $\\sigma_3 = 0.5$, $\\sigma_4 = 0.5$. All variables are centered at $0$ by construction. This defines a ground-truth DAG with edges $x_1 \\rightarrow x_3$, $x_2 \\rightarrow x_3$, and $x_3 \\rightarrow x_4$.\n\nYour task is to implement a constraint-based causal discovery algorithm to infer the DAG from samples, and to test the stability of the inferred structure under synthetic interventions $do(x_i = \\tilde{x})$, which cut all incoming edges into $x_i$ and set $x_i$ to a constant value $\\tilde{x}$ across all samples. To avoid degeneracies introduced by zero variance under $do(x_i=\\tilde{x})$, infer the DAG only among the non-intervened variables in interventional test cases.\n\nAlgorithmic requirements:\n- Use a constraint-based approach rooted in the Peter-Clark (PC) algorithm to learn the skeleton by conditional independence tests and then orient edges using collider identification and a single Meek orientation rule.\n- Conditional independence testing must rely on the well-tested fact that, for jointly Gaussian variables, conditional independence $x_i \\perp x_j \\mid S$ is equivalent to zero partial correlation $\\rho_{ij\\cdot S} = 0$. Implement testing via Fisher’s $z$-transform: if the sample correlation of residuals is $r$, then the test statistic is $z = \\frac{1}{2} \\ln \\left( \\frac{1+r}{1-r} \\right) \\sqrt{n - |S| - 3}$, where $n$ is the sample size and $|S|$ is the cardinality of the conditioning set. Use a two-sided test at significance level $\\alpha$.\n- Learn the undirected skeleton by removing edges when conditional independence is detected with conditioning sets up to size $2$.\n- Orient v-structures (colliders): for triples $(i,k,j)$ with $i - k$ and $j - k$ edges and $i$ and $j$ non-adjacent, orient $i \\rightarrow k \\leftarrow j$ if $k$ is not in any separating set found for $(i,j)$ during skeleton discovery.\n- Apply one Meek orientation rule to propagate directions: if $i \\rightarrow j$ and $j - k$ and $i$ and $k$ are non-adjacent, orient $j \\rightarrow k$.\n\nEvaluation metric:\n- Compute the Structural Hamming Distance (SHD) between the inferred directed graph and the ground-truth directed graph, restricted to the set of non-intervened variables for interventional cases. For each unordered pair $\\{i,j\\}$ in the restricted set, count $1$ if there is a mismatch between the inferred and true adjacency: a missing edge when the truth has one, an extra edge when the truth has none, or a wrong orientation. If an inferred undirected edge remains where the truth has a directed edge, count $1$.\n\nSynthetic data generation:\n- Observational samples are drawn from the structural equations above.\n- Interventional samples $do(x_i = \\tilde{x})$ are produced by replacing the structural equation of $x_i$ with $x_i := \\tilde{x}$ identically across samples; all outgoing edges remain intact, and all incoming edges into $x_i$ are effectively cut for the purposes of data generation. Inference should be performed only on the subgraph induced by non-intervened variables.\n\nTest suite:\nProvide the SHD results for the following $4$ test cases. In all cases, use the ground-truth weights and noise parameters specified above except where noted. The significance level for independence tests is $\\alpha$ as specified per case, and conditioning sets are limited to size at most $2$.\n\n- Test case $1$ (happy path): observational data with $n=1000$, $\\alpha = 0.01$, infer over variables $\\{x_1, x_2, x_3, x_4\\}$.\n- Test case $2$ (intervention on a parent): interventional data $do(x_1 = \\tilde{x})$ with $\\tilde{x} = 0$, $n=1000$, $\\alpha = 0.01$, infer over variables $\\{x_2, x_3, x_4\\}$.\n- Test case $3$ (boundary sample size): observational data with $n=80$, $\\alpha = 0.05$, infer over variables $\\{x_1, x_2, x_3, x_4\\}$.\n- Test case $4$ (intervention on collider): interventional data $do(x_3 = \\tilde{x})$ with $\\tilde{x} = 0$, $n=1000$, $\\alpha = 0.01$, infer over variables $\\{x_1, x_2, x_4\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the SHD results for the four test cases as a comma-separated list enclosed in square brackets (e.g., $[h_1,h_2,h_3,h_4]$), where each $h_i$ is an integer.",
            "solution": "The problem requires the implementation of a constraint-based causal discovery algorithm to infer the structure of a small gene regulatory network modeled as a Directed Acyclic Graph (DAG). The underlying system is a linear Gaussian Structural Equation Model (SEM). The solution involves data generation, causal structure inference, and evaluation against a ground-truth model for several test cases, including observational and interventional data.\n\nFirst, we address the problem validation. All givens, including the structural equations, parameters ($w_{ij}, \\sigma_i$), algorithmic requirements (PC-like skeletonization, v-structure orientation, one Meek rule), and evaluation metric (Structural Hamming Distance, SHD), are explicitly stated. The problem is scientifically grounded in the well-established theory of causal inference, particularly the Causal Markov Condition and Faithfulness assumptions, which provide the theoretical link between statistical independence in data and the graphical property of d-separation in the causal DAG. The formulation is well-posed, objective, and contains no scientific or logical contradictions. Therefore, the problem is deemed valid.\n\nThe solution proceeds in a sequence of principled steps:\n\n**1. Data Generation from the Structural Equation Model**\n\nThe ground-truth network is defined by a set of linear equations with Gaussian noise, representing causal mechanisms:\n$$x_1 = \\varepsilon_1$$\n$$x_2 = \\varepsilon_2$$\n$$x_3 = w_{13} x_1 + w_{23} x_2 + \\varepsilon_3$$\n$$x_4 = w_{34} x_3 + \\varepsilon_4$$\nwhere $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)$ are independent noise terms. Samples are generated by first drawing from the noise distributions and then computing the values of $x_i$ in topological order. For interventional data, $do(x_i = \\tilde{x})$, the structural equation for $x_i$ is replaced by the assignment $x_i := \\tilde{x}$ before computing the values of its descendants. This correctly models the effect of an ideal intervention, which breaks all causal influences into the intervened variable.\n\n**2. Conditional Independence (CI) Testing**\n\nThe cornerstone of constraint-based causal discovery is the ability to test for conditional independence. For jointly Gaussian variables, the condition $x_i \\perp x_j \\mid S$ is equivalent to zero partial correlation, $\\rho_{ij \\cdot S} = 0$. We test this hypothesis using sample data. The sample partial correlation $r$ is computed by finding the correlation between the residuals of $x_i$ and $x_j$ after regressing both on the variables in the conditioning set $S$.\n\nThe statistical significance of the sample partial correlation is assessed using Fisher's $z$-transformation. The test statistic is given by:\n$$z = \\frac{1}{2} \\ln \\left( \\frac{1+r}{1-r} \\right) \\sqrt{n - |S| - 3}$$\nwhere $n$ is the sample size and $|S|$ is the size of the conditioning set. Under the null hypothesis of zero partial correlation, $z$ follows a standard normal distribution, $\\mathcal{N}(0, 1)$. We compute the two-sided p-value from this $z$-statistic. If the p-value is greater than a predefined significance level $\\alpha$, we fail to reject the null hypothesis and conclude that the variables are conditionally independent.\n\n**3. Causal Graph Inference Algorithm**\n\nWe implement a simplified version of the Peter-Clark (PC) algorithm, which consists of three main stages:\n\n*   **Skeleton Discovery**: We begin with a fully connected undirected graph over the set of variables. We then systematically test for conditional independence between every pair of adjacent variables $(i,j)$ for conditioning sets $S$ of increasing size (here, $|S|=0, 1, 2$). If an independence $x_i \\perp x_j \\mid S$ is found, the edge between $i$ and $j$ is removed, and the set $S$ is recorded as a separating set for the pair $(i, j)$. This process thins the graph to an undirected skeleton that represents the conditional independence relationships present in the data.\n\n*   **V-Structure Orientation**: After establishing the skeleton, we orient a specific type of substructure known as a v-structure or collider. For any triple of variables $(i, k, j)$ forming an uncoupled chain $i - k - j$ (i.e., $i$ and $j$ are not adjacent), we orient the edges as $i \\rightarrow k \\leftarrow j$ if and only if the node $k$ is *not* in the separating set that was found to make $i$ and $j$ independent. This rule follows from the principles of d-separation: a collider $k$ on a path between $i$ and $j$ does not block the path, but conditioning on it opens the path. Therefore, if $i$ and $j$ are made independent by a set $S$ *not* containing $k$, then $k$ must be a collider on the path between them.\n\n*   **Edge Orientation Propagation (Meek's Rules)**: Finally, we apply further orientation rules to propagate the directions from the identified v-structures throughout the graph, avoiding the creation of new v-structures or directed cycles. As specified, we use the first Meek rule: If there is an edge $i \\rightarrow j$ and an undirected edge $j - k$, and nodes $i$ and $k$ are not adjacent, then orient the edge $j - k$ as $j \\rightarrow k$. This process is applied iteratively until no more edges can be oriented.\n\n**4. Evaluation via Structural Hamming Distance (SHD)**\n\nThe accuracy of the inferred graph is quantified by the Structural Hamming Distance (SHD) against the ground-truth DAG. The SHD counts the number of edge discrepancies between the two graphs:\n1.  **Extra Edge**: An edge exists in the inferred graph but not in the true graph.\n2.  **Missing Edge**: An edge exists in the true graph but not in the inferred graph.\n3.  **Incorrect Orientation**: An edge exists in both graphs but has a different orientation (e.g., $i \\rightarrow j$ in true vs. $j \\rightarrow i$ or $i - j$ in inferred).\n\nThe algorithm is applied to four distinct test cases to assess its performance under varying sample sizes and in the presence of interventions. The resulting SHD for each case is computed and reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom itertools import combinations, permutations\n\ndef solve():\n    \"\"\"\n    Main function to run the four test cases and print the results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of results.\n    np.random.seed(42)\n\n    # Ground-truth parameters\n    W_13 = 0.9\n    W_23 = 0.8\n    W_34 = 0.7\n    SIGMAS = [1.0, 1.0, 0.5, 0.5]\n\n    test_cases = [\n        # Case 1: Observational, n=1000, alpha=0.01\n        {'n': 1000, 'alpha': 0.01, 'intervention': None, 'vars': [0, 1, 2, 3],\n         'true_adj': np.array([[0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 0]])},\n        # Case 2: Intervention do(x1=0), n=1000, alpha=0.01, on vars {x2, x3, x4}\n        {'n': 1000, 'alpha': 0.01, 'intervention': (0, 0.0), 'vars': [1, 2, 3],\n         'true_adj': np.array([[0, 1, 0], [0, 0, 1], [0, 0, 0]])},\n        # Case 3: Observational, n=80, alpha=0.05\n        {'n': 80, 'alpha': 0.05, 'intervention': None, 'vars': [0, 1, 2, 3],\n         'true_adj': np.array([[0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 0, 0, 0]])},\n        # Case 4: Intervention do(x3=0), n=1000, alpha=0.01, on vars {x1, x2, x4}\n        {'n': 1000, 'alpha': 0.01, 'intervention': (2, 0.0), 'vars': [0, 1, 3],\n         'true_adj': np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])}\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Generate Data\n        data_full = _generate_data(case['n'], SIGMAS, W_13, W_23, W_34, case['intervention'])\n        data_subset = data_full[:, case['vars']]\n\n        # 2. Discover DAG\n        inferred_adj = _discover_dag(data_subset, case['alpha'])\n\n        # 3. Calculate SHD\n        shd = _calculate_shd(inferred_adj, case['true_adj'])\n        results.append(shd)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _generate_data(n, sigmas, w13, w23, w34, intervention=None):\n    \"\"\"Generates synthetic data from the linear Gaussian SEM.\"\"\"\n    eps = [np.random.normal(0, s, n) for s in sigmas]\n    x = [np.zeros(n) for _ in range(4)]\n\n    # Equations are solved in topological order, applying interventions\n    x[0] = eps[0]\n    if intervention and intervention[0] == 0:\n        x[0] = np.full(n, intervention[1])\n    \n    x[1] = eps[1]\n    if intervention and intervention[0] == 1:\n        x[1] = np.full(n, intervention[1])\n    \n    x[2] = w13 * x[0] + w23 * x[1] + eps[2]\n    if intervention and intervention[0] == 2:\n        x[2] = np.full(n, intervention[1])\n        \n    x[3] = w34 * x[2] + eps[3]\n    if intervention and intervention[0] == 3:\n        x[3] = np.full(n, intervention[1])\n        \n    return np.stack(x, axis=1)\n\ndef _ci_test(data, i, j, S, alpha):\n    \"\"\"Performs a conditional independence test using Fisher's z-transform.\"\"\"\n    n = data.shape[0]\n    k = len(S)\n    \n    if k == 0:\n        r = np.corrcoef(data[:, i], data[:, j])[0, 1]\n    else:\n        y_i = data[:, i]\n        y_j = data[:, j]\n        X_S = data[:, S]\n        \n        X_S_intercept = np.hstack([np.ones((n, 1)), X_S.reshape(n, k)])\n        \n        try:\n            beta_i = np.linalg.lstsq(X_S_intercept, y_i, rcond=None)[0]\n            res_i = y_i - X_S_intercept @ beta_i\n            beta_j = np.linalg.lstsq(X_S_intercept, y_j, rcond=None)[0]\n            res_j = y_j - X_S_intercept @ beta_j\n        except np.linalg.LinAlgError:\n            return False\n\n        with np.errstate(invalid='ignore'):\n            r_matrix = np.corrcoef(res_i, res_j)\n        \n        if np.any(np.isnan(r_matrix)) or np.any(np.isinf(r_matrix)):\n             return False # Treat as dependent if correlation is undefined\n        r = r_matrix[0, 1]\n\n    # Clip for numerical stability with arctanh\n    r = np.clip(r, -1.0 + 1e-12, 1.0 - 1e-12)\n    # Fisher's z-transform\n    z = 0.5 * np.log((1 + r) / (1 - r)) * np.sqrt(n - k - 3)\n    p_val = 2 * (1 - norm.cdf(np.abs(z)))\n    \n    return p_val > alpha\n\ndef _discover_dag(data, alpha):\n    \"\"\"Infers the DAG structure using a simplified PC algorithm.\"\"\"\n    num_vars = data.shape[1]\n    \n    # 1. Skeleton Discovery\n    adj = np.ones((num_vars, num_vars), dtype=int) - np.eye(num_vars)\n    separating_sets = {}\n    \n    for d in range(3): # Conditioning sets up to size 2\n        for i in range(num_vars):\n            for j in range(i + 1, num_vars):\n                if adj[i, j] == 1:\n                    neighbors_i = [k for k in range(num_vars) if adj[i, k] == 1 and k != j]\n                    if len(neighbors_i) >= d:\n                        for S in combinations(neighbors_i, d):\n                            if _ci_test(data, i, j, list(S), alpha):\n                                adj[i, j] = adj[j, i] = 0\n                                separating_sets[(i, j)] = list(S)\n                                separating_sets[(j, i)] = list(S)\n                                break\n\n    # 2. V-structure orientation\n    pdag = adj.copy() # Partially Directed Acyclic Graph\n    for i, j, k in permutations(range(num_vars), 3):\n        # Find uncoupled colliders i-k-j where i and j are not adjacent\n        if pdag[i, k] == 1 and pdag[k, j] == 1 and pdag[i, j] == 0 and pdag[j, i] == 0:\n            sep_set = separating_sets.get((i, j))\n            if sep_set is not None and k not in sep_set:\n                pdag[k, i] = 0\n                pdag[k, j] = 0\n\n    # 3. Meek Rule R1 Application\n    changed = True\n    while changed:\n        changed = False\n        for i, j, k in permutations(range(num_vars), 3):\n            is_i_to_j = (pdag[i, j] == 1 and pdag[j, i] == 0)\n            is_j_undir_k = (pdag[j, k] == 1 and pdag[k, j] == 1)\n            is_i_k_nonadj = (pdag[i, k] == 0 and pdag[k, i] == 0)\n            \n            if is_i_to_j and is_j_undir_k and is_i_k_nonadj:\n                pdag[k, j] = 0\n                changed = True\n                \n    return pdag\n\ndef _calculate_shd(inferred_adj, true_adj):\n    \"\"\"Calculates the Structural Hamming Distance.\"\"\"\n    shd = 0\n    num_vars = inferred_adj.shape[0]\n    for i in range(num_vars):\n        for j in range(i + 1, num_vars):\n            true_edge_exists = (true_adj[i, j] != 0 or true_adj[j, i] != 0)\n            inferred_edge_exists = (inferred_adj[i, j] != 0 or inferred_adj[j, i] != 0)\n            \n            if true_edge_exists != inferred_edge_exists:\n                shd += 1 # Extra or missing edge\n            elif true_edge_exists: # Both have an edge, check orientation\n                if (inferred_adj[i, j] != true_adj[i, j] or inferred_adj[j, i] != true_adj[j, i]):\n                    shd += 1 # Wrong orientation or undirected vs directed\n    return shd\n\nsolve()\n```"
        },
        {
            "introduction": "After identifying the key players in a system, the next grand challenge is to determine the precise mathematical laws that govern their behavior. This exercise guides you through implementing a powerful sparse regression technique, a core component of methods for automatically discovering differential equations from data. By building an optimization solver from first principles, you will learn how to search through a vast library of candidate mathematical terms to find the select few that accurately describe a system's dynamics, effectively rediscovering a physical law from simulated measurements .",
            "id": "3157268",
            "problem": "You will implement a sparse regression method to select relevant terms from a polynomial library designed for partial differential equation structure discovery using a convex composite optimization model based on sparse group regularization. Start from the following fundamental base: the linear empirical risk minimization principle for supervised learning that minimizes the empirical least-squares risk, the convexity of the squared loss, and the definition of the proximal operator for structured penalties. Specifically, let the feature matrix be denoted by $X \\in \\mathbb{R}^{n \\times p}$ and the target vector by $y \\in \\mathbb{R}^{n}$. Consider the convex objective that combines least squares with a sparse group penalty of the form\n$$\n\\min_{w \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\lVert X w - y \\rVert_2^2 \\;+\\; \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 \\;+\\; \\alpha \\lVert w \\rVert_1 \\right),\n$$\nwhere $n$ is the number of samples, $p$ is the number of features, $w$ is the coefficient vector, $\\lambda \\in \\mathbb{R}_{>0}$ is the regularization strength, $\\alpha \\in (0,1)$ interpolates between group sparsity and elementwise sparsity, and $\\mathcal{G}$ is a partition of the feature indices into non-overlapping groups, with $w_g$ denoting the subvector of $w$ indexed by group $g$. The grouping $\\mathcal{G}$ you will use corresponds to terms grouped by spatial derivative order in a one-dimensional scalar field. You must solve this optimization using a principled first-order proximal gradient method with a provably valid stepsize derived from the gradient’s Lipschitz constant. Do not use any prepackaged machine learning solvers.\n\nConstruct a synthetic dataset as follows, using analytic expressions to avoid numerical differentiation errors. Let $x$ and $t$ be independent variables sampled on a rectangular grid with $N_x$ and $N_t$ equally spaced points on $[0,1]$ and $[0,1]$; let $n = N_x \\cdot N_t$. Define the scalar field\n$$\nu(x,t) = \\sin(2\\pi x)\\, e^{-t} + \\frac{1}{2} \\cos(4\\pi x)\\, e^{-0.2\\, t},\n$$\nand its first and second spatial derivatives\n$$\nu_x(x,t) = 2\\pi \\cos(2\\pi x)\\, e^{-t} - 2\\pi \\sin(4\\pi x)\\, e^{-0.2\\, t},\n$$\n$$\nu_{xx}(x,t) = - (2\\pi)^2 \\sin(2\\pi x)\\, e^{-t} - 8\\pi^2 \\cos(4\\pi x)\\, e^{-0.2\\, t}.\n$$\nBuild a polynomial library $X \\in \\mathbb{R}^{n \\times p}$ with $p = 9$ columns in the following order:\n$[\\, 1,\\; u,\\; u^2,\\; u_x,\\; u\\,u_x,\\; u^2 u_x,\\; u_{xx},\\; u\\,u_{xx},\\; (u_x)^2 \\,]$.\nDefine the true generating mechanism for the response as a linear combination of exactly two terms:\n$$\ny = \\beta_{u u_x} \\cdot (u\\,u_x) + \\beta_{u_{xx}} \\cdot u_{xx} + \\varepsilon,\n$$\nwith $\\beta_{u u_x} = 2.0$, $\\beta_{u_{xx}} = 0.1$, and additive independent Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ of prescribed standard deviation $\\sigma$. The ground-truth nonzero support of coefficients thus consists of exactly the indices corresponding to $u\\,u_x$ and $u_{xx}$ in the specified ordering.\n\nPartition the $p = 9$ features into $3$ non-overlapping groups by spatial derivative order:\n- Group $0$ (order $0$): indices $[0,1,2]$ for $[1,\\; u,\\; u^2]$.\n- Group $1$ (order $1$): indices $[3,4,5]$ for $[u_x,\\; u\\,u_x,\\; u^2 u_x]$.\n- Group $2$ (order $2$): indices $[6,7,8]$ for $[u_{xx},\\; u\\,u_{xx},\\; (u_x)^2]$.\n\nImplement a proximal gradient algorithm that minimizes the given objective. Derive a valid constant stepsize from a bound on the gradient Lipschitz constant, and implement the correct proximal operator for the sparse group penalty. Standardize each column of $X$ to zero mean and unit variance, and center $y$ by subtracting its mean before optimization. After optimization, determine the predicted support by thresholding the absolute value of coefficients with a user-specified threshold $\\tau > 0$, counting an index as selected if $|w_j| \\ge \\tau$. Compute precision and recall as follows: precision is the number of true positives divided by the total number of predicted positives, and recall is the number of true positives divided by the total number of true nonzeros. Express these as decimals (not percentages).\n\nYour program must implement the following test suite, run all cases, and aggregate the results. Use $N_x = 64$ and $N_t = 16$ so that $n = 1024$. For each case construct a noise vector with the specified standard deviation $\\sigma$. The regularization parameter should be set as $\\lambda = \\kappa \\cdot \\Lambda(\\alpha)$, where $\\kappa$ is a provided positive scalar and $\\Lambda(\\alpha)$ is a data-dependent scale computed from standardized $(X,y)$ by\n$$\n\\Lambda(\\alpha) = \\max\\left( \\frac{1}{n}\\lVert X^\\top y \\rVert_{\\infty} \\cdot \\frac{1}{\\alpha}, \\; \\max_{g \\in \\mathcal{G}} \\frac{1}{n}\\lVert X_g^\\top y \\rVert_2 \\cdot \\frac{1}{1 - \\alpha} \\right),\n$$\nwith the conventions that division by zero is ignored by omitting the corresponding term when $\\alpha$ equals $0$ or $1$, which will not occur in the test suite. This scaling provides a robust reference magnitude for $\\lambda$ relative to the observed data.\n\nTest suite parameters, each specified as a tuple $(\\alpha, \\kappa, \\sigma, \\tau)$:\n- Case $1$: $(0.5, 0.10, 0.0, 10^{-3})$.\n- Case $2$: $(0.5, 0.20, 0.01, 10^{-3})$.\n- Case $3$: $(0.8, 0.30, 0.02, 10^{-3})$.\n- Case $4$: $(0.2, 0.60, 0.05, 10^{-3})$.\n\nFor each case, fit the model using proximal gradient descent with a principled stepsize, then compute precision and recall of the recovered support against the ground truth. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$[\\, \\text{precision}_1, \\text{recall}_1, \\text{precision}_2, \\text{recall}_2, \\text{precision}_3, \\text{recall}_3, \\text{precision}_4, \\text{recall}_4 \\,]$,\nwith each value rounded to exactly three decimal places. No other output should be printed. Angles are not used, so no angle unit is needed. There are no physical units in this problem. All computations are to be performed using real numbers. The final output must strictly adhere to the specified format.",
            "solution": "The user has provided a valid problem statement.\n\n### Step 1: Extract Givens\n\n- **Objective Function**: Minimize the convex objective function for $w \\in \\mathbb{R}^{p}$:\n$$\n\\frac{1}{2n}\\lVert X w - y \\rVert_2^2 \\;+\\; \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 + \\alpha \\lVert w \\rVert_1 \\right)\n$$\n- **Data**: Feature matrix $X \\in \\mathbb{R}^{n \\times p}$ and target vector $y \\in \\mathbb{R}^{n}$, with $n$ samples and $p$ features.\n- **Parameters**: Regularization strength $\\lambda \\in \\mathbb{R}_{>0}$, interpolation parameter $\\alpha \\in (0,1)$.\n- **Groups**: $\\mathcal{G}$ is a partition of feature indices $\\{0, 1, \\dots, p-1\\}$ into non-overlapping groups. $w_g$ is the subvector of $w$ for group $g$.\n- **Synthetic Dataset**:\n    - Grid: $N_x = 64$ points on $[0,1]$ and $N_t = 16$ points on $[0,1]$, giving $n = N_x \\cdot N_t = 1024$ total sample points.\n    - Scalar field: $u(x,t) = \\sin(2\\pi x)\\, e^{-t} + \\frac{1}{2} \\cos(4\\pi x)\\, e^{-0.2\\, t}$.\n    - Spatial derivatives: $u_x(x,t) = 2\\pi \\cos(2\\pi x)\\, e^{-t} - 2\\pi \\sin(4\\pi x)\\, e^{-0.2\\, t}$ and $u_{xx}(x,t) = - (2\\pi)^2 \\sin(2\\pi x)\\, e^{-t} - 8\\pi^2 \\cos(4\\pi x)\\, e^{-0.2\\, t}$.\n    - Feature library ($p=9$): $X$ has columns in the order $[\\, 1,\\; u,\\; u^2,\\; u_x,\\; u\\,u_x,\\; u^2 u_x,\\; u_{xx},\\; u\\,u_{xx},\\; (u_x)^2 \\,]$.\n    - Target vector: $y = \\beta_{u u_x} \\cdot (u\\,u_x) + \\beta_{u_{xx}} \\cdot u_{xx} + \\varepsilon$, with coefficients $\\beta_{u u_x} = 2.0$, $\\beta_{u_{xx}} = 0.1$, and noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n- **Ground Truth**: The true non-zero coefficients correspond to terms $u\\,u_x$ (index $4$) and $u_{xx}$ (index $6$).\n- **Grouping Structure**: $p=9$ features partitioned into $3$ groups by derivative order:\n    - Group $0$: indices $[0, 1, 2]$ for $[1, u, u^2]$.\n    - Group $1$: indices $[3, 4, 5]$ for $[u_x, u\\,u_x, u^2 u_x]$.\n    - Group $2$: indices $[6, 7, 8]$ for $[u_{xx}, u\\,u_{xx}, (u_x)^2]$.\n- **Method**: Proximal gradient descent with a principled constant stepsize.\n- **Preprocessing**: Standardize columns of $X$ to zero mean and unit variance. Center $y$ by subtracting its mean.\n- **Regularization Scaling**: $\\lambda = \\kappa \\cdot \\Lambda(\\alpha)$, where $\\Lambda(\\alpha) = \\max\\left( \\frac{1}{n}\\lVert X^\\top y \\rVert_{\\infty} \\cdot \\frac{1}{\\alpha}, \\; \\max_{g \\in \\mathcal{G}} \\frac{1}{n}\\lVert X_g^\\top y \\rVert_2 \\cdot \\frac{1}{1 - \\alpha} \\right)$, computed on standardized/centered data.\n- **Evaluation**:\n    - Predicted support: Indices $j$ where $|w_j| \\ge \\tau$.\n    - Metrics: Precision and Recall, defined as $\\text{precision} = \\frac{\\text{TP}}{\\text{predicted positives}}$ and $\\text{recall} = \\frac{\\text{TP}}{\\text{true nonzeros}}$.\n- **Test Suite**: $4$ cases with parameters $(\\alpha, \\kappa, \\sigma, \\tau)$:\n    1. $(0.5, 0.10, 0.0, 10^{-3})$\n    2. $(0.5, 0.20, 0.01, 10^{-3})$\n    3. $(0.8, 0.30, 0.02, 10^{-3})$\n    4. $(0.2, 0.60, 0.05, 10^{-3})$\n- **Output Format**: Single line `[precision_1, recall_1, ..., precision_4, recall_4]` with values rounded to $3$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded, well-posed, and objective. It is based on established principles of convex optimization and sparse regression, standard in machine learning and computational science. The problem is self-contained, providing all necessary formulas, parameters, and procedures for constructing the dataset and implementing the algorithm. The setup is consistent, and the task is computationally feasible. No flaws according to the validation checklist were found.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Design of the Solution\n\nThe problem requires solving a convex composite optimization problem using a proximal gradient method. The objective function is of the form $\\min_w F(w) = f(w) + g(w)$, where $f(w)$ is a smooth, convex function and $g(w)$ is a convex, possibly non-smooth, regularizer.\n\n1.  **Decomposition of the Objective Function**:\n    The objective function is decomposed as:\n    - Smooth part (least-squares loss): $f(w) = \\frac{1}{2n}\\lVert X w - y \\rVert_2^2$.\n    - Non-smooth part (sparse group penalty): $g(w) = \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 + \\alpha \\lVert w \\rVert_1 \\right)$.\n    For the optimization procedure, we use the standardized feature matrix $\\tilde{X}$ and centered target vector $\\tilde{y}$. Thus, we minimize $f(w) = \\frac{1}{2n}\\lVert \\tilde{X} w - \\tilde{y} \\rVert_2^2$ plus the same regularizer $g(w)$.\n\n2.  **Proximal Gradient Method**:\n    The proximal gradient algorithm generates a sequence of iterates via the update rule:\n    $$\n    w^{(k+1)} = \\text{prox}_{\\gamma g}\\left(w^{(k)} - \\gamma \\nabla f(w^{(k)})\\right)\n    $$\n    where $\\gamma > 0$ is the stepsize and $\\text{prox}_{\\gamma g}$ is the proximal operator of the function $\\gamma g$.\n\n3.  **Gradient of the Smooth Part**:\n    The gradient of the least-squares term $f(w)$ is:\n    $$\n    \\nabla f(w) = \\frac{1}{n} \\tilde{X}^\\top (\\tilde{X} w - \\tilde{y})\n    $$\n\n4.  **Stepsize Selection**:\n    The convergence of the proximal gradient method is guaranteed if the stepsize $\\gamma$ satisfies $0 < \\gamma \\le 1/L$, where $L$ is the Lipschitz constant of $\\nabla f(w)$. The gradient is Lipschitz continuous with constant $L = \\frac{1}{n} \\lVert \\tilde{X}^\\top \\tilde{X} \\rVert_2 = \\frac{1}{n} \\sigma_{\\max}(\\tilde{X}^\\top \\tilde{X})$, where $\\sigma_{\\max}$ denotes the maximum eigenvalue (since $\\tilde{X}^\\top \\tilde{X}$ is positive semi-definite). A principled choice is to set the stepsize to its upper bound, $\\gamma = 1/L$, which we compute from the data.\n\n5.  **Proximal Operator for Sparse Group Penalty**:\n    The proximal operator is defined as $\\text{prox}_{\\eta h}(v) = \\arg\\min_z \\left\\{ \\frac{1}{2}\\lVert z - v \\rVert_2^2 + \\eta h(z) \\right\\}$. The penalty $g(w)$ is separable across the predefined groups $\\mathcal{G}$. This means we can compute the proximal operator for each group subvector $w_g$ independently. For a single group $g$, we must solve:\n    $$\n    \\text{prox}_{\\gamma g_g}(v_g) = \\arg\\min_{w_g} \\left\\{ \\frac{1}{2}\\lVert w_g - v_g \\rVert_2^2 + \\gamma \\lambda(1-\\alpha)\\lVert w_g \\rVert_2 + \\gamma \\lambda\\alpha\\lVert w_g \\rVert_1 \\right\\}\n    $$\n    The solution to this subproblem, known as the sparse group lasso proximal operator, is a two-stage procedure:\n    a. Apply the element-wise soft-thresholding operator $S_{\\eta}(\\cdot)$ with threshold $\\eta_1 = \\gamma \\lambda \\alpha$ to the input vector $v_g$:\n    $$\n    u_g = S_{\\eta_1}(v_g) \\quad \\text{where } (S_{\\eta_1}(v_g))_j = \\text{sign}((v_g)_j) \\max(|\\,(v_g)_j| - \\eta_1, 0)\n    $$\n    b. Apply group-wise soft-thresholding (or block soft-thresholding) to the result $u_g$ with threshold $\\eta_2 = \\gamma \\lambda (1-\\alpha)$:\n    $$\n    w_g^* = u_g \\cdot \\max\\left(0, 1 - \\frac{\\eta_2}{\\lVert u_g \\rVert_2}\\right)\n    $$\n    This operation is performed for all groups $g \\in \\mathcal{G}$ to obtain the full updated vector $w^{(k+1)}$.\n\n6.  **Algorithm Summary**:\n    The implemented algorithm will proceed as follows for each test case:\n    a. Generate the data matrix $X$ and target vector $y$ according to the specified analytic functions and noise level $\\sigma$.\n    b. Standardize $X$ to get $\\tilde{X}$ and center $y$ to get $\\tilde{y}$.\n    c. Calculate the data-dependent regularization scale $\\Lambda(\\alpha)$ and the final regularization parameter $\\lambda = \\kappa \\Lambda(\\alpha)$.\n    d. Compute the Lipschitz constant $L$ and the stepsize $\\gamma = 1/L$.\n    e. Initialize the weight vector $w = \\mathbf{0}$.\n    f. Iterate the proximal gradient update rule until the change in $w$ is below a tolerance or a maximum number of iterations is reached.\n    g. On the final coefficient vector $w$, identify the non-zero support by thresholding with $\\tau$.\n    h. Compute the precision and recall of the recovered support against the known true support (indices $\\{4, 6\\}$).\n    i. Store and format the results as required.\n\nThis principled approach ensures a correct and robust implementation for solving the given scientific machine learning problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for sparse group regression.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, kappa, sigma, tau)\n        (0.5, 0.10, 0.0, 1e-3),\n        (0.5, 0.20, 0.01, 1e-3),\n        (0.8, 0.30, 0.02, 1e-3),\n        (0.2, 0.60, 0.05, 1e-3),\n    ]\n\n    results = []\n    \n    # Fixed parameters for data generation\n    Nx = 64\n    Nt = 16\n    n = Nx * Nt\n    p = 9\n    \n    # Grid generation\n    x_grid = np.linspace(0, 1, Nx)\n    t_grid = np.linspace(0, 1, Nt)\n    xx, tt = np.meshgrid(x_grid, t_grid)\n    x = xx.ravel()\n    t = tt.ravel()\n\n    # Analytic field and derivatives\n    u = np.sin(2 * np.pi * x) * np.exp(-t) + 0.5 * np.cos(4 * np.pi * x) * np.exp(-0.2 * t)\n    u_x = 2 * np.pi * np.cos(2 * np.pi * x) * np.exp(-t) - 2 * np.pi * np.sin(4 * np.pi * x) * np.exp(-0.2 * t)\n    u_xx = -(2 * np.pi)**2 * np.sin(2 * np.pi * x) * np.exp(-t) - 8 * np.pi**2 * np.cos(4 * np.pi * x) * np.exp(-0.2 * t)\n\n    # Polynomial library\n    X = np.zeros((n, p))\n    X[:, 0] = 1.0\n    X[:, 1] = u\n    X[:, 2] = u**2\n    X[:, 3] = u_x\n    X[:, 4] = u * u_x\n    X[:, 5] = u**2 * u_x\n    X[:, 6] = u_xx\n    X[:, 7] = u * u_xx\n    X[:, 8] = u_x**2\n\n    # Group definitions\n    groups = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    true_support = {4, 6}\n\n    for i, case in enumerate(test_cases):\n        alpha, kappa, sigma, tau = case\n        \n        # Set seed for reproducible noise generation for each case\n        np.random.seed(i)\n        \n        # Generate target vector y\n        y_true = 2.0 * X[:, 4] + 0.1 * X[:, 6]\n        noise = np.random.normal(0, sigma, size=n)\n        y = y_true + noise\n\n        # Preprocessing: Standardize X and center y\n        y_mean = y.mean()\n        y_c = y - y_mean\n        \n        X_mean = X.mean(axis=0)\n        X_std = X.std(axis=0)\n        # Avoid division by zero for columns with zero variance (e.g., constant column)\n        X_std[X_std == 0] = 1.0\n        X_s = (X - X_mean) / X_std\n\n        # Calculate data-dependent regularization scale Lambda(alpha)\n        X_s_T_y_c = X_s.T @ y_c\n        \n        term1 = np.linalg.norm(X_s_T_y_c, ord=np.inf) / (n * alpha)\n        \n        term2_vals = []\n        for g_indices in groups:\n            norm_Xg_T_y_c = np.linalg.norm(X_s_T_y_c[g_indices], ord=2)\n            term2_vals.append(norm_Xg_T_y_c / (n * (1 - alpha)))\n        term2 = np.max(term2_vals)\n        \n        lambda_scale = max(term1, term2)\n        lambda_val = kappa * lambda_scale\n\n        # Proximal Gradient Descent\n        # Compute Lipschitz constant and stepsize\n        L = np.max(np.linalg.eigvalsh(X_s.T @ X_s / n))\n        if L == 0:\n            gamma = 1.0\n        else:\n            gamma = 1.0 / L\n\n        # Algorithm parameters\n        w = np.zeros(p)\n        max_iter = 20000\n        tol = 1e-7\n        lambda1 = lambda_val * (1 - alpha)\n        lambda2 = lambda_val * alpha\n\n        for _ in range(max_iter):\n            w_old = w.copy()\n            grad = X_s.T @ (X_s @ w - y_c) / n\n            v = w - gamma * grad\n            \n            w_new = np.zeros(p)\n            for g_indices in groups:\n                v_g = v[g_indices]\n                # Soft-thresholding for L1 penalty\n                u_g = np.sign(v_g) * np.maximum(np.abs(v_g) - gamma * lambda2, 0)\n                norm_u_g = np.linalg.norm(u_g)\n                # Group soft-thresholding for group L2 penalty\n                if norm_u_g > 0:\n                    scale_factor = np.maximum(0, 1 - (gamma * lambda1) / norm_u_g)\n                    w_new[g_indices] = scale_factor * u_g\n            \n            w = w_new\n            if np.linalg.norm(w - w_old) < tol:\n                break\n        \n        # Evaluation\n        predicted_support = {j for j, val in enumerate(w) if np.abs(val) >= tau}\n        \n        tp = len(true_support.intersection(predicted_support))\n        num_predicted_positives = len(predicted_support)\n        num_true_nonzeros = len(true_support)\n        \n        precision = tp / num_predicted_positives if num_predicted_positives > 0 else 0.0\n        recall = tp / num_true_nonzeros\n        \n        results.append(precision)\n        results.append(recall)\n\n    # Format the final output string\n    output_str = \",\".join([f\"{r:.3f}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Scientific models are often incredibly detailed, making them too computationally expensive for extensive analysis. This practice addresses the essential task of creating and validating simpler, \"coarse-grained\" models that are faster to simulate but must remain scientifically valid. You will take data from a high-resolution simulation of a fundamental stochastic process, learn a simplified autoregressive model from it, and then perform a critical test: does the simple model faithfully reproduce the essential long-term statistical memory of the original, more complex system? This exercise highlights the crucial interplay between model reduction and statistical validation .",
            "id": "3157253",
            "problem": "Consider a one-dimensional stationary stochastic process defined by the fundamental law of linear relaxation with thermal fluctuations: the Ornstein–Uhlenbeck process (OU), whose dynamics in continuous time are modeled by the stochastic differential equation (SDE) $$dx(t) = -\\kappa \\, x(t) \\, dt + \\sqrt{2 D} \\, dW(t),$$ where $x(t)$ is the state, $\\kappa > 0$ is the relaxation rate, $D > 0$ is the diffusion level, and $W(t)$ is a standard Wiener process with independent increments. A high-resolution numerical simulation of this process can be performed using the forward Euler–Maruyama method on a computational grid with fine time step $dt$, which is a well-tested method for simulating SDEs.\n\nIn machine learning for scientific discovery, it is common to \"learn\" a coarse-grained model from high-resolution data. A widely used coarse-grained representation for a scalar, approximately Markovian process sampled at a fixed coarse interval $\\Delta$ is the first-order autoregressive model, written in discrete time as $$y_{n+1} = a \\, y_n + \\varepsilon_n,$$ where $a$ is an unknown parameter to be learned from data and $\\varepsilon_n$ is a zero-mean Gaussian innovation with variance $\\sigma_\\varepsilon^2$ to be estimated from training residuals. The long-time statistics of interest include the normalized autocorrelation function $$\\rho(k) = \\frac{C(k)}{C(0)}, \\quad C(k) = \\mathbb{E}\\left[(z_n - \\mu)(z_{n+k} - \\mu)\\right],$$ where $z_n$ is a discrete-time stationary series, $\\mu$ is its mean, and $k$ is a nonnegative integer lag. For empirical estimation from a single realization, use the standard time-average estimator $$\\widehat{C}(k) = \\frac{1}{M - k} \\sum_{n=0}^{M-k-1} \\left(z_n - \\overline{z}\\right)\\left(z_{n+k} - \\overline{z}\\right), \\quad \\overline{z} = \\frac{1}{M} \\sum_{n=0}^{M-1} z_n,$$ and the normalized autocorrelation $$\\widehat{\\rho}(k) = \\frac{\\widehat{C}(k)}{\\widehat{C}(0)}.$$ The objective is to test whether a learned coarse-grained model preserves long-time statistics by comparing $\\widehat{\\rho}(k)$ computed from coarse samples of the full-resolution simulation to $\\widehat{\\rho}(k)$ from the learned model simulation over a range of lags.\n\nProgram requirements:\n- Implement a high-resolution simulation of the Ornstein–Uhlenbeck process via the Euler–Maruyama method over $N$ steps of size $dt$, with initial condition $x(0)$ drawn from the stationary distribution.\n- Downsample the high-resolution trajectory by a positive integer factor $m$ to obtain a coarse series of length $M = \\lfloor (N+1)/m \\rfloor$ at coarse interval $\\Delta = m \\, dt$.\n- Learn the coarse parameter $a$ from an initial training fraction $f_{\\text{train}}$ of the downsampled series using least squares under the zero-mean assumption, that is, fit $$a = \\arg\\min_{a} \\sum_{n=0}^{N_{\\text{tr}}-2} \\left(z_{n+1} - a \\, z_n\\right)^2,$$ where $N_{\\text{tr}} = \\lfloor f_{\\text{train}} \\, M \\rfloor$ and $z_n$ are the training samples. To model measurement imperfections, optionally perturb the training samples by adding independent Gaussian noise with standard deviation $\\sigma_{\\text{train}}$ before fitting.\n- Estimate the innovation variance $\\sigma_\\varepsilon^2$ from the residuals on the same training subset using $$\\widehat{\\sigma}_\\varepsilon^2 = \\frac{1}{N_{\\text{tr}}-1} \\sum_{n=0}^{N_{\\text{tr}}-2} \\left(z_{n+1} - \\widehat{a} \\, z_n\\right)^2.$$\n- Simulate the learned coarse model for $M$ steps using the estimated $\\widehat{a}$ and $\\widehat{\\sigma}_\\varepsilon^2$, with initial value equal to the first coarse sample of the full-resolution series.\n- For both the downsampled full-resolution series and the learned model series, compute the normalized autocorrelation $\\widehat{\\rho}(k)$ for integer lags $k = 0, 1, \\dots, K_{\\max}$ using the estimator above.\n- Compute the maximum absolute deviation $$\\Delta_{\\max} = \\max_{0 \\le k \\le K_{\\max}} \\left| \\widehat{\\rho}_{\\text{full}}(k) - \\widehat{\\rho}_{\\text{learned}}(k) \\right|.$$ A coarse-grained model is said to preserve long-time statistics in this test if $\\Delta_{\\max} \\le \\text{tol}$ for a given tolerance $\\text{tol}$.\n\nYour program must implement the above and evaluate the following test suite, which explores typical, slow, and noisy-training regimes:\n- Test case $1$: $\\kappa = 0.5$, $D = 1.0$, $dt = 0.001$, $N = 100000$, $m = 50$, $f_{\\text{train}} = 0.5$, $\\sigma_{\\text{train}} = 0.0$, $K_{\\max} = 60$, $\\text{tol} = 0.05$.\n- Test case $2$: $\\kappa = 0.05$, $D = 1.0$, $dt = 0.001$, $N = 200000$, $m = 100$, $f_{\\text{train}} = 0.3$, $\\sigma_{\\text{train}} = 0.0$, $K_{\\max} = 80$, $\\text{tol} = 0.07$.\n- Test case $3$: $\\kappa = 0.9$, $D = 1.0$, $dt = 0.001$, $N = 100000$, $m = 80$, $f_{\\text{train}} = 0.2$, $\\sigma_{\\text{train}} = 0.5$, $K_{\\max} = 50$, $\\text{tol} = 0.05$.\n\nFor reproducibility, use a fixed random seed and ensure all random draws are independent across steps but deterministic given the seed.\n\nFinal output specification:\n- For each test case, output a boolean indicating whether the learned coarse model preserves long-time statistics according to the criterion $\\Delta_{\\max} \\le \\text{tol}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[{\\text{result}_1},{\\text{result}_2},{\\text{result}_3}]$, where each entry is either $\\text{True}$ or $\\text{False}$.",
            "solution": "The problem poses a well-defined and scientifically sound task in computational science, specifically at the intersection of stochastic processes and machine learning for model discovery. The components of the problem—the Ornstein-Uhlenbeck (OU) process, the Euler-Maruyama numerical integration method, the autoregressive (AR) model, least-squares parameter estimation, and autocorrelation analysis—are all standard and rigorously defined concepts in their respective fields. The parameters provided are physically and computationally reasonable, and the specified procedure is unambiguous and algorithmically formalizable. Therefore, the problem is deemed valid and a solution can be constructed by carefully following the specified steps.\n\nThe core of the problem is to generate data from a \"true\" high-resolution physical model, coarse-grain it, learn a simplified model from this coarse data, and then validate whether the learned model accurately reproduces the long-time statistical behavior of the true system.\n\nFirst, we simulate the high-resolution dynamics of the Ornstein-Uhlenbeck process, governed by the stochastic differential equation (SDE):\n$$dx(t) = -\\kappa \\, x(t) \\, dt + \\sqrt{2 D} \\, dW(t)$$\nwhere $\\kappa$ is the relaxation rate, $D$ is the diffusion constant, and $dW(t)$ represents the increments of a Wiener process. We employ the Euler-Maruyama method for numerical integration. This method discretizes the SDE over a small time step $dt$. The update rule for the state $x$ at step $i$ is:\n$$x_{i+1} = x_i - \\kappa x_i dt + \\sqrt{2 D dt} \\, \\xi_i$$\nwhere $\\xi_i$ is a random variable drawn from a standard normal distribution, $\\mathcal{N}(0, 1)$. The simulation is initialized with $x_0$ drawn from the process's stationary distribution, which is a zero-mean Gaussian with variance $\\sigma_x^2 = D/\\kappa$, i.e., $x_0 \\sim \\mathcal{N}(0, D/\\kappa)$. The simulation is run for $N$ steps, generating a trajectory $\\{x_i\\}_{i=0}^{N}$.\n\nSecond, we perform coarse-graining. The high-resolution trajectory $\\{x_i\\}$ is downsampled by an integer factor $m$ to produce a coarse-grained time series $\\{z_n\\}$. The new time interval is $\\Delta = m \\, dt$. The resulting series has a length of $M = \\lfloor (N+1)/m \\rfloor$, as specified. This is achieved by taking every $m$-th point of the original trajectory, truncated to length $M$.\n\nThird, we learn a coarse-grained model from this data. The proposed model is a first-order autoregressive, or AR($1$), process:\n$$y_{n+1} = a \\, y_n + \\varepsilon_n$$\nwhere $a$ is the autoregressive coefficient and $\\varepsilon_n$ is a zero-mean Gaussian white noise term with variance $\\sigma_\\varepsilon^2$. Both $a$ and $\\sigma_\\varepsilon^2$ are to be learned from an initial fraction $f_{\\text{train}}$ of the coarse series $\\{z_n\\}$. The training data consists of the first $N_{\\text{tr}} = \\lfloor f_{\\text{train}} M \\rfloor$ points of $\\{z_n\\}$. Before fitting, this training data may be perturbed by adding independent Gaussian noise with standard deviation $\\sigma_{\\text{train}}$. The parameter $a$ is estimated using ordinary least squares under a zero-mean (zero-intercept) assumption. The objective is to minimize the sum of squared errors:\n$$\\min_{a} \\sum_{n=0}^{N_{\\text{tr}}-2} (z_{n+1} - a z_n)^2$$\nThe solution to this minimization problem provides the estimate $\\widehat{a}$:\n$$\\widehat{a} = \\frac{\\sum_{n=0}^{N_{\\text{tr}}-2} z_n z_{n+1}}{\\sum_{n=0}^{N_{\\text{tr}}-2} z_n^2}$$\nThe variance of the innovation term, $\\widehat{\\sigma}_\\varepsilon^2$, is then estimated from the residuals on the same training data using the provided formula:\n$$\\widehat{\\sigma}_\\varepsilon^2 = \\frac{1}{N_{\\text{tr}}-1} \\sum_{n=0}^{N_{\\text{tr}}-2} (z_{n+1} - \\widehat{a} z_n)^2$$\n\nFourth, a new time series $\\{y_n\\}_{n=0}^{M-1}$ is generated by simulating the learned AR($1$) model for $M$ steps, using the estimated parameters $\\widehat{a}$ and $\\widehat{\\sigma}_\\varepsilon = \\sqrt{\\widehat{\\sigma}_\\varepsilon^2}$. The simulation is initialized with the first value of the coarse-grained series, $y_0 = z_0$.\n\nFinally, we compare the statistical properties of the \"true\" coarse-grained series $\\{z_n\\}$ and the \"learned\" series $\\{y_n\\}$. The comparison is based on the normalized autocorrelation function (ACF), $\\widehat{\\rho}(k)$. For a given time series $\\{s_n\\}$ of length $M$, the empirical autocovariance at lag $k$ is computed as:\n$$\\widehat{C}(k) = \\frac{1}{M - k} \\sum_{n=0}^{M-k-1} (s_n - \\overline{s})(s_{n+k} - \\overline{s}), \\quad \\text{where} \\quad \\overline{s} = \\frac{1}{M}\\sum_{n=0}^{M-1}s_n$$\nThe normalized ACF is then $\\widehat{\\rho}(k) = \\widehat{C}(k) / \\widehat{C}(0)$. We compute $\\widehat{\\rho}_{\\text{full}}(k)$ from $\\{z_n\\}$ and $\\widehat{\\rho}_{\\text{learned}}(k)$ from $\\{y_n\\}$ for lags $k = 0, 1, \\dots, K_{\\max}$. The performance of the learned model is quantified by the maximum absolute deviation between these two functions:\n$$\\Delta_{\\max} = \\max_{0 \\le k \\le K_{\\max}} \\left| \\widehat{\\rho}_{\\text{full}}(k) - \\widehat{\\rho}_{\\text{learned}}(k) \\right|$$\nThe learned model is considered to have successfully preserved the long-time statistics if this deviation is within a given tolerance, i.e., $\\Delta_{\\max} \\le \\text{tol}$. This entire procedure is then applied to each test case defined in the problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # A fixed random seed is used for reproducibility.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'kappa': 0.5, 'D': 1.0, 'dt': 0.001, 'N': 100000, 'm': 50, 'f_train': 0.5, 'sigma_train': 0.0, 'K_max': 60, 'tol': 0.05},\n        {'kappa': 0.05, 'D': 1.0, 'dt': 0.001, 'N': 200000, 'm': 100, 'f_train': 0.3, 'sigma_train': 0.0, 'K_max': 80, 'tol': 0.07},\n        {'kappa': 0.9, 'D': 1.0, 'dt': 0.001, 'N': 100000, 'm': 80, 'f_train': 0.2, 'sigma_train': 0.5, 'K_max': 50, 'tol': 0.05},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_test_case(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_test_case(kappa, D, dt, N, m, f_train, sigma_train, K_max, tol):\n    \"\"\"\n    Executes the full simulation and analysis for a single test case.\n    \"\"\"\n    # Step 1: High-resolution simulation of the Ornstein-Uhlenbeck process.\n    # The stationary distribution is N(0, D/kappa).\n    sigma_x = np.sqrt(D / kappa)\n    x = np.zeros(N + 1)\n    x[0] = np.random.normal(0, sigma_x)\n\n    # Wiener process increments, pre-generated for efficiency\n    dW = np.random.normal(0, np.sqrt(dt), N)\n\n    for i in range(N):\n        x[i+1] = x[i] - kappa * x[i] * dt + np.sqrt(2 * D) * dW[i]\n\n    # Step 2: Downsample the high-resolution trajectory.\n    M = int((N + 1) / m)\n    z_full = x[0::m][:M]\n\n    # Step 3: Learn the coarse parameters 'a' and 'sigma_epsilon^2'.\n    N_tr = int(f_train * M)\n    z_train_orig = z_full[:N_tr]\n    \n    # Optionally perturb training samples with measurement noise.\n    if sigma_train > 0.0:\n        noise = np.random.normal(0, sigma_train, size=N_tr)\n        z_train_fit = z_train_orig + noise\n    else:\n        z_train_fit = z_train_orig\n\n    y_vec = z_train_fit[1:N_tr]\n    x_vec = z_train_fit[0:N_tr-1]\n\n    # Estimate 'a' using the least squares formula.\n    a_hat = np.dot(x_vec, y_vec) / np.dot(x_vec, x_vec)\n\n    # Estimate innovation variance from residuals on the (potentially noisy) training set.\n    residuals = y_vec - a_hat * x_vec\n    sigma_eps_sq_hat = np.sum(residuals**2) / (N_tr - 1)\n    sigma_eps_hat = np.sqrt(sigma_eps_sq_hat)\n    \n    # Step 4: Simulate the learned coarse model.\n    y_learned = np.zeros(M)\n    y_learned[0] = z_full[0]  # Initialize with the first coarse sample.\n    \n    # Generate innovation terms for the learned model simulation.\n    innovations = np.random.normal(0, sigma_eps_hat, M - 1)\n    for n in range(M - 1):\n        y_learned[n+1] = a_hat * y_learned[n] + innovations[n]\n\n    # Step 5: Compute normalized autocorrelation for both series.\n    acf_full = compute_acf(z_full, K_max)\n    acf_learned = compute_acf(y_learned, K_max)\n\n    # Step 6: Compute the maximum absolute deviation and check against tolerance.\n    delta_max = np.max(np.abs(acf_full - acf_learned))\n    \n    return delta_max <= tol\n\ndef compute_acf(series, k_max):\n    \"\"\"\n    Computes the normalized autocorrelation function for a given series.\n    \"\"\"\n    M = len(series)\n    mean_series = np.mean(series)\n    centered_series = series - mean_series\n    \n    # Calculate C(0) for normalization.\n    # The estimator specifies division by (M-k), so for k=0, it's M.\n    c0 = np.dot(centered_series, centered_series) / M\n    if c0 == 0:\n        # Handle the case of a constant series to avoid division by zero.\n        return np.ones(k_max + 1)\n\n    # Calculate C(k) for k > 0.\n    acf = np.zeros(k_max + 1)\n    acf[0] = 1.0 # rho(0) is always 1\n    \n    for k in range(1, k_max + 1):\n        # Slices for the dot product sum from n=0 to M-k-1.\n        term1 = centered_series[:M-k]\n        term2 = centered_series[k:]\n        ck = np.dot(term1, term2) / (M - k)\n        acf[k] = ck / c0\n        \n    return acf\n\nsolve()\n\n```"
        }
    ]
}