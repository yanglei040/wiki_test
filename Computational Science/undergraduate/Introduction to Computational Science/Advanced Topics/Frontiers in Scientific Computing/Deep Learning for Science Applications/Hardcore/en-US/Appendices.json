{
    "hands_on_practices": [
        {
            "introduction": "A key challenge in applying deep learning to scientific problems is ensuring that models respect fundamental physical laws. This practice demonstrates a powerful and common technique: modifying the model's loss function to penalize violations of a known constraint. You will learn how to enforce the monotonicity of a cumulative distribution function, guiding a flexible model towards physically plausible solutions without needing to change its underlying architecture .",
            "id": "3116982",
            "problem": "You are modeling a Cumulative Distribution Function (CDF) as a function $\\hat{F}_{\\theta}(x)$ using a simple differentiable parametric form that mimics an unconstrained neural network. In scientific applications, the learned CDF must satisfy the fundamental property of monotonicity: for a real-valued random variable $X$, the Cumulative Distribution Function (CDF) $F(x) = \\mathbb{P}(X \\le x)$ is non-decreasing, and when differentiable its derivative $F'(x)$ equals the Probability Density Function (PDF) $f(x)$, which obeys $f(x) \\ge 0$. This implies the monotonicity constraint $F'(x) \\ge 0$ almost everywhere. In many deep learning architectures used for science applications, unconstrained models may violate this monotonicity, so we seek a principled loss that penalizes violations.\n\nStarting from the core definitions above, design a loss that comprises:\n- A data fidelity term between the learned CDF $\\hat{F}_{\\theta}(x)$ and an empirical CDF $\\tilde{F}(x)$ computed from sample data.\n- A monotonicity penalty that enforces the constraint $F'(x) \\ge 0$ by penalizing negative discrete slopes of $\\hat{F}_{\\theta}(x)$ on a grid.\n\nUse the following foundations and definitions:\n- The empirical CDF $\\tilde{F}(x)$ for a sample $\\{s_j\\}_{j=1}^m$ is defined by $\\tilde{F}(x) = \\frac{1}{m}\\sum_{j=1}^{m} \\mathbf{1}\\{s_j \\le x\\}$.\n- Let a grid $\\{x_i\\}_{i=1}^{n}$ with $x_1 < x_2 < \\cdots < x_n$ be given. Define the discrete slope $D_i = \\frac{\\hat{F}_{\\theta}(x_{i+1}) - \\hat{F}_{\\theta}(x_i)}{x_{i+1} - x_i}$ for $i \\in \\{1,\\dots,n-1\\}$.\n- Define the Rectified Linear Unit (ReLU) by $\\operatorname{ReLU}(t) = \\max(0,t)$.\n\nYour task:\n- Implement a program that, for each provided test case, computes the total loss\n$$\nL(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{F}_{\\theta}(x_i) - \\tilde{F}(x_i)\\right)^2 \\;+\\; \\lambda \\sum_{i=1}^{n-1}\\left(\\operatorname{ReLU}\\!\\left(-D_i\\right)\\right)^2,\n$$\nwhere $\\lambda$ is a nonnegative penalty weight.\n- The parametric CDF model is specified as\n$$\n\\hat{F}_{\\theta}(x) = \\sigma\\!\\left(\\alpha \\sin(2\\pi x) + \\beta x + \\gamma\\right),\n$$\nwhere $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic sigmoid, and $\\theta = (\\alpha,\\beta,\\gamma)$ are real parameters. This form is bounded in $[0,1]$ but not guaranteed to be monotone, making it suitable for testing the penalty.\n\nAlgorithmic specifications:\n- Compute $\\tilde{F}(x_i)$ on the grid directly from the given sample set by counting the proportion of sample points less than or equal to $x_i$.\n- Compute the mean-squared error term and the monotonicity penalty as specified above.\n- No physical units are involved.\n- Angles in trigonometric functions must be in radians.\n- Each test case specifies $(\\theta, \\lambda)$, a grid $\\{x_i\\}_{i=1}^{n}$, and a sample set for computing $\\tilde{F}(x)$.\n\nTest suite:\n- Use grid points $\\{x_i\\}_{i=1}^{n}$ uniformly spaced on $[0,1]$ unless otherwise stated.\n- Case one (general monotone trend, happy path):\n  - Grid: $n = 21$ points $x_i = \\frac{i-1}{20}$ for $i \\in \\{1,\\dots,21\\}$.\n  - Sample set (uniform-like): $\\{0.05, 0.12, 0.18, 0.22, 0.31, 0.37, 0.41, 0.48, 0.53, 0.60, 0.66, 0.74, 0.80, 0.86, 0.92\\}$.\n  - Parameters: $\\alpha = 0.0$, $\\beta = 8.0$, $\\gamma = 0.0$.\n  - Penalty weight: $\\lambda = 10.0$.\n- Case two (non-monotone oscillations):\n  - Grid: $n = 21$ points $x_i = \\frac{i-1}{20}$.\n  - Sample set: same as case one.\n  - Parameters: $\\alpha = 1.0$, $\\beta = 0.5$, $\\gamma = 0.0$.\n  - Penalty weight: $\\lambda = 10.0$.\n- Case three (negative local slopes with stronger penalty):\n  - Grid: $n = 21$ points $x_i = \\frac{i-1}{20}$.\n  - Sample set (more concentrated near zero): $\\{0.01, 0.03, 0.07, 0.10, 0.14, 0.18, 0.25, 0.35, 0.50, 0.70, 0.85, 0.95\\}$.\n  - Parameters: $\\alpha = 0.6$, $\\beta = -0.1$, $\\gamma = -0.2$.\n  - Penalty weight: $\\lambda = 50.0$.\n- Case four (constant model, boundary scenario for monotonicity):\n  - Grid: $n = 21$ points $x_i = \\frac{i-1}{20}$.\n  - Sample set: same as case three.\n  - Parameters: $\\alpha = 0.0$, $\\beta = 0.0$, $\\gamma = 0.0$.\n  - Penalty weight: $\\lambda = 10.0$.\n- Case five (minimal grid, duplicates in empirical sample):\n  - Grid: $n = 2$ points $\\{0.0, 1.0\\}$.\n  - Sample set: $\\{0.2, 0.2, 0.2, 0.8, 0.8\\}$.\n  - Parameters: $\\alpha = -0.8$, $\\beta = 0.7$, $\\gamma = 0.5$.\n  - Penalty weight: $\\lambda = 5.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the five cases in order, rounded to six decimal places (for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$).",
            "solution": "The problem is valid. It is scientifically grounded in the principles of probability theory and numerical optimization, well-posed with all necessary information provided, and objective in its formulation. It presents a standard task in scientific machine learning: designing a loss function to enforce a physical or mathematical constraint (monotonicity of a Cumulative Distribution Function) on a parametric model.\n\nThe task is to compute the total loss $L(\\theta)$ for five distinct test cases. The loss function is composed of two parts: a data fidelity term and a monotonicity penalty.\n$$\nL(\\theta) = \\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{F}_{\\theta}(x_i) - \\tilde{F}(x_i)\\right)^2}_{\\text{Mean Squared Error (MSE)}} \\;+\\; \\underbrace{\\lambda \\sum_{i=1}^{n-1}\\left(\\operatorname{ReLU}\\!\\left(-D_i\\right)\\right)^2}_{\\text{Monotonicity Penalty}}\n$$\nWe will systematically compute each component for every test case.\n\nThe parametric model for the CDF is given by $\\hat{F}_{\\theta}(x) = \\sigma\\!\\left(\\alpha \\sin(2\\pi x) + \\beta x + \\gamma\\right)$, where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic sigmoid function and $\\theta = (\\alpha, \\beta, \\gamma)$.\n\nThe process for each test case is as follows:\n1.  Define the grid of points $\\{x_i\\}_{i=1}^{n}$ and the sample set $\\{s_j\\}_{j=1}^{m}$.\n2.  Compute the values of the empirical CDF, $\\tilde{F}(x_i)$, at each grid point $x_i$. By definition, $\\tilde{F}(x_i) = \\frac{1}{m}\\sum_{j=1}^{m} \\mathbf{1}\\{s_j \\le x_i\\}$, which is the fraction of samples less than or equal to $x_i$.\n3.  Compute the values of the parametric model CDF, $\\hat{F}_{\\theta}(x_i)$, at each grid point $x_i$ using the given parameters $\\theta = (\\alpha, \\beta, \\gamma)$.\n4.  Calculate the MSE term: This is the mean of the squared differences between $\\hat{F}_{\\theta}(x_i)$ and $\\tilde{F}(x_i)$ over all grid points.\n$$\n\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{F}_{\\theta}(x_i) - \\tilde{F}(x_i)\\right)^2\n$$\n5.  Calculate the monotonicity penalty term:\n    a. First, compute the discrete slopes $D_i$ for $i \\in \\{1, \\dots, n-1\\}$:\n    $$\n    D_i = \\frac{\\hat{F}_{\\theta}(x_{i+1}) - \\hat{F}_{\\theta}(x_i)}{x_{i+1} - x_i}\n    $$\n    b. Next, penalize any negative slopes. The penalty for each interval is $\\left(\\operatorname{ReLU}(-D_i)\\right)^2$, where $\\operatorname{ReLU}(t) = \\max(0, t)$. This term is non-zero only if $D_i < 0$.\n    c. The total penalty is the weighted sum over all intervals:\n    $$\n    \\text{Penalty} = \\lambda \\sum_{i=1}^{n-1}\\left(\\operatorname{ReLU}(-D_i)\\right)^2\n    $$\n6.  The total loss $L(\\theta)$ is the sum of the MSE and the Penalty.\n\nWe now apply this procedure to each test case.\n\n**Case 1:**\n- Grid: $n=21$ points on $[0,1]$, $x_i = \\frac{i-1}{20}$.\n- Sample set: $m=15$ points $\\{0.05, \\dots, 0.92\\}$.\n- Parameters: $\\theta = (\\alpha=0.0, \\beta=8.0, \\gamma=0.0)$.\n- Penalty weight: $\\lambda = 10.0$.\nThe model is $\\hat{F}_{\\theta}(x) = \\sigma(8x)$. The derivative of the argument $8x$ is $8 > 0$. Since $\\sigma$ is a strictly increasing function, $\\hat{F}_{\\theta}(x)$ is guaranteed to be monotonic. Therefore, all discrete slopes $D_i$ will be positive. The term $\\operatorname{ReLU}(-D_i)$ will be $0$ for all $i$, resulting in a monotonicity penalty of $0$. The total loss will be solely the MSE between $\\sigma(8x_i)$ and the computed $\\tilde{F}(x_i)$.\n\n**Case 2:**\n- Grid, Sample set, $\\lambda$ are the same as in Case 1.\n- Parameters: $\\theta = (\\alpha=1.0, \\beta=0.5, \\gamma=0.0)$.\nThe model is $\\hat{F}_{\\theta}(x) = \\sigma(\\sin(2\\pi x) + 0.5x)$. The sinusoidal term $\\sin(2\\pi x)$ introduces oscillations. The derivative of the argument is $2\\pi \\cos(2\\pi x) + 0.5$, which can be negative (e.g., near $x=0.5$). This will lead to intervals with negative slopes $D_i$, resulting in a non-zero monotonicity penalty. The total loss will be the sum of the MSE and this penalty term.\n\n**Case 3:**\n- Grid: $n=21$ points on $[0,1]$.\n- Sample set: $m=12$ points $\\{0.01, \\dots, 0.95\\}$.\n- Parameters: $\\theta = (\\alpha=0.6, \\beta=-0.1, \\gamma=-0.2)$.\n- Penalty weight: $\\lambda = 50.0$.\nThe model is $\\hat{F}_{\\theta}(x) = \\sigma(0.6\\sin(2\\pi x) - 0.1x - 0.2)$. The negative coefficient $\\beta=-0.1$ contributes to a decreasing trend, making negative slopes more likely and more pronounced. Consequently, we expect a significant non-zero monotonicity penalty, which is amplified by the larger weight $\\lambda=50.0$.\n\n**Case 4:**\n- Grid, Sample set are the same as in Case 3.\n- Parameters: $\\theta = (\\alpha=0.0, \\beta=0.0, \\gamma=0.0)$.\n- Penalty weight: $\\lambda = 10.0$.\nThe model is $\\hat{F}_{\\theta}(x) = \\sigma(0) = 0.5$. This is a constant function. The discrete slopes $D_i$ are all identically $0$. The term $\\operatorname{ReLU}(-D_i) = \\operatorname{ReLU}(0) = 0$. The monotonicity penalty will be $0$. The total loss will be the MSE between the constant value $0.5$ and the empirical CDF $\\tilde{F}(x_i)$.\n\n**Case 5:**\n- Grid: $n=2$ points, $x_1=0.0, x_2=1.0$.\n- Sample set: $m=5$ points $\\{0.2, 0.2, 0.2, 0.8, 0.8\\}$.\n- Parameters: $\\theta = (\\alpha=-0.8, \\beta=0.7, \\gamma=0.5)$.\n- Penalty weight: $\\lambda = 5.0$.\nThere is only one interval, from $x_1=0.0$ to $x_2=1.0$.\nThe empirical CDF values are $\\tilde{F}(0.0) = \\frac{0}{5} = 0.0$ and $\\tilde{F}(1.0) = \\frac{5}{5} = 1.0$.\nThe model values are $\\hat{F}_{\\theta}(0.0) = \\sigma(-0.8\\sin(0) + 0.7(0) + 0.5) = \\sigma(0.5)$ and $\\hat{F}_{\\theta}(1.0) = \\sigma(-0.8\\sin(2\\pi) + 0.7(1) + 0.5) = \\sigma(1.2)$.\nThe MSE term is $\\frac{1}{2}\\left((\\sigma(0.5) - 0.0)^2 + (\\sigma(1.2) - 1.0)^2\\right)$.\nThere is only one discrete slope, $D_1 = \\frac{\\hat{F}_{\\theta}(1.0) - \\hat{F}_{\\theta}(0.0)}{1.0-0.0} = \\sigma(1.2) - \\sigma(0.5)$. Since $\\sigma$ is increasing and $1.2 > 0.5$, $D_1$ is positive.\nTherefore, $\\operatorname{ReLU}(-D_1) = 0$, and the monotonicity penalty is $0$. The total loss is just the MSE term.\n\nThe implementation will follow this logic for each case, performing the numerical calculations as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the total loss for a parametric CDF model across five test cases.\n    The loss includes a data fidelity term and a monotonicity penalty.\n    \"\"\"\n\n    test_cases = [\n        # Case one\n        {\n            \"grid_n\": 21,\n            \"grid_lims\": (0.0, 1.0),\n            \"samples\": np.array([0.05, 0.12, 0.18, 0.22, 0.31, 0.37, 0.41, 0.48, 0.53, 0.60, 0.66, 0.74, 0.80, 0.86, 0.92]),\n            \"params\": {\"alpha\": 0.0, \"beta\": 8.0, \"gamma\": 0.0},\n            \"lambda_val\": 10.0\n        },\n        # Case two\n        {\n            \"grid_n\": 21,\n            \"grid_lims\": (0.0, 1.0),\n            \"samples\": np.array([0.05, 0.12, 0.18, 0.22, 0.31, 0.37, 0.41, 0.48, 0.53, 0.60, 0.66, 0.74, 0.80, 0.86, 0.92]),\n            \"params\": {\"alpha\": 1.0, \"beta\": 0.5, \"gamma\": 0.0},\n            \"lambda_val\": 10.0\n        },\n        # Case three\n        {\n            \"grid_n\": 21,\n            \"grid_lims\": (0.0, 1.0),\n            \"samples\": np.array([0.01, 0.03, 0.07, 0.10, 0.14, 0.18, 0.25, 0.35, 0.50, 0.70, 0.85, 0.95]),\n            \"params\": {\"alpha\": 0.6, \"beta\": -0.1, \"gamma\": -0.2},\n            \"lambda_val\": 50.0\n        },\n        # Case four\n        {\n            \"grid_n\": 21,\n            \"grid_lims\": (0.0, 1.0),\n            \"samples\": np.array([0.01, 0.03, 0.07, 0.10, 0.14, 0.18, 0.25, 0.35, 0.50, 0.70, 0.85, 0.95]),\n            \"params\": {\"alpha\": 0.0, \"beta\": 0.0, \"gamma\": 0.0},\n            \"lambda_val\": 10.0\n        },\n        # Case five\n        {\n            \"grid_points\": np.array([0.0, 1.0]),\n            \"samples\": np.array([0.2, 0.2, 0.2, 0.8, 0.8]),\n            \"params\": {\"alpha\": -0.8, \"beta\": 0.7, \"gamma\": 0.5},\n            \"lambda_val\": 5.0\n        }\n    ]\n\n    results = []\n    \n    # Define helper functions based on the problem statement\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def f_hat(x, alpha, beta, gamma):\n        z = alpha * np.sin(2.0 * np.pi * x) + beta * x + gamma\n        return sigmoid(z)\n\n    def relu(t):\n        return np.maximum(0, t)\n\n    for case in test_cases:\n        # Step 1: Define grid and samples\n        if \"grid_points\" in case:\n            x_grid = case[\"grid_points\"]\n        else:\n            x_grid = np.linspace(case[\"grid_lims\"][0], case[\"grid_lims\"][1], case[\"grid_n\"])\n        \n        samples = case[\"samples\"]\n        n = len(x_grid)\n        m = len(samples)\n        \n        params = case[\"params\"]\n        alpha, beta, gamma = params[\"alpha\"], params[\"beta\"], params[\"gamma\"]\n        lambda_val = case[\"lambda_val\"]\n\n        # Step 2: Compute empirical CDF F_tilde\n        # For each x_i in x_grid, count how many samples are <= x_i\n        counts = np.sum(samples <= x_grid[:, np.newaxis], axis=1)\n        f_tilde_values = counts / m\n        \n        # Step 3: Compute parametric CDF F_hat\n        f_hat_values = f_hat(x_grid, alpha, beta, gamma)\n\n        # Step 4: Calculate the MSE term\n        mse_term = np.mean((f_hat_values - f_tilde_values)**2)\n\n        # Step 5: Calculate the monotonicity penalty term\n        # a. Compute discrete slopes D_i\n        if n > 1:\n            delta_f_hat = np.diff(f_hat_values)\n            delta_x = np.diff(x_grid)\n            # Avoid division by zero, though not expected in these cases\n            # A small tolerance epsilon could be added for robustness\n            discrete_slopes = delta_f_hat / delta_x\n            \n            # b. Apply ReLU to negative slopes and square\n            penalty_per_interval = relu(-discrete_slopes)**2\n            \n            # c. Sum and weight by lambda\n            monotonicity_penalty = lambda_val * np.sum(penalty_per_interval)\n        else:\n            monotonicity_penalty = 0.0\n\n        # Step 6: Compute total loss\n        total_loss = mse_term + monotonicity_penalty\n        results.append(total_loss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Instead of only penalizing violations, we can build models that are physically consistent by design. This exercise introduces the concept of geometric deep learning, where symmetries are embedded directly into the network's architecture . You will implement a simple message-passing scheme that guarantees rotational invariance, a critical property for predicting scalar quantities like molecular energy, which must not depend on the system's orientation in space.",
            "id": "3117017",
            "problem": "You are asked to design and implement a minimal, rotation-equivariant message passing scheme that produces a rotation-invariant scalar prediction for molecular configurations. The goal is to encode the scientific requirement that scalar observables like energy are invariant under any rotation from the Special Orthogonal group (SO) in three dimensions, denoted as $\\mathrm{SO}(3)$.\n\nStarting from fundamental definitions, a rotation matrix $\\mathbf{R} \\in \\mathrm{SO}(3)$ satisfies $\\mathbf{R}^\\top \\mathbf{R} = \\mathbf{I}$ and $\\det(\\mathbf{R}) = 1$. A scalar physical property such as energy $E$ must satisfy $E(\\mathbf{R}\\mathbf{r}) = E(\\mathbf{r})$ for all $\\mathbf{R} \\in \\mathrm{SO}(3)$, where $\\mathbf{r} = (\\mathbf{r}_1,\\dots,\\mathbf{r}_N)$ are atom coordinates in $\\mathbb{R}^3$ and integer types $Z_i \\in \\mathbb{Z}_{\\ge 1}$ (use atomic numbers).\n\nYou will implement a single-layer equivariant message passing mechanism and a readout that produces an invariant energy-like scalar. The program must follow this specification:\n\n1) Input representation. A molecule is a set of $N$ atoms with positions $\\mathbf{r}_i \\in \\mathbb{R}^3$ and integer types $Z_i \\in \\mathbb{Z}_{\\ge 1}$ (use atomic numbers). Define pairwise relative vectors $\\mathbf{r}_{ij} = \\mathbf{r}_j - \\mathbf{r}_i$, distances $d_{ij} = \\|\\mathbf{r}_{ij}\\|_2$, and unit directions $\\hat{\\mathbf{r}}_{ij} = \\mathbf{r}_{ij}/d_{ij}$. To avoid division by zero, if $d_{ij} \\le \\delta$ for a very small $\\delta > 0$, set $\\hat{\\mathbf{r}}_{ij} = \\mathbf{0}$ by convention.\n\n2) Message passing (vector-valued, rotation equivariant). Define a scalar, distance- and type-dependent coefficient\n$$\ns_{ij} = \\exp(-\\beta\\, d_{ij})\\left(a_0 + a_1 Z_i + a_2 Z_j + a_3 Z_i Z_j\\right),\n$$\nand compute a node-wise vector message\n$$\n\\mathbf{m}_i = \\sum_{j \\ne i} s_{ij}\\, \\hat{\\mathbf{r}}_{ij}.\n$$\nHere $\\beta, a_0, a_1, a_2, a_3$ are fixed real constants.\n\n3) Invariant readout (scalar energy-like prediction). Define a per-node scalar\n$$\ne_i = c_1 \\|\\mathbf{m}_i\\|_2^2 + c_2 \\sum_{j \\ne i} \\exp(-\\gamma\\, d_{ij}),\n$$\nand the total prediction\n$$\nE(\\mathbf{r}, \\mathbf{Z}) = \\sum_{i=1}^N e_i + c_3 \\sum_{i=1}^N Z_i,\n$$\nwith fixed real constants $c_1, c_2, c_3, \\gamma$.\n\n4) Equivariance/invariance requirement. The message $\\mathbf{m}_i$ must transform as a vector under $\\mathbf{R} \\in \\mathrm{SO}(3)$, and the final $E(\\mathbf{r}, \\mathbf{Z})$ must satisfy $E(\\mathbf{R}\\mathbf{r}, \\mathbf{Z}) = E(\\mathbf{r}, \\mathbf{Z})$.\n\n5) Rotation generation. Construct $\\mathbf{R}$ from a given axis-angle $(\\hat{\\mathbf{a}}, \\theta)$ using Rodriguesâ€™ formula\n$$\n\\mathbf{R} = \\cos\\theta\\, \\mathbf{I} + \\sin\\theta\\, [\\hat{\\mathbf{a}}]_\\times + (1-\\cos\\theta)\\, \\hat{\\mathbf{a}}\\hat{\\mathbf{a}}^\\top,\n$$\nwhere $[\\hat{\\mathbf{a}}]_\\times$ is the skew-symmetric matrix of the unit axis $\\hat{\\mathbf{a}}$ and $\\theta$ is an angle in radians.\n\nConstants to use in your program:\n- $\\delta = 10^{-12}$,\n- $a_0 = 0.5$, $a_1 = 0.1$, $a_2 = -0.05$, $a_3 = 0.02$,\n- $\\beta = 1.3$, $\\gamma = 0.9$,\n- $c_1 = 0.7$, $c_2 = 0.2$, $c_3 = 0.05$.\n\nNumerical tolerance for equality:\n- Two floating-point predictions $x$ and $y$ are considered equal if $|x-y| \\le \\tau$ with $\\tau = 10^{-10}$.\n\nAngle unit:\n- All angles $\\theta$ are specified in radians.\n\nTest suite. Your program must hard-code the following four test cases, each consisting of positions $\\mathbf{r}$ (in any consistent length unit), integer types $\\mathbf{Z}$, and a rotation axis-angle $(\\hat{\\mathbf{a}}, \\theta)$:\n\n- Case $1$ (general, non-degenerate triangle): $\\mathbf{Z} = [8, 1, 1]$, positions\n  $$\n  \\mathbf{r}_1 = (0.0, 0.0, 0.0),\\;\n  \\mathbf{r}_2 = (0.9572, 0.0, 0.0),\\;\n  \\mathbf{r}_3 = (-0.2399872, 0.927297, 0.0),\n  $$\n  rotation axis $\\hat{\\mathbf{a}} \\propto (1.0, 2.0, 3.0)$ normalized to unit length, angle $\\theta = 1.234$.\n\n- Case $2$ (linear triatomic, $180^\\circ$ rotation): $\\mathbf{Z} = [6, 1, 1]$, positions\n  $$\n  \\mathbf{r}_1 = (0.0, 0.0, 0.0),\\;\n  \\mathbf{r}_2 = (1.1, 0.0, 0.0),\\;\n  \\mathbf{r}_3 = (-1.1, 0.0, 0.0),\n  $$\n  rotation axis $\\hat{\\mathbf{a}} = (0.0, 0.0, 1.0)$, angle $\\theta = \\pi$.\n\n- Case $3$ (single atom): $\\mathbf{Z} = [8]$, position\n  $$\n  \\mathbf{r}_1 = (1.0, 2.0, 3.0),\n  $$\n  rotation axis $\\hat{\\mathbf{a}} = (0.0, 0.0, 1.0)$, angle $\\theta = 0.7$.\n\n- Case $4$ (degenerate, coincident positions to stress-test $\\delta$ handling): $\\mathbf{Z} = [1, 1, 1]$, positions\n  $$\n  \\mathbf{r}_1 = (0.0, 0.0, 0.0),\\;\n  \\mathbf{r}_2 = (0.0, 0.0, 0.0),\\;\n  \\mathbf{r}_3 = (0.0, 0.0, 0.0),\n  $$\n  rotation axis $\\hat{\\mathbf{a}} = (0.0, 1.0, 0.0)$, angle $\\theta = 2.2$.\n\nFor each case, compute $E(\\mathbf{r}, \\mathbf{Z})$ and $E(\\mathbf{R}\\mathbf{r}, \\mathbf{Z})$, then compare them using the tolerance $\\tau$. The expected outcome is a boolean per case indicating whether rotational invariance holds within tolerance.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example\n$[b_1,b_2,b_3,b_4]$\nwhere each $b_k$ is a boolean value for case $k$.",
            "solution": "The problem requires the design and verification of a computational model for a scalar molecular property that is, by construction, invariant under three-dimensional rotations. This is a foundational concept in physics-informed machine learning, where incorporating known physical symmetries into a model's architecture improves its accuracy, data efficiency, and generalizability. The symmetry group of interest is the Special Orthogonal group in three dimensions, $\\mathrm{SO}(3)$, which represents all proper rotations.\n\nA scalar property $E$ is said to be rotation-invariant if for any rotation matrix $\\mathbf{R} \\in \\mathrm{SO}(3)$ and any configuration of atomic positions $\\mathbf{r} = (\\mathbf{r}_1, \\dots, \\mathbf{r}_N)$, the property remains unchanged: $E(\\mathbf{R}\\mathbf{r}, \\mathbf{Z}) = E(\\mathbf{r}, \\mathbf{Z})$, where $\\mathbf{Z}$ represents the invariant atomic types. We will demonstrate that the prescribed model architecture guarantees this invariance by analyzing the transformation properties of each component.\n\nThe design relies on the concepts of equivariance and invariance. A vector-valued function $f(\\mathbf{x})$ is equivariant if $f(\\mathbf{R}\\mathbf{x}) = \\mathbf{R}f(\\mathbf{x})$, meaning its output transforms like a vector. A scalar-valued function $g(\\mathbf{x})$ is invariant if $g(\\mathbf{R}\\mathbf{x}) = g(\\mathbf{x})$. The model is constructed by a sequence of operations that maintain these properties.\n\n**1. Input Features and Transformation Properties**\n\nThe model begins with fundamental geometric quantities derived from the atomic positions $\\mathbf{r}_i \\in \\mathbb{R}^3$. Under a rotation $\\mathbf{R}$, a position vector transforms as $\\mathbf{r}'_i = \\mathbf{R}\\mathbf{r}_i$.\n\n- **Relative Position Vectors**: For any pair of atoms $i$ and $j$, the relative position vector is $\\mathbf{r}_{ij} = \\mathbf{r}_j - \\mathbf{r}_i$. These vectors are equivariant because $\\mathbf{r}'_{ij} = \\mathbf{r}'_j - \\mathbf{r}'_i = \\mathbf{R}\\mathbf{r}_j - \\mathbf{R}\\mathbf{r}_i = \\mathbf{R}(\\mathbf{r}_j - \\mathbf{r}_i) = \\mathbf{R}\\mathbf{r}_{ij}$.\n\n- **Pairwise Distances**: The distance $d_{ij} = \\|\\mathbf{r}_{ij}\\|_2$ is the Euclidean norm. Since rotations are isometries (length-preserving), distances are invariant: $d'_{ij} = \\|\\mathbf{r}'_{ij}\\|_2 = \\|\\mathbf{R}\\mathbf{r}_{ij}\\|_2 = \\|\\mathbf{r}_{ij}\\|_2 = d_{ij}$.\n\n- **Unit Direction Vectors**: Defined as $\\hat{\\mathbf{r}}_{ij} = \\mathbf{r}_{ij} / d_{ij}$ (for $d_{ij} > \\delta$), these vectors are equivariant: $\\hat{\\mathbf{r}}'_{ij} = \\mathbf{r}'_{ij} / d'_{ij} = (\\mathbf{R}\\mathbf{r}_{ij}) / d_{ij} = \\mathbf{R}(\\mathbf{r}_{ij} / d_{ij}) = \\mathbf{R}\\hat{\\mathbf{r}}_{ij}$. The special case where $d_{ij} \\le \\delta$ sets $\\hat{\\mathbf{r}}_{ij} = \\mathbf{0}$, which is a fixed point of rotation ($\\mathbf{R}\\mathbf{0} = \\mathbf{0}$).\n\nThe atomic types $Z_i$ are scalars that do not depend on position and are thus inherently invariant.\n\n**2. Equivariant Message Passing**\n\nThe model computes an intermediate vector representation $\\mathbf{m}_i$ for each atom $i$ through a message passing mechanism. This stage is designed to be equivariant.\n\n- **Invariant Message Coefficient ($s_{ij}$)**: The strength of the interaction between atoms $i$ and $j$ is modulated by a scalar coefficient $s_{ij} = \\exp(-\\beta\\, d_{ij})\\left(a_0 + a_1 Z_i + a_2 Z_j + a_3 Z_i Z_j\\right)$. This coefficient is a function of only invariant quantities ($d_{ij}$, $Z_i$, $Z_j$) and fixed constants ($a_k$, $\\beta$), making $s_{ij}$ itself an invariant scalar.\n\n- **Equivariant Vector Message ($\\mathbf{m}_i$)**: The message for atom $i$ is a weighted sum of the equivariant direction vectors pointing to its neighbors:\n$$\n\\mathbf{m}_i = \\sum_{j \\ne i} s_{ij}\\, \\hat{\\mathbf{r}}_{ij}\n$$\nSince this is a linear combination of equivariant vectors ($\\hat{\\mathbf{r}}_{ij}$) with invariant scalar weights ($s_{ij}$), the resulting vector $\\mathbf{m}_i$ is also equivariant. Its transformation under rotation is:\n$$\n\\mathbf{m}'_i = \\sum_{j \\ne i} s'_{ij}\\, \\hat{\\mathbf{r}}'_{ij} = \\sum_{j \\ne i} s_{ij}\\, (\\mathbf{R}\\hat{\\mathbf{r}}_{ij}) = \\mathbf{R} \\left(\\sum_{j \\ne i} s_{ij}\\, \\hat{\\mathbf{r}}_{ij}\\right) = \\mathbf{R}\\mathbf{m}_i\n$$\n\n**3. Invariant Readout and Total Prediction**\n\nThe final stage maps the set of equivariant vectors $\\{\\mathbf{m}_i\\}$ to a single, total invariant scalar $E$.\n\n- **Per-Node Invariant Scalar ($e_i$)**: An invariant scalar is first computed for each node.\n$$\ne_i = c_1 \\|\\mathbf{m}_i\\|_2^2 + c_2 \\sum_{j \\ne i} \\exp(-\\gamma\\, d_{ij})\n$$\nThe term $\\|\\mathbf{m}_i\\|_2^2$ is the squared norm of the equivariant vector $\\mathbf{m}_i$. The norm of an equivariant vector is always invariant, as $\\|\\mathbf{R}\\mathbf{v}\\|_2^2 = (\\mathbf{R}\\mathbf{v})^\\top(\\mathbf{R}\\mathbf{v}) = \\mathbf{v}^\\top\\mathbf{R}^\\top\\mathbf{R}\\mathbf{v} = \\mathbf{v}^\\top\\mathbf{I}\\mathbf{v} = \\|\\mathbf{v}\\|_2^2$. The second term is a sum of functions of the invariant distance $d_{ij}$, and is thus also invariant. Consequently, $e_i$, being a linear combination of invariant terms, is invariant.\n\n- **Total Invariant Prediction ($E$)**: The final prediction is the sum of these per-node invariants and an invariant atomic-type contribution.\n$$\nE(\\mathbf{r}, \\mathbf{Z}) = \\sum_{i=1}^N e_i + c_3 \\sum_{i=1}^N Z_i\n$$\nAs a sum of invariant quantities, the total prediction $E$ is guaranteed to be rotation-invariant, fulfilling the problem's central requirement.\n\n**4. Implementation and Verification**\n\nThe algorithm is implemented using `numpy` for numerical computations. A rotation matrix $\\mathbf{R}$ is generated from a specified axis-angle pair $(\\hat{\\mathbf{a}}, \\theta)$ via Rodrigues' formula: $\\mathbf{R} = \\cos\\theta\\, \\mathbf{I} + \\sin\\theta\\, [\\hat{\\mathbf{a}}]_\\times + (1-\\cos\\theta)\\, \\hat{\\mathbf{a}}\\hat{\\mathbf{a}}^\\top$, where $[\\hat{\\mathbf{a}}]_\\times$ is the skew-symmetric matrix corresponding to the cross-product with $\\hat{\\mathbf{a}}$. The logic is applied to the provided test cases to numerically confirm that, within a defined tolerance $\\tau = 10^{-10}$, the computed value of $E$ is identical for both the original and the rotated atomic coordinates.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and verifies a rotation-invariant scalar prediction for molecular configurations.\n    \"\"\"\n    # Define constants from the problem statement.\n    DELTA = 1e-12\n    A0, A1, A2, A3 = 0.5, 0.1, -0.05, 0.02\n    BETA = 1.3\n    GAMMA = 0.9\n    C1, C2, C3 = 0.7, 0.2, 0.05\n    TOLERANCE_TAU = 1e-10\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Z\": np.array([8, 1, 1], dtype=int),\n            \"r\": np.array([\n                [0.0, 0.0, 0.0],\n                [0.9572, 0.0, 0.0],\n                [-0.2399872, 0.927297, 0.0]\n            ]),\n            \"a\": np.array([1.0, 2.0, 3.0]),\n            \"theta\": 1.234\n        },\n        {\n            \"Z\": np.array([6, 1, 1], dtype=int),\n            \"r\": np.array([\n                [0.0, 0.0, 0.0],\n                [1.1, 0.0, 0.0],\n                [-1.1, 0.0, 0.0]\n            ]),\n            \"a\": np.array([0.0, 0.0, 1.0]),\n            \"theta\": np.pi\n        },\n        {\n            \"Z\": np.array([8], dtype=int),\n            \"r\": np.array([\n                [1.0, 2.0, 3.0]\n            ]),\n            \"a\": np.array([0.0, 0.0, 1.0]),\n            \"theta\": 0.7\n        },\n        {\n            \"Z\": np.array([1, 1, 1], dtype=int),\n            \"r\": np.array([\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0]\n            ]),\n            \"a\": np.array([0.0, 1.0, 0.0]),\n            \"theta\": 2.2\n        }\n    ]\n    \n    def compute_E(positions, types):\n        \"\"\"\n        Computes the total scalar prediction E for a given molecular configuration.\n        \"\"\"\n        num_atoms = len(types)\n        \n        # Handle single-atom and empty cases.\n        if num_atoms <= 1:\n            return C3 * np.sum(types)\n\n        total_e_sum = 0.0\n        for i in range(num_atoms):\n            m_i = np.zeros(3)\n            e_i_dist_term = 0.0\n            \n            for j in range(num_atoms):\n                if i == j:\n                    continue\n                \n                # Calculate relative vector, distance, and unit vector\n                r_ij = positions[j] - positions[i]\n                d_ij = np.linalg.norm(r_ij)\n                \n                if d_ij <= DELTA:\n                    r_hat_ij = np.zeros(3)\n                else:\n                    r_hat_ij = r_ij / d_ij\n                \n                # Calculate scalar coefficient s_ij\n                s_ij = np.exp(-BETA * d_ij) * (A0 + A1 * types[i] + A2 * types[j] + A3 * types[i] * types[j])\n                \n                # Update message vector m_i\n                m_i += s_ij * r_hat_ij\n                \n                # Update distance-dependent term for e_i\n                e_i_dist_term += np.exp(-GAMMA * d_ij)\n            \n            # Calculate per-node scalar e_i\n            m_i_norm_sq = np.dot(m_i, m_i) # More efficient than np.linalg.norm()**2\n            e_i = C1 * m_i_norm_sq + C2 * e_i_dist_term\n            total_e_sum += e_i\n            \n        # Calculate total prediction E\n        total_E = total_e_sum + C3 * np.sum(types)\n        return total_E\n\n    results = []\n    for case in test_cases:\n        Z, r, a, theta = case[\"Z\"], case[\"r\"], case[\"a\"], case[\"theta\"]\n        \n        # 1. Compute E for the original configuration\n        E_original = compute_E(r, Z)\n        \n        # 2. Generate the rotation matrix R using Rodrigues' formula\n        a_norm = np.linalg.norm(a)\n        if a_norm < DELTA: # Handles zero vector for axis\n             a_hat = np.zeros(3)\n        else:\n             a_hat = a / a_norm\n        \n        I = np.identity(3)\n        a_cross_matrix = np.array([\n            [0, -a_hat[2], a_hat[1]],\n            [a_hat[2], 0, -a_hat[0]],\n            [-a_hat[1], a_hat[0], 0]\n        ])\n        a_outer_product = np.outer(a_hat, a_hat)\n        \n        R = np.cos(theta) * I + np.sin(theta) * a_cross_matrix + (1 - np.cos(theta)) * a_outer_product\n\n        # 3. Apply rotation to coordinates\n        # r is (N, 3). R is (3, 3). We need (r @ R.T) or (R @ r.T).T\n        r_rotated = r @ R.T\n\n        # 4. Compute E for the rotated configuration\n        E_rotated = compute_E(r_rotated, Z)\n        \n        # 5. Check for invariance within the given tolerance\n        is_invariant = np.abs(E_original - E_rotated) <= TOLERANCE_TAU\n        results.append(is_invariant)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond analyzing existing data, deep learning models can accelerate scientific discovery by guiding the experimental process itself. This practice explores the concept of active learning, where a model helps design the most informative experiment possible . You will implement a strategy to determine the optimal placement of sensors to most efficiently estimate a parameter in a heat diffusion model, demonstrating how to \"close the loop\" between theory and experiment.",
            "id": "3117036",
            "problem": "You will design and implement an active learning strategy for optimal sensor placement to estimate the unknown thermal diffusivity parameter $\\alpha$ in the one-dimensional heat equation. Consider the Partial Differential Equation (PDE) $u_t = \\alpha u_{xx}$ on the spatial domain $x \\in [0,1]$ with time $t \\ge 0$, subject to homogeneous Dirichlet boundary conditions $u(0,t)=0$ and $u(1,t)=0$, and initial condition $u(x,0)=\\sin(\\pi x)$. The goal is to place $k$ point sensors at distinct spatial positions $x_i$ selected from a discrete candidate set, in order to minimize the posterior variance of $\\alpha$ under a Gaussian prior and Gaussian observation noise, using a first-order linearization of the forward model about the prior mean. All quantities are nondimensional; no physical units are required.\n\nFundamental base to use:\n- The PDE $u_t=\\alpha u_{xx}$ with $u(0,t)=0$, $u(1,t)=0$, and $u(x,0)=\\sin(\\pi x)$ admits a solution derivable by separation of variables.\n- Gaussian prior on $\\alpha$: $\\alpha \\sim \\mathcal{N}(\\mu_\\alpha, s_\\alpha^2)$.\n- Independent additive Gaussian noise on each sensor measurement: $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n- Linearization of the forward map $F(\\alpha) = u(x,t;\\alpha)$ at $\\alpha=\\mu_\\alpha$ yields a linear observation model suitable for a Gaussian posterior update.\n\nObservation model and design:\n- A single-time snapshot is collected at time $t_\\star$.\n- You may place exactly $k$ sensors at distinct positions $x_i$ chosen from the candidate grid $\\{x_j\\}_{j=0}^{20}$, where $x_j = j/20$ for $j \\in \\{0,1,\\dots,20\\}$.\n- Using a first-order linearization of $u(x,t_\\star;\\alpha)$ about $\\alpha=\\mu_\\alpha$, define a linear observation model and derive the scalar posterior variance of $\\alpha$ as a function of the chosen sensor positions. The active learning objective is to select the $k$ positions that minimize this posterior variance.\n- Your algorithm must use this principle-based derivation to compute the optimal selection from the discrete candidate set.\n\nYour program must implement the above and, for each test case below, compute the minimized posterior variance of $\\alpha$ after selecting the $k$ optimal sensor locations. Use the following test suite, where each case specifies $(\\mu_\\alpha, s_\\alpha^2, \\sigma^2, k, t_\\star)$:\n\n- Case $1$: $(\\mu_\\alpha, s_\\alpha^2, \\sigma^2, k, t_\\star) = (0.1, 0.25, 0.01, 3, 0.5)$.\n- Case $2$: $(\\mu_\\alpha, s_\\alpha^2, \\sigma^2, k, t_\\star) = (0.1, 0.25, 0.01, 0, 0.5)$.\n- Case $3$: $(\\mu_\\alpha, s_\\alpha^2, \\sigma^2, k, t_\\star) = (0.1, 0.25, 1.0, 3, 0.5)$.\n- Case $4$: $(\\mu_\\alpha, s_\\alpha^2, \\sigma^2, k, t_\\star) = (0.1, 0.0001, 0.01, 5, 0.5)$.\n- Case $5$: $(\\mu_\\alpha, s_\\alpha^2, \\sigma^2, k, t_\\star) = (0.1, 0.25, 0.01, 3, 0.0)$.\n\nRequirements:\n- Implement the active learning step by selecting exactly $k$ distinct positions from the candidate grid that minimize the posterior variance under the linearized, additive Gaussian model.\n- For each test case, output the minimized posterior variance of $\\alpha$ as a floating-point number.\n- All quantities are nondimensional; do not include any physical units.\n- Angle evaluations (e.g., $\\sin(\\cdot)$) use radians.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of floating-point numbers rounded to six decimal places, enclosed in square brackets, in the same order as the cases listed above (for example, $[v_1,v_2,v_3,v_4,v_5]$).",
            "solution": "The problem is valid as it is scientifically grounded in the principles of partial differential equations and Bayesian inference, is well-posed with a clear objective, and all parameters are defined. We proceed with the solution.\n\nThe core of the problem is to select $k$ sensor locations $\\{x_i\\}_{i=1}^k$ from a discrete candidate set to minimize the posterior variance of the thermal diffusivity parameter $\\alpha$. This is a classic problem in optimal experimental design.\n\nFirst, we solve the given partial differential equation (PDE) to establish the forward model. The PDE is the one-dimensional heat equation $u_t = \\alpha u_{xx}$ on the domain $x \\in [0,1]$, with homogeneous Dirichlet boundary conditions $u(0,t)=0$, $u(1,t)=0$, and initial condition $u(x,0)=\\sin(\\pi x)$. Using the method of separation of variables, the solution is found to be:\n$$\nu(x,t;\\alpha) = \\sin(\\pi x) e^{-\\alpha \\pi^2 t}\n$$\nThis function $u(x,t;\\alpha)$ is the forward model, which maps the parameter $\\alpha$ to the observable temperature field.\n\nThe problem specifies a Bayesian framework. The prior belief about $\\alpha$ is given by a Gaussian distribution, $\\alpha \\sim \\mathcal{N}(\\mu_\\alpha, s_\\alpha^2)$. Measurements are taken at a single time $t_\\star$ at $k$ sensor locations $\\{x_i\\}_{i=1}^k$. Each measurement $y_i$ is corrupted by independent, additive Gaussian noise, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. The observation at sensor $i$ is thus $y_i = u(x_i, t_\\star; \\alpha) + \\varepsilon_i$.\n\nTo make the inference tractable, we linearize the forward model $u(x,t;\\alpha)$ around the prior mean $\\mu_\\alpha$ using a first-order Taylor expansion:\n$$\nu(x_i, t_\\star; \\alpha) \\approx u(x_i, t_\\star; \\mu_\\alpha) + \\frac{\\partial u}{\\partial \\alpha}\\bigg|_{\\alpha=\\mu_\\alpha} (\\alpha - \\mu_\\alpha)\n$$\nLet $J(x_i, t_\\star) = \\frac{\\partial u}{\\partial \\alpha}\\big|_{\\alpha=\\mu_\\alpha}$. The derivative is:\n$$\n\\frac{\\partial u}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left( \\sin(\\pi x) e^{-\\alpha \\pi^2 t} \\right) = -\\pi^2 t \\sin(\\pi x) e^{-\\alpha \\pi^2 t}\n$$\nEvaluating at $\\alpha = \\mu_\\alpha$ and $t = t_\\star$, the Jacobian (or sensitivity) at location $x_i$ is:\n$$\nJ(x_i, t_\\star) = -\\pi^2 t_\\star \\sin(\\pi x_i) e^{-\\mu_\\alpha \\pi^2 t_\\star}\n$$\nThe observation model for a vector of $k$ measurements $\\mathbf{y} = [y_1, \\dots, y_k]^T$ can be written in a linear form. Let $\\tilde{\\alpha} = \\alpha - \\mu_\\alpha$ be the centered parameter, which has a prior $\\tilde{\\alpha} \\sim \\mathcal{N}(0, s_\\alpha^2)$. Let $\\tilde{\\mathbf{y}} = \\mathbf{y} - \\mathbf{u}(t_\\star; \\mu_\\alpha)$ be the centered observation vector. The linearized model is:\n$$\n\\tilde{\\mathbf{y}} = \\mathbf{J} \\tilde{\\alpha} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{J}$ is a $k \\times 1$ column vector with entries $J(x_i, t_\\star)$, and $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_k)$.\n\nFor this standard Bayesian linear model, the posterior distribution of $\\tilde{\\alpha}$ (and thus $\\alpha$) is also Gaussian. The posterior precision (inverse variance) is the sum of the prior precision and the precision gained from the data:\n$$\n\\frac{1}{s^2_{\\text{post}}} = \\frac{1}{s_\\alpha^2} + \\frac{1}{\\sigma^2} \\mathbf{J}^T \\mathbf{J}\n$$\nThe term $\\mathbf{J}^T \\mathbf{J}$ is the Fisher information for this linearized scalar parameter problem.\n$$\n\\mathbf{J}^T \\mathbf{J} = \\sum_{i=1}^k J(x_i, t_\\star)^2 = \\sum_{i=1}^k \\left( -\\pi^2 t_\\star \\sin(\\pi x_i) e^{-\\mu_\\alpha \\pi^2 t_\\star} \\right)^2\n$$\n$$\n\\mathbf{J}^T \\mathbf{J} = \\left( (\\pi^2 t_\\star)^2 e^{-2\\mu_\\alpha \\pi^2 t_\\star} \\right) \\sum_{i=1}^k \\sin^2(\\pi x_i)\n$$\nThe posterior variance is therefore:\n$$\ns^2_{\\text{post}}(\\{x_i\\}_{i=1}^k) = \\left( \\frac{1}{s_\\alpha^2} + \\frac{(\\pi^2 t_\\star)^2 e^{-2\\mu_\\alpha \\pi^2 t_\\star}}{\\sigma^2} \\sum_{i=1}^k \\sin^2(\\pi x_i) \\right)^{-1}\n$$\nThe objective is to choose $k$ distinct positions $\\{x_i\\}$ from the candidate grid to minimize this posterior variance $s^2_{\\text{post}}$. Minimizing $s^2_{\\text{post}}$ is equivalent to maximizing its reciprocal, the posterior precision. Since the terms $s_\\alpha^2$, $\\sigma^2$, $\\mu_\\alpha$, and $t_\\star$ are fixed for each test case, maximizing the posterior precision is equivalent to maximizing the sum:\n$$\n\\underset{\\{x_i\\}_{i=1}^k \\subset X_{\\text{cand}}}{\\text{maximize}} \\quad \\sum_{i=1}^k \\sin^2(\\pi x_i)\n$$\nThe optimal sensor placement strategy is a greedy algorithm:\n1. For each candidate location $x_j$ in the grid $\\{j/20\\}_{j=0}^{20}$, calculate the information contribution term $\\sin^2(\\pi x_j)$.\n2. Sort these contribution values in descending order.\n3. Select the top $k$ values from this sorted list. The sum of these $k$ values is the maximized sum $\\sum_{i=1}^k \\sin^2(\\pi x_i)$.\n4. Substitute this sum into the expression for $s^2_{\\text{post}}$ to find the minimal posterior variance.\n\nSpecial cases must be considered. If $k=0$ (no sensors), no data is collected, so the posterior is the same as the prior, and $s^2_{\\text{post}} = s_\\alpha^2$. If $t_\\star=0$, the solution $u(x,0)=\\sin(\\pi x)$ is independent of $\\alpha$. Consequently, the Jacobian $J(x,0)$ is zero, no information about $\\alpha$ can be gained, and again $s^2_{\\text{post}} = s_\\alpha^2$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the minimized posterior variance of the thermal diffusivity parameter `alpha`\n    by selecting optimal sensor locations for the 1D heat equation.\n    \"\"\"\n    test_cases = [\n        # (mu_alpha, s_alpha_sq, sigma_sq, k, t_star)\n        (0.1, 0.25, 0.01, 3, 0.5),\n        (0.1, 0.25, 0.01, 0, 0.5),\n        (0.1, 0.25, 1.0, 3, 0.5),\n        (0.1, 0.0001, 0.01, 5, 0.5),\n        (0.1, 0.25, 0.01, 3, 0.0),\n    ]\n\n    results = []\n\n    # Pre-calculate candidate grid properties, which are common to all test cases.\n    # The candidate grid has 21 points: x_j = j/20 for j in {0, ..., 20}.\n    num_candidates = 21\n    candidate_x = np.arange(num_candidates) / (num_candidates - 1.0)\n    \n    # The information contribution of each sensor location is proportional to sin^2(pi*x).\n    # We pre-calculate these values and sort them to facilitate greedy selection.\n    info_values = np.sin(np.pi * candidate_x)**2\n    sorted_info_values = np.sort(info_values)[::-1]  # Sort in descending order\n\n    for case in test_cases:\n        mu_alpha, s_alpha_sq, sigma_sq, k, t_star = case\n\n        # Special Cases: If k=0 (no sensors are placed) or t_star=0 (measurements\n        # are insensitive to alpha), no information is gained from data. The posterior\n        # variance is simply the prior variance.\n        if k == 0 or t_star == 0.0:\n            post_var = s_alpha_sq\n            results.append(post_var)\n            continue\n\n        # Active Learning Step: Select the top k sensor locations by picking\n        # the locations with the highest information contribution.\n        # The total information from the optimal set of sensors is the sum of the\n        # top k sorted information values.\n        optimal_info_sum = np.sum(sorted_info_values[:k])\n\n        # Bayesian Update Calculation:\n        # 1. Prior precision is the reciprocal of prior variance.\n        prior_precision = 1.0 / s_alpha_sq\n\n        # 2. The data-based precision term depends on a scaling factor C_factor\n        # and the sum of information contributions.\n        # C_factor = ( (d(u)/d(alpha)) / sin(pi*x) )^2\n        # C_factor = (pi^2 * t_star)^2 * exp(-2 * mu_alpha * pi^2 * t_star)\n        pi_sq_t_star = np.pi**2 * t_star\n        C_factor = (pi_sq_t_star**2) * np.exp(-2.0 * mu_alpha * pi_sq_t_star)\n        \n        # 3. Precision gained from the data.\n        data_precision = (C_factor / sigma_sq) * optimal_info_sum\n        \n        # 4. Posterior precision is the sum of prior precision and data precision.\n        post_precision = prior_precision + data_precision\n        \n        # 5. The minimized posterior variance is the reciprocal of the posterior precision.\n        post_var = 1.0 / post_precision\n        \n        results.append(post_var)\n\n    # Format the final output as a comma-separated list of floating-point numbers\n    # rounded to six decimal places, enclosed in square brackets.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}