{
    "hands_on_practices": [
        {
            "introduction": "Many fundamental discrete probability distributions arise directly from the statistical analysis of simple physical systems. This practice demonstrates how the well-known binomial distribution can be derived from the first principles of statistical mechanics applied to an ideal gas. By considering the random positions of gas particles after mixing, you will see how the discrete probability of finding a certain number of particles in a sub-volume naturally emerges . This exercise provides a concrete physical anchor for the abstract concepts of Bernoulli trials and binomial probabilities.",
            "id": "1961995",
            "problem": "A rigid, insulated container of total volume $V$ is divided by a removable partition into two sub-volumes, $V_1$ and $V_2$. The volume $V_1$ contains $N_A$ particles of an ideal gas A, while volume $V_2$ contains $N_B$ particles of a different ideal gas B. The fractional volume occupied by gas A is given by $\\alpha$, such that $V_1 = \\alpha V$ and $V_2 = (1-\\alpha)V$, where $0 < \\alpha < 1$.\n\nInitially, both gases are in thermal equilibrium. The partition is then removed, allowing the two gases to mix and the combined system to reach a new state of thermodynamic equilibrium. Based on the fundamental postulates of statistical mechanics, derive an expression for the probability, $P(n)$, of finding exactly $n$ particles of gas A within the spatial region that was originally volume $V_2$. The variable $n$ is an integer that can range from $0$ to $N_A$.",
            "solution": "After the partition is removed, the combined system evolves to equilibrium. By the fundamental postulate of statistical mechanics (equal a priori probabilities), the microcanonical measure is uniform over the accessible region of phase space. For an ideal gas mixture with no interparticle interactions and hard-wall confinement to the volume $V$, the Hamiltonian separates as\n$$\nH=\\sum_{i=1}^{N_{A}} \\frac{|\\mathbf{p}_{i}|^{2}}{2 m_{A}}+\\sum_{j=1}^{N_{B}} \\frac{|\\mathbf{p}'_{j}|^{2}}{2 m_{B}},\n$$\nwith all positions constrained to lie within $V$. Hence, in the microcanonical ensemble at fixed total energy, momentum and position variables factorize. For any event that constrains only the spatial positions (such as the number of gas A particles lying in a specified subvolume), the ratio of its probability to the total is given by the ratio of the corresponding configuration-space volumes; the momentum integrals are common factors and cancel.\n\nLet $V_{1}=\\alpha V$ and $V_{2}=(1-\\alpha) V$ denote the original subvolumes. At equilibrium, the marginal distribution of each gas A particle’s position is uniform over $V$. Therefore, the probability that a given A particle lies in $V_{2}$ is $V_{2}/V=1-\\alpha$, and in $V_{1}$ is $V_{1}/V=\\alpha$. Because the particles are noninteracting, the joint distribution over the $N_{A}$ positions factorizes, and one may equivalently compute probabilities by configuration-space volume counting.\n\nCompute $P(n)$ as a ratio of configuration-space measures. For definiteness, treat gas A particles as distinguishable during the counting (the indistinguishability Gibbs factors cancel in the final ratio). The total configuration-space volume for the $N_{A}$ positions is $V^{N_{A}}$. The configuration-space volume corresponding to exactly $n$ of the $N_{A}$ A particles lying in $V_{2}$ and the remaining $N_{A}-n$ in $V_{1}$ is\n$$\n\\binom{N_{A}}{n} V_{2}^{n} V_{1}^{N_{A}-n},\n$$\nwhere the binomial coefficient counts the choices of which $n$ particles lie in $V_{2}$. Therefore,\n$$\nP(n)=\\frac{\\binom{N_{A}}{n} V_{2}^{n} V_{1}^{N_{A}-n}}{V^{N_{A}}}\n=\\binom{N_{A}}{n} \\left(\\frac{V_{2}}{V}\\right)^{n} \\left(\\frac{V_{1}}{V}\\right)^{N_{A}-n}.\n$$\nSubstituting $V_{1}=\\alpha V$ and $V_{2}=(1-\\alpha) V$ gives\n$$\nP(n)=\\binom{N_{A}}{n} (1-\\alpha)^{n} \\alpha^{N_{A}-n},\n$$\nvalid for all integers $n$ with $0 \\leq n \\leq N_{A}$. The presence of gas B only contributes a multiplicative factor to the phase-space volume that cancels in the probability ratio, so $P(n)$ is independent of $N_{B}$ for an ideal mixture.",
            "answer": "$$\\boxed{\\binom{N_{A}}{n} (1-\\alpha)^{n} \\alpha^{N_{A}-n}}$$"
        },
        {
            "introduction": "In computational science, it is often practical to approximate a complex discrete distribution with a simpler continuous one, especially when the number of trials is large. This practice explores one of the most important examples: the approximation of the binomial distribution by a normal (or Gaussian) distribution, which is justified by the Central Limit Theorem. You will move beyond theory to write a program that numerically validates this approximation, and in doing so, discover the importance of the continuity correction—a small adjustment that significantly improves the accuracy of the transition from a discrete sum to a continuous integral .",
            "id": "3119292",
            "problem": "You are given independent and identically distributed Bernoulli random variables $Y_1, Y_2, \\dots, Y_n$ with parameter $p$, that is, $Y_i \\in \\{0,1\\}$ and $\\mathbb{P}(Y_i = 1) = p$. Define the binomial count $X = \\sum_{i=1}^{n} Y_i$. Starting from the core definitions $\\mathbb{E}[Y_i] = p$, $\\mathrm{Var}(Y_i) = p(1-p)$, and the Central Limit Theorem (CLT), derive in words the continuous approximation for the cumulative distribution function $\\mathbb{P}(X \\leq k)$ using a normal distribution, both without and with continuity correction. Implement a program to numerically validate these approximations against the exact binomial cumulative distribution function.\n\nYour program must:\n\n1. Compute the exact cumulative probability $\\mathbb{P}(X \\leq k)$ using an analytically correct method for the binomial distribution.\n2. Compute two normal approximations to $\\mathbb{P}(X \\leq k)$:\n   - A direct approximation based on CLT without continuity correction.\n   - An approximation with continuity correction by shifting the discrete threshold to a half-integer before applying the continuous model.\n3. Simulate binomial counts to estimate $\\mathbb{P}(X \\leq k)$ empirically using a fixed random seed and a fixed number of Monte Carlo trials. Use $N_{\\text{sim}} = 100000$ trials and the seed $12345$ so that the simulation is reproducible. The simulation is for illustration; the acceptance checks below must compare approximations against the exact analytic binomial probabilities.\n4. For each test case, compute the absolute errors of both normal approximations relative to the exact binomial cumulative probability. Let the tolerance be $\\tau = 0.02$. For each case, produce three booleans:\n   - Whether the error without continuity correction is at most $\\tau$.\n   - Whether the error with continuity correction is at most $\\tau$.\n   - Whether the error with continuity correction is less than or equal to the error without continuity correction.\n\nTest suite:\nUse the following $(n,p,k)$ cases to test a range of regimes (large $n$ and small $p$, near-mean thresholds, and boundary conditions):\n- Case 1: $(n=1000,\\; p=0.01,\\; k=0)$.\n- Case 2: $(n=1000,\\; p=0.01,\\; k=12)$.\n- Case 3: $(n=10000,\\; p=0.001,\\; k=20)$.\n- Case 4: $(n=500,\\; p=0.02,\\; k=15)$.\n- Case 5: $(n=200,\\; p=0.001,\\; k=0)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be the flattened concatenation, in order of the test cases, of the three booleans per case: $[$no-correction-pass, continuity-correction-pass, improvement$]$ for Case $1$, then the same triple for Case $2$, and so on. For example, a five-case run would look like\n$[$b$_{1,1}$, b$_{1,2}$, b$_{1,3}$, b$_{2,1}$, b$_{2,2}$, b$_{2,3}$, \\dots, b$_{5,1}$, b$_{5,2}$, b$_{5,3}]$,\nwhere each $b_{i,j}$ is a boolean.",
            "solution": "The problem requires the derivation of the normal approximation to the binomial distribution, with and without continuity correction, and a numerical validation of these approximations against exact values. The derivation will proceed from the foundational properties of Bernoulli variables and the Central Limit Theorem (CLT).\n\nA binomial random variable $X$ represents the total number of successes in $n$ independent trials, where each trial has a success probability of $p$. It is defined as the sum of $n$ independent and identically distributed (i.i.d.) Bernoulli random variables, $Y_1, Y_2, \\dots, Y_n$.\n$$X = \\sum_{i=1}^{n} Y_i$$\nEach $Y_i$ takes a value of $1$ (success) with probability $p$ or $0$ (failure) with probability $1-p$. The problem provides the expectation and variance for a single Bernoulli trial:\n- Expectation: $\\mathbb{E}[Y_i] = 1 \\cdot p + 0 \\cdot (1-p) = p$\n- Variance: $\\mathrm{Var}(Y_i) = \\mathbb{E}[Y_i^2] - (\\mathbb{E}[Y_i])^2 = (1^2 \\cdot p + 0^2 \\cdot (1-p)) - p^2 = p - p^2 = p(1-p)$\n\nDue to the linearity of expectation and the fact that the variables are independent for variance summation, the expectation and variance of the binomial random variable $X$ are:\n- Expectation of $X$: $\\mu = \\mathbb{E}[X] = \\mathbb{E}\\left[\\sum_{i=1}^{n} Y_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[Y_i] = \\sum_{i=1}^{n} p = np$\n- Variance of $X$: $\\sigma^2 = \\mathrm{Var}(X) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} Y_i\\right) = \\sum_{i=1}^{n} \\mathrm{Var}(Y_i) = \\sum_{i=1}^{n} p(1-p) = np(1-p)$\n\nThe Central Limit Theorem (CLT) states that for a sufficiently large number of i.i.d. random variables, their sum (or average) will be approximately normally distributed. Applying the CLT to $X$, we can approximate its distribution with a normal distribution having the same mean $\\mu = np$ and variance $\\sigma^2 = np(1-p)$. Let this approximating normal variable be $X_{norm} \\sim \\mathcal{N}(np, np(1-p))$. The standardized version of $X$ is $Z = \\frac{X - \\mu}{\\sigma} = \\frac{X - np}{\\sqrt{np(1-p)}}$, which converges in distribution to the standard normal distribution $\\mathcal{N}(0,1)$ as $n \\to \\infty$.\n\nWe wish to approximate the cumulative distribution function (CDF) of the binomial distribution, $\\mathbb{P}(X \\leq k) = \\sum_{j=0}^{k} \\mathbb{P}(X=j)$.\n\n**1. Normal Approximation without Continuity Correction**\n\nThis is the most direct application of the CLT. We approximate the discrete binomial variable $X$ with the continuous normal variable $X_{norm}$. The probability $\\mathbb{P}(X \\leq k)$ is approximated by $\\mathbb{P}(X_{norm} \\leq k)$. To compute this probability, we standardize the variable:\n$$ \\mathbb{P}(X \\leq k) \\approx \\mathbb{P}(X_{norm} \\leq k) = \\mathbb{P}\\left(\\frac{X_{norm} - \\mu}{\\sigma} \\leq \\frac{k - \\mu}{\\sigma}\\right) = \\mathbb{P}\\left(Z \\leq \\frac{k - np}{\\sqrt{np(1-p)}}\\right) $$\nThis probability is given by the CDF of the standard normal distribution, denoted by $\\Phi(z)$.\n$$ \\mathbb{P}(X \\leq k) \\approx \\Phi\\left(\\frac{k - np}{\\sqrt{np(1-p)}}\\right) $$\n\n**2. Normal Approximation with Continuity Correction**\n\nThis method provides a more refined approximation by accounting for the fact that we are using a continuous distribution to model a discrete one. The probability mass function (PMF) of a binomial distribution is defined only at integer values. A common visualization is a histogram where the probability $\\mathbb{P}(X=j)$ is represented by a bar of width $1$ centered at the integer $j$. This bar spans the interval $[j-0.5, j+0.5]$.\nThe cumulative probability $\\mathbb{P}(X \\leq k)$ is the sum of probabilities for all integers from $0$ up to $k$. In the histogram representation, this corresponds to the total area of the bars for $j=0, 1, \\dots, k$. The total region covered by these bars extends up to $k+0.5$ on the continuous axis.\nTherefore, to better approximate this sum, we integrate the normal probability density function up to $k+0.5$.\n$$ \\mathbb{P}(X \\leq k) = \\sum_{j=0}^{k} \\mathbb{P}(X=j) \\approx \\mathbb{P}(X_{norm} \\leq k+0.5) $$\nStandardizing this corrected value gives the continuity-corrected approximation:\n$$ \\mathbb{P}(X \\leq k) \\approx \\mathbb{P}\\left(Z \\leq \\frac{(k+0.5) - np}{\\sqrt{np(1-p)}}\\right) = \\Phi\\left(\\frac{k+0.5 - np}{\\sqrt{np(1-p)}}\\right) $$\nThis correction generally improves the accuracy of the normal approximation, especially when $n$ is not excessively large or when $p$ is close to $0$ or $1$.\n\n**3. Numerical Validation**\n\nThe program will implement the following computations for each test case $(n,p,k)$:\n- **Exact Binomial CDF**: $\\mathbb{P}(X \\leq k) = \\sum_{j=0}^{k} \\binom{n}{j} p^j (1-p)^{n-j}$, computed using `scipy.stats.binom.cdf`.\n- **Normal Approximation (no CC)**: $\\Phi\\left(\\frac{k - np}{\\sqrt{np(1-p)}}\\right)$, computed using `scipy.stats.norm.cdf`.\n- **Normal Approximation (with CC)**: $\\Phi\\left(\\frac{k+0.5 - np}{\\sqrt{np(1-p)}}\\right)$, computed using `scipy.stats.norm.cdf`.\n- **Monte Carlo Simulation**: An empirical estimate is obtained by generating $N_{\\text{sim}} = 100000$ samples from a binomial distribution with parameters $n$ and $p$, and then calculating the fraction of samples that are less than or equal to $k$. This is for illustration purposes.\n- **Error Analysis**: The absolute errors of the two normal approximations relative to the exact binomial CDF are calculated. These errors are compared against a tolerance $\\tau = 0.02$ to determine if each approximation is acceptable. It is also checked whether the continuity correction provides an improvement (i.e., a smaller or equal error).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import binom, norm\n\ndef solve():\n    \"\"\"\n    Validates normal approximations to the binomial distribution.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, p, k)\n        (1000, 0.01, 0),\n        (1000, 0.01, 12),\n        (10000, 0.001, 20),\n        (500, 0.02, 15),\n        (200, 0.001, 0),\n    ]\n\n    # Constants for simulation and validation\n    N_sim = 100000\n    seed = 12345\n    tau = 0.02\n    \n    # Initialize a random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n    \n    results = []\n    for case in test_cases:\n        n, p, k = case\n        \n        # Calculate mean and standard deviation of the binomial distribution\n        mu = n * p\n        sigma_sq = n * p * (1 - p)\n\n        # 1. Compute the exact cumulative probability P(X = k)\n        exact_prob = binom.cdf(k, n, p)\n\n        # 2. Compute normal approximations\n        # Handle the edge case where variance is zero (p=0 or p=1),\n        # though not present in the test suite.\n        if sigma_sq > 0:\n            sigma = np.sqrt(sigma_sq)\n            \n            # 2a. Direct approximation without continuity correction\n            z_no_cc = (k - mu) / sigma\n            approx_no_cc = norm.cdf(z_no_cc)\n            \n            # 2b. Approximation with continuity correction\n            z_cc = (k + 0.5 - mu) / sigma\n            approx_cc = norm.cdf(z_cc)\n        else:\n            # If sigma is 0, the distribution is deterministic.\n            # X = mu with probability 1. The CDF is a step function.\n            approx_no_cc = 1.0 if k >= mu else 0.0\n            approx_cc = 1.0 if k + 0.5 >= mu else 0.0\n        \n        # 3. Simulate binomial counts to estimate P(X = k) empirically (for illustration)\n        # This part is required by the prompt but its result is not used in the final checks.\n        sim_samples = rng.binomial(n, p, size=N_sim)\n        mc_prob = np.mean(sim_samples = k)\n\n        # 4. Compute absolute errors and perform validation checks.\n        # The checks compare the approximations against the exact analytical probability.\n        err_no_cc = abs(approx_no_cc - exact_prob)\n        err_cc = abs(approx_cc - exact_prob)\n        \n        # Boolean check 1: Is error without correction within tolerance?\n        no_cc_pass = err_no_cc = tau\n        \n        # Boolean check 2: Is error with correction within tolerance?\n        cc_pass = err_cc = tau\n        \n        # Boolean check 3: Is continuity correction an improvement?\n        cc_improves = err_cc = err_no_cc\n        \n        results.extend([no_cc_pass, cc_pass, cc_improves])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A crucial task in any simulation is to verify that the numbers being generated truly follow the intended probability distribution. This advanced computational exercise introduces a powerful and elegant method for this task: the probability integral transform. This theorem provides a universal way to transform any random variable into a uniformly distributed one using its own cumulative distribution function (CDF). By implementing a verification workflow using the formal Kolmogorov-Smirnov statistical test, you will gain hands-on experience with a professional-grade validation technique and explore its limitations when dealing with discrete distributions or the finite-precision realities of computation .",
            "id": "3119395",
            "problem": "A developer is asked to implement an automated verification of a sampler using the probability integral transform under finite precision constraints in the context of discrete and continuous probability distributions. The foundational starting point is the definition of the cumulative distribution function (CDF): for a real-valued random variable $X$, the cumulative distribution function $F_X(x)$ is defined as $F_X(x) = \\mathbb{P}(X \\le x)$. For a continuous distribution with a strictly increasing CDF, if $X$ is sampled from that distribution and one computes $U = F_X(X)$, then the probability integral transform implies that $U$ is distributed as $\\text{Uniform}(0,1)$. Under finite precision constraints, such as quantization of $X$ to a grid of step size $\\Delta$ or rounding $U$ to $d$ decimal places, the transformed values $U$ may deviate from exact uniformity. For discrete distributions (for example, a Poisson distribution), the CDF has jumps, and computing $U = F_X(X)$ without randomization produces a discrete distribution for $U$, which typically deviates from exact uniformity.\n\nThe verification must use the Kolmogorov-Smirnov (KS) test, whose purpose is to compare the empirical distribution function of observed data to a reference cumulative distribution function. For the one-sample case against a uniform reference, given observed values $u_1, u_2, \\ldots, u_n$ in $[0,1]$, the Kolmogorov-Smirnov test statistic compares the empirical CDF $\\hat{F}_n(u)$ with the reference $F_U(u) = u$ for the $\\text{Uniform}(0,1)$ distribution via a supremum distance, and returns a $p$-value. The decision rule is: accept the null hypothesis that the data are $\\text{Uniform}(0,1)$ if the returned $p$-value is at least a chosen significance level $\\alpha$, otherwise reject.\n\nYou must write a complete, runnable program that:\n- For each test case, generates a deterministic base set of $n$ values $U^{\\text{base}}_i = \\frac{i + 0.5}{n}$ for $i = 0, 1, \\ldots, n-1$, uses the inverse cumulative distribution function (quantile function) $F_X^{-1}$ to produce samples $X_i = F_X^{-1}(U^{\\text{base}}_i)$ for a specified distribution, applies optional quantization $X_i^{\\Delta} = \\Delta \\cdot \\mathrm{round}\\!\\left(\\frac{X_i}{\\Delta}\\right)$ if a finite precision constraint $\\Delta$ is provided, then computes $U_i = F_X(X_i^{\\Delta})$ and optionally rounds $U_i$ to $d$ decimal places if requested. It next performs the one-sample Kolmogorov-Smirnov test of $U_i$ against the $\\text{Uniform}(0,1)$ distribution at significance level $\\alpha$, and records a boolean result indicating whether the uniformity is accepted ($p \\ge \\alpha$).\n- Uses only the specified libraries and produces deterministic results without requiring any input.\n\nTest suite of parameter values to exercise different facets:\n- Case $1$ (continuous, no finite precision constraints, happy path):\n  - Distribution: standard normal with parameters $\\mu = 0$, $\\sigma = 1$.\n  - Sample size: $n = 2000$.\n  - Quantization step: $\\Delta = \\varnothing$ (no quantization).\n  - Rounding of $U$: $d = \\varnothing$ (no rounding).\n  - Significance level: $\\alpha = 10^{-3}$.\n- Case $2$ (continuous with coarse quantization, expected deviation):\n  - Distribution: standard normal with parameters $\\mu = 0$, $\\sigma = 1$.\n  - Sample size: $n = 4000$.\n  - Quantization step: $\\Delta = 0.5$.\n  - Rounding of $U$: $d = \\varnothing$.\n  - Significance level: $\\alpha = 10^{-3}$.\n- Case $3$ (continuous with rounding on $U$, expected deviation):\n  - Distribution: exponential with rate $\\lambda = 1$ (that is, scale parameter $\\theta = 1/\\lambda = 1$).\n  - Sample size: $n = 3000$.\n  - Quantization step: $\\Delta = \\varnothing$.\n  - Rounding of $U$: $d = 2$ (round to two decimal places).\n  - Significance level: $\\alpha = 10^{-3}$.\n- Case $4$ (discrete distribution, expected deviation):\n  - Distribution: Poisson with mean $\\lambda = 4$.\n  - Sample size: $n = 5000$.\n  - Quantization step: $\\Delta = \\varnothing$.\n  - Rounding of $U$: $d = \\varnothing$.\n  - Significance level: $\\alpha = 10^{-3}$.\n- Case $5$ (continuous, bounded support, no finite precision constraints, happy path):\n  - Distribution: Beta with parameters $a = 2$, $b = 5$.\n  - Sample size: $n = 3000$.\n  - Quantization step: $\\Delta = \\varnothing$.\n  - Rounding of $U$: $d = \\varnothing$.\n  - Significance level: $\\alpha = 10^{-3}$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the boolean results for the five test cases in the order listed above as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\text{True},\\text{True},\\text{False}]$.\n\nNo external input is allowed. No physical units are involved in this computation. Angles are not involved. All intermediate and final computations must be in mathematically consistent real numbers with the constraints and parameters specified above. The program must adhere to deterministic execution and the exact output format specified.",
            "solution": "The foundational definitions and facts are as follows. The cumulative distribution function (CDF) for a real-valued random variable $X$ is $F_X(x) = \\mathbb{P}(X \\le x)$. For any continuous distribution with strictly increasing $F_X$, the probability integral transform states that $U = F_X(X)$ is distributed as $\\text{Uniform}(0,1)$. This follows because the event $\\{U \\le u\\}$ is equivalent to $\\{F_X(X) \\le u\\}$, and since $F_X$ is strictly increasing and continuous, this is $\\{X \\le F_X^{-1}(u)\\}$, which has probability $F_X(F_X^{-1}(u)) = u$ for $u \\in [0,1]$, precisely the cumulative distribution function of a uniform distribution on $[0,1]$.\n\nFinite precision constraints, such as quantization of $X$ to a grid of step size $\\Delta$ or rounding the transformed value $U$ to a fixed number of decimal places $d$, break the strict monotonic mapping used in the probability integral transform. If we quantize $X$ via $X^{\\Delta} = \\Delta \\cdot \\mathrm{round}\\!\\left(\\frac{X}{\\Delta}\\right)$, then multiple pre-images in the continuous sample space collapse to the same quantized value, and the mapping $u \\mapsto F_X(X^{\\Delta})$ becomes step-like. Similarly, rounding $U$ to $d$ decimals creates a discrete set of possible values for $U$, which deviates from the continuous uniform distribution used as the reference in the test.\n\nFor discrete distributions (such as a Poisson distribution), the cumulative distribution function $F_X$ has jumps. If we compute $U = F_X(X)$ for a discrete $X$ as generated by a quantile function $F_X^{-1}$ (which, for discrete distributions, returns integer-valued quantiles), the values of $U$ are concentrated on the jump points and produce a discrete distribution that is not exactly uniform on $[0,1]$. In such cases, the general probability integral transform theorem requires a randomized version to achieve exact uniformity, namely sampling $U$ uniformly within the interval $\\big[F_X(x^-), F_X(x)\\big]$ for each realized $x$, where $F_X(x^-)$ denotes the left limit of the CDF. Since we are deliberately not using this randomized variant, we expect deviations from uniformity for discrete distributions.\n\nTo verify uniformity, we use the Kolmogorov-Smirnov (KS) test, a nonparametric test comparing the empirical distribution function $\\hat{F}_n(u)$ of the sample to a specified reference cumulative distribution function $F_U(u)$. The test statistic is the supremum of the absolute difference $D_n = \\sup_{u \\in [0,1]} \\big|\\hat{F}_n(u) - F_U(u)\\big|$. The KS test returns a $p$-value corresponding to the probability, under the null hypothesis, that the observed or a more extreme $D_n$ would occur. The decision rule is: accept the null hypothesis that the sample is $\\text{Uniform}(0,1)$ if the returned $p$-value is at least the significance level $\\alpha$; otherwise reject. This test is used here because it directly measures the maximal discrepancy in cumulative distributions, which aligns with the CDF-based construction $U = F_X(X)$.\n\nAlgorithmic design following the principles:\n- Deterministic base sequence: For each case, construct $U^{\\text{base}}_i = \\frac{i + 0.5}{n}$ for $i = 0, 1, \\ldots, n-1$. This low-discrepancy choice minimizes the supremum deviation of the empirical distribution from the true $\\text{Uniform}(0,1)$ and ensures deterministic behavior.\n- Sampling via inverse CDF: Use $X_i = F_X^{-1}(U^{\\text{base}}_i)$, where $F_X^{-1}$ is the quantile function of the specified distribution. For continuous distributions, if there is no quantization and no rounding, the mapping $U_i = F_X(X_i)$ returns exactly $U^{\\text{base}}_i$ under ideal arithmetic, so the KS test should accept uniformity at reasonable $\\alpha$.\n- Finite precision constraints:\n  - If a quantization step $\\Delta$ is provided, set $X_i^{\\Delta} = \\Delta \\cdot \\mathrm{round}\\!\\left(\\frac{X_i}{\\Delta}\\right)$ and compute $U_i = F_X(X_i^{\\Delta})$. This step-like behavior introduces ties and deviations that the KS test should detect, especially with larger $n$ and coarser $\\Delta$.\n  - If rounding $U$ is requested to $d$ decimals, compute $U_i^{(d)} = \\mathrm{round}(U_i, d)$ and then use $U_i^{(d)}$ as the observed values in the KS test. This creates a discrete set of $U$ values and should lead to rejection at typical $\\alpha$ for sufficiently large $n$.\n- Testing: Perform the one-sample KS test against the $\\text{Uniform}(0,1)$ reference at the specified $\\alpha$ and record the boolean $p \\ge \\alpha$.\n\nCoverage of the test suite:\n- Case $1$ uses a continuous distribution with no finite precision constraints, probing the happy path of the probability integral transform.\n- Case $2$ introduces coarse quantization of $X$ for a continuous distribution, probing the effect of finite precision on the transform.\n- Case $3$ introduces rounding on $U$ for a continuous distribution, probing a different finite precision constraint.\n- Case $4$ uses a discrete distribution, probing the limitation of the non-randomized transform for distributions with jump discontinuities.\n- Case $5$ uses a continuous distribution with bounded support and no constraints, probing the happy path in a different distribution family.\n\nThe program implements all of the above deterministically and outputs a single-line list of booleans in the specified order, indicating whether the KS test accepts uniformity in each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef base_uniform_grid(n: int) -> np.ndarray:\n    \"\"\"\n    Construct a deterministic base sequence U_base in (0,1):\n    U_i = (i + 0.5) / n for i = 0..n-1.\n    \"\"\"\n    i = np.arange(n, dtype=np.float64)\n    return (i + 0.5) / n\n\ndef sample_via_ppf(distribution: str, params: dict, u: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Sample X via the inverse CDF (quantile function) for the given distribution.\n    Uses scipy.stats distributions.\n    \"\"\"\n    if distribution == \"normal\":\n        # params: mu (loc), sigma (scale)\n        dist = stats.norm(loc=params[\"mu\"], scale=params[\"sigma\"])\n        x = dist.ppf(u)\n    elif distribution == \"exponential\":\n        # params: lambda (rate). SciPy expon uses scale = 1/lambda.\n        scale = 1.0 / params[\"lambda\"]\n        dist = stats.expon(scale=scale)\n        x = dist.ppf(u)\n    elif distribution == \"beta\":\n        # params: a, b\n        dist = stats.beta(a=params[\"a\"], b=params[\"b\"])\n        x = dist.ppf(u)\n    elif distribution == \"poisson\":\n        # params: lambda (mean)\n        # For discrete distributions, ppf returns integer-valued quantiles.\n        dist = stats.poisson(mu=params[\"lambda\"])\n        x = dist.ppf(u).astype(np.float64)  # keep as float for uniform processing\n    else:\n        raise ValueError(f\"Unsupported distribution: {distribution}\")\n    return x\n\ndef cdf_values(distribution: str, params: dict, x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute U = F_X(x) for the given distribution and array x.\n    \"\"\"\n    if distribution == \"normal\":\n        dist = stats.norm(loc=params[\"mu\"], scale=params[\"sigma\"])\n        return dist.cdf(x)\n    elif distribution == \"exponential\":\n        scale = 1.0 / params[\"lambda\"]\n        dist = stats.expon(scale=scale)\n        return dist.cdf(x)\n    elif distribution == \"beta\":\n        dist = stats.beta(a=params[\"a\"], b=params[\"b\"])\n        return dist.cdf(x)\n    elif distribution == \"poisson\":\n        dist = stats.poisson(mu=params[\"lambda\"])\n        return dist.cdf(x)\n    else:\n        raise ValueError(f\"Unsupported distribution: {distribution}\")\n\ndef apply_quantization(x: np.ndarray, delta: float | None) -> np.ndarray:\n    \"\"\"\n    If delta is provided, quantize x to the nearest multiple of delta.\n    \"\"\"\n    if delta is None:\n        return x\n    # Avoid issues with extremely small delta: assume meaningful positive delta\n    return np.round(x / delta) * delta\n\ndef round_u(u: np.ndarray, digits: int | None) -> np.ndarray:\n    \"\"\"\n    If digits is provided, round u to the given number of decimal places and clip to [0,1].\n    \"\"\"\n    if digits is None:\n        return u\n    u_rounded = np.round(u, digits)\n    # Clip to the valid support\n    return np.clip(u_rounded, 0.0, 1.0)\n\ndef ks_accept_uniform(u: np.ndarray, alpha: float) -> bool:\n    \"\"\"\n    Perform one-sample KS test against Uniform(0,1). Return True if p >= alpha, else False.\n    \"\"\"\n    # scipy.stats.kstest with 'uniform' defaults to loc=0, scale=1\n    statistic, p_value = stats.kstest(u, 'uniform')\n    return p_value >= alpha\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a dict with fields:\n    # - distribution: str\n    # - params: dict of distribution parameters\n    # - n: int sample size\n    # - delta: float or None for X quantization step\n    # - round_u_digits: int or None for rounding U\n    # - alpha: float significance level\n    test_cases = [\n        {\n            \"distribution\": \"normal\",\n            \"params\": {\"mu\": 0.0, \"sigma\": 1.0},\n            \"n\": 2000,\n            \"delta\": None,\n            \"round_u_digits\": None,\n            \"alpha\": 1e-3,\n        },\n        {\n            \"distribution\": \"normal\",\n            \"params\": {\"mu\": 0.0, \"sigma\": 1.0},\n            \"n\": 4000,\n            \"delta\": 0.5,\n            \"round_u_digits\": None,\n            \"alpha\": 1e-3,\n        },\n        {\n            \"distribution\": \"exponential\",\n            \"params\": {\"lambda\": 1.0},\n            \"n\": 3000,\n            \"delta\": None,\n            \"round_u_digits\": 2,\n            \"alpha\": 1e-3,\n        },\n        {\n            \"distribution\": \"poisson\",\n            \"params\": {\"lambda\": 4.0},\n            \"n\": 5000,\n            \"delta\": None,\n            \"round_u_digits\": None,\n            \"alpha\": 1e-3,\n        },\n        {\n            \"distribution\": \"beta\",\n            \"params\": {\"a\": 2.0, \"b\": 5.0},\n            \"n\": 3000,\n            \"delta\": None,\n            \"round_u_digits\": None,\n            \"alpha\": 1e-3,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Deterministic base uniform sequence\n        u_base = base_uniform_grid(case[\"n\"])\n        # Sample X via inverse CDF\n        x = sample_via_ppf(case[\"distribution\"], case[\"params\"], u_base)\n        # Apply finite precision quantization to X, if any\n        xq = apply_quantization(x, case[\"delta\"])\n        # Compute U = F_X(Xq)\n        u = cdf_values(case[\"distribution\"], case[\"params\"], xq)\n        # Apply rounding to U, if requested\n        u_final = round_u(u, case[\"round_u_digits\"])\n        # Perform KS test\n        accept = ks_accept_uniform(u_final, case[\"alpha\"])\n        results.append(accept)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}