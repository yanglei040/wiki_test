{
    "hands_on_practices": [
        {
            "introduction": "Understanding a concept from first principles is a cornerstone of scientific proficiency. This exercise  challenges you to derive the familiar formulas for the matrix $1$-norm and $\\infty$-norm directly from the definition of an induced operator norm. This practice reinforces the theoretical foundations of matrix norms and then extends these ideas to block matrices, a common structure in large-scale scientific computing, to see how the norm of a whole relates to the norms of its parts.",
            "id": "3158838",
            "problem": "Consider the block matrix $A \\in \\mathbb{R}^{4 \\times 4}$ partitioned into $2 \\times 2$ blocks,\n$$\nA \\;=\\; \\begin{pmatrix}\nB  C \\\\\nD  E\n\\end{pmatrix},\n\\quad\nB \\;=\\; \\begin{pmatrix} 1  -2 \\\\ 3  0 \\end{pmatrix},\\;\nC \\;=\\; \\begin{pmatrix} -1  4 \\\\ 2  -3 \\end{pmatrix},\\;\nD \\;=\\; \\begin{pmatrix} 0  5 \\\\ -4  1 \\end{pmatrix},\\;\nE \\;=\\; \\begin{pmatrix} 2  -1 \\\\ 0  3 \\end{pmatrix}.\n$$\nFor a vector $x \\in \\mathbb{R}^{n}$, the vector $1$-norm is defined by $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$ and the vector $\\infty$-norm is defined by $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$. For a matrix $A \\in \\mathbb{R}^{n \\times n}$ and a given vector norm $\\|\\cdot\\|$, the induced operator norm (also called subordinate norm) is\n$$\n\\|A\\| \\;=\\; \\sup_{x \\neq 0} \\frac{\\|A x\\|}{\\|x\\|}.\n$$\nTasks:\n- Using only these core definitions and the triangle inequality, derive expressions that allow you to compute $\\|A\\|_{1}$ and $\\|A\\|_{\\infty}$ exactly, and then compute their numerical values for the matrix $A$ given above.\n- Let $x \\in \\mathbb{R}^{4}$ be partitioned conformally with the block structure of $A$ as $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$ with $x_{1}, x_{2} \\in \\mathbb{R}^{2}$. Explain, starting from the same core definitions and basic inequalities, how the block structure of $A$ leads to induced norm bounds on $\\|A x\\|_{1}$ and $\\|A x\\|_{\\infty}$ in terms of the induced norms of the blocks $B, C, D, E$ and the norms of $x_{1}, x_{2}$. Illustrate your explanation by forming a $2 \\times 2$ scalar matrix whose entries are induced norms of the blocks and show how its induced norm bounds $\\|A x\\|$.\n\nProvide exact values for $\\|A\\|_{1}$ and $\\|A\\|_{\\infty}$. No rounding is required.",
            "solution": "The problem statement has been validated and is deemed sound. It is a well-posed problem in introductory computational science, specifically within the topic of matrix algebra and norms. All definitions are standard, and the required tasks are clear and mathematically formalizable.\n\nThe full matrix $A \\in \\mathbb{R}^{4 \\times 4}$ is constructed by assembling its constituent blocks:\n$$\nA \\;=\\; \\begin{pmatrix}\nB  C \\\\\nD  E\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n1  -2  -1  4 \\\\\n3  0  2  -3 \\\\\n0  5  2  -1 \\\\\n-4  1  0  3\n\\end{pmatrix}.\n$$\n\n### Derivation and Computation of the Matrix $1$-norm, $\\|A\\|_{1}$\n\nThe induced matrix $1$-norm is defined as $\\|A\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}$. Let $y = Ax$. The $i$-th component of $y$ is $y_i = \\sum_{j=1}^{4} A_{ij} x_j$.\nThe vector $1$-norm of $y=Ax$ is given by $\\|Ax\\|_{1} = \\sum_{i=1}^{4} |y_i| = \\sum_{i=1}^{4} \\left| \\sum_{j=1}^{4} A_{ij} x_j \\right|$.\nApplying the triangle inequality for sums of scalars, $|\\sum_j a_j| \\le \\sum_j |a_j|$, we get:\n$$\n\\|Ax\\|_{1} \\le \\sum_{i=1}^{4} \\sum_{j=1}^{4} |A_{ij} x_j| = \\sum_{i=1}^{4} \\sum_{j=1}^{4} |A_{ij}| |x_j|.\n$$\nBy changing the order of summation, we can group terms by $|x_j|$:\n$$\n\\|Ax\\|_{1} \\le \\sum_{j=1}^{4} \\left( |x_j| \\sum_{i=1}^{4} |A_{ij}| \\right).\n$$\nLet $C_j = \\sum_{i=1}^{4} |A_{ij}|$ be the sum of the absolute values of the entries in the $j$-th column of $A$. Let $C_{\\max} = \\max_{1 \\le j \\le 4} C_j$ be the maximum absolute column sum. We can bound each $C_j$ by $C_{\\max}$:\n$$\n\\|Ax\\|_{1} \\le \\sum_{j=1}^{4} |x_j| C_j \\le \\sum_{j=1}^{4} |x_j| C_{\\max} = C_{\\max} \\sum_{j=1}^{4} |x_j| = C_{\\max} \\|x\\|_{1}.\n$$\nThis implies that for any non-zero $x$, $\\frac{\\|A x\\|_{1}}{\\|x\\|_{1}} \\le C_{\\max}$, and therefore $\\|A\\|_{1} \\le C_{\\max}$.\n\nTo show that this bound is achieved, we must find a vector $x$ for which the equality holds. Let $k$ be the index of the column with the maximum absolute sum, such that $C_k = C_{\\max}$. Consider the standard basis vector $x = e_k$, where $(e_k)_j = \\delta_{kj}$. For this vector, $\\|x\\|_{1} = \\sum_{j=1}^{4} |(e_k)_j| = 1$. The product $Ax = Ae_k$ is the $k$-th column of $A$.\nThe $1$-norm of this product is:\n$$\n\\|Ae_k\\|_{1} = \\sum_{i=1}^{4} |(Ae_k)_i| = \\sum_{i=1}^{4} |A_{ik}| = C_k = C_{\\max}.\n$$\nFor this specific choice of $x$, we have $\\frac{\\|A x\\|_{1}}{\\|x\\|_{1}} = \\frac{C_{\\max}}{1} = C_{\\max}$. Since we found a vector that achieves the upper bound, the supremum must be this value. Thus, the expression for the matrix $1$-norm is the maximum absolute column sum:\n$$\n\\|A\\|_{1} = \\max_{1 \\le j \\le 4} \\sum_{i=1}^{4} |A_{ij}|.\n$$\nFor the given matrix $A$, the absolute column sums are:\n\\begin{itemize}\n    \\item Column $1$: $|1| + |3| + |0| + |-4| = 1 + 3 + 0 + 4 = 8$.\n    \\item Column $2$: $|-2| + |0| + |5| + |1| = 2 + 0 + 5 + 1 = 8$.\n    \\item Column $3$: $|-1| + |2| + |2| + |0| = 1 + 2 + 2 + 0 = 5$.\n    \\item Column $4$: $|4| + |-3| + |-1| + |3| = 4 + 3 + 1 + 3 = 11$.\n\\end{itemize}\nThe maximum of these values is $11$. Therefore, $\\|A\\|_{1} = 11$.\n\n### Derivation and Computation of the Matrix $\\infty$-norm, $\\|A\\|_{\\infty}$\n\nThe induced matrix $\\infty$-norm is defined as $\\|A\\|_{\\infty} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{\\infty}}{\\|x\\|_{\\infty}}$. The vector $\\infty$-norm of $y=Ax$ is $\\|Ax\\|_{\\infty} = \\max_{1 \\le i \\le 4} |y_i| = \\max_{1 \\le i \\le 4} \\left| \\sum_{j=1}^{4} A_{ij} x_j \\right|$.\nFor any specific row $i$, we apply the triangle inequality:\n$$\n\\left| \\sum_{j=1}^{4} A_{ij} x_j \\right| \\le \\sum_{j=1}^{4} |A_{ij} x_j| = \\sum_{j=1}^{4} |A_{ij}| |x_j|.\n$$\nBy definition, $|x_j| \\le \\max_{1 \\le k \\le 4} |x_k| = \\|x\\|_{\\infty}$ for all $j$. Thus:\n$$\n\\left| \\sum_{j=1}^{4} A_{ij} x_j \\right| \\le \\sum_{j=1}^{4} |A_{ij}| \\|x\\|_{\\infty} = \\|x\\|_{\\infty} \\sum_{j=1}^{4} |A_{ij}|.\n$$\nLet $R_i = \\sum_{j=1}^{4} |A_{ij}|$ be the sum of the absolute values of the entries in the $i$-th row. Then $|(Ax)_i| \\le R_i \\|x\\|_{\\infty}$. Since this holds for all $i$, it must also hold for the maximum component:\n$$\n\\|Ax\\|_{\\infty} = \\max_{1 \\le i \\le 4} |(Ax)_i| \\le \\max_{1 \\le i \\le 4} (R_i \\|x\\|_{\\infty}) = \\left(\\max_{1 \\le i \\le 4} R_i\\right) \\|x\\|_{\\infty}.\n$$\nLet $R_{\\max} = \\max_{1 \\le i \\le 4} R_i$. Then $\\frac{\\|A x\\|_{\\infty}}{\\|x\\|_{\\infty}} \\le R_{\\max}$, which implies $\\|A\\|_{\\infty} \\le R_{\\max}$.\n\nTo show this bound is achieved, let $k$ be the index of the row with the maximum absolute sum, $R_k = R_{\\max}$. We construct a vector $x \\in \\mathbb{R}^4$ with components $x_j = \\text{sgn}(A_{kj})$, where $\\text{sgn}(z) = 1$ if $z0$, $-1$ if $z0$, and we can define $\\text{sgn}(0)=1$ for simplicity. For this vector $x$, all components have absolute value $|x_j| \\le 1$, and at least one component has absolute value $1$ (unless the $k$-th row is all zeros, a trivial case). Therefore, $\\|x\\|_{\\infty} = 1$.\nNow, consider the $k$-th component of the product $Ax$:\n$$\n(Ax)_k = \\sum_{j=1}^{4} A_{kj} x_j = \\sum_{j=1}^{4} A_{kj} \\text{sgn}(A_{kj}) = \\sum_{j=1}^{4} |A_{kj}| = R_k = R_{\\max}.\n$$\nThe $\\infty$-norm of $Ax$ must be at least the absolute value of its $k$-th component:\n$$\n\\|Ax\\|_{\\infty} = \\max_{1 \\le i \\le 4} |(Ax)_i| \\ge |(Ax)_k| = R_{\\max}.\n$$\nCombined with the earlier inequality $\\|Ax\\|_{\\infty} \\le R_{\\max}\\|x\\|_{\\infty} = R_{\\max}$ (since $\\|x\\|_\\infty=1$), we must have $\\|Ax\\|_{\\infty} = R_{\\max}$.\nFor this specific $x$, $\\frac{\\|A x\\|_{\\infty}}{\\|x\\|_{\\infty}} = \\frac{R_{\\max}}{1} = R_{\\max}$. The supremum is therefore $R_{\\max}$. The expression for the matrix $\\infty$-norm is the maximum absolute row sum:\n$$\n\\|A\\|_{\\infty} = \\max_{1 \\le i \\le 4} \\sum_{j=1}^{4} |A_{ij}|.\n$$\nFor the given matrix $A$, the absolute row sums are:\n\\begin{itemize}\n    \\item Row $1$: $|1| + |-2| + |-1| + |4| = 1 + 2 + 1 + 4 = 8$.\n    \\item Row $2$: $|3| + |0| + |2| + |-3| = 3 + 0 + 2 + 3 = 8$.\n    \\item Row $3$: $|0| + |5| + |2| + |-1| = 0 + 5 + 2 + 1 = 8$.\n    \\item Row $4$: $|-4| + |1| + |0| + |3| = 4 + 1 + 0 + 3 = 8$.\n\\end{itemize}\nAll absolute row sums are equal to $8$. The maximum is $8$. Therefore, $\\|A\\|_{\\infty} = 8$.\n\n### Block Matrix Norm Bounds\n\nLet the vector $x \\in \\mathbb{R}^4$ be partitioned as $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, where $x_1, x_2 \\in \\mathbb{R}^2$. The matrix-vector product $Ax$ can be written in block form:\n$$\nAx = \\begin{pmatrix} B  C \\\\ D  E \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} Bx_1 + Cx_2 \\\\ Dx_1 + Ex_2 \\end{pmatrix}.\n$$\nWe analyze the $1$-norm and $\\infty$-norm of $Ax$. First, we compute the norms of the individual blocks using the formulas derived above.\n\\begin{itemize}\n    \\item $\\|B\\|_{1} = \\max(|1|+|3|, |-2|+|0|) = 4$.\n    \\item $\\|C\\|_{1} = \\max(|-1|+|2|, |4|+|-3|) = 7$.\n    \\item $\\|D\\|_{1} = \\max(|0|+|-4|, |5|+|1|) = 6$.\n    \\item $\\|E\\|_{1} = \\max(|2|+|0|, |-1|+|3|) = 4$.\n\\end{itemize}\n\\begin{itemize}\n    \\item $\\|B\\|_{\\infty} = \\max(|1|+|-2|, |3|+|0|) = 3$.\n    \\item $\\|C\\|_{\\infty} = \\max(|-1|+|4|, |2|+|-3|) = 5$.\n    \\item $\\|D\\|_{\\infty} = \\max(|0|+|5|, |-4|+|1|) = 5$.\n    \\item $\\|E\\|_{\\infty} = \\max(|2|+|-1|, |0|+|3|) = 3$.\n\\end{itemize}\n\n**Bound for $\\|Ax\\|_{\\infty}$:**\nThe vector $\\infty$-norm of a block vector $\\begin{pmatrix} u \\\\ v \\end{pmatrix}$ is $\\max(\\|u\\|_{\\infty}, \\|v\\|_{\\infty})$.\n$$\n\\|Ax\\|_{\\infty} = \\left\\| \\begin{pmatrix} Bx_1 + Cx_2 \\\\ Dx_1 + Ex_2 \\end{pmatrix} \\right\\|_{\\infty} = \\max\\left( \\|Bx_1 + Cx_2\\|_{\\infty}, \\|Dx_1 + Ex_2\\|_{\\infty} \\right).\n$$\nUsing the triangle inequality and the definition of induced norms for each block:\n\\begin{align*}\n\\|Bx_1 + Cx_2\\|_{\\infty} \\le \\|Bx_1\\|_{\\infty} + \\|Cx_2\\|_{\\infty} \\le \\|B\\|_{\\infty}\\|x_1\\|_{\\infty} + \\|C\\|_{\\infty}\\|x_2\\|_{\\infty}. \\\\\n\\|Dx_1 + Ex_2\\|_{\\infty} \\le \\|Dx_1\\|_{\\infty} + \\|Ex_2\\|_{\\infty} \\le \\|D\\|_{\\infty}\\|x_1\\|_{\\infty} + \\|E\\|_{\\infty}\\|x_2\\|_{\\infty}.\n\\end{align*}\nCombining these, we get:\n$$\n\\|Ax\\|_{\\infty} \\le \\max\\left( \\|B\\|_{\\infty}\\|x_1\\|_{\\infty} + \\|C\\|_{\\infty}\\|x_2\\|_{\\infty}, \\|D\\|_{\\infty}\\|x_1\\|_{\\infty} + \\|E\\|_{\\infty}\\|x_2\\|_{\\infty} \\right).\n$$\nThis expression can be related to a $2 \\times 2$ matrix of norms. Let $\\mathcal{A}_{\\infty} = \\begin{pmatrix} \\|B\\|_{\\infty}  \\|C\\|_{\\infty} \\\\ \\|D\\|_{\\infty}  \\|E\\|_{\\infty} \\end{pmatrix}$ and $\\chi_{\\infty} = \\begin{pmatrix} \\|x_1\\|_{\\infty} \\\\ \\|x_2\\|_{\\infty} \\end{pmatrix}$. The right side of the inequality is precisely the $\\infty$-norm of the product $\\mathcal{A}_{\\infty}\\chi_{\\infty}$.\n$$\n\\|Ax\\|_{\\infty} \\le \\|\\mathcal{A}_{\\infty}\\chi_{\\infty}\\|_{\\infty}.\n$$\nUsing the induced norm property for $\\mathcal{A}_{\\infty}$, we have $\\|\\mathcal{A}_{\\infty}\\chi_{\\infty}\\|_{\\infty} \\le \\|\\mathcal{A}_{\\infty}\\|_{\\infty}\\|\\chi_{\\infty}\\|_{\\infty}$. Furthermore, $\\|\\chi_{\\infty}\\|_{\\infty} = \\max(\\|x_1\\|_{\\infty}, \\|x_2\\|_{\\infty}) = \\|x\\|_{\\infty}$. This gives the final bound on $\\|Ax\\|_{\\infty}$:\n$$\n\\|Ax\\|_{\\infty} \\le \\|\\mathcal{A}_{\\infty}\\|_{\\infty} \\|x\\|_{\\infty}.\n$$\nThis further implies a bound on the matrix norm itself: $\\|A\\|_{\\infty} \\le \\|\\mathcal{A}_{\\infty}\\|_{\\infty}$. For our specific matrix, $\\mathcal{A}_{\\infty} = \\begin{pmatrix} 3  5 \\\\ 5  3 \\end{pmatrix}$. Its $\\infty$-norm is $\\|\\mathcal{A}_{\\infty}\\|_{\\infty} = \\max(3+5, 5+3) = 8$. Thus, $\\|A\\|_{\\infty} \\le 8$, which is consistent with our exact calculation.\n\n**Bound for $\\|Ax\\|_{1}$:**\nThe vector $1$-norm of a block vector $\\begin{pmatrix} u \\\\ v \\end{pmatrix}$ is $\\|u\\|_{1} + \\|v\\|_{1}$.\n$$\n\\|Ax\\|_{1} = \\left\\| \\begin{pmatrix} Bx_1 + Cx_2 \\\\ Dx_1 + Ex_2 \\end{pmatrix} \\right\\|_{1} = \\|Bx_1 + Cx_2\\|_{1} + \\|Dx_1 + Ex_2\\|_{1}.\n$$\nUsing the triangle inequality and the definition of induced norms:\n$$\n\\|Ax\\|_{1} \\le (\\|Bx_1\\|_{1} + \\|Cx_2\\|_{1}) + (\\|Dx_1\\|_{1} + \\|Ex_2\\|_{1}) \\le (\\|B\\|_{1}\\|x_1\\|_{1} + \\|C\\|_{1}\\|x_2\\|_{1}) + (\\|D\\|_{1}\\|x_1\\|_{1} + \\|E\\|_{1}\\|x_2\\|_{1}).\n$$\nLet $\\mathcal{A}_{1} = \\begin{pmatrix} \\|B\\|_{1}  \\|C\\|_{1} \\\\ \\|D\\|_{1}  \\|E\\|_{1} \\end{pmatrix}$ and $\\chi_{1} = \\begin{pmatrix} \\|x_1\\|_{1} \\\\ \\|x_2\\|_{1} \\end{pmatrix}$. The right-hand side can be recognized as the $1$-norm of the product $\\mathcal{A}_{1}\\chi_{1}$, since all quantities $\\|B\\|_{1}, \\|x_1\\|_{1}$, etc., are non-negative.\n$$\n\\|Ax\\|_{1} \\le (\\|B\\|_{1}\\|x_1\\|_{1} + \\|C\\|_{1}\\|x_2\\|_{1}) + (\\|D\\|_{1}\\|x_1\\|_{1} + \\|E\\|_{1}\\|x_2\\|_{1}) = \\|\\mathcal{A}_{1}\\chi_{1}\\|_{1}.\n$$\nUsing the induced norm property for $\\mathcal{A}_{1}$, we have $\\|\\mathcal{A}_{1}\\chi_{1}\\|_{1} \\le \\|\\mathcal{A}_{1}\\|_{1}\\|\\chi_{1}\\|_{1}$. Also, $\\|\\chi_{1}\\|_{1} = \\|x_1\\|_{1} + \\|x_2\\|_{1} = \\|x\\|_{1}$. This gives the bound:\n$$\n\\|Ax\\|_{1} \\le \\|\\mathcal{A}_{1}\\|_{1} \\|x\\|_{1}.\n$$\nThis implies the matrix norm bound $\\|A\\|_{1} \\le \\|\\mathcal{A}_{1}\\|_{1}$. For our specific matrix, $\\mathcal{A}_{1} = \\begin{pmatrix} 4  7 \\\\ 6  4 \\end{pmatrix}$. Its $1$-norm is $\\|\\mathcal{A}_{1}\\|_{1} = \\max(4+6, 7+4) = \\max(10, 11) = 11$. Thus, $\\|A\\|_{1} \\le 11$, which is also consistent with our exact calculation.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11  8\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In computational science, we often use inequalities to bound a quantity that is difficult to compute (like the spectral norm, $\\lVert A \\rVert_2$) with quantities that are easier to compute (like $\\lVert A \\rVert_1$ and $\\lVert A \\rVert_\\infty$). This exercise  delves into the well-known inequality $\\lVert A \\rVert_2 \\le \\sqrt{\\lVert A \\rVert_1 \\lVert A \\rVert_\\infty}$ by asking you to analyze its 'tightness' for different types of matrices. By exploring cases where the bound is exact versus where it is loose, you will develop critical intuition about the reliability and limitations of such theoretical bounds in practice.",
            "id": "3158812",
            "problem": "In introduction to computational science, matrix norms quantify how linear transformations amplify vectors measured in different ways. For a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $x \\in \\mathbb{R}^{n}$, the induced matrix norm from a vector norm $\\|\\cdot\\|$ is defined by $\\|A\\| = \\sup_{x \\neq 0} \\frac{\\|Ax\\|}{\\|x\\|}$. Consider the following three induced norms: the $1$-norm ($\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$), the infinity norm ($\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$), and the $2$-norm ($\\|x\\|_{2} = \\sqrt{\\sum_{i=1}^{n} x_{i}^{2}}$). The corresponding induced matrix norms are denoted by $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$, and $\\|A\\|_{2}$. Define the tightness ratio\n$$\nR(A) \\;=\\; \\frac{\\|A\\|_{2}}{\\sqrt{\\|A\\|_{1}\\,\\|A\\|_{\\infty}}}.\n$$\nStarting from the definitions above and standard facts about eigenvalues of $A^{\\top}A$, compute the exact values of $R(A)$ for the following two matrices:\n\n1. Let $E \\in \\mathbb{R}^{m \\times n}$ have a single nonzero entry $E_{11} = s$ with $s0$ and all other entries equal to $0$. Compute $R(E)$.\n\n2. Let $H_{n} \\in \\mathbb{R}^{n \\times n}$ be a matrix whose $n$ columns are pairwise orthogonal and every entry is either $+1$ or $-1$. Compute $R(H_{n})$ in closed form as a function of $n$, and then evaluate this ratio for $n=16$.\n\nReport your final answers for the ordered pair $\\left(R(E),\\,R(H_{16})\\right)$ as exact values. No rounding is required.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- A real matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $x \\in \\mathbb{R}^{n}$.\n- Induced matrix norm definition: $\\|A\\| = \\sup_{x \\neq 0} \\frac{\\|Ax\\|}{\\|x\\|}$.\n- Vector $1$-norm: $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$.\n- Vector infinity-norm: $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$.\n- Vector $2$-norm: $\\|x\\|_{2} = \\sqrt{\\sum_{i=1}^{n} x_{i}^{2}}$.\n- Corresponding induced matrix norms: $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$, and $\\|A\\|_{2}$.\n- Tightness ratio definition: $R(A) = \\frac{\\|A\\|_{2}}{\\sqrt{\\|A\\|_{1}\\,\\|A\\|_{\\infty}}}$.\n- Matrix $E \\in \\mathbb{R}^{m \\times n}$ with a single nonzero entry $E_{11} = s$, where $s0$, and all other entries are $0$.\n- Matrix $H_{n} \\in \\mathbb{R}^{n \\times n}$ whose $n$ columns are pairwise orthogonal and every entry is either $+1$ or $-1$.\n- The task is to compute the exact values of $R(E)$ and $R(H_{n})$ as a function of $n$, evaluate the latter for $n=16$, and report the ordered pair $\\left(R(E),\\,R(H_{16})\\right)$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is based on standard, correct definitions from linear algebra and numerical analysis concerning vector and matrix norms. The matrices described are well-defined mathematical objects. For matrix $H_n$, the properties describe a scaled Hadamard matrix; such matrices are known to exist for orders $n$ that are powers of $2$, including $n=16$. The problem is mathematically and scientifically sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is a formal exercise in matrix algebra and is directly relevant to the topic of matrix norms in computational science.\n3.  **Incomplete or Contradictory Setup**: The definitions and properties provided for both matrices $E$ and $H_n$ are sufficient and consistent for computing the required norms. There are no missing or contradictory conditions.\n4.  **Unrealistic or Infeasible**: The problem is purely mathematical. The conditions are mathematically consistent and feasible.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed, with clear definitions and objectives that lead to a unique, determinable solution.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a solid understanding of matrix norm properties and their calculation. It is a substantive exercise, neither trivial nor artificially complicated.\n7.  **Outside Scientific Verifiability**: The results are derivable and verifiable through standard mathematical proofs.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n***\n\nThe solution proceeds by calculating the required norms for each matrix and then computing the tightness ratio $R(A)$.\n\n**Part 1: Calculation for Matrix $E$**\n\nThe matrix $E \\in \\mathbb{R}^{m \\times n}$ has only one non-zero entry, $E_{11} = s  0$. All other entries $E_{ij}$ are $0$.\n\n1.  **Calculation of $\\|E\\|_{1}$**: The induced $1$-norm of a matrix is the maximum absolute column sum.\n    $$\n    \\|E\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |E_{ij}|\n    $$\n    The sum of absolute values for the first column ($j=1$) is $\\sum_{i=1}^{m} |E_{i1}| = |E_{11}| + \\sum_{i=2}^{m} |E_{i1}| = |s| + 0 = s$, since $s0$.\n    For any other column ($j  1$), the sum is $\\sum_{i=1}^{m} |E_{ij}| = 0$, as all entries are $0$.\n    The maximum of these column sums is $s$. Thus, $\\|E\\|_{1} = s$.\n\n2.  **Calculation of $\\|E\\|_{\\infty}$**: The induced $\\infty$-norm of a matrix is the maximum absolute row sum.\n    $$\n    \\|E\\|_{\\infty} = \\max_{1 \\le i \\le m} \\sum_{j=1}^{n} |E_{ij}|\n    $$\n    The sum of absolute values for the first row ($i=1$) is $\\sum_{j=1}^{n} |E_{1j}| = |E_{11}| + \\sum_{j=2}^{n} |E_{1j}| = |s| + 0 = s$.\n    For any other row ($i  1$), the sum is $\\sum_{j=1}^{n} |E_{ij}| = 0$.\n    The maximum of these row sums is $s$. Thus, $\\|E\\|_{\\infty} = s$.\n\n3.  **Calculation of $\\|E\\|_{2}$**: The induced $2$-norm is the largest singular value of the matrix, which is the square root of the largest eigenvalue of $E^{\\top}E$. The matrix $E^{\\top}$ is an $n \\times m$ matrix with $(E^{\\top})_{11} = s$ and all other entries equal to $0$. Let's compute the product $E^{\\top}E \\in \\mathbb{R}^{n \\times n}$.\n    The entry $(k,l)$ of $E^{\\top}E$ is given by $(E^{\\top}E)_{kl} = \\sum_{i=1}^{m} (E^{\\top})_{ki}E_{il} = \\sum_{i=1}^{m} E_{ik}E_{il}$.\n    The only non-zero entry in $E$ is $E_{11}=s$. So, $E_{ik}$ is non-zero only if $i=1$ and $k=1$. Similarly, $E_{il}$ is non-zero only if $i=1$ and $l=1$.\n    Therefore, the sum is non-zero only if $k=1$ and $l=1$:\n    $(E^{\\top}E)_{11} = \\sum_{i=1}^{m} E_{i1}E_{i1} = (E_{11})^2 = s^2$.\n    All other entries $(E^{\\top}E)_{kl}$ are $0$.\n    So, $E^{\\top}E$ is a matrix with $s^2$ in the $(1,1)$ position and zeros everywhere else. The eigenvalues of this matrix are its diagonal entries, which are $s^2$ and $n-1$ zeros.\n    The maximum eigenvalue is $\\lambda_{\\max}(E^{\\top}E) = s^2$.\n    The $2$-norm is $\\|E\\|_{2} = \\sqrt{\\lambda_{\\max}(E^{\\top}E)} = \\sqrt{s^2} = s$ (since $s0$).\n\n4.  **Calculation of $R(E)$**:\n    $$\n    R(E) = \\frac{\\|E\\|_{2}}{\\sqrt{\\|E\\|_{1}\\,\\|E\\|_{\\infty}}} = \\frac{s}{\\sqrt{s \\cdot s}} = \\frac{s}{\\sqrt{s^2}} = \\frac{s}{s} = 1.\n    $$\n\n**Part 2: Calculation for Matrix $H_n$**\n\nThe matrix $H_{n} \\in \\mathbb{R}^{n \\times n}$ has pairwise orthogonal columns, and every entry $(H_n)_{ij}$ is either $+1$ or $-1$.\n\nLet $h_j$ be the $j$-th column of $H_n$. The given properties are:\n-   $h_i^{\\top}h_j = 0$ for $i \\neq j$.\n-   The entries of $H_n$ are $(H_n)_{ij} \\in \\{+1, -1\\}$.\n\nFrom the second property, the squared Euclidean norm of any column $h_j$ is:\n$h_j^{\\top}h_j = \\|h_j\\|^2_2 = \\sum_{i=1}^{n} ((H_n)_{ij})^2 = \\sum_{i=1}^{n} (\\pm 1)^2 = \\sum_{i=1}^{n} 1 = n$.\n\nCombining these properties, we can determine the matrix product $H_n^{\\top}H_n$. The $(i,j)$-th entry of this product is $h_i^{\\top}h_j$.\n$$\n(H_n^{\\top}H_n)_{ij} = h_i^{\\top}h_j = \\begin{cases} n,  \\text{if } i=j \\\\ 0,  \\text{if } i \\neq j \\end{cases}\n$$\nThis means $H_n^{\\top}H_n = n I_n$, where $I_n$ is the $n \\times n$ identity matrix.\n\n1.  **Calculation of $\\|H_n\\|_{1}$**: The maximum absolute column sum.\n    For any column $j$, the absolute column sum is $\\sum_{i=1}^{n} |(H_n)_{ij}|$. Since every entry is $\\pm 1$, its absolute value is $1$.\n    $$\n    \\sum_{i=1}^{n} |(H_n)_{ij}| = \\sum_{i=1}^{n} 1 = n.\n    $$\n    Since this sum is $n$ for every column, the maximum is $n$. Thus, $\\|H_n\\|_{1} = n$.\n\n2.  **Calculation of $\\|H_n\\|_{\\infty}$**: The maximum absolute row sum.\n    Similarly, for any row $i$, the absolute row sum is $\\sum_{j=1}^{n} |(H_n)_{ij}| = \\sum_{j=1}^{n} 1 = n$.\n    The maximum of these identical sums is $n$. Thus, $\\|H_n\\|_{\\infty} = n$.\n\n3.  **Calculation of $\\|H_n\\|_{2}$**: The square root of the largest eigenvalue of $H_n^{\\top}H_n$.\n    As established, $H_n^{\\top}H_n = n I_n$. This is a diagonal matrix with all diagonal entries equal to $n$.\n    The eigenvalues of $n I_n$ are all equal to $n$.\n    The maximum eigenvalue is $\\lambda_{\\max}(H_n^{\\top}H_n) = n$.\n    Therefore, the $2$-norm is $\\|H_n\\|_{2} = \\sqrt{n}$.\n\n4.  **Calculation of $R(H_n)$**:\n    $$\n    R(H_n) = \\frac{\\|H_n\\|_{2}}{\\sqrt{\\|H_n\\|_{1}\\,\\|H_n\\|_{\\infty}}} = \\frac{\\sqrt{n}}{\\sqrt{n \\cdot n}} = \\frac{\\sqrt{n}}{\\sqrt{n^2}} = \\frac{\\sqrt{n}}{n} = \\frac{1}{\\sqrt{n}}.\n    $$\n\nFinally, we evaluate $R(H_{n})$ for $n=16$:\n$$\nR(H_{16}) = \\frac{1}{\\sqrt{16}} = \\frac{1}{4}.\n$$\n\nThe required ordered pair is $(R(E), R(H_{16}))$.\nWe found $R(E)=1$ and $R(H_{16}) = \\frac{1}{4}$.\nThe pair is $(1, \\frac{1}{4})$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\frac{1}{4} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Matrix norms are not merely abstract mathematical measures; they have profound consequences for the stability and efficiency of computational algorithms. This practice  provides a powerful illustration of this connection by examining the matrix that arises from discretizing the Laplacian operator, a cornerstone of physics and engineering models. By analyzing how its spectral norm and condition number scale with the problem size, you will discover how these properties directly determine the maximum stable time step for simulating physical phenomena like heat diffusion, bridging the gap between linear algebra theory and practical scientific simulation.",
            "id": "3158860",
            "problem": "You will write a complete, runnable program that constructs the matrix corresponding to the standard five-point finite difference discretization of the negative two-dimensional Laplace operator on the unit square with homogeneous Dirichlet boundary conditions, and then uses this matrix to study how the spectral two-norm and the two-norm condition number scale with the grid resolution. Your program must also quantify the largest stable time step for an explicit Forward Euler solver for the associated linear ordinary differential equation (ODE) system. All computations are to be performed in pure, unitless mathematical terms.\n\nConsider an $n \\times n$ grid of interior points with uniform spacing $h = \\frac{1}{n+1}$ over the unit square. Let $N = n^2$ be the number of unknowns. Define the matrix $A \\in \\mathbb{R}^{N \\times N}$ to represent the standard five-point stencil finite difference approximation to the negative Laplacian $- \\Delta$ with homogeneous Dirichlet boundary conditions. Formally, let $T \\in \\mathbb{R}^{n \\times n}$ be the tridiagonal matrix with $2$ on the main diagonal and $-1$ on the first sub- and super-diagonals, and let $I \\in \\mathbb{R}^{n \\times n}$ denote the identity. Then $A$ is the Kronecker sum scaled by $1/h^2$, that is,\n$$\nA = \\frac{1}{h^2}\\left( I \\otimes T + T \\otimes I \\right).\n$$\nYou must use this $A$ as the unique matrix in all further computations.\n\nLet $\\lVert \\cdot \\rVert_2$ denote the spectral (operator) two-norm and let $\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2$ be the two-norm condition number. Using only fundamental definitions and facts about norms, eigenvalues, Kronecker sums, and stability of linear multistep methods, do the following for each test case below:\n1. Construct $A$ exactly as specified.\n2. Compute $\\lVert A \\rVert_2$.\n3. Compute $\\kappa_2(A)$.\n4. Consider the linear ODE system $u'(t) = - A \\, u(t)$ obtained by semi-discretizing the heat equation with a unit diffusion coefficient. Using only the definition of linear stability for the explicit Forward Euler method applied to $u'(t) = -A \\, u(t)$, determine the largest stable time step $\\Delta t_{\\max}$ for this system.\n\nYour program must produce numerical answers for the following test suite of grid sizes:\n- $n = 1$ (edge case, smallest nontrivial system),\n- $n = 4$ (small system),\n- $n = 8$ (moderate system),\n- $n = 16$ (larger system),\n- $n = 32$ (stress test within reasonable runtime).\n\nEach test case output must be a list of three real numbers $[\\lVert A \\rVert_2, \\kappa_2(A), \\Delta t_{\\max}]$, with each number rounded to six significant digits. The final output must be a single line containing a list of these per-case results, in the same order as listed above for $n$, with no spaces anywhere in the line. For example, the output must look like\n$$\n[[x_{11},x_{12},x_{13}],[x_{21},x_{22},x_{23}],\\dots,[x_{51},x_{52},x_{53}]]\n$$\nwhere each $x_{ij}$ is a real number rounded to six significant digits.\n\nYour program must require no user input and must not access external files or networks. It must use a numerically reliable method for estimating $\\lVert A \\rVert_2$ and $\\kappa_2(A)$ based on the structure of $A$ and the definitions of the two-norm and condition number.\n\nYour final program must print only the single line described above as its output.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the principles of numerical analysis for partial differential equations, well-posed with a unique and computable solution, and expressed in objective, formal language. The problem is a standard exercise in computational science that tests the understanding of finite difference methods, matrix properties derived from Kronecker products, and the stability of numerical methods for ordinary differential equations (ODEs). All necessary data and definitions are provided.\n\nThe task is to compute three quantities for a series of grid sizes $n$: the spectral norm $\\lVert A \\rVert_2$, the two-norm condition number $\\kappa_2(A)$, and the maximum stable timestep $\\Delta t_{\\max}$ for a related ODE system. The matrix $A \\in \\mathbb{R}^{N \\times N}$ with $N=n^2$ is the five-point finite difference approximation of the negative Laplacian on a unit square, given by\n$$\nA = \\frac{1}{h^2}\\left( I \\otimes T + T \\otimes I \\right)\n$$\nwhere $h = \\frac{1}{n+1}$, $I \\in \\mathbb{R}^{n \\times n}$ is the identity matrix, and $T \\in \\mathbb{R}^{n \\times n}$ is the tridiagonal matrix with $2$ on the main diagonal and $-1$ on the first sub- and super-diagonals.\n\nThe most effective and numerically reliable method to compute the required quantities is to leverage the known analytical expressions for the eigenvalues of $A$, rather than constructing the full $N \\times N$ matrix and using numerical eigensolvers, which would be computationally expensive and prone to floating-point errors for large $n$. The structure of $A$ allows for an exact analytical approach.\n\n**1. Eigenvalues of the Matrix $A$**\n\nThe eigenvalues of the matrix $T \\in \\mathbb{R}^{n \\times n}$ are known analytically and are given by:\n$$\n\\lambda_k(T) = 2 - 2 \\cos\\left(\\frac{k \\pi}{n+1}\\right) \\quad \\text{for } k = 1, 2, \\dots, n.\n$$\nThe matrix $A$ is defined as the scaled Kronecker sum $A = \\frac{1}{h^2}(I \\otimes T + T \\otimes I)$. A fundamental property of the Kronecker sum is that its eigenvalues are the sums of the eigenvalues of the constituent matrices. Specifically, the eigenvalues of $I \\otimes T + T \\otimes I$ are $\\lambda_j(T) + \\lambda_k(T)$ for all pairs of indices $j,k \\in \\{1, \\dots, n\\}$.\nTherefore, the eigenvalues of $A$ are:\n$$\n\\lambda_{j,k}(A) = \\frac{1}{h^2} \\left( \\lambda_j(T) + \\lambda_k(T) \\right) = \\frac{1}{h^2} \\left[ \\left(2 - 2 \\cos\\left(\\frac{j \\pi}{n+1}\\right)\\right) + \\left(2 - 2 \\cos\\left(\\frac{k \\pi}{n+1}\\right)\\right) \\right]\n$$\nfor $j,k = 1, 2, \\dots, n$. Since $h = \\frac{1}{n+1}$, we have $1/h^2 = (n+1)^2$.\n\n**2. Spectral Norm $\\lVert A \\rVert_2$**\n\nThe matrix $T$ is symmetric, and thus $A$ is also symmetric. For a symmetric matrix, the spectral norm (or $2$-norm) is equal to its spectral radius, which is the maximum of the absolute values of its eigenvalues:\n$$\n\\lVert A \\rVert_2 = \\rho(A) = \\max_{j,k} |\\lambda_{j,k}(A)|.\n$$\nFor $j,k \\in \\{1, \\dots, n\\}$, the term $\\frac{k \\pi}{n+1}$ is in the interval $(0, \\pi)$, so $\\cos(\\frac{k \\pi}{n+1})  1$. This ensures that $\\lambda_k(T)  0$ for all $k$, and consequently, all eigenvalues $\\lambda_{j,k}(A)$ are strictly positive. The norm is simply the maximum eigenvalue, $\\lambda_{\\max}(A)$.\nThe maximum eigenvalue is achieved when the cosine terms are minimized. The cosine function is decreasing on $[0, \\pi]$, so we must choose the largest possible arguments for the cosine, which corresponds to indices $j=n$ and $k=n$.\n$$\n\\lVert A \\rVert_2 = \\lambda_{\\max}(A) = \\lambda_{n,n}(A) = \\frac{1}{h^2} \\left[ 4 - 4 \\cos\\left(\\frac{n \\pi}{n+1}\\right) \\right].\n$$\nUsing the identity $1 - \\cos(\\theta) = 2 \\sin^2(\\theta/2)$, this simplifies to:\n$$\n\\lVert A \\rVert_2 = \\frac{4}{h^2} \\left( 2 \\sin^2\\left(\\frac{n \\pi}{2(n+1)}\\right) \\right) = \\frac{8}{h^2} \\sin^2\\left(\\frac{n \\pi}{2(n+1)}\\right).\n$$\n\n**3. Condition Number $\\kappa_2(A)$**\n\nThe $2$-norm condition number is defined as $\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2$. Since $A$ is symmetric and positive definite, its eigenvalues are positive, and the eigenvalues of $A^{-1}$ are $1/\\lambda_{j,k}(A)$. The norm of the inverse is $\\lVert A^{-1} \\rVert_2 = \\max(1/\\lambda) = 1/\\min(\\lambda) = 1/\\lambda_{\\min}(A)$.\nThus, the condition number is the ratio of the maximum to the minimum eigenvalue:\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}.\n$$\nThe minimum eigenvalue, $\\lambda_{\\min}(A)$, occurs when the cosine terms in its definition are maximized, which corresponds to the smallest indices $j=1$ and $k=1$.\n$$\n\\lambda_{\\min}(A) = \\lambda_{1,1}(A) = \\frac{1}{h^2} \\left[ 4 - 4 \\cos\\left(\\frac{\\pi}{n+1}\\right) \\right] = \\frac{8}{h^2} \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right).\n$$\nThe ratio is then:\n$$\n\\kappa_2(A) = \\frac{\\frac{8}{h^2} \\sin^2\\left(\\frac{n \\pi}{2(n+1)}\\right)}{\\frac{8}{h^2} \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)} = \\frac{\\sin^2\\left(\\frac{n \\pi}{2(n+1)}\\right)}{\\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)}.\n$$\nUsing the identity $\\sin(\\frac{\\pi}{2} - x) = \\cos(x)$, we have $\\sin(\\frac{n \\pi}{2(n+1)}) = \\sin(\\frac{\\pi}{2} - \\frac{\\pi}{2(n+1)}) = \\cos(\\frac{\\pi}{2(n+1)})$. This gives a simpler expression:\n$$\n\\kappa_2(A) = \\frac{\\cos^2\\left(\\frac{\\pi}{2(n+1)}\\right)}{\\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)} = \\cot^2\\left(\\frac{\\pi}{2(n+1)}\\right).\n$$\n\n**4. Maximum Stable Timestep $\\Delta t_{\\max}$**\n\nThe ODE system is $u'(t) = -A u(t)$. Applying the explicit Forward Euler method yields the iteration:\n$$\nu_{m+1} = u_m + \\Delta t (-A u_m) = (I - \\Delta t A) u_m.\n$$\nFor this method to be stable, the spectral radius of the iteration matrix, $G = I - \\Delta t A$, must satisfy $\\rho(G) \\le 1$. The eigenvalues of $G$ are $1 - \\Delta t \\lambda$ for each eigenvalue $\\lambda$ of $A$. The stability condition is therefore $|1 - \\Delta t \\lambda_{j,k}(A)| \\le 1$ for all $j,k$.\nThis expands to $-1 \\le 1 - \\Delta t \\lambda_{j,k}(A) \\le 1$.\nThe right inequality, $1 - \\Delta t \\lambda_{j,k}(A) \\le 1$, implies $\\Delta t \\lambda_{j,k}(A) \\ge 0$, which is always true since $\\Delta t > 0$ and all eigenvalues of $A$ are positive.\nThe left inequality, $-1 \\le 1 - \\Delta t \\lambda_{j,k}(A)$, implies $\\Delta t \\lambda_{j,k}(A) \\le 2$, or $\\Delta t \\le \\frac{2}{\\lambda_{j,k}(A)}$.\nTo satisfy this for all eigenvalues, the timestep $\\Delta t$ must be bounded by the most restrictive case, which involves the largest eigenvalue, $\\lambda_{\\max}(A) = \\lVert A \\rVert_2$.\n$$\n\\Delta t \\le \\frac{2}{\\lambda_{\\max}(A)}.\n$$\nThe largest stable timestep is therefore:\n$$\n\\Delta t_{\\max} = \\frac{2}{\\lambda_{\\max}(A)} = \\frac{2}{\\lVert A \\rVert_2}.\n$$\n\n**5. Computational Implementation**\n\nThe program will iterate through the given values of $n$. For each $n$, it will calculate $h$ and then apply the derived analytical formulas for $\\lVert A \\rVert_2$, $\\kappa_2(A)$, and $\\Delta t_{\\max}$. The results will be formatted to six significant digits and assembled into the required final output string.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs the matrix for the 2D Laplace operator, calculates its norm,\n    condition number, and the max stable timestep for an associated ODE system,\n    based on analytical formulas.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases_n = [1, 4, 8, 16, 32]\n\n    # A helper function to format a number to a specified number of significant figures.\n    # The '{:g}' format specifier in Python is suitable for this purpose.\n    def format_to_sig_figs(value, sig_figs):\n        return '{:.{p}g}'.format(value, p=sig_figs)\n\n    all_results = []\n    for n in test_cases_n:\n        # Mesh spacing h for an n x n grid of interior points on the unit square.\n        h = 1.0 / (n + 1)\n        pi = np.pi\n\n        # 1. Compute the spectral norm ||A||_2.\n        # This is equal to the maximum eigenvalue of A.\n        # norm_A = lambda_max(A) = (8/h^2) * sin^2(n*pi / (2*(n+1))).\n        # We can use the identity sin(pi/2 - x) = cos(x) to write\n        # sin(n*pi / (2*(n+1))) = sin(pi/2 - pi/(2*(n+1))) = cos(pi/(2*(n+1))).\n        arg_cos = pi / (2.0 * (n + 1))\n        norm_A = (8.0 / h**2) * (np.cos(arg_cos))**2\n\n        # 2. Compute the 2-norm condition number kappa_2(A).\n        # This is the ratio of the max to min eigenvalue.\n        # kappa_2(A) = cot^2(pi / (2*(n+1))).\n        arg_cot = pi / (2.0 * (n + 1))\n        # numpy does not have a cot function, but cot(x) = cos(x)/sin(x).\n        kappa_2_A = (np.cos(arg_cot) / np.sin(arg_cot))**2\n\n        # 3. Determine the largest stable time step dt_max for u'(t) = -A u(t).\n        # For the Forward Euler method, the stability condition is dt = 2/lambda_max(A).\n        # Therefore, dt_max = 2 / norm_A.\n        dt_max = 2.0 / norm_A\n\n        # Format results to six significant digits.\n        sig_figs = 6\n        rounded_norm = format_to_sig_figs(norm_A, sig_figs)\n        rounded_kappa = format_to_sig_figs(kappa_2_A, sig_figs)\n        rounded_dt = format_to_sig_figs(dt_max, sig_figs)\n\n        # Append the list of results for the current n.\n        all_results.append(f\"[{rounded_norm},{rounded_kappa},{rounded_dt}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}