{
    "hands_on_practices": [
        {
            "introduction": "Before we can rely on sophisticated software libraries, it is crucial to understand the fundamental algorithms they are built upon. This practice takes you into the engine room of direct solvers by walking through a single, critical step of Gaussian elimination. By manually applying the partial pivoting strategy, you will gain a concrete understanding of how this technique helps maintain numerical stability, a cornerstone of reliable scientific computation.",
            "id": "2207645",
            "problem": "Consider the system of linear equations represented by the following augmented matrix $[A|b]$:\n\n$$\n\\left[\n\\begin{array}{ccc|c}\n1 & 2 & 3 & 1 \\\\\n4 & 5 & 6 & 2 \\\\\n2 & 1 & 1 & 3\n\\end{array}\n\\right]\n$$\n\nYour task is to apply the Gaussian elimination algorithm with a partial pivoting strategy to this matrix. Perform the first full step of this algorithm, which corresponds to the operations for the first column (i.e., identifying the pivot in the first column, performing any necessary row swaps, and then using the pivot row to eliminate the entries below the pivot in the first column).\n\nAfter completing this first step, provide the four elements of the third row of the resulting modified augmented matrix. The elements should be presented as a row vector $[c_1, c_2, c_3, c_4]$.",
            "solution": "We apply Gaussian elimination with partial pivoting to the first column. The first column entries are $1$, $4$, $2$, so the pivot is chosen as the entry with largest absolute value, which is $4$ in the second row. Swap row $1$ and row $2$:\n$$\n\\left[\n\\begin{array}{ccc|c}\n4 & 5 & 6 & 2 \\\\\n1 & 2 & 3 & 1 \\\\\n2 & 1 & 1 & 3\n\\end{array}\n\\right].\n$$\nUse the pivot row to eliminate the entries below the pivot in the first column. For the new row $2$, the multiplier is $m_{21}=\\frac{1}{4}$, so\n$$\nR_{2}\\leftarrow R_{2}-\\frac{1}{4}R_{1}:\\quad\n\\begin{cases}\n0,\\\\\n2-\\frac{1}{4}\\cdot 5=\\frac{3}{4},\\\\\n3-\\frac{1}{4}\\cdot 6=\\frac{3}{2},\\\\\n1-\\frac{1}{4}\\cdot 2=\\frac{1}{2}.\n\\end{cases}\n$$\nFor row $3$, the multiplier is $m_{31}=\\frac{2}{4}=\\frac{1}{2}$, so\n$$\nR_{3}\\leftarrow R_{3}-\\frac{1}{2}R_{1}:\\quad\n\\begin{cases}\n0,\\\\\n1-\\frac{1}{2}\\cdot 5=-\\frac{3}{2},\\\\\n1-\\frac{1}{2}\\cdot 6=-2,\\\\\n3-\\frac{1}{2}\\cdot 2=2.\n\\end{cases}\n$$\nTherefore, after the first step, the third row of the modified augmented matrix is $[0,-\\frac{3}{2},-2,2]$.",
            "answer": "$$\\boxed{\\begin{pmatrix}0 & -\\frac{3}{2} & -2 & 2\\end{pmatrix}}$$"
        },
        {
            "introduction": "In the world of computational science, exact solutions are a rarity due to the limitations of floating-point arithmetic. A more practical question than \"Is my solution correct?\" is \"How good is my solution?\". This exercise introduces the concept of backward error analysis, a powerful method for quantifying the reliability of an approximate solution. You will calculate the perturbation required in the original problem to make your approximate solution exact, providing a tangible measure of its quality.",
            "id": "2207654",
            "problem": "In numerical analysis, an approximate solution to a system of linear equations is often evaluated by determining the size of the perturbation needed on the original problem for the approximate solution to be exact.\n\nConsider the system of linear equations $A\\mathbf{x} = \\mathbf{b}$, where the matrix $A$ and the vector $\\mathbf{b}$ are given by:\n$$\nA = \\begin{pmatrix}\n4 & 1 & -1 \\\\\n1 & 5 & 2 \\\\\n-1 & 2 & 6\n\\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix}\n8 \\\\\n-3 \\\\\n1\n\\end{pmatrix}\n$$\nA numerical algorithm provides an approximate solution $\\tilde{\\mathbf{x}}$:\n$$\n\\tilde{\\mathbf{x}} = \\begin{pmatrix}\n2.75 \\\\\n-1.60 \\\\\n1.15\n\\end{pmatrix}\n$$\nThis approximate solution $\\tilde{\\mathbf{x}}$ can be considered the exact solution to a perturbed system of the form $A\\mathbf{x} = \\mathbf{b} + \\mathbf{\\delta b}$, where $\\mathbf{\\delta b}$ is the perturbation vector added to $\\mathbf{b}$.\n\nCalculate the relative magnitude of this perturbation, which is defined as the ratio of the Euclidean norm of the perturbation vector $\\mathbf{\\delta b}$ to the Euclidean norm of the original vector $\\mathbf{b}$. Express your answer as a real number rounded to four significant figures.",
            "solution": "We seek the perturbation vector $\\delta \\mathbf{b}$ such that $A\\tilde{\\mathbf{x}}=\\mathbf{b}+\\delta \\mathbf{b}$, which implies $\\delta \\mathbf{b}=A\\tilde{\\mathbf{x}}-\\mathbf{b}$. The relative magnitude is defined as $\\|\\delta \\mathbf{b}\\|_{2}/\\|\\mathbf{b}\\|_{2}$.\n\nCompute $A\\tilde{\\mathbf{x}}$ exactly by writing $\\tilde{\\mathbf{x}}=\\begin{pmatrix}11/4 \\\\ -8/5 \\\\ 23/20\\end{pmatrix}$:\n$$\nA\\tilde{\\mathbf{x}}=\n\\begin{pmatrix}\n4\\cdot \\frac{11}{4}+1\\cdot\\left(-\\frac{8}{5}\\right)-1\\cdot\\frac{23}{20} \\\\\n1\\cdot \\frac{11}{4}+5\\cdot\\left(-\\frac{8}{5}\\right)+2\\cdot\\frac{23}{20} \\\\\n-1\\cdot \\frac{11}{4}+2\\cdot\\left(-\\frac{8}{5}\\right)+6\\cdot\\frac{23}{20}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{33}{4} \\\\\n-\\frac{59}{20} \\\\\n\\frac{19}{20}\n\\end{pmatrix}.\n$$\nTherefore,\n$$\n\\delta \\mathbf{b}=A\\tilde{\\mathbf{x}}-\\mathbf{b}\n=\n\\begin{pmatrix}\n\\frac{33}{4}-8 \\\\\n-\\frac{59}{20}-(-3) \\\\\n\\frac{19}{20}-1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{4} \\\\\n\\frac{1}{20} \\\\\n-\\frac{1}{20}\n\\end{pmatrix}.\n$$\nCompute the Euclidean norms:\n$$\n\\|\\delta \\mathbf{b}\\|_{2}=\\sqrt{\\left(\\frac{1}{4}\\right)^{2}+\\left(\\frac{1}{20}\\right)^{2}+\\left(-\\frac{1}{20}\\right)^{2}}\n=\\sqrt{\\frac{1}{16}+\\frac{1}{400}+\\frac{1}{400}}\n=\\sqrt{\\frac{27}{400}}\n=\\frac{3\\sqrt{3}}{20},\n$$\n$$\n\\|\\mathbf{b}\\|_{2}=\\sqrt{8^{2}+(-3)^{2}+1^{2}}=\\sqrt{64+9+1}=\\sqrt{74}.\n$$\nHence the relative magnitude is\n$$\n\\frac{\\|\\delta \\mathbf{b}\\|_{2}}{\\|\\mathbf{b}\\|_{2}}=\\frac{\\frac{3\\sqrt{3}}{20}}{\\sqrt{74}}=\\frac{3\\sqrt{3}}{20\\sqrt{74}}.\n$$\nNumerically,\n$$\n\\frac{3\\sqrt{3}}{20\\sqrt{74}}\\approx 0.0302020226\\ldots\n$$\nRounded to four significant figures, this equals $0.03020$.",
            "answer": "$$\\boxed{0.03020}$$"
        },
        {
            "introduction": "This capstone exercise moves beyond manual calculations to a realistic computational experiment, addressing the critical issue of ill-conditioning in linear systems. Many real-world problems generate matrices with wildly varying scales, which can lead to significant numerical error. You will implement and test the effects of equilibration, a preconditioning technique designed to improve stability, and empirically demonstrate its power in producing more accurate solutions.",
            "id": "3199896",
            "problem": "You are tasked with writing a complete, runnable program that compares the numerical stability of solving the linear system $A \\mathbf{x} = \\mathbf{b}$ using Lower-Upper (LU) factorization with partial pivoting, both without and with diagonal equilibration, on matrices whose entries have widely varying scales. The investigation must be grounded in fundamental numerical analysis: the definition of a system of linear equations $A \\mathbf{x} = \\mathbf{b}$, the concept of floating-point arithmetic based on the Institute of Electrical and Electronics Engineers 754 floating-point (IEEE 754 FP) model, and well-established norms and conditioning concepts for matrices. The stability assessment must be based on forward error metrics derived from first principles and not on shortcut formulas. Your program must be self-contained, reproducible, and conform to the output specification described below.\n\nBackground and fundamental base. A system of linear equations $A \\mathbf{x} = \\mathbf{b}$ with $A \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{x}, \\mathbf{b} \\in \\mathbb{R}^{n}$ is solved computationally in finite precision arithmetic. Under the IEEE 754 FP model, each arithmetic operation incurs rounding, so the computed solution $\\hat{\\mathbf{x}}$ generally differs from the exact solution $\\mathbf{x}$. The stability of a numerical method to solve $A \\mathbf{x} = \\mathbf{b}$ is assessed by its forward error, commonly defined using the vector $2$-norm, as $\\frac{\\|\\hat{\\mathbf{x}} - \\mathbf{x}\\|_{2}}{\\|\\mathbf{x}\\|_{2}}$, and its backward error via the relative residual $\\frac{\\|A \\hat{\\mathbf{x}} - \\mathbf{b}\\|_{2}}{\\|\\mathbf{b}\\|_{2}}$. Lower-Upper (LU) factorization with partial pivoting is a standard direct method to solve $A \\mathbf{x} = \\mathbf{b}$. Diagonal equilibration is a preconditioning step that constructs diagonal matrices $D_{r}$ and $D_{c}$ to scale rows and columns, respectively, forming the equilibrated system $(D_{r} A D_{c}) \\mathbf{y} = D_{r} \\mathbf{b}$ and mapping the solution back via $\\mathbf{x} = D_{c} \\mathbf{y}$. Equilibration aims to reduce the dynamic range of entries in $A$, thereby mitigating pivot growth and improving numerical stability under finite precision. The condition number $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2}$ provides a measure of sensitivity of the solution to perturbations, but your program must not compute or rely on this quantity as a shortcut; instead, it should directly compare forward errors obtained by the two approaches.\n\nYour task. Implement two solvers for $A \\mathbf{x} = \\mathbf{b}$:\n- A direct solver using LU factorization with partial pivoting without any equilibration.\n- A solver that first applies diagonal row scaling $D_{r}$ and then diagonal column scaling $D_{c}$, forming $A_{\\text{hat}} = D_{r} A D_{c}$ and $\\mathbf{b}_{\\text{hat}} = D_{r} \\mathbf{b}$, solves $A_{\\text{hat}} \\mathbf{y} = \\mathbf{b}_{\\text{hat}}$ via LU with partial pivoting, and returns $\\mathbf{x} = D_{c} \\mathbf{y}$.\n\nDefine $D_{r}$ and $D_{c}$ using the following well-tested approach: for each row $i$, set $(D_{r})_{ii} = \\frac{1}{\\max_{j} |A_{ij}|}$ when the maximum is nonzero and $(D_{r})_{ii} = 1$ otherwise; after applying row scaling, for each column $j$, set $(D_{c})_{jj} = \\frac{1}{\\max_{i} |(D_{r} A)_{ij}|}$ when the maximum is nonzero and $(D_{c})_{jj} = 1$ otherwise.\n\nStability metric. For each test case, construct $\\mathbf{b}$ using a known $\\mathbf{x}_{\\text{true}}$ by setting $\\mathbf{b} = A \\mathbf{x}_{\\text{true}}$. Compute the forward error for the non-equilibrated solution $\\hat{\\mathbf{x}}_{\\text{noeq}}$ as $e_{\\text{noeq}} = \\frac{\\|\\hat{\\mathbf{x}}_{\\text{noeq}} - \\mathbf{x}_{\\text{true}}\\|_{2}}{\\|\\mathbf{x}_{\\text{true}}\\|_{2}}$ and for the equilibrated solution $\\hat{\\mathbf{x}}_{\\text{eq}}$ as $e_{\\text{eq}} = \\frac{\\|\\hat{\\mathbf{x}}_{\\text{eq}} - \\mathbf{x}_{\\text{true}}\\|_{2}}{\\|\\mathbf{x}_{\\text{true}}\\|_{2}}$. Report, for each test case, the ratio $r = \\frac{e_{\\text{noeq}}}{e_{\\text{eq}}}$ as a floating-point number. Values $r > 1$ indicate that equilibration improved the forward error.\n\nComputational environment. All computations must be performed in double precision ($64$-bit) floating-point arithmetic. No physical units or angle units are involved in this problem.\n\nTest suite. Use a fixed random seed of $42$ for reproducibility. Implement the following test cases, each defined purely mathematically:\n- Case $1$ (wild row and column scales, square random): Let $n = 40$. Draw $A_{0} \\in \\mathbb{R}^{n \\times n}$ with independent standard normal entries. Define row scales $r_{i} = 10^{\\alpha_{i}}$ and column scales $c_{j} = 10^{\\beta_{j}}$, where $(\\alpha_{i})_{i=1}^{n}$ and $(\\beta_{j})_{j=1}^{n}$ are random permutations of the equally spaced vector from $-8$ to $8$ inclusive. Set $A = \\operatorname{diag}(r) \\, A_{0} \\, \\operatorname{diag}(c)$. Draw $\\mathbf{x}_{\\text{true}} \\in \\mathbb{R}^{n}$ with independent standard normal entries and set $\\mathbf{b} = A \\mathbf{x}_{\\text{true}}$.\n- Case $2$ (nearly singular diagonal with tiny coupling): Let $n = 25$. Define diagonal entries $d_{i} = 10^{\\gamma_{i}}$ where $(\\gamma_{i})_{i=1}^{n}$ are equally spaced from $-12$ to $0$. Set $A = \\operatorname{diag}(d) + \\delta$, where $\\delta$ has independent standard normal entries scaled by $10^{-12}$. Draw $\\mathbf{x}_{\\text{true}}$ standard normal and set $\\mathbf{b} = A \\mathbf{x}_{\\text{true}}$.\n- Case $3$ (balanced orthogonal): Let $n = 40$. Draw $M \\in \\mathbb{R}^{n \\times n}$ with independent standard normal entries and compute its QR factorization $M = Q R$ with $Q \\in \\mathbb{R}^{n \\times n}$ orthogonal. Set $A = Q$. Draw $\\mathbf{x}_{\\text{true}}$ standard normal and set $\\mathbf{b} = A \\mathbf{x}_{\\text{true}}$.\n- Case $4$ (Hilbert with column scaling): Let $n = 12$. Define the Hilbert matrix $H \\in \\mathbb{R}^{n \\times n}$ by $H_{ij} = \\frac{1}{i + j - 1}$. Define column scales $c_{j} = 10^{j - 1}$. Set $A = H \\, \\operatorname{diag}(c)$. Draw $\\mathbf{x}_{\\text{true}}$ standard normal and set $\\mathbf{b} = A \\mathbf{x}_{\\text{true}}$.\n- Case $5$ (two-scale block with coupling): Let $n_{1} = 15$, $n_{2} = 15$, and $n = n_{1} + n_{2} = 30$. Draw $A_{1}, A_{2}$ with independent standard normal entries of sizes $n_{1} \\times n_{1}$ and $n_{2} \\times n_{2}$, respectively. Set $\\tilde{A}_{1} = 10^{-9} A_{1}$ and $\\tilde{A}_{2} = 10^{9} A_{2}$. Form the block-diagonal matrix $B = \\operatorname{diag}(\\tilde{A}_{1}, \\tilde{A}_{2})$. Draw a coupling matrix $C$ with independent standard normal entries scaled by $10^{-3}$ of size $n \\times n$, and set $A = B + C$. Draw $\\mathbf{x}_{\\text{true}}$ standard normal and set $\\mathbf{b} = A \\mathbf{x}_{\\text{true}}$.\n\nAnswer specification. For each case, compute the ratio $r = \\frac{e_{\\text{noeq}}}{e_{\\text{eq}}}$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as cases $1$ through $5$. Each ratio must be printed in scientific notation with six significant digits (for example, `[1.234560e+02,3.210000e-05]`). This line must contain only the list, with no extra text.",
            "solution": "The problem requires a comparative analysis of the numerical stability of solving a system of linear equations, $A \\mathbf{x} = \\mathbf{b}$, using two distinct methods: direct LU factorization with partial pivoting, and the same method applied after a specific diagonal equilibration preconditioning step. The analysis is to be performed on a suite of five test matrices designed to exhibit properties that challenge numerical solvers, such as poor scaling or ill-conditioning. The entire investigation must be conducted within the framework of double-precision floating-point arithmetic.\n\nThe fundamental principle being investigated is that the stability of numerical algorithms for solving linear systems can be highly sensitive to the properties of the matrix $A$. In finite-precision arithmetic, such as the specified IEEE 754 64-bit standard, rounding errors can accumulate and severely degrade the accuracy of the computed solution, $\\hat{\\mathbf{x}}$. Equilibration is a preconditioning technique aimed at mitigating such effects.\n\nThe problem specifies two solution pathways for a given system $A \\mathbf{x} = \\mathbf{b}$, where $A \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{x}, \\mathbf{b} \\in \\mathbb{R}^{n}$. For each test case, a known true solution, $\\mathbf{x}_{\\text{true}}$, is used to construct the right-hand side vector $\\mathbf{b} = A \\mathbf{x}_{\\text{true}}$. The accuracy of a computed solution $\\hat{\\mathbf{x}}$ is then quantified by the relative forward error, defined by the vector $2$-norm as $e = \\frac{\\|\\hat{\\mathbf{x}} - \\mathbf{x}_{\\text{true}}\\|_{2}}{\\|\\mathbf{x}_{\\text{true}}\\|_{2}}$.\n\nThe two computational methods are:\n1.  **No Equilibration**: The system $A \\mathbf{x} = \\mathbf{b}$ is solved directly. We use LU factorization with partial pivoting, as implemented in the `scipy.linalg.lu_factor` and `scipy.linalg.lu_solve` functions, to find the computed solution $\\hat{\\mathbf{x}}_{\\text{noeq}}$. The corresponding forward error is $e_{\\text{noeq}} = \\frac{\\|\\hat{\\mathbf{x}}_{\\text{noeq}} - \\mathbf{x}_{\\text{true}}\\|_{2}}{\\|\\mathbf{x}_{\\text{true}}\\|_{2}}$.\n\n2.  **With Equilibration**: The system is preconditioned before solving. This involves constructing two diagonal matrices, a row-scaling matrix $D_{r}$ and a column-scaling matrix $D_{c}$. Following the problem specification, their diagonal entries are defined sequentially. First, for each row $i$, $(D_{r})_{ii} = (\\max_{j} |A_{ij}|)^{-1}$ if this maximum is non-zero, and $(D_{r})_{ii} = 1$ otherwise. After this row scaling is conceptually applied, the column scaling is determined. For each column $j$, $(D_{c})_{jj} = (\\max_{i} |(D_{r}A)_{ij}|)^{-1}$ if this maximum is non-zero, and $(D_{c})_{jj} = 1$ otherwise.\nThis transforms the original system $A\\mathbf{x} = \\mathbf{b}$ into an equilibrated system $(D_{r} A D_{c}) \\mathbf{y} = D_{r} \\mathbf{b}$. Let us denote $A_{\\text{hat}} = D_{r} A D_{c}$ and $\\mathbf{b}_{\\text{hat}} = D_{r} \\mathbf{b}$. We solve $A_{\\text{hat}} \\mathbf{y} = \\mathbf{b}_{\\text{hat}}$ for $\\mathbf{y}$ using the same LU factorization method with partial pivoting. The solution to the original system is then recovered via the relationship $\\mathbf{x} = D_{c} \\mathbf{y}$. Let this computed solution be $\\hat{\\mathbf{x}}_{\\text{eq}}$. The corresponding forward error is $e_{\\text{eq}} = \\frac{\\|\\hat{\\mathbf{x}}_{\\text{eq}} - \\mathbf{x}_{\\text{true}}\\|_{2}}{\\|\\mathbf{x}_{\\text{true}}\\|_{2}}$.\n\nThe comparison metric is the ratio of these forward errors: $r = \\frac{e_{\\text{noeq}}}{e_{\\text{eq}}}$. A value of $r > 1$ signifies that the equilibration procedure improved the accuracy of the computed solution.\n\nThe implementation will proceed by first defining functions to construct each of the five test cases as specified. A fixed random seed of $42$ ensures reproducibility. The core of the program is a function that takes a matrix $A$ and a true solution $\\mathbf{x}_{\\text{true}}$, calculates $\\mathbf{b}$, solves the system using both methods, computes the respective forward errors, and returns their ratio. This function is then applied to each of the five test cases. All matrix and vector operations, including random number generation and norm calculations, are performed using the `numpy` library. The LU factorization and solver are from the `scipy.linalg` package, as this choice explicitly provides access to the required algorithm. The final results are formatted into scientific notation with six significant digits and printed as a single comma-separated list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    \n    def get_error_ratio(A: np.ndarray, x_true: np.ndarray) -> float:\n        \"\"\"\n        Computes the ratio of forward errors for solving Ax=b with and without\n        equilibration.\n\n        Args:\n            A: The matrix of the linear system.\n            x_true: The known true solution.\n\n        Returns:\n            The ratio e_noeq / e_eq.\n        \"\"\"\n        # Ensure all computations are done in double precision.\n        A = np.array(A, dtype=np.float64)\n        x_true = np.array(x_true, dtype=np.float64)\n\n        # Construct b from the known x_true. This is done in finite precision.\n        b = A @ x_true\n        \n        norm_x_true = np.linalg.norm(x_true, 2)\n        if norm_x_true == 0:\n            # This should not happen with the given test cases.\n            return np.nan\n\n        # Method 1: Solve Ax=b without equilibration.\n        try:\n            lu, piv = linalg.lu_factor(A)\n            x_noeq = linalg.lu_solve((lu, piv), b)\n            e_noeq = np.linalg.norm(x_noeq - x_true, 2) / norm_x_true\n        except np.linalg.LinAlgError:\n            e_noeq = np.inf\n\n\n        # Method 2: Solve Ax=b with diagonal equilibration.\n        n = A.shape[0]\n\n        # Construct row scaling matrix Dr.\n        dr_diag = np.ones(n, dtype=np.float64)\n        row_maxs = np.max(np.abs(A), axis=1)\n        nonzero_rows = row_maxs > 0\n        dr_diag[nonzero_rows] = 1.0 / row_maxs[nonzero_rows]\n        Dr = np.diag(dr_diag)\n\n        # Apply row scaling.\n        A_prime = Dr @ A\n\n        # Construct column scaling matrix Dc.\n        dc_diag = np.ones(n, dtype=np.float64)\n        col_maxs = np.max(np.abs(A_prime), axis=0)\n        nonzero_cols = col_maxs > 0\n        dc_diag[nonzero_cols] = 1.0 / col_maxs[nonzero_cols]\n        Dc = np.diag(dc_diag)\n\n        # Form the equilibrated system.\n        A_hat = A_prime @ Dc\n        b_hat = Dr @ b\n\n        # Solve the equilibrated system for y.\n        try:\n            lu_hat, piv_hat = linalg.lu_factor(A_hat)\n            y_hat = linalg.lu_solve((lu_hat, piv_hat), b_hat)\n\n            # Map the solution y back to x.\n            x_eq = Dc @ y_hat\n            e_eq = np.linalg.norm(x_eq - x_true, 2) / norm_x_true\n        except np.linalg.LinAlgError:\n            e_eq = np.inf\n\n        # Compute the ratio of forward errors.\n        if e_eq == 0:\n            return np.inf if e_noeq > 0 else 1.0\n            \n        return e_noeq / e_eq\n\n    def generate_test_cases():\n        \"\"\"\n        Generates the five test cases as specified in the problem statement.\n        \"\"\"\n        rng = np.random.default_rng(42)\n        test_cases = []\n\n        # Case 1: Wild row and column scales\n        n = 40\n        A0 = rng.standard_normal(size=(n, n))\n        alphas = rng.permutation(np.linspace(-8, 8, num=n))\n        betas = rng.permutation(np.linspace(-8, 8, num=n))\n        r = 10.0**alphas\n        c = 10.0**betas\n        A1 = np.diag(r) @ A0 @ np.diag(c)\n        x_true1 = rng.standard_normal(size=n)\n        test_cases.append((A1, x_true1))\n\n        # Case 2: Nearly singular diagonal with tiny coupling\n        n = 25\n        gammas = np.linspace(-12, 0, num=n)\n        d = 10.0**gammas\n        A2 = np.diag(d) + 1e-12 * rng.standard_normal(size=(n, n))\n        x_true2 = rng.standard_normal(size=n)\n        test_cases.append((A2, x_true2))\n\n        # Case 3: Balanced orthogonal matrix\n        n = 40\n        M = rng.standard_normal(size=(n, n))\n        Q, _ = np.linalg.qr(M)\n        A3 = Q\n        x_true3 = rng.standard_normal(size=n)\n        test_cases.append((A3, x_true3))\n\n        # Case 4: Hilbert matrix with column scaling\n        n = 12\n        i_indices = np.arange(1, n + 1).reshape(n, 1)\n        j_indices = np.arange(1, n + 1).reshape(1, n)\n        H = 1.0 / (i_indices + j_indices - 1)\n        c_diag = 10.0**np.arange(n)\n        A4 = H @ np.diag(c_diag)\n        x_true4 = rng.standard_normal(size=n)\n        test_cases.append((A4, x_true4))\n\n        # Case 5: Two-scale block matrix with coupling\n        n1, n2 = 15, 15\n        n = n1 + n2\n        A1_block = rng.standard_normal(size=(n1, n1))\n        A2_block = rng.standard_normal(size=(n2, n2))\n        B = np.zeros((n, n), dtype=np.float64)\n        B[:n1, :n1] = 1e-9 * A1_block\n        B[n1:, n1:] = 1e9 * A2_block\n        C = 1e-3 * rng.standard_normal(size=(n, n))\n        A5 = B + C\n        x_true5 = rng.standard_normal(size=n)\n        test_cases.append((A5, x_true5))\n        \n        return test_cases\n\n    test_cases = generate_test_cases()\n    results = []\n    for A, x_true in test_cases:\n        ratio = get_error_ratio(A, x_true)\n        results.append(ratio)\n    \n    # Format results as specified\n    formatted_results = [f\"{res:.6e}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}