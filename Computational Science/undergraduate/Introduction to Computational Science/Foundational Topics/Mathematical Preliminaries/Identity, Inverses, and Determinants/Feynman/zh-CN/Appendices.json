{
    "hands_on_practices": [
        {
            "introduction": "这项练习超越了简单的计算，深入探讨了矩阵及其逆矩阵之间的结构性关系。我们将探究一个关键问题：对于一个整数矩阵，其行列式必须满足什么条件才能保证其逆矩阵也完全由整数构成？这项练习  要求对条件的必要性和充分性进行严格证明，从而揭示整数矩阵理论中的一个基本原则，并锻炼严谨的逻辑推理能力。",
            "id": "1361629",
            "problem": "设 $M$ 是一个所有元素均为整数的 $2 \\times 2$ 矩阵。众所周知，对于这样的矩阵，如果其逆矩阵 $M^{-1}$ 存在且所有元素也都是整数，那么其行列式 $\\det(M)$ 必定是一个整数。然而，反之不一定成立；整数行列式并不能保证逆矩阵也是整数矩阵。\n\n任务是确定行列式的哪些整数值是*充分*条件，可以保证*任何*具有该行列式的此类矩阵 $M$ 的逆矩阵也完全由整数构成，同时这些值也是该性质对所有整矩阵都成立的*必要*条件。从以下 $\\det(M)$ 的可能值列表中，选择所有满足这一充要条件的选项。\n\nA. $2$\n\nB. $1$\n\nC. $-1$\n\nD. $-2$\n\nE. $0$",
            "solution": "设元素为整数的 $2 \\times 2$ 矩阵 $M$ 为\n$$\nM = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}\n$$\n其中 $a, b, c, d$ 都是整数，即 $a, b, c, d \\in \\mathbb{Z}$。\n\n$M$ 的行列式是 $\\det(M) = ad - bc$。由于 $a, b, c, d$ 都是整数，它们的乘积和差也都是整数。因此，$\\det(M)$ 是一个整数。\n\n如果 $\\det(M) \\neq 0$，$M$ 的逆矩阵，记作 $M^{-1}$，由以下公式给出：\n$$\nM^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix} = \\begin{pmatrix} \\frac{d}{\\det(M)}  \\frac{-b}{\\det(M)} \\\\ \\frac{-c}{\\det(M)}  \\frac{a}{\\det(M)} \\end{pmatrix}\n$$\n\n问题要求的是，$\\det(M)$ 的整数值需要满足什么充要条件，才能保证 $M^{-1}$ 的所有元素都是整数。让我们通过分析条件的充分性和必要性来研究这个问题。\n\n首先，让我们分析选项E，即 $\\det(M) = 0$ 的情况。如果一个矩阵的行列式为零，该矩阵是奇异的，其逆矩阵不存在。因此，选项E是错误的，因为它不满足存在逆矩阵的前提。\n\n**充分性分析：**\n让我们测试行列式为 $1$ 或 $-1$ 是否是充分条件。假设 $\\det(M) = k$，其中 $k \\in \\{1, -1\\}$。\n$M^{-1}$ 的元素形式为 $\\frac{x}{k}$，其中 $x$是整数 $d, -b, -c, a$ 之一。\n如果 $k=1$，则 $M^{-1}$ 的元素为 $d, -b, -c, a$，它们都是整数。\n如果 $k=-1$，则 $M^{-1}$ 的元素为 $-d, b, c, -a$，它们也都是整数。\n因此，如果 $\\det(M)$ 为 $1$ 或 $-1$，可以保证 $M^{-1}$ 的所有元素都是整数。这意味着选项B和C给出的条件是充分的。\n\n**必要性分析：**\n现在，我们必须证明这些是唯一可能的值。换句话说，如果一个整数矩阵 $M$ 有一个整数逆矩阵 $M^{-1}$，那么 $\\det(M)$ 必须为 $1$ 或 $-1$ 是否是必要的？\n\n设 $M$ 和 $M^{-1}$ 都是元素为整数的矩阵。根据逆矩阵的定义，我们有恒等式 $M M^{-1} = I$，其中 $I$ 是 $2 \\times 2$ 单位矩阵。\n利用矩阵乘积的行列式等于行列式的乘积这一性质，我们有：\n$$\n\\det(M M^{-1}) = \\det(I)\n$$\n$$\n\\det(M) \\det(M^{-1}) = 1\n$$\n由于 $M$ 的元素是整数，其行列式 $\\det(M)$ 必须是一个整数。\n同样地，由于 $M^{-1}$ 的元素是整数，其行列式 $\\det(M^{-1})$ 也必须是一个整数。\n设 $k_1 = \\det(M)$ 和 $k_2 = \\det(M^{-1})$，其中 $k_1, k_2 \\in \\mathbb{Z}$。方程变为：\n$$\nk_1 k_2 = 1\n$$\n两个整数的乘积为 $1$ 的唯一可能是它们是 $(1, 1)$ 或 $(-1, -1)$。\n因此，$\\det(M)$ 的值必须是 $1$ 或 $-1$。\n这证明了条件 $\\det(M) \\in \\{1, -1\\}$ 是必要的。\n\n行列式的任何其他整数值都不是充分的。例如，考虑选项A，其中 $\\det(M) = 2$。我们可以构造一个具有该行列式的简单整数矩阵：\n$$\nM = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}\n$$\n其行列式为 $2 \\cdot 1 - 0 \\cdot 0 = 2$。其逆矩阵是：\n$$\nM^{-1} = \\frac{1}{2} \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ 0  1 \\end{pmatrix}\n$$\n这个逆矩阵并非所有元素都是整数。因此，$\\det(M)=2$ 不是一个充分条件。对于任何绝对值大于 1 的整数行列式 $k$，例如选项 A ($2$) 和 D ($-2$)，都可以构造类似的反例。\n\n综合充分性和必要性的论证，能够保证一个整数矩阵的逆矩阵也是整数矩阵的行列式值只有 $1$ 和 $-1$。\n查看所提供的选项：\nA. $2$: 无效。\nB. $1$: 有效。\nC. $-1$: 有效。\nD. $-2$: 无效。\nE. $0$: 无效 (逆矩阵不存在)。\n\n因此，正确的选项是B和C。",
            "answer": "$$\\boxed{BC}$$"
        },
        {
            "introduction": "现在，我们从代数规则转向几何直觉。一个变换的行列式不仅仅是一个标量值，它更像一个几何罗盘，告诉我们该变换如何缩放体积，以及——至关重要地——它是否保持空间的方向。在这个动手编程练习  中，你将通过检查一个二维映射在其定义域上各点雅可比行列式的符号，来实现一个数值测试来分析该映射的保向性，这是计算机图形学、网格生成和连续介质力学等领域的关键技术。",
            "id": "3141219",
            "problem": "给定定义了从二维实平面到其自身的映射的光滑函数，记为 $F:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$，其中 $F(x,y)=\\left(f_1(x,y),f_2(x,y)\\right)$。在计算科学的背景下，此类映射的定向保持性和局部可逆性可以通过雅可比矩阵及其行列式进行分析。点 $\\left(x,y\\right)$ 处的雅可比矩阵 $J$ 定义为 $F$ 在点 $\\left(x,y\\right)$ 处取值的一阶偏导数矩阵。$\\det J$ 的符号编码了映射是保持定向（$\\det J>0$）、反转定向（$\\det J<0$）还是局部奇异（$\\det J=0$）。您的任务是实现一个鲁棒的计算测试，对于给定的映射和矩形域，该测试会在一个均匀点网格上采样，使用有限差分近似雅可比矩阵 $J$ 的元素，并对网格上各处的映射定向行为进行分类。\n\n起点和基本依据：\n- 使用雅可比矩阵 $J$ 作为 $F$ 的偏导数矩阵的定义。\n- 使用熟知的事实：行列式 $\\det J$ 给出 $F$ 的局部线性近似的带符号面积缩放因子；在 $\\det J\\neq 0$ 处局部可逆性成立，在 $\\det J>0$ 处保持定向，在 $\\det J<0$ 处反转定向。\n\n实现要求：\n- 为保持数值稳定性，在每个网格点上，对内部点使用对称有限差分，对边界点使用单边差分来近似偏导数。\n- 在分类符号时，使用一个鲁棒的非零阈值 $\\tau$（一个小的正实数）以减轻浮点和离散化误差。具体来说：\n  - 如果 $\\det J>\\tau$，则将网格点分类为保持定向。\n  - 如果 $\\det J<-\\tau$，则将网格点分类为反转定向。\n  - 如果 $|\\det J|\\le \\tau$，则将网格点分类为不确定/奇异。\n- 通过汇总每个点的分类来对整个网格上的映射进行分类：\n  - 如果所有网格点都保持定向，则输出 $+1$。\n  - 如果所有网格点都反转定向，则输出 $-1$。\n  - 否则（网格上任何地方出现混合符号和/或接近零的行列式），输出 $0$。\n\n网格采样：\n- 对闭合矩形域 $\\left[x_{\\min},x_{\\max}\\right]\\times\\left[y_{\\min},y_{\\max}\\right]$ 进行均匀采样，在 $x$ 方向上有 $N_x$ 个点，在 $y$ 方向上有 $N_y$ 个点，包括端点。\n- 设间距为 $h_x=\\dfrac{x_{\\max}-x_{\\min}}{N_x-1}$ 和 $h_y=\\dfrac{y_{\\max}-y_{\\min}}{N_y-1}$。\n\n测试套件：\n实现您的程序，对以下五个映射、域和网格分辨率进行分类。对于旋转映射，使用弧度单位的角度。\n- 情况 1（单位映射，理想情况）：在 $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ 上的 $F(x,y)=\\left(x,y\\right)$，其中 $N_x=21$，$N_y=21$，$\\tau=10^{-10}$。\n- 情况 2（旋转 $\\pi/4$，理想情况）：在 $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ 上的 $F(x,y)=\\left(\\cos\\left(\\frac{\\pi}{4}\\right)x-\\sin\\left(\\frac{\\pi}{4}\\right)y,\\ \\sin\\left(\\frac{\\pi}{4}\\right)x+\\cos\\left(\\frac{\\pi}{4}\\right)y\\right)$，其中 $N_x=21$，$N_y=21$，$\\tau=10^{-10}$。\n- 情况 3（反射，反转定向）：在 $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ 上的 $F(x,y)=\\left(-x,y\\right)$，其中 $N_x=21$，$N_y=21$，$\\tau=10^{-10}$。\n- 情况 4（非线性，混合和奇异）：在 $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ 上的 $F(x,y)=\\left(x^2,y\\right)$，其中 $N_x=21$，$N_y=21$，$\\tau=10^{-10}$。\n- 情况 5（小行列式的均匀缩放，边界和数值鲁棒性）：在 $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ 上的 $F(x,y)=\\left(\\varepsilon x,\\varepsilon y\\right)$，其中 $\\varepsilon=10^{-4}$，$N_x=21$，$N_y=21$，$\\tau=10^{-10}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含五个情况的分类结果，格式为方括号内的逗号分隔列表；例如 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$，其中每个条目是按上述定义的 $+1$、$-1$ 或 $0$ 之一。\n- 不涉及物理单位，除指定的弧度角外不出现其他角度。输出为整数。\n\n科学真实性和推导重点：\n- 程序必须从雅可比行列式的基本定义推导定向分类，并且不得依赖任何为测试套件预先计算的解析行列式公式。\n- 基于网格的近似实现必须是自洽的、数值上合理的，并且对边界条件和小行列式具有鲁棒性。",
            "solution": "经评估，用户提供的问题是有效的。它在科学上基于多变量微积分和数值分析，问题设定良好（适定），具有明确的目标和约束，并且没有矛盾或歧义。该问题要求实现一种计算方法，通过分析雅可比矩阵的行列式来分类二维映射的定向保持性质，这是计算科学中一项标准且有意义的任务。\n\n解决方案首先建立理论基础，然后详细说明数值算法，最后将其应用于指定的测试用例。\n\n### 1. 理论基础：雅可比矩阵和定向\n\n一个由 $F(x,y) = (f_1(x,y), f_2(x,y))$ 定义的光滑映射 $F: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$，在任何点 $(x_0, y_0)$ 处都可以通过线性变换进行局部近似。表示此线性变换的矩阵是在该点求值的雅可比矩阵 $J$。雅可比矩阵是向量值函数 $F$ 的所有一阶偏导数的矩阵：\n\n$$\nJ(x,y) =\n\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x}(x,y)  \\frac{\\partial f_1}{\\partial y}(x,y) \\\\\n\\frac{\\partial f_2}{\\partial x}(x,y)  \\frac{\\partial f_2}{\\partial y}(x,y)\n\\end{pmatrix}\n$$\n\n雅可比矩阵的行列式 $\\det(J)$ 是一个标量值，描述了映射如何局部缩放面积。行列式的符号表示映射对定向的影响：\n- 如果 $\\det(J) > 0$，映射是**保持定向**的。它变换一个小区域而不“翻转”它。\n- 如果 $\\det(J) < 0$，映射是**反转定向**的。它“翻转”一个小区域的定向，类似于反射。\n- 如果 $\\det(J) = 0$，映射在该点是**奇异**的。它会压缩面积，并且反函数定理不保证局部可逆性。\n\n### 2. 雅可比矩阵的数值近似\n\n由于我们必须创建一个不依赖解析导数的通用工具，我们使用有限差分来数值近似偏导数。我们在一个离散的点网格 $(x_i, y_j)$ 上操作，该网格跨越域 $[x_{\\min}, x_{\\max}] \\times [y_{\\min}, y_{\\max}]$。网格间距为 $h_x = \\frac{x_{\\max} - x_{\\min}}{N_x - 1}$ 和 $h_y = \\frac{y_{\\max} - y_{\\min}}{N_y - 1}$，其中 $N_x$ 和 $N_y$ 是每个方向上的点数。\n\n为了在整个网格上保持数值准确性和稳定性，我们对内部点和边界点使用不同的有限差分公式。\n\n**对于内部点 $(x_i, y_j)$，其中 $0 < i < N_x-1$ 且 $0 < j < N_y-1$：**\n我们使用二阶精度的**对称（中心）差分**公式。对于一个通用函数 $g(x,y)$：\n$$ \\frac{\\partial g}{\\partial x}(x_i, y_j) \\approx \\frac{g(x_i + h_x, y_j) - g(x_i - h_x, y_j)}{2h_x} $$\n$$ \\frac{\\partial g}{\\partial y}(x_i, y_j) \\approx \\frac{g(x_i, y_j + h_y) - g(x_i, y_j - h_y)}{2h_y} $$\n\n**对于边界点：**\n我们必须使用单边公式。\n- 在左边界（$i=0$）：对 $\\frac{\\partial}{\\partial x}$ 使用**前向差分**。\n  $$ \\frac{\\partial g}{\\partial x}(x_0, y_j) \\approx \\frac{g(x_0 + h_x, y_j) - g(x_0, y_j)}{h_x} $$\n- 在右边界（$i=N_x-1$）：对 $\\frac{\\partial}{\\partial x}$ 使用**后向差分**。\n  $$ \\frac{\\partial g}{\\partial x}(x_{N_x-1}, y_j) \\approx \\frac{g(x_{N_x-1}, y_j) - g(x_{N_x-1} - h_x, y_j)}{h_x} $$\n类似地，在下边界（$j=0$）和上边界（$j=N_y-1$），分别对 $\\frac{\\partial}{\\partial y}$ 使用前向和后向差分公式。这些公式是一阶精度的。\n\n### 3. 算法流程\n\n在指定域上对给定映射 $F$ 进行分类的总体算法如下：\n\n1.  **网格生成**：通过创建两个坐标数组来定义采样网格，一个用于 $x$ 轴，从 $x_{\\min}$ 到 $x_{\\max}$ 有 $N_x$ 个点；另一个用于 $y$ 轴，从 $y_{\\min}$ 到 $y_{\\max}$ 有 $N_y$ 个点。计算步长 $h_x$ 和 $h_y$。\n\n2.  **迭代和分类**：遍历网格上的每个点 $(x_i, y_j)$。对于每个点：\n    a.  计算构成雅可比矩阵的四个偏导数：$\\frac{\\partial f_1}{\\partial x}$、$\\frac{\\partial f_1}{\\partial y}$、$\\frac{\\partial f_2}{\\partial x}$ 和 $\\frac{\\partial f_2}{\\partial y}$。根据点 $(i,j)$ 相对于网格边界的位置，选择合适的有限差分公式（中心、前向或后向）。\n    b.  根据这四个近似值构造雅可比矩阵 $J_{ij}$。\n    c.  计算其行列式：$\\det(J_{ij}) = \\frac{\\partial f_1}{\\partial x}\\frac{\\partial f_2}{\\partial y} - \\frac{\\partial f_1}{\\partial y}\\frac{\\partial f_2}{\\partial x}$。\n    d.  使用给定的阈值 $\\tau$ 对该点的局部行为进行分类：\n        - 如果 $\\det(J_{ij}) > \\tau$，该点为`保持定向`。\n        - 如果 $\\det(J_{ij}) < -\\tau$，该点为`反转定向`。\n        - 如果 $|\\det(J_{ij})| \\le \\tau$，该点为`不确定/奇异`。\n    e.  记录找到的分类类型。我们只需要知道是否每种类型都至少遇到过一次。\n\n3.  **汇总结果**：在评估完网格上所有点之后，将单个分类组合成整个域上映射的单一结果：\n    a.  如果所有点都被分类为`保持定向`，则最终输出为 $+1$。\n    b.  如果所有点都被分类为`反转定向`，则最终输出为 $-1$。\n    c.  如果存在任何混合分类（例如，一些保持定向和一些反转定向），或者任何点被分类为`不确定/奇异`，则最终输出为 $0$。\n\n此流程对问题陈述中提供的五个测试用例中的每一个都进行实现。然后将结果汇总成所需的最终格式。\n- 对于 $F(x,y)=(x,y)$，$\\det(J) = 1$，它大于任何小的正数 $\\tau$。结果是 $+1$。\n- 对于旋转，$\\det(J) = \\cos^2(\\frac{\\pi}{4}) + \\sin^2(\\frac{\\pi}{4}) = 1$。结果是 $+1$。\n- 对于 $F(x,y)=(-x,y)$，$\\det(J) = -1$，它小于任何小的负数 $-\\tau$。结果是 $-1$。\n- 对于 $F(x,y)=(x^2,y)$，$\\det(J) = 2x$。在域 $[-1,1]\\times[-1,1]$ 上，该行列式取正值、负值和零值。因此，这是一个混合情况，结果为 $0$。\n- 对于 $F(x,y)=(\\varepsilon x, \\varepsilon y)$，其中 $\\varepsilon=10^{-4}$，$\\tau=10^{-10}$ 是一个小值，行列式为 $\\det(J) = \\varepsilon^2 = 10^{-8}$。由于 $10^{-8} > \\tau = 10^{-10}$，该映射始终保持定向。结果是 $+1$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classify_mapping(F, domain, grid_size, tau):\n    \"\"\"\n    Classifies a mapping's orientation behavior on a grid.\n    \n    Args:\n        F (callable): The mapping R^2 -> R^2, F(x, y) = (f1(x,y), f2(x,y)).\n        domain (tuple): A tuple (xmin, xmax, ymin, ymax) defining the rectangular domain.\n        grid_size (tuple): A tuple (Nx, Ny) with the number of grid points.\n        tau (float): The non-zero threshold for classification.\n        \n    Returns:\n        int: +1 for preserving, -1 for reversing, 0 for mixed/singular.\n    \"\"\"\n    xmin, xmax, ymin, ymax = domain\n    Nx, Ny = grid_size\n    \n    # Handle cases where grid is a single line/point. Derivatives require >1 point.\n    if Nx == 1 or Ny == 1:\n        # If Nx > 1 and Ny == 1, we can't compute y derivatives.\n        # As per problem, N_x, N_y are 21, so this is for robustness.\n        # Fallback to singular classification for ill-defined grids.\n        return 0\n\n    hx = (xmax - xmin) / (Nx - 1)\n    hy = (ymax - ymin) / (Ny - 1)\n    \n    x_coords = np.linspace(xmin, xmax, Nx)\n    y_coords = np.linspace(ymin, ymax, Ny)\n\n    # Use component functions for clarity\n    def F1(x, y):\n        return F(x, y)[0]\n\n    def F2(x, y):\n        return F(x, y)[1]\n\n    found_preserving = False\n    found_reversing = False\n    found_singular = False\n    \n    for i in range(Nx):\n        for j in range(Ny):\n            x = x_coords[i]\n            y = y_coords[j]\n            \n            # Calculate partial derivatives using appropriate finite difference schemes\n            \n            # Partial derivative with respect to x\n            if i == 0:  # Forward difference at left boundary\n                df1_dx = (F1(x + hx, y) - F1(x, y)) / hx\n                df2_dx = (F2(x + hx, y) - F2(x, y)) / hx\n            elif i == Nx - 1:  # Backward difference at right boundary\n                df1_dx = (F1(x, y) - F1(x - hx, y)) / hx\n                df2_dx = (F2(x, y) - F2(x - hx, y)) / hx\n            else:  # Central difference for interior\n                df1_dx = (F1(x + hx, y) - F1(x - hx, y)) / (2 * hx)\n                df2_dx = (F2(x + hx, y) - F2(x - hx, y)) / (2 * hx)\n            \n            # Partial derivative with respect to y\n            if j == 0:  # Forward difference at bottom boundary\n                df1_dy = (F1(x, y + hy) - F1(x, y)) / hy\n                df2_dy = (F2(x, y + hy) - F2(x, y)) / hy\n            elif j == Ny - 1:  # Backward difference at top boundary\n                df1_dy = (F1(x, y) - F1(x, y - hy)) / hy\n                df2_dy = (F2(x, y) - F2(x, y - hy)) / hy\n            else:  # Central difference for interior\n                df1_dy = (F1(x, y + hy) - F1(x, y - hy)) / (2 * hy)\n                df2_dy = (F2(x, y + hy) - F2(x, y - hy)) / (2 * hy)\n\n            # Calculate Jacobian determinant\n            det_J = df1_dx * df2_dy - df1_dy * df2_dx\n            \n            # Classify the point and update flags\n            if det_J > tau:\n                found_preserving = True\n            elif det_J < -tau:\n                found_reversing = True\n            else:\n                found_singular = True\n            \n            # Optimization: if mixed behavior is found, we know the result is 0\n            if found_singular or (found_preserving and found_reversing):\n                return 0\n    \n    # Aggregate results after checking all points\n    # The optimization above means we only reach here if all points are of one type.\n    if found_preserving and not found_reversing and not found_singular:\n        return 1\n    elif found_reversing and not found_preserving and not found_singular:\n        return -1\n    else:\n        # This branch handles any cases missed by the loop optimization,\n        # e.g., if the grid was only singular points.\n        return 0\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases\n    theta = np.pi / 4\n    c, s = np.cos(theta), np.sin(theta)\n    epsilon = 1e-4\n\n    test_cases = [\n        {\n            \"F\": lambda x, y: (x, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (c * x - s * y, s * x + c * y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (-x, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (x**2, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (epsilon * x, epsilon * y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = classify_mapping(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这项练习将展示单位矩阵在解决实际数值问题中的重要作用。我们将探讨如何通过给一个给定矩阵加上一个缩放后的单位矩阵 $\\lambda I$（一种称为“移位”的技术）来显著改善其数值特性。这项练习  对比了不同的计算方法，并揭示了一个简单的移位操作如何能够改善矩阵的条件数，从而加速用于大规模科学计算的迭代求解器的收敛速度。",
            "id": "3141142",
            "problem": "本题要求您使用两种互补的方法来研究位移矩阵逆的计算，并量化一个单位矩阵倍数的位移如何影响数值条件和迭代求解器的性能。核心研究对象是位移矩阵 $(A + \\lambda I)$ 的逆，其中 $A$ 是一个实对称正定矩阵，$I$ 是单位矩阵，$\\lambda$ 是一个实标量。您的任务是从基本定义出发，推导、实现并比较以下方法。\n\n基本概念：\n- 对于所有向量 $x$，单位矩阵 $I$ 满足 $Ix = x$。\n- 当矩阵 $(A + \\lambda I)^{-1}$ 的逆存在时，它被定义为满足 $(A + \\lambda I)M = I$ 的唯一矩阵 $M$。\n- 对于一个实对称矩阵 $A$，存在一个正交矩阵 $Q$，使得 $Q^{\\mathsf T} A Q$ 是一个对角元素为实数的对角矩阵。\n- 行列式 $\\det(M)$ 是线性变换 $M$ 对体积的缩放因子，对于三角矩阵，它等于对角元素的乘积。\n- 2-范数条件数 $\\kappa_2(M)$ 量化了解对扰动的敏感度，等于最大奇异值与最小奇异值之比；对于对称正定矩阵，它等于最大特征值与最小特征值之比。\n\n问题设置（为保证可复现性而固定）：\n- 按如下方式构造一个对称正定矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其中 $n = 20$。令 $R \\in \\mathbb{R}^{n \\times n}$ 的元素是由一个以 7 为种子的伪随机数生成器生成的独立标准正态分布数值。计算 $R$ 的简约QR分解（正交-上三角分解）以获得一个正交矩阵 $Q \\in \\mathbb{R}^{n \\times n}$。令 $d \\in \\mathbb{R}^n$ 是一个向量，其元素 $d_i$ 构成一个包含 $n$ 个项、从 $10^{-6}$ 到 $10^{2}$（含两端）的等比数列。定义 $A = Q \\,\\mathrm{diag}(d)\\, Q^{\\mathsf T}$。这样可以确保 $A$ 是对称正定的。\n- 定义一个右端向量 $b \\in \\mathbb{R}^n$，其元素是由一个以 13 为种子的伪随机数生成器生成的独立标准正态分布数值。\n- 考虑位移参数集 $\\Lambda = \\{\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4\\} = \\{0,\\;10^{-3},\\;1,\\;100\\}$。\n\n对每个 $\\lambda \\in \\Lambda$ 需要执行的任务：\n1. 仅使用 $A$ 的特征分解和基本定义，计算向量 $y_{\\mathrm{eig}}$，其等于 $(A + \\lambda I)^{-1} b$，过程中不显式构造任何矩阵的逆。\n2. 实现一个基于共轭梯度法的迭代求解器，用于求解对称正定系统，以计算近似满足 $(A + \\lambda I) y_{\\mathrm{cg}} = b$ 的 $y_{\\mathrm{cg}}$。使用以下停止准则：当相对残差 $\\lVert r_k \\rVert_2 / \\lVert b \\rVert_2 \\le 10^{-10}$（其中 $r_k = b - (A + \\lambda I) y_k$）时，或达到 200 次迭代上限时终止。记录所用的迭代次数。\n3. 计算相对误差 $e_{\\mathrm{rel}} = \\lVert y_{\\mathrm{cg}} - y_{\\mathrm{eig}} \\rVert_2 / \\lVert y_{\\mathrm{eig}} \\rVert_2$。\n4. 利用 $A$ 的构造所隐含的特征值以及 $\\lambda I$ 位移的定义，计算 2-范数条件数 $\\kappa_2(A + \\lambda I)$。\n5. 用两种方法计算行列式的自然对数 $\\log \\det(A + \\lambda I)$：(i) 基于特征分解的推理；(ii) 对 $(A + \\lambda I)$ 进行 Cholesky 分解，并利用三角矩阵行列式的性质。将这两种方法计算值的绝对差记为 $\\Delta_{\\log\\det}$。\n\n数值精度和输出格式：\n- 对于每个 $\\lambda \\in \\Lambda$，生成一个包含四个条目的结果列表，顺序如下：共轭梯度迭代的整数次数、浮点数 $e_{\\mathrm{rel}}$（四舍五入到小数点后 12 位）、浮点数 $\\kappa_2(A + \\lambda I)$（四舍五入到小数点后 6 位）以及浮点数 $\\Delta_{\\log\\det}$（四舍五入到小数点后 12 位）。\n- 您的程序应生成单行输出，其中包含所有 $\\lambda \\in \\Lambda$ 的结果，形式为这些列表的逗号分隔列表，并用方括号括起来，例如：$[\\,[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],\\dots\\,]$。\n\n测试套件：\n- 使用指定的固定矩阵 $A$ 和向量 $b$，其中 $n = 20$，种子分别为 7 和 13，位移集为 $\\Lambda = \\{0,\\;10^{-3},\\;1,\\;100\\}$。该套件涵盖：\n  - 无位移情况 $\\lambda = 0$（边界情况，条件最差）。\n  - 小位移 $\\lambda = 10^{-3}$（部分特征值聚集）。\n  - 中等位移 $\\lambda = 1$（强聚集）。\n  - 大位移 $\\lambda = 100$（接近均匀谱）。\n- 所有答案都应是如上所述的纯数值。不涉及物理单位。\n\n最终输出格式要求：\n- 您的程序必须只输出一行，内容为上述的聚合列表，其中每个浮点数都按指定要求四舍五入，并使用定点表示法格式化到指定的小数位数。",
            "solution": "用户提供的问题在科学上是合理的、适定的、客观且自洽的。所有必要的参数和方法都有明确定义，并基于数值线性代数的基本原理。这些任务构成了一个标准的、富有洞察力的数值实验，旨在探究位移矩阵的性质，特别是关于特征值分布、条件数以及迭代求解器性能的性质。该问题有效。\n\n下文将提供一个基于原理的详细解答。\n\n### 1. 预备知识和矩阵构造\n\n问题核心是分析位移矩阵 $(A + \\lambda I)$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定（SPD）矩阵，$I$ 是 $n \\times n$ 单位矩阵，$\\lambda$ 是一个实标量。\n\n矩阵 $A$ 的构造使其具有已知的特征系统。其定义为 $A = Q D Q^{\\mathsf T}$，其中：\n-   $n = 20$。\n-   $Q \\in \\mathbb{R}^{n \\times n}$ 是一个正交矩阵（$Q^{\\mathsf T} Q = Q Q^{\\mathsf T} = I$），由一个随机元素矩阵的 QR 分解得到。这确保了 $A$ 的特征向量（即 $Q$ 的列向量）是良好分布的。\n-   $D = \\mathrm{diag}(d_1, d_2, \\dots, d_n)$ 是一个对角矩阵，其对角元是 $A$ 的特征值。特征值向量 $d$ 被指定为一个包含 $n=20$ 项的等比数列，从 $d_1 = 10^{-6}$ 到 $d_n = 10^2$。该数列的公比 $r$ 由 $d_n = d_1 r^{n-1}$ 确定，即 $10^2 = 10^{-6} r^{19}$，或 $r^{19} = 10^8$。因此，$r = (10^8)^{1/19}$。\n\n由于所有特征值 $d_i$ 均为严格正数，矩阵 $A$ 是对称且正定的（SPD）。向量 $b \\in \\mathbb{R}^n$ 是一个固定的随机向量。\n\n### 2. 位移矩阵 $(A + \\lambda I)$ 的性质\n\n矩阵 $(A + \\lambda I)$ 的性质直接由 $A$ 的性质推导而来。设 $v_i$ 是 $A$ 对应于特征值 $d_i$ 的一个特征向量，使得 $A v_i = d_i v_i$。将位移矩阵作用于此特征向量，得到：\n$$\n(A + \\lambda I) v_i = A v_i + \\lambda I v_i = d_i v_i + \\lambda v_i = (d_i + \\lambda) v_i\n$$\n这表明 $(A + \\lambda I)$ 与 $A$ 具有相同的特征向量 $v_i$，其特征值则被 $\\lambda$ 位移，变为 $d_i' = d_i + \\lambda$。因此，位移矩阵的特征分解为：\n$$\nA + \\lambda I = Q (D + \\lambda I) Q^{\\mathsf T}\n$$\n由于所有 $d_i > 0$ 且给定的位移 $\\lambda \\ge 0$，所有特征值 $d_i' = d_i + \\lambda$ 都是严格正数。因此，对于所有 $\\lambda \\in \\Lambda = \\{0, 10^{-3}, 1, 100\\}$，$(A + \\lambda I)$ 也是对称正定的。\n\n### 3. 任务 1：通过特征分解求精确解\n\n第一个任务是求解线性系统 $(A + \\lambda I)y = b$ 以得到 $y_{\\mathrm{eig}}$，过程中不显式构造矩阵的逆。利用特征分解：\n$$\ny_{\\mathrm{eig}} = (A + \\lambda I)^{-1} b = \\left( Q (D + \\lambda I) Q^{\\mathsf T} \\right)^{-1} b\n$$\n乘积的逆等于逆的乘积，但顺序相反。对于正交矩阵，有 $Q^{-1} = Q^{\\mathsf T}$。\n$$\ny_{\\mathrm{eig}} = (Q^{\\mathsf T})^{-1} (D + \\lambda I)^{-1} Q^{-1} b = Q (D + \\lambda I)^{-1} Q^{\\mathsf T} b\n$$\n对角矩阵 $(D + \\lambda I)$ 的逆是一个对角元为 $1/(d_i + \\lambda)$ 的对角矩阵。计算过程分三步进行，避免了矩阵-矩阵乘法：\n1.  将右端向量 $b$ 变换到 $A$ 的特征基中：$\\hat{b} = Q^{\\mathsf T} b$。\n2.  在特征基中求解该系统，这是一个简单的逐元素除法：$\\hat{y}_i = \\hat{b}_i / (d_i + \\lambda)$。这得到了向量 $\\hat{y} = (D + \\lambda I)^{-1} \\hat{b}$。\n3.  将解向量变换回标准基：$y_{\\mathrm{eig}} = Q \\hat{y}$。\n\n此方法数值稳定且高效，可作为本问题的基准真相（ground truth）。\n\n### 4. 任务 2：通过共轭梯度（CG）法进行迭代求解\n\n共轭梯度（CG）法是一种迭代算法，用于求解线性方程组 $Mx=c$，其中矩阵 $M$ 是对称正定的。在我们的问题中，$M = A + \\lambda I$ 且 $c=b$。\n\n该算法从一个初始猜测 $y_0$（通常是零向量）开始，并迭代地优化解。在每次迭代 $k$ 中，它会计算一个新的解 $y_{k+1}$，该解在特定方向上使误差最小化。CG方法的关键特性是它找到的这些搜索方向是相互 $M$-正交的，这保证了在精确算术中最多 $n$ 步收敛。在浮点运算中，其性能由矩阵 $M$ 的条件数决定。\n\n具体的实现将遵循标准算法，当相对残差范数 $\\lVert r_k \\rVert_2 / \\lVert b \\rVert_2$ 低于 $10^{-10}$ 的容差时，或在最多 200 次迭代后终止。收敛所需的迭代次数是一个主要输出。\n\n### 5. 任务 3：相对误差计算\n\n相对误差 $e_{\\mathrm{rel}}$ 量化了CG解 $y_{\\mathrm{cg}}$ 相对于从特征分解得到的“精确”解 $y_{\\mathrm{eig}}$ 的准确性。其计算公式为：\n$$\ne_{\\mathrm{rel}} = \\frac{\\lVert y_{\\mathrm{cg}} - y_{\\mathrm{eig}} \\rVert_2}{\\lVert y_{\\mathrm{eig}} \\rVert_2}\n$$\n该度量标准直接衡量了迭代近似的质量。\n\n### 6. 任务 4：条件数计算\n\n一个SPD矩阵 $M$ 的 2-范数条件数 $\\kappa_2(M)$ 是其最大特征值与最小特征值之比：\n$$\n\\kappa_2(M) = \\frac{\\mu_{\\max}}{\\mu_{\\min}}\n$$\n对于我们的位移矩阵 $M = A + \\lambda I$，其特征值为 $d_i + \\lambda$。特征值 $d_i$ 按升序排列，从 $10^{-6}$ 到 $10^{2}$。因此，条件数为：\n$$\n\\kappa_2(A + \\lambda I) = \\frac{\\max_i(d_i + \\lambda)}{\\min_i(d_i + \\lambda)} = \\frac{(10^2) + \\lambda}{ (10^{-6}) + \\lambda}\n$$\n随着 $\\lambda$ 从 0 开始增加，该比率逐渐减小并趋近于 1。较小的条件数表示一个条件更好的矩阵，这通常会导致像CG这样的迭代求解器收敛更快。这是本问题旨在通过数值方式证明的核心理论点。\n\n### 7. 任务 5：对数行列式比较\n\n行列式的自然对数 $\\log \\det(A + \\lambda I)$ 通过两种方法计算，以验证数值上的一致性。\n\n**方法 (i)：使用特征值**\n矩阵的行列式是其特征值的乘积。\n$$\n\\det(A + \\lambda I) = \\prod_{i=1}^n (d_i + \\lambda)\n$$\n为避免在乘以多个数时可能发生的数值上溢或下溢，我们直接计算行列式的对数：\n$$\n\\log \\det(A + \\lambda I) = \\log\\left(\\prod_{i=1}^n (d_i + \\lambda)\\right) = \\sum_{i=1}^n \\log(d_i + \\lambda)\n$$\n\n**方法 (ii)：使用Cholesky分解**\n对于任何SPD矩阵 $M$，Cholesky分解会计算出一个唯一的、对角元为正数的下三角矩阵 $L$，使得 $M = L L^{\\mathsf T}$。行列式则为：\n$$\n\\det(M) = \\det(L L^{\\mathsf T}) = \\det(L) \\det(L^{\\mathsf T}) = (\\det(L))^2\n$$\n三角矩阵的行列式是其对角元素的乘积，因此 $\\det(L) = \\prod_{i=1}^n L_{ii}$。\n$$\n\\det(M) = \\left(\\prod_{i=1}^n L_{ii}\\right)^2\n$$\n同样，为了数值稳定性，最好使用对数进行计算：\n$$\n\\log \\det(M) = \\log\\left(\\left(\\prod_{i=1}^n L_{ii}\\right)^2\\right) = 2 \\log\\left(\\prod_{i=1}^n L_{ii}\\right) = 2 \\sum_{i=1}^n \\log(L_{ii})\n$$\n这两种方法所得值的绝对差 $\\Delta_{\\log\\det}$ 应在机器精度量级，这证实了两种理论方法及其数值实现的正确性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to perform the analysis of the shifted matrix (A + lambda*I).\n    It generates the matrix A, the vector b, and then iterates through a set of\n    shift parameters lambda to compute several quantities of interest.\n    \"\"\"\n\n    # Problem setup (fixed for reproducibility)\n    n = 20\n    seed_A = 7\n    seed_b = 13\n    lambda_set = [0.0, 1e-3, 1.0, 100.0]\n    \n    # Construct the symmetric positive definite matrix A\n    rng_A = np.random.default_rng(seed_A)\n    R = rng_A.standard_normal((n, n))\n    Q, _ = np.linalg.qr(R)\n    \n    d = np.geomspace(1e-6, 1e2, n)\n    D = np.diag(d)\n    A = Q @ D @ Q.T\n\n    # Define the right-hand side vector b\n    rng_b = np.random.default_rng(seed_b)\n    b = rng_b.standard_normal(n)\n\n    def conjugate_gradient(M, c, tol=1e-10, max_iter=200):\n        \"\"\"\n        Implements the Conjugate Gradient method for solving M*y = c.\n        - M: A symmetric positive definite matrix.\n        - c: The right-hand side vector.\n        - tol: The tolerance for the relative residual stopping criterion.\n        - max_iter: The maximum number of iterations.\n        Returns the solution vector y and the number of iterations performed.\n        \"\"\"\n        y = np.zeros_like(c)\n        r = c - M @ y\n        p = r.copy()\n        rs_old = r.T @ r\n        norm_c = np.linalg.norm(c)\n\n        if norm_c == 0:\n            return y, 0\n\n        num_iter = 0\n        for i in range(max_iter):\n            num_iter = i + 1\n            Mp = M @ p\n            alpha = rs_old / (p.T @ Mp)\n            y = y + alpha * p\n            r = r - alpha * Mp\n            \n            rs_new = r.T @ r\n            \n            if np.sqrt(rs_new) / norm_c < tol:\n                break\n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n            \n        return y, num_iter\n\n    all_results = []\n    \n    for lambda_val in lambda_set:\n        # Define the shifted matrix for the current lambda\n        M = A + lambda_val * np.eye(n)\n        \n        # Task 1: Compute y_eig using eigendecomposition\n        b_hat = Q.T @ b\n        y_hat = b_hat / (d + lambda_val)\n        y_eig = Q @ y_hat\n\n        # Task 2: Compute y_cg using Conjugate Gradient\n        y_cg, cg_iterations = conjugate_gradient(M, b, tol=1e-10, max_iter=200)\n\n        # Task 3: Compute relative error e_rel\n        norm_y_eig = np.linalg.norm(y_eig)\n        e_rel = np.linalg.norm(y_cg - y_eig) / norm_y_eig if norm_y_eig > 0 else 0.0\n\n        # Task 4: Compute 2-norm condition number kappa_2\n        shifted_eigenvalues = d + lambda_val\n        kappa_2 = np.max(shifted_eigenvalues) / np.min(shifted_eigenvalues)\n\n        # Task 5: Compute log-determinant difference\n        # (i) From eigendecomposition\n        log_det_eig = np.sum(np.log(shifted_eigenvalues))\n\n        # (ii) From Cholesky factorization\n        # Using scipy.linalg.cholesky as it is specified in allowed libraries.\n        # np.linalg.cholesky would also work.\n        try:\n            L = scipy.linalg.cholesky(M, lower=True)\n            log_det_chol = 2 * np.sum(np.log(np.diag(L)))\n            delta_log_det = np.abs(log_det_eig - log_det_chol)\n        except np.linalg.LinAlgError:\n            # This should not happen since M is SPD for lambda >= 0\n            delta_log_det = np.nan\n\n        all_results.append([cg_iterations, e_rel, kappa_2, delta_log_det])\n\n    # Format the final output string as per requirements\n    output_parts = []\n    for res in all_results:\n        iters, err, kappa, d_log_det = res\n        part = f\"[{iters},{err:.12f},{kappa:.6f},{d_log_det:.12f}]\"\n        output_parts.append(part)\n        \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}