{
    "hands_on_practices": [
        {
            "introduction": "Understanding the algebraic structure of matrix sets is fundamental in linear algebra. This exercise tests whether the set of invertible matrices forms a vector subspace by checking a key axiom: closure under addition. By considering a simple case involving the identity matrix and its additive inverse, you will demonstrate why this property fails and, in doing so, reinforce the critical link between a determinant of zero and the loss of invertibility .",
            "id": "28828",
            "problem": "Let $V = M_{2 \\times 2}(\\mathbb{R})$ be the vector space of all $2 \\times 2$ matrices with real entries. A subset $W$ of a vector space $V$ is a subspace if it satisfies three criteria:\n1. The zero vector of $V$ is in $W$.\n2. $W$ is closed under vector addition: for any $\\mathbf{u}, \\mathbf{v} \\in W$, their sum $\\mathbf{u} + \\mathbf{v}$ is also in $W$.\n3. $W$ is closed under scalar multiplication: for any $\\mathbf{u} \\in W$ and any scalar $c$, the product $c\\mathbf{u}$ is also in $W$.\n\nConsider the subset $W \\subset V$ consisting of all invertible $2 \\times 2$ matrices. A matrix $A$ is invertible if and only if its determinant is non-zero, i.e., $\\det(A) \\neq 0$.\n\nTo test if $W$ is a subspace, we can investigate its closure under addition. Take two matrices from $W$: the $2 \\times 2$ identity matrix, $I_2$, and its additive inverse, $-I_2$. Let their sum be denoted by the matrix $S$.\n\nCalculate the determinant of the resulting matrix $S$.",
            "solution": "We take two invertible matrices in $W$, namely the identity and its additive inverse, and form their sum. \n\nThe $2\\times2$ identity and its negative are\n$$\nI_2 = \\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}, \n\\quad\n-I_2 = \\begin{pmatrix}-1  0 \\\\ 0  -1\\end{pmatrix}.\n$$\n\nTheir sum is the zero matrix:\n$$\nS = I_2 + (-I_2)\n=\n\\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}\n+\n\\begin{pmatrix}-1  0 \\\\ 0  -1\\end{pmatrix}\n=\n\\begin{pmatrix}0  0 \\\\ 0  0\\end{pmatrix}.\n$$\n\nSince the determinant of the zero matrix is zero, we have\n$$\n\\det(S) = \\det(0_{2\\times2}) = 0.\n$$",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The formula for a matrix inverse, $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$, makes the determinant's central role explicit. This practice challenges you to explore the deeper implications of this formula within the realm of integer matrices. You will derive the necessary and sufficient condition on the determinant that guarantees an integer matrix will have an inverse that also contains only integer entries, revealing a fascinating intersection of linear algebra and number theory .",
            "id": "1361629",
            "problem": "Let $M$ be a $2 \\times 2$ matrix with all entries being integers. It is a known fact that for such a matrix, if its inverse $M^{-1}$ exists and also has all integer entries, then the determinant $\\det(M)$ must be an integer. However, the converse is not always true; an integer determinant does not guarantee an integer inverse.\n\nThe task is to identify which integer values of the determinant are *sufficient* to guarantee that the inverse of *any* such matrix $M$ (with that determinant) is also composed entirely of integers, and are also *necessary* for this property to hold true for the entire class of integer matrices. From the following list of potential values for $\\det(M)$, select all that fulfill this necessary and sufficient condition.\n\nA. $2$\n\nB. $1$\n\nC. $-1$\n\nD. $-2$\n\nE. $0$",
            "solution": "Let the $2 \\times 2$ matrix $M$ with integer entries be given by\n$$\nM = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}\n$$\nwhere $a, b, c, d$ are all integers, i.e., $a, b, c, d \\in \\mathbb{Z}$.\n\nThe determinant of $M$ is $\\det(M) = ad - bc$. Since $a, b, c, d$ are integers, their products and differences are also integers. Thus, $\\det(M)$ is an integer.\n\nIf $\\det(M) \\neq 0$, the inverse of $M$, denoted $M^{-1}$, is given by the formula:\n$$\nM^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix} = \\begin{pmatrix} \\frac{d}{\\det(M)}  \\frac{-b}{\\det(M)} \\\\ \\frac{-c}{\\det(M)}  \\frac{a}{\\det(M)} \\end{pmatrix}\n$$\n\nThe problem asks for the necessary and sufficient condition on the integer value of $\\det(M)$ that guarantees all entries of $M^{-1}$ are integers. Let's analyze this by considering the sufficiency and necessity of the condition.\n\nFirst, let's analyze the option E, where $\\det(M) = 0$. If the determinant of a matrix is zero, the matrix is singular, and its inverse does not exist. Therefore, option E is incorrect as the premise of having an inverse is not met.\n\n**Sufficiency Analysis:**\nLet's test if having a determinant of $1$ or $-1$ is a sufficient condition. Suppose $\\det(M) = k$, where $k \\in \\{1, -1\\}$.\nThe entries of $M^{-1}$ are of the form $\\frac{x}{k}$, where $x$ is one of the integers $d, -b, -c, a$.\nIf $k=1$, the entries of $M^{-1}$ are $d, -b, -c, a$, which are all integers.\nIf $k=-1$, the entries of $M^{-1}$ are $-d, b, c, -a$, which are also all integers.\nThus, if $\\det(M)$ is either $1$ or $-1$, it is guaranteed that $M^{-1}$ will have all integer entries. This means the conditions given in options B and C are sufficient.\n\n**Necessity Analysis:**\nNow, we must show that these are the only possible values. In other words, if an integer matrix $M$ has an integer inverse $M^{-1}$, is it necessary that $\\det(M)$ must be $1$ or $-1$?\n\nLet $M$ and $M^{-1}$ both be matrices with integer entries. From the definition of an inverse matrix, we have the identity $M M^{-1} = I$, where $I$ is the $2 \\times 2$ identity matrix.\nUsing the property that the determinant of a product of matrices is the product of their determinants, we have:\n$$\n\\det(M M^{-1}) = \\det(I)\n$$\n$$\n\\det(M) \\det(M^{-1}) = 1\n$$\nSince $M$ has integer entries, its determinant, $\\det(M)$, must be an integer.\nSimilarly, since $M^{-1}$ has integer entries, its determinant, $\\det(M^{-1})$, must also be an integer.\nLet $k_1 = \\det(M)$ and $k_2 = \\det(M^{-1})$, where $k_1, k_2 \\in \\mathbb{Z}$. The equation becomes:\n$$\nk_1 k_2 = 1\n$$\nThe only way the product of two integers can be $1$ is if they are either $(1, 1)$ or $(-1, -1)$.\nTherefore, it must be the case that $\\det(M)$ is either $1$ or $-1$.\nThis proves that the condition $\\det(M) \\in \\{1, -1\\}$ is necessary.\n\nAny other integer value for the determinant is not sufficient. For example, consider option A where $\\det(M) = 2$. We can construct a simple integer matrix with this determinant:\n$$\nM = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}\n$$\nIts determinant is $2 \\cdot 1 - 0 \\cdot 0 = 2$. Its inverse is:\n$$\nM^{-1} = \\frac{1}{2} \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThis inverse matrix does not have all integer entries. Therefore, $\\det(M)=2$ is not a sufficient condition. A similar counterexample can be constructed for any integer determinant $k$ where $|k|  1$, such as for options A ($2$) and D ($-2$).\n\nCombining both the sufficiency and necessity arguments, the only values for the determinant of an integer matrix that guarantee its inverse is also an integer matrix are $1$ and $-1$.\nLooking at the options provided:\nA. $2$: Not valid.\nB. $1$: Valid.\nC. $-1$: Valid.\nD. $-2$: Not valid.\nE. $0$: Not valid (no inverse exists).\n\nThus, the correct choices are B and C.",
            "answer": "$$\\boxed{BC}$$"
        },
        {
            "introduction": "The determinant's significance extends beyond algebra into geometry, where it quantifies how a transformation scales area and whether it preserves orientation. This hands-on coding problem applies this geometric insight to a practical computational task: verifying the behavior of a two-dimensional mapping. You will implement an algorithm to analyze the sign of the Jacobian determinant across a discrete grid, bridging abstract theory with its application in fields like computer graphics and continuum mechanics .",
            "id": "3141219",
            "problem": "You are given smooth functions that define mappings from the two-dimensional real plane to itself, written as $F:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$, where $F(x,y)=\\left(f_1(x,y),f_2(x,y)\\right)$. In the context of computational science, orientation preservation and local invertibility of such mappings can be analyzed through the Jacobian matrix and its determinant. The Jacobian matrix $J$ at a point $\\left(x,y\\right)$ is defined as the matrix of first-order partial derivatives of $F$ evaluated at $\\left(x,y\\right)$. The sign of $\\det J$ encodes whether the mapping preserves orientation ($\\det J0$), reverses orientation ($\\det J0$), or is locally singular ($\\det J=0$). Your task is to implement a robust computational test that, for a given mapping and rectangular domain, samples a uniform grid of points, approximates the entries of $J$ using finite differences, and classifies the mappingâ€™s orientation behavior everywhere on the grid.\n\nStarting point and fundamental base:\n- Use the definition of the Jacobian matrix $J$ as the matrix of partial derivatives of $F$.\n- Use well-known facts: the determinant $\\det J$ gives the signed area-scaling factor of the local linear approximation of $F$; local invertibility holds where $\\det J\\neq 0$, and orientation is preserved where $\\det J0$ and reversed where $\\det J0$.\n\nImplementation requirements:\n- Approximate the partial derivatives at each grid point using symmetric finite differences in the interior and one-sided differences at boundaries to maintain numerical stability.\n- Use a robust nonzero threshold $\\tau$ (a small positive real number) when classifying signs to mitigate floating-point and discretization errors. Specifically:\n  - Classify a grid point as orientation-preserving if $\\det J\\tau$.\n  - Classify a grid point as orientation-reversing if $\\det J-\\tau$.\n  - Classify a grid point as indeterminate/singular if $|\\det J|\\le \\tau$.\n- The mapping is classified over the entire grid by aggregating per-point classifications:\n  - Output $+1$ if all grid points are orientation-preserving.\n  - Output $-1$ if all grid points are orientation-reversing.\n  - Output $0$ otherwise (mixed signs and/or near-zero determinants occur anywhere on the grid).\n\nGrid sampling:\n- Use uniform sampling of a closed rectangular domain $\\left[x_{\\min},x_{\\max}\\right]\\times\\left[y_{\\min},y_{\\max}\\right]$ with $N_x$ points in the $x$-direction and $N_y$ points in the $y$-direction, inclusive of endpoints.\n- Let the spacings be $h_x=\\dfrac{x_{\\max}-x_{\\min}}{N_x-1}$ and $h_y=\\dfrac{y_{\\max}-y_{\\min}}{N_y-1}$.\n\nTest suite:\nImplement your program to classify the following five mappings, domains, and grid resolutions. For the rotation mapping, use angles in radians.\n- Case $1$ (identity, happy path): $F(x,y)=\\left(x,y\\right)$ on $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ with $N_x=21$, $N_y=21$, and $\\tau=10^{-10}$.\n- Case $2$ (rotation by $\\pi/4$, happy path): $F(x,y)=\\left(\\cos\\left(\\frac{\\pi}{4}\\right)x-\\sin\\left(\\frac{\\pi}{4}\\right)y,\\ \\sin\\left(\\frac{\\pi}{4}\\right)x+\\cos\\left(\\frac{\\pi}{4}\\right)y\\right)$ on $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ with $N_x=21$, $N_y=21$, and $\\tau=10^{-10}$.\n- Case $3$ (reflection, orientation reversing): $F(x,y)=\\left(-x,y\\right)$ on $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ with $N_x=21$, $N_y=21$, and $\\tau=10^{-10}$.\n- Case $4$ (nonlinear, mixed and singular): $F(x,y)=\\left(x^2,y\\right)$ on $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ with $N_x=21$, $N_y=21$, and $\\tau=10^{-10}$.\n- Case $5$ (uniform scaling with small determinant, boundary and numerical robustness): $F(x,y)=\\left(\\varepsilon x,\\varepsilon y\\right)$ with $\\varepsilon=10^{-4}$ on $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ with $N_x=21$, $N_y=21$, and $\\tau=10^{-10}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the classification results for the five cases as a comma-separated list enclosed in square brackets; for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$, where each entry is one of $+1$, $-1$, or $0$ as defined above.\n- No physical units are involved, and no angles other than the specified radian angle appear. The outputs are integers.\n\nScientific realism and derivation emphasis:\n- The program must derive orientation classification from the fundamental definitions of the Jacobian and determinant and must not rely on any precomputed analytical determinant formula for the test suite.\n- The grid-based approximation must be implemented in a way that is self-consistent, numerically plausible, and robust to boundary conditions and small determinants.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in multivariable calculus and numerical analysis, well-posed with a clear objective and constraints, and free of contradictions or ambiguities. The problem requires the implementation of a computational method to classify the orientation-preserving nature of two-dimensional mappings by analyzing the determinant of the Jacobian matrix, which is a standard and meaningful task in computational science.\n\nThe solution proceeds by first establishing the theoretical foundation, then detailing the numerical algorithm, and finally applying it to the specified test cases.\n\n### 1. Theoretical Foundation: Jacobian Matrix and Orientation\n\nA smooth mapping $F: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$, defined by $F(x,y) = (f_1(x,y), f_2(x,y))$, can be locally approximated by a linear transformation at any point $(x_0, y_0)$. The matrix representing this linear transformation is the Jacobian matrix, $J$, evaluated at that point. The Jacobian matrix is the matrix of all first-order partial derivatives of the vector-valued function $F$:\n\n$$\nJ(x,y) =\n\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x}(x,y)  \\frac{\\partial f_1}{\\partial y}(x,y) \\\\\n\\frac{\\partial f_2}{\\partial x}(x,y)  \\frac{\\partial f_2}{\\partial y}(x,y)\n\\end{pmatrix}\n$$\n\nThe determinant of the Jacobian matrix, $\\det(J)$, is a scalar value that describes how the mapping scales area locally. The sign of the determinant indicates the effect of the mapping on orientation:\n- If $\\det(J)  0$, the mapping is **orientation-preserving**. It transforms a small region without \"flipping\" it.\n- If $\\det(J)  0$, the mapping is **orientation-reversing**. It \"flips\" the orientation of a small region, akin to a reflection.\n- If $\\det(J) = 0$, the mapping is **singular** at that point. It collapses area, and the Inverse Function Theorem does not guarantee local invertibility.\n\n### 2. Numerical Approximation of the Jacobian\n\nSince we must create a general tool without relying on analytical derivatives, we approximate the partial derivatives numerically using finite differences. We operate on a discrete grid of points $(x_i, y_j)$ spanning the domain $[x_{\\min}, x_{\\max}] \\times [y_{\\min}, y_{\\max}]$. The grid spacings are $h_x = \\frac{x_{\\max} - x_{\\min}}{N_x - 1}$ and $h_y = \\frac{y_{\\max} - y_{\\min}}{N_y - 1}$, where $N_x$ and $N_y$ are the number of points in each direction.\n\nTo maintain numerical accuracy and stability across the entire grid, we use different finite difference formulas for interior points and boundary points.\n\n**For an interior point $(x_i, y_j)$ where $0  i  N_x-1$ and $0  j  N_y-1$:**\nWe use the second-order accurate **symmetric (central) difference** formula. For a generic function $g(x,y)$:\n$$ \\frac{\\partial g}{\\partial x}(x_i, y_j) \\approx \\frac{g(x_i + h_x, y_j) - g(x_i - h_x, y_j)}{2h_x} $$\n$$ \\frac{\\partial g}{\\partial y}(x_i, y_j) \\approx \\frac{g(x_i, y_j + h_y) - g(x_i, y_j - h_y)}{2h_y} $$\n\n**For boundary points:**\nWe must use one-sided formulas.\n- At the left boundary ($i=0$): **Forward difference** for $\\frac{\\partial}{\\partial x}$.\n  $$ \\frac{\\partial g}{\\partial x}(x_0, y_j) \\approx \\frac{g(x_0 + h_x, y_j) - g(x_0, y_j)}{h_x} $$\n- At the right boundary ($i=N_x-1$): **Backward difference** for $\\frac{\\partial}{\\partial x}$.\n  $$ \\frac{\\partial g}{\\partial x}(x_{N_x-1}, y_j) \\approx \\frac{g(x_{N_x-1}, y_j) - g(x_{N_x-1} - h_x, y_j)}{h_x} $$\nAnalogous forward and backward difference formulas are used for $\\frac{\\partial}{\\partial y}$ at the bottom ($j=0$) and top ($j=N_y-1$) boundaries, respectively. These formulas are first-order accurate.\n\n### 3. Algorithmic Procedure\n\nThe overall algorithm to classify a given mapping $F$ on a specified domain is as follows:\n\n1.  **Grid Generation**: Define the sampling grid by creating two arrays of coordinates, one for the $x$-axis from $x_{\\min}$ to $x_{\\max}$ with $N_x$ points, and one for the $y$-axis from $y_{\\min}$ to $y_{\\max}$ with $N_y$ points. Calculate step sizes $h_x$ and $h_y$.\n\n2.  **Iterate and Classify**: Loop through each point $(x_i, y_j)$ on the grid. For each point:\n    a.  Calculate the four partial derivatives that form the Jacobian matrix: $\\frac{\\partial f_1}{\\partial x}$, $\\frac{\\partial f_1}{\\partial y}$, $\\frac{\\partial f_2}{\\partial x}$, and $\\frac{\\partial f_2}{\\partial y}$. Select the appropriate finite difference formula (central, forward, or backward) based on the point's position $(i,j)$ relative to the grid boundaries.\n    b.  Construct the Jacobian matrix $J_{ij}$ from these four approximate values.\n    c.  Compute its determinant: $\\det(J_{ij}) = \\frac{\\partial f_1}{\\partial x}\\frac{\\partial f_2}{\\partial y} - \\frac{\\partial f_1}{\\partial y}\\frac{\\partial f_2}{\\partial x}$.\n    d.  Classify the point's local behavior using the given threshold $\\tau$:\n        - If $\\det(J_{ij})  \\tau$, the point is `orientation-preserving`.\n        - If $\\det(J_{ij})  -\\tau$, the point is `orientation-reversing`.\n        - If $|\\det(J_{ij})| \\le \\tau$, the point is `indeterminate/singular`.\n    e.  Record the classification type found. We only need to know if we have encountered at least one of each type.\n\n3.  **Aggregate Results**: After evaluating all points on the grid, combine the individual classifications into a single result for the entire mapping over the domain:\n    a.  If all points were classified as `orientation-preserving`, the final output is $+1$.\n    b.  If all points were classified as `orientation-reversing`, the final output is $-1$.\n    c.  If there is any mix of classifications (e.g., some preserving and some reversing) or if any point was classified as `indeterminate/singular`, the final output is $0$.\n\nThis procedure is implemented for each of the five test cases provided in the problem statement. The results are then aggregated into the required final format.\n- For $F(x,y)=(x,y)$, $\\det(J) = 1$, which is greater than any small positive $\\tau$. The result is $+1$.\n- For the rotation, $\\det(J) = \\cos^2(\\frac{\\pi}{4}) + \\sin^2(\\frac{\\pi}{4}) = 1$. The result is $+1$.\n- For $F(x,y)=(-x,y)$, $\\det(J) = -1$, which is less than any small negative $-\\tau$. The result is $-1$.\n- For $F(x,y)=(x^2,y)$, $\\det(J) = 2x$. On the domain $[-1,1]\\times[-1,1]$, this determinant takes positive, negative, and zero values. Thus, it is a mixed case, resulting in $0$.\n- For $F(x,y)=(\\varepsilon x, \\varepsilon y)$ with $\\varepsilon=10^{-4}$ and $\\tau=10^{-10}$, a small value, the determinant is $\\det(J) = \\varepsilon^2 = 10^{-8}$. Since $10^{-8}  \\tau = 10^{-10}$, the mapping is consistently orientation-preserving. The result is $+1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classify_mapping(F, domain, grid_size, tau):\n    \"\"\"\n    Classifies a mapping's orientation behavior on a grid.\n    \n    Args:\n        F (callable): The mapping R^2 - R^2, F(x, y) = (f1(x,y), f2(x,y)).\n        domain (tuple): A tuple (xmin, xmax, ymin, ymax) defining the rectangular domain.\n        grid_size (tuple): A tuple (Nx, Ny) with the number of grid points.\n        tau (float): The non-zero threshold for classification.\n        \n    Returns:\n        int: +1 for preserving, -1 for reversing, 0 for mixed/singular.\n    \"\"\"\n    xmin, xmax, ymin, ymax = domain\n    Nx, Ny = grid_size\n    \n    # Handle cases where grid is a single line/point. Derivatives require 1 point.\n    if Nx = 1 or Ny = 1:\n        # If Nx  1 and Ny = 1, we can't compute y derivatives.\n        # As per problem, N_x, N_y are 21, so this is for robustness.\n        # Fallback to singular classification for ill-defined grids.\n        return 0\n\n    hx = (xmax - xmin) / (Nx - 1)\n    hy = (ymax - ymin) / (Ny - 1)\n    \n    x_coords = np.linspace(xmin, xmax, Nx)\n    y_coords = np.linspace(ymin, ymax, Ny)\n\n    # Use component functions for clarity\n    def F1(x, y):\n        return F(x, y)[0]\n\n    def F2(x, y):\n        return F(x, y)[1]\n\n    found_preserving = False\n    found_reversing = False\n    found_singular = False\n    \n    for i in range(Nx):\n        for j in range(Ny):\n            x = x_coords[i]\n            y = y_coords[j]\n            \n            # Calculate partial derivatives using appropriate finite difference schemes\n            \n            # Partial derivative with respect to x\n            if i == 0:  # Forward difference at left boundary\n                df1_dx = (F1(x + hx, y) - F1(x, y)) / hx\n                df2_dx = (F2(x + hx, y) - F2(x, y)) / hx\n            elif i == Nx - 1:  # Backward difference at right boundary\n                df1_dx = (F1(x, y) - F1(x - hx, y)) / hx\n                df2_dx = (F2(x, y) - F2(x - hx, y)) / hx\n            else:  # Central difference for interior\n                df1_dx = (F1(x + hx, y) - F1(x - hx, y)) / (2 * hx)\n                df2_dx = (F2(x + hx, y) - F2(x - hx, y)) / (2 * hx)\n            \n            # Partial derivative with respect to y\n            if j == 0:  # Forward difference at bottom boundary\n                df1_dy = (F1(x, y + hy) - F1(x, y)) / hy\n                df2_dy = (F2(x, y + hy) - F2(x, y)) / hy\n            elif j == Ny - 1:  # Backward difference at top boundary\n                df1_dy = (F1(x, y) - F1(x, y - hy)) / hy\n                df2_dy = (F2(x, y) - F2(x, y - hy)) / hy\n            else:  # Central difference for interior\n                df1_dy = (F1(x, y + hy) - F1(x, y - hy)) / (2 * hy)\n                df2_dy = (F2(x, y + hy) - F2(x, y - hy)) / (2 * hy)\n\n            # Calculate Jacobian determinant\n            det_J = df1_dx * df2_dy - df1_dy * df2_dx\n            \n            # Classify the point and update flags\n            if det_J  tau:\n                found_preserving = True\n            elif det_J  -tau:\n                found_reversing = True\n            else:\n                found_singular = True\n            \n            # Optimization: if mixed behavior is found, we know the result is 0\n            if found_singular or (found_preserving and found_reversing):\n                return 0\n    \n    # Aggregate results after checking all points\n    # The optimization above means we only reach here if all points are of one type.\n    if found_preserving and not found_reversing and not found_singular:\n        return 1\n    elif found_reversing and not found_preserving and not found_singular:\n        return -1\n    else:\n        # This branch handles any cases missed by the loop optimization,\n        # e.g., if the grid was only singular points.\n        return 0\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases\n    theta = np.pi / 4\n    c, s = np.cos(theta), np.sin(theta)\n    epsilon = 1e-4\n\n    test_cases = [\n        {\n            \"F\": lambda x, y: (x, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (c * x - s * y, s * x + c * y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (-x, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (x**2, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (epsilon * x, epsilon * y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = classify_mapping(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}