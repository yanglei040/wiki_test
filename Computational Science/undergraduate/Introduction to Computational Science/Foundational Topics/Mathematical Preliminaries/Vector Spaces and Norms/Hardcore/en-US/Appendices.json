{
    "hands_on_practices": [
        {
            "introduction": "In computational science, we often work with functions as our primary objects of study, such as the solution to a differential equation or a signal over time. To quantify how well an approximation matches a true solution, we need a way to measure the \"distance\" between two functions. This exercise  introduces the $L_1$ norm, a fundamental tool from functional analysis that allows us to calculate this distance by integrating the absolute difference between the functions.",
            "id": "2308541",
            "problem": "In the field of functional analysis, we can measure the \"distance\" between functions in various ways. Consider the vector space $C[0,1]$, which consists of all continuous real-valued functions defined on the closed interval $[0, 1]$.\n\nOne common way to define distance is by using the $L_1$ norm. For any function $f(x)$ in $C[0,1]$, its $L_1$ norm, denoted by $\\|f\\|_1$, is defined as:\n$$\n\\|f\\|_1 = \\int_{0}^{1} |f(x)| dx\n$$\nThe distance between two functions, $p(x)$ and $q(x)$, in this space is then given by the norm of their difference, i.e., $d(p, q) = \\|p - q\\|_1$.\n\nCalculate the exact distance between the function $p(x) = x$ and the constant function $q(x) = \\frac{1}{2}$ in the space $C[0,1]$ with respect to the $L_1$ norm. Express your answer as a fraction in simplest terms.",
            "solution": "We are given $p(x)=x$ and $q(x)=\\frac{1}{2}$ on $[0,1]$. The $L_{1}$ distance is\n$$\nd(p,q)=\\|p-q\\|_{1}=\\int_{0}^{1}|p(x)-q(x)|\\,dx=\\int_{0}^{1}\\left|x-\\frac{1}{2}\\right|\\,dx.\n$$\nThe absolute value splits at $x=\\frac{1}{2}$, with $\\left|x-\\frac{1}{2}\\right|=\\frac{1}{2}-x$ for $x\\in[0,\\frac{1}{2}]$ and $\\left|x-\\frac{1}{2}\\right|=x-\\frac{1}{2}$ for $x\\in[\\frac{1}{2},1]$. Therefore,\n$$\n\\int_{0}^{1}\\left|x-\\frac{1}{2}\\right|\\,dx=\\int_{0}^{\\frac{1}{2}}\\left(\\frac{1}{2}-x\\right)\\,dx+\\int_{\\frac{1}{2}}^{1}\\left(x-\\frac{1}{2}\\right)\\,dx.\n$$\nCompute each integral explicitly. For the first,\n$$\n\\int_{0}^{\\frac{1}{2}}\\left(\\frac{1}{2}-x\\right)\\,dx=\\left[\\frac{1}{2}x-\\frac{x^{2}}{2}\\right]_{0}^{\\frac{1}{2}}=\\left(\\frac{1}{2}\\cdot\\frac{1}{2}-\\frac{\\left(\\frac{1}{2}\\right)^{2}}{2}\\right)-0=\\frac{1}{4}-\\frac{1}{8}=\\frac{1}{8}.\n$$\nFor the second,\n$$\n\\int_{\\frac{1}{2}}^{1}\\left(x-\\frac{1}{2}\\right)\\,dx=\\left[\\frac{x^{2}}{2}-\\frac{1}{2}x\\right]_{\\frac{1}{2}}^{1}=\\left(\\frac{1}{2}-\\frac{1}{2}\\right)-\\left(\\frac{\\left(\\frac{1}{2}\\right)^{2}}{2}-\\frac{1}{2}\\cdot\\frac{1}{2}\\right)=0-\\left(\\frac{1}{8}-\\frac{1}{4}\\right)=\\frac{1}{8}.\n$$\nAdding these yields\n$$\n\\int_{0}^{1}\\left|x-\\frac{1}{2}\\right|\\,dx=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\nThus the exact $L_{1}$ distance is $\\frac{1}{4}$.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "While the Euclidean ($L_2$) norm is familiar, other norms like the $L_1$ norm offer different ways to measure distance, emphasizing different geometric properties. This choice is not merely academic; it has direct consequences for many algorithms, particularly in data science and machine learning. This hands-on coding exercise  will guide you through constructing a scenario where the $L_1$ and $L_2$ norms identify different 'nearest neighbors,' demonstrating how the underlying geometry of a norm can alter the outcome of a classification task.",
            "id": "3201753",
            "problem": "You will work in the finite-dimensional real vector space $\\mathbb{R}^d$ and compare the behavior of the induced metrics from two norms: the $L^1$ norm and the $L^2$ norm. The $L^1$ norm is defined by $\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^d |x_i|$ and the $L^2$ norm is defined by $\\|\\mathbf{x}\\|_2 = \\left(\\sum_{i=1}^d x_i^2\\right)^{1/2}$. For any norm $\\|\\cdot\\|$, the induced metric on $\\mathbb{R}^d$ is $d(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x}-\\mathbf{y}\\|$.\n\nConstruct a dataset parameterized by integers and real numbers $d \\in \\mathbb{N}$, $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$, and $k \\in \\mathbb{N}$, as follows:\n- The query point is the origin $\\mathbf{0} \\in \\mathbb{R}^d$.\n- Define the axis-aligned points $\\mathcal{A} = \\{a \\mathbf{e}_i : i \\in \\{1,2,\\dots,d\\}\\}$, where $\\mathbf{e}_i$ is the $i$-th standard basis vector in $\\mathbb{R}^d$. Assign each point in $\\mathcal{A}$ the class label $0$.\n- Define a single diagonal point $\\mathbf{u} = b(\\mathbf{e}_1+\\mathbf{e}_2)$ and assign it the class label $1$.\n- Index the points in the order: axis points first with indices $1,2,\\dots,d$, then the diagonal point with index $d+1$.\n\nLet the comparison be between the metrics $d_1(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x}-\\mathbf{y}\\|_1$ and $d_2(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x}-\\mathbf{y}\\|_2$. Among equal-distance ties, break ties by choosing the point with the smallest index. For the downstream algorithm, use the $k$-Nearest Neighbors (KNN) majority-vote classifier applied to the query point $\\mathbf{0}$, where the class label is the majority among the labels of the $k$ nearest neighbors; if there is a tie in the majority vote, choose the smaller integer label.\n\nYour program must:\n- Compute the nearest neighbor of $\\mathbf{0}$ under $d_1$ and under $d_2$ and output whether they differ as an integer ($1$ if different, $0$ if the same).\n- Compute the sets of indices of the $k$ nearest neighbors of $\\mathbf{0}$ under $d_1$ and under $d_2$, and output the size of their symmetric difference as an integer.\n- Compute the KNN majority-vote class label under $d_1$ and under $d_2$ for the query $\\mathbf{0}$ and output whether the classification result changes as an integer ($1$ if different, $0$ if the same).\n\nUse the following test suite of parameter values to exercise different behaviors:\n- Test case $1$: $d=2$, $a=\\frac{3}{2}$, $b=1$, $k=3$.\n- Test case $2$: $d=2$, $a=\\sqrt{2}$, $b=1$, $k=2$.\n- Test case $3$: $d=3$, $a=1$, $b=\\frac{1}{5}$, $k=2$.\n- Test case $4$: $d=3$, $a=3$, $b=2$, $k=1$.\n\nYour program should produce a single line of output containing a comma-separated list of the results for each test case, where each test case result is itself a triple formatted as $[x_1,x_2,x_3]$ with no spaces, in the same order as the test suite above. The final output must therefore be a single list of lists, for example, $[[x_{1,1},x_{1,2},x_{1,3}],[x_{2,1},x_{2,2},x_{2,3}],\\dots]$. No physical units or angle units apply in this problem, and all returned values must be integers.",
            "solution": "The user-provided problem is rigorously validated and found to be self-contained, scientifically sound, and well-posed. All required data, definitions, and conditions are explicitly stated, and there are no contradictions, ambiguities, or violations of scientific or mathematical principles. The problem is a standard exercise in computational science, specifically exploring the effect of different norms on the geometry of a vector space and its implications for distance-based algorithms like $k$-Nearest Neighbors (KNN). I will now proceed with a complete solution.\n\nThe core of the problem lies in comparing the ranking of distances from a query point to a set of data points, using two different metrics: $d_1$ induced by the $L^1$ norm and $d_2$ induced by the $L^2$ norm. The query point is the origin $\\mathbf{q} = \\mathbf{0} \\in \\mathbb{R}^d$. The distance from the origin to any point $\\mathbf{p}$ simplifies to its norm, i.e., $d(\\mathbf{0}, \\mathbf{p}) = \\|\\mathbf{p}\\|$.\n\nThe dataset consists of two types of points:\n1.  A set of $d$ axis-aligned points, $\\mathcal{A} = \\{ \\mathbf{p}_i = a \\mathbf{e}_i : i \\in \\{1, \\dots, d\\} \\}$.\n2.  A single diagonal point, $\\mathbf{u} = \\mathbf{p}_{d+1} = b(\\mathbf{e}_1 + \\mathbf{e}_2)$.\n\nLet us compute the distances of these points from the origin $\\mathbf{0}$ under both norms.\n\nFor any axis-aligned point $\\mathbf{p}_i = a \\mathbf{e}_i$:\nThe $L^1$ distance is $d_1(\\mathbf{0}, \\mathbf{p}_i) = \\|a \\mathbf{e}_i\\|_1 = \\sum_{j=1}^d |(a \\mathbf{e}_i)_j| = |a|$.\nThe $L^2$ distance is $d_2(\\mathbf{0}, \\mathbf{p}_i) = \\|a \\mathbf{e}_i\\|_2 = \\left(\\sum_{j=1}^d (a \\mathbf{e}_i)_j^2\\right)^{1/2} = \\sqrt{a^2} = |a|$.\nFor all axis-aligned points, the $L^1$ and $L^2$ distances to the origin are identical.\n\nFor the diagonal point $\\mathbf{u} = b(\\mathbf{e}_1 + \\mathbf{e}_2) = (b, b, 0, \\dots, 0)$:\nThe $L^1$ distance is $d_1(\\mathbf{0}, \\mathbf{u}) = \\|\\mathbf{u}\\|_1 = |b| + |b| = 2|b|$.\nThe $L^2$ distance is $d_2(\\mathbf{0}, \\mathbf{u}) = \\|\\mathbf{u}\\|_2 = \\sqrt{b^2 + b^2} = \\sqrt{2b^2} = \\sqrt{2}|b|$.\n\nThe relative ordering of neighbors depends on the comparison between $|a|$ and $2|b|$ for the $L^1$ metric, and between $|a|$ and $\\sqrt{2}|b|$ for the $L^2$ metric. This difference is the source of all variations observed in the problem. We now analyze each test case.\n\n**Test Case 1: $d=2$, $a=3/2$, $b=1$, $k=3$**\nThe points are $\\mathbf{p}_1=(3/2, 0)$ (index $1$, label $0$), $\\mathbf{p}_2=(0, 3/2)$ (index $2$, label $0$), and $\\mathbf{p}_3=\\mathbf{u}=(1, 1)$ (index $3$, label $1$).\nThe distances from the origin are:\n- For $\\mathbf{p}_1, \\mathbf{p}_2$: $d_1 = d_2 = |a| = |3/2| = 1.5$.\n- For $\\mathbf{p}_3=\\mathbf{u}$: $d_1 = 2|b| = 2|1| = 2$ and $d_2 = \\sqrt{2}|b| = \\sqrt{2}|1| \\approx 1.414$.\n\nOrdering of neighbors (from nearest to farthest, using smallest index for ties):\n- Under $d_1$: Distances are $1.5$ (for indices $1,2$) and $2$ (for index $3$). Sorted list of indices: $[1, 2, 3]$.\n- Under $d_2$: Distances are $1.5$ (for indices $1,2$) and $\\sqrt{2}$ (for index $3$). Since $\\sqrt{2}  1.5$, the order is different. Sorted list of indices: $[3, 1, 2]$.\n\n1.  **Nearest Neighbor Comparison ($x_1$)**: The nearest neighbor under $d_1$ is $\\mathbf{p}_1$ (index $1$). The nearest neighbor under $d_2$ is $\\mathbf{p}_3$ (index $3$). They differ. Thus, $x_1=1$.\n2.  **Symmetric Difference ($x_2$)**: For $k=3$, the set of nearest neighbors under $d_1$ is $N_1=\\{1, 2, 3\\}$. Under $d_2$, it is $N_2=\\{3, 1, 2\\}$. These sets are identical ($N_1=N_2$). The size of their symmetric difference is $0$. Thus, $x_2=0$.\n3.  **KNN Classification ($x_3$)**: For $k=3$, we use neighbors $\\{1, 2, 3\\}$. The labels are $\\{0, 0, 1\\}$. The majority vote is label $0$. This holds for both $d_1$ and $d_2$. The classification does not change. Thus, $x_3=0$.\nResult for Test Case 1: $[1, 0, 0]$.\n\n**Test Case 2: $d=2$, $a=\\sqrt{2}$, $b=1$, $k=2$**\nThe points are $\\mathbf{p}_1=(\\sqrt{2}, 0)$ (index $1$, label $0$), $\\mathbf{p}_2=(0, \\sqrt{2})$ (index $2$, label $0$), and $\\mathbf{p}_3=\\mathbf{u}=(1, 1)$ (index $3$, label $1$).\nThe distances from the origin are:\n- For $\\mathbf{p}_1, \\mathbf{p}_2$: $d_1 = d_2 = |a| = |\\sqrt{2}| = \\sqrt{2}$.\n- For $\\mathbf{p}_3=\\mathbf{u}$: $d_1 = 2|b| = 2|1| = 2$ and $d_2 = \\sqrt{2}|b| = \\sqrt{2}|1| = \\sqrt{2}$.\n\nOrdering of neighbors:\n- Under $d_1$: Distances are $\\sqrt{2}$ (for indices $1,2$) and $2$ (for index $3$). Sorted list of indices: $[1, 2, 3]$.\n- Under $d_2$: All three points are equidistant at distance $\\sqrt{2}$. We break ties by index. Sorted list of indices: $[1, 2, 3]$.\n\n1.  **Nearest Neighbor Comparison ($x_1$)**: The nearest neighbor is $\\mathbf{p}_1$ (index $1$) for both metrics. They are the same. Thus, $x_1=0$.\n2.  **Symmetric Difference ($x_2$)**: For $k=2$, the set of nearest neighbors under $d_1$ is $N_1=\\{1, 2\\}$. Under $d_2$, it is $N_2=\\{1, 2\\}$. The sets are identical. The size of their symmetric difference is $0$. Thus, $x_2=0$.\n3.  **KNN Classification ($x_3$)**: For $k=2$, we use neighbors $\\{1, 2\\}$. The labels are $\\{0, 0\\}$. The majority vote is label $0$ for both metrics. The classification does not change. Thus, $x_3=0$.\nResult for Test Case 2: $[0, 0, 0]$.\n\n**Test Case 3: $d=3$, $a=1$, $b=1/5$, $k=2$**\nThe points are $\\mathbf{p}_1=(1,0,0)$, $\\mathbf{p}_2=(0,1,0)$, $\\mathbf{p}_3=(0,0,1)$ (indices $1,2,3$, label $0$) and $\\mathbf{p}_4=\\mathbf{u}=(1/5, 1/5, 0)$ (index $4$, label $1$).\nThe distances from the origin are:\n- For $\\mathbf{p}_1, \\mathbf{p}_2, \\mathbf{p}_3$: $d_1 = d_2 = |a| = |1| = 1$.\n- For $\\mathbf{p}_4=\\mathbf{u}$: $d_1 = 2|b| = 2|1/5| = 0.4$ and $d_2 = \\sqrt{2}|b| = \\sqrt{2}|1/5| \\approx 0.283$.\n\nOrdering of neighbors:\n- Under $d_1$: Distances are $1$ (for indices $1,2,3$) and $0.4$ (for index $4$). Sorted list of indices: $[4, 1, 2, 3]$.\n- Under $d_2$: Distances are $1$ (for indices $1,2,3$) and $\\sqrt{2}/5$ (for index $4$). Since $\\sqrt{2}/5  1$, the order is the same. Sorted list of indices: $[4, 1, 2, 3]$.\n\n1.  **Nearest Neighbor Comparison ($x_1$)**: The nearest neighbor is $\\mathbf{p}_4$ (index $4$) for both metrics. They are the same. Thus, $x_1=0$.\n2.  **Symmetric Difference ($x_2$)**: For $k=2$, the set of nearest neighbors is $N_1=N_2=\\{4, 1\\}$ for both metrics. The symmetric difference is empty. Thus, $x_2=0$.\n3.  **KNN Classification ($x_3$)**: For $k=2$, we use neighbors $\\{4, 1\\}$. The labels are $\\{1, 0\\}$. There is a tie in the majority vote. The tie-breaking rule specifies choosing the smaller integer label, so the class is $0$. This holds for both metrics. The classification does not change. Thus, $x_3=0$.\nResult for Test Case 3: $[0, 0, 0]$.\n\n**Test Case 4: $d=3$, $a=3$, $b=2$, $k=1$**\nThe points are $\\mathbf{p}_1=(3,0,0)$, $\\mathbf{p}_2=(0,3,0)$, $\\mathbf{p}_3=(0,0,3)$ (indices $1,2,3$, label $0$) and $\\mathbf{p}_4=\\mathbf{u}=(2, 2, 0)$ (index $4$, label $1$).\nThe distances from the origin are:\n- For $\\mathbf{p}_1, \\mathbf{p}_2, \\mathbf{p}_3$: $d_1 = d_2 = |a| = |3| = 3$.\n- For $\\mathbf{p}_4=\\mathbf{u}$: $d_1 = 2|b| = 2|2| = 4$ and $d_2 = \\sqrt{2}|b| = \\sqrt{2}|2| = 2\\sqrt{2} \\approx 2.828$.\n\nOrdering of neighbors:\n- Under $d_1$: Distances are $3$ (for indices $1,2,3$) and $4$ (for index $4$). Sorted list of indices: $[1, 2, 3, 4]$.\n- Under $d_2$: Distances are $3$ (for indices $1,2,3$) and $2\\sqrt{2}$ (for index $4$). Since $2\\sqrt{2}  3$, the order changes. Sorted list of indices: $[4, 1, 2, 3]$.\n\n1.  **Nearest Neighbor Comparison ($x_1$)**: The nearest neighbor under $d_1$ is $\\mathbf{p}_1$ (index $1$). Under $d_2$, it is $\\mathbf{p}_4$ (index $4$). They differ. Thus, $x_1=1$.\n2.  **Symmetric Difference ($x_2$)**: For $k=1$, the set of nearest neighbors under $d_1$ is $N_1=\\{1\\}$. Under $d_2$, it is $N_2=\\{4\\}$. The symmetric difference is $N_1 \\Delta N_2 = \\{1, 4\\}$, which has size $2$. Thus, $x_2=2$.\n3.  **KNN Classification ($x_3$)**: For $k=1$, the classification is determined by the label of the nearest neighbor. For $d_1$, the neighbor is $\\mathbf{p}_1$ (label $0$). For $d_2$, the neighbor is $\\mathbf{p}_4$ (label $1$). The classification changes. Thus, $x_3=1$.\nResult for Test Case 4: $[1, 2, 1]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the KNN comparison problem for a given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (d, a, b, k)\n        (2, 3/2, 1, 3),\n        (2, np.sqrt(2), 1, 2),\n        (3, 1, 1/5, 2),\n        (3, 3, 2, 1),\n    ]\n\n    all_results = []\n\n    for d, a, b, k in test_cases:\n        # 1. Construct the dataset for the current test case.\n        points = []\n        point_labels = {}\n\n        # Axis-aligned points\n        for i in range(1, d + 1):\n            vector = np.zeros(d)\n            vector[i-1] = a\n            index = i\n            label = 0\n            points.append({'vector': vector, 'index': index, 'label': label})\n            point_labels[index] = label\n\n        # Diagonal point\n        vector_u = np.zeros(d)\n        vector_u[0] = b\n        vector_u[1] = b\n        index_u = d + 1\n        label_u = 1\n        points.append({'vector': vector_u, 'index': index_u, 'label': label_u})\n        point_labels[index_u] = label_u\n        \n        # 2. Calculate distances from the origin (query point)\n        query_point = np.zeros(d)\n        for p in points:\n            p['dist1'] = np.linalg.norm(p['vector'] - query_point, ord=1)\n            p['dist2'] = np.linalg.norm(p['vector'] - query_point, ord=2)\n\n        # 3. Sort points based on each metric, breaking ties by index\n        sorted_points_1 = sorted(points, key=lambda p: (p['dist1'], p['index']))\n        sorted_points_2 = sorted(points, key=lambda p: (p['dist2'], p['index']))\n\n        sorted_indices_1 = [p['index'] for p in sorted_points_1]\n        sorted_indices_2 = [p['index'] for p in sorted_points_2]\n\n        # 4. Compute the three required outputs\n        \n        # x1: Nearest neighbor comparison\n        nn_1 = sorted_indices_1[0]\n        nn_2 = sorted_indices_2[0]\n        x1 = 1 if nn_1 != nn_2 else 0\n\n        # x2: Symmetric difference of k-NN sets\n        k_neighbors_1 = set(sorted_indices_1[:k])\n        k_neighbors_2 = set(sorted_indices_2[:k])\n        x2 = len(k_neighbors_1.symmetric_difference(k_neighbors_2))\n\n        # x3: KNN classification comparison\n        def get_knn_label(neighbor_indices, labels_map):\n            \"\"\"\n            Performs majority vote classification with tie-breaking.\n            \"\"\"\n            neighbor_labels = [labels_map[i] for i in neighbor_indices]\n            count_0 = neighbor_labels.count(0)\n            count_1 = neighbor_labels.count(1)\n            \n            if count_1 > count_0:\n                return 1\n            # If count_0 > count_1 or if count_0 == count_1, return 0.\n            # The second case handles the tie-breaking rule (choose smaller label).\n            else:\n                return 0\n\n        knn_label_1 = get_knn_label(k_neighbors_1, point_labels)\n        knn_label_2 = get_knn_label(k_neighbors_2, point_labels)\n        x3 = 1 if knn_label_1 != knn_label_2 else 0\n\n        all_results.append([x1, x2, x3])\n\n    # Final print statement in the exact required format.\n    # e.g., [[x_1,x_2,x_3],[y_1,y_2,y_3]]\n    result_str = \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The stability of numerical algorithms is a central concern in computational science, and norms provide the mathematical language to analyze it. A problem is considered ill-conditioned if small changes in the input can lead to large changes in the output. This practice  delves into this concept by examining how scaling the columns of a matrix—a common issue when variables have different units or scales—affects the residual of a linear system, using norm-based analysis to both quantify the sensitivity and motivate a practical solution through column normalization.",
            "id": "3201741",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with columns denoted by $a_j \\in \\mathbb{R}^m$ for $j \\in \\{1,\\dots,n\\}$, a vector $x \\in \\mathbb{R}^n$, and a target vector $b \\in \\mathbb{R}^m$. For a norm parameter $p \\in \\{1,2,\\infty\\}$, define the residual function $r(A,b,x,p) = \\|A x - b\\|_p$. Column scaling is represented by a diagonal matrix $S = \\mathrm{diag}(s_1,\\dots,s_n)$ with strictly positive entries $s_j  0$, which produces the scaled matrix $A S$. Your task is to analyze the sensitivity of the residual $\\|A x - b\\|_p$ to column scaling of $A$, and to propose and implement column normalization strategies based on per-column $\\ell_2$ or $\\ell_1$ norms.\n\nStart from the following foundational base:\n- The definition of the $p$-norm for a vector $v \\in \\mathbb{R}^m$:\n  - For finite $p \\in \\{1,2\\}$, $\\|v\\|_p = \\left(\\sum_{i=1}^m |v_i|^p\\right)^{1/p}$.\n  - For $p = \\infty$, $\\|v\\|_\\infty = \\max_{i} |v_i|$.\n- The properties of norms including positive homogeneity $\\|c v\\|_p = |c| \\|v\\|_p$ for any scalar $c \\in \\mathbb{R}$, and the triangle inequality $\\|u+v\\|_p \\le \\|u\\|_p + \\|v\\|_p$ for any $u,v \\in \\mathbb{R}^m$.\n- The decomposition $A x = \\sum_{j=1}^n x_j a_j$ and $A S x = \\sum_{j=1}^n s_j x_j a_j$, where $a_j$ denotes the $j$-th column of $A$.\n\nFrom this base, derive a bound on the change in the residual due to column scaling. Express the absolute change using the triangle inequality and positive homogeneity. Then, propose a column normalization rule using per-column norms and explain why normalization can reduce the sensitivity of the residual to arbitrary column scalings of $A$.\n\nImplementation requirements:\n1. Define the absolute relative change for a given scaling $S$ as\n   $$\\delta(A,b,x,p,S) = \\begin{cases}\n   \\dfrac{ \\big| r(A S,b,x,p) - r(A,b,x,p) \\big| }{ r(A,b,x,p) },  \\text{if } r(A,b,x,p)  0, \\\\\n   \\big| r(A S,b,x,p) - r(A,b,x,p) \\big|,  \\text{if } r(A,b,x,p) = 0.\n   \\end{cases}$$\n   Also compute the corresponding relative triangle-inequality bound\n   $$B(A,b,x,p,S) = \\begin{cases}\n   \\dfrac{ \\sum_{j=1}^n |s_j - 1| \\, |x_j| \\, \\|a_j\\|_p }{ r(A,b,x,p) },  \\text{if } r(A,b,x,p)  0, \\\\\n   \\sum_{j=1}^n |s_j - 1| \\, |x_j| \\, \\|a_j\\|_p,  \\text{if } r(A,b,x,p) = 0.\n   \\end{cases}$$\n   Here, $a_j$ denotes the $j$-th column of $A$ and $\\|a_j\\|_p$ is the $p$-norm of that column.\n2. Define two normalized matrices by column-wise scaling:\n   - $\\tilde{A}_2 = A D_2$ where $D_2 = \\mathrm{diag}(d^{(2)}_1,\\dots,d^{(2)}_n)$ and $d^{(2)}_j = 1 / \\|a_j\\|_2$ if $\\|a_j\\|_2  0$, and $d^{(2)}_j = 1$ if $\\|a_j\\|_2 = 0$ (zero columns are left unchanged).\n   - $\\tilde{A}_1 = A D_1$ where $D_1 = \\mathrm{diag}(d^{(1)}_1,\\dots,d^{(1)}_n)$ and $d^{(1)}_j = 1 / \\|a_j\\|_1$ if $\\|a_j\\|_1  0$, and $d^{(1)}_j = 1$ if $\\|a_j\\|_1 = 0$.\n   For each normalized matrix, repeat the calculations in item $1$ using the same scaling matrix $S$, producing $\\delta(\\tilde{A}_2,b,x,p,S)$, $B(\\tilde{A}_2,b,x,p,S)$ and $\\delta(\\tilde{A}_1,b,x,p,S)$, $B(\\tilde{A}_1,b,x,p,S)$.\n3. For zero columns (i.e., $\\|a_j\\|_q = 0$ for $q \\in \\{1,2\\}$), leave the column unchanged during normalization as indicated above to avoid division by zero. If $r(A,b,x,p) = 0$, use the absolute change definitions above to avoid division by zero.\n4. Round all outputs to $6$ decimal places.\n\nTest suite:\n- Case $1$ (general case):\n  - $A = \\begin{bmatrix} 2  -1  0 \\\\ 0  3  4 \\\\ 1  0  -2 \\end{bmatrix}$,\n    $b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0.5 \\end{bmatrix}$,\n    $x = \\begin{bmatrix} 0.5 \\\\ -0.25 \\\\ 1 \\end{bmatrix}$,\n    $p = 2$,\n    $S = \\mathrm{diag}(5, 0.2, 3)$.\n- Case $2$ (edge case with a zero column and $\\ell_1$ residual norm):\n  - $A = \\begin{bmatrix} 0  1  -2 \\\\ 0  -1  0 \\\\ 0  3  1 \\end{bmatrix}$,\n    $b = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}$,\n    $x = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}$,\n    $p = 1$,\n    $S = \\mathrm{diag}(10, 0.1, 0.5)$.\n- Case $3$ (tall matrix and $\\ell_\\infty$ residual norm):\n  - $A = \\begin{bmatrix} 1  0  3  -1 \\\\ 0  -2  0  1 \\\\ 2  1  0  0 \\\\ 0  3  -1  2 \\end{bmatrix}$,\n    $b = \\begin{bmatrix} 0 \\\\ 1 \\\\ -2 \\\\ 1 \\end{bmatrix}$,\n    $x = \\begin{bmatrix} 0.25 \\\\ -0.5 \\\\ 0.75 \\\\ -1 \\end{bmatrix}$,\n    $p = \\infty$,\n    $S = \\mathrm{diag}(0.5, 4, 0.1, 2)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. Each test case produces an inner list of six floats in the order $[\\delta, B, \\delta(\\tilde{A}_2), B(\\tilde{A}_2), \\delta(\\tilde{A}_1), B(\\tilde{A}_1)]$, all rounded to $6$ decimal places. The final line should therefore look like\n  $$[\\,[\\delta_1,B_1,\\delta_{1,2},B_{1,2},\\delta_{1,1},B_{1,1}],\\,[\\delta_2,B_2,\\delta_{2,2},B_{2,2},\\delta_{2,1},B_{2,1}],\\,[\\delta_3,B_3,\\delta_{3,2},B_{3,2},\\delta_{3,1},B_{3,1}]\\,]$$\n  with no spaces anywhere in the string.",
            "solution": "The user has provided a problem concerning the sensitivity of a residual's norm to column scaling of a matrix. The task involves a theoretical derivation, a conceptual explanation of column normalization, and a numerical implementation for specific test cases.\n\n### Theoretical Derivation\n\nThe problem asks to derive a bound on the absolute change of the residual norm, $|\\, \\|A S x - b\\|_p - \\|A x - b\\|_p \\,|$, due to column scaling of the matrix $A$ by a diagonal matrix $S$.\n\nLet the original residual vector be $r = A x - b$ and the scaled residual vector be $r_S = A S x - b$. We are interested in bounding the quantity $|\\, \\|r_S\\|_p - \\|r\\|_p \\,|$.\n\nA fundamental property of any norm is the reverse triangle inequality, which states that for any two vectors $u$ and $v$ in a normed vector space, $|\\, \\|u\\|_p - \\|v\\|_p \\,| \\le \\|u - v\\|_p$. Applying this inequality with $u = r_S$ and $v = r$, we get:\n$$\n|\\, \\|A S x - b\\|_p - \\|A x - b\\|_p \\,| \\le \\| (A S x - b) - (A x - b) \\|_p\n$$\nThe term inside the norm on the right-hand side simplifies to:\n$$\n(A S x - b) - (A x - b) = A S x - A x = A(S - I)x\n$$\nwhere $I$ is the $n \\times n$ identity matrix. Thus, the bound becomes:\n$$\n|\\, \\|A S x - b\\|_p - \\|A x - b\\|_p \\,| \\le \\| A(S - I)x \\|_p\n$$\nThe problem specifies using the column-wise decomposition of the matrix-vector product. The matrix $A$ has columns $a_j \\in \\mathbb{R}^m$. The matrix $S$ is diagonal, $S = \\mathrm{diag}(s_1, \\dots, s_n)$. Therefore, the matrix $A(S-I)$ has columns $(s_j - 1)a_j$. The product $A(S-I)x$ can be expressed as a linear combination of these columns:\n$$\nA(S - I)x = \\sum_{j=1}^n (s_j - 1) x_j a_j\n$$\nwhere $x_j$ are the components of the vector $x$.\n\nNow, we apply the triangle inequality for norms to this sum:\n$$\n\\left\\| \\sum_{j=1}^n (s_j - 1) x_j a_j \\right\\|_p \\le \\sum_{j=1}^n \\left\\| (s_j - 1) x_j a_j \\right\\|_p\n$$\nUsing the positive homogeneity property of norms, $\\|c v\\|_p = |c| \\|v\\|_p$ for any scalar $c \\in \\mathbb{R}$, we can pull the scalar coefficients out of the norm:\n$$\n\\sum_{j=1}^n \\left\\| (s_j - 1) x_j a_j \\right\\|_p = \\sum_{j=1}^n |(s_j - 1) x_j| \\|a_j\\|_p = \\sum_{j=1}^n |s_j - 1| |x_j| \\|a_j\\|_p\n$$\nCombining all steps, we arrive at the final bound on the absolute change of the residual norm:\n$$\n|\\, \\|A S x - b\\|_p - \\|A x - b\\|_p \\,| \\le \\sum_{j=1}^n |s_j - 1| |x_j| \\|a_j\\|_p\n$$\nThis expression is the numerator for the relative bound $B(A,b,x,p,S)$ defined in the problem statement.\n\n### Rationale for Column Normalization\n\nThe derived bound reveals that the sensitivity of the residual norm to scaling is a sum of contributions from each column. The contribution of the $j$-th column is proportional to three factors:\n1.  $|s_j - 1|$: The magnitude of the change in the scaling factor for column $j$.\n2.  $|x_j|$: The magnitude of the corresponding component of the vector $x$.\n3.  $\\|a_j\\|_p$: The $p$-norm of the $j$-th column of $A$.\n\nIf the column norms $\\|a_j\\|_p$ vary significantly, columns with large norms can disproportionately influence the sensitivity of the residual. This can lead to ill-conditioning and numerical instability in algorithms that involve the matrix $A$, such as iterative methods for solving linear systems or optimization problems. For instance, a small perturbation in $x_j$ or a small scaling $s_j \\approx 1$ could lead to a large change in the residual if $\\|a_j\\|_p$ is very large.\n\nColumn normalization is a pre-processing step that aims to mitigate this issue. By scaling each column $a_j$ to produce a new column $\\tilde{a}_j$ such that its norm is unity (i.e., $\\|\\tilde{a}_j\\|_q=1$ for some chosen norm $q$, typically $q=1$ or $q=2$), we regularize the matrix. The problem defines two such normalizations:\n-   $\\tilde{A}_2 = A D_2$, where columns are scaled to have a unit $\\ell_2$-norm. For any non-zero column $j$, the new column is $\\tilde{a}_j^{(2)} = a_j/\\|a_j\\|_2$, so $\\|\\tilde{a}_j^{(2)}\\|_2 = 1$.\n-   $\\tilde{A}_1 = A D_1$, where columns are scaled to have a unit $\\ell_1$-norm. For any non-zero column $j$, the new column is $\\tilde{a}_j^{(1)} = a_j/\\|a_j\\|_1$, so $\\|\\tilde{a}_j^{(1)}\\|_1 = 1$.\n\nAfter normalization, the matrix $\\tilde{A}$ has columns $\\tilde{a}_j$ with more uniform norms. The sensitivity bound for this new matrix $\\tilde{A}$ becomes:\n$$\n|\\, \\|\\tilde{A} S x - b\\|_p - \\|\\tilde{A} x - b\\|_p \\,| \\le \\sum_{j=1}^n |s_j - 1| |x_j| \\|\\tilde{a}_j\\|_p\n$$\nIf the normalization norm $q$ is the same as the residual norm $p$ (e.g., $p=q=2$), then $\\|\\tilde{a}_j\\|_p = 1$ for all non-zero columns. The bound then simplifies, removing the explicit dependence on column magnitudes: $\\sum |s_j - 1| |x_j|$. This makes the sensitivity more evenly dependent on the components of $x$ and the scaling factors $s_j$, rather than being dominated by columns of $A$ with large norms. Even if $p \\ne q$, norm equivalence inequalities ensure that normalizing in one norm controls the magnitude in other norms, thus achieving the goal of making columns more uniform in scale. This process generally enhances the numerical stability and robustness of subsequent computations.\n\n### Implementation Strategy\n\nThe implementation will consist of a primary loop that iterates through the provided test cases. For each case, a set of six metrics will be computed: $\\delta$ and $B$ for the original matrix $A$, the $\\ell_2$-normalized matrix $\\tilde{A}_2$, and the $\\ell_1$-normalized matrix $\\tilde{A}_1$.\n\nA helper function, `compute_metrics(A, b, x, S, p)`, will be implemented to calculate $\\delta$ and $B$ for a given matrix `A`. This function will perform the following steps:\n1.  Compute the original residual $r = \\|A x - b\\|_p$.\n2.  Compute the scaled residual $r_S = \\|A S x - b\\|_p$.\n3.  Calculate $\\delta$ using the formula provided, with a conditional check for $r  0$ to avoid division by zero.\n4.  Calculate the bound $B$. This involves computing the $p$-norm of each column of $A$, then evaluating the sum $\\sum |s_j - 1| |x_j| \\|a_j\\|_p$. The result is divided by $r$ if $r  0$.\n\nThe main function will first call `compute_metrics` for the original matrix $A$. Then, it will construct the normalized matrices $\\tilde{A}_2$ and $\\tilde{A}_1$. For each column $a_j$, the normalization factor is $1/\\|a_j\\|_q$ for $q \\in \\{1,2\\}$. If a column norm is zero, the column is left unchanged (i.e., the scaling factor is $1$) to prevent division by zero, as per the problem specification. The helper function `compute_metrics` is then called for $\\tilde{A}_2$ and $\\tilde{A}_1$ to obtain the remaining four metrics. All final results are rounded to six decimal places. The final output is formatted into a single-line string with no spaces, as per the precise output requirements.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and generate the final output.\n    \"\"\"\n\n    def compute_delta_and_B(A, b, x, S, p_ord):\n        \"\"\"\n        Computes the relative change delta and its theoretical bound B.\n\n        Args:\n            A (np.ndarray): The matrix.\n            b (np.ndarray): The target vector.\n            x (np.ndarray): The vector.\n            S (np.ndarray): The diagonal scaling matrix.\n            p_ord (int or np.inf): The order for the p-norm.\n\n        Returns:\n            tuple[float, float]: A tuple containing (delta, B).\n        \"\"\"\n        # Calculate residuals\n        r_vec = A @ x - b\n        r_val = np.linalg.norm(r_vec, ord=p_ord)\n\n        # Note: A @ S @ x is the evaluation of (AS)x\n        r_S_vec = (A @ S) @ x - b\n        r_S_val = np.linalg.norm(r_S_vec, ord=p_ord)\n\n        # Calculate delta\n        abs_change = abs(r_S_val - r_val)\n        if r_val > 1e-9:  # Use a small tolerance for floating point comparison\n            delta = abs_change / r_val\n        else:\n            delta = abs_change\n\n        # Calculate bound B\n        s_diag = np.diag(S)\n        col_norms = np.linalg.norm(A, ord=p_ord, axis=0)\n        bound_numerator = np.sum(np.abs(s_diag - 1) * np.abs(x) * col_norms)\n\n        if r_val > 1e-9:\n            B = bound_numerator / r_val\n        else:\n            B = bound_numerator\n\n        return delta, B\n\n    test_cases = [\n        (np.array([[2, -1, 0], [0, 3, 4], [1, 0, -2]]),\n         np.array([1, -1, 0.5]),\n         np.array([0.5, -0.25, 1]),\n         2,\n         np.array([5, 0.2, 3])),\n        (np.array([[0, 1, -2], [0, -1, 0], [0, 3, 1]]),\n         np.array([0, 1, -1]),\n         np.array([1, 0.5, -0.5]),\n         1,\n         np.array([10, 0.1, 0.5])),\n        (np.array([[1, 0, 3, -1], [0, -2, 0, 1], [2, 1, 0, 0], [0, 3, -1, 2]]),\n         np.array([0, 1, -2, 1]),\n         np.array([0.25, -0.5, 0.75, -1]),\n         np.inf,\n         np.array([0.5, 4, 0.1, 2]))\n    ]\n\n    all_results = []\n    for A, b, x, p_val, S_diag in test_cases:\n        p_ord = p_val if p_val != np.inf else np.inf\n        S = np.diag(S_diag)\n\n        # 1. Calculations for original matrix A\n        delta, B = compute_delta_and_B(A, b, x, S, p_ord)\n\n        # 2. Calculations for l2-normalized matrix A_tilde_2\n        col_norms_2 = np.linalg.norm(A, ord=2, axis=0)\n        D2_diag = np.ones_like(col_norms_2, dtype=float)\n        non_zero_mask_2 = col_norms_2 > 1e-9\n        D2_diag[non_zero_mask_2] = 1.0 / col_norms_2[non_zero_mask_2]\n        A_tilde_2 = A @ np.diag(D2_diag)\n        delta_2, B_2 = compute_delta_and_B(A_tilde_2, b, x, S, p_ord)\n\n        # 3. Calculations for l1-normalized matrix A_tilde_1\n        col_norms_1 = np.linalg.norm(A, ord=1, axis=0)\n        D1_diag = np.ones_like(col_norms_1, dtype=float)\n        non_zero_mask_1 = col_norms_1 > 1e-9\n        D1_diag[non_zero_mask_1] = 1.0 / col_norms_1[non_zero_mask_1]\n        A_tilde_1 = A @ np.diag(D1_diag)\n        delta_1, B_1 = compute_delta_and_B(A_tilde_1, b, x, S, p_ord)\n        \n        # Round all results to 6 decimal places\n        case_results = [round(val, 6) for val in [delta, B, delta_2, B_2, delta_1, B_1]]\n        all_results.append(case_results)\n\n    # Format the final output string to have no spaces\n    outer_parts = []\n    for res_list in all_results:\n        inner_str = ','.join(map(str, res_list))\n        outer_parts.append(f\"[{inner_str}]\")\n    final_str = f\"[{','.join(outer_parts)}]\"\n    print(final_str)\n\nsolve()\n```"
        }
    ]
}