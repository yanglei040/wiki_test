{
    "hands_on_practices": [
        {
            "introduction": "In computational science, the concept of a \"vector\" extends far beyond simple arrows in space; functions, for instance, can also form a vector space. This exercise introduces the powerful idea of measuring the \"distance\" between two functions using the $L_1$ norm. By calculating this distance, you will practice applying the definition of a norm in an infinite-dimensional setting, a concept fundamental to approximation theory and analyzing errors in numerical solutions .",
            "id": "2308541",
            "problem": "In the field of functional analysis, we can measure the \"distance\" between functions in various ways. Consider the vector space $C[0,1]$, which consists of all continuous real-valued functions defined on the closed interval $[0, 1]$.\n\nOne common way to define distance is by using the $L_1$ norm. For any function $f(x)$ in $C[0,1]$, its $L_1$ norm, denoted by $\\|f\\|_1$, is defined as:\n$$\n\\|f\\|_1 = \\int_{0}^{1} |f(x)| dx\n$$\nThe distance between two functions, $p(x)$ and $q(x)$, in this space is then given by the norm of their difference, i.e., $d(p, q) = \\|p - q\\|_1$.\n\nCalculate the exact distance between the function $p(x) = x$ and the constant function $q(x) = \\frac{1}{2}$ in the space $C[0,1]$ with respect to the $L_1$ norm. Express your answer as a fraction in simplest terms.",
            "solution": "We are given $p(x)=x$ and $q(x)=\\frac{1}{2}$ on $[0,1]$. The $L_{1}$ distance is\n$$\nd(p,q)=\\|p-q\\|_{1}=\\int_{0}^{1}|p(x)-q(x)|\\,dx=\\int_{0}^{1}\\left|x-\\frac{1}{2}\\right|\\,dx.\n$$\nThe absolute value splits at $x=\\frac{1}{2}$, with $\\left|x-\\frac{1}{2}\\right|=\\frac{1}{2}-x$ for $x\\in[0,\\frac{1}{2}]$ and $\\left|x-\\frac{1}{2}\\right|=x-\\frac{1}{2}$ for $x\\in[\\frac{1}{2},1]$. Therefore,\n$$\n\\int_{0}^{1}\\left|x-\\frac{1}{2}\\right|\\,dx=\\int_{0}^{\\frac{1}{2}}\\left(\\frac{1}{2}-x\\right)\\,dx+\\int_{\\frac{1}{2}}^{1}\\left(x-\\frac{1}{2}\\right)\\,dx.\n$$\nCompute each integral explicitly. For the first,\n$$\n\\int_{0}^{\\frac{1}{2}}\\left(\\frac{1}{2}-x\\right)\\,dx=\\left[\\frac{1}{2}x-\\frac{x^{2}}{2}\\right]_{0}^{\\frac{1}{2}}=\\left(\\frac{1}{2}\\cdot\\frac{1}{2}-\\frac{\\left(\\frac{1}{2}\\right)^{2}}{2}\\right)-0=\\frac{1}{4}-\\frac{1}{8}=\\frac{1}{8}.\n$$\nFor the second,\n$$\n\\int_{\\frac{1}{2}}^{1}\\left(x-\\frac{1}{2}\\right)\\,dx=\\left[\\frac{x^{2}}{2}-\\frac{1}{2}x\\right]_{\\frac{1}{2}}^{1}=\\left(\\frac{1}{2}-\\frac{1}{2}\\right)-\\left(\\frac{\\left(\\frac{1}{2}\\right)^{2}}{2}-\\frac{1}{2}\\cdot\\frac{1}{2}\\right)=0-\\left(\\frac{1}{8}-\\frac{1}{4}\\right)=\\frac{1}{8}.\n$$\nAdding these yields\n$$\n\\int_{0}^{1}\\left|x-\\frac{1}{2}\\right|\\,dx=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\nThus the exact $L_{1}$ distance is $\\frac{1}{4}$.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "While vector norms measure the magnitude of vectors, induced matrix norms measure the \"size\" or \"strength\" of the linear transformations they represent. This practice demonstrates how to calculate the induced 1-norm of a matrix, which quantifies the maximum factor by which it can \"stretch\" a vector in the $L_1$ sense. Mastering this calculation provides a concrete tool for analyzing the behavior and stability of matrix-based computational algorithms .",
            "id": "2308606",
            "problem": "Consider the vector space $\\mathbb{R}^2$. The $L_1$-norm, also known as the taxicab norm, of a vector $x = (x_1, x_2)$ in this space is defined as $\\|x\\|_1 = |x_1| + |x_2|$.\nA linear transformation from $\\mathbb{R}^2$ to itself can be represented by a $2 \\times 2$ matrix. The operator norm of a matrix $A$ induced by the vector norm $\\|\\cdot\\|_1$ is defined as\n$$ \\|A\\|_1 = \\sup_{x \\in \\mathbb{R}^2, x \\neq 0} \\frac{\\|Ax\\|_1}{\\|x\\|_1} $$\nwhich is equivalent to finding the maximum value of $\\|Ax\\|_1$ over all vectors $x$ with $\\|x\\|_1=1$.\n\nGiven the matrix $A = \\begin{pmatrix} 1 & -4 \\\\ 2 & -1 \\end{pmatrix}$, calculate the value of its induced norm $\\|A\\|_1$.",
            "solution": "We use the operator norm induced by the vector 1-norm. For a matrix $A = (a_{ij}) \\in \\mathbb{R}^{2 \\times 2}$ and any $x \\in \\mathbb{R}^{2}$, we have\n$$\n\\|Ax\\|_{1} = \\sum_{i=1}^{2} \\left| \\sum_{j=1}^{2} a_{ij} x_{j} \\right| \\leq \\sum_{i=1}^{2} \\sum_{j=1}^{2} |a_{ij}|\\,|x_{j}| = \\sum_{j=1}^{2} \\left( \\sum_{i=1}^{2} |a_{ij}| \\right) |x_{j}|.\n$$\nLet $c_{j} := \\sum_{i=1}^{2} |a_{ij}|$ denote the absolute column sums. Then\n$$\n\\|Ax\\|_{1} \\leq \\left( \\max_{j} c_{j} \\right) \\sum_{j=1}^{2} |x_{j}| = \\left( \\max_{j} c_{j} \\right) \\|x\\|_{1}.\n$$\nTaking the supremum over all $x \\neq 0$ and using the definition of the induced norm yields\n$$\n\\|A\\|_{1} \\leq \\max_{j} c_{j}.\n$$\nTo show equality, choose $x = e_{k}$ where $k$ is an index attaining $\\max_{j} c_{j}$, with $e_{k}$ the $k$-th standard basis vector. Then $Ax$ equals the $k$-th column of $A$, so\n$$\n\\|Ax\\|_{1} = c_{k} = \\max_{j} c_{j}, \\quad \\|x\\|_{1} = 1,\n$$\nwhich implies\n$$\n\\|A\\|_{1} = \\max_{j} c_{j}.\n$$\nFor the given matrix $A = \\begin{pmatrix} 1 & -4 \\\\ 2 & -1 \\end{pmatrix}$, the absolute column sums are\n$$\nc_{1} = |1| + |2| = 3, \\qquad c_{2} = |-4| + |-1| = 5.\n$$\nTherefore,\n$$\n\\|A\\|_{1} = \\max\\{3, 5\\} = 5.\n$$",
            "answer": "$$\\boxed{5}$$"
        },
        {
            "introduction": "The choice of a norm is not merely a theoretical curiosity; it has tangible consequences for data analysis and machine learning. This computational experiment directly investigates how the geometry induced by the $L_1$ (\"Manhattan\") and $L_2$ (\"Euclidean\") norms can alter the results of the k-Nearest Neighbors algorithm. By building a specific dataset and comparing outcomes, you will gain a practical understanding of why selecting the right \"ruler\" to measure distance is a critical step in algorithm design .",
            "id": "3201753",
            "problem": "You will work in the finite-dimensional real vector space $\\mathbb{R}^d$ and compare the behavior of the induced metrics from two norms: the $L^1$ norm and the $L^2$ norm. The $L^1$ norm is defined by $\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^d |x_i|$ and the $L^2$ norm is defined by $\\|\\mathbf{x}\\|_2 = \\left(\\sum_{i=1}^d x_i^2\\right)^{1/2}$. For any norm $\\|\\cdot\\|$, the induced metric on $\\mathbb{R}^d$ is $d(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x}-\\mathbf{y}\\|$.\n\nConstruct a dataset parameterized by integers and real numbers $d \\in \\mathbb{N}$, $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$, and $k \\in \\mathbb{N}$, as follows:\n- The query point is the origin $\\mathbf{0} \\in \\mathbb{R}^d$.\n- Define the axis-aligned points $\\mathcal{A} = \\{a \\mathbf{e}_i : i \\in \\{1,2,\\dots,d\\}\\}$, where $\\mathbf{e}_i$ is the $i$-th standard basis vector in $\\mathbb{R}^d$. Assign each point in $\\mathcal{A}$ the class label $0$.\n- Define a single diagonal point $\\mathbf{u} = b(\\mathbf{e}_1+\\mathbf{e}_2)$ and assign it the class label $1$.\n- Index the points in the order: axis points first with indices $1,2,\\dots,d$, then the diagonal point with index $d+1$.\n\nLet the comparison be between the metrics $d_1(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x}-\\mathbf{y}\\|_1$ and $d_2(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x}-\\mathbf{y}\\|_2$. Among equal-distance ties, break ties by choosing the point with the smallest index. For the downstream algorithm, use the $k$-Nearest Neighbors (KNN) majority-vote classifier applied to the query point $\\mathbf{0}$, where the class label is the majority among the labels of the $k$ nearest neighbors; if there is a tie in the majority vote, choose the smaller integer label.\n\nYour program must:\n- Compute the nearest neighbor of $\\mathbf{0}$ under $d_1$ and under $d_2$ and output whether they differ as an integer ($1$ if different, $0$ if the same).\n- Compute the sets of indices of the $k$ nearest neighbors of $\\mathbf{0}$ under $d_1$ and under $d_2$, and output the size of their symmetric difference as an integer.\n- Compute the KNN majority-vote class label under $d_1$ and under $d_2$ for the query $\\mathbf{0}$ and output whether the classification result changes as an integer ($1$ if different, $0$ if the same).\n\nUse the following test suite of parameter values to exercise different behaviors:\n- Test case $1$: $d=2$, $a=\\frac{3}{2}$, $b=1$, $k=3$.\n- Test case $2$: $d=2$, $a=\\sqrt{2}$, $b=1$, $k=2$.\n- Test case $3$: $d=3$, $a=1$, $b=\\frac{1}{5}$, $k=2$.\n- Test case $4$: $d=3$, $a=3$, $b=2$, $k=1$.\n\nYour program should produce a single line of output containing a comma-separated list of the results for each test case, where each test case result is itself a triple formatted as $[x_1,x_2,x_3]$ with no spaces, in the same order as the test suite above. The final output must therefore be a single list of lists, for example, $[[x_{1,1},x_{1,2},x_{1,3}],[x_{2,1},x_{2,2},x_{2,3}],\\dots]$. No physical units or angle units apply in this problem, and all returned values must be integers.",
            "solution": "The user-provided problem is rigorously validated and found to be self-contained, scientifically sound, and well-posed. All required data, definitions, and conditions are explicitly stated, and there are no contradictions, ambiguities, or violations of scientific or mathematical principles. The problem is a standard exercise in computational science, specifically exploring the effect of different norms on the geometry of a vector space and its implications for distance-based algorithms like $k$-Nearest Neighbors (KNN). I will now proceed with a complete solution.\n\nThe core of the problem lies in comparing the ranking of distances from a query point to a set of data points, using two different metrics: $d_1$ induced by the $L^1$ norm and $d_2$ induced by the $L^2$ norm. The query point is the origin $\\mathbf{q} = \\mathbf{0} \\in \\mathbb{R}^d$. The distance from the origin to any point $\\mathbf{p}$ simplifies to its norm, i.e., $d(\\mathbf{0}, \\mathbf{p}) = \\|\\mathbf{p}\\|$.\n\nThe dataset consists of two types of points:\n1.  A set of $d$ axis-aligned points, $\\mathcal{A} = \\{ \\mathbf{p}_i = a \\mathbf{e}_i : i \\in \\{1, \\dots, d\\} \\}$.\n2.  A single diagonal point, $\\mathbf{u} = \\mathbf{p}_{d+1} = b(\\mathbf{e}_1 + \\mathbf{e}_2)$.\n\nLet us compute the distances of these points from the origin $\\mathbf{0}$ under both norms.\n\nFor any axis-aligned point $\\mathbf{p}_i = a \\mathbf{e}_i$:\nThe $L^1$ distance is $d_1(\\mathbf{0}, \\mathbf{p}_i) = \\|a \\mathbf{e}_i\\|_1 = \\sum_{j=1}^d |(a \\mathbf{e}_i)_j| = |a|$.\nThe $L^2$ distance is $d_2(\\mathbf{0}, \\mathbf{p}_i) = \\|a \\mathbf{e}_i\\|_2 = \\left(\\sum_{j=1}^d (a \\mathbf{e}_i)_j^2\\right)^{1/2} = \\sqrt{a^2} = |a|$.\nFor all axis-aligned points, the $L^1$ and $L^2$ distances to the origin are identical.\n\nFor the diagonal point $\\mathbf{u} = b(\\mathbf{e}_1 + \\mathbf{e}_2) = (b, b, 0, \\dots, 0)$:\nThe $L^1$ distance is $d_1(\\mathbf{0}, \\mathbf{u}) = \\|\\mathbf{u}\\|_1 = |b| + |b| = 2|b|$.\nThe $L^2$ distance is $d_2(\\mathbf{0}, \\mathbf{u}) = \\|\\mathbf{u}\\|_2 = \\sqrt{b^2 + b^2} = \\sqrt{2b^2} = \\sqrt{2}|b|$.\n\nThe relative ordering of neighbors depends on the comparison between $|a|$ and $2|b|$ for the $L^1$ metric, and between $|a|$ and $\\sqrt{2}|b|$ for the $L^2$ metric. This difference is the source of all variations observed in the problem. We now analyze each test case.\n\n**Test Case 1: $d=2$, $a=3/2$, $b=1$, $k=3$**\nThe points are $\\mathbf{p}_1=(3/2, 0)$ (index $1$, label $0$), $\\mathbf{p}_2=(0, 3/2)$ (index $2$, label $0$), and $\\mathbf{p}_3=\\mathbf{u}=(1, 1)$ (index $3$, label $1$).\nThe distances from the origin are:\n- For $\\mathbf{p}_1, \\mathbf{p}_2$: $d_1 = d_2 = |a| = |3/2| = 1.5$.\n- For $\\mathbf{p}_3=\\mathbf{u}$: $d_1 = 2|b| = 2|1| = 2$ and $d_2 = \\sqrt{2}|b| = \\sqrt{2}|1| \\approx 1.414$.\n\nOrdering of neighbors (from nearest to farthest, using smallest index for ties):\n- Under $d_1$: Distances are $1.5$ (for indices $1,2$) and $2$ (for index $3$). Sorted list of indices: $[1, 2, 3]$.\n- Under $d_2$: Distances are $1.5$ (for indices $1,2$) and $\\sqrt{2}$ (for index $3$). Since $\\sqrt{2} < 1.5$, the order is different. Sorted list of indices: $[3, 1, 2]$.\n\n1.  **Nearest Neighbor Comparison ($x_1$)**: The nearest neighbor under $d_1$ is $\\mathbf{p}_1$ (index $1$). The nearest neighbor under $d_2$ is $\\mathbf{p}_3$ (index $3$). They differ. Thus, $x_1=1$.\n2.  **Symmetric Difference ($x_2$)**: For $k=3$, the set of nearest neighbors under $d_1$ is $N_1=\\{1, 2, 3\\}$. Under $d_2$, it is $N_2=\\{3, 1, 2\\}$. These sets are identical ($N_1=N_2$). The size of their symmetric difference is $0$. Thus, $x_2=0$.\n3.  **KNN Classification ($x_3$)**: For $k=3$, we use neighbors $\\{1, 2, 3\\}$. The labels are $\\{0, 0, 1\\}$. The majority vote is label $0$. This holds for both $d_1$ and $d_2$. The classification does not change. Thus, $x_3=0$.\nResult for Test Case 1: $[1, 0, 0]$.\n\n**Test Case 2: $d=2$, $a=\\sqrt{2}$, $b=1$, $k=2$**\nThe points are $\\mathbf{p}_1=(\\sqrt{2}, 0)$ (index $1$, label $0$), $\\mathbf{p}_2=(0, \\sqrt{2})$ (index $2$, label $0$), and $\\mathbf{p}_3=\\mathbf{u}=(1, 1)$ (index $3$, label $1$).\nThe distances from the origin are:\n- For $\\mathbf{p}_1, \\mathbf{p}_2$: $d_1 = d_2 = |a| = |\\sqrt{2}| = \\sqrt{2}$.\n- For $\\mathbf{p}_3=\\mathbf{u}$: $d_1 = 2|b| = 2|1| = 2$ and $d_2 = \\sqrt{2}|b| = \\sqrt{2}|1| = \\sqrt{2}$.\n\nOrdering of neighbors:\n- Under $d_1$: Distances are $\\sqrt{2}$ (for indices $1,2$) and $2$ (for index $3$). Sorted list of indices: $[1, 2, 3]$.\n- Under $d_2$: All three points are equidistant at distance $\\sqrt{2}$. We break ties by index. Sorted list of indices: $[1, 2, 3]$.\n\n1.  **Nearest Neighbor Comparison ($x_1$)**: The nearest neighbor is $\\mathbf{p}_1$ (index $1$) for both metrics. They are the same. Thus, $x_1=0$.\n2.  **Symmetric Difference ($x_2$)**: For $k=2$, the set of nearest neighbors under $d_1$ is $N_1=\\{1, 2\\}$. Under $d_2$, it is $N_2=\\{1, 2\\}$. The sets are identical. The size of their symmetric difference is $0$. Thus, $x_2=0$.\n3.  **KNN Classification ($x_3$)**: For $k=2$, we use neighbors $\\{1, 2\\}$. The labels are $\\{0, 0\\}$. The majority vote is label $0$ for both metrics. The classification does not change. Thus, $x_3=0$.\nResult for Test Case 2: $[0, 0, 0]$.\n\n**Test Case 3: $d=3$, $a=1$, $b=1/5$, $k=2$**\nThe points are $\\mathbf{p}_1=(1,0,0)$, $\\mathbf{p}_2=(0,1,0)$, $\\mathbf{p}_3=(0,0,1)$ (indices $1,2,3$, label $0$) and $\\mathbf{p}_4=\\mathbf{u}=(1/5, 1/5, 0)$ (index $4$, label $1$).\nThe distances from the origin are:\n- For $\\mathbf{p}_1, \\mathbf{p}_2, \\mathbf{p}_3$: $d_1 = d_2 = |a| = |1| = 1$.\n- For $\\mathbf{p}_4=\\mathbf{u}$: $d_1 = 2|b| = 2|1/5| = 0.4$ and $d_2 = \\sqrt{2}|b| = \\sqrt{2}|1/5| \\approx 0.283$.\n\nOrdering of neighbors:\n- Under $d_1$: Distances are $1$ (for indices $1,2,3$) and $0.4$ (for index $4$). Sorted list of indices: $[4, 1, 2, 3]$.\n- Under $d_2$: Distances are $1$ (for indices $1,2,3$) and $\\sqrt{2}/5$ (for index $4$). Since $\\sqrt{2}/5 < 1$, the order is the same. Sorted list of indices: $[4, 1, 2, 3]$.\n\n1.  **Nearest Neighbor Comparison ($x_1$)**: The nearest neighbor is $\\mathbf{p}_4$ (index $4$) for both metrics. They are the same. Thus, $x_1=0$.\n2.  **Symmetric Difference ($x_2$)**: For $k=2$, the set of nearest neighbors is $N_1=N_2=\\{4, 1\\}$ for both metrics. The symmetric difference is empty. Thus, $x_2=0$.\n3.  **KNN Classification ($x_3$)**: For $k=2$, we use neighbors $\\{4, 1\\}$. The labels are $\\{1, 0\\}$. There is a tie in the majority vote. The tie-breaking rule specifies choosing the smaller integer label, so the class is $0$. This holds for both metrics. The classification does not change. Thus, $x_3=0$.\nResult for Test Case 3: $[0, 0, 0]$.\n\n**Test Case 4: $d=3$, $a=3$, $b=2$, $k=1$**\nThe points are $\\mathbf{p}_1=(3,0,0)$, $\\mathbf{p}_2=(0,3,0)$, $\\mathbf{p}_3=(0,0,3)$ (indices $1,2,3$, label $0$) and $\\mathbf{p}_4=\\mathbf{u}=(2, 2, 0)$ (index $4$, label $1$).\nThe distances from the origin are:\n- For $\\mathbf{p}_1, \\mathbf{p}_2, \\mathbf{p}_3$: $d_1 = d_2 = |a| = |3| = 3$.\n- For $\\mathbf{p}_4=\\mathbf{u}$: $d_1 = 2|b| = 2|2| = 4$ and $d_2 = \\sqrt{2}|b| = \\sqrt{2}|2| = 2\\sqrt{2} \\approx 2.828$.\n\nOrdering of neighbors:\n- Under $d_1$: Distances are $3$ (for indices $1,2,3$) and $4$ (for index $4$). Sorted list of indices: $[1, 2, 3, 4]$.\n- Under $d_2$: Distances are $3$ (for indices $1,2,3$) and $2\\sqrt{2}$ (for index $4$). Since $2\\sqrt{2} < 3$, the order changes. Sorted list of indices: $[4, 1, 2, 3]$.\n\n1.  **Nearest Neighbor Comparison ($x_1$)**: The nearest neighbor under $d_1$ is $\\mathbf{p}_1$ (index $1$). Under $d_2$, it is $\\mathbf{p}_4$ (index $4$). They differ. Thus, $x_1=1$.\n2.  **Symmetric Difference ($x_2$)**: For $k=1$, the set of nearest neighbors under $d_1$ is $N_1=\\{1\\}$. Under $d_2$, it is $N_2=\\{4\\}$. The symmetric difference is $N_1 \\Delta N_2 = \\{1, 4\\}$, which has size $2$. Thus, $x_2=2$.\n3.  **KNN Classification ($x_3$)**: For $k=1$, the classification is determined by the label of the nearest neighbor. For $d_1$, the neighbor is $\\mathbf{p}_1$ (label $0$). For $d_2$, the neighbor is $\\mathbf{p}_4$ (label $1$). The classification changes. Thus, $x_3=1$.\nResult for Test Case 4: $[1, 2, 1]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the KNN comparison problem for a given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (d, a, b, k)\n        (2, 3/2, 1, 3),\n        (2, np.sqrt(2), 1, 2),\n        (3, 1, 1/5, 2),\n        (3, 3, 2, 1),\n    ]\n\n    all_results = []\n\n    for d, a, b, k in test_cases:\n        # 1. Construct the dataset for the current test case.\n        points = []\n        point_labels = {}\n\n        # Axis-aligned points\n        for i in range(1, d + 1):\n            vector = np.zeros(d)\n            vector[i-1] = a\n            index = i\n            label = 0\n            points.append({'vector': vector, 'index': index, 'label': label})\n            point_labels[index] = label\n\n        # Diagonal point\n        vector_u = np.zeros(d)\n        vector_u[0] = b\n        vector_u[1] = b\n        index_u = d + 1\n        label_u = 1\n        points.append({'vector': vector_u, 'index': index_u, 'label': label_u})\n        point_labels[index_u] = label_u\n        \n        # 2. Calculate distances from the origin (query point)\n        query_point = np.zeros(d)\n        for p in points:\n            p['dist1'] = np.linalg.norm(p['vector'] - query_point, ord=1)\n            p['dist2'] = np.linalg.norm(p['vector'] - query_point, ord=2)\n\n        # 3. Sort points based on each metric, breaking ties by index\n        sorted_points_1 = sorted(points, key=lambda p: (p['dist1'], p['index']))\n        sorted_points_2 = sorted(points, key=lambda p: (p['dist2'], p['index']))\n\n        sorted_indices_1 = [p['index'] for p in sorted_points_1]\n        sorted_indices_2 = [p['index'] for p in sorted_points_2]\n\n        # 4. Compute the three required outputs\n        \n        # x1: Nearest neighbor comparison\n        nn_1 = sorted_indices_1[0]\n        nn_2 = sorted_indices_2[0]\n        x1 = 1 if nn_1 != nn_2 else 0\n\n        # x2: Symmetric difference of k-NN sets\n        k_neighbors_1 = set(sorted_indices_1[:k])\n        k_neighbors_2 = set(sorted_indices_2[:k])\n        x2 = len(k_neighbors_1.symmetric_difference(k_neighbors_2))\n\n        # x3: KNN classification comparison\n        def get_knn_label(neighbor_indices, labels_map):\n            \"\"\"\n            Performs majority vote classification with tie-breaking.\n            \"\"\"\n            neighbor_labels = [labels_map[i] for i in neighbor_indices]\n            count_0 = neighbor_labels.count(0)\n            count_1 = neighbor_labels.count(1)\n            \n            if count_1 > count_0:\n                return 1\n            # If count_0 > count_1 or if count_0 == count_1, return 0.\n            # The second case handles the tie-breaking rule (choose smaller label).\n            else:\n                return 0\n\n        knn_label_1 = get_knn_label(k_neighbors_1, point_labels)\n        knn_label_2 = get_knn_label(k_neighbors_2, point_labels)\n        x3 = 1 if knn_label_1 != knn_label_2 else 0\n\n        all_results.append([x1, x2, x3])\n\n    # Final print statement in the exact required format.\n    # e.g., [[x_1,x_2,x_3],[y_1,y_2,y_3]]\n    result_str = \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}