{
    "hands_on_practices": [
        {
            "introduction": "在代数中，二次方程求根公式是一个精确而可靠的工具。然而，在有限精度的计算世界里，直接套用这个我们熟知的公式有时会得出严重失准的答案。本练习将引导你亲手计算一个特殊二次方程的根，揭示“灾难性抵消”这一数值计算中的关键问题，并学习如何通过一个简单的变换来恢复计算精度 。",
            "id": "2393691",
            "problem": "考虑一个实系数二次多项式 $p(x) = x^2 - 10^8 x + 1$。本题中所有的算术运算都在一个标准化的10进制浮点系统中进行，该系统具有 $t=8$ 位有效数字的精度，采用“四舍六入五成双”的舍入规则，并且具有足够大的指数范围，以确保所遇到的数值不会发生上溢或下溢。每个基本运算（加、减、乘、除和平方根）都先被精确执行，然后根据所述规则舍入到最接近的可表示浮点数，之后再用于任何后续计算。\n\n(a) 在此浮点系统中使用标准二次公式，计算将作为 $p(x)=0$ 的根返回的两个浮点数。\n\n(b) 然后，仅使用对首一二次多项式精确成立的恒等式（特别是，两根之积等于 $c/a$ 这一事实），通过首先在此浮点系统中使用标准二次公式计算较大幅值的根，然后使用精确的乘积关系在此浮点系统中获得较小幅值的根，来计算一个数值稳定的较小幅值根。\n\n仅报告(b)部分得到的稳定的较小幅值根作为最终答案，并四舍五入到八位有效数字。以科学记数法形式 $a \\times 10^b$（其中 $1 \\le |a|  10$）表示您的最终答案。",
            "solution": "该问题要求在一个指定的浮点算术系统内计算一个二次多项式的根。在进行求解之前，我们必须验证问题陈述的有效性。\n\n已知条件如下：\n1.  二次多项式为 $p(x) = x^2 - 10^8 x + 1$。由此，我们确定系数为 $a=1$，$b=-10^8$ 和 $c=1$。\n2.  算术框架是一个标准化的10进制浮点系统。\n3.  精度为 $t=8$ 位有效数字。\n4.  舍入规则是四舍六入五成双。\n5.  不会发生上溢或下溢。\n6.  每个基本算术运算（$+$、$-$、$\\*$、$/$、$\\sqrt{\\cdot}$）都单独进行舍入。\n\n该问题具有科学依据，是数值分析中关于有效数字损失的一个标准练习。问题提法恰当，所有必要参数均已定义。问题是客观的，其结构是合理的。因此，该问题被认为是有效的。我们开始求解。\n\n二次方程 $ax^2 + bx + c = 0$ 的根由标准二次公式给出：\n$$x_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\n所有计算都在指定的浮点系统中执行，我们用 $fl(\\cdot)$ 表示。\n\n首先，我们计算判别式 $D = b^2 - 4ac$。\n系数为 $a = 1.0000000 \\times 10^0$，$b = -1.0000000 \\times 10^8$ 和 $c = 1.0000000 \\times 10^0$。\n$b^2$ 项是 $(-10^8)^2 = 10^{16}$。这可以精确表示为 $1.0000000 \\times 10^{16}$。\n$4ac$ 项是 $4 \\times 1 \\times 1 = 4$。这可以精确表示为 $4.0000000 \\times 10^0$。\n\n我们现在必须计算减法 $fl(b^2 - 4ac) = fl(10^{16} - 4)$。为了执行此操作，必须移动较小数的小数点以匹配较大数的指数。\n$$10^{16} - 4 = (1.0000000 \\times 10^{16}) - (0.0000000000000004 \\times 10^{16})$$\n$10^{16}$ 的尾数是 $1.0000000$。该系统存储 $t=8$ 位有效数字。减去 $4$ 会影响远超出系统精度的数字位。尾数的减法是 $1.0000000 - 0.0000000000000004$，当舍入回 $8$ 位有效数字时，结果仍然是 $1.0000000$。这是一个典型的“大数吃小数”（absorption）现象。\n因此，计算出的判别式是：\n$$\\hat{D} = fl(b^2 - 4ac) = 1.0000000 \\times 10^{16}$$\n接下来，我们计算 $\\hat{D}$ 的平方根。\n$$fl(\\sqrt{\\hat{D}}) = fl(\\sqrt{1.0000000 \\times 10^{16}}) = 1.0000000 \\times 10^8$$\n这是一个精确计算。我们记此结果为 $\\hat{S} = 1.0000000 \\times 10^8$。\n\n现在，我们按照(a)部分的要求计算两个根。\n较大的根 $\\hat{x}_1$ 使用分子中的“+”号计算，因为 $-b = 10^8$ 是正数。\n$$\\hat{x}_1 = fl\\left(\\frac{-b + \\hat{S}}{2a}\\right) = fl\\left(\\frac{10^8 + 10^8}{2}\\right) = fl\\left(\\frac{2 \\times 10^8}{2}\\right) = 1.0000000 \\times 10^8$$\n这个计算是数值稳定的，因为它涉及两个数量级相似的正数相加。\n\n较小的根 $\\hat{x}_2$ 使用“-”号计算。\n$$\\hat{x}_2 = fl\\left(\\frac{-b - \\hat{S}}{2a}\\right) = fl\\left(\\frac{10^8 - 10^8}{2}\\right) = fl\\left(\\frac{0}{2}\\right) = 0$$\n这个计算遭受了灾难性抵消。两个几乎相等的数 $-b$ 和 $\\hat{S}$ 相减，导致有效数字的完全丧失。计算出的根 $\\hat{x}_2=0$ 非常不准确。真实的较小根约为 $10^{-8}$。\n\n对于(b)部分，我们被要求使用一种数值稳定的方法。对于首一二次方程 $x^2 + \\frac{b}{a}x + \\frac{c}{a} = 0$，Vieta's 公式（韦达定理）指出，根的乘积是 $x_1 x_2 = \\frac{c}{a}$。\n在我们的例子中，$a=1$ 且 $c=1$，所以 $x_1 x_2 = 1$。\n\n稳定的步骤是首先计算较大幅值的根 $\\hat{x}_1$，我们已经发现它是准确的：\n$$\\hat{x}_1 = 1.0000000 \\times 10^8$$\n然后，我们使用乘积关系来找到较小的根 $\\hat{x}'_2$：\n$$\\hat{x}'_2 = fl\\left(\\frac{c/a}{\\hat{x}_1}\\right)$$\n代入数值：\n$$\\hat{x}'_2 = fl\\left(\\frac{1}{1.0000000 \\times 10^8}\\right) = fl(1.0000000 \\times 10^{-8})$$\n数字 $1.0000000 \\times 10^{-8}$ 在指定的浮点系统中是精确可表示的。因此，计算得出该值，没有任何舍入误差。\n稳定的较小根是：\n$$\\hat{x}'_2 = 1.0000000 \\times 10^{-8}$$\n该值是要求的最终答案，以科学记数法表示，并保留八位有效数字。",
            "answer": "$$\\boxed{1.0000000 \\times 10^{-8}}$$"
        },
        {
            "introduction": "在上一个练习中我们看到单次减法如何导致巨大误差，而本练习将探讨一个更隐蔽的问题：微小误差在多次运算中的累积效应。你将通过编程探究为何重复将 $0.1$ 相加的结果会与直接的乘法结果产生偏差，这个过程将揭示一个核心事实：许多我们习以为常的十进制小数在计算机的二进制浮点世界里并非精确值 。",
            "id": "2393733",
            "problem": "在电气与电子工程师协会 (IEEE) 754 单精度（通常表示为 $\\texttt{float32}$）标准所定义的有限精度算术中，每次基本算术运算都会发生舍入。考虑用两种方法计算涉及同一加数的部分和：重复加法与单次乘法。尽管这些表达式在实数算术中是代数恒等的，但它们在浮点算术中不一定相等，因为舍入在不同的步骤中应用。\n\n从基本的浮点舍入模型出发，对于在单精度下执行的任何基本运算 $\\circ \\in \\{+,-,\\times,\\div\\}$，其计算结果可以建模为\n$$\n\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta), \\quad |\\delta| \\le u,\n$$\n其中 $u$ 是单位舍入误差。对于精度 $p = 24$（包括隐含的前导位）且基数为 $2$ 的 IEEE 754 单精度标准，单位舍入误差为 $u = 2^{-24}$。在本问题中，所有算术都必须在 IEEE 754 单精度下执行，并采用“向最近的偶数舍入”（round-to-nearest, ties-to-even）的规则。\n\n对于给定的实数步长 $s$，为每个整数 $N \\ge 1$ 定义以下两个单精度计算量：\n- 累加和\n$$\nS_N = \\operatorname{fl}\\left(\\sum_{i=1}^{N} s\\right),\n$$\n其中，该和是通过对一个在单精度下初始化为 $0$ 的累加器，连续进行 $N$ 次单精度值 $s$ 的加法来执行的。\n- 基于乘积的值\n$$\nP_N = \\operatorname{fl}\\big(\\operatorname{fl}(N) \\times \\operatorname{fl}(s)\\big),\n$$\n即，整数 $N$ 首先被转换为单精度，然后在单精度下乘以 $s$ 的单精度值，结果再舍入到单精度。\n\n任务：\n- 对于每个测试用例，找到范围 $1 \\le N \\le N_{\\max}$ 内最大的整数 $N$，使得 $S_N$ 与 $P_N$ 作为单精度数完全相等。此处的完全相等意味着两个 $\\texttt{float32}$ 值在相等运算符下比较为真，这等同于其单精度表示的按位等价。\n\n实现要求：\n- 所有中间量，包括累加器、加数、转换后的整数和乘积，都必须是 IEEE 754 单精度。在累加 $S_N$ 的过程中，每次加法都必须应用单精度舍入。\n- 程序必须将输入步长 $s$ 视为一个实数小数常量，在使用前舍入到单精度，这与小数常量转换为 $\\texttt{float32}$ 的方式一致。\n- 不涉及物理单位；所有量都是无量纲的。\n\n测试套件：\n- 用例 1：$s = 0.1$, $N_{\\max} = 1{,}000{,}000$。\n- 用例 2：$s = 0.125$, $N_{\\max} = 1{,}000{,}000$。\n- 用例 3：$s = 1\\times 10^{-8}$, $N_{\\max} = 2{,}000{,}000$。\n- 用例 4：$s = 9.5367431640625\\times 10^{-7}$ (此值恰好为 $2^{-20}$), $N_{\\max} = 2{,}000{,}000$。\n\n输出规格：\n- 您的程序应生成单行输出，其中包含按顺序为四个用例找到的最大 $N$ 值，格式为逗号分隔的列表并用方括号括起，例如 $[N_1,N_2,N_3,N_4]$。",
            "solution": "问题陈述已经过验证，被认为是科学上合理、良构且无歧义的。它提出了计算科学中的一个经典问题，特别是在数值分析领域，该问题涉及由 IEEE 754 标准定义的有限精度浮点算术的影响。因此可以推导出一个严谨的解。\n\n问题的核心是比较在单精度算术中计算表达式 $N \\times s$ 的两种不同方法的结果，并找出在给定范围 $[1, N_{\\max}]$ 内使两种方法产生相同结果的最大整数 $N$。\n\n这两种方法是：\n1.  **累加和 ($S_N$)**：通过迭代加法计算。令 $s_{fp} = \\operatorname{fl}(s)$ 为实数 $s$ 的单精度表示。和被初始化为 $S_0 = \\operatorname{fl}(0)$，然后迭代计算：$S_k = \\operatorname{fl}(S_{k-1} + s_{fp})$，其中 $k = 1, \\dots, N$。每次加法都会产生舍入误差，这些误差会在 $N$ 个步骤中累积。\n2.  **基于乘积的值 ($P_N$)**：通过单次乘法计算。整数 $N$ 首先被转换为其单精度表示 $N_{fp} = \\operatorname{fl}(N)$，然后乘以 $s_{fp}$：$P_N = \\operatorname{fl}(N_{fp} \\times s_{fp})$。\n\n该问题要求对这两个计算过程进行直接模拟。对于从 $1$ 到 $N_{\\max}$ 的每个整数 $N$，我们必须使用单精度算术计算 $S_N$ 和 $P_N$，并检查它们是否完全相等。对于该测试用例，使 $S_N = P_N$ 成立的最大 $N$ 值就是所求结果。对于所有测试用例，$N_{\\max}$ 小于 $2^{24} = 16,777,216$。由于单精度浮点数有 24 位的尾数（包括隐含位），任何大小小于或等于 $2^{24}$ 的整数 $N$ 都可以被精确表示。因此，对于本问题中所有相关的 $N$，转换 $\\operatorname{fl}(N)$ 都是精确的，即 $N_{fp} = N$。因此，基于乘积的值简化为 $P_N = \\operatorname{fl}(N \\times s_{fp})$。\n\n我们现在根据步长值 $s$ 的属性来分析每个测试用例。\n\n**用例 2 ($s = 0.125$) 和 用例 4 ($s = 9.5367431640625\\times 10^{-7} = 2^{-20}$)**\n\n在这两个用例中，步长值 $s$ 是一个在二进制下具有精确、有限表示的实数。\n- 用例 2, $s = 0.125 = 1/8 = 2^{-3}$。\n- 用例 4, $s = 2^{-20}$。\n\n因为 $s_{fp}$ 是 $s$ 的精确表示，所以在 $s_{fp}$ 中没有初始舍入误差。此外，在所考虑的 $N$ 的范围内（$N \\le N_{\\max}  2^{24}$），数学乘积 $N \\times s$ 也可以在单精度下精确表示。这是因为 $N \\times 2^{-k}$ 仅仅对应于 $N$ 的整数表示的位移，这不会增加所需有效位的数量。因此，浮点乘法 $\\operatorname{fl}(N \\times s_{fp})$ 不会产生舍入误差，且 $P_N = N \\times s$。\n\n类似地，对于累加和 $S_N$，对于 $k \\le N_{\\max}$，每个中间和 $S_k = \\sum_{i=1}^k s$ 也是可以精确表示的。这意味着加法 $S_{k-1} + s_{fp}$ 在每一步都是精确的，即 $\\operatorname{fl}(S_{k-1} + s_{fp}) = S_{k-1} + s_{fp}$。根据归纳法，$S_N = \\sum_{i=1}^N s = N \\times s$。\n\n由于两种计算都得到真实的数学值 $N \\times s$，因此对于范围 $1 \\le N \\le N_{\\max}$ 内的所有 $N$，$S_N = P_N$。因此，使等式成立的最大整数 $N$ 就是 $N_{\\max}$。\n- 对于用例 2，结果是 $N_{\\max} = 1,000,000$。\n- 对于用例 4，结果是 $N_{\\max} = 2,000,000$。\n\n**用例 1 ($s = 0.1$) 和 用例 3 ($s = 1 \\times 10^{-8}$)**\n\n在这些用例中，步长值 $s$ 没有精确的有限二进制表示。\n- $s = 0.1$ 在二进制中是循环小数 $0.0001100110011\\dots_2$。\n- $s = 10^{-8}$ 同样会导致一个无限二进制展开。\n\n因此，初始的单精度转换会引入一个表示误差：$s_{fp} = s(1+\\epsilon)$，其中 $\\epsilon$ 是某个很小的值。\n\n$S_N$ 的计算涉及 $N-1$ 次加法，每次都可能引入新的舍入误差。$S_N$ 中的误差是 $s_{fp}$ 的初始误差和加法累积误差共同作用的结果。和的误差的标准模型很复杂，但它表明误差是路径依赖的，并且随 $N$ 的增大而增长。\n$$S_N = \\operatorname{fl}\\left(\\sum_{i=1}^{N} s_{fp}\\right) \\approx N s_{fp} + E_{sum}$$\n$P_N$ 的计算只涉及一次乘法，因此在 $s$ 的初始表示之后只有一个舍入误差。\n$$P_N = \\operatorname{fl}(N \\times s_{fp}) = (N \\times s_{fp})(1+\\delta_N)$$\n由于误差传播机制不同，通常不期望 $S_N$ 和 $P_N$ 相等。虽然对于较小的 $N$ 它们会相等（例如 $N=1$，通常 $N=2$ 也成立，因为 $\\operatorname{fl}(x+x) = \\operatorname{fl}(2x)$ 通常为真），但随着 $S_N$ 中的累积误差变得显著，它们最终会偏离。\n\n要找到使等式成立的最大 $N$，需要进行直接模拟。不能安全地假设一旦 $S_N \\neq P_N$，对于更大的 $N$ 它们就永远不会再相等，因为误差可能会巧合地抵消。因此，模拟必须遍历从 $N=1$ 到 $N_{\\max}$ 的整个范围，并记录满足相等条件的最后一个 $N$ 值。\n\n实现将通过对每个测试用例的所有指定 $N$ 值进行循环来完成。所有算术运算都将使用 `numpy.float32` 数据类型来严格执行 IEEE 754 单精度语义。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the floating-point comparison problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (s_as_string, N_max).\n    # Using strings for 's' ensures correct parsing to float32 without\n    # intermediate double-precision representation.\n    test_cases = [\n        ('0.1', 1_000_000),\n        ('0.125', 1_000_000),\n        ('1e-8', 2_000_000),\n        ('9.5367431640625e-7', 2_000_000),  # This is 2**-20\n    ]\n\n    results = []\n    \n    for s_str, n_max in test_cases:\n        # Convert the step value `s` to its single-precision (float32) representation.\n        s_fp = np.float32(s_str)\n        \n        # Initialize the accumulator for S_N. It must be float32 to ensure\n        # single-precision accumulation.\n        accumulated_sum = np.float32(0.0)\n        \n        # This variable will store the largest N for which S_N == P_N.\n        largest_n_equal = 0\n        \n        # Loop from N = 1 to N_max to find the largest N where equality holds.\n        # We must check the entire range, as a divergence might be followed\n        # by a coincidental re-convergence.\n        for n in range(1, n_max + 1):\n            # Compute S_N: perform one more single-precision addition.\n            accumulated_sum += s_fp\n            \n            # Compute P_N:\n            # 1. Convert integer N to float32. For N = 2**24, this is an exact conversion.\n            #    All N_max values in the test suite are less than 2**24.\n            n_fp = np.float32(n)\n            \n            # 2. Perform the single-precision multiplication.\n            product_val = n_fp * s_fp\n            \n            # Compare the results. For numpy scalars, `==` performs an exact bitwise comparison.\n            if accumulated_sum == product_val:\n                largest_n_equal = n\n                \n        results.append(largest_n_equal)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在探究了浮点误差的产生与累积之后，这最后一个练习将向你介绍一种用于主动管理这些误差的高级技术。你将实现一种名为“补偿算法”的方法，来捕捉并“补偿”减法运算中损失的精度。通过这个实践，你将了解到如何恢复丢失的精度，从而编写出更加稳健的数值代码，实现从识别误差到修正误差的跨越 。",
            "id": "3131169",
            "problem": "您将获得若干实数对，需要在电子电气工程师协会 (IEEE) 754 binary64 格式（双精度）下进行表示和计算。此任务的基本依据包括：IEEE 754 的“向最近舍入，偶数优先”(rounding to nearest, ties to even) 规则；机器ε（在 binary64 中 $1$ 与下一个可表示数之间的距离）的定义；以及 Sterbenz 引理，该引理指出如果两个浮点数 $x$ 和 $y$ 满足 $0.5 \\leq \\left|x/y\\right| \\leq 2$，那么减法 $x - y$ 在二进制浮点运算中可以精确执行，不会产生舍入误差。您的目标是，当计算 $x - y$（其中 $x \\approx y$）时，经验性地评估其数值误差，评估方法包括直接计算和使用补偿算法（将 Dekker 加法无误差变换，通常称为“two-sum”算法，应用于 $x + (-y)$）。您必须量化补偿算法相对于直接减法恢复了多少精度（以二进制位数衡量）。\n\n待完成的任务：\n- 将每个输入对 $(x, y)$ 表示为 IEEE 754 binary64 值。\n- 使用 binary64 运算计算直接差值 $d_{\\text{naive}} = \\mathrm{fl}(x - y)$。\n- 将 Dekker 加法无误差变换应用于 $a + b$（其中 $a = x$ 且 $b = -y$），生成一个高位部分 $h$ 和一个低位部分 $\\ell$，并将 $(h, \\ell)$ 解释为精确和 $x + (-y)$ 的补偿表示。\n- 通过将 $x$ 和 $y$ 转换为其 IEEE 754 所隐含的精确有理数值（使用 binary64 表示的精确整数比），然后在精确有理数运算中构成 $d_{\\text{true}} = x - y$，来构建一个数学上精确的基准差值 $d_{\\text{true}}$。\n- 定义直接绝对误差 $E_{\\text{naive}} = \\left|d_{\\text{naive}} - d_{\\text{true}}\\right|$ 和补偿绝对误差 $E_{\\text{comp}} = \\left|(h + \\ell) - d_{\\text{true}}\\right|$，其中求和 $h + \\ell$ 必须在精确有理数运算中进行，即将 $h$ 和 $\\ell$ 都转换为其 IEEE 754 所隐含的精确有理数值。\n- 将恢复的精度位数定义为\n$$\nB = \\begin{cases}\n0,  \\text{若 } E_{\\text{naive}} = 0 \\text{ 且 } E_{\\text{comp}} = 0, \\\\\n\\log_2\\left(\\dfrac{E_{\\text{naive}}}{E_{\\text{comp}}}\\right),  \\text{若 } E_{\\text{naive}}  0 \\text{ 且 } E_{\\text{comp}}  0, \\\\\n53,  \\text{若 } E_{\\text{naive}}  0 \\text{ 且 } E_{\\text{comp}} = 0,\n\\end{cases}\n$$\n其中 $53$ 是 IEEE 754 binary64 尾数中的二进制位数。将每个测试用例的 $B$ 值报告为一个浮点数。\n\n使用以下测试套件（所有值都将在 binary64 中解释和计算）：\n- 测试用例 $1$：$x = 1.234567890123456$， $y = 1.234567890123455$（十进制数量级接近，典型的相消场景）。\n- 测试用例 $2$：$x = 1 \\times 10^{308}$， $y = \\operatorname{nextafter}(x, 0)$，其中 $\\operatorname{nextafter}(x, 0)$ 表示在朝向 $0$ 的方向上最接近 $x$ 的可表示 binary64 值（大数量级，最后一个位置单位的分离）。\n- 测试用例 $3$：$x = 1 + 2^{-30}$， $y = 1 + 2^{-30} - 2^{-52}$（在 $1$ 附近发生相消，间距与机器ε相关）。\n- 测试用例 $4$：$x = 2^{-1022}$， $y = x - 2^{-1074}$（在非规格化数范围内求差；$2^{-1022}$ 是 binary64 中最小的正规格化数，$2^{-1074}$ 是最小的正非规格化数）。\n- 测试用例 $5$：$x = 2^{53} + 1$ 在 binary64 中解释，$y = 2^{53}$ 在 binary64 中解释（精确整数表示的边界）。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3,result4,result5]”），每个结果是对应测试用例的 $B$ 值，表示为 Python 浮点数。不涉及物理单位或角度单位，所有量均为纯实数。\n\n程序必须是自包含的，不需要任何输入，并严格遵守指定的执行环境。目标是测试 IEEE 754 舍入、精确性属性、机器ε的原理，以及补偿算法在减法近似相消时的实际行为。",
            "solution": "用户希望验证并解决一个关于浮点运算的问题。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n-   **计算系统**：IEEE 754 binary64 格式（双精度）。\n-   **舍入规则**：向最近舍入，偶数优先。\n-   **核心概念**：机器ε、Sterbenz 引理 ($0.5 \\leq |x/y| \\leq 2 \\implies \\mathrm{fl}(x - y) = x - y$)、Dekker 无误差变换 (two-sum)。\n-   **目标**：量化对于 $x \\approx y$ 的数对 $(x, y)$，补偿算法相对于直接减法所恢复的精度位数。\n\n-   **计算任务**：\n    1.  将输入 $(x, y)$ 表示为 binary64 值。\n    2.  计算直接差值：$d_{\\text{naive}} = \\mathrm{fl}(x - y)$。\n    3.  计算补偿和：将 two-sum 算法应用于 $x + (-y)$，得到数对 $(h, \\ell)$。\n    4.  计算基准差值：将 binary64 的 $x, y$ 转换为精确有理数并计算 $d_{\\text{true}} = x - y$。\n    5.  计算误差：\n        -   直接绝对误差：$E_{\\text{naive}} = |d_{\\text{naive}} - d_{\\text{true}}|$，使用精确有理数运算。\n        -   补偿绝对误差：$E_{\\text{comp}} = |(h + \\ell) - d_{\\text{true}}|$，其中 $h+\\ell$ 在精确有理数运算中计算。\n    6.  计算恢复的精度位数 $B$：\n        $$\n        B = \\begin{cases}\n        0,  \\text{若 } E_{\\text{naive}} = 0 \\text{ 且 } E_{\\text{comp}} = 0, \\\\\n        \\log_2\\left(\\dfrac{E_{\\text{naive}}}{E_{\\text{comp}}}\\right),  \\text{若 } E_{\\text{naive}}  0 \\text{ 且 } E_{\\text{comp}}  0, \\\\\n        53,  \\text{若 } E_{\\text{naive}}  0 \\text{ 且 } E_{\\text{comp}} = 0,\n        \\end{cases}\n        $$\n        其中 $53$ 是 binary64 的尾数精度。\n\n-   **测试用例（将在 binary64 中解释）**：\n    1.  $x = 1.234567890123456$, $y = 1.234567890123455$。\n    2.  $x = 1 \\times 10^{308}$, $y = \\operatorname{nextafter}(x, 0)$。\n    3.  $x = 1 + 2^{-30}$, $y = 1 + 2^{-30} - 2^{-52}$。\n    4.  $x = 2^{-1022}$, $y = x - 2^{-1074}$。\n    5.  $x = 2^{53} + 1$, $y = 2^{53}$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n-   **科学依据**：该问题基于数值分析和 IEEE 754 标准的既定原则。诸如灾难性相消、补偿算法 (Dekker 算法)、机器ε、非规格化数和舍入规则等概念是计算科学的核心。该问题在科学上是合理的。\n-   **适定性**：问题定义清晰。输入、所需计算、误差度量和最终输出格式都已精确定义。$B$ 的公式涵盖了预期结果 ($E_{\\text{naive}} \\ge 0, E_{\\text{comp}} \\ge 0$)。对于 $E_{\\text{naive}}  0$ 且补偿变换产生精确结果 ($E_{\\text{comp}} = 0$) 的情况，通过赋予 53 位的结果来处理，这对应于 binary64 格式的完全精度恢复。该问题是适定的。\n-   **客观性**：问题以客观、技术性的语言陈述，没有偏见或主观论断。\n\n该问题没有表现出说明中列出的任何无效性缺陷。这是一个计算科学领域中有效且不平凡的问题。\n\n**步骤 3：结论与行动**\n\n问题是**有效的**。我将继续提供一个完整的解决方案。\n\n### 解决方案\n\n解决方案要求为每个测试用例实现指定的计算。这涉及到标准浮点运算和精确有理数运算的结合。所有数学实体都按要求使用 LaTeX 排版。\n\n1.  **框架设置**：我们使用 Python，其中标准的 `float` 对象对应于 IEEE 754 binary64。`numpy` 库提供了 `nextafter` 函数。对于精确有理数运算，Python 的 `fractions.Fraction` 类是理想的选择，因为 `Fraction(some_float)` 能将一个 binary64 数正确地转换为其精确的有理数表示，$M/2^k$，其中 $M, k \\in \\mathbb{Z}$。\n\n2.  **Dekker 的 `two-sum` 算法**：问题指定使用 Dekker 的加法无误差变换，通常称为 `two-sum` 算法。给定两个浮点数 $a$ 和 $b$，该算法计算出两个数，一个高位部分 $h$ 和一个低位部分 $\\ell$，使得 $h = \\mathrm{fl}(a+b)$ 且 $h+\\ell = a+b$ 精确成立。不要求 $|a| \\geq |b|$ 的标准算法是：\n    -   $h = a + b$\n    -   $a' = h - b$\n    -   $b' = h - a'$\n    -   $\\delta_a = a - a'$\n    -   $\\delta_b = b - b'$\n    -   $\\ell = \\delta_a + \\delta_b$\n    所有运算都是标准浮点运算。在我们的问题中，我们将其应用于 $a=x$ 和 $b=-y$。\n\n3.  **基准真相与误差计算**：对于每个测试用例 $(x, y)$：\n    -   输入首先被实现为 binary64 值。\n    -   直接差值计算为 $d_{\\text{naive}} = \\mathrm{fl}(x - y)$。\n    -   基准差值 $d_{\\text{true}}$ 通过将 $x$ 和 $y$ 转换为 `Fraction` 对象然后相减来计算：$d_{\\text{true}} = \\text{Fraction}(x) - \\text{Fraction}(y)$。\n    -   直接误差为 $E_{\\text{naive}} = |\\text{Fraction}(d_{\\text{naive}}) - d_{\\text{true}}|$。\n    -   将 `two-sum` 算法应用于 $(x, -y)$，得到 $(h, \\ell)$。补偿和在精确有理数中计算为 $\\text{Fraction}(h) + \\text{Fraction}(\\ell)$。\n    -   补偿误差为 $E_{\\text{comp}} = |(\\text{Fraction}(h) + \\text{Fraction}(\\ell)) - d_{\\text{true}}|$。根据无误差变换的性质，这个误差 $E_{\\text{comp}}$ 预期为精确的 $0$。\n\n4.  **测试用例分析**：\n    -   **用例 1**：$x = 1.234567890123456, y = 1.234567890123455$。这些十进制字面量被转换为它们最接近的 binary64 表示，我们称之为 $x_f$ 和 $y_f$。减法 $x_f - y_f$ 是灾难性相消的典型例子。这些 binary64 数的精确差值 $d_{\\text{true}}$ 非零。直接计算的浮点结果 $d_{\\text{naive}} = \\mathrm{fl}(x_f - y_f)$ 会产生舍入误差，因此 $E_{\\text{naive}}  0$。补偿和是精确的，所以 $E_{\\text{comp}} = 0$。因此，我们预期恢复全部精度，即 $B=53$。\n    -   **用例 2**：$x = 1 \\times 10^{308}, y = \\operatorname{nextafter}(x, 0)$。这里，$x$ 和 $y$ 是相邻的 binary64 数。它们的比值 $|x/y|$ 极度接近 $1$，满足 Sterbenz 引理的条件 ($0.5 \\le |x/y| \\le 2$)。因此，减法 $d_{\\text{naive}} = \\mathrm{fl}(x-y)$ 保证是精确的。这意味着 $E_{\\text{naive}} = 0$。因此，$E_{\\text{comp}}$ 也将为 $0$，并且 $B=0$。\n    -   **用例 3**：$x = 1 + 2^{-30}, y = 1 + 2^{-30} - 2^{-52}$。$x$ 和 $y$（由浮点计算定义）在 binary64 中都可以精确表示。它们的比值也接近 $1$，因此 Sterbenz 引理适用于它们的减法。$d_{\\text{naive}}$ 将是精确的，意味着 $E_{\\text{naive}} = 0$，导致 $B=0$。\n    -   **用例 4**：$x = 2^{-1022}, y = x - 2^{-1074}$。这里，$x$ 是最小的正规格化数，$y$ 是一个非规格化数。两者都可以精确表示。差值 $d_{\\text{true}} = 2^{-1074}$ 是最小的正非规格化数，也可以精确表示。比值 $|x/y|$ 接近 $1$，因此 Sterbenz 引理适用。减法是精确的，$E_{\\text{naive}} = 0$，并且 $B=0$。\n    -   **用例 5**：$x = 2^{53} + 1, y = 2^{53}$。值 $2^{53}+1$ 在 binary64 中无法表示。它恰好位于两个可表示浮点数 $2^{53}$ 和 $2^{53}+2$ 的中间。根据“向最近舍入，偶数优先”规则，$2^{53}+1$ 会舍入到 $2^{53}$，因为 $2^{53}$ 的尾数是偶数。因此，在 binary64 中，输入值变为 $x = 2^{53}$ 和 $y = 2^{53}$。减法 $x-y$ 显然为 $0$。$d_{\\text{naive}}$ 和 $d_{\\text{true}}$ 都为 $0$，所以 $E_{\\text{naive}} = 0$ 且 $B=0$。\n\n实现将执行这些步骤，并报告每个用例计算出的 $B$ 值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\nfrom fractions import Fraction\n\ndef solve():\n    \"\"\"\n    Solves the floating-point arithmetic problem as described.\n    \"\"\"\n\n    def two_sum(a: float, b: float) - tuple[float, float]:\n        \"\"\"\n        Implements Dekker's error-free transform for addition (two-sum).\n        Given two floating-point numbers a and b, returns a pair (h, l)\n        such that h = fl(a + b) and h + l = a + b exactly.\n        All internal calculations are floating-point operations.\n        Reference: Ogita, T., Rump, S. M.,  Oishi, S. (2005).\n        \"Accurate sum and dot product.\"\n        \"\"\"\n        h = a + b\n        a_prime = h - b\n        b_prime = h - a_prime\n        delta_a = a - a_prime\n        delta_b = b - b_prime\n        l = delta_a + delta_b\n        return (h, l)\n\n    # Define the test cases from the problem statement.\n    # Case 1: Close decimal magnitudes, typical cancellation scenario.\n    x1 = 1.234567890123456\n    y1 = 1.234567890123455\n    \n    # Case 2: Large magnitude with one ULP separation.\n    x2 = 1.0e308\n    y2 = np.nextafter(x2, 0.0)\n    \n    # Case 3: Cancellation near 1 with spacing tied to machine epsilon.\n    x3 = 1.0 + 2.0**-30\n    y3 = (1.0 + 2.0**-30) - 2.0**-52\n    \n    # Case 4: Difference in the subnormal regime.\n    x4 = 2.0**-1022\n    y4 = x4 - 2.0**-1074\n\n    # Case 5: Boundary of exact integer representability.\n    # float(2**53 + 1) is rounded to 2**53 due to ties-to-even rule.\n    x5 = float(2**53 + 1)\n    y5 = float(2**53)\n\n    test_cases = [\n        (x1, y1),\n        (x2, y2),\n        (x3, y3),\n        (x4, y4),\n        (x5, y5),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x, y = case\n        \n        # Compute the naive difference using binary64 arithmetic.\n        d_naive = x - y\n        \n        # Apply Dekker’s error-free transform for addition to x + (-y).\n        h, l = two_sum(x, -y)\n        \n        # Construct the mathematically exact ground truth difference.\n        x_frac = Fraction(x)\n        y_frac = Fraction(y)\n        d_true = x_frac - y_frac\n        \n        # Define the absolute naive error.\n        d_naive_frac = Fraction(d_naive)\n        E_naive = abs(d_naive_frac - d_true)\n        \n        # Define the compensated absolute error.\n        h_frac = Fraction(h)\n        l_frac = Fraction(l)\n        comp_sum_frac = h_frac + l_frac\n        E_comp = abs(comp_sum_frac - d_true)\n\n        # Define the bits of precision recovered.\n        B = 0.0\n        if E_naive == Fraction(0) and E_comp == Fraction(0):\n            B = 0.0\n        elif E_naive  Fraction(0) and E_comp == Fraction(0):\n            B = 53.0\n        elif E_naive  Fraction(0) and E_comp  Fraction(0):\n            # This case implies the two-sum was not perfectly error-free\n            # in practice, or d_true itself had some representation issues.\n            # We proceed with the formula as specified.\n            B = math.log2(float(E_naive / E_comp))\n        \n        results.append(B)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}