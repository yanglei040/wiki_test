{
    "hands_on_practices": [
        {
            "introduction": "Understanding floating-point arithmetic begins with recognizing its potential pitfalls. This first exercise uncovers one of the most common sources of numerical error: catastrophic cancellation. By manually computing the roots of a quadratic equation in a simulated low-precision environment, you will see how a mathematically correct formula can produce grossly inaccurate results and learn how a simple algebraic rearrangement can restore precision .",
            "id": "2393691",
            "problem": "Consider the quadratic polynomial $p(x) = x^2 - 10^8 x + 1$ with real coefficients. All arithmetic in this problem is to be carried out in a normalized base-$10$ floating-point system with precision $t=8$ significant digits, rounding to nearest with ties to even, and with a sufficiently large exponent range so that no overflow or underflow occurs for the quantities encountered. Each elementary operation (addition, subtraction, multiplication, division, and square root) is performed exactly and then rounded to the nearest representable floating-point number according to the stated rule before being used in any subsequent computation.\n\n(a) Using the standard quadratic formula applied in this floating-point system, compute the two floating-point numbers that would be returned as the roots of $p(x)=0$.\n\n(b) Then, using only identities that hold exactly for monic quadratics (in particular, the fact that the product of the two roots equals $c/a$), compute a numerically stabilized value for the smaller-magnitude root by first computing the larger-magnitude root with the standard quadratic formula in this floating-point system and then using the exact product relation to obtain the smaller-magnitude root in this floating-point system.\n\nReport only the stabilized smaller-magnitude root from part (b) as your final answer, rounded to eight significant figures. Express your final answer in scientific notation of the form $a \\times 10^b$ with $1 \\leq |a|  10$.",
            "solution": "The problem requires the computation of the roots of a quadratic polynomial within a specified floating-point arithmetic system. Before proceeding to a solution, we must validate the problem statement.\n\nThe givens are:\n1.  The quadratic polynomial is $p(x) = x^2 - 10^8 x + 1$. From this, we identify the coefficients as $a=1$, $b=-10^8$, and $c=1$.\n2.  The arithmetic framework is a normalized base-$10$ floating-point system.\n3.  The precision is $t=8$ significant digits.\n4.  The rounding rule is round to nearest, with ties to even.\n5.  There is no overflow or underflow.\n6.  Each elementary arithmetic operation ($+$, $-$, $*$, $/$, $\\sqrt{\\cdot}$) is individually rounded.\n\nThe problem is scientifically grounded, being a standard exercise in numerical analysis concerning loss of significance. It is well-posed, with all necessary parameters defined. It is objective and its structure is sound. The problem is therefore deemed valid. We proceed to the solution.\n\nThe roots of the quadratic equation $ax^2 + bx + c = 0$ are given by the standard quadratic formula:\n$$x_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\nAll computations are performed in the specified floating-point system, which we denote by $fl(\\cdot)$.\n\nFirst, we compute the discriminant, $D = b^2 - 4ac$.\nThe coefficients are $a = 1.0000000 \\times 10^0$, $b = -1.0000000 \\times 10^8$, and $c = 1.0000000 \\times 10^0$.\nThe term $b^2$ is $(-10^8)^2 = 10^{16}$. This is represented exactly as $1.0000000 \\times 10^{16}$.\nThe term $4ac$ is $4 \\times 1 \\times 1 = 4$. This is represented exactly as $4.0000000 \\times 10^0$.\n\nWe must now compute the subtraction $fl(b^2 - 4ac) = fl(10^{16} - 4)$. To perform this operation, the smaller number must be shifted to match the exponent of the larger number.\n$$10^{16} - 4 = (1.0000000 \\times 10^{16}) - (0.0000000000000004 \\times 10^{16})$$\nThe mantissa for $10^{16}$ is $1.0000000$. The system stores $t=8$ significant digits. The subtraction of $4$ affects digits far beyond the precision of the system. The subtraction of the mantissas is $1.0000000 - 0.0000000000000004$, which, when rounded back to $8$ significant digits, remains $1.0000000$. This is a classic example of absorption.\nThus, the computed discriminant is:\n$$\\hat{D} = fl(b^2 - 4ac) = 1.0000000 \\times 10^{16}$$\nNext, we compute the square root of $\\hat{D}$.\n$$fl(\\sqrt{\\hat{D}}) = fl(\\sqrt{1.0000000 \\times 10^{16}}) = 1.0000000 \\times 10^8$$\nThis is an exact computation. Let us denote this result $\\hat{S} = 1.0000000 \\times 10^8$.\n\nNow, we compute the two roots as specified in part (a).\nThe larger root, $\\hat{x}_1$, is computed using the '$+$' sign in the numerator, as $-b = 10^8$ is positive.\n$$\\hat{x}_1 = fl\\left(\\frac{-b + \\hat{S}}{2a}\\right) = fl\\left(\\frac{10^8 + 10^8}{2}\\right) = fl\\left(\\frac{2 \\times 10^8}{2}\\right) = 1.0000000 \\times 10^8$$\nThis computation is numerically stable, as it involves the addition of two positive numbers of similar magnitude.\n\nThe smaller root, $\\hat{x}_2$, is computed using the '$-$' sign.\n$$\\hat{x}_2 = fl\\left(\\frac{-b - \\hat{S}}{2a}\\right) = fl\\left(\\frac{10^8 - 10^8}{2}\\right) = fl\\left(\\frac{0}{2}\\right) = 0$$\nThis computation suffers from catastrophic cancellation. The subtraction of two nearly identical numbers, $-b$ and $\\hat{S}$, results in a complete loss of significant digits. The computed root $\\hat{x}_2=0$ is grossly inaccurate. The true smaller root is approximately $10^{-8}$.\n\nFor part (b), we are instructed to use a numerically stabilized method. For a monic quadratic equation $x^2 + \\frac{b}{a}x + \\frac{c}{a} = 0$, Vieta's formulas state that the product of the roots is $x_1 x_2 = \\frac{c}{a}$.\nIn our case, $a=1$ and $c=1$, so $x_1 x_2 = 1$.\n\nThe stable procedure is to first compute the larger-magnitude root $\\hat{x}_1$, which we have already found to be accurate:\n$$\\hat{x}_1 = 1.0000000 \\times 10^8$$\nThen, we use the product relation to find the smaller root, $\\hat{x}'_2$:\n$$\\hat{x}'_2 = fl\\left(\\frac{c/a}{\\hat{x}_1}\\right)$$\nSubstituting the values:\n$$\\hat{x}'_2 = fl\\left(\\frac{1}{1.0000000 \\times 10^8}\\right) = fl(1.0000000 \\times 10^{-8})$$\nThe number $1.0000000 \\times 10^{-8}$ is exactly representable in the specified floating-point system. Therefore, the computation yields this value without any rounding error.\nThe stabilized smaller root is:\n$$\\hat{x}'_2 = 1.0000000 \\times 10^{-8}$$\nThis value is required as the final answer, expressed in scientific notation with eight significant figures.",
            "answer": "$$\\boxed{1.0000000 \\times 10^{-8}}$$"
        },
        {
            "introduction": "The issue of catastrophic cancellation is not just a theoretical curiosity; it appears frequently in scientific code when evaluating functions near specific points. This practice moves from manual calculation to a practical coding challenge, comparing the direct evaluation of the hyperbolic sine function, $\\sinh(x)$, with its Taylor series expansion for small values of $x$. You will empirically measure the error in both methods and confirm that for numerically sensitive regions, an alternative mathematical representation is often the key to accuracy .",
            "id": "2395275",
            "problem": "You are to study the numerical behavior of evaluating the hyperbolic sine function for small arguments under finite-precision floating-point arithmetic. Let $s(x)$ denote the mathematical function $\\sinh(x)$. Define two numerical approximations for $s(x)$: the direct exponential difference and a truncated Maclaurin polynomial. For each input $x$, compute the two approximations and quantify their accuracy relative to a reference value of $s(x)$, as specified below. All computations are to be carried out in double precision, and all outputs are dimensionless.\n\nDefinitions to be used:\n- Direct exponential difference: $s_{\\mathrm{dir}}(x) = \\dfrac{e^{x} - e^{-x}}{2}$.\n- Truncated Maclaurin polynomial of degree $7$: $s_{\\mathrm{ts}}(x) = x + \\dfrac{x^{3}}{3!} + \\dfrac{x^{5}}{5!} + \\dfrac{x^{7}}{7!}$.\n- Reference value: $s_{\\ast}(x)$ is the value of $\\sinh(x)$ computed in double precision using a standard, reliable numerical library for the hyperbolic sine function.\n\nError metrics to be reported for each $x$:\n- Relative error function $\\mathrm{relerr}(a,b)$ is defined by $\\mathrm{relerr}(a,b) = \\dfrac{|a-b|}{|b|}$ if $b \\neq 0$, and $\\mathrm{relerr}(a,b) = |a-b|$ if $b = 0$.\n- Compute $e_{\\mathrm{dir}}(x) = \\mathrm{relerr}\\!\\left(s_{\\mathrm{dir}}(x), s_{\\ast}(x)\\right)$ and $e_{\\mathrm{ts}}(x) = \\mathrm{relerr}\\!\\left(s_{\\mathrm{ts}}(x), s_{\\ast}(x)\\right)$.\n\nTest suite (inputs $x$):\nUse the ordered set $X = \\{\\,0,\\;10^{-16},\\;-10^{-16},\\;10^{-12},\\;10^{-8},\\;-10^{-8},\\;10^{-4},\\;10^{-1}\\,\\}$.\n\nRequired final output format:\n- Your program must produce a single line of output that is a comma-separated list enclosed in square brackets.\n- Each element of this list corresponds to one input $x \\in X$ (in the order given), and must itself be a two-element list $[\\,e_{\\mathrm{dir}}(x),\\,e_{\\mathrm{ts}}(x)\\,]$.\n- Each floating-point number must be printed in scientific notation with $12$ significant digits and no spaces anywhere in the line.\n- Therefore, for the $|X| = 8$ inputs, the output line must contain $8$ two-element lists enclosed in one outer pair of brackets, in the specified order.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of numerical analysis, specifically concerning floating-point arithmetic and function approximation. All definitions are mathematically precise, the objective is clear, and the problem is well-posed, admitting a unique and verifiable solution. We may therefore proceed with the analysis and solution.\n\nThe objective is to analyze the numerical accuracy of two different methods for computing the hyperbolic sine function, $\\sinh(x)$, for arguments $x$ near zero. The hyperbolic sine is mathematically defined as $s(x) = \\sinh(x) = \\dfrac{e^x - e^{-x}}{2}$. We are to compare the direct evaluation of this expression, denoted $s_{\\mathrm{dir}}(x)$, with a truncated Maclaurin series approximation, $s_{\\mathrm{ts}}(x)$, against a high-precision reference value, $s_{\\ast}(x)$, provided by a standard numerical library.\n\nThe first approximation, $s_{\\mathrm{dir}}(x) = \\dfrac{e^{x} - e^{-x}}{2}$, is a direct implementation of the mathematical definition. For values of $|x|$ that are not close to zero, this formula is numerically stable and accurate. However, as $x \\to 0$, we have $e^x \\to 1$ and $e^{-x} \\to 1$. In finite-precision arithmetic, the subtraction of two nearly equal numbers, $e^x$ and $e^{-x}$, leads to a phenomenon known as catastrophic cancellation. When two numbers are very close, their leading significant digits are identical. Subtracting them cancels these digits, leaving a result composed of the less-significant, and often noisy, trailing digits. This causes a drastic loss of relative precision. The relative error of the computed difference can be shown to be approximately proportional to $\\dfrac{\\epsilon_{mach}}{|x|}$, where $\\epsilon_{mach}$ is the machine epsilon of the floating-point system (for double precision, $\\epsilon_{mach} \\approx 2.22 \\times 10^{-16}$). As $|x|$ becomes smaller, this relative error grows, leading to completely inaccurate results for $|x|$ on the order of $\\sqrt{\\epsilon_{mach}}$ or smaller.\n\nThe second approximation is the truncated Maclaurin series of degree $7$, defined as $s_{\\mathrm{ts}}(x) = x + \\dfrac{x^{3}}{3!} + \\dfrac{x^{5}}{5!} + \\dfrac{x^{7}}{7!}$. This is derived from the full Maclaurin series for $\\sinh(x)$, which is $\\sum_{n=0}^{\\infty} \\dfrac{x^{2n+1}}{(2n+1)!}$. For small $|x|$, this series converges very rapidly. The primary source of error in this approximation is the truncation error, which arises from neglecting the terms of degree $9$ and higher. The leading neglected term is $\\dfrac{x^9}{9!}$. For small $|x|$, this term is exceedingly small. For instance, if $|x|=10^{-4}$, the leading error term is on the order of $(10^{-4})^9 / 9! \\approx 10^{-36} / 3.6 \\times 10^5 \\approx 2.7 \\times 10^{-42}$, which is far below machine precision. Furthermore, the computation of $s_{\\mathrm{ts}}(x)$ involves sums of terms that do not suffer from catastrophic cancellation (for $x0$, all terms are positive; for $x0$, all terms are negative). Therefore, for small $|x|$, we expect $s_{\\mathrm{ts}}(x)$ to be a highly accurate approximation, with its error dominated by the very small truncation error and minimal floating-point round-off errors.\n\nThe reference value, $s_{\\ast}(x)$, is computed using a robust library function. Such functions are designed to maintain high accuracy across the entire domain of a function. They typically employ hybrid algorithms, using a stable formulation like one based on the `expm1(x)` function (which accurately computes $e^x - 1$ for small $x$) in the region where catastrophic cancellation would otherwise occur, and switching to the direct exponential definition for larger $|x|$ where it is stable.\n\nThe analysis predicts a clear trade-off. For very small $|x|$ (e.g., $10^{-16}, 10^{-12}, 10^{-8}$), the direct method $s_{\\mathrm{dir}}(x)$ will exhibit large relative errors due to catastrophic cancellation, whereas the truncated series $s_{\\mathrm{ts}}(x)$ will be extremely accurate. As $|x|$ increases (e.g., to $10^{-4}$ and $10^{-1}$), the catastrophic cancellation in $s_{\\mathrm{dir}}(x)$ will lessen, improving its accuracy. Conversely, the truncation error of $s_{\\mathrm{ts}}(x)$, while still small, will grow as $|x|^9$. At some point, the error from the direct method will become smaller than the truncation error of the degree-7 polynomial. The provided test suite, $X = \\{\\,0,\\;10^{-16},\\;-10^{-16},\\;10^{-12},\\;10^{-8},\\;-10^{-8},\\;10^{-4},\\;10^{-1}\\,\\}$, is well-chosen to demonstrate this behavior. For $x=0$, both methods are exact, yielding $\\sinh(0)=0$ and thus zero error. Due to the odd symmetry of all functions involved ($\\sinh(-x)=-\\sinh(x)$), the error metrics $e_{\\mathrm{dir}}(x)$ and $e_{\\mathrm{ts}}(x)$ will be identical for $x$ and $-x$.\n\nThe computation proceeds by implementing the functions $s_{\\mathrm{dir}}(x)$ and $s_{\\mathrm{ts}}(x)$ in double-precision arithmetic. For each $x$ in the test suite, we compute these values, obtain the reference $s_{\\ast}(x)$ from a standard library, and then calculate the relative errors $e_{\\mathrm{dir}}(x) = \\mathrm{relerr}\\!\\left(s_{\\mathrm{dir}}(x), s_{\\ast}(x)\\right)$ and $e_{\\mathrm{ts}}(x) = \\mathrm{relerr}\\!\\left(s_{\\mathrm{ts}}(x), s_{\\ast}(x)\\right)$ using the provided error definition $\\mathrm{relerr}(a,b)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the numerical accuracy of two approximations\n    for the hyperbolic sine function for small arguments.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        0.0,\n        1e-16,\n        -1e-16,\n        1e-12,\n        1e-8,\n        -1e-8,\n        1e-4,\n        1e-1,\n    ]\n\n    results = []\n    \n    # Precompute factorials for the Taylor series.\n    # 3! = 6, 5! = 120, 7! = 5040\n    F3 = 6.0\n    F5 = 120.0\n    F7 = 5040.0\n\n    for x_val in test_cases:\n        x = np.double(x_val)\n\n        # 1. Reference value from a reliable library function\n        s_star = np.sinh(x)\n\n        # 2. Direct exponential difference approximation\n        s_dir = (np.exp(x) - np.exp(-x)) / 2.0\n\n        # 3. Truncated Maclaurin polynomial of degree 7\n        x_cubed = x * x * x\n        x_fifth = x_cubed * x * x\n        x_seventh = x_fifth * x * x\n        s_ts = x + x_cubed / F3 + x_fifth / F5 + x_seventh / F7\n\n        # 4. Compute relative errors\n        # The problem defines relerr(a,b) = |a-b| if b=0.\n        if s_star == 0.0:\n            e_dir = np.abs(s_dir - s_star)\n            e_ts = np.abs(s_ts - s_star)\n        else:\n            e_dir = np.abs(s_dir - s_star) / np.abs(s_star)\n            e_ts = np.abs(s_ts - s_star) / np.abs(s_star)\n\n        results.append((e_dir, e_ts))\n\n    # Format the output as specified: a list of [e_dir, e_ts] pairs,\n    # with each float in scientific notation with 12 significant digits and no spaces.\n    formatted_pairs = []\n    for e_dir, e_ts in results:\n        # Format specifier \".11e\" gives 1 digit before decimal point + 11 after.\n        s_dir_formatted = f\"{e_dir:.11e}\"\n        s_ts_formatted = f\"{e_ts:.11e}\"\n        formatted_pairs.append(f\"[{s_dir_formatted},{s_ts_formatted}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_pairs)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While reformulating an algorithm can often avoid numerical instability, a more general approach is to capture and account for the rounding error as it occurs. This advanced exercise introduces the powerful technique of compensated arithmetic through Dekker's `two-sum` algorithm, an error-free transformation for addition. By implementing this method and comparing its result to a naive subtraction and an exact ground truth, you will quantify the bits of precision recovered and gain insight into sophisticated methods for writing high-precision numerical code .",
            "id": "3131169",
            "problem": "You are given pairs of real numbers to be represented and computed in the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 format (double precision). The fundamental base for this task includes: the IEEE 754 rounding to nearest, ties to even; the definition of machine epsilon (the distance between $1$ and the next representable number in binary64); and the Sterbenz lemma stating that if two floating-point numbers $x$ and $y$ satisfy $0.5 \\leq \\left|x/y\\right| \\leq 2$, then the subtraction $x - y$ is performed exactly in binary floating-point arithmetic without rounding error. Your objective is to empirically assess the numerical error when computing $x - y$ for $x \\approx y$, both naively and with compensated arithmetic using Dekker’s error-free transform for addition (commonly called the “two-sum” algorithm) applied to $x + (-y)$. You must quantify how many bits of precision, measured in binary digits, are recovered by compensated arithmetic relative to the naive subtraction.\n\nTasks to be completed:\n- Represent each input pair $(x, y)$ as IEEE 754 binary64 values.\n- Compute the naive difference $d_{\\text{naive}} = \\mathrm{fl}(x - y)$ using binary64 arithmetic.\n- Apply Dekker’s error-free transform for addition to $a + b$ with $a = x$ and $b = -y$, producing a high part $h$ and a low part $\\ell$, and interpret $(h, \\ell)$ as a compensated representation of the exact sum $x + (-y)$.\n- Construct a mathematically exact ground truth difference $d_{\\text{true}}$ by converting $x$ and $y$ to their exact rational values implied by IEEE 754 (using the exact integer ratio of the binary64 representations), and then forming $d_{\\text{true}} = x - y$ in exact rational arithmetic.\n- Define the absolute naive error $E_{\\text{naive}} = \\left|d_{\\text{naive}} - d_{\\text{true}}\\right|$ and the compensated absolute error $E_{\\text{comp}} = \\left|(h + \\ell) - d_{\\text{true}}\\right|$, where the sum $h + \\ell$ must be evaluated in exact rational arithmetic by converting both $h$ and $\\ell$ to their exact rational values implied by IEEE 754.\n- Define the bits of precision recovered as\n$$\nB = \\begin{cases}\n0,  \\text{if } E_{\\text{naive}} = 0 \\text{ and } E_{\\text{comp}} = 0, \\\\\n\\log_2\\left(\\dfrac{E_{\\text{naive}}}{E_{\\text{comp}}}\\right),  \\text{if } E_{\\text{naive}}  0 \\text{ and } E_{\\text{comp}}  0, \\\\\n53,  \\text{if } E_{\\text{naive}}  0 \\text{ and } E_{\\text{comp}} = 0,\n\\end{cases}\n$$\nwhere $53$ is the number of binary digits in the significand of IEEE 754 binary64. Report $B$ for each test case as a floating-point number.\n\nUse the following test suite (all values are to be interpreted and computed in binary64):\n- Test case $1$: $x = 1.234567890123456$, $y = 1.234567890123455$ (close decimal magnitudes, typical cancellation scenario).\n- Test case $2$: $x = 1 \\times 10^{308}$, $y = \\operatorname{nextafter}(x, 0)$, where $\\operatorname{nextafter}(x, 0)$ denotes the nearest representable binary64 value to $x$ in the direction of $0$ (large magnitude with one unit in the last place separation).\n- Test case $3$: $x = 1 + 2^{-30}$, $y = 1 + 2^{-30} - 2^{-52}$ (cancellation near $1$ with spacing tied to machine epsilon).\n- Test case $4$: $x = 2^{-1022}$, $y = x - 2^{-1074}$ (difference in the subnormal regime; $2^{-1022}$ is the smallest positive normal number and $2^{-1074}$ is the smallest positive subnormal number in binary64).\n- Test case $5$: $x = 2^{53} + 1$ interpreted in binary64, $y = 2^{53}$ interpreted in binary64 (boundary of exact integer representability).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[result1,result2,result3,result4,result5]”), where each result is the value of $B$ for the corresponding test case as a Python floating-point number. No physical units or angle units are involved, and all quantities are pure real numbers.\n\nThe program must be self-contained, require no input, and strictly adhere to the specified execution environment. The goal is to test the principles of IEEE 754 rounding, exactness properties, machine epsilon, and the practical behavior of compensated arithmetic for near-cancellation in subtraction.",
            "solution": "The user wants to validate and solve a problem concerning floating-point arithmetic.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Computational System**: IEEE 754 binary64 format (double precision).\n-   **Rounding Rule**: Round to nearest, ties to even.\n-   **Core Concepts**: Machine epsilon, Sterbenz lemma ($0.5 \\leq |x/y| \\leq 2 \\implies \\mathrm{fl}(x - y) = x - y$), Dekker’s error-free transform (two-sum).\n-   **Objective**: Quantify the bits of precision recovered by compensated arithmetic over naive subtraction for pairs $(x, y)$ where $x \\approx y$.\n\n-   **Computational Tasks**:\n    1.  Represent inputs $(x, y)$ as binary64 values.\n    2.  Compute naive difference: $d_{\\text{naive}} = \\mathrm{fl}(x - y)$.\n    3.  Compute compensated sum: Apply two-sum algorithm to $x + (-y)$ to get a pair $(h, \\ell)$.\n    4.  Compute ground truth difference: Convert binary64 $x, y$ to exact rationals and compute $d_{\\text{true}} = x - y$.\n    5.  Compute errors:\n        -   Naive absolute error: $E_{\\text{naive}} = |d_{\\text{naive}} - d_{\\text{true}}|$, using exact rational arithmetic.\n        -   Compensated absolute error: $E_{\\text{comp}} = |(h + \\ell) - d_{\\text{true}}|$, where $h+\\ell$ is computed in exact rational arithmetic.\n    6.  Compute bits of precision recovered, $B$:\n        $$\n        B = \\begin{cases}\n        0,  \\text{if } E_{\\text{naive}} = 0 \\text{ and } E_{\\text{comp}} = 0, \\\\\n        \\log_2\\left(\\dfrac{E_{\\text{naive}}}{E_{\\text{comp}}}\\right),  \\text{if } E_{\\text{naive}}  0 \\text{ and } E_{\\text{comp}}  0, \\\\\n        53,  \\text{if } E_{\\text{naive}}  0 \\text{ and } E_{\\text{comp}} = 0,\n        \\end{cases}\n        $$\n        where $53$ is the significand precision of binary64.\n\n-   **Test Cases (to be interpreted in binary64)**:\n    1.  $x = 1.234567890123456$, $y = 1.234567890123455$.\n    2.  $x = 1 \\times 10^{308}$, $y = \\operatorname{nextafter}(x, 0)$.\n    3.  $x = 1 + 2^{-30}$, $y = 1 + 2^{-30} - 2^{-52}$.\n    4.  $x = 2^{-1022}$, $y = x - 2^{-1074}$.\n    5.  $x = 2^{53} + 1$, $y = 2^{53}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is based on established principles of numerical analysis and the IEEE 754 standard. Concepts like catastrophic cancellation, compensated arithmetic (Dekker's algorithm), machine epsilon, subnormal numbers, and rounding rules are central to computational science. The problem is scientifically sound.\n-   **Well-Posed**: The problem is clearly specified. The inputs, required computations, error metrics, and final output format are all defined precisely. The formula for $B$ covers the expected outcomes ($E_{\\text{naive}} \\ge 0, E_{\\text{comp}} \\ge 0$). The case where $E_{\\text{naive}}  0$ and the compensated transform yields an exact result ($E_{\\text{comp}} = 0$) is handled by assigning a result of $53$ bits, which corresponds to full precision recovery for the binary64 format. The problem is well-posed.\n-   **Objective**: The problem is stated in objective, technical language, free of bias or subjective claims.\n\nThe problem does not exhibit any of the invalidity flaws listed in the instructions. It is a valid, non-trivial problem in computational science.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed to provide a complete solution.\n\n### Solution\n\nThe solution requires implementing the specified calculations for each test case. This involves a combination of standard floating-point arithmetic and exact rational arithmetic. All mathematical entities are typeset in LaTeX as required.\n\n1.  **Framework Setup**: We use Python, where standard `float` objects correspond to IEEE 754 binary64. The `numpy` library provides the `nextafter` function. For exact rational arithmetic, Python's `fractions.Fraction` class is ideal, as `Fraction(some_float)` correctly converts a binary64 number to its exact rational representation, $M/2^k$ where $M, k \\in \\mathbb{Z}$.\n\n2.  **Dekker's `two-sum` Algorithm**: The problem specifies using Dekker's error-free transform for addition, commonly known as the `two-sum` algorithm. Given two floating-point numbers $a$ and $b$, this algorithm computes two numbers, a high part $h$ and a low part $\\ell$, such that $h = \\mathrm{fl}(a+b)$ and $h+\\ell = a+b$ exactly. The standard algorithm that does not require $|a| \\geq |b|$ is:\n    -   $h = a + b$\n    -   $a' = h - b$\n    -   $b' = h - a'$\n    -   $\\delta_a = a - a'$\n    -   $\\delta_b = b - b'$\n    -   $\\ell = \\delta_a + \\delta_b$\n    All operations are standard floating-point operations. In our problem, we apply this with $a=x$ and $b=-y$.\n\n3.  **Ground Truth and Error Calculation**: For each test case $(x, y)$:\n    -   The inputs are first realized as binary64 values.\n    -   The naive difference is computed as $d_{\\text{naive}} = \\mathrm{fl}(x - y)$.\n    -   The ground truth difference, $d_{\\text{true}}$, is computed by converting $x$ and $y$ to `Fraction` objects and then subtracting them: $d_{\\text{true}} = \\text{Fraction}(x) - \\text{Fraction}(y)$.\n    -   The naive error is $E_{\\text{naive}} = |\\text{Fraction}(d_{\\text{naive}}) - d_{\\text{true}}|$.\n    -   The `two-sum` algorithm is applied to $(x, -y)$, yielding $(h, \\ell)$. The compensated sum is calculated in exact rationals as $\\text{Fraction}(h) + \\text{Fraction}(\\ell)$.\n    -   The compensated error is $E_{\\text{comp}} = |(\\text{Fraction}(h) + \\text{Fraction}(\\ell)) - d_{\\text{true}}|$. By the nature of the error-free transform, this error, $E_{\\text{comp}}$, is expected to be exactly $0$.\n\n4.  **Analysis of Test Cases**:\n    -   **Case 1**: $x = 1.234567890123456, y = 1.234567890123455$. These decimal literals are converted to their nearest binary64 representations, which we call $x_f$ and $y_f$. The subtraction $x_f - y_f$ is a classic example of catastrophic cancellation. The exact difference of these binary64 numbers, $d_{\\text{true}}$, is non-zero. The naively computed floating-point result, $d_{\\text{naive}} = \\mathrm{fl}(x_f - y_f)$, incurs a rounding error, so $E_{\\text{naive}}  0$. The compensated sum is exact, so $E_{\\text{comp}} = 0$. Thus, we expect to recover full precision, $B=53$.\n    -   **Case 2**: $x = 1 \\times 10^{308}, y = \\operatorname{nextafter}(x, 0)$. Here, $x$ and $y$ are adjacent binary64 numbers. Their ratio $|x/y|$ is extremely close to $1$, satisfying the condition of the Sterbenz lemma ($0.5 \\le |x/y| \\le 2$). Therefore, the subtraction $d_{\\text{naive}} = \\mathrm{fl}(x-y)$ is guaranteed to be exact. This means $E_{\\text{naive}} = 0$. Consequently, $E_{\\text{comp}}$ will also be $0$, and $B=0$.\n    -   **Case 3**: $x = 1 + 2^{-30}, y = 1 + 2^{-30} - 2^{-52}$. Both $x$ and $y$ (as defined by the floating-point computation) are exactly representable in binary64. Their ratio is also close to $1$, so the Sterbenz lemma applies to their subtraction. $d_{\\text{naive}}$ will be exact, meaning $E_{\\text{naive}} = 0$, leading to $B=0$.\n    -   **Case 4**: $x = 2^{-1022}, y = x - 2^{-1074}$. Here, $x$ is the smallest positive normal number and $y$ is a subnormal number. Both are exactly representable. The difference $d_{\\text{true}} = 2^{-1074}$ is the smallest positive subnormal number, also exactly representable. The ratio $|x/y|$ is close to $1$, so Sterbenz lemma applies. The subtraction is exact, $E_{\\text{naive}} = 0$, and $B=0$.\n    -   **Case 5**: $x = 2^{53} + 1, y = 2^{53}$. The value $2^{53}+1$ is not representable in binary64. It is exactly halfway between two representable floats, $2^{53}$ and $2^{53}+2$. Following the \"round to nearest, ties to even\" rule, $2^{53}+1$ rounds to $2^{53}$, as the significand of $2^{53}$ is even. Thus, in binary64, the input values become $x = 2^{53}$ and $y = 2^{53}$. The subtraction $x-y$ is trivially $0$. Both $d_{\\text{naive}}$ and $d_{\\text{true}}$ are $0$, so $E_{\\text{naive}} = 0$ and $B=0$.\n\nThe implementation will execute these steps and report the computed value of $B$ for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\nfrom fractions import Fraction\n\ndef solve():\n    \"\"\"\n    Solves the floating-point arithmetic problem as described.\n    \"\"\"\n\n    def two_sum(a: float, b: float) - tuple[float, float]:\n        \"\"\"\n        Implements Dekker's error-free transform for addition (two-sum).\n        Given two floating-point numbers a and b, returns a pair (h, l)\n        such that h = fl(a + b) and h + l = a + b exactly.\n        All internal calculations are floating-point operations.\n        Reference: Ogita, T., Rump, S. M.,  Oishi, S. (2005).\n        \"Accurate sum and dot product.\"\n        \"\"\"\n        h = a + b\n        a_prime = h - b\n        b_prime = h - a_prime\n        delta_a = a - a_prime\n        delta_b = b - b_prime\n        l = delta_a + delta_b\n        return (h, l)\n\n    # Define the test cases from the problem statement.\n    # Case 1: Close decimal magnitudes, typical cancellation scenario.\n    x1 = 1.234567890123456\n    y1 = 1.234567890123455\n    \n    # Case 2: Large magnitude with one ULP separation.\n    x2 = 1.0e308\n    y2 = np.nextafter(x2, 0.0)\n    \n    # Case 3: Cancellation near 1 with spacing tied to machine epsilon.\n    x3 = 1.0 + 2.0**-30\n    y3 = (1.0 + 2.0**-30) - 2.0**-52\n    \n    # Case 4: Difference in the subnormal regime.\n    x4 = 2.0**-1022\n    y4 = x4 - 2.0**-1074\n\n    # Case 5: Boundary of exact integer representability.\n    # float(2**53 + 1) is rounded to 2**53 due to ties-to-even rule.\n    x5 = float(2**53 + 1)\n    y5 = float(2**53)\n\n    test_cases = [\n        (x1, y1),\n        (x2, y2),\n        (x3, y3),\n        (x4, y4),\n        (x5, y5),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x, y = case\n        \n        # Compute the naive difference using binary64 arithmetic.\n        d_naive = x - y\n        \n        # Apply Dekker’s error-free transform for addition to x + (-y).\n        h, l = two_sum(x, -y)\n        \n        # Construct the mathematically exact ground truth difference.\n        x_frac = Fraction(x)\n        y_frac = Fraction(y)\n        d_true = x_frac - y_frac\n        \n        # Define the absolute naive error.\n        d_naive_frac = Fraction(d_naive)\n        E_naive = abs(d_naive_frac - d_true)\n        \n        # Define the compensated absolute error.\n        h_frac = Fraction(h)\n        l_frac = Fraction(l)\n        comp_sum_frac = h_frac + l_frac\n        E_comp = abs(comp_sum_frac - d_true)\n\n        # Define the bits of precision recovered.\n        B = 0.0\n        if E_naive == Fraction(0) and E_comp == Fraction(0):\n            B = 0.0\n        elif E_naive  Fraction(0) and E_comp == Fraction(0):\n            B = 53.0\n        elif E_naive  Fraction(0) and E_comp  Fraction(0):\n            # This case implies the two-sum was not perfectly error-free\n            # in practice, or d_true itself had some representation issues.\n            # We proceed with the formula as specified.\n            B = math.log2(float(E_naive / E_comp))\n        \n        results.append(B)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}