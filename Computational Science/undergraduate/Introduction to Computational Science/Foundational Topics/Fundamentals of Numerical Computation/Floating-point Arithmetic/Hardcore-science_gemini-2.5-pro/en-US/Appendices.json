{
    "hands_on_practices": [
        {
            "introduction": "Even formulas as familiar as the quadratic equation can behave unexpectedly in the world of finite precision. This exercise reveals a classic pitfall known as catastrophic cancellation, where subtracting two nearly identical numbers can erase most of your significant digits. By computing the roots of a carefully chosen quadratic equation, you will witness this loss of precision firsthand and then use an alternative, stable formulation to recover the correct answer . This practice is fundamental to developing an intuition for numerical instability.",
            "id": "2393691",
            "problem": "Consider the quadratic polynomial $p(x) = x^2 - 10^8 x + 1$ with real coefficients. All arithmetic in this problem is to be carried out in a normalized base-$10$ floating-point system with precision $t=8$ significant digits, rounding to nearest with ties to even, and with a sufficiently large exponent range so that no overflow or underflow occurs for the quantities encountered. Each elementary operation (addition, subtraction, multiplication, division, and square root) is performed exactly and then rounded to the nearest representable floating-point number according to the stated rule before being used in any subsequent computation.\n\n(a) Using the standard quadratic formula applied in this floating-point system, compute the two floating-point numbers that would be returned as the roots of $p(x)=0$.\n\n(b) Then, using only identities that hold exactly for monic quadratics (in particular, the fact that the product of the two roots equals $c/a$), compute a numerically stabilized value for the smaller-magnitude root by first computing the larger-magnitude root with the standard quadratic formula in this floating-point system and then using the exact product relation to obtain the smaller-magnitude root in this floating-point system.\n\nReport only the stabilized smaller-magnitude root from part (b) as your final answer, rounded to eight significant figures. Express your final answer in scientific notation of the form $a \\times 10^b$ with $1 \\leq |a|  10$.",
            "solution": "The problem requires the computation of the roots of a quadratic polynomial within a specified floating-point arithmetic system. Before proceeding to a solution, we must validate the problem statement.\n\nThe givens are:\n1.  The quadratic polynomial is $p(x) = x^2 - 10^8 x + 1$. From this, we identify the coefficients as $a=1$, $b=-10^8$, and $c=1$.\n2.  The arithmetic framework is a normalized base-$10$ floating-point system.\n3.  The precision is $t=8$ significant digits.\n4.  The rounding rule is round to nearest, with ties to even.\n5.  There is no overflow or underflow.\n6.  Each elementary arithmetic operation ($+$, $-$, $*$, $/$, $\\sqrt{\\cdot}$) is individually rounded.\n\nThe problem is scientifically grounded, being a standard exercise in numerical analysis concerning loss of significance. It is well-posed, with all necessary parameters defined. It is objective and its structure is sound. The problem is therefore deemed valid. We proceed to the solution.\n\nThe roots of the quadratic equation $ax^2 + bx + c = 0$ are given by the standard quadratic formula:\n$$x_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\nAll computations are performed in the specified floating-point system, which we denote by $fl(\\cdot)$.\n\nFirst, we compute the discriminant, $D = b^2 - 4ac$.\nThe coefficients are $a = 1.0000000 \\times 10^0$, $b = -1.0000000 \\times 10^8$, and $c = 1.0000000 \\times 10^0$.\nThe term $b^2$ is $(-10^8)^2 = 10^{16}$. This is represented exactly as $1.0000000 \\times 10^{16}$.\nThe term $4ac$ is $4 \\times 1 \\times 1 = 4$. This is represented exactly as $4.0000000 \\times 10^0$.\n\nWe must now compute the subtraction $fl(b^2 - 4ac) = fl(10^{16} - 4)$. To perform this operation, the smaller number must be shifted to match the exponent of the larger number.\n$$10^{16} - 4 = (1.0000000 \\times 10^{16}) - (0.0000000000000004 \\times 10^{16})$$\nThe mantissa for $10^{16}$ is $1.0000000$. The system stores $t=8$ significant digits. The subtraction of $4$ affects digits far beyond the precision of the system. The subtraction of the mantissas is $1.0000000 - 0.0000000000000004$, which, when rounded back to $8$ significant digits, remains $1.0000000$. This is a classic example of absorption.\nThus, the computed discriminant is:\n$$\\hat{D} = fl(b^2 - 4ac) = 1.0000000 \\times 10^{16}$$\nNext, we compute the square root of $\\hat{D}$.\n$$fl(\\sqrt{\\hat{D}}) = fl(\\sqrt{1.0000000 \\times 10^{16}}) = 1.0000000 \\times 10^8$$\nThis is an exact computation. Let us denote this result $\\hat{S} = 1.0000000 \\times 10^8$.\n\nNow, we compute the two roots as specified in part (a).\nThe larger root, $\\hat{x}_1$, is computed using the '$+$' sign in the numerator, as $-b = 10^8$ is positive.\n$$\\hat{x}_1 = fl\\left(\\frac{-b + \\hat{S}}{2a}\\right) = fl\\left(\\frac{10^8 + 10^8}{2}\\right) = fl\\left(\\frac{2 \\times 10^8}{2}\\right) = 1.0000000 \\times 10^8$$\nThis computation is numerically stable, as it involves the addition of two positive numbers of similar magnitude.\n\nThe smaller root, $\\hat{x}_2$, is computed using the '$-$' sign.\n$$\\hat{x}_2 = fl\\left(\\frac{-b - \\hat{S}}{2a}\\right) = fl\\left(\\frac{10^8 - 10^8}{2}\\right) = fl\\left(\\frac{0}{2}\\right) = 0$$\nThis computation suffers from catastrophic cancellation. The subtraction of two nearly identical numbers, $-b$ and $\\hat{S}$, results in a complete loss of significant digits. The computed root $\\hat{x}_2=0$ is grossly inaccurate. The true smaller root is approximately $10^{-8}$.\n\nFor part (b), we are instructed to use a numerically stabilized method. For a monic quadratic equation $x^2 + \\frac{b}{a}x + \\frac{c}{a} = 0$, Vieta's formulas state that the product of the roots is $x_1 x_2 = \\frac{c}{a}$.\nIn our case, $a=1$ and $c=1$, so $x_1 x_2 = 1$.\n\nThe stable procedure is to first compute the larger-magnitude root $\\hat{x}_1$, which we have already found to be accurate:\n$$\\hat{x}_1 = 1.0000000 \\times 10^8$$\nThen, we use the product relation to find the smaller root, $\\hat{x}'_2$:\n$$\\hat{x}'_2 = fl\\left(\\frac{c/a}{\\hat{x}_1}\\right)$$\nSubstituting the values:\n$$\\hat{x}'_2 = fl\\left(\\frac{1}{1.0000000 \\times 10^8}\\right) = fl(1.0000000 \\times 10^{-8})$$\nThe number $1.0000000 \\times 10^{-8}$ is exactly representable in the specified floating-point system. Therefore, the computation yields this value without any rounding error.\nThe stabilized smaller root is:\n$$\\hat{x}'_2 = 1.0000000 \\times 10^{-8}$$\nThis value is required as the final answer, expressed in scientific notation with eight significant figures.",
            "answer": "$$\\boxed{1.0000000 \\times 10^{-8}}$$"
        },
        {
            "introduction": "Moving from a single formula to an iterative process, this practice explores how small rounding errors can accumulate over time in a common programming task: summation. You will compare the result of adding a number to itself $N$ times versus simply multiplying the number by $N$ . The exercise starkly illustrates why these two mathematically equivalent operations can produce different results in a computer, especially when dealing with numbers like $0.1$ that lack an exact finite binary representation. This hands-on coding task is crucial for understanding the reliability of loops in numerical computations.",
            "id": "2393733",
            "problem": "In finite-precision arithmetic as defined by the Institute of Electrical and Electronics Engineers (IEEE) 754 standard for single precision (commonly denoted as $\\texttt{float32}$), rounding occurs on every elementary arithmetic operation. Consider the two ways to compute a partial sum involving the same addend: repeated addition versus a single multiplication. Although these expressions are algebraically identical in real arithmetic, they need not agree in floating-point arithmetic because rounding is applied at different steps.\n\nStarting from the foundational floating-point rounding model, for any basic operation $\\circ \\in \\{+,-,\\times,\\div\\}$ carried out in single precision, the computed result can be modeled as\n$$\n\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta), \\quad |\\delta| \\le u,\n$$\nwhere $u$ is the unit roundoff. For IEEE 754 single precision with precision $p = 24$ (including the implicit leading bit) and base $2$, the unit roundoff is $u = 2^{-24}$. In this problem, all arithmetic must be performed in IEEE 754 single precision with round-to-nearest, ties-to-even.\n\nDefine, for a given real step $s$, the following two single-precision computed quantities for each integer $N \\ge 1$:\n- The accumulated sum\n$$\nS_N = \\operatorname{fl}\\left(\\sum_{i=1}^{N} s\\right),\n$$\nwhere the sum is carried out by $N$ successive single-precision additions of the single-precision value of $s$ to an accumulator initialized at $0$ in single precision.\n- The product-based value\n$$\nP_N = \\operatorname{fl}\\big(\\operatorname{fl}(N) \\times \\operatorname{fl}(s)\\big),\n$$\nthat is, the integer $N$ is first converted to single precision, multiplied in single precision by the single-precision value of $s$, and the result is rounded to single precision.\n\nTask:\n- For each test case, find the largest integer $N$ in the range $1 \\le N \\le N_{\\max}$ such that $S_N$ is exactly equal to $P_N$ as single-precision numbers. Exact equality here means the two $\\texttt{float32}$ values compare equal under the equality operator, which is equivalent to bitwise equality of their single-precision representations.\n\nImplementation requirements:\n- All intermediate quantities, including the accumulator, the addend, the converted integer, and the product, must be in IEEE 754 single precision. The single-precision rounding must be applied at each addition in the accumulation of $S_N$.\n- The program must treat the input step $s$ as a real decimal literal to be rounded to single precision before use, consistent with how decimal literals are converted to $\\texttt{float32}$.\n- No physical units are involved; all quantities are dimensionless.\n\nTest suite:\n- Case $1$: $s = 0.1$, $N_{\\max} = 1{,}000{,}000$.\n- Case $2$: $s = 0.125$, $N_{\\max} = 1{,}000{,}000$.\n- Case $3$: $s = 1\\times 10^{-8}$, $N_{\\max} = 2{,}000{,}000$.\n- Case $4$: $s = 9.5367431640625\\times 10^{-7}$ (this is $2^{-20}$ exactly), $N_{\\max} = 2{,}000{,}000$.\n\nOutput specification:\n- Your program should produce a single line of output containing the largest $N$ found for each of the four cases, in order, as a comma-separated list enclosed in square brackets, for example, $[N_1,N_2,N_3,N_4]$.",
            "solution": "The problem statement has been validated and is found to be scientifically sound, well-posed, and unambiguous. It presents a classic problem in computational science, specifically within the domain of numerical analysis, concerning the effects of finite-precision floating-point arithmetic as defined by the IEEE 754 standard. A rigorous solution can therefore be derived.\n\nThe core of the problem is to compare the results of two computationally distinct methods for evaluating the expression $N \\times s$ in single-precision arithmetic and to find the largest integer $N$ within a given range $[1, N_{\\max}]$ for which the two methods yield identical results.\n\nThe two methods are:\n1.  **Accumulated Sum ($S_N$)**: This is computed via iterative addition. Let $s_{fp} = \\operatorname{fl}(s)$ be the single-precision representation of the real number $s$. The sum is initialized as $S_0 = \\operatorname{fl}(0)$, and then computed iteratively: $S_k = \\operatorname{fl}(S_{k-1} + s_{fp})$ for $k = 1, \\dots, N$. Each addition is subject to rounding error, and these errors can accumulate over the $N$ steps.\n2.  **Product-based Value ($P_N$)**: This is computed via a single multiplication. The integer $N$ is first converted to its single-precision representation, $N_{fp} = \\operatorname{fl}(N)$, and then multiplied by $s_{fp}$: $P_N = \\operatorname{fl}(N_{fp} \\times s_{fp})$.\n\nThe problem requires a direct simulation of these two computational procedures. For each integer $N$ from $1$ to $N_{\\max}$, we must compute $S_N$ and $P_N$ using single-precision arithmetic and check for exact equality. The largest value of $N$ for which $S_N = P_N$ is the desired result for that test case. For all test cases, $N_{\\max}$ is less than $2^{24} = 16,777,216$. Since single-precision floating-point numbers have a $24$-bit significand (including the implicit bit), any integer $N$ with magnitude less than or equal to $2^{24}$ can be represented exactly. Therefore, for all relevant $N$ in this problem, the conversion $\\operatorname{fl}(N)$ is exact, meaning $N_{fp} = N$. Consequently, the product-based value simplifies to $P_N = \\operatorname{fl}(N \\times s_{fp})$.\n\nWe now analyze each test case based on the properties of the step value $s$.\n\n**Case 2 ($s = 0.125$) and Case 4 ($s = 9.5367431640625\\times 10^{-7} = 2^{-20}$)**\n\nIn these two cases, the step value $s$ is a real number that has an exact, finite representation in binary.\n- For Case 2, $s = 0.125 = 1/8 = 2^{-3}$.\n- For Case 4, $s = 2^{-20}$.\n\nBecause $s_{fp}$ is an exact representation of $s$, there is no initial rounding error in $s_{fp}$. Furthermore, for the range of $N$ considered ($N \\le N_{\\max}  2^{24}$), the mathematical product $N \\times s$ is also exactly representable in single precision. This is because $N \\times 2^{-k}$ simply corresponds to a bit shift of the integer representation of $N$, which does not increase the number of significant bits required. Therefore, the floating-point multiplication $\\operatorname{fl}(N \\times s_{fp})$ incurs no rounding error, and $P_N = N \\times s$.\n\nSimilarly, for the accumulated sum $S_N$, each intermediate sum $S_k = \\sum_{i=1}^k s$ is also exactly representable for $k \\le N_{\\max}$. This means that the addition $S_{k-1} + s_{fp}$ is exact at each step, i.e., $\\operatorname{fl}(S_{k-1} + s_{fp}) = S_{k-1} + s_{fp}$. By induction, $S_N = \\sum_{i=1}^N s = N \\times s$.\n\nSince both computations yield the true mathematical value $N \\times s$, it follows that $S_N = P_N$ for all $N$ in the range $1 \\le N \\le N_{\\max}$. Thus, the largest integer $N$ for which equality holds is simply $N_{\\max}$.\n- For Case 2, the result is $N_{\\max} = 1,000,000$.\n- For Case 4, the result is $N_{\\max} = 2,000,000$.\n\n**Case 1 ($s = 0.1$) and Case 3 ($s = 1 \\times 10^{-8}$)**\n\nIn these cases, the step value $s$ does not have an exact finite binary representation.\n- $s = 0.1$ in binary is the repeating fraction $0.0001100110011\\dots_2$.\n- $s = 10^{-8}$ also results in a non-terminating binary expansion.\n\nConsequently, the initial conversion to single precision introduces a representation error: $s_{fp} = s(1+\\epsilon)$ for some small $\\epsilon$.\n\nThe computation of $S_N$ involves $N-1$ additions, each potentially contributing a new rounding error. The error in $S_N$ is a result of both the initial error in $s_{fp}$ and the accumulated errors from the additions. The standard model for the error of a sum is complex, but it demonstrates that the error is path-dependent and grows with $N$.\n$$S_N = \\operatorname{fl}\\left(\\sum_{i=1}^{N} s_{fp}\\right) \\approx N s_{fp} + E_{sum}$$\nThe computation of $P_N$ involves only one multiplication and thus one rounding error after the initial representation of $s$.\n$$P_N = \\operatorname{fl}(N \\times s_{fp}) = (N \\times s_{fp})(1+\\delta_N)$$\nBecause the error propagation mechanisms are different, $S_N$ and $P_N$ are not expected to be equal in general. While they will be equal for small $N$ (e.g., $N=1$, and often $N=2$ since $\\operatorname{fl}(x+x) = \\operatorname{fl}(2x)$ is typically true), they will eventually diverge as the accumulated error in $S_N$ becomes significant.\n\nTo find the largest $N$ for which equality holds, a direct simulation is required. It is not safe to assume that once $S_N \\neq P_N$, they will never be equal again for a larger $N$, as errors could coincidentally cancel. Therefore, the simulation must iterate through the entire range from $N=1$ to $N_{\\max}$ and record the last value of $N$ for which the equality condition was met.\n\nThe implementation will proceed by looping through all specified values of $N$ for each test case. All arithmetic will be performed using the `numpy.float32` data type to strictly enforce IEEE 754 single-precision semantics.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the floating-point comparison problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (s_as_string, N_max).\n    # Using strings for 's' ensures correct parsing to float32 without\n    # intermediate double-precision representation.\n    test_cases = [\n        ('0.1', 1_000_000),\n        ('0.125', 1_000_000),\n        ('1e-8', 2_000_000),\n        ('9.5367431640625e-7', 2_000_000),  # This is 2**-20\n    ]\n\n    results = []\n    \n    for s_str, n_max in test_cases:\n        # Convert the step value `s` to its single-precision (float32) representation.\n        s_fp = np.float32(s_str)\n        \n        # Initialize the accumulator for S_N. It must be float32 to ensure\n        # single-precision accumulation.\n        accumulated_sum = np.float32(0.0)\n        \n        # This variable will store the largest N for which S_N == P_N.\n        largest_n_equal = 0\n        \n        # Loop from N = 1 to N_max to find the largest N where equality holds.\n        # We must check the entire range, as a divergence might be followed\n        # by a coincidental re-convergence.\n        for n in range(1, n_max + 1):\n            # Compute S_N: perform one more single-precision addition.\n            accumulated_sum += s_fp\n            \n            # Compute P_N:\n            # 1. Convert integer N to float32. For N = 2**24, this is an exact conversion.\n            #    All N_max values in the test suite are less than 2**24.\n            n_fp = np.float32(n)\n            \n            # 2. Perform the single-precision multiplication.\n            product_val = n_fp * s_fp\n            \n            # Compare the results. For numpy scalars, `==` performs an exact bitwise comparison.\n            if accumulated_sum == product_val:\n                largest_n_equal = n\n                \n        results.append(largest_n_equal)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Writing robust numerical code often involves choosing the right algorithm for the right situation. This exercise demonstrates this principle by examining the computation of the hyperbolic sine function, $\\sinh(x)$, for values of $x$ close to zero . You will see how the direct definition, $\\sinh(x)=(e^x - e^{-x})/2$, becomes numerically unstable, and you will compare its performance against a stable alternativeâ€”a truncated Taylor series. This practice highlights the powerful strategy of using different mathematical approximations to maintain accuracy across a function's entire domain.",
            "id": "2395275",
            "problem": "You are to study the numerical behavior of evaluating the hyperbolic sine function for small arguments under finite-precision floating-point arithmetic. Let $s(x)$ denote the mathematical function $\\sinh(x)$. Define two numerical approximations for $s(x)$: the direct exponential difference and a truncated Maclaurin polynomial. For each input $x$, compute the two approximations and quantify their accuracy relative to a reference value of $s(x)$, as specified below. All computations are to be carried out in double precision, and all outputs are dimensionless.\n\nDefinitions to be used:\n- Direct exponential difference: $s_{\\mathrm{dir}}(x) = \\dfrac{e^{x} - e^{-x}}{2}$.\n- Truncated Maclaurin polynomial of degree $7$: $s_{\\mathrm{ts}}(x) = x + \\dfrac{x^{3}}{3!} + \\dfrac{x^{5}}{5!} + \\dfrac{x^{7}}{7!}$.\n- Reference value: $s_{\\ast}(x)$ is the value of $\\sinh(x)$ computed in double precision using a standard, reliable numerical library for the hyperbolic sine function.\n\nError metrics to be reported for each $x$:\n- Relative error function $\\mathrm{relerr}(a,b)$ is defined by $\\mathrm{relerr}(a,b) = \\dfrac{|a-b|}{|b|}$ if $b \\neq 0$, and $\\mathrm{relerr}(a,b) = |a-b|$ if $b = 0$.\n- Compute $e_{\\mathrm{dir}}(x) = \\mathrm{relerr}\\!\\left(s_{\\mathrm{dir}}(x), s_{\\ast}(x)\\right)$ and $e_{\\mathrm{ts}}(x) = \\mathrm{relerr}\\!\\left(s_{\\mathrm{ts}}(x), s_{\\ast}(x)\\right)$.\n\nTest suite (inputs $x$):\nUse the ordered set $X = \\{\\,0,\\;10^{-16},\\;-10^{-16},\\;10^{-12},\\;10^{-8},\\;-10^{-8},\\;10^{-4},\\;10^{-1}\\,\\}$.\n\nRequired final output format:\n- Your program must produce a single line of output that is a comma-separated list enclosed in square brackets.\n- Each element of this list corresponds to one input $x \\in X$ (in the order given), and must itself be a two-element list $[\\,e_{\\mathrm{dir}}(x),\\,e_{\\mathrm{ts}}(x)\\,]$.\n- Each floating-point number must be printed in scientific notation with $12$ significant digits and no spaces anywhere in the line.\n- Therefore, for the $|X| = 8$ inputs, the output line must contain $8$ two-element lists enclosed in one outer pair of brackets, in the specified order.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of numerical analysis, specifically concerning floating-point arithmetic and function approximation. All definitions are mathematically precise, the objective is clear, and the problem is well-posed, admitting a unique and verifiable solution. We may therefore proceed with the analysis and solution.\n\nThe objective is to analyze the numerical accuracy of two different methods for computing the hyperbolic sine function, $\\sinh(x)$, for arguments $x$ near zero. The hyperbolic sine is mathematically defined as $s(x) = \\sinh(x) = \\dfrac{e^x - e^{-x}}{2}$. We are to compare the direct evaluation of this expression, denoted $s_{\\mathrm{dir}}(x)$, with a truncated Maclaurin series approximation, $s_{\\mathrm{ts}}(x)$, against a high-precision reference value, $s_{\\ast}(x)$, provided by a standard numerical library.\n\nThe first approximation, $s_{\\mathrm{dir}}(x) = \\dfrac{e^{x} - e^{-x}}{2}$, is a direct implementation of the mathematical definition. For values of $|x|$ that are not close to zero, this formula is numerically stable and accurate. However, as $x \\to 0$, we have $e^x \\to 1$ and $e^{-x} \\to 1$. In finite-precision arithmetic, the subtraction of two nearly equal numbers, $e^x$ and $e^{-x}$, leads to a phenomenon known as catastrophic cancellation. When two numbers are very close, their leading significant digits are identical. Subtracting them cancels these digits, leaving a result composed of the less-significant, and often noisy, trailing digits. This causes a drastic loss of relative precision. The relative error of the computed difference can be shown to be approximately proportional to $\\dfrac{\\epsilon_{\\text{mach}}}{|x|}$, where $\\epsilon_{\\text{mach}}$ is the machine epsilon of the floating-point system (for double precision, $\\epsilon_{\\text{mach}} \\approx 2.22 \\times 10^{-16}$). As $|x|$ becomes smaller, this relative error grows, leading to completely inaccurate results for $|x|$ on the order of $\\sqrt{\\epsilon_{\\text{mach}}}$ or smaller.\n\nThe second approximation is the truncated Maclaurin series of degree $7$, defined as $s_{\\mathrm{ts}}(x) = x + \\dfrac{x^{3}}{3!} + \\dfrac{x^{5}}{5!} + \\dfrac{x^{7}}{7!}$. This is derived from the full Maclaurin series for $\\sinh(x)$, which is $\\sum_{n=0}^{\\infty} \\dfrac{x^{2n+1}}{(2n+1)!}$. For small $|x|$, this series converges very rapidly. The primary source of error in this approximation is the truncation error, which arises from neglecting the terms of degree $9$ and higher. The leading neglected term is $\\dfrac{x^9}{9!}$. For small $|x|$, this term is exceedingly small. For instance, if $|x|=10^{-4}$, the leading error term is on the order of $(10^{-4})^9 / 9! \\approx 10^{-36} / 3.6 \\times 10^5 \\approx 2.7 \\times 10^{-42}$, which is far below machine precision. Furthermore, the computation of $s_{\\mathrm{ts}}(x)$ involves sums of terms that do not suffer from catastrophic cancellation (for $x>0$, all terms are positive; for $x0$, all terms are negative). Therefore, for small $|x|$, we expect $s_{\\mathrm{ts}}(x)$ to be a highly accurate approximation, with its error dominated by the very small truncation error and minimal floating-point round-off errors.\n\nThe reference value, $s_{\\ast}(x)$, is computed using a robust library function. Such functions are designed to maintain high accuracy across the entire domain of a function. They typically employ hybrid algorithms, using a stable formulation like one based on the `expm1(x)` function (which accurately computes $e^x - 1$ for small $x$) in the region where catastrophic cancellation would otherwise occur, and switching to the direct exponential definition for larger $|x|$ where it is stable.\n\nThe analysis predicts a clear trade-off. For very small $|x|$ (e.g., $10^{-16}, 10^{-12}, 10^{-8}$), the direct method $s_{\\mathrm{dir}}(x)$ will exhibit large relative errors due to catastrophic cancellation, whereas the truncated series $s_{\\mathrm{ts}}(x)$ will be extremely accurate. As $|x|$ increases (e.g., to $10^{-4}$ and $10^{-1}$), the catastrophic cancellation in $s_{\\mathrm{dir}}(x)$ will lessen, improving its accuracy. Conversely, the truncation error of $s_{\\mathrm{ts}}(x)$, while still small, will grow as $|x|^9$. At some point, the error from the direct method will become smaller than the truncation error of the degree-7 polynomial. The provided test suite, $X = \\{\\,0,\\;10^{-16},\\;-10^{-16},\\;10^{-12},\\;10^{-8},\\;-10^{-8},\\;10^{-4},\\;10^{-1}\\,\\}$, is well-chosen to demonstrate this behavior. For $x=0$, both methods are exact, yielding $\\sinh(0)=0$ and thus zero error. Due to the odd symmetry of all functions involved ($\\sinh(-x)=-\\sinh(x)$), the error metrics $e_{\\mathrm{dir}}(x)$ and $e_{\\mathrm{ts}}(x)$ will be identical for $x$ and $-x$.\n\nThe computation proceeds by implementing the functions $s_{\\mathrm{dir}}(x)$ and $s_{\\mathrm{ts}}(x)$ in double-precision arithmetic. For each $x$ in the test suite, we compute these values, obtain the reference $s_{\\ast}(x)$ from a standard library, and then calculate the relative errors $e_{\\mathrm{dir}}(x) = \\mathrm{relerr}\\!\\left(s_{\\mathrm{dir}}(x), s_{\\ast}(x)\\right)$ and $e_{\\mathrm{ts}}(x) = \\mathrm{relerr}\\!\\left(s_{\\mathrm{ts}}(x), s_{\\ast}(x)\\right)$ using the provided error definition $\\mathrm{relerr}(a,b)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the numerical accuracy of two approximations\n    for the hyperbolic sine function for small arguments.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        0.0,\n        1e-16,\n        -1e-16,\n        1e-12,\n        1e-8,\n        -1e-8,\n        1e-4,\n        1e-1,\n    ]\n\n    results = []\n    \n    # Precompute factorials for the Taylor series.\n    # 3! = 6, 5! = 120, 7! = 5040\n    F3 = 6.0\n    F5 = 120.0\n    F7 = 5040.0\n\n    for x_val in test_cases:\n        x = np.double(x_val)\n\n        # 1. Reference value from a reliable library function\n        s_star = np.sinh(x)\n\n        # 2. Direct exponential difference approximation\n        s_dir = (np.exp(x) - np.exp(-x)) / 2.0\n\n        # 3. Truncated Maclaurin polynomial of degree 7\n        x_cubed = x * x * x\n        x_fifth = x_cubed * x * x\n        x_seventh = x_fifth * x * x\n        s_ts = x + x_cubed / F3 + x_fifth / F5 + x_seventh / F7\n\n        # 4. Compute relative errors\n        # The problem defines relerr(a,b) = |a-b| if b=0.\n        if s_star == 0.0:\n            e_dir = np.abs(s_dir - s_star)\n            e_ts = np.abs(s_ts - s_star)\n        else:\n            e_dir = np.abs(s_dir - s_star) / np.abs(s_star)\n            e_ts = np.abs(s_ts - s_star) / np.abs(s_star)\n\n        results.append((e_dir, e_ts))\n\n    # Format the output as specified: a list of [e_dir, e_ts] pairs,\n    # with each float in scientific notation with 12 significant digits and no spaces.\n    formatted_pairs = []\n    for e_dir, e_ts in results:\n        # Format specifier \".11e\" gives 1 digit before decimal point + 11 after.\n        s_dir_formatted = f\"{e_dir:.11e}\"\n        s_ts_formatted = f\"{e_ts:.11e}\"\n        formatted_pairs.append(f\"[{s_dir_formatted},{s_ts_formatted}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_pairs)}]\")\n\nsolve()\n```"
        }
    ]
}