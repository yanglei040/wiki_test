{
    "hands_on_practices": [
        {
            "introduction": "While Big-O notation is essential for understanding how an algorithm scales, it often hides constant factors and hardware-dependent costs that dictate real-world performance. This exercise bridges the gap between theoretical complexity and practical speed by building a detailed performance model for convolution, a key operation in scientific computing. By incorporating both arithmetic floating-point operations (FLOPs) and memory access costs from cache misses, you will determine the precise problem size $N$ at which the asymptotically superior Fast Fourier Transform (FFT) method actually becomes faster than the simpler direct method .",
            "id": "3096816",
            "problem": "A one-dimensional Partial Differential Equation (PDE) solver on a uniform mesh of size $N$ computes a full linear convolution (equivalently, multiplication by a dense Toeplitz operator derived from a Green's function). Two algorithmic strategies are available: a direct real-space convolution and a Fast Fourier Transform (FFT)-based convolution. The FFT-based approach may be implemented either with a Real-to-Complex FFT (exploiting Hermitian symmetry) or by performing a Complex FFT on zero-padded real data.\n\nUse the following foundational base:\n- Definitions from algorithmic complexity: the direct real-space convolution performs a number of primitive arithmetic operations that scales as $O(N^2)$, while an FFT-based convolution performs a number of primitive arithmetic operations that scales as $O(N \\log N)$.\n- In a simplified Central Processing Unit (CPU) cache model, total wall-clock time is modeled as the sum of arithmetic time and data-movement penalty due to cache misses: $T = \\alpha \\times (\\text{number of floating-point operations}) + \\tau \\times (\\text{number of cache misses})$.\n- For this solver, the linear convolution via FFT uses zero-padding to length $M = 2N$. A Cooley–Tukey FFT performs $\\log_{2}(M)$ stages, each stage comprising one contiguous streaming pass across the $M$-length array. Each streaming pass across an array of length $X$ incurs approximately $X / w$ cache misses, where $w$ is the number of double-precision floating-point values per cache line.\n- Arithmetic costs and constants:\n  - Direct real-space convolution uses approximately $2N^2$ floating-point operations (one multiply and one add per pair).\n  - Real-to-Complex FFT of length $M$ costs approximately $k_{rc} \\, M \\log_{2}(M)$ floating-point operations per transform, and a Complex FFT costs approximately $k_{c} \\, M \\log_{2}(M)$ floating-point operations per transform. Assume $k_{rc} = 2.5$ and $k_{c} = 5.0$.\n  - The pointwise spectrum multiplication costs $b_{rc} \\, M$ floating-point operations when using Real-to-Complex FFTs and $b_{c} \\, M$ floating-point operations when using Complex FFTs; assume $b_{rc} = 3$ and $b_{c} = 6$.\n- Cache-miss penalties and hardware parameters:\n  - Per-floating-point operation time $\\alpha = 1.0 \\times 10^{-9}$ seconds.\n  - Per-cache-miss penalty $\\tau = 2.0 \\times 10^{-8}$ seconds.\n  - Cache line holds $w = 8$ double-precision values.\n\nModel the cache-miss counts as follows:\n- Direct real-space convolution (dense, naive two-loop implementation): two input arrays are streamed $N$ times each, incurring approximately $\\frac{2N^2}{w}$ misses, and the output array is written once incurring approximately $\\frac{N}{w}$ misses.\n- FFT-based convolution at length $M = 2N$:\n  - Three FFT transforms (two forwards and one inverse) incur approximately $\\frac{M}{w} \\log_{2}(M)$ misses per transform, for a total of $\\frac{3 M}{w} \\log_{2}(M)$.\n  - One pointwise spectrum multiplication pass incurs approximately $\\frac{M}{w}$ misses.\n\nTasks:\n1. Starting from the given definitions and facts, derive explicit time models $T_{\\text{direct}}(N)$, $T_{\\text{rc}}(N)$, and $T_{\\text{cpx}}(N)$ for the direct convolution, the Real-to-Complex FFT-based convolution, and the Complex FFT-based convolution, respectively, in terms of $N$, $\\alpha$, $\\tau$, $w$, $k_{rc}$, $k_{c}$, $b_{rc}$, and $b_{c}$.\n2. Compare $T_{\\text{rc}}(N)$ and $T_{\\text{cpx}}(N)$ to identify which FFT variant has the smaller constant factor for all relevant $N$ under the provided model.\n3. Using the faster FFT variant, solve for the smallest integer mesh size $N^{\\ast}$ such that the FFT-based convolution time is strictly less than the direct convolution time, i.e., $T_{\\text{FFT}}(N) < T_{\\text{direct}}(N)$ for all $N \\ge N^{\\ast}$. Provide $N^{\\ast}$ as a single integer. No units are required for $N^{\\ast}$.\n\nThe final answer must be a single number.",
            "solution": "The user wants me to solve a problem that involves modeling the performance of two different algorithms for convolution and finding the crossover point in their execution time.\n\n### Step 1: Extract Givens\n\n- **Problem Type**: One-dimensional PDE solver on a uniform mesh of size $N$.\n- **Core Operation**: Full linear convolution.\n- **Algorithms**:\n  1. Direct real-space convolution ($T_{\\text{direct}}$).\n  2. FFT-based convolution with two variants:\n     a. Real-to-Complex FFT ($T_{\\text{rc}}$).\n     b. Complex FFT on zero-padded real data ($T_{\\text{cpx}}$).\n- **Asymptotic Complexity**:\n  - Direct: $O(N^2)$ operations.\n  - FFT-based: $O(N \\log N)$ operations.\n- **Time Model**: $T = \\alpha \\times (\\text{number of floating-point operations}) + \\tau \\times (\\text{number of cache misses})$.\n- **FFT Parameters**: Zero-padding from length $N$ to $M = 2N$. FFT has $\\log_{2}(M)$ stages, each being a streaming pass.\n- **Cache Miss Model**:\n  - Streaming pass over length $X$: $X/w$ misses.\n  - Direct convolution misses: $\\frac{2N^2}{w} + \\frac{N}{w}$.\n  - FFT-based convolution misses (for 3 transforms and 1 pointwise op): $3 \\frac{M}{w} \\log_{2}(M) + \\frac{M}{w}$.\n- **Arithmetic Cost (FLOPs)**:\n  - Direct convolution: $2N^2$.\n  - Real-to-Complex FFT of length $M$: $k_{rc} M \\log_{2}(M)$ per transform, with $k_{rc} = 2.5$.\n  - Complex FFT of length $M$: $k_c M \\log_{2}(M)$ per transform, with $k_c = 5.0$.\n  - Pointwise spectrum multiplication (Real-to-Complex): $b_{rc} M$, with $b_{rc} = 3$.\n  - Pointwise spectrum multiplication (Complex): $b_c M$, with $b_c = 6$.\n- **Hardware Parameters**:\n  - $\\alpha = 1.0 \\times 10^{-9}$ seconds/FLOP.\n  - $\\tau = 2.0 \\times 10^{-8}$ seconds/miss.\n  - $w = 8$ doubles/cache line.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded. It presents a simplified but standard performance model used in computational science to compare algorithms. The model components—arithmetic cost (FLOPs) and memory access cost (cache misses)—are fundamental to performance engineering. The asymptotic complexities and specific operation counts for direct and FFT-based convolutions are standard estimates. The cache miss model is based on a plausible streaming access pattern. All parameters are explicitly defined and dimensionally consistent. The problem is well-posed, with a clear objective and sufficient information to derive a unique solution for the specified tasks. The language is objective and precise. Therefore, the problem is deemed valid.\n\n### Step 3: Action\n\nThe problem is valid. I will now proceed with the solution.\n\nThe solution is divided into three parts as requested by the tasks.\n\n**Task 1: Derive explicit time models.**\n\nFirst, we derive the expression for the total time for the direct real-space convolution, $T_{\\text{direct}}(N)$.\nThe number of floating-point operations (FLOPs) is given as $2N^2$.\nThe number of cache misses is given as $\\frac{2N^2}{w} + \\frac{N}{w}$.\nThe total time $T_{\\text{direct}}(N)$ is the sum of the arithmetic time and the cache-miss penalty:\n$$T_{\\text{direct}}(N) = \\alpha \\cdot (2N^2) + \\tau \\cdot \\left(\\frac{2N^2}{w} + \\frac{N}{w}\\right)$$\nGrouping terms by powers of $N$:\n$$T_{\\text{direct}}(N) = \\left(2\\alpha + \\frac{2\\tau}{w}\\right)N^2 + \\left(\\frac{\\tau}{w}\\right)N$$\n\nNext, we derive the time for the FFT-based convolutions. The overall procedure for convolution of two real signals of length $N$ involves:\n1. Zero-padding two input signals to length $M=2N$.\n2. Performing two forward FFTs.\n3. Performing a pointwise multiplication on the spectra.\n4. Performing one inverse FFT.\nThis amounts to a total of $3$ transforms.\n\nFor the Real-to-Complex FFT-based approach, $T_{\\text{rc}}(N)$:\nThe number of FLOPs for $3$ transforms of length $M=2N$ is $3 \\cdot (k_{rc} M \\log_2(M)) = 3k_{rc}(2N)\\log_2(2N) = 6k_{rc}N\\log_2(2N)$.\nThe number of FLOPs for the pointwise spectrum multiplication is $b_{rc}M = 2b_{rc}N$.\nTotal FLOPs = $6k_{rc}N\\log_2(2N) + 2b_{rc}N$.\nThe number of cache misses for $3$ transforms is $3 \\cdot (\\frac{M}{w} \\log_2(M)) = 3\\frac{2N}{w}\\log_2(2N) = \\frac{6N}{w}\\log_2(2N)$.\nThe number of cache misses for the pointwise pass is $\\frac{M}{w} = \\frac{2N}{w}$.\nTotal misses = $\\frac{6N}{w}\\log_2(2N) + \\frac{2N}{w}$.\nThe total time $T_{\\text{rc}}(N)$ is:\n$$T_{\\text{rc}}(N) = \\alpha \\left(6k_{rc}N\\log_2(2N) + 2b_{rc}N\\right) + \\tau \\left(\\frac{6N}{w}\\log_2(2N) + \\frac{2N}{w}\\right)$$\nGrouping by terms:\n$$T_{\\text{rc}}(N) = \\left(6\\alpha k_{rc} + \\frac{6\\tau}{w}\\right)N\\log_2(2N) + \\left(2\\alpha b_{rc} + \\frac{2\\tau}{w}\\right)N$$\n\nFor the Complex FFT-based approach, $T_{\\text{cpx}}(N)$, the structure is identical, but with constants $k_c$ and $b_c$:\nThe number of FLOPs is $6k_{c}N\\log_2(2N) + 2b_{c}N$.\nThe cache miss model is identical to the real-to-complex case as it depends on data movement, not the arithmetic performed. Total misses = $\\frac{6N}{w}\\log_2(2N) + \\frac{2N}{w}$.\nThe total time $T_{\\text{cpx}}(N)$ is:\n$$T_{\\text{cpx}}(N) = \\alpha \\left(6k_{c}N\\log_2(2N) + 2b_{c}N\\right) + \\tau \\left(\\frac{6N}{w}\\log_2(2N) + \\frac{2N}{w}\\right)$$\nGrouping by terms:\n$$T_{\\text{cpx}}(N) = \\left(6\\alpha k_{c} + \\frac{6\\tau}{w}\\right)N\\log_2(2N) + \\left(2\\alpha b_{c} + \\frac{2\\tau}{w}\\right)N$$\n\n**Task 2: Compare FFT variants.**\n\nWe compare $T_{\\text{rc}}(N)$ and $T_{\\text{cpx}}(N)$.\n$T_{\\text{rc}}(N) = C_{1,rc} N\\log_2(2N) + C_{2,rc} N$\n$T_{\\text{cpx}}(N) = C_{1,cpx} N\\log_2(2N) + C_{2,cpx} N$\nThe coefficients are:\n$C_{1,rc} = 6\\alpha k_{rc} + \\frac{6\\tau}{w}$\n$C_{1,cpx} = 6\\alpha k_{c} + \\frac{6\\tau}{w}$\n$C_{2,rc} = 2\\alpha b_{rc} + \\frac{2\\tau}{w}$\n$C_{2,cpx} = 2\\alpha b_{c} + \\frac{2\\tau}{w}$\n\nWe are given $k_{rc} = 2.5$ and $k_c = 5.0$, so $k_{rc} < k_c$. Since $\\alpha > 0$, it follows that $C_{1,rc} < C_{1,cpx}$.\nWe are given $b_{rc} = 3$ and $b_c = 6$, so $b_{rc} < b_c$. Since $\\alpha > 0$, it follows that $C_{2,rc} < C_{2,cpx}$.\nSince both coefficients are smaller for the Real-to-Complex FFT variant, and $N > 0$, $T_{\\text{rc}}(N) < T_{\\text{cpx}}(N)$ for all $N \\ge 1$.\nThe faster FFT variant is the Real-to-Complex FFT approach. We will denote its time as $T_{\\text{FFT}}(N) = T_{\\text{rc}}(N)$.\n\n**Task 3: Find the crossover mesh size $N^{\\ast}$.**\n\nWe need to find the smallest integer $N^{\\ast}$ such that $T_{\\text{FFT}}(N) < T_{\\text{direct}}(N)$ for all $N \\ge N^{\\ast}$. This is the inequality:\n$$T_{\\text{rc}}(N) < T_{\\text{direct}}(N)$$\n$$\\left(6\\alpha k_{rc} + \\frac{6\\tau}{w}\\right)N\\log_2(2N) + \\left(2\\alpha b_{rc} + \\frac{2\\tau}{w}\\right)N < \\left(2\\alpha + \\frac{2\\tau}{w}\\right)N^2 + \\left(\\frac{\\tau}{w}\\right)N$$\nFor $N > 0$, we can divide the inequality by $N$:\n$$\\left(6\\alpha k_{rc} + \\frac{6\\tau}{w}\\right)\\log_2(2N) + \\left(2\\alpha b_{rc} + \\frac{2\\tau}{w}\\right) < \\left(2\\alpha + \\frac{2\\tau}{w}\\right)N + \\frac{\\tau}{w}$$\nRearranging terms to isolate the $N$ term:\n$$\\left(6\\alpha k_{rc} + \\frac{6\\tau}{w}\\right)\\log_2(2N) + \\left(2\\alpha b_{rc} + \\frac{\\tau}{w}\\right) < \\left(2\\alpha + \\frac{2\\tau}{w}\\right)N$$\nNow, substitute the given numerical values: $\\alpha = 1.0 \\times 10^{-9}$, $\\tau = 2.0 \\times 10^{-8}$, $w=8$, $k_{rc}=2.5$, $b_{rc}=3$.\n\nCalculate the coefficients for the inequality:\n- Coefficient of $\\log_2(2N)$:\n$6\\alpha k_{rc} + \\frac{6\\tau}{w} = 6(1.0 \\times 10^{-9})(2.5) + \\frac{6(2.0 \\times 10^{-8})}{8} = 15 \\times 10^{-9} + 1.5 \\times 10^{-8} = 15 \\times 10^{-9} + 15 \\times 10^{-9} = 30 \\times 10^{-9}$.\n- Constant term on the left side:\n$2\\alpha b_{rc} + \\frac{\\tau}{w} = 2(1.0 \\times 10^{-9})(3) + \\frac{2.0 \\times 10^{-8}}{8} = 6 \\times 10^{-9} + 0.25 \\times 10^{-8} = 6 \\times 10^{-9} + 2.5 \\times 10^{-9} = 8.5 \\times 10^{-9}$.\n- Coefficient of $N$ on the right side:\n$2\\alpha + \\frac{2\\tau}{w} = 2(1.0 \\times 10^{-9}) + \\frac{2(2.0 \\times 10^{-8})}{8} = 2 \\times 10^{-9} + 0.5 \\times 10^{-8} = 2 \\times 10^{-9} + 5 \\times 10^{-9} = 7 \\times 10^{-9}$.\n\nThe inequality becomes:\n$$(30 \\times 10^{-9})\\log_2(2N) + (8.5 \\times 10^{-9}) < (7 \\times 10^{-9})N$$\nWe can divide the entire inequality by $10^{-9}$:\n$$30\\log_2(2N) + 8.5 < 7N$$\nThis is a transcendental inequality which we solve by testing integer values for $N$. Let $f(N) = 7N - 30\\log_2(2N) - 8.5$. We seek the smallest integer $N$ for which $f(N)>0$. The function $f(N)$ is monotonically increasing for $N > \\frac{30}{7\\ln(2)} \\approx 6.2$, so once we find the crossover point, the inequality will hold for all larger $N$.\nLet's test integer values of $N$:\n- For $N=25$:\n$f(25) = 7(25) - 30\\log_2(50) - 8.5 \\approx 175 - 30(5.6439) - 8.5 = 175 - 169.317 - 8.5 = -2.817 < 0$.\nSo for $N=25$, $T_{\\text{direct}}(25) < T_{\\text{FFT}}(25)$.\n- For $N=26$:\n$f(26) = 7(26) - 30\\log_2(52) - 8.5 \\approx 182 - 30(5.7004) - 8.5 = 182 - 171.012 - 8.5 = 2.488 > 0$.\nSo for $N=26$, $T_{\\text{FFT}}(26) < T_{\\text{direct}}(26)$.\n\nThe smallest integer mesh size for which the FFT-based method is strictly faster is $N=26$. As the $O(N^2)$ direct method cost grows faster than the $O(N\\log N)$ FFT method cost, the FFT method will remain faster for all $N \\ge 26$. Thus, $N^\\ast = 26$.",
            "answer": "$$\\boxed{26}$$"
        },
        {
            "introduction": "The optimal choice of algorithm often depends not just on the size of the input, but on its underlying structure and distribution. This practice explores this principle in the context of spatial neighbor searches, a fundamental task in simulations from astrophysics to molecular dynamics. You will model the expected performance of two classic data structures—uniform grids and k-d trees—and derive the critical data density $\\rho^{*}$ that marks the boundary where one becomes more efficient than the other, providing a clear example of how to tailor algorithmic choices to the physical characteristics of a problem .",
            "id": "3096897",
            "problem": "A simulation places $n$ particles independently and uniformly at random in a two-dimensional ($2$-D) square domain of area $A$. Two particles interact if their Euclidean separation is less than a fixed interaction radius $r$, where $r \\ll \\sqrt{A}$. You must perform neighbor queries for all particles to identify all interacting pairs.\n\nTwo algorithmic strategies are considered:\n\n- A uniform grid with hashing: the domain is partitioned into square cells of side length $\\ell = r$. Each particle is inserted into a hash table keyed by its cell index. The neighbor query for a particle scans all particles in the $3 \\times 3$ block (i.e., $9$ cells) comprising the particle’s own cell and its adjacent cells.\n\n- A $k$-dimensional tree (k-d tree): a binary space-partitioning tree is built for the point set, and each particle performs a fixed-radius range query of radius $r$.\n\nAssume the following cost model, measured in primitive operations:\n\n- Uniform grid:\n  - Build cost per particle: $c_{b,g}$.\n  - Per-query constant overhead (excluding distance checks): $c_{o,g}$.\n  - Per candidate distance check cost: $c_{d,g}$.\n\n- k-d tree:\n  - Build cost per particle proportional to $\\log_{2}(n)$: $c_{b,k} \\log_{2}(n)$.\n  - Per-query overhead proportional to $\\log_{2}(n)$: $c_{o,k} \\log_{2}(n)$.\n  - Per neighbor distance check cost: $c_{d,k}$.\n\nUse the foundational facts that Big-O notation $O(\\cdot)$ classifies growth, the expected number of points in any region under a uniform distribution equals the density times the region’s area, where the density is $\\rho = n/A$, and the area of a circle of radius $r$ is $\\pi r^{2}$.\n\nModel the expected total operation count for each method across all $n$ queries, using the above constants, under the assumption that:\n\n- The uniform grid’s $3 \\times 3$ cell neighborhood has area $9\\ell^2 = 9r^2$, so the expected number of candidate points per query is $9\\rho r^2$.\n- The k-d tree returns exactly the points within the radius $r$, with expected count $\\rho \\pi r^2$ per query.\n\nDerive the break-even density threshold $\\rho^{*}$ (as a closed-form analytic expression) at which the expected total operation counts of the uniform grid and the k-d tree are equal. Express your final answer in terms of $n$, $r$, and the constants $c_{b,g}$, $c_{o,g}$, $c_{d,g}$, $c_{b,k}$, $c_{o,k}$, and $c_{d,k}$. No numerical approximation is required, and no units should be included in the final expression.",
            "solution": "The goal is to derive a threshold density $\\rho^{*}$ at which the expected total operation counts of the uniform grid and the $k$-dimensional tree (k-d tree) are equal. We begin from fundamental definitions:\n\n- Big-O notation $O(\\cdot)$ describes asymptotic growth rates, but here we retain explicit constant factors to obtain a concrete break-even expression.\n- For points that are independently and uniformly distributed with density $\\rho = n/A$, the expected number of points in a measurable region equals $\\rho$ times the region’s area.\n- The area of a circle of radius $r$ is $\\pi r^2$.\n\nWe now translate algorithmic behavior into expected-operation models:\n\nUniform grid with hashing:\n- The build phase inserts each particle into a hash table keyed by its cell index, costing $c_{b,g}$ per particle. Over $n$ particles, the expected build cost is $n c_{b,g}$.\n- For each particle’s query, a fixed constant overhead $c_{o,g}$ is incurred to identify and traverse the constant-size neighborhood of $9$ cells. The expected number of candidate points in the union of these cells is the density times the area of the neighborhood. Since the cell side length is $\\ell = r$, the neighborhood area is $9\\ell^2 = 9r^2$, so the expected candidates per query is $9\\rho r^2$. Each candidate incurs a distance check costing $c_{d,g}$. Therefore, the expected per-query cost is\n$$\nc_{o,g} + c_{d,g} \\cdot 9 \\rho r^2.\n$$\nSummed over all $n$ queries, the expected query cost is\n$$\nn c_{o,g} + n c_{d,g} \\cdot 9\\rho r^2.\n$$\nThe expected total operation count for the grid method is thus\n$$\nT_{\\text{grid}} = n c_{b,g} + n c_{o,g} + 9 n c_{d,g} \\rho r^2.\n$$\n\nk-d tree:\n- The build phase incurs $c_{b,k} \\log_{2}(n)$ per particle, giving an expected build cost of $n c_{b,k} \\log_{2}(n)$.\n- Each query incurs an overhead $c_{o,k} \\log_{2}(n)$ to traverse the tree, plus a cost per returned neighbor. Under uniformity, the expected number of true neighbors within radius $r$ is the density times the area of the disk of radius $r$, namely $\\rho \\pi r^2$. Each neighbor checked costs $c_{d,k}$, so the expected per-query cost is\n$$\nc_{o,k} \\log_{2}(n) + c_{d,k} \\cdot \\rho \\pi r^2.\n$$\nSummed over all $n$ queries, the expected query cost is\n$$\nn c_{o,k} \\log_{2}(n) + n c_{d,k} \\rho \\pi r^2.\n$$\nThe expected total operation count for the k-d tree method is thus\n$$\nT_{\\text{kdtree}} = n c_{b,k} \\log_{2}(n) + n c_{o,k} \\log_{2}(n) + n c_{d,k} \\rho \\pi r^2.\n$$\nWe can group the $\\log_{2}(n)$ terms:\n$$\nT_{\\text{kdtree}} = n \\left( c_{b,k} + c_{o,k} \\right) \\log_{2}(n) + n c_{d,k} \\rho \\pi r^2.\n$$\n\nBreak-even condition:\nSet $T_{\\text{grid}} = T_{\\text{kdtree}}$ and solve for $\\rho$:\n$$\nn c_{b,g} + n c_{o,g} + 9 n c_{d,g} \\rho r^2 = n \\left( c_{b,k} + c_{o,k} \\right) \\log_{2}(n) + n c_{d,k} \\rho \\pi r^2.\n$$\nDivide both sides by $n$:\n$$\nc_{b,g} + c_{o,g} + 9 c_{d,g} \\rho r^2 = \\left( c_{b,k} + c_{o,k} \\right) \\log_{2}(n) + c_{d,k} \\rho \\pi r^2.\n$$\nCollect terms in $\\rho$ on the left-hand side and constants on the right-hand side:\n$$\n\\left( 9 c_{d,g} - \\pi c_{d,k} \\right) \\rho r^2 = \\left( c_{b,k} + c_{o,k} \\right) \\log_{2}(n) - \\left( c_{b,g} + c_{o,g} \\right).\n$$\nAssuming $9 c_{d,g} \\neq \\pi c_{d,k}$ so that the coefficient of $\\rho$ is nonzero, solve for the break-even density $\\rho^{*}$:\n$$\n\\rho^{*} = \\frac{ \\left( c_{b,k} + c_{o,k} \\right) \\log_{2}(n) - \\left( c_{b,g} + c_{o,g} \\right) }{ \\left( 9 c_{d,g} - \\pi c_{d,k} \\right) r^2 }.\n$$\n\nInterpretation relative to $O(n)$ versus $O(n \\log n)$:\n- For the grid, the dominant terms are linear in $n$, and the neighbor-dependent term scales as $n \\rho r^2$. When $\\rho$ is below $\\rho^{*}$, the grid’s total cost is lower and behaves as $O(n)$ for fixed $r$ and bounded $\\rho r^2$.\n- For the k-d tree, there is an inherent $n \\log_{2}(n)$ overhead from both building and per-query traversal. When $\\rho$ exceeds $\\rho^{*}$, the neighbor term can dominate, but below $\\rho^{*}$, the $\\log_{2}(n)$ term is decisive and the total cost is $O(n \\log n)$.\n\nThus, the closed-form break-even density threshold is the expression derived above.",
            "answer": "$$\\boxed{\\frac{\\left(c_{b,k}+c_{o,k}\\right)\\log_{2}(n)-\\left(c_{b,g}+c_{o,g}\\right)}{\\left(9c_{d,g}-\\pi c_{d,k}\\right) r^{2}}}$$"
        },
        {
            "introduction": "A robust algorithm should perform well not just on average, but also when faced with challenging or \"adversarial\" inputs. This exercise delves into the critical concept of worst-case analysis by examining how certain data geometries can degrade the performance of common spatial search structures like k-d trees. By analyzing a specific dataset of points on a circle, you will prove from first principles how and why pruning can fail, forcing the search to become linear in time, and investigate whether an alternative, metric-based vantage-point tree offers a better guarantee .",
            "id": "3096826",
            "problem": "Consider the Euclidean metric space $\\mathbb{R}^2$ with distance $d(\\mathbf{x},\\mathbf{y}) = \\|\\mathbf{x}-\\mathbf{y}\\|_{2}$. You will analyze nearest-neighbor queries in a $k$-d tree and in a balanced vantage-point tree (VP-tree), using only the following fundamental base: (i) the definition of a $k$-d tree as an axis-aligned binary space-partitioning tree that splits a point set by alternating coordinate axes at a threshold, (ii) the definition of a VP-tree as a metric tree that partitions a point set by distances to a chosen vantage point at a radius, and (iii) the triangle inequality $d(\\mathbf{x},\\mathbf{z}) \\le d(\\mathbf{x},\\mathbf{y}) + d(\\mathbf{y},\\mathbf{z})$ along with the reverse triangle inequality $|d(\\mathbf{x},\\mathbf{z}) - d(\\mathbf{x},\\mathbf{y})| \\le d(\\mathbf{y},\\mathbf{z})$ for any metric. Assume each leaf of either tree stores exactly one point, and the nearest-neighbor search computes $d(\\mathbf{q},\\mathbf{p})$ once per visited leaf point $\\mathbf{p}$, maintaining the current best distance $\\tau$ to decide pruning based on geometry alone.\n\nConstruct an adversarial dataset that forces a linear-time query in the $k$-d tree while motivating balanced VP-trees as a mitigation, and then analyze the worst-case bounds. Specifically:\n\nGiven an even integer $n \\ge 4$ and a radius $R > 0$, let $P = \\{\\mathbf{p}_{k}\\}_{k=1}^{n}$ be the set of $n$ points on the circle of radius $R$ centered at the origin, with $\\mathbf{p}_{k} = \\big(R\\cos(2\\pi k/n),\\, R\\sin(2\\pi k/n)\\big)$. Let the query be $\\mathbf{q} = (0,0)$. Build:\n- a balanced $k$-d tree by recursively splitting on alternating axes at a threshold equal to the median coordinate value among the points in the node, and\n- a balanced vantage-point tree (VP-tree) by recursively selecting any data point in the node as the vantage point and choosing the split radius equal to the median of the distances from the vantage point to the remaining points in the node.\n\nFor both trees, consider the standard backtracking nearest-neighbor search that descends to the side containing $\\mathbf{q}$, updates $\\tau$ when reaching a leaf, and then, upon backtracking at an internal node, explores the other child if and only if geometry does not exclude the possibility of finding a point at distance strictly less than the current best $\\tau$.\n\nTasks:\n- Using only the definitions above and Euclidean geometry, derive from first principles the geometric pruning conditions used by each tree’s search (in terms of $d(\\mathbf{q},\\cdot)$, $\\tau$, and the node’s split geometry), without appealing to any pre-stated algorithmic shortcut formulas.\n- Prove that, for the dataset $P$ and query $\\mathbf{q}$, the balanced $k$-d tree nearest-neighbor search visits every leaf and thus computes $T_{\\mathrm{kd}}(n)$ point-to-query distances, where $T_{\\mathrm{kd}}(n)$ is the exact number of leaf distance computations expressed as a function of $n$.\n- Prove that, for the same $P$ and $\\mathbf{q}$, the balanced VP-tree nearest-neighbor search also visits every leaf and thus computes $T_{\\mathrm{vp}}(n)$ point-to-query distances, where $T_{\\mathrm{vp}}(n)$ is the exact number of leaf distance computations expressed as a function of $n$.\n- Define the ratio $\\rho(n) = T_{\\mathrm{kd}}(n) / T_{\\mathrm{vp}}(n)$ and determine its exact value.\n\nYour final answer must be the exact value of $\\rho(n)$ as a single real number. No rounding is required and no units apply. Express the final answer as a single number.",
            "solution": "The problem is assessed as valid as it is self-contained, scientifically grounded in computational geometry, and poses a well-defined set of tasks. The premises lead to a unique, verifiable solution.\n\nThe solution proceeds in four parts as requested: deriving the pruning conditions, analyzing the $k$-d tree search, analyzing the VP-tree search, and computing the final ratio.\n\n### Part 1: Derivation of Geometric Pruning Conditions\n\nLet $\\mathbf{q}$ be the query point and $\\tau$ be the current best distance to a nearest neighbor found so far. The search algorithm explores a child node if and only if the region of space it represents cannot be geometrically proven to contain no points closer to $\\mathbf{q}$ than $\\tau$. This means we prune a region if the minimum possible distance from $\\mathbf{q}$ to any point in that region is greater than or equal to $\\tau$.\n\n**$k$-d Tree Pruning Condition:**\nA node in a $k$-d tree partitions space with an axis-aligned hyperplane, for instance $x_j=s$, into two child regions: $H_1 = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid x_j \\le s\\}$ and $H_2 = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid x_j > s\\}$. Let the search algorithm first explore the \"near\" child, whose region contains $\\mathbf{q}$. Suppose $\\mathbf{q}$ is in $H_1$, so its $j$-th coordinate $q_j$ satisfies $q_j \\le s$. To decide whether to explore the \"far\" child, corresponding to region $H_2$, we must find the minimum distance from $\\mathbf{q}$ to any point $\\mathbf{p} \\in H_2$.\nThe squared Euclidean distance is $d(\\mathbf{q}, \\mathbf{p})^2 = (q_1 - p_1)^2 + (q_2 - p_2)^2$. For $\\mathbf{p} \\in H_2$, its $j$-th coordinate $p_j$ satisfies $p_j > s$. The distance $d(\\mathbf{q}, \\mathbf{p})$ is minimized when $\\mathbf{p}$ is the orthogonal projection of $\\mathbf{q}$ onto the boundary of $H_2$, which is the line $x_j=s$. The closest point in $H_2$ to $\\mathbf{q}$ is infinitesimally close to the point on the splitting line with coordinates identical to $\\mathbf{q}$ except in the $j$-th dimension, which is $s$. The minimum distance is therefore the distance from $\\mathbf{q}$ to the splitting hyperplane:\n$$ \\min_{\\mathbf{p} \\in H_2} d(\\mathbf{q}, \\mathbf{p}) = |q_j - s| $$\nThe search prunes the far child region ($H_2$) if this minimum distance is greater than or equal to the current best distance $\\tau$. Thus, the other child is explored if and only if:\n$$ |q_j - s| < \\tau $$\n\n**Vantage-Point (VP) Tree Pruning Condition:**\nA node in a VP-tree partitions space using a vantage point $\\mathbf{v}$ and a radius $\\mu$. The two child regions are the \"inside\" region $R_{in} = \\{\\mathbf{p} \\in \\mathbb{R}^2 \\mid d(\\mathbf{p}, \\mathbf{v}) \\le \\mu\\}$ and the \"outside\" region $R_{out} = \\{\\mathbf{p} \\in \\mathbb{R}^2 \\mid d(\\mathbf{p}, \\mathbf{v}) > \\mu\\}$. The search explores the unvisited child region if it cannot be pruned.\n\n1.  **Pruning the inside region ($R_{in}$):** The minimum distance from the query $\\mathbf{q}$ to any point $\\mathbf{p} \\in R_{in}$ is given by the distance from $\\mathbf{q}$ to the ball $B(\\mathbf{v}, \\mu)$. From the triangle inequality, this is $\\min_{\\mathbf{p} \\in R_{in}} d(\\mathbf{q}, \\mathbf{p}) = \\max(0, d(\\mathbf{q}, \\mathbf{v}) - \\mu)$. We prune $R_{in}$ if this minimum distance is $\\ge \\tau$, which requires $d(\\mathbf{q}, \\mathbf{v}) - \\mu \\ge \\tau$, or:\n    $$ d(\\mathbf{q}, \\mathbf{v}) \\ge \\mu + \\tau $$\n2.  **Pruning the outside region ($R_{out}$):** The minimum distance from $\\mathbf{q}$ to any point $\\mathbf{p} \\in R_{out}$ is the distance from $\\mathbf{q}$ to the exterior of the ball $B(\\mathbf{v}, \\mu)$. This is $\\min_{\\mathbf{p} \\in R_{out}} d(\\mathbf{q}, \\mathbf{p}) = \\max(0, \\mu - d(\\mathbf{q}, \\mathbf{v}))$. We prune $R_{out}$ if this minimum is $\\ge \\tau$, which requires $\\mu - d(\\mathbf{q}, \\mathbf{v}) \\ge \\tau$, or:\n    $$ d(\\mathbf{q}, \\mathbf{v}) \\le \\mu - \\tau $$\n\n### Part 2: Analysis of the $k$-d Tree Search\n\nThe dataset is $P = \\{\\mathbf{p}_{k}\\}_{k=1}^{n}$ with $\\mathbf{p}_{k} = (R\\cos(2\\pi k/n), R\\sin(2\\pi k/n))$ for an even integer $n \\ge 4$ and $R > 0$. The query point is $\\mathbf{q} = (0,0)$. For any point $\\mathbf{p}_k \\in P$, the distance to the query is $d(\\mathbf{q}, \\mathbf{p}_k) = \\sqrt{(R\\cos(2\\pi k/n))^2 + (R\\sin(2\\pi k/n))^2} = \\sqrt{R^2} = R$.\n\nThe search algorithm descends to a first leaf, computes the distance to its point, and sets the initial best distance $\\tau = R$. Since all points in $P$ are at distance $R$ from $\\mathbf{q}$, $\\tau$ will remain fixed at $R$ throughout the search.\n\nAt any internal node, the tree splits on an alternating coordinate ($x_1$ or $x_2$) at the median value $s$ of that coordinate for the points in the node. The query point is $\\mathbf{q}=(0,0)$, so its coordinates are $q_1=0$ and $q_2=0$. The condition to explore the far child across a split $x_j=s$ is $|q_j - s| < \\tau$, which becomes $|0 - s| < R$, or $|s| < R$.\n\nWe must prove that for any split in the tree, the median value $s$ satisfies $|s| < R$.\nA split is performed at a node containing a subset of points $S \\subseteq P$, where $|S| \\ge 2$. All points in $S$ lie on the circle of radius $R$. Let the split be on the $j$-th coordinate. The coordinate values for the points in $S$ are $\\{p_j \\mid \\mathbf{p} \\in S\\}$. Since each $\\mathbf{p} \\in S$ is on the circle, its coordinates satisfy $|p_j| \\le R$. The median $s$ of these values is defined to be the coordinate of one of the points in $S$, or the average of two such coordinates. In either case, the median $s$ must lie within the range of the coordinate values, so $|s| \\le \\max_{\\mathbf{p} \\in S} |p_j| \\le R$.\n\nFor pruning to occur, we need $|s| \\ge R$. Given $|s| \\le R$, this implies we would need $|s| = R$. For a split on $x_1=s$, this would require $s \\in \\{-R, R\\}$. For $s=R$ to be the median of the $x_1$-coordinates of points in $S$, at least half of the points in $S$ must have an $x_1$-coordinate of $R$. The only point in the entire set $P$ with $x_1=R$ is $\\mathbf{p}_n = (R,0)$. Since each leaf stores a unique point, a node with $|S| \\ge 2$ cannot have half or more of its points being $\\mathbf{p}_n$. Therefore, the median $x_1$-coordinate cannot be $R$. By similar logic, the median cannot be $-R$, nor can the median $y_1$-coordinate be $\\pm R$.\nThus, for any split in the tree, the median value $s$ must strictly satisfy $|s| < R$.\n\nSince the pruning condition $|s| \\ge \\tau=R$ is never met, the search algorithm never prunes. At every internal node, it explores both children. This means the search visits every node in the tree, including all leaf nodes.\nThe tree is specified to have exactly one point per leaf, so there are $n$ leaves. The number of distance computations is the number of visited leaves.\nTherefore, $T_{\\mathrm{kd}}(n) = n$.\n\n### Part 3: Analysis of the VP-Tree Search\n\nThe dataset, query $\\mathbf{q}=(0,0)$, and resulting best distance $\\tau=R$ are the same as before.\nAt any internal node, a vantage point $\\mathbf{v} \\in P$ is chosen. Since $\\mathbf{v}$ is on the circle of radius $R$ centered at the origin, its distance to the query point is $d(\\mathbf{q}, \\mathbf{v}) = R$. The split radius $\\mu$ is the median of distances from $\\mathbf{v}$ to the other points in the node's subset.\n\nSubstituting $d(\\mathbf{q}, \\mathbf{v}) = R$ and $\\tau = R$ into the pruning conditions:\n1.  Prune inside region: $R \\ge \\mu + R \\implies \\mu \\le 0$.\n2.  Prune outside region: $R \\le \\mu - R \\implies \\mu \\ge 2R$.\n\nLet's analyze the split radius $\\mu$ at the root node. The vantage point $\\mathbf{v}$ can be any point in $P$; by symmetry, we choose $\\mathbf{v} = \\mathbf{p}_n = (R,0)$. We need to find the median of the $n-1$ distances $d(\\mathbf{p}_k, \\mathbf{v})$ for $k \\in \\{1, 2, \\dots, n-1\\}$.\nThe squared distance is:\n$d(\\mathbf{p}_k, \\mathbf{v})^2 = (R\\cos(2\\pi k/n) - R)^2 + (R\\sin(2\\pi k/n) - 0)^2$\n$d(\\mathbf{p}_k, \\mathbf{v})^2 = R^2(\\cos^2(2\\pi k/n) - 2\\cos(2\\pi k/n) + 1 + \\sin^2(2\\pi k/n))$\n$d(\\mathbf{p}_k, \\mathbf{v})^2 = R^2(2 - 2\\cos(2\\pi k/n)) = 4R^2\\sin^2(\\pi k/n)$\n$d(\\mathbf{p}_k, \\mathbf{v}) = 2R\\sin(\\pi k/n)$, since $k \\in \\{1, \\dots, n-1\\}$ ensures $\\sin(\\pi k/n) > 0$.\n\nThe set of distances is $\\{ 2R\\sin(\\pi k/n) \\}_{k=1}^{n-1}$. Due to the symmetry of sine, $\\sin(\\pi k/n) = \\sin(\\pi(n-k)/n)$, so the distances for $k$ and $n-k$ are identical. The set of values consists of $d_k$ for $k \\in \\{1, \\dots, n/2-1\\}$ each appearing twice, and $d_{n/2} = 2R\\sin(\\pi/2)=2R$ appearing once. Total number of distances is $2(n/2-1)+1=n-1$.\nWe need the median of this list of $n-1$ numbers. We sort them to find the item at position $(n-1+1)/2 = n/2$. The unique values sorted are $v_j = 2R\\sin(j\\pi/n)$ for $j=1, \\dots, n/2$. The full sorted list is $v_1, v_1, v_2, v_2, \\dots$. The element at the $m$-th position is $v_{\\lceil m/2 \\rceil}$. For the median, we need the element at position $m=n/2$.\nThe median radius is $\\mu = v_{\\lceil (n/2)/2 \\rceil} = v_{\\lceil n/4 \\rceil} = 2R\\sin(\\lceil n/4 \\rceil \\pi / n)$.\n\nLet $\\theta = \\lceil n/4 \\rceil \\pi / n$. We need to check if $\\mu \\le 0$ or $\\mu \\ge 2R$.\nSince $n \\ge 4$, we have $n/4 \\ge 1$, so $\\lceil n/4 \\rceil \\ge 1$, which means $\\theta > 0$ and $\\sin(\\theta)>0$, so $\\mu > 0$. The \"inside\" region is never pruned.\nFor the \"outside\" region, we check if $\\mu \\ge 2R$, which is $2R\\sin(\\theta) \\ge 2R \\implies \\sin(\\theta) \\ge 1$.\nSince $\\sin(\\theta) \\le 1$, this would require $\\sin(\\theta) = 1$, which means $\\theta = \\pi/2$. This would imply $\\lceil n/4 \\rceil / n = 1/2$, or $\\lceil n/4 \\rceil = n/2$.\nLet $x = n/4$. We need $\\lceil x \\rceil = 2x$. If $x$ is an integer, $x=2x \\implies x=0 \\implies n=0$, which is not allowed. If $x$ is not an integer, let $x=k+\\epsilon$ with $k$ an integer and $0 < \\epsilon < 1$. $\\lceil k+\\epsilon \\rceil = k+1$. So $k+1 = 2(k+\\epsilon) \\implies k+1=2k+2\\epsilon \\implies k=1-2\\epsilon$. Since $k$ is an integer, this is impossible for $0 < \\epsilon < 1$.\nActually, $\\lceil x \\rceil = 2x$ holds for $x \\in (0, 1/2]$ where $x=k/2$, e.g. $x=1/2$. $\\lceil 1/2 \\rceil = 1$, $2(1/2)=1$. So $n/4 = 1/2 \\implies n=2$.\nHowever, the problem specifies $n \\ge 4$. For $n \\ge 4$, $n/4 \\ge 1$, so $\\lceil n/4 \\rceil = n/2$ is not possible.\nTherefore, $\\sin(\\theta) < 1$, which implies $\\mu < 2R$.\n\nNeither pruning condition ($\\mu \\le 0$ or $\\mu \\ge 2R$) can be met at the root node. The same logic applies to any sub-problem. A node at any level of the tree contains a subset of points from $P$, all still lying on the circle of radius $R$. The query is still $\\mathbf{q}=(0,0)$, and the best distance found will always be $\\tau=R$. The median radius $\\mu$ for any sub-problem will also be a chord length, thus strictly between $0$ and $2R$.\nConsequently, pruning never occurs in the VP-tree search either. The search must visit every leaf. As the tree has $n$ leaves, the number of distance computations is:\n$T_{\\mathrm{vp}}(n) = n$.\n\n### Part 4: Calculation of the Ratio $\\rho(n)$\n\nThe ratio $\\rho(n)$ is defined as $T_{\\mathrm{kd}}(n) / T_{\\mathrm{vp}}(n)$.\nUsing the results from the previous parts:\n$$ \\rho(n) = \\frac{T_{\\mathrm{kd}}(n)}{T_{\\mathrm{vp}}(n)} = \\frac{n}{n} = 1 $$\nThe ratio is exactly $1$.",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}