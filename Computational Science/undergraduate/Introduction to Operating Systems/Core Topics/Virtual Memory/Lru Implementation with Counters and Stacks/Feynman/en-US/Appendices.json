{
    "hands_on_practices": [
        {
            "introduction": "Before diving into the mechanics of implementing LRU, it is crucial to understand *why* tracking recency is so effective. This exercise contrasts the Least Recently Used (LRU) policy with another classic, Least Frequently Used (LFU), to highlight their different responses to changes in a program's locality of reference. By tracing a workload with a distinct phase change , you will discover how LFU's reliance on historical frequency can be a liability, while LRU's focus on recency allows for rapid and efficient adaptation.",
            "id": "3655493",
            "problem": "Consider a demand-paged system with a fixed working set of memory frames of size $M = 3$, using page replacement when a referenced page is not currently resident. A page fault occurs on any reference to a page not resident. Two classic replacement strategies are often implemented with counters: Least Recently Used (LRU) and Least Frequently Used (LFU). Least Recently Used (LRU) selects for eviction the page whose last access time is furthest in the past; one implementation maintains a stack (most-recently-used at the top, least-recently-used at the bottom) and on each reference moves the page to the top, evicting from the bottom when needed. Least Frequently Used (LFU) selects for eviction the page with the smallest reference count; one implementation maintains per-page frequency counters that increment on each reference, with ties broken by Least Recently Used to make a deterministic choice. Frequency counters are assumed to be persistent across evictions (they do not reset when a page leaves memory).\n\nSome students conflate frequency counters with recency counters and assume they behave similarly. Your task is to reason from the core definitions above and show a workload where LFU counters make strictly worse decisions than LRU recency. Consider the following reference string comprising two phases that model a phase change in the program’s locality:\n\n- Phase $1$: the sequence $\\left(A,B,C\\right)$ repeated $5$ times, for a total of $15$ references.\n- Phase $2$: the sequence $\\left(D,E,F\\right)$ repeated $4$ times, for a total of $12$ references.\n\nAssume the memory starts empty and that all pages $A,B,C,D,E,F$ are distinct. Using only the operational definitions of LRU and LFU given above, determine which statement best captures the page fault counts and the underlying reason for the difference between the two strategies on this workload.\n\nA. LRU incurs $6$ faults while LFU incurs $15$ faults, because LFU’s persistent frequency counters keep the high-frequency but cold pages $A,B,C$ resident throughout Phase $2$, thrashing among $D,E,F$ until their counts can catch up; LRU’s stack property immediately adapts to the phase change by evicting the least-recently-used pages.\n\nB. LRU and LFU both incur $6$ faults on this workload, because recency and frequency coincide when the working set fits in $3$ frames, so counters of either type behave the same.\n\nC. LFU incurs fewer faults ($9$) than LRU ($12$) on this workload, because frequency counters prioritize globally popular pages, whereas recency evicts useful pages too aggressively at phase boundaries.\n\nD. LRU and LFU have identical fault counts on all workloads with distinct pages when ties are broken by Least Recently Used, so any observed difference must be due to implementation overhead rather than policy.",
            "solution": "I will trace the page frames for both LRU and LFU algorithms.\nMemory size $M=3$. Initial state: empty frames `[ , , ]`.\n\n**LRU TRACE**\n\n*   **Reference String:** A, B, C, A, B, C, ... (Phase 1) followed by D, E, F, D, E, F, ... (Phase 2)\n\n*   **Phase 1: (A, B, C) x 5**\n    *   Ref 1: `A`. Fault. Memory: `[A]`. Faults: 1.\n    *   Ref 2: `B`. Fault. Memory: `[B, A]`. Faults: 2.\n    *   Ref 3: `C`. Fault. Memory: `[C, B, A]`. Faults: 3.\n    *   Ref 4: `A`. Hit. Memory: `[A, C, B]`. (A moves to top of stack). Faults: 3.\n    *   Ref 5: `B`. Hit. Memory: `[B, A, C]`. (B moves to top of stack). Faults: 3.\n    *   Ref 6: `C`. Hit. Memory: `[C, B, A]`. (C moves to top of stack). Faults: 3.\n    *   ... and so on for the rest of Phase 1. The pages A, B, C are now in memory. The sequence is a repeating loop within the working set size. There will be no more faults in Phase 1.\n    *   **End of Phase 1 Faults (LRU): 3**.\n    *   Memory state at end of Phase 1 (after 15 refs): The last three references were C, B, A in some order. The last reference is C. Before that B. Before that A. So the LRU stack is `[C, B, A]` (Top to Bottom).\n\n*   **Phase 2: (D, E, F) x 4**\n    *   Ref 16: `D`. Fault. Memory does not contain `D`. LRU page is `A` (bottom of stack `[C, B, A]`). Evict `A`. Memory: `[D, C, B]`. Faults: $3+1=4$.\n    *   Ref 17: `E`. Fault. Memory does not contain `E`. LRU page is `B`. Evict `B`. Memory: `[E, D, C]`. Faults: $4+1=5$.\n    *   Ref 18: `F`. Fault. Memory does not contain `F`. LRU page is `C`. Evict `C`. Memory: `[F, E, D]`. Faults: $5+1=6$.\n    *   At this point, memory contains `D, E, F`.\n    *   Ref 19: `D`. Hit. Memory: `[D, F, E]`. Faults: 6.\n    *   Ref 20: `E`. Hit. Memory: `[E, D, F]`. Faults: 6.\n    *   Ref 21: `F`. Hit. Memory: `[F, E, D]`. Faults: 6.\n    *   ... and so on. The reference string now cycles through `D, E, F`, which are all in memory. There will be no more faults in Phase 2.\n    *   **End of Phase 2 Faults (LRU): 3**.\n\n*   **Total Faults (LRU):** $3$ (from Phase 1) + $3$ (from Phase 2) = $6$ faults.\n\n**LFU TRACE**\n\n*   **State Tracking:** Memory frames `[ , , ]` and frequency counts `{}`. Tie-break with LRU.\n\n*   **Phase 1: (A, B, C) x 5**\n    *   Ref 1: `A`. Fault. Memory: `[A]`. Counts: `{A:1}`. Faults: 1.\n    *   Ref 2: `B`. Fault. Memory: `[A, B]`. Counts: `{A:1, B:1}`. Faults: 2.\n    *   Ref 3: `C`. Fault. Memory: `[A, B, C]`. Counts: `{A:1, B:1, C:1}`. Faults: 3.\n    *   After the first loop (A, B, C):\n        *   Ref 4: `A`. Hit. Memory: `[A, B, C]`. Counts: `{A:2, B:1, C:1}`. Faults: 3.\n        *   Ref 5: `B`. Hit. Memory: `[A, B, C]`. Counts: `{A:2, B:2, C:1}`. Faults: 3.\n        *   Ref 6: `C`. Hit. Memory: `[A, B, C]`. Counts: `{A:2, B:2, C:2}`. Faults: 3.\n    *   This continues. The pages `A, B, C` remain in memory.\n    *   After 5 full repetitions of (A, B, C):\n        *   Each page `A`, `B`, `C` has been referenced 5 times.\n    *   **End of Phase 1 State (LFU):**\n        *   Memory: `[A, B, C]` (order doesn't matter for LFU eviction, only LRU tie-break).\n        *   Counts: `{A:5, B:5, C:5}`.\n        *   LRU order for tie-break: The last reference was `C`, before that `B`, before that `A`. So LRU stack is `[C, B, A]`.\n        *   Faults: 3.\n\n*   **Phase 2: (D, E, F) x 4**\n    *   Ref 16: `D`. Fault. Memory is full. Need to evict.\n        *   Current residents: `A, B, C`.\n        *   Counts: `Count(A)=5`, `Count(B)=5`, `Count(C)=5`.\n        *   All counts are equal. LFU must tie-break using LRU.\n        *   LRU order is `[C, B, A]` (Top to Bottom). Least recently used is `A`. Evict `A`.\n        *   Memory: `[B, C, D]`.\n        *   Counts: `{A:5, B:5, C:5, D:1}`. (Counters are persistent).\n        *   Faults: $3+1=4$.\n\n    *   Ref 17: `E`. Fault. Memory is full. Need to evict.\n        *   Current residents: `B, C, D`.\n        *   Counts: `Count(B)=5`, `Count(C)=5`, `Count(D)=1`.\n        *   LFU choice: Evict page with minimum count, which is `D`.\n        *   Memory: `[B, C, E]`.\n        *   Counts: `{A:5, B:5, C:5, D:1, E:1}`.\n        *   Faults: $4+1=5$.\n\n    *   Ref 18: `F`. Fault. Memory is full. Need to evict.\n        *   Current residents: `B, C, E`.\n        *   Counts: `Count(B)=5`, `Count(C)=5`, `Count(E)=1`.\n        *   LFU choice: Evict page with minimum count, which is `E`.\n        *   Memory: `[B, C, F]`.\n        *   Counts: `{A:5, B:5, C:5, D:1, E:1, F:1}`.\n        *   Faults: $5+1=6$.\n\n    *   Ref 19: `D`. Fault. Memory is full. Need to evict.\n        *   Current residents: `B, C, F`.\n        *   Counts: `Count(B)=5`, `Count(C)=5`, `Count(F)=1`.\n        *   LFU choice: Evict page with minimum count, which is `F`.\n        *   Memory: `[B, C, D]`.\n        *   Counts update: `D` is now referenced, so `Count(D)` becomes $1+1=2$.\n        *   Counts: `{A:5, B:5, C:5, D:2, E:1, F:1}`.\n        *   Faults: $6+1=7$.\n    *   This is a pattern of thrashing. For the triplet `(D,E,F)`, one frame is available, but the other two are occupied by high-frequency but no-longer-used pages `B` and `C`.\n    *   Every single reference in Phase 2 will cause a page fault. Phase 2 has $4 \\times 3 = 12$ references.\n    *   Total faults for LFU:\n        Phase 1: 3 faults (`A`, `B`, `C` initially).\n        Phase 2: 12 faults (one for each reference).\n    *   **Total Faults (LFU):** $3 + 12 = 15$ faults.\n\n**Summary of Results:**\n*   LRU faults: 6\n*   LFU faults: 15\n\n**Reasoning:**\n*   LRU is adaptive. As soon as the program's locality of reference shifts from the set $\\{A, B, C\\}$ to $\\{D, E, F\\}$, LRU rapidly evicts the now \"cold\" pages $A, B$, and $C$. It takes exactly 3 faults for LRU to load the new working set, after which it operates without faults.\n*   LFU is not adaptive. The pages $A, B, C$ acquired a high frequency count ($5$) in Phase 1. When Phase 2 begins, LFU continues to perceive $A, B, C$ as highly valuable and is unwilling to evict them because their counts are higher than the new pages' counts. This forces the new working set of size 3 (pages $\\{D,E,F\\}$) to compete for a smaller number of frames, leading to thrashing.\n\n**Option Evaluation:**\n\n*   **A. LRU incurs $6$ faults while LFU incurs $15$ faults...** The fault counts are correct, and the reasoning is sound. LFU retains high-frequency \"cold\" pages, causing thrashing, while LRU adapts immediately. **This is the correct answer.**\n\n*   **B. LRU and LFU both incur $6$ faults...** The fault counts are incorrect.\n\n*   **C. LFU incurs fewer faults ($9$) than LRU ($12$)...** The fault counts are incorrect.\n\n*   **D. LRU and LFU have identical fault counts...** The premise is fundamentally false.",
            "answer": "$$\n\\boxed{A}\n$$"
        },
        {
            "introduction": "A perfect LRU implementation requires maintaining a full, ordered stack of all resident pages, which can be costly. A common practical alternative is to use a coarse-grained counter for each page, approximating its recency rank. This practice  explores such a scheme, where the $N$ possible stack positions are mapped into a much smaller set of $2^b$ buckets. Your task is to analyze the inherent trade-offs of this approximation, calculating the probability of ranking collisions and the resulting rate of 'mis-evictions'—cases where the true LRU page is not chosen for replacement.",
            "id": "3655422",
            "problem": "A system designer wishes to replace a full Least Recently Used (LRU) stack with a minimal per-page counter to approximate LRU for eviction decisions. Least Recently Used (LRU) maintains a total order of pages by recency: each resident page has a unique stack position, with position $1$ being most recently referenced and position $N$ being least recently referenced, where $N$ is the number of resident frames. To conserve memory, the designer proposes a counter-based approximation with $b=4$ bits per page that stores only a coarse-grained rank rather than the exact stack position.\n\nThe minimal counter-LRU is constructed as follows. Let $N$ be the number of resident frames, and define a bucket width\n$$\nq \\equiv \\frac{N}{2^{b}}.\n$$\nPartition the LRU stack positions $\\{1,2,\\dots,N\\}$ into $2^{b}$ contiguous buckets of width $q$ each. For a page with true stack position $s \\in \\{1,\\dots,N\\}$, define its stored counter value\n$$\nc(s) \\equiv \\min\\!\\left(\\left\\lfloor \\frac{s-1}{q}\\right\\rfloor,\\,2^{b}-1\\right),\n$$\nwhich maps $s$ into a bucket index $c \\in \\{0,1,\\dots,2^{b}-1\\}$. On eviction, the algorithm selects uniformly at random among pages whose counter equals $2^{b}-1$ (the last bucket), without consulting the exact stack order or timestamps. Assume $N$ is a positive integer multiple of $2^{b}$ so that $q$ is an integer and every bucket contains exactly $q$ pages.\n\nUsing only the fundamental properties of LRU (unique stack order of resident pages) and the symmetry of uniformly random stack orderings under steady-state random references within the resident set, perform the following:\n\n1. Construct the counter mapping $c(s)$ and explain why bucket sizes are equal and why ranking collisions (distinct pages sharing the same counter) necessarily occur.\n2. Derive, from first principles, the probability that two distinct randomly chosen resident pages have the same counter value under the uniform random distribution of stack positions.\n3. Derive the expected mis-eviction rate of the counter-LRU described above, defined as the probability that the algorithm evicts a page that is not the true least recently used page at the time of eviction.\n\nAssume $N=64$ frames and $b=4$ bits. Provide the final results for the collision probability and the mis-eviction rate as decimals. Round your answers to four significant figures. No units are required.",
            "solution": "**Problem Solution**\n\nFirst, let's compute the system parameters based on the given values $N=64$ and $b=4$.\nThe number of possible counter values (and thus, buckets) is $2^{b} = 2^{4} = 16$.\nThe bucket width is $q \\equiv \\frac{N}{2^{b}} = \\frac{64}{16} = 4$.\n\n**1. Counter Mapping and Collisions**\n\nThe counter value for a page at true stack position $s \\in \\{1, 2, \\dots, N\\}$ is given by $c(s) \\equiv \\min\\!\\left(\\left\\lfloor \\frac{s-1}{q}\\right\\rfloor,\\,2^{b}-1\\right)$.\nWith $N=64$ and $q=4$, the argument of the floor function, $\\frac{s-1}{q}$, ranges from $\\frac{1-1}{4}=0$ for $s=1$ to $\\frac{64-1}{4} = \\frac{63}{4} = 15.75$ for $s=64$.\nThe floor of this argument, $\\left\\lfloor \\frac{s-1}{q}\\right\\rfloor$, therefore ranges from $\\lfloor 0 \\rfloor = 0$ to $\\lfloor 15.75 \\rfloor = 15$.\nThe maximum possible value is $15$, which is equal to $2^{b}-1 = 16-1$. So, the argument of the floor function never produces a value greater than or equal to $2^b$. Consequently, the $\\min$ operator is redundant for the given parameters, and the mapping simplifies to:\n$$\nc(s) = \\left\\lfloor \\frac{s-1}{q}\\right\\rfloor\n$$\nThis function maps the $N$ stack positions to the $2^b$ counter values $\\{0, 1, \\dots, 2^{b}-1\\}$.\n\nTo determine the size of each bucket, we find the set of stack positions $s$ that map to a given counter value $k \\in \\{0, 1, \\dots, 2^{b}-1\\}$.\n$$\nc(s) = k \\implies k = \\left\\lfloor \\frac{s-1}{q}\\right\\rfloor\n$$\nBy the definition of the floor function, this is equivalent to:\n$$\nk \\le \\frac{s-1}{q} < k+1\n$$\nMultiplying by $q$ yields:\n$$\nkq \\le s-1 < (k+1)q\n$$\nAdding $1$ to all parts gives the range of stack positions for bucket $k$:\n$$\nkq+1 \\le s \\le (k+1)q\n$$\nThe number of integer positions in this range is $((k+1)q) - (kq+1) + 1 = kq + q - kq - 1 + 1 = q$.\nThus, each counter value $k$ corresponds to a contiguous block of exactly $q$ stack positions. This confirms that all bucket sizes are equal, as stated in the problem's assumptions. For our parameters, each of the $16$ buckets contains $q=4$ stack positions.\n\nRanking collisions are instances where two or more distinct pages are assigned the same counter value. In this system, there are $N=64$ resident pages, each occupying a unique stack position. The number of available distinct counter values is $2^b=16$. According to the Pigeonhole Principle, if $N$ items (pages) are put into $2^b$ containers (counter values), and $N > 2^b$, then at least one container must hold more than one item. Here, $64 > 16$, so collisions are guaranteed. More specifically, since we have established that each counter value corresponds to $q=4$ stack positions, and each stack position is occupied by a unique page, exactly $q=4$ distinct pages will share any given counter value.\n\n**2. Collision Probability**\n\nWe wish to find the probability that two distinct, randomly chosen resident pages have the same counter value. We assume a uniform random distribution of stack positions, which implies that any permutation of the $N$ pages is equally likely.\n\nLet's select two distinct pages, $P_1$ and $P_2$. Let their stack positions be $s_1$ and $s_2$, respectively. Since the pages are distinct, $s_1 \\neq s_2$. We want to find the probability $P(c(s_1) = c(s_2))$.\n\nConsider page $P_1$. Its stack position $s_1$ falls into one of the $2^b$ buckets. Regardless of which bucket it is, that bucket contains a total of $q$ stack positions. Now, consider page $P_2$. There are $N-1$ remaining stack positions available for it. For $P_2$ to have the same counter as $P_1$, its stack position $s_2$ must be one of the other positions in the same bucket as $s_1$. Since the bucket containing $s_1$ has $q$ positions in total, and one is occupied by $P_1$, there are $q-1$ remaining positions in that bucket.\n\nThe probability that $s_2$ lands in one of these $q-1$ favorable positions, out of the $N-1$ total available positions, is:\n$$\nP(\\text{collision}) = \\frac{q-1}{N-1}\n$$\nAlternatively, using a combinatorial approach: The total number of ways to choose an ordered pair of distinct stack positions $(s_1, s_2)$ is $N(N-1)$. For the counters to be equal, both $s_1$ and $s_2$ must come from the same bucket. There are $2^b$ buckets. Within each bucket of size $q$, we can choose an ordered pair of distinct positions in $q(q-1)$ ways. The total number of favorable pairs is thus $2^b \\cdot q(q-1)$.\nThe probability is the ratio of favorable outcomes to total outcomes:\n$$\nP(\\text{collision}) = \\frac{2^b \\cdot q(q-1)}{N(N-1)}\n$$\nSubstituting $N = q \\cdot 2^b$:\n$$\nP(\\text{collision}) = \\frac{2^b \\cdot q(q-1)}{(q \\cdot 2^b)(N-1)} = \\frac{q(q-1)}{q(N-1)} = \\frac{q-1}{N-1}\n$$\nBoth methods yield the same result. Using the given values $N=64$ and $q=4$:\n$$\nP(\\text{collision}) = \\frac{4-1}{64-1} = \\frac{3}{63} = \\frac{1}{21}\n$$\nAs a decimal, this is $1 \\div 21 \\approx 0.0476190...$. Rounded to four significant figures, the probability is $0.04762$.\n\n**3. Mis-eviction Rate**\n\nA mis-eviction occurs if the algorithm evicts a page that is not the true least recently used (LRU) page. The true LRU page is the one at stack position $s=N$.\n\nThe eviction algorithm identifies all pages with the maximum counter value, $c_{max} = 2^b - 1$, and selects one uniformly at random for eviction. The set of pages eligible for eviction are those whose stack positions $s$ map to this counter value.\nFrom Part 1, the stack positions corresponding to counter $k$ are $s \\in [kq+1, (k+1)q]$. For the last bucket, $k = 2^b - 1$:\n$$\ns \\in [(2^b-1)q+1, (2^b-1+1)q] = [(2^b-1)q+1, 2^b q]\n$$\nSubstituting $N = 2^b q$:\n$$\ns \\in [N-q+1, N]\n$$\nWith $N=64$ and $q=4$, the eviction candidates are the pages at stack positions $\\{61, 62, 63, 64\\}$. This set consists of $q=4$ pages.\nThe true LRU page is at position $s=64$, which is a member of this candidate set.\nThe algorithm selects one page from this set of $q$ pages with uniform probability. A mis-eviction occurs if the selected page is not the one at $s=N$.\n\nThe set of eviction candidates contains one true LRU page and $q-1$ other pages. The probability of selecting any specific page from this set is $\\frac{1}{q}$.\nThe probability of selecting the true LRU page is $\\frac{1}{q}$.\nThe probability of a mis-eviction is the probability of selecting any of the other $q-1$ pages:\n$$\nP(\\text{mis-eviction}) = \\frac{q-1}{q}\n$$\nUsing the value $q=4$:\n$$\nP(\\text{mis-eviction}) = \\frac{4-1}{4} = \\frac{3}{4} = 0.75\n$$\nRounding to four significant figures gives $0.7500$.\n\nThe final results are:\n- Collision Probability: $\\frac{1}{21} \\approx 0.04762$\n- Mis-eviction Rate: $\\frac{3}{4} = 0.7500$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.04762 & 0.7500\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While counter-based schemes can approximate LRU well, their physical implementation with finite-bit counters can introduce subtle but severe vulnerabilities. This advanced exercise  challenges you to design an adversarial workload that exploits the counter 'wrap-around' phenomenon, where a finite-bit counter resets to zero after reaching its maximum value. You will demonstrate how this predictable implementation detail can be used to completely invert the notion of recency, causing the eviction policy to make catastrophically poor decisions and leading to a dramatic inflation in page faults compared to a true LRU algorithm.",
            "id": "3655499",
            "problem": "Consider a demand-paged memory system with cache capacity of $C$ page frames. True Least Recently Used (LRU) is defined as the policy that, on a miss, evicts the page that has not been used for the longest time, which induces the LRU stack property: when comparing two capacities $C_{1}$ and $C_{2}$ with $C_{1} < C_{2}$ under the same reference string, the set of pages held by capacity $C_{1}$ is always a subset of those held by capacity $C_{2}$ at the same time. Now consider an implementation that attempts to realize LRU with per-page counters (call it counter-LRU): it maintains a global $b$-bit counter $t \\in \\{0,1,\\ldots,2^{b}-1\\}$ that increments by $1$ at every reference and wraps modulo $M = 2^{b}$. On each access to page $p$, the page’s stored timestamp $p.\\mathrm{ts}$ is set to the current value of $t$. On a miss, the victim is chosen as the resident page with the numerically smallest $p.\\mathrm{ts}$ (ties are broken by the smallest page identifier). This implementation compares timestamp values as ordinary integers and is unaware of wrap-around, so it does not preserve the true recency order at counter wrap.\n\nYou will construct an adversarial access pattern with cyclic working sets that causes counter-LRU to misorder recency at each phase boundary and thereby fault substantially more than true LRU. Use the following fixed parameters and conventions:\n\n- Let the cache capacity be $C = W$, for some integer $W \\ge 2$.\n- Let there be $2$ disjoint working sets $A = \\{a_{1},\\ldots,a_{W}\\}$ and $B = \\{b_{1},\\ldots,b_{W}\\}$, so that at any time the active working set has size $W$.\n- A phase on working set $X \\in \\{A,B\\}$ consists of $K$ complete rounds over $X$ in a fixed cyclic order without immediate self-repeats (for example, $x_{1},x_{2},\\ldots,x_{W},x_{1},x_{2},\\ldots$ for $K$ rounds), followed by a padding sequence that repeatedly references the final item of $X$ until just before the phase switch the global counter satisfies $t \\equiv M - 1 \\pmod{M}$. Assume $K W \\le M - 1$ so that $t$ does not wrap during a phase.\n- The trace alternates phases $A$ and $B$ forever. The initial cache is empty and the initial counter is $t = 0$.\n\nStarting only from the core definitions above, do the following:\n\n- Construct the explicit adversarial access pattern implied by the description that, at each switch between phases, ensures that the numerical timestamps of the pages in the incoming working set are strictly smaller than those of the outgoing set.\n- Explain why, under counter-LRU, this causes the incoming set to evict itself rather than the outgoing set throughout the phase, and count the steady-state number of faults per phase under counter-LRU.\n- Explain why, under true LRU, once the incoming set is loaded at the start of a phase, all subsequent references within the phase are hits (ignore the initial cold-start of the entire trace and count steady-state faults per phase).\n- Define the blow-up factor $\\rho$ as the ratio of the steady-state number of faults per phase under counter-LRU to that under true LRU for this adversarial trace, and derive a closed-form expression for $\\rho$ in terms of $K$, $W$, and $b$ subject to $K W \\le M - 1$ with $M = 2^{b}$.\n\nYour final answer must be a single closed-form symbolic expression for $\\rho$. No rounding is necessary, and no units are required.",
            "solution": "The problem requires the construction and analysis of an adversarial access pattern for a counter-based LRU approximation, comparing its performance to true LRU. The goal is to derive the performance blow-up factor $\\rho$.\n\nThe analysis will proceed in four parts:\n1.  Construction of the explicit adversarial access pattern.\n2.  Calculation of the steady-state number of faults per phase for a true LRU cache.\n3.  Calculation of the steady-state number of faults per phase for the counter-LRU implementation.\n4.  Derivation of the blow-up factor $\\rho$.\n\nLet's establish the context based on the provided definitions. The system has a cache of capacity $C=W$ frames. There are two disjoint working sets, $A = \\{a_{1}, \\ldots, a_{W}\\}$ and $B = \\{b_{1}, \\ldots, b_{W}\\}$. The global counter $t$ is a $b$-bit integer, operating modulo $M=2^b$.\n\n1.  Construction of the Adversarial Access Pattern\n\nThe trace consists of alternating phases on working sets $A$ and $B$. A phase on a working set $X \\in \\{A, B\\}$ with pages $\\{x_1, \\ldots, x_W\\}$ is defined as:\n- A main part, consisting of $K$ full rounds of cyclic access to the pages of $X$: the sequence $(x_1, x_2, \\ldots, x_W)$ is repeated $K$ times. This constitutes $K \\times W$ memory accesses.\n- A padding part, consisting of repeated accesses to the last page, $x_W$, until the global counter $t$ reaches the value $M-1$.\n\nLet's consider the system in steady state, transitioning from a phase on set $A$ to a phase on set $B$. We can analyze the counter values by assuming the phase on set $A$ begins just after a counter wrap-around, so its first access occurs at $t=0$.\n- **Phase A (main part):** The trace is $K$ repetitions of $(a_1, \\ldots, a_W)$. This involves $KW$ accesses. The counter $t$ advances from $0$ to $KW-1$. The last access to page $a_i$ in the final round occurs at time $t = (K-1)W + i - 1$.\n- **Phase A (padding part):** The trace consists of repeated accesses to $a_W$. The counter increments from $KW$ up to $M-1$.\n- **State at the end of Phase A:** The cache contains the set of pages $A = \\{a_1, \\ldots, a_W\\}$. Their timestamps, $p.\\mathrm{ts}$, are the values of the counter $t$ at their last access.\n    - For $i \\in \\{1, \\ldots, W-1\\}$, page $a_i$ was last accessed during the final round of the main part. Thus, $a_i.\\mathrm{ts} = (K-1)W + i - 1$.\n    - Page $a_W$ was accessed last during the padding sequence, at the very end of the phase. Thus, $a_W.\\mathrm{ts} = M-1$.\n- **Start of Phase B:** The next access initiates Phase B. The counter $t$ increments from $M-1$ to $M$, which wraps around to $t=0$.\n    - The first access of Phase B, to page $b_1$, sets $b_1.\\mathrm{ts} = 0$.\n    - The second access, to $b_2$, sets $b_2.\\mathrm{ts} = 1$.\n    - In general, the accesses in the main part of Phase B will assign timestamps from $0$ to $KW-1$ to the pages in set $B$.\n\nThis construction achieves the adversarial condition: at the phase boundary, the resident pages from set $A$ have numerically large timestamps, while the incoming pages from set $B$ receive numerically small timestamps.\n\n2.  Analysis of True LRU\n\nTrue LRU evicts the page that has not been used for the longest duration. We analyze the number of faults in a single steady-state phase (e.g., Phase B).\n- **Initial state for Phase B:** The cache is full and contains the pages of working set $A$, i.e., $\\{a_1, \\ldots, a_W\\}$. These pages were accessed during the previous phase.\n- **First $W$ accesses of Phase B:** The access sequence is $b_1, b_2, \\ldots, b_W$. Since these pages are not in the cache, each access results in a fault.\n    - Access to $b_1$: Miss. The least recently used page in the cache is $a_1$ (it was accessed earliest among all pages in $A$ during the last round of Phase A). $a_1$ is evicted. $b_1$ is loaded.\n    - Access to $b_2$: Miss. The least recently used page is now $a_2$. $a_2$ is evicted. $b_2$ is loaded.\n    - ...\n    - Access to $b_W$: Miss. The least recently used page is $a_W$. $a_W$ is evicted. $b_W$ is loaded.\n- **State after $W$ accesses:** After these initial $W$ faults, the cache contains exactly the working set $B$, i.e., $\\{b_1, \\ldots, b_W\\}$.\n- **Subsequent accesses in Phase B:** The remaining $(K-1)W$ accesses in the main part, and all accesses in the padding part, are to pages in set $B$. Since the entire working set $B$ is now resident in the cache of capacity $C=W$, all these subsequent accesses are hits.\n- **Fault Count:** The total number of faults per phase for true LRU is the number of compulsory misses required to load the new working set.\n$$\nN_{\\mathrm{LRU}} = W\n$$\n\n3.  Analysis of Counter-LRU\n\nCounter-LRU evicts the resident page with the numerically smallest timestamp. We analyze its performance on the same steady-state Phase B.\n- **Initial state for Phase B:** The cache contains pages $\\{a_1, \\ldots, a_W\\}$. Their timestamps are $\\{(K-1)W, (K-1)W+1, \\ldots, KW-2, M-1\\}$.\n- **Core Argument:** We demonstrate that at least one page from set $A$ remains in the cache throughout the main part of Phase B.\n    - The page $a_W$ has timestamp $a_W.\\mathrm{ts} = M-1$.\n    - During the main part of Phase B, the global counter $t$ ranges from $0$ to $KW-1$. Any timestamp $p.\\mathrm{ts}$ assigned to a page in set $B$ during this part is therefore less than or equal to $KW-1$.\n    - The problem states the constraint $KW \\le M-1$, which implies $KW-1 < M-1$.\n    - Therefore, for any page $b_j$ accessed during the main part of Phase B, its timestamp $b_j.\\mathrm{ts}$ will be strictly smaller than $a_W.\\mathrm{ts}$.\n    - Since counter-LRU evicts the page with the minimum timestamp, and $a_W$'s timestamp is always larger than any timestamp of a page in set $B$ (during the main phase), $a_W$ will never be a candidate for eviction during the main part of Phase B.\n- **Consequence:** Throughout the $KW$ accesses of the main part of Phase B, at least one frame is occupied by an A-page ($a_W$). This means the effective cache size available for the working set $B$ is at most $W-1$ frames.\n- The access pattern for set $B$ is a cyclic sequence over $W$ distinct pages. It is a classic result in caching theory that a cyclic access pattern on a working set of size $W$ in a cache of size less than $W$ results in a fault on every access.\n- Since the $W$ pages of set $B$ are competing for at most $W-1$ frames, every one of the $KW$ accesses $(b_1, \\ldots, b_W, \\ldots)$ during the main part of the phase will result in a miss.\n- The padding accesses to $b_W$ will be hits, as the very last access of the main part is to $b_W$, which loads it into the cache.\n- **Fault Count:** The total number of faults per phase for counter-LRU is equal to the number of accesses in the main part of the phase.\n$$\nN_{\\mathrm{C-LRU}} = K W\n$$\n\n4.  Derivation of the Blow-up Factor $\\rho$\n\nThe blow-up factor $\\rho$ is defined as the ratio of the steady-state number of faults per phase under counter-LRU to that under true LRU.\n$$\n\\rho = \\frac{N_{\\mathrm{C-LRU}}}{N_{\\mathrm{LRU}}}\n$$\nSubstituting the derived fault counts:\n$$\n\\rho = \\frac{KW}{W}\n$$\nThe factor $W$ cancels out.\n$$\n\\rho = K\n$$\nThe expression for the blow-up factor $\\rho$ is simply $K$. This result depends on the parameter $K$, which sets the number of rounds per phase. The parameters $W$ and $b$ are embedded in the problem's premise ($C=W$, $M=2^b$, $KW \\le M-1$) that makes the adversarial pattern effective, but they do not appear in the final expression for the ratio of faults.",
            "answer": "$$\\boxed{K}$$"
        }
    ]
}