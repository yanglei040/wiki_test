{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental component of a file system is the data structure that represents a file, classically known as an inode. This first exercise explores a critical design decision at the file organization layer: whether to pre-allocate a fixed table of inodes or create them on demand. By deriving the upper bounds on the number of files each system can support, you will quantify the fundamental trade-off between the overhead of static provisioning and the flexibility of dynamic allocation. ",
            "id": "3642840",
            "problem": "A storage stack implements two contrasting on-disk metadata designs at the file organization layer within a single partition of total size $P$ bytes, understood through the classical layering where the logical file layer uses index nodes (inodes) to name and locate file data and the directory layer stores directory entries that bind names to inode identifiers. Consider the following two designs.\n\nFixed-inode design. The file system (FS) preallocates an on-disk inode table of size $I$ bytes. Every index node (inode) occupies $i_s$ bytes. Exactly $r$ inodes are permanently unavailable to users because they are reserved for system objects and the root directory; the remaining inodes, if any, can be used for user-created regular files. Non-inode fixed overheads (for example, superblock, journaling structures, and allocation bitmaps), excluding the inode table itself, occupy $F_f$ bytes and do not scale with the number of user files. Each user-created regular file must appear in a directory; the directory layer stores exactly $d$ bytes per file as directory entry payload for that file, and these directory bytes are placed in the same general data region that also stores ordinary file contents. You may assume that to maximize the count of distinct files, files of size $0$ bytes are created so that no data blocks are consumed beyond directory entries, and that the root directory and other system objects are already accounted for in the $r$ reserved inodes and $F_f$ bytes.\n\nDynamic-inode design. The FS does not preallocate a fixed-size inode table. Instead, inodes are carved on demand from the general free space region. Each user-created inode still stores $i_s$ bytes of inode state and additionally incurs an amortized allocation bookkeeping cost of $a$ bytes that scales linearly with the number of user inodes. Non-inode fixed overheads for this design (including any precreated system objects such as the root directory and their metadata) occupy $F_d$ bytes and do not scale with the number of user files. The per-file directory entry cost remains exactly $d$ bytes, stored in the same general free space region. As above, assume all user files have size $0$ bytes.\n\nWorking from first principles of the file and directory layers, and without invoking any prepackaged formulas, derive the tightest possible upper bounds consistent with the stated model on the maximum number of distinct user regular files that can be created under each design. Then, define the multiplicative improvement factor $\\gamma$ to be the ratio of the dynamic-inode upper bound to the fixed-inode upper bound, using only algebraic operators together with the standard floor and minimum operators as needed to capture discreteness and shared-space coupling. Express your final answer as a single closed-form analytic expression for $\\gamma$ in terms of $P$, $I$, $i_s$, $r$, $F_f$, $F_d$, $a$, and $d$. State no units in your final expression. Assume parameters are such that all denominators used are nonzero and the bounds are positive.",
            "solution": "The problem requires the derivation of the tightest possible upper bounds on the number of user-creatable files for two different on-disk metadata designs, and then to compute the ratio of these bounds. Let us proceed by analyzing each design from first principles. Let $N$ denote the number of distinct user regular files. The analysis assumes files of size $0$ bytes are created to maximize the file count by minimizing data block consumption, thereby focusing the analysis on metadata overhead.\n\nFirst, we analyze the **Fixed-inode design**.\nIn this design, two primary resources constrain the number of files: the finite number of preallocated inodes and the finite amount of space for directory entries. The maximum number of files, let us call it $N_{fixed}$, will be the minimum of the bounds imposed by these two constraints.\n\n1.  **Inode Constraint**: The total size of the preallocated inode table is $I$ bytes, and each inode occupies $i_s$ bytes. The total number of inodes that can be stored in this table is the integer part of the ratio of these sizes, which is $\\lfloor \\frac{I}{i_s} \\rfloor$. The problem states that $r$ of these inodes are reserved for system use. Therefore, the maximum number of inodes available for user files is the total number of inodes minus the reserved count. This imposes an upper bound on $N_{fixed}$:\n    $$ N_{fixed} \\le \\left\\lfloor \\frac{I}{i_s} \\right\\rfloor - r $$\n\n2.  **Space Constraint**: The total partition size is $P$ bytes. The space consumed by fixed overheads consists of the inode table itself ($I$ bytes) and other non-inode structures ($F_f$ bytes). The remaining space constitutes the general data region, available for storing file contents and directory entries. The size of this region is $P - I - F_f$. According to the model, each of the $N_{fixed}$ user files requires $d$ bytes for its directory entry, which are stored in this data region. Since we assume user files have size $0$, the total space consumed by these directory entries is $N_{fixed} \\cdot d$. This consumption cannot exceed the available space in the data region.\n    $$ N_{fixed} \\cdot d \\le P - I - F_f $$\n    This yields a second upper bound on $N_{fixed}$ by dividing by $d$ and taking the floor, as a non-integer number of files cannot be created:\n    $$ N_{fixed} \\le \\left\\lfloor \\frac{P - I - F_f}{d} \\right\\rfloor $$\n\nThe number of user files $N_{fixed}$ must satisfy both constraints simultaneously. The tightest possible upper bound is therefore the most restrictive of these two limits. This is expressed using the minimum operator:\n$$ N_{fixed} = \\min\\left( \\left\\lfloor \\frac{I}{i_s} \\right\\rfloor - r, \\left\\lfloor \\frac{P - I - F_f}{d} \\right\\rfloor \\right) $$\n\nNext, we analyze the **Dynamic-inode design**.\nIn this design, there is no preallocated inode table. Instead, inodes and directory entries are both allocated from a single general free space region. This means there is only one primary resource constraint: the total available space. Let $N_{dynamic}$ be the maximum number of user files in this design.\n\n1.  **Space Constraint**: The total partition size is $P$ bytes. Non-inode fixed overheads occupy $F_d$ bytes. The remaining space, $P - F_d$, is the general free space region from which all per-file metadata is allocated. For each user file created, a certain amount of space is consumed from this pool. This per-file cost includes:\n    - The inode itself: $i_s$ bytes.\n    - The amortized allocation bookkeeping for the inode: $a$ bytes.\n    - The directory entry: $d$ bytes.\n    - The file data a size of $0$ bytes.\n    The total space consumed per user file is $i_s + a + d$ bytes. For $N_{dynamic}$ files, the total space consumed is $N_{dynamic} \\cdot (i_s + a + d)$. This total must not exceed the available free space.\n    $$ N_{dynamic} \\cdot (i_s + a + d) \\le P - F_d $$\n    Solving for $N_{dynamic}$ and taking the floor to ensure an integer number of files gives the tightest upper bound:\n    $$ N_{dynamic} = \\left\\lfloor \\frac{P - F_d}{i_s + a + d} \\right\\rfloor $$\n\nFinally, we define the multiplicative improvement factor $\\gamma$ as the ratio of the dynamic-inode upper bound to the fixed-inode upper bound.\n$$ \\gamma = \\frac{N_{dynamic}}{N_{fixed}} $$\nSubstituting the derived expressions for $N_{dynamic}$ and $N_{fixed}$ yields the final closed-form expression for $\\gamma$.\n$$ \\gamma = \\frac{\\left\\lfloor \\frac{P - F_d}{i_s + a + d} \\right\\rfloor}{\\min\\left( \\left\\lfloor \\frac{I}{i_s} \\right\\rfloor - r, \\left\\lfloor \\frac{P - I - F_f}{d} \\right\\rfloor \\right)} $$\nThis expression is formulated using only the given parameters and the allowed algebraic, floor, and minimum operators, satisfying all conditions of the problem statement. The assumptions that denominators are non-zero and bounds are positive ensure this expression is well-defined.",
            "answer": "$$\n\\boxed{\\frac{\\left\\lfloor \\frac{P - F_d}{i_s + a + d} \\right\\rfloor}{\\min\\left( \\left\\lfloor \\frac{I}{i_s} \\right\\rfloor - r, \\left\\lfloor \\frac{P - I - F_f}{d} \\right\\rfloor \\right)}}\n$$"
        },
        {
            "introduction": "Once a file's existence is recorded, the file system must manage the locations of its data blocks. This practice contrasts two classic strategies for file-to-block mapping: the linked-list approach of the File Allocation Table (FAT) and modern extent-based allocation. You will construct a performance model to calculate the expected random access time, revealing how underlying data structures, combined with mechanisms like caching, dramatically affect file I/O latency. ",
            "id": "3642743",
            "problem": "An operating system’s file subsystem uses a layering in which the file mapping layer translates a file’s logical block index to an on-disk location before the storage layer issues the input/output operation. Consider two alternative mapping organizations for a file: File Allocation Table (FAT) and an extent-based mapping. In the File Allocation Table (FAT) organization, a file is a singly linked list of clusters, and the directory entry stores the starting cluster number. Let each cluster contain $C$ logical blocks. A random access to logical block index $b \\in \\mathbb{Z}_{\\ge 0}$ within the file requires traversing the FAT chain from the starting cluster to the cluster containing block $b$. Assume the following cost model for the mapping layer and storage layer:\n\n- Accessing a FAT entry that is already in the in-memory cache costs $t_{H}$ time units.\n- Accessing a FAT entry that is not in the cache costs $t_{F}$ time units (to fetch from storage into memory).\n- Independently for each needed FAT entry during a traversal, the probability that it is in the cache is $p \\in [0,1]$.\n- The cost to read the target data block from the storage device after mapping completes is $t_{B}$ time units.\n- The directory entry’s starting cluster number is assumed known in memory at cost $0$.\n- In the extent-based organization, the mapping layer performs an $O(1)$ extent lookup with cost $t_{E}$ time units and then the storage layer reads the target data block with the same cost $t_{B}$ time units.\n\nStarting from the basic definitions of how the File Allocation Table (FAT) chain traversal computes the cluster containing a given logical block and the linearity of expectation for independent cache-hit events, derive a closed-form expression for the ratio $R(b)$ of the expected total time to perform a random access to logical block $b$ in the FAT organization to the total time in the extent-based organization. Express your final answer as a single simplified analytic expression in terms of $b$, $C$, $p$, $t_{H}$, $t_{F}$, $t_{B}$, and $t_{E}$. No numerical evaluation is required. The final answer must be a single expression; do not provide an inequality or an equation other than the requested expression. If you choose to present any intermediate quantities, they must not appear in the final answer. Do not include units in the final answer.",
            "solution": "The problem requires the derivation of a closed-form expression for the ratio $R(b)$ of the expected total time for a random access to logical block $b$ in a File Allocation Table (FAT) organization to the total time for the same access in an extent-based organization. We will approach this by first determining the expression for the total time in the extent-based case, then deriving the expected total time for the FAT-based case, and finally computing their ratio.\n\nFirst, let us analyze the total time for an access in the extent-based organization. The problem states that the mapping layer performs an extent lookup with a cost of $t_{E}$ time units, which is described as an $O(1)$ operation. Following this, the storage layer reads the target data block, which incurs a cost of $t_{B}$ time units. These two operations are sequential. Therefore, the total time for an access in the extent-based organization, which we denote as $T_{\\text{extent}}$, is the sum of these costs:\n$$T_{\\text{extent}} = t_{E} + t_{B}$$\nThis time is deterministic.\n\nNext, we analyze the expected total time for an access in the FAT organization. The access to a logical block with index $b \\in \\mathbb{Z}_{\\ge 0}$ involves two main steps: first, the mapping layer must translate the logical block index to a physical disk location by traversing the FAT chain, and second, the storage layer reads the actual data block.\n\nThe logical blocks of a file are grouped into clusters, where each cluster contains $C$ logical blocks. The blocks are indexed starting from $0$. Thus, cluster $0$ of the file contains logical blocks $0, 1, \\dots, C-1$. Cluster $1$ contains logical blocks $C, C+1, \\dots, 2C-1$. In general, file-relative cluster $k$ contains logical blocks with indices from $kC$ to $(k+1)C-1$. To find the file-relative cluster index $k$ that contains the logical block $b$, we must find the integer $k$ such that $kC \\le b < (k+1)C$. Dividing by $C$ gives $k \\le \\frac{b}{C} < k+1$. This implies that $k$ is the integer part of $\\frac{b}{C}$, which is given by the floor function:\n$$k = \\left\\lfloor \\frac{b}{C} \\right\\rfloor$$\nIn the FAT organization, a file is a singly linked list of clusters. The directory entry contains the address of the first cluster (cluster $0$ in the file's sequence), which is assumed to be known in memory at zero cost. To find the location of the target cluster at index $k$, the file system must traverse the linked list starting from the first cluster. This requires $k$ sequential lookups in the FAT. For instance, to find the second cluster (index $1$), one must read the FAT entry corresponding to the first cluster (index $0$). To find the third cluster (index $2$), one must read the FAT entry for the second cluster, and so on. Therefore, to reach the cluster at index $k = \\lfloor \\frac{b}{C} \\rfloor$, a total of $N = \\lfloor \\frac{b}{C} \\rfloor$ FAT entry accesses are necessary.\n\nLet $X_i$ be the random variable representing the cost of the $i$-th FAT access in the traversal chain, for $i \\in \\{1, 2, \\dots, N\\}$. The cost of each access depends on whether the corresponding FAT entry is in the in-memory cache.\n- The cost is $t_H$ if the entry is in the cache (a \"hit\"). This occurs with probability $p$.\n- The cost is $t_F$ if the entry is not in the cache (a \"miss\"). This occurs with probability $1-p$.\n\nThe expected cost of a single FAT access, $E[X_i]$, is the weighted average of these two outcomes:\n$$E[X_i] = p \\cdot t_{H} + (1-p) \\cdot t_{F}$$\nThe problem states that the cache hit/miss events are independent for each required FAT entry. Thus, the expected cost $E[X_i]$ is the same for all $i=1, \\dots, N$. Let us denote this common expected cost by $E[X_{\\text{access}}]$.\n\nThe total time for the mapping layer traversal is the sum of the costs of the $N$ individual accesses, $T_{\\text{mapping-FAT}} = \\sum_{i=1}^{N} X_i$. By the linearity of expectation, the expected total mapping time is the sum of the expected costs of each access:\n$$E[T_{\\text{mapping-FAT}}] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i]$$\nSince $E[X_i]$ is constant for all $i$ and $N = \\lfloor \\frac{b}{C} \\rfloor$, this simplifies to:\n$$E[T_{\\text{mapping-FAT}}] = N \\cdot E[X_{\\text{access}}] = \\left\\lfloor \\frac{b}{C} \\right\\rfloor (p t_{H} + (1-p) t_{F})$$\nAfter the mapping layer determines the physical location of the data block, the storage layer performs the read operation, which has a constant cost of $t_B$. The total expected time for the FAT-based access, $E[T_{\\text{FAT}}(b)]$, is the sum of the expected mapping time and the block read time:\n$$E[T_{\\text{FAT}}(b)] = E[T_{\\text{mapping-FAT}}] + t_{B} = \\left\\lfloor \\frac{b}{C} \\right\\rfloor (p t_{H} + (1-p) t_{F}) + t_{B}$$\n\nFinally, we can compute the ratio $R(b)$ of the expected total time for the FAT organization to the total time for the extent-based organization:\n$$R(b) = \\frac{E[T_{\\text{FAT}}(b)]}{T_{\\text{extent}}}$$\nSubstituting the derived expressions for the numerator and the denominator, we obtain the final closed-form expression:\n$$R(b) = \\frac{\\left\\lfloor \\frac{b}{C} \\right\\rfloor (p t_{H} + (1-p) t_{F}) + t_{B}}{t_{E} + t_{B}}$$\nThis expression is simplified and contains only the variables specified in the problem statement.",
            "answer": "$$\\boxed{\\frac{\\left\\lfloor \\frac{b}{C} \\right\\rfloor \\left(p t_{H} + (1-p) t_{F}\\right) + t_{B}}{t_{E} + t_{B}}}$$"
        },
        {
            "introduction": "Efficiently translating a human-readable filename into a file's metadata is a crucial task of the directory layer. This final practice delves into a modern, high-performance directory indexing structure, the Hashed Tree (HTREE), used in file systems like ext4. By calculating the expected lookup time under a realistic, non-uniform workload, you will analyze how file systems contend with hash collisions to maintain fast lookups even in directories containing millions of files. ",
            "id": "3642793",
            "problem": "An operating system directory is indexed by a Hashed Tree (HTREE), a hash-based directory indexing structure used in modern file systems such as the Fourth Extended Filesystem (EXT4). The HTREE uses a fixed node degree at each index level, with leaves corresponding to hash buckets. Collisions within a leaf bucket are handled by chaining entries in a singly linked list. A successful lookup proceeds by computing a hash of the target name, descending the HTREE via binary search at each index node to select the appropriate child, and then linearly scanning the collided chain in the leaf bucket until the target entry is found. Assume that a “step” is counted as a single key comparison-equivalent operation, and count the hash computation as one step.\n\nThe directory contains $n = 1{,}966{,}080$ entries. The HTREE has $2$ index levels, each with node degree $C = 64$, so the number of leaf buckets is $m = C^{2} = 4{,}096$. The hash distribution is skewed as follows: a set of $h = 256$ “hot” buckets receive a fraction $f = \\frac{1}{2}$ of all entries, and the remaining $m - h = 3{,}840$ “cold” buckets receive the remaining fraction $1 - f = \\frac{1}{2}$. Within the hot subset and within the cold subset, entries are evenly distributed among the buckets in that subset. The singly linked chains in buckets are unsorted, and within any given bucket the target entry’s position is equally likely to be any integer from $1$ to the chain length of that bucket. The search algorithm uses binary search among the $C$ child pivots at each index node.\n\nStarting from core definitions of expected value in probability theory and the comparison complexity of binary search, derive the exact expected total number of steps required for a successful lookup of a uniformly random existing entry under these assumptions. Express your final answer as an exact real number. Do not round.",
            "solution": "The problem asks for the expected total number of steps for a successful lookup of a uniformly random existing entry in the HTREE structure. Let $E[\\text{Total}]$ be this value. The lookup process can be broken down into three sequential stages: hash computation, tree traversal, and leaf bucket scanning. By the linearity of expectation, the total expected number of steps is the sum of the expected steps for each stage:\n$$E[\\text{Total}] = E[\\text{Hash}] + E[\\text{Traversal}] + E[\\text{Scan}]$$\n\n1.  **Expected Hash Computation Steps ($E[\\text{Hash}]$)**: The problem states that the hash computation counts as a single step.\n    $$E[\\text{Hash}] = 1$$\n\n2.  **Expected Tree Traversal Steps ($E[\\text{Traversal}]$)**: The lookup descends through 2 index levels. At each level, a binary search is performed on the $C=64$ child pivots. A binary search to find which of $C$ intervals a value falls into (by searching on $C-1$ pivots) takes $\\log_2(C)$ comparisons, since $C=64=2^6$ is a power of two, making the comparison tree perfectly balanced.\n    - Steps per level = $\\log_2(C) = \\log_2(64) = 6$ steps.\n    - Total traversal steps for 2 levels = $2 \\times 6 = 12$ steps.\n    $$E[\\text{Traversal}] = 12$$\n\n3.  **Expected Leaf Bucket Scan Steps ($E[\\text{Scan}]$)**: This depends on the length of the collision chain in the target bucket. For a successful search in an unsorted chain of length $k$, where the target entry's position is uniformly random from 1 to $k$, the expected number of comparisons is $E[\\text{Scan}|k] = \\frac{k+1}{2}$.\n\n    We first determine the chain lengths for the \"hot\" and \"cold\" buckets.\n    - Total entries $n = 1,966,080$.\n    - Number of entries in hot buckets: $n_H = n \\cdot f = 1,966,080 \\cdot \\frac{1}{2} = 983,040$.\n    - Number of entries in cold buckets: $n_C = n \\cdot (1-f) = 983,040$.\n    - Number of hot buckets $h = 256$. Number of cold buckets $m-h = 3,840$.\n    - Chain length per hot bucket: $k_H = \\frac{n_H}{h} = \\frac{983,040}{256} = 3,840$.\n    - Chain length per cold bucket: $k_C = \\frac{n_C}{m-h} = \\frac{983,040}{3,840} = 256$.\n\n    The expected scan time for an entry in a hot bucket is $E[\\text{Scan}|k_H] = \\frac{3,840+1}{2} = 1,920.5$.\n    The expected scan time for an entry in a cold bucket is $E[\\text{Scan}|k_C] = \\frac{256+1}{2} = 128.5$.\n\n    We are finding the expectation for a uniformly random *entry*. The probability that such an entry resides in a hot bucket is $f = \\frac{1}{2}$, and the probability it resides in a cold bucket is $1-f = \\frac{1}{2}$. Using the Law of Total Expectation:\n    $$E[\\text{Scan}] = E[\\text{Scan}|\\text{Hot}] \\cdot P(\\text{Hot}) + E[\\text{Scan}|\\text{Cold}] \\cdot P(\\text{Cold})$$\n    $$E[\\text{Scan}] = (1,920.5) \\cdot \\left(\\frac{1}{2}\\right) + (128.5) \\cdot \\left(\\frac{1}{2}\\right) = \\frac{1,920.5 + 128.5}{2} = \\frac{2,049}{2} = 1,024.5$$\n\n4.  **Total Expected Steps**: We sum the expected values for each stage.\n    $$E[\\text{Total}] = 1 + 12 + 1,024.5 = 1,037.5$$\n\nThe exact expected total number of steps is 1037.5.",
            "answer": "$$ \\boxed{1037.5} $$"
        }
    ]
}