## 引言
文件系统是[操作系统](@entry_id:752937)中负责管理和组织持久化数据的核心组件，其设计优劣直接决定了整个系统的性能、可靠性与安全性。然而，对于许多开发者和系统管理员而言，文件系统往往像一个深不可测的黑盒。这种知识上的隔阂，使得在面对性能瓶颈、[数据一致性](@entry_id:748190)挑战或安全漏洞时，常常难以从根本上分析和解决问题。

本文旨在系统性地揭开[文件系统](@entry_id:749324)的神秘面纱，为你构建一个从底层原理到上层应用的完整知识框架。我们将分三部分展开：

在 **“原理与机制”** 一章中，我们将深入文件系统的内部，剖析其精巧的分层架构，理解虚拟[文件系统](@entry_id:749324)（VFS）如何统一管理不同的文件系统实现。你将学到像索引节点（[inode](@entry_id:750667)）和目录项（dentry）这样的核心[数据结构](@entry_id:262134)是如何将文件名与磁盘上的数据关联起来的，以及从[多级索引](@entry_id:752249)到区段（Extent）的分配策略演进。此外，我们还将探讨页面缓存如何提升I/O性能，以及日志技术如何保证系统在崩溃后的[数据一致性](@entry_id:748190)。

随后，在 **“应用与跨学科连接”** 一章，我们将把这些理论知识置于真实世界的场景中。你将看到文件系统的[原子操作](@entry_id:746564)、快照和加密功能如何保障数据库和关键应用的[数据完整性](@entry_id:167528)与安全。我们还将探索[文件系统](@entry_id:749324)在容器化技术（如OverlayFS）和[虚拟化](@entry_id:756508)中的核心作用，并理解一个写请求是如何穿越从应用层到物理磁盘的整个复杂存储堆栈。

最后，**“动手实践”** 部分提供了一系列精心设计的问题，旨在挑战和巩固你对前两章知识的理解。通过对具体场景的量化分析，你将把抽象的理论转化为解决实际问题的能力。

通过本次学习，你将不再仅仅是[文件系统](@entry_id:749324)的使用者，而是能够洞察其内部机制、进行[性能调优](@entry_id:753343)、并利用其强大功能构建更高效、更可靠软件系统的专业人士。

## 原理与机制

[文件系统](@entry_id:749324)是[操作系统](@entry_id:752937)中最核心、最复杂的组件之一。它不仅负责数据的持久化存储，还提供了一套丰富的抽象，使用户和应用程序能够以结构化、有组织的方式访问和管理数据。本章将深入探讨[文件系统](@entry_id:749324)实现的关键原理与核心机制，从其分层架构和基本数据结构入手，逐步剖析文件在磁盘上的物理布局、[性能优化](@entry_id:753341)的[缓存策略](@entry_id:747066)，以及确保[数据完整性](@entry_id:167528)的可靠性技术。

### [文件系统](@entry_id:749324)抽象：一个分层的视角

现代[操作系统](@entry_id:752937)中的[文件系统](@entry_id:749324)实现普遍采用分层架构，这种设计将复杂的I/O任务分解为一系列定义明确、功能独立的模块。一个典型的I/O请求，例如应用程序调用 `read()`，会自上而下地穿过这个层次结构。

这个[分层模型](@entry_id:274952)通常包括：
1.  **应用程序（Application）**：通过标准API（如POSIX）发起文件操作请求。
2.  **虚拟文件系统（Virtual File System, VFS）**：这是内核中的一个关键抽象层。它为[上层](@entry_id:198114)应用提供了统一的[文件系统](@entry_id:749324)接口（如 `open`, `read`, `write` 等），同时对下层多种不同类型的具体[文件系统](@entry_id:749324)进行管理和适配。VFS使得Linux可以同时支持ext4、XFS、NTFS等多种[文件系统](@entry_id:749324)。
3.  **具体文件系统（Concrete Filesystem）**：这一层是特定文件系统（如ext4）的实现。它负责解释VF[S层](@entry_id:171381)传来的通用指令，并将其转换为对自身[数据结构](@entry_id:262134)（如inode、目录、数据块）的操作。
4.  **页面缓存（Page Cache）**：这是一个位于内存中的高速缓存，用于存储最近访问过的文件数据。它是I/O[性能优化](@entry_id:753341)的核心。
5.  **块设备层（Block Device Layer）**：负责管理对块设备（如硬盘、SSD）的I/O请求。它包含I/O调度器，可以对请求进行合并与排序，以优化磁盘访问效率。
6.  **[设备驱动程序](@entry_id:748349)（Device Driver）**：这是与特定硬件交互的最底层软件，它将块设备层的逻辑请求转换为硬件能够理解的命令。

这种分层设计的好处是显而易见的：它实现了关注点分离。文件系统开发者可以专注于逻辑[数据结构](@entry_id:262134)的管理，而不必关心底层硬件的细节；而设备驱动开发者则可以专注于硬件控制，无需理解[文件系统](@entry_id:749324)的复杂语义。

以一个顺序读取大文件的场景为例，我们可以清晰地看到各层的作用与性能瓶颈的转移 。当应用程序首次（冷缓存）读取文件时，请求会穿透整个栈：VFS分派请求，文件系统层将文件偏移量转换为逻辑块号，页面缓存未命中，于是向块设备层发起读请求。此时，系统的性能瓶颈在于物理存储设备的顺序读取带宽。块设备层通过合并请求、驱动程序通过高效的DMA（直接内存访问）来尽可能地喂饱设备。然而，当应用程序立即再次读取同一文件时（热缓存），情况截然不同。由于文件内容已经被加载到页面缓存中，`read()` 请求在VF[S层](@entry_id:171381)之后，可以直接从页面缓存中获得数据，并将其复制到用户空间。整个块设备层和驱动层都被绕过。此时，性能瓶颈从I/O设备转移到了CPU和内存总线，主要开销在于内核将数据从页面缓存复制到应用程序缓冲区。

### 核心[数据结构](@entry_id:262134)：从名字到数据

初学者常常将文件视为一个整体，但从文件系统的角度看，一个文件实际上由两个核心部分组成：它的**名称（name）**和它的**内容（content）**，而这两者是通过一系列精心设计的数据结构关联起来的。

#### [inode](@entry_id:750667)：文件的元数据与存在之锚

**[inode](@entry_id:750667)（索引节点）** 是文件系统中最核心的数据结构。每个文件或目录都由一个唯一的inode来表示。它存储了关于文件的所有**元数据（metadata）**，包括：文件大小、所有者（用户ID和组ID）、权限模式、时间戳（创建、修改、最后访问时间），以及最重要的——指向文件数据所在物理磁盘块的指针。[inode](@entry_id:750667)是文件存在的根本，只要inode存在，文件就存在，即使它没有任何名称。

一个关键的元数据字段是**链接计数（link count）**。这个计数器记录了有多少个目录项（即文件名）指向这个inode。每当创建一个指向该文件的新**硬链接（hard link）**时，链接计数加一；每当删除一个文件名（`unlink`）时，链接计数减一。

#### 目录项（dentry）：名称与inode的绑定

如果说[inode](@entry_id:750667)是文件的“实体”，那么**目录项（directory entry, dentry）**就是文件的“名片”。目录本身也是一种特殊类型的文件，其内容就是一张列表，列表中的每一项都是一个目录项。一个目录项包含两部分信息：一个文件名（如 "myfile.txt"）和与之对应的[inode](@entry_id:750667)号。当我们通过路径 `/home/user/myfile.txt` 访问文件时，[文件系统](@entry_id:749324)会依次查找根目录 `/`、`home` 目录、`user` 目录，在 `user` 目录的内容中找到名为 `myfile.txt` 的目录项，并从中获取其inode号，最终定位到文件的[inode](@entry_id:750667)。

为了具体理解目录的物理结构，我们可以考察一个类似EXT2[文件系统](@entry_id:749324)的简化模型 。在这种模型中，一个目录被存储在一个或多个固定大小的块中。每个块内紧密[排列](@entry_id:136432)着若干个可变长度的目录项。每个目录项由一个头部和一个文件名组成。头部包含了诸如inode号（4字节）、记录长度（2字节）、名称长度（1字节）和文件类型（1字节）等信息。记录长度 `r` 必须是4的倍数，并且足够容纳头部和文件名。一个重要的设计是，块内最后一个目录项的记录长度会延伸至块的末尾，这样遍历目录时就可以通过简单的指针偏移从一个记录跳到下一个记录，直到块的边界。一个新创建的目录必须至少包含两个特殊的目录项：`"."`（指向当前目录自身的inode）和 `".."`（指向父目录的[inode](@entry_id:750667)）。

#### 打开文件对象与“打开-删除”语义

当一个进程成功`open()`一个文件时，内核会在内存中创建一个**打开文件对象（open file object）**，这个对象包含了文件的访问模式、当前的读写偏移量等信息，并且它持有一个指向该文件inode的引用。进程得到的是一个**文件描述符（file descriptor）**，它是在该进程内部指向这个打开文件对象的索引。

这种设计导致了一个非常重要且强大的语义，即文件的生命周期由两种引用共同决定：磁盘上的链接计数和内存中的打开文件引用计数。一个文件只有在**链接计数为零**且**打开文件引用计数也为零**时，其[inode](@entry_id:750667)和相关[数据块](@entry_id:748187)才会被系统回收。

这个机制催生了经典的“打开即删除”（open-unlink）编程模式，这对于创建安全的临时文件至关重要。让我们通过一个场景来深入理解VFS各对象之间的交互 ：

1.  **时刻 $t_0$**：进程 $P_1$ 打开文件 `/tmp/x`。此时，`/tmp/x` 对应的[inode](@entry_id:750667)（我们称之为 $inode_A$）的链接计数为1，同时由于 $P_1$ 的打开操作，其内存中的打开文件引用计数也变为1。
2.  **时刻 $t_1$**：另一个进程 $P_2$ 执行 `unlink("/tmp/x")`。这个操作会移除 `/tmp` 目录中名为 `x` 的目录项，并使 $inode_A$ 的链接计数减为0。
3.  **状态分析**：此时，$inode_A$ 的链接计数为0，但其打开文件引用计数仍为1（因为 $P_1$ 还持有着文件描述符）。因此，系统不会回收 $inode_A$ 及其[数据块](@entry_id:748187)。这个文件变成了一个“匿名文件”：它在[文件系统](@entry_id:749324)的命名空间中不再可见（任何通过路径 `/tmp/x` 的查找都会失败），但它依然真实存在，并且可以被 $P_1$ 通过其文件描述符继续访问。
4.  **时刻 $t_1$ 到 $t_2$**：$P_1$ 通过其文件描述符向文件写入数据。这些写操作会成功，数据被写入与 $inode_A$关联的页面缓存中，并最终可以通过 `[fsync](@entry_id:749614)` 同步到磁盘。
5.  **时刻 $t_3$**：第三个进程 $P_3$ 创建了一个同名新文件 `/tmp/x`。这个操作会分配一个全新的inode（$inode_B$），并创建一个新的目录项 `x` 指向 $inode_B$。这与 $P_1$ 正在访问的匿名文件 $inode_A$ 毫无关系。
6.  **时刻 $t_4$**：$P_1$ 关闭其文件描述符。内核将 $inode_A$ 的打开文件引用计数减为0。此时，内核检查发现 $inode_A$ 的链接计数和打开文件引用计数都为0，回收条件满足。于是，内核开始回收 $inode_A$ 及其占用的所有[数据块](@entry_id:748187)。

这个例子完美地展示了文件名（dentry）与文件实体（[inode](@entry_id:750667)）的分离，这是理解Unix-like文件系统行为的关键。

### 虚拟[文件系统](@entry_id:749324)（VFS）：统一的抽象接口

VF[S层](@entry_id:171381)不仅提供了统一的API，还定义和实施了一系列跨[文件系统](@entry_id:749324)类型的通用规则和约束。

#### `rename` 系统调用的[原子性](@entry_id:746561)

`rename(S, D)` 操作的语义是将路径为 $S$ 的文件移动或重命名为路径 $D$。POSIX标准要求，当 $S$ 和 $D$ 在同一个[文件系统](@entry_id:749324)上时，这个操作必须是**原子的（atomic）**。这意味着操作要么完全成功，要么完全不发生，系统状态不会停留在任何中间状态（例如，一个文件同时有两个名字，或暂时没有名字）。

这种[原子性](@entry_id:746561)之所以能够实现，是因为在单个[文件系统](@entry_id:749324)内部，`rename` 只是一个纯粹的元数据操作：它原子地从源目录中移除一个目录项，并在目标目录中添加一个指向相同inode的新目录项。文件的庞大数据内容完全不需要移动。

然而，当 $S$ 和 $D$ 位于不同的[文件系统](@entry_id:749324)时（例如，从 `/home` 移动到 `/mnt/usb`），情况就完全不同了 。由于inode号只在各自的[文件系统](@entry_id:749324)内唯一且有意义，我们无法简单地在目标文件系统中创建一个指向源[文件系统](@entry_id:749324)[inode](@entry_id:750667)的目录项。一个真正的跨文件系统移动必须涉及数据的物理复制：在目标[文件系统](@entry_id:749324)上创建一个全新的[inode](@entry_id:750667)和文件副本，然后删除源文件。这个“复制-然后-删除”的过程漫长且复杂，VFS无法保证其[原子性](@entry_id:746561)（例如，在复制完成但删除未开始时发生系统崩溃）。因此，在这种情况下，`rename` [系统调用](@entry_id:755772)会直接失败，并返回错误码 `EXDEV`（跨设备链接）。

此时，实现移动的责任就落在了用户空间的程序（如 `mv` 命令）身上。它会捕获 `EXDEV` 错误，然后执行一个非原子的回退策略：首先将源文件复制到目标位置，然后（在确认复制成功后）删除源文件。

#### 链接的限制

VFS还对链接操作施加了重要限制，其根本原因同样在于维护[文件系统结构](@entry_id:749349)的完整性 ：
*   **不允许对目录创建硬链接**：除了系统自动创建的 `.` 和 `..` 之外，用户不能为目录创建新的硬链接。这是为了保证[文件系统](@entry_id:749324)的[目录结构](@entry_id:748458)是一个**[有向无环图](@entry_id:164045)（DAG）**。如果允许任意创建目录的硬链接，就可能引入循环（例如，将一个目录链接到其子目录中），这将导致许多标准的目录遍历工具（如 `find`, `du`）陷入无限循环。
*   **不允许跨文件系统创建硬链接**：如前所述，硬链接本质上是同一个inode的多个名字。由于inode的作用域被限制在单个[文件系统](@entry_id:749324)内，因此硬链接自然也无法跨越[文件系统](@entry_id:749324)的边界。

与硬链接不同，**[符号链接](@entry_id:755709)（symbolic link）**则没有这些限制。[符号链接](@entry_id:755709)本身是一个特殊类型的文件，其数据内容就是要链接到的目标的路径字符串。VFS在解析[符号链接](@entry_id:755709)时，只是简单地读取这个路径字符串，然后从头开始解析新路径。因此，[符号链接](@entry_id:755709)可以指向目录，也可以跨越文件系统，甚至可以指向一个不存在的路径（成为一个“悬空链接”）。

### 磁盘数据布局：文件分配策略

文件系统面临的一个核心挑战是：如何将用户视角下的、连续的逻辑文件块序列，映射到磁盘上不一定连续的物理块地址。不同的分配策略在空间利用率、顺序访问性能和随机访问性能之间做出了不同的权衡。

#### 经典策略：连续、链接与索引

-   **[连续分配](@entry_id:747800)（Contiguous Allocation）**：将一个文件的所有[数据块](@entry_id:748187)存放在磁盘上连续的物理块中。这种方法的优点是顺序读写性能极佳，因为磁盘磁头可以一次性读取大量数据而无需寻道。但其缺点也同样致命：存在严重的**[外部碎片](@entry_id:634663)**问题，并且文件增长困难。
-   **[链接分配](@entry_id:751340)（Linked Allocation）**：将每个[数据块](@entry_id:748187)的地址存储在前一个[数据块](@entry_id:748187)的末尾，形成一个[链表](@entry_id:635687)。这种方法消除了[外部碎片](@entry_id:634663)，文件可以方便地增长。但它的随机访问性能极差，要访问第 $N$ 个块，必须从头遍历前 $N-1$ 个块。
-   **[索引分配](@entry_id:750607)（Indexed Allocation）**：为每个文件分配一个**索引块（index block）**，该块不存储文件数据，而是专门存储指向文件所有[数据块](@entry_id:748187)的指针列表。这种方法支持高效的随机访问（只需读取索引块和目标数据块），也易于文件增长。它的主要开销是索引块本身占用的空间。

一个简单的性能模型可以很好地揭示这些策略的差异 。假设我们在一个拥有 $M$ 个柱面的磁盘上，以步长 $s$ 访问一个文件。对于**[连续分配](@entry_id:747800)**，如果步长 $s$ 小于一个柱面包含的块数 $C$，那么每次访问跳跃 $s$ 个块，发生跨柱面寻道的概率大约是 $\frac{s}{C}$。而对于**链接或[索引分配](@entry_id:750607)**，如果[数据块](@entry_id:748187)是随机[分布](@entry_id:182848)在磁盘上的，那么任意两次连续的访问都几乎肯定会访问不同柱面上的块，寻道概率接近 $1 - \frac{1}{M}$。这表明，对于具有[空间局部性](@entry_id:637083)的访问模式，[连续分配](@entry_id:747800)的性能远优于随机放置的策略。

#### 现代策略：从[多级索引](@entry_id:752249)到Extent

为了支持大文件，简单的[索引分配](@entry_id:750607)策略演变成了**[多级索引](@entry_id:752249)分配（Multi-Level Indexed Allocation）**，这是经典Unix文件系统（如UFS, ext2）的标志。在这种方案中，inode本身包含少量（例如12个）直接指向数据块的指针，以及一个指向**单级间接块**的指针，一个指向**双级间接块**的指针，和一个指向**三级间接块**的指针。间接块里存储的不是数据，而是下一级块的地址。这种结构像一棵树，可以以很小的[inode](@entry_id:750667)空间支持非常巨大的文件。

然而，对于大文件，特别是连续存放的大文件，[多级索引](@entry_id:752249)的效率并不高。为了访问一个[数据块](@entry_id:748187)，可能需要多次读取间接块，增加了元数据I/O的开销。现代[文件系统](@entry_id:749324)（如ext4, XFS）普遍采用了一种更高效的策略：**基于Extent的分配（Extent-Based Allocation）**。

**Extent** 是一个连续的物理块区间，可以用一个简单的元组 `(起始块号, 块长度)` 来描述。文件系统不再为每个[数据块](@entry_id:748187)存储一个指针，而是存储一系列的extent。例如，一个 $100\,\mathrm{MB}$ 的连续文件可能只需要一个extent来描述，而不是 $25600$ 个单独的块指针。这些extent通常被组织在一棵平衡的、类似[B+树](@entry_id:636070)的**Extent树**中，其根节点存储在[inode](@entry_id:750667)里。

让我们通过一个计算实例来比较这两种策略的随机访问性能 。假设一个 $10^6$ 个块的文件，由于碎片化，它由 $10^4$ 个extent组成。
-   在**Extent树**方案中，假设一个树节点可以容纳 $256$ 个子节点指针或叶子节点的extent记录。要索引 $10^4$ 个extent，需要大约 $40$ 个叶子节点。这 $40$ 个叶子节点可以用一个单一的根节点来索引。因此，[树的高度](@entry_id:264337)为2。在冷缓存下，任何一次随机访问都需要从根节点遍历到叶子节点，总共需要2次源数据I/O。
-   在**[多级索引](@entry_id:752249)**方案中，假设一个间接块可以存储 $512$ 个指针。对于一个 $10^6$ 个块的文件，大部分块（约73%）都需要通过三级间接指针来访问。这意味着一次随机访问平均需要大约 $2.74$ 次源数据I/O。

在这个例子中，Extent树方案的[元数据](@entry_id:275500)开销比[多级索引](@entry_id:752249)方案低了约27%。这清晰地表明，对于大文件，特别是那些具有良好空间连续性的文件，Extent是一种远比传统间接块指针更高效的表示方法。

### 性能与缓存

缓存是[文件系统](@entry_id:749324)性能的生命线。[操作系统](@entry_id:752937)通过在内存中保留最近访问过的数据副本，来避免昂贵的磁盘I/O操作。

#### 页面缓存：统一文件与内存缓存

早期的[操作系统](@entry_id:752937)中存在一个经典问题：系统维护了两个独立的缓存。一个是**[缓冲区缓存](@entry_id:747008)（Buffer Cache）**，用于缓存原始块设备I/O（以文件系统的块大小 $B$ 为单位）；另一个是**页面缓存（Page Cache）**，用于[虚拟内存管理](@entry_id:756522)和[内存映射](@entry_id:175224)文件（以CPU的页面大小 $P$ 为单位）。

当一个文件同时通过 `read/write` 系统调用和[内存映射](@entry_id:175224)（`mmap`）被访问时，如果 $B \neq P$，就可能导致**双重缓存（double buffering）**的问题 。同一份磁盘数据可能会在内存中存在两份拷贝：一份在[缓冲区缓存](@entry_id:747008)中，一份在页面缓存中。这不仅浪费了宝贵的物理内存，更带来了[缓存一致性](@entry_id:747053)的难题：如果通过 `write()` 修改了[缓冲区缓存](@entry_id:747008)中的数据，如何确保[内存映射](@entry_id:175224)区域能看到最新的内容？

现代Unix-like[操作系统](@entry_id:752937)的解决方案是**统一缓存（Unified Cache）**。它们将**页面缓存**作为唯一的、统一的[数据缓存](@entry_id:748188)。无论是标准的文件I/O（`read/write`）还是[内存映射](@entry_id:175224)I/O，都通过页面缓存进行。当进行`read/write`时，数据从磁盘读入页面缓存，然后再从页面缓存复制到用户提供的缓冲区。[缓冲区缓存](@entry_id:747008)的角色被大大削弱，主要只用于缓存那些不与任何文件关联的[元数据](@entry_id:275500)块（如超级块），或者在逻辑上成为页面缓存的一部分，负责将块号映射到页面缓存中的物理页。

这种统一架构的优势可以通过一个思想实验来体现 。设想一个系统，文件I/O通过页面缓存（有预读机制），而原始块设备I/O通过[缓冲区缓存](@entry_id:747008)（无预读机制）。
-   当**顺序**扫描一个大文件时，文件I/O的命中率会非常高（例如，预读16页，则每17次访问有16次命中，命中率约为94%），因为预读机制有效地将未来的请求提前载入缓存。而对该文件对应的块设备进行顺序扫描，由于没有预读，每次访问都是缓存未命中，命中率为0%。
-   当**随机**访问一个大文件（文件大小是缓存的8倍）时，无论通过文件I/O还是块设备I/O，命中率都约等于缓存大小与文件大小之比（$\frac{1}{8} = 12.5\%$），因为访问是随机的，预读不起作用。

这个对比鲜明地展示了**预读（read-ahead）**机制对于顺序访问性能的巨大提升，以及一个集成了智能策略（如预读）的统一页面缓存的重要性。

### 可靠性：[崩溃一致性](@entry_id:748042)与日志

[文件系统](@entry_id:749324)必须保证在系统意外崩溃（如断电）后，其内部结构依然是一致的。一个简单的文件创建操作可能涉及多个独立的磁盘写操作：修改目录的数据块、更新目录的inode、分配新文件的inode、更新inode[位图](@entry_id:746847)、更新[数据块](@entry_id:748187)[位图](@entry_id:746847)等。如果在这些写操作之间发生崩溃，[文件系统](@entry_id:749324)可能会处于一种损坏的、不一致的状态。

现代[文件系统](@entry_id:749324)通过**日志（Journaling）**技术来解决这个问题。其核心思想是**[预写式日志](@entry_id:636758)（Write-Ahead Logging, WAL）**。

基本原理是：在对[文件系统](@entry_id:749324)的主要[数据结构](@entry_id:262134)进行任何修改之前，先将一个描述这些修改的**事务（transaction）**写入一个独立的、连续的磁盘区域，即**日志（journal）**。只有当代表整个事务的**提交记录（commit record）**被安全地写入日志后，文件系统才会开始将这些修改应用（checkpoint）到它们在磁盘上的最终位置。如果在应用过程中发生崩溃，系统重启后只需读取日志。通过重放（replay）那些已提交但可能未完全应用的事务，[文件系统](@entry_id:749324)就可以恢复到一个一致的状态。

日志虽然保证了元数据的一致性，但对于用户数据的处理，不同的日志模式在性能和安全性之间做出了不同的权衡 ：

-   **`data` 模式（数据日志）**：这是最安全的模式。它将元数据和所有被修改的用户数据都写入日志。这意味着事务的提交是完全原子的，可以保证在崩溃后，文件内容和其[元数据](@entry_id:275500)（如文件大小）是完全一致的。但它的性能开销最大，因为所有数据都被写了两次（一次到日志，一次到最终位置）。

-   **`ordered` 模式（有序模式）**：这是许多文件系统（如ext3, ext4）的默认模式，它在性能和安全之间取得了很好的平衡。该模式只将[元数据](@entry_id:275500)写入日志，但它强制规定了一个写入顺序：相关的用户数据块必须在包含其[元数据](@entry_id:275500)更新的事务提交之前，被写入到磁盘上的最终位置。这样，如果在崩溃后，一个文件创建或扩大的事务被重放，系统可以保证那些新分配的数据块已经在磁盘上包含了正确的数据，从而避免了文件出现“有大小但无内容”或包含垃圾数据的情况。

-   **`writeback` 模式（回写模式）**：这是性能最高的模式。它也只将[元数据](@entry_id:275500)写入日志，但与有序模式不同，它对用户数据和元数据事务的写入顺序没有任何保证。[元数据](@entry_id:275500)事务可能会在相关的[数据块](@entry_id:748187)写入磁盘之前就提交。如果在此时发生崩溃，重启后文件系统会重放元数据事务，文件可能会被正确创建或改变大小，但其指向的[数据块](@entry_id:748187)可能仍然包含旧的、无意义的垃圾数据。这种模式提供了最弱的一致性保证，但I/O调度器可以最大程度地自由重排写操作，从而获得最佳性能。

总结来说，`data`模式提供了最强的[数据完整性](@entry_id:167528)保证；`ordered`模式保证了元数据和数据状态的[逻辑一致性](@entry_id:637867)，是大多数场景下的合理选择；而`writeback`模式则将[数据一致性](@entry_id:748190)的责任更多地交给了应用程序（需要通过`[fsync](@entry_id:749614)`等调用来手动确保数据落盘），以换取最高的吞吐率。