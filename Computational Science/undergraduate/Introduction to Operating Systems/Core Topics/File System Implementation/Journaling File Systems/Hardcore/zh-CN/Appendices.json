{
    "hands_on_practices": [
        {
            "introduction": "本练习将引导你进行一个思想实验，以深入理解日志文件系统的核心——崩溃一致性。通过在一个仅记录元数据的日志文件系统中，追踪一系列文件操作（创建、写入、重命名）中每一步之后发生崩溃可能导致的磁盘状态，你将亲身体会到“提交记录”（commit record）和“有序写入”（ordered write）在保证原子性和数据完整性方面的关键作用。这个练习旨在将抽象的理论具象化，帮助你建立对文件系统如何在故障中自我恢复的直观认识。",
            "id": "3651370",
            "problem": "一个在有序模式下使用仅元数据日志记录的日志文件系统 (FS) 以默认的回写缓存方式挂载。目录路径 $/ \\mathrm{dir}$ 初始存在，并且不包含名为 $\\mathrm{new}$ 或 $\\mathrm{final}$ 的条目。目标名称 $\\mathrm{final}$ 在以下序列之前不存在。考虑以下操作序列：创建一个文件，写入数据，然后重命名它，在此过程中应用程序在任何时候都没有发出文件同步 (fsync) 调用。可移植操作系统接口 (POSIX) 的重命名语义保证了原子性：从崩溃中恢复后，要么旧名称存在，要么新名称存在，但不会两者都存在。日志记录层提供以下保证：事务将元数据记录在日志中；当事务的日志提交记录被持久化写入时，该事务即被视为已提交；恢复过程会将任何具有有效提交记录的事务重放到主文件系统结构中；在有序模式下，任何因操作而变为可达（通过元数据）的脏数据块，都会在相应元数据事务的提交记录被持久化写入之前，刷新到主存储中。\n\n系统执行以下步骤，用 $k$ 编号：\n- $k=1$：应用程序在 $/ \\mathrm{dir}$ 中创建文件 $\\mathrm{new}$，导致分配一个新的 inode $I$ 并插入目录条目 $\\mathrm{new} \\rightarrow I$。这是由日志子系统作为事务 $T_1$ 跟踪的元数据。\n- $k=2$：应用程序向文件中写入 $2$ 个数据块 $B_1$ 和 $B_2$。这些数据被缓存在页面缓存中并标记为脏。这是数据，不会作为元数据被记入日志。\n- $k=3$：inode $I$ 的大小被更新为 $S$ 字节以反映写入的数据。这是元数据，并且仍然是 $T_1$ 的一部分。\n- $k=4$：日志记录层写入 $T_1$ 的日志描述符，将元数据变更（例如，目录条目和 inode 的变更）暂存到日志区域，并在任何提交之前调度 $B_1$ 和 $B_2$ 的有序回写。\n- $k=5$：在 $B_1$ 和 $B_2$ 的有序数据写入被发出后，日志记录层将 $T_1$ 的提交记录写入日志。此时，如果发生崩溃，恢复过程将重放 $T_1$ 的元数据。\n- $k=6$：检查点进程将 $T_1$ 的元数据从日志复制到主文件系统结构中。然后，$T_1$ 的日志空间可能会被回收。\n- $k=7$：应用程序在 $/ \\mathrm{dir}$ 内将文件从 $\\mathrm{new}$ 重命名为 $\\mathrm{final}$。此重命名操作被记录为事务 $T_2$（仅元数据），其日志描述符和提交记录在此步骤中被写入（有序模式下不需要为 $T_2$ 进行额外的数据刷新，因为没有新的数据变得可达）。\n- $k=8$：检查点进程将 $T_2$ 的元数据从日志复制到主文件系统结构中。\n\n假设在完成步骤 $k$ 后（即在步骤 $k$ 和 $k+1$ 之间）立即发生电源故障。考虑以下可能的恢复后磁盘状态：\n- $\\mathrm{S1}$：在 $/ \\mathrm{dir}$ 中，$\\mathrm{new}$ 和 $\\mathrm{final}$ 都不存在。不存在为 $I$ 分配的元数据。数据块 $B_1$ 和 $B_2$ 可能已经物理写入，也可能没有，但它们无法通过任何文件访问（并且任何物理写入，如果存在，也是无关紧要的，因为相应的元数据没有被提交）。\n- $\\mathrm{S2}$：条目 $\\mathrm{new}$ 存在于 $/ \\mathrm{dir}$ 中，指向大小为 $S$ 的 inode $I$。数据块 $B_1$ 和 $B_2$ 在磁盘上，并可通过 $I$ 访问。条目 $\\mathrm{final}$ 不存在。\n- $\\mathrm{S3}$：条目 $\\mathrm{final}$ 存在于 $/ \\mathrm{dir}$ 中，指向大小为 $S$ 的 inode $I$。数据块 $B_1$ 和 $B_2$ 在磁盘上，并可通过 $I$ 访问。条目 $\\mathrm{new}$ 不存在。\n- $\\mathrm{S4}$：在 $/ \\mathrm{dir}$ 中，$\\mathrm{new}$ 和 $\\mathrm{final}$ 同时存在，指向同一个 inode $I$。\n\n哪个选项正确描述了每个故障点 $k$ 可能的恢复后磁盘状态集合？\n\nA. 对于 $k \\in \\{1,2,3,4\\}$，只有 $\\mathrm{S1}$；对于 $k \\in \\{5,6\\}$，只有 $\\mathrm{S2}$；对于 $k \\in \\{7,8\\}$，只有 $\\mathrm{S3}$。\n\nB. 对于 $k \\in \\{1,2,3\\}$，只有 $\\mathrm{S1}$；对于 $k=4$，可能是 $\\mathrm{S1}$ 或 $\\mathrm{S2}$；对于 $k \\in \\{5,6\\}$，只有 $\\mathrm{S2}$；对于 $k \\in \\{7,8\\}$，只有 $\\mathrm{S3}$。\n\nC. 对于 $k \\in \\{1,2,3,4\\}$，只有 $\\mathrm{S1}$；对于 $k \\in \\{5,6\\}$，只有 $\\mathrm{S2}$；对于 $k=7$，可能是 $\\mathrm{S2}$、$\\mathrm{S3}$ 或 $\\mathrm{S4}$；对于 $k=8$，只有 $\\mathrm{S3}$。\n\nD. 对于 $k \\in \\{1,2,3,4\\}$，只有 $\\mathrm{S1}$；对于 $k=5$，可能是 $\\mathrm{S1}$ 或 $\\mathrm{S2}$，取决于 $B_1$ 和 $B_2$ 是否已到达磁盘；对于 $k=6$，只有 $\\mathrm{S2}$；对于 $k \\in \\{7,8\\}$，只有 $\\mathrm{S3}$。",
            "solution": "我们从在有序模式下运行的仅元数据日志文件系统 (FS) 的核心定义和保证开始：\n\n- 日志事务将元数据变更记录到一个单独的日志区域。当一个事务的日志提交记录被持久化写入时，该事务被认为是已提交的。在崩溃恢复期间，任何具有有效提交记录的事务都会被重放到主文件系统结构中，以确保元数据的原子性。\n- 在有序模式下，任何事务使其可达的脏数据块（例如，由 inode 的大小和块映射引用的新分配的文件内容）都会在相应事务的提交记录写入之前被刷新到主存储。因此，如果元数据提交使得文件及其大小可见，那么该元数据引用的数据块在此之前已经刷新到主存储（取决于提交时硬件的持久性限制）。\n- 可移植操作系统接口 (POSIX) 的重命名语义保证了原子性：恢复后，要么旧名称存在，要么新名称存在，但不会两者同时存在。\n- 检查点操作将已提交的元数据从日志复制到主文件系统结构中；然而，仅凭提交记录的存在就足以让恢复过程使事务的元数据可见，即使在崩溃时检查点操作尚未完成。\n\n我们现在按 $k$ 分析该序列：\n\n- 对于 $k \\in \\{1,2,3\\}$：尚未写入 $T_1$ 的提交记录。尽管在 $k=4$ 时元数据可能被暂存到日志中，但在 $k=4$ 之前甚至连日志描述符都没有。因此，在 $k \\in \\{1,2,3\\}$ 之后发生崩溃不会产生任何已提交的元数据。恢复过程不会创建该文件，所以磁盘状态为 $\\mathrm{S1}$。\n\n- 对于 $k=4$：$T_1$ 的日志描述符存在，但还没有提交记录。没有提交记录的事务会被恢复过程忽略。主区域的元数据更新尚未进行检查点操作，日志中未提交的元数据也不会被重放。因此，在 $k=4$ 时崩溃后，系统将处于 $\\mathrm{S1}$ 状态。\n\n- 对于 $k=5$：$T_1$ 的提交记录已被持久化写入。根据有序模式的规定，$T_1$ 使其可达的数据块 $B_1$ 和 $B_2$ 在提交记录写入之前已被刷新。因此，恢复后，文件以 $\\mathrm{new}$ 的形式存在，其 inode 为 $I$，大小为 $S$，并且 $B_1,B_2$ 在磁盘上且可达。磁盘状态为 $\\mathrm{S2}$。\n\n- 对于 $k=6$：将 $T_1$ 的元数据进行检查点操作写入主区域，与通过已提交但未检查点的事务进行恢复相比，并不会改变外部可见的状态。在 $k=6$ 时崩溃后，恢复过程会发现 $T_1$ 已提交，并在需要时重放它；磁盘状态仍然是 $\\mathrm{S2}$。\n\n- 对于 $k=7$：重命名操作被记录为 $T_2$。在有序模式下，$T_2$ 不需要额外的数据刷新，因为重命名只影响元数据（目录条目和可能的链接计数）。$T_2$ 的提交记录在 $k=7$ 时写入，这确保了恢复时的原子重命名语义。崩溃后，只有新名称存在，旧名称不存在。因此，磁盘状态为 $\\mathrm{S3}$。\n\n- 对于 $k=8$：对 $T_2$ 进行检查点操作不会改变外部可见的状态，其结果与通过已提交的 $T_2$ 进行恢复所能提供的状态相同。崩溃后，恢复过程将产生 $\\mathrm{S3}$ 状态。\n\n根据此分析，正确的描述是：\n- $k \\in \\{1,2,3,4\\} \\Rightarrow \\mathrm{S1}$,\n- $k \\in \\{5,6\\} \\Rightarrow \\mathrm{S2}$,\n- $k \\in \\{7,8\\} \\Rightarrow \\mathrm{S3}$.\n\n我们现在评估各个选项：\n\n选项 A：对于 $k \\in \\{1,2,3,4\\}$，状态被指定为 $\\mathrm{S1}$；对于 $k \\in \\{5,6\\}$，为 $\\mathrm{S2}$；对于 $k \\in \\{7,8\\}$，为 $\\mathrm{S3}$。这与我们的推导相符。结论 — 正确。\n\n选项 B：声称在 $k=4$ 时，可能出现 $\\mathrm{S1}$ 或 $\\mathrm{S2}$。这与日志系统要求恢复需要提交记录的原则相矛盾；仅有日志描述符（没有提交记录）时，恢复过程会忽略该事务，因此在 $k=4$ 时不可能出现 $\\mathrm{S2}$。结论 — 错误。\n\n选项 C：声称在 $k=7$ 时，可能出现 $\\mathrm{S2}$、$\\mathrm{S3}$ 或 $\\mathrm{S4}$。实际上，由于 $T_2$ 的提交记录在 $k=7$ 时已写入，恢复过程必须产生重命名后的状态 $\\mathrm{S3}$；只有在没有 $T_2$ 提交的情况下才会出现 $\\mathrm{S2}$。此外，$\\mathrm{S4}$ 违反了 POSIX 的原子重命名语义和日志系统的元数据原子性。结论 — 错误。\n\n选项 D：声称在 $k=5$ 时，可能出现 $\\mathrm{S1}$ 或 $\\mathrm{S2}$，这取决于 $B_1$ 和 $B_2$ 是否已到达磁盘。在有序模式下，$T_1$ 的提交记录只有在 $T_1$ 使其可达的数据块被刷新后才会被写入；因此，如果提交记录存在，恢复过程将产生 $\\mathrm{S2}$，而不是 $\\mathrm{S1}$。已提交元数据的存在保证了文件的存在，而有序回写确保了数据在提交前已到达磁盘。结论 — 错误。\n\n因此，正确选项是 A。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "在理解了日志系统如何保证一致性之后，一个自然的问题是：这种可靠性需要付出多大的性能代价？本练习通过引入“写放大”（Write Amplification, $WA$）这一关键性能指标，让你能够量化评估不同日志模式的开销。你将推导出“数据日志”（data journaling）和“元数据日志”（metadata-only journaling）两种模式下的写放大公式，从而清晰地看到更强的数据一致性保证与更高写入成本之间的权衡关系。",
            "id": "3651432",
            "problem": "考虑一个执行事务以提供原子更新的日志文件系统。每个事务恰好由应用程序逻辑上更新的 $n$ 个应用程序数据块和 $m$ 个文件系统元数据块组成。所有块的大小相等，任何一个块的物理写入都计为写入总数的一个单位。该系统可以工作在以下两种模式之一：\n\n- 数据日志模式：应用程序数据和元数据更新都首先被写入日志，然后传播到它们的原始位置。\n- 仅元数据日志模式：只有元数据更新被写入日志；应用程序数据以保持一致性的顺序直接写入其原始位置。\n\n对于这两种模式，假设以下操作细节：\n- 每个事务都会向日志追加一个大小为 $1$ 个块的日志提交记录，以标记原子完成。\n- 提交后，所有写入日志且代表实际文件系统状态的块（即应用程序数据和元数据更新）随后会被检出到它们的原始位置。\n- 忽略后台垃圾回收、检出之外的日志清理、压缩、部分块更新、设备内复制以及任何底层设备效应；仅计算主机可见的物理块写入。\n- 每个事务的逻辑数据负载为 $n$ 个块（应用程序的数据写入），文件系统元数据更新总计为每个事务 $m$ 个块。\n- 日志提交记录不对应于逻辑应用程序数据块，也不会被检出到原始位置。\n\n从写入放大 $WA$ 的定义（即物理写入的总块数与逻辑应用程序数据写入的块数之比）出发，推导两种模式下写入放大关于 $n$ 和 $m$ 的闭式表达式。以最简形式给出两个分别对应于数据日志模式和仅元数据日志模式的解析表达式作为最终答案。请将最终答案表示为一个行矩阵，其中第一个条目是数据日志模式的表达式，第二个条目是仅元数据日志模式的表达式。无需四舍五入，且 $WA$ 是无量纲的，不涉及物理单位。",
            "solution": "写入放大，记为 $WA$，定义为物理写入的总块数与逻辑应用程序数据写入的块数之比。\n$$WA = \\frac{\\text{物理写入的总块数}}{\\text{逻辑应用程序数据写入的块数}}$$\n根据问题陈述，每个事务的逻辑数据负载为 $n$ 个块。因此，对于两种日志模式，$WA$ 表达式的分母都是 $n$。\n$$\\text{逻辑应用程序数据写入的块数} = n$$\n\n我们现在将推导每种模式下物理写入的总块数。\n\n**1. 数据日志模式**\n\n在数据日志模式下，应用程序数据和元数据都首先被写入日志，然后被检出到存储设备上的最终位置（它们的“原始”位置）。每个事务的物理写入总数是写入日志的块数和检出期间写入的块数之和。\n\n首先，我们计算写入日志的块数：\n- $n$ 个应用程序数据块被写入日志。\n- $m$ 个文件系统元数据块被写入日志。\n- 一个大小为 $1$ 个块的日志提交记录被写入日志，以标记事务完成。\n\n写入日志的总块数是这些部分的总和：\n$$W_{\\text{journal, data}} = n + m + 1$$\n\n接下来，我们计算检出阶段写入的块数。检出过程涉及将日志中的数据和元数据写入其原始位置。\n- $n$ 个应用程序数据块从日志写入其原始位置。\n- $m$ 个元数据块从日志写入其原始位置。\n- 日志提交记录是日志的内部结构，不会被检出到原始位置。\n\n检出期间写入的总块数是：\n$$W_{\\text{checkpoint, data}} = n + m$$\n\n在数据日志模式下，每个事务的物理写入总块数 $W_{\\text{total, data}}$ 是日志写入和检出写入的总和：\n$$W_{\\text{total, data}} = W_{\\text{journal, data}} + W_{\\text{checkpoint, data}} = (n + m + 1) + (n + m) = 2n + 2m + 1$$\n\n现在，我们可以计算数据日志模式的写入放大 $WA_{\\text{data}}$：\n$$WA_{\\text{data}} = \\frac{W_{\\text{total, data}}}{n} = \\frac{2n + 2m + 1}{n}$$\n这可以简化为：\n$$WA_{\\text{data}} = \\frac{2n}{n} + \\frac{2m + 1}{n} = 2 + \\frac{2m + 1}{n}$$\n\n**2. 仅元数据日志模式**\n\n在仅元数据日志模式下，只有元数据更新被写入日志。应用程序数据直接写入其原始位置。这改变了物理写入的计算方式。\n\n首先，我们统计在事务执行过程中发生的初始写入：\n- $n$ 个应用程序数据块直接写入其原始位置。\n- $m$ 个文件系统元数据块被写入日志。\n- 一个大小为 $1$ 个块的日志提交记录被写入日志。\n\n初始物理写入的总数是：\n$$W_{\\text{initial, meta}} = n + m + 1$$\n\n接下来，我们计算检出阶段写入的块数。在这种模式下，只有元数据位于日志中。\n- $m$ 个元数据块从日志写入其原始位置。\n- 应用程序数据已经写入其原始位置，因此不需要再为其进行写入。\n- 日志提交记录不被检出。\n\n检出期间写入的总块数是：\n$$W_{\\text{checkpoint, meta}} = m$$\n\n在仅元数据日志模式下，每个事务的物理写入总块数 $W_{\\text{total, meta}}$ 是初始写入和检出写入的总和：\n$$W_{\\text{total, meta}} = W_{\\text{initial, meta}} + W_{\\text{checkpoint, meta}} = (n + m + 1) + m = n + 2m + 1$$\n\n现在，我们可以计算仅元数据日志模式的写入放大 $WA_{\\text{meta}}$：\n$$WA_{\\text{meta}} = \\frac{W_{\\text{total, meta}}}{n} = \\frac{n + 2m + 1}{n}$$\n这可以简化为：\n$$WA_{\\text{meta}} = \\frac{n}{n} + \\frac{2m + 1}{n} = 1 + \\frac{2m + 1}{n}$$\n\n问题要求最终答案是一个行矩阵，分别包含数据日志模式和仅元数据日志模式的表达式。",
            "answer": "$$\\boxed{\\begin{pmatrix} 2 + \\frac{2m+1}{n} & 1 + \\frac{2m+1}{n} \\end{pmatrix}}$$"
        },
        {
            "introduction": "理论上的文件系统保证通常依赖于理想化的假设。本练习将带你进入一个更具挑战性的真实世界场景：当禁用“写屏障”（write barriers）时会发生什么？你将设计一个实验，利用存储设备 volatile write cache 的特性，诱发一个经典的“提交先于数据”的竞态条件，并分析其导致的灾难性后果。通过这个练习，你将深刻理解文件系统的可靠性并非孤立存在，而是建立在操作系统与硬件之间正确协作的微妙契约之上。",
            "id": "3651387",
            "problem": "一个使用元数据预写式日志的日志文件系统（例如，一个类似 ext4 的有序模式设计）在正常情况下保证，在那些元数据所依赖的数据块被持久化之前，记录在日志中的元数据不会被认为是持久的。这一保证依赖于使用写入屏障或缓存刷新等机制来强制执行数据、日志元数据和日志提交记录之间的写入顺序。考虑一个带有易失性写缓存的存储设备，该缓存可能会重排写入并在断电时丢失缓存的数据。文件系统在挂载时明确禁用了屏障，因此文件系统不会发出强制单元访问（force-unit-access）或刷新命令来强制执行顺序。\n\n你的任务是确定哪一个实验既 (i) 最有可能导致日志的提交记录在相应的数据块之前到达稳定存储，又 (ii) 正确预测了在突然断电和随后的日志恢复之后可观察到的磁盘上不一致性。假设应用程序执行适合单个 $4\\,\\text{KB}$ 大小块的小文件更新，系统缓冲缓存可以延迟数据回写，并且 rename 更新和目录条目作为元数据记录在日志中。你可以假设崩溃恢复会重放日志中存在有效提交记录的任何事务。\n\n哪个选项最能同时满足这两个标准？\n\nA. 执行原子替换模式，但不显式同步数据文件：将新的 $4\\,\\text{KB}$ 负载写入一个临时文件，调用 rename 将其移动到目标路径上，然后对包含该文件的目录调用 fsync，但从不对临时文件的文件描述符调用 fsync 或 fdatasync。在目录 fsync 返回后立即切断电源。恢复后，目标文件名存在且具有预期的新大小，但其内容是旧版本或全为零，这表明日志提交已到达稳定存储，而数据块没有。\n\nB. 使用 O_DIRECT 打开目标文件并将新的 $4\\,\\text{K}$ 负载直接写入文件，然后将一个占位符文件重命名覆盖它，不对任何文件或目录调用 fsync 或 fdatasync。立即切断电源。恢复后，目标文件完全丢失，这表明提交记录在数据之前到达了稳定存储。\n\nC. 执行原子替换模式，但在写入新的 $4\\,\\text{KB}$ 负载后立即对临时文件额外调用 fsync，然后调用 rename 并对包含的目录调用 fsync。在目录 fsync 返回后立即切断电源。恢复后，目标文件名存在且具有预期的大小，但包含旧数据，这表明提交先于数据。\n\nD. 在数据日志模式下挂载文件系统并禁用屏障。将新的 $4\\,\\text{KB}$ 负载写入文件，调用 rename，且不对任何文件或目录调用 fsync。立即切断电源。恢复后，如果数据丢失，日志校验和会阻止任何重放，因此文件不会以新名称出现，这证明了提交先于数据的情况不会发生。\n\nE. 使用原子替换模式，不调用任何 fsync，然后休眠 $t$ 秒（其中 $t$ 足够大），再切断电源。恢复后，文件包含新数据，因为基于时间的延迟即使没有屏障也能保证顺序，所以提交先于数据的情况是不可观察的。",
            "solution": "这个问题的核心在于文件系统的日志模式保证、操作系统的 I/O 调度和硬件存储设备的缓存行为之间的相互作用。\n\n$1$. **`ordered` 模式**：在像 `ext4` 这样以 `ordered` 模式运行的日志文件系统中，有一个特定的约定：数据块必须*在*引用它们的元数据提交到日志之前被写入其在磁盘上的最终位置。例如，在创建一个新文件时，文件的内容数据必须在使文件的 inode 和目录条目可见的日志事务提交之前写入磁盘。这可以防止在崩溃后出现元数据指向未初始化或垃圾数据块的情况。\n\n$2$. **写入屏障**：`ordered` 模式的保证是通过控制向物理存储介质写入的顺序来强制执行的。操作的逻辑顺序是：\n    a. 发出数据块的写命令。\n    b. 发出一个写入屏障（例如，缓存刷新命令或设置了“强制单元访问”位的写命令）。此命令指示磁盘驱动器确保在继续之前，所有先前发出的写入都已提交到非易失性的稳定存储。\n    c. 发出日志元数据和最终日志提交记录的写命令。\n步骤 (b) 的屏障至关重要。它创建了一个同步点，确保数据持久性先于元数据提交持久性。\n\n$3$. **禁用的屏障和易失性缓存**：问题指明屏障被禁用，并且设备具有一个可以重排写入的易失性写缓存。此配置破坏了 `ordered` 模式的保证。文件系统仍然以正确的逻辑顺序发出写入（先数据，后日志提交），但没有屏障，就没有强制执行机制。这两组写入——数据和日志提交——被发送到磁盘控制器，并可能停留在其易失性缓存中。磁盘控制器可以自由地以其认为最有效的任何顺序将缓存的块写入物理盘片（例如，为了最小化磁头寻道时间）。一个小的、顺序的写入，如单个日志提交块，通常比一个可能更大或更分散的数据写入更快地被持久化。\n\n$4$. **竞争条件与不一致性**：这造成了一个竞争条件。日志提交记录有可能（甚至很可能）在依赖的数据块仍处于易失性缓存中时被写入稳定存储。如果此时发生电源故障，易失性缓存的内容将丢失。重启后，文件系统恢复过程会扫描日志。根据问题陈述，它会找到一个有效的提交记录并重放该事务。此事务更新文件系统的元数据结构，使其指向新数据。然而，新数据本身在电源故障中丢失了。inode 中的块指针将指向磁盘上的一个块，该块包含陈旧数据（来自先前删除的文件）或全为零（如果该块是新分配的）。这导致了一个可观察到的磁盘不一致性：文件具有正确的名称和大小，但其内容已损坏。\n\n任务是找到最有可能引发并正确描述这种特定不一致性的实验。\n\n### 逐项分析\n\n**A. 执行原子替换模式，但不显式同步数据文件：将新的 $4\\,\\text{KB}$ 负载写入一个临时文件，调用 rename 将其移动到目标路径上，然后对包含该文件的目录调用 fsync，但从不对临时文件的文件描述符调用 fsync 或 fdatasync。在目录 fsync 返回后立即切断电源。恢复后，目标文件名存在且具有预期的新大小，但其内容是旧版本或全为零，这表明日志提交已到达稳定存储，而数据块没有。**\n\n-   **分析**：这个实验旨在触发上述的确切竞争条件。\n    -   `write()`：新的 $4\\,\\text{KB}$ 数据被写入操作系统页缓存，将该页标记为脏页。\n    -   `rename()`：这是一个元数据操作，被记录在一个日志事务中。在 `ordered` 模式下，文件系统知道这个元数据依赖于脏数据块，并为该数据块发出一个写操作。\n    -   `fsync(directory)`：这个调用强制目录的元数据被持久化。在日志文件系统中，这意味着强制提交包含 `rename()` 的日志事务。文件系统将发出数据块的写操作，然后是日志提交记录的写操作，但关键是*两者之间没有屏障*。\n    -   这为设备重排写入创造了最高的可能性，即在数据之前持久化日志提交。\n    -   预测的结果也是正确的。恢复过程会从日志中重放 `rename` 操作。文件以其新名称和正确的大小（来自日志中的 inode 元数据）出现，但它所指向的数据块包含垃圾/陈旧数据，因为新数据的写入丢失了。\n-   **结论**：**正确**。此选项正确地确定了一个极有可能导致该问题的实验，并准确地预测了由此产生的磁盘不一致性。\n\n**B. 使用 O_DIRECT 打开目标文件并将新的 $4\\,\\text{K}$ 负载直接写入文件，然后将一个占位符文件重命名覆盖它，不对任何文件或目录调用 fsync 或 fdatasync。立即切断电源。恢复后，目标文件完全丢失，这表明提交记录在数据之前到达了稳定存储。**\n\n-   **分析**：\n    -   `O_DIRECT`：此标志绕过操作系统页缓存，但它不绕过设备的易失性写缓存。写操作被提交给设备，但在 `write()` 调用返回时，不保证它已在稳定存储上。\n    -   `No fsync`：没有对文件或目录进行 `fsync`，就没有触发器来强制 `rename` 的日志事务快速提交到磁盘。它将在一定延迟后由一个后台内核进程提交。在“立即”切断电源的情况下，提交记录甚至可能还没有被*写入*磁盘缓存，更不用说被持久化了。\n    -   预测的结果“目标文件完全丢失”意味着 `rename` 事务没有被重放，这意味着提交记录没有到达稳定存储。这与展示“提交记录在数据之前到达稳定存储”的目标相矛盾。\n-   **结论**：**不正确**。这个实验触发竞争条件可能性比 A 小，并且预测的结果未能展示所期望的故障模式。\n\n**C. 执行原子替换模式，但在写入新的 $4\\,\\text{KB}$ 负载后立即对临时文件额外调用 fsync，然后调用 rename 并对包含的目录调用 fsync。在目录 fsync 返回后立即切断电源。恢复后，目标文件名存在且具有预期的大小，但包含旧数据，这表明提交先于数据。**\n\n-   **分析**：这个实验包含了 `fsync(temporary file)`。这个系统调用的目的正是为了强制文件的内容数据写入稳定存储。虽然禁用屏障削弱了 `fsync` 的保证，但该调用仍然明确指示系统优先写入该数据。这个动作主动地*阻止*了元数据在数据之前被写入的竞争条件的产生。它使得期望的故障模式变得*更不可能*，而不是*更可能*。我们的目标是找到*最有可能*导致不一致性的实验。添加一个数据 `fsync` 是为了*防止*这种不一致性的标准编程实践。\n-   **结论**：**不正确**。这个实验的设计很差，无法达到所述目标，因为它包含了一个旨在防止它试图制造的问题的步骤。\n\n**D. 在数据日志模式下挂载文件系统并禁用屏障。将新的 $4\\,\\text{KB}$ 负载写入文件，调用 rename，且不对任何文件或目录调用 fsync。立即切断电源。恢复后，如果数据丢失，日志校验和会阻止任何重放，因此文件不会以新名称出现，这证明了提交先于数据的情况不会发生。**\n\n-   **分析**：此选项将日志模式更改为 `data=journal`。在此模式下，数据和元数据都被写入日志。提交先于数据的竞争条件仍然可能发生（即，日志提交块在日志数据块之前被写入磁盘）。然而，现代日志包含内部一致性检查，如校验和。如果恢复过程发现了一个提交记录，但日志中相关的数据块丢失或损坏（因为它们从易失性缓存中丢失了），它将认为整个事务无效并丢弃它。结果是 `rename` 不被重放，文件系统保持其先前的状态。这不会导致文件数据损坏的“可观察到的磁盘不一致性”；它只会导致一个事务被安全地中止。该选项的结论“这证明了提交先于数据的情况不会发生”也不精确；物理上的写入重排可能发生，但文件系统的恢复逻辑稳健地处理了它，防止了数据损坏。\n-   **结论**：**不正确**。由于不同的日志模式及其相关的恢复保证，该实验不会产生目标不一致性（损坏的文件数据）。\n\n**E. 使用原子替换模式，不调用任何 fsync，然后休眠 $t$ 秒（其中 $t$ 足够大），再切断电源。恢复后，文件包含新数据，因为基于时间的延迟即使没有屏障也能保证顺序，所以提交先于数据的情况是不可观察的。**\n\n-   **分析**：此选项的核心前提——“基于时间的延迟即使没有屏障也能保证顺序”——是根本错误的。操作系统可能会在超时（$t$）后将脏页回写到磁盘，但这些写入会进入磁盘的易失性缓存。没有明确的屏障或刷新命令，就无法保证物理介质级别的持久性或顺序。`sleep` 只会增加写入被发送到驱动器的概率，而不能保证它们以特定顺序被持久存储。因此，该实验不能可靠地产生任何特定的结果，其推理基于对存储保证的错误理解。\n-   **结论**：**不正确**。其推理基于一个错误的科学前提。",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}