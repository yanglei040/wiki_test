{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge for any file system is maintaining consistency in the face of unexpected power failures. Journaling file systems address this by ensuring that complex updates are atomic—they either complete fully or not at all. This thought experiment  invites you to play the role of a forensic analyst, determining the precise on-disk state after a crash at different points in a file-update sequence, thereby solidifying your understanding of how commit records and ordered writes provide crash safety.",
            "id": "3651370",
            "problem": "A journaling File System (FS) that uses metadata-only journaling in ordered mode is mounted with default write-back caching. The directory path $/ \\mathrm{dir}$ initially exists and contains no entries named $\\mathrm{new}$ or $\\mathrm{final}$. The destination name $\\mathrm{final}$ does not exist prior to the sequence below. Consider the following sequence of operations that creates a file, writes data, and then renames it, with no File Synchronization (fsync) calls issued by the application at any point. The Portable Operating System Interface (POSIX) rename semantics guarantee atomicity: after recovery from a crash, either the old name is present or the new name, but not both. The journaling layer provides the following guarantees: transactions record metadata in a journal; a transaction becomes committed when its journal commit record is durably written; recovery replays any transaction that has a valid commit record to the main file system structures; and in ordered mode, any data blocks dirtied by operations that make those blocks reachable via metadata are flushed to the main storage before the commit record for the corresponding metadata transaction is durably written.\n\nThe system executes the following steps, numbered by $k$:\n- $k=1$: The application creates the file $\\mathrm{new}$ in $/ \\mathrm{dir}$, causing allocation of a fresh inode $I$ and insertion of a directory entry $\\mathrm{new} \\rightarrow I$. This is metadata tracked by the journaling subsystem as transaction $T_1$.\n- $k=2$: The application writes $2$ data blocks $B_1$ and $B_2$ into the file. These are buffered in the page cache and marked dirty. This is data, not journaled as metadata.\n- $k=3$: The inode $I$’s size is updated to $S$ bytes to reflect the written data. This is metadata and remains part of $T_1$.\n- $k=4$: The journaling layer writes the journal descriptor for $T_1$, staging the metadata changes (e.g., the directory entry and inode changes) into the journal area, and schedules ordered writeback of $B_1$ and $B_2$ before any commit.\n- $k=5$: After the ordered data writes for $B_1$ and $B_2$ have been issued, the journaling layer writes the commit record for $T_1$ to the journal. At this point, recovery will replay $T_1$’s metadata if a crash occurs.\n- $k=6$: The checkpointing process copies $T_1$’s metadata from the journal to the main file system structures. The journal space for $T_1$ may then be reclaimed.\n- $k=7$: The application renames the file from $\\mathrm{new}$ to $\\mathrm{final}$ within $/ \\mathrm{dir}$. The rename is recorded as transaction $T_2$ (metadata only), whose journal descriptor and commit record are written in this step (ordered mode does not require additional data flushes for $T_2$ because no new data is made reachable).\n- $k=8$: The checkpointing process copies $T_2$’s metadata from the journal to the main file system structures.\n\nAssume that a power failure occurs exactly after completing step $k$ (i.e., between steps $k$ and $k+1$). Consider the following possible post-recovery on-disk states:\n- $\\mathrm{S1}$: Neither $\\mathrm{new}$ nor $\\mathrm{final}$ exists in $/ \\mathrm{dir}$. No metadata allocation for $I$ is present. The data blocks $B_1$ and $B_2$ may or may not have been physically written, but they are not reachable by any file (and any physical writes, if present, are irrelevant because the corresponding metadata was not committed).\n- $\\mathrm{S2}$: The entry $\\mathrm{new}$ exists in $/ \\mathrm{dir}$, pointing to inode $I$ with size $S$. The data blocks $B_1$ and $B_2$ are on disk and reachable via $I$. The entry $\\mathrm{final}$ does not exist.\n- $\\mathrm{S3}$: The entry $\\mathrm{final}$ exists in $/ \\mathrm{dir}$, pointing to inode $I$ with size $S$. The data blocks $B_1$ and $B_2$ are on disk and reachable via $I$. The entry $\\mathrm{new}$ does not exist.\n- $\\mathrm{S4}$: Both $\\mathrm{new}$ and $\\mathrm{final}$ exist simultaneously in $/ \\mathrm{dir}$, pointing to the same inode $I$.\n\nWhich option correctly characterizes the set of possible post-recovery on-disk states for each failure point $k$?\n\nA. For $k \\in \\{1,2,3,4\\}$, only $\\mathrm{S1}$; for $k \\in \\{5,6\\}$, only $\\mathrm{S2}$; for $k \\in \\{7,8\\}$, only $\\mathrm{S3}$.\n\nB. For $k \\in \\{1,2,3\\}$, only $\\mathrm{S1}$; for $k=4$, either $\\mathrm{S1}$ or $\\mathrm{S2}$; for $k \\in \\{5,6\\}$, only $\\mathrm{S2}$; for $k \\in \\{7,8\\}$, only $\\mathrm{S3}$.\n\nC. For $k \\in \\{1,2,3,4\\}$, only $\\mathrm{S1}$; for $k \\in \\{5,6\\}$, only $\\mathrm{S2}$; for $k=7$, either $\\mathrm{S2}$ or $\\mathrm{S3}$ or $\\mathrm{S4}$; for $k=8$, only $\\mathrm{S3}$.\n\nD. For $k \\in \\{1,2,3,4\\}$, only $\\mathrm{S1}$; for $k=5$, either $\\mathrm{S1}$ or $\\mathrm{S2}$ depending on whether $B_1$ and $B_2$ reached the disk; for $k=6$, only $\\mathrm{S2}$; for $k \\in \\{7,8\\}$, only $\\mathrm{S3}$.",
            "solution": "We begin from the core definitions and guarantees of a metadata-only journaling File System (FS) operating in ordered mode:\n\n- A journaling transaction records metadata changes to a separate journal area. A transaction is considered committed when its journal commit record is durably written. During crash recovery, any transaction with a valid commit record is replayed to the main file system structures, ensuring metadata atomicity.\n- In ordered mode, any dirty data blocks that the transaction makes reachable (e.g., newly allocated file content referenced by the inode’s size and block mappings) are flushed to the main storage before the corresponding transaction’s commit record is written. Consequently, if the metadata commit makes the file and its size visible, the data blocks referenced by that metadata have already been flushed to main storage (subject to the constraints of hardware durability at the time of the commit).\n- Portable Operating System Interface (POSIX) rename semantics guarantee atomicity: after recovery, either the old name or the new name exists, but not both simultaneously.\n- Checkpointing copies committed metadata from the journal to the main file system structures; however, the presence of a commit record alone is sufficient for recovery to make the transaction’s metadata visible, even if checkpointing has not yet completed at the time of the crash.\n\nWe now analyze the sequence by $k$:\n\n- For $k \\in \\{1,2,3\\}$: No commit record for $T_1$ has been written. Although metadata may be staged into the journal at $k=4$, before $k=4$ there is not even a journal descriptor. Therefore, a crash after $k \\in \\{1,2,3\\}$ yields no committed metadata. Recovery will not create the file, so the on-disk state is $\\mathrm{S1}$.\n\n- For $k=4$: The journal descriptor for $T_1$ exists, but there is no commit record yet. Transactions without a commit record are ignored by recovery. No main-area metadata updates have been checkpointed, and the journal’s uncommitted metadata is not replayed. Thus, after a crash at $k=4$, the system yields $\\mathrm{S1}$.\n\n- For $k=5$: The commit record for $T_1$ has been durably written. By ordered mode, the data blocks $B_1$ and $B_2$ that $T_1$ makes reachable have been flushed before the commit record was written. Therefore, post-recovery, the file exists as $\\mathrm{new}$ with inode $I$ of size $S$, and $B_1,B_2$ are on disk and reachable. The on-disk state is $\\mathrm{S2}$.\n\n- For $k=6$: Checkpointing $T_1$’s metadata to the main area does not change the externally visible state compared to recovery with a committed-but-not-checkpointed transaction. After a crash at $k=6$, recovery finds $T_1$ committed and replays it if needed; the on-disk state remains $\\mathrm{S2}$.\n\n- For $k=7$: The rename operation is recorded as $T_2$. In ordered mode, no additional data flush is required for $T_2$ because rename affects only metadata (directory entries and possibly link counts). The commit record for $T_2$ is written at $k=7$, which ensures atomic rename semantics upon recovery. After the crash, only the new name exists, and the old name does not. Hence, the on-disk state is $\\mathrm{S3}$.\n\n- For $k=8$: Checkpointing $T_2$ does not alter the externally visible state beyond what recovery would provide with a committed $T_2$. After the crash, recovery yields $\\mathrm{S3}$.\n\nFrom this analysis, the correct characterization is:\n- $k \\in \\{1,2,3,4\\} \\Rightarrow \\mathrm{S1}$,\n- $k \\in \\{5,6\\} \\Rightarrow \\mathrm{S2}$,\n- $k \\in \\{7,8\\} \\Rightarrow \\mathrm{S3}$.\n\nWe now evaluate the options:\n\nOption A: States are assigned as $\\mathrm{S1}$ for $k \\in \\{1,2,3,4\\}$; $\\mathrm{S2}$ for $k \\in \\{5,6\\}$; $\\mathrm{S3}$ for $k \\in \\{7,8\\}$. This matches the derivation. Verdict — Correct.\n\nOption B: Claims that at $k=4$, either $\\mathrm{S1}$ or $\\mathrm{S2}$ can occur. This contradicts the journaling requirement of a commit record for recovery; with only a journal descriptor (no commit), recovery ignores the transaction, so $\\mathrm{S2}$ is not possible at $k=4$. Verdict — Incorrect.\n\nOption C: Claims that at $k=7$, $\\mathrm{S2}$ or $\\mathrm{S3}$ or $\\mathrm{S4}$ may occur. In fact, with a commit record for $T_2$ written at $k=7$, recovery must produce the renamed state $\\mathrm{S3}$; $\\mathrm{S2}$ would only arise without a $T_2$ commit. Furthermore, $\\mathrm{S4}$ violates POSIX atomic rename semantics and the journaling system’s metadata atomicity. Verdict — Incorrect.\n\nOption D: Claims that at $k=5$, either $\\mathrm{S1}$ or $\\mathrm{S2}$ may occur depending on whether $B_1$ and $B_2$ reached the disk. In ordered mode, the commit record for $T_1$ is written only after the data blocks made reachable by $T_1$ are flushed; therefore, if the commit record is present, recovery will produce $\\mathrm{S2}$, not $\\mathrm{S1}$. The presence of committed metadata guarantees the existence of the file, and ordered writeback ensures the data reached disk before commit. Verdict — Incorrect.\n\nTherefore, the correct option is A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While journaling provides critical crash safety, this robustness comes at a performance cost, quantified by a metric called write amplification ($WA$). This practice challenges you to derive the $WA$ for two common journaling modes: data journaling and metadata-only journaling. By working through the block-level accounting , you will develop a quantitative feel for the trade-offs between crash-safety guarantees and I/O overhead.",
            "id": "3651432",
            "problem": "Consider a journaling file system that executes transactions to provide atomic updates. Each transaction consists of exactly $n$ application data blocks and $m$ file system metadata blocks that the application logically updates. All blocks have equal size, and any physical write of one block counts as one unit toward the write tally. The system can operate in one of two modes:\n\n- Data journaling mode: both application data and metadata updates are first written to the journal and later propagated to their home locations.\n- Metadata-only journaling mode: only metadata updates are written to the journal; application data is written directly to its home locations in an order that preserves consistency.\n\nFor both modes, assume the following operational details:\n- Every transaction appends a single journal commit record of $1$ block to the journal to mark atomic completion.\n- After a commit, any blocks that were written to the journal and that represent actual file system state (that is, application data and metadata updates) are later checkpointed to their home locations.\n- Ignore background garbage collection, journal cleaning beyond checkpointing, compression, partial-block updates, device-internal duplication, and any lower-level device effects; count only host-visible physical block writes.\n- The logical data payload per transaction is $n$ blocks (the application’s data writes), and the file system metadata updates total $m$ blocks per transaction.\n- The journal commit record does not correspond to a logical application data block and is not checkpointed to a home location.\n\nStarting from the definition that write amplification $WA$ is the ratio of total physical blocks written to the logical application data blocks written, derive closed-form expressions for the write amplification in both modes as functions of $n$ and $m$. Express your final answer as two analytical expressions in their simplest form, corresponding to data journaling and metadata-only journaling, respectively. Provide your final answer as a row matrix, with the first entry being the data journaling expression and the second entry being the metadata-only journaling expression. No rounding is required, and no physical units are involved because $WA$ is dimensionless.",
            "solution": "The write amplification, denoted as $WA$, is defined as the ratio of the total number of physical blocks written to the number of logical application data blocks written.\n$$WA = \\frac{\\text{Total Physical Blocks Written}}{\\text{Logical Application Data Blocks Written}}$$\nAccording to the problem statement, the logical data payload per transaction consists of $n$ blocks. Therefore, for both journaling modes, the denominator of the $WA$ expression is $n$.\n$$\\text{Logical Application Data Blocks Written} = n$$\n\nWe will now derive the total number of physical blocks written for each mode.\n\n**1. Data Journaling Mode**\n\nIn data journaling mode, both application data and metadata are first written to the journal and then checkpointed to their final locations on the storage device (their \"home\" locations). The total physical writes per transaction are the sum of writes to the journal and writes during checkpointing.\n\nFirst, we calculate the number of blocks written to the journal:\n- The $n$ application data blocks are written to the journal.\n- The $m$ file system metadata blocks are written to the journal.\n- A single journal commit record of $1$ block is written to the journal to mark the transaction as complete.\n\nThe total number of blocks written to the journal is the sum of these components:\n$$W_{\\text{journal, data}} = n + m + 1$$\n\nNext, we calculate the number of blocks written during the checkpointing phase. The checkpointing process involves writing the journaled data and metadata to their home locations.\n- The $n$ application data blocks are written from the journal to their home locations.\n- The $m$ metadata blocks are written from the journal to their home locations.\n- The journal commit record is an internal journal structure and is not checkpointed to a home location.\n\nThe total number of blocks written during checkpointing is:\n$$W_{\\text{checkpoint, data}} = n + m$$\n\nThe total physical blocks written per transaction in data journaling mode, $W_{\\text{total, data}}$, is the sum of the journal writes and the checkpoint writes:\n$$W_{\\text{total, data}} = W_{\\text{journal, data}} + W_{\\text{checkpoint, data}} = (n + m + 1) + (n + m) = 2n + 2m + 1$$\n\nNow, we can compute the write amplification for data journaling, $WA_{\\text{data}}$:\n$$WA_{\\text{data}} = \\frac{W_{\\text{total, data}}}{n} = \\frac{2n + 2m + 1}{n}$$\nThis can be simplified to:\n$$WA_{\\text{data}} = \\frac{2n}{n} + \\frac{2m + 1}{n} = 2 + \\frac{2m + 1}{n}$$\n\n**2. Metadata-only Journaling Mode**\n\nIn metadata-only journaling mode, only the metadata updates are written to the journal. The application data is written directly to its home location. This changes the accounting of physical writes.\n\nFirst, let's tally the initial writes that occur as part of the transaction's execution:\n- The $n$ application data blocks are written directly to their home locations.\n- The $m$ file system metadata blocks are written to the journal.\n- A single journal commit record of $1$ block is written to the journal.\n\nThe total number of initial physical writes is:\n$$W_{\\text{initial, meta}} = n + m + 1$$\n\nNext, we calculate the number of blocks written during the checkpointing phase. In this mode, only the metadata resides in the journal.\n- The $m$ metadata blocks are written from the journal to their home locations.\n- The application data was already written to its home location, so no further write is needed for it.\n- The journal commit record is not checkpointed.\n\nThe total number of blocks written during checkpointing is:\n$$W_{\\text{checkpoint, meta}} = m$$\n\nThe total physical blocks written per transaction in metadata-only journaling mode, $W_{\\text{total, meta}}$, is the sum of the initial writes and the checkpoint writes:\n$$W_{\\text{total, meta}} = W_{\\text{initial, meta}} + W_{\\text{checkpoint, meta}} = (n + m + 1) + m = n + 2m + 1$$\n\nNow, we can compute the write amplification for metadata-only journaling, $WA_{\\text{meta}}$:\n$$WA_{\\text{meta}} = \\frac{W_{\\text{total, meta}}}{n} = \\frac{n + 2m + 1}{n}$$\nThis can be simplified to:\n$$WA_{\\text{meta}} = \\frac{n}{n} + \\frac{2m + 1}{n} = 1 + \\frac{2m + 1}{n}$$\n\nThe problem requires the final answer to be a row matrix containing the expressions for data journaling and metadata-only journaling, respectively.",
            "answer": "$$\\boxed{\\begin{pmatrix} 2 + \\frac{2m+1}{n}  1 + \\frac{2m+1}{n} \\end{pmatrix}}$$"
        },
        {
            "introduction": "For applications demanding high durability, frequent synchronous writes (via `fsync`) can create a performance bottleneck by forcing many small journal commits. To mitigate this, file systems often employ a strategy called commit coalescing, batching multiple requests into a single commit. This exercise  asks you to model this optimization using a Poisson process, allowing you to calculate the expected batch size and appreciate how systems use probabilistic techniques to improve performance.",
            "id": "3651371",
            "problem": "A journaling file system in an Operating System (OS) amortizes commit costs by coalescing synchronous flush requests initiated by the file synchronization system call (fsync) into a single journal commit using a fixed coalescing timeout. When the journal is idle and an fsync arrives at time $t_0$, the system opens a commit window of length $\\tau$ seconds, and performs exactly one commit at time $t_0 + \\tau$ that includes all fsync requests that arrived in the half-open interval $\\left[t_0,\\, t_0 + \\tau\\right)$. Assume the commit duration is negligible relative to $\\tau$ so that it does not influence arrivals, and that after the commit the journal immediately returns to the idle state awaiting the next fsync to open a new window.\n\nThere are $n$ independent files, each managed by a separate application thread. Each thread produces small writes followed by fsync requests over time such that, for a given thread, the issuance times of its fsync requests form a Poisson process of rate $\\rho$ per second, independent across threads. By superposition, the aggregate arrival process of fsync requests is then a Poisson process of rate $\\lambda = n \\rho$ per second.\n\nStarting from first principles about Poisson processes and without invoking any batching-specific formulas, derive the expected number of fsync requests that are included in a single journal commit (that is, the expected coalesced batch size per commit) under the fixed coalescing timeout $\\tau$. Express your final answer as a closed-form analytic expression in terms of $n$, $\\rho$, and $\\tau$. No rounding is required, and the final quantity is a dimensionless expected count.",
            "solution": "The problem asks for the expected number of `fsync` requests coalesced into a single journal commit. The aggregate arrival of `fsync` requests from all $n$ threads constitutes a single Poisson process with a total rate of $\\lambda = n \\rho$ requests per second.\n\nA key property of a Poisson process is that the number of arrivals, $N(T)$, in any time interval of duration $T$ follows a Poisson distribution with parameter (and mean) $\\lambda T$. The probability mass function is given by:\n$$ P(N(T) = k) = \\frac{(\\lambda T)^k \\exp(-\\lambda T)}{k!} \\quad \\text{for } k = 0, 1, 2, \\dots $$\nThe expected value of this distribution is:\n$$ E[N(T)] = \\lambda T $$\n\nThe problem specifies the mechanism for forming a commit batch. The process begins when the journal is in an idle state and an `fsync` request arrives. Let this initial arrival occur at time $t_0$. This event triggers the opening of a commit window of a fixed duration $\\tau$. The commit, which occurs at time $t_0 + \\tau$, includes all `fsync` requests that have arrived in the time interval $[t_0, t_0 + \\tau)$.\n\nLet $X$ be the random variable representing the total number of `fsync` requests included in a single commit batch. We can decompose $X$ into two components:\n1. The single `fsync` request that arrived at time $t_0$, which initiated the commit window. This request is, by definition, part of the batch.\n2. The additional `fsync` requests that arrive during the subsequent open interval $(t_0, t_0 + \\tau)$.\n\nLet $N_{add}$ be the number of these additional `fsync` requests. The total number of requests in the batch is therefore:\n$$ X = 1 + N_{add} $$\nWe are asked to find the expected value of $X$, denoted $E[X]$. Using the linearity of expectation, we have:\n$$ E[X] = E[1 + N_{add}] = E[1] + E[N_{add}] = 1 + E[N_{add}] $$\nOur task reduces to finding the expected number of additional arrivals, $E[N_{add}]$.\n\nThe additional arrivals occur in the time interval $(t_0, t_0 + \\tau)$. The duration of this interval is $(t_0 + \\tau) - t_0 = \\tau$. Because the aggregate arrival stream is a Poisson process with rate $\\lambda$, the number of arrivals in any interval of length $\\tau$ follows a Poisson distribution with mean $\\lambda \\tau$. The memoryless property of the Poisson process ensures that the arrival rate is constant and independent of when the interval starts. Therefore, the number of additional arrivals, $N_{add}$, is a random variable following a Poisson distribution with parameter $\\lambda \\tau$.\n$$ N_{add} \\sim \\text{Poisson}(\\lambda \\tau) $$\nThe expected value of a Poisson-distributed random variable with parameter $\\mu$ is simply $\\mu$. Thus, the expected number of additional arrivals is:\n$$ E[N_{add}] = \\lambda \\tau $$\nSubstituting this result back into our expression for $E[X]$:\n$$ E[X] = 1 + \\lambda \\tau $$\nFinally, the problem provides the aggregate rate $\\lambda$ in terms of the number of threads $n$ and the per-thread rate $\\rho$, as $\\lambda = n \\rho$. Substituting this into our expression for the expected batch size gives the final answer.\n$$ E[X] = 1 + (n \\rho) \\tau $$\nThis expression represents the expected number of `fsync` requests per commit. It is composed of the one request that triggers the commit plus the average number of requests that arrive during the coalescing window. This derivation starts from the fundamental properties of the Poisson process (number of events in an interval and linearity of expectation) as required.",
            "answer": "$$\\boxed{1 + n \\rho \\tau}$$"
        }
    ]
}