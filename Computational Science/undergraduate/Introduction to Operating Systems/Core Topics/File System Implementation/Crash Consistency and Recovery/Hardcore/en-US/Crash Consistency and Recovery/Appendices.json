{
    "hands_on_practices": [
        {
            "introduction": "To ensure a system can recover quickly from a crash, designers must balance the overhead of consistency mechanisms against the desired recovery time. This exercise explores this fundamental trade-off through a simplified model of a journaling filesystem. By analyzing the relationship between the rate of incoming writes, the frequency of checkpoints, and the time required for journal replay, you will derive a crucial design parameter that bounds the worst-case recovery time, demonstrating how system-level policies directly impact reliability guarantees .",
            "id": "3630989",
            "problem": "You are designing a journaling storage subsystem for an operating system that uses Write-Ahead Logging (WAL) with periodic checkpointing to ensure crash consistency. A checkpoint flushes all updates up to that point into the main on-disk state so that the journal prior to the checkpoint can be discarded. Suppose the system is subjected to a sustained, steady write workload at a constant rate of $r$ operations per second (ops/sec). The system takes checkpoints periodically at a fixed frequency $f$ (in hertz), meaning one checkpoint every $1/f$ seconds. If a crash occurs, recovery replays all journaled operations that occurred after the most recent checkpoint.\n\nAssume the following foundational principles:\n- By the definition of rate, if a process executes at a rate of $q$ operations per second for a duration of $t$ seconds, it completes $q \\cdot t$ operations; conversely, the time to process $n$ operations at rate $q$ is $n/q$ seconds.\n- During recovery, the journal replay engine applies logged operations at the same throughput as the sustained write workload, namely $r$ operations per second. This models recovery as being I/O-bound on the same underlying device.\n\nYour design goal is to bound the worst-case journal replay time after a crash to at most $T$ seconds. Under the stated assumptions and workload, derive a closed-form expression for the minimal checkpointing frequency $f$ as a function of $T$ and $r$ that achieves this bound for any crash time relative to the checkpoint schedule. Express $f$ in hertz. Provide your final answer as a single closed-form analytic expression for $f(T,r)$ with no numerical evaluation or approximation.",
            "solution": "The problem statement has been validated and found to be self-contained, scientifically grounded, and well-posed. We may proceed with the solution.\n\nOur objective is to derive an expression for the minimal checkpointing frequency, $f$, required to ensure that the worst-case a journal replay time after a crash does not exceed a specified duration $T$. The system is characterized by a constant write workload rate $r$ and a recovery replay rate also equal to $r$.\n\nLet the checkpointing frequency be $f$, measured in hertz. The time period between successive checkpoints is therefore $P = \\frac{1}{f}$ seconds. Checkpoints are taken at times $t = 0, \\frac{1}{f}, \\frac{2}{f}, \\dots, \\frac{k}{f}, \\dots$.\n\nAfter a crash, recovery consists of replaying all operations that were logged since the most recent successful checkpoint. Let us consider the time interval between two checkpoints, starting at time $t_{ckpt}$. The next checkpoint is scheduled for time $t_{ckpt} + \\frac{1}{f}$.\n\nIf a crash occurs at some time $t_{crash}$ within this interval ($t_{ckpt} < t_{crash} \\le t_{ckpt} + \\frac{1}{f}$), the time elapsed since the last checkpoint is $\\Delta t = t_{crash} - t_{ckpt}$.\n\nDuring this time $\\Delta t$, the system has been subjected to a sustained write workload at a rate of $r$ operations per second. Based on the provided principle that the number of operations is the product of rate and time, the number of operations logged in the journal, $N_{logged}$, is:\n$$N_{logged} = r \\cdot \\Delta t$$\n\nDuring recovery, these $N_{logged}$ operations must be replayed. The problem states that the journal replay engine applies these logged operations at the same rate $r$. The time required for this replay, $T_{replay}$, is the total number of operations to be replayed divided by the replay rate:\n$$T_{replay} = \\frac{N_{logged}}{r}$$\nSubstituting the expression for $N_{logged}$, we find:\n$$T_{replay} = \\frac{r \\cdot \\Delta t}{r} = \\Delta t$$\nThis is a critical intermediate result: the recovery time is exactly equal to the time elapsed since the last checkpoint completed. This is a direct consequence of the model's assumption that the write workload rate and the recovery replay rate are identical.\n\nThe design goal is to bound the *worst-case* journal replay time. To find the worst-case replay time, $T_{replay, worst}$, we must identify the scenario that maximizes $T_{replay}$. Since $T_{replay} = \\Delta t$, this is equivalent to finding the maximum possible value of $\\Delta t$.\n\nThe time since the last checkpoint, $\\Delta t = t_{crash} - t_{ckpt}$, is maximized when the crash occurs at the latest possible moment before the next checkpoint would have completed. The maximum possible duration of the interval between the last checkpoint and the crash is the full checkpointing period, $\\frac{1}{f}$.\nTherefore, the maximum possible value of $\\Delta t$ is:\n$$\\Delta t_{max} = \\frac{1}{f}$$\nThis leads to the worst-case recovery time:\n$$T_{replay, worst} = \\Delta t_{max} = \\frac{1}{f}$$\n\nThe problem specifies that this worst-case recovery time must be at most $T$ seconds. This translates to the following inequality:\n$$T_{replay, worst} \\le T$$\nSubstituting our expression for $T_{replay, worst}$:\n$$\\frac{1}{f} \\le T$$\nWe are asked to find the *minimal* checkpointing frequency $f$ that satisfies this condition. To do this, we solve the inequality for $f$. Given that $f$ (frequency) and $T$ (time) are positive physical quantities, we can manipulate the inequality. Multiplying both sides by $f$ yields $1 \\le f \\cdot T$. Dividing both sides by $T$ gives:\n$$f \\ge \\frac{1}{T}$$\nThis inequality states that for the recovery time to be bounded by $T$, the checkpointing frequency $f$ must be greater than or equal to $\\frac{1}{T}$. The minimal frequency that satisfies this condition is the one at the boundary of the acceptable range.\nThus, the minimal required frequency is:\n$$f_{min} = \\frac{1}{T}$$\nThis expression gives the minimal checkpointing frequency as a function of the maximum allowable recovery time $T$. Notably, the result is independent of the workload rate $r$, as this parameter influenced both the accumulation of writes and the speed of their replay symmetrically, causing it to cancel from the final relationship for recovery time.",
            "answer": "$$\n\\boxed{\\frac{1}{T}}\n$$"
        },
        {
            "introduction": "Journaling is a powerful technique for ensuring filesystem consistency, but it does not come for free. The additional I/O operations required by the journal lead to a phenomenon known as write amplification, where the total bytes written to the storage device exceed the bytes written by the application. This practice guides you through a quantitative analysis of this overhead, asking you to derive the write amplification for two common journaling strategies: metadata-only and full data journaling. Understanding this cost is essential for making informed decisions about performance tuning and selecting the right consistency level for a given workload .",
            "id": "3631096",
            "problem": "A storage stack uses a journaling file system to provide crash consistency. A journal is written in fixed-size blocks of size $j$, and data blocks written by applications have size $b$. Consider a random write workload in which each logical write updates exactly one data block of size $b$ and, because of randomness, cannot be coalesced with other writes into a shared journal transaction. Each logical write therefore forms its own journal transaction. The system uses one of two policies:\n- Metadata-only journaling: only the metadata describing the change is written to the journal before the change is considered committed; the data block is written directly to its home location.\n- Full data journaling: both the data and metadata are written to the journal before commit; subsequently, the data is propagated to its home location.\n\nAssume the following widely accepted facts about journaling:\n1. In both policies, each transaction writes a metadata journal record that occupies exactly one journal block of size $j$, and a commit record that occupies exactly one journal block of size $j$.\n2. In full data journaling, the data payload of size $b$ is first written to the journal, occupying $\\lceil b / j \\rceil$ journal blocks, and later written once to its home location.\n3. In metadata-only journaling, the data payload of size $b$ is written once directly to its home location and is not written to the journal.\n4. Write amplification is defined as the ratio of total bytes written to the storage device per logical write divided by the application payload bytes of that logical write.\n\nUsing only these facts and the definitions, derive exact, closed-form expressions for the write amplification $W_{\\text{meta}}(b,j)$ under metadata-only journaling and $W_{\\text{full}}(b,j)$ under full data journaling, in terms of $b$ and $j$. Express your final answer as a pure number (dimensionless) for each policy. If your expression requires a rounding operation, use the ceiling function $\\lceil \\cdot \\rceil$; do not approximate or round numerically. Provide the two expressions as a single row matrix with $W_{\\text{meta}}(b,j)$ first and $W_{\\text{full}}(b,j)$ second.",
            "solution": "We begin from the definition of write amplification. For a single logical write of payload size $b$, the write amplification $W$ is defined as the ratio of total device bytes written to the payload bytes written by the application:\n$$\nW \\equiv \\frac{\\text{total device bytes written per logical write}}{\\text{payload bytes per logical write}} = \\frac{\\text{device bytes}}{b}.\n$$\n\nUnder the stated assumptions, we enumerate the device bytes written for each policy.\n\nMetadata-only journaling:\n- The application data block is written once to its home location, contributing $b$ bytes.\n- The journal receives one metadata record occupying exactly one journal block of size $j$, contributing $j$ bytes.\n- The journal receives one commit record occupying exactly one journal block of size $j$, contributing another $j$ bytes.\nSumming these, the total device bytes per logical write under metadata-only journaling is\n$$\nb + j + j = b + 2j.\n$$\nTherefore, the write amplification under metadata-only journaling is\n$$\nW_{\\text{meta}}(b,j) = \\frac{b + 2j}{b}.\n$$\n\nFull data journaling:\n- The data payload of size $b$ is first written to the journal, which is organized in blocks of size $j$. Writing $b$ bytes to the journal consumes $\\lceil b / j \\rceil$ journal blocks, for a total of $\\lceil b / j \\rceil \\cdot j$ bytes.\n- The journal receives one metadata record occupying exactly one journal block of size $j$, contributing $j$ bytes.\n- The journal receives one commit record occupying exactly one journal block of size $j$, contributing another $j$ bytes.\n- After commit, the data is propagated to its home location once, contributing $b$ bytes.\nSumming these, the total device bytes per logical write under full data journaling is\n$$\n\\left\\lceil \\frac{b}{j} \\right\\rceil \\cdot j + j + j + b = \\left\\lceil \\frac{b}{j} \\right\\rceil j + 2j + b.\n$$\nTherefore, the write amplification under full data journaling is\n$$\nW_{\\text{full}}(b,j) = \\frac{\\left\\lceil \\frac{b}{j} \\right\\rceil j + 2j + b}{b}.\n$$\n\nBoth expressions are dimensionless ratios, consistent with the definition of write amplification. No numerical rounding is performed; the ceiling function $\\lceil \\cdot \\rceil$ exactly captures the journal block granularity for writing $b$ bytes to the journal.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{b+2j}{b} & \\frac{\\left\\lceil \\frac{b}{j} \\right\\rceil j + 2j + b}{b}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Achieving crash consistency is a collaborative effort between the operating system and the applications it runs. While the filesystem provides powerful primitives like atomic `rename` and durable `fsync`, it is the application's responsibility to use them in the correct sequence to protect its own data invariants. This problem presents a classic challenge in concurrent programming: analyzing an attractive but incorrect protocol to find a subtle race condition that can lead to data corruption during a crash. By identifying the failure and proposing a minimal fix, you will gain hands-on experience in the disciplined thinking required to build truly robust, crash-safe applications .",
            "id": "3631038",
            "problem": "You are reviewing an application-level update protocol that aims to atomically publish a new immutable data object and a small manifest file that names the current object. The system runs on a typical Portable Operating System Interface (POSIX) file system. Assume the following standard and well-tested semantics as your base: a write system call returns when data are in the page cache and can reach the storage device later in any order; a rename system call is atomic with respect to the directory namespace but not necessarily durable without further actions; the File Synchronization system call (fsync) on a regular file flushes its file contents and metadata to stable storage; and fsync on a directory flushes directory-entry changes such as create, unlink, and rename.\n\nFiles and invariant:\n- The directory contains immutable content files named like \"obj.v$k$\" and a manifest file named \"manifest\".\n- The manifest file contains exactly one line: the basename of the current object file, for example \"obj.v$17$\".\n- Invariant: After any crash and subsequent reboot, the manifest must contain exactly the basename of an existing, fully written object file in the same directory.\n\nAttractive but wrong update protocol $P$ (no fsync calls anywhere):\n- Step $1$: Create and write the new object into a temporary file \"obj.tmp\"; close it.\n- Step $2$: rename \"obj.tmp\" to the final name \"obj.v$k+1$\" in the same directory.\n- Step $3$: Create \"manifest.tmp\", write the ASCII name \"obj.v$k+1$\" into it; close it.\n- Step $4$: rename \"manifest.tmp\" over \"manifest\" in the same directory; return success.\n\nQuestion. Which option identifies a concrete crash interleaving that violates the invariant under the stated semantics and also gives a correct minimal placement of fsync calls that fixes protocol $P$ so that the invariant is preserved for any crash? Here, “minimal” means using the fewest fsync calls necessary to prevent the invariant violation, without introducing a separate write-ahead log or changing the file layout.\n\nChoose one:\n\nA. No crash interleaving can violate the invariant because rename is atomic; therefore protocol $P$ is already correct. No fsync calls are needed.\n\nB. Crash interleaving: The system crashes immediately after step $4$; the storage has persisted the rename of \"manifest.tmp\" to \"manifest\" but has not persisted the rename of \"obj.tmp\" to \"obj.v$k+1$\". On recovery, \"manifest\" names \"obj.v$k+1$\", which does not exist. Fix: Add a single fsync call on \"manifest\" after step $4$; no other fsync is necessary.\n\nC. Crash interleaving: The system crashes immediately after step $4$; due to out-of-order persistence, the rename in step $4$ reaches the storage device, but the rename in step $2$ does not. On recovery, \"manifest\" names \"obj.v$k+1$\", which is missing. Fix (minimal): Add fsync on the new object file descriptor before step $2$ to make its contents durable, then perform step $2$ and fsync the containing directory to make the \"obj.v$k+1$\" directory entry durable. Next, perform step $3$ and fsync the \"manifest.tmp\" file to make its contents durable before it can be published. Finally, perform step $4$. A final fsync of the directory after step $4$ is optional for durability of the return value but not required for preserving the invariant.\n\nD. Crash interleaving: The system crashes between steps $1$ and $2$. On recovery, the invariant is violated because \"manifest\" might name \"obj.v$k+1$\" while the new object is not present. Fix: Insert a single fsync on \"obj.tmp\" after step $1$.\n\nE. Crash interleaving: The system crashes immediately after step $4$, and although both renames persist, the contents of \"manifest\" are empty because the write to \"manifest.tmp\" was not made durable. Fix: Open both \"obj.tmp\" and \"manifest.tmp\" with the synchronous-write flag and omit all fsync calls; synchronous writes alone are sufficient.",
            "solution": "We begin from the stated base semantics. A write system call only guarantees that the data reach the page cache; the operating system may push dirty pages to the storage device later, potentially reordering writes across different files. A rename system call is atomic with respect to the directory namespace in the sense that readers never observe a partially updated directory entry; however, rename does not guarantee durability of the new directory entry unless the containing directory is synchronized with fsync. The File Synchronization system call (fsync) on a regular file flushes the file’s data and metadata to the storage device. To make directory entries such as creates, unlinks, and renames durable, one must fsync the directory that contains those entries. Lastly, to prevent exposing a file whose contents are not yet durable, one must ensure the file’s contents are made durable before publishing a reference to it in the namespace.\n\nThe invariant requires that after any crash, the manifest contains exactly the basename of an existing, fully written object file. A natural failure mode to look for is a publish-order anomaly: a reference (the manifest) becomes durable before the referred-to object (the new object file) exists durably, or the manifest’s contents themselves are not durable when the rename makes them visible.\n\nOption-by-option analysis:\n\n- Option A: This claims protocol $P$ is safe because rename is atomic. This confuses atomicity with durability. Protocol $P$ performs two renames in steps $2$ and $4$ across two different files with no ordering or durability guarantees. Due to out-of-order persistence, the rename of \"manifest.tmp\" to \"manifest\" can reach the device while the rename of \"obj.tmp\" to \"obj.v$k+1$\" has not, leaving a manifest that names a non-existent object after a crash. Therefore the claim that no crash can violate the invariant is false. Verdict — Incorrect.\n\n- Option B: The interleaving is plausible: the rename of the manifest can persist while the rename of the object does not, which yields \"manifest\" naming a missing \"obj.v$k+1$\". However, the proposed fix is to add only a single fsync on \"manifest\" after step $4$. This does not address the core hazard. Fsync on \"manifest\" flushes the content of the manifest file and its metadata; it does not make the directory-entry change durable unless the containing directory is fsynced. More importantly, it does nothing to ensure that \"obj.v$k+1$\" either exists or is durable by the time the manifest names it; the manifest could still become durable before the \"obj.v$k+1$\" directory entry exists on disk. Hence the invariant can still be violated. Verdict — Incorrect.\n\n- Option C: The interleaving described matches the realistic hazard: step $4$ persists, step $2$ does not. The proposed fix uses three fsync calls placed at the minimal publication boundaries:\n  - Fsync the new object file before publishing it via rename in step $2$. This ensures the object’s data and metadata are durable.\n  - After step $2$, fsync the containing directory to make the \"obj.v$k+1$\" directory entry durable. Now, even if the manifest later becomes visible, it will refer to an object that exists durably.\n  - Before publishing the new manifest via rename in step $4$, fsync the \"manifest.tmp\" file to ensure the manifest’s contents (the pathname \"obj.v$k+1$\") are durable before exposure. This prevents a crash from leaving a zero-length or corrupted new manifest after the rename reaches the device.\n  - A final directory fsync after step $4$ is not required to preserve the invariant: if it does not occur and a crash happens, the system will either retain the old manifest (safe) or may have already persisted the new manifest whose contents are durable by construction and which points to a durably present object (also safe). A final directory fsync is only necessary if the application needs to guarantee that the update is durable at the moment it returns success.\n  This placement is minimal with respect to the invariant: omitting any one of these fsync calls can reintroduce a window where a durable manifest names a non-durable or missing object, or where a durable rename exposes a manifest whose contents are not durable. Verdict — Correct.\n\n- Option D: The interleaving posits a crash between steps $1$ and $2$ and claims the invariant is violated. Between steps $1$ and $2$, nothing has been published: the old manifest still names \"obj.v$k$\", and the new object is only in \"obj.tmp\". A crash at this point leaves the old state intact. Therefore no invariant violation arises here. Furthermore, the proposed fix (fsync \"obj.tmp\" after step $1$) is unnecessary for correctness at that point because the temporary file is not yet reachable through the namespace; it addresses neither a correctness gap nor a publication boundary. Verdict — Incorrect.\n\n- Option E: The interleaving identifies a real hazard concerning manifest contents: if the write to \"manifest.tmp\" is not durable, a rename can expose a manifest whose contents are empty or stale. However, the proposed fix (opening with synchronous-write flags and omitting fsync) is insufficient. Synchronous writes on the regular files do not make directory-entry changes durable; thus, directory renames can still be lost, producing a manifest that names a missing object or an object that is not yet durably present. Additionally, relying solely on synchronous writes does not ensure that the object’s contents are durable before it is published via rename unless the ordering is carefully controlled and the directory is synchronized. Therefore this fix does not meet the invariant for all crash interleavings. Verdict — Incorrect.\n\nTherefore, only Option C both identifies a concrete violating interleaving and supplies a correct minimal fsync placement that fixes protocol $P$ with respect to the stated invariant.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}