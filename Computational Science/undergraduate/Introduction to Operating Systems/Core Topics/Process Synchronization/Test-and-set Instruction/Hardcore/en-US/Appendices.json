{
    "hands_on_practices": [
        {
            "introduction": "While the `test-and-set` instruction guarantees atomicity for a single lock, building correct concurrent programs often requires managing multiple locks. This practice explores a classic and dangerous scenario where two threads attempt to acquire two separate locks in opposite orders. By analyzing this thought experiment , you will gain a first-principles understanding of the conditions that lead to deadlock and learn about the most fundamental technique to prevent it: enforcing a global lock acquisition order.",
            "id": "3686956",
            "problem": "Consider a concurrent program in an operating system (OS) where two spinlocks implement mutual exclusion using the hardware test-and-set instruction. The test-and-set instruction atomically reads a memory location and sets it to a specified value, returning the previous value; when used to build a lock, a thread spins until the instruction returns an \"unlocked\" value and then sets the lock to the \"locked\" value. Let there be two locks, $L_A$ and $L_B$, protecting two distinct resources, $R_A$ and $R_B$, respectively. Assume 2 threads, $T_1$ and $T_2$, scheduled on 2 Central Processing Units (CPU), and that both threads occasionally require joint access to both $R_A$ and $R_B$. Due to legacy module boundaries, $T_1$ acquires $L_A$ first and then attempts $L_B$ when it needs both; symmetrically, $T_2$ acquires $L_B$ first and then attempts $L_A$ when it needs both. Once a thread holds a lock, it will not release it until it completes its critical section for that resource, and locks are only released voluntarily by the holding thread.\n\nFrom first principles, reason about the atomicity of test-and-set, mutual exclusion, and deadlock conditions (including the possibility of circular wait), and select all statements that are true in this scenario.\n\nA. With $T_1$ attempting $L_A$ then $L_B$, and $T_2$ attempting $L_B$ then $L_A$, a deadlock is possible because mutual exclusion, hold-and-wait, no preemption, and circular wait can all hold simultaneously.\n\nB. The atomicity of test-and-set across a single lock implicitly makes the acquisition of multiple locks atomic, which prevents circular wait and thereby deadlock regardless of acquisition order.\n\nC. Enforcing a global total order on lock acquisition (for example, requiring all threads to acquire $L_A$ before $L_B$ whenever both are needed) eliminates the circular wait condition and thus prevents deadlock.\n\nD. Replacing test-and-set with Compare-And-Swap (CAS) for implementing the locks, while keeping the same acquisition pattern, eliminates deadlock because CAS is a stronger atomic primitive.\n\nE. Even if all threads obey a global lock ordering, starvation cannot occur because test-and-set spinlocks guarantee bounded waiting under contention.",
            "solution": "Begin with the fundamental definitions relevant to this scenario. The hardware test-and-set instruction provides atomicity at the level of a single memory location: a single thread can atomically read and set a lock variable, ensuring mutual exclusion for that lock. A spinlock built from test-and-set causes a thread to busy-wait until the lock appears free, at which point the atomic instruction sets the lock to the \"locked\" state. Deadlock analysis employs the classical necessary conditions for deadlock (often attributed to Coffman): mutual exclusion, hold-and-wait, no preemption, and circular wait. If all four conditions are present simultaneously, deadlock can occur.\n\nAnalyze the given acquisition pattern. Thread $T_1$ acquires $L_A$ and then attempts to acquire $L_B$; thread $T_2$ acquires $L_B$ and then attempts to acquire $L_A$. Because locks are released only voluntarily at the end of a critical section, the operating system does not forcibly preempt resource ownership.\n\nEvaluate each option:\n\nA. With $T_1$ attempting $L_A$ then $L_B$, and $T_2$ attempting $L_B$ then $L_A$, a deadlock is possible because mutual exclusion, hold-and-wait, no preemption, and circular wait can all hold simultaneously. Derivation: Mutual exclusion holds because each lock $L_A$ and $L_B$ only permits one thread at a time in its critical section. Hold-and-wait holds because $T_1$ can hold $L_A$ while waiting for $L_B$, and simultaneously $T_2$ can hold $L_B$ while waiting for $L_A$. No preemption holds because the operating system will not forcibly remove a lock from a thread; locks are released only when the holder voluntarily exits its critical section. Circular wait holds because $T_1$ waits for $L_B$ (held by $T_2$) and $T_2$ waits for $L_A$ (held by $T_1$), creating a cycle in the wait-for graph. When these four conditions are present, a deadlock is possible. Therefore, option A is Correct.\n\nB. The atomicity of test-and-set across a single lock implicitly makes the acquisition of multiple locks atomic, which prevents circular wait and thereby deadlock regardless of acquisition order. Derivation: Atomicity of test-and-set applies to the operation on one memory location, not to a sequence of operations across multiple distinct locks. Acquiring $L_A$ and then $L_B$ is not a single atomic transaction; a thread may acquire $L_A$ successfully and then spin indefinitely for $L_B$. The possibility of interleaving between threads on different locks remains, and circular wait can arise if acquisition orders are inconsistent. Hence multi-lock atomicity is not implied by single-lock atomicity, and this statement is Incorrect.\n\nC. Enforcing a global total order on lock acquisition (for example, requiring all threads to acquire $L_A$ before $L_B$ whenever both are needed) eliminates the circular wait condition and thus prevents deadlock. Derivation: Impose a strict ordering relation, say $L_A \\prec L_B$, and require that any thread needing both locks acquires them in increasing order. Suppose, for contradiction, that a circular wait occurs under this policy. In a cycle, each thread holds a lock and waits for a higher-ordered lock. But with a strict total order, there cannot be a cycle in the wait-for graph: a thread waiting for a higher-ordered lock cannot be part of a cycle that returns to a lower-ordered lock, because the order is acyclic. Therefore, circular wait is eliminated, and one of the deadlock necessary conditions is broken; deadlock cannot occur from this cause. Option C is Correct.\n\nD. Replacing test-and-set with Compare-And-Swap (CAS) for implementing the locks, while keeping the same acquisition pattern, eliminates deadlock because CAS is a stronger atomic primitive. Derivation: Compare-And-Swap (CAS) is an atomic instruction that updates a memory location only if it holds an expected value, but like test-and-set it provides atomicity for single memory locations. The deadlock described arises from the resource acquisition policy (inconsistent lock ordering), not from a deficiency in the atomic primitive. Keeping the same acquisition pattern preserves mutual exclusion, hold-and-wait, no preemption, and circular wait; CAS does not make multi-lock acquisition atomic nor does it break circular wait. Therefore, option D is Incorrect.\n\nE. Even if all threads obey a global lock ordering, starvation cannot occur because test-and-set spinlocks guarantee bounded waiting under contention. Derivation: Test-and-set spinlocks do not guarantee bounded waiting or fairness; a thread can, in principle, spin indefinitely while other threads repeatedly acquire and release the lock (for example, under heavy contention without a fairness mechanism). Global lock ordering prevents deadlock by eliminating circular wait, but it does not introduce fairness or bounded waiting. Starvation remains possible. Therefore, option E is Incorrect.\n\nIn summary, options A and C are correct; B, D, and E are incorrect.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "Once you have a correct locking protocol, the next challenge is performance. A naive spinlock, where threads repeatedly execute `test-and-set` when a lock is busy, can lead to a \"coherence storm\" on a multiprocessor system, crippling performance. This exercise  challenges you to think about how to make a spinlock \"smarter\" by exploring different backoff strategies, teaching you why adaptive, randomized algorithms are essential for managing contention and achieving high throughput.",
            "id": "3686949",
            "problem": "Consider a single shared spinlock implemented with the test-and-set (TAS) instruction on a cache-coherent multiprocessor. The test-and-set (TAS) instruction atomically reads a memory location and sets it to a locked value; if the previous value indicates the lock was free, the calling thread acquires the lock; otherwise, the attempt fails and the thread must try again. There are $N$ identical threads. Each thread alternates between a non-critical section of average duration $t_{n}$ and a critical section of average duration $t_{c}$ protected by the spinlock. A failed TAS attempt imposes coherence traffic and a stall cost of roughly $\\tau$ per attempt, and repeated concurrent TAS attempts can cause the lock’s cache line to ping-pong between cores, inflating the handoff time. Suppose the machine provides a Central Processing Unit (CPU) pause instruction such that waiting does not generate coherence traffic until the next TAS.\n\nYour task is to choose the backoff schedule that adapts to contention using only the number $k$ of consecutive failed TAS operations since the last success (resetting $k$ to $0$ on success), and to select the schedule whose throughput prediction is consistent with first principles. Throughput is defined as the steady-state rate of successful lock acquisitions per unit time, and is upper-bounded by $1/t_{c}$ in the absence of lock-transfer overheads.\n\nWhich option best specifies both:\n- a contention-adaptive backoff schedule based on $k$, and\n- a qualitatively correct prediction of how throughput behaves as $N$ varies from small to large?\n\nA. Exponential randomized backoff with floor and cap: after $k$ consecutive failures, wait a random time $W \\sim \\mathrm{Uniform}\\!\\left(0,\\, s_{\\min}\\cdot 2^{k}\\right)$, but cap the window so that $W \\le s_{\\max}$; reset to $s_{\\min}$ on success and add small random jitter at all times. Prediction: for small $N$ (e.g., $N \\le 4$), the floor $s_{\\min}$ keeps overhead negligible so throughput is close to the ideal $1/t_{c}$; for large $N$, the window expands until the expected number of contenders per effective slot is $O(1)$, so coherence storms are suppressed and lock handoff overhead remains $O(1)$, making throughput approach a constant close to $1/\\left(t_{c}+O(\\tau)\\right)$, essentially independent of $N$.\n\nB. Fixed deterministic delay: after each failure, wait exactly $s_{0}$ cycles (no randomness), independent of $k$. Prediction: as $N$ increases, the fixed delay causes attempts to become uniformly spaced, so throughput strictly increases with $N$ and can exceed $1/t_{c}$ because the fixed delay smooths contention.\n\nC. Linear randomized backoff without cap: after $k$ failures, wait $W \\sim \\mathrm{Uniform}\\!\\left(0,\\, s_{\\min}+c\\cdot k\\right)$ with no upper cap; reset to $s_{\\min}$ on success. Prediction: for large $N$, this avoids over-throttling compared to exponential backoff and therefore produces higher throughput than exponential backoff; throughput continues to improve slowly with $N$ because the average window expands only linearly in $k$.\n\nD. Additive-decrease, multiplicative-increase with deterministic steps: on each failure, multiply the current wait by a factor $\\beta1$; on each success, subtract a fixed $\\delta0$ from the current wait (but not below $0$); no randomness. Prediction: the scheme self-tunes so that the average window size is proportional to $N$, yielding throughput $\\approx \\left(1-\\Theta\\!\\left(1/N\\right)\\right)/t_{c}$ at large $N$, improving as $N$ grows due to better tuning.\n\nSelect the correct option(s).",
            "solution": "The core of the problem is to manage contention for a single shared resource (the spinlock) among $N$ competing threads.\n\n**First Principles of Contention Control:**\n1.  **Contention Problem**: When the lock is released, multiple waiting threads may attempt a TAS instruction simultaneously. On a cache-coherent system, this leads to a \"coherence storm,\" where the cache line containing the lock variable is repeatedly invalidated and fetched across the interconnect. This generates massive amounts of bus/interconnect traffic and significantly increases the latency of the lock handoff. In the worst case, throughput can collapse as $N$ increases.\n2.  **Role of Backoff**: To mitigate this, threads should wait (or \"back off\") for a period of time after a failed TAS attempt before retrying. This is done using the `pause` instruction to avoid consuming execution resources and generating traffic while waiting.\n3.  **Properties of an Effective Backoff Strategy**:\n    *   **Adaptivity**: The waiting time should increase with the level of contention. A local heuristic for contention is the number of consecutive failed attempts, $k$.\n    *   **Randomization**: Waiting times should be randomized. If all threads use the same deterministic backoff delay, they will likely retry in lockstep, leading to repeated cycles of mass attempts and collisions. Randomization breaks this symmetry.\n4.  **Throughput Characteristics**:\n    *   The absolute maximum throughput is $1/t_c$, achievable only if the lock transfer overhead is zero.\n    *   For small $N$, contention is low, so the backoff-induced overhead should be minimal. Throughput should be close to $1/t_c$.\n    *   For large $N$, an effective backoff algorithm should stabilize the system. It should ensure that, on average, only a small, constant number of threads are actively trying to acquire the lock when it is released. This keeps the lock handoff overhead bounded, i.e., $O(1)$ with respect to $N$. Consequently, the total time per acquisition becomes $t_c + \\text{constant_overhead}$. The throughput should thus saturate and approach a constant value less than $1/t_c$. It should *not* continue to decrease towards zero, nor should it continue to increase with $N$.\n\n### Option-by-Option Analysis\n\n**A. Exponential randomized backoff with floor and cap... Prediction: ...throughput approach a constant close to $1/\\left(t_{c}+O(\\tau)\\right)$...**\n- **Schedule**: The proposed schedule is truncated randomized exponential backoff. The wait time $W$ is drawn from a uniform distribution $\\mathrm{Uniform}\\!\\left(0,\\, s_{\\min}\\cdot 2^{k}\\right)$ and is capped by $s_{\\max}$. This mechanism is adaptive (wait time grows exponentially with $k$), randomized (breaks symmetry), and bounded (avoids pathological delays). This is the canonical and highly effective strategy for contention control, used in contexts like Ethernet's CSMA/CD protocol.\n- **Prediction**: The prediction is qualitatively correct.\n    - For small $N$, contention is low, $k$ is small, backoff is minimal, and throughput is near the ideal $1/t_c$.\n    - For large $N$, the exponential increase in the backoff window size effectively throttles the number of active contenders, preventing coherence storms. This leads to a stable, bounded lock handoff time, which is characterized as $O(1)$ or, more specifically, related to the cost of a few failed attempts, $O(\\tau)$. The total time per critical section execution becomes $t_c + \\text{constant overhead}$. Therefore, throughput saturates at a constant value, $1/(t_c + \\text{constant overhead})$, which is consistent with $1/(t_c + O(\\tau))$.\n- **Verdict**: **Correct**.\n\n**B. Fixed deterministic delay... Prediction: ...throughput strictly increases with $N$ and can exceed $1/t_{c}$...**\n- **Schedule**: A fixed, deterministic delay $s_0$ is used. This is flawed. It is not adaptive to the level of contention. More importantly, its deterministic nature is a critical weakness. If multiple threads fail simultaneously, they will wait for the same duration $s_0$ and retry in lockstep, causing another collision.\n- **Prediction**: The prediction is fundamentally flawed.\n    - \"...throughput strictly increases with aN$\": This is incorrect. As $N$ grows, contention increases, and a non-adaptive, deterministic scheme is likely to lead to performance degradation, not improvement.\n    - \"...can exceed $1/t_c$\": This violates a first principle. The critical section itself imposes a serial bottleneck; it can only be executed by one thread at a time, for an average duration of $t_c$. The maximum rate of completion is therefore $1/t_c$. No backoff scheme can increase throughput beyond this physical limit.\n- **Verdict**: **Incorrect**.\n\n**C. Linear randomized backoff without cap... Prediction: ...produces higher throughput than exponential backoff; throughput continues to improve slowly with $N$...**\n- **Schedule**: The schedule, $W \\sim \\mathrm{Uniform}\\!\\left(0,\\, s_{\\min}+c\\cdot k\\right)$, is adaptive and randomized. Linear backoff is a possible strategy.\n- **Prediction**: The prediction is questionable.\n    - \"...avoids over-throttling...produces higher throughput than exponential backoff\": This is not a general truth. While an untuned exponential backoff can over-throttle, a well-tuned one is known to be robust. Linear backoff may not increase its delay fast enough to quell very high contention, potentially leading to *lower* throughput than exponential backoff.\n    - \"...throughput continues to improve slowly with $N$\": This is incorrect for a saturated system. Once the number of threads $N$ is large enough to ensure there is always at least one thread waiting to enter the critical section, adding more threads ($N'  N$) cannot increase throughput. At best, an ideal backoff scheme will maintain a constant saturation throughput.\n- **Verdict**: **Incorrect**.\n\n**D. Additive-decrease, multiplicative-increase with deterministic steps... Prediction: ...throughput $\\approx \\left(1-\\Theta\\!\\left(1/N\\right)\\right)/t_{c}$ at large $N$, improving as $N$ grows...**\n- **Schedule**: This describes a multiplicative-increase, additive-decrease (MIAD) scheme. On failure, the wait time is multiplied by $\\beta  1$; on success, it is reduced by $\\delta  0$. The key flaw is that it is \"deterministic\". Like in option B, this lack of randomness invites synchronized retries and collisions.\n- **Prediction**: The prediction is physically unrealistic.\n    - The throughput form $\\approx \\left(1-\\Theta\\!\\left(1/N\\right)\\right)/t_{c}$ implies that as $N \\to \\infty$, the term $\\Theta(1/N) \\to 0$, and thus throughput approaches the ideal limit of $1/t_c$. This suggests that the lock transfer overhead vanishes as contention becomes infinite, which is impossible. There will always be some non-zero overhead for lock acquisition.\n    - It also claims throughput \"improving as $N$ grows,\" which, as explained for option C, is not the expected behavior for a saturated system.\n- **Verdict**: **Incorrect**.\n\nBased on the analysis, Option A provides both a standard, effective algorithm (truncated randomized exponential backoff) and a qualitatively correct prediction of its performance characteristics (saturation to a constant throughput at high contention).",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Effective performance engineering relies on precise measurement. To justify optimizations like the backoff algorithms from the previous practice, we must be able to quantify their impact. This exercise  puts you in the role of a systems researcher, tasking you with designing a microbenchmark to isolate and measure the specific hardware cost of cache-line contention, a phenomenon that is invisible in the source code but has a massive impact on performance.",
            "id": "3686907",
            "problem": "An operating systems laboratory assignment asks you to design a microbenchmark to quantitatively separate the intrinsic cost of the atomic test-and-set instruction from the cost due to cache-line movement (often called cache-line bouncing) under contention. You have a symmetric multiprocessor with a uniform memory access architecture and a strong memory ordering model. You can pin threads to specific cores. You can read a cycle counter such as the Time Stamp Counter (TSC) via a serializing instruction on entry and exit from code regions. You can control thread count, placement, and lock data placement, and you can write simple spin loops. You want an experimental design that uses a fake lock that spins on a read-only line as a baseline to remove loop and branch overheads from the measurements.\n\nThe fundamental bases you may assume are the following. The atomic test-and-set instruction is an atomic read-modify-write that takes a cache line into an exclusive state and performs a write, incurring a read-for-ownership transfer when the line is shared across cores. In contrast, a pure load that reads a location that is not written by any thread can be served from a shared state without invalidating other caches. Under contention across cores, repeated atomic read-modify-write attempts on the same memory location cause the cache line to transfer between cores, while a single thread executing the same atomic instruction repeatedly on its own keeps the line locally owned and does not cause inter-core transfers. You can place the lock word alone on a cache line by padding.\n\nSuppose you plan to run with $P$ threads, where you may choose $P$ and their placement relative to the $C$ physical cores. Let $M$ be the total number of successful lock acquisitions recorded across all threads during a fixed-duration run. Let $T_{\\mathrm{cont}}$ be the total cycles measured for a contended test-and-set lock run across $P$ threads, $T_{\\mathrm{solo}}$ be the total cycles for a single-thread test-and-set run on the same lock variable, and $T_{\\mathrm{fake}}$ be the total cycles for a run of a fake lock that spins on a read-only word using the same loop structure and branch decisions but never performs an atomic read-modify-write nor any write. You must choose the arrangement of $P$ and $C$ and how to collect $T_{\\mathrm{cont}}$, $T_{\\mathrm{solo}}$, and $T_{\\mathrm{fake}}$, and propose a formula that estimates the per-acquisition cache-line bouncing overhead $B$ in cycles, separated from the intrinsic atomic instruction cost.\n\nWhich option best describes an experimental design and estimator that isolates the cache-line bouncing overhead using the fake-lock baseline, while controlling for confounders such as loop overhead and line placement?\n\nA. Pin $P$ threads to $P$ distinct physical cores with $P \\le C$. Place a single lock word on its own cache line via padding. For the contended run, use a pure test-and-set lock with no backoff: each thread loops performing an atomic test-and-set until it acquires, executes a fixed-size critical section that does not touch the lock’s cache line, then writes $0$ to release and repeats, measuring per-thread cycles and summing to get $T_{\\mathrm{cont}}$ and counting total acquisitions $M$. For the single-thread run, pin 1 thread alone on a core and have it acquire and release the same lock in a tight loop to obtain $T_{\\mathrm{solo}}$ for the same $M$. For the fake baseline, run the same loop body and branch structure, but replace the lock with a read-only flag that never changes and spin on a read-only cache line to obtain $T_{\\mathrm{fake}}$ for the same number of loop iterations. Estimate the per-acquisition cache-line bouncing overhead as $B \\approx \\left(\\frac{T_{\\mathrm{cont}} - T_{\\mathrm{fake}}}{M}\\right) - \\left(\\frac{T_{\\mathrm{solo}} - T_{\\mathrm{fake}}}{M}\\right)$, ensuring the lock line is isolated and threads are placed on distinct cores to maximize inter-core transfers.\n\nB. Run only two measurements on $P$ threads pinned to distinct cores: a contended test-and-set lock and a fake spin on a read-only flag. Compute $B \\approx \\frac{T_{\\mathrm{cont}} - T_{\\mathrm{fake}}}{M}$, because subtracting the fake baseline removes both loop overhead and the intrinsic atomic instruction cost, leaving only cache-line bouncing.\n\nC. Use test-and-test-and-set (TATAS) in the contended run, where threads spin on a read-only load until the lock appears free, then perform a single atomic test-and-set; compare against a fake spin that reads a never-changing flag. Set $B \\approx \\frac{T_{\\mathrm{cont}} - T_{\\mathrm{fake}}}{M}$. Single-thread measurement is unnecessary because TATAS already eliminates most bouncing.\n\nD. Pin all $P$ threads to the same physical core (using hardware threads if available), run a contended test-and-set to get $T_{\\mathrm{cont}}$, and a fake spin to get $T_{\\mathrm{fake}}$, and estimate $B \\approx \\frac{T_{\\mathrm{cont}} - T_{\\mathrm{fake}}}{M}$. Because the threads share the lowest-level cache, any residual difference is purely the intrinsic atomic instruction latency, so cache-line bouncing is zero by construction and can be inferred for multi-core runs by scaling with $P$.",
            "solution": "The goal is to estimate $B$, the per-acquisition cache-line bouncing overhead. This overhead arises from inter-core contention.\n\nLet us define the average cost per acquisition in the various experiments:\n*   $t_{\\mathrm{solo}} = T_{\\mathrm{solo}}/M$: The average cost per acquisition for a single thread. In this uncontended scenario, the lock's cache line is always owned by the core executing the thread. Each acquisition ideally involves a single `test-and-set` instruction on a 'hot' cache line. Therefore, this measurement is dominated by the intrinsic cost of the atomic instruction, plus any minimal, constant overhead from the loop structure and release operation.\n*   $t_{\\mathrm{cont}} = T_{\\mathrm{cont}}/M$: The average cost per acquisition under contention from $P$ threads on $P$ different cores. This cost includes the intrinsic cost of the successful `test-and-set`, but it is dominated by the cost of contention. For a pure `test-and-set` spinlock (without a read-first-test, i.e., not TATAS), every spin iteration in the wait loop is an atomic RMW (`test-and-set`). Under contention, each of these RMWs by a waiting thread will miss in its local cache and issue an RFO, causing the cache line to be transferred between cores. This is the \"bouncing\". The total time $T_{\\mathrm{cont}}$ is thus the sum of costs of the $M$ successful acquisitions and a large number of failed, spinning `test-and-set` attempts.\n\nThe total additional cost per acquisition due to contention is therefore $t_{\\mathrm{cont}} - t_{\\mathrm{solo}}$. For a pure `test-and-set` lock, this entire additional cost is attributable to the repeated RMWs on a shared line, i.e., the cache-line bouncing. The extra loop iterations *are* the bouncing events. Therefore, a direct and valid estimator for the average per-acquisition bouncing overhead is:\n$$B \\approx t_{\\mathrm{cont}} - t_{\\mathrm{solo}} = \\frac{T_{\\mathrm{cont}}}{M} - \\frac{T_{\\mathrm{solo}}}{M} = \\frac{T_{\\mathrm{cont}} - T_{\\mathrm{solo}}}{M}$$\nThe purpose of the $T_{\\mathrm{fake}}$ measurement is stated to be removal of \"loop and branch overheads\". In a rigorous differential measurement, one must subtract the appropriate baseline overhead from each measurement. However, the overhead in the contended case (many spin iterations) is vastly different from the solo case (one spin iteration). A single $T_{\\mathrm{fake}}$ measurement cannot be a correct baseline for both. The formula presented in option A, however, uses $T_{\\mathrm{fake}}$ in a way that it algebraically cancels, leading to the simple, direct estimator derived above. This suggests that the formula's structure is intended to represent a conceptual subtraction of a common baseline, which, when simplified, yields the correct estimator for this specific type of lock.\n\nNow, I will evaluate each option based on this derivation.\n\n**Option A. Pin $P$ threads to $P$ distinct physical cores... Estimate the per-acquisition cache-line bouncing overhead as $B \\approx \\left(\\frac{T_{\\mathrm{cont}} - T_{\\mathrm{fake}}}{M}\\right) - \\left(\\frac{T_{\\mathrm{solo}} - T_{\\mathrm{fake}}}{M}\\right)$, ensuring the lock line is isolated and threads are placed on distinct cores to maximize inter-core transfers.**\n\n*   **Experimental Design:** The design is sound. Pinning $P$ threads to $P$ distinct cores is the correct way to induce and measure inter-core contention. Placing the lock on its own cache line (padding) is a crucial step to prevent false sharing, which would confound the results. Using a pure `test-and-set` lock without backoff is correct for measuring the raw hardware penalty. The single-thread run provides the necessary uncontended baseline.\n*   **Estimator Formula:** The formula is $B \\approx \\frac{(T_{\\mathrm{cont}} - T_{\\mathrm{fake}}) - (T_{\\mathrm{solo}} - T_{\\mathrm{fake}})}{M}$. Algebraically, this simplifies to $B \\approx \\frac{T_{\\mathrm{cont}} - T_{\\mathrm{solo}}}{M}$. As derived above, this is a valid direct estimator for the per-acquisition bouncing overhead for a pure `test-and-set` lock. The convoluted form including the canceling $T_{\\mathrm{fake}}$ terms may be a flawed attempt to show cancellation of a hypothetical common overhead, but the resulting simplified expression is correct in this context. While the methodological value of a read-only spin `fake_lock` for a pure `test-and-set` lock is questionable (a TATAS lock would be a better match), this option provides the most coherent design and a correct resulting estimator among the choices.\n\n*   **Verdict:** **Correct**. This option describes the best experimental setup and provides a formula that correctly estimates the desired quantity, despite its convoluted presentation and the subtle methodological mismatch of the baseline.\n\n**Option B. Run only two measurements on $P$ threads pinned to distinct cores: a contended test-and-set lock and a fake spin on a read-only flag. Compute $B \\approx \\frac{T_{\\mathrm{cont}} - T_{\\mathrm{fake}}}{M}$, because subtracting the fake baseline removes both loop overhead and the intrinsic atomic instruction cost, leaving only cache-line bouncing.**\n\n*   **Analysis:** This approach is flawed. First, it omits the $T_{\\mathrm{solo}}$ measurement, which is essential for establishing the baseline cost of an uncontended acquisition (the intrinsic cost). Second, the claim that subtracting $T_{\\mathrm{fake}}$ removes the \"intrinsic atomic instruction cost\" is false. The fake lock, spinning on a read-only word, involves no atomic RMW instructions, so its measurement cannot account for the cost of one. The formula $B \\approx \\frac{T_{\\mathrm{cont}} - T_{\\mathrm{fake}}}{M}$ would estimate the sum of the intrinsic cost and the bouncing overhead ($C_{\\mathrm{intrinsic}} + B$), not $B$ alone.\n\n*   **Verdict:** **Incorrect**.\n\n**Option C. Use test-and-test-and-set (TATAS) in the contended run... compare against a fake spin that reads a never-changing flag. Set $B \\approx \\frac{T_{\\mathrm{cont}} - T_{\\mathrm{fake}}}{M}$. Single-thread measurement is unnecessary because TATAS already eliminates most bouncing.**\n\n*   **Analysis:** This option changes the experiment from `test-and-set` to `test-and-test-and-set` (TATAS). For a TATAS lock, a read-only fake spin is indeed a good baseline for the read-only spinning part. However, the formula proposed, $B \\approx \\frac{T_{\\mathrm{cont}} - T_{\\mathrm{fake}}}{M}$, is incorrect for the same reason as in Option B: it fails to subtract the intrinsic cost of the final `test-and-set` instruction and thus estimates $C_{\\mathrm{intrinsic}} + B$. Furthermore, the justification that \"TATAS already eliminates most bouncing\" is misleading. TATAS eliminates bouncing during the spin-wait phase but still incurs a burst of bouncing when multiple waiters see the lock as free and all attempt an atomic `test-and-set`. This burst is exactly the contention that needs to be measured. A single-thread measurement is still required to isolate the intrinsic cost.\n\n*   **Verdict:** **Incorrect**.\n\n**Option D. Pin all $P$ threads to the same physical core (using hardware threads if available), run a contended test-and-set to get $T_{\\mathrm{cont}}$, and a fake spin to get $T_{\\mathrm{fake}}$, and estimate $B \\approx \\frac{T_{\\mathrm{cont}} - T_{\\mathrm{fake}}}{M}$. Because the threads share the lowest-level cache, any residual difference is purely the intrinsic atomic instruction latency, so cache-line bouncing is zero by construction and can be inferred for multi-core runs by scaling with $P$.**\n\n*   **Analysis:** This experimental design is fundamentally flawed. The objective is to measure *inter-core* cache-line bouncing. By pinning all threads to the same physical core, they will share at least one level of cache (e.g., L1 and L2). This setup intentionally *prevents* the inter-core cache traffic that is the subject of the measurement. The claim that the cost can be \"inferred for multi-core runs by scaling with $P$\" is baseless; you cannot measure a phenomenon by ensuring it does not occur.\n\n*   **Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}