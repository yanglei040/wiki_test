{
    "hands_on_practices": [
        {
            "introduction": "To master the concept of Effective Memory Access Time ($EMAT$), we begin with the simplest possible scenario: an access pattern with perfect spatial locality. This exercise grounds your understanding in the fundamental definition of $EMAT$ as an expected value. By analyzing a case where the Translation Lookaside Buffer (TLB) hit rate approaches 100%, you will derive the best-case access time from first principles and see how it forms a baseline for all performance calculations .",
            "id": "3638183",
            "problem": "A system uses paged virtual memory with a Translation Lookaside Buffer (TLB). The TLB stores recent virtual-to-physical page translations and is consulted before data is fetched from main memory. Consider a program that, after a short warm-up, performs many successive memory references that all fall within a single virtual page. As a result, after the first successful translation, the TLB hit ratio $h$ for subsequent accesses satisfies $h \\approx 1$. The TLB lookup time is $t_{T} = 0.90$ ns, and the main memory access latency is $t_{m} = 72$ ns.\n\nStarting from first principles of expectation for latency conditioned on TLB hit versus miss, derive the leading-order effective memory access time for one memory reference in this edge-case regime. For this calculation, assume the hardware does not overlap the TLB lookup with the subsequent memory access on hits (that is, the memory access begins only after the TLB has produced the translation). Express your final numerical answer in nanoseconds and round to four significant figures. In your reasoning, explicitly articulate the role of the overlap assumption for this edge case without using any shortcut formulas or pre-stated expressions for effective memory access time.",
            "solution": "The Translation Lookaside Buffer (TLB) provides recent virtual-to-physical page translations, so references to addresses whose page translations are present in the TLB are TLB hits, while others are misses. The effective memory access time is the expected latency per memory reference when the process of address translation and data access is accounted for. The fundamental starting point for this derivation is the definition of expectation for a random variable conditioned on events. Let the random variable $X$ denote the latency of a single memory reference. The reference can be a TLB hit with probability $h$ or a TLB miss with probability $1 - h$. Therefore,\n$$\n\\mathbb{E}[X] = h \\cdot \\mathbb{E}[X \\mid \\text{hit}] + (1 - h) \\cdot \\mathbb{E}[X \\mid \\text{miss}] \\, .\n$$\n\nIn the regime described, all references after warm-up fall within a single virtual page. After the first translation is cached, this implies $h \\approx 1$, so $1 - h \\approx 0$. To leading order, the term involving misses is negligible in the expectation:\n$$\n\\mathbb{E}[X] \\approx \\mathbb{E}[X \\mid \\text{hit}] \\, .\n$$\n\nWe now determine $\\mathbb{E}[X \\mid \\text{hit}]$ under the stated microarchitectural assumption. The problem specifies that the hardware does not overlap the TLB lookup with the subsequent memory access on hits. That means on a TLB hit, the TLB must be consulted first to produce the physical address, and only then can the main memory access begin. Consequently, the latency for a hit is the sum of the TLB lookup time and the main memory access time:\n$$\n\\mathbb{E}[X \\mid \\text{hit}] = t_{T} + t_{m} \\, .\n$$\n\nSubstituting the given numerical values,\n$$\nt_{T} = 0.90 \\text{ ns}, \\quad t_{m} = 72 \\text{ ns},\n$$\nso\n$$\n\\mathbb{E}[X] \\approx t_{T} + t_{m} = 0.90 \\text{ ns} + 72 \\text{ ns} = 72.90 \\text{ ns}.\n$$\n\nRounded to four significant figures and expressed in nanoseconds, the result is 72.90 ns.\n\nDiscussion of the overlap assumption: If the hardware were able to overlap the TLB lookup with the memory access on hits (for example, through virtually-indexed, physically-tagged caches that allow the data access to proceed while the TLB produces the tag), then the hit-path latency would be dominated by the main memory access time, and one would have $\\mathbb{E}[X \\mid \\text{hit}] \\approx t_{m}$ rather than $t_{T} + t_{m}$. In the present problem, however, the explicit non-overlap assumption means the TLB latency is additive in the hit-path latency, yielding the derived leading-order result above. Because $h \\approx 1$, the difference between overlapped and non-overlapped designs in this edge case is precisely the additional $t_{T}$ on the critical path.",
            "answer": "$$\\boxed{72.90}$$"
        },
        {
            "introduction": "Building on the ideal case, this practice demonstrates how dramatically performance can change based on how a program accesses data in memory. This exercise vividly illustrates why understanding data access patterns is crucial for performance optimization. By comparing the $EMAT$ for sequential (stride-1) versus large-stride access over the same data array, you will quantify the significant performance penalty of poor spatial locality and learn to predict TLB behavior based on code structure .",
            "id": "3638194",
            "problem": "A system implements demand-paged virtual memory with a fully associative Data Translation Lookaside Buffer (DTLB), using least-recently-used replacement. The DTLB has $N = 64$ entries. The page size is $S = 4$ KiB. Consider a contiguous array of length $L = 128$ MiB containing $64$-bit integers (each element size $B = 8$ bytes). All pages of the array are present in physical memory; there are no page faults. Each memory access requires a DTLB lookup before the physical access can proceed. A DTLB hit introduces an additional lookup overhead of $t_{\\mathrm{TLB}} = 0.5$ ns before the memory access time of $t_{\\mathrm{mem}} = 60$ ns. A DTLB miss introduces, in addition to the $t_{\\mathrm{TLB}}$, an extra translation delay of $t_{\\mathrm{miss}} = 80$ ns for the page table walk and TLB refill prior to performing the memory access. Assume the cache hierarchy does not alter these timings and that the costs combine additively as described.\n\nCompare the Effective Memory Access Time ($EMAT$) per access under the following two data access patterns over the same array:\n- Pattern $(a)$: stride $1$, that is, the program reads each element exactly once in increasing index order, accessing elements $0, 1, 2, \\dots$ sequentially.\n- Pattern $(b)$: stride equal to the page size $S$, that is, the program reads only the first element of each page once, in increasing page order, accessing element indices $0, \\frac{S}{B}, 2\\frac{S}{B}, \\dots$ until the end of the array.\n\nYou are to compute the $EMAT$ for pattern $(a)$ and for pattern $(b)$ as two separate values. Round your answers to four significant figures. Express each $EMAT$ in nanoseconds.",
            "solution": "The Effective Memory Access Time ($EMAT$) is the weighted average of the time for a Translation Lookaside Buffer (TLB) hit and the time for a TLB miss. Let $h$ be the TLB hit rate. The time for a memory access after a TLB hit is the sum of the TLB lookup time, $t_{\\mathrm{TLB}}$, and the memory access time, $t_{\\mathrm{mem}}$.\n$$T_{\\mathrm{hit}} = t_{\\mathrm{TLB}} + t_{\\mathrm{mem}}$$\nThe time for a memory access after a TLB miss is the sum of the TLB lookup time, $t_{\\mathrm{TLB}}$, the additional page table walk penalty, $t_{\\mathrm{miss}}$, and the memory access time, $t_{\\mathrm{mem}}$.\n$$T_{\\mathrm{miss}} = t_{\\mathrm{TLB}} + t_{\\mathrm{miss}} + t_{\\mathrm{mem}}$$\nThe general formula for $EMAT$ is:\n$$EMAT = h \\cdot T_{\\mathrm{hit}} + (1-h) \\cdot T_{\\mathrm{miss}}$$\nSubstituting the expressions for $T_{\\mathrm{hit}}$ and $T_{\\mathrm{miss}}$:\n$$EMAT = h \\cdot (t_{\\mathrm{TLB}} + t_{\\mathrm{mem}}) + (1-h) \\cdot (t_{\\mathrm{TLB}} + t_{\\mathrm{miss}} + t_{\\mathrm{mem}})$$\nThis expression can be simplified by factoring out common terms:\n$$EMAT = (h + (1-h)) \\cdot (t_{\\mathrm{TLB}} + t_{\\mathrm{mem}}) + (1-h) \\cdot t_{\\mathrm{miss}}$$\n$$EMAT = t_{\\mathrm{TLB}} + t_{\\mathrm{mem}} + (1-h) \\cdot t_{\\mathrm{miss}}$$\nWe are given the following values:\n$t_{\\mathrm{TLB}} = 0.5$ ns\n$t_{\\mathrm{mem}} = 60$ ns\n$t_{\\mathrm{miss}} = 80$ ns\n\nFirst, let's establish the system parameters.\nThe page size is $S = 4 \\text{ KiB} = 4 \\times 2^{10} \\text{ bytes} = 4096 \\text{ bytes}$.\nThe element size is $B = 8 \\text{ bytes}$.\nThe number of elements that fit into a single page is:\n$$N_{\\mathrm{elements\\_per\\_page}} = \\frac{S}{B} = \\frac{4096}{8} = 512$$\nThe total length of the array is $L = 128 \\text{ MiB} = 128 \\times 2^{20} \\text{ bytes}$.\nThe total number of pages spanned by the array is:\n$$N_{\\mathrm{pages}} = \\frac{L}{S} = \\frac{128 \\times 2^{20}}{4 \\times 2^{10}} = 32 \\times 2^{10} = 32768$$\nThe total number of elements in the array is:\n$$N_{\\mathrm{elements}} = \\frac{L}{B} = \\frac{128 \\times 2^{20}}{8} = 16 \\times 2^{20} = 16777216$$\nThe number of DTLB entries is $N = 64$. Since $N_{\\mathrm{pages}} \\gg N$, the DTLB can only hold a small fraction of the total page translations for the array at any given time.\n\nNow, we analyze each access pattern to determine its TLB hit rate.\n\nPattern (a): Stride $1$ (sequential access)\nIn this pattern, the program reads array elements in the order $0, 1, 2, \\dots$. These accesses exhibit strong spatial locality.\nThe first access to any given page (e.g., to element $0$ on page $0$) will result in a compulsory TLB miss, as the translation for that page is not yet in the DTLB. This miss causes the page translation to be loaded into the DTLB.\nThe subsequent $N_{\\mathrm{elements\\_per\\_page}} - 1 = 512 - 1 = 511$ accesses to the same page will all be TLB hits, as the translation is now resident in the DTLB.\nWhen the program moves to the next page, the first access to this new page will again cause a TLB miss, and the cycle repeats.\nBecause the array is traversed sequentially, for every block of $512$ accesses corresponding to one page, there is exactly $1$ miss and $511$ hits. The LRU replacement policy does not negatively affect this pattern, as by the time a page's translation is evicted, it is no longer needed.\nThe hit rate for pattern (a), $h_a$, is the ratio of hits to total accesses within any given page traversal:\n$$h_a = \\frac{\\text{Number of hits per page}}{\\text{Number of accesses per page}} = \\frac{511}{512}$$\nThe miss rate is $1 - h_a = \\frac{1}{512}$.\nWe can now compute the $EMAT$ for pattern (a):\n$$EMAT_a = t_{\\mathrm{TLB}} + t_{\\mathrm{mem}} + (1-h_a) \\cdot t_{\\mathrm{miss}}$$\n$$EMAT_a = 0.5 \\text{ ns} + 60 \\text{ ns} + \\left(\\frac{1}{512}\\right) \\cdot 80 \\text{ ns}$$\n$$EMAT_a = 60.5 + 0.15625 = 60.65625 \\text{ ns}$$\nRounding to four significant figures, we get $EMAT_a = 60.66$ ns.\n\nPattern (b): Stride equal to page size $S$\nIn this pattern, the program reads the first element of each page in increasing page order. The indices of the accessed elements are $0, \\frac{S}{B}, 2\\frac{S}{B}, \\dots$.\nSince $\\frac{S}{B} = 512$, the accessed element indices are $0, 512, 1024, \\dots$.\nElement $0$ is on page $0$. Element $512$ is on page $1$. Element $1024$ is on page $2$, and so on.\nEvery single access is to a new, distinct page.\nThe sequence of page accesses is $0, 1, 2, \\dots, 32767$.\nLet's trace the DTLB behavior:\n- Access to page $0$: Miss. DTLB is populated with translation for page $0$.\n- Access to page $1$: Miss. DTLB is populated with translation for page $1$.\n- ...\n- Access to page $63$: Miss. DTLB is now full with translations for pages $0$ through $63$.\n- Access to page $64$: Miss. Since the DTLB is full and uses an LRU replacement policy, the entry for page $0$ (the least recently used) is evicted to make room for the translation for page $64$.\nThis continues for all subsequent accesses. Each access is to a page whose translation is not in the DTLB, because the pages are accessed in a sequence much longer than the DTLB capacity, with no reuse. Therefore, every access results in a DTLB miss.\nThe hit rate for pattern (b), $h_b$, is $0$.\nThe miss rate is $1 - h_b = 1$.\nWe can now compute the $EMAT$ for pattern (b):\n$$EMAT_b = t_{\\mathrm{TLB}} + t_{\\mathrm{mem}} + (1-h_b) \\cdot t_{\\mathrm{miss}}$$\n$$EMAT_b = 0.5 \\text{ ns} + 60 \\text{ ns} + (1) \\cdot 80 \\text{ ns}$$\n$$EMAT_b = 60.5 + 80 = 140.5 \\text{ ns}$$\nThis value is already given to four significant figures.\n\nThe final values are $EMAT_a = 60.66$ ns and $EMAT_b = 140.5$ ns.",
            "answer": "$$\\boxed{\\begin{pmatrix} 60.66 & 140.5 \\end{pmatrix}}$$"
        },
        {
            "introduction": "This final practice explores a pathological performance scenario where the access pattern is fundamentally at odds with the TLB's hardware design, a phenomenon known as thrashing. This advanced exercise challenges you to analyze a \"worst-case\" scenario where the reference stream causes constant evictions within a single TLB set. By dissecting this behavior, you will gain a deeper appreciation for the role of set-associativity and the mechanisms behind performance cliffs in memory systems .",
            "id": "3638178",
            "problem": "A processor uses virtual memory with hardware translation. It has a Translation Lookaside Buffer (TLB) that is set-associative with associativity $A$ and Least Recently Used (LRU) replacement. Consider a pair of disjoint virtual memory regions composed of pages $\\{P_{0},P_{1},\\dots,P_{A}\\}$ chosen so that all $A+1$ pages map to the same single TLB set. The processor repeatedly issues a round-robin reference stream that touches one word in each page in the order $P_{0},P_{1},\\dots,P_{A},P_{0},P_{1},\\dots$, indefinitely. Assume the following timing model and system properties.\n\n- A TLB probe takes $t_{\\mathrm{TLB}} = 0.5$ ns. If the TLB probe hits, the processor then performs the data access in the Level-1 (L1) data cache, which takes $t_{\\mathrm{L1}} = 1.0$ ns.\n- If the TLB probe misses, the hardware walks a two-level page table ($L=2$), with each level requiring one main-memory access of $t_{\\mathrm{MEM}} = 50$ ns. The page walk is serialized (no overlap between levels), its entries are not cached, and after the walk the TLB is updated. After translation completes, the processor performs the L1 data access taking $t_{\\mathrm{L1}} = 1.0$ ns.\n- There are no page faults (all pages are resident), and every data access hits in the L1 cache once the translation is available. TLB lookup must complete before the L1 access begins, so latencies are additive along the chosen path.\n- Ignore compulsory warm-up effects and assume steady-state behavior of the round-robin stream.\n\nUsing only the definitions of hit, miss, and average time across mutually exclusive cases, analyze the steady-state behavior of the given access pattern with respect to the TLB set, and compute the steady-state Effective Memory Access Time (EMAT) per memory reference under this pattern. Express your final answer in nanoseconds and round to four significant figures.",
            "solution": "The problem asks for the steady-state Effective Memory Access Time (EMAT) for a given memory reference pattern. The EMAT is the weighted average of the access times for Translation Lookaside Buffer (TLB) hits and TLB misses. The general formula for EMAT is:\n$$\n\\text{EMAT} = (h \\times T_{\\mathrm{hit}}) + (m \\times T_{\\mathrm{miss}})\n$$\nwhere $h$ is the TLB hit rate, $m$ is the TLB miss rate, $T_{\\mathrm{hit}}$ is the time for a memory access on a TLB hit, and $T_{\\mathrm{miss}}$ is the time for a memory access on a TLB miss. By definition, every access is either a hit or a miss, so $h + m = 1$.\n\nFirst, we determine the access times for a hit and a miss based on the provided latencies.\n\nThe time for a TLB hit, $T_{\\mathrm{hit}}$, includes the TLB probe time and the subsequent Level-1 (L1) data cache access time. The problem states these are additive.\n$$\nT_{\\mathrm{hit}} = t_{\\mathrm{TLB}} + t_{\\mathrm{L1}}\n$$\nGiven the values $t_{\\mathrm{TLB}} = 0.5$ ns and $t_{\\mathrm{L1}} = 1.0$ ns:\n$$\nT_{\\mathrm{hit}} = 0.5 \\text{ ns} + 1.0 \\text{ ns} = 1.5 \\text{ ns}\n$$\n\nThe time for a TLB miss, $T_{\\mathrm{miss}}$, includes the initial TLB probe time, the time for the hardware page table walk, and the final L1 data cache access time. The page table walk involves $L=2$ levels, with each level requiring a main memory access of time $t_{\\mathrm{MEM}}$. These accesses are serialized.\n$$\nT_{\\mathrm{miss}} = t_{\\mathrm{TLB}} + (L \\times t_{\\mathrm{MEM}}) + t_{\\mathrm{L1}}\n$$\nGiven the values $t_{\\mathrm{TLB}} = 0.5$ ns, $L = 2$, $t_{\\mathrm{MEM}} = 50$ ns, and $t_{\\mathrm{L1}} = 1.0$ ns:\n$$\nT_{\\mathrm{miss}} = 0.5 \\text{ ns} + (2 \\times 50 \\text{ ns}) + 1.0 \\text{ ns} = 0.5 \\text{ ns} + 100 \\text{ ns} + 1.0 \\text{ ns} = 101.5 \\text{ ns}\n$$\n\nNext, we must determine the steady-state hit rate $h$ and miss rate $m$. This depends on the specific access pattern and the TLB's organization. The system has a set-associative TLB with associativity $A$ and a Least Recently Used (LRU) replacement policy. The access pattern is a round-robin stream over $A+1$ pages, $\\{P_{0}, P_{1}, \\dots, P_{A}\\}$, all of which map to the *same* TLB set.\n\nLet us analyze the state of this single TLB set. The set has $A$ slots, or ways, to store page translations. The access stream consists of a repeating cycle of $A+1$ distinct page references. Because we are accessing $A+1$ pages that map to a set that can only hold $A$ entries, a conflict is guaranteed for every access once the set is full.\n\nConsider the system in steady state. Let's trace the access to an arbitrary page $P_i$ from the sequence. For the access to $P_i$ to be a hit, its translation must be resident in the TLB set. According to the LRU policy, the $A$ entries in the set will be the translations for the $A$ most recently used pages.\n\nThe access pattern is $P_{0}, P_{1}, \\dots, P_{A}, P_{0}, \\dots$. The sequence of accesses immediately preceding the reference to page $P_i$ consists of the $A$ distinct pages $\\{P_{i-1}, P_{i-2}, \\dots, P_{i-A}\\}$ (indices are taken modulo $A+1$). For example, before accessing $P_A$, the last $A$ accesses were to $P_{A-1}, P_{A-2}, \\dots, P_0$. Before accessing $P_0$ in the next cycle, the last $A$ accesses were to $P_A, P_{A-1}, \\dots, P_1$.\n\nIn general, at the time of access to page $P_i$, the $A$ slots in the TLB set are occupied by the translations for the $A$ most recently referenced pages, which are $\\{P_{i-1}, P_{i-2}, \\dots, P_{i-A}\\}$. The translation for page $P_i$ itself is not in this group. Consequently, the access to $P_i$ will result in a TLB miss.\n\nUpon this miss, the translation for $P_i$ is fetched. Due to the LRU policy, the entry evicted from the set will be the one corresponding to the least recently used page, which is $P_{i-A}$. The new entry for $P_i$ is then loaded. This cycle of misses and replacements repeats for every access in the sequence. Each page access evicts the translation that will be needed $A$ steps later, guaranteeing that it will not be present when its turn comes. This phenomenon is known as thrashing.\n\nTherefore, in the steady state, every single memory reference in this pattern will cause a TLB miss.\nThis leads to a TLB hit rate $h=0$ and a TLB miss rate $m=1$.\n\nWe can now compute the EMAT using these rates:\n$$\n\\text{EMAT} = (0 \\times T_{\\mathrm{hit}}) + (1 \\times T_{\\mathrm{miss}})\n$$\n$$\n\\text{EMAT} = T_{\\mathrm{miss}}\n$$\nSubstituting the calculated value for $T_{\\mathrm{miss}}$:\n$$\n\\text{EMAT} = 101.5 \\text{ ns}\n$$\nThe problem requires the answer to be rounded to four significant figures. The value $101.5$ already has four significant figures.",
            "answer": "$$\\boxed{101.5}$$"
        }
    ]
}