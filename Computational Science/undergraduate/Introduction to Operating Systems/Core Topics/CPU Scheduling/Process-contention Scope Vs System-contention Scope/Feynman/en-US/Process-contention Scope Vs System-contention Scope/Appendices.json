{
    "hands_on_practices": [
        {
            "introduction": "Understanding the theoretical performance of a multithreaded application often begins by analyzing how it behaves under load. This first exercise guides you through modeling the performance slowdown when a system has more runnable threads than available CPU cores, a common scenario known as oversubscription. By deriving the slowdown factor from first principles, you will quantify how scheduling overhead under both Process-Contention Scope (PCS) and System-Contention Scope (SCS) impacts overall throughput, revealing the trade-offs tied to context-switching costs .",
            "id": "3672426",
            "problem": "You are given a model to reason about how Process-Contention Scope (PCS) and System-Contention Scope (SCS) scheduling affect execution time when there is oversubscription. From first principles, use the following base definitions and facts to derive a quantitative slowdown and implement it in a program.\n\nBase definitions and facts:\n- Process-Contention Scope (PCS): scheduling of user-level threads within a single process by a user-level library, where the kernel schedules only a smaller set of kernel-visible execution entities (for example, one per central processing unit (CPU)). The user-level library performs context switches among its user threads.\n- System-Contention Scope (SCS): scheduling performed by the operating system kernel among kernel-visible threads across all processes on the system, with kernel-aware load balancing distributing runnable threads across CPUs.\n- Round-robin time sharing: each runnable thread receives a time quantum of length $q$ on a CPU before being preempted. A context switch incurs a fixed overhead $s$ during which no productive work is performed. For PCS, denote the overhead by $s_{\\mathrm{PCS}}$; for SCS, denote the overhead by $s_{\\mathrm{SCS}}$.\n- Identical workloads: Each of $N$ identical threads requires $W$ units of pure compute time. Assume a normalized CPU speed of $1$ unit of work per second per CPU, so $W$ is measured in seconds of ideal compute time.\n- Fair sharing and conservation of CPU time: Under round-robin with identical threads and perfect load balancing, each CPU divides its available productive time equally among the runnable threads assigned to it. The total productive rate per CPU is reduced by the fraction of time spent in context switch overhead. Idle time is zero when there are runnable threads available.\n\nSetup:\n- Let the number of CPUs be $C$ and set the number of threads $N = 2C$ to create oversubscription. Assume kernel-aware load balancing under SCS keeps the per-CPU runnable thread count equal across CPUs.\n- For PCS, assume $C$ kernel-visible execution entities are pinned to $C$ CPUs and the user-level library multiplexes $N$ user threads across these $C$ entities using round-robin with quantum $q$ and overhead $s_{\\mathrm{PCS}}$ per user-level context switch.\n- For SCS, assume the kernel multiplexes the $N$ kernel-visible threads across the $C$ CPUs using round-robin with quantum $q$ and overhead $s_{\\mathrm{SCS}}$ per kernel-level context switch, with kernel-aware load balancing eliminating additional migration penalties.\n\nTask:\n1. Starting only from the base definitions and facts above, logically derive the completion time $T_{\\mathrm{PCS}}$ for a single thread under PCS and $T_{\\mathrm{SCS}}$ for a single thread under SCS when $N=2C$, in terms of $W$, $q$, $s_{\\mathrm{PCS}}$, $s_{\\mathrm{SCS}}$, $N$, and $C$. Define the slowdown as $\\delta = T/T_{\\mathrm{ideal}}$, where $T_{\\mathrm{ideal}}$ is the completion time of a single thread when $N=C$ with no time slicing overhead. Express $\\delta_{\\mathrm{PCS}}$ and $\\delta_{\\mathrm{SCS}}$ as dimensionless ratios.\n2. Implement a complete, runnable program that computes $\\delta_{\\mathrm{PCS}}$ and $\\delta_{\\mathrm{SCS}}$ for the following test suite. Treat all times $q$, $s_{\\mathrm{PCS}}$, and $s_{\\mathrm{SCS}}$ in seconds, and $W$ in seconds of ideal compute time. Your program must not use any randomness or operating system concurrency; it must be purely computational.\n3. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with values ordered as $[\\delta_{\\mathrm{PCS}}^{(1)},\\delta_{\\mathrm{SCS}}^{(1)},\\delta_{\\mathrm{PCS}}^{(2)},\\delta_{\\mathrm{SCS}}^{(2)},\\dots]$, rounded to six decimal places.\n\nTest suite (with $N=2C$ in each case):\n- Case 1 (happy path): $C=4$, $N=8$, $W=1.0$, $q=0.010$, $s_{\\mathrm{PCS}}=0.0005$, $s_{\\mathrm{SCS}}=0.0010$.\n- Case 2 (zero overhead boundary): $C=4$, $N=8$, $W=2.5$, $q=0.010$, $s_{\\mathrm{PCS}}=0.0000$, $s_{\\mathrm{SCS}}=0.0000$.\n- Case 3 (small quantum, noticeable overhead): $C=2$, $N=4$, $W=1.0$, $q=0.0010$, $s_{\\mathrm{PCS}}=0.0005$, $s_{\\mathrm{SCS}}=0.0005$.\n- Case 4 (single CPU boundary): $C=1$, $N=2$, $W=3.0$, $q=0.0200$, $s_{\\mathrm{PCS}}=0.0008$, $s_{\\mathrm{SCS}}=0.0010$.\n- Case 5 (larger system, mild overhead imbalance): $C=8$, $N=16$, $W=0.5$, $q=0.0050$, $s_{\\mathrm{PCS}}=0.0002$, $s_{\\mathrm{SCS}}=0.0006$.\n\nAnswer format:\n- Output exactly one line in the format $[\\text{float},\\text{float},\\dots]$, with no spaces, and each float rounded to six decimal places as specified.",
            "solution": "The task is to derive a quantitative model for the performance slowdown introduced by Process-Contention Scope (PCS) and System-Contention Scope (SCS) scheduling under a specific oversubscription scenario. The derivation must proceed from the provided first principles.\n\nFirst, we establish the baseline for comparison, the ideal completion time, $T_{\\mathrm{ideal}}$. This is defined as the time taken for one thread to complete its work when the number of threads $N$ equals the number of CPUs $C$, and there is no overhead from time slicing. In this case, each of the $C$ threads is assigned to one of the $C$ CPUs and runs to completion without interruption. Given that each thread requires $W$ seconds of pure compute time and the CPU has a normalized speed of $1$ unit of work per second, the ideal completion time is simply:\n$$ T_{\\mathrm{ideal}} = W $$\n\nNext, we analyze the performance of a CPU under round-robin scheduling with overhead. Consider a single CPU scheduling $k$ threads. A full scheduling cycle involves giving a quantum $q$ to each of the $k$ threads, with each grant of the CPU followed by a context switch of duration $s$. The total time for one full cycle is the sum of the time for all quanta and all context switches:\n$$ T_{\\mathrm{cycle}} = k \\cdot q + k \\cdot s = k(q+s) $$\nThe productive work done during this cycle is the sum of the time spent executing threads' code:\n$$ W_{\\mathrm{cycle}} = k \\cdot q $$\nThe effective service rate, $\\eta$, of the CPU is the fraction of time it spends on productive work. This is the ratio of productive time to total cycle time:\n$$ \\eta = \\frac{W_{\\mathrm{cycle}}}{T_{\\mathrm{cycle}}} = \\frac{k \\cdot q}{k(q+s)} = \\frac{q}{q+s} $$\nThis productive fraction $\\eta$ represents the portion of the CPU's total time that translates into useful computation. Note that this rate is independent of the number of threads $k$ being scheduled, provided $k \\ge 1$.\n\nNow, we apply this model to the two scheduling scopes for the given setup of $N=2C$ threads on $C$ CPUs.\n\nFor System-Contention Scope (SCS), the operating system kernel schedules all $N=2C$ threads across the $C$ CPUs. The problem states that kernel-aware load balancing distributes these threads evenly. Thus, each of the $C$ CPUs has $k = N/C = 2C/C = 2$ threads to manage. These are kernel-visible threads, so each context switch incurs the kernel-level overhead $s_{\\mathrm{SCS}}$. The effective service rate of each CPU is therefore:\n$$ \\eta_{\\mathrm{SCS}} = \\frac{q}{q + s_{\\mathrm{SCS}}} $$\nThis productive rate is shared equally between the $2$ threads on the CPU due to the fair sharing policy. The rate at which an individual thread can get its work done is half of the CPU's effective rate:\n$$ \\text{Rate}_{\\text{per thread, SCS}} = \\frac{1}{2} \\eta_{\\mathrm{SCS}} = \\frac{1}{2} \\left( \\frac{q}{q + s_{\\mathrm{SCS}}} \\right) $$\nThe total time $T_{\\mathrm{SCS}}$ required for one thread to complete its workload $W$ is the work divided by the rate at which it is serviced:\n$$ T_{\\mathrm{SCS}} = \\frac{W}{\\text{Rate}_{\\text{per thread, SCS}}} = \\frac{W}{\\frac{1}{2} \\left( \\frac{q}{q + s_{\\mathrm{SCS}}} \\right)} = \\frac{2W(q + s_{\\mathrm{SCS}})}{q} $$\n\nFor Process-Contention Scope (PCS), the setup involves $C$ kernel-visible entities pinned to the $C$ CPUs. The operating system sees one entity per CPU and thus performs no time-slicing at the kernel level. The user-level library is responsible for scheduling the $N=2C$ user threads on these $C$ kernel entities. Assuming the library distributes them evenly, each kernel entity (and by extension, each CPU) is responsible for scheduling $k = N/C = 2$ user threads. This user-level scheduling uses a quantum $q$ and has a context-switch overhead of $s_{\\mathrm{PCS}}$. The logical structure is identical to the SCS case, but the overhead parameter is different.\nThe effective service rate of a CPU, as mediated by the user-level scheduler, is:\n$$ \\eta_{\\mathrm{PCS}} = \\frac{q}{q + s_{\\mathrm{PCS}}} $$\nThis rate is shared between the $2$ user threads assigned to the kernel entity. The rate per thread is:\n$$ \\text{Rate}_{\\text{per thread, PCS}} = \\frac{1}{2} \\eta_{\\mathrm{PCS}} = \\frac{1}{2} \\left( \\frac{q}{q + s_{\\mathrm{PCS}}} \\right) $$\nThe total time $T_{\\mathrm{PCS}}$ for a thread to complete its workload $W$ is:\n$$ T_{\\mathrm{PCS}} = \\frac{W}{\\text{Rate}_{\\text{per thread, PCS}}} = \\frac{W}{\\frac{1}{2} \\left( \\frac{q}{q + s_{\\mathrm{PCS}}} \\right)} = \\frac{2W(q + s_{\\mathrm{PCS}})}{q} $$\n\nFinally, we compute the slowdown $\\delta$, defined as the ratio of the actual completion time $T$ to the ideal time $T_{\\mathrm{ideal}}$.\nFor PCS, the slowdown $\\delta_{\\mathrm{PCS}}$ is:\n$$ \\delta_{\\mathrm{PCS}} = \\frac{T_{\\mathrm{PCS}}}{T_{\\mathrm{ideal}}} = \\frac{ \\frac{2W(q + s_{\\mathrm{PCS}})}{q} }{W} = \\frac{2(q + s_{\\mathrm{PCS}})}{q} = 2 \\left(1 + \\frac{s_{\\mathrm{PCS}}}{q}\\right) $$\nFor SCS, the slowdown $\\delta_{\\mathrm{SCS}}$ is:\n$$ \\delta_{\\mathrm{SCS}} = \\frac{T_{\\mathrm{SCS}}}{T_{\\mathrm{ideal}}} = \\frac{ \\frac{2W(q + s_{\\mathrm{SCS}})}{q} }{W} = \\frac{2(q + s_{\\mathrm{SCS}})}{q} = 2 \\left(1 + \\frac{s_{\\mathrm{SCS}}}{q}\\right) $$\nThese expressions are dimensionless ratios as required. The factor of $2$ arises from contention (the oversubscription factor $N/C = 2$), while the term $(1 + s/q)$ represents the penalty from context switch overhead relative to the time quantum.",
            "answer": "```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    int C;\n    int N;\n    double W;\n    double q;\n    double s_pcs;\n    double s_scs;\n} TestCase;\n\n// Function to calculate slowdown.\n// The slowdown formula is derived as delta = 2 * (1 + s/q).\n// The derivation shows that N, C, and W are not in the final slowdown formula,\n// as long as the oversubscription N/C = 2.\ndouble calculate_slowdown(double q, double s) {\n    // Handle the case where the quantum q is zero to avoid division by zero.\n    // Based on the problem's first principles, q > 0 is implied for time-slicing.\n    if (q <= 0.0) {\n        // Return a value indicating an error or undefined state,\n        // though test cases ensure q > 0.\n        return -1.0; \n    }\n    return 2.0 * (1.0 + s / q);\n}\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        {4, 8,  1.0, 0.0100, 0.0005, 0.0010}, // Case 1\n        {4, 8,  2.5, 0.0100, 0.0000, 0.0000}, // Case 2\n        {2, 4,  1.0, 0.0010, 0.0005, 0.0005}, // Case 3\n        {1, 2,  3.0, 0.0200, 0.0008, 0.0010}, // Case 4\n        {8, 16, 0.5, 0.0050, 0.0002, 0.0006}  // Case 5\n    };\n\n    // Calculate the number of test cases.\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    // Each case produces two results: delta_pcs and delta_scs.\n    int num_results = num_cases * 2;\n    double results[num_results];\n\n    // Calculate the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        TestCase tc = test_cases[i];\n\n        // Calculate slowdown for Process-Contention Scope (PCS)\n        double delta_pcs = calculate_slowdown(tc.q, tc.s_pcs);\n        \n        // Calculate slowdown for System-Contention Scope (SCS)\n        double delta_scs = calculate_slowdown(tc.q, tc.s_scs);\n        \n        // Store results in the array\n        results[i * 2] = delta_pcs;\n        results[i * 2 + 1] = delta_scs;\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement.\n    // Format: [float,float,...] with no spaces and 6 decimal places.\n    printf(\"[\");\n    for (int i = 0; i < num_results; ++i) {\n        printf(\"%.6f\", results[i]);\n        if (i < num_results - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "Beyond throughput, the responsiveness of a system, often measured by latency, is a critical performance indicator. This practice shifts our focus from overall completion time to the specific delay a thread experiences when it needs to be woken up. You will build a deterministic model to calculate the expected wake-up latency, accounting for the complex, two-level scheduling path in PCS versus the more direct path in SCS, and see how factors like internal process state and the type of wake-up signal create different outcomes .",
            "id": "3672487",
            "problem": "You are to derive and implement a deterministic, simulation-free model that computes the expected wake-up latency for a sleeping thread under two scheduling scopes in operating systems: Process-Contention Scope (PCS) and System-Contention Scope (SCS). The wake-up event can occur by a user-level signal or a kernel wakeup. The program must output results for a given test suite using a single line as specified below, with all latency values expressed in seconds. The model must be derived from core definitions and well-tested scheduling facts rather than ad hoc heuristics.\n\nDefinitions and assumptions:\n\n- Under Process-Contention Scope (PCS), the operating system kernel schedules at the process level; the kernel sees only one schedulable entity per process. Under System-Contention Scope (SCS), the kernel schedules individual threads; the kernel sees each thread as a separate schedulable entity.\n\n- The system uses Round-Robin scheduling across kernel-visible runnable entities with a fixed per-entity time slice of duration $\\tau$ seconds, a per-context-switch overhead of $\\kappa$ seconds, and a per-scheduling-decision overhead of $\\sigma$ seconds. There are $C$ identical processor cores.\n\n- Per-process user-level scheduling uses Round-Robin across user-level threads within that process with a time slice $\\tau_u$ seconds, a per-user-level context-switch overhead $\\kappa_u$ seconds, and a per-user-level scheduling-decision overhead $\\sigma_u$ seconds.\n\n- There are $P$ processes, each with $N$ runnable threads (for SCS visibility) or $N$ runnable user-level threads (for PCS internal scheduling).\n\n- The expected number of entities ahead of a randomly inserted thread in a First-In-First-Out (FIFO) queue is $\\dfrac{M-1}{2}$, where $M$ is the total number of runnable entities in that queue. With $C$ cores and fully loaded execution, $C$ entities are serviced per time slice duration, so expected waiting time scales by a factor of $\\dfrac{1}{C}$.\n\n- For a user-level signal under PCS, with probability $r$ the process is currently running at the moment of the signal, enabling an immediate user-level handoff; this immediate handoff costs only the internal overhead $\\delta = \\kappa_u + \\sigma_u$ and incurs no system-level wait. With probability $1-r$, the process is not running; the wake-up must wait for system-level scheduling and then for user-level scheduling within the process.\n\n- For a kernel wakeup under PCS, the kernel wakes the process-level runnable entity; if the process is currently running (probability $r$), the woken thread is placed into the user-level ready queue and must wait for typical user-level scheduling (no immediate handoff). If the process is not running (probability $1-r$), both system-level and user-level waiting apply.\n\n- Under System-Contention Scope (SCS), both user-level signal and kernel wakeup yield the same expected latency because the kernel directly schedules threads. The total number of kernel-visible runnable entities is $M_{\\mathrm{SCS}} = P \\cdot N$.\n\nFrom these definitions, derive the following expected latencies:\n\n- The expected system-level waiting time to obtain a core for a newly woken entity in a Round-Robin queue with $M$ runnable entities and $C$ cores is\n$$\nW_{\\mathrm{sys}}(M,C,\\tau,\\kappa,\\sigma) = \\frac{M-1}{2C}\\,\\left(\\tau + \\kappa + \\sigma\\right).\n$$\n\n- The expected user-level waiting time within a process with $N$ runnable user-level threads is\n$$\nW_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u) = \\frac{N-1}{2}\\,\\left(\\tau_u + \\kappa_u + \\sigma_u\\right).\n$$\n\n- Expected wake-up latencies:\n    - PCS with user-level signal:\n    $$\n    t_{\\mathrm{wake}}^{\\mathrm{PCS,\\,user}} = r \\cdot \\delta + (1-r)\\cdot\\left[ W_{\\mathrm{sys}}(P,C,\\tau,\\kappa,\\sigma) + W_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u) \\right],\n    $$\n    where $\\delta = \\kappa_u + \\sigma_u$.\n    - PCS with kernel wakeup:\n    $$\n    t_{\\mathrm{wake}}^{\\mathrm{PCS,\\,kernel}} = r \\cdot W_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u) + (1-r)\\cdot\\left[ W_{\\mathrm{sys}}(P,C,\\tau,\\kappa,\\sigma) + W_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u) \\right].\n    $$\n    - SCS with user-level signal:\n    $$\n    t_{\\mathrm{wake}}^{\\mathrm{SCS,\\,user}} = W_{\\mathrm{sys}}(P\\cdot N,C,\\tau,\\kappa,\\sigma).\n    $$\n    - SCS with kernel wakeup:\n    $$\n    t_{\\mathrm{wake}}^{\\mathrm{SCS,\\,kernel}} = W_{\\mathrm{sys}}(P\\cdot N,C,\\tau,\\kappa,\\sigma).\n    $$\n\nYour program must compute all four $t_{\\mathrm{wake}}$ values above for each test case and print a single line containing a list of lists. Each inner list corresponds to one test case and must contain the four values in the order $\\left[t_{\\mathrm{wake}}^{\\mathrm{PCS,\\,user}}, t_{\\mathrm{wake}}^{\\mathrm{PCS,\\,kernel}}, t_{\\mathrm{wake}}^{\\mathrm{SCS,\\,user}}, t_{\\mathrm{wake}}^{\\mathrm{SCS,\\,kernel}}\\right]$. All values must be expressed in seconds as decimal floating-point numbers, formatted to six digits after the decimal point.\n\nTest suite:\n\n- Test case $1$: $P=4$, $N=8$, $C=2$, $\\tau=0.005$, $\\kappa=0.0001$, $\\sigma=0.00005$, $\\tau_u=0.003$, $\\kappa_u=0.00005$, $\\sigma_u=0.00002$, $r=0.6$.\n\n- Test case $2$: $P=1$, $N=16$, $C=4$, $\\tau=0.004$, $\\kappa=0.00008$, $\\sigma=0.00004$, $\\tau_u=0.002$, $\\kappa_u=0.00004$, $\\sigma_u=0.00002$, $r=1.0$.\n\n- Test case $3$: $P=32$, $N=32$, $C=8$, $\\tau=0.008$, $\\kappa=0.00012$, $\\sigma=0.00006$, $\\tau_u=0.006$, $\\kappa_u=0.00006$, $\\sigma_u=0.00003$, $r=0.2$.\n\n- Test case $4$: $P=2$, $N=2$, $C=16$, $\\tau=0.005$, $\\kappa=0.00010$, $\\sigma=0.00005$, $\\tau_u=0.003$, $\\kappa_u=0.00005$, $\\sigma_u=0.00002$, $r=0.5$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The line must be a list of four-element lists with no spaces, for example: $[\\,[0.012345,0.023456,0.034567,0.034567],[\\dots]\\,]$. All values must be in seconds, with six digits after the decimal point.",
            "solution": "The problem requires the derivation and implementation of a model to compute the expected wake-up latency for a thread under two different scheduling models: Process-Contention Scope (PCS) and System-Contention Scope (SCS). The solution will be derived from the provided first principles and assumptions.\n\n### Model and Principles\n\nThe foundation of the model rests on standard concepts from operating systems scheduling theory.\n\n1.  **Scheduling Scopes**:\n    -   **Process-Contention Scope (PCS)**: The kernel schedules processes, not threads. A user-level library within each process is responsible for scheduling the threads of that process onto the kernel-schedulable entity (often a light-weight process). This is a many-to-one or many-to-many threading model. In this problem's model, there are $P$ processes, each being a single entity from the kernel's perspective.\n    -   **System-Contention Scope (SCS)**: The kernel schedules threads directly. Each thread is an independent schedulable entity. This is a one-to-one threading model. In this model, the kernel sees a total of $M_{\\mathrm{SCS}} = P \\cdot N$ schedulable entities.\n\n2.  **Scheduling Algorithm and Queuing Theory**:\n    -   The model assumes a Round-Robin (RR) scheduler at both the kernel level and the user level. RR is a preemptive algorithm where each entity in the ready queue is given a fixed time slice (quantum) to run.\n    -   The core of the latency calculation is the expected waiting time. For a new entity inserted into a ready queue of $M$ entities at a random position, there are, on average, $\\frac{M-1}{2}$ other entities ahead of it.\n    -   The time to service a single entity is the sum of its time slice $\\tau$ and any associated overheads, here specified as context-switch overhead $\\kappa$ and scheduling-decision overhead $\\sigma$. Thus, the service time per entity is $(\\tau + \\kappa + \\sigma)$.\n    -   On a single-core system, the expected waiting time would be $\\frac{M-1}{2} (\\tau + \\kappa + \\sigma)$.\n    -   With $C$ identical processor cores, the system services entities at a rate $C$ times faster than a single core, assuming perfect load distribution and no idle cores (an assumption of an oversubscribed system). The expected waiting time is therefore scaled by $\\frac{1}{C}$.\n\n### Derivation of Waiting Time Formulas\n\nBased on these principles, the problem provides two fundamental waiting time formulas, which we verify.\n\n-   **Expected System-Level Waiting Time ($W_{\\mathrm{sys}}$)**:\n    This is the wait for a kernel-schedulable entity. There are $M$ such entities in the kernel's ready queue and $C$ cores.\n    $$\n    W_{\\mathrm{sys}}(M,C,\\tau,\\kappa,\\sigma) = \\frac{M-1}{2} \\cdot (\\tau + \\kappa + \\sigma) \\cdot \\frac{1}{C} = \\frac{M-1}{2C}(\\tau + \\kappa + \\sigma)\n    $$\n    This formula correctly combines the average number of preceding entities with the per-entity service time, scaled by the number of cores.\n\n-   **Expected User-Level Waiting Time ($W_{\\mathrm{usr}}$)**:\n    This is the wait for a user-level thread within a single process. The user-level scheduler runs on a single execution context provided by the kernel, so it is conceptually a single-core scheduler ($C=1$). There are $N$ user-level threads competing. The time parameters are the user-level ones ($\\tau_u, \\kappa_u, \\sigma_u$).\n    $$\n    W_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u) = \\frac{N-1}{2 \\cdot 1}(\\tau_u + \\kappa_u + \\sigma_u) = \\frac{N-1}{2}(\\tau_u + \\kappa_u + \\sigma_u)\n    $$\n    This formula is a direct application of the same principle to the user-level scheduling domain.\n\n### Derivation of Wake-up Latency Formulas\n\nThe total expected wake-up latency, $t_{\\mathrm{wake}}$, is calculated by considering the different scheduling paths for each scope and wake-up type. We use the law of total expectation, $E[X] = \\sum_i P(Y_i)E[X|Y_i]$, where the events $Y_i$ partition the sample space.\n\n1.  **PCS with User-Level Signal ($t_{\\mathrm{wake}}^{\\mathrm{PCS,\\,user}}$)**:\n    A user-level signal wakes a thread within the *same* process. Two scenarios are possible:\n    -   **Case 1**: The process is currently running on a core. This occurs with probability $r$. The user-level scheduler can perform an immediate handoff, which only incurs the user-level context-switch and scheduling overheads, defined as $\\delta = \\kappa_u + \\sigma_u$. The latency is $\\delta$.\n    -   **Case 2**: The process is not running (i.e., it is in the kernel's ready queue or blocked). This occurs with probability $1-r$. The wake-up requires two stages of waiting: first, the kernel must schedule the process, incurring an expected wait of $W_{\\mathrm{sys}}(P,C,\\tau,\\kappa,\\sigma)$ (since there are $P$ processes competing for cores); second, once the process is running, the user-level scheduler must schedule the newly woken thread, incurring an expected wait of $W_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u)$. The total latency is the sum of these two waits.\n    Applying the law of total expectation:\n    $$\n    t_{\\mathrm{wake}}^{\\mathrm{PCS,\\,user}} = r \\cdot \\delta + (1-r)\\cdot\\left[ W_{\\mathrm{sys}}(P,C,\\tau,\\kappa,\\sigma) + W_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u) \\right]\n    $$\n\n2.  **PCS with Kernel Wakeup ($t_{\\mathrm{wake}}^{\\mathrm{PCS,\\,kernel}}$)**:\n    A kernel-level event wakes the thread. The kernel makes the process-level entity runnable.\n    -   **Case 1**: The process is already running (probability $r$). The kernel event is delivered, and the target user-level thread located within the process is moved to the user-level ready queue. It must then wait for the user-level scheduler to select it. The latency is purely the user-level wait, $W_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u)$.\n    -   **Case 2**: The process is not currently running (probability $1-r$). The kernel makes the process runnable, and it must wait in the kernel's ready queue ($W_{\\mathrm{sys}}$). Once the process runs, the target user-level thread must wait in the user-level ready queue ($W_{\\mathrm{usr}}$). The total latency is $W_{\\mathrm{sys}} + W_{\\mathrm{usr}}$.\n    Applying the law of total expectation:\n    $$\n    t_{\\mathrm{wake}}^{\\mathrm{PCS,\\,kernel}} = r \\cdot W_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u) + (1-r)\\cdot\\left[ W_{\\mathrm{sys}}(P,C,\\tau,\\kappa,\\sigma) + W_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u) \\right]\n    $$\n    This can be algebraically simplified to $W_{\\mathrm{usr}}(N,\\tau_u,\\kappa_u,\\sigma_u) + (1-r) \\cdot W_{\\mathrm{sys}}(P,C,\\tau,\\kappa,\\sigma)$.\n\n3.  **SCS with User-Level Signal ($t_{\\mathrm{wake}}^{\\mathrm{SCS,\\,user}}$)**:\n    Under SCS, all $P \\cdot N$ threads are visible to and scheduled by the kernel. There is no user-level scheduling. A user-level signal translates to a system call that instructs the kernel to wake a specific thread. The kernel places this thread in its single global ready queue. The expected latency is simply the system-level wait time, where the number of competing entities is the total number of threads, $M_{\\mathrm{SCS}} = P \\cdot N$.\n    $$\n    t_{\\mathrm{wake}}^{\\mathrm{SCS,\\,user}} = W_{\\mathrm{sys}}(P\\cdot N,C,\\tau,\\kappa,\\sigma)\n    $$\n\n4.  **SCS with Kernel Wakeup ($t_{\\mathrm{wake}}^{\\mathrm{SCS,\\,kernel}}$)**:\n    Similar to the SCS user-level signal case, a kernel event makes a specific thread runnable. The kernel places this thread in its ready queue. The subsequent latency is identical to the user-level signal case, as both result in the thread waiting in the same kernel ready queue.\n    $$\n    t_{\\mathrm{wake}}^{\\mathrm{SCS,\\,kernel}} = W_{\\mathrm{sys}}(P\\cdot N,C,\\tau,\\kappa,\\sigma)\n    $$\n\nThese derived formulas are identical to those provided in the problem statement, confirming their validity within the defined model. The implementation will involve creating functions to compute these values for the specified test suite.",
            "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include <stdio.h>\n#include <stdlib.h>\n#include <math.h>\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    int P;         // Number of processes\n    int N;         // Number of threads per process\n    int C;         // Number of CPU cores\n    double tau;    // System time slice\n    double kappa;  // System context switch overhead\n    double sigma;  // System scheduling decision overhead\n    double tau_u;  // User-level time slice\n    double kappa_u;// User-level context switch overhead\n    double sigma_u;// User-level scheduling decision overhead\n    double r;      // Probability process is running\n} TestCase;\n\n// Computes the expected system-level waiting time.\n// M: Total number of kernel-visible runnable entities\n// C: Number of processor cores\n// tau, kappa, sigma: System-level time parameters\ndouble calculate_W_sys(int M, int C, double tau, double kappa, double sigma) {\n    if (M <= 1 || C <= 0) {\n        return 0.0;\n    }\n    return (double)(M - 1) / (2.0 * C) * (tau + kappa + sigma);\n}\n\n// Computes the expected user-level waiting time within a process.\n// N: Number of user-level threads in the process\n// tau_u, kappa_u, sigma_u: User-level time parameters\ndouble calculate_W_usr(int N, double tau_u, double kappa_u, double sigma_u) {\n    if (N <= 1) {\n        return 0.0;\n    }\n    return (double)(N - 1) / 2.0 * (tau_u + kappa_u + sigma_u);\n}\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        {4, 8, 2, 0.005, 0.0001, 0.00005, 0.003, 0.00005, 0.00002, 0.6},\n        {1, 16, 4, 0.004, 0.00008, 0.00004, 0.002, 0.00004, 0.00002, 1.0},\n        {32, 32, 8, 0.008, 0.00012, 0.00006, 0.006, 0.00006, 0.00003, 0.2},\n        {2, 2, 16, 0.005, 0.00010, 0.00005, 0.003, 0.00005, 0.00002, 0.5}\n    };\n\n    // Calculate the number of test cases.\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    double results[num_cases][4];\n\n    // Calculate the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        TestCase tc = test_cases[i];\n\n        // Calculate intermediate wait times\n        double w_sys_pcs = calculate_W_sys(tc.P, tc.C, tc.tau, tc.kappa, tc.sigma);\n        double w_usr = calculate_W_usr(tc.N, tc.tau_u, tc.kappa_u, tc.sigma_u);\n        \n        // PCS, user-level signal\n        double delta = tc.kappa_u + tc.sigma_u;\n        double t_wake_pcs_user = tc.r * delta + (1.0 - tc.r) * (w_sys_pcs + w_usr);\n        results[i][0] = t_wake_pcs_user;\n\n        // PCS, kernel wakeup\n        double t_wake_pcs_kernel = tc.r * w_usr + (1.0 - tc.r) * (w_sys_pcs + w_usr);\n        results[i][1] = t_wake_pcs_kernel;\n\n        // SCS, both wakeups\n        int M_scs = tc.P * tc.N;\n        double w_sys_scs = calculate_W_sys(M_scs, tc.C, tc.tau, tc.kappa, tc.sigma);\n        results[i][2] = w_sys_scs;\n        results[i][3] = w_sys_scs;\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"[%.6f,%.6f,%.6f,%.6f]\", results[i][0], results[i][1], results[i][2], results[i][3]);\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "After modeling theoretical performance, a crucial skill for any systems programmer is to measure and diagnose real-world behavior. This final practice bridges theory and application by asking you to design a methodology for dissecting performance bottlenecks using system tracing data, hypothetically collected with a tool like eBPF. By classifying different sources of delay into either userspace (PCS) or kernel-space (SCS) contention, you will learn to calculate a \"PCS contribution ratio,\" providing a powerful metric for identifying where optimization efforts should be focused in a complex, multithreaded process .",
            "id": "3672486",
            "problem": "Consider a userspace runtime that multiplexes many user-level threads onto a smaller number of kernel-level threads, creating potential separation between Process-Contention Scope (PCS) and System-Contention Scope (SCS). Process-Contention Scope refers to contention that is entirely managed and manifested within a single process's userspace runtime (for example, user-level locks, spins, and cooperative yields), while System-Contention Scope refers to contention visible to and managed by the operating system kernel scheduler (for example, a runnable thread waiting on a kernel run queue or blocking in a kernel-managed wait).\n\nYou are asked to design a tracing methodology using Extended Berkeley Packet Filter (eBPF) to separate contention at PCS versus SCS and to propose counters that quantify each scope. eBPF uprobes can be attached to userspace functions to capture user-level events, and eBPF kprobes or kernel tracepoints can be attached to kernel functions to capture kernel-level events.\n\nDefine counters $C_{\\text{user}}$ and $C_{\\text{kernel}}$ as aggregate contention-time measures over all threads of the target process during a measurement window of duration $T$, where $C_{\\text{user}}$ captures userspace contention time and $C_{\\text{kernel}}$ captures kernel-visible contention time. Assume that during the window, the following instrumented event classes were collected and classified:\n\n- Userspace spinning on user-level synchronization: total aggregated time across all threads $D_{\\text{spin}} = 6.4$ seconds.\n- Userspace blocking in user-level mutexes (entirely managed in userspace, not invoking kernel wait): total aggregated time $D_{\\text{umutex}} = 3.6$ seconds.\n- Kernel-managed waits (for example, futex wait or other blocking calls visible to the kernel): total aggregated time $D_{\\text{futex}} = 2.2$ seconds.\n- Runnable-but-not-running due to kernel run queue contention (for example, measured via scheduler tracepoints showing a thread ready to run but not scheduled): total aggregated time $D_{\\text{rq}} = 1.1$ seconds.\n\nAll durations are aggregated across threads, thus may exceed the wall-clock measurement duration $T$. You may assume the classifications are mutually exclusive and collectively cover the contention states of interest for this process.\n\nStarting from fundamental definitions of PCS and SCS and a coherent time-accounting model of contention, derive an estimator for the PCS contribution ratio $\\rho$ in terms of $C_{\\text{user}}$ and $C_{\\text{kernel}}$, and then compute $\\rho$ from the given measurements. Express the final ratio as a dimensionless decimal number and round your answer to four significant figures.",
            "solution": "The problem requires us to develop a quantitative measure for the contribution of Process-Contention Scope (PCS) to the total contention experienced by a multi-threaded application. We are provided with definitions for PCS and System-Contention Scope (SCS) and a set of aggregated time measurements for different types of contention, collected via eBPF.\n\nFirst, we must formalize the definitions of the aggregate contention-time counters, $C_{\\text{user}}$ and $C_{\\text{kernel}}$, based on the provided conceptual descriptions and measurement data.\n\nProcess-Contention Scope (PCS) is defined as contention managed entirely within a process's userspace. The counter $C_{\\text{user}}$ is the aggregate time spent in such states. From the provided data, two categories fall under this definition:\n$1$. Userspace spinning on user-level synchronization ($D_{\\text{spin}}$). In this state, a thread actively consumes CPU cycles in a user-level loop waiting for a resource. This is invisible to the kernel scheduler, which sees the thread as running. This is a form of PCS.\n$2$. Userspace blocking in user-level mutexes ($D_{\\text{umutex}}$). The problem specifies that this is \"entirely managed in userspace, not invoking kernel wait.\" This means the userspace runtime scheduler deschedules the user-level thread and may run another user-level thread on the same kernel thread. The kernel is unaware of this context switch. This is also a form of PCS.\n\nTherefore, the total aggregate userspace contention time, $C_{\\text{user}}$, is the sum of the durations of these events.\n$$C_{\\text{user}} = D_{\\text{spin}} + D_{\\text{umutex}}$$\n\nSystem-Contention Scope (SCS) is defined as contention that is visible to and managed by the operating system kernel scheduler. The counter $C_{\\text{kernel}}$ is the aggregate time spent in these states. Two categories from the data fit this description:\n$1$. Kernel-managed waits ($D_{\\text{futex}}$). This includes time spent in blocking system calls where the kernel explicitly puts the thread into a sleep state (e.g., waiting on a futex, I/O, or other kernel synchronization primitive). The contention for the resource is managed by the kernel. This is a form of SCS.\n$2$. Runnable-but-not-running ($D_{\\text{rq}}$). This represents the time a kernel thread is ready to execute but is waiting in the kernel's run queue for a CPU core to become available. The contention is for CPU cores, a resource managed directly by the kernel scheduler. This is a form of SCS.\n\nTherefore, the total aggregate kernel-visible contention time, $C_{\\text{kernel}}$, is the sum of the durations of these events.\n$$C_{\\text{kernel}} = D_{\\text{futex}} + D_{\\text{rq}}$$\n\nThe total contention time, $C_{\\text{total}}$, is the sum of contention from both scopes. Given the assumption that the event classes are mutually exclusive and collectively cover the contention states of interest, we can write:\n$$C_{\\text{total}} = C_{\\text{user}} + C_{\\text{kernel}}$$\n\nThe problem asks for the PCS contribution ratio, which we denote by $\\rho$. A logical definition for this ratio is the fraction of total contention time that is attributable to PCS.\n$$\\rho = \\frac{C_{\\text{user}}}{C_{\\text{total}}} = \\frac{C_{\\text{user}}}{C_{\\text{user}} + C_{\\text{kernel}}}$$\nSubstituting the expressions for $C_{\\text{user}}$ and $C_{\\text{kernel}}$:\n$$\\rho = \\frac{D_{\\text{spin}} + D_{\\text{umutex}}}{ (D_{\\text{spin}} + D_{\\text{umutex}}) + (D_{\\text{futex}} + D_{\\text{rq}}) }$$\n\nNow, we substitute the given numerical values into these expressions:\n$D_{\\text{spin}} = 6.4$ s\n$D_{\\text{umutex}} = 3.6$ s\n$D_{\\text{futex}} = 2.2$ s\n$D_{\\text{rq}} = 1.1$ s\n\nFirst, we calculate $C_{\\text{user}}$ and $C_{\\text{kernel}}$:\n$$C_{\\text{user}} = 6.4 + 3.6 = 10.0 \\text{ s}$$\n$$C_{\\text{kernel}} = 2.2 + 1.1 = 3.3 \\text{ s}$$\n\nNext, we calculate the total contention time $C_{\\text{total}}$:\n$$C_{\\text{total}} = C_{\\text{user}} + C_{\\text{kernel}} = 10.0 + 3.3 = 13.3 \\text{ s}$$\n\nFinally, we compute the ratio $\\rho$:\n$$\\rho = \\frac{C_{\\text{user}}}{C_{\\text{total}}} = \\frac{10.0}{13.3}$$\n$$\\rho \\approx 0.751879699...$$\n\nThe problem requires the answer to be rounded to four significant figures.\n$$\\rho \\approx 0.7519$$\nThis dimensionless ratio indicates that approximately $75.19\\%$ of the measured contention occurs within the userspace runtime (PCS), while the remaining portion is due to contention for resources managed by the kernel (SCS).",
            "answer": "$$\\boxed{0.7519}$$"
        }
    ]
}