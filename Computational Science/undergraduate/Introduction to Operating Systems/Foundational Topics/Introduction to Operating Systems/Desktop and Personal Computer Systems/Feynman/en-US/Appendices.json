{
    "hands_on_practices": [
        {
            "introduction": "A core function of any desktop operating system is to manage and arbitrate access to finite hardware resources like the CPU, disk, and network. This exercise challenges you to step into the role of a system designer and answer a fundamental question: is a given set of applications, each with its own resource demands, theoretically supportable on a given machine? By modeling a typical desktop workload, you will apply the principles of proportional-share scheduling to determine system feasibility, learning how to identify and reason about resource bottlenecks . This practice provides a crucial foundation for understanding system performance and capacity planning.",
            "id": "3633834",
            "problem": "You are asked to formalize and implement a steady-state resource-partitioning analysis for a mixed desktop workload consisting of a backup process, an interactive video call, and an interactive coding session. The desktop runs on a single personal computer system. Use the following foundational definitions as the base for your reasoning.\n\nA proportional-share scheduler allocates a resource by dividing it into fractions across tasks. For a resource with total capacity $C_r$ and tasks indexed by $t$, each task $t$ is assigned a nonnegative share $s_{r,t}$. The scheduler provides task $t$ a realized service rate equal to $f_{r,t} \\cdot C_r$, where $f_{r,t} = \\dfrac{s_{r,t}}{\\sum_j s_{r,j}}$ is the fraction of the resource. In steady state, a task $t$ that requires a minimum service rate $R_{t,r}$ on resource $r$ is satisfied if and only if $f_{r,t} \\cdot C_r \\ge R_{t,r}$. A Central Processing Unit (CPU), disk Input/Output (I/O), and network are treated as independent resources. The backup is a throughput-oriented task with nontrivial disk I/O and network requirements plus small CPU overhead, the video call is a latency-sensitive workload with nontrivial CPU and network requirement and negligible disk I/O, and the coding session is interactive with nontrivial CPU and small disk I/O.\n\nTasks:\n- You must determine, for each test case, whether there exists a static resource partitioning design with share vectors $s_{\\text{cpu}} = [s_{\\text{cpu},B}, s_{\\text{cpu},V}, s_{\\text{cpu},C}]$, $s_{\\text{io}} = [s_{\\text{io},B}, s_{\\text{io},V}, s_{\\text{io},C}]$, and $s_{\\text{net}} = [s_{\\text{net},B}, s_{\\text{net},V}, s_{\\text{net},C}]$ such that the realized service rates meet the per-task minimum requirements for all three resources simultaneously under a proportional-share scheduler as defined above. Your design must use a share assignment method grounded in the foundational definitions but must not rely on unproven shortcuts.\n\nAssumptions and units:\n- Resource capacities: CPU capacity $C_{\\text{cpu}}$ in giga-instructions per second (GIPS), disk I/O capacity $C_{\\text{io}}$ in megabytes per second (MB/s), and network capacity $C_{\\text{net}}$ in megabits per second (Mb/s).\n- Per-task per-resource minimum required rates $R_{t,r}$ are specified in the same units as the corresponding $C_r$.\n- The three tasks are $B$ (backup), $V$ (video call), and $C$ (coding).\n- Floating-point comparisons should treat sums less than or equal to capacity as feasible, using an absolute tolerance $\\varepsilon = 10^{-9}$ to guard against numerical round-off; that is, treat $x \\le y + \\varepsilon$ as satisfied.\n\nWhat you must implement:\n- For each test case, decide if there exists a feasible static share assignment $s_{\\text{cpu}}$, $s_{\\text{io}}$, and $s_{\\text{net}}$ that makes all three tasks satisfied on all three resources under proportional-share scheduling. Your algorithm must be justified from the given definitions.\n- You may compute any intermediate share vectors internally, but your program’s final reported result for each test case must be a boolean feasibility indicator only.\n\nTest suite:\nProvide results for the following five cases. Each case specifies $(C_{\\text{cpu}}, C_{\\text{io}}, C_{\\text{net}})$ and the nine requirements $\\{R_{t,r}\\}$ for $t \\in \\{B,V,C\\}$ and $r \\in \\{\\text{cpu}, \\text{io}, \\text{net}\\}$.\n\n- Case $1$ (typical desktop resources, moderate demands):\n  - Capacities: $C_{\\text{cpu}} = 50$ GIPS, $C_{\\text{io}} = 500$ MB/s, $C_{\\text{net}} = 1000$ Mb/s.\n  - Backup: $R_{B,\\text{cpu}} = 1$, $R_{B,\\text{io}} = 200$, $R_{B,\\text{net}} = 500$.\n  - Video: $R_{V,\\text{cpu}} = 5$, $R_{V,\\text{io}} = 0.5$, $R_{V,\\text{net}} = 8$.\n  - Coding: $R_{C,\\text{cpu}} = 8$, $R_{C,\\text{io}} = 5$, $R_{C,\\text{net}} = 1$.\n\n- Case $2$ (CPU exactly saturated by demands, others ample):\n  - Capacities: $C_{\\text{cpu}} = 14$, $C_{\\text{io}} = 1000$, $C_{\\text{net}} = 2000$.\n  - Requirements identical to Case $1$.\n\n- Case $3$ (network bottleneck):\n  - Capacities: $C_{\\text{cpu}} = 100$, $C_{\\text{io}} = 1000$, $C_{\\text{net}} = 450$.\n  - Requirements identical to Case $1$.\n\n- Case $4$ (disk I/O bottleneck):\n  - Capacities: $C_{\\text{cpu}} = 20$, $C_{\\text{io}} = 190$, $C_{\\text{net}} = 1000$.\n  - Requirements identical to Case $1$.\n\n- Case $5$ (near-boundary network, slight excess):\n  - Capacities: $C_{\\text{cpu}} = 100$, $C_{\\text{io}} = 1000$, $C_{\\text{net}} = 509$.\n  - Backup: $R_{B,\\text{cpu}} = 1$, $R_{B,\\text{io}} = 200$, $R_{B,\\text{net}} = 500$.\n  - Video: $R_{V,\\text{cpu}} = 5$, $R_{V,\\text{io}} = 0.5$, $R_{V,\\text{net}} = 8.000001$.\n  - Coding: $R_{C,\\text{cpu}} = 8$, $R_{C,\\text{io}} = 5$, $R_{C,\\text{net}} = 1$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of booleans, enclosed in square brackets, in the order of Cases $1$ through $5$ (for example, $\\texttt{[true,false,true,false,true]}$). No units are to be printed, and no additional text is allowed in the output.",
            "solution": "The problem requires determining the existence of a feasible static resource-partitioning on a system with three tasks, $t \\in \\{B, V, C\\}$ (Backup, Video, Coding), and three resources, $r \\in \\{\\text{cpu}, \\text{io}, \\text{net}\\}$. A feasible partitioning is one where a set of non-negative shares, $s_{r,t} \\ge 0$, can be assigned such that every task meets its minimum required service rate, $R_{t,r}$, on every resource.\n\nThe analysis proceeds from the foundational definitions provided.\n\nAccording to the definition of a proportional-share scheduler, the realized service rate for a task $t$ on a resource $r$ with total capacity $C_r$ is given by:\n$$ \\text{ServiceRate}_{t,r} = f_{r,t} \\cdot C_r $$\nwhere $f_{r,t}$ is the fraction of the resource allocated to task $t$. This fraction is defined in terms of shares $s_{r,t}$ as:\n$$ f_{r,t} = \\frac{s_{r,t}}{\\sum_{j \\in \\{B,V,C\\}} s_{r,j}} $$\nLet $S_r = \\sum_{j \\in \\{B,V,C\\}} s_{r,j}$ be the total shares allocated for resource $r$. We can assume $S_r > 0$, otherwise no task receives any service. The fraction is then $f_{r,t} = s_{r,t} / S_r$.\n\nIn steady state, a task $t$ is satisfied on resource $r$ if and only if its realized service rate is greater than or equal to its minimum requirement $R_{t,r}$. This gives the inequality:\n$$ f_{r,t} \\cdot C_r \\ge R_{t,r} $$\nSubstituting the expression for $f_{r,t}$:\n$$ \\left( \\frac{s_{r,t}}{S_r} \\right) \\cdot C_r \\ge R_{t,r} $$\nThis inequality must hold for every task $t$ on the resource $r$.\n\nTo determine if a feasible set of shares $\\{s_{r,t}\\}$ exists, we can analyze this system of inequalities. By rearranging the inequality, we can isolate the resource fraction required by each task:\n$$ \\frac{s_{r,t}}{S_r} \\ge \\frac{R_{t,r}}{C_r} $$\nThis shows that the fraction of resource $r$ allocated to task $t$ must be at least its demand fraction, defined as $d_{t,r} = R_{t,r} / C_r$.\n\nFor a feasible share assignment to exist for resource $r$, a set of non-negative shares $s_{r,t}$ must satisfy the above inequality for all tasks $t \\in \\{B, V, C\\}$. Summing this inequality over all three tasks for the resource $r$:\n$$ \\sum_{t \\in \\{B,V,C\\}} \\frac{s_{r,t}}{S_r} \\ge \\sum_{t \\in \\{B,V,C\\}} \\frac{R_{t,r}}{C_r} $$\nThe left side of the inequality simplifies:\n$$ \\sum_{t \\in \\{B,V,C\\}} \\frac{s_{r,t}}{S_r} = \\frac{1}{S_r} \\sum_{t \\in \\{B,V,C\\}} s_{r,t} = \\frac{S_r}{S_r} = 1 $$\nThis leads to a necessary condition for the existence of a feasible share assignment for resource $r$:\n$$ 1 \\ge \\sum_{t \\in \\{B,V,C\\}} \\frac{R_{t,r}}{C_r} $$\nMultiplying by the resource capacity $C_r$ (which is always positive), we get:\n$$ C_r \\ge \\sum_{t \\in \\{B,V,C\\}} R_{t,r} $$\nThis condition is also sufficient. If it holds, we can construct a valid set of shares. For example, a simple and valid assignment is to set each task's share equal to its minimum resource requirement: $s_{r,t} = R_{t,r}$. Since all $R_{t,r}$ are non-negative, the shares are non-negative. With this choice, $S_r = \\sum_j R_{r,j}$. The satisfaction condition for task $t$ becomes:\n$$ \\left( \\frac{R_{t,r}}{\\sum_j R_{r,j}} \\right) \\cdot C_r \\ge R_{t,r} $$\nIf $R_{t,r} > 0$, we can divide by $R_{t,r}$ to get $C_r / (\\sum_j R_{r,j}) \\ge 1$, which is equivalent to our derived condition $C_r \\ge \\sum_j R_{r,j}$. If $R_{t,r} = 0$, the satisfaction condition $0 \\ge 0$ is trivially true.\n\nTherefore, a feasible static share assignment for a given resource $r$ exists if and only if the total capacity of the resource is greater than or equal to the sum of the minimum requirements of all tasks on that resource.\n\nFor the entire system to be feasible, this condition must hold simultaneously for all three independent resources: CPU, disk I/O, and network. A single resource bottleneck, where the sum of demands exceeds capacity, renders the entire system infeasible.\n\nThe final algorithm for each test case is as follows, incorporating the specified numerical tolerance $\\varepsilon = 10^{-9}$:\n1. For resource CPU: Check if $\\sum_{t \\in \\{B,V,C\\}} R_{t,\\text{cpu}} \\le C_{\\text{cpu}} + \\varepsilon$.\n2. For resource I/O: Check if $\\sum_{t \\in \\{B,V,C\\}} R_{t,\\text{io}} \\le C_{\\text{io}} + \\varepsilon$.\n3. For resource NET: Check if $\\sum_{t \\in \\{B,V,C\\}} R_{t,\\text{net}} \\le C_{\\text{net}} + \\varepsilon$.\n4. The system is feasible if and only if all three conditions are true.\n\nWe now apply this to each test case.\n\n**Case 1:**\n- Capacities: $C_{\\text{cpu}} = 50$, $C_{\\text{io}} = 500$, $C_{\\text{net}} = 1000$.\n- Requirements Sum:\n  - $\\sum R_{\\text{cpu}} = 1 + 5 + 8 = 14$.\n  - $\\sum R_{\\text{io}} = 200 + 0.5 + 5 = 205.5$.\n  - $\\sum R_{\\text{net}} = 500 + 8 + 1 = 509$.\n- Checks:\n  - CPU: $14 \\le 50$. True.\n  - I/O: $205.5 \\le 500$. True.\n  - Net: $509 \\le 1000$. True.\n- Result: **Feasible (true)**.\n\n**Case 2:**\n- Capacities: $C_{\\text{cpu}} = 14$, $C_{\\text{io}} = 1000$, $C_{\\text{net}} = 2000$.\n- Requirements Sum (same as Case 1): $\\sum R_{\\text{cpu}} = 14$, $\\sum R_{\\text{io}} = 205.5$, $\\sum R_{\\text{net}} = 509$.\n- Checks:\n  - CPU: $14 \\le 14$. True.\n  - I/O: $205.5 \\le 1000$. True.\n  - Net: $509 \\le 2000$. True.\n- Result: **Feasible (true)**.\n\n**Case 3:**\n- Capacities: $C_{\\text{cpu}} = 100$, $C_{\\text{io}} = 1000$, $C_{\\text{net}} = 450$.\n- Requirements Sum (same as Case 1): $\\sum R_{\\text{cpu}} = 14$, $\\sum R_{\\text{io}} = 205.5$, $\\sum R_{\\text{net}} = 509$.\n- Checks:\n  - CPU: $14 \\le 100$. True.\n  - I/O: $205.5 \\le 1000$. True.\n  - Net: $509 \\le 450$. False.\n- Result: **Infeasible (false)**.\n\n**Case 4:**\n- Capacities: $C_{\\text{cpu}} = 20$, $C_{\\text{io}} = 190$, $C_{\\text{net}} = 1000$.\n- Requirements Sum (same as Case 1): $\\sum R_{\\text{cpu}} = 14$, $\\sum R_{\\text{io}} = 205.5$, $\\sum R_{\\text{net}} = 509$.\n- Checks:\n  - CPU: $14 \\le 20$. True.\n  - I/O: $205.5 \\le 190$. False.\n  - Net: $509 \\le 1000$. True.\n- Result: **Infeasible (false)**.\n\n**Case 5:**\n- Capacities: $C_{\\text{cpu}} = 100$, $C_{\\text{io}} = 1000$, $C_{\\text{net}} = 509$.\n- Requirements Sum:\n  - $\\sum R_{\\text{cpu}} = 1 + 5 + 8 = 14$.\n  - $\\sum R_{\\text{io}} = 200 + 0.5 + 5 = 205.5$.\n  - $\\sum R_{\\text{net}} = 500 + 8.000001 + 1 = 509.000001$.\n- Checks (with $\\varepsilon = 10^{-9}$):\n  - CPU: $14 \\le 100 + \\varepsilon$. True.\n  - I/O: $205.5 \\le 1000 + \\varepsilon$. True.\n  - Net: $509.000001 \\le 509 + 10^{-9}$. This is $509 + 10^{-6} \\le 509 + 10^{-9}$, which is false.\n- Result: **Infeasible (false)**.",
            "answer": "```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\n// A struct to hold all parameters for a single test case.\ntypedef struct {\n    // Resource capacities\n    double C_cpu;\n    double C_io;\n    double C_net;\n    // Backup task requirements\n    double R_B_cpu;\n    double R_B_io;\n    double R_B_net;\n    // Video task requirements\n    double R_V_cpu;\n    double R_V_io;\n    double R_V_net;\n    // Coding task requirements\n    double R_C_cpu;\n    double R_C_io;\n    double R_C_net;\n} TestCase;\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        // Case 1 (typical desktop resources, moderate demands)\n        {\n            .C_cpu = 50.0, .C_io = 500.0, .C_net = 1000.0,\n            .R_B_cpu = 1.0, .R_B_io = 200.0, .R_B_net = 500.0,\n            .R_V_cpu = 5.0, .R_V_io = 0.5, .R_V_net = 8.0,\n            .R_C_cpu = 8.0, .R_C_io = 5.0, .R_C_net = 1.0\n        },\n        // Case 2 (CPU exactly saturated by demands, others ample)\n        {\n            .C_cpu = 14.0, .C_io = 1000.0, .C_net = 2000.0,\n            .R_B_cpu = 1.0, .R_B_io = 200.0, .R_B_net = 500.0,\n            .R_V_cpu = 5.0, .R_V_io = 0.5, .R_V_net = 8.0,\n            .R_C_cpu = 8.0, .R_C_io = 5.0, .R_C_net = 1.0\n        },\n        // Case 3 (network bottleneck)\n        {\n            .C_cpu = 100.0, .C_io = 1000.0, .C_net = 450.0,\n            .R_B_cpu = 1.0, .R_B_io = 200.0, .R_B_net = 500.0,\n            .R_V_cpu = 5.0, .R_V_io = 0.5, .R_V_net = 8.0,\n            .R_C_cpu = 8.0, .R_C_io = 5.0, .R_C_net = 1.0\n        },\n        // Case 4 (disk I/O bottleneck)\n        {\n            .C_cpu = 20.0, .C_io = 190.0, .C_net = 1000.0,\n            .R_B_cpu = 1.0, .R_B_io = 200.0, .R_B_net = 500.0,\n            .R_V_cpu = 5.0, .R_V_io = 0.5, .R_V_net = 8.0,\n            .R_C_cpu = 8.0, .R_C_io = 5.0, .R_C_net = 1.0\n        },\n        // Case 5 (near-boundary network, slight excess)\n        {\n            .C_cpu = 100.0, .C_io = 1000.0, .C_net = 509.0,\n            .R_B_cpu = 1.0, .R_B_io = 200.0, .R_B_net = 500.0,\n            .R_V_cpu = 5.0, .R_V_io = 0.5, .R_V_net = 8.000001,\n            .R_C_cpu = 8.0, .R_C_io = 5.0, .R_C_net = 1.0\n        }\n    };\n\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    int results[num_cases];\n    const double epsilon = 1e-9;\n\n    // Calculate the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        TestCase tc = test_cases[i];\n\n        // Calculate the sum of required rates for each resource.\n        double sum_R_cpu = tc.R_B_cpu + tc.R_V_cpu + tc.R_C_cpu;\n        double sum_R_io = tc.R_B_io + tc.R_V_io + tc.R_C_io;\n        double sum_R_net = tc.R_B_net + tc.R_V_net + tc.R_C_net;\n\n        // A feasible share allocation exists if and only if the sum of required rates\n        // for each resource does not exceed the resource's capacity.\n        int cpu_ok = (sum_R_cpu <= tc.C_cpu + epsilon);\n        int io_ok = (sum_R_io <= tc.C_io + epsilon);\n        int net_ok = (sum_R_net <= tc.C_net + epsilon);\n\n        // All three conditions must be met for the system to be feasible.\n        results[i] = (cpu_ok && io_ok && net_ok);\n    }\n\n    // Print the results in the EXACT REQUIRED format.\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"%s\", results[i] ? \"true\" : \"false\");\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\\n\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "Modern desktop applications, from web browsers to chat clients, are highly event-driven and must efficiently manage numerous I/O sources like network sockets and user input. This exercise dives into the core mechanism that makes this possible: I/O multiplexing. Through a carefully constructed simulation, you will mathematically model and compare two key strategies—a classic linear scan and a modern readiness queue—to understand their scaling properties . This analysis will reveal why operating systems like Linux and FreeBSD developed advanced primitives like `epoll` and `kqueue` to handle massive numbers of concurrent connections efficiently.",
            "id": "3633789",
            "problem": "You are to design a principled simulation of desktop event loop behavior for Input/Output (I/O) multiplexing on personal computer systems. The purpose is to analyze the expected polling cost per cycle, denoted by $t_{\\text{poll}}$, and how it scales with the number of file descriptors $N$. Rather than invoking operating system primitives directly, you will model two strategies that correspond to widely used approaches: a linear-scan polling loop (akin to classical polling mechanisms that inspect all $N$ descriptors each cycle) and a readiness-queue loop (akin to Linux epoll or FreeBSD kqueue that returns only the set of ready descriptors). The final output of your program must be computed in abstract operation-equivalent units, which are dimensionless and represent the count of elementary operations; do not use seconds.\n\nFundamental base and assumptions:\n- A desktop event loop inspects non-blocking file descriptors each cycle to determine which ones are ready to be processed. Let there be $N$ independent descriptors, each independently ready with probability $p$ during a given cycle. This independence is a standard simplifying assumption for tractable modeling.\n- Let $K$ be the random variable denoting the number of ready descriptors in a cycle. Under the independence assumption with common readiness probability $p$, $K$ follows a binomial distribution with parameters $(N,p)$, and the expected number of ready descriptors is $\\mathbb{E}[K] = N p$.\n- Define an abstract cost model where each elementary operation contributes one operation-equivalent unit. The following per-cycle constants are given and fixed across all test cases:\n  - Base loop overhead $c_{\\text{base}} = 100$.\n  - Per-descriptor scan cost $c_{\\text{scan}} = 2$ for the linear-scan strategy.\n  - Per-ready-descriptor processing cost $c_{\\text{process}} = 5$ for both strategies.\n  - Per-ready-descriptor fetch cost $c_{\\text{fetch}} = 1$ for the readiness-queue strategy.\n- Interpret the costs as follows:\n  - Linear-scan strategy: The event loop inspects all $N$ descriptors each cycle, incurring a scan cost; if a descriptor is ready, there is an additional processing cost per ready descriptor.\n  - Readiness-queue strategy: The event loop incurs base overhead and fetches only ready descriptors from a readiness queue; there is no scan cost proportional to $N$, but there is a fetch cost per ready descriptor plus the processing cost.\n\nTasks:\n1. Using the independence assumption and the binomial distribution for $K$, derive the expected polling cost $t_{\\text{poll,linear}}$ for the linear-scan polling loop in terms of $N$, $p$, $c_{\\text{base}}$, $c_{\\text{scan}}$, and $c_{\\text{process}}$.\n2. Derive the expected polling cost $t_{\\text{poll,queue}}$ for the readiness-queue loop in terms of $N$, $p$, $c_{\\text{base}}$, $c_{\\text{fetch}}$, and $c_{\\text{process}}$.\n3. Implement a program that computes both expected costs for a set of test cases. Report each expected cost as a floating-point value. All readiness probabilities must be treated as decimal fractions (not percentages).\n\nUnit and output specification:\n- All outputs are expected values in operation-equivalent units per poll cycle. Express each as a floating-point number.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-item list $[t_{\\text{poll,linear}}, t_{\\text{poll,queue}}]$ for the corresponding test case. For example, the output must look like $[[x_1,y_1],[x_2,y_2],\\dots]$.\n- Print all floating-point numbers with exactly six digits after the decimal point.\n\nTest suite:\nUse the following test cases to exercise correctness and scaling behavior:\n- Case A (boundary: no descriptors): $N = 0$, $p = 0.0$.\n- Case B (boundary: single descriptor always ready): $N = 1$, $p = 1.0$.\n- Case C (happy path: sparse readiness): $N = 128$, $p = 0.01$.\n- Case D (moderate load): $N = 4096$, $p = 0.5$.\n- Case E (large scale, very sparse): $N = 65536$, $p = 0.0001$.\n\nYour program must compute and output $[[t_{\\text{poll,linear}}^{(A)}, t_{\\text{poll,queue}}^{(A)}],[t_{\\text{poll,linear}}^{(B)}, t_{\\text{poll,queue}}^{(B)}],\\dots]$ for the five cases in the specified order, with each value rounded to six digits after the decimal point.",
            "solution": "The analysis hinges on modeling the expected cost per event loop cycle for two distinct I/O multiplexing strategies. The core of the model is the number of ready file descriptors in a given cycle, denoted by the random variable $K$. We are given $N$ independent file descriptors, each with a probability $p$ of being ready. Therefore, $K$ follows a binomial distribution with parameters $N$ and $p$, written as $K \\sim \\text{Binomial}(N, p)$. A key property of this distribution is its expected value, which is given as $\\mathbb{E}[K] = Np$. We will use the linearity of expectation, which states that for random variables $X$ and $Y$ and constants $a$ and $b$, $\\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]$. The expected value of a constant $c$ is simply the constant itself, $\\mathbb{E}[c] = c$.\n\nThe abstract cost model defines the following constants in operation-equivalent units:\n-   Base loop overhead: $c_{\\text{base}} = 100$\n-   Per-descriptor scan cost: $c_{\\text{scan}} = 2$\n-   Per-ready-descriptor processing cost: $c_{\\text{process}} = 5$\n-   Per-ready-descriptor fetch cost: $c_{\\text{fetch}} = 1$\n\n**1. Derivation of Expected Polling Cost for the Linear-Scan Strategy ($t_{\\text{poll,linear}}$)**\n\nThe linear-scan strategy involves a base overhead, a cost for scanning all $N$ descriptors, and a cost for processing the $K$ descriptors that are found to be ready. The total cost for a single cycle, $C_{\\text{linear}}$, is a function of the random variable $K$:\n$$C_{\\text{linear}}(K) = c_{\\text{base}} + N \\cdot c_{\\text{scan}} + K \\cdot c_{\\text{process}}$$\nThe expected polling cost, $t_{\\text{poll,linear}}$, is the expected value of $C_{\\text{linear}}(K)$. We apply the expectation operator $\\mathbb{E}[\\cdot]$ to this expression:\n$$t_{\\text{poll,linear}} = \\mathbb{E}[C_{\\text{linear}}(K)] = \\mathbb{E}[c_{\\text{base}} + N \\cdot c_{\\text{scan}} + K \\cdot c_{\\text{process}}]$$\nBy the linearity of expectation, we can separate the terms. The terms $c_{\\text{base}}$, $N$, and $c_{\\text{scan}}$ are constants for a given scenario.\n$$t_{\\text{poll,linear}} = \\mathbb{E}[c_{\\text{base}}] + \\mathbb{E}[N \\cdot c_{\\text{scan}}] + \\mathbb{E}[K \\cdot c_{\\text{process}}]$$\n$$t_{\\text{poll,linear}} = c_{\\text{base}} + N \\cdot c_{\\text{scan}} + c_{\\text{process}} \\cdot \\mathbb{E}[K]$$\nSubstituting the known expected value $\\mathbb{E}[K] = Np$, we obtain the final expression for the expected cost of the linear-scan strategy:\n$$t_{\\text{poll,linear}} = c_{\\text{base}} + N \\cdot c_{\\text{scan}} + c_{\\text{process}} \\cdot (Np)$$\nThis can be rearranged to highlight the dependency on $N$:\n$$t_{\\text{poll,linear}} = c_{\\text{base}} + N \\cdot (c_{\\text{scan}} + p \\cdot c_{\\text{process}})$$\nThis formula shows a linear dependency of the cost on the total number of descriptors, $N$.\n\n**2. Derivation of Expected Polling Cost for the Readiness-Queue Strategy ($t_{\\text{poll,queue}}$)**\n\nThe readiness-queue strategy avoids scanning all $N$ descriptors. Instead, it directly fetches the $K$ ready descriptors from a queue maintained by the operating system. The cost comprises the base overhead, a fetch cost for each of the $K$ ready descriptors, and a processing cost for each of them. The total cost for a single cycle, $C_{\\text{queue}}$, is:\n$$C_{\\text{queue}}(K) = c_{\\text{base}} + K \\cdot c_{\\text{fetch}} + K \\cdot c_{\\text{process}}$$\nThis can be simplified to:\n$$C_{\\text{queue}}(K) = c_{\\text{base}} + K \\cdot (c_{\\text{fetch}} + c_{\\text{process}})$$\nTo find the expected polling cost, $t_{\\text{poll,queue}}$, we take the expectation of $C_{\\text{queue}}(K)$:\n$$t_{\\text{poll,queue}} = \\mathbb{E}[C_{\\text{queue}}(K)] = \\mathbb{E}[c_{\\text{base}} + K \\cdot (c_{\\text{fetch}} + c_{\\text{process}})]$$\nUsing the linearity of expectation:\n$$t_{\\text{poll,queue}} = \\mathbb{E}[c_{\\text{base}}] + \\mathbb{E}[K \\cdot (c_{\\text{fetch}} + c_{\\text{process}})]$$\n$$t_{\\text{poll,queue}} = c_{\\text{base}} + (c_{\\text{fetch}} + c_{\\text{process}}) \\cdot \\mathbb{E}[K]$$\nSubstituting $\\mathbb{E}[K] = Np$:\n$$t_{\\text{poll,queue}} = c_{\\text{base}} + (c_{\\text{fetch}} + c_{\\text{process}}) \\cdot (Np)$$\nThis formula shows that the cost's dependency on $N$ is scaled by the readiness probability $p$. For scenarios where $p$ is small (sparse readiness), this strategy is expected to be significantly more efficient than the linear scan, especially for large $N$.\n\n**Summary of Formulas for Implementation**\nWith the given constant values substituted, the formulas for computation are:\n-   $t_{\\text{poll,linear}} = 100 + N \\cdot (2 + 5p)$\n-   $t_{\\text{poll,queue}} = 100 + Np \\cdot (1 + 5) = 100 + 6Np$",
            "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include <stdio.h>\n#include <stdlib.h>\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    long N;\n    double p;\n} TestCase;\n\n// A struct to hold the results for a single test case.\ntypedef struct {\n    double linear_cost;\n    double queue_cost;\n} Result;\n\nint main(void) {\n    // Define the abstract cost model constants.\n    const double C_BASE = 100.0;\n    const double C_SCAN = 2.0;\n    const double C_PROCESS = 5.0;\n    const double C_FETCH = 1.0;\n\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        {0, 0.0},      // Case A\n        {1, 1.0},      // Case B\n        {128, 0.01},   // Case C\n        {4096, 0.5},   // Case D\n        {65536, 0.0001} // Case E\n    };\n\n    // Calculate the number of test cases.\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    Result results[num_cases];\n\n    // Calculate the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        // Use double for N in calculations to prevent any potential issues\n        // with mixed-type arithmetic and to handle large numbers seamlessly.\n        double N = (double)test_cases[i].N;\n        double p = test_cases[i].p;\n\n        // Calculate expected cost for the linear-scan strategy.\n        // Formula: t_poll,linear = c_base + N * (c_scan + p * c_process)\n        results[i].linear_cost = C_BASE + N * (C_SCAN + p * C_PROCESS);\n\n        // Calculate expected cost for the readiness-queue strategy.\n        // Formula: t_poll,queue = c_base + Np * (c_fetch + c_process)\n        results[i].queue_cost = C_BASE + (N * p) * (C_FETCH + C_PROCESS);\n    }\n\n    // Print the results in the exact required format.\n    // The format is a single line: [[x1,y1],[x2,y2],...]\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"[%.6f,%.6f]\", results[i].linear_cost, results[i].queue_cost);\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "Beyond performance, an operating system's design choices have profound implications for software development and portability. This hands-on problem explores a classic example: filesystem case sensitivity, a subtle difference between systems like Windows/macOS and Linux that can cause perplexing build failures. You will analyze hypothetical project data to quantify the probability of build errors arising from inconsistent filename casing on different filesystems . This practice highlights the importance of writing portable code and illustrates how abstract OS features translate into concrete challenges for developers.",
            "id": "3633837",
            "problem": "Consider desktop and personal computer systems that differ in filesystem case sensitivity. Adopt the following fundamental base and definitions, grounded in widely accepted concepts in operating systems and probability. A filesystem is case-sensitive if distinct pathnames differing only by letter case denote distinct files; it is case-insensitive if such pathnames are considered equivalent under a case-folding transformation. Define the case-folding function $\\phi$ that maps any pathname string to its canonical case-insensitive representative (e.g., by lowercasing), and the equivalence relation $x \\sim y \\iff \\phi(x) = \\phi(y)$. An equivalence class under $\\sim$ is called a case-fold cluster. In a case-insensitive filesystem, at a given directory location, only one member of a case-fold cluster can be stored; in a case-sensitive filesystem, multiple members can coexist.\n\nA build system issues references to files (by pathnames) during compilation, linking, or packaging. Let the set of logical files be indexed by $j \\in \\{0,1,\\dots,M-1\\}$, where $M$ is the number of files in the project. File $j$ belongs to a cluster $c(j) \\in \\{0,1,\\dots,K-1\\}$, where $K$ is the number of clusters. For file $j$, there are $k_j$ distinct casings observed in the build references, with $r_{j,\\ell}$ denoting the nonnegative integer count of references that use casing index $\\ell \\in \\{0,1,\\dots,k_j-1\\}$. The canonical casing index for file $j$ on a case-sensitive system is $m_j \\in \\{0,1,\\dots,k_j-1\\}$. On a case-insensitive system, for each cluster $i \\in \\{0,1,\\dots,K-1\\}$, there is a canonical file index $\\gamma_i$ identifying which single file in that cluster would be selected or retained when the cluster is forced to collapse to one member (e.g., the first created file). The total number of references across the project is $R = \\sum_{j=0}^{M-1} \\sum_{\\ell=0}^{k_j-1} r_{j,\\ell}$.\n\nDefine the following probabilities as project-level metrics:\n- $p_{\\text{CS}}$ is the probability that a random build reference fails on a case-sensitive filesystem due to casing mismatch. Under the principle that a mismatch occurs exactly when a reference to file $j$ uses a casing $\\ell \\neq m_j$, the numerator counts all such mismatches.\n- $p_{\\text{CI}}$ is the probability that a random build reference resolves to the wrong file on a case-insensitive filesystem due to cluster conflation. Under the principle that conflation occurs exactly when a reference targets a non-canonical file in its cluster, the numerator counts all references whose target file index $j$ is not $\\gamma_{c(j)}$.\n- $p_{\\text{VNS}}$ is the probability that a random build reference experiences either failure or wrong-file resolution under a mitigation that introduces virtual namespaces. A virtual namespace is a mapping $\\nu$ that enforces a per-project bijection from each pair $(i, \\text{local\\_index})$ to a unique path string that is invariant across filesystems and that rewrites all observed casings to a canonical casing per file before path resolution. Under a well-defined bijection that rewrites all observed casings and disambiguates all cluster members, the probability of failure or wrong-file resolution over the observed references is $0$.\n\nYour task is to implement a complete program that, for each test case described below, computes $p_{\\text{CS}}$, $p_{\\text{CI}}$, and $p_{\\text{VNS}}$, and outputs them as decimals. Use the following definitions from probability theory: if events are counted by nonnegative integers with total $R$, then the empirical probability of an event is its count divided by $R$. Express all probabilities as decimals.\n\nFor each test case, you are given $M$, $K$, the mapping $c(j)$, the per-file $k_j$, the per-file counts $\\{r_{j,\\ell}\\}$, the per-file canonical casing index $m_j$, and the per-cluster canonical file index $\\gamma_i$. Use these as follows:\n- Compute $R = \\sum_{j=0}^{M-1} \\sum_{\\ell=0}^{k_j-1} r_{j,\\ell}$.\n- Compute the case-sensitive mismatch count $E_{\\text{CS}} = \\sum_{j=0}^{M-1} \\sum_{\\ell=0}^{k_j-1} [\\ell \\neq m_j] \\cdot r_{j,\\ell}$, where $[\\cdot]$ is $1$ if the condition is true and $0$ otherwise, so $p_{\\text{CS}} = E_{\\text{CS}} / R$.\n- Compute the case-insensitive wrong-file count $E_{\\text{CI}} = \\sum_{j=0}^{M-1} [j \\neq \\gamma_{c(j)}] \\cdot \\left(\\sum_{\\ell=0}^{k_j-1} r_{j,\\ell}\\right)$, so $p_{\\text{CI}} = E_{\\text{CI}} / R$.\n- Under virtual namespaces as defined, set $p_{\\text{VNS}} = 0$.\n\nTest suite:\n- Test case A (happy path with mixed casings and a two-member collision cluster):\n  - $M = 4$, $K = 3$.\n  - Cluster canonical file indices: $\\gamma_0 = 0$, $\\gamma_1 = 2$, $\\gamma_2 = 3$.\n  - File $0$: $c(0) = 0$, $k_0 = 2$, $m_0 = 0$, counts $r_{0,0} = 30$, $r_{0,1} = 10$.\n  - File $1$: $c(1) = 0$, $k_1 = 2$, $m_1 = 0$, counts $r_{1,0} = 20$, $r_{1,1} = 5$.\n  - File $2$: $c(2) = 1$, $k_2 = 2$, $m_2 = 0$, counts $r_{2,0} = 50$, $r_{2,1} = 2$.\n  - File $3$: $c(3) = 2$, $k_3 = 1$, $m_3 = 0$, counts $r_{3,0} = 60$.\n- Test case B (boundary case with zero errors):\n  - $M = 3$, $K = 3$.\n  - Cluster canonical file indices: $\\gamma_0 = 0$, $\\gamma_1 = 1$, $\\gamma_2 = 2$.\n  - File $0$: $c(0) = 0$, $k_0 = 1$, $m_0 = 0$, counts $r_{0,0} = 100$.\n  - File $1$: $c(1) = 1$, $k_1 = 1$, $m_1 = 0$, counts $r_{1,0} = 40$.\n  - File $2$: $c(2) = 2$, $k_2 = 1$, $m_2 = 0$, counts $r_{2,0} = 30$.\n- Test case C (edge case with a three-member collision cluster and widespread casing mismatches):\n  - $M = 4$, $K = 2$.\n  - Cluster canonical file indices: $\\gamma_0 = 1$, $\\gamma_1 = 3$.\n  - File $0$: $c(0) = 0$, $k_0 = 2$, $m_0 = 0$, counts $r_{0,0} = 15$, $r_{0,1} = 5$.\n  - File $1$: $c(1) = 0$, $k_1 = 2$, $m_1 = 0$, counts $r_{1,0} = 40$, $r_{1,1} = 10$.\n  - File $2$: $c(2) = 0$, $k_2 = 2$, $m_2 = 0$, counts $r_{2,0} = 5$, $r_{2,1} = 25$.\n  - File $3$: $c(3) = 1$, $k_3 = 2$, $m_3 = 0$, counts $r_{3,0} = 50$, $r_{3,1} = 5$.\n- Test case D (edge case with near-total wrong-file resolution on case-insensitive systems and heavy casing mismatch on case-sensitive systems):\n  - $M = 3$, $K = 2$.\n  - Cluster canonical file indices: $\\gamma_0 = 0$, $\\gamma_1 = 2$.\n  - File $0$: $c(0) = 0$, $k_0 = 1$, $m_0 = 0$, counts $r_{0,0} = 1$.\n  - File $1$: $c(1) = 0$, $k_1 = 2$, $m_1 = 1$, counts $r_{1,0} = 80$, $r_{1,1} = 19$.\n  - File $2$: $c(2) = 1$, $k_2 = 1$, $m_2 = 0$, counts $r_{2,0} = 50$.\n\nFinal output specification:\nYour program should produce a single line of output containing the results as a comma-separated list of per-test-case triples, enclosed in square brackets, with each triple of the form $[p_{\\text{CS}},p_{\\text{CI}},p_{\\text{VNS}}]$. For the four test cases A, B, C, and D, the output must be a single line like $[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$, where each $x_i$, $y_i$, and $z_i$ is a decimal number.",
            "solution": "The task is to compute three probabilities for four different test cases, based on a model of file references in a software build system. The probabilities quantify the risk of errors on case-sensitive ($p_{\\text{CS}}$) and case-insensitive ($p_{\\text{CI}}$) filesystems, and under a hypothetical mitigation ($p_{\\text{VNS}}$). The calculation is based on empirical probability, defined as the ratio of event occurrences to the total number of trials.\n\nThe fundamental quantities are:\n- The total number of build references, $R = \\sum_{j=0}^{M-1} \\sum_{\\ell=0}^{k_j-1} r_{j,\\ell}$.\n- The count of case-sensitive mismatch events, $E_{\\text{CS}} = \\sum_{j=0}^{M-1} \\sum_{\\ell=0}^{k_j-1} [\\ell \\neq m_j] \\cdot r_{j,\\ell}$, where $[\\cdot]$ is the Iverson bracket. The probability is $p_{\\text{CS}} = E_{\\text{CS}} / R$.\n- The count of case-insensitive wrong-file events, $E_{\\text{CI}} = \\sum_{j=0}^{M-1} [j \\neq \\gamma_{c(j)}] \\cdot \\left(\\sum_{\\ell=0}^{k_j-1} r_{j,\\ell}\\right)$. The probability is $p_{\\text{CI}} = E_{\\text{CI}} / R$.\n- The probability of error under a virtual namespace, which is given axiomatically as $p_{\\text{VNS}} = 0$.\n\nLet us demonstrate the calculation for Test Case A. The given data is:\n- $M = 4$, $K = 3$.\n- Cluster canonicals: $\\gamma_0 = 0$, $\\gamma_1 = 2$, $\\gamma_2 = 3$.\n- File $j=0$: $c(0)=0, m_0=0, r_{0,0}=30, r_{0,1}=10$. Total references: $40$.\n- File $j=1$: $c(1)=0, m_1=0, r_{1,0}=20, r_{1,1}=5$. Total references: $25$.\n- File $j=2$: $c(2)=1, m_2=0, r_{2,0}=50, r_{2,1}=2$. Total references: $52$.\n- File $j=3$: $c(3)=2, m_3=0, r_{3,0}=60$. Total references: $60$.\n\nFirst, we compute the total number of references, $R$:\n$$R = (30+10) + (20+5) + (50+2) + 60 = 40 + 25 + 52 + 60 = 177$$\n\nNext, we compute the case-sensitive error count, $E_{\\text{CS}}$. This is the sum of references to non-canonical casings:\n- File $j=0$: $m_0=0$. The non-canonical casing is $\\ell=1$, with $r_{0,1}=10$ references.\n- File $j=1$: $m_1=0$. The non-canonical casing is $\\ell=1$, with $r_{1,1}=5$ references.\n- File $j=2$: $m_2=0$. The non-canonical casing is $\\ell=1$, with $r_{2,1}=2$ references.\n- File $j=3$: $m_3=0, k_3=1$. There are no non-canonical casings.\n$$E_{\\text{CS}} = 10 + 5 + 2 + 0 = 17$$\nThe probability is $p_{\\text{CS}} = E_{\\text{CS}} / R = 17 / 177$.\n\nThen, we compute the case-insensitive error count, $E_{\\text{CI}}$. This is the sum of all references to files that are not canonical for their cluster:\n- File $j=0$: Belongs to cluster $c(0)=0$. The canonical file for this cluster is $\\gamma_0=0$. Since $j=\\gamma_{c(j)}$, there is no error.\n- File $j=1$: Belongs to cluster $c(1)=0$. The canonical file is $\\gamma_0=0$. Since $j \\neq \\gamma_{c(j)}$ (i.e., $1 \\neq 0$), all references to this file are counted as errors. Total references for file $j=1$ are $20+5=25$.\n- File $j=2$: Belongs to cluster $c(2)=1$. The canonical file is $\\gamma_1=2$. Since $j=\\gamma_{c(j)}$, there is no error.\n- File $j=3$: Belongs to cluster $c(3)=2$. The canonical file is $\\gamma_2=3$. Since $j=\\gamma_{c(j)}$, there is no error.\n$$E_{\\text{CI}} = 25$$\nThe probability is $p_{\\text{CI}} = E_{\\text{CI}} / R = 25 / 177$.\n\nFinally, $p_{\\text{VNS}} = 0$ by definition. The results for Test Case A are $[17/177, 25/177, 0]$. This same procedure is applied to all other test cases.",
            "answer": "```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\n// Note: The following headers are permitted but not used in this solution.\n// #include <complex.h>\n// #include <threads.h>\n// #include <stdatomic.h>\n\n// Data structure for a single file's properties.\n// This corresponds to the information given for each file j.\ntypedef struct {\n    int file_idx;          // j\n    int cluster_idx;       // c(j)\n    int canonical_casing;  // m_j\n    int num_casings;       // k_j\n    const int* refs;       // r_{j,ell} counts\n} FileData;\n\n// Data structure for a complete test case.\ntypedef struct {\n    int M;                   // Number of files\n    int K;                   // Number of clusters\n    const FileData* files;   // Array of file data structs\n    const int* gamma;        // Canonical file indices for clusters, gamma_i\n} TestCase;\n\n// Computes the probabilities p_CS, p_CI, and p_VNS for a given test case.\nvoid calculate_probabilities(const TestCase* tc, double* p_cs, double* p_ci, double* p_vns) {\n    long long total_refs_R = 0;\n    long long errors_CS = 0;\n    long long errors_CI = 0;\n\n    // Iterate over each file j in the project.\n    for (int j = 0; j < tc->M; ++j) {\n        const FileData* file = &tc->files[j];\n        long long total_refs_for_file = 0;\n\n        // Sum references for the current file and count case-sensitive (CS) errors.\n        for (int l = 0; l < file->num_casings; ++l) {\n            total_refs_for_file += file->refs[l];\n            // Condition for CS error: casing index l is not the canonical one m_j.\n            if (l != file->canonical_casing) {\n                errors_CS += file->refs[l];\n            }\n        }\n        \n        total_refs_R += total_refs_for_file;\n\n        // Condition for CI error: file index j is not the canonical one gamma_{c(j)} for its cluster.\n        if (file->file_idx != tc->gamma[file->cluster_idx]) {\n            errors_CI += total_refs_for_file;\n        }\n    }\n\n    // Calculate probabilities as ratios. Handle division by zero, though not expected here.\n    *p_cs = (total_refs_R > 0) ? (double)errors_CS / total_refs_R : 0.0;\n    *p_ci = (total_refs_R > 0) ? (double)errors_CI / total_refs_R : 0.0;\n    *p_vns = 0.0; // As per problem definition.\n}\n\nint main(void) {\n    // --- Test Case A Data ---\n    const int refs_A0[] = {30, 10};\n    const int refs_A1[] = {20, 5};\n    const int refs_A2[] = {50, 2};\n    const int refs_A3[] = {60};\n    const FileData files_A[] = {\n        {0, 0, 0, 2, refs_A0}, {1, 0, 0, 2, refs_A1},\n        {2, 1, 0, 2, refs_A2}, {3, 2, 0, 1, refs_A3}\n    };\n    const int gamma_A[] = {0, 2, 3};\n\n    // --- Test Case B Data ---\n    const int refs_B0[] = {100};\n    const int refs_B1[] = {40};\n    const int refs_B2[] = {30};\n    const FileData files_B[] = {\n        {0, 0, 0, 1, refs_B0}, {1, 1, 0, 1, refs_B1}, {2, 2, 0, 1, refs_B2}\n    };\n    const int gamma_B[] = {0, 1, 2};\n    \n    // --- Test Case C Data ---\n    const int refs_C0[] = {15, 5};\n    const int refs_C1[] = {40, 10};\n    const int refs_C2[] = {5, 25};\n    const int refs_C3[] = {50, 5};\n    const FileData files_C[] = {\n        {0, 0, 0, 2, refs_C0}, {1, 0, 0, 2, refs_C1},\n        {2, 0, 0, 2, refs_C2}, {3, 1, 0, 2, refs_C3}\n    };\n    const int gamma_C[] = {1, 3};\n\n    // --- Test Case D Data ---\n    const int refs_D0[] = {1};\n    const int refs_D1[] = {80, 19};\n    const int refs_D2[] = {50};\n    const FileData files_D[] = {\n        {0, 0, 0, 1, refs_D0}, {1, 0, 1, 2, refs_D1}, {2, 1, 0, 1, refs_D2}\n    };\n    const int gamma_D[] = {0, 2};\n\n    // Array of all Test Cases\n    TestCase test_cases[] = {\n        {4, 3, files_A, gamma_A},\n        {3, 3, files_B, gamma_B},\n        {4, 2, files_C, gamma_C},\n        {3, 2, files_D, gamma_D}\n    };\n    \n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    double results[num_cases][3];\n\n    // Calculate results for each test case\n    for (int i = 0; i < num_cases; ++i) {\n        calculate_probabilities(&test_cases[i], &results[i][0], &results[i][1], &results[i][2]);\n    }\n\n    // Print the results in the EXACT required format before the final return statement.\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"[%f,%f,%f]\", results[i][0], results[i][1], results[i][2]);\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        }
    ]
}