{
    "hands_on_practices": [
        {
            "introduction": "In multiprocessor programming, performance is not solely dictated by algorithmic efficiency or lock contention. The underlying hardware, particularly the cache coherence protocol, can introduce subtle yet significant bottlenecks. This exercise delves into the phenomenon of \"false sharing,\" where threads accessing logically independent data inadvertently cause performance degradation because that data resides on the same cache line. By designing an experiment to expose and mitigate this issue, you will gain a deeper appreciation for the critical interplay between software data layout and hardware memory systems.",
            "id": "3661589",
            "problem": "An array of $M$ elements, each element of size $b$ bytes, is shared among $N$ threads running on a multicore processor. Each core has private level-$1$ and level-$2$ caches and a shared last-level cache, and cache coherence is maintained via a Modified, Exclusive, Shared, Invalid (MESI) protocol. The cache line size is $L$ bytes. The operating system scheduler may migrate threads across cores unless prevented by processor affinity (pinning). A throughput experiment is proposed in which each thread $t \\in \\{0, 1, \\dots, N-1\\}$ repeatedly performs atomic-free increments on an assigned array element whose index is a function of a stride parameter $s$; the stride $s$ will be varied across a set of integer values to observe throughput cliffs. Throughput is measured as total increments per unit time.\n\nUsing only the following foundational facts:\n- A write by a core to an address contained in a cache line forces other cores’ copies of that line to transition to invalid under coherence, even if the cores are writing to different words within the same line.\n- When multiple cores frequently write to different words within the same cache line, coherence traffic (“ping-ponging”) can dominate, reducing throughput despite no true data dependency (false sharing).\n- Pinning threads prevents scheduler migration, which would otherwise introduce additional noise and confounding cache behavior.\n- Padding or aligning data so that concurrently written words occupy distinct cache lines reduces coherence conflicts.\n\nSelect the option that most rigorously specifies an experiment to expose false sharing by varying $s$ and proposes both an operating system scheduler policy and a data layout policy that will mitigate throughput cliffs attributable specifically to false sharing, while controlling for other confounders (such as Non-Uniform Memory Access (NUMA) effects and thread migration). Assume $N$ is at least $4$, $L$ is a typical line size (for example $L = 64$ bytes), and $b$ corresponds to a $64$-bit integer (for example $b = 8$ bytes). The array base address can be aligned if desired.\n\nA. Pin each thread to a distinct physical core on the same socket. Align the base of the array to a cache-line boundary. Assign thread $t$ to update index $i_t = t \\cdot s$ continuously for a fixed duration, and sweep $s$ over values such as $s \\in \\{1, 2, 4, 8, 16, 32\\}$. Record total increments per second. To mitigate throughput cliffs, introduce per-thread padding of $p$ elements so that the per-thread slot index becomes $i_t = t \\cdot (s + p)$ with $p$ chosen minimally such that $p \\cdot b \\geq L$, and maintain pinning to prevent migration.\n\nB. Allow the operating system to freely schedule and migrate threads. For each $s$, assign each thread $t$ to update random indices uniformly in $\\{0, 1, \\dots, M-1\\}$ with step size $s$ skipped probabilistically. Measure throughput. To mitigate throughput cliffs, increase the number of threads and enable Simultaneous Multithreading (SMT) to hide memory latency.\n\nC. Pin half of the threads to cores on socket $0$ and the other half to cores on socket $1$ in a dual-socket system with Non-Uniform Memory Access (NUMA). Assign thread $t$ to update index $i_t = t \\cdot s$ and sweep $s$ over large values, for example $s \\geq M/N$, ensuring threads update widely separated elements. To mitigate throughput cliffs, interleave memory allocation across NUMA nodes and rely on the hardware prefetcher.\n\nD. Pin all threads to a single core to maximize cache reuse. Assign thread $t$ to update index $i_t = t \\cdot s$ and vary $s$. To mitigate throughput cliffs, disable caches using page table attributes and rely exclusively on main memory accesses to avoid coherence interactions.\n\nE. Pin each thread to a distinct physical core on the same socket. Do not align the array base deliberately. Assign thread $t$ to update index $i_t = t \\cdot s$ and vary $s$ over $\\{1, 2, 3, 5, 7, 9\\}$. To mitigate throughput cliffs, enable aggressive compiler vectorization and hardware prefetching without changing data layout.\n\nWhich option best meets the experimental design goals stated above and provides sound scheduler pinning and padding policies to specifically mitigate false-sharing-induced throughput cliffs as $s$ varies?",
            "solution": "### Solution Analysis\nThe primary goal is to design an experiment to demonstrate the performance impact of false sharing. False sharing occurs when multiple threads on different cores repeatedly write to different data elements that happen to reside on the same cache line.\n\nLet the element size be $b$ bytes and the cache line size be $L$ bytes. In the experiment, thread $t$ accesses index $i_t = t \\cdot s$. The memory address distance between elements accessed by adjacent threads ($t$ and $t+1$) is $s \\cdot b$. False sharing will be severe if this distance is less than the cache line size, i.e., $s \\cdot b  L$. For the given parameters ($L=64, b=8$), this occurs when $s  8$. When $s \\cdot b \\geq L$ (i.e., $s \\geq 8$), the elements are on different cache lines, and performance should improve dramatically.\n\nA rigorous experiment must:\n1.  **Isolate the Coherence Effect:** This requires controlling for confounders. Pinning each thread to a distinct physical core on the same socket eliminates thread migration overhead and avoids NUMA (Non-Uniform Memory Access) effects. Aligning the data array to a cache line boundary ensures a predictable mapping of elements to cache lines.\n2.  **Demonstrate the Throughput Cliff:** The stride parameter $s$ must be swept across the critical value of $L/b = 8$. Sweeping $s$ through values like $\\{1, 2, 4, 8, 16\\}$ should reveal a sharp increase in throughput at $s=8$.\n\nA sound mitigation strategy must ensure concurrently accessed elements reside on separate cache lines. This is typically done by adding padding to the data structure or increasing the access stride to be at least $L$ bytes.\n\n**Option Analysis:**\n\n*   **A:** This option correctly implements all principles of a rigorous experiment: it pins threads to a single socket, aligns the array, and sweeps the stride $s$ across the critical threshold. The proposed mitigation of adding padding to increase the effective stride is also correct.\n*   **B:** This option fails by allowing thread migration and using a random access pattern, which would obscure the specific effect of false sharing.\n*   **C:** This option incorrectly introduces NUMA effects as a confounder and sweeps over large strides, which would fail to expose the false sharing phenomenon that occurs at small strides.\n*   **D:** Pinning all threads to a single core eliminates inter-core false sharing, thus failing to test the phenomenon. Disabling caches is an absurd and impractical \"solution\".\n*   **E:** This option fails by not aligning the array (less rigorous) and proposing ineffective mitigation strategies (vectorization and prefetching do not solve inter-thread write contention).\n\nBased on this analysis, Option A is the only one that proposes a scientifically rigorous experimental setup and a sound mitigation strategy.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Moving from the hardware level to application architecture, a central challenge is managing concurrency effectively, especially for applications that mix computation with I/O operations. This practice explores a classic performance tuning problem: determining the optimal size of a thread pool. By developing a simple analytical model, you will learn to balance the goal of keeping all CPU cores busy against the risk of creating excessive context-switching overhead, a crucial skill for building scalable and responsive systems.",
            "id": "3661557",
            "problem": "A server runs on a multicore machine with $N$ hardware cores. It uses a fixed-size thread pool of size $P$ to handle a steady stream of homogeneous tasks. Each thread repeatedly alternates between executing on the Central Processing Unit (CPU) and waiting for Input/Output (I/O). At any scheduling instant, a thread is blocked (waiting for I/O) with probability $p_b$, independently of other threads, and runnable with probability $1 - p_b$. The operating system employs a round-robin scheduler: when more than one runnable thread competes for a given core, the scheduler assigns a time slice of length $q$ to each runnable thread on that core and performs a context switch with cost $c$ at the end of each slice (and also when a running thread blocks before its slice completes).\n\nYou wish to design a tuning test that varies $P$ to maximize throughput while controlling context-switch overhead. You will measure, for each $P$, two metrics under a sustained workload: (i) aggregate throughput (completed CPU segments per unit time) and (ii) context switches per second. Your aim is to select $P$ to keep the expected number of runnable threads close to the number of cores to avoid idle cores, yet not so large that round-robin preemption and frequent blocking induce excessive context-switch overhead.\n\nWhich choice best captures the rule you should apply when selecting $P$ and the rationale behind it?\n\nA. Set $P = N$, because any larger $P$ only increases context-switching without improving throughput; the blocking probability $p_b$ has no bearing on ideal $P$ when $q$ is fixed.\n\nB. Set $P \\approx \\left\\lceil \\dfrac{N}{1 - p_b} \\right\\rceil$, because the expected number of runnable threads $\\mathbb{E}[R] = P (1 - p_b)$ should be near $N$: if $\\mathbb{E}[R]  N$ some cores idle and throughput drops; if $\\mathbb{E}[R] \\gg N$ round-robin preemption ensures a context-switch overhead per core of approximately $c / q$, lowering effective CPU time per core.\n\nC. Set $P \\approx N (1 - p_b)$, because blocking reduces effective CPU demand; the pool size should be reduced in proportion to $1 - p_b$ to minimize context-switch overhead.\n\nD. Set $P \\gg N$ (for example $P = 10 N$), because aggressive oversubscription hides I/O stalls; the throughput gain from keeping cores busy always outweighs context-switch overhead $c$ when $q$ is fixed.\n\nE. Set $P \\approx \\left\\lfloor \\dfrac{N}{1 + p_b} \\right\\rfloor$, because adding $p_b$ to $1$ in the denominator accounts for blocking and reduces preemption-induced context switches while maintaining full core utilization.",
            "solution": "The optimal strategy is to ensure that, on average, the number of runnable threads matches the number of available cores, $N$. This maximizes core utilization without creating excessive contention and context-switching overhead.\n\nLet $P$ be the size of the thread pool. Each thread has a probability of being runnable of $(1 - p_b)$. The expected number of runnable threads, $\\mathbb{E}[R]$, is therefore given by the product of the number of threads and their probability of being runnable:\n$$\n\\mathbb{E}[R] = P \\cdot (1 - p_b)\n$$\nTo fully utilize the CPU, we set the expected number of runnable threads equal to the number of cores:\n$$\n\\mathbb{E}[R] = N\n$$\nSubstituting the expression for $\\mathbb{E}[R]$ gives:\n$$\nP \\cdot (1 - p_b) = N\n$$\nSolving for $P$ yields the formula for the optimal thread pool size:\n$$\nP = \\frac{N}{1 - p_b}\n$$\nThis formula correctly sizes the pool to compensate for the time threads spend blocked on I/O. If $\\mathbb{E}[R]  N$, cores will be idle. If $\\mathbb{E}[R] \\gg N$, the system will suffer from excessive context-switching overhead. Option B presents this exact logic and formula, proposing $P \\approx \\left\\lceil \\dfrac{N}{1 - p_b} \\right\\rceil$ to ensure the number of cores is met or slightly exceeded, which is a robust practical approach.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "While standard synchronization primitives are essential, building high-performance systems often requires designing custom, more scalable mechanisms. This final practice challenges you to engineer a synchronization barrier using Linux's efficient futex facility and to analyze a common scalability bottleneck known as a \"wakeup storm.\" You will use a quantitative cost model to compare a naive implementation with a more sophisticated hierarchical strategy, learning how to design synchronization logic that is mindful of the operating system scheduler's own performance characteristics.",
            "id": "3661498",
            "problem": "Consider a symmetric multiprocessor system (SMP) with $N$ cooperating threads that repeatedly synchronize at a barrier implemented on the Linux Fast Userspace mutex (futex) facility. The barrier must be kernel-ready, meaning waiters do not busy-wait in the kernel and the design must be robust against spurious wakeups and missed wakeups due to races. Each thread increments a shared arrival count and blocks when it has not yet observed the barrier’s release, and the last arriving thread releases the barrier.\n\nAssume the following foundational elements and facts about the operating system and concurrency:\n- A barrier requires that all $N$ threads arriving in a generation must wait until the generation completes; the last arriving thread releases the barrier.\n- A futex wait sleeps only if the memory word equals the expected value at the moment of the syscall entry; otherwise, the syscall returns immediately. Spurious wakeups may occur, so code must recheck the condition after wake.\n- Correct barrier design for futex requires a generation indicator so that waiters sleep on a specific generation value and do not miss the release if the release occurs between the user-space check and the futex sleep.\n- Scheduler runqueue insertion for each awakened thread incurs some cost; when many threads are awakened nearly simultaneously, lock contention and cache-coherence traffic on runqueue structures increases the cost nonlinearly. With per-CPU runqueues, $P$ cores distribute wakeups, reducing contention.\n\nModel the wakeup storm cost for a single futex wake that makes $k$ threads runnable by the function\n$$\nC_w(k) = c_s + c_r \\, k + \\frac{\\sigma}{P} \\, k^2,\n$$\nwhere $c_s$ is the base futex wake syscall overhead, $c_r$ is the per-thread runqueue insertion cost, and the quadratic term captures contention and coherence overhead spread across $P$ cores.\n\nNow analyze two barrier release strategies:\n1. Naive wake: the last thread calls a single wake with $k = N - 1$ to awaken all waiters in one burst.\n2. Hierarchical wake: the last thread first wakes $f$ group leaders; each leader, upon running, wakes approximately $(N - 1)/f$ threads in its subgroup. Thus, the release consists of $1 + f$ wake calls, one with $k = f$ and then $f$ calls each with $k \\approx (N - 1)/f$.\n\nTake $N = 64$ threads, $P = 16$ cores, $c_s = 2 \\,\\mu s$, $c_r = 0.5 \\,\\mu s$, and $\\sigma = 1.0 \\,\\mu s$. Using first-principles reasoning from the above facts:\n- Derive the wakeup storm cost for the naive strategy.\n- Derive the wakeup storm cost for the hierarchical strategy as a function of $f$, and determine the integer $f$ that minimizes it.\n- Ensure the barrier design avoids missed wakeups and handles spurious wakeups via a generation counter and correct memory ordering between the last-arriver’s release and the futex wake.\n\nWhich option correctly specifies a kernel-ready futex barrier design and gives the correct numerical wakeup storm costs and optimal fanout for the hierarchical strategy under the stated parameters?\n\nA. Use a shared arrival count and a generation counter $g$. Each non-last thread reads $g$ into a local variable $g_0$, then, if the arrival count is not $N$, calls futex wait with the expected value $g_0$ and loops until $g$ changes; the last-arriver resets the count, increments $g$ with a store-release, and then issues futex wake. The naive wake cost is\n$$\nC_w^{\\text{naive}} = c_s + c_r (N - 1) + \\frac{\\sigma}{P} (N - 1)^2,\n$$\nwhich evaluates to approximately $281.56 \\,\\mu s$. The hierarchical strategy cost is\n$$\nC_w^{\\text{hier}}(f) = \\bigl(c_s + c_r f + \\frac{\\sigma}{P} f^2\\bigr) + f \\left( c_s + c_r \\frac{N - 1}{f} + \\frac{\\sigma}{P} \\left(\\frac{N - 1}{f}\\right)^2 \\right),\n$$\nwhich simplifies to\n$$\nC_w^{\\text{hier}}(f) = (1 + f)c_s + c_r \\,(N - 1 + f) + \\frac{\\sigma}{P} \\left( f^2 + \\frac{(N - 1)^2}{f} \\right).\n$$\nFor the given parameters, the minimizing integer is $f = 8$, yielding $C_w^{\\text{hier}}(8) \\approx 88.51 \\,\\mu s$.\n\nB. Sleep directly on the arrival count without a generation, relying on futex wait to put threads to sleep, and wake all at once. The wakeup storm cost is purely linear,\n$$\nC_w^{\\text{naive}} = c_s + c_r (N - 1),\n$$\nand hierarchical wake provides no benefit, so choose $f = 1$. For the given parameters, $C_w^{\\text{naive}} \\approx 33.5 \\,\\mu s$.\n\nC. Implement the barrier entirely in the kernel with semaphores; have the last-arriver release a semaphore $N - 1$ times to wake each thread. The wakeup storm cost is\n$$\nC_w^{\\text{naive}} = (N - 1)\\, c_s,\n$$\nand hierarchical wake increases cost due to multiple syscalls, so choose $f = 0$; for the given parameters, $C_w^{\\text{naive}} \\approx 126 \\,\\mu s$.\n\nD. Use a generation counter and futex as in option A, but the hierarchical wake cost is\n$$\nC_w^{\\text{hier}}(f) = (1 + f)c_s + c_r (N - 1) + \\frac{\\sigma}{P} \\left(\\frac{N - 1}{f}\\right)^2,\n$$\nand the optimal fanout is $f \\approx \\sqrt{N - 1}$, so choose $f = 8$ and obtain $C_w^{\\text{hier}}(8) \\approx 65 \\,\\mu s$.",
            "solution": "### Solution Derivation\n\n**1. Barrier Design Principle**\nA correct futex-based barrier must use a generation counter to avoid the \"lost wakeup\" race condition. A waiting thread loops, checking the generation number before calling `futex_wait`. The last arriving thread increments the generation counter (with appropriate memory ordering) and then issues `futex_wake` calls. This design handles spurious wakeups and prevents missed wakeups. Option A correctly describes this design.\n\n**2. Naive Wake Strategy Cost**\nThe naive strategy involves one `futex_wake` call for $k = N-1$ threads. The cost is given by the cost function $C_w(k)$:\n$$\nC_w^{\\text{naive}} = C_w(N-1) = c_s + c_r (N-1) + \\frac{\\sigma}{P} (N-1)^2\n$$\nSubstituting the parameters $N=64, P=16, c_s=2, c_r=0.5, \\sigma=1.0$:\n$$\nC_w^{\\text{naive}} = 2 + 0.5(63) + \\frac{1.0}{16}(63)^2 = 2 + 31.5 + 248.0625 = 281.5625 \\,\\mu s\n$$\nThis matches the value in Option A.\n\n**3. Hierarchical Wake Strategy Cost and Optimization**\nThe hierarchical strategy involves one wake call for $f$ leaders and $f$ subsequent calls, each waking $(N-1)/f$ threads. The total cost is the sum of the costs of these $1+f$ calls:\n$$\nC_w^{\\text{hier}}(f) = C_w(f) + f \\cdot C_w\\left(\\frac{N-1}{f}\\right)\n$$\n$$\nC_w^{\\text{hier}}(f) = \\left(c_s + c_r f + \\frac{\\sigma}{P} f^2\\right) + f \\left( c_s + c_r \\frac{N-1}{f} + \\frac{\\sigma}{P} \\left(\\frac{N-1}{f}\\right)^2 \\right)\n$$\nSimplifying this expression gives:\n$$\nC_w^{\\text{hier}}(f) = (1+f)c_s + c_r(f + N-1) + \\frac{\\sigma}{P}\\left(f^2 + \\frac{(N-1)^2}{f}\\right)\n$$\nThis matches the formula in Option A. To find the optimal integer fanout $f$, we can evaluate this cost function for integer values of $f$ near the minimum (found by setting the derivative to zero).\n*   For $f=8$:\n$C_w^{\\text{hier}}(8) = (1+8) \\cdot 2 + 0.5(8+63) + \\frac{1}{16}\\left(8^2 + \\frac{63^2}{8}\\right) = 18 + 35.5 + \\frac{1}{16}(64 + 496.125) = 53.5 + 35.0078125 \\approx 88.51 \\,\\mu s$.\n*   For $f=9$:\n$C_w^{\\text{hier}}(9) \\approx 88.63 \\,\\mu s$.\n\nThe cost is minimized at $f=8$. Option A correctly identifies the robust barrier design, provides the correct cost formulas, and finds the correct optimal fanout and its associated cost. The other options are flawed either in their barrier design, their cost model application, or their calculations.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}