{
    "hands_on_practices": [
        {
            "introduction": "One of the most fundamental challenges in NUMA systems is the trade-off between memory access latency and memory consumption. To achieve the lowest possible latency, we might want to place data local to every thread that needs it, but this can lead to significant memory overhead. This practice challenges you to quantify this trade-off by analyzing a common optimization: replicating a read-only data structure across all NUMA nodes to ensure local access for every thread .",
            "id": "3663572",
            "problem": "A multiprocessor machine uses Non-Uniform Memory Access (NUMA), where the time to access memory depends on whether a thread accesses memory local to its own node or memory attached to a different node. Consider a read-mostly configuration key-value data structure used by threads uniformly distributed across nodes. Assume the following scientifically realistic parameters and conditions:\n\n- There are $8$ NUMA nodes. Threads are uniformly distributed across the $8$ nodes.\n- The average local DRAM access latency is $80$ nanoseconds, denoted $L_{\\text{loc}} = 80~\\text{ns}$.\n- The average remote DRAM access latency (to any non-local node) is $150$ nanoseconds, denoted $L_{\\text{rem}} = 150~\\text{ns}$.\n- Each configuration lookup experiences a single DRAM-dominant cache miss whose latency equals the average memory access latency for that lookup.\n- The baseline places a single shared copy of the configuration store entirely in the memory of a single node by first-touch allocation. Under the baseline, a lookup is local only if the executing thread happens to be on that node.\n- A proposed read-replica strategy keeps one read-only copy of the configuration store on every node. Replication is done at boot and there are no runtime write costs. Under this strategy, lookups are always served from local memory on each node.\n- The configuration store has size $64$ mebibytes (MiB), denoted $S = 64~\\text{MiB}$.\n\nUsing only the core definition of expected value and the distinction between local and remote DRAM access in a NUMA system, do the following:\n\n1. Derive an expression for the expected per-lookup latency under the baseline and under the per-node replica strategy.\n2. From these expressions, derive the average per-lookup latency reduction due to replication.\n3. Compute the additional memory overhead (in MiB) introduced by keeping one copy per node instead of a single copy.\n4. Define the ratio $\\rho$ as the average per-lookup latency reduction (in microseconds) divided by the additional memory overhead (in MiB). Compute $\\rho$.\n\nExpress the final ratio $\\rho$ in microseconds per mebibyte (MiB) and round your answer to four significant figures.",
            "solution": "The user wants to analyze the performance trade-offs of memory replication in a Non-Uniform Memory Access (NUMA) system. I will first validate the problem statement.\n\n### Step 1: Extract Givens\n- Number of NUMA nodes, $N = 8$.\n- Threads are uniformly distributed across the $8$ nodes.\n- Average local DRAM access latency, $L_{\\text{loc}} = 80~\\text{ns}$.\n- Average remote DRAM access latency, $L_{\\text{rem}} = 150~\\text{ns}$.\n- Each lookup results in a single cache miss with a latency equal to the average memory access latency.\n- Baseline strategy: A single copy of the configuration store is located on one node.\n- Replica strategy: One read-only copy of the store is located on every node.\n- Size of the configuration store, $S = 64~\\text{MiB}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as NUMA is a standard computer architecture, and the provided latency values ($80~\\text{ns}$ and $150~\\text{ns}$) are realistic for modern systems. The problem is well-posed; it provides all necessary constants and a clear set of assumptions (e.g., uniform thread distribution) to allow for a unique, computable solution. The language is objective and precise. The problem does not violate any fundamental principles, is not based on false premises, is not metaphorical, is directly relevant to the topic of operating systems, and is not missing any critical information. The setup is self-consistent and leads to a meaningful analysis of the trade-off between memory overhead and latency reduction, a core concept in systems performance engineering.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full solution.\n\nThe problem requires a four-part analysis: deriving expected latencies, computing the latency reduction, calculating memory overhead, and finally, computing the ratio of latency reduction to memory overhead.\n\nLet $N$ be the number of NUMA nodes, so $N=8$. Let $L_{\\text{loc}}$ be the local access latency, $L_{\\text{loc}} = 80~\\text{ns}$, and $L_{\\text{rem}}$ be the remote access latency, $L_{\\text{rem}} = 150~\\text{ns}$. The data store size is $S = 64~\\text{MiB}$.\n\n**1. Expected Per-Lookup Latency**\n\nThe expected latency of an operation is the sum of the latencies of possible outcomes, weighted by their probabilities. We denote the expected latency for a given strategy as $E[L]$.\n\n- **Per-Node Replica Strategy:**\nIn this strategy, a full copy of the configuration store resides on each of the $N=8$ nodes. Since threads are distributed across all nodes, a thread on any given node will always find a local copy of the data. Therefore, every memory access is a local access. The latency for every lookup is deterministically $L_{\\text{loc}}$.\nThe expected latency under the replica strategy, $E[L_{\\text{rep}}]$, is:\n$$E[L_{\\text{rep}}] = L_{\\text{loc}} = 80~\\text{ns}$$\n\n- **Baseline Strategy:**\nIn the baseline, a single copy of the store is placed on one specific node (the \"memory node\"). Since threads are uniformly distributed across all $N=8$ nodes, the probability that a thread making a request is on the same node as the memory is $P(\\text{local}) = \\frac{1}{N}$. The probability that the thread is on a different node is $P(\\text{remote}) = 1 - \\frac{1}{N} = \\frac{N-1}{N}$.\nThe expected latency under the baseline strategy, $E[L_{\\text{base}}]$, is given by the law of total expectation:\n$$E[L_{\\text{base}}] = P(\\text{local}) \\cdot L_{\\text{loc}} + P(\\text{remote}) \\cdot L_{\\text{rem}}$$\n$$E[L_{\\text{base}}] = \\frac{1}{N} L_{\\text{loc}} + \\frac{N-1}{N} L_{\\text{rem}}$$\nSubstituting the given values:\n$$E[L_{\\text{base}}] = \\frac{1}{8} (80~\\text{ns}) + \\frac{8-1}{8} (150~\\text{ns}) = \\frac{1}{8} (80~\\text{ns}) + \\frac{7}{8} (150~\\text{ns})$$\n$$E[L_{\\text{base}}] = 10~\\text{ns} + \\frac{1050}{8}~\\text{ns} = 10~\\text{ns} + 131.25~\\text{ns} = 141.25~\\text{ns}$$\n\n**2. Average Per-Lookup Latency Reduction**\n\nThe average latency reduction, $\\Delta L$, is the difference between the expected latency of the baseline strategy and the replica strategy.\n$$\\Delta L = E[L_{\\text{base}}] - E[L_{\\text{rep}}]$$\n$$\\Delta L = 141.25~\\text{ns} - 80~\\text{ns} = 61.25~\\text{ns}$$\nSymbolically, the reduction is:\n$$\\Delta L = \\left(\\frac{1}{N} L_{\\text{loc}} + \\frac{N-1}{N} L_{\\text{rem}}\\right) - L_{\\text{loc}} = \\frac{N-1}{N} L_{\\text{rem}} - \\left(1 - \\frac{1}{N}\\right) L_{\\text{loc}} = \\frac{N-1}{N} (L_{\\text{rem}} - L_{\\text{loc}})$$\nUsing the symbolic form with our values:\n$$\\Delta L = \\frac{8-1}{8} (150~\\text{ns} - 80~\\text{ns}) = \\frac{7}{8} (70~\\text{ns}) = 61.25~\\text{ns}$$\nThe problem requires this value in microseconds for the final ratio. Since $1~\\mu s = 1000~\\text{ns}$:\n$$\\Delta L = 61.25 \\times 10^{-3}~\\mu s = 0.06125~\\mu s$$\n\n**3. Additional Memory Overhead**\n\nThe memory overhead is the additional memory consumed by the replica strategy compared to the baseline.\n- Memory used in baseline: $M_{\\text{base}} = S = 64~\\text{MiB}$.\n- Memory used in replica strategy: $M_{\\text{rep}} = N \\times S = 8 \\times 64~\\text{MiB} = 512~\\text{MiB}$.\n\nThe additional memory overhead, $\\Delta M$, is:\n$$\\Delta M = M_{\\text{rep}} - M_{\\text{base}} = NS - S = (N-1)S$$\nSubstituting the given values:\n$$\\Delta M = (8-1) \\times 64~\\text{MiB} = 7 \\times 64~\\text{MiB} = 448~\\text{MiB}$$\n\n**4. Ratio $\\rho$**\n\nThe ratio $\\rho$ is defined as the average per-lookup latency reduction (in microseconds) divided by the additional memory overhead (in MiB).\n$$\\rho = \\frac{\\Delta L \\text{ (in } \\mu s)}{\\Delta M \\text{ (in MiB)}}$$\nUsing the values calculated in the previous steps:\n$$\\rho = \\frac{0.06125}{448}~\\frac{\\mu s}{\\text{MiB}}$$\nNow, we perform the division:\n$$\\rho \\approx 0.00013671875~\\frac{\\mu s}{\\text{MiB}}$$\nThe problem requires the answer to be rounded to four significant figures.\nThe first four significant figures are $1$, $3$, $6$, and $7$. The following digit is $1$, so we round down.\n$$\\rho \\approx 0.0001367~\\frac{\\mu s}{\\text{MiB}}$$\nExpressing this result in scientific notation as required:\n$$\\rho = 1.367 \\times 10^{-4}~\\frac{\\mu s}{\\text{MiB}}$$",
            "answer": "$$\\boxed{1.367 \\times 10^{-4}}$$"
        },
        {
            "introduction": "While optimizing for latency is important, any memory allocation strategy must operate within the hard physical constraints of the system. An allocation policy that is not mindful of per-node capacity can lead to one node becoming exhausted while others remain underutilized. In this exercise, you will explore how an interleaving policy interacts with a system's physical memory limits, deriving the maximum application footprint a system can support before memory starvation occurs on a given node .",
            "id": "3663621",
            "problem": "A two-node Non-Uniform Memory Access (NUMA) system has memory capacities $C_0 = 64$ GiB on node $0$ and $C_1 = 32$ GiB on node $1$. A single process has a memory footprint of size $F$ GiB that the operating system allocates in page units. Assume a fixed page size of $P = 4$ KiB and that there is no other background memory usage and no kernel reservation beyond the stated capacities. Under an interleaving placement policy, a fraction $w$ of the process’s pages are placed on node $1$, and the remaining fraction $1 - w$ are placed on node $0$, throughout the lifetime of the allocation. Define starvation of node $1$ as the event that node $1$ exhausts its free capacity before the process’s entire footprint has been allocated, forcing any remaining pages that would have been placed on node $1$ to be placed on node $0$.\n\nStarting only from the core definitions of page-granular allocation, capacity constraints, and the interleaving placement described above, derive a closed-form expression for the largest footprint $F^{\\star}(w)$ (in GiB) for which interleaving with parameter $w$ avoids starvation of node $1$. Then, specialize your expression to the equal-interleaving case $w = \\tfrac{1}{2}$ with the given capacities $C_0 = 64$ GiB and $C_1 = 32$ GiB. Report the final numeric value of $F^{\\star}\\!\\left(\\tfrac{1}{2}\\right)$ in GiB. Do not include units in your final boxed answer. No rounding is required.",
            "solution": "The user wants to solve a problem about memory allocation in a two-node Non-Uniform Memory Access (NUMA) system.\n\n### Step 1: Extract Givens\n-   System: A two-node NUMA system (node $0$ and node $1$).\n-   Node 0 capacity: $C_0 = 64$ GiB.\n-   Node 1 capacity: $C_1 = 32$ GiB.\n-   Process memory footprint: $F$ GiB.\n-   Page size: $P = 4$ KiB.\n-   Interleaving policy: A fraction $w$ of the process's pages are placed on node $1$, and the remaining fraction $1-w$ are placed on node $0$.\n-   Assumptions: No other background memory usage or kernel reservations.\n-   Definition of starvation: Starvation of node $1$ is defined as the event that node $1$ exhausts its free capacity before the process’s entire footprint has been allocated, forcing any remaining pages that would have been placed on node $1$ to be placed on node $0$.\n-   Objective 1: Derive a closed-form expression for the largest footprint $F^{\\star}(w)$ (in GiB) for which interleaving with parameter $w$ avoids starvation of node $1$.\n-   Objective 2: Calculate the numerical value of $F^{\\star}(\\frac{1}{2})$ using the given capacities.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem describes a simplified but fundamentally sound model of NUMA memory management. The concepts (nodes, capacity, interleaving, page-based allocation) are standard in operating systems and computer architecture. The model is a valid abstraction for analysis. No flaws found.\n2.  **Non-Formalizable or Irrelevant**: The problem is well-defined and can be formalized mathematically. It is directly relevant to the topic of NUMA memory policies. No flaws found.\n3.  **Incomplete or Contradictory Setup**: All necessary parameters ($C_0$, $C_1$, $w$) are provided. The definition of starvation is explicit. The problem is self-contained. The mention of page size $P$ is consistent with the allocation being described as a fraction of pages, as the total memory on a node is the number of pages times the page size. This means the page size will cancel out when calculating memory usage in terms of the total footprint $F$. The setup is complete and consistent. No flaws found.\n4.  **Unrealistic or Infeasible**: The memory capacities are realistic for modern servers. The simplifying assumption of no other memory usage is a standard practice for creating a tractable analytical problem. No flaws found.\n5.  **Ill-Posed or Poorly Structured**: The problem is clearly stated. It asks for a maximum value under a well-defined constraint (\"avoids starvation\"). This structure leads to a unique and meaningful solution. No flaws found.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires careful interpretation of the constraints and system limits, involving more than a trivial substitution. It is a legitimate exercise in resource constraint analysis. No flaws found.\n7.  **Outside Scientific Verifiability**: The derivation is mathematical and entirely verifiable. No flaws found.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A reasoned solution will be provided.\n\n### Solution Derivation\nThe problem asks for the largest process footprint, denoted $F^{\\star}(w)$, that can be allocated on a two-node NUMA system without causing \"starvation\" of node $1$. The allocation is governed by an interleaving policy.\n\nLet $F$ be the total memory footprint of the process in GiB. According to the interleaving policy, a fraction $w$ of the process's pages are allocated on node $1$ and the remaining fraction $1-w$ are allocated on node $0$. Since the page size $P$ is uniform, these fractions apply directly to the memory footprint size.\n\nThe memory allocated to node $0$, $M_0$, is:\n$$M_0 = (1 - w)F$$\n\nThe memory allocated to node $1$, $M_1$, is:\n$$M_1 = wF$$\n\nThe problem defines \"starvation of node $1$\" as the event where node $1$ exhausts its capacity before the entire footprint is allocated. This occurs if the memory intended for node $1$ exceeds its capacity, $C_1$. To *avoid* starvation of node $1$, the memory allocated to it must not exceed its capacity:\n$$M_1 \\le C_1$$\nSubstituting the expression for $M_1$:\n$$wF \\le C_1$$\nThis inequality gives an upper bound on the footprint $F$ based on the capacity of node $1$:\n$$F \\le \\frac{C_1}{w} \\quad (\\text{for } w  0)$$\n\nHowever, the problem implicitly requires the *entire* process footprint $F$ to be successfully allocated. A successful allocation under the specified policy means that the memory intended for *both* nodes must fit within their respective capacities. Therefore, the memory allocated to node $0$ must also not exceed its capacity, $C_0$:\n$$M_0 \\le C_0$$\nSubstituting the expression for $M_0$:\n$$(1 - w)F \\le C_0$$\nThis inequality gives a second upper bound on the footprint $F$ based on the capacity of node $0$:\n$$F \\le \\frac{C_0}{1-w} \\quad (\\text{for } w  1)$$\n\nFor a complete and successful allocation of footprint $F$ to occur (which inherently avoids starvation on either node), both conditions must be satisfied simultaneously. Therefore, $F$ must be less than or equal to the minimum of these two upper bounds. The largest footprint, $F^{\\star}(w)$, for which the allocation is successful is the tightest constraint.\n\n$$F^{\\star}(w) = \\min\\left(\\frac{C_0}{1-w}, \\frac{C_1}{w}\\right)$$\n\nThis is the required closed-form expression for the largest footprint that avoids starvation, as it represents the maximum footprint that can be fully allocated according to the policy without exhausting the capacity of either node.\n\nNext, we specialize this expression for the given parameters:\n-   Capacities: $C_0 = 64$ GiB, $C_1 = 32$ GiB.\n-   Interleaving fraction: $w = \\frac{1}{2}$.\n\nSubstituting these values into the expression for $F^{\\star}(w)$:\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = \\min\\left(\\frac{C_0}{1 - \\frac{1}{2}}, \\frac{C_1}{\\frac{1}{2}}\\right)$$\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = \\min\\left(\\frac{C_0}{\\frac{1}{2}}, \\frac{C_1}{\\frac{1}{2}}\\right)$$\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = \\min(2C_0, 2C_1)$$\n\nNow, we insert the numerical values for the capacities $C_0 = 64$ and $C_1 = 32$:\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = \\min(2 \\times 64, 2 \\times 32)$$\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = \\min(128, 64)$$\nThe minimum of these two values is $64$.\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = 64$$\n\nThus, the largest footprint that can be allocated with an equal-interleaving policy on this system is $64$ GiB. At this size, node $1$ is completely filled ($M_1 = \\frac{1}{2} \\times 64 = 32$ GiB, which equals $C_1$), while node $0$ has space to spare ($M_0 = \\frac{1}{2} \\times 64 = 32$ GiB, which is less than $C_0 = 64$ GiB). Any larger footprint would cause starvation of node $1$.",
            "answer": "$$\\boxed{64}$$"
        },
        {
            "introduction": "Theoretical models become powerful when applied to real-world scenarios with the tools provided by modern operating systems. In this case study, we will investigate the behavior of a production web server experiencing a memory leak on a NUMA machine. You will apply your knowledge of Linux memory policies and control groups to predict how the system responds to memory pressure, illustrating how administrators can choose policies to either strictly contain a problem or allow for graceful degradation .",
            "id": "3663644",
            "problem": "Consider a production web server deployed on a machine with Non-Uniform Memory Access (NUMA). The system has $N=2$ nodes: Node $0$ and Node $1$, each with $128$ gibibytes (GiB) of physical memory. The server uses $W=16$ worker processes, with $8$ workers pinned to Central Processing Unit (CPU) cores on Node $0$ and $8$ pinned to CPU cores on Node $1$. The operating system uses a first-touch page allocation policy: pages are allocated on the NUMA node where the allocating thread first touches the memory, subject to the process’s memory policy.\n\nAll workers are identical except for a bug that introduces a memory leak confined to the $8$ workers on Node $0$. Each Node $0$ worker has a steady-state working set of $3$ GiB and additionally leaks $0.5$ GiB per minute of anonymous, unswappable memory. Assume swap is disabled. The operating system’s own memory usage on Node $0$ is approximately $4$ GiB. The workload is largely compute and network bound with negligible file cache, so page cache reclaim does not materially slow the leak’s consumption of free memory.\n\nTwo experiments are conducted:\n\n- Experiment $1$: Node $0$ workers are placed in a memory control group (memcg) limited to Node $0$’s capacity and configured with a strict memory policy (MPOL\\_BIND) allowing allocations only on Node $0$ (cpuset.mems restricted to Node $0$). Node $1$ workers use Node $1$ without leaks and have no special limits other than being pinned to Node $1$ CPUs.\n\n- Experiment $2$: Node $0$ workers are configured with a preferred memory policy (MPOL\\_PREFERRED) to Node $0$ but permitted to allocate from Node $1$ when Node $0$ is out of free memory (cpuset.mems includes Node $0$ and Node $1$). The memcg for Node $0$ workers allows growth across both nodes up to the combined capacity.\n\nBased on fundamental definitions of Non-Uniform Memory Access (NUMA), first-touch allocation, memory policies, and memory control groups (memcg), select all statements that are correct.\n\nA. In Experiment $1$, when Node $0$ exhausts free memory, the Out Of Memory (OOM) event is confined to the Node $0$ memory control group, and allocations for its workers do not spill into Node $1$.\n\nB. In Experiment $2$, once Node $0$ exhausts free memory, subsequent allocations for Node $0$ workers are satisfied from Node $1$, and their average memory access time increases due to remote accesses.\n\nC. Given the leak rate and initial free memory on Node $0$, the time at which Node $0$ first stops allocating locally and begins falling back to remote pages in Experiment $2$ is approximately $25$ minutes, assuming the leak uses anonymous unswappable memory and swap is disabled.\n\nD. In Experiment $1$, the Linux Out Of Memory (OOM) killer terminates workers on both nodes because OOM decisions are always system-wide and ignore memory control groups and allowed node sets.\n\nE. In Experiment $2$, Node $1$ workers are guaranteed to see no throughput degradation during remote spillover because their pages remain local; cross-node traffic does not contend for the Node $1$ memory controllers or interconnect.",
            "solution": "Herein, I will first validate the problem statement and, finding it valid, proceed to a detailed analysis of the system's behavior and an evaluation of each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   System Architecture: Non-Uniform Memory Access (NUMA)\n*   Number of NUMA nodes: $N=2$ (Node $0$ and Node $1$)\n*   Memory per node: $128$ gibibytes (GiB)\n*   Total worker processes: $W=16$\n*   Worker pinning: $8$ workers pinned to CPU cores on Node $0$; $8$ workers pinned to CPU cores on Node $1$.\n*   OS allocation policy: first-touch page allocation.\n*   Memory leak: Confined to the $8$ workers on Node $0$.\n*   Node $0$ worker memory profile:\n    *   Steady-state working set: $3$ GiB per worker.\n    *   Leak rate: $0.5$ GiB per minute per worker.\n    *   Leaked memory type: anonymous, unswappable.\n*   Swap: disabled.\n*   OS memory usage on Node $0$: approximately $4$ GiB.\n*   File cache: negligible impact.\n*   Experiment $1$ configuration for Node $0$ workers:\n    *   Memory control group (memcg) limited to Node $0$'s capacity.\n    *   Strict memory policy (MPOL_BIND) allowing allocations only on Node $0$.\n    *   `cpuset.mems` is restricted to Node $0$.\n*   Experiment $2$ configuration for Node $0$ workers:\n    *   Preferred memory policy (MPOL_PREFERRED) to Node $0$.\n    *   Permitted to allocate from Node $1$ on fallback.\n    *   `cpuset.mems` includes Node $0$ and Node $1$.\n    *   The memcg allows growth across both nodes.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded:** The problem is firmly rooted in the principles of modern computer architecture and operating systems, specifically NUMA, CPU/memory pinning (`cpuset`), memory allocation policies (`mbind` with `MPOL_BIND`/`MPOL_PREFERRED`), and resource isolation (`memcg`). These are all well-defined, real-world mechanisms, particularly in the Linux kernel.\n*   **Well-Posed:** The problem is clearly stated with two distinct experimental setups. The provided numerical data is sufficient to analyze a time-dependent behavior (the memory leak). The questions posed relate directly to the expected outcomes of the described configurations.\n*   **Objective:** The language is technical and precise. It describes system configurations and behaviors without subjectivity or ambiguity. The terms used have specific, formal meanings in the context of operating systems.\n*   **Consistency and Completeness:** The setup is self-consistent. The parameters (memory sizes, worker counts, leak rates) are provided and allow for quantitative analysis. For instance, the total memory, OS overhead, and initial worker sets are specified, allowing a calculation of the initial free memory. There are no contradictions.\n*   **Realism:** The scenario is highly realistic. Production servers often run on NUMA hardware, and memory leaks are a common class of software bug. The use of cgroups (`memcg`, `cpuset`) to manage workloads is standard practice in containerized and virtualized environments. The described policies and their effects are true to life.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and complete. I will proceed with a full solution.\n\n### Derivation and Option Analysis\n\nFirst, I will establish the initial state of memory on Node $0$.\n*   Total memory on Node $0$: $128$ GiB.\n*   Memory consumed by the operating system on Node $0$: $4$ GiB.\n*   Initial steady-state memory consumed by the $8$ workers on Node $0$: $8 \\text{ workers} \\times 3 \\text{ GiB/worker} = 24$ GiB.\n*   Total initial memory usage on Node $0$: $4 \\text{ GiB} + 24 \\text{ GiB} = 28$ GiB.\n*   Initial free memory on Node $0$ available to be consumed by the leak: $128 \\text{ GiB} - 28 \\text{ GiB} = 100$ GiB.\n\nNext, I will determine the total rate of memory consumption by the leak.\n*   Total leak rate from $8$ workers on Node $0$: $8 \\text{ workers} \\times 0.5 \\text{ GiB/min/worker} = 4$ GiB/min.\n\nNow, I will analyze the behavior in each experiment and evaluate the corresponding options.\n\n**A. In Experiment 1, when Node 0 exhausts free memory, the Out Of Memory (OOM) event is confined to the Node 0 memory control group, and allocations for its workers do not spill into Node 1.**\n\n*   **Analysis:** In Experiment $1$, the Node $0$ workers are subject to two constraints that prevent allocation on Node $1$:\n    1.  `cpuset.mems` is restricted to Node $0$. This is a powerful cgroup-based constraint that forbids the kernel from allocating pages for these processes on any other node.\n    2.  The `MPOL_BIND` policy is also specified, which strictly requires allocations on the given nodemask (Node $0$). If memory is unavailable on the specified node(s), the allocation fails.\n    Consequently, when the $100$ GiB of free memory on Node $0$ is exhausted, any further allocation request from a Node $0$ worker will fail. This allocation failure will trigger an Out Of Memory (OOM) condition.\n    The problem states these workers are in a memory control group (`memcg`). A primary function of `memcg` is to isolate memory resources. When a process within a `memcg` triggers an OOM condition by exceeding the cgroup's limits, the kernel invokes the OOM killer specifically for that `memcg` (a \"memcg OOM\"). It does not trigger a system-wide OOM. Therefore, the OOM event is confined to the Node $0$ `memcg`, and processes outside this group (like the workers on Node $1$) are not considered candidates for termination. The statement is entirely consistent with the expected behavior of these Linux kernel mechanisms.\n*   **Verdict: Correct**\n\n**B. In Experiment 2, once Node 0 exhausts free memory, subsequent allocations for Node 0 workers are satisfied from Node 1, and their average memory access time increases due to remote accesses.**\n\n*   **Analysis:** In Experiment $2$, the policy is `MPOL_PREFERRED` for Node $0$, and `cpuset.mems` allows allocations on both Node $0$ and Node $1$. The \"preferred\" policy directs the kernel to attempt allocations on Node $0$ first. However, unlike `MPOL_BIND`, it allows the kernel to fall back to other allowed nodes (per `cpuset.mems`) if the preferred node is out of memory.\n    Therefore, once Node $0$ is full, subsequent allocation requests from Node $0$ workers will be serviced by allocating pages from Node $1$. Since the worker processes are running on Node $0$ CPUs, any accesses to these newly allocated pages on Node $1$ will be \"remote\" accesses. Remote memory access incurs higher latency and offers lower bandwidth compared to local memory access due to the need to traverse the NUMA interconnect (e.g., QPI or UPI). As the proportion of remote pages in a process's working set increases, its average memory access time will degrade (increase).\n*   **Verdict: Correct**\n\n**C. Given the leak rate and initial free memory on Node 0, the time at which Node 0 first stops allocating locally and begins falling back to remote pages in Experiment 2 is approximately 25 minutes, assuming the leak uses anonymous unswappable memory and swap is disabled.**\n\n*   **Analysis:** This is a quantitative claim that can be verified with the givens.\n    *   Initial free memory on Node $0$: $100$ GiB (as calculated above).\n    *   Total leak rate: $4$ GiB/min (as calculated above).\n    The time to exhaust the free memory is the total free memory divided by the rate of consumption.\n    $$ \\text{Time} = \\frac{\\text{Initial Free Memory}}{\\text{Total Leak Rate}} = \\frac{100 \\text{ GiB}}{4 \\text{ GiB/min}} = 25 \\text{ minutes} $$\n    At the $25$-minute mark, Node $0$ will have exhausted its free memory. In Experiment $2$, this is precisely the point at which the `MPOL_PREFERRED` policy will cause the kernel to begin falling back to Node $1$ for new allocations. The calculation is correct.\n*   **Verdict: Correct**\n\n**D. In Experiment 1, the Linux Out Of Memory (OOM) killer terminates workers on both nodes because OOM decisions are always system-wide and ignore memory control groups and allowed node sets.**\n\n*   **Analysis:** This statement is fundamentally incorrect. It misrepresents the purpose and function of memory control groups (`memcg`). A key feature of `memcg` is to enable per-group memory accounting and limit enforcement. When a cgroup's memory limit is breached, it triggers a `memcg`-specific OOM event. The OOM killer is then invoked only on the processes within that offending cgroup. The statement \"OOM decisions are always system-wide\" is false; `memcg` OOM events are, by design, localized. Therefore, the workers on Node $1$, which are not in the Node $0$ workers' `memcg`, would be completely unaffected by the OOM event.\n*   **Verdict: Incorrect**\n\n**E. In Experiment 2, Node 1 workers are guaranteed to see no throughput degradation during remote spillover because their pages remain local; cross-node traffic does not contend for the Node 1 memory controllers or interconnect.**\n\n*   **Analysis:** This statement is incorrect. It makes a false claim about the nature of NUMA hardware. The NUMA interconnect, which connects Node $0$ and Node $1$, is a shared resource. The memory controller on Node $1$ is also a shared resource. When Node $0$ CPUs begin accessing memory physically located on Node $1$ (the \"remote spillover\"), this traffic must traverse the interconnect and be serviced by the Node $1$ memory controller. This creates contention. The local memory requests from Node $1$ workers must compete with the remote memory requests from Node $0$ workers for access to the Node $1$ memory controller and for bandwidth on the interconnect. This contention will increase the effective memory latency for Node $1$ workers, which can lead to performance and throughput degradation. The word \"guaranteed\" makes the statement definitively false, as any level of contention violates this guarantee.\n*   **Verdict: Incorrect**",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}