{
    "hands_on_practices": [
        {
            "introduction": "在 NUMA 系统中，一个核心的性能挑战是在内存开销和访问延迟之间做出权衡。将数据复制到每个节点可以最大限度地减少远程访问，但这会消耗大量内存。这个练习将指导你通过一个具体的场景，使用期望值的基本概念来量化这种权衡，从而让你能够为数据放置策略做出有理有据的决策。",
            "id": "3663572",
            "problem": "一台多处理器机器使用非统一内存访问 (NUMA) 架构，其中访问内存的时间取决于线程访问的是其自身节点的本地内存还是附加到不同节点的内存。考虑一个以读为主的配置键值数据结构，由均匀分布在各个节点上的线程使用。假设存在以下科学上符合现实的参数和条件：\n\n- 有 $8$ 个 NUMA 节点。线程均匀分布在这 $8$ 个节点上。\n- 平均本地 DRAM 访问延迟为 $80$ 纳秒，记作 $L_{\\text{loc}} = 80~\\text{ns}$。\n- 平均远程 DRAM 访问延迟（到任何非本地节点）为 $150$ 纳秒，记作 $L_{\\text{rem}} = 150~\\text{ns}$。\n- 每次配置查找都会经历一次由 DRAM 主导的缓存未命中，其延迟等于该次查找的平均内存访问延迟。\n- 基准方案通过首次接触分配策略，将配置存储的单个共享副本完全放置在单个节点的内存中。在基准方案下，只有当执行线程恰好位于该节点时，查找才是本地的。\n- 一种提议的读取副本策略在每个节点上都保留一个配置存储的只读副本。复制在启动时完成，没有运行时写入成本。在此策略下，查找总是由每个节点上的本地内存提供服务。\n- 配置存储的大小为 $64$ mebibytes (MiB)，记作 $S = 64~\\text{MiB}$。\n\n仅使用期望值的核心定义以及 NUMA 系统中本地和远程 DRAM 访问的区别，完成以下任务：\n\n1. 推导在基准方案和每节点副本策略下，单次查找的期望延迟表达式。\n2. 根据这些表达式，推导因复制而带来的平均单次查找延迟降低量。\n3. 计算在每个节点上保留一个副本（而不是单个副本）所引入的额外内存开销（以 MiB 为单位）。\n4. 将比率 $\\rho$ 定义为平均单次查找延迟降低量（以微秒为单位）除以额外内存开销（以 MiB 为单位）。计算 $\\rho$。\n\n以微秒每 mebibyte (MiB) 为单位表示最终比率 $\\rho$，并将您的答案四舍五入到四位有效数字。",
            "solution": "用户希望分析在非统一内存访问 (NUMA) 系统中内存复制的性能权衡。我将首先验证问题陈述。\n\n### 第 1 步：提取给定条件\n- NUMA 节点数，$N = 8$。\n- 线程均匀分布在这 $8$ 个节点上。\n- 平均本地 DRAM 访问延迟，$L_{\\text{loc}} = 80~\\text{ns}$。\n- 平均远程 DRAM 访问延迟，$L_{\\text{rem}} = 150~\\text{ns}$。\n- 每次查找导致一次缓存未命中，其延迟等于平均内存访问延迟。\n- 基准策略：配置存储的单个副本位于一个节点上。\n- 副本策略：存储的一个只读副本位于每个节点上。\n- 配置存储的大小，$S = 64~\\text{MiB}$。\n\n### 第 2 步：使用提取的给定条件进行验证\n这个问题具有科学依据，因为 NUMA 是一种标准的计算机体系结构，并且提供的延迟值（$80~\\text{ns}$ 和 $150~\\text{ns}$）对于现代系统是符合现实的。这个问题定义明确；它提供了所有必要的常数和一套清晰的假设（例如，线程均匀分布），从而可以得出一个唯一的、可计算的解。语言客观而精确。这个问题没有违反任何基本原则，不基于错误的前提，不是比喻性的，与操作系统主题直接相关，并且没有缺少任何关键信息。该设置是自洽的，并导向了对内存开销和延迟降低之间权衡的有意义的分析，这是系统性能工程中的一个核心概念。\n\n### 第 3 步：结论与行动\n问题是有效的。我将继续提供完整的解决方案。\n\n问题要求进行四部分分析：推导期望延迟，计算延迟降低量，计算内存开销，最后计算延迟降低量与内存开销的比率。\n\n设 $N$ 为 NUMA 节点数，因此 $N=8$。设 $L_{\\text{loc}}$ 为本地访问延迟，$L_{\\text{loc}} = 80~\\text{ns}$，设 $L_{\\text{rem}}$ 为远程访问延迟，$L_{\\text{rem}} = 150~\\text{ns}$。数据存储的大小为 $S = 64~\\text{MiB}$。\n\n**1. 单次查找的期望延迟**\n\n一项操作的期望延迟是所有可能结果的延迟与其概率的加权和。我们将给定策略的期望延迟表示为 $E[L]$。\n\n- **每节点副本策略：**\n在此策略中，配置存储的完整副本驻留在 $N=8$ 个节点中的每一个上。由于线程分布在所有节点上，任何给定节点上的线程总能找到数据的本地副本。因此，每次内存访问都是本地访问。每次查找的延迟确定性地为 $L_{\\text{loc}}$。\n副本策略下的期望延迟 $E[L_{\\text{rep}}]$ 为：\n$$E[L_{\\text{rep}}] = L_{\\text{loc}} = 80~\\text{ns}$$\n\n- **基准策略：**\n在基准方案中，存储的单个副本被放置在一个特定节点上（“内存节点”）。由于线程均匀分布在所有 $N=8$ 个节点上，发出请求的线程与内存位于同一节点的概率为 $P(\\text{local}) = \\frac{1}{N}$。线程位于不同节点的概率为 $P(\\text{remote}) = 1 - \\frac{1}{N} = \\frac{N-1}{N}$。\n根据全期望定律，基准策略下的期望延迟 $E[L_{\\text{base}}]$ 为：\n$$E[L_{\\text{base}}] = P(\\text{local}) \\cdot L_{\\text{loc}} + P(\\text{remote}) \\cdot L_{\\text{rem}}$$\n$$E[L_{\\text{base}}] = \\frac{1}{N} L_{\\text{loc}} + \\frac{N-1}{N} L_{\\text{rem}}$$\n代入给定值：\n$$E[L_{\\text{base}}] = \\frac{1}{8} (80~\\text{ns}) + \\frac{8-1}{8} (150~\\text{ns}) = \\frac{1}{8} (80~\\text{ns}) + \\frac{7}{8} (150~\\text{ns})$$\n$$E[L_{\\text{base}}] = 10~\\text{ns} + \\frac{1050}{8}~\\text{ns} = 10~\\text{ns} + 131.25~\\text{ns} = 141.25~\\text{ns}$$\n\n**2. 平均单次查找延迟降低量**\n\n平均延迟降低量 $\\Delta L$ 是基准策略和副本策略的期望延迟之差。\n$$\\Delta L = E[L_{\\text{base}}] - E[L_{\\text{rep}}]$$\n$$\\Delta L = 141.25~\\text{ns} - 80~\\text{ns} = 61.25~\\text{ns}$$\n用符号表示，降低量为：\n$$\\Delta L = \\left(\\frac{1}{N} L_{\\text{loc}} + \\frac{N-1}{N} L_{\\text{rem}}\\right) - L_{\\text{loc}} = \\frac{N-1}{N} L_{\\text{rem}} - \\left(1 - \\frac{1}{N}\\right) L_{\\text{loc}} = \\frac{N-1}{N} (L_{\\text{rem}} - L_{\\text{loc}})$$\n使用符号形式并代入我们的值：\n$$\\Delta L = \\frac{8-1}{8} (150~\\text{ns} - 80~\\text{ns}) = \\frac{7}{8} (70~\\text{ns}) = 61.25~\\text{ns}$$\n问题要求在最终比率中使用微秒作为单位。因为 $1~\\mu s = 1000~\\text{ns}$：\n$$\\Delta L = 61.25 \\times 10^{-3}~\\mu s = 0.06125~\\mu s$$\n\n**3. 额外内存开销**\n\n内存开销是副本策略相比基准策略额外消耗的内存。\n- 基准方案使用的内存：$M_{\\text{base}} = S = 64~\\text{MiB}$。\n- 副本策略使用的内存：$M_{\\text{rep}} = N \\times S = 8 \\times 64~\\text{MiB} = 512~\\text{MiB}$。\n\n额外内存开销 $\\Delta M$ 为：\n$$\\Delta M = M_{\\text{rep}} - M_{\\text{base}} = NS - S = (N-1)S$$\n代入给定值：\n$$\\Delta M = (8-1) \\times 64~\\text{MiB} = 7 \\times 64~\\text{MiB} = 448~\\text{MiB}$$\n\n**4. 比率 $\\rho$**\n\n比率 $\\rho$ 定义为平均单次查找延迟降低量（以微秒为单位）除以额外内存开销（以 MiB 为单位）。\n$$\\rho = \\frac{\\Delta L \\text{ (in } \\mu s)}{\\Delta M \\text{ (in MiB)}}$$\n使用前面步骤中计算出的值：\n$$\\rho = \\frac{0.06125}{448}~\\frac{\\mu s}{\\text{MiB}}$$\n现在，我们进行除法运算：\n$$\\rho \\approx 0.00013671875~\\frac{\\mu s}{\\text{MiB}}$$\n问题要求将答案四舍五入到四位有效数字。\n前四位有效数字是 $1$、$3$、$6$ 和 $7$。后面的数字是 $1$，所以我们向下舍入。\n$$\\rho \\approx 0.0001367~\\frac{\\mu s}{\\text{MiB}}$$\n该结果可以用科学记数法表示为：\n$$\\rho = 1.367 \\times 10^{-4}~\\frac{\\mu s}{\\text{MiB}}$$",
            "answer": "$$\\boxed{1.367 \\times 10^{-4}}$$"
        },
        {
            "introduction": "理解了数据放置的“为什么”之后，我们来探讨操作系统“如何”实现它。并非所有 NUMA 节点的内存容量都相同，操作系统必须在分配内存时考虑到这些物理限制。这个练习模拟了一种内存交错分配策略，要求你确定在非对称内存配置下，系统能支持的最大应用内存占用。 通过解决这个问题，你将学会如何通过识别最受限的资源来分析系统的容量上限。",
            "id": "3663621",
            "problem": "一个双节点非统一内存访问 (NUMA) 系统在节点 $0$ 上的内存容量为 $C_0 = 64$ GiB，在节点 $1$ 上的内存容量为 $C_1 = 32$ GiB。单个进程的内存足迹大小为 $F$ GiB，操作系统以页为单位进行分配。假设固定的页大小为 $P = 4$ KiB，并且除了所述容量外，没有其他后台内存使用，也没有内核保留。在交错放置策略下，进程页面的 $w$ 部分放置在节点 $1$ 上，剩余的 $1 - w$ 部分放置在节点 $0$ 上，贯穿整个分配的生命周期。将节点 $1$ 的饥饿定义为以下事件：在进程的整个足迹被分配完毕之前，节点 $1$ 就耗尽了其空闲容量，迫使任何本应放置在节点 $1$ 上的剩余页面都放置到节点 $0$ 上。\n\n仅从页粒度分配、容量限制以及上述交错放置的核心定义出发，推导出一个闭式表达式，用于计算最大内存足迹 $F^{\\star}(w)$（以 GiB 为单位），在该足迹下，参数为 $w$ 的交错放置可以避免节点 $1$ 的饥饿。然后，将您的表达式应用于等量交错情况 $w = \\tfrac{1}{2}$，并使用给定的容量 $C_0 = 64$ GiB 和 $C_1 = 32$ GiB 进行特化。报告 $F^{\\star}\\!\\left(\\tfrac{1}{2}\\right)$ 的最终数值（以 GiB 为单位）。在最终的方框答案中不要包含单位。不需要四舍五入。",
            "solution": "用户想要解决一个关于双节点非统一内存访问 (NUMA) 系统中内存分配的问题。\n\n### 第 1 步：提取已知条件\n-   系统：一个双节点 NUMA 系统（节点 $0$ 和节点 $1$）。\n-   节点 0 容量：$C_0 = 64$ GiB。\n-   节点 1 容量：$C_1 = 32$ GiB。\n-   进程内存足迹：$F$ GiB。\n-   页大小：$P = 4$ KiB。\n-   交错策略：进程页面的 $w$ 部分放置在节点 $1$ 上，剩余的 $1-w$ 部分放置在节点 $0$ 上。\n-   假设：没有其他后台内存使用或内核保留。\n-   饥饿的定义：节点 $1$ 的饥饿被定义为在进程的整个足迹被分配完毕之前，节点 $1$ 就耗尽了其空闲容量，迫使任何本应放置在节点 $1$ 上的剩余页面都放置到节点 $0$ 上的事件。\n-   目标 1：推导出一个闭式表达式，用于计算最大内存足迹 $F^{\\star}(w)$（以 GiB 为单位），在该足迹下，参数为 $w$ 的交错放置可以避免节点 $1$ 的饥饿。\n-   目标 2：使用给定的容量计算 $F^{\\star}(\\frac{1}{2})$ 的数值。\n\n### 第 2 步：使用提取的已知条件进行验证\n1.  **科学或事实不健全**：该问题描述了一个简化但基本健全的 NUMA 内存管理模型。这些概念（节点、容量、交错、基于页的分配）在操作系统和计算机体系结构中是标准的。该模型是用于分析的有效抽象。未发现缺陷。\n2.  **非形式化或不相关**：该问题定义明确，可以进行数学形式化。它与 NUMA 内存策略的主题直接相关。未发现缺陷。\n3.  **不完整或矛盾的设置**：所有必要的参数（$C_0$、$C_1$、$w$）都已提供。饥饿的定义是明确的。该问题是自洽的。提到页大小 $P$ 与将分配描述为页的一部分是一致的，因为节点上的总内存是页数乘以页大小。这意味着在根据总足迹 $F$ 计算内存使用量时，页大小将被抵消。设置是完整且一致的。未发现缺陷。\n4.  **不切实际或不可行**：内存容量对于现代服务器是现实的。没有其他内存使用的简化假设是创建易于处理的分析问题的标准做法。未发现缺陷。\n5.  **不适定或结构不良**：问题陈述清晰。它要求在明确定义的约束（“避免饥饿”）下求最大值。这种结构导向一个唯一且有意义的解。未发现缺陷。\n6.  **伪深刻、琐碎或同义反复**：该问题需要仔细解释约束和系统限制，涉及的不仅仅是简单的代入。这是资源约束分析中的一个合理练习。未发现缺陷。\n7.  **超出科学可验证性**：推导是数学性的，完全可以验证。未发现缺陷。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。将提供一个合理的解答。\n\n### 求解推导\n问题要求在双节点 NUMA 系统上，能够分配而不会导致节点 $1$ “饥饿”的最大进程内存足迹，记为 $F^{\\star}(w)$。分配由交错策略控制。\n\n设 $F$ 为进程的总内存足迹（以 GiB 为单位）。根据交错策略，进程页面的 $w$ 部分分配到节点 $1$ 上，剩余的 $1-w$ 部分分配到节点 $0$ 上。由于页大小 $P$ 是统一的，这些比例直接适用于内存足迹的大小。\n\n分配给节点 $0$ 的内存 $M_0$ 为：\n$$M_0 = (1 - w)F$$\n\n分配给节点 $1$ 的内存 $M_1$ 为：\n$$M_1 = wF$$\n\n问题将“节点 $1$ 的饥饿”定义为在整个足迹被分配完毕之前，节点 $1$ 耗尽其容量的事件。如果为节点 $1$ 预留的内存超过了其容量 $C_1$，就会发生这种情况。为了*避免*节点 $1$ 的饥饿，分配给它的内存不能超过其容量：\n$$M_1 \\le C_1$$\n代入 $M_1$ 的表达式：\n$$wF \\le C_1$$\n这个不等式给出了基于节点 $1$ 容量的足迹 $F$ 的一个上界：\n$$F \\le \\frac{C_1}{w} \\quad (\\text{对于 } w > 0)$$\n\n然而，问题隐含地要求*整个*进程足迹 $F$ 都能成功分配。在指定策略下成功分配意味着为*两个*节点预留的内存都必须符合其各自的容量。因此，分配给节点 $0$ 的内存也不得超过其容量 $C_0$：\n$$M_0 \\le C_0$$\n代入 $M_0$ 的表达式：\n$$(1 - w)F \\le C_0$$\n这个不等式给出了基于节点 $0$ 容量的足迹 $F$ 的第二个上界：\n$$F \\le \\frac{C_0}{1-w} \\quad (\\text{对于 } w  1)$$\n\n为了使足迹 $F$ 的分配能够完整且成功地进行（这本身就避免了任一节点的饥饿），两个条件必须同时满足。因此，$F$ 必须小于或等于这两个上界中的最小值。能够成功分配的最大足迹 $F^{\\star}(w)$ 由最严格的约束决定。\n\n$$F^{\\star}(w) = \\min\\left(\\frac{C_0}{1-w}, \\frac{C_1}{w}\\right)$$\n\n这就是避免饥饿的最大足迹所需的闭式表达式，因为它代表了可以根据策略完全分配而不会耗尽任一节点容量的最大足迹。\n\n接下来，我们将此表达式应用于给定的参数：\n-   容量：$C_0 = 64$ GiB，$C_1 = 32$ GiB。\n-   交错分数：$w = \\frac{1}{2}$。\n\n将这些值代入 $F^{\\star}(w)$ 的表达式中：\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = \\min\\left(\\frac{C_0}{1 - \\frac{1}{2}}, \\frac{C_1}{\\frac{1}{2}}\\right)$$\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = \\min\\left(\\frac{C_0}{\\frac{1}{2}}, \\frac{C_1}{\\frac{1}{2}}\\right)$$\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = \\min(2C_0, 2C_1)$$\n\n现在，我们代入容量的数值 $C_0 = 64$ 和 $C_1 = 32$：\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = \\min(2 \\times 64, 2 \\times 32)$$\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = \\min(128, 64)$$\n这两个值的最小值是 $64$。\n$$F^{\\star}\\left(\\frac{1}{2}\\right) = 64$$\n\n因此，在该系统上使用等量交错策略可以分配的最大足迹是 $64$ GiB。在此大小下，节点 $1$ 将被完全填满（$M_1 = \\frac{1}{2} \\times 64 = 32$ GiB，等于 $C_1$），而节点 $0$ 尚有空余空间（$M_0 = \\frac{1}{2} \\times 64 = 32$ GiB，小于 $C_0 = 64$ GiB）。任何更大的足迹都会导致节点 $1$ 发生饥饿。",
            "answer": "$$\\boxed{64}$$"
        },
        {
            "introduction": "理论和简化模型是学习的起点，但真实世界的系统管理要复杂得多。这个练习将我们带入一个实际的生产场景：一个 NUMA 节点上发生了内存泄漏。我们将探讨如何运用现代 Linux 内核提供的 NUMA 控制工具（如 cgroups 和内存策略）来诊断、隔离和管理这个问题。 这个案例研究将迫使你思考系统在压力下的行为，并理解不同的策略如何导致截然不同的结果——从局部化的内存溢出（OOM）事件到因远程内存访问导致的性能下降。",
            "id": "3663644",
            "problem": "考虑一个部署在具有非统一内存访问 (NUMA) 架构的机器上的生产 Web 服务器。该系统有 $N=2$ 个节点：节点 $0$ 和节点 $1$，每个节点拥有 $128$ 吉比字节 (GiB) 的物理内存。该服务器使用 $W=16$ 个工作进程，其中 $8$ 个工作进程被绑定到节点 $0$ 的中央处理器 (CPU) 核心上，另外 $8$ 个被绑定到节点 $1$ 的 CPU 核心上。操作系统采用首次接触页面分配策略：页面会分配在分配线程首次接触该内存的 NUMA 节点上，并受制于进程的内存策略。\n\n所有工作进程都是相同的，除了一个 bug 导致仅在节点 $0$ 上的 $8$ 个工作进程中出现内存泄漏。每个节点 $0$ 的工作进程有一个 $3$ GiB 的稳态工作集，并额外每分钟泄漏 $0.5$ GiB 的匿名、不可交换的内存。假设交换功能被禁用。操作系统本身在节点 $0$ 上的内存使用量约为 $4$ GiB。工作负载主要受计算和网络限制，文件缓存可忽略不计，因此页面缓存回收不会显著减慢内存泄漏消耗可用内存的速度。\n\n进行了两个实验：\n\n- 实验 $1$：节点 $0$ 的工作进程被置于一个内存控制组 (memcg) 中，该控制组的容量限制为节点 $0$ 的容量，并配置了严格内存策略 (MPOL_BIND)，只允许在节点 $0$ 上进行分配（cpuset.mems 限制为节点 $0$）。节点 $1$ 的工作进程使用节点 $1$ 且没有泄漏，除了被绑定到节点 $1$ 的 CPU 外，没有其他特殊限制。\n\n- 实验 $2$：节点 $0$ 的工作进程配置了首选内存策略 (MPOL_PREFERRED)，首选节点 $0$，但在节点 $0$ 内存不足时允许从节点 $1$ 分配（cpuset.mems 包括节点 $0$ 和节点 $1$）。用于节点 $0$ 工作进程的 memcg 允许跨两个节点增长，直至达到两者的总容量。\n\n基于非统一内存访问 (NUMA)、首次接触分配、内存策略和内存控制组 (memcg) 的基本定义，选择所有正确的陈述。\n\nA. 在实验 $1$ 中，当节点 $0$ 耗尽可用内存时，内存不足 (OOM) 事件被限制在节点 $0$ 的内存控制组内，其工作进程的分配不会溢出到节点 $1$。\n\nB. 在实验 $2$ 中，一旦节点 $0$ 耗尽可用内存，节点 $0$ 工作进程的后续分配将从节点 $1$ 中满足，由于远程访问，它们的平均内存访问时间会增加。\n\nC. 考虑到泄漏率和节点 $0$ 上的初始可用内存，假设泄漏使用的是匿名不可交换内存且交换功能被禁用，在实验 $2$ 中，节点 $0$ 首次停止本地分配并开始回退到远程页面的时间大约是 $25$ 分钟。\n\nD. 在实验 $1$ 中，Linux 的内存不足 (OOM) 查杀器会终止两个节点上的工作进程，因为 OOM 决策总是系统级的，会忽略内存控制组和允许的节点集。\n\nE. 在实验 $2$ 中，节点 $1$ 的工作进程保证在远程溢出期间不会出现吞吐量下降，因为它们的页面保持本地；跨节点流量不会与节点 $1$ 的内存控制器或互连产生争用。",
            "solution": "在此，我将首先验证问题陈述，并在确认其有效后，继续对系统行为进行详细分析并评估每个选项。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n\n*   系统架构：非统一内存访问 (NUMA)\n*   NUMA 节点数量：$N=2$（节点 $0$ 和节点 $1$）\n*   每个节点的内存：$128$ 吉比字节 (GiB)\n*   总工作进程数：$W=16$\n*   工作进程绑定：$8$ 个工作进程绑定到节点 $0$ 的 CPU 核心；$8$ 个工作进程绑定到节点 $1$ 的 CPU 核心。\n*   操作系统分配策略：首次接触页面分配。\n*   内存泄漏：仅限于节点 $0$ 上的 $8$ 个工作进程。\n*   节点 $0$ 工作进程的内存概况：\n    *   稳态工作集：每个工作进程 $3$ GiB。\n    *   泄漏率：每个工作进程每分钟 $0.5$ GiB。\n    *   泄漏的内存类型：匿名的、不可交换的。\n*   交换：禁用。\n*   节点 $0$ 上的操作系统内存使用量：约 $4$ GiB。\n*   文件缓存：影响可忽略。\n*   实验 $1$ 中节点 $0$ 工作进程的配置：\n    *   内存控制组 (memcg) 的容量限制为节点 $0$ 的容量。\n    *   严格内存策略 (MPOL_BIND)，只允许在节点 $0$ 上分配。\n    *   `cpuset.mems` 被限制为节点 $0$。\n*   实验 $2$ 中节点 $0$ 工作进程的配置：\n    *   对节点 $0$ 采用首选内存策略 (MPOL_PREFERRED)。\n    *   在回退时允许从节点 $1$ 分配。\n    *   `cpuset.mems` 包括节点 $0$ 和节点 $1$。\n    *   memcg 允许跨两个节点增长。\n\n**第 2 步：使用提取的已知条件进行验证**\n\n*   **科学依据充分：** 该问题牢固地植根于现代计算机体系结构和操作系统的原理，特别是 NUMA、CPU/内存绑定 (`cpuset`)、内存分配策略（`mbind` 与 `MPOL_BIND`/`MPOL_PREFERRED`）和资源隔离 (`memcg`)。这些都是定义明确的现实世界机制，尤其是在 Linux 内核中。\n*   **问题定义明确：** 问题陈述清晰，有两个不同的实验设置。提供的数值数据足以分析一个与时间相关的行为（内存泄漏）。提出的问题直接关系到所描述配置的预期结果。\n*   **客观性：** 语言技术性强且精确。它描述了系统配置和行为，没有主观性或歧义。所使用的术语在操作系统上下文中具有特定的、正式的含义。\n*   **一致性与完整性：** 设置是自洽的。提供了参数（内存大小、工作进程数量、泄漏率）并允许进行定量分析。例如，指定了总内存、操作系统开销和初始工作集，从而可以计算初始可用内存。没有矛盾之处。\n*   **真实性：** 该场景非常现实。生产服务器通常在 NUMA 硬件上运行，而内存泄漏是一类常见的软件错误。使用 cgroups (`memcg`, `cpuset`) 来管理工作负载是容器化和虚拟化环境中的标准做法。所描述的策略及其效果与现实相符。\n\n**第 3 步：结论与行动**\n\n问题陈述是有效的。它科学合理、定义明确、客观且完整。我将继续进行完整解答。\n\n### 推导与选项分析\n\n首先，我将确定节点 $0$ 的初始内存状态。\n*   节点 $0$ 上的总内存：$128$ GiB。\n*   节点 $0$ 上操作系统消耗的内存：$4$ GiB。\n*   节点 $0$ 上 $8$ 个工作进程初始稳态消耗的内存：$8 \\text{ 个工作进程} \\times 3 \\text{ GiB/工作进程} = 24$ GiB。\n*   节点 $0$ 上的总初始内存使用量：$4 \\text{ GiB} + 24 \\text{ GiB} = 28$ GiB。\n*   节点 $0$ 上可供泄漏消耗的初始可用内存：$128 \\text{ GiB} - 28 \\text{ GiB} = 100$ GiB。\n\n接下来，我将确定内存泄漏的总消耗速率。\n*   节点 $0$ 上 $8$ 个工作进程的总泄漏率：$8 \\text{ 个工作进程} \\times 0.5 \\text{ GiB/分钟/工作进程} = 4$ GiB/分钟。\n\n现在，我将分析每个实验中的行为并评估相应的选项。\n\n**A. 在实验 1 中，当节点 0 耗尽可用内存时，内存不足 (OOM) 事件被限制在节点 0 的内存控制组内，其工作进程的分配不会溢出到节点 1。**\n\n*   **分析：** 在实验 $1$ 中，节点 $0$ 的工作进程受到两个约束，阻止其在节点 $1$ 上分配内存：\n    1.  `cpuset.mems` 被限制为节点 $0$。这是一个强大的基于 cgroup 的约束，禁止内核为这些进程在任何其他节点上分配页面。\n    2.  还指定了 `MPOL_BIND` 策略，该策略严格要求在给定的节点掩码（节点 $0$）上进行分配。如果指定节点上没有可用内存，分配将失败。\n    因此，当节点 $0$ 上的 $100$ GiB 可用内存耗尽时，来自节点 $0$ 工作进程的任何进一步分配请求都将失败。这种分配失败将触发内存不足 (OOM) 条件。\n    问题指出这些工作进程位于一个内存控制组 (`memcg`) 中。`memcg` 的一个主要功能是隔离内存资源。当 `memcg` 内的进程因超出 cgroup 的限制而触发 OOM 条件时，内核会专门为该 `memcg` 调用 OOM killer（即“memcg OOM”）。它不会触发系统级的 OOM。因此，OOM 事件被限制在节点 $0$ 的 `memcg` 内，而此组之外的进程（如节点 $1$ 上的工作进程）不被视为终止的候选对象。该陈述完全符合这些 Linux 内核机制的预期行为。\n*   **结论：正确**\n\n**B. 在实验 2 中，一旦节点 0 耗尽可用内存，节点 0 工作进程的后续分配将从节点 1 中满足，由于远程访问，它们的平均内存访问时间会增加。**\n\n*   **分析：** 在实验 $2$ 中，策略是对节点 $0$ 使用 `MPOL_PREFERRED`，并且 `cpuset.mems` 允许在节点 $0$ 和节点 $1$ 上进行分配。“首选”策略指示内核首先尝试在节点 $0$ 上进行分配。然而，与 `MPOL_BIND` 不同，如果首选节点内存不足，它允许内核回退到其他允许的节点（根据 `cpuset.mems`）。\n    因此，一旦节点 $0$ 已满，来自节点 $0$ 工作进程的后续分配请求将通过从节点 $1$ 分配页面来满足。由于工作进程在节点 $0$ 的 CPU 上运行，任何对这些新分配在节点 $1$ 上的页面的访问都将是“远程”访问。与本地内存访问相比，远程内存访问由于需要穿越 NUMA 互连（例如 QPI 或 UPI）而产生更高的延迟和提供更低的带宽。随着进程工作集中远程页面比例的增加，其平均内存访问时间将会恶化（增加）。\n*   **结论：正确**\n\n**C. 考虑到泄漏率和节点 0 上的初始可用内存，假设泄漏使用的是匿名不可交换内存且交换功能被禁用，在实验 2 中，节点 0 首次停止本地分配并开始回退到远程页面的时间大约是 25 分钟。**\n\n*   **分析：** 这是一个可以用给定条件验证的定量声明。\n    *   节点 $0$ 上的初始可用内存：$100$ GiB（如上计算）。\n    *   总泄漏率：$4$ GiB/分钟（如上计算）。\n    耗尽可用内存的时间等于总可用内存除以消耗速率。\n    $$ \\text{时间} = \\frac{\\text{初始可用内存}}{\\text{总泄漏速率}} = \\frac{100 \\text{ GiB}}{4 \\text{ GiB/min}} = 25 \\text{ 分钟} $$\n    在 $25$ 分钟时，节点 $0$ 将耗尽其可用内存。在实验 $2$ 中，这正是 `MPOL_PREFERRED` 策略导致内核开始回退到节点 $1$ 进行新分配的时间点。计算是正确的。\n*   **结论：正确**\n\n**D. 在实验 1 中，Linux 的内存不足 (OOM) 查杀器会终止两个节点上的工作进程，因为 OOM 决策总是系统级的，会忽略内存控制组和允许的节点集。**\n\n*   **分析：** 这个陈述从根本上是错误的。它歪曲了内存控制组 (`memcg`) 的目的和功能。`memcg` 的一个关键特性是实现按组的内存核算和限制执行。当 cgroup 的内存限制被突破时，它会触发一个特定于 `memcg` 的 OOM 事件。OOM killer 随后仅对该违规 cgroup 内的进程调用。“OOM 决策总是系统级的”这一说法是错误的；根据设计，`memcg` OOM 事件是局部化的。因此，节点 $1$ 上的工作进程（它们不在节点 $0$ 工作进程的 `memcg` 中）将完全不受 OOM 事件的影响。\n*   **结论：不正确**\n\n**E. 在实验 2 中，节点 1 的工作进程保证在远程溢出期间不会出现吞吐量下降，因为它们的页面保持本地；跨节点流量不会与节点 1 的内存控制器或互连产生争用。**\n\n*   **分析：** 这个陈述是不正确的。它对 NUMA 硬件的特性做出了错误的断言。连接节点 $0$ 和节点 $1$ 的 NUMA 互连是一个共享资源。节点 $1$ 上的内存控制器也是一个共享资源。当节点 $0$ 的 CPU 开始访问物理上位于节点 $1$ 的内存（“远程溢出”）时，这些流量必须穿越互连并由节点 $1$ 的内存控制器提供服务。这会产生争用。来自节点 $1$ 工作进程的本地内存请求必须与来自节点 $0$ 工作进程的远程内存请求竞争，以访问节点 $1$ 的内存控制器和互连上的带宽。这种争用将增加节点 $1$ 工作进程的有效内存延迟，可能导致性能和吞吐量下降。“保证”这个词使该陈述绝对错误，因为任何程度的争用都违反了这一保证。\n*   **结论：不正确**",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}