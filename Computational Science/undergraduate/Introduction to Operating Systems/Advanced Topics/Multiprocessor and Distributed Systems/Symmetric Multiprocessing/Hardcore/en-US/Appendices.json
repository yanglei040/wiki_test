{
    "hands_on_practices": [
        {
            "introduction": "In symmetric multiprocessing systems, performance is not just about parallel execution; it is fundamentally about managing data access. A subtle but severe performance pitfall known as \"false sharing\" can occur when independent variables, modified by different cores, happen to reside on the same cache line, causing unnecessary and expensive coherence traffic. This exercise provides a quantitative model to understand and predict the dramatic impact of false sharing, allowing you to see firsthand why data layout is a critical concern in high-performance concurrent programming .",
            "id": "3685550",
            "problem": "You are studying symmetric multiprocessing (SMP) systems where multiple processor cores share a single coherent memory space. In such systems, the cache coherence protocol (for example, Modified, Exclusive, Shared, Invalid (MESI)) maintains a consistent view of memory by invalidating or updating cache lines across cores when writes occur. When multiple threads frequently write to independent variables that occupy the same cache line, a phenomenon known as false sharing occurs, producing unnecessary coherence traffic and increasing write latency.\n\nConsider an array of $N$ per-thread counters, where thread $i$ repeatedly increments its own counter. In the unpadded layout, the counters are placed contiguously in memory, each counter occupying $s$ bytes. The memory system uses a cache line size of $L$ bytes, so that up to $\\left\\lfloor \\frac{L}{s} \\right\\rfloor$ distinct counters may occupy the same cache line. In the padded layout, each counter is placed in its own dedicated $64$-byte region, ensuring no two counters share a cache line.\n\nAssume the following performance model for the time cost of increments on a symmetric multiprocessing system:\n- Each increment to a counter on a cache line that is exclusively owned by the executing core takes a base time $p_{\\text{base}}$ (in nanoseconds).\n- Due to false sharing, if $k$ threads are concurrently incrementing counters located on the same cache line, then each increment induces, on average, an additional coherence penalty $p_{\\text{remote}}$ (in nanoseconds) per other participating thread on that line for reacquiring exclusive ownership. That is, the average per-increment penalty scales proportionally with $(k-1)$.\n\nLet each thread perform $M$ increments on its own counter. Let $F = \\left\\lfloor \\frac{L}{s} \\right\\rfloor$ be the maximum number of counters per cache line in the unpadded layout. Threads are assigned counters consecutively, so the largest sharing group size in the unpadded layout is $k_{\\max} = \\min(N, F)$. In the padded layout with $64$-byte spacing, we assume that each counter maps to a distinct cache line, so $k=1$.\n\nUnder these assumptions:\n- Define $T_{\\text{unpadded}}$ as the time for one thread to complete $M$ increments in the unpadded layout, dominated by the slowest sharing group.\n- Define $T_{\\text{padded}}$ as the time for one thread to complete $M$ increments in the padded $64$-byte layout.\n- The speedup ratio is $R = \\frac{T_{\\text{unpadded}}}{T_{\\text{padded}}}$.\n\nYour task is to derive $R$ from the definitions above using first principles of cache coherence behavior and implement a program that computes $R$ for a provided test suite. All times are to be handled in nanoseconds. Angles are not involved. Percentages should not be used; express any proportions as decimals.\n\nTest Suite:\nFor each test case, the parameters are $(N, s, L, M, p_{\\text{base}}, p_{\\text{remote}})$, with $N$, $s$, $L$, and $M$ in integers and $p_{\\text{base}}$, $p_{\\text{remote}}$ in floating-point nanoseconds. Use the following test cases:\n1. $(N=8, s=8, L=64, M=1000000, p_{\\text{base}}=5.0, p_{\\text{remote}}=30.0)$\n2. $(N=1, s=8, L=64, M=1000000, p_{\\text{base}}=5.0, p_{\\text{remote}}=30.0)$\n3. $(N=4, s=16, L=64, M=500000, p_{\\text{base}}=5.0, p_{\\text{remote}}=30.0)$\n4. $(N=9, s=8, L=64, M=1000000, p_{\\text{base}}=5.0, p_{\\text{remote}}=30.0)$\n5. $(N=8, s=64, L=64, M=1000000, p_{\\text{base}}=5.0, p_{\\text{remote}}=30.0)$\n6. $(N=8, s=8, L=64, M=2000000, p_{\\text{base}}=10.0, p_{\\text{remote}}=10.0)$\n7. $(N=10, s=12, L=64, M=750000, p_{\\text{base}}=8.0, p_{\\text{remote}}=24.0)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each $R$ should be printed as a floating-point value in decimal form.\n\nThe program must be entirely self-contained, require no input, and compute $R$ for each of the test cases listed. No extraneous text should be printed.",
            "solution": "The problem requires the derivation of a speedup ratio, $R$, which quantifies the performance improvement of using a padded memory layout over a contiguous (unpadded) layout to mitigate false sharing in a symmetric multiprocessing (SMP) system. The solution is derived from the provided performance model.\n\nFirst, we formalize the time cost of a single counter increment operation. The model states that an increment takes a base time $p_{\\text{base}}$ plus a penalty for cache coherence traffic if other threads are accessing the same cache line. For a group of $k$ threads concurrently incrementing counters on the same cache line, each increment incurs a penalty of $p_{\\text{remote}}$ for each of the $(k-1)$ other contending threads. Thus, the average time for a single increment, $T_{\\text{inc}}(k)$, as a function of the number of sharing threads $k$, is:\n$$T_{\\text{inc}}(k) = p_{\\text{base}} + (k-1) p_{\\text{remote}}$$\nThis equation assumes $k \\ge 1$. If $k=1$, there are no other contending threads, and the time correctly reduces to $T_{\\text{inc}}(1) = p_{\\text{base}}$.\n\nNext, we determine the total time for a thread to complete its work in both the padded and unpadded scenarios. Each thread performs $M$ increments.\n\nIn the padded layout, each counter is allocated in its own $64$-byte memory region. Assuming the cache line size $L$ is less than or equal to $64$ bytes (a standard assumption, and true for all test cases where $L=64$), this padding ensures that no two counters reside on the same cache line. Consequently, when a thread increments its counter, it is the sole user of that cache line. The number of contending threads is always $k=1$. The time for a single increment is therefore $T_{\\text{inc}}(1) = p_{\\text{base}}$. The total time, $T_{\\text{padded}}$, for a thread to perform $M$ increments is:\n$$T_{\\text{padded}} = M \\cdot T_{\\text{inc}}(1) = M \\cdot p_{\\text{base}}$$\n\nIn the unpadded layout, the counters are placed contiguously in memory. Each counter has a size of $s$ bytes. Given a cache line size of $L$ bytes, the maximum number of counters that can occupy a single cache line is $F = \\left\\lfloor \\frac{L}{s} \\right\\rfloor$.\nThe problem states that the threads are assigned to counters consecutively, and the performance is dominated by the \"slowest sharing group\". This corresponds to the group of threads contending for the most populated cache line. The size of this largest sharing group, $k_{\\text{max}}$, is the minimum of the total number of threads, $N$, and the maximum number of counters per cache line, $F$.\n$$k_{\\text{max}} = \\min(N, F) = \\min\\left(N, \\left\\lfloor \\frac{L}{s} \\right\\rfloor\\right)$$\nFor any thread in this slowest group, the time for a single increment is $T_{\\text{inc}}(k_{\\text{max}})$. The total time, $T_{\\text{unpadded}}$, for such a thread to complete $M$ increments is:\n$$T_{\\text{unpadded}} = M \\cdot T_{\\text{inc}}(k_{\\text{max}}) = M \\cdot (p_{\\text{base}} + (k_{\\text{max}} - 1) p_{\\text{remote}})$$\nNote that if $k_{\\text{max}} = 1$ (which occurs if $N=1$ or if $F=1$), there is no false sharing, and $T_{\\text{unpadded}}$ correctly simplifies to $M \\cdot p_{\\text{base}}$.\n\nFinally, we derive the speedup ratio $R$, defined as $R = \\frac{T_{\\text{unpadded}}}{T_{\\text{padded}}}$. Substituting the expressions for $T_{\\text{unpadded}}$ and $T_{\\text{padded}}$:\n$$R = \\frac{M \\cdot (p_{\\text{base}} + (k_{\\text{max}} - 1) p_{\\text{remote}})}{M \\cdot p_{\\text{base}}}$$\nThe factor $M$, representing the number of increments, cancels out, showing that the speedup ratio is independent of the total workload and depends only on the system parameters and contention level:\n$$R = \\frac{p_{\\text{base}} + (k_{\\text{max}} - 1) p_{\\text{remote}}}{p_{\\text{base}}}$$\nSubstituting the expression for $k_{\\text{max}}$ yields the final formula for calculation:\n$$R = \\frac{p_{\\text{base}} + \\left(\\min\\left(N, \\left\\lfloor \\frac{L}{s} \\right\\rfloor\\right) - 1\\right) p_{\\text{remote}}}{p_{\\text{base}}}$$\nThis can also be expressed as:\n$$R = 1 + \\frac{\\left(\\min\\left(N, \\left\\lfloor \\frac{L}{s} \\right\\rfloor\\right) - 1\\right) p_{\\text{remote}}}{p_{\\text{base}}}$$\nThis final expression will be implemented to compute the ratio for each test case provided. For all test cases, $N \\ge 1$ and $L \\ge s$, which ensures $k_{\\text{max}} \\ge 1$, so the term $(k_{\\text{max}}-1)$ is non-negative.",
            "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n// #include <complex.h>\n// #include <threads.h>\n// #include <stdatomic.h>\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    int N;             // Number of threads/counters\n    int s;             // Size of each counter in bytes\n    int L;             // Cache line size in bytes\n    int M;             // Number of increments per thread\n    double p_base;     // Base time for an increment in nanoseconds\n    double p_remote;   // Remote coherence penalty in nanoseconds\n} TestCase;\n\n// Helper function for integer minimum\nint min_int(int a, int b) {\n    return (a < b) ? a : b;\n}\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        {8, 8, 64, 1000000, 5.0, 30.0},\n        {1, 8, 64, 1000000, 5.0, 30.0},\n        {4, 16, 64, 500000, 5.0, 30.0},\n        {9, 8, 64, 1000000, 5.0, 30.0},\n        {8, 64, 64, 1000000, 5.0, 30.0},\n        {8, 8, 64, 2000000, 10.0, 10.0},\n        {10, 12, 64, 750000, 8.0, 24.0}\n    };\n\n    // Calculate the number of test cases.\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    double results[num_cases];\n\n    // Calculate the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        TestCase tc = test_cases[i];\n\n        // F is the maximum number of counters per cache line.\n        // Since L and s are positive integers, integer division truncates, which\n        // is equivalent to floor() for positive numbers.\n        int F = tc.L / tc.s;\n\n        // k_max is the size of the largest sharing group.\n        int k_max = min_int(tc.N, F);\n\n        // Calculate the speedup ratio R.\n        // The term (k_max - 1) is guaranteed to be non-negative because N >= 1 and L >= s\n        // for all test cases, which implies F >= 1, and thus k_max >= 1.\n        double R = (tc.p_base + (double)(k_max - 1) * tc.p_remote) / tc.p_base;\n        \n        results[i] = R;\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement.\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"%f\", results[i]);\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "After seeing the high cost of cache contention, a logical next step is to explore programming patterns that avoid it. Thread-local storage (TLS) offers a powerful solution by giving each thread a private copy of a variable, eliminating sharing and coherence overhead entirely. This practice guides you through a quantitative analysis of the trade-offs, comparing the expected cost of shared memory access against the amortized cost of TLS, which includes a one-time initialization overhead . This modeling exercise will sharpen your ability to make data-driven design choices in parallel software.",
            "id": "3685589",
            "problem": "You are given a scenario in symmetric multiprocessing (SMP) where multiple threads run on multiple processors and access either thread-local storage (TLS) or a shared global variable. The objective is to analyze and quantify the performance trade-offs between TLS and shared globals under cache coherence, using a principled model grounded in memory hierarchy and probability.\n\nFoundational base to use:\n- In symmetric multiprocessing (SMP), all processors have equal access to memory and maintain coherence via a cache-coherence protocol. A write to a shared cache line can invalidate copies in other caches, incurring extra latency. This added latency will be modeled as a penalty cost per invalidation event.\n- Thread-local storage (TLS) maps each threadâ€™s variable to a distinct memory location, eliminating inter-thread coherence for that variable. TLS may have a one-time initialization cost per thread for setting up the storage.\n- Expected values in probability theory: For a Bernoulli random variable with success probability $p$, the expected penalty contribution per trial is $p$ times the cost of the penalty. Amortization across $N$ identical operations yields a per-operation overhead equal to the total overhead divided by $N$.\n\nDefinitions and assumptions:\n- Let $P$ denote the number of processors. All processors are identical and run threads concurrently.\n- Each thread performs $N$ accesses to one variable, either TLS or shared global.\n- TLS has a one-time initialization cost $c_{\\text{init}}$ (in cycles) per thread and a steady per-access cost $c_{\\text{tls}}$ (in cycles).\n- Shared global access has a steady per-access base cost $c_{\\text{shrd}}$ (in cycles). If an access is a write, and the most recent valid copy of the cache line is held by another processor (remote), the write triggers invalidation and coherence traffic with additional per-access penalty $c_{\\text{coh}}$ (in cycles).\n- Let $f_{\\text{write}} \\in [0,1]$ be the fraction of accesses that are writes. Let $p_{\\text{remote}} \\in [0,1]$ be the probability that a write hits a cache line owned by a remote processor. For shared global:\n  - The per-access invalidation probability is $p_{\\text{inv}} = f_{\\text{write}} \\cdot p_{\\text{remote}}$.\n  - The expected per-access shared cost is $E_{\\text{shared}} = c_{\\text{shrd}} + p_{\\text{inv}} \\cdot c_{\\text{coh}}$.\n- TLS avoids coherence invalidations for the variable by design. With amortization of initialization:\n  - The expected per-access TLS cost is $E_{\\text{tls}} = c_{\\text{tls}} + \\dfrac{c_{\\text{init}}}{N}$.\n\nYour program must compute, for each test case:\n- The TLS expected per-access cost $E_{\\text{tls}}$ (in cycles).\n- The shared expected per-access cost $E_{\\text{shared}}$ (in cycles).\n- The per-access speedup $S = \\dfrac{E_{\\text{shared}}}{E_{\\text{tls}}}$ (unitless).\n- The coherence traffic avoided per access due to TLS, quantified as $p_{\\text{inv}}$ (unitless fraction).\n\nAll outputs that represent cycle counts must be expressed in cycles. The final speedup and coherence probability must be expressed as decimal fractions.\n\nTest suite:\n- Case $1$: $P = 8$, $N = 1000000$, $f_{\\text{write}} = 0.1$, $p_{\\text{remote}} = 0.7$, $c_{\\text{init}} = 2000$, $c_{\\text{tls}} = 4$, $c_{\\text{shrd}} = 6$, $c_{\\text{coh}} = 60$.\n- Case $2$: $P = 1$, $N = 100000$, $f_{\\text{write}} = 0.5$, $p_{\\text{remote}} = 0$, $c_{\\text{init}} = 1000$, $c_{\\text{tls}} = 4$, $c_{\\text{shrd}} = 6$, $c_{\\text{coh}} = 60$.\n- Case $3$: $P = 32$, $N = 10000$, $f_{\\text{write}} = 1$, $p_{\\text{remote}} = 1$, $c_{\\text{init}} = 2000$, $c_{\\text{tls}} = 4$, $c_{\\text{shrd}} = 6$, $c_{\\text{coh}} = 200$.\n- Case $4$: $P = 4$, $N = 1$, $f_{\\text{write}} = 0.2$, $p_{\\text{remote}} = 0.5$, $c_{\\text{init}} = 5000$, $c_{\\text{tls}} = 4$, $c_{\\text{shrd}} = 6$, $c_{\\text{coh}} = 60$.\n- Case $5$: $P = 16$, $N = 1000000$, $f_{\\text{write}} = 0.01$, $p_{\\text{remote}} = 0.9$, $c_{\\text{init}} = 2000$, $c_{\\text{tls}} = 4$, $c_{\\text{shrd}} = 6$, $c_{\\text{coh}} = 60$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-case result lists, each in the form $[E_{\\text{tls}},E_{\\text{shared}},S,p_{\\text{inv}}]$ with each numeric value formatted to exactly six digits after the decimal point, and the overall output enclosed in square brackets. For example: $[[\\dots],[\\dots],\\dots]$.",
            "solution": "The analysis hinges on calculating the expected per-access cost for two different memory access strategies: using a shared global variable versus using thread-local storage.\n\nFirst, we analyze the cost of accessing a shared global variable. The base cost for any access (read or write) is given as $c_{\\text{shrd}}$. An additional cost, $c_{\\text{coh}}$, is incurred only under specific circumstances: the access must be a write, and that write must require invalidating a copy of the corresponding cache line held by a remote processor. The problem provides the fraction of accesses that are writes, $f_{\\text{write}}$, and the probability that a write invalidates a remote cache line, $p_{\\text{remote}}$. Since these two conditions are independent events for any given access, the probability of incurring the coherence penalty is the product of their individual probabilities. This gives the per-access invalidation probability, $p_{\\text{inv}}$:\n$$p_{\\text{inv}} = f_{\\text{write}} \\cdot p_{\\text{remote}}$$\nThis is a Bernoulli trial where the \"success\" is a coherence event with probability $p_{\\text{inv}}$ and cost $c_{\\text{coh}}$. The expected value of the penalty per access is the probability of the event multiplied by its cost. Therefore, the total expected per-access cost for a shared variable, $E_{\\text{shared}}$, is the sum of the base cost and the expected penalty cost:\n$$E_{\\text{shared}} = c_{\\text{shrd}} + p_{\\text{inv}} \\cdot c_{\\text{coh}}$$\n\nNext, we analyze the cost of using thread-local storage. By design, TLS assigns a unique memory location to each thread's variable, thereby avoiding inter-thread data sharing and the associated cache coherence traffic. Thus, there is no coherence penalty, $c_{\\text{coh}}$. Accessing a TLS variable has a steady per-access cost of $c_{\\text{tls}}$. However, there is a one-time setup cost, $c_{\\text{init}}$, for each thread to initialize its local storage. To compare this fairly with the per-access shared cost, we amortize this one-time cost over the total number of accesses, $N$, performed by the thread. The amortized initialization cost per access is $\\frac{c_{\\text{init}}}{N}$. The total expected per-access cost for TLS, $E_{\\text{tls}}$, is therefore:\n$$E_{\\text{tls}} = c_{\\text{tls}} + \\frac{c_{\\text{init}}}{N}$$\n\nFinally, to compare the two strategies, we calculate the speedup, $S$, which is the ratio of the shared variable cost to the TLS cost. A speedup $S > 1$ indicates that TLS is more performant.\n$$S = \\frac{E_{\\text{shared}}}{E_{\\text{tls}}}$$\n\nThe quantity $p_{\\text{inv}}$ itself is a useful metric, representing the fraction of memory accesses that would cause coherence traffic if a shared variable were used. This quantifies the amount of coherence activity that TLS avoids by its design. These formulas can be directly implemented to solve for the required values in each test case.",
            "answer": "```c\n// This program models and compares the performance of thread-local storage (TLS)\n// versus shared global variables in a symmetric multiprocessing (SMP) environment.\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\n// A struct to hold the parameters for a single test case.\n// P is included for completeness, though not used in the provided formulas.\ntypedef struct {\n    int P;\n    int N;\n    double f_write;\n    double p_remote;\n    int c_init;\n    int c_tls;\n    int c_shrd;\n    int c_coh;\n} TestCase;\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        {8, 1000000, 0.1, 0.7, 2000, 4, 6, 60},\n        {1, 100000, 0.5, 0.0, 1000, 4, 6, 60},\n        {32, 10000, 1.0, 1.0, 2000, 4, 6, 200},\n        {4, 1, 0.2, 0.5, 5000, 4, 6, 60},\n        {16, 1000000, 0.01, 0.9, 2000, 4, 6, 60}\n    };\n\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n\n    printf(\"[\");\n    // Calculate and print the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        TestCase tc = test_cases[i];\n\n        // Calculate the per-access invalidation probability.\n        double p_inv = tc.f_write * tc.p_remote;\n\n        // Calculate the expected per-access TLS cost.\n        // The one-time initialization cost is amortized over N accesses.\n        double E_tls = (double)tc.c_tls + (double)tc.c_init / (double)tc.N;\n\n        // Calculate the expected per-access shared variable cost.\n        // This includes the base cost and the expected coherence penalty.\n        double E_shared = (double)tc.c_shrd + p_inv * (double)tc.c_coh;\n\n        // Calculate the speedup of TLS over shared variables.\n        double S = E_shared / E_tls;\n        \n        // Print the results for the current case in the required format.\n        printf(\"[%.6f,%.6f,%.6f,%.6f]\", E_tls, E_shared, S, p_inv);\n        \n        // Add a comma separator if this is not the last test case.\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "Beyond memory access patterns, the correct and efficient use of synchronization primitives like locks is paramount in SMP. While locks ensure mutual exclusion, they can introduce complex system-level performance issues, most notably priority inversion, where a high-priority thread is forced to wait for a lower-priority thread. This exercise challenges you to act as a systems analyst, diagnosing and quantifying priority inversion by parsing an execution trace of lock events . Mastering this skill is essential for debugging and optimizing the performance of sophisticated multithreaded applications.",
            "id": "3685586",
            "problem": "You are given a symmetric multiprocessing (SMP) scenario in which a single global mutual-exclusion lock protects a critical section. Multiple threads execute on $N$ identical cores, repeatedly requesting to enter the critical section. Each thread emits a trace of timestamped events of three kinds: a lock request event, a lock acquisition event, and a lock release event. All cores share a single memory and the lock provides mutual exclusion but no guarantees of fairness. The timeline is observable as a set of discrete events with timestamps.\n\nFundamental base and definitions:\n- Symmetric multiprocessing (SMP) is a system model in which multiple identical processor cores share the same memory and run simultaneously. The scheduling of threads onto cores is independent of the lock semantics.\n- A critical section is a segment of code that must not be executed by more than one thread at the same time. Mutual exclusion ensures at most one thread holds the lock at any time.\n- A trace event is represented as a tuple $(t, c, i, p, \\tau)$ where $t$ is the timestamp (nonnegative integer time in microseconds), $c$ is the core identifier, $i$ is the thread identifier, $p$ is the thread priority (integer, larger values denote higher priority), and $\\tau \\in \\{\\text{REQ}, \\text{ACQ}, \\text{REL}\\}$ indicates the event type: a lock request (REQ), a lock acquisition (ACQ), or a lock release (REL).\n- Priority inversion is a condition where a high-priority thread is indirectly blocked by a lower-priority thread holding a resource (here, the lock). Formally, at time $t$, if the lock is held by thread $i$ with priority $p_i$, and there exists at least one waiting thread $j$ with priority $p_j$ such that $p_j > p_i$, then priority inversion holds at time $t$. The total duration of priority inversion over a trace is the sum over all intervals during which this condition is true.\n\nThe task is to infer the lock acquisition order and diagnose priority inversion from the observed timeline without assuming any lock fairness beyond mutual exclusion. You must:\n1. Sort the events by timestamp $t$ ascending. For events with equal timestamps, resolve ties with the following deterministic order: process all request events (REQ) first, then all release events (REL), and finally all acquisition events (ACQ). Within the same type and timestamp, break ties deterministically (e.g., by thread identifier) to ensure a total order, but this tie-break does not affect the correctness of the required outputs under mutual exclusion.\n2. Construct the sequence of thread identifiers in the exact order in which the lock is actually acquired by reading the events after tie resolution. This is the inferred lock acquisition order.\n3. Detect priority inversion and compute its total duration. Consider the system state piecewise-constant between successive event timestamps. Over each interval $[t_k, t_{k+1})$, let $h$ be the current lock holder (if any) with priority $p_h$, and let $W$ be the set of currently waiting threads with priorities $\\{p_w : w \\in W\\}$. If $h$ exists and $\\max_{w \\in W} p_w > p_h$, then add $(t_{k+1}-t_k)$ microseconds to the inversion total. If the condition does not hold, add $0$ for that interval. The inversion-detected flag is $1$ if the total is strictly positive and $0$ otherwise. Report the total inversion duration as an integer number of microseconds.\n\nAngle units are not applicable. All timestamps and durations must be treated as integer counts of microseconds.\n\nTest suite:\nImplement your program to process the following four test cases. Each test case provides $N$, the number of cores, and a list of events $(t, c, i, p, \\tau)$ satisfying mutual exclusion when read in the specified tie order. The program must embed these exact cases and compute the outputs.\n\n- Test case $1$ (happy path, no priority inversion):\n  - $N = 3$\n  - Events:\n    - $(10, 0, 1, 1, \\text{REQ})$, $(10, 0, 1, 1, \\text{ACQ})$, $(50, 0, 1, 1, \\text{REL})$\n    - $(60, 1, 2, 3, \\text{REQ})$, $(60, 1, 2, 3, \\text{ACQ})$, $(90, 1, 2, 3, \\text{REL})$\n    - $(95, 2, 3, 2, \\text{REQ})$, $(95, 2, 3, 2, \\text{ACQ})$, $(120, 2, 3, 2, \\text{REL})$\n\n- Test case $2$ (priority inversion present; overlapping waits):\n  - $N = 3$\n  - Events:\n    - $(10, 0, 1, 1, \\text{REQ})$, $(10, 0, 1, 1, \\text{ACQ})$, $(80, 0, 1, 1, \\text{REL})$\n    - $(20, 2, 2, 3, \\text{REQ})$, $(100, 2, 2, 3, \\text{ACQ})$, $(140, 2, 2, 3, \\text{REL})$\n    - $(85, 1, 3, 2, \\text{REQ})$, $(85, 1, 3, 2, \\text{ACQ})$, $(95, 1, 3, 2, \\text{REL})$\n\n- Test case $3$ (simultaneous request and release at the same timestamp; tie handling):\n  - $N = 3$\n  - Events:\n    - $(50, 0, 1, 2, \\text{REQ})$, $(50, 0, 1, 2, \\text{ACQ})$, $(100, 0, 1, 2, \\text{REL})$\n    - $(100, 1, 2, 5, \\text{REQ})$, $(100, 1, 2, 5, \\text{ACQ})$, $(125, 1, 2, 5, \\text{REL})$\n    - $(100, 2, 3, 4, \\text{REQ})$, $(130, 2, 3, 4, \\text{ACQ})$, $(160, 2, 3, 4, \\text{REL})$\n\n- Test case $4$ (repeated acquisitions by the same thread; single inversion interval):\n  - $N = 2$\n  - Events:\n    - $(10, 0, 1, 2, \\text{REQ})$, $(10, 0, 1, 2, \\text{ACQ})$, $(40, 0, 1, 2, \\text{REL})$\n    - $(20, 1, 2, 3, \\text{REQ})$, $(40, 1, 2, 3, \\text{ACQ})$, $(70, 1, 2, 3, \\text{REL})$\n    - $(55, 0, 1, 2, \\text{REQ})$, $(70, 0, 1, 2, \\text{ACQ})$, $(90, 0, 1, 2, \\text{REL})$\n\nRequired outputs:\n- For each test case, compute:\n  1. The lock acquisition order as a list of thread identifiers (e.g., $[i_1, i_2, \\dots]$).\n  2. The inversion-detected flag ($0$ or $1$).\n  3. The total duration of priority inversion in microseconds as an integer (e.g., $70$).\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of the form $[\\text{acq\\_order\\_list}, \\text{inversion\\_detected}, \\text{total\\_inversion\\_microseconds}]$. For example, the overall output should look like $[[[i_1,i_2,\\dots],b,d],[[\\dots],b,d],[[\\dots],b,d],[[\\dots],b,d]]$ with no spaces anywhere in the line.",
            "solution": "The core of this problem is to perform a discrete-event simulation by processing a trace of lock-related events. The goal is to track the state of the system over time to determine the lock acquisition order and the total duration of priority inversion.\n\nThe algorithm proceeds as follows:\n\n1.  **Event Sorting**: The first step is to establish a total chronological order of events. The provided trace events must be sorted primarily by their timestamp in ascending order. To handle events that occur at the exact same time, a specific tie-breaking rule must be applied: Request (REQ) events are processed first, followed by Release (REL) events, and finally Acquisition (ACQ) events. This ensures that if a lock is released and requested at the same time, the request is registered before any new acquisition, which correctly models the system state.\n\n2.  **State Tracking and Interval Processing**: After sorting, we iterate through the events one by one, tracking the system's state. The key state variables are:\n    *   The identity and priority of the current lock-holding thread.\n    *   A list of threads that are currently waiting for the lock, along with their priorities.\n    *   The maximum priority among all waiting threads.\n\n    The timeline is analyzed as a series of intervals, where each interval is the time between two consecutive events. For an interval from time $t_k$ to $t_{k+1}$, the system state is constant. Before processing the event at $t_{k+1}$, we analyze this interval:\n\n3.  **Diagnosing Priority Inversion**: Within the interval $[t_k, t_{k+1})$, we check if the priority inversion condition is met. According to the definition, this occurs if:\n    *   A thread is currently holding the lock.\n    *   The maximum priority of any thread in the waiting list is strictly greater than the priority of the lock-holding thread.\n\n    If this condition is true, the duration of the interval, $(t_{k+1} - t_k)$, is added to a running total for the total priority inversion time.\n\n4.  **State Updates**: After analyzing the interval, the event at $t_{k+1}$ is used to update the system state:\n    *   **REQ event**: A thread is added to the waiting list, and the maximum waiting priority is updated if necessary.\n    *   **ACQ event**: The thread becomes the new lock holder. It is removed from the waiting list, and the maximum waiting priority is re-calculated from the remaining waiters. The thread's ID is appended to the lock acquisition order list.\n    *   **REL event**: The lock becomes free (no holder).\n\nBy repeating this process for all events, we can construct the full lock acquisition sequence and calculate the total priority inversion duration precisely. The final `inversion-detected` flag is simply a check on whether this total duration is greater than zero.",
            "answer": "```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\n// Define event types based on tie-breaking order\ntypedef enum {\n    EV_REQ, // 0\n    EV_REL, // 1\n    EV_ACQ  // 2\n} EventType;\n\n// A struct to represent a single trace event\ntypedef struct {\n    int t; // timestamp\n    int c; // core id\n    int i; // thread id\n    int p; // priority\n    EventType type;\n} Event;\n\n// A struct to hold the parameters for a single test case\ntypedef struct {\n    int N_cores;\n    Event* events;\n    int num_events;\n} TestCase;\n\n// A struct to represent a waiting thread\ntypedef struct {\n    int id;\n    int priority;\n} WaitingThread;\n\n// A struct to hold the final results for a test case\ntypedef struct {\n    int acquisition_order[20];\n    int acq_order_len;\n    int inversion_detected;\n    int total_inversion_duration;\n} Result;\n\n// Comparison function for qsort\nint compare_events(const void* a, const void* b) {\n    const Event* event_a = (const Event*)a;\n    const Event* event_b = (const Event*)b;\n\n    // Primary sort key: timestamp t\n    if (event_a->t < event_b->t) return -1;\n    if (event_a->t > event_b->t) return 1;\n\n    // Secondary sort key: event type (REQ < REL < ACQ)\n    if (event_a->type < event_b->type) return -1;\n    if (event_a->type > event_b->type) return 1;\n\n    // Tertiary sort key: thread id i (for deterministic order)\n    if (event_a->i < event_b->i) return-1;\n    if (event_a->i > event_b->i) return 1;\n    \n    return 0;\n}\n\n// Function to find and update the max priority in the waiting list\nint find_max_waiting_priority(const WaitingThread* waiting_list, int count) {\n    if (count == 0) {\n        return -1;\n    }\n    int max_p = -1;\n    for (int i = 0; i < count; ++i) {\n        if (waiting_list[i].priority > max_p) {\n            max_p = waiting_list[i].priority;\n        }\n    }\n    return max_p;\n}\n\n// Function to remove a thread from the waiting list\nvoid remove_from_waiting(WaitingThread* waiting_list, int* count, int thread_id) {\n    int found_idx = -1;\n    for (int i = 0; i < *count; ++i) {\n        if (waiting_list[i].id == thread_id) {\n            found_idx = i;\n            break;\n        }\n    }\n    if (found_idx != -1) {\n        for (int i = found_idx; i < (*count) - 1; ++i) {\n            waiting_list[i] = waiting_list[i + 1];\n        }\n        (*count)--;\n    }\n}\n\n// Main processing logic for a single test case\nResult process_trace(TestCase tc) {\n    // Sort events\n    qsort(tc.events, tc.num_events, sizeof(Event), compare_events);\n\n    Result res = { .acq_order_len = 0, .inversion_detected = 0, .total_inversion_duration = 0 };\n    if (tc.num_events == 0) {\n        return res;\n    }\n\n    // State variables\n    int lock_holder_thread = -1;\n    int lock_holder_priority = -1;\n    WaitingThread waiting_list[20]; // Max 20 waiting threads\n    int waiting_count = 0;\n    int max_waiting_priority = -1;\n    int last_event_time = tc.events[0].t;\n\n    for (int i = 0; i < tc.num_events; ++i) {\n        Event current_event = tc.events[i];\n        int current_time = current_event.t;\n\n        // Calculate inversion for the interval [last_event_time, current_time)\n        int delta_t = current_time - last_event_time;\n        if (delta_t > 0) {\n            if (lock_holder_thread != -1 && max_waiting_priority > lock_holder_priority) {\n                res.total_inversion_duration += delta_t;\n            }\n        }\n\n        // Update state based on the current event\n        switch (current_event.type) {\n            case EV_REQ:\n                waiting_list[waiting_count++] = (WaitingThread){current_event.i, current_event.p};\n                if (current_event.p > max_waiting_priority) {\n                    max_waiting_priority = current_event.p;\n                }\n                break;\n            case EV_REL:\n                lock_holder_thread = -1;\n                lock_holder_priority = -1;\n                break;\n            case EV_ACQ:\n                lock_holder_thread = current_event.i;\n                lock_holder_priority = current_event.p;\n                res.acquisition_order[res.acq_order_len++] = current_event.i;\n                \n                remove_from_waiting(waiting_list, &waiting_count, current_event.i);\n                max_waiting_priority = find_max_waiting_priority(waiting_list, waiting_count);\n                break;\n        }\n        \n        last_event_time = current_time;\n    }\n\n    if (res.total_inversion_duration > 0) {\n        res.inversion_detected = 1;\n    }\n\n    return res;\n}\n\nint main(void) {\n    Event events1[] = {\n        {10, 0, 1, 1, EV_REQ}, {10, 0, 1, 1, EV_ACQ}, {50, 0, 1, 1, EV_REL},\n        {60, 1, 2, 3, EV_REQ}, {60, 1, 2, 3, EV_ACQ}, {90, 1, 2, 3, EV_REL},\n        {95, 2, 3, 2, EV_REQ}, {95, 2, 3, 2, EV_ACQ}, {120, 2, 3, 2, EV_REL}\n    };\n    Event events2[] = {\n        {10, 0, 1, 1, EV_REQ}, {10, 0, 1, 1, EV_ACQ}, {80, 0, 1, 1, EV_REL},\n        {20, 2, 2, 3, EV_REQ}, {100, 2, 2, 3, EV_ACQ}, {140, 2, 2, 3, EV_REL},\n        {85, 1, 3, 2, EV_REQ}, {85, 1, 3, 2, EV_ACQ}, {95, 1, 3, 2, EV_REL}\n    };\n    Event events3[] = {\n        {50, 0, 1, 2, EV_REQ}, {50, 0, 1, 2, EV_ACQ}, {100, 0, 1, 2, EV_REL},\n        {100, 1, 2, 5, EV_REQ}, {100, 1, 2, 5, EV_ACQ}, {125, 1, 2, 5, EV_REL},\n        {100, 2, 3, 4, EV_REQ}, {130, 2, 3, 4, EV_ACQ}, {160, 2, 3, 4, EV_REL}\n    };\n    Event events4[] = {\n        {10, 0, 1, 2, EV_REQ}, {10, 0, 1, 2, EV_ACQ}, {40, 0, 1, 2, EV_REL},\n        {20, 1, 2, 3, EV_REQ}, {40, 1, 2, 3, EV_ACQ}, {70, 1, 2, 3, EV_REL},\n        {55, 0, 1, 2, EV_REQ}, {70, 0, 1, 2, EV_ACQ}, {90, 0, 1, 2, EV_REL}\n    };\n\n    TestCase test_cases[] = {\n        {3, events1, sizeof(events1) / sizeof(events1[0])},\n        {3, events2, sizeof(events2) / sizeof(events2[0])},\n        {3, events3, sizeof(events3) / sizeof(events3[0])},\n        {2, events4, sizeof(events4) / sizeof(events4[0])}\n    };\n\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    Result results[num_cases];\n\n    for (int i = 0; i < num_cases; ++i) {\n        results[i] = process_trace(test_cases[i]);\n    }\n\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        if (i > 0) {\n            printf(\",\");\n        }\n        printf(\"[[\");\n        for (int j = 0; j < results[i].acq_order_len; ++j) {\n            if (j > 0) {\n                printf(\",\");\n            }\n            printf(\"%d\", results[i].acquisition_order[j]);\n        }\n        printf(\"],%d,%d]\", results[i].inversion_detected, results[i].total_inversion_duration);\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        }
    ]
}