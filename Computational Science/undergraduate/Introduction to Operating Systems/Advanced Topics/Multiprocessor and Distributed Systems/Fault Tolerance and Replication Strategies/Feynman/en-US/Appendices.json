{
    "hands_on_practices": [
        {
            "introduction": "Understanding fault tolerance begins with quantifying the core trade-offs between different replication strategies. This first exercise challenges you to build a simple probabilistic model to compare the durability of synchronous versus asynchronous replication. By deriving the expected data loss for each strategy, you will gain a concrete understanding of how replication factor $n$ impacts system safety in different designs .",
            "id": "3641421",
            "problem": "A distributed storage service in an operating system replicates each client write across $n$ identical storage nodes. Consider two replication strategies for each client write: synchronous replication to all $n$ nodes versus asynchronous replication that acknowledges at a single primary and propagates to the remaining $n-1$ nodes later.\n\nAdopt the following probabilistic model for crash-induced data loss during a write:\n- For each write and each replica that is actively persisting that write at the time of a system crash, the probability that the replica does not contain the write after recovery is $p \\in [0,1]$. Interpret $p$ as a per-write, per-replica loss probability that aggregates the chance and timing of a crash relative to the write’s persistence operation. Assume independence of these loss events across replicas for a given write, and independence of writes across time.\n- Under synchronous replication, all $n$ replicas begin persisting immediately, and the write is acknowledged only after all $n$ replicas have made the write durable. A write is considered lost if after a crash none of the $n$ replicas contains it.\n- Under asynchronous replication, at the instant a crash can affect a given write during its vulnerable window, only the primary replica is actively persisting that write; the other $n-1$ replicas have not yet started persisting it. A write is considered lost if the primary does not contain it after recovery.\n\nLet $t$ be the number of client writes issued, with $t \\in \\mathbb{N}$, and assume the independence of loss events across writes.\n\nStarting only from the definitions of independence, indicator random variables, and linearity of expectation, derive closed-form expressions for the expected total number of lost writes over $t$ writes under:\n- synchronous replication to $n$ replicas, and\n- asynchronous replication as described above.\n\nExpress your final answer as a $1 \\times 2$ row matrix whose first entry is the synchronous case and second entry is the asynchronous case, in terms of $p$, $n$, and $t$. Do not round; provide exact expressions without units.",
            "solution": "Let $L$ be the random variable representing the total number of lost writes over $t$ total client writes. We are asked to find the expected value of $L$, denoted $E[L]$, for two different replication strategies.\n\nThe derivation will be based on three fundamental principles as required:\n1.  **Indicator Random Variables:** For any event $A$, let $I_A$ be a random variable such that $I_A = 1$ if event $A$ occurs, and $I_A = 0$ otherwise. The expectation of an indicator variable is the probability of the event: $E[I_A] = 1 \\cdot P(A) + 0 \\cdot P(A^c) = P(A)$.\n2.  **Linearity of Expectation:** For any set of random variables $X_1, X_2, \\dots, X_t$, the expectation of their sum is the sum of their expectations: $E[\\sum_{i=1}^{t} X_i] = \\sum_{i=1}^{t} E[X_i]$. This holds regardless of whether the random variables are independent.\n3.  **Independence of Events:** If events $A_1, A_2, \\dots, A_n$ are mutually independent, the probability of their intersection is the product of their individual probabilities: $P(A_1 \\cap A_2 \\cap \\dots \\cap A_n) = \\prod_{j=1}^{n} P(A_j)$.\n\nLet $L_i$ be an indicator random variable for the event that the $i$-th write (for $i \\in \\{1, 2, \\dots, t\\}$) is lost.\n$$\nL_i = \\begin{cases} 1 & \\text{if write } i \\text{ is lost} \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe total number of lost writes, $L$, is the sum of these indicator variables:\n$$\nL = \\sum_{i=1}^{t} L_i\n$$\nBy linearity of expectation, the expected total number of lost writes is:\n$$\nE[L] = E\\left[\\sum_{i=1}^{t} L_i\\right] = \\sum_{i=1}^{t} E[L_i]\n$$\nFrom the property of indicator variables, $E[L_i] = P(\\text{write } i \\text{ is lost})$. The problem states that loss events are independent across writes. This implies that the probability of any given write being lost is constant for all writes. Let this probability be $\\pi$.\n$$\nP(\\text{write } i \\text{ is lost}) = \\pi \\quad \\forall i \\in \\{1, 2, \\dots, t\\}\n$$\nTherefore, the expected total number of lost writes simplifies to:\n$$\nE[L] = \\sum_{i=1}^{t} \\pi = t \\pi\n$$\nThe task reduces to finding the specific probability of loss, $\\pi$, for each replication strategy.\n\n**1. Synchronous Replication**\n\nLet $\\pi_{sync}$ be the probability that a single write is lost under the synchronous replication strategy.\nA write is defined as lost if, after a crash, none of the $n$ replicas contain it.\nLet $F_j$ be the event that replica $j$ (for $j \\in \\{1, \\dots, n\\}$) does not contain the write after recovery.\nIn the synchronous model, all $n$ replicas are actively persisting the write. The problem states that for each such replica, the probability of not containing the write after a crash is $p$.\nThus, $P(F_j) = p$ for all $j \\in \\{1, \\dots, n\\}$.\nThe event \"the write is lost\" is the intersection of all these failure events: $F_1 \\cap F_2 \\cap \\dots \\cap F_n$.\nThe problem assumes that these loss events are independent across replicas. Therefore, we can find the probability of their intersection by multiplying their individual probabilities:\n$$\n\\pi_{sync} = P(F_1 \\cap F_2 \\cap \\dots \\cap F_n) = \\prod_{j=1}^{n} P(F_j) = \\prod_{j=1}^{n} p = p^n\n$$\nThe expected total number of lost writes under synchronous replication, $E[L_{sync}]$, is:\n$$\nE[L_{sync}] = t \\pi_{sync} = t p^n\n$$\n\n**2. Asynchronous Replication**\n\nLet $\\pi_{async}$ be the probability that a single write is lost under the asynchronous replication strategy.\nA write is defined as lost if the primary replica does not contain it after recovery.\nIn the asynchronous model, at the instant a crash can occur, only the primary replica is actively persisting the write. Let us label the primary as replica $1$. The other $n-1$ replicas have not yet begun persisting.\nThe event \"the write is lost\" is simply the event $F_1$, where $F_1$ is the event that the primary replica fails to contain the write after recovery.\nThe state of the other $n-1$ replicas is irrelevant to the loss condition as defined for this strategy.\nThe probability of failure for the single actively-persisting primary replica is given as $p$.\n$$\n\\pi_{async} = P(F_1) = p\n$$\nThe expected total number of lost writes under asynchronous replication, $E[L_{async}]$, is:\n$$\nE[L_{async}] = t \\pi_{async} = t p\n$$\n\nThe final answer is a $1 \\times 2$ row matrix containing the expected values for the synchronous and asynchronous cases, respectively.\nFirst entry: $t p^n$\nSecond entry: $t p$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} t p^{n} & t p \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While the previous exercise focused on the probability of single events, real systems evolve continuously between states of health, failure, and recovery. This practice introduces a more powerful tool: the Continuous-Time Markov Chain (CTMC), which allows us to model the lifecycle of a replica over time. By calculating the steady-state availability, you will learn how to quantitatively reason about a system's long-term reliability based on its failure and repair rates .",
            "id": "3641355",
            "problem": "An operating system service employs replication for fault tolerance. Consider a single replica whose operational state evolves over time according to a Continuous-Time Markov Chain (CTMC). The replica can be in exactly $3$ states: healthy ($H$), recovering ($R$), or failed ($F$). The system provides service only when the replica is in the healthy state $H$; the recovering $R$ and failed $F$ states are unavailable.\n\nAssume the following, grounded in the memoryless property of exponential holding times and the definition of a CTMC:\n- While healthy, the replica fails with an exponential time-to-failure having rate $\\lambda \\gt 0$, transitioning from $H$ to $F$.\n- Repair occurs in two exponential phases, each with rate $\\mu \\gt 0$, capturing diagnosis/initiation and state reconstruction. Specifically, the chain transitions from $F$ to $R$ at rate $\\mu$, and from $R$ back to $H$ at rate $\\mu$.\n\nUsing only the foundational definitions of a CTMC and steady-state (long-run) probabilities, formulate this $3$-state CTMC and derive the steady-state availability $A(\\lambda,\\mu)$, defined as the stationary probability that the replica is in state $H$. Express your final result as a single simplified closed-form algebraic expression in terms of $\\lambda$ and $\\mu$. No rounding is required, and no units should be included in the final expression.",
            "solution": "The system is modeled as a CTMC with a state space $S = \\{H, R, F\\}$, where $H$ denotes the healthy state, $R$ the recovering state, and $F$ the failed state. The transitions between these states are governed by exponential holding times with given rates. The transitions can be summarized as:\n1.  From state $H$ to state $F$ at a rate of $\\lambda$.\n2.  From state $F$ to state $R$ at a rate of $\\mu$.\n3.  From state $R$ to state $H$ at a rate of $\\mu$.\n\nWe seek the steady-state (or stationary) probability distribution of this chain. Let $\\pi_H$, $\\pi_R$, and $\\pi_F$ be the long-run proportions of time the system spends in states $H$, $R$, and $F$, respectively. These probabilities are constant in steady state, which implies that for each state, the rate of flow into the state must equal the rate of flow out of the state. This principle yields a set of balance equations.\n\nThe balance equation for each state is formulated as follows:\n- For state $H$: The flow into state $H$ comes from state $R$ at rate $\\mu$. The flow out of state $H$ goes to state $F$ at rate $\\lambda$. Thus, the balance equation is:\n$$ \\mu \\pi_R = \\lambda \\pi_H $$\n\n- For state $R$: The flow into state $R$ comes from state $F$ at rate $\\mu$. The flow out of state $R$ goes to state $H$ at rate $\\mu$. Thus, the balance equation is:\n$$ \\mu \\pi_F = \\mu \\pi_R $$\nThis equation simplifies to:\n$$ \\pi_F = \\pi_R $$\n\n- For state $F$: The flow into state $F$ comes from state $H$ at rate $\\lambda$. The flow out of state $F$ goes to state $R$ at rate $\\mu$. Thus, the balance equation is:\n$$ \\lambda \\pi_H = \\mu \\pi_F $$\n\nNote that these three equations are linearly dependent. We can use any two of them. Let us use $\\mu \\pi_R = \\lambda \\pi_H$ and $\\pi_F = \\pi_R$. From the first equation, we can express $\\pi_R$ in terms of $\\pi_H$:\n$$ \\pi_R = \\frac{\\lambda}{\\mu} \\pi_H $$\nSince $\\pi_F = \\pi_R$, we also have:\n$$ \\pi_F = \\frac{\\lambda}{\\mu} \\pi_H $$\n\nTo find a unique solution, we must use the normalization condition that the sum of all stationary probabilities must equal $1$:\n$$ \\pi_H + \\pi_R + \\pi_F = 1 $$\n\nNow, we substitute the expressions for $\\pi_R$ and $\\pi_F$ in terms of $\\pi_H$ into the normalization equation:\n$$ \\pi_H + \\left(\\frac{\\lambda}{\\mu} \\pi_H\\right) + \\left(\\frac{\\lambda}{\\mu} \\pi_H\\right) = 1 $$\n\nFactor out $\\pi_H$:\n$$ \\pi_H \\left(1 + \\frac{\\lambda}{\\mu} + \\frac{\\lambda}{\\mu}\\right) = 1 $$\n$$ \\pi_H \\left(1 + \\frac{2\\lambda}{\\mu}\\right) = 1 $$\n\nTo solve for $\\pi_H$, we first combine the terms inside the parenthesis:\n$$ \\pi_H \\left(\\frac{\\mu + 2\\lambda}{\\mu}\\right) = 1 $$\n\nFinally, we isolate $\\pi_H$:\n$$ \\pi_H = \\frac{\\mu}{\\mu + 2\\lambda} $$\n\nThe problem defines the steady-state availability, $A(\\lambda, \\mu)$, as the stationary probability that the replica is in the healthy state $H$. Therefore, $A(\\lambda, \\mu) = \\pi_H$.\n\nThe resulting expression for the steady-state availability is:\n$$ A(\\lambda, \\mu) = \\frac{\\mu}{\\mu + 2\\lambda} $$\nThis is a closed-form algebraic expression in terms of the given parameters $\\lambda$ and $\\mu$.",
            "answer": "$$\\boxed{\\frac{\\mu}{\\mu + 2\\lambda}}$$"
        },
        {
            "introduction": "Abstract models of failure must ultimately confront the messy realities of physical hardware and protocol design. This problem moves from high-level probabilities to the practical mechanics of data integrity in a quorum-based system, exploring how physical faults like torn sectors can corrupt data. You will analyze how low-level mechanisms like checksums interact with high-level consensus protocols to detect corruption and correctly reconstruct the state of a replicated log after a crash .",
            "id": "3641407",
            "problem": "Consider a replicated append-only log in an operating system storage subsystem. There are $n$ replicas, and a write is considered committed when acknowledgments from at least $q$ replicas have been received. Each log record consists of a sequence number $S$, a payload $P$ of size $L$ bytes, and a fixed-size header that stores the pair $(L,H)$, where $H$ is a checksum computed over $(S,P)$. The checksum function is a Cyclic Redundancy Check (CRC) of $b$ bits, and we assume it behaves as a uniformly distributed hash over valid inputs. Disks write in sectors of size $\\sigma$ bytes, and a power failure can cause a partial write of a sector, producing a torn sector in which only a prefix of the sector is durable.\n\nOn recovery, the system must detect torn writes and reconstruct the latest committed record. The recovery procedure scans each replica’s log from the beginning, validating each record by checking that the next $L$ bytes are present and that the recomputed checksum $H'$ equals the stored $H$. When a record fails validation, the scan stops and the replica’s log is truncated at the last valid record. Assume crash-stop faults (replicas either faithfully store the bits they attempted to write or lose a suffix due to a torn sector; replicas are not malicious) and that acknowledgments are sent only after the header $(L,H)$ and payload $P$ have been issued to the device with appropriate write barriers.\n\nSuppose $n=5$, $q=3$, $\\sigma=4096$ bytes, and $b=32$. Analyze the impact of partial writes (torn sectors) on replication correctness and propose how checksum-plus-length framing enables repair from a quorum. Which of the following statements are correct? Select all that apply.\n\nA. Majority quorum alone suffices to mask torn sectors without any integrity mechanisms, because any two majority quorums intersect and the intersection replica can disambiguate the correct tail.\n\nB. With a uniformly distributed $b$-bit checksum, the probability that a random torn payload happens to match both the stored $L$ and $H$ is at most $2^{-b}$, making undetected corruption negligible for $b=32$.\n\nC. In repair, reading any $q$ replicas and selecting the payload whose checksum appears most frequently among them always returns the latest committed payload.\n\nD. A sound repair rule is: among candidates with valid $(L,H)$, find the largest sequence number $S$ such that the pair $(L,H)$ appears identically on at least $q$ replicas; declare that payload committed and rewrite all other replicas to match; under crash-stop faults and torn sectors detectable by framing, this reconstructs the committed record.\n\nE. Aligning records to the sector size $\\sigma$ eliminates torn sectors, so checksum-plus-length framing is unnecessary if $L$ is a multiple of $\\sigma$.",
            "solution": "The system uses a majority quorum ($n=5, q=3$). A key property of a majority quorum system is that any two quorums have a non-empty intersection. Specifically, for any two quorum sets $Q_1, Q_2$, we have $|Q_1 \\cap Q_2| \\ge |Q_1|+|Q_2|-n = q+q-n = 3+3-5=1$. This intersection is fundamental to ensuring that information about the latest committed state can be retrieved. The problem revolves around how this quorum property interacts with physical data corruption (torn sectors) and the mechanism used to detect it (checksum-plus-length framing).\n\n**A. Majority quorum alone suffices to mask torn sectors without any integrity mechanisms, because any two majority quorums intersect and the intersection replica can disambiguate the correct tail.**\n\nThis statement is **Incorrect**. The quorum intersection property is designed to resolve conflicts between different *valid* versions of the log, not to detect or correct data corruption. A torn sector results in corrupted, invalid data. Without an integrity mechanism like a checksum, it is impossible to distinguish a valid record from a prefix of a record or other random data left by a partial write. The replica(s) in the intersection of two quorums might themselves contain the corrupted, torn data. Relying on such a replica to \"disambiguate\" would propagate the corruption. For example, if a write for sequence number $S_k$ is acknowledged by replicas $\\{R_1, R_2, R_3\\}$ and committed, and subsequently a new write for $S_{k+1}$ is attempted, a power failure might leave $R_1$ with a torn write of $S_{k+1}$ while $R_2$ and $R_3$ never started the write. On recovery, if we poll a quorum $\\{R_1, R_4, R_5\\}$, the replica $R_1$ has corrupted data. Without a checksum, a recovery protocol cannot determine that $R_1$'s tail is corrupt and must be truncated. Thus, an integrity mechanism is essential and quorums alone are insufficient.\n\n**B. With a uniformly distributed $b$-bit checksum, the probability that a random torn payload happens to match both the stored $L$ and $H$ is at most $2^{-b}$, making undetected corruption negligible for $b=32$.**\n\nThis statement is **Correct**. The checksum-plus-length framing mechanism works as follows upon recovery: read a header $(L, H)$, read the subsequent $L$ bytes to get the payload $P$, and verify if `checksum(S, P) == H`. An undetected error occurs if a corrupted sequence of bytes on disk is accidentally interpreted as a valid record. This requires the corrupted data to be parsed as a valid-looking header $(L_{corr}, H_{corr})$ and a payload $P_{corr}$ of length $L_{corr}$ such that the recomputed checksum matches $H_{corr}$. The core of the protection lies in the checksum. Assuming the checksum function behaves like a random oracle (as implied by \"uniformly distributed hash\"), for any arbitrary input data (like a corrupted payload), the probability that its $b$-bit checksum evaluates to a specific predetermined value is $1/2^b$. Therefore, the probability of a random corruption passing the checksum validation is $2^{-b}$. With $b=32$, this probability is $1/2^{32} \\approx 2.3 \\times 10^{-10}$, which is an extremely small value. For most applications, this risk is considered negligible. The statement describes this fundamental property of checksums correctly.\n\n**C. In repair, reading any $q$ replicas and selecting the payload whose checksum appears most frequently among them always returns the latest committed payload.**\n\nThis statement is **Incorrect**. This describes a simple voting mechanism that is known to be flawed. It can lead to choosing a write that was never committed. Consider the following scenario: The latest committed record is $S_k$, present on all $n=5$ replicas. A new write for record $S_{k+1}$ is initiated. It gets successfully written to $2$ replicas, $\\{R_1, R_2\\}$, which is fewer than the quorum size $q=3$. Therefore, $S_{k+1}$ is *not* committed. A power failure occurs. After recovery and truncation of partial writes, the states of the tails of the logs are:\n-   $R_1$: contains valid record $S_{k+1}$.\n-   $R_2$: contains valid record $S_{k+1}$.\n-   $R_3, R_4, R_5$: contain valid record $S_k$.\nNow, a repair process reads a quorum of $q=3$ replicas, for instance, $\\{R_1, R_2, R_3\\}$. It observes two instances of the checksum for $S_{k+1}$ and one instance for $S_k$. Following the proposed rule, it would select $S_{k+1}$ as the correct state because its checksum appears most frequently. However, $S_{k+1}$ was never committed. This would violate the system's correctness by resurrecting uncommitted data.\n\n**D. A sound repair rule is: among candidates with valid $(L,H)$, find the largest sequence number $S$ such that the pair $(L,H)$ appears identically on at least $q$ replicas; declare that payload committed and rewrite all other replicas to match; under crash-stop faults and torn sectors detectable by framing, this reconstructs the committed record.**\n\nThis statement is **Correct**. This rule is a simplified version of the read/recovery protocol used in systems like Paxos or Raft. Let's analyze its logic:\n1.  A record write is committed only if it has been successfully stored on a quorum of $q$ replicas.\n2.  The `(L,H)` pair serves as a unique identifier for the content of a specific version (sequence number $S$) of a record.\n3.  The rule searches for the highest sequence number $S_{max}$ whose record content (identified by $(L,H)$) is present on at least $q$ replicas.\n4.  Because any committed write for a sequence number $S_{commit}$ must have been stored on a quorum $Q_{write}$, and the recovery process reads its own quorum $Q_{read}$, the intersection $Q_{write} \\cap Q_{read}$ is non-empty. This ensures that the recovery process will see evidence of any committed write.\n5.  By requiring a full quorum count ($q$) for a given record $(S, (L,H))$, the rule ensures it only considers records that *could have been* committed.\n6.  By picking the *largest* such sequence number, it finds the most recent state that is guaranteed to be stable. Any record with a higher sequence number that appears on fewer than $q$ replicas was, by definition, not committed and must be rolled back. Any record with a lower sequence number is part of a log prefix that is superseded by this newer committed state. This procedure correctly identifies the head of the committed log.\n\n**E. Aligning records to the sector size $\\sigma$ eliminates torn sectors, so checksum-plus-length framing is unnecessary if $L$ is a multiple of $\\sigma$.**\n\nThis statement is **Incorrect**. The premise that alignment eliminates torn sectors is false. A \"torn sector\" is a physical phenomenon where a single sector write is interrupted. The problem statement explicitly allows this. Even if a record write involves only one sector (i.e., $L < \\sigma$ and the record is contained within a sector), that sector itself can be torn upon power failure. If a record spans multiple sectors (i.e., $L > \\sigma$), a power failure can occur after some sectors are written but before all are, leaving an incomplete (torn) record. The write of $k=\\lceil L/\\sigma \\rceil$ sectors is generally not an atomic operation. Therefore, alignment does not remove the possibility of a record being partially written. Consequently, an integrity mechanism like checksum-plus-length framing is still necessary to detect such partial writes and distinguish a valid, complete record from a corrupted, incomplete one.",
            "answer": "$$\\boxed{BD}$$"
        }
    ]
}