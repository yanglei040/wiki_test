{
    "hands_on_practices": [
        {
            "introduction": "Network communication is inherently unreliable; messages can be lost, and clients may retry requests. This exercise challenges you to explore the crucial concept of idempotency, which ensures that retrying an operation produces the same result as performing it once. By classifying familiar file system operations, you will develop the intuition needed to design reliable RPCs and learn standard patterns, like using idempotency keys, to make any operation safe under at-least-once delivery semantics.",
            "id": "3677029",
            "problem": "A client communicates with a remote file service using Remote Procedure Call (RPC), defined formally as a determination procedure that maps a request and current server state to a new state and a reply. Let the server maintain a state space denoted by $S$ and define an RPC execution as a function $F$ where $F(S, x) = (S', r)$, with $x$ the operation request, $S'$ the new state, and $r$ the reply. The network realizes at-least-once delivery semantics due to retries: if the client does not receive a reply within a timeout, it re-sends the same request, and the server may execute the same request multiple times. An operation $x$ is said to be idempotent with respect to both state and observable reply if for any state $S$,\n$$F(S, x) = (S', r) \\quad \\text{and} \\quad F(S', x) = (S', r),$$\nmeaning that re-execution does not change the resulting state or the reply relative to a single execution.\n\nConsider a Portable Operating System Interface (POSIX)-like file service offering the following operations on a path $P$ or a file $F$:\n$\\mathsf{read}(F, o, n)$ which returns $n$ bytes from offset $o$, $\\mathsf{writeAt}(F, o, B)$ which writes the byte sequence $B$ at offset $o$, $\\mathsf{append}(F, B)$ which appends $B$ to the end of $F$, $\\mathsf{create}(P)$ which creates a new empty file at $P$ and returns success if $P$ did not exist, $\\mathsf{delete}(P)$ which removes the file at $P$, $\\mathsf{chmod}(P, m)$ which sets the mode bits to $m$ (absolute assignment), and $\\mathsf{rename}(A, B)$ which atomically moves the path $A$ to $B$.\n\nAssume:\n- The server executes each operation atomically and immediately persists the effect before replying.\n- There is a single client issuing these operations, and the only source of duplicate executions is network-level retry of the same request; there are no concurrent writes by other clients in between retries.\n- When an operation is retried, the client re-sends an identical request message.\n\nFrom the foundational definition of idempotency above and standard POSIX-like semantics, classify which operations are naturally idempotent under at-least-once semantics and which ones are non-idempotent because retries change user-visible semantics (state and replies). For the non-idempotent ones, propose wrapper strategies that make them safe under retries without requiring global transactions, such as incorporating client-generated idempotency keys and conditional preconditions that the server can verify to produce the same effect and reply for re-executions of the same logical request.\n\nWhich option correctly classifies idempotent versus non-idempotent operations and proposes wrappers that achieve idempotent behavior under retries while respecting typical file system invariants?\n\nA. Classify $\\mathsf{read}(F, o, n)$, $\\mathsf{writeAt}(F, o, B)$, and $\\mathsf{chmod}(P, m)$ as idempotent; classify $\\mathsf{append}(F, B)$, $\\mathsf{create}(P)$, $\\mathsf{delete}(P)$, and $\\mathsf{rename}(A, B)$ as non-idempotent because retries can change replies even when state does not change. Make non-idempotent operations safe using request-scoped idempotency keys $k$: for $\\mathsf{append}$ use $\\mathsf{appendToken}(F, k, B)$ that writes $B$ exactly once, records the chosen offset, and returns the same offset on retries of the same $k$; for $\\mathsf{create}$ use $\\mathsf{createToken}(P, k)$ that either creates and records $\\langle P, k \\rangle$ or, if $P$ already exists due to the same $k$, returns success, and if $P$ exists due to a different cause returns “already exists”; for $\\mathsf{delete}$ use $\\mathsf{deleteToken}(P, k)$ that deletes and records $\\langle P, k \\rangle$, returning the same success on a retry of $k$ and “not found” if $P$ is absent and was not removed by $k$; for $\\mathsf{rename}$ use $\\mathsf{renameToken}(A, B, k, i, v)$ that conditionally renames only if $A$ currently refers to inode $i$ at version $v$, records $\\langle A \\to B, k \\rangle$, and returns the same success or error on retried $k$.\n\nB. Classify $\\mathsf{append}(F, B)$ as idempotent because appending the same $B$ twice yields two copies of $B$ which is “acceptable,” classify $\\mathsf{writeAt}(F, o, B)$ as non-idempotent because writing twice “overwrites different bytes,” and make all operations safe by client-side retry counters $c$ without server support, assuming the server will drop duplicates if $c$ increases.\n\nC. Classify $\\mathsf{rename}(A, B)$ as idempotent because moving a path twice does not “move it again,” classify $\\mathsf{delete}(P)$ as idempotent because removing an already absent file has no state effect, and make $\\mathsf{create}(P)$ safe by renaming a temporary file to $P$ without any idempotency key, assuming that retries of the rename always return success.\n\nD. Classify $\\mathsf{read}(F, o, n)$ as non-idempotent because the file may change between retries, classify $\\mathsf{writeAt}(F, o, B)$ and $\\mathsf{append}(F, B)$ as idempotent, and make non-idempotent operations safe by introducing client-side sleeps before retry so that the server “stabilizes,” without any server-side deduplication or conditional checks.\n\nSelect the single best option.",
            "solution": "Begin from first principles. An RPC operation $x$ is idempotent if $F(F(S, x)_1, x) = F(S, x)$, where $F(S, x) = (S', r)$ and $F(S', x) = (S', r)$; here the subscript $1$ selects the state component. In words, applying $x$ twice produces the same state and reply as applying it once. Under at-least-once semantics, the same logical request $x$ may execute multiple times, so idempotency is determined by whether repeats of the identical request alter either the final state or the reply relative to a single execution.\n\nAssumptions constrain variability: a single client, atomic operations, and identical retries mean that any change between repeated executions is due to the operation itself, not concurrent interference.\n\nAnalyze each operation:\n\n- $\\mathsf{read}(F, o, n)$: Under the assumptions, the file content does not change between retries because no other client writes and the same client is only retrying due to a lost reply. Therefore the state remains unchanged and the reply, the byte sequence returned, is the same on re-execution. Thus $\\mathsf{read}$ is idempotent.\n\n- $\\mathsf{writeAt}(F, o, B)$: Writing the same bytes $B$ at the same offset $o$ twice yields the same final state as writing once: the file bytes in the range are equal to $B$ after the first write, and the second write writes $B$ again at the same positions, leaving the file unchanged. Replies in POSIX-like semantics for a successful write indicate the count of bytes written; with atomicity and persistence, duplicate execution returns the same success. Thus $\\mathsf{writeAt}$ is idempotent under the stated conditions.\n\n- $\\mathsf{chmod}(P, m)$ with absolute assignment: Setting the mode bits to $m$ twice results in the same mode $m$; because the assignment is absolute (not a toggle or arithmetic update), the reply indicating success is the same on a retry. Thus $\\mathsf{chmod}$ is idempotent.\n\n- $\\mathsf{append}(F, B)$: Appending $B$ to the end of a file changes file length by $\\lvert B \\rvert$. Re-executing the same append will further increase length by $\\lvert B \\rvert$ again, and the content will contain two copies of $B$. The reply typically includes the number of bytes appended or the new end offset; this reply would differ between the first and second execution. Therefore $\\mathsf{append}$ is not idempotent.\n\n- $\\mathsf{create}(P)$: Creating a file at $P$ once changes the state from “absent” to “present.” A second execution over the same path $P$ yields a different reply: standard semantics return “already exists” error; even if the state remains “present,” the reply changes. Therefore, relative to the definition incorporating reply semantics, $\\mathsf{create}$ is not idempotent.\n\n- $\\mathsf{delete}(P)$: Removing a file once changes the state from “present” to “absent.” A second execution on an absent file produces a different reply, “not found,” although the state remains “absent.” Given the definition includes reply, $\\mathsf{delete}$ is not idempotent.\n\n- $\\mathsf{rename}(A, B)$: Moving a path from $A$ to $B$ once causes $A$ to become absent and $B$ to become present (pointing to the moved inode). A second execution over the same arguments typically fails because $A$ no longer exists, producing a different reply; therefore $\\mathsf{rename}$ is not idempotent.\n\nFor non-idempotent operations, wrappers can provide idempotent semantics by binding a logical request to an idempotency key $k$ and making the server return the same effect and reply on re-execution of the same logical request. Principles for such wrappers include:\n\n- Deduplication via a per-client key: the server stores a mapping from $(\\text{client}, k)$ to the outcome $(S', r)$ for operations that completed, ensuring that a duplicate request with the same $(\\text{client}, k)$ returns the stored reply $r$ and does not re-apply the state change.\n\n- Conditional preconditions: the server checks that the state satisfies the same preconditions the client saw when issuing the request (for example, the source inode and version for a rename). If the preconditions fail, the same error is returned; if the preconditions hold and the operation was completed under $k$, the same success is returned.\n\nApply those to each non-idempotent operation:\n\n- $\\mathsf{append}(F, B)$: Use $\\mathsf{appendToken}(F, k, B)$ that records $k$ in a per-file ledger when appending $B$, along with the chosen offset $o$ and a hash of $B$ to detect mismatch. On retry with the same $k$, the server returns the previously recorded $o$ and does not append again, thereby ensuring $F(F(S, x)_1, x) = F(S, x)$ for $x = \\mathsf{appendToken}(F, k, B)$.\n\n- $\\mathsf{create}(P)$: Use $\\mathsf{createToken}(P, k)$ that creates the file and records $\\langle P, k \\rangle$ upon success. If a retry arrives with the same $k$, the server detects the existing record and returns success again. If the path $P$ exists but was not created under $k$, the server returns the “already exists” error, preserving invariants.\n\n- $\\mathsf{delete}(P)$: Use $\\mathsf{deleteToken}(P, k)$ that deletes and records $\\langle P, k \\rangle$. On retry under the same $k$, the server returns success even if $P$ is already absent, because the same logical deletion completed earlier under $k$. If $P$ is absent and there is no record for $k$, the server returns “not found,” preserving correctness relative to a different logical request.\n\n- $\\mathsf{rename}(A, B)$: Use $\\mathsf{renameToken}(A, B, k, i, v)$ where the client includes the source inode $i$ and version $v$ observed before issuing the rename. The server performs the rename only if $A$ refers to inode $i$ at version $v$, then records $\\langle A \\to B, k \\rangle$. A retry with the same $k$ returns the same success; if $A$ no longer matches $i, v$ and there is no record for $k$, the server returns the same error as the first attempt would, avoiding unintended extra moves.\n\nEvaluate each option:\n\nA. This option classifies $\\mathsf{read}$, $\\mathsf{writeAt}$, and $\\mathsf{chmod}$ as idempotent and the rest as non-idempotent under retries because replies change even if state may not. This matches the analysis above. The proposed wrappers use idempotency keys $k$ and conditional checks to ensure that a duplicate execution yields the same reply without re-applying the state change. Each wrapper respects common file system invariants: $\\mathsf{appendToken}$ does not duplicate content on retries, $\\mathsf{createToken}$ distinguishes between files created by the same logical request versus pre-existing files, $\\mathsf{deleteToken}$ preserves “not found” for different logical requests while making the same logical deletion idempotent, and $\\mathsf{renameToken}$ uses preconditions ($i, v$) to avoid unintended extra moves and returns the same outcome under retries. Verdict — Correct.\n\nB. This option incorrectly declares $\\mathsf{append}$ idempotent on the basis that duplicating content is “acceptable.” By the formal definition, appending twice produces different state and reply than appending once, so it is non-idempotent. It also incorrectly claims $\\mathsf{writeAt}$ is non-idempotent under the stated assumptions; writing the same $B$ at the same $o$ twice yields the same state and reply. Furthermore, relying only on client-side retry counters $c$ without server-side deduplication cannot guarantee that the server will suppress duplicates, especially across reconnections or stateless servers. Verdict — Incorrect.\n\nC. This option mistakenly classifies $\\mathsf{rename}$ as idempotent, arguing that moving “does not move it again,” but retries produce different replies because the source no longer exists, violating the idempotency condition. It also treats $\\mathsf{delete}$ as idempotent, which fails when the second execution returns “not found.” The proposed $\\mathsf{create}$ wrapper relying solely on renaming a temporary file to $P$ without an idempotency key does not ensure that a retried rename returns the same reply; if the first rename succeeded and the second executes, the source temporary path may be missing and the reply would differ. Verdict — Incorrect.\n\nD. This option incorrectly classifies $\\mathsf{read}$ as non-idempotent under the given assumptions; with no intervening writes, a retried read returns the same data and reply. It misclassifies $\\mathsf{append}$ as idempotent. The suggested “sleep before retry” does not provide any server-side mechanism to ensure duplicate suppression or reply consistency, so it fails to achieve idempotent behavior under retries. Verdict — Incorrect.\n\nTherefore, the correct option is A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A robust RPC server must not only be correct but also resilient against misbehaving or malicious clients. This practice puts you in the role of a security-conscious developer, tasking you with simulating a defensive deserialization policy to prevent resource exhaustion attacks. You will implement and test a strategy that combines input validation with incremental memory allocation, a vital technique for protecting services from denial-of-service vulnerabilities.",
            "id": "3677042",
            "problem": "You are to design and implement a self-contained program that simulates a Remote Procedure Call (RPC) message deserializer and evaluates whether a defensively coded, partial-deserialization policy prevents resource exhaustion under adversarial and boundary inputs. The context is a length-prefixed RPC message where the first field declares the message length. Your task is to reason from first principles about resource allocation under streaming inputs and implement a test harness that verifies safety properties without performing any network input or actual memory allocation.\n\nFundamental base and core definitions:\n- Remote Procedure Call (RPC) is a mechanism in which a client invokes a procedure on a remote server as if it were a local call. A common message framing approach is to prefix each message with a declared length. Deserialization is the process of reconstructing a structured message from a sequence of bytes.\n- Resource exhaustion occurs if an implementation allows unbounded or overly large allocations based on untrusted input. To avoid this, memory allocation should be bounded by a configured limit and performed incrementally.\n- We consider a single connection processing at most one message at a time, with a parser maintaining constant overhead.\n\nSafety policy to simulate (do not assume any shortcut formulas; instead, faithfully apply the rules below):\n- Let $L$ be the declared message length in bytes, $C$ be a configured per-message length cap in bytes, $M$ be the available memory budget in bytes for this connection, $O$ be the constant per-connection parser overhead in bytes, $P$ be the total number of payload bytes that actually arrive (which may be less than $L$ if the stream truncates), and $s$ be the size in bytes of each payload chunk that arrives incrementally.\n- The safe deserializer must enforce the following invariants:\n  - Immediately abort the message if the declared length $L$ exceeds the cap $C$ on reading the header, without allocating any message buffer. The parser overhead $O$ remains resident.\n  - If $L \\leq C$, do not preallocate $L$ bytes. Instead, grow the message buffer incrementally as payload arrives, in increments of at most the arriving chunk size $s$, never exceeding the smaller of the already received payload and the cap $C$.\n  - If the total received payload reaches the declared length $L$, the message completes and the message buffer is released. If the stream truncates early ($P < L$), the partially filled message buffer is released when input ends.\n  - At all times, the currently allocated memory equals the parser overhead $O$ plus the current message buffer; the peak allocation is the maximum of this quantity observed during processing of the message.\n- The system avoids resource exhaustion if and only if the peak allocation does not exceed the budget $M$.\n\nYour program must:\n- Simulate the above process for each test case in the provided test suite by stepping through arrival of payload in chunks of size $s$ bytes (the last chunk may be smaller), honoring the abort condition when $L > C$, the incremental growth rule when $L \\leq C$, and the buffer release on completion or truncation.\n- For each test case, output an integer $1$ if the safe policy avoids resource exhaustion (peak allocation $\\leq M$), or $0$ otherwise.\n\nTest suite:\n- Test case $1$: $L = 1024$ bytes, $P = 1024$ bytes, $s = 128$ bytes, $C = 2048$ bytes, $M = 4096$ bytes, $O = 64$ bytes.\n- Test case $2$: $L = 1000000000$ bytes, $P = 1$ byte, $s = 1$ byte, $C = 65536$ bytes, $M = 10000000$ bytes, $O = 64$ bytes.\n- Test case $3$: Boundary where $L = C$: $L = 4096$ bytes, $P = 4096$ bytes, $s = 257$ bytes, $C = 4096$ bytes, $M = 5000$ bytes, $O = 64$ bytes.\n- Test case $4$: Truncated payload with cap binding: $L = 50000$ bytes, $P = 10000$ bytes, $s = 3000$ bytes, $C = 8192$ bytes, $M = 8300$ bytes, $O = 64$ bytes.\n- Test case $5$: Zero-length message: $L = 0$ bytes, $P = 0$ bytes, $s = 1$ byte, $C = 16$ bytes, $M = 64$ bytes, $O = 64$ bytes.\n- Test case $6$: Tight budget failure: $L = 1024$ bytes, $P = 1024$ bytes, $s = 512$ bytes, $C = 1024$ bytes, $M = 1000$ bytes, $O = 64$ bytes.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[ x,y,z \\right]$), where each element corresponds to a test case in order and is an integer $1$ if the safe policy avoids resource exhaustion for that case, or $0$ otherwise. The outputs are unitless integers representing truth values encoded as $1$ for true and $0$ for false. No other text should be printed.",
            "solution": "The problem requires the simulation of a remote procedure call (RPC) deserialization process under a specific safety policy to determine if resource exhaustion is avoided. We will validate the problem statement, then systematically derive the outcome for each test case by simulating the state of memory allocation according to the provided rules.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n\nThe problem defines the following symbolic parameters:\n- $L$: Declared message length in bytes.\n- $C$: Configured per-message length cap in bytes.\n- $M$: Available memory budget in bytes for the connection.\n- $O$: Constant per-connection parser overhead in bytes.\n- $P$: Total number of payload bytes that actually arrive.\n- $s$: Size in bytes of each payload chunk that arrives incrementally.\n\nThe safety policy is defined by a set of invariants:\n1.  If $L > C$, the message is aborted upon reading the header. No message buffer is allocated. The parser overhead $O$ remains.\n2.  If $L \\leq C$, the message buffer is not pre-allocated. It is grown incrementally as payload arrives. The buffer size at any time must not exceed the smaller of the total payload received so far and the cap $C$.\n3.  The message is considered complete if the total received payload reaches $L$. The buffer is released on completion. If the stream is truncated ($P < L$), the partial buffer is released when input ends.\n4.  Current memory allocation is the sum of parser overhead $O$ and the current message buffer size.\n5.  Peak allocation is the maximum memory allocation observed during the processing of one message.\n6.  Resource exhaustion is avoided if and only if peak allocation does not exceed the budget $M$.\n\nThe test suite provides six distinct cases with specific values for the parameters $L, P, s, C, M, O$.\n\n#### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the following criteria:\n- **Scientifically Grounded**: The problem is well-grounded in fundamental computer science principles, specifically in the domain of network protocols, operating systems, and defensive programming. The concepts of length-prefixed messages, resource exhaustion attacks, and incremental allocation with caps are standard and realistic topics in software engineering.\n- **Well-Posed**: The problem is well-posed. The rules for the simulation are deterministic and stated algorithmically. For each set of input parameters from the test suite, a unique outcome (peak allocation) can be calculated, leading to a definitive binary result ($1$ or $0$).\n- **Objective**: The problem is stated in precise, quantitative, and objective language. It is free from subjectivity or ambiguity.\n- **Completeness and Consistency**: The problem is self-contained. All necessary data and rules are provided for each test case. The rules are structured as a set of conditional invariants that are not contradictory. The `if L > C` case is mutually exclusive with the `if L <= C` case, forming a complete decision procedure.\n- **Realism**: The scenario and parameters are realistic for modeling and testing the logic of a network service. The values are chosen to probe boundary conditions, which is standard practice in algorithm testing.\n\n#### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, and all necessary information is provided. We will proceed with a step-by-step solution.\n\n### Algorithmic Solution\nFor each test case, we simulate the memory allocation process to find the peak allocation and compare it against the memory budget $M$. Let `peak_alloc` be the peak memory allocation observed and `current_alloc` be the memory allocated at a given step.\n\nThe simulation algorithm for a single test case is as follows:\n1.  Initialize `peak_alloc` to the constant parser overhead $O$. This is the baseline allocation.\n2.  Evaluate the first invariant: Compare the declared length $L$ with the cap $C$.\n    - If $L > C$, the policy dictates immediate abortion. No message buffer is ever allocated. The memory usage never exceeds the initial parser overhead. Thus, `peak_alloc` remains $O$.\n    - If $L \\leq C$, proceed to the incremental allocation phase.\n3.  For cases where $L \\leq C$, simulate the arrival of payload in chunks. Initialize `bytes_received = 0` and `buffer_size = 0`.\n4.  Loop as long as more payload can arrive (`bytes_received < P`) and the message is not yet complete (`bytes_received < L`):\n    - a. Determine the size of the next incoming data chunk: `chunk_size = min(s, P - bytes_received)`.\n    - b. Update the total received data: `bytes_received = bytes_received + chunk_size`.\n    - c. Update the allocated buffer size according to the rule: `buffer_size = min(bytes_received, C)`. The buffer grows to hold the received data, but is clamped by the cap $C$.\n    - d. Calculate the current total memory allocation: `current_alloc = O + buffer_size`.\n    - e. Update the peak allocation: `peak_alloc = max(peak_alloc, current_alloc)`.\n5.  After the loop terminates (due to either receiving all payload $P$ or completing the message at length $L$), the buffer is released. The `peak_alloc` value holds the maximum memory footprint during the message's lifetime.\n6.  The final result for the test case is $1$ if `peak_alloc` $\\leq M$, and $0$ otherwise.\n\nWe now apply this algorithm to each test case.\n\n**Test Case 1**: $L=1024, P=1024, s=128, C=2048, M=4096, O=64$.\n- $L \\leq C$ ($1024 \\leq 2048$). Proceed with allocation.\n- `peak_alloc` is initialized to $O=64$.\n- The simulation receives payload until `bytes_received` reaches $1024$. The buffer grows with the received payload.\n- The maximum buffer size will be $\\min(1024, C=2048) = 1024$ bytes.\n- The peak allocation occurs when the buffer is full: `peak_alloc` $= O + 1024 = 64 + 1024 = 1088$ bytes.\n- Condition check: $1088 \\leq M=4096$. This is true.\n- Result: $1$.\n\n**Test Case 2**: $L=10^9, P=1, s=1, C=65536, M=10^7, O=64$.\n- $L > C$ ($10^9 > 65536$). The policy requires immediate abortion.\n- No buffer is allocated. Peak allocation is simply the parser overhead.\n- `peak_alloc` $= O = 64$ bytes.\n- Condition check: $64 \\leq M=10^7$. This is true.\n- Result: $1$.\n\n**Test Case 3**: $L=4096, P=4096, s=257, C=4096, M=5000, O=64$.\n- $L \\leq C$ ($4096 \\leq 4096$). Proceed with allocation.\n- `peak_alloc` is initialized to $O=64$.\n- The simulation receives payload until `bytes_received` reaches $4096$.\n- The maximum buffer size will be $\\min(4096, C=4096) = 4096$ bytes.\n- The peak allocation is `peak_alloc` $= O + 4096 = 64 + 4096 = 4160$ bytes.\n- Condition check: $4160 \\leq M=5000$. This is true.\n- Result: $1$.\n\n**Test Case 4**: $L=50000, P=10000, s=3000, C=8192, M=8300, O=64$.\n- $L > C$ ($50000 > 8192$). The policy requires immediate abortion.\n- No buffer is allocated. The name of the test case, \"Truncated payload with cap binding,\" is a distractor; the primary rule of aborting on $L > C$ takes precedence.\n- `peak_alloc` $= O = 64$ bytes.\n- Condition check: $64 \\leq M=8300$. This is true.\n- Result: $1$.\n\n**Test Case 5**: $L=0, P=0, s=1, C=16, M=64, O=64$.\n- $L \\leq C$ ($0 \\leq 16$). Proceed.\n- `peak_alloc` is initialized to $O=64$.\n- $P=0$, so no payload arrives. The reception loop does not execute. The message is considered complete immediately as `bytes_received` ($0$) is not less than $L$ ($0$).\n- No buffer is allocated (`buffer_size` remains $0$).\n- The peak allocation does not exceed the initial overhead: `peak_alloc` $= 64$ bytes.\n- Condition check: $64 \\leq M=64$. This is true.\n- Result: $1$.\n\n**Test Case 6**: $L=1024, P=1024, s=512, C=1024, M=1000, O=64$.\n- $L \\leq C$ ($1024 \\leq 1024$). Proceed with allocation.\n- `peak_alloc` is initialized to $O=64$.\n- The simulation receives payload until `bytes_received` reaches $1024$.\n- Maximum buffer size will be $\\min(1024, C=1024) = 1024$ bytes.\n- The peak allocation is `peak_alloc` $= O + 1024 = 64 + 1024 = 1088$ bytes.\n- Condition check: $1088 \\leq M=1000$. This is false.\n- Result: $0$.\n\n### Summary of Results\nThe results for the test cases are:\n1.  Case 1: $1$\n2.  Case 2: $1$\n3.  Case 3: $1$\n4.  Case 4: $1$\n5.  Case 5: $1$\n6.  Case 6: $0$\nThe program should output these results as a comma-separated list in brackets.",
            "answer": "```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\n// No other headers are permitted.\n// #include <complex.h>\n// #include <threads.h>\n// #include <stdatomic.h>\n\n/**\n * @brief A struct to hold the parameters for a single test case.\n *\n * Using unsigned long long to accommodate large values like L in test case 2.\n */\ntypedef struct {\n    unsigned long long L; // Declared message length\n    unsigned long long P; // Actual payload bytes arrived\n    unsigned long long s; // Payload chunk size\n    unsigned long long C; // Per-message length cap\n    unsigned long long M; // Available memory budget\n    unsigned long long O; // Parser overhead\n} TestCase;\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        {1024, 1024, 128, 2048, 4096, 64},        // Test Case 1\n        {1000000000, 1, 1, 65536, 10000000, 64}, // Test Case 2\n        {4096, 4096, 257, 4096, 5000, 64},        // Test Case 3\n        {50000, 10000, 3000, 8192, 8300, 64},     // Test Case 4\n        {0, 0, 1, 16, 64, 64},                    // Test Case 5\n        {1024, 1024, 512, 1024, 1000, 64}         // Test Case 6\n    };\n\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    int results[num_cases];\n\n    // Calculate the result for each test case by simulating the deserializer policy.\n    for (int i = 0; i < num_cases; ++i) {\n        TestCase tc = test_cases[i];\n        unsigned long long peak_allocation;\n\n        // The baseline allocation is always the parser overhead. This is also the\n        // initial peak until/unless a larger allocation occurs.\n        peak_allocation = tc.O;\n\n        // Safety Policy Rule 1: Abort if declared length exceeds the cap.\n        // This check happens on reading the header, before any payload is processed.\n        if (tc.L > tc.C) {\n            // No message buffer is allocated. The peak allocation remains the overhead.\n            // The simulation for this message ends here.\n        } else {\n            // Safety Policy Rule 2: Incrementally grow buffer for L <= C.\n            unsigned long long bytes_received = 0;\n            \n            // Loop while payload is arriving AND message is not yet complete.\n            while (bytes_received < tc.P && bytes_received < tc.L) {\n                // Determine the size of the next chunk.\n                unsigned long long chunk_size = tc.s;\n                if (tc.P - bytes_received < chunk_size) {\n                    chunk_size = tc.P - bytes_received;\n                }\n                \n                bytes_received += chunk_size;\n\n                // The buffer size is the minimum of what's received and the cap.\n                unsigned long long buffer_size = bytes_received;\n                if (buffer_size > tc.C) {\n                    buffer_size = tc.C;\n                }\n                \n                // Calculate current total allocation.\n                unsigned long long current_allocation = tc.O + buffer_size;\n                \n                // Update the peak allocation if the current is greater.\n                if (current_allocation > peak_allocation) {\n                    peak_allocation = current_allocation;\n                }\n            }\n        }\n        \n        // Final Check: Does the peak allocation exceed the budget?\n        // Result is 1 if safe (<= M), 0 otherwise.\n        if (peak_allocation <= tc.M) {\n            results[i] = 1;\n        } else {\n            results[i] = 0;\n        }\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement.\n    // Format: [r1,r2,r3,...]\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"%d\", results[i]);\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\\n\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "RPC client libraries are not standalone entities; they must integrate safely within the complex environment of a modern operating system. This problem delves into a classic and subtle systems programming challenge: the interaction between a multi-threaded RPC client and the `fork()` system call. By analyzing the potential for deadlocks and data corruption, you will learn why naively using an RPC client in a forked child process is hazardous and discover the established patterns for safely creating new processes from a concurrent application.",
            "id": "3677100",
            "problem": "A process in a Portable Operating System Interface (POSIX) compliant operating system uses a Remote Procedure Call (RPC) client library to communicate with a server over Transmission Control Protocol (TCP). The library maintains $N$ persistent connections, a background heartbeat thread, and per-connection mutex-protected send queues. At time $t_0$, the process is multi-threaded: the main thread is executing application logic, a heartbeat thread is periodically sending health checks, and worker threads are issuing RPCs concurrently. At some moment $t_1 > t_0$, the main thread calls the system call `fork()`. Immediately after the call returns, the child process attempts to reuse the inherited RPC client object for further RPCs, while the parent continues running and using the same library.\n\nFrom first principles, use the following foundational base to reason about what is and is not safe after `fork()` in a multi-threaded process:\n- By definition, `fork()` creates a child process that is an almost-exact copy of the parent’s address space at the instant of the call, but only the calling thread is present in the child; other threads from the parent do not exist in the child.\n- By well-tested operating system facts, open file descriptors (FDs) are duplicated such that both parent and child refer to the same underlying open file description in the kernel; for sockets, this means the same TCP connection state is shared. The reference count for the open file description is incremented; kernel-side state such as sequence numbers, send buffers, and protocol control blocks are shared.\n- In a multi-threaded process, immediately after `fork()` and before any `exec()`, only functions classified as asynchronous-signal-safe are guaranteed to be safe to call; functions that acquire user-space locks or allocate memory are not guaranteed safe.\n- The close-on-exec flag (`FD_CLOEXEC`) causes the kernel to close an FD on `exec()`, not on `fork()`.\n- The POSIX function `pthread_atfork()` registers three handlers: a prepare handler called in the parent before `fork()`, a parent handler called in the parent after `fork()`, and a child handler called in the child after `fork()`; these can be used to coordinate lock states across `fork()`.\n\nAssume the RPC library uses non-async-signal-safe operations internally (for example, it uses `malloc`, mutexes, and background threads), and the TCP-based RPC protocol relies on application-level message framing atop a stream with no inherent message boundaries.\n\nWhich of the following recommendations or claims are correct under these constraints? Select all that apply.\n\nA. In a multi-threaded parent, it is safe to perform RPC operations in the child immediately after `fork()` as long as the child does not touch shared memory not owned by the calling thread.\n\nB. A safe pattern is: in the child, either call `exec()` immediately (doing no RPC), or, if the child must perform RPC without `exec()`, first close all inherited RPC-related FDs and reinitialize a fresh RPC client state before issuing any RPC calls.\n\nC. Marking the RPC sockets with `FD_CLOEXEC` ensures they are closed in the child at `fork()`, preventing any risk from inherited connections.\n\nD. Sharing an established TCP connection between parent and child is safe because each process has independent sequence number spaces on the socket, so writes from the two processes cannot interleave in a way that breaks the RPC protocol.\n\nE. Using `pthread_atfork()` to quiesce the library before `fork()` (for example, stopping background threads and acquiring all mutexes), and then resetting internal locks and rebuilding RPC client state in the child before any RPC use, is a safe mitigation strategy.\n\nF. When launching new helper processes from an RPC-using, multi-threaded parent, replacing `fork()`+`exec()` with `posix_spawn()` avoids post-`fork()` hazards and is recommended as a safer approach.\n\nChoose the correct option(s).",
            "solution": "The problem statement describes a classic and critical issue in POSIX systems programming: the interaction between multi-threading and the `fork()` system call. We must validate the problem setup before proceeding to a solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem statement provides the following definitions, conditions, and assumptions, which constitute a \"foundational base\":\n*   A process uses a Remote Procedure Call (RPC) client library over Transmission Control Protocol (TCP).\n*   The library maintains `$N$` persistent connections, a background heartbeat thread, and per-connection mutex-protected send queues.\n*   At time `$t_0$`, the process is multi-threaded (main thread, heartbeat thread, worker threads).\n*   At time `$t_1 > t_0$`, the main thread calls `fork()`.\n*   The child process attempts to reuse the inherited RPC client object. The parent process also continues running and using the library.\n*   `fork()` creates a copy of the parent's address space but only replicates the calling thread in the child.\n*   Open file descriptors (FDs) are duplicated, pointing to the same underlying kernel open file description (e.g., a shared TCP connection state, including sequence numbers and buffers).\n*   In a multi-threaded process, only async-signal-safe functions are safe to call in the child after `fork()` and before `exec()`. Functions using user-space locks (like mutexes) or memory allocation (`malloc()`) are not safe.\n*   The `FD_CLOEXEC` flag causes an FD to be closed on `exec()`, not on `fork()`.\n*   The `pthread_atfork()` function registers handlers to manage state (e.g., locks) across a `fork()` call.\n*   The RPC library is assumed to use non-async-signal-safe operations.\n*   The RPC protocol uses application-level message framing over a TCP stream.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is evaluated against the validation criteria:\n\n*   **Scientifically Grounded**: The premises are firmly based on the well-documented behavior of the POSIX `fork()` system call in the context of multi-threading and I/O. The described hazards—deadlocks from inherited mutexes and data corruption from shared file descriptors—are real, well-known problems in systems programming. The description of TCP connection sharing and async-signal safety is accurate.\n*   **Well-Posed**: The problem is clearly structured. It presents a scenario and asks for an evaluation of proposed solutions and claims based on a specified set of first principles. A definite set of correct and incorrect statements can be derived.\n*   **Objective**: The language is precise and technical. It avoids subjectivity and relies on standard operating system terminology and concepts (`fork()`, `exec()`, mutex, file descriptor).\n\nThe problem does not exhibit any invalidating flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, or trivial. It is a standard, albeit complex, computer science problem.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. We may proceed to the solution.\n\n### Derivation of Solution\n\nThe core of the problem lies in two distinct hazards that arise when `fork()` is called in a multi-threaded process that manages resources like mutexes and network connections.\n\n1.  **Internal State Inconsistency (Mutex Deadlock)**: The RPC library uses mutexes. If a thread other than the one calling `fork()` holds a mutex at the moment of the call, the child process inherits the memory state where that mutex is locked. However, the thread that held the lock does not exist in the child. Therefore, the child process's single thread has no way to release the lock. Any attempt by the child to acquire this same mutex will result in an irrecoverable deadlock. The problem states the library uses \"per-connection mutex-protected send queues,\" making this a near-certainty in a concurrent application.\n\n2.  **External Resource Contention (Shared TCP Connection)**: The problem correctly states that `fork()` duplicates file descriptors such that they point to the same underlying kernel object. For a TCP socket, this means both the parent and child processes are now writing to and reading from the *same* TCP stream. The kernel will serialize access to its send/receive buffers, but from the perspective of the application-level RPC protocol, this is disastrous. An RPC message sent by the parent might be interleaved with an RPC message from the child. The server would receive a garbled stream of data that violates the message framing, leading to communication failure and protocol errors. The claim is that the protocol relies on \"application-level message framing,\" which is acutely vulnerable to such interleaving.\n\nA safe use of the RPC library in the child process must mitigate both of these hazards. The child must either not use the library at all (and typically call `exec()`), or it must completely reset its state to a clean, known-good configuration, independent of the parent.\n\n### Option-by-Option Analysis\n\n**A. In a multi-threaded parent, it is safe to perform RPC operations in the child immediately after `fork()` as long as the child does not touch shared memory not owned by the calling thread.**\n\nThis statement is fundamentally incorrect. The primary danger is not related to \"shared memory\" in the sense of `shmget()`, but to the state of the process's *entire duplicated address space*. The child inherits a copy of the mutexes. If any mutex was locked by another thread in the parent, the child will deadlock when it tries to use the RPC library. Furthermore, the RPC operations would use the inherited socket FDs, leading to the TCP stream corruption described above. The condition \"as long as the child does not touch shared memory not owned by the calling thread\" is ill-defined and does not address the actual risks.\n\n**Verdict: Incorrect.**\n\n**B. A safe pattern is: in the child, either call `exec()` immediately (doing no RPC), or, if the child must perform RPC without `exec()`, first close all inherited RPC-related FDs and reinitialize a fresh RPC client state before issuing any RPC calls.**\n\nThis recommendation describes two valid patterns.\n1.  `fork()` followed immediately by `exec()`: This is the canonical safe way to create a new process. The `exec()` call replaces the child's problematic memory image (including any locked mutexes) with a fresh one from an executable file. This completely avoids the internal state inconsistency problem.\n2.  Child performs RPC without `exec()`: The prescribed actions are to (i) close inherited FDs and (ii) reinitialize the RPC client. Closing the FDs correctly severs the connection to the shared kernel TCP state, preventing stream corruption. Reinitializing the client from scratch (e.g., calling its `init()` function) would create new, unlocked mutexes, allocate fresh memory, and establish new, independent network connections. This avoids both the inherited locked-mutex problem and the shared-connection problem. This is the correct \"manual cleanup\" procedure.\n\n**Verdict: Correct.**\n\n**C. Marking the RPC sockets with `FD_CLOEXEC` ensures they are closed in the child at `fork()`, preventing any risk from inherited connections.**\n\nThis statement is factually wrong. As explicitly stated in the problem's foundational base, \"The close-on-exec flag (`FD_CLOEXEC`) causes the kernel to close an FD on `exec()`, not on `fork()`.\" Therefore, setting this flag has no effect on the child process immediately after `fork()` returns. The file descriptors remain open and shared with the parent, and the risk of stream corruption persists.\n\n**Verdict: Incorrect.**\n\n**D. Sharing an established TCP connection between parent and child is safe because each process has independent sequence number spaces on the socket, so writes from the two processes cannot interleave in a way that breaks the RPC protocol.**\n\nThis claim is directly contradicted by the provided foundational base, which correctly states that parent and child \"refer to the same underlying open file description in the kernel; for sockets, this means the same TCP connection state is shared... kernel-side state such as sequence numbers, send buffers, and protocol control blocks are shared.\" There is only one TCP connection and one set of sequence numbers. Writes from both processes are sent over this single connection. The non-deterministic interleaving of these writes will corrupt the application-level message stream.\n\n**Verdict: Incorrect.**\n\n**E. Using `pthread_atfork()` to quiesce the library before `fork()` (for example, stopping background threads and acquiring all mutexes), and then resetting internal locks and rebuilding RPC client state in the child before any RPC use, is a safe mitigation strategy.**\n\nThis describes the canonical, library-centric solution to the `fork()` safety problem. The `pthread_atfork()` mechanism is designed for precisely this purpose.\n*   The `prepare` handler, called in the parent before `fork()`, acquires all necessary locks. This ensures the process enters `fork()` in a known state.\n*   The `parent` handler, called in the parent after `fork()` returns, releases the locks, allowing the parent to resume normal operation.\n*   The `child` handler, called in the child after `fork()` returns, inherits the locks acquired by the `prepare` handler. Since the child is single-threaded, its main thread is the one that \"holds\" these locks, so it can safely release them. The handler would then proceed to re-establish a valid state for the child, which might involve restarting background threads (which were not inherited) and cleaning up other state. This is a robust and correct mitigation strategy.\n\n**Verdict: Correct.**\n\n**F. When launching new helper processes from an RPC-using, multi-threaded parent, replacing `fork()`+`exec()` with `posix_spawn()` avoids post-`fork()` hazards and is recommended as a safer approach.**\n\nThe `posix_spawn()` function family was introduced to the POSIX standard to provide a more efficient and safer way to create processes, especially from multi-threaded callers. Unlike `fork()`, which creates a vulnerable window where arbitrary code can run in the child before `exec()`, `posix_spawn()` is designed as a higher-level abstraction that combines process creation and program execution. Implementations are free to use mechanisms like `vfork()` to avoid duplicating the entire parent address space. By mapping directly from the parent to the new executable image, it bypasses the execution of any intermediate code in the child that could trigger deadlocks or use inconsistent data. Thus, it effectively prevents the post-`fork()` hazards associated with using a non-fork-safe library.\n\n**Verdict: Correct.**",
            "answer": "$$\\boxed{BEF}$$"
        }
    ]
}