{
    "hands_on_practices": [
        {
            "introduction": "High-performance locks often pack state, such as flags and counters, into a single atomic integer to save space and enable efficient updates. This practice exercise explores the hidden dangers of this technique, specifically how a fixed-width reader count can overflow. By working through this problem , you will see how a simple integer wrap-around can catastrophically violate the lock's fundamental safety guarantees, allowing a writer to enter a critical section while readers are still active.",
            "id": "3675673",
            "problem": "A reader-writer lock stores its state in a fixed-width unsigned integer with standard two’s-complement modulo arithmetic semantics for unsigned types: all additions and subtractions on a $w$-bit unsigned integer are performed modulo $2^{w}$. The design uses the high-order $f$ bits for flags and the remaining low-order $w-f$ bits for the reader count. The reader count is incremented atomically by each acquiring reader and decremented atomically by each releasing reader. A writer acquires exclusivity only when the reader count field is equal to $0$.\n\nAssume the following implementation choices and constraints:\n- The integer width is $w=16$ bits (that is, a $uint16$).\n- The number of flag bits is $f=2$ (the two most significant bits hold a write-locked and a writer-waiting indicator).\n- All arithmetic on the reader count field follows the induced modulo $2^{w-f}$ semantics, because it is a subfield of a $w$-bit unsigned integer.\n\nTasks:\n1. Starting from the definition of modulo arithmetic on fixed-width unsigned integers and the invariants of a reader-writer lock, explain why a reader-count “saturation” at the maximum representable value followed by one more reader acquire will wrap the reader count field to $0$, and why this can violate writer safety if a writer is waiting.\n2. Under the above $w$ and $f$, derive the exact maximum number of concurrent readers $N_{\\max}$ that can be represented without ambiguity in the reader count field, and identify the next increment that would trigger the wrap-around.\n3. Suppose the system architect can guarantee an absolute concurrency bound of $B=1\\times 10^{6}$ simultaneous readers in the worst case, while retaining the same $f=2$ flag bits. Determine the minimal integer width $w_{\\min}$ (in bits) such that the reader count field can represent all values from $0$ through $B$ without wrap-around. Express your final answer as the single integer $w_{\\min}$ in bits.\n\nIn addition, propose one robust type choice and one concrete run-time guard that would prevent the overflow in practice under these constraints. These proposals will be graded qualitatively and do not affect the required numeric answer.\n\nReport only the value of $w_{\\min}$ for grading. No rounding is required; provide an exact integer in bits.",
            "solution": "The problem statement is analyzed to be valid. It is scientifically grounded in the principles of computer architecture and operating systems, specifically concerning integer representation and concurrency control mechanisms. The problem is well-posed, with all necessary parameters and constraints defined to derive a unique, logical solution. The language is objective and precise.\n\nLet the state of the reader-writer lock be stored in a $w$-bit unsigned integer, denoted by a variable $S$. The range of values for $S$ is $[0, 2^w - 1]$. All arithmetic on $S$ is performed modulo $2^w$.\nThe integer $S$ is partitioned into two fields:\n1. The most significant $f$ bits are used for flags.\n2. The least significant $c = w-f$ bits are used for the reader count, denoted by $R$.\n\nThe numerical value of the reader count $R$ can be extracted from $S$ using a bitmask or modulo arithmetic: $R = S \\pmod{2^c}$. A reader acquiring the lock corresponds to an atomic increment of $S$, i.e., $S \\leftarrow S+1$. A writer can acquire the lock only when the reader count $R$ is $0$.\n\n**1. Analysis of Reader-Count Overflow and Writer Safety Violation**\n\nThe reader count field has $c = w-f$ bits. As a $c$-bit unsigned integer field, it can represent values from $0$ to $2^c - 1$. The state of having $N$ readers is represented by the reader count field having the value $N$.\n\nLet us consider the case where the reader count is at its maximum representable value, which is $R_{\\max} = 2^c - 1$. This corresponds to $2^c - 1$ readers concurrently holding the lock. Assume for simplicity that no flags are set, so the state variable $S$ has the value $S = 2^c - 1$.\n\nWhen one more reader attempts to acquire the lock, the state variable $S$ is atomically incremented:\n$$S_{new} = S_{old} + 1 = (2^c - 1) + 1 = 2^c$$\nThe new reader count $R_{new}$ is found by taking this new state value modulo $2^c$:\n$$R_{new} = S_{new} \\pmod{2^c} = 2^c \\pmod{2^c} = 0$$\nThis is a wrap-around, or an integer overflow, within the reader count field. The state of the lock now indicates a reader count of $0$. However, in reality, there are $2^c$ readers holding the lock.\n\nIf a writer thread is waiting to acquire the lock, it continuously checks the reader count. Upon observing that the reader count is $R_{new} = 0$, the writer's acquisition condition is met. The writer proceeds to acquire the lock and modify the shared resource. This is a critical violation of writer safety, which is a core invariant of the reader-writer lock protocol. The protocol requires that a writer has exclusive access. In this overflow scenario, a writer is granted access while $2^c$ readers are simultaneously accessing the resource, leading to a race condition and potential data corruption.\n\n**2. Maximum Concurrent Readers for $w=16$ and $f=2$**\n\nGiven the integer width $w=16$ bits and the number of flag bits $f=2$, the number of bits available for the reader count is:\n$$c = w - f = 16 - 2 = 14 \\text{ bits}$$\nA $14$-bit field can represent unsigned integer values in the range $[0, 2^{14}-1]$. The value $0$ represents zero readers. Each integer value $N$ in this range represents $N$ concurrent readers. Therefore, the maximum number of concurrent readers $N_{\\max}$ that can be represented without ambiguity is the maximum value the field can hold.\n$$N_{\\max} = 2^{14} - 1 = 16384 - 1 = 16383$$\nThe next increment, corresponding to the $(N_{\\max}+1)$-th reader acquisition, would attempt to set the reader count to $16383+1=16384$. Since $16384 = 2^{14}$, this value represented in a $14$-bit field is $0$, triggering the wrap-around described in the first part.\n\n**3. Minimal Integer Width for a Concurrency Bound of $B=1 \\times 10^6$**\n\nThe system must support an absolute concurrency bound of $B=1 \\times 10^6$ readers. This means the reader count field must be able to represent all integer values from $0$ to $B$ inclusive. Let the minimal required integer width be $w_{\\min}$ and the number of bits for the reader count be $c_{\\min}$. We are given that the number of flag bits remains $f=2$. Thus, $c_{\\min} = w_{\\min} - f = w_{\\min} - 2$.\n\nA $c_{\\min}$-bit field can represent $2^{c_{\\min}}$ distinct values, from $0$ to $2^{c_{\\min}}-1$. To accommodate up to $B$ readers, the maximum value representable by the field must be at least $B$.\n$$2^{c_{\\min}} - 1 \\ge B$$\nSubstituting $B = 1 \\times 10^6$:\n$$2^{c_{\\min}} - 1 \\ge 10^6$$\n$$2^{c_{\\min}} \\ge 10^6 + 1$$\nTo find the minimum integer $c_{\\min}$ that satisfies this inequality, we use the base-$2$ logarithm:\n$$c_{\\min} \\ge \\log_2(10^6 + 1)$$\nSince $c_{\\min}$ must be an integer, we take the ceiling of the result:\n$$c_{\\min} = \\lceil \\log_2(1000001) \\rceil$$\nWe can compute the logarithm:\n$$\\log_2(1000001) = \\frac{\\ln(1000001)}{\\ln(2)} \\approx \\frac{13.8155105}{0.6931472} \\approx 19.931568$$\nThe ceiling of this value is:\n$$c_{\\min} = \\lceil 19.931568 \\rceil = 20 \\text{ bits}$$\nSo, a minimum of $20$ bits are required for the reader count field.\nLet's verify this: a $20$-bit field can store values up to $2^{20}-1 = 1048576 - 1 = 1048575$. This value is indeed greater than or equal to $10^6$. A $19$-bit field could only store up to $2^{19}-1 = 524287$, which is insufficient.\nThe minimal total integer width $w_{\\min}$ is the sum of the bits for the reader count and the flags:\n$$w_{\\min} = c_{\\min} + f = 20 + 2 = 22 \\text{ bits}$$\n\n**Proposals to Prevent Overflow**\n\nAs requested, here are two proposals to prevent reader count overflow in practice.\n\n1.  **Robust Type Choice**: The fundamental problem is the entanglement of the reader count and flags within a single integer of fixed, and potentially insufficient, width. A more robust design would be to separate these concerns using a `struct` or `class`.\n    ```c++\n    struct SafeRWLockState {\n        std::atomic<uint_fast16_t> flags;      // e.g., for 2 flags, a 16-bit type is ample.\n        std::atomic<size_t>        reader_count; // size_t is large enough for any practical count.\n    };\n    ```\n    By using `std::atomic<size_t>`, the reader count is stored in an integer type that is guaranteed by the C++ standard to be large enough to hold the size of any object, which on a $64$-bit system is a $64$-bit integer. This provides a capacity of $2^{64}-1$ readers, effectively eliminating overflow as a practical concern. Operations on flags and the reader count would be performed on distinct atomic variables, preventing overflow in one from corrupting the other.\n\n2.  **Concrete Run-Time Guard**: If the packed-integer design must be retained, a run-time guard is essential. Before incrementing the reader count, the operation must check if the count is already at a saturation point. This is correctly implemented using a compare-and-swap (CAS) loop. A safe saturation limit should be chosen, for instance, `MAX_READERS = (1U << c) - 2`, to leave a buffer.\n    ```c++\n    // Pseudocode for a safe reader acquire on a w-bit state variable `lock_state`\n    const unsigned int READER_INC = 1;\n    const unsigned int READER_MASK = (1U << (w-f)) - 1;\n    const unsigned int SATURATION_LIMIT = READER_MASK - 1; // A safe upper bound\n\n    unsigned int old_state = lock_state.load(std::memory_order_relaxed);\n    while (true) {\n        if ((old_state & WRITER_BITS_MASK) != 0) {\n            // Writer is active or waiting, spin or wait.\n            old_state = lock_state.load(std::memory_order_relaxed);\n            continue;\n        }\n        if ((old_state & READER_MASK) >= SATURATION_LIMIT) {\n            return ACQUIRE_FAILED_TOO_MANY_READERS; // Fail the acquisition\n        }\n        // Attempt to atomically increment the reader count\n        if (lock_state.compare_exchange_weak(old_state, old_state + READER_INC)) {\n            return ACQUIRE_SUCCESS; // Success\n        }\n        // CAS failed, old_state was updated, loop will retry\n    }\n    ```\n    This guard actively prevents the count from reaching the value that would wrap around to zero, thereby preserving writer safety.\n\nThe question requires reporting only the value of $w_{\\min}$ for grading.\n$$w_{\\min} = 22$$",
            "answer": "$$\\boxed{22}$$"
        },
        {
            "introduction": "A lock that is logically correct can still exhibit poor performance on modern multicore processors due to the subtle effects of the cache coherence protocol. This problem delves into the common performance pitfall of \"false sharing,\" which occurs when unrelated data items on the same cache line cause excessive and unnecessary cache invalidations. You will quantify the performance cost of a naive reader-count implementation and analyze the dramatic improvement gained by using padded, per-core counters .",
            "id": "3675750",
            "problem": "You are analyzing a reader-preference reader-writer lock on a symmetric multiprocessor with $N$ identical cores, private level-one caches, and a directory-based cache coherence protocol that implements the Modified, Exclusive, Shared, Invalid (MESI) states. A read acquire increments a reader count to indicate that a reader is active, and a read release decrements it. Assume atomic read-modify-write operations (e.g., fetch-add) are used for correctness.\n\nA production implementation stores a single shared reader count variable $r$ on a cache line that also contains an unrelated, frequently read field $g$ (for example, a writer-pending hint). Many cores performing read acquires concurrently update $r$. Writers rarely modify $g$, but all cores read $g$ frequently to decide on fast-path behavior, causing many caches to hold the $r\\text{-}g$ line in Shared state. This causes false sharing with respect to $g$, because writes to $r$ invalidate the cache line containing $g$ in other cores even though $g$ is not being written.\n\nYou are asked to (i) articulate how padding/alignment prevents this false sharing and (ii) quantify how much padding and per-core sharding reduce coherence writes on the read-acquire path under a simple probabilistic model:\n\n- Use the following model for read acquires: one acquire occurs at a time, and the core issuing the next acquire is independently and uniformly distributed over the $N$ cores. Ignore capacity and conflict evictions, and assume that between consecutive read acquires the cache line written by the last acquirer remains in Modified state on that acquirer’s core.\n- Define a “coherence write” on the acquire path as a Read For Ownership (RFO)-like ownership transfer of a cache line under MESI that occurs when a core performs the atomic increment and does not already hold the line in Modified state.\n- Baseline design: a single shared $r$ (placed with $g$ on the same line). Under the model above, derive the expected number of coherence writes per read acquire as a function of $N$.\n- Improved design: use a per-core array $\\{r_i\\}_{i=1}^{N}$ where core $i$ increments only $r_i$, and each $r_i$ is placed alone on its own cache line by padding to the cache line size $B$ and aligning each $r_i$ to a $B$-byte boundary, so that no two $r_i$ share a cache line. Under the same model, derive the expected number of coherence writes per read acquire.\n- Compute the expected reduction in coherence writes per acquire (baseline minus improved), simplified to a single closed-form expression in $N$.\n\nYour final answer must be a single simplified analytic expression. Do not include units. Do not approximate; no rounding is required.",
            "solution": "The problem requires an analysis of two implementation strategies for a reader count variable in a reader-writer lock, focusing on the performance implications related to cache coherence. The analysis consists of two parts: a qualitative explanation of how memory layout can mitigate false sharing, and a quantitative derivation of the reduction in coherence-related writes achieved by an improved design.\n\nFirst, we address the qualitative part of the problem. The baseline design places a shared reader-count variable, $r$, on the same cache line as an unrelated but frequently read field, $g$. A cache coherence protocol like MESI (Modified, Exclusive, Shared, Invalid) maintains consistency at the granularity of a cache line. When multiple cores perform read acquires, they must atomically increment $r$. An atomic increment is a read-modify-write operation. To perform the write, a core must gain exclusive ownership of the cache line, placing it in the Modified (M) state. This action forces the coherence protocol to send invalidation messages to all other cores that hold a copy of that line. In the described scenario, many cores hold a copy of the line in the Shared (S) state because they frequently read the field $g$. A write to $r$ by a single core will thus invalidate the line in all these other cores. This phenomenon is known as false sharing: the invalidation is triggered by a write to a variable ($r$) that the other cores were not using, but it forces them to discard a variable ($g$) they were using because both reside on the same cache line. On their next access to $g$, these cores will experience a cache miss, leading to performance degradation.\n\nPadding and alignment provide a solution. The improved design utilizes a per-core array of counters, $\\{r_i\\}_{i=1}^{N}$. By padding each counter $r_i$ to the size of a cache line, $B$, and aligning its starting address to a $B$-byte boundary, we ensure that each $r_i$ resides on its own unique cache line. For instance, $r_i$ would occupy the entirety of line $L_i$, and $r_{j}$ (for $j \\neq i$) would occupy a different line $L_j$. Consequently, when core $i$ writes to its dedicated counter $r_i$, it only needs to acquire ownership of line $L_i$. This action has no effect on the cache lines containing other counters like $r_j$ or the field $g$ (which would now also be on its own line). This spatial separation of data at the cache-line level completely eliminates the false sharing problem between the counters, and between the counters and the field $g$.\n\nNext, we proceed with the quantitative analysis under the specified probabilistic model. The model states that read acquires occur sequentially, and the core performing each acquire is chosen independently and uniformly at random from the $N$ available cores. A \"coherence write\" is defined as an ownership transfer (e.g., a Read For Ownership request) that occurs when the acquiring core does not already hold the relevant cache line in the Modified (M) state.\n\nLet $E_B$ be the expected number of coherence writes per acquire for the baseline design. In this design, there is a single shared counter $r$ on one cache line, let's call it $L_r$. Let the core performing the $(k-1)$-th acquire be $C_{k-1}$. According to the model, after this operation, line $L_r$ remains in the M state in the cache of core $C_{k-1}$ and is Invalid in all other cores. The $k$-th acquire is performed by core $C_k$. To increment $r$, core $C_k$ must have line $L_r$ in M state. A coherence write will occur if and only if core $C_k$ is not the same as core $C_{k-1}$, as only $C_{k-1}$ holds the line in M state.\nThe probability that the same core performs two consecutive acquires is $P(C_k = C_{k-1})$. Since the choice of core is independent and uniform, the probability that the $k$-th acquiring core is any specific core $j$ is $P(C_k = j) = \\frac{1}{N}$. The event $C_k = C_{k-1}$ means that for some core $j$, both $C_k=j$ and $C_{k-1}=j$. The probability of this is:\n$$ P(C_k = C_{k-1}) = \\sum_{j=1}^{N} P(C_k=j \\text{ and } C_{k-1}=j) = \\sum_{j=1}^{N} P(C_k=j)P(C_{k-1}=j) = \\sum_{j=1}^{N} \\left(\\frac{1}{N}\\right)\\left(\\frac{1}{N}\\right) = N \\cdot \\frac{1}{N^2} = \\frac{1}{N} $$\nA coherence write occurs if $C_k \\neq C_{k-1}$. The probability of this event is:\n$$ P(C_k \\neq C_{k-1}) = 1 - P(C_k = C_{k-1}) = 1 - \\frac{1}{N} = \\frac{N-1}{N} $$\nThe expected number of coherence writes is the probability of an event that causes one write. Thus,\n$$ E_B = 1 \\cdot P(C_k \\neq C_{k-1}) + 0 \\cdot P(C_k = C_{k-1}) = \\frac{N-1}{N} $$\n\nNow, let $E_I$ be the expected number of coherence writes per acquire for the improved design. Here, each core $i$ writes exclusively to its own counter $r_i$ on a dedicated cache line $L_i$. Other cores never read or write to $r_i$. The problem assumption that we can \"ignore capacity and conflict evictions\" is critical. The first time a core, say core $j$, performs a read acquire, it must write to $r_j$. It does not hold line $L_j$ in M state, so a coherence write occurs (a cold miss). This brings line $L_j$ into the M state in core $j$'s cache. Because no other core ever accesses line $L_j$ and there are no evictions, line $L_j$ will remain in the M state in core $j$'s cache indefinitely. For all subsequent read acquires by core $j$, it will find line $L_j$ already in M state, and no coherence write will be necessary.\nThe problem asks for the expected number of coherence writes *per read acquire*, which typically implies a long-term, steady-state average. In this steady state, we assume the system has run long enough for each of the $N$ cores to have performed at least one acquire. After this initial warm-up phase, every core $j$ holds its respective line $L_j$ in M state. Therefore, any subsequent acquire by any core will not generate a coherence write. The number of initial coherence writes is $N$ (one for each core). Over a long sequence of $K$ acquires, where $K \\gg N$, the average number of coherence writes per acquire is $\\frac{N}{K}$, which approaches $0$ as $K \\to \\infty$. So, the steady-state expected number of coherence writes is zero.\n$$ E_I = 0 $$\n\nFinally, we compute the expected reduction in coherence writes per acquire, which is the difference between the baseline and improved designs.\n$$ \\text{Reduction} = E_B - E_I = \\frac{N-1}{N} - 0 = \\frac{N-1}{N} $$\nThis result demonstrates a significant performance improvement. For a system with many cores (large $N$), the reduction approaches $1$, meaning the improved design almost completely eliminates the coherence traffic associated with updating the reader count on the acquire path.",
            "answer": "$$\\boxed{\\frac{N-1}{N}}$$"
        },
        {
            "introduction": "Synchronization primitives do not exist in a vacuum; they interact profoundly with the operating system's scheduler. This exercise examines one of the most critical system-level interactions: priority inversion, where a high-priority thread becomes blocked by a low-priority thread for an unbounded amount of time. You will analyze how a standard reader-writer lock can trigger this hazardous condition and evaluate advanced mechanisms like priority inheritance and priority ceilings that are designed to prevent it .",
            "id": "3675645",
            "problem": "A uniprocessor operating system uses a preemptive, fixed-priority scheduler: at any instant, the runnable thread with the highest numerical priority value runs, and a higher-priority arrival preempts a lower-priority thread immediately. A reader-writer lock allows any number of concurrent readers but requires exclusive access for writers. A thread holding a lock is blocked only by the lock semantics; otherwise, if it is runnable and has the highest priority among runnable threads, it runs. Assume negligible context-switch overhead and that critical sections of interest are CPU-bound.\n\nConsider the following scenario. There are three classes of threads: a low-priority reader $R_L$ with priority $p_L$, a high-priority writer $W_H$ with priority $p_H$, and a stream of medium-priority threads $\\{M_i\\}$, each with priority $p_M$ such that $p_H \\gt p_M \\gt p_L$. At time $t=0$, $R_L$ acquires the read lock and begins a critical section of remaining CPU time $C_r \\gt 0$. At time $t=\\varepsilon$ (for some arbitrarily small $\\varepsilon \\gt 0$), $W_H$ becomes runnable and attempts to acquire the write lock, blocking because $R_L$ holds a read lock. From time $t=\\varepsilon$ onward, an arbitrary number of medium-priority threads $\\{M_i\\}$ may become runnable over time. The lock implementation admits multiple concurrent readers by default and does not have any built-in priority handling unless specified by a variant.\n\nUsing only the core definitions above (reader-writer mutual exclusion, preemptive fixed-priority scheduling, and the notion that a blocked thread cannot run), reason about priority inversion and mitigation. Which of the following statements are correct? Select all that apply.\n\nA. In the baseline implementation described (no priority-aware features), $W_H$ can suffer unbounded delay even though only $R_L$ holds the read lock at $t=\\varepsilon$, because arbitrarily many medium-priority arrivals can continuously preempt $R_L$ and prevent it from finishing its critical section.\n\nB. A variant that, upon the arrival of any blocked writer, immediately boosts the priority of every current reader-holding thread to $p_H$ and prevents admission of new readers until the writer acquires and releases the lock eliminates unbounded priority inversion. On a single core with exactly $1$ current reader at $t=\\varepsilon$ and remaining critical-section time $C_r$, the additional waiting time experienced by $W_H$ starting at $t=\\varepsilon$ is at most $C_r$.\n\nC. A variant that, upon the arrival of a blocked writer, boosts the priority of exactly one arbitrarily chosen current reader-holding thread to $p_H$ but allows new readers to continue acquiring the lock is sufficient to prevent unbounded priority inversion in general.\n\nD. A variant that, upon the arrival of a blocked writer, assigns the lock object itself a temporary priority of $p_H$ (without modifying the priorities of lock-holding threads) suffices to prevent unbounded priority inversion, because the scheduler will prefer the lock’s priority.\n\nE. A variant that implements a priority-ceiling style policy for the reader-writer lock—define a ceiling $p_C = \\max\\{p \\mid \\text{some thread may write with priority } p\\}$; when any reader acquires the lock, it temporarily executes at priority $p_C$; if any writer is pending, no new readers are admitted—eliminates unbounded priority inversion without starving the writer under the given scheduler.",
            "solution": "The user has provided a problem concerning priority inversion in the context of a preemptive, fixed-priority uniprocessor operating system using a reader-writer lock. The task is to validate the problem statement and, if valid, evaluate several statements about the system's behavior and potential mitigation strategies.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n#### Step 1: Extract Givens\n\n*   **System Model**: Uniprocessor with a preemptive, fixed-priority scheduler.\n*   **Scheduling Policy**: The runnable thread with the highest numerical priority value runs. A higher-priority arrival immediately preempts a lower-priority thread.\n*   **Synchronization Primitive**: A reader-writer lock.\n    *   Allows any number of concurrent readers.\n    *   Requires exclusive access for writers.\n    *   Baseline implementation has no built-in priority-aware features.\n*   **Thread States**: A thread holding a lock is considered blocked only by lock semantics. If it is not blocked by a lock it is trying to acquire, it is runnable.\n*   **Assumptions**: Negligible context-switch overhead; critical sections are CPU-bound.\n*   **Scenario Participants**:\n    *   One low-priority reader, $R_L$, with priority $p_L$.\n    *   One high-priority writer, $W_H$, with priority $p_H$.\n    *   A stream of medium-priority threads, $\\{M_i\\}$, each with priority $p_M$.\n*   **Priority Ordering**: $p_H > p_M > p_L$.\n*   **Timeline**:\n    *   At time $t=0$, $R_L$ acquires the read lock and enters a critical section with remaining CPU time $C_r > 0$.\n    *   At time $t=\\varepsilon$ (for an arbitrarily small $\\varepsilon > 0$), $W_H$ becomes runnable and attempts to acquire the write lock, subsequently blocking because $R_L$ holds a read lock.\n    *   For $t \\geq \\varepsilon$, an arbitrary number of medium-priority threads $\\{M_i\\}$ may become runnable.\n\n#### Step 2: Validate Using Extracted Givens\n\n*   **Scientifically Grounded**: The problem is based on fundamental and well-established concepts in operating systems theory, including preemptive priority scheduling, reader-writer locks, and priority inversion. The scenario described is a canonical example used to illustrate these concepts. It is scientifically sound.\n*   **Well-Posed**: The problem is clearly defined. The system rules (scheduler, lock), initial conditions, and subsequent events are specified, allowing for a logical analysis of the system's behavior. The question asks to evaluate the correctness of given statements based on this model.\n*   **Objective**: The language is technical and precise. Terms like \"uniprocessor,\" \"preemptive,\" \"CPU-bound,\" and the priority definitions are objective and unambiguous.\n*   **Completeness and Consistency**: The problem statement is self-contained and provides all necessary information to reason about the outcomes. The condition \"an arbitrary number of medium-priority threads\" is a necessary part of the setup to test for *unbounded* delay. There are no contradictions.\n*   **Realism**: This scenario is a classic problem in real-time systems and is entirely realistic from a computer science perspective.\n\n#### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is a well-formed problem in operating systems theory. I will proceed to derive the solution and evaluate each option.\n\n### Solution Derivation and Option Analysis\n\nThe core issue to analyze is priority inversion. This occurs when a high-priority thread is forced to wait for a low-priority thread, and critically, the duration of this wait is prolonged by the execution of unrelated medium-priority threads.\n\n**Baseline Scenario Analysis**\n\nAt time $t=0$, $R_L$ (priority $p_L$) holds the read lock and is running.\nAt time $t=\\varepsilon$, $W_H$ (priority $p_H$) arrives and tries to acquire the write lock. Since $R_L$ holds a read lock, $W_H$ becomes blocked on the lock. According to the scheduler rules, blocked threads do not run.\nNow, consider any time $t' > \\varepsilon$ when a medium-priority thread $M_i$ (priority $p_M$) becomes runnable. The set of runnable threads now contains at least $\\{R_L, M_i\\}$. Since the scheduler is preemptive and $p_M > p_L$, the scheduler will preempt $R_L$ and run $M_i$.\nThe problem states that an arbitrary number of such threads, $\\{M_i\\}$, can become runnable. This implies that the CPU could be continuously occupied by a stream of medium-priority tasks. As long as there is at least one runnable thread $M_i$, the low-priority reader $R_L$ will never be scheduled to run.\nBecause $R_L$ cannot run, it cannot complete its critical section and release the read lock. Because $R_L$ never releases the lock, the high-priority writer $W_H$ remains blocked indefinitely. This is a classic case of unbounded priority inversion, where the blocking time of $W_H$ is not determined by the critical section of $R_L$ but by the execution of an arbitrary number of $M_i$ threads.\n\n**Option-by-Option Analysis**\n\n**A. In the baseline implementation described (no priority-aware features), $W_H$ can suffer unbounded delay even though only $R_L$ holds the read lock at $t=\\varepsilon$, because arbitrarily many medium-priority arrivals can continuously preempt $R_L$ and prevent it from finishing its critical section.**\n\n*   **Analysis**: This statement accurately describes the baseline scenario analyzed above. The high-priority thread $W_H$ is blocked by the low-priority thread $R_L$. However, $R_L$ is prevented from running by the continuous preemption from medium-priority threads $\\{M_i\\}$. This leads to an unbounded waiting time for $W_H$.\n*   **Verdict**: **Correct**.\n\n**B. A variant that, upon the arrival of any blocked writer, immediately boosts the priority of every current reader-holding thread to $p_H$ and prevents admission of new readers until the writer acquires and releases the lock eliminates unbounded priority inversion. On a single core with exactly $1$ current reader at $t=\\varepsilon$ and remaining critical-section time $C_r$, the additional waiting time experienced by $W_H$ starting at $t=\\varepsilon$ is at most $C_r$.**\n\n*   **Analysis**: This describes a priority inheritance-like mechanism. At $t=\\varepsilon$, $W_H$ blocks. The variant's rules trigger:\n    1.  The priority of the sole reader, $R_L$, is boosted from $p_L$ to $p_H$.\n    2.  New readers are prevented from acquiring the lock.\n    With its priority now at $p_H$, $R_L$ cannot be preempted by any arriving medium-priority thread $M_i$ (since $p_H > p_M$). $R_L$ will run to completion. The time taken for this depends on how much of its critical section it completed before $t=\\varepsilon$. Let's say it had executed for time $\\delta \\le \\varepsilon$. Its remaining CPU time is $C_r' = C_r - \\delta$. From $t=\\varepsilon$, $R_L$ will run for $C_r'$ time, then release the lock. The waiting time for $W_H$ starting from its arrival at $t=\\varepsilon$ is therefore $C_r'$. Since $\\delta \\ge 0$, we have $C_r' \\le C_r$. The statement claims the waiting time is at most $C_r$, which is a correct upper bound. The mechanism successfully bounds the priority inversion.\n*   **Verdict**: **Correct**.\n\n**C. A variant that, upon the arrival of a blocked writer, boosts the priority of exactly one arbitrarily chosen current reader-holding thread to $p_H$ but allows new readers to continue acquiring the lock is sufficient to prevent unbounded priority inversion in general.**\n\n*   **Analysis**: This variant has two significant flaws.\n    1.  **Boosting only one reader**: Suppose at the time $W_H$ blocks, there are multiple low-priority readers, say $R_{L1}$ and $R_{L2}$. The variant boosts $R_{L1}$'s priority to $p_H$. $R_{L1}$ runs and finishes, releasing its hold on the lock. However, $R_{L2}$ still holds a read lock and remains at its low priority $p_L$. $R_{L2}$ will be preempted by any medium-priority threads $\\{M_i\\}$, and the priority inversion problem persists. $W_H$ remains blocked by $R_{L2}$.\n    2.  **Allowing new readers**: Even if there is only one reader $R_L$ initially, its priority is boosted and it finishes. However, the policy allows new readers to acquire the lock while $W_H$ is waiting. A new medium-priority reader $R_M$ could acquire the lock just before $R_L$ releases it. Now, $W_H$ is blocked by $R_M$. This can repeat, leading to writer starvation, which is a form of unbounded delay.\n    Therefore, this proposed solution is not sufficient in the general case.\n*   **Verdict**: **Incorrect**.\n\n**D. A variant that, upon the arrival of a blocked writer, assigns the lock object itself a temporary priority of $p_H$ (without modifying the priorities of lock-holding threads) suffices to prevent unbounded priority inversion, because the scheduler will prefer the lock’s priority.**\n\n*   **Analysis**: This statement is ill-defined. The problem's core scheduling rule is: \"at any instant, the runnable thread with the highest numerical priority value runs.\" Schedulers operate on threads, not on synchronization objects like locks. The statement provides no mechanism for how a \"lock's priority\" would influence the scheduling of threads. The claim \"because the scheduler will prefer the lock’s priority\" is an unsubstantiated assertion. For this variant to work, it would need to be re-specified as a rule that modifies thread priorities, e.g., \"any thread holding a lock inherits the lock's priority.\" Without this crucial detail, the statement describes a mechanism that does not integrate with the defined scheduling policy. It is fatally vague.\n*   **Verdict**: **Incorrect**.\n\n**E. A variant that implements a priority-ceiling style policy for the reader-writer lock—define a ceiling $p_C = \\max\\{p \\mid \\text{some thread may write with priority } p\\}$; when any reader acquires the lock, it temporarily executes at priority $p_C$; if any writer is pending, no new readers are admitted—eliminates unbounded priority inversion without starving the writer under the given scheduler.**\n\n*   **Analysis**: This describes a Priority Ceiling Protocol adapted for reader-writer locks. Let's trace it:\n    1.  The priority ceiling $p_C$ is set to the highest priority of any potential writer, which is $p_H$.\n    2.  At $t=0$, when $R_L$ acquires the read lock, its priority is immediately raised to $p_C = p_H$. It begins executing at this high priority.\n    3.  Because $R_L$ is running at $p_H$, it cannot be preempted by any arriving medium-priority threads $\\{M_i\\}$ (since $p_H > p_M$). Thus, priority inversion is prevented from the start.\n    4.  At $t=\\varepsilon$, $W_H$ arrives and blocks, becoming a \"pending writer\".\n    5.  The \"no new readers\" rule is now active, preventing any other readers from acquiring the lock and prolonging the wait for $W_H$. This prevents writer starvation.\n    $R_L$ continues to run at priority $p_H$, finishes its critical section, and releases the lock. Once the lock is free, $W_H$ can acquire it. The blocking time for $W_H$ is bounded by the remaining critical section time of the lock-holding reader(s), which run at the ceiling priority. This protocol correctly eliminates unbounded priority inversion and prevents writer starvation.\n*   **Verdict**: **Correct**.",
            "answer": "$$\\boxed{ABE}$$"
        }
    ]
}