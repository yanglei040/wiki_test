{
    "hands_on_practices": [
        {
            "introduction": "To effectively manage lock contention, we must first be able to predict its impact. This exercise introduces a fundamental approach from queueing theory, modeling a lock as a single-server system to derive the average wait time experienced by threads. By applying principles like Little's Law, you will develop a quantitative understanding of how arrival rates and critical section durations contribute to performance degradation. ",
            "id": "3654523",
            "problem": "An operating system kernel uses a single mutual exclusion lock to protect updates to a shared run queue. Threads arrive to attempt lock acquisition according to a Poisson process with rate $\\lambda$ arrivals per second. Upon acquiring the lock, a thread executes a critical section whose time is a random variable $S$ (the lock holding time), independent across threads and independent of the arrival process. The lock uses First-Come First-Served (FCFS) admission, and the scheduler ensures non-preemptive service while a thread holds the lock. Assume a steady-state regime in which $\\lambda \\mathbb{E}[S] < 1$.\n\nYou may use the following foundational facts: (i) Poisson Arrivals See Time Averages (PASTA) for a Poisson arrival process, (ii) Little’s Law, and (iii) the mean residual life identity for a renewal, $\\mathbb{E}[R] = \\mathbb{E}[S^{2}] / \\left(2 \\mathbb{E}[S]\\right)$, where $R$ is the residual service time observed at a random time during a busy period.\n\nIn this workload, arrivals occur at rate $\\lambda = 150{,}000$ per second. The critical section time $S$ has a bimodal distribution: with probability $0.9$, $S = 1$ microsecond, and with probability $0.1$, $S = 10$ microseconds.\n\nStarting from the above core definitions and facts, model the lock as a single-server FCFS queue and derive an expression for the mean time an arriving thread waits before acquiring the lock (excluding its own critical section time). Then evaluate this mean waiting time under the given parameters. Express your final numerical answer in microseconds and round to four significant figures.",
            "solution": "The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n### Step 1: Extract Givens\n-   **Model**: A single-server First-Come First-Served (FCFS) queue.\n-   **Arrival Process**: Poisson process with rate $\\lambda = 150{,}000$ arrivals per second.\n-   **Service Time**: A random variable $S$. Its distribution is bimodal: $P(S = 1 \\text{ µs}) = 0.9$ and $P(S = 10 \\text{ µs}) = 0.1$. The service times are independent across threads and independent of the arrival process.\n-   **Stability Condition**: The system is in a steady-state regime where $\\lambda \\mathbb{E}[S] < 1$.\n-   **Provided Foundational Facts**:\n    1.  Poisson Arrivals See Time Averages (PASTA).\n    2.  Little’s Law.\n    3.  Mean residual life identity: $\\mathbb{E}[R] = \\frac{\\mathbb{E}[S^2]}{2 \\mathbb{E}[S]}$, where $R$ is the residual service time observed at a random time during a busy period.\n-   **Objective**: Derive an expression for the mean waiting time in the queue, $\\mathbb{E}[W_q]$, and then evaluate it for the given parameters. The final answer should be in microseconds, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a single-server queue with Poisson arrivals and a general service time distribution (M/G/1 queue), a standard and well-understood model in queuing theory, frequently applied to computer systems performance analysis. The problem is scientifically grounded.\n\nThe problem provides all necessary parameters ($\\lambda$ and the distribution of $S$) and theoretical tools to derive the solution. The stability of the queue must be verified.\nThe mean service time $\\mathbb{E}[S]$ is:\n$$ \\mathbb{E}[S] = (1 \\text{ µs}) \\times 0.9 + (10 \\text{ µs}) \\times 0.1 = 0.9 \\text{ µs} + 1.0 \\text{ µs} = 1.9 \\text{ µs} = 1.9 \\times 10^{-6} \\text{ s} $$\nThe arrival rate is $\\lambda = 150{,}000 \\text{ s}^{-1}$.\nThe server utilization, $\\rho$, is given by $\\lambda \\mathbb{E}[S]$:\n$$ \\rho = (150{,}000 \\text{ s}^{-1}) \\times (1.9 \\times 10^{-6} \\text{ s}) = 1.5 \\times 10^5 \\times 1.9 \\times 10^{-6} = 0.285 $$\nSince $\\rho = 0.285 < 1$, the stability condition is met, and a steady-state solution exists. The problem is well-posed and internally consistent. It is objective and free of ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of Mean Waiting Time\nThe system is modeled as an M/G/1 queue. We aim to find the mean waiting time in the queue, denoted as $\\mathbb{E}[W_q]$. This is the average time an arriving thread spends waiting before it begins its critical section.\n\nConsider a thread arriving at the queue. The time it must wait, $W_q$, is equal to the total time required to serve all threads already present in the system. This time consists of two components:\n1.  The remaining service time of the thread currently holding the lock (if any). Let's call this random variable $R_a$.\n2.  The sum of the full service times of all threads waiting in the queue ahead of the new arrival.\n\nThus, we can write the waiting time for an arriving thread as:\n$$ W_q = R_a + \\sum_{i=1}^{N_q} S_i $$\nwhere $N_q$ is the number of threads waiting in the queue upon arrival, and $S_i$ is the service time of the $i$-th thread in the queue.\n\nBy the linearity of expectation, the mean waiting time is:\n$$ \\mathbb{E}[W_q] = \\mathbb{E}[R_a] + \\mathbb{E}\\left[\\sum_{i=1}^{N_q} S_i\\right] $$\n\nDue to the PASTA property, an arriving thread observes the system in its steady-state. The time-average number of threads in the queue is $\\mathbb{E}[N_q]$. The service times $S_i$ are independent of $N_q$. We can apply Wald's identity to the second term:\n$$ \\mathbb{E}\\left[\\sum_{i=1}^{N_q} S_i\\right] = \\mathbb{E}[S] \\mathbb{E}[N_q] $$\nBy Little's Law applied to the queue, the mean number of threads waiting is the product of the arrival rate and the mean waiting time:\n$$ \\mathbb{E}[N_q] = \\lambda \\mathbb{E}[W_q] $$\nSubstituting this into the previous equation gives:\n$$ \\mathbb{E}\\left[\\sum_{i=1}^{N_q} S_i\\right] = \\mathbb{E}[S] (\\lambda \\mathbb{E}[W_q]) = (\\lambda \\mathbb{E}[S]) \\mathbb{E}[W_q] = \\rho \\mathbb{E}[W_q] $$\n\nNow we analyze the first term, $\\mathbb{E}[R_a]$, the mean residual service time seen by an arrival. An arriving thread finds the server (the lock) busy with probability $\\rho = \\lambda \\mathbb{E}[S]$ and idle with probability $1 - \\rho$.\nIf the server is idle, the residual time is $0$. If the server is busy, the expected residual service time is given by the mean residual life identity conditional on the server being busy, $\\mathbb{E}[R] = \\frac{\\mathbb{E}[S^2]}{2 \\mathbb{E}[S]}$.\nTherefore, the unconditional expected residual time seen by an arrival is:\n$$ \\mathbb{E}[R_a] = \\rho \\cdot \\mathbb{E}[R] + (1-\\rho) \\cdot 0 = (\\lambda \\mathbb{E}[S]) \\cdot \\frac{\\mathbb{E}[S^2]}{2 \\mathbb{E}[S]} = \\frac{\\lambda \\mathbb{E}[S^2]}{2} $$\n\nCombining the expressions for the two components, we get:\n$$ \\mathbb{E}[W_q] = \\frac{\\lambda \\mathbb{E}[S^2]}{2} + \\rho \\mathbb{E}[W_q] $$\nWe can now solve for $\\mathbb{E}[W_q]$:\n$$ \\mathbb{E}[W_q] - \\rho \\mathbb{E}[W_q] = \\frac{\\lambda \\mathbb{E}[S^2]}{2} $$\n$$ \\mathbb{E}[W_q] (1 - \\rho) = \\frac{\\lambda \\mathbb{E}[S^2]}{2} $$\n$$ \\mathbb{E}[W_q] = \\frac{\\lambda \\mathbb{E}[S^2]}{2(1-\\rho)} $$\nThis is the Pollaczek-Khinchine formula for the mean waiting time.\n\n### Numerical Evaluation\nTo evaluate this expression, we first compute the necessary moments of the service time $S$. It is convenient to perform calculations with time measured in microseconds (µs).\nThe arrival rate is $\\lambda = 150{,}000 \\text{ s}^{-1} = 150{,}000 \\times 10^{-6} \\text{ µs}^{-1} = 0.15 \\text{ µs}^{-1}$.\nThe service time $S$ has the distribution $P(S=1 \\text{ µs}) = 0.9$ and $P(S=10 \\text{ µs}) = 0.1$.\n\nFirst moment (mean service time):\n$$ \\mathbb{E}[S] = (0.9 \\times 1) + (0.1 \\times 10) = 0.9 + 1.0 = 1.9 \\text{ µs} $$\n\nSecond moment of the service time:\n$$ \\mathbb{E}[S^2] = \\sum_{i} s_i^2 P(S=s_i) = (0.9 \\times 1^2) + (0.1 \\times 10^2) = (0.9 \\times 1) + (0.1 \\times 100) = 0.9 + 10 = 10.9 \\text{ µs}^2 $$\n\nServer utilization $\\rho$:\n$$ \\rho = \\lambda \\mathbb{E}[S] = (0.15 \\text{ µs}^{-1}) \\times (1.9 \\text{ µs}) = 0.285 $$\nAs confirmed during validation, the system is stable.\n\nNow, we substitute these values into the formula for $\\mathbb{E}[W_q]$:\n$$ \\mathbb{E}[W_q] = \\frac{\\lambda \\mathbb{E}[S^2]}{2(1-\\rho)} = \\frac{(0.15 \\text{ µs}^{-1}) \\times (10.9 \\text{ µs}^2)}{2(1-0.285)} $$\n$$ \\mathbb{E}[W_q] = \\frac{1.635 \\text{ µs}}{2(0.715)} = \\frac{1.635 \\text{ µs}}{1.43} $$\n$$ \\mathbb{E}[W_q] \\approx 1.14335664... \\text{ µs} $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ \\mathbb{E}[W_q] \\approx 1.143 \\text{ µs} $$\nThe mean time an arriving thread waits before acquiring the lock is approximately $1.143$ microseconds.",
            "answer": "$$\\boxed{1.143}$$"
        },
        {
            "introduction": "One of the most effective strategies to combat lock contention is to eliminate the single point of serialization altogether. This practice guides you through a comparative analysis of a traditional global lock versus a scalable per-CPU data structure design, a common pattern in modern operating systems. You will model not only the queueing delays but also the practical overheads of migration and data consolidation, revealing the complex trade-offs involved in scalable system design. ",
            "id": "3654522",
            "problem": "You are to design and implement a single, self-contained program that models and compares the average per-operation latency under two designs in a multiprocessing operating system context: a global lock protecting a single shared data structure versus per-Central Processing Unit (per-CPU) private data structures. The objective is to reason about lock contention and scalability from first principles, derive a quantitative model, and compute a latency speedup ratio. The speedup ratio for each test case is defined as the average latency under the global lock design divided by the average latency under the per-CPU design.\n\nBegin from the following foundational base, without using or assuming any shortcut formulas beyond widely accepted results and definitions:\n- Mutual exclusion serializes access to a critical section. When many threads attempt to enter a single critical section, they queue and wait, increasing average latency.\n- Little’s Law states that for a stable system, the average number in the system $L$ satisfies $L = \\lambda W$, where $\\lambda$ is the arrival rate and $W$ is the average time in the system.\n- The $M/M/1$ queue is a single-server queue with Poisson arrivals and exponential service times. It is a well-tested model for contention in a single resource. Use it to model contention for a global lock and for each per-CPU lock. Assume stability conditions hold, meaning that the arrival rate is strictly less than the service rate.\n\nModeling assumptions to be used by your implementation:\n- Global lock design: All operations across $n$ CPUs that require mutual exclusion are serialized by a single lock, modeling the critical section as an $M/M/1$ server with service rate $\\mu_g$ and total arrival rate $\\lambda = n r$, where $r$ is the per-CPU arrival rate of operations requiring the critical section. The average time in system (latency) under this model should be computed using the $M/M/1$ queue under stability.\n- Per-CPU design: Each CPU has its own private lock and data structure, modeled independently as $M/M/1$ queues, each with arrival rate $r$ and service rate $\\mu_\\ell$. In addition to queueing latency at the local lock, incorporate the expected cost of cross-CPU task migration and the amortized cost of periodic consolidation (aggregation of per-CPU data into a global view). Assume:\n  - Migration: Each operation independently incurs an additional cost $c_m$ with probability $p$. Model this as an expected additive latency of $p c_m$ per operation.\n  - Consolidation: A consolidation occurs every $\\Delta$ seconds and costs $c_s$ per CPU. Amortize this as an additive per-operation latency $\\frac{c_s}{r \\Delta}$.\n\nDefinitions of parameters to be used in your program:\n- $n$: number of CPUs (dimensionless).\n- $r$: per-CPU arrival rate of operations (in operations per second, $\\text{s}^{-1}$).\n- $c_g$: critical section service time for the global lock design (in seconds, $\\text{s}$). The service rate is $\\mu_g = \\frac{1}{c_g}$.\n- $c_\\ell$: critical section service time for the per-CPU design (in seconds, $\\text{s}$). The service rate is $\\mu_\\ell = \\frac{1}{c_\\ell}$.\n- $p$: migration probability per operation (dimensionless).\n- $c_m$: migration cost per migrating operation (in seconds, $\\text{s}$).\n- $\\Delta$: consolidation interval (in seconds, $\\text{s}$).\n- $c_s$: consolidation cost per CPU per consolidation (in seconds, $\\text{s}$).\n\nScientific realism requirements:\n- All times must be expressed in seconds ($\\text{s}$).\n- All arrival rates must be expressed in operations per second ($\\text{s}^{-1}$).\n- Use the $M/M/1$ stability conditions $n r < \\mu_g$ and $r < \\mu_\\ell$.\n\nYour task is to:\n- Derive the average per-operation latency for both designs using only the above base and modeling assumptions.\n- Implement a program that computes, for each test case, the latency speedup ratio $S$ defined as the global-lock latency divided by the per-CPU latency.\n- Round each output $S$ to six decimal places.\n\nTest suite and parameters:\nProvide results for the following five test cases, each specified by $(n, r, c_g, c_\\ell, p, c_m, \\Delta, c_s)$:\n\n- Case $1$ (happy path, moderate contention, light migration and consolidation):\n  - $n = 8$, $r = 4000$, $c_g = 1.2 \\times 10^{-5}$, $c_\\ell = 3.2 \\times 10^{-6}$, $p = 0.02$, $c_m = 5.0 \\times 10^{-6}$, $\\Delta = 0.2$, $c_s = 2.0 \\times 10^{-4}$.\n- Case $2$ (boundary condition: single CPU, no migration, no consolidation overhead):\n  - $n = 1$, $r = 4000$, $c_g = 1.0 \\times 10^{-5}$, $c_\\ell = 3.0 \\times 10^{-6}$, $p = 0.0$, $c_m = 1.0 \\times 10^{-6}$, $\\Delta = 1.0$, $c_s = 0.0$.\n- Case $3$ (near-global saturation, per-CPU remains fast):\n  - $n = 16$, $r = 1200$, $c_g = 5.0 \\times 10^{-5}$, $c_\\ell = 4.0 \\times 10^{-6}$, $p = 0.05$, $c_m = 1.0 \\times 10^{-5}$, $\\Delta = 0.05$, $c_s = 1.0 \\times 10^{-4}$.\n- Case $4$ (high migration and high consolidation overhead, many CPUs):\n  - $n = 32$, $r = 800$, $c_g = 3.0 \\times 10^{-5}$, $c_\\ell = 2.0 \\times 10^{-6}$, $p = 0.3$, $c_m = 1.0 \\times 10^{-5}$, $\\Delta = 0.02$, $c_s = 5.0 \\times 10^{-4}$.\n- Case $5$ (heavy consolidation cost but very infrequent):\n  - $n = 8$, $r = 5000$, $c_g = 1.1 \\times 10^{-5}$, $c_\\ell = 2.5 \\times 10^{-6}$, $p = 0.15$, $c_m = 2.0 \\times 10^{-6}$, $\\Delta = 5.0$, $c_s = 2.0 \\times 10^{-2}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the speedup results as a comma-separated list enclosed in square brackets, for example, $\\left[ s_1, s_2, s_3, s_4, s_5 \\right]$, where each $s_i$ is a float rounded to six decimal places. Concretely, print using the exact textual format:\n- \"[result1,result2,result3,result4,result5]\"",
            "solution": "The problem requires a quantitative comparison of two design patterns for managing shared data in a multi-CPU operating system: a single global lock versus per-CPU private data structures. The comparison metric is the speedup ratio $S$, defined as the average per-operation latency of the global lock design divided by that of the per-CPU design. The derivation will be based on first principles of queueing theory, specifically the $M/M/1$ model.\n\nFirst, we must validate the problem statement.\n\n**Step 1: Extract Givens**\n- **Designs to compare**: Global lock vs. Per-CPU private data structures.\n- **Performance Metric**: Latency speedup ratio $S = W_g / W_\\ell$, where $W_g$ is the global lock latency and $W_\\ell$ is the per-CPU latency.\n- **Modeling Framework**:\n  - Mutual exclusion serializes access, leading to queueing.\n  - Little’s Law: $L = \\lambda W$.\n  - Contention is modeled using the $M/M/1$ queue.\n  - Stability requires arrival rate $\\lambda$ to be less than service rate $\\mu$.\n- **Global Lock Model ($W_g$)**:\n  - A single $M/M/1$ server.\n  - Total arrival rate $\\lambda_g = n \\cdot r$.\n  - Service rate $\\mu_g = 1/c_g$.\n- **Per-CPU Model ($W_\\ell$)**:\n  - Each of the $n$ CPUs has an independent $M/M/1$ queue.\n  - Per-CPU arrival rate $\\lambda_\\ell = r$.\n  - Per-CPU service rate $\\mu_\\ell = 1/c_\\ell$.\n  - Additional additive latency from migration: expected cost of $p \\cdot c_m$ per operation.\n  - Additional additive latency from consolidation: amortized cost of $\\frac{c_s}{r \\cdot \\Delta}$ per operation.\n- **Parameters**:\n  - $n$: number of CPUs.\n  - $r$: per-CPU arrival rate ($\\text{s}^{-1}$).\n  - $c_g$: global lock service time ($\\text{s}$).\n  - $c_\\ell$: per-CPU lock service time ($\\text{s}$).\n  - $p$: migration probability.\n  - $c_m$: migration cost ($\\text{s}$).\n  - $\\Delta$: consolidation interval ($\\text{s}$).\n  - $c_s$: per-CPU consolidation cost ($\\text{s}$).\n- **Stability Conditions**: $n \\cdot r < \\mu_g$ and $r < \\mu_\\ell$.\n- **Test Cases**: Five sets of parameters are provided.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing the standard $M/M/1$ queueing model to analyze system performance, a common practice in computer science and engineering. The concepts of lock contention, scalability, migration overhead, and data aggregation are fundamental to operating systems design. The problem is well-posed, providing all necessary parameters and a clear objective. The language is objective and quantitative.\n\nWe must verify the stability conditions for all provided test cases. The conditions are $n \\cdot r \\cdot c_g < 1$ and $r \\cdot c_\\ell < 1$.\n1.  **Case 1**: $n \\cdot r \\cdot c_g = 8 \\cdot 4000 \\cdot 1.2 \\times 10^{-5} = 0.384 < 1$. $r \\cdot c_\\ell = 4000 \\cdot 3.2 \\times 10^{-6} = 0.0128 < 1$. Valid.\n2.  **Case 2**: $n \\cdot r \\cdot c_g = 1 \\cdot 4000 \\cdot 1.0 \\times 10^{-5} = 0.04 < 1$. $r \\cdot c_\\ell = 4000 \\cdot 3.0 \\times 10^{-6} = 0.012 < 1$. Valid.\n3.  **Case 3**: $n \\cdot r \\cdot c_g = 16 \\cdot 1200 \\cdot 5.0 \\times 10^{-5} = 0.96 < 1$. $r \\cdot c_\\ell = 1200 \\cdot 4.0 \\times 10^{-6} = 0.0048 < 1$. Valid.\n4.  **Case 4**: $n \\cdot r \\cdot c_g = 32 \\cdot 800 \\cdot 3.0 \\times 10^{-5} = 0.768 < 1$. $r \\cdot c_\\ell = 800 \\cdot 2.0 \\times 10^{-6} = 0.0016 < 1$. Valid.\n5.  **Case 5**: $n \\cdot r \\cdot c_g = 8 \\cdot 5000 \\cdot 1.1 \\times 10^{-5} = 0.44 < 1$. $r \\cdot c_\\ell = 5000 \\cdot 2.5 \\times 10^{-6} = 0.0125 < 1$. Valid.\n\nAll test cases satisfy the stability conditions. The problem is self-contained, consistent, and scientifically sound.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the derivation and solution.\n\n**Derivation of Latency Formulas**\n\nThe average time an item spends in an $M/M/1$ queueing system, denoted $W$, is a standard result from queueing theory. Given an arrival rate $\\lambda$ and a service rate $\\mu$, the average number of items in the system is $L = \\frac{\\lambda}{\\mu - \\lambda}$. By Little's Law, $L = \\lambda W$, which implies $W = \\frac{L}{\\lambda} = \\frac{1}{\\mu - \\lambda}$. This formula will be used for both the global and per-CPU queueing models.\n\n**1. Average Latency for the Global Lock Design ($W_g$)**\nThis design is modeled as a single $M/M/1$ queue.\n- The total arrival rate of operations to the single global lock is the sum of rates from all $n$ CPUs:\n  $$ \\lambda_g = n \\cdot r $$\n- The service rate $\\mu_g$ is the reciprocal of the service time $c_g$:\n  $$ \\mu_g = \\frac{1}{c_g} $$\n- Applying the $M/M/1$ latency formula, the average time per operation in the system (queueing wait time plus service time) is:\n  $$ W_g = \\frac{1}{\\mu_g - \\lambda_g} = \\frac{1}{\\frac{1}{c_g} - n r} $$\n- To avoid potential floating-point issues with subtraction of large numbers, we can rewrite this as:\n  $$ W_g = \\frac{c_g}{1 - n r c_g} $$\nThis expression represents the total average latency for an operation under the global lock design.\n\n**2. Average Latency for the Per-CPU Design ($W_\\ell$)**\nThe latency in the per-CPU design is the sum of three distinct components: the local queueing latency, the expected migration cost, and the amortized consolidation cost.\n$$ W_\\ell = W_{\\text{queue}} + W_{\\text{migration}} + W_{\\text{consolidation}} $$\n\n- **Local Queueing Latency ($W_{\\text{queue}}$)**: Each CPU has its own data structure protected by its own lock. This is modeled as an independent $M/M/1$ queue.\n  - The arrival rate to a single CPU's queue is simply $r$:\n    $$ \\lambda_\\ell = r $$\n  - The service rate is the reciprocal of the local service time $c_\\ell$:\n    $$ \\mu_\\ell = \\frac{1}{c_\\ell} $$\n  - The average latency for this local queue is:\n    $$ W_{\\text{queue}} = \\frac{1}{\\mu_\\ell - \\lambda_\\ell} = \\frac{1}{\\frac{1}{c_\\ell} - r} = \\frac{c_\\ell}{1 - r c_\\ell} $$\n\n- **Migration Cost ($W_{\\text{migration}}$)**: The problem specifies that each operation has a probability $p$ of incurring an additional migration cost $c_m$. The expected value of this cost, averaged over all operations, is an additive component to the latency.\n  $$ W_{\\text{migration}} = p \\cdot c_m $$\n\n- **Consolidation Cost ($W_{\\text{consolidation}}$)**: A consolidation process runs every $\\Delta$ seconds and imposes a cost of $c_s$ on each CPU. This cost must be amortized over the operations that occur during one interval.\n  - The number of operations processed by a single CPU in an interval of $\\Delta$ seconds is $N_{ops} = r \\cdot \\Delta$.\n  - The total cost $c_s$ is distributed over these $N_{ops}$ operations. The amortized cost per operation is therefore:\n  $$ W_{\\text{consolidation}} = \\frac{c_s}{r \\Delta} $$\n\n- **Total Per-CPU Latency ($W_\\ell$)**: Summing the three components gives the total average latency for an operation under the per-CPU design:\n  $$ W_\\ell = \\frac{c_\\ell}{1 - r c_\\ell} + p c_m + \\frac{c_s}{r \\Delta} $$\n\n**3. Latency Speedup Ratio ($S$)**\nThe speedup ratio $S$ is defined as the ratio of the global lock latency to the per-CPU latency.\n$$ S = \\frac{W_g}{W_\\ell} = \\frac{\\frac{c_g}{1 - n r c_g}}{\\frac{c_\\ell}{1 - r c_\\ell} + p c_m + \\frac{c_s}{r \\Delta}} $$\nThis is the final expression that will be implemented to compute the results for the given test cases.",
            "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n// #include <complex.h> // Not used\n// #include <threads.h> // Not used\n// #include <stdatomic.h> // Not used\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    int n;          // number of CPUs\n    double r;       // per-CPU arrival rate (ops/sec)\n    double cg;      // global lock service time (sec)\n    double cl;      // per-CPU lock service time (sec)\n    double p;       // migration probability\n    double cm;      // migration cost (sec)\n    double delta;   // consolidation interval (sec)\n    double cs;      // consolidation cost (sec)\n} TestCase;\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        // Case 1 (happy path, moderate contention)\n        {8, 4000.0, 1.2e-5, 3.2e-6, 0.02, 5.0e-6, 0.2, 2.0e-4},\n        // Case 2 (single CPU, no overheads)\n        {1, 4000.0, 1.0e-5, 3.0e-6, 0.0, 1.0e-6, 1.0, 0.0},\n        // Case 3 (near-global saturation)\n        {16, 1200.0, 5.0e-5, 4.0e-6, 0.05, 1.0e-5, 0.05, 1.0e-4},\n        // Case 4 (high migration/consolidation overhead)\n        {32, 800.0, 3.0e-5, 2.0e-6, 0.3, 1.0e-5, 0.02, 5.0e-4},\n        // Case 5 (heavy but infrequent consolidation)\n        {8, 5000.0, 1.1e-5, 2.5e-6, 0.15, 2.0e-6, 5.0, 2.0e-2}\n    };\n\n    // Calculate the number of test cases.\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    double results[num_cases];\n\n    // Calculate the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        TestCase tc = test_cases[i];\n\n        // 1. Calculate Average Latency for Global Lock Design (Wg)\n        // Wg = cg / (1 - n * r * cg)\n        double utilization_g = (double)tc.n * tc.r * tc.cg;\n        // Stability check: utilization must be < 1. This was validated manually.\n        double Wg = tc.cg / (1.0 - utilization_g);\n\n        // 2. Calculate Average Latency for Per-CPU Design (Wl)\n        // Wl is the sum of three components:\n        // W_queue + W_migration + W_consolidation\n\n        // W_queue = cl / (1 - r * cl)\n        double utilization_l = tc.r * tc.cl;\n        // Stability check: utilization must be < 1. This was validated manually.\n        double W_queue = tc.cl / (1.0 - utilization_l);\n\n        // W_migration = p * cm\n        double W_migration = tc.p * tc.cm;\n\n        // W_consolidation = cs / (r * delta)\n        // Handle case where r or delta might be zero to prevent division by zero,\n        // though problem constraints ensure they are positive.\n        double W_consolidation = 0.0;\n        if (tc.r > 0 && tc.delta > 0) {\n            W_consolidation = tc.cs / (tc.r * tc.delta);\n        }\n        \n        double Wl = W_queue + W_migration + W_consolidation;\n\n        // 3. Calculate Speedup Ratio S\n        // S = Wg / Wl\n        if (Wl > 0) {\n            results[i] = Wg / Wl;\n        } else {\n            // Handle edge case, though not expected with given test data.\n            results[i] = 0.0;\n        }\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement.\n    // Format: [result1,result2,result3,result4,result5]\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"%.6f\", results[i]);\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "When a thread must wait for a lock, the system faces a critical policy choice: should the thread spin (busy-wait) or sleep (block)? This exercise delves into the energy implications of this decision, a crucial consideration in both mobile and data center environments. By calculating a break-even point, you will uncover the core logic behind adaptive locking strategies that dynamically choose the most efficient waiting method. ",
            "id": "3654546",
            "problem": "A multicore application runs on a system where a thread waiting for a lock can either actively spin on the lock or voluntarily sleep (park) and wake later. Define the wasted energy due to lock contention as the energy consumed by threads while they are not making forward progress because they are waiting for the lock. Use only the following foundational facts: energy equals the time integral of power, and the expected value of a sum equals the sum of expected values.\n\nConsider the following simplified model and parameters:\n\n- There are $N$ identical threads, each attempting to enter the same critical section at rate $a$ attempts per second. A fraction $f$ of attempts find the lock contended and must wait. The average wait duration (from arrival to acquisition) under a given contention level is $W$.\n- While spinning, a waiting thread consumes power $P_{\\text{spin}}$ continuously for the entire wait duration.\n- While sleeping, a waiting thread consumes power $P_{\\text{sleep}}$ during the wait, and also incurs a one-time wake transition energy cost $E_{\\text{wake}}$ per wait. Ignore any performance side effects to other threads, and assume no other work is scheduled on the core during a wait.\n- Use $E=\\int P(t)\\,dt$ for energy and assume the power levels are constant during each regime as modeled above.\n\nTasks:\n\n1. Starting from $E=\\int P(t)\\,dt$, derive an expression for the expected wasted energy per second under a pure spinning policy and under a pure sleeping policy, in terms of $N$, $a$, $f$, $W$, $P_{\\text{spin}}$, $P_{\\text{sleep}}$, and $E_{\\text{wake}}$. Clearly state any intermediate expectations you use.\n\n2. Evaluate the expressions from Task $1$ for three contention levels using the parameters\n   $N=8$, $a=1000$ s$^{-1}$, $P_{\\text{spin}}=10$ W, $P_{\\text{sleep}}=1.5$ W, $E_{\\text{wake}}=5\\times 10^{-4}$ J. For each contention level, report the wasted energy per second for spinning and for sleeping in J/s.\n   - Low contention: $f=0.01$, $W=0.05$ ms.\n   - Medium contention: $f=0.20$, $W=0.50$ ms.\n   - High contention: $f=0.60$, $W=5.00$ ms.\n\n3. Using the same model, define $T^{\\star}$ to be the wait duration at which the energy consumed by spinning on a single wait equals the energy consumed by sleeping on a single wait. Derive a closed-form expression for $T^{\\star}$ in terms of $P_{\\text{spin}}$, $P_{\\text{sleep}}$, and $E_{\\text{wake}}$, and then evaluate it numerically with the parameters above. Express $T^{\\star}$ in milliseconds and round your final numerical answer to three significant figures.\n\n4. Briefly propose an energy-aware locking policy that uses your result for $T^{\\star}$ to decide dynamically whether a waiter should spin or sleep, and justify why it is energy-favorable under varying contention.\n\nOnly the numerical value of $T^{\\star}$ from Task $3$ will be graded as the final answer. Express $T^{\\star}$ in milliseconds as instructed.",
            "solution": "The problem statement has been validated and is deemed sound, well-posed, and objective. It provides a formalizable model based on established physical principles and computer science concepts. All necessary parameters are defined, and the tasks are structured to lead to a unique, meaningful solution.\n\n### Task 1: Derivation of Expected Wasted Energy Rate\n\nWe are asked to derive the expected wasted energy per second, which is the average wasted power, for two different locking policies: pure spinning and pure sleeping. Let us denote the average wasted power as $\\mathcal{P}_{\\text{wasted}}$.\n\nFirst, we determine the rate at which lock contention events occur. There are $N$ threads, each attempting to acquire the lock at a rate of $a$ attempts per second. A fraction $f$ of these attempts are contended. The total rate of contended attempts, $R_{\\text{contend}}$, across all threads is the product of these quantities:\n$$\nR_{\\text{contend}} = N \\times a \\times f\n$$\nThis rate has units of \"contended events per second\" or s$^{-1}$.\n\nNext, we determine the expected energy wasted for a single contention event. The problem defines the wasted energy, $E$, as the time integral of power, $P(t)$: $E = \\int P(t) \\, dt$. Let $T_{\\text{wait}}$ be the random variable for the duration a thread waits for a contended lock. We are given its expected value as $E[T_{\\text{wait}}] = W$.\n\n**Pure spinning policy:**\nUnder this policy, a waiting thread consumes a constant power $P_{\\text{spin}}$ for the entire wait duration $T_{\\text{wait}}$. The energy wasted in a single wait event, $E_{\\text{wait,spin}}$, is:\n$$\nE_{\\text{wait,spin}} = \\int_0^{T_{\\text{wait}}} P_{\\text{spin}} \\, dt = P_{\\text{spin}} T_{\\text{wait}}\n$$\nThe expected energy wasted per contention event is the intermediate expectation requested:\n$$\nE[E_{\\text{wait,spin}}] = E[P_{\\text{spin}} T_{\\text{wait}}] = P_{\\text{spin}} E[T_{\\text{wait}}] = P_{\\text{spin}} W\n$$\n\n**Pure sleeping policy:**\nUnder this policy, a waiting thread consumes power $P_{\\text{sleep}}$ for the duration $T_{\\text{wait}}$ and incurs a one-time energy cost $E_{\\text{wake}}$ for the wake-up transition. The energy wasted in a single wait event, $E_{\\text{wait,sleep}}$, is:\n$$\nE_{\\text{wait,sleep}} = \\left(\\int_0^{T_{\\text{wait}}} P_{\\text{sleep}} \\, dt\\right) + E_{\\text{wake}} = P_{\\text{sleep}} T_{\\text{wait}} + E_{\\text{wake}}\n$$\nThe expected energy wasted per contention event is the intermediate expectation requested:\n$$\nE[E_{\\text{wait,sleep}}] = E[P_{\\text{sleep}} T_{\\text{wait}} + E_{\\text{wake}}] = P_{\\text{sleep}} E[T_{\\text{wait}}] + E_{\\text{wake}} = P_{\\text{sleep}} W + E_{\\text{wake}}\n$$\n\nFinally, the total expected wasted energy per second (average wasted power) is the rate of contention events multiplied by the expected energy per event. This follows from the principle that the expected value of a sum is the sum of expected values, applied over a large number of events in a given time interval.\n\nFor the pure spinning policy, the expected wasted energy per second is:\n$$\n\\mathcal{P}_{\\text{wasted,spin}} = R_{\\text{contend}} \\times E[E_{\\text{wait,spin}}] = (Naf) \\times (P_{\\text{spin}} W) = NafP_{\\text{spin}}W\n$$\n\nFor the pure sleeping policy, the expected wasted energy per second is:\n$$\n\\mathcal{P}_{\\text{wasted,sleep}} = R_{\\text{contend}} \\times E[E_{\\text{wait,sleep}}] = (Naf) \\times (P_{\\text{sleep}}W + E_{\\text{wake}})\n$$\n\n### Task 2: Evaluation for Different Contention Levels\n\nWe are given the parameters: $N=8$, $a=1000$ s$^{-1}$, $P_{\\text{spin}}=10$ W, $P_{\\text{sleep}}=1.5$ W, and $E_{\\text{wake}}=5 \\times 10^{-4}$ J. We must evaluate the derived expressions for three scenarios, ensuring units are consistent. The wait duration $W$ is given in milliseconds (ms), so we convert it to seconds (s) by multiplying by $10^{-3}$. The unit for wasted energy per second is Joules per second (J/s), which is equivalent to Watts (W).\n\nThe common factor for the rate of attempts is $Na = 8 \\times 1000 \\text{ s}^{-1} = 8000 \\text{ s}^{-1}$.\n\n**Low contention:** $f=0.01$, $W=0.05 \\text{ ms} = 0.05 \\times 10^{-3}$ s.\nThe rate of contended events is $R_{\\text{contend}} = Naf = 8000 \\times 0.01 = 80$ s$^{-1}$.\n$$\n\\mathcal{P}_{\\text{wasted,spin}} = (80 \\text{ s}^{-1}) \\times (10 \\text{ J/s}) \\times (0.05 \\times 10^{-3} \\text{ s}) = 0.04 \\text{ J/s}\n$$\n$$\n\\mathcal{P}_{\\text{wasted,sleep}} = (80 \\text{ s}^{-1}) \\times \\left( (1.5 \\text{ J/s}) \\times (0.05 \\times 10^{-3} \\text{ s}) + 5 \\times 10^{-4} \\text{ J} \\right) = 80 \\times (0.075 \\times 10^{-3} + 0.5 \\times 10^{-3}) = 80 \\times (0.575 \\times 10^{-3}) = 0.046 \\text{ J/s}\n$$\n- For low contention: Spin wastes $0.04$ J/s, Sleep wastes $0.046$ J/s.\n\n**Medium contention:** $f=0.20$, $W=0.50 \\text{ ms} = 0.50 \\times 10^{-3}$ s.\nThe rate of contended events is $R_{\\text{contend}} = Naf = 8000 \\times 0.20 = 1600$ s$^{-1}$.\n$$\n\\mathcal{P}_{\\text{wasted,spin}} = (1600 \\text{ s}^{-1}) \\times (10 \\text{ J/s}) \\times (0.50 \\times 10^{-3} \\text{ s}) = 8 \\text{ J/s}\n$$\n$$\n\\mathcal{P}_{\\text{wasted,sleep}} = (1600 \\text{ s}^{-1}) \\times \\left( (1.5 \\text{ J/s}) \\times (0.50 \\times 10^{-3} \\text{ s}) + 5 \\times 10^{-4} \\text{ J} \\right) = 1600 \\times (0.75 \\times 10^{-3} + 0.5 \\times 10^{-3}) = 1600 \\times (1.25 \\times 10^{-3}) = 2 \\text{ J/s}\n$$\n- For medium contention: Spin wastes $8$ J/s, Sleep wastes $2$ J/s.\n\n**High contention:** $f=0.60$, $W=5.00 \\text{ ms} = 5.00 \\times 10^{-3}$ s.\nThe rate of contended events is $R_{\\text{contend}} = Naf = 8000 \\times 0.60 = 4800$ s$^{-1}$.\n$$\n\\mathcal{P}_{\\text{wasted,spin}} = (4800 \\text{ s}^{-1}) \\times (10 \\text{ J/s}) \\times (5.00 \\times 10^{-3} \\text{ s}) = 240 \\text{ J/s}\n$$\n$$\n\\mathcal{P}_{\\text{wasted,sleep}} = (4800 \\text{ s}^{-1}) \\times \\left( (1.5 \\text{ J/s}) \\times (5.00 \\times 10^{-3} \\text{ s}) + 5 \\times 10^{-4} \\text{ J} \\right) = 4800 \\times (7.5 \\times 10^{-3} + 0.5 \\times 10^{-3}) = 4800 \\times (8.0 \\times 10^{-3}) = 38.4 \\text{ J/s}\n$$\n- For high contention: Spin wastes $240$ J/s, Sleep wastes $38.4$ J/s.\n\n### Task 3: Derivation and Evaluation of Break-even Wait Duration $T^{\\star}$\n\nWe define $T^{\\star}$ as the wait duration for a single event where the energy consumed by spinning equals the energy consumed by sleeping. Let this duration be $T$.\nThe energy for spinning for time $T$ is $E_{\\text{spin}}(T) = P_{\\text{spin}} T$.\nThe energy for sleeping for time $T$ is $E_{\\text{sleep}}(T) = P_{\\text{sleep}} T + E_{\\text{wake}}$.\n\nSetting these two expressions equal to find the break-even point $T^{\\star}$:\n$$\nP_{\\text{spin}} T^{\\star} = P_{\\text{sleep}} T^{\\star} + E_{\\text{wake}}\n$$\nWe solve for $T^{\\star}$:\n$$\n(P_{\\text{spin}} - P_{\\text{sleep}}) T^{\\star} = E_{\\text{wake}}\n$$\n$$\nT^{\\star} = \\frac{E_{\\text{wake}}}{P_{\\text{spin}} - P_{\\text{sleep}}}\n$$\nThis is the required closed-form expression.\n\nNow we evaluate $T^{\\star}$ using the given parameters: $P_{\\text{spin}}=10$ W, $P_{\\text{sleep}}=1.5$ W, and $E_{\\text{wake}}=5 \\times 10^{-4}$ J.\n$$\nT^{\\star} = \\frac{5 \\times 10^{-4} \\text{ J}}{10 \\text{ J/s} - 1.5 \\text{ J/s}} = \\frac{5 \\times 10^{-4}}{8.5} \\text{ s} \\approx 0.588235... \\times 10^{-4} \\text{ s}\n$$\nThe problem requires the answer in milliseconds (ms), rounded to three significant figures. We convert seconds to milliseconds by multiplying by $1000$ (or $10^3$).\n$$\nT^{\\star} \\approx (0.588235... \\times 10^{-4}) \\times 10^3 \\text{ ms} = 0.0588235... \\text{ ms}\n$$\nRounding to three significant figures gives $0.0588$ ms.\n\n### Task 4: Proposed Energy-Aware Locking Policy\n\nThe value $T^{\\star}$ represents the energy break-even point for a single wait.\n- If a wait is shorter than $T^{\\star}$, spinning consumes less energy.\n- If a wait is longer than $T^{\\star}$, sleeping consumes less energy.\n\nA waiting thread does not know the wait duration in advance. A robust, energy-aware policy should adapt to this uncertainty. A suitable policy is a hybrid, two-phase waiting strategy.\n\n**Policy Proposal: Spin-then-Sleep**\nWhen a thread finds a lock contended, it should first spin for a duration of $T^{\\star}$. If the lock is acquired within this time, the thread has used the more energy-efficient strategy for short waits. If the lock is still held by another thread after time $T^{\\star}$ has elapsed, the waiting thread should then stop spinning and transition to a sleep state (i.e., park itself), to be woken up when the lock becomes available.\n\n**Justification:**\nThis policy is energy-favorable because it dynamically selects the better strategy based on the wait time.\n1.  **For short waits ($W < T^{\\star}$):** The thread will likely acquire the lock during the initial spinning phase. This correctly employs spinning, which is more energy-efficient than sleeping for these short durations, as it avoids the relatively high, fixed wake-up cost $E_{\\text{wake}}$.\n2.  **For long waits ($W > T^{\\star}$):** The thread will spin for the initial period $T^{\\star}$ and then sleep for the remaining duration $W - T^{\\star}$. The energy penalty for this initial spin is minimal because, by the definition of $T^{\\star}$, the energy to spin for $T^{\\star}$ is equal to the energy to sleep for $T^{\\star}$ (including the wake-up cost if we were to model it that way). For the rest of the long wait, the thread benefits from the much lower power consumption of the sleep state ($P_{\\text{sleep}} < P_{\\text{spin}}$), leading to significant energy savings compared to a pure spinning policy.\n\nThis adaptive strategy effectively hedges against both scenarios, providing near-optimal energy performance under varying contention levels without prior knowledge of the wait duration.",
            "answer": "$$\\boxed{0.0588}$$"
        }
    ]
}