{
    "hands_on_practices": [
        {
            "introduction": "I/O virtualization often involves emulating modern hardware features, like scatter-gather I/O, for older or simpler devices. This emulation, while providing compatibility, introduces performance overhead that must be carefully managed. This exercise models the cost of such software emulation using a common technique called \"bounce buffering,\" where data is copied to a contiguous memory region for the device. By calculating the expected CPU and memory bandwidth costs for a typical workload , you will gain a concrete understanding of the performance trade-offs inherent in I/O virtualization and its tangible impact on system resources.",
            "id": "3648915",
            "problem": "A hypervisor implements Input/Output (I/O) virtualization for a legacy Peripheral Component Interconnect Express (PCIe) storage device that lacks scatter-gather (SG) support. When a guest operating system issues an SG I/O request, the hypervisor must software-copy the non-contiguous guest buffers into a contiguous bounce buffer before the device performs Direct Memory Access (DMA). Assume the following realistic and measured costs for the copy path on a single core:\n\n- Per-byte copy cost is $c_{b} = 0.8$ cycles per byte.\n- Per-segment setup cost is $c_{s} = 300$ cycles per segment.\n- Per-I/O fixed cost is $c_{0} = 500$ cycles for bounce-buffer management.\n\nA typical workload has I/O sizes and segment counts distributed as follows, independently per I/O:\n- With probability $0.5$, the I/O size is $64$ KiB and the segment count is $8$.\n- With probability $0.3$, the I/O size is $128$ KiB and the segment count is $16$.\n- With probability $0.2$, the I/O size is $32$ KiB and the segment count is $4$.\n\nAssume $1$ KiB $= 1024$ bytes. Also assume the sustained Input/Output Operations Per Second (IOPS) rate is $R = 25000$ IOPS. For the purposes of memory bandwidth accounting, define the bandwidth consumed by the software copy as the sum of bytes read and bytes written by the copy per second; that is, each byte copied contributes twice to memory traffic.\n\nUsing only fundamental definitions of expected value, cycles as time quanta for the processor, and memory bandwidth as bytes per second, derive from first principles and compute:\n\n- The expected additional Central Processing Unit (CPU) cycles per I/O incurred by the software SG emulation.\n- The sustained memory bandwidth consumed by the software copy at the given IOPS rate, expressed in GiB/s.\n\nRound your answers to four significant figures. Express the final memory bandwidth in GiB/s. Provide both answers in a single row, in the order: cycles per I/O, GiB/s.",
            "solution": "The problem is subjected to validation against the specified criteria.\n\n**Step 1: Extract Givens**\n- **System Model**: A hypervisor implements Input/Output (I/O) virtualization for a legacy Peripheral Component Interconnect Express (PCIe) storage device lacking scatter-gather (SG) support. For SG requests, the hypervisor performs a software copy of non-contiguous guest buffers into a contiguous bounce buffer for Direct Memory Access (DMA).\n- **Cost Parameters**:\n  - Per-byte copy cost: $c_{b} = 0.8$ cycles/byte.\n  - Per-segment setup cost: $c_{s} = 300$ cycles/segment.\n  - Per-I/O fixed cost for bounce-buffer management: $c_{0} = 500$ cycles.\n- **Workload Distribution**:\n  - Case 1: Probability $P_1 = 0.5$, I/O size $S_1 = 64$ KiB, segment count $N_1 = 8$.\n  - Case 2: Probability $P_2 = 0.3$, I/O size $S_2 = 128$ KiB, segment count $N_2 = 16$.\n  - Case 3: Probability $P_3 = 0.2$, I/O size $S_3 = 32$ KiB, segment count $N_3 = 4$.\n- **Constants and Rates**:\n  - Unit conversion: $1$ KiB $= 1024$ bytes.\n  - Sustained I/O Operations Per Second rate: $R = 25000$ IOPS.\n- **Definitions**:\n  - Memory bandwidth consumed by the software copy is the sum of bytes read and bytes written per second. Each byte copied contributes twice to memory traffic.\n- **Required Outputs**:\n  1. The expected additional CPU cycles per I/O.\n  2. The sustained memory bandwidth consumed by the software copy, in GiB/s.\n- **Rounding**: Final answers must be rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated as follows:\n- **Scientifically Grounded**: The problem is well-grounded in the principles of computer architecture and operating systems, specifically I/O virtualization. The scenario of emulating scatter-gather capabilities in software via bounce buffering is a standard technique. The provided cost values and I/O rates are realistic for performance analysis.\n- **Well-Posed**: The problem is fully specified. All necessary data, constants, and probabilistic distributions are provided. The objectives are clearly stated, and a unique, stable solution can be derived from the given model. The probabilities sum to $0.5 + 0.3 + 0.2 = 1.0$, ensuring consistency.\n- **Objective**: The problem statement is composed of objective, quantitative statements and definitions, free of subjective or ambiguous language.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be derived from first principles.\n\n**Solution Derivation**\n\nFirst, we establish a formal model for the additional CPU cycles incurred per I/O operation due to the software scatter-gather emulation. The total cost is a function of the I/O size $S$ (in bytes) and the number of segments $N$. This cost, $C(S, N)$, is the sum of three distinct components as defined in the problem: a fixed per-I/O cost, a per-segment cost, and a per-byte cost.\n\nThe cost function is:\n$$C(S, N) = c_{0} + N \\cdot c_{s} + S \\cdot c_{b}$$\n\nThe workload is characterized by a discrete probability distribution over three types of I/O operations. To find the expected additional CPU cycles per I/O, we must calculate the expected value of the cost function $C(S, N)$ over this distribution. The expected value, $E[C]$, of a function $g(x_i)$ of a discrete random variable is given by the fundamental definition $E[g(X)] = \\sum_{i} P(X=x_i) g(x_i)$. In our case, the outcomes are pairs $(S_i, N_i)$ with probabilities $P_i$.\n\nThe expected cost is therefore:\n$$E[C] = \\sum_{i=1}^{3} P_i \\cdot C(S_i, N_i) = P_1 \\cdot C(S_1, N_1) + P_2 \\cdot C(S_2, N_2) + P_3 \\cdot C(S_3, N_3)$$\n\nBefore calculating the costs, we must convert the I/O sizes from KiB to bytes, using the given conversion $1 \\text{ KiB} = 1024$ bytes.\n- $S_1 = 64 \\text{ KiB} = 64 \\times 1024 \\text{ bytes} = 65536 \\text{ bytes}$\n- $S_2 = 128 \\text{ KiB} = 128 \\times 1024 \\text{ bytes} = 131072 \\text{ bytes}$\n- $S_3 = 32 \\text{ KiB} = 32 \\times 1024 \\text{ bytes} = 32768 \\text{ bytes}$\n\nNow we calculate the individual costs for each case using the given constants $c_{0} = 500$, $c_{s} = 300$, and $c_{b} = 0.8$:\n- For Case 1 ($P_1 = 0.5$, $S_1 = 65536$, $N_1 = 8$):\n$$C_1 = c_{0} + N_1 \\cdot c_{s} + S_1 \\cdot c_{b} = 500 + 8 \\cdot 300 + 65536 \\cdot 0.8 = 500 + 2400 + 52428.8 = 55328.8 \\text{ cycles}$$\n- For Case 2 ($P_2 = 0.3$, $S_2 = 131072$, $N_2 = 16$):\n$$C_2 = c_{0} + N_2 \\cdot c_{s} + S_2 \\cdot c_{b} = 500 + 16 \\cdot 300 + 131072 \\cdot 0.8 = 500 + 4800 + 104857.6 = 110157.6 \\text{ cycles}$$\n- For Case 3 ($P_3 = 0.2$, $S_3 = 32768$, $N_3 = 4$):\n$$C_3 = c_{0} + N_3 \\cdot c_{s} + S_3 \\cdot c_{b} = 500 + 4 \\cdot 300 + 32768 \\cdot 0.8 = 500 + 1200 + 26214.4 = 27914.4 \\text{ cycles}$$\n\nWe can now compute the expected cycles per I/O:\n$$E[C] = 0.5 \\cdot (55328.8) + 0.3 \\cdot (110157.6) + 0.2 \\cdot (27914.4)$$\n$$E[C] = 27664.4 + 33047.28 + 5582.88 = 66294.56 \\text{ cycles/I/O}$$\nRounding to four significant figures, the expected cost is $66290$ cycles/I/O.\n\nNext, we derive the sustained memory bandwidth. The memory bandwidth is determined by the total amount of data copied per second. First, we find the expected size of a single I/O operation, $E[S]$, in bytes.\n$$E[S] = \\sum_{i=1}^{3} P_i \\cdot S_i = P_1 \\cdot S_1 + P_2 \\cdot S_2 + P_3 \\cdot S_3$$\n$$E[S] = 0.5 \\cdot (65536) + 0.3 \\cdot (131072) + 0.2 \\cdot (32768)$$\n$$E[S] = 32768 + 39321.6 + 6553.6 = 78643.2 \\text{ bytes/I/O}$$\n\nThe total rate of data copying, $D_{copy}$, is the expected size per I/O multiplied by the I/O rate $R$:\n$$D_{copy} = E[S] \\cdot R = 78643.2 \\frac{\\text{bytes}}{\\text{I/O}} \\times 25000 \\frac{\\text{I/O}}{\\text{s}} = 1966080000 \\frac{\\text{bytes}}{\\text{s}}$$\n\nThe problem defines the memory bandwidth, $B$, as the sum of bytes read and written. Since the software copy reads each byte from the guest's non-contiguous buffers and writes it to the hypervisor's contiguous bounce buffer, each byte copied generates two bytes of memory traffic.\n$$B = 2 \\cdot D_{copy} = 2 \\times 1966080000 \\frac{\\text{bytes}}{\\text{s}} = 3932160000 \\frac{\\text{bytes}}{\\text{s}}$$\n\nFinally, we must convert this bandwidth from bytes per second to gibibytes per second (GiB/s). Based on the provided unit $1 \\text{ KiB} = 1024$ bytes, we use binary prefixes. Therefore, $1 \\text{ GiB} = 1024^3 \\text{ bytes} = 2^{30} \\text{ bytes} = 1073741824 \\text{ bytes}$.\n$$B (\\text{in GiB/s}) = \\frac{3932160000}{1073741824} \\approx 3.662109375 \\text{ GiB/s}$$\nRounding to four significant figures, the consumed memory bandwidth is $3.662$ GiB/s.\n\nThe two requested values are the expected CPU cycles per I/O and the sustained memory bandwidth.\n- Expected CPU cycles per I/O: $66290$\n- Sustained memory bandwidth: $3.662$ GiB/s",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n66290 & 3.662\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After understanding the fundamental costs of virtualization, the next step is to optimize them. A key challenge in high-performance device passthrough is minimizing the overhead from VM exits, which occur when the guest OS must trap into the hypervisor for services like managing the IOMMU. This practice explores batching, a powerful optimization where multiple IOMMU operations are grouped to reduce the frequency of these expensive exits. By modeling a bursty I/O workload, you will analyze the effectiveness of this strategy and quantify its performance benefits, demonstrating how thoughtful software design can overcome architectural bottlenecks .",
            "id": "3648965",
            "problem": "A system runs a device passthrough workload under a hypervisor that uses an Input-Output Memory Management Unit (IOMMU) with an Input-Output Translation Lookaside Buffer (IOTLB). Each Direct Memory Access (DMA) mapping teardown triggers an unmap operation that requires an IOTLB invalidation to ensure that stale translations are not used by the device. Under virtualization, each invalidation is performed by the hypervisor, incurring a Virtual Machine (VM) exit. Assume the following well-tested facts and modeling assumptions grounded in operating systems and probability:\n\n- Each VM exit costs a fixed number of cycles, denoted $c_{x}$, and each IOTLB invalidation costs a fixed number of cycles, denoted $c_{i}$, independent of how many I/O virtual addresses are being invalidated when the operation is a global or domain-wide flush.\n- An IOTLB invalidation is required for correctness after unmap to make updated page tables visible to the device. Without batching, the hypervisor performs one invalidation per unmap, thus one VM exit and one invalidation per unmap.\n- Unmap requests arrive in bursts due to the device completing groups of DMA operations. Let bursts arrive at a rate of $r$ bursts per second. In each burst, the number of unmaps $K$ is an independent geometric random variable with parameter $p$ on $\\{1,2,3,\\ldots\\}$, meaning $\\mathbb{P}(K=k)=(1-p)^{k-1}p$ and $\\mathbb{E}[K]=\\frac{1}{p}$. Within a burst, inter-arrival times are negligible relative to batching timers; between bursts, the idle time is large enough that any pending batch will be flushed at burst end.\n- Consider the batching strategy “size-threshold with long timeout”: the hypervisor accumulates unmap descriptors for up to $B$ unmaps and then issues a single invalidation (one VM exit plus one IOTLB invalidation) for that batch; if the burst ends with fewer than $B$ accumulated unmaps, the hypervisor flushes once at the end of the burst.\n\nUsing only these assumptions and first principles, compute the expected savings in cycles per second achieved by batching relative to the baseline (no batching). Use the parameter values $r=3000$ bursts/s, $p=0.05$, $B=16$, $c_{x}=9000$ cycles, and $c_{i}=3000$ cycles. Round your final answer to four significant figures. Express the result in cycles per second.",
            "solution": "The goal is to calculate the expected savings in cycles per second from using a batching strategy for IOTLB invalidations compared to a baseline of no batching. The savings, $S$, is the difference between the baseline cost per second, $C_{base}$, and the batching cost per second, $C_{batch}$.\n$$S = C_{base} - C_{batch}$$\nThe cost of a single invalidation event (one VM exit and one IOTLB invalidation) is $C_{inv} = c_x + c_i$.\n\n**1. Baseline Cost (No Batching)**\nWithout batching, each unmap request triggers one invalidation event. The number of unmaps per burst, $K$, is a geometric random variable with parameter $p$ on $\\{1, 2, \\dots\\}$, so its expected value is $\\mathbb{E}[K] = 1/p$. With bursts arriving at a rate of $r$ bursts per second, the expected number of unmaps per second is:\n$$\\lambda_{unmap} = r \\cdot \\mathbb{E}[K] = \\frac{r}{p}$$\nThe total expected cost per second for the baseline is:\n$$C_{base} = \\lambda_{unmap} \\cdot C_{inv} = \\frac{r}{p} (c_x + c_i)$$\n\n**2. Batching Cost**\nWith batching, the number of invalidations for a burst of size $K$ is $\\lceil K/B \\rceil$. We need to find the expected value of this quantity, $\\mathbb{E}[\\lceil K/B \\rceil]$. Using the formula $\\mathbb{E}[X] = \\sum_{j=1}^{\\infty} \\mathbb{P}(X \\ge j)$ for a positive integer-valued random variable $X$:\n$$\\mathbb{E}\\left[\\lceil \\frac{K}{B} \\rceil\\right] = \\sum_{j=1}^{\\infty} \\mathbb{P}\\left(\\lceil \\frac{K}{B} \\rceil \\ge j\\right) = \\sum_{j=1}^{\\infty} \\mathbb{P}(K > (j-1)B)$$\nFor a geometric random variable, $\\mathbb{P}(K > n) = (1-p)^n$. The sum becomes a geometric series:\n$$\\mathbb{E}\\left[\\lceil \\frac{K}{B} \\rceil\\right] = \\sum_{j=1}^{\\infty} (1-p)^{(j-1)B} = \\sum_{m=0}^{\\infty} ((1-p)^B)^m = \\frac{1}{1 - (1-p)^B}$$\nThe expected number of invalidations per second with batching is:\n$$\\lambda_{batch} = r \\cdot \\mathbb{E}\\left[\\lceil \\frac{K}{B} \\rceil\\right] = \\frac{r}{1 - (1-p)^B}$$\nThe total expected cost per second with batching is:\n$$C_{batch} = \\lambda_{batch} \\cdot C_{inv} = \\frac{r}{1 - (1-p)^B} (c_x + c_i)$$\n\n**3. Expected Savings**\nThe savings per second is the difference:\n$$S = C_{base} - C_{batch} = \\left(\\frac{r}{p} - \\frac{r}{1 - (1-p)^B}\\right) (c_x + c_i)$$\n$$S = r(c_x + c_i) \\left( \\frac{1}{p} - \\frac{1}{1 - (1-p)^B} \\right)$$\n\n**4. Numerical Calculation**\nUsing the given values: $r=3000$ bursts/s, $p=0.05$, $B=16$, $c_x=9000$ cycles, and $c_i=3000$ cycles.\n$$c_x + c_i = 12000 \\text{ cycles}$$\n$$S = 3000 \\cdot 12000 \\left( \\frac{1}{0.05} - \\frac{1}{1 - (1-0.05)^{16}} \\right)$$\n$$S = 3.6 \\times 10^7 \\left( 20 - \\frac{1}{1 - (0.95)^{16}} \\right)$$\n$$S \\approx 3.6 \\times 10^7 \\left( 20 - \\frac{1}{1 - 0.440127} \\right) = 3.6 \\times 10^7 \\left( 20 - \\frac{1}{0.559873} \\right)$$\n$$S \\approx 3.6 \\times 10^7 (20 - 1.786105) = 3.6 \\times 10^7 (18.213895)$$\n$$S \\approx 655700220 \\text{ cycles/s}$$\nRounding to four significant figures gives $6.557 \\times 10^8$ cycles/s.",
            "answer": "$$\\boxed{6.557 \\times 10^8}$$"
        },
        {
            "introduction": "Performance is crucial, but system stability is paramount. A robust virtualized system must handle errors gracefully, and I/O virtualization introduces unique failure modes, such as IOMMU faults on DMA attempts. This practice shifts our focus from performance to correctness by simulating an IOMMU page fault and challenging you to identify the proper recovery procedure. Navigating this scenario  requires a deep understanding of driver invariants and memory safety, highlighting the critical importance of designing fault-tolerant drivers to prevent data corruption and ensure reliable operation in complex virtual environments.",
            "id": "3648968",
            "problem": "A guest operating system with a passed-through Peripheral Component Interconnect Express (PCIe) network device implements a transmit queue using a ring of $N$ descriptors. Each descriptor $d_i$ (for $i \\in \\{0,\\dots,N-1\\}$) records a guest physical address $p_i$, a length $\\ell_i$, and an ownership bit that transitions from the driver to the device when the driver posts a transmit. The device performs Direct Memory Access (DMA) reads from guest memory to fetch packet data and metadata for offloads. An Input-Output Memory Management Unit (IOMMU) provides DMA remapping and isolation: on a DMA access to an unmapped address, the IOMMU blocks the access and records a fault without modifying guest memory. The hypervisor can inject synthetic IOMMU faults by temporarily unmapping a page that backs some $p_k$, and it exposes fault information to the guest via a virtual interrupt and a fault log with the device requester identifier and the faulting guest physical address $g$.\n\nAssume the following widely accepted facts and definitions as the base:\n- Direct Memory Access (DMA) reads or writes occur outside central processing unit (CPU) caches and, when blocked by the IOMMU due to an unmapped translation, do not modify guest physical memory.\n- The IOMMU records a fault that includes the faulting address and device requester, and the device may signal an error interrupt or time out in response to the blocked DMA.\n- Correct driver operation hinges on descriptor ownership, at-most-once completion, and never reusing a buffer while it is owned by the device.\n\nA test harness injects a synthetic IOMMU fault by unmapping exactly one page that backs $p_k$ at time $t_f$, just after the driver rings the device doorbell for descriptors $d_j$ through $d_k$. The device attempts the DMA, the IOMMU blocks it, and a virtual IOMMU fault interrupt is delivered to the guest. The driver must detect and recover without corrupting data, preserving the invariant that no buffer is freed or repurposed while the device may still access it, and ensuring progress after the fault is cleared or worked around.\n\nWhich of the following driver behaviors best satisfies these constraints for robust detection and recovery in this scenario?\n\nA. Immediately free the buffer of $d_k$ to avoid leaks, requeue the buffer for new traffic, and rely on the device to retry the DMA later when the IOMMU mapping is restored. Do not stop the transmit queue, because the hardware will eventually make forward progress.\n\nB. Quiesce the affected transmit queue by masking interrupts and stopping new posts; read device and virtual IOMMU fault status to correlate the faulting address $g$ to $d_k$ using a maintained map from DMA addresses to descriptors; mark $d_k$ as failed and prevent its buffer from being reused until the device is either reset or the fault condition is cleared; establish a valid translation for $p_k$ (for example, by pinning and mapping or by allocating a pinned bounce buffer and copying), then either resubmit $d_k$ or drop it explicitly; finally, restart the queue ensuring at-most-once completion.\n\nC. Invalidate CPU caches covering the buffer of $d_k$, assume the DMA write may have partially modified memory, and deliver whatever bytes are present with checksum offload disabled. Do not consult IOMMU fault logs to avoid hypervisor dependencies.\n\nD. Ignore error and fault interrupts entirely to avoid false positives; instead, use a periodic timer to detect descriptors whose completion time exceeds a threshold, and unconditionally recycle their buffers to prevent head-of-line blocking.\n\nE. Because the device is passed through, the guest cannot observe IOMMU faults; therefore, the only recoverable strategy is to reboot the virtual machine after a stall exceeding a threshold $T$.\n\nF. For DMA reads by the device, assume the device may have consumed a prefix of the buffer before the fault, so never resubmit the same descriptor after remapping; always allocate a fresh buffer at a different physical address to avoid stale DMA affecting the old location later.\n\nChoose all that apply.",
            "solution": "To determine the best recovery strategy, we evaluate each option against the core principles of robust driver design: stabilizing the system, diagnosing the fault, preserving data and state invariants, recovering from the error, and ensuring forward progress.\n\n**A. Immediately free the buffer of $d_k$ to avoid leaks, requeue the buffer for new traffic, and rely on the device to retry the DMA later when the IOMMU mapping is restored. Do not stop the transmit queue, because the hardware will eventually make forward progress.**\n*   **Analysis**: This approach violates fundamental safety invariants. Freeing and requeuing the buffer for $d_k$ creates a use-after-free race condition. If the device retries the DMA for $d_k$ after the buffer has been reused for new data, it will read the new data and transmit it as if it were the original packet, causing severe data corruption. Furthermore, not stopping the transmit queue is reckless; it allows the system to continue operating in a known-faulty state.\n*   **Verdict**: **Incorrect**.\n\n**B. Quiesce the affected transmit queue by masking interrupts and stopping new posts; read device and virtual IOMMU fault status to correlate the faulting address $g$ to $d_k$ using a maintained map from DMA addresses to descriptors; mark $d_k$ as failed and prevent its buffer from being reused until the device is either reset or the fault condition is cleared; establish a valid translation for $p_k$ (for example, by pinning and mapping or by allocating a pinned bounce buffer and copying), then either resubmit $d_k$ or drop it explicitly; finally, restart the queue ensuring at-most-once completion.**\n*   **Analysis**: This option describes a complete and correct recovery procedure. It follows the standard steps: (1) Stabilize (\"Quiesce the... queue\"), (2) Diagnose (\"correlate the faulting address\"), (3) Preserve Invariants (\"prevent its buffer from being reused\"), (4) Recover (\"establish a valid translation\"), and (5) Ensure Progress and Resume (\"resubmit... or drop... restart the queue\"). This represents the industry-standard best practice for robust IOMMU fault handling.\n*   **Verdict**: **Correct**.\n\n**C. Invalidate CPU caches covering the buffer of $d_k$, assume the DMA write may have partially modified memory, and deliver whatever bytes are present with checksum offload disabled. Do not consult IOMMU fault logs to avoid hypervisor dependencies.**\n*   **Analysis**: This option is flawed. The operation is a device DMA *read* from memory, not a write to it. The problem states the IOMMU blocks the access, preventing memory modification, so the assumption of partial writes is false. DMA bypasses CPU caches, so cache invalidation is irrelevant to this fault. Ignoring the IOMMU fault log discards precise diagnostic information.\n*   **Verdict**: **Incorrect**.\n\n**D. Ignore error and fault interrupts entirely to avoid false positives; instead, use a periodic timer to detect descriptors whose completion time exceeds a threshold, and unconditionally recycle their buffers to prevent head-of-line blocking.**\n*   **Analysis**: This is a poor strategy. It replaces a precise, immediate interrupt with an imprecise, slow timer. Ignoring interrupts is wilfully blind to critical hardware events. Unconditionally recycling buffers creates the same use-after-free vulnerability as option A, leading to data corruption.\n*   **Verdict**: **Incorrect**.\n\n**E. Because the device is passed through, the guest cannot observe IOMMU faults; therefore, the only recoverable strategy is to reboot the virtual machine after a stall exceeding a threshold $T$.**\n*   **Analysis**: The premise (\"the guest cannot observe IOMMU faults\") is explicitly contradicted by the problem statement, which describes a virtual interrupt and fault log. Modern virtualization provides vIOMMU features for this purpose. Rebooting is a last resort, not a primary recovery strategy for a manageable fault.\n*   **Verdict**: **Incorrect**.\n\n**F. For DMA reads by the device, assume the device may have consumed a prefix of the buffer before the fault, so never resubmit the same descriptor after remapping; always allocate a fresh buffer at a different physical address to avoid stale DMA affecting the old location later.**\n*   **Analysis**: This option's premise is inconsistent with how IOMMU page faults work. The IOMMU blocks the entire DMA transaction that attempts to access an unmapped page. No partial data transfer occurs for that transaction. Therefore, the device has not \"consumed a prefix of the buffer\". The data is untouched. The procedure in option B is the correct and necessary one.\n*   **Verdict**: **Incorrect**.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}