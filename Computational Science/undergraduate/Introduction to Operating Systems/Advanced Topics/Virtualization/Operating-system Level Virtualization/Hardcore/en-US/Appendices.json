{
    "hands_on_practices": [
        {
            "introduction": "At its core, operating-system level virtualization relies on the fundamental fact that all containers share the same host kernel. This makes the system call interface a critical boundary for both security and compatibility. This exercise  challenges you to analyze how `seccomp` security profiles and the GNU C Library (`glibc`) behavior interact, a crucial skill for diagnosing and resolving compatibility issues in containerized environments.",
            "id": "3665412",
            "problem": "A containerized application is built against a recent version of the GNU C Library (glibc) 2.36 and makes use of newer Linux system calls such as `openat2` and `pidfd_open`. The container runs on an x86_64 host with Linux kernel version 5.4, which does not implement those new system calls. The container runtime applies a default Secure Computing Mode (`seccomp`) profile whose default action for any non-listed system call is to return an error to the caller (that is, the filter uses an action equivalent to “return error” rather than “allow” or “kill”). Assume the filter explicitly denies `openat2` and `pidfd_open` and allows a conventional baseline such as `openat`, `fstat`, `mmap`, `mprotect`, `futex`, and `rt_sigaction`. \n\nFrom first principles, consider the following facts:\n\n- In operating-system level virtualization, processes inside containers share the same host kernel and its system call Application Binary Interface (ABI). There is no separate container kernel.\n- A system call is an invocation from user space into the kernel via a well-defined system call ABI. If the kernel does not implement the requested system call, the kernel returns -1 and sets the global error indicator `errno` to `ENOSYS` (function not implemented). If the kernel denies a call for sandboxing via `seccomp`, the kernel returns -1 and sets `errno` according to the filter’s selected error action (commonly `EPERM` for operation not permitted), or it may deliver a signal depending on the chosen action.\n- The GNU C Library (glibc) provides wrappers for system calls and may attempt fallbacks to older, widely implemented system call sequences if it detects `ENOSYS` (indicating lack of implementation). It does not perform such fallbacks when the error is `EPERM` (indicating a permission failure), because `EPERM` denotes a policy denial rather than an absent implementation.\n\nPredict the behavior when the application attempts to use `openat2` on this host under the described `seccomp` profile, and choose the option that both correctly explains the outcome and recommends a safe profile adjustment that preserves sandboxing while enabling compatibility.\n\nA. Because the container shares the host kernel, `openat2` on Linux 5.4 with a `seccomp` default “return error” action that sets `errno = EPERM` will return -1 with `EPERM`, and glibc will not fallback. A safe compatibility fix is to change the `seccomp` action for newer system calls such as `openat2` and `pidfd_open` to “return `ENOSYS`” and explicitly allow the older equivalents (for example, `openat` and related calls needed by the fallback), keeping restrictive actions for genuinely risky calls.\n\nB. The container has its own kernel separate from the host, so glibc 2.36 will see the correct implementation and the `seccomp` profile has no effect on `openat2`; the call succeeds. No profile changes are needed.\n\nC. glibc always falls back when a call fails, including on `EPERM`, so denying `openat2` is harmless. The correct safe profile is to continue returning `EPERM` for `openat2` and `pidfd_open` without allowing any older equivalents.\n\nD. Denying `openat2` via `seccomp` will deliver `SIGSYS` that glibc automatically catches and emulates with `openat`, so no specific allowlist changes are required; the safest profile is to trap all new calls and rely on glibc to emulate them in user space.\n\nSelect the single best option.",
            "solution": "The user wants me to act as a meticulous STEM professor, validate the provided problem statement, and then solve it.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified criteria.\n\n**Step 1: Extract Givens**\n\nThe problem statement provides the following information:\n*   An application is built against GNU C Library (glibc) version 2.36.\n*   The application uses newer Linux system calls: `openat2` and `pidfd_open`.\n*   The container hosting the application runs on an `x86_64` host with Linux kernel version 5.4.\n*   The host kernel (version 5.4) does not implement the system calls `openat2` and `pidfd_open`.\n*   The container runtime uses a Secure Computing Mode (`seccomp`) profile.\n*   The `seccomp` profile's default action for non-whitelisted system calls is to return an error.\n*   The `seccomp` filter explicitly denies `openat2` and `pidfd_open`.\n*   The `seccomp` filter allows a baseline of conventional system calls including `openat`, `fstat`, `mmap`, `mprotect`, `futex`, and `rt_sigaction`.\n\nThe problem also provides three foundational principles:\n1.  Operating-system level virtualization (containerization) involves sharing the host kernel and its system call Application Binary Interface (ABI); there is no separate container kernel.\n2.  An unimplemented system call results in the kernel returning -1 with the global error indicator `errno` set to `ENOSYS`. A system call blocked by a `seccomp` filter with a \"return error\" action results in the kernel returning -1 with `errno` set according to the filter's action (commonly `EPERM`). Other `seccomp` actions, like delivering a signal, are also possible.\n3.  The GNU C Library (`glibc`) may use fallback mechanisms (e.g., trying older system calls) when it detects a system call failure with `errno` set to `ENOSYS`. It does not perform such fallbacks when `errno` is `EPERM`, as this indicates a policy denial.\n\n**Step 2: Validate Using Extracted Givens**\n\nI will now assess the problem's validity:\n\n*   **Scientifically Grounded:** The problem is firmly grounded in the principles of operating systems, specifically Linux kernel behavior, system call interfaces, containerization, and the `seccomp` sandboxing mechanism. The described behavior of `glibc`, `seccomp`, and the kernel regarding system call handling and error codes (`ENOSYS` vs. `EPERM`) is factually correct and represents a real-world software engineering challenge. For example, `openat2` was introduced in Linux kernel 5.6, so it is indeed absent in kernel 5.4. The behavior of `glibc` regarding syscall fallbacks is a well-documented aspect of the library.\n*   **Well-Posed:** The problem provides a complete set of conditions and clearly stated principles. The question asks for a prediction of behavior and a recommendation for a safe fix, for which a unique, logical solution can be derived from the provided information.\n*   **Objective:** The problem is stated in precise, objective, technical language, free from ambiguity or subjective claims.\n\nThe problem statement does not exhibit any of the listed flaws (e.g., scientific unsoundness, incompleteness, contradiction, or ambiguity). It describes a standard and non-trivial compatibility issue encountered in containerized environments.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **VALID**. I will proceed with the solution derivation.\n\n### Solution Derivation\n\nThe analysis proceeds by tracing the execution flow of a system call from the application to the kernel and back.\n\n1.  **Application Call and `glibc` Wrapper:** The application, linked against `glibc` version 2.36, makes a library call that requires opening a file (e.g., `open()`). Because `glibc` 2.36 is modern, its wrapper function will attempt to use the most feature-rich underlying system call available for this purpose, which is `openat2`.\n2.  **System Call Invocation:** The `glibc` wrapper executes the `syscall` instruction to transition from user mode to kernel mode, requesting the `openat2` system call.\n3.  **Kernel-Side `seccomp` Interception:** As dictated by Principle 1, the system call is directed to the shared host kernel. Before the kernel's main system call dispatcher can process the request, the `seccomp-bpf` filter associated with the container's process intercepts the call.\n4.  **`seccomp` Filter Action:** The problem states the `seccomp` profile explicitly denies `openat2` with a \"return error\" action. As explained in Principle 2, this action directs the kernel to immediately terminate the system call processing and return an error to the user-space caller. The typical `errno` value for this type of denial is `EPERM` (Operation not permitted). The call is rejected at this stage and never reaches the code path where the kernel would check if `openat2` is implemented. Therefore, `ENOSYS` is not generated at this point.\n5.  **Return to User Space:** The kernel returns control to the `glibc` wrapper function. The return value is -1, and `errno` is set to `EPERM`.\n6.  **`glibc` Fallback Logic:** The `glibc` wrapper function checks the `errno` value to decide its next action. According to Principle 3, `glibc` differentiates between `ENOSYS` (function not implemented on this kernel) and `EPERM` (operation denied by policy). Upon seeing `EPERM`, `glibc` infers a security restriction and does *not* attempt to fall back to an older system call like `openat`. Instead, it preserves the `EPERM` error and returns -1 to the application.\n7.  **Outcome:** The application's file-opening operation fails with an `Operation not permitted` error. The application will likely malfunction or terminate unless it is specifically coded to handle this unexpected `EPERM` error for a common operation.\n\n### Proposed Safe Profile Adjustment\n\nTo achieve compatibility while preserving the principle of least privilege, the `seccomp` profile must be adjusted. The goal is to allow `glibc`'s fallback mechanism to function as intended.\n\n1.  The problem is the `EPERM` error code. To trigger the `glibc` fallback, the `seccomp` filter should instead return `ENOSYS`. The `seccomp` BPF facility allows filters to return specific error codes using the `SECCOMP_RET_ERRNO` action.\n2.  The `seccomp` profile should be modified to change the action for `openat2` (and other similar new syscalls like `pidfd_open`) from `SECCOMP_RET_ERRNO | EPERM` (or its equivalent) to `SECCOMP_RET_ERRNO | ENOSYS`.\n3.  With this change, when the application attempts to call `openat2`, the `seccomp` filter will cause the kernel to return -1 with `errno` set to `ENOSYS`.\n4.  `glibc` will see `ENOSYS` and, per Principle 3, correctly deduce that the kernel is old. It will then automatically attempt a fallback to an older, equivalent system call, such as `openat`.\n5.  The problem states that the `seccomp` profile explicitly allows `openat`, and the host kernel 5.4 implements it. Therefore, the fallback call will succeed.\n6.  This approach is safe because it maintains a strict whitelist of allowed system calls. It does not broaden the attack surface; it simply changes the denial signal for specific, known-to-be-absent syscalls to enable a well-defined and safe compatibility layer within `glibc`, while continuing to block genuinely dangerous or unwanted system calls.\n\n### Option-by-Option Analysis\n\n*   **A. Because the container shares the host kernel, `openat2` on Linux 5.4 with a `seccomp` default “return error” action that sets `errno = EPERM` will return -1 with `EPERM`, and glibc will not fallback. A safe compatibility fix is to change the `seccomp` action for newer system calls such as `openat2` and `pidfd_open` to “return `ENOSYS`” and explicitly allow the older equivalents (for example, `openat` and related calls needed by the fallback), keeping restrictive actions for genuinely risky calls.**\n    This option correctly describes the entire sequence of events: the `seccomp` filter returns `EPERM`, which prevents the `glibc` fallback. It then correctly identifies the canonical solution: change the `seccomp` return code to `ENOSYS` to trigger the `glibc` fallback, while ensuring the fallback target syscall (`openat`) is on the allowlist. This preserves the sandboxing security posture.\n    **Verdict: Correct.**\n\n*   **B. The container has its own kernel separate from the host, so glibc 2.36 will see the correct implementation and the `seccomp` profile has no effect on `openat2`; the call succeeds. No profile changes are needed.**\n    This option is fundamentally flawed. It mischaracterizes operating-system level virtualization by claiming the container has a separate kernel. This contradicts Principle 1 and the very definition of a container. A container shares the host kernel.\n    **Verdict: Incorrect.**\n\n*   **C. glibc always falls back when a call fails, including on `EPERM`, so denying `openat2` is harmless. The correct safe profile is to continue returning `EPERM` for `openat2` and `pidfd_open` without allowing any older equivalents.**\n    This option makes a false claim about `glibc`'s behavior. It contradicts Principle 3, which explicitly states that `glibc` does *not* fall back on an `EPERM` error. Therefore, denying `openat2` with `EPERM` is not harmless; it breaks the application.\n    **Verdict: Incorrect.**\n\n*   **D. Denying `openat2` via `seccomp` will deliver `SIGSYS` that glibc automatically catches and emulates with `openat`, so no specific allowlist changes are required; the safest profile is to trap all new calls and rely on glibc to emulate them in user space.**\n    This option incorrectly assumes the `seccomp` action is `trap` (which generates `SIGSYS`) when the problem describes an action that returns an error code. While `seccomp` can be configured to generate `SIGSYS`, that is not what is described. The standard \"return error\" action maps to `SECCOMP_RET_ERRNO`. The primary, most reliable, and intended `glibc` compatibility mechanism for this scenario relies on `ENOSYS`, not signal trapping and emulation.\n    **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A key feature of OS-level virtualization is the ability to manage shared hardware resources fairly and predictably. The Linux Completely Fair Scheduler (CFS) uses a weighted algorithm in conjunction with Control Groups (`cgroups`) to divide CPU time among containers. In this practice , you will derive the steady-state CPU allocation formula directly from the scheduler's first principles, providing a deep, quantitative understanding of how resource controls translate into container performance.",
            "id": "3665364",
            "problem": "You are configuring operating-system level virtualization with Linux control groups (cgroups) on a host that exposes exactly 1 central processing unit (CPU) of capacity. There are $k$ containers, each with a single, always-runnable, CPU-bound process, assigned control-group CPU shares $w_1, w_2, \\dots, w_k$ where each $w_i \\in \\mathbb{R}_{>0}$. The host uses the Linux Completely Fair Scheduler (CFS). Use only the following fundamental facts about CFS with cgroup shares as your starting point:\n- Each runnable entity (here, each container’s schedulable entity) maintains a virtual runtime $v_i$. While entity $i$ runs for an amount of real time $\\Delta t$, its virtual runtime increases by an amount proportional to $\\Delta t$ and inversely proportional to its weight $w_i$, so that the instantaneous rate satisfies $\\frac{dv_i}{dt} \\propto \\frac{1}{w_i}$ when $i$ is running, and $\\frac{dv_i}{dt} = 0$ when it is not running.\n- The scheduler always selects the entity with the smallest $v_i$ to run next, and over time attempts to keep all $v_i$ equalized among runnable entities.\nAssume total demand strictly exceeds capacity in steady state (that is, all $k$ entities remain runnable at all times), so the CPU is fully utilized. Over any horizon during which the set of runnable entities does not change, let $f_i$ denote the expected fraction of CPU time allocated to container $i$, so that $f_i \\in (0,1)$ and $\\sum_{i=1}^{k} f_i = 1$.\n\nTask:\n1. Starting from the two facts above and no others, derive the steady-state expression for $f_i$ in terms of $w_1, \\dots, w_k$.\n2. Express your final result as a closed-form analytic expression for the vector $\\mathbf{f} = (f_1, f_2, \\dots, f_k)$ as unitless fractions of one CPU. No numerical substitution is required.\n3. Brief discussion (no calculation needed for grading): In this idealized model without minimum-granularity effects, argue whether starvation can occur for any container with $w_i &gt; 0$, and give a bound (in terms of a fairness window of length $L &gt; 0$) on the maximum time between two runs of container $i$.\n\nProvide the final answer as the expression for $\\mathbf{f}$ only. Since your answer is symbolic, no rounding is required. The fractions are unitless, as required.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of operating system schedulers, specifically an idealized model of the Linux Completely Fair Scheduler (CFS). The problem is well-posed, objective, self-contained, and formalizable, allowing for a rigorous derivation of the required expressions.\n\n### Part 1: Derivation of the Steady-State CPU Fraction $f_i$\n\nThe problem provides two fundamental facts governing the scheduler's behavior. We will use these, and only these, to derive the expression for $f_i$, the fraction of CPU time allocated to container $i$.\n\nThe givens are:\n- A single CPU, so total capacity is $1$.\n- $k$ containers with CPU shares $w_1, w_2, \\dots, w_k$, where each $w_i \\in \\mathbb{R}_{>0}$.\n- All $k$ containers are always runnable.\n- The total CPU time is fully utilized, so $\\sum_{i=1}^{k} f_i = 1$.\n- Fact 1: The rate of change of virtual runtime $v_i$ for a running container $i$ is $\\frac{dv_i}{dt} \\propto \\frac{1}{w_i}$. Let $c$ be the positive constant of proportionality. Thus, when container $i$ is running, its virtual runtime increases at a rate of $\\frac{dv_i}{dt} = \\frac{c}{w_i}$. When it is not running, $\\frac{dv_i}{dt} = 0$.\n- Fact 2: The scheduler always selects the runnable entity with the minimum virtual runtime $v_i$ and, over time, aims to maintain $v_i \\approx v_j$ for all runnable entities $i,j$.\n\nIn a steady-state system observed over a sufficiently long time interval of duration $T$, the scheduler's effort to equalize all virtual runtimes implies that the total accumulated virtual runtime, $\\Delta v_i$, must be the same for every container.\n$$ \\Delta v_1 = \\Delta v_2 = \\dots = \\Delta v_k $$\n\nLet $T_i$ be the total amount of real CPU time that container $i$ runs during this interval $T$. The fraction of CPU time for container $i$ is defined as $f_i$. In the context of our interval $T$, this means $f_i = \\frac{T_i}{T}$, or $T_i = f_i T$. Since the CPU is always busy, the sum of the run times for all containers must equal the total duration of the interval:\n$$ \\sum_{i=1}^{k} T_i = T $$\n\nThe accumulated virtual runtime $\\Delta v_i$ for container $i$ is the integral of its rate of change over the interval $T$. Since the rate is zero when the container is not running, we only need to consider the total time $T_i$ during which it was running at the constant rate $\\frac{c}{w_i}$.\n$$ \\Delta v_i = \\left( \\frac{c}{w_i} \\right) T_i $$\n\nUsing the steady-state condition that all total accumulated virtual runtimes are equal, we can equate the expressions for any two containers $i$ and $j$:\n$$ \\Delta v_i = \\Delta v_j $$\n$$ \\left( \\frac{c}{w_i} \\right) T_i = \\left( \\frac{c}{w_j} \\right) T_j $$\n\nSince $c > 0$, we can divide by $c$ to obtain:\n$$ \\frac{T_i}{w_i} = \\frac{T_j}{w_j} $$\nThis shows that the ratio of the total run time to the weight is a constant for all containers. Let us denote this constant by $K_T$.\n$$ \\frac{T_i}{w_i} = K_T \\implies T_i = K_T w_i $$\n\nNow, we use the constraint that the CPU is fully utilized. We substitute the expression for $T_i$ into the sum:\n$$ \\sum_{i=1}^{k} (K_T w_i) = T $$\n$$ K_T \\sum_{i=1}^{k} w_i = T $$\n\nWe can solve for the constant of proportionality $K_T$:\n$$ K_T = \\frac{T}{\\sum_{j=1}^{k} w_j} $$\nNote that the denominator $\\sum_{j=1}^{k} w_j$ is strictly positive since all $w_j > 0$.\n\nNow we substitute this expression for $K_T$ back into the equation for $T_i$:\n$$ T_i = \\left( \\frac{T}{\\sum_{j=1}^{k} w_j} \\right) w_i $$\n\nFinally, we find the fraction of CPU time $f_i$ by dividing $T_i$ by $T$:\n$$ f_i = \\frac{T_i}{T} = \\frac{1}{T} \\left( \\frac{T w_i}{\\sum_{j=1}^{k} w_j} \\right) = \\frac{w_i}{\\sum_{j=1}^{k} w_j} $$\nThis is the steady-state expression for the fraction of CPU time allocated to container $i$.\n\n### Part 2: Vector Expression for $\\mathbf{f}$\n\nThe problem requires expressing the result as a vector $\\mathbf{f} = (f_1, f_2, \\dots, f_k)$. Using the derived expression for each component $f_i$, we have:\n$$ \\mathbf{f} = \\left( \\frac{w_1}{\\sum_{j=1}^{k} w_j}, \\frac{w_2}{\\sum_{j=1}^{k} w_j}, \\dots, \\frac{w_k}{\\sum_{j=1}^{k} w_j} \\right) $$\n\n### Part 3: Discussion on Starvation and Run Intervals\n\n**Starvation:** In this idealized model, starvation for any container $i$ with an assigned weight $w_i > 0$ cannot occur. The scheduler's core principle is to always run the entity with the minimum virtual runtime $v_i$. When any other container $j \\neq i$ runs, its virtual runtime $v_j$ increases, while $v_i$ remains constant. Eventually, the virtual runtimes of all other running containers will surpass $v_i$. At that point, container $i$ will have the minimum virtual runtime among all runnable entities, and the scheduler will be forced to select it for execution. Because $w_i > 0$, the rate of increase of $v_i$ is finite, ensuring it does not remain the minimum indefinitely. This mechanism guarantees that every runnable container will eventually be scheduled.\n\n**Bound on Time Between Runs:** The maximum time between two successive executions of container $i$ can be reasoned about in terms of an idealized fairness window of length $L > 0$. In such a window, container $i$ receives a total run time of $f_i L$. The worst-case scenario for container $i$ occurs immediately after it has finished running, at which point its virtual runtime is maximal relative to the other containers. It must then wait for the other $k-1$ containers to run long enough for one of their virtual runtimes to exceed its own, or more simply, for its own virtual runtime to become the minimum again. A loose but intuitive upper bound on this wait time is the time it would take for all other containers to receive their proportional share of the CPU within the fairness window $L$. This time is the sum of the shares of all other containers, which is $(1 - f_i)L$. Substituting the derived expression for $f_i$, this maximum wait time is bounded by:\n$$ L \\left(1 - \\frac{w_i}{\\sum_{j=1}^{k} w_j}\\right) = L \\frac{\\sum_{j \\neq i} w_j}{\\sum_{j=1}^{k} w_j} $$\nIn a real CFS implementation, this period $L$ is related to the `sched_latency` parameter, which defines the target time period over which every runnable task should run at least once.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{w_1}{\\sum_{j=1}^{k} w_j} & \\frac{w_2}{\\sum_{j=1}^{k} w_j} & \\dots & \\frac{w_k}{\\sum_{j=1}^{k} w_j}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "OS-level virtualization masterfully balances isolation with efficiency, and nowhere is this more apparent than in memory management. The kernel's unified page cache is shared across all containers to avoid redundant data copies and conserve physical memory. This thought experiment  explores the subtleties of page cache sharing and memory accounting with `cgroups`, helping to build an accurate mental model of how container memory usage is managed and reported.",
            "id": "3665429",
            "problem": "Two Linux containers, $C_1$ and $C_2$, run on the same host kernel with Linux control groups version $2$ (cgroups v2). Both containers have a bind mount to the same large read-only file of size $S = 2\\,\\text{GiB}$ on the host, and both perform a sequential buffered read (for example, using the system call $\\text{read}()$ in a loop) until end-of-file. The kernel page size is $p = 4\\,\\text{KiB}$. The memory controller for each container is configured as follows: for $C_1$, $\\text{memory.high} = 600\\,\\text{MiB}$ and $\\text{memory.max} = 1\\,\\text{GiB}$; for $C_2$, $\\text{memory.high} = 400\\,\\text{MiB}$ and $\\text{memory.max} = 512\\,\\text{MiB}$. Assume the file is not in the page cache initially.\n\nUse the following fundamental base to reason about page cache sharing and memory accounting behavior:\n\n- Definition: In a monolithic kernel shared by all containers, the page cache is keyed by the tuple $(\\text{inode}, \\text{offset})$. For any given $(\\text{inode}, \\text{offset})$, at most one cached page exists in the host kernel’s page cache.\n- Fact: Under cgroups v2, the memory controller charges page cache pages to the memory control group (denoted $\\text{memcg}$) that first instantiates them (for example, on the first page fault or readahead that populates the page into the cache). Subsequent accesses by tasks in other $\\text{memcg}$ groups reuse the cached page without duplicating it; the charged owner of the page remains the original $\\text{memcg}$.\n- Fact: $\\text{memory.high}$ is a soft limit. When a $\\text{memcg}$ usage exceeds $\\text{memory.high}$, the kernel attempts reclaim and applies proportional throttling (delaying further charging) rather than immediate killing; exceeding $\\text{memory.high}$ does not, by itself, guarantee an Out-Of-Memory kill.\n- Fact: $\\text{memory.max}$ is a hard limit. If a new charge would exceed $\\text{memory.max}$, the kernel first tries to reclaim within the $\\text{memcg}$; if sufficient reclaim is not possible, the charge fails, and the kernel may trigger an Out-Of-Memory kill within that $\\text{memcg}$.\n\nAssume typical sequential buffered I/O where the kernel performs readahead and can reclaim clean page cache pages behind the read cursor. Let $n = S / p$ be the total number of file pages, and let $w$ denote the approximate steady-state number of file pages charged to a $\\text{memcg}$ while streaming the file (the working set, determined by readahead and pipeline depth), with $w \\ll n$ under normal conditions.\n\nWhich of the following statements are correct in this scenario?\n\nA. The host page cache stores each $(\\text{inode}, \\text{offset})$ page only once, so $C_1$ and $C_2$ reuse the same cached pages; container boundaries do not cause duplicate cache pages for the same file content.\n\nB. Each container maintains its own separate page cache namespace, so reading the same file from both containers duplicates the cache and roughly doubles host memory usage for those pages.\n\nC. If $C_1$ reads first and populates the cache, then when $C_2$ reads the same file afterward, $C_2$’s $\\text{memory.current}$ does not increase by the size of the already cached pages, because those pages remain charged to $C_1$.\n\nD. If a container’s $\\text{memcg}$ usage exceeds $\\text{memory.high}$, the kernel immediately performs an Out-Of-Memory kill within that $\\text{memcg}$; by contrast, exceeding $\\text{memory.max}$ only causes throttling without kill.\n\nE. Because $S = 2\\,\\text{GiB}$ while $C_1$ has $\\text{memory.max} = 1\\,\\text{GiB}$, $C_1$ inevitably fails to read the file to completion: the page cache must hold all $n$ pages concurrently, and the hard limit prevents this.\n\nF. If $C_2$ reads the file after $C_1$ has populated the page cache, $C_2$ can proceed at near full speed without being throttled by its $\\text{memory.high}$, provided $C_2$ does not perform additional allocations that are charged to $C_2$ beyond minor transient kernel overhead.\n\nSelect all that apply.",
            "solution": "The problem statement is internally consistent, scientifically grounded in the principles of the Linux kernel's memory management and control groups (cgroups v2), and well-posed. The provided definitions and facts accurately describe the behavior of the system, and the numerical values are realistic. Therefore, the problem is valid, and we may proceed with the analysis.\n\nThe core principles governing this scenario are as follows:\n1.  **Unified Page Cache**: The Linux kernel on the host maintains a single, global page cache. Any page of any file is uniquely identified by its inode and the offset within that file, represented by the tuple $(\\text{inode}, \\text{offset})$. Regardless of how many processes or containers access it, a given page exists at most once in this cache.\n2.  **Cgroups v2 Memory Accounting**: When a file page is first read from disk and populated into the page cache, its memory cost is \"charged\" to the memory control group ($\\text{memcg}$) of the process that triggered the read. The `memory.current` counter of that $\\text{memcg}$ is incremented. If a process from a different $\\text{memcg}$ later accesses this same page, it reuses the existing cached page, and no new charge is incurred. The memory charge remains with the original $\\text{memcg}$.\n3.  **Streaming I/O**: A sequential read of a large file does not require the entire file to be resident in memory. The kernel reads data in a \"window\" of pages, employing readahead to fetch upcoming pages and reclaiming old pages that are no longer needed (i.e., those \"behind\" the current read position). The concurrent memory usage for file data, denoted $w$, is therefore typically much smaller than the total file size $S$ (i.e., $w \\ll n$, where $n=S/p$).\n4.  **Memory Limits**: For a given $\\text{memcg}$, $\\text{memory.high}$ is a soft limit that triggers memory reclaim and throttling when exceeded. $\\text{memory.max}$ is a hard limit; an attempt to allocate memory that would push usage beyond this limit will fail, potentially leading to an Out-Of-Memory (OOM) kill.\n\nBased on these principles, we evaluate each statement.\n\n**A. The host page cache stores each $(\\text{inode}, \\text{offset})$ page only once, so $C_1$ and $C_2$ reuse the same cached pages; container boundaries do not cause duplicate cache pages for the same file content.**\n\nThis statement is a direct consequence of the fundamental design of the Linux page cache. The problem's \"Definition\" explicitly states: \"In a monolithic kernel shared by all containers, the page cache is keyed by the tuple $(\\text{inode}, \\text{offset})$. For any given $(\\text{inode}, \\text{offset})$, at most one cached page exists...\". Containers $C_1$ and $C_2$ run on the same kernel and access the same file (same inode). Therefore, they share a single instance of each file page in the host kernel's page cache. Containerization isolates many resources, but the page cache is unified to maximize memory efficiency.\n\n**Verdict: Correct**\n\n**B. Each container maintains its own separate page cache namespace, so reading the same file from both containers duplicates the cache and roughly doubles host memory usage for those pages.**\n\nThis statement contradicts the principle of a unified page cache. As established in the analysis of A, Linux does not create separate page caches for each container. Doing so would defeat a primary benefit of memory management: sharing identical data to conserve physical memory. This statement describes a behavior that is fundamentally incorrect for Linux containers.\n\n**Verdict: Incorrect**\n\n**C. If $C_1$ reads first and populates the cache, then when $C_2$ reads the same file afterward, $C_2$’s $\\text{memory.current}$ does not increase by the size of the already cached pages, because those pages remain charged to $C_1$.**\n\nThis statement accurately describes the cgroups v2 memory accounting rule for page cache. The problem \"Fact\" states: \"Under cgroups v2, the memory controller charges page cache pages to the memory control group ($\\text{memcg}$) that first instantiates them... the charged owner of the page remains the original $\\text{memcg}$.\" Since $C_1$ reads the file first, its $\\text{memcg}$ is charged for populating the pages. When $C_2$ subsequently reads the file, it finds the pages in the cache and reuses them. The charge for these pages is not transferred to $C_2$'s $\\text{memcg}$, so $C_2$'s `memory.current` usage will not increase from accessing this pre-existing cached data.\n\n**Verdict: Correct**\n\n**D. If a container’s $\\text{memcg}$ usage exceeds $\\text{memory.high}$, the kernel immediately performs an Out-Of-Memory kill within that $\\text{memcg}$; by contrast, exceeding $\\text{memory.max}$ only causes throttling without kill.**\n\nThis statement incorrectly reverses the functions of $\\text{memory.high}$ and $\\text{memory.max}$. The problem's \"Facts\" clearly define their roles:\n- $\\text{memory.high}$: A soft limit. Exceeding it causes throttling and reclaim, not an OOM kill.\n- $\\text{memory.max}$: A hard limit. Exceeding it can trigger an OOM kill if reclaim is insufficient.\nThis statement claims a kill for `high` and only throttling for `max`, which is the opposite of their actual behavior.\n\n**Verdict: Incorrect**\n\n**E. Because $S = 2\\,\\text{GiB}$ while $C_1$ has $\\text{memory.max} = 1\\,\\text{GiB}$, $C_1$ inevitably fails to read the file to completion: the page cache must hold all $n$ pages concurrently, and the hard limit prevents this.**\n\nThis statement is based on the false premise that reading a file requires holding the entire file in memory simultaneously. The problem describes a \"sequential buffered read,\" which implies a streaming operation. The kernel maintains a moving window of pages in the cache, with size $w$, where $w$ is the working set size. The problem correctly states that for such a workload, $w \\ll n$. As long as the memory for this window ($w \\times p$) plus any other memory used by the container stays below the $\\text{memory.max}$ of $1\\,\\text{GiB}$, the read will proceed. A typical readahead window is on the order of megabytes, far below the $1\\,\\text{GiB}$ limit. Therefore, $C_1$ can successfully stream the $2\\,\\text{GiB}$ file without exceeding its hard limit.\n\n**Verdict: Incorrect**\n\n**F. If $C_2$ reads the file after $C_1$ has populated the page cache, $C_2$ can proceed at near full speed without being throttled by its $\\text{memory.high}$, provided $C_2$ does not perform additional allocations that are charged to $C_2$ beyond minor transient kernel overhead.**\n\nThis statement is a logical consequence of statements A and C. Since $C_1$ has already populated the page cache, $C_2$'s read requests will be served from memory (cache hits), which is extremely fast (\"near full speed\"). Furthermore, as established in C, the memory for these pages remains charged to $C_1$. Thus, $C_2$'s `memory.current` will not increase significantly from reading the file. Since its memory usage remains low, it will not approach its $\\text{memory.high}$ limit of $400\\,\\text{MiB}$. As a result, $C_2$ will not be subject to the memory-pressure-induced throttling associated with exceeding `memory.high`.\n\n**Verdict: Correct**",
            "answer": "$$\\boxed{ACF}$$"
        }
    ]
}