{
    "hands_on_practices": [
        {
            "introduction": "操作系统级虚拟化的核心优势之一是能够精确控制资源分配。此练习旨在揭示Linux控制组（cgroups）中CPU份额设置的底层机制。通过从完全公平调度器（CFS）的基本原理出发，您将推导出在CPU资源竞争激烈时，不同容器如何根据其权重分配到相应的计算时间，这对于理解和配置容器的性能至关重要。",
            "id": "3665364",
            "problem": "您正在一台只有一个中央处理器（CPU）容量的主机上，使用 Linux 控制组（cgroups）配置操作系统级虚拟化。现有 $k$ 个容器，每个容器都有一个单一的、始终可运行的、CPU 密集型进程，并被分配了控制组 CPU 份额 $w_1, w_2, \\dots, w_k$，其中每个 $w_i \\in \\mathbb{R}_{>0}$。该主机使用 Linux 完全公平调度器（CFS）。请仅使用以下关于 CFS 配合 cgroup 份额的基本事实作为您的出发点：\n- 每个可运行实体（此处指每个容器的可调度实体）都维护一个虚拟运行时间 $v_i$。当实体 $i$ 运行了实际时间 $\\Delta t$ 后，其虚拟运行时间增加的量与 $\\Delta t$ 成正比，与其权重 $w_i$ 成反比，因此当 $i$ 运行时，瞬时速率满足 $\\frac{dv_i}{dt} \\propto \\frac{1}{w_i}$，当它不运行时，$\\frac{dv_i}{dt} = 0$。\n- 调度器总是选择 $v_i$ 最小的实体来下一次运行，并随着时间的推移，试图使所有可运行实体的 $v_i$ 保持均等。\n假设在稳态下，总需求严格超过容量（即所有 $k$ 个实体始终保持可运行状态），因此 CPU 被完全利用。在任何可运行实体集合不发生变化的时间范围内，令 $f_i$ 表示分配给容器 $i$ 的 CPU 时间的预期分数，因此 $f_i \\in (0,1)$ 且 $\\sum_{i=1}^{k} f_i = 1$。\n\n任务：\n1. 从上述两个事实出发，且仅基于这两个事实，推导出 $f_i$ 关于 $w_1, \\dots, w_k$ 的稳态表达式。\n2. 将您的最终结果表示为向量 $\\mathbf{f} = (f_1, f_2, \\dots, f_k)$ 的封闭形式解析表达式，作为单个 CPU 的无单位分数。无需进行数值代入。\n3. 简要讨论（评分无需计算）：在这个没有最小粒度效应的理想化模型中，论证任何权重 $w_i > 0$ 的容器是否会发生饥饿，并给出一个关于容器 $i$ 两次运行之间最大时间的界限（用长度为 $L > 0$ 的公平性窗口表示）。\n\n仅提供 $\\mathbf{f}$ 的表达式作为最终答案。由于您的答案是符号化的，因此不需要四舍五入。如要求，分数为无单位的。",
            "solution": "问题陈述已经过验证，被认为是有效的。它科学地基于操作系统调度器的原理，特别是 Linux 完全公平调度器（CFS）的一个理想化模型。该问题提法恰当、客观、自洽且可形式化，从而可以严格推导出所需的表达式。\n\n### 第 1 部分：稳态 CPU 分数 $f_i$ 的推导\n\n该问题提供了控制调度器行为的两个基本事实。我们将仅使用这两个事实来推导分配给容器 $i$ 的 CPU 时间分数 $f_i$ 的表达式。\n\n已知条件如下：\n- 单个 CPU，因此总容量为 $1$。\n- $k$ 个容器，其 CPU 份额为 $w_1, w_2, \\dots, w_k$，其中每个 $w_i \\in \\mathbb{R}_{>0}$。\n- 所有 $k$ 个容器始终可运行。\n- 总 CPU 时间被完全利用，因此 $\\sum_{i=1}^{k} f_i = 1$。\n- 事实 1：对于正在运行的容器 $i$，其虚拟运行时间 $v_i$ 的变化率为 $\\frac{dv_i}{dt} \\propto \\frac{1}{w_i}$。设 $c$ 为正常数比例常数。因此，当容器 $i$ 运行时，其虚拟运行时间以 $\\frac{dv_i}{dt} = \\frac{c}{w_i}$ 的速率增加。当它不运行时，$\\frac{dv_i}{dt} = 0$。\n- 事实 2：调度器总是选择具有最小虚拟运行时间 $v_i$ 的可运行实体，并随着时间的推移，旨在为所有可运行实体 $i, j$ 维持 $v_i \\approx v_j$。\n\n在一个持续时间为 $T$ 的足够长的时间间隔内观察一个稳态系统，调度器为均衡所有虚拟运行时间所做的努力意味着，每个容器的总累积虚拟运行时间 $\\Delta v_i$ 必须相同。\n$$ \\Delta v_1 = \\Delta v_2 = \\dots = \\Delta v_k $$\n\n设 $T_i$ 为容器 $i$ 在此时间间隔 $T$ 内运行的总实际 CPU 时间。容器 $i$ 的 CPU 时间分数为 $f_i$。在我们的时间间隔 $T$ 的背景下，这意味着 $f_i = \\frac{T_i}{T}$，或 $T_i = f_i T$。由于 CPU 始终繁忙，所有容器的运行时间总和必须等于该时间间隔的总持续时间：\n$$ \\sum_{i=1}^{k} T_i = T $$\n\n容器 $i$ 的累积虚拟运行时间 $\\Delta v_i$ 是其变化率在时间间隔 $T$ 上的积分。由于当容器不运行时速率为零，我们只需考虑它以恒定速率 $\\frac{c}{w_i}$ 运行的总时间 $T_i$。\n$$ \\Delta v_i = \\left( \\frac{c}{w_i} \\right) T_i $$\n\n使用所有总累积虚拟运行时间相等的稳态条件，我们可以将任意两个容器 $i$ 和 $j$ 的表达式相等：\n$$ \\Delta v_i = \\Delta v_j $$\n$$ \\left( \\frac{c}{w_i} \\right) T_i = \\left( \\frac{c}{w_j} \\right) T_j $$\n\n由于 $c > 0$，我们可以除以 $c$ 得到：\n$$ \\frac{T_i}{w_i} = \\frac{T_j}{w_j} $$\n这表明总运行时间与权重的比率对所有容器来说是一个常数。我们用 $K_T$ 表示这个常数。\n$$ \\frac{T_i}{w_i} = K_T \\implies T_i = K_T w_i $$\n\n现在，我们使用 CPU 被完全利用的约束条件。我们将 $T_i$ 的表达式代入总和中：\n$$ \\sum_{i=1}^{k} (K_T w_i) = T $$\n$$ K_T \\sum_{i=1}^{k} w_i = T $$\n\n我们可以解出比例常数 $K_T$：\n$$ K_T = \\frac{T}{\\sum_{j=1}^{k} w_j} $$\n注意，由于所有 $w_j > 0$，分母 $\\sum_{j=1}^{k} w_j$ 是严格为正的。\n\n现在我们将 $K_T$ 的这个表达式代回到 $T_i$ 的方程中：\n$$ T_i = \\left( \\frac{T}{\\sum_{j=1}^{k} w_j} \\right) w_i $$\n\n最后，我们通过将 $T_i$ 除以 $T$ 来求得 CPU 时间的分数 $f_i$：\n$$ f_i = \\frac{T_i}{T} = \\frac{1}{T} \\left( \\frac{T w_i}{\\sum_{j=1}^{k} w_j} \\right) = \\frac{w_i}{\\sum_{j=1}^{k} w_j} $$\n这就是分配给容器 $i$ 的 CPU 时间分数的稳态表达式。\n\n### 第 2 部分：$\\mathbf{f}$ 的向量表达式\n\n问题要求将结果表示为向量 $\\mathbf{f} = (f_1, f_2, \\dots, f_k)$。使用每个分量 $f_i$ 的推导表达式，我们得到：\n$$ \\mathbf{f} = \\left( \\frac{w_1}{\\sum_{j=1}^{k} w_j}, \\frac{w_2}{\\sum_{j=1}^{k} w_j}, \\dots, \\frac{w_k}{\\sum_{j=1}^{k} w_j} \\right) $$\n\n### 第 3 部分：关于饥饿和运行间隔的讨论\n\n**饥饿：** 在这个理想化模型中，任何分配了权重 $w_i > 0$ 的容器 $i$ 都不会发生饥饿。调度器的核心原则是始终运行虚拟运行时间 $v_i$ 最小的实体。当任何其他容器 $j \\neq i$ 运行时，其虚拟运行时间 $v_j$ 增加，而 $v_i$ 保持不变。最终，所有其他正在运行的容器的虚拟运行时间都将超过 $v_i$。届时，容器 $i$ 将在所有可运行实体中拥有最小的虚拟运行时间，调度器将被迫选择它来执行。因为 $w_i > 0$，所以 $v_i$ 的增长率是有限的，这确保了它不会无限期地保持最小值。这个机制保证了每个可运行的容器最终都会被调度。\n\n**两次运行之间的时限：** 容器 $i$ 两次连续执行之间的最大时间可以从一个长度为 $L > 0$ 的理想化公平性窗口的角度来推断。在这样一个窗口内，容器 $i$ 获得的总运行时间为 $f_i L$。对容器 $i$ 来说，最坏的情况发生在它刚运行完毕时，此时它的虚拟运行时间相对于其他容器是最大的。然后它必须等待其他 $k-1$ 个容器运行足够长的时间，直到其中一个的虚拟运行时间超过它自己，或者更简单地说，直到它自己的虚拟运行时间再次成为最小值。这个等待时间的一个宽松但直观的上限是，在公平性窗口 $L$ 内，所有其他容器获得其 CPU 比例份额所需的时间。这个时间是所有其他容器份额的总和，即 $(1 - f_i)L$。代入推导出的 $f_i$ 表达式，这个最大等待时间有界于：\n$$ L \\left(1 - \\frac{w_i}{\\sum_{j=1}^{k} w_j}\\right) = L \\frac{\\sum_{j \\neq i} w_j}{\\sum_{j=1}^{k} w_j} $$\n在 CFS 的实际实现中，这个周期 $L$ 与 `sched_latency` 参数相关，该参数定义了每个可运行任务至少应运行一次的目标时间段。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{w_1}{\\sum_{j=1}^{k} w_j}  \\frac{w_2}{\\sum_{j=1}^{k} w_j}  \\dots  \\frac{w_k}{\\sum_{j=1}^{k} w_j}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "容器的内存隔离是一个充满细节的领域，远不止是设置一个内存上限那么简单。此练习将探讨一个常见但关键的场景：多个容器读取同一个文件。通过分析此场景，您将理解Linux内核如何通过统一的页缓存（page cache）高效地共享内存，同时cgroups如何对这些共享页面进行记账，从而揭示了`memory.high`和`memory.max`等限制的实际效果。",
            "id": "3665429",
            "problem": "两个 Linux 容器 $C_1$ 和 $C_2$ 运行在同一个主机内核上，使用 Linux 控制组版本 $2$ (cgroups v2)。两个容器都有一个到主机上同一个大小为 $S = 2\\,\\text{GiB}$ 的大型只读文件的绑定挂载，并且两者都执行顺序缓冲读取（例如，在循环中使用系统调用 $\\text{read}()$），直到文件末尾。内核页面大小为 $p = 4\\,\\text{KiB}$。每个容器的内存控制器配置如下：对于 $C_1$，$\\text{memory.high} = 600\\,\\text{MiB}$ 且 $\\text{memory.max} = 1\\,\\text{GiB}$；对于 $C_2$，$\\text{memory.high} = 400\\,\\text{MiB}$ 且 $\\text{memory.max} = 512\\,\\text{MiB}$。假设文件最初不在页面缓存中。\n\n使用以下基本原则来推断页面缓存共享和内存记账行为：\n\n- 定义：在所有容器共享的单体内核中，页面缓存由元组 $(\\text{inode}, \\text{offset})$ 索引。对于任何给定的 $(\\text{inode}, \\text{offset})$，主机内核的页面缓存中最多只存在一个缓存页面。\n- 事实：在 cgroups v2下，内存控制器将页面缓存页面的费用记在最先实例化它们的内存控制组（表示为 $\\text{memcg}$）的账上（例如，在第一次缺页或预读将页面填充到缓存中时）。其他 $\\text{memcg}$ 组中的任务后续访问会重用该缓存页面而不会复制它；页面的记账所有者仍然是最初的 $\\text{memcg}$。\n- 事实：$\\text{memory.high}$ 是一个软限制。当一个 $\\text{memcg}$ 的使用量超过 $\\text{memory.high}$ 时，内核会尝试回收并应用按比例节流（延迟进一步的记账），而不是立即杀死进程；超过 $\\text{memory.high}$ 本身并不保证会发生内存不足（Out-Of-Memory）杀死。\n- 事实：$\\text{memory.max}$ 是一个硬限制。如果一笔新的记账会导致使用量超过 $\\text{memory.max}$，内核会首先尝试在该 $\\text{memcg}$ 内部进行回收；如果无法回收足够的内存，记账将失败，内核可能会在该 $\\text{memcg}$ 内触发内存不足（Out-Of-Memory）杀死。\n\n假设是典型的顺序缓冲 I/O，其中内核执行预读，并能在读取光标后回收干净的页面缓存页面。设 $n = S / p$ 为文件页面的总数，并设 $w$ 表示在流式读取文件时记入一个 $\\text{memcg}$ 的文件页面的近似稳态数量（即工作集，由预读和流水线深度决定），在正常情况下 $w \\ll n$。\n\n在这种情况下，下列哪些陈述是正确的？\n\nA. 主机页面缓存对每个 $(\\text{inode}, \\text{offset})$ 页面只存储一次，因此 $C_1$ 和 $C_2$ 重用相同的缓存页面；容器边界不会对相同的文件内容导致重复的缓存页面。\n\nB. 每个容器维护自己独立的页面缓存命名空间，因此从两个容器读取相同的文件会复制缓存，并大致使这些页面的主机内存使用量翻倍。\n\nC. 如果 $C_1$ 首先读取并填充缓存，那么当 $C_2$ 稍后读取相同文件时，$C_2$ 的 $\\text{memory.current}$ 不会因已缓存页面的大小而增加，因为这些页面仍然记在 $C_1$ 的账上。\n\nD. 如果一个容器的 $\\text{memcg}$ 使用量超过 $\\text{memory.high}$，内核会立即在该 $\\text{memcg}$ 内执行内存不足（OOM）杀死；相反，超过 $\\text{memory.max}$ 只会导致节流而不会杀死。\n\nE. 因为 $S = 2\\,\\text{GiB}$ 而 $C_1$ 的 $\\text{memory.max} = 1\\,\\text{GiB}$，$C_1$ 必然无法完成文件读取：页面缓存必须同时持有所有 $n$ 个页面，而硬限制阻止了这一点。\n\nF. 如果 $C_2$ 在 $C_1$ 填充页面缓存后读取该文件，$C_2$ 可以接近全速进行，而不会被其 $\\text{memory.high}$ 节流，前提是除了微小的瞬时内核开销外，$C_2$ 不执行任何计入 $C_2$ 账上的额外分配。\n\n选择所有适用项。",
            "solution": "问题陈述内部一致，科学上基于 Linux 内核内存管理和控制组 (cgroups v2) 的原理，并且定义良好。所提供的定义和事实准确地描述了系统行为，数值也是现实的。因此，该问题是有效的，我们可以进行分析。\n\n支配此场景的核心原则如下：\n1.  **统一页面缓存**：主机上的 Linux 内核维护一个单一的、全局的页面缓存。任何文件的任何页面都由其 inode 和文件内偏移量唯一标识，表示为元组 $(\\text{inode}, \\text{offset})$。无论有多少进程或容器访问它，一个给定的页面在此缓存中最多只存在一次。\n2.  **Cgroups v2 内存记账**：当一个文件页面首次从磁盘读取并填充到页面缓存中时，其内存成本会“记账”到触发读取的进程所属的内存控制组（$\\text{memcg}$）上。该 $\\text{memcg}$ 的 `memory.current` 计数器会增加。如果来自不同 $\\text{memcg}$ 的进程稍后访问同一个页面，它会重用已存在的缓存页面，并且不会产生新的记账。内存费用仍归属于最初的 $\\text{memcg}$。\n3.  **流式 I/O**：顺序读取一个大文件不需要将整个文件都驻留在内存中。内核在一个页面的“窗口”中读取数据，采用预读来获取即将到来的页面，并回收不再需要的旧页面（即那些在当前读取位置“后面”的页面）。因此，文件数据的并发内存使用量 $w$ 通常远小于总文件大小 $S$（即 $w \\ll n$，其中 $n=S/p$）。\n4.  **内存限制**：对于给定的 $\\text{memcg}$，$\\text{memory.high}$ 是一个软限制，当超过该限制时会触发内存回收和节流。$\\text{memory.max}$ 是一个硬限制；尝试分配内存若会导致使用量超过此限制将会失败，并可能导致内存不足（OOM）杀死。\n\n基于这些原则，我们评估每个陈述。\n\n**A. 主机页面缓存对每个 $(\\text{inode}, \\text{offset})$ 页面只存储一次，因此 $C_1$ 和 $C_2$ 重用相同的缓存页面；容器边界不会对相同的文件内容导致重复的缓存页面。**\n\n这个陈述是 Linux 页面缓存基本设计的直接结果。问题的“定义”明确指出：“在所有容器共享的单体内核中，页面缓存由元组 $(\\text{inode}, \\text{offset})$ 索引。对于任何给定的 $(\\text{inode}, \\text{offset})$，……最多只存在一个缓存页面”。容器 $C_1$ 和 $C_2$ 在同一个内核上运行并访问同一个文件（相同的 inode）。因此，它们在主机内核的页面缓存中共享每个文件页面的单个实例。容器化隔离了许多资源，但页面缓存是统一的，以最大化内存效率。\n\n**结论：正确**\n\n**B. 每个容器维护自己独立的页面缓存命名空间，因此从两个容器读取相同的文件会复制缓存，并大致使这些页面的主机内存使用量翻倍。**\n\n这个陈述与统一页面缓存的原则相矛盾。正如在对 A 的分析中确立的，Linux 不会为每个容器创建独立的页面缓存。这样做会破坏内存管理的一个主要好处：共享相同数据以节省物理内存。这个陈述描述的行为对于 Linux 容器来说是根本不正确的。\n\n**结论：不正确**\n\n**C. 如果 $C_1$ 首先读取并填充缓存，那么当 $C_2$ 稍后读取相同文件时，$C_2$ 的 $\\text{memory.current}$ 不会因已缓存页面的大小而增加，因为这些页面仍然记在 $C_1$ 的账上。**\n\n这个陈述准确地描述了 cgroups v2 对页面缓存的内存记账规则。问题的“事实”指出：“在 cgroups v2下，内存控制器将页面缓存页面的费用记在最先实例化它们的内存控制组（$\\text{memcg}$）的账上……页面的记账所有者仍然是最初的 $\\text{memcg}$。” 由于 $C_1$ 首先读取文件，其 $\\text{memcg}$ 会为填充页面而被记账。当 $C_2$ 随后读取文件时，它会在缓存中找到这些页面并重用它们。这些页面的费用不会转移到 $C_2$ 的 $\\text{memcg}$，因此 $C_2$ 的 `memory.current` 使用量不会因为访问这些已存在的缓存数据而增加。\n\n**结论：正确**\n\n**D. 如果一个容器的 $\\text{memcg}$ 使用量超过 $\\text{memory.high}$，内核会立即在该 $\\text{memcg}$ 内执行内存不足（OOM）杀死；相反，超过 $\\text{memory.max}$ 只会导致节流而不会杀死。**\n\n这个陈述错误地颠倒了 $\\text{memory.high}$ 和 $\\text{memory.max}$ 的功能。问题的“事实”清楚地定义了它们的作用：\n- $\\text{memory.high}$：软限制。超过它会导致节流和回收，而不是 OOM 杀死。\n- $\\text{memory.max}$：硬限制。如果回收不足，超过它可以触发 OOM 杀死。\n这个陈述声称 `high` 会导致杀死，而 `max` 只会导致节流，这与它们的实际行为相反。\n\n**结论：不正确**\n\n**E. 因为 $S = 2\\,\\text{GiB}$ 而 $C_1$ 的 $\\text{memory.max} = 1\\,\\text{GiB}$，$C_1$ 必然无法完成文件读取：页面缓存必须同时持有所有 $n$ 个页面，而硬限制阻止了这一点。**\n\n这个陈述基于一个错误的前提，即读取文件需要将整个文件同时保存在内存中。问题描述了“顺序缓冲读取”，这意味着一个流式操作。内核在缓存中维护一个移动的页面窗口，其大小为 $w$，其中 $w$ 是工作集大小。问题正确地指出，对于这样的工作负载，$w \\ll n$。只要这个窗口的内存（$w \\times p$）加上容器使用的任何其他内存保持在 $1\\,\\text{GiB}$ 的 $\\text{memory.max}$ 限制以下，读取就会继续进行。一个典型的预读窗口大小在兆字节级别，远低于 $1\\,\\text{GiB}$ 的限制。因此，$C_1$ 可以在不超出其硬限制的情况下成功地流式读取 $2\\,\\text{GiB}$ 的文件。\n\n**结论：不正确**\n\n**F. 如果 $C_2$ 在 $C_1$ 填充页面缓存后读取该文件，$C_2$ 可以接近全速进行，而不会被其 $\\text{memory.high}$ 节流，前提是除了微小的瞬时内核开销外，$C_2$ 不执行任何计入 $C_2$ 账上的额外分配。**\n\n这个陈述是陈述 A 和 C 的逻辑推论。由于 $C_1$ 已经填充了页面缓存，$C_2$ 的读取请求将从内存中服务（缓存命中），速度极快（“接近全速”）。此外，正如在 C 中确立的，这些页面的内存仍然记在 $C_1$ 的账上。因此，$C_2$ 的 `memory.current` 不会因读取文件而显著增加。由于其内存使用量保持在较低水平，它不会接近其 $400\\,\\text{MiB}$ 的 $\\text{memory.high}$ 限制。因此，$C_2$ 不会受到因超过 $\\text{memory.high}$ 而引发的内存压力节流的影响。\n\n**结论：正确**",
            "answer": "$$\\boxed{ACF}$$"
        },
        {
            "introduction": "由于所有容器共享宿主机的内核，系统调用（syscall）接口成为了一个关键的边界，既关系到兼容性，也关系到安全性。本练习模拟了一个现实世界中的挑战：当容器内的应用程序与宿主机内核版本不兼容时会发生什么。通过分析安全计算模式（seccomp）配置文件如何与GNU C库（glibc）的备用机制（fallback）相互作用，您将学会如何构建既安全又具有良好兼容性的容器环境。",
            "id": "3665412",
            "problem": "一个容器化应用程序基于新版 GNU C 库 (glibc) 2.36 构建，并使用了较新的 Linux 系统调用，例如 `openat2` 和 `pidfd_open`。该容器运行在一个 x86_64 主机上，其 Linux 内核版本为 5.4，该版本未实现那些新的系统调用。容器运行时应用了一个默认的安全计算模式（`seccomp`）配置文件，其对任何未列出的系统调用的默认操作是向调用者返回一个错误（即，过滤器使用的操作等效于“返回错误”，而不是“允许”或“终止”）。假设该过滤器明确拒绝 `openat2` 和 `pidfd_open`，并允许一个常规的基线调用集，例如 `openat`、`fstat`、`mmap`、`mprotect`、`futex` 和 `rt_sigaction`。\n\n从基本原理出发，考虑以下事实：\n\n- 在操作系统级虚拟化中，容器内的进程共享相同的主机内核及其系统调用应用程序二进制接口（ABI）。不存在独立的容器内核。\n- 系统调用是通过一个明确定义的系统调用 ABI 从用户空间到内核的调用。如果内核未实现所请求的系统调用，内核会返回 `-1` 并将全局错误指示符 `errno` 设置为 `ENOSYS`（函数未实现）。如果内核通过 `seccomp` 拒绝了用于沙箱的调用，内核会返回 `-1` 并根据过滤器选择的错误操作设置 `errno`（通常是 `EPERM`，表示操作不允许），或者根据所选操作传递一个信号。\n- GNU C 库 (glibc) 为系统调用提供包装器，并且在检测到 `ENOSYS`（表示缺少实现）时，可能会尝试回退到更旧、被广泛实现的系统调用序列。当错误为 `EPERM`（表示权限失败）时，它不会执行此类回退，因为 `EPERM` 表示策略拒绝，而非实现缺失。\n\n请预测当应用程序在该主机上、在所述的 `seccomp` 配置文件下尝试使用 `openat2` 时的行为，并选择那个既能正确解释结果，又能推荐一个安全的配置文件调整方案（该方案在保留沙箱功能的同时实现兼容性）的选项。\n\nA. 因为容器共享主机内核，在 Linux 5.4 上，当 `seccomp` 的默认“返回错误”操作将 `errno = EPERM` 时，`openat2` 调用将返回 `-1` 且 `errno` 为 `EPERM`，glibc 不会回退。一个安全的兼容性修正是将针对较新系统调用（如 `openat2` 和 `pidfd_open`）的 `seccomp` 操作更改为“返回 `ENOSYS`”，并明确允许其旧的等效调用（例如，回退所需的 `openat` 及相关调用），同时对真正有风险的调用保持限制性操作。\n\nB. 容器拥有独立于主机的内核，因此 glibc 2.36 将能看到正确的实现，`seccomp` 配置文件对 `openat2` 没有影响；调用会成功。无需更改配置文件。\n\nC. glibc 在调用失败时总会回退，包括在遇到 `EPERM` 时，因此拒绝 `openat2` 是无害的。正确的安全配置文件是继续为 `openat2` 和 `pidfd_open` 返回 `EPERM`，而不允许任何旧的等效调用。\n\nD. 通过 `seccomp` 拒绝 `openat2` 将传递一个 `SIGSYS` 信号，glibc 会自动捕获该信号并使用 `openat` 进行模拟，因此不需要特定的白名单更改；最安全的配置文件是捕获所有新调用，并依赖 glibc 在用户空间中模拟它们。\n\n选择唯一的最佳选项。",
            "solution": "用户希望我扮演一位严谨的STEM教授，验证所给的问题陈述，然后解决它。\n\n### 问题验证\n\n首先，我将根据指定标准验证问题陈述。\n\n**步骤1：提取已知条件**\n\n问题陈述提供了以下信息：\n*   一个应用程序基于 GNU C 库 (glibc) 版本 `2.36` 构建。\n*   该应用程序使用较新的 Linux 系统调用：`openat2` 和 `pidfd_open`。\n*   托管应用程序的容器运行在 Linux 内核版本为 `5.4` 的 `x86_64` 主机上。\n*   主机内核（版本 `5.4`）未实现系统调用 `openat2` 和 `pidfd_open`。\n*   容器运行时使用安全计算模式（`seccomp`）配置文件。\n*   `seccomp` 配置文件对于未列入白名单的系统调用的默认操作是返回错误。\n*   `seccomp` 过滤器明确拒绝 `openat2` 和 `pidfd_open`。\n*   `seccomp` 过滤器允许一个常规系统调用的基线集，包括 `openat`、`fstat`、`mmap`、`mprotect`、`futex` 和 `rt_sigaction`。\n\n问题还提供了三个基本原理：\n1.  操作系统级虚拟化（容器化）涉及共享主机内核及其系统调用应用程序二进制接口（ABI）；不存在独立的容器内核。\n2.  一个未实现的系统调用会导致内核返回 `-1`，并将全局错误指示符 `errno` 设置为 `ENOSYS`。一个被 `seccomp` 过滤器以“返回错误”操作阻止的系统调用，会导致内核返回 `-1`，并根据过滤器的操作设置 `errno`（通常是 `EPERM`）。其他 `seccomp` 操作，如传递信号，也是可能的。\n3.  GNU C 库（`glibc`）在检测到系统调用失败且 `errno` 被设置为 `ENOSYS` 时，可能会使用回退机制（例如，尝试旧的系统调用）。当 `errno` 为 `EPERM` 时，它不会执行此类回退，因为这表示策略拒绝。\n\n**步骤2：使用提取的已知条件进行验证**\n\n我现在将评估问题的有效性：\n\n*   **科学依据：** 该问题牢固地植根于操作系统的原理，特别是 Linux 内核行为、系统调用接口、容器化和 `seccomp` 沙箱机制。所描述的 `glibc`、`seccomp` 和内核关于系统调用处理和错误码（`ENOSYS` vs `EPERM`）的行为在事实上是正确的，并代表了一个真实的软件工程挑战。例如，`openat2` 是在 Linux 内核 `5.6` 中引入的，因此在内核 `5.4` 中确实不存在。`glibc` 关于系统调用回退的行为是该库有据可查的一个方面。\n*   **问题明确：** 问题提供了一套完整的条件和清晰陈述的原理。问题要求预测行为并推荐一个安全的修复方案，可以从所提供的信息中推导出一个唯一的、合乎逻辑的解决方案。\n*   **客观性：** 问题以精确、客观、技术性的语言陈述，没有歧义或主观论断。\n\n问题陈述没有表现出任何列出的缺陷（例如，科学上不健全、不完整、矛盾或模棱两可）。它描述了在容器化环境中遇到的一个标准的、不容忽视的兼容性问题。\n\n**步骤3：结论与行动**\n\n问题陈述是**有效的**。我将继续进行解答推导。\n\n### 解答推导\n\n分析过程通过跟踪一个系统调用从应用程序到内核再返回的执行流程来进行。\n\n1.  **应用程序调用与 `glibc` 包装器：** 与 `glibc` 版本 `2.36` 链接的应用程序进行一个需要打开文件的库调用（例如 `open()`）。因为 `glibc` `2.36` 是现代版本，其包装函数将尝试使用为此目的可用的功能最丰富的底层系统调用，即 `openat2`。\n2.  **系统调用：** `glibc` 包装器执行 `syscall` 指令，从用户模式转换到内核模式，请求 `openat2` 系统调用。\n3.  **内核侧的 `seccomp` 拦截：** 根据原理1，该系统调用被导向共享的主机内核。在内核的主系统调用分派器处理该请求之前，与容器进程关联的 `seccomp-bpf` 过滤器会拦截此调用。\n4.  **`seccomp` 过滤器操作：** 问题陈述指出 `seccomp` 配置文件以“返回错误”操作明确拒绝 `openat2`。如原理2所述，此操作指示内核立即终止系统调用处理，并向用户空间调用者返回一个错误。此类拒绝的典型 `errno` 值是 `EPERM`（操作不允许）。调用在此阶段被拒绝，永远不会到达内核检查 `openat2` 是否已实现的代码路径。因此，此时不会生成 `ENOSYS`。\n5.  **返回用户空间：** 内核将控制权返回给 `glibc` 包装函数。返回值为 `-1`，且 `errno` 被设置为 `EPERM`。\n6.  **`glibc` 回退逻辑：** `glibc` 包装函数检查 `errno` 值以决定其下一步操作。根据原理3，`glibc` 区分 `ENOSYS`（此内核上函数未实现）和 `EPERM`（策略拒绝操作）。当看到 `EPERM` 时，`glibc` 推断这是一个安全限制，*不会* 尝试回退到像 `openat` 这样的旧系统调用。相反，它会保留 `EPERM` 错误并向应用程序返回 `-1`。\n7.  **结果：** 应用程序的文件打开操作失败，并出现 `Operation not permitted`（操作不允许）错误。除非应用程序经过专门编码以处理这种针对常见操作的意外 `EPERM` 错误，否则它很可能会出现故障或终止。\n\n### 建议的安全配置文件调整\n\n为了在实现兼容性的同时保留最小权限原则，必须调整 `seccomp` 配置文件。目标是让 `glibc` 的回退机制能按预期工作。\n\n1.  问题在于 `EPERM` 错误码。为了触发 `glibc` 回退，`seccomp` 过滤器应改为返回 `ENOSYS`。`seccomp` BPF 工具允许过滤器使用 `SECCOMP_RET_ERRNO` 操作返回特定的错误码。\n2.  应修改 `seccomp` 配置文件，将对 `openat2`（以及其他类似的新系统调用，如 `pidfd_open`）的操作从 `SECCOMP_RET_ERRNO | EPERM`（或其等效操作）更改为 `SECCOMP_RET_ERRNO | ENOSYS`。\n3.  做出此更改后，当应用程序尝试调用 `openat2` 时，`seccomp` 过滤器将导致内核返回 `-1`，并将 `errno` 设置为 `ENOSYS`。\n4.  `glibc` 将看到 `ENOSYS`，并根据原理3，正确推断出内核版本较旧。然后它将自动尝试回退到一个更旧的、等效的系统调用，例如 `openat`。\n5.  问题陈述指出 `seccomp` 配置文件明确允许 `openat`，并且主机内核 `5.4` 实现了它。因此，回退调用将会成功。\n6.  这种方法是安全的，因为它维持了一个严格的允许系统调用白名单。它没有扩大攻击面；它只是为特定的、已知不存在的系统调用更改了拒绝信号，以启用 `glibc` 内部一个明确定义且安全的兼容性层，同时继续阻止真正危险或不需要的系统调用。\n\n### 逐项分析选项\n\n*   **A. 因为容器共享主机内核，在 Linux 5.4 上，当 `seccomp` 的默认“返回错误”操作将 `errno = EPERM` 时，`openat2` 调用将返回 `-1` 且 `errno` 为 `EPERM`，glibc 不会回退。一个安全的兼容性修正是将针对较新系统调用（如 `openat2` 和 `pidfd_open`）的 `seccomp` 操作更改为“返回 `ENOSYS`”，并明确允许其旧的等效调用（例如，回退所需的 `openat` 及相关调用），同时对真正有风险的调用保持限制性操作。**\n    此选项正确描述了整个事件序列：`seccomp` 过滤器返回 `EPERM`，这阻止了 `glibc` 回退。然后，它正确地指出了标准的解决方案：将 `seccomp` 返回码更改为 `ENOSYS` 以触发 `glibc` 回退，同时确保回退目标系统调用（`openat`）在白名单上。这保留了沙箱的安全态势。\n    **结论：正确。**\n\n*   **B. 容器拥有独立于主机的内核，因此 glibc 2.36 将能看到正确的实现，`seccomp` 配置文件对 `openat2` 没有影响；调用会成功。无需更改配置文件。**\n    此选项有根本性错误。它错误地描述了操作系统级虚拟化，声称容器拥有独立的内核。这与原理1以及容器的定义相矛盾。容器共享主机内核。\n    **结论：错误。**\n\n*   **C. glibc 在调用失败时总会回退，包括在遇到 `EPERM` 时，因此拒绝 `openat2` 是无害的。正确的安全配置文件是继续为 `openat2` 和 `pidfd_open` 返回 `EPERM`，而不允许任何旧的等效调用。**\n    此选项对 `glibc` 的行为做出了错误断言。它与原理3相矛盾，该原理明确指出 `glibc` 在遇到 `EPERM` 错误时*不会*回退。因此，用 `EPERM` 拒绝 `openat2` 不是无害的；它会破坏应用程序。\n    **结论：错误。**\n\n*   **D. 通过 `seccomp` 拒绝 `openat2` 将传递一个 `SIGSYS` 信号，glibc 会自动捕获该信号并使用 `openat` 进行模拟，因此不需要特定的白名单更改；最安全的配置文件是捕获所有新调用，并依赖 glibc 在用户空间中模拟它们。**\n    当问题描述的操作是返回一个错误码时，此选项错误地假设 `seccomp` 的操作是 `trap`（它会生成 `SIGSYS`）。虽然 `seccomp` 可以配置为生成 `SIGSYS`，但这并非问题中所描述的情况。标准的“返回错误”操作映射到 `SECCOMP_RET_ERRNO`。针对此场景的、最可靠且预期的主要 `glibc` 兼容性机制依赖于 `ENOSYS`，而不是信号捕获和模拟。\n    **结论：错误。**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}