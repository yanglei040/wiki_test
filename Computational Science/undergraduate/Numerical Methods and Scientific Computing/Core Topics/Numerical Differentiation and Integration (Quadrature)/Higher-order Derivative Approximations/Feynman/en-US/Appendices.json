{
    "hands_on_practices": [
        {
            "introduction": "The foundation of numerical differentiation lies in approximating derivatives using function values at discrete points. This practice guides you through the process of deriving some of the most common finite difference stencils directly from Taylor series expansions. By implementing these formulas and comparing their output to known analytical derivatives, you will computationally verify their theoretical orders of accuracy, a fundamental skill in validating any numerical method .",
            "id": "3238903",
            "problem": "Consider the function $f(x) = \\exp(-x^2)$. Your task is to approximate higher derivatives using centered finite difference stencils, derive the truncation error order from first principles, and verify the observed convergence rates against analytic derivatives.\n\nYou must proceed from the following foundational base:\n- Use Taylor series expansions of $f(x \\pm kh)$ about a point $x = x_0$, where $k \\in \\mathbb{Z}$ and $h > 0$ is small, to construct consistent centered finite difference approximations for derivatives. Specifically, use the Taylor expansion\n$$\nf(x_0 \\pm kh) = \\sum_{n=0}^{\\infty} \\frac{(\\pm kh)^n}{n!} f^{(n)}(x_0).\n$$\n- Use the definition of truncation error order: if an approximation $A_h$ to a quantity $Q$ satisfies $A_h = Q + C h^p + \\mathcal{O}(h^{p+1})$ for some constant $C \\neq 0$, then we say the approximation is of order $p$.\n- Use the definition of observed order between two step sizes $h_1$ and $h_2$ for errors $E(h_1)$ and $E(h_2)$ as\n$$\np_{\\mathrm{obs}} = \\frac{\\log\\big(E(h_1)/E(h_2)\\big)}{\\log(h_1/h_2)}.\n$$\n\nTasks:\n1. Derive symbolic expressions for the analytic second and fourth derivatives $f^{(2)}(x)$ and $f^{(4)}(x)$ for $f(x) = \\exp(-x^2)$ by direct differentiation.\n2. Using Taylor series, derive centered finite difference stencils and their leading truncation errors for:\n   - the standard centered $3$-point approximation to $f^{(2)}(x_0)$,\n   - the centered $5$-point approximation to $f^{(2)}(x_0)$ with higher accuracy than the $3$-point stencil,\n   - the centered $5$-point approximation to $f^{(4)}(x_0)$.\n   Do not assume any pre-known coefficients; derive them by matching Taylor series terms at $x_0$.\n3. Implement a program that evaluates the approximations at specific points and verifies the convergence order by computing the error against the analytic derivatives from Task $1$ and the observed orders from the error ratios as $h$ halves.\n\nTest suite:\n- Use the grid spacing sequence $h_k = 0.2 \\cdot 2^{-k}$ for $k \\in \\{0,1,2,3,4,5\\}$.\n- For each case below, compute the error $E(h_k)$ as the absolute difference between the numerical approximation and the analytic derivative at $x_0$, then compute the observed orders $p_k$ for $k \\in \\{1,2,3,4,5\\}$ using successive pairs $(h_{k-1}, h_k)$.\n- For robustness, define the reported observed order for each case as the median of the last three values $\\{p_3, p_4, p_5\\}$.\n- Verify the following four cases with their expected orders and points:\n  - Case A: centered $3$-point stencil for $f^{(2)}(x_0)$ at $x_0 = 0.7$, expected order $2$.\n  - Case B: centered $5$-point stencil for $f^{(2)}(x_0)$ at $x_0 = 1.3$, expected order $4$.\n  - Case C: centered $5$-point stencil for $f^{(4)}(x_0)$ at $x_0 = 0.0$, expected order $2$.\n  - Case D (edge magnitude case): centered $5$-point stencil for $f^{(2)}(x_0)$ at $x_0 = 2.0$, expected order $4$.\n\nPass-fail criterion for each case:\n- Let $\\tilde{p}$ be the reported observed order (median of $\\{p_3, p_4, p_5\\}$). The case passes if $|\\tilde{p} - p_{\\mathrm{expected}}| \\le 0.3$ and fails otherwise, where $p_{\\mathrm{expected}}$ is the expected order for that case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Case A, Case B, Case C, Case D], where each entry is the boolean result for that case, for example, \"[True,True,False,True]\".\n- No user input is required, and no physical units or angle units apply in this problem. All numerical values are dimensionless.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in numerical analysis, well-posed with a clear objective, and free from inconsistencies or ambiguities. We proceed with a complete solution.\n\nThe solution is presented in three parts, corresponding to the tasks outlined in the problem statement. First, we derive the analytic expressions for the required derivatives. Second, we derive the finite difference stencils and their truncation errors using Taylor series. Third, we outline the numerical verification procedure that will be implemented in the final code.\n\n### Part 1: Analytic Derivatives\n\nWe are given the function $f(x) = \\exp(-x^2)$. We will compute its second and fourth derivatives, $f^{(2)}(x)$ and $f^{(4)}(x)$, by direct differentiation. Let us denote derivatives with respect to $x$ using prime notation for brevity in the intermediate steps, i.e., $f'(x)$, $f''(x)$, etc.\n\nThe first derivative is:\n$$\nf'(x) = \\frac{d}{dx} \\exp(-x^2) = -2x \\exp(-x^2) = -2x f(x)\n$$\n\nThe second derivative is found using the product rule:\n$$\nf''(x) = \\frac{d}{dx} (-2x f(x)) = -2 f(x) - 2x f'(x)\n$$\nSubstituting $f'(x) = -2x f(x)$:\n$$\nf''(x) = -2 f(x) - 2x (-2x f(x)) = (-2 + 4x^2) f(x)\n$$\nSo, the analytic expression for the second derivative is:\n$$\nf^{(2)}(x) = (4x^2 - 2) \\exp(-x^2)\n$$\n\nTo find the fourth derivative, we first compute the third derivative:\n$$\nf'''(x) = \\frac{d}{dx} \\left( (4x^2 - 2) f(x) \\right) = (8x) f(x) + (4x^2 - 2) f'(x)\n$$\nSubstituting $f'(x) = -2x f(x)$:\n$$\nf'''(x) = 8x f(x) + (4x^2 - 2) (-2x f(x)) = (8x - 8x^3 + 4x) f(x) = (-8x^3 + 12x) f(x)\n$$\n\nFinally, the fourth derivative is computed by differentiating $f'''(x)$:\n$$\nf^{(4)}(x) = \\frac{d}{dx} \\left( (-8x^3 + 12x) f(x) \\right) = (-24x^2 + 12) f(x) + (-8x^3 + 12x) f'(x)\n$$\nSubstituting $f'(x) = -2x f(x)$:\n$$\nf^{(4)}(x) = (-24x^2 + 12) f(x) + (-8x^3 + 12x) (-2x f(x)) = (-24x^2 + 12 + 16x^4 - 24x^2) f(x)\n$$\nSo, the analytic expression for the fourth derivative is:\n$$\nf^{(4)}(x) = (16x^4 - 48x^2 + 12) \\exp(-x^2)\n$$\n\n### Part 2: Finite Difference Stencils and Truncation Errors\n\nWe derive the centered finite difference formulas and their leading truncation error terms by forming linear combinations of Taylor series expansions around a point $x_0$. Let $f_0^{(n)}$ denote $f^{(n)}(x_0)$. The relevant expansions are:\n$$\nf(x_0 \\pm h) = f_0 \\pm h f_0' + \\frac{h^2}{2} f_0'' + \\frac{\\pm h^3}{6} f_0''' + \\frac{h^4}{24} f_0^{(4)} + \\frac{\\pm h^5}{120} f_0^{(5)} + \\frac{h^6}{720} f_0^{(6)} + \\mathcal{O}(h^7)\n$$\n$$\nf(x_0 \\pm 2h) = f_0 \\pm 2h f_0' + \\frac{(2h)^2}{2} f_0'' + \\frac{\\pm (2h)^3}{6} f_0''' + \\frac{(2h)^4}{24} f_0^{(4)} + \\frac{\\pm (2h)^5}{120} f_0^{(5)} + \\frac{(2h)^6}{720} f_0^{(6)} + \\mathcal{O}(h^7)\n$$\n\n**2a. $3$-point centered stencil for $f^{(2)}(x_0)$**\nWe seek an approximation of the form $f_0'' \\approx \\frac{c_{-1}f(x_0-h) + c_0 f(x_0) + c_1 f(x_0+h)}{h^2}$. For a centered stencil, symmetry requires $c_{-1}=c_1$.\nConsider the combination $f(x_0+h) - 2f(x_0) + f(x_0-h)$:\n$$\n(f_0 + h f_0' + \\frac{h^2}{2} f_0'' + \\frac{h^3}{6} f_0''' + \\frac{h^4}{24} f_0^{(4)} + \\mathcal{O}(h^6)) - 2f_0 + (f_0 - h f_0' + \\frac{h^2}{2} f_0'' - \\frac{h^3}{6} f_0''' + \\frac{h^4}{24} f_0^{(4)} + \\mathcal{O}(h^6))\n$$\nCombining terms, the odd-powered derivative terms cancel:\n$$\n= (1-2+1)f_0 + (1-1)h f_0' + (\\frac{1}{2}+\\frac{1}{2})h^2 f_0'' + (\\frac{1}{6}-\\frac{1}{6})h^3 f_0''' + (\\frac{1}{24}+\\frac{1}{24})h^4 f_0^{(4)} + \\mathcal{O}(h^6)\n$$\n$$\n= h^2 f_0'' + \\frac{h^4}{12} f_0^{(4)} + \\mathcal{O}(h^6)\n$$\nDividing by $h^2$, we get the approximation:\n$$\n\\frac{f(x_0-h) - 2f(x_0) + f(x_0+h)}{h^2} = f_0'' + \\frac{h^2}{12} f_0^{(4)} + \\mathcal{O}(h^4)\n$$\nThe stencil is $\\frac{1}{h^2}[f(x_0-h) - 2f(x_0) + f(x_0+h)]$. The leading truncation error is $E_T = \\frac{h^2}{12} f_0^{(4)}$, so the method is of order $p=2$.\n\n**2b. $5$-point centered stencil for $f^{(2)}(x_0)$**\nWe seek a more accurate approximation of the form $\\frac{1}{h^2} \\sum_{j=-2}^{2} c_j f(x_0+jh)$. Symmetry implies $c_{-j}=c_j$. The linear combination is $c_2(f(x_0-2h)+f(x_0+2h)) + c_1(f(x_0-h)+f(x_0+h)) + c_0 f(x_0)$. We use the summed expansions:\n$$\nf(x_0-h) + f(x_0+h) = 2f_0 + h^2 f_0'' + \\frac{h^4}{12} f_0^{(4)} + \\frac{h^6}{360}f_0^{(6)} + \\mathcal{O}(h^8)\n$$\n$$\nf(x_0-2h)+f(x_0+2h) = 2f_0 + 4h^2 f_0'' + \\frac{4h^4}{3} f_0^{(4)} + \\frac{8h^6}{45}f_0^{(6)} + \\mathcal{O}(h^8)\n$$\nWe form a system of equations to determine $c_0, c_1, c_2$ by matching coefficients of the derivatives:\n$$\n\\text{Numerator} = (2c_2 + 2c_1 + c_0)f_0 + (4c_2 + c_1)h^2 f_0'' + (\\frac{4}{3}c_2 + \\frac{1}{12}c_1)h^4 f_0^{(4)} + \\dots\n$$\nTo approximate $h^2 f_0''$, we require:\n\\begin{enumerate}\n    \\item Coeff of $f_0$: $2c_2 + 2c_1 + c_0 = 0$\n    \\item Coeff of $f_0''$: $4c_2 + c_1 = 1$\n    \\item To achieve higher order, we cancel the next error term ($f_0^{(4)}$): $\\frac{4}{3}c_2 + \\frac{1}{12}c_1 = 0$\n\\end{enumerate}\nFrom (3), $16c_2 + c_1 = 0 \\implies c_1 = -16c_2$.\nSubstituting into (2): $4c_2 + (-16c_2) = 1 \\implies -12c_2 = 1 \\implies c_2 = -1/12$.\nThen $c_1 = -16(-1/12) = 4/3$.\nFrom (1): $c_0 = -2c_1 - 2c_2 = -2(4/3) - 2(-1/12) = -8/3 + 1/6 = -16/6 + 1/6 = -15/6 = -5/2$.\nThe coefficients are $c_2=-1/12, c_1=4/3, c_0=-5/2$. The stencil is:\n$$\n\\frac{-\\frac{1}{12}f(x_0-2h) + \\frac{4}{3}f(x_0-h) - \\frac{5}{2}f(x_0) + \\frac{4}{3}f(x_0+h) - \\frac{1}{12}f(x_0+2h)}{h^2}\n$$\nThe next term in the numerator expansion involves $f_0^{(6)}$: $(\\frac{8}{45}c_2 + \\frac{1}{360}c_1)h^6 f_0^{(6)}$.\nSubstituting values: $(\\frac{8}{45}(-\\frac{1}{12}) + \\frac{1}{360}(\\frac{4}{3})) h^6 f_0^{(6)} = (-\\frac{2}{135} + \\frac{1}{270})h^6 f_0^{(6)} = -\\frac{3}{270}h^6 f_0^{(6)} = -\\frac{1}{90}h^6 f_0^{(6)}$.\nThe approximation is $f_0'' - \\frac{h^4}{90}f_0^{(6)} + \\mathcal{O}(h^6)$. The truncation error is $E_T = -\\frac{h^4}{90}f_0^{(6)}$, so the method is of order $p=4$.\n\n**2c. $5$-point centered stencil for $f^{(4)}(x_0)$**\nWe seek an approximation for $f_0^{(4)}$ of the form $\\frac{1}{h^4} \\sum_{j=-2}^{2} c_j f(x_0+jh)$. We use the same expansion for the numerator as in 2b, but match coefficients differently to isolate $f_0^{(4)}$:\n$$\n\\text{Numerator} = (2c_2 + 2c_1 + c_0)f_0 + (4c_2 + c_1)h^2 f_0'' + (\\frac{4}{3}c_2 + \\frac{1}{12}c_1)h^4 f_0^{(4)} + \\dots\n$$\nTo approximate $h^4 f_0^{(4)}$, we require:\n\\begin{enumerate}\n    \\item Coeff of $f_0$: $2c_2 + 2c_1 + c_0 = 0$\n    \\item Coeff of $f_0''$: $4c_2 + c_1 = 0 \\implies c_1 = -4c_2$\n    \\item Coeff of $f_0^{(4)}$: $\\frac{4}{3}c_2 + \\frac{1}{12}c_1 = 1$\n\\end{enumerate}\nFrom (2), substitute $c_1 = -4c_2$ into (3): $\\frac{4}{3}c_2 + \\frac{1}{12}(-4c_2) = 1 \\implies \\frac{4}{3}c_2 - \\frac{1}{3}c_2 = 1 \\implies c_2 = 1$.\nThen $c_1 = -4(1) = -4$.\nFrom (1): $c_0 = -2c_1 - 2c_2 = -2(-4) - 2(1) = 8 - 2 = 6$.\nThe coefficients are $c_2=1, c_1=-4, c_0=6$. The stencil is:\n$$\n\\frac{f(x_0-2h) - 4f(x_0-h) + 6f(x_0) - 4f(x_0+h) + f(x_0+2h)}{h^4}\n$$\nThe next term in the numerator expansion involves $f_0^{(6)}$: $(\\frac{8}{45}c_2 + \\frac{1}{360}c_1)h^6 f_0^{(6)}$.\nSubstituting values: $(\\frac{8}{45}(1) + \\frac{1}{360}(-4))h^6 f_0^{(6)} = (\\frac{64}{360} - \\frac{4}{360})h^6 f_0^{(6)} = \\frac{60}{360}h^6 f_0^{(6)} = \\frac{1}{6}h^6 f_0^{(6)}$.\nThe approximation is $f_0^{(4)} + \\frac{h^2}{6}f_0^{(6)} + \\mathcal{O}(h^4)$. The truncation error is $E_T = \\frac{h^2}{6}f_0^{(6)}$, so the method is of order $p=2$.\n\n### Part 3: Numerical Verification Strategy\n\nThe theoretical derivations will be verified computationally. For each of the four test cases specified:\n\\begin{enumerate}\n    \\item A sequence of step sizes $h_k = 0.2 \\cdot 2^{-k}$ for $k \\in \\{0, 1, 2, 3, 4, 5\\}$ is used.\n    \\item For each $h_k$, the appropriate finite difference approximation is computed at the specified point $x_0$.\n    \\item The true value of the derivative at $x_0$ is computed using the analytic formulas from Part 1.\n    \\item The absolute error $E(h_k)$ is calculated as the absolute difference between the numerical approximation and the analytic value.\n    \\item The observed order of convergence $p_k$ is computed for $k \\in \\{1, 2, 3, 4, 5\\}$ using pairs of successive errors:\n    $$\n    p_k = \\frac{\\log(E(h_{k-1})/E(h_k))}{\\log(h_{k-1}/h_k)} = \\frac{\\log(E(h_{k-1})/E(h_k))}{\\log(2)}\n    $$\n    \\item The reported observed order for each case, $\\tilde{p}$, is defined as the median of the last three computed orders, $\\{p_3, p_4, p_5\\}$. This provides a stable estimate of the asymptotic convergence rate as $h \\to 0$.\n    \\item Finally, each case is validated against its expected theoretical order $p_{\\mathrm{expected}}$ using the criterion $|\\tilde{p} - p_{\\mathrm{expected}}| \\le 0.3$.\n\\end{enumerate}\nThis procedure will be encapsulated in a Python program to generate the final boolean results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of approximating higher derivatives and verifying convergence orders.\n    \"\"\"\n\n    # Part 1: Analytic Functions\n    def f(x: float) -> float:\n        \"\"\"The base function f(x) = exp(-x^2).\"\"\"\n        return np.exp(-x**2)\n\n    def f_d2(x: float) -> float:\n        \"\"\"The analytic second derivative of f(x).\"\"\"\n        return (4 * x**2 - 2) * np.exp(-x**2)\n\n    def f_d4(x: float) -> float:\n        \"\"\"The analytic fourth derivative of f(x).\"\"\"\n        return (16 * x**4 - 48 * x**2 + 12) * np.exp(-x**2)\n\n    # Part 2: Finite Difference Stencils\n    def approx_d2_3pt(func, x0: float, h: float) -> float:\n        \"\"\"3-point centered difference approximation for the 2nd derivative.\"\"\"\n        return (func(x0 - h) - 2 * func(x0) + func(x0 + h)) / h**2\n\n    def approx_d2_5pt(func, x0: float, h: float) -> float:\n        \"\"\"5-point centered difference approximation for the 2nd derivative (order 4).\"\"\"\n        return (-func(x0 - 2 * h) + 16 * func(x0 - h) - 30 * func(x0) + 16 * func(x0 + h) - func(x0 + 2 * h)) / (12 * h**2)\n\n    def approx_d4_5pt(func, x0: float, h: float) -> float:\n        \"\"\"5-point centered difference approximation for the 4th derivative (order 2).\"\"\"\n        return (func(x0 - 2 * h) - 4 * func(x0 - h) + 6 * func(x0) - 4 * func(x0 + h) + func(x0 + 2 * h)) / h**4\n\n    # Part 3: Numerical Verification\n    h_values = [0.2 * (2**-k) for k in range(6)]\n    \n    # Test cases: (approximation_function, analytic_function, evaluation_point_x0, expected_order)\n    test_cases = [\n        # Case A: 3-point f''(0.7), expected order 2\n        (approx_d2_3pt, f_d2, 0.7, 2),\n        # Case B: 5-point f''(1.3), expected order 4\n        (approx_d2_5pt, f_d2, 1.3, 4),\n        # Case C: 5-point f^(4)(0.0), expected order 2\n        (approx_d4_5pt, f_d4, 0.0, 2),\n        # Case D: 5-point f''(2.0) edge case, expected order 4\n        (approx_d2_5pt, f_d2, 2.0, 4),\n    ]\n\n    final_results = []\n    for approx_func, analytic_func, x0, p_expected in test_cases:\n        errors = []\n        for h in h_values:\n            approx_val = approx_func(f, x0, h)\n            analytic_val = analytic_func(x0)\n            error = np.abs(approx_val - analytic_val)\n            errors.append(error)\n\n        observed_orders = []\n        # Calculate observed orders p_k for k in {1,2,3,4,5}\n        for k in range(1, len(h_values)):\n            # Ratio of step sizes is 2\n            h_ratio = h_values[k-1] / h_values[k]\n            \n            # Avoid division by zero if error becomes numerically zero\n            if errors[k] > 0 and errors[k-1] > 0:\n                error_ratio = errors[k-1] / errors[k]\n                order = np.log(error_ratio) / np.log(h_ratio)\n                observed_orders.append(order)\n            else:\n                # If error is zero, convergence is perfect/infinite.\n                # This case isn't expected to be hit, but we handle it.\n                observed_orders.append(np.inf)\n\n        # Per problem, use median of last three observed orders {p_3, p_4, p_5}\n        # These correspond to indices 2, 3, 4 of observed_orders list\n        # which has 5 elements (p_1 to p_5).\n        if len(observed_orders) >= 5:\n            last_three_orders = observed_orders[2:5]\n            reported_order = np.median(last_three_orders)\n            \n            # Apply pass-fail criterion\n            passed = np.abs(reported_order - p_expected) <= 0.3\n            final_results.append(passed)\n        else:\n            # This path should not be taken given the problem setup\n            final_results.append(False)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While higher-order stencils can increase accuracy, another powerful technique, Richardson extrapolation, can improve a result by combining estimates from different step sizes, $h$. This exercise challenges you to explore how the underlying error structure of a numerical method is critical for extrapolation to succeed. You will see firsthand why this technique dramatically improves a symmetric central-difference scheme but fails for a standard forward-difference scheme when applied naively .",
            "id": "3238865",
            "problem": "Consider a smooth scalar function $f:\\mathbb{R}\\to\\mathbb{R}$ with continuous derivatives up to at least order $5$. The goal is to design and implement a numerical experiment that demonstrates how Richardson extrapolation behaves when applied to the second derivative $f''(x)$ using two finite-difference schemes. Begin from the fundamental base: the Taylor series expansion of $f$ around a point $x$, which states that for any sufficiently small step $h$, there exist coefficients involving derivatives of $f$ such that discrete stencil combinations approximate $f''(x)$ with a leading truncation error that can be expressed as a power of $h$. This principle underlies the analysis of asymptotic error models used by Richardson extrapolation.\n\nIn all computations involving trigonometric functions, use angles in radians.\n\nYour task is to write a program that performs the following steps for each test case listed below.\n\n1. Implement the second-derivative approximation using the symmetric central difference stencil defined on the points $x-h$, $x$, and $x+h$. This stencil is known to have a leading truncation error that is an even power of $h$ for smooth $f$.\n\n2. Implement the second-derivative approximation using the forward difference stencil defined on the points $x$, $x+h$, and $x+2h$. This stencil is known to have a leading truncation error that is an odd power of $h$ for smooth $f$.\n\n3. For both stencils, construct a two-level Richardson extrapolation using values at step sizes $h$ and $h/2$ that assumes the leading error term scales like $h^p$ with $p=2$, as inherited from the symmetry of the central difference stencil. Apply this same $p=2$ assumption to the forward difference stencil without any modification. Compute extrapolated estimates using this assumption.\n\n4. For each stencil and its corresponding extrapolated estimate, measure the absolute error $|A(h)-f''(x)|$ for a geometric sequence of step sizes $\\{h,\\,h/2,\\,h/4,\\,\\dots\\}$, and compute the empirical order of accuracy by fitting a straight line to $(\\log h, \\log\\text{error})$ pairs. The empirical order is the slope of the best-fit line, which quantifies how the error scales with $h$.\n\n5. Report, for each test case, four floating-point numbers rounded to two decimal places: the empirical order of the base central-difference estimate, the empirical order of its extrapolated estimate, the empirical order of the base forward-difference estimate, and the empirical order of its extrapolated estimate constructed naively with $p=2$. The numerical experiment should illustrate that Richardson extrapolation improves the central difference significantly (increasing the order), but fails to produce the same improvement for the forward difference when the same unmodified $p=2$ assumption is used.\n\nTest Suite:\n- Case $1$: $f(x)=e^x$, $x_0=0.3$, initial step $h_0=0.2$, refinement levels $5$.\n- Case $2$: $f(x)=\\sin(x)$ (angles in radians), $x_0=1.0$, initial step $h_0=0.3$, refinement levels $5$.\n- Case $3$: $f(x)=x^7-3x^3+2$, $x_0=0.1$, initial step $h_0=0.2$, refinement levels $5$.\n\nFor each case, construct the geometric sequence of step sizes $\\{h_k\\}_{k=0}^{L-1}$ with $h_k=h_0/2^k$ and $L=5$. For extrapolated estimates, pair each $h_k$ with $h_{k+1}=h_k/2$; report orders using all available pairs.\n\nFinal Output Format:\nYour program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets. The list must be ordered case-by-case, and inside each case in the order: central base order, central extrapolated order, forward base order, forward extrapolated order. For example, the output format must be\n$[o_{1,\\text{c-base}},o_{1,\\text{c-extrap}},o_{1,\\text{f-base}},o_{1,\\text{f-extrap}},o_{2,\\text{c-base}},\\dots,o_{3,\\text{f-extrap}}]$,\nwhere each $o$ is a floating-point number rounded to two decimal places.",
            "solution": "The problem statement is a valid exercise in numerical analysis. It is scientifically grounded, well-posed, and objective. It asks for the implementation and analysis of a standard numerical experiment involving finite difference approximations and Richardson extrapolation. All required parameters, including the functions, evaluation points, and numerical parameters ($h_0, L$), are explicitly provided. The expected outcome—that Richardson extrapolation with an assumed error order of $p=2$ improves the central difference scheme but fails to improve the forward difference scheme—is consistent with the established theory of numerical methods.\n\nThe theoretical basis for this problem is the Taylor series expansion of a sufficiently smooth function $f(x)$ around a point $x$. For a step size $h$, we have:\n$$f(x \\pm h) = f(x) \\pm hf'(x) + \\frac{h^2}{2!}f''(x) \\pm \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) \\pm \\dots$$\n\n**1. Finite Difference Schemes**\n\nWe can derive approximations for the second derivative, $f''(x)$, by combining these expansions for different stencil points.\n\n**Central Difference Stencil:**\nUsing the expansions for $f(x+h)$ and $f(x-h)$:\n$$f(x+h) + f(x-h) = 2f(x) + h^2f''(x) + \\frac{h^4}{12}f^{(4)}(x) + O(h^6)$$\nSolving for $f''(x)$ gives:\n$$f''(x) = \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2} - \\frac{h^2}{12}f^{(4)}(x) - O(h^4)$$\nThe central difference approximation, $D_C(h)$, is therefore:\n$$D_C(h) = \\frac{f(x+h) - 2f(x) + f(x-h)}{h^2}$$\nThe error, $E_C(h) = D_C(h) - f''(x)$, has an asymptotic expansion in even powers of $h$:\n$$E_C(h) = C_2h^2 + C_4h^4 + C_6h^6 + \\dots$$\nwhere $C_2 = -\\frac{1}{12}f^{(4)}(x)$. The leading error term is $O(h^2)$, so the method is second-order accurate.\n\n**Forward Difference Stencil:**\nUsing the expansions for $f(x+h)$ and $f(x+2h)$:\n$$f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f'''(x) + O(h^4)$$\n$$f(x+2h) = f(x) + 2hf'(x) + 2h^2f''(x) + \\frac{4h^3}{3}f'''(x) + O(h^4)$$\nTo approximate $f''(x)$, we can form the linear combination $f(x+2h) - 2f(x+h) + f(x)$, which yields:\n$$f(x+2h) - 2f(x+h) + f(x) = h^2f''(x) + h^3f'''(x) + O(h^4)$$\nSolving for $f''(x)$ gives:\n$$f''(x) = \\frac{f(x+2h) - 2f(x+h) + f(x)}{h^2} - hf'''(x) - O(h^2)$$\nThe forward difference approximation, $D_F(h)$, is:\n$$D_F(h) = \\frac{f(x+2h) - 2f(x+h) + f(x)}{h^2}$$\nThe error, $E_F(h) = D_F(h) - f''(x)$, has an asymptotic expansion in all powers of $h$:\n$$E_F(h) = C_1h + C_2h^2 + C_3h^3 \\dots$$\nwhere $C_1 = f'''(x)$. The leading error term is $O(h)$, so the method is first-order accurate.\n\n**2. Richardson Extrapolation**\n\nLet $A(h)$ be an approximation to a true value $A_{true}$ with a leading error term of order $p$, i.e., $A(h) = A_{true} + Ch^p + O(h^q)$ with $q>p$. We can compute a more accurate estimate by combining approximations at two different step sizes, $h$ and $h/2$:\n$$A_{true} \\approx A(h) - Ch^p$$\n$$A_{true} \\approx A(h/2) - C(h/2)^p$$\nEliminating the unknown constant $C$ gives the extrapolated value $A_{extrap}$:\n$$A_{extrap} = \\frac{2^p A(h/2) - A(h)}{2^p-1}$$\nThe problem specifies using an assumed order of $p=2$ for both schemes. The formula is:\n$$A_{extrap}(h) = \\frac{4A(h/2) - A(h)}{3}$$\n\n- **Application to Central Difference:** The error structure is $E_C(h) = C_2h^2 + C_4h^4 + \\dots$. The extrapolation formula with $p=2$ is correctly matched to the leading error term. The $O(h^2)$ term is eliminated, and the new leading error term is $O(h^4)$. The order of accuracy is expected to increase from $2$ to $4$.\n\n- **Application to Forward Difference:** The error structure is $E_F(h) = C_1h + C_2h^2 + \\dots$. Applying the extrapolation formula with an incorrect assumption of $p=2$:\n$$D_{F,extrap}(h) = \\frac{4 D_F(h/2) - D_F(h)}{3} = \\frac{4(f''(x) + C_1\\frac{h}{2} + \\dots) - (f''(x) + C_1h + \\dots)}{3}$$\n$$D_{F,extrap}(h) = \\frac{3f''(x) + (2C_1h - C_1h) + \\dots}{3} = f''(x) + \\frac{C_1}{3}h + \\dots$$\nThe $O(h)$ term is not eliminated. The method remains first-order accurate, and the extrapolation fails to improve the order.\n\n**3. Empirical Order of Accuracy**\n\nIf the error $E(h)$ of an approximation behaves like $E(h) \\approx Kh^p$ for small $h$, we can determine the order $p$ empirically. Taking the logarithm of both sides gives:\n$$\\log|E(h)| \\approx \\log|K| + p \\log h$$\nThis shows a linear relationship between $\\log|E(h)|$ and $\\log h$. The slope of this line is the order of accuracy, $p$. We compute this slope by performing a linear regression (least-squares fit) on the set of points $\\{(\\log h_k, \\log|E(h_k)|)\\}_{k=0}^{L-1}$.\n\nThe program will execute the following steps for each test case:\n1. Define the function $f$ and its exact second derivative $f''$.\n2. Generate a geometric sequence of $L=5$ step sizes $h_k = h_0/2^k$.\n3. For each $h_k$, compute the base approximations $D_C(h_k)$ and $D_F(h_k)$ and their absolute errors.\n4. Compute the extrapolated values for both schemes using pairs $(h_k, h_{k+1})$ and their corresponding absolute errors. This produces $L-1=4$ data points for each extrapolated series.\n5. For each of the four sets of errors (central base, central extrapolated, forward base, forward extrapolated), compute the empirical order of accuracy by finding the slope of the best-fit line to the log-log error data.\n6. Collect and format the four resulting orders for each test case as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical experiment problem for Richardson extrapolation.\n    \"\"\"\n\n    def linear_regression_slope(x_data, y_data):\n        \"\"\"\n        Calculates the slope of the best-fit line for (x, y) data.\n        This is equivalent to fitting y = m*x + c.\n        \"\"\"\n        # np.polyfit is a robust way to perform linear regression.\n        # It fits a polynomial of degree 1 and returns [slope, intercept].\n        slope, _ = np.polyfit(x_data, y_data, 1)\n        return slope\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda x: np.exp(x),\n            \"f_pp\": lambda x: np.exp(x), # f''(x)\n            \"x0\": 0.3,\n            \"h0\": 0.2,\n            \"L\": 5,\n        },\n        {\n            \"f\": lambda x: np.sin(x),\n            \"f_pp\": lambda x: -np.sin(x), # f''(x)\n            \"x0\": 1.0,\n            \"h0\": 0.3,\n            \"L\": 5,\n        },\n        {\n            \"f\": lambda x: x**7 - 3 * x**3 + 2,\n            \"f_pp\": lambda x: 42 * x**5 - 18 * x, # f''(x)\n            \"x0\": 0.1,\n            \"h0\": 0.2,\n            \"L\": 5,\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        f = case[\"f\"]\n        f_pp = case[\"f_pp\"]\n        x0 = case[\"x0\"]\n        h0 = case[\"h0\"]\n        L = case[\"L\"]\n\n        # Generate geometric sequence of step sizes\n        h_values = np.array([h0 / (2**k) for k in range(L)])\n        true_val = f_pp(x0)\n\n        # --- Central Difference Calculations ---\n        \n        # Base approximation\n        dc_base_vals = (f(x0 + h_values) - 2 * f(x0) + f(x0 - h_values)) / h_values**2\n        errors_c_base = np.abs(dc_base_vals - true_val)\n        order_c_base = linear_regression_slope(np.log(h_values), np.log(errors_c_base))\n\n        # Extrapolated approximation\n        # Uses L-1 pairs of (h, h/2), corresponding to h_k and h_{k+1}\n        h_extrap = h_values[:-1] # The larger step size in each pair\n        dc_base_h = dc_base_vals[:-1] # A(h)\n        dc_base_h_half = dc_base_vals[1:] # A(h/2)\n        \n        # Richardson formula for p=2: (4*A(h/2) - A(h))/3\n        dc_extrap_vals = (4 * dc_base_h_half - dc_base_h) / 3\n        errors_c_extrap = np.abs(dc_extrap_vals - true_val)\n        order_c_extrap = linear_regression_slope(np.log(h_extrap), np.log(errors_c_extrap))\n\n        # --- Forward Difference Calculations ---\n\n        # Base approximation\n        df_base_vals = (f(x0 + 2 * h_values) - 2 * f(x0 + h_values) + f(x0)) / h_values**2\n        errors_f_base = np.abs(df_base_vals - true_val)\n        order_f_base = linear_regression_slope(np.log(h_values), np.log(errors_f_base))\n        \n        # Extrapolated approximation (naively using p=2)\n        df_base_h = df_base_vals[:-1] # A(h)\n        df_base_h_half = df_base_vals[1:] # A(h/2)\n        \n        df_extrap_vals = (4 * df_base_h_half - df_base_h) / 3\n        errors_f_extrap = np.abs(df_extrap_vals - true_val)\n        order_f_extrap = linear_regression_slope(np.log(h_extrap), np.log(errors_f_extrap))\n\n        # Append results for the current case, rounded to two decimal places\n        all_results.extend([\n            order_c_base,\n            order_c_extrap,\n            order_f_base,\n            order_f_extrap\n        ])\n\n    # Format the final output string\n    formatted_results = [f\"{x:.2f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The theoretical elegance of finite difference methods rests on the assumption that the function being differentiated is sufficiently smooth. This practice explores what happens when this core assumption is violated, using the absolute value function $f(x)=|x|$ as a case study. You will diagnose the catastrophic failure of a standard stencil as it crosses the \"kink\" at the origin and then design an adaptive strategy to produce reliable results, teaching you to be mindful of the limitations of numerical algorithms .",
            "id": "3239009",
            "problem": "Consider the problem of numerically approximating the third derivative $f^{(3)}(x)$ for the function $f(x) = |x|$ on a uniform grid with spacing $h > 0$. The fundamental base for this problem is the Taylor expansion of a sufficiently smooth function $f$ about a point $x$, which states that for any integer $k \\ge 0$,\n$$\nf(x + j h) = \\sum_{m=0}^{\\infty} \\frac{f^{(m)}(x)}{m!} (j h)^m,\n$$\nwhere $j$ is an integer indexing grid points. A general finite-difference estimator of order $m$ at $x$ with stencil offsets $\\{j_0, j_1, \\dots, j_{n-1}\\}$ seeks weights $\\{a_0, a_1, \\dots, a_{n-1}\\}$ satisfying the moment-matching conditions\n$$\n\\sum_{i=0}^{n-1} a_i j_i^k = \\begin{cases}\nm! & \\text{if } k = m, \\\\\n0 & \\text{if } 0 \\le k \\le n-1,\\ k \\ne m,\n\\end{cases}\n$$\nso that the finite-difference approximation\n$$\nD_m[f](x; h) = \\frac{1}{h^m} \\sum_{i=0}^{n-1} a_i f(x + j_i h)\n$$\nis exact for polynomials up to degree $n-1$ and has truncation error $O(h^{n-m})$, or better when the stencil symmetry causes additional even-moment cancellations. When $f$ is not smooth, the Taylor expansion does not hold at, or across, points of non-differentiability, and finite-difference estimators that straddle such points can fail catastrophically.\n\nYour tasks are:\n- Construct a central finite-difference estimator for $f^{(3)}(x)$ using a symmetric stencil that does not include $x$ itself, specifically offsets $\\{-2, -1, 1, 2\\}$. Derive the weights by satisfying the moment-matching conditions up to degree $3$ using the Taylor expansion as the fundamental base.\n- Diagnose the failure of this central estimator when its stencil crosses the non-smooth point $x = 0$ for $f(x) = |x|$. Explain why, due to the lack of smoothness, the approximation can produce large spurious values or misleading cancellations.\n- Design a modified, localized estimator that avoids singular points by using one-sided stencils chosen to lie entirely on one smooth side of the kink. Use forward offsets $\\{0, 1, 2, 3, 4\\}$ when $x \\ge 0$, and backward offsets $\\{-4, -3, -2, -1, 0\\}$ when $x < 0$. Derive the one-sided weights with the same moment-matching principle.\n- Implement a diagnostic to detect whether the central stencil crosses the non-smooth point, using the interval test $[x - 2h, x + 2h]$ containing $0$. Additionally, compute the scale-normalized second difference\n$$\nS(x; h) = \\frac{|f(x+h) - 2 f(x) + f(x-h)|}{h},\n$$\nand discuss how its behavior with refined $h$ distinguishes smooth from non-smooth behavior near $x = 0$ for $f(x) = |x|$.\n- For each test case, compute:\n    1. The central estimator value $D_3^{\\text{central}}[f](x; h)$ using offsets $\\{-2, -1, 1, 2\\}$.\n    2. The left one-sided estimator $D_3^{\\text{left}}[f](x; h)$ using offsets $\\{-4, -3, -2, -1, 0\\}$.\n    3. The right one-sided estimator $D_3^{\\text{right}}[f](x; h)$ using offsets $\\{0, 1, 2, 3, 4\\}$.\n    4. The modified localized estimator $D_3^{\\text{mod}}[f](x; h)$, which selects the right one-sided estimator when $x \\ge 0$, the left one-sided estimator when $x < 0$, and otherwise the central estimator if the stencil does not cross $0$.\n    5. A boolean indicating whether the central stencil crosses $0$, i.e., whether $x - 2h \\le 0 \\le x + 2h$.\n- Use the function $f(x) = |x|$, angle-free and unit-free.\n\nTest suite:\n- Case $1$: $x = 1.0$, $h = 0.1$ (smooth region away from $0$; central stencil does not cross the kink).\n- Case $2$: $x = 0.05$, $h = 0.1$ (near $0$; central stencil crosses the kink).\n- Case $3$: $x = 0.0$, $h = 0.1$ (exactly at the kink; central stencil includes points symmetrically around $0$).\n- Case $4$: $x = 0.05$, $h = 0.01$ (near $0$, but central stencil does not cross due to small $h$).\n- Case $5$: $x = -0.075$, $h = 0.05$ (negative side; central stencil crosses the kink).\n\nAnswer types:\n- For each test case, the outputs are floats for items $1$ to $4$, and a boolean for item $5$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of five sublists, one per test case, each sublist in the fixed order\n$$\n\\left[ D_3^{\\text{central}},\\ D_3^{\\text{left}},\\ D_3^{\\text{right}},\\ D_3^{\\text{mod}},\\ \\text{kink\\_crossed} \\right],\n$$\nand the entire list should be printed exactly as a Python list literal, for example\n$$\n\\big[ [\\dots], [\\dots], [\\dots], [\\dots], [\\dots] \\big].\n$$",
            "solution": "The problem requires the derivation, analysis, and implementation of finite-difference estimators for the third derivative, $f^{(3)}(x)$, of the function $f(x) = |x|$. The core of the problem lies in addressing the failure of standard estimators at the point of non-differentiability at $x=0$.\n\n### **1. Central Finite-Difference Estimator for $f^{(3)}(x)$**\n\nA finite-difference estimator for the $m$-th derivative, $D_m[f](x; h)$, is constructed by taking a weighted sum of function values at nearby grid points. For an estimator of $f^{(3)}(x)$ using the symmetric stencil of offsets $\\{j_i\\} = \\{-2, -1, 1, 2\\}$, we seek weights $\\{a_{-2}, a_{-1}, a_1, a_2\\}$ such that the approximation is exact for low-degree polynomials. This is formalized by the moment-matching conditions:\n$$\n\\sum_{j \\in \\{-2, -1, 1, 2\\}} a_j j^k = 3! \\cdot \\delta_{k3} \\quad \\text{for } k=0, 1, 2, 3.\n$$\nThis yields a system of linear equations for the weights:\n\\begin{itemize}\n    \\item $k=0$ (constant polynomials): $a_{-2} + a_{-1} + a_1 + a_2 = 0$\n    \\item $k=1$ (linear polynomials): $-2a_{-2} - a_{-1} + a_1 + 2a_2 = 0$\n    \\item $k=2$ (quadratic polynomials): $4a_{-2} + a_{-1} + a_1 + 4a_2 = 0$\n    \\item $k=3$ (cubic polynomials): $-8a_{-2} - a_{-1} + a_1 + 8a_2 = 3! = 6$\n\\end{itemize}\nFor a symmetric stencil approximating an odd derivative ($m=3$), the weights must be anti-symmetric, i.e., $a_{-j} = -a_j$. Thus, $a_{-2} = -a_2$ and $a_{-1} = -a_1$.\nSubstituting this symmetry into the system:\n\\begin{itemize}\n    \\item $k=0$: $(-a_2) + (-a_1) + a_1 + a_2 = 0$. This is satisfied automatically.\n    \\item $k=1$: $-2(-a_2) - (-a_1) + a_1 + 2a_2 = 2a_2 + a_1 + a_1 + 2a_2 = 4a_2 + 2a_1 = 0 \\implies a_1 = -2a_2$.\n    \\item $k=2$: $4(-a_2) + (-a_1) + a_1 + 4a_2 = 0$. This is satisfied automatically.\n    \\item $k=3$: $-8(-a_2) - (-a_1) + a_1 + 8a_2 = 8a_2 + a_1 + a_1 + 8a_2 = 16a_2 + 2a_1 = 6$.\n\\end{itemize}\nSubstituting $a_1 = -2a_2$ into the $k=3$ equation gives:\n$$\n16a_2 + 2(-2a_2) = 12a_2 = 6 \\implies a_2 = \\frac{1}{2}.\n$$\nFrom this, we find the other weights: $a_1 = -2a_2 = -1$, $a_{-1} = -a_1 = 1$, and $a_{-2} = -a_2 = -\\frac{1}{2}$.\nThe weights for the stencil $\\{-2, -1, 1, 2\\}$ are therefore $\\{-\\frac{1}{2}, 1, -1, \\frac{1}{2}\\}$. The central estimator is:\n$$\nD_3^{\\text{central}}[f](x; h) = \\frac{1}{h^3} \\left( -\\frac{1}{2}f(x-2h) + f(x-h) - f(x+h) + \\frac{1}{2}f(x+2h) \\right)\n$$\nThis can be rewritten as:\n$$\nD_3^{\\text{central}}[f](x; h) = \\frac{-f(x-2h) + 2f(x-h) - 2f(x+h) + f(x+2h)}{2h^3}.\n$$\n\n### **2. Failure of the Central Estimator for $f(x) = |x|$**\n\nThe derivation of finite-difference formulas relies on the Taylor series expansion being valid for the function $f$ across the entire stencil. The function $f(x)=|x|$ has a \"kink\" at $x=0$; its first derivative is discontinuous ($f'(x) = \\text{sgn}(x)$), and its higher derivatives are undefined in the classical sense at $x=0$. Away from $x=0$, all derivatives beyond the first are zero.\n\nWhen the central stencil $[x-2h, x+2h]$ contains the point $x=0$, the assumption of smoothness is violated. The formula misinterprets the sharp change at the kink as evidence of a large third derivative. For instance, for $x=0.05$ and $h=0.1$, the stencil is $[-0.15, 0.25]$, which crosses $0$. The estimator yields:\n$$\nD_3^{\\text{central}}[f](0.05; 0.1) = \\frac{-|-0.15| + 2|-0.05| - 2|0.15| + |0.25|}{2(0.1)^3} = \\frac{-0.15 + 0.1 - 0.3 + 0.25}{0.002} = \\frac{-0.1}{0.002} = -50.\n$$\nThis large, spurious value is drastically different from the true derivative of $0$ that holds for any $x>0$. A special case occurs at $x=0$, where the symmetry of $f(x)=|x|$ and the anti-symmetry of the operator cause a cancellation, yielding $D_3^{\\text{central}}[f](0; h) = 0$. While this result happens to be correct, it is an artifact of symmetry, not an indication of robustness.\n\n### **3. Modified One-Sided Estimators**\n\nTo avoid the singularity, a modified estimator can use one-sided stencils that lie entirely on a smooth part of the function.\n\n**Right-sided (Forward) Estimator:** For $x \\ge 0$, we can use a stencil on the positive axis, e.g., $\\{0, 1, 2, 3, 4\\}$. We seek weights $\\{a_0, a_1, a_2, a_3, a_4\\}$ by solving the system $\\sum_{i=0}^4 a_i j_i^k = 3! \\delta_{k3}$ for $k=0, \\dots, 4$. This is a $5 \\times 5$ Vandermonde-type linear system. Solving this system yields the weights:\n$$\n\\{a_0, a_1, a_2, a_3, a_4\\} = \\left\\{-\\frac{5}{2}, 9, -12, 7, -\\frac{3}{2}\\right\\}.\n$$\nThe right-sided estimator is:\n$$\nD_3^{\\text{right}}[f](x; h) = \\frac{1}{h^3} \\left(-\\frac{5}{2}f(x) + 9f(x+h) - 12f(x+2h) + 7f(x+3h) - \\frac{3}{2}f(x+4h)\\right).\n$$\n\n**Left-sided (Backward) Estimator:** For $x < 0$, we use a stencil on the negative axis, e.g., $\\{-4, -3, -2, -1, 0\\}$. By symmetry, the weights are the negated reverse of the forward weights:\n$$\n\\{a_{-4}, a_{-3}, a_{-2}, a_{-1}, a_0\\} = \\left\\{\\frac{3}{2}, -7, 12, -9, \\frac{5}{2}\\right\\}.\n$$\nThe left-sided estimator is:\n$$\nD_3^{\\text{left}}[f](x; h) = \\frac{1}{h^3} \\left(\\frac{3}{2}f(x-4h) - 7f(x-3h) + 12f(x-2h) - 9f(x-h) + \\frac{5}{2}f(x)\\right).\n$$\nSince these one-sided stencils lie entirely within a smooth region (where $f(x)=x$ or $f(x)=-x$), and both formulas are exact for polynomials of degree up to $4$, they will correctly compute the third derivative as $0$.\n\nThe problem states a slightly ambiguous rule for the modified estimator $D_3^{\\text{mod}}$. The most coherent interpretation, consistent with standard numerical practice for adaptive stencils, is to use the central estimator when its stencil is clear of the singularity at $x=0$, and switch to an appropriate one-sided estimator otherwise. The implemented logic is: if the central stencil interval $[x-2h, x+2h]$ contains $0$, use the right-sided estimator for $x \\ge 0$ and the left-sided for $x < 0$; otherwise, use the central estimator.\n\n### **4. Kink Detection and Scale-Normalized Second Difference**\n\nA simple diagnostic to detect if the central stencil crosses the kink at $x=0$ is to check if the interval $[x-2h, x+2h]$ contains $0$, which is true if and only if $x-2h \\le 0$ and $x+2h \\ge 0$, or $|x| \\le 2h$.\n\nThe scale-normalized second difference, $S(x; h) = \\frac{|f(x+h) - 2 f(x) + f(x-h)|}{h}$, provides another way to diagnose non-smoothness.\n\\begin{itemize}\n    \\item For a smooth function $f$, the numerator is approximately $h^2 f''(x)$, so $S(x; h) \\approx h |f''(x)|$, which tends to $0$ as $h \\to 0$.\n    \\item For $f(x)=|x|$ at $x=0$, we have $S(0; h) = \\frac{|h - 0 + h|}{h} = \\frac{2h}{h} = 2$. The value is constant and non-zero.\n    \\item For $f(x)=|x|$ where the stencil avoids the kink (i.e., $|x|>h$), the function is locally linear, so $f(x+h) - 2f(x) + f(x-h) = 0$ and thus $S(x;h)=0$.\n\\end{itemize}\nThe failure of $S(x; h)$ to converge to $0$ as $h \\to 0$ at a point $x$ is a clear indicator that $f''(x)$ does not exist in a classical sense.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical differentiation problem for f(x)=|x| for a suite of test cases.\n    \"\"\"\n\n    # Define the function f(x) = |x|.\n    f = np.abs\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 0.1),    # Case 1\n        (0.05, 0.1),   # Case 2\n        (0.0, 0.1),    # Case 3\n        (0.05, 0.01),  # Case 4\n        (-0.075, 0.05) # Case 5\n    ]\n\n    all_results = []\n\n    for x, h in test_cases:\n        # 1. Central estimator: D_3^central\n        # Stencil: {-2, -1, 1, 2}, Weights: {-1/2, 1, -1, 1/2} normalized by 1/h^3.\n        # Formula: (-f(x-2h) + 2f(x-h) - 2f(x+h) + f(x+2h)) / (2h^3)\n        numerator_central = -f(x - 2*h) + 2*f(x - h) - 2*f(x + h) + f(x + 2*h)\n        d3_central = numerator_central / (2 * h**3)\n\n        # 2. Left one-sided estimator: D_3^left\n        # Stencil: {-4, -3, -2, -1, 0}, Weights: {3/2, -7, 12, -9, 5/2}\n        numerator_left = (3/2)*f(x - 4*h) - 7*f(x - 3*h) + 12*f(x - 2*h) - 9*f(x - h) + (5/2)*f(x)\n        d3_left = numerator_left / h**3\n\n        # 3. Right one-sided estimator: D_3^right\n        # Stencil: {0, 1, 2, 3, 4}, Weights: {-5/2, 9, -12, 7, -3/2}\n        numerator_right = -(5/2)*f(x) + 9*f(x + h) - 12*f(x + 2*h) + 7*f(x + 3*h) - (3/2)*f(x + 4*h)\n        d3_right = numerator_right / h**3\n        \n        # 5. Kink crossed diagnostic\n        # The central stencil [x - 2h, x + 2h] crosses the kink at 0.\n        kink_crossed = (x - 2*h <= 0) and (x + 2*h >= 0)\n\n        # 4. Modified localized estimator: D_3^mod\n        # If the central stencil crosses the kink, use a one-sided estimator that avoids it.\n        # Otherwise, use the more accurate central estimator.\n        if kink_crossed:\n            if x >= 0:\n                d3_mod = d3_right\n            else:\n                d3_mod = d3_left\n        else:\n            d3_mod = d3_central\n\n        # Collect results for this case in the specified order.\n        case_result = [d3_central, d3_left, d3_right, d3_mod, kink_crossed]\n        all_results.append(case_result)\n\n    # Format the final output as a Python list literal string representation.\n    # To avoid spaces and use Python's native bool representation (True/False),\n    # we convert each list to its string representation and then join them.\n    result_strings = []\n    for res in all_results:\n        # Manually format each sub-list to match specification, ensuring no spaces\n        # and correct boolean representation.\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]}]\"\n        result_strings.append(res_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}