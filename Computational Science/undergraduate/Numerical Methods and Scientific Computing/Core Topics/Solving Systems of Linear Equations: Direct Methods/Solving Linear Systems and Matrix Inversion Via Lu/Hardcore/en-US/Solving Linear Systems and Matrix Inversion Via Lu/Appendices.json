{
    "hands_on_practices": [
        {
            "introduction": "While LU decomposition is a general tool, its true power often emerges when applied to matrices with special structures. Tridiagonal matrices, which appear frequently in applications like solving differential equations and spline interpolation, allow for a dramatically simplified and faster factorization. This practice challenges you to derive this specialized $O(n)$ algorithm, often related to the Thomas algorithm, directly from the definition of LU factorization and then apply it to solve a concrete linear system. ",
            "id": "3275855",
            "problem": "Consider an $n \\times n$ tridiagonal matrix $A$ with subdiagonal entries $\\{a_i\\}_{i=2}^{n}$, diagonal entries $\\{b_i\\}_{i=1}^{n}$, and superdiagonal entries $\\{c_i\\}_{i=1}^{n-1}$, so that $A_{i,i}=b_i$, $A_{i+1,i}=a_{i+1}$, and $A_{i,i+1}=c_i$. Starting only from the definition of Lower-Upper (LU) factorization and the mechanics of Gaussian elimination via elementary row operations, derive explicit scalar recurrences for the entries of a unit lower triangular matrix $L$ and an upper triangular matrix $U$ satisfying $A=LU$. Your derivation must show that each index update uses a constant number of arithmetic operations and thus yields an order-of $n$ (big-$O(n)$) algorithm. Assume $A$ is strictly diagonally dominant so that no row pivoting is needed.\n\nThen, apply your derived recurrences to solve the linear system $A \\boldsymbol{x}=\\boldsymbol{f}$ for the specific $5 \\times 5$ tridiagonal matrix\n$$\nA=\\begin{pmatrix}\n4 & -1 & 0 & 0 & 0 \\\\\n-1 & 4 & -1 & 0 & 0 \\\\\n0 & -1 & 4 & -1 & 0 \\\\\n0 & 0 & -1 & 4 & -1 \\\\\n0 & 0 & 0 & -1 & 4\n\\end{pmatrix},\n$$\nwith right-hand side vector\n$$\n\\boldsymbol{f}=\\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 2 \\\\ 1\\end{pmatrix}.\n$$\nReport the value of the fifth component $x_5$ of the solution $\\boldsymbol{x}$ in exact rational form. Do not round; provide the exact value as a rational number.",
            "solution": "The problem is valid as it is a well-posed, scientifically grounded problem in numerical linear algebra, with all necessary data and conditions provided.\n\nThe problem consists of two parts. First, deriving the general recursive formulas for the LU factorization of a tridiagonal matrix. Second, applying these formulas to solve a specific linear system.\n\n**Part 1: Derivation of the Recurrence Relations**\n\nLet $A$ be an $n \\times n$ tridiagonal matrix with subdiagonal entries $\\{a_i\\}_{i=2}^{n}$, diagonal entries $\\{b_i\\}_{i=1}^{n}$, and superdiagonal entries $\\{c_i\\}_{i=1}^{n-1}$.\n$$\nA = \\begin{pmatrix}\nb_1 & c_1 & 0 & \\dots & 0 \\\\\na_2 & b_2 & c_2 & \\ddots & \\vdots \\\\\n0 & a_3 & b_3 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & c_{n-1} \\\\\n0 & \\dots & 0 & a_n & b_n\n\\end{pmatrix}\n$$\nWe seek an LU factorization $A = LU$, where $L$ is a unit lower triangular matrix and $U$ is an upper triangular matrix. The problem states that $A$ is strictly diagonally dominant, which ensures that an LU factorization exists without the need for pivoting. The tridiagonal structure of $A$ results in a bidiagonal structure for both $L$ and $U$ (beyond their main diagonals).\n\nLet $L$ be a unit lower bidiagonal matrix and $U$ be an upper bidiagonal matrix:\n$$\nL = \\begin{pmatrix}\n1 & 0 & 0 & \\dots & 0 \\\\\nl_2 & 1 & 0 & \\ddots & \\vdots \\\\\n0 & l_3 & 1 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & 0 \\\\\n0 & \\dots & 0 & l_n & 1\n\\end{pmatrix}, \\quad\nU = \\begin{pmatrix}\nu_1 & d_1 & 0 & \\dots & 0 \\\\\n0 & u_2 & d_2 & \\ddots & \\vdots \\\\\n0 & 0 & u_3 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & d_{n-1} \\\\\n0 & \\dots & 0 & 0 & u_n\n\\end{pmatrix}\n$$\nHere, $L$ is unit lower triangular with non-zero entries $L_{i,i}=1$ and $L_{i,i-1}=l_i$. $U$ is upper triangular with non-zero entries $U_{i,i}=u_i$ and $U_{i,i+1}=d_i$.\n\nBy definition of LU factorization, we equate the entries of $A$ with the entries of the product $LU$:\n$$\nA = LU = \\begin{pmatrix}\n1 & 0 & \\dots \\\\\nl_2 & 1 & \\\\\n\\vdots & \\ddots & \\ddots\n\\end{pmatrix}\n\\begin{pmatrix}\nu_1 & d_1 & 0 & \\dots \\\\\n0 & u_2 & d_2 & \\\\\n\\vdots & & \\ddots & \\ddots\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nu_1 & d_1 & 0 & \\dots \\\\\nl_2 u_1 & l_2 d_1 + u_2 & d_2 & \\\\\n0 & l_3 u_2 & l_3 d_2 + u_3 & \\ddots \\\\\n\\vdots & & \\ddots & \\ddots\n\\end{pmatrix}\n$$\nBy comparing the entries of $A$ and $LU$ row by row:\n\nFor the first row ($i=1$):\n- Diagonal: $A_{1,1} = b_1 = (LU)_{1,1} = u_1$. So, $u_1 = b_1$.\n- Superdiagonal: $A_{1,2} = c_1 = (LU)_{1,2} = d_1$. So, $d_1 = c_1$.\n\nFor any subsequent row $i=2, \\dots, n-1$:\n- Subdiagonal: $A_{i,i-1} = a_i = (LU)_{i,i-1} = l_i u_{i-1}$. This gives $l_i = \\frac{a_i}{u_{i-1}}$.\n- Diagonal: $A_{i,i} = b_i = (LU)_{i,i} = l_i d_{i-1} + u_i$. This gives $u_i = b_i - l_i d_{i-1}$.\n- Superdiagonal: $A_{i,i+1} = c_i = (LU)_{i,i+1} = d_i$. So, $d_i = c_i$.\n\nFor the last row ($i=n$):\n- Subdiagonal: $A_{n,n-1} = a_n = (LU)_{n,n-1} = l_n u_{n-1}$. This gives $l_n = \\frac{a_n}{u_{n-1}}$.\n- Diagonal: $A_{n,n} = b_n = (LU)_{n,n} = l_n d_{n-1} + u_n$. This gives $u_n = b_n - l_n d_{n-1}$.\n\nCombining these, we note that $d_i = c_i$ for all $i=1, \\dots, n-1$. We can write the complete set of recurrences:\n\n1. Initialize for $i=1$:\n   $u_1 = b_1$\n\n2. For $i=2, 3, \\dots, n$, compute:\n   $l_i = \\frac{a_i}{u_{i-1}}$\n   $u_i = b_i - l_i c_{i-1}$\n\nThis is known as the Crout factorization algorithm for a tridiagonal matrix. To find all the entries of $L$ and $U$, we iterate from $i=2$ to $n$. In each iteration, we compute one value $l_i$ (one division) and one value $u_i$ (one multiplication and one subtraction). The number of arithmetic operations per iteration is constant (independent of $n$). Since the loop runs $n-1$ times, the total number of operations is proportional to $n-1$. Therefore, the algorithm has a time complexity of $O(n)$.\n\n**Part 2: Application to the Specific System**\n\nWe need to solve $A \\boldsymbol{x}=\\boldsymbol{f}$ for the specific $5 \\times 5$ system.\nThe matrix $A$ gives the parameters: $n=5$, $b_i = 4$ for all $i$, $a_i = -1$ for $i \\geq 2$, and $c_i = -1$ for $i < 5$. The vector $\\boldsymbol{f}$ has components $f_1=1, f_2=2, f_3=3, f_4=2, f_5=1$.\n\nFirst, we find the LU factorization of $A$ using the derived recurrences.\n- $i=1$:\n  $u_1 = b_1 = 4$\n- $i=2$:\n  $l_2 = \\frac{a_2}{u_1} = \\frac{-1}{4}$\n  $u_2 = b_2 - l_2 c_1 = 4 - (-\\frac{1}{4})(-1) = 4 - \\frac{1}{4} = \\frac{15}{4}$\n- $i=3$:\n  $l_3 = \\frac{a_3}{u_2} = \\frac{-1}{15/4} = -\\frac{4}{15}$\n  $u_3 = b_3 - l_3 c_2 = 4 - (-\\frac{4}{15})(-1) = 4 - \\frac{4}{15} = \\frac{56}{15}$\n- $i=4$:\n  $l_4 = \\frac{a_4}{u_3} = \\frac{-1}{56/15} = -\\frac{15}{56}$\n  $u_4 = b_4 - l_4 c_3 = 4 - (-\\frac{15}{56})(-1) = 4 - \\frac{15}{56} = \\frac{224-15}{56} = \\frac{209}{56}$\n- $i=5$:\n  $l_5 = \\frac{a_5}{u_4} = \\frac{-1}{209/56} = -\\frac{56}{209}$\n  $u_5 = b_5 - l_5 c_4 = 4 - (-\\frac{56}{209})(-1) = 4 - \\frac{56}{209} = \\frac{836-56}{209} = \\frac{780}{209}$\n\nThe equation $A\\boldsymbol{x}=\\boldsymbol{f}$ is equivalent to $LU\\boldsymbol{x}=\\boldsymbol{f}$. We solve this in two steps:\n1. Solve $L\\boldsymbol{y}=\\boldsymbol{f}$ for $\\boldsymbol{y}$ (forward substitution).\n2. Solve $U\\boldsymbol{x}=\\boldsymbol{y}$ for $\\boldsymbol{x}$ (backward substitution).\n\nStep 1: Forward substitution to find $\\boldsymbol{y}$.\nThe system $L\\boldsymbol{y}=\\boldsymbol{f}$ is:\n$y_1 = f_1$\n$l_i y_{i-1} + y_i = f_i$ for $i=2, \\dots, 5$, which implies $y_i = f_i - l_i y_{i-1}$.\n- $y_1 = f_1 = 1$\n- $y_2 = f_2 - l_2 y_1 = 2 - (-\\frac{1}{4})(1) = 2 + \\frac{1}{4} = \\frac{9}{4}$\n- $y_3 = f_3 - l_3 y_2 = 3 - (-\\frac{4}{15})(\\frac{9}{4}) = 3 + \\frac{9}{15} = 3 + \\frac{3}{5} = \\frac{18}{5}$\n- $y_4 = f_4 - l_4 y_3 = 2 - (-\\frac{15}{56})(\\frac{18}{5}) = 2 + \\frac{15 \\cdot 18}{56 \\cdot 5} = 2 + \\frac{3 \\cdot 18}{56} = 2 + \\frac{3 \\cdot 9}{28} = 2 + \\frac{27}{28} = \\frac{56+27}{28} = \\frac{83}{28}$\n- $y_5 = f_5 - l_5 y_4 = 1 - (-\\frac{56}{209})(\\frac{83}{28}) = 1 + \\frac{56 \\cdot 83}{209 \\cdot 28} = 1 + \\frac{2 \\cdot 83}{209} = 1 + \\frac{166}{209} = \\frac{209+166}{209} = \\frac{375}{209}$\n\nStep 2: Backward substitution to find $\\boldsymbol{x}$.\nThe system $U\\boldsymbol{x}=\\boldsymbol{y}$ only needs to be solved for $x_5$. The last equation of the system is $u_5 x_5 = y_5$.\n- $\\frac{780}{209} x_5 = \\frac{375}{209}$\n- $x_5 = \\frac{375/209}{780/209} = \\frac{375}{780}$\n\nFinally, we simplify the fraction for $x_5$:\n$$\nx_5 = \\frac{375}{780} = \\frac{375 \\div 5}{780 \\div 5} = \\frac{75}{156} = \\frac{75 \\div 3}{156 \\div 3} = \\frac{25}{52}\n$$\nThe numbers $25=5^2$ and $52=4 \\times 13$ share no common factors, so the fraction is fully reduced.",
            "answer": "$$\n\\boxed{\\frac{25}{52}}\n$$"
        },
        {
            "introduction": "The factors $L$ and $U$ are more than just computational intermediaries; they hold essential information about the original matrix $A$. This exercise explores the deep connection between the entries of the factors and the fundamental property of singularity. By determining the smallest change to a single entry in $L$ that makes the reconstructed matrix $A'$ singular, you will gain a practical understanding of how invertibility is reflected in the LU factorization. ",
            "id": "3275833",
            "problem": "Consider the following matrices in a Lower-Upper (LU) factorization with partial pivoting. Let the permutation matrix be\n$$\nP=\\begin{pmatrix}\n0 & 1 & 0\\\\\n1 & 0 & 0\\\\\n0 & 0 & 1\n\\end{pmatrix},\n$$\nthe lower triangular matrix be\n$$\nL=\\begin{pmatrix}\n2 & 0 & 0\\\\\n-1 & 0.08 & 0\\\\\n3 & -0.5 & 5\n\\end{pmatrix},\n$$\nand the upper triangular matrix be\n$$\nU=\\begin{pmatrix}\n1 & 4 & -2\\\\\n0 & -3 & 1\\\\\n0 & 0 & 2\n\\end{pmatrix}.\n$$\nThe reconstructed matrix is defined as\n$$\nA=P^{T}LU.\n$$\nYou are allowed to change exactly one entry of the matrix $L$ by adding a scalar perturbation $\\Delta$ to that entry, leaving $P$ and $U$ fixed. Denote the perturbed lower triangular factor by $L'$, identical to $L$ except at the chosen entry where $L'_{ij}=L_{ij}+\\Delta$. Define the perturbed reconstructed matrix as\n$$\nA'=P^{T}L'U.\n$$\nUsing only fundamental facts about determinants, invertibility, and properties of triangular and permutation matrices, determine the smallest possible magnitude of the single-entry change, $\\min|\\Delta|$, that makes $A'$ singular. Report the exact value of this minimal magnitude as a real number. No rounding is required.",
            "solution": "The user wants to find the smallest possible magnitude of a single-entry perturbation to matrix $L$ that makes the matrix $A' = P^{T}L'U$ singular.\n\nA matrix is singular if and only if its determinant is zero. We must therefore find the smallest $|\\Delta|$ such that $\\det(A') = 0$.\n\nUsing the multiplicative property of determinants, we have:\n$$\n\\det(A') = \\det(P^T L' U) = \\det(P^T) \\det(L') \\det(U)\n$$\nFor $\\det(A')$ to be zero, at least one of the determinants on the right-hand side must be zero. We will analyze each factor.\n\n1.  **Determinant of the permutation matrix, $\\det(P^T)$**:\n    The given permutation matrix is\n    $$\n    P = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n    $$\n    This matrix is obtained by swapping the first and second rows of the $3 \\times 3$ identity matrix $I$. A single row swap negates the determinant. Since $\\det(I)=1$, we have $\\det(P) = -1$.\n    A fundamental property of determinants is that $\\det(P^T) = \\det(P)$.\n    Therefore, $\\det(P^T) = -1$. Since this value is non-zero, this factor cannot cause $A'$ to be singular.\n\n2.  **Determinant of the upper triangular matrix, $\\det(U)$**:\n    The upper triangular matrix is\n    $$\n    U = \\begin{pmatrix} 1 & 4 & -2 \\\\ 0 & -3 & 1 \\\\ 0 & 0 & 2 \\end{pmatrix}\n    $$\n    The determinant of a triangular matrix is the product of its diagonal entries.\n    $$\n    \\det(U) = 1 \\times (-3) \\times 2 = -6\n    $$\n    Since $\\det(U) \\neq 0$, this factor also cannot cause $A'$ to be singular.\n\n3.  **Determinant of the perturbed lower triangular matrix, $\\det(L')$**:\n    Since $\\det(P^T) \\neq 0$ and $\\det(U) \\neq 0$, the condition $\\det(A') = 0$ is satisfied if and only if $\\det(L') = 0$.\n    The original matrix $L$ is\n    $$\n    L = \\begin{pmatrix} 2 & 0 & 0 \\\\ -1 & 0.08 & 0 \\\\ 3 & -0.5 & 5 \\end{pmatrix}\n    $$\n    The matrix $L'$ is obtained by perturbing a single entry $L_{ij}$ by an amount $\\Delta$, such that $L'_{ij} = L_{ij} + \\Delta$. We must systematically consider the perturbation of each entry of $L$ to find the minimal $|\\Delta|$ that makes $\\det(L') = 0$.\n\n    **Case 1: Perturbation of a diagonal element ($i=j$)**\n    If a diagonal element $L_{ii}$ is perturbed, the matrix $L'$ remains lower triangular. Its determinant is the product of its new diagonal elements.\n    -   Perturbing $L_{11}$: $L'_{11} = 2 + \\Delta$. To make the new determinant zero, we need $L'_{11} = 0$, which means $2 + \\Delta = 0$, so $\\Delta = -2$. The magnitude is $|\\Delta|=2$.\n    -   Perturbing $L_{22}$: $L'_{22} = 0.08 + \\Delta$. To make the new determinant zero, we need $L'_{22} = 0$, which means $0.08 + \\Delta = 0$, so $\\Delta = -0.08$. The magnitude is $|\\Delta|=0.08$.\n    -   Perturbing $L_{33}$: $L'_{33} = 5 + \\Delta$. To make the new determinant zero, we need $L'_{33} = 0$, which means $5 + \\Delta = 0$, so $\\Delta = -5$. The magnitude is $|\\Delta|=5$.\n\n    **Case 2: Perturbation of a strictly lower-triangular element ($i>j$)**\n    If an element $L_{ij}$ with $i>j$ (i.e., $L_{21}$, $L_{31}$, or $L_{32}$) is perturbed, the matrix $L'$ remains lower triangular, and its diagonal elements are unchanged.\n    The determinant is still the product of the original diagonal elements:\n    $$\n    \\det(L') = L_{11} \\times L_{22} \\times L_{33} = 2 \\times 0.08 \\times 5 = 0.8\n    $$\n    Since $\\det(L') = 0.8 \\neq 0$, perturbing any of these entries cannot make $L'$ singular.\n\n    **Case 3: Perturbation of a strictly upper-triangular element ($i<j$)**\n    The original elements in the strictly upper-triangular part of $L$ are all zero ($L_{12}=0$, $L_{13}=0$, $L_{23}=0$). If one of these is perturbed by $\\Delta$, the matrix $L'$ is no longer triangular, and we must compute its determinant fully.\n    -   Perturbing $L_{12}$: $L'_{12} = \\Delta$.\n        $$\n        L' = \\begin{pmatrix} 2 & \\Delta & 0 \\\\ -1 & 0.08 & 0 \\\\ 3 & -0.5 & 5 \\end{pmatrix}\n        $$\n        Expanding the determinant along the third column:\n        $\\det(L') = 5 \\times \\det \\begin{pmatrix} 2 & \\Delta \\\\ -1 & 0.08 \\end{pmatrix} = 5 \\times (2 \\times 0.08 - \\Delta \\times (-1)) = 5 \\times (0.16 + \\Delta) = 0.8 + 5\\Delta$.\n        Setting $\\det(L') = 0$ gives $0.8 + 5\\Delta = 0$, so $\\Delta = -0.8/5 = -0.16$. The magnitude is $|\\Delta|=0.16$.\n\n    -   Perturbing $L_{13}$: $L'_{13} = \\Delta$.\n        $$\n        L' = \\begin{pmatrix} 2 & 0 & \\Delta \\\\ -1 & 0.08 & 0 \\\\ 3 & -0.5 & 5 \\end{pmatrix}\n        $$\n        Expanding the determinant along the first row:\n        $\\det(L') = 2 \\times \\det \\begin{pmatrix} 0.08 & 0 \\\\ -0.5 & 5 \\end{pmatrix} + \\Delta \\times \\det \\begin{pmatrix} -1 & 0.08 \\\\ 3 & -0.5 \\end{pmatrix} = 2(0.4) + \\Delta(0.5 - 0.24) = 0.8 + 0.26\\Delta$.\n        Setting $\\det(L') = 0$ gives $0.8 + 0.26\\Delta = 0$, so $\\Delta = -0.8/0.26 = -80/26 = -40/13$. The magnitude is $|\\Delta|=40/13 \\approx 3.077$.\n\n    -   Perturbing $L_{23}$: $L'_{23} = \\Delta$.\n        $$\n        L' = \\begin{pmatrix} 2 & 0 & 0 \\\\ -1 & 0.08 & \\Delta \\\\ 3 & -0.5 & 5 \\end{pmatrix}\n        $$\n        Expanding the determinant along the first row:\n        $\\det(L') = 2 \\times \\det \\begin{pmatrix} 0.08 & \\Delta \\\\ -0.5 & 5 \\end{pmatrix} = 2 \\times (0.08 \\times 5 - \\Delta \\times (-0.5)) = 2 \\times (0.4 + 0.5\\Delta) = 0.8 + \\Delta$.\n        Setting $\\det(L') = 0$ gives $0.8 + \\Delta = 0$, so $\\Delta = -0.8$. The magnitude is $|\\Delta|=0.8$.\n\n    **Conclusion**\n    We have found all possible magnitudes of $\\Delta$ that can make $A'$ singular by perturbing a single entry of $L$:\n    $\\{2, 0.08, 5, 0.16, 40/13, 0.8\\}$.\n    The smallest value in this set is the minimum possible magnitude:\n    $$\n    \\min|\\Delta| = \\min\\{2, 0.08, 5, 0.16, 40/13, 0.8\\} = 0.08\n    $$\nThis minimum magnitude is achieved by perturbing the diagonal entry $L_{22}$ by $\\Delta = -0.08$.",
            "answer": "$$\n\\boxed{0.08}\n$$"
        },
        {
            "introduction": "The primary motivation for computing matrix factorizations is computational efficiency, especially when a matrix is involved in multiple operations. This practice provides a vivid illustration of this principle by asking you to compare two different strategies for solving the system $A^k x = b$. By carefully counting the floating-point operations for each method, you will quantitatively demonstrate the immense performance benefit of factoring $A$ once and reusing its factors, a cornerstone concept in designing efficient numerical algorithms. ",
            "id": "3275805",
            "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a dense, nonsingular matrix with no special structure, and let $k \\geq 2$ be an integer. You wish to compute the vector $x$ that satisfies $A^{k} x = b$ for a given right-hand-side $b \\in \\mathbb{R}^{n}$. Consider two strategies:\n\n(a) Explicitly form $A^{k}$ via sequential dense matrix multiplications, then solve the linear system $A^{k} x = b$ by computing a Lower-Upper (LU) factorization of $A^{k}$ followed by forward and backward substitution.\n\n(b) Compute a single Lower-Upper (LU) factorization of $A$, and then solve the sequence of $k$ linear systems $A y_{1} = b$, $A y_{2} = y_{1}$, $\\dots$, $A y_{k} = y_{k-1}$ to obtain $x = y_{k}$ by forward and backward substitution with the reused factors.\n\nUsing the following widely accepted operation counts for dense algorithms as foundational facts: a dense $n \\times n$ matrix-matrix multiplication costs $2n^{3}$ floating-point operations, a dense Lower-Upper (LU) factorization with partial pivoting costs $\\frac{2}{3}n^{3}$ floating-point operations, and a triangular solve (either forward with $L$ or backward with $U$) for a single right-hand side costs $n^{2}$ floating-point operations, derive expressions for the total floating-point operation counts for strategies (a) and (b), and then compute the ratio of the total operation count of strategy (a) to that of strategy (b) as a closed-form expression in $n$ and $k$. Ignore all lower-order terms beyond those stated and treat permutation application as negligible.\n\nExpress your final answer as a single simplified analytic expression in $n$ and $k$.",
            "solution": "The problem requires a comparative analysis of the computational cost, measured in floating-point operations (flops), for two distinct strategies to solve the linear system $A^{k} x = b$. Let $C_a$ and $C_b$ denote the total flop counts for strategies (a) and (b), respectively. The analysis will be based on the provided operation counts for fundamental dense matrix operations.\n\nLet us first analyze strategy (a). This strategy consists of two main stages: explicitly forming the matrix $A^{k}$, and then solving the resulting linear system.\n\nStage 1(a): Formation of $A^{k}$.\nThe matrix $A^{k}$ is computed by performing $k-1$ sequential matrix-matrix multiplications: $A^2 = A \\cdot A$, $A^3 = A^2 \\cdot A$, ..., $A^k = A^{k-1} \\cdot A$. Each multiplication involves two dense $n \\times n$ matrices.\nThe cost of a single dense $n \\times n$ matrix-matrix multiplication is given as $2n^3$ flops.\nSince there are $k-1$ such multiplications, the total cost for this stage is:\n$$C_{1,a} = (k-1) \\times (2n^3) = (2k-2)n^3 \\text{ flops.}$$\n\nStage 2(a): Solving the system $A^{k} x = b$.\nLet $B = A^k$. The system to be solved is $B x = b$. The prescribed method is LU factorization followed by forward and backward substitution.\nFirst, the LU factorization of the dense $n \\times n$ matrix $B$ is computed. The cost for this is given as $\\frac{2}{3}n^3$ flops.\nNext, the system is solved in two steps:\n1.  Solve $L y = b$ for $y$ (forward substitution). The cost for a triangular solve on a single right-hand side is $n^2$ flops.\n2.  Solve $U x = y$ for $x$ (backward substitution). This also costs $n^2$ flops.\nThe total cost for this stage is the sum of the costs of factorization and the two substitutions:\n$$C_{2,a} = \\frac{2}{3}n^3 + n^2 + n^2 = \\frac{2}{3}n^3 + 2n^2 \\text{ flops.}$$\n\nThe total operation count for strategy (a) is the sum of the costs of these two stages:\n$$C_a(n,k) = C_{1,a} + C_{2,a} = (2k-2)n^3 + \\left(\\frac{2}{3}n^3 + 2n^2\\right)$$\n$$C_a(n,k) = \\left(2k - 2 + \\frac{2}{3}\\right)n^3 + 2n^2 = \\left(2k - \\frac{4}{3}\\right)n^3 + 2n^2.$$\n\nNext, let us analyze strategy (b). This strategy avoids forming $A^k$ and instead solves a sequence of $k$ linear systems. The equation $A^k x = b$ can be written as $A(A(...A x)) = b$. This suggests solving a sequence of systems. Let $y_1, y_2, \\dots, y_k$ be intermediate vectors.\n- Let $A y_1 = b$.\n- Let $A y_2 = y_1$. Substituting the first equation into this gives $A(A y_2) = A^2 y_2 = b$.\n- Continuing this process, we define $A y_i = y_{i-1}$ for $i=2, \\dots, k$.\nThe final system is $A y_k = y_{k-1}$, which implies $A^k y_k = b$. Thus, the solution is $x = y_k$.\n\nStage 1(b): LU factorization of $A$.\nThis is performed only once at the beginning. The cost is the standard cost for an LU factorization of a dense $n \\times n$ matrix:\n$$C_{1,b} = \\frac{2}{3}n^3 \\text{ flops.}$$\n\nStage 2(b): Solving $k$ sequential linear systems.\nThe $k$ systems to be solved are $A y_1 = b$, and $A y_i = y_{i-1}$ for $i=2, \\dots, k$.\nFor each system, the LU factors of $A$ are reused. Solving one such system $A z = w$ using existing factors $L$ and $U$ requires one forward substitution ($L v = w$, cost $n^2$) and one backward substitution ($U z = v$, cost $n^2$).\nThe cost to solve a single system is $n^2 + n^2 = 2n^2$ flops.\nSince there are $k$ such systems to solve, the total cost for this stage is:\n$$C_{2,b} = k \\times (2n^2) = 2kn^2 \\text{ flops.}$$\n\nThe total operation count for strategy (b) is the sum of the costs of the initial factorization and the $k$ solves:\n$$C_b(n,k) = C_{1,b} + C_{2,b} = \\frac{2}{3}n^3 + 2kn^2.$$\n\nFinally, we are asked to compute the ratio of the total operation count of strategy (a) to that of strategy (b).\n$$\\text{Ratio} = \\frac{C_a(n,k)}{C_b(n,k)} = \\frac{\\left(2k - \\frac{4}{3}\\right)n^3 + 2n^2}{\\frac{2}{3}n^3 + 2kn^2}.$$\nTo simplify this expression, we can factor out $n^2$ from the numerator and the denominator, as $n \\neq 0$:\n$$\\text{Ratio} = \\frac{n^2 \\left[ \\left(2k - \\frac{4}{3}\\right)n + 2 \\right]}{n^2 \\left[ \\frac{2}{3}n + 2k \\right]} = \\frac{\\left(2k - \\frac{4}{3}\\right)n + 2}{\\frac{2}{3}n + 2k}.$$\nTo eliminate the fractions, we multiply the numerator and the denominator by $3$:\n$$\\text{Ratio} = \\frac{3 \\left[ \\left(2k - \\frac{4}{3}\\right)n + 2 \\right]}{3 \\left[ \\frac{2}{3}n + 2k \\right]} = \\frac{(6k - 4)n + 6}{2n + 6k}.$$\nWe can factor out a common factor of $2$ from the numerator and the denominator:\n$$\\text{Ratio} = \\frac{2 \\left[ (3k - 2)n + 3 \\right]}{2 [n + 3k]} = \\frac{(3k - 2)n + 3}{n + 3k}.$$\nThis is the final simplified analytic expression for the ratio of the costs.",
            "answer": "$$\n\\boxed{\\frac{(3k - 2)n + 3}{n + 3k}}\n$$"
        }
    ]
}