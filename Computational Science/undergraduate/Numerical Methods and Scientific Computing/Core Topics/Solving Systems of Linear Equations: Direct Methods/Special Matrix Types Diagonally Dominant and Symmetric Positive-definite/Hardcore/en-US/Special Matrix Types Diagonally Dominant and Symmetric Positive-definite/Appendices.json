{
    "hands_on_practices": [
        {
            "introduction": "Understanding the properties of a matrix is the first step toward choosing the right numerical tool. This exercise provides a foundational workout on the core definitions of strict diagonal dominance and symmetric positive-definiteness (SPD). By analyzing a given matrix and then determining the precise modification needed to make it SPD, you will gain a concrete understanding of how the eigenvalues govern this crucial property and how it can be controlled.",
            "id": "3276792",
            "problem": "Consider the symmetric matrix $A \\in \\mathbb{R}^{4 \\times 4}$ defined by\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 2 & 0 \\\\\n0 & 0 & 0 & 3\n\\end{pmatrix}.\n$$\nYou will analyze $A$ in the context of special matrix types and spectral conditioning. Use only foundational definitions and well-tested facts from linear algebra.\n\n(a) Decide whether $A$ is strictly diagonally dominant and whether it is symmetric positive-definite (SPD). Justify your conclusions starting from the definition of diagonal dominance and the definition of positive-definiteness $x^{\\top} A x > 0$ for all nonzero $x \\in \\mathbb{R}^{n}$, together with the well-tested fact that for symmetric matrices, positive-definiteness is equivalent to all eigenvalues being strictly positive.\n\n(b) Find the smallest perturbation $\\epsilon \\in \\mathbb{R}$ added to the identity matrix $I$ such that $A + \\epsilon I$ is symmetric positive-definite. Interpret “smallest” in the precise threshold sense: determine the value $\\epsilon_{\\mathrm{th}}$ such that $A + \\epsilon I$ is symmetric positive-definite for all $\\epsilon > \\epsilon_{\\mathrm{th}}$, and report that single threshold value $\\epsilon_{\\mathrm{th}}$ as your answer. No rounding is required.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in linear algebra, well-posed, objective, and self-contained, with no identifiable flaws.\n\nThe given matrix is\n$$\nA = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 2 & 0 \\\\\n0 & 0 & 0 & 3\n\\end{pmatrix}.\n$$\nThe problem statement asserts that $A$ is symmetric. A matrix $M$ is symmetric if $M = M^\\top$. For the given matrix $A$, we can see that $a_{12} = a_{21} = 1$, and all other off-diagonal elements $a_{ij}$ with $i \\neq j$ are zero, which implies $a_{ij} = a_{ji}$ for all $i,j$. Thus, $A$ is indeed symmetric.\n\n(a) We analyze whether $A$ is strictly diagonally dominant and whether it is symmetric positive-definite.\n\nFirst, we consider strict diagonal dominance. A matrix $A$ is strictly diagonally dominant if for each row $i$, the magnitude of the diagonal element is strictly greater than the sum of the magnitudes of all other elements in that row. That is, for all $i \\in \\{1, 2, 3, 4\\}$, the condition $|a_{ii}| > \\sum_{j \\neq i} |a_{ij}|$ must hold.\n\nLet's test this condition for the matrix $A$:\nFor row $i=1$: We must check if $|a_{11}| > |a_{12}| + |a_{13}| + |a_{14}|$.\nThe values are $|0| > |1| + |0| + |0|$, which simplifies to $0 > 1$. This inequality is false.\nSince the condition for strict diagonal dominance is not met for the first row, the matrix $A$ is not strictly diagonally dominant. We do not need to check the other rows.\n\nNext, we determine if $A$ is symmetric positive-definite (SPD). A symmetric matrix is positive-definite if and only if all of its eigenvalues are strictly positive. We will compute the eigenvalues of $A$.\nThe matrix $A$ is a block diagonal matrix of the form $A = \\begin{pmatrix} A_1 & 0 \\\\ 0 & A_2 \\end{pmatrix}$, where\n$$\nA_1 = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\quad \\text{and} \\quad A_2 = \\begin{pmatrix} 2 & 0 \\\\ 0 & 3 \\end{pmatrix}.\n$$\nThe eigenvalues of $A$ are the union of the eigenvalues of $A_1$ and $A_2$.\nThe eigenvalues of the diagonal matrix $A_2$ are its diagonal entries, which are $2$ and $3$. Both are positive.\nTo find the eigenvalues of $A_1$, we solve the characteristic equation $\\det(A_1 - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix}\n0 - \\lambda & 1 \\\\\n1 & 0 - \\lambda\n\\end{pmatrix} = (-\\lambda)(-\\lambda) - (1)(1) = \\lambda^2 - 1 = 0.\n$$\nThe solutions are $\\lambda = 1$ and $\\lambda = -1$.\nThe set of all eigenvalues of $A$ is the collection $\\{-1, 1, 2, 3\\}$. Since one of the eigenvalues is negative ($\\lambda = -1$), the matrix $A$ is not positive-definite.\n\nAs required, we can also justify this conclusion using the definition $x^\\top A x > 0$ for all nonzero vectors $x \\in \\mathbb{R}^4$. If a matrix is not positive-definite, there must exist at least one nonzero vector $x$ for which $x^\\top A x \\le 0$. A canonical choice for such a vector is an eigenvector corresponding to a non-positive eigenvalue. We found an eigenvalue $\\lambda = -1$. Let's find its corresponding eigenvector. The eigenvector $v = (v_1, v_2)^\\top$ for $A_1$ and $\\lambda=-1$ must satisfy $(A_1 - (-1)I)v=0$:\n$$\n\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis gives the equation $v_1 + v_2 = 0$. A simple nonzero solution is $v_1=1, v_2=-1$, so $v = (1, -1)^\\top$. We can extend this to a nonzero eigenvector for the full matrix $A$ by padding with zeros: $x = (1, -1, 0, 0)^\\top$.\nNow we compute the quadratic form $x^\\top A x$:\n$$\nx^\\top A x = \\begin{pmatrix} 1 & -1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & 2 & 0 \\\\\n0 & 0 & 0 & 3\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = (1)(-1) + (-1)(1) = -2.\n$$\nSince we have found a nonzero vector $x$ for which $x^\\top A x = -2 \\le 0$, the matrix $A$ is not positive-definite.\n\nIn summary for part (a): $A$ is not strictly diagonally dominant and it is not symmetric positive-definite.\n\n(b) We seek the smallest real number $\\epsilon$, which we denote $\\epsilon_{\\mathrm{th}}$, such that the matrix $B(\\epsilon) = A + \\epsilon I$ is symmetric positive-definite for all $\\epsilon > \\epsilon_{\\mathrm{th}}$.\n\nFirst, $B(\\epsilon)$ is a sum of two symmetric matrices ($A$ and $\\epsilon I$), so it is also symmetric for any $\\epsilon \\in \\mathbb{R}$.\nA key property relating the eigenvalues of $A$ and $A + \\epsilon I$ is that if $\\lambda$ is an eigenvalue of $A$, then $\\lambda + \\epsilon$ is an eigenvalue of $A + \\epsilon I$. This is because if $Av = \\lambda v$ for an eigenvector $v$, then $(A + \\epsilon I)v = Av + \\epsilon Iv = \\lambda v + \\epsilon v = (\\lambda + \\epsilon)v$.\nThe eigenvalues of $A$ are $\\{-1, 1, 2, 3\\}$.\nTherefore, the eigenvalues of $B(\\epsilon) = A + \\epsilon I$ are $\\{-1+\\epsilon, 1+\\epsilon, 2+\\epsilon, 3+\\epsilon\\}$.\n\nFor $B(\\epsilon)$ to be symmetric positive-definite, all its eigenvalues must be strictly positive. This gives us a system of inequalities:\n\\begin{enumerate}\n    \\item $-1 + \\epsilon > 0 \\implies \\epsilon > 1$\n    \\item $1 + \\epsilon > 0 \\implies \\epsilon > -1$\n    \\item $2 + \\epsilon > 0 \\implies \\epsilon > -2$\n    \\item $3 + \\epsilon > 0 \\implies \\epsilon > -3$\n\\end{enumerate}\nTo satisfy all four inequalities simultaneously, we must satisfy the most restrictive one, which is $\\epsilon > 1$.\nThe set of all $\\epsilon$ for which $A+\\epsilon I$ is SPD is the interval $(1, \\infty)$. The problem asks for the threshold value $\\epsilon_{\\mathrm{th}}$ such that the property holds for all $\\epsilon > \\epsilon_{\\mathrm{th}}$. This threshold is the infimum of the set of valid $\\epsilon$ values.\nThe infimum of the interval $(1, \\infty)$ is $1$.\nThus, the smallest such perturbation threshold is $\\epsilon_{\\mathrm{th}} = 1$. For any $\\epsilon > 1$, the matrix $A+\\epsilon I$ will be symmetric positive-definite. At $\\epsilon=1$, the smallest eigenvalue is $0$, making the matrix positive-semidefinite but not positive-definite.\nThe required threshold value is $\\epsilon_{\\mathrm{th}} = 1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "The symmetric positive-definite property is not just a theoretical curiosity; it is the key that unlocks powerful and efficient algorithms like the Cholesky factorization. This computational exercise bridges theory and practice by asking you to observe exactly how and when the Cholesky algorithm fails as a matrix is perturbed out of the SPD class. By implementing a bisection search to find this \"breaking point,\" you will develop a deeper appreciation for the tight coupling between a matrix's mathematical properties and the behavior of numerical solvers.",
            "id": "3276796",
            "problem": "You are asked to investigate, by construction and computation, whether the unpivoted Cholesky factorization of a symmetric positive-definite matrix fails in a controlled way when the matrix is driven out of the symmetric positive-definite class by modifying a single entry. Work purely in mathematics and algorithmic logic, and assume exact arithmetic is the conceptual base. Your implementation should emulate finite precision real arithmetic with double-precision floating-point.\n\nStart from the following fundamental base:\n- Definition: A real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive-definite (SPD) if $x^{\\top} A x \\gt 0$ for all nonzero $x \\in \\mathbb{R}^{n}$.\n- Fact: The unpivoted Cholesky algorithm attempts to compute $A = LL^{\\top}$ with $L$ lower-triangular and strictly positive diagonal, and succeeds if and only if $A$ is SPD in exact arithmetic.\n- Fact: In the unpivoted Cholesky algorithm (Doolittle-like form for $LL^{\\top}$), at step $k$ one forms the updated diagonal (the $k$-th Schur complement pivot) $s_k = a_{kk} - \\sum_{j=1}^{k-1} \\ell_{kj}^{2}$. If $s_k \\le 0$, then $A$ is not SPD and the algorithm stops; otherwise $\\ell_{kk} = \\sqrt{s_k}$ and the step continues.\n\nYour task:\n- For a given symmetric base matrix $A$ and a symmetric update direction $S$ having a single nonzero position on the diagonal or a symmetric pair of nonzeros off the diagonal, consider the one-parameter family $A(t) = A + t S$ for $t \\in [0, t_{\\max}]$.\n- Assume $A(0)$ is SPD and that there exists some $t^\\star \\in (0, t_{\\max}]$ where $A(t)$ ceases to be SPD.\n- Implement a routine that uses unpivoted Cholesky factorization to test whether $A(t)$ is SPD by checking positivity of each updated diagonal $s_k$ in sequence and stopping at the first nonpositive $s_k$. Log the sequence $\\{s_1, s_2, \\dots\\}$ up to and including the failing pivot.\n- Use bisection on $t$ over $[0, t_{\\max}]$ to find the smallest $t^\\star$ in that interval (to within a prescribed absolute tolerance) such that unpivoted Cholesky fails (i.e., some $s_k \\le 0$). You must not rely on eigenvalue computations; decide SPD solely by the Cholesky test. The bisection relies on the continuity of eigenvalues and the equivalence of SPD with the success of unpivoted Cholesky.\n\nScientific realism requirements:\n- Treat failure as occurring when any updated diagonal $s_k \\le \\varepsilon_{\\text{chol}}$ for a small positive tolerance $\\varepsilon_{\\text{chol}}$ to reflect finite precision. Use $\\varepsilon_{\\text{chol}} = 10^{-12}$.\n- Use absolute bisection tolerance $\\varepsilon_t = 10^{-12}$ and return the upper endpoint of the final bracket as $t^\\star$ so that $A(t^\\star)$ is detected as failing by your Cholesky test.\n\nTracking requirement:\n- For the final $t^\\star$ you find in each test case, run the unpivoted Cholesky test once more on $A(t^\\star)$ and record:\n  1. The failure step index $k_{\\text{fail}}$ (use $1$-based indexing),\n  2. The list of updated diagonal values $\\left[s_1, s_2, \\dots, s_{k_{\\text{fail}}}\\right]$ encountered up to and including the failing pivot.\n\nTest suite:\nProvide results for the following three cases. In each case, $A$ is symmetric, $S$ is symmetric with a single nonzero diagonal entry or a symmetric off-diagonal pair, and $t_{\\max}$ is chosen so that failure occurs within the interval.\n- Case $1$ (strictly diagonally dominant base, off-diagonal perturbation):\n  - $$A = \\begin{bmatrix} 4 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 3 \\end{bmatrix}$$,\n  - $$S = \\begin{bmatrix} 0 & 0 & -1 \\\\ 0 & 0 & 0 \\\\ -1 & 0 & 0 \\end{bmatrix}$$,\n  - $t_{\\max} = 10$.\n- Case $2$ (diagonal reduction to loss of definiteness):\n  - $$A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$$,\n  - $$S = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$$,\n  - $t_{\\max} = 3$.\n- Case $3$ (near-boundary block structure, off-diagonal increase):\n  - $$A = \\begin{bmatrix} 1 & 0.999 & 0 & 0 \\\\ 0.999 & 1 & 0 & 0 \\\\ 0 & 0 & 1.5 & 0.1 \\\\ 0 & 0 & 0.1 & 1.5 \\end{bmatrix}$$,\n  - $$S = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$$,\n  - $t_{\\max} = 0.01$.\n\nFor each case, your program must compute and report:\n- $t^\\star$ rounded to $12$ decimal places,\n- $k_{\\text{fail}}$ as an integer,\n- the list of updated diagonals $\\left[s_1, \\dots, s_{k_{\\text{fail}}}\\right]$ at $t^\\star$, each rounded to $12$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-free, space-free JSON-style list. The output must be a single line string of the form\n  \"[[t1,k1,[p11,p12,...]],[t2,k2,[p21,p22,...]],[t3,k3,[p31,p32,...]]]\"\nwith all floating-point values printed in decimal form. No intermediate text should be printed.\n\nAngle units are not applicable. No physical units are involved. All numeric answers must be returned as decimal floats or integers as described. The problem is universally applicable and must be fully solvable in any modern language; you must implement it in the specified language with the required libraries only.",
            "solution": "### Step 1: Extract Givens\n- **Definition**: A real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive-definite (SPD) if $x^{\\top} A x > 0$ for all nonzero $x \\in \\mathbb{R}^{n}$.\n- **Fact**: The unpivoted Cholesky algorithm attempts to compute $A = LL^{\\top}$ where $L$ is lower-triangular with a strictly positive diagonal. It succeeds if and only if $A$ is SPD (in exact arithmetic).\n- **Algorithm Detail**: At step $k$ of the unpivoted Cholesky algorithm, the updated diagonal pivot is $s_k = a_{kk} - \\sum_{j=1}^{k-1} \\ell_{kj}^{2}$. If $s_k \\le 0$, the algorithm fails. Otherwise, $\\ell_{kk} = \\sqrt{s_k}$.\n- **Problem Setup**:\n    - A one-parameter family of matrices is defined as $A(t) = A + t S$ for $t \\in [0, t_{\\max}]$.\n    - $A(0)$ is given to be SPD.\n    - $A(t)$ is known to cease being SPD for some $t^\\star \\in (0, t_{\\max}]$.\n- **Task**:\n    1.  Implement an unpivoted Cholesky factorization routine to test for the SPD property. Failure is defined as any pivot $s_k \\le \\varepsilon_{\\text{chol}}$.\n    2.  Use a bisection method on the parameter $t$ over the interval $[0, t_{\\max}]$ to find the smallest $t^\\star$ where the Cholesky factorization fails.\n    3.  For the computed $t^\\star$, record the failure step index $k_{\\text{fail}}$ ($1$-based) and the sequence of pivots $[s_1, s_2, \\dots, s_{k_{\\text{fail}}}]$.\n- **Tolerances**:\n    - Cholesky failure tolerance: $\\varepsilon_{\\text{chol}} = 10^{-12}$.\n    - Bisection absolute tolerance: $\\varepsilon_t = 10^{-12}$.\n- **Test Cases**:\n    - **Case 1**:\n        - $A = \\begin{bmatrix} 4 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 3 \\end{bmatrix}$\n        - $S = \\begin{bmatrix} 0 & 0 & -1 \\\\ 0 & 0 & 0 \\\\ -1 & 0 & 0 \\end{bmatrix}$\n        - $t_{\\max} = 10$\n    - **Case 2**:\n        - $A = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$\n        - $S = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$\n        - $t_{\\max} = 3$\n    - **Case 3**:\n        - $A = \\begin{bmatrix} 1 & 0.999 & 0 & 0 \\\\ 0.999 & 1 & 0 & 0 \\\\ 0 & 0 & 1.5 & 0.1 \\\\ 0 & 0 & 0.1 & 1.5 \\end{bmatrix}$\n        - $S = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$\n        - $t_{\\max} = 0.01$\n- **Output Requirements**:\n    - Report $t^\\star$ rounded to $12$ decimal places.\n    - Report $k_{\\text{fail}}$ as an integer.\n    - Report the list of pivots at $t^\\star$, each rounded to $12$ decimal places.\n    - The final program output must be a single-line string: `[[t1,k1,[p11,p12,...]],[t2,k2,[p21,p22,...]],[t3,k3,[p31,p32,...]]]`\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the fundamental theory of matrix analysis and numerical methods. The relationship between symmetric positive-definiteness and the success of Cholesky factorization is a cornerstone theorem. The use of bisection to find the boundary of the SPD cone for a parameterized matrix is a standard and well-posed numerical investigation. The problem is stated with mathematical precision, providing all necessary matrices, parameters, algorithms, and tolerances. It is objective, self-contained, and free of contradictions or ambiguities. It does not violate any of the specified criteria for invalidity.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be constructed.\n\n### Method\nThe problem requires finding the critical parameter $t^\\star$ at which a matrix family $A(t) = A + tS$ ceases to be symmetric positive-definite (SPD). The core of the method is to leverage the fact that a symmetric matrix is SPD if and only if its unpivoted Cholesky factorization $A=LL^\\top$ exists, where $L$ is lower triangular with strictly positive diagonal entries.\n\nThe overall procedure is as follows:\n\n1.  **Cholesky SPD Test**: A function is implemented to test if a given symmetric matrix $M$ is SPD. This function attempts to compute the Cholesky factor $L$. At each step $k$ (for $k=1, \\dots, n$), it computes the prospective diagonal term $\\ell_{kk}$ via the pivot $s_k$. The formula for the pivot is derived from the matrix equation $M=LL^\\top$:\n    $$m_{kk} = \\sum_{j=1}^{k} \\ell_{kj}^2 = \\ell_{kk}^2 + \\sum_{j=1}^{k-1} \\ell_{kj}^2$$\n    This gives the expression for the $k$-th pivot, which is the $k$-th diagonal entry of the Schur complement:\n    $$s_k = \\ell_{kk}^2 = m_{kk} - \\sum_{j=1}^{k-1} \\ell_{kj}^2$$\n    The matrix $M$ is not SPD if any $s_k \\le 0$. To account for finite-precision arithmetic, we test against a small positive tolerance, $s_k \\le \\varepsilon_{\\text{chol}}$. If this condition is met, the factorization fails, and the function reports that the matrix is not SPD, returning the failure index $k$ and the list of pivots computed up to that point. If the factorization completes for all $k$, the matrix is deemed SPD.\n\n2.  **Bisection Search**: The Cholesky SPD test serves as a boolean function $f(t)$ which is true if $A(t)$ is SPD and false otherwise. We are given that $f(0)$ is true and $f(t_{\\max})$ is false. Since the eigenvalues of $A(t)$ are continuous functions of $t$, and a matrix is SPD if and only if all its eigenvalues are positive, the set of $t$ for which $A(t)$ is SPD is a continuous interval $[0, t^\\star)$. The bisection method is therefore perfectly suited to find $t^\\star$.\n    - Initialize the search interval $[t_{\\text{low}}, t_{\\text{high}}] = [0, t_{\\max}]$.\n    - Iterate while the interval width $(t_{\\text{high}} - t_{\\text{low}})$ is greater than the specified tolerance $\\varepsilon_t$.\n    - In each iteration, calculate the midpoint $t_{\\text{mid}} = t_{\\text{low}} + (t_{\\text{high}} - t_{\\text{low}}) / 2$.\n    - Test the matrix $A(t_{\\text{mid}})$ using the Cholesky SPD test.\n        - If $A(t_{\\text{mid}})$ is SPD, it means the failure point is at a higher value of $t$. We update the interval by setting $t_{\\text{low}} = t_{\\text{mid}}$.\n        - If $A(t_{\\text{mid}})$ is not SPD, the failure point is at or below $t_{\\text{mid}}$. We update the interval by setting $t_{\\text{high}} = t_{\\text{mid}}$.\n    - The process terminates when the search interval is sufficiently small. The problem specifies returning the upper endpoint of the final bracket, $t_{\\text{high}}$, as the estimate for $t^\\star$.\n\n3.  **Result Computation**: For each test case, the bisection search is run to find its corresponding $t^\\star$. Then, the matrix $A(t^\\star)$ is formed, and the Cholesky SPD test is run one final time to obtain the definitive failure index $k_{\\text{fail}}$ and the list of pivots $[s_1, s_2, \\dots, s_{k_{\\text{fail}}}]$. These results are rounded and formatted as per the problem specification.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the boundary of the symmetric positive-definite\n    cone for a parameterized family of matrices using bisection and Cholesky factorization.\n    \"\"\"\n    \n    # Define constants\n    EPS_CHOL = 1e-12\n    EPS_T = 1e-12\n\n    def cholesky_spd_test(M, eps_chol):\n        \"\"\"\n        Tests if a matrix M is symmetric positive-definite (SPD) by attempting\n        an unpivoted Cholesky factorization.\n\n        Args:\n            M (np.ndarray): The matrix to test.\n            eps_chol (float): The tolerance for checking pivot positivity.\n\n        Returns:\n            tuple: (is_spd, pivots, k_fail)\n                is_spd (bool): True if M is SPD, False otherwise.\n                pivots (list): List of calculated pivots s_k.\n                k_fail (int): 1-based index of the failing step, or n if successful.\n        \"\"\"\n        n = M.shape[0]\n        L = np.zeros_like(M, dtype=np.float64)\n        pivots = []\n\n        for k in range(n):  # k is the 0-based index for the current column/row\n            # Calculate s_k = m_kk - sum_{j=0}^{k-1} l_kj^2\n            sum_sq = np.dot(L[k, :k], L[k, :k])\n            s_k = M[k, k] - sum_sq\n            pivots.append(s_k)\n\n            if s_k = eps_chol:\n                return False, pivots, k + 1  # 1-based failure index\n\n            l_kk = np.sqrt(s_k)\n            L[k, k] = l_kk\n\n            # Calculate the rest of column k in L\n            # l_ik = (m_ik - sum_{j=0}^{k-1} l_ij * l_kj) / l_kk\n            for i in range(k + 1, n):\n                sum_prod = np.dot(L[i, :k], L[k, :k])\n                L[i, k] = (M[i, k] - sum_prod) / l_kk\n        \n        return True, pivots, n\n\n    def find_failure_t(A, S, t_max, eps_t, eps_chol):\n        \"\"\"\n        Finds the smallest t* where A(t) = A + t*S is no longer SPD\n        using a bisection search.\n        \"\"\"\n        t_low = 0.0\n        t_high = t_max\n\n        # The problem statement guarantees A(0) is SPD and A(t_max) is not.\n        \n        while (t_high - t_low) > eps_t:\n            t_mid = t_low + (t_high - t_low) / 2.0\n            A_mid = A + t_mid * S\n            is_spd, _, _ = cholesky_spd_test(A_mid, eps_chol)\n\n            if is_spd:\n                t_low = t_mid\n            else:\n                t_high = t_mid\n        \n        return t_high\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[4, -1, 0], [-1, 4, -1], [0, -1, 3]], dtype=np.float64),\n            \"S\": np.array([[0, 0, -1], [0, 0, 0], [-1, 0, 0]], dtype=np.float64),\n            \"t_max\": 10.0\n        },\n        {\n            \"A\": np.array([[2, 0, 0], [0, 2, 0], [0, 0, 2]], dtype=np.float64),\n            \"S\": np.array([[0, 0, 0], [0, -1, 0], [0, 0, 0]], dtype=np.float64),\n            \"t_max\": 3.0\n        },\n        {\n            \"A\": np.array([[1, 0.999, 0, 0], [0.999, 1, 0, 0], [0, 0, 1.5, 0.1], [0, 0, 0.1, 1.5]], dtype=np.float64),\n            \"S\": np.array([[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], dtype=np.float64),\n            \"t_max\": 0.01\n        }\n    ]\n\n    results_data = []\n    for case in test_cases:\n        A = case[\"A\"]\n        S = case[\"S\"]\n        t_max = case[\"t_max\"]\n\n        # Find t_star using bisection\n        t_star = find_failure_t(A, S, t_max, EPS_T, EPS_CHOL)\n        \n        # Get final failure data at t_star\n        A_star = A + t_star * S\n        is_spd_final, pivots, k_fail = cholesky_spd_test(A_star, EPS_CHOL)\n        \n        results_data.append((t_star, k_fail, pivots))\n\n    # Format the final output string exactly as required\n    results_str_list = []\n    for t_star, k_fail, pivots in results_data:\n        pivots_str = \",\".join([f\"{p:.12f}\" for p in pivots])\n        case_str = f\"[{t_star:.12f},{k_fail},[{pivots_str}]]\"\n        results_str_list.append(case_str)\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "In the world of scientific computing, theory and practice can sometimes diverge due to the finite-precision nature of computers. This exercise explores a critical aspect of numerical stability: how a matrix that is theoretically symmetric positive-definite, like the Hilbert matrix, can fail a Cholesky factorization test in a real-world computational environment due to the accumulation of roundoff errors. By simulating finite-precision arithmetic and experimenting with regularization, you will uncover why numerical analysts must be concerned not just with mathematical correctness but also with algorithmic robustness.",
            "id": "3276870",
            "problem": "Consider the foundational fact that a real, symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is Symmetric Positive-Definite (SPD) if and only if $x^{\\top} A x  0$ for all nonzero $x \\in \\mathbb{R}^n$. From this definition and the theory of matrix factorization, it follows that an SPD matrix admits a unique Cholesky factorization, namely $A = L L^{\\top}$ where $L$ is a lower-triangular matrix with strictly positive diagonal entries. In exact arithmetic, this factorization exists for every SPD matrix.\n\nHowever, in finite-precision floating-point arithmetic, each elementary operation is subject to rounding. Let a single finite-precision operation be modeled as mapping a real value $y$ to $\\operatorname{round}(y)$, where $\\operatorname{round}(y)$ denotes the value after rounding to a specified number of significant digits. Because of the accumulation of rounding errors, an algorithm that in exact arithmetic would produce a valid Cholesky factorization may, in finite precision, encounter a nonpositive pivot (that is, a computed value of $A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2 \\le 0$) and terminate with failure. This phenomenon is especially pronounced for ill-conditioned SPD matrices such as the Hilbert matrix, whose entries are defined by $H_{ij} = \\frac{1}{i + j - 1}$.\n\nYour task is to design and implement a self-contained program that simulates finite-precision arithmetic by rounding intermediate results to a fixed number of significant digits and uses this simulation to demonstrate how floating-point roundoff can cause a theoretically SPD matrix (specifically, the Hilbert matrix) to fail a Cholesky factorization test in practice. The program must implement:\n- Construction of the Hilbert matrix $H \\in \\mathbb{R}^{n \\times n}$ via $H_{ij} = \\frac{1}{i + j - 1}$.\n- A Cholesky factorization routine that performs all intermediate arithmetic in simulated finite precision by rounding to a specified number of significant digits after each addition, subtraction, multiplication, division, and square root.\n- A diagonal regularization operation that replaces $A$ by $A + \\varepsilon I$, where $I$ is the identity matrix and $\\varepsilon  0$ is a chosen regularization parameter, to test how adding a small positive diagonal term can stabilize the factorization under finite precision.\n\nUse the following test suite of parameter values to cover multiple facets of the phenomenon:\n1. Happy path (high precision): $n = 5$, significant digits $s = 16$, no regularization ($\\varepsilon = 0$). Report a boolean indicating whether the simulated-precision Cholesky factorization fails.\n2. Rounding-induced failure (moderate precision): $n = 12$, significant digits $s = 6$, no regularization ($\\varepsilon = 0$). Report a boolean indicating whether the factorization fails.\n3. Strong rounding-induced failure: $n = 12$, significant digits $s = 4$, no regularization ($\\varepsilon = 0$). Report a boolean indicating whether the factorization fails.\n4. Stabilization by diagonal regularization: For $n = 12$ and significant digits $s = 6$, search for the smallest $\\varepsilon$ in the set $\\{10^{-12}, 10^{-10}, 10^{-8}, 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\\}$ such that the simulated-precision Cholesky factorization of $H + \\varepsilon I$ succeeds. Report this $\\varepsilon$ as a float. If none succeeds, report $\\mathrm{NaN}$.\n5. Verification after regularization: Using the $\\varepsilon$ found in Test 4 (or $\\mathrm{NaN}$ if none), run the simulated-precision Cholesky factorization again for $n = 12$, $s = 6$. Report a boolean indicating whether the factorization fails.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where results correspond in order to the five tests above. All reported values must be booleans or floats as specified. No physical units or angle units are involved in this problem.",
            "solution": "The problem requires an analysis of the numerical stability of the Cholesky factorization algorithm when applied to an ill-conditioned Symmetric Positive-Definite (SPD) matrix under simulated finite-precision arithmetic. We will construct a program to demonstrate this phenomenon using the Hilbert matrix.\n\n### Principle of Cholesky Factorization\n\nA real, symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined as Symmetric Positive-Definite (SPD) if the quadratic form $x^{\\top} A x$ is strictly positive for all non-zero vectors $x \\in \\mathbb{R}^n$. A fundamental theorem in linear algebra states that a matrix is SPD if and only if it admits a unique Cholesky factorization, $A = L L^{\\top}$, where $L \\in \\mathbb{R}^{n \\times n}$ is a lower-triangular matrix with strictly positive diagonal entries ($L_{ii}  0$).\n\nThe elements of $L$ can be computed sequentially. The formulas for the entries $L_{ij}$ (using $1$-based indexing) are:\n- For diagonal elements ($j=i$):\n$$L_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2}$$\n- For off-diagonal elements ($ji$):\n$$L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=1}^{j-1} L_{ik}L_{jk} \\right)$$\n\nIn exact arithmetic, for any SPD matrix $A$, the term under the square root, $A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2$, which corresponds to a pivot in Gaussian elimination, is guaranteed to be positive.\n\n### The Hilbert Matrix and Ill-Conditioning\n\nThe Hilbert matrix, $H$, is a classic example of an ill-conditioned SPD matrix. Its entries are given by:\n$$H_{ij} = \\frac{1}{i + j - 1}$$\nThe condition number of $H$, a measure of its sensitivity to numerical errors, grows extremely rapidly with its dimension $n$. A high condition number implies that small relative errors in the input data can lead to large relative errors in the solution.\n\n### Finite-Precision Arithmetic and Numerical Instability\n\nDigital computers represent real numbers using a finite number of bits (e.g., in `float64` format). This limitation means that most real numbers cannot be stored exactly, and arithmetic operations are subject to rounding errors. We simulate this by rounding the result of every elementary operation ($+,-,\\times,/, \\sqrt{\\cdot}$) to a specified number of significant digits, $s$.\n\nA number $x$ is rounded to $s$ significant digits by converting it to a floating-point representation of the form $\\pm d_1.d_2...d_s \\times 10^e$. This can be achieved by a function $\\mathrm{round}(x, s)$.\n\nWhen the Cholesky algorithm is executed in finite-precision arithmetic, the accumulation of rounding errors can become significant, especially for ill-conditioned matrices like the Hilbert matrix. A critical failure occurs if the computed value of the radicand $A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2$ becomes non-positive ($\\le 0$). This is often due to catastrophic cancellation, where subtracting two nearly equal numbers results in a loss of significant digits.\n\n### Algorithmic Design\n\n1.  **Finite-Precision Simulation**: We will implement a helper utility that takes a number and rounds it to $s$ significant digits. This rounding function will be applied after every addition, subtraction, multiplication, division, and square root operation within the Cholesky factorization routine.\n\n2.  **Cholesky Factorization with Failure Detection**: The standard Cholesky algorithm is implemented. However, at each step $i$ of computing the diagonal element $L_{ii}$, the program will calculate the term $v_i = A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2$ using the simulated finite-precision operations. It will then check if $v_i \\le 0$. If this condition is met, the factorization is declared a failure, and the algorithm terminates. Otherwise, it proceeds by computing $L_{ii} = \\sqrt{v_i}$.\n\n3.  **Diagonal Regularization**: To counteract the effects of rounding errors, a technique called diagonal regularization can be used. This involves modifying the original matrix $A$ to $A' = A + \\varepsilon I$, where $I$ is the identity matrix and $\\varepsilon  0$ is a small positive parameter. For an SPD matrix $A$ with eigenvalues $\\lambda_k$, the regularized matrix $A'$ is also SPD and has eigenvalues $\\lambda_k + \\varepsilon$. This uniform increase in eigenvalues makes the matrix \"more\" positive-definite, often improving its condition number and stabilizing the Cholesky factorization by ensuring the pivots remain positive.\n\n### Execution of Test Cases\n\nThe provided test suite explores the interplay between matrix dimension $n$, numerical precision $s$, and regularization $\\varepsilon$:\n-   **Test 1 ($n=5, s=16, \\varepsilon=0$)**: A low-dimension Hilbert matrix with high precision. Failure is not expected.\n-   **Test 2 ($n=12, s=6, \\varepsilon=0$)**: A higher-dimension matrix with reduced precision. This is designed to show failure due to roundoff.\n-   **Test 3 ($n=12, s=4, \\varepsilon=0$)**: The same matrix with even lower precision, making failure more certain.\n-   **Test 4 ($n=12, s=6$)**: A search is conducted for the minimum regularization parameter $\\varepsilon$ from a given set that is sufficient to prevent factorization failure.\n-   **Test 5**: This test verifies the result of Test 4, confirming that the factorization succeeds with the identified $\\varepsilon$.\n\nBy executing these tests, the program will concretely demonstrate a core concept in numerical analysis: algorithms that are mathematically sound in theory can fail in practice due to the finite-precision nature of computer arithmetic, and numerical stabilization techniques are essential for robust scientific computing.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass FinitePrecisionArithmetic:\n    \"\"\"\n    A class to simulate arithmetic operations with a fixed number of\n    significant digits.\n    \"\"\"\n    def __init__(self, significant_digits):\n        self.s = significant_digits\n\n    def round(self, value):\n        \"\"\"Rounds a number to the specified number of significant digits.\"\"\"\n        if not np.isfinite(value) or value == 0:\n            return value\n        return float(f\"{value:.{self.s}g}\")\n\n    def add(self, a, b):\n        return self.round(a + b)\n\n    def sub(self, a, b):\n        return self.round(a - b)\n\n    def mul(self, a, b):\n        return self.round(a * b)\n\n    def div(self, a, b):\n        if b == 0:\n            return np.inf\n        return self.round(a / b)\n\n    def sqrt(self, a):\n        if a  0:\n            return np.nan\n        return self.round(np.sqrt(a))\n\ndef create_hilbert_matrix(n):\n    \"\"\"Constructs an n x n Hilbert matrix.\"\"\"\n    H = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            H[i, j] = 1 / (i + j + 1)\n    return H\n\ndef cholesky_sim(A, s):\n    \"\"\"\n    Performs Cholesky factorization with simulated finite-precision arithmetic.\n    Returns (True, L) if factorization fails, (False, L) if it succeeds.\n    \"\"\"\n    n = A.shape[0]\n    L = np.zeros_like(A, dtype=float)\n    fpa = FinitePrecisionArithmetic(s)\n\n    for i in range(n):\n        for j in range(i + 1):\n            if i == j:  # Diagonal elements L[i, i]\n                sum_sq = 0.0\n                for k in range(j):\n                    term = fpa.mul(L[j, k], L[j, k])\n                    sum_sq = fpa.add(sum_sq, term)\n                \n                val_under_sqrt = fpa.sub(A[j, j], sum_sq)\n                \n                if val_under_sqrt = 0:\n                    return True, L  # Factorization fails\n\n                L[j, j] = fpa.sqrt(val_under_sqrt)\n            else:  # Off-diagonal elements L[i, j]\n                sum_prod = 0.0\n                for k in range(j):\n                    term = fpa.mul(L[i, k], L[j, k])\n                    sum_prod = fpa.add(sum_prod, term)\n                \n                numerator = fpa.sub(A[i, j], sum_prod)\n\n                if L[j, j] == 0:\n                    return True, L # Division by zero indicates failure\n\n                L[i, j] = fpa.div(numerator, L[j, j])\n    \n    return False, L  # Factorization succeeds\n\n\ndef solve():\n    \"\"\"\n    Executes the test suite and prints the results in the required format.\n    \"\"\"\n    results = []\n\n    # Test 1: Happy path (high precision)\n    n, s, eps = 5, 16, 0.0\n    H1 = create_hilbert_matrix(n)\n    fails1, _ = cholesky_sim(H1, s)\n    results.append(fails1)\n\n    # Test 2: Rounding-induced failure (moderate precision)\n    n, s, eps = 12, 6, 0.0\n    H2 = create_hilbert_matrix(n)\n    fails2, _ = cholesky_sim(H2, s)\n    results.append(fails2)\n\n    # Test 3: Strong rounding-induced failure\n    n, s, eps = 12, 4, 0.0\n    H3 = create_hilbert_matrix(n)\n    fails3, _ = cholesky_sim(H3, s)\n    results.append(fails3)\n\n    # Test 4: Stabilization by diagonal regularization\n    n, s = 12, 6\n    H_base = create_hilbert_matrix(n)\n    eps_values = [1e-12, 1e-10, 1e-8, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n    \n    found_eps = np.nan\n    for eps_test in eps_values:\n        H_reg = H_base.copy()\n        for i in range(n):\n            H_reg[i, i] += eps_test\n        \n        fails, _ = cholesky_sim(H_reg, s)\n        if not fails:\n            found_eps = eps_test\n            break\n    results.append(found_eps)\n\n    # Test 5: Verification after regularization\n    if np.isnan(found_eps):\n        # If no epsilon was found, factorization is defined to fail.\n        results.append(True)\n    else:\n        H_reg_verify = H_base.copy()\n        for i in range(n):\n            H_reg_verify[i, i] += found_eps\n            \n        fails5, _ = cholesky_sim(H_reg_verify, s)\n        results.append(fails5)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}