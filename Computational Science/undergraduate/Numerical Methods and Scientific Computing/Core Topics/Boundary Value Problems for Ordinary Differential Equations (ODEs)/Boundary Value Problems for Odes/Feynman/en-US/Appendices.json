{
    "hands_on_practices": [
        {
            "introduction": "The best way to learn numerical methods is by building them. This first practice guides you through constructing a finite difference solver for a simple boundary value problem from the ground up. You will use the Method of Manufactured Solutions—where you define the solution beforehand to derive the problem—which is a fundamental technique for verifying that your code is working correctly and achieving the expected accuracy .",
            "id": "3104017",
            "problem": "Consider the following manufactured-solution benchmark for a linear two-point Boundary Value Problem (BVP) for an ordinary differential equation (ODE) on the interval $[0,1]$. The exact solution is prescribed as $u(x)=e^{x}-x$, with Dirichlet boundary conditions $u(0)=1$ and $u(1)=e-1$. Your tasks:\n\n1) Derive a linear, constant-coefficient, second-order ODE with a polynomial right-hand side such that the exact solution $u(x)=e^{x}-x$ satisfies the ODE and the boundary conditions. Start from the definitions of the first and second derivatives and compute $u'(x)$ and $u''(x)$ to construct an ODE of the form $u''(x)-u(x)=g(x)$, then identify $g(x)$ that makes $u(x)$ an exact solution.\n\n2) Discretize the resulting BVP on a uniform grid $x_{i}=ih$ with $i\\in\\{0,1,\\dots,N+1\\}$, where $N$ is the number of interior points and $h=1/(N+1)$. Using the central-difference approximation for the second derivative derived from Taylor expansions, build the corresponding tridiagonal linear system for the unknowns $\\{u_{1},u_{2},\\dots,u_{N}\\}$ that enforces the boundary conditions $u_{0}=1$ and $u_{N+1}=e-1$.\n\n3) Implement a complete, runnable program that:\n- Constructs and solves the tridiagonal linear system for each $N$ in the test suite $N\\in\\{1,10,20,40\\}$.\n- Reconstructs the grid function including boundary values and computes the maximum absolute error over all grid points $x_{i}$, $i\\in\\{0,1,\\dots,N+1\\}$, compared to the exact solution $u(x)=e^{x}-x$.\n- Reports, for each $N$, the maximum absolute error as a floating-point number rounded to eight decimal places.\n\nFoundational base you may use:\n- Definitions of the first derivative $u'(x)=\\lim_{h\\to 0}\\dfrac{u(x+h)-u(x)}{h}$ and the second derivative $u''(x)=\\lim_{h\\to 0}\\dfrac{u(x+h)-2u(x)+u(x-h)}{h^{2}}$.\n- Taylor expansions about a point $x$ for a sufficiently smooth function.\n- Standard linear algebra for solving linear systems.\n\nScientific realism requirements:\n- Work purely in mathematical terms without physical units.\n- Angles and percentages are not involved.\n\nTest suite and output specification:\n- Use the test suite $N\\in\\{1,10,20,40\\}$.\n- For each $N$, compute the maximum absolute error over the full grid, including the endpoints.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite values of $N$. Each entry must be rounded to eight decimal places. For example, an output with three hypothetical results should look like [$r_{1},$ $r_{2},$ $r_{3}$], where each $r_{k}$ is a decimal number rounded to eight decimal places.\n- The program must be self-contained and require no user input.",
            "solution": "The user-provided problem is a well-defined exercise in numerical analysis, specifically the application of the finite difference method to a two-point boundary value problem (BVP). The problem is valid as it is mathematically sound, self-contained, and objective. It is based on established principles of calculus and numerical methods. All provided data, including the manufactured solution and boundary conditions, are consistent.\n$u(x) = e^x - x$\n$u(0) = e^0 - 0 = 1$\n$u(1) = e^1 - 1 = e-1$\nThe boundary conditions are correctly stated. The problem will be solved in three parts as requested.\n\n### Part 1: Derivation of the Boundary Value Problem\n\nThe problem asks to derive a linear, constant-coefficient, second-order ODE of the form $u''(x) - u(x) = g(x)$ for which the exact solution is $u(x) = e^x - x$.\n\nFirst, we compute the first and second derivatives of the given solution $u(x)$.\nThe exact solution is:\n$$u(x) = e^x - x$$\n\nThe first derivative, $u'(x)$, is:\n$$u'(x) = \\frac{d}{dx}(e^x - x) = e^x - 1$$\n\nThe second derivative, $u''(x)$, is:\n$$u''(x) = \\frac{d}{dx}(e^x - 1) = e^x$$\n\nNow, we substitute $u(x)$ and $u''(x)$ into the specified form of the ODE, $u''(x) - u(x) = g(x)$, to find the right-hand side function $g(x)$:\n$$g(x) = u''(x) - u(x) = (e^x) - (e^x - x)$$\n$$g(x) = e^x - e^x + x = x$$\n\nThe function $g(x) = x$ is a polynomial, as required. Therefore, the complete BVP is:\n$$\n\\begin{cases}\nu''(x) - u(x) = x, & x \\in [0, 1] \\\\\nu(0) = 1 \\\\\nu(1) = e - 1\n\\end{cases}\n$$\n\n### Part 2: Discretization and Formulation of the Linear System\n\nNext, we discretize the derived BVP using the finite difference method on a uniform grid. The domain $[0, 1]$ is divided into $N+1$ subintervals of equal width $h = 1/(N+1)$. The grid points are $x_i = ih$ for $i = 0, 1, \\dots, N+1$. The values of the solution at these grid points are denoted by $u_i = u(x_i)$.\n\nThe ODE $u''(x) - u(x) = x$ must hold at each interior grid point $x_i$ for $i = 1, 2, \\dots, N$:\n$$u''(x_i) - u(x_i) = x_i$$\n\nWe approximate the second derivative $u''(x_i)$ using the second-order central difference formula:\n$$u''(x_i) \\approx \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}$$\n\nSubstituting this approximation into the ODE gives a system of algebraic equations for the unknown interior values $\\{u_1, u_2, \\dots, u_N\\}$:\n$$\\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2} - u_i = x_i, \\quad \\text{for } i = 1, 2, \\dots, N$$\n\nTo form a linear system, we rearrange the equation, grouping terms involving the unknowns on the left side:\n$$u_{i-1} - 2u_i - h^2 u_i + u_{i+1} = h^2 x_i$$\n$$1 \\cdot u_{i-1} + (-2 - h^2)u_i + 1 \\cdot u_{i+1} = h^2 (ih) = ih^3$$\n\nThis set of $N$ equations forms a tridiagonal linear system $A\\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u} = [u_1, u_2, \\dots, u_N]^T$.\n\nThe $N \\times N$ matrix $A$ has the following structure:\n- Main diagonal elements: $A_{ii} = -2 - h^2$\n- Sub-diagonal elements: $A_{i, i-1} = 1$\n- Super-diagonal elements: $A_{i, i+1} = 1$\n\n$$A = \\begin{pmatrix}\n-2-h^2 & 1 & 0 & \\dots & 0 \\\\\n1 & -2-h^2 & 1 & \\dots & 0 \\\\\n0 & 1 & -2-h^2 & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & 1 \\\\\n0 & \\dots & 0 & 1 & -2-h^2\n\\end{pmatrix}$$\n\nThe right-hand side vector $\\mathbf{b}$ is an $N \\times 1$ column vector. We must incorporate the given Dirichlet boundary conditions, $u_0 = 1$ and $u_{N+1} = e-1$.\n\nFor the first equation ($i=1$):\n$$u_0 + (-2 - h^2)u_1 + u_2 = 1 \\cdot h^3$$\nSince $u_0 = 1$ is known, we move it to the right-hand side:\n$$(-2 - h^2)u_1 + u_2 = h^3 - u_0 = h^3 - 1$$\nThus, the first element of the vector $\\mathbf{b}$ is $b_1 = h^3 - 1$.\n\nFor the last equation ($i=N$):\n$$u_{N-1} + (-2 - h^2)u_N + u_{N+1} = N h^3$$\nSince $u_{N+1} = e-1$ is known, we move it to the right-hand side:\n$$u_{N-1} + (-2 - h^2)u_N = Nh^3 - u_{N+1} = Nh^3 - (e-1)$$\nThus, the last element of the vector $\\mathbf{b}$ is $b_N = Nh^3 - e + 1$.\n\nFor the general interior equations ($i=2, \\dots, N-1$):\n$$u_{i-1} + (-2-h^2)u_i + u_{i+1} = ih^3$$\nThe corresponding elements of $\\mathbf{b}$ are $b_i = ih^3$.\n\nIn summary, the right-hand side vector $\\mathbf{b} = [b_1, b_2, \\dots, b_N]^T$ is:\n$$b_i = \\begin{cases}\nh^3 - 1 & \\text{if } i=1 \\\\\nih^3 & \\text{if } 2 \\le i \\le N-1 \\\\\nNh^3 - (e-1) & \\text{if } i=N\n\\end{cases}$$\nThe case $N=1$ is handled correctly by this construction, as the conditions for $i=1$ and $i=N$ would both apply to $b_1$, resulting in $b_1 = (h^3-1) - (e-1) = h^3 - e$, which is correct for a single equation $u_0 + (-2-h^2)u_1 + u_2 = h^3 \\implies (-2-h^2)u_1 = h^3 - u_0 - u_2$.\n\n### Part 3: Implementation Algorithm\n\nThe implementation will perform the following steps for each value of $N$ in the test suite $\\{1, 10, 20, 40\\}$:\n1.  **Initialize Parameters**: Calculate the step size $h = 1/(N+1)$ and the value of $e$.\n2.  **Construct Linear System**:\n    -   Create the tridiagonal matrix $A$ in a banded format suitable for efficient solvers (e.g., `scipy.linalg.solve_banded`). This requires creating a $3 \\times N$ array where rows represent the super-diagonal, main diagonal, and sub-diagonal.\n    -   Construct the right-hand side vector $\\mathbf{b}$ of size $N$ according to the formulas derived in Part 2.\n3.  **Solve System**: Solve the linear system $A\\mathbf{u} = \\mathbf{b}$ for the vector of interior solutions $\\mathbf{u} = [u_1, \\dots, u_N]^T$.\n4.  **Reconstruct Full Solution**: Assemble the complete numerical solution vector, including the boundary values: $U_{num} = [u_0, u_1, \\dots, u_N, u_{N+1}] = [1, \\mathbf{u}^T, e-1]^T$.\n5.  **Compute Exact Solution**: Create the vector of grid points $X = [x_0, x_1, \\dots, x_{N+1}]$ where $x_i = ih$. Evaluate the exact solution $u(x) = e^x - x$ at each grid point to get the vector $U_{exact}$.\n6.  **Calculate Error**: Compute the maximum absolute error between the numerical and exact solutions over all grid points: $E_{max} = \\max_{i \\in \\{0, \\dots, N+1\\}} |U_{num, i} - U_{exact, i}|$.\n7.  **Store and Format Result**: Round the computed maximum error to eight decimal places and store it. After processing all values of $N$, format the collected results into the specified string format.\n\nThis procedure will be encapsulated in a Python program using the `numpy` and `scipy` libraries as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Solves the BVP u''(x) - u(x) = x on [0,1] with u(0)=1, u(1)=e-1\n    using a finite difference scheme for a given set of N values.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [1, 10, 20, 40]\n\n    results = []\n    for N in test_cases:\n        # Step 1: Initialize Parameters\n        # N: number of interior points\n        # There are N+2 total points (including boundaries)\n        h = 1.0 / (N + 1)\n        \n        # Exact boundary values\n        u0 = 1.0\n        uN_plus_1 = np.e - 1.0\n\n        # Step 2: Construct the tridiagonal linear system A*u = b\n        \n        # Construct the tridiagonal matrix A in banded format for SciPy.\n        # The format is a (3, N) array:\n        # Row 0: Super-diagonal (padded with a 0 at the start)\n        # Row 1: Main diagonal\n        # Row 2: Sub-diagonal (padded with a 0 at the end)\n        ab = np.zeros((3, N))\n        ab[0, 1:] = 1.0  # Super-diagonal\n        ab[1, :] = -2.0 - h**2  # Main diagonal\n        ab[2, :-1] = 1.0  # Sub-diagonal\n\n        # Construct the right-hand side vector b\n        # Start with the base value b_i = i * h^3\n        b = np.array([(i + 1) * h**3 for i in range(N)])\n        \n        # Incorporate boundary conditions\n        b[0] -= u0\n        b[-1] -= uN_plus_1\n        \n        # Step 3: Solve the system for the interior points u\n        u_interior = solve_banded((1, 1), ab, b)\n        \n        # Step 4: Reconstruct the full numerical solution grid function\n        u_numerical = np.concatenate(([u0], u_interior, [uN_plus_1]))\n\n        # Step 5: Compute the exact solution at grid points\n        # Grid points x_i = i*h for i = 0, ..., N+1\n        x = np.linspace(0, 1, N + 2)\n        u_exact = np.exp(x) - x\n\n        # Step 6: Calculate the maximum absolute error\n        # The error at the boundaries is zero by construction (up to float precision)\n        max_error = np.max(np.abs(u_numerical - u_exact))\n        \n        # Add the rounded result to the list\n        results.append(max_error)\n\n    # Final print statement in the exact required format.\n    # Each result is formatted to eight decimal places.\n    formatted_results = [f\"{res:.8f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many physical models require specifying derivatives at the boundary, such as heat flux in thermodynamics or stress in mechanics. This exercise tackles this common scenario by introducing the \"ghost point\" method to handle Neumann boundary conditions. By applying a clever combination of a centered difference approximation and the governing differential equation at the boundary, you will learn how to preserve the second-order accuracy of your solver, making it both more versatile and robust .",
            "id": "3211343",
            "problem": "You are tasked with designing and implementing a second-order finite difference solver for a one-dimensional boundary value problem for an ordinary differential equation with a Neumann boundary condition. The mathematical model is the linear Poisson equation\n$$\nu''(x) = f(x), \\quad x \\in [0,1],\n$$\nwith a Neumann boundary condition at the left endpoint and a Dirichlet boundary condition at the right endpoint:\n$$\nu'(0) = \\alpha, \\quad u(1) = \\beta.\n$$\nYou must use a uniform grid with $m$ equal subdivisions of the interval $[0,1]$, with grid points $x_i = i h$ where $h = 1/m$ and $i = 0,1,\\dots,m$. Your numerical method must:\n- Use the standard second-order centered difference approximation of the second derivative at interior nodes.\n- Implement the Neumann condition at $x=0$ using a \"ghost point\" so that the derivative at the boundary is approximated by a centered difference, and eliminate the ghost point by invoking the ordinary differential equation $u''(x)=f(x)$ and appropriate Taylor expansions to preserve second-order accuracy. The design must start from fundamental definitions (grid, Taylor expansions, and the ordinary differential equation) and avoid ad hoc formulas.\n- Incorporate the Dirichlet boundary condition at $x=1$ in a way that retains second-order accuracy at the last interior node.\n\nYour program must solve the discrete linear system for the interior unknowns $u(x_i)$ at $i=1,2,\\dots,m-1$, and then evaluate the maximum absolute error relative to an exact analytical solution over the interior grid points $\\{x_i\\}_{i=1}^{m-1}$.\n\nFor validation, derive exact analytical solutions by integrating $u''(x)=f(x)$ twice and using the boundary conditions to determine integration constants. For each test case below, the program must compute:\n- The numerical solution at interior grid points using your second-order scheme with the ghost point treatment at $x=0$.\n- The exact solution at the same interior grid points.\n- The maximum absolute error (the infinity norm of the pointwise error vector) over interior points, expressed as a real number.\n\nAdditionally, for a convergence assessment test, compute the observed order of accuracy $p$ defined by\n$$\np = \\log_2\\left(\\frac{E_{h}}{E_{h/2}}\\right),\n$$\nwhere $E_{h}$ and $E_{h/2}$ are the maximum absolute errors for grids with $m$ and $2m$ subdivisions, respectively.\n\nUse the following test suite:\n1. Happy path: $f(x) = 2$, $\\alpha = 1$, $\\beta = 0$, $m = 50$. The exact solution is found by solving $u''(x)=2$ with the given boundary conditions.\n2. Boundary-condition-dominant coarse grid: $f(x) = 0$, $\\alpha = 0$, $\\beta = 1$, $m = 4$. The exact solution is constant.\n3. Oscillatory forcing: $f(x) = -\\pi^2 \\sin(\\pi x)$, $\\alpha = 0$, $\\beta = 0$, $m = 64$. The exact solution is obtained by solving $u''(x) = -\\pi^2 \\sin(\\pi x)$ with the given boundary conditions.\n4. Convergence order: $f(x) = x$, $\\alpha = 0$, $\\beta = 0$ with two grids $m=20$ and $m=40$. Compute the observed order $p$ as defined above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above. The first three entries must be real numbers representing the maximum absolute error for each case, and the last entry must be a real number representing the observed order for the convergence test. For example, the output should look like $[e_1,e_2,e_3,p]$, where each $e_i$ and $p$ are real numbers.",
            "solution": "The user-provided problem is valid as it is scientifically grounded in the theory of numerical methods for ordinary differential equations, is mathematically well-posed, and is defined with objective, formal criteria. This document provides a complete derivation of the numerical method and the analytical solutions for validation.\n\nThe problem is to solve the one-dimensional Poisson equation, a linear second-order ordinary differential equation (ODE), on the interval $x \\in [0,1]$:\n$$\nu''(x) = f(x)\n$$\nwith a Neumann boundary condition at $x=0$ and a Dirichlet boundary condition at $x=1$:\n$$\nu'(0) = \\alpha, \\quad u(1) = \\beta.\n$$\n\nWe will construct a second-order finite difference scheme to approximate the solution.\n\n**1. Discretization of the Domain**\n\nThe domain $[0,1]$ is discretized into a uniform grid with $m$ subdivisions. The grid spacing is $h = 1/m$. The grid points are defined as $x_i = i h$ for $i = 0, 1, \\dots, m$. Let $U_i$ be the numerical approximation of the exact solution $u(x_i)$ at grid point $x_i$. The boundary condition at $x=1$ gives $U_m = u(1) = \\beta$. The values to be determined are $U_0, U_1, \\dots, U_{m-1}$.\n\n**2. Finite Difference Approximation for Interior Points**\n\nFor any interior grid point $x_i$ where $i = 1, 2, \\dots, m-1$, we approximate the second derivative $u''(x_i)$ using the second-order centered difference formula. This is derived from Taylor series expansions of $u(x_{i+1})$ and $u(x_{i-1})$ around $x_i$:\n$$\nu(x_{i+1}) = u(x_i+h) = u(x_i) + h u'(x_i) + \\frac{h^2}{2} u''(x_i) + \\frac{h^3}{6} u'''(x_i) + O(h^4)\n$$\n$$\nu(x_{i-1}) = u(x_i-h) = u(x_i) - h u'(x_i) + \\frac{h^2}{2} u''(x_i) - \\frac{h^3}{6} u'''(x_i) + O(h^4)\n$$\nAdding these two expansions and rearranging for $u''(x_i)$ yields:\n$$\nu''(x_i) = \\frac{u(x_{i+1}) - 2u(x_i) + u(x_{i-1})}{h^2} - \\frac{h^2}{12} u^{(4)}(x_i) + \\dots\n$$\nThe approximation is second-order accurate, with a local truncation error of $O(h^2)$. Replacing $u(x_i)$ with $U_i$ and $u''(x_i)$ with $f(x_i)$ from the ODE, we obtain the discrete equations for the interior nodes:\n$$\n\\frac{U_{i-1} - 2U_i + U_{i+1}}{h^2} = f(x_i), \\quad \\text{for } i = 1, 2, \\dots, m-1.\n$$\n\n**3. Discretization of the Neumann Boundary Condition at $x=0$**\n\nThe problem requires using a ghost point to maintain second-order accuracy. We introduce a ghost point at $x_{-1} = -h$.\nFirst, we approximate the derivative $u'(0) = \\alpha$ using a second-order centered difference at $x_0 = 0$:\n$$\nu'(0) = \\frac{u(x_1) - u(x_{-1})}{2h} + O(h^2) \\implies \\frac{U_1 - U_{-1}}{2h} = \\alpha\n$$\nThis gives an expression for the ghost value $U_{-1}$:\n$$\nU_{-1} = U_1 - 2h\\alpha\n$$\nNext, we assume the ODE $u''(x) = f(x)$ also holds at the boundary point $x_0=0$. We apply the centered difference formula for $u''(0)$:\n$$\n\\frac{U_{-1} - 2U_0 + U_1}{h^2} = f(x_0)\n$$\nNow, we eliminate the ghost point $U_{-1}$ by substituting its expression into this equation:\n$$\n\\frac{(U_1 - 2h\\alpha) - 2U_0 + U_1}{h^2} = f(x_0)\n$$\nSimplifying this expression gives the equation for the node $i=0$:\n$$\n\\frac{-2U_0 + 2U_1 - 2h\\alpha}{h^2} = f(x_0) \\implies -2U_0 + 2U_1 = h^2 f(x_0) + 2h\\alpha\n$$\nThe local truncation error of this boundary scheme is $O(h)$, which is sufficient to ensure the global error of the overall method is $O(h^2)$.\n\n**4. Assembly of the Linear System**\n\nWe have a system of $m$ linear equations for the $m$ unknowns $U_0, U_1, \\dots, U_{m-1}$. Let $\\mathbf{U} = [U_0, U_1, \\dots, U_{m-1}]^T$ be the vector of unknowns. The system can be written as $A \\mathbf{U} = \\mathbf{b}$.\n\nThe equations are:\n- For $i=0$: $-2U_0 + 2U_1 = h^2 f(x_0) + 2h\\alpha$\n- For $i=1, \\dots, m-2$: $U_{i-1} - 2U_i + U_{i+1} = h^2 f(x_i)$\n- For $i=m-1$: $U_{m-2} - 2U_{m-1} + U_m = h^2 f(x_{m-1})$. Since $U_m = \\beta$, this becomes $U_{m-2} - 2U_{m-1} = h^2 f(x_{m-1}) - \\beta$.\n\nThe $m \\times m$ coefficient matrix $A$ is:\n$$\nA = \\begin{pmatrix}\n-2 & 2 & 0 & \\cdots & \\cdots & 0 \\\\\n1 & -2 & 1 & \\ddots & & \\vdots \\\\\n0 & 1 & -2 & 1 & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\ddots & 0 \\\\\n\\vdots & & \\ddots & 1 & -2 & 1 \\\\\n0 & \\cdots & \\cdots & 0 & 1 & -2\n\\end{pmatrix}\n$$\nThe right-hand side vector $\\mathbf{b}$ of size $m$ is:\n$$\n\\mathbf{b} = \\begin{pmatrix}\nh^2 f(x_0) + 2h\\alpha \\\\\nh^2 f(x_1) \\\\\n\\vdots \\\\\nh^2 f(x_{m-2}) \\\\\nh^2 f(x_{m-1}) - \\beta\n\\end{pmatrix}\n$$\nThis linear system $A \\mathbf{U} = \\mathbf{b}$ is non-singular and can be solved to find the numerical solution $\\mathbf{U}$.\n\n**5. Error Calculation and Convergence Order**\n\nThe maximum absolute error $E_h$ is computed over the interior grid points $\\{x_i\\}_{i=1}^{m-1}$ as the infinity norm of the error vector:\n$$\nE_h = \\max_{i=1,\\dots,m-1} |U_i - u(x_i)|\n$$\nFor a second-order accurate method, the error is expected to behave as $E_h \\approx C h^2$ for some constant $C$. The order of accuracy $p$ can be observed numerically by comparing errors from two different grid sizes, $h$ and $h/2$.\n$$\nE_h \\approx C h^p \\quad \\text{and} \\quad E_{h/2} \\approx C (h/2)^p\n$$\nTaking the ratio gives $\\frac{E_h}{E_{h/2}} \\approx 2^p$. Solving for $p$ yields:\n$$\np = \\log_2\\left(\\frac{E_{h}}{E_{h/2}}\\right)\n$$\n\n**6. Analytical Solutions for Test Cases**\n\nThe exact solution $u(x)$ is found by integrating $u''(x) = f(x)$ twice and applying the boundary conditions $u'(0) = \\alpha$ and $u(1) = \\beta$ to determine the constants of integration.\n\n- **Case 1:** $f(x) = 2$, $\\alpha = 1$, $\\beta = 0$.\n$u'' = 2 \\implies u'(x) = 2x+C_1$. $u'(0)=C_1=1$. So $u'(x)=2x+1$.\n$u(x) = x^2+x+C_2$. $u(1)=1+1+C_2=0 \\implies C_2=-2$.\n$u(x) = x^2+x-2$.\n\n- **Case 2:** $f(x) = 0$, $\\alpha = 0$, $\\beta = 1$.\n$u'' = 0 \\implies u'(x) = C_1$. $u'(0)=C_1=0$. So $u'(x)=0$.\n$u(x) = C_2$. $u(1)=C_2=1$.\n$u(x) = 1$.\n\n- **Case 3:** $f(x) = -\\pi^2 \\sin(\\pi x)$, $\\alpha = 0$, $\\beta = 0$.\n$u''(x) = -\\pi^2 \\sin(\\pi x) \\implies u'(x) = \\pi \\cos(\\pi x)+C_1$. $u'(0)=\\pi+C_1=0 \\implies C_1=-\\pi$.\n$u'(x) = \\pi \\cos(\\pi x) - \\pi$.\n$u(x) = \\sin(\\pi x) - \\pi x + C_2$. $u(1)=\\sin(\\pi)-\\pi+C_2=0 \\implies C_2=\\pi$.\n$u(x) = \\sin(\\pi x) - \\pi x + \\pi$.\n\n- **Case 4:** $f(x) = x$, $\\alpha = 0$, $\\beta = 0$.\n$u''(x) = x \\implies u'(x) = x^2/2+C_1$. $u'(0)=C_1=0$. So $u'(x)=x^2/2$.\n$u(x) = x^3/6+C_2$. $u(1)=1/6+C_2=0 \\implies C_2=-1/6$.\n$u(x) = \\frac{x^3}{6} - \\frac{1}{6}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the finite difference solver.\n    \"\"\"\n\n    def run_solver(f, alpha, beta, m):\n        \"\"\"\n        Solves u''(x) = f(x) with u'(0)=alpha, u(1)=beta on a grid with m subdivisions.\n        \n        Args:\n            f (callable): The forcing function f(x).\n            alpha (float): The Neumann boundary condition value at x=0.\n            beta (float): The Dirichlet boundary condition value at x=1.\n            m (int): The number of grid subdivisions.\n            \n        Returns:\n            tuple: A tuple containing:\n                - np.ndarray: Grid points for the interior, x_1, ..., x_{m-1}.\n                - np.ndarray: Numerical solution at the interior points, U_1, ..., U_{m-1}.\n        \"\"\"\n        # Grid setup\n        h = 1.0 / m\n        # We solve for unknowns at x_0, x_1, ..., x_{m-1}\n        x_nodes = np.linspace(0, 1, m + 1)\n        \n        # Assemble the m x m matrix A\n        A = np.zeros((m, m))\n        \n        # Fill diagonals for the standard interior stencil (1, -2, 1)\n        np.fill_diagonal(A, -2.0)\n        # Lower diagonal (k=-1): A[i, i-1] = 1 for i=1,...,m-1\n        np.fill_diagonal(A[1:], 1.0)\n        # Upper diagonal (k=1): A[i, i+1] = 1 for i=0,...,m-2\n        np.fill_diagonal(A[:, 1:], 1.0)\n        \n        # Modify the first row for the Neumann condition: -2*U_0 + 2*U_1 = ...\n        A[0, 1] = 2.0\n        \n        # Assemble the right-hand side vector b of size m\n        # We evaluate f at x_0, ..., x_{m-1}\n        b = h**2 * f(x_nodes[0:m])\n        \n        # Modify first and last elements of b for boundary conditions\n        # b_0 = h^2*f(x_0) + 2*h*alpha\n        b[0] += 2.0 * h * alpha\n        # b_{m-1} = h^2*f(x_{m-1}) - beta\n        b[m-1] -= beta\n        \n        # Solve the linear system A*U = b for U = [U_0, ..., U_{m-1}]\n        U = np.linalg.solve(A, b)\n        \n        # Return interior grid points and corresponding solution values\n        return x_nodes[1:m], U[1:]\n\n    results = []\n\n    # Test Case 1: Happy path\n    f1 = lambda x: 2.0 * np.ones_like(x)\n    u_exact1 = lambda x: x**2 + x - 2.0\n    alpha1, beta1, m1 = 1.0, 0.0, 50\n    x_interior1, U_interior1 = run_solver(f1, alpha1, beta1, m1)\n    u_exact_vals1 = u_exact1(x_interior1)\n    error1 = np.max(np.abs(U_interior1 - u_exact_vals1))\n    results.append(error1)\n\n    # Test Case 2: Boundary-condition-dominant coarse grid\n    f2 = lambda x: np.zeros_like(x)\n    u_exact2 = lambda x: np.ones_like(x)\n    alpha2, beta2, m2 = 0.0, 1.0, 4\n    x_interior2, U_interior2 = run_solver(f2, alpha2, beta2, m2)\n    u_exact_vals2 = u_exact2(x_interior2)\n    error2 = np.max(np.abs(U_interior2 - u_exact_vals2))\n    results.append(error2)\n\n    # Test Case 3: Oscillatory forcing\n    f3 = lambda x: -np.pi**2 * np.sin(np.pi * x)\n    u_exact3 = lambda x: np.sin(np.pi * x) - np.pi * x + np.pi\n    alpha3, beta3, m3 = 0.0, 0.0, 64\n    x_interior3, U_interior3 = run_solver(f3, alpha3, beta3, m3)\n    u_exact_vals3 = u_exact3(x_interior3)\n    error3 = np.max(np.abs(U_interior3 - u_exact_vals3))\n    results.append(error3)\n\n    # Test Case 4: Convergence order\n    f4 = lambda x: x\n    u_exact4 = lambda x: x**3 / 6.0 - 1.0 / 6.0\n    alpha4, beta4 = 0.0, 0.0\n    \n    # Solve on grid with m=20\n    m_h = 20\n    x_h, U_h = run_solver(f4, alpha4, beta4, m_h)\n    u_h = u_exact4(x_h)\n    E_h = np.max(np.abs(U_h - u_h))\n    \n    # Solve on grid with m=40\n    m_h2 = 40\n    x_h2, U_h2 = run_solver(f4, alpha4, beta4, m_h2)\n    u_h2 = u_exact4(x_h2)\n    E_h2 = np.max(np.abs(U_h2 - u_h2))\n    \n    order_p = np.log2(E_h / E_h2)\n    results.append(order_p)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond implementing a single method, an expert needs to choose the *right* method for the job. This exercise moves from implementation to a higher-level analysis, comparing the finite difference (or \"relaxation\") method you've practiced with an entirely different strategy: the \"shooting method.\" You will analyze the trade-offs in computational cost and robustness between these two major approaches, particularly in the context of challenging nonlinear problems, which are common in science and engineering .",
            "id": "2377667",
            "problem": "You are tasked with solving a nonlinear boundary value problem (BVP) for an ordinary differential equation (ODE) of the form\n$$\ny''(x) + q(x)\\,y'(x) + g\\big(x, y(x)\\big) = r(x), \\quad x \\in [0,1],\n$$\nwith boundary conditions\n$$\ny(0) = \\alpha,\\qquad y(1) = \\beta,\n$$\nwhere $q$, $g$, and $r$ are smooth functions, $g$ is nonlinear in $y$, and $\\alpha$ and $\\beta$ are prescribed constants. You plan to compare two standard numerical strategies at comparable accuracy: the shooting method with Newton updates on the unknown initial slope and the relaxation method based on a finite-difference discretization and Newton linearization. Assume both strategies target second-order accuracy on a uniform mesh with $N+1$ nodes and step size $h = 1/N$.\n\nFor the shooting method, assume a Runge–Kutta (RK) integrator of order $4$ with adaptive step control switched off (fixed $h$) is used to integrate the initial value problem that results from guessing the unknown initial slope $s = y'(0)$. To perform Newton updates of $s$, assume one integrates, per Newton iteration, both the state ODE and its variational (sensitivity) ODE with respect to $s$ to obtain the derivative of the terminal residual with respect to $s$.\n\nFor the relaxation method, use second-order centered finite differences to form a nonlinear system for the interior unknowns, and solve it by Newton’s method. At each Newton step, solve the linearized system exactly using a direct banded solver appropriate for the stencil structure.\n\nSuppose the coefficients are such that the initial value problem is stable for forward integration when $q$ and $g$ are moderate, but can become unstable or stiff for certain parameter regimes (for example, when the linearization about a solution has a positive growth rate). Based only on first principles of how many steps and operations each method performs per iteration as a function of $N$, and on the structural properties of the discrete systems they generate, which statement best characterizes the typical computational expense and robustness trade-offs between the two methods?\n\nA. For comparable second-order accuracy, the per-Newton-iteration cost of both shooting (integrating the state and its sensitivity once across the domain with step size $h = 1/N$) and relaxation (assembling and solving a tridiagonal linear system from a second-order stencil) scales like $O(N)$. In practice, shooting often has a smaller constant factor when the initial value problem is stable and mildly nonlinear, but for unstable or stiff regimes the exponential sensitivity to the initial slope can make iteration counts and effective work grow dramatically, whereas relaxation typically remains robust with $O(N)$ work per Newton step due to its sparse global solve.\n\nB. Shooting necessarily requires $O(N)$ distinct initial-slope guesses even with Newton updates, giving $O(N^2)$ cost, while relaxation leverages a fast Fourier transform to achieve $O(N\\log N)$ per iteration regardless of boundary conditions.\n\nC. Because only one scalar, the initial slope, is unknown in shooting, its overall computational cost is $O(1)$ with respect to $N$, whereas relaxation must solve a dense $N\\times N$ system costing $O(N^3)$ per Newton iteration.\n\nD. Relaxation is always asymptotically slower because assembling its Jacobian costs $O(N^2)$ even for second-order stencils, while shooting attains $O(\\log N)$ complexity by bisection on the initial slope.",
            "solution": "The problem asks for a comparison of the computational expense and robustness of two standard numerical methods for solving a nonlinear two-point boundary value problem (BVP): the shooting method and the relaxation method. I will analyze each method from first principles.\n\nThe BVP is given by:\n$$\ny''(x) + q(x)\\,y'(x) + g\\big(x, y(x)\\big) = r(x), \\quad x \\in [0,1]\n$$\nwith boundary conditions $y(0) = \\alpha$ and $y(1) = \\beta$. The problem states that a uniform mesh with $N+1$ nodes and step size $h = 1/N$ is used.\n\n**Analysis of the Shooting Method**\n\nThe shooting method transforms the BVP into an initial value problem (IVP) by guessing the missing initial condition, $y'(0) = s$. The resulting IVP is:\n$$\ny''(x) = -q(x)y'(x) - g(x, y) + r(x), \\quad y(0) = \\alpha, \\quad y'(0) = s.\n$$\nThis second-order ODE is typically solved by converting it to a system of two first-order ODEs. Let $\\mathbf{u}(x) = [u_1(x), u_2(x)]^T = [y(x), y'(x)]^T$. The system is:\n$$\n\\mathbf{u}'(x) = \\begin{pmatrix} u_2(x) \\\\ -q(x)u_2(x) - g(x, u_1(x)) + r(x) \\end{pmatrix}, \\quad \\mathbf{u}(0) = \\begin{pmatrix} \\alpha \\\\ s \\end{pmatrix}.\n$$\nWe \"shoot\" from $x=0$ to $x=1$ by integrating this IVP. Let the solution at $x=1$ be denoted $y(1; s)$. The goal is to find the value of $s$ that satisfies the second boundary condition, $y(1;s) = \\beta$. This is a root-finding problem for the scalar function:\n$$\nF(s) = y(1; s) - \\beta = 0.\n$$\nThe problem specifies using Newton's method to solve for $s$:\n$$\ns_{k+1} = s_k - \\frac{F(s_k)}{F'(s_k)}.\n$$\nThe derivative $F'(s) = \\frac{d}{ds}y(1;s)$ is found by solving the variational (or sensitivity) equation. Let $\\xi(x) = \\frac{\\partial y(x;s)}{\\partial s}$. Differentiating the original ODE with respect to $s$ yields a linear second-order ODE for $\\xi(x)$:\n$$\n\\xi''(x) + q(x)\\xi'(x) + \\frac{\\partial g}{\\partial y}\\big(x, y(x;s)\\big) \\xi(x) = 0,\n$$\nwith initial conditions $\\xi(0) = \\frac{\\partial y(0)}{\\partial s} = 0$ and $\\xi'(0) = \\frac{\\partial y'(0)}{\\partial s} = \\frac{\\partial s}{\\partial s} = 1$. This can also be written as a system of two first-order ODEs.\n\n**Computational Cost per Newton Iteration (Shooting):**\nOne Newton iteration requires evaluating $F(s_k)$ and $F'(s_k)$. This is done by integrating the original ODE system (for $y$) and the variational ODE system (for $\\xi$) simultaneously from $x=0$ to $x=1$. A fourth-order Runge-Kutta method with a fixed step size $h=1/N$ requires $N$ steps. The work per step is constant (a fixed number of evaluations of the right-hand side of a $4 \\times 4$ system of ODEs). Therefore, the total work for one integration across the domain is proportional to the number of steps, $N$. The computational cost per Newton iteration for the shooting method is $O(N)$.\n\n**Analysis of the Relaxation Method**\n\nThe relaxation method discretizes the entire domain at once. Using a second-order centered finite difference approximation on the mesh $x_i = ih$ for $i=0, \\dots, N$:\n$$\ny'(x_i) \\approx \\frac{y_{i+1} - y_{i-1}}{2h}, \\qquad y''(x_i) \\approx \\frac{y_{i+1} - 2y_i + y_{i-1}}{h^2}.\n$$\nSubstituting these into the ODE for each interior node $i=1, \\dots, N-1$ yields a system of $N-1$ nonlinear algebraic equations for the $N-1$ unknown values $y_1, \\dots, y_{N-1}$ (since $y_0 = \\alpha$ and $y_N = \\beta$ are known):\n$$\nF_i(\\mathbf{y}) \\equiv \\frac{y_{i+1} - 2y_i + y_{i-1}}{h^2} + q(x_i)\\frac{y_{i+1} - y_{i-1}}{2h} + g(x_i, y_i) - r(x_i) = 0.\n$$\nThis nonlinear system $\\mathbf{F}(\\mathbf{y}) = \\mathbf{0}$ is solved using Newton's method. At each iteration $k$, we solve the linear system for the update $\\Delta \\mathbf{y}_k$:\n$$\nJ(\\mathbf{y}_k) \\Delta \\mathbf{y}_k = -\\mathbf{F}(\\mathbf{y}_k),\n$$\nwhere $J(\\mathbf{y}_k)$ is the Jacobian matrix with entries $J_{ij} = \\frac{\\partial F_i}{\\partial y_j}$. Because the finite difference approximation for $F_i$ involves only $y_{i-1}$, $y_i$, and $y_{i+1}$, the Jacobian matrix is tridiagonal.\n\n**Computational Cost per Newton Iteration (Relaxation):**\nOne Newton iteration involves:\n1.  Assembling the vector $\\mathbf{F}(\\mathbf{y}_k)$: This requires $O(N)$ operations since each of the $N-1$ components is a simple calculation.\n2.  Assembling the tridiagonal Jacobian $J(\\mathbf{y}_k)$: This requires computing derivatives for the main diagonal and the two off-diagonals, which is also an $O(N)$ operation.\n3.  Solving the linear system $J\\Delta\\mathbf{y} = -\\mathbf{F}$: A tridiagonal system of size $(N-1) \\times (N-1)$ can be solved using the Thomas algorithm (a form of Gaussian elimination for banded matrices) in $O(N)$ operations.\n\nThus, the total computational cost per Newton iteration for the relaxation method is $O(N)$.\n\n**Comparison of Robustness**\n\n*   **Shooting Method:** The method's stability is tied to the stability of the IVP it solves. If the homogeneous part of the ODE exhibits solutions with exponential growth (i.e., the problem is unstable), small errors in the initial guess $s$ will be amplified exponentially across the integration interval $[0, 1]$. This makes $F(s)$ extremely steep, and Newton's method can fail to converge unless the initial guess is exceptionally accurate. This is a critical weakness for unstable or stiff problems.\n*   **Relaxation Method:** This method solves for all points simultaneously. The global coupling imposed by the matrix equation implicitly enforces both boundary conditions at once. This makes the method far more robust for unstable problems, as the exponential growth modes are suppressed by the overall structure of the problem enforced by the linear algebra solve. Convergence of Newton's method is typically more reliable and less sensitive to the stability characteristics of the underlying IVP.\n\n**Evaluation of Options**\n\n*   **A. For comparable second-order accuracy, the per-Newton-iteration cost of both shooting (integrating the state and its sensitivity once across the domain with step size $h = 1/N$) and relaxation (assembling and solving a tridiagonal linear system from a second-order stencil) scales like $O(N)$. In practice, shooting often has a smaller constant factor when the initial value problem is stable and mildly nonlinear, but for unstable or stiff regimes the exponential sensitivity to the initial slope can make iteration counts and effective work grow dramatically, whereas relaxation typically remains robust with $O(N)$ work per Newton step due to its sparse global solve.**\n    This statement is fully consistent with my analysis. The complexity per iteration for both methods is $O(N)$. The practical observation about the constant factor in stable regimes is correct, as is the crucial point about the superior robustness of relaxation in unstable regimes. The reasoning provided is sound.\n    Verdict: **Correct**.\n\n*   **B. Shooting necessarily requires $O(N)$ distinct initial-slope guesses even with Newton updates, giving $O(N^2)$ cost, while relaxation leverages a fast Fourier transform to achieve $O(N\\log N)$ per iteration regardless of boundary conditions.**\n    This is incorrect. Newton's method typically converges in a small number of iterations, independent of $N$, if the initial guess is good. The total cost of shooting is therefore $O(N)$, not $O(N^2)$. The claim about relaxation using FFT is also false; FFT solvers are for linear, constant-coefficient problems, not for the general nonlinear problem described, which is solved here with a tridiagonal solver costing $O(N)$.\n    Verdict: **Incorrect**.\n\n*   **C. Because only one scalar, the initial slope, is unknown in shooting, its overall computational cost is $O(1)$ with respect to $N$, whereas relaxation must solve a dense $N\\times N$ system costing $O(N^3)$ per Newton iteration.**\n    This is incorrect on both counts. The cost of shooting is $O(N)$ because each function evaluation requires integrating over $N$ steps. The relaxation matrix is tridiagonal, not dense, and solving the corresponding linear system costs $O(N)$, not $O(N^3)$.\n    Verdict: **Incorrect**.\n\n*   **D. Relaxation is always asymptotically slower because assembling its Jacobian costs $O(N^2)$ even for second-order stencils, while shooting attains $O(\\log N)$ complexity by bisection on the initial slope.**\n    This is incorrect. Assembling the tridiagonal Jacobian for relaxation costs $O(N)$, not $O(N^2)$. The complexity of shooting is $O(N)$, not $O(\\log N)$, because the cost is dominated by the numerical integration over $N$ steps, not the root-finding algorithm itself.\n    Verdict: **Incorrect**.\n\nBased on a rigorous analysis of the algorithms, option A provides the only accurate characterization of the trade-offs.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}