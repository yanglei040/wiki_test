{
    "hands_on_practices": [
        {
            "introduction": "Root-finding methods are not just for abstract equations; they are powerful tools for solving tangible problems in geometry and optimization. This practice illustrates how to reframe an optimization problem—finding the minimum distance between a point and a curve—as a root-finding problem. By setting the derivative of the squared distance function to zero, we transform the task into solving an equation of the form $f'(x)=0$, a perfect application for open methods like Newton's or the secant method .",
            "id": "3259979",
            "problem": "You are to write a complete, runnable program that computes the closest point on the parabola $y = x^2$ to a fixed point $P = (2, -\\frac{1}{2})$ by posing the task as a one-dimensional root-finding problem and solving it with open methods. The fundamental base for the derivation is the Euclidean distance in the plane and the calculus fact that for a differentiable function, a local minimum occurs at a point where the derivative is zero, provided appropriate second-order conditions hold.\n\nConsider the squared Euclidean distance from a point on the curve to the fixed point. If a point on the curve has coordinates $(x, x^2)$, the squared distance to $P$ is $D(x) = (x - 2)^2 + (x^2 + \\frac{1}{2})^2$. To find the closest point, minimize $D(x)$ by characterizing stationary points using the first derivative test. The minimizer must satisfy $\\frac{d}{dx}D(x) = 0$. This condition defines a scalar nonlinear equation in $x$ whose unique solution corresponds to the $x$-coordinate of the closest point on the parabola to $P$. Because we want to use open methods, you must solve this equation using both Newton's method and the secant method. You must use the analytical derivative for Newton's method and purely function evaluations for the secant method.\n\nYour program must implement the following components:\n- A function $f(x)$ equal to the first derivative $\\frac{d}{dx}D(x)$.\n- A function $f'(x)$ equal to the derivative of $f(x)$, used by Newton's method.\n- An implementation of Newton's method that iterates $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$ until convergence.\n- An implementation of the secant method that iterates $x_{k+1} = x_k - f(x_k)\\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}$ until convergence.\n- A stopping criterion that terminates when either $|x_{k+1} - x_k|  \\varepsilon$ or $|f(x_{k+1})|  \\varepsilon$, or when the iteration count reaches a specified maximum without convergence.\n\nFor each converged root $x^\\star$, report the closest point $(x^\\star, (x^\\star)^2)$ and the Euclidean distance to $P$, namely $\\sqrt{(x^\\star - 2)^2 + ((x^\\star)^2 + \\frac{1}{2})^2}$. No physical units apply here; all quantities are dimensionless.\n\nUse the following test suite of parameter values to exercise your implementation. Each test case specifies the method and its initial guess parameters, along with the absolute tolerance and maximum iteration count:\n- Test $1$: Newton's method with initial guess $x_0 = 0$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $50$.\n- Test $2$: Newton's method with initial guess $x_0 = 2$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $50$.\n- Test $3$: Secant method with initial guesses $x_0 = 0$, $x_1 = 1$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $100$.\n- Test $4$: Secant method with initial guesses $x_0 = -1$, $x_1 = 1$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $100$.\n- Test $5$: Newton's method with initial guess $x_0 = -5$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $50$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing a list of results for all test cases in order. Each result must be the list $[x^\\star, (x^\\star)^2, \\text{dist}]$, where $x^\\star$ is the approximated minimizer, $(x^\\star)^2$ is the corresponding $y$-coordinate, and $\\text{dist}$ is the Euclidean distance to $P$. Each number must be rounded to exactly $12$ decimal places.\n- The final output must be a single line, formatted as a comma-separated list of the five triples enclosed in square brackets, for example, $[[\\dots],[\\dots],[\\dots],[\\dots],[\\dots]]$.",
            "solution": "The problem is to find the point $(x, y)$ on the parabola $y = x^2$ that is closest to the fixed point $P = (2, -1/2)$. This is an optimization problem that can be transformed into a root-finding problem.\n\n### Principle and Derivation\n\nThe Euclidean distance between a point $(x, x^2)$ on the parabola and the point $P$ is given by $d(x) = \\sqrt{(x - 2)^2 + (x^2 + 1/2)^2}$. Minimizing the distance $d(x)$ is equivalent to minimizing its square, $D(x) = d(x)^2$, which avoids dealing with the square root during differentiation.\n$$ D(x) = (x - 2)^2 + (x^2 + \\frac{1}{2})^2 $$\nTo find the value of $x$ that minimizes this distance, we apply the first derivative test from calculus. The minimum must occur at a critical point where the derivative of $D(x)$ is zero. Let's define the function $f(x)$ as the derivative of $D(x)$:\n$$ f(x) = \\frac{d}{dx}D(x) = 2(x - 2) \\cdot 1 + 2(x^2 + \\frac{1}{2}) \\cdot (2x) $$\nSimplifying this expression gives:\n$$ f(x) = (2x - 4) + (4x^3 + 2x) = 4x^3 + 4x - 4 $$\nThe problem of finding the closest point is now reduced to finding the root of the nonlinear equation $f(x) = 0$. This can be solved using open methods like Newton's method or the secant method.\n\nFor Newton's method, we also need the derivative of $f(x)$:\n$$ f'(x) = \\frac{d}{dx}(4x^3 + 4x - 4) = 12x^2 + 4 $$\nSince $f'(x) = 12x^2 + 4 \\ge 4$ for all real $x$, the function $f(x)$ is strictly increasing, guaranteeing a unique real root. This also corresponds to the unique global minimum of the distance function $D(x)$.\n\n### Algorithmic Approach\n\n1.  **Newton's Method**: The iteration is given by $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$. Starting from an initial guess $x_0$, this formula is applied until convergence.\n2.  **Secant Method**: This method avoids the need for an analytical derivative by approximating it with a finite difference: $f'(x_k) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}$. The iteration is $x_{k+1} = x_k - f(x_k)\\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}$. This requires two initial guesses, $x_0$ and $x_1$.\n\nThe implementation will apply these methods according to the test cases, using the specified stopping criteria. Once the root $x^\\star$ is found, the closest point on the parabola is $(x^\\star, (x^\\star)^2)$, and the minimum distance can be calculated.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the closest point on the parabola y = x^2 to a fixed point P\n    by solving a root-finding problem with Newton's and secant methods.\n    \"\"\"\n\n    # The function f(x) is the derivative of the squared distance function D(x).\n    # D(x) = (x - 2)^2 + (x^2 + 1/2)^2\n    # f(x) = D'(x) = 2(x - 2) + 2(x^2 + 1/2)(2x) = 4x^3 + 4x - 4\n    def f(x):\n        return 4 * x**3 + 4 * x - 4\n\n    # The derivative of f(x), required for Newton's method.\n    # f'(x) = 12x^2 + 4\n    def f_prime(x):\n        return 12 * x**2 + 4\n\n    def newton_method(x0, tol, max_iter):\n        \"\"\"\n        Implementation of Newton's method for root finding.\n        x_{k+1} = x_k - f(x_k) / f'(x_k)\n        \"\"\"\n        xk = float(x0)\n        for _ in range(max_iter):\n            fxk = f(xk)\n            f_prime_xk = f_prime(xk)\n            \n            # Avoid division by zero, although not possible for this specific f_prime(x)\n            if abs(f_prime_xk)  1e-15:\n                # This would indicate failure, but our f' is always >= 4.\n                return None  \n            \n            xk_plus_1 = xk - fxk / f_prime_xk\n            \n            # Stopping criteria: step size or residual is below tolerance\n            if abs(xk_plus_1 - xk)  tol or abs(f(xk_plus_1))  tol:\n                return xk_plus_1\n            \n            xk = xk_plus_1\n        \n        # Return the last computed value if max_iter is reached\n        return xk\n\n    def secant_method(x0, x1, tol, max_iter):\n        \"\"\"\n        Implementation of the secant method for root finding.\n        x_{k+1} = x_k - f(x_k) * (x_k - x_{k-1}) / (f(x_k) - f(x_{k-1}))\n        \"\"\"\n        xk_minus_1 = float(x0)\n        xk = float(x1)\n        fxk_minus_1 = f(xk_minus_1)\n\n        for _ in range(max_iter):\n            fxk = f(xk)\n            \n            denominator = fxk - fxk_minus_1\n            # Avoid division by zero. f(x) is monotonic, so this only happens\n            # if iterates are identical or due to precision loss.\n            if abs(denominator)  1e-15:\n                return xk\n\n            xk_plus_1 = xk - fxk * (xk - xk_minus_1) / denominator\n\n            # Stopping criteria: step size or residual is below tolerance\n            if abs(xk_plus_1 - xk)  tol or abs(f(xk_plus_1))  tol:\n                return xk_plus_1\n            \n            xk_minus_1 = xk\n            fxk_minus_1 = fxk\n            xk = xk_plus_1\n            \n        # Return the last computed value if max_iter is reached\n        return xk\n\n    # Test cases as defined in the problem statement.\n    # (method_name, [initial_guesses], tolerance, max_iterations)\n    test_cases = [\n        ('newton', [0], 1e-12, 50),\n        ('newton', [2], 1e-12, 50),\n        ('secant', [0, 1], 1e-12, 100),\n        ('secant', [-1, 1], 1e-12, 100),\n        ('newton', [-5], 1e-12, 50),\n    ]\n\n    all_results = []\n    fixed_point_p = (2.0, -0.5)\n\n    for case in test_cases:\n        method_name, initial_guesses, tol, max_iter = case\n        \n        root = None\n        if method_name == 'newton':\n            root = newton_method(initial_guesses[0], tol, max_iter)\n        elif method_name == 'secant':\n            root = secant_method(initial_guesses[0], initial_guesses[1], tol, max_iter)\n        \n        if root is not None:\n            x_star = root\n            y_star = x_star**2\n            \n            # Calculate Euclidean distance to P\n            distance = np.sqrt((x_star - fixed_point_p[0])**2 + (y_star - fixed_point_p[1])**2)\n            \n            # Format results to 12 decimal places as strings\n            result_str = f\"[{x_star:.12f},{y_star:.12f},{distance:.12f}]\"\n            all_results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While open methods like Newton's can converge rapidly, they lack the guaranteed convergence of bracketing methods. This exercise guides you in building a hybrid algorithm that captures the best of both worlds: the speed of Newton's method and the reliability of bisection. The resulting \"safeguarded\" solver uses Newton's method as its primary engine but intelligently switches to a bisection step if a Newton step is unstable or would leave a known root-containing bracket $[a, b]$ . This approach is fundamental to creating the kind of robust numerical tools used in professional scientific software.",
            "id": "3260144",
            "problem": "You are to design and implement a program that computes approximations of simple real roots using a safeguarded open method. Your method must combine Newton’s method with the bisection method, switching to bisection whenever the Newton trial step would leave a bracketing interval or when the derivative magnitude is too small to define a stable Newton step. The derivation must begin from the first-order Taylor approximation and proceed to the iterative update rule; the code must reflect this principle. The method must maintain a bracketing invariant at every iteration.\n\nAssume the following foundational facts:\n- If a function $f$ is continuous on an interval $[a,b]$ and $f(a)f(b) \\le 0$, then there exists at least one $x^{\\ast}\\in[a,b]$ with $f(x^{\\ast})=0$.\n- The first-order Taylor approximation of a differentiable function $f$ near a point $x_k$ is $f(x_k+s) \\approx f(x_k)+f'(x_k)s$.\n\nYour tasks:\n- From the first-order Taylor approximation, derive an iteration that attempts to make the next residual vanish and thus produces a Newton trial point. Your method must use the bracketing information as a safeguard: if the Newton trial point leaves $[a_k,b_k]$, or if the derivative has magnitude below a specified safeguarding threshold, then take a bisection step instead.\n- Maintain a valid bracket $[a_k,b_k]$ at each iteration by updating the interval endpoints according to the sign of $f$ evaluated at the accepted next iterate. If $f(a_k)=0$ or $f(b_k)=0$ at any time, the corresponding endpoint is a root and must be returned immediately.\n- Use the following parameter values in your implementation:\n  - Function tolerance $\\varepsilon_f = 10^{-10}$,\n  - Interval half-width tolerance $\\varepsilon_x = 10^{-10}$,\n  - Maximum number of iterations $N_{\\max}=100$,\n  - Derivative safeguarding threshold $\\delta = 10^{-14}$.\n- Use the initial iterate $x_0=(a_0+b_0)/2$.\n- Stopping criteria: stop and return the current approximation $x_k$ as soon as any of the following hold:\n  - $|f(x_k)| \\le \\varepsilon_f$,\n  - $\\frac{1}{2}(b_k-a_k) \\le \\varepsilon_x$,\n  - $k \\ge N_{\\max}$, in which case return the current midpoint $\\frac{a_k+b_k}{2}$.\n\nImplementation contract:\n- Implement a function that applies the above hybrid rule to compute a single root for a given function $f$, its derivative $f'$, and an initial bracket $[a_0,b_0]$.\n- The Newton trial must be rejected and replaced by a bisection step whenever the trial lies outside $[a_k,b_k]$ or when $|f'(x_k)|  \\delta$.\n\nTest suite:\nCompute the root approximations for the following six cases. Angles for trigonometric functions must be interpreted in radians.\n- Case $1$: $f_1(x)=\\cos(x)-x$ on $[0,1]$.\n- Case $2$: $f_2(x)=x^3-x-2$ on $[1,2]$.\n- Case $3$: $f_3(x)=e^x-3x^2$ on $[0,1]$.\n- Case $4$: $f_4(x)=(x-0.1)^3$ on $[0,1]$.\n- Case $5$: $f_5(x)=x^3-2x+2$ on $[-2,0]$.\n- Case $6$: $f_6(x)=\\sin(x)$ on $[0,4]$.\n\nOutput specification:\n- Your program must produce a single line of output containing the six computed roots in the order of the cases above, formatted as a comma-separated list of floating-point numbers enclosed in square brackets, with no spaces.\n- The output type for each test case is a floating-point number. No physical units apply. Angles are in radians.",
            "solution": "The problem requires the design and implementation of a safeguarded root-finding algorithm. This algorithm combines the rapid convergence of Newton's method with the guaranteed convergence of the bisection method. The bisection method acts as a safeguard, ensuring that the iterate remains within a valid bracket containing the root and that the algorithm remains stable even when the derivative is near zero.\n\n### Principle and Derivation\n\nThe fundamental goal is to find a root $x^{\\ast}$ for a function $f(x)$, which is a value such that $f(x^{\\ast}) = 0$. The method assumes we are given an initial interval $[a_0, b_0]$ where the function is continuous and for which $f(a_0)f(b_0) \\le 0$. The Intermediate Value Theorem guarantees that at least one root exists within this interval.\n\n**1. Newton's Method Trial Step**\n\nNewton's method is an open method that approximates the function with its tangent line at the current iterate. The next iterate is chosen as the root of this tangent line. This is derived from the first-order Taylor series expansion of $f(x)$ around a point $x_k$:\n$$f(x) \\approx f(x_k) + f'(x_k)(x - x_k)$$\nWe seek the next iterate, which we will call $x_{k+1}$, such that $f(x_{k+1}) = 0$. Substituting $x = x_{k+1}$ and setting the approximation to zero gives:\n$$0 \\approx f(x_k) + f'(x_k)(x_{k+1} - x_k)$$\nAssuming the derivative $f'(x_k)$ is non-zero, we can solve for $x_{k+1}$:\n$$f'(x_k)(x_{k+1} - x_k) = -f(x_k)$$\n$$x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$$\nThis is the iterative formula for Newton's method. It generally exhibits quadratic convergence when the initial guess is sufficiently close to a simple root. However, it can diverge if the initial guess is poor, or converge slowly or fail if the derivative $f'(x_k)$ is close to or equal to zero.\n\n**2. Bisection Method Safeguard**\n\nThe bisection method is a bracketing method that is slower (linear convergence) but guaranteed to converge. Given a bracket $[a_k, b_k]$ such that $f(a_k)f(b_k) \\le 0$, the method proposes the midpoint as the next test point:\n$$x_{k+1} = \\frac{a_k + b_k}{2}$$\nThe bracket is then updated by replacing either $a_k$ or $b_k$ with $x_{k+1}$, ensuring the new, smaller bracket $[a_{k+1}, b_{k+1}]$ still contains a root. This process systematically halves the interval of uncertainty.\n\n**3. The Hybrid Algorithm**\n\nThe specified algorithm leverages the strengths of both methods. At each iteration $k$, starting with an iterate $x_k$ and a bracket $[a_k, b_k]$:\n\n1.  A Newton step is proposed: $x_{N} = x_k - f(x_k)/f'(x_k)$.\n2.  This step is checked against two safeguarding conditions:\n    a.  **Stability Condition:** The magnitude of the derivative must be sufficiently large, $|f'(x_k)| \\ge \\delta$, where $\\delta$ is a small positive threshold (given as $10^{-14}$). A derivative close to zero can cause the Newton step to be erratically large and numerically unstable.\n    b.  **Bracketing Condition:** The Newton trial point $x_{N}$ must lie strictly within the current bracket, i.e., $x_{N} \\in (a_k, b_k)$. This prevents the iterate from leaving the region where a root is known to exist.\n3.  If either safeguard is violated, the algorithm rejects the Newton step and falls back to the robust bisection step: $x_{k+1} = (a_k + b_k)/2$.\n4.  If the Newton step is accepted, the next iterate is $x_{k+1} = x_{N}$.\n5.  After determining the next iterate $x_{k+1}$, the bracketing interval is updated. Let $f_a = f(a_k)$ and $f_{new} = f(x_{k+1})$. If $f_a \\cdot f_{new}  0$, the root is in $[a_k, x_{k+1}]$, so the new bracket becomes $[a_{k+1}, b_{k+1}] = [a_k, x_{k+1}]$. Otherwise, the root must be in $[x_{k+1}, b_k]$, and the new bracket becomes $[a_{k+1}, b_{k+1}] = [x_{k+1}, b_k]$. This ensures the bracketing invariant $f(a_{k+1})f(b_{k+1}) \\le 0$ is maintained.\n\nThe process is initiated with $x_0 = (a_0 + b_0)/2$ and continues until one of the following stopping criteria is met:\n-   The function value is sufficiently close to zero: $|f(x_k)| \\le \\varepsilon_f$.\n-   The interval of uncertainty is sufficiently small: $\\frac{1}{2}(b_k - a_k) \\le \\varepsilon_x$.\n-   A maximum number of iterations, $N_{\\max}$, is reached.\n\n### Algorithmic Steps\n\nThe procedure can be formally stated as follows:\n\n1.  **Initialization**:\n    -   Given a function $f$, its derivative $f'$, an initial interval $[a, b]$, and parameters $\\varepsilon_f=10^{-10}$, $\\varepsilon_x=10^{-10}$, $\\delta=10^{-14}$, $N_{\\max}=100$.\n    -   Set $a_k \\leftarrow a$, $b_k \\leftarrow b$. Compute $f(a_k)$ and $f(b_k)$.\n    -   Check for initial conditions: If $f(a_k) \\cdot f(b_k) > 0$, the initial interval is invalid. If $f(a_k)=0$ or $f(b_k)=0$, return the corresponding endpoint.\n    -   Set the initial iterate $x_k \\leftarrow (a_k + b_k)/2$.\n\n2.  **Iteration**: For $k = 0, 1, 2, \\ldots, N_{\\max}-1$:\n    a.  **Check stopping criteria**: Evaluate $f(x_k)$. If $|f(x_k)| \\le \\varepsilon_f$ or if $(b_k - a_k)/2 \\le \\varepsilon_x$, terminate and return the current iterate $x_k$.\n    b.  **Propose next iterate**:\n        i.  Evaluate $f'(x_k)$.\n        ii. If $|f'(x_k)|  \\delta$, set the next iterate $x_{next} \\leftarrow (a_k + b_k)/2$.\n        iii. Else, calculate the Newton iterate $x_{N} \\leftarrow x_k - f(x_k)/f'(x_k)$. If $a_k  x_{N}  b_k$, set $x_{next} \\leftarrow x_{N}$. Otherwise, set $x_{next} \\leftarrow (a_k + b_k)/2$.\n    c.  **Update bracket and iterate**:\n        i.  Evaluate $f(x_{next})$. If $f(x_{next})=0$, return $x_{next}$.\n        ii. If $f(a_k) \\cdot f(x_{next})  0$, set $b_k \\leftarrow x_{next}$. Else, set $a_k \\leftarrow x_{next}$ (and update corresponding cached function values).\n        iii. Set $x_k \\leftarrow x_{next}$ for the subsequent iteration.\n\n3.  **Termination**: If the loop completes without meeting the stopping criteria, return the midpoint of the final bracket, $(a_k + b_k)/2$, as the best available approximation.\n\nThis structured, safeguarded approach ensures both efficiency and robustness, making it a reliable method for finding real roots of differentiable functions.",
            "answer": "```python\nimport numpy as np\n\ndef find_root_safeguarded(f, fp, a, b, eps_f, eps_x, delta, n_max):\n    \"\"\"\n    Computes a root of f(x)=0 within the interval [a, b] using a safeguarded\n    Newton's method.\n\n    The method combines Newton's method with bisection as a fallback.\n    \"\"\"\n    ak, bk = float(a), float(b)\n    fak, fbk = f(ak), f(bk)\n\n    # Initial validation and endpoint checks\n    if fak * fbk > 0.0:\n        # According to the problem statement, all initial brackets are valid.\n        # This is for robustness in a general context.\n        raise ValueError(\"Root not bracketed or multiple roots in initial interval.\")\n    \n    if fak == 0.0:\n        return ak\n    if fbk == 0.0:\n        return bk\n\n    # Per problem statement: \"Use the initial iterate x_0=(a_0+b_0)/2\"\n    xk = (ak + bk) / 2.0\n    \n    for _ in range(n_max):\n        # 1. Check stopping criteria at the beginning of the iteration\n        fxk = f(xk)\n        if abs(fxk) = eps_f:\n            return xk\n        \n        # The problem asks to return xk if interval is small, which is the\n        # point from the previous iteration.\n        if (bk - ak) / 2.0 = eps_x:\n            return xk\n\n        # 2. Propose the next iterate, x_next\n        fpxk = fp(xk)\n        \n        # Default to bisection step (safeguard)\n        x_bisection = (ak + bk) / 2.0\n        \n        # Decide whether to use the Newton step\n        if abs(fpxk) >= delta:\n            x_newton = xk - fxk / fpxk\n            # Check if Newton step is safely within the bracket\n            if ak  x_newton  bk:\n                x_next = x_newton\n            else:\n                x_next = x_bisection # Fallback to bisection\n        else:\n            x_next = x_bisection # Fallback to bisection\n            \n        # 3. Update the bracket [ak, bk] using the new point x_next\n        fx_next = f(x_next)\n        \n        # Check for an exact root hit\n        if fx_next == 0.0:\n            return x_next\n\n        # Update the bracket while maintaining the invariant f(a)*f(b)  0\n        if fak * fx_next  0.0:\n            bk, fbk = x_next, fx_next\n        else:\n            ak, fak = x_next, fx_next\n        \n        # 4. The new point becomes the iterate for the next loop\n        xk = x_next\n\n    # 5. If max iterations reached, return the midpoint of the final bracket\n    return (ak + bk) / 2.0\n\ndef solve():\n    \"\"\"\n    Solves the root-finding problem for the specified test suite.\n    \"\"\"\n    # Define parameters from the problem statement\n    params = {\n        'eps_f': 1e-10,\n        'eps_x': 1e-10,\n        'n_max': 100,\n        'delta': 1e-14\n    }\n\n    # Define the test cases\n    test_cases = [\n        # Case 1: f(x) = cos(x) - x on [0, 1]\n        (lambda x: np.cos(x) - x, lambda x: -np.sin(x) - 1, 0.0, 1.0),\n        # Case 2: f(x) = x^3 - x - 2 on [1, 2]\n        (lambda x: x**3 - x - 2, lambda x: 3*x**2 - 1, 1.0, 2.0),\n        # Case 3: f(x) = e^x - 3x^2 on [0, 1]\n        (lambda x: np.exp(x) - 3*x**2, lambda x: np.exp(x) - 6*x, 0.0, 1.0),\n        # Case 4: f(x) = (x - 0.1)^3 on [0, 1]\n        (lambda x: (x - 0.1)**3, lambda x: 3*(x - 0.1)**2, 0.0, 1.0),\n        # Case 5: f(x) = x^3 - 2x + 2 on [-2, 0]\n        (lambda x: x**3 - 2*x + 2, lambda x: 3*x**2 - 2, -2.0, 0.0),\n        # Case 6: f(x) = sin(x) on [0, 4]\n        (lambda x: np.sin(x), lambda x: np.cos(x), 0.0, 4.0),\n    ]\n\n    results = []\n    for f, fp, a, b in test_cases:\n        root = find_root_safeguarded(f, fp, a, b, **params)\n        results.append(root)\n\n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Many critical problems in science and engineering, from electrical circuit analysis to quantum mechanics, require finding roots in the complex plane. This exercise introduces Müller's method, a powerful technique that generalizes the secant method by fitting a parabola through three points instead of a line through two. You will implement this method to solve for the complex roots of $z^4 + 1 = 0$, a classic problem demonstrating how higher-order interpolation can naturally find non-real solutions .",
            "id": "2422736",
            "problem": "Consider the complex-valued function $f(z) = z^4 + 1$ defined on the field of complex numbers. A root is any complex number $z$ such that $f(z) = 0$. Your task is to write a complete, runnable program that, for each provided set of three distinct initial complex guesses $(z_0, z_1, z_2)$, uses an open root-finding approach that operates without bracketing and allows complex-valued iterates to converge to a single root of $f(z) = 0$. For numerical termination, use an absolute step-size tolerance of $10^{-12}$ on the update in $z$ and an absolute function-value tolerance of $10^{-12}$ on $|f(z)|$, with a maximum of $100$ iterations per case. All computations are dimensionless. Angles, if any arise internally, must be treated in radians.\n\nTest suite of initial guesses (each case is an ordered triple $(z_0, z_1, z_2)$ of complex numbers):\n- Case $1$: $(0.4 + 0.6 i,\\; 0.9 + 1.1 i,\\; 0.7 + 0.5 i)$\n- Case $2$: $(-1.2 + 1.0 i,\\; -0.6 + 0.8 i,\\; -0.9 + 0.7 i)$\n- Case $3$: $(-0.9 - 1.1 i,\\; -0.6 - 0.5 i,\\; -0.8 - 0.7 i)$\n- Case $4$: $(1.0 - 1.0 i,\\; 0.6 - 0.8 i,\\; 0.9 - 0.6 i)$\n- Case $5$ (edge case near the real axis): $(0.1 + 10^{-12} i,\\; 0.5 + 10^{-12} i,\\; 0.9 + 10^{-12} i)$\n\nFor each case, your program must compute one root to which the method converges, and report it as its real and imaginary parts. The required final output is a single line containing a list of length $5$, where each element is a two-element list $[x,y]$ with $x$ the real part and $y$ the imaginary part of the computed root. Round each $x$ and $y$ to $10$ decimal places. The final line must therefore look like\n$[[x_1,y_1],[x_2,y_2],[x_3,y_3],[x_4,y_4],[x_5,y_5]]$\nwith no extra spaces and no additional text. All numbers must be reported as plain real-valued floats rounded to $10$ decimal places.",
            "solution": "The problem requires finding the complex roots of the polynomial $f(z) = z^4 + 1$ using an open, non-bracketing method that starts with three initial points. This specification points to the use of **Muller's method**, a powerful generalization of the secant method.\n\n### Principle and Derivation\n\nWhile the secant method approximates a function with a line passing through two points, Muller's method fits a parabola through three points to find the next approximation of the root. This higher-order interpolation allows the method to naturally find complex roots even from real initial guesses and generally provides faster convergence (approximately order $1.84$).\n\nLet the three most recent estimates of a root be $z_{k-2}$, $z_{k-1}$, and $z_k$. The core idea is to construct a quadratic polynomial $P(z)$ that interpolates the function $f(z)$ at these three points. A numerically stable way to write this parabola is centered at the latest point, $z_k$:\n$$P(z) = a(z - z_k)^2 + b(z - z_k) + c$$\nThe coefficients $a$, $b$, and $c$ are efficiently determined using divided differences, which avoids issues with ill-conditioned matrices that can arise from solving a linear system:\n- $c = f(z_k)$\n- $b = f[z_{k-1}, z_k] + (z_k - z_{k-1}) f[z_{k-2}, z_{k-1}, z_k]$\n- $a = f[z_{k-2}, z_{k-1}, z_k]$\n\nHere, $f[z_i, z_j] = \\frac{f(z_j) - f(z_i)}{z_j - z_i}$ is a first-order divided difference, and $f[z_{k-2}, z_{k-1}, z_k] = \\frac{f[z_{k-1}, z_k] - f[z_{k-2}, z_{k-1}]}{z_k - z_{k-2}}$ is a second-order divided difference.\n\nThe next iterate, $z_{k+1}$, is found by solving the quadratic equation $P(z)=0$. The change from the current point, $\\Delta z = z_{k+1} - z_k$, is a root of $a(\\Delta z)^2 + b(\\Delta z) + c = 0$. To avoid loss of precision from the subtraction of nearly equal numbers in the standard quadratic formula, a stabilized version is used:\n$$ \\Delta z = \\frac{-2c}{b \\pm \\sqrt{b^2 - 4ac}} $$\nThe sign in the denominator is chosen to maximize its magnitude. This makes the resulting update step $\\Delta z$ the smaller of the two possible steps, guiding the iteration towards the root closest to the current estimate $z_k$.\n\n### Algorithmic Approach\n\nThe iterative procedure is as follows:\n1.  Start with three distinct complex guesses $z_0, z_1, z_2$.\n2.  In each iteration $k$, use the points $(z_{k-2}, z_{k-1}, z_k)$ to compute the coefficients $a, b, c$ of the interpolating parabola.\n3.  Calculate the update step $\\Delta z$ using the stabilized quadratic root formula.\n4.  Compute the new approximation $z_{k+1} = z_k + \\Delta z$.\n5.  Check for convergence based on the absolute step size $|\\Delta z|$ and the function value $|f(z_{k+1})|$.\n6.  If not converged, update the set of points for the next iteration: $(z_{k-1}, z_k, z_{k+1})$.\n7.  Repeat until convergence is achieved or the maximum number of iterations is reached.",
            "answer": "```python\n# The complete and runnable Python 3 code for solving the root-finding problem.\n# This program uses Muller's method to find roots of f(z) = z^4 + 1.\n\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It defines the function, test cases, and calls the root-finding algorithm,\n    then formats and prints the final result.\n    \"\"\"\n    \n    def f(z: complex) -> complex:\n        \"\"\"\n        The complex-valued function f(z) = z^4 + 1.\n        \"\"\"\n        return z**4 + 1\n\n    def muller(f_func, z0, z1, z2, tol=1e-12, max_iter=100):\n        \"\"\"\n        Implements Muller's method for finding a complex root.\n\n        Args:\n            f_func: The function for which to find a root.\n            z0, z1, z2: Three initial distinct complex guesses.\n            tol: The tolerance for termination.\n            max_iter: The maximum number of iterations.\n\n        Returns:\n            The complex root found, or the last estimate if max_iter is reached.\n        \"\"\"\n        p = [z0, z1, z2]\n        \n        for _ in range(max_iter):\n            p0, p1, p2 = p[0], p[1], p[2]\n            f0, f1, f2 = f_func(p0), f_func(p1), f_func(p2)\n\n            # To avoid division by zero in degenerate cases.\n            eps = np.finfo(complex).eps\n            if abs(p1 - p0)  eps or abs(p2 - p1)  eps or abs(p2 - p0)  eps:\n                return p2\n\n            # Coefficients of the interpolating parabola using divided differences.\n            # Polynomial P(z) = a(z - p2)^2 + b(z - p2) + c\n            d1 = (f1 - f0) / (p1 - p0)\n            d2 = (f2 - f1) / (p2 - p1)\n            \n            # Coefficient 'a'\n            a = (d2 - d1) / (p2 - p0)\n            \n            # Coefficient 'b'\n            b = d2 + (p2 - p1) * a\n            \n            # Coefficient 'c'\n            c = f2\n            \n            # Calculate the update step using the numerically stable formula.\n            discriminant = np.sqrt(b**2 - 4*a*c)\n            \n            # Choose the denominator with the largest magnitude.\n            den_plus = b + discriminant\n            den_minus = b - discriminant\n            \n            if abs(den_plus) > abs(den_minus):\n                denominator = den_plus\n            else:\n                denominator = den_minus\n            \n            # If denominator is too small, we might be at a root or stagnating.\n            if abs(denominator)  eps:\n                return p2\n\n            dz = -2 * c / denominator\n            p_new = p2 + dz\n            \n            # Termination criteria: step-size AND function-value tolerances met.\n            if abs(dz)  tol and abs(f_func(p_new))  tol:\n                return p_new\n\n            # Update points for the next iteration.\n            p = [p1, p2, p_new]\n            \n        return p[2]\n\n    # Test suite of initial guesses as specified in the problem statement.\n    test_cases = [\n        (0.4 + 0.6j, 0.9 + 1.1j, 0.7 + 0.5j),\n        (-1.2 + 1.0j, -0.6 + 0.8j, -0.9 + 0.7j),\n        (-0.9 - 1.1j, -0.6 - 0.5j, -0.8 - 0.7j),\n        (1.0 - 1.0j, 0.6 - 0.8j, 0.9 - 0.6j),\n        (0.1 + 1e-12j, 0.5 + 1e-12j, 0.9 + 1e-12j),\n    ]\n\n    results = []\n    for case in test_cases:\n        z0, z1, z2 = case\n        root = muller(f, z0, z1, z2)\n        \n        # Round the real and imaginary parts to 10 decimal places.\n        x = round(root.real, 10)\n        y = round(root.imag, 10)\n        \n        # Ensure -0.0 is represented as 0.0\n        if x == -0.0: x = 0.0\n        if y == -0.0: y = 0.0\n            \n        formatted_root = [x, y]\n        results.append(formatted_root)\n\n    # Convert the list of results to a string and remove spaces for exact output format.\n    final_output_string = str(results).replace(\" \", \"\")\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}