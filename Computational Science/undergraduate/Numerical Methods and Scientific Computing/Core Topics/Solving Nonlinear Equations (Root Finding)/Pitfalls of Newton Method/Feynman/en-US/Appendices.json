{
    "hands_on_practices": [
        {
            "introduction": "The celebrated quadratic convergence of Newton's method is one of its most powerful features, but it relies on the assumption that the root is simple. This exercise explores the common pitfall where this assumption fails, specifically for roots of multiplicity $p \\gt 1$. You will discover through analysis and implementation how the convergence degrades to a slow linear rate and, more importantly, how a simple modification known as the scaled-step method can restore the method's rapid convergence .",
            "id": "3262250",
            "problem": "Let $f:\\mathbb{R}\\to\\mathbb{R}$ be given by $f(x)=(x-\\pi)^4$, which has a root at $x^\\star=\\pi$ with multiplicity $p=4$. Consider the fundamental base for root-finding given by Newton's method, defined by the iteration $x_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}$, and the notion of multiplicity of a root: if $f(x)=(x-x^\\star)^p g(x)$ with $g(x^\\star)\\neq 0$, then $x^\\star$ is a root of multiplicity $p>1$. It is well known that standard Newton's method can lose its typical quadratic convergence at multiple roots. Your task is to reason from these fundamentals, derive a modification that rescales the Newton step by a constant factor, analyze its error dynamics for multiple roots, and then implement and evaluate it on $f(x)=(x-\\pi)^4$.\n\nRequirements:\n- Starting from the base Newton iteration $x_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}$ and the definition of multiplicity, derive the error update for a scaled-step family where the Newton step is multiplied by a real parameter $m$. Write the error $e_k=x_k-x^\\star$ and, using only algebra and the product and chain rules of differentiation, derive the exact mapping for $e_{k+1}$ when $f(x)=(x-x^\\star)^p$.\n- Use your derivation to explain, in purely mathematical terms, why standard Newton's method on a multiple root exhibits only linear convergence and how an appropriate choice of the scaling restores at least quadratic convergence. Your reasoning must proceed from the base definitions and not assume any pre-known specialized formula.\n- Implement the scaled-step iteration in a program and evaluate the following test suite on $f(x)=(x-\\pi)^4$, using $x^\\star=\\pi$ and $p=4$. For each case, define $e_k=x_k-\\pi$ and follow the instructions to compute the required output for that case:\n    - Test $1$ (loss of quadratic convergence): use $m=1$ and $x_0=0$. Perform exactly $1$ iteration to obtain $x_1$ and report the observed linear factor $r_1=\\frac{|e_1|}{|e_0|}$ as a floating-point number.\n    - Test $2$ (restored fast convergence): use $m=4$ and $x_0=0$. Iterate until $|e_k|<\\varepsilon$ with tolerance $\\varepsilon=10^{-12}$ or until a maximum of $50$ iterations is reached. Report the integer iteration count $N_4$ required to meet the tolerance.\n    - Test $3$ (undercorrected scaling): use $m=3$ and $x_0=-1$. Perform exactly $1$ iteration and report $r_3=\\frac{|e_1|}{|e_0|}$ as a floating-point number.\n    - Test $4$ (overcorrected but convergent scaling): use $m=5$ and $x_0=4$. Perform exactly $1$ iteration and report the signed ratio $\\rho_5=\\frac{e_1}{e_0}$ as a floating-point number.\n    - Test $5$ (divergence due to excessive scaling): use $m=9$ and $x_0=0$. Perform $3$ iterations, producing $e_1$, $e_2$, and $e_3$. Report a boolean that is True if $|e_1|>|e_0|$, $|e_2|>|e_1|$, and $|e_3|>|e_2|$, and False otherwise.\n- Your program should:\n    - Implement the function $f(x)=(x-\\pi)^4$ and its derivative $f'(x)=4(x-\\pi)^3$.\n    - Implement a single scaled-step Newton update using parameter $m$.\n    - For floating-point outputs, round to $6$ decimal places before producing the final output.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., [resultA,resultB,resultC,resultD,resultE]). The results must appear in the order $[r_1,N_4,r_3,\\rho_5,\\text{diverged}]$, where $r_1$, $r_3$, and $\\rho_5$ are floats rounded to $6$ decimal places, $N_4$ is an integer, and $\\text{diverged}$ is a boolean. No units are involved in this problem, and all angles are in radians by definition of $\\pi$.",
            "solution": "The problem of finding a root $x^\\star$ of a function $f(x)$ is a cornerstone of numerical analysis. Newton's method provides a powerful iterative algorithm for this purpose. However, its celebrated quadratic convergence depends on the root being simple. For a root $x^\\star$ of multiplicity $p > 1$, the convergence degrades to linear. This analysis will derive the precise error dynamics for a scaled version of Newton's method when applied to a function with a multiple root, explain the mechanism of this convergence degradation, and show how a specific scaling can restore rapid convergence.\n\nLet the function be $f:\\mathbb{R}\\to\\mathbb{R}$, and let $x^\\star$ be a root of multiplicity $p$, meaning $f(x) = (x-x^\\star)^p g(x)$ where $g(x^\\star) \\neq 0$. The problem asks for an analysis of the specific case where $f(x)=(x-x^\\star)^p$, which corresponds to $g(x)=1$ for all $x$.\n\nThe scaled Newton's iteration is given by\n$$x_{k+1} = x_k - m \\frac{f(x_k)}{f'(x_k)}$$\nwhere $m$ is a real-valued scaling parameter. The error at iteration $k$ is defined as $e_k = x_k - x^\\star$. Consequently, we can write $x_k = x^\\star + e_k$ and $x_{k+1} = x^\\star + e_{k+1}$. Substituting these into the iteration formula yields:\n$$x^\\star + e_{k+1} = (x^\\star + e_k) - m \\frac{f(x^\\star + e_k)}{f'(x^\\star + e_k)}$$\nSubtracting $x^\\star$ from both sides gives the error update equation:\n$$e_{k+1} = e_k - m \\frac{f(x^\\star + e_k)}{f'(x^\\star + e_k)}$$\n\nFollowing the problem's directive, we analyze this for the specific function $f(x) = (x-x^\\star)^p$.\nFirst, we evaluate the function at $x_k = x^\\star + e_k$:\n$$f(x_k) = f(x^\\star + e_k) = ((x^\\star + e_k) - x^\\star)^p = e_k^p$$\nNext, we find the derivative of $f(x)$ using the chain rule and power rule:\n$$f'(x) = \\frac{d}{dx}(x-x^\\star)^p = p(x-x^\\star)^{p-1} \\cdot \\frac{d}{dx}(x-x^\\star) = p(x-x^\\star)^{p-1}$$\nEvaluating the derivative at $x_k = x^\\star + e_k$:\n$$f'(x_k) = f'(x^\\star + e_k) = p((x^\\star + e_k) - x^\\star)^{p-1} = p e_k^{p-1}$$\nNow, we substitute these expressions for $f(x_k)$ and $f'(x_k)$ into the error update equation:\n$$e_{k+1} = e_k - m \\frac{e_k^p}{p e_k^{p-1}}$$\nAssuming $e_k \\neq 0$ (i.e., we have not yet converged), we can simplify the fraction:\n$$e_{k+1} = e_k - m \\frac{e_k}{p}$$\nFactoring out $e_k$, we arrive at the exact error mapping for this function:\n$$e_{k+1} = e_k \\left(1 - \\frac{m}{p}\\right)$$\n\nThis derived relationship, $e_{k+1} = C e_k$ where $C = 1 - \\frac{m}{p}$ is a constant, is the definition of linear convergence. The error is reduced by a fixed factor at each step. The speed of convergence depends on the magnitude of this factor, $|C|$. For convergence, we require $|C|<1$.\n\nWe can now analyze the behavior of Newton's method based on this result.\nFirst, consider standard Newton's method, which corresponds to a scaling factor of $m=1$. The error update equation becomes:\n$$e_{k+1} = e_k \\left(1 - \\frac{1}{p}\\right)$$\nFor a multiple root, we have $p > 1$. Therefore, the convergence factor $C = 1 - \\frac{1}{p}$ is a constant satisfying $0 < C < 1$. For the specific case of $p=4$, the factor is $1 - \\frac{1}{4} = \\frac{3}{4}$. The error is reduced by only $25\\%$ at each step, a clear demonstration of linear, not quadratic, convergence. This explains the loss of rapid convergence for standard Newton's method at multiple roots.\n\nSecond, consider how to restore faster convergence. The goal is to choose the scaling parameter $m$ such that the convergence factor $|C| = |1 - \\frac{m}{p}|$ is minimized. The ideal value for $C$ is $0$, as this would imply $e_{k+1}=0$ and convergence in a single step. Setting the factor to zero:\n$$1 - \\frac{m}{p} = 0 \\implies m=p$$\nBy choosing the scaling factor $m$ to be equal to the root's multiplicity $p$, we achieve $e_{k+1} = e_k(1-\\frac{p}{p}) = 0$. This demonstrates that for a function of the form $f(x)=(x-x^\\star)^p$, the modified Newton's method with $m=p$ finds the exact root in a single iteration (assuming infinite precision arithmetic). For the more general case $f(x)=(x-x^\\star)^p g(x)$, a more detailed analysis involving Taylor series shows that setting $m=p$ eliminates the linear term in the error expansion, leading to $e_{k+1} = O(e_k^2)$, thus restoring at least quadratic convergence. The appropriate choice of $m$ fundamentally alters the error dynamics, removing the bottleneck that causes linear convergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and evaluates a scaled-step Newton's method for a function with a multiple root.\n    \"\"\"\n    # Define pi from numpy for precision\n    pi = np.pi\n    p = 4  # Multiplicity of the root for f(x) = (x-pi)^4\n\n    # Define the function and its derivative\n    def f(x_val):\n        return (x_val - pi)**p\n\n    def fp(x_val):\n        if x_val == pi:\n            return 0.0\n        return p * (x_val - pi)**(p - 1)\n\n    # --- Test Cases ---\n\n    results = []\n\n    # Test 1: Standard Newton's method (m=1), showing linear convergence\n    m1 = 1\n    x0_1 = 0.0\n    # Calculate one iteration\n    e0_1 = x0_1 - pi\n    # The term f(x)/f'(x) simplifies to (x-pi)/p\n    x1_1 = x0_1 - m1 * (x0_1 - pi) / p\n    e1_1 = x1_1 - pi\n    r1 = abs(e1_1 / e0_1)\n    results.append(round(r1, 6))\n\n    # Test 2: Modified Newton (m=p), restoring fast convergence\n    m2 = p\n    x0_2 = 0.0\n    tol = 1e-12\n    max_iter = 50\n    xk_2 = x0_2\n    N4 = 0\n    for k in range(max_iter + 1):\n        ek_2 = xk_2 - pi\n        if abs(ek_2) < tol:\n            N4 = k\n            break\n        # Avoid division by zero if already at the root\n        deriv_val = fp(xk_2)\n        if deriv_val == 0:\n            N4 = k\n            break\n        xk_2 = xk_2 - m2 * f(xk_2) / deriv_val\n    else:\n        N4 = max_iter\n    results.append(N4)\n\n    # Test 3: Undercorrected scaling (m=3)\n    m3 = 3\n    x0_3 = -1.0\n    # Calculate one iteration\n    e0_3 = x0_3 - pi\n    # The term f(x)/f'(x) simplifies to (x-pi)/p\n    x1_3 = x0_3 - m3 * (x0_3 - pi) / p\n    e1_3 = x1_3 - pi\n    r3 = abs(e1_3 / e0_3)\n    results.append(round(r3, 6))\n\n    # Test 4: Overcorrected but convergent scaling (m=5)\n    m4 = 5\n    x0_4 = 4.0\n    # Calculate one iteration\n    e0_4 = x0_4 - pi\n    # The term f(x)/f'(x) simplifies to (x-pi)/p\n    x1_4 = x0_4 - m4 * (x0_4 - pi) / p\n    e1_4 = x1_4 - pi\n    rho5 = e1_4 / e0_4\n    results.append(round(rho5, 6))\n\n    # Test 5: Divergence from excessive scaling (m=9)\n    m5 = 9\n    x0_5 = 0.0\n    errors = [x0_5 - pi]\n    xk_5 = x0_5\n    for _ in range(3):\n        # The term f(x)/f'(x) simplifies to (x-pi)/p\n        xk_5 = xk_5 - m5 * (xk_5 - pi) / p\n        errors.append(xk_5 - pi)\n    \n    diverged = (abs(errors[1]) > abs(errors[0])) and \\\n               (abs(errors[2]) > abs(errors[1])) and \\\n               (abs(errors[3]) > abs(errors[2]))\n    results.append(diverged)\n\n    # Final print statement in the exact required format.\n    # Results are [r1, N4, r3, rho5, diverged]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Deciding when an iterative algorithm has converged sufficiently is a critical aspect of numerical methods. A tempting but potentially dangerous stopping criterion is to check if the step size, $|x_{k+1} - x_k|$, has become smaller than a tolerance. This hands-on practice will demonstrate how this heuristic can fail dramatically, leading to premature termination far from the true root, particularly for functions with very large derivative values .",
            "id": "3262113",
            "problem": "Consider the task of implementing the classical Newton iteration for solving a scalar nonlinear equation. The fundamental base is the definition of Newton's method as a fixed-point iteration for root finding: given a differentiable function $f:\\mathbb{R}\\to\\mathbb{R}$ and its derivative $f'$, the sequence $\\{x_k\\}$ is generated by the update rule $x_{k+1}=x_k-\\dfrac{f(x_k)}{f'(x_k)}$. It is conventional to use termination criteria to decide when to stop iterating. A common, but potentially misleading, choice is the step-size criterion $\\lvert x_{k+1}-x_k\\rvert<\\tau$, where $\\tau$ is a prescribed small tolerance. The numerical pitfall is that a small step size does not guarantee small residual $\\lvert f(x_k)\\rvert$, especially for functions with large derivatives or poor scaling. Your program must demonstrate this pitfall objectively.\n\nYour task is to write a complete program that, for a given test suite of functions and initial guesses, performs Newton's method using only the step-size termination criterion $\\lvert x_{k+1}-x_k\\rvert<\\tau$ with $\\tau=10^{-8}$ and records whether the following event occurs at any iteration:\n\"There exists $k$ such that $\\lvert x_{k+1}-x_k\\rvert<10^{-8}$ and simultaneously $\\lvert f(x_k)\\rvert>100$.\"\nThe indicator for each test case must be a boolean value. If the event occurs for a test case, the program should return True for that case; otherwise return False. The iteration should proceed as follows for each case: start at $x_0$; at each iteration compute $f(x_k)$ and $f'(x_k)$; if $f'(x_k)=0$ or any non-finite quantity is encountered, abort the iteration and return False for that case; otherwise compute $x_{k+1}=x_k-\\dfrac{f(x_k)}{f'(x_k)}$ and check the step-size criterion. If $\\lvert x_{k+1}-x_k\\rvert<10^{-8}$, immediately check $\\lvert f(x_k)\\rvert>100$ and return the corresponding boolean for that case. If the criterion is never met within a maximum of $N_{\\max}$ iterations, return False. Use $N_{\\max}=40$ iterations.\n\nThe test suite is designed to probe different facets of the pitfall:\n\n- Case $1$: $f(x)=a\\,x$ with $a=2\\times 10^{12}$ and $x_0=10^{-10}$. This is a scaled linear function with an enormous slope. Its derivative is $f'(x)=a$.\n- Case $2$: $f(x)=a\\,x$ with $a=10^{6}$ and $x_0=10^{-9}$. This is a moderately scaled linear function. Its derivative is $f'(x)=a$.\n- Case $3$: $f(x)=x^2-2$ with $x_0=1.5$. This is a standard nonlinear test with derivative $f'(x)=2x$.\n- Case $4$: $f(x)=\\exp(b\\,x)-1$ with $b=2\\times 10^{12}$ and $x_0=10^{-10}$. This is a stiff exponential function with derivative $f'(x)=b\\,\\exp(b\\,x)$.\n- Case $5$: $f(x)=x^3$ with $x_0=10$. This is a slowly contracting cubic near the origin with derivative $f'(x)=3x^2$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the boolean result for Case $i$ in the order listed above. No physical units are involved, and no angles are used.\n\nThe final output must be exactly one line in the format $[r_1,r_2,r_3,r_4,r_5]$.",
            "solution": "The problem requires an implementation of Newton's method to find the root of a scalar function $f(x)$, demonstrating a specific pitfall related to a common termination criterion. The method generates a sequence of approximations $\\{x_k\\}$ using the iterative formula:\n$$\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n$$\nwhere $f'(x)$ is the derivative of $f(x)$.\n\nA typical way to terminate the iteration is to check if the step size, $\\lvert x_{k+1} - x_k \\rvert$, becomes smaller than a prescribed tolerance $\\tau$. The problem highlights that this criterion can be misleading. A small step size does not necessarily imply that the point $x_k$ is near a root, i.e., that the residual $\\lvert f(x_k) \\rvert$ is small. The step size is given by:\n$$\n\\lvert \\Delta x_k \\rvert = \\lvert x_{k+1} - x_k \\rvert = \\left\\lvert \\frac{f(x_k)}{f'(x_k)} \\right\\rvert\n$$\nIf the magnitude of the derivative, $\\lvert f'(x_k) \\rvert$, is very large, the step size $\\lvert \\Delta x_k \\rvert$ can be small even when the residual $\\lvert f(x_k) \\rvert$ is large. The iteration might stop prematurely at a point that is not a good approximation of the root.\n\nThe task is to check for the occurrence of this specific event: for any iteration $k$, does the condition $\\lvert x_{k+1} - x_k \\rvert < 10^{-8}$ hold while simultaneously $\\lvert f(x_k) \\rvert > 100$? The algorithm is to run for a maximum of $N_{\\max}=40$ iterations. If the condition is met, the result for that test case is True. If the iteration stops for any other reason (zero derivative, non-finite values, step-size criterion met but residual is not large, or maximum iterations reached), the result is False.\n\nWe analyze each test case as follows:\n\n**Case 1**: $f(x)=a\\,x$ with $a=2\\times 10^{12}$ and $x_0=10^{-10}$. The derivative is $f'(x)=a$.\nAt the first iteration ($k=0$):\n$x_0 = 10^{-10}$.\n$f(x_0) = (2 \\times 10^{12}) \\cdot (10^{-10}) = 200$.\n$f'(x_0) = a = 2 \\times 10^{12}$.\nThe update yields $x_1$:\n$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)} = x_0 - \\frac{a x_0}{a} = x_0 - x_0 = 0$.\nThe step size is $\\lvert \\Delta x_0 \\rvert = \\lvert x_1 - x_0 \\rvert = \\lvert 0 - 10^{-10} \\rvert = 10^{-10}$.\nWe check the termination criterion: $10^{-10} < 10^{-8}$, which is true.\nNow we check the pitfall condition using the values at iteration $k=0$: $\\lvert f(x_0) \\rvert = \\lvert 200 \\rvert = 200$.\nThe condition $\\lvert f(x_0) \\rvert > 100$ is satisfied since $200 > 100$.\nThus, the event occurs. The result is True.\n\n**Case 2**: $f(x)=a\\,x$ with $a=10^{6}$ and $x_0=10^{-9}$. The derivative is $f'(x)=a$.\nAt $k=0$:\n$x_0 = 10^{-9}$.\n$f(x_0) = (10^6) \\cdot (10^{-9}) = 10^{-3} = 0.001$.\n$f'(x_0) = a = 10^6$.\n$x_1 = x_0 - \\frac{a x_0}{a} = 0$.\nThe step size is $\\lvert \\Delta x_0 \\rvert = \\lvert 0 - 10^{-9} \\rvert = 10^{-9}$.\nThe termination criterion $10^{-9} < 10^{-8}$ is met.\nWe check the pitfall condition: $\\lvert f(x_0) \\rvert = \\lvert 0.001 \\rvert = 0.001$.\nThe condition $\\lvert f(x_0) \\rvert > 100$ is not satisfied.\nThe result is False.\n\n**Case 3**: $f(x)=x^2-2$ with $x_0=1.5$. The derivative is $f'(x)=2x$.\nThis is the standard application of Newton's method to find $\\sqrt{2}$. The method exhibits quadratic convergence. As $x_k$ approaches the root $\\sqrt{2}$, the residual $f(x_k)=x_k^2-2$ approaches $0$. The iteration will eventually satisfy the step-size criterion, but at that point, $\\lvert f(x_k) \\rvert$ will be very close to $0$, and certainly not greater than $100$. For example, at $x_0=1.5$, $f(x_0)=0.25$, which is already small. The pitfall condition is never met.\nThe result is False.\n\n**Case 4**: $f(x)=\\exp(b\\,x)-1$ with $b=2\\times 10^{12}$ and $x_0=10^{-10}$. The derivative is $f'(x)=b\\,\\exp(b\\,x)$.\nAt $k=0$, the argument of the exponential is $b x_0 = (2\\times 10^{12}) \\cdot (10^{-10}) = 200$.\n$f(x_0) = \\exp(200) - 1$.\n$f'(x_0) = b \\exp(b x_0) = (2\\times 10^{12})\\exp(200)$.\nThe step size is $\\lvert \\Delta x_0 \\rvert = \\left\\lvert -\\frac{f(x_0)}{f'(x_0)} \\right\\rvert = \\left\\lvert \\frac{\\exp(200)-1}{(2\\times 10^{12})\\exp(200)} \\right\\rvert = \\frac{1 - \\exp(-200)}{2\\times 10^{12}}$.\nSince $\\exp(-200)$ is negligibly small, $\\lvert \\Delta x_0 \\rvert \\approx \\frac{1}{2\\times 10^{12}} = 5 \\times 10^{-13}$.\nThe termination criterion $5 \\times 10^{-13} < 10^{-8}$ is met.\nWe check the pitfall condition: $\\lvert f(x_0) \\rvert = \\lvert \\exp(200) - 1 \\rvert \\approx 7.22 \\times 10^{86}$.\nThis value is vastly greater than $100$.\nThe event occurs. The result is True.\n\n**Case 5**: $f(x)=x^3$ with $x_0=10$. The derivative is $f'(x)=3x^2$.\nThe iteration formula simplifies to a linear recurrence:\n$x_{k+1} = x_k - \\frac{x_k^3}{3x_k^2} = x_k - \\frac{x_k}{3} = \\frac{2}{3}x_k$.\nThe sequence is given by $x_k = 10 \\cdot (\\frac{2}{3})^k$.\nThe step size is $\\lvert \\Delta x_k \\rvert = \\lvert x_{k+1} - x_k \\rvert = \\lvert \\frac{2}{3}x_k - x_k \\rvert = \\frac{1}{3}\\lvert x_k \\rvert = \\frac{10}{3} (\\frac{2}{3})^k$.\nWe need to find if there exists an integer $k < 40$ such that $\\frac{10}{3}(\\frac{2}{3})^k < 10^{-8}$.\nThis inequality can be solved for $k$:\n$(\\frac{2}{3})^k < 3 \\times 10^{-9} \\implies k \\ln(\\frac{2}{3}) < \\ln(3 \\times 10^{-9})$.\nSince $\\ln(\\frac{2}{3}) \\approx -0.40547$ is negative, we reverse the inequality sign:\n$k > \\frac{\\ln(3 \\times 10^{-9})}{\\ln(2/3)} \\approx \\frac{-19.62}{-0.40547} \\approx 48.39$.\nThe smallest integer $k$ satisfying the condition is $k=49$.\nSince the maximum number of iterations is $N_{\\max}=40$, the step-size criterion will not be met within the allowed budget. As per the problem rules, the iteration aborts after $40$ steps, and the result is False.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Newton's method for a suite of test cases to demonstrate\n    the pitfall of using a small step-size as a sole termination criterion.\n    \"\"\"\n\n    def check_newton_pitfall(f, fp, x0, tau, residual_threshold, n_max):\n        \"\"\"\n        Performs Newton's method and checks for the specified pitfall.\n\n        Args:\n            f: The function for which to find a root.\n            fp: The derivative of the function f.\n            x0: The initial guess.\n            tau: The step-size tolerance for termination.\n            residual_threshold: The threshold for checking large residual.\n            n_max: The maximum number of iterations.\n\n        Returns:\n            True if the pitfall condition is met, False otherwise.\n        \"\"\"\n        x_k = float(x0)\n\n        for _ in range(n_max):\n            try:\n                # Use numpy's float64 for calculations to handle large numbers.\n                f_xk = f(np.float64(x_k))\n                fp_xk = fp(np.float64(x_k))\n            except (OverflowError, ValueError):\n                # Handle cases where calculations result in overflow or invalid values (e.g., from np.exp)\n                return False\n\n            # Abort if derivative is zero or values are not finite.\n            if not np.isfinite(f_xk) or not np.isfinite(fp_xk) or fp_xk == 0:\n                return False\n\n            # Newton step\n            delta_x = -f_xk / fp_xk\n\n            # Check termination criterion\n            if np.abs(delta_x) < tau:\n                # If criterion is met, check for the pitfall condition\n                if np.abs(f_xk) > residual_threshold:\n                    return True  # Pitfall detected\n                else:\n                    return False # Converged properly (small residual)\n            \n            # Update for the next iteration\n            x_k += delta_x\n\n        # If the loop completes without meeting the criterion, the pitfall was not detected.\n        return False\n\n    # Define common parameters\n    tau = 1e-8\n    residual_threshold = 100.0\n    n_max = 40\n\n    # Test cases from the problem statement\n    test_cases = [\n        {\n            \"f\": lambda x, a=2e12: a * x,\n            \"fp\": lambda x, a=2e12: a,\n            \"x0\": 1e-10\n        },\n        {\n            \"f\": lambda x, a=1e6: a * x,\n            \"fp\": lambda x, a=1e6: a,\n            \"x0\": 1e-9\n        },\n        {\n            \"f\": lambda x: x**2 - 2.0,\n            \"fp\": lambda x: 2.0 * x,\n            \"x0\": 1.5\n        },\n        {\n            \"f\": lambda x, b=2e12: np.exp(b * x) - 1.0,\n            \"fp\": lambda x, b=2e12: b * np.exp(b * x),\n            \"x0\": 1e-10\n        },\n        {\n            \"f\": lambda x: x**3,\n            \"fp\": lambda x: 3.0 * x**2,\n            \"x0\": 10.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = check_newton_pitfall(\n            case[\"f\"],\n            case[\"fp\"],\n            case[\"x0\"],\n            tau,\n            residual_threshold,\n            n_max\n        )\n        results.append(result)\n\n    # Format the final output string as specified: [r1,r2,r3,r4,r5]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While Newton's method is powerful near a root, its behavior can be erratic when the initial guess is far from the solution, often leading to \"overshooting\" where iterates jump to unfavorable regions. This exercise introduces a fundamental globalization strategy, the backtracking line search, to make the method more robust. By implementing and testing this damping technique, you will see how to control the step size to ensure consistent progress toward the root, even when the standard method diverges or stagnates .",
            "id": "3262151",
            "problem": "Consider the scalar root-finding problem for the nonlinear function $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=\\tanh(10x)-0.5$. Newton's method for solving $f(x)=0$ is based on the first-order Taylor expansion of $f$ around a current iterate $x_k$, namely $f(x_k+p)\\approx f(x_k)+f'(x_k)p$, which neglects higher-order terms. Setting this linearization equal to zero and solving for $p$ yields the Newton step $p_k=-\\frac{f(x_k)}{f'(x_k)}$, and the next iterate $x_{k+1}=x_k+p_k$. The derivative $f'(x)$ is the derivative of the hyperbolic tangent, and must be computed exactly. In practice, this method can exhibit overshooting when $|f'(x_k)|$ is small or $|p_k|$ is large, sending $x_k$ into regions where the function $f$ saturates and $f'(x)$ is near zero, compromising convergence.\n\nTo mitigate overshooting, you must implement a damping strategy via Backtracking Line Search (BLS). Adopt the merit function $\\phi(x)=\\tfrac{1}{2}f(x)^2$, which encodes a scalar measure of residual size. For the scalar Newton direction $p_k=-\\frac{f(x_k)}{f'(x_k)}$, Backtracking Line Search chooses a step size $t_k\\in(0,1]$ to ensure sufficient decrease in $\\phi$. Specifically, starting from $t_k=1$, repeatedly replace $t_k\\leftarrow\\beta\\,t_k$ with $\\beta\\in(0,1)$ until the Armijo-type inequality\n$$\n\\phi(x_k+t_k p_k)\\le (1-c\\,t_k)\\,\\phi(x_k)\n$$\nis satisfied for a chosen constant $c\\in(0,1)$. If the derivative magnitude $|f'(x_k)|$ is below a prescribed threshold, the step must be rejected to avoid numerical instability arising from division by an extremely small number. If no $t_k$ above a minimum $t_{\\min}>0$ satisfies the inequality, the algorithm should terminate and report failure for that iteration sequence.\n\nYour program must implement two algorithms on the same test suite:\n- Undamped Newton's method (full step $t_k=1$ each iteration).\n- Damped Newton's method using Backtracking Line Search as described above.\n\nUse the following scientifically consistent specifications:\n- Function $f(x)=\\tanh(10x)-0.5$ and derivative $f'(x)=10\\,\\operatorname{sech}^2(10x)$, where $\\operatorname{sech}(z)=\\frac{1}{\\cosh(z)}$ and $\\cosh(z)$ is the hyperbolic cosine.\n- Merit function $\\phi(x)=\\tfrac{1}{2}f(x)^2$.\n- Convergence tolerance on the residual magnitude: stop if $|f(x_k)|\\le 10^{-12}$.\n- Maximum number of iterations per run: $50$.\n- Derivative safeguard: if $|f'(x_k)|<10^{-12}$, terminate the current run.\n- Backtracking parameters: $c=10^{-4}$, $\\beta=0.5$, $t_{\\min}=10^{-8}$.\n\nTest suite and coverage:\n- Happy path: $x_0=0.1$ (moderately close to the root).\n- Overshooting-prone initial point: $x_0=0.3$ (Newton's full step is very large).\n- Boundary case with near-zero derivative: $x_0=2.0$ (saturation region where $f'(x)$ is extremely small).\n- Sign-opposite starting point: $x_0=-0.2$ (must cross toward the unique positive root).\n\nFor each test case, run both methods and record the final residual magnitude $|f(x_{\\text{final}})|$ as a floating-point number for each method. The required final output format is a single line containing a comma-separated list enclosed in square brackets that aggregates, in order, the undamped residual then the damped residual for each test case. For example, the output must look like\n$$\n[\\text{residual\\_undamped\\_case1},\\text{residual\\_damped\\_case1},\\text{residual\\_undamped\\_case2},\\text{residual\\_damped\\_case2},\\text{residual\\_undamped\\_case3},\\text{residual\\_damped\\_case3},\\text{residual\\_undamped\\_case4},\\text{residual\\_damped\\_case4}]\n$$\nwith each entry a floating-point number. There are no physical units involved; all quantities are unitless real numbers.",
            "solution": "The design starts from the scalar Newton method derived via first-order Taylor expansion. For a differentiable scalar function $f:\\mathbb{R}\\to\\mathbb{R}$, the Taylor expansion around a current iterate $x_k$ is\n$$\nf(x_k+p)\\approx f(x_k)+f'(x_k)\\,p.\n$$\nSetting $f(x_k+p)=0$ in this linear model yields the Newton step\n$$\np_k=-\\frac{f(x_k)}{f'(x_k)},\n$$\nand the next iterate is $x_{k+1}=x_k+p_k$. This method is locally quadratically convergent under standard conditions: $f$ is sufficiently smooth, $f'(x^\\star)\\ne 0$ at the root $x^\\star$, and the initial guess is sufficiently close to $x^\\star$. However, far from the root or in regions where $f'(x)$ is small, $|p_k|$ can be very large, potentially sending the iterate $x_k$ into regions where $f$ saturates and $f'(x)$ is near zero, degrading progress. This phenomenon is overshooting.\n\nFor the specific function $f(x)=\\tanh(10x)-0.5$, there is one root because the hyperbolic tangent is strictly increasing and bounded between $-1$ and $1$. The root $x^\\star$ satisfies $\\tanh(10x^\\star)=0.5$, so $10x^\\star=\\operatorname{artanh}(0.5)$ and $x^\\star=\\operatorname{artanh}(0.5)/10$, where $\\operatorname{artanh}$ denotes the inverse hyperbolic tangent. The derivative is\n$$\nf'(x)=10\\,\\operatorname{sech}^2(10x)=10\\left(\\frac{1}{\\cosh(10x)}\\right)^2.\n$$\nWhen $|10x|$ is large, $\\cosh(10x)$ is large, so $f'(x)$ becomes very small, making the Newton step $p_k=-\\frac{f(x_k)}{f'(x_k)}$ extremely large in magnitude and unstable. Such steps easily propagate the iterates $x_k$ into the saturation zone where $\\tanh(10x)\\approx \\pm 1$, with $f'(x)\\approx 0$, leading to numerical difficulty.\n\nTo mitigate overshooting, we damp the Newton step using Backtracking Line Search (BLS) with a merit function\n$$\n\\phi(x)=\\frac{1}{2}f(x)^2,\n$$\nwhich measures residual magnitude. The scalar Newton direction $p_k=-\\frac{f(x_k)}{f'(x_k)}$ is a descent direction for $\\phi$ near the root. Indeed, the directional derivative of $\\phi$ in direction $p$ is\n$$\n\\phi'(x_k;p)=f(x_k)f'(x_k)\\,p,\n$$\nand substituting $p_k$ gives $\\phi'(x_k;p_k)=-f(x_k)^2$, which is negative whenever $f(x_k)\\ne 0$. This shows that, locally, the Newton step decreases the merit function $\\phi$. However, taking a full step $t_k=1$ may fail to decrease $\\phi$ globally. Therefore, Backtracking Line Search selects a step size $t_k\\in(0,1]$ such that the Armijo-type sufficient decrease condition holds:\n$$\n\\phi(x_k+t_k p_k)\\le (1-c\\,t_k)\\,\\phi(x_k),\n$$\nwhere $c\\in(0,1)$ is a small constant, and $t_k$ starts at $1$ and is reduced by a factor $\\beta\\in(0,1)$ until the inequality is satisfied. This procedure ensures a monotone reduction in the residual measure $\\phi$, guarding against overshooting. Continuity of $\\phi$ implies that sufficiently small $t_k$ will produce a decrease because $p_k$ is a descent direction, so unless $f'(x_k)$ is effectively zero, the backtracking process terminates with an acceptable $t_k$.\n\nAlgorithmic steps for the damped method at iteration $k$:\n1. Compute $f(x_k)$ and $f'(x_k)$. If $|f(x_k)|\\le 10^{-12}$, stop.\n2. If $|f'(x_k)|<10^{-12}$, terminate to avoid unstable steps.\n3. Compute the Newton direction $p_k=-\\frac{f(x_k)}{f'(x_k)}$.\n4. Initialize $t_k=1$. While $t_k\\ge t_{\\min}$:\n   - Form $x_k^{\\text{cand}}=x_k+t_k p_k$ and compute $\\phi(x_k^{\\text{cand}})$.\n   - If $\\phi(x_k^{\\text{cand}})\\le (1-c\\,t_k)\\,\\phi(x_k)$, accept the step: set $x_{k+1}=x_k^{\\text{cand}}$ and break.\n   - Else, reduce step size: $t_k\\leftarrow\\beta\\,t_k$.\n5. If no acceptable $t_k$ is found before $t_k<t_{\\min}$, terminate.\n6. Repeat until convergence or the iteration cap $50$ is reached.\n\nFor the undamped method, we simply set $t_k=1$ and apply $x_{k+1}=x_k+p_k$, with the same derivative safeguard.\n\nTest cases are designed to expose different behaviors:\n- $x_0=0.1$: Newton's method is expected to converge quickly; both methods should succeed.\n- $x_0=0.3$: The undamped step is large because $|f'(0.3)|$ is small, causing overshooting into saturation; the damped method reduces step sizes and should converge.\n- $x_0=2.0$: The derivative $f'(2.0)$ is extremely small ($\\cosh(20)$ is very large), producing an unstable undamped step; the damped method will initially take tiny steps but should move toward the root by monotonically reducing $\\phi$.\n- $x_0=-0.2$: The undamped step likely overshoots into the positive saturation region with $f'(x)\\approx 0$, while the damped method controls step length and converges.\n\nThe program outputs the final residual magnitudes $|f(x_{\\text{final}})|$ for each method and each test case, aggregated in a single list as specified:\n$[\\text{undamped}(0.1),\\text{damped}(0.1),\\text{undamped}(0.3),\\text{damped}(0.3),\\text{undamped}(2.0),\\text{damped}(2.0),\\text{undamped}(-0.2),\\text{damped}(-0.2)]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(x: float) -> float:\n    # f(x) = tanh(10x) - 0.5\n    return float(np.tanh(10.0 * x) - 0.5)\n\ndef df(x: float) -> float:\n    # f'(x) = 10 * sech^2(10x), where sech(z) = 1/cosh(z)\n    # Implement carefully to avoid overflow: for large |z|, cosh(z) can overflow to inf; 1/inf -> 0 safely.\n    z = 10.0 * x\n    cosh_z = np.cosh(z)\n    # Handle potential overflow: if cosh_z is inf, then sech^2 is 0.\n    if not np.isfinite(cosh_z):\n        return 0.0\n    sech_z = 1.0 / cosh_z\n    return float(10.0 * (sech_z ** 2))\n\ndef newton_undamped(x0: float, max_iters: int = 50, tol_res: float = 1e-12, min_df: float = 1e-12) -> float:\n    x = float(x0)\n    for _ in range(max_iters):\n        fx = f(x)\n        if abs(fx) <= tol_res:\n            break\n        dfx = df(x)\n        if abs(dfx) < min_df:\n            # Derivative too small; stop to avoid unstable steps\n            break\n        step = -fx / dfx\n        x = x + step\n    return abs(f(x))\n\ndef newton_backtracking(x0: float,\n                        max_iters: int = 50,\n                        tol_res: float = 1e-12,\n                        c: float = 1e-4,\n                        beta: float = 0.5,\n                        t_min: float = 1e-8,\n                        min_df: float = 1e-12) -> float:\n    x = float(x0)\n    for _ in range(max_iters):\n        fx = f(x)\n        if abs(fx) <= tol_res:\n            break\n        dfx = df(x)\n        if abs(dfx) < min_df:\n            # Derivative too small; stop to avoid unstable steps\n            break\n        p = -fx / dfx\n        phi_x = 0.5 * (fx ** 2)\n        t = 1.0\n        accepted = False\n        # Backtracking loop\n        while t >= t_min:\n            x_candidate = x + t * p\n            f_cand = f(x_candidate)\n            phi_cand = 0.5 * (f_cand ** 2)\n            if phi_cand <= (1.0 - c * t) * phi_x:\n                x = x_candidate\n                accepted = True\n                break\n            t *= beta\n        if not accepted:\n            # Could not find an acceptable step size\n            break\n    return abs(f(x))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Initial guesses: happy path, overshooting-prone, boundary case (near-zero derivative), sign-opposite start\n    initial_guesses = [0.1, 0.3, 2.0, -0.2]\n\n    # Algorithm parameters as specified\n    max_iters = 50\n    tol_res = 1e-12\n    c = 1e-4\n    beta = 0.5\n    t_min = 1e-8\n    min_df = 1e-12\n\n    results = []\n    for x0 in initial_guesses:\n        r_undamped = newton_undamped(x0, max_iters=max_iters, tol_res=tol_res, min_df=min_df)\n        r_damped = newton_backtracking(x0, max_iters=max_iters, tol_res=tol_res,\n                                       c=c, beta=beta, t_min=t_min, min_df=min_df)\n        results.append(r_undamped)\n        results.append(r_damped)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}