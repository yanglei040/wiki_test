{
    "hands_on_practices": [
        {
            "introduction": "The best way to understand an algorithm is to perform its operations by hand. This first exercise grounds the theory of the method of false position in a concrete calculation. You will apply the fundamental formula to find the first approximation of a root for a given function, giving you a feel for how the secant line homes in on the solution. ",
            "id": "2217545",
            "problem": "In a simplified model of a self-regulating thermal system, the non-trivial equilibrium temperature is described by the positive root of the transcendental equation $f(x) = \\exp(-x) - x = 0$, where $x$ is a dimensionless temperature parameter. It is known from physical constraints that a root lies within the interval $[0, 1]$.\n\nTo approximate this root, a numerical one-step method is employed. The first approximation, denoted as $x_1$, is determined by finding the x-intercept of the secant line that connects the two points on the curve $y=f(x)$ corresponding to the interval's endpoints.\n\nCalculate the value of this first approximation, $x_1$. Round your final answer to four significant figures.",
            "solution": "We are given $f(x) = \\exp(-x) - x$ and the interval $[0,1]$. The secant line through the points $(0,f(0))$ and $(1,f(1))$ has slope\n$$\nm=\\frac{f(1)-f(0)}{1-0} = \\left(\\exp(-1)-1\\right) - 1 = \\exp(-1) - 2.\n$$\nThe equation of this secant line is\n$$\ny = f(0) + m(x-0) = 1 + \\left(\\exp(-1) - 2\\right)x.\n$$\nThe first approximation $x_{1}$ is the $x$-intercept of this line, obtained by setting $y=0$ and solving for $x$:\n$$\n0 = 1 + \\left(\\exp(-1) - 2\\right)x \\quad \\Longrightarrow \\quad x_{1} = \\frac{1}{2 - \\exp(-1)}.\n$$\nFor the required numerical value, evaluate\n$$\n\\exp(-1) \\approx 0.3678794412,\\quad 2 - \\exp(-1) \\approx 1.6321205588,\\quad x_{1} \\approx \\frac{1}{1.6321205588} \\approx 0.6126998368.\n$$\nRounding to four significant figures gives $x_{1} \\approx 0.6127$.",
            "answer": "$$\\boxed{0.6127}$$"
        },
        {
            "introduction": "While the method of false position often converges faster than the bisection method, this is not always the case. This practice presents a scenario where high functional curvature causes the method to perform poorly, a phenomenon known as endpoint locking. By comparing its performance directly against the bisection method, you will gain a quantitative understanding of convergence rates and the importance of choosing the right tool for the job. ",
            "id": "3251509",
            "problem": "Let $f(x)$ be a continuous scalar function on an interval $[a,b]$ with $f(a)f(b)  0$. Two bracketing root-finding methods that rely on continuity and sign changes are the bisection method and the method of false position (regula falsi). The bisection method uses repeated halving of the interval guided by the Intermediate Value Theorem (IVT), while the method of false position replaces the midpoint with the $x$-intercept of the line joining the endpoints on the graph of $f$. High curvature can cause the method of false position to converge more slowly than the bisection method because the secant may intersect the $x$-axis close to the same endpoint repeatedly, leaving one endpoint effectively fixed and yielding small updates.\n\nModel this situation concretely with the function $f(x) = \\exp(10x) - 2$ on the interval $[0,1]$.\n\nTasks:\n- Using the Intermediate Value Theorem (IVT) and monotonicity, justify that the equation $f(x) = 0$ has a unique root in $[0,1]$ and write the exact root symbolically.\n- From first principles of linear interpolation on $[a,b]$ with a sign change, derive the update rule for the bisection method and for the method of false position (regula falsi) without quoting any pre-existing iteration formulas.\n- Starting with $a_0 = 0$ and $b_0 = 1$, perform $5$ iterations of each method for the given $f$. In both methods, at each iteration, define the current root approximation as the point that the method evaluates for deciding the next subinterval: for the bisection method, the midpoint of the current interval; for the method of false position, the $x$-intercept of the secant through $(a_k,f(a_k))$ and $(b_k,f(b_k))$.\n- Let $r$ denote the exact root. Compute the absolute error after $5$ iterations for each method, $E_{\\mathrm{bisect}} = |x_{\\mathrm{bisect},5} - r|$ and $E_{\\mathrm{rf}} = |x_{\\mathrm{rf},5} - r|$, and report the single quantity $E_{\\mathrm{rf}} / E_{\\mathrm{bisect}}$.\n\nRound your final ratio to four significant figures. No physical units are needed.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and complete. It describes a standard numerical analysis exercise comparing the convergence of the bisection and regula falsi methods. All necessary data and definitions are provided. We may proceed with the solution.\n\nThe function under consideration is $f(x) = \\exp(10x) - 2$ on the interval $[0,1]$.\n\n**Justification of a Unique Root**\n\nFirst, we establish the existence of a root in the interval $[0,1]$ using the Intermediate Value Theorem (IVT). The function $f(x)$ is a composition of the exponential function and a linear function, both of which are continuous for all real numbers. Thus, $f(x)$ is continuous on $[0,1]$. We evaluate the function at the endpoints of the interval:\n$f(0) = \\exp(10 \\cdot 0) - 2 = \\exp(0) - 2 = 1 - 2 = -1$.\n$f(1) = \\exp(10 \\cdot 1) - 2 = \\exp(10) - 2$. Since $e \\approx 2.718$, $\\exp(10)$ is a very large positive number, so $f(1)  0$.\nSpecifically, $f(1) \\approx 22026.466 - 2 = 22024.466$.\nSince $f(0)  0$ and $f(1)  0$, we have $f(0)f(1)  0$. By the IVT, there must exist at least one root $r \\in (0,1)$ such that $f(r)=0$.\n\nTo establish the uniqueness of this root, we examine the function's monotonicity. We compute the first derivative of $f(x)$:\n$$f'(x) = \\frac{d}{dx}(\\exp(10x) - 2) = 10\\exp(10x)$$\nFor any $x \\in [0,1]$, the exponential function $\\exp(10x)$ is strictly positive. Therefore, $f'(x)  0$ for all $x \\in [0,1]$. This means that $f(x)$ is strictly increasing on the interval. A strictly monotonic function can cross the x-axis at most once. Hence, the root $r$ is unique.\n\nThe exact value of the root, denoted by $r$, is found by solving $f(x) = 0$:\n$$\\exp(10r) - 2 = 0$$\n$$\\exp(10r) = 2$$\n$$10r = \\ln(2)$$\n$$r = \\frac{\\ln(2)}{10}$$\nNumerically, $r \\approx 0.069314718$.\n\n**Derivation of Update Rules**\n\nLet the current bracketing interval be $[a,b]$ with $f(a)f(b)  0$.\n\nFor the **bisection method**, the next approximation is chosen as the midpoint of the interval. This choice stems from the desire to reduce the interval size by the largest guaranteed factor, which is $2$. The update rule for the approximation at iteration $k+1$, which we denote $x_{k+1}$, is:\n$$x_{k+1} = c = \\frac{a_k + b_k}{2}$$\nThe new interval $[a_{k+1}, b_{k+1}]$ is then chosen as either $[a_k, c]$ or $[c, b_k]$ to maintain the sign change, i.e., $f(a_{k+1})f(b_{k+1})0$.\n\nFor the **method of false position (regula falsi)**, the next approximation is the $x$-intercept of the secant line connecting the points $(a, f(a))$ and $(b, f(b))$. The equation of a line passing through these two points can be written using the point-slope form:\n$$y - f(a) = \\frac{f(b) - f(a)}{b - a}(x - a)$$\nThe $x$-intercept is the value of $x$ for which $y=0$. Let's call this intercept $c$.\n$$0 - f(a) = \\frac{f(b) - f(a)}{b - a}(c - a)$$\nSolving for $c$:\n$$(c - a) = -f(a) \\frac{b - a}{f(b) - f(a)}$$\n$$c = a - f(a) \\frac{b - a}{f(b) - f(a)}$$\nThis can be rearranged into a more symmetric form:\n$$c = \\frac{a(f(b) - f(a)) - f(a)(b - a)}{f(b) - f(a)} = \\frac{a f(b) - a f(a) - b f(a) + a f(a)}{f(b) - f(a)} = \\frac{a f(b) - b f(a)}{f(b) - f(a)}$$\nSo, the update rule for the approximation is:\n$$x_{k+1} = c = \\frac{a_k f(b_k) - b_k f(a_k)}{f(b_k) - f(a_k)}$$\nSimilar to the bisection method, the new interval is chosen to preserve the sign change.\n\n**Execution of Five Iterations**\n\nWe start with the interval $[a_0, b_0] = [0,1]$.\n\n**Bisection Method:**\n- Iteration $1$: $x_{\\mathrm{bisect},1} = \\frac{0+1}{2} = 0.5$. $f(0.5) = \\exp(5)-2  0$. New interval is $[0, 0.5]$.\n- Iteration $2$: $x_{\\mathrm{bisect},2} = \\frac{0+0.5}{2} = 0.25$. $f(0.25) = \\exp(2.5)-2  0$. New interval is $[0, 0.25]$.\n- Iteration $3$: $x_{\\mathrm{bisect},3} = \\frac{0+0.25}{2} = 0.125$. $f(0.125) = \\exp(1.25)-2  0$. New interval is $[0, 0.125]$.\n- Iteration $4$: $x_{\\mathrm{bisect},4} = \\frac{0+0.125}{2} = 0.0625$. $f(0.0625) = \\exp(0.625)-2  0$. New interval is $[0.0625, 0.125]$.\n- Iteration $5$: $x_{\\mathrm{bisect},5} = \\frac{0.0625+0.125}{2} = 0.09375$.\n\nThe fifth approximation is $x_{\\mathrm{bisect},5} = 0.09375$.\n\n**Method of False Position:**\nWe start with $[a_0, b_0] = [0,1]$, $f(a_0)=-1$, $f(b_0)=\\exp(10)-2$.\n- Iteration $1$: $x_{\\mathrm{rf},1} = \\frac{0 \\cdot f(1) - 1 \\cdot f(0)}{f(1)-f(0)} = \\frac{-(-1)}{\\exp(10)-2 - (-1)} = \\frac{1}{\\exp(10)-1} \\approx 0.00004540$. Since $f(x_{\\mathrm{rf},1})  0$, the new interval is $[x_{\\mathrm{rf},1}, 1]$.\n- Iteration $2$: $a_1=x_{\\mathrm{rf},1}, b_1=1$. $x_{\\mathrm{rf},2} = \\frac{a_1 f(b_1) - b_1 f(a_1)}{f(b_1)-f(a_1)} \\approx 0.00009078$. Since $f(x_{\\mathrm{rf},2})  0$, the new interval is $[x_{\\mathrm{rf},2}, 1]$.\n- Iteration $3$: $a_2=x_{\\mathrm{rf},2}, b_2=1$. $x_{\\mathrm{rf},3} \\approx 0.00013612$. Since $f(x_{\\mathrm{rf},3})  0$, the new interval is $[x_{\\mathrm{rf},3}, 1]$.\n- Iteration $4$: $a_3=x_{\\mathrm{rf},3}, b_3=1$. $x_{\\mathrm{rf},4} \\approx 0.00018144$. Since $f(x_{\\mathrm{rf},4})  0$, the new interval is $[x_{\\mathrm{rf},4}, 1]$.\n- Iteration $5$: $a_4=x_{\\mathrm{rf},4}, b_4=1$. $x_{\\mathrm{rf},5} \\approx 0.00022673$.\n\nNote the characteristic slow convergence of regula falsi for this function. Due to the high curvature, $f(x_{\\mathrm{rf},k})$ is always negative, causing the right endpoint $b_k=1$ to remain fixed (or \"stuck\"). The approximation crawls very slowly toward the root from the left.\nUsing high-precision calculation, the fifth approximation is $x_{\\mathrm{rf},5} \\approx 0.0002268285$.\n\n**Error Ratio Calculation**\n\nThe exact root is $r = \\frac{\\ln(2)}{10} \\approx 0.0693147181$.\nThe fifth approximation for the bisection method is $x_{\\mathrm{bisect},5} = 0.09375$.\nThe absolute error for the bisection method is:\n$E_{\\mathrm{bisect}} = |x_{\\mathrm{bisect},5} - r| = |0.09375 - 0.0693147181| \\approx 0.0244352819$.\n\nThe fifth approximation for the method of false position is $x_{\\mathrm{rf},5} \\approx 0.0002268285$.\nThe absolute error for the method of false position is:\n$E_{\\mathrm{rf}} = |x_{\\mathrm{rf},5} - r| = |0.0002268285 - 0.0693147181| \\approx 0.0690878896$.\n\nThe ratio of the errors is:\n$$\\frac{E_{\\mathrm{rf}}}{E_{\\mathrm{bisect}}} = \\frac{0.0690878896}{0.0244352819} \\approx 2.827367468$$\nRounding this result to four significant figures gives $2.827$.\nThis confirms that for this particular function and initial interval, the error after $5$ iterations is significantly larger for the method of false position than for the bisection method.",
            "answer": "$$\n\\boxed{2.827}\n$$"
        },
        {
            "introduction": "Having seen both the power and the pitfalls of the pure method of false position, we can now engineer a more intelligent solution. This final practice moves from analysis to synthesis, challenging you to implement a hybrid algorithm that combines the speed of regula falsi with the reliability of bisection. This is how robust numerical libraries are built, addressing theoretical weaknesses to create practical, high-performance tools. ",
            "id": "3251464",
            "problem": "Design and implement a complete, runnable program that computes roots of continuous real-valued functions by a hybrid bracketing method that primarily uses the method of false position (regula falsi) and temporarily switches to a bisection step if one endpoint of the bracketing interval remains stationary for two consecutive iterations. The method must operate on a continuous function $f$ on a closed interval $[a,b]$ with $f(a)\\cdot f(b)\\le 0$ so that, by the Intermediate Value Theorem, there exists at least one root in $[a,b]$. The program must not rely on any user input.\n\nYou must build the algorithm from first principles. Begin from the following base:\n- Continuity of $f$ on $[a,b]$ and the Intermediate Value Theorem guarantee the existence of at least one $x^\\star\\in[a,b]$ with $f(x^\\star)=0$ whenever $f(a)\\cdot f(b)\\le 0$.\n- The concept of linear interpolation of $(a,f(a))$ and $(b,f(b))$ yields an $x$-location where that interpolant crosses the horizontal axis, which serves as a candidate point inside $[a,b]$ for updating the bracket in a sign-preserving manner.\n- Partitioning an interval by its midpoint (the bisection method) preserves a sign change bracket and reduces interval width by a factor of $2$.\n\nYour hybrid method must satisfy all of the following requirements.\n- Initialization and bracketing:\n  - Accept a continuous function $f$, an initial bracket $[a,b]$ with $f(a)\\cdot f(b)\\le 0$, an absolute function tolerance $\\tau_f>0$, an absolute interval-length tolerance $\\tau_x>0$, and a maximum iteration count $N_{\\max}\\in\\mathbb{N}$.\n  - If $f(a)=0$ or $f(b)=0$ at initialization, return that endpoint as the root approximation immediately.\n- Iteration and update rule:\n  - On each iteration, compute a candidate point $c$ inside $[a,b]$ by linear interpolation of $(a,f(a))$ and $(b,f(b))$ as the primary step. Use the sign of $f$ at the candidate to select the new subinterval that preserves a sign change, and update the bracketing endpoints accordingly.\n  - Track which endpoint was updated at each iteration. If the same endpoint is updated on two consecutive iterations, then the opposite endpoint has remained stationary for two iterations. In that case, on the next iteration you must perform a single bisection update instead of a linear-interpolation update, and after this bisection update you must reset the counters that track endpoint stationarity.\n  - For numerical robustness, if the linear-interpolation step would be ill-posed due to an effectively vanishing denominator (for example, $|f(b)-f(a)|$ below machine scale), perform a bisection update for that iteration instead.\n- Termination and returned value:\n  - Terminate when either $|f(c)|\\le \\tau_f$ at the current candidate $c$, or when the current bracketing interval width satisfies $b-a\\le \\tau_x$, or when the iteration counter reaches $N_{\\max}$. Return the most recent candidate $c$ (or an exact endpoint root if detected at initialization).\n- Angle unit: whenever trigonometric functions are used, treat angles in radians.\n- Output formatting: for each test case in the test suite below, return the computed root approximation as a floating-point number rounded to $10$ decimal places.\n- Final output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, for example $[r_1,r_2,\\dots]$.\n\nTest suite. Implement your program so that it computes, in this exact order, the root approximations for the following $5$ cases, each with $\\tau_x=10^{-10}$, $\\tau_f=10^{-12}$, and $N_{\\max}=100$.\n- Case $1$: $f(x)=x^3-x-2$ on $[1,2]$.\n- Case $2$: $f(x)=e^{-x}-x$ on $[0,1]$.\n- Case $3$: $f(x)=(x-1)(x-2)$ on $[1,3]$.\n- Case $4$: $f(x)=\\cos(x)-x$ on $[0,1]$; interpret $x$ in radians.\n- Case $5$: $f(x)=x^3$ on $[-1,1]$.\n\nYour program must compute the five root approximations in the order above and print a single line containing the list $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is rounded to $10$ decimal places as specified. No additional text may be printed.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of numerical analysis, specifically the Intermediate Value Theorem, the method of false position (regula falsi), and the bisection method. The problem is well-posed, providing a complete and consistent set of requirements for designing a hybrid root-finding algorithm. The language is objective and the test cases are well-defined and suitable for verifying the algorithm's implementation.\n\nThe task is to design and implement a hybrid root-finding algorithm for a continuous function $f(x)$ on a closed interval $[a, b]$, where it is guaranteed that $f(a) \\cdot f(b) \\le 0$. The existence of at least one root $x^\\star \\in [a, b]$ is thus assured by the Intermediate Value Theorem. The algorithm primarily uses the method of false position but incorporates a bisection step to mitigate the slow convergence that can occur when one of the bracketing endpoints becomes stationary.\n\nThe core of the method of false position is linear interpolation. Given two points $(a, f(a))$ and $(b, f(b))$ that bracket a root, the algorithm approximates the function $f(x)$ with a straight line (a secant line) connecting these points. The next approximation of the root, $c$, is the $x$-intercept of this line. The formula for $c$ is derived from the point-slope form of a line and is given by:\n$$\nc = a - f(a) \\frac{b - a}{f(b) - f(a)}\n$$\nThis form is generally preferred for numerical stability over the alternative $c = \\frac{a f(b) - b f(a)}{f(b) - f(a)}$, as it is less susceptible to catastrophic cancellation when $a$ and $b$ are large and close in value.\n\nThe bisection method provides a robust, albeit slower, alternative. It guarantees convergence by repeatedly bisecting the interval and selecting the subinterval that maintains the sign change. The midpoint $c$ is calculated as:\n$$\nc = a + \\frac{1}{2}(b - a)\n$$\n\nThe hybrid algorithm specified in the problem combines these two methods. The iteration proceeds as follows:\n1.  **Initialization**: Given $f$, $[a, b]$, tolerances $\\tau_f  0$ and $\\tau_x  0$, and max iterations $N_{\\max}$. Evaluate $f_a = f(a)$ and $f_b = f(b)$. If $|f_a| \\le \\tau_f$ or $|f_b| \\le \\tau_f$, the corresponding endpoint is returned as the root. Two counters, `stagnant_a` and `stagnant_b`, are initialized to $0$ to track the number of consecutive iterations an endpoint remains stationary.\n\n2.  **Iteration Loop**: The loop proceeds for a maximum of $N_{\\max}$ iterations. In each iteration, a new candidate root $c$ is computed.\n\n3.  **Step Selection**:\n    -   **Stagnation Check**: If either `stagnant_a` or `stagnant_b` is greater than or equal to $2$, it indicates that one endpoint has been stationary for two consecutive steps. A bisection step is performed, and both stagnation counters are reset to $0$.\n    -   **Robustness Check**: If the denominator $|f(b) - f(a)|$ in the false position formula is close to zero (e.g., smaller than machine epsilon), a bisection step is performed to prevent numerical instability. The stagnation counters are not reset in this case.\n    -   **Default Step**: If neither of the above conditions is met, a standard false position step is performed.\n\n4.  **Candidate Evaluation and Termination**: The function is evaluated at the new candidate, $f_c = f(c)$. The algorithm terminates and returns $c$ if the function value is sufficiently close to zero, i.e., $|f_c| \\le \\tau_f$.\n\n5.  **Bracket Update and Counter Management**: If the algorithm does not terminate, the bracketing interval is updated.\n    -   If $f_a$ and $f_c$ have the same sign (i.e., $f_a \\cdot f_c  0$), the root must lie in $[c, b]$. Thus, $a$ is updated to $c$, and $f_a$ is updated to $f_c$. Since $b$ remained stationary, a`stagnant_b` is incremented, and `stagnant_a` is reset to $0$.\n    -   Otherwise, the root must lie in $[a, c]$. So, $b$ is updated to $c$, and $f_b$ is updated to $f_c$. Since $a$ remained stationary, `stagnant_a` is incremented, and `stagnant_b` is reset to $0$.\n\n6.  **Interval Width Termination**: After updating the bracket, the width of the new interval, $b-a$, is checked. If $b-a \\le \\tau_x$, the interval is considered sufficiently small. The algorithm terminates and returns the candidate $c$ from the current iteration.\n\n7.  **Maximum Iterations**: If the loop completes $N_{\\max}$ iterations without meeting any other termination criteria, it terminates and returns the last computed candidate $c$.\n\nThis structured approach ensures robust convergence by leveraging the speed of the false position method while using bisection to escape situations where it performs poorly. The specified test cases will validate the implementation of this logic, including edge cases like an initial endpoint being a root and functions that are prone to causing stagnation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hybrid_root_finder(f, a, b, tau_f, tau_x, n_max):\n    \"\"\"\n    Computes a root of a function f on an interval [a, b] using a hybrid\n    of the method of false position and the bisection method.\n\n    Args:\n        f (callable): The function for which to find a root.\n        a (float): The lower bound of the bracketing interval.\n        b (float): The upper bound of the bracketing interval.\n        tau_f (float): The absolute function value tolerance for convergence.\n        tau_x (float): The absolute interval width tolerance for convergence.\n        n_max (int): The maximum number of iterations.\n\n    Returns:\n        float: The approximated root.\n    \"\"\"\n    fa = f(a)\n    fb = f(b)\n\n    # Initial check if endpoints are roots\n    if abs(fa) = tau_f:\n        return a\n    if abs(fb) = tau_f:\n        return b\n\n    # Ensure the initial interval brackets a root\n    if fa * fb  0:\n        raise ValueError(\"The initial interval [a, b] does not bracket a root.\")\n\n    stagnant_a_counter = 0\n    stagnant_b_counter = 0\n    \n    # Initialize c to a value within the interval for safety in case of early termination\n    c = a\n\n    for _ in range(n_max):\n        # 1. Determine step type and calculate the new candidate point 'c'\n        use_bisection_stagnation = stagnant_a_counter = 2 or stagnant_b_counter = 2\n        \n        denom = fb - fa\n        use_bisection_robustness = abs(denom)  np.finfo(float).eps\n\n        if use_bisection_stagnation:\n            c = a + 0.5 * (b - a)\n            # Reset counters after a stagnation-induced bisection step\n            stagnant_a_counter = 0\n            stagnant_b_counter = 0\n        elif use_bisection_robustness:\n            c = a + 0.5 * (b - a)\n        else:\n            # Method of False Position (Regula Falsi)\n            c = a - fa * (b - a) / denom\n\n        fc = f(c)\n\n        # 2. Check for convergence based on function value at c\n        if abs(fc) = tau_f:\n            return c\n\n        # 3. Update the bracketing interval and stagnation counters\n        if fa * fc  0:\n            # Root is in [c, b], so update the left endpoint 'a'\n            a, fa = c, fc\n            stagnant_b_counter += 1\n            stagnant_a_counter = 0\n        else:\n            # Root is in [a, c], so update the right endpoint 'b'\n            b, fb = c, fc\n            stagnant_a_counter += 1\n            stagnant_b_counter = 0\n            \n        # 4. Check for convergence based on the width of the new interval\n        if b - a = tau_x:\n            return c\n\n    # Return the last computed candidate if max iterations reached\n    return c\n\ndef solve():\n    \"\"\"\n    Solves the defined test cases and prints the results in the specified format.\n    \"\"\"\n    # Define common parameters for all test cases\n    tau_x = 1e-10\n    tau_f = 1e-12\n    n_max = 100\n\n    # Define the functions for the test cases\n    def f1(x): return x**3 - x - 2\n    def f2(x): return np.exp(-x) - x\n    def f3(x): return (x - 1) * (x - 2)\n    def f4(x): return np.cos(x) - x\n    def f5(x): return x**3\n\n    # Define the test cases as a list of tuples: (function, a, b)\n    test_cases = [\n        (f1, 1.0, 2.0),\n        (f2, 0.0, 1.0),\n        (f3, 1.0, 3.0),\n        (f4, 0.0, 1.0),\n        (f5, -1.0, 1.0),\n    ]\n\n    results = []\n    for func, a_val, b_val in test_cases:\n        root = hybrid_root_finder(func, a_val, b_val, tau_f, tau_x, n_max)\n        results.append(root)\n\n    # Format the results rounded to 10 decimal places\n    formatted_results = [f'{round(r, 10):.10f}' for r in results]\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}