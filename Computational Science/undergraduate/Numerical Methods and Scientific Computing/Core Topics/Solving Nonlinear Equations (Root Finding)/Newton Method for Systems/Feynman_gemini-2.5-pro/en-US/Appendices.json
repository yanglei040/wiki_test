{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's solidify our understanding of the core mechanics of Newton's method. This first practice problem  guides you through a single, manual iteration for a 2D system. By computing the Jacobian, evaluating the system, and solving for the Newton step, you will see firsthand how the method uses a linear model to approximate a new, improved guess for the root.",
            "id": "2190496",
            "problem": "An autonomous robot is programmed to navigate to a target location $(x, y)$ on a 2D plane. The target is defined as the intersection of two signal-defined paths. The equations for these paths are given by the following system of non-linear equations:\n$$\n\\begin{cases}\n    x^2 - y + \\cos(y) - 3 = 0 \\\\\n    x + y + \\sin(x) - 2 = 0\n\\end{cases}\n$$\nThe robot's current position is $\\mathbf{x}_0 = (x_0, y_0) = (1.5, 0.5)$. The coordinate system is measured in meters. To find the target, the robot's navigation algorithm will use Newton's method for systems of non-linear equations.\n\nPerform exactly one iteration of Newton's method to find the robot's next estimated position, $\\mathbf{x}_1 = (x_1, y_1)$. Assume that all arguments of trigonometric functions are given in radians.\n\nProvide the components of the updated position vector $\\mathbf{x}_1$. Express your answer for each component in meters, rounded to three significant figures.",
            "solution": "We frame the system as $\\mathbf{F}(x,y) = \\begin{pmatrix} f_{1}(x,y) \\\\ f_{2}(x,y) \\end{pmatrix}$ with\n$$\nf_{1}(x,y) = x^{2} - y + \\cos(y) - 3,\\quad f_{2}(x,y) = x + y + \\sin(x) - 2.\n$$\nNewtonâ€™s method for systems updates $\\mathbf{x}_{1} = \\mathbf{x}_{0} - J(\\mathbf{x}_{0})^{-1}\\mathbf{F}(\\mathbf{x}_{0})$, equivalently solve $J(\\mathbf{x}_{0})\\,\\mathbf{s} = -\\mathbf{F}(\\mathbf{x}_{0})$ for $\\mathbf{s}$ and set $\\mathbf{x}_{1} = \\mathbf{x}_{0} + \\mathbf{s}$.\n\nThe Jacobian is\n$$\nJ(x,y) = \\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x} & \\frac{\\partial f_{1}}{\\partial y} \\\\\n\\frac{\\partial f_{2}}{\\partial x} & \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2x & -1 - \\sin(y) \\\\\n1 + \\cos(x) & 1\n\\end{pmatrix}.\n$$\nAt $\\mathbf{x}_{0} = (x_{0},y_{0}) = (1.5, 0.5)$ (radians for trigonometric arguments),\n$$\n\\sin(0.5) \\approx 0.4794255386,\\quad \\cos(0.5) \\approx 0.8775825620,\\quad \\sin(1.5) \\approx 0.9974949866,\\quad \\cos(1.5) \\approx 0.0707372017.\n$$\nEvaluate the function:\n$$\n\\mathbf{F}(\\mathbf{x}_{0}) = \\begin{pmatrix}\n1.5^{2} - 0.5 + \\cos(0.5) - 3 \\\\\n1.5 + 0.5 + \\sin(1.5) - 2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-0.372417438 \\\\\n0.9974949866\n\\end{pmatrix}.\n$$\nEvaluate the Jacobian:\n$$\nJ(\\mathbf{x}_{0}) = \\begin{pmatrix}\n3 & -1 - \\sin(0.5) \\\\\n1 + \\cos(1.5) & 1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 & -1.4794255386 \\\\\n1.0707372017 & 1\n\\end{pmatrix}.\n$$\nSet up and solve $J(\\mathbf{x}_{0})\\,\\mathbf{s} = -\\mathbf{F}(\\mathbf{x}_{0})$:\n$$\n\\begin{pmatrix}\n3 & -1.4794255386 \\\\\n1.0707372017 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\ns_{1} \\\\ s_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.372417438 \\\\\n-0.9974949866\n\\end{pmatrix}.\n$$\nFrom the second equation, $s_{2} = -0.9974949866 - 1.0707372017\\,s_{1}$. Substitute into the first:\n$$\n(3 + 1.4794255386 \\cdot 1.0707372017)\\,s_{1} + 1.4794255386 \\cdot 0.9974949866 = 0.372417438,\n$$\nso\n$$\ns_{1} = \\frac{0.372417438 - 1.4794255386 \\cdot 0.9974949866}{3 + 1.4794255386 \\cdot 1.0707372017} \\approx \\frac{-1.103302119}{4.5840759613} \\approx -0.240682.\n$$\nThen\n$$\ns_{2} = -0.9974949866 - 1.0707372017\\,(-0.240682) \\approx -0.739788.\n$$\nUpdate the estimate:\n$$\nx_{1} = x_{0} + s_{1} \\approx 1.5 - 0.240682 = 1.259318,\\quad\ny_{1} = y_{0} + s_{2} \\approx 0.5 - 0.739788 = -0.239788.\n$$\nRounded to three significant figures (meters): $x_{1} \\approx 1.26$, $y_{1} \\approx -0.240$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1.26 & -0.240 \\end{pmatrix}}$$"
        },
        {
            "introduction": "While powerful, Newton's method is not without its limitations, particularly when the Jacobian matrix becomes singular. This thought experiment  challenges you to analyze the method's behavior when starting from a point where the Jacobian's determinant is zero. Investigating this scenario provides crucial insight into the method's underlying linear algebra and the potential need for more advanced, robust solvers.",
            "id": "3255524",
            "problem": "Consider a twice continuously differentiable mapping $F:\\mathbb{R}^2 \\to \\mathbb{R}^2$ and the root-finding task $F(x,y)=\\mathbf{0}$. Design a system that satisfies the following two properties: \n- Its set of solutions contains the isolated root at $(0,0)$.\n- Its Jacobian determinant vanishes along the line $y=x$.\n\nOne such system is\n$$\nF(x,y)=\\begin{bmatrix}\n(y-x)^2 \\\\\nx+y\n\\end{bmatrix}.\n$$\nAnalyze the initial behavior of Newton's method for systems when the initial guess lies on the singular line $y=x$, specifically at $(a,a)$ with $a\\neq 0$. Based on first principles (linearization and the definition of the Jacobian), determine which statement best characterizes the Newton step and the immediate next iterate from such an initial guess.\n\nChoose the single best option.\n\nA. The Jacobian is singular, so the Newton step is not uniquely defined; any algorithm that requires inverting the Jacobian will fail at $(a,a)$. However, if one replaces the inverse by the least-norm solution to the linearized system, the resulting Newton step drives the iterate to $(0,0)$ in a single iteration.\n\nB. The linearized Newton system at $(a,a)$ has a unique solution $s=(-a,-a)$, therefore the Newton step is uniquely defined and converges in one iteration.\n\nC. The linearized Newton system at $(a,a)$ is inconsistent for all $a\\neq 0$, so Newton's method cannot take even a single step from $(a,a)$.\n\nD. From $(a,a)$, any solution of the linearized system yields a next iterate different from $(0,0)$, hence one-step convergence is impossible from points on the line $y=x$.\n\nE. Newton's method converges quadratically from any initial guess on the line $y=x$, even though the Jacobian determinant is zero there, because the first component $(y-x)^2$ is nonnegative.",
            "solution": "The user has requested an analysis of Newton's method for a specific nonlinear system, starting from an initial guess on a line where the Jacobian matrix is singular.\n\n### Step 1: Problem Validation\n\nI will first validate the problem statement according to the specified protocol.\n\n**1. Extract Givens:**\n- A twice continuously differentiable mapping $F:\\mathbb{R}^2 \\to \\mathbb{R}^2$.\n- The root-finding problem is $F(x,y)=\\mathbf{0}$.\n- The system has two properties:\n    1. The set of solutions contains the isolated root at $(0,0)$.\n    2. The Jacobian determinant vanishes along the line $y=x$.\n- A specific system is provided: $F(x,y)=\\begin{bmatrix} (y-x)^2 \\\\ x+y \\end{bmatrix}$.\n- The initial guess for Newton's method is $\\mathbf{x}_0 = (a,a)$ with $a \\neq 0$.\n- The task is to analyze the behavior of the first Newton step from this initial guess.\n\n**2. Validate Using Extracted Givens:**\n- **Scientific Grounding and Objectivity**: The problem is a well-defined mathematical exercise in the field of numerical analysis. It concerns the behavior of Newton's method, a standard algorithm. The language is precise and objective. The problem is scientifically sound.\n- **Consistency Check of the Provided System**:\n    - **Root at $(0,0)$**: We evaluate $F(0,0) = \\begin{bmatrix} (0-0)^2 \\\\ 0+0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\mathbf{0}$. So, $(0,0)$ is a root.\n    - **Isolated Root**: To find all roots, we set $F(x,y)=\\mathbf{0}$. This requires $(y-x)^2=0$ and $x+y=0$. The first equation implies $y=x$. Substituting this into the second equation gives $x+x=0$, which simplifies to $2x=0$, so $x=0$. If $x=0$, then $y=0$. Thus, $(0,0)$ is the unique and therefore isolated root. The first property is satisfied.\n    - **Singular Jacobian on $y=x$**: We must compute the Jacobian matrix, $J_F(x,y)$.\n      Let $F(x,y) = \\begin{bmatrix} f_1(x,y) \\\\ f_2(x,y) \\end{bmatrix} = \\begin{bmatrix} (y-x)^2 \\\\ x+y \\end{bmatrix}$.\n      The partial derivatives are:\n      $\\frac{\\partial f_1}{\\partial x} = 2(y-x)(-1) = -2(y-x)$\n      $\\frac{\\partial f_1}{\\partial y} = 2(y-x)(1) = 2(y-x)$\n      $\\frac{\\partial f_2}{\\partial x} = 1$\n      $\\frac{\\partial f_2}{\\partial y} = 1$\n      The Jacobian matrix is $J_F(x,y) = \\begin{bmatrix} -2(y-x) & 2(y-x) \\\\ 1 & 1 \\end{bmatrix}$.\n      The determinant is $\\det(J_F(x,y)) = (-2(y-x))(1) - (2(y-x))(1) = -4(y-x)$.\n      Along the line $y=x$, the determinant is $\\det(J_F(x,x)) = -4(x-x) = 0$. The second property is satisfied.\n- **Well-Posedness**: The problem is well-posed. It asks for the analysis of a specific, well-defined iteration of a standard numerical method on a given function from a given starting point. All necessary information is provided.\n\n**3. Verdict and Action:**\nThe problem statement is valid, self-contained, and mathematically sound. I will proceed with the solution.\n\n### Step 2: Solution Derivation\n\nThe core of Newton's method is to find a step $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ by solving the linear system derived from the first-order Taylor expansion of $F$ around the current iterate $\\mathbf{x}_k$:\n$$J_F(\\mathbf{x}_k) \\mathbf{s}_k = -F(\\mathbf{x}_k)$$\nThe next iterate is then given by $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{s}_k$.\n\nWe analyze the first step ($k=0$) starting from the initial guess $\\mathbf{x}_0 = \\begin{bmatrix} a \\\\ a \\end{bmatrix}$, where $a \\neq 0$.\n\n**1. Evaluate $F(\\mathbf{x}_0)$ and $J_F(\\mathbf{x}_0)$:**\n- **Function value:**\n  $F(\\mathbf{x}_0) = F(a,a) = \\begin{bmatrix} (a-a)^2 \\\\ a+a \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2a \\end{bmatrix}$.\n- **Jacobian matrix:**\n  $J_F(\\mathbf{x}_0) = J_F(a,a) = \\begin{bmatrix} -2(a-a) & 2(a-a) \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 \\\\ 1 & 1 \\end{bmatrix}$.\n  As established, this matrix is singular since its determinant is $0$.\n\n**2. Formulate and Analyze the Linearized Newton System:**\nThe system for the Newton step $\\mathbf{s}_0 = \\begin{bmatrix} s_x \\\\ s_y \\end{bmatrix}$ is $J_F(\\mathbf{x}_0) \\mathbf{s}_0 = -F(\\mathbf{x}_0)$:\n$$\n\\begin{bmatrix} 0 & 0 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} s_x \\\\ s_y \\end{bmatrix} = - \\begin{bmatrix} 0 \\\\ 2a \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -2a \\end{bmatrix}\n$$\nThis matrix equation corresponds to the following system of scalar equations:\n1. $0 \\cdot s_x + 0 \\cdot s_y = 0$\n2. $1 \\cdot s_x + 1 \\cdot s_y = -2a$\n\nThe first equation, $0=0$, is redundant. The system effectively reduces to the single equation $s_x + s_y = -2a$.\nA linear system $A\\mathbf{x} = \\mathbf{b}$ is consistent if $\\mathbf{b}$ is in the column space of $A$. Here, the column space of $J_F(a,a)$ is the span of $\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. The vector $-F(a,a) = \\begin{bmatrix} 0 \\\\ -2a \\end{bmatrix}$ is indeed in this column space (with a scaling factor of $-2a$). Therefore, the system is **consistent**.\n\nSince the Jacobian matrix is singular, the solution is **not unique**. The set of all possible Newton steps $\\mathbf{s}_0$ is the line in the $(s_x, s_y)$-plane defined by $s_x + s_y = -2a$.\n\n**3. Analyze the Consequence for the Newton Step:**\nBecause the solution for $\\mathbf{s}_0$ is not unique, any algorithm that relies on computing a unique matrix inverse $J_F^{-1}$ will fail. However, robust linear solvers do not compute the inverse directly. For a consistent but underdetermined system, they find a particular solution. A standard choice is the unique **least-norm solution**, which is the solution $\\mathbf{s}_0$ that minimizes the Euclidean norm $\\| \\mathbf{s}_0 \\|_2 = \\sqrt{s_x^2 + s_y^2}$.\n\nTo find this least-norm solution, we minimize $s_x^2 + s_y^2$ subject to the constraint $s_x + s_y = -2a$. Substituting $s_y = -2a - s_x$ into the expression to minimize:\n$g(s_x) = s_x^2 + (-2a - s_x)^2 = s_x^2 + 4a^2 + 4as_x + s_x^2 = 2s_x^2 + 4as_x + 4a^2$.\nTo find the minimum, we set the derivative with respect to $s_x$ to zero:\n$g'(s_x) = 4s_x + 4a = 0 \\implies s_x = -a$.\nFrom the constraint, $s_y = -2a - s_x = -2a - (-a) = -a$.\nThus, the unique least-norm solution for the Newton step is $\\mathbf{s}_{LN} = \\begin{bmatrix} -a \\\\ -a \\end{bmatrix}$.\n\n**4. Determine the Next Iterate:**\nIf this specific least-norm step is taken, the next iterate $\\mathbf{x}_1$ is:\n$$\\mathbf{x}_1 = \\mathbf{x}_0 + \\mathbf{s}_{LN} = \\begin{bmatrix} a \\\\ a \\end{bmatrix} + \\begin{bmatrix} -a \\\\ -a \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$\nThis means that if the algorithm selects the least-norm solution, Newton's method converges to the root $(0,0)$ in a single iteration.\n\n### Step 3: Option-by-Option Analysis\n\n**A. The Jacobian is singular, so the Newton step is not uniquely defined; any algorithm that requires inverting the Jacobian will fail at $(a,a)$. However, if one replaces the inverse by the least-norm solution to the linearized system, the resulting Newton step drives the iterate to $(0,0)$ in a single iteration.**\n- **Verdict: Correct.** This statement accurately summarizes our findings. The Jacobian at $(a,a)$ is singular. The Newton system is consistent but has infinitely many solutions, so the step is not unique. A naive implementation seeking $J_F^{-1}$ would fail. The specific least-norm solution is $\\mathbf{s}_0 = (-a, -a)$, which makes the next iterate $\\mathbf{x}_1 = (0,0)$.\n\n**B. The linearized Newton system at $(a,a)$ has a unique solution $s=(-a,-a)$, therefore the Newton step is uniquely defined and converges in one iteration.**\n- **Verdict: Incorrect.** The central claim that the system has a unique solution is false. The singularity of the Jacobian implies that if a solution exists, there must be infinitely many.\n\n**C. The linearized Newton system at $(a,a)$ is inconsistent for all $a\\neq 0$, so Newton's method cannot take even a single step from $(a,a)$.**\n- **Verdict: Incorrect.** We explicitly showed that the system is consistent because the right-hand side vector $-F(a,a)$ lies in the column space of the Jacobian matrix $J_F(a,a)$. A step can be taken.\n\n**D. From $(a,a)$, any solution of the linearized system yields a next iterate different from $(0,0)$, hence one-step convergence is impossible from points on the line $y=x$.**\n- **Verdict: Incorrect.** The quantifier \"any\" makes this statement false. While it is true that most solutions lead to a next iterate different from $(0,0)$, we found a specific solution, $\\mathbf{s}_0 = (-a,-a)$, which leads to one-step convergence. This single counterexample invalidates the statement.\n\n**E. Newton's method converges quadratically from any initial guess on the line $y=x$, even though the Jacobian determinant is zero there, because the first component $(y-x)^2$ is nonnegative.**\n- **Verdict: Incorrect.** This statement makes two flawed claims. First, quadratic convergence is not guaranteed; in fact, a singular Jacobian at the root typically leads to linear convergence. Our analysis for a generic step from $(a,a)$ would show that subsequent steps exhibit linear, not quadratic, convergence if one-step convergence does not occur. Second, the reason provided (\"because the first component... is nonnegative\") is a non-sequitur and has no bearing on the rate of convergence in this context.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Theory and simple calculations provide a foundation, but real-world problems often require more robust algorithms. This coding exercise  demonstrates the power of a \"damped\" Newton's method, which uses a backtracking line search to ensure convergence even from poor initial guesses where the standard method fails. By implementing and comparing both approaches, you will gain a practical appreciation for the globalization strategies that make Newton's method a reliable tool in scientific computing.",
            "id": "3255431",
            "problem": "You must write a complete, runnable program that implements a damped Newton's method using a backtracking line search for solving nonlinear systems of equations. The goal is to demonstrate how damping via line search can achieve convergence for a system where the standard full-step Newton's method fails to converge from a poor initial guess. The fundamental base is the following: given a continuously differentiable nonlinear mapping $F:\\mathbb{R}^n \\to \\mathbb{R}^n$, a root is a vector $x^\\star\\in\\mathbb{R}^n$ such that $F(x^\\star)=0$. The first-order Taylor expansion gives $F(x+p)\\approx F(x)+J(x)p$, where $J(x)$ denotes the Jacobian matrix of $F$ at $x$. Use this approximation to determine a search direction at each iteration, and then apply a backtracking line search that ensures a sufficient decrease of the merit function $\\phi(x)=\\frac{1}{2}\\lVert F(x)\\rVert_2^2$. The sufficient decrease criterion must enforce a bound of the form $\\phi(x+\\alpha p)\\le \\phi(x)+\\sigma\\alpha\\,\\nabla\\phi(x)\\cdot p$ for a fixed constant $\\sigma\\in(0,1)$ and step size $\\alpha\\in(0,1]$, where $\\nabla\\phi(x)$ is the gradient of $\\phi$.\n\nYour program must implement two solvers:\n- A standard full-step Newton's method that always takes the step size $\\alpha=1$.\n- A damped Newton's method with backtracking line search that reduces the step size $\\alpha$ multiplicatively until the sufficient decrease condition is met.\n\nYou must solve the following systems with the specified initial points, tolerances, and limits. For each system, use analytically correct mappings $F(x)$ and Jacobians $J(x)$:\n\n1. Rosenbrock-gradient system. Define the Rosenbrock function $f:\\mathbb{R}^2 \\to \\mathbb{R}$ by $f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2$ and let $F_R(x)=\\nabla f(x)$, explicitly\n$$\nF_R(x)=\\begin{pmatrix}\n2(x_1-1)-400x_1(x_2-x_1^2)\\\\\n200(x_2-x_1^2)\n\\end{pmatrix}.\n$$\nIts Jacobian $J_R(x)$ is the Hessian of $f$,\n$$\nJ_R(x)=\\begin{pmatrix}\n2 - 400(x_2 - x_1^2) + 800 x_1^2 & -400 x_1\\\\\n-400 x_1 & 200\n\\end{pmatrix}.\n$$\nUse initial point $x^{(0)}=(-1.2,\\,1.0)$, tolerance $\\varepsilon=10^{-10}$ for the stopping criterion $\\lVert F(x^{(k)})\\rVert_2\\le\\varepsilon$, and a maximum of $N_{\\max}=50$ iterations. Use line search parameters $\\sigma=10^{-4}$, reduction factor $\\beta=\\frac{1}{2}$, and a minimum step size $\\alpha_{\\min}=10^{-12}$. This case is intended to demonstrate a scenario where the standard full-step Newton method fails to converge from a poor initial guess while the damped method with line search successfully converges.\n\n2. Rosenbrock-gradient system near the solution. Use the same $F_R(x)$ and $J_R(x)$ as above, with initial point $x^{(0)}=(1.2,\\,1.2)$, tolerance $\\varepsilon=10^{-10}$, $N_{\\max}=50$, and line search parameters $\\sigma=10^{-4}$, $\\beta=\\frac{1}{2}$, $\\alpha_{\\min}=10^{-12}$. This case is intended to demonstrate a scenario where both methods converge when starting sufficiently close to the solution.\n\n3. Near-singular Jacobian at the start. Define\n$$\nF_S(x)=\\begin{pmatrix}\nx_1 x_2\\\\\nx_1^2 - x_2\n\\end{pmatrix},\\quad\nJ_S(x)=\\begin{pmatrix}\nx_2 & x_1\\\\\n2x_1 & -1\n\\end{pmatrix}.\n$$\nNote that $J_S(0,0)$ is singular. Use initial point $x^{(0)}=(10^{-4},\\,10^{-4})$, tolerance $\\varepsilon=10^{-8}$, $N_{\\max}=100$, and line search parameters $\\sigma=10^{-4}$, $\\beta=\\frac{1}{2}$, $\\alpha_{\\min}=10^{-12}$. This case is intended to test robustness of the implementation near a singular Jacobian.\n\nImplementation requirements:\n- At each iteration, compute the search direction $p$ by solving the linear system $J(x)p=-F(x)$ using a numerically stable method. If the Jacobian is singular or ill-conditioned, compute a least-squares solution to determine $p$ that minimizes $\\lVert J(x)p+F(x)\\rVert_2$.\n- For the damped method, perform a backtracking line search on $\\alpha$ until the sufficient decrease condition is satisfied or $\\alpha$ falls below $\\alpha_{\\min}$.\n- Stop when $\\lVert F(x)\\rVert_2\\le\\varepsilon$ or when $N_{\\max}$ iterations are reached.\n\nTest suite and required outputs:\n- Case A (Rosenbrock-gradient with $x^{(0)}=(-1.2,\\,1.0)$, $\\varepsilon=10^{-10}$, $N_{\\max}=50$): Output a boolean that is `true` if and only if the damped method converges within $N_{\\max}$ iterations and the full-step method fails to converge within $N_{\\max}$ iterations under the same tolerance.\n- Case B (Rosenbrock-gradient with $x^{(0)}=(1.2,\\,1.2)$, $\\varepsilon=10^{-10}$, $N_{\\max}=50$): Output a boolean that is `true` if and only if both methods converge within $N_{\\max}$ iterations under the same tolerance.\n- Case C (Near-singular system with $x^{(0)}=(10^{-4},\\,10^{-4})$, $\\varepsilon=10^{-8}$, $N_{\\max}=100$): Output a boolean that is `true` if and only if the damped method converges within $N_{\\max}$ iterations under the specified tolerance and its final merit value $\\phi(x)$ is less than or equal to that of the full-step method at termination.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC]\") where each result is the boolean defined above for its corresponding case.",
            "solution": "The user requests the implementation and comparison of two variants of Newton's method for solving systems of nonlinear equations, $F(x) = 0$. The first is the standard full-step Newton's method, and the second is a damped version employing a backtracking line search to ensure convergence from a wider range of initial guesses.\n\n### Problem Formulation\n\nA nonlinear system of equations is given by a function $F:\\mathbb{R}^n \\to \\mathbb{R}^n$. We seek a root $x^\\star$ such that $F(x^\\star) = 0$. Newton's method is an iterative process that, given an iterate $x^{(k)}$, finds the next iterate $x^{(k+1)}$ by approximating $F$ with its first-order Taylor expansion around $x^{(k)}$:\n$$\nF(x^{(k)} + p) \\approx F(x^{(k)}) + J(x^{(k)})p\n$$\nwhere $J(x^{(k)})$ is the Jacobian matrix of $F$ at $x^{(k)}$. To find a root of the approximation, we set the right-hand side to zero and solve for the step $p$, called the Newton step:\n$$\nJ(x^{(k)})p^{(k)} = -F(x^{(k)})\n$$\nThis is a system of linear equations for the search direction $p^{(k)}$.\n\n### Standard vs. Damped Newton's Method\n\n**1. Standard Full-Step Newton's Method:**\nThe next iterate is always computed by taking the full Newton step:\n$$\nx^{(k+1)} = x^{(k)} + p^{(k)}\n$$\nThis is equivalent to setting the step size $\\alpha=1$. This method exhibits local quadratic convergence but may fail to converge (i.e., diverge or oscillate) if the initial guess $x^{(0)}$ is not sufficiently close to the solution $x^\\star$.\n\n**2. Damped Newton's Method with Backtracking Line Search:**\nTo globalize the convergence, a step size $\\alpha \\in (0, 1]$ is introduced:\n$$\nx^{(k+1)} = x^{(k)} + \\alpha p^{(k)}\n$$\nThe step size $\\alpha$ is chosen to ensure a sufficient decrease in a merit function, which measures how far we are from a solution. A common choice for the merit function is $\\phi(x) = \\frac{1}{2}\\lVert F(x)\\rVert_2^2$. The goal is to find an $\\alpha$ that satisfies the Armijo-Goldstein sufficient decrease condition:\n$$\n\\phi(x^{(k)} + \\alpha p^{(k)}) \\le \\phi(x^{(k)}) + \\sigma \\alpha \\nabla\\phi(x^{(k)})^T p^{(k)}\n$$\nfor a small constant $\\sigma \\in (0, 1)$, typically $\\sigma = 10^{-4}$.\n\nThe gradient of the merit function is $\\nabla\\phi(x) = J(x)^T F(x)$. The directional derivative in the direction $p^{(k)}$ is:\n$$\n\\nabla\\phi(x^{(k)})^T p^{(k)} = (J(x^{(k)})^T F(x^{(k)}))^T p^{(k)} = F(x^{(k)})^T J(x^{(k)}) p^{(k)}\n$$\nSubstituting $J(x^{(k)})p^{(k)} = -F(x^{(k)})$, we get:\n$$\n\\nabla\\phi(x^{(k)})^T p^{(k)} = F(x^{(k)})^T (-F(x^{(k)})) = -\\lVert F(x^{(k)}) \\rVert_2^2\n$$\nThis shows that the Newton direction $p^{(k)}$ is a descent direction for the merit function $\\phi(x)$, provided $J(x^{(k)})$ is non-singular. The sufficient decrease condition simplifies to:\n$$\n\\phi(x^{(k)} + \\alpha p^{(k)}) \\le \\phi(x^{(k)}) - \\sigma \\alpha \\lVert F(x^{(k)}) \\rVert_2^2\n$$\nThe backtracking line search algorithm starts with $\\alpha=1$ and successively reduces it by a factor $\\beta \\in (0,1)$ (e.g., $\\beta = \\frac{1}{2}$) until this condition is met or $\\alpha$ becomes smaller than a prescribed minimum $\\alpha_{\\min}$.\n\n### Algorithmic Implementation\n\nA robust solver is developed to encapsulate both methods. It accepts the system's function $F$, its Jacobian $J$, an initial guess $x^{(0)}$, and control parameters.\n\n**Core Steps within the Solver:**\n1.  Initialize the iteration counter $k=0$ and the solution vector $x = x^{(0)}$.\n2.  Begin the main loop, which continues as long as the iteration count $k  N_{\\max}$.\n3.  At each iteration, evaluate $F(x)$ and check for convergence: if $\\lVert F(x) \\rVert_2 \\le \\varepsilon$, the process terminates successfully.\n4.  Evaluate the Jacobian $J(x)$.\n5.  Solve the linear system $J(x) p = -F(x)$ for the search direction $p$. To handle cases where $J(x)$ is singular or ill-conditioned, we use a least-squares solver, which finds the $p$ that minimizes $\\lVert J(x)p + F(x) \\rVert_2$.\n6.  If damping is enabled, perform a backtracking line search:\n    a. Start with $\\alpha = 1$.\n    b. Repeatedly check the sufficient decrease condition.\n    c. If the condition fails, update $\\alpha \\leftarrow \\beta\\alpha$.\n    d. If $\\alpha$ falls below $\\alpha_{\\min}$, the search is considered stalled, and the iteration halts.\n7.  If damping is disabled, simply set $\\alpha=1$.\n8.  Update the solution: $x \\leftarrow x + \\alpha p$.\n9.  Increment the iteration counter $k \\leftarrow k+1$.\n10. If the loop completes without convergence, the method has failed. The final state and a failure status are returned.\n\n### Test Cases Analysis\n\n**Case A  B: Rosenbrock-gradient System**\nThis system is derived from the gradient of the classic Rosenbrock function, a difficult non-convex optimization benchmark. The solution is at $x^\\star = (1, 1)$, where $F_R(x^\\star) = 0$.\n-   **Case A (poor initial guess):** Starting from $x^{(0)}=(-1.2,\\,1.0)$, the initial iterate is far from the solution. The local quadratic approximation of Newton's method is inaccurate, causing the full-step method to take large, non-productive steps, leading to divergence or oscillation. The damped method, by enforcing a decrease in the merit function, takes smaller, more cautious steps, successfully navigating the complex landscape towards the solution. This demonstrates the superior global convergence properties of the damped method.\n-   **Case B (good initial guess):** Starting from $x^{(0)}=(1.2,\\,1.2)$, the iterate is in the basin of attraction of the solution. Here, the local quadratic model is a good approximation. The full step $\\alpha=1$ satisfies the sufficient decrease condition, so the damped method behaves identically to the full-step method. Both are expected to converge rapidly due to Newton's quadratic convergence rate near a solution.\n\n**Case C: Near-singular Jacobian System**\nThe system has a solution at $x^\\star = (0, 0)$, where the Jacobian $J_S(0,0)$ is singular. Starting at $x^{(0)}=(10^{-4},\\,10^{-4})$, the initial Jacobian is nearly singular. This tests the robustness of the linear solver. The use of a least-squares solver for the Newton step is crucial here. The condition for this test case is that the damped method must converge and its final merit function value, $\\phi(x_{final})$, must be less than or equal to that of the full-step method. As the damped method is guaranteed to not worsen the merit function value at each step (unlike the full-step method), it is expected to achieve a final state that is at least as good as, if not better than, the full-step method's final state, thus satisfying the test condition.\n\nThe program implements these functions, solvers, and test cases to produce the required boolean results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests standard and damped Newton's methods for solving\n    systems of nonlinear equations.\n    \"\"\"\n\n    # --- System Definitions ---\n\n    def F_R(x):\n        \"\"\"Rosenbrock-gradient system function F(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        f1 = 2 * (x1 - 1) - 400 * x1 * (x2 - x1**2)\n        f2 = 200 * (x2 - x1**2)\n        return np.array([f1, f2], dtype=float)\n\n    def J_R(x):\n        \"\"\"Jacobian of the Rosenbrock-gradient system J(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        j11 = 2 - 400 * (x2 - x1**2) + 800 * x1**2\n        j12 = -400 * x1\n        j21 = -400 * x1\n        j22 = 200\n        return np.array([[j11, j12], [j21, j22]], dtype=float)\n\n    def F_S(x):\n        \"\"\"Near-singular system function F(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        f1 = x1 * x2\n        f2 = x1**2 - x2\n        return np.array([f1, f2], dtype=float)\n\n    def J_S(x):\n        \"\"\"Jacobian of the near-singular system J(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        j11 = x2\n        j12 = x1\n        j21 = 2 * x1\n        j22 = -1.0\n        return np.array([[j11, j12], [j21, j22]], dtype=float)\n\n    # --- Newton Solver Implementation ---\n\n    def newton_solver(F, J, x0, tol, max_iter, use_damping, sigma, beta, alpha_min):\n        \"\"\"\n        Solves a nonlinear system F(x)=0 using Newton's method.\n\n        Returns:\n            x_final (np.ndarray): The final solution vector.\n            Fx_final (np.ndarray): The final residual vector F(x_final).\n            converged (bool): True if the method converged to the tolerance.\n        \"\"\"\n        x = np.copy(x0).astype(float)\n        k = 0\n\n        while k  max_iter:\n            Fx = F(x)\n            norm_Fx = np.linalg.norm(Fx)\n\n            if norm_Fx = tol:\n                return x, Fx, True\n\n            Jx = J(x)\n            \n            # Solve the linear system using least-squares for robustness\n            try:\n                p = np.linalg.lstsq(Jx, -Fx, rcond=None)[0]\n            except np.linalg.LinAlgError:\n                # This may happen for extremely ill-conditioned matrices, though lstsq is robust.\n                return x, Fx, False\n\n            alpha = 1.0\n            if use_damping:\n                phi_x = 0.5 * norm_Fx**2\n                grad_phi_p = -norm_Fx**2\n\n                line_search_success = False\n                while alpha  alpha_min:\n                    x_new = x + alpha * p\n                    phi_new = 0.5 * np.linalg.norm(F(x_new))**2\n                    \n                    if phi_new = phi_x + sigma * alpha * grad_phi_p:\n                        line_search_success = True\n                        break\n                    \n                    alpha *= beta\n                \n                if not line_search_success:\n                    # Unable to find a step that satisfies the condition, indicating stalling.\n                    return x, Fx, False\n            \n            x = x + alpha * p\n            k += 1\n\n        # Check for convergence after the final iteration\n        final_Fx = F(x)\n        converged = np.linalg.norm(final_Fx) = tol\n        return x, final_Fx, converged\n\n    # --- Test Cases ---\n    \n    results = []\n    \n    # Common line search parameters\n    ls_params = {'sigma': 1e-4, 'beta': 0.5, 'alpha_min': 1e-12}\n\n    # Case A: Rosenbrock-gradient with poor initial guess\n    x0_A = np.array([-1.2, 1.0])\n    tol_A = 1e-10\n    max_iter_A = 50\n    _, _, converged_damped_A = newton_solver(F_R, J_R, x0_A, tol_A, max_iter_A, True, **ls_params)\n    _, _, converged_full_A = newton_solver(F_R, J_R, x0_A, tol_A, max_iter_A, False, **ls_params)\n    resultA = converged_damped_A and not converged_full_A\n    results.append(resultA)\n\n    # Case B: Rosenbrock-gradient with good initial guess\n    x0_B = np.array([1.2, 1.2])\n    tol_B = 1e-10\n    max_iter_B = 50\n    _, _, converged_damped_B = newton_solver(F_R, J_R, x0_B, tol_B, max_iter_B, True, **ls_params)\n    _, _, converged_full_B = newton_solver(F_R, J_R, x0_B, tol_B, max_iter_B, False, **ls_params)\n    resultB = converged_damped_B and converged_full_B\n    results.append(resultB)\n\n    # Case C: Near-singular Jacobian at the start\n    x0_C = np.array([1e-4, 1e-4])\n    tol_C = 1e-8\n    max_iter_C = 100\n    _, Fx_damped_C, converged_damped_C = newton_solver(F_S, J_S, x0_C, tol_C, max_iter_C, True, **ls_params)\n    _, Fx_full_C, _ = newton_solver(F_S, J_S, x0_C, tol_C, max_iter_C, False, **ls_params)\n    \n    phi_damped_C = 0.5 * np.linalg.norm(Fx_damped_C)**2\n    phi_full_C = 0.5 * np.linalg.norm(Fx_full_C)**2\n    \n    resultC = converged_damped_C and (phi_damped_C = phi_full_C)\n    results.append(resultC)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}