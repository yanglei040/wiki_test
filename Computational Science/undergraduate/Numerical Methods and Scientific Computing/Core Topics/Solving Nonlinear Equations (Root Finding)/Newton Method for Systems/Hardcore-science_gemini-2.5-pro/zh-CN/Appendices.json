{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的巩固方式就是亲手实践。这个练习将引导你完成系统牛顿法的一次完整迭代，这是一个基础但至关重要的计算过程。通过解决一个机器人导航到目标位置的非线性方程组，你将练习计算雅可比矩阵、建立并求解用于更新步骤的线性系统，从而对方法的核心机制有更深刻的理解。",
            "id": "2190496",
            "problem": "一个自主机器人被编程用于在一个二维平面上导航到目标位置 $(x, y)$。目标被定义为两条由信号定义的路径的交点。这些路径的方程由以下非线性方程组给出：\n$$\n\\begin{cases}\n    x^2 - y + \\cos(y) - 3 = 0 \\\\\n    x + y + \\sin(x) - 2 = 0\n\\end{cases}\n$$\n机器人的当前位置是 $\\mathbf{x}_0 = (x_0, y_0) = (1.5, 0.5)$。坐标系以米为单位。为了找到目标，机器人的导航算法将使用用于非线性方程组的牛顿法。\n\n执行恰好一次牛顿法迭代，以找到机器人的下一个估计位置 $\\mathbf{x}_1 = (x_1, y_1)$。假设三角函数的所有参数均以弧度为单位。\n\n提供更新后的位置向量 $\\mathbf{x}_1$ 的分量。将每个分量的答案以米为单位表示，并四舍五入到三位有效数字。",
            "solution": "我们将该方程组表示为 $\\mathbf{F}(x,y) = \\begin{pmatrix} f_{1}(x,y) \\\\ f_{2}(x,y) \\end{pmatrix}$，其中\n$$\nf_{1}(x,y) = x^{2} - y + \\cos(y) - 3,\\quad f_{2}(x,y) = x + y + \\sin(x) - 2.\n$$\n用于方程组的牛顿法更新公式为 $\\mathbf{x}_{1} = \\mathbf{x}_{0} - J(\\mathbf{x}_{0})^{-1}\\mathbf{F}(\\mathbf{x}_{0})$，等价于求解 $J(\\mathbf{x}_{0})\\,\\mathbf{s} = -\\mathbf{F}(\\mathbf{x}_{0})$ 得到 $\\mathbf{s}$，然后令 $\\mathbf{x}_{1} = \\mathbf{x}_{0} + \\mathbf{s}$。\n\n雅可比矩阵是\n$$\nJ(x,y) = \\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x}  \\frac{\\partial f_{1}}{\\partial y} \\\\\n\\frac{\\partial f_{2}}{\\partial x}  \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2x  -1 - \\sin(y) \\\\\n1 + \\cos(x)  1\n\\end{pmatrix}.\n$$\n在 $\\mathbf{x}_{0} = (x_{0},y_{0}) = (1.5, 0.5)$ 处（三角函数参数以弧度为单位），\n$$\n\\sin(0.5) \\approx 0.4794255386,\\quad \\cos(0.5) \\approx 0.8775825620,\\quad \\sin(1.5) \\approx 0.9974949866,\\quad \\cos(1.5) \\approx 0.0707372017.\n$$\n计算函数值：\n$$\n\\mathbf{F}(\\mathbf{x}_{0}) = \\begin{pmatrix}\n1.5^{2} - 0.5 + \\cos(0.5) - 3 \\\\\n1.5 + 0.5 + \\sin(1.5) - 2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-0.372417438 \\\\\n0.9974949866\n\\end{pmatrix}.\n$$\n计算雅可比矩阵：\n$$\nJ(\\mathbf{x}_{0}) = \\begin{pmatrix}\n3  -1 - \\sin(0.5) \\\\\n1 + \\cos(1.5)  1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3  -1.4794255386 \\\\\n1.0707372017  1\n\\end{pmatrix}.\n$$\n建立并求解 $J(\\mathbf{x}_{0})\\,\\mathbf{s} = -\\mathbf{F}(\\mathbf{x}_{0})$：\n$$\n\\begin{pmatrix}\n3  -1.4794255386 \\\\\n1.0707372017  1\n\\end{pmatrix}\n\\begin{pmatrix}\ns_{1} \\\\ s_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.372417438 \\\\\n-0.9974949866\n\\end{pmatrix}.\n$$\n根据第二个方程，有 $s_{2} = -0.9974949866 - 1.0707372017\\,s_{1}$。将其代入第一个方程：\n$$\n(3 + 1.4794255386 \\cdot 1.0707372017)\\,s_{1} + 1.4794255386 \\cdot 0.9974949866 = 0.372417438,\n$$\n因此\n$$\ns_{1} = \\frac{0.372417438 - 1.4794255386 \\cdot 0.9974949866}{3 + 1.4794255386 \\cdot 1.0707372017} \\approx \\frac{-1.103302119}{4.5840759613} \\approx -0.240682.\n$$\n那么\n$$\ns_{2} = -0.9974949866 - 1.0707372017\\,(-0.240682) \\approx -0.739788.\n$$\n更新估计值：\n$$\nx_{1} = x_{0} + s_{1} \\approx 1.5 - 0.240682 = 1.259318,\\quad\ny_{1} = y_{0} + s_{2} \\approx 0.5 - 0.739788 = -0.239788.\n$$\n四舍五入到三位有效数字（米）：$x_{1} \\approx 1.26$，$y_{1} \\approx -0.240$。",
            "answer": "$$\\boxed{\\begin{pmatrix} 1.26  -0.240 \\end{pmatrix}}$$"
        },
        {
            "introduction": "标准的牛顿法虽然在理想情况下收敛很快，但当年初值选取不佳时，它可能会失效。本练习通过一个经典的挑战性问题（Rosenbrock 函数），让你实现并对比标准牛頓法和带回溯线搜索的阻尼牛頓法。通过亲手编程实现，你将直观地看到全局化策略如何通过控制步长来保证算法的收敛性，这对于解决现实世界中的复杂问题至关重要。",
            "id": "3255431",
            "problem": "您必须编写一个完整、可运行的程序，该程序使用回溯线搜索来实现求解非线性方程组的阻尼牛顿法。目标是展示在线搜索阻尼的帮助下，该方法如何在一个标准全步长牛顿法因初始猜测不佳而无法收敛的系统上实现收敛。其基本原理如下：给定一个连续可微的非线性映射 $F:\\mathbb{R}^n\\to\\mathbb{R}^n$，其根是一个向量 $x^\\star\\in\\mathbb{R}^n$，满足 $F(x^\\star)=0$。一阶泰勒展开给出 $F(x+p)\\approx F(x)+J(x)p$，其中 $J(x)$ 表示 $F$ 在 $x$ 处的雅可比矩阵。在每次迭代中，使用此近似来确定搜索方向，然后应用回溯线搜索，以确保价值函数 $\\phi(x)=\\frac{1}{2}\\lVert F(x)\\rVert_2^2$ 的充分下降。充分下降准则必须强制执行 $\\phi(x+\\alpha p)\\le \\phi(x)+\\sigma\\alpha\\,\\nabla\\phi(x)\\cdot p$ 形式的界限，其中 $\\sigma\\in(0,1)$ 是一个固定常数，$\\alpha\\in(0,1]$ 是步长，$\\nabla\\phi(x)$ 是 $\\phi$ 的梯度。\n\n您的程序必须实现两个求解器：\n- 一个标准的、始终采用步长 $\\alpha=1$ 的全步长牛顿法。\n- 一个带回溯线搜索的阻尼牛顿法，该方法通过乘法减小步长 $\\alpha$，直到满足充分下降条件。\n\n您必须使用指定的初始点、容差和限制来求解以下系统。对于每个系统，请使用解析上正确的映射 $F(x)$ 和雅可比矩阵 $J(x)$：\n\n1. Rosenbrock 梯度系统。定义 Rosenbrock 函数 $f:\\mathbb{R}^2\\to\\mathbb{R}$ 为 $f(x_1,x_2)=(1-x_1)^2+100(x_2-x_1^2)^2$ 并令 $F_R(x)=\\nabla f(x)$，具体为\n$$\nF_R(x)=\\begin{pmatrix}\n2(x_1-1)-400x_1(x_2-x_1^2)\\\\\n200(x_2-x_1^2)\n\\end{pmatrix}.\n$$\n其雅可比矩阵 $J_R(x)$ 是 $f$ 的海森矩阵，\n$$\nJ_R(x)=\\begin{pmatrix}\n2 - 400(x_2 - x_1^2) + 800 x_1^2  -400 x_1\\\\\n-400 x_1  200\n\\end{pmatrix}.\n$$\n使用初始点 $x^{(0)}=(-1.2,\\,1.0)$，停止准则 $\\lVert F(x^{(k)})\\rVert_2\\le\\varepsilon$ 的容差为 $\\varepsilon=10^{-10}$，最大迭代次数为 $N_{\\max}=50$。使用线搜索参数 $\\sigma=10^{-4}$，缩减因子 $\\beta=\\tfrac{1}{2}$，以及最小步长 $\\alpha_{\\min}=10^{-12}$。此案例旨在展示一个场景：标准全步长牛顿法从一个较差的初始猜测出发未能收敛，而带线搜索的阻尼方法成功收敛。\n\n2. 靠近解的 Rosenbrock 梯度系统。使用与上述相同的 $F_R(x)$ 和 $J_R(x)$，初始点为 $x^{(0)}=(1.2,\\,1.2)$，容差 $\\varepsilon=10^{-10}$，$N_{\\max}=50$，线搜索参数为 $\\sigma=10^{-4}$、$\\beta=\\tfrac{1}{2}$、$\\alpha_{\\min}=10^{-12}$。此案例旨在展示一个场景：当从足够靠近解的位置开始时，两种方法都能收敛。\n\n3. 初始时雅可比矩阵近奇异。定义\n$$\nF_S(x)=\\begin{pmatrix}\nx_1 x_2\\\\\nx_1^2 - x_2\n\\end{pmatrix},\\quad\nJ_S(x)=\\begin{pmatrix}\nx_2  x_1\\\\\n2x_1  -1\n\\end{pmatrix}.\n$$\n注意 $J_S(0,0)$ 是奇异的。使用初始点 $x^{(0)}=(10^{-4},\\,10^{-4})$，容差 $\\varepsilon=10^{-8}$，$N_{\\max}=100$，线搜索参数为 $\\sigma=10^{-4}$、$\\beta=\\tfrac{1}{2}$、$\\alpha_{\\min}=10^{-12}$。此案例旨在测试实现在近奇异雅可比矩阵时的鲁棒性。\n\n实现要求：\n- 在每次迭代中，通过使用数值稳定方法求解线性系统 $J(x)p=-F(x)$ 来计算搜索方向 $p$。如果雅可比矩阵是奇异或病态的，则计算一个最小二乘解来确定使 $\\lVert J(x)p+F(x)\\rVert_2$ 最小化的 $p$。\n- 对于阻尼方法，对 $\\alpha$ 执行回溯线搜索，直到满足充分下降条件或 $\\alpha$ 低于 $\\alpha_{\\min}$。\n- 当 $\\lVert F(x)\\rVert_2\\le\\varepsilon$ 或达到 $N_{\\max}$ 次迭代时停止。\n\n测试套件和要求输出：\n- 案例 A (Rosenbrock 梯度系统，使用 $x^{(0)}=(-1.2,\\,1.0)$，$\\varepsilon=10^{-10}$，$N_{\\max}=50$)：输出一个布尔值，当且仅当在相同的容差下，阻尼方法在 $N_{\\max}$ 次迭代内收敛，而全步长方法在 $N_{\\max}$ 次迭代内未能收敛时，该值为 $true$。\n- 案例 B (Rosenbrock 梯度系统，使用 $x^{(0)}=(1.2,\\,1.2)$，$\\varepsilon=10^{-10}$，$N_{\\max}=50$)：输出一个布尔值，当且仅当在相同的容差下，两种方法都在 $N_{\\max}$ 次迭代内收敛时，该值为 $true$。\n- 案例 C (近奇异系统，使用 $x^{(0)}=(10^{-4},\\,10^{-4})$，$\\varepsilon=10^{-8}$，$N_{\\max}=100$)：输出一个布尔值，当且仅当在指定的容差下，阻尼方法在 $N_{\\max}$ 次迭代内收敛，并且其最终价值函数值 $\\phi(x)$ 小于或等于全步长方法在终止时的值时，该值为 $true$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[resultA,resultB,resultC]”），其中每个结果是其对应案例定义的布尔值。",
            "solution": "用户要求实现并比较用于求解非线性方程组 $F(x) = 0$ 的牛顿法的两种变体。第一种是标准的全步长牛顿法，第二种是采用回溯线搜索的阻尼版本，以确保从更广范围的初始猜测收敛。\n\n### 问题表述\n\n一个非线性方程组由函数 $F:\\mathbb{R}^n \\to \\mathbb{R}^n$ 给出。我们寻求一个根 $x^\\star$ 使得 $F(x^\\star) = 0$。牛顿法是一个迭代过程，给定一个迭代点 $x^{(k)}$，通过用 $F$ 在 $x^{(k)}$ 周围的一阶泰勒展开来近似它，从而找到下一个迭代点 $x^{(k+1)}$：\n$$\nF(x^{(k)} + p) \\approx F(x^{(k)}) + J(x^{(k)})p\n$$\n其中 $J(x^{(k)})$ 是 $F$ 在 $x^{(k)}$ 处的雅可比矩阵。为了找到近似式的根，我们将右侧设为零，并求解步长 $p$，称为牛顿步：\n$$\nJ(x^{(k)})p^{(k)} = -F(x^{(k)})\n$$\n这是一个关于搜索方向 $p^{(k)}$ 的线性方程组。\n\n### 标准牛顿法与阻尼牛顿法\n\n**1. 标准全步长牛顿法：**\n下一个迭代点总是通过取完整的牛顿步来计算：\n$$\nx^{(k+1)} = x^{(k)} + p^{(k)}\n$$\n这等同于设置步长 $\\alpha=1$。该方法表现出局部二次收敛性，但如果初始猜测 $x^{(0)}$ 不够接近解 $x^\\star$，则可能无法收敛（即发散或振荡）。\n\n**2. 带回溯线搜索的阻尼牛顿法：**\n为了全局化收敛，引入了步长 $\\alpha \\in (0, 1]$：\n$$\nx^{(k+1)} = x^{(k)} + \\alpha p^{(k)}\n$$\n选择步长 $\\alpha$ 是为了确保价值函数（merit function）的充分下降，该函数衡量我们距离解的远近。价值函数的一个常用选择是 $\\phi(x) = \\frac{1}{2}\\lVert F(x)\\rVert_2^2$。目标是找到一个满足 Armijo-Goldstein 充分下降条件的 $\\alpha$：\n$$\n\\phi(x^{(k)} + \\alpha p^{(k)}) \\le \\phi(x^{(k)}) + \\sigma \\alpha \\nabla\\phi(x^{(k)})^T p^{(k)}\n$$\n对于一个小常数 $\\sigma \\in (0, 1)$，通常取 $\\sigma = 10^{-4}$。\n\n价值函数的梯度是 $\\nabla\\phi(x) = J(x)^T F(x)$。在方向 $p^{(k)}$ 上的方向导数是：\n$$\n\\nabla\\phi(x^{(k)})^T p^{(k)} = (J(x^{(k)})^T F(x^{(k)}))^T p^{(k)} = F(x^{(k)})^T J(x^{(k)}) p^{(k)}\n$$\n代入 $J(x^{(k)})p^{(k)} = -F(x^{(k)})$，我们得到：\n$$\n\\nabla\\phi(x^{(k)})^T p^{(k)} = F(x^{(k)})^T (-F(x^{(k)})) = -\\lVert F(x^{(k)}) \\rVert_2^2\n$$\n这表明，只要 $J(x^{(k)})$ 是非奇异的，牛顿方向 $p^{(k)}$ 就是价值函数 $\\phi(x)$ 的一个下降方向。充分下降条件简化为：\n$$\n\\phi(x^{(k)} + \\alpha p^{(k)}) \\le \\phi(x^{(k)}) - \\sigma \\alpha \\lVert F(x^{(k)}) \\rVert_2^2\n$$\n回溯线搜索算法从 $\\alpha=1$ 开始，并以因子 $\\beta \\in (0,1)$（例如 $\\beta = \\frac{1}{2}$）连续减小它，直到满足此条件或 $\\alpha$ 小于预设的最小值 $\\alpha_{\\min}$。\n\n### 算法实现\n\n开发了一个鲁棒的求解器来封装这两种方法。它接受系统的函数 $F$、其雅可比矩阵 $J$、初始猜测 $x^{(0)}$ 和控制参数。\n\n**求解器内的核心步骤：**\n1.  初始化迭代计数器 $k=0$ 和解向量 $x = x^{(0)}$。\n2.  开始主循环，只要迭代次数 $k  N_{\\max}$ 就继续。\n3.  在每次迭代时，评估 $F(x)$ 并检查收敛性：如果 $\\lVert F(x) \\rVert_2 \\le \\varepsilon$，过程成功终止。\n4.  评估雅可比矩阵 $J(x)$。\n5.  求解线性系统 $J(x) p = -F(x)$ 以获得搜索方向 $p$。为了处理 $J(x)$ 是奇异或病态的情况，我们使用最小二乘求解器，它找到最小化 $\\lVert J(x)p + F(x) \\rVert_2$ 的 $p$。\n6.  如果启用了阻尼，执行回溯线搜索：\n    a. 从 $\\alpha = 1$ 开始。\n    b. 反复检查充分下降条件。\n    c. 如果条件不满足，更新 $\\alpha \\leftarrow \\beta\\alpha$。\n    d. 如果 $\\alpha$ 小于 $\\alpha_{\\min}$，则认为搜索停滞，迭代停止。\n7.  如果禁用了阻尼，则简单地设置 $\\alpha=1$。\n8.  更新解：$x \\leftarrow x + \\alpha p$。\n9.  增加迭代计数器 $k \\leftarrow k+1$。\n10. 如果循环完成而未收敛，则该方法失败。返回最终状态和失败状态。\n\n### 测试案例分析\n\n**案例 A 和 B：Rosenbrock 梯度系统**\n该系统源自经典的 Rosenbrock 函数的梯度，这是一个困难的非凸优化基准问题。解位于 $x^\\star = (1, 1)$，此处 $F_R(x^\\star) = 0$。\n-   **案例 A（较差的初始猜测）：** 从 $x^{(0)}=(-1.2,\\,1.0)$ 开始，初始迭代点远离解。牛顿法的局部二次近似不准确，导致全步长方法采取大的、无效的步长，从而导致发散或振荡。阻尼方法通过强制价值函数下降，采取更小、更谨慎的步长，成功地在复杂的“地形”中向解导航。这展示了阻尼方法卓越的全局收敛特性。\n-   **案例 B（良好初始猜测）：** 从 $x^{(0)}=(1.2,\\,1.2)$ 开始，迭代点位于解的吸引盆内。此处，局部二次模型是一个很好的近似。全步长 $\\alpha=1$ 满足充分下降条件，因此阻尼方法的行为与全步长方法完全相同。由于牛顿法在解附近的二次收敛率，预计两者都会快速收敛。\n\n**案例 C：近奇异雅可比矩阵系统**\n该系统在 $x^\\star = (0, 0)$ 处有解，其中雅可比矩阵 $J_S(0,0)$ 是奇异的。从 $x^{(0)}=(10^{-4},\\,10^{-4})$ 开始，初始雅可比矩阵接近奇异。这测试了线性求解器的鲁棒性。在此处，使用最小二乘求解器来求解牛顿步是至关重要的。此测试案例的条件是阻尼方法必须收敛，并且其最终价值函数值 $\\phi(x_{final})$ 必须小于或等于全步长方法的最终值。由于阻尼方法保证每一步都不会使价值函数值恶化（与全步长方法不同），因此预计其达到的最终状态至少与全步长方法的最终状态一样好，甚至更好，从而满足测试条件。\n\n该程序实现了这些函数、求解器和测试案例，以产生所需的布尔结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests standard and damped Newton's methods for solving\n    systems of nonlinear equations.\n    \"\"\"\n\n    # --- System Definitions ---\n\n    def F_R(x):\n        \"\"\"Rosenbrock-gradient system function F(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        f1 = 2 * (x1 - 1) - 400 * x1 * (x2 - x1**2)\n        f2 = 200 * (x2 - x1**2)\n        return np.array([f1, f2], dtype=float)\n\n    def J_R(x):\n        \"\"\"Jacobian of the Rosenbrock-gradient system J(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        j11 = 2 - 400 * (x2 - x1**2) + 800 * x1**2\n        j12 = -400 * x1\n        j21 = -400 * x1\n        j22 = 200\n        return np.array([[j11, j12], [j21, j22]], dtype=float)\n\n    def F_S(x):\n        \"\"\"Near-singular system function F(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        f1 = x1 * x2\n        f2 = x1**2 - x2\n        return np.array([f1, f2], dtype=float)\n\n    def J_S(x):\n        \"\"\"Jacobian of the near-singular system J(x).\"\"\"\n        x1, x2 = x[0], x[1]\n        j11 = x2\n        j12 = x1\n        j21 = 2 * x1\n        j22 = -1.0\n        return np.array([[j11, j12], [j21, j22]], dtype=float)\n\n    # --- Newton Solver Implementation ---\n\n    def newton_solver(F, J, x0, tol, max_iter, use_damping, sigma, beta, alpha_min):\n        \"\"\"\n        Solves a nonlinear system F(x)=0 using Newton's method.\n\n        Returns:\n            x_final (np.ndarray): The final solution vector.\n            Fx_final (np.ndarray): The final residual vector F(x_final).\n            converged (bool): True if the method converged to the tolerance.\n        \"\"\"\n        x = np.copy(x0).astype(float)\n        k = 0\n\n        while k  max_iter:\n            Fx = F(x)\n            norm_Fx = np.linalg.norm(Fx)\n\n            if norm_Fx = tol:\n                return x, Fx, True\n\n            Jx = J(x)\n            \n            # Solve the linear system using least-squares for robustness\n            try:\n                p = np.linalg.lstsq(Jx, -Fx, rcond=None)[0]\n            except np.linalg.LinAlgError:\n                # This may happen for extremely ill-conditioned matrices, though lstsq is robust.\n                return x, Fx, False\n\n            alpha = 1.0\n            if use_damping:\n                phi_x = 0.5 * norm_Fx**2\n                grad_phi_p = -norm_Fx**2\n\n                line_search_success = False\n                while alpha > alpha_min:\n                    x_new = x + alpha * p\n                    phi_new = 0.5 * np.linalg.norm(F(x_new))**2\n                    \n                    if phi_new = phi_x + sigma * alpha * grad_phi_p:\n                        line_search_success = True\n                        break\n                    \n                    alpha *= beta\n                \n                if not line_search_success:\n                    # Unable to find a step that satisfies the condition, indicating stalling.\n                    return x, Fx, False\n            \n            x = x + alpha * p\n            k += 1\n\n        # Check for convergence after the final iteration\n        final_Fx = F(x)\n        converged = np.linalg.norm(final_Fx) = tol\n        return x, final_Fx, converged\n\n    # --- Test Cases ---\n    \n    results = []\n    \n    # Common line search parameters\n    ls_params = {'sigma': 1e-4, 'beta': 0.5, 'alpha_min': 1e-12}\n\n    # Case A: Rosenbrock-gradient with poor initial guess\n    x0_A = np.array([-1.2, 1.0])\n    tol_A = 1e-10\n    max_iter_A = 50\n    _, _, converged_damped_A = newton_solver(F_R, J_R, x0_A, tol_A, max_iter_A, True, **ls_params)\n    _, _, converged_full_A = newton_solver(F_R, J_R, x0_A, tol_A, max_iter_A, False, **ls_params)\n    resultA = converged_damped_A and not converged_full_A\n    results.append(resultA)\n\n    # Case B: Rosenbrock-gradient with good initial guess\n    x0_B = np.array([1.2, 1.2])\n    tol_B = 1e-10\n    max_iter_B = 50\n    _, _, converged_damped_B = newton_solver(F_R, J_R, x0_B, tol_B, max_iter_B, True, **ls_params)\n    _, _, converged_full_B = newton_solver(F_R, J_R, x0_B, tol_B, max_iter_B, False, **ls_params)\n    resultB = converged_damped_B and converged_full_B\n    results.append(resultB)\n\n    # Case C: Near-singular Jacobian at the start\n    x0_C = np.array([1e-4, 1e-4])\n    tol_C = 1e-8\n    max_iter_C = 100\n    _, Fx_damped_C, converged_damped_C = newton_solver(F_S, J_S, x0_C, tol_C, max_iter_C, True, **ls_params)\n    _, Fx_full_C, _ = newton_solver(F_S, J_S, x0_C, tol_C, max_iter_C, False, **ls_params)\n    \n    phi_damped_C = 0.5 * np.linalg.norm(Fx_damped_C)**2\n    phi_full_C = 0.5 * np.linalg.norm(Fx_full_C)**2\n    \n    resultC = converged_damped_C and (phi_damped_C = phi_full_C)\n    results.append(resultC)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "牛顿法的一个重要应用是在科学与工程领域中寻找函数的平稳点，这本质上是一个优化问题。本练习将这一优化任务转化为一个求解梯度方程组的根寻找问题。你不仅将应用牛顿法来寻找一个著名测试函数（六驼峰函数）的多个平稳点，还将接触到正则化技术，这是一种处理雅可比矩阵病态或奇异情况的实用策略。",
            "id": "3255464",
            "problem": "考虑二次连续可微的二元函数 $$f(x,y) = \\left(4 - 2.1x^2 + \\frac{x^4}{3}\\right)x^2 + xy + \\left(-4 + 4y^2\\right)y^2,$$ 该函数被称为六驼峰函数。该函数的一个驻点是任意满足梯度为零的点 $$(x^\\star,y^\\star)$$，即 $$\\nabla f(x^\\star,y^\\star) = \\mathbf{0}。$$ 您的任务是设计并实现一个程序，通过求解非线性系统 $$\\mathbf{F}(\\mathbf{z}) = \\mathbf{0}, \\quad \\text{其中 } \\mathbf{z} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}, \\ \\mathbf{F}(\\mathbf{z}) = \\nabla f(x,y),$$ 来计算驻点。该方法使用针对方程组的牛顿法，并从指定的初始猜测值开始。\n\n使用从多元泰勒展开以及梯度和雅可比矩阵的定义开始的推导。通过将牛顿步与基于价值函数 $$\\phi(\\mathbf{z}) = \\frac{1}{2}\\left\\|\\mathbf{F}(\\mathbf{z})\\right\\|_2^2$$ 的回溯线搜索相结合，实现一种全局收敛策略。如果雅可比矩阵是奇异或病态的，则对其应用一个小的对角正则化 $$\\mu \\mathbf{I}$$（其中 $$\\mu  0$$ 根据需要进行调整），以获得一个可解的线性系统。当无穷范数 $$\\|\\mathbf{F}(\\mathbf{z}_k)\\|_\\infty \\le 10^{-10}$$ 或步长的无穷范数 $$\\|\\mathbf{s}_k\\|_\\infty \\le 10^{-12}$$ 时，或者迭代次数达到 $$100$$ 时，终止迭代。如果出现任何隐式的角度度量，请使用弧度（本问题中不需要）。所有浮点输出必须四舍五入到六位小数。\n\n您的程序必须处理以下初始猜测值的测试套件：\n- 测试用例 1：$$\\mathbf{z}_0 = (0.1,-0.7)$$。\n- 测试用例 2：$$\\mathbf{z}_0 = (-0.2,0.6)$$。\n- 测试用例 3（边界情况：已是驻点）：$$\\mathbf{z}_0 = (0.0,0.0)$$。\n- 测试用例 4：$$\\mathbf{z}_0 = (-1.7,0.8)$$。\n- 测试用例 5：$$\\mathbf{z}_0 = (1.7,-0.8)$$。\n\n对于每个测试用例，输出找到的驻点 $$(x^\\star,y^\\star)$$ 和对应的函数值 $$f(x^\\star,y^\\star)$$。最终输出必须是单行，包含一个列表的列表，格式完全如下\n$$[[x_1,y_1,f_1],[x_2,y_2,f_2],[x_3,y_3,f_3],[x_4,y_4,f_4],[x_5,y_5,f_5]],$$\n其中每个 $$x_i$$、$$y_i$$ 和 $$f_i$$ 都是四舍五入到六位小数的浮点数，且输出字符串中没有空格。",
            "solution": "经评估，用户提供的问题是**有效的**。这是一个在数值分析领域内提法得当、有科学依据的问题，没有矛盾、歧义或伪科学主张。任务是使用标准的系统牛顿法，并辅以全局化和正则化技术，来寻找给定函数的驻点。所有必要的数据，包括目标函数、初始条件和终止准则，均已提供。\n\n### 基于原理的解决方案设计\n\n该问题要求寻找一个二次连续可微函数 $f(x,y)$ 的驻点 $(x^\\star, y^\\star)$。驻点定义为函数梯度为零向量的点，即 $\\nabla f(x^\\star, y^\\star) = \\mathbf{0}$。这个条件建立了一个必须求解的非线性方程组。\n\n**1. 非线性系统的构建**\n\n函数由下式给出：\n$$f(x,y) = \\left(4 - 2.1x^2 + \\frac{x^4}{3}\\right)x^2 + xy + \\left(-4 + 4y^2\\right)y^2$$\n展开此表达式得到：\n$$f(x,y) = 4x^2 - 2.1x^4 + \\frac{x^6}{3} + xy - 4y^2 + 4y^4$$\n状态变量向量为 $\\mathbf{z} = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$。需要求解的非线性方程组为 $\\mathbf{F}(\\mathbf{z}) = \\mathbf{0}$，其中 $\\mathbf{F}(\\mathbf{z})$ 是 $f(x,y)$ 的梯度：\n$$\\mathbf{F}(\\mathbf{z}) = \\nabla f(x,y) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$$\n偏导数是：\n$$\\frac{\\partial f}{\\partial x} = 8x - 8.4x^3 + 2x^5 + y$$\n$$\\frac{\\partial f}{\\partial y} = x - 8y + 16y^3$$\n因此，方程组为：\n$$\\mathbf{F}(\\mathbf{z}) = \\begin{bmatrix} F_1(x,y) \\\\ F_2(x,y) \\end{bmatrix} = \\begin{bmatrix} 2x^5 - 8.4x^3 + 8x + y \\\\ 16y^3 - 8y + x \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$\n\n**2. 系统牛顿法**\n\n牛顿法是用于寻找方程组根的迭代过程。从一个初始猜测值 $\\mathbf{z}_0$ 开始，它生成一个逼近根的迭代序列 $\\mathbf{z}_k$。更新规则源于 $\\mathbf{F}$ 在当前迭代点 $\\mathbf{z}_k$ 附近的一阶多元泰勒展开：\n$$\\mathbf{F}(\\mathbf{z}_{k+1}) \\approx \\mathbf{F}(\\mathbf{z}_k) + J(\\mathbf{z}_k)(\\mathbf{z}_{k+1} - \\mathbf{z}_k)$$\n其中 $J(\\mathbf{z}_k)$ 是 $\\mathbf{F}$ 在 $\\mathbf{z}_k$ 处计算的雅可比矩阵。\n我们寻求 $\\mathbf{z}_{k+1}$ 使得 $\\mathbf{F}(\\mathbf{z}_{k+1}) = \\mathbf{0}$。将左侧设为零，并定义牛顿步为 $\\mathbf{s}_k = \\mathbf{z}_{k+1} - \\mathbf{z}_k$，我们得到关于步长 $\\mathbf{s}_k$ 的以下线性系统：\n$$J(\\mathbf{z}_k)\\mathbf{s}_k = -\\mathbf{F}(\\mathbf{z}_k)$$\n然后通过 $\\mathbf{z}_{k+1} = \\mathbf{z}_k + \\mathbf{s}_k$ 找到下一个迭代点。\n\n**3. 雅可比矩阵推导**\n\n系统 $\\mathbf{F}(\\mathbf{z})$ 的雅可比矩阵 $J(\\mathbf{z})$ 由下式给出：\n$$J(\\mathbf{z}) = \\begin{bmatrix} \\frac{\\partial F_1}{\\partial x}  \\frac{\\partial F_1}{\\partial y} \\\\ \\frac{\\partial F_2}{\\partial x}  \\frac{\\partial F_2}{\\partial y} \\end{bmatrix}$$\n由于 $\\mathbf{F} = \\nabla f$，雅可比矩阵 $J$ 就是 $f$ 的海森矩阵 $H_f$。对于像 $f$ 这样的 $C^2$ 函数，根据 Clairaut 的混合偏导数相等定理，海森矩阵是对称的。其分量为：\n$$\\frac{\\partial F_1}{\\partial x} = \\frac{\\partial^2 f}{\\partial x^2} = 10x^4 - 25.2x^2 + 8$$\n$$\\frac{\\partial F_1}{\\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x} = 1$$\n$$\\frac{\\partial F_2}{\\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y} = 1$$\n$$\\frac{\\partial F_2}{\\partial y} = \\frac{\\partial^2 f}{\\partial y^2} = 48y^2 - 8$$\n因此，雅可比矩阵为：\n$$J(\\mathbf{z}) = \\begin{bmatrix} 10x^4 - 25.2x^2 + 8  1 \\\\ 1  48y^2 - 8 \\end{bmatrix}$$\n\n**4. 通过回溯线搜索实现全局化**\n\n完整的牛顿步 $\\mathbf{s}_k$ 可能不总能保证收敛，特别是当初始猜测值远离根时。需要一种全局化策略。我们采用回溯线搜索来寻找一个合适的步长 $\\alpha_k \\in (0,1]$，以确保朝向解有足够的进展。更新规则变为：\n$$\\mathbf{z}_{k+1} = \\mathbf{z}_k + \\alpha_k \\mathbf{s}_k$$\n进展通过一个价值函数来衡量，对于寻根问题，该函数通常是残差的L2范数平方：\n$$\\phi(\\mathbf{z}) = \\frac{1}{2}\\|\\mathbf{F}(\\mathbf{z})\\|_2^2 = \\frac{1}{2}\\mathbf{F}(\\mathbf{z})^T\\mathbf{F}(\\mathbf{z})$$\n选择步长 $\\alpha_k$ 以满足 Armijo 充分下降条件：\n$$\\phi(\\mathbf{z}_k + \\alpha_k \\mathbf{s}_k) \\le \\phi(\\mathbf{z}_k) + c \\alpha_k \\nabla \\phi(\\mathbf{z}_k)^T \\mathbf{s}_k$$\n其中 $c \\in (0, 1)$ 是一个小常数（例如，$c=10^{-4}$）。方向导数 $\\nabla \\phi(\\mathbf{z}_k)^T \\mathbf{s}_k$ 使用链式法则计算：$\\nabla \\phi(\\mathbf{z}) = J(\\mathbf{z})^T \\mathbf{F}(\\mathbf{z})$。\n$$\\nabla \\phi(\\mathbf{z}_k)^T \\mathbf{s}_k = \\left(J(\\mathbf{z}_k)^T \\mathbf{F}(\\mathbf{z}_k)\\right)^T \\mathbf{s}_k = \\mathbf{F}(\\mathbf{z}_k)^T J(\\mathbf{z}_k) \\mathbf{s}_k$$\n回溯算法从 $\\alpha=1$ 开始，并重复减小它（例如，$\\alpha \\leftarrow \\tau \\alpha$，其中 $\\tau=0.5$），直到满足 Armijo 条件。对于标准牛顿步 $\\mathbf{s}_k = -J(\\mathbf{z}_k)^{-1}\\mathbf{F}(\\mathbf{z}_k)$，方向导数简化为 $-\\|\\mathbf{F}(\\mathbf{z}_k)\\|_2^2$，这保证了 $\\phi$ 的一个下降方向。\n\n**5. 雅可比矩阵正则化**\n\n如果雅可比矩阵 $J(\\mathbf{z}_k)$ 变得奇异或数值上病态，则关于 $\\mathbf{s}_k$ 的线性系统无法被可靠地求解。为了处理这个问题，添加一个对角正则化项 $\\mu\\mathbf{I}$，其中 $\\mu  0$ 是一个小参数，$\\mathbf{I}$ 是单位矩阵。修改后的系统是：\n$$(J(\\mathbf{z}_k) + \\mu\\mathbf{I})\\mathbf{s}_k = -\\mathbf{F}(\\mathbf{z}_k)$$\n这种技术与 Levenberg-Marquardt 方法相关，确保了矩阵是可逆的。参数 $\\mu$ 是动态调整的：默认设置为零。如果在求解线性系统时检测到奇异性，$\\mu$ 会被初始化为一个小的正值（例如，$10^{-8}$），然后乘性增加（例如，乘以 $10$），直到系统可以被求解。\n\n**6. 终止准则**\n\n当满足以下条件之一时，迭代过程终止：\n1.  函数残差的无穷范数足够小：$\\|\\mathbf{F}(\\mathbf{z}_k)\\|_\\infty \\le 10^{-10}$。\n2.  所采取的完整步长的无穷范数足够小：$\\|\\alpha_k \\mathbf{s}_k\\|_\\infty \\le 10^{-12}$。\n3.  迭代次数 $k$ 达到最大限制 $100$。\n\n**7. 算法总结**\n\n对于每个初始猜测值 $\\mathbf{z}_0$：\n1.  初始化 $k=0$ 和 $\\mathbf{z}=\\mathbf{z}_0$。\n2.  循环，当 $k  100$ 时：\n    a. 计算 $\\mathbf{F}_k = \\mathbf{F}(\\mathbf{z})$ 并检查收敛性：如果 $\\|\\mathbf{F}_k\\|_\\infty \\le 10^{-10}$，则终止。\n    b. 计算雅可比矩阵 $J_k = J(\\mathbf{z})$。\n    c. 求解 $(J_k + \\mu\\mathbf{I})\\mathbf{s}_k = -\\mathbf{F}_k$ 以获得步长 $\\mathbf{s}_k$，自适应地选择 $\\mu \\ge 0$ 以确保系统可解。\n    d. 执行回溯线搜索以找到满足价值函数 $\\phi$ 的 Armijo 条件的步长 $\\alpha_k$。\n    e. 计算更新量：$\\Delta\\mathbf{z}_k = \\alpha_k \\mathbf{s}_k$。\n    f. 更新解：$\\mathbf{z} \\leftarrow \\mathbf{z} + \\Delta\\mathbf{z}_k$。\n    g. 检查收敛性：如果 $\\|\\Delta\\mathbf{z}_k\\|_\\infty \\le 10^{-12}$，则终止。\n3.  终止后，驻点为 $\\mathbf{z}^\\star = \\mathbf{z}$。计算函数值 $f(\\mathbf{z}^\\star)$。\n4.  存储并格式化结果 $(x^\\star, y^\\star, f(x^\\star, y^\\star))$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    def f_camel(z):\n        \"\"\"Computes the six-hump camel back function value.\"\"\"\n        x, y = z[0], z[1]\n        term1 = (4 - 2.1 * x**2 + (x**4) / 3) * x**2\n        term2 = x * y\n        term3 = (-4 + 4 * y**2) * y**2\n        return term1 + term2 + term3\n\n    def F_grad(z):\n        \"\"\"Computes the gradient of the six-hump camel back function.\"\"\"\n        x, y = z[0], z[1]\n        df_dx = 2 * x**5 - 8.4 * x**3 + 8 * x + y\n        df_dy = 16 * y**3 - 8 * y + x\n        return np.array([df_dx, df_dy])\n\n    def J_hess(z):\n        \"\"\"Computes the Jacobian of the gradient (Hessian of the function).\"\"\"\n        x, y = z[0], z[1]\n        d2f_dx2 = 10 * x**4 - 25.2 * x**2 + 8\n        d2f_dxdy = 1.0\n        d2f_dy2 = 48 * y**2 - 8\n        return np.array([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])\n\n    def find_stationary_point(z0):\n        \"\"\"\n        Finds a stationary point using Newton's method with line search and regularization.\n        \"\"\"\n        z = np.array(z0, dtype=float)\n        \n        # Parameters\n        max_iter = 100\n        tol_F = 1e-10\n        tol_s = 1e-12\n        c1_armijo = 1e-4\n        tau_backtrack = 0.5\n        max_line_search_iter = 20\n\n        for _ in range(max_iter):\n            Fk = F_grad(z)\n\n            # Termination condition 1: residual norm\n            if np.linalg.norm(Fk, np.inf) = tol_F:\n                break\n\n            Jk = J_hess(z)\n\n            # Solve for Newton step with adaptive regularization\n            mu = 0.0\n            sk = None\n            while True:\n                try:\n                    J_reg = Jk if mu == 0.0 else Jk + mu * np.identity(2)\n                    sk = np.linalg.solve(J_reg, -Fk)\n                    break\n                except np.linalg.LinAlgError:\n                    if mu == 0.0:\n                        mu = 1e-8\n                    else:\n                        mu *= 10\n                    # Failsafe to prevent extreme mu values\n                    if mu > 1e16:\n                        sk = np.zeros_like(z) # Force stop if regularization fails\n                        break\n\n            # Backtracking line search\n            alpha = 1.0\n            phi_k = 0.5 * (Fk @ Fk)\n            # The directional derivative of the merit function phi\n            dir_deriv = Fk @ Jk @ sk\n\n            # If dir_deriv is non-negative, the line search will fail naturally\n            # by shrinking alpha, leading to a small step and termination.\n            for _ in range(max_line_search_iter):\n                z_trial = z + alpha * sk\n                F_trial = F_grad(z_trial)\n                phi_trial = 0.5 * (F_trial @ F_trial)\n\n                if phi_trial = phi_k + c1_armijo * alpha * dir_deriv:\n                    break\n                \n                alpha *= tau_backtrack\n            else:\n                # If the line search loop completes without a break,\n                # it means a suitable alpha was not found.\n                # In this case, the step size will be effectively zero.\n                alpha = 0.0\n\n            step = alpha * sk\n            z += step\n\n            # Termination condition 2: step norm\n            if np.linalg.norm(step, np.inf) = tol_s:\n                break\n\n        return z\n\n    # Test cases defined in the problem\n    test_cases = [\n        (0.1, -0.7),\n        (-0.2, 0.6),\n        (0.0, 0.0),\n        (-1.7, 0.8),\n        (1.7, -0.8),\n    ]\n\n    results = []\n    for z0 in test_cases:\n        z_star = find_stationary_point(z0)\n        f_star = f_camel(z_star)\n        \n        # Round to six decimal places for output\n        x_out = round(z_star[0], 6)\n        y_out = round(z_star[1], 6)\n        f_out = round(f_star, 6)\n\n        results.append(f\"[{x_out:.6f},{y_out:.6f},{f_out:.6f}]\")\n\n    # Final print statement in the exact required format with no spaces\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}