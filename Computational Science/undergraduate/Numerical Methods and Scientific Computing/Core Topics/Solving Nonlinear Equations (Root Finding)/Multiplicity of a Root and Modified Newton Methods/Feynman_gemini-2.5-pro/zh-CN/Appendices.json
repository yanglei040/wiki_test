{
    "hands_on_practices": [
        {
            "introduction": "对于重根，标准牛顿法的二次收敛会退化为线性收敛，效率大打折扣。本练习旨在通过一个直观的编程实践，让你亲手量化这一差异。你将对一个具有已知重数 $m$ 的函数，分别应用标准牛顿法和修正牛顿法，通过比较达到预设精度所需的迭代次数，直观感受在重数已知时，修正方法所带来的巨大效率提升。",
            "id": "3254008",
            "problem": "考虑求解 $f(x)=0$ 的任务，其中根的重数已知。对于一个充分可微的函数 $f$，如果 $f(x^{\\star})=0$，对于所有满足 $1 \\le k \\le m-1$ 的整数 $k$ 都有 $f^{(k)}(x^{\\star})=0$，并且 $f^{(m)}(x^{\\star}) \\ne 0$，则点 $x^{\\star}$ 是一个重数为 $m$ 的根。标准 Newton 法通过在当前迭代点用 $f$ 的一阶 Taylor 近似局部替换 $f$，并求解得到的线性化方程而导出。当根的重数大于一时，未经修改的迭代可能会失去其通常的二次收敛性。一种针对已知重数 $m$ 的修正 Newton 法通过重新缩放迭代来恢复快速收敛。\n\n你的任务是编写一个完整、可运行的程序，对于函数族 $f_m(x)=(x-1)^m$（其中 $m \\in \\{2,3,5\\}$），比较标准 Newton 法和考虑了重数的修正 Newton 法。对于每个函数，使用三个初始猜测值 $x_0 \\in \\{2,1,-5\\}$，并应用两种方法计算使残差低于固定容差所需的迭代次数。残差应以 $|f_m(x_n)|$ 来衡量，停止条件是 $|f_m(x_n)| \\le \\tau$，其中 $\\tau=10^{-12}$。如果初始猜测值已经满足 $|f_m(x_0)| \\le \\tau$，则迭代次数为 $0$。如果方法在最多 $N_{\\text{max}}=1000$ 次迭代内未能达到容差，则报告该测试用例上该方法的迭代次数为整数 $-1$。\n\n实现要求：\n- 使用切线线性化原理实现用于求根的标准 Newton 法。\n- 实现针对已知重数 $m$ 的修正 Newton 法，适当重新缩放步长以处理重根问题。\n- 在两种方法中都使用 $f_m(x)=(x-1)^m$ 及其导数 $f_m'(x)$。\n- 使用残差 $|f_m(x_n)|$ 和容差 $\\tau=10^{-12}$ 作为停止检查的条件。\n- 使用最大迭代次数上限 $N_{\\text{max}}=1000$。\n\n测试套件规范：\n- 测试用例的有序列表是 $m \\in \\{2,3,5\\}$ 和 $x_0 \\in \\{2,1,-5\\}$ 的笛卡尔积，按 $m$ 然后按 $x_0$ 的字典序排列。具体来说，测试用例为 $(m,x_0)$ 等于 $(2,2)$, $(2,1)$, $(2,-5)$, $(3,2)$, $(3,1)$, $(3,-5)$, $(5,2)$, $(5,1)$, $(5,-5)$。\n\n对于每个测试用例，你的程序必须输出一对整数 $[n_{\\text{std}},n_{\\text{mod}}]$，其中 $n_{\\text{std}}$ 是标准 Newton 法的迭代次数， $n_{\\text{mod}}$ 是修正 Newton 法的迭代次数。最终输出必须是单行，包含按指定测试用例顺序排列的所有九对整数，形式为用方括号括起来的逗号分隔列表，不含空格。例如，包含三对假设结果的输出将如下所示：$[[1,1],[0,0],[7,2]]$。你的程序应生成确切的最终格式：\n- 单行：$[[n_{\\text{std},1},n_{\\text{mod},1}],[n_{\\text{std},2},n_{\\text{mod},2}],\\dots,[n_{\\text{std},9},n_{\\text{mod},9}]]$。",
            "solution": "该问题要求比较标准 Newton 法和修正 Newton 法在寻找已知重数 $m$ 的根时的表现。分析将在函数族 $f_m(x)=(x-1)^m$ 上进行，该函数族在 $x^{\\star}=1$ 处有一个重数为 $m$ 的根。\n\n对于一个函数 $f$，如果 $f(x^{\\star})=0$ 且其在 $x^{\\star}$ 处的前 $m-1$ 阶导数均为零，即对于 $1 \\le k \\le m-1$ 有 $f^{(k)}(x^{\\star})=0$，而第 $m$ 阶导数不为零，$f^{(m)}(x^{\\star}) \\ne 0$，则点 $x^{\\star}$ 被定义为 $f$ 的一个重数为 $m$ 的根。\n\n### 标准 Newton 法\n\n标准 Newton-Raphson 法通过构建一系列逐渐更优的近似值来寻找方程 $f(x)=0$ 的根。该迭代由 $f(x)$ 在迭代点 $x_n$ 附近的一阶 Taylor 级数展开式导出：\n$$f(x) \\approx f(x_n) + f'(x_n)(x - x_n)$$\n为了找到根，我们令 $f(x)=0$ 并求解 $x$，所得结果即为下一个迭代点 $x_{n+1}$：\n$$0 = f(x_n) + f'(x_n)(x_{n+1} - x_n)$$\n这就得到了标准的 Newton 迭代公式：\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n该方法对单根（其中 $m=1$）表现出二次收敛性，这意味着每次迭代后，正确的小数位数大约会翻倍。然而，对于重数 $m>1$ 的根，收敛性会降至线性，收敛率为 $(m-1)/m$。\n\n对于特定函数 $f_m(x)=(x-1)^m$，其导数为 $f_m'(x)=m(x-1)^{m-1}$。将这些代入迭代公式可得：\n$$x_{n+1} = x_n - \\frac{(x_n-1)^m}{m(x_n-1)^{m-1}} = x_n - \\frac{x_n-1}{m}$$\n设第 $n$ 步的误差为 $e_n = x_n-1$。误差更新规则为：\n$$e_{n+1} = x_{n+1}-1 = \\left(x_n - \\frac{x_n-1}{m}\\right) - 1 = (x_n - 1) - \\frac{x_n-1}{m} = e_n \\left(1 - \\frac{1}{m}\\right)$$\n这证实了线性收敛性，因为每一步误差都会减少一个常数因子 $1 - 1/m$。\n\n### 针对重根的修正 Newton 法\n\n为了恢复已知重数 $m$ 的根的二次收敛性，可以修改标准 Newton 步长。修正后的迭代公式为：\n$$x_{n+1} = x_n - m \\frac{f(x_n)}{f'(x_n)}$$\n这个公式有几种推导方法。一种方法是对辅助函数 $u(x) = [f(x)]^{1/m}$ 应用标准 Newton 法。如果 $f(x)$ 在 $x^{\\star}$ 处有一个重数为 $m$ 的根，那么 $u(x)$ 在 $x^{\\star}$ 处有一个单根，标准 Newton 法对单根是二次收敛的。$u(x)$ 的导数是 $u'(x) = \\frac{1}{m} f(x)^{(1/m)-1}f'(x)$。对 $u(x)$ 的 Newton 迭代是：\n$$x_{n+1} = x_n - \\frac{u(x_n)}{u'(x_n)} = x_n - \\frac{f(x_n)^{1/m}}{\\frac{1}{m} f(x_n)^{(1/m)-1}f'(x_n)} = x_n - m \\frac{f(x_n)}{f'(x_n)}$$\n这正是修正后的公式。\n\n对于特定函数 $f_m(x)=(x-1)^m$，修正后的迭代变为：\n$$x_{n+1} = x_n - m \\frac{(x_n-1)^m}{m(x_n-1)^{m-1}} = x_n - (x_n-1) = 1$$\n这个显著的结果表明，对于函数族 $f_m(x)=(x-1)^m$，修正 Newton 法对于任何不等于 1 的初始猜测值 $x_0 \\ne 1$，都能在单次迭代中收敛到精确根 $x^{\\star}=1$。\n\n### 算法实现\n\n对于由数对 $(m, x_0)$ 定义的每个测试用例，其中 $m \\in \\{2,3,5\\}$ 且 $x_0 \\in \\{2,1,-5\\}$，我们将对标准方法和修正方法执行以下步骤：\n\n1.  初始化迭代次数 $n=0$ 和当前猜测值 $x=x_0$。\n2.  检查初始停止条件：如果残差 $|f_m(x_0)| = |(x_0 - 1)^m|$ 小于或等于容差 $\\tau=10^{-12}$，则过程终止，迭代次数为 $0$。当 $x_0=1$ 时会出现这种情况。\n3.  开始一个迭代循环，最多进行 $N_{\\text{max}}=1000$ 次迭代。在每一步 $n = 1, 2, \\dots, N_{\\text{max}}$ 中：\n    a. 使用相应的公式（标准或修正）计算下一个迭代点 $x_{n}$。\n    b. 更新当前猜测值：$x \\leftarrow x_{n}$。\n    c. 计算残差 $|f_m(x)| = |(x-1)^m|$。\n    d. 如果残差小于或等于 $\\tau$，则循环终止，并返回当前迭代次数 $n$。\n4.  如果循环完成时仍未满足容差（即 $n=N_{\\text{max}}$），则该方法未能在允许的迭代次数内收敛。在这种情况下，报告值 $-1$。\n\n此过程将对所有九个指定的测试用例执行，并将收集迭代次数对 $[n_{\\text{std}}, n_{\\text{mod}}]$ 并格式化为最终输出。根据上述分析，对于任何初始猜测值 $x_0 \\ne 1$，修正方法预计将在 $n_{\\text{mod}}=1$ 次迭代中收敛。对于 $x_0=1$，两种方法都将报告 $n=0$ 次迭代。标准方法将表现出线性收敛性，所需的迭代次数将随着重数 $m$ 和与根的初始距离的增加而增加。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef newton_standard(m, x0, tau, n_max):\n    \"\"\"\n    Performs the standard Newton's method for f(x) = (x-1)^m.\n    The update rule simplifies to x_next = x - (x-1)/m.\n\n    Args:\n        m (int): The multiplicity of the root.\n        x0 (float): The initial guess.\n        tau (float): The tolerance for the residual |f(x)|.\n        n_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required, or -1 if not converged.\n    \"\"\"\n    x = np.float64(x0)\n\n    # Check if the initial guess already satisfies the tolerance.\n    # The exponentiation can be numerically sensitive, but for the given\n    # test cases, the values are well within float64 range.\n    try:\n        initial_residual = np.abs(np.power(x - 1.0, m))\n    except OverflowError:\n        # If the initial guess is very far, residual could overflow.\n        # This is safe for the given test cases but good practice.\n        initial_residual = np.inf\n\n    if initial_residual = tau:\n        return 0\n\n    for n in range(1, n_max + 1):\n        # The update rule is derived as:\n        # f(x) = (x-1)^m\n        # f'(x) = m * (x-1)^(m-1)\n        # x_next = x - f(x)/f'(x) = x - (x-1)/m\n        if x == 1.0:\n            # Landed exactly on the root. This will satisfy the check below,\n            # but this handles the case explicitly.\n            return n\n        \n        x = x - (x - 1.0) / np.float64(m)\n\n        try:\n            residual = np.abs(np.power(x - 1.0, m))\n        except OverflowError:\n            residual = np.inf\n\n        if residual = tau:\n            return n\n\n    return -1\n\ndef newton_modified(m, x0, tau, n_max):\n    \"\"\"\n    Performs the modified Newton's method for f(x) = (x-1)^m.\n    The update rule simplifies to x_next = 1.\n\n    Args:\n        m (int): The multiplicity of the root.\n        x0 (float): The initial guess.\n        tau (float): The tolerance for the residual |f(x)|.\n        n_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required.\n    \"\"\"\n    x = np.float64(x0)\n\n    try:\n        initial_residual = np.abs(np.power(x - 1.0, m))\n    except OverflowError:\n        initial_residual = np.inf\n\n    if initial_residual = tau:\n        return 0\n\n    # The modified Newton's method for f(x) = (x-1)^m converges in one step:\n    # x_next = x - m * f(x)/f'(x) = x - m * (x-1)^m / (m*(x-1)^(m-1))\n    #        = x - (x-1) = 1\n    # The residual at step 1 will be |(1-1)^m| = 0, which is = tau.\n    return 1\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the problem parameters\n    tau = 1.0e-12\n    n_max = 1000\n    multiplicities = [2, 3, 5]\n    initial_guesses = [2, 1, -5]\n\n    # Generate the ordered list of test cases\n    test_cases = []\n    for m in multiplicities:\n        for x0 in initial_guesses:\n            test_cases.append((m, x0))\n\n    results = []\n    for m, x0 in test_cases:\n        n_std = newton_standard(m, x0, tau, n_max)\n        n_mod = newton_modified(m, x0, tau, n_max)\n        results.append([n_std, n_mod])\n\n    # Format the output string as specified: [[r1_1,r1_2],[r2_1,r2_2],...]\n    # with no spaces.\n    result_strings = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "上一个练习假定我们已知根的重数 $m$，但在实际问题中，这通常是未知的。那么，我们如何从计算过程中“推断”出 $m$ 呢？本练习将引导你深入理论，首先通过推导一个仅依赖于函数及其导数局部信息的公式来建立一个重数估计器。接着，你将通过一个数值实验来检验这个估计器的实际表现，从而揭示有限精度算术中理论与计算现实之间的差距，特别是灾难性抵消（catastrophic cancellation）带来的挑战。",
            "id": "3254009",
            "problem": "考虑一个实值函数 $f$，它在 $x=\\alpha$ 处有一个已知重数结构的根。具体来说，假设 $f(x)=(x-\\alpha)^m g(x)$，其中整数 $m\\ge 1$，$g$ 是一个二阶连续可微函数，且 $g(\\alpha)\\ne 0$。这保证了 $f(\\alpha)=0$ 且 $m$ 等于该根的重数。在精确算术中，可以根据 $x=\\alpha$ 附近的局部微分信息推断出 $m$。在遵循电气和电子工程师协会（IEEE）754 双精度标准的浮点算术中，由于舍入和相消误差，此类推断可能会表现出偏差。\n\n任务：\n- 在结构假设 $f(x)=(x-\\alpha)^m g(x)$ 且 $g(\\alpha)\\ne 0$ 的条件下，推导比值 $\\dfrac{f(x) f''(x)}{\\left(f'(x)\\right)^2}$ 当 $x\\to \\alpha$ 时的精确极限，结果仅用 $m$ 表示。\n- 提出并论证一个关于重数 $m$ 的逐点估计量 $\\hat m(x)$，该估计量是 $x\\ne \\alpha$ 的函数，且仅使用 $f(x)$、$f'(x)$ 和 $f''(x)$。\n- 实现一个数值实验，以评估当在非常接近 $\\alpha$ 的点 $x$ 处计算该估计量时，在 IEEE 754 双精度下产生的偏差。\n\n使用以下函数和参数的测试套件。在每种情况下，使用点 $x=\\alpha+\\delta$ 在五个偏移量 $\\delta\\in\\{10^{-1},10^{-4},10^{-8},10^{-12},10^{-16}\\}$ 上评估估计量。\n1. 情况 A（理想路径）：$m=3$，$\\alpha=0$，$g(x)=1+x$。\n2. 情况 B（非零 $\\alpha$ 和弯曲的 $g$）：$m=2$，$\\alpha=0.3$，$g(x)=e^{x}$。\n3. 情况 C（单根边缘情况）：$m=1$，$\\alpha=-0.2$，$g(x)=1+x^2$。\n4. 情况 D（更高重数和振荡的 $g$）：$m=5$，$\\alpha=0$，$g(x)=\\cos(x)$。\n\n对于每种情况，定义 $f(x)=(x-\\alpha)^m g(x)$，使用乘法法则符号化计算 $f'(x)$ 和 $f''(x)$，在每个指定的 $\\delta$ 处评估估计量 $\\hat m(x)$，然后报告每种情况的单个数值摘要：五个偏移量上的平均绝对偏差，定义为\n$$\n\\frac{1}{5}\\sum_{\\delta\\in\\{10^{-1},10^{-4},10^{-8},10^{-12},10^{-16}\\}} \\left|\\hat m(\\alpha+\\delta)-m\\right|.\n$$\n所有计算都应在 IEEE 754 双精度浮点算术中执行。不涉及物理单位。当角度作为三角函数的参数出现时，必须以弧度为单位进行解释。\n\n您的程序应产生单行输出，其中包含四个情况的结果，格式为方括号内以逗号分隔的列表，顺序为 A、B、C、D，例如 `[r_A,r_B,r_C,r_D]`，其中每个 `r_·` 是该情况下的平均绝对偏差，表示为浮点数。",
            "solution": "本问题要求推导一个与具有已知重数结构的函数 $f(x)$ 的根相关的极限，提出一个重数 $m$ 的估计量，并对该估计量在浮点算术中的偏差进行数值评估。\n\n### 第1部分：极限的推导\n\n我们给定一个函数 $f(x)$，它在 $x=\\alpha$ 处有一个重数为 $m$ 的根。这可以形式化地表示为 $f(x) = (x-\\alpha)^m g(x)$，其中 $m \\ge 1$ 是一个整数，$g(x)$ 是一个二阶连续可微函数，且 $g(\\alpha) \\ne 0$。我们的目标是求出比值 $Q(x) = \\dfrac{f(x) f''(x)}{\\left(f'(x)\\right)^2}$ 当 $x \\to \\alpha$ 时的极限。\n\n首先，我们使用乘法法则计算 $f(x)$ 的一阶和二阶导数。\n函数为：\n$$f(x) = (x-\\alpha)^m g(x)$$\n\n一阶导数 $f'(x)$ 为：\n$$f'(x) = \\frac{d}{dx}\\left[(x-\\alpha)^m g(x)\\right] = m(x-\\alpha)^{m-1}g(x) + (x-\\alpha)^m g'(x)$$\n提出公因子 $(x-\\alpha)^{m-1}$，我们得到：\n$$f'(x) = (x-\\alpha)^{m-1} [m g(x) + (x-\\alpha) g'(x)]$$\n\n二阶导数 $f''(x)$ 是通过对 $f'(x)$ 求导得到的：\n$$f''(x) = \\frac{d}{dx}\\left[(x-\\alpha)^{m-1} [m g(x) + (x-\\alpha) g'(x)]\\right]$$\n$$f''(x) = (m-1)(x-\\alpha)^{m-2}[m g(x) + (x-\\alpha) g'(x)] + (x-\\alpha)^{m-1}[m g'(x) + g'(x) + (x-\\alpha) g''(x)]$$\n提出公因子 $(x-\\alpha)^{m-2}$：\n$$f''(x) = (x-\\alpha)^{m-2} \\left[ (m-1)(m g(x) + (x-\\alpha) g'(x)) + (x-\\alpha)((m+1)g'(x) + (x-\\alpha)g''(x)) \\right]$$\n展开并合并同类项：\n$$f''(x) = (x-\\alpha)^{m-2} \\left[ m(m-1)g(x) + (m-1+m+1)(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n$$f''(x) = (x-\\alpha)^{m-2} \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n\n现在我们构造比值 $Q(x)$ 所需的各个部分。\n乘积 $f(x) f''(x)$ 是：\n$$f(x) f''(x) = (x-\\alpha)^m g(x) \\cdot (x-\\alpha)^{m-2} \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n$$f(x) f''(x) = (x-\\alpha)^{2m-2} g(x) \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n项 $(f'(x))^2$ 是：\n$$\\left(f'(x)\\right)^2 = \\left( (x-\\alpha)^{m-1} [m g(x) + (x-\\alpha) g'(x)] \\right)^2$$\n$$\\left(f'(x)\\right)^2 = (x-\\alpha)^{2m-2} [m g(x) + (x-\\alpha) g'(x)]^2$$\n\n对于 $x \\ne \\alpha$，我们可以构建比值 $Q(x)$ 并消去公因子 $(x-\\alpha)^{2m-2}$：\n$$Q(x) = \\frac{g(x) \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]}{[m g(x) + (x-\\alpha) g'(x)]^2}$$\n\n为了求出 $x \\to \\alpha$ 时的极限，我们利用 $g(x)$、$g'(x)$ 和 $g''(x)$ 的连续性。所有包含因子 $(x-\\alpha)$ 的项都将趋于零。\n$$\\lim_{x\\to \\alpha} Q(x) = \\frac{g(\\alpha) \\left[ m(m-1)g(\\alpha) + 2m(0)g'(\\alpha) + (0)^2 g''(\\alpha) \\right]}{[m g(\\alpha) + (0) g'(\\alpha)]^2}$$\n$$\\lim_{x\\to \\alpha} Q(x) = \\frac{g(\\alpha) [m(m-1)g(\\alpha)]}{[m g(\\alpha)]^2} = \\frac{m(m-1)g(\\alpha)^2}{m^2 g(\\alpha)^2}$$\n由于 $m \\ge 1$ 且 $g(\\alpha) \\ne 0$，我们可以简化此表达式。当 $m=1$ 时，分子为 $0$，因此极限为 $0$。当 $m  1$ 时，我们可以消去 $m g(\\alpha)^2$。最终得到的公式对所有 $m \\ge 1$ 都成立：\n$$\\lim_{x\\to \\alpha} \\frac{f(x) f''(x)}{\\left(f'(x)\\right)^2} = \\frac{m-1}{m} = 1 - \\frac{1}{m}$$\n\n### 第2部分：重数估计量\n\n推导出的极限建立了重数 $m$ 与比值 $Q(x)$ 的渐近值之间的关系。设 $L = \\lim_{x\\to \\alpha} Q(x)$。我们有 $L = 1 - \\frac{1}{m}$。我们可以解这个方程得到 $m$：\n$$\\frac{1}{m} = 1 - L$$\n$$m = \\frac{1}{1 - L}$$\n这启发我们通过将在一个接近但又不等于 $\\alpha$ 的点 $x$ 处计算的比值 $Q(x)$ 替换极限 $L$，从而得到一个重数的逐点估计量 $\\hat{m}(x)$。\n$$\\hat{m}(x) = \\frac{1}{1 - Q(x)} = \\frac{1}{1 - \\frac{f(x) f''(x)}{(f'(x))^2}}$$\n简化 $\\hat{m}(x)$ 的表达式：\n$$\\hat{m}(x) = \\frac{(f'(x))^2}{(f'(x))^2 - f(x) f''(x)}$$\n这是一个用于重数估计的标准公式，它构成了修正牛顿法的基础，该方法能够为重根恢复二次收敛性。该估计量的合理性在于，当 $x \\to \\alpha$ 时，$\\hat{m}(x) \\to m$。\n\n### 第3部分：数值实验的基本原理\n\n该数值实验旨在量化此估计量在有限精度算术（IEEE 754 双精度）中的偏差。$\\hat{m}(x)$ 中的误差有两个来源：\n1.  **逼近误差**：对于 $x \\ne \\alpha$，$\\hat{m}(x)$ 只是 $m$ 的一个近似。通常 $x$ 离 $\\alpha$ 越远，这个误差就越大。\n2.  **舍入误差**：当 $x$ 非常接近 $\\alpha$ 时，分子 $(f'(x))^2$ 和分母 $(f'(x))^2 - f(x) f''(x)$ 都趋于零。分母涉及两个几乎相等的数相减，因为它们的比值 $\\frac{f(x) f''(x)}{(f'(x))^2} \\to 1 - \\frac{1}{m}$。这种减法会导致灾难性相消，这是浮点算术中相对精度的重大损失。\n\n该实验评估了一系列偏移量 $\\delta$ 对应的 $\\hat{m}(\\alpha+\\delta)$。我们预计会观察到总误差曲线作为 $\\log(\\delta)$ 的函数呈 U 形：当 $\\delta$ 较大时，由于逼近误差导致偏差较高；当 $\\delta$ 非常小时，由于舍入误差导致偏差较高；而在某个中间值 $\\delta$ 处，偏差达到最小值。$m=1$ 的情况是例外，此时项 $f(x)f''(x)$ 趋于零的速度比 $(f'(x))^2$ 更快，因此灾难性相消不是问题，偏差应随 $\\delta$ 单调递减。\n\n下面的 Python 代码实现了这个实验，它通过解析方式计算导数，然后评估估计量，以计算每个测试用例的平均绝对偏差。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a numerical experiment to assess the bias of a multiplicity estimator\n    for roots of a function.\n    \"\"\"\n\n    # Define the set of offsets from the root\n    deltas = np.array([1e-1, 1e-4, 1e-8, 1e-12, 1e-16])\n\n    # Case A: m=3, alpha=0, g(x)=1+x\n    m_a, alpha_a = 3, 0.0\n    g_a = lambda x: 1.0 + x\n    gp_a = lambda x: 1.0\n    gpp_a = lambda x: 0.0\n\n    # Case B: m=2, alpha=0.3, g(x)=e^x\n    m_b, alpha_b = 2, 0.3\n    g_b = lambda x: np.exp(x)\n    gp_b = lambda x: np.exp(x)\n    gpp_b = lambda x: np.exp(x)\n\n    # Case C: m=1, alpha=-0.2, g(x)=1+x^2\n    m_c, alpha_c = 1, -0.2\n    g_c = lambda x: 1.0 + x**2\n    gp_c = lambda x: 2.0 * x\n    gpp_c = lambda x: 2.0\n\n    # Case D: m=5, alpha=0, g(x)=cos(x)\n    m_d, alpha_d = 5, 0.0\n    g_d = lambda x: np.cos(x)\n    gp_d = lambda x: -np.sin(x)\n    gpp_d = lambda x: -np.cos(x)\n\n    test_cases = [\n        (m_a, alpha_a, g_a, gp_a, gpp_a),\n        (m_b, alpha_b, g_b, gp_b, gpp_b),\n        (m_c, alpha_c, g_c, gp_c, gpp_c),\n        (m_d, alpha_d, g_d, gp_d, gpp_d),\n    ]\n\n    def calculate_m_hat(m, alpha, g, gp, gpp, x):\n        \"\"\"\n        Calculates the estimated multiplicity m_hat at a point x.\n        \"\"\"\n        xa = x - alpha\n        \n        gx = g(x)\n        gpx = gp(x)\n        gppx = gpp(x)\n\n        # f(x) = (x-alpha)^m * g(x)\n        f = (xa**m) * gx\n        \n        # f'(x) = (x-alpha)^(m-1) * [m*g(x) + (x-alpha)*g'(x)]\n        fp = (xa**(m-1)) * (m * gx + xa * gpx)\n        \n        # f''(x) is computed based on m to ensure numerical stability.\n        # For m=1, the general formula involves 0 * inf which can be NaN.\n        # f''(x) = (x-alpha)^(m-2) * [m(m-1)g + 2m(x-alpha)g' + (x-alpha)^2 g'']\n        if m == 1:\n            # Simplified formula for m=1: f''(x) = 2g'(x) + (x-alpha)g''(x)\n            fpp = 2.0 * gpx + xa * gppx\n        else:\n            fpp = (xa**(m - 2)) * (m * (m - 1) * gx + xa * (2 * m * gpx + xa * gppx))\n\n        # Estimator: m_hat = (f')^2 / ((f')^2 - f*f'')\n        numerator = fp**2\n        denominator = fp**2 - f * fpp\n        \n        if denominator == 0.0:\n            # Handle cases where the denominator is exactly zero.\n            return np.nan if numerator == 0.0 else np.inf\n\n        return numerator / denominator\n\n    all_mean_biases = []\n    for m, alpha, g, gp, gpp in test_cases:\n        biases = []\n        for delta in deltas:\n            x = alpha + delta\n            m_hat = calculate_m_hat(m, alpha, g, gp, gpp, x)\n            bias = np.abs(m_hat - m)\n            biases.append(bias)\n        \n        mean_abs_bias = np.mean(biases)\n        all_mean_biases.append(mean_abs_bias)\n\n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, all_mean_biases))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们已经了解了标准牛顿法为何在重根处会失效，并探索了如何估计根的重数。现在，是时候将这些知识融会贯通，构建一个更“智能”的求解器了。这项综合性练习将挑战你设计一个混合算法，它能像数值分析专家一样工作：从标准方法开始，实时监测其收敛行为，一旦发现收敛缓慢，便能自动诊断原因（即存在重根），并切换到更合适的修正策略以加速求解。",
            "id": "3253971",
            "problem": "设计并实现一个完整、可运行的程序，用于构建一个基于牛顿法的标量非线性方程混合求根算法。该算法必须以标准牛顿迭代开始，并在运行期间自主检测观测到的渐近收敛是否呈线性，根据观测到的迭代值估计根的重数 $m$，如果估计的重数表明存在重根，则切换到一种能补偿重数影响的修正牛顿策略。你可以假定的基础理论包括：对于一个足够光滑的函数，其根的重数的定义；用于单根的标准牛顿迭代；以及光滑函数的泰勒展开的基本性质。你不得先验地假设任何专门的重数加速公式；相反，必须从重根的基本模型以及牛顿迭代在此类根附近的行为中推导出所有必要的关系。\n\n你的程序必须遵循以下要求。\n\n- 输入在程序内部固定；不允许用户输入。你将实现该混合方法并将其应用于指定的测试套件。\n- 从基本定义出发：对于一个足够光滑的函数 $f$，若在点 $r$ 附近可以写作 $f(x) = (x - r)^m g(x)$，其中 $g$ 是光滑函数且 $g(r) \\neq 0$，则称 $r$ 是 $f$ 的一个重数为 $m \\in \\mathbb{N}$ 的根。你也可以假定对于一个可微函数 $f$，在根附近且 $f'(x_k) \\neq 0$ 的情况下，其标准牛顿迭代为 $x_{k+1} = x_k - f(x_k)/f'(x_k)$。\n- 你的算法必须在运行期间判断当前迭代是否呈线性收敛，如果是，则根据不依赖于已知根 $r$ 的可观测量来实时估计重数 $m$。在估计出 $m$ 之后，算法必须从标准牛顿迭代切换到一种能为重根恢复快速收敛的、适当的修正牛顿迭代。\n- 检测必须是稳健的：决策应基于最近几次迭代的一个短窗口，并要求某个可观测比率具有稳定性，以此来判断是否为线性收敛。避免将二次收敛误判为线性收敛。提供合理的数值保护措施。\n- 停止准则必须同时包括函数值容差和步长容差。使用函数值容差 $|f(x_k)| \\le 10^{-12}$，步长容差 $|x_{k+1} - x_k| \\le 10^{-14}$，以及最多100次迭代。角度（若适用）必须以弧度为单位。\n\n测试套件。将你的算法应用于以下每个测试用例；对于每个用例，都指定了函数 $f$、其导数 $f'$ 和初始猜测值 $x_0$。以下所有常数和数值均为精确值，必须按给定值使用。\n\n- 用例 $1$：$f(x) = (x - 2)^3$，$f'(x) = 3(x - 2)^2$，$x_0 = 2.8$。\n- 用例 $2$：$f(x) = (x + 1)^2$，$f'(x) = 2(x + 1)$，$x_0 = -1.6$。\n- 用例 $3$：$f(x) = \\sin(x)$，$f'(x) = \\cos(x)$，$x_0 = 0.1$。角度以弧度为单位。\n- 用例 $4$：$f(x) = (x - 0.5)^4$，$f'(x) = 4(x - 0.5)^3$，$x_0 = 1.25$。\n- 用例 $5$：$f(x) = (x - 1)^3 e^{x}$，$f'(x) = e^{x}(x - 1)^2(x + 2)$，$x_0 = 1.3$。\n- 用例 $6$：$f(x) = x^3 - x - 2$，$f'(x) = 3x^2 - 1$，$x_0 = 1.0$。\n\n对于每个测试用例，你的程序必须返回一个包含恰好三个值的列表：\n- 最终估计的重数 $m$，为一个非负整数（如果方法确定无需切换，则使用 $m = 1$），\n- 一个整数指示符，如果算法至少切换到修正牛顿阶段一次，则为 $1$，否则为 $0$，\n- 直到收敛或达到最大迭代次数所执行的总迭代次数。\n\n最终输出格式。你的程序应生成单行输出，其中包含6个测试用例的结果，形式为一个用方括号括起来的逗号分隔列表，其中每个元素本身是一个不含空格的三元素列表。例如，输出必须类似于 $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots,[a_6,b_6,c_6]]$，其中每个 $a_i$、$b_i$ 和 $c_i$ 都是如上所述的整数。",
            "solution": "我们从定义和局部模型开始。设 $f$ 是一个足够光滑的标量函数，其根 $r$ 的重数为 $m \\in \\mathbb{N}$。根据定义，存在一个光滑函数 $g$，满足 $g(r) \\neq 0$，使得在 $r$ 的一个邻域内有 $f(x) = (x - r)^m g(x)$。求解 $f(x) = 0$ 的标准牛顿迭代是 $x_{k+1} = x_k - f(x_k)/f'(x_k)$，假设 $f'(x_k) \\neq 0$。我们用 $e_k = x_k - r$ 表示误差。\n\n重根附近的局部行为。使用 $f(x) = (x - r)^m g(x)$ 且 $g(r) \\neq 0$，我们有 $f'(x) = m(x - r)^{m-1} g(x) + (x - r)^m g'(x)$。在 $x = r$ 处，如果 $m \\ge 2$，则 $f'(r) = 0$，这解释了标准牛顿法为何从二次收敛退化为线性收敛。在 $r$ 附近使用 $g(x) \\approx g(r)$ 进行一阶近似，可以得到\n$$\n\\frac{f(x)}{f'(x)} \\approx \\frac{(x - r)^m g(r)}{m (x - r)^{m-1} g(r)} = \\frac{x - r}{m}.\n$$\n代入牛顿更新公式可得\n$$\nx_{k+1} \\approx x_k - \\frac{e_k}{m} = r + \\left(1 - \\frac{1}{m}\\right) e_k.\n$$\n因此，标准牛顿法在重根处的渐近误差递推关系为\n$$\ne_{k+1} \\approx q e_k, \\quad q = \\frac{m - 1}{m},\n$$\n这是收敛率为 $q \\in (0,1)$ 的线性收敛（当 $m \\ge 2$ 时）。对于 $m = 1$ 的单根，一个更精确的泰勒展开显示 $e_{k+1} \\approx C e_k^2$，其中某个常数 $C \\neq 0$，即二次收敛。\n\n从观测到的迭代值估计重数。由于真实的根 $r$ 是未知的，误差 $e_k$ 无法直接获得。因此我们改用观测到的牛顿步长\n$$\ns_k = x_k - x_{k+1}.\n$$\n由 $e_{k+1} \\approx q e_k$ 可得\n$$\ns_k = x_k - x_{k+1} = e_k - e_{k+1} \\approx (1 - q) e_k.\n$$\n类似地，$s_{k+1} \\approx (1 - q) e_{k+1} \\approx (1 - q) q e_k$。因此，连续步长大小的比率将稳定到\n$$\n\\frac{|s_{k+1}|}{|s_k|} \\approx q = \\frac{m - 1}{m}.\n$$\n这为检测线性收敛提供了一个可观测的判据：如果连续步长大小的比率稳定到一个在 $(0,1)$ 区间内且远离零的值，那么迭代行为是线性的而不是二次的。此外，我们可以通过对最近比率的一个短窗口应用一个稳健的统计量（例如中位数）来估计 $q$，然后推断出\n$$\nm \\approx \\frac{1}{1 - q}.\n$$\n四舍五入到最近的且不小于2的整数，得到一个整数重数估计值 $\\widehat{m}$，我们可以将其用于修正的迭代中。\n\n修正的牛顿迭代。基本局部模型 $f(x) = (x - r)^m g(x)$ 且 $g(r) \\neq 0$ 暗示可以通过缩放牛顿修正量来补偿重根的影响。考虑修正的更新\n$$\nx_{k+1} = x_k - \\widehat{m} \\frac{f(x_k)}{f'(x_k)}.\n$$\n代入 $f/f'$ 的局部近似可得\n$$\nx_{k+1} \\approx x_k - \\widehat{m} \\frac{e_k}{m} = r + \\left(1 - \\frac{\\widehat{m}}{m}\\right) e_k.\n$$\n如果 $\\widehat{m} = m$，一阶项会消失，泰勒展开中的下一项将决定一个更高阶（通常是二次）的收敛。因此，一旦获得一个可靠的估计值 $\\widehat{m}$，切换到修正的更新就能恢复快速收敛。\n\n检测逻辑与保护措施。为避免将二次收敛（单根）误判为线性收敛，我们施加以下条件：\n- 稳定性要求：在最近比率 $r_i = |s_{i+1}|/|s_i|$ 的窗口内，它们的相对变化必须很小；\n- 幅值要求：稳定后的比率必须明显远离零，例如 $r_i \\in [\\rho_{\\min}, \\rho_{\\max}]$ 且 $\\rho_{\\min}  0$；\n- 一致性要求：窗口内没有明显的下降趋势，因为二次收敛会使比率趋向于零。\n\n有了这些措施，我们计算 $\\widehat{q}$ 作为窗口内比率的中位数，并设置 $\\widehat{m} = \\max\\{2, \\min\\{M_{\\max}, \\mathrm{round}(1/(1 - \\widehat{q}))\\}\\}$，其中为了稳健性设置一个合理的上限 $M_{\\max}$。每次运行只切换一次，并且如果未满足标准则不进行切换。\n\n停止准则。如果 $|f(x_k)| \\le 10^{-12}$ 或 $|x_{k+1} - x_k| \\le 10^{-14}$，或者 $k$ 达到 $100$，我们就停止。我们还通过在 $|f'(x_k)|$ 过小时平稳中止来防范导数接近零的情况。\n\n算法摘要。\n- 以标准牛顿更新 $x_{k+1} = x_k - f(x_k)/f'(x_k)$ 初始化。\n- 维护最近几个步长比率 $|s_{k+1}|/|s_k|$ 的一个滑动窗口。\n- 如果比率在一个容差范围内稳定下来，并位于一个预设区间内，则判断为线性收敛并估计 $\\widehat{m} \\approx 1/(1 - \\widehat{q})$。\n- 如果 $\\widehat{m} \\ge 2$，则切换到修正的更新 $x_{k+1} = x_k - \\widehat{m} f(x_k)/f'(x_k)$ 并继续迭代直至收敛。\n- 对每个测试用例，输出最终的 $\\widehat{m}$（若未切换则为1）、一个指示是否发生切换的标志，以及总迭代次数。\n\n应用于测试套件。这些用例包括已知重数的重根：\n- $(x - 2)^3$，重数 $m = 3$，\n- $(x + 1)^2$，重数 $m = 2$，\n- $(x - 0.5)^4$，重数 $m = 4$，\n- $(x - 1)^3 e^{x}$，重数 $m = 3$，\n以及两个单根用例：\n- 在 $x = 0$ 处的 $\\sin(x)$，\n- 在唯一实根 $x \\approx 1.521\\dots$ 附近的 $x^3 - x - 2$。\n对于重根，步长比率会稳定到 $q = (m - 1)/m$，算法会相应地估计 $\\widehat{m}$，然后切换方法，并实现快速收敛。对于单根，步长比率会趋向于零，而不会在远离零的位置稳定下来，因此算法不会切换，且 $\\widehat{m} = 1$。最终程序为每个用例计算所要求的三元组，并以指定的格式将其打印为单个带括号的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hybrid_newton(f, df, x0, tol_f=1e-12, tol_x=1e-14, max_iter=100):\n    \"\"\"\n    Hybrid Newton solver:\n    - Starts with standard Newton.\n    - Detects linear convergence via stabilization of step ratios.\n    - Estimates multiplicity m_hat = round(1/(1 - q_hat)) from ratios and switches to modified Newton.\n    Returns: (m_hat_final, switched_flag, iterations)\n    \"\"\"\n    x = float(x0)\n    switched = False\n    m_hat = 1  # default multiplicity estimate\n    step_history = []\n    ratio_window = []\n    window_size = 4\n\n    # thresholds for detection of linear convergence\n    rho_min = 0.15   # away from zero to avoid misclassifying quadratic convergence\n    rho_max = 0.98   # bounded away from 1 to avoid extremely slow cases\n    stab_rel_tol = 0.03  # max relative spread across the window\n    deriv_eps = 1e-16\n\n    it = 0\n    # We'll run at most max_iter iterations\n    while it  max_iter:\n        fx = f(x)\n        dfx = df(x)\n        if abs(fx) = tol_f:\n            break\n        if abs(dfx)  deriv_eps:\n            # Derivative too small, abort iteration\n            break\n\n        # Choose the update depending on switch state\n        if not switched:\n            delta = -fx / dfx\n        else:\n            delta = -m_hat * fx / dfx\n\n        x_new = x + delta\n        it += 1\n\n        # Check step tolerance\n        if abs(delta) = tol_x:\n            x = x_new\n            break\n\n        # Collect step and ratio info only during standard Newton before switching\n        if not switched:\n            step_history.append(abs(delta))\n            # compute ratio if possible\n            if len(step_history) >= 2:\n                r = step_history[-1] / step_history[-2] if step_history[-2] != 0.0 else np.inf\n                ratio_window.append(r)\n                if len(ratio_window) > window_size:\n                    ratio_window.pop(0)\n\n                # Detection: require full window\n                if len(ratio_window) == window_size:\n                    # Check bounds and stability\n                    ratios = np.array(ratio_window)\n                    # Ensure all ratios in [rho_min, rho_max]\n                    in_range = np.all((ratios >= rho_min)  (ratios = rho_max)  np.isfinite(ratios))\n                    # Stability via relative spread: (max - min)/max = tol\n                    rel_spread = (ratios.max() - ratios.min()) / max(ratios.max(), 1e-300)\n                    stable = rel_spread = stab_rel_tol\n                    # Optional: ensure no strong downward trend\n                    non_decreasingish = ratios[-1] >= 0.9 * ratios[0]\n                    if in_range and stable and non_decreasingish:\n                        q_hat = float(np.median(ratios))\n                        if q_hat  1.0:\n                            m_est = 1.0 / (1.0 - q_hat)\n                            # Round to nearest integer and clamp to [2, 20]\n                            m_hat_candidate = int(max(2, min(20, int(np.floor(m_est + 0.5)))))\n                            # Switch only if candidate >= 2\n                            if m_hat_candidate >= 2:\n                                m_hat = m_hat_candidate\n                                switched = True\n                                # Clear windows to avoid re-triggering\n                                ratio_window.clear()\n                                step_history.clear()\n\n        # Advance iteration\n        x = x_new\n\n    # Ensure m_hat reported as 1 if no switch occurred\n    if not switched:\n        m_hat_report = 1\n    else:\n        m_hat_report = int(m_hat)\n\n    return (int(m_hat_report), int(1 if switched else 0), int(it))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (f, df, x0)\n    def f1(x): return (x - 2.0) ** 3\n    def df1(x): return 3.0 * (x - 2.0) ** 2\n\n    def f2(x): return (x + 1.0) ** 2\n    def df2(x): return 2.0 * (x + 1.0)\n\n    def f3(x): return np.sin(x)\n    def df3(x): return np.cos(x)\n\n    def f4(x): return (x - 0.5) ** 4\n    def df4(x): return 4.0 * (x - 0.5) ** 3\n\n    def f5(x): return ((x - 1.0) ** 3) * np.exp(x)\n    def df5(x): return np.exp(x) * (x - 1.0) ** 2 * (x + 2.0)\n\n    def f6(x): return x ** 3 - x - 2.0\n    def df6(x): return 3.0 * x ** 2 - 1.0\n\n    test_cases = [\n        (f1, df1, 2.8),\n        (f2, df2, -1.6),\n        (f3, df3, 0.1),\n        (f4, df4, 1.25),\n        (f5, df5, 1.3),\n        (f6, df6, 1.0),\n    ]\n\n    results = []\n    for f, df, x0 in test_cases:\n        m_hat, switched, iters = hybrid_newton(f, df, x0, tol_f=1e-12, tol_x=1e-14, max_iter=100)\n        results.append([m_hat, switched, iters])\n\n    # Format without spaces as required: [[a,b,c],[...],...]\n    inner = \",\".join(\"[\" + \",\".join(str(v) for v in triple) + \"]\" for triple in results)\n    print(f\"[{inner}]\")\n\nsolve()\n```"
        }
    ]
}