{
    "hands_on_practices": [
        {
            "introduction": "Understanding a transformation is often best tested by trying to reverse it. This exercise challenges you to do just that by reconstructing an original symmetric matrix from its final tridiagonal form and the sequence of Householder vectors used in the reduction. Successfully solving this puzzle  demonstrates a deep understanding of the geometric nature of Householder reflections and the properties of similarity transformations, such as the preservation of eigenvalues and determinants.",
            "id": "3239697",
            "problem": "A real symmetric matrix $A \\in \\mathbb{R}^{4 \\times 4}$ is reduced to a real symmetric tridiagonal matrix $T$ by a sequence of two Householder reflections constructed from unit vectors $v_1$ and $v_2$. A Householder reflector $H(v)$ associated with a unit vector $v \\in \\mathbb{R}^n$ is defined by $H(v) = I - 2 v v^{\\top}$, where $I$ denotes the identity matrix. Orthogonal similarity transformations preserve symmetry. You are given\n$$\nT = \\begin{pmatrix}\n4 & 1 & 0 & 0 \\\\\n1 & 3 & 2 & 0 \\\\\n0 & 2 & 5 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix}, \\quad\nv_1 = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}, \\quad\nv_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}.\n$$\nReconstruct the original symmetric matrix $A$ consistent with this reduction and then compute the determinant of $A$. Express your final answer as an exact real number (no rounding).",
            "solution": "The problem requires the reconstruction of a real symmetric matrix $A \\in \\mathbb{R}^{4 \\times 4}$ from its tridiagonal form $T$, which was obtained through a sequence of two Householder reflections. Subsequently, the determinant of $A$ must be computed.\n\nThe process of tridiagonalization of a symmetric matrix $A$ using Householder transformations is a sequence of similarity transformations. For a $4 \\times 4$ matrix, two steps are required. The process can be written as:\n$A^{(1)} = A$\n$A^{(2)} = H_1 A^{(1)} H_1$\n$T = A^{(3)} = H_2 A^{(2)} H_2 = H_2 (H_1 A H_1) H_2 = H_2 H_1 A H_1 H_2$\nHere, $H_1$ and $H_2$ are Householder matrices constructed from the given unit vectors $v_1$ and $v_2$, respectively. A Householder matrix $H(v) = I - 2vv^\\top$ for a unit vector $v$ is both symmetric ($H^\\top = H$) and orthogonal ($H^{-1} = H^\\top = H$).\n\nTo reconstruct the original matrix $A$, we can reverse the transformation sequence:\n$A = H_1 H_2 T H_2 H_1$.\n\nFirst, we construct the Householder matrices $H_1$ and $H_2$.\nThe first unit vector is $v_1 = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}$.\nThe outer product $v_1 v_1^\\top$ is:\n$$v_1 v_1^\\top = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}$$\nThe Householder matrix $H_1$ is:\n$$H_1 = I - 2v_1v_1^\\top = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} - 2\\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & -1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}$$\nThe second unit vector is $v_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}$.\nThe outer product $v_2 v_2^\\top$ is:\n$$v_2 v_2^\\top = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 0 & 0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}$$\nThe Householder matrix $H_2$ is:\n$$H_2 = I - 2v_2v_2^\\top = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} - 2\\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 0 & \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & -1 \\\\ 0 & 0 & -1 & 0 \\end{pmatrix}$$\nNow we compute $A = H_1 H_2 T H_2 H_1$ with the given matrix $T$:\n$$T = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 2 & 0 \\\\ 0 & 2 & 5 & -1 \\\\ 0 & 0 & -1 & 2 \\end{pmatrix}$$\nLet's first compute the intermediate matrix $T' = H_2 T H_2$:\n$$H_2 T = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & -1 \\\\ 0 & 0 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 2 & 0 \\\\ 0 & 2 & 5 & -1 \\\\ 0 & 0 & -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 2 & 0 \\\\ 0 & 0 & 1 & -2 \\\\ 0 & -2 & -5 & 1 \\end{pmatrix}$$\n$$T' = (H_2 T) H_2 = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 2 & 0 \\\\ 0 & 0 & 1 & -2 \\\\ 0 & -2 & -5 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & -1 \\\\ 0 & 0 & -1 & 0 \\end{pmatrix} = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 0 & -2 \\\\ 0 & 0 & 2 & -1 \\\\ 0 & -2 & -1 & 5 \\end{pmatrix}$$\nNow, we compute $A = H_1 T' H_1$:\n$$H_1 T' = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & -1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 0 & -2 \\\\ 0 & 0 & 2 & -1 \\\\ 0 & -2 & -1 & 5 \\end{pmatrix} = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 0 & 0 & -2 & 1 \\\\ -1 & -3 & 0 & 2 \\\\ 0 & -2 & -1 & 5 \\end{pmatrix}$$\n$$A = (H_1 T') H_1 = \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 0 & 0 & -2 & 1 \\\\ -1 & -3 & 0 & 2 \\\\ 0 & -2 & -1 & 5 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & -1 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 & -1 & 0 \\\\ 0 & 2 & 0 & 1 \\\\ -1 & 0 & 3 & 2 \\\\ 0 & 1 & 2 & 5 \\end{pmatrix}$$\nThis is the reconstructed original symmetric matrix $A$.\n\nThe final step is to compute the determinant of $A$. Since $A$ and $T$ are related by a similarity transformation (because $H_1$ and $H_2$ are orthogonal), their determinants are equal.\n$\\det(A) = \\det(H_1 H_2 T H_2 H_1)$. Let $Q = H_1 H_2$. $Q$ is orthogonal, so $Q^{-1} = Q^\\top = (H_1 H_2)^\\top = H_2^\\top H_1^\\top = H_2 H_1$. Thus $A = Q T Q^{-1}$.\n$\\det(A) = \\det(Q T Q^{-1}) = \\det(Q) \\det(T) \\det(Q^{-1}) = \\det(T)$.\nThis allows us to compute the determinant from the simpler tridiagonal matrix $T$.\n\nCalculating $\\det(T)$:\n$$\\det(T) = \\det \\begin{pmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 3 & 2 & 0 \\\\ 0 & 2 & 5 & -1 \\\\ 0 & 0 & -1 & 2 \\end{pmatrix}$$\nWe use cofactor expansion along the first row:\n$$\\det(T) = 4 \\cdot \\det \\begin{pmatrix} 3 & 2 & 0 \\\\ 2 & 5 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} - 1 \\cdot \\det \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 5 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix}$$\nThe first $3 \\times 3$ determinant is:\n$$\\det \\begin{pmatrix} 3 & 2 & 0 \\\\ 2 & 5 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} = 3(5 \\cdot 2 - (-1)(-1)) - 2(2 \\cdot 2 - 0) = 3(10 - 1) - 2(4) = 3(9) - 8 = 27 - 8 = 19$$\nThe second $3 \\times 3$ determinant is:\n$$\\det \\begin{pmatrix} 1 & 2 & 0 \\\\ 0 & 5 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} = 1(5 \\cdot 2 - (-1)(-1)) = 10 - 1 = 9$$\nTherefore, the determinant of $T$ is:\n$$\\det(T) = 4(19) - 1(9) = 76 - 9 = 67$$\nThus, $\\det(A) = 67$.\n\nAs a verification, we can compute the determinant of the reconstructed matrix $A$ directly:\n$$\\det(A) = \\det \\begin{pmatrix} 4 & 0 & -1 & 0 \\\\ 0 & 2 & 0 & 1 \\\\ -1 & 0 & 3 & 2 \\\\ 0 & 1 & 2 & 5 \\end{pmatrix}$$\nUsing cofactor expansion along the first row, the full expression is $\\sum_{j} a_{1j}C_{1j} = a_{11}C_{11} + a_{13}C_{13}$.\n$$\\det(A) = 4 \\cdot \\det \\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 3 & 2 \\\\ 1 & 2 & 5 \\end{pmatrix} + (-1) \\cdot \\det \\begin{pmatrix} 0 & 2 & 1 \\\\ -1 & 0 & 2 \\\\ 0 & 1 & 5 \\end{pmatrix}$$\nThe first $3 \\times 3$ determinant is:\n$$\\det \\begin{pmatrix} 2 & 0 & 1 \\\\ 0 & 3 & 2 \\\\ 1 & 2 & 5 \\end{pmatrix} = 2(3 \\cdot 5 - 2 \\cdot 2) + 1(0 \\cdot 2 - 3 \\cdot 1) = 2(15 - 4) - 3 = 2(11) - 3 = 22 - 3 = 19$$\nThe second $3 \\times 3$ determinant is calculated using cofactor expansion along its first column:\n$$\\det \\begin{pmatrix} 0 & 2 & 1 \\\\ -1 & 0 & 2 \\\\ 0 & 1 & 5 \\end{pmatrix} = -(-1) \\det \\begin{pmatrix} 2 & 1 \\\\ 1 & 5 \\end{pmatrix} = 10 - 1 = 9$$\nSo, the determinant of $A$ is:\n$$\\det(A) = 4(19) + (-1)(9) = 76 - 9 = 67$$\nThe results match, confirming the correctness of the calculations.",
            "answer": "$$\\boxed{67}$$"
        },
        {
            "introduction": "Translating mathematical algorithms into correct and robust code is a fundamental skill in scientific computing. This practice  presents a common implementation pitfall where an incomplete update rule leads to a symmetric but non-tridiagonal result. By implementing both the flawed and correct procedures, you will gain firsthand experience in debugging and appreciate the importance of applying the full similarity transformation at every step.",
            "id": "3239695",
            "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$. A sequence of orthogonal similarity transformations using Householder reflectors can reduce $A$ to a symmetric tridiagonal matrix $T$ without altering its eigenvalues. A Householder reflector acting on a vector $x \\in \\mathbb{R}^{m}$ is defined by choosing $u = x - \\alpha e_1$ where $e_1$ is the first standard basis vector, and $\\alpha = -\\operatorname{sign}(x_1)\\lVert x \\rVert_2$. The vector $v = \\dfrac{u}{\\lVert u \\rVert_2}$ defines the reflector $H = I_m - 2 v v^\\top$, which satisfies $H x = \\alpha e_1$, $H^\\top = H$, and $H$ is orthogonal. The symmetric tridiagonalization applies a similarity at each step $k$ by a block-diagonal orthogonal matrix $Q_k = \\operatorname{diag}(I_{k+1}, H_k)$, where $H_k$ acts on the trailing subvector of the $k$-th column of $A$ below the diagonal. The exact similarity is $A \\leftarrow Q_k A Q_k^\\top$. When correctly implemented for $k = 0, 1, \\dots, n-3$, this produces a matrix $T$ where all entries $T_{ij}$ with $|i - j| > 1$ are exactly zero in exact arithmetic and are numerically negligible in floating-point arithmetic.\n\nA common implementation bug is to update only the trailing principal submatrix $A_{22}$ via $A_{22} \\leftarrow H_k A_{22} H_k$ while neglecting the off-diagonal coupling blocks $A_{12}$ and $A_{21}$. To see why this is incorrect, write the block partitioning at step $k$ as\n$$\nA = \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}, \\quad Q_k = \\begin{bmatrix}\nI_{k+1} & 0 \\\\\n0 & H_k\n\\end{bmatrix}.\n$$\nThen the mathematically correct similarity gives\n$$\nQ_k A Q_k^\\top = \n\\begin{bmatrix}\nA_{11} & A_{12} H_k \\\\\nH_k A_{21} & H_k A_{22} H_k\n\\end{bmatrix}.\n$$\nIf one updates only $A_{22}$ and leaves $A_{12}$ and $A_{21}$ unchanged, the global similarity is not performed, and fill-in outside the tridiagonal band persists. The resulting matrix can remain symmetric but not be perfectly tridiagonal.\n\nTask: Implement both the faulty and the corrected Householder tridiagonalization procedures. The faulty procedure must perform only the trailing principal submatrix update at each step, while the corrected procedure must apply the full block similarity to all involved blocks. Implement a function to count how many entries of the final matrix lie outside the tridiagonal band with magnitude exceeding a threshold $\\varepsilon$. Use the threshold $\\varepsilon = 10^{-10}$.\n\nYour program must process the following test suite of symmetric matrices:\n- Test case $1$: $A_1 \\in \\mathbb{R}^{5 \\times 5}$,\n$$\nA_1 =\n\\begin{bmatrix}\n6 & -2 & 3 & 0 & 1 \\\\\n-2 & 5 & 2 & -1 & 4 \\\\\n3 & 2 & 4 & 2 & 0 \\\\\n0 & -1 & 2 & 3 & -2 \\\\\n1 & 4 & 0 & -2 & 7\n\\end{bmatrix}.\n$$\n- Test case $2$: $A_2 \\in \\mathbb{R}^{2 \\times 2}$,\n$$\nA_2 =\n\\begin{bmatrix}\n2 & -1 \\\\\n-1 & 3\n\\end{bmatrix}.\n$$\n- Test case $3$: $A_3 \\in \\mathbb{R}^{1 \\times 1}$,\n$$\nA_3 =\n\\begin{bmatrix}\n5\n\\end{bmatrix}.\n$$\n- Test case $4$: $A_4 \\in \\mathbb{R}^{6 \\times 6}$ already tridiagonal,\n$$\nA_4 =\n\\begin{bmatrix}\n4 & 1 & 0 & 0 & 0 & 0 \\\\\n1 & 5 & -2 & 0 & 0 & 0 \\\\\n0 & -2 & 6 & 3 & 0 & 0 \\\\\n0 & 0 & 3 & 7 & -4 & 0 \\\\\n0 & 0 & 0 & -4 & 8 & 5 \\\\\n0 & 0 & 0 & 0 & 5 & 9\n\\end{bmatrix}.\n$$\n- Test case $5$: $A_5 \\in \\mathbb{R}^{5 \\times 5}$ tridiagonal with tiny off-tridiagonal perturbations of size $10^{-12}$,\n$$\nA_5 =\n\\begin{bmatrix}\n10 & 2 & 0 & 10^{-12} & 0 \\\\\n2 & 9 & -1 & 0 & 10^{-12} \\\\\n0 & -1 & 8 & 3 & 0 \\\\\n10^{-12} & 0 & 3 & 7 & 2 \\\\\n0 & 10^{-12} & 0 & 2 & 6\n\\end{bmatrix}.\n$$\n\nDefine a function that, for a given matrix $T$ and threshold $\\varepsilon$, counts the number of entries $T_{ij}$ with $|i-j| > 1$ and $|T_{ij}| > \\varepsilon$. For each test case, produce two integers: the count from the faulty implementation and the count from the corrected implementation.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-integer list for a test case. For example, the output should look like $[[b_1,f_1],[b_2,f_2],\\dots]$, where $b_k$ is the faulty count and $f_k$ is the corrected count for test case $k$. No physical units or angle units are involved in this problem.",
            "solution": "The user's request is to implement and compare two versions of the Householder tridiagonalization algorithm for a real symmetric matrix $A$: a mathematically correct version and a common faulty version.\n\n### 1. Problem Validation\n\nThe problem statement has been meticulously validated.\n\n- **Extraction of Givens**: All definitions, constants, matrices, and procedural descriptions were extracted verbatim.\n- **Scientific Grounding**: The problem is well-grounded in the theory of numerical linear algebra. The description of Householder reflectors and the tridiagonalization process is standard and mathematically sound. The \"faulty\" algorithm represents a plausible and illustrative implementation error.\n- **Well-Posedness**: The task is clearly defined. The inputs are specific matrices, and the output is a quantitative comparison (counts of off-tridiagonal elements) based on a precise threshold. The problem is self-contained and free of contradictions or ambiguities.\n- **Verdict**: The problem is valid and can be solved as stated.\n\n### 2. Householder Transformation\n\nThe core of the algorithm is the Householder transformation. For a given vector $x \\in \\mathbb{R}^m$, a Householder reflector $H$ is an orthogonal matrix that transforms $x$ into a multiple of the first standard basis vector $e_1$. The construction, as defined in the problem, is as follows:\n1.  Calculate a scalar $\\alpha = -\\operatorname{sign}(x_1) \\lVert x \\rVert_2$. To handle the case $x_1=0$ robustly and maintain numerical stability (avoiding subtractive cancellation), we define $\\operatorname{sign}(0)=+1$.\n2.  Form the vector $u = x - \\alpha e_1$.\n3.  Normalize $u$ to get the direction vector $v = \\frac{u}{\\lVert u \\rVert_2}$.\n4.  The reflector is given by the matrix $H = I_m - 2 v v^\\top$.\n\nThis matrix $H$ is symmetric ($H=H^\\top$) and orthogonal ($H^\\top H = I_m$). When applied to $x$, it yields $H x = \\alpha e_1$.\n\n### 3. Tridiagonalization Algorithm\n\nThe process reduces a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ to a symmetric tridiagonal matrix $T$ by applying a sequence of similarity transformations. The algorithm proceeds in steps, $k=0, 1, \\dots, n-3$. At each step $k$, the goal is to introduce zeros in the $k$-th column below the first subdiagonal element, and symmetrically in the $k$-th row.\n\nLet $A^{(k)}$ be the matrix at the beginning of step $k$ (with $A^{(0)}=A$).\n1.  Extract the vector $x = A^{(k)}[k+1:n, k]$. This vector has dimension $m=n-(k+1)$.\n2.  Construct the $m \\times m$ Householder reflector $H_k$ for this vector $x$.\n3.  Form the full transformation matrix $Q_k = \\begin{bmatrix} I_{k+1} & 0 \\\\ 0 & H_k \\end{bmatrix}$.\n4.  Apply the similarity transformation: $A^{(k+1)} = Q_k A^{(k)} Q_k^{\\top}$.\n\nSince $H_k$ is symmetric, $Q_k$ is also symmetric, so $A^{(k+1)} = Q_k A^{(k)} Q_k$.\n\n### 4. Correct vs. Faulty Implementation\n\nThe crucial difference between the two implementations lies in how the similarity transformation $A \\leftarrow Q_k A Q_k$ is applied. Using the block-matrix form from the problem statement:\n$$\nA = \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & A_{22}\n\\end{bmatrix}, \\quad Q_k = \\begin{bmatrix}\nI_{k+1} & 0 \\\\\n0 & H_k\n\\end{bmatrix}\n$$\nwhere the partition is after row/column $k$.\n\n**Correct Procedure:** The full similarity transformation must be applied.\n$$\nA \\leftarrow Q_k A Q_k^\\top =\n\\begin{bmatrix}\nA_{11} & A_{12} H_k \\\\\nH_k A_{21} & H_k A_{22} H_k\n\\end{bmatrix}\n$$\nThis involves three updates:\n-   $A_{21} \\leftarrow H_k A_{21}$: This is the key step that zeros out the target elements in the $k$-th column.\n-   $A_{12} \\leftarrow A_{12} H_k$: Symmetrically zeros out elements in the $k$-th row.\n-   $A_{22} \\leftarrow H_k A_{22} H_k$: A similarity transformation on the trailing principal submatrix to maintain the global similarity and preserve eigenvalues.\n\n**Faulty Procedure:** The problem describes a bug where the updates to the off-diagonal blocks $A_{12}$ and $A_{21}$ are neglected.\n$$\nA \\leftarrow \\begin{bmatrix}\nA_{11} & A_{12} \\\\\nA_{21} & H_k A_{22} H_k\n\\end{bmatrix}\n$$\nOnly the trailing submatrix $A_{22}$ is updated. This fails to zero out the elements in column $k$, and since the full similarity transformation is not performed, the resulting matrix is not guaranteed to be tridiagonal. It does, however, remain symmetric.\n\n### 5. Implementation Strategy\n\nFor each test case matrix $A$:\n1.  Two copies, `A_correct` and `A_faulty`, are created.\n2.  The correct and faulty tridiagonalization procedures are applied to their respective matrices. The algorithms iterate from $k=0$ to $n-3$. For matrices of size $n<3$, no transformations are performed.\n3.  Inside the loop for step $k$, the vector $x = A[k+1:n, k]$ is used to construct the Householder reflector $H_k$.\n4.  For `A_correct`, the transformation $H_k$ is applied to the block of rows `A_correct[k+1:n, k:]` from the left, and then to the block of columns `A_correct[:, k+1:n]` from the right. This correctly implements $A \\leftarrow Q_k A Q_k$.\n5.  For `A_faulty`, only the trailing submatrix `A_faulty[k+1:n, k+1:n]` is updated via `Hk @ submatrix @ Hk`.\n6.  Finally, a counting function inspects both resulting matrices. It tallies the number of elements $T_{i,j}$ for which $|i-j| > 1$ and $|T_{i,j}| > \\varepsilon = 10^{-10}$.\n\nThe correct algorithm is expected to produce a count of $0$ for all test cases, as all elements outside the tridiagonal band should be numerically zero. The faulty algorithm is expected to leave non-zero elements in these positions for matrices that are not already tridiagonal.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef count_off_tridiagonal(T, eps):\n    \"\"\"\n    Counts the number of entries T_ij with |i-j| > 1 and |T_ij| > eps.\n    \"\"\"\n    n = T.shape[0]\n    count = 0\n    for i in range(n):\n        for j in range(n):\n            if abs(i - j) > 1 and abs(T[i, j]) > eps:\n                count += 1\n    return count\n\ndef correct_tridiagonalization(A_in):\n    \"\"\"\n    Performs Householder tridiagonalization using the correct similarity transformation.\n    \"\"\"\n    A = A_in.copy()\n    n = A.shape[0]\n\n    for k in range(n - 2):\n        # 1. Define the vector x\n        x = A[k+1:n, k].copy()\n        m = x.shape[0]\n        if m == 0:\n            continue\n        \n        norm_x = np.linalg.norm(x)\n        # If the sub-column is already zero, reflector is identity.\n        if norm_x < 1e-15:\n            continue\n        \n        # 2. Construct the Householder reflector H_k for x\n        # Use copysign for sign(0)=1 behavior, ensuring stability\n        sign_x0 = np.copysign(1.0, x[0]) if x[0] != 0 else 1.0\n        alpha = -sign_x0 * norm_x\n        \n        u = x.copy()\n        u[0] -= alpha\n        \n        norm_u = np.linalg.norm(u)\n        if norm_u < 1e-15:\n            continue\n        v = u / norm_u\n        \n        Hk = np.eye(m) - 2 * np.outer(v, v)\n        \n        # 3. Apply the full similarity transformation A <-- Q_k * A * Q_k^T\n        # This is done by applying H_k to the relevant sub-blocks of A.\n        \n        # Apply H_k from the left: A[k+1:n, :] <-- Hk @ A[k+1:n, :]\n        sub_block_rows = A[k+1:n, k:]\n        A[k+1:n, k:] = Hk @ sub_block_rows\n        \n        # Apply H_k from the right: A[:, k+1:n] <-- A[:, k+1:n] @ Hk\n        sub_block_cols = A[:, k+1:n]\n        A[:, k+1:n] = sub_block_cols @ Hk\n        \n    return A\n\ndef faulty_tridiagonalization(A_in):\n    \"\"\"\n    Performs a faulty Householder tridiagonalization, updating only A_22.\n    \"\"\"\n    A = A_in.copy()\n    n = A.shape[0]\n\n    for k in range(n - 2):\n        # 1. Define the vector x\n        x = A[k+1:n, k].copy()\n        m = x.shape[0]\n        if m == 0:\n            continue\n        \n        norm_x = np.linalg.norm(x)\n        if norm_x < 1e-15:\n            continue\n        \n        # 2. Construct the Householder reflector H_k for x\n        sign_x0 = np.copysign(1.0, x[0]) if x[0] != 0 else 1.0\n        alpha = -sign_x0 * norm_x\n        \n        u = x.copy()\n        u[0] -= alpha\n        \n        norm_u = np.linalg.norm(u)\n        if norm_u < 1e-15:\n            continue\n        v = u / norm_u\n        \n        Hk = np.eye(m) - 2 * np.outer(v, v)\n        \n        # 3. Apply faulty update: only the trailing submatrix A_22 is updated.\n        sub_A22 = A[k+1:n, k+1:n]\n        A[k+1:n, k+1:n] = Hk @ sub_A22 @ Hk\n    \n    return A\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        np.array([\n            [6, -2, 3, 0, 1],\n            [-2, 5, 2, -1, 4],\n            [3, 2, 4, 2, 0],\n            [0, -1, 2, 3, -2],\n            [1, 4, 0, -2, 7]\n        ], dtype=float),\n        np.array([\n            [2, -1],\n            [-1, 3]\n        ], dtype=float),\n        np.array([\n            [5]\n        ], dtype=float),\n        np.array([\n            [4, 1, 0, 0, 0, 0],\n            [1, 5, -2, 0, 0, 0],\n            [0, -2, 6, 3, 0, 0],\n            [0, 0, 3, 7, -4, 0],\n            [0, 0, 0, -4, 8, 5],\n            [0, 0, 0, 0, 5, 9]\n        ], dtype=float),\n        np.array([\n            [10, 2, 0, 1e-12, 0],\n            [2, 9, -1, 0, 1e-12],\n            [0, -1, 8, 3, 0],\n            [1e-12, 0, 3, 7, 2],\n            [0, 1e-12, 0, 2, 6]\n        ], dtype=float)\n    ]\n\n    results = []\n    eps = 1e-10\n\n    for A in test_cases:\n        # Run faulty implementation\n        A_faulty_result = faulty_tridiagonalization(A)\n        count_faulty = count_off_tridiagonal(A_faulty_result, eps)\n\n        # Run corrected implementation\n        A_correct_result = correct_tridiagonalization(A)\n        count_correct = count_off_tridiagonal(A_correct_result, eps)\n        \n        results.append(f\"[{count_faulty},{count_correct}]\")\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practical, large-scale applications, explicitly forming the full $n \\times n$ orthogonal matrix $Q$ from the Householder reflectors is prohibitively expensive in terms of both memory and computation. This advanced practice  guides you through the professional-grade approach: implementing the tridiagonalization while storing the reflectors in a compact format. Mastering this technique of implicit transformation is key to developing efficient numerical linear algebra software.",
            "id": "3239709",
            "problem": "You are given a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$. The goal is to implement symmetric tridiagonalization using Householder reflections while storing the Householder vectors $v_k$ compactly instead of forming the full orthogonal matrix $Q$. The compact storage must use the convention that for each step $k$, the Householder vector $v_k \\in \\mathbb{R}^{n-k-1}$ is scaled so that its first entry is $1$, and only the tail entries $v_k[1:], \\dots, v_k[n-k-2]$ are stored. Additionally, store the corresponding scalar factor $\\tau_k \\in \\mathbb{R}$ for the reflector $H_k = I - \\tau_k v_k v_k^\\top$.\n\nYour program must:\n\n- Implement a function that, given a symmetric matrix $A$, performs Householder tridiagonalization to produce a symmetric tridiagonal matrix $T$, while compactly storing the Householder vectors $v_k$ and their scalars $\\tau_k$ without ever forming the full orthogonal matrix $Q$.\n- Implement functions to apply $Q$ and $Q^\\top$ to any vector $y \\in \\mathbb{R}^n$ using only the stored $v_k$ and $\\tau_k$.\n- Implement a function to reconstruct $Q$ by applying the stored reflectors to the standard basis vectors $e_i$ (for verification purposes only), still without explicitly forming the reflectors as full matrices.\n\nFundamental base to use:\n- Use the Householder reflection definition: for a vector $x \\in \\mathbb{R}^m$, define $v = x - \\alpha e_1$ with $\\alpha = -\\mathrm{sign}(x_1)\\,\\|x\\|_2$, and a scalar $\\tau = \\frac{2}{v^\\top v}$ so that $H = I - \\tau v v^\\top$ satisfies $H x = \\alpha e_1$.\n- Use the two-sided similarity transformation $A \\leftarrow H A H$ with $H$ acting only on the trailing submatrix and the updated row and column, without forming $H$ explicitly.\n\nThe program must be self-contained, use no inputs, and run deterministically. Use the following test suite of parameter values:\n\n- Test case $1$ (boundary condition, trivial size): $A_1 = \\begin{bmatrix} 5 \\end{bmatrix}$.\n- Test case $2$ (small case, explicit values): $A_2 = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}$.\n- Test case $3$ (general case, random but deterministic): Let $B \\in \\mathbb{R}^{5 \\times 5}$ have entries $b_{ij}$ drawn independently from the standard normal distribution with pseudorandom seed $42$, and define $A_3 = \\dfrac{1}{2}(B + B^\\top)$ to make it symmetric.\n- Test case $4$ (already tridiagonal input): $A_4 \\in \\mathbb{R}^{6 \\times 6}$ with diagonal entries $[2,3,4,5,6,7]$ and subdiagonal (and superdiagonal) entries all equal to $1$, and all other entries equal to $0$.\n- Test case $5$ (orthogonality check via compact application): Reuse the reflectors obtained from tridiagonalizing $A_3$, and let $y \\in \\mathbb{R}^5$ be a vector with entries drawn independently from the standard normal distribution with pseudorandom seed $42$.\n\nFor each test case, compute the following quantifiable answers:\n\n- For test case $1$: Return a boolean $r_1$ indicating whether $T_1$ has all off-tridiagonal entries with absolute value less than or equal to $10^{-12}$.\n- For test case $2$: Return a float $r_2 = \\max_{i,j} \\left| \\left(Q_2^\\top A_2 Q_2 - T_2 \\right)_{ij} \\right|$ computed by reconstructing $Q_2$ through applying the stored reflectors to the standard basis vectors. This must not form any Householder matrix explicitly.\n- For test case $3$: Return a float $r_3 = \\max_{|i-j|>1} \\left| (T_3)_{ij} \\right|$.\n- For test case $4$: Return a boolean $r_4$ indicating whether $\\|T_4 - A_4\\|_\\infty \\le 10^{-10}$, where $\\|\\cdot\\|_\\infty$ is the elementwise maximum absolute difference over all entries.\n- For test case $5$: Return a float $r_5 = \\|Q_3^\\top (Q_3 y) - y\\|_2$, where $Q_3$ is implicitly defined by the stored reflectors from $A_3$ and $\\|\\cdot\\|_2$ is the Euclidean norm.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[r_1, r_2, r_3, r_4, r_5]$.\n\nNo physical units, angle units, or percentages are involved in this problem. All values are dimensionless real numbers and booleans.",
            "solution": "The user requires an implementation of the symmetric Householder tridiagonalization algorithm. A critical aspect of the problem is the specified compact storage scheme for the Householder reflectors and the implementation of related matrix-vector operations using this compact representation.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n\n- **Matrix**: A real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$.\n- **Algorithm**: Symmetric tridiagonalization via Householder reflections. The algorithm iteratively applies similarity transformations $A \\leftarrow H A H$.\n- **Householder Reflector Definition**: For a vector $x \\in \\mathbb{R}^m$, the reflector is $H = I - \\tau v v^\\top$, where the Householder vector is $v = x - \\alpha e_1$ with $\\alpha = -\\mathrm{sign}(x_1)\\|x\\|_2$, and the scalar is $\\tau = \\dfrac{2}{v^\\top v}$.\n- **Compact Storage**: For each step $k$, the Householder vector $v_k \\in \\mathbb{R}^{n-k-1}$ is to be scaled such that its first entry is $1$. The tail of this scaled vector, $v_k[1:], \\dots, v_k[n-k-2]$, is stored. The corresponding scalar factor $\\tau_k$ for the reflector $H_k = I - \\tau_k v_k v_k^\\top$ (where $v_k$ is the scaled vector) must also be stored.\n- **Required Functions**:\n    1.  A function to tridiagonalize $A$ into $T$ and return $T$ along with the compactly stored reflectors.\n    2.  Functions to apply the full orthogonal matrix $Q$ and its transpose $Q^\\top$ to a vector $y$, using only the compact storage. $Q$ is the product of all elementary reflectors.\n    3.  A function to reconstruct $Q$ from the compact storage by applying it to the identity matrix.\n- **Test Cases**:\n    1.  $A_1 = \\begin{bmatrix} 5 \\end{bmatrix}$ ($n=1$)\n    2.  $A_2 = \\begin{bmatrix} 4 & 1 \\\\ 1 & 3 \\end{bmatrix}$ ($n=2$)\n    3.  $A_3 = \\frac{1}{2}(B + B^\\top)$, where $B \\in \\mathbb{R}^{5 \\times 5}$ is from a standard normal distribution with seed $42$.\n    4.  $A_4 \\in \\mathbb{R}^{6 \\times 6}$ is a tridiagonal matrix with diagonal $[2,3,4,5,6,7]$ and sub/superdiagonal entries of $1$.\n    5.  $y \\in \\mathbb{R}^5$ from a standard normal distribution with seed $42$, using reflectors from $A_3$.\n- **Outputs**:\n    1.  $r_1$: Boolean, whether all off-tridiagonal ($|i-j|>1$) entries of $T_1$ have absolute value $\\le 10^{-12}$.\n    2.  $r_2$: Float, $\\max_{i,j} \\left| \\left(Q_2^\\top A_2 Q_2 - T_2 \\right)_{ij} \\right|$, where $Q_2$ is reconstructed.\n    3.  $r_3$: Float, $\\max_{|i-j|>1} \\left| (T_3)_{ij} \\right|$.\n    4.  $r_4$: Boolean, whether $\\|T_4 - A_4\\|_\\infty \\le 10^{-10}$.\n    5.  $r_5$: Float, $\\|Q_3^\\top (Q_3 y) - y\\|_2$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientific Grounding**: The problem describes Householder tridiagonalization, a cornerstone algorithm in numerical linear algebra. The specified formulas for the Householder vector and reflector are standard and correct. The chosen method for constructing $v = x - \\alpha e_1$ with $\\alpha = -\\mathrm{sign}(x_1)\\|x\\|_2$ simplifies to $v = x + \\mathrm{sign}(x_1)\\|x\\|_2 e_1$, which is the numerically robust choice to avoid subtractive cancellation.\n-   **Well-Posedness**: The problem is well-posed. The algorithm is deterministic and produces a unique tridiagonal matrix $T$ and orthogonal matrix $Q$ (up to signs, which are fixed by the definition of $\\alpha$). The tasks and required outputs are precisely defined.\n-   **Objectivity**: The language is formal and mathematical, free of any subjectivity.\n-   **Consistency of Compact Storage**: The problem requires scaling the standard Householder vector $u = x - \\alpha e_1$ to a new vector $v$ where $v_1=1$. This is done via $v = u/u_1$. The associated scalar $\\tau_k$ for the formula $H=I-\\tau_k v v^\\top$ can be consistently determined as $\\tau_k = 2/(v^\\top v)$. This is equivalent to scaling the original scalar, $\\tau_u = 2/(u^\\top u)$, by $u_1^2$, i.e., $\\tau_v = \\tau_u u_1^2$. The requirement is unambiguous and internally consistent.\n\n**Step 3: Verdict and Action**\n\nThe problem is scientifically sound, well-posed, objective, and contains no contradictions or ambiguities. It is a valid problem. I will proceed with providing a solution.\n\n### Algorithmic Design and Implementation\n\nThe solution will consist of a main `solve` function that orchestrates the test cases and four helper functions: `tridiagonalize`, `apply_Q`, `apply_QT`, and `reconstruct_Q`.\n\n1.  **`tridiagonalize(A)`**: This function takes a symmetric matrix $A$ and returns the tridiagonal matrix $T$ and two lists for the compact representation of $Q$: one for the tails of the scaled Householder vectors (`v_list`) and one for the scalars (`tau_list`).\n    - The process iterates from $k=0$ to $n-3$. In each step $k$, it considers the vector $x = A[k+1:n, k]$.\n    - It computes the Householder reflector $P_k$ that zeros out all but the first element of $x$. The transformation is $x \\rightarrow \\alpha e_1$.\n    - The full similarity transformation is $A \\leftarrow H_k A H_k$, where $H_k = \\mathrm{diag}(I_{k+1}, P_k)$. This is efficiently implemented using a rank-$2$ update on the trailing submatrix $A[k+1:n, k+1:n]$. The new subdiagonal element $A[k+1, k]$ is set to $\\alpha$.\n    - For storage, the unscaled Householder vector $u_k$ is computed, then scaled to $v_k = u_k/u_{k,0}$. The tail $v_k[1:]$ and the corresponding scalar $\\tau_k = 2/(v_k^\\top v_k)$ are stored.\n\n2.  **`apply_QT(y, ...)` and `apply_Q(y, ...)`**: These functions apply $Q^\\top$ and $Q$ to a vector $y$.\n    - The orthogonal matrix is $Q = H_0 H_1 \\dots H_{n-3}$.\n    - $Q^\\top y = (H_{n-3} \\dots H_0) y$. This is computed by applying the reflectors $H_k$ in the order of their generation ($k=0, 1, \\dots$).\n    - $Q y = (H_0 \\dots H_{n-3}) y$. This is computed by applying the reflectors $H_k$ in the reverse order of their generation ($k=n-3, n-4, \\dots, 0$).\n    - Each application $z \\leftarrow H_k z$ requires reconstructing the full scaled vector $v_k$ from its stored tail by prepending a $1$ and then applying the transformation $z_{sub} \\leftarrow z_{sub} - \\tau_k v_k (v_k^\\top z_{sub})$ to the appropriate subvector of $z$.\n\n3.  **`reconstruct_Q(...)`**: This function explicitly forms the matrix $Q$ by applying $Q$ to the identity matrix, $Q = Q I$. It starts with $Q_{matrix} = I$ and sequentially applies the reflectors $H_k$ in reverse order ($k=n-3, \\dots, 0$) to the current $Q_{matrix}$, i.e., $Q_{matrix} \\leftarrow H_k Q_{matrix}$. Each transformation is applied to all columns of $Q_{matrix}$ simultaneously.\n\nThe main `solve` function will execute these steps for the specified test cases and compute the five required results, $r_1$ through $r_5$. The results are then printed in the specified format.",
            "answer": "```python\nimport numpy as np\n\ndef tridiagonalize(A):\n    \"\"\"\n    Performs symmetric Householder tridiagonalization of a matrix A.\n\n    Args:\n        A (np.ndarray): A real symmetric n x n matrix.\n\n    Returns:\n        tuple: A tuple containing:\n            - T (np.ndarray): The tridiagonal matrix.\n            - v_list (list): A list of the tails of the scaled Householder vectors.\n            - tau_list (list): A list of the scalar factors tau.\n    \"\"\"\n    T = A.copy()\n    n = T.shape[0]\n    \n    if n <= 2:\n        return T, [], []\n\n    v_list = []\n    tau_list = []\n    \n    # Loop from k = 0 to n-3\n    for k in range(n - 2):\n        x = T[k+1:n, k]\n        m = len(x)\n        norm_x = np.linalg.norm(x)\n        \n        # Unscaled Householder vector v (denoted as u in thoughts)\n        u = x.copy()\n        \n        if norm_x > 1e-15:\n            # alpha = -sign(x_0) * ||x||\n            sign_x0 = np.copysign(1.0, x[0]) if x[0] != 0.0 else 1.0\n            alpha = -sign_x0 * norm_x\n            \n            # v = x - alpha * e_1 = x + sign(x_0) * ||x|| * e_1\n            u[0] -= alpha\n            \n            # Check for non-zero first element before scaling\n            u0 = u[0]\n            if abs(u0) < 1e-15:\n                # This case implies x is already zero or nearly zero.\n                # Treat as a no-op for this iteration.\n                v_list.append(np.zeros(m - 1))\n                tau_list.append(0.0)\n                # Set subdiagonal element and zero out rest of column\n                T[k+1, k] = norm_x if sign_x0 >= 0 else -norm_x\n                T[k, k+1] = T[k+1, k]\n                T[k+2:n, k] = 0.0\n                T[k, k+2:n] = 0.0\n                continue\n                \n            # Scaled vector v_k, where first component is 1\n            v = u / u0\n            v_list.append(v[1:])\n            \n            # Corresponding tau for the scaled vector\n            tau = 2.0 / (v @ v)\n            tau_list.append(tau)\n            \n            # Apply similarity transformation T <- H T H using the unscaled vector u\n            # for simpler formula application, since the underlying reflector is identical.\n            tau_u = 2.0 / (u @ u) if (u @ u) > 1e-15 else 0.0\n            \n            if tau_u > 0.0:\n              sub_T = T[k+1:n, k+1:n]\n              p = tau_u * (sub_T @ u)\n              beta = (u @ p) / 2.0\n              w = p - beta * u\n              sub_T -= np.outer(w, u) + np.outer(u, w)\n              T[k+1:n, k+1:n] = sub_T\n            \n            T[k+1, k] = alpha\n            T[k, k+1] = alpha\n        else:\n            # norm_x is zero, column is already zeroed out.\n            v_list.append(np.zeros(m - 1))\n            tau_list.append(0.0)\n\n        # Enforce zeros for numerical stability\n        T[k+2:n, k] = 0.0\n        T[k, k+2:n] = 0.0\n\n    return T, v_list, tau_list\n\ndef apply_Q(y, v_list, tau_list, n):\n    \"\"\"Applies Q to a vector y, where Q = H_0 ... H_{n-3}.\"\"\"\n    z = y.copy()\n    num_reflectors = len(v_list)\n    for k in range(num_reflectors - 1, -1, -1):\n        if tau_list[k] == 0.0:\n            continue\n        v_compact = v_list[k]\n        v = np.hstack([1.0, v_compact])\n        tau = tau_list[k]\n        \n        z_sub = z[k+1:n]\n        z_sub -= tau * v * (v @ z_sub)\n        z[k+1:n] = z_sub\n    return z\n\ndef apply_QT(y, v_list, tau_list, n):\n    \"\"\"Applies Q^T to a vector y, where Q^T = H_{n-3} ... H_0.\"\"\"\n    z = y.copy()\n    num_reflectors = len(v_list)\n    for k in range(num_reflectors):\n        if tau_list[k] == 0.0:\n            continue\n        v_compact = v_list[k]\n        v = np.hstack([1.0, v_compact])\n        tau = tau_list[k]\n        \n        z_sub = z[k+1:n]\n        z_sub -= tau * v * (v @ z_sub)\n        z[k+1:n] = z_sub\n    return z\n\ndef reconstruct_Q(n, v_list, tau_list):\n    \"\"\"Reconstructs the orthogonal matrix Q from its compact representation.\"\"\"\n    if n == 0:\n        return np.array([])\n    Q = np.identity(n)\n    num_reflectors = len(v_list)\n    for k in range(num_reflectors - 1, -1, -1):\n        if tau_list[k] == 0.0:\n            continue\n        v_compact = v_list[k]\n        v = np.hstack([1.0, v_compact])\n        tau = tau_list[k]\n        \n        Q_sub = Q[k+1:n, :]\n        update = tau * np.outer(v, v @ Q_sub)\n        Q[k+1:n, :] -= update\n    return Q\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    results = []\n\n    # Test Case 1: Trivial size\n    A1 = np.array([[5.0]])\n    T1, _, _ = tridiagonalize(A1)\n    n1 = T1.shape[0]\n    off_tridiag_abs_vals_1 = [np.abs(T1[i, j]) for i in range(n1) for j in range(n1) if abs(i - j) > 1]\n    r1 = all(val <= 1e-12 for val in off_tridiag_abs_vals_1)\n    results.append(r1)\n\n    # Test Case 2: Small case\n    A2 = np.array([[4.0, 1.0], [1.0, 3.0]])\n    T2, v_list2, tau_list2 = tridiagonalize(A2)\n    n2 = A2.shape[0]\n    Q2 = reconstruct_Q(n2, v_list2, tau_list2)\n    err_matrix2 = Q2.T @ A2 @ Q2 - T2\n    r2 = np.max(np.abs(err_matrix2))\n    results.append(r2)\n\n    # Test Case 3 & 5: General random case and orthogonality check\n    rng = np.random.default_rng(42)\n    B = rng.standard_normal((5, 5))\n    A3 = (B + B.T) / 2.0\n    T3, v_list3, tau_list3 = tridiagonalize(A3)\n    n3 = A3.shape[0]\n    \n    # r3 calculation\n    off_tridiag_abs_vals_3 = [np.abs(T3[i, j]) for i in range(n3) for j in range(n3) if abs(i - j) > 1]\n    r3 = 0.0 if not off_tridiag_abs_vals_3 else max(off_tridiag_abs_vals_3)\n    results.append(r3)\n\n    # Test Case 4: Already tridiagonal input\n    A4 = np.diag(np.arange(2.0, 8.0)) + np.diag(np.ones(5), 1) + np.diag(np.ones(5), -1)\n    T4, _, _ = tridiagonalize(A4)\n    r4 = np.max(np.abs(T4 - A4)) <= 1e-10\n    results.append(r4)\n\n    # r5 calculation (uses artifacts from TC3)\n    y = rng.standard_normal(5)\n    Qy = apply_Q(y, v_list3, tau_list3, n3)\n    QTQy = apply_QT(Qy, v_list3, tau_list3, n3)\n    r5 = np.linalg.norm(QTQy - y)\n    results.append(r5)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}