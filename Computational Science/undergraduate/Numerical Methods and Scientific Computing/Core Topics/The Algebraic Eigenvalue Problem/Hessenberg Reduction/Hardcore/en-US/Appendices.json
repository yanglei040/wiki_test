{
    "hands_on_practices": [
        {
            "introduction": "Before we delve into the mechanics of transforming a matrix, let's start with the \"why.\" This exercise illustrates one of the key motivations for Hessenberg reduction: the resulting structure can make certain properties of the matrix, like its eigenvalues, much easier to find. By constructing a special Hessenberg matrix, you will see how a simple observation can reveal an eigenvalue without complex calculations, providing a clear link between a matrix's form and its spectral properties .",
            "id": "3238609",
            "problem": "You are studying the role of upper Hessenberg structure in iterative eigenvalue algorithms, where reducing a dense matrix to an upper Hessenberg form preserves the eigenvalues but introduces a nearly triangular pattern that enables efficient computations. An $n \\times n$ matrix is called upper Hessenberg if all entries strictly below the first subdiagonal are zero; it is called unreduced if all first subdiagonal entries are nonzero.\n\nConstruct explicitly a real $5 \\times 5$ unreduced upper Hessenberg matrix $A$ with the following properties:\n- $A_{i,j} = 0$ for all $i > j + 1$.\n- $A_{i+1,i} \\neq 0$ for $i = 1, 2, 3, 4$.\n- The sum of the entries in each row equals the same constant $c$, where $c = 3$.\n\nUsing only the definition of an eigenvalue and eigenvector, explain why one eigenvalue of $A$ can be determined by simple inspection from your construction, and identify that eigenvalue. Provide one explicit example of such a matrix $A$ and justify why it satisfies the stated properties. Your final answer must be the single eigenvalue that is immediately identifiable by inspection. No rounding is required.",
            "solution": "We begin from core definitions. A matrix $A \\in \\mathbb{R}^{n \\times n}$ is upper Hessenberg if $A_{i,j} = 0$ whenever $i > j + 1$. It is unreduced if $A_{i+1,i} \\neq 0$ for all $i = 1, \\dots, n-1$. A scalar $\\lambda \\in \\mathbb{R}$ is an eigenvalue of $A$ if there exists a nonzero vector $x \\in \\mathbb{R}^{n}$ such that $A x = \\lambda x$.\n\nWe will construct an explicit $5 \\times 5$ matrix $A$ meeting the requested properties. Consider\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0 & 1 & 0 & 0 & 2 \\\\\n1 & 0 & 0 & 2 & 0 \\\\\n0 & 1 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 2 & 0 \\\\\n0 & 0 & 0 & 1 & 2\n\\end{pmatrix}.\n$$\nWe verify the required structural properties:\n- Upper Hessenberg: Entries strictly below the first subdiagonal are zero. Indeed, for all $i > j + 1$, the corresponding entries are $0$ in $A$. Only the first subdiagonal positions $(2,1)$, $(3,2)$, $(4,3)$, $(5,4)$ may be nonzero.\n- Unreduced: The first subdiagonal entries are $A_{2,1} = 1$, $A_{3,2} = 1$, $A_{4,3} = 1$, $A_{5,4} = 1$, all of which are nonzero.\n\nNext, we ensure that each row sum equals the same constant $c = 3$:\n- Row $1$: $0 + 1 + 0 + 0 + 2 = 3$.\n- Row $2$: $1 + 0 + 0 + 2 + 0 = 3$.\n- Row $3$: $0 + 1 + 0 + 1 + 1 = 3$.\n- Row $4$: $0 + 0 + 1 + 2 + 0 = 3$.\n- Row $5$: $0 + 0 + 0 + 1 + 2 = 3$.\n\nLet $ \\mathbf{1} = (1, 1, 1, 1, 1)^{\\top}$. By direct matrix-vector multiplication and linearity, the $i$-th component of $A \\mathbf{1}$ equals the sum of the entries in the $i$-th row of $A$. Since each row sum is $3$, we obtain\n$$\nA \\mathbf{1} = 3 \\mathbf{1}.\n$$\nBy the definition of an eigenvalue and eigenvector, this shows that $\\lambda = 3$ is an eigenvalue of $A$ with eigenvector $\\mathbf{1}$. This eigenvalue is determinable by simple inspection because, once we notice that all row sums equal the common constant $3$, the equality $A \\mathbf{1} = 3 \\mathbf{1}$ follows immediately.\n\nTherefore, the eigenvalue that can be read off by inspection from the constructed unreduced upper Hessenberg matrix is $3$.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Having seen the utility of the Hessenberg form, we now turn to the \"how.\" This practice guides you through the fundamental mechanics of Hessenberg reduction using Householder reflectors, which are the building blocks of modern, stable algorithms. By performing the reduction for a symmetric matrix by hand, you will gain a concrete understanding of the step-by-step process of introducing zeros. This particular problem  is designed so that all calculations can be done exactly, highlighting the precise algebraic properties of the Householder transformation.",
            "id": "3238489",
            "problem": "Consider the task of reducing a real matrix to Hessenberg form using orthogonal similarity transformations built from Householder reflections. Recall that for a real symmetric matrix, the upper Hessenberg form coincides with tridiagonal form. Construct the specific integer matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n7 & 1 & 2 & 2 \\\\\n1 & 1 & 0 & 0 \\\\\n2 & 0 & 1 & 0 \\\\\n2 & 0 & 0 & 1\n\\end{pmatrix},\n$$\nwhich is symmetric and of size $4 \\times 4$. Starting from the fundamental definitions of orthogonal matrices and Householder reflections (without appealing to any pre-packaged algorithmic shortcut), perform the first similarity step that acts nontrivially only on the trailing $(4-1) \\times (4-1)$ block (that is, rows and columns indexed $2$ through $4$), so as to introduce zeros below the first subdiagonal in column $1$. Then, argue carefully why, for this particular $A$, completing the tridiagonalization can be carried out using only rational arithmetic.\n\nLet $T$ denote the resulting tridiagonal matrix obtained from $A$ by this Hessenberg (tridiagonal) reduction. Compute the exact value of the $(2,1)$ entry of $T$. Express your final answer exactly; no rounding is required.",
            "solution": "The problem asks for two things: first, to compute the $(2,1)$ entry of the final tridiagonal matrix $T$ obtained from a given symmetric matrix $A$ via Householder reduction; second, to provide a careful argument as to why the entire tridiagonalization procedure can be performed using only rational arithmetic for this specific matrix.\n\nThe given matrix is:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n7 & 1 & 2 & 2 \\\\\n1 & 1 & 0 & 0 \\\\\n2 & 0 & 1 & 0 \\\\\n2 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThis is a real, symmetric $4 \\times 4$ matrix. The Hessenberg reduction of a symmetric matrix produces a tridiagonal matrix. The process involves a sequence of similarity transformations using Householder matrices. For an $n \\times n$ matrix, this requires $n-2$ steps. In our case, $n=4$, so at most $2$ steps are needed.\n\nThe first step of the reduction aims to introduce zeros in the first column of $A$ at positions $(3,1)$ and $(4,1)$. This is achieved by a similarity transformation $A_1 = P_1 A P_1$. The matrix $P_1$ is an orthogonal matrix of the form:\n$$\nP_1 = \\begin{pmatrix}\n1 & \\mathbf{0}^\\top \\\\\n\\mathbf{0} & H\n\\end{pmatrix}\n$$\nwhere $H$ is a $3 \\times 3$ Householder matrix. The matrix $H$ is chosen to act on the sub-vector of the first column of $A$ that we want to modify. Let $a_1$ be the first column of $A$. We partition $a_1$ as:\n$$\na_1 = \\begin{pmatrix} a_{11} \\\\ x \\end{pmatrix} = \\begin{pmatrix} 7 \\\\ 1 \\\\ 2 \\\\ 2 \\end{pmatrix}, \\quad \\text{where} \\quad x = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}.\n$$\nThe Householder matrix $H$ is constructed such that it maps the vector $x$ to a multiple of the first standard basis vector $e_1 = (1, 0, 0)^\\top$. That is, $Hx = \\alpha e_1$ for some scalar $\\alpha$.\n\nA Householder reflection is an orthogonal transformation, so it preserves the Euclidean norm. Thus, we must have $\\|Hx\\|_2 = \\|x\\|_2$. This implies $|\\alpha| = \\|x\\|_2$. We calculate the norm of $x$:\n$$\n\\|x\\|_2 = \\sqrt{1^2 + 2^2 + 2^2} = \\sqrt{1 + 4 + 4} = \\sqrt{9} = 3.\n$$\nFor numerical stability, the sign of $\\alpha$ is chosen to be opposite to the sign of the first component of $x$. Here, $x_1=1$, which is positive. So we choose $\\alpha = -\\operatorname{sgn}(x_1)\\|x\\|_2 = -(+1)(3) = -3$.\nThe target vector is $Hx = \\begin{pmatrix} -3 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nThe first column of the transformed matrix $A_1 = P_1 A P_1$ is given by $P_1 A (P_1 e_1)$. Since $P_1$ has $e_1$ as its first column, $P_1 e_1 = e_1$. So the first column of $A_1$ is $P_1 a_1$.\n$$\nP_1 a_1 = \\begin{pmatrix} 1 & \\mathbf{0}^\\top \\\\ \\mathbf{0} & H \\end{pmatrix} \\begin{pmatrix} a_{11} \\\\ x \\end{pmatrix} = \\begin{pmatrix} a_{11} \\\\ Hx \\end{pmatrix} = \\begin{pmatrix} 7 \\\\ -3 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe matrix $A$ is symmetric, and the Householder matrix $P_1$ is also symmetric ($P_1^\\top = P_1$) and orthogonal ($P_1^{-1} = P_1^\\top = P_1$). Therefore, the transformed matrix $A_1 = P_1 A P_1$ is also symmetric. This means the first row of $A_1$ must be the transpose of its first column: $(7, -3, 0, 0)$.\nSo, after the first step, the matrix $A_1$ has the form:\n$$\nA_1 = \\begin{pmatrix}\n7 & -3 & 0 & 0 \\\\\n-3 & A_1(2,2) & A_1(2,3) & A_1(2,4) \\\\\n0 & A_1(3,2) & A_1(3,3) & A_1(3,4) \\\\\n0 & A_1(4,2) & A_1(4,3) & A_1(4,4)\n\\end{pmatrix}.\n$$\nThe entry $A_1(2,1)$ is $-3$.\n\nThe second step of the tridiagonalization procedure, if needed, would apply a transformation $A_2 = P_2 A_1 P_2$, where $P_2$ has the form:\n$$\nP_2 = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & \\tilde{H} & \\\\\n0 & 0 & &\n\\end{pmatrix}.\n$$\nThis transformation acts on rows and columns $3$ and $4$, and leaves the first two rows and columns unchanged. Specifically, the $(2,1)$ entry of $A_2$ will be the same as the $(2,1)$ entry of $A_1$.\nThus, the $(2,1)$ entry of the final tridiagonal matrix $T$ is determined by the first step alone. The value is $T(2,1) = A_1(2,1) = -3$.\n\nNow, we must argue why the tridiagonalization can be completed using only rational arithmetic.\nThe general Householder transformation for a vector $y$ is given by a matrix $P = I - 2 \\frac{vv^{\\top}}{v^{\\top}v}$, where the Householder vector is $v = y \\pm \\|y\\|_2 e_1$. For the transformation matrix $P$ to have rational entries, given a vector $y$ with rational entries, it is necessary and sufficient that $\\|y\\|_2$ be a rational number. If this condition holds at every step, and the initial matrix $A$ is rational, then the entire procedure remains within the field of rational numbers $\\mathbb{Q}$.\n\nThe initial matrix $A$ has integer entries, which are rational.\n\nFor the first step, the vector being transformed is $x = (1, 2, 2)^\\top$. Its entries are rational. Its norm is $\\|x\\|_2 = 3$, which is a rational number.\nTherefore, the Householder vector $v = x - (-3)e_1 = (1,2,2)^\\top + (3,0,0)^\\top = (4,2,2)^\\top$ has rational entries. The corresponding $3 \\times 3$ matrix $H = I_3 - 2\\frac{vv^\\top}{v^\\top v}$ and the full $4 \\times 4$ matrix $P_1$ have rational entries. As a consequence, the matrix $A_1 = P_1 A P_1$ must also have all its entries in $\\mathbb{Q}$.\n\nFor the second (and final) step of the $4 \\times 4$ reduction, we would apply a transformation to zero out the entry $A_1(4,2)$. This step operates on the vector $y = (A_1(3,2), A_1(4,2))^\\top$. Let's compute this vector.\nFrom the symmetry of $A_1$, we have $A_1(3,2) = A_1(2,3)$ and $A_1(4,2) = A_1(2,4)$. We need to compute the trailing $3\\times3$ principal submatrix of $A_1$. An efficient way to calculate the transformed matrix is using $A_1 = A - P_1' A - A P_1' + P_1' A P_1'$ where $P_1' = P_1 - I$. Or, more directly, $A_1=A-\\frac{1}{\\|v\\|_2^2/2}(v_{ext}w^{\\top}+wv_{ext}^{\\top})+\\frac{v_{ext}^\\top A v_{ext}}{(\\|v\\|_2^2/2)^2}v_{ext}v_{ext}^{\\top}$, where $v_{ext}=(0,4,2,2)^\\top$ and $w=Av_{ext}=(12,4,2,2)^\\top$. However, since we showed $A_1$ is symmetric with first row $(7, -3, 0, 0)$, we can deduce $A_1(3,2)=0$ and $A_1(4,2)=0$ if $A_1(2,3)=0$ and $A_1(2,4)=0$. Since the first step zeros out elements below the main diagonal in the first column, and due to symmetry, elements to the right of the main diagonal in the first row are also zeroed (except for the superdiagonal element). Indeed, $A_1$'s first row is $(7, -3, 0, 0)$. Thus $A_1(1,3)=0$ and $A_1(1,4)=0$. By symmetry, $A_1(3,1)=0$ and $A_1(4,1)=0$. This is what we already established.\n\nThe vector to consider for the second step is $y = \\begin{pmatrix} A_1(3,2) \\\\ A_1(4,2) \\end{pmatrix}$. By symmetry, $A_1(3,2) = A_1(2,3)$ and $A_1(4,2) = A_1(2,4)$.\nLet's compute $A_1(2,3)$ and $A_1(2,4)$. $A_1=P_1AP_1$. The second row of $A_1$ is $e_2^\\top P_1 A P_1$.\nThe vector $p_{1,2}^\\top = e_2^\\top P_1 = (0, -1/3, -2/3, -2/3)$ using $H = \\frac{1}{3}\\begin{pmatrix}-1 & -2 & -2 \\\\ -2 & 2 & -1 \\\\ -2 & -1 & 2\\end{pmatrix}$.\nThen $p_{1,2}^\\top A = (0, -1/3, -2/3, -2/3) \\begin{pmatrix} 7 & 1 & 2 & 2 \\\\ 1 & 1 & 0 & 0 \\\\ 2 & 0 & 1 & 0 \\\\ 2 & 0 & 0 & 1 \\end{pmatrix} = (-1/3-4/3-4/3, -1/3, -2/3, -2/3) = (-3, -1/3, -2/3, -2/3)$.\nFinally, $(p_{1,2}^\\top A)P_1 = (-3, -1/3, -2/3, -2/3) \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & -1/3 & -2/3 & -2/3 \\\\ 0 & -2/3 & 2/3 & -1/3 \\\\ 0 & -2/3 & -1/3 & 2/3 \\end{pmatrix}$.\n$A_1(2,3) = (-3)(0) + (-1/3)(-2/3) + (-2/3)(2/3) + (-2/3)(-1/3) = 2/9 - 4/9 + 2/9 = 0$.\n$A_1(2,4) = (-3)(0) + (-1/3)(-2/3) + (-2/3)(-1/3) + (-2/3)(2/3) = 2/9 + 2/9 - 4/9 = 0$.\nSo the vector for the second step is $y = (0, 0)^\\top$.\n\nThe norm is $\\|y\\|_2 = 0$, which is rational. The Householder transformation for a zero vector is the identity matrix (or, in practice, the step is skipped). This means $A_1$ is already tridiagonal.\nSince the norms of the vectors at each stage of the reduction were rational numbers ($3$ and $0$), the entire tridiagonalization process can be carried out using only rational arithmetic.\n\nThe final tridiagonal matrix is $T=A_1$. The $(2,1)$ entry is $-3$.",
            "answer": "$$\\boxed{-3}$$"
        },
        {
            "introduction": "Theoretical elegance does not always translate to practical success in the world of floating-point computation. This final hands-on practice moves from pen-and-paper theory to computational reality, confronting the critical issue of numerical stability. You will implement and compare two different methods for creating a Hessenberg matrix: one based on the stable Householder reflections and another on the notoriously unstable classical Gram-Schmidt process. This exercise  powerfully demonstrates why the choice of algorithm is paramount, as you will see a theoretically correct method fail dramatically in practice while a robust alternative succeeds.",
            "id": "3238560",
            "problem": "You are given the task of contrasting two approaches to upper Hessenberg reduction for a real square matrix: a naive approach based on the classical Gram-Schmidt orthonormalization embedded in the Arnoldi process, and a stable approach based on Householder reflections applied as two-sided similarity transformations. The goal is to construct and analyze matrices for which the classical Gram-Schmidt approach exhibits numerical instability, while the Householder-based method remains robust.\n\nFundamental base and core definitions:\n- A matrix $H \\in \\mathbb{R}^{n \\times n}$ is upper Hessenberg if $H_{i,j} = 0$ for all $i \\ge j + 2$.\n- The classical Gram-Schmidt orthonormalization for a sequence of vectors $\\{w_j\\}$ constructs $v_1 = w_1 / \\|w_1\\|_2$ and, for $j \\ge 1$, defines\n$$\nh_{i,j} = v_i^\\top w_j, \\quad \\tilde{w}_j = w_j - \\sum_{i=1}^{j} h_{i,j} v_i, \\quad v_{j+1} = \\tilde{w}_j / \\|\\tilde{w}_j\\|_2,\n$$\nwhenever $\\|\\tilde{w}_j\\|_2 \\neq 0$. The Arnoldi process applies this to $w_j = A v_j$ for a matrix $A \\in \\mathbb{R}^{n \\times n}$, generating an orthonormal basis $V_k = [v_1,\\dots,v_k]$ and an upper Hessenberg $H_k \\in \\mathbb{R}^{k \\times k}$ satisfying $A V_k \\approx V_k H_k$.\n- A Householder reflector is $P = I - 2 u u^{\\top}$, where $u \\in \\mathbb{R}^m$ is chosen so that $P x = \\alpha e_1$ for any given $x \\in \\mathbb{R}^m$ and some scalar $\\alpha \\in \\mathbb{R}$. Householder-based Hessenberg reduction applies a sequence of such reflectors on the left and right to compute an orthogonal $Q$ and an upper Hessenberg $H$ such that $A = Q H Q^{\\top}$.\n\nYour tasks:\n1. Implement a classical Gram-Schmidt Arnoldi routine that, given $A \\in \\mathbb{R}^{n \\times n}$, a starting unit vector $v_1 \\in \\mathbb{R}^n$, and target dimension $k$, returns an orthonormal matrix $V \\in \\mathbb{R}^{n \\times m}$ (with $m \\le k$ depending on breakdown), and an upper Hessenberg matrix $H \\in \\mathbb{R}^{m \\times m}$ such that $A V \\approx V H$. Use a single pass of classical Gram-Schmidt without any reorthonormalization. Quantify numerical stability using:\n   - The orthogonality loss measure $o = \\max_{i \\neq j} |(V^\\top V)_{i,j}|$.\n   - The Arnoldi residual ratio $r_A = \\|A V - V H\\|_F / \\|A\\|_F$.\n   - A breakdown is declared if a new vector norm falls below a threshold $\\tau = 10^{-14}$, resulting in $m < k$.\n\n2. Implement a Householder-based full Hessenberg reduction that computes $Q \\in \\mathbb{R}^{n \\times n}$ and $H \\in \\mathbb{R}^{n \\times n}$ satisfying $A = Q H Q^{\\top}$. Apply a sequence of Householder reflectors $P_j = I - 2 u_j u_j^\\top$ for $j = 0,1,\\dots,n-3$, each targeted to zero out entries below the first subdiagonal in column $j$. Quantify numerical success using:\n   - The below-second-subdiagonal norm $b = \\sqrt{\\sum_{i \\ge j+2} \\sum_j H_{i,j}^2}$.\n   - The reconstruction residual ratio $r_H = \\|A - Q H Q^{\\top}\\|_F / \\|A\\|_F$.\n\n3. Construct and analyze the following test suite of matrices and starting vectors:\n   - Case $1$ (ill-conditioned near-identity Jordan perturbation): Let $n = 25$, $\\varepsilon = 10^{-16}$, and $J \\in \\mathbb{R}^{n \\times n}$ be the Jordan block with $J_{i,i} = 0$ for all $i$, $J_{i,i+1} = 1$ for $i = 1,\\dots,n-1$, and all other entries $0$. Define $A_1 = I + \\varepsilon J$. Use starting vector $v_1$ with entries $(v_1)_i = 1$ for all $i$, normalized to unit $2$-norm. This matrix is chosen so that the Krylov sequence $\\{A_1^j v_1\\}$ becomes nearly colinear, triggering numerical instability in the classical Gram-Schmidt Arnoldi process.\n   - Case $2$ (well-conditioned symmetric tridiagonal Toeplitz): Let $n = 20$ and $A_2 \\in \\mathbb{R}^{n \\times n}$ be defined by $(A_2)_{i,i} = 2$, $(A_2)_{i,i+1} = -1$, $(A_2)_{i+1,i} = -1$ for $i = 1,\\dots,n-1$, and all other entries $0$. Use starting vector $v_1$ with entries $(v_1)_i = \\sin(i)$, normalized to unit $2$-norm.\n   - Case $3$ (moderately sized well-behaved tridiagonal): Let $n = 6$ and define $A_3$ in the same way as $A_2$ with size $n = 6$. Use starting vector $v_1$ with entries $(v_1)_i = 1$ for all $i$, normalized to unit $2$-norm.\n\nDecision rule for each case:\n- Declare the classical Gram-Schmidt Arnoldi method as having failed if any of the following hold: $m < n$, $o > 10^{-3}$, or $r_A > 10^{-8}$.\n- Declare the Householder method as having succeeded if both $b < 10^{-12}$ and $r_H < 10^{-12}$.\n- For each case, the program must output a boolean indicating whether the naive Gram-Schmidt-based Hessenberg reduction fails due to numerical instability while the Householder-based method succeeds. That is, output $\\text{True}$ if classical Gram-Schmidt Arnoldi fails and Householder succeeds, and $\\text{False}$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a boolean corresponding to the three cases in the order $A_1$, $A_2$, $A_3$.\n\nNo physical units, angles, or percentages are involved in this problem; all quantities are dimensionless real numbers and matrices. The program must be completely self-contained and require no user input.",
            "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded in the principles of numerical linear algebra, well-posed with clear definitions and objective criteria, and internally consistent. It presents a standard, albeit important, comparative analysis between two fundamental algorithms for Hessenberg reduction.\n\nThe task is to compare the numerical stability of two methods for computing the upper Hessenberg decomposition of a real square matrix $A \\in \\mathbb{R}^{n \\times n}$. The first method is the Arnoldi process employing classical Gram-Schmidt (CGS) without reorthonormalization. The second is a sequence of Householder similarity transformations.\n\n### Method 1: Classical Gram-Schmidt Arnoldi (CGS-Arnoldi) Procedure\n\nThe Arnoldi process is an iterative method that constructs an orthonormal basis for the Krylov subspace $\\mathcal{K}_k(A, v_1) = \\text{span}\\{v_1, A v_1, \\dots, A^{k-1} v_1\\}$. When implemented with the classical Gram-Schmidt procedure, it generates a sequence of orthonormal vectors $v_1, v_2, \\dots, v_k$ and an upper Hessenberg matrix $H_k \\in \\mathbb{R}^{k \\times k}$ which satisfy the Arnoldi relation $A V_k = V_k H_k + f_k e_k^{\\top}$, where $V_k = [v_1, \\dots, v_k]$, $f_k$ is the residual vector, and $e_k$ is the $k$-th standard basis vector.\n\nThe algorithm proceeds as follows, given $A \\in \\mathbb{R}^{n \\times n}$, a starting unit vector $v_1 \\in \\mathbb{R}^n$, and a target dimension $k$:\n1. Initialize an $n \\times k$ matrix $V$ and a $k \\times k$ matrix $H$. Set the first column of $V$ to $v_1$.\n2. For $j = 1, \\dots, k$:\n   a. Compute the next Krylov vector, $w = A v_j$.\n   b. Orthogonalize $w$ against the previously computed basis vectors $\\{v_1, \\dots, v_j\\}$:\n      For $i = 1, \\dots, j$:\n      i. Compute the projection coefficient: $h_{i,j} = v_i^{\\top} w$.\n      ii. Subtract the projection from $w$: $w \\leftarrow w - h_{i,j} v_i$.\n   c. If $j < k$:\n      i. Compute the norm of the new vector: $h_{j+1,j} = \\|w\\|_2$.\n      ii. If $h_{j+1,j}$ is below a threshold $\\tau$ (e.g., $10^{-14}$), the process is said to break down. The Krylov subspace is invariant, and the algorithm terminates, resulting in a matrix $V$ of dimension $n \\times j$ and $H$ of dimension $j \\times j$.\n      iii. Normalize to get the next basis vector: $v_{j+1} = w / h_{j+1,j}$.\n\nThe core numerical issue with CGS arises in step 2b. If the vector $A v_j$ is nearly linearly dependent on the preceding basis vectors $\\{v_1, \\dots, v_j\\}$, then the vector $w$ after orthogonalization will be the result of subtracting large, nearly equal quantities. This leads to catastrophic cancellation and a large relative error in the computed $w$. Consequently, the newly computed vector $v_{j+1}$ will not be orthogonal to the previous vectors, leading to a loss of orthogonality in the basis $V$. This instability is quantified by the orthogonality loss measure $o = \\max_{i \\neq j} |(V^{\\top} V)_{i,j}|$. The Arnoldi residual ratio, $r_A = \\|A V - V H\\|_F / \\|A\\|_F$, measures how well the decomposition approximates the action of $A$ on the subspace spanned by $V$.\n\n### Method 2: Householder Hessenberg Reduction\n\nThis method reduces a dense matrix $A$ to an upper Hessenberg form $H$ via a sequence of similarity transformations using Householder reflectors. The goal is to find an orthogonal matrix $Q$ such that $A = Q H Q^{\\top}$. A Householder reflector is an orthogonal matrix of the form $P = I - 2 u u^{\\top}$ where $u$ is a unit vector. For a given vector $x$, $u$ can be chosen such that $P x$ is a multiple of the first standard basis vector $e_1$.\n\nThe algorithm for a full reduction of $A \\in \\mathbb{R}^{n \\times n}$ proceeds as follows:\n1. Initialize $H = A$ and $Q = I_n$.\n2. For $j = 0, 1, \\dots, n-3$:\n   a. Consider the vector $x$ consisting of the entries in column $j$ below the first subdiagonal: $x = H_{j+2:n, j}$.\n   b. Construct a Householder vector $u_j \\in \\mathbb{R}^{n-j-1}$ such that the corresponding reflector $P'_j = I - 2 u_j u_j^{\\top}$ zeroes out all but the first entry of $x$. A stable choice for the intermediate vector $v$ is $v = x + \\operatorname{sign}(x_1) \\|x\\|_2 e_1$, from which $u_j=v/\\|v\\|_2$.\n   c. Form the full $n \\times n$ reflector $P_j$ by embedding $P'_j$ into an identity matrix:\n      $$ P_j = \\begin{pmatrix} I_{j+1} & 0 \\\\ 0 & P'_{j} \\end{pmatrix} $$\n   d. Apply the similarity transformation to the matrix: $H \\leftarrow P_j H P_j^{\\top}$. Since $P_j$ is its own inverse, $P_j^\\top=P_j$. This is done by a left multiplication $H \\leftarrow P_j H$ followed by a right multiplication $H \\leftarrow H P_j$.\n   e. Accumulate the orthogonal transformation: $Q \\leftarrow Q P_j$.\n\nAfter $n-2$ steps, the resulting matrix $H$ is upper Hessenberg. The final orthogonal matrix is $Q = P_0 P_1 \\dots P_{n-3}$. Since each transformation $P_j$ is orthogonal, their product $Q$ is also orthogonal. This property makes the Householder reduction backward stable. The computed $H$ is the exact Hessenberg form of a matrix very close to the original $A$. This stability is evaluated by the reconstruction residual ratio $r_H = \\|A - Q H Q^{\\top}\\|_F / \\|A\\|_F$ and the structure of $H$ is checked by the below-second-subdiagonal norm $b = \\sqrt{\\sum_{i \\ge j+2} \\sum_j H_{i,j}^2}$.\n\n### Analysis of Test Cases\n\nThe provided test cases are designed to highlight the differing stability properties of these two methods.\n\n**Case 1: $A_1 = I + \\varepsilon J$ with $\\varepsilon = 10^{-16}$, $n=25$.**\nThe matrix $J$ is a nilpotent Jordan block. The powers $J^m$ push the non-zero diagonal of ones further to the upper right, becoming zero for $m \\ge n$. For a small $\\varepsilon$, $A_1^m v_1 = (I + \\varepsilon J)^m v_1 \\approx (I + m \\varepsilon J) v_1$. The vectors $v_1, A_1 v_1, A_1^2 v_1, \\dots$ forming the Krylov sequence will become nearly collinear very quickly. This is the archetypal scenario where CGS fails due to catastrophic cancellation. We expect a severe loss of orthogonality (large $o$) and a high Arnoldi residual (large $r_A$). Conversely, Householder reduction operates via stable orthogonal transformations and should be unaffected, successfully reducing $A_1$ to Hessenberg form with high accuracy. Thus, CGS-Arnoldi is expected to fail while Householder succeeds.\n\n**Case 2: Symmetric Tridiagonal Toeplitz Matrix, $n=20$.**\nThis matrix, $A_2$, is the negative of the 1D discrete Laplacian operator. It is symmetric and well-conditioned. For a symmetric matrix, the Arnoldi process (in exact arithmetic) becomes the Lanczos algorithm, producing a symmetric tridiagonal matrix $H$. The eigenvalues of $A_2$ are well-separated, which typically leads to a well-conditioned Krylov basis. Therefore, CGS-Arnoldi is expected to perform well, maintaining good orthogonality. The Householder method, being universally stable, will also succeed. The condition for the test to be `True` (CGS fails AND Householder succeeds) will not be met.\n\n**Case 3: Symmetric Tridiagonal Toeplitz Matrix, $n=6$.**\nThis is a smaller version of Case 2. The same logic applies: $A_3$ is a well-behaved symmetric matrix, for which CGS-Arnoldi is stable. Both methods will succeed. The test condition will not be met.\n\nThe implementation will now proceed to verify these theoretical predictions computationally.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classical_gram_schmidt_arnoldi(A, v1_start, k, tau=1e-14):\n    \"\"\"\n    Performs Hessenberg reduction using the classical Gram-Schmidt Arnoldi process.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, k), dtype=np.float64)\n    H = np.zeros((k, k), dtype=np.float64)\n    \n    # Normalize the starting vector\n    V[:, 0] = v1_start / np.linalg.norm(v1_start)\n    \n    m = k\n    for j in range(k):\n        w = A @ V[:, j]\n        \n        for i in range(j + 1):\n            H[i, j] = V[:, i].T @ w\n            w = w - H[i, j] * V[:, i]\n            \n        if j + 1 < k:\n            norm_w = np.linalg.norm(w)\n            if norm_w < tau:\n                # Breakdown\n                m = j + 1\n                break\n            H[j + 1, j] = norm_w\n            V[:, j + 1] = w / H[j + 1, j]\n\n    if m < k:\n        V = V[:, :m]\n        H = H[:m, :m]\n    \n    # Calculate stability metrics\n    # Orthogonality loss\n    ortho_matrix = V.T @ V - np.eye(m)\n    np.fill_diagonal(ortho_matrix, 0)\n    ortho_loss = np.max(np.abs(ortho_matrix)) if m > 1 else 0.0\n\n    # Arnoldi residual ratio\n    norm_A_F = np.linalg.norm(A, 'fro')\n    if norm_A_F == 0:\n        norm_A_F = 1.0 # Avoid division by zero\n    \n    residual_matrix = A @ V - V @ H\n    arnoldi_residual = np.linalg.norm(residual_matrix, 'fro') / norm_A_F\n    \n    return m, ortho_loss, arnoldi_residual\n\ndef householder_hessenberg(A):\n    \"\"\"\n    Performs full Hessenberg reduction using Householder transformations.\n    \"\"\"\n    n = A.shape[0]\n    H = A.copy()\n    Q = np.eye(n, dtype=np.float64)\n\n    for j in range(n - 2):\n        x = H[j + 1:, j].copy()\n        norm_x = np.linalg.norm(x)\n        \n        # Construct Householder vector\n        v = x.copy()\n        # Stable choice for sign\n        v[0] += np.copysign(norm_x, x[0]) if x[0] != 0 else norm_x\n        \n        norm_v = np.linalg.norm(v)\n        if norm_v < 1e-15: # No reflection needed if already zero\n            continue\n            \n        u = v / norm_v\n        u = u.reshape(-1, 1)\n\n        # Apply similarity transformation H = P H P\n        # Left multiplication: H_sub = H_sub - 2 * u * (u.T @ H_sub)\n        H_sub = H[j + 1:, j:]\n        H[j + 1:, j:] -= 2 * u @ (u.T @ H_sub)\n        \n        # Right multiplication: H_sub = H_sub - 2 * (H_sub @ u) * u.T\n        H_sub = H[:, j + 1:]\n        H[:, j + 1:] -= 2 * (H_sub @ u) @ u.T\n\n        # Accumulate Q: Q = Q P\n        Q_sub = Q[:, j + 1:]\n        Q[:, j + 1:] -= 2 * (Q_sub @ u) @ u.T\n\n    # Calculate success metrics\n    # Below-second-subdiagonal norm\n    b_norm = 0.0\n    for j in range(n - 2):\n        for i in range(j + 2, n):\n            b_norm += H[i, j]**2\n    b_norm = np.sqrt(b_norm)\n\n    # Reconstruction residual ratio\n    norm_A_F = np.linalg.norm(A, 'fro')\n    if norm_A_F == 0:\n        norm_A_F = 1.0\n    \n    reconstruction_residual = np.linalg.norm(A - Q @ H @ Q.T, 'fro') / norm_A_F\n\n    return b_norm, reconstruction_residual\n\ndef solve():\n    \"\"\"\n    Sets up test cases, runs the analyses, and prints the final result.\n    \"\"\"\n    \n    # Case 1: Ill-conditioned near-identity Jordan perturbation\n    n1 = 25\n    eps1 = 1e-16\n    A1 = np.eye(n1, dtype=np.float64) + eps1 * np.diag(np.ones(n1 - 1), 1)\n    v1_1_unnormalized = np.ones(n1, dtype=np.float64)\n    v1_1 = v1_1_unnormalized / np.linalg.norm(v1_1_unnormalized)\n\n    # Case 2: Well-conditioned symmetric tridiagonal Toeplitz\n    n2 = 20\n    diag2 = 2 * np.ones(n2)\n    off_diag2 = -1 * np.ones(n2 - 1)\n    A2 = np.diag(diag2) + np.diag(off_diag2, 1) + np.diag(off_diag2, -1)\n    v1_2_unnormalized = np.sin(np.arange(1, n2 + 1, dtype=np.float64))\n    v1_2 = v1_2_unnormalized / np.linalg.norm(v1_2_unnormalized)\n\n    # Case 3: Moderately sized well-behaved tridiagonal\n    n3 = 6\n    diag3 = 2 * np.ones(n3)\n    off_diag3 = -1 * np.ones(n3 - 1)\n    A3 = np.diag(diag3) + np.diag(off_diag3, 1) + np.diag(off_diag3, -1)\n    v1_3_unnormalized = np.ones(n3, dtype=np.float64)\n    v1_3 = v1_3_unnormalized / np.linalg.norm(v1_3_unnormalized)\n\n    test_cases = [\n        (A1, v1_1, n1),\n        (A2, v1_2, n2),\n        (A3, v1_3, n3)\n    ]\n\n    results = []\n    for A, v1, n in test_cases:\n        # Run Classical Gram-Schmidt Arnoldi\n        m, o, rA = classical_gram_schmidt_arnoldi(A, v1, k=n)\n        \n        # Check for CGS-Arnoldi failure\n        cgs_failed = (m < n) or (o > 1e-3) or (rA > 1e-8)\n\n        # Run Householder Hessenberg\n        b, rH = householder_hessenberg(A)\n\n        # Check for Householder success\n        hh_succeeded = (b < 1e-12) and (rH < 1e-12)\n\n        # Final decision for the case\n        decision = cgs_failed and hh_succeeded\n        results.append(str(decision))\n\n    # Print results in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}