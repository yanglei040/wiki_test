{
    "hands_on_practices": [
        {
            "introduction": "To truly master an algorithm, it is essential to understand its core mechanics. This first practice exercise grounds your understanding of Hessenberg reduction by having you perform a single, crucial step using a Householder reflection by hand. By working through this calculation on a carefully chosen symmetric matrix, you will gain a concrete intuition for how zeros are introduced below the subdiagonal and see firsthand how the arithmetic can, in special cases, remain perfectly rational, avoiding any floating-point error. This foundational exercise demystifies the geometric transformation at the heart of the reduction process .",
            "id": "3238489",
            "problem": "Consider the task of reducing a real matrix to Hessenberg form using orthogonal similarity transformations built from Householder reflections. Recall that for a real symmetric matrix, the upper Hessenberg form coincides with tridiagonal form. Construct the specific integer matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n7 & 1 & 2 & 2 \\\\\n1 & 1 & 0 & 0 \\\\\n2 & 0 & 1 & 0 \\\\\n2 & 0 & 0 & 1\n\\end{pmatrix},\n$$\nwhich is symmetric and of size $4 \\times 4$. Starting from the fundamental definitions of orthogonal matrices and Householder reflections (without appealing to any pre-packaged algorithmic shortcut), perform the first similarity step that acts nontrivially only on the trailing $(4-1) \\times (4-1)$ block (that is, rows and columns indexed $2$ through $4$), so as to introduce zeros below the first subdiagonal in column $1$. Then, argue carefully why, for this particular $A$, completing the tridiagonalization can be carried out using only rational arithmetic.\n\nLet $T$ denote the resulting tridiagonal matrix obtained from $A$ by this Hessenberg (tridiagonal) reduction. Compute the exact value of the $(2,1)$ entry of $T$. Express your final answer exactly; no rounding is required.",
            "solution": "The problem asks for two things: first, to compute the $(2,1)$ entry of the final tridiagonal matrix $T$ obtained from a given symmetric matrix $A$; second, to argue why the entire reduction can be performed using only rational arithmetic.\n\nThe given matrix is:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n7 & 1 & 2 & 2 \\\\\n1 & 1 & 0 & 0 \\\\\n2 & 0 & 1 & 0 \\\\\n2 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThis is a real, symmetric $4 \\times 4$ matrix. Its Hessenberg reduction results in a symmetric tridiagonal matrix. This should require $n-2 = 2$ steps, but we will see that only one is needed for this specific matrix.\n\n**Step 1: Compute the (2,1) entry**\n\nThe first step of the reduction introduces zeros in the first column at positions $(3,1)$ and $(4,1)$. We apply a similarity transformation $A_1 = P_1 A P_1$, where $P_1$ is a Householder matrix that acts on rows and columns 2 through 4. It is constructed from the sub-vector of the first column, $x = (1, 2, 2)^T$.\n\nThe Householder reflection transforms $x$ into a vector parallel to $e_1 = (1,0,0)^T$. The length is preserved, so the target vector is $\\alpha e_1$ where $|\\alpha| = \\|x\\|_2$.\n$$\n\\|x\\|_2 = \\sqrt{1^2 + 2^2 + 2^2} = \\sqrt{9} = 3.\n$$\nFor numerical stability, we choose $\\alpha = -\\text{sgn}(x_1)\\|x\\|_2 = -(+1)(3) = -3$.\n\nThe transformation $P_1$ is of the form $\\begin{pmatrix} 1 & \\mathbf{0}^T \\\\ \\mathbf{0} & H \\end{pmatrix}$, where $H$ is a $3 \\times 3$ Householder matrix. The first column of the transformed matrix $A_1$ is:\n$$\nA_1 e_1 = P_1 A P_1 e_1 = P_1 A e_1 = P_1 \\begin{pmatrix} 7 \\\\ 1 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 7 \\\\ Hx \\end{pmatrix} = \\begin{pmatrix} 7 \\\\ -3 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nSince $A$ is symmetric and $P_1$ is symmetric and orthogonal, $A_1 = P_1 A P_1$ is also symmetric. This means the first row of $A_1$ is the transpose of its first column: $(7, -3, 0, 0)$.\n\nThe second step of the reduction, if necessary, would not alter the first two rows or columns. Therefore, the $(2,1)$ entry of the final tridiagonal matrix $T$ is determined by this first step.\n$$ T(2,1) = A_1(2,1) = -3. $$\n\n**Step 2: Argue for Rational Arithmetic**\n\nThe entire procedure uses only rational arithmetic if, at every step, the input matrix is rational and the squared norm of the vector being reflected is a perfect square of a rational number (ensuring its norm is rational).\n\n1.  The initial matrix $A$ contains only integers, which are rational.\n2.  In the first step, the vector to be transformed is $x = (1, 2, 2)^T$. Its entries are rational, and its norm $\\|x\\|_2 = 3$ is rational. The Householder vector $v = x - \\alpha e_1 = (1,2,2)^T - (-3)(1,0,0)^T = (4,2,2)^T$ has rational entries. The corresponding reflector matrix $P_1$ is therefore rational, and so is the resulting matrix $A_1 = P_1 A P_1$.\n3.  For the second step, we would act on the sub-vector of the second column of $A_1$, specifically $y = (A_1(3,2), A_1(4,2))^T$. Due to the symmetry of $A_1$, these entries are equal to $A_1(2,3)$ and $A_1(2,4)$. Let's verify if $A_1$ is already tridiagonal by calculating these entries.\n    From the vector $v = (4,2,2)^T$, we can construct the $3\\times3$ Householder matrix $H = I - 2 \\frac{vv^T}{v^Tv} = I - \\frac{1}{12}vv^T$.\n    $$ H = \\frac{1}{3}\\begin{pmatrix}-1 & -2 & -2 \\\\ -2 & 2 & -1 \\\\ -2 & -1 & 2\\end{pmatrix} $$\n    The second row of $A_1$ is given by $e_2^T A_1 = (e_2^T P_1) A P_1$. The vector $p_{1,2}^T = e_2^T P_1$ is the second row of $P_1$, which is $(0, H_{11}, H_{12}, H_{13}) = (0, -1/3, -2/3, -2/3)$.\n    First, $p_{1,2}^T A = (0, -1/3, -2/3, -2/3) A = (-3, -1/3, -2/3, -2/3)$.\n    Now we apply $P_1$ from the right.\n    $A_1(2,3) = (p_{1,2}^T A) \\cdot (\\text{3rd column of } P_1) = (-3, -1/3, -2/3, -2/3) \\cdot (0, -2/3, 2/3, -1/3)^T = 0 + \\frac{2}{9} - \\frac{4}{9} + \\frac{2}{9} = 0$.\n    $A_1(2,4) = (p_{1,2}^T A) \\cdot (\\text{4th column of } P_1) = (-3, -1/3, -2/3, -2/3) \\cdot (0, -2/3, -1/3, 2/3)^T = 0 + \\frac{2}{9} + \\frac{2}{9} - \\frac{4}{9} = 0$.\n    Since $A_1(2,3)=0$ and $A_1(2,4)=0$, the vector for the second step is $y=(0,0)^T$. Its norm is $\\|y\\|_2=0$, which is rational. The transformation is trivial (the identity).\n4.  Since the norms of the vectors at each stage were rational (3 and 0), all calculations remain within the field of rational numbers $\\mathbb{Q}$.\n\nThe final tridiagonal matrix is $T=A_1$, and its $(2,1)$ entry is -3.",
            "answer": "$$\\boxed{-3}$$"
        },
        {
            "introduction": "In numerical computing, a theoretically correct algorithm is only as good as its performance in the face of finite-precision arithmetic. This coding challenge explores the critical concept of numerical stability by comparing two different methods for achieving Hessenberg form: the Arnoldi process using classical Gram-Schmidt and the standard Householder reduction. By implementing both and testing them on a matrix designed to cause instability, you will directly observe why the latter is overwhelmingly preferred in professional software and gain a practical appreciation for building robust numerical tools .",
            "id": "3238560",
            "problem": "You are given the task of contrasting two approaches to upper Hessenberg reduction for a real square matrix: a naive approach based on the classical Gram-Schmidt orthonormalization embedded in the Arnoldi process, and a stable approach based on Householder reflections applied as two-sided similarity transformations. The goal is to construct and analyze matrices for which the classical Gram-Schmidt approach exhibits numerical instability, while the Householder-based method remains robust.\n\nFundamental base and core definitions:\n- A matrix $H \\in \\mathbb{R}^{n \\times n}$ is upper Hessenberg if $H_{i,j} = 0$ for all $i \\ge j + 2$.\n- The classical Gram-Schmidt orthonormalization for a sequence of vectors $\\{w_j\\}$ constructs $v_1 = w_1 / \\|w_1\\|_2$ and, for $j \\ge 1$, defines\n$$\nh_{i,j} = v_i^\\top w_j, \\quad \\tilde{w}_j = w_j - \\sum_{i=1}^{j} h_{i,j} v_i, \\quad v_{j+1} = \\tilde{w}_j / \\|\\tilde{w}_j\\|_2,\n$$\nwhenever $\\|\\tilde{w}_j\\|_2 \\ne 0$. The Arnoldi process applies this to $w_j = A v_j$ for a matrix $A \\in \\mathbb{R}^{n \\times n}$, generating an orthonormal basis $V_k = [v_1,\\dots,v_k]$ and an upper Hessenberg $H_k \\in \\mathbb{R}^{k \\times k}$ satisfying $A V_k \\approx V_k H_k$.\n- A Householder reflector is $P = I - 2 u u^\\top$, where $u \\in \\mathbb{R}^m$ is chosen so that $P x = \\alpha e_1$ for any given $x \\in \\mathbb{R}^m$ and some scalar $\\alpha \\in \\mathbb{R}$. Householder-based Hessenberg reduction applies a sequence of such reflectors on the left and right to compute an orthogonal $Q$ and an upper Hessenberg $H$ such that $A = Q H Q^\\top$.\n\nYour tasks:\n1. Implement a classical Gram-Schmidt Arnoldi routine that, given $A \\in \\mathbb{R}^{n \\times n}$, a starting unit vector $v_1 \\in \\mathbb{R}^n$, and target dimension $k$, returns an orthonormal matrix $V \\in \\mathbb{R}^{n \\times m}$ (with $m \\le k$ depending on breakdown), and an upper Hessenberg matrix $H \\in \\mathbb{R}^{m \\times m}$ such that $A V \\approx V H$. Use a single pass of classical Gram-Schmidt without any reorthonormalization. Quantify numerical stability using:\n   - The orthogonality loss measure $o = \\max_{i \\ne j} |(V^\\top V)_{i,j}|$.\n   - The Arnoldi residual ratio $r_A = \\|A V - V H\\|_F / \\|A\\|_F$.\n   - A breakdown is declared if a new vector norm falls below a threshold $\\tau = 10^{-14}$, resulting in $m < k$.\n\n2. Implement a Householder-based full Hessenberg reduction that computes $Q \\in \\mathbb{R}^{n \\times n}$ and $H \\in \\mathbb{R}^{n \\times n}$ satisfying $A = Q H Q^\\top$. Apply a sequence of Householder reflectors $P_j = I - 2 u_j u_j^\\top$ for $j = 0,1,\\dots,n-3$, each targeted to zero out entries below the first subdiagonal in column $j$. Quantify numerical success using:\n   - The below-second-subdiagonal norm $b = \\sqrt{\\sum_{i \\ge j+2} \\sum_j H_{i,j}^2}$.\n   - The reconstruction residual ratio $r_H = \\|A - Q H Q^\\top\\|_F / \\|A\\|_F$.\n\n3. Construct and analyze the following test suite of matrices and starting vectors:\n   - Case $1$ (ill-conditioned near-identity Jordan perturbation): Let $n = 25$, $\\varepsilon = 10^{-16}$, and $J \\in \\mathbb{R}^{n \\times n}$ be the Jordan block with $J_{i,i} = 0$ for all $i$, $J_{i,i+1} = 1$ for $i = 1,\\dots,n-1$, and all other entries $0$. Define $A_1 = I + \\varepsilon J$. Use starting vector $v_1$ with entries $(v_1)_i = 1$ for all $i$, normalized to unit $2$-norm. This matrix is chosen so that the Krylov sequence $\\{A_1^j v_1\\}$ becomes nearly colinear, triggering numerical instability in the classical Gram-Schmidt Arnoldi process.\n   - Case $2$ (well-conditioned symmetric tridiagonal Toeplitz): Let $n = 20$ and $A_2 \\in \\mathbb{R}^{n \\times n}$ be defined by $(A_2)_{i,i} = 2$, $(A_2)_{i,i+1} = -1$, $(A_2)_{i+1,i} = -1$ for $i = 1,\\dots,n-1$, and all other entries $0$. Use starting vector $v_1$ with entries $(v_1)_i = \\sin(i)$, normalized to unit $2$-norm.\n   - Case $3$ (moderately sized well-behaved tridiagonal): Let $n = 6$ and define $A_3$ in the same way as $A_2$ with size $n = 6$. Use starting vector $v_1$ with entries $(v_1)_i = 1$ for all $i$, normalized to unit $2$-norm.\n\nDecision rule for each case:\n- Declare the classical Gram-Schmidt Arnoldi method as having failed if any of the following hold: $m < n$, $o > 10^{-3}$, or $r_A > 10^{-8}$.\n- Declare the Householder method as having succeeded if both $b < 10^{-12}$ and $r_H < 10^{-12}$.\n- For each case, the program must output a boolean indicating whether the naive Gram-Schmidt-based Hessenberg reduction fails due to numerical instability while the Householder-based method succeeds. That is, output $\\text{True}$ if classical Gram-Schmidt Arnoldi fails and Householder succeeds, and $\\text{False}$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a boolean corresponding to the three cases in the order $A_1$, $A_2$, $A_3$.\n\nNo physical units, angles, or percentages are involved in this problem; all quantities are dimensionless real numbers and matrices. The program must be completely self-contained and require no user input.",
            "solution": "The problem statement has been critically evaluated and is determined to be valid. It is scientifically grounded in the principles of numerical linear algebra, well-posed with clear definitions and objective criteria, and internally consistent. It presents a standard, albeit important, comparative analysis between two fundamental algorithms for Hessenberg reduction.\n\nThe task is to compare the numerical stability of two methods for computing the upper Hessenberg decomposition of a real square matrix $A \\in \\mathbb{R}^{n \\times n}$. The first method is the Arnoldi process employing classical Gram-Schmidt (CGS) without reorthonormalization. The second is a sequence of Householder similarity transformations.\n\n### Method 1: Classical Gram-Schmidt Arnoldi (CGS-Arnoldi) Procedure\n\nThe Arnoldi process is an iterative method that constructs an orthonormal basis for the Krylov subspace $\\mathcal{K}_k(A, v_1) = \\text{span}\\{v_1, A v_1, \\dots, A^{k-1} v_1\\}$. When implemented with the classical Gram-Schmidt procedure, it generates a sequence of orthonormal vectors $v_1, v_2, \\dots, v_k$ and an upper Hessenberg matrix $H_k \\in \\mathbb{R}^{k \\times k}$ which satisfy the Arnoldi relation $A V_k = V_k H_k + f_k e_k^\\top$, where $V_k = [v_1, \\dots, v_k]$, $f_k$ is the residual vector, and $e_k$ is the $k$-th standard basis vector.\n\nThe algorithm proceeds as follows, given $A \\in \\mathbb{R}^{n \\times n}$, a starting unit vector $v_1 \\in \\mathbb{R}^n$, and a target dimension $k$:\n1. Initialize an $n \\times k$ matrix $V$ and a $k \\times k$ matrix $H$. Set the first column of $V$ to $v_1$.\n2. For $j = 1, \\dots, k$:\n   a. Compute the next Krylov vector, $w = A v_j$.\n   b. Orthogonalize $w$ against the previously computed basis vectors $\\{v_1, \\dots, v_j\\}$:\n      For $i = 1, \\dots, j$:\n      i. Compute the projection coefficient: $h_{i,j} = v_i^\\top w$.\n      ii. Subtract the projection from $w$: $w \\leftarrow w - h_{i,j} v_i$.\n   c. If $j < k$:\n      i. Compute the norm of the new vector: $h_{j+1,j} = \\|w\\|_2$.\n      ii. If $h_{j+1,j}$ is below a threshold $\\tau$ (e.g., $10^{-14}$), the process is said to break down. The Krylov subspace is invariant, and the algorithm terminates, resulting in a matrix $V$ of dimension $n \\times j$ and $H$ of dimension $j \\times j$.\n      iii. Normalize to get the next basis vector: $v_{j+1} = w / h_{j+1,j}$.\n\nThe core numerical issue with CGS arises in step 2b. If the vector $A v_j$ is nearly linearly dependent on the preceding basis vectors $\\{v_1, \\dots, v_j\\}$, then the vector $w$ after orthogonalization will be the result of subtracting large, nearly equal quantities. This leads to catastrophic cancellation and a large relative error in the computed $w$. Consequently, the newly computed vector $v_{j+1}$ will not be orthogonal to the previous vectors, leading to a loss of orthogonality in the basis $V$. This instability is quantified by the orthogonality loss measure $o = \\max_{i \\ne j} |(V^\\top V)_{i,j}|$. The Arnoldi residual ratio, $r_A = \\|A V - V H\\|_F / \\|A\\|_F$, measures how well the decomposition approximates the action of $A$ on the subspace spanned by $V$.\n\n### Method 2: Householder Hessenberg Reduction\n\nThis method reduces a dense matrix $A$ to an upper Hessenberg form $H$ via a sequence of similarity transformations using Householder reflectors. The goal is to find an orthogonal matrix $Q$ such that $A = Q H Q^\\top$. A Householder reflector is an orthogonal matrix of the form $P = I - 2 u u^\\top$ where $u$ is a unit vector. For a given vector $x$, $u$ can be chosen such that $P x$ is a multiple of the first standard basis vector $e_1$.\n\nThe algorithm for a full reduction of $A \\in \\mathbb{R}^{n \\times n}$ proceeds as follows:\n1. Initialize $H = A$ and $Q = I_n$.\n2. For $j = 0, 1, \\dots, n-3$:\n   a. Consider the vector $x$ consisting of the entries in column $j$ below the first subdiagonal: $x = H_{j+2:n, j}$.\n   b. Construct a Householder vector $u_j \\in \\mathbb{R}^{n-j-1}$ such that the corresponding reflector $P'_j = I - 2 u_j u_j^\\top$ zeroes out all but the first entry of $x$. A stable choice for the intermediate vector $v$ is $v = x + \\text{sign}(x_1) \\|x\\|_2 e_1$, from which $u_j=v/\\|v\\|_2$.\n   c. Form the full $n \\times n$ reflector $P_j$ by embedding $P'_j$ into an identity matrix:\n      $$ P_j = \\begin{pmatrix} I_{j+1} & 0 \\\\ 0 & P'_{j} \\end{pmatrix} $$\n   d. Apply the similarity transformation to the matrix: $H \\leftarrow P_j H P_j^\\top$. Since $P_j$ is its own inverse, $P_j^\\top=P_j$. This is done by a left multiplication $H \\leftarrow P_j H$ followed by a right multiplication $H \\leftarrow H P_j$.\n   e. Accumulate the orthogonal transformation: $Q \\leftarrow Q P_j$.\n\nAfter $n-2$ steps, the resulting matrix $H$ is upper Hessenberg. The final orthogonal matrix is $Q = P_0 P_1 \\dots P_{n-3}$. Since each transformation $P_j$ is orthogonal, their product $Q$ is also orthogonal. This property makes the Householder reduction backward stable. The computed $H$ is the exact Hessenberg form of a matrix very close to the original $A$. This stability is evaluated by the reconstruction residual ratio $r_H = \\|A - Q H Q^\\top\\|_F / \\|A\\|_F$ and the structure of $H$ is checked by the below-second-subdiagonal norm $b = \\sqrt{\\sum_{i \\ge j+2} \\sum_j H_{i,j}^2}$.\n\n### Analysis of Test Cases\n\nThe provided test cases are designed to highlight the differing stability properties of these two methods.\n\n**Case 1: $A_1 = I + \\varepsilon J$ with $\\varepsilon = 10^{-16}$, $n=25$.**\nThe matrix $J$ is a nilpotent Jordan block. The powers $J^m$ push the non-zero diagonal of ones further to the upper right, becoming zero for $m \\ge n$. For a small $\\varepsilon$, $A_1^m v_1 = (I + \\varepsilon J)^m v_1 \\approx (I + m \\varepsilon J) v_1$. The vectors $v_1, A_1 v_1, A_1^2 v_1, \\dots$ forming the Krylov sequence will become nearly collinear very quickly. This is the archetypal scenario where CGS fails due to catastrophic cancellation. We expect a severe loss of orthogonality (large $o$) and a high Arnoldi residual (large $r_A$). Conversely, Householder reduction operates via stable orthogonal transformations and should be unaffected, successfully reducing $A_1$ to Hessenberg form with high accuracy. Thus, CGS-Arnoldi is expected to fail while Householder succeeds.\n\n**Case 2: Symmetric Tridiagonal Toeplitz Matrix, $n=20$.**\nThis matrix, $A_2$, is the negative of the 1D discrete Laplacian operator. It is symmetric and well-conditioned. For a symmetric matrix, the Arnoldi process (in exact arithmetic) becomes the Lanczos algorithm, producing a symmetric tridiagonal matrix $H$. The eigenvalues of $A_2$ are well-separated, which typically leads to a well-conditioned Krylov basis. Therefore, CGS-Arnoldi is expected to perform well, maintaining good orthogonality. The Householder method, being universally stable, will also succeed. The condition for the test to be `True` (CGS fails AND Householder succeeds) will not be met.\n\n**Case 3: Symmetric Tridiagonal Toeplitz Matrix, $n=6$.**\nThis is a smaller version of Case 2. The same logic applies: $A_3$ is a well-behaved symmetric matrix, for which CGS-Arnoldi is stable. Both methods will succeed. The test condition will not be met.\n\nThe implementation will now proceed to verify these theoretical predictions computationally.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classical_gram_schmidt_arnoldi(A, v1_start, k, tau=1e-14):\n    \"\"\"\n    Performs Hessenberg reduction using the classical Gram-Schmidt Arnoldi process.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, k), dtype=np.float64)\n    H = np.zeros((k, k), dtype=np.float64)\n    \n    # Normalize the starting vector\n    V[:, 0] = v1_start / np.linalg.norm(v1_start)\n    \n    m = k\n    for j in range(k):\n        w = A @ V[:, j]\n        \n        for i in range(j + 1):\n            H[i, j] = V[:, i].T @ w\n            w = w - H[i, j] * V[:, i]\n            \n        if j + 1 < k:\n            norm_w = np.linalg.norm(w)\n            if norm_w < tau:\n                # Breakdown\n                m = j + 1\n                break\n            H[j + 1, j] = norm_w\n            V[:, j + 1] = w / H[j + 1, j]\n\n    if m < k:\n        V = V[:, :m]\n        H = H[:m, :m]\n    \n    # Calculate stability metrics\n    # Orthogonality loss\n    ortho_matrix = V.T @ V - np.eye(m)\n    np.fill_diagonal(ortho_matrix, 0)\n    ortho_loss = np.max(np.abs(ortho_matrix)) if m > 1 else 0.0\n\n    # Arnoldi residual ratio\n    norm_A_F = np.linalg.norm(A, 'fro')\n    if norm_A_F == 0:\n        norm_A_F = 1.0 # Avoid division by zero\n    \n    residual_matrix = A @ V - V @ H\n    arnoldi_residual = np.linalg.norm(residual_matrix, 'fro') / norm_A_F\n    \n    return m, ortho_loss, arnoldi_residual\n\ndef householder_hessenberg(A):\n    \"\"\"\n    Performs full Hessenberg reduction using Householder transformations.\n    \"\"\"\n    n = A.shape[0]\n    H = A.copy()\n    Q = np.eye(n, dtype=np.float64)\n\n    for j in range(n - 2):\n        x = H[j + 1:, j].copy()\n        norm_x = np.linalg.norm(x)\n        \n        # Construct Householder vector\n        v = x.copy()\n        # Stable choice for sign\n        v[0] += np.copysign(norm_x, x[0]) if x[0] != 0 else norm_x\n        \n        norm_v = np.linalg.norm(v)\n        if norm_v < 1e-15: # No reflection needed if already zero\n            continue\n            \n        u = v / norm_v\n        u = u.reshape(-1, 1)\n\n        # Apply similarity transformation H = P H P\n        # Left multiplication: H_sub = H_sub - 2 * u * (u.T @ H_sub)\n        H_sub = H[j + 1:, j:]\n        H[j + 1:, j:] -= 2 * u @ (u.T @ H_sub)\n        \n        # Right multiplication: H_sub = H_sub - 2 * (H_sub @ u) * u.T\n        H_sub = H[:, j + 1:]\n        H[:, j + 1:] -= 2 * (H_sub @ u) @ u.T\n\n        # Accumulate Q: Q = Q P\n        Q_sub = Q[:, j + 1:]\n        Q[:, j + 1:] -= 2 * (Q_sub @ u) @ u.T\n\n    # Calculate success metrics\n    # Below-second-subdiagonal norm\n    b_norm = 0.0\n    for j in range(n - 2):\n        for i in range(j + 2, n):\n            b_norm += H[i, j]**2\n    b_norm = np.sqrt(b_norm)\n\n    # Reconstruction residual ratio\n    norm_A_F = np.linalg.norm(A, 'fro')\n    if norm_A_F == 0:\n        norm_A_F = 1.0\n    \n    reconstruction_residual = np.linalg.norm(A - Q @ H @ Q.T, 'fro') / norm_A_F\n\n    return b_norm, reconstruction_residual\n\ndef solve():\n    \"\"\"\n    Sets up test cases, runs the analyses, and prints the final result.\n    \"\"\"\n    \n    # Case 1: Ill-conditioned near-identity Jordan perturbation\n    n1 = 25\n    eps1 = 1e-16\n    A1 = np.eye(n1, dtype=np.float64) + eps1 * np.diag(np.ones(n1 - 1), 1)\n    v1_1_unnormalized = np.ones(n1, dtype=np.float64)\n    v1_1 = v1_1_unnormalized / np.linalg.norm(v1_1_unnormalized)\n\n    # Case 2: Well-conditioned symmetric tridiagonal Toeplitz\n    n2 = 20\n    diag2 = 2 * np.ones(n2)\n    off_diag2 = -1 * np.ones(n2 - 1)\n    A2 = np.diag(diag2) + np.diag(off_diag2, 1) + np.diag(off_diag2, -1)\n    v1_2_unnormalized = np.sin(np.arange(1, n2 + 1, dtype=np.float64))\n    v1_2 = v1_2_unnormalized / np.linalg.norm(v1_2_unnormalized)\n\n    # Case 3: Moderately sized well-behaved tridiagonal\n    n3 = 6\n    diag3 = 2 * np.ones(n3)\n    off_diag3 = -1 * np.ones(n3 - 1)\n    A3 = np.diag(diag3) + np.diag(off_diag3, 1) + np.diag(off_diag3, -1)\n    v1_3_unnormalized = np.ones(n3, dtype=np.float64)\n    v1_3 = v1_3_unnormalized / np.linalg.norm(v1_3_unnormalized)\n\n    test_cases = [\n        (A1, v1_1, n1),\n        (A2, v1_2, n2),\n        (A3, v1_3, n3)\n    ]\n\n    results = []\n    for A, v1, n in test_cases:\n        # Run Classical Gram-Schmidt Arnoldi\n        m, o, rA = classical_gram_schmidt_arnoldi(A, v1, k=n)\n        \n        # Check for CGS-Arnoldi failure\n        cgs_failed = (m < n) or (o > 1e-3) or (rA > 1e-8)\n\n        # Run Householder Hessenberg\n        b, rH = householder_hessenberg(A)\n\n        # Check for Householder success\n        hh_succeeded = (b < 1e-12) and (rH < 1e-12)\n\n        # Final decision for the case\n        decision = cgs_failed and hh_succeeded\n        results.append(str(decision))\n\n    # Print results in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the robust Householder reduction method, this final practice introduces a key technique for further enhancing its stability: column pivoting. For matrices with certain challenging structures, the standard algorithm's choices can be suboptimal and amplify rounding errors. This coding exercise guides you through implementing a pivoting strategy that selects the best data at each stage of the reduction, thereby minimizing error propagation and leading to a more accurate final decomposition. This practice reflects the sophisticated engineering that goes into creating high-quality, production-level numerical linear algebra software .",
            "id": "3238599",
            "problem": "You are asked to design, implement, and validate a column-pivoted Hessenberg reduction algorithm for a real square matrix. The goal is to construct a factorization that reorganizes the columns of a given matrix and then applies orthogonal transformations from the left to produce an upper Hessenberg matrix. The fundamental base for this problem must begin with the following definitions and well-tested facts: a permutation matrix is orthogonal, an orthogonal matrix preserves Euclidean norms, a Householder reflector is orthogonal and can zero out components of a vector, and an upper Hessenberg matrix has all entries below its first subdiagonal equal to zero.\n\nGiven a real square matrix $A \\in \\mathbb{R}^{n \\times n}$, introduce column pivoting into a left-applied Hessenberg reduction as follows. For each reduction step indexed by $k$ from $0$ to $n-2$, choose a pivot column index $j \\in \\{k, k+1, \\dots, n-1\\}$ that maximizes the Euclidean norm of the tail of column $j$ from row $k+1$ to row $n-1$, swap columns $k$ and $j$, then construct a Householder reflector that acts on rows $k+1$ to $n-1$ to eliminate entries of column $k$ below row $k+1$. Accumulate the right column-swaps into a permutation matrix $P \\in \\mathbb{R}^{n \\times n}$, and accumulate the left Householder reflectors into an orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ so that the final relation is $A P = Q H$ where $H \\in \\mathbb{R}^{n \\times n}$ is upper Hessenberg. Analyze the properties of the decomposition $P A = Q H$ by relating it to the constructed $A P = Q H$, and characterize the role of column pivoting in stability and structure.\n\nYou must implement a program that:\n- For each input matrix $A$, computes matrices $Q$, $H$, and $P$ such that $A P = Q H$, where $Q$ is orthogonal and $H$ is upper Hessenberg, using the following steps grounded in the fundamental base:\n  1. At step $k$, select the pivot column index $j$ maximizing $\\|A_{k+1:n-1,j}\\|_2$.\n  2. Form the permutation that swaps columns $k$ and $j$ and update $A$ and $P$ by right-multiplication with this permutation.\n  3. Construct a Householder vector $v \\in \\mathbb{R}^{n-k-1}$ that maps the vector $x = A_{k+1:n-1,k}$ to a multiple of the first standard basis vector and define the reflector $L_k = I - 2 v v^\\top$ acting on the subspace of rows $k+1$ to $n-1$.\n  4. Apply the left transformation to update $A$ and accumulate the orthogonal factor $Q$ from all left reflectors so that the output satisfies $A P = Q H$.\n- Computes and reports, for each test matrix, the following three quantities:\n  1. The Frobenius residual $r = \\|A P - Q H\\|_F$.\n  2. The orthogonality error $e = \\|Q^\\top Q - I\\|_F$.\n  3. A boolean flag $b$ that is $\\text{True}$ if $H$ is upper Hessenberg within tolerance $10^{-10}$, that is, $|H_{i,j}| \\le 10^{-10}$ for all $i > j + 1$, and $\\text{False}$ otherwise.\n\nYou must use the following fixed test suite of matrices, which together exercise typical, boundary, and edge-case behavior:\n- Test $1$ (general $5 \\times 5$ case):\n  $$A_1 = \\begin{bmatrix}\n  4 & 1 & -2 & 2 & 3 \\\\\n  3 & 6 & -1 & -3 & 2 \\\\\n  2 & -1 & 8 & 1 & -4 \\\\\n  1 & 0 & -3 & 7 & 5 \\\\\n  5 & -2 & 1 & -1 & 9\n  \\end{bmatrix}.$$\n- Test $2$ (identity $4 \\times 4$ case):\n  $$A_2 = I_4.$$\n- Test $3$ (column-dominant rank-$1$ $4 \\times 4$ case to exercise pivot selection):\n  $$A_3 = \\begin{bmatrix}\n  0 & 100 & 0 & 0 \\\\\n  0 & -50 & 0 & 0 \\\\\n  0 & 25 & 0 & 0 \\\\\n  0 & -12.5 & 0 & 0\n  \\end{bmatrix}.$$\n- Test $4$ ($1 \\times 1$ case):\n  $$A_4 = \\begin{bmatrix} 7 \\end{bmatrix}.$$\n- Test $5$ ($2 \\times 2$ case):\n  $$A_5 = \\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\end{bmatrix}.$$\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the form $[r, e, b]$ for that test case, in that order. No physical units are involved in this problem. Angles are not applicable. Percentages are not applicable; all numeric outputs must be presented as standard decimal numbers. The program must be entirely self-contained, must not read input, and must use only the specified libraries.",
            "solution": "The problem requires the design and implementation of a column-pivoted Hessenberg reduction algorithm for a real square matrix $A \\in \\mathbb{R}^{n \\times n}$. The objective is to compute an orthogonal matrix $Q$, a permutation matrix $P$, and an upper Hessenberg matrix $H$ such that the factorization $A P = Q H$ holds. The solution must be validated by computing the Frobenius norm of the residual $r = \\|A P - Q H\\|_F$, the orthogonality error $e = \\|Q^\\top Q - I\\|_F$, and a boolean flag $b$ indicating whether $H$ is structurally upper Hessenberg.\n\nThe specified algorithm is a sequence of transformations applied to the input matrix $A$. Let the initial matrix be $A^{(0)} = A$. The process iterates for steps $k = 0, 1, \\dots, n-2$. At each step $k$, we modify the current matrix $A^{(k)}$ to introduce zeros in the $k$-th column below the first subdiagonal.\n\n**Step-by-Step Algorithm Derivation**\n\nLet $H_0 = A$, $Q_0 = I_n$, and $P_0 = I_n$. We will generate a sequence of matrices $H_k$, $Q_k$, $P_k$ such that at the end of the process, $H = H_{n-1}$, $Q = Q_{n-1}$, and $P = P_{n-1}$.\n\nFor $k = 0, 1, \\dots, n-2$:\n\n1.  **Column Pivoting**: To enhance numerical stability, we select a pivot column. The transformation at step $k$ will act on a vector taken from the $k$-th column. The stability is improved if this vector has a large norm. We therefore examine the columns of the current matrix, $H_k$, from index $k$ to $n-1$. The pivot column, indexed by $j_{pivot}$, is chosen such that the Euclidean norm of the sub-column vector below the $k$-th row is maximized:\n    $$ j_{pivot} = \\underset{j \\in \\{k, k+1, \\dots, n-1\\}}{\\text{argmax}} \\| (H_k)_{k+1:n-1, j} \\|_2 $$\n    Let $\\Pi_k$ be the elementary permutation matrix that swaps columns $k$ and $j_{pivot}$. We update the matrix $H_k$ by applying this permutation from the right: $H_k \\leftarrow H_k \\Pi_k$. The total permutation is accumulated: $P_{k+1} = P_k \\Pi_k$.\n\n2.  **Householder Transformation**: After pivoting, the new $k$-th column is used to construct a Householder reflector. Let $x = (H_k)_{k+1:n-1, k}$ be the vector to be transformed. This vector $x$ has dimension $m = n-(k+1)$. A Householder reflector is an orthogonal matrix $L_k$ that transforms $x$ into a vector parallel to the first standard basis vector $e_1 \\in \\mathbb{R}^m$. Specifically, $L_k x = \\sigma e_1$.\n    The reflector is defined as $L_k = I_m - \\beta v v^\\top$, where $v$ is the Householder vector. To ensure stability and avoid subtractive cancellation, $v$ is constructed as:\n    $$ v = x + \\text{sign}(x_1) \\|x\\|_2 e_1 $$\n    where $x_1$ is the first component of $x$. The coefficient $\\beta$ is given by $\\beta = 2 / (v^\\top v)$. If $x$ is a zero vector, the transformation is the identity and this step is skipped.\n\n3.  **Applying the Transformations**: The $m \\times m$ reflector $L_k$ is embedded into an $n \\times n$ matrix, which we denote $\\mathcal{Q}_k$, by placing it in the bottom-right block:\n    $$ \\mathcal{Q}_k = \\begin{pmatrix} I_{k+1} & 0 \\\\ 0 & L_k \\end{pmatrix} $$\n    This orthogonal transformation is applied from the left to the current matrix $H_k$. This operation only affects rows $k+1$ through $n-1$. Crucially, since the first $k$ columns of $H_k$ already have the desired zero structure, and the transformation $\\mathcal{Q}_k$ does not mix rows $0, \\dots, k$ with rows $k+1, \\dots, n-1$, this structure is preserved.\n    The update rule for the matrix is: $H_{k+1} = \\mathcal{Q}_k H_k$.\n    The total orthogonal matrix $Q$ is accumulated by updating $Q_k$ with $\\mathcal{Q}_k$. The final decomposition after $n-2$ steps will be $H = (\\mathcal{Q}_{n-2} \\dots \\mathcal{Q}_0) A (\\Pi_0 \\dots \\Pi_{n-2})$. Let $P = \\Pi_0 \\dots \\Pi_{n-2}$ and $Q^T = \\mathcal{Q}_{n-2} \\dots \\mathcal{Q}_0$. Then $H = Q^T A P$, which rearranges to $A P = Q H$. To obtain $Q$ from the reflectors, we can start with $Q=I$ and update it at each step as $Q \\leftarrow Q \\mathcal{Q}_k^T = Q \\mathcal{Q}_k$ (since reflectors are symmetric).\n\n**Analysis of the $P A = Q H$ Relation**\n\nThe problem asks to analyze the properties of a hypothetical decomposition $P A = Q H$ based on the constructed factorization $A P = Q H$. From our algorithm, we have $A P = Q H$. We can isolate $A$ using the orthogonality of $P$ (i.e., $P^{-1} = P^T$):\n$$ A = Q H P^T $$\nPre-multiplying this equation by $P$ yields an expression for the matrix product $P A$:\n$$ P A = P (Q H P^T) = (P Q) H P^T $$\nHere, $P Q$ is the product of two orthogonal matrices, which is itself an orthogonal matrix. $H$ is upper Hessenberg, and $P^T$ is orthogonal. This gives a factorization of $P A$ into a product of three matrices with specific structures, but it is not a Hessenberg decomposition of $P A$ in the standard form $(U H' U^T)$, nor is it of the form $Q H$. The query in the problem statement may have been a minor misstatement. The derived relation $P A = (P Q) H P^T$ is the correct consequence of the computed $A P = Q H$ factorization.\n\n**Role of Column Pivoting**\n\nColumn pivoting plays a critical role in the numerical stability of the algorithm. The construction of the Householder vector $v$ and the application of the reflector involve floating-point arithmetic. If the norm of the vector $x$ being transformed is very small, the relative effect of round-off errors can become significant. By choosing the column that maximizes $\\|x\\|_2 = \\|(H_k)_{k+1:n-1, j}\\|_2$, we ensure that the vector being used to generate the reflector is as large as possible. This minimizes the influence of floating-point errors on the computed orthogonal matrix $Q$ and the Hessenberg matrix $H$, leading to a smaller orthogonality error $e = \\|Q^\\top Q - I\\|_F$ and a smaller final residual $r = \\|A P - Q H\\|_F$.\n\nThe implementation will follow this detailed procedure, processing the provided test cases and computing the required validation metrics.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef column_pivoted_hessenberg(A):\n    \"\"\"\n    Computes the column-pivoted Hessenberg decomposition A P = Q H of a real\n    square matrix A.\n\n    Args:\n        A (np.ndarray): The input square matrix.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple (Q, H, P) where\n        Q is orthogonal, H is upper Hessenberg, P is a permutation matrix,\n        and A P = Q H.\n    \"\"\"\n    H = A.copy().astype(np.float64)\n    n = H.shape[0]\n    \n    if n == 0:\n        return np.array([[]]), np.array([[]]), np.array([[]])\n        \n    Q = np.identity(n, dtype=np.float64)\n    P = np.identity(n, dtype=np.float64)\n\n    for k in range(n - 2):\n        # Step 1: Pivot selection\n        # Select column j in {k, ..., n-1} that maximizes the norm of H[k+1:n, j].\n        sub_matrix_for_pivot = H[k + 1:n, k:n]\n        if sub_matrix_for_pivot.size > 0:\n            col_norms = np.linalg.norm(sub_matrix_for_pivot, axis=0)\n            j_rel = np.argmax(col_norms)\n            j_pivot = k + j_rel\n        else:\n            # This branch is for robustness, but the loop range n-2 prevents it for n > 2.\n            # For n<=2, the loop does not run.\n            continue\n            \n        # Step 2: Column swap\n        if j_pivot != k:\n            H[:, [k, j_pivot]] = H[:, [j_pivot, k]]\n            P[:, [k, j_pivot]] = P[:, [j_pivot, k]]\n\n        # Step 3: Householder reflector construction\n        x = H[k + 1:n, k]\n        norm_x = np.linalg.norm(x)\n\n        # Use a tolerance to check if the vector is effectively zero\n        if norm_x > 1e-15:\n            # Define sigma to avoid catastrophic cancellation in v's first component\n            sigma = -np.copysign(norm_x, x[0]) if x[0] != 0 else -norm_x\n            \n            # Construct the un-normalized Householder vector v\n            v = x.copy()\n            v[0] -= sigma  # This corresponds to v = x - sigma * e_1\n            \n            v_dot_v = v @ v\n            if v_dot_v < 1e-15:\n                # If v is a zero vector, the reflector is the identity.\n                continue\n            \n            beta = 2.0 / v_dot_v\n\n            # Step 4: Apply the transformation\n            # Apply reflector to H: H_new = (I - beta*v*v.T) * H_sub\n            # which is calculated as H_sub - beta * v * (v.T * H_sub)\n            sub_H = H[k + 1:n, k:n]\n            vT_subH = v @ sub_H\n            update_H = np.outer(v, vT_subH)\n            H[k + 1:n, k:n] -= beta * update_H\n\n            # Accumulate reflector in Q: Q_new = Q_old * (I - beta*v*v.T)_embedded\n            # which is Q_old - beta * (Q_old * v_embedded) * v_embedded.T\n            sub_Q = Q[:, k + 1:n]\n            Q_v = sub_Q @ v\n            update_Q = np.outer(Q_v, v)\n            Q[:, k + 1:n] -= beta * update_Q\n\n    return Q, H, P\n\ndef check_hessenberg(H, tol=1e-10):\n    \"\"\"\n    Checks if a matrix is upper Hessenberg within a given tolerance.\n    |H_ij| <= tol for all i > j + 1.\n    \"\"\"\n    n = H.shape[0]\n    if n <= 2:\n        return True\n    # np.tril(H, k=-2) gives the part of H below the first subdiagonal.\n    return np.all(np.abs(np.tril(H, k=-2)) <= tol)\n\ndef solve():\n    \"\"\"\n    Main function to run the algorithm on the test suite and print results.\n    \"\"\"\n    test_cases = [\n        np.array([\n            [4, 1, -2, 2, 3],\n            [3, 6, -1, -3, 2],\n            [2, -1, 8, 1, -4],\n            [1, 0, -3, 7, 5],\n            [5, -2, 1, -1, 9]\n        ]),\n        np.identity(4),\n        np.array([\n            [0, 100, 0, 0],\n            [0, -50, 0, 0],\n            [0, 25, 0, 0],\n            [0, -12.5, 0, 0]\n        ]),\n        np.array([[7]]),\n        np.array([[0, 1], [2, 3]])\n    ]\n\n    results = []\n    for A in test_cases:\n        n = A.shape[0]\n        \n        Q, H, P = column_pivoted_hessenberg(A)\n        \n        if n > 0:\n            residual = np.linalg.norm(A @ P - Q @ H, 'fro')\n            orth_error = np.linalg.norm(Q.T @ Q - np.identity(n), 'fro')\n            is_hessenberg = check_hessenberg(H, tol=1e-10)\n        else:\n            residual, orth_error, is_hessenberg = 0.0, 0.0, True\n\n        results.append([residual, orth_error, bool(is_hessenberg)])\n    \n    formatted_results = []\n    for res in results:\n        # Format [r, e, b] as a string, with boolean as True/False\n        formatted_results.append(f\"[{res[0]},{res[1]},{'True' if res[2] else 'False'}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}