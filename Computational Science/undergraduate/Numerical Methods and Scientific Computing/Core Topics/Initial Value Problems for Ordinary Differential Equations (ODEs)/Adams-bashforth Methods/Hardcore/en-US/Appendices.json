{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's walk through a direct application of an Adams-Bashforth method. This first exercise  focuses on the core mechanics of the two-step Adams-Bashforth (AB2) method, including the crucial 'startup' phase where a one-step method is needed to generate the initial history. By performing this calculation manually, you will gain a concrete understanding of how the method advances the solution step by step.",
            "id": "2152563",
            "problem": "Consider the following Initial Value Problem (IVP):\n$$ y'(t) = -y(t)\\sin(t), \\quad y(0) = 1 $$\nWe wish to find a numerical approximation for $y(0.3)$ using a multistep method.\n\nYou are to use the two-step Adams-Bashforth (AB2) method with a constant step size of $h=0.1$. The formula for the AB2 method is given by:\n$$ y_{n+1} = y_n + \\frac{h}{2}(3f_n - f_{n-1}) $$\nwhere $t_k = kh$, $y_k \\approx y(t_k)$, and $f_k = f(t_k, y_k) = -y_k\\sin(t_k)$.\n\nSince the AB2 method is a two-step method, it requires two initial values to start. Use the given initial condition $y_0 = y(0) = 1$ and compute the first approximation $y_1 \\approx y(0.1)$ using the Forward Euler method:\n$$ y_{n+1} = y_n + h f_n $$\nAfter finding $y_1$, use the AB2 method to compute the subsequent steps to find the approximation for $y(0.3)$.\n\nCalculate the numerical value for the approximation of $y(0.3)$. Round your final answer to five significant figures.",
            "solution": "We are given the IVP $y'(t)=-y(t)\\sin(t)$ with $y(0)=1$, step size $h=0.1$, and $f(t,y)=-y\\sin(t)$. Define $t_{k}=kh$, $y_{k}\\approx y(t_{k})$, and $f_{k}=f(t_{k},y_{k})$.\n\nStart with Forward Euler to obtain $y_{1}$:\n$$\ny_{0}=1,\\quad f_{0}=-y_{0}\\sin(0)=0,\\quad y_{1}=y_{0}+h f_{0}=1+0.1\\cdot 0=1.\n$$\n\nNow apply the AB2 method $y_{n+1}=y_{n}+\\frac{h}{2}\\left(3f_{n}-f_{n-1}\\right)$.\n\nStep to $t_{2}=0.2$:\n$$\nf_{1}=-y_{1}\\sin(0.1)=-\\sin(0.1)\\approx -0.0998334166468282,\n$$\n$$\ny_{2}=y_{1}+\\frac{h}{2}\\left(3f_{1}-f_{0}\\right)=1+0.05\\left(3(-0.0998334166468282)-0\\right)\n=1-0.0149750124970242\\approx 0.9850249875029758.\n$$\n\nCompute $f_{2}$:\n$$\nf_{2}=-y_{2}\\sin(0.2)\\approx -0.9850249875029758\\cdot 0.19866933079506122\\approx -0.19569425508362975.\n$$\n\nStep to $t_{3}=0.3$:\n$$\ny_{3}=y_{2}+\\frac{h}{2}\\left(3f_{2}-f_{1}\\right)\n=0.9850249875029758+0.05\\left(3(-0.19569425508362975)-(-0.0998334166468282)\\right)\n$$\n$$\n=0.9850249875029758+0.05\\left(-0.58708276525088925+0.0998334166468282\\right)\n=0.9850249875029758-0.0243624674302031\\approx 0.9606625200727727.\n$$\n\nThus, the AB2 approximation to $y(0.3)$ is $0.9606625200727727$, which rounded to five significant figures is $0.96066$.",
            "answer": "$$\\boxed{0.96066}$$"
        },
        {
            "introduction": "Higher-order methods are often assumed to be more accurate, but this is not always the case. This next problem  presents a carefully chosen scenario where the second-order Adams-Bashforth method produces a larger error than the simple first-order Forward Euler method. This exercise serves as a valuable reminder that the effectiveness of a numerical method depends heavily on the properties of the problem being solved, particularly the smoothness of the derivative.",
            "id": "2152541",
            "problem": "Consider the initial value problem given by the ordinary differential equation:\n$$\n\\frac{dy}{dt} = |t - 1|\n$$\nwith the initial condition $y(0) = 0$.\n\nWe wish to approximate the value of $y(2.0)$ using two different numerical methods, both with a step size of $h=1.0$.\n\nMethod 1 is the Forward Euler method, defined as:\n$y_{n+1} = y_n + h f(t_n, y_n)$.\n\nMethod 2 is the two-step Adams-Bashforth (AB2) method, defined as:\n$y_{n+1} = y_n + \\frac{h}{2} [3 f(t_n, y_n) - f(t_{n-1}, y_{n-1})]$.\nTo initialize the AB2 method, the value at the first step, an approximation for $y(1.0)$, is to be computed using one step of the Forward Euler method.\n\nLet $E_{FE}$ be the absolute error $|y_{true}(2.0) - y_{approx}^{FE}(2.0)|$ for the Forward Euler method, and let $E_{AB2}$ be the absolute error $|y_{true}(2.0) - y_{approx}^{AB2}(2.0)|$ for the two-step Adams-Bashforth method.\n\nBased on your calculations, which of the following statements correctly describes the relationship between these two errors?\n\nA. $E_{AB2}  E_{FE}$\n\nB. $E_{AB2} = E_{FE}$\n\nC. $E_{AB2}  E_{FE}$\n\nD. Both methods are exact, i.e., $E_{AB2} = E_{FE} = 0$.\n\nE. The Adams-Bashforth method cannot be applied because the function $f(t,y)=|t-1|$ is not differentiable at $t=1$.",
            "solution": "We are given the initial value problem $y'(t) = |t - 1|$ with $y(0)=0$, and we seek $y(2.0)$ using step size $h=1.0$.\n\nFirst, compute the exact solution at $t=2$. Since $y'(t)=|t-1|$ depends only on $t$, the solution is the integral\n$$\ny(t) = \\int_{0}^{t} |s - 1| \\, ds.\n$$\nThus,\n$$\ny(2) = \\int_{0}^{1} (1 - s) \\, ds + \\int_{1}^{2} (s - 1) \\, ds = \\left[ s - \\frac{s^{2}}{2} \\right]_{0}^{1} + \\left[ \\frac{s^{2}}{2} - s \\right]_{1}^{2} = \\frac{1}{2} + \\frac{1}{2} = 1.\n$$\n\nForward Euler (FE) method with $h=1.0$ uses $t_{0}=0$, $t_{1}=1$, $t_{2}=2$, and $y_{0}=0$, with update\n$$\ny_{n+1} = y_{n} + h f(t_{n}, y_{n}), \\quad f(t,y)=|t-1|.\n$$\nStep from $t_{0}$ to $t_{1}$:\n$$\ny_{1} = y_{0} + 1 \\cdot |0 - 1| = 0 + 1 = 1.\n$$\nStep from $t_{1}$ to $t_{2}$:\n$$\ny_{2}^{FE} = y_{1} + 1 \\cdot |1 - 1| = 1 + 0 = 1.\n$$\nHence $y_{approx}^{FE}(2.0)=1$, so\n$$\nE_{FE} = |y_{true}(2.0) - y_{approx}^{FE}(2.0)| = |1 - 1| = 0.\n$$\n\nTwo-step Adams-Bashforth (AB2) method with initialization by one FE step. The FE initialization gives $y_{1}=1$ as above. The AB2 update is\n$$\ny_{n+1} = y_{n} + \\frac{h}{2} \\big(3 f(t_{n}, y_{n}) - f(t_{n-1}, y_{n-1}) \\big).\n$$\nFor $n=1$, with $h=1$, $t_{1}=1$, $t_{0}=0$, we have\n$$\nf(t_{1}, y_{1}) = |1 - 1| = 0, \\quad f(t_{0}, y_{0}) = |0 - 1| = 1,\n$$\nso\n$$\ny_{2}^{AB2} = y_{1} + \\frac{1}{2} \\big(3 \\cdot 0 - 1 \\big) = 1 - \\frac{1}{2} = \\frac{1}{2}.\n$$\nThus\n$$\nE_{AB2} = |y_{true}(2.0) - y_{approx}^{AB2}(2.0)| = \\left| 1 - \\frac{1}{2} \\right| = \\frac{1}{2}.\n$$\n\nTherefore, the errors satisfy $E_{AB2}  E_{FE}$. The Adams-Bashforth method is applicable here because it only requires function evaluations of $f(t,y)$, and $f(t,y)=|t-1|$ is continuous and well-defined at the required points; nondifferentiability at a single point does not preclude applying the method.\n\nThe correct choice is C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Our final practice moves from manual calculation to a more realistic computational setting, exploring the long-term consequences of our initial choices. This coding challenge  asks you to investigate how the accuracy of the starting values—generated by either a low-order or high-order method—impacts the global error of a fourth-order Adams-Bashforth simulation. By quantifying this effect, you will gain a deeper appreciation for the principle that the overall accuracy of a multistep scheme is limited by its weakest link: the starter method.",
            "id": "3202848",
            "problem": "Consider the initial value problem for an ordinary differential equation (ODE) $y'(t) = f(t,y(t))$ with $y(t_0) = y_0$, where $f$ is sufficiently smooth. The explicit four-step Adams–Bashforth method (AB4) advances the numerical state using a linear combination of past evaluations of $f$, and requires four starting values at $t_0, t_1, t_2, t_3$. From first principles, construct the AB4 method by starting from the integral form $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(s,y(s))\\,ds$ and approximating $f(s,y(s))$ on $[t_n,t_{n+1}]$ by the Lagrange interpolation polynomial in the uniform grid points $t_n, t_{n-1}, t_{n-2}, t_{n-3}$, all spaced by a constant step size $h = t_{n+1}-t_n$. Do not use any pre-tabulated Adams–Bashforth coefficients in your construction.\n\nYou will quantify the impact on the global error of an AB4 simulation when the four starting values are generated by the forward Euler method versus a fourth-order Runge–Kutta method (RK4). Define the global error on a grid $\\{t_n\\}_{n=0}^N$ as the maximum absolute difference between the numerical solution and the exact solution over all grid nodes, that is, $\\max_{0 \\le n \\le N} \\lvert y_n - y(t_n) \\rvert$.\n\nFor each test case below, implement two AB4 runs:\n- AB4 with starting values computed by forward Euler for the first three steps.\n- AB4 with starting values computed by fourth-order Runge–Kutta (RK4) for the first three steps.\n\nThen, for each test case, compute the ratio of global errors\n$$R = \\frac{\\max_{0 \\le n \\le N} \\lvert y_n^{\\text{(Euler start)}} - y(t_n) \\rvert}{\\max_{0 \\le n \\le N} \\lvert y_n^{\\text{(RK4 start)}} - y(t_n) \\rvert}.$$\n\nAngle-dependent functions must use angles in radians. There are no physical units in this problem. Use the following test suite, where $N$ is chosen such that $T/h$ is an integer and $t_n = t_0 + n h$:\n- Test case $1$: $f(t,y) = -y$, $y(0) = 1$, $t_0 = 0$, $T = 4$, $h = 0.1$, exact solution $y(t) = e^{-t}$.\n- Test case $2$: $f(t,y) = -y$, $y(0) = 1$, $t_0 = 0$, $T = 4$, $h = 0.5$, exact solution $y(t) = e^{-t}$.\n- Test case $3$: $f(t,y) = -4\\,y$, $y(0) = 1$, $t_0 = 0$, $T = 2.4$, $h = 0.2$, exact solution $y(t) = e^{-4 t}$.\n- Test case $4$: $f(t,y) = -4\\,y$, $y(0) = 1$, $t_0 = 0$, $T = 2.4$, $h = 0.6$, exact solution $y(t) = e^{-4 t}$.\n- Test case $5$: $f(t,y) = -y + \\sin(t)$, $y(0) = 1$, $t_0 = 0$, $T = 10$, $h = 0.1$, exact solution $y(t) = \\tfrac{1}{2}\\big(\\sin(t) - \\cos(t)\\big) + \\tfrac{3}{2} e^{-t}$, angles in radians.\n- Test case $6$: $f(t,y) = -y + \\sin(t)$, $y(0) = 1$, $t_0 = 0$, $T = 10$, $h = 0.25$, exact solution $y(t) = \\tfrac{1}{2}\\big(\\sin(t) - \\cos(t)\\big) + \\tfrac{3}{2} e^{-t}$, angles in radians.\n\nYour program should produce a single line of output containing the results for the six test cases as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5,r_6]$), where each $r_i$ is the float value of $R$ for test case $i$.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is a canonical problem in numerical analysis, designed to explore the properties of the explicit four-step Adams-Bashforth ($AB4$) method, particularly the impact of the starting procedure's accuracy on the global error. The test cases are standard and include scenarios that test both the convergence and stability properties of the method. The problem is therefore deemed valid and a full solution is warranted.\n\nThe solution will be presented in three parts: first, the derivation of the four-step Adams-Bashforth method from first principles; second, an analysis of the role of the starter method and its effect on global error and stability; and third, the implementation strategy for solving the provided test cases.\n\n### 1. Derivation of the Four-Step Adams-Bashforth (AB4) Method\n\nWe begin with the initial value problem for a first-order ordinary differential equation ($ODE$):\n$$ y'(t) = f(t, y(t)), \\quad y(t_0) = y_0 $$\nBy integrating this equation from $t_n$ to $t_{n+1}$, where $t_k = t_0 + k h$ for a constant step size $h$, we obtain the exact relation:\n$$ y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(s, y(s)) \\, ds $$\nThe Adams-Bashforth methods approximate the integral by replacing the (unknown) function $f(s, y(s))$ with an interpolating polynomial. The explicit $k$-step Adams-Bashforth method uses a polynomial that interpolates $k$ previously computed values of $f$ at times $t_n, t_{n-1}, \\ldots, t_{n-k+1}$.\n\nFor the four-step method ($AB4$), we approximate $f(s,y(s))$ with a polynomial $P_3(s)$ of degree at most $3$ that passes through the four points $(t_n, f_n), (t_{n-1}, f_{n-1}), (t_{n-2}, f_{n-2}),$ and $(t_{n-3}, f_{n-3})$, where $f_k = f(t_k, y_k)$. The numerical scheme is then:\n$$ y_{n+1} = y_n + \\int_{t_n}^{t_{n+1}} P_3(s) \\, ds $$\nTo find the coefficients, we construct $P_3(s)$ using Lagrange basis polynomials. For ease of integration, we perform a change of variables $s = t_n + \\alpha h$, which implies $ds = h \\, d\\alpha$. The integration interval $[t_n, t_{n+1}]$ becomes $[\\alpha=0, \\alpha=1]$. The interpolation nodes $t_n, t_{n-1}, t_{n-2}, t_{n-3}$ correspond to $\\alpha=0, -1, -2, -3$, respectively.\n\nThe Lagrange interpolating polynomial is given by:\n$$ P_3(t_n + \\alpha h) = f_n L_0(\\alpha) + f_{n-1} L_1(\\alpha) + f_{n-2} L_2(\\alpha) + f_{n-3} L_3(\\alpha) $$\nwhere the basis polynomials $L_j(\\alpha)$ are defined over the nodes $\\{0, -1, -2, -3\\}$:\n\\begin{align*} L_0(\\alpha) = \\frac{(\\alpha - (-1))(\\alpha - (-2))(\\alpha - (-3))}{(0 - (-1))(0 - (-2))(0 - (-3))} = \\frac{(\\alpha+1)(\\alpha+2)(\\alpha+3)}{6} \\\\ L_1(\\alpha) = \\frac{(\\alpha - 0)(\\alpha - (-2))(\\alpha - (-3))}{(-1 - 0)(-1 - (-2))(-1 - (-3))} = \\frac{\\alpha(\\alpha+2)(\\alpha+3)}{-2} \\\\ L_2(\\alpha) = \\frac{(\\alpha - 0)(\\alpha - (-1))(\\alpha - (-3))}{(-2 - 0)(-2 - (-1))(-2 - (-3))} = \\frac{\\alpha(\\alpha+1)(\\alpha+3)}{2} \\\\ L_3(\\alpha) = \\frac{(\\alpha - 0)(\\alpha - (-1))(\\alpha - (-2))}{(-3 - 0)(-3 - (-1))(-3 - (-2))} = \\frac{\\alpha(\\alpha+1)(\\alpha+2)}{-6} \\end{align*}\nThe integral term becomes:\n$$ \\int_{t_n}^{t_{n+1}} P_3(s) \\, ds = h \\int_0^1 \\left( f_n L_0(\\alpha) + f_{n-1} L_1(\\alpha) + f_{n-2} L_2(\\alpha) + f_{n-3} L_3(\\alpha) \\right) \\, d\\alpha $$\nWe must calculate the definite integrals of the Lagrange basis polynomials from $0$ to $1$:\n\\begin{align*} \\int_0^1 L_0(\\alpha) \\, d\\alpha = \\frac{1}{6} \\int_0^1 (\\alpha^3 + 6\\alpha^2 + 11\\alpha + 6) \\, d\\alpha = \\frac{1}{6} \\left[\\frac{\\alpha^4}{4} + 2\\alpha^3 + \\frac{11\\alpha^2}{2} + 6\\alpha\\right]_0^1 = \\frac{1}{6} \\left(\\frac{1}{4} + 2 + \\frac{11}{2} + 6\\right) = \\frac{55}{24} \\\\ \\int_0^1 L_1(\\alpha) \\, d\\alpha = -\\frac{1}{2} \\int_0^1 (\\alpha^3 + 5\\alpha^2 + 6\\alpha) \\, d\\alpha = -\\frac{1}{2} \\left[\\frac{\\alpha^4}{4} + \\frac{5\\alpha^3}{3} + 3\\alpha^2\\right]_0^1 = -\\frac{1}{2} \\left(\\frac{1}{4} + \\frac{5}{3} + 3\\right) = -\\frac{59}{24} \\\\ \\int_0^1 L_2(\\alpha) \\, d\\alpha = \\frac{1}{2} \\int_0^1 (\\alpha^3 + 4\\alpha^2 + 3\\alpha) \\, d\\alpha = \\frac{1}{2} \\left[\\frac{\\alpha^4}{4} + \\frac{4\\alpha^3}{3} + \\frac{3\\alpha^2}{2}\\right]_0^1 = \\frac{1}{2} \\left(\\frac{1}{4} + \\frac{4}{3} + \\frac{3}{2}\\right) = \\frac{37}{24} \\\\ \\int_0^1 L_3(\\alpha) \\, d\\alpha = -\\frac{1}{6} \\int_0^1 (\\alpha^3 + 3\\alpha^2 + 2\\alpha) \\, d\\alpha = -\\frac{1}{6} \\left[\\frac{\\alpha^4}{4} + \\alpha^3 + \\alpha^2\\right]_0^1 = -\\frac{1}{6} \\left(\\frac{1}{4} + 1 + 1\\right) = -\\frac{9}{24} \\end{align*}\nSubstituting these coefficients into the numerical scheme yields the four-step Adams-Bashforth ($AB4$) method:\n$$ y_{n+1} = y_n + \\frac{h}{24} \\left( 55 f(t_n, y_n) - 59 f(t_{n-1}, y_{n-1}) + 37 f(t_{n-2}, y_{n-2}) - 9 f(t_{n-3}, y_{n-3}) \\right) $$\n\n### 2. Starter Method and Global Error Analysis\n\nThe $AB4$ formula is a multistep method; to compute $y_{n+1}$, it requires knowledge of the solution at four previous steps. To compute $y_4$ (for $n=3$), we need the values $(y_0, y_1, y_2, y_3)$. The initial condition provides $y_0$. The values $y_1, y_2,$ and $y_3$ must be generated by a different, self-starting method (a one-step method). The accuracy of these starting values critically impacts the global accuracy of the entire simulation.\n\nThe local truncation error of the $AB4$ method is $O(h^5)$, leading to a global error of order $O(h^4)$, assuming the starting values are sufficiently accurate (i.e., at least $O(h^4)$).\n\nThe problem specifies two starter methods:\n1.  **Forward Euler Method**: $y_{k+1} = y_k + h f(t_k, y_k)$. This is a first-order method, with local truncation error $O(h^2)$ and global error $O(h)$. When used to start an $AB4$ integration, the $O(h)$ error in the starting values $y_1, y_2, y_3$ will contaminate the entire solution. The low accuracy of the starter dominates, and the resulting global error for the combined scheme is only $O(h)$.\n\n2.  **Fourth-Order Runge-Kutta (RK4) Method**: $y_{k+1} = y_k + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)$, where the $k_i$ are intermediate slope evaluations. This method has a global error of order $O(h^4)$. Since the accuracy of the starter matches the accuracy of the main $AB4$ method, the overall scheme maintains the desired $O(h^4)$ global error.\n\nThe ratio of global errors, $R = \\frac{\\text{Global Error (Euler start)}}{\\text{Global Error (RK4 start)}}$, is therefore expected to scale as:\n$$ R \\approx \\frac{C_1 h}{C_4 h^4} = K h^{-3} $$\nThis implies that as the step size $h$ is reduced, the ratio should increase cubically, highlighting the significant performance degradation caused by an inconsistent starter.\n\nA crucial consideration is numerical stability. For the test equation $y' = \\lambda y$, a method is absolutely stable if the numerical solution does not grow for a given complex value of $h\\lambda$. For $AB4$, the interval of absolute stability on the negative real axis is approximately $[-0.723, 0]$.\n-   Test Cases $1, 2, 5, 6$: These have $h\\lambda$ products (or effective $h\\lambda_{eff}$) that lie within this stability region for the given parameters. For these cases, we expect the theoretical error scaling to hold.\n-   Test Cases $3, 4$: Here, $f(t,y) = -4y$, so $\\lambda = -4$.\n    -   Case $3$: $h\\lambda = 0.2 \\times (-4) = -0.8$. This is outside the stability region.\n    -   Case $4$: $h\\lambda = 0.6 \\times (-4) = -2.4$. This is far outside the stability region.\nFor these unstable cases, the error will be dominated by exponential amplification of any initial perturbation, not by the accumulation of local truncation error. While the Euler-started simulation will still have a larger error than the RK4-started one (as it starts with a larger initial perturbation), the $R \\propto h^{-3}$ scaling is not expected to hold.\n\n### 3. Implementation Strategy\n\nThe solution will be computed by implementing a general function that performs an $AB4$ simulation. This function will take the $ODE$ functionhandle, initial conditions, time parameters, and a flag indicating the starter method ('euler' or 'rk4').\n\nFor each of the six test cases:\n1.  Two separate simulations are run: one initialized with three steps of the forward Euler method, and the other with three steps of the $RK4$ method.\n2.  The initial values $(y_1, y_2, y_3)$ are computed and stored.\n3.  After the startup phase, the $AB4$ formula is applied iteratively from $n=3$ until the final time $T$. A list or queue of size $4$ is used to maintain the history of $f_k$ values required by the formula.\n4.  The global error for each simulation is calculated by finding the maximum absolute difference between the numerical solution array and the exact solution evaluated at each grid point: $\\max_{0 \\le n \\le N} |y_n - y(t_n)|$.\n5.  The ratio $R$ of the Euler-start error to the $RK4$-start error is computed and stored.\n6.  The final output is a list of these ratios for all six test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_ab4_simulation(f, y0, t0, T, h, starter_method):\n    \"\"\"\n    Solves an ODE y'(t) = f(t,y) using the four-step Adams-Bashforth method.\n\n    The first three steps are computed using a specified one-step method\n    (forward Euler or RK4) to generate the necessary starting values.\n\n    Args:\n        f (callable): The function f(t, y) defining the ODE.\n        y0 (float): The initial value y(t0).\n        t0 (float): The initial time.\n        T (float): The final time.\n        h (float): The step size.\n        starter_method (str): The method to use for starting values,\n                              either 'euler' or 'rk4'.\n\n    Returns:\n        tuple: A tuple containing:\n            - t_grid (np.ndarray): The array of time points.\n            - y (np.ndarray): The numerical solution array.\n    \"\"\"\n    N = int(round(T / h))\n    t_grid = np.linspace(t0, T, N + 1)\n    y = np.zeros(N + 1)\n    y[0] = y0\n\n    # Startup phase: Calculate y[1], y[2], y[3]\n    if starter_method == 'euler':\n        for i in range(3):\n            y[i + 1] = y[i] + h * f(t_grid[i], y[i])\n    elif starter_method == 'rk4':\n        for i in range(3):\n            y_i = y[i]\n            t_i = t_grid[i]\n            k1 = f(t_i, y_i)\n            k2 = f(t_i + h / 2.0, y_i + h / 2.0 * k1)\n            k3 = f(t_i + h / 2.0, y_i + h / 2.0 * k2)\n            k4 = f(t_i + h, y_i + h * k3)\n            y[i + 1] = y_i + h / 6.0 * (k1 + 2 * k2 + 2 * k3 + k4)\n    else:\n        raise ValueError(\"Invalid starter_method. Must be 'euler' or 'rk4'.\")\n\n    # AB4 requires the four most recent values of f(t, y).\n    # Initialize a list to store f_k for k = 0, 1, 2, 3.\n    f_hist = [f(t_grid[i], y[i]) for i in range(4)]\n\n    # AB4 coefficients\n    b0, b1, b2, b3 = 55.0/24.0, -59.0/24.0, 37.0/24.0, -9.0/24.0\n\n    # Main AB4 loop\n    for n in range(3, N):\n        # f_hist contains [f(n-3), f(n-2), f(n-1), f(n)]\n        y[n + 1] = y[n] + h * (b0 * f_hist[3] + b1 * f_hist[2] + b2 * f_hist[1] + b3 * f_hist[0])\n        \n        # Update history for the next step\n        f_hist.pop(0)\n        f_hist.append(f(t_grid[n + 1], y[n + 1]))\n\n    return t_grid, y\n\ndef solve():\n    \"\"\"\n    Main function to define and run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {'f': lambda t, y: -y,\n         'y_exact': lambda t: np.exp(-t),\n         'y0': 1.0, 't0': 0.0, 'T': 4.0, 'h': 0.1},\n        # Test case 2\n        {'f': lambda t, y: -y,\n         'y_exact': lambda t: np.exp(-t),\n         'y0': 1.0, 't0': 0.0, 'T': 4.0, 'h': 0.5},\n        # Test case 3\n        {'f': lambda t, y: -4 * y,\n         'y_exact': lambda t: np.exp(-4 * t),\n         'y0': 1.0, 't0': 0.0, 'T': 2.4, 'h': 0.2},\n        # Test case 4\n        {'f': lambda t, y: -4 * y,\n         'y_exact': lambda t: np.exp(-4 * t),\n         'y0': 1.0, 't0': 0.0, 'T': 2.4, 'h': 0.6},\n        # Test case 5\n        {'f': lambda t, y: -y + np.sin(t),\n         'y_exact': lambda t: 0.5 * (np.sin(t) - np.cos(t)) + 1.5 * np.exp(-t),\n         'y0': 1.0, 't0': 0.0, 'T': 10.0, 'h': 0.1},\n        # Test case 6\n        {'f': lambda t, y: -y + np.sin(t),\n         'y_exact': lambda t: 0.5 * (np.sin(t) - np.cos(t)) + 1.5 * np.exp(-t),\n         'y0': 1.0, 't0': 0.0, 'T': 10.0, 'h': 0.25},\n    ]\n\n    results = []\n    for case in test_cases:\n        f = case['f']\n        y_exact_func = case['y_exact']\n        y0 = case['y0']\n        t0 = case['t0']\n        T = case['T']\n        h = case['h']\n\n        # Run simulation with Euler starter\n        t_grid, y_euler_start = run_ab4_simulation(f, y0, t0, T, h, 'euler')\n        \n        # Run simulation with RK4 starter\n        _, y_rk4_start = run_ab4_simulation(f, y0, t0, T, h, 'rk4')\n        \n        # Calculate exact solution on the grid\n        y_exact_vals = y_exact_func(t_grid)\n        \n        # Compute global errors\n        error_euler = np.max(np.abs(y_euler_start - y_exact_vals))\n        error_rk4 = np.max(np.abs(y_rk4_start - y_exact_vals))\n        \n        # Compute the ratio, handle potential division by zero\n        if error_rk4 == 0.0:\n            ratio = np.inf if error_euler > 0.0 else 1.0\n        else:\n            ratio = error_euler / error_rk4\n        results.append(ratio)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}