{
    "hands_on_practices": [
        {
            "introduction": "Understanding the relationship between step size and global error is fundamental to numerical analysis. This exercise provides a direct conceptual check on this principle without requiring any complex calculations. By exploring how the error of a second-order method changes when the step size is doubled, you will solidify your grasp of what the 'order' of a method, denoted by $p$, truly signifies in practice. ",
            "id": "2185101",
            "problem": "A numerical analyst is studying the accuracy of a stable numerical method for solving an ordinary differential equation over a fixed interval. The method is known to be a second-order method. In a first computational experiment, using a constant step size of $h$, the analyst computes the solution and finds that the global truncation error at the end of the interval is approximately $\\epsilon$.\n\nIn a second experiment, the analyst decides to change the step size to $2h$ while keeping all other parameters and the differential equation the same. Assuming the method remains stable and the relationship between global error and step size holds in this range, what is the estimated global truncation error for this second experiment?\n\nA. $\\frac{\\epsilon}{4}$\n\nB. $\\frac{\\epsilon}{2}$\n\nC. $\\epsilon$\n\nD. $2\\epsilon$\n\nE. $4\\epsilon$",
            "solution": "A stable method of order $p$ has a global truncation error at a fixed final time that scales as $E(h)=C h^{p}$ for some constant $C$ independent of $h$, provided the method remains stable and the asymptotic error relation holds.\n\nGiven that the method is second-order, $p=2$. For step size $h$, the observed global error is $\\epsilon$, so\n$$\n\\epsilon = C h^{2}.\n$$\nIf the step size is changed to $2h$, the new global error is\n$$\nE(2h) = C (2h)^{2} = 4 C h^{2} = 4 \\epsilon.\n$$\nTherefore, doubling the step size multiplies the global error by $4$, which corresponds to option E.",
            "answer": "$$\\boxed{E}$$"
        },
        {
            "introduction": "Theoretical error estimates are powerful, but how can we verify them in our own code? This hands-on coding challenge guides you through the process of conducting an empirical convergence study for the celebrated fourth-order Runge-Kutta method. By fitting your numerical results to the expected error model $E(h) \\approx C h^{p}$, you will not only confirm the method's order but also develop a crucial skill for validating any numerical simulation. ",
            "id": "3236754",
            "problem": "Consider the initial value problem defined by the ordinary differential equation $y^{\\prime}(t) = f(t, y)$ with initial condition $y(t_{0}) = y_{0}$. The explicit classical Runge-Kutta method of order four (RK4) is a four-stage one-step method that advances the numerical solution $y_{n}$ from time $t_{n}$ to $t_{n+1} = t_{n} + h$ using stage evaluations of $f$ at $t_{n}$, $t_{n} + \\frac{h}{2}$, and $t_{n} + h$. The global truncation error at a final time $T$ associated with a uniform step size $h$ is defined as $e(h) = \\left|y_{N} - y_{\\mathrm{exact}}(T)\\right|$, where $N = \\frac{T - t_{0}}{h}$ and $y_{N}$ is the numerical approximation produced by RK4 after $N$ steps.\n\nYour task is to empirically reconstruct the global truncation error law for RK4 by fitting the observed errors $e(h)$ across multiple step sizes $h$ and inferring both the empirical order $p$ and the leading constant $C$ through a linear fit on a log-log scale. Specifically, for each test case below, you must:\n- Implement a solver that, for each provided step size $h$, computes the RK4 numerical solution at the final time $T$ and evaluates the corresponding error $e(h)$.\n- Use natural logarithms to fit the model $\\log\\left(e(h)\\right) = \\log(C) + p \\,\\log(h)$ via least squares over the provided set of step sizes, thereby inferring $p$ and $C$.\n- Aggregate the inferred $\\left[p, C\\right]$ across the test cases in the exact output format specified at the end of this statement.\n\nFoundational base to use and justify: the initial value problem definition $y^{\\prime}(t) = f(t, y)$ with $y(t_{0}) = y_{0}$, the definition of global truncation error $e(h)$ as the difference at $T$ between the numerical solution and the exact solution, and the empirical linearization of a power-law relationship on a log-log scale.\n\nTest suite to evaluate and fit the empirical error law:\n- Test case $1$: $f(t, y) = y$, $t_{0} = 0$, $y_{0} = 1$, $T = 1$, exact solution $y_{\\mathrm{exact}}(t) = e^{t}$, step sizes $h \\in \\left\\{0.5, 0.25, 0.125, 0.0625\\right\\}$.\n- Test case $2$: $f(t, y) = -3\\,y$, $t_{0} = 0$, $y_{0} = 2$, $T = 2$, exact solution $y_{\\mathrm{exact}}(t) = 2\\,e^{-3t}$, step sizes $h \\in \\left\\{0.5, 0.25, 0.125, 0.0625\\right\\}$.\n- Test case $3$: $f(t, y) = y + t$, $t_{0} = 0$, $y_{0} = 0$, $T = 0.8$, exact solution $y_{\\mathrm{exact}}(t) = e^{t} - t - 1$, step sizes $h \\in \\left\\{0.4, 0.2, 0.1, 0.05\\right\\}$.\n- Test case $4$: $f(t, y) = -5\\,y$, $t_{0} = 0$, $y_{0} = 1$, $T = 1$, exact solution $y_{\\mathrm{exact}}(t) = e^{-5t}$, step sizes $h \\in \\left\\{0.1, 0.05, 0.025, 0.0125\\right\\}$.\n\nAll time quantities are dimensionless. No physical units are involved. Angles do not appear. Your program must:\n- Use natural logarithms for the least-squares fit of $\\left(\\log(h), \\log(e(h))\\right)$ pairs.\n- Produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list $\\left[p, C\\right]$. For example, the required format is $\\left[\\left[p_{1}, C_{1}\\right], \\left[p_{2}, C_{2}\\right], \\left[p_{3}, C_{3}\\right], \\left[p_{4}, C_{4}\\right]\\right]$.\n- Report numerical values as floating-point numbers. No rounding or specific significant figures are required.\n\nYour final answer must be a complete, runnable program that performs these computations and prints only the specified output line.",
            "solution": "The problem requires the empirical determination of the global truncation error law for the fourth-order classical Runge-Kutta (RK4) method. For a numerical method of order $p$, the global error $e(h)$ at a fixed final time $T$, committed when using a step size $h$, is expected to follow the power law $e(h) \\approx C h^p$ for sufficiently small $h$. Here, $C$ is a constant that depends on the differential equation and the final time $T$, but not on $h$. The objective is to determine the empirical values of the order $p$ and the constant $C$ for several initial value problems by performing a linear regression on transformed error data.\n\nThe initial value problem is given by the ordinary differential equation (ODE) $y^{\\prime}(t) = f(t, y)$ with an initial condition $y(t_0) = y_0$.\n\nThe core of the numerical solution is the RK4 method, a one-step, four-stage explicit method. To advance the solution from $(t_n, y_n)$ to $(t_{n+1}, y_{n+1})$, where $t_{n+1} = t_n + h$, the method computes four intermediate slope estimates (stages) $k_1, k_2, k_3, k_4$:\n$$k_1 = f(t_n, y_n)$$\n$$k_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right)$$\n$$k_3 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right)$$\n$$k_4 = f(t_n + h, y_n + hk_3)$$\nThe solution is then updated using a weighted average of these slopes:\n$$y_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$\n\nTo determine the parameters $p$ and $C$ in the error model $e(h) \\approx C h^p$, we linearize the relationship by taking the natural logarithm of both sides:\n$$\\log(e(h)) \\approx \\log(C h^p) = \\log(C) + \\log(h^p) = \\log(C) + p \\log(h)$$\nThis equation has the form of a line, $Y = mX + b$, where $Y = \\log(e(h))$, the slope is $m = p$, the independent variable is $X = \\log(h)$, and the y-intercept is $b = \\log(C)$.\n\nThe overall algorithm for each test case is as follows:\n1.  For each given step size $h_i$ in the provided set, solve the ODE from $t_0$ to $T$ using the RK4 method. This involves taking $N_i = (T - t_0) / h_i$ steps to obtain the numerical solution $y_{N_i}$.\n2.  Calculate the exact solution at the final time, $y_{\\mathrm{exact}}(T)$.\n3.  Compute the absolute global error for each step size: $e(h_i) = |y_{N_i} - y_{\\mathrm{exact}}(T)|$.\n4.  This procedure yields a set of data points $(h_i, e(h_i))$.\n5.  Transform these data points to a logarithmic scale, creating a new set of points $(\\log(h_i), \\log(e(h_i)))$.\n6.  Perform a linear least-squares regression on these transformed points to find the best-fit line. The slope of this line is the empirical order $p$, and the intercept is $\\log(C)$.\n7.  The constant $C$ is then recovered by exponentiating the intercept: $C = \\exp(\\log(C))$.\n8.  The resulting pair $[p, C]$ is stored for the test case.\n\nThis process is repeated for all four test cases provided in the problem statement. The final output is an aggregation of the $[p, C]$ pairs for each case. The use of multiple step sizes for each regression improves the robustness of the inferred parameters, assuming the step sizes are in the asymptotic regime where the leading-order error term dominates.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically reconstructs the global truncation error law for the RK4 method\n    by fitting observed errors to a power law e(h) = C*h^p.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"f\": lambda t, y: y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"y_exact\": lambda t: np.exp(t),\n            \"h_values\": np.array([0.5, 0.25, 0.125, 0.0625])\n        },\n        {\n            \"f\": lambda t, y: -3.0 * y,\n            \"t0\": 0.0,\n            \"y0\": 2.0,\n            \"T\": 2.0,\n            \"y_exact\": lambda t: 2.0 * np.exp(-3.0 * t),\n            \"h_values\": np.array([0.5, 0.25, 0.125, 0.0625])\n        },\n        {\n            \"f\": lambda t, y: y + t,\n            \"t0\": 0.0,\n            \"y0\": 0.0,\n            \"T\": 0.8,\n            \"y_exact\": lambda t: np.exp(t) - t - 1.0,\n            \"h_values\": np.array([0.4, 0.2, 0.1, 0.05])\n        },\n        {\n            \"f\": lambda t, y: -5.0 * y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"y_exact\": lambda t: np.exp(-5.0 * t),\n            \"h_values\": np.array([0.1, 0.05, 0.025, 0.0125])\n        }\n    ]\n\n    def run_rk4(f, y0, t0, T, h):\n        \"\"\"\n        Solves an ODE y'(t) = f(t,y) from t0 to T with step size h using RK4.\n        \n        Args:\n            f (callable): The function f(t, y).\n            y0 (float): The initial value y(t0).\n            t0 (float): The initial time.\n            T (float): The final time.\n            h (float): The step size.\n\n        Returns:\n            float: The numerical solution y(T).\n        \"\"\"\n        t = t0\n        y = y0\n        # Use rounding to ensure integer number of steps for the loop\n        num_steps = int(round((T - t0) / h))\n        \n        for _ in range(num_steps):\n            k1 = f(t, y)\n            k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n            k3 = f(t + 0.5 * h, y + 0.5 * h * k2)\n            k4 = f(t + h, y + h * k3)\n            y += (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n            t += h\n            \n        return y\n\n    results = []\n    for case in test_cases:\n        errors = []\n        h_vals = case[\"h_values\"]\n        \n        for h in h_vals:\n            y_numerical = run_rk4(case[\"f\"], case[\"y0\"], case[\"t0\"], case[\"T\"], h)\n            y_true = case[\"y_exact\"](case[\"T\"])\n            error = np.abs(y_numerical - y_true)\n            errors.append(error)\n        \n        # Transform data to log-log scale\n        log_h = np.log(h_vals)\n        log_e = np.log(np.array(errors))\n        \n        # Perform linear least-squares fit: log(e) = p*log(h) + log(C)\n        # np.polyfit returns [p, log(C)]\n        p, log_C = np.polyfit(log_h, log_e, 1)\n        \n        # Recover C from the intercept\n        C = np.exp(log_C)\n        \n        results.append([p, C])\n\n    # Convert results to a string with the specified format\n    # The default str() representation of a list is [item1, item2], which matches the requirement\n    # Example: str([4.0, 0.001]) - '[4.0, 0.001]'\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A common assumption is that decreasing the step size $h$ will always improve accuracy, but is this really true in practice? This problem delves into the subtle interplay between truncation error, which decreases with $h$, and rounding error, which can accumulate and increase as $h$ becomes smaller. You will construct a counterexample to demonstrate how, under certain conditions, making the step size too small can actually harm the solution's accuracy, a vital lesson in practical scientific computing. ",
            "id": "3236622",
            "problem": "Consider a linear initial value problem $y'(t) = f(t,y(t))$ with $y(0) = y_0$ on a fixed final time interval $[0,T]$. Let the right-hand side be $f(t,y) = -\\lambda y$ with $\\lambda  0$, and suppose $f$ is continuously differentiable and globally Lipschitz in $y$. The global truncation error at final time $T$ for a one-step method with step size $h$ is defined as $E(h) = \\lvert y_h(T) - y(T) \\rvert$, where $y_h(T)$ is the numerical approximation produced by the method and $y(T)$ is the exact solution. Starting from fundamental definitions of zero-stability and consistency of one-step methods, and the Lipschitz condition for the initial value problem, derive sufficient conditions under which the mapping $h \\mapsto E(h)$ is monotone decreasing for all sufficiently small $h$ on $[0,T]$. Your derivation should explicitly connect zero-stability, consistency, and regularity of the exact solution (bounded derivatives as needed) to the behavior of $E(h)$ as $h \\to 0^+$; do not assume any specific method beyond the general properties of a zero-stable, consistent one-step method.\n\nNext, construct a counterexample where reducing $h$ slightly increases the overall error due to the interplay of finite precision rounding and stiffness. Use the explicit Euler method applied to the stiff linear test equation $y'(t) = -\\lambda y(t)$ with $\\lambda \\gg 1$, $y(0) = 1$, and emulate finite precision by quantizing the state after each step to the nearest multiple of a fixed quantization level $q  0$. Explain why the accumulated rounding component scales inversely with $h$ in the stiff regime and may dominate the truncation component, thereby breaking monotonicity of $E(h)$ for smaller $h$.\n\nThen, implement a complete, runnable program that:\n- Uses explicit Euler for $y'(t) = -\\lambda y(t)$ with $y(0) = 1$ and computes $E(h)$ at $T$ for specified lists of $h$.\n- Optionally applies quantization after each step: given $q  0$, replace the new iterate $y_{n+1}$ by $\\operatorname{round}(y_{n+1}/q)\\,q$; if $q = 0$, do not quantize.\n- Returns results for the following test suite:\n    1. Non-stiff case without quantization: $\\lambda = 1$, $T = 1$, $q = 0$, and $h \\in \\{0.2, 0.1, 0.05, 0.025\\}$. Compute the list of errors $[E(h)]$ and output a boolean indicating whether this list is strictly monotone decreasing in $h$.\n    2. Stiff case with quantization (counterexample): $\\lambda = 1000$, $T = 1$, $q = 10^{-8}$, and $h \\in \\{2\\times 10^{-4}, 10^{-4}, 5\\times 10^{-5}, 2.5\\times 10^{-5}\\}$. Compute and output the list of errors $[E(h)]$ for these $h$ values to demonstrate non-monotonicity caused by the rounding-stiffness interplay.\n    3. Stiff case without quantization: $\\lambda = 1000$, $T = 1$, $q = 0$, and $h \\in \\{2\\times 10^{-4}, 10^{-4}, 5\\times 10^{-5}, 2.5\\times 10^{-5}\\}$. Compute the list of errors $[E(h)]$ and output a boolean indicating whether this list is strictly monotone decreasing in $h$.\n\nAll mathematical quantities including numbers must be presented in LaTeX in this problem statement, as required. There are no physical units or angles in this problem. The final program output must be a single line containing the results as a comma-separated list enclosed in square brackets with no spaces, in the exact format\n$[b_1,[e_1,e_2,e_3,e_4],b_3]$\nwhere $b_1$ and $b_3$ are booleans for tests $1$ and $3$, and $e_1,\\dots,e_4$ are floating-point numbers for test $2$ (the error values for the specified $h$ in the stiff quantized case). The lists for tests $1$ and $3$ are not printed; only the booleans for monotonicity are printed in positions $1$ and $3$ and the list of errors for test $2$ is printed in position $2$.",
            "solution": "The problem asks for a theoretical derivation of conditions for monotonic error behavior, an explanation of a counterexample involving stiffness and rounding, and a numerical implementation to demonstrate these phenomena.\n\n### Part 1: Derivation of Sufficient Conditions for Monotonic Error\n\nWe consider a general one-step method for solving the initial value problem $y'(t) = f(t,y(t))$ with $y(t_0)=y_0$ on the interval $[t_0, T]$. The method is given by the recurrence\n$$ y_{n+1} = y_n + h \\Phi(t_n, y_n, h) $$\nwhere $y_n$ approximates the true solution $y(t_n)$ at time $t_n = t_0 + nh$, and $h$ is the step size. The problem defines the global error at the final time $T$ as $E(h) = |y(T) - y_h(T)|$. We seek sufficient conditions under which a decrease in $h$ leads to a decrease in $E(h)$ for sufficiently small $h$. This means we are looking for conditions under which $E(h)$ is a strictly increasing function of $h$ for small $h  0$.\n\n1.  **Zero-Stability and Consistency**:\n    A one-step method is **zero-stable** if small perturbations in the data do not cause unbounded growth in the numerical solution. For one-step methods, this property is guaranteed if the increment function $\\Phi(t,y,h)$ is Lipschitz continuous with respect to its second argument, $y$, uniformly in $t$ and for small $h$. That is, there exists a constant $L_\\Phi$ such that for any $y_1, y_2$:\n    $$ |\\Phi(t, y_1, h) - \\Phi(t, y_2, h)| \\le L_\\Phi |y_1 - y_2| $$\n    A method is **consistent** with the differential equation if the increment function $\\Phi$ converges to the right-hand side function $f$ as the step size $h$ approaches zero. Formally,\n    $$ \\Phi(t,y,0) = f(t,y) $$\n    A method is **consistent of order $p$** (where $p \\ge 1$ is an integer) if the local truncation error (LTE), $\\tau(t,h)$, defined by the defect when the exact solution is inserted into the scheme, satisfies $\\tau(t,h) = \\mathcal{O}(h^p)$. The LTE is defined as:\n    $$ \\tau(t,h) = \\frac{y(t+h) - y(t)}{h} - \\Phi(t, y(t), h) $$\n    Consistency of order $p$ implies $\\tau(t,h) = C(t)h^p + \\mathcal{O}(h^{p+1})$ for some function $C(t)$ related to the derivatives of $y(t)$.\n    A fundamental result by Dahlquist states that for a one-step method, convergence is equivalent to consistency. Specifically, if a method is consistent of order $p \\ge 1$ and $\\Phi$ is Lipschitz, the global error is bounded: $E(h) \\le K h^p$ for some constant $K$. This guarantees that the error approaches zero as $h \\to 0$, but it does not guarantee monotonic behavior.\n\n2.  **Asymptotic Error Expansion and Monotonicity**:\n    To establish monotonicity, a more precise characterization of the error is needed. Under stronger regularity conditions, the global error can be shown to possess an asymptotic expansion in powers of $h$.\n    \n    **Sufficient Conditions**:\n    - The one-step method is consistent of order $p \\ge 1$. For one-step methods, this implies zero-stability and thus convergence.\n    - The functions $f(t,y)$ and $\\Phi(t,y,h)$ are sufficiently smooth with respect to their arguments. For an expansion up to order $p$, this typically requires $f$ to be of class $C^p$ or higher.\n    - The exact solution $y(t)$ is also sufficiently smooth, which is guaranteed if $f$ is smooth.\n    \n    Under these conditions, it can be proven (e.g., via Gragg's extrapolation theory) that the global error at time $T$ admits an asymptotic expansion:\n    $$ y(T) - y_h(T) = \\psi(T) h^p + \\mathcal{O}(h^{p+1}) $$\n    where $\\psi(t)$ is the *principal error function*, which is the solution to a differential equation related to $f$ and the leading term of the LTE.\n    \n    The magnitude of the global error is therefore:\n    $$ E(h) = |y(T) - y_h(T)| = |\\psi(T) h^p + \\mathcal{O}(h^{p+1})| $$\n    If the principal error function is non-zero at the final time, $\\psi(T) \\neq 0$, then for sufficiently small $h0$, the leading term dominates the expansion. We can write:\n    $$ E(h) = |\\psi(T)|h^p + \\mathcal{O}(h^{p+1}) $$\n    Let's analyze the behavior of this function for small $h$. Consider two step sizes $h_1$ and $h_2$ such that $0  h_2  h_1$, with $h_1$ being in the asymptotic regime.\n    $$ E(h_1) \\approx |\\psi(T)|h_1^p $$\n    $$ E(h_2) \\approx |\\psi(T)|h_2^p $$\n    Since $h_1  h_2  0$ and $p \\ge 1$, we have $h_1^p  h_2^p$. Thus, $|\\psi(T)|h_1^p  |\\psi(T)|h_2^p$, which implies $E(h_1)  E(h_2)$.\n    \n    This demonstrates that for sufficiently small $h$, the global error $E(h)$ is a strictly increasing function of $h$. Consequently, reducing the step size $h$ results in a smaller global error. The phrase \"monotone decreasing for all sufficiently small $h$\" is interpreted as the error decreasing as $h$ decreases.\n\n### Part 2: Counterexample due to Stiffness and Rounding\n\nThe monotonic decrease of error when reducing $h$ holds only in the context of exact arithmetic. In finite precision computation, the total error is a combination of truncation error and rounding error.\n$$ E_{\\text{total}}(h) \\approx E_{\\text{trunc}}(h) + E_{\\text{round}}(h) $$\n- The **truncation error**, $E_{\\text{trunc}}(h)$, arises from approximating the continuous differential equation with a discrete formula. As derived above, for a method of order $p$, $E_{\\text{trunc}}(h) \\propto h^p$. This component decreases as $h$ decreases.\n- The **rounding error**, $E_{\\text{round}}(h)$, arises from representing real numbers with finite precision. At each step of the numerical method, a small rounding error is introduced. The accumulation of these errors over all steps constitutes the global rounding error.\n\nLet us analyze this for the explicit Euler method applied to the stiff test problem $y'(t) = -\\lambda y(t)$ with $\\lambda \\gg 1$, $y(0)=1$, and quantization step $q$. The Euler method is $y_{n+1} = y_n - h\\lambda y_n = (1 - h\\lambda)y_n$. With quantization, the computed state $\\hat{y}_n$ evolves as:\n$$ \\hat{y}_{n+1} = (1 - h\\lambda)\\hat{y}_n + \\delta_n $$\nwhere $\\delta_n$ is the rounding error introduced at step $n$ due to quantization. Its magnitude is bounded by $|\\delta_n| \\le q/2$.\n\nThe solution for $\\hat{y}_N$ at the final time $T=Nh$ is:\n$$ \\hat{y}_N = (1-h\\lambda)^N \\hat{y}_0 + \\sum_{k=0}^{N-1} (1-h\\lambda)^{N-1-k} \\delta_k $$\nThe exact solution is $y(T) = e^{-\\lambda T}$. For a stiff problem ($\\lambda \\gg 1$), $y(T)$ is negligibly small. The error is thus $E(h) \\approx |\\hat{y}_N|$. The first term, $(1-h\\lambda)^N \\hat{y}_0$, is the numerical solution without rounding, corresponding to the truncation error. The second term represents the accumulated rounding error.\n\nLet's focus on the rounding error component: $R_N = \\sum_{k=0}^{N-1} (1-h\\lambda)^{N-1-k} \\delta_k$.\nFor stability, the explicit Euler method requires $|1-h\\lambda|  1$, which means $0  h\\lambda  2$. In the stiff regime where $\\lambda \\gg 1$, this forces $h$ to be very small. Let us assume we are in the stable regime but where $h\\lambda$ is not close to zero (e.g., $h\\lambda \\in (0,1)$). The magnitude of the accumulated rounding error can be bounded:\n$$ |R_N| \\le \\sum_{j=0}^{N-1} |1-h\\lambda|^j |\\delta_{N-1-j}| \\le \\frac{q}{2}\\sum_{j=0}^{N-1} |1-h\\lambda|^j $$\nThis is a geometric series. As $N=T/h \\to \\infty$, the sum approaches its limit:\n$$ \\sum_{j=0}^{\\infty} |1-h\\lambda|^j = \\frac{1}{1-|1-h\\lambda|} $$\nFor $h\\lambda \\in (0,1)$, this is $\\frac{1}{1-(1-h\\lambda)} = \\frac{1}{h\\lambda}$.\nThus, the magnitude of the accumulated rounding error can be estimated as:\n$$ E_{\\text{round}}(h) \\approx C \\frac{q}{h\\lambda} $$\nThis component of the error is inversely proportional to the step size $h$.\n\nThe total error behaves as:\n$$ E_{\\text{total}}(h) \\approx C_1 h^p + \\frac{C_2 q}{h} $$\nFor the first-order Euler method ($p=1$), this is $E_{\\text{total}}(h) \\approx C_1 h + \\frac{C_2 q}{h}$. This function has a minimum at $h_{\\text{opt}} = \\sqrt{C_2 q / C_1}$.\n- For $h  h_{\\text{opt}}$, the truncation error ($C_1 h$) dominates. Decreasing $h$ decreases the total error.\n- For $h  h_{\\text{opt}}$, the rounding error ($C_2 q / h$) dominates. Decreasing $h$ now *increases* the total error.\n\nThis explains the non-monotonic behavior. When solving a stiff problem, even if the method is stable, the large number of steps ($N=T/h$) and the error propagation dynamics can cause rounding errors to accumulate significantly, leading to a situation where further reduction of $h$ degrades accuracy.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef euler_solver(lambda_val, T, h, q, y0):\n    \"\"\"\n    Solves y'(t) = -lambda*y(t) using explicit Euler with optional quantization.\n\n    Args:\n        lambda_val (float): The stiffness parameter lambda.\n        T (float): The final time.\n        h (float): The step size.\n        q (float): The quantization level. If 0, no quantization is applied.\n        y0 (float): The initial condition y(0).\n\n    Returns:\n        float: The absolute error |y_numerical(T) - y_exact(T)|.\n    \"\"\"\n    # Exact solution at time T\n    y_exact = y0 * np.exp(-lambda_val * T)\n\n    # Number of steps\n    # Assume T is a multiple of h as is typical for such test problems.\n    N = int(round(T / h))\n    \n    # Check if the number of steps is reasonable given T and h.\n    if not np.isclose(N * h, T):\n        # This case is not expected based on the problem's values but is good practice.\n        # A more robust implementation might handle a final partial step.\n        pass\n\n    # Initial condition for numerical solution\n    y_num = float(y0)\n\n    # Time-stepping loop\n    for _ in range(N):\n        # Explicit Euler step\n        y_num = y_num * (1.0 - h * lambda_val)\n        \n        # Apply quantization if q  0\n        if q  0:\n            y_num = np.round(y_num / q) * q\n            \n    # Compute the final absolute error\n    error = np.abs(y_num - y_exact)\n    \n    return error\n\ndef solve():\n    \"\"\"\n    Runs the test suite as specified in the problem statement and prints the results.\n    \"\"\"\n    # Test case 1: Non-stiff, no quantization\n    params1 = {\n        'lambda_val': 1.0, \n        'T': 1.0, \n        'q': 0.0, \n        'y0': 1.0, \n        'h_list': [0.2, 0.1, 0.05, 0.025]\n    }\n    errors1 = [euler_solver(params1['lambda_val'], params1['T'], h, params1['q'], params1['y0']) for h in params1['h_list']]\n    is_monotone1 = all(errors1[i]  errors1[i+1] for i in range(len(errors1) - 1))\n\n    # Test case 2: Stiff, with quantization\n    params2 = {\n        'lambda_val': 1000.0, \n        'T': 1.0, \n        'q': 1e-8, \n        'y0': 1.0, \n        'h_list': [2e-4, 1e-4, 5e-5, 2.5e-5]\n    }\n    errors2 = [euler_solver(params2['lambda_val'], params2['T'], h, params2['q'], params2['y0']) for h in params2['h_list']]\n\n    # Test case 3: Stiff, no quantization\n    params3 = {\n        'lambda_val': 1000.0, \n        'T': 1.0, \n        'q': 0.0, \n        'y0': 1.0, \n        'h_list': [2e-4, 1e-4, 5e-5, 2.5e-5]\n    }\n    errors3 = [euler_solver(params3['lambda_val'], params3['T'], h, params3['q'], params3['y0']) for h in params3['h_list']]\n    is_monotone3 = all(errors3[i]  errors3[i+1] for i in range(len(errors3) - 1))\n\n    # Assemble the final results list\n    results = [is_monotone1, errors2, is_monotone3]\n\n    # Format the output string to be exactly \"[b_1,[e_1,...,e_4],b_3]\" with no spaces.\n    # Standard string formatting of a list includes spaces, so we remove them.\n    output_str = f\"[{results[0]},{results[1]},{results[2]}]\"\n    output_str_no_spaces = output_str.replace(\" \", \"\")\n    \n    print(output_str_no_spaces)\n\nsolve()\n```"
        }
    ]
}