{
    "hands_on_practices": [
        {
            "introduction": "Many physical models, such as those for population dynamics or chemical concentrations, require solutions that are inherently positive. It is therefore crucial to understand whether a chosen numerical method respects such physical constraints. This exercise explores the property of positivity preservation for the trapezoidal method, a key qualitative feature that determines its suitability for certain applications. By analyzing the simple linear test equation, you will derive a fundamental condition on the step size that governs whether the numerical solution can become unphysically negative, providing deep insight into the method's behavior. ",
            "id": "3284056",
            "problem": "Consider the scalar initial value problem for an ordinary differential equation (ODE)\n$$\n\\frac{dy}{dt} = f(t,y), \\quad y(t_0) = y_0,\n$$\nwith the requirement that the true solution satisfies $y(t)  0$ for all $t \\ge t_0$. Many models, such as population dynamics, impose $y(t)  0$ as a physical constraint. A numerical method is said to preserve positivity if, starting from $y_0  0$, it produces numerical approximations $y_n$ that remain strictly positive for all steps $n$.\n\nYour task is to investigate whether the trapezoidal method preserves positivity. Start from the fundamental integral form of the initial value problem over a single time step:\n$$\ny(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t))\\, dt,\n$$\nand recall that the trapezoidal quadrature approximates an integral over an interval by the average of the integrand at the endpoints times the interval length. Using only this base, derive the trapezoidal time-stepping relation. Then, specialize to the linear test equation\n$$\n\\frac{dy}{dt} = \\mu y, \\quad \\mu \\in \\mathbb{R},\n$$\nand determine from first principles the condition on the time step size $h$ (in relation to $\\mu$) under which the trapezoidal method does or does not preserve positivity, assuming $y_0  0$.\n\nWrite a program that:\n- Implements the trapezoidal method update for the linear test equation in a way that can generate the discrete sequence $\\{y_n\\}_{n=0}^N$ starting from $y_0  0$ with uniform step size $h$.\n- For each test case below, returns a boolean indicating whether $y_n  0$ for all $n = 1,2,\\ldots,N$ (strict positivity at every step).\n\nUse the following test suite of parameter sets $(\\mu, y_0, h, N)$:\n- Case $1$: $(\\mu, y_0, h, N) = (-1, 1.0, 0.1, 10)$.\n- Case $2$: $(\\mu, y_0, h, N) = (-1, 1.0, 3.0, 5)$.\n- Case $3$: $(\\mu, y_0, h, N) = (1, 0.1, 0.5, 6)$.\n- Case $4$: $(\\mu, y_0, h, N) = (1, 1.0, 3.0, 1)$.\n- Case $5$: $(\\mu, y_0, h, N) = (-10, 0.5, 0.19, 3)$.\n\nNo physical units are involved. Angles are not involved. Percentages are not involved.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets, with no spaces. For example, the format must be exactly like\n```\n[True,False,True,False,True]\n```",
            "solution": "The problem requires an investigation into the positivity-preserving properties of the trapezoidal method for ordinary differential equations (ODEs). This validation will proceed in three steps: first, deriving the trapezoidal method from the provided integral form; second, specializing this method for the linear test equation; and third, deriving the condition on the time step $h$ that ensures positivity is maintained.\n\n**Step 1: Derivation of the Trapezoidal Method**\n\nThe fundamental theorem of calculus applied to the ODE $\\frac{dy}{dt} = f(t,y)$ over the time interval $[t_n, t_{n+1}]$ gives the exact relation:\n$$\ny(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t))\\, dt\n$$\nThe problem specifies approximating the integral using the trapezoidal quadrature rule. This rule approximates the area under a curve by the area of a trapezoid formed by the function's values at the interval endpoints. For an integrand $g(t)$ over $[a, b]$, the approximation is $\\int_a^b g(t) dt \\approx \\frac{b-a}{2}(g(a) + g(b))$.\n\nApplying this to our integral, we set the interval as $[t_n, t_{n+1}]$ with length $h = t_{n+1} - t_n$, and the integrand is $f(t, y(t))$. The approximation is:\n$$\n\\int_{t_n}^{t_{n+1}} f(t, y(t))\\, dt \\approx \\frac{h}{2} \\left[ f(t_n, y(t_n)) + f(t_{n+1}, y(t_{n+1})) \\right]\n$$\nWe define the numerical approximations as $y_n \\approx y(t_n)$ and $y_{n+1} \\approx y(t_{n+1})$. Substituting this approximation back into the exact integral relation yields the trapezoidal time-stepping formula:\n$$\ny_{n+1} = y_n + \\frac{h}{2} \\left[ f(t_n, y_n) + f(t_{n+1}, y_{n+1}) \\right]\n$$\nThis is an implicit method because the unknown value $y_{n+1}$ appears on both sides of the equation, as an argument to the function $f$ on the right-hand side.\n\n**Step 2: Specialization for the Linear Test Equation**\n\nThe linear test equation is given by $\\frac{dy}{dt} = \\mu y$, where $\\mu \\in \\mathbb{R}$. For this ODE, the function $f(t, y)$ is simply $f(t, y) = \\mu y$. Substituting this specific form of $f$ into the trapezoidal formula, we get:\n$$\ny_{n+1} = y_n + \\frac{h}{2} \\left[ (\\mu y_n) + (\\mu y_{n+1}) \\right]\n$$\nTo obtain an explicit update rule, we must algebraically solve for $y_{n+1}$:\n$$\ny_{n+1} - \\frac{h\\mu}{2} y_{n+1} = y_n + \\frac{h\\mu}{2} y_n\n$$\nFactoring out $y_{n+1}$ on the left-hand side and $y_n$ on the right-hand side gives:\n$$\ny_{n+1} \\left(1 - \\frac{h\\mu}{2}\\right) = y_n \\left(1 + \\frac{h\\mu}{2}\\right)\n$$\nAssuming that $1 - \\frac{h\\mu}{2} \\neq 0$, we can isolate $y_{n+1}$:\n$$\ny_{n+1} = y_n \\left( \\frac{1 + \\frac{h\\mu}{2}}{1 - \\frac{h\\mu}{2}} \\right)\n$$\nThis is the discrete recurrence relation for the trapezoidal method applied to the linear test equation. The term in the parenthesis is the amplification factor that determines how the solution evolves from one step to the next.\n\n**Step 3: Derivation of the Positivity-Preserving Condition**\n\nA method preserves positivity if, starting with an initial condition $y_0  0$, all subsequent approximations $y_n$ remain strictly positive. From the recurrence relation derived above, if we assume $y_n  0$, then the sign of $y_{n+1}$ is determined entirely by the sign of the amplification factor:\n$$\nR(h\\mu) = \\frac{1 + \\frac{h\\mu}{2}}{1 - \\frac{h\\mu}{2}}\n$$\nFor $y_{n+1}$ to be strictly positive, we require $R(h\\mu)  0$. Let the product $z = h\\mu$. The condition is:\n$$\n\\frac{1 + z/2}{1 - z/2}  0\n$$\nA quotient is positive if and only if the numerator and denominator have the same sign. This is equivalent to their product being positive, provided the denominator is non-zero.\n$$\n\\left(1 + \\frac{z}{2}\\right) \\left(1 - \\frac{z}{2}\\right)  0 \\quad \\text{and} \\quad 1 - \\frac{z}{2} \\neq 0\n$$\nThe product simplifies to a difference of squares:\n$$\n1 - \\left(\\frac{z}{2}\\right)^2  0\n$$\n$$\n1  \\frac{z^2}{4}\n$$\n$$\n4  z^2\n$$\nTaking the square root of both sides gives:\n$$\n|z|  2\n$$\nSubstituting back $z = h\\mu$, we arrive at the condition for strict positivity preservation:\n$$\n|h\\mu|  2\n$$\nIf $|h\\mu| = 2$, then $z^2 = 4$, which means $1-z^2/4=0$. This implies that either the numerator or the denominator (but not both) of $R(z)$ is zero. If $z=2$, the denominator is zero, leading to a division by zero. If $z=-2$, the numerator is zero, leading to $y_{n+1}=0$, which violates strict positivity. If $|h\\mu|  2$, the amplification factor is negative, causing the solution to change sign at each step, violating positivity.\n\nTherefore, the trapezoidal method preserves positivity for the linear test equation if and only if the product of the step size $h$ and the parameter $\\mu$ satisfies $|h\\mu|  2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Investigates the positivity preservation of the trapezoidal method for the\n    linear test ODE dy/dt = mu*y.\n    \"\"\"\n    # Define the test cases as tuples of (mu, y0, h, N).\n    test_cases = [\n        (-1.0, 1.0, 0.1, 10),\n        (-1.0, 1.0, 3.0, 5),\n        (1.0, 0.1, 0.5, 6),\n        (1.0, 1.0, 3.0, 1),\n        (-10.0, 0.5, 0.19, 3),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        mu, y0, h, N = case\n        \n        # Initialize the solution variable and the positivity flag.\n        y = np.float64(y0)\n        preserves_positivity = True\n\n        # The amplification factor for the trapezoidal method on the test equation is\n        # R = (1 + h*mu/2) / (1 - h*mu/2).\n        # We check the condition |h*mu|  2, which is equivalent to R  0.\n        # This pre-check is sufficient, but the simulation is performed as requested.\n        \n        # Calculate the numerator and denominator of the amplification factor.\n        # Use np.float64 for precision consistent with typical scientific computing.\n        numerator = np.float64(1.0) + h * mu / np.float64(2.0)\n        denominator = np.float64(1.0) - h * mu / np.float64(2.0)\n\n        # A zero denominator means h*mu = 2, violating positivity.\n        if denominator == 0.0:\n            preserves_positivity = False\n        else:\n            amp_factor = numerator / denominator\n            \n            # If the amplification factor is not positive, positivity is violated.\n            if amp_factor = 0:\n                preserves_positivity = False\n            else:\n                # If the amplification factor is positive, y will never become non-positive\n                # starting from y0  0, as it's just repeated multiplication by a\n                # positive number. The simulation loop is technically redundant if we\n                # trust the analysis, but we run it to confirm step-by-step.\n                for _ in range(N):\n                    y = y * amp_factor\n                    # Check for strict positivity at each step.\n                    if y = 0.0:\n                        preserves_positivity = False\n                        break\n        \n        results.append(preserves_positivity)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A defining feature of the trapezoidal method is its implicit nature, which means that each time step requires solving an algebraic equation to find the next point in the solution. For stiff ordinary differential equations, where different physical processes occur on vastly different time scales, the choice of how to solve this implicit equation is critical for both efficiency and stability. This hands-on problem guides you through implementing and comparing two common techniques for this task: a simple fixed-point iteration and the more robust Newton's method. By applying them to a classic stiff problem, you will discover firsthand why Newton's method is often essential for creating a reliable solver. ",
            "id": "3284122",
            "problem": "Implement a program that constructs and analyzes an implicit one-step method for a scalar initial value problem derived from the fundamental integral form. Consider the initial value problem defined by the ordinary differential equation $y'(t) = -100\\,y(t) + 100\\,t + 101$ with initial condition $y(0) = 1$. Starting from the identity $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t))\\,dt$ and the fact that numerical quadrature can approximate the integral of a smooth function on a short interval, derive the implicit discrete equation resulting from applying the classical trapezoidal quadrature rule to the integral. Do not write down any pre-existing step formulas; instead, use the definition of the trapezoidal rule for numerical integration as the only input and express the resulting nonlinear equation that defines $y_{n+1}$ in terms of known quantities at $t_n$ and unknown quantities at $t_{n+1}$.\n\nTo compute $y_{n+1}$ from the implicit equation at each step, implement both of the following inner solvers for the nonlinear scalar equation that must be satisfied by $y_{n+1}$:\n- A fixed-point (Picard) iteration based on repeatedly substituting the right-hand side expression that emerges from the trapezoidal construction, initialized with $y^{(0)} = y_n$.\n- Newton’s method using the derivative with respect to $y$ of the left-hand side of the implicit equation, with the same initial guess $y^{(0)} = y_n$.\n\nUse an absolute stopping tolerance of $10^{-12}$ for both inner solvers, applied to the natural residual of the equation for Newton’s method and to the difference between successive iterates for the fixed-point iteration. Use a per-step maximum of $100$ inner iterations. If the fixed-point iteration fails to converge at any step within this cap, consider the entire fixed-point solve for that step size to have failed and report a sentinel value as specified below. For both methods, count the total number of inner iterations accumulated over all steps.\n\nTo quantify accuracy, derive the exact solution of the given initial value problem using the integrating factor method and use it to compute the absolute error $|y_N - y_{\\text{exact}}(T)|$ at final time $T$. The integration interval is $[0, T]$ with $T = 1$, and the step size is $h$, so that $t_n = n h$ and $N = T/h$ is an integer.\n\nTest suite:\n- Integrate to $T = 1$ with step sizes $h \\in \\{0.001, 0.01, 0.02, 0.05\\}$, in this order. That is, $h = 0.001$, $h = 0.01$, $h = 0.02$, and $h = 0.05$.\n- For each $h$, compute:\n  1) the final value produced by the trapezoidal method when the implicit solve is carried out by Newton’s method, $y_N^{\\text{(Newton)}}$,\n  2) the absolute error at $T$, namely $|y_N^{\\text{(Newton)}} - y_{\\text{exact}}(1)|$,\n  3) the total number of inner Newton iterations summed over all steps,\n  4) the total number of inner fixed-point iterations summed over all steps if all steps converged; otherwise output the integer $-1$ to indicate failure.\n\nYour program should produce a single line of output containing the results in a flat, comma-separated list enclosed in square brackets and ordered by the step sizes listed above, with four numbers per step size in the order specified. Concretely, the output format must be\n```\n[y_N^(Newton)(h1), |y_N^(Newton)(h1) - y_exact(1)|, iters^(Newton)(h1), iters^(Fixed)(h1), ..., y_N^(Newton)(h4), |y_N^(Newton)(h4) - y_exact(1)|, iters^(Newton)(h4), iters^(Fixed)(h4)]\n```\nwhere $h_1 = 0.001$, $h_2 = 0.01$, $h_3 = 0.02$, and $h_4 = 0.05$. If the fixed-point iteration fails for any step for a given $h$, print $-1$ for `iters^(Fixed)(h)` for that $h$. All numbers must be printed as plain decimal numerals on a single line in the exact list format shown, with no units and no extra whitespace.",
            "solution": "The user-provided problem is a well-defined task in the field of numerical analysis for ordinary differential equations (ODEs). It requires the derivation, implementation, and analysis of the implicit trapezoidal method.\n\n### Step 1: Problem Validation\n\nThe problem is validated against the required criteria.\n\n*   **Extraction of Givens**:\n    *   **ODE**: The scalar initial value problem (IVP) is $y'(t) = -100\\,y(t) + 100\\,t + 101$.\n    *   **Initial Condition**: $y(0) = 1$.\n    *   **Integral Form of ODE**: $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t))\\,dt$.\n    *   **Method Derivation**: The numerical method must be derived by applying the trapezoidal quadrature rule to the integral form.\n    *   **Inner Solvers**: The implicit equation at each step is to be solved using two methods: a fixed-point (Picard) iteration and Newton's method.\n    *   **Initial Guess for Inner Solvers**: $y^{(0)} = y_n$ for both methods.\n    *   **Inner Solver Tolerance**: Absolute tolerance of $10^{-12}$. For Newton's method, this applies to the residual. For fixed-point, it applies to the difference between successive iterates.\n    *   **Inner Solver Iteration Limit**: Maximum of $100$ iterations per time step.\n    *   **Fixed-Point Failure**: If convergence is not achieved within the iteration limit at any step, the total iteration count for that step size $h$ is reported as $-1$.\n    *   **Analysis**: Compute the absolute error $|y_N - y_{\\text{exact}}(T)|$ at the final time $T=1$.\n    *   **Discretization**: Time interval is $[0, 1]$. Step size $h$ leads to $t_n = n h$ and total steps $N = T/h$.\n    *   **Test Suite**: Step sizes $h \\in \\{0.001, 0.01, 0.02, 0.05\\}$.\n    *   **Required Outputs per `h`**: $y_N^{\\text{(Newton)}}$, $|y_N^{\\text{(Newton)}} - y_{\\text{exact}}(1)|$, total Newton iterations, total fixed-point iterations (or $-1$).\n\n*   **Validation Verdict**:\n    *   **Scientifically Grounded**: Yes. The problem is a standard an exercise in numerical analysis, involving established methods (trapezoidal rule, Newton's method, fixed-point iteration) applied to a linear ODE.\n    *   **Well-Posed**: Yes. The linear IVP is well-posed, guaranteeing a unique solution. The numerical task is clearly specified.\n    *   **Objective**: Yes. The problem is defined with precise mathematical and computational requirements.\n    *   The problem is complete, consistent, and computationally feasible. It is a substantive and non-trivial task that requires both derivation and implementation.\n\n*   **Conclusion**: The problem is valid.\n\n### Step 2: Solution Derivations and Method Analysis\n\nThe solution requires several analytical derivations before implementation. The ODE is of the form $y'(t) = f(t, y(t))$, where $f(t, y) = -100y + 100t + 101$.\n\n#### Derivation of the Exact Solution\nThe ODE $y' + 100y = 100t + 101$ is a first-order linear ODE. We solve it using an integrating factor $I(t) = e^{\\int 100 dt} = e^{100t}$. Multiplying the ODE by $I(t)$ gives:\n$$ e^{100t}y' + 100e^{100t}y = (100t + 101)e^{100t} $$\nThe left side is the derivative of a product:\n$$ \\frac{d}{dt}(y(t)e^{100t}) = (100t + 101)e^{100t} $$\nIntegrating both sides with respect to $t$:\n$$ y(t)e^{100t} = \\int (100t + 101)e^{100t} dt $$\nThe integral on the right is solved using integration by parts, $\\int u dv = uv - \\int v du$, with $u = 100t+101$ and $dv = e^{100t}dt$. This yields $du = 100dt$ and $v = \\frac{1}{100}e^{100t}$.\n$$ \\int (100t + 101)e^{100t} dt = (100t+101)\\frac{e^{100t}}{100} - \\int \\frac{e^{100t}}{100} (100) dt = (t + 1.01)e^{100t} - \\int e^{100t}dt $$\n$$ = (t + 1.01)e^{100t} - \\frac{1}{100}e^{100t} + C = (t+1)e^{100t} + C $$\nThus, the general solution is:\n$$ y(t) = t + 1 + Ce^{-100t} $$\nApplying the initial condition $y(0) = 1$:\n$$ 1 = 0 + 1 + Ce^0 \\implies C = 0 $$\nThe exact solution is $y_{\\text{exact}}(t) = t + 1$. The value at the final time $T=1$ is $y_{\\text{exact}}(1) = 1+1=2$.\n\n#### Derivation of the Trapezoidal Method\nStarting from the integral form of the ODE over a single step $[t_n, t_{n+1}]$:\n$$ y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t)) dt $$\nWe approximate the integral using the trapezoidal rule, $\\int_a^b g(x)dx \\approx \\frac{b-a}{2}(g(a)+g(b))$. Let $h=t_{n+1}-t_n$, $y_n \\approx y(t_n)$, and $y_{n+1} \\approx y(t_{n+1})$.\n$$ \\int_{t_n}^{t_{n+1}} f(t, y(t)) dt \\approx \\frac{h}{2}(f(t_n, y(t_n)) + f(t_{n+1}, y(t_{n+1}))) $$\nSubstituting this into the integral equation gives the implicit trapezoidal method:\n$$ y_{n+1} = y_n + \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, y_{n+1})) $$\nThis is an implicit equation because the unknown $y_{n+1}$ appears on both sides.\n\n#### Setup for Inner Solvers\nTo find $y_{n+1}$ at each step, we must solve this equation. Let $w$ be the unknown value for $y_{n+1}$.\n\n**1. Fixed-Point Iteration:**\nThe equation is rearranged to form a fixed-point mapping $w = G(w)$:\n$$ w = y_n + \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, w)) $$\nThe iteration is $w^{(k+1)} = G(w^{(k)})$, starting with $w^{(0)} = y_n$. The process is repeated until $|w^{(k+1)} - w^{(k)}|  10^{-12}$.\nConvergence is governed by the contraction mapping principle, which requires $|G'(w)|  1$. The derivative is:\n$$ G'(w) = \\frac{d}{dw} \\left( y_n + \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, w)) \\right) = \\frac{h}{2} \\frac{\\partial f}{\\partial y}(t_{n+1}, w) $$\nFor the given problem, $\\frac{\\partial f}{\\partial y} = -100$. Thus, $G'(w) = -50h$. The iteration converges if $|-50h|  1$, or $h  0.02$.\n*   For $h=0.001$: $|-50(0.001)|=0.05  1$. Converges.\n*   For $h=0.01$: $|-50(0.01)|=0.5  1$. Converges.\n*   For $h=0.02$: $|-50(0.02)|=1$. This is a boundary case where convergence is not guaranteed and will be extremely slow, or may fail. The iteration will not meet the tolerance within $100$ steps.\n*   For $h=0.05$: $|-50(0.05)|=2.5  1$. Diverges.\nTherefore, the fixed-point method is expected to fail for $h=0.02$ and $h=0.05$.\n\n**2. Newton's Method:**\nWe solve for the root of the residual function $F(w) = 0$, where:\n$$ F(w) = w - y_n - \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, w)) = 0 $$\nThe Newton-Raphson iteration is $w^{(k+1)} = w^{(k)} - \\frac{F(w^{(k)})}{F'(w^{(k)})}$, starting with $w^{(0)} = y_n$. The process repeats until the residual is small: $|F(w^{(k)})|  10^{-12}$.\nThe derivative $F'(w)$ is:\n$$ F'(w) = \\frac{d}{dw} \\left( w - y_n - \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, w)) \\right) = 1 - \\frac{h}{2} \\frac{\\partial f}{\\partial y}(t_{n+1}, w) $$\nFor our problem, $F'(w) = 1 - \\frac{h}{2}(-100) = 1 + 50h$.\nSince $f(t,y)$ is linear in $y$, the residual $F(w)$ is a linear function of $w$. Consequently, Newton's method will find the exact root of this linear equation in a single iteration (barring floating-point inaccuracies). Thus, for each time step, the number of Newton iterations is expected to be $1$.\n\n### Step 3: Computational Algorithm\nThe overall algorithm is as follows:\n1.  Initialize a list to store the final results.\n2.  Define the function $f(t,y) = -100y + 100t + 101$ and its partial derivative $\\frac{\\partial f}{\\partial y} = -100$.\n3.  Define the exact solution $y_{\\text{exact}}(t) = t+1$. The final value is $y_{\\text{exact}}(1)=2$.\n4.  Loop through each step size $h \\in \\{0.001, 0.01, 0.02, 0.05\\}$.\n5.  For each $h$, run two separate simulations from $t=0$ to $t=1$: one using Newton's method as the inner solver, and one using the fixed-point method.\n6.  In each simulation, initialize $t=0$, $y=1$, and the total iteration counter.\n7.  Iterate $N=T/h$ times. In each step, call the respective inner solver (Newton or fixed-point) to find $y_{n+1}$ from $y_n$. Accumulate the number of iterations returned by the solver.\n8.  For the fixed-point simulation, if any step fails to converge in $100$ iterations, set a failure flag. The final iteration count will be $-1$.\n9.  After the loops, calculate the final error for the Newton method result.\n10. Store the four required values for the current $h$: $y_N^{\\text{(Newton)}}$, error, Newton iterations, and fixed-point iterations.\n11. After processing all $h$ values, flatten the list of results and format it for printing.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes the trapezoidal method for a given ODE\n    using both Newton and Fixed-Point inner solvers.\n    \"\"\"\n\n    # --- Problem Definition ---\n    # ODE: y'(t) = -100*y(t) + 100*t + 101\n    # Initial Condition: y(0) = 1\n    # Time interval: [0, 1]\n    \n    y0 = 1.0\n    t_end = 1.0\n    tol = 1e-12\n    max_inner_iters = 100\n    \n    test_h_values = [0.001, 0.01, 0.02, 0.05]\n\n    def f(t, y):\n        \"\"\"RHS of the ODE y' = f(t, y).\"\"\"\n        return -100.0 * y + 100.0 * t + 101.0\n\n    def dfdy(t, y):\n        \"\"\"Partial derivative of f with respect to y.\"\"\"\n        # For this problem, df/dy is constant.\n        return -100.0\n\n    def y_exact(t):\n        \"\"\"Exact solution to the IVP.\"\"\"\n        return t + 1.0\n\n    # --- Solver Implementations ---\n\n    def run_simulation(h, inner_solver_type):\n        \"\"\"\n        Integrates the ODE from t=0 to t=T with step size h,\n        using the specified inner solver.\n\n        Args:\n            h (float): The step size.\n            inner_solver_type (str): 'newton' or 'fixed_point'.\n\n        Returns:\n            A tuple (final_y, total_iters). `total_iters` is -1 on failure.\n        \"\"\"\n        y = y0\n        t = 0.0\n        n_steps = int(round(t_end / h))\n        total_iters = 0\n        \n        for n in range(n_steps):\n            tn = n * h\n            tn1 = (n + 1) * h\n        \n            if inner_solver_type == 'newton':\n                y_next, iters_step = solve_newton_step(y, tn, h)\n            elif inner_solver_type == 'fixed_point':\n                y_next, iters_step = solve_fp_step(y, tn, h)\n            \n            if iters_step == -1:\n                return None, -1 # Failure to converge\n            \n            y = y_next\n            total_iters += iters_step\n            \n        return y, total_iters\n\n    def solve_newton_step(yn, tn, h):\n        \"\"\"Solves for y_{n+1} using Newton's method.\"\"\"\n        tn1 = tn + h\n        fn = f(tn, yn)\n        \n        # F'(w) is constant for this linear ODE\n        F_prime = 1.0 - (h / 2.0) * dfdy(tn1, 0)\n        \n        w = yn  # Initial guess w^(0)\n        \n        # Check initial guess residual\n        fw = f(tn1, w)\n        F_w = w - yn - (h / 2.0) * (fn + fw)\n        if np.abs(F_w)  tol:\n            return w, 0\n\n        for iters in range(1, max_inner_iters + 1):\n            w = w - F_w / F_prime\n            \n            # Check residual of the new iterate\n            fw = f(tn1, w)\n            F_w = w - yn - (h / 2.0) * (fn + fw)\n            \n            if np.abs(F_w)  tol:\n                return w, iters\n                \n        return None, -1  # Did not converge\n\n    def solve_fp_step(yn, tn, h):\n        \"\"\"Solves for y_{n+1} using fixed-point iteration.\"\"\"\n        tn1 = tn + h\n        fn = f(tn, yn)\n        \n        w_k = yn # Initial guess w^(0)\n        \n        for iters in range(1, max_inner_iters + 1):\n            fw_k = f(tn1, w_k)\n            w_k_plus_1 = yn + (h / 2.0) * (fn + fw_k)\n            \n            if np.abs(w_k_plus_1 - w_k)  tol:\n                return w_k_plus_1, iters\n            \n            w_k = w_k_plus_1\n        \n        return None, -1 # Did not converge\n\n    # --- Main Execution Logic ---\n    results = []\n    y_final_exact = y_exact(t_end)\n\n    for h in test_h_values:\n        # 1. Newton's method results\n        yN_newton, iters_newton = run_simulation(h, 'newton')\n        error_newton = np.abs(yN_newton - y_final_exact)\n        \n        # 2. Fixed-point method results\n        # We don't need the final y value, just the iteration count.\n        _, iters_fp = run_simulation(h, 'fixed_point')\n\n        # 3. Collect results for this h\n        results.extend([yN_newton, error_newton, iters_newton, iters_fp])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practice, using a fixed step size for an entire integration is highly inefficient, as it must be small enough for the most rapidly changing parts of the solution, wasting computational effort in smoother regions. Professional-grade ODE solvers overcome this by using adaptive step-size control to maintain a desired accuracy with minimal work. This capstone exercise challenges you to build a complete adaptive integrator from first principles, using the trapezoidal method as the foundation. You will implement a robust error estimation scheme and use it to create a controller that automatically adjusts the step size, representing a significant leap from basic methods to practical scientific computing. ",
            "id": "3284120",
            "problem": "Design and implement an adaptive step size ordinary differential equation integrator for the implicit trapezoidal method that controls the local truncation error without using any external packages beyond the standard scientific computing tools permitted below. The integrator must be derived from the integral form of the initial value problem and the classical trapezoid rule for numerical quadrature. The step size controller must be based on an estimate of the local truncation error that comes from comparing one full step with two half-steps and reasoning about the error order, starting from basic definitions of consistency and local truncation error. The algorithm must be implemented for scalar initial value problems of the form $y'(t) = f(t,y)$ with $y(t_0) = y_0$ over a finite interval $[t_0,T]$. The implicit equation at each step must be solved by a root-finding iteration grounded in first principles. You must not assume availability of an exact Jacobian; if a derivative is needed, you must construct a numerically sound finite-difference approximation.\n\nYou must base your derivation on the following starting points only:\n- The integral form of the initial value problem: $y(t_{n+1}) - y(t_{n}) = \\int_{t_{n}}^{t_{n+1}} f(t,y(t))\\,dt$.\n- The composite trapezoid rule for a definite integral over one subinterval: $\\int_{a}^{b} g(t)\\,dt \\approx \\tfrac{b-a}{2}\\big(g(a)+g(b)\\big)$, with a local quadrature error of order $O((b-a)^{3})$ under appropriate smoothness.\n- The definition of local truncation error, global error order, and the idea of step doubling and Richardson-type reasoning to construct an error estimate from approximations at different step sizes.\n\nYour program must implement:\n- An implicit trapezoidal one-step update derived from the integral form and trapezoid rule. The resulting nonlinear equation for the new value must be solved using a principled iteration based on root finding, with a stopping criterion scaling with the magnitude of the iterate.\n- An adaptive step size controller that:\n  1. Constructs a computable local error estimate by comparing a single step of size $h$ with two successive steps of size $h/2$.\n  2. Uses the known local error order of the implicit trapezoidal method to normalize this difference and produce a per-step error indicator.\n  3. Accepts or rejects a tentative step based on a user-specified absolute and relative tolerance using a scale of the form $\\text{atol} + \\text{rtol}\\cdot\\max(\\lvert y_{\\text{coarse}}\\rvert,\\lvert y_{\\text{fine}}\\rvert)$.\n  4. Updates the next step size using a power-law rule derived from the local error order, together with a safety factor and lower and upper bounds on growth and shrinkage.\n\nImplementation constraints and requirements:\n- Scalar problems only.\n- The implicit solve at each step must use a root-finding iteration with a finite-difference derivative and a stopping threshold that scales like $\\epsilon \\cdot \\max(1,\\lvert y\\rvert)$ for some small $\\epsilon$ related to the requested accuracy.\n- If the implicit solve fails to converge for a tentative step size, the algorithm must robustly reduce the step size and retry.\n- Enforce a minimum and maximum step size, $h_{\\min}$ and $h_{\\max}$, and ensure that the final step reaches exactly $T$ by truncation when necessary.\n- Angles, where applicable, must be in radians.\n- All outputs must be dimensionless real numbers.\n\nTest suite to implement and evaluate:\n- Case $\\mathbf{1}$ (stable linear decay): $f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $T = 5$, absolute tolerance $\\text{atol} = 10^{-12}$, relative tolerance $\\text{rtol} = 10^{-10}$, initial step $h_0 = 5\\times 10^{-1}$, $h_{\\min} = 10^{-12}$, $h_{\\max} = 1$.\n- Case $\\mathbf{2}$ (stiff but exactly solvable): $f(t,y) = -1000\\,(y - e^{-t}) - e^{-t}$, $t_0 = 0$, $y_0 = 1$, $T = 1$, absolute tolerance $\\text{atol} = 10^{-10}$, relative tolerance $\\text{rtol} = 10^{-8}$, initial step $h_0 = 10^{-2}$, $h_{\\min} = 10^{-12}$, $h_{\\max} = 1$.\n- Case $\\mathbf{3}$ (nonlinear logistic growth): $f(t,y) = y\\,(1-y)$, $t_0 = 0$, $y_0 = 10^{-1}$, $T = 5$, absolute tolerance $\\text{atol} = 10^{-12}$, relative tolerance $\\text{rtol} = 10^{-10}$, initial step $h_0 = 5\\times 10^{-1}$, $h_{\\min} = 10^{-12}$, $h_{\\max} = 1$.\n- Case $\\mathbf{4}$ (forced linear oscillator with decay): $f(t,y) = \\cos(t) - y$, $t_0 = 0$, $y_0 = 0$, $T = 10$, absolute tolerance $\\text{atol} = 10^{-12}$, relative tolerance $\\text{rtol} = 10^{-10}$, initial step $h_0 = 5\\times 10^{-1}$, $h_{\\min} = 10^{-12}$, $h_{\\max} = 1$.\n\nWhat to compute and return:\n- For each case, integrate from $t_0$ to $T$ and return the numerical approximation to $y(T)$ as a real number.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[result1,result2,result3,result4]`), in the order of Cases $\\mathbf{1}$, $\\mathbf{2}$, $\\mathbf{3}$, $\\mathbf{4}$. Each entry must be a floating-point number.\n```\n[result1,result2,result3,result4]\n```",
            "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded, and objective task within the domain of numerical methods for ordinary differential equations (ODEs). The instructions provide a clear and complete specification for the design and implementation of an adaptive step size integrator based on the implicit trapezoidal method, including the derivation, the implicit solver, the adaptive control logic, and a full set of test cases.\n\nHere follows the derivation and algorithmic design as per the problem statement.\n\n### 1. The Implicit Trapezoidal Method\n\nThe initial value problem (IVP) is defined as $y'(t) = f(t,y)$ with an initial condition $y(t_0) = y_0$. The solution can be expressed in integral form over a single step from $t_n$ to $t_{n+1}$:\n$$\ny(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt\n$$\nLet $h = t_{n+1} - t_n$ be the step size. We approximate the integral using the trapezoid rule, $\\int_{a}^{b} g(t)\\,dt \\approx \\tfrac{b-a}{2}\\big(g(a)+g(b)\\big)$. Applying this to the integral of $f(t, y(t))$ yields:\n$$\n\\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt \\approx \\frac{h}{2} \\left[ f(t_n, y(t_n)) + f(t_{n+1}, y(t_{n+1})) \\right]\n$$\nSubstituting this into the integral form gives the one-step numerical method for the approximations $y_n \\approx y(t_n)$:\n$$\ny_{n+1} = y_n + \\frac{h}{2} \\left[ f(t_n, y_n) + f(t_{n+1}, y_{n+1}) \\right]\n$$\nThis is the implicit trapezoidal rule. It is \"implicit\" because the unknown value $y_{n+1}$ appears on both sides of the equation. To compute $y_{n+1}$, we must solve a (generally nonlinear) algebraic equation.\n\n### 2. Solving the Implicit Equation with Newton's Method\n\nTo find $y_{n+1}$, we must find the root of the function $G(y_{n+1})$:\n$$\nG(y_{n+1}) = y_{n+1} - y_n - \\frac{h}{2} \\left[ f(t_n, y_n) + f(t_{n+1}, y_{n+1}) \\right] = 0\n$$\nNewton's method is a suitable iterative root-finding algorithm. Given a guess $y_{n+1}^{(k)}$, the next iteration is:\n$$\ny_{n+1}^{(k+1)} = y_{n+1}^{(k)} - \\frac{G(y_{n+1}^{(k)})}{G'(y_{n+1}^{(k)})}\n$$\nThe derivative of $G$ with respect to its argument, which we denote $y^* \\equiv y_{n+1}$, is:\n$$\nG'(y^*) = \\frac{d}{dy^*} \\left( y^* - y_n - \\frac{h}{2} \\left[ f(t_n, y_n) + f(t_{n+1}, y^*) \\right] \\right) = 1 - \\frac{h}{2} \\frac{\\partial f}{\\partial y}(t_{n+1}, y^*)\n$$\nAs the Jacobian $\\frac{\\partial f}{\\partial y}$ is not assumed to be available analytically, we approximate it using a forward finite difference. The derivative of $f$ with respect to $y$ at $(t, y)$ is estimated as:\n$$\n\\frac{\\partial f}{\\partial y}(t, y) \\approx \\frac{f(t, y + \\delta) - f(t, y)}{\\delta}\n$$\nA numerically sound choice for the perturbation $\\delta$ depends on machine precision $\\epsilon_{\\text{mach}}$ and the scale of $y$. We use $\\delta = \\sqrt{\\epsilon_{\\text{mach}}} \\cdot \\max(1, |y|)$.\n\nThe iteration starts with an initial guess, such as $y_{n+1}^{(0)} = y_n$. The process is terminated when the magnitude of the update step, $|y_{n+1}^{(k+1)} - y_{n+1}^{(k)}|$, falls below a threshold $\\epsilon_{\\text{Newton}} \\cdot \\max(1, |y_{n+1}^{(k+1)}|)$, where $\\epsilon_{\\text{Newton}}$ is a small tolerance (e.g., $10^{-14}$) ensuring the algebraic equation is solved with high precision. If convergence is not achieved within a maximum number of iterations, the procedure fails, signaling that the step size $h$ is too large.\n\n### 3. Adaptive Step Size Control\n\nThe step size $h$ is adapted to control the local error per step. This is achieved by comparing the results of taking one large step versus two small steps.\n\n1.  **Step Computations**: From $(t_n, y_n)$, we compute two approximations for the solution at $t_n + h$:\n    -   $y_{\\text{coarse}}$: Result of one trapezoidal step of size $h$.\n    -   $y_{\\text{fine}}$: Result of two successive trapezoidal steps, each of size $h/2$.\n\n2.  **Error Estimation**: The trapezoidal method has a global error of order $O(h^2)$ and a local error of order $O(h^3)$. For a single step starting from an exact value, the error $e(h)$ is $e(h) = y(t_n+h) - y_{\\text{step}}(h) \\approx C h^3$.\n    -   The error for the coarse step is $E_{\\text{coarse}} \\approx C h^3$. Thus, $y_{\\text{coarse}} \\approx y(t_n+h) - C h^3$.\n    -   The error for two fine steps is the sum of the local errors: $E_{\\text{fine}} \\approx 2 \\cdot C (h/2)^3 = C h^3 / 4$. Thus, $y_{\\text{fine}} \\approx y(t_n+h) - C h^3 / 4$.\n    \n    Subtracting the two approximations gives:\n    $$\n    y_{\\text{fine}} - y_{\\text{coarse}} \\approx \\left(y(t_n+h) - \\frac{Ch^3}{4}\\right) - \\left(y(t_n+h) - Ch^3\\right) = \\frac{3}{4}Ch^3\n    $$\n    The error in the more accurate fine solution, $E_{\\text{fine}} \\approx C h^3/4$, can be estimated from the computed values:\n    $$\n    \\text{err\\_est} = |E_{\\text{fine}}| \\approx \\frac{1}{3} |y_{\\text{fine}} - y_{\\text{coarse}}|\n    $$\n\n3.  **Step Acceptance/Rejection**: A tolerance `tol` is computed based on user-specified absolute (`atol`) and relative (`rtol`) tolerances:\n    $$\n    \\text{tol} = \\text{atol} + \\text{rtol} \\cdot \\max(|y_{\\text{coarse}}|, |y_{\\text{fine}}|)\n    $$\n    The step is accepted if $\\text{err\\_est} \\le \\text{tol}$. If accepted, the solution is advanced to $(t_{n+1}, y_{\\text{fine}})$ using the more accurate result (a technique known as local extrapolation). If rejected, the step is retried with a smaller $h$.\n\n4.  **New Step Size Selection**: A new step size $h_{\\text{new}}$ is calculated to aim for an error equal to the tolerance on the next attempt. Since $\\text{err\\_est} \\propto h^3$, we have:\n    $$\n    \\frac{\\text{tol}}{\\text{err\\_est}} \\approx \\frac{K h_{\\text{new}}^3}{K h^3} \\implies h_{\\text{new}} = h \\left( \\frac{\\text{tol}}{\\text{err\\_est}} \\right)^{1/3}\n    $$\n    A safety factor $S  1$ (e.g., $S=0.9$) is included to be conservative. The growth and shrinkage of the step size are also bounded by multiplicative factors (e.g., between $0.2$ and $5.0$). The final proposed step size is clamped between specified minimum ($h_{\\text{min}}$) and maximum ($h_{\\text{max}}$) values.\n\n### 4. Algorithm Summary\n\nThe integrator proceeds as follows:\n- Initialize $t=t_0, y=y_0, h=h_0$.\n- Loop while $t  T$:\n    a. Truncate $h$ if $t+h  T$ to land exactly on $T$.\n    b. Enter an inner loop to find an acceptable step:\n        i. Attempt to compute $y_{\\text{coarse}}$ (one step of size $h$) and $y_{\\text{fine}}$ (two steps of size $h/2$). This involves three calls to the Newton solver.\n        ii. If any Newton solve fails, reduce $h$ (e.g., halve it), ensure $h \\ge h_{\\text{min}}$, and restart the inner loop.\n        iii. Calculate `err_est` and `tol`.\n        iv. If $\\text{err\\_est} \\le \\text{tol}$ (step accepted):\n            - Advance state: $t \\leftarrow t+h$, $y \\leftarrow y_{\\text{fine}}$.\n            - Calculate $h_{\\text{new}}$ for the *next* step based on the error ratio.\n            - Set $h \\leftarrow h_{\\text{new}}$ and break the inner loop.\n        v. If $\\text{err\\_est}  \\text{tol}$ (step rejected):\n            - Calculate $h_{\\text{new}}$ (which will be smaller) based on the error ratio.\n            - Set $h \\leftarrow h_{\\text{new}}$, ensure $h \\ge h_{\\text{min}}$, and continue the inner loop to retry the current step.\n- Return the final value of $y$.\nThis constitutes a robust, adaptive ODE solver built from first principles as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef trapezoid_step_solver(f, t_n, y_n, h):\n    \"\"\"\n    Solves for a single step of the implicit trapezoidal method using Newton's method.\n    y_{n+1} = y_n + h/2 * (f(t_n, y_n) + f(t_{n+1}, y_{n+1}))\n    \"\"\"\n    t_np1 = t_n + h\n    fn_val = f(t_n, y_n)\n    \n    # Newton's method parameters\n    newton_rtol = 1e-14\n    max_iter = 10\n    \n    # Initial guess for y_{n+1}\n    y_guess = y_n\n    \n    for _ in range(max_iter):\n        f_np1_guess = f(t_np1, y_guess)\n        \n        # G(y_guess) = 0 is the equation to solve.\n        G = y_guess - y_n - h / 2.0 * (fn_val + f_np1_guess)\n        \n        # Check for convergence of the residual G itself\n        if abs(G)  1e-15:\n            return y_guess, True\n\n        # Finite difference for the derivative of f w.r.t y\n        mach_eps = np.finfo(float).eps\n        delta = np.sqrt(mach_eps) * max(1.0, abs(y_guess))\n        dfdy = (f(t_np1, y_guess + delta) - f_np1_guess) / delta\n        \n        # Derivative of G w.r.t y_guess\n        dG_dy = 1.0 - h / 2.0 * dfdy\n        \n        if abs(dG_dy)  1e-15:\n            # Jacobian is singular, Newton's method fails.\n            return y_guess, False\n        \n        # Newton update\n        update = -G / dG_dy\n        y_guess += update\n        \n        # Check for convergence based on update size\n        newton_tol = newton_rtol * max(1.0, abs(y_guess))\n        if abs(update)  newton_tol:\n            return y_guess, True\n            \n    # If loop finishes, convergence failed.\n    return y_guess, False\n\ndef adaptive_trapezoid_solver(f, t0, y0, T, atol, rtol, h0, h_min, h_max):\n    \"\"\"\n    An adaptive step size ODE integrator using the implicit trapezoidal method.\n    \"\"\"\n    t = float(t0)\n    y = float(y0)\n    h = float(h0)\n\n    # Controller parameters\n    safety = 0.9\n    min_factor = 0.2\n    max_factor = 5.0\n    \n    while t  T:\n        if t + h  T:\n            h = T - t\n        \n        # Ensure h does not fall below h_min, except for the final step.\n        h = max(h, h_min)\n\n        while True: # Inner loop to find an acceptable step\n            # Coarse step\n            y_coarse, coarse_ok = trapezoid_step_solver(f, t, y, h)\n            if not coarse_ok:\n                h = max(h * 0.5, h_min)\n                if h == h_min and t + h  T:\n                     raise RuntimeError(f\"Newton solver failed at t={t} with minimum step size h={h}.\")\n                continue\n\n            # Fine steps (two half-steps)\n            y_mid, mid_ok = trapezoid_step_solver(f, t, y, h / 2.0)\n            if not mid_ok:\n                h = max(h * 0.5, h_min)\n                if h == h_min and t + h  T:\n                     raise RuntimeError(f\"Newton solver failed at t={t} with minimum step size h={h}.\")\n                continue\n            \n            y_fine, fine_ok = trapezoid_step_solver(f, t + h / 2.0, y_mid, h / 2.0)\n            if not fine_ok:\n                h = max(h * 0.5, h_min)\n                if h == h_min and t + h  T:\n                     raise RuntimeError(f\"Newton solver failed at t={t} with minimum step size h={h}.\")\n                continue\n\n            # Error estimation\n            error_norm = abs(y_fine - y_coarse)\n            err_est = error_norm / 3.0 # Based on local error O(h^3)\n                                       # Error_fine = 1/3 * (y_fine - y_coarse)\n            \n            # Tolerance scale\n            y_scale = max(abs(y_coarse), abs(y_fine))\n            tol = atol + rtol * y_scale\n            \n            # Step size update logic\n            if err_est = tol:\n                # Step accepted\n                t += h\n                y = y_fine # Local extrapolation\n                \n                # Calculate next step size\n                if error_norm == 0.0:\n                    factor = max_factor\n                else:\n                    factor = safety * (tol / err_est)**(1.0/3.0)\n                    factor = min(max_factor, max(min_factor, factor))\n                \n                h_new = h * factor\n                h = min(h_max, max(h_min, h_new))\n                \n                break # Exit inner loop, proceed to next step\n            else:\n                # Step rejected, reduce step size and retry\n                factor = safety * (tol / err_est)**(1.0/3.0)\n                factor = min(max_factor, max(min_factor, factor)) # factor should be  1\n                \n                h_new = h * factor\n                h = max(h_min, h_new)\n                # Continue inner loop with smaller h\n                if h == h_min:\n                    # If on minimum step, must accept to move forward\n                    t += h\n                    y = y_fine\n                    break\n\n\n    return y\n\ndef solve():\n    # Test cases\n    # Case 1: Stable linear decay\n    f1 = lambda t, y: -y\n    case1 = {'f': f1, 't0': 0.0, 'y0': 1.0, 'T': 5.0, \n             'atol': 1e-12, 'rtol': 1e-10, 'h0': 0.5, 'h_min': 1e-12, 'h_max': 1.0}\n\n    # Case 2: Stiff but exactly solvable\n    f2 = lambda t, y: -1000.0 * (y - np.exp(-t)) - np.exp(-t)\n    case2 = {'f': f2, 't0': 0.0, 'y0': 1.0, 'T': 1.0,\n             'atol': 1e-10, 'rtol': 1e-8, 'h0': 0.01, 'h_min': 1e-12, 'h_max': 1.0}\n\n    # Case 3: Nonlinear logistic growth\n    f3 = lambda t, y: y * (1.0 - y)\n    case3 = {'f': f3, 't0': 0.0, 'y0': 0.1, 'T': 5.0,\n             'atol': 1e-12, 'rtol': 1e-10, 'h0': 0.5, 'h_min': 1e-12, 'h_max': 1.0}\n    \n    # Case 4: Forced linear oscillator with decay\n    f4 = lambda t, y: np.cos(t) - y\n    case4 = {'f': f4, 't0': 0.0, 'y0': 0.0, 'T': 10.0,\n             'atol': 1e-12, 'rtol': 1e-10, 'h0': 0.5, 'h_min': 1e-12, 'h_max': 1.0}\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        y_T = adaptive_trapezoid_solver(**case)\n        results.append(y_T)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}