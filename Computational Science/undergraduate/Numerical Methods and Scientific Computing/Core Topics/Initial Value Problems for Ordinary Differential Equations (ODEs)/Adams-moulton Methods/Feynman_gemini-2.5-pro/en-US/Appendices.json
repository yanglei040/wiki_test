{
    "hands_on_practices": [
        {
            "introduction": "Adams-Moulton methods are implicit, meaning the unknown value appears on both sides of the equation, which can be computationally intensive to solve directly. A common and practical way to leverage their high accuracy is through a \"predictor-corrector\" scheme, where a simpler, explicit method first \"predicts\" a value, which is then refined or \"corrected\" by the Adams-Moulton formula. This exercise provides direct, hands-on experience with a classic predictor-corrector pairing, allowing you to perform a single step and get a feel for the rhythm of this elegant and efficient algorithm .",
            "id": "2152822",
            "problem": "Consider the initial value problem defined by the ordinary differential equation $y'(t) = y(t) - t^2 + 1$. Suppose we are using a numerical method with a fixed step size $h = 0.2$ to approximate the solution. We are given the first two points on the approximate solution curve: at $t_0 = 0.0$, the value is $y_0 = 0.5$, and at $t_1 = 0.2$, the value is $y_1 = 0.82930$.\n\nYour task is to compute the next value, $y_2$, at time $t_2 = 0.4$, by applying a single step of the following predictor-corrector scheme.\n\nFirst, compute a predicted value $y_{i+1}^*$ using the formula:\n$$y_{i+1}^* = y_i + \\frac{h}{2}\\left(3f(t_i, y_i) - f(t_{i-1}, y_{i-1})\\right)$$\nwhere $f(t,y) = y'(t)$.\n\nSecond, use this predicted value to compute the corrected value $y_{i+1}$ using the formula:\n$$y_{i+1} = y_i + \\frac{h}{2}\\left(f(t_{i+1}, y_{i+1}^*) + f(t_i, y_i)\\right)$$\n\nReport the final corrected value $y_2$. Round your final answer to five significant figures.",
            "solution": "We are given the ODE $y'(t)=f(t,y)=y-t^{2}+1$ and step size $h=0.2$, with known values $(t_{0},y_{0})=(0.0,0.5)$ and $(t_{1},y_{1})=(0.2,0.82930)$. We aim to compute $y_{2}$ at $t_{2}=0.4$ using the specified predictor-corrector scheme.\n\nFirst compute the needed function values at the known points:\n$$f(t_{0},y_{0})=y_{0}-t_{0}^{2}+1=0.5-0+1=1.5,$$\n$$f(t_{1},y_{1})=y_{1}-t_{1}^{2}+1=0.82930-0.04+1=1.78930.$$\n\nPredictor (Adams-Bashforth two-step):\n$$y_{2}^{*}=y_{1}+\\frac{h}{2}\\left(3f(t_{1},y_{1})-f(t_{0},y_{0})\\right)\n=0.82930+0.1\\left(3\\cdot 1.78930-1.5\\right)=0.82930+0.1\\cdot 3.86790=1.216090.$$\n\nEvaluate $f$ at the predicted point $(t_{2},y_{2}^{*})=(0.4,1.216090)$:\n$$f(t_{2},y_{2}^{*})=y_{2}^{*}-t_{2}^{2}+1=1.216090-0.16+1=2.056090.$$\n\nCorrector (trapezoidal step using the predicted endpoint value):\n$$y_{2}=y_{1}+\\frac{h}{2}\\left(f(t_{2},y_{2}^{*})+f(t_{1},y_{1})\\right)\n=0.82930+0.1\\left(2.056090+1.78930\\right)\n=0.82930+0.3845390=1.2138390.$$\n\nRounding to five significant figures yields $1.2138$.",
            "answer": "$$\\boxed{1.2138}$$"
        },
        {
            "introduction": "While a single correction is efficient, a more accurate solution to the implicit Adams-Moulton equation can be found by iterating the corrector step until it converges. This exercise moves beyond a single correction and asks you to analyze the conditions under which this iterative process succeeds or fails. By examining the fixed-point iteration for a stiff ODE, you will discover a critical limitation related to the step size $h$ and the system's properties, revealing why simple iteration is not always sufficient . This insight is crucial for understanding the need for more robust root-finding techniques, like Newton's method, in practical solvers.",
            "id": "3203180",
            "problem": "Consider the initial value problem for the scalar ordinary differential equation $y^{\\prime}(t)=f(t,y(t))$ with $f(t,y)=10\\,y+\\sin(t)$. Let $t_{n}$ and $t_{n+1}=t_{n}+h$ be two consecutive time levels with step size $h=0.3$. A predictor-corrector scheme is to be used, where the corrector is the second-order Adamsâ€“Moulton method (also known as the implicit trapezoidal rule), and the corrector equation is to be solved by fixed-point iteration of the form $y_{n+1}^{(k+1)}=g\\!\\left(y_{n+1}^{(k)}\\right)$. \n\nStarting from the fundamental identity $y(t_{n+1})=y(t_{n})+\\int_{t_{n}}^{t_{n+1}} f\\!\\left(t,y(t)\\right)\\,dt$ and approximating the integral using the trapezoidal rule applied to $f$, derive the implicit corrector equation and thereby the fixed-point map $g$ as a function of $y_{n+1}$. Using the definition of a Lipschitz constant with respect to $y_{n+1}$ for the map $g$, determine the Lipschitz constant $L$ of $g$ at this step. Conclude whether the simple fixed-point iteration is guaranteed to converge or may fail. \n\nReport only the numerical value of $L$, rounded to four significant figures.",
            "solution": "The problem requires the derivation of a fixed-point iteration scheme for the second-order Adams-Moulton method and the analysis of its convergence for a specific ordinary differential equation (ODE). The starting point is the fundamental integral identity for the solution $y(t)$ of the ODE $y'(t) = f(t, y(t))$:\n$$y(t_{n+1}) = y(t_{n}) + \\int_{t_{n}}^{t_{n+1}} f(t, y(t)) \\, dt$$\nHere, $t_n$ and $t_{n+1} = t_n + h$ are two consecutive time points, with step size $h$.\n\nThe corrector method is the second-order Adams-Moulton method, which is obtained by approximating the integral using the trapezoidal rule. The trapezoidal rule for the integral of a function $\\phi(t)$ over $[t_n, t_{n+1}]$ is given by $\\int_{t_n}^{t_{n+1}} \\phi(t) \\, dt \\approx \\frac{h}{2}[\\phi(t_n) + \\phi(t_{n+1})]$. Applying this rule to the function $f(t, y(t))$ yields:\n$$\\int_{t_{n}}^{t_{n+1}} f(t, y(t)) \\, dt \\approx \\frac{h}{2} [f(t_n, y(t_n)) + f(t_{n+1}, y(t_{n+1}))]$$\nSubstituting this approximation into the integral identity and replacing the exact solutions $y(t_n)$ with their numerical approximations $y_n$, we obtain the implicit formula for $y_{n+1}$:\n$$y_{n+1} = y_n + \\frac{h}{2} [f(t_n, y_n) + f(t_{n+1}, y_{n+1})]$$\nThis is the second-order Adams-Moulton method, also known as the implicit trapezoidal rule. The equation is implicit because the unknown quantity $y_{n+1}$ appears on both sides.\n\nTo solve this implicit equation for $y_{n+1}$, a fixed-point iteration is used, as stated in the problem. The iteration is of the form $y_{n+1}^{(k+1)} = g(y_{n+1}^{(k)})$, where $k$ is the iteration index. By inspection of the implicit equation, the fixed-point map $g$ is defined by the right-hand side. Let us use a generic variable $z$ to represent the argument of $g$ to avoid confusion. The map $g$ is:\n$$g(z) = y_n + \\frac{h}{2} [f(t_n, y_n) + f(t_{n+1}, z)]$$\nThe fixed-point iteration is then $y_{n+1}^{(k+1)} = g(y_{n+1}^{(k)})$.\n\nFor the fixed-point iteration to be guaranteed to converge to a unique solution, the map $g$ must be a contraction mapping on the domain of interest. According to the Contraction Mapping Theorem (or Banach Fixed-Point Theorem), this is true if the Lipschitz constant $L$ of $g$ is strictly less than $1$. For a continuously differentiable map $g$, the Lipschitz constant $L$ with respect to its variable $z$ can be taken as the supremum of the absolute value of its derivative:\n$$L = \\sup_{z} \\left| \\frac{\\partial g}{\\partial z} \\right|$$\nWe compute the derivative of $g(z)$ with respect to $z$. The terms $y_n$ and $f(t_n, y_n)$ are constants with respect to $z$.\n$$\\frac{\\partial g}{\\partial z} = \\frac{\\partial}{\\partial z} \\left( y_n + \\frac{h}{2} f(t_n, y_n) + \\frac{h}{2} f(t_{n+1}, z) \\right) = \\frac{h}{2} \\frac{\\partial f}{\\partial y}(t_{n+1}, z)$$\nNote that $\\frac{\\partial f}{\\partial z}$ is shorthand for the partial derivative of $f(t, y)$ with respect to its second argument $y$, evaluated at $(t_{n+1}, z)$.\n\nThe problem specifies the function $f(t, y) = 10y + \\sin(t)$. Its partial derivative with respect to $y$ is:\n$$\\frac{\\partial f}{\\partial y}(t, y) = \\frac{\\partial}{\\partial y} (10y + \\sin(t)) = 10$$\nThis partial derivative is a constant, independent of both $t$ and $y$. Therefore, its value at $(t_{n+1}, z)$ is also $10$.\n\nSubstituting this result back into the expression for the derivative of $g$:\n$$\\frac{\\partial g}{\\partial z} = \\frac{h}{2} \\times 10 = 5h$$\nThe Lipschitz constant $L$ is the supremum of the absolute value of this derivative. Since $5h$ is a constant, we have:\n$$L = |5h|$$\nThe problem provides the step size $h = 0.3$. We can now calculate the numerical value of $L$:\n$$L = |5 \\times 0.3| = |1.5| = 1.5$$\nThe condition for guaranteed convergence of the fixed-point iteration is $L  1$. In this case, $L = 1.5$, which is greater than $1$. Therefore, the map $g$ is not a contraction, and the simple fixed-point iteration is not guaranteed to converge. It is expected to diverge unless the initial guess $y_{n+1}^{(0)}$ happens to be the exact solution.\n\nThe problem asks for the numerical value of the Lipschitz constant $L$, rounded to four significant figures.\n$L = 1.5$. To express this with four significant figures, we write $1.500$.",
            "answer": "$$\n\\boxed{1.500}\n$$"
        },
        {
            "introduction": "Our final practice integrates the previous concepts into the design of a modern, adaptive ODE solver, which is the gold standard for numerical integration. Real-world solvers don't use a fixed step size; they dynamically adjust it to maintain a target accuracy with minimal computational effort. This problem guides you through implementing an algorithm that uses the predictor-corrector discrepancy as a local error estimate to control the step size $h$ . By combining this adaptive control with a robust Newton's method for the implicit corrector, you will build a complete, \"smart\" solver that mirrors the logic found in professional scientific computing libraries.",
            "id": "3203128",
            "problem": "Consider the initial value problem (IVP) for a scalar ordinary differential equation (ODE) $y'(t) = f(t, y(t))$ with initial condition $y(t_0) = y_0$. The task is to implement an implicit Adams-Moulton method with step-size control using the difference between a predictor and a corrector as a local error estimate. Use the following principle-based construction.\n\nStart from the fundamental definition that the exact solution satisfies $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt$. Approximate this integral with the trapezoidal rule to obtain the second-order Adams-Moulton (AM) corrector, which requires solving the implicit equation\n$$\ny_{n+1} = y_n + \\frac{h}{2} \\left( f(t_n, y_n) + f(t_{n+1}, y_{n+1}) \\right),\n$$\nwhere $h = t_{n+1} - t_n$. Use the forward Euler predictor\n$$\ny_{n+1}^{p} = y_n + h f(t_n, y_n),\n$$\nand then solve the AM corrector implicitly for $y_{n+1}^{c}$ at $t_{n+1}$ using a one-dimensional Newton method applied to the scalar residual\n$$\nG(y) = y - y_n - \\frac{h}{2} \\left( f(t_n, y_n) + f(t_{n+1}, y) \\right).\n$$\nLet the Newton step be defined by $y \\leftarrow y - G(y)/G'(y)$ with $G'(y) = 1 - \\frac{h}{2}\\,\\partial f/\\partial y(t_{n+1}, y)$. If the analytic derivative $\\partial f/\\partial y$ is not available, approximate it numerically.\n\nDefine the local error estimate at step $n$ by the predictor-corrector discrepancy\n$$\ne_n = \\left| y_{n+1}^{p} - y_{n+1}^{c} \\right|.\n$$\nAccept the step if $e_n \\le \\mathrm{tol}$ and reject otherwise. When a step is accepted, update the step size using the rule\n$$\nh_{\\text{new}} = \\min\\left(h_{\\max}, \\max\\left(h_{\\min}, h \\cdot \\min\\left(g_{\\max}, \\max\\left(g_{\\min}, s \\left(\\frac{\\mathrm{tol}}{e_n + \\epsilon}\\right)^{\\alpha}\\right)\\right)\\right)\\right),\n$$\nwith safety factor $s \\in (0, 1)$, small $\\epsilon  0$ to prevent division by zero, lower and upper growth limits $g_{\\min}$ and $g_{\\max}$ (with $0  g_{\\min} \\le 1 \\le g_{\\max}$), and exponent $\\alpha$ chosen according to the scaling of the error estimate. Using the forward Euler predictor with the AM trapezoidal corrector yields $e_n = \\mathcal{O}(h^2)$ for sufficiently smooth $f$, so choose $\\alpha = 1/2$.\n\nThe program must implement this variable-step algorithm, solving the IVP over the given interval and returning the final value $y(T)$ for each test case. Each Newton solve must either converge within a reasonable iteration cap or trigger step rejection with step-size reduction. Ensure that the final time $T$ is reached exactly by truncating the last step size if necessary.\n\nYour program must implement the following test suite of IVPs, each specified as a tuple $(f, \\partial f/\\partial y, y_0, t_0, T, \\mathrm{tol}, h_0, h_{\\min}, h_{\\max}, s, g_{\\min}, g_{\\max})$ with all quantities given:\n\n- Test $1$ (happy path, stable linear decay): $f(t, y) = -2 y$, $\\partial f/\\partial y(t, y) = -2$, $y_0 = 1$, $t_0 = 0$, $T = 1$, $\\mathrm{tol} = 10^{-6}$, $h_0 = 10^{-1}$, $h_{\\min} = 10^{-6}$, $h_{\\max} = 5 \\cdot 10^{-1}$, $s = 9 \\cdot 10^{-1}$, $g_{\\min} = 2 \\cdot 10^{-1}$, $g_{\\max} = 5$.\n- Test $2$ (stiff linear decay within short horizon): $f(t, y) = -50 y$, $\\partial f/\\partial y(t, y) = -50$, $y_0 = 1$, $t_0 = 0$, $T = 10^{-1}$, $\\mathrm{tol} = 10^{-5}$, $h_0 = 10^{-1}$, $h_{\\min} = 10^{-6}$, $h_{\\max} = 2 \\cdot 10^{-1}$, $s = 9 \\cdot 10^{-1}$, $g_{\\min} = 2 \\cdot 10^{-1}$, $g_{\\max} = 5$.\n- Test $3$ (non-autonomous linear with forcing): $f(t, y) = \\sin(t) - y$, $\\partial f/\\partial y(t, y) = -1$, $y_0 = 0$, $t_0 = 0$, $T = 3$, $\\mathrm{tol} = 10^{-6}$, $h_0 = 5 \\cdot 10^{-2}$, $h_{\\min} = 10^{-6}$, $h_{\\max} = 2 \\cdot 10^{-1}$, $s = 9 \\cdot 10^{-1}$, $g_{\\min} = 2 \\cdot 10^{-1}$, $g_{\\max} = 5$.\n- Test $4$ (edge case with zero right-hand side): $f(t, y) = 0$, $\\partial f/\\partial y(t, y) = 0$, $y_0 = 3.14$, $t_0 = 0$, $T = 2$, $\\mathrm{tol} = 10^{-8}$, $h_0 = 10^{-1}$, $h_{\\min} = 10^{-6}$, $h_{\\max} = 1$, $s = 9 \\cdot 10^{-1}$, $g_{\\min} = 2 \\cdot 10^{-1}$, $g_{\\max} = 5$.\n- Test $5$ (nonlinear growth away from blow-up): $f(t, y) = y^2$, $\\partial f/\\partial y(t, y) = 2 y$, $y_0 = 1/2$, $t_0 = 0$, $T = 1$, $\\mathrm{tol} = 10^{-6}$, $h_0 = 5 \\cdot 10^{-2}$, $h_{\\min} = 10^{-6}$, $h_{\\max} = 2 \\cdot 10^{-1}$, $s = 9 \\cdot 10^{-1}$, $g_{\\min} = 2 \\cdot 10^{-1}$, $g_{\\max} = 5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, in the order of the tests $1$ through $5$. Each result should be the final value $y(T)$ as a floating-point number. For example, the output should look like $[r_1,r_2,r_3,r_4,r_5]$ where each $r_i$ is a decimal floating-point representation of $y(T)$ for test $i$.",
            "solution": "The problem is valid as it presents a well-defined, scientifically-grounded task in numerical analysis. It asks for the implementation of a second-order Adams-Moulton method with adaptive step-size control to solve a set of initial value problems (IVPs). All necessary functions, parameters, and algorithmic components are specified, forming a complete and non-contradictory problem statement.\n\nThe core of the problem is to find a numerical approximation to the solution of a scalar ordinary differential equation (ODE) of the form $y'(t) = f(t, y(t))$ with a given initial condition $y(t_0) = y_0$. The exact solution satisfies the integral equation\n$$\ny(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt\n$$\nThe method specified is a predictor-corrector scheme that approximates this integral.\n\n**1. Predictor-Corrector Framework**\n\nThe algorithm is a PECE (Predict-Evaluate-Correct-Evaluate) scheme. For each step from time $t_n$ to $t_{n+1} = t_n + h$, where $h$ is the step size, the procedure is as follows:\n\n- **Predictor (P):** An explicit method is used to generate a first estimate, $y_{n+1}^p$, of the solution at $t_{n+1}$. The problem specifies the Forward Euler method:\n  $$\n  y_{n+1}^{p} = y_n + h f(t_n, y_n)\n  $$\n  The local truncation error of this predictor is $\\mathcal{O}(h^2)$.\n\n- **Corrector (C):** An implicit method is used to refine the prediction. The problem specifies the second-order Adams-Moulton (AM2) method, which is equivalent to the trapezoidal rule for quadrature:\n  $$\n  y_{n+1}^{c} = y_n + \\frac{h}{2} \\left( f(t_n, y_n) + f(t_{n+1}, y_{n+1}^{c}) \\right)\n  $$\n  This equation is implicit because the unknown $y_{n+1}^{c}$ appears on both sides. The local truncation error of this corrector is $\\mathcal{O}(h^3)$.\n\n**2. Solving the Implicit Corrector Equation**\n\nTo find $y_{n+1}^{c}$, we must solve the nonlinear algebraic equation. This is achieved using a one-dimensional Newton's method. We define a residual function $G(y)$ whose root is the desired solution $y_{n+1}^{c}$:\n$$\nG(y) = y - y_n - \\frac{h}{2} \\left( f(t_n, y_n) + f(t_{n+1}, y) \\right) = 0\n$$\nNewton's method generates a sequence of approximations $y^{(k)}$ starting from an initial guess, which is naturally the predictor value, $y^{(0)} = y_{n+1}^p$. The iterative update rule is:\n$$\ny^{(k+1)} = y^{(k)} - \\frac{G(y^{(k)})}{G'(y^{(k)})}\n$$\nThe derivative of $G(y)$ with respect to $y$ is:\n$$\nG'(y) = \\frac{d}{dy} \\left[ y - y_n - \\frac{h}{2} \\left( f(t_n, y_n) + f(t_{n+1}, y) \\right) \\right] = 1 - \\frac{h}{2} \\frac{\\partial f}{\\partial y}(t_{n+1}, y)\n$$\nThe problem provides the analytical partial derivative $\\partial f/\\partial y$ for each test case, simplifying the implementation. The Newton iteration proceeds for a fixed maximum number of steps or until the change $|y^{(k+1)} - y^{(k)}|$ falls below a small tolerance. If convergence is not achieved, the step is considered to have failed.\n\n**3. Adaptive Step-Size Control**\n\nA key feature of the algorithm is its ability to adjust the step-size $h$ to maintain the local error below a specified tolerance, $\\mathrm{tol}$.\n\n- **Local Error Estimation:** The difference between the predictor and corrector values serves as a proxy for the local error. The error estimate $e_n$ for the step from $t_n$ to $t_{n+1}$ is:\n  $$\n  e_n = \\left| y_{n+1}^{p} - y_{n+1}^{c} \\right|\n  $$\n  Since the predictor's local error is $\\mathcal{O}(h^2)$ and the corrector's is $\\mathcal{O}(h^3)$, their difference is dominated by the lower-order term, so $e_n = \\mathcal{O}(h^2)$.\n\n- **Step Acceptance/Rejection:** After computing $y_{n+1}^c$ and $e_n$, the step is accepted if $e_n \\le \\mathrm{tol}$. If accepted, the state is updated: $t_{n+1} = t_n + h$ and $y_{n+1} = y_{n+1}^c$. If $e_n  \\mathrm{tol}$ or if the Newton's method failed to converge, the step is rejected, and the state $(t_n, y_n)$ is retained.\n\n- **Step-Size Update:** A new step size, $h_{\\text{new}}$, is computed after every attempted step (whether accepted or rejected). The goal is to choose a step size for which the error would be approximately equal to $\\mathrm{tol}$. Since $e_n \\approx C h^2$ for some constant $C$, the ideal step size $h_{\\text{opt}}$ would satisfy $\\mathrm{tol} \\approx C h_{\\text{opt}}^2$. This implies $h_{\\text{opt}} \\approx h \\sqrt{\\mathrm{tol}/e_n}$. The problem provides a more robust and practical formula, including a safety factor $s$, growth/shrink limits $g_{\\min}$ and $g_{\\max}$, and absolute bounds $h_{\\min}$ and $h_{\\max}$:\n  $$\n  h_{\\text{new}} = \\min\\left(h_{\\max}, \\max\\left(h_{\\min}, h \\cdot \\min\\left(g_{\\max}, \\max\\left(g_{\\min}, s \\left(\\frac{\\mathrm{tol}}{e_n + \\epsilon}\\right)^{\\alpha}\\right)\\right)\\right)\\right)\n  $$\n  Here, $\\alpha = 1/2$ is chosen to match the error order $e_n = \\mathcal{O}(h^2)$. The small constant $\\epsilon  0$ prevents division by zero if $e_n=0$.\n\n**4. Overall Algorithm**\n\nThe complete algorithm to integrate from $t_0$ to $T$ is:\n\n$1$. Initialize $t \\leftarrow t_0$, $y \\leftarrow y_0$, and $h \\leftarrow h_0$.\n$2$. Begin a loop that continues as long as $t  T$.\n$3$. At the start of each step, if $t+h  T$, set $h = T-t$ to ensure the final step lands exactly on $T$.\n$4$. Calculate the predictor $y_{n+1}^p = y + h f(t, y)$.\n$5$. Use Newton's method to solve for the corrector $y_{n+1}^c$, starting with $y_{n+1}^p$.\n$6$. If Newton's method fails:\n    a. The step is rejected.\n    b. The step size is aggressively reduced, for instance, $h \\leftarrow \\max(h_{\\min}, h \\cdot g_{\\min})$.\n    c. Go back to step $3$ to retry the step with the smaller $h$.\n$7$. If Newton's method converges:\n    a. Compute the error estimate $e_n = |y_{n+1}^p - y_{n+1}^c|$.\n    b. Calculate the proposed new step size $h_{\\text{new}}$ using the full-control formula.\n    c. If $e_n \\le \\mathrm{tol}$:\n        i. The step is accepted. Update state: $t \\leftarrow t+h$, $y \\leftarrow y_{n+1}^c$.\n        ii. Update the step size for the next attempt: $h \\leftarrow h_{\\text{new}}$.\n    d. If $e_n  \\mathrm{tol}$:\n        i. The step is rejected. The state $(t, y)$ is not changed.\n        ii. Update the step size for the next attempt: $h \\leftarrow h_{\\text{new}}$.\n        iii. Go back to step $3$ to retry the step.\n$8$. End loop when $t \\ge T$. The final value of $y$ is the result $y(T)$.\nThis procedure is implemented for each of the five test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef adaptive_am2_solver(f, dfdy, y0, t0, T, tol, h0, h_min, h_max, s, g_min, g_max):\n    \"\"\"\n    Solves an IVP using a variable-step Adams-Moulton (2nd order) method.\n\n    This function implements a predictor-corrector scheme with:\n    - Predictor: Forward Euler method.\n    - Corrector: Adams-Moulton 2nd order (trapezoidal rule), solved with Newton's method.\n    - Step-size control: Based on the difference between predictor and corrector.\n    \"\"\"\n    t = t0\n    y = y0\n    h = h0\n\n    # Constants for the algorithm\n    alpha = 0.5  # For error estimate of order O(h^2)\n    epsilon = np.finfo(float).eps  # Small number to avoid division by zero\n    newton_max_iter = 10\n    newton_tol = tol * 1e-2 # Convergence tolerance for Newton's method\n\n    while t  T:\n        # Ensure the last step hits T exactly\n        if t + h > T:\n            h = T - t\n        \n        t_next = t + h\n\n        # --- Step Attempt ---\n        f_n = f(t, y)\n\n        # 1. Predictor (Forward Euler)\n        y_p = y + h * f_n\n\n        # 2. Corrector (Adams-Moulton 2nd order via Newton's method)\n        y_c = y_p  # Initial guess for Newton's method\n        newton_converged = False\n        for _ in range(newton_max_iter):\n            f_next_c = f(t_next, y_c)\n            \n            # G(y) = y - y_n - (h/2)*(f_n + f(t_{n+1}, y))\n            G = y_c - y - (h / 2.0) * (f_n + f_next_c)\n            \n            # G'(y) = 1 - (h/2)*df/dy(t_{n+1}, y)\n            dfdy_val = dfdy(t_next, y_c)\n            Gp = 1.0 - (h / 2.0) * dfdy_val\n            \n            if abs(Gp)  epsilon:\n                # Newton's method fails if derivative is near zero\n                break \n\n            y_c_new = y_c - G / Gp\n            \n            if abs(y_c_new - y_c)  newton_tol:\n                y_c = y_c_new\n                newton_converged = True\n                break\n            \n            y_c = y_c_new\n\n        # 3. Step control\n        if not newton_converged:\n            # Newton solver failed, reject step and reduce step size aggressively\n            h = max(h_min, h * g_min)\n            continue # Retry step with new h\n\n        # Newton converged, now check the error estimate\n        error_est = abs(y_p - y_c)\n\n        # Calculate optimal step size factor\n        if error_est  epsilon: # Avoid division by zero, error is very small\n            factor = g_max\n        else:\n            factor = s * (tol / error_est)**alpha\n\n        # Limit step size change\n        factor = min(g_max, max(g_min, factor))\n        h_new = h * factor\n        h_new = min(h_max, max(h_min, h_new))\n        \n        if error_est = tol:\n            # Step accepted\n            t = t_next\n            y = y_c\n            h = h_new\n        else:\n            # Step rejected\n            h = h_new # Use the new, smaller h for the next attempt\n            # Do not update t and y, retry the same step\n\n    return y\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the IVP solver.\n    \"\"\"\n    test_cases = [\n        # Test 1: stable linear decay\n        (lambda t, y: -2.0 * y, lambda t, y: -2.0, 1.0, 0.0, 1.0, 1e-6, 1e-1, 1e-6, 5e-1, 0.9, 0.2, 5.0),\n        # Test 2: stiff linear decay\n        (lambda t, y: -50.0 * y, lambda t, y: -50.0, 1.0, 0.0, 0.1, 1e-5, 1e-1, 1e-6, 2e-1, 0.9, 0.2, 5.0),\n        # Test 3: non-autonomous linear\n        (lambda t, y: math.sin(t) - y, lambda t, y: -1.0, 0.0, 0.0, 3.0, 1e-6, 5e-2, 1e-6, 2e-1, 0.9, 0.2, 5.0),\n        # Test 4: zero RHS\n        (lambda t, y: 0.0, lambda t, y: 0.0, 3.14, 0.0, 2.0, 1e-8, 1e-1, 1e-6, 1.0, 0.9, 0.2, 5.0),\n        # Test 5: nonlinear growth\n        (lambda t, y: y**2, lambda t, y: 2.0 * y, 0.5, 0.0, 1.0, 1e-6, 5e-2, 1e-6, 2e-1, 0.9, 0.2, 5.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        f, dfdy, y0, t0, T, tol, h0, h_min, h_max, s, g_min, g_max = case\n        final_y = adaptive_am2_solver(f, dfdy, y0, t0, T, tol, h0, h_min, h_max, s, g_min, g_max)\n        results.append(f\"{final_y:.15g}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}