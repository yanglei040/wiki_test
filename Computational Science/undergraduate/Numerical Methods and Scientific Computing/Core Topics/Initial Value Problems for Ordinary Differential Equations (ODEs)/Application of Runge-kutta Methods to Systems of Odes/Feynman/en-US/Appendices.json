{
    "hands_on_practices": [
        {
            "introduction": "Choosing the right numerical method often involves a fundamental trade-off between computational cost and accuracy. Higher-order methods typically offer greater accuracy for a given step size but require more calculations per step. This practice provides a concrete, quantitative exploration of this trade-off by tasking you with comparing the performance of second, third, and fourth-order Runge-Kutta methods on a simple linear system . By analyzing the relationship between the number of function evaluations and the resulting global error, you will build essential intuition for making practical decisions when selecting a solver.",
            "id": "3205643",
            "problem": "You are to investigate the trade-off between computational cost and accuracy when applying fixed-step explicit Runge–Kutta methods to a coupled, non-stiff, linear system of ordinary differential equations (ODEs). Consider the initial value problem for the system\n$$\n\\begin{cases}\n\\dfrac{d y_1}{d t} = - y_1 \\\\[4pt]\n\\dfrac{d y_2}{d t} = y_1 - 2 y_2 \\\\[4pt]\n\\dfrac{d y_3}{d t} = y_2 - 3 y_3\n\\end{cases}\n\\quad t \\in [0,T], \\quad\ny(0) = \\begin{bmatrix} y_1(0) \\\\ y_2(0) \\\\ y_3(0) \\end{bmatrix}\n= \\begin{bmatrix} 1.3 \\\\ -0.9 \\\\ 0.7 \\end{bmatrix},\n\\quad T = 4.0.\n$$\n\nYour task is to:\n- Implement three explicit Runge–Kutta methods for systems of ODEs:\n  - the second-order method (explicit midpoint),\n  - the third-order method (classic Kutta of order three),\n  - the fourth-order method (classical Runge–Kutta of order four).\n- Use a fixed time step for each simulation and propagate the numerical solution from $t=0$ to $t=T$.\n- Define the computational cost for a run as the total number of function evaluations (right-hand side evaluations of the ODE system). For an $s$-stage Runge–Kutta method with $N$ steps, the cost is $s \\cdot N$.\n- Measure the accuracy as the Euclidean norm of the global error at $t=T$, namely $\\lVert y_{\\text{num}}(T) - y_{\\text{true}}(T) \\rVert_2$.\n\nFor error evaluation, use the exact solution obtained from first principles for this triangular linear system. Let $a = y_1(0)$, $b = y_2(0)$, and $c = y_3(0)$. The exact solution components at time $t$ are:\n$$\ny_1(t) = a e^{-t},\n$$\n$$\ny_2(t) = a e^{-t} + (b - a) e^{-2t},\n$$\n$$\ny_3(t) = \\frac{a}{2} e^{-t} + (b - a) e^{-2t} + \\left(c - b + \\frac{a}{2}\\right) e^{-3t}.\n$$\n\nUse the following test suite to explore the cost-versus-accuracy behavior. For each method, simulate with $N \\in \\{1, 2, 5, 10, 20, 40\\}$ uniform steps over $[0, T]$:\n- For the second-order method (explicit midpoint), use $N \\in \\{1, 2, 5, 10, 20, 40\\}$.\n- For the third-order method (classic Kutta of order three), use $N \\in \\{1, 2, 5, 10, 20, 40\\}$.\n- For the fourth-order method (classical), use $N \\in \\{1, 2, 5, 10, 20, 40\\}$.\n\nFor each run, record the pair $[m, e]$ where $m$ is the total number of function evaluations and $e$ is the Euclidean norm of the global error at $t=T$. The final program output must aggregate all results into a single list in the following order:\n- First list all results for the second-order method in the order of $N$ given above,\n- then list all results for the third-order method in the order of $N$ given above,\n- then list all results for the fourth-order method in the order of $N$ given above.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[ [m_1,e_1], [m_2,e_2], \\dots ]$). There are no physical units involved in this problem, and all angles, if any appear, should be treated as dimensionless real numbers. All numerical answers must be reported as raw decimal numbers in the output list without additional text.",
            "solution": "The user's request is to analyze the computational cost versus accuracy for three different explicit Runge-Kutta (RK) methods applied to a system of linear ordinary differential equations (ODEs). The problem is well-defined, scientifically sound, and self-contained, providing the ODE system, initial conditions, time interval, specific numerical methods, definitions for cost and error, and the exact analytical solution for verification. The problem is therefore deemed **valid**.\n\nThe core of the problem lies in the numerical integration of an initial value problem (IVP) of the form:\n$$\n\\frac{d\\mathbf{y}}{dt} = f(t, \\mathbf{y}), \\quad \\mathbf{y}(t_0) = \\mathbf{y}_0\n$$\nwhere $\\mathbf{y}(t) \\in \\mathbb{R}^3$. The given system is:\n$$\nf(t, \\mathbf{y}) = \\begin{bmatrix} -y_1 \\\\ y_1 - 2y_2 \\\\ y_2 - 3y_3 \\end{bmatrix}\n$$\nwith initial condition $\\mathbf{y}(0) = [1.3, -0.9, 0.7]^T$ over the interval $t \\in [0, 4.0]$.\n\nAn $s$-stage explicit Runge-Kutta method advances the solution from time $t_n$ to $t_{n+1} = t_n + h$ using the following general formula:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + h \\sum_{i=1}^s b_i \\mathbf{k}_i\n$$\nwhere the stages $\\mathbf{k}_i$ are computed sequentially:\n$$\n\\mathbf{k}_i = f\\left(t_n + c_i h, \\mathbf{y}_n + h \\sum_{j=1}^{i-1} a_{ij} \\mathbf{k}_j\\right)\n$$\nEach calculation of a $\\mathbf{k}_i$ constitutes one evaluation of the function $f$. Thus, for a single time step, an $s$-stage method requires $s$ function evaluations. Over $N$ steps, the total number of function evaluations, defined as the computational cost $m$, is $m = s \\cdot N$.\n\nThe accuracy is measured by the Euclidean norm of the global error at the final time $T$, $e = \\lVert \\mathbf{y}_{\\text{num}}(T) - \\mathbf{y}_{\\text{true}}(T) \\rVert_2$.\n\nThe three specified methods are:\n\n1.  **Second-Order Method (Explicit Midpoint Rule)**: This is a $2$-stage method ($s=2$) of order $p=2$. Its update formulas are:\n    $$\n    \\begin{align*}\n    \\mathbf{k}_1 = f(t_n, \\mathbf{y}_n) \\\\\n    \\mathbf{k}_2 = f\\left(t_n + \\frac{1}{2}h, \\mathbf{y}_n + \\frac{1}{2}h\\mathbf{k}_1\\right) \\\\\n    \\mathbf{y}_{n+1} = \\mathbf{y}_n + h\\mathbf{k}_2\n    \\end{align*}\n    $$\n\n2.  **Third-Order Method (Classic Kutta)**: This is a $3$-stage method ($s=3$) of order $p=3$. The specific variant used is:\n    $$\n    \\begin{align*}\n    \\mathbf{k}_1 = f(t_n, \\mathbf{y}_n) \\\\\n    \\mathbf{k}_2 = f\\left(t_n + \\frac{1}{2}h, \\mathbf{y}_n + \\frac{1}{2}h\\mathbf{k}_1\\right) \\\\\n    \\mathbf{k}_3 = f\\left(t_n + h, \\mathbf{y}_n - h\\mathbf{k}_1 + 2h\\mathbf{k}_2\\right) \\\\\n    \\mathbf{y}_{n+1} = \\mathbf{y}_n + \\frac{h}{6}(\\mathbf{k}_1 + 4\\mathbf{k}_2 + \\mathbf{k}_3)\n    \\end{align*}\n    $$\n\n3.  **Fourth-Order Method (Classical RK4)**: This is a $4$-stage method ($s=4$) of order $p=4$. Its formulas are:\n    $$\n    \\begin{align*}\n    \\mathbf{k}_1 = f(t_n, \\mathbf{y}_n) \\\\\n    \\mathbf{k}_2 = f\\left(t_n + \\frac{1}{2}h, \\mathbf{y}_n + \\frac{1}{2}h\\mathbf{k}_1\\right) \\\\\n    \\mathbf{k}_3 = f\\left(t_n + \\frac{1}{2}h, \\mathbf{y}_n + \\frac{1}{2}h\\mathbf{k}_2\\right) \\\\\n    \\mathbf{k}_4 = f\\left(t_n + h, \\mathbf{y}_n + h\\mathbf{k}_3\\right) \\\\\n    \\mathbf{y}_{n+1} = \\mathbf{y}_n + \\frac{h}{6}(\\mathbf{k}_1 + 2\\mathbf{k}_2 + 2\\mathbf{k}_3 + \\mathbf{k}_4)\n    \\end{align*}\n    $$\n\nThe analytical solution, required for error calculation, is provided and can be implemented as a function of time $t$. With $a=y_1(0)$, $b=y_2(0)$, and $c=y_3(0)$:\n$$\n\\begin{align*}\ny_1(t) = a e^{-t} \\\\\ny_2(t) = a e^{-t} + (b - a) e^{-2t} \\\\\ny_3(t) = \\frac{a}{2} e^{-t} + (b - a) e^{-2t} + \\left(c - b + \\frac{a}{2}\\right) e^{-3t}\n\\end{align*}\n$$\n\nThe solution is implemented by creating a general-purpose solver function that accepts a stepper function (one of the RK methods) as an argument. A main loop iterates through each of the three methods and, for each method, through the specified list of step counts $N \\in \\{1, 2, 5, 10, 20, 40\\}$. For each run, the fixed time step is $h = T/N$. The solver propagates the solution from $t=0$ to $t=T$. Finally, the cost $m$ and error $e$ are calculated and stored. The results from all runs are aggregated into a single list as specified. It is important to note that for explicit methods, the step size $h$ must be small enough to stay within the region of absolute stability. The eigenvalues of the system matrix are $\\lambda = -1, -2, -3$. For the most restrictive eigenvalue $\\lambda = -3$, stability requires $h  2/3$. Thus, simulations with large step sizes ($N=1, h=4.0$; $N=2, h=2.0$; $N=5, h=0.8$) are expected to be unstable and yield very large errors. This behavior is part of the analysis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ODE system using three Runge-Kutta methods and calculates\n    the cost-vs-accuracy trade-off for various step counts.\n    \"\"\"\n    \n    # Problem parameters\n    T = 4.0\n    y0 = np.array([1.3, -0.9, 0.7])\n    a, b, c = y0[0], y0[1], y0[2]\n    N_values = [1, 2, 5, 10, 20, 40]\n\n    # Define the ODE system dy/dt = f(t, y)\n    def f(t, y):\n        \"\"\"\n        Right-hand side of the ODE system.\n        \"\"\"\n        # The 't' argument is unused as the system is autonomous, but it is\n        # included for compatibility with general-purpose ODE solver signatures.\n        dydt = np.zeros_like(y, dtype=float)\n        dydt[0] = -y[0]\n        dydt[1] = y[0] - 2.0 * y[1]\n        dydt[2] = y[1] - 3.0 * y[2]\n        return dydt\n\n    # Define the exact analytical solution\n    def y_true_func(t):\n        \"\"\"\n        Calculates the exact solution at a given time t.\n        \"\"\"\n        exp_t = np.exp(-t)\n        exp_2t = np.exp(-2.0 * t)\n        exp_3t = np.exp(-3.0 * t)\n        \n        y1_t = a * exp_t\n        y2_t = a * exp_t + (b - a) * exp_2t\n        y3_t = (a / 2.0) * exp_t + (b - a) * exp_2t + (c - b + a / 2.0) * exp_3t\n        \n        return np.array([y1_t, y2_t, y3_t])\n\n    # --- Runge-Kutta Stepper Functions ---\n    # Each stepper function advances the solution by one time step h.\n    \n    def rk2_step(f_func, t, y, h):\n        \"\"\"Explicit Midpoint (2 stages, 2nd order)\"\"\"\n        k1 = f_func(t, y)\n        k2 = f_func(t + 0.5 * h, y + 0.5 * h * k1)\n        return y + h * k2\n\n    def rk3_step(f_func, t, y, h):\n        \"\"\"Classic Kutta (3 stages, 3rd order)\"\"\"\n        k1 = f_func(t, y)\n        k2 = f_func(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f_func(t + h, y - h * k1 + 2.0 * h * k2)\n        return y + (h / 6.0) * (k1 + 4.0 * k2 + k3)\n\n    def rk4_step(f_func, t, y, h):\n        \"\"\"Classical Runge-Kutta (4 stages, 4th order)\"\"\"\n        k1 = f_func(t, y)\n        k2 = f_func(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f_func(t + 0.5 * h, y + 0.5 * h * k2)\n        k4 = f_func(t + h, y + h * k3)\n        return y + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\n    # General-purpose ODE solver for a fixed number of steps\n    def solve_ivp(stepper, f_func, y_initial, t_final, n_steps):\n        h = t_final / n_steps\n        y = y_initial.copy()\n        t = 0.0\n        for _ in range(n_steps):\n            y = stepper(f_func, t, y, h)\n            t += h\n        return y\n\n    # --- Main Calculation Loop ---\n    all_results = []\n    \n    methods = [\n        (rk2_step, 2),  # (stepper_function, number_of_stages)\n        (rk3_step, 3),\n        (rk4_step, 4)\n    ]\n    \n    y_exact_at_T = y_true_func(T)\n    \n    for stepper, s in methods:\n        for N in N_values:\n            # Calculate numerical solution at T\n            y_num_at_T = solve_ivp(stepper, f, y0, T, N)\n            \n            # Calculate Euclidean norm of the global error\n            error = np.linalg.norm(y_num_at_T - y_exact_at_T)\n            \n            # Calculate computational cost (total function evaluations)\n            cost = s * N\n            \n            all_results.append([cost, error])\n\n    # Format the results into a single string `[[m1,e1],[m2,e2],...]`\n    # This manual formatting ensures no spaces are included.\n    string_parts = [f\"[{m},{e}]\" for m, e in all_results]\n    final_output = f\"[{','.join(string_parts)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Many physical, chemical, and biological systems are defined by conservation laws, such as the conservation of energy, mass, or, in this case, total population. While the exact differential equations preserve these invariants perfectly, standard numerical methods like RK4 can accumulate small errors at each step, causing the numerical solution to \"drift\" away from this fundamental property over long simulations. This exercise introduces the powerful concept of structure-preserving integration, where you will modify the standard RK4 method with a minimal correction to enforce a linear invariant—the total population in an SIR model—exactly at each step .",
            "id": "3205637",
            "problem": "Consider a system of ordinary differential equations (ODEs) of the form $y'(t) = f(t, y(t))$, where $y(t) \\in \\mathbb{R}^m$ and $f : \\mathbb{R} \\times \\mathbb{R}^m \\to \\mathbb{R}^m$ is sufficiently smooth. A linear invariant is a quantity of the form $c^\\top y(t)$, where $c \\in \\mathbb{R}^m$ is a fixed vector, such that $\\frac{d}{dt}(c^\\top y(t)) = 0$ along exact solutions. The Susceptible-Infectious-Recovered (SIR) model is a well-known epidemiological system with state $y(t) = [S(t), I(t), R(t)]^\\top$ and parameters $\\beta  0$ (infection rate) and $\\gamma  0$ (recovery rate), governed by the ODEs\n$$\n\\frac{dS}{dt} = -\\beta S I, \\quad \\frac{dI}{dt} = \\beta S I - \\gamma I, \\quad \\frac{dR}{dt} = \\gamma I,\n$$\nfor which the total population $S(t) + I(t) + R(t)$ is conserved exactly, that is, $S(t) + I(t) + R(t) = S(0) + I(0) + R(0)$ for all $t \\ge 0$. In this case, the invariant is $c^\\top y = S + I + R$ with $c = [1, 1, 1]^\\top$.\n\nThe classical fourth-order explicit Runge–Kutta method (RK4) advances the numerical state from $y_n \\approx y(t_n)$ to $y_{n+1} \\approx y(t_{n+1})$ using four intermediate slope evaluations. In general, a non-symplectic explicit method such as RK4 does not guarantee exact preservation of linear invariants when applied to nonlinear systems, even when the exact ODE solution does preserve them. However, after computing a standard RK4 step producing an intermediate value $y_{n+1}^\\ast$, one can apply a minimal correction to $y_{n+1}^\\ast$ to enforce the linear invariant $c^\\top y = \\text{constant}$ exactly at the discrete level.\n\nYour task is to:\n- Implement the standard RK4 method for the SIR system described above.\n- Derive and implement a single-scalar correction applied after each RK4 step that enforces the invariant $c^\\top y = c^\\top y_0$ exactly at every discrete step (with $c = [1, 1, 1]^\\top$). The correction should minimally change $y_{n+1}^\\ast$ in the Euclidean norm while enforcing $c^\\top y_{n+1} = c^\\top y_n$ at each step.\n- For each test case below, run both the standard RK4 method and the invariant-corrected RK4 method over a specified time interval using a fixed time step. For each method and test case, report the maximum absolute deviation of $S(t_n) + I(t_n) + R(t_n)$ from the initial value $S(0) + I(0) + R(0)$ measured over all discrete times $t_n$ computed by the method. Because the variables $S$, $I$, and $R$ represent fractions of a population, they are dimensionless; all reported deviations must therefore be dimensionless real numbers.\n- Your program must produce a single line containing the results for all test cases in the specified order and format.\n\nUse the following test suite, expressed as tuples $(\\beta, \\gamma, S_0, I_0, R_0, h, T)$, where $h$ is the fixed time step and $T$ is the final time:\n1. Test case A (general case): $(\\beta, \\gamma, S_0, I_0, R_0, h, T) = (0.3, 0.1, 0.99, 0.01, 0.0, 0.5, 100.0)$.\n2. Test case B (boundary condition with zero infection rate): $(\\beta, \\gamma, S_0, I_0, R_0, h, T) = (0.0, 0.2, 0.6, 0.4, 0.0, 1.0, 50.0)$.\n3. Test case C (coarse time step stress test): $(\\beta, \\gamma, S_0, I_0, R_0, h, T) = (0.5, 0.1, 0.7, 0.3, 0.0, 5.0, 50.0)$.\n4. Test case D (edge case with no initial infection): $(\\beta, \\gamma, S_0, I_0, R_0, h, T) = (0.3, 0.1, 1.0, 0.0, 0.0, 0.1, 40.0)$.\n\nFor each test case, compute two values: the maximum absolute invariant deviation for the standard RK4 method and for the invariant-corrected RK4 method. Your program should produce a single line of output containing these results as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\text{A\\_RK4}, \\text{A\\_corr}, \\text{B\\_RK4}, \\text{B\\_corr}, \\text{C\\_RK4}, \\text{C\\_corr}, \\text{D\\_RK4}, \\text{D\\_corr}],\n$$\nwhere each entry is a floating-point number.",
            "solution": "The problem requires the implementation and comparison of two numerical schemes for solving the Susceptible-Infectious-Recovered (SIR) model: the standard fourth-order Runge-Kutta method (RK4) and a modified RK4 method that enforces the conservation of total population. The goal is to quantify how much the non-corrected method deviates from this conservation law, versus the corrected method which is designed to preserve it exactly.\n\nFirst, we define the system of ordinary differential equations (ODEs) for the SIR model. Let the state vector be $y(t) = [S(t), I(t), R(t)]^\\top$, representing the susceptible, infectious, and recovered fractions of the population. The dynamics are governed by:\n$$\n\\frac{d}{dt}y(t) = \n\\begin{pmatrix} dS/dt \\\\ dI/dt \\\\ dR/dt \\end{pmatrix} = \n\\begin{pmatrix} -\\beta S I \\\\ \\beta S I - \\gamma I \\\\ \\gamma I \\end{pmatrix}\n= f(y(t))\n$$\nwhere $\\beta  0$ is the infection rate and $\\gamma  0$ is the recovery rate.\n\nThis system possesses a linear invariant. The sum of the components represents the total population, which is conserved. We can verify this by summing the derivatives:\n$$\n\\frac{d}{dt}(S(t) + I(t) + R(t)) = \\frac{dS}{dt} + \\frac{dI}{dt} + \\frac{dR}{dt} = (-\\beta S I) + (\\beta S I - \\gamma I) + (\\gamma I) = 0\n$$\nThis implies that $S(t) + I(t) + R(t) = \\text{constant}$ for all time $t \\ge 0$. The constant is determined by the initial conditions: $S(t) + I(t) + R(t) = S(0) + I(0) + R(0)$. In vector notation, this invariant is expressed as $c^\\top y(t) = c^\\top y(0)$, where $c = [1, 1, 1]^\\top$.\n\nThe standard fourth-order explicit Runge-Kutta (RK4) method is used to approximate the solution numerically. Given a state $y_n$ at time $t_n$, the state $y_{n+1}$ at time $t_{n+1} = t_n + h$ is computed via four intermediate stages:\n$$\n\\begin{aligned}\nk_1 = f(y_n) \\\\\nk_2 = f\\left(y_n + \\frac{h}{2} k_1\\right) \\\\\nk_3 = f\\left(y_n + \\frac{h}{2} k_2\\right) \\\\\nk_4 = f(y_n + h k_3)\n\\end{aligned}\n$$\nThe next state, which we denote $y_{n+1}^\\ast$ for the standard uncorrected step, is then given by:\n$$\ny_{n+1}^\\ast = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n$$\nFor nonlinear systems like the SIR model, this method does not in general preserve the linear invariant, meaning $c^\\top y_{n+1}^\\ast \\neq c^\\top y_n$. The error arises because the function $f$ is evaluated at intermediate points that do not necessarily satisfy the invariant.\n\nTo enforce the invariant, we seek a minimal correction $\\delta \\in \\mathbb{R}^3$ to the provisional state $y_{n+1}^\\ast$ such that the corrected state $y_{n+1} = y_{n+1}^\\ast + \\delta$ satisfies the invariant exactly with respect to the initial total population, $C_0 = c^\\top y_0$. The problem is to find $\\delta$ that minimizes its Euclidean norm, $\\|\\delta\\|_2$, subject to the constraint $c^\\top(y_{n+1}^\\ast + \\delta) = C_0$. This is a constrained optimization problem.\n\nWe use the method of Lagrange multipliers. The objective is to minimize $\\frac{1}{2}\\|\\delta\\|_2^2 = \\frac{1}{2}\\delta^\\top\\delta$. The Lagrangian is:\n$$\n\\mathcal{L}(\\delta, \\lambda) = \\frac{1}{2}\\delta^\\top\\delta + \\lambda \\left( c^\\top(y_{n+1}^\\ast + \\delta) - C_0 \\right)\n$$\nTo find the minimum, we set the gradient with respect to $\\delta$ to zero:\n$$\n\\nabla_\\delta \\mathcal{L} = \\delta + \\lambda c = 0 \\implies \\delta = -\\lambda c\n$$\nThe optimal correction $\\delta$ is proportional to the vector $c$. We find the scalar Lagrange multiplier $\\lambda$ by substituting this back into the constraint equation:\n$$\nc^\\top(y_{n+1}^\\ast - \\lambda c) = C_0 \\implies c^\\top y_{n+1}^\\ast - \\lambda (c^\\top c) = C_0\n$$\nSolving for $\\lambda$ gives:\n$$\n\\lambda = \\frac{c^\\top y_{n+1}^\\ast - C_0}{c^\\top c}\n$$\nIn our specific case, $c = [1, 1, 1]^\\top$, so $c^\\top c = 1^2 + 1^2 + 1^2 = 3$. The initial invariant value is $C_0 = S_0 + I_0 + R_0$. The provisional sum after the RK4 step is $c^\\top y_{n+1}^\\ast = S_{n+1}^\\ast + I_{n+1}^\\ast + R_{n+1}^\\ast$. Thus, the scalar $\\lambda$ is:\n$$\n\\lambda = \\frac{(S_{n+1}^\\ast + I_{n+1}^\\ast + R_{n+1}^\\ast) - (S_0 + I_0 + R_0)}{3}\n$$\nThe corrected state $y_{n+1} = y_{n+1}^\\ast - \\lambda c$ is then computed component-wise:\n$$\n\\begin{aligned}\nS_{n+1} = S_{n+1}^\\ast - \\lambda \\\\\nI_{n+1} = I_{n+1}^\\ast - \\lambda \\\\\nR_{n+1} = R_{n+1}^\\ast - \\lambda\n\\end{aligned}\n$$\nBy construction, the sum $S_{n+1} + I_{n+1} + R_{n+1}$ will be exactly equal to $S_0 + I_0 + R_0$, up to floating-point precision limitations.\n\nFor each test case, we perform two simulations over the time interval $[0, T]$ with a fixed step size $h$. One simulation uses the standard RK4 method, and the other uses the corrected RK4 method. In each simulation, we track the maximum absolute deviation of the invariant, $\\max_n |(S_n + I_n + R_n) - (S_0 + I_0 + R_0)|$, over all computed steps.\n\nSpecial attention is given to test cases B and D.\nFor case B, where $\\beta=0$, the SIR system becomes a linear ODE system. Explicit Runge-Kutta methods are known to preserve linear invariants of linear ODE systems, so the deviation for standard RK4 is expected to be near machine precision.\nFor case D, where $I_0=0$, the initial state is an equilibrium point of the ODE system. The derivatives are all zero, so the exact solution is constant. The RK4 method will exactly reproduce this, resulting in zero deviation for both methods.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares standard and invariant-corrected RK4 methods for the SIR model.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (beta, gamma, S0, I0, R0, h, T)\n        (0.3, 0.1, 0.99, 0.01, 0.0, 0.5, 100.0),  # Case A\n        (0.0, 0.2, 0.6, 0.4, 0.0, 1.0, 50.0),   # Case B\n        (0.5, 0.1, 0.7, 0.3, 0.0, 5.0, 50.0),   # Case C\n        (0.3, 0.1, 1.0, 0.0, 0.0, 0.1, 40.0),    # Case D\n    ]\n\n    def sir_ode(y, beta, gamma):\n        \"\"\"\n        Defines the SIR system of ODEs.\n        y: state vector [S, I, R]\n        beta: infection rate\n        gamma: recovery rate\n        \"\"\"\n        S, I, R = y\n        dS_dt = -beta * S * I\n        dI_dt = beta * S * I - gamma * I\n        dR_dt = gamma * I\n        return np.array([dS_dt, dI_dt, dR_dt])\n\n    results = []\n    \n    for case in test_cases:\n        beta, gamma, S0, I0, R0, h, T = case\n        \n        y0 = np.array([S0, I0, R0])\n        inv_0 = np.sum(y0)\n        num_steps = int(round(T / h))\n\n        # 1. Standard RK4 Simulation\n        y_rk4 = np.copy(y0)\n        max_dev_rk4 = 0.0\n        for _ in range(num_steps):\n            k1 = sir_ode(y_rk4, beta, gamma)\n            k2 = sir_ode(y_rk4 + 0.5 * h * k1, beta, gamma)\n            k3 = sir_ode(y_rk4 + 0.5 * h * k2, beta, gamma)\n            k4 = sir_ode(y_rk4 + h * k3, beta, gamma)\n            y_rk4 += (h / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n            \n            current_inv = np.sum(y_rk4)\n            max_dev_rk4 = max(max_dev_rk4, abs(current_inv - inv_0))\n        \n        results.append(max_dev_rk4)\n\n        # 2. Invariant-Corrected RK4 Simulation\n        y_corr = np.copy(y0)\n        max_dev_corr = 0.0\n        c = np.array([1.0, 1.0, 1.0])\n        c_dot_c = np.dot(c, c)\n        \n        for _ in range(num_steps):\n            k1 = sir_ode(y_corr, beta, gamma)\n            k2 = sir_ode(y_corr + 0.5 * h * k1, beta, gamma)\n            k3 = sir_ode(y_corr + 0.5 * h * k2, beta, gamma)\n            k4 = sir_ode(y_corr + h * k3, beta, gamma)\n            y_star = y_corr + (h / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n            \n            # Apply the minimal correction\n            inv_star = np.sum(y_star)\n            lambda_corr = (inv_star - inv_0) / c_dot_c\n            y_corr = y_star - lambda_corr * c\n\n            current_inv = np.sum(y_corr)\n            max_dev_corr = max(max_dev_corr, abs(current_inv - inv_0))\n            \n        results.append(max_dev_corr)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world systems can exhibit complex and rapidly changing dynamics, including finite-time singularities where a solution \"blows up\" to infinity. A fixed-step solver is often inefficient or can fail entirely when faced with such behavior. This practice demonstrates the power and elegance of adaptive step-size control, a cornerstone of modern numerical solvers . You will implement an embedded Runge-Kutta method that automatically adjusts its step size to meet a specified error tolerance, and you will observe how it intelligently and robustly handles an approaching singularity by dramatically shrinking its steps.",
            "id": "3205626",
            "problem": "You are to design and analyze a small system of ordinary differential equations (ODEs) that exhibits a finite-time singularity, and then implement an adaptive embedded Runge–Kutta solver to numerically integrate it while observing how the step size contracts as the solution approaches the singular point. The task is entirely mathematical and computational, and it must be solved by writing a complete, runnable program.\n\nConsider the system\n$$\n\\begin{cases}\n\\frac{dr}{dt} = r^2, \\\\\n\\frac{d\\theta}{dt} = 1,\n\\end{cases}\n$$\nwith initial condition $r(t_0) = r_0$, $\\theta(t_0) = \\theta_0$, where $\\theta$ is an angle variable measured in radians. This system is autonomous, and the angular variable is included to make the problem a two-dimensional system while being dynamically simple. The scalar component $r$ has a finite-time singularity that can be derived from separation of variables as a fundamental fact of the differential equation $\\frac{dr}{dt} = r^2$, which implies that the solution cannot be extended beyond a finite blow-up time $t_{\\mathrm{sing}}$ that depends on $r_0$ and $t_0$.\n\nYour program must implement an adaptive embedded explicit Runge–Kutta method that uses two approximations of different orders to estimate the local truncation error and adapt the step size $h$ accordingly so that the local error target is met. The method must:\n- Propagate the solution forward in time starting from $t_0$ with state vector $[r,\\theta]^T$ using a step size $h$ that is adapted based on an error estimate from an embedded pair.\n- Use absolute and relative error tolerances $a_{\\mathrm{tol}}$ and $r_{\\mathrm{tol}}$ to scale the error estimate component-wise in the standard way from numerical analysis, then aggregate it to a single error norm used to decide accepting or rejecting a step.\n- On step rejection, shrink $h$; on acceptance, advance the time and update the state, and possibly increase $h$ if the estimated error is well below tolerance.\n- Halt when either the final time $t_{\\mathrm{end}}$ is reached, or the adaptive step size required by the error control drops below a prescribed minimum $h_{\\min}$ before reaching $t_{\\mathrm{end}}$. In the latter case, the solver is understood to have approached the singularity to the limit of the prescribed minimum step size.\n- Track the minimum accepted step size encountered during the integration.\n\nFor each test, report the following:\n- The final time $t_{\\mathrm{final}}$ reached by the solver.\n- The minimum accepted step size $h_{\\mathrm{min,acc}}$ encountered.\n- A boolean flag indicating whether the solver terminated before $t_{\\mathrm{end}}$ because $h$ fell below $h_{\\min}$ due to error control (interpreted as approaching the singularity under the given tolerances).\n- The difference $t_{\\mathrm{sing}} - t_{\\mathrm{final}}$, where $t_{\\mathrm{sing}}$ is the theoretical blow-up time derived from the initial condition $(t_0,r_0)$.\n\nImplement the adaptive method as described from fundamental definitions of local truncation error control and embedded Runge–Kutta pairs. Do not assume any a priori special formulas for this specific system beyond what is derivable from $\\frac{dr}{dt} = r^2$ by separation of variables and the basic principles of embedded Runge–Kutta adaptation.\n\nTest suite. Your program must run the solver on the following $4$ parameter sets:\n- Test A: $t_0 = 0$, $r_0 = 1$, $\\theta_0 = 0$, $t_{\\mathrm{end}} = 1.5$, $r_{\\mathrm{tol}} = 10^{-6}$, $a_{\\mathrm{tol}} = 10^{-8}$, $h_{\\min} = 10^{-12}$.\n- Test B: $t_0 = 0$, $r_0 = 0.1$, $\\theta_0 = 0$, $t_{\\mathrm{end}} = 2$, $r_{\\mathrm{tol}} = 10^{-6}$, $a_{\\mathrm{tol}} = 10^{-8}$, $h_{\\min} = 10^{-12}$.\n- Test C: $t_0 = 0$, $r_0 = 5$, $\\theta_0 = 0$, $t_{\\mathrm{end}} = 1$, $r_{\\mathrm{tol}} = 10^{-6}$, $a_{\\mathrm{tol}} = 10^{-8}$, $h_{\\min} = 10^{-12}$.\n- Test D: $t_0 = 0$, $r_0 = 1$, $\\theta_0 = 0$, $t_{\\mathrm{end}} = 1.5$, $r_{\\mathrm{tol}} = 10^{-9}$, $a_{\\mathrm{tol}} = 10^{-12}$, $h_{\\min} = 10^{-12}$.\n\nFor each test, compute the analytical blow-up time $t_{\\mathrm{sing}}$ implied by the initial condition and equation $\\frac{dr}{dt} = r^2$ using separation of variables, and report $t_{\\mathrm{sing}} - t_{\\mathrm{final}}$ alongside the numerical outputs.\n\nFinal output format. Your program should produce a single line of output containing a list of results, one per test, where each test result is itself a list of the form $[t_{\\mathrm{final}}, h_{\\mathrm{min,acc}}, \\text{blew\\_up}, t_{\\mathrm{sing}} - t_{\\mathrm{final}}]$. The overall output must be a single line that is a comma-separated list of these per-test lists enclosed in square brackets, for example: [[t_final_A,hmin_A,flag_A,delta_A],[t_final_B,hmin_B,flag_B,delta_B],[t_final_C,hmin_C,flag_C,delta_C],[t_final_D,hmin_D,flag_D,delta_D]].",
            "solution": "The user wants to solve a system of ordinary differential equations (ODEs) that exhibits a finite-time singularity using a custom-built adaptive Runge-Kutta solver.\n\n### Step 1: Extract Givens\n\n- **System of ODEs**:\n  $$\n  \\begin{cases}\n  \\frac{dr}{dt} = r^2, \\\\\n  \\frac{d\\theta}{dt} = 1,\n  \\end{cases}\n  $$\n- **Initial Conditions**: $r(t_0) = r_0$, $\\theta(t_0) = \\theta_0$.\n- **Solver Requirements**:\n  - An adaptive embedded explicit Runge-Kutta method.\n  - Step size adaptation based on local truncation error estimation.\n  - Error control using absolute tolerance ($a_{\\mathrm{tol}}$) and relative tolerance ($r_{\\mathrm{tol}}$).\n  - Component-wise error scaling and aggregation to a single error norm.\n  - Step rejection and acceptance logic.\n- **Termination Conditions**:\n  1.  Solver time $t$ reaches the final time $t_{\\mathrm{end}}$.\n  2.  Adapted step size $h$ drops below a minimum threshold $h_{\\min}$.\n- **Required Outputs per Test**:\n  - $t_{\\mathrm{final}}$: The final time reached by the solver.\n  - $h_{\\mathrm{min,acc}}$: The minimum accepted step size encountered.\n  - `blew_up`: A boolean flag, true if termination occurred because $h  h_{\\min}$.\n  - $t_{\\mathrm{sing}} - t_{\\mathrm{final}}$: The difference between the theoretical blow-up time and the final solver time.\n- **Test Suite**:\n  - Test A: $t_0 = 0$, $r_0 = 1$, $\\theta_0 = 0$, $t_{\\mathrm{end}} = 1.5$, $r_{\\mathrm{tol}} = 10^{-6}$, $a_{\\mathrm{tol}} = 10^{-8}$, $h_{\\min} = 10^{-12}$.\n  - Test B: $t_0 = 0$, $r_0 = 0.1$, $\\theta_0 = 0$, $t_{\\mathrm{end}} = 2$, $r_{\\mathrm{tol}} = 10^{-6}$, $a_{\\mathrm{tol}} = 10^{-8}$, $h_{\\min} = 10^{-12}$.\n  - Test C: $t_0 = 0$, $r_0 = 5$, $\\theta_0 = 0$, $t_{\\mathrm{end}} = 1$, $r_{\\mathrm{tol}} = 10^{-6}$, $a_{\\mathrm{tol}} = 10^{-8}$, $h_{\\min} = 10^{-12}$.\n  - Test D: $t_0 = 0$, $r_0 = 1$, $\\theta_0 = 0$, $t_{\\mathrm{end}} = 1.5$, $r_{\\mathrm{tol}} = 10^{-9}$, $a_{\\mathrm{tol}} = 10^{-12}$, $h_{\\min} = 10^{-12}$.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded**: The problem is based on the well-established theory of ordinary differential equations and numerical methods for their solution. The equation $\\frac{dr}{dt} = r^2$ is a canonical example of an ODE with a finite-time singularity (blow-up). The use of adaptive Runge-Kutta methods is a standard and fundamental technique in scientific computing. The problem is firmly grounded in mathematics and numerical analysis.\n2.  **Well-Posed**: The problem is well-posed. The system of ODEs with given initial conditions has a unique local solution. The task is to approximate this solution numerically up to a point where it ceases to exist or a predefined time is reached. The criteria for the solver and the required outputs are specified clearly.\n3.  **Objective**: The problem is stated in precise, objective mathematical and computational language. There are no subjective or opinion-based components.\n4.  **Completeness**: All necessary data for each test case ($t_0, r_0, \\theta_0, t_{\\mathrm{end}}, r_{\\mathrm{tol}}, a_{\\mathrm{tol}}, h_{\\min}$) are provided. The ODE system and the principles of the numerical method are clearly defined.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a standard, well-defined exercise in numerical analysis. I will proceed with the solution.\n\n### Analytical Preamble: The Singularity\nThe system is composed of two independent ODEs. The equation for $\\theta$ is trivial: $\\frac{d\\theta}{dt} = 1$ integrates to $\\theta(t) = t - t_0 + \\theta_0$. The critical dynamics are in the $r$ component:\n$$\n\\frac{dr}{dt} = r^2\n$$\nThis is a separable equation. For $r \\neq 0$:\n$$\n\\frac{dr}{r^2} = dt\n$$\nIntegrating both sides from the initial condition $(t_0, r_0)$ to a later state $(t, r(t))$:\n$$\n\\int_{r_0}^{r(t)} \\frac{1}{\\rho^2} d\\rho = \\int_{t_0}^t d\\tau\n$$\n$$\n\\left[ -\\frac{1}{\\rho} \\right]_{r_0}^{r(t)} = \\left[ \\tau \\right]_{t_0}^t\n$$\n$$\n-\\frac{1}{r(t)} + \\frac{1}{r_0} = t - t_0\n$$\nSolving for $r(t)$:\n$$\n\\frac{1}{r(t)} = \\frac{1}{r_0} - (t - t_0)\n$$\n$$\nr(t) = \\frac{1}{\\frac{1}{r_0} - (t - t_0)}\n$$\nThe solution $r(t)$ \"blows up\" to infinity when the denominator becomes zero. This defines the singularity time, $t_{\\mathrm{sing}}$:\n$$\n\\frac{1}{r_0} - (t_{\\mathrm{sing}} - t_0) = 0 \\implies t_{\\mathrm{sing}} = t_0 + \\frac{1}{r_0}\n$$\nThis analytical result for $t_{\\mathrm{sing}}$ will be used to evaluate the performance of the numerical solver.\n\n### Numerical Method: Adaptive Embedded Runge-Kutta\nAn embedded Runge-Kutta method computes two approximations of different orders, say $p$ and $p+1$, in a single step. Let the higher-order solution be $y_{n+1}$ and the lower-order (embedded) solution be $y_{n+1}^*$. The difference $E_n = y_{n+1} - y_{n+1}^*$ serves as an estimate of the local truncation error of the lower-order method.\n\nThe core of the adaptive algorithm is to adjust the step size $h$ to keep this error estimate within a desired tolerance.\n1.  **Error Scaling**: The error estimate for each component $j$ of the state vector, $E_{n,j}$, is compared against a tolerance scale $\\text{Tol}_j$:\n    $$\n    \\text{Tol}_j = a_{\\mathrm{tol}} + r_{\\mathrm{tol}} \\cdot \\max(|y_{n,j}|, |y_{n+1,j}|)\n    $$\n2.  **Error Norm**: The individual scaled errors are aggregated into a single scalar norm. A common choice is the weighted root mean square:\n    $$\n    S = \\sqrt{\\frac{1}{D} \\sum_{j=1}^D \\left( \\frac{E_{n,j}}{\\text{Tol}_j} \\right)^2}\n    $$\n    where $D$ is the dimension of the system ($D=2$ in this problem).\n3.  **Step Size Control**:\n    - If $S \\le 1$, the step is accepted. The solution is advanced: $t_{n+1} = t_n + h$, $y_{n+1} = y_{n+1}$ (using the higher-order result).\n    - If $S  1$, the step is rejected. The current step is re-attempted with a smaller step size.\n    - In both cases, a new optimal step size $h_{\\mathrm{new}}$ is proposed:\n      $$\n      h_{\\mathrm{new}} = h \\cdot S_f \\cdot \\left( \\frac{1}{S} \\right)^{1/(p+1)}\n      $$\n      Here, $S_f$ is a safety factor (typically $\\approx 0.9$) and $p$ is the order of the embedded (lower-order) method. The new step size is typically constrained to prevent excessively rapid changes.\n\nFor this implementation, the Cash-Karp 4(5) method will be used. It is a well-known 6-stage method providing a 5th-order solution with an embedded 4th-order solution for error estimation (so $p=4$). The Butcher tableau defines the coefficients for the stages and the solution updates.\n\n### Implementation Strategy\nA Python program will be developed to perform the following:\n1.  Define the ODE system $\\frac{d\\vec{y}}{dt} = \\vec{f}(t, \\vec{y})$, where $\\vec{y} = [r, \\theta]^T$ and $\\vec{f}(t, \\vec{y}) = [r^2, 1]^T$.\n2.  Implement a function that performs the adaptive Runge-Kutta stepping logic as described above, using the Cash-Karp coefficients.\n3.  For each test case provided:\n    a. Calculate the theoretical singularity time $t_{\\mathrm{sing}} = t_0 + 1/r_0$.\n    b. Run the adaptive solver with the specified parameters.\n    c. Collect the required outputs: $t_{\\mathrm{final}}$, $h_{\\mathrm{min,acc}}$, `blew_up`, and $t_{\\mathrm{sing}} - t_{\\mathrm{final}}$.\n4.  Format and print the collected results as a single line, adhering to the specified list-of-lists structure.\n\nThe solver will be built from first principles using `numpy` for vector operations, as stipulated. The final code will be self-contained and runnable.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the given ODE system for multiple test cases using an adaptive\n    Runge-Kutta method and formats the output as specified.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test A\n        {'t0': 0.0, 'r0': 1.0, 'theta0': 0.0, 'tend': 1.5, 'rtol': 1e-6, 'atol': 1e-8, 'hmin': 1e-12},\n        # Test B\n        {'t0': 0.0, 'r0': 0.1, 'theta0': 0.0, 'tend': 2.0, 'rtol': 1e-6, 'atol': 1e-8, 'hmin': 1e-12},\n        # Test C\n        {'t0': 0.0, 'r0': 5.0, 'theta0': 0.0, 'tend': 1.0, 'rtol': 1e-6, 'atol': 1e-8, 'hmin': 1e-12},\n        # Test D\n        {'t0': 0.0, 'r0': 1.0, 'theta0': 0.0, 'tend': 1.5, 'rtol': 1e-9, 'atol': 1e-12, 'hmin': 1e-12}\n    ]\n\n    # Cash-Karp 4(5) Butcher Tableau and properties.\n    # C: Nodes, A: Matrix, B: 5th-order weights, E: Error weights (B - B*), P: Lower order\n    C_CK = np.array([0., 1/5., 3/10., 3/5., 1., 7/8.], dtype=np.float64)\n    A_CK = np.array([\n        [0., 0., 0., 0., 0., 0.],\n        [1/5., 0., 0., 0., 0., 0.],\n        [3/40., 9/40., 0., 0., 0., 0.],\n        [3/10., -9/10., 6/5., 0., 0., 0.],\n        [-11/54., 5/2., -70/27., 35/27., 0., 0.],\n        [1631/55296., 175/512., 575/13824., 44275/110592., 253/4096., 0.]\n    ], dtype=np.float64)\n    B_CK = np.array([37/378., 0., 250/621., 125/594., 0., 512/1771.], dtype=np.float64)\n    B_STAR_CK = np.array([2825/27648., 0., 18575/48384., 13525/55296., 277/14336., 1/4.], dtype=np.float64)\n    E_CK = B_CK - B_STAR_CK\n    P_CK = 4\n    \n    def ode_func(t, y):\n        \"\"\" The ODE system: dr/dt = r^2, d(theta)/dt = 1 \"\"\"\n        r, theta = y\n        return np.array([r**2, 1.0], dtype=np.float64)\n\n    def adaptive_rk_solver(f, t0, y0, tend, rtol, atol, hmin):\n        \"\"\"\n        Implements an adaptive embedded Runge-Kutta solver using the Cash-Karp 4(5) method.\n        \"\"\"\n        # Solver control parameters\n        SAFETY_FACTOR = 0.9\n        MAX_GROWTH = 5.0\n        MIN_SHRINK = 0.2\n        \n        # Initialization\n        t = float(t0)\n        y = np.array(y0, dtype=np.float64)\n        h = 1e-6  # A conservative initial step size\n\n        h_min_accepted = float('inf')\n        blew_up = False\n        \n        while t  tend:\n            # Check for termination due to step size limit\n            if h  hmin:\n                blew_up = True\n                break\n            \n            # Ensure the last step does not overshoot the end time\n            if t + h > tend:\n                h = tend - t\n            \n            # If the remaining interval is too small, break\n            if h = 0:\n                break\n\n            # Calculate the six stages (K_i) for the Cash-Karp method\n            K = np.zeros((6, len(y0)), dtype=np.float64)\n            for i in range(6):\n                y_stage_update = h * np.dot(A_CK[i, :i], K[:i, :])\n                y_stage = y + y_stage_update\n                K[i, :] = f(t + C_CK[i] * h, y_stage)\n\n            # Compute the higher-order (5th) solution and the error estimate\n            y_next = y + h * np.dot(K.T, B_CK)\n            error_est = h * np.dot(K.T, E_CK)\n\n            # Compute the error norm\n            scale = atol + rtol * np.maximum(np.abs(y), np.abs(y_next))\n            scale[scale = 0] = atol # Prevent division by zero or negative scale\n            \n            error_norm = np.sqrt(np.mean((error_est / scale)**2))\n\n            # Step size control logic\n            if error_norm = 1.0:  # Step accepted\n                h_min_accepted = min(h_min_accepted, h)\n                t += h\n                y = y_next\n                \n                # Propose new step size\n                if error_norm == 0.0:\n                    h_new = h * MAX_GROWTH\n                else:\n                    h_new = h * SAFETY_FACTOR * (1.0 / error_norm)**(1.0 / (P_CK + 1))\n                h = min(h_new, h * MAX_GROWTH)\n            else:  # Step rejected\n                # Propose new (smaller) step size\n                h_new = h * SAFETY_FACTOR * (1.0 / error_norm)**(1.0 / (P_CK + 1))\n                h = max(h_new, h * MIN_SHRINK)\n        \n        t_final = t\n        if np.isinf(h_min_accepted):\n            h_min_accepted = 0.0  # Case where no steps were accepted\n        \n        return t_final, h_min_accepted, blew_up\n\n    results = []\n    for case in test_cases:\n        y0 = [case['r0'], case['theta0']]\n        \n        # Calculate theoretical singularity time\n        t_sing = case['t0'] + 1.0/case['r0'] if case['r0'] != 0 else float('inf')\n\n        # Run the solver\n        t_final, h_min_acc, blew_up = adaptive_rk_solver(\n            ode_func, case['t0'], y0, case['tend'], \n            case['rtol'], case['atol'], case['hmin']\n        )\n        \n        # Calculate the difference to the singularity\n        delta_t = t_sing - t_final\n        \n        results.append([t_final, h_min_acc, blew_up, delta_t])\n\n    # Final print statement in the exact required format.\n    # Manually construct the string to avoid spaces from standard list conversion.\n    list_of_strs = []\n    for res in results:\n        # Convert boolean to lowercase 'true'/'false' for JSON-like format\n        bool_str = 'true' if res[2] else 'false'\n        res_str = f\"[{res[0]},{res[1]},{bool_str},{res[3]}]\"\n        list_of_strs.append(res_str)\n    final_output = f\"[{','.join(list_of_strs)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}