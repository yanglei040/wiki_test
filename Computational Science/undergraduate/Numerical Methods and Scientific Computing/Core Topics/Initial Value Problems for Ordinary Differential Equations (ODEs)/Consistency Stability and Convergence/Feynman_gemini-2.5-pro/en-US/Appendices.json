{
    "hands_on_practices": [
        {
            "introduction": "To understand how numerical methods handle oscillatory phenomena, a fundamental test is their application to the equation $y' = i\\omega y$, which models pure rotation in the complex plane. This exercise  challenges you to derive the amplification factor for three cornerstone methods and connect its magnitude directly to the geometric behavior of the numerical solution. By doing so, you will gain a core intuition for numerical dissipation and instability.",
            "id": "3216925",
            "problem": "Consider the linear Ordinary Differential Equation (ODE) $y'(t) = i\\omega y(t)$ with real frequency $\\omega > 0$ and complex initial condition $y(0) = y_{0} \\in \\mathbb{C}$. Let a fixed time step $h > 0$ be given, and consider the one-step numerical methods forward Euler, backward Euler, and the trapezoid rule (also known as the Crankâ€“Nicolson method). Starting from the fundamental definitions of these methods as explicit or implicit one-step approximations of $y'(t) = f(t,y)$, derive the corresponding discrete update relations for this ODE. Using the foundational notion of the amplification factor (also called the stability function) for the linear test equation $y' = \\lambda y$, where the one-step update can be written as $y_{n+1} = R(z)\\,y_{n}$ with $z = \\lambda h$, obtain the amplification factors for each method. Then specialize to the case $\\lambda = i\\omega$ and determine the absolute value of the amplification factor for each method. From first principles, explain how these absolute values determine whether the discrete solutions spiral outward, inward, or remain on a circle in the complex plane as $n$ increases. Finally, let $r_{0} = |y_{0}|$ and $N \\in \\mathbb{N}$ be the number of steps taken. Provide the closed-form expressions for the modulus $|y_{N}|$ produced by each method in terms of $\\omega$, $h$, $r_{0}$, and $N$. Express your final answer as a single row matrix containing the three moduli in the order: forward Euler, backward Euler, trapezoid rule. No rounding is required, and all quantities are dimensionless.",
            "solution": "The problem statement is first validated against the required criteria.\n\n**Step 1: Extract Givens**\n- **ODE**: $y'(t) = i\\omega y(t)$\n- **Constants and variables**: $\\omega \\in \\mathbb{R}$, $\\omega > 0$; $h > 0$; $N \\in \\mathbb{N}$\n- **Initial condition**: $y(0) = y_{0} \\in \\mathbb{C}$\n- **Definition**: $r_{0} = |y_{0}|$\n- **Methods**: Forward Euler, Backward Euler, Trapezoid rule (Crank-Nicolson)\n- **Framework**: Linear test equation $y' = \\lambda y$ leads to the one-step update $y_{n+1} = R(z)y_{n}$ with $z = \\lambda h$.\n- **Task**: Derive update relations, find amplification factors $R(z)$, specialize to $\\lambda = i\\omega$ to find $|R(i\\omega h)|$, explain the geometric interpretation of the magnitude of the amplification factor, and provide closed-form expressions for $|y_N|$ for each method.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard, fundamental problem in the field of numerical analysis for ordinary differential equations. It tests the understanding of stability for basic numerical methods when applied to purely oscillatory systems. The problem statement is self-contained, with all necessary definitions and variables provided. The terms used are precise and standard. There are no contradictions, ambiguities, or violations of scientific principles.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of Discrete Update Relations**\nWe are given the ordinary differential equation $y'(t) = f(t,y)$ where $f(t,y) = i\\omega y(t)$. Let $y_n$ be the numerical approximation of $y(t_n)$ at time $t_n = n h$.\n\n1.  **Forward Euler Method**: This is an explicit method defined by $y_{n+1} = y_n + h f(t_n, y_n)$.\n    Substituting $f(t_n, y_n) = i\\omega y_n$, we get:\n    $$y_{n+1} = y_n + h (i\\omega y_n) = (1 + i\\omega h) y_n$$\n\n2.  **Backward Euler Method**: This is an implicit method defined by $y_{n+1} = y_n + h f(t_{n+1}, y_{n+1})$.\n    Substituting $f(t_{n+1}, y_{n+1}) = i\\omega y_{n+1}$, we get:\n    $$y_{n+1} = y_n + h (i\\omega y_{n+1})$$\n    To find $y_{n+1}$, we must solve for it:\n    $$y_{n+1} - i\\omega h y_{n+1} = y_n$$\n    $$y_{n+1}(1 - i\\omega h) = y_n$$\n    $$y_{n+1} = \\frac{1}{1 - i\\omega h} y_n$$\n\n3.  **Trapezoid Rule (Crank-Nicolson Method)**: This is an implicit method defined by $y_{n+1} = y_n + \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, y_{n+1}))$.\n    Substituting the specific form of $f$, we have:\n    $$y_{n+1} = y_n + \\frac{h}{2}(i\\omega y_n + i\\omega y_{n+1})$$\n    Again, we solve for $y_{n+1}$:\n    $$y_{n+1} - \\frac{i\\omega h}{2} y_{n+1} = y_n + \\frac{i\\omega h}{2} y_n$$\n    $$y_{n+1}\\left(1 - \\frac{i\\omega h}{2}\\right) = y_n\\left(1 + \\frac{i\\omega h}{2}\\right)$$\n    $$y_{n+1} = \\frac{1 + \\frac{i\\omega h}{2}}{1 - \\frac{i\\omega h}{2}} y_n$$\n\n**Amplification Factors and Their Moduli**\nThe numerical update for the linear test equation $y' = \\lambda y$ is of the form $y_{n+1} = R(z)y_n$, where $z = \\lambda h$. The function $R(z)$ is the amplification factor or stability function. For our specific problem, $\\lambda = i\\omega$, so $z = i\\omega h$.\n\n1.  **Forward Euler**: The update is $y_{n+1} = (1 + i\\omega h) y_n$. Comparing this to $y_{n+1} = R(i\\omega h)y_n$, we identify the amplification factor as:\n    $$R_{\\text{FE}}(i\\omega h) = 1 + i\\omega h$$\n    The absolute value (modulus) of this complex number is:\n    $$|R_{\\text{FE}}(i\\omega h)| = |1 + i\\omega h| = \\sqrt{1^2 + (\\omega h)^2} = \\sqrt{1 + (\\omega h)^2}$$\n\n2.  **Backward Euler**: The update is $y_{n+1} = \\frac{1}{1 - i\\omega h} y_n$. The amplification factor is:\n    $$R_{\\text{BE}}(i\\omega h) = \\frac{1}{1 - i\\omega h}$$\n    Its absolute value is:\n    $$|R_{\\text{BE}}(i\\omega h)| = \\left|\\frac{1}{1 - i\\omega h}\\right| = \\frac{|1|}{|1 - i\\omega h|} = \\frac{1}{\\sqrt{1^2 + (-\\omega h)^2}} = \\frac{1}{\\sqrt{1 + (\\omega h)^2}}$$\n\n3.  **Trapezoid Rule**: The update is $y_{n+1} = \\frac{1 + i\\omega h/2}{1 - i\\omega h/2} y_n$. The amplification factor is:\n    $$R_{\\text{TR}}(i\\omega h) = \\frac{1 + \\frac{i\\omega h}{2}}{1 - \\frac{i\\omega h}{2}}$$\n    Its absolute value is:\n    $$|R_{\\text{TR}}(i\\omega h)| = \\left|\\frac{1 + \\frac{i\\omega h}{2}}{1 - \\frac{i\\omega h}{2}}\\right| = \\frac{|1 + \\frac{i\\omega h}{2}|}{|1 - \\frac{i\\omega h}{2}|} = \\frac{\\sqrt{1^2 + (\\frac{\\omega h}{2})^2}}{\\sqrt{1^2 + (-\\frac{\\omega h}{2})^2}} = \\frac{\\sqrt{1 + \\frac{(\\omega h)^2}{4}}}{\\sqrt{1 + \\frac{(\\omega h)^2}{4}}} = 1$$\n    This is because the numerator and denominator are complex conjugates of each other, which always have the same modulus.\n\n**Geometric Interpretation**\nThe exact solution to $y'(t) = i\\omega y(t)$ with $y(0) = y_0$ is $y(t) = y_0 \\exp(i\\omega t)$. The modulus of the exact solution is $|y(t)| = |y_0||\\exp(i\\omega t)| = |y_0| = r_0$, which is constant for all time $t$. This means the exact solution traverses a circle of radius $r_0$ in the complex plane.\n\nThe numerical solution follows the recurrence $y_{n+1} = R(i\\omega h) y_n$. Taking the modulus of both sides gives $|y_{n+1}| = |R(i\\omega h) y_n| = |R(i\\omega h)| |y_n|$. By induction, after $n$ steps, we have $|y_n| = |R(i\\omega h)|^n |y_0|$. The behavior of the numerical solution's modulus depends on the value of $|R(i\\omega h)|$:\n\n-   If $|R(i\\omega h)| > 1$: The modulus $|y_n|$ grows with each step. The numerical solution spirals outwards, away from the true solution's circular path. For the Forward Euler method, since $\\omega > 0$ and $h > 0$, we have $(\\omega h)^2 > 0$, which implies $|R_{\\text{FE}}(i\\omega h)| = \\sqrt{1 + (\\omega h)^2} > 1$. Thus, the Forward Euler method is unconditionally unstable for this problem, always producing a solution that spirals outwards with exponentially increasing amplitude.\n\n-   If $|R(i\\omega h)| < 1$: The modulus $|y_n|$ decreases with each step. The numerical solution spirals inwards, decaying towards the origin. For the Backward Euler method, since $(\\omega h)^2 > 0$, we have $\\sqrt{1 + (\\omega h)^2} > 1$, which implies $|R_{\\text{BE}}(i\\omega h)| = \\frac{1}{\\sqrt{1 + (\\omega h)^2}} < 1$. Thus, the Backward Euler method introduces artificial numerical dissipation, causing the solution to spiral inwards.\n\n-   If $|R(i\\omega h)| = 1$: The modulus $|y_n|$ remains constant, $|y_n| = |y_0|$ for all $n$. The numerical solution points remain on the initial circle of radius $r_0$, just like the exact solution. For the Trapezoid rule, we found $|R_{\\text{TR}}(i\\omega h)| = 1$ for all $\\omega$ and $h$. This means the method is conservative; it exactly preserves the modulus of the solution, which is a key property of the underlying physical system being modeled (e.g., conservation of energy or probability).\n\n**Closed-Form Expressions for $|y_N|$**\nUsing the relation $|y_N| = |R(i\\omega h)|^N |y_0|$ and the definition $r_0 = |y_0|$, we can write the closed-form expressions for the modulus after $N$ steps.\n\n1.  **Forward Euler**:\n    $$|y_N|_{\\text{FE}} = \\left(\\sqrt{1 + (\\omega h)^2}\\right)^N r_0 = r_0 \\left(1 + (\\omega h)^2\\right)^{\\frac{N}{2}}$$\n\n2.  **Backward Euler**:\n    $$|y_N|_{\\text{BE}} = \\left(\\frac{1}{\\sqrt{1 + (\\omega h)^2}}\\right)^N r_0 = r_0 \\left(1 + (\\omega h)^2\\right)^{-\\frac{N}{2}}$$\n\n3.  **Trapezoid Rule**:\n    $$|y_N|_{\\text{TR}} = (1)^N r_0 = r_0$$\n\nThese expressions quantify the long-term behavior of the magnitude of the numerical solution for each method.",
            "answer": "$$\\boxed{\\begin{pmatrix} r_{0} \\left(1 + (\\omega h)^2\\right)^{\\frac{N}{2}} & r_{0} \\left(1 + (\\omega h)^2\\right)^{-\\frac{N}{2}} & r_{0} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The Dahlquist Equivalence Theorem is a landmark result stating that a linear multistep method converges if and only if it is both consistent and zero-stable. This practice  provides a powerful, hands-on demonstration of this theorem by analyzing a method that is consistent yet fails the zero-stability test. Through both theoretical analysis and computation, you will witness how an initially small error can be amplified catastrophically, leading to divergence even as the step size shrinks.",
            "id": "3112000",
            "problem": "You are to demonstrate, in both analysis and computation, that a linear multistep method can be consistent yet fail zero-stability, and consequently diverge on the linear test problem even as the step size tends to zero. Work strictly from core definitions and well-tested facts. Consider the linear test ordinary differential equation (ODE) $y'=\\lambda y$ with constant $\\lambda\\in\\mathbb{R}$ and the two-step method\n$$\ny_{n+1}-2y_n+y_{n-1}=h\\left(f_{n+1}-f_n\\right),\n$$\nwhere $f_k=f(t_k,y_k)$ with $t_k=t_0+kh$ and $h>0$ is the time step. This scheme is defined for sufficiently smooth $f$ and uses two past values $y_{n}$ and $y_{n-1}$ to compute $y_{n+1}$.\n\nYou must: \n- Derive from first principles whether the method is consistent, using the definition of consistency for linear multistep methods that relies on the local truncation error tending to zero as $h\\to 0$ for smooth exact solutions. Base your derivation on the Taylor expansion of exact solutions and the standard linear multistep method representation, without invoking shortcut formulas beyond the core definitions.\n- Analyze the root condition for zero-stability starting from the characteristic polynomial induced by the homogeneous part of the method (that is, the polynomial connecting $y_{n+1}$, $y_n$, $y_{n-1}$ when $f\\equiv 0$), and determine whether the method satisfies zero-stability.\n- Explain why failure of zero-stability implies that global numerical error can grow unboundedly as $h\\to 0$ at fixed final time $T>0$, even when the underlying ODE has bounded solutions (for instance when $\\lambda\\le 0$). Your reasoning should connect the repeated root phenomenon to the growth of parasitic modes and to the dependence on the number of steps $N=T/h$.\n\nThen, implement the method on the linear test problem $y'=\\lambda y$ with exact initial value $y(0)=y_0$ and a deliberately perturbed second starting value. Use $y_0=\\exp(0\\cdot\\lambda)=1$ and $y(h)=\\exp(\\lambda h)$ as the exact first step. Introduce a fixed perturbation of magnitude $\\varepsilon\\ge 0$ to the second initial value by setting $y_1^{\\text{start}}=\\exp(\\lambda h)+\\varepsilon$. Evolve the recurrence to $t=T$ and measure the final absolute error $E(h)=|y_N-\\exp(\\lambda T)|$, where $N=T/h$ is an integer number of steps. For each test case, compare $E(h_\\text{small})$ to $E(h_\\text{large})$ to detect divergence as $h\\to 0$.\n\nTest suite specifications:\n- Use final time $T=1$ for all cases.\n- Use two step sizes $h_\\text{large}=0.1$ and $h_\\text{small}=0.05$ (so that $N$ is an integer for both).\n- Provide three test cases that cover different facets:\n  1. General case: $\\lambda=-1$, $\\varepsilon=10^{-8}$.\n  2. Boundary case with constant solution: $\\lambda=0$, $\\varepsilon=10^{-8}$.\n  3. Edge case with perfect starting values: $\\lambda=-1$, $\\varepsilon=0$.\n- For each test case, compute the boolean $B=\\big(E(h_\\text{small})>E(h_\\text{large})\\big)$, which should be `True` in cases that exhibit divergence as the step size is reduced and `False` otherwise.\n\nYour program should produce a single line of output containing the three booleans for the test cases as a comma-separated list enclosed in square brackets (for example, `[True,False,True]`). No physical units or angle units are involved in this problem. All mathematical entities must be written in LaTeX. The program must be self-contained, require no input, and implement the specified method exactly on the given test cases.",
            "solution": "The problem statement is a valid exercise in the numerical analysis of ordinary differential equations (ODEs). It is self-contained, scientifically grounded in the theory of linear multistep methods, and poses a well-defined set of analytical and computational tasks. We will proceed with the solution.\n\nThe problem requires an analysis of the two-step method\n$$\ny_{n+1}-2y_n+y_{n-1}=h\\left(f_{n+1}-f_n\\right)\n$$\napplied to the linear test equation $y' = \\lambda y$, for which $f(t,y) = \\lambda y$ and thus $f_k = \\lambda y_k$.\n\n### Part 1: Consistency Analysis\n\nA linear multistep method is consistent if its local truncation error (LTE) approaches zero as the step size $h \\to 0$. We derive the LTE from first principles. Let $y(t)$ be a sufficiently smooth exact solution to the ODE $y' = f(t, y(t))$. The LTE at step $n+1$ is defined by substituting the exact solution into the numerical scheme. To align with the standard definition of a $k$-step method, we shift the indices by $+1$ to express the method as computing step $n+2$ from previous steps:\n$$\ny_{n+2} - 2y_{n+1} + y_n = h(f_{n+2} - f_{n+1})\n$$\nThe linear difference operator $\\mathcal{L}$ for this method is:\n$$\n\\mathcal{L}[y(t); h] = y(t+2h) - 2y(t+h) + y(t) - h\\left(y'(t+2h) - y'(t+h)\\right)\n$$\nThe local truncation error is $T_{n+2} = \\mathcal{L}[y(t_n); h]/h$. Consistency requires that for any sufficiently smooth solution $y(t)$, $T_{n+k} \\to 0$ as $h \\to 0$.\n\nWe perform Taylor series expansions of $y(t+h)$, $y(t+2h)$, $y'(t+h)$, and $y'(t+2h)$ around the point $t$:\n\\begin{align*}\ny(t+h) &= y(t) + h y'(t) + \\frac{h^2}{2} y''(t) + \\frac{h^3}{6} y'''(t) + O(h^4) \\\\\ny(t+2h) &= y(t) + 2h y'(t) + \\frac{(2h)^2}{2} y''(t) + \\frac{(2h)^3}{6} y'''(t) + O(h^4) \\\\\n&= y(t) + 2h y'(t) + 2h^2 y''(t) + \\frac{4h^3}{3} y'''(t) + O(h^4) \\\\\ny'(t+h) &= y'(t) + h y''(t) + \\frac{h^2}{2} y'''(t) + O(h^3) \\\\\ny'(t+2h) &= y'(t) + 2h y''(t) + \\frac{(2h)^2}{2} y'''(t) + O(h^3) \\\\\n&= y'(t) + 2h y''(t) + 2h^2 y'''(t) + O(h^3)\n\\end{align*}\nSubstitute these expansions into the operator $\\mathcal{L}[y(t); h]$:\n\nFirst part (involving $y$):\n\\begin{align*}\n& y(t+2h) - 2y(t+h) + y(t) \\\\\n&= \\left(y(t) + 2h y' + 2h^2 y'' + \\frac{4h^3}{3} y'''\\right) - 2\\left(y(t) + h y' + \\frac{h^2}{2} y'' + \\frac{h^3}{6} y'''\\right) + y(t) + O(h^4) \\\\\n&= (1-2+1)y(t) + (2-2)h y'(t) + (2-1)h^2 y''(t) + \\left(\\frac{4}{3} - \\frac{2}{6}\\right)h^3 y'''(t) + O(h^4) \\\\\n&= h^2 y''(t) + h^3 y'''(t) + O(h^4)\n\\end{align*}\n\nSecond part (involving $y'$):\n\\begin{align*}\n& h\\left(y'(t+2h) - y'(t+h)\\right) \\\\\n&= h\\left[ \\left(y'(t) + 2h y'' + 2h^2 y'''\\right) - \\left(y'(t) + h y'' + \\frac{h^2}{2} y'''\\right) + O(h^3) \\right] \\\\\n&= h\\left[ h y''(t) + \\frac{3h^2}{2} y'''(t) + O(h^3) \\right] \\\\\n&= h^2 y''(t) + \\frac{3h^3}{2} y'''(t) + O(h^4)\n\\end{align*}\n\nCombining both parts:\n\\begin{align*}\n\\mathcal{L}[y(t); h] &= \\left(h^2 y''(t) + h^3 y'''(t)\\right) - \\left(h^2 y''(t) + \\frac{3h^3}{2} y'''(t)\\right) + O(h^4) \\\\\n&= -\\frac{1}{2}h^3 y'''(t) + O(h^4)\n\\end{align*}\n\nThe local truncation error is $T_{n+2} = \\mathcal{L}[y(t_n); h]/h = -\\frac{1}{2}h^2 y'''(t_n) + O(h^3)$.\nSince $T_{n+2} \\to 0$ as $h \\to 0$, the method is **consistent**. The order of accuracy is $p=2$, as the leading term in the LTE is $O(h^2)$.\n\n### Part 2: Zero-Stability Analysis\n\nZero-stability pertains to the behavior of the numerical solution for the homogeneous problem $y' = 0$, or equivalently, in the limit $h \\to 0$. For the given method, setting $f \\equiv 0$ (or $h=0$) gives the homogeneous linear difference equation:\n$$\ny_{n+1} - 2y_n + y_{n-1} = 0\n$$\nTo analyze its stability, we examine the roots of its first characteristic polynomial, $\\rho(z)$. Assuming a solution of the form $y_n=z^n$, we substitute it into the difference equation:\n$$\nz^{n-1}(z^2 - 2z + 1) = 0\n$$\nThe characteristic polynomial is $\\rho(z) = z^2 - 2z + 1 = (z-1)^2$. The roots of $\\rho(z)=0$ are $z_1=1$ and $z_2=1$, a double root at $z=1$.\n\nThe **root condition** for zero-stability states that:\n1.  All roots of $\\rho(z)$ must lie within or on the unit circle in the complex plane (i.e., $|z_i| \\le 1$).\n2.  Any root on the unit circle (i.e., $|z_i|=1$) must be simple (i.e., have multiplicity $1$).\n\nThe roots of our method are $z_1=z_2=1$. They are on the unit circle, satisfying the first condition. However, the root at $z=1$ has multiplicity $2$, which violates the second condition.\nTherefore, the method is **not zero-stable**.\n\n### Part 3: Explanation of Divergence\n\nThe Dahlquist Equivalence Theorem states that a linear multistep method is convergent if and only if it is both consistent and zero-stable. Since our method is consistent but not zero-stable, it is **not convergent**.\n\nThe failure of zero-stability implies that errors, once introduced, will be amplified in an unbounded manner as the number of steps $n$ increases. This can be seen by examining the general solution to the homogeneous recurrence $y_{n+1} - 2y_n + y_{n-1} = 0$. For a double root at $z=1$, the general solution is not just $c_1 (1)^n$, but includes a linearly growing term:\n$$\ny_n = c_1 + c_2 n\n$$\nThe $c_1$ term corresponds to the principal root that approximates the true solution. The $c_2 n$ term is a parasitic solution that does not correspond to the behavior of the ODE. At a fixed final time $T$, the number of steps is $n = T/h$. The parasitic component thus behaves as $c_2 n = c_2 T/h$. As the step size $h$ is reduced ($h \\to 0$), the number of steps $n$ grows, and this parasitic solution grows without bound.\n\nFor the full problem $y'=\\lambda y$, the recurrence relation is $y_{n+1}(1 - h\\lambda) = (2 - h\\lambda)y_n - y_{n-1}$. The characteristic equation for this recurrence is $(1-h\\lambda)z^2 - (2-h\\lambda)z + 1 = 0$. Its roots are $z_1 = (1-h\\lambda)^{-1}$ and $z_2 = 1$. The general solution is $y_n = c_1 z_1^n + c_2 z_2^n = c_1 ((1-h\\lambda)^{-1})^n + c_2$.\nThe root $z_1 \\approx 1+h\\lambda$ for small $h$, and $z_1^n \\approx (e^{h\\lambda})^n = e^{\\lambda nh} = e^{\\lambda t_n}$, so $z_1$ is the principal root that tracks the true solution. The root $z_2=1$ is the parasitic root.\n\nThe coefficients $c_1$ and $c_2$ are determined by the starting values $y_0$ and $y_1$. A small perturbation $\\varepsilon$ in the starting values, $y_1 = y_{\\text{exact}}(h) + \\varepsilon$, excites the parasitic mode. An analysis shows that $c_2$ is proportional to $\\varepsilon/h$. Specifically, for $\\lambda \\ne 0$, $c_2 \\approx -\\varepsilon/(\\lambda h)$, and for $\\lambda=0$, $c_2=\\varepsilon$. In both cases, the coefficient of the parasitic term grows as $h \\to 0$. This means that even an infinitesimal initial perturbation is amplified, leading to a global error that diverges as $O(1/h)$.\n\nFor the case with perfect starting values ($\\varepsilon=0$), the coefficient $c_2$ should be zero in exact arithmetic. However, any floating-point round-off error introduced at any step of the computation acts like a new perturbation, exciting the parasitic mode. Therefore, divergence is inevitable, though it may take more steps to become apparent if the initial data is very accurate.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and implements an unstable linear multistep method to demonstrate\n    the consequences of failing the zero-stability condition.\n    \"\"\"\n\n    def compute_error(lambda_val, epsilon, h, T):\n        \"\"\"\n        Implements the given linear multistep method and computes the final error.\n\n        The method is: y_{n+1} - 2*y_n + y_{n-1} = h*(f_{n+1} - f_n)\n        For f(t,y) = lambda*y, this rearranges to:\n        y_{n+1} = ((2 - h*lambda)*y_n - y_{n-1}) / (1 - h*lambda)\n        \"\"\"\n        N = int(round(T / h))\n\n        # Starting values\n        # y_0 is the exact initial condition y(0)=1\n        y_prev = 1.0\n        # y_1 is the exact solution at t=h, plus a perturbation epsilon\n        y_curr = np.exp(lambda_val * h) + epsilon\n\n        # Handle the special case where lambda is 0\n        if lambda_val == 0:\n            # The recurrence simplifies to y_{n+1} = 2*y_n - y_{n-1}\n            for _ in range(N - 1): # Loop to compute y_2, y_3, ..., y_N\n                y_next = 2.0 * y_curr - y_prev\n                y_prev = y_curr\n                y_curr = y_next\n        else:\n            # Coefficients for the recurrence relation, pre-calculated for efficiency\n            c1 = (2.0 - h * lambda_val) / (1.0 - h * lambda_val)\n            c2 = -1.0 / (1.0 - h * lambda_val)\n            for _ in range(N - 1):\n                y_next = c1 * y_curr + c2 * y_prev\n                y_prev = y_curr\n                y_curr = y_next\n        \n        y_N = y_curr\n        \n        # Exact solution at time T\n        y_exact_T = np.exp(lambda_val * T)\n        \n        # Absolute error at final time T\n        error = np.abs(y_N - y_exact_T)\n        \n        return error\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda, epsilon)\n        (-1.0, 1e-8),  # General case with stable ODE and perturbation\n        (0.0, 1e-8),   # Boundary case with constant solution and perturbation\n        (-1.0, 0.0),   # Edge case with stable ODE and no initial perturbation\n    ]\n    \n    T = 1.0\n    h_large = 0.1\n    h_small = 0.05\n    \n    results = []\n    for lambda_val, epsilon in test_cases:\n        # Compute error for the larger step size\n        error_large_h = compute_error(lambda_val, epsilon, h_large, T)\n        \n        # Compute error for the smaller step size\n        error_small_h = compute_error(lambda_val, epsilon, h_small, T)\n        \n        # Check for divergence: error increases as step size decreases\n        divergence_observed = error_small_h > error_large_h\n        results.append(divergence_observed)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practical applications, adaptive time-stepping is used to efficiently solve ODEs by adjusting the step size $h$ to meet a prescribed local error tolerance. However, for stiff systems, this can lead to a critical pitfall: the error controller may select a step size that is accurate enough locally but violates the system's stringent stability requirements. This coding-based practice  tasks you with implementing an adaptive solver to reveal how a focus on consistency alone can lead to instability and non-convergence, a crucial lesson in computational science.",
            "id": "3216936",
            "problem": "Implement and analyze an adaptive explicit Euler time-stepper for a linear stiff system, focusing on the interplay among consistency, stability, and convergence. The system is the linear autonomous ordinary differential equation with diagonal matrix $$A = \\mathrm{diag}(-1,-1000)$$ so that $$y'(t) = A\\,y(t), \\quad y(t) \\in \\mathbb{R}^2,$$ with exact solution $$y(t) = \\big(y_1(0)\\,e^{-t},\\; y_2(0)\\,e^{-1000\\,t}\\big).$$ Your program must do the following, grounded in fundamental definitions.\n\n- Use the one-step explicit Euler method as the base scheme. Consistency means the local truncation error tends to zero as the time step size tends to zero. For the scalar linear test equation $$z'(t) = \\lambda z(t),$$ the explicit Euler update yields $$z_{n+1} = z_n + h\\,\\lambda\\,z_n,$$ where $$h>0$$ is the time step. Derive the absolute stability region using the linear test equation and the definition of absolute stability in terms of amplification factors for one-step methods. You must implement a diagnostic that, for each accepted step of size $$h$$, checks whether the step lies inside the absolute stability region simultaneously for both eigenvalues $\\lambda_1 = -1$ and $\\lambda_2 = -1000$.\n\n- Use step doubling as a local error estimator. For a current state $$y_n$$ at time $$t_n$$ and trial step size $$h$$, compute\n  - One full step of explicit Euler: $$y^{(1)} = y_n + h\\,f(y_n), \\quad f(y) = A\\,y.$$\n  - Two half-steps of explicit Euler: $$y^{(2)} = y_n + \\tfrac{h}{2}\\,f(y_n), \\quad y^{(2)} = y^{(2)} + \\tfrac{h}{2}\\,f\\big(y^{(2)}\\big),$$ reusing the symbol $$y^{(2)}$$ for the final doubled result for brevity.\n  - A scalar error estimate $$\\mathrm{est} = \\lVert y^{(2)} - y^{(1)} \\rVert_{\\infty}.$$\n  Accept the step if $$\\mathrm{est} \\le \\mathrm{tol},$$ where $$\\mathrm{tol} > 0$$ is a user-specified tolerance. On acceptance, advance $$y_{n+1} := y^{(2)}$$ and $$t_{n+1} := t_n + h.$$\n  Use the standard controller based on the second-order behavior of the step-doubling difference for a first-order base method to update the next step size:\n  $$h_{\\mathrm{new}} = \\min\\big(h_{\\max},\\; \\max\\big(h_{\\min},\\; s \\cdot h \\cdot \\max(0.1,\\; (\\mathrm{tol}/\\max(\\mathrm{est},\\epsilon))^{1/2})\\big)\\big),$$\n  where the safety factor is $$s=0.9,$$ the minimum step is $$h_{\\min} = 10^{-16},$$ and $$\\epsilon = 10^{-300}$$ prevents division by zero. On rejection, keep $$t_n, y_n$$ unchanged and only update $$h \\leftarrow h_{\\mathrm{new}}.$$ Ensure the final step hits the final time exactly by clipping $$h \\le T - t_n$$ at each iteration.\n\n- Convergence assessment: compute the final-time global error as $$\\lVert y_{\\mathrm{num}}(T) - y_{\\mathrm{exact}}(T)\\rVert_{\\infty}.$$\n\n- Stability assessment under adaptivity: during the integration, maintain a boolean flag that is `True` if and only if every accepted step satisfies the absolute stability condition for explicit Euler simultaneously for $\\lambda_1 = -1$ and $\\lambda_2 = -1000$, according to the region derived from the linear test equation definition. If any accepted step violates this, set the flag to `False` and keep integrating.\n\n- Termination and safeguards: terminate when $$t \\ge T$$. Use a hard cap of $$10^7$$ total trial steps to avoid infinite loops; if the cap is reached, terminate and report the metrics computed up to that point.\n\nTest suite and required inputs. For each test case, you are given $$(\\mathrm{tol},\\; y_1(0),\\; y_2(0),\\; T,\\; h_0,\\; h_{\\max}),$$ where $$h_0$$ is the initial step size guess and $$h_{\\max}$$ is the maximum allowed step size. Use $$h_{\\min} = 10^{-16}$$ as above. The test suite contains the following four cases:\n\n- Case A (happy path with moderate tolerance over a short horizon): $$(\\mathrm{tol}, y_1(0), y_2(0), T, h_0, h_{\\max}) = (10^{-2},\\; 1,\\; 1,\\; 10^{-1},\\; 5\\times 10^{-2},\\; 5\\times 10^{-2}).$$\n- Case B (initial guess outside stability; adaptivity should reduce and maintain stability): $$(5\\times 10^{-2},\\; 1,\\; 1,\\; 10^{-2},\\; 10^{-2},\\; 10^{-2}).$$\n- Case C (stiff component initially tiny; local error control may accept unstable steps, demonstrating how adaptivity can break stability): $$(10^{-1},\\; 1,\\; 10^{-12},\\; 5\\times 10^{-1},\\; 5\\times 10^{-2},\\; 5\\times 10^{-2}).$$\n- Case D (hard cap at the absolute stability boundary via $$h_{\\max}$$): $$(10^{-3},\\; 1,\\; 1,\\; 1,\\; 2\\times 10^{-3},\\; 2\\times 10^{-3}).$$\n\nFor each case, your program must compute:\n- The final-time global error $$E = \\lVert y_{\\mathrm{num}}(T) - y_{\\mathrm{exact}}(T)\\rVert_{\\infty}.$$\n- The boolean $$S$$ indicating whether every accepted step lay in the absolute stability region for both eigenvalues.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$$[E_A,\\;S_A,\\;E_B,\\;S_B,\\;E_C,\\;S_C,\\;E_D,\\;S_D],$$\nwhere each $$E_\\bullet$$ is a floating-point number and each $$S_\\bullet$$ is a boolean. No units are involved in this problem, and no angles appear. Express numbers as plain decimal floats in the output.",
            "solution": "The problem requires the implementation and analysis of an adaptive explicit Euler method for a stiff linear system of ordinary differential equations (ODEs). The solution will be presented in two parts: first, a theoretical derivation of the stability condition, and second, an outline of the adaptive algorithm as specified.\n\nThe ODE system is given by $y'(t) = A y(t)$, where $y(t) \\in \\mathbb{R}^2$ and $A = \\mathrm{diag}(-1, -1000)$. The eigenvalues of the matrix $A$ are therefore $\\lambda_1 = -1$ and $\\lambda_2 = -1000$. The disparate magnitudes of these eigenvalues indicate that the system is stiff.\n\nTo analyze the numerical stability of the explicit Euler method, we consider the scalar linear test equation $z'(t) = \\lambda z(t)$, where $\\lambda \\in \\mathbb{C}$ is a constant. Applying the explicit Euler method, $z_{n+1} = z_n + h f(t_n, z_n)$, with a time step $h>0$ and $f(t,z) = \\lambda z$, yields the recurrence relation:\n$$z_{n+1} = z_n + h \\lambda z_n = (1 + h\\lambda) z_n$$\nThe term $g(w) = 1+w$, where $w = h\\lambda$, is the amplification factor. For a one-step method to be absolutely stable, the magnitude of the amplification factor must not exceed unity, ensuring that numerical errors are not amplified over successive steps. The region of absolute stability is the set of $w \\in \\mathbb{C}$ for which $|g(w)| \\le 1$.\n$$ |1 + h\\lambda| \\le 1 $$\nFor the given problem, the eigenvalues $\\lambda_1 = -1$ and $\\lambda_2 = -1000$ are real and negative. Consequently, $w = h\\lambda$ is also real and negative. The stability condition simplifies to:\n$$ -1 \\le 1 + h\\lambda \\le 1 $$\nThis is equivalent to the two inequalities:\n1. $1 + h\\lambda \\le 1 \\implies h\\lambda \\le 0$. Since $h>0$ and $\\lambda<0$, this is always satisfied.\n2. $-1 \\le 1 + h\\lambda \\implies -2 \\le h\\lambda$.\n\nThus, for a real, negative $\\lambda$, the explicit Euler method is absolutely stable if and only if $h\\lambda \\in [-2, 0]$. This imposes an upper bound on the step size $h$:\n$$ h \\le -\\frac{2}{\\lambda} $$\nFor the given system, this condition must hold for both eigenvalues simultaneously:\n- For $\\lambda_1 = -1$: $h \\le -\\frac{2}{-1} = 2$.\n- For $\\lambda_2 = -1000$: $h \\le -\\frac{2}{-1000} = 0.002$.\n\nTo ensure stability for the entire system, the step size $h$ must satisfy the most restrictive condition, which is $h \\le 0.002$. The numerical implementation will track whether every accepted step adheres to this bound.\n\nThe adaptive algorithm is constructed as follows:\n1.  **Base Method**: The one-step explicit Euler method, $y_{n+1} = y_n + h f(y_n)$, where $f(y) = Ay$. This method has first-order accuracy, meaning its local truncation error (LTE) is $\\mathcal{O}(h^2)$.\n\n2.  **Local Error Estimation**: Step doubling is used. From the current state $y_n$ at time $t_n$, two separate approximations for the solution at $t_n+h$ are computed:\n    - A single step of size $h$: $y^{(1)} = y_n + h A y_n$.\n    - Two consecutive steps of size $h/2$: A temporary state $y_{n+1/2} = y_n + \\frac{h}{2} A y_n$ is computed, followed by $y^{(2)} = y_{n+1/2} + \\frac{h}{2} A y_{n+1/2}$.\n    The LTE of $y^{(2)}$ is smaller than that of $y^{(1)}$. The difference between these two approximations provides an estimate of the local error. A scalar error measure is defined as $\\mathrm{est} = \\lVert y^{(2)} - y^{(1)} \\rVert_{\\infty}$.\n\n3.  **Step Size Control**: The step is accepted if the error estimate is within a specified tolerance, $\\mathrm{est} \\le \\mathrm{tol}$. If accepted, the state is advanced to $y_{n+1} = y^{(2)}$ (using the more accurate approximation) and time is advanced to $t_{n+1} = t_n + h$. If rejected, the state and time remain $y_n$ and $t_n$. In either case, a new step size $h_{\\mathrm{new}}$ is proposed for the next iteration using a standard Proportional-Integral (PI) type controller logic. The LTE for a first-order method is proportional to $h^2$. To achieve a target error $\\mathrm{tol}$, the new step size is adjusted based on the current estimated error: $h_{\\mathrm{new}} \\approx h (\\mathrm{tol}/\\mathrm{est})^{1/2}$. The algorithm employs the specific formula:\n    $$h_{\\mathrm{new}} = \\min\\big(h_{\\max},\\; \\max\\big(h_{\\min},\\; s \\cdot h \\cdot \\max(0.1,\\; (\\mathrm{tol}/\\max(\\mathrm{est},\\epsilon))^{1/2})\\big)\\big)$$\n    Here, $s=0.9$ is a safety factor, $\\epsilon$ avoids division by zero, and the bounds $h_{\\min}$, $h_{\\max}$, and the factor $0.1$ prevent excessively large or small changes in the step size.\n\n4.  **Convergence and Stability Assessment**:\n    - **Convergence**: The global error is computed at the final time $T$ as $E = \\lVert y_{\\mathrm{num}}(T) - y_{\\mathrm{exact}}(T)\\rVert_{\\infty}$, where $y_{\\mathrm{exact}}(T) = (y_1(0)e^{-T}, y_2(0)e^{-1000T})$.\n    - **Stability**: A boolean flag, initialized to $\\mathrm{True}$, is maintained. If any accepted step has a size $h > 0.002$, the flag is set to $\\mathrm{False}$, indicating that at least one step violated the absolute stability condition dictated by the stiffest component of the system.\n\nThis design enables a practical examination of the relationship between consistency (local error control), stability (boundedness of the numerical solution), and convergence (accuracy of the final global solution), particularly in the context of stiff equations where local error control does not guarantee stability. Case C is designed to illustrate this specific failure mode, where an unstable step can be accepted by the error controller because the corresponding unstable solution component is initially too small to produce a significant local error estimate. Case D, by contrast, enforces stability via the choice of $h_{\\max}$, demonstrating that while stability is maintained, the step size may be severely restricted by accuracy requirements even if it is within the stable range.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_adaptive_euler(tol, y1_0, y2_0, T, h0, h_max):\n    \"\"\"\n    Implements an adaptive explicit Euler time-stepper for a linear stiff system.\n\n    Args:\n        tol (float): User-specified error tolerance.\n        y1_0 (float): Initial condition for the first component.\n        y2_0 (float): Initial condition for the second component.\n        T (float): Final integration time.\n        h0 (float): Initial step size guess.\n        h_max (float): Maximum allowed step size.\n\n    Returns:\n        tuple: A tuple containing:\n            - global_error (float): The final-time global error.\n            - all_steps_stable (bool): True if every accepted step was stable.\n    \"\"\"\n    # --- Problem Definition and Constants ---\n    A = np.diag([-1.0, -1000.0])\n    y0 = np.array([y1_0, y2_0], dtype=float)\n    \n    # --- Algorithm Parameters ---\n    h_min = 1e-16\n    s = 0.9  # Safety factor\n    epsilon = 1e-300\n    max_steps = 10_000_000\n    \n    # --- Initialization ---\n    t = 0.0\n    y = y0.copy()\n    h = h0\n    total_steps = 0\n    all_steps_stable = True\n    stability_limit = 2.0 / 1000.0  # h = 0.002 for stability\n\n    # --- Main Adaptive Loop ---\n    while t  T and total_steps  max_steps:\n        total_steps += 1\n        \n        # Use a temporary variable for the current step size\n        h_current = h\n        \n        # Clip step to not overshoot final time T\n        if t + h_current > T:\n            h_current = T - t\n            \n        # --- Step Doubling for Error Estimation ---\n        # Right-hand side evaluation\n        f_n = A @ y\n        \n        # One full step of size h_current\n        y_1_step = y + h_current * f_n\n        \n        # Two half-steps of size h_current/2\n        y_half_step_1 = y + (h_current / 2.0) * f_n\n        f_half = A @ y_half_step_1\n        y_2_step = y_half_step_1 + (h_current / 2.0) * f_half\n        \n        # --- Error Estimate and Step Control ---\n        error_est = np.linalg.norm(y_2_step - y_1_step, ord=np.inf)\n        \n        step_accepted = (error_est = tol or h_current = h_min)\n\n        if step_accepted:\n            t += h_current\n            y = y_2_step  # Advance with the more accurate result\n            \n            # Check if the accepted step violates the stability condition\n            if h_current > stability_limit:\n                all_steps_stable = False\n        # On rejection, t and y are not changed.\n        \n        # --- Update Step Size for Next Iteration ---\n        # The new step size is calculated based on the attempted step (h_current)\n        # and the resulting error estimate, as per the specified formula.\n        scale_factor = (tol / max(error_est, epsilon))**0.5\n        h_new = s * h_current * max(0.1, scale_factor)\n        \n        # Enforce step size bounds\n        h = min(h_max, max(h_min, h_new))\n        \n        # Break if the final time is reached\n        if t >= T:\n            break\n            \n    # --- Final Convergence and Stability Assessment ---\n    y_num_T = y\n    y_exact_T = np.array([y0[0] * np.exp(-T), y0[1] * np.exp(-1000.0 * T)])\n    \n    global_error = np.linalg.norm(y_num_T - y_exact_T, ord=np.inf)\n    \n    return global_error, all_steps_stable\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = {\n        'A': (1e-2, 1.0, 1.0, 1e-1, 5e-2, 5e-2),\n        'B': (5e-2, 1.0, 1.0, 1e-2, 1e-2, 1e-2),\n        'C': (1e-1, 1.0, 1e-12, 5e-1, 5e-2, 5e-2),\n        'D': (1e-3, 1.0, 1.0, 1.0, 2e-3, 2e-3),\n    }\n\n    results = []\n    # Process cases in specified order\n    for case_id in ['A', 'B', 'C', 'D']:\n        params = test_cases[case_id]\n        tol, y1_0, y2_0, T, h0, h_max = params\n        \n        error, stable = run_adaptive_euler(tol, y1_0, y2_0, T, h0, h_max)\n        \n        results.append(error)\n        results.append(stable)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}