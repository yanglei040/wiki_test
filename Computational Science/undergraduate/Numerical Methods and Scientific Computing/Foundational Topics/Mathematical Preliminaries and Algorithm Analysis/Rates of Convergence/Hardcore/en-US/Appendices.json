{
    "hands_on_practices": [
        {
            "introduction": "Newton's method is renowned for its rapid, quadratic convergence, making it a cornerstone of numerical root-finding. However, its performance is not universally guaranteed. This exercise explores a critical exception: when the target root has a multiplicity greater than one, the convergence rate degrades to being merely linear. Through implementing Newton's method and applying it to functions with multiple roots, you will numerically verify this behavior and gain a deeper appreciation for the theoretical conditions that underpin the method's celebrated speed .",
            "id": "3265302",
            "problem": "You are asked to construct and analyze a case where Newton’s method exhibits only linear convergence due to root multiplicity. Work from foundational definitions of convergence rates and Newton’s method. You must implement the algorithm and quantify the observed behavior using precise numerical metrics.\n\nDefinitions to use as the fundamental base:\n- A point $\\alpha \\in \\mathbb{R}$ is a root of multiplicity $m \\in \\mathbb{N}$ of a function $f:\\mathbb{R}\\to\\mathbb{R}$ if $f(\\alpha)=0$, $f'(\\alpha)=0$, $\\dots$, $f^{(m-1)}(\\alpha)=0$, and $f^{(m)}(\\alpha)\\neq 0$.\n- Newton’s method for approximating a simple root of a differentiable function $f$ is defined by the iteration $x_{k+1} = x_k - \\dfrac{f(x_k)}{f'(x_k)}$ for $k \\ge 0$.\n- Given a sequence $\\{x_k\\}$ converging to $\\alpha$, define the error $e_k = |x_k - \\alpha|$. The method is said to converge linearly if there exists a constant $c \\in (0,1)$ such that $\\displaystyle \\lim_{k\\to\\infty} \\dfrac{e_{k+1}}{e_k} = c$, and to converge with order $p0$ if $\\displaystyle \\lim_{k\\to\\infty} \\dfrac{e_{k+1}}{e_k^p} = \\lambda$ for some $\\lambda \\in (0,\\infty)$. An empirical estimator of the order is\n$$\np_k = \\dfrac{\\ln\\!\\left(\\dfrac{e_{k+1}}{e_k}\\right)}{\\ln\\!\\left(\\dfrac{e_k}{e_{k-1}}\\right)} \\quad \\text{for } k \\ge 1,\n$$\nwhere $\\ln$ denotes the natural logarithm.\n\nTasks:\n1) Provide one explicit function $f$ with a root of multiplicity $3$. Justify multiplicity by the above definition.\n2) Implement Newton’s method for $f$ and compute the sequence of errors $e_k$ for several initial guesses. Use these errors to estimate:\n   - The asymptotic error ratio $r_k = \\dfrac{e_{k+1}}{e_k}$ as $k$ increases.\n   - The empirical order $p_k$ as defined above.\n3) Demonstrate numerically that Newton’s method converges only linearly for a triple root by showing that $r_k$ stabilizes near a constant strictly between $0$ and $1$, and that $p_k$ stabilizes near $1$.\n\nFor reproducibility and assessment, use the following test suite of parameter sets. In each case, you are given $f$, $f'$, the true root $\\alpha$, an initial guess $x_0$, and a fixed iteration count $N$:\n- Test A (exact factorization case): $f(x) = (x-1)^3$, $f'(x) = 3(x-1)^2$, $\\alpha = 1$, $x_0 = 2$, $N = 6$.\n- Test B (nontrivial factor case): $f(x) = (x-1)^3 (x^2+1)$, $f'(x) = 3(x-1)^2 (x^2+1) + (x-1)^3 (2x)$, $\\alpha = 1$, $x_0 = 1.7$, $N = 10$.\n- Test C (analytic nonpolynomial factor case): $f(x) = (x+0.5)^3 e^x$, $f'(x) = e^x\\left(3(x+0.5)^2 + (x+0.5)^3\\right)$, $\\alpha = -0.5$, $x_0 = 0$, $N = 12$.\n\nComputation and output requirements:\n- For each test case, run exactly $N$ iterations of Newton’s method starting from $x_0$, collecting the error sequence $e_k = |x_k - \\alpha|$ for $k=0,1,\\dots,N$. Compute the last ratio $r_{N-1} = \\dfrac{e_N}{e_{N-1}}$ and the last order estimate $p_{N-1} = \\dfrac{\\ln(e_N/e_{N-1})}{\\ln(e_{N-1}/e_{N-2})}$.\n- Your program should output a single line containing a comma-separated flat list with the values $[r_A, p_A, r_B, p_B, r_C, p_C]$, where $r_{\\cdot}$ and $p_{\\cdot}$ are the final ratio and final order estimate for each test case, respectively.\n- Output all numbers as floating-point values. For clarity and reproducibility, round each to $6$ decimal places before printing.\n- No physical units are involved, and no angles are used.\n\nYour program must be a complete, runnable program that performs all computations with the specified test suite and produces the required single-line output format: a single list on one line like $[r_A,p_A,r_B,p_B,r_C,p_C]$.",
            "solution": "The problem requires a theoretical and numerical analysis of Newton's method when applied to a function with a root of multiplicity greater than one, specifically for a multiplicity of $m=3$. We will first establish the theoretical basis for the expected linear convergence, then verify the multiplicity of a sample function, and finally design and execute a numerical experiment to quantify the convergence behavior for the provided test cases.\n\n**1. Theoretical Analysis of Convergence**\n\nNewton's method is an iterative scheme defined by $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$. This can be expressed as a fixed-point iteration $x_{k+1} = g(x_k)$, where the iteration function is $g(x) = x - \\frac{f(x)}{f'(x)}$. The convergence behavior near a root $\\alpha$ is determined by the derivative of $g(x)$ at $\\alpha$. If $|g'(\\alpha)|  1$, the iteration converges locally to $\\alpha$. The rate of convergence is linear if $g'(\\alpha) \\neq 0$ and at least quadratic if $g'(\\alpha) = 0$.\n\nLet $\\alpha$ be a root of multiplicity $m \\in \\mathbb{N}$ with $m  1$. By definition, the function $f(x)$ can be written in the form $f(x) = (x-\\alpha)^m h(x)$, where $h(x)$ is a function such that $h(\\alpha) \\neq 0$. Taking the derivative of $f(x)$ gives:\n$$\nf'(x) = m(x-\\alpha)^{m-1}h(x) + (x-\\alpha)^m h'(x)\n$$\nSubstituting these into the expression for $g(x)$:\n$$\ng(x) = x - \\frac{(x-\\alpha)^m h(x)}{m(x-\\alpha)^{m-1}h(x) + (x-\\alpha)^m h'(x)} = x - \\frac{(x-\\alpha)h(x)}{m h(x) + (x-\\alpha)h'(x)}\n$$\nTo find the convergence rate, we compute $g'(x)$:\n$$\ng'(x) = 1 - \\frac{[h(x) + (x-\\alpha)h'(x)][m h(x) + (x-\\alpha)h'(x)] - [(x-\\alpha)h(x)][m h'(x) + h'(x) + (x-\\alpha)h''(x)]}{[m h(x) + (x-\\alpha)h'(x)]^2}\n$$\nEvaluating the limit as $x \\to \\alpha$:\n$$\n\\lim_{x\\to\\alpha} g'(x) = g'(\\alpha) = 1 - \\frac{[h(\\alpha) + 0][m h(\\alpha) + 0] - [0]}{[m h(\\alpha) + 0]^2} = 1 - \\frac{m h(\\alpha)^2}{m^2 h(\\alpha)^2} = 1 - \\frac{1}{m}\n$$\nThus, for a root of multiplicity $m  1$, Newton's method converges linearly with an asymptotic error ratio of $c = |g'(\\alpha)| = |1 - 1/m|$. Since $m  1$, we have $0  c  1$.\n\nIn this problem, we are specifically interested in a root of multiplicity $m=3$. The theoretical asymptotic error ratio is therefore:\n$$\nc = 1 - \\frac{1}{3} = \\frac{2}{3}\n$$\nThe convergence order is $p=1$, indicating linear convergence.\n\n**2. Justification of Root Multiplicity**\n\nThe problem asks to provide and justify one function with a root of multiplicity $3$. We will use the function from Test A: $f(x) = (x-1)^3$. The root is $\\alpha=1$. According to the provided definition, we must verify that $f(\\alpha)=f'(\\alpha)=f''(\\alpha)=0$ and $f'''(\\alpha)\\neq 0$.\n\nThe function and its successive derivatives are:\n- $f(x) = (x-1)^3$\n- $f'(x) = 3(x-1)^2$\n- $f''(x) = 6(x-1)$\n- $f'''(x) = 6$\n\nEvaluating these at the root $\\alpha=1$:\n- $f(1) = (1-1)^3 = 0$\n- $f'(1) = 3(1-1)^2 = 0$\n- $f''(1) = 6(1-1) = 0$\n- $f'''(1) = 6 \\neq 0$\n\nSince the first two derivatives are zero at $\\alpha=1$ and the third derivative is non-zero, this confirms that $\\alpha=1$ is a root of $f(x)$ with multiplicity $m=3$. The other provided functions can be verified similarly, as they are constructed in the form $(x-\\alpha)^3 h(x)$ with $h(\\alpha) \\neq 0$.\n\n**3. Numerical Experiment and Expected Results**\n\nWe will implement Newton's method for each of the three test cases (A, B, C). For each case, we start with an initial guess $x_0$ and iterate $N$ times.\nThe process is as follows:\n1. Initialize $k=0$ with $x_k = x_0$. An array will store the errors $e_k = |x_k - \\alpha|$ for $k=0, 1, \\dots, N$.\n2. For $k$ from $0$ to $N-1$, compute the next iterate: $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$.\n3. Calculate and store the corresponding error $e_{k+1} = |x_{k+1} - \\alpha|$.\n4. After $N$ iterations, the error sequence $e_0, e_1, \\dots, e_N$ is available.\n5. Compute the final asymptotic error ratio estimate: $r_{N-1} = \\frac{e_N}{e_{N-1}}$.\n6. Compute the final empirical order of convergence: $p_{N-1} = \\frac{\\ln(e_N/e_{N-1})}{\\ln(e_{N-1}/e_{N-2})}$.\n\nBased on our theoretical analysis, the numerical results should show that for each test case, the value of $r_{N-1}$ approaches the theoretical limit of $2/3 \\approx 0.666667$, and the value of $p_{N-1}$ approaches $1$. This will numerically demonstrate that Newton's method exhibits linear convergence for a triple root, as required. The final implementation will perform these calculations and format the output as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes cases where Newton’s method exhibits linear convergence\n    due to a root of multiplicity 3. The implementation runs three test cases\n    and computes the final asymptotic error ratio and empirical order of convergence.\n    \"\"\"\n    \n    # Test A (exact factorization case)\n    # f(x) = (x-1)^3\n    # f'(x) = 3(x-1)^2\n    # alpha = 1, x_0 = 2, N = 6\n    case_A = {\n        \"f\": lambda x: (x - 1.0)**3,\n        \"fp\": lambda x: 3.0 * (x - 1.0)**2,\n        \"alpha\": 1.0,\n        \"x0\": 2.0,\n        \"N\": 6,\n    }\n\n    # Test B (nontrivial factor case)\n    # f(x) = (x-1)^3 * (x^2+1)\n    # f'(x) = 3(x-1)^2 * (x^2+1) + (x-1)^3 * (2x)\n    # alpha = 1, x_0 = 1.7, N = 10\n    case_B = {\n        \"f\": lambda x: (x - 1.0)**3 * (x**2 + 1.0),\n        \"fp\": lambda x: 3.0 * (x - 1.0)**2 * (x**2 + 1.0) + (x - 1.0)**3 * (2.0 * x),\n        \"alpha\": 1.0,\n        \"x0\": 1.7,\n        \"N\": 10,\n    }\n\n    # Test C (analytic nonpolynomial factor case)\n    # f(x) = (x+0.5)^3 * e^x\n    # f'(x) = e^x * (3(x+0.5)^2 + (x+0.5)^3)\n    # alpha = -0.5, x_0 = 0, N = 12\n    case_C = {\n        \"f\": lambda x: (x + 0.5)**3 * np.exp(x),\n        \"fp\": lambda x: np.exp(x) * (3.0 * (x + 0.5)**2 + (x + 0.5)**3),\n        \"alpha\": -0.5,\n        \"x0\": 0.0,\n        \"N\": 12,\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    results = []\n\n    for case in test_cases:\n        f, fp = case[\"f\"], case[\"fp\"]\n        alpha, x0, N = case[\"alpha\"], case[\"x0\"], case[\"N\"]\n        \n        # Array to store the sequence of errors e_k = |x_k - alpha|\n        # Size is N+1 for e_0, e_1, ..., e_N\n        errors = np.zeros(N + 1, dtype=float)\n        \n        x_k = float(x0)\n        errors[0] = np.abs(x_k - alpha)\n        \n        # Perform N iterations of Newton's method\n        for k in range(N):\n            f_val = f(x_k)\n            fp_val = fp(x_k)\n            \n            # Newton's iteration step\n            # Note: Given problem setup ensures fp_val is not zero away from the root.\n            x_k = x_k - f_val / fp_val\n            \n            # Store the error of the new iterate\n            errors[k + 1] = np.abs(x_k - alpha)\n            \n        # The last three errors needed for calculations are e_{N-2}, e_{N-1}, e_N\n        e_N = errors[N]\n        e_N_minus_1 = errors[N - 1]\n        e_N_minus_2 = errors[N - 2]\n\n        # Calculate the final asymptotic error ratio r_{N-1}\n        # r_{N-1} = e_N / e_{N-1}\n        r_final = e_N / e_N_minus_1\n        \n        # Calculate the final empirical order of convergence p_{N-1}\n        # p_{N-1} = ln(e_N / e_{N-1}) / ln(e_{N-1} / e_{N-2})\n        # numpy.log is the natural logarithm (ln)\n        p_final = np.log(e_N / e_N_minus_1) / np.log(e_N_minus_1 / e_N_minus_2)\n        \n        results.extend([r_final, p_final])\n\n    # Final print statement in the exact required format.\n    # Output is a flat list [r_A, p_A, r_B, p_B, r_C, p_C]\n    # with each value rounded to 6 decimal places.\n    print(f\"[{','.join([f'{val:.6f}' for val in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The False Position Method, or Regula Falsi, is a classic root-finding algorithm often presented as an intelligent enhancement of the bisection method. While it can converge faster, it possesses a significant vulnerability known as stagnation, where one bracketing endpoint remains fixed, causing the convergence to become linear and sometimes extremely slow. This practice challenges you to implement the method and observe this pathological behavior firsthand, empirically measuring the linear convergence constant to understand how the geometry of the function can drastically impact algorithmic performance .",
            "id": "3265240",
            "problem": "Consider the False Position Method (Regula Falsi), which, given a continuous function $f$ on an interval $[a,b]$ with $f(a)  0  f(b)$, constructs at each iteration a new point $c$ given by the intersection of the secant line through $(a,f(a))$ and $(b,f(b))$ with the $x$-axis, and then replaces either $a$ or $b$ to maintain the bracketing condition. Let the true root be $r$, and define the error sequence $e_k = |x_k - r|$ associated with a chosen sequence of approximations $\\{x_k\\}$. The rate of convergence is characterized by the following core definitions:\n- Linear convergence: there exists a constant $C \\in (0,1)$ such that $\\lim_{k \\to \\infty} \\frac{e_{k+1}}{e_k} = C$.\n- Superlinear convergence: $\\lim_{k \\to \\infty} \\frac{e_{k+1}}{e_k} = 0$.\n- Quadratic convergence: there exists a constant $M  0$ such that $e_{k+1} \\le M e_k^2$ for all sufficiently large $k$.\n\nYour task is to implement the False Position Method (Regula Falsi) and empirically estimate the asymptotic linear convergence constant $C$ in scenarios where the method exhibits stagnation due to highly imbalanced endpoint function values. Specifically, use the sequence of secant-intersection iterates $\\{c_k\\}$ as the approximations $x_k$, compute the error sequence $e_k = |c_k - r|$ using the analytically known root $r$, and estimate $C$ by averaging the ratios $\\frac{e_{k+1}}{e_k}$ over the final portion of the iteration history. The estimation procedure must be deterministic and reproducible.\n\nUse the following scientifically sound test suite, each case defined by an exponential function with a known root and an interval chosen to induce varying degrees of stagnation:\n\n- Test case $1$ (extreme stagnation, large positive endpoint value):\n  - Function: $f(x) = e^{x} - (1 + \\delta)$ with $\\delta = 10^{-6}$.\n  - Interval: $[a,b] = [0,20]$.\n  - Iterations: $N = 5000$.\n  - True root: $r = \\ln(1+\\delta)$.\n  - Expected qualitative behavior: linear convergence with constant $C$ extremely close to $1$.\n\n- Test case $2$ (moderate stagnation, smaller positive endpoint value):\n  - Function: $f(x) = e^{x} - (1 + \\delta)$ with $\\delta = 10^{-3}$.\n  - Interval: $[a,b] = [0,5]$.\n  - Iterations: $N = 50$.\n  - True root: $r = \\ln(1+\\delta)$.\n  - Expected qualitative behavior: linear convergence with constant $C$ noticeably below $1$.\n\n- Test case $3$ (numerical precision edge case):\n  - Function: $f(x) = e^{x} - (1 + \\delta)$ with $\\delta = 10^{-12}$.\n  - Interval: $[a,b] = [0,20]$.\n  - Iterations: $N = 5000$.\n  - True root: $r = \\ln(1+\\delta)$.\n  - Expected qualitative behavior: empirical constant $C$ effectively equal to $1$ within double-precision floating-point resolution due to step sizes that are smaller than machine epsilon.\n\nImplementation requirements:\n- Use the standard False Position update. At each iteration, compute\n  $$ c = \\frac{a \\, f(b) - b \\, f(a)}{f(b) - f(a)}, $$\n  and replace $a$ by $c$ if $f(a) \\cdot f(c)  0$, otherwise replace $b$ by $c$. Accumulate the sequence $\\{c_k\\}$.\n- For each test case, compute the error sequence $e_k = |c_k - r|$, form the ratios $\\rho_k = \\frac{e_{k+1}}{e_k}$ for $k$ where $e_k  0$, and estimate $C$ as the median of the last $m$ ratios, where $m$ should be chosen as a fixed positive integer that is small relative to $N$ but large enough to smooth fluctuations.\n- Express all final answers as dimensionless floating-point numbers. No physical units are involved.\n\nFinal output specification:\n- Your program should produce a single line of output containing the estimated constants for the three test cases as a comma-separated list enclosed in square brackets, for example, $[C_1,C_2,C_3]$. Each $C_i$ must be a floating-point number.\n\nDesign for coverage:\n- The test suite covers the happy-path case (moderate stagnation), the extreme stagnation case (linear $C$ very close to $1$), and a numerical edge case where stagnation interacts with floating-point precision.\n\nYour program must be self-contained and must not read any input. It must strictly adhere to the specified output format.",
            "solution": "The problem statement is assessed to be valid. It is a well-posed, scientifically grounded problem in numerical analysis, free from contradictions, ambiguities, or factual errors. The problem asks for the empirical estimation of the linear convergence constant for the False Position Method under conditions of stagnation, which is a standard topic in scientific computing. The provided test cases are canonical examples used to illustrate this specific behavior.\n\nThe solution proceeds by first implementing the False Position Method and then applying a specified statistical procedure to estimate the convergence constant from the sequence of generated iterates.\n\nThe False Position Method is an iterative root-finding algorithm for a continuous function $f(x)$ on an interval $[a, b]$ where $f(a)$ and $f(b)$ have opposite signs. At each iteration $k$, a new approximation of the root, $c_k$, is computed as the $x$-intercept of the secant line connecting the points $(a_k, f(a_k))$ and $(b_k, f(b_k))$. The formula for $c_k$ is:\n$$\nc_k = \\frac{a_k f(b_k) - b_k f(a_k)}{f(b_k) - f(a_k)}\n$$\nThe interval for the next iteration, $[a_{k+1}, b_{k+1}]$, is then chosen by retaining $c_k$ and one of the previous endpoints, $a_k$ or $b_k$, such that the new interval continues to bracket the root. The update rule specified is to replace $a_k$ with $c_k$ if $f(a_k) \\cdot f(c_k)  0$ (i.e., they have the same sign), otherwise $b_k$ is replaced by $c_k$.\n\nFor the function $f(x) = e^{x} - (1 + \\delta)$ with $\\delta  0$, the second derivative is $f''(x) = e^x  0$, meaning the function is strictly convex. For the given intervals $[a, b]$, where $a=0$ and $b  0$, we have $f(a)  0$ and $f(b)  0$. Due to the convexity, the secant line always lies above the function graph, and its $x$-intercept $c_k$ will always be to the left of the true root $r$. This implies $f(c_k)  0$. Since the initial $f(a_0)=f(0)$ is also negative, the condition $f(a_k) \\cdot f(c_k)  0$ will be met in every iteration. Consequently, the left endpoint is always updated, $a_{k+1} = c_k$, while the right endpoint $b$ remains fixed throughout the process. This phenomenon, known as stagnation, degrades the method's convergence rate from superlinear to linear.\n\nThe rate of convergence is characterized by the behavior of the error sequence $e_k = |x_k - r|$, where $r$ is the true root and $x_k$ is the sequence of approximations. For this problem, we use the secant intersects, so $x_k = c_k$. Linear convergence implies that for large $k$:\n$$\ne_{k+1} \\approx C \\cdot e_k \\quad \\implies \\quad \\frac{e_{k+1}}{e_k} \\approx C\n$$\nwhere $C \\in (0,1)$ is the asymptotic linear convergence constant.\n\nTo empirically estimate $C$ for each test case, the following algorithm is implemented:\n$1$. The False Position Method is run for a specified number of iterations, $N$, for each test case configuration $(\\delta, [a, b], N)$. The sequence of iterates $\\{c_k\\}_{k=0}^{N-1}$ is stored.\n$2$. The true root $r$ is calculated analytically as $r = \\ln(1 + \\delta)$.\n$3$. The error sequence $e_k = |c_k - r|$ is computed for $k = 0, 1, \\dots, N-1$.\n$4$. A sequence of ratios $\\rho_k = e_{k+1}/e_k$ is generated for all $k$ where $e_k  0$. This avoids division by zero.\n$5$. The constant $C$ is estimated as the median of the last $m$ computed ratios. The median is used to provide a robust estimate that is insensitive to transient fluctuations that might occur before the iterates enter the fully asymptotic regime. A fixed value of $m=10$ is chosen, which is small relative to the number of iterations in the test cases, yet sufficient to average out noise.\n\nThis procedure is applied to three distinct test cases designed to probe different aspects of the method's behavior:\n- **Case 1:** Extreme stagnation ($b=20, \\delta=10^{-6}$). The large value of $f(b)$ relative to $|f(a)|$ leads to a very slow convergence, with an expected value of $C$ extremely close to $1$.\n- **Case 2:** Moderate stagnation ($b=5, \\delta=10^{-3}$). The endpoint imbalance is less severe, resulting in faster (but still linear) convergence and a constant $C$ that is noticeably less than $1$.\n- **Case 3:** Numerical precision limit ($b=20, \\delta=10^{-12}$). The true root $r \\approx 10^{-12}$ is small, and the convergence constant $C$ is theoretically close to $1$. The update step for the iterate $a_k$, which is approximately $a_{k+1} \\approx a_k - \\frac{b (a_k-r) f'(r)}{f(b)}$, becomes so small that for $a_k$ near $r$, the change is smaller than the machine precision relative to $a_k$. This causes $a_k$ to stagnate numerically ($a_{k+1}$ becomes computationally indistinguishable from $a_k$), leading to a constant error $e_k$ for large $k$. The ratio $e_{k+1}/e_k$ thus becomes exactly $1$, and the median of the final ratios will be $1$.\n\nThe implementation will systematically execute these steps for each case and report the estimated constant $C$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the False Position Method to estimate the linear convergence\n    constant C for three test cases demonstrating stagnation.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (extreme stagnation)\n        {'delta': 1e-6, 'interval': [0.0, 20.0], 'N': 5000},\n        # Test case 2 (moderate stagnation)\n        {'delta': 1e-3, 'interval': [0.0, 5.0], 'N': 50},\n        # Test case 3 (numerical precision edge case)\n        {'delta': 1e-12, 'interval': [0.0, 20.0], 'N': 5000},\n    ]\n\n    results = []\n    # m is the number of final ratios to use for the median calculation\n    m = 10 \n\n    for case in test_cases:\n        delta = case['delta']\n        a, b = case['interval']\n        N = case['N']\n\n        # Define the function f(x) for the current case\n        f = lambda x: np.exp(x) - (1.0 + delta)\n\n        # Calculate the true root\n        r = np.log(1.0 + delta)\n\n        c_k_sequence = []\n        \n        # Ensure initial interval is valid\n        f_a = f(a)\n        f_b = f(b)\n        \n        if f_a * f_b >= 0:\n            # This should not happen for the given test cases\n            # but is good practice for a general implementation.\n            raise ValueError(\"Function has the same sign at interval endpoints.\")\n\n        # False Position Method iteration\n        for _ in range(N):\n            # The check for (f_b - f_a == 0) is omitted as it is highly\n            # unlikely for these continuous, non-constant functions.\n            c = (a * f_b - b * f_a) / (f_b - f_a)\n            c_k_sequence.append(c)\n            \n            f_c = f(c)\n\n            # Update the interval\n            if f_a * f_c > 0:\n                a = c\n                f_a = f_c\n            else:\n                b = c\n                f_b = f_c\n\n        # Post-processing to estimate C\n        c_k = np.array(c_k_sequence, dtype=np.float64)\n        \n        # Compute the error sequence e_k = |c_k - r|\n        e_k = np.abs(c_k - r)\n        \n        # The sequence of ratios rho_k = e_{k+1} / e_k\n        # We must filter out cases where the denominator e_k is zero.\n        denominators = e_k[:-1]\n        numerators = e_k[1:]\n        \n        # Create a mask to select only elements where the denominator is > 0\n        mask = denominators > 0\n        \n        if np.any(mask):\n            valid_ratios = numerators[mask] / denominators[mask]\n\n            if len(valid_ratios) >= m:\n                # Estimate C as the median of the last m ratios\n                C_estimate = np.median(valid_ratios[-m:])\n            elif len(valid_ratios) > 0:\n                # If there are fewer than m ratios, use all available ones\n                C_estimate = np.median(valid_ratios)\n            else:\n                # No valid ratios could be computed\n                C_estimate = np.nan # Should not happen in these test cases\n        else:\n            C_estimate = np.nan\n\n        results.append(C_estimate)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "After observing how powerful algorithms can be slowed to linear convergence, a crucial question emerges: can we systematically accelerate this convergence? Steffensen's method offers a remarkable solution, capable of transforming a linearly convergent fixed-point iteration into a quadratically convergent one, often without requiring the function's derivative. This hands-on exercise will guide you to implement and compare a basic fixed-point iteration with its Steffensen-accelerated counterpart, allowing you to numerically confirm the dramatic improvement from linear to quadratic convergence and appreciate the power of convergence acceleration techniques .",
            "id": "3265247",
            "problem": "Write a complete program that studies and compares the rates of convergence of a basic fixed-point iteration and its Steffensen-accelerated variant for the function $\\cos(x)$, using angles in radians. You must base your reasoning on the core definition of order of convergence: a sequence $\\{x_k\\}$ converges to $x^\\star$ with order $p \\ge 1$ if there exists a constant $C \\in (0,\\infty)$ such that\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^p} = C.\n$$\nThe problem is to implement the following, starting from this definition and without using any external data sources.\n\nTask:\n- Consider the fixed-point mapping $g(x) = \\cos(x)$ and the fixed-point iteration $x_{k+1} = g(x_k)$. This is known to converge linearly to the unique fixed point $x^\\star$ of $g$, that is, the solution of $x = \\cos(x)$.\n- Apply Steffensen’s method to the same mapping $g$, which, at a current iterate $x_k$, uses only function values of $g$ and is given by the algebraic update\n$$\nx_{k+1} = x_k - \\frac{\\left(g(x_k) - x_k\\right)^2}{g(g(x_k)) - 2 g(x_k) + x_k},\n$$\nwhenever the denominator is nonzero. If the denominator equals $0$ or is numerically indistinguishable from $0$ within machine precision, then for that step you must fall back to the plain fixed-point update $x_{k+1} = g(x_k)$.\n- Use the observed-order estimator computed from three consecutive errors with respect to a high-accuracy reference $x^\\star$. For errors $e_k = |x_k - x^\\star|$, define\n$$\np_k = \\frac{\\ln\\left(e_{k} / e_{k-1}\\right)}{\\ln\\left(e_{k-1} / e_{k-2}\\right)},\n$$\nwhenever the logarithms are defined. Use the latest available $p_k$ that is computed from errors not too close to floating-point underflow; specifically, prefer the largest index $k$ such that $e_{k-2} \\ge 10^{-10}$; if no such triple exists, use the largest valid $p_k$ available.\n- Use a stopping criterion based on the absolute error with respect to $x^\\star$ falling below a tolerance, or a maximum iteration cap, whichever occurs first.\n\nReference solution $x^\\star$:\n- Internally compute a high-accuracy reference for $x^\\star$ as the solution to $f(x) = \\cos(x) - x = 0$ by applying Newton’s method\n$$\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}, \\quad f'(x) = -\\sin(x) - 1,\n$$\nstarting from $x_0 = 0.7$ and iterating until the Newton update magnitude is less than $10^{-16}$ or a maximum of $100$ iterations is reached. Use this internally computed $x^\\star$ for all error calculations. Angles are in radians.\n\nImplementation and numerical details:\n- Implement two solvers that generate sequences $\\{x_k\\}$:\n  1. The plain fixed-point iteration $x_{k+1} = \\cos(x_k)$.\n  2. The Steffensen-accelerated iteration defined above.\n- For both solvers, stop when $|x_k - x^\\star|  10^{-14}$ or after $200$ iterations, whichever happens first.\n- For Steffensen’s method, if the denominator $g(g(x_k)) - 2 g(x_k) + x_k$ equals $0$ in exact arithmetic or has absolute value below $10^{-14}$ in floating-point arithmetic, use the fallback $x_{k+1} = g(x_k)$ for that step.\n- For each produced sequence, compute the observed order using the $p_k$ estimator defined above, and report a single estimate per sequence as specified earlier.\n\nTest suite:\n- Use the following four initial guesses for both methods: $x_0 \\in \\{0.1, 1.0, 2.0, -1.0\\}$.\n- For each $x_0$, run both the plain fixed-point iteration and Steffensen’s method. For each sequence, return the observed order estimate rounded to $2$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the $8$ results as a comma-separated list enclosed in square brackets, ordered as\n$$\n[\\text{plain}(0.1),\\ \\text{steff}(0.1),\\ \\text{plain}(1.0),\\ \\text{steff}(1.0),\\ \\text{plain}(2.0),\\ \\text{steff}(2.0),\\ \\text{plain}(-1.0),\\ \\text{steff}(-1.0)].\n$$\n- Each entry is the observed order estimate rounded to $2$ decimal places and must be printed as a floating-point number.\n- No user input is required. No physical units are involved. Angles are in radians. The single-line output must exactly follow the format above, for example: $[1.00,2.00,1.00,2.00,1.00,2.00,1.00,2.00]$.",
            "solution": "The problem requires a comparative study of the convergence rates for a standard fixed-point iteration and its Steffensen-accelerated counterpart. The analysis will be performed on the function $g(x) = \\cos(x)$, with all calculations using angles in radians. The core of the task is to implement both methods, generate sequences of iterates, and then numerically estimate the order of convergence for each sequence.\n\n### Theoretical Framework\n\nA fixed point of a function $g(x)$ is a value $x^\\star$ such that $x^\\star = g(x^\\star)$. For the given function $g(x) = \\cos(x)$, the fixed point is the unique solution to the equation $x = \\cos(x)$, also known as the Dottie number.\n\n**1. Plain Fixed-Point Iteration**\n\nThe basic fixed-point iteration is defined by the sequence $x_{k+1} = g(x_k)$, for $k=0, 1, 2, \\dots$. The convergence of this method is governed by the properties of the function $g(x)$ in the neighborhood of the fixed point $x^\\star$. According to the Fixed-Point Theorem, if $|g'(x^\\star)|  1$, the iteration will converge linearly for any starting guess $x_0$ sufficiently close to $x^\\star$.\n\nFor $g(x) = \\cos(x)$, the derivative is $g'(x) = -\\sin(x)$. The fixed point $x^\\star$ is approximately $0.739085$. At this point, the derivative is $g'(x^\\star) = -\\sin(x^\\star) \\approx -0.674$. Since $|g'(x^\\star)| \\approx 0.674  1$, the iteration is guaranteed to converge.\n\nThe convergence is linear, which means the error $e_k = |x_k - x^\\star|$ decreases by a roughly constant factor at each step. Formally, a sequence $\\{x_k\\}$ converges to $x^\\star$ with order $p=1$ (linearly) if\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^1} = |g'(x^\\star)| = C,\n$$\nwhere $C \\in (0, 1)$ is the asymptotic error constant. For this problem, we expect the estimated order of convergence to be close to $p=1$.\n\n**2. Steffensen's Method**\n\nSteffensen's method is an acceleration technique that transforms a linearly convergent fixed-point iteration into a quadratically convergent one, without requiring the computation of derivatives. For a given mapping $g(x)$, the update rule is:\n$$\nx_{k+1} = x_k - \\frac{\\left(g(x_k) - x_k\\right)^2}{g(g(x_k)) - 2 g(x_k) + x_k}.\n$$\nThis can be viewed as applying Newton's method to the function $f(x) = g(x) - x = 0$, where the derivative $f'(x) = g'(x) - 1$ is approximated using a finite difference. Under suitable conditions (specifically, $g'(x^\\star) \\neq 1$), Steffensen's method exhibits quadratic convergence.\n\nQuadratic convergence (order $p=2$) means the number of correct significant digits roughly doubles at each iteration. Formally,\n$$\n\\lim_{k \\to \\infty} \\frac{|x_{k+1} - x^\\star|}{|x_k - x^\\star|^2} = C,\n$$\nfor some constant $C \\in (0, \\infty)$. We therefore expect the estimated order of convergence for Steffensen's method to be close to $p=2$. A fallback to the plain fixed-point step $x_{k+1}=g(x_k)$ is necessary if the denominator of the Steffensen update is numerically close to zero, which can occur if $g(x_k)-x_k$ is very small, i.e., when we are already very close to the a solution.\n\n### Implementation Strategy\n\n**1. High-Accuracy Reference Solution, $x^\\star$**\n\nTo calculate errors $e_k = |x_k - x^\\star|$, a highly accurate value for $x^\\star$ is required. This will be computed by applying Newton's method to the equation $f(x) = \\cos(x) - x = 0$. The Newton iteration is\n$$\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} = x_n - \\frac{\\cos(x_n) - x_n}{-\\sin(x_n) - 1}.\n$$\nStarting with $x_0 = 0.7$, we iterate until the update step magnitude $|x_{n+1} - x_n|$ falls below a stringent tolerance of $10^{-16}$.\n\n**2. Iterative Solvers**\n\nTwo solvers will be implemented: one for the plain fixed-point iteration $x_{k+1} = \\cos(x_k)$ and one for Steffensen's method. Both solvers will start from a given initial guess $x_0$ and generate a sequence of iterates $\\{x_k\\}$. Iterations will terminate when the absolute error $|x_k - x^\\star|$ is less than $10^{-14}$ or a maximum of $200$ iterations is reached. Steffensen's solver will incorporate the specified fallback mechanism where if $|g(g(x_k)) - 2g(x_k) + x_k|  10^{-14}$, the simpler update $x_{k+1} = \\cos(x_k)$ is used.\n\n**3. Observed Order of Convergence, $p_k$**\n\nThe order of convergence is estimated numerically from the sequence of errors $\\{e_k\\}$. The formula provided is:\n$$\np_k = \\frac{\\ln\\left(e_{k} / e_{k-1}\\right)}{\\ln\\left(e_{k-1} / e_{k-2}\\right)}.\n$$\nThis formula is derived from the theoretical definition $e_k \\approx C e_{k-1}^p$, which implies $\\ln(e_k) - \\ln(e_{k-1}) \\approx p (\\ln(e_{k-1}) - \\ln(e_{k-2}))$. After generating a sequence, we compute $p_k$ for all possible indices $k$. To obtain a single representative order estimate, we select the value of $p_k$ for the largest index $k$ such that the error $e_{k-2}$ is not excessively small (specifically, $e_{k-2} \\ge 10^{-10}$). This ensures the estimate is taken from the asymptotic regime of convergence but before floating-point precision limits the accuracy of the error ratios. If no such index exists, the last computed value of $p_k$ is used.\n\nThe final program will execute this process for each initial guess $x_0 \\in \\{0.1, 1.0, 2.0, -1.0\\}$, producing eight order estimates in total, which will be formatted and printed as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the numerical experiment and print the final results.\n    \"\"\"\n\n    def compute_reference_solution():\n        \"\"\"\n        Computes a high-accuracy reference solution x* for x = cos(x) using Newton's method.\n        The equation is f(x) = cos(x) - x = 0.\n        The derivative is f'(x) = -sin(x) - 1.\n        \"\"\"\n        x = 0.7  # Initial guess\n        max_iter = 100\n        tolerance = 1e-16\n        \n        for _ in range(max_iter):\n            f_x = np.cos(x) - x\n            fp_x = -np.sin(x) - 1\n            if fp_x == 0:  # Avoid division by zero\n                break\n            update = -f_x / fp_x\n            x += update\n            if np.abs(update)  tolerance:\n                break\n        return x\n\n    def run_iteration(method, x0, x_star):\n        \"\"\"\n        Generates a sequence of iterates for a given method.\n\n        Args:\n            method (str): 'plain' for fixed-point, 'steffensen' for Steffensen's method.\n            x0 (float): The initial guess.\n            x_star (float): The high-accuracy reference solution.\n\n        Returns:\n            list: The history of iterates [x0, x1, ...].\n        \"\"\"\n        x_k = float(x0)\n        history = [x_k]\n        max_iter = 200\n        stop_tol = 1e-14\n        fallback_tol = 1e-14\n\n        for _ in range(max_iter):\n            if np.abs(x_k - x_star)  stop_tol:\n                break\n            \n            if method == 'plain':\n                x_k_plus_1 = np.cos(x_k)\n            elif method == 'steffensen':\n                g_xk = np.cos(x_k)\n                g_g_xk = np.cos(g_xk)\n                denominator = g_g_xk - 2 * g_xk + x_k\n                \n                if np.abs(denominator)  fallback_tol:\n                    # Fallback to plain fixed-point iteration\n                    x_k_plus_1 = g_xk\n                else:\n                    numerator = (g_xk - x_k)**2\n                    x_k_plus_1 = x_k - numerator / denominator\n            else:\n                raise ValueError(\"Unknown method specified.\")\n            \n            x_k = x_k_plus_1\n            history.append(x_k)\n            \n        return history\n\n    def estimate_order(x_history, x_star):\n        \"\"\"\n        Estimates the order of convergence from a sequence of iterates.\n\n        Args:\n            x_history (list): The sequence of iterates.\n            x_star (float): The high-accuracy reference solution.\n\n        Returns:\n            float: The estimated order of convergence.\n        \"\"\"\n        if len(x_history)  3:\n            # Not enough data points to compute even one p_k value.\n            # Based on the problem, this scenario is not expected.\n            return np.nan\n\n        errors = np.abs(np.array(x_history) - x_star)\n        \n        p_k_values = []\n        for k in range(2, len(errors)):\n            e_k2, e_k1, e_k = errors[k-2], errors[k-1], errors[k]\n            \n            # Avoid log(0) and division by zero from the ratios\n            if e_k1 == 0 or e_k2 == 0 or e_k1 == e_k2:\n                continue\n            \n            # Ratios for the logarithms\n            ratio1 = e_k / e_k1\n            ratio2 = e_k1 / e_k2\n\n            # Ensure arguments to log are positive\n            if ratio1 = 0 or ratio2 = 0:\n                continue\n            \n            log_numerator = np.log(ratio1)\n            log_denominator = np.log(ratio2)\n            \n            if log_denominator == 0:\n                continue\n            \n            p_k = log_numerator / log_denominator\n            p_k_values.append((k, p_k))\n\n        if not p_k_values:\n            # Could not compute any p_k.\n            return np.nan\n        \n        # Selection rule: Find the latest p_k where e_{k-2} is not too small.\n        for k, p_k in reversed(p_k_values):\n            if errors[k-2] >= 1e-10:\n                return p_k\n        \n        # Fallback: if no such p_k exists, use the latest one available.\n        return p_k_values[-1][1]\n\n    # Compute the reference solution once\n    x_star = compute_reference_solution()\n\n    test_cases = [0.1, 1.0, 2.0, -1.0]\n    final_results = []\n\n    for x0_val in test_cases:\n        # Plain fixed-point iteration\n        history_plain = run_iteration('plain', x0_val, x_star)\n        order_plain = estimate_order(history_plain, x_star)\n        final_results.append(order_plain)\n\n        # Steffensen-accelerated iteration\n        history_steff = run_iteration('steffensen', x0_val, x_star)\n        order_steff = estimate_order(history_steff, x_star)\n        final_results.append(order_steff)\n\n    # Format the results to 2 decimal places and print\n    formatted_output = [f\"{res:.2f}\" for res in final_results]\n    print(f\"[{','.join(formatted_output)}]\")\n\nsolve()\n```"
        }
    ]
}