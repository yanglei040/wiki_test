{
    "hands_on_practices": [
        {
            "introduction": "要构建更好的算法，我们首先必须理解并量化其误差。第一个实践练习聚焦于一个基本任务——数值微分——来展示一种提高精度的强大技术。通过分析简单前向差分公式的截断误差，我们可以构建一个更复杂的理查森外推法 (Richardson extrapolation)，它能以极少的额外计算量实现更高阶的收敛性。",
            "id": "3204708",
            "problem": "考虑在点 $x_0$ 处数值近似一个光滑函数 $f$ 的导数的任务。将比较两种算法的准确性和效率：一种是简单的前向有限差分，另一种是由前向有限差分构造的 Richardson 外推法。比较将基于绝对误差对步长 $h$ 的经验依赖性，对于足够小的 $h$，该依赖性表示为幂律 $E(h) \\approx C h^p$，其中 $E(h)$ 是绝对误差，$C$ 是一个依赖于 $f$ 和 $x_0$ 的常数，$p$ 是收敛阶。研究将基于导数的定义和光滑函数的泰勒级数展开。\n\n从导数的基本定义 $f'(x_0) = \\lim_{h \\to 0} \\frac{f(x_0+h) - f(x_0)}{h}$ 和 $f$ 在 $x_0$ 附近的泰勒级数展开出发，推导以下方法的截断误差对 $h$ 的主阶依赖性：\n- 前向有限差分 $D_{\\mathrm{fd}}(x_0,h) = \\frac{f(x_0+h) - f(x_0)}{h}$。\n- 由前向差分构造的 Richardson 外推法 $D_{\\mathrm{rich}}(x_0,h) = 2 D_{\\mathrm{fd}}(x_0,\\frac{h}{2}) - D_{\\mathrm{fd}}(x_0,h)$。\n\n然后实现一个程序，对于一组给定的具有已知精确导数的光滑测试函数，为每种方法计算经验收敛阶 $p$ 和由 $r(h) = \\frac{E(h)}{E(h/2)}$ 定义的局部误差比 $r$。在截断误差主导舍入误差的预定 $h$ 值范围内，对数据 $(\\log h, \\log E(h))$ 使用线性最小二乘回归来估计 $p$。\n\n假设角度以弧度为单位测量。不涉及物理单位。所有数学量都必须以无单位方式处理。\n\n测试套件包括以下函数-点对和步长方案：\n- $f_1(x) = \\sin(x)$ 在 $x_0 = 1.3$ 处，步长为 $h_k = 2^{-k}$，其中 $k \\in \\{3,4,5,\\dots,20\\}$。\n- $f_2(x) = e^{x}$ 在 $x_0 = 1.0$ 处，步长为 $h_k = 2^{-k}$，其中 $k \\in \\{3,4,5,\\dots,20\\}$。\n- $f_3(x) = \\frac{1}{1+x^2}$ 在 $x_0 = 0.5$ 处，步长为 $h_k = 2^{-k}$，其中 $k \\in \\{3,4,5,\\dots,20\\}$。\n\n对于每个测试用例：\n- 计算绝对误差 $E_{\\mathrm{fd}}(h_k) = \\left|D_{\\mathrm{fd}}(x_0,h_k) - f'(x_0)\\right|$ 和 $E_{\\mathrm{rich}}(h_k) = \\left|D_{\\mathrm{rich}}(x_0,h_k) - f'(x_0)\\right|$。\n- 使用子集 $k \\in \\{3,4,5,6,7,8,9,10\\}$，通过 $\\log E$ 对 $\\log h$ 的线性最小二乘回归来估计经验收敛阶 $p_{\\mathrm{fd}}$ 和 $p_{\\mathrm{rich}}$。\n- 计算局部误差比 $r_{\\mathrm{fd}} = \\frac{E_{\\mathrm{fd}}(h_8)}{E_{\\mathrm{fd}}(h_9)}$ 和 $r_{\\mathrm{rich}} = \\frac{E_{\\mathrm{rich}}(h_8)}{E_{\\mathrm{rich}}(h_9)}$，其中 $h_8 = 2^{-8}$ 且 $h_9 = 2^{-9}$。\n\n您的程序应生成单行输出，其中包含用方括号括起来且无空格的逗号分隔列表形式的结果。每个测试用例的结果必须格式化为列表 $[p_{\\mathrm{fd}},p_{\\mathrm{rich}},r_{\\mathrm{fd}},r_{\\mathrm{rich}}]$，并按上面列出的测试用例的顺序排列。因此，最终输出的形式为 $[[p_{\\mathrm{fd},1},p_{\\mathrm{rich},1},r_{\\mathrm{fd},1},r_{\\mathrm{rich},1}],[p_{\\mathrm{fd},2},p_{\\mathrm{rich},2},r_{\\mathrm{fd},2},r_{\\mathrm{rich},2}],[p_{\\mathrm{fd},3},p_{\\mathrm{rich},3},r_{\\mathrm{fd},3},r_{\\mathrm{rich},3}]]$，其中所有条目均为十进制浮点数。",
            "solution": "用户提供的问题是数值分析中一个明确定义的练习，涉及比较两种数值微分算法：前向有限差分和 Richardson 外推方案。该问题在科学上是合理的、自洽的，并且所有数据和过程都得到了明确的规定。因此，该问题是有效的，我将继续提供完整的解决方案。\n\n解决方案包括两部分：首先，对两种方法的截断误差进行理论推导；其次，解释为验证这些理论发现而设计的数值实现。\n\n### 截断误差的理论分析\n\n分析的核心是光滑函数 $f(x)$ 在点 $x_0$ 附近的泰勒级数展开。对于一个小的位移 $h$，级数为：\n$$\nf(x_0+h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2!}f''(x_0) + \\frac{h^3}{3!}f'''(x_0) + \\dots + \\frac{h^n}{n!}f^{(n)}(x_0) + \\mathcal{O}(h^{n+1})\n$$\n其中 $f^{(n)}(x_0)$ 是 $f$ 在 $x_0$ 处计算的 $n$ 阶导数，而 $\\mathcal{O}(h^{n+1})$ 表示 $h^{n+1}$ 阶及更高阶的项。\n\n#### 1. 前向有限差分 ($D_{\\mathrm{fd}}$)\n\n前向有限差分公式定义为：\n$$\nD_{\\mathrm{fd}}(x_0,h) = \\frac{f(x_0+h) - f(x_0)}{h}\n$$\n为了分析其精度，我们代入 $f(x_0+h)$ 的泰勒级数：\n$$\nD_{\\mathrm{fd}}(x_0,h) = \\frac{1}{h} \\left[ \\left( f(x_0) + hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\mathcal{O}(h^3) \\right) - f(x_0) \\right]\n$$\n通过消去 $f(x_0)$ 并除以 $h$ 来简化表达式：\n$$\nD_{\\mathrm{fd}}(x_0,h) = \\frac{1}{h} \\left[ hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\mathcal{O}(h^3) \\right] = f'(x_0) + \\frac{h}{2}f''(x_0) + \\mathcal{O}(h^2)\n$$\n因此，截断误差 $E_{\\mathrm{trunc}}(h) = D_{\\mathrm{fd}}(x_0,h) - f'(x_0)$ 为：\n$$\nE_{\\mathrm{trunc}}(h) = \\frac{h}{2}f''(x_0) + \\mathcal{O}(h^2)\n$$\n误差的主阶项与 $h^1$ 成正比。因此，前向有限差分法是一阶精确的，其收敛阶为 $p=1$。对于小 $h$，绝对误差 $E(h)$ 近似为 $E(h) \\approx |\\frac{f''(x_0)}{2}| h$。\n\n#### 2. Richardson 外推法 ($D_{\\mathrm{rich}}$)\nRichardson 外推法是一种提高数值方法精度阶数的技术。所提供的具体公式旨在消除前向有限差分法的主阶误差项。\n$$\nD_{\\mathrm{rich}}(x_0,h) = 2 D_{\\mathrm{fd}}(x_0,\\frac{h}{2}) - D_{\\mathrm{fd}}(x_0,h)\n$$\n为了找到其截断误差，我们需要将 $D_{\\mathrm{fd}}$ 的误差展开到更高阶。设 $A(h)$ 为近似值 $D_{\\mathrm{fd}}(x_0, h)$。我们已经确定 $A(h)$ 的误差展开式具有以下形式：\n$$\nA(h) = f'(x_0) + C_1 h + C_2 h^2 + C_3 h^3 + \\dots\n$$\n其中 $C_k = \\frac{f^{(k+1)}(x_0)}{(k+1)!}$。具体来说，$C_1 = \\frac{f''(x_0)}{2}$ 且 $C_2 = \\frac{f'''(x_0)}{6}$。\n\n步长为 $h/2$ 的近似值为：\n$$\nA(\\frac{h}{2}) = f'(x_0) + C_1 \\left(\\frac{h}{2}\\right) + C_2 \\left(\\frac{h}{2}\\right)^2 + \\mathcal{O}(h^3) = f'(x_0) + \\frac{C_1}{2}h + \\frac{C_2}{4}h^2 + \\mathcal{O}(h^3)\n$$\n现在，将这些代入 $D_{\\mathrm{rich}}(x_0,h)$ 的公式中：\n$$\nD_{\\mathrm{rich}}(x_0,h) = 2 \\left( f'(x_0) + \\frac{C_1}{2}h + \\frac{C_2}{4}h^2 + \\mathcal{O}(h^3) \\right) - \\left( f'(x_0) + C_1 h + C_2 h^2 + \\mathcal{O}(h^3) \\right)\n$$\n分配各项可得：\n$$\nD_{\\mathrm{rich}}(x_0,h) = (2f'(x_0) - f'(x_0)) + (C_1 - C_1)h + \\left(\\frac{C_2}{2} - C_2\\right)h^2 + \\mathcal{O}(h^3)\n$$\n$$\nD_{\\mathrm{rich}}(x_0,h) = f'(x_0) - \\frac{C_2}{2}h^2 + \\mathcal{O}(h^3)\n$$\n代回 $C_2$ 的表达式：\n$$\nD_{\\mathrm{rich}}(x_0,h) = f'(x_0) - \\frac{f'''(x_0)}{12}h^2 + \\mathcal{O}(h^3)\n$$\nRichardson 外推法的截断误差为：\n$$\nE_{\\mathrm{trunc}}(h) = D_{\\mathrm{rich}}(x_0,h) - f'(x_0) = - \\frac{f'''(x_0)}{12}h^2 + \\mathcal{O}(h^3)\n$$\n主阶误差项与 $h^2$ 成正比。因此，该方法是二阶精确的，其收敛阶为 $p=2$。对于小 $h$，绝对误差 $E(h)$ 近似为 $E(h) \\approx |\\frac{f'''(x_0)}{12}| h^2$。\n\n### 局部误差比与数值验证\n\n理论关系 $E(h) \\approx C h^p$ 提供了一种经验性验证收敛阶的方法。局部误差比 $r(h)$ 定义为：\n$$\nr(h) = \\frac{E(h)}{E(h/2)} \\approx \\frac{C h^p}{C (h/2)^p} = \\frac{h^p}{h^p / 2^p} = 2^p\n$$\n根据我们的推导，我们期望：\n- 对于前向差分 ($p=1$)：$r(h) \\approx 2^1 = 2$。\n- 对于 Richardson 外推法 ($p=2$)：$r(h) \\approx 2^2 = 4$。\n\n经验收敛阶 $p$ 是从方程 $\\log E(h) \\approx \\log C + p \\log h$ 估计的。这表明 $\\log E$ 和 $\\log h$ 之间存在线性关系，其中斜率是收敛阶 $p$。对一组 $(\\log h, \\log E)$ 数据点进行线性最小二乘回归可提供此斜率的估计值。\n\n该实现将计算每个函数在指定步长范围 $h_k = 2^{-k}$ 内的绝对误差 $E_{\\mathrm{fd}}(h_k)$ 和 $E_{\\mathrm{rich}}(h_k)$。然后，它将对指定的数据子集（$k \\in \\{3, \\dots, 10\\}$）执行回归以找到 $p_{\\mathrm{fd}}$ 和 $p_{\\mathrm{rich}}$，并计算在 $h=h_8=2^{-8}$ 处的局部比率 $r_{\\mathrm{fd}}$ 和 $r_{\\mathrm{rich}}$。结果将按指定格式整理和打印。数值结果预计将与前向差分的理论预测 $p=1, r \\approx 2$ 和 Richardson 外推法的理论预测 $p=2, r \\approx 4$ 非常吻合。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes theoretical and empirical convergence properties for numerical differentiation.\n    \"\"\"\n\n    # Define the test functions and their analytical derivatives\n    def f1(x):\n        return np.sin(x)\n\n    def df1(x):\n        return np.cos(x)\n\n    def f2(x):\n        return np.exp(x)\n\n    def df2(x):\n        return np.exp(x)\n\n    def f3(x):\n        return 1.0 / (1.0 + x**2)\n\n    def df3(x):\n        return -2.0 * x / (1.0 + x**2)**2\n\n    # Define the numerical differentiation algorithms\n    def forward_diff(f, x0, h):\n        \"\"\"Computes the forward finite difference approximation of the derivative.\"\"\"\n        return (f(x0 + h) - f(x0)) / h\n\n    def richardson_extrap(f, x0, h):\n        \"\"\"Computes the Richardson extrapolation from forward differences.\"\"\"\n        # This formula is equivalent to the central difference approximation\n        # (f(x0+h/2) - f(x0-h/2))/h but derived from Richardson's method\n        # D_rich = 2 * D_fd(h/2) - D_fd(h)\n        d_h = forward_diff(f, x0, h)\n        d_h_half = forward_diff(f, x0, h / 2.0)\n        return 2.0 * d_h_half - d_h\n\n    # Define the test cases as specified in the problem statement\n    test_cases = [\n        {'f': f1, 'df': df1, 'x0': 1.3},\n        {'f': f2, 'df': df2, 'x0': 1.0},\n        {'f': f3, 'df': df3, 'x0': 0.5}\n    ]\n\n    all_results = []\n\n    # Define the range of step sizes\n    k_values = np.arange(3, 21)\n    h_values = 2.0**(-k_values)\n\n    # Define the slice of data to be used for regression (k=3 to 10)\n    k_min_reg, k_max_reg = 3, 10\n    reg_start_index = k_min_reg - k_values[0]\n    reg_end_index = k_max_reg - k_values[0] + 1\n    reg_slice = slice(reg_start_index, reg_end_index)\n\n    # Define indices for calculating the local error ratio (k=8, k=9)\n    k8_index = 8 - k_values[0]\n    k9_index = 9 - k_values[0]\n\n\n    for case in test_cases:\n        f = case['f']\n        df = case['df']\n        x0 = case['x0']\n\n        exact_derivative = df(x0)\n\n        # Compute numerical approximations for all h values\n        d_fd = np.array([forward_diff(f, x0, h) for h in h_values])\n        d_rich = np.array([richardson_extrap(f, x0, h) for h in h_values])\n        \n        # Compute absolute errors\n        e_fd = np.abs(d_fd - exact_derivative)\n        e_rich = np.abs(d_rich - exact_derivative)\n\n        # To prevent log(0) errors, replace any zero errors with a very small number.\n        # This is a safeguard; for the given functions, non-zero error is expected.\n        e_fd[e_fd == 0] = np.finfo(float).tiny\n        e_rich[e_rich == 0] = np.finfo(float).tiny\n        \n        # --- Empirical Convergence Order (p) ---\n        # Select the data for the regression analysis\n        h_reg = h_values[reg_slice]\n        e_fd_reg = e_fd[reg_slice]\n        e_rich_reg = e_rich[reg_slice]\n\n        # Prepare data for log-log regression\n        log_h = np.log(h_reg)\n        log_e_fd = np.log(e_fd_reg)\n        log_e_rich = np.log(e_rich_reg)\n\n        # Perform linear least squares regression (polyfit of degree 1)\n        # The slope of the line in the log-log plot is the order of convergence p.\n        p_fd = np.polyfit(log_h, log_e_fd, 1)[0]\n        p_rich = np.polyfit(log_h, log_e_rich, 1)[0]\n\n        # --- Local Error Ratio (r) ---\n        # Compute the ratio of errors for h_8 and h_9\n        r_fd = e_fd[k8_index] / e_fd[k9_index]\n        r_rich = e_rich[k8_index] / e_rich[k9_index]\n\n        all_results.append([p_fd, p_rich, r_fd, r_rich])\n\n    # Format the final output string as a list of lists of floats\n    inner_lists_str = []\n    for sublist in all_results:\n        inner_lists_str.append(f\"[{','.join(map(str, sublist))}]\")\n    \n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "从静态计算转向动态系统，我们现在来探讨效率与稳定性之间的关键权衡。我们将使用简单的前向欧拉法 (Forward Euler method) 来模拟一个经典的捕食者-被捕食者模型。这个练习将生动地展示一个看似无害的选择——时间步长 $h$——会如何产生戏剧性的后果，甚至可能导致物理上不可能的结果，从而凸显了在模拟中谨慎选择参数的必要性。",
            "id": "3204844",
            "problem": "您的任务是研究前向欧拉法应用于经典的 Lotka–Volterra 捕食者-猎物模型时的数值精度和效率。该连续模型由两个耦合的常微分方程（ODE）组成，代表猎物种群 $x(t)$ 和捕食者种群 $y(t)$ 的时间演化：\n$$\n\\frac{dx}{dt} = \\alpha x - \\beta x y,\\quad\n\\frac{dy}{dt} = \\delta x y - \\gamma y,\n$$\n其中参数 $\\alpha  0$, $\\beta  0$, $\\gamma  0$, $\\delta  0$，初始条件为 $x(0) = x_0  0$, $y(0) = y_0  0$。在这些条件下，精确的连续时间动态系统能保持种群的非负性。前向欧拉法是一种一阶显式时间步进格式，它通过从 $t_n = n h$ 到 $t_{n+1} = (n+1) h$ 的步长为 $h  0$ 的步进来近似求解。\n\n您的任务是：\n- 从导数的极限定义和均匀步长 $h$ 的时间离散化概念出发，推导上述 ODE 系统的前向欧拉更新方程。解释为什么尽管真实解保持非负，但较大的步长 $h$ 会导致数值解中出现不符合物理意义的负值。\n- 实现一个程序，给定 $(\\alpha,\\beta,\\gamma,\\delta)$、$(x_0,y_0)$、步长 $h$ 和最终时间 $T$，应用前向欧拉法进行 $N = T/h$ 步计算（假设所有测试用例中 $T/h$ 均为整数）。在每次模拟中，检测任一种群在任何步骤是否变为负值，跟踪 $x_n$ 和 $y_n$ 达到的最小值，并返回最终值 $x_N$ 和 $y_N$ 以及作为计算量代理的步数 $N$。\n- 讨论 $h$ 的选择如何影响精度（通过截断误差）和效率（通过步数），并将其与观察到的不符合物理意义的负种群值的出现与否联系起来。\n\n使用以下测试套件，每个套件由 $(\\alpha,\\beta,\\gamma,\\delta,x_0,y_0,h,T)$ 指定，所有量均为无量纲：\n- 测试 $1$：$(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.1,\\,0.4,\\,0.4,\\,0.1\\,)$, $(x_0,y_0) = (\\,10.0,\\,5.0\\,)$, $h = 0.01$, $T = 10.0$。\n- 测试 $2$：$(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.1,\\,0.4,\\,0.4,\\,0.1\\,)$, $(x_0,y_0) = (\\,10.0,\\,5.0\\,)$, $h = 0.1$, $T = 10.0$。\n- 测试 $3$：$(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.1,\\,0.4,\\,0.4,\\,0.1\\,)$, $(x_0,y_0) = (\\,10.0,\\,5.0\\,)$, $h = 1.2$, $T = 6.0$。\n- 测试 $4$：$(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.0,\\,0.5,\\,1.0,\\,0.05\\,)$, $(x_0,y_0) = (\\,5.0,\\,20.0\\,)$, $h = 0.12$, $T = 1.2$。\n- 边界测试 $5$：$(\\alpha,\\beta,\\gamma,\\delta) = (\\,1.0,\\,1.0,\\,1.0,\\,0.5\\,)$, $(x_0,y_0) = (\\,1.0,\\,2.0\\,)$, $h = 1.0$, $T = 3.0$。\n\n程序输出格式：\n- 对每个测试，生成一个列表 $[\\mathrm{neg}, \\min x, \\min y, x_{\\mathrm{final}}, y_{\\mathrm{final}}, N]$，其中 $\\mathrm{neg}$ 是一个布尔值，指示在任何步骤（包括最后一步）是否出现任何不符合物理意义的负种群值，$\\min x$ 和 $\\min y$ 是在轨迹（包括初始值和最终值）中观察到的 $x_n$ 和 $y_n$ 的最小值，$x_{\\mathrm{final}}$ 和 $y_{\\mathrm{final}}$ 是 $N$ 步后的最终值，而 $N$ 是所采取的总步数。\n- 将所有测试的结果聚合为单行，形式为用方括号括起来的逗号分隔列表。即，您的程序应打印单行，格式为\n$[$result\\_1,result\\_2,result\\_3,result\\_4,result\\_5$]$\n其中每个 result 都是上述列表，布尔值、整数和浮点数采用标准十进制表示法。",
            "solution": "所陈述的问题在科学上是合理的、适定的和客观的。它基于数学生物学（Lotka-Volterra 模型）和数值分析（前向欧拉法）的基本原理，这些都是成熟的研究领域。该问题提供了所有必要的数据，并定义了一个清晰、可验证的任务。这是一个用于研究数值方法特性的典型例子，特别是精度、效率和稳定性之间的权衡。因此，该问题是有效的，我将提供完整的解答。\n\nLotka-Volterra 捕食者-猎物模型是一个由两个耦合的一阶非线性常微分方程（ODE）组成的系统。设 $x(t)$ 为时间 $t$ 时的猎物种群， $y(t)$ 为捕食者种群。该系统由以下方程给出：\n$$\n\\frac{dx}{dt} = \\alpha x - \\beta x y = f(x, y)\n$$\n$$\n\\frac{dy}{dt} = \\delta x y - \\gamma y = g(x, y)\n$$\n参数 $\\alpha, \\beta, \\gamma, \\delta$ 是代表相互作用速率的正实常数。项 $\\alpha x$ 模拟了在没有捕食者的情况下猎物的指数增长。项 $-\\beta x y$ 代表猎物被捕食者消耗的速率。项 $\\delta x y$ 代表捕食者种群因消耗猎物而增长。项 $-\\gamma y$ 代表捕食者的自然死亡。初始条件为 $x(0) = x_0  0$ 和 $y(0) = y_0  0$。精确解的一个重要性质是，如果种群初始为正，那么在所有时间 $t  0$ 内它们都将保持为正。\n\n### 前向欧拉法的推导\n\n前向欧拉法是一种用于近似求解初值问题的数值程序。它源于导数的定义。对于函数 $x(t)$，其导数定义为：\n$$\n\\frac{dx}{dt} = \\lim_{h \\to 0} \\frac{x(t+h) - x(t)}{h}\n$$\n对于一个小的、有限的时间步长 $h  0$，我们可以通过去掉极限来近似导数，这是一种一阶前向差分近似：\n$$\n\\frac{dx}{dt} \\approx \\frac{x(t+h) - x(t)}{h}\n$$\n我们将时间离散化为大小为 $h$ 的步长，使得 $t_n = n h$，其中 $n = 0, 1, 2, \\ldots$。设 $x_n$ 和 $y_n$ 分别是真实解 $x(t_n)$ 和 $y(t_n)$ 的数值近似。因此，$x_n \\approx x(t_n)$ 且 $x_{n+1} \\approx x(t_{n+1}) = x(t_n + h)$。\n\n将此近似代入时间 $t_n$ 的 ODE 系统中，我们得到：\n$$\n\\frac{x_{n+1} - x_n}{h} \\approx \\left.\\frac{dx}{dt}\\right|_{t=t_n} = \\alpha x(t_n) - \\beta x(t_n) y(t_n)\n$$\n$$\n\\frac{y_{n+1} - y_n}{h} \\approx \\left.\\frac{dy}{dt}\\right|_{t=t_n} = \\delta x(t_n) y(t_n) - \\gamma y(t_n)\n$$\n前向欧拉法在当前时间步 $t_n$ 使用已知值 $x_n$ 和 $y_n$ 来评估右侧（即导数）。这产生了以下显式更新规则：\n$$\nx_{n+1} = x_n + h \\cdot f(x_n, y_n) = x_n + h(\\alpha x_n - \\beta x_n y_n)\n$$\n$$\ny_{n+1} = y_n + h \\cdot g(x_n, y_n) = y_n + h(\\delta x_n y_n - \\gamma y_n)\n$$\n这些方程使我们能够从当前时间步的状态 $(x_n, y_n)$ 计算出下一个时间步的状态 $(x_{n+1}, y_{n+1})$，从初始条件 $(x_0, y_0)$ 开始。\n\n### 不符合物理意义的负种群问题\n\n尽管 Lotka-Volterra 方程的精确解保持非负，但前向欧拉法产生的数值解可能变为负值。这是数值不稳定的一种表现。我们可以分析更新方程来看这是如何发生的。\n\n考虑捕食者种群 $y$ 的更新：\n$$\ny_{n+1} = y_n + h(\\delta x_n y_n - \\gamma y_n) = y_n (1 + h(\\delta x_n - \\gamma))\n$$\n如果我们从一个正的种群 $y_n  0$ 开始，更新后的种群 $y_{n+1}$ 将为负，当且仅当括号中的项为负：\n$$\n1 + h(\\delta x_n - \\gamma)  0 \\implies h(\\delta x_n - \\gamma)  -1 \\implies h(\\gamma - \\delta x_n)  1\n$$\n当步长 $h$ 较大时，这个不等式更容易满足。它也取决于状态变量 $x_n$。如果猎物种群 $x_n$ 非常小，以至于 $\\delta x_n$ 远小于 $\\gamma$，那么捕食者种群的人均增长率 $(\\delta x_n - \\gamma)$ 将成为一个大的负数。一个大的步长 $h$ 可能导致更新“过冲”超过 $y=0$ 的值，从而产生不符合物理意义的负种群密度。\n\n对猎物种群 $x$ 进行类似分析可得：\n$$\nx_{n+1} = x_n + h(\\alpha x_n - \\beta x_n y_n) = x_n(1 + h(\\alpha - \\beta y_n))\n$$\n从 $x_n  0$ 开始，$x_{n+1}$ 可能变为负值，如果：\n$$\n1 + h(\\alpha - \\beta y_n)  0 \\implies h(\\beta y_n - \\alpha)  1\n$$\n如果步长 $h$ 较大且捕食者种群 $y_n$ 非常高，导致猎物的增长率为较大的负值，这种情况就可能发生。\n\n### 精度、效率和稳定性\n\n步长 $h$ 的选择涉及精度、效率和稳定性之间的关键权衡。\n- **精度**：前向欧拉法是一种一阶方法，意味着其全局截断误差（在固定时间 $T$ 后数值解与真实解之间的差异）与步长成正比，即 误差 $\\propto O(h)$。较小的 $h$ 会减少误差，并产生对真实动态更准确的近似。\n- **效率**：计算量由达到最终时间 $T$ 所需的总步数 $N$ 决定。由于 $N = T/h$，成本与 $h$ 成反比。较小的 $h$ 需要更多步骤，增加了计算时间，使模拟效率降低。\n- **稳定性**：如上所示，大的步长 $h$ 可能导致数值不稳定，产生物理上无意义的结果，例如负种群。对稳定且物理上合理的解的要求对步长 $h$ 施加了上限。这个上限被称为稳定性条件，它通常依赖于系统参数和状态变量本身，这使得非线性系统的情况变得复杂。\n\n总之，选择 $h$ 需要一种平衡。它必须足够小以确保解是稳定的（例如，保持非负）和达到所需程度的准确性，但又要足够大以保持计算成本的可管理性。所提供的测试用例旨在说明这种权衡：小的 $h$ 值（测试 1）应在较高计算成本（$N=1000$）下保持稳定和准确，而大的 $h$ 值（测试 3、4、5）则为了高效率（低 $N$）而冒着不稳定和显著不准确的风险。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Applies the Forward Euler method to the Lotka-Volterra model for a suite of test cases\n    and analyzes the numerical behavior.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is: (alpha, beta, gamma, delta, x0, y0, h, T)\n    test_cases = [\n        (1.1, 0.4, 0.4, 0.1, 10.0, 5.0, 0.01, 10.0),\n        (1.1, 0.4, 0.4, 0.1, 10.0, 5.0, 0.1, 10.0),\n        (1.1, 0.4, 0.4, 0.1, 10.0, 5.0, 1.2, 6.0),\n        (1.0, 0.5, 1.0, 0.05, 5.0, 20.0, 0.12, 1.2),\n        (1.0, 1.0, 1.0, 0.5, 1.0, 2.0, 1.0, 3.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, beta, gamma, delta, x0, y0, h, T = case\n\n        # The problem statement guarantees T/h is an integer.\n        N = int(round(T / h))\n\n        # Initialize state variables\n        x = x0\n        y = y0\n\n        # Initialize tracking variables\n        min_x = x0\n        min_y = y0\n        neg_occurred = False\n\n        # Time-stepping loop\n        for _ in range(N):\n            # Calculate the next state using Forward Euler update rules\n            x_next = x + h * (alpha * x - beta * x * y)\n            y_next = y + h * (delta * x * y - gamma * y)\n            \n            # Update state\n            x = x_next\n            y = y_next\n\n            # Check for non-physical negative populations\n            if x  0.0 or y  0.0:\n                neg_occurred = True\n\n            # Track the minimum values encountered\n            if x  min_x:\n                min_x = x\n            if y  min_y:\n                min_y = y\n        \n        # In case the initial conditions were the minimums and a negative value\n        # occurred, we must ensure the minimum reflects the negative value.\n        # This is already handled by the loop logic correctly.\n\n        # Store the results for the current test case.\n        # Format: [neg_occurred, min_x, min_y, x_final, y_final, N]\n        result_for_case = [neg_occurred, min_x, min_y, x, y, N]\n        results.append(result_for_case)\n\n    # The final print statement must follow this exact format, with each sublist\n    # represented by its standard string form, joined by commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "我们最后的实践练习将通过在复平面上研究牛顿法 (Newton's method) 求根，深入探索非线性动力学复杂而美丽的世界。你将发现方程 $z^3 - 1 = 0$ 的根的吸引盆 (basins of attraction) 并非简单的区域，而是形成了错综复杂的分形图案。这个练习展示了对初始条件的敏感依赖性这一概念，揭示了对于某些算法，“精度”和“效率”与其问题本身的混沌性质是深度交织在一起的。",
            "id": "3204731",
            "problem": "本题要求您研究牛顿法应用于复值多项式时的算法准确性和效率，并将观察到的吸引盆结构与混沌行为联系起来。此任务的基本依据是在复平面上求解非线性方程 $f(z)=0$ 的标准牛顿法定义。给定一个复值可微函数 $f:\\mathbb{C}\\to\\mathbb{C}$，牛顿法产生一个迭代映射 $g:\\mathbb{C}\\to\\mathbb{C}$，定义为 $g(z)=z-\\dfrac{f(z)}{f'(z)}$。对于每个初始点 $z_0\\in\\mathbb{C}$，生成序列 $z_{n+1}=g(z_n)$，直到满足停止准则。函数 $f$ 的一个根 $z^\\star$ 的吸引盆是所有使得迭代收敛于 $z^\\star$ 的初始点 $z_0$ 的集合。在本题中，考虑 $f(z)=z^3-1$，它在 $\\mathbb{C}$ 中有 3 个不同的根。\n\n实现一个完整的程序，该程序：\n- 处理函数 $f(z)=z^3-1$ 并如上文定义在 $\\mathbb{C}$ 中应用牛顿法。\n- 使用覆盖指定区域的初始点 $z_0=x+iy$ 的矩形网格。对每个初始点，迭代应用 $z_{n+1}=z_n-\\dfrac{f(z_n)}{f'(z_n)}$，直到检测到收敛或达到最大迭代次数。\n- 使用收敛准则 $\\min_k |z_n-r_k|  \\varepsilon$，其中 $\\{r_k\\}_{k=1}^3$ 是 $f$ 的根，$\\varepsilon0$ 是一个指定的容差。仅当执行牛顿更新时，迭代次数才增加。\n- 通过在 $|f'(z_n)| \\le \\delta$ (其中 $\\delta$ 取 $10^{-14}$) 时不执行更新，以及在 $|z_n|  B$ (其中 $B$ 取 $10^6$) 时将点声明为不收敛，来确保数值安全性。\n- 将每个初始点分为 4 类之一：收敛到根 $r_1$、收敛到根 $r_2$、收敛到根 $r_3$ 或不收敛。\n- 为每个测试用例计算以下量化指标：\n    1. 收敛到每个根 $r_1$、$r_2$ 和 $r_3$ 的网格点所占的比例（以小数表示）。\n    2. 不收敛点所占的比例（以小数表示）。\n    3. 一个量化吸引盆边界粗糙度的边界比例代理指标：其分类与至少一个四邻域（上、下、左、右）分类不同的网格点所占的比例。在边界评估中，将不收敛的点视为一个独立的类别；与任何邻居的差异都算作一个边界点。\n    4. 收敛点中牛顿更新的平均次数，以小数形式报告。如果没有点收敛，则报告 $0$。\n- 将算法的准确性（在给定容差下，该方法达到正确根分类的可靠性）和效率（收敛的平均迭代次数及所隐含的计算成本）与观察到的边界比例代理指标联系起来，该指标反映了在分形吸引盆边界附近对初始条件的敏感依赖性。\n\n使用以下测试套件，其中每个用例由 $(x_{\\min},x_{\\max},y_{\\min},y_{\\max},N_x,N_y,\\varepsilon,\\text{max\\_iter})$ 描述：\n- 用例 $1$：$(-2,2,-2,2,80,80,10^{-8},50)$。\n- 用例 $2$：$(-2,2,-2,2,20,20,10^{-6},20)$。\n- 用例 $3$：$(-1.5,1.5,-1.5,1.5,100,100,10^{-12},80)$。\n- 用例 $4$：$(-2,2,-2,2,80,80,10^{-8},2)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例贡献一个包含六个小数的列表，顺序为 $[\\text{frac}_{r_1},\\text{frac}_{r_2},\\text{frac}_{r_3},\\text{frac}_{\\text{non}},\\text{boundary\\_fraction},\\text{avg\\_iters}]$。例如，格式必须是 $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$，所有小数都应在程序中四舍五入到合理的小数位数，以确保输出简洁明了。此任务不涉及任何物理单位或角度单位，所有比例必须以小数形式报告。",
            "solution": "该问题要求对用于寻找复多项式 $f(z) = z^3-1$ 根的牛顿法进行数值研究。我们的任务是实现该算法，将其应用于初始点网格，并分析结果以理解其准确性和效率，特别是与吸引盆的分形性质相关的方面。\n\n### 数学公式\n\n问题的核心是牛顿法，这是一种迭代求根算法。对于一个复值可微函数 $f(z)$，迭代由映射 $g(z)$ 定义：\n$$\nz_{n+1} = g(z_n) = z_n - \\frac{f(z_n)}{f'(z_n)}\n$$\n对于特定多项式 $f(z) = z^3 - 1$，其导数为 $f'(z) = 3z^2$。$f(z)$ 的根是三个单位立方根：\n$$\nr_1 = 1\n$$\n$$\nr_2 = e^{i2\\pi/3} = \\cos\\left(\\frac{2\\pi}{3}\\right) + i\\sin\\left(\\frac{2\\pi}{3}\\right) = -0.5 + i\\frac{\\sqrt{3}}{2}\n$$\n$$\nr_3 = e^{i4\\pi/3} = \\cos\\left(\\frac{4\\pi}{3}\\right) + i\\sin\\left(\\frac{4\\pi}{3}\\right) = -0.5 - i\\frac{\\sqrt{3}}{2}\n$$\n将 $f(z)$ 和 $f'(z)$ 代入牛顿法公式，得到此问题的特定迭代映射：\n$$\nz_{n+1} = z_n - \\frac{z_n^3 - 1}{3z_n^2} = \\frac{3z_n^3 - (z_n^3 - 1)}{3z_n^2} = \\frac{2z_n^3 + 1}{3z_n^2}\n$$\n对于序列 $\\{z_n\\}$ 收敛于特定根 $r_k$ 的所有初始点 $z_0$ 的集合，称为该根的吸引盆。对于此函数，已知这些吸引盆之间的边界本质上是分形的，形成一个 Julia 集。这种分形结构意味着对初始条件的敏感依赖性：无穷小的初始点差异可能导致迭代序列发散到不同的根，且通常是在大量迭代之后。\n\n### 算法设计\n\n该实现将在一个离散的初始点网格 $z_0 = x+iy$ 上模拟牛顿法的行为。为提高效率，我们选择使用 `NumPy` 的向量化方法，这允许对网格上的所有点同时进行计算。\n\n1.  **初始化**：我们在域 $[x_{\\min}, x_{\\max}] \\times [y_{\\min}, y_{\\max}]$ 上定义一个 $N_x \\times N_y$ 的复数矩形网格。初始化三个相同维度的数组：\n    *   `Z`：存储每个网格点的当前复数值 $z_n$。初始时用 $z_0$ 网格填充。\n    *   `classification`：一个整数数组，用于存储每个点的最终结果。初始化为 $0$，表示不收敛。当收敛到相应的根时，它将被更新为 $1$、$2$ 或 $3$。\n    *   `iterations`：一个整数数组，初始化为 $0$，用于计算每个点的牛顿更新次数。\n    *   `active_mask`：一个布尔数组，初始时全为 `True`，用于跟踪哪些点仍在进行迭代。\n\n2.  **迭代过程**：算法的核心是一个最多运行 `max_iter` 步的循环。在每一步中，我们只对标记为 `active` 的点应用逻辑。\n    *   **安全性检查**：在执行更新之前，我们必须识别应从活动集合中移除的点。\n        *   **停滞点**：导数接近于零的点，即 $|f'(z_n)| \\leq \\delta = 10^{-14}$。这种情况发生在 $z=0$ 及其附近，此时该方法是病态的。这些点被标记为非活动状态，并保持不收敛的分类。\n        *   **发散点**：迭代值的模变得过大的点，即 $|z_n|  B = 10^6$。这些点也被标记为非活动状态且不收敛。\n    *   **牛顿更新**：对于安全检查后仍处于活动状态的所有点，执行牛顿步骤：$Z \\leftarrow Z - (Z^3 - 1) / (3Z^2)$。这些特定点的迭代次数增加 $1$。\n    *   **收敛检查**：更新后，我们检查是否有任何活动点已经收敛。对于每个根 $r_k$，我们测试是否满足 $|z_{n+1} - r_k|  \\varepsilon$。如果一个活动点满足此准则，其分类将被设置为 $k$，并被标记为非活动状态。\n\n3.  **终止**：如果达到 `max_iter` 或所有点都变为非活动状态，则循环终止。在最后一次迭代后仍处于活动状态的任何点都被分类为不收敛。\n\n### 指标计算\n\n迭代过程完成后，分析最终的 `classification` 和 `iterations` 数组以计算所需的指标：\n1.  **吸引盆比例 ($\\text{frac}_{r_k}$)**：收敛到每个根的点所占的比例，通过计算每个分类索引（$1, 2, 3$）的出现次数并除以总点数 $N_x \\times N_y$ 来得出。\n2.  **不收敛比例 ($\\text{frac}_{\\text{non}}$)**：分类为 $0$（不收敛）的点所占的比例，计算方法类似。\n3.  **边界比例代理指标**：该指标估计吸引盆边界的复杂性。如果一个点的最终分类与其四个基本邻居（上、下、左、右）中至少一个不同，则该点被视为边界点。该指标是此类边界点的总数除以网格点总数。通过在分类网格上使用向量化逻辑运算可以高效地计算此指标。\n4.  **平均迭代次数**：该指标衡量成功收敛所需的平均计算量。它是所有收敛点的迭代次数总和除以收敛点的数量。如果没有点收敛，则定义为 $0$。\n\n这些指标之间的关系是分析的核心。较高的边界比例表示更复杂（分形）的吸引盆结构。这种复杂性通常会导致更高的平均迭代次数，因为靠近边界的点在收敛前可能会沿着更长、更曲折的路径移动。它也可能增加不收敛点的比例，因为一些混沌轨迹可能无法在 `max_iter` 内稳定下来，或者可能落入临界点。模拟的参数，如网格分辨率和 `max_iter`，会显著影响这些测量指标，较低的参数值可能会掩盖底层动力系统的真实复杂性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_newton_fractal_analysis(params):\n    \"\"\"\n    Performs Newton's method analysis for z^3 - 1 on a grid.\n    \n    Args:\n        params (tuple): A tuple containing the simulation parameters:\n            (xmin, xmax, ymin, ymax, Nx, Ny, epsilon, max_iter).\n\n    Returns:\n        list: A list of six computed metrics:\n            [frac_r1, frac_r2, frac_r3, frac_non, boundary_fraction, avg_iters].\n    \"\"\"\n    xmin, xmax, ymin, ymax, Nx, Ny, epsilon, max_iter = params\n    delta = 1e-14\n    B = 1e6\n    total_points = Nx * Ny\n\n    # 1. Setup grids and initial state\n    x = np.linspace(xmin, xmax, Nx)\n    y = np.linspace(ymin, ymax, Ny)\n    X, Y = np.meshgrid(x, y)\n    Z = X + 1j * Y\n\n    # Roots of z^3 - 1\n    roots = np.array([\n        1.0 + 0.0j,\n        -0.5 + 0.5j * np.sqrt(3),\n        -0.5 - 0.5j * np.sqrt(3)\n    ])\n\n    # State arrays\n    # Classification: 0 for non-convergent, 1-3 for root index\n    classification = np.zeros((Ny, Nx), dtype=np.int32)\n    iterations = np.zeros((Ny, Nx), dtype=np.int32)\n    active_mask = np.ones((Ny, Nx), dtype=bool)\n\n    # 2. Main iterative process\n    for _ in range(max_iter):\n        if not np.any(active_mask):\n            break\n\n        # Select currently active points\n        active_z = Z[active_mask]\n        \n        # Safety checks for active points\n        f_prime_active = 3.0 * active_z**2\n        stalled_local = np.abs(f_prime_active) = delta\n        diverged_local = np.abs(active_z) > B\n        bad_points_local = stalled_local | diverged_local\n        \n        # Create a temporary mask to map local indices back to the global grid\n        temp_bad_mask = active_mask.copy()\n        temp_bad_mask[active_mask] = bad_points_local\n        \n        # Deactivate points that stalled or diverged\n        active_mask[temp_bad_mask] = False\n\n        # Identify points that are still active for an update\n        update_mask = active_mask.copy()\n        if not np.any(update_mask):\n            break\n            \n        # Perform Newton update on the good points\n        Z[update_mask] -= (Z[update_mask]**3 - 1.0) / (3.0 * Z[update_mask]**2)\n        iterations[update_mask] += 1\n        \n        # Check for convergence among the points that were just updated\n        for k_idx, r in enumerate(roots):\n            dist = np.abs(Z[update_mask] - r)\n            converged_local = dist  epsilon\n            \n            # Create another temporary mask for converged points\n            temp_conv_mask = update_mask.copy()\n            # The mask must reflect the subset we are checking\n            temp_conv_mask[update_mask] = converged_local\n            \n            # Update classification and deactivate\n            classification[temp_conv_mask] = k_idx + 1\n            active_mask[temp_conv_mask] = False\n    \n    # 3. Calculate metrics\n    frac_r1 = np.sum(classification == 1) / total_points\n    frac_r2 = np.sum(classification == 2) / total_points\n    frac_r3 = np.sum(classification == 3) / total_points\n    frac_non = 1.0 - (frac_r1 + frac_r2 + frac_r3)\n\n    converged_mask = classification > 0\n    num_converged = np.sum(converged_mask)\n    if num_converged > 0:\n        avg_iters = np.sum(iterations[converged_mask]) / num_converged\n    else:\n        avg_iters = 0.0\n\n    # Calculate boundary fraction using vectorized operations\n    boundary_mask = np.zeros_like(classification, dtype=bool)\n    # Check differences with neighbor below\n    diff_down = classification[:-1, :] != classification[1:, :]\n    boundary_mask[:-1, :] |= diff_down\n    boundary_mask[1:, :] |= diff_down\n    # Check differences with neighbor to the right\n    diff_right = classification[:, :-1] != classification[:, 1:]\n    boundary_mask[:, :-1] |= diff_right\n    boundary_mask[:, 1:] |= diff_right\n    \n    boundary_fraction = np.sum(boundary_mask) / total_points\n\n    return [frac_r1, frac_r2, frac_r3, frac_non, boundary_fraction, avg_iters]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (-2.0, 2.0, -2.0, 2.0, 80, 80, 1e-8, 50),\n        (-2.0, 2.0, -2.0, 2.0, 20, 20, 1e-6, 20),\n        (-1.5, 1.5, -1.5, 1.5, 100, 100, 1e-12, 80),\n        (-2.0, 2.0, -2.0, 2.0, 80, 80, 1e-8, 2),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_results = run_newton_fractal_analysis(case)\n        # Format results for concise output\n        formatted_case_results = [f\"{val:.6f}\" for val in case_results]\n        all_results.append(f\"[{','.join(formatted_case_results)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}