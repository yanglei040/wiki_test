{
    "hands_on_practices": [
        {
            "introduction": "In the world of pure mathematics, addition is associative: $(a+b)+c = a+(b+c)$. However, in the finite world of computer arithmetic, this fundamental property does not always hold. This exercise provides a hands-on demonstration of this surprising fact by exploring how floating-point numbers are represented and manipulated. By summing a list of numbers in different orders, you will uncover the phenomenon of 'swamping,' where small numbers can be lost when added to large ones, leading to significant and often counter-intuitive errors. Understanding this principle is the first step toward writing numerically robust code .",
            "id": "3271349",
            "problem": "Consider a normalized floating-point system with base $\\beta = 10$ and precision $t = 3$ significant digits, using rounding to nearest with ties to even as specified by the Institute of Electrical and Electronics Engineers (IEEE) 754 standard. In this system, every elementary addition is rounded to the nearest representable floating-point number with $3$ significant digits.\n\nLet the list of positive real numbers be\n$$\n\\{10^{6},\\, 2400,\\, 1700,\\, 1600,\\, 1200,\\, 1100,\\, 800,\\, 500,\\, 400\\}.\n$$\nYou will perform two floating-point summations of this list, each time applying rounding after every addition to conform to the floating-point model:\n\n1. Sum from largest to smallest.\n2. Sum from smallest to largest.\n\nStarting from the foundational definition of floating-point representation and the rounding mechanism, explain why the two orders can produce different accumulated rounding errors. Then, carry out both summations exactly within this floating-point system, compare each result to the exact real sum, and compute the ratio $R$ of the absolute error when summing from largest to smallest to the absolute error when summing from smallest to largest.\n\nExpress your final answer for $R$ exactly as a fraction in simplest form. No units are required.",
            "solution": "The problem requires an analysis of floating-point summation order effects within a specified numerical system. The system is a normalized floating-point system with base $\\beta = 10$, precision $t = 3$, and rounding to the nearest representable number, with ties rounded to the number with an even least significant digit.\n\nA number $x$ in this system is represented as $fl(x) = \\pm m \\times \\beta^{e}$, where the mantissa $m$ is a number with $t$ digits of the form $d_1.d_2...d_t$ with $d_1 \\in \\{1, 2, ..., 9\\}$ and $d_i \\in \\{0, 1, ..., 9\\}$ for $i>1$, and $e$ is an integer exponent. For our system, this is $fl(x) = \\pm d_1.d_2d_3 \\times 10^{e}$.\n\nThe core of the problem lies in the properties of floating-point addition. When two floating-point numbers of different magnitudes are added, their exponents must be aligned. Let us consider the addition of a large number $L$ and a small number $s$. If $L = m_L \\times 10^{e_L}$ and $s = m_s \\times 10^{e_s}$ with $e_L > e_s$, the addition is performed as $(m_L + m_s \\times 10^{e_s - e_L}) \\times 10^{e_L}$. If the difference $e_L - e_s$ is large enough, the term $m_s \\times 10^{e_s - e_L}$ becomes so small that it does not affect the first $t$ significant digits of $m_L$. After rounding, the result is simply $fl(L+s) = L$. This phenomenon is known as \"swamping\" or \"absorption,\" and it leads to a loss of information, as the contribution of $s$ is entirely discarded.\n\nSumming a list of numbers from largest to smallest exacerbates this issue. The running sum quickly becomes very large, causing subsequent smaller numbers to be \"swamped\" and their values lost. Conversely, summing from smallest to largest allows smaller numbers to accumulate. The running sum grows more gradually. By the time the largest numbers are added, the accumulated sum of the small numbers may be large enough to be significant relative to the large numbers, thus avoiding or mitigating the loss of information.\n\nFirst, we establish the exact real sum, $S_{exact}$, of the given numbers for error comparison.\nThe list of numbers is $\\{10^{6}, 2400, 1700, 1600, 1200, 1100, 800, 500, 400\\}$.\n$$\nS_{exact} = 1000000 + 2400 + 1700 + 1600 + 1200 + 1100 + 800 + 500 + 400\n$$\n$$\nS_{exact} = 1000000 + 9700 = 1009700\n$$\n\nNow, we perform the two summations in the specified floating-point system. Let $fl(\\cdot)$ denote the rounding operation to a $t=3$ digit mantissa.\n\n**1. Summation from Largest to Smallest**\nThe list is ordered as $\\{10^{6}, 2400, 1700, 1600, 1200, 1100, 800, 500, 400\\}$.\nLet $S_{large,i}$ be the running sum after the $i$-th addition.\nInitial sum: $S_{large,0} = 10^{6} = 1.00 \\times 10^{6}$.\nStep 1: $S_{large,1} = fl(1.00 \\times 10^{6} + 2400) = fl(1.00 \\times 10^{6} + 2.40 \\times 10^{3})$.\nTo add, we align the exponents:\n$1.00 \\times 10^{6} + 0.00240 \\times 10^{6} = 1.00240 \\times 10^{6}$.\nThe mantissa is $1.00240$. To round to $3$ significant digits, we inspect the fourth digit, which is $2$. Since $2 < 5$, we round down.\n$S_{large,1} = 1.00 \\times 10^{6}$.\nThe number $2400$ has been lost due to swamping.\n\nStep 2: $S_{large,2} = fl(1.00 \\times 10^{6} + 1700) = fl(1.00 \\times 10^{6} + 1.70 \\times 10^{3})$.\n$1.00 \\times 10^{6} + 0.00170 \\times 10^{6} = 1.00170 \\times 10^{6}$.\nRounding down gives $S_{large,2} = 1.00 \\times 10^{6}$.\n\nFor any number $x$ in the rest of the list, its magnitude is less than or equal to $2400$. The addition to the running sum $1.00 \\times 10^{6}$ will take the form $fl(1.00 \\times 10^{6} + x)$. The exact sum will be $1.00... \\times 10^{6}$. The fourth significant digit of the mantissa will always be less than $5$, so the result of rounding will always be $1.00 \\times 10^{6}$.\nTherefore, the final sum when adding from largest to smallest is:\n$$\nS_{large\\_first} = 1.00 \\times 10^{6} = 1000000\n$$\n\n**2. Summation from Smallest to Largest**\nThe list is ordered as $\\{400, 500, 800, 1100, 1200, 1600, 1700, 2400, 10^{6}\\}$.\nLet $S_{small,i}$ be the running sum. All numbers are represented with $t=3$ digits.\n$S_{small,0} = fl(400) = 4.00 \\times 10^{2}$.\n$S_{small,1} = fl(400 + 500) = fl(900) = 9.00 \\times 10^{2}$.\n$S_{small,2} = fl(900 + 800) = fl(1700) = 1.70 \\times 10^{3}$.\n$S_{small,3} = fl(1700 + 1100) = fl(2800) = 2.80 \\times 10^{3}$.\n$S_{small,4} = fl(2800 + 1200) = fl(4000) = 4.00 \\times 10^{3}$.\n$S_{small,5} = fl(4000 + 1600) = fl(5600) = 5.60 \\times 10^{3}$.\n$S_{small,6} = fl(5600 + 1700) = fl(7300) = 7.30 \\times 10^{3}$.\n$S_{small,7} = fl(7300 + 2400) = fl(9700) = 9.70 \\times 10^{3}$.\nAll intermediate sums so far are exactly representable in the floating-point system, so no rounding error has been introduced yet.\n\nFinal step:\n$S_{small,8} = fl(9700 + 10^{6}) = fl(9.70 \\times 10^{3} + 1.00 \\times 10^{6})$.\nAlign exponents:\n$0.00970 \\times 10^{6} + 1.00 \\times 10^{6} = 1.00970 \\times 10^{6}$.\nThe exact mantissa is $1.00970$. We must round this to $3$ significant digits. The first three digits are $1.00$. The fourth digit is $9$. Since $9 \\ge 5$, we round up the third digit.\nThe new mantissa is $1.01$.\nThus, the final sum when adding from smallest to largest is:\n$$\nS_{small\\_first} = 1.01 \\times 10^{6} = 1010000\n$$\n\n**3. Error Calculation and Ratio**\nNow we compute the absolute errors for both summation orders.\nAbsolute error for largest-to-smallest summation:\n$$\nE_{large} = |S_{large\\_first} - S_{exact}| = |1000000 - 1009700| = |-9700| = 9700\n$$\nAbsolute error for smallest-to-largest summation:\n$$\nE_{small} = |S_{small\\_first} - S_{exact}| = |1010000 - 1009700| = |300| = 300\n$$\nThe problem asks for the ratio $R$ of these absolute errors.\n$$\nR = \\frac{E_{large}}{E_{small}} = \\frac{9700}{300}\n$$\nSimplifying the fraction gives:\n$$\nR = \\frac{97}{3}\n$$\nThe numerator $97$ is a prime number, and it is not divisible by $3$. Therefore, this fraction is in its simplest form. This result numerically demonstrates that summing from smallest to largest yielded a significantly more accurate result than summing from largest to smallest.",
            "answer": "$$\\boxed{\\frac{97}{3}}$$"
        },
        {
            "introduction": "When we model physical or chemical systems that change over time, we often encounter 'stiffness'â€”a scenario where different processes occur on vastly different timescales. Naively applying simple numerical methods can lead to catastrophic failure, as the step size required for stability is dictated by the fastest, often transient, process, making the simulation prohibitively slow. This practice explores this critical concept by solving a stiff system of ordinary differential equations (ODEs) . By comparing an explicit method (Forward Euler) with an implicit one (Backward Euler), you will see firsthand why the stability properties of an algorithm are just as important as its accuracy.",
            "id": "3271344",
            "problem": "Consider the initial value problem for a linear, dimensionless chemical reaction network described by an ordinary differential equation (ODE) system. Let $y_1$ denote the amount of species $A$ and let $y_2$ denote the amount of species $B$. Species $A$ converts to species $B$ with a fast rate constant $k_1$ (measured in per second), and species $B$ decays with a slow rate constant $k_2$ (measured in per second). The governing equations are\n$$\n\\frac{d}{dt}\\begin{bmatrix} y_1(t) \\\\ y_2(t) \\end{bmatrix}\n=\n\\begin{bmatrix}\n-k_1 & 0 \\\\\nk_1 & -k_2\n\\end{bmatrix}\n\\begin{bmatrix} y_1(t) \\\\ y_2(t) \\end{bmatrix},\n\\quad\n\\begin{bmatrix} y_1(0) \\\\ y_2(0) \\end{bmatrix}\n=\n\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\nYou must write a complete program to numerically approximate the solution at a final time $T$ using two time-stepping methods:\n- The explicit Forward Euler method (first-order explicit), defined by discretizing time with step size $\\Delta t$.\n- The implicit Backward Euler method (first-order implicit), defined by discretizing time with step size $\\Delta t$.\n\nThe foundational starting point is the definition of the initial value problem for an ODE and the principle that the stability of a numerical method applied to a linear system $y' = A y$ is governed by the amplification factors associated with the eigenvalues of the Jacobian matrix $A$. The conceptual goal is to show, by first-principles reasoning and computation, how stiffness arises from widely separated time scales and why an implicit method can remain stable for larger $\\Delta t$ values, while the explicit method fails under those same conditions.\n\nYour program must:\n- For each test case, integrate the system to time $T$ using both methods and return the final values $y_1(T)$ and $y_2(T)$ for both methods.\n- Determine the linear stability of the Forward Euler method by checking the eigenvalues $\\lambda_i$ of the matrix $A$ against the scalar Forward Euler stability condition $|1 + \\Delta t\\,\\lambda_i| < 1$ for all $i$. Return a boolean indicating whether this strict stability condition holds for the given $\\Delta t$.\n- All returned values of $y_1(T)$ and $y_2(T)$ are dimensionless.\n\nUse the following test suite of parameter sets $(k_1, k_2, T, \\Delta t)$:\n- Case $1$: $k_1 = 1000$, $k_2 = 1$, $T = 0.05$, $\\Delta t = 0.0001$.\n- Case $2$: $k_1 = 1000$, $k_2 = 1$, $T = 0.05$, $\\Delta t = 0.01$.\n- Case $3$: $k_1 = 1000$, $k_2 = 1$, $T = 0.05$, $\\Delta t = 0.002$.\n- Case $4$: $k_1 = 50$, $k_2 = 1$, $T = 0.05$, $\\Delta t = 0.01$.\n\nDesign for coverage:\n- Case $1$ is a happy path with small $\\Delta t$ where both methods should perform well.\n- Case $2$ uses a large $\\Delta t$ relative to the fast time scale, designed to make the explicit method unstable while the implicit method remains stable.\n- Case $3$ sets $\\Delta t$ at the explicit stability boundary associated with the fast mode to illustrate non-decaying oscillations and the failure of strict stability.\n- Case $4$ reduces stiffness (smaller $k_1$) so the explicit method is stable at the chosen $\\Delta t$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, return a list in the form $[y_1^{\\text{BE}}(T), y_2^{\\text{BE}}(T), y_1^{\\text{FE}}(T), y_2^{\\text{FE}}(T), \\text{FE\\_stable}]$, where $y_1^{\\text{BE}}(T)$ and $y_2^{\\text{BE}}(T)$ denote the Backward Euler final values, $y_1^{\\text{FE}}(T)$ and $y_2^{\\text{FE}}(T)$ denote the Forward Euler final values, and $\\text{FE\\_stable}$ is a boolean.\n- The final printed line must aggregate the results for all cases into a single list, for example $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$, with no spaces in the output line.\n\nAngles are not involved. All returned concentrations are dimensionless numbers. Rate constants are measured in per second and time in seconds, but you do not need to print units. The output values must be floats or booleans exactly as specified.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded, well-posed, objective, and self-contained. The problem describes a classic stiff ordinary differential equation (ODE) system and asks for its numerical solution using standard methods to demonstrate core concepts in numerical stability. All necessary data and definitions are provided, and there are no contradictions or ambiguities.\n\n### Problem Formulation\n\nThe problem describes an initial value problem (IVP) for a system of two linear, first-order ODEs:\n$$\n\\frac{d\\mathbf{y}(t)}{dt} = A \\mathbf{y}(t), \\quad \\mathbf{y}(0) = \\mathbf{y}_0\n$$\nwhere $\\mathbf{y}(t) = \\begin{bmatrix} y_1(t) \\\\ y_2(t) \\end{bmatrix}$ is the vector of species concentrations, the matrix $A$ is given by\n$$\nA = \\begin{bmatrix} -k_1 & 0 \\\\ k_1 & -k_2 \\end{bmatrix}\n$$\nand the initial condition is\n$$\n\\mathbf{y}_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\nThe parameters $k_1$ and $k_2$ are positive rate constants. The problem specifies that $k_1$ is a \"fast\" rate and $k_2$ is a \"slow\" rate, implying $k_1 \\gg k_2$.\n\n### Analysis of the System and Stiffness\n\nThe behavior of the linear system $\\mathbf{y}' = A\\mathbf{y}$ is governed by the eigenvalues of the matrix $A$. Since $A$ is a lower triangular matrix, its eigenvalues are simply its diagonal entries:\n$$\n\\lambda_1 = -k_1\n$$\n$$\n\\lambda_2 = -k_2\n$$\nThe analytical solution is a linear combination of terms involving $e^{\\lambda_1 t}$ and $e^{\\lambda_2 t}$. Since $k_1, k_2 > 0$, both eigenvalues are real and negative, so the solution components decay over time. The system is stable.\n\nThe term \"stiffness\" in an ODE system arises when there are two or more processes evolving on widely different time scales. In this system, the time scales are related to the inverse of the magnitudes of the eigenvalues: $\\tau_1 = 1/|\\lambda_1| = 1/k_1$ and $\\tau_2 = 1/|\\lambda_2| = 1/k_2$. The condition $k_1 \\gg k_2$ implies $\\tau_1 \\ll \\tau_2$. The \"stiffness ratio\" is given by $|\\lambda_{\\max}|/|\\lambda_{\\min}| = k_1/k_2 \\gg 1$. The presence of a very fast-decaying component (associated with $\\lambda_1 = -k_1$) alongside a slow-decaying component (associated with $\\lambda_2 = -k_2$) is the defining characteristic of this stiff system.\n\n### Numerical Methods and Stability\n\nWe will implement two first-order numerical methods to approximate the solution at discrete time steps $t_n = n\\Delta t$, where $\\Delta t$ is the time step size.\n\n#### Forward Euler (Explicit) Method\n\nThe Forward Euler (FE) method is an explicit method defined by the recurrence relation:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\Delta t f(t_n, \\mathbf{y}_n)\n$$\nFor our linear system, this becomes:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\Delta t (A \\mathbf{y}_n) = (I + \\Delta t A) \\mathbf{y}_n\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. The matrix $G_{FE} = I + \\Delta t A$ is the amplification matrix. The numerical solution remains stable only if the magnitudes of all eigenvalues of $G_{FE}$ are less than or equal to $1$. The eigenvalues of $G_{FE}$ are $1 + \\Delta t \\lambda_i$. The problem specifies a strict stability condition:\n$$\n|1 + \\Delta t \\lambda_i| < 1 \\quad \\text{for all } i\n$$\nSubstituting our eigenvalues $\\lambda_1 = -k_1$ and $\\lambda_2 = -k_2$:\n1.  $|1 - \\Delta t k_1| < 1 \\implies -1 < 1 - \\Delta t k_1 < 1$. This simplifies to $0 < \\Delta t k_1 < 2$, or $\\Delta t < 2/k_1$.\n2.  $|1 - \\Delta t k_2| < 1 \\implies -1 < 1 - \\Delta t k_2 < 1$. This simplifies to $0 < \\Delta t k_2 < 2$, or $\\Delta t < 2/k_2$.\n\nSince $k_1 \\gg k_2$, we have $2/k_1 \\ll 2/k_2$. The stability of the method is therefore constrained by the fastest time scale of the system:\n$$\n\\Delta t < \\frac{2}{k_1}\n$$\nThis is a severe restriction. To maintain stability, the time step $\\Delta t$ must be very small, which makes the method computationally expensive for integrating over long time intervals.\n\n#### Backward Euler (Implicit) Method\n\nThe Backward Euler (BE) method is an implicit method defined by:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\Delta t f(t_{n+1}, \\mathbf{y}_{n+1})\n$$\nFor our linear system, this gives:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\Delta t (A \\mathbf{y}_{n+1})\n$$\nTo find $\\mathbf{y}_{n+1}$, we must solve a linear system:\n$$\n(I - \\Delta t A) \\mathbf{y}_{n+1} = \\mathbf{y}_n\n$$\n$$\n\\mathbf{y}_{n+1} = (I - \\Delta t A)^{-1} \\mathbf{y}_n\n$$\nThe amplification matrix is $G_{BE} = (I - \\Delta t A)^{-1}$. Its eigenvalues are $(1 - \\Delta t \\lambda_i)^{-1}$. For our system's eigenvalues $\\lambda_i < 0$, the stability condition requires $|(1 - \\Delta t \\lambda_i)^{-1}| < 1$.\nSubstituting $\\lambda_i = -k_i$:\n$$\n\\left|\\frac{1}{1 + \\Delta t k_i}\\right| < 1\n$$\nSince $\\Delta t > 0$ and $k_i > 0$, the denominator $1 + \\Delta t k_i$ is always greater than $1$. Therefore, the magnitude of the amplification factor is always less than $1$ for any choice of $\\Delta t > 0$. The Backward Euler method is unconditionally stable (or A-stable) for this problem, meaning it remains stable regardless of the step size $\\Delta t$. This property makes it far more suitable for stiff systems, as it is not constrained by the fast time scale.\n\n### Algorithm\n\nFor each test case $(k_1, k_2, T, \\Delta t)$:\n1.  Initialize $\\mathbf{y}_{FE} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $\\mathbf{y}_{BE} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n2.  Calculate the number of steps, $N = \\text{round}(T / \\Delta t)$.\n3.  Construct the matrix $A = \\begin{bmatrix} -k_1 & 0 \\\\ k_1 & -k_2 \\end{bmatrix}$.\n4.  Construct the FE update matrix $M_{FE} = I + \\Delta t A$.\n5.  Construct the BE update matrix $M_{BE} = (I - \\Delta t A)^{-1}$.\n6.  Perform $N$ integration steps for both methods:\n    -   For FE: $\\mathbf{y}_{FE} \\leftarrow M_{FE} \\mathbf{y}_{FE}$ for each step.\n    -   For BE: $\\mathbf{y}_{BE} \\leftarrow M_{BE} \\mathbf{y}_{BE}$ for each step.\n7.  Check the strict FE stability condition: boolean `FE_stable` is `True` if $\\Delta t < 2/k_1$, and `False` otherwise.\n8.  Return the final vectors $\\mathbf{y}_{FE}$ and $\\mathbf{y}_{BE}$ and the stability boolean `FE_stable`.\n\nThis procedure will be implemented for all provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a stiff ODE system using Forward and Backward Euler methods\n    and analyzes the stability of the Forward Euler method.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k1, k2, T, dt)\n        (1000.0, 1.0, 0.05, 0.0001), # Case 1: Stable FE\n        (1000.0, 1.0, 0.05, 0.01),   # Case 2: Unstable FE\n        (1000.0, 1.0, 0.05, 0.002),  # Case 3: FE at stability boundary\n        (50.0, 1.0, 0.05, 0.01),     # Case 4: Less stiff, stable FE\n    ]\n\n    all_results = []\n    \n    for k1, k2, T, dt in test_cases:\n        # Define the system matrix A\n        A = np.array([[-k1, 0.0], \n                      [k1, -k2]])\n\n        # Initial condition\n        y0 = np.array([1.0, 0.0])\n\n        # Number of time steps\n        num_steps = int(round(T / dt))\n\n        # --- Forward Euler Method ---\n        y_fe = y0.copy()\n        # Pre-compute the Forward Euler update matrix\n        I = np.identity(2)\n        M_fe = I + dt * A\n        for _ in range(num_steps):\n            y_fe = M_fe @ y_fe\n\n        # --- Backward Euler Method ---\n        y_be = y0.copy()\n        # Pre-compute the Backward Euler update matrix\n        # M_be = inv(I - dt * A)\n        M_be = np.linalg.inv(I - dt * A)\n        for _ in range(num_steps):\n            y_be = M_be @ y_be\n            \n        # --- Forward Euler Stability Check ---\n        # The eigenvalues of A are lambda_1 = -k1 and lambda_2 = -k2.\n        # Stability requires |1 + dt*lambda_i| < 1 for all i.\n        # The condition simplifies to dt < 2/k1.\n        # The problem asks for the result of the strict inequality.\n        fe_stable = dt  (2.0 / k1)\n\n        # Collect results for the current test case\n        case_results = [\n            y_be[0], y_be[1],\n            y_fe[0], y_fe[1],\n            fe_stable\n        ]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is converted to a string\n    # and spaces are removed to match the required output format.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Many fundamental problems in science and engineering, from structural analysis to heat flow, can be modeled by partial differential equations (PDEs). Discretizing these equations often results in enormous systems of linear equations, $A x = b$, where the matrix $A$ is too large to store or invert directly. This practice introduces the Conjugate Gradient method, a powerful and widely used iterative algorithm for solving such large, sparse systems efficiently . You will implement the method without ever forming the full matrix, a key technique in large-scale scientific computing, and see how it can be accelerated with preconditioning.",
            "id": "3271471",
            "problem": "Consider solving a linear system of equations $A x = b$ where $A$ is the discrete negative Laplacian on the unit square with homogeneous Dirichlet boundary conditions, constructed by the standard $5$-point finite difference stencil on a uniform grid of $M \\times M$ interior points. Let the grid spacing be $h = 1/(M+1)$ and index the interior points by $(i,j)$ with $i,j \\in \\{1,2,\\dots,M\\}$. For an interior grid function $u$, the discrete operator acts as\n$$\n(A u)_{i,j} = \\frac{1}{h^2}\\left(4 u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}\\right),\n$$\nwith contributions from points outside the interior treated as zeros due to the Dirichlet boundary. The matrix $A$ is symmetric positive-definite (SPD). The manufactured exact solution is defined by sampling the smooth function $u^*(x,y) = \\sin(\\pi x)\\sin(\\pi y)$ at the interior grid points $(x_i,y_j) = (i h, j h)$. The right-hand side is defined consistently as $b = A x^*$ where $x^*$ is the vectorization of $u^*$ on the interior grid, ensuring that $x^*$ is the exact solution of $A x = b$.\n\nTask: Implement the Conjugate Gradient (CG) method from first principles to solve $A x = b$ for this problem. Do not form $A$ explicitly; instead, implement an efficient matrix-vector product using the $5$-point stencil on the $M \\times M$ grid. Optionally implement diagonal (Jacobi) preconditioning using the diagonal of $A$, which equals $4/h^2$ at every interior point.\n\nFundamental base to be used:\n- The definition of the inner product in Euclidean space, the $\\ell^2$ norm, and the property that an SPD matrix $A$ induces the $A$-inner product $\\langle u, v \\rangle_A = u^\\top A v$ and the $A$-norm $\\|v\\|_A = \\sqrt{v^\\top A v}$.\n- The equivalence of solving $A x = b$ to minimizing the strictly convex quadratic functional $\\phi(x) = \\frac{1}{2} x^\\top A x - b^\\top x$.\n- The iterative principle of searching within Krylov subspaces generated by the residual and $A$.\n\nConvergence criterion: Let $r_k = b - A x_k$ be the residual at iteration $k$. Your implementation must terminate when $\\|r_k\\|_2 \\le \\text{tol} \\cdot \\|r_0\\|_2$ (relative residual tolerance), or when a prescribed maximum number of iterations is reached. Report whether convergence occurred according to this criterion.\n\nOutput specification per test case:\n- The integer number of iterations $k$ performed.\n- The floating-point final residual two-norm $\\|r_k\\|_2$.\n- The floating-point relative solution error $\\|x_k - x^*\\|_2 / \\|x^*\\|_2$.\n- A boolean indicating whether convergence occurred according to the stated criterion.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list in the order $[k,\\|r_k\\|_2,\\|x_k - x^*\\|_2 / \\|x^*\\|_2,\\text{converged}]$, and the overall output must be the list of these lists, for example, $[[k_1,r_1,e_1,c_1],[k_2,r_2,e_2,c_2],\\dots]$.\n\nTest suite:\n- Case $1$ (general, happy path): $M = 64$, $\\text{tol} = 10^{-8}$, $\\text{max\\_iter} = 1000$, no preconditioning.\n- Case $2$ (boundary size): $M = 1$, $\\text{tol} = 10^{-12}$, $\\text{max\\_iter} = 20$, no preconditioning.\n- Case $3$ (limited iterations, potential non-convergence): $M = 128$, $\\text{tol} = 10^{-6}$, $\\text{max\\_iter} = 50$, no preconditioning.\n- Case $4$ (preconditioning performance): $M = 96$, $\\text{tol} = 10^{-10}$, $\\text{max\\_iter} = 1000$, with diagonal (Jacobi) preconditioning.\n\nAll angles, where applicable, must be treated in radians; no physical units are involved. The final answers for the test cases must be expressed as integers, floating-point numbers, and booleans exactly as specified, with no percentage signs.",
            "solution": "The problem presented requires the solution of a linear system of equations, $A x = b$, using the Conjugate Gradient (CG) method. The matrix $A$ represents the discrete negative Laplacian operator on a uniform grid over the unit square, derived from a $5$-point finite difference stencil with homogeneous Dirichlet boundary conditions. This is a classic problem in numerical partial differential equations. The problem is well-posed, scientifically sound, and all parameters are clearly defined.\n\nThe system is defined on a grid of $M \\times M$ interior points. The grid spacing is $h = 1/(M+1)$. An unknown grid function $u$, represented as a vector, is subject to the linear operator $A$. At an interior grid point indexed by $(i,j)$ where $i,j \\in \\{1,2,\\dots,M\\}$, the action of $A$ is given by:\n$$\n(A u)_{i,j} = \\frac{1}{h^2}\\left(4 u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}\\right)\n$$\nThe terms corresponding to points outside the interior are zero, consistent with the homogeneous Dirichlet boundary conditions. The matrix $A$ is known to be symmetric and positive-definite (SPD), a necessary condition for the application of the standard Conjugate Gradient method.\n\nTo verify the correctness of the implementation, a manufactured solution is employed. The exact solution $x^*$ is constructed by vectorizing the smooth function $u^*(x,y) = \\sin(\\pi x)\\sin(\\pi y)$ evaluated at the interior grid points $(x_i, y_j) = (ih, jh)$. The right-hand side vector $b$ is then defined as $b = A x^*$, which guarantees that the known vector $x^*$ is the exact solution to the system $A x = b$.\n\nA critical constraint of the problem is that the matrix $A$, which is of size $(M^2) \\times (M^2)$, must not be formed explicitly in memory. This is standard practice for large-scale problems where $A$ is sparse, as explicit storage would be computationally prohibitive. Instead, the matrix-vector product $v \\mapsto Av$ must be implemented as a function that directly applies the $5$-point stencil to a vector $v$, after reshaping it into an $M \\times M$ grid. In our implementation, a vector of length $M^2$ is reshaped into an $M \\times M$ matrix corresponding to the grid values, using row-major ordering. The stencil operation is then applied efficiently using `numpy` array slicing, and the resulting $M \\times M$ grid is flattened back into a vector.\n\nThe Conjugate Gradient algorithm is an iterative method for solving SPD linear systems. It is equivalent to minimizing the quadratic functional $\\phi(x) = \\frac{1}{2} x^\\top A x - b^\\top x$. Starting with an initial guess $x_0$ (typically the zero vector), the method iteratively generates a sequence of approximations $x_k$ that minimize $\\phi(x)$ over successively larger Krylov subspaces $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$, where $r_0 = b - Ax_0$ is the initial residual.\n\nThe standard CG algorithm proceeds as follows, starting with $x_0 = \\vec{0}$, $r_0 = b$, and $p_0 = r_0$:\nFor $k = 0, 1, 2, \\dots$:\n1.  Compute the matrix-vector product $q_k = A p_k$.\n2.  Compute the step size: $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top q_k}$.\n3.  Update the solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n4.  Update the residual: $r_{k+1} = r_k - \\alpha_k q_k$.\n5.  Check for convergence. The process terminates if $\\|r_{k+1}\\|_2 \\le \\text{tol} \\cdot \\|r_0\\|_2$, where $\\text{tol}$ is a specified tolerance.\n6.  Update the search direction: $p_{k+1} = r_{k+1} + \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k} p_k$.\n\nFor one test case, diagonal (Jacobi) preconditioning is required. A preconditioner $P$ is a matrix that approximates $A$ but for which the system $Pz=r$ is easy to solve. The Preconditioned Conjugate Gradient (PCG) method solves the modified system $P^{-1}Ax = P^{-1}b$. The Jacobi preconditioner is simply the diagonal of $A$, $P = \\text{diag}(A)$. For the discrete Laplacian, all diagonal entries of $A$ are equal to $4/h^2$. Solving $Pz=r$ is trivial: $z = P^{-1}r$, which amounts to an element-wise multiplication $z_i = r_i / P_{ii} = r_i \\cdot (h^2/4)$.\n\nThe PCG algorithm modifies the standard CG algorithm as follows:\nStarting with $x_0 = \\vec{0}$, $r_0 = b$. Solve $P z_0 = r_0$. Set $p_0 = z_0$.\nFor $k = 0, 1, 2, \\dots$:\n1.  Compute $q_k = A p_k$.\n2.  Compute step size: $\\alpha_k = \\frac{r_k^\\top z_k}{p_k^\\top q_k}$.\n3.  Update solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n4.  Update residual: $r_{k+1} = r_k - \\alpha_k q_k$.\n5.  Check for convergence.\n6.  Solve the preconditioning system: $P z_{k+1} = r_{k+1}$.\n7.  Update search direction: $p_{k+1} = z_{k+1} + \\frac{r_{k+1}^\\top z_{k+1}}{r_k^\\top z_k} p_k$.\n\nThe implementation will cater to both the standard and preconditioned versions of the algorithm. The final output for each test case will report the number of iterations performed, the final residual two-norm $\\|r_k\\|_2$, the relative error in the solution $\\|x_k - x^*\\|_2 / \\|x^*\\|_2$, and a boolean indicating if convergence was achieved within the maximum allowed iterations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef cg_solver(M, tol, max_iter, use_preconditioner):\n    \"\"\"\n    Solves the discretized Poisson equation using the Conjugate Gradient method.\n\n    Args:\n        M (int): Number of interior grid points in one dimension.\n        tol (float): Relative residual tolerance for convergence.\n        max_iter (int): Maximum number of iterations.\n        use_preconditioner (bool): If True, use diagonal (Jacobi) preconditioning.\n\n    Returns:\n        list: A list containing [iterations, final_residual_norm, relative_error, converged_flag].\n    \"\"\"\n    # 1. Problem Setup\n    N = M * M\n    h = 1.0 / (M + 1)\n    \n    # Grid and exact solution\n    x_coords = h * np.arange(1, M + 1)\n    y_coords = h * np.arange(1, M + 1)\n    xx, yy = np.meshgrid(x_coords, y_coords, indexing='xy')\n    \n    u_star_grid = np.sin(np.pi * xx) * np.sin(np.pi * yy)\n    x_star = u_star_grid.flatten(order='C')\n    norm_x_star = np.linalg.norm(x_star)\n\n    # Matrix-vector product function A*v using 5-point stencil\n    h2_inv = 1.0 / (h * h)\n    def mat_vec(v):\n        v_grid = v.reshape((M, M))\n        Av_grid = 4.0 * v_grid\n        if M  1:\n            Av_grid[1:, :] -= v_grid[:-1, :]  # Neighbor from top (y-)\n            Av_grid[:-1, :] -= v_grid[1:, :]  # Neighbor from bottom (y+)\n            Av_grid[:, 1:] -= v_grid[:, :-1]  # Neighbor from left (x-)\n            Av_grid[:, :-1] -= v_grid[:, 1:]  # Neighbor from right (x+)\n        return (Av_grid * h2_inv).flatten(order='C')\n\n    # Right-hand side b = A*x_star\n    b = mat_vec(x_star)\n\n    # Preconditioner application function P_inv*r\n    h2_by_4 = (h * h) / 4.0\n    def apply_preconditioner(r):\n        return r * h2_by_4\n\n    # 2. CG/PCG Initialization and Iteration\n    x = np.zeros(N)\n    r = b.copy()  # Since x_0=0, r_0 = b - A*x_0 = b\n    norm_r0 = np.linalg.norm(r)\n\n    if norm_r0 == 0:\n        rel_err = 0.0 if norm_x_star == 0 else np.linalg.norm(x - x_star) / norm_x_star\n        return [0, 0.0, rel_err, True]\n        \n    tol_abs = tol * norm_r0\n    converged = False\n    k = 0\n\n    if use_preconditioner:\n        z = apply_preconditioner(r)\n        p = z.copy()\n        rho_old = np.dot(r, z)\n    else:  # Standard CG\n        z = r # z is not used, but for consistency in variable names\n        p = r.copy()\n        rho_old = np.dot(r, r)\n\n    if rho_old == 0: # initial residual is 0\n        converged = True\n    \n    for k_iter in range(max_iter):\n        k = k_iter + 1\n\n        q = mat_vec(p)\n        alpha = rho_old / np.dot(p, q)\n        \n        x += alpha * p\n        r -= alpha * q\n        \n        norm_r = np.linalg.norm(r)\n        if norm_r = tol_abs:\n            converged = True\n            break\n            \n        if use_preconditioner:\n            z = apply_preconditioner(r)\n            rho_new = np.dot(r, z)\n        else:  # Standard CG\n            rho_new = np.dot(r, r)\n            \n        beta = rho_new / rho_old\n        \n        if use_preconditioner:\n            p = z + beta * p\n        else:\n            p = r + beta * p\n            \n        rho_old = rho_new\n\n    # 3. Final calculations and return\n    final_residual_norm = np.linalg.norm(b - mat_vec(x))\n    relative_error = np.linalg.norm(x - x_star) / norm_x_star if norm_x_star > 0 else 0.0\n    \n    return [k, final_residual_norm, relative_error, converged]\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        (64, 1e-8, 1000, False),\n        (1, 1e-12, 20, False),\n        (128, 1e-6, 50, False),\n        (96, 1e-10, 1000, True),\n    ]\n\n    results = []\n    for M, tol, max_iter, use_precon in test_cases:\n        result = cg_solver(M, tol, max_iter, use_precon)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default string representation of lists and booleans is used as specified.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}