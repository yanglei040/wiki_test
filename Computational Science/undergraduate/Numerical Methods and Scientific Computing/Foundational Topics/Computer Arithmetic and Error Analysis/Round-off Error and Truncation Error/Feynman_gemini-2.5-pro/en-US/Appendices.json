{
    "hands_on_practices": [
        {
            "introduction": "Our exploration of numerical errors begins with a classic illustration: computing the hyperbolic sine function, $\\sinh(x)$. This seemingly simple function provides a perfect laboratory for observing the fundamental tension between round-off error and truncation error, especially for small values of its argument $x$. This exercise  challenges you to compare a mathematically direct formula, which suffers from catastrophic cancellation, against a simple Taylor approximation, which introduces truncation error, revealing the critical trade-offs inherent in computational science.",
            "id": "3268991",
            "problem": "Consider the hyperbolic sine function defined by the identity $\\,\\sinh(x) = \\dfrac{e^{x} - e^{-x}}{2}\\,$ and its Maclaurin series, which is a special case of the general Taylor series for analytic functions. The Maclaurin series is constructed by differentiating at the origin and summing the resulting terms. For sufficiently small $\\,x\\,$, it is common to use the first-order truncation $\\,\\sinh(x) \\approx x\\,$, which introduces a truncation error by discarding higher-order terms. In a digital computer using finite-precision arithmetic under the Institute of Electrical and Electronics Engineers (IEEE) 754 standard for floating-point numbers, computations of $\\,e^{x}\\,$ and $\\,e^{-x}\\,$ are performed with rounding at each operation. When evaluating $\\,\\sinh(x)\\,$ via $\\,\\dfrac{e^{x} - e^{-x}}{2}\\,$ for small $\\,x\\,$, subtracting two nearly equal numbers may amplify round-off error due to catastrophic cancellation. Your task is to quantify, by direct computation, the interplay between truncation error and round-off error for several representative inputs.\n\nImplement a program that, for each input $\\,x\\,$ in the test suite specified below, computes three numerical approximations to $\\,\\sinh(x)\\,$ and reports their relative errors against a high-quality reference value computed in floating-point arithmetic:\n- The direct formula $\\,\\dfrac{e^{x} - e^{-x}}{2}\\,$ (this is susceptible to cancellation for small $\\,x\\,$).\n- A numerically improved formula that reduces cancellation by using the function $\\,\\operatorname{expm1}(x) = e^{x} - 1\\,$ to compute $\\,\\dfrac{\\operatorname{expm1}(x) - \\operatorname{expm1}(-x)}{2}\\,$.\n- The first-order Taylor approximation $\\,x\\,$.\n\nUse a numerically stable implementation of $\\,\\sinh(x)\\,$ in floating-point arithmetic as the reference value (for example, a well-tested library implementation). For each approximation $\\,y_{\\text{approx}}\\,$ and reference $\\,y_{\\text{ref}}\\,$, compute the relative error $\\,E = \\dfrac{|y_{\\text{approx}} - y_{\\text{ref}}|}{|y_{\\text{ref}}|}\\,$. If $\\,y_{\\text{ref}} = 0\\,$, report the absolute error $\\,E = |y_{\\text{approx}} - 0|\\,$ instead to avoid division by zero.\n\nTest Suite:\n- $\\,x = 10^{-9}\\,$\n- $\\,x = 10^{-16}\\,$\n- $\\,x = 10^{-1}\\,$\n- $\\,x = 0\\,$\n- $\\,x = -10^{-9}\\,$\n- $\\,x = 20\\,$\n\nDesign for coverage:\n- The case $\\,x = 10^{-9}\\,$ exercises small arguments where cancellation affects the direct formula and truncation error is small.\n- The case $\\,x = 10^{-16}\\,$ probes behavior near double-precision machine epsilon, intensifying cancellation.\n- The case $\\,x = 10^{-1}\\,$ is a moderate argument where both truncation and round-off errors are modest.\n- The case $\\,x = 0\\,$ serves as a boundary condition where $\\,\\sinh(0) = 0\\,$.\n- The case $\\,x = -10^{-9}\\,$ checks odd symmetry and sign handling.\n- The case $\\,x = 20\\,$ represents a large argument where cancellation is negligible and exponential growth dominates.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each $\\,x\\,$ in the test suite in the order given, append three floating-point numbers in order: $\\,E_{\\text{naive}}\\,$ for the direct formula, $\\,E_{\\text{stable}}\\,$ for the improved $\\,\\operatorname{expm1}\\,$-based formula, and $\\,E_{\\text{taylor}}\\,$ for the first-order Taylor approximation. The final output is a single flat list with $\\,18\\,$ entries corresponding to $\\,6\\,$ test inputs times $\\,3\\,$ error values per input, for example, $\\,\\big[ E_{\\text{naive}}(10^{-9}), E_{\\text{stable}}(10^{-9}), E_{\\text{taylor}}(10^{-9}), \\ldots \\big]\\,$. All outputs must be dimensionless real numbers (floats).",
            "solution": "The problem requires a quantitative analysis of numerical errors in the computation of the hyperbolic sine function, $\\sinh(x)$. The analysis will compare three different methods against a high-precision reference value. The two dominant sources of error under consideration are round-off error, which arises from the finite precision of floating-point arithmetic, and truncation error, which results from approximating an infinite series with a finite number of terms.\n\nThe function is defined as:\n$$\n\\sinh(x) = \\frac{e^x - e^{-x}}{2}\n$$\nIts Maclaurin series expansion is:\n$$\n\\sinh(x) = x + \\frac{x^3}{3!} + \\frac{x^5}{5!} + \\frac{x^7}{7!} + \\dots = \\sum_{n=0}^{\\infty} \\frac{x^{2n+1}}{(2n+1)!}\n$$\nThis series converges for all real $x$.\n\nOur methodology involves computing $\\sinh(x)$ for a given set of input values $x$ using three distinct approximations. We will then calculate the error of each approximation relative to a reference value, $y_{\\text{ref}}$, which is obtained from a numerically robust library implementation (`numpy.sinh`). The relative error, $E$, for an approximation $y_{\\text{approx}}$ is defined as:\n$$\nE = \\frac{|y_{\\text{approx}} - y_{\\text{ref}}|}{|y_{\\text{ref}}|}\n$$\nIn the special case where $y_{\\text{ref}} = 0$, division by zero is undefined. For this case, as specified, we will compute the absolute error, $E = |y_{\\text{approx}} - y_{\\text{ref}}|$.\n\nThe three approximations to be evaluated are:\n\n$1$. **The Naive Direct Formula ($y_{\\text{naive}}$)**:\nThis method uses the definitional identity directly:\n$$\ny_{\\text{naive}} = \\frac{e^x - e^{-x}}{2}\n$$\nFor values of $x$ close to $0$, $e^x \\approx 1 + x$ and $e^{-x} \\approx 1 - x$. Consequently, $e^x$ and $e^{-x}$ become two nearly equal numbers. In finite-precision arithmetic, the subtraction of these two numbers results in a phenomenon known as catastrophic cancellation. The leading, significant digits cancel each other out, leaving a result dominated by the less-significant, rounded-off parts of the original numbers. This effectively amplifies the relative round-off error, leading to a significant loss of precision.\n\n$2$. **The Numerically Stable Formula ($y_{\\text{stable}}$)**:\nThis method reformulates the expression to avoid catastrophic cancellation. It utilizes the standard library function $\\operatorname{expm1}(x) = e^x - 1$, which is designed to compute its result accurately even for $|x| \\ll 1$. The formula for $\\sinh(x)$ is rewritten as:\n$$\ny_{\\text{stable}} = \\frac{\\operatorname{expm1}(x) - \\operatorname{expm1}(-x)}{2}\n$$\nMathematically, this is equivalent to the naive formula since $(\\operatorname{expm1}(x) - \\operatorname{expm1}(-x)) / 2 = ((e^x-1) - (e^{-x}-1))/2 = (e^x - e^{-x})/2$. Numerically, however, it is superior for small $|x|$. When $x$ is small, $\\operatorname{expm1}(x) \\approx x$ and $\\operatorname{expm1}(-x) \\approx -x$. The subtraction is then $\\operatorname{expm1}(x) - \\operatorname{expm1}(-x) \\approx x - (-x) = 2x$. This operation does not involve the subtraction of nearly equal numbers, thus preserving numerical precision.\n\n$3$. **The First-Order Taylor Approximation ($y_{\\text{taylor}}$)**:\nThis method uses the first term of the Maclaurin series for $\\sinh(x)$:\n$$\ny_{\\text{taylor}} = x\n$$\nThis approximation introduces a truncation error by omitting all higher-order terms of the series, starting with $\\frac{x^3}{3!}$. The truncation error is approximately equal to the first neglected term, $\\frac{x^3}{6}$. The relative truncation error is therefore approximately:\n$$\n\\frac{|\\frac{x^3}{6}|}{|\\sinh(x)|} \\approx \\frac{|x^3/6|}{|x|} = \\frac{x^2}{6} \\quad \\text{for small } x \\neq 0\n$$\nThis error is inherent to the mathematical approximation itself and is independent of floating-point representation, although its final computed value will be affected by round-off. The approximation is only accurate for values of $x$ very close to $0$.\n\nThe provided test suite is designed to demonstrate the behavior of these errors in different regimes:\n- For small $|x|$ (e.g., $10^{-9}$ and $10^{-16}$), we expect $y_{\\text{naive}}$ to exhibit large error due to catastrophic cancellation, while $y_{\\text{stable}}$ should be highly accurate. The truncation error of $y_{\\text{taylor}}$ will be very small, making it a good approximation as well.\n- For a moderate $x$ like $10^{-1}$, cancellation is less of an issue, so $y_{\\text{naive}}$ will be more accurate. However, the truncation error of $y_{\\text{taylor}}$ ($x^2/6 \\approx (10^{-1})^2/6 \\approx 1.6 \\times 10^{-3}$) will become significant.\n- For a large $x$ like $20$, $e^x$ is very large and $e^{-x}$ is very small. There is no cancellation, so $y_{\\text{naive}}$ and $y_{\\text{stable}}$ are expected to perform equally well. In contrast, $y_{\\text{taylor}} = 20$ is a poor approximation for $\\sinh(20) \\approx 2.4 \\times 10^8$, leading to a large error.\n- For $x=0$, all three methods should yield the exact result of $0$, resulting in zero error.\n\nThe program will execute these computations and report the errors, providing a clear, quantitative illustration of these fundamental principles of numerical analysis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes numerical errors for three different approximations of sinh(x)\n    for a given test suite of x values.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Use numpy.float64 for consistent high-precision floating-point arithmetic.\n    test_cases = [\n        np.float64(1e-9),\n        np.float64(1e-16),\n        np.float64(1e-1),\n        np.float64(0.0),\n        np.float64(-1e-9),\n        np.float64(20.0),\n    ]\n\n    results = []\n\n    def calculate_error(y_approx, y_ref):\n        \"\"\"\n        Calculates relative error, or absolute error if the reference is zero.\n        \"\"\"\n        if y_ref == 0.0:\n            # For y_ref = 0, use absolute error to avoid division by zero.\n            return np.abs(y_approx - y_ref)\n        else:\n            # Otherwise, use relative error.\n            return np.abs(y_approx - y_ref) / np.abs(y_ref)\n\n    for x in test_cases:\n        # 1. Compute the high-quality reference value using the library function.\n        y_ref = np.sinh(x)\n\n        # 2. Compute the three approximations.\n        \n        # Naive formula: susceptible to catastrophic cancellation for small x.\n        y_naive = (np.exp(x) - np.exp(-x)) / 2.0\n        \n        # Stable formula: uses expm1 to avoid cancellation for small x.\n        # np.expm1(x) computes exp(x) - 1 with high precision for small x.\n        y_stable = (np.expm1(x) - np.expm1(-x)) / 2.0\n        \n        # First-order Taylor approximation: introduces truncation error.\n        y_taylor = x\n        \n        # 3. Calculate and store the errors for each approximation.\n        e_naive = calculate_error(y_naive, y_ref)\n        e_stable = calculate_error(y_stable, y_ref)\n        e_taylor = calculate_error(y_taylor, y_ref)\n        \n        results.extend([e_naive, e_stable, e_taylor])\n\n    # Final print statement in the exact required format.\n    # map(str, results) converts all float results to their string representations.\n    # ','.join(...) joins them into a single comma-separated string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on our understanding of cancellation, we now tackle a more dramatic case: approximating $e^{-x}$ for a large argument $x$. A naive summation of the alternating Taylor series involves adding and subtracting enormous intermediate terms to arrive at a tiny final result—a recipe for numerical disaster. This practice  provides a striking demonstration of this catastrophic loss of precision and challenges you to implement and compare common strategies for restoring accuracy, such as reordering the summation and reformulating the problem entirely.",
            "id": "3268948",
            "problem": "Develop a program that numerically demonstrates the interplay between round-off error and truncation error when evaluating the exponential function at large negative arguments. Work entirely in double-precision floating-point arithmetic (IEEE 754 binary64 as provided by the language runtime and standard libraries). Use the following fundamental bases: the Taylor series definition of the exponential function, the standard floating-point rounding model with bounded relative error per operation, and the alternating series remainder bound. Your task is to investigate the stability of different computational strategies for approximating $e^{-x}$.\n\nStart from the fundamental definitions and facts:\n- For any real $x$, the exponential function has the Taylor series expansion about $0$: \n$$e^{x} = \\sum_{k=0}^{\\infty} \\frac{x^{k}}{k!}.$$\n- The floating-point rounding model can be abstracted as $fl(a \\circ b) = (a \\circ b)(1 + \\delta)$ with $|\\delta| \\leq u$ for a single binary operation $\\circ$ (addition, subtraction, multiplication, or division) and unit roundoff $u$ characteristic of the arithmetic.\n- For an alternating series $\\sum_{k=0}^{\\infty} (-1)^{k} a_{k}$ with $a_{k} \\ge 0$ monotonically decreasing to $0$, the truncation error after $K$ terms is bounded in absolute value by the first neglected term: \n$$\\left|\\sum_{k=0}^{\\infty} (-1)^{k} a_{k} - \\sum_{k=0}^{K} (-1)^{k} a_{k}\\right| \\le a_{K+1}.$$\n\nDesign an algorithm to approximate $e^{-x}$ by summing the Taylor series for $e^{-x}$,\n$$e^{-x} = \\sum_{k=0}^{\\infty} \\frac{(-x)^{k}}{k!},$$\nusing the term recurrence\n$$t_{0} = 1,\\quad t_{k+1} = t_{k} \\cdot \\frac{-x}{k+1},\\quad k \\ge 0,$$\nand a truncation rule that stops when the magnitude of the next term satisfies $|t_{k}| \\le \\tau_{\\text{abs}}$ for a prescribed absolute tolerance $\\tau_{\\text{abs}}$. The truncation rule is justified by the alternating series bound once the terms are in the decreasing regime. Use two accumulation orders:\n- Forward summation: add terms in increasing $k$ order from $k=0$ upward.\n- Backward summation: first generate all terms up to the stopping criterion, then add them in decreasing $k$ order down to $k=0$.\n\nAlso consider the alternative strategy of computing $e^{-x}$ via the reciprocal of $e^{x}$:\n$$e^{-x} = \\frac{1}{e^{x}},$$\ncomputed using the standard library function for $e^{x}$ followed by one division.\n\nFor numerical assessment, define the reference value as the runtime’s library value $e_{\\text{ref}} = \\exp(-x)$ and report the relative error of an approximation $\\widehat{e^{-x}}$ as\n$$\\mathrm{rel\\_err} = \\frac{|\\widehat{e^{-x}} - e_{\\text{ref}}|}{|e_{\\text{ref}}|}.$$\n\nImplement a single program that carries out the following test suite, each test returning a single floating-point value equal to $\\mathrm{rel\\_err}$:\n- Test A (catastrophic cancellation target): $x = 25$, compute $e^{-x}$ by forward Taylor summation with absolute tolerance $\\tau_{\\text{abs}} = 10^{-30}$.\n- Test B (stable reciprocal): $x = 25$, compute $e^{-x}$ as $1/\\exp(x)$ using the standard library.\n- Test C (benign series): $x = 1$, compute $e^{-x}$ by forward Taylor summation with absolute tolerance $\\tau_{\\text{abs}} = 10^{-30}$.\n- Test D (mitigated cancellation): $x = 25$, compute $e^{-x}$ by backward Taylor summation with absolute tolerance $\\tau_{\\text{abs}} = 10^{-30}$.\n\nYour program must:\n- Use the term recurrence above to avoid overflow in intermediate $k!$ or $x^{k}$.\n- Ensure the truncation error is negligible relative to the phenomena under study by using $\\tau_{\\text{abs}} = 10^{-30}$.\n- Compute and return the relative error for each test using the reference $e_{\\text{ref}} = \\exp(-x)$ from the standard library in the same floating-point precision.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD]\"), in the exact order A, B, C, D as defined above. The results must be floating-point numbers.\n\nScientific realism and applicability:\n- This setup isolates truncation error via the alternating series bound and exhibits round-off effects due to catastrophic cancellation when summing large alternating terms whose absolute sum is vastly larger than the tiny final result for $x = 25$. By contrast, the reciprocal strategy avoids such subtraction-induced loss of significance.",
            "solution": "The problem requires the development and analysis of three distinct numerical methods for approximating the exponential function $e^{-x}$ for a positive argument $x$. The core of the problem lies in demonstrating the practical consequences of two fundamental types of numerical error: truncation error and round-off error. We will operate within the standard double-precision floating-point arithmetic framework (IEEE 754).\n\nThe function to be approximated is $e^{-x}$, whose Taylor series expansion around $0$ is given by:\n$$e^{-x} = \\sum_{k=0}^{\\infty} \\frac{(-x)^{k}}{k!} = 1 - x + \\frac{x^2}{2!} - \\frac{x^3}{3!} + \\dots$$\nThis is an alternating series for $x>0$. Let $t_k = \\frac{(-x)^k}{k!}$ be the $k$-th term. To avoid computing large factorials and powers which could lead to overflow, we use the stable recurrence relation:\n$$t_0 = 1, \\quad t_{k+1} = t_k \\cdot \\frac{-x}{k+1} \\quad \\text{for } k \\ge 0.$$\n\nTwo primary sources of error must be considered:\n$1$. **Truncation Error**: This error arises from approximating an infinite series with a finite sum. If we truncate the series after the $K$-th term, the truncation error $E_{\\text{trunc}}$ is the remainder:\n$$E_{\\text{trunc}} = \\sum_{k=K+1}^{\\infty} \\frac{(-x)^{k}}{k!}.$$\nFor an alternating series where the terms' magnitudes, $|t_k| = \\frac{x^k}{k!}$, eventually decrease monotonically to zero, the absolute value of the truncation error is bounded by the absolute value of the first neglected term. That is, for a sufficiently large $K$, $|\\sum_{k=K+1}^{\\infty} t_k| \\le |t_{K+1}|$. The problem specifies a stopping criterion based on an absolute tolerance $\\tau_{\\text{abs}} = 10^{-30}$: we stop summing when $|t_k|$ becomes smaller than this value. This tolerance is much smaller than the machine epsilon for double precision ($u \\approx 2.22 \\times 10^{-16}$), ensuring that the truncation error is, by design, negligible relative to the round-off errors we seek to investigate.\n\n$2$. **Round-off Error**: This error is inherent to floating-point arithmetic. Every arithmetic operation is subject to rounding, modeled as $fl(a \\circ b) = (a \\circ b)(1 + \\delta)$ where $|\\delta| \\leq u$. When summing a series, these small errors accumulate. A particularly pernicious form of round-off error is **catastrophic cancellation**, which occurs when subtracting two nearly equal numbers. The leading significant digits cancel, and the result is dominated by the noise from previous computations, leading to a massive loss of relative accuracy.\n\nWe will analyze the following three computational strategies in light of these error sources. The reference value for calculating relative error will be $e_{\\text{ref}} = \\exp(-x)$, as computed by the standard library's highly optimized function. The relative error is $\\mathrm{rel\\_err} = |\\widehat{e^{-x}} - e_{\\text{ref}}| / |e_{\\text{ref}}|$.\n\n**Test A: Forward Taylor Summation for $x=25$**\nThe algorithm sums the terms in their natural order: $S_N = \\sum_{k=0}^{N} t_k$. For $x=25$, the terms $\\frac{(-25)^k}{k!}$ initially grow very large in magnitude. The maximum magnitude occurs around $k \\approx x = 25$. For instance, $|t_{24}| = \\frac{25^{24}}{24!} \\approx 1.00 \\times 10^{11}$ and $|t_{25}| = \\frac{25^{25}}{25!} \\approx 1.00 \\times 10^{11}$. We are adding and subtracting these enormous numbers to ultimately obtain a very small result, $e^{-25} \\approx 1.39 \\times 10^{-11}$. In floating-point arithmetic, the sum $S_k = S_{k-1} + t_k$ will involve numbers of vastly different scales. More critically, when $k$ is large enough, $S_k$ is the difference of two large, nearly equal numbers, which is the textbook definition of catastrophic cancellation. The limited precision of approximately $16$ decimal digits in double-precision means that the result will have very few, if any, correct significant figures. A very large relative error is expected.\n\n**Test C: Forward Taylor Summation for $x=1$** (Analyzed out of order for comparison)\nIn this case, the terms are $t_k = \\frac{(-1)^k}{k!}$. The magnitudes $|t_k|$ are monotonically decreasing from the start: $1, 1, 0.5, 0.166...,$ etc. The sum converges to $e^{-1} \\approx 0.368$. At no point do we add or subtract large numbers to get a small one. The partial sums never grow much larger than the final result. Catastrophic cancellation does not occur. The primary source of error will be the gentle accumulation of standard round-off errors from each addition and the negligible truncation error. A very low relative error, close to machine epsilon, is expected.\n\n**Test D: Backward Taylor Summation for $x=25$**\nThis strategy first computes and stores all terms $t_0, t_1, \\dots, t_K$ that are needed to meet the truncation tolerance. The summation is then performed in reverse order: $S = t_K + t_{K-1} + \\dots + t_0$. This is a standard technique to mitigate round-off error accumulation. The summation begins by adding the smallest terms together. This allows the partial sum to accumulate precision before it is added to the larger-magnitude terms. While this method does not eliminate the fundamental problem of subtracting large numbers ($t_{24}$ and $t_{25}$ are still present in the list), it organizes the operations in a way that is less susceptible to the immediate loss of information that occurs in forward summation when a small term is added to a large running sum. The result should be significantly more accurate than forward summation for $x=25$, but likely not as accurate as a method that avoids cancellation altogether.\n\n**Test B: Reciprocal Method for $x=25$**\nThis method computes $e^{-x}$ as $1/e^x$. The standard library function for $e^x$ is highly optimized. For large positive arguments like $x=25$, it does not suffer from cancellation, as all terms in its series expansion are positive. The library routine will compute $e^{25}$ to near machine precision. The subsequent single division operation introduces at most one additional unit of round-off. This method entirely sidesteps the catastrophic cancellation issue that plagues the direct Taylor series evaluation of $e^{-x}$. It is the numerically stable and preferred approach. The expected relative error should be very small, on the order of machine epsilon.\n\nThe program will implement these four tests and report the resulting relative errors, providing a clear numerical demonstration of these principles.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef forward_taylor_sum(x, tol):\n    \"\"\"\n    Computes exp(-x) using forward summation of the Taylor series.\n    \n    Args:\n        x (float): The argument to the exponential function.\n        tol (float): The absolute tolerance for the stopping criterion.\n        \n    Returns:\n        float: The approximation of exp(-x).\n    \"\"\"\n    total_sum = 0.0\n    term = 1.0  # t_0\n    k = 0\n    while abs(term) > tol:\n        total_sum += term\n        k += 1\n        term = term * (-x) / k\n    # Add the last term that was computed but failed the while condition\n    total_sum += term\n    return total_sum\n\ndef backward_taylor_sum(x, tol):\n    \"\"\"\n    Computes exp(-x) using backward summation of the Taylor series.\n    \n    Args:\n        x (float): The argument to the exponential function.\n        tol (float): The absolute tolerance for the stopping criterion.\n        \n    Returns:\n        float: The approximation of exp(-x).\n    \"\"\"\n    terms = []\n    term = 1.0  # t_0\n    k = 0\n    while abs(term) > tol:\n        terms.append(term)\n        k += 1\n        term = term * (-x) / k\n    # Add the last term that was computed but failed the while condition\n    terms.append(term)\n    \n    # Sum in reverse order\n    total_sum = 0.0\n    for t in reversed(terms):\n        total_sum += t\n    return total_sum\n    \ndef reciprocal_method(x):\n    \"\"\"\n    Computes exp(-x) as 1.0 / exp(x) using the standard library function.\n    \n    Args:\n        x (float): The argument to the exponential function.\n        \n    Returns:\n        float: The approximation of exp(-x).\n    \"\"\"\n    return 1.0 / np.exp(x)\n\ndef solve():\n    \"\"\"\n    Runs the full test suite and prints the results.\n    \"\"\"\n    # Define the shared parameters\n    x_large = 25.0\n    x_small = 1.0\n    tolerance = 1e-30\n    \n    results = []\n\n    # Test A: Catastrophic cancellation target\n    ref_A = np.exp(-x_large)\n    approx_A = forward_taylor_sum(x_large, tolerance)\n    rel_err_A = abs(approx_A - ref_A) / abs(ref_A)\n    results.append(rel_err_A)\n\n    # Test B: Stable reciprocal\n    ref_B = np.exp(-x_large)\n    approx_B = reciprocal_method(x_large)\n    rel_err_B = abs(approx_B - ref_B) / abs(ref_B)\n    results.append(rel_err_B)\n    \n    # Test C: Benign series\n    ref_C = np.exp(-x_small)\n    approx_C = forward_taylor_sum(x_small, tolerance)\n    rel_err_C = abs(approx_C - ref_C) / abs(ref_C)\n    results.append(rel_err_C)\n\n    # Test D: Mitigated cancellation\n    ref_D = np.exp(-x_large)\n    approx_D = backward_taylor_sum(x_large, tolerance)\n    rel_err_D = abs(approx_D - ref_D) / abs(ref_D)\n    results.append(rel_err_D)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "We have seen how the slow accumulation of round-off error can degrade the accuracy of a result, particularly in long summations. This final exercise introduces a powerful, general-purpose tool to combat this very problem: Kahan's compensated summation algorithm. By implementing and testing this algorithm , you will see firsthand how a clever approach can meticulously track and correct for the small bits of information that a naive summation loop discards, leading to dramatically more accurate results.",
            "id": "3268973",
            "problem": "You are asked to design and implement an experiment to quantify and compare the round-off error incurred by two summation strategies over floating-point numbers: a naive loop and compensated summation (Kahan’s algorithm). You must work in the floating-point arithmetic model consistent with the Institute of Electrical and Electronics Engineers (IEEE) 754 double precision as implemented by standard Python floating-point numbers. Use the following fundamental base:\n- Floating-point rounding model: each elementary operation is performed as $fl(a \\circ b) = (a \\circ b)(1 + \\delta)$ with $|\\delta| \\leq u$, where $u$ is the unit roundoff.\n- For base $\\beta = 2$ and precision $p = 53$ (IEEE 754 double precision), $u = \\tfrac{1}{2}\\beta^{1-p} = 2^{-53}$.\n- Absolute error is defined as $| \\widehat{S} - S |$, where $S$ is the exact real result and $\\widehat{S}$ is the computed floating-point result.\n\nYour program must:\n- Implement two summation routines over a sequence $(x_i)$:\n  1. A naive loop that accumulates $s \\leftarrow s + x_i$ in the given order.\n  2. A compensated summation (Kahan) loop that uses a compensation variable to capture low-order bits lost to rounding, maintaining a running pair $(s,c)$, where $s$ is the running sum and $c$ is the compensation. Do not use any external libraries beyond the specified environment.\n- For each test case below, compute both absolute errors with respect to the exact real sum, using high-precision decimal arithmetic to represent the exact value $S$ and to evaluate $|\\widehat{S} - S|$. Treat the mathematical constants in the inputs as exact real numbers (for example, interpret the string \"0.1\" as the exact rational number $1/10$).\n- Report for each test whether the compensated summation’s absolute error is less than or equal to the naive loop’s absolute error. The result for each test must therefore be a boolean.\n\nImportant clarifications:\n- This task investigates round-off error only. The series in this problem are finite and exactly summable in exact arithmetic, so the truncation error is $0$ in all test cases. Your analysis and implementation should clearly separate round-off error (due to finite precision) from truncation error (due to approximating an infinite or continuous process).\n- Angles and physical units are not involved in this problem.\n- Express boolean outputs using the programming language’s native boolean literals.\n\nTest suite to implement and evaluate:\n- Case A (large identical addends): Sum the series $\\sum_{i=1}^{10^7} 0.1$. The exact value is $10^6$. Verify whether compensated summation has absolute error less than or equal to the naive loop.\n- Case B (catastrophic cancellation in a short list): Sum the list $[10^{16}, 1, -10^{16}]$ in this order. The exact value is $1$. Verify whether compensated summation has absolute error less than or equal to the naive loop.\n- Case C (zero sequence sanity check): Sum the list $[0, 0, 0]$. The exact value is $0$. Verify whether compensated summation has absolute error less than or equal to the naive loop.\n- Case D (many tiny identical addends): Sum the series $\\sum_{i=1}^{10^6} 10^{-10}$. The exact value is $10^{-4}$. Verify whether compensated summation has absolute error less than or equal to the naive loop.\n\nFinal output format:\n- Your program should produce a single line of output containing the four boolean results, in order [Case A, Case B, Case C, Case D], as a comma-separated list enclosed in square brackets (for example, \"[True,False,True,True]\").",
            "solution": "The user has requested an analysis and implementation of two summation algorithms—naive summation and compensated (Kahan) summation—to compare their susceptibility to round-off error. The problem is valid, scientifically grounded in the principles of numerical analysis, and well-posed. It requires implementing a computational experiment and reporting the results.\n\nThe core of this problem lies in the finite precision of floating-point arithmetic. Standard computer representations of real numbers, such as the IEEE 754 double-precision format used by Python's `float` type, have a limited number of bits. This limitation means that most real numbers cannot be represented exactly, and arithmetic operations can introduce small errors.\n\n**Floating-Point Arithmetic and Round-off Error**\n\nWe operate within the standard model of floating-point arithmetic. For any real numbers $a$ and $b$ and an arithmetic operation $\\circ \\in \\{+, -, \\times, \\div\\}$, the computed floating-point result, denoted $fl(a \\circ b)$, is given by:\n$$\nfl(a \\circ b) = (a \\circ b)(1 + \\delta)\n$$\nwhere $|\\delta| \\leq u$. The constant $u$ is the unit roundoff, or machine epsilon. For IEEE 754 double precision, the base is $\\beta=2$ and the precision (number of significand bits, including the implicit leading bit) is $p=53$. The unit roundoff is defined as $u = \\frac{1}{2}\\beta^{1-p} = 2^{-53}$, which is approximately $1.11 \\times 10^{-16}$.\n\nThis model implies that every elementary operation may introduce a small relative error. When summing a long sequence of numbers, these small errors can accumulate, leading to a significant discrepancy between the computed sum and the true mathematical sum. The problem asks us to investigate this phenomenon by comparing two algorithms. Note that as all sums are over a finite number of terms, the truncation error (error from approximating an infinite process with a finite one) is zero. We are concerned exclusively with round-off error.\n\n**Algorithm 1: Naive Summation**\n\nThis is the most straightforward approach to summation. Given a sequence of numbers $x_1, x_2, \\ldots, x_N$, the sum $S_N = \\sum_{i=1}^N x_i$ is computed iteratively.\n\nLet $\\widehat{S}_k$ be the computed sum after $k$ terms. The algorithm is:\n1. Initialize the running sum: $\\widehat{S}_0 = 0$.\n2. For $i = 1, \\ldots, N$, update the sum: $\\widehat{S}_i = fl(\\widehat{S}_{i-1} + x_i)$.\n\nThe primary source of error occurs when a number $x_i$ is added to a running sum $\\widehat{S}_{i-1}$ of a much larger magnitude. To perform the addition, the floating-point unit must align the binary points of the two numbers by shifting the significand of the smaller number to the right. This can cause the lower-order bits of the smaller number to be shifted out of the representation entirely, effectively being lost. This lost information is the round-off error for that step, and it is never recovered. Over many additions, this error accumulates, often in a systematic way, leading to a large final error.\n\n**Algorithm 2: Compensated Summation (Kahan's Algorithm)**\n\nKahan's algorithm is a refined technique designed to mitigate the accumulation of round-off error. It maintains a second variable, the *compensation* $c$, which tracks the \"lost\" low-order part from each addition. This captured error is then incorporated back into the sum at the next step.\n\nThe algorithm proceeds as follows, maintaining a running sum $s$ and a compensation $c$:\n1. Initialize sum and compensation: $s \\leftarrow 0$, $c \\leftarrow 0$.\n2. For each term $x_i$ in the sequence:\n   a. Correct the next term by the prior error: $y \\leftarrow x_i - c$.\n   b. Add the corrected term to the sum. A new, small error is introduced here: $t \\leftarrow s + y$.\n   c. Calculate the error of the previous step: $c \\leftarrow (t - s) - y$. In exact arithmetic, this would be $0$. In floating-point arithmetic, this calculation isolates the low-order bits lost when $y$ was added to $s$.\n   d. Update the sum: $s \\leftarrow t$.\n\nThe final result is the value of $s$ after iterating through all terms. By tracking the error in $c$ and using it to correct the next term, the algorithm prevents the systematic accumulation of round-off error. The error bound for Kahan summation is remarkably better than for naive summation and is notably independent of the number of terms $N$.\n\n**Analysis of Test Cases**\n\nTo find the absolute error $|\\widehat{S} - S|$, we require the exact sum $S$. As instructed, this is calculated using high-precision arithmetic, treating the inputs as exact rational numbers (e.g., $0.1$ is treated as $1/10$).\n\n- **Case A: Sum $\\sum_{i=1}^{10^7} 0.1$. Exact sum $S = 10^6$.**\nThe number $0.1$ has a non-terminating binary representation ($0.000110011..._2$) and thus cannot be stored exactly as a standard float. Each addition of the inexact `float(0.1)` to the growing sum introduces a small error. With $10^7$ additions, the naive sum accumulates a significant error. Kahan's algorithm is designed specifically for this scenario; the compensation variable $c$ will capture the small amount lost at each step and re-introduce it, yielding a much more accurate final result. Thus, the error for Kahan summation will be less than or equal to the error for the naive loop.\n\n- **Case B: Sum $[10^{16}, 1, -10^{16}]$. Exact sum $S = 1$.**\nThis case demonstrates catastrophic cancellation.\nIn a naive sum, the first operation is $10^{16} + 1$. Since the precision of a double-precision float is about $16$ decimal digits, and $1$ is $16$ orders of magnitude smaller than $10^{16}$, the addition results in $10^{16}$. The number $1$ is completely lost due to rounding (this phenomenon is called swamping). The next operation is $10^{16} - 10^{16} = 0$. The naive sum is $0.0$, an error of $1$.\nKahan's algorithm detects and corrects this. After computing $t \\leftarrow s + y = 10^{16} + 1 \\approx 10^{16}$, the compensation step is $c \\leftarrow (t - s) - y = (10^{16} - 10^{16}) - 1 = -1$. The algorithm has \"remembered\" that the $1$ was lost. In the next step, this lost part is used to correct the term $-10^{16}$, leading to the correct final answer of $1$. The error will be less than or equal to the naive loop's error.\n\n- **Case C: Sum $[0, 0, 0]$. Exact sum $S = 0$.**\nThis is a trivial sanity check. The addition of zeros is an exact operation in floating-point arithmetic. Both the naive loop and Kahan's algorithm will produce a result of exactly $0$, and thus the absolute error for both will be $0$. The condition \"less than or equal to\" holds true as a result of equality ($0 \\le 0$).\n\n- **Case D: Sum $\\sum_{i=1}^{10^6} 10^{-10}$. Exact sum $S = 10^{-4}$.**\nThis scenario is similar to Case A but with smaller numbers. The term $10^{-10}$ is not an exact power of $2$ and thus has an inexact floating-point representation. Summing $10^6$ of these inexact values will cause round-off error to accumulate in the naive sum. While the magnitude of the error may be less severe than in Case A (since the ratio of the running sum to the term being added does not grow as extreme), Kahan's algorithm will still provide a more accurate result by compensating for the representation error at each step. Its error will be less than or equal to that of the naive loop.\n\nIn all four cases, theoretical analysis indicates that Kahan's compensated summation will perform at least as well as, and in most cases significantly better than, the naive summation loop.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Implements and compares naive and Kahan summation for four test cases,\n    reporting if Kahan summation provides an error less than or equal to\n    naive summation.\n    \"\"\"\n    \n    # Set precision for Decimal calculations to ensure it's high enough.\n    getcontext().prec = 50\n\n    def naive_sum(numbers):\n        \"\"\"Computes the sum of a sequence using a naive iterative loop.\"\"\"\n        s = 0.0\n        for x in numbers:\n            s += x\n        return s\n\n    def kahan_sum(numbers):\n        \"\"\"Computes the sum of a sequence using Kahan's compensated summation algorithm.\"\"\"\n        s = 0.0\n        c = 0.0  # A running compensation for lost low-order bits.\n        for x in numbers:\n            y = x - c             # So far, so good: c is zero.\n            t = s + y             # Alas, s is big, y small, so low-order bits of y are lost.\n            c = (t - s) - y       # (t - s) recovers the high-order part of y; subtracting y extracts the low part.\n            s = t                 # Algebraically, c should be zero. Beware eagerly optimizing compilers!\n        return s\n\n    def run_test(sequence_generator, exact_sum_decimal):\n        \"\"\"\n        Runs a single test case for both summation methods and compares their absolute errors.\n        \n        Args:\n            sequence_generator: A lambda function that generates the sequence of floats for summation.\n            exact_sum_decimal: The exact sum as a high-precision Decimal object.\n            \n        Returns:\n            A boolean indicating if the absolute error of Kahan summation is = the absolute error of naive summation.\n        \"\"\"\n        # Generate the sequence of floating point numbers\n        sequence = list(sequence_generator())\n        \n        # Compute sums using both methods\n        naive_result = naive_sum(sequence)\n        kahan_result = kahan_sum(sequence)\n        \n        # Calculate absolute errors using high-precision Decimal arithmetic\n        err_naive = abs(Decimal(naive_result) - exact_sum_decimal)\n        err_kahan = abs(Decimal(kahan_result) - exact_sum_decimal)\n        \n        return err_kahan = err_naive\n\n    # Define the test cases\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"generator\": lambda: (0.1 for _ in range(10**7)),\n            \"exact_sum\": Decimal(10**7) * Decimal('0.1')\n        },\n        {\n            \"name\": \"Case B\",\n            \"generator\": lambda: [1.0e16, 1.0, -1.0e16],\n            \"exact_sum\": Decimal('1')\n        },\n        {\n            \"name\": \"Case C\",\n            \"generator\": lambda: [0.0, 0.0, 0.0],\n            \"exact_sum\": Decimal('0')\n        },\n        {\n            \"name\": \"Case D\",\n            \"generator\": lambda: (1e-10 for _ in range(10**6)),\n            \"exact_sum\": Decimal(10**6) * Decimal('1e-10')\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test(case[\"generator\"], case[\"exact_sum\"])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}