{
    "hands_on_practices": [
        {
            "introduction": "理论知识通向实践技能的桥梁始于具体的例子。本练习将通过计算一个简单的函数 $f(x) = \\sqrt{1+x} - 1$ 来直观地揭示灾难性抵消的根源。当 $x$ 的值很小时，$\\sqrt{1+x}$ 的结果将非常接近 $1$，直接相减会导致有效数字的严重损失。通过亲手实现直接计算法和代数等价的“有理化”方法，你将能量化地比较两种算法的数值稳定性，并理解为何简单的代数重构能显著提高计算精度。 ",
            "id": "3212218",
            "problem": "您需要编写一个完整、可运行的程序，研究在 $x$ 值很小时计算函数 $f(x)=\\sqrt{1+x}-1$ 的数值稳定性，并在有限精度算术下比较以下两种代数等价的公式：\n- 直接相减法：$f_{\\mathrm{dir}}(x) = \\sqrt{1+x} - 1$。\n- 有理化形式：$f_{\\mathrm{rat}}(x) = \\dfrac{x}{\\sqrt{1+x}+1}$。\n\n您的程序必须完成以下任务，仅使用标准双精度浮点算术进行近似计算，并使用一个高精度参考值来量化误差。\n\n基本原理和背景：\n- 使用标准浮点舍入模型：对于一个产生实数结果 $t$ 的实数运算，其计算出的浮点结果为 $\\operatorname{fl}(t)=t(1+\\delta)$，其中 $|\\delta|\\leq u$，$u$ 是单位舍入误差（在64位二进制，通常称为双精度中，大约为 $u\\approx 10^{-16}$）。\n- 认识到两个几乎相等的数相减会导致灾难性抵消，即许多有效数字的前导位被抵消，可能放大结果中的相对误差。\n- 代数恒等式 $f(x)=\\dfrac{x}{\\sqrt{1+x}+1}$ 在实数算术中对所有 $x\\geq -1$, $x\\neq -1$ 精确成立，并且对于小的 $|x|$，它避免了两个几乎相等的量的直接相减。\n\n您的程序必须执行的任务：\n1. 对于下面测试套件中的每个测试值 $x$，计算两个浮点近似值：\n   - $A_{\\mathrm{dir}}=\\operatorname{fl}\\left(\\sqrt{1+x}-1\\right)$，\n   - $A_{\\mathrm{rat}}=\\operatorname{fl}\\left(\\dfrac{x}{\\sqrt{1+x}+1}\\right)$。\n2. 使用至少80位精度的精确实数算術模擬，计算一个高精度参考值 $f_{\\ast}(x)=\\sqrt{1+x}-1$。您不得使用外部文件或网络。如果 $f_{\\ast}(x)=0$，按照约定将其相对误差定义为 $0$。\n3. 对于每个近似值 $A\\in\\{A_{\\mathrm{dir}},A_{\\mathrm{rat}}\\}$，计算绝对误差 $E_{\\mathrm{abs}}=\\left|A-f_{\\ast}(x)\\right|$ 和相对误差 $E_{\\mathrm{rel}}=\\dfrac{\\left|A-f_{\\ast}(x)\\right|}{\\left|f_{\\ast}(x)\\right|}$ （当分母为零时，使用前述约定）。\n4. 将每个 $x$ 的结果按顺序汇总为一个列表 $[E_{\\mathrm{abs}}^{\\mathrm{dir}},E_{\\mathrm{rel}}^{\\mathrm{dir}},E_{\\mathrm{abs}}^{\\mathrm{rat}},E_{\\mathrm{rel}}^{\\mathrm{rat}}]$。\n\n测试套件：\n- 使用以下 $7$ 个 $x$ 值来探究不同区间内的行为，包括正、零和负的小值，以及接近双精度抵消阈值的值：\n  - $x\\in\\{10^{-1},\\,10^{-8},\\,10^{-12},\\,10^{-16},\\,0,\\,-10^{-16},\\,-10^{-12}\\}$。\n- 由于所列所有情况均满足 $x\\geq -1$，故所有平方根的参数均为非负数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，格式为标准编程语言字面表示法中的逗号分隔的列表之列表。第 $k$ 个内部列表必须对应于上面列出的第 $k$ 个测试值 $x$，并且必须按 $[E_{\\mathrm{abs}}^{\\mathrm{dir}},E_{\\mathrm{rel}}^{\\mathrm{dir}},E_{\\mathrm{abs}}^{\\mathrm{rat}},E_{\\mathrm{rel}}^{\\mathrm{rat}}]$ 的顺序排列。\n- 例如，格式应类似于 $[[e_{11},e_{12},e_{13},e_{14}],[e_{21},e_{22},e_{23},e_{24}],\\dots]$，其中每个 $e_{ij}$ 是一个浮点数。\n- 不涉及物理单位。不要打印任何附加文本。\n\n科学真实性和推导期望：\n- 需要揭示的现象是，尽管函数 $f(x)$ 在 $x=0$ 附近是良态的（其条件数趋向于 $1$），但由于灾难性抵消，直接相减法 $f_{\\mathrm{dir}}(x)$ 对于小的 $|x|$ 在算法上是不稳定的，而有理化形式 $f_{\\mathrm{rat}}(x)$ 则是数值稳定的。您的代码必须通过报告误差来从数值上证实这一点。",
            "solution": "问题要求分析在 $x$ 接近 $0$ 时，计算函数 $f(x) = \\sqrt{1+x} - 1$ 的两种代数等价公式的数值稳定性。这两个公式是直接求值法 $f_{\\mathrm{dir}}(x) = \\sqrt{1+x} - 1$ 和有理化形式 $f_{\\mathrm{rat}}(x) = \\frac{x}{\\sqrt{1+x}+1}$。其等价性通过代数变换建立：\n$$\nf(x) = (\\sqrt{1+x} - 1) \\times \\frac{\\sqrt{1+x}+1}{\\sqrt{1+x}+1} = \\frac{(\\sqrt{1+x})^2 - 1^2}{\\sqrt{1+x}+1} = \\frac{(1+x)-1}{\\sqrt{1+x}+1} = \\frac{x}{\\sqrt{1+x}+1}.\n$$\n该恒等式对所有 $x \\ge -1$ 成立。虽然两者在数学上是相同的，但它们在有限精度浮点算术下的行为差异显著，尤其是在 $|x|$ 很小时。此分析是灾难性抵消的一个经典例证。\n\n问题的核心在于直接计算公式 $f_{\\mathrm{dir}}(x)$。当 $x$ 趋近于 $0$ 时，$\\sqrt{1+x}$ 项趋近于 $1$。$\\sqrt{1+x}$ 在 $x=0$ 附近的泰勒级数展开为 $1 + \\frac{1}{2}x - \\frac{1}{8}x^2 + O(x^3)$。因此，对于小的 $|x|$，我们有 $\\sqrt{1+x} \\approx 1 + \\frac{1}{2}x$。于是，相减运算 $\\sqrt{1+x} - 1$ 就变成了两个几乎相等的数相减。\n\n在标准浮点算術中，实数 $y$被表示为有限精度的近似值 $\\operatorname{fl}(y)$。计算过程会引入舍入误差。当我们计算两个几乎相等的数之差 $a-b$（其中 $a \\approx b$）时，它们尾数的有效数字前导位会相互抵消。如果原始数值 $a$ 和 $b$ 存在舍入误差，这些原先位于最低有效位的误差现在会被提升到结果的最高有效位。这种现象被称为灾难性抵消，可能导致相对精度的急剧损失。\n\n让我们为计算 $f_{\\mathrm{dir}}(x)$ 的误差建模。首先，$1+x$ 被计算为 $\\operatorname{fl}(1+x) = (1+x)(1+\\delta_1)$。然后计算其平方根：$\\hat{y} = \\operatorname{fl}(\\sqrt{\\operatorname{fl}(1+x)}) \\approx \\sqrt{1+x}(1+\\delta_2)$。最后一步是相减：$A_{\\mathrm{dir}} = \\operatorname{fl}(\\hat{y}-1) \\approx (\\sqrt{1+x}(1+\\delta_2) - 1)(1+\\delta_3)$，其中 $|\\delta_i| \\le u$，$u$ 是单位舍入误差（对于64位双精度，$u \\approx 1.11 \\times 10^{-16}$）。结果的绝对误差约为 $|A_{\\mathrm{dir}} - f(x)| \\approx |\\sqrt{1+x}\\delta_2| \\approx |\\delta_2|$，因为 $\\sqrt{1+x} \\approx 1$。真实值为 $f(x) \\approx \\frac{1}{2}x$。因此，相对误差为\n$$\nE_{\\mathrm{rel}} = \\frac{|A_{\\mathrm{dir}} - f(x)|}{|f(x)|} \\approx \\frac{|\\delta_2|}{|\\frac{1}{2}x|} = \\frac{2|\\delta_2|}{|x|}.\n$$\n由于 $|\\delta_2|$ 的大小可以达到单位舍入误差 $u$，相对误差可能被一个与 $1/|x|$ 成正比的因子放大。例如，如果 $|x| \\approx u$，相对误差可能接近 $2$，这意味着所有有效数字都丢失了。\n\n相比之下，有理化公式 $f_{\\mathrm{rat}}(x) = \\frac{x}{\\sqrt{1+x}+1}$ 是数值稳定的。对于小的 $|x|$，分母 $\\sqrt{1+x}+1$ 是两个正数相加，这是一个良性运算。分母接近 $2$，因此不会发生抵消。计算涉及一次平方根、一次加法和一次除法。每次运算引入的相对误差都很小，量级与单位舍入误差 $u$ 相当。最终计算结果 $A_{\\mathrm{rat}}$ 的相对误差也将是 $u$ 的一个小数倍，而与 $|x|$ 的大小无关。\n\n为了从数值上验证此分析，我们将实现一个程序，对给定的 $x$ 值测试套件 $\\{10^{-1}, 10^{-8}, 10^{-12}, 10^{-16}, 0, -10^{-16}, -10^{-12}\\}$ 执行计算。\n1. 对于每个 $x$，将使用至少80位精度计算一个高精度参考值 $f_*(x)$。这可以通过使用Python的 `decimal` 模块来实现，该模块模拟任意精度算术。此参考值作为我们衡量误差的“真实”值。\n2. 两个近似值 $A_{\\mathrm{dir}} = \\operatorname{fl}(\\sqrt{1+x}-1)$ 和 $A_{\\mathrm{rat}} = \\operatorname{fl}(\\frac{x}{\\sqrt{1+x}+1})$ 将使用标准双精度浮点算术（对应于 `numpy.float64`）进行计算。\n3. 对于每个近似值 $A$，我们将计算绝对误差 $E_{\\mathrm{abs}} = |A-f_*(x)|$ 和相对误差 $E_{\\mathrm{rel}} = E_{\\mathrm{abs}}/|f_*(x)|$。对于 $x=0$ 的情况，其中 $f_*(0)=0$，按照约定，相对误差取为 $0$。\n4. 每个 $x$ 的结果将被聚合成一个包含四个值的列表：$[E_{\\mathrm{abs}}^{\\mathrm{dir}}, E_{\\mathrm{rel}}^{\\mathrm{dir}}, E_{\\mathrm{abs}}^{\\mathrm{rat}}, E_{\\mathrm{rel}}^{\\mathrm{rat}}]$。\n\n预期结果是，对于小的 $|x|$（例如 $10^{-8}$ 及更小的值），直接公式的相对误差 $E_{\\mathrm{rel}}^{\\mathrm{dir}}$ 将显著增大，从而证实灾难性抵消的发生。相反，有理化公式的相对误差 $E_{\\mathrm{rel}}^{\\mathrm{rat}}$ 在所有测试用例中应保持很小，量级与单位舍入误差 $u \\approx 10^{-16}$ 相当，从而证明其数值稳定性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Analyzes the numerical stability of two formulas for f(x) = sqrt(1+x) - 1.\n    \"\"\"\n    # Set precision for the high-precision reference calculation (more than 80 digits).\n    decimal.getcontext().prec = 100\n\n    # Define the test cases from the problem statement.\n    # Each tuple contains the string representation for high-precision Decimal\n    # and the float representation for standard double-precision numpy calculation.\n    test_cases = [\n        ('1e-1', 1e-1),\n        ('1e-8', 1e-8),\n        ('1e-12', 1e-12),\n        ('1e-16', 1e-16),\n        ('0', 0.0),\n        ('-1e-16', -1e-16),\n        ('-1e-12', -1e-12),\n    ]\n\n    results = []\n    \n    # Pre-calculate Decimal(1) for efficiency.\n    one_dec = decimal.Decimal(1)\n\n    for x_str, x_float in test_cases:\n        # Task 2: Compute a high-precision reference value.\n        x_dec = decimal.Decimal(x_str)\n        f_star = (one_dec + x_dec).sqrt() - one_dec\n\n        # Task 1: Compute two floating-point approximations.\n        x_f64 = np.float64(x_float)\n        one_f64 = np.float64(1.0)\n        \n        # Direct subtraction formula\n        A_dir = np.sqrt(one_f64 + x_f64) - one_f64\n        \n        # Rationalized formula\n        A_rat = x_f64 / (np.sqrt(one_f64 + x_f64) + one_f64)\n\n        # Task 3: Compute absolute and relative errors.\n        # Convert double-precision results to Decimal for accurate error calculation.\n        A_dir_dec = decimal.Decimal(A_dir)\n        A_rat_dec = decimal.Decimal(A_rat)\n\n        # Absolute errors\n        e_abs_dir = abs(A_dir_dec - f_star)\n        e_abs_rat = abs(A_rat_dec - f_star)\n\n        # Relative errors, with special handling for f_star = 0.\n        if f_star == 0:\n            e_rel_dir = decimal.Decimal(0)\n            e_rel_rat = decimal.Decimal(0)\n        else:\n            e_rel_dir = e_abs_dir / abs(f_star)\n            e_rel_rat = e_abs_rat / abs(f_star)\n\n        # Task 4: Aggregate the results for the current x.\n        # Convert Decimal error values to float for the final output.\n        results.append([\n            float(e_abs_dir), float(e_rel_dir),\n            float(e_abs_rat), float(e_rel_rat)\n        ])\n\n    # Final print statement in the exact required format.\n    # Creates a string like '[[e1,e2,e3,e4],[...]]' without extra whitespace.\n    inner_lists = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output = f\"[{','.join(inner_lists)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了通过代数重构来避免灾难性抵消的基本技巧后，我们进一步拓宽视野，探索更多样的数值稳定化策略。本练习聚焦于函数 $f(x) = \\frac{1 - \\cos x}{x}$ 在 $x$ 趋近于零时的计算问题，这同样是一个灾难性抵消的经典案例。你将有机会实现并比较四种不同的计算方法：朴素的直接计算、利用三角恒等式的稳定算法、基于泰勒级数展开的近似，以及使用补偿算法的更高精度计算，从而领略数值分析工具箱的丰富性与实用性。 ",
            "id": "3212289",
            "problem": "考虑当 $x$ 很小时，函数 $f(x) = \\dfrac{1 - \\cos x}{x}$ 的浮点求值。在减去两个几乎相等的数时会发生灾难性抵消，有效位的损失会影响后续运算的精度。使用以下基本依据：余弦函数的麦克劳林级数定义和二进制浮点算术的标准舍入模型。\n\n您必须为 $f(x)$ 推导并实现三种计算策略：\n- 使用直接求值 $f(x) = \\dfrac{1 - \\cos x}{x}$ 的朴素公式。\n- 一种避免减去几乎相等数值的数值稳定的三角恒等式。\n- 一种 $f(x)$ 的截断级数近似，从余弦函数的麦克劳林级数推导而来，使用 $f(x)$ 级数的前四个非零项，表示为 $x$ 的奇次多项式，并以数值上合理的方式进行求值。\n\n此外，为减法 $1 - \\cos x$ 实现一种补偿算法策略，该策略使用两个浮点数之和的无误差变换。此补偿减法必须产生两个浮点数，它们的和等于实数算术中输入的精确和。由此，通过将补偿后的差值除以 $x$ 来构成补偿后的 $f(x)$。\n\n使用以下定义和事实作为您推导和程序设计的起点：\n- 余弦函数的麦克劳林级数为 $\\cos x = \\sum_{k=0}^{\\infty} (-1)^k \\dfrac{x^{2k}}{(2k)!}$，其中 $x$ 以弧度为单位。\n- 二进制浮点算术的标准舍入模型为 $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$，其中 $|\\delta| \\leq u$，$u$ 是单位舍入误差，$\\circ \\in \\{+, -, \\times, \\div\\}$。\n\n您的程序必须：\n- 处理以弧度为单位的角度 $x$。\n- 对每个测试用例 $x$，计算四种近似值：朴素求值、稳定恒等式求值、使用前四个非零项的截断级数求值以及补偿减法求值。\n- 使用稳定恒等式求值作为参考值，为其他三种方法计算相对误差。相对误差必须报告为一个浮点数，计算公式为 $\\displaystyle \\mathrm{rel\\_err} = \\dfrac{|f_{\\text{approx}}(x) - f_{\\text{ref}}(x)|}{|f_{\\text{ref}}(x)|}$。\n\n设计一个小型的测试套件，以检验不同的情况：\n- 一般的小 $x$ 情况：$x = 10^{-1}$。\n- 一个非常小的 $x$ 情况，此时抵消现象严重：$x = 10^{-8}$。\n- 一个极小的 $x$ 情况：$x = 10^{-16}$。\n- 一个超小 $x$ 情况，这将迫使朴素减法在双精度下下溢为零：$x = 10^{-20}$。\n- 一个负的小 $x$ 情况，以测试奇函数的符号行为：$x = -10^{-8}$。\n- 一个中等大小的 $x$ 情况：$x = 1$。\n- 一个大角度的情况，此时级数截断应该会变差，但恒等式仍然准确：$x = \\pi$。\n\n对于每个测试用例 $x$，您的程序必须按以下确切顺序生成一个包含 7 个浮点数的列表：\n$[f_{\\text{naive}}(x), f_{\\text{identity}}(x), f_{\\text{series}}(x), f_{\\text{compensated}}(x), \\mathrm{rel\\_err}_{\\text{naive}}(x), \\mathrm{rel\\_err}_{\\text{series}}(x), \\mathrm{rel\\_err}_{\\text{compensated}}(x)]$。\n\n您的程序应生成单行输出，其中包含这些按测试用例排列的列表，以逗号分隔，并用方括号括起来，且不含空格，例如 $[[a_1,a_2,\\dots,a_7],[b_1,b_2,\\dots,b_7],\\dots]$，其中每个 $a_i$、$b_i$ 等都是浮点数。\n\n角度必须以弧度为单位。除了角度之外，没有其他物理单位；不得使用百分比，任何分数或小数必须表示为浮点数。",
            "solution": "在尝试任何解决方案之前，对用户提供的问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- **待求值函数**：$f(x) = \\dfrac{1 - \\cos x}{x}$。\n- **背景**：小 $x$ 值在浮点运算中存在的灾难性抵消。\n- **基本依据**：\n    - 余弦函数的麦克劳林级数：$\\cos x = \\sum_{k=0}^{\\infty} (-1)^k \\dfrac{x^{2k}}{(2k)!}$。\n    - 标准舍入模型：$\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$，其中 $|\\delta| \\leq u$。\n- **待实现的计算策略**：\n    1.  **朴素法**：直接求值 $f(x) = \\dfrac{1 - \\cos x}{x}$。\n    2.  **稳定恒等式**：一种避免问题减法的三角恒等式。\n    3.  **截断级数**：使用 $f(x)$ 麦克劳林级数的前四个非零项，并进行高效求值的近似方法。\n    4.  **补偿减法**：使用一种针对和/差的无误差变换来以更高精度计算 $1 - \\cos x$。\n- **程序要求**：\n    - 输入角度 $x$ 以弧度为单位。\n    - 对每个测试用例 $x$，计算四种近似值。\n    - 使用稳定恒等式求值作为参考值 $f_{\\text{ref}}(x)$。\n    - 为其他三种方法计算相对误差：$\\mathrm{rel\\_err} = \\dfrac{|f_{\\text{approx}}(x) - f_{\\text{ref}}(x)|}{|f_{\\text{ref}}(x)|}$。\n- **测试用例**：$x = \\{10^{-1}, 10^{-8}, 10^{-16}, 10^{-20}, -10^{-8}, 1, \\pi\\}$。\n- **输出格式**：对于每个 $x$，一个包含 7 个浮点数的列表：$[f_{\\text{naive}}(x), f_{\\text{identity}}(x), f_{\\text{series}}(x), f_{\\text{compensated}}(x), \\mathrm{rel\\_err}_{\\text{naive}}(x), \\mathrm{rel\\_err}_{\\text{series}}(x), \\mathrm{rel\\_err}_{\\text{compensated}}(x)]$。\n- **最终输出**：单行包含一个由逗号分隔的、按测试用例排列的列表的列表，并用括号括起来（例如，`[[...],[...],...]`）。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据既定标准对问题进行评估：\n- **科学依据**：该问题是数值分析中的一个典型例子，探讨了灾难性抵消和有效位损失这一被广泛理解的现象。所提出的方法是缓解此类误差的标准技术。\n- **适定性**：问题被精确地说明。函数、分析工具（级数、恒等式）、计算方法、测试用例和输出格式都已明确定义。这允许计算出唯一且可验证的一组结果。\n- **客观性**：问题以正式的数学和计算语言陈述，没有主观或模糊的术语。\n\n该问题未表现出任何无效性缺陷。它在科学上是合理的，形式上是明确的，是完整的，并且与数值计算的主题直接相关。\n\n### 步骤 3：结论与行动\n问题**有效**。将制定解决方案。\n\n---\n\n### 基于原理的解决方案设计\n\n该问题的核心是当 $x$ 趋近于 $0$ 时，计算 $f(x) = \\dfrac{1 - \\cos x}{x}$ 的数值不稳定性。当 $x \\to 0$ 时，$\\cos x \\to 1$。在浮点运算中，两个几乎相等的数 $1$ 和 $\\operatorname{fl}(\\cos x)$ 相减会导致灾难性抵消，其中最高有效位相消，留下的结果主要由舍入误差主导。我们将实现并比较四种计算 $f(x)$ 的方法。\n\n#### 1. 朴素求值\n朴素法直接实现公式：\n$$ f_{\\text{naive}}(x) = \\frac{1 - \\cos x}{x} $$\n设 $c_x = \\operatorname{fl}(\\cos x)$。对于小 $x$，$c_x$ 非常接近 $1$。计算出的差值 $d = \\operatorname{fl}(1 - c_x)$ 将有很大的相对误差。如果 $x$ 足够小（例如，$|x|  \\sqrt{u}$，其中 $u$ 是单位舍入误差），那么 $\\operatorname{fl}(\\cos x)$ 将精确为 $1$，分子将计算为 $0$，从而得到一个完全不准确的结果 $f(x) = 0$。\n\n#### 2. 稳定的三角恒等式\n为了避免几乎相等的数相减，我们可以重新表示分子 $1 - \\cos x$。正弦的半角恒等式是 $\\sin^2(\\theta) = \\dfrac{1 - \\cos(2\\theta)}{2}$。设 $\\theta = x/2$，我们得到 $2\\sin^2(x/2) = 1 - \\cos x$。将此代入 $f(x)$ 的表达式中，得到：\n$$ f_{\\text{identity}}(x) = \\frac{2\\sin^2(x/2)}{x} $$\n此公式在数值上是稳定的。对于小 $x$，正弦函数的参数 $x/2$ 也很小，且 $\\sin(x/2)$ 表现良好且计算准确。没有发生几乎相等量的减法。预计此方法在所有测试的 $x$ 范围内都是最准确的，并将作为我们的参考值 $f_{\\text{ref}}(x)$。\n\n#### 3. 截断级数近似\n我们可以从 $\\cos x$ 的级数推导出 $f(x)$ 的麦克劳林级数：\n$$ \\cos x = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\frac{x^8}{8!} - \\dots $$\n将此代入分子：\n$$ 1 - \\cos x = 1 - \\left(1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\frac{x^8}{8!} - \\dots\\right) = \\frac{x^2}{2!} - \\frac{x^4}{4!} + \\frac{x^6}{6!} - \\frac{x^8}{8!} + \\dots $$\n除以 $x$ 得到 $f(x)$ 的级数：\n$$ f(x) = \\frac{x}{2!} - \\frac{x^3}{4!} + \\frac{x^5}{6!} - \\frac{x^7}{8!} + \\dots $$\n问题要求使用前四个非零项。近似值为：\n$$ f_{\\text{series}}(x) \\approx \\frac{x}{2} - \\frac{x^3}{24} + \\frac{x^5}{720} - \\frac{x^7}{40320} $$\n为了以数值稳定且高效的方式求值此多项式，我们使用霍纳法则，该方法可以最小化算术运算的数量。多项式可因式分解为：\n$$ f_{\\text{series}}(x) = x \\left( \\frac{1}{2} + x^2 \\left( -\\frac{1}{24} + x^2 \\left( \\frac{1}{720} - \\frac{x^2}{40320} \\right) \\right) \\right) $$\n对于小的 $|x|$，这个近似非常准确，但随着 $|x|$ 的增加，由于截断误差，其精度会下降。\n\n#### 4. 补偿减法\n此方法旨在通过捕获浮点运算中的误差项来改进朴素减法 $1 - \\cos x$。一种无误差变换，例如 `TwoSum` 算法，可以计算出两个数 $s$ 和 $e$，它们表示一个加法的精确结果。我们将其应用于 $1 + (-\\cos x)$。针对 $a+b$ 的 `TwoSum` 算法是：\n$s = \\operatorname{fl}(a+b)$\n$v = \\operatorname{fl}(s-a)$\n$e = \\operatorname{fl}((a - (s-v)) + (b-v))$\n设 $a = 1.0$ 和 $b = -\\operatorname{fl}(\\cos x)$，我们计算 $s$ 和 $e$ 使得 $s+e = 1.0 - \\operatorname{fl}(\\cos x)$ 精确成立。$s$ 的值是朴素减法的结果，即 $f_{\\text{naive}}(x) \\cdot x$，而 $e$ 是计算出的误差。\n$f(x)$ 的补偿值为：\n$$ f_{\\text{compensated}}(x) = \\frac{s + e}{x} $$\n此方法可以恢复减法本身过程中损失的精度。然而，它无法恢复在初始计算 $\\operatorname{fl}(\\cos x)$ 时损失的信息。如果 $\\operatorname{fl}(\\cos x)$ 四舍五入到恰好为 $1.0$，则补偿求和的输入已经有缺陷，该方法将无法产生正确的非零结果，与朴素方法一样。\n\n#### 相对误差计算\n每种近似方法（$f_{\\text{approx}}$）的相对误差是相对于稳定恒等式求值（$f_{\\text{ref}}$）计算的：\n$$ \\mathrm{rel\\_err} = \\frac{|f_{\\text{approx}}(x) - f_{\\text{ref}}(x)|}{|f_{\\text{ref}}(x)|} $$\n选择 $f_{\\text{identity}}$ 作为参考是合理的，因为它在广泛的输入范围内具有数值稳定性，既避免了朴素方法的抵消问题，也避免了级数方法的截断误差。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes f(x) = (1 - cos(x)) / x using four different numerical methods\n    and calculates their relative errors with respect to a stable reference method.\n    \"\"\"\n\n    def two_sum(a, b):\n        \"\"\"\n        Computes s, e such that s + e = a + b exactly.\n        This is Knuth's two-sum algorithm.\n        \"\"\"\n        s = a + b\n        v = s - a\n        # (a - (s - v)) is the high-order part of a's error\n        # (b - v) is the low-order part of b's error\n        e = (a - (s - v)) + (b - v)\n        return s, e\n\n    # Test cases in radians\n    test_cases = [\n        1e-1,\n        1e-8,\n        1e-16,\n        1e-20,\n        -1e-8,\n        1.0,\n        np.pi\n    ]\n\n    all_results = []\n    for x in test_cases:\n        # Ensure a copy is used and it's a standard float\n        x_fl = float(x)\n        \n        # --- Method 1: Naive evaluation ---\n        # Suffers from catastrophic cancellation for small x.\n        cos_x = np.cos(x_fl)\n        if x_fl == 0.0:\n            f_naive = 0.0\n        else:\n            f_naive = (1.0 - cos_x) / x_fl\n\n        # --- Method 2: Stable trigonometric identity (Reference) ---\n        # Avoids subtraction of nearly equal numbers.\n        if x_fl == 0.0:\n            f_identity = 0.0\n        else:\n            f_identity = 2.0 * np.sin(x_fl / 2.0)**2 / x_fl\n        \n        f_ref = f_identity\n\n        # --- Method 3: Truncated series approximation ---\n        # Uses first four non-zero terms, evaluated via Horner's method.\n        # f(x) = x/2! - x^3/4! + x^5/6! - x^7/8!\n        #      = x * (1/2 + x^2*(-1/24 + x^2*(1/720 - x^2/40320)))\n        x_sq = x_fl * x_fl\n        c0 = 1.0 / 2.0\n        c1 = -1.0 / 24.0\n        c2 = 1.0 / 720.0\n        c3 = -1.0 / 40320.0\n        # Horner's method evaluation\n        f_series = x_fl * (c0 + x_sq * (c1 + x_sq * (c2 + x_sq * c3)))\n\n        # --- Method 4: Compensated subtraction ---\n        # Uses TwoSum to capture the error in 1 - cos(x).\n        if x_fl == 0.0:\n            f_compensated = 0.0\n        else:\n            # cos_x was computed for the naive method\n            s, e = two_sum(1.0, -cos_x)\n            f_compensated = (s + e) / x_fl\n            \n        # --- Relative Error Calculation ---\n        # Handle case where reference value is zero to avoid division by zero\n        if f_ref == 0.0:\n            rel_err_naive = 0.0 if f_naive == 0.0 else np.inf\n            rel_err_series = 0.0 if f_series == 0.0 else np.inf\n            rel_err_compensated = 0.0 if f_compensated == 0.0 else np.inf\n        else:\n            rel_err_naive = np.abs(f_naive - f_ref) / np.abs(f_ref)\n            rel_err_series = np.abs(f_series - f_ref) / np.abs(f_ref)\n            rel_err_compensated = np.abs(f_compensated - f_ref) / np.abs(f_ref)\n            \n        case_results = [\n            f_naive, \n            f_identity, \n            f_series, \n            f_compensated, \n            rel_err_naive, \n            rel_err_series, \n            rel_err_compensated\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string as specified: [[...],[...],...] with no spaces.\n    inner_parts = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output_string = f\"[{','.join(inner_parts)}]\"\n    \n    print(final_output_string)\n\nsolve()\n\n```"
        },
        {
            "introduction": "数值不稳定性不仅仅是理论上的趣闻，它在现实世界的数据分析中可能导致严重的错误。本练习将带你进入统计学领域，探讨一个核心概念——方差的计算。教科书中常见的“捷径”公式 $\\sigma^2 = E[X^2] - (E[X])^2$ 在数值上是出了名的不稳定，尤其当样本均值远大于其标准差时。通过这个实践，你将实现并对比这种朴素算法与多种工业界和科学计算软件中采用的稳健算法（如双遍法和Welford在线算法），深刻体会到为特定任务选择正确算法的至关重要性。 ",
            "id": "3212118",
            "problem": "你的任务是研究在使用公式 $E[X^2] - (E[X])^2$ 计算总体方差时出现的灾难性抵消和有效位数损失问题，并比较不同算法的数值稳定性。此项研究的基础是总体均值和方差的定义，以及带舍入的有限精度算术模型。\n\n需要使用的基本定义：\n- 对于 $n$ 个实数样本 $x_1, x_2, \\dots, x_n$，其总体均值为 $\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$。\n- 总体方差为 $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2$。\n- 方差的另一个等价恒等式是 $\\sigma^2 = E[X^2] - (E[X])^2$，其中 $E[X] = \\mu$ 且 $E[X^2] = \\frac{1}{n}\\sum_{i=1}^{n} x_i^2$。\n\n有限精度模型：\n- 实际计算在浮点运算中进行，该运算符合电气和电子工程师协会 (IEEE) $754$ 双精度格式。设单位舍入误差为 $u \\approx 2^{-53}$，并假设每次基本运算引入的微小相对误差的界限为一个常数乘以 $u$。\n\n你的任务：\n1. 实现四种算法来计算给定实数列表的总体方差 $\\sigma^2$：\n   - 算法 $A_1$ (朴素单遍法)：通过单遍累加 $\\sum x_i$ 和 $\\sum x_i^2$ 来计算 $E[X]$ 和 $E[X^2]$，然后返回 $E[X^2] - (E[X])^2$。\n   - 算法 $A_2$ (双遍法，未补偿)：在第一遍中使用 $\\mu = \\frac{1}{n}\\sum x_i$ 计算 $\\mu$，然后在第二遍中计算 $\\sigma^2 = \\frac{1}{n}\\sum (x_i - \\mu)^2$。\n   - 算法 $A_3$ (双遍法，使用 Kahan 补偿求和)：使用 Kahan 补偿求和来计算第一遍的均值和第二遍的离差平方和。\n   - 算法 $A_4$ (Welford 在线算法)：使用以下递推公式单遍计算 $\\sigma^2$\n     $$\\text{for } i = 1,2,\\dots,n: \\quad \\delta_i = x_i - \\mu_{i-1}, \\quad \\mu_i = \\mu_{i-1} + \\frac{\\delta_i}{i}, \\quad M2_i = M2_{i-1} + \\delta_i \\cdot (x_i - \\mu_i),$$\n     并返回 $\\sigma^2 = \\frac{M2_n}{n}$。\n\n2. 使用高精度基准来近似精确方差以进行比较。使用精度为 $p = 100$ 位的十进制算术计算参考均值和方差，将数据集中的值视为所提供的精确小数或整数。\n\n3. 对于每种算法，测量其与高精度基准的差异：\n   - 如果参考方差非零，计算相对误差 $r = \\frac{|\\widehat{\\sigma^2} - \\sigma^2|}{|\\sigma^2|}$，其中 $\\widehat{\\sigma^2}$ 是算法的结果。\n   - 如果参考方差为零，计算绝对误差 $a = |\\widehat{\\sigma^2} - 0|$。\n\n测试套件：\n在以下数据集上提供结果，这些数据集旨在测试一般情况、易于发生抵消的情况和边界情况。在每种情况下，将列表解释为实数：\n- 情况 1 (一般“理想”情况)：$[1,2,3,4,5]$。\n- 情况 2 (大偏移量，小方差，在 $E[X^2] - (E[X])^2$ 中导致灾难性抵消)：$[10^{8} + 0, 10^{8} + 1, 10^{8} + 2, 10^{8} + 3, 10^{8} + 4]$。\n- 情况 3 (常数序列，真实方差为零)：$[12345678, 12345678, 12345678, 12345678, 12345678]$。\n- 情况 4 (高动态范围，带符号变化)：$[10^{16}, 10^{16} + 1, -10^{16}, -10^{16} + 1]$。\n- 情况 5 (多点，大偏移量，小增量)：$[10^{12} + i \\text{ for } i = 0,1,2,\\dots,999]$。\n- 情况 6 (单个元素，真实方差为零)：$[42]$。\n\n实现约束：\n- 程序必须是一个完整的、可运行的 Python 程序，仅使用 Python 标准库和版本为 $1.23.5$ 的 NumPy 库。\n- 程序不得读取任何输入，也不得访问文件或网络。\n- 对每种情况，使用精度为 $p=100$ 的十进制算术计算参考均值和方差。\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，包含一个由方括号括起来的列表的列表，其中不含空格，格式为逗号分隔。每个内部列表按上述顺序对应一个测试用例，并包含四个数字（浮点数），分别代表算法 $A_1$、$A_2$、$A_3$ 和 $A_4$ 的误差度量。\n- 示例格式：$[[e_{1,1},e_{1,2},e_{1,3},e_{1,4}],[e_{2,1},e_{2,2},e_{2,3},e_{2,4}],\\dots]$，其中每个 $e_{i,j}$ 根据参考方差是否为零，是相对误差（浮点数）或绝对误差（浮点数）。",
            "solution": "该问题陈述已经过验证，被认为是合理的。它提出了一个适定的、有科学依据且可形式化的数值分析任务。所有定义、算法和测试用例都是标准的且明确规定，允许直接和无歧义的实现与分析。该问题是统计计算中数值不稳定性及其缓解方法的经典演示。\n\n问题的核心在于计算总体方差 $\\sigma^2$ 的不同公式的数值特性。对于一个包含 $n$ 个样本 $\\{x_1, x_2, \\dots, x_n\\}$ 且均值为 $\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ 的总体，其方差定义为与均值之差的平方的均值：\n$$\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\mu)^2\n$$\n一个通过展开平方得到的数学上等价的公式是：\n$$\n\\sigma^2 = E[X^2] - (E[X])^2 = \\left(\\frac{1}{n}\\sum_{i=1}^{n} x_i^2\\right) - \\left(\\frac{1}{n}\\sum_{i=1}^{n} x_i\\right)^2\n$$\n虽然这些公式在精确算术中是等价的，但它们在有限精度浮点运算中的表现可能截然不同。我们将分析指定的四种算法。\n\n**高精度基准**\n\n为了量化浮点算法的误差，需要一个“真实值”或参考值。由于输入值是作为精确数字（整数或简单小数）给出的，我们可以使用任意精度算术来计算一个高度精确的参考方差。Python 的 `decimal` 模块，配置为高精度（例如 $p=100$ 位），可以达到此目的。通过使用这种高精度执行所有计算（均值、减法、平方和求和），我们得到的结果与标准双精度浮点数的有限精度相比，可以认为是完全精确的。\n\n**算法 $A_1$：朴素单遍算法**\n\n该算法直接实现公式 $\\sigma^2 = E[X^2] - (E[X])^2$。它在单遍处理中计算样本总和 $\\sum x_i$ 和平方和 $\\sum x_i^2$。这种方法容易受到**灾难性抵消**的影响。当两个几乎相等的数相减时，就会发生这种情况，导致结果的相对精度损失。如果数据由围绕一个大均值 $C$ 聚集的值组成（即 $x_i \\approx C$），那么 $E[X] \\approx C$，并且 $E[X^2] \\approx C^2$。方差依赖于 $x_i$ 与均值之间的微小偏差，它是通过两个相近的大数 $E[X^2]$ 和 $(E[X])^2$ 相减得到的。在浮点运算中，这两个数的高位数字会相互抵消，结果将主要由舍入误差主导，常常产生一个高度不准确、甚至可能是负数的方差。\n\n**算法 $A_2$：双遍算法**\n\n该算法遵循定义公式 $\\sigma^2 = \\frac{1}{n}\\sum (x_i - \\mu)^2$。它需要对数据进行两遍处理：\n1.  第一遍：计算均值 $\\mu = \\frac{1}{n}\\sum x_i$。\n2.  第二遍：计算离差平方和 $\\sum (x_i - \\mu)^2$ 并除以 $n$。\n\n这种方法在数值上比 $A_1$ 稳定得多。通过首先计算均值，然后从每个数据点中减去它，后续的求和是在以零为中心的值 $(x_i - \\mu)$ 上执行的。这避免了两个大数相减，从而防止了灾难性抵消。该方法的准确性主要取决于第一遍中计算出的均值的准确性。\n\n**算法 $A_3$：带 Kahan 补偿求和的双遍算法**\n\n这是 $A_2$ 的一个改进。浮点数的标准求和会累积显著的误差，特别是对于大型数据集。Kahan 求和是一种通过跟踪每次加法中丢失的低位比特的运行补偿来缓解此问题的算法。对于一个和 $S = \\sum x_i$，Kahan 算法的流程如下：\n`sum = 0.0`, `c = 0.0`\n对于每个 `x`:\n  `y = x - c`\n  `t = sum + y`\n  `c = (t - sum) - y`\n  `sum = t`\n变量 `c` 累积了舍入误差。算法 $A_3$ 将此补偿求和应用于第一遍（计算 $\\mu$）和第二遍（计算 $\\sum(x_i-\\mu)^2$），从而产生比 $A_2$ 更准确的结果，尤其是在数据点数量 $n$ 很大时。\n\n**算法 $A_4$：Welford 在线算法**\n\nWelford 算法是一种数值稳定的单遍方法。它避免了存储整个数据集，使其成为一种适用于流式数据的“在线”算法。它迭代更新均值 $\\mu$ 和与均值之差的平方和 $M_2$。对于一个新的数据点 $x_i$，更新公式为：\n$$\n\\delta_i = x_i - \\mu_{i-1} \\\\\n\\mu_i = \\mu_{i-1} + \\frac{\\delta_i}{i} \\\\\nM_{2,i} = M_{2,i-1} + \\delta_i(x_i - \\mu_i)\n$$\n初始条件为 $\\mu_0 = 0$ 和 $M_{2,0} = 0$。最终的总体方差为 $\\sigma^2 = M_{2,n}/n$。该算法通过基于微小差异（$\\delta_i$）的更新来整合新数据点，从而保持稳定性，这在精神上类似于双遍法，但在单遍中完成。其稳定性与双遍算法相当。\n\n**测试用例的预期结果**\n\n- **情况 1 (一般)：** $[1, 2, 3, 4, 5]$。预计所有算法都会表现良好，因为数值较小，且方差相对于均值而言不小。\n- **情况 2 (大偏移量，小方差)：** $[10^8, \\dots, 10^8+4]$。这是灾难性抵消的经典案例。预计 $A_1$ 会严重失效，可能产生接近零的结果，而 $A_2$、$A_3$ 和 $A_4$ 应该会很准确。\n- **情况 3 (零方差)：** $[12345678, \\dots]$。真实方差为 $0$。$A_1$ 可能因舍入误差而产生一个小的非零或负数结果。其他算法应产生非常接近 $0$ 的结果。\n- **情况 4 (高动态范围)：** $[10^{16}, 10^{16}+1, -10^{16}, -10^{16}+1]$。这测试了对带有不同符号的大数求和的效果。这些数的量级接近 `float64` 的精度极限，可能导致中间计算中有效位数损失。所有算法都可能出现一些误差，但预计 $A_2$、$A_3$ 和 $A_4$ 会更稳健。\n- **情况 5 (多点，大偏移量)：** $[10^{12}, \\dots, 10^{12}+999]$。与情况 2 类似，但 $n=1000$。更大的数据集可能会放大求和误差。$A_1$ 将会失效。由于补偿求和，$A_3$ 可能比 $A_2$ 略有优势。\n- **情况 6 (单点)：** $[42]$。真实方差为 $0$。所有算法都应正确计算出方差为 $0$。\n\n该实现将计算每种算法和每种情况下相对于高精度基准的误差，从而在实践中展示这些数值特性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Main function to run the variance algorithm comparison.\n    \"\"\"\n    decimal.getcontext().prec = 100\n\n    def get_reference_variance(data_list):\n        \"\"\"\n        Computes the population variance using high-precision decimal arithmetic.\n        \"\"\"\n        if not data_list or len(data_list) == 0:\n            return decimal.Decimal(0)\n        \n        n = len(data_list)\n        data_dec = [decimal.Decimal(str(x)) for x in data_list]\n        \n        mean_dec = sum(data_dec) / decimal.Decimal(n)\n        \n        var_dec = sum((x - mean_dec)**2 for x in data_dec) / decimal.Decimal(n)\n        \n        return var_dec\n\n    def alg1_naive(data: np.ndarray) - np.float64:\n        \"\"\"Algorithm A1: Naive one-pass E[X^2] - (E[X])^2.\"\"\"\n        n = data.size\n        if n == 0:\n            return np.float64(0.0)\n        \n        # In a single pass, one would loop. np.sum is equivalent and fast.\n        sum_x = np.sum(data)\n        sum_x_sq = np.sum(data**2)\n        \n        mean_x = sum_x / n\n        mean_x_sq = sum_x_sq / n\n        \n        return mean_x_sq - mean_x**2\n\n    def alg2_twopass(data: np.ndarray) - np.float64:\n        \"\"\"Algorithm A2: Two-pass definitional formula.\"\"\"\n        n = data.size\n        if n == 0:\n            return np.float64(0.0)\n        \n        mean = np.sum(data) / n\n        sum_sq_dev = np.sum((data - mean)**2)\n        \n        return sum_sq_dev / n\n\n    def kahan_sum(arr):\n        \"\"\"Kahan compensated summation.\"\"\"\n        s = np.float64(0.0)\n        c = np.float64(0.0)\n        for x in arr:\n            y = np.float64(x - c)\n            t = s + y\n            c = (t - s) - y\n            s = t\n        return s\n\n    def alg3_kahan(data: np.ndarray) - np.float64:\n        \"\"\"Algorithm A3: Two-pass with Kahan summation.\"\"\"\n        n = data.size\n        if n == 0:\n            return np.float64(0.0)\n            \n        mean = kahan_sum(data) / n\n        \n        # The terms (data - mean)**2 are computed in standard float arithmetic\n        # before being summed with Kahan.\n        sq_devs = (data - mean)**2\n        sum_sq_dev = kahan_sum(sq_devs)\n        \n        return sum_sq_dev / n\n        \n    def alg4_welford(data: np.ndarray) - np.float64:\n        \"\"\"Algorithm A4: Welford's one-pass online algorithm.\"\"\"\n        n = data.size\n        if n == 0:\n            return np.float64(0.0)\n\n        mean = np.float64(0.0)\n        m2 = np.float64(0.0)\n        \n        for i, x in enumerate(data, 1):\n            delta = x - mean\n            mean += delta / i\n            delta2 = x - mean\n            m2 += delta * delta2\n            \n        return m2 / n\n\n    test_cases = [\n        # Case 1: general happy path\n        ([1.0, 2.0, 3.0, 4.0, 5.0]),\n        # Case 2: large offset, small variance\n        ([1e8 + 0.0, 1e8 + 1.0, 1e8 + 2.0, 1e8 + 3.0, 1e8 + 4.0]),\n        # Case 3: constant sequence, true variance zero\n        ([12345678.0] * 5),\n        # Case 4: high dynamic range\n        ([1e16, 1e16 + 1.0, -1e16, -1e16 + 1.0]),\n        # Case 5: many points, large offset\n        ([1e12 + float(i) for i in range(1000)]),\n        # Case 6: single element, true variance zero\n        ([42.0])\n    ]\n\n    results = []\n    algorithms = [alg1_naive, alg2_twopass, alg3_kahan, alg4_welford]\n\n    for data_list in test_cases:\n        case_errors = []\n        data_np = np.array(data_list, dtype=np.float64)\n        ref_var = get_reference_variance(data_list)\n\n        for alg in algorithms:\n            computed_var = alg(data_np)\n            \n            if ref_var == 0:\n                error = abs(computed_var)\n            else:\n                error = abs(computed_var - float(ref_var)) / float(ref_var)\n            case_errors.append(float(error))\n        results.append(case_errors)\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}