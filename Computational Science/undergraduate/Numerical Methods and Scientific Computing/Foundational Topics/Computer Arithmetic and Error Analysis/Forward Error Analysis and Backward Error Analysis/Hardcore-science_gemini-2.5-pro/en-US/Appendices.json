{
    "hands_on_practices": [
        {
            "introduction": "Understanding the distinction between a problem's inherent sensitivity and an algorithm's performance is fundamental. This first exercise explores the phenomenon of catastrophic cancellation, where subtracting two nearly equal numbers leads to a significant loss of precision. By calculating the backward error, you will see how a computed result, though far from the true answer (large forward error), can be explained as the exact answer to a very slightly modified problem (small backward error) .",
            "id": "3231943",
            "problem": "Let $x$ and $y$ be real numbers stored and subtracted in a decimal floating-point system that rounds to nearest with precision $p=7$ significant digits. Let the true inputs be $x=1.0000004$ and $y=1.0000001$. The stored values are $\\mathrm{fl}(x)$ and $\\mathrm{fl}(y)$, each obtained by rounding the true value to $7$ significant digits, and the computed subtraction is $z_{\\mathrm{comp}}=\\mathrm{fl}(\\mathrm{fl}(x)-\\mathrm{fl}(y))$. Use the core definitions of forward error and backward error, together with the fact that rounding to $p$ significant digits is a well-tested model of floating-point storage, to analyze the subtraction of nearly equal numbers. Specifically, determine the relative backward error in $x$, defined as $\\frac{|\\Delta x|}{|x|}$ where $\\Delta x$ is the smallest perturbation to $x$ such that the exact subtraction $(x+\\Delta x)-y$ equals $0$. Round your final numerical answer to $4$ significant figures.",
            "solution": "The problem has been validated and is deemed a well-posed and scientifically sound problem in the domain of numerical analysis. It requires the calculation of a relative backward error for a subtraction operation under specific definitions.\n\nThe objective is to determine the relative backward error in the input $x$ for the operation $x-y$, where the backward error is defined with respect to a target result of $0$. The given true inputs are $x=1.0000004$ and $y=1.0000001$. The relative backward error in $x$ is given by the formula $\\frac{|\\Delta x|}{|x|}$, where $\\Delta x$ is the smallest perturbation to $x$ that causes the exact subtraction to yield $0$.\n\nFirst, let us analyze the context provided by the floating-point system. The system is a decimal floating-point system with precision $p=7$ and rounds to the nearest representable number.\nThe true value of $x$ is $1.0000004$. To store this value with $7$ significant digits, we must round it. The number can be written as $1.0000004 \\times 10^0$. The $8^{th}$ significant digit is $4$. Since $4 < 5$, we round down.\nThus, the stored value of $x$ is $\\mathrm{fl}(x) = 1.000000$.\nThe true value of $y$ is $1.0000001$. Similarly, this is $1.0000001 \\times 10^0$. The $8^{th}$ significant digit is $1$. Since $1 < 5$, we round down.\nThus, the stored value of $y$ is $\\mathrm{fl}(y) = 1.000000$.\n\nThe computed subtraction is given by $z_{\\mathrm{comp}}=\\mathrm{fl}(\\mathrm{fl}(x)-\\mathrm{fl}(y))$.\nSubstituting the stored values:\n$z_{\\mathrm{comp}} = \\mathrm{fl}(1.000000 - 1.000000) = \\mathrm{fl}(0) = 0$.\nThis confirms that the computed result of the subtraction in the specified floating-point system is indeed $0$. The problem is therefore asking for the backward error of the actual computation performed, where the computed result is $0$. This phenomenon, where the subtraction of two nearly equal numbers results in a loss of precision (in this case, total loss), is known as catastrophic cancellation.\n\nThe core task is to find the perturbation $\\Delta x$ as defined in the problem. The problem states that $\\Delta x$ is the smallest perturbation to $x$ such that the exact subtraction $(x+\\Delta x)-y$ equals $0$.\nWe set up the equation based on this definition:\n$$ (x + \\Delta x) - y = 0 $$\nThis is a linear equation in $\\Delta x$. We can solve for $\\Delta x$ by rearranging the terms:\n$$ \\Delta x = y - x $$\nNow, we substitute the given true values for $x$ and $y$:\n$$ x = 1.0000004 $$\n$$ y = 1.0000001 $$\nSo, the perturbation $\\Delta x$ is:\n$$ \\Delta x = 1.0000001 - 1.0000004 = -0.0000003 $$\nThe problem specifies the \"smallest perturbation\". Since the equation for $\\Delta x$ has a unique solution, this solution is trivially the smallest in magnitude. The magnitude of the perturbation is:\n$$ |\\Delta x| = |-0.0000003| = 0.0000003 = 3 \\times 10^{-7} $$\n\nNext, we compute the relative backward error in $x$, which is defined as $\\frac{|\\Delta x|}{|x|}$.\nThe magnitude of $x$ is $|x| = |1.0000004| = 1.0000004$.\nThe relative backward error is therefore:\n$$ \\frac{|\\Delta x|}{|x|} = \\frac{3 \\times 10^{-7}}{1.0000004} $$\nNow, we perform the numerical division:\n$$ \\frac{|\\Delta x|}{|x|} \\approx 2.999998800000479... \\times 10^{-7} $$\nThe problem requires this final numerical answer to be rounded to $4$ significant figures. Let the value be $V = 2.9999988... \\times 10^{-7}$.\nThe first four significant digits are $2$, $9$, $9$, $9$. The fifth significant digit is $9$. Since $9 \\ge 5$, we must round up the fourth digit. Rounding $2.999$ up results in $3.000$. The trailing zeros are significant and must be included to indicate the precision of the rounded result.\nSo, the relative backward error, rounded to $4$ significant figures, is $3.000 \\times 10^{-7}$.\n\nThis small backward error indicates that the algorithm (subtraction) is backward stable for this instance, as a very small relative change in the input $x$ is sufficient to explain the computed output of $0$. However, it is important to note that the forward error is large. The true result is $x-y = 0.0000003$, while the computed result is $0$. The relative forward error is $\\frac{|(x-y) - z_{\\mathrm{comp}}|}{|x-y|} = \\frac{|0.0000003 - 0|}{|0.0000003|} = 1$, which represents a $100\\%$ error. This highlights the distinction between backward stability and forward error amplification.",
            "answer": "$$\\boxed{3.000 \\times 10^{-7}}$$"
        },
        {
            "introduction": "We now extend our analysis from a single operation to a system of equations, using a familiar geometric scenario. Finding the intersection of two nearly parallel lines is a classic example of an ill-conditioned problem, where the solution is extremely sensitive to small changes in the input data. This practice will guide you through quantifying this sensitivity by calculating an amplification factor that connects the small backward error in the line coefficients to the large forward error in the intersection point .",
            "id": "3232092",
            "problem": "You are given two lines in the plane, described in slope-intercept form by $y = m_1 x + d_1$ and $y = m_2 x + d_2$, with $m_1 = 1$, $d_1 = 0$, $m_2 = 1 + 10^{-6}$, and $d_2 = 1$. Their intersection can be framed as a $2 \\times 2$ linear system $A \\mathbf{x} = \\mathbf{c}$ where $A = \\begin{pmatrix} -m_1 & 1 \\\\ -m_2 & 1 \\end{pmatrix}$, $\\mathbf{x} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$, and $\\mathbf{c} = \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix}$. Because the lines are nearly parallel, the matrix $A$ is nearly singular. Suppose the coefficients are measured with small relative errors, yielding perturbed values $\\tilde{m}_2 = m_2 (1 + \\varepsilon_m)$ and $\\tilde{d}_2 = d_2 (1 + \\varepsilon_d)$ with $\\varepsilon_m = 10^{-8}$ and $\\varepsilon_d = -10^{-8}$, while $m_1$ and $d_1$ are unchanged. Treat $\\tilde{m}_2$ and $\\tilde{d}_2$ as the only perturbed data. \n\nStarting from the core definitions of forward error, backward error, and the geometric interpretation of line intersections, complete the following tasks:\n\n- Compute the exact intersection $\\mathbf{x}^{\\ast} = \\begin{pmatrix} x^{\\ast} \\\\ y^{\\ast} \\end{pmatrix}$ of the unperturbed lines.\n- Compute the intersection $\\tilde{\\mathbf{x}} = \\begin{pmatrix} \\tilde{x} \\\\ \\tilde{y} \\end{pmatrix}$ of the perturbed lines using the perturbed coefficients $\\tilde{m}_2$ and $\\tilde{d}_2$.\n- Using the Euclidean norm (also called the $2$-norm), evaluate the normwise relative forward error $\\frac{\\|\\tilde{\\mathbf{x}} - \\mathbf{x}^{\\ast}\\|_{2}}{\\|\\mathbf{x}^{\\ast}\\|_{2}}$.\n- Define the coefficientwise relative backward error as $\\max\\!\\left\\{\\frac{|\\tilde{m}_2 - m_2|}{|m_2|}, \\frac{|\\tilde{d}_2 - d_2|}{|d_2|}\\right\\}$ and evaluate it for the given perturbations.\n- Compute the amplification factor, defined as the ratio of the normwise relative forward error to the coefficientwise relative backward error, and provide its numerical value.\n\nRound your final amplification factor to four significant figures. No units are required.",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. It provides a complete and consistent set of data and definitions to solve a standard problem in numerical analysis concerning error amplification in ill-conditioned linear systems. Therefore, the problem is valid, and a solution will be provided.\n\nThe problem asks for the analysis of the intersection of two lines, which is equivalent to solving a $2 \\times 2$ linear system. We are given the unperturbed system and a perturbed version and tasked with calculating the forward error, backward error, and their ratio (the amplification factor).\n\nThe two lines are described by:\nLine 1: $y = m_1 x + d_1$\nLine 2: $y = m_2 x + d_2$\n\nGiven unperturbed coefficients are $m_1 = 1$, $d_1 = 0$, $m_2 = 1 + 10^{-6}$, and $d_2 = 1$.\n\n**Step 1: Compute the exact intersection $\\mathbf{x}^{\\ast}$**\nThe intersection $(x^{\\ast}, y^{\\ast})$ is found by setting the expressions for $y$ equal:\n$$m_1 x^{\\ast} + d_1 = m_2 x^{\\ast} + d_2$$\nSolving for $x^{\\ast}$:\n$$(m_1 - m_2) x^{\\ast} = d_2 - d_1$$\n$$x^{\\ast} = \\frac{d_2 - d_1}{m_1 - m_2}$$\nSubstituting the given values:\n$$x^{\\ast} = \\frac{1 - 0}{1 - (1 + 10^{-6})} = \\frac{1}{-10^{-6}} = -10^6$$\nNow, we find $y^{\\ast}$ using the equation for Line 1:\n$$y^{\\ast} = m_1 x^{\\ast} + d_1 = (1)(-10^6) + 0 = -10^6$$\nThus, the exact intersection is $\\mathbf{x}^{\\ast} = \\begin{pmatrix} x^{\\ast} \\\\ y^{\\ast} \\end{pmatrix} = \\begin{pmatrix} -10^6 \\\\ -10^6 \\end{pmatrix}$.\n\n**Step 2: Compute the perturbed intersection $\\tilde{\\mathbf{x}}$**\nThe coefficients $m_2$ and $d_2$ are perturbed. The new coefficients are $\\tilde{m}_2$ and $\\tilde{d}_2$.\nThe perturbations are given by $\\varepsilon_m = 10^{-8}$ and $\\varepsilon_d = -10^{-8}$.\n$$\\tilde{m}_2 = m_2(1 + \\varepsilon_m) = (1 + 10^{-6})(1 + 10^{-8}) = 1 + 10^{-6} + 10^{-8} + 10^{-14}$$\n$$\\tilde{d}_2 = d_2(1 + \\varepsilon_d) = 1(1 - 10^{-8}) = 1 - 10^{-8}$$\nThe coefficients $m_1$ and $d_1$ remain unchanged. The perturbed intersection $(\\tilde{x}, \\tilde{y})$ is found by solving the system with the perturbed coefficients:\n$$m_1 \\tilde{x} + d_1 = \\tilde{m}_2 \\tilde{x} + \\tilde{d}_2$$\n$$\\tilde{x} = \\frac{\\tilde{d}_2 - d_1}{m_1 - \\tilde{m}_2}$$\nSubstituting the values:\n$$\\tilde{x} = \\frac{(1 - 10^{-8}) - 0}{1 - (1 + 10^{-6} + 10^{-8} + 10^{-14})} = \\frac{1 - 10^{-8}}{-10^{-6} - 10^{-8} - 10^{-14}}$$\nFor the new $y$-coordinate, using the unperturbed Line 1 equation:\n$$\\tilde{y} = m_1 \\tilde{x} + d_1 = (1)\\tilde{x} + 0 = \\tilde{x}$$\nSo, the perturbed intersection is $\\tilde{\\mathbf{x}} = \\begin{pmatrix} \\tilde{x} \\\\ \\tilde{x} \\end{pmatrix}$.\n\n**Step 3: Evaluate the normwise relative forward error**\nThe normwise relative forward error is defined as $\\frac{\\|\\tilde{\\mathbf{x}} - \\mathbf{x}^{\\ast}\\|_{2}}{\\|\\mathbf{x}^{\\ast}\\|_{2}}$.\nFirst, calculate the norms and the difference vector.\n$$\\|\\mathbf{x}^{\\ast}\\|_{2} = \\sqrt{(x^{\\ast})^2 + (y^{\\ast})^2} = \\sqrt{(-10^6)^2 + (-10^6)^2} = \\sqrt{2 \\cdot 10^{12}} = \\sqrt{2} \\cdot 10^6$$\nThe difference vector is:\n$$\\tilde{\\mathbf{x}} - \\mathbf{x}^{\\ast} = \\begin{pmatrix} \\tilde{x} - (-10^6) \\\\ \\tilde{x} - (-10^6) \\end{pmatrix} = \\begin{pmatrix} \\tilde{x} + 10^6 \\\\ \\tilde{x} + 10^6 \\end{pmatrix}$$\nThe norm of the difference vector is:\n$$\\|\\tilde{\\mathbf{x}} - \\mathbf{x}^{\\ast}\\|_{2} = \\sqrt{(\\tilde{x} + 10^6)^2 + (\\tilde{x} + 10^6)^2} = \\sqrt{2(\\tilde{x} + 10^6)^2} = \\sqrt{2} |\\tilde{x} + 10^6|$$\nThe relative forward error, $E_{fwd}$, is the ratio of these norms:\n$$E_{fwd} = \\frac{\\sqrt{2} |\\tilde{x} + 10^6|}{\\sqrt{2} \\cdot 10^6} = \\frac{|\\tilde{x} + 10^6|}{10^6}$$\nLet's compute the value of $\\tilde{x} + 10^6$:\n$$\\tilde{x} + 10^6 = \\frac{1 - 10^{-8}}{-(10^{-6} + 10^{-8} + 10^{-14})} + 10^6$$\n$$= \\frac{(1 - 10^{-8}) - 10^6(10^{-6} + 10^{-8} + 10^{-14})}{-(10^{-6} + 10^{-8} + 10^{-14})}$$\n$$= \\frac{1 - 10^{-8} - (1 + 10^{-2} + 10^{-8})}{-(10^{-6} + 10^{-8} + 10^{-14})}$$\n$$= \\frac{-10^{-2} - 2 \\cdot 10^{-8}}{-(10^{-6} + 10^{-8} + 10^{-14})} = \\frac{10^{-2} + 2 \\cdot 10^{-8}}{10^{-6} + 10^{-8} + 10^{-14}}$$\nTherefore, the relative forward error is:\n$$E_{fwd} = \\frac{1}{10^6} \\left( \\frac{10^{-2} + 2 \\cdot 10^{-8}}{10^{-6} + 10^{-8} + 10^{-14}} \\right) = \\frac{10^{-8} + 2 \\cdot 10^{-14}}{10^{-6} + 10^{-8} + 10^{-14}}$$\nWe can factor out common terms to simplify:\n$$E_{fwd} = \\frac{10^{-2}(1 + 2 \\cdot 10^{-6})}{10^6 \\cdot 10^{-6}(1 + 10^{-2} + 10^{-8})} = 10^{-2} \\frac{1 + 2 \\cdot 10^{-6}}{1 + 10^{-2} + 10^{-8}}$$\n\n**Step 4: Evaluate the coefficientwise relative backward error**\nThe coefficientwise relative backward error, $E_{bwd}$, is defined as:\n$$E_{bwd} = \\max\\!\\left\\{\\frac{|\\tilde{m}_2 - m_2|}{|m_2|}, \\frac{|\\tilde{d}_2 - d_2|}{|d_2|}\\right\\}$$\nUsing the definitions $\\tilde{m}_2 = m_2(1+\\varepsilon_m)$ and $\\tilde{d}_2 = d_2(1+\\varepsilon_d)$:\n$$\\frac{|\\tilde{m}_2 - m_2|}{|m_2|} = \\frac{|m_2(1+\\varepsilon_m) - m_2|}{|m_2|} = \\frac{|m_2 \\varepsilon_m|}{|m_2|} = |\\varepsilon_m|$$\n$$\\frac{|\\tilde{d}_2 - d_2|}{|d_2|} = \\frac{|d_2(1+\\varepsilon_d) - d_2|}{|d_2|} = \\frac{|d_2 \\varepsilon_d|}{|d_2|} = |\\varepsilon_d|$$\nGiven $\\varepsilon_m = 10^{-8}$ and $\\varepsilon_d = -10^{-8}$:\n$$E_{bwd} = \\max\\{|10^{-8}|, |-10^{-8}|\\} = \\max\\{10^{-8}, 10^{-8}\\} = 10^{-8}$$\n\n**Step 5: Compute the amplification factor**\nThe amplification factor is the ratio of the relative forward error to the relative backward error:\n$$\\text{Amplification Factor} = \\frac{E_{fwd}}{E_{bwd}}$$\n$$\\text{Amplification Factor} = \\frac{10^{-2} \\frac{1 + 2 \\cdot 10^{-6}}{1 + 10^{-2} + 10^{-8}}}{10^{-8}} = 10^6 \\frac{1 + 2 \\cdot 10^{-6}}{1 + 10^{-2} + 10^{-8}}$$\nNow, we compute the numerical value:\n$$\\text{Amplification Factor} = 10^6 \\frac{1.000002}{1.01000001} \\approx 10^6(0.990100980297)$$\n$$\\text{Amplification Factor} \\approx 990100.980297$$\nThe problem requires rounding the final answer to four significant figures. The first four significant digits are $9, 9, 0, 1$. The fifth digit is $0$, so we round down.\n$$\\text{Amplification Factor} \\approx 990100$$\nIn scientific notation, this is $9.901 \\times 10^5$.",
            "answer": "$$\\boxed{9.901 \\times 10^5}$$"
        },
        {
            "introduction": "Our final practice shifts focus from the conditioning of the problem to the stability of the algorithm. Two algorithms can be mathematically equivalent in exact arithmetic but behave very differently in the presence of floating-point errors. This hands-on coding exercise contrasts the classical and modified Gram-Schmidt methods for matrix orthonormalization, allowing you to directly measure and compare their numerical stability through forward error and loss of orthogonality .",
            "id": "3232097",
            "problem": "You are given the task of comparing forward error and backward error for two widely used orthonormalization procedures, classical Gram–Schmidt and modified Gram–Schmidt, implemented in standard floating-point arithmetic. Use the definitions of forward error and backward error grounded in the model of floating-point arithmetic: for any basic operation applied to real numbers, the computed result can be represented as $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)(1+\\delta)$ where $\\mathrm{op} \\in \\{+,-,\\times,\\div\\}$ and $|\\delta| \\le u$, with $u$ being the unit roundoff of the arithmetic in use. In exact arithmetic, the Gram–Schmidt method produces a factorization $A = Q R$ with $Q^\\top Q = I$ and $R$ upper triangular with nonnegative diagonal. In floating-point arithmetic, the computed factors $\\widehat{Q}$ and $\\widehat{R}$ satisfy $A \\approx \\widehat{Q}\\widehat{R}$ and $\\widehat{Q}^\\top \\widehat{Q} \\approx I$.\n\nYour program must:\n- Implement two functions that, given a full-rank or rank-deficient real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, return $(Q,R)$ using:\n  1. Classical Gram–Schmidt (single pass, no reorthogonalization).\n  2. Modified Gram–Schmidt (single pass, no reorthogonalization).\n- For each $(Q,R)$ pair and each input matrix $A$, compute:\n  1. The forward error of the factorization as the relative spectral-norm residual\n     $$\\eta_{\\mathrm{fwd}} = \\frac{\\lVert A - Q R \\rVert_2}{\\lVert A \\rVert_2}.$$\n  2. The backward error expressed as loss of orthogonality via\n     $$F = Q^\\top Q - I,\\quad \\eta_{\\mathrm{orth}} = \\lVert F \\rVert_2.$$\n- Use the spectral matrix norm $\\lVert \\cdot \\rVert_2$ for all matrix norms.\n- Round each reported scalar to $12$ decimal places.\n\nTest suite:\n- Case $1$ (well-conditioned tall matrix):\n  $$A_1 = \\begin{bmatrix}\n  1 & 2 & 3 & 4 \\\\\n  2 & 1 & 0 & 1 \\\\\n  0 & 1 & 2 & 3 \\\\\n  1 & 0 & 1 & 0 \\\\\n  2 & 2 & 2 & 2 \\\\\n  3 & 1 & 4 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{6 \\times 4}.$$\n- Case $2$ (nearly dependent columns): Let $\\varepsilon = 10^{-8}$ and\n  $$c_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix},\\quad\n  w = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{bmatrix},\\quad\n  c_2 = c_1 + \\varepsilon w,\\quad\n  c_3 = \\begin{bmatrix} 2 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 2 \\end{bmatrix},\\quad\n  A_2 = [\\, c_1\\ \\ c_2\\ \\ c_3 \\,] \\in \\mathbb{R}^{5 \\times 3}.$$\n- Case $3$ (Hilbert-type tall matrix, ill-conditioned columns): For $m = 8$, $n = 5$, define\n  $$\\left(A_3\\right)_{i,j} = \\frac{1}{i + j + 1},\\quad 0 \\le i \\le 7,\\ 0 \\le j \\le 4,$$\n  so that $A_3 \\in \\mathbb{R}^{8 \\times 5}$.\n- Case $4$ (already orthonormal columns): Let $m = 6$, $n = 3$, and\n  $$A_4 = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{6 \\times 3}.$$\n\nFor each test case $A_k$, compute four scalars:\n- $\\eta_{\\mathrm{fwd}}^{\\mathrm{CGS}}$,\n- $\\eta_{\\mathrm{fwd}}^{\\mathrm{MGS}}$,\n- $\\eta_{\\mathrm{orth}}^{\\mathrm{CGS}}$,\n- $\\eta_{\\mathrm{orth}}^{\\mathrm{MGS}}$,\neach rounded to $12$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all four cases as a comma-separated list of lists, each inner list ordered as\n  $$[\\, \\eta_{\\mathrm{fwd}}^{\\mathrm{CGS}},\\ \\eta_{\\mathrm{fwd}}^{\\mathrm{MGS}},\\ \\eta_{\\mathrm{orth}}^{\\mathrm{CGS}},\\ \\eta_{\\mathrm{orth}}^{\\mathrm{MGS}} \\,],$$\n  enclosed in square brackets. For example: \n  $$[\\ [a_1,b_1,c_1,d_1],\\ [a_2,b_2,c_2,d_2],\\ [a_3,b_3,c_3,d_3],\\ [a_4,b_4,c_4,d_4]\\ ].$$\n\nAll computations must be performed in standard double-precision floating-point arithmetic. No user input is allowed; the program must be self-contained and must print exactly one line in the specified format. All reported values are unitless real numbers rounded to $12$ decimal places.",
            "solution": "The problem statement is a valid, well-posed, and objective exercise in numerical linear algebra. It requests a comparison of two standard algorithms, Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS), for computing the QR factorization of a matrix. The problem is scientifically grounded, providing precise mathematical definitions for the algorithms and the error metrics to be computed. All necessary data and test conditions are provided, and there are no ambiguities or contradictions. The task is to implement the specified algorithms and metrics and apply them to a suite of test cases designed to highlight the numerical properties of CGS and MGS.\n\nThe core of the problem is to compute the QR factorization of a given matrix $A \\in \\mathbb{R}^{m \\times n}$ (where $m \\ge n$), which is a decomposition $A = QR$, where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns ($Q^\\top Q = I_n$) and $R \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix. In floating-point arithmetic, the computed factors, denoted $\\widehat{Q}$ and $\\widehat{R}$, will only approximate these properties.\n\nThe Gram-Schmidt process is a fundamental method for constructing such a factorization. It generates a sequence of orthonormal vectors $q_0, q_1, \\dots, q_{n-1}$ that form an orthonormal basis for the column space of $A$. The two variants to be implemented are:\n\n$1$. **Classical Gram-Schmidt (CGS)**: This algorithm computes each vector $q_j$ by explicitly subtracting its projections onto all previously computed orthonormal vectors $q_0, \\dots, q_{j-1}$ from the corresponding column $a_j$ of $A$. The a_j vector is used for all projection computations for column $j$. The steps for column $j$ are:\n$$v_j = a_j - \\sum_{i=0}^{j-1} (q_i^\\top a_j) q_i$$\n$$r_{jj} = \\lVert v_j \\rVert_2, \\quad q_j = v_j / r_{jj}$$\nThe coefficients $r_{ij} = q_i^\\top a_j$ for $i < j$ are also computed.\nWhile mathematically sound, CGS is known to be numerically unstable. Small rounding errors in the computation of $q_i^\\top a_j$ can lead to a significant loss of orthogonality in the computed columns of $\\widehat{Q}$.\n\n$2$. **Modified Gram-Schmidt (MGS)**: This is an algebraically equivalent but numerically more stable rearrangement of CGS. Instead of projecting $a_j$ onto each $q_i$, MGS updates the remaining columns of $A$ at each step. After computing $q_i$, its component is removed from all subsequent columns $a_{i+1}, \\dots, a_{n-1}$. This can be expressed as:\nLet $v^{(0)}_j = a_j$ for all $j$. For $i = 0, \\dots, n-1$:\n$$r_{ii} = \\lVert v^{(i)}_i \\rVert_2, \\quad q_i = v^{(i)}_i / r_{ii}$$\nFor $j = i+1, \\dots, n-1$:\n$$r_{ij} = q_i^\\top v^{(i)}_j, \\quad v^{(i+1)}_j = v^{(i)}_j - r_{ij} q_i$$\nThis process ensures that each new vector is orthogonalized against already-orthogonalized vectors, reducing the accumulation of errors and resulting in a matrix $\\widehat{Q}$ with columns that are much closer to being perfectly orthogonal.\n\nIn cases of rank deficiency, a vector $v_j$ (in CGS) or $v_i^{(i)}$ (in MGS) may become zero or numerically close to zero. The implementation will handle this by checking if the norm is below a small tolerance ($10^{-12}$). If so, the corresponding column $q_j$ is set to a zero vector, and its diagonal entry $r_{jj}$ in $R$ is set to $0$.\n\nThe stability and accuracy of these algorithms are evaluated using two metrics:\n\n$1$. **Forward Error**: $\\eta_{\\mathrm{fwd}} = \\frac{\\lVert A - \\widehat{Q} \\widehat{R} \\rVert_2}{\\lVert A \\rVert_2}$. This measures the relative residual of the factorization. A smaller value indicates that the computed factors $\\widehat{Q}$ and $\\widehat{R}$ more accurately reconstruct the original matrix $A$. This can be interpreted as a measure of backward stability of the factorization problem itself; the computed factors are the exact factors for a nearby matrix $A+E$, and this error measures the size of $E$.\n\n$2$. **Loss of Orthogonality**: $\\eta_{\\mathrm{orth}} = \\lVert \\widehat{Q}^\\top \\widehat{Q} - I \\rVert_2$. This is a direct measure of the backward error of the algorithm with respect to the property of producing an orthonormal $Q$. A value close to $0$ indicates that the columns of $\\widehat{Q}$ are nearly orthonormal, while a value close to $1$ indicates a significant loss of orthogonality. This metric is expected to clearly distinguish the stability of MGS from the instability of CGS, especially for ill-conditioned matrices.\n\nThe program will implement both CGS and MGS, apply them to the four provided test matrices, and compute both error metrics for each factorization. The spectral norm $\\lVert \\cdot \\rVert_2$ is used for all matrix norm computations. The final results for each of the four test cases, consisting of the two errors for both algorithms, are rounded to $12$ decimal places and presented in the specified list-of-lists format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classical_gram_schmidt(A):\n    \"\"\"\n    Computes the QR factorization of a matrix A using Classical Gram-Schmidt.\n    \"\"\"\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    \n    for j in range(n):\n        v = A[:, j].copy()\n        for i in range(j):\n            R[i, j] = Q[:, i].T @ A[:, j] # Project a_j onto q_i\n            v -= R[i, j] * Q[:, i]\n            \n        norm_v = np.linalg.norm(v)\n        \n        if norm_v > 1e-12:\n            R[j, j] = norm_v\n            Q[:, j] = v / R[j, j]\n        else:\n            R[j, j] = 0.0\n            # Q[:, j] remains a zero vector\n            \n    return Q, R\n\ndef modified_gram_schmidt(A):\n    \"\"\"\n    Computes the QR factorization of a matrix A using Modified Gram-Schmidt.\n    \"\"\"\n    m, n = A.shape\n    V = A.copy()\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    \n    for i in range(n):\n        norm_v_i = np.linalg.norm(V[:, i])\n        R[i, i] = norm_v_i\n        \n        if R[i, i] > 1e-12:\n            Q[:, i] = V[:, i] / R[i, i]\n            for j in range(i + 1, n):\n                R[i, j] = Q[:, i].T @ V[:, j]\n                V[:, j] -= R[i, j] * Q[:, i]\n        else:\n            # R[i, i] is already set to the small norm (or 0)\n            # Q[:, i] remains a zero vector.\n            # Projections of V[:, j] onto a zero Q[:, i] are zero, so R[i, j] for j>i are zero\n            # and subsequent V columns are not modified.\n            pass\n            \n    return Q, R\n\ndef compute_errors(A, Q, R):\n    \"\"\"\n    Computes the forward error and orthogonality loss for a given QR factorization.\n    \"\"\"\n    m, n = A.shape\n    \n    # Forward error: ||A - QR||_2 / ||A||_2\n    norm_A = np.linalg.norm(A, 2)\n    if norm_A == 0:\n        fwd_error = 0.0\n    else:\n        fwd_error = np.linalg.norm(A - Q @ R, 2) / norm_A\n\n    # Orthogonality loss: ||Q^T Q - I||_2\n    if n > 0:\n        orth_error = np.linalg.norm(Q.T @ Q - np.eye(n), 2)\n    else:\n        orth_error = 0.0\n        \n    return fwd_error, orth_error\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on the defined test cases.\n    \"\"\"\n    # Case 1: Well-conditioned tall matrix\n    A1 = np.array([\n        [1, 2, 3, 4],\n        [2, 1, 0, 1],\n        [0, 1, 2, 3],\n        [1, 0, 1, 0],\n        [2, 2, 2, 2],\n        [3, 1, 4, 1]\n    ], dtype=float)\n\n    # Case 2: Nearly dependent columns\n    epsilon = 1e-8\n    c1 = np.array([1, 2, 3, 4, 5], dtype=float).reshape(-1, 1)\n    w = np.array([1, -1, 1, -1, 1], dtype=float).reshape(-1, 1)\n    c2 = c1 + epsilon * w\n    c3 = np.array([2, 0, 1, 0, 2], dtype=float).reshape(-1, 1)\n    A2 = np.hstack([c1, c2, c3])\n\n    # Case 3: Hilbert-type tall matrix\n    m3, n3 = 8, 5\n    A3 = np.fromfunction(lambda i, j: 1 / (i + j + 1), (m3, n3), dtype=float)\n\n    # Case 4: Already orthonormal columns\n    A4 = np.array([\n        [1, 0, 0],\n        [0, 0, 0],\n        [0, 1, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 1]\n    ], dtype=float)\n\n    test_cases = [A1, A2, A3, A4]\n    all_results = []\n\n    for A in test_cases:\n        # Classical Gram-Schmidt\n        Q_cgs, R_cgs = classical_gram_schmidt(A)\n        fwd_cgs, orth_cgs = compute_errors(A, Q_cgs, R_cgs)\n\n        # Modified Gram-Schmidt\n        Q_mgs, R_mgs = modified_gram_schmidt(A)\n        fwd_mgs, orth_mgs = compute_errors(A, Q_mgs, R_mgs)\n\n        case_results = [\n            round(fwd_cgs, 12),\n            round(fwd_mgs, 12),\n            round(orth_cgs, 12),\n            round(orth_mgs, 12)\n        ]\n        all_results.append(case_results)\n\n    # Format the output as a string representation of a list of lists, with spaces\n    # Example: [[a1, b1, c1, d1], [a2, ...]]\n    # str() on a list provides the desired spacing.\n    result_strings = [str(res) for res in all_results]\n    print(f\"[{', '.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}