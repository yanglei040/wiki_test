{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of penalty methods, we begin with a foundational exercise. This problem uses a simple quadratic objective function with a single linear constraint, a scenario that is analytically solvable. By deriving the exact solutions for both the constrained problem and the penalized version, you will see precisely how the method works and prove that the approximate solution converges to the true one as the penalty parameter $\\mu$ grows . This practice is crucial for grasping the core mechanics and analyzing the convergence rate, which measures how quickly the error decreases.",
            "id": "3261466",
            "problem": "Consider the unconstrained objective defined by $f(x) = \\sum_{i=1}^{n} x_i^2$, where $x \\in \\mathbb{R}^n$, and a single equality constraint given by $g(x) = c^\\top x - 1 = 0$, with a fixed nonzero vector $c \\in \\mathbb{R}^n$. The penalty method constructs a penalized objective $F_\\mu(x) = f(x) + \\mu \\, g(x)^2$ for a penalty parameter $\\mu > 0$. Starting from core definitions of unconstrained and constrained minimization, and the properties of convex quadratic functions, your tasks are:\n- Task A: Using first principles, derive the constrained minimizer $x^\\star$ of $f(x)$ subject to $g(x) = 0$.\n- Task B: Using first principles, derive the unconstrained minimizer $x_\\mu$ of the penalized objective $F_\\mu(x)$ for a given $\\mu$, and demonstrate that $x_\\mu \\to x^\\star$ as $\\mu \\to +\\infty$.\n- Task C: Estimate the rates at which the error in the decision variable, measured by $\\|x_\\mu - x^\\star\\|_2$, and the constraint violation, measured by $|g(x_\\mu)|$, decay with increasing $\\mu$. Express the rates in Big-O notation as functions of $\\mu$ and the norm of $c$.\n\nYour program must implement the derived formulas and compute, for each test case, the following quantities:\n- The error norm in the decision variable: $\\|x_\\mu - x^\\star\\|_2$.\n- The absolute constraint violation: $|g(x_\\mu)|$.\n- The scaled error norm: $\\mu \\, \\|x_\\mu - x^\\star\\|_2$.\n- The scaled constraint violation: $\\mu \\, |g(x_\\mu)|$.\n\nAll numerical results must be reported as dimensionless quantities (no physical units). Results should be rounded to $8$ decimal places.\n\nUse the following test suite of parameter values, where each case is a pair $(c, \\mu)$ with $c \\in \\mathbb{R}^n$ and $\\mu \\in \\mathbb{R}$:\n- Case $1$ (happy path): $n = 3$, $c = [1, 2, -1]$, $\\mu = 10$.\n- Case $2$ (boundary in penalty strength): $n = 3$, $c = [1, 2, -1]$, $\\mu = 0.1$.\n- Case $3$ (asymptotic regime): $n = 3$, $c = [1, 2, -1]$, $\\mu = 10^6$.\n- Case $4$ (small-norm constraint vector): $n = 3$, $c = [10^{-3}, 0, 0]$, $\\mu = 10$.\n- Case $5$ (large-norm constraint vector): $n = 3$, $c = [3, 4, 0]$, $\\mu = 10$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test caseâ€™s result must be a list of four floats in the fixed order $[\\|x_\\mu - x^\\star\\|_2, |g(x_\\mu)|, \\mu \\, \\|x_\\mu - x^\\star\\|_2, \\mu \\, |g(x_\\mu)|]$, rounded to $8$ decimal places. The final output line therefore has the form\n$[ [r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}],[r_{2,1}, r_{2,2}, r_{2,3}, r_{2,4}],\\dots,[r_{5,1}, r_{5,2}, r_{5,3}, r_{5,4}] ]$,\nwith no spaces.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. All necessary information is provided, and the tasks are clearly defined within the established mathematical framework of constrained optimization and penalty methods.\n\nThe problem asks for an analysis of the quadratic penalty method for a simple convex optimization problem. We are given an objective function $f(x) = \\sum_{i=1}^{n} x_i^2 = x^\\top x$ and a single linear equality constraint $g(x) = c^\\top x - 1 = 0$, where $x \\in \\mathbb{R}^n$ and $c \\in \\mathbb{R}^n$ is a fixed nonzero vector. The quadratic penalty method approximates the solution to this constrained problem by solving a sequence of unconstrained problems with the penalized objective function $F_\\mu(x) = f(x) + \\mu \\, g(x)^2 = x^\\top x + \\mu(c^\\top x - 1)^2$ for a penalty parameter $\\mu > 0$.\n\nOur analysis proceeds in three parts as requested.\n\nTask A: Derivation of the constrained minimizer $x^\\star$.\nTo find the minimizer of $f(x)$ subject to the constraint $g(x)=0$, we use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}(x, \\lambda)$ is defined as:\n$$ \\mathcal{L}(x, \\lambda) = f(x) - \\lambda g(x) = x^\\top x - \\lambda (c^\\top x - 1) $$\nThe first-order necessary conditions for optimality require the gradient of the Lagrangian with respect to $x$ to be the zero vector, and the constraint to be satisfied.\n$$ \\nabla_x \\mathcal{L}(x, \\lambda) = 2x - \\lambda c = 0 $$\n$$ g(x) = c^\\top x - 1 = 0 $$\nFrom the first condition, we express the optimal solution $x^\\star$ in terms of the Lagrange multiplier $\\lambda$:\n$$ x^\\star = \\frac{\\lambda}{2} c $$\nSubstituting this into the constraint equation allows us to solve for $\\lambda$:\n$$ c^\\top \\left(\\frac{\\lambda}{2} c\\right) - 1 = 0 $$\n$$ \\frac{\\lambda}{2} (c^\\top c) = 1 $$\n$$ \\frac{\\lambda}{2} \\|c\\|_2^2 = 1 $$\nSince $c$ is a nonzero vector, $\\|c\\|_2^2 > 0$, and we can solve for $\\lambda$:\n$$ \\lambda = \\frac{2}{\\|c\\|_2^2} $$\nSubstituting the value of $\\lambda$ back into the expression for $x^\\star$, we obtain the constrained minimizer:\n$$ x^\\star = \\frac{1}{2} \\left( \\frac{2}{\\|c\\|_2^2} \\right) c = \\frac{c}{\\|c\\|_2^2} = \\frac{c}{c^\\top c} $$\nSince the objective function $f(x)$ is strictly convex and the constraint defines a convex set (a hyperplane), this stationary point is the unique global minimizer.\n\nTask B: Derivation of the unconstrained minimizer $x_\\mu$ and convergence analysis.\nThe penalized objective function is $F_\\mu(x) = x^\\top x + \\mu(c^\\top x - 1)^2$. This is an unconstrained, strictly convex function, so its unique minimizer $x_\\mu$ can be found by setting its gradient to zero.\n$$ \\nabla_x F_\\mu(x) = \\nabla_x (x^\\top x) + \\mu \\nabla_x ((c^\\top x - 1)^2) $$\nUsing the chain rule for the penalty term, where $\\nabla_x(u^2) = 2u \\nabla_x u$, we have:\n$$ \\nabla_x F_\\mu(x) = 2x + \\mu \\cdot 2(c^\\top x - 1) \\cdot c = 0 $$\nDividing by $2$ and rearranging terms gives:\n$$ x_\\mu + \\mu(c^\\top x_\\mu - 1)c = 0 $$\n$$ x_\\mu + \\mu(c^\\top x_\\mu)c - \\mu c = 0 $$\n$$ x_\\mu + \\mu c(c^\\top x_\\mu) = \\mu c $$\nThis can be written as a linear system: $(I + \\mu c c^\\top) x_\\mu = \\mu c$. To solve for $x_\\mu$, we can use the Sherman-Morrison formula for the inverse of $(I + \\mu c c^\\top)$, which yields:\n$$(I + \\mu c c^\\top)^{-1} = I - \\frac{\\mu c c^\\top}{1 + \\mu c^\\top c}$$\nApplying this inverse to find $x_\\mu$:\n$$ x_\\mu = \\left(I - \\frac{\\mu c c^\\top}{1 + \\mu \\|c\\|_2^2}\\right) (\\mu c) = \\mu c - \\frac{\\mu^2 c (c^\\top c)}{1 + \\mu \\|c\\|_2^2} = \\mu c - \\frac{\\mu^2 \\|c\\|_2^2 c}{1 + \\mu \\|c\\|_2^2} $$\nSimplifying the scalar coefficient of $c$:\n$$ \\mu - \\frac{\\mu^2 \\|c\\|_2^2}{1 + \\mu \\|c\\|_2^2} = \\frac{\\mu(1 + \\mu \\|c\\|_2^2) - \\mu^2 \\|c\\|_2^2}{1 + \\mu \\|c\\|_2^2} = \\frac{\\mu}{1 + \\mu \\|c\\|_2^2} $$\nThus, the minimizer of the penalized function is:\n$$ x_\\mu = \\frac{\\mu}{1 + \\mu \\|c\\|_2^2} c $$\nTo demonstrate that $x_\\mu \\to x^\\star$ as $\\mu \\to +\\infty$, we take the limit of the expression for $x_\\mu$:\n$$ \\lim_{\\mu \\to +\\infty} x_\\mu = \\lim_{\\mu \\to +\\infty} \\left( \\frac{\\mu}{1 + \\mu \\|c\\|_2^2} \\right) c $$\nWe can divide the numerator and denominator of the scalar fraction by $\\mu$:\n$$ \\lim_{\\mu \\to +\\infty} \\left( \\frac{1}{1/\\mu + \\|c\\|_2^2} \\right) c = \\left( \\frac{1}{0 + \\|c\\|_2^2} \\right) c = \\frac{c}{\\|c\\|_2^2} $$\nThis limit is exactly the expression for $x^\\star$, which confirms the convergence $x_\\mu \\to x^\\star$.\n\nTask C: Estimation of decay rates.\nWe now analyze the rates at which the error and constraint violation approach zero.\n\nFirst, we analyze the error in the decision variable, $\\|x_\\mu - x^\\star\\|_2$.\n$$ x_\\mu - x^\\star = \\frac{\\mu}{1 + \\mu \\|c\\|_2^2} c - \\frac{1}{\\|c\\|_2^2} c = \\left( \\frac{\\mu \\|c\\|_2^2 - (1 + \\mu \\|c\\|_2^2)}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} \\right) c = \\frac{-1}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} c $$\nTaking the Euclidean norm:\n$$ \\|x_\\mu - x^\\star\\|_2 = \\left\\| \\frac{-c}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} \\right\\|_2 = \\frac{\\|c\\|_2}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} = \\frac{1}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2} $$\nFor large $\\mu$, the term $1$ is negligible compared to $\\mu \\|c\\|_2^2$. Therefore, the asymptotic behavior is:\n$$ \\|x_\\mu - x^\\star\\|_2 \\approx \\frac{1}{(\\mu \\|c\\|_2^2)\\|c\\|_2} = \\frac{1}{\\mu \\|c\\|_2^3} $$\nThis shows that the error norm decays as $O(\\mu^{-1})$. Specifically, $\\|x_\\mu - x^\\star\\|_2 = O(\\mu^{-1}\\|c\\|_2^{-3})$.\n\nNext, we analyze the constraint violation, $|g(x_\\mu)|$.\n$$ g(x_\\mu) = c^\\top x_\\mu - 1 = c^\\top \\left(\\frac{\\mu}{1 + \\mu \\|c\\|_2^2} c\\right) - 1 = \\frac{\\mu (c^\\top c)}{1 + \\mu \\|c\\|_2^2} - 1 $$\n$$ g(x_\\mu) = \\frac{\\mu \\|c\\|_2^2 - (1 + \\mu \\|c\\|_2^2)}{1 + \\mu \\|c\\|_2^2} = \\frac{-1}{1 + \\mu \\|c\\|_2^2} $$\nTaking the absolute value gives the constraint violation:\n$$ |g(x_\\mu)| = \\frac{1}{1 + \\mu \\|c\\|_2^2} $$\nFor large $\\mu$, the asymptotic behavior is:\n$$ |g(x_\\mu)| \\approx \\frac{1}{\\mu \\|c\\|_2^2} $$\nThis shows that the constraint violation also decays as $O(\\mu^{-1})$. Specifically, $|g(x_\\mu)| = O(\\mu^{-1}\\|c\\|_2^{-2})$.\n\nSummary of formulas for implementation:\nFor a given $c \\in \\mathbb{R}^n$ and $\\mu > 0$:\n$1$. Error norm: $\\|x_\\mu - x^\\star\\|_2 = \\frac{1}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2}$\n$2$. Constraint violation: $|g(x_\\mu)| = \\frac{1}{1 + \\mu \\|c\\|_2^2}$\n$3$. Scaled error norm: $\\mu \\|x_\\mu - x^\\star\\|_2 = \\frac{\\mu}{(1 + \\mu \\|c\\|_2^2)\\|c\\|_2}$\n$4$. Scaled constraint violation: $\\mu |g(x_\\mu)| = \\frac{\\mu}{1 + \\mu \\|c\\|_2^2}$\nThese formulas will be used to compute the required quantities for the test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the derived formulas for the penalty method analysis\n    and computes results for the specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (c, mu), where c is a list of floats\n    # and mu is a float.\n    test_cases = [\n        # Case 1 (happy path)\n        ([1.0, 2.0, -1.0], 10.0),\n        # Case 2 (boundary in penalty strength)\n        ([1.0, 2.0, -1.0], 0.1),\n        # Case 3 (asymptotic regime)\n        ([1.0, 2.0, -1.0], 1e6),\n        # Case 4 (small-norm constraint vector)\n        ([1e-3, 0.0, 0.0], 10.0),\n        # Case 5 (large-norm constraint vector)\n        ([3.0, 4.0, 0.0], 10.0),\n    ]\n\n    all_results = []\n    for c_list, mu in test_cases:\n        c = np.array(c_list)\n\n        # Calculate squared L2 norm of c\n        c_norm_sq = np.dot(c, c)\n        \n        # Calculate L2 norm of c\n        c_norm = np.sqrt(c_norm_sq)\n\n        # The term (1 + mu * ||c||^2) appears in all denominators\n        denominator = 1.0 + mu * c_norm_sq\n\n        # Task C derived the following exact formulas:\n        # ||x_mu - x*||_2 = 1 / ((1 + mu * ||c||^2) * ||c||)\n        # |g(x_mu)| = 1 / (1 + mu * ||c||^2)\n\n        # Calculate error norm in the decision variable\n        error_norm = 1.0 / (denominator * c_norm)\n\n        # Calculate absolute constraint violation\n        constraint_violation = 1.0 / denominator\n\n        # Calculate scaled error norm\n        scaled_error_norm = mu * error_norm\n\n        # Calculate scaled constraint violation\n        scaled_constraint_violation = mu * constraint_violation\n\n        # Store the four computed values for the current test case\n        case_result = [\n            error_norm,\n            constraint_violation,\n            scaled_error_norm,\n            scaled_constraint_violation,\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string as specified in the problem statement.\n    # The output is a list of lists, with no spaces and 8 decimal places.\n    case_strings = []\n    for res in all_results:\n        # Format each number to 8 decimal places and join into a string like \"[n1,n2,n3,n4]\"\n        num_strings = [f\"{val:.8f}\" for val in res]\n        case_strings.append(f\"[{','.join(num_strings)}]\")\n    \n    # Join the case strings into a final string like \"[[...],[...],...]\"\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from pure theory to a practical application, we now explore how penalty methods are used in data fitting and regression. In this exercise, you will fit a polynomial to a set of data points while trying to enforce specific values at the endpoints, a common requirement in many scientific and engineering models . This hands-on task illuminates the concept of a \"soft\" constraint and reveals an important trade-off: as you increase the penalty parameter to better enforce the constraints, you may introduce undesirable oscillations into your polynomial fit, especially for higher degrees.",
            "id": "3261566",
            "problem": "Consider the following constrained polynomial fitting problem on the unit interval. Let $p(x)$ be a polynomial of degree $n$ with coefficients $a_0, a_1, \\ldots, a_n$ so that $p(x) = \\sum_{k=0}^{n} a_k x^k$. You are given data pairs $(x_i, y_i)$ for $i=0,1,\\ldots,N$ where $x_i \\in [0,1]$ and $y_i \\in \\mathbb{R}$. The goal is to approximate the data in the least-squares sense while enforcing endpoint conditions $p(0) = 1$ and $p(1) = 0$ through a penalty term with parameter $\\mu \\ge 0$. Specifically, define the objective\n$$\nJ(a_0,\\ldots,a_n) = \\sum_{i=0}^{N} \\left(p(x_i) - y_i\\right)^2 + \\mu\\left(\\left(p(0)-1\\right)^2 + \\left(p(1)-0\\right)^2\\right).\n$$\nYour tasks are:\n- Starting from the fundamental definition of least-squares minimization and the gradient condition for optimality, derive the linear system that determines the optimal coefficients $a_0,\\ldots,a_n$ for a given degree $n$ and penalty parameter $\\mu$. Use only the definition of $p(x)$, the Euclidean norm squared, and basic matrix calculus identities.\n- Implement a program that:\n  1. Constructs the data set with $N=10$ and $x_i = i/10$ for $i=0,1,\\ldots,10$, and $y_i = x_i(1-x_i) + 0.1\\,(x_i - 0.5)$.\n  2. For each test case, solves for the polynomial coefficients $a_0,\\ldots,a_n$ that minimize $J$.\n  3. Evaluates the fitted polynomial on a uniform grid of $M=101$ points $x_j = j/100$, $j=0,1,\\ldots,100$, and computes an oscillation metric defined by the mean squared second finite difference\n     $$\n     \\mathcal{O} = \\frac{1}{M-2} \\sum_{j=1}^{M-2}\\left( p(x_{j+1}) - 2\\,p(x_j) + p(x_{j-1}) \\right)^2.\n     $$\n  4. Reports the absolute constraint violations at the endpoints, namely $|p(0) - 1|$ and $|p(1) - 0|$, together with the oscillation metric $\\mathcal{O}$.\nThe derivation must begin from the principle that the minimizer of a differentiable objective occurs where its gradient is zero, and proceed to a linear system involving a data matrix and a constraint matrix. You must not assume or quote pre-derived formulas beyond these fundamental principles.\n\nThe test suite consists of the following combinations of polynomial degree $n$ and penalty parameter $\\mu$:\n- $n=3$ with $\\mu=0$,\n- $n=3$ with $\\mu=1$,\n- $n=3$ with $\\mu=100$,\n- $n=3$ with $\\mu=10^4$,\n- $n=7$ with $\\mu=0$,\n- $n=7$ with $\\mu=1$,\n- $n=7$ with $\\mu=100$,\n- $n=7$ with $\\mu=10^4$.\n\nFor each test case, your program must output a list with five entries $[n, \\mu, |p(0)-1|, |p(1)-0|, \\mathcal{O}]$, where $n$ is an integer and the remaining quantities are real numbers. Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each element is the per-test-case list described above (for example, $[[\\cdots],[\\cdots],\\ldots]$). No physical units are involved. No angles are involved. Express all numeric outputs in standard decimal form.",
            "solution": "The user wants me to solve a constrained polynomial fitting problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Polynomial Definition**: $p(x) = \\sum_{k=0}^{n} a_k x^k$, where $a_k$ are coefficients and $n$ is the degree.\n*   **Data**: Data pairs $(x_i, y_i)$ for $i=0,1,\\ldots,N$, where $x_i \\in [0,1]$.\n    *   Specifically, $N=10$, so there are $11$ data points.\n    *   $x_i = i/10$ for $i=0,1,\\ldots,10$.\n    *   $y_i = x_i(1-x_i) + 0.1\\,(x_i - 0.5)$.\n*   **Constraints**: Endpoint conditions $p(0) = 1$ and $p(1) = 0$.\n*   **Objective Function**: The function to be minimized is\n    $$\n    J(a_0,\\ldots,a_n) = \\sum_{i=0}^{N} \\left(p(x_i) - y_i\\right)^2 + \\mu\\left(\\left(p(0)-1\\right)^2 + \\left(p(1)-0\\right)^2\\right)\n    $$\n    with penalty parameter $\\mu \\ge 0$.\n*   **Evaluation Grid**: A uniform grid of $M=101$ points $x_j = j/100$, for $j=0,1,\\ldots,100$.\n*   **Oscillation Metric**:\n    $$\n    \\mathcal{O} = \\frac{1}{M-2} \\sum_{j=1}^{M-2}\\left( p(x_{j+1}) - 2\\,p(x_j) + p(x_{j-1}) \\right)^2.\n    $$\n*   **Outputs**: For each test case, report the absolute constraint violations $|p(0) - 1|$ and $|p(1) - 0|$, and the oscillation metric $\\mathcal{O}$.\n*   **Test Cases**:\n    *   $n=3$ with $\\mu \\in \\{0, 1, 100, 10^4\\}$\n    *   $n=7$ with $\\mu \\in \\{0, 1, 100, 10^4\\}$\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is a well-defined task in numerical linear algebra and optimization, specifically concerning polynomial regression with penalty-based constraints.\n*   **Scientifically Grounded**: The problem is based on the principle of least squares, a cornerstone of statistics and numerical analysis, and the penalty method, a standard technique in constrained optimization. All mathematical concepts are standard and sound.\n*   **Well-Posed**: For a given degree $n$ and penalty parameter $\\mu > 0$, the objective function $J$ is a strictly convex quadratic function of the coefficients $a_k$. This guarantees the existence of a unique minimizer. For $\\mu=0$, the problem reduces to standard polynomial least squares, which is well-posed as long as the matrix $\\mathbf{X}^T\\mathbf{X}$ is invertible, a condition met by the given distinct data points $x_i$. The problem is well-posed.\n*   **Objective**: The problem is stated using precise mathematical definitions and objective numerical tasks. There are no subjective or ambiguous elements.\n*   **Completeness**: All necessary data ($N$, $x_i$, $y_i$), definitions (objective function, metrics), and parameters ($n$, $\\mu$) are provided. The problem is self-contained.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. The derivation and implementation will proceed.\n\n### Derivation of the Linear System\n\nThe goal is to find the vector of coefficients $\\mathbf{a} = [a_0, a_1, \\ldots, a_n]^T$ that minimizes the objective function $J(\\mathbf{a})$. The fundamental principle for minimizing a differentiable function is that its gradient must be zero at the minimum. We begin by expressing the objective function in matrix-vector notation.\n\nLet $\\mathbf{v}(x)$ be a row vector of monomials, $\\mathbf{v}(x) = [1, x, x^2, \\ldots, x^n]$. The polynomial can then be written as $p(x) = \\mathbf{v}(x)\\mathbf{a}$.\n\nThe data fitting term, $\\sum_{i=0}^{N} (p(x_i) - y_i)^2$, can be expressed using a data matrix $\\mathbf{X}$ and a data vector $\\mathbf{y}$. Let $\\mathbf{X}$ be an $(N+1) \\times (n+1)$ matrix where the $i$-th row is $\\mathbf{v}(x_i)$, i.e., $X_{ik} = x_i^k$ for $i=0, \\ldots, N$ and $k=0, \\ldots, n$. Let $\\mathbf{y} = [y_0, y_1, \\ldots, y_N]^T$. The vector of polynomial evaluations at the data points is $\\mathbf{X}\\mathbf{a}$. The data fitting term is the squared Euclidean norm of the residual vector, $||\\mathbf{X}\\mathbf{a} - \\mathbf{y}||_2^2$.\n\nThe penalty term involves the constraints $p(0)=1$ and $p(1)=0$. We evaluate the polynomial at the endpoints:\n$p(0) = \\mathbf{v}(0)\\mathbf{a} = [1, 0, \\ldots, 0]\\mathbf{a}$.\n$p(1) = \\mathbf{v}(1)\\mathbf{a} = [1, 1, \\ldots, 1]\\mathbf{a}$.\nLet us define a constraint matrix $\\mathbf{C}$ of size $2 \\times (n+1)$ and a constraint value vector $\\mathbf{d}$ of size $2 \\times 1$:\n$$\n\\mathbf{C} = \\begin{pmatrix} \\mathbf{v}(0) \\\\ \\mathbf{v}(1) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & \\cdots & 0 \\\\ 1 & 1 & 1 & \\cdots & 1 \\end{pmatrix}, \\quad \\mathbf{d} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nThe penalty term $\\mu((p(0)-1)^2 + (p(1)-0)^2)$ can be written as $\\mu ||\\mathbf{C}\\mathbf{a} - \\mathbf{d}||_2^2$.\n\nThe full objective function is now in quadratic form:\n$$\nJ(\\mathbf{a}) = ||\\mathbf{X}\\mathbf{a} - \\mathbf{y}||_2^2 + \\mu ||\\mathbf{C}\\mathbf{a} - \\mathbf{d}||_2^2.\n$$\nThis can be expanded as:\n$$\nJ(\\mathbf{a}) = (\\mathbf{X}\\mathbf{a} - \\mathbf{y})^T(\\mathbf{X}\\mathbf{a} - \\mathbf{y}) + \\mu (\\mathbf{C}\\mathbf{a} - \\mathbf{d})^T(\\mathbf{C}\\mathbf{a} - \\mathbf{d}).\n$$\nTo find the minimum, we compute the gradient of $J(\\mathbf{a})$ with respect to $\\mathbf{a}$ and set it to the zero vector. We use the standard matrix calculus identity $\\nabla_{\\mathbf{z}} ||\\mathbf{A}\\mathbf{z} - \\mathbf{b}||_2^2 = 2\\mathbf{A}^T(\\mathbf{A}\\mathbf{z} - \\mathbf{b})$.\n\nApplying this identity to both terms in $J(\\mathbf{a})$:\n$\\nabla_{\\mathbf{a}} ||\\mathbf{X}\\mathbf{a} - \\mathbf{y}||_2^2 = 2\\mathbf{X}^T(\\mathbf{X}\\mathbf{a} - \\mathbf{y})$.\n$\\nabla_{\\mathbf{a}} (\\mu ||\\mathbf{C}\\mathbf{a} - \\mathbf{d}||_2^2) = 2\\mu\\mathbf{C}^T(\\mathbf{C}\\mathbf{a} - \\mathbf{d})$.\n\nThe gradient of the full objective function is the sum of these two gradients:\n$$\n\\nabla_{\\mathbf{a}} J(\\mathbf{a}) = 2\\mathbf{X}^T(\\mathbf{X}\\mathbf{a} - \\mathbf{y}) + 2\\mu\\mathbf{C}^T(\\mathbf{C}\\mathbf{a} - \\mathbf{d}).\n$$\nSetting the gradient to zero to find the optimal $\\mathbf{a}$:\n$$\n2\\mathbf{X}^T(\\mathbf{X}\\mathbf{a} - \\mathbf{y}) + 2\\mu\\mathbf{C}^T(\\mathbf{C}\\mathbf{a} - \\mathbf{d}) = \\mathbf{0}.\n$$\nDividing by $2$ and distributing the terms:\n$$\n\\mathbf{X}^T\\mathbf{X}\\mathbf{a} - \\mathbf{X}^T\\mathbf{y} + \\mu\\mathbf{C}^T\\mathbf{C}\\mathbf{a} - \\mu\\mathbf{C}^T\\mathbf{d} = \\mathbf{0}.\n$$\nNow, we group the terms involving the unknown vector $\\mathbf{a}$ on the left side and the known terms on the right side:\n$$\n(\\mathbf{X}^T\\mathbf{X})\\mathbf{a} + (\\mu\\mathbf{C}^T\\mathbf{C})\\mathbf{a} = \\mathbf{X}^T\\mathbf{y} + \\mu\\mathbf{C}^T\\mathbf{d}.\n$$\nThis gives the final linear system for the coefficients $\\mathbf{a}$:\n$$\n(\\mathbf{X}^T\\mathbf{X} + \\mu\\mathbf{C}^T\\mathbf{C})\\mathbf{a} = \\mathbf{X}^T\\mathbf{y} + \\mu\\mathbf{C}^T\\mathbf{d}.\n$$\nThis is a linear system of the form $\\mathbf{M}\\mathbf{a} = \\mathbf{b}$, where the system matrix is $\\mathbf{M} = \\mathbf{X}^T\\mathbf{X} + \\mu\\mathbf{C}^T\\mathbf{C}$ and the right-hand side vector is $\\mathbf{b} = \\mathbf{X}^T\\mathbf{y} + \\mu\\mathbf{C}^T\\mathbf{d}$. Both $\\mathbf{X}^T\\mathbf{X}$ and $\\mathbf{C}^T\\mathbf{C}$ are positive semi-definite, so for $\\mu > 0$ their sum is positive definite, guaranteeing a unique solution.\n\nThe implementation will construct these matrices and solve this system for each test case.\nSpecifically, the matrices are:\n*   $(\\mathbf{X}^T\\mathbf{X})_{kj} = \\sum_{i=0}^{N} x_i^{k+j}$ for $k,j=0,\\ldots,n$.\n*   $\\mathbf{C}^T\\mathbf{C} = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}\\begin{pmatrix} 1 & 0 & \\cdots & 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 1 & 1 & \\cdots & 1 \\end{pmatrix}$.\n*   The right-hand side components are $(\\mathbf{X}^T\\mathbf{y})_k = \\sum_{i=0}^{N} y_i x_i^k$ and $\\mathbf{C}^T\\mathbf{d} = [1, 0, \\ldots, 0]^T$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a constrained polynomial fitting problem using the penalty method\n    for various polynomial degrees (n) and penalty parameters (mu).\n    \"\"\"\n    \n    # Define the 8 test cases from the problem statement.\n    test_cases = [\n        (3, 0.0),\n        (3, 1.0),\n        (3, 100.0),\n        (3, 1.0e4),\n        (7, 0.0),\n        (7, 1.0),\n        (7, 100.0),\n        (7, 1.0e4)\n    ]\n\n    # 1. Construct the data set (N=10 -> 11 points)\n    N = 10\n    x_data = np.linspace(0.0, 1.0, N + 1)\n    y_data = x_data * (1.0 - x_data) + 0.1 * (x_data - 0.5)\n\n    all_results = []\n\n    for n, mu in test_cases:\n        # For each test case, solve for the polynomial coefficients\n        \n        # Assemble the data matrix X (Vandermonde matrix)\n        # X_ik = x_i^k, so we need n+1 columns for degree n\n        X = np.vander(x_data, N=n + 1, increasing=True)\n        \n        # Assemble the constraint matrix C and vector d\n        # p(0) = a_0, p(1) = sum(a_k)\n        C = np.zeros((2, n + 1))\n        C[0, 0] = 1.0  # Corresponds to p(0)\n        C[1, :] = 1.0  # Corresponds to p(1)\n        d = np.array([1.0, 0.0]) # Target values for constraints\n\n        # Construct the linear system M*a = b\n        # M = X^T*X + mu * C^T*C\n        # b = X^T*y + mu * C^T*d\n        XTX = X.T @ X\n        CTC = C.T @ C\n        M = XTX + mu * CTC\n\n        XTY = X.T @ y_data\n        CTd = C.T @ d\n        b_vec = XTY + mu * CTd\n        \n        # 2. Solve the linear system for coefficients a\n        try:\n            a = np.linalg.solve(M, b_vec)\n        except np.linalg.LinAlgError:\n            # For n=7, mu=0, the matrix can be ill-conditioned, use pseudoinverse\n            a = np.linalg.pinv(M) @ b_vec\n\n        # 3. Evaluate the fitted polynomial and compute metrics\n        # Evaluation grid\n        M_eval = 101\n        x_eval = np.linspace(0.0, 1.0, M_eval)\n        \n        # Evaluate polynomial p(x) on the grid\n        V_eval = np.vander(x_eval, N=n + 1, increasing=True)\n        p_eval = V_eval @ a\n\n        # Compute oscillation metric O\n        # O = mean squared second finite difference\n        # np.diff computes p(j+1)-p(j), n=2 applies it twice\n        d2p = np.diff(p_eval, n=2)\n        oscillation_metric = np.mean(d2p**2)\n\n        # 4. Report the absolute constraint violations\n        # p(0) = a_0\n        p0_val = a[0]\n        # p(1) = sum of all coefficients\n        p1_val = np.sum(a)\n\n        p0_violation = np.abs(p0_val - 1.0)\n        p1_violation = np.abs(p1_val - 0.0)\n\n        result_list = [\n            n,\n            mu,\n            p0_violation,\n            p1_violation,\n            oscillation_metric\n        ]\n        all_results.append(result_list)\n\n    # Format the final output string as a list of lists.\n    # Numbers should be in standard decimal form.\n    outer_list_str = []\n    for r_list in all_results:\n        # Format integers and floats appropriately to avoid scientific notation.\n        # Using .14f for high precision on floating point results.\n        inner_list_str = (\n            f\"[{int(r_list[0])},{int(r_list[1])},{r_list[2]:.14f},\"\n            f\"{r_list[3]:.14f},{r_list[4]:.14f}]\"\n        )\n        outer_list_str.append(inner_list_str)\n\n    # The final print statement must produce a single line with the specified format.\n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While powerful, the standard quadratic penalty method has a significant practical weakness: it requires an increasingly large penalty parameter, which leads to numerical instability and ill-conditioned systems. This final practice introduces a more sophisticated and powerful alternative, the Augmented Lagrangian method, which overcomes this very issue . By implementing both the basic penalty method and the Augmented Lagrangian method to solve the same ill-conditioned problem, you will directly compare their performance and understand why the latter is often a more robust and efficient choice in modern optimization software.",
            "id": "3261582",
            "problem": "Consider the equality-constrained optimization problem in $2$-dimensional Euclidean space:\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) \\quad \\text{subject to} \\quad c(x) = a^\\top x - b = 0,\n$$\nwhere $f(x) = \\tfrac{1}{2} x^\\top Q x$, the matrix $Q \\in \\mathbb{R}^{2 \\times 2}$ is ill-conditioned, $a \\in \\mathbb{R}^2$, and $b \\in \\mathbb{R}$. The ill-conditioning is introduced by taking $Q$ to be diagonal with entries that differ by several orders of magnitude. The constraint is linear and specified by the vector $a$ and the scalar $b$. You will implement and compare two methods: the standard quadratic penalty method and the augmented Lagrangian method.\n\nStarting from the fundamental definition of the quadratic penalty method, construct the sequence of unconstrained minimization problems\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) + \\tfrac{\\rho}{2} \\, c(x)^2,\n$$\nin which the penalty parameter $\\rho$ is increased when the constraint violation $|c(x)|$ is not within a prescribed tolerance. For each fixed $\\rho$, perform the inner unconstrained minimization exactly by solving the first-order optimality condition, which yields a symmetric positive definite (SPD) linear system. Continue increasing $\\rho$ geometrically until the constraint violation is below the tolerance or a maximum number of outer iterations is reached. Record the number of outer iterations required, the final constraint violation $|c(x)|$, and the final objective value $f(x)$.\n\nStarting from the fundamental definition of the augmented Lagrangian method, construct the sequence of unconstrained minimization problems\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) + \\lambda \\, c(x) + \\tfrac{\\mu}{2} \\, c(x)^2,\n$$\nwith a Lagrange multiplier estimate $\\lambda$ and a penalty parameter $\\mu$. For each fixed $(\\lambda, \\mu)$, perform the inner unconstrained minimization exactly by solving the first-order optimality condition, which yields a symmetric positive definite linear system. After obtaining the minimizer, update the Lagrange multiplier using the classical rule derived from the method of multipliers and adjust $\\mu$ geometrically if the constraint violation does not decrease sufficiently. Terminate when the constraint violation $|c(x)|$ is within the tolerance or a maximum number of outer iterations is reached. Record the number of outer iterations required, the final constraint violation $|c(x)|$, and the final objective value $f(x)$.\n\nImplement both methods to solve the problem with the following fixed data:\n- The matrix is $Q = \\operatorname{diag}(q_1, q_2)$.\n- The constraint is defined by $a = [1, 1]^\\top$ and $b = 1$.\n\nImplement the program to process the following test suite. Each test case specifies $(q_1, q_2)$, penalty-method parameters $(\\rho_0, \\gamma_\\rho)$, augmented-Lagrangian parameters $(\\mu_0, \\gamma_\\mu)$, and the tolerance:\n- Test case $1$ (very ill-conditioned):\n  - $q_1 = 10^{-6}$, $q_2 = 1$,\n  - penalty method: initial $\\rho_0 = 1$, geometric factor $\\gamma_\\rho = 10$,\n  - augmented Lagrangian: initial $\\mu_0 = 1$, geometric factor $\\gamma_\\mu = 2$,\n  - tolerance $\\text{tol} = 10^{-8}$.\n- Test case $2$ (moderately ill-conditioned):\n  - $q_1 = 10^{-4}$, $q_2 = 1$,\n  - penalty method: initial $\\rho_0 = 10^{-1}$, geometric factor $\\gamma_\\rho = 5$,\n  - augmented Lagrangian: initial $\\mu_0 = \\tfrac{1}{2}$, geometric factor $\\gamma_\\mu = 2$,\n  - tolerance $\\text{tol} = 10^{-6}$.\n- Test case $3$ (extremely ill-conditioned):\n  - $q_1 = 10^{-8}$, $q_2 = 1$,\n  - penalty method: initial $\\rho_0 = 10^{-3}$, geometric factor $\\gamma_\\rho = 10$,\n  - augmented Lagrangian: initial $\\mu_0 = \\tfrac{1}{2}$, geometric factor $\\gamma_\\mu = 3$,\n  - tolerance $\\text{tol} = 10^{-10}$.\n\nFor each test case, your program must:\n- Run the penalty method and report $(N_P, V_P, F_P)$ where $N_P$ is the number of outer iterations, $V_P$ is the final constraint violation $|c(x)|$, and $F_P$ is the final objective $f(x)$.\n- Run the augmented Lagrangian method and report $(N_A, V_A, F_A)$ with analogous definitions.\n\nDesign choices:\n- For the inner solves, use exact solutions obtained from the first-order optimality conditions, resulting in SPD linear systems of dimension $2$.\n- Use a maximum of $100$ outer iterations for each method to avoid infinite loops.\n- Use a geometric update rule for $\\rho$ and $\\mu$ with the specified factors.\n- For the augmented Lagrangian, update $\\lambda$ using the classical method-of-multipliers rule and increase $\\mu$ only when the constraint violation does not decrease sufficiently compared to the previous iteration.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The outer list must contain one inner list per test case, each inner list ordered as $[N_P,N_A,V_P,V_A,F_P,F_A]$. For example, the final output format must be:\n$ \\big[ \\, [N_P^{(1)},N_A^{(1)},V_P^{(1)},V_A^{(1)},F_P^{(1)},F_A^{(1)}], \\; [N_P^{(2)},N_A^{(2)},V_P^{(2)},V_A^{(2)},F_P^{(2)},F_A^{(2)}], \\; [N_P^{(3)},N_A^{(3)},V_P^{(3)},V_A^{(3)},F_P^{(3)},F_A^{(3)}] \\, \\big] $.\nAll reported values must be real numbers or integers, with no physical units required, and angles are not involved.",
            "solution": "The user has provided a well-posed problem in constrained optimization. The problem is scientifically grounded, objective, and contains all necessary information for its solution. Therefore, it is deemed valid.\n\nThe problem requires solving an equality-constrained quadratic program (QP) using two distinct methods: the quadratic penalty method and the augmented Lagrangian method. We will analyze each method, derive the necessary equations for implementation, and then present the algorithm.\n\nThe optimization problem is defined as:\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) = \\frac{1}{2} x^\\top Q x\n$$\n$$\n\\text{subject to} \\quad c(x) = a^\\top x - b = 0\n$$\nwhere $x = [x_1, x_2]^\\top$, $Q = \\operatorname{diag}(q_1, q_2)$ is a positive definite but ill-conditioned matrix, $a = [1, 1]^\\top$, and $b = 1$.\n\n### The Quadratic Penalty Method\n\nThis method transforms the constrained problem into a sequence of unconstrained problems by adding a penalty term to the objective function that penalizes constraint violations. The penalized objective function for a given penalty parameter $\\rho > 0$ is:\n$$\nP(x; \\rho) = f(x) + \\frac{\\rho}{2} c(x)^2 = \\frac{1}{2} x^\\top Q x + \\frac{\\rho}{2} (a^\\top x - b)^2\n$$\nFor a fixed $\\rho_k$ at outer iteration $k$, we find the minimizer $x_{k+1}$ by solving the unconstrained problem $\\min_x P(x; \\rho_k)$. The first-order optimality condition is $\\nabla_x P(x; \\rho_k) = 0$. The gradient is:\n$$\n\\nabla_x P(x; \\rho_k) = \\nabla f(x) + \\rho_k c(x) \\nabla c(x) = Qx + \\rho_k (a^\\top x - b) a\n$$\nSetting the gradient to zero yields:\n$$\nQx + \\rho_k (a^\\top x) a - \\rho_k b a = 0\n$$\nThis can be rearranged into a linear system of equations for $x$:\n$$\n(Q + \\rho_k a a^\\top) x = \\rho_k b a\n$$\nThe matrix $H_P(\\rho_k) = Q + \\rho_k a a^\\top$ is the Hessian of the penalty function $P(x; \\rho_k)$. Since $Q$ is positive definite and $a a^\\top$ is positive semidefinite, $H_P(\\rho_k)$ is symmetric and positive definite (SPD) for any $\\rho_k > 0$. At each step of the outer loop, we solve this $2 \\times 2$ linear system for $x_{k+1}$. The penalty parameter is then increased, i.e., $\\rho_{k+1} = \\gamma_\\rho \\rho_k$ with $\\gamma_\\rho > 1$, and the process is repeated.\n\nThe method terminates when the constraint violation, $|c(x_{k+1})| = |a^\\top x_{k+1} - b|$, falls below a specified tolerance $\\text{tol}$. A known deficiency of this method is that as $\\rho_k \\to \\infty$ (which is required for $c(x) \\to 0$), the condition number of the Hessian $H_P(\\rho_k)$ grows without bound, leading to a numerically ill-conditioned linear system.\n\n### The Augmented Lagrangian Method\n\nThis method, also known as the method of multipliers, enhances the simple penalty approach by adding an estimate of the Lagrange multiplier to the objective. This typically results in better convergence properties and avoids the need for the penalty parameter to go to infinity. The unconstrained subproblem involves minimizing the augmented Lagrangian:\n$$\n\\mathcal{L}_A(x, \\lambda; \\mu) = f(x) + \\lambda c(x) + \\frac{\\mu}{2} c(x)^2\n$$\nwhere $\\lambda$ is the Lagrange multiplier estimate and $\\mu > 0$ is a penalty parameter. At outer iteration $k$, we use the current estimates $\\lambda_k$ and $\\mu_k$ to find the next iterate $x_{k+1}$ by solving $\\min_x \\mathcal{L}_A(x, \\lambda_k; \\mu_k)$. The first-order optimality condition is $\\nabla_x \\mathcal{L}_A(x, \\lambda_k; \\mu_k) = 0$:\n$$\n\\nabla_x \\mathcal{L}_A = \\nabla f(x) + \\lambda_k \\nabla c(x) + \\mu_k c(x) \\nabla c(x) = Qx + \\lambda_k a + \\mu_k (a^\\top x - b) a\n$$\nSetting the gradient to zero and rearranging gives the linear system:\n$$\n(Q + \\mu_k a a^\\top) x = (\\mu_k b - \\lambda_k) a\n$$\nThe Hessian matrix $H_A(\\mu_k) = Q + \\mu_k a a^\\top$ is structurally identical to that of the penalty method and is also SPD. After solving for $x_{k+1}$, the Lagrange multiplier estimate is updated using the rule:\n$$\n\\lambda_{k+1} = \\lambda_k + \\mu_k c(x_{k+1})\n$$\nThe penalty parameter $\\mu_k$ does not need to go to infinity. It only needs to be large enough to make the Hessian of the augmented Lagrangian positive definite in the vicinity of the solution, which is already guaranteed here since $Q$ is positive definite. In practice, $\\mu_k$ is increased if the reduction in constraint violation is not sufficient. We will adopt the rule that if $|c(x_{k+1})| > 0.25 |c(x_k)|$ for $k>0$, we update $\\mu_{k+1} = \\gamma_\\mu \\mu_k$; otherwise, $\\mu_{k+1} = \\mu_k$. This strategy allows the method to converge to a feasible and optimal solution under much milder conditions on the penalty parameter, thus circumventing the severe ill-conditioning inherent in the quadratic penalty method.\n\n### Algorithmic Implementation\n\nFor both methods, we implement an outer loop that iteratively refines the solution. Each outer iteration involves constructing and solving a $2 \\times 2$ SPD linear system. The loop terminates when the constraint violation $|c(x)|$ is below the tolerance $\\text{tol}$ or a maximum of $100$ iterations is reached.\n\nFor the given problem parameters $a = [1, 1]^\\top$ and $b=1$, the matrices $H_P$ and $H_A$ are:\n$$\nH_P(\\rho_k) = \\begin{pmatrix} q_1 + \\rho_k & \\rho_k \\\\ \\rho_k & q_2 + \\rho_k \\end{pmatrix}, \\quad H_A(\\mu_k) = \\begin{pmatrix} q_1 + \\mu_k & \\mu_k \\\\ \\mu_k & q_2 + \\mu_k \\end{pmatrix}\n$$\nThe right-hand side vectors are:\n$$\nd_P = \\rho_k b a = \\begin{pmatrix} \\rho_k \\\\ \\rho_k \\end{pmatrix}, \\quad d_A = (\\mu_k b - \\lambda_k) a = \\begin{pmatrix} \\mu_k - \\lambda_k \\\\ \\mu_k - \\lambda_k \\end{pmatrix}\n$$\nThe implementation will process the three specified test cases, reporting the number of iterations ($N$), final constraint violation ($V$), and final objective value ($F$) for both the penalty ($P$) and augmented Lagrangian ($A$) methods.",
            "answer": "```python\nimport numpy as np\n\ndef penalty_method(Q, a, b, rho0, gamma_rho, tol, max_iter):\n    \"\"\"\n    Solves a constrained QP using the quadratic penalty method.\n    \"\"\"\n    rho = rho0\n    x = np.zeros(a.shape)\n    \n    for k in range(max_iter):\n        # Form the Hessian and the right-hand side of the linear system\n        H = Q + rho * np.outer(a, a)\n        d = rho * b * a\n        \n        try:\n            x = np.linalg.solve(H, d)\n        except np.linalg.LinAlgError:\n            # This should not occur with the given problem structure (SPD Hessian)\n            return max_iter, np.inf, np.inf\n\n        # Calculate constraint violation\n        violation = np.abs(np.dot(a, x) - b)\n        \n        # Check for convergence\n        if violation < tol:\n            obj_val = 0.5 * np.dot(x, Q @ x)\n            return k + 1, violation, obj_val\n        \n        # Update the penalty parameter\n        rho *= gamma_rho\n        \n    # If max_iter is reached\n    obj_val = 0.5 * np.dot(x, Q @ x)\n    violation = np.abs(np.dot(a, x) - b)\n    return max_iter, violation, obj_val\n\ndef augmented_lagrangian_method(Q, a, b, mu0, gamma_mu, tol, max_iter):\n    \"\"\"\n    Solves a constrained QP using the augmented Lagrangian method.\n    \"\"\"\n    mu = mu0\n    lambda_ = 0.0  # Lagrange multiplier estimate\n    x = np.zeros(a.shape)\n    prev_violation = np.inf\n    \n    for k in range(max_iter):\n        # Form the Hessian and the right-hand side of the linear system\n        H = Q + mu * np.outer(a, a)\n        d = (mu * b - lambda_) * a\n        \n        try:\n            x = np.linalg.solve(H, d)\n        except np.linalg.LinAlgError:\n            return max_iter, np.inf, np.inf\n        \n        # Calculate constraint violation\n        c_x = np.dot(a, x) - b\n        violation = np.abs(c_x)\n        \n        # Check for convergence\n        if violation < tol:\n            obj_val = 0.5 * np.dot(x, Q @ x)\n            return k + 1, violation, obj_val\n        \n        # Update mu if constraint violation does not decrease sufficiently\n        if k > 0 and violation > 0.25 * prev_violation:\n            mu *= gamma_mu\n        \n        # Update lambda\n        lambda_ += mu * c_x\n        \n        prev_violation = violation\n        \n    # If max_iter is reached\n    obj_val = 0.5 * np.dot(x, Q @ x)\n    violation = np.abs(np.dot(a, x) - b)\n    return max_iter, violation, obj_val\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (very ill-conditioned)\n        {'q1': 1e-6, 'q2': 1.0, 'rho0': 1.0, 'gamma_rho': 10.0, 'mu0': 1.0, 'gamma_mu': 2.0, 'tol': 1e-8},\n        # Test case 2 (moderately ill-conditioned)\n        {'q1': 1e-4, 'q2': 1.0, 'rho0': 0.1, 'gamma_rho': 5.0, 'mu0': 0.5, 'gamma_mu': 2.0, 'tol': 1e-6},\n        # Test case 3 (extremely ill-conditioned)\n        {'q1': 1e-8, 'q2': 1.0, 'rho0': 1e-3, 'gamma_rho': 10.0, 'mu0': 0.5, 'gamma_mu': 3.0, 'tol': 1e-10}\n    ]\n    \n    a = np.array([1.0, 1.0])\n    b = 1.0\n    max_iter = 100\n    \n    all_results = []\n    \n    for case in test_cases:\n        Q = np.diag([case['q1'], case['q2']])\n        \n        # Run Penalty Method\n        Np, Vp, Fp = penalty_method(Q, a, b, case['rho0'], case['gamma_rho'], case['tol'], max_iter)\n        \n        # Run Augmented Lagrangian Method\n        Na, Va, Fa = augmented_lagrangian_method(Q, a, b, case['mu0'], case['gamma_mu'], case['tol'], max_iter)\n        \n        case_results = [Np, Na, Vp, Va, Fp, Fa]\n        all_results.append(case_results)\n\n    # Format the output string as a list of lists\n    inner_strings = []\n    for res in all_results:\n        # Format each number to string\n        inner_strings.append(f\"[{','.join(map(str, res))}]\")\n    output_str = f\"[{','.join(inner_strings)}]\"\n    \n    print(output_str)\n\nsolve()\n```"
        }
    ]
}