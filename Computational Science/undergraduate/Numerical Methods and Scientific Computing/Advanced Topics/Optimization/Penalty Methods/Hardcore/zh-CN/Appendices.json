{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握罚函数法，我们必须从一个基础的例子入手。第一个练习将引导你分析一个简单的凸二次规划问题，其解可以通过解析方法推导出来。通过推导罚函数解的精确形式及其收敛速度，你将具体理解罚参数 $\\mu$ 如何驱动解趋向可行域，以及相关的误差表现如何。这个练习 () 将抽象的理论与可触摸的数值行为联系起来。",
            "id": "3261466",
            "problem": "考虑由 $f(x) = \\sum_{i=1}^{n} x_i^2$ 定义的无约束目标，其中 $x \\in \\mathbb{R}^n$，以及由 $g(x) = c^\\top x - 1 = 0$ 给出的单个等式约束，其中 $c \\in \\mathbb{R}^n$ 是一个固定的非零向量。罚函数法为罚参数 $\\mu  0$ 构建了一个罚函数目标 $F_\\mu(x) = f(x) + \\frac{\\mu}{2} \\, g(x)^2$。从无约束和约束最小化的核心定义以及凸二次函数的性质出发，您的任务是：\n- 任务 A：使用基本原理，推导在约束 $g(x) = 0$ 下 $f(x)$ 的约束最小化子 $x^\\star$。\n- 任务 B：使用基本原理，推导给定 $\\mu$ 时罚函数目标 $F_\\mu(x)$ 的无约束最小化子 $x_\\mu$，并证明当 $\\mu \\to +\\infty$ 时，$x_\\mu \\to x^\\star$。\n- 任务 C：估计决策变量中的误差（由 $\\|x_\\mu - x^\\star\\|_2$ 衡量）和约束违反量（由 $|g(x_\\mu)|$ 衡量）随 $\\mu$ 增加而衰减的速率。使用大O表示法将速率表示为 $\\mu$ 和 $c$ 的范数的函数。\n\n您的程序必须实现推导出的公式，并为每个测试用例计算以下量：\n- 决策变量的误差范数：$\\|x_\\mu - x^\\star\\|_2$。\n- 绝对约束违反量：$|g(x_\\mu)|$。\n- 缩放后的误差范数：$\\mu \\, \\|x_\\mu - x^\\star\\|_2$。\n- 缩放后的约束违反量：$\\mu \\, |g(x_\\mu)|$。\n\n所有数值结果必须报告为无量纲量（无物理单位）。结果应四舍五入到 $8$ 位小数。\n\n使用以下参数值测试套件，其中每个用例是一对 $(c, \\mu)$，其中 $c \\in \\mathbb{R}^n$ 和 $\\mu \\in \\mathbb{R}$：\n- 用例 $1$ (正常路径)：$n = 3$, $c = [1, 2, -1]$, $\\mu = 10$。\n- 用例 $2$ (罚强度边界)：$n = 3$, $c = [1, 2, -1]$, $\\mu = 0.1$。\n- 用例 $3$ (渐进状态)：$n = 3$, $c = [1, 2, -1]$, $\\mu = 10^6$。\n- 用例 $4$ (小范数约束向量)：$n = 3$, $c = [10^{-3}, 0, 0]$, $\\mu = 10$。\n- 用例 $5$ (大范数约束向量)：$n = 3$, $c = [3, 4, 0]$, $\\mu = 10$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果必须是按 $[\\|x_\\mu - x^\\star\\|_2, |g(x_\\mu)|, \\mu \\, \\|x_\\mu - x^\\star\\|_2, \\mu \\, |g(x_\\mu)|]$ 固定顺序排列的四个浮点数列表，四舍五入到 $8$ 位小数。因此，最终输出行的格式为\n$[[r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}],[r_{2,1}, r_{2,2}, r_{2,3}, r_{2,4}],\\dots,[r_{5,1}, r_{5,2}, r_{5,3}, r_{5,4}]]$，\n无空格。",
            "solution": "该问题被验证为具有科学依据、适定且客观。所有必要信息均已提供，并且任务在约束优化和罚函数法的既定数学框架内得到了明确定义。\n\n该问题要求对一个简单的凸优化问题分析二次罚函数法。我们给定一个目标函数 $f(x) = \\sum_{i=1}^{n} x_i^2 = x^\\top x$ 和一个线性等式约束 $g(x) = c^\\top x - 1 = 0$，其中 $x \\in \\mathbb{R}^n$，而 $c \\in \\mathbb{R}^n$ 是一个固定的非零向量。二次罚函数法通过求解一系列无约束问题来近似这个约束问题的解，其罚函数目标为 $F_\\mu(x) = f(x) + \\frac{\\mu}{2} \\, g(x)^2 = x^\\top x + \\frac{\\mu}{2}(c^\\top x - 1)^2$，其中罚参数为 $\\mu  0$。\n\n我们的分析按要求分三部分进行。\n\n任务 A：推导约束最小化子 $x^\\star$。\n为找到在约束 $g(x)=0$ 下 $f(x)$ 的最小化子，我们使用拉格朗日乘子法。拉格朗日函数 $\\mathcal{L}(x, \\lambda)$ 定义为：\n$$ \\mathcal{L}(x, \\lambda) = f(x) - \\lambda g(x) = x^\\top x - \\lambda (c^\\top x - 1) $$\n最优性的一阶必要条件要求拉格朗日函数关于 $x$ 的梯度为零向量，并且约束必须满足。\n$$ \\nabla_x \\mathcal{L}(x, \\lambda) = 2x - \\lambda c = 0 $$\n$$ g(x) = c^\\top x - 1 = 0 $$\n从第一个条件，我们用拉格朗日乘子 $\\lambda$ 来表示最优解 $x^\\star$：\n$$ x^\\star = \\frac{\\lambda}{2} c $$\n将此表达式代入约束方程，可以解出 $\\lambda$：\n$$ c^\\top \\left(\\frac{\\lambda}{2} c\\right) - 1 = 0 $$\n$$ \\frac{\\lambda}{2} (c^\\top c) = 1 $$\n$$ \\frac{\\lambda}{2} \\|c\\|_2^2 = 1 $$\n由于 $c$ 是非零向量，$\\|c\\|_2^2  0$，我们可以解出 $\\lambda$：\n$$ \\lambda = \\frac{2}{\\|c\\|_2^2} $$\n将 $\\lambda$ 的值代回 $x^\\star$ 的表达式，我们得到约束最小化子：\n$$ x^\\star = \\frac{1}{2} \\left( \\frac{2}{\\|c\\|_2^2} \\right) c = \\frac{c}{\\|c\\|_2^2} = \\frac{c}{c^\\top c} $$\n由于目标函数 $f(x)$ 是严格凸的，且约束定义了一个凸集（一个超平面），因此这个驻点是唯一的全局最小化子。\n\n任务 B：推导无约束最小化子 $x_\\mu$ 并进行收敛性分析。\n罚函数目标为 $F_\\mu(x) = x^\\top x + \\frac{\\mu}{2}(c^\\top x - 1)^2$。这是一个无约束的、严格凸的函数，因此其唯一的最小化子 $x_\\mu$ 可以通过将其梯度设为零来找到。\n$$ \\nabla_x F_\\mu(x) = \\nabla_x (x^\\top x) + \\frac{\\mu}{2} \\nabla_x ((c^\\top x - 1)^2) $$\n对罚项使用链式法则，其中 $\\nabla_x(u^2) = 2u \\nabla_x u$，我们有：\n$$ \\nabla_x F_\\mu(x) = 2x + \\frac{\\mu}{2} \\cdot 2(c^\\top x - 1) \\cdot c = 2x + \\mu(c^\\top x - 1)c = 0 $$\n整理项得到：\n$$ 2x_\\mu + \\mu(c^\\top x_\\mu - 1)c = 0 $$\n$$ 2x_\\mu + \\mu (c^\\top x_\\mu) c = \\mu c $$\n这可以写成一个线性系统：$(2I + \\mu c c^\\top) x_\\mu = \\mu c$。虽然可以使用矩阵求逆公式，但我们可以通过代数方法更直接地解出 $x_\\mu$。注意到解 $x_\\mu$ 必须是向量 $c$ 的一个标量倍，即 $x_\\mu = k c$。将此代入方程：\n$$ 2(kc) + \\mu(c^\\top (kc))c = \\mu c $$\n$$ 2kc + \\mu k (c^\\top c) c = \\mu c $$\n由于 $c \\neq 0$，我们可以消去 $c$：\n$$ 2k + \\mu k \\|c\\|_2^2 = \\mu $$\n$$ k(2 + \\mu \\|c\\|_2^2) = \\mu $$\n$$ k = \\frac{\\mu}{2 + \\mu \\|c\\|_2^2} $$\n因此，罚函数的最小化子是：\n$$ x_\\mu = \\frac{\\mu}{2 + \\mu \\|c\\|_2^2} c $$\n为了证明当 $\\mu \\to +\\infty$ 时 $x_\\mu \\to x^\\star$，我们对 $x_\\mu$ 的表达式取极限：\n$$ \\lim_{\\mu \\to +\\infty} x_\\mu = \\lim_{\\mu \\to +\\infty} \\left( \\frac{\\mu}{2 + \\mu \\|c\\|_2^2} \\right) c $$\n我们可以将标量分数的分子和分母同除以 $\\mu$：\n$$ \\lim_{\\mu \\to +\\infty} \\left( \\frac{1}{2/\\mu + \\|c\\|_2^2} \\right) c = \\left( \\frac{1}{0 + \\|c\\|_2^2} \\right) c = \\frac{c}{\\|c\\|_2^2} $$\n这个极限恰好是 $x^\\star$ 的表达式，这证实了收敛性 $x_\\mu \\to x^\\star$。\n\n任务 C：估计衰减速率。\n我们现在分析误差和约束违反量趋近于零的速率。\n\n首先，我们分析决策变量中的误差 $\\|x_\\mu - x^\\star\\|_2$。\n$$ x_\\mu - x^\\star = \\frac{\\mu}{2 + \\mu \\|c\\|_2^2} c - \\frac{1}{\\|c\\|_2^2} c = \\left( \\frac{\\mu \\|c\\|_2^2 - (2 + \\mu \\|c\\|_2^2)}{(2 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} \\right) c = \\frac{-2}{(2 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} c $$\n取欧几里得范数：\n$$ \\|x_\\mu - x^\\star\\|_2 = \\left\\| \\frac{-2c}{(2 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} \\right\\|_2 = \\frac{2\\|c\\|_2}{(2 + \\mu \\|c\\|_2^2)\\|c\\|_2^2} = \\frac{2}{(2 + \\mu \\|c\\|_2^2)\\|c\\|_2} $$\n对于大的 $\\mu$，项 $2$ 与 $\\mu \\|c\\|_2^2$ 相比可以忽略不计。因此，渐进行为是：\n$$ \\|x_\\mu - x^\\star\\|_2 \\approx \\frac{2}{(\\mu \\|c\\|_2^2)\\|c\\|_2} = \\frac{2}{\\mu \\|c\\|_2^3} $$\n这表明误差范数以 $O(\\mu^{-1})$ 的速率衰减。\n\n接下来，我们分析约束违反量 $|g(x_\\mu)|$。\n$$ g(x_\\mu) = c^\\top x_\\mu - 1 = c^\\top \\left(\\frac{\\mu}{2 + \\mu \\|c\\|_2^2} c\\right) - 1 = \\frac{\\mu (c^\\top c)}{2 + \\mu \\|c\\|_2^2} - 1 $$\n$$ g(x_\\mu) = \\frac{\\mu \\|c\\|_2^2 - (2 + \\mu \\|c\\|_2^2)}{2 + \\mu \\|c\\|_2^2} = \\frac{-2}{2 + \\mu \\|c\\|_2^2} $$\n取绝对值得到约束违反量：\n$$ |g(x_\\mu)| = \\frac{2}{2 + \\mu \\|c\\|_2^2} $$\n对于大的 $\\mu$，渐进行为是：\n$$ |g(x_\\mu)| \\approx \\frac{2}{\\mu \\|c\\|_2^2} $$\n这表明约束违反量也以 $O(\\mu^{-1})$ 的速率衰减。\n\n用于实现的公式总结：\n对于给定的 $c \\in \\mathbb{R}^n$ 和 $\\mu  0$：\n$1$. 误差范数：$\\|x_\\mu - x^\\star\\|_2 = \\frac{2}{(2 + \\mu \\|c\\|_2^2)\\|c\\|_2}$\n$2$. 约束违反量：$|g(x_\\mu)| = \\frac{2}{2 + \\mu \\|c\\|_2^2}$\n$3$. 缩放后的误差范数：$\\mu \\|x_\\mu - x^\\star\\|_2 = \\frac{2\\mu}{(2 + \\mu \\|c\\|_2^2)\\|c\\|_2}$\n$4$. 缩放后的约束违反量：$\\mu |g(x_\\mu)| = \\frac{2\\mu}{2 + \\mu \\|c\\|_2^2}$\n这些公式将用于计算测试用例所需的量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the derived formulas for the penalty method analysis\n    and computes results for the specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (c, mu), where c is a list of floats\n    # and mu is a float.\n    test_cases = [\n        # Case 1 (happy path)\n        ([1.0, 2.0, -1.0], 10.0),\n        # Case 2 (boundary in penalty strength)\n        ([1.0, 2.0, -1.0], 0.1),\n        # Case 3 (asymptotic regime)\n        ([1.0, 2.0, -1.0], 1e6),\n        # Case 4 (small-norm constraint vector)\n        ([1e-3, 0.0, 0.0], 10.0),\n        # Case 5 (large-norm constraint vector)\n        ([3.0, 4.0, 0.0], 10.0),\n    ]\n\n    all_results = []\n    for c_list, mu in test_cases:\n        c = np.array(c_list)\n\n        # Calculate squared L2 norm of c\n        c_norm_sq = np.dot(c, c)\n        \n        # Calculate L2 norm of c\n        c_norm = np.sqrt(c_norm_sq)\n\n        # The term (2 + mu * ||c||^2) appears in all denominators\n        denominator = 2.0 + mu * c_norm_sq\n\n        # Task C derived the following exact formulas:\n        # ||x_mu - x*||_2 = 2 / ((2 + mu * ||c||^2) * ||c||)\n        # |g(x_mu)| = 2 / (2 + mu * ||c||^2)\n\n        # Calculate error norm in the decision variable\n        error_norm = 2.0 / (denominator * c_norm)\n\n        # Calculate absolute constraint violation\n        constraint_violation = 2.0 / denominator\n\n        # Calculate scaled error norm\n        scaled_error_norm = mu * error_norm\n\n        # Calculate scaled constraint violation\n        scaled_constraint_violation = mu * constraint_violation\n\n        # Store the four computed values for the current test case\n        case_result = [\n            error_norm,\n            constraint_violation,\n            scaled_error_norm,\n            scaled_constraint_violation,\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string as specified in the problem statement.\n    # The output is a list of lists, with no spaces and 8 decimal places.\n    case_strings = []\n    for res in all_results:\n        # Format each number to 8 decimal places and join into a string like \"[n1,n2,n3,n4]\"\n        num_strings = [f\"{val:.8f}\" for val in res]\n        case_strings.append(f\"[{','.join(num_strings)}]\")\n    \n    # Join the case strings into a final string like \"[[...],[...],...]\"\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "尽管基本的罚函数法很直观，但它存在一个主要的实际缺陷：随着罚参数的增大，其底层的数值问题会变得严重病态。本练习将直面这一问题，通过比较标准罚函数法与其强大的继任者——增广拉格朗日法。通过在一个病态问题 () 上实现这两种算法，你将亲眼见证增广拉格朗日法在收敛性和数值稳定性上的优越性。这项对比研究对于理解现代优化算法的演进至关重要。",
            "id": "3261582",
            "problem": "考虑一个在$2$维欧几里得空间中的等式约束优化问题：\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) \\quad \\text{subject to} \\quad c(x) = a^\\top x - b = 0,\n$$\n其中 $f(x) = \\tfrac{1}{2} x^\\top Q x$，矩阵 $Q \\in \\mathbb{R}^{2 \\times 2}$ 是病态的，$a \\in \\mathbb{R}^2$ 且 $b \\in \\mathbb{R}$。病态性是通过将 $Q$ 取为对角矩阵引入的，其对角线上的元素相差几个数量级。该约束是线性的，由向量 $a$ 和标量 $b$ 指定。您将实现并比较两种方法：标准的二次罚函数法和增广拉格朗日方法。\n\n从二次罚函数法的基本定义出发，构建如下一系列无约束最小化问题\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) + \\tfrac{\\rho}{2} \\, c(x)^2,\n$$\n在这些问题中，当约束违反度 $|c(x)|$ 不在规定的容差范围内时，罚参数 $\\rho$ 会增加。对于每个固定的 $\\rho$，通过求解一阶最优性条件来精确地执行内部无约束最小化，这会得到一个对称正定（SPD）线性系统。以几何级数方式持续增加 $\\rho$，直到约束违反度低于容差或达到最大外部迭代次数。记录所需的外部迭代次数、最终的约束违反度 $|c(x)|$ 以及最终的目标函数值 $f(x)$。\n\n从增广拉格朗日方法的基本定义出发，构建如下一系列无约束最小化问题\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) + \\lambda \\, c(x) + \\tfrac{\\mu}{2} \\, c(x)^2,\n$$\n其中包含拉格朗日乘子估计值 $\\lambda$ 和一个罚参数 $\\mu$。对于每个固定的 $(\\lambda, \\mu)$，通过求解一阶最优性条件来精确地执行内部无约束最小化，这会得到一个对称正定线性系统。在获得极小值点后，使用从乘子法推导出的经典规则更新拉格朗日乘子，并且如果约束违反度没有充分减小，则以几何级数方式调整 $\\mu$。当约束违反度 $|c(x)|$ 在容差范围内或达到最大外部迭代次数时终止。记录所需的外部迭代次数、最终的约束违反度 $|c(x)|$ 以及最终的目标函数值 $f(x)$。\n\n实现这两种方法来解决问题，使用以下固定数据：\n- 矩阵为 $Q = \\operatorname{diag}(q_1, q_2)$。\n- 约束由 $a = [1, 1]^\\top$ 和 $b = 1$ 定义。\n\n实现程序以处理以下测试套件。每个测试用例指定了 $(q_1, q_2)$、罚函数法参数 $(\\rho_0, \\gamma_\\rho)$、增广拉格朗日方法参数 $(\\mu_0, \\gamma_\\mu)$ 以及容差：\n- 测试用例 1 (非常病态)：\n  - $q_1 = 10^{-6}$, $q_2 = 1$,\n  - 罚函数法：初始 $\\rho_0 = 1$，几何因子 $\\gamma_\\rho = 10$,\n  - 增广拉格朗日方法：初始 $\\mu_0 = 1$，几何因子 $\\gamma_\\mu = 2$,\n  - 容差 $\\text{tol} = 10^{-8}$。\n- 测试用例 2 (中等病态)：\n  - $q_1 = 10^{-4}$, $q_2 = 1$,\n  - 罚函数法：初始 $\\rho_0 = 10^{-1}$，几何因子 $\\gamma_\\rho = 5$,\n  - 增广拉格朗日方法：初始 $\\mu_0 = \\tfrac{1}{2}$，几何因子 $\\gamma_\\mu = 2$,\n  - 容差 $\\text{tol} = 10^{-6}$。\n- 测试用例 3 (极端病态)：\n  - $q_1 = 10^{-8}$, $q_2 = 1$,\n  - 罚函数法：初始 $\\rho_0 = 10^{-3}$，几何因子 $\\gamma_\\rho = 10$,\n  - 增广拉格朗日方法：初始 $\\mu_0 = \\tfrac{1}{2}$，几何因子 $\\gamma_\\mu = 3$,\n  - 容差 $\\text{tol} = 10^{-10}$。\n\n对于每个测试用例，您的程序必须：\n- 运行罚函数法并报告 $(N_P, V_P, F_P)$，其中 $N_P$是外部迭代次数，$V_P$ 是最终约束违反度 $|c(x)|$，$F_P$ 是最终目标函数值 $f(x)$。\n- 运行增广拉格朗日方法并报告 $(N_A, V_A, F_A)$，其定义与前者类似。\n\n设计选择：\n- 对于内部求解，使用从一阶最优性条件获得的精确解，这会产生维度为 $2$ 的 SPD 线性系统。\n- 为避免无限循环，每种方法最多使用 $100$ 次外部迭代。\n- 对 $\\rho$ 和 $\\mu$ 使用指定的因子进行几何更新。\n- 对于增广拉格朗日方法，使用经典的乘子法规则更新 $\\lambda$，并且仅当与前一次迭代相比，约束违反度没有充分减小时才增加 $\\mu$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。外部列表必须为每个测试用例包含一个内部列表，每个内部列表按 $[N_P,N_A,V_P,V_A,F_P,F_A]$ 的顺序排列。例如，最终输出格式必须是：\n$$\n\\big[ \\, [N_P^{(1)},N_A^{(1)},V_P^{(1)},V_A^{(1)},F_P^{(1)},F_A^{(1)}], \\; [N_P^{(2)},N_A^{(2)},V_P^{(2)},V_A^{(2)},F_P^{(2)},F_A^{(2)}], \\; [N_P^{(3)},N_A^{(3)},V_P^{(3)},V_A^{(3)},F_P^{(3)},F_A^{(3)}] \\, \\big].\n$$\n所有报告的值必须是实数或整数，不需要物理单位，也不涉及角度。",
            "solution": "用户提供了一个适定的约束优化问题。该问题具有科学依据，是客观的，并包含其求解所需的所有必要信息。因此，该问题被视为有效。\n\n该问题要求使用两种不同的方法来解决一个等式约束二次规划（QP）问题：二次罚函数法和增广拉格朗日方法。我们将分析每种方法，推导实现所需的方程，然后给出算法。\n\n该优化问题定义如下：\n$$\n\\min_{x \\in \\mathbb{R}^2} \\; f(x) = \\frac{1}{2} x^\\top Q x\n$$\n$$\n\\text{subject to} \\quad c(x) = a^\\top x - b = 0\n$$\n其中 $x = [x_1, x_2]^\\top$，$Q = \\operatorname{diag}(q_1, q_2)$ 是一个正定但病态的矩阵，$a = [1, 1]^\\top$，$b = 1$。\n\n### 二次罚函数法\n\n该方法通过在目标函数中添加一个惩罚约束违反的罚项，将约束问题转化为一系列无约束问题。对于给定的罚参数 $\\rho  0$，带罚项的目标函数为：\n$$\nP(x; \\rho) = f(x) + \\frac{\\rho}{2} c(x)^2 = \\frac{1}{2} x^\\top Q x + \\frac{\\rho}{2} (a^\\top x - b)^2\n$$\n在外部迭代第 $k$ 次时，对于一个固定的 $\\rho_k$，我们通过求解无约束问题 $\\min_x P(x; \\rho_k)$ 来找到极小值点 $x_{k+1}$。一阶最优性条件是 $\\nabla_x P(x; \\rho_k) = 0$。梯度为：\n$$\n\\nabla_x P(x; \\rho_k) = \\nabla f(x) + \\rho_k c(x) \\nabla c(x) = Qx + \\rho_k (a^\\top x - b) a\n$$\n令梯度为零可得：\n$$\nQx + \\rho_k (a^\\top x) a - \\rho_k b a = 0\n$$\n这可以重排为一个关于 $x$ 的线性方程组：\n$$\n(Q + \\rho_k a a^\\top) x = \\rho_k b a\n$$\n矩阵 $H_P(\\rho_k) = Q + \\rho_k a a^\\top$ 是罚函数 $P(x; \\rho_k)$ 的海森矩阵。由于 $Q$ 是正定的，且 $a a^\\top$ 是半正定的，因此对于任何 $\\rho_k  0$，$H_P(\\rho_k)$ 都是对称正定的（SPD）。在外循环的每一步，我们求解这个 $2 \\times 2$ 线性系统以得到 $x_{k+1}$。然后增加罚参数，即 $\\rho_{k+1} = \\gamma_\\rho \\rho_k$ (其中 $\\gamma_\\rho  1$)，并重复该过程。\n\n当约束违反度 $|c(x_{k+1})| = |a^\\top x_{k+1} - b|$ 小于指定容差 $\\text{tol}$ 时，该方法终止。该方法的一个已知缺陷是，当 $\\rho_k \\to \\infty$ (这是 $c(x) \\to 0$ 的要求) 时，海森矩阵 $H_P(\\rho_k)$ 的条件数会无界增长，导致数值上病态的线性系统。\n\n### 增广拉格朗日方法\n\n这种方法，也称为乘子法，通过在目标函数中加入拉格朗日乘子的估计值来增强简单的罚函数方法。这通常会带来更好的收敛性质，并避免了罚参数需要趋于无穷大的问题。无约束子问题涉及最小化增广拉格朗日函数：\n$$\n\\mathcal{L}_A(x, \\lambda; \\mu) = f(x) + \\lambda c(x) + \\frac{\\mu}{2} c(x)^2\n$$\n其中 $\\lambda$ 是拉格朗日乘子的估计值，$\\mu  0$ 是一个罚参数。在外部迭代第 $k$ 次时，我们使用当前的估计值 $\\lambda_k$ 和 $\\mu_k$，通过求解 $\\min_x \\mathcal{L}_A(x, \\lambda_k; \\mu_k)$ 来找到下一个迭代点 $x_{k+1}$。一阶最优性条件是 $\\nabla_x \\mathcal{L}_A(x, \\lambda_k; \\mu_k) = 0$：\n$$\n\\nabla_x \\mathcal{L}_A = \\nabla f(x) + \\lambda_k \\nabla c(x) + \\mu_k c(x) \\nabla c(x) = Qx + \\lambda_k a + \\mu_k (a^\\top x - b) a\n$$\n令梯度为零并重新整理，得到线性系统：\n$$\n(Q + \\mu_k a a^\\top) x = (\\mu_k b - \\lambda_k) a\n$$\n海森矩阵 $H_A(\\mu_k) = Q + \\mu_k a a^\\top$ 在结构上与罚函数法的海森矩阵相同，并且也是SPD。求解出 $x_{k+1}$ 后，使用以下规则更新拉格朗日乘子的估计值：\n$$\n\\lambda_{k+1} = \\lambda_k + \\mu_k c(x_{k+1})\n$$\n罚参数 $\\mu_k$ 不需要趋于无穷大。它只需要足够大，以使增广拉格朗日函数的海森矩阵在解的邻域内是正定的，而在这里由于 $Q$ 是正定的，这一点已经得到了保证。在实践中，如果约束违反度的减小不充分，则会增加 $\\mu_k$。我们将采用这样的规则：如果对于 $k0$ 有 $|c(x_{k+1})|  0.25 |c(x_k)|$，则更新 $\\mu_{k+1} = \\gamma_\\mu \\mu_k$；否则，$\\mu_{k+1} = \\mu_k$。这种策略使得该方法能够在对罚参数要求宽松得多的条件下收敛到可行最优解，从而避免了二次罚函数法固有的严重病态问题。\n\n### 算法实现\n\n对于这两种方法，我们都实现一个外部循环来迭代地精化解。每次外部迭代都涉及构建并求解一个 $2 \\times 2$ 的SPD线性系统。当约束违反度 $|c(x)|$ 低于容差 $\\text{tol}$ 或达到最大迭代次数 $100$ 次时，循环终止。\n\n对于给定的问题参数 $a = [1, 1]^\\top$ 和 $b=1$，矩阵 $H_P$ 和 $H_A$ 分别为：\n$$\nH_P(\\rho_k) = \\begin{pmatrix} q_1 + \\rho_k  \\rho_k \\\\ \\rho_k  q_2 + \\rho_k \\end{pmatrix}, \\quad H_A(\\mu_k) = \\begin{pmatrix} q_1 + \\mu_k  \\mu_k \\\\ \\mu_k  q_2 + \\mu_k \\end{pmatrix}\n$$\n右侧向量为：\n$$\nd_P = \\rho_k b a = \\begin{pmatrix} \\rho_k \\\\ \\rho_k \\end{pmatrix}, \\quad d_A = (\\mu_k b - \\lambda_k) a = \\begin{pmatrix} \\mu_k - \\lambda_k \\\\ \\mu_k - \\lambda_k \\end{pmatrix}\n$$\n实现将处理三个指定的测试用例，报告罚函数（$P$）和增广拉格朗日（$A$）方法的迭代次数（$N$）、最终约束违反度（$V$）和最终目标函数值（$F$）。",
            "answer": "```python\nimport numpy as np\n\ndef penalty_method(Q, a, b, rho0, gamma_rho, tol, max_iter):\n    \"\"\"\n    Solves a constrained QP using the quadratic penalty method.\n    \"\"\"\n    rho = rho0\n    x = np.zeros(a.shape)\n    \n    for k in range(max_iter):\n        # Form the Hessian and the right-hand side of the linear system\n        H = Q + rho * np.outer(a, a)\n        d = rho * b * a\n        \n        try:\n            x = np.linalg.solve(H, d)\n        except np.linalg.LinAlgError:\n            # This should not occur with the given problem structure (SPD Hessian)\n            return max_iter, np.inf, np.inf\n\n        # Calculate constraint violation\n        violation = np.abs(np.dot(a, x) - b)\n        \n        # Check for convergence\n        if violation  tol:\n            obj_val = 0.5 * np.dot(x, Q @ x)\n            return k + 1, violation, obj_val\n        \n        # Update the penalty parameter\n        rho *= gamma_rho\n        \n    # If max_iter is reached\n    obj_val = 0.5 * np.dot(x, Q @ x)\n    violation = np.abs(np.dot(a, x) - b)\n    return max_iter, violation, obj_val\n\ndef augmented_lagrangian_method(Q, a, b, mu0, gamma_mu, tol, max_iter):\n    \"\"\"\n    Solves a constrained QP using the augmented Lagrangian method.\n    \"\"\"\n    mu = mu0\n    lambda_ = 0.0  # Lagrange multiplier estimate\n    x = np.zeros(a.shape)\n    prev_violation = np.inf\n    \n    for k in range(max_iter):\n        # Form the Hessian and the right-hand side of the linear system\n        H = Q + mu * np.outer(a, a)\n        d = (mu * b - lambda_) * a\n        \n        try:\n            x = np.linalg.solve(H, d)\n        except np.linalg.LinAlgError:\n            return max_iter, np.inf, np.inf\n        \n        # Calculate constraint violation\n        c_x = np.dot(a, x) - b\n        violation = np.abs(c_x)\n        \n        # Check for convergence\n        if violation  tol:\n            obj_val = 0.5 * np.dot(x, Q @ x)\n            return k + 1, violation, obj_val\n        \n        # Update mu if constraint violation does not decrease sufficiently\n        if k > 0 and violation > 0.25 * prev_violation:\n            mu *= gamma_mu\n        \n        # Update lambda\n        lambda_ += mu * c_x\n        \n        prev_violation = violation\n        \n    # If max_iter is reached\n    obj_val = 0.5 * np.dot(x, Q @ x)\n    violation = np.abs(np.dot(a, x) - b)\n    return max_iter, violation, obj_val\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (very ill-conditioned)\n        {'q1': 1e-6, 'q2': 1.0, 'rho0': 1.0, 'gamma_rho': 10.0, 'mu0': 1.0, 'gamma_mu': 2.0, 'tol': 1e-8},\n        # Test case 2 (moderately ill-conditioned)\n        {'q1': 1e-4, 'q2': 1.0, 'rho0': 0.1, 'gamma_rho': 5.0, 'mu0': 0.5, 'gamma_mu': 2.0, 'tol': 1e-6},\n        # Test case 3 (extremely ill-conditioned)\n        {'q1': 1e-8, 'q2': 1.0, 'rho0': 1e-3, 'gamma_rho': 10.0, 'mu0': 0.5, 'gamma_mu': 3.0, 'tol': 1e-10}\n    ]\n    \n    a = np.array([1.0, 1.0])\n    b = 1.0\n    max_iter = 100\n    \n    all_results = []\n    \n    for case in test_cases:\n        Q = np.diag([case['q1'], case['q2']])\n        \n        # Run Penalty Method\n        Np, Vp, Fp = penalty_method(Q, a, b, case['rho0'], case['gamma_rho'], case['tol'], max_iter)\n        \n        # Run Augmented Lagrangian Method\n        Na, Va, Fa = augmented_lagrangian_method(Q, a, b, case['mu0'], case['gamma_mu'], case['tol'], max_iter)\n        \n        case_results = [Np, Na, Vp, Va, Fp, Fa]\n        all_results.append(case_results)\n\n    # Format the output string as a list of lists\n    inner_strings = []\n    for res in all_results:\n        # Format each number to string\n        inner_strings.append(f\"[{','.join(map(str, res))}]\")\n    output_str = f\"[{','.join(inner_strings)}]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "到目前为止，我们的探索主要集中在只有一个全局最优解的凸问题上。然而，许多现实世界的问题是非凸的，这导致了具有多个局部最优解的复杂优化景观。最后一个练习将探讨罚函数法在非凸环境下的挑战性行为。本练习 () 要求你开发一个算法来描绘目标函数的地形，并识别“伪”局部最优解——那些由罚函数法引入但对原问题并不可行的解。这将加深你对这些方法潜在陷阱的理解，以及在非凸优化中进行审慎分析的重要性。",
            "id": "3261519",
            "problem": "您将使用数值方法和科学计算中的罚函数法，实现并分析一个由约束优化问题产生的非凸惩罚目标函数。目标是针对几个惩罚参数，通过算法找出惩罚函数的伪局部最小值点，并报告出现了多少个这样的伪最小值点。在整个问题中，所有三角函数的角度都必须以弧度为单位进行解释。\n\n考虑以下包含两个实变量 $x$ 和 $y$ 的原始约束问题：\n最小化非凸函数\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2}\\,\\sin(3x)\\sin(3y),\n$$\n服从等式约束\n$$\ng(x,y) = x + y = 0,\n$$\n和不等式约束\n$$\nh(x,y) = x^2 + y^2 - 3 \\le 0.\n$$\n使用二次罚函数法，为惩罚参数 $r \\ge 0$ 定义惩罚目标函数：\n$$\nF_r(x,y) = f(x,y) + \\frac{r}{2}\\,g(x,y)^2 + \\frac{r}{2}\\,\\max\\{0, h(x,y)\\}^2.\n$$\n由于 $f$ 是非凸的，因此对于所有的 $r$，这个惩罚目标函数 $F_r$ 也是非凸的。\n\n如果存在点 $(x^\\ast,y^\\ast)$ 的一个邻域，在该邻域内 $F_r$ 在 $(x^\\ast,y^\\ast)$ 处取得最小值，则称该点为 $F_r$ 的一个局部最小值点。如果 $F_r$ 的一个局部最小值点 $(x^\\ast,y^\\ast)$ 对于原始问题是不可行的，即它不满足 $g(x^\\ast,y^\\ast)=0$ 或 $h(x^\\ast,y^\\ast)\\le 0$，则称该点为伪（spurious）局部最小值点。\n\n您的任务是根据第一性原理设计一个算法，通过离散化来近似有界域上 $F_r$ 的局部最小值点集合，然后对这些局部最小值点进行分类，判断哪些是伪的。请使用以下基本依据：\n- 基于邻域内函数值的局部最小值定义。\n- 通过为违反约束的行为增加非负惩罚项来构造罚函数法。\n- 三角函数的角度以弧度为单位进行解释。\n\n您的算法必须遵守以下规范。\n- 离散化域：方形域 $[-2,2]\\times[-2,2]$。\n- 网格分辨率：沿每个轴有 $401$ 个点的均匀网格。\n- 局部最小值检测：在网格上使用 $8$ 邻域准则。如果一个网格点 $(i,j)$ 的函数值小于或等于其所有 $8$ 个相邻邻居的函数值，并且严格小于其中至少一个邻居的函数值，则该点为局部最小值点。使用数值容差 $\\varepsilon_{\\text{lm}}=10^{-12}$ 来比较浮点数值。\n- 可行性分类：在坐标 $(x,y)$ 处检测到的一个局部最小值点，如果满足 $|g(x,y)|\\le \\tau$ 和 $h(x,y)\\le \\tau$（容差 $\\tau = 10^{-6}$），则该点是可行的。任何不满足此准则的已检测局部最小值点都是伪的。\n\n惩罚参数测试集：\n- 情况 1: $r=0$（无惩罚；一个边界情况）。\n- 情况 2: $r=1$（小惩罚）。\n- 情况 3: $r=10$（中等惩罚）。\n- 情况 4: $r=200$（大惩罚）。\n\n对于每种情况，根据上述规则计算网格上 $F_r$ 的伪局部最小值点的数量。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试集的顺序排列结果。例如，如果四种情况下伪局部最小值点的数量分别为 $a$、$b$、$c$ 和 $d$，程序必须打印\n$$\n[a,b,c,d]\n$$\n在单行上，不含空格。",
            "solution": "用户提供了一个数值优化领域的问题，具体涉及分析将二次罚函数法应用于非凸约束优化问题时产生的伪局部最小值点。任务是实现一个基于网格离散化和邻域比较的特定算法，以便为一组给定的惩罚参数计算这些伪最小值点的数量。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n*   **目标函数：** $f(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2}\\,\\sin(3x)\\sin(3y)$\n*   **等式约束：** $g(x,y) = x + y = 0$\n*   **不等式约束：** $h(x,y) = x^2 + y^2 - 3 \\le 0$\n*   **惩罚目标函数：** $F_r(x,y) = f(x,y) + \\frac{r}{2}\\,g(x,y)^2 + \\frac{r}{2}\\,\\max\\{0, h(x,y)\\}^2$，惩罚参数 $r \\ge 0$。\n*   **伪局部最小值点的定义：** $F_r$ 的一个局部最小值点 $(x^\\ast,y^\\ast)$ 如果对原始问题不可行，即不满足 $g(x^\\ast,y^\\ast)=0$ 或 $h(x^\\ast,y^\\ast)\\le 0$，则称其为伪的。\n*   **离散化域：** $[-2,2]\\times[-2,2]$。\n*   **网格分辨率：** 沿每个轴有 $401$ 个点的均匀网格。\n*   **局部最小值检测准则：** 如果一个网格点 $(i,j)$ 的函数值小于或等于其所有 $8$ 个相邻邻居的值，并且严格小于其中至少一个邻居的值，则该点为局部最小值点。\n*   **局部最小值数值容差：** $\\varepsilon_{\\text{lm}}=10^{-12}$。\n*   **可行性分类准则：** 在 $(x,y)$ 处检测到的一个局部最小值点，如果 $|g(x,y)|\\le \\tau$ 且 $h(x,y)\\le \\tau$（容差 $\\tau = 10^{-6}$），则为可行的。否则，它是伪的。\n*   **惩罚参数测试集：** $r \\in \\{0, 1, 10, 200\\}$。\n*   **角度约定：** 所有三角函数的参数均以弧度为单位。\n*   **最终输出格式：** `[a,b,c,d]`，其中 $a,b,c,d$ 是相应测试用例的伪局部最小值点的数量。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n根据验证标准对问题进行评估：\n\n*   **科学依据：** 该问题牢固地植根于约束优化和数值方法的理论。罚函数法是一种标准技术，所涉及的函数是定义明确的数学构造。整个设置是科学合理的。\n*   **适定性：** 该问题是适定的。它要求根据一套完全指定的规则和数值容差，计算一个具体的、可数的量（网格上的伪局部最小值点的数量）。对于每个输入 $r$，算法唯一确定输出。\n*   **客观性：** 问题使用精确、客观的数学语言进行表述。所有分类标准（局部最小值、可行性）都给出了明确的数值容差，没有主观解释的余地。\n*   **完整性和一致性：** 问题陈述是自洽的，提供了所有必要的函数、常数、域和算法定义。没有矛盾之处。\n\n**步骤 3：结论与行动**\n\n问题有效。这是一个在数值分析领域中定义明确的计算任务。现在开始求解过程。\n\n### 解法\n\n问题的核心是实现一个数值方案，用于在离散网格上识别和分类惩罚目标函数 $F_r(x,y)$ 的局部最小值点。解法是通过系统地遵循所提供的算法规范来制定的。\n\n**1. 问题建模**\n\n原始约束优化问题是最小化非凸目标函数，\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2}\\,\\sin(3x)\\sin(3y)\n$$\n服从约束\n$$\ng(x,y) = x + y = 0 \\quad \\text{(等式)}\n$$\n$$\nh(x,y) = x^2 + y^2 - 3 \\le 0 \\quad \\text{(不等式)}\n$$\n二次罚函数法通过在目标函数中增加仅在违反约束时才非零的惩罚项，将此问题转化为一个无约束问题。得到的惩罚目标函数为：\n$$\nF_r(x,y) = f(x,y) + \\frac{r}{2}\\,g(x,y)^2 + \\frac{r}{2}\\,(\\max\\{0, h(x,y)\\})^2\n$$\n其中 $r \\ge 0$ 是惩罚参数。当 $r \\to \\infty$ 时，$F_r(x,y)$ 的最小值点预计会收敛到原始约束问题的解。伪最小值点是对于有限的 $r$，$F_r$ 的局部最小值点，但它们不满足原始约束。\n\n**2. 算法设计**\n\n指定的算法按以下步骤进行：\n\n*   **离散化：** 将连续的方形域 $[-2,2]\\times[-2,2]$ 离散化为一个 $401 \\times 401$ 的均匀网格。每个轴上的步长为 $\\Delta x = \\Delta y = (2 - (-2)) / (401 - 1) = 0.01$。网格点为 $(x_j, y_i)$，其中 $x_j = -2 + j \\cdot \\Delta x$ 且 $y_i = -2 + i \\cdot \\Delta y$，对于 $i,j \\in \\{0, 1, \\dots, 400\\}$。\n\n*   **函数求值：** 对于每个测试用例 $r \\in \\{0, 1, 10, 200\\}$，在网格上的每个点计算函数 $F_r(x,y)$ 的值，得到一个 $401 \\times 401$ 的值矩阵，我们将其记为 $\\mathbf{F}$。元素 $\\mathbf{F}_{i,j}$ 对应于值 $F_r(x_j, y_i)$。\n\n*   **局部最小值识别：** 我们遍历网格的内部（$1 \\le i,j \\le 399$）以检查每个点的局部最小值属性。一个值为 $V_c = \\mathbf{F}_{i,j}$ 的网格点 $(x_j, y_i)$ 是局部最小值点，当且仅当它相对于其八个邻居 $V_n$ 满足两个条件：\n    1.  $V_c \\le V_n + \\varepsilon_{\\text{lm}}$ 对于所有八个邻居。\n    2.  $V_c  V_n - \\varepsilon_{\\text{lm}}$ 对于至少一个邻居。\n    容差 $\\varepsilon_{\\text{lm}} = 10^{-12}$ 用于鲁棒的浮点数比较。这个定义能正确处理斜坡区域，同时避免将整个平坦区域都分类为最小值点。\n\n*   **可行性与伪点分类：** 每个在网格坐标 $(x_j, y_i)$ 处识别出的局部最小值点，都会根据原始约束进行可行性测试。使用容差 $\\tau = 10^{-6}$。如果满足以下条件，该最小值点被视为**可行的**：\n    $$\n    |g(x_j, y_i)| \\le \\tau \\quad \\text{且} \\quad h(x_j, y_i) \\le \\tau\n    $$\n    如果这些条件中任意一个不满足，该局部最小值点就被分类为**伪的**（spurious）。\n\n*   **计数：** 对于每个 $r$ 值，计算在网格上找到的伪局部最小值点的总数。\n\n**3. 惩罚参数分析**\n\n参数 $r$ 控制对违反约束的惩罚权重。\n*   当 $r=0$ 时，我们有 $F_0(x,y) = f(x,y)$。算法将找到原始目标函数的局部最小值点，完全忽略约束。鉴于正弦项的振荡性质和多项式部分的四个“井”，预计会有很多局部最小值点。其中大部分不会满足约束，因此将被计为伪的。\n*   随着 $r$ 的增加（$r=1, 10, 200$），惩罚项对远离可行区域（圆 $x^2+y^2=3$ 内的线段 $y=-x$）的点增加了高昂的“成本”。这在 $F_r$ 的函数地貌上沿着可行集形成了一个深深的“峡谷”。位于此峡谷之外的局部最小值点被提升到更高的函数值，并趋于消失。剩余的最小值点被推向可行集。因此，我们预计伪局部最小值点的数量会随着 $r$ 的增加而减少。对于足够大的 $r$， $F_r$ 的局部最小值点将位于非常接近或恰好在可行集上的网格点，从而导致伪最小值点的计数很低或为零。\n\n该算法使用 Python 实现，并遵循这些原则。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a non-convex penalized objective to find the number of\n    spurious local minima for different penalty parameters.\n    \"\"\"\n\n    # Define the test cases (penalty parameters) from the problem statement.\n    test_cases = [0, 1, 10, 200]\n    \n    # List to store the count of spurious minima for each case.\n    results = []\n\n    # --- Problem-specific functions ---\n    def f_obj(x, y):\n        \"\"\"Vectorized objective function f(x, y).\"\"\"\n        return (x**2 - 1)**2 + (y**2 - 1)**2 + 0.5 * np.sin(3 * x) * np.sin(3 * y)\n\n    def g_constraint(x, y):\n        \"\"\"Vectorized equality constraint g(x, y).\"\"\"\n        return x + y\n\n    def h_constraint(x, y):\n        \"\"\"Vectorized inequality constraint h(x, y).\"\"\"\n        return x**2 + y**2 - 3\n\n    def F_penalized(x, y, r):\n        \"\"\"Vectorized penalized objective function F_r(x, y).\"\"\"\n        penalty_g = (r / 2) * g_constraint(x, y)**2\n        penalty_h = (r / 2) * np.maximum(0, h_constraint(x, y))**2\n        return f_obj(x, y) + penalty_g + penalty_h\n\n    # --- Main loop for each test case ---\n    for r in test_cases:\n        # --- Algorithm Specification ---\n        N = 401  # Grid resolution\n        domain_min, domain_max = -2, 2\n        eps_lm = 1e-12  # Tolerance for local minimum detection\n        tau = 1e-6  # Tolerance for feasibility classification\n\n        # 1. Discretization of the domain\n        x_range = np.linspace(domain_min, domain_max, N)\n        y_range = np.linspace(domain_min, domain_max, N)\n        # Note: meshgrid with 'xy' indexing (default) means xx varies with the second index (columns)\n        # and yy varies with the first index (rows). F_values[i, j] corresponds to (x[j], y[i]).\n        xx, yy = np.meshgrid(x_range, y_range)\n\n        # 2. Evaluation of the penalized function on the grid\n        F_values = F_penalized(xx, yy, r)\n\n        spurious_minima_count = 0\n\n        # 3. Local Minimum Detection and Classification\n        # Iterate through the interior points of the grid.\n        for i in range(1, N - 1):\n            for j in range(1, N - 1):\n                center_val = F_values[i, j]\n\n                # Extract the 8 neighbors' values\n                neighbors = np.array([\n                    F_values[i - 1, j - 1], F_values[i - 1, j], F_values[i - 1, j + 1],\n                    F_values[i,     j - 1],                     F_values[i,     j + 1],\n                    F_values[i + 1, j - 1], F_values[i + 1, j], F_values[i + 1, j + 1]\n                ])\n\n                # Check the two conditions for a local minimum\n                is_le_all_neighbors = np.all(center_val = neighbors + eps_lm)\n                is_lt_one_neighbor = np.any(center_val  neighbors - eps_lm)\n\n                if is_le_all_neighbors and is_lt_one_neighbor:\n                    # This grid point is a local minimum. Now classify it.\n                    x_coord = x_range[j]\n                    y_coord = y_range[i]\n                    \n                    # 4. Feasibility Classification\n                    g_val = g_constraint(x_coord, y_coord)\n                    h_val = h_constraint(x_coord, y_coord)\n                    \n                    is_feasible = (np.abs(g_val) = tau) and (h_val = tau)\n                    \n                    if not is_feasible:\n                        spurious_minima_count += 1\n        \n        results.append(spurious_minima_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}