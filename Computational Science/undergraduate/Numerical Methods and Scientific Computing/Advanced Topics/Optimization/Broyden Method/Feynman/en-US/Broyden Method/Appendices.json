{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration of Broyden's method, let's focus on the first fundamental action: computing the next iterate from an initial guess. This exercise will guide you through calculating the step vector $s_0$ by solving the core linear system $B_0 s_0 = -F(x_0)$, and then using it to update your position from $x_0$ to $x_1$. Mastering this calculation is the essential first skill for implementing any quasi-Newton method .",
            "id": "2158069",
            "problem": "Consider the system of nonlinear equations given by $F(x) = 0$, where $x = (x_1, x_2)^T$ and the function $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$ is defined as:\n$$\nF(x) = \\begin{pmatrix} x_1^2 - x_2 + 1 \\\\ x_1 + x_2^2 - 3 \\end{pmatrix}\n$$\nYou are to perform a single step of the secant-updating quasi-Newton method known as Broyden's method to find an improved approximation of the root. Starting with the initial guess $x_0 = (1, 1)^T$ and using the initial approximation of the Jacobian matrix\n$$\nB_0 = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix},\n$$\ncalculate the next iterate, $x_1$. Express your answer as a vector of two components.",
            "solution": "We are given the system $F(x) = 0$ with $F: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ defined by\n$$\nF(x) = \\begin{pmatrix} x_{1}^{2} - x_{2} + 1 \\\\ x_{1} + x_{2}^{2} - 3 \\end{pmatrix},\n$$\nan initial guess $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and an initial Jacobian approximation\n$$\nB_{0} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}.\n$$\nIn Broyden’s method, the next iterate $x_{1}$ is computed by first solving the linear system\n$$\nB_{0} s_{0} = -F(x_{0}),\n$$\nand then updating\n$$\nx_{1} = x_{0} + s_{0}.\n$$\n\nFirst evaluate $F(x_{0})$:\n$$\nF(x_{0}) = \\begin{pmatrix} 1^{2} - 1 + 1 \\\\ 1 + 1^{2} - 3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nSolve for $s_{0}$ using $B_{0} s_{0} = -F(x_{0})$. Since $B_{0} = 2 I$, we have\n$$\ns_{0} = B_{0}^{-1}\\big(-F(x_{0})\\big) = \\frac{1}{2} I \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\nThen,\n$$\nx_{1} = x_{0} + s_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}.\n$$\nThus the next iterate is the vector with components $\\frac{1}{2}$ and $\\frac{3}{2}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Having calculated a new iterate $x_1$, the next task is to update our knowledge of the system's local behavior by refining the approximate Jacobian. This practice focuses on applying the \"good\" Broyden update formula, which intelligently incorporates the information from the most recent step to generate $B_1$ from $B_0$. This exercise completes a full cycle of the Broyden iteration and reveals the mechanism that allows the method to \"learn\" about the function as it progresses .",
            "id": "2158053",
            "problem": "In the process of numerically solving a system of nonlinear equations $F(x) = 0$, where $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$, a quasi-Newton method is employed. The initial guess for the root is the vector $x_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. At this point, the function evaluates to $F(x_0) = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$.\n\nThe initial approximation for the Jacobian matrix is chosen to be the 2x2 identity matrix, $B_0 = I$. After the first step of the iterative method, the new point is found to be $x_1 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$, with the corresponding function value $F(x_1) = \\begin{pmatrix} 0.5 \\\\ 0.5 \\end{pmatrix}$.\n\nUsing the provided information, calculate the updated Jacobian approximation, $B_1$, by applying a single step of Broyden's \"good\" method. Express your answer as a 2x2 matrix.",
            "solution": "We apply Broyden's \"good\" update, which enforces the secant equation $B_{1}s_{0}=y_{0}$ and minimizes the change in $B$ in the Frobenius norm. The update formula is\n$$\nB_{1}=B_{0}+\\frac{\\left(y_{0}-B_{0}s_{0}\\right)s_{0}^{T}}{s_{0}^{T}s_{0}}.\n$$\nFrom the data, compute the step and function difference:\n$$\ns_{0}=x_{1}-x_{0}=\\begin{pmatrix}2 \\\\ -1\\end{pmatrix}-\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}=\\begin{pmatrix}1 \\\\ -2\\end{pmatrix},\\quad\ny_{0}=F(x_{1})-F(x_{0})=\\begin{pmatrix}\\tfrac{1}{2} \\\\ \\tfrac{1}{2}\\end{pmatrix}-\\begin{pmatrix}-1 \\\\ 2\\end{pmatrix}=\\begin{pmatrix}\\tfrac{3}{2} \\\\ -\\tfrac{3}{2}\\end{pmatrix}.\n$$\nWith $B_{0}=I$, we have\n$$\nB_{0}s_{0}=s_{0}=\\begin{pmatrix}1 \\\\ -2\\end{pmatrix},\\quad y_{0}-B_{0}s_{0}=\\begin{pmatrix}\\tfrac{3}{2} \\\\ -\\tfrac{3}{2}\\end{pmatrix}-\\begin{pmatrix}1 \\\\ -2\\end{pmatrix}=\\begin{pmatrix}\\tfrac{1}{2} \\\\ \\tfrac{1}{2}\\end{pmatrix},\n$$\nand\n$$\ns_{0}^{T}s_{0}=1^{2}+(-2)^{2}=5.\n$$\nForm the rank-one correction:\n$$\n\\frac{\\left(y_{0}-B_{0}s_{0}\\right)s_{0}^{T}}{s_{0}^{T}s_{0}}=\\frac{1}{5}\\begin{pmatrix}\\tfrac{1}{2} \\\\ \\tfrac{1}{2}\\end{pmatrix}\\begin{pmatrix}1 & -2\\end{pmatrix}\n=\\frac{1}{5}\\begin{pmatrix}\\tfrac{1}{2} & -1 \\\\ \\tfrac{1}{2} & -1\\end{pmatrix}\n=\\begin{pmatrix}\\tfrac{1}{10} & -\\tfrac{1}{5} \\\\ \\tfrac{1}{10} & -\\tfrac{1}{5}\\end{pmatrix}.\n$$\nTherefore,\n$$\nB_{1}=I+\\begin{pmatrix}\\tfrac{1}{10} & -\\tfrac{1}{5} \\\\ \\tfrac{1}{10} & -\\tfrac{1}{5}\\end{pmatrix}\n=\\begin{pmatrix}1+\\tfrac{1}{10} & -\\tfrac{1}{5} \\\\ \\tfrac{1}{10} & 1-\\tfrac{1}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\tfrac{11}{10} & -\\tfrac{1}{5} \\\\ \\tfrac{1}{10} & \\tfrac{4}{5}\\end{pmatrix}.\n$$\nA quick check shows $B_{1}s_{0}=\\begin{pmatrix}\\tfrac{3}{2} \\\\ -\\tfrac{3}{2}\\end{pmatrix}=y_{0}$, confirming the secant condition.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\tfrac{11}{10} & -\\tfrac{1}{5} \\\\ \\tfrac{1}{10} & \\tfrac{4}{5}\\end{pmatrix}}$$"
        },
        {
            "introduction": "We now move from the mechanics of a single iteration to a deeper, and perhaps surprising, property of the method's overall convergence. It is natural to assume that for the iterates $x_k$ to converge to a root $x^*$, the approximate Jacobians $B_k$ must converge to the true Jacobian $J(x^*)$, but this is not necessarily true. This advanced practice challenges you to implement Broyden's method and observe firsthand how it can successfully find a root even when its final Jacobian approximation remains different from the true Jacobian, illustrating a key reason for the method's efficiency .",
            "id": "3211914",
            "problem": "You are asked to design and implement a program that demonstrates a core phenomenon of Broyden's method for solving systems of nonlinear equations: for a nonlinear two-dimensional system, the sequence of approximate Jacobians, denoted by $B_k$, can converge to a matrix that differs from the true Jacobian $J(x^*)$ at the solution $x^*$, while the iterates $x_k$ still converge to the root. The fundamental base you must employ is the definition of solving $F(x) = 0$ for a vector-valued function $F:\\mathbb{R}^n \\to \\mathbb{R}^n$ via iterative methods, the Newton iteration using the Jacobian, and the secant condition that underpins quasi-Newton schemes such as Broyden's method. You must not assume any special formula beyond these core definitions within your reasoning when crafting the algorithm. Your program must implement the standard Broyden update that enforces the secant condition and seeks a minimal change to the approximate Jacobian in the Frobenius norm sense at each iteration.\n\nImplement a solver for a two-dimensional system (that is, $n = 2$) using Broyden's method with the following requirements:\n- Use the iteration $x_{k+1} = x_k + s_k$, where $s_k$ solves $B_k s_k = -F(x_k)$, with $B_k$ being the current approximation to the Jacobian. If $B_k$ is singular or nearly singular, use a least-squares step computed by a pseudoinverse or an equivalent stable fallback that remains within the rules of standard numerical linear algebra. Use a simple backtracking strategy on the step length to ensure that the sequence $\\|F(x_k)\\|_2$ is not increasing.\n- Update $B_k$ at each step using the classic (so-called “good”) Broyden formula that enforces the most recent secant equation and produces the smallest change in $B_k$ in the Frobenius norm among all rank-one updates that satisfy the secant condition. Do not hard-code any special-case algebraic simplifications tied to a particular $F$.\n- Terminate when either $\\|F(x_k)\\|_2 \\le \\varepsilon_f$ or $\\|s_k\\|_2 \\le \\varepsilon_s$, with prescribed tolerances $\\varepsilon_f$ and $\\varepsilon_s$, or when a maximum number of iterations $N_{\\max}$ is reached. All norms are Euclidean norms for vectors and Frobenius norms for matrices.\n\nYou will test your implementation on the following small test suite, each with fully specified data and an exact root to compare against. There are no physical units involved in this task. All angles, if any were to appear, would be in radians, but no trigonometric functions of dimensional angles are required here.\n\nDefine the nonlinear system $F_{\\mathrm{nl}}:\\mathbb{R}^2 \\to \\mathbb{R}^2$ by\n$$\nF_{\\mathrm{nl}}(x) =\n\\begin{bmatrix}\nx_1^2 + x_2 - 1 \\\\\nx_1 + x_2^2 - 1\n\\end{bmatrix},\n$$\nwith analytic Jacobian\n$$\nJ_{\\mathrm{nl}}(x) =\n\\begin{bmatrix}\n2 x_1 & 1 \\\\\n1 & 2 x_2\n\\end{bmatrix}.\n$$\nThis system has an exact root at $x^*_{\\mathrm{nl}} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n\nDefine the linear system $F_{\\mathrm{lin}}:\\mathbb{R}^2 \\to \\mathbb{R}^2$ by\n$$\nF_{\\mathrm{lin}}(x) = A x - b, \\quad A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix},\n$$\nwith constant Jacobian $J_{\\mathrm{lin}}(x) \\equiv A$ and exact root $x^*_{\\mathrm{lin}} = A^{-1} b$.\n\nYour program must run these three test cases in order, using Broyden's method as specified:\n- Test $1$ (happy path, nonlinear): Use $F_{\\mathrm{nl}}$, initial guess $x_0 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}$, initial approximate Jacobian $B_0 = I_2$ (the $2 \\times 2$ identity), residual tolerance $\\varepsilon_f = 10^{-10}$, step tolerance $\\varepsilon_s = 10^{-12}$, and maximum iterations $N_{\\max} = 200$. After termination, compute two outputs:\n  - $d_1 = \\|x_{\\mathrm{final}} - x^*_{\\mathrm{nl}}\\|_2$,\n  - $e_1 = \\|B_{\\mathrm{final}} - J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})\\|_F$.\n- Test $2$ (boundary, start at the root): Use $F_{\\mathrm{nl}}$, initial guess $x_0 = x^*_{\\mathrm{nl}} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, initial approximate Jacobian $B_0 = 2 I_2$, tolerances $\\varepsilon_f = 10^{-10}$, $\\varepsilon_s = 10^{-12}$, and maximum iterations $N_{\\max} = 200$. Since the starting point is the exact root, the method should terminate immediately without updates. Compute:\n  - $d_2 = \\|x_{\\mathrm{final}} - x^*_{\\mathrm{nl}}\\|_2$,\n  - $e_2 = \\|B_{\\mathrm{final}} - J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})\\|_F$.\n- Test $3$ (contrast with linear system): Use $F_{\\mathrm{lin}}$, initial guess $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, initial approximate Jacobian $B_0 = I_2$, tolerances $\\varepsilon_f = 10^{-12}$, $\\varepsilon_s = 10^{-12}$, and maximum iterations $N_{\\max} = 50$. Compute:\n  - $d_3 = \\|x_{\\mathrm{final}} - x^*_{\\mathrm{lin}}\\|_2$,\n  - $e_3 = \\|B_{\\mathrm{final}} - A\\|_F$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the six floating-point values in this exact order:\n$$\n[d_1, e_1, d_2, e_2, d_3, e_3].\n$$\nDo not include any spaces in the output. The values should be represented as decimal floating-point numbers (scientific notation is acceptable). No additional text should be printed.",
            "solution": "The problem requires the implementation of Broyden's method to solve systems of nonlinear equations, $F(x) = 0$, and to demonstrate a key property of the method: the sequence of approximate Jacobians, $B_k$, does not necessarily converge to the true Jacobian at the root, $J(x^*)$, even when the iterates $x_k$ converge to the root $x^*$.\n\n**1. Broyden's Method: Foundations**\n\nBroyden's method is a quasi-Newton method. Newton's method for solving $F(x)=0$ follows the iteration $x_{k+1} = x_k - J(x_k)^{-1} F(x_k)$, where $J(x_k)$ is the Jacobian matrix of $F$ at $x_k$. This requires computing and inverting the Jacobian at each step, which can be computationally expensive.\n\nQuasi-Newton methods replace the true Jacobian $J(x_k)$ with an approximation $B_k$. The iteration becomes:\n1. Solve the linear system $B_k s_k = -F(x_k)$ for the step $s_k$.\n2. Update the solution: $x_{k+1} = x_k + s_k$.\n\nThe core idea is to update $B_k$ to $B_{k+1}$ in a way that is computationally cheap and incorporates new information about the function $F$. The update is based on the secant condition, which is a multidimensional generalization of the secant method for root-finding in 1 dimension. The secant condition requires the new approximate Jacobian $B_{k+1}$ to correctly relate the change in $x$ to the change in $F$ for the most recent step:\n$$B_{k+1} s_k = y_k$$\nwhere $s_k = x_{k+1} - x_k$ and $y_k = F(x_{k+1}) - F(x_k)$.\n\n**2. The \"Good\" Broyden Update**\n\nThere are infinitely many matrices $B_{k+1}$ that satisfy the secant equation. Broyden's \"good\" method chooses the matrix $B_{k+1}$ that is closest to the previous approximation $B_k$ in the Frobenius norm, subject to the secant condition. This is a constrained optimization problem:\n$$\\min_{B} \\|B - B_k\\|_F \\quad \\text{subject to} \\quad B s_k = y_k$$\nThe solution to this problem is a rank-1 update to $B_k$:\n$$B_{k+1} = B_k + \\frac{(y_k - B_k s_k) s_k^T}{s_k^T s_k}$$\nThis formula forms the heart of the Jacobian approximation update in our implementation. The term $y_k - B_k s_k$ represents the discrepancy between the actual change in $F$ and the change predicted by the current approximation $B_k$. This discrepancy is used to \"correct\" $B_k$ in the direction of the step $s_k$.\n\n**3. Algorithmic Implementation**\n\nThe solver is structured as an iterative loop with the following key components at each iteration $k$:\n\n*   **Step Calculation**: The step $s_k$ is found by solving the linear system $B_k s_k = -F(x_k)$. For robustness against singular or ill-conditioned matrices $B_k$, which can occur during the iteration, a least-squares solver is employed. This is equivalent to using the Moore-Penrose pseudoinverse, finding the solution $s_k = -B_k^\\dagger F(x_k)$ that minimizes $\\|B_k s_k + F(x_k)\\|_2$. In NumPy, `numpy.linalg.lstsq` provides this functionality.\n\n*   **Line Search**: A full step $s_k$ might not lead to a decrease in the residual norm $\\|F(x)\\|_2$. To ensure robust convergence, a simple backtracking line search is used. We start with a step length $\\alpha=1$ and successively reduce it (e.g., by halving) until the condition $\\|F(x_k + \\alpha s_k)\\|_2 < \\|F(x_k)\\|_2$ is satisfied. This ensures that each step makes progress towards the solution. The final step taken is then $\\alpha s_k$.\n\n*   **Jacobian Update**: After taking the step $s_k$ which leads to $x_{k+1} = x_k + s_k$, the new Jacobian approximation $B_{k+1}$ is computed using the Broyden formula mentioned above. To avoid numerical instability, the update is skipped if the step norm $\\|s_k\\|_2$ is close to $0$, as this would involve division by a very small number.\n\n*   **Termination Conditions**: The iteration stops when one of the following criteria is met:\n    1. The norm of the residual is below a tolerance: $\\|F(x_k)\\|_2 \\le \\varepsilon_f$.\n    2. The norm of the step is below a tolerance: $\\|s_k\\|_2 \\le \\varepsilon_s$. This indicates that the iterates are no longer changing significantly.\n    3. A maximum number of iterations, $N_{\\max}$, is reached.\n\n**4. Analysis of Test Cases**\n\nThe algorithm is applied to three test cases to observe its behavior.\n\n*   **Test 1 (Nonlinear System)**: Starting from $x_0 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}$ and $B_0=I_2$, the method is applied to the nonlinear system $F_{\\mathrm{nl}}$. The iterates $x_k$ are expected to converge to the root $x^*_{\\mathrm{nl}} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, so the distance $d_1 = \\|x_{\\mathrm{final}} - x^*_{\\mathrm{nl}}\\|_2$ should be near $0$. However, as the iterates converge, the steps $s_k$ may become aligned in a specific direction (subspace concentration). The Broyden update only enforces the secant condition in these directions. Consequently, the components of $B_k$ corresponding to orthogonal directions are not refined. Thus, the final matrix $B_{\\mathrm{final}}$ may not equal the true Jacobian $J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})$, and the error $e_1 = \\|B_{\\mathrm{final}} - J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})\\|_F$ is expected to be non-zero. The true Jacobian at the root is $J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}}) = \\begin{bmatrix} 2 & 1 \\\\ 1 & 0 \\end{bmatrix}$.\n\n*   **Test 2 (Boundary Case)**: The method starts at the exact root, $x_0 = x^*_{\\mathrm{nl}}$. The initial residual $\\|F(x_0)\\|_2$ is $0$. The algorithm correctly identifies this and terminates on the first check, without performing any steps or updates. Therefore, $x_{\\mathrm{final}} = x_0$ and $B_{\\mathrm{final}} = B_0$. The distance $d_2$ will be $0$. The error $e_2 = \\|B_0 - J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})\\|_F$ will be non-zero, calculated as $\\|2I_2 - J_{\\mathrm{nl}}(x^*_{\\mathrm{nl}})\\|_F$.\n\n*   **Test 3 (Linear System)**: The method is applied to a linear system $F(x) = Ax-b$. For a linear system, the change in function value is $y_k = F(x_{k+1}) - F(x_k) = A(x_{k+1}-x_k) = As_k$. The Broyden update becomes $B_{k+1} = B_k + \\frac{(A-B_k)s_k s_k^T}{s_k^T s_k}$. This update has the property that $(B_{k+1}-A)s_k=0$. In $n$ dimensions, if the method generates $n$ linearly independent steps, $B_{n}$ will equal $A$. Broyden's method is known to terminate for linear systems in at most $2n$ iterations. For this 2-dimensional case, convergence should be very fast, and the final approximation $B_{\\mathrm{final}}$ is expected to be very close to the true (and constant) Jacobian $A$. Both errors $d_3$ and $e_3$ should be close to $0$.\n\nThis structured set of tests illuminates the fundamental behavior of Broyden's method in different scenarios, particularly highlighting the distinction between convergence of the iterates $x_k$ and convergence of the approximate Jacobians $B_k$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef broyden_solver(F, x0, B0, tol_f, tol_s, max_iter):\n    \"\"\"\n    Solves a system of nonlinear equations F(x) = 0 using Broyden's method.\n\n    Args:\n        F (callable): The vector-valued function to find a root of.\n        x0 (np.ndarray): The initial guess for the solution x.\n        B0 (np.ndarray): The initial guess for the Jacobian matrix.\n        tol_f (float): Tolerance for the norm of the residual F(x).\n        tol_s (float): Tolerance for the norm of the step s.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        tuple: A tuple (x, B) containing the final solution iterate and the\n               final approximate Jacobian.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    B = np.array(B0, dtype=float)\n    \n    Fx = F(x)\n    \n    for _ in range(max_iter):\n        # 1. Check termination on residual norm\n        if np.linalg.norm(Fx) <= tol_f:\n            break\n            \n        # 2. Solve for the step s_trial = -B_k^{-1} F(x_k)\n        # We use lstsq for robustness against singularity.\n        try:\n            s_trial = np.linalg.lstsq(B, -Fx, rcond=None)[0]\n        except np.linalg.LinAlgError:\n            # This is a fallback, but lstsq should generally not fail.\n            # If it does, the matrix is extremely ill-conditioned. Stop iteration.\n            break\n\n        # 3. Check termination on step size (before backtracking)\n        if np.linalg.norm(s_trial) <= tol_s:\n            break\n\n        # 4. Backtracking line search\n        alpha = 1.0\n        norm_Fx = np.linalg.norm(Fx)\n        x_next = x + alpha * s_trial\n        Fx_next = F(x_next)\n        \n        # Backtrack until the residual norm is not increasing. Limit to 10 steps.\n        for _ in range(10):\n            if np.linalg.norm(Fx_next) < norm_Fx:\n                break\n            alpha /= 2.0\n            x_next = x + alpha * s_trial\n            Fx_next = F(x_next)\n        else: # if loop finished without break, step is not productive\n            # Could indicate stalling, so we halt.\n            break\n\n        s = x_next - x\n        y = Fx_next - Fx\n        \n        # 5. Update Jacobian B using Broyden's \"good\" formula\n        # B_{k+1} = B_k + (y_k - B_k s_k)s_k^T / (s_k^T s_k)\n        s_norm_sq = np.dot(s, s)\n        if s_norm_sq > 1e-14: # Avoid division by zero for very small steps\n            update_vec = y - B @ s\n            B += np.outer(update_vec, s) / s_norm_sq\n            \n        # 6. Update x and Fx for the next iteration\n        x = x_next\n        Fx = Fx_next\n        \n    return x, B\n\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    # --- Define Systems and Exact Solutions ---\n    def F_nl(x):\n        return np.array([\n            x[0]**2 + x[1] - 1.0, \n            x[0] + x[1]**2 - 1.0\n        ])\n\n    def J_nl(x):\n        return np.array([\n            [2.0 * x[0], 1.0], \n            [1.0, 2.0 * x[1]]\n        ])\n    \n    A = np.array([[3.0, 1.0], [1.0, 2.0]])\n    b = np.array([1.0, 0.0])\n    def F_lin(x):\n        return A @ x - b\n\n    x_star_nl = np.array([1.0, 0.0])\n    x_star_lin = np.linalg.solve(A, b)\n\n    results = []\n\n    # --- Test Case 1: Nonlinear system, standard start ---\n    x0_1 = [0.5, 0.5]\n    B0_1 = np.identity(2)\n    tol_f_1, tol_s_1, N_max_1 = 1e-10, 1e-12, 200\n    \n    x_final_1, B_final_1 = broyden_solver(F_nl, x0_1, B0_1, tol_f_1, tol_s_1, N_max_1)\n    \n    d1 = np.linalg.norm(x_final_1 - x_star_nl)\n    J_star_nl = J_nl(x_star_nl)\n    e1 = np.linalg.norm(B_final_1 - J_star_nl, ord='fro')\n    results.extend([d1, e1])\n\n    # --- Test Case 2: Nonlinear system, start at root ---\n    x0_2 = x_star_nl\n    B0_2 = 2.0 * np.identity(2)\n    tol_f_2, tol_s_2, N_max_2 = 1e-10, 1e-12, 200\n\n    x_final_2, B_final_2 = broyden_solver(F_nl, x0_2, B0_2, tol_f_2, tol_s_2, N_max_2)\n    \n    d2 = np.linalg.norm(x_final_2 - x_star_nl)\n    e2 = np.linalg.norm(B_final_2 - J_star_nl, ord='fro')\n    results.extend([d2, e2])\n\n    # --- Test Case 3: Linear system ---\n    x0_3 = [0.0, 0.0]\n    B0_3 = np.identity(2)\n    tol_f_3, tol_s_3, N_max_3 = 1e-12, 1e-12, 50\n\n    x_final_3, B_final_3 = broyden_solver(F_lin, x0_3, B0_3, tol_f_3, tol_s_3, N_max_3)\n    \n    d3 = np.linalg.norm(x_final_3 - x_star_lin)\n    e3 = np.linalg.norm(B_final_3 - A, ord='fro')\n    results.extend([d3, e3])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}