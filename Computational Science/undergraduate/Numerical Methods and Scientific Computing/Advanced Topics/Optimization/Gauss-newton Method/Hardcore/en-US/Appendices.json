{
    "hands_on_practices": [
        {
            "introduction": "To truly understand an algorithm, it's often best to start by performing its steps by hand. This first practice grounds you in the fundamental mechanics of the Gauss-Newton method. By manually calculating a single iterative step for a simple one-parameter model, you will directly engage with the core components—the residual vector, the Jacobian matrix, and the parameter update—demystifying the process before automating it. ",
            "id": "2214282",
            "problem": "In an experimental study, a certain physical process is modeled by the function $y(x) = \\frac{x}{1+ax}$, where $a$ is an unknown parameter to be determined. A researcher has collected two data points $(x_i, y_i)$: the first point is $(1, 0.5)$ and the second point is $(2, 0.8)$.\n\nTo find the optimal value of the parameter $a$ that best fits the data in a least-squares sense, the researcher decides to use the Gauss-Newton method. Starting with an initial guess of $a_0 = 1$, perform exactly one iteration of the Gauss-Newton method to find the updated estimate for the parameter, denoted as $a_1$.\n\nExpress your answer for $a_1$ as an exact fraction in simplest form.",
            "solution": "We model the data by $y(x;a)=\\dfrac{x}{1+a x}$. For least squares with residuals $r_{i}(a)=y(x_{i};a)-y_{i}$, the Gauss-Newton update for a single parameter $a$ from $a_{0}$ is\n$$\n\\Delta a=-(J^{\\top}J)^{-1}J^{\\top}r,\n$$\nwhere $J_{i}=\\dfrac{\\partial r_{i}}{\\partial a}=\\dfrac{\\partial y(x_{i};a)}{\\partial a}$ evaluated at $a_{0}$, and $r$ is the residual vector evaluated at $a_{0}$. Then $a_{1}=a_{0}+\\Delta a$.\n\nFirst compute the derivative of the model with respect to $a$. Writing $y(x;a)=x(1+a x)^{-1}$, we obtain\n$$\n\\frac{\\partial y}{\\partial a}=-x^{2}(1+a x)^{-2}.\n$$\n\nWith data points $(x_{1},y_{1})=(1,\\tfrac{1}{2})$ and $(x_{2},y_{2})=(2,\\tfrac{4}{5})$, and initial guess $a_{0}=1$, the Jacobian entries are\n$$\nJ_{1}=\\left.-\\frac{x_{1}^{2}}{(1+a x_{1})^{2}}\\right|_{a=1}=-\\frac{1}{(1+1)^{2}}=-\\frac{1}{4},\\quad\nJ_{2}=\\left.-\\frac{x_{2}^{2}}{(1+a x_{2})^{2}}\\right|_{a=1}=-\\frac{4}{(1+2)^{2}}=-\\frac{4}{9}.\n$$\n\nThe residuals at $a_{0}=1$ are\n$$\nr_{1}=y(1;1)-\\frac{1}{2}=\\frac{1}{2}-\\frac{1}{2}=0,\\quad\nr_{2}=y(2;1)-\\frac{4}{5}=\\frac{2}{3}-\\frac{4}{5}=-\\frac{2}{15}.\n$$\n\nCompute the scalar quantities $J^{\\top}r$ and $J^{\\top}J$:\n$$\nJ^{\\top}r=J_{1}r_{1}+J_{2}r_{2}=0+\\left(-\\frac{4}{9}\\right)\\left(-\\frac{2}{15}\\right)=\\frac{8}{135},\n$$\n$$\nJ^{\\top}J=J_{1}^{2}+J_{2}^{2}=\\frac{1}{16}+\\frac{16}{81}=\\frac{81+256}{1296}=\\frac{337}{1296}.\n$$\n\nThus,\n$$\n\\Delta a=-\\frac{J^{\\top}r}{J^{\\top}J}=-\\frac{\\frac{8}{135}}{\\frac{337}{1296}}=-\\frac{8}{135}\\cdot\\frac{1296}{337}=-\\frac{8\\cdot 48}{5\\cdot 337}=-\\frac{384}{1685}.\n$$\n\nTherefore, the updated estimate is\n$$\na_{1}=a_{0}+\\Delta a=1-\\frac{384}{1685}=\\frac{1685-384}{1685}=\\frac{1301}{1685}.\n$$",
            "answer": "$$\\boxed{\\frac{1301}{1685}}$$"
        },
        {
            "introduction": "A full Gauss-Newton step is optimal for the local linear model, but it can overshoot the minimum of the true nonlinear objective, sometimes even increasing the error. This practice moves from a single manual calculation to building a complete, robust algorithm in code. You will implement a backtracking line search governed by the Armijo condition to ensure that each step makes progress, a crucial technique for guaranteeing convergence from starting points that are far from the solution. ",
            "id": "3232827",
            "problem": "Consider the nonlinear least-squares fitting of an exponential model to data. Let the parameter vector be $x = [a,b]^{\\top}$ and the model be $m(t;a,b) = a \\exp(b t)$. For a given set of observation times $\\{t_i\\}_{i=1}^m$ and observed values $\\{y_i\\}_{i=1}^m$, define the residual vector $r(x) \\in \\mathbb{R}^m$ by $r_i(x) = m(t_i;a,b) - y_i$, and define the objective function $\\phi(x) = \\tfrac{1}{2} \\sum_{i=1}^m r_i(x)^2$. The goal is to minimize $\\phi(x)$ with respect to $x$.\n\nStarting from the core definitions of nonlinear least squares, the residual function, and the Jacobian matrix $J(x) \\in \\mathbb{R}^{m \\times 2}$ with entries $J_{i1}(x) = \\partial r_i / \\partial a$ and $J_{i2}(x) = \\partial r_i / \\partial b$, derive the Gauss-Newton iteration from the first-order Taylor linearization of $r(x)$ around a current iterate $x_k$. Use this derivation to implement an algorithm that, at each iteration $k$, computes a Gauss-Newton search direction $p_k$ by solving the linear least-squares subproblem that arises from the linearized residuals. Because the local quadratic model can be inaccurate far from the solution, a full step $x_{k+1} = x_k + p_k$ can increase $\\phi(x)$; therefore, enforce a backtracking line search using the Armijo sufficient decrease condition. Specifically, for a trial step length $\\alpha \\in (0,1]$, require\n$$\n\\phi(x_k + \\alpha p_k) \\le \\phi(x_k) + c_1 \\alpha \\nabla \\phi(x_k)^{\\top} p_k,\n$$\nwith a fixed $c_1 \\in (0,1)$ and reduce $\\alpha$ by a constant factor in $(0,1)$ until the condition is satisfied. Use the gradient identity $\\nabla \\phi(x) = J(x)^{\\top} r(x)$ that follows from the chain rule and the definition of $\\phi$.\n\nYour program must implement the Gauss-Newton method with backtracking line search using only the definitions above and standard linear algebra operations, stopping when a reasonable stationarity condition holds or a maximum number of iterations is reached. Report both whether a full Gauss-Newton step increases the objective on the first iteration (as detected by the need for backtracking) and the final quality of fit.\n\nTest suite. Use the common dataset defined by\n- time points $\\{t_i\\}_{i=1}^6 = \\{0,1,2,3,4,5\\}$,\n- true parameters $a_{\\mathrm{true}} = 2.0$ and $b_{\\mathrm{true}} = 0.5$,\n- noiseless observations $y_i = a_{\\mathrm{true}} \\exp(b_{\\mathrm{true}} t_i)$ for $i \\in \\{1,\\dots,6\\}$.\n\nRun three test cases that differ only in the initial guess $x_0$:\n- Case $1$ (happy path): $x_0 = [1.8, 0.45]^{\\top}$, which is close to the true parameters.\n- Case $2$ (full-step increase expected): $x_0 = [0.1, -1.0]^{\\top}$, which is far from the true parameters and has the wrong sign for $b$.\n- Case $3$ (nearly singular Jacobian at start): $x_0 = [10^{-6}, 0.5]^{\\top}$, which makes the second Jacobian column initially very small.\n\nImplementation requirements:\n- Compute the Gauss-Newton direction $p_k$ by solving the linear least-squares subproblem that results from linearizing $r(x)$ at $x_k$.\n- Use a backtracking line search with parameters $c_1 = 10^{-4}$ and contraction factor $\\beta = 1/2$, starting each iteration with $\\alpha = 1$. If the Armijo condition is not met, reduce $\\alpha \\leftarrow \\beta \\alpha$ until it is met or a minimum step length $\\alpha_{\\min} = 10^{-8}$ is reached, in which case you should accept the current $\\alpha$ and proceed.\n- Terminate when $\\|\\nabla \\phi(x_k)\\|_{\\infty} \\le 10^{-8}$ or after $200$ iterations, whichever comes first.\n\nOutput specification:\n- For each test case, output a list of three values: the integer number of backtracking reductions applied during the first iteration only (zero if the full step was accepted), the integer total number of iterations performed until termination, and the final objective value $\\phi(x)$ rounded to six decimal places as a floating-point number.\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the order described above. For example, a valid output format is $[[n_1,i_1,f_1],[n_2,i_2,f_2],[n_3,i_3,f_3]]$.\n\nAngles are not involved in this problem. There are no physical units to report, and all numerical outputs must be unitless real numbers.",
            "solution": "The user-provided problem is valid. It is a well-posed, scientifically grounded problem in the field of numerical optimization, with a complete and consistent setup.\n\nThe problem requires the implementation of the Gauss-Newton method with a backtracking line search to solve a nonlinear least-squares problem. The goal is to fit the model $m(t; a, b) = a \\exp(b t)$ to a set of data points $\\{t_i, y_i\\}_{i=1}^m$. The parameter vector to be optimized is $x = [a, b]^{\\top}$.\n\nThe objective function to be minimized is the sum of squared residuals:\n$$\n\\phi(x) = \\frac{1}{2} \\sum_{i=1}^m r_i(x)^2 = \\frac{1}{2} r(x)^{\\top} r(x)\n$$\nwhere the residual vector $r(x)$ has components $r_i(x) = m(t_i; a, b) - y_i = a \\exp(b t_i) - y_i$.\n\nThe Gauss-Newton method is an iterative procedure that, at each iteration $k$, approximates the nonlinear problem with a linear one. Given a current iterate $x_k$, we seek a step $p_k$ such that $x_{k+1} = x_k + p_k$ improves the solution. The method is derived by forming a first-order Taylor series approximation of the residual vector $r(x)$ around $x_k$:\n$$\nr(x_k + p_k) \\approx r(x_k) + J(x_k) p_k\n$$\nHere, $J(x_k)$ is the Jacobian matrix of the residual vector $r(x)$ evaluated at $x_k$. Its entries are $J_{ij}(x) = \\frac{\\partial r_i}{\\partial x_j}$. For our specific model with $x=[a,b]^\\top$, the two columns of the Jacobian are:\n$$\n\\frac{\\partial r_i}{\\partial a} = \\frac{\\partial}{\\partial a} (a \\exp(b t_i) - y_i) = \\exp(b t_i)\n$$\n$$\n\\frac{\\partial r_i}{\\partial b} = \\frac{\\partial}{\\partial b} (a \\exp(b t_i) - y_i) = a t_i \\exp(b t_i)\n$$\nSo, the $i$-th row of the Jacobian $J(x_k)$ for $x_k = [a_k, b_k]^\\top$ is $[\\exp(b_k t_i), a_k t_i \\exp(b_k t_i)]$.\n\nSubstituting the linear approximation of $r(x_k + p_k)$ into the objective function $\\phi(x)$ gives a quadratic model of the objective function at $x_k$:\n$$\n\\phi(x_k + p_k) \\approx \\frac{1}{2} \\|r(x_k) + J(x_k) p_k\\|^2\n$$\nThe Gauss-Newton method determines the search direction $p_k$ by finding the $p_k$ that minimizes this quadratic approximation. This is a linear least-squares problem:\n$$\np_k = \\arg\\min_{p \\in \\mathbb{R}^2} \\|J(x_k) p - (-r(x_k))\\|^2\n$$\nThe solution to this subproblem is the Gauss-Newton direction. In practice, this system is solved using numerically stable techniques like QR factorization, as implemented in standard linear algebra libraries.\n\nA full step $x_{k+1} = x_k + p_k$ may not decrease the objective function $\\phi(x)$ if the iterate $x_k$ is far from the minimizer, where the linear approximation of $r(x)$ is inaccurate. To ensure convergence from a wider range of initial guesses, a backtracking line search is employed. The update becomes $x_{k+1} = x_k + \\alpha_k p_k$, where $\\alpha_k \\in (0, 1]$ is a step length. The line search finds a suitable $\\alpha_k$ by starting with $\\alpha = 1$ and reducing it until the Armijo sufficient decrease condition is met:\n$$\n\\phi(x_k + \\alpha p_k) \\le \\phi(x_k) + c_1 \\alpha \\nabla \\phi(x_k)^{\\top} p_k\n$$\nThe constant $c_1$ is a small number, given as $c_1 = 10^{-4}$. The gradient of the objective function, $\\nabla \\phi(x)$, is given by the identity $\\nabla \\phi(x) = J(x)^{\\top} r(x)$. The line search proceeds by reducing $\\alpha$ by a factor $\\beta \\in (0, 1)$, given as $\\beta = 1/2$, until the condition is satisfied or a minimum step length $\\alpha_{\\min} = 10^{-8}$ is reached.\n\nThe overall algorithm is as follows:\n$1$. Initialize $x_0$, set $k=0$, and define tolerance `tol`$=10^{-8}$ and `max_iter`$=200$.\n$2$. For $k=0, 1, \\dots, \\text{max\\_iter}-1$:\n    a. Evaluate $r_k = r(x_k)$, $J_k = J(x_k)$, and the gradient $\\nabla \\phi_k = J_k^{\\top} r_k$.\n    b. Check for convergence: if $\\|\\nabla \\phi_k\\|_{\\infty} \\le \\text{tol}$, terminate.\n    c. Solve the linear least-squares problem $\\min_p \\|J_k p + r_k\\|^2$ to find the Gauss-Newton direction $p_k$.\n    d. Perform a backtracking line search to find a step length $\\alpha_k$ satisfying the Armijo condition. Start with $\\alpha=1$ and decrease as $\\alpha \\leftarrow \\beta \\alpha$.\n    e. Update the parameters: $x_{k+1} = x_k + \\alpha_k p_k$.\n$3$. If the loop completes, the maximum number of iterations has been reached.\n\nThe implementation will apply this algorithm to the given dataset and three initial guesses, reporting statistics on the first-iteration behavior and overall convergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gauss_newton(x0, t, y, c1, beta, alpha_min, tol, max_iter):\n    \"\"\"\n    Solves a nonlinear least-squares problem using Gauss-Newton with backtracking.\n    \"\"\"\n    \n    # Helper functions for model, residual, objective, and jacobian\n    def model(x, t_vec):\n        a, b = x\n        # Handle potential overflow for large b\n        try:\n            with np.errstate(over='raise'):\n                 return a * np.exp(b * t_vec)\n        except FloatingPointError:\n            # Return an array of infinities with the correct sign\n            return np.full_like(t_vec, np.inf * np.sign(a))\n\n    def residual(x, t_vec, y_vec):\n        return model(x, t_vec) - y_vec\n\n    def objective(x, t_vec, y_vec):\n        res = residual(x, t_vec, y_vec)\n        # If model overflowed, residual is inf, dot product is inf\n        if np.isinf(res).any():\n            return np.inf\n        return 0.5 * np.dot(res, res)\n\n    def jacobian(x, t_vec):\n        a, b = x\n        jac = np.zeros((len(t_vec), 2), dtype=float)\n        try:\n            with np.errstate(over='raise'):\n                exp_bt = np.exp(b * t_vec)\n        except FloatingPointError:\n            exp_bt = np.full_like(t_vec, np.inf)\n\n        jac[:, 0] = exp_bt\n        jac[:, 1] = a * t_vec * exp_bt\n        return jac\n\n    x_k = np.array(x0, dtype=float)\n    k = 0\n    backtrack_count_first_iter = 0\n\n    while k  max_iter:\n        r_k = residual(x_k, t, y)\n        J_k = jacobian(x_k, t)\n        \n        # Check if Jacobian has inf/nan values due to overflow\n        if not np.all(np.isfinite(J_k)):\n            # This indicates a very bad parameter region, stop iteration\n            break\n            \n        grad_phi_k = J_k.T @ r_k\n\n        if np.max(np.abs(grad_phi_k)) = tol:\n            break\n\n        # Solve linear least squares subproblem for p_k: min ||J_k * p_k - (-r_k)||^2\n        try:\n            p_k = np.linalg.lstsq(J_k, -r_k, rcond=None)[0]\n        except np.linalg.LinAlgError:\n            # Failed to solve, likely singular matrix. Stop.\n            break\n\n        # Backtracking line search\n        alpha = 1.0\n        phi_k = 0.5 * np.dot(r_k, r_k)\n        dir_deriv = np.dot(grad_phi_k, p_k)\n        \n        n_backtrack = 0\n        while True:\n            phi_trial = objective(x_k + alpha * p_k, t, y)\n            \n            # Armijo condition\n            if phi_trial = phi_k + c1 * alpha * dir_deriv:\n                break\n            \n            if alpha = alpha_min:\n                break\n\n            alpha *= beta\n            n_backtrack += 1\n\n        if k == 0:\n            backtrack_count_first_iter = n_backtrack\n        \n        x_k += alpha * p_k\n        k += 1\n    \n    final_phi = objective(x_k, t, y)\n    \n    return [backtrack_count_first_iter, k, round(final_phi, 6)]\n\ndef solve():\n    # Dataset definition\n    t = np.array([0.0, 1.0, 2.0, 3.0, 4.0, 5.0])\n    a_true = 2.0\n    b_true = 0.5\n    y = a_true * np.exp(b_true * t)\n\n    # Algorithm parameters\n    c1 = 1e-4\n    beta = 0.5\n    alpha_min = 1e-8\n    tol = 1e-8\n    max_iter = 200\n\n    # Test cases\n    test_cases = [\n        [1.8, 0.45],   # Case 1: Close to solution\n        [0.1, -1.0],   # Case 2: Far from solution\n        [1e-6, 0.5]    # Case 3: Nearly singular Jacobian\n    ]\n    \n    results = []\n    for x0 in test_cases:\n        result = run_gauss_newton(x0, t, y, c1, beta, alpha_min, tol, max_iter)\n        results.append(result)\n\n    # Format output according to specification\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "The standard Gauss-Newton method can fail when faced with ill-posed problems, where parameters are not uniquely identifiable or are highly correlated, leading to a singular or ill-conditioned Jacobian matrix. This final practice introduces a powerful solution: Tikhonov regularization. You will derive and implement a regularized Gauss-Newton update that stabilizes the system, allowing you to find meaningful solutions even when the problem is inherently difficult, providing a glimpse into the principles behind widely used solvers like the Levenberg-Marquardt algorithm. ",
            "id": "3232783",
            "problem": "Consider the unconstrained nonlinear least squares problem with a parameter vector $\\theta \\in \\mathbb{R}^p$ and residual vector $r(\\theta) \\in \\mathbb{R}^m$, where each component $r_i(\\theta)$ is a smooth function of $\\theta$. Augment the objective with Tikhonov regularization to obtain the penalized objective $\\phi(\\theta) = \\tfrac{1}{2}\\|r(\\theta)\\|_2^2 + \\tfrac{1}{2}\\lambda\\|\\theta\\|_2^2$, where $\\lambda \\ge 0$ is a fixed regularization parameter. Starting from the definitions of nonlinear least squares and Tikhonov regularization, and using first-order Taylor linearization of the residual $r(\\theta + \\Delta)$ around a current iterate $\\theta$, derive the Gauss-Newton (GN) step for $\\Delta$ that results from minimizing the quadratic approximation of $\\phi(\\theta + \\Delta)$ with respect to $\\Delta$. Explain each step of the derivation, including how the Jacobian matrix $J(\\theta) = \\frac{\\partial r(\\theta)}{\\partial \\theta}$ enters, and justify why the resulting linear system is better conditioned when $\\lambda  0$ for ill-posed problems.\n\nThen, implement a GN algorithm that uses this regularized step to update $\\theta \\leftarrow \\theta + \\Delta$ until convergence, where convergence is declared when the step norm $\\|\\Delta\\|_2$ is below a tolerance. For concreteness, adopt the parametric model $F(x;\\theta) = \\theta_1 \\exp(\\theta_2 x)$, residuals $r_i(\\theta) = F(x_i;\\theta) - y_i$, and the Jacobian $J(\\theta)$ whose $i$-th row contains the partial derivatives with respect to $\\theta_1$ and $\\theta_2$, corresponding to the given $x_i$ values.\n\nYour program must implement this algorithm and evaluate it on the following test suite of parameter sets. In all cases, angles do not appear, and no physical units are involved. The program must compute, for each test case, the Euclidean norm $\\|r(\\theta^\\star)\\|_2$ of the final residual vector at convergence, expressed as a floating-point number.\n\nTest Suite:\n- Case 1 (well-posed, boundary $\\lambda = 0$): $x = [0.0, 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.0]$, $\\theta_{\\text{true}} = [2.0, -0.5]$, $y_i = \\theta_{1,\\text{true}} \\exp(\\theta_{2,\\text{true}} x_i)$, initial guess $\\theta^{(0)} = [1.0, 0.0]$, regularization parameter $\\lambda = 0.0$.\n- Case 2 (ill-posed, identifiability failure when $x=0$): $x = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$, $\\theta_{\\text{true}} = [1.5, 0.3]$, $y_i = \\theta_{1,\\text{true}} \\exp(\\theta_{2,\\text{true}} x_i)$ (constant values), initial guess $\\theta^{(0)} = [1.0, 1.0]$, regularization parameter $\\lambda = 0.5$.\n- Case 3 (ill-conditioned due to nearly collinear Jacobian columns): $x = [0.0, 0.004, 0.008, 0.012, 0.016, 0.02]$, $\\theta_{\\text{true}} = [2.0, 5.0]$, $y_i = \\theta_{1,\\text{true}} \\exp(\\theta_{2,\\text{true}} x_i)$, initial guess $\\theta^{(0)} = [1.0, 0.0]$, regularization parameter $\\lambda = 0.001$.\n\nAlgorithmic Details:\n- Use the residual $r(\\theta)$ and Jacobian $J(\\theta)$ corresponding to the model $F(x;\\theta) = \\theta_1 \\exp(\\theta_2 x)$.\n- At each iteration, update $\\theta$ by solving a linear system for the GN step $\\Delta$ derived from the quadratic approximation and move to $\\theta \\leftarrow \\theta + \\Delta$.\n- Terminate when $\\|\\Delta\\|_2 \\le \\text{tol}$ or when a maximum number of iterations is reached. Use a tolerance $\\text{tol} = 10^{-10}$ and a maximum of $50$ iterations.\n\nRequired Final Output Format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, in the order Case 1, Case 2, Case 3. For example, the format must be exactly like $[r_1,r_2,r_3]$, where each $r_k$ is the floating-point value of $\\|r(\\theta^\\star)\\|_2$ for the $k$-th case.",
            "solution": "The user has provided a well-defined problem in numerical optimization. The problem is valid as it is scientifically grounded in the principles of nonlinear least squares and regularization, is well-posed, objective, and contains all necessary information to derive and implement a solution.\n\n### Derivation of the Regularized Gauss-Newton Step\n\nThe objective is to find a parameter vector $\\theta \\in \\mathbb{R}^p$ that minimizes the penalized objective function $\\phi(\\theta)$, which consists of a sum-of-squares error term and a Tikhonov regularization term:\n$$ \\phi(\\theta) = \\frac{1}{2}\\|r(\\theta)\\|_2^2 + \\frac{1}{2}\\lambda\\|\\theta\\|_2^2 $$\nHere, $r(\\theta) \\in \\mathbb{R}^m$ is the residual vector, and $\\lambda \\ge 0$ is the regularization parameter. The Gauss-Newton method is an iterative algorithm that, at each step $k$, finds an update $\\Delta$ to move from the current estimate $\\theta^{(k)}$ to a new estimate $\\theta^{(k+1)} = \\theta^{(k)} + \\Delta$. For simplicity, we will drop the superscript $(k)$ and denote the current estimate as $\\theta$.\n\nThe step $\\Delta$ is found by minimizing a quadratic approximation of the objective function at $\\theta + \\Delta$. The core approximation of the Gauss-Newton method is to linearize the nonlinear residual function $r(\\theta + \\Delta)$ using its first-order Taylor expansion around $\\theta$:\n$$ r(\\theta + \\Delta) \\approx r(\\theta) + J(\\theta)\\Delta $$\nwhere $J(\\theta)$ is the $m \\times p$ Jacobian matrix of the residual vector, with entries $J_{ij} = \\frac{\\partial r_i}{\\partial \\theta_j}$.\n\nWe substitute this approximation into the penalized objective function $\\phi(\\theta + \\Delta)$. This yields a quadratic model for $\\phi(\\theta+\\Delta)$, which we denote as $\\Phi(\\Delta)$:\n$$ \\Phi(\\Delta) = \\frac{1}{2} \\|r(\\theta) + J(\\theta)\\Delta\\|_2^2 + \\frac{1}{2}\\lambda\\|\\theta + \\Delta\\|_2^2 $$\nTo find the step $\\Delta$ that minimizes this function, we expand the terms. Using vector notation where $r \\equiv r(\\theta)$ and $J \\equiv J(\\theta)$:\n$$ \\Phi(\\Delta) = \\frac{1}{2} (r + J\\Delta)^T(r + J\\Delta) + \\frac{1}{2}\\lambda (\\theta + \\Delta)^T(\\theta + \\Delta) $$\nExpanding the products gives:\n$$ \\Phi(\\Delta) = \\frac{1}{2} (r^T r + 2r^T J\\Delta + \\Delta^T J^T J\\Delta) + \\frac{1}{2}\\lambda (\\theta^T\\theta + 2\\theta^T\\Delta + \\Delta^T\\Delta) $$\nThis expression is a quadratic function of $\\Delta$. To find its minimum, we compute its gradient with respect to $\\Delta$ and set it to zero. Using the matrix calculus identities $\\nabla_x(a^Tx) = a$ and $\\nabla_x(x^TAx) = (A+A^T)x$:\n$$ \\nabla_{\\Delta} \\Phi(\\Delta) = \\frac{1}{2} (2J^T r + 2J^T J\\Delta) + \\frac{1}{2}\\lambda (2\\theta + 2I\\Delta) $$\n$$ \\nabla_{\\Delta} \\Phi(\\Delta) = (J^T r + J^T J\\Delta) + \\lambda(\\theta + I\\Delta) $$\nSetting the gradient to zero, $\\nabla_{\\Delta} \\Phi(\\Delta) = 0$:\n$$ J^T J\\Delta + J^T r + \\lambda\\theta + \\lambda I\\Delta = 0 $$\nRearranging the terms to solve for $\\Delta$, we group the terms involving $\\Delta$:\n$$ (J^T J + \\lambda I)\\Delta = -J^T r - \\lambda\\theta $$\nThis is the linear system for the regularized Gauss-Newton step $\\Delta$. The matrix on the left-hand side is an approximation to the Hessian of $\\phi(\\theta)$, and the right-hand side is the negative of the gradient of $\\phi(\\theta)$.\n\n### Justification for Regularization in Ill-Posed Problems\n\nThe standard, unregularized Gauss-Newton step is found by solving the normal equations, which are obtained by setting $\\lambda=0$ in the derived system:\n$$ J^T J \\Delta = -J^T r $$\nFor many problems, the matrix $J^T J$ is singular or ill-conditioned.\n1.  **Singularity**: If the columns of the Jacobian matrix $J$ are linearly dependent, $J$ is rank-deficient. This implies that $J^T J$ is singular (not invertible), as $\\text{rank}(J^T J) = \\text{rank}(J)$. In this case, the linear system for $\\Delta$ does not have a unique solution. This occurs in problems with parameter non-identifiability, such as Case 2 where the model's dependence on $\\theta_2$ vanishes for $x=0$.\n\n2.  **Ill-Conditioning**: If the columns of $J$ are nearly linearly dependent, the matrix $J^T J$ is ill-conditioned. Its condition number (the ratio of its largest to smallest eigenvalue, $\\kappa = \\mu_{\\max}/\\mu_{\\min}$) is very large. This makes the solution for $\\Delta$ extremely sensitive to small perturbations in the right-hand side (e.g., from numerical error or noise in the data), leading to unstable and oscillating updates.\n\nThe Tikhonov regularization term $\\lambda I$ mitigates these issues. The matrix to be inverted is $A_\\lambda = J^T J + \\lambda I$.\n- The matrix $J^T J$ is always symmetric and positive semi-definite. Its eigenvalues, $\\mu_j$, are all non-negative ($\\mu_j \\ge 0$).\n- If $\\lambda  0$, the matrix $A_\\lambda = J^T J + \\lambda I$ has eigenvalues $\\nu_j = \\mu_j + \\lambda$. Since $\\lambda  0$, all $\\nu_j \\ge \\lambda  0$.\n- This guarantees that $A_\\lambda$ is strictly positive definite and therefore always invertible, ensuring a unique solution for $\\Delta$ exists.\n- Furthermore, the condition number of $A_\\lambda$ is $\\kappa(A_\\lambda) = \\frac{\\mu_{\\max}+\\lambda}{\\mu_{\\min}+\\lambda}$. Even if $\\mu_{\\min}$ is zero or very close to zero, the denominator is bounded below by $\\lambda$. This prevents the condition number from becoming infinite or excessively large, thereby stabilizing the solution of the linear system. The regularization effectively adds a penalty that biases the solution towards smaller parameter norms, resolving the ambiguities present in the ill-posed problem.\n\n### Algorithm and Model Specification\n\nThe algorithm iteratively solves for $\\Delta$ and updates $\\theta \\leftarrow \\theta + \\Delta$.\n- **Model**: $F(x;\\theta) = \\theta_1 \\exp(\\theta_2 x)$ with $\\theta = [\\theta_1, \\theta_2]^T$.\n- **Residuals**: $r_i(\\theta) = F(x_i; \\theta) - y_i = \\theta_1 \\exp(\\theta_2 x_i) - y_i$.\n- **Jacobian**: The $i$-th row of $J(\\theta)$ is $[\\frac{\\partial r_i}{\\partial \\theta_1}, \\frac{\\partial r_i}{\\partial \\theta_2}]$.\n  $$ \\frac{\\partial r_i}{\\partial \\theta_1} = \\exp(\\theta_2 x_i) $$\n  $$ \\frac{\\partial r_i}{\\partial \\theta_2} = \\theta_1 x_i \\exp(\\theta_2 x_i) $$\n  So, the $i$-th row is $[\\exp(\\theta_2 x_i), \\theta_1 x_i \\exp(\\theta_2 x_i)]$.\n- **Termination**: The iteration stops when the Euclidean norm of the step, $\\|\\Delta\\|_2$, falls below a tolerance $\\text{tol} = 10^{-10}$, or a maximum of $50$ iterations is reached.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gauss_newton(x, y, theta0, lam, tol, max_iter):\n    \"\"\"\n    Implements the regularized Gauss-Newton algorithm.\n    \"\"\"\n    theta = np.array(theta0, dtype=float)\n    p = len(theta0) # Number of parameters\n\n    for _ in range(max_iter):\n        # Unpack parameters for clarity\n        theta1, theta2 = theta\n\n        # 1. Calculate residual vector r\n        model_y = theta1 * np.exp(theta2 * x)\n        r = model_y - y\n\n        # 2. Calculate Jacobian matrix J\n        J1 = np.exp(theta2 * x)\n        J2 = theta1 * x * np.exp(theta2 * x)\n        J = np.stack((J1, J2), axis=1)\n\n        # 3. Form and solve the linear system for the step delta\n        # (J^T J + lambda*I) * delta = -(J^T r + lambda*theta)\n        JtJ = J.T @ J\n        I = np.identity(p)\n        A = JtJ + lam * I\n\n        Jtr = J.T @ r\n        b = -(Jtr + lam * theta)\n        \n        try:\n            delta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # If the matrix is singular, terminate.\n            # This can happen in the unregularized case (lambda=0).\n            break\n            \n        # 4. Update the parameters\n        theta = theta + delta\n\n        # 5. Check for convergence\n        step_norm = np.linalg.norm(delta)\n        if step_norm = tol:\n            break\n\n    # After loop, calculate final residual norm\n    final_model_y = theta[0] * np.exp(theta[1] * x)\n    final_r = final_model_y - y\n    final_residual_norm = np.linalg.norm(final_r)\n    \n    return final_residual_norm\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test cases for the regularized Gauss-Newton algorithm.\n    \"\"\"\n    # Algorithmic parameters\n    tol = 1e-10\n    max_iter = 50\n\n    # Test Case 1: well-posed, lambda = 0\n    x1 = np.linspace(0.0, 1.0, 10)\n    theta_true1 = np.array([2.0, -0.5])\n    y1 = theta_true1[0] * np.exp(theta_true1[1] * x1)\n    theta0_1 = [1.0, 0.0]\n    lam1 = 0.0\n\n    # Test Case 2: ill-posed, non-identifiability\n    x2 = np.zeros(6)\n    theta_true2 = np.array([1.5, 0.3])\n    y2 = theta_true2[0] * np.exp(theta_true2[1] * x2) # y2 will be a constant vector\n    theta0_2 = [1.0, 1.0]\n    lam2 = 0.5\n\n    # Test Case 3: ill-conditioned, near-collinear Jacobian\n    x3 = np.arange(6, dtype=float) * 0.004\n    theta_true3 = np.array([2.0, 5.0])\n    y3 = theta_true3[0] * np.exp(theta_true3[1] * x3)\n    theta0_3 = [1.0, 0.0]\n    lam3 = 0.001\n\n    test_cases = [\n        (x1, y1, theta0_1, lam1),\n        (x2, y2, theta0_2, lam2),\n        (x3, y3, theta0_3, lam3),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        x, y, theta0, lam = case\n        result = run_gauss_newton(x, y, theta0, lam, tol, max_iter)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}