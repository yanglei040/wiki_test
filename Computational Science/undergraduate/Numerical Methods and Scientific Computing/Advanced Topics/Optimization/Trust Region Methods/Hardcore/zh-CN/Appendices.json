{
    "hands_on_practices": [
        {
            "introduction": "信赖域方法的核心在于求解二次模型子问题。当模型是凸的（即海森矩阵 $H$ 对称正定）时，这个过程尤其具有启发性。这个练习将引导你从第一性原理出发，推导出在解位于信赖域边界时所必须满足的“长期方程”（secular equation），这是一个关于拉格朗日乘子 $\\lambda$ 的标量方程。通过解决这个问题，你将掌握精确求解信赖域子问题的基本数学机制。",
            "id": "3284949",
            "problem": "考虑一个信赖域子问题，其中包含一个对称正定 (SPD) 矩阵 $H \\in \\mathbb{R}^{n \\times n}$，梯度 $g \\in \\mathbb{R}^{n}$，以及信赖域半径 $\\Delta > 0$：最小化二次模型 $m(p) = g^{\\mathsf{T}} p + \\tfrac{1}{2} p^{\\mathsf{T}} H p$，约束条件为欧几里得范数约束 $\\|p\\| \\le \\Delta$。从基本原理出发（即优化问题的定义和约束优化的 Karush–Kuhn–Tucker 条件），推导一个关于单变量的标量方程，该方程刻画了在 SPD 情况下，边界解满足 $\\|p\\| = \\Delta$ 的情形。然后，仅使用基本的线性代数事实和 SPD 矩阵的性质，证明为何该标量方程对于 $\\lambda \\ge 0$ 恰好有一个解，并解释为何相应的残差函数是严格递减的。\n\n接下来，概述一种高效的求根策略，用于计算求解该标量方程的唯一拉格朗日乘子 $\\lambda \\ge 0$。你的概述应指明：\n- 如何使用关于 $H + \\lambda I$ 的线性求解来计算残差及其导数，\n- 一种保证收敛的安全全局化策略，\n- 如何获得能在实践中实现快速收敛的初始区间或迭代点。\n\n最后，对于以下具体实例：\n- $H = \\mathrm{diag}(3, 1)$，\n- $g = \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix}$，\n- $\\Delta = 2$，\n计算满足边界条件的唯一 $\\lambda \\ge 0$ 的精确值。提供 $\\lambda$ 的精确值作为最终答案。不要四舍五入。",
            "solution": "该问题已经过验证，被认定为有效。这是一个来自数值优化领域的、定义明确且具有科学依据的问题，所有必要的数据和定义均已提供。\n\n信赖域子问题是寻找一个步长 $p \\in \\mathbb{R}^{n}$，该步长在步长约束下最小化一个函数的二次模型。该问题可表述为：\n$$\n\\min_{p \\in \\mathbb{R}^{n}} m(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} H p \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta\n$$\n其中 $H \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定 (SPD) 矩阵，$g \\in \\mathbb{R}^{n}$ 是一个梯度向量，$\\Delta > 0$ 是信赖域半径。约束是关于欧几里得范数 $\\|p\\| = \\sqrt{p^{\\mathsf{T}}p}$ 的。\n\n为了推导刻画信赖域边界上解的方程，我们采用约束优化的 Karush–Kuhn–Tucker (KKT) 条件。不等式约束为 $g_c(p) = \\|p\\|^2 - \\Delta^2 \\le 0$。拉格朗日函数是：\n$$\nL(p, \\lambda) = m(p) + \\frac{\\lambda}{2} g_c(p) = g^{\\mathsf{T}} p + \\frac{1}{2} p^{\\mathsf{T}} H p + \\frac{\\lambda}{2} (p^{\\mathsf{T}}p - \\Delta^2)\n$$\n其中 $\\lambda$ 是拉格朗日乘子。引入因子 $\\frac{1}{2}$ 是为了方便。对于一个最小化子 $p^*$，KKT 条件为：\n1.  平稳性条件：$\\nabla_p L(p^*, \\lambda) = g + H p^* + \\lambda p^* = 0$。\n2.  原始可行性：$\\|p^*\\|^2 \\le \\Delta^2$。\n3.  对偶可行性：$\\lambda \\ge 0$。\n4.  互补松弛性：$\\lambda (\\|p^*\\|^2 - \\Delta^2) = 0$。\n\n问题要求刻画边界上的解，即 $\\|p^*\\| = \\Delta$。根据互补松弛性条件，这意味着 $\\lambda$ 可以为非负数，即 $\\lambda \\ge 0$。平稳性条件可以重排为：\n$$\n(H + \\lambda I) p^* = -g\n$$\n其中 $I$ 是单位矩阵。由于 $H$ 是 SPD 矩阵，其特征值 $\\mu_i$ 均为严格正数，即 $\\mu_i > 0$。对于任何 $\\lambda \\ge 0$，矩阵 $(H + \\lambda I)$ 的特征值为 $\\mu_i + \\lambda$，这些值也都是严格正数。因此，对于所有 $\\lambda \\ge 0$，$(H + \\lambda I)$ 也是 SPD 矩阵，从而可逆。因此，我们可以将解 $p^*$ 唯一地表示为 $\\lambda$ 的函数：\n$$\np(\\lambda) = -(H + \\lambda I)^{-1} g\n$$\n对于一个边界解，步长 $p(\\lambda)$ 必须满足约束条件 $\\|p(\\lambda)\\| = \\Delta$。将 $p(\\lambda)$ 的表达式代入，得到关于单变量 $\\lambda$ 的标量方程：\n$$\n\\|-(H + \\lambda I)^{-1} g\\| = \\Delta\n$$\n这就是基本方程，通常称为长期方程（secular equation），它刻画了信赖域边界上解的拉格朗日乘子 $\\lambda$。\n\n接下来，我们证明对于边界解，此方程在 $\\lambda \\ge 0$ 上有且仅有一个解。让我们分析函数 $\\phi(\\lambda) = \\|p(\\lambda)\\| = \\|(H + \\lambda I)^{-1} g\\|$ 在 $\\lambda \\ge 0$ 上的性质。分析其平方 $\\psi(\\lambda) = \\phi(\\lambda)^2 = \\|p(\\lambda)\\|^2$ 会更方便。令 $H = Q \\Lambda Q^{\\mathsf{T}}$ 为 $H$ 的谱分解，其中 $Q$ 是一个正交矩阵，$\\Lambda = \\mathrm{diag}(\\mu_1, \\ldots, \\mu_n)$ 是 $H$ 的正特征值构成的对角矩阵。我们可以将 $\\psi(\\lambda)$ 写为：\n$$\n\\psi(\\lambda) = g^{\\mathsf{T}} (H + \\lambda I)^{-2} g = g^{\\mathsf{T}} (Q (\\Lambda + \\lambda I) Q^{\\mathsf{T}})^{-2} g = g^{\\mathsf{T}} Q (\\Lambda + \\lambda I)^{-2} Q^{\\mathsf{T}} g\n$$\n令 $\\hat{g} = Q^{\\mathsf{T}} g$。则 $\\psi(\\lambda)$ 变为：\n$$\n\\psi(\\lambda) = \\hat{g}^{\\mathsf{T}} (\\Lambda + \\lambda I)^{-2} \\hat{g} = \\sum_{i=1}^{n} \\frac{\\hat{g}_i^2}{(\\mu_i + \\lambda)^2}\n$$\n为了分析此函数的单调性，我们计算它关于 $\\lambda$ 的导数：\n$$\n\\psi'(\\lambda) = \\sum_{i=1}^{n} \\hat{g}_i^2 \\frac{d}{d\\lambda} (\\mu_i + \\lambda)^{-2} = \\sum_{i=1}^{n} \\hat{g}_i^2 (-2)(\\mu_i + \\lambda)^{-3} (1) = -2 \\sum_{i=1}^{n} \\frac{\\hat{g}_i^2}{(\\mu_i + \\lambda)^3}\n$$\n由于 $\\mu_i > 0$ 且我们考虑 $\\lambda \\ge 0$，分母 $(\\mu_i + \\lambda)^3$ 总是正的。分子 $\\hat{g}_i^2$ 是非负的。如果 $g \\neq 0$，则 $\\hat{g} \\neq 0$，并且至少有一个 $\\hat{g}_i^2$ 是正的。因此，对于 $g \\neq 0$，$\\psi'(\\lambda) < 0$ 对所有 $\\lambda \\ge 0$ 成立。这证明了 $\\psi(\\lambda)$ 是在 $[0, \\infty)$ 上关于 $\\lambda$ 的严格递减函数。由于 $\\phi(\\lambda)=\\sqrt{\\psi(\\lambda)}$ 且平方根函数对正自变量是严格递增的，所以 $\\phi(\\lambda)$ 在 $[0, \\infty)$ 上也是关于 $\\lambda$ 的严格递减函数。\n\n解 $\\phi(\\lambda) = \\Delta$ 在 $\\lambda \\ge 0$ 上的存在性和唯一性取决于 $\\phi(0) = \\|-H^{-1}g\\|$ 的值，这是无约束最小化子（即完整牛顿步）的范数。\n- 如果 $\\| -H^{-1}g \\| \\le \\Delta$，则无约束解是可行的。KKT 条件由 $p^* = -H^{-1}g$ 和 $\\lambda = 0$ 满足。这是内部解。\n- 如果 $\\| -H^{-1}g \\| > \\Delta$，解必在边界上。我们分析函数 $\\phi(\\lambda)$ 在 $[0, \\infty)$ 上的性质。我们有 $\\phi(0) = \\|-H^{-1}g\\| > \\Delta$。\n在 $\\lambda \\to \\infty$ 的极限情况下，我们有 $\\phi(\\lambda) = \\|-(H+\\lambda I)^{-1}g\\| \\approx \\|\\frac{-1}{\\lambda}I g\\| = \\frac{\\|g\\|}{\\lambda} \\to 0$。\n由于 $\\phi(\\lambda)$ 是 $[0, \\infty)$ 上的一个连续且严格递减的函数，且 $\\phi(0) > \\Delta$ 和 $\\lim_{\\lambda \\to \\infty} \\phi(\\lambda) = 0$，根据介值定理，必然存在唯一的 $\\lambda^* > 0$ 使得 $\\phi(\\lambda^*) = \\Delta$。因此，在边界情况下，存在一个唯一的正拉格朗日乘子。\n\n针对长期方程的高效求根策略通常是在一个相关函数上使用牛顿法，例如 $r(\\lambda) = \\|p(\\lambda)\\| - \\Delta = 0$。\n牛顿迭代为 $\\lambda_{k+1} = \\lambda_k - r(\\lambda_k)/r'(\\lambda_k)$。\n为了计算 $r(\\lambda)$ 及其导数 $r'(\\lambda)$：\n1.  **计算 $r(\\lambda)$**：对于给定的 $\\lambda$，我们必须计算 $p(\\lambda) = -(H+\\lambda I)^{-1}g$。这可以通过求解线性系统 $(H+\\lambda I)p = -g$ 来完成。由于 $(H+\\lambda I)$ 是 SPD 矩阵，这个系统可以通过 Cholesky 分解高效求解。计算出 $p$ 后，我们再计算 $r(\\lambda) = \\|p\\| - \\Delta$。\n2.  **计算 $r'(\\lambda)$**：我们必须计算 $\\frac{d}{d\\lambda}\\|p(\\lambda)\\|$。\n    $r'(\\lambda) = \\frac{p(\\lambda)^{\\mathsf{T}} p'(\\lambda)}{\\|p(\\lambda)\\|}$，其中 $p'(\\lambda) = \\frac{d p(\\lambda)}{d\\lambda}$。\n    对等式 $(H + \\lambda I) p(\\lambda) = -g$ 关于 $\\lambda$ 求导，我们得到：\n    $I p(\\lambda) + (H + \\lambda I) p'(\\lambda) = 0 \\implies p'(\\lambda) = -(H + \\lambda I)^{-1} p(\\lambda)$。\n    令 $q = p'(\\lambda)$。为了计算 $q$，我们求解另一个线性系统 $(H + \\lambda I)q = -p(\\lambda)$。注意，计算 $p(\\lambda)$ 时得到的 $(H+\\lambda I)$ 的 Cholesky 分解可以被重用。\n    然后，$r'(\\lambda) = \\frac{p(\\lambda)^{\\mathsf{T}} q}{\\|p(\\lambda)\\|}$。\n一个牛顿步包含两次使用相同矩阵的线性求解。\n- **全局化策略**：牛顿法是局部二次收敛的，但可能不全局收敛。因此，必须有保障策略。由于我们知道根 $\\lambda^*$ 位于区间 $[0, \\infty)$ 内，且函数 $r(\\lambda)$ 是凸且单调的，简单的保障措施是有效的。例如，如果一个牛顿步 $\\lambda_{k+1}$ 是负数，可以用一个已知区间（如 $[\\lambda_k/2, \\lambda_k]$）上的二分步来代替。\n- **初始迭代点**：我们仅在 $\\|-H^{-1}g\\| > \\Delta$ 时需要解长期方程。在这种情况下，我们知道 $\\lambda^* > 0$。一个常见且简单的初始猜测是 $\\lambda_0 = 0$。由此，牛顿迭代将产生一个正的 $\\lambda_1$。更复杂的初始猜测可以从特征值界导出，例如 $\\lambda_0 = \\frac{\\|g\\|}{\\Delta} - \\mu_{\\min}$，尽管计算 $\\mu_{\\min}$ 可能代价高昂。在有保障的方案中从 $\\lambda_0=0$ 开始是稳健的。\n\n最后，我们为给定的具体实例计算 $\\lambda$ 的精确值：\n- $H = \\mathrm{diag}(3, 1)$\n- $g = \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix}$\n- $\\Delta = 2$\n\n首先，我们检查解是否在边界上。我们计算无约束最小化子 $p_{unc} = -H^{-1}g$。\n$H^{-1} = \\mathrm{diag}(1/3, 1)$。\n$p_{unc} = -\\begin{pmatrix} 1/3 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -10/3 \\\\ 0 \\end{pmatrix}$。\n该步长的范数是 $\\|p_{unc}\\| = \\sqrt{(-10/3)^2 + 0^2} = 10/3$。\n由于 $10/3 \\approx 3.33 > \\Delta = 2$，解位于边界上，我们必须找到一个 $\\lambda > 0$。\n\n我们求解长期方程 $\\|p(\\lambda)\\| = \\Delta$。步长 $p(\\lambda)$ 由下式给出：\n$p(\\lambda) = -(H + \\lambda I)^{-1} g$。\n$H + \\lambda I = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 3+\\lambda & 0 \\\\ 0 & 1+\\lambda \\end{pmatrix}$。\n$(H + \\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{3+\\lambda} & 0 \\\\ 0 & \\frac{1}{1+\\lambda} \\end{pmatrix}$。\n$p(\\lambda) = -\\begin{pmatrix} \\frac{1}{3+\\lambda} & 0 \\\\ 0 & \\frac{1}{1+\\lambda} \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{10}{3+\\lambda} \\\\ 0 \\end{pmatrix}$。\n\n现在，我们施加边界条件 $\\|p(\\lambda)\\| = 2$：\n$\\left\\| \\begin{pmatrix} -\\frac{10}{3+\\lambda} \\\\ 0 \\end{pmatrix} \\right\\| = 2$。\n$\\sqrt{\\left(-\\frac{10}{3+\\lambda}\\right)^2 + 0^2} = 2$。\n因为我们要找 $\\lambda > 0$，项 $3+\\lambda$ 是正的，所以我们可以写成：\n$\\frac{10}{3+\\lambda} = 2$。\n$10 = 2(3+\\lambda)$。\n$10 = 6 + 2\\lambda$。\n$4 = 2\\lambda$。\n$\\lambda = 2$。\n\n这就是满足边界条件的唯一的正拉格朗日乘子。",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "在理解了如何求解最优步长之后，一个自然的问题是：我们为什么需要这个步骤？这个思想实验将构建一个包含负曲率的场景，并展示最简单的下降步——柯西点（Cauchy point）——在这种情况下表现得非常糟糕。通过对比柯西点和子问题的全局最优解，你将直观地理解充分利用二次模型（尤其是其曲率信息）的重要性。",
            "id": "3284825",
            "problem": "考虑一个在无约束优化的信赖域 (TR) 方法中出现的二次模型。在当前迭代点，局部二阶模型为\n$$\nm(p) \\;=\\; f(x_k) \\;+\\; g^{\\top}p \\;+\\; \\frac{1}{2}p^{\\top}B\\,p,\n$$\n其中 $p \\in \\mathbb{R}^{2}$ 是步长，$g \\in \\mathbb{R}^{2}$ 是在 $x_k$ 处的梯度，$B \\in \\mathbb{R}^{2 \\times 2}$ 是一个对称的海森矩阵近似。TR 子问题是在信赖域约束 $\\|p\\| \\leq \\Delta$ 下最小化 $m(p)$。柯西点被定义为在射线 $p = -\\alpha g$ 上，满足约束 $\\|p\\| \\leq \\Delta$ 时 $m(p)$ 的最小化子。\n\n构建具有以下参数的特定二维实例\n$$\ng \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \n\\qquad\nB \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & -1000 \\end{pmatrix},\n\\qquad\n\\Delta \\;=\\; 1,\n$$\n并考虑相应的 TR 子问题。仅使用 TR 方法和二次模型的基本定义和原理，确定柯西点处的预测下降量与 TR 子问题全局解处的预测下降量之比 $\\rho$。步长 $p$ 处的预测下降量定义为 $m(0) - m(p)$。\n\n请以科学记数法提供 $\\rho$ 的最终数值，并四舍五入到四位有效数字。",
            "solution": "信赖域 (TR) 子问题旨在寻求\n$$\n\\min_{p \\in \\mathbb{R}^{2}} \\;\\; m(p) \\;=\\; g^{\\top}p + \\frac{1}{2}p^{\\top}B\\,p \n\\quad \\text{subject to} \\quad \\|p\\| \\leq \\Delta,\n$$\n其中 $f(x_k)$ 相对于 $p$ 是常数，因此在最小化过程中被省略。步长 $p$ 处的预测下降量为\n$$\n\\text{predicted reduction}(p) \\;=\\; m(0) - m(p) \\;=\\; -\\,g^{\\top}p \\;-\\; \\frac{1}{2}p^{\\top}B\\,p.\n$$\n\n给定\n$$\ng \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\n\\qquad\nB \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & -1000 \\end{pmatrix},\n\\qquad\n\\Delta \\;=\\; 1.\n$$\n我们首先计算柯西点，它在射线 $p=-\\alpha g$ 上最小化 $m(p)$，并满足约束 $\\|p\\| \\leq \\Delta$。将 $p$ 参数化为 $p(\\alpha) = -\\alpha g = \\begin{pmatrix} -\\alpha \\\\ 0 \\end{pmatrix}$，其中 $\\alpha \\geq 0$，约束为 $\\|p(\\alpha)\\| = \\alpha\\|g\\| \\leq \\Delta$。由于 $\\|g\\| = 1$，这变为 $\\alpha \\leq \\Delta = 1$。\n\n将 $m$ 限制在这条射线上，\n$$\nm(-\\alpha g) \\;=\\; g^{\\top}(-\\alpha g) + \\frac{1}{2}(-\\alpha g)^{\\top}B(-\\alpha g).\n$$\n我们计算 $g^{\\top}(-\\alpha g) = -\\alpha \\|g\\|^{2} = -\\alpha$ 以及\n$$\n(-\\alpha g)^{\\top}B(-\\alpha g) \\;=\\; \\alpha^{2} g^{\\top}B g \\;=\\; \\alpha^{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}^{\\top} \\begin{pmatrix} 1 & 0 \\\\ 0 & -1000 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\;=\\; \\alpha^{2} \\cdot 1 \\;=\\; \\alpha^{2}.\n$$\n因此，\n$$\nm(-\\alpha g) \\;=\\; -\\alpha + \\frac{1}{2}\\alpha^{2}.\n$$\n这是一个关于 $\\alpha$ 的一维二次函数；无约束最小化子满足\n$$\n\\frac{d}{d\\alpha}\\left(-\\alpha + \\frac{1}{2}\\alpha^{2}\\right) \\;=\\; -1 + \\alpha \\;=\\; 0\n\\quad \\Rightarrow \\quad \\alpha^{\\star} \\;=\\; 1.\n$$\n由于信赖域边界为 $\\alpha \\leq 1$，无约束最小化子位于边界上并且是可行的。因此，柯西点为\n$$\np_{\\mathrm{C}} \\;=\\; -\\alpha^{\\star} g \\;=\\; \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\n柯西点处的预测下降量为\n$$\n\\text{PR}_{\\mathrm{C}} \n\\;=\\; -\\,g^{\\top}p_{\\mathrm{C}} - \\frac{1}{2}p_{\\mathrm{C}}^{\\top}B\\,p_{\\mathrm{C}}\n\\;=\\; -\\,\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n\\;-\\; \\frac{1}{2}\\begin{pmatrix} -1 & 0 \\end{pmatrix}\\begin{pmatrix} 1 & 0 \\\\ 0 & -1000 \\end{pmatrix}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\n计算各项：\n$$\n-\\,g^{\\top}p_{\\mathrm{C}} \\;=\\; -(-1) \\;=\\; 1, \n\\qquad\np_{\\mathrm{C}}^{\\top}B\\,p_{\\mathrm{C}} \\;=\\; (-1)^{2}\\cdot 1 + 0^{2}\\cdot (-1000) \\;=\\; 1.\n$$\n因此，\n$$\n\\text{PR}_{\\mathrm{C}} \\;=\\; 1 - \\frac{1}{2}\\cdot 1 \\;=\\; \\frac{1}{2}.\n$$\n\n接下来，我们确定全局 TR 解。矩阵 $B$ 是不定的，其特征值为 $1$ 和 $-1000$。模型沿着第二个坐标方向具有强的负曲率。TR 子问题最小化\n$$\nm(p) \\;=\\; x + \\frac{1}{2}\\left(x^{2} - 1000\\,y^{2}\\right)\n$$\n在 $x^{2}+y^{2} \\leq 1$ 的 $(x,y)$ 上。对于任何可行的 $(x,y)$，项 $-\\frac{1}{2}\\cdot 1000\\,y^{2}$ 随着 $|y|$ 的增加而严格减小，这表明最小化子将在 $y$ 方向上达到信赖域边界。在边界 $x^{2}+y^{2}=1$ 上，项 $x + \\frac{1}{2}x^{2}$ 对模型值的贡献远小于 $-500y^2$。为了最大化模型下降，应选择 $|y|$ 最大的点，即 $x=0, y=\\pm 1$。因此，一个近似的全局最小化子是\n$$\np^{\\star} \\;=\\; \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\quad \\text{or} \\quad\np^{\\star} \\;=\\; \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix},\n$$\n由于对称性，两者产生相同的模型值。为具体起见，我们选择 $p^{\\star} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。全局 TR 解处的预测下降量为\n$$\n\\text{PR}_{\\star} \n\\;=\\; -\\,g^{\\top}p^{\\star} - \\frac{1}{2}(p^{\\star})^{\\top}B\\,p^{\\star}\n\\;=\\; -\\,\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n\\;-\\; \\frac{1}{2}\\begin{pmatrix} 0 & 1 \\end{pmatrix}\\begin{pmatrix} 1 & 0 \\\\ 0 & -1000 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\n计算各项：\n$$\n-\\,g^{\\top}p^{\\star} \\;=\\; -0 \\;=\\; 0,\n\\qquad\n(p^{\\star})^{\\top}B\\,p^{\\star} \\;=\\; 0^{2}\\cdot 1 + 1^{2}\\cdot (-1000) \\;=\\; -1000.\n$$\n因此，\n$$\n\\text{PR}_{\\star} \\;=\\; 0 - \\frac{1}{2}\\cdot(-1000) \\;=\\; 500.\n$$\n\n所要求的柯西点处的预测下降量与全局 TR 解处的预测下降量之比 $\\rho$ 为\n$$\n\\rho \\;=\\; \\frac{\\text{PR}_{\\mathrm{C}}}{\\text{PR}_{\\star}} \\;=\\; \\frac{\\frac{1}{2}}{500} \\;=\\; \\frac{1}{1000} \\;=\\; 10^{-3}.\n$$\n用科学记数法表示并四舍五入到四位有效数字，结果是\n$$\n\\rho \\;=\\; 1.000 \\times 10^{-3}.\n$$\n这表明，在这个二维实例中，TR 全局解位于边界上，并且相对于最优边界步长，柯西点产生的预测下降量非常差。",
            "answer": "$$\\boxed{1.000 \\times 10^{-3}}$$"
        },
        {
            "introduction": "现在，我们将理论付诸实践，解决现代优化中的一个重要挑战：逃离鞍点。这个编程练习将展示信赖域方法如何利用负曲率信息，在复杂的非凸函数曲面上航行，而传统的线搜索牛顿法则容易在鞍点附近停滞不前。通过这个实践，你将亲手实现并验证信赖域框架在处理非凸问题时的强大鲁棒性。",
            "id": "3284791",
            "problem": "您需要通过一个完整且可运行的程序，来演示配备了负曲率检测的信赖域方法如何比线搜索牛顿法更可靠地逃离一个综合非凸函数的鞍点区域。其教学背景是面向高等本科水平的数值方法和科学计算课程。您的程序必须从第一性原理出发实现这两种算法，仅使用这些方法所依据的、经过充分检验的定义和事实，并且必须将一个小型测试集的结果汇总为指定的输出格式。\n\n综合目标函数是一个双变量标量函数\n$$\nf(x,y) \\;=\\; x^3 \\;-\\; 3\\,x\\,y^2 \\;+\\; 0.1\\,(x^2+y^2)^2.\n$$\n该函数是非凸的，在原点附近具有鞍点行为，并受四次项的径向约束。从这个明确的定义出发，推导实现二阶优化方案所需的梯度和Hessian矩阵。使用二阶泰勒模型定义和曲率概念来证明算法决策的合理性。\n\n实现两种算法：\n\n- 一个信赖域方法，它在每次迭代中求解子问题\n$$\n\\min_{\\|s\\|\\le \\Delta}\\; m(s) \\;=\\; f(x) \\;+\\; \\nabla f(x)^\\top s \\;+\\; \\tfrac{1}{2}\\,s^\\top \\nabla^2 f(x)\\,s,\n$$\n其中 $\\Delta$ 是信赖域半径。该子问题必须使用Steihaug–Toint的截断共轭梯度法（Truncated Conjugate Gradient）来近似求解。该方法通过搜索方向 $d$ 上 $d^\\top \\nabla^2 f(x)\\,d$ 的符号来检测负曲率，并在遇到负曲率时，沿着该方向移动到信赖域的边界。步长必须根据实际减少量与预测减少量的比率来接受或拒绝。信赖域半径必须根据步长是否成功以及是否达到边界来进行调整。\n\n- 一个线搜索牛顿法，它在每次迭代中通过求解线性系统来构建牛顿方向\n$$\n\\nabla^2 f(x)\\,p \\;=\\; -\\nabla f(x),\n$$\n然后执行回溯线搜索，强制满足充分下降条件。如果Hessian矩阵不是正定的（通过其特征值判断）或该方向不是下降方向，则该方法在该次迭代中不应采取任何步长。这实现了一个标准的、朴素的线搜索牛顿方案，众所周知，该方案在Hessian矩阵不定或奇异的鞍点附近会遇到困难。\n\n使用基于梯度范数低于一个小的容差或达到固定的最大迭代次数的终止准则，以先到者为准。为了量化从鞍点区域的“逃离”，定义一个逃离准则：如果在迭代限制内，迭代点的欧几里得范数超过一个预定阈值，同时目标函数值相对于初始值减小，则标记为成功。\n\n测试集与参数：\n- 在函数 $f(x,y)$ 上实现这两种算法，从以下初始点开始（每对 $(x_0,y_0)$ 是一个独立的测试用例）：\n    - $(10^{-3},10^{-3})$\n    - $(10^{-4},-2\\times 10^{-3})$\n    - $(5\\times 10^{-3},0)$\n    - $(0,5\\times 10^{-3})$\n- 对于信赖域方法，使用初始信赖域半径 $\\Delta_0 = 10^{-2}$，最大半径 $\\Delta_{\\max} = 1$，接受阈值 $\\eta = 0.1$，以及梯度范数容差 $10^{-8}$。\n- 对于线搜索牛顿法，使用充分下降常数 $c_1 = 10^{-4}$，以步长 $1$ 开始线搜索，每次回溯尝试时将步长减半，如果步长低于 $10^{-8}$ 则停止回溯。如果Hessian矩阵不是正定的或该方向不是下降方向，则不采取步长。使用相同的梯度范数容差 $10^{-8}$。\n- 每种方法最多使用 $100$ 次迭代。\n- 逃离准则定义如下：如果最终迭代点 $(x_{\\text{final}},y_{\\text{final}})$ 满足 $\\sqrt{x_{\\text{final}}^2+y_{\\text{final}}^2} \\ge 0.2$ 且 $f(x_{\\text{final}},y_{\\text{final}}) < f(x_0,y_0)$，则认为该方法已逃离鞍点区域。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身是一个包含两个布尔值的列表，顺序为 $[\\text{trust\\_region\\_escaped},\\text{newton\\_escaped}]$。例如，四个测试用例的输出必须与以下形式完全一致\n$$\n[[\\text{True},\\text{False}],[\\text{True},\\text{False}],[\\text{True},\\text{False}],[\\text{True},\\text{False}]].\n$$\n程序的输出中不允许有任何额外的文本。",
            "solution": "所述问题是有效的。这是一个适定的、有科学依据的数值优化练习，要求在一个指定的非凸函数上实现并比较两种标准的二阶方法。其目标是展示与朴素的线搜索牛顿法相比，带有负曲率检测的信赖域方法在逃离鞍点区域方面具有更强的能力。所有参数、初始条件和评估标准都得到了明确且一致的定义。\n\n问题的核心在于目标函数在临界点附近的局部几何性质，以及不同算法如何响应这种性质。目标函数由下式给出\n$$\nf(x,y) = x^3 - 3xy^2 + 0.1(x^2+y^2)^2.\n$$\n为了实现二阶方法，我们必须首先推导梯度向量 $\\nabla f(x,y)$ 和Hessian矩阵 $\\nabla^2 f(x,y)$。梯度是一阶偏导数的向量：\n$$\n\\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 3x^2 - 3y^2 + 0.4x(x^2+y^2) \\\\ -6xy + 0.4y(x^2+y^2) \\end{pmatrix}.\n$$\nHessian矩阵是二阶偏导数的矩阵：\n$$\n\\nabla^2 f(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 6x + 1.2x^2 + 0.4y^2 & -6y + 0.8xy \\\\ -6y + 0.8xy & -6x + 0.4x^2 + 1.2y^2 \\end{pmatrix}.\n$$\n在原点 $(x,y) = (0,0)$，我们有 $\\nabla f(0,0) = \\mathbf{0}$ 和 $\\nabla^2 f(0,0) = \\mathbf{0}$。这是一个退化的临界点。对于原点附近的一个点 $(x,y)$，三次项 $x^3 - 3xy^2$ 主导了 $f(x,y)$ 的行为，而Hessian矩阵近似为 $\\nabla^2 f(x,y) \\approx \\begin{pmatrix} 6x & -6y \\\\ -6y & -6x \\end{pmatrix}$。该矩阵的行列式为 $-36x^2 - 36y^2 \\le 0$，特征值为 $\\pm 6\\sqrt{x^2+y^2}$，表明Hessian矩阵在原点的任何邻域（不包括原点本身）内都是不定的。这种不定性是鞍点区域的标志，在该区域中，函数在某些方向上增加，在其他方向上减少。一个有效的优化算法必须能够利用负曲率方向（与负特征值相关）来取得进展。\n\n信赖域（TR）方法旨在优雅地处理这种情况。在每次迭代 $k$ 中，给定一个点 $x_k$，它会构建函数的二次模型：\n$$\nm_k(s) = f(x_k) + \\nabla f(x_k)^\\top s + \\frac{1}{2}s^\\top \\nabla^2 f(x_k) s,\n$$\n其中 $s$ 是要采取的步长。该方法仅在某个半径 $\\Delta_k > 0$ 内信任此模型，从而引出子问题：\n$$\n\\min_{\\|s\\| \\le \\Delta_k} m_k(s).\n$$\nSteihaug-Toint截断共轭梯度（TCG）法是近似求解该子问题的一种有效方法。它将共轭梯度（CG）算法应用于线性系统 $\\nabla^2 f(x_k) s = -\\nabla f(x_k)$，但有两个关键的修改。首先，如果一个CG步会导致点超出信赖域，该步将被截断，使其正好落在边界 $\\|s\\| = \\Delta_k$ 上。其次，也是对本问题最重要的，在CG迭代期间，如果遇到非正曲率方向 $d$（即 $d^\\top \\nabla^2 f(x_k) d \\le 0$），算法将通过从当前CG迭代点沿着该方向 $d$ 移动到信赖域边界来生成一个步长。这一操作明确利用了负曲率信息来寻找一个更好的步长，该步长能进一步减小模型 $m_k(s)$，从而使算法能够有效逃离鞍点区域。在找到近似解 $s_k$ 后，计算 $f$ 的实际减少量与模型预测的减少量之比 $\\rho_k$。根据这个比率，步长被接受或拒绝，并为下一次迭代调整信赖域半径 $\\Delta_k$。\n\n相比之下，问题中指定的线搜索牛顿法是朴素的。在每次迭代 $k$ 中，它通过求解以下系统来计算牛顿方向 $p_k$：\n$$\n\\nabla^2 f(x_k) p_k = -\\nabla f(x_k).\n$$\n只有当Hessian矩阵 $\\nabla^2 f(x_k)$ 是正定的时候，这个方向对于二次模型才是最优的。问题规定，如果发现Hessian矩阵不是正定的（通过检查其特征值），或者产生的方向 $p_k$ 不是下降方向（即 $\\nabla f(x_k)^\\top p_k \\ge 0$），该方法就不能采取步长。在鞍点附近，Hessian矩阵是不定的，所以这个条件会经常满足。因此，该方法将会停滞不前，在许多迭代中无法更新其位置，也就无法逃离鞍点区域。如果找到了一个有效的下降方向，则执行回溯线搜索以找到一个步长 $\\alpha_k$，满足Armijo充分下降条件：$f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k$，其中 $c_1 \\in (0,1)$ 是一个常数。\n\n程序将实现这两种算法，并将其应用于靠近原点的初始点。当梯度范数低于 $10^{-8}$ 的容差或达到100次迭代的最大值时，终止迭代。逃离是通过检查最终迭代点 $x_{\\text{final}}$ 是否满足 $\\|x_{\\text{final}}\\|_2 \\ge 0.2$ 和 $f(x_{\\text{final}}) < f(x_0)$ 来量化的。信赖域方法预计将通过利用负曲率在所有情况下成功逃离，而朴素的牛顿法预计将失败。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the comparison and print results.\n    \"\"\"\n\n    # --- Objective Function, Gradient, and Hessian ---\n\n    def objective_function(p):\n        \"\"\"The synthetic nonconvex objective function f(x,y).\"\"\"\n        x, y = p\n        return x**3 - 3*x*y**2 + 0.1*(x**2 + y**2)**2\n\n    def gradient(p):\n        \"\"\"The gradient of f(x,y).\"\"\"\n        x, y = p\n        r_sq = x**2 + y**2\n        gx = 3*x**2 - 3*y**2 + 0.4*x*r_sq\n        gy = -6*x*y + 0.4*y*r_sq\n        return np.array([gx, gy])\n\n    def hessian(p):\n        \"\"\"The Hessian of f(x,y).\"\"\"\n        x, y = p\n        h_xx = 6*x + 1.2*x**2 + 0.4*y**2\n        h_yy = -6*x + 0.4*x**2 + 1.2*y**2\n        h_xy = -6*y + 0.8*x*y\n        return np.array([[h_xx, h_xy], [h_xy, h_yy]])\n\n    # --- Trust Region Method with Steihaug-Toint TCG ---\n\n    def truncated_cg(grad_f, hess_f, delta, tol=1e-10):\n        \"\"\"\n        Solves the trust-region subproblem using Steihaug-Toint TCG method.\n        Approximately solves min m(s) = g's + 1/2 s'Hs s.t. ||s|| = delta.\n        This corresponds to solving Hs = -g.\n        \"\"\"\n        s = np.zeros_like(grad_f)\n        r = -grad_f\n        p = r.copy()\n        \n        if np.linalg.norm(r)  tol:\n            return s\n        \n        # Max CG iterations can be capped at the dimension of the problem.\n        for _ in range(len(grad_f)):\n            pBp = p.T @ hess_f @ p\n\n            if pBp = 0:\n                # Negative curvature detected. Move to boundary.\n                a = p.T @ p\n                b = 2 * (s.T @ p)\n                c = s.T @ s - delta**2\n                discriminant = b**2 - 4*a*c\n                # Since s is in the trust region, a TC intersection is guaranteed\n                tau = (-b + np.sqrt(discriminant)) / (2*a)\n                return s + tau * p\n\n            alpha = (r.T @ r) / pBp\n            s_new = s + alpha * p\n\n            if np.linalg.norm(s_new) >= delta:\n                # Step goes outside boundary. Intersect with boundary.\n                a = p.T @ p\n                b = 2 * (s.T @ p)\n                c = s.T @ s - delta**2\n                discriminant = b**2 - 4*a*c\n                tau = (-b + np.sqrt(discriminant)) / (2*a)\n                return s + tau * p\n            \n            s = s_new\n            r_new = r - alpha * (hess_f @ p)\n\n            if np.linalg.norm(r_new)  tol:\n                break\n\n            beta = (r_new.T @ r_new) / (r.T @ r)\n            p = r_new + beta * p\n            r = r_new\n        return s\n\n    def trust_region_method(start_point, params):\n        \"\"\"Implements the trust region optimization algorithm.\"\"\"\n        x = np.array(start_point, dtype=float)\n        f_initial = objective_function(x)\n        delta = params['delta_0']\n\n        for _ in range(params['max_iter']):\n            grad = gradient(x)\n            if np.linalg.norm(grad)  params['grad_tol']:\n                break\n            \n            hess = hessian(x)\n            \n            s = truncated_cg(grad, hess, delta)\n            \n            ared = objective_function(x) - objective_function(x + s)\n            pred = -(grad.T @ s + 0.5 * s.T @ hess @ s)\n\n            # Avoid division by zero if pred is very small\n            if abs(pred)  1e-12:\n                rho = 0\n            else:\n                rho = ared / pred\n\n            if rho  0.25:\n                delta *= 0.25\n            else:\n                s_norm = np.linalg.norm(s)\n                if rho > 0.75 and np.isclose(s_norm, delta):\n                    delta = min(2 * delta, params['delta_max'])\n            \n            if rho > params['eta']:\n                x = x + s\n        \n        # Check escape criterion\n        escaped = (np.linalg.norm(x) >= 0.2) and (objective_function(x)  f_initial)\n        return escaped\n\n    # --- Line Search Newton Method ---\n\n    def line_search_newton(start_point, params):\n        \"\"\"Implements the naive line search Newton method.\"\"\"\n        x = np.array(start_point, dtype=float)\n        f_initial = objective_function(x)\n\n        for _ in range(params['max_iter']):\n            grad = gradient(x)\n            if np.linalg.norm(grad)  params['grad_tol']:\n                break\n\n            hess = hessian(x)\n            \n            # Check if Hessian is positive definite\n            try:\n                eigvals = np.linalg.eigvalsh(hess)\n                if np.min(eigvals) = 0:\n                    continue  # Do not take a step\n            except np.linalg.LinAlgError:\n                continue # Do not take a step\n\n            # Solve for Newton direction p\n            try:\n                p = np.linalg.solve(hess, -grad)\n            except np.linalg.LinAlgError:\n                continue # Do not take a step\n            \n            # Check for descent direction\n            if grad.T @ p >= 0:\n                continue # Do not take a step\n\n            # Backtracking line search\n            alpha = 1.0\n            f_x = objective_function(x)\n            descent_term = params['c1'] * grad.T @ p\n            \n            while True:\n                if objective_function(x + alpha * p) = f_x + alpha * descent_term:\n                    break\n                alpha /= 2.0\n                if alpha  1e-8:\n                    alpha = 0.0 # Failed to find a step\n                    break\n            \n            if alpha > 0.0:\n                x = x + alpha * p\n        \n        # Check escape criterion\n        escaped = (np.linalg.norm(x) >= 0.2) and (objective_function(x)  f_initial)\n        return escaped\n\n    # --- Test Suite and Main Execution ---\n\n    test_cases = [\n        (1e-3, 1e-3),\n        (1e-4, -2e-3),\n        (5e-3, 0),\n        (0, 5e-3)\n    ]\n    \n    tr_params = {\n        'delta_0': 1e-2,\n        'delta_max': 1.0,\n        'eta': 0.1,\n        'grad_tol': 1e-8,\n        'max_iter': 100\n    }\n\n    newton_params = {\n        'c1': 1e-4,\n        'grad_tol': 1e-8,\n        'max_iter': 100\n    }\n\n    results = []\n    for start_point in test_cases:\n        tr_escaped = trust_region_method(start_point, tr_params)\n        newton_escaped = line_search_newton(start_point, newton_params)\n        results.append([tr_escaped, newton_escaped])\n    \n    # Format the output string exactly as specified\n    outer_parts = []\n    for res_pair in results:\n        part = f\"[{str(res_pair[0])},{str(res_pair[1])}]\"\n        outer_parts.append(part)\n    final_string = f\"[{','.join(outer_parts)}]\"\n    \n    print(final_string)\n\nsolve()\n```"
        }
    ]
}