{
    "hands_on_practices": [
        {
            "introduction": "掌握牛顿法的核心在于理解其迭代步骤。这个练习将引导你完成一个基本但至关重要的计算：在给定当前点、梯度和Hessian矩阵的情况下，如何计算牛顿法的下一步迭代点。通过解决这个问题，你将熟悉在多维空间中应用牛顿法所涉及的基础矩阵运算，为后续更复杂的应用奠定坚实的计算基础。",
            "id": "2190699",
            "problem": "一个优化算法正被用来寻找函数 $f(\\mathbf{x})$ 的最小值，其中 $\\mathbf{x}$ 是一个二维向量。该算法当前位于点 $\\mathbf{x}_k = [3, -4]^T$。在该点，函数的梯度已计算为 $\\nabla f(\\mathbf{x}_k) = [2, -1]^T$，海森矩阵为 $H(\\mathbf{x}_k) = \\begin{pmatrix} 5  1 \\\\ 1  1 \\end{pmatrix}$。通过应用单步完整牛顿法，确定下一个点 $\\mathbf{x}_{k+1}$。将最终答案表示为行向量 $[x, y]$ 的形式，其中分量为精确分数。",
            "solution": "用于无约束优化的牛顿法的核心思想是：在当前点创建一个二次近似，然后移动到该近似的最小值点，从而迭代地找到函数的最小值。完整一步牛顿法的更新规则如下：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - [H(\\mathbf{x}_k)]^{-1} \\nabla f(\\mathbf{x}_k)\n$$\n其中 $\\mathbf{x}_k$ 是当前点，$\\nabla f(\\mathbf{x}_k)$ 是函数在该点的梯度，而 $H(\\mathbf{x}_k)$ 是在该点的海森矩阵。\n\n题目提供了以下信息：\n- 当前迭代点：$\\mathbf{x}_k = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$\n- 在 $\\mathbf{x}_k$ 处的梯度：$\\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$\n- 在 $\\mathbf{x}_k$ 处的海森矩阵：$H(\\mathbf{x}_k) = \\begin{pmatrix} 5  1 \\\\ 1  1 \\end{pmatrix}$\n\n首先，我们需要计算海森矩阵的逆矩阵 $H(\\mathbf{x}_k)^{-1}$。对于一个一般的 2x2 矩阵 $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆矩阵由以下公式给出：\n$$\nA^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}\n$$\n项 $ad-bc$ 是矩阵的行列式。让我们计算 $H(\\mathbf{x}_k)$ 的行列式：\n$$\n\\det(H(\\mathbf{x}_k)) = (5)(1) - (1)(1) = 5 - 1 = 4\n$$\n现在，我们可以求出海森矩阵的逆：\n$$\nH(\\mathbf{x}_k)^{-1} = \\frac{1}{4} \\begin{pmatrix} 1  -1 \\\\ -1  5 \\end{pmatrix} = \\begin{pmatrix} 1/4  -1/4 \\\\ -1/4  5/4 \\end{pmatrix}\n$$\n接下来，我们计算海森逆矩阵与梯度的乘积，这通常被称为牛顿步长方向 $\\mathbf{p}_k = -H(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k)$。\n$$\nH(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} 1/4  -1/4 \\\\ -1/4  5/4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\n执行矩阵-向量乘法：\n$$\nH(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} (1/4)(2) + (-1/4)(-1) \\\\ (-1/4)(2) + (5/4)(-1) \\end{pmatrix} = \\begin{pmatrix} 2/4 + 1/4 \\\\ -2/4 - 5/4 \\end{pmatrix} = \\begin{pmatrix} 3/4 \\\\ -7/4 \\end{pmatrix}\n$$\n现在我们可以更新点 $\\mathbf{x}_k$ 来找到 $\\mathbf{x}_{k+1}$：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - H(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix} - \\begin{pmatrix} 3/4 \\\\ -7/4 \\end{pmatrix}\n$$\n逐分量地执行向量减法：\n$$\n\\mathbf{x}_{k+1} = \\begin{pmatrix} 3 - 3/4 \\\\ -4 - (-7/4) \\end{pmatrix} = \\begin{pmatrix} 12/4 - 3/4 \\\\ -16/4 + 7/4 \\end{pmatrix} = \\begin{pmatrix} 9/4 \\\\ -9/4 \\end{pmatrix}\n$$\n题目要求答案以行向量 $[x, y]$ 的形式表示，且分量为精确分数。\n因此，下一个点是 $\\mathbf{x}_{k+1} = [9/4, -9/4]$。",
            "answer": "$$\\boxed{[9/4, -9/4]}$$"
        },
        {
            "introduction": "在学会如何计算牛顿步之后，探索其潜在的缺陷同样重要。这个练习展示了一个看似矛盾但极具启发性的场景：一个定义良好的牛顿步实际上可能导致目标函数值增加，从而使迭代点远离最小值。通过分析这个案例，你将深刻体会到纯牛顿法并非全局收敛，并理解为何必须引入线搜索等策略来保证算法的可靠性。",
            "id": "2190682",
            "problem": "在一个工程系统中，一个成本函数被用来基于两个可调参数 $p$ 和 $q$ 来量化其性能。目标是找到能使该成本最小化的参数对 $(p, q)$。该成本函数由下式给出：\n$$C(p, q) = (pq - 2)^2$$\n一个优化算法在参数设置 $(p_0, q_0) = (1, 1)$ 处进行初始化。为了找到最优参数，我们执行一次无约束优化的牛顿法迭代。计算这一单步迭代后的新参数设置 $(p_1, q_1)$。",
            "solution": "我们要对从 $(p_{0},q_{0})=(1,1)$ 开始的最小化问题 $C(p,q)=(pq-2)^{2}$ 进行一步牛顿法迭代。无约束优化的牛顿更新公式为\n$$\n\\begin{pmatrix} p_{1} \\\\ q_{1} \\end{pmatrix}\n=\n\\begin{pmatrix} p_{0} \\\\ q_{0} \\end{pmatrix}\n-\n\\left[\\nabla^{2} C(p_{0},q_{0})\\right]^{-1}\\nabla C(p_{0},q_{0}).\n$$\n首先，使用链式法则计算梯度。令 $r=pq-2$。则 $C=r^{2}$，所以\n$$\n\\frac{\\partial C}{\\partial p}=2r\\frac{\\partial r}{\\partial p}=2r\\,q=2q(pq-2),\\qquad\n\\frac{\\partial C}{\\partial q}=2r\\frac{\\partial r}{\\partial q}=2r\\,p=2p(pq-2).\n$$\n因此\n$$\n\\nabla C(p,q)=\\begin{pmatrix} 2q(pq-2) \\\\ 2p(pq-2) \\end{pmatrix}.\n$$\n接下来计算海森矩阵。对梯度分量求导：\n$$\n\\frac{\\partial^{2} C}{\\partial p^{2}}=\\frac{\\partial}{\\partial p}\\left(2q(pq-2)\\right)=2q\\frac{\\partial}{\\partial p}(pq-2)=2q^{2},\n$$\n$$\n\\frac{\\partial^{2} C}{\\partial q^{2}}=\\frac{\\partial}{\\partial q}\\left(2p(pq-2)\\right)=2p\\frac{\\partial}{\\partial q}(pq-2)=2p^{2},\n$$\n$$\n\\frac{\\partial^{2} C}{\\partial p\\,\\partial q}=\\frac{\\partial}{\\partial q}\\left(2q(pq-2)\\right)=2(pq-2)+2pq=4pq-4.\n$$\n因此\n$$\n\\nabla^{2} C(p,q)=\\begin{pmatrix} 2q^{2}  4pq-4 \\\\ 4pq-4  2p^{2} \\end{pmatrix}.\n$$\n在点 $(p_{0},q_{0})=(1,1)$ 处求值。此处 $pq-2=-1$，所以\n$$\n\\nabla C(1,1)=\\begin{pmatrix} 2\\cdot 1 \\cdot (-1) \\\\ 2\\cdot 1 \\cdot (-1) \\end{pmatrix}=\\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix},\n\\qquad\n\\nabla^{2} C(1,1)=\\begin{pmatrix} 2\\cdot 1^{2}  4\\cdot 1\\cdot 1 - 4 \\\\ 4\\cdot 1\\cdot 1 - 4  2\\cdot 1^{2} \\end{pmatrix}\n=\\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}.\n$$\n在点 (1,1) 处的海森矩阵的逆矩阵是\n$$\n\\left[\\nabla^{2} C(1,1)\\right]^{-1}=\\frac{1}{2}\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}.\n$$\n应用牛顿更新公式：\n$$\n\\begin{pmatrix} p_{1} \\\\ q_{1} \\end{pmatrix}\n=\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n-\n\\frac{1}{2}\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n-\n\\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}\n=\n\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}.\n$$\n因此，经过一步牛顿迭代后，参数为 $(p_{1},q_{1})=(2,2)$。",
            "answer": "$$\\boxed{[2, 2]}$$"
        },
        {
            "introduction": "理论知识最终要服务于实践。这个综合性的编程练习旨在将前述概念融会贯通，指导你构建一个在实际应用中更稳健、更高效的优化算法。你将通过实现一个混合算法来解决纯牛顿法的不足，该算法结合了梯度下降法的全局收敛性和牛顿法的快速局部收敛性。通过完成这个挑战，你将掌握Hessian矩阵正则化和回溯线搜索等关键技术，从而将理论知识转化为解决实际问题的代码实现。",
            "id": "3255777",
            "problem": "考虑一个二次连续可微函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$ 的无约束最小化问题。目标是设计并实现一个稳健的混合算法，该算法以梯度下降法（GD）开始，并在适当的时候切换到牛顿法（NM），以平衡全局收敛性与快速局部收敛性。此任务的基本依据是梯度 $\\nabla f(\\mathbf{x})$ 作为一阶偏导数向量的定义，海森矩阵 $\\nabla^2 f(\\mathbf{x})$ 作为二阶偏导数矩阵的定义，以及对于一个足够正则的函数，其无约束极小值点满足平稳点条件 $\\nabla f(\\mathbf{x}^\\star) = \\mathbf{0}$。\n\n算法要求：\n- 当梯度的欧几里得范数 $\\|\\nabla f(\\mathbf{x}_k)\\|_2$ 大于切换阈值 $\\tau$ 时，使用梯度下降法（GD）更新迭代点 $\\mathbf{x}_k$。\n- 当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\tau$ 时，切换到牛顿法（NM）。为确保即使在 $\\nabla^2 f(\\mathbf{x}_k)$ 不是正定时也能得到下降方向，应用对称正则化：通过加上一个单位矩阵的倍数来增广海森矩阵，该倍数根据海森矩阵的最小特征值选择。如果候选的牛顿方向不是一个下降方向，则回退到梯度下降方向。\n- 在梯度下降（GD）和牛顿法（NM）阶段，均使用带有 Armijo 充分下降准则的回溯线搜索（BLS）来选择步长。设充分下降常数为 $c = 10^{-4}$，缩减因子为 $\\beta = \\tfrac{1}{2}$。从初始步长 $1$ 开始线搜索。\n- 所有范数计算均使用欧几里得范数 $\\|\\cdot\\|_2$。\n- 当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\epsilon$（其中 $\\epsilon = 10^{-8}$）时，或达到最大迭代次数 $N_{\\max} = 1000$ 时终止。\n\n切换阈值和正则化：\n- 使用切换阈值 $\\tau = 10^{-1}$。\n- 对于牛顿法中的海森正则化，将 $\\lambda I$ 加到 $\\nabla^2 f(\\mathbf{x}_k)$ 上，其中选择 $\\lambda$ 以利用海森矩阵的最小特征值和一个小的缓冲值来确保正定性。使用缓冲值 $\\delta = 10^{-6}$。\n\n你的程序必须实现此算法，并在以下测试集上进行评估。对于每个测试用例，定义 $f$、其梯度 $\\nabla f$ 和其海森矩阵 $\\nabla^2 f$，并使用给定的初始点：\n- 测试用例 1（$n = 2$ 维的强凸二次函数）：\n  - $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} Q \\mathbf{x} + \\mathbf{b}^{\\top} \\mathbf{x} + c$，其中 $Q = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}$，$\\mathbf{b} = \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}$，$c = 0$。\n  - $\\nabla f(\\mathbf{x}) = Q \\mathbf{x} + \\mathbf{b}$。\n  - $\\nabla^2 f(\\mathbf{x}) = Q$。\n  - 初始点 $\\mathbf{x}_0 = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$。\n- 测试用例 2（$n = 2$ 维的 Rosenbrock 函数）：\n  - $f(\\mathbf{x}) = (1 - x_1)^2 + 100 \\left(x_2 - x_1^2\\right)^2$。\n  - $\\nabla f(\\mathbf{x}) = \\begin{bmatrix} -2(1 - x_1) - 400 x_1 \\left(x_2 - x_1^2\\right) \\\\ 200 \\left(x_2 - x_1^2\\right) \\end{bmatrix}$。\n  - $\\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix} 2 - 400 x_2 + 1200 x_1^2  -400 x_1 \\\\ -400 x_1  200 \\end{bmatrix}$。\n  - 初始点 $\\mathbf{x}_0 = \\begin{bmatrix} -1.2 \\\\ 1 \\end{bmatrix}$。\n- 测试用例 3（$n = 2$ 维的边界切换情况）：\n  - $f(\\mathbf{x}) = \\tfrac{1}{2} \\|\\mathbf{x}\\|_2^2$。\n  - $\\nabla f(\\mathbf{x}) = \\mathbf{x}$。\n  - $\\nabla^2 f(\\mathbf{x}) = I$。\n  - 初始点 $\\mathbf{x}_0 = \\begin{bmatrix} 0.1 \\\\ 0 \\end{bmatrix}$，因此 $\\|\\nabla f(\\mathbf{x}_0)\\|_2 = 0.1 = \\tau$。\n- 测试用例 4（$n = 2$ 维的带鞍点的非凸四次函数）：\n  - $f(\\mathbf{x}) = x_1^4 - x_1^2 + x_2^2$。\n  - $\\nabla f(\\mathbf{x}) = \\begin{bmatrix} 4 x_1^3 - 2 x_1 \\\\ 2 x_2 \\end{bmatrix}$。\n  - $\\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix} 12 x_1^2 - 2  0 \\\\ 0  2 \\end{bmatrix}$。\n  - 初始点 $\\mathbf{x}_0 = \\begin{bmatrix} 0.1 \\\\ 0 \\end{bmatrix}$。\n\n可量化的输出：\n- 对于每个测试用例，计算终止时的最终目标函数值 $f(\\mathbf{x}_{\\mathrm{final}})$，表示为四舍五入到六位小数的浮点数。\n\n最终输出格式：\n- 你的程序应生成一行输出，包含一个用方括号括起来的逗号分隔列表，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是对应测试用例的浮点结果，四舍五入到六位小数。这些计算不涉及角度，也不涉及物理单位。",
            "solution": "该问题要求设计并实现一种混合优化算法，用于二次连续可微函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的无约束最小化。该算法结合了梯度下降法（GD）的全局稳健性和牛顿法（NM）的快速局部收敛性。解是通过迭代更新估计值 $\\mathbf{x}_k$ 来获得的，目的是找到一个满足一阶必要条件 $\\nabla f(\\mathbf{x}^\\star) = \\mathbf{0}$ 的点 $\\mathbf{x}^\\star$。\n\n算法的核心是一个迭代过程 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$，其中 $\\mathbf{p}_k$ 是搜索方向，$\\alpha_k$ 是步长。选择 $\\mathbf{p}_k$ 的方法取决于迭代的状态，特别是梯度的大小。\n\n**1. 算法结构：混合切换策略**\n\n该算法根据梯度的欧几里得范数 $\\|\\nabla f(\\mathbf{x}_k)\\|_2$ 相对于预定义阈值 $\\tau$ 的大小，在两种模式之间切换。\n\n- **梯度下降模式**：如果 $\\|\\nabla f(\\mathbf{x}_k)\\|_2  \\tau$，则认为当前迭代点 $\\mathbf{x}_k$ 远离局部极小值。在此区域，梯度下降法的全局收敛特性更受青睐。搜索方向被设置为最速下降方向。\n- **牛顿法模式**：如果 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\tau$，则假定迭代点位于一个极小值点的邻域内，在该邻域中函数的局部二次近似是准确的。此时，算法切换到正则化的牛顿法，以利用其典型的快速（二次）收敛性。\n\n迭代过程持续进行，直到梯度范数低于容差 $\\epsilon = 10^{-8}$，表明已找到一个平稳点，或直到达到最大迭代次数 $N_{\\max} = 1000$。\n\n**2. 搜索方向计算**\n\n- **梯度下降方向**：在梯度下降（GD）模式下，搜索方向被选为负梯度：\n    $$\n    \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)\n    $$\n    对于任何非零梯度，该方向保证是一个下降方向，因为方向导数为 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)^\\top \\nabla f(\\mathbf{x}_k) = -\\|\\nabla f(\\mathbf{x}_k)\\|_2^2  0$。\n\n- **正则化牛顿法方向**：在牛顿法（NM）模式下，搜索方向基于 $f$ 在 $\\mathbf{x}_k$ 处的二阶泰勒展开。纯牛顿方向 $\\mathbf{p}_k^{\\text{N}}$ 最小化二次模型 $m_k(\\mathbf{p}) = f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\top \\nabla^2 f(\\mathbf{x}_k) \\mathbf{p}$，并通过求解以下线性系统得到：\n    $$\n    \\nabla^2 f(\\mathbf{x}_k) \\mathbf{p}_k^{\\text{N}} = -\\nabla f(\\mathbf{x}_k)\n    $$\n    只有当海森矩阵 $\\nabla^2 f(\\mathbf{x}_k)$ 是对称正定时，此步骤才是良定义的，并保证产生一个下降方向。为了处理海森矩阵非正定的情况（例如，在鞍点附近或非凸区域），采用了一种正则化技术。通过加上一个单位矩阵的倍数来修改海森矩阵：\n    $$\n    \\mathbf{H}_k^{\\text{reg}} = \\nabla^2 f(\\mathbf{x}_k) + \\lambda \\mathbf{I}\n    $$\n    必须选择正则化参数 $\\lambda$ 以确保 $\\mathbf{H}_k^{\\text{reg}}$ 是正定的。设 $\\lambda_{\\min}$ 是海森矩阵 $\\nabla^2 f(\\mathbf{x}_k)$ 的最小特征值。$\\mathbf{H}_k^{\\text{reg}}$ 的特征值是 $\\lambda_i + \\lambda$。为确保正则化后的海森矩阵的所有特征值都严格为正，我们要求 $\\lambda_{\\min} + \\lambda  0$。一个合适的选择是：\n    $$\n    \\lambda = \\max(0, -\\lambda_{\\min} + \\delta)\n    $$\n    其中 $\\delta = 10^{-6}$ 是一个小的正缓冲值，以确保严格正定性并避免数值问题。然后，正则化的牛顿方向 $\\mathbf{p}_k$ 通过求解以下良态线性系统得到：\n    $$\n    (\\nabla^2 f(\\mathbf{x}_k) + \\lambda \\mathbf{I}) \\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k)\n    $$\n    通过构造，这个 $\\mathbf{p}_k$ 是一个下降方向。作为一项安全措施，该实现包括一个回退机制：如果计算出的方向 $\\mathbf{p}_k$ 未能满足下降条件 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k  0$，算法在该次迭代中将回退到最速下降方向 $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$。\n\n**3. 步长选择：回溯线搜索**\n\n对于任何选定的下降方向 $\\mathbf{p}_k$，必须确定一个合适的步长 $\\alpha_k$ 以确保朝向极小值取得进展。固定的步长可能导致发散。带有 Armijo 条件的回溯线搜索为选择 $\\alpha_k$ 提供了一种稳健的方法。\n\n从初始试验步长 $\\alpha = 1$ 开始，该过程迭代检查是否满足充分下降条件：\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k\n$$\n其中 $c = 10^{-4}$ 是一个控制所需下降量的常数。项 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k$ 是 $f$ 在 $\\mathbf{x}_k$ 沿 $\\mathbf{p}_k$ 的方向导数。如果条件不满足，步长将乘以一个缩减因子 $\\beta = \\frac{1}{2}$，即 $\\alpha \\leftarrow \\beta \\alpha$，然后重复检查。此过程保证在有限次迭代后终止，并得到一个能使目标函数充分下降的步长。\n\n**4. 实现总结**\n\n最终的实现将此逻辑封装在一个函数中。它接受目标函数 $f$、其梯度 $\\nabla f$、其海森矩阵 $\\nabla^2 f$ 和一个初始点 $\\mathbf{x}_0$。该函数迭代执行梯度范数计算、模式切换、方向寻找（带有正则化和回退）和线搜索等步骤，直到满足终止准则。然后将此求解器应用于四个指定的测试用例中的每一个，以计算最终的目标函数值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the hybrid optimization algorithm.\n    \"\"\"\n\n    def hybrid_optimizer(f, grad_f, hess_f, x0, tau, epsilon, n_max, c, beta, delta):\n        \"\"\"\n        Implements the hybrid Gradient Descent / Newton's Method algorithm.\n        \n        Args:\n            f (callable): The objective function.\n            grad_f (callable): The gradient of the objective function.\n            hess_f (callable): The Hessian of the objective function.\n            x0 (np.ndarray): The initial point.\n            tau (float): The switching threshold for the gradient norm.\n            epsilon (float): The termination tolerance for the gradient norm.\n            n_max (int): The maximum number of iterations.\n            c (float): The Armijo condition constant.\n            beta (float): The backtracking line search contraction factor.\n            delta (float): The Hessian regularization buffer.\n\n        Returns:\n            float: The final objective function value.\n        \"\"\"\n        x = x0.copy().astype(float)\n        \n        for _ in range(n_max):\n            g = grad_f(x)\n            grad_norm = np.linalg.norm(g, 2)\n            \n            # Termination condition\n            if grad_norm = epsilon:\n                break\n            \n            # Mode selection: GD vs. NM\n            if grad_norm > tau:\n                # Gradient Descent mode\n                pk = -g\n            else:\n                # Newton's Method mode\n                H = hess_f(x)\n                \n                # Regularization to ensure positive definiteness\n                try:\n                    # Use eigvalsh for symmetric matrices\n                    eigvals = np.linalg.eigvalsh(H)\n                    lambda_min = eigvals[0]\n                    lam = max(0, -lambda_min + delta)\n                except np.linalg.LinAlgError:\n                    # Fallback if eigendecomposition fails\n                    lam = delta\n\n                H_reg = H + lam * np.identity(len(x))\n                \n                try:\n                    # Solve for Newton direction\n                    pk = np.linalg.solve(H_reg, -g)\n                    \n                    # Fallback to GD if not a descent direction (safety check)\n                    if g.T @ pk >= 0:\n                        pk = -g\n                except np.linalg.LinAlgError:\n                    # Fallback if solver fails\n                    pk = -g\n\n            # Backtracking Line Search (BLS)\n            alpha = 1.0\n            fx = f(x)\n            g_dot_p = g.T @ pk\n            \n            # Prevent infinite loop in BLS if direction is not descent\n            if g_dot_p >= 0:\n                # This should not happen if pk is a descent direction\n                # but as a safeguard, we stop if we can't find a step\n                break\n\n            while f(x + alpha * pk) > fx + c * alpha * g_dot_p:\n                alpha *= beta\n                if alpha  1e-16: # Practical limit for step size\n                    break\n            \n            # Update\n            x = x + alpha * pk\n            \n        return f(x)\n\n    # --- Define Test Cases ---\n\n    # Common parameters\n    params = {\n        'tau': 1e-1,\n        'epsilon': 1e-8,\n        'n_max': 1000,\n        'c': 1e-4,\n        'beta': 0.5,\n        'delta': 1e-6\n    }\n\n    # Test Case 1: Strongly convex quadratic\n    Q1 = np.array([[3., 1.], [1., 2.]])\n    b1 = np.array([-1., -2.])\n    f1 = lambda x: 0.5 * x.T @ Q1 @ x + b1.T @ x\n    grad_f1 = lambda x: Q1 @ x + b1\n    hess_f1 = lambda x: Q1\n    x0_1 = np.array([2., -1.])\n\n    # Test Case 2: Rosenbrock function\n    f2 = lambda x: (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n    grad_f2 = lambda x: np.array([\n        -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2),\n        200 * (x[1] - x[0]**2)\n    ])\n    hess_f2 = lambda x: np.array([\n        [2 - 400 * x[1] + 1200 * x[0]**2, -400 * x[0]],\n        [-400 * x[0], 200]\n    ])\n    x0_2 = np.array([-1.2, 1.])\n\n    # Test Case 3: Boundary switching case\n    f3 = lambda x: 0.5 * x.T @ x\n    grad_f3 = lambda x: x\n    hess_f3 = lambda x: np.identity(2)\n    x0_3 = np.array([0.1, 0.])\n\n    # Test Case 4: Nonconvex quartic\n    f4 = lambda x: x[0]**4 - x[0]**2 + x[1]**2\n    grad_f4 = lambda x: np.array([4 * x[0]**3 - 2 * x[0], 2 * x[1]])\n    hess_f4 = lambda x: np.array([[12 * x[0]**2 - 2, 0], [0, 2.]])\n    x0_4 = np.array([0.1, 0.])\n\n    test_cases = [\n        (f1, grad_f1, hess_f1, x0_1),\n        (f2, grad_f2, hess_f2, x0_2),\n        (f3, grad_f3, hess_f3, x0_3),\n        (f4, grad_f4, hess_f4, x0_4),\n    ]\n\n    results = []\n    for f_case, grad_f_case, hess_f_case, x0_case in test_cases:\n        final_f_val = hybrid_optimizer(f_case, grad_f_case, hess_f_case, x0_case, **params)\n        results.append(f\"{final_f_val:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}