{
    "hands_on_practices": [
        {
            "introduction": "To begin, we will practice the core computational step of Newton's method. This exercise provides the gradient $\\nabla f(\\mathbf{x}_k)$ and Hessian $H(\\mathbf{x}_k)$ directly, so you can focus on applying the update formula $\\mathbf{x}_{k+1} = \\mathbf{x}_k - [H(\\mathbf{x}_k)]^{-1} \\nabla f(\\mathbf{x}_k)$. Mastering this fundamental calculation is the first step toward understanding how Newton's method uses local curvature to find a minimum.",
            "id": "2190699",
            "problem": "An optimization algorithm is being used to find the minimum of a function $f(\\mathbf{x})$, where $\\mathbf{x}$ is a two-dimensional vector. The algorithm is currently at the point $\\mathbf{x}_k = [3, -4]^T$. At this point, the gradient of the function has been computed as $\\nabla f(\\mathbf{x}_k) = [2, -1]^T$, and the Hessian matrix is $H(\\mathbf{x}_k) = \\begin{pmatrix} 5  1 \\\\ 1  1 \\end{pmatrix}$. Determine the next point, $\\mathbf{x}_{k+1}$, by applying a single full step of Newton's method. Express your final answer as a row vector $[x, y]$ where the components are given as exact fractions.",
            "solution": "The core of Newton's method for unconstrained optimization is to iteratively find the minimum of a function by creating a quadratic approximation at the current point and then moving to the minimum of that approximation. The update rule for a full step of Newton's method is given by:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - [H(\\mathbf{x}_k)]^{-1} \\nabla f(\\mathbf{x}_k)\n$$\nwhere $\\mathbf{x}_k$ is the current point, $\\nabla f(\\mathbf{x}_k)$ is the gradient of the function at that point, and $H(\\mathbf{x}_k)$ is the Hessian matrix at that point.\n\nThe problem provides the following information:\n- The current iterate: $\\mathbf{x}_k = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$\n- The gradient at $\\mathbf{x}_k$: $\\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$\n- The Hessian at $\\mathbf{x}_k$: $H(\\mathbf{x}_k) = \\begin{pmatrix} 5  1 \\\\ 1  1 \\end{pmatrix}$\n\nFirst, we need to compute the inverse of the Hessian matrix, $H(\\mathbf{x}_k)^{-1}$. For a general 2x2 matrix $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is given by the formula:\n$$\nA^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}\n$$\nThe term $ad-bc$ is the determinant of the matrix. Let's calculate the determinant of $H(\\mathbf{x}_k)$:\n$$\n\\det(H(\\mathbf{x}_k)) = (5)(1) - (1)(1) = 5 - 1 = 4\n$$\nNow, we can find the inverse of the Hessian:\n$$\nH(\\mathbf{x}_k)^{-1} = \\frac{1}{4} \\begin{pmatrix} 1  -1 \\\\ -1  5 \\end{pmatrix} = \\begin{pmatrix} 1/4  -1/4 \\\\ -1/4  5/4 \\end{pmatrix}\n$$\nNext, we compute the product of the inverse Hessian and the gradient, which is often called the Newton step direction, $\\mathbf{p}_k = -H(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k)$.\n$$\nH(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} 1/4  -1/4 \\\\ -1/4  5/4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\nPerforming the matrix-vector multiplication:\n$$\nH(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} (1/4)(2) + (-1/4)(-1) \\\\ (-1/4)(2) + (5/4)(-1) \\end{pmatrix} = \\begin{pmatrix} 2/4 + 1/4 \\\\ -2/4 - 5/4 \\end{pmatrix} = \\begin{pmatrix} 3/4 \\\\ -7/4 \\end{pmatrix}\n$$\nNow we can update the point $\\mathbf{x}_k$ to find $\\mathbf{x}_{k+1}$:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - H(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix} - \\begin{pmatrix} 3/4 \\\\ -7/4 \\end{pmatrix}\n$$\nPerforming the vector subtraction component-wise:\n$$\n\\mathbf{x}_{k+1} = \\begin{pmatrix} 3 - 3/4 \\\\ -4 - (-7/4) \\end{pmatrix} = \\begin{pmatrix} 12/4 - 3/4 \\\\ -16/4 + 7/4 \\end{pmatrix} = \\begin{pmatrix} 9/4 \\\\ -9/4 \\end{pmatrix}\n$$\nThe problem asks for the answer to be expressed as a row vector $[x, y]$ with components as exact fractions.\nThus, the next point is $\\mathbf{x}_{k+1} = [9/4, -9/4]$.",
            "answer": "$$\\boxed{\\begin{bmatrix} \\frac{9}{4}  -\\frac{9}{4} \\end{bmatrix}}$$"
        },
        {
            "introduction": "Newton's method is powerful, but not foolproof. In this problem, you will apply a Newton step to a simple function and discover a surprising result: the step, while pointed in a descent direction, actually increases the objective function's value. This classic example of \"overshooting\"  perfectly illustrates why a full step is often too aggressive and motivates the need for step-size control mechanisms like a line search.",
            "id": "2190682",
            "problem": "In an engineering system, a cost function is used to quantify the performance based on two adjustable parameters, $p$ and $q$. The goal is to find the pair of parameters $(p, q)$ that minimizes this cost. The cost function is given by:\n$$C(p, q) = (pq - 2)^2$$\nAn optimization algorithm is initialized at the parameter setting $(p_0, q_0) = (1, 1)$. To find the optimal parameters, one iteration of Newton's method for unconstrained optimization is performed. Calculate the new parameter setting $(p_1, q_1)$ after this single step.",
            "solution": "We want one Newton step for minimizing $C(p,q)=(pq-2)^{2}$ starting at $(p_{0},q_{0})=(1,1)$. Newton's update for unconstrained optimization is\n$$\n\\begin{pmatrix} p_{1} \\\\ q_{1} \\end{pmatrix}\n=\n\\begin{pmatrix} p_{0} \\\\ q_{0} \\end{pmatrix}\n-\n\\left[\\nabla^{2} C(p_{0},q_{0})\\right]^{-1}\\nabla C(p_{0},q_{0}).\n$$\nFirst compute the gradient using the chain rule. Let $r=pq-2$. Then $C=r^{2}$, so\n$$\n\\frac{\\partial C}{\\partial p}=2r\\frac{\\partial r}{\\partial p}=2r\\,q=2q(pq-2),\\qquad\n\\frac{\\partial C}{\\partial q}=2r\\frac{\\partial r}{\\partial q}=2r\\,p=2p(pq-2).\n$$\nThus\n$$\n\\nabla C(p,q)=\\begin{pmatrix} 2q(pq-2) \\\\ 2p(pq-2) \\end{pmatrix}.\n$$\nNext compute the Hessian. Differentiate the gradient components:\n$$\n\\frac{\\partial^{2} C}{\\partial p^{2}}=\\frac{\\partial}{\\partial p}\\left(2q(pq-2)\\right)=2q\\frac{\\partial}{\\partial p}(pq-2)=2q^{2},\n$$\n$$\n\\frac{\\partial^{2} C}{\\partial q^{2}}=\\frac{\\partial}{\\partial q}\\left(2p(pq-2)\\right)=2p\\frac{\\partial}{\\partial q}(pq-2)=2p^{2},\n$$\n$$\n\\frac{\\partial^{2} C}{\\partial p\\,\\partial q}=\\frac{\\partial}{\\partial q}\\left(2q(pq-2)\\right)=2(pq-2)+2pq=4pq-4.\n$$\nTherefore\n$$\n\\nabla^{2} C(p,q)=\\begin{pmatrix} 2q^{2}  4pq-4 \\\\ 4pq-4  2p^{2} \\end{pmatrix}.\n$$\nEvaluate at $(p_{0},q_{0})=(1,1)$. Here $pq-2=-1$, so\n$$\n\\nabla C(1,1)=\\begin{pmatrix} 2\\cdot 1 \\cdot (-1) \\\\ 2\\cdot 1 \\cdot (-1) \\end{pmatrix}=\\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix},\n\\qquad\n\\nabla^{2} C(1,1)=\\begin{pmatrix} 2\\cdot 1^{2}  4\\cdot 1\\cdot 1 - 4 \\\\ 4\\cdot 1\\cdot 1 - 4  2\\cdot 1^{2} \\end{pmatrix}\n=\\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}.\n$$\nThe inverse Hessian at $(1,1)$ is\n$$\n\\left[\\nabla^{2} C(1,1)\\right]^{-1}=\\frac{1}{2}\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}.\n$$\nApply the Newton update:\n$$\n\\begin{pmatrix} p_{1} \\\\ q_{1} \\end{pmatrix}\n=\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n-\n\\frac{1}{2}\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} -2 \\\\ -2 \\end{pmatrix}\n=\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n-\n\\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}\n=\n\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}.\n$$\nThus, after one Newton step, the parameters are $(p_{1},q_{1})=(2,2)$.",
            "answer": "$$\\boxed{\\begin{bmatrix} 2  2 \\end{bmatrix}}$$"
        },
        {
            "introduction": "Now, let's put everything together to build a modern, robust optimization algorithm. This hands-on coding challenge guides you through creating a hybrid method that intelligently switches from Gradient Descent to a regularized Newton's method. You will implement solutions to the practical issues we've discussed, such as handling non-convexity and preventing overshooting , culminating in an effective solver for a wide range of optimization problems .",
            "id": "3255777",
            "problem": "Consider unconstrained minimization of a twice continuously differentiable function $f : \\mathbb{R}^n \\to \\mathbb{R}$. The goal is to design and implement a robust hybrid algorithm that begins with Gradient Descent (GD) and switches to Newton's Method (NM) when appropriate, to balance global convergence with rapid local convergence. The fundamental base for this task is the definition of the gradient $\\nabla f(\\mathbf{x})$ as the vector of first partial derivatives and the Hessian $\\nabla^2 f(\\mathbf{x})$ as the matrix of second partial derivatives, and the principle that unconstrained minimizers of a sufficiently regular function satisfy the stationarity condition $\\nabla f(\\mathbf{x}^\\star) = \\mathbf{0}$.\n\nAlgorithmic requirements:\n- Use GD to update the iterate $\\mathbf{x}_k$ while the Euclidean norm of the gradient $\\|\\nabla f(\\mathbf{x}_k)\\|_2$ is greater than a switching threshold $\\tau$.\n- When $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\tau$, switch to NM. To ensure a descent direction even when $\\nabla^2 f(\\mathbf{x}_k)$ is not positive definite, apply a symmetric regularization: augment the Hessian by a multiple of the identity, chosen based on the lowest eigenvalue of the Hessian. If the candidate Newton direction is not a descent direction, fall back to the GD direction.\n- Use Backtracking Line Search (BLS) with the Armijo sufficient decrease rule for step-size selection in both GD and NM phases. Let the sufficient decrease constant be $c = 10^{-4}$ and the contraction factor be $\\beta = \\tfrac{1}{2}$. Begin the line search from an initial step-size of $1$.\n- Use the Euclidean norm $\\|\\cdot\\|_2$ for all norm computations.\n- Terminate when $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\epsilon$ with $\\epsilon = 10^{-8}$, or when a maximum of $N_{\\max} = 1000$ iterations is reached.\n\nSwitching threshold and regularization:\n- Use a switching threshold of $\\tau = 10^{-1}$.\n- For Hessian regularization in NM, add $\\lambda I$ to $\\nabla^2 f(\\mathbf{x}_k)$, where $\\lambda$ is chosen to ensure positive definiteness using the smallest eigenvalue of the Hessian and a small buffer. Use a buffer $\\delta = 10^{-6}$.\n\nYour program must implement this algorithm and evaluate it on the following test suite. For each test case, define $f$, its gradient $\\nabla f$, and its Hessian $\\nabla^2 f$, and use the stated initial point:\n- Test case $1$ (strongly convex quadratic in $n = 2$):\n  - $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{\\top} Q \\mathbf{x} + \\mathbf{b}^{\\top} \\mathbf{x} + c$, with $Q = \\begin{bmatrix} 3  1 \\\\ 1  2 \\end{bmatrix}$, $\\mathbf{b} = \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}$, $c = 0$.\n  - $\\nabla f(\\mathbf{x}) = Q \\mathbf{x} + \\mathbf{b}$.\n  - $\\nabla^2 f(\\mathbf{x}) = Q$.\n  - Initial point $\\mathbf{x}_0 = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$.\n- Test case $2$ (Rosenbrock function in $n = 2$):\n  - $f(\\mathbf{x}) = (1 - x_1)^2 + 100 \\left(x_2 - x_1^2\\right)^2$.\n  - $\\nabla f(\\mathbf{x}) = \\begin{bmatrix} -2(1 - x_1) - 400 x_1 \\left(x_2 - x_1^2\\right) \\\\ 200 \\left(x_2 - x_1^2\\right) \\end{bmatrix}$.\n  - $\\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix} 2 - 400 x_2 + 1200 x_1^2  -400 x_1 \\\\ -400 x_1  200 \\end{bmatrix}$.\n  - Initial point $\\mathbf{x}_0 = \\begin{bmatrix} -1.2 \\\\ 1 \\end{bmatrix}$.\n- Test case $3$ (boundary switching case in $n = 2$):\n  - $f(\\mathbf{x}) = \\tfrac{1}{2} \\|\\mathbf{x}\\|_2^2$.\n  - $\\nabla f(\\mathbf{x}) = \\mathbf{x}$.\n  - $\\nabla^2 f(\\mathbf{x}) = I$.\n  - Initial point $\\mathbf{x}_0 = \\begin{bmatrix} 0.1 \\\\ 0 \\end{bmatrix}$, so that $\\|\\nabla f(\\mathbf{x}_0)\\|_2 = 0.1 = \\tau$.\n- Test case $4$ (nonconvex quartic with saddles in $n = 2$):\n  - $f(\\mathbf{x}) = x_1^4 - x_1^2 + x_2^2$.\n  - $\\nabla f(\\mathbf{x}) = \\begin{bmatrix} 4 x_1^3 - 2 x_1 \\\\ 2 x_2 \\end{bmatrix}$.\n  - $\\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix} 12 x_1^2 - 2  0 \\\\ 0  2 \\end{bmatrix}$.\n  - Initial point $\\mathbf{x}_0 = \\begin{bmatrix} 0.1 \\\\ 0 \\end{bmatrix}$.\n\nQuantifiable outputs:\n- For each test case, compute the final objective value $f(\\mathbf{x}_{\\mathrm{final}})$ at termination, expressed as a floating-point number rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the float result for the corresponding test case rounded to six decimal places. Angles are not involved, and no physical units are involved in these computations.",
            "solution": "The problem requires the design and implementation of a hybrid optimization algorithm for unconstrained minimization of a twice continuously differentiable function $f: \\mathbb{R}^n \\to \\mathbb{R}$. The algorithm combines Gradient Descent (GD) for global robustness and Newton's Method (NM) for fast local convergence. The solution is obtained by iteratively updating an estimate $\\mathbf{x}_k$ to find a point $\\mathbf{x}^\\star$ that satisfies the first-order necessary condition for a minimum, $\\nabla f(\\mathbf{x}^\\star) = \\mathbf{0}$.\n\nThe core of the algorithm is an iterative process, $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, where $\\mathbf{p}_k$ is a search direction and $\\alpha_k$ is a step size. The method for choosing $\\mathbf{p}_k$ depends on the state of the iteration, specifically the magnitude of the gradient.\n\n**1. Algorithmic Structure: Hybrid Switching Strategy**\n\nThe algorithm switches between two modes based on the Euclidean norm of the gradient, $\\|\\nabla f(\\mathbf{x}_k)\\|_2$, relative to a predefined threshold $\\tau$.\n\n-   **Gradient Descent Mode**: If $\\|\\nabla f(\\mathbf{x}_k)\\|_2  \\tau$, the current iterate $\\mathbf{x}_k$ is considered to be far from a local minimum. In this regime, the global convergence properties of Gradient Descent are favored. The search direction is set to the direction of steepest descent.\n-   **Newton's Method Mode**: If $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\tau$, the iterate is assumed to be within a neighborhood of a minimum where the function's local quadratic approximation is accurate. Here, the algorithm switches to a regularized Newton's Method to leverage its characteristic rapid (quadratic) convergence.\n\nIterations proceed until the gradient norm falls below a tolerance $\\epsilon = 10^{-8}$, indicating that a stationary point has been found, or until a maximum of $N_{\\max} = 1000$ iterations is reached.\n\n**2. Search Direction Calculation**\n\n-   **Gradient Descent Direction**: In GD mode, the search direction is chosen as the negative gradient:\n    $$\n    \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)\n    $$\n    This direction is guaranteed to be a descent direction for any non-zero gradient, as the directional derivative is $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)^\\top \\nabla f(\\mathbf{x}_k) = -\\|\\nabla f(\\mathbf{x}_k)\\|_2^2  0$.\n\n-   **Regularized Newton's Method Direction**: In NM mode, the search direction is based on the second-order Taylor expansion of $f$ at $\\mathbf{x}_k$. The pure Newton direction $\\mathbf{p}_k^{\\text{N}}$ minimizes the quadratic model $m_k(\\mathbf{p}) = f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p} + \\frac{1}{2}\\mathbf{p}^\\top \\nabla^2 f(\\mathbf{x}_k) \\mathbf{p}$, and is found by solving the linear system:\n    $$\n    \\nabla^2 f(\\mathbf{x}_k) \\mathbf{p}_k^{\\text{N}} = -\\nabla f(\\mathbf{x}_k)\n    $$\n    This step is only well-defined and guaranteed to yield a descent direction if the Hessian matrix $\\nabla^2 f(\\mathbf{x}_k)$ is symmetric and positive definite. To handle cases where the Hessian is not positive definite (e.g., near saddle points or in non-convex regions), a regularization technique is employed. The Hessian is modified by adding a multiple of the identity matrix:\n    $$\n    \\mathbf{H}_k^{\\text{reg}} = \\nabla^2 f(\\mathbf{x}_k) + \\lambda \\mathbf{I}\n    $$\n    The regularization parameter $\\lambda$ must be chosen to ensure $\\mathbf{H}_k^{\\text{reg}}$ is positive definite. Let $\\lambda_{\\min}$ be the smallest eigenvalue of the Hessian $\\nabla^2 f(\\mathbf{x}_k)$. The eigenvalues of $\\mathbf{H}_k^{\\text{reg}}$ are $\\lambda_i + \\lambda$. To ensure all eigenvalues of the regularized Hessian are strictly positive, we require $\\lambda_{\\min} + \\lambda  0$. A suitable choice is:\n    $$\n    \\lambda = \\max(0, -\\lambda_{\\min} + \\delta)\n    $$\n    where $\\delta = 10^{-6}$ is a small positive buffer to ensure strict positive definiteness and avoid numerical issues. The regularized Newton direction $\\mathbf{p}_k$ is then found by solving the well-posed linear system:\n    $$\n    (\\nabla^2 f(\\mathbf{x}_k) + \\lambda \\mathbf{I}) \\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k)\n    $$\n    By construction, this $\\mathbf{p}_k$ is a descent direction. As a safeguard, the implementation includes a fallback mechanism: if the computed direction $\\mathbf{p}_k$ fails the descent condition $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k  0$, the algorithm reverts to the steepest descent direction $\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$ for that iteration.\n\n**3. Step-Size Selection: Backtracking Line Search**\n\nFor any chosen descent direction $\\mathbf{p}_k$, an appropriate step size $\\alpha_k$ must be determined to ensure progress towards the minimum. A fixed step size may lead to divergence. The Backtracking Line Search with the Armijo condition provides a robust method for selecting $\\alpha_k$.\n\nStarting with an initial trial step size $\\alpha = 1$, the procedure iteratively checks if the sufficient decrease condition is met:\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k\n$$\nwhere $c = 10^{-4}$ is a constant controlling the required decrease. The term $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k$ is the directional derivative of $f$ at $\\mathbf{x}_k$ along $\\mathbf{p}_k$. If the condition is not met, the step size is reduced by a contraction factor $\\beta = \\frac{1}{2}$, i.e., $\\alpha \\leftarrow \\beta \\alpha$, and the check is repeated. This process guarantees a finite termination with a step size that produces a sufficient decrease in the objective function.\n\n**4. Implementation Summary**\n\nThe final implementation encapsulates this logic in a single function. It accepts the objective function $f$, its gradient $\\nabla f$, its Hessian $\\nabla^2 f$, and an initial point $\\mathbf{x}_0$. The function iterates through the steps of gradient norm calculation, mode switching, direction finding (with regularization and fallback), and line search, until a termination criterion is met. This solver is then applied to each of the four specified test cases to compute the final objective function values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the hybrid optimization algorithm.\n    \"\"\"\n\n    def hybrid_optimizer(f, grad_f, hess_f, x0, tau, epsilon, n_max, c, beta, delta):\n        \"\"\"\n        Implements the hybrid Gradient Descent / Newton's Method algorithm.\n        \n        Args:\n            f (callable): The objective function.\n            grad_f (callable): The gradient of the objective function.\n            hess_f (callable): The Hessian of the objective function.\n            x0 (np.ndarray): The initial point.\n            tau (float): The switching threshold for the gradient norm.\n            epsilon (float): The termination tolerance for the gradient norm.\n            n_max (int): The maximum number of iterations.\n            c (float): The Armijo condition constant.\n            beta (float): The backtracking line search contraction factor.\n            delta (float): The Hessian regularization buffer.\n\n        Returns:\n            float: The final objective function value.\n        \"\"\"\n        x = x0.copy().astype(float)\n        \n        for _ in range(n_max):\n            g = grad_f(x)\n            grad_norm = np.linalg.norm(g, 2)\n            \n            # Termination condition\n            if grad_norm = epsilon:\n                break\n            \n            # Mode selection: GD vs. NM\n            if grad_norm  tau:\n                # Gradient Descent mode\n                pk = -g\n            else:\n                # Newton's Method mode\n                H = hess_f(x)\n                \n                # Regularization to ensure positive definiteness\n                try:\n                    # Use eigvalsh for symmetric matrices\n                    eigvals = np.linalg.eigvalsh(H)\n                    lambda_min = eigvals[0]\n                    lam = max(0, -lambda_min + delta)\n                except np.linalg.LinAlgError:\n                    # Fallback if eigendecomposition fails\n                    lam = delta\n\n                H_reg = H + lam * np.identity(len(x))\n                \n                try:\n                    # Solve for Newton direction\n                    pk = np.linalg.solve(H_reg, -g)\n                    \n                    # Fallback to GD if not a descent direction (safety check)\n                    if g.T @ pk = 0:\n                        pk = -g\n                except np.linalg.LinAlgError:\n                    # Fallback if solver fails\n                    pk = -g\n\n            # Backtracking Line Search (BLS)\n            alpha = 1.0\n            fx = f(x)\n            g_dot_p = g.T @ pk\n            \n            # Prevent infinite loop in BLS if direction is not descent\n            if g_dot_p = 0:\n                # This should not happen if pk is a descent direction\n                # but as a safeguard, we stop if we can't find a step\n                break\n\n            while f(x + alpha * pk)  fx + c * alpha * g_dot_p:\n                alpha *= beta\n                if alpha  1e-16: # Practical limit for step size\n                    break\n            \n            # Update\n            x = x + alpha * pk\n            \n        return f(x)\n\n    # --- Define Test Cases ---\n\n    # Common parameters\n    params = {\n        'tau': 1e-1,\n        'epsilon': 1e-8,\n        'n_max': 1000,\n        'c': 1e-4,\n        'beta': 0.5,\n        'delta': 1e-6\n    }\n\n    # Test Case 1: Strongly convex quadratic\n    Q1 = np.array([[3., 1.], [1., 2.]])\n    b1 = np.array([-1., -2.])\n    f1 = lambda x: 0.5 * x.T @ Q1 @ x + b1.T @ x\n    grad_f1 = lambda x: Q1 @ x + b1\n    hess_f1 = lambda x: Q1\n    x0_1 = np.array([2., -1.])\n\n    # Test Case 2: Rosenbrock function\n    f2 = lambda x: (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n    grad_f2 = lambda x: np.array([\n        -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2),\n        200 * (x[1] - x[0]**2)\n    ])\n    hess_f2 = lambda x: np.array([\n        [2 - 400 * x[1] + 1200 * x[0]**2, -400 * x[0]],\n        [-400 * x[0], 200]\n    ])\n    x0_2 = np.array([-1.2, 1.])\n\n    # Test Case 3: Boundary switching case\n    f3 = lambda x: 0.5 * x.T @ x\n    grad_f3 = lambda x: x\n    hess_f3 = lambda x: np.identity(2)\n    x0_3 = np.array([0.1, 0.])\n\n    # Test Case 4: Nonconvex quartic\n    f4 = lambda x: x[0]**4 - x[0]**2 + x[1]**2\n    grad_f4 = lambda x: np.array([4 * x[0]**3 - 2 * x[0], 2 * x[1]])\n    hess_f4 = lambda x: np.array([[12 * x[0]**2 - 2, 0], [0, 2.]])\n    x0_4 = np.array([0.1, 0.])\n\n    test_cases = [\n        (f1, grad_f1, hess_f1, x0_1),\n        (f2, grad_f2, hess_f2, x0_2),\n        (f3, grad_f3, hess_f3, x0_3),\n        (f4, grad_f4, hess_f4, x0_4),\n    ]\n\n    results = []\n    for f_case, grad_f_case, hess_f_case, x0_case in test_cases:\n        final_f_val = hybrid_optimizer(f_case, grad_f_case, hess_f_case, x0_case, **params)\n        results.append(f\"{final_f_val:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}