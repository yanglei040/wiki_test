{
    "hands_on_practices": [
        {
            "introduction": "在我们优化一个非线性模型之前，我们必须首先理解它如何响应其参数的微小变化。雅可比矩阵（Jacobian matrix）恰好提供了这些信息，它代表了我们模型的局部线性近似。这项练习将指导你掌握为给定模型和数据计算雅可比矩阵的基本技能，这是列文伯格-马夸尔特算法每次迭代中至关重要的第一步。",
            "id": "2217052",
            "problem": "在非线性优化领域，像Levenberg-Marquardt算法这样的算法被用于通过最小化残差平方和，将一个模型函数拟合到一组数据点上。这个过程中的一个关键组成部分是模型函数的雅可比矩阵。\n\n考虑一个用于描述物质浓度随时间变化的模型，由以下双参数有理函数给出：\n$$f(t; a, b) = \\frac{a}{1 + bt}$$\n其中，$t$ 是自变量（例如，时间），而 $\\mathbf{p} = [a, b]^T$ 是待确定的参数向量。\n\n假设我们收集了以下三个数据点 $(t_i, y_i)$：\n- $(t_1, y_1) = (1.0, 2.5)$\n- $(t_2, y_2) = (2.0, 1.8)$\n- $(t_3, y_3) = (4.0, 1.1)$\n\n对于这个拟合问题，雅可比矩阵 $\\mathbf{J}$ 是一个 $m \\times n$ 的矩阵，其中 $m$ 是数据点的数量，$n$ 是参数的数量。其元素定义为 $J_{ij} = \\frac{\\partial f(t_i; \\mathbf{p})}{\\partial p_j}$。\n\n在初始参数猜测值 $\\mathbf{p}_0 = [a_0, b_0]^T = [3.0, 0.5]^T$ 处，计算雅可比矩阵 $\\mathbf{J}$ 的数值。所有给定的数值都是无量纲的。\n\n将您的最终答案表示为一个 $3 \\times 2$ 矩阵，每个元素四舍五入到三位有效数字。",
            "solution": "我们给定的模型函数为 $f(t; a, b) = \\dfrac{a}{1 + bt}$，并且对于参数向量 $\\mathbf{p} = [a, b]^{T}$，雅可比矩阵定义为 $J_{ij} = \\dfrac{\\partial f(t_{i}; \\mathbf{p})}{\\partial p_{j}}$。因此，$\\mathbf{J}$ 的每一行对应一个数据点 $t_{i}$，两列分别对应关于 $a$ 和 $b$ 的导数。\n\n首先，以符号方式计算偏导数。将 $f(t; a, b)$ 写作 $a(1 + bt)^{-1}$。然后，使用幂法则和链式法则：\n$$\n\\frac{\\partial f}{\\partial a} = (1 + bt)^{-1} = \\frac{1}{1 + bt},\n$$\n$$\n\\frac{\\partial f}{\\partial b} = a \\cdot \\frac{\\partial}{\\partial b}(1 + bt)^{-1} = a \\cdot \\left[-(1 + bt)^{-2} \\cdot t\\right] = -\\frac{a t}{(1 + bt)^{2}}.\n$$\n\n因此，对于每个数据点 $t_{i}$，雅可比矩阵的对应行为\n$$\n\\left[\\frac{1}{1 + b t_{i}},\\ -\\frac{a t_{i}}{(1 + b t_{i})^{2}}\\right].\n$$\n\n在初始猜测值 $\\mathbf{p}_{0} = [a_{0}, b_{0}]^{T} = [3.0, 0.5]^{T}$ 和给定的 $t$ 值处进行计算。\n\n对于 $t_{1} = 1$：\n$$\n1 + b_{0} t_{1} = 1 + 0.5 \\cdot 1 = \\frac{3}{2},\\quad \\frac{\\partial f}{\\partial a} = \\frac{2}{3},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 1}{(3/2)^{2}} = -\\frac{12}{9} = -\\frac{4}{3}.\n$$\n\n对于 $t_{2} = 2$：\n$$\n1 + b_{0} t_{2} = 1 + 0.5 \\cdot 2 = 2,\\quad \\frac{\\partial f}{\\partial a} = \\frac{1}{2},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 2}{2^{2}} = -\\frac{6}{4} = -\\frac{3}{2}.\n$$\n\n对于 $t_{3} = 4$：\n$$\n1 + b_{0} t_{3} = 1 + 0.5 \\cdot 4 = 3,\\quad \\frac{\\partial f}{\\partial a} = \\frac{1}{3},\\quad \\frac{\\partial f}{\\partial b} = -\\frac{3 \\cdot 4}{3^{2}} = -\\frac{12}{9} = -\\frac{4}{3}.\n$$\n\n组合成雅可比矩阵，并将每个元素四舍五入到三位有效数字：\n$$\n\\mathbf{J}(\\mathbf{p}_{0}) \\approx\n\\begin{pmatrix}\n0.667  -1.33 \\\\\n0.500  -1.50 \\\\\n0.333  -1.33\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}0.667  -1.33 \\\\ 0.500  -1.50 \\\\ 0.333  -1.33\\end{pmatrix}}$$"
        },
        {
            "introduction": "列文伯格-马夸尔特算法常被描述为高斯-牛顿法的一个“阻尼”版本，但这种阻尼实际上实现了什么？在这个练习中，你将创建一个场景，其中无阻尼的高斯-牛顿法会变得不稳定并失效。通过将其行为与稳定的列文伯格-马夸尔特方法进行比较，你将对阻尼参数$\\lambda$如何提供鲁棒性有一个深刻、直观的理解。",
            "id": "3232802",
            "problem": "您需要编写一个完整、可运行的程序，用于分析高斯-牛顿法及其Levenberg-Marquardt (LM) 稳定化方法在一个单参数非线性最小二乘问题中的表现。目标是通过有据可依的推导和具体的实现来证明，当雅可比矩阵接近奇异时，无阻尼的高斯-牛顿步长会发生振荡或发散，而添加Levenberg-Marquardt阻尼项可以稳定迭代过程。\n\n从以下基本原理开始：\n- 非线性最小二乘的目标函数是 $F(x) = \\tfrac{1}{2}\\lVert r(x)\\rVert_2^2$，其中 $r(x)$ 是残差函数向量。\n- $r(x)$ 在 $x$ 处的一阶泰勒线性化为 $r(x + \\Delta) \\approx r(x) + J(x)\\Delta$，其中 $J(x)$ 是 $r(x)$ 的雅可比矩阵。\n- 最小化该线性化模型可得到正规方程 $J(x)^\\top J(x)\\Delta = -J(x)^\\top r(x)$。\n- Levenberg-Marquardt (LM) 方法通过增加一个阻尼项 $\\lambda I$（其中 $I$ 是单位矩阵）来增强正规矩阵，求解方程 $\\big(J(x)^\\top J(x) + \\lambda I\\big)\\Delta = -J(x)^\\top r(x)$，其中 $\\lambda > 0$。\n\n您的程序必须实例化一个一维残差模型 $r(x) = \\sin(x)$（角度以弧度为单位），并执行高斯-牛顿（无阻尼，对应于 $\\lambda = 0$）和Levenberg-Marquardt（有阻尼，使用指定的 $\\lambda > 0$）迭代。该残差模型意味着最小二乘目标函数为 $F(x) = \\tfrac{1}{2}\\sin^2(x)$。在这种情况下，雅可比矩阵 $J(x)$ 是一个标量。您必须：\n- 基于基本定义，推导出针对一维残差 $r(x) = \\sin(x)$ 的高斯-牛顿和LM更新规则。\n- 使用推导出的更新规则，为两种方法实现固定迭代次数的求解器。\n- 证明在 $J(x)$ 接近奇异的点附近（特别是在 $x \\approx \\tfrac{\\pi}{2} + k\\pi$ 附近，其中 $k$ 为整数），无阻尼的高斯-牛顿迭代会表现出振荡或发散（大幅度的交替步长或步长超出有界域），而带有正阻尼参数的LM方法通过限制步长大小来稳定迭代。\n\n角度必须以弧度为单位。不涉及任何物理单位。除了恒定的LM阻尼外，您的算法不得使用任何线搜索或信赖域自适应方法。\n\n测试套件和输出规范：\n- 使用以下测试用例，每个用例指定为一个元组 $(x_0, \\lambda, N, L)$：\n  1. 一个接近奇异雅可比矩阵以引发不稳定性的情况：$x_0 = \\tfrac{\\pi}{2} - 10^{-6}$，$\\lambda = 1.0$，$N = 10$，$L = 50$。\n  2. 一个振荡但可恢复的情况：$x_0 = 1.4$，$\\lambda = 0.5$，$N = 10$，$L = 50$。\n  3. 一个远离奇异点的理想情况：$x_0 = 2.0$，$\\lambda = 0.1$，$N = 10$，$L = 50$。\n- 对于每个用例，运行 $N$ 次无阻尼高斯-牛顿迭代（即，对于高斯-牛顿法，设置 $\\lambda = 0$）和 $N$ 次使用给定 $\\lambda$ 的LM迭代。对于每个用例中的每种方法，计算：\n  - 最终残差范数 $|r(x_N)|$，作为浮点数。\n  - 一个布尔类型的稳定性标志，如果所有迭代值 $x_k$ 都满足 $|x_k| \\le L$，则为true，否则为false。\n  - 最大步长幅度 $\\max_k |\\Delta_k|$，作为浮点数，其中 $\\Delta_k$ 是第 $k$ 次迭代的更新量。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。结果必须构造成一个列表的列表，每个内部列表对应一个测试用例，每个内部列表按以下顺序包含六个条目：$[\\text{gn\\_final\\_residual}, \\text{lm\\_final\\_residual}, \\text{gn\\_stable}, \\text{lm\\_stable}, \\text{gn\\_max\\_step}, \\text{lm\\_max\\_step}]$。例如，输出格式必须类似于 $[[r_{11}, r_{12}, b_{11}, b_{12}, s_{11}, s_{12}], [r_{21}, r_{22}, b_{21}, b_{22}, s_{21}, s_{22}], [r_{31}, r_{32}, b_{31}, b_{32}, s_{31}, s_{32}]]$，其中 $r_{ij}$ 是浮点数，$b_{ij}$ 是布尔值。\n\n科学真实性与覆盖范围：\n- 接近奇异的情况强制了一个边界条件，在该条件下，无阻尼方法会遇到接近于零的雅可比矩阵，这可能导致巨大的步长。LM阻尼项 $\\lambda I$ 必须能缓解这种不稳定性。\n- 振荡情况展示了无阻尼高斯-牛顿法的大幅交替步长，而在LM阻尼下这些步长会减小。\n- 理想情况展示了两种方法的收敛性。\n- 角度以弧度为单位。\n- 所有输出必须是指定的基本类型（布爾值和浮點數）。",
            "solution": "该问题要求针对一个特定的一维非线性最小二乘问题，验证并实现高斯-牛顿 (GN) 法及其 Levenberg-Marquardt (LM) 稳定化方法。其目标是展示当雅可比矩阵接近奇异时，LM稳定化如何纠正无阻尼高斯-牛頓法固有的不稳定性。\n\n首先，我们建立理论基础。一般的非线性最小二乘问题旨在最小化一个目标函数 $F(x)$，该函数定义为残差向量 $r(x)$ 的欧几里得范数平方的一半：\n$$\nF(x) = \\frac{1}{2}\\lVert r(x)\\rVert_2^2\n$$\n高斯-牛顿法在每次迭代中，通过在当前迭代点 $x_k$ 周围对残差函数 $r(x)$ 进行线性化来近似目标函数。更新步长 $\\Delta$ 通过求解以下线性化最小二乘问题得到：\n$$\n\\min_{\\Delta} \\frac{1}{2}\\lVert r(x_k) + J(x_k)\\Delta\\rVert_2^2\n$$\n其中 $J(x_k)$ 是 $r(x)$ 在 $x_k$ 处的雅可比矩阵。这个线性问题的解由以下正规方程给出：\n$$\nJ(x_k)^\\top J(x_k)\\Delta_{GN} = -J(x_k)^\\top r(x_k)\n$$\nLevenberg-Marquardt方法引入一个阻尼参数 $\\lambda > 0$ 来对问题进行正则化，这在矩阵 $J(x_k)^\\top J(x_k)$ 是奇异或病态时尤其有用。LM更新步长 $\\Delta_{LM}$ 通过求解修正后的正规方程得到：\n$$\n\\left(J(x_k)^\\top J(x_k) + \\lambda I\\right)\\Delta_{LM} = -J(x_k)^\\top r(x_k)\n$$\n其中 $I$ 是单位矩阵。\n\n现在，我们将这些通用公式具体化到给定的一维问题上，其中残差是一个标量函数 $r(x) = \\sin(x)$。\n目标函数变为：\n$$\nF(x) = \\frac{1}{2}(r(x))^2 = \\frac{1}{2}\\sin^2(x)\n$$\n$F(x)$ 的最小值出现在 $\\sin(x) = 0$ 的地方，即对于任何整数 $n$，$x = n\\pi$。\n在这个一维情况下，雅可比矩阵 $J(x)$ 是一个 $1 \\times 1$ 的矩阵（一个标量），对应于 $r(x)$ 的一阶导数：\n$$\nJ(x) = \\frac{dr}{dx} = \\frac{d}{dx}(\\sin(x)) = \\cos(x)\n$$\n当 $J(x) = \\cos(x) \\approx 0$ 时，雅可比矩阵是（接近）奇异的，这发生在 $x \\approx \\frac{\\pi}{2} + k\\pi$（对于任何整数 $k$）的情况下。\n\n我们为这两种方法推导具体的更新规则。\n对于高斯-牛顿法（无阻尼，等效于 $\\lambda=0$），正规方程为：\n$$\n(\\cos(x_k))^\\top (\\cos(x_k))\\Delta_{GN} = -(\\cos(x_k))^\\top (\\sin(x_k))\n$$\n$$\n\\cos^2(x_k) \\Delta_{GN} = -\\sin(x_k)\\cos(x_k)\n$$\n假设 $\\cos(x_k) \\neq 0$，我们可以解出步长 $\\Delta_{GN}$：\n$$\n\\Delta_{GN} = -\\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k)} = -\\frac{\\sin(x_k)}{\\cos(x_k)} = -\\tan(x_k)\n$$\n因此，高斯-牛顿法的更新规则是：\n$$\nx_{k+1} = x_k + \\Delta_{GN} = x_k - \\tan(x_k)\n$$\n当 $x_k$ 接近雅可比矩阵为奇异的点，即 $x_k \\to \\frac{\\pi}{2} + k\\pi$ 时，该方法的不稳定性变得很明显。在这些点，$\\cos(x_k) \\to 0$，导致 $|\\tan(x_k)| \\to \\infty$。更新步长 $\\Delta_{GN}$ 变得任意大，从而导致发散或剧烈振荡。\n\n对于Levenberg-Marquardt方法，修正后的正规方程包含阻尼项 $\\lambda > 0$。在这个一维情况下，单位矩阵 $I$ 就是标量 $1$：\n$$\n(\\cos^2(x_k) + \\lambda)\\Delta_{LM} = -\\sin(x_k)\\cos(x_k)\n$$\n因为 $\\cos^2(x_k) \\ge 0$ 且 $\\lambda > 0$，所以项 $(\\cos^2(x_k) + \\lambda)$ 总是正的，并且其下界为 $\\lambda$，不会为零。因此，该系统总是良态的。LM更新步长为：\n$$\n\\Delta_{LM} = -\\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k) + \\lambda}\n$$\nLevenberg-Marquardt的更新规则是：\n$$\nx_{k+1} = x_k + \\Delta_{LM} = x_k - \\frac{\\sin(x_k)\\cos(x_k)}{\\cos^2(x_k) + \\lambda}\n$$\n现在考虑在奇异点附近的行为，此时 $\\cos(x_k) \\to 0$。在此极限下，分子 $\\sin(x_k)\\cos(x_k)$ 也趋近于 $0$。分母趋近于 $\\lambda$。因此，步长 $\\Delta_{LM}$ 趋近于 $0$。阻尼项有效地限制了步长大小，防止了在无阻尼高斯-牛顿法中观察到的灾难性行为。这确保了稳定性，并即使从雅可比矩阵奇异的区域附近开始，也能促进收敛。\n\n接下来的实现将通过对给定的测试用例迭代这两个更新规则，来数值化地展示这一推导出的行为。结果将量化最终残差、在给定界限内迭代序列的稳定性以及所采取步长的大小。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes Gauss-Newton and Levenberg-Marquardt methods for a 1D\n    nonlinear least-squares problem, demonstrating LM stabilization.\n    \"\"\"\n\n    # Test cases as tuples of (x0, lambda, N, L)\n    test_cases = [\n        (np.pi / 2.0 - 1e-6, 1.0, 10, 50.0), # Near-singular Jacobian case\n        (1.4, 0.5, 10, 50.0),               # Oscillatory-but-recoverable case\n        (2.0, 0.1, 10, 50.0)                # Happy-path case\n    ]\n\n    results = []\n\n    def run_iterations(x0, N, L, damp_lambda):\n        \"\"\"\n        Performs N iterations of a solver for the objective F(x) = 0.5*sin^2(x).\n\n        Args:\n            x0 (float): Initial guess.\n            N (int): Number of iterations.\n            L (float): Stability bound for |x_k|.\n            damp_lambda (float): Damping parameter. If 0, use Gauss-Newton.\n                                 If > 0, use Levenberg-Marquardt.\n\n        Returns:\n            A tuple containing:\n            - final_residual (float): |r(x_N)|.\n            - is_stable (bool): True if all |x_k| = L.\n            - max_step (float): Maximum magnitude of any step delta_k.\n        \"\"\"\n        x = x0\n        stable = True\n        max_step_mag = 0.0\n\n        for _ in range(N):\n            # The Jacobian is singular when cos(x) is zero.\n            # Avoid division by zero for the pure Gauss-Newton case if x is exactly pi/2 + k*pi.\n            # np.tan will handle large values gracefully, returning inf.\n            cos_x = np.cos(x)\n            sin_x = np.sin(x)\n\n            if damp_lambda == 0:  # Gauss-Newton\n                # Delta = -tan(x)\n                # Avoid explicit division by zero if cos_x is extremely small.\n                # np.tan handles this by returning large numbers or inf.\n                delta = -np.tan(x)\n\n            else:  # Levenberg-Marquardt\n                # Delta = -sin(x)cos(x) / (cos^2(x) + lambda)\n                numerator = -sin_x * cos_x\n                denominator = cos_x**2 + damp_lambda\n                delta = numerator / denominator\n\n            if np.isinf(delta) or np.isnan(delta):\n                # If step is infinite/NaN, it's definitively unstable and huge.\n                # To assign a finite but large value for max_step.\n                # Using 2*L is arbitrary but indicates a large step.\n                current_step_mag = 2 * L \n                x = x + (2 * L * np.sign(-delta) if not np.isnan(delta) else 0)\n            else:\n                current_step_mag = abs(delta)\n\n            if current_step_mag > max_step_mag:\n                max_step_mag = current_step_mag\n\n            x = x + delta\n            \n            if abs(x) > L:\n                stable = False\n        \n        final_residual = abs(np.sin(x))\n        \n        return final_residual, stable, max_step_mag\n\n    for x0, lm_lambda, N, L in test_cases:\n        # Run Gauss-Newton (undamped, lambda = 0)\n        gn_final_residual, gn_stable, gn_max_step = run_iterations(x0, N, L, 0)\n\n        # Run Levenberg-Marquardt (damped)\n        lm_final_residual, lm_stable, lm_max_step = run_iterations(x0, N, L, lm_lambda)\n\n        case_results = [\n            gn_final_residual,\n            lm_final_residual,\n            gn_stable,\n            lm_stable,\n            gn_max_step,\n            lm_max_step,\n        ]\n        results.append(case_results)\n\n    # Format the final output string exactly as specified.\n    # The default str() representation for lists includes spaces, which is acceptable.\n    # The boolean values will be represented as 'True' and 'False'.\n    output_str = \"[\" + \",\".join(map(str, results)) + \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在探索了列文伯格-马夸尔特算法的核心组件和稳定原理之后，是时候构建一个完整的求解器了。这项综合性练习挑战你从零开始实现完整的算法，包括数值雅可比矩阵的近似和阻尼参数的自适应策略。通过完成这项练习，你将把理论知识转化为一个用于解决真实世界非线性拟合问题的实用而强大的工具。",
            "id": "3256843",
            "problem": "您的任务是实现一个完整的、自包含的程序，使用Levenberg–Marquardt算法解决非线性最小二乘拟合问题，其中阻尼参数根据平方和的实际减少量与预测减少量之比自动调整。推导和实现必须从非线性最小二乘目标的基本定义开始，并逐步发展为一个在科学上和数值上都稳健的算法。\n\n从非线性最小二乘目标的核心定义开始。设参数向量为 $p \\in \\mathbb{R}^m$，自变量向量为 $x \\in \\mathbb{R}^n$，观测数据（因变量）为 $y \\in \\mathbb{R}^n$，模型是一个函数 $f:\\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}^n$，该函数根据参数预测数据。通过 $r(p) = y - f(x,p)$ 定义残差向量 $r(p) \\in \\mathbb{R}^n$，通过 $F(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2$ 定义目标函数。定义雅可比矩阵 $J(p) \\in \\mathbb{R}^{n \\times m}$，其元素为 $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$。您的程序必须在每次迭代中使用中心有限差分法来数值近似雅可比矩阵。\n\n在每次迭代中，使用 $r(p)$ 和 $J(p)$ 在当前参数 $p$ 周围进行二阶近似，构建 $F(p)$ 的局部二次模型，并通过求解一个阻尼正规方程组来计算一个在Gauss–Newton方向和梯度下降方向之间取得平衡的参数增量。算法必须使用一个增益比来决定接受还是拒绝提议的步长，该增益比比较 $F(p)$ 的实际减少量与用于计算步长的局部模型所预测的减少量。阻尼参数必须基于此增益比自动调整：当步长有效时减小阻尼，当步长无效时增加阻尼。使用一个终止准则，当梯度的无穷范数很小、参数更新很小或目标函数的变化可以忽略不计时，停止迭代。\n\n正弦模型中使用的角度量 $\\phi$ 必须以弧度为单位进行处理和计算。\n\n您的程序必须为以下三个测试用例实现上述算法，每个用例都有特定的模型、数据集和初始参数猜测。对于每个测试用例，返回拟合后的参数向量，形式为浮点数列表，并四舍五入到六位小数。\n\n测试用例1（一般收敛性）：\n- 模型：$f(x, \\mathbf{p}) = p_1 \\exp(p_2 x) + p_3$，参数为 $\\mathbf{p} = [p_1, p_2, p_3]$。\n- 数据：$x$ 由 $x_i = 0.0 + i\\Delta$ 定义，其中 $i=0,1,\\dots,30$ 且 $\\Delta = 0.1$，因此 $x \\in [0,3.0]$，包含 $31$ 个点。观测值由 $y = 2.0\\exp(-0.7 x) + 0.5 + 0.01\\sin(5x)$ 定义。\n- 初始参数：$p^{(0)} = [1.0,-0.2,0.0]$。\n\n测试用例2（陡峭非线性）：\n- 模型：logistic曲线 $f(x, \\mathbf{p}) = \\frac{p_1}{1 + \\exp(-p_2(x - p_3))}$，参数为 $\\mathbf{p} = [p_1, p_2, p_3]$。\n- 数据：$x$ 由 $x_i = -1.5 + i\\Delta$ 定义，其中 $i=0,1,\\dots,14$ 且 $\\Delta = \\frac{2.0 - (-1.5)}{14}$，因此 $x \\in [-1.5,2.0]$，包含 $15$ 个点。观测值由 $y = \\frac{1.5}{1 + \\exp(-3.0(x - 0.5))} + 0.02\\cos(3x)$ 定义。\n- 初始参数：$p^{(0)} = [1.0,1.0,0.0]$。\n\n测试用例3（灵敏度近乎简并）：\n- 模型：角频率固定为 $\\omega = 2.5$ 弧度/单位$x$的正弦曲线，$f(x, \\mathbf{p}) = p_1 \\cos(\\omega x + p_2) + p_3$，参数为 $\\mathbf{p} = [p_1, p_2, p_3]$ 且 $\\phi = p_2$ 以弧度计。\n- 数据：$x$ 由 $x_i = 0.0 + i\\Delta$ 定义，其中 $i=0,1,\\dots,20$ 且 $\\Delta = 0.1$，因此 $x \\in [0,2.0]$，包含 $21$ 个点。观测值由 $y = 0.8\\cos(2.5 x + 0.4) - 0.05 + 0.01\\sin(7x)$ 定义。\n- 初始参数：$p^{(0)} = [0.5,0.0,0.0]$。\n\n算法要求：\n- 使用中心有限差分法计算雅可比矩阵，扰动步长与 $\\sqrt{\\varepsilon}(1 + |p_j|)$ 成正比，其中 $\\varepsilon$ 是双精度浮点数的机器精度。\n- 在每次迭代中，构建并求解一个阻尼正规方程组以获得参数增量。\n- 计算目标函数的实际减少量和用于步长计算的局部模型所预测的减少量。使用它们的比率来决定是否接受步长并调整阻尼参数。\n- 当任何标准的“小量”准则被满足时终止：梯度的无穷范数很小，参数变化很小，或目标函数的变化很小。同时，包含一个最大迭代次数上限以确保算法能够停止。\n- 在整个过程中，正弦模型中的角度 $\\phi$ 必须以弧度计算。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的、以逗号分隔的结果列表。每个测试用例的结果必须是一个拟合参数值的列表，四舍五入到六位小数。最终输出格式必须严格遵循以下形式：\n\"[[p11,p12,p13],[p21,p22,p23],[p31,p32,p33]]\"\n其中 $p1k$、$p2k$ 和 $p3k$ 分别是测试用例1、2和3的拟合参数，以十进制数表示。",
            "solution": "用户提供的问题被评估为**有效**。这是一个适定、有科学依据且计算上可行的数值方法领域的任务。它要求实现用于非线性最小二乘的Levenberg-Marquardt算法，这是科学计算中一个标准且重要的技术。所有必要的组成部分，包括目标函数、算法流程、测试模型、数据集和初始条件，都已提供且内部一致。\n\n在此，从第一性原理出发推导出一个完整的解决方案。\n\n### 1. 非线性最小二乘问题\n\n问题的核心是找到一组参数，使得模型能最好地拟合一组观测数据。设参数向量为 $p \\in \\mathbb{R}^m$，自变量向量为 $x \\in \\mathbb{R}^n$，对应的观测数据向量为 $y \\in \\mathbb{R}^n$。一个模型函数 $f(x, p)$ 将参数 $p$ 和自变量 $x$ 映射到一组预测数据点。目标是找到参数向量 $p$，以最小化观测数据 $y$ 与预测数据 $f(x,p)$ 之间差值的平方和。\n\n这可以形式化地通过定义一个残差向量 $r(p) \\in \\mathbb{R}^n$ 来表达：\n$$\nr(p) = y - f(x, p)\n$$\n目标是最小化标量目标函数 $F(p)$，其定义为残差向量的欧几里得范数平方的一半：\n$$\nF(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2 = \\frac{1}{2} \\sum_{i=1}^{n} r_i(p)^2\n$$\n由于模型 $f(x,p)$ 通常是参数 $p$ 的非线性函数，这是一个非线性最小二乘问题，必须使用迭代方法求解。\n\n### 2. 通过局部近似的迭代解法\n\nLevenberg-Marquardt算法是一个寻找 $F(p)$ 局部最小值的迭代过程。从一个初始猜测 $p^{(0)}$ 开始，该算法生成一个参数向量序列 $p^{(1)}, p^{(2)}, \\dots$，逐步减小 $F(p)$ 的值。每次迭代计算一个步长向量 $h$，用于更新当前的参数向量：$p_{k+1} = p_k + h$。\n\n步长 $h$ 是通过在当前点 $p_k$ 周围构建目标函数 $F(p)$ 的一个局部二次模型来找到的。该模型通过使用一阶泰勒级数展开来线性化残差向量 $r(p_k+h)$ 来构建：\n$$\nr(p_k + h) \\approx r(p_k) + J(p_k)h\n$$\n其中 $J(p_k)$ 是残差向量 $r$ 在 $p_k$ 处求值的雅可比矩阵。雅可比矩阵的元素由 $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$ 给出。由于 $r(p) = y - f(x,p)$，这等价于 $J_{ij}(p) = -\\frac{\\partial f_i(x,p)}{\\partial p_j}$。\n\n将这个残差的线性近似代入目标函数 $F(p_k+h)$，得到一个局部二次模型 $L(h)$：\n$$\nF(p_k + h) = \\frac{1}{2}\\lVert r(p_k+h) \\rVert_2^2 \\approx \\frac{1}{2}\\lVert r(p_k) + J(p_k)h \\rVert_2^2 = L(h)\n$$\n展开平方范数得到：\n$$\nL(h) = \\frac{1}{2} (r_k + J_k h)^T (r_k + J_k h) = \\frac{1}{2} (r_k^T r_k + 2h^T J_k^T r_k + h^T J_k^T J_k h)\n$$\n$$\nL(h) = F(p_k) + h^T (J_k^T r_k) + \\frac{1}{2} h^T (J_k^T J_k) h\n$$\n其中 $r_k = r(p_k)$ 且 $J_k = J(p_k)$。\n\n### 3. Gauss-Newton步与Levenberg-Marquardt步\n\n选择步长 $h$ 以最小化这个二次模型 $L(h)$。将 $L(h)$ 对 $h$ 的梯度设为零，得到：\n$$\n\\nabla_h L(h) = J_k^T r_k + (J_k^T J_k) h = 0\n$$\n这导出了**Gauss-Newton正规方程组**：\n$$\n(J_k^T J_k) h_{gn} = -J_k^T r_k\n$$\n步长 $h_{gn}$ 是Gauss-Newton步。当矩阵 $J_k^T J_k$ 是良态的时，此方法效果很好。然而，如果 $J_k^T J_k$ 是奇异或病态的，步长可能会过大且无效。\n\n**Levenberg-Marquardt算法**通过引入一个阻尼参数 $\\lambda \\ge 0$ 来解决这个问题。步长 $h_{lm}$ 通过求解一个修正的或阻尼的正规方程组来找到：\n$$\n(J_k^T J_k + \\lambda I) h_{lm} = -J_k^T r_k\n$$\n其中 $I$ 是单位矩阵。项 $\\lambda I$ 是一个正则化项。\n- 如果 $\\lambda$ 很小，步长 $h_{lm}$ 近似于Gauss-Newton步 $h_{gn}$。\n- 如果 $\\lambda$ 很大，项 $\\lambda I$ 在 $J_k^T J_k$ 中占主导地位，方程近似为 $\\lambda I h_{lm} \\approx -J_k^T r_k$，这意味着 $h_{lm} \\approx -\\frac{1}{\\lambda} (J_k^T r_k)$。由于目标函数的梯度是 $\\nabla F(p_k) = J_k^T r_k$，步长变成一个在最速下降方向上的小步。\n\n因此，参数 $\\lambda$ 自适应地将快速收敛的Gauss-Newton方法与稳健但较慢的梯度下降方法融合在一起。\n\n### 4. 算法实现细节\n\n#### 数值雅可比矩阵近似\n雅可比矩阵 $J_k$ 在每次迭代中使用**中心有限差分**来计算。对于每个参数 $p_j$（向量 $p$ 的第 $j$ 个分量），计算一个小的扰动 $\\delta_j$：\n$$\n\\delta_j = \\sqrt{\\varepsilon} (1 + |p_j|)\n$$\n其中 $\\varepsilon$ 是双精度浮点数的机器精度。雅可比矩阵的第 $j$ 列，代表了残差对参数 $p_j$ 的敏感度，然后近似为：\n$$\nJ_{\\cdot, j}(p_k) = \\frac{\\partial r(p_k)}{\\partial p_j} \\approx \\frac{r(p_k + \\delta_j e_j) - r(p_k - \\delta_j e_j)}{2\\delta_j} = -\\frac{f(x, p_k + \\delta_j e_j) - f(x, p_k - \\delta_j e_j)}{2\\delta_j}\n$$\n其中 $e_j$ 是第 $j$ 个标准基向量。\n\n#### 阻尼参数自适应\n提议的步长 $h=h_{lm}$ 的成功与否通过一个**增益比** $\\rho$ 来评估。该比率将目标函数的实际减少量与局部二次模型 $L(h)$ 预测的减少量进行比较。\n- **实际减少量**：$\\Delta F_{actual} = F(p_k) - F(p_k + h)$\n- **预测减少量**：$\\Delta F_{pred} = L(0) - L(h) = -h^T J_k^T r_k - \\frac{1}{2}h^T (J_k^T J_k) h$\n\n增益比为 $\\rho = \\frac{\\Delta F_{actual}}{\\Delta F_{pred}}$。自适应策略如下：\n1. 如果 $\\rho$ 是正的且显著（例如，$\\rho > 10^{-4}$），则步长是“有效的”。接受该步长（$p_{k+1} = p_k + h$），并减小阻尼 $\\lambda$ 以在下一次迭代中更接近更快的Gauss-Newton方法。一个常见的更新规则是 $\\lambda \\leftarrow \\lambda \\cdot \\max(1/3, 1-(2\\rho-1)^3)$。\n2. 如果 $\\rho$ 很小或为负，则步长是“无效的”。拒绝该步长（$p_{k+1} = p_k$），并增加阻尼 $\\lambda$ 以朝着更稳健的梯度下降方向移动。然后算法在同一次迭代 $k$ 中用更大的 $\\lambda$ 重新计算步长 $h$。一个常见的规则是 $\\lambda \\leftarrow \\lambda \\cdot v$，其中 $v$ 是一个乘法因子，通常从 $v=2$ 开始，并在连续拒绝时增加。\n\n$\\lambda$ 的初始值根据问题的规模来选择，例如，$\\lambda_0 = \\tau \\cdot \\max_{ii}((J_0^T J_0)_{ii})$，其中 $\\tau$ 是一个小的常数（例如 $10^{-4}$）。\n\n#### 终止准则\n当满足以下条件之一时，迭代过程终止，表明已找到一个满意的解：\n1. **小梯度**：目标函数梯度的量级接近于零：$\\lVert J_k^T r_k \\rVert_{\\infty}  \\epsilon_g$。\n2. **小参数更新**：参数向量的相对变化可以忽略不计：$\\lVert p_{k+1} - p_k \\rVert_2  \\epsilon_p (\\lVert p_k \\rVert_2 + \\epsilon_p)$。\n3. **小目标函数变化**：目标函数值的相对变化可以忽略不计：$|F(p_{k+1}) - F(p_k)|  \\epsilon_f (F(p_k) + \\epsilon_f)$。\n4. **最大迭代次数**：达到预定的最大迭代次数 $k_{max}$，以防止无限循环。\n\n这里，$\\epsilon_g, \\epsilon_p, \\epsilon_f$ 是小的容差值（例如，$10^{-8}$）。\n\n### 5. Levenberg-Marquardt算法总结\n完整的算法如下：\n\n1. **初始化**：\n    - 选择一个初始参数猜测 $p_0$。\n    - 设置容差 $\\epsilon_g, \\epsilon_p, \\epsilon_f$ 和最大迭代次数 $k_{max}$。\n    - 计算初始残差 $r_0 = y - f(x, p_0)$ 和目标函数 $F_0 = \\frac{1}{2}r_0^T r_0$。\n    - 计算初始雅可比矩阵 $J_0$ 和梯度 $g_0 = J_0^T r_0$。\n    - 初始化阻尼参数 $\\lambda_0$ 和增加因子 $v$。\n    - 设置迭代计数器 $k = 0$。\n\n2. **主循环**：当终止准则未满足时：\n    a. 求解阻尼正规方程组 $(J_k^T J_k + \\lambda_k I) h = -g_k$ 以获得步长 $h$。\n    b. 评估提议的新参数向量 $p_{new} = p_k + h$。\n    c. 计算增益比 $\\rho = \\frac{F(p_k) - F(p_{new})}{\\Delta F_{pred}}$。\n    d. **如果 $\\rho > \\epsilon_{\\rho}$**：\n        i. 接受步长：$p_{k+1} = p_{new}$。\n        ii. 更新目标函数 $F_{k+1}$、残差 $r_{k+1}$、雅可比矩阵 $J_{k+1}$ 和梯度 $g_{k+1}$。\n        iii. 减小阻尼：$\\lambda_{k+1} = \\lambda_k \\cdot \\max(1/3, 1-(2\\rho-1)^3)$ 并重置 $v=2$。\n        iv. 检查关于步长和目标函数变化的终止准则。\n        v. 增加 $k \\leftarrow k+1$。\n    e. **否则 ($\\rho \\le \\epsilon_{\\rho}$)**：\n        i. 拒绝步长。参数不更新。\n        ii. 增加阻尼：$\\lambda_k \\leftarrow \\lambda_k \\cdot v$，然后 $v \\leftarrow 2v$。\n        iii. 从步骤 2a 开始，使用新的 $\\lambda_k$ 重复。\n\n3. **终止**：返回最终的参数向量 $p_k$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef levenberg_marquardt(model_func, p_init, x_data, y_data, max_iter=100, tol_g=1e-8, tol_p=1e-8, tol_f=1e-8):\n    \"\"\"\n    Implements the Levenberg-Marquardt algorithm for nonlinear least squares.\n\n    Args:\n        model_func: The model function f(x, p).\n        p_init: Initial guess for the parameters.\n        x_data: Independent variable data.\n        y_data: Dependent variable data (observations).\n        max_iter: Maximum number of iterations.\n        tol_g: Tolerance for the infinity norm of the gradient.\n        tol_p: Tolerance for the parameter update step.\n        tol_f: Tolerance for the change in the objective function.\n\n    Returns:\n        The fitted parameter vector.\n    \"\"\"\n    p_k = np.array(p_init, dtype=float)\n    m = len(p_k)\n    eps = np.finfo(np.float64).eps\n\n    # Initial evaluation\n    r_k = y_data - model_func(x_data, p_k)\n    F_k = 0.5 * np.dot(r_k, r_k)\n\n    # Jacobian calculation function\n    def calculate_jacobian(p):\n        J = np.zeros((len(x_data), m))\n        for j in range(m):\n            delta = np.sqrt(eps) * (1.0 + np.abs(p[j]))\n            p_plus = p.copy()\n            p_minus = p.copy()\n            p_plus[j] += delta\n            p_minus[j] -= delta\n            \n            # Central difference for jacobian of residual r = y - f\n            # dr/dp = -df/dp\n            # Using (f(p - d) - f(p + d)) / 2d to get -df/dp\n            f_plus = model_func(x_data, p_plus)\n            f_minus = model_func(x_data, p_minus)\n\n            J[:, j] = (f_minus - f_plus) / (2.0 * delta)\n        return J\n\n    J_k = calculate_jacobian(p_k)\n    g_k = np.dot(J_k.T, r_k)\n\n    if np.linalg.norm(g_k, np.inf) <= tol_g:\n        return p_k\n\n    # Initial damping parameter\n    A = np.dot(J_k.T, J_k)\n    tau = 1e-4\n    mu = tau * np.max(np.diag(A))\n    nu = 2.0\n    \n    k = 0\n    while k < max_iter:\n        \n        while True:\n            H_lm = A + mu * np.identity(m)\n            try:\n                h = np.linalg.solve(H_lm, -g_k)\n            except np.linalg.LinAlgError:\n                mu *= nu\n                nu *= 2.0\n                continue\n\n            p_new = p_k + h\n            r_new = y_data - model_func(x_data, p_new)\n            F_new = 0.5 * np.dot(r_new, r_new)\n\n            # Predicted reduction\n            # pred_red = - (h^T * g_k + 0.5 * h^T * A * h)\n            pred_red = - (np.dot(h, g_k) + 0.5 * np.dot(h, np.dot(A, h)))\n            \n            # Gain ratio\n            actual_red = F_k - F_new\n            \n            if pred_red > 0: # Avoid division by zero or negative\n                rho = actual_red / pred_red\n            else:\n                rho = -1.0\n\n            if rho > 0:  # Step is accepted\n                # Check termination criteria on parameter and function change\n                p_norm = np.linalg.norm(p_k)\n                if np.linalg.norm(h) <= tol_p * (p_norm + tol_p):\n                    return p_new\n                \n                F_norm = F_k\n                if abs(actual_red) <= tol_f * (F_norm + tol_f):\n                    return p_new\n\n                p_k = p_new\n                F_k = F_new\n                r_k = r_new\n                \n                J_k = calculate_jacobian(p_k)\n                A = np.dot(J_k.T, J_k)\n                g_k = np.dot(J_k.T, r_k)\n\n                # Check termination on gradient\n                if np.linalg.norm(g_k, np.inf) <= tol_g:\n                    return p_k\n\n                mu = mu * max(1/3.0, 1 - (2*rho - 1)**3)\n                nu = 2.0\n                break # Exit inner loop and start next iteration\n            else: # Step is rejected\n                mu *= nu\n                nu *= 2.0\n        \n        k += 1\n\n    return p_k\n\n\ndef solve():\n    \"\"\"\n    Defines and solves the three test cases specified in the problem.\n    \"\"\"\n    \n    # Test Case 1: Exponential Model\n    def model1(x, p):\n        return p[0] * np.exp(p[1] * x) + p[2]\n    \n    x1 = np.linspace(0.0, 3.0, 31)\n    y1 = 2.0 * np.exp(-0.7 * x1) + 0.5 + 0.01 * np.sin(5 * x1)\n    p_init1 = [1.0, -0.2, 0.0]\n    \n    # Test Case 2: Logistic Model\n    def model2(x, p):\n        return p[0] / (1.0 + np.exp(-p[1] * (x - p[2])))\n        \n    x2 = np.linspace(-1.5, 2.0, 15)\n    y2 = 1.5 / (1.0 + np.exp(-3.0 * (x2 - 0.5))) + 0.02 * np.cos(3 * x2)\n    p_init2 = [1.0, 1.0, 0.0]\n    \n    # Test Case 3: Sinusoidal Model\n    def model3(x, p):\n        omega = 2.5\n        return p[0] * np.cos(omega * x + p[1]) + p[2]\n        \n    x3 = np.linspace(0.0, 2.0, 21)\n    y3 = 0.8 * np.cos(2.5 * x3 + 0.4) - 0.05 + 0.01 * np.sin(7 * x3)\n    p_init3 = [0.5, 0.0, 0.0]\n\n    test_cases = [\n        (model1, p_init1, x1, y1),\n        (model2, p_init2, x2, y2),\n        (model3, p_init3, x3, y3),\n    ]\n\n    results = []\n    for model_func, p_init, x_data, y_data in test_cases:\n        p_fit = levenberg_marquardt(model_func, p_init, x_data, y_data)\n        rounded_p = [round(p, 6) for p in p_fit]\n        results.append(str(rounded_p).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}