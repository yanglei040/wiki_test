{
    "hands_on_practices": [
        {
            "introduction": "The true power of an interpolation method lies in its ability to reconstruct a continuous function from a discrete set of samples. This first practice frames trigonometric interpolation as a \"super-resolution\" engine, a tool to generate a plausible, high-resolution periodic signal from coarse, equispaced measurements. By implementing the fundamental pipeline of Fourier analysis—transforming data to the frequency domain, manipulating coefficients, and transforming back—you will gain hands-on experience with the core mechanics of trigonometric interpolation and its most direct application .",
            "id": "3284404",
            "problem": "You are tasked with designing and implementing a principle-based \"super-resolution\" algorithm for periodic signals via trigonometric interpolation, derived from first principles in numerical methods and scientific computing. Consider real-valued, continuous functions that are periodic with period $2\\pi$ on the domain $[0,2\\pi]$ (angles in radians). Your algorithm must generate a high-resolution approximation of the underlying periodic signal from low-resolution, equispaced samples by constructing a trigonometric interpolant that is consistent with the given samples and extends naturally across the domain. The derivation and algorithm must start from the fundamental base of the Fourier series definition and the discrete sampling of periodic functions. You must ensure numerical soundness without assuming shortcut formulas beyond these foundational definitions.\n\nDefinitions and requirements:\n- Let $f:[0,2\\pi]\\to\\mathbb{R}$ be a $2\\pi$-periodic function. The Fourier series formalism states that $f(x)$ can be expressed in terms of complex exponentials $e^{\\mathrm{i}kx}$, where $k\\in\\mathbb{Z}$, and Fourier coefficients $\\hat{f}_k$ characterizing the contribution of each mode.\n- Given $N$ equispaced samples $f(x_j)$ at nodes $x_j = 2\\pi j/N$ for $j\\in\\{0,1,\\dots,N-1\\}$, a trigonometric interpolant is a finite trigonometric polynomial whose values match the sampled data at these nodes. Using the definition of Fourier series and the structure of discrete sampling on a uniform grid, the coefficients of this interpolant are obtained from the discrete Fourier transform of the sample values, up to scaling consistent with the inverse relationship between sample values and coefficients. The trigonometric interpolant, when evaluated at arbitrary points $x$ in $[0,2\\pi]$, provides a \"super-resolution\" reconstruction in the sense that it yields a plausible high-resolution periodic signal consistent with the coarse samples.\n- Your numerical procedure must compute the discrete frequency coefficients from the coarse samples using the fundamental relationships implied by Fourier analysis, and then evaluate the corresponding trigonometric interpolant on a finer grid of $M$ points $y_m = 2\\pi m/M$ for $m\\in\\{0,1,\\dots,M-1\\}$.\n\nAngle unit specification:\n- All angles are to be interpreted in radians.\n\nTest suite:\nImplement your algorithm and evaluate it on the following four test cases. For each case, use the specified $f(x)$, number of low-resolution samples $N$, and desired high-resolution evaluation count $M$.\n\n1. Happy path (band-limited):\n   - $f(x) = \\sin(3x) + 0.5\\cos(5x)$\n   - $N = 32$\n   - $M = 1024$\n\n2. Boundary case with minimal sampling and the Nyquist component:\n   - $f(x) = \\cos(x)$\n   - $N = 2$\n   - $M = 128$\n\n3. Aliasing present (frequency beyond Nyquist limit in the coarse grid):\n   - $f(x) = \\sin(20x) + 0.3\\cos(2x)$\n   - $N = 16$\n   - $M = 1024$\n\n4. Non-smooth periodic signal (piecewise-linear sawtooth on $[0,2\\pi)$):\n   - $f(x) = \\left(\\frac{(x \\bmod 2\\pi)}{\\pi}\\right) - 1$\n   - $N = 64$\n   - $M = 2048$\n\nFor each test case, compute the following two quantitative metrics:\n- Consistency error on the coarse grid: the maximum absolute difference between the trigonometric interpolant evaluated at the coarse nodes and the given coarse samples ($\\max_j |p(x_j) - f(x_j)|$). This must be a float.\n- High-resolution error: the maximum absolute difference between the trigonometric interpolant evaluated on the fine grid and the true function values on the same fine grid ($\\max_m |p(y_m) - f(y_m)|$). This must be a float.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should contain eight floats, in the following order for the four test cases: $[\\text{consistency}_1,\\text{hires}_1,\\text{consistency}_2,\\text{hires}_2,\\text{consistency}_3,\\text{hires}_3,\\text{consistency}_4,\\text{hires}_4]$.\n- No additional text or characters are permitted in the output line.\n\nAll computations must be performed in radians with the specified period $2\\pi$. Ensure numerical stability and correctness based on the foundational definitions of Fourier series and discrete sampling for trigonometric interpolation. The program must be complete and runnable without external input or resources.",
            "solution": "The problem requires the design and implementation of a trigonometric interpolation algorithm for signal super-resolution, derived from first principles. The algorithm will reconstruct a high-resolution periodic signal from a set of low-resolution, equispaced samples.\n\n### 1. Problem Validation\n\n**Step 1: Extract Givens**\n- **Function:** A real-valued, continuous function $f(x)$ periodic with period $2\\pi$ on the domain $[0, 2\\pi]$.\n- **Low-Resolution Sampling:** $N$ equispaced samples $f(x_j)$ at nodes $x_j = 2\\pi j/N$ for $j \\in \\{0, 1, \\dots, N-1\\}$.\n- **High-Resolution Evaluation:** The interpolant is to be evaluated on a finer grid of $M$ points $y_m = 2\\pi m/M$ for $m \\in \\{0, 1, \\dots, M-1\\}$.\n- **Core Principle:** The interpolant's coefficients are derived from the discrete Fourier transform (DFT) of the sample values.\n- **Test Cases:**\n    1. $f(x) = \\sin(3x) + 0.5\\cos(5x)$, $N = 32$, $M = 1024$.\n    2. $f(x) = \\cos(x)$, $N = 2$, $M = 128$.\n    3. $f(x) = \\sin(20x) + 0.3\\cos(2x)$, $N = 16$, $M = 1024$.\n    4. $f(x) = (\\frac{(x \\bmod 2\\pi)}{\\pi}) - 1$, $N = 64$, $M = 2048$.\n- **Metrics:**\n    - Consistency Error: $\\max_j |p(x_j) - f(x_j)|$\n    - High-Resolution Error: $\\max_m |p(y_m) - f(y_m)|$\n- **Angle Unit:** All angles are in radians.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in numerical analysis and signal processing, based on the established theory of Fourier analysis and the Discrete Fourier Transform. The givens are complete, consistent, and formalizable into a solvable numerical problem. The test cases are well-chosen to probe different aspects of trigonometric interpolation: a band-limited case, a Nyquist boundary case, an aliasing case, and a non-smooth case. There are no scientific flaws, ambiguities, or contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be developed.\n\n### 2. Derivation from First Principles\n\nOur objective is to find a trigonometric polynomial $p(x)$ that interpolates the given data points $(x_j, y_j)$ where $y_j = f(x_j)$. This means $p(x_j) = y_j$ for $j = 0, \\dots, N-1$.\n\nA general $2\\pi$-periodic function $f(x)$ can be represented by its Fourier series:\n$$\nf(x) = \\sum_{k=-\\infty}^{\\infty} \\hat{f}_k e^{\\mathrm{i}kx}\n$$\nwhere the Fourier coefficients $\\hat{f}_k$ are given by:\n$$\n\\hat{f}_k = \\frac{1}{2\\pi} \\int_0^{2\\pi} f(t) e^{-\\mathrm{i}kt} dt\n$$\nSince we only have samples of $f(x)$ at discrete points $x_j$, we cannot compute this integral directly. We can, however, approximate it using a numerical quadrature rule, specifically a Riemann sum over the uniform grid points $x_j$:\n$$\n\\hat{f}_k \\approx \\frac{1}{2\\pi} \\sum_{j=0}^{N-1} f(x_j) e^{-\\mathrm{i}kx_j} \\Delta x\n$$\nWith $x_j = 2\\pi j/N$ and $\\Delta x = 2\\pi/N$, this becomes:\n$$\n\\hat{f}_k \\approx \\frac{1}{2\\pi} \\sum_{j=0}^{N-1} y_j e^{-\\mathrm{i}k(2\\pi j/N)} \\left(\\frac{2\\pi}{N}\\right) = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j e^{-2\\pi\\mathrm{i}kj/N}\n$$\nThe expression on the right is the definition of the scaled Discrete Fourier Transform (DFT). Let us define the discrete Fourier coefficients $c_k$ for $k=0, \\dots, N-1$ as:\n$$\nc_k = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j e^{-2\\pi\\mathrm{i}kj/N}\n$$\nThe trigonometric interpolant $p(x)$ is a finite trigonometric polynomial constructed using a limited set of frequency modes that can be uniquely determined from the $N$ samples. For $N$ samples, we can resolve $N$ frequency components. For real-valued data $y_j$, the coefficients exhibit conjugate symmetry: $c_{N-k} = \\overline{c_k}$.\n\nFor $N$ even, the set of unique integer frequencies that can be resolved are $k \\in \\{0, \\pm 1, \\dots, \\pm(N/2-1), N/2\\}$. On the sampling grid, the modes $e^{\\mathrm{i}(N/2)x_j}$ and $e^{-\\mathrm{i}(N/2)x_j}$ are identical, as $e^{\\pm\\mathrm{i}(N/2)x_j} = e^{\\pm\\mathrm{i}\\pi j} = (-1)^j$. Thus, they cannot be distinguished, and we combine them into a single cosine term.\n\nThe trigonometric interpolant $p(x)$ for an even number of points $N$ is given by:\n$$\np(x) = \\sideset{}{'}\\sum_{k=-N/2}^{N/2} \\hat{c}_k e^{\\mathrm{i}kx}\n$$\nwhere the prime on the sum indicates that the first ($k=-N/2$) and last ($k=N/2$) terms are weighted by $1/2$. The coefficients $\\hat{c}_k$ are obtained by re-indexing the DFT coefficients $c_k$:\n- $\\hat{c}_k = c_k$ for $k = 0, 1, \\dots, N/2$.\n- $\\hat{c}_k = c_{k+N}$ for $k = -1, \\dots, -N/2+1$.\n\nDue to the conjugate symmetry property ($c_{N-k} = \\overline{c_k}$), we have $\\hat{c}_{-k} = \\overline{\\hat{c}_k}$. This ensures that $p(x)$ is real-valued. The Nyquist coefficient $\\hat{c}_{N/2} = c_{N/2}$ is real for real data.\nThe sum can be expanded as:\n$$\np(x) = \\hat{c}_0 + \\sum_{k=1}^{N/2-1} (\\hat{c}_k e^{\\mathrm{i}kx} + \\hat{c}_{-k} e^{-\\mathrm{i}kx}) + \\frac{1}{2}(\\hat{c}_{N/2} e^{\\mathrm{i}(N/2)x} + \\hat{c}_{-N/2} e^{-\\mathrm{i}(N/2)x})\n$$\nUsing $\\hat{c}_{-k} = \\overline{\\hat{c}_k}$ and $\\hat{c}_{-N/2} = \\hat{c}_{N/2}$ (since $c_{N/2}$ is real), this simplifies to:\n$$\np(x) = \\hat{c}_0 + \\sum_{k=1}^{N/2-1} 2 \\text{Re}(\\hat{c}_k e^{\\mathrm{i}kx}) + \\hat{c}_{N/2} \\cos(Nx/2)\n$$\nThis form guarantees $p(x_j) = y_j$ for all $j=0, \\dots, N-1$.\n\n### 3. Super-resolution via Zero-Padding in Fourier Space\n\nTo evaluate $p(x)$ on a fine grid of $M$ points, $y_m=2\\pi m/M$, we could compute the sum directly. However, a more efficient method is to use the Inverse Fast Fourier Transform (IFFT). This is achieved by creating a larger vector of $M$ Fourier coefficients, where the original coefficients are placed at their corresponding frequency locations and the additional high-frequency coefficients are set to zero. This procedure is called zero-padding.\n\nLet `c_low` be the array of $N$ unscaled DFT coefficients, computed via `fft(y)`. The new $M$-point coefficient array, `c_high`, is constructed as follows (for $N, M$ even):\n1.  The coefficients for non-negative frequencies up to the original Nyquist limit are copied:\n    `c_high[0 : N//2] = c_low[0 : N//2]`\n2.  The coefficients for negative frequencies are copied to their corresponding locations in the new array:\n    `c_high[M - N//2 + 1 : M] = c_low[N//2 + 1 : N]`\n3.  The original Nyquist frequency $k=N/2$ is no longer a Nyquist frequency in the $M$-point context. Its energy must be split between the modes for $k=N/2$ and $k=-N/2$.\n    `c_high[N//2] = c_low[N//2] / 2.0`\n    `c_high[M - N//2] = c_low[N//2] / 2.0`\n4.  All other entries of `c_high` remain zero, which assumes the underlying signal contains no frequencies with magnitude greater than $N/2$.\n\nThe interpolated values $p(y_m)$ are then obtained by applying a scaled IFFT:\n$$\np_m = \\frac{M}{N} \\text{real}(\\text{ifft}(\\text{c\\_high}))_m\n$$\nThe scaling factor $M/N$ accounts for the scaling conventions of the DFT ($1/N$ in our definition) and the IFFT ($1/M$ in the standard algorithm).\n\nThis procedure yields the values of the trigonometric interpolant $p(x)$ at the $M$ high-resolution grid points, thus achieving \"super-resolution\".\n\n### 4. Algorithmic Implementation\n\nThe overall algorithm is as follows:\n1.  **Input:** A function definition $f(x)$, a low-resolution sample count $N$, and a high-resolution sample count $M$.\n2.  **Sample:** Generate the coarse grid $x_j = 2\\pi j/N$ and the sample values $y_j = f(x_j)$ for $j=0, \\dots, N-1$.\n3.  **DFT:** Compute the unscaled DFT of the samples: `c_low = np.fft.fft(y_j)`.\n4.  **Zero-Pad:** Construct the $M$-point coefficient array `c_high` from `c_low` using the logic described in Section 3.\n5.  **IDFT:** Compute the high-resolution signal via scaled IFFT: `p_highres = (M/N) * np.real(np.fft.ifft(c_high))`.\n6.  **Calculate Errors:**\n    a.  **Consistency Error:** By construction, the trigonometric interpolant must match the samples perfectly. The error $p(x_j) - y_j$ is evaluated by subsampling the high-resolution result: `p_highres[::M//N]`. The maximum absolute difference is computed. This error should be on the order of machine precision.\n    b.  **High-Resolution Error:** Generate the true function values on the fine grid, $f(y_m)$, and compute the maximum absolute difference between `p_highres` and these true values.\n\nThis algorithm will now be applied to the four test cases specified in the problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trigonometric interpolation problem for a suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda x: np.sin(3 * x) + 0.5 * np.cos(5 * x),\n            \"N\": 32,\n            \"M\": 1024,\n            \"name\": \"Happy path (band-limited)\"\n        },\n        {\n            \"f\": lambda x: np.cos(x),\n            \"N\": 2,\n            \"M\": 128,\n            \"name\": \"Boundary case with minimal sampling\"\n        },\n        {\n            \"f\": lambda x: np.sin(20 * x) + 0.3 * np.cos(2 * x),\n            \"N\": 16,\n            \"M\": 1024,\n            \"name\": \"Aliasing present\"\n        },\n        {\n            \"f\": lambda x: np.fmod(x, 2 * np.pi) / np.pi - 1.0,\n            \"N\": 64,\n            \"M\": 2048,\n            \"name\": \"Non-smooth periodic signal\"\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        f, N, M = case[\"f\"], case[\"N\"], case[\"M\"]\n\n        # 1. Generate low-resolution grid and sample the function.\n        # x_j = 2 * pi * j / N for j = 0, ..., N-1\n        x_coarse = 2 * np.pi * np.arange(N) / N\n        y_coarse = f(x_coarse)\n\n        # 2. Compute the DFT of the low-resolution samples.\n        # The coefficients are c_k = (1/N) * sum(y_j * exp(-2*pi*i*j*k/N)).\n        # We compute the unscaled DFT first: c_low = fft(y_coarse).\n        c_low = np.fft.fft(y_coarse)\n\n        # 3. Create high-resolution coefficients by zero-padding in Fourier space.\n        # This is the core of the \"super-resolution\" process.\n        c_high = np.zeros(M, dtype=np.complex128)\n\n        # All test cases have N as an even number.\n        # Positive frequencies (and DC component at k=0)\n        # Indices 0 to N/2 - 1 for freqs k=0,...,N/2-1\n        k_pos_limit = N // 2\n        c_high[0:k_pos_limit] = c_low[0:k_pos_limit]\n\n        # Negative frequencies go at the end of the array for 'fft' convention\n        # Indices N/2 + 1 to N - 1 correspond to freqs k = -N/2+1,...,-1\n        # These are mapped to indices M-N/2+1 to M-1 in the high-res array.\n        k_neg_start = N // 2 + 1\n        c_high[M - (N - k_neg_start):] = c_low[k_neg_start:]\n\n        # The Nyquist frequency k = N/2 component must be split.\n        # Its energy is distributed between the +k and -k modes in the new basis.\n        # In the original N-point DFT, +N/2 and -N/2 were indistinguishable.\n        nyquist_idx = N // 2\n        nyquist_coeff = c_low[nyquist_idx]\n        c_high[nyquist_idx] = nyquist_coeff / 2.0\n        c_high[M - nyquist_idx] = nyquist_coeff / 2.0\n        \n        # 4. Perform inverse FFT to get the high-resolution signal.\n        # The scaling factor M/N arises from our coefficient definition (1/N)\n        # and the standard IFFT algorithm's scaling (1/M).\n        # We want p(x) = sum(c_k * exp(ikx)), where c_k is the scaled coefficient.\n        # Our c_high is based on unscaled DFT.\n        # ifft(c_high) computes (1/M) * sum(c_high[k] * exp(2*pi*i*k*m/M)).\n        # We have c_high_k ~ N*c_k. So ifft(c_high) ~ (N/M) * p(y_m).\n        # Thus, p(y_m) = (M/N) * ifft(c_high).\n        p_highres = (M / N) * np.real(np.fft.ifft(c_high))\n\n        # 5. Calculate the two required error metrics.\n        \n        # Consistency error on the coarse grid.\n        # The trigonometric interpolant must match the samples at the coarse nodes.\n        # The coarse nodes correspond to every M/N-th sample of the fine grid.\n        # Due to floating-point arithmetic, this will be close to zero.\n        subsample_step = M // N\n        p_coarse_from_highres = p_highres[::subsample_step]\n        consistency_error = np.max(np.abs(p_coarse_from_highres - y_coarse))\n        \n        # High-resolution error\n        # Compare the interpolant with the true function on the fine grid.\n        y_fine = 2 * np.pi * np.arange(M) / M\n        y_true_fine = f(y_fine)\n        hires_error = np.max(np.abs(p_highres - y_true_fine))\n        \n        results.append(consistency_error)\n        results.append(hires_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An interpolant, by definition, passes exactly through every data point. While this guarantees a perfect fit for the training data, it can be a significant drawback in the presence of measurement noise. This practice critically examines the trade-off between bias and variance by comparing exact interpolation with a truncated least-squares trigonometric model, a form of regression. You will discover firsthand how the \"perfect fit\" of interpolation can lead to overfitting, and why a simpler model often provides a better approximation of the true underlying signal .",
            "id": "3284517",
            "problem": "You are given the task of designing and implementing an experiment that distinguishes between trigonometric interpolation and least-squares trigonometric regression in the presence of noise, and to quantify their generalization performance on unseen points. All angles must be treated in radians.\n\nStarting from the following fundamental base:\n- The orthogonality of complex exponentials on a periodic interval, namely that the set $\\{e^{\\mathrm{i} k t}\\}_{k\\in\\mathbb{Z}}$ is orthogonal over a period.\n- The definition of a trigonometric interpolant as a periodic trigonometric polynomial that passes exactly through given data sampled at equally spaced nodes on a full period.\n- The definition of least-squares regression as the minimizer of the sum of squared residuals of a specified model class against the observed data.\n\nYou must:\n1) Derive the computational formulation for constructing the exact trigonometric interpolant of a periodic function sampled at $N$ equally spaced nodes over one period. The interpolant must exactly match the noisy samples at the training nodes and be representable as a finite trigonometric series. You may use the discrete Fourier transform as a computational device, but you must justify its use from orthogonality and interpolation conditions.\n2) Derive the computational formulation for fitting a truncated real trigonometric model of degree $M$ in the least-squares sense to the same training data. The model has the form\n$$\ns_M(t) \\;=\\; a_0 \\;+\\; \\sum_{k=1}^{M} \\big( a_k \\cos(kt) + b_k \\sin(kt) \\big),\n$$\nand its coefficients must be obtained by minimizing the sum of squared residuals on the training nodes.\n3) Implement both approaches and compare their out-of-sample performance by computing the root-mean-square error (RMSE) of each method against the true noise-free periodic function on a dense test grid.\n\nData generation protocol (angles in radians):\n- Training nodes: for each case use $t_j = 2\\pi j / N$ for $j=0,1,\\dots,N-1$.\n- A true periodic target function is specified as\n$$\nf(t) \\;=\\; a_0 \\;+\\; \\sum_{k=1}^{K} \\big( a_k \\cos(kt) + b_k \\sin(kt) \\big).\n$$\n- Observations are noisy: $y_j = f(t_j) + \\varepsilon_j$, where $\\varepsilon_j$ are independent and identically distributed normal random variables with mean $0$ and standard deviation $\\sigma$, generated using a fixed random seed for reproducibility.\n- The test grid must be a set of $T$ equally spaced points on one period: $t^{\\ast}_m = 2\\pi m / T$ for $m=0,1,\\dots,T-1$. Use $T = 4096$.\n\nPerformance metric:\n- For each method, compute the out-of-sample root-mean-square error\n$$\n\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{T} \\sum_{m=0}^{T-1} \\big( \\hat{f}(t^{\\ast}_m) - f(t^{\\ast}_m) \\big)^2 },\n$$\nwhere $\\hat{f}$ is either the trigonometric interpolant or the truncated least-squares fit. Lower $\\mathrm{RMSE}$ indicates better generalization to the true underlying function.\n\nYour program must implement the above and evaluate the following test suite of cases. In every case, initialize a pseudorandom number generator with the same seed $314159$ before generating noise so that results are reproducible. For each case, report a boolean indicating whether the least-squares truncated model generalizes better than the exact interpolant, i.e., whether $\\mathrm{RMSE}_{\\mathrm{LS}} < \\mathrm{RMSE}_{\\mathrm{Interp}}$.\n\nTest suite with coefficients and parameters:\n- Case $1$: $N = 64$, $K = 3$, $\\sigma = 0.2$, $M = 3$, $a_0 = 0.1$, with\n  $a_k$ for $k = 1,2,3$ equal to $\\{1.0,\\,0.5,\\,0.2\\}$ and\n  $b_k$ for $k = 1,2,3$ equal to $\\{0.5,\\,-0.3,\\,0.1\\}$.\n- Case $2$: $N = 64$, $K = 6$, $\\sigma = 0.0$, $M = 3$, $a_0 = 0.0$, with\n  $a_k$ for $k = 1,2,3,4,5,6$ equal to $\\{0.7,\\,-0.4,\\,0.3,\\,0.2,\\,-0.1,\\,0.05\\}$ and\n  $b_k$ for $k = 1,2,3,4,5,6$ equal to $\\{-0.2,\\,0.3,\\,-0.1,\\,0.25,\\,0.2,\\,-0.05\\}$.\n- Case $3$: $N = 64$, $K = 5$, $\\sigma = 1.0$, $M = 3$, $a_0 = 0.2$, with\n  $a_k$ for $k = 1,2,3,4,5$ equal to $\\{0.9,\\,-0.6,\\,0.4,\\,-0.3,\\,0.15\\}$ and\n  $b_k$ for $k = 1,2,3,4,5$ equal to $\\{0.5,\\,0.4,\\,-0.2,\\,0.3,\\,-0.1\\}$.\n- Case $4$: $N = 16$, $K = 3$, $\\sigma = 0.5$, $M = 3$, $a_0 = -0.1$, with\n  $a_k$ for $k = 1,2,3$ equal to $\\{0.9,\\,0.4,\\,-0.2\\}$ and\n  $b_k$ for $k = 1,2,3$ equal to $\\{-0.4,\\,0.35,\\,0.15\\}$.\n\nImplementation requirements:\n- Construct the exact trigonometric interpolant using its defining property of matching all training samples and the orthogonality of complex exponentials over a period. You may compute its coefficients using the discrete Fourier transform suitable for equally spaced samples and evaluate the interpolant on the dense test grid.\n- Construct the least-squares truncated real trigonometric fit of degree $M$ by solving the linear least-squares problem for the coefficients $\\{a_0,a_1,\\dots,a_M,b_1,\\dots,b_M\\}$ using the training data, and then evaluate the fitted model on the dense test grid.\n- For each case, compute both $\\mathrm{RMSE}_{\\mathrm{Interp}}$ and $\\mathrm{RMSE}_{\\mathrm{LS}}$ against the true $f(t)$ on the test grid.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases above. Each entry must be a boolean, equal to $\\mathrm{True}$ if and only if $\\mathrm{RMSE}_{\\mathrm{LS}} < \\mathrm{RMSE}_{\\mathrm{Interp}}$ for that case, and $\\mathrm{False}$ otherwise. For example: `[True,False,True,True]`.",
            "solution": "### Step 1: Extract Givens\n- **Task**: Design and implement an experiment to compare trigonometric interpolation and least-squares trigonometric regression. All angles are in radians.\n- **Fundamental Base**:\n    - Orthogonality of $\\{e^{\\mathrm{i} k t}\\}_{k\\in\\mathbb{Z}}$ over a periodic interval.\n    - Trigonometric interpolant: periodic trigonometric polynomial passing exactly through data sampled at $N$ equally spaced nodes on a full period.\n    - Least-squares regression: minimizer of the sum of squared residuals for a specified model class.\n\n- **Derivation Requirements**:\n    1.  Computational formulation for the exact trigonometric interpolant for $N$ equally spaced nodes, using DFT and justifying its use.\n    2.  Computational formulation for a truncated real trigonometric model of degree $M$ fitted via least squares:\n        $$\n        s_M(t) \\;=\\; a_0 \\;+\\; \\sum_{k=1}^{M} \\big( a_k \\cos(kt) + b_k \\sin(kt) \\big)\n        $$\n\n- **Implementation and Comparison**:\n    - Implement both approaches.\n    - Compare out-of-sample performance using root-mean-square error (RMSE) against the true noise-free function on a dense test grid.\n\n- **Data Generation Protocol**:\n    - Training nodes: $t_j = 2\\pi j / N$ for $j=0,1,\\dots,N-1$.\n    - True target function: $f(t) = a_0 + \\sum_{k=1}^{K} \\big( a_k \\cos(kt) + b_k \\sin(kt) \\big)$.\n    - Noisy observations: $y_j = f(t_j) + \\varepsilon_j$, where $\\varepsilon_j \\sim \\mathcal{N}(0, \\sigma^2)$, i.i.d.\n    - Random seed for noise generation: $314159$.\n    - Test grid: $t^{\\ast}_m = 2\\pi m / T$ for $m=0,1,\\dots,T-1$, with $T = 4096$.\n\n- **Performance Metric**:\n    - RMSE: $\\sqrt{\\frac{1}{T} \\sum_{m=0}^{T-1} \\big( \\hat{f}(t^{\\ast}_m) - f(t^{\\ast}_m) \\big)^2 }$, where $\\hat{f}$ is the interpolant or the least-squares fit.\n\n- **Final Output**: A boolean list, where an entry is $\\mathrm{True}$ if $\\mathrm{RMSE}_{\\mathrm{LS}} < \\mathrm{RMSE}_{\\mathrm{Interp}}$.\n\n- **Test Suite**:\n    - Case $1$: $N = 64$, $K = 3$, $\\sigma = 0.2$, $M = 3$, $a_0 = 0.1$, $a_k = \\{1.0,\\,0.5,\\,0.2\\}$, $b_k = \\{0.5,\\,-0.3,\\,0.1\\}$.\n    - Case $2$: $N = 64$, $K = 6$, $\\sigma = 0.0$, $M = 3$, $a_0 = 0.0$, $a_k = \\{0.7,\\,-0.4,\\,0.3,\\,0.2,\\,-0.1,\\,0.05\\}$, $b_k = \\{-0.2,\\,0.3,\\,-0.1,\\,0.25,\\,0.2,\\,-0.05\\}$.\n    - Case $3$: $N = 64$, $K = 5$, $\\sigma = 1.0$, $M = 3$, $a_0 = 0.2$, $a_k = \\{0.9,\\,-0.6,\\,0.4,\\,-0.3,\\,0.15\\}$, $b_k = \\{0.5,\\,0.4,\\,-0.2,\\,0.3,\\,-0.1\\}$.\n    - Case $4$: $N = 16$, $K = 3$, $\\sigma = 0.5$, $M = 3$, $a_0 = -0.1$, $a_k = \\{0.9,\\,0.4,\\,-0.2\\}$, $b_k = \\{-0.4,\\,0.35,\\,0.15\\}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard exercise in numerical analysis and applied Fourier analysis. It deals with the well-established concepts of trigonometric interpolation, least-squares approximation, and the bias-variance trade-off in statistical modeling. The problem is well-posed: it provides all necessary data, parameters, and definitions for each test case. The objective is clearly stated and uses precise, unambiguous language. The tasks are formalizable into a concrete computational experiment. The setup is self-contained and consistent. No scientific or logical flaws are present.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Principle-Based Solution\n\nThe problem requires a comparative analysis of two methods for approximating a periodic function from noisy samples: exact trigonometric interpolation and truncated least-squares trigonometric regression. This comparison highlights the fundamental trade-off between bias and variance in function approximation. Interpolation perfectly fits the training data, including noise, which often leads to a high-variance model that generalizes poorly (overfitting). In contrast, least-squares regression with a truncated model of degree $M$ acts as a low-pass filter, reducing variance by not fitting high-frequency noise, but potentially introducing bias if the true signal contains frequencies greater than $M$.\n\n**1. Formulation of Exact Trigonometric Interpolation**\n\nGiven a set of $N$ data points $(t_j, y_j)$, where $t_j = 2\\pi j/N$ for $j=0, 1, \\dots, N-1$, the goal is to find a trigonometric polynomial $p(t)$ that satisfies the interpolation condition $p(t_j) = y_j$ for all $j$. For an even number of points $N$, as specified in the test cases, the unique trigonometric polynomial of degree at most $N/2$ can be written in real form as:\n$$\np(t) = A_0 + \\sum_{k=1}^{N/2-1} \\left( A_k \\cos(kt) + B_k \\sin(kt) \\right) + A_{N/2} \\cos\\left(\\frac{N}{2}t\\right)\n$$\nThis polynomial has $1 + 2(N/2 - 1) + 1 = N$ degrees of freedom, corresponding to the coefficients $\\{A_k\\}_{k=0}^{N/2}$ and $\\{B_k\\}_{k=1}^{N/2-1}$.\n\nThe coefficients are determined by exploiting the discrete orthogonality properties of the trigonometric basis functions over the set of nodes $\\{t_j\\}$. These properties are a direct consequence of the orthogonality of complex exponentials on a discrete set of points. Specifically, for integers $k, l \\in [0, N/2]$:\n$$\n\\sum_{j=0}^{N-1} \\cos(kt_j)\\cos(lt_j) = \\begin{cases} N & k=l=0 \\text{ or } N/2 \\\\ N/2 & k=l \\in \\{1,\\dots,N/2-1\\} \\\\ 0 & k \\neq l \\end{cases}\n$$\n$$\n\\sum_{j=0}^{N-1} \\sin(kt_j)\\sin(lt_j) = \\begin{cases} N/2 & k=l \\in \\{1,\\dots,N/2-1\\} \\\\ 0 & k \\neq l \\end{cases}\n$$\n$$\n\\sum_{j=0}^{N-1} \\cos(kt_j)\\sin(lt_j) = 0 \\quad \\forall k, l\n$$\nBy substituting $t=t_j$ into the expression for $p(t)$, setting it equal to $y_j$, multiplying by a basis function (e.g., $\\cos(kt_j)$), and summing over all $j$, we can isolate each coefficient due to these orthogonality relations. This procedure yields the following formulas for the coefficients:\n$$\nA_0 = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j\n$$\n$$\nA_k = \\frac{2}{N} \\sum_{j=0}^{N-1} y_j \\cos(kt_j), \\quad k \\in \\{1, \\dots, N/2-1\\}\n$$\n$$\nB_k = \\frac{2}{N} \\sum_{j=0}^{N-1} y_j \\sin(kt_j), \\quad k \\in \\{1, \\dots, N/2-1\\}\n$$\n$$\nA_{N/2} = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j \\cos\\left(\\frac{N}{2}t_j\\right) = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j (-1)^j\n$$\nThese sums define the Discrete Fourier Transform (DFT). The coefficients can be computed efficiently using the Fast Fourier Transform (FFT) algorithm. Let $\\{C_k\\}_{k=0}^{N/2}$ be the result of a real-valued FFT of the sequence $\\{y_j\\}_{j=0}^{N-1}$. The coefficients are related to the FFT output as:\n$A_0 = C_0 / N$, $A_{N/2} = C_{N/2} / N$, and for $k=1, \\dots, N/2-1$:\n$$\nA_k = \\frac{2}{N} \\mathrm{Re}(C_k)\n$$\n$$\nB_k = -\\frac{2}{N} \\mathrm{Im}(C_k)\n$$\n\n**2. Formulation of Least-Squares Trigonometric Regression**\n\nThe second method involves fitting a trigonometric polynomial of a specified degree $M  N/2$ to the data. The model is:\n$$\ns_M(t) = a_0 + \\sum_{k=1}^{M} \\left( a_k \\cos(kt) + b_k \\sin(kt) \\right)\n$$\nThe goal is to find the coefficients $\\{a_k\\}_{k=0}^M$ and $\\{b_k\\}_{k=1}^M$ that minimize the sum of squared residuals:\n$$\nS = \\sum_{j=0}^{N-1} \\left( y_j - s_M(t_j) \\right)^2\n$$\nThis is a linear least-squares problem. Let the vector of unknown coefficients be $\\boldsymbol{\\theta} = [a_0, a_1, \\dots, a_M, b_1, \\dots, b_M]^T$. The problem can be written in matrix form as minimizing $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}\\|_2^2$, where $\\mathbf{y} = [y_0, \\dots, y_{N-1}]^T$ is the vector of observations, and $\\mathbf{X}$ is the $N \\times (1+2M)$ design matrix whose columns are the basis functions evaluated at the training nodes $t_j$:\n$$\n\\mathbf{X} = \\begin{bmatrix}\n1  \\cos(1 t_0)  \\dots  \\cos(M t_0)  \\sin(1 t_0)  \\dots  \\sin(M t_0) \\\\\n1  \\cos(1 t_1)  \\dots  \\cos(M t_1)  \\sin(1 t_1)  \\dots  \\sin(M t_1) \\\\\n\\vdots  \\vdots  \\ddots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n1  \\cos(1 t_{N-1})  \\dots  \\cos(M t_{N-1})  \\sin(1 t_{N-1})  \\dots  \\sin(M t_{N-1})\n\\end{bmatrix}\n$$\nThe solution $\\boldsymbol{\\theta}$ is found by solving the normal equations $\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} = \\mathbf{X}^T\\mathbf{y}$. Since $M  N/2$, the discrete orthogonality relations hold for the columns of $\\mathbf{X}$, making the matrix $\\mathbf{X}^T\\mathbf{X}$ diagonal:\n$$\n\\mathbf{X}^T\\mathbf{X} = \\mathrm{diag}\\left(N, \\frac{N}{2}, \\dots, \\frac{N}{2}, \\frac{N}{2}, \\dots, \\frac{N}{2}\\right)\n$$\nSolving for $\\boldsymbol{\\theta}$ gives expressions for the coefficients that are identical to the first $M$ Fourier coefficients of the full interpolation:\n$$\na_0 = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j\n$$\n$$\na_k = \\frac{2}{N} \\sum_{j=0}^{N-1} y_j \\cos(kt_j), \\quad k \\in \\{1, \\dots, M\\}\n$$\n$$\nb_k = \\frac{2}{N} \\sum_{j=0}^{N-1} y_j \\sin(kt_j), \\quad k \\in \\{1, \\dots, M\\}\n$$\nThus, the least-squares approximation with this model is equivalent to truncating the Fourier series of the data. While this analytical solution exists, a robust numerical implementation can directly solve the linear system using a standard least-squares solver, which is the approach adopted here.\n\n**3. Experimental Design and Evaluation**\n\nThe experiment will proceed as follows for each test case:\n1.  Generate the true function values $f(t_j)$ and noisy observations $y_j = f(t_j) + \\varepsilon_j$ at the $N$ training nodes.\n2.  Compute the coefficients for the exact trigonometric interpolant $p(t)$ using the FFT of $\\{y_j\\}$.\n3.  Solve the linear least-squares problem to find the coefficients for the truncated model $s_M(t)$.\n4.  Evaluate both $p(t)$ and $s_M(t)$ on a dense grid of $T$ test points, $\\{t^*_m\\}$.\n5.  Compute the true function values $f(t^*_m)$ on the same test grid.\n6.  Calculate the RMSE for both methods against the true function values $f(t^*_m)$.\n7.  Compare the RMSE values to determine which method provides better generalization performance, i.e., a lower error on unseen data. The boolean result $\\mathrm{RMSE}_{\\mathrm{LS}}  \\mathrm{RMSE}_{\\mathrm{Interp}}$ quantifies this comparison.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing trigonometric interpolation and least-squares\n    regression by running a suite of test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        {\n            'N': 64, 'K': 3, 'sigma': 0.2, 'M': 3, 'a0': 0.1,\n            'a_k': [1.0, 0.5, 0.2], 'b_k': [0.5, -0.3, 0.1]\n        },\n        # Case 2\n        {\n            'N': 64, 'K': 6, 'sigma': 0.0, 'M': 3, 'a0': 0.0,\n            'a_k': [0.7, -0.4, 0.3, 0.2, -0.1, 0.05],\n            'b_k': [-0.2, 0.3, -0.1, 0.25, 0.2, -0.05]\n        },\n        # Case 3\n        {\n            'N': 64, 'K': 5, 'sigma': 1.0, 'M': 3, 'a0': 0.2,\n            'a_k': [0.9, -0.6, 0.4, -0.3, 0.15],\n            'b_k': [0.5, 0.4, -0.2, 0.3, -0.1]\n        },\n        # Case 4\n        {\n            'N': 16, 'K': 3, 'sigma': 0.5, 'M': 3, 'a0': -0.1,\n            'a_k': [0.9, 0.4, -0.2], 'b_k': [-0.4, 0.35, 0.15]\n        }\n    ]\n\n    T = 4096\n    seed = 314159\n    results = []\n\n    for case in test_cases:\n        N = case['N']\n        K = case['K']\n        sigma = case['sigma']\n        M = case['M']\n        \n        # --- Data Generation ---\n        # Initialize RNG with fixed seed for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Training nodes\n        t_train = 2 * np.pi * np.arange(N) / N\n\n        # True function definition\n        def f(t, params):\n            val = np.full_like(t, params['a0'])\n            for k in range(1, params['K'] + 1):\n                val += params['a_k'][k-1] * np.cos(k * t) + params['b_k'][k-1] * np.sin(k * t)\n            return val\n\n        # True values and noisy observations at training nodes\n        f_train = f(t_train, case)\n        noise = rng.normal(0, sigma, N)\n        y_train = f_train + noise\n\n        # Test grid and true values on the test grid\n        t_test = 2 * np.pi * np.arange(T) / T\n        f_test = f(t_test, case)\n\n        # --- Method 1: Trigonometric Interpolation ---\n        # Compute coefficients using Real FFT\n        C = np.fft.rfft(y_train)\n        A_interp = np.zeros(N // 2 + 1)\n        B_interp = np.zeros(N // 2 - 1)\n        \n        A_interp[0] = C[0] / N\n        if N > 1:\n            A_interp[1:N//2] = 2 * np.real(C[1:N//2]) / N\n            B_interp[:] = -2 * np.imag(C[1:N//2]) / N\n            A_interp[N//2] = C[N//2] / N\n\n        # Evaluate interpolant on the test grid\n        y_interp_pred = np.full_like(t_test, A_interp[0])\n        if N > 1:\n            for k in range(1, N // 2):\n                y_interp_pred += A_interp[k] * np.cos(k * t_test) + B_interp[k-1] * np.sin(k * t_test)\n            y_interp_pred += A_interp[N//2] * np.cos((N // 2) * t_test)\n\n        # --- Method 2: Least-Squares Trigonometric Regression ---\n        # Construct the design matrix X\n        num_coeffs = 1 + 2 * M\n        X = np.zeros((N, num_coeffs))\n        X[:, 0] = 1  # Column for a0\n        for k in range(1, M + 1):\n            X[:, k] = np.cos(k * t_train)      # Columns for a_k\n            X[:, M + k] = np.sin(k * t_train)  # Columns for b_k\n            \n        # Solve the linear least-squares problem\n        coeffs_ls, _, _, _ = np.linalg.lstsq(X, y_train, rcond=None)\n        a_ls = coeffs_ls[0:M+1]\n        b_ls = coeffs_ls[M+1:]\n        \n        # Evaluate the least-squares model on the test grid\n        y_ls_pred = np.full_like(t_test, a_ls[0])\n        for k in range(1, M + 1):\n            y_ls_pred += a_ls[k] * np.cos(k * t_test) + b_ls[k-1] * np.sin(k * t_test)\n\n        # --- Performance Evaluation ---\n        # Calculate RMSE for both methods\n        rmse_interp = np.sqrt(np.mean((y_interp_pred - f_test)**2))\n        rmse_ls = np.sqrt(np.mean((y_ls_pred - f_test)**2))\n\n        # Append boolean result\n        results.append(rmse_ls  rmse_interp)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The Fast Fourier Transform provides a computationally efficient and numerically stable path for trigonometric interpolation. However, alternative mathematical formulations, such as the elegant barycentric formula, also exist. This practice delves into the crucial topic of numerical stability, challenging you to implement this alternative formula and discover its catastrophic failure when evaluating the interpolant near a sample node . This is a powerful lesson in why theoretically equivalent formulas can have vastly different performances in finite-precision arithmetic, and it reinforces the robustness of the Fourier-based approach.",
            "id": "3284441",
            "problem": "Consider a real-valued, $2\\pi$-periodic function sampled at $n$ equispaced nodes on the interval $[0,2\\pi)$. Let the nodes be $x_j = \\frac{2\\pi j}{n}$ for $j=0,1,\\dots,n-1$, and let the sampled values be $f_j = f(x_j)$. A trigonometric interpolant of degree $m$ with $n=2m+1$ is the unique function in the span of $\\{\\exp(i k x) : k=-m,\\dots,m\\}$ that matches the samples $\\{f_j\\}$. The canonical way to evaluate this interpolant away from the nodes is to use a barycentric-like formula built from fundamental trigonometric identities; however, a straightforward implementation of this standard barycentric evaluation can suffer catastrophic cancellation when the evaluation point is extremely close to a node, since terms of size $O\\!\\left(\\frac{1}{|x-x_j|}\\right)$ with alternating signs are summed.\n\nYour task is to:\n- Start from the discrete Fourier basis and the Dirichlet kernel as a fundamental base, and construct the standard barycentric form for trigonometric interpolation at equispaced nodes (without listing any explicit evaluation formula in the problem statement). Explain why the naive evaluation can lose many digits of accuracy when $x$ approaches a node $x_j$.\n- Implement two evaluators of the trigonometric interpolant:\n  1. A standard barycentric evaluator that directly realizes the derived barycentric form by naive summation in floating-point arithmetic.\n  2. A numerically stable evaluator that uses discrete Fourier series coefficients computed from $\\{f_j\\}$ and then evaluates $\\sum_{k=-m}^m c_k e^{ikx}$ at the target point $x$, where the coefficients $c_k$ are determined from the samples. This representation avoids near-pole cancellations of the barycentric form.\n- Create a specific dataset using the function $f(x) = \\sin(x) + \\frac{1}{2}\\sin(5x) + \\frac{1}{4}\\cos(12x)$, which is exactly representable in a trigonometric space of degree $m \\ge 12$. Use nodes $x_j = \\frac{2\\pi j}{n}$ with odd $n$ so that $n=2m+1$, ensuring representation without aliasing for the chosen $f(x)$.\n\nAngle unit: Use radians throughout.\n\nDefine the following test suite of evaluation scenarios, each specified by $(n, j^\\*, \\varepsilon, x)$, where $x$ is either constructed as $x = x_{j^\\*} + \\varepsilon$ or given directly:\n- Case $1$: $(n=\\;25,\\; j^\\*=\\;10,\\; \\varepsilon=\\;5\\times 10^{-16},\\; x = x_{10} + \\varepsilon)$, an extreme near-node case intended to induce catastrophic cancellation in the naive barycentric evaluation.\n- Case $2$: $(n=\\;25,\\; j^\\*=\\;10,\\; \\varepsilon=\\;0,\\; x = x_{10})$, the boundary case where the interpolant must return the data value exactly.\n- Case $3$: $(n=\\;25,\\; j^\\* \\text{ unused},\\; \\varepsilon \\text{ unused},\\; x=\\;1.234)$, a general “happy path” evaluation well away from nodes.\n- Case $4$: $(n=\\;101,\\; j^\\*=\\;50,\\; \\varepsilon=\\;10^{-12},\\; x = x_{50} + \\varepsilon)$, a larger sample size that increases the dynamic range of near-node terms.\n- Case $5$: $(n=\\;25,\\; j^\\*=\\;10,\\; \\varepsilon=\\;10^{-8},\\; x = x_{10} + \\varepsilon)$, a moderately near-node case.\n\nFor each case:\n- Construct the nodes $\\{x_j\\}$ and samples $f_j = f(x_j)$ with $f(x) = \\sin(x) + \\frac{1}{2}\\sin(5x) + \\frac{1}{4}\\cos(12x)$.\n- Evaluate the interpolant at the specified $x$ using both the naive barycentric evaluator and the stable Fourier-coefficient-based evaluator.\n- Let $p_{\\text{naive}}(x)$ denote the result of the naive barycentric evaluation, and $p_{\\text{stable}}(x)$ denote the Fourier-coefficient-based evaluation.\n- Compute the absolute error $E = |p_{\\text{naive}}(x) - p_{\\text{stable}}(x)|$.\n\nYour program should produce a single line of output containing the results for Cases $1$ through $5$ as a comma-separated list enclosed in square brackets, i.e., $[E_1,E_2,E_3,E_4,E_5]$, where each $E_k$ is a floating-point number in radians-derived units (unitless scalar values), computed as specified above. The final output must be exactly one line in this format.",
            "solution": "The problem requires the derivation of the barycentric formula for trigonometric interpolation at equispaced nodes, an explanation of its numerical instability, and a comparative implementation of a naive barycentric evaluator against a stable Fourier-based evaluator.\n\nA real-valued, $2\\pi$-periodic function $f(x)$ is sampled at $n$ equispaced nodes $x_j = \\frac{2\\pi j}{n}$ for $j=0, 1, \\dots, n-1$, where $n=2m+1$ is an odd integer. The sampled values are $f_j = f(x_j)$. The trigonometric interpolant $p(x)$ is the unique function in the space spanned by $\\{e^{ikx}\\}_{k=-m}^{m}$ such that $p(x_j)=f_j$ for all $j$.\n\nThe interpolant can be expressed in the discrete Fourier basis as:\n$$ p(x) = \\sum_{k=-m}^{m} c_k e^{ikx} $$\nwhere the coefficients $c_k$ are given by the discrete Fourier transform of the sample values $\\{f_j\\}$:\n$$ c_k = \\frac{1}{n} \\sum_{j=0}^{n-1} f_j e^{-ikx_j} $$\nSubstituting the expression for $c_k$ into the formula for $p(x)$ yields:\n$$ p(x) = \\sum_{k=-m}^{m} \\left( \\frac{1}{n} \\sum_{j=0}^{n-1} f_j e^{-ikx_j} \\right) e^{ikx} $$\nBy interchanging the order of summation, we can group terms by the sample values $f_j$:\n$$ p(x) = \\sum_{j=0}^{n-1} f_j \\left( \\frac{1}{n} \\sum_{k=-m}^{m} e^{ik(x-x_j)} \\right) = \\sum_{j=0}^{n-1} f_j L_j(x) $$\nThis is the Lagrange form of the interpolant, where $L_j(x)$ are the cardinal basis functions. The expression for $L_j(x)$ is a scaled version of the Dirichlet kernel:\n$$ L_j(x) = \\frac{1}{n} \\sum_{k=-m}^{m} (e^{i(x-x_j)})^k $$\nThis is a finite geometric series. Letting $u = e^{i(x-x_j)}$, the sum is $\\frac{1}{n} u^{-m} \\sum_{l=0}^{2m} u^l$. Since $n=2m+1$, the sum evaluates to:\n$$ L_j(x) = \\frac{1}{n} u^{-m} \\frac{u^{2m+1}-1}{u-1} = \\frac{1}{n} \\frac{u^{m+1}-u^{-m}}{u-1} = \\frac{1}{n} \\frac{u^{m+1/2}-u^{-(m+1/2)}}{u^{1/2}-u^{-1/2}} $$\nUsing Euler's identity $e^{i\\theta}-e^{-i\\theta} = 2i\\sin(\\theta)$, we obtain the closed form for $L_j(x)$:\n$$ L_j(x) = \\frac{1}{n} \\frac{2i\\sin((m+1/2)(x-x_j))}{2i\\sin((x-x_j)/2)} = \\frac{\\sin(n(x-x_j)/2)}{n\\sin((x-x_j)/2)} $$\nsince $m+1/2 = n/2$.\n\nTo derive the barycentric formula, we simplify the numerator. Given $x_j = 2\\pi j/n$ and that $n$ is odd, we have:\n$$ \\sin\\left(\\frac{n(x-x_j)}{2}\\right) = \\sin\\left(\\frac{nx}{2} - \\pi j\\right) = \\sin\\left(\\frac{nx}{2}\\right)\\cos(\\pi j) - \\cos\\left(\\frac{nx}{2}\\right)\\sin(\\pi j) = (-1)^j \\sin\\left(\\frac{nx}{2}\\right) $$\nThus, the cardinal function becomes:\n$$ L_j(x) = \\frac{(-1)^j\\sin(nx/2)}{n\\sin((x-x_j)/2)} $$\nThe interpolant is $p(x) = \\sum_{j=0}^{n-1} f_j L_j(x)$. Crucially, if we interpolate the constant function $g(x)=1$, its samples are $g_j=1$ for all $j$, and its unique interpolant is $p(x)=1$. This implies that the cardinal functions must sum to one:\n$$ 1 = \\sum_{j=0}^{n-1} L_j(x) = \\sum_{j=0}^{n-1} \\frac{(-1)^j\\sin(nx/2)}{n\\sin((x-x_j)/2)} $$\nDividing the expression for $p(x)$ by the expression for $1$ cancels the common factor $\\sin(nx/2)/n$, yielding the standard barycentric formula for trigonometric interpolation (for odd $n$):\n$$ p(x) = \\frac{\\sum_{j=0}^{n-1} \\frac{(-1)^j f_j}{\\sin((x-x_j)/2)}}{\\sum_{j=0}^{n-1} \\frac{(-1)^j}{\\sin((x-x_j)/2)}} $$\n\n**Numerical Instability of the Naive Barycentric Evaluation:**\nThe instability of this formula arises when the evaluation point $x$ is very close to, but not exactly at, one of the interpolation nodes $x_k$. Let $x = x_k + \\varepsilon$ for a small value $\\varepsilon$.\n1.  The term in the sums corresponding to $j=k$ involves $\\sin((x-x_k)/2) = \\sin(\\varepsilon/2) \\approx \\varepsilon/2$. This denominator is extremely small.\n2.  Consequently, the $k$-th term in both the numerator sum, $\\frac{(-1)^k f_k}{\\sin(\\varepsilon/2)}$, and the denominator sum, $\\frac{(-1)^k}{\\sin(\\varepsilon/2)}$, becomes enormous, with magnitude $O(1/|\\varepsilon|)$.\n3.  All other terms ($j \\neq k$) in the sums remain of moderate size, as their denominators $\\sin((x-x_j)/2)$ are not close to zero.\n4.  In floating-point arithmetic, the summation of one very large number with several smaller numbers causes a loss of the less significant digits of the large number, which are precisely the digits that would have combined with the smaller numbers. Effectively, the information contributed by the smaller terms ($j \\neq k$) is partially or completely lost.\n5.  The final result is a ratio of two computed large numbers. Since the true value of the interpolant $p(x)$ must be close to $f_k$, the true numerator and denominator must be very close to a simple ratio. This implies that the sum of all terms for $j \\neq k$ must nearly cancel the dominant $k$-th term. This subtraction of nearly equal, large quantities (where one quantity is a single term and the other is a sum) is a classic recipe for catastrophic cancellation, leading to a result with a large relative error and few, if any, correct digits.\n\n**Stable Evaluation Method:**\nA numerically stable alternative is to revert to the fundamental definition of the interpolant:\n$$ p(x) = \\sum_{k=-m}^{m} c_k e^{ikx} $$\nThis method involves two steps:\n1.  Compute the Fourier coefficients $c_k = \\frac{1}{n} \\sum_{j=0}^{n-1} f_j e^{-i k (2\\pi j/n)}$ for $k = -m, \\ldots, m$. This is performed efficiently and accurately using the Fast Fourier Transform (FFT) algorithm.\n2.  Evaluate the sum directly. This summation is numerically stable. The terms being added, $c_k e^{ikx}$, are of magnitudes comparable to the coefficients $c_k$ (since $|e^{ikx}|=1$). There are no large intermediate terms that cancel, thus avoiding catastrophic cancellation. This method is used as the \"ground truth\" to measure the error of the naive barycentric approach.\n\nThe implementation will compute the absolute error $|p_{\\text{naive}}(x) - p_{\\text{stable}}(x)|$ for each test case. For a case where $x$ is exactly a node $x_k$, the naive evaluator must be handled specially to return $f_k$ and avoid division by zero.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and compares two trigonometric interpolation evaluation methods.\n    \"\"\"\n\n    # Define the target function f(x)\n    def f(x):\n        \"\"\"\n        The function to be interpolated. It is a trigonometric polynomial of degree 12.\n        f(x) = sin(x) + 1/2*sin(5x) + 1/4*cos(12x)\n        \"\"\"\n        return np.sin(x) + 0.5 * np.sin(5 * x) + 0.25 * np.cos(12 * x)\n\n    # Method 1: Numerically stable evaluator using Fourier coefficients\n    def stable_evaluator(x, f_j, n):\n        \"\"\"\n        Evaluates the trigonometric interpolant using its Fourier series representation.\n        This method is numerically stable.\n        \"\"\"\n        m = (n - 1) // 2\n        \n        # Compute Fourier coefficients c_k using the Fast Fourier Transform (FFT)\n        f_coeffs = np.fft.fft(f_j) / n\n        \n        # Shift coefficients to correspond to frequencies -m, ..., 0, ..., m\n        c_shifted = np.fft.fftshift(f_coeffs)\n        \n        # Corresponding integer frequencies k = -m, ..., m\n        k_vals = np.arange(-m, m + 1)\n        \n        # Evaluate p(x) = sum_{k=-m to m} c_k * exp(i*k*x)\n        val = np.sum(c_shifted * np.exp(1j * k_vals * x))\n        \n        # The result must be real as f(x) is real. Take the real part to discard\n        # imaginary noise from floating point inaccuracies.\n        return np.real(val)\n\n    # Method 2: Naive barycentric formula evaluator\n    def naive_barycentric_evaluator(x, f_j, x_j, n):\n        \"\"\"\n        Evaluates the trigonometric interpolant using the standard barycentric formula.\n        This method is prone to catastrophic cancellation near nodes.\n        \"\"\"\n        # A direct check for whether x is exactly one of the nodes.\n        # This is necessary to handle the interpolating property p(x_j) = f_j\n        # and to avoid division by zero. Using '==' is intentional to only catch\n        # the exact node case (Test Case 2) and not the near-node cases.\n        match_indices = np.where(x == x_j)[0]\n        if len(match_indices) > 0:\n            return f_j[match_indices[0]]\n\n        # If x is not an exact node, proceed with the barycentric formula\n        numerator = 0.0\n        denominator = 0.0\n        for j in range(n):\n            # Barycentric weights for equispaced points with n odd\n            w_j = (-1)**j\n            \n            # Denominator of the j-th term in the barycentric sum.\n            # This becomes very small if x is close to x_j, leading to a large term.\n            term_den = np.sin((x - x_j[j]) / 2.0)\n            \n            term = w_j / term_den\n            \n            numerator += term * f_j[j]\n            denominator += term\n        \n        # The ratio gives the interpolated value. For x near a node, this is a\n        # ratio of two very large numbers, which is where precision is lost.\n        return numerator / denominator\n\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (Parameter set 1): Case 1: Extreme near-node case\n        {'n': 25, 'j_star': 10, 'epsilon': 5e-16, 'x_spec': 'relative'},\n        # (Parameter set 2): Case 2: Exactly at a node\n        {'n': 25, 'j_star': 10, 'epsilon': 0.0, 'x_spec': 'relative'},\n        # (Parameter set 3): Case 3: A generic point far from nodes\n        {'n': 25, 'x': 1.234, 'x_spec': 'absolute'},\n        # (Parameter set 4): Case 4: Larger n, near-node\n        {'n': 101, 'j_star': 50, 'epsilon': 1e-12, 'x_spec': 'relative'},\n        # (Parameter set 5): Case 5: Moderately near-node\n        {'n': 25, 'j_star': 10, 'epsilon': 1e-8, 'x_spec': 'relative'},\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case['n']\n        \n        # Construct nodes and sample data\n        nodes_x_j = (2 * np.pi / n) * np.arange(n)\n        samples_f_j = f(nodes_x_j)\n        \n        # Determine the evaluation point x for the current case\n        if case['x_spec'] == 'relative':\n            j_star = case['j_star']\n            epsilon = case['epsilon']\n            x_eval = nodes_x_j[j_star] + epsilon\n        else:  # 'absolute'\n            x_eval = case['x']\n            \n        # Evaluate the interpolant at x_eval using both methods\n        p_stable = stable_evaluator(x_eval, samples_f_j, n)\n        p_naive = naive_barycentric_evaluator(x_eval, samples_f_j, nodes_x_j, n)\n        \n        # Compute the absolute error between the two evaluations.\n        # p_stable is treated as the ground truth.\n        error = np.abs(p_naive - p_stable)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}