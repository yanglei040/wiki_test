{
    "hands_on_practices": [
        {
            "introduction": "The foundation of trigonometric interpolation lies in representing a periodic function as a sum of sines and cosines, or more generally, complex exponentials. This first practice challenges you to build a trigonometric interpolant from first principles for complex-valued periodic data sampled at equally spaced points . By leveraging the discrete orthogonality of the complex exponential basis $\\{e^{ikx}\\}$ and its connection to the Discrete Fourier Transform, you will develop a complete algorithm to compute the interpolant's coefficients and visualize the resulting smooth curve in the complex plane.",
            "id": "3284416",
            "problem": "You are given the task of building and analyzing a trigonometric interpolant for complex-valued periodic data sampled at equally spaced nodes. Trigonometric interpolation is grounded in the representation of periodic functions by trigonometric polynomials and the orthogonality of complex exponentials. The foundation for your derivation should be the following core facts: the space of trigonometric polynomials of degree at most $m$ is spanned by $\\{e^{i k x}\\}_{k=-m}^{m}$; the interpolation nodes are equally spaced over one period; and the complex exponentials exhibit discrete orthogonality over the equispaced grid.\n\nStarting from the above foundations, derive an algorithm to construct the unique trigonometric polynomial of degree at most $m$ that interpolates a complex-valued function at $N$ equally spaced nodes over one period, with $N$ odd and $m = \\frac{N - 1}{2}$. Your derivation must use the orthogonality of $\\{e^{i k x}\\}$ on the equispaced grid and produce an implementable procedure to:\n- compute the coefficients of the interpolating trigonometric polynomial from the complex samples, and\n- evaluate the interpolating polynomial at arbitrary points to obtain a smooth curve in the complex plane.\n\nAll angles must be in radians. You must treat complex-valued data rigorously by interpolating both real and imaginary parts simultaneously through the complex arithmetic implied by the basis $\\{e^{i k x}\\}$.\n\nImplement your algorithm in a complete and runnable program that, for the specified test suite below, constructs the interpolant, evaluates it on a uniform grid to represent the smooth curve, and then reports a single scalar diagnostic for each test case: the maximum absolute error between the interpolant and the underlying generating function on the evaluation grid. Aggregate these diagnostics into the required final output format.\n\nDefinitions and setup:\n- Let the nodes be $x_j = \\frac{2\\pi j}{N}$ for $j=0,1,\\dots,N-1$ over the interval $[0,2\\pi)$.\n- Let $f_j = f(x_j)$ be complex-valued samples.\n- Let the interpolating trigonometric polynomial be $p(x)$ of degree at most $m = \\frac{N-1}{2}$ so that $p(x_j) = f_j$ for all $j$.\n\nTest suite and evaluation requirements (angles in radians):\n- Case 1 (happy path): $N = 15$, $f(x) = e^{i 2 x} + 0.3\\, e^{-i 3 x} + 0.2 i\\, e^{i 5 x}$. Evaluate the interpolant on $200$ uniformly spaced points in $[0,2\\pi)$ and return the maximum absolute error $\\max_{x} |p(x) - f(x)|$.\n- Case 2 (constant complex function): $N = 9$, $f(x) = 2 + 0.5 i$. Evaluate on $64$ points in $[0,2\\pi)$ and return $\\max_{x} |p(x) - f(x)|$.\n- Case 3 (boundary-degree coverage): $N = 21$, $f(x) = 0.2\\, e^{i 10 x} + 0.1\\, e^{-i 9 x} + 0.05 i\\, e^{i x}$. Evaluate on $256$ points in $[0,2\\pi)$ and return $\\max_{x} |p(x) - f(x)|$.\n- Case 4 (minimal sample set): $N = 1$, $f(x) = 1 + 2 i$. Evaluate on $32$ points in $[0,2\\pi)$ and return $\\max_{x} |p(x) - f(x)|$.\n\nFinal output format:\n- Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$), where each $r_k$ is the floating-point maximum absolute error for the corresponding test case.\n\nYour solution must be self-contained and runnable, use angles in radians, and compute the interpolant for odd $N$ via a principled algorithm derived from orthogonality of the complex exponential basis. Do not rely on external data or user input. The implementation language is unrestricted for the conceptual derivation, but the final answer must be Python code that matches the specified runtime environment and output format.",
            "solution": "The problem requires the derivation and implementation of an algorithm for trigonometric interpolation of complex-valued data sampled at $N$ equally spaced nodes, where $N$ is odd. The derivation must be based on the discrete orthogonality of the complex exponential basis functions.\n\nLet the interpolation nodes be $x_j = \\frac{2\\pi j}{N}$ for $j=0, 1, \\dots, N-1$. We are given $N$ complex-valued samples $f_j = f(x_j)$. We seek a trigonometric polynomial $p(x)$ of degree at most $m = \\frac{N-1}{2}$ that interpolates these samples, i.e., $p(x_j) = f_j$ for all $j$.\n\n**Step 1: The Form of the Trigonometric Interpolant**\n\nThe space of trigonometric polynomials of degree at most $m$ is spanned by the set of complex exponential functions $\\{e^{ikx}\\}_{k=-m}^{m}$. Therefore, the interpolating polynomial $p(x)$ can be written as a linear combination of these basis functions:\n$$\np(x) = \\sum_{k=-m}^{m} c_k e^{ikx}\n$$\nThe coefficients $\\{c_k\\}$ are complex numbers. There are $2m+1$ coefficients to be determined. Since $m = \\frac{N-1}{2}$, we have $2m+1 = 2\\left(\\frac{N-1}{2}\\right) + 1 = (N-1) + 1 = N$ unknown coefficients, which matches the number of data points.\n\n**Step 2: The Interpolation Conditions**\n\nThe polynomial $p(x)$ must match the sample values $f_j$ at the nodes $x_j$. This provides a system of $N$ linear equations for the $N$ unknown coefficients $c_k$:\n$$\nf_j = p(x_j) = \\sum_{k=-m}^{m} c_k e^{ikx_j} = \\sum_{k=-m}^{m} c_k e^{ik(2\\pi j/N)}, \\quad \\text{for } j = 0, 1, \\dots, N-1\n$$\n\n**Step 3: Discrete Orthogonality of Basis Functions**\n\nTo solve for the coefficients $c_k$, we exploit the discrete orthogonality property of the basis functions $\\{e^{ikx}\\}$ over the set of nodes $\\{x_j\\}$. Let's consider the sum over the nodes of the product of two basis functions, $e^{ikx_j}$ and $e^{-ilx_j}$, where $k$ and $l$ are integers in the range $[-m, m]$:\n$$\n\\sum_{j=0}^{N-1} e^{ikx_j} e^{-ilx_j} = \\sum_{j=0}^{N-1} e^{i(k-l)x_j} = \\sum_{j=0}^{N-1} e^{i(k-l)\\frac{2\\pi j}{N}} = \\sum_{j=0}^{N-1} \\left(e^{i(k-l)\\frac{2\\pi}{N}}\\right)^j\n$$\nThis is a finite geometric series with ratio $r = e^{i(k-l)\\frac{2\\pi}{N}}$.\n\nCase 1: $k = l$. The ratio is $r = e^0 = 1$. The sum becomes:\n$$\n\\sum_{j=0}^{N-1} 1 = N\n$$\nCase 2: $k \\neq l$. Since $k, l \\in \\{-m, \\dots, m\\}$, the difference $k-l$ is an integer satisfying $-(2m) \\le k-l \\le 2m$. With $m = (N-1)/2$, this range is $-(N-1) \\le k-l \\le N-1$. As $k \\neq l$, $k-l$ is a non-zero integer that is not a multiple of $N$. Therefore, the ratio $r = e^{i(k-l)\\frac{2\\pi}{N}}$ is not equal to $1$. The sum of the geometric series is:\n$$\n\\sum_{j=0}^{N-1} r^j = \\frac{r^N - 1}{r - 1} = \\frac{\\left(e^{i(k-l)\\frac{2\\pi}{N}}\\right)^N - 1}{r - 1} = \\frac{e^{i(k-l)2\\pi} - 1}{r - 1}\n$$\nSince $k-l$ is an integer, $e^{i(k-l)2\\pi} = \\cos(2\\pi(k-l)) + i\\sin(2\\pi(k-l)) = 1$. Thus, the numerator is $1-1=0$, and the sum is $0$.\n\nCombining these two cases, we obtain the discrete orthogonality relation:\n$$\n\\sum_{j=0}^{N-1} e^{ikx_j} e^{-ilx_j} = N \\delta_{kl}\n$$\nwhere $\\delta_{kl}$ is the Kronecker delta.\n\n**Step 4: Derivation of the Coefficient Formula**\n\nWe can now isolate each coefficient $c_l$ by using the orthogonality relation. We take the interpolation condition equation, multiply both sides by $e^{-ilx_j}$ for a specific integer $l \\in \\{-m, \\dots, m\\}$, and sum over all nodes $j=0, \\dots, N-1$:\n$$\n\\sum_{j=0}^{N-1} f_j e^{-ilx_j} = \\sum_{j=0}^{N-1} \\left( \\sum_{k=-m}^{m} c_k e^{ikx_j} \\right) e^{-ilx_j}\n$$\nSwapping the order of summation on the right-hand side, which is permissible for finite sums:\n$$\n\\sum_{j=0}^{N-1} f_j e^{-ilx_j} = \\sum_{k=-m}^{m} c_k \\left( \\sum_{j=0}^{N-1} e^{ikx_j} e^{-ilx_j} \\right)\n$$\nApplying the orthogonality relation $\\sum_{j=0}^{N-1} e^{ikx_j} e^{-ilx_j} = N \\delta_{kl}$:\n$$\n\\sum_{j=0}^{N-1} f_j e^{-ilx_j} = \\sum_{k=-m}^{m} c_k (N \\delta_{kl})\n$$\nThe sum on the right-hand side collapses to a single non-zero term when $k=l$:\n$$\n\\sum_{j=0}^{N-1} f_j e^{-ilx_j} = N c_l\n$$\nSolving for $c_l$, we get the explicit formula for the coefficients:\n$$\nc_l = \\frac{1}{N} \\sum_{j=0}^{N-1} f_j e^{-ilx_j} = \\frac{1}{N} \\sum_{j=0}^{N-1} f_j e^{-i 2\\pi lj/N}\n$$\nThis formula holds for all $l \\in \\{-m, \\dots, m\\}$.\n\n**Step 5: Connection to the Discrete Fourier Transform (DFT)**\n\nThe standard definition of the Discrete Fourier Transform of a sequence $\\{y_j\\}_{j=0}^{N-1}$ is given by:\n$$\n\\hat{y}_k = \\sum_{j=0}^{N-1} y_j e^{-i 2\\pi kj/N}, \\quad \\text{for } k=0, 1, \\dots, N-1\n$$\nComparing this with our formula for $c_l$, we see that the coefficients are simply a scaled version of the DFT of the data sequence $\\{f_j\\}$:\n$$\nc_l = \\frac{1}{N} \\hat{f}_l\n$$\nwhere $\\hat{f}_l = \\text{DFT}(\\{f_j\\})_l$. Standard DFT algorithms compute $\\hat{f}_k$ for indices $k=0, \\dots, N-1$. These must be mapped to our polynomial coefficient indices $l=-m, \\dots, m$. The mapping is based on the periodicity of the complex exponential kernel, $e^{-i 2\\pi (l+N) j/N} = e^{-i 2\\pi lj/N} e^{-i 2\\pi j} = e^{-i 2\\pi lj/N}$. Thus, the DFT coefficient at index $k=N+l$ is the same as at index $l$. For negative indices $l = -1, \\dots, -m$, we can use the corresponding positive indices $k=N-1, \\dots, N-m$. Efficient computational libraries for the Fast Fourier Transform (FFT) handle this indexing implicitly, often providing a corresponding frequency array that maps directly to the indices $l = 0, \\dots, m, -m, \\dots, -1$.\n\n**Step 6: The Final Algorithm**\n\nThe complete procedure for constructing and evaluating the trigonometric interpolant is as follows:\n\n1.  **Coefficient Calculation**:\n    a. Given the $N$ data samples $f_0, f_1, \\dots, f_{N-1}$.\n    b. Compute the Discrete Fourier Transform of this sequence: $\\{\\hat{f}_k\\} = \\text{DFT}(\\{f_j\\})$.\n    c. Calculate the polynomial coefficients: $c_k = \\frac{1}{N} \\hat{f}_k$. These coefficients correspond to the basis functions $e^{ilx}$ where the integer wavenumber $l$ is associated with the DFT index $k$.\n\n2.  **Polynomial Evaluation**:\n    a. To evaluate the interpolant $p(x)$ at an arbitrary set of points $\\{x_{eval}\\}$, compute the sum:\n    $$\n    p(x_{eval}) = \\sum_{l=-m}^{m} c_l e^{il x_{eval}}\n    $$\n    b. In an implementation, this is achieved by identifying the vector of wavenumbers $l \\in \\{0, 1, \\dots, m, -m, \\dots, -1\\}$ that corresponds to the standard DFT output ordering and then computing the matrix-vector product between the coefficients and the values of the basis functions at the evaluation points.\n\nFor the given test cases, the generating function $f(x)$ is itself a trigonometric polynomial of degree at most $m$. By the uniqueness of the trigonometric interpolant, the constructed polynomial $p(x)$ must be identical to $f(x)$. Consequently, the theoretical error $|p(x) - f(x)|$ is zero. Any non-zero result will be due to floating-point representation and arithmetic errors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trigonometric interpolation problem for a suite of test cases.\n    \"\"\"\n\n    def construct_and_evaluate_interpolant(N, f_func, n_eval):\n        \"\"\"\n        Constructs and evaluates a trigonometric interpolant for a given function.\n\n        Args:\n            N (int): The number of sample points (must be odd).\n            f_func (callable): The complex-valued generating function f(x).\n            n_eval (int): The number of points for evaluation.\n\n        Returns:\n            float: The maximum absolute error |p(x) - f(x)| on the evaluation grid.\n        \"\"\"\n        if N <= 0:\n            raise ValueError(\"Number of samples N must be positive.\")\n        if N % 2 == 0:\n            raise ValueError(\"Number of samples N must be odd for this problem.\")\n\n        # Step 1: Generate sample points and data\n        # Nodes: x_j = 2*pi*j / N for j=0,...,N-1\n        x_j_grid = 2 * np.pi * np.arange(N) / N\n        # Sample data: f_j = f(x_j)\n        f_j_data = f_func(x_j_grid)\n\n        # Step 2: Compute coefficients using FFT\n        # The derivation shows c_k = (1/N) * DFT(f_j), where p(x) = sum(c_k * e^(i*k*x))\n        # numpy.fft.fft computes the DFT.\n        coeffs = np.fft.fft(f_j_data) / N\n\n        # Step 3: Get the corresponding integer wavenumbers\n        # numpy.fft.fftfreq gives frequencies [0, 1/N, ..., m/N, -m/N, ..., -1/N]\n        # To get integer wavenumbers k, we multiply by N.\n        # This gives k = [0, 1, ..., m, -m, ..., -1], with m=(N-1)/2.\n        k_vals = np.fft.fftfreq(N) * N\n\n        # Step 4: Define evaluation grid\n        # Uniform grid of n_eval points in [0, 2*pi)\n        x_eval_grid = np.linspace(0, 2 * np.pi, n_eval, endpoint=False)\n\n        # Step 5: Evaluate the interpolating polynomial p(x) on the evaluation grid\n        # p(x) = sum_k c_k * exp(i*k*x)\n        # This can be computed efficiently using an outer product and a matrix multiplication.\n        # V_kj = exp(i * k_k * x_j), a Vandermonde-like matrix of basis functions\n        V = np.exp(1j * np.outer(k_vals, x_eval_grid))\n        # p_eval = coeffs . V\n        p_eval = coeffs @ V\n\n        # Step 6: Evaluate the true function f(x) on the evaluation grid\n        f_eval = f_func(x_eval_grid)\n\n        # Step 7: Calculate the maximum absolute error\n        max_error = np.max(np.abs(p_eval - f_eval))\n\n        return max_error\n\n    # Define test cases\n    test_cases = [\n        {\n            'N': 15,\n            'f_func': lambda x: np.exp(1j * 2 * x) + 0.3 * np.exp(-1j * 3 * x) + 0.2j * np.exp(1j * 5 * x),\n            'n_eval': 200,\n        },\n        {\n            'N': 9,\n            'f_func': lambda x: (2 + 0.5j) * np.ones_like(x),\n            'n_eval': 64,\n        },\n        {\n            'N': 21,\n            'f_func': lambda x: 0.2 * np.exp(1j * 10 * x) + 0.1 * np.exp(-1j * 9 * x) + 0.05j * np.exp(1j * x),\n            'n_eval': 256,\n        },\n        {\n            'N': 1,\n            'f_func': lambda x: (1 + 2j) * np.ones_like(x),\n            'n_eval': 32,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        error = construct_and_evaluate_interpolant(case['N'], case['f_func'], case['n_eval'])\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While interpolation provides an exact fit to a given set of data points, this is not always desirable, especially when the data is corrupted by noise. This exercise explores the critical distinction between interpolation and regression by challenging you to fit a noisy periodic signal with both an exact trigonometric interpolant and a truncated least-squares model . By comparing their ability to recover the true underlying function, you will gain firsthand insight into the concept of overfitting and the fundamental bias-variance trade-off in data modeling.",
            "id": "3284517",
            "problem": "You are given the task of designing and implementing an experiment that distinguishes between trigonometric interpolation and least-squares trigonometric regression in the presence of noise, and to quantify their generalization performance on unseen points. All angles must be treated in radians.\n\nStarting from the following fundamental base:\n- The orthogonality of complex exponentials on a periodic interval, namely that the set $\\{e^{\\mathrm{i} k t}\\}_{k\\in\\mathbb{Z}}$ is orthogonal over a period.\n- The definition of a trigonometric interpolant as a periodic trigonometric polynomial that passes exactly through given data sampled at equally spaced nodes on a full period.\n- The definition of least-squares regression as the minimizer of the sum of squared residuals of a specified model class against the observed data.\n\nYou must:\n1) Derive the computational formulation for constructing the exact trigonometric interpolant of a periodic function sampled at $N$ equally spaced nodes over one period. The interpolant must exactly match the noisy samples at the training nodes and be representable as a finite trigonometric series. You may use the discrete Fourier transform as a computational device, but you must justify its use from orthogonality and interpolation conditions.\n2) Derive the computational formulation for fitting a truncated real trigonometric model of degree $M$ in the least-squares sense to the same training data. The model has the form\n$$\ns_M(t) \\;=\\; a_0 \\;+\\; \\sum_{k=1}^{M} \\big( a_k \\cos(kt) + b_k \\sin(kt) \\big),\n$$\nand its coefficients must be obtained by minimizing the sum of squared residuals on the training nodes.\n3) Implement both approaches and compare their out-of-sample performance by computing the root-mean-square error (RMSE) of each method against the true noise-free periodic function on a dense test grid.\n\nData generation protocol (angles in radians):\n- Training nodes: for each case use $t_j = 2\\pi j / N$ for $j=0,1,\\dots,N-1$.\n- A true periodic target function is specified as\n$$\nf(t) \\;=\\; a_0 \\;+\\; \\sum_{k=1}^{K} \\big( a_k \\cos(kt) + b_k \\sin(kt) \\big).\n$$\n- Observations are noisy: $y_j = f(t_j) + \\varepsilon_j$, where $\\varepsilon_j$ are independent and identically distributed normal random variables with mean $0$ and standard deviation $\\sigma$, generated using a fixed random seed for reproducibility.\n- The test grid must be a set of $T$ equally spaced points on one period: $t^{\\ast}_m = 2\\pi m / T$ for $m=0,1,\\dots,T-1$. Use $T = 4096$.\n\nPerformance metric:\n- For each method, compute the out-of-sample root-mean-square error\n$$\n\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{T} \\sum_{m=0}^{T-1} \\big( \\hat{f}(t^{\\ast}_m) - f(t^{\\ast}_m) \\big)^2 },\n$$\nwhere $\\hat{f}$ is either the trigonometric interpolant or the truncated least-squares fit. Lower $\\mathrm{RMSE}$ indicates better generalization to the true underlying function.\n\nYour program must implement the above and evaluate the following test suite of cases. In every case, initialize a pseudorandom number generator with the same seed $314159$ before generating noise so that results are reproducible. For each case, report a boolean indicating whether the least-squares truncated model generalizes better than the exact interpolant, i.e., whether $\\mathrm{RMSE}_{\\mathrm{LS}} < \\mathrm{RMSE}_{\\mathrm{Interp}}$.\n\nTest suite with coefficients and parameters:\n- Case $1$: $N = 64$, $K = 3$, $\\sigma = 0.2$, $M = 3$, $a_0 = 0.1$, with\n  $a_k$ for $k = 1,2,3$ equal to $\\{1.0,\\,0.5,\\,0.2\\}$ and\n  $b_k$ for $k = 1,2,3$ equal to $\\{0.5,\\,-0.3,\\,0.1\\}$.\n- Case $2$: $N = 64$, $K = 6$, $\\sigma = 0.0$, $M = 3$, $a_0 = 0.0$, with\n  $a_k$ for $k = 1,2,3,4,5,6$ equal to $\\{0.7,\\,-0.4,\\,0.3,\\,0.2,\\,-0.1,\\,0.05\\}$ and\n  $b_k$ for $k = 1,2,3,4,5,6$ equal to $\\{-0.2,\\,0.3,\\,-0.1,\\,0.25,\\,0.2,\\,-0.05\\}$.\n- Case $3$: $N = 64$, $K = 5$, $\\sigma = 1.0$, $M = 3$, $a_0 = 0.2$, with\n  $a_k$ for $k = 1,2,3,4,5$ equal to $\\{0.9,\\,-0.6,\\,0.4,\\,-0.3,\\,0.15\\}$ and\n  $b_k$ for $k = 1,2,3,4,5$ equal to $\\{0.5,\\,0.4,\\,-0.2,\\,0.3,\\,-0.1\\}$.\n- Case $4$: $N = 16$, $K = 3$, $\\sigma = 0.5$, $M = 3$, $a_0 = -0.1$, with\n  $a_k$ for $k = 1,2,3$ equal to $\\{0.9,\\,0.4,\\,-0.2\\}$ and\n  $b_k$ for $k = 1,2,3$ equal to $\\{-0.4,\\,0.35,\\,0.15\\}$.\n\nImplementation requirements:\n- Construct the exact trigonometric interpolant using its defining property of matching all training samples and the orthogonality of complex exponentials over a period. You may compute its coefficients using the discrete Fourier transform suitable for equally spaced samples and evaluate the interpolant on the dense test grid.\n- Construct the least-squares truncated real trigonometric fit of degree $M$ by solving the linear least-squares problem for the coefficients $\\{a_0,a_1,\\dots,a_M,b_1,\\dots,b_M\\}$ using the training data, and then evaluate the fitted model on the dense test grid.\n- For each case, compute both $\\mathrm{RMSE}_{\\mathrm{Interp}}$ and $\\mathrm{RMSE}_{\\mathrm{LS}}$ against the true $f(t)$ on the test grid.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases above. Each entry must be a boolean, equal to $\\mathrm{True}$ if and only if $\\mathrm{RMSE}_{\\mathrm{LS}} < \\mathrm{RMSE}_{\\mathrm{Interp}}$ for that case, and $\\mathrm{False}$ otherwise. For example: $[True, False, True, True]$.",
            "solution": "### Step 1: Extract Givens\n- **Task**: Design and implement an experiment to compare trigonometric interpolation and least-squares trigonometric regression. All angles are in radians.\n- **Fundamental Base**:\n    - Orthogonality of $\\{e^{\\mathrm{i} k t}\\}_{k\\in\\mathbb{Z}}$ over a periodic interval.\n    - Trigonometric interpolant: periodic trigonometric polynomial passing exactly through data sampled at $N$ equally spaced nodes on a full period.\n    - Least-squares regression: minimizer of the sum of squared residuals for a specified model class.\n\n- **Derivation Requirements**:\n    1.  Computational formulation for the exact trigonometric interpolant for $N$ equally spaced nodes, using DFT and justifying its use.\n    2.  Computational formulation for a truncated real trigonometric model of degree $M$ fitted via least squares:\n        $$\n        s_M(t) \\;=\\; a_0 \\;+\\; \\sum_{k=1}^{M} \\big( a_k \\cos(kt) + b_k \\sin(kt) \\big)\n        $$\n\n- **Implementation and Comparison**:\n    - Implement both approaches.\n    - Compare out-of-sample performance using root-mean-square error (RMSE) against the true noise-free function on a dense test grid.\n\n- **Data Generation Protocol**:\n    - Training nodes: $t_j = 2\\pi j / N$ for $j=0,1,\\dots,N-1$.\n    - True target function: $f(t) = a_0 + \\sum_{k=1}^{K} \\big( a_k \\cos(kt) + b_k \\sin(kt) \\big)$.\n    - Noisy observations: $y_j = f(t_j) + \\varepsilon_j$, where $\\varepsilon_j \\sim \\mathcal{N}(0, \\sigma^2)$, i.i.d.\n    - Random seed for noise generation: $314159$.\n    - Test grid: $t^{\\ast}_m = 2\\pi m / T$ for $m=0,1,\\dots,T-1$, with $T = 4096$.\n\n- **Performance Metric**:\n    - RMSE: $\\sqrt{\\frac{1}{T} \\sum_{m=0}^{T-1} \\big( \\hat{f}(t^{\\ast}_m) - f(t^{\\ast}_m) \\big)^2 }$, where $\\hat{f}$ is the interpolant or the least-squares fit.\n\n- **Final Output**: A boolean list, where an entry is $\\mathrm{True}$ if $\\mathrm{RMSE}_{\\mathrm{LS}} < \\mathrm{RMSE}_{\\mathrm{Interp}}$.\n\n- **Test Suite**:\n    - Case $1$: $N = 64$, $K = 3$, $\\sigma = 0.2$, $M = 3$, $a_0 = 0.1$, $a_k = \\{1.0,\\,0.5,\\,0.2\\}$, $b_k = \\{0.5,\\,-0.3,\\,0.1\\}$.\n    - Case $2$: $N = 64$, $K = 6$, $\\sigma = 0.0$, $M = 3$, $a_0 = 0.0$, $a_k = \\{0.7,\\,-0.4,\\,0.3,\\,0.2,\\,-0.1,\\,0.05\\}$, $b_k = \\{-0.2,\\,0.3,\\,-0.1,\\,0.25,\\,0.2,\\,-0.05\\}$.\n    - Case $3$: $N = 64$, $K = 5$, $\\sigma = 1.0$, $M = 3$, $a_0 = 0.2$, $a_k = \\{0.9,\\,-0.6,\\,0.4,\\,-0.3,\\,0.15\\}$, $b_k = \\{0.5,\\,0.4,\\,-0.2,\\,0.3,\\,-0.1\\}$.\n    - Case $4$: $N = 16$, $K = 3$, $\\sigma = 0.5$, $M = 3$, $a_0 = -0.1$, $a_k = \\{0.9,\\,0.4,\\,-0.2\\}$, $b_k = \\{-0.4,\\,0.35,\\,0.15\\}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard exercise in numerical analysis and applied Fourier analysis. It deals with the well-established concepts of trigonometric interpolation, least-squares approximation, and the bias-variance trade-off in statistical modeling. The problem is well-posed: it provides all necessary data, parameters, and definitions for each test case. The objective is clearly stated and uses precise, unambiguous language. The tasks are formalizable into a concrete computational experiment. The setup is self-contained and consistent. No scientific or logical flaws are present.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\n### Principle-Based Solution\n\nThe problem requires a comparative analysis of two methods for approximating a periodic function from noisy samples: exact trigonometric interpolation and truncated least-squares trigonometric regression. This comparison highlights the fundamental trade-off between bias and variance in function approximation. Interpolation perfectly fits the training data, including noise, which often leads to a high-variance model that generalizes poorly (overfitting). In contrast, least-squares regression with a truncated model of degree $M$ acts as a low-pass filter, reducing variance by not fitting high-frequency noise, but potentially introducing bias if the true signal contains frequencies greater than $M$.\n\n**1. Formulation of Exact Trigonometric Interpolation**\n\nGiven a set of $N$ data points $(t_j, y_j)$, where $t_j = 2\\pi j/N$ for $j=0, 1, \\dots, N-1$, the goal is to find a trigonometric polynomial $p(t)$ that satisfies the interpolation condition $p(t_j) = y_j$ for all $j$. For an even number of points $N$, as specified in the test cases, the unique trigonometric polynomial of degree at most $N/2$ can be written in real form as:\n$$\np(t) = A_0 + \\sum_{k=1}^{N/2-1} \\left( A_k \\cos(kt) + B_k \\sin(kt) \\right) + A_{N/2} \\cos\\left(\\frac{N}{2}t\\right)\n$$\nThis polynomial has $1 + 2(N/2 - 1) + 1 = N$ degrees of freedom, corresponding to the coefficients $\\{A_k\\}_{k=0}^{N/2}$ and $\\{B_k\\}_{k=1}^{N/2-1}$.\n\nThe coefficients are determined by exploiting the discrete orthogonality properties of the trigonometric basis functions over the set of nodes $\\{t_j\\}$. These properties are a direct consequence of the orthogonality of complex exponentials on a discrete set of points. Specifically, for integers $k, l \\in [0, N/2]$:\n$$\n\\sum_{j=0}^{N-1} \\cos(kt_j)\\cos(lt_j) = \\begin{cases} N & k=l=0 \\text{ or } N/2 \\\\ N/2 & k=l \\in \\{1,\\dots,N/2-1\\} \\\\ 0 & k \\neq l \\end{cases}\n$$\n$$\n\\sum_{j=0}^{N-1} \\sin(kt_j)\\sin(lt_j) = \\begin{cases} N/2 & k=l \\in \\{1,\\dots,N/2-1\\} \\\\ 0 & k \\neq l \\end{cases}\n$$\n$$\n\\sum_{j=0}^{N-1} \\cos(kt_j)\\sin(lt_j) = 0 \\quad \\forall k, l\n$$\nBy substituting $t=t_j$ into the expression for $p(t)$, setting it equal to $y_j$, multiplying by a basis function (e.g., $\\cos(kt_j)$), and summing over all $j$, we can isolate each coefficient due to these orthogonality relations. This procedure yields the following formulas for the coefficients:\n$$\nA_0 = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j\n$$\n$$\nA_k = \\frac{2}{N} \\sum_{j=0}^{N-1} y_j \\cos(kt_j), \\quad k \\in \\{1, \\dots, N/2-1\\}\n$$\n$$\nB_k = \\frac{2}{N} \\sum_{j=0}^{N-1} y_j \\sin(kt_j), \\quad k \\in \\{1, \\dots, N/2-1\\}\n$$\n$$\nA_{N/2} = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j \\cos\\left(\\frac{N}{2}t_j\\right) = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j (-1)^j\n$$\nThese sums define the Discrete Fourier Transform (DFT). The coefficients can be computed efficiently using the Fast Fourier Transform (FFT) algorithm. Let $\\{C_k\\}_{k=0}^{N/2}$ be the result of a real-valued FFT of the sequence $\\{y_j\\}_{j=0}^{N-1}$. The coefficients are related to the FFT output as:\n$A_0 = C_0 / N$, $A_{N/2} = C_{N/2} / N$, and for $k=1, \\dots, N/2-1$:\n$$\nA_k = \\frac{2}{N} \\mathrm{Re}(C_k)\n$$\n$$\nB_k = -\\frac{2}{N} \\mathrm{Im}(C_k)\n$$\n\n**2. Formulation of Least-Squares Trigonometric Regression**\n\nThe second method involves fitting a trigonometric polynomial of a specified degree $M < N/2$ to the data. The model is:\n$$\ns_M(t) = a_0 + \\sum_{k=1}^{M} \\left( a_k \\cos(kt) + b_k \\sin(kt) \\right)\n$$\nThe goal is to find the coefficients $\\{a_k\\}_{k=0}^M$ and $\\{b_k\\}_{k=1}^M$ that minimize the sum of squared residuals:\n$$\nS = \\sum_{j=0}^{N-1} \\left( y_j - s_M(t_j) \\right)^2\n$$\nThis is a linear least-squares problem. Let the vector of unknown coefficients be $\\boldsymbol{\\theta} = [a_0, a_1, \\dots, a_M, b_1, \\dots, b_M]^T$. The problem can be written in matrix form as minimizing $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}\\|_2^2$, where $\\mathbf{y} = [y_0, \\dots, y_{N-1}]^T$ is the vector of observations, and $\\mathbf{X}$ is the $N \\times (1+2M)$ design matrix whose columns are the basis functions evaluated at the training nodes $t_j$:\n$$\n\\mathbf{X} = \\begin{bmatrix}\n1 & \\cos(1 t_0) & \\dots & \\cos(M t_0) & \\sin(1 t_0) & \\dots & \\sin(M t_0) \\\\\n1 & \\cos(1 t_1) & \\dots & \\cos(M t_1) & \\sin(1 t_1) & \\dots & \\sin(M t_1) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & \\cos(1 t_{N-1}) & \\dots & \\cos(M t_{N-1}) & \\sin(1 t_{N-1}) & \\dots & \\sin(M t_{N-1})\n\\end{bmatrix}\n$$\nThe solution $\\boldsymbol{\\theta}$ is found by solving the normal equations $\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} = \\mathbf{X}^T\\mathbf{y}$. Since $M < N/2$, the discrete orthogonality relations hold for the columns of $\\mathbf{X}$, making the matrix $\\mathbf{X}^T\\mathbf{X}$ diagonal:\n$$\n\\mathbf{X}^T\\mathbf{X} = \\mathrm{diag}\\left(N, \\frac{N}{2}, \\dots, \\frac{N}{2}, \\frac{N}{2}, \\dots, \\frac{N}{2}\\right)\n$$\nSolving for $\\boldsymbol{\\theta}$ gives expressions for the coefficients that are identical to the first $M$ Fourier coefficients of the full interpolation:\n$$\na_0 = \\frac{1}{N} \\sum_{j=0}^{N-1} y_j\n$$\n$$\na_k = \\frac{2}{N} \\sum_{j=0}^{N-1} y_j \\cos(kt_j), \\quad k \\in \\{1, \\dots, M\\}\n$$\n$$\nb_k = \\frac{2}{N} \\sum_{j=0}^{N-1} y_j \\sin(kt_j), \\quad k \\in \\{1, \\dots, M\\}\n$$\nThus, the least-squares approximation with this model is equivalent to truncating the Fourier series of the data. While this analytical solution exists, a robust numerical implementation can directly solve the linear system using a standard least-squares solver, which is the approach adopted here.\n\n**3. Experimental Design and Evaluation**\n\nThe experiment will proceed as follows for each test case:\n1.  Generate the true function values $f(t_j)$ and noisy observations $y_j = f(t_j) + \\varepsilon_j$ at the $N$ training nodes.\n2.  Compute the coefficients for the exact trigonometric interpolant $p(t)$ using the FFT of $\\{y_j\\}$.\n3.  Solve the linear least-squares problem to find the coefficients for the truncated model $s_M(t)$.\n4.  Evaluate both $p(t)$ and $s_M(t)$ on a dense grid of $T$ test points, $\\{t^*_m\\}$.\n5.  Compute the true function values $f(t^*_m)$ on the same test grid.\n6.  Calculate the RMSE for both methods against the true function values $f(t^*_m)$.\n7.  Compare the RMSE values to determine which method provides better generalization performance, i.e., a lower error on unseen data. The boolean result $\\mathrm{RMSE}_{\\mathrm{LS}} < \\mathrm{RMSE}_{\\mathrm{Interp}}$ quantifies this comparison.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing trigonometric interpolation and least-squares\n    regression by running a suite of test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        {\n            'N': 64, 'K': 3, 'sigma': 0.2, 'M': 3, 'a0': 0.1,\n            'a_k': [1.0, 0.5, 0.2], 'b_k': [0.5, -0.3, 0.1]\n        },\n        # Case 2\n        {\n            'N': 64, 'K': 6, 'sigma': 0.0, 'M': 3, 'a0': 0.0,\n            'a_k': [0.7, -0.4, 0.3, 0.2, -0.1, 0.05],\n            'b_k': [-0.2, 0.3, -0.1, 0.25, 0.2, -0.05]\n        },\n        # Case 3\n        {\n            'N': 64, 'K': 5, 'sigma': 1.0, 'M': 3, 'a0': 0.2,\n            'a_k': [0.9, -0.6, 0.4, -0.3, 0.15],\n            'b_k': [0.5, 0.4, -0.2, 0.3, -0.1]\n        },\n        # Case 4\n        {\n            'N': 16, 'K': 3, 'sigma': 0.5, 'M': 3, 'a0': -0.1,\n            'a_k': [0.9, 0.4, -0.2], 'b_k': [-0.4, 0.35, 0.15]\n        }\n    ]\n\n    T = 4096\n    seed = 314159\n    results = []\n\n    for case in test_cases:\n        N = case['N']\n        K = case['K']\n        sigma = case['sigma']\n        M = case['M']\n        \n        # --- Data Generation ---\n        # Initialize RNG with fixed seed for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Training nodes\n        t_train = 2 * np.pi * np.arange(N) / N\n\n        # True function definition\n        def f(t, params):\n            val = np.full_like(t, params['a0'])\n            for k in range(1, params['K'] + 1):\n                val += params['a_k'][k-1] * np.cos(k * t) + params['b_k'][k-1] * np.sin(k * t)\n            return val\n\n        # True values and noisy observations at training nodes\n        f_train = f(t_train, case)\n        noise = rng.normal(0, sigma, N)\n        y_train = f_train + noise\n\n        # Test grid and true values on the test grid\n        t_test = 2 * np.pi * np.arange(T) / T\n        f_test = f(t_test, case)\n\n        # --- Method 1: Trigonometric Interpolation ---\n        # Compute coefficients using Real FFT\n        C = np.fft.rfft(y_train)\n        A_interp = np.zeros(N // 2 + 1)\n        B_interp = np.zeros(N // 2 - 1)\n        \n        A_interp[0] = C[0] / N\n        if N > 1:\n            A_interp[1:N//2] = 2 * np.real(C[1:N//2]) / N\n            B_interp[:] = -2 * np.imag(C[1:N//2]) / N\n            A_interp[N//2] = C[N//2] / N\n\n        # Evaluate interpolant on the test grid\n        y_interp_pred = np.full_like(t_test, A_interp[0])\n        if N > 1:\n            for k in range(1, N // 2):\n                y_interp_pred += A_interp[k] * np.cos(k * t_test) + B_interp[k-1] * np.sin(k * t_test)\n            y_interp_pred += A_interp[N//2] * np.cos((N // 2) * t_test)\n\n        # --- Method 2: Least-Squares Trigonometric Regression ---\n        # Construct the design matrix X\n        num_coeffs = 1 + 2 * M\n        X = np.zeros((N, num_coeffs))\n        X[:, 0] = 1  # Column for a0\n        for k in range(1, M + 1):\n            X[:, k] = np.cos(k * t_train)      # Columns for a_k\n            X[:, M + k] = np.sin(k * t_train)  # Columns for b_k\n            \n        # Solve the linear least-squares problem\n        coeffs_ls, _, _, _ = np.linalg.lstsq(X, y_train, rcond=None)\n        a_ls = coeffs_ls[0:M+1]\n        b_ls = coeffs_ls[M+1:]\n        \n        # Evaluate the least-squares model on the test grid\n        y_ls_pred = np.full_like(t_test, a_ls[0])\n        for k in range(1, M + 1):\n            y_ls_pred += a_ls[k] * np.cos(k * t_test) + b_ls[k-1] * np.sin(k * t_test)\n\n        # --- Performance Evaluation ---\n        # Calculate RMSE for both methods\n        rmse_interp = np.sqrt(np.mean((y_interp_pred - f_test)**2))\n        rmse_ls = np.sqrt(np.mean((y_ls_pred - f_test)**2))\n\n        # Append boolean result\n        results.append(rmse_ls < rmse_interp)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A correct mathematical formula does not always translate into a reliable numerical algorithm due to the limitations of floating-point arithmetic. This practice delves into the critical issue of numerical stability by examining the evaluation of a trigonometric interpolant . You will implement and compare two methods: the standard barycentric formula, which is prone to catastrophic cancellation, and a stable alternative based on summing the Fourier series, thereby discovering why the choice of algorithm is as important as the underlying mathematical theory.",
            "id": "3284441",
            "problem": "Consider a real-valued, $2\\pi$-periodic function sampled at $n$ equispaced nodes on the interval $[0,2\\pi)$. Let the nodes be $x_j = \\frac{2\\pi j}{n}$ for $j=0,1,\\dots,n-1$, and let the sampled values be $f_j = f(x_j)$. A trigonometric interpolant of degree $m$ with $n=2m+1$ is the unique function in the span of $\\{\\exp(i k x) : k=-m,\\dots,m\\}$ that matches the samples $\\{f_j\\}$. The canonical way to evaluate this interpolant away from the nodes is to use a barycentric-like formula built from fundamental trigonometric identities; however, a straightforward implementation of this standard barycentric evaluation can suffer catastrophic cancellation when the evaluation point is extremely close to a node, since terms of size $O\\!\\left(\\frac{1}{|x-x_j|}\\right)$ with alternating signs are summed.\n\nYour task is to:\n- Start from the discrete Fourier basis and the Dirichlet kernel as a fundamental base, and construct the standard barycentric form for trigonometric interpolation at equispaced nodes (without listing any explicit evaluation formula in the problem statement). Explain why the naive evaluation can lose many digits of accuracy when $x$ approaches a node $x_j$.\n- Implement two evaluators of the trigonometric interpolant:\n  1. A standard barycentric evaluator that directly realizes the derived barycentric form by naive summation in floating-point arithmetic.\n  2. A numerically stable evaluator that uses discrete Fourier series coefficients computed from $\\{f_j\\}$ and then evaluates $\\sum_{k=-m}^m c_k e^{ikx}$ at the target point $x$, where the coefficients $c_k$ are determined from the samples. This representation avoids near-pole cancellations of the barycentric form.\n- Create a specific dataset using the function $f(x) = \\sin(x) + \\frac{1}{2}\\sin(5x) + \\frac{1}{4}\\cos(12x)$, which is exactly representable in a trigonometric space of degree $m \\ge 12$. Use nodes $x_j = \\frac{2\\pi j}{n}$ with odd $n$ so that $n=2m+1$, ensuring representation without aliasing for the chosen $f(x)$.\n\nAngle unit: Use radians throughout.\n\nDefine the following test suite of evaluation scenarios, each specified by $(n, j^\\*, \\varepsilon, x)$, where $x$ is either constructed as $x = x_{j^\\*} + \\varepsilon$ or given directly:\n- Case $1$: $(n=\\;25,\\; j^\\*=\\;10,\\; \\varepsilon=\\;5\\times 10^{-16},\\; x = x_{10} + \\varepsilon)$, an extreme near-node case intended to induce catastrophic cancellation in the naive barycentric evaluation.\n- Case $2$: $(n=\\;25,\\; j^\\*=\\;10,\\; \\varepsilon=\\;0,\\; x = x_{10})$, the boundary case where the interpolant must return the data value exactly.\n- Case $3$: $(n=\\;25,\\; j^\\* \\text{ unused},\\; \\varepsilon \\text{ unused},\\; x=\\;1.234)$, a general “happy path” evaluation well away from nodes.\n- Case $4$: $(n=\\;101,\\; j^\\*=\\;50,\\; \\varepsilon=\\;10^{-12},\\; x = x_{50} + \\varepsilon)$, a larger sample size that increases the dynamic range of near-node terms.\n- Case $5$: $(n=\\;25,\\; j^\\*=\\;10,\\; \\varepsilon=\\;10^{-8},\\; x = x_{10} + \\varepsilon)$, a moderately near-node case.\n\nFor each case:\n- Construct the nodes $\\{x_j\\}$ and samples $f_j = f(x_j)$ with $f(x) = \\sin(x) + \\frac{1}{2}\\sin(5x) + \\frac{1}{4}\\cos(12x)$.\n- Evaluate the interpolant at the specified $x$ using both the naive barycentric evaluator and the stable Fourier-coefficient-based evaluator.\n- Let $p_{\\text{naive}}(x)$ denote the result of the naive barycentric evaluation, and $p_{\\text{stable}}(x)$ denote the Fourier-coefficient-based evaluation.\n- Compute the absolute error $E = |p_{\\text{naive}}(x) - p_{\\text{stable}}(x)|$.\n\nYour program should produce a single line of output containing the results for Cases $1$ through $5$ as a comma-separated list enclosed in square brackets, i.e., $[E_1,E_2,E_3,E_4,E_5]$, where each $E_k$ is a floating-point number in radians-derived units (unitless scalar values), computed as specified above. The final output must be exactly one line in this format.",
            "solution": "The problem requires the derivation of the barycentric formula for trigonometric interpolation at equispaced nodes, an explanation of its numerical instability, and a comparative implementation of a naive barycentric evaluator against a stable Fourier-based evaluator.\n\nA real-valued, $2\\pi$-periodic function $f(x)$ is sampled at $n$ equispaced nodes $x_j = \\frac{2\\pi j}{n}$ for $j=0, 1, \\dots, n-1$, where $n=2m+1$ is an odd integer. The sampled values are $f_j = f(x_j)$. The trigonometric interpolant $p(x)$ is the unique function in the space spanned by $\\{e^{ikx}\\}_{k=-m}^{m}$ such that $p(x_j)=f_j$ for all $j$.\n\nThe interpolant can be expressed in the discrete Fourier basis as:\n$$ p(x) = \\sum_{k=-m}^{m} c_k e^{ikx} $$\nwhere the coefficients $c_k$ are given by the discrete Fourier transform of the sample values $\\{f_j\\}$:\n$$ c_k = \\frac{1}{n} \\sum_{j=0}^{n-1} f_j e^{-ikx_j} $$\nSubstituting the expression for $c_k$ into the formula for $p(x)$ yields:\n$$ p(x) = \\sum_{k=-m}^{m} \\left( \\frac{1}{n} \\sum_{j=0}^{n-1} f_j e^{-ikx_j} \\right) e^{ikx} $$\nBy interchanging the order of summation, we can group terms by the sample values $f_j$:\n$$ p(x) = \\sum_{j=0}^{n-1} f_j \\left( \\frac{1}{n} \\sum_{k=-m}^{m} e^{ik(x-x_j)} \\right) = \\sum_{j=0}^{n-1} f_j L_j(x) $$\nThis is the Lagrange form of the interpolant, where $L_j(x)$ are the cardinal basis functions. The expression for $L_j(x)$ is a scaled version of the Dirichlet kernel:\n$$ L_j(x) = \\frac{1}{n} \\sum_{k=-m}^{m} (e^{i(x-x_j)})^k $$\nThis is a finite geometric series. Letting $u = e^{i(x-x_j)}$, the sum is $\\frac{1}{n} u^{-m} \\sum_{l=0}^{2m} u^l$. Since $n=2m+1$, the sum evaluates to:\n$$ L_j(x) = \\frac{1}{n} u^{-m} \\frac{u^{2m+1}-1}{u-1} = \\frac{1}{n} \\frac{u^{m+1}-u^{-m}}{u-1} = \\frac{1}{n} \\frac{u^{m+1/2}-u^{-(m+1/2)}}{u^{1/2}-u^{-1/2}} $$\nUsing Euler's identity $e^{i\\theta}-e^{-i\\theta} = 2i\\sin(\\theta)$, we obtain the closed form for $L_j(x)$:\n$$ L_j(x) = \\frac{1}{n} \\frac{2i\\sin((m+1/2)(x-x_j))}{2i\\sin((x-x_j)/2)} = \\frac{\\sin(n(x-x_j)/2)}{n\\sin((x-x_j)/2)} $$\nsince $m+1/2 = n/2$.\n\nTo derive the barycentric formula, we simplify the numerator. Given $x_j = 2\\pi j/n$ and that $n$ is odd, we have:\n$$ \\sin\\left(\\frac{n(x-x_j)}{2}\\right) = \\sin\\left(\\frac{nx}{2} - \\pi j\\right) = \\sin\\left(\\frac{nx}{2}\\right)\\cos(\\pi j) - \\cos\\left(\\frac{nx}{2}\\right)\\sin(\\pi j) = (-1)^j \\sin\\left(\\frac{nx}{2}\\right) $$\nThus, the cardinal function becomes:\n$$ L_j(x) = \\frac{(-1)^j\\sin(nx/2)}{n\\sin((x-x_j)/2)} $$\nThe interpolant is $p(x) = \\sum_{j=0}^{n-1} f_j L_j(x)$. Crucially, if we interpolate the constant function $g(x)=1$, its samples are $g_j=1$ for all $j$, and its unique interpolant is $p(x)=1$. This implies that the cardinal functions must sum to one:\n$$ 1 = \\sum_{j=0}^{n-1} L_j(x) = \\sum_{j=0}^{n-1} \\frac{(-1)^j\\sin(nx/2)}{n\\sin((x-x_j)/2)} $$\nDividing the expression for $p(x)$ by the expression for $1$ cancels the common factor $\\sin(nx/2)/n$, yielding the standard barycentric formula for trigonometric interpolation (for odd $n$):\n$$ p(x) = \\frac{\\sum_{j=0}^{n-1} \\frac{(-1)^j f_j}{\\sin((x-x_j)/2)}}{\\sum_{j=0}^{n-1} \\frac{(-1)^j}{\\sin((x-x_j)/2)}} $$\n\n**Numerical Instability of the Naive Barycentric Evaluation:**\nThe instability of this formula arises when the evaluation point $x$ is very close to, but not exactly at, one of the interpolation nodes $x_k$. Let $x = x_k + \\varepsilon$ for a small value $\\varepsilon$.\n1.  The term in the sums corresponding to $j=k$ involves $\\sin((x-x_k)/2) = \\sin(\\varepsilon/2) \\approx \\varepsilon/2$. This denominator is extremely small.\n2.  Consequently, the $k$-th term in both the numerator sum, $\\frac{(-1)^k f_k}{\\sin(\\varepsilon/2)}$, and the denominator sum, $\\frac{(-1)^k}{\\sin(\\varepsilon/2)}$, becomes enormous, with magnitude $O(1/|\\varepsilon|)$.\n3.  All other terms ($j \\neq k$) in the sums remain of moderate size, as their denominators $\\sin((x-x_j)/2)$ are not close to zero.\n4.  In floating-point arithmetic, the summation of one very large number with several smaller numbers causes a loss of the less significant digits of the large number, which are precisely the digits that would have combined with the smaller numbers. Effectively, the information contributed by the smaller terms ($j \\neq k$) is partially or completely lost.\n5.  The final result is a ratio of two computed large numbers. Since the true value of the interpolant $p(x)$ must be close to $f_k$, the true numerator and denominator must be very close to a simple ratio. This implies that the sum of all terms for $j \\neq k$ must nearly cancel the dominant $k$-th term. This subtraction of nearly equal, large quantities (where one quantity is a single term and the other is a sum) is a classic recipe for catastrophic cancellation, leading to a result with a large relative error and few, if any, correct digits.\n\n**Stable Evaluation Method:**\nA numerically stable alternative is to revert to the fundamental definition of the interpolant:\n$$ p(x) = \\sum_{k=-m}^{m} c_k e^{ikx} $$\nThis method involves two steps:\n1.  Compute the Fourier coefficients $c_k = \\frac{1}{n} \\sum_{j=0}^{n-1} f_j e^{-i k (2\\pi j/n)}$ for $k = -m, \\ldots, m$. This is performed efficiently and accurately using the Fast Fourier Transform (FFT) algorithm.\n2.  Evaluate the sum directly. This summation is numerically stable. The terms being added, $c_k e^{ikx}$, are of magnitudes comparable to the coefficients $c_k$ (since $|e^{ikx}|=1$). There are no large intermediate terms that cancel, thus avoiding catastrophic cancellation. This method is used as the \"ground truth\" to measure the error of the naive barycentric approach.\n\nThe implementation will compute the absolute error $|p_{\\text{naive}}(x) - p_{\\text{stable}}(x)|$ for each test case. For a case where $x$ is exactly a node $x_k$, the naive evaluator must be handled specially to return $f_k$ and avoid division by zero.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives, implements, and compares two trigonometric interpolation evaluation methods.\n    \"\"\"\n\n    # Define the target function f(x)\n    def f(x):\n        \"\"\"\n        The function to be interpolated. It is a trigonometric polynomial of degree 12.\n        f(x) = sin(x) + 1/2*sin(5x) + 1/4*cos(12x)\n        \"\"\"\n        return np.sin(x) + 0.5 * np.sin(5 * x) + 0.25 * np.cos(12 * x)\n\n    # Method 1: Numerically stable evaluator using Fourier coefficients\n    def stable_evaluator(x, f_j, n):\n        \"\"\"\n        Evaluates the trigonometric interpolant using its Fourier series representation.\n        This method is numerically stable.\n        \"\"\"\n        m = (n - 1) // 2\n        \n        # Compute Fourier coefficients c_k using the Fast Fourier Transform (FFT)\n        f_coeffs = np.fft.fft(f_j) / n\n        \n        # Shift coefficients to correspond to frequencies -m, ..., 0, ..., m\n        c_shifted = np.fft.fftshift(f_coeffs)\n        \n        # Corresponding integer frequencies k = -m, ..., m\n        k_vals = np.arange(-m, m + 1)\n        \n        # Evaluate p(x) = sum_{k=-m to m} c_k * exp(i*k*x)\n        val = np.sum(c_shifted * np.exp(1j * k_vals * x))\n        \n        # The result must be real as f(x) is real. Take the real part to discard\n        # imaginary noise from floating point inaccuracies.\n        return np.real(val)\n\n    # Method 2: Naive barycentric formula evaluator\n    def naive_barycentric_evaluator(x, f_j, x_j, n):\n        \"\"\"\n        Evaluates the trigonometric interpolant using the standard barycentric formula.\n        This method is prone to catastrophic cancellation near nodes.\n        \"\"\"\n        # A direct check for whether x is exactly one of the nodes.\n        # This is necessary to handle the interpolating property p(x_j) = f_j\n        # and to avoid division by zero. Using '==' is intentional to only catch\n        # the exact node case (Test Case 2) and not the near-node cases.\n        match_indices = np.where(x == x_j)[0]\n        if len(match_indices) > 0:\n            return f_j[match_indices[0]]\n\n        # If x is not an exact node, proceed with the barycentric formula\n        numerator = 0.0\n        denominator = 0.0\n        for j in range(n):\n            # Barycentric weights for equispaced points with n odd\n            w_j = (-1)**j\n            \n            # Denominator of the j-th term in the barycentric sum.\n            # This becomes very small if x is close to x_j, leading to a large term.\n            term_den = np.sin((x - x_j[j]) / 2.0)\n            \n            term = w_j / term_den\n            \n            numerator += term * f_j[j]\n            denominator += term\n        \n        # The ratio gives the interpolated value. For x near a node, this is a\n        # ratio of two very large numbers, which is where precision is lost.\n        return numerator / denominator\n\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (Parameter set 1): Case 1: Extreme near-node case\n        {'n': 25, 'j_star': 10, 'epsilon': 5e-16, 'x_spec': 'relative'},\n        # (Parameter set 2): Case 2: Exactly at a node\n        {'n': 25, 'j_star': 10, 'epsilon': 0.0, 'x_spec': 'relative'},\n        # (Parameter set 3): Case 3: A generic point far from nodes\n        {'n': 25, 'x': 1.234, 'x_spec': 'absolute'},\n        # (Parameter set 4): Case 4: Larger n, near-node\n        {'n': 101, 'j_star': 50, 'epsilon': 1e-12, 'x_spec': 'relative'},\n        # (Parameter set 5): Case 5: Moderately near-node\n        {'n': 25, 'j_star': 10, 'epsilon': 1e-8, 'x_spec': 'relative'},\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case['n']\n        \n        # Construct nodes and sample data\n        nodes_x_j = (2 * np.pi / n) * np.arange(n)\n        samples_f_j = f(nodes_x_j)\n        \n        # Determine the evaluation point x for the current case\n        if case['x_spec'] == 'relative':\n            j_star = case['j_star']\n            epsilon = case['epsilon']\n            x_eval = nodes_x_j[j_star] + epsilon\n        else:  # 'absolute'\n            x_eval = case['x']\n            \n        # Evaluate the interpolant at x_eval using both methods\n        p_stable = stable_evaluator(x_eval, samples_f_j, n)\n        p_naive = naive_barycentric_evaluator(x_eval, samples_f_j, nodes_x_j, n)\n        \n        # Compute the absolute error between the two evaluations.\n        # p_stable is treated as the ground truth.\n        error = np.abs(p_naive - p_stable)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}