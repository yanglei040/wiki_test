{
    "hands_on_practices": [
        {
            "introduction": "The best way to understand forward mode automatic differentiation is to build it yourself. This practice guides you in creating a `Dual` number class, an elegant structure that bundles a value $v$ with its derivative $v'$. By teaching standard arithmetic operators how to handle these `Dual` numbers, you will see how derivatives can be propagated forward through any calculation automatically. This hands-on implementation demystifies the mechanics of forward mode AD and showcases the power of operator overloading in scientific computing .",
            "id": "3207038",
            "problem": "You are asked to implement forward mode automatic differentiation (forward mode AD) using operator overloading in a high-level language. The core idea is to track both the value and the derivative of expressions with respect to a single scalar input by propagating derivatives through elementary operations. You will create a class that represents a pair consisting of a real value and its derivative with respect to an independent variable. You must then use this class to evaluate a specific polynomial and its derivative at a set of inputs. The implementation must not rely on any prebuilt automatic differentiation tools or symbolic manipulation libraries.\n\nImplement a class whose instances represent numbers augmented with derivative information. The class must support standard arithmetic so that when the class is used in a computation, both the value and the derivative are computed automatically. The implementation must support addition, subtraction, multiplication, division, and integer exponentiation. Interactions with built-in numeric types must behave naturally on either side of the operator.\n\nUse the class to evaluate the polynomial function\n$$\nf(x) = 3x^{5} - 2x^{3} + 7x - 11\n$$\nand its derivative at several inputs. Your program should compute, for each input $x$, both the function value $f(x)$ and the derivative $f'(x)$ using forward mode automatic differentiation, and also compute the same two quantities using ordinary floating-point evaluation and standard calculus for verification. For each input $x$, compute the absolute errors\n$$\ne_{\\text{val}} = \\lvert f_{\\text{AD}}(x) - f(x) \\rvert, \\quad e_{\\text{der}} = \\lvert f'_{\\text{AD}}(x) - f'(x) \\rvert,\n$$\nwhere $f_{\\text{AD}}(x)$ and $f'_{\\text{AD}}(x)$ denote, respectively, the value and derivative returned by your automatic differentiation evaluation.\n\nAngle units are not involved. No physical units are involved.\n\nTest suite:\n- Evaluate at the following ordered list of inputs $[0.0, 1.0, -1.0, 2.5, 10.0]$. These cover a boundary input $0.0$, simple integer inputs $1.0$ and $-1.0$, a non-integer input $2.5$, and a larger magnitude input $10.0$.\n\nProgram requirements:\n- Define the polynomial $f(x)$ once using arithmetic operators; a single definition must work identically whether $x$ is a built-in real number or an instance of your class.\n- For each test input $x$ in the specified order, create an instance representing the independent variable with derivative $1.0$, evaluate the polynomial to obtain $f_{\\text{AD}}(x)$ and $f'_{\\text{AD}}(x)$, evaluate the same polynomial using ordinary floating-point arithmetic to obtain $f(x)$, evaluate the analytical derivative $f'(x)$ using standard calculus, and compute $e_{\\text{val}}$ and $e_{\\text{der}}$ as defined above.\n- Collect the results in the same order as the inputs.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element must be a two-element list $[e_{\\text{val}}, e_{\\text{der}}]$ for the corresponding input. For example, the printed line should look like\n\"[ [e_val_for_x0, e_der_for_x0], [e_val_for_x1, e_der_for_x1], ... ]\"\nwith no spaces requirement enforced other than those produced by your print, but it must be a single line that encodes a list of lists in the natural languageâ€™s list syntax.\n\nYour program must be self-contained and must not read any input from the user or any external file. The results for the provided test suite must be the only content of the printed output line.",
            "solution": "We derive forward mode automatic differentiation (automatic differentiation (AD)) for a single scalar input using well-established differential calculus rules. The foundational base comprises standard differentiation rules: linearity, the product rule, the quotient rule, the chain rule, and the power rule for integer exponents.\n\nWe represent a computation on an independent variable $x$ by associating to each intermediate quantity a pair $\\left(v, d\\right)$, where $v$ is the real value and $d = \\frac{dv}{dx}$ is the derivative with respect to $x$. We now derive how to propagate these pairs through arithmetic operations directly from calculus.\n\nAddition and subtraction:\nSuppose $u = (u, u')$ and $v = (v, v')$ represent two differentiable quantities with respect to $x$. Then\n$$\n(u + v, \\, \\frac{d}{dx}(u+v)) = (u + v, \\, u' + v'),\n$$\nand similarly for subtraction,\n$$\n(u - v, \\, \\frac{d}{dx}(u-v)) = (u - v, \\, u' - v').\n$$\nThese follow from linearity of differentiation.\n\nMultiplication:\nUsing the product rule,\n$$\n\\frac{d}{dx}(uv) = u'v + uv'.\n$$\nHence, for pairs $(u,u')$ and $(v,v')$, the product becomes\n$$\n(uv, \\, u'v + uv').\n$$\n\nDivision:\nUsing the quotient rule,\n$$\n\\frac{d}{dx}\\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^{2}},\n$$\nso the propagated pair is\n$$\n\\left(\\frac{u}{v}, \\, \\frac{u'v - uv'}{v^{2}}\\right),\n$$\nprovided $v \\neq 0$.\n\nInteger power:\nFor an integer $n$, by the power rule and chain rule,\n$$\n\\frac{d}{dx}\\left(u^{n}\\right) = n u^{n-1} u'.\n$$\nThus the propagated pair is\n$$\n\\left(u^{n}, \\, n u^{n-1} u'\\right).\n$$\nFor a real constant exponent $p$, the same expression $\\left(u^{p}, \\, p u^{p-1} u'\\right)$ holds when $u > 0$, but for this task it suffices to correctly support integer exponents because the target function is a polynomial.\n\nForward mode AD implementation:\nWe encode each quantity as a class instance storing two real numbers $(v,d)$. The class overloads the arithmetic operators so that when it participates in an expression, it applies the above propagation formulas. Interactions with built-in real numbers are handled by treating a real $c$ as the pair $(c, 0)$. The independent variable $x$ is represented as $(x,1)$.\n\nTarget function:\nWe consider the polynomial\n$$\nf(x) = 3x^{5} - 2x^{3} + 7x - 11.\n$$\nBecause $f$ is a polynomial, when it is written once in terms of overloaded arithmetic, it computes both the value and the derivative automatically. For verification, we use its analytical derivative obtained from standard calculus:\n$$\nf'(x) = 15x^{4} - 6x^{2} + 7.\n$$\n\nAlgorithmic steps:\n1. Define a class whose instances store $(v,d)$ and implement the overloaded operators $+$, $-$, $\\times$, $\\div$, and exponentiation by an integer as derived above.\n2. Define $f(x)$ once using arithmetic operators. This single definition can be applied to either a built-in real or to an instance of the class.\n3. For each test input $x \\in \\{0.0, 1.0, -1.0, 2.5, 10.0\\}$, create the seeded variable $(x,1)$, evaluate $f$ to obtain $(f_{\\text{AD}}(x), f'_{\\text{AD}}(x))$, evaluate $f(x)$ and $f'(x)$ as ordinary floating-point computations, and compute\n$$\ne_{\\text{val}} = \\lvert f_{\\text{AD}}(x) - f(x) \\rvert, \\quad e_{\\text{der}} = \\lvert f'_{\\text{AD}}(x) - f'(x) \\rvert.\n$$\n4. Collect the pair $[e_{\\text{val}}, e_{\\text{der}}]$ for each $x$ in the specified order and print them as a single list on one line.\n\nCorrectness:\nThe propagation rules are directly derived from the linearity of differentiation, the product rule, the quotient rule, and the power rule. Because a polynomial is composed via repeated applications of these elementary operations, the forward mode AD evaluation yields the exact analytical derivative of $f$ at any point $x$ within floating-point rounding. Therefore, the absolute errors $e_{\\text{val}}$ and $e_{\\text{der}}$ are expected to be extremely small, typically near zero up to floating-point roundoff, across all test inputs.\n\nOutput:\nThe program prints a single line representing a list of two-element lists $[e_{\\text{val}}, e_{\\text{der}}]$, one per test case, in the same order as the inputs. No other output is produced.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass Dual:\n    \"\"\"\n    Dual number for forward-mode automatic differentiation with respect to a single scalar variable.\n    Each instance represents a pair (value, derivative).\n    \"\"\"\n    __slots__ = (\"val\", \"der\")\n\n    def __init__(self, val, der=0.0):\n        self.val = float(val)\n        self.der = float(der)\n\n    @staticmethod\n    def _coerce(other):\n        if isinstance(other, Dual):\n            return other\n        else:\n            return Dual(other, 0.0)\n\n    # Addition\n    def __add__(self, other):\n        o = Dual._coerce(other)\n        return Dual(self.val + o.val, self.der + o.der)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    # Subtraction\n    def __sub__(self, other):\n        o = Dual._coerce(other)\n        return Dual(self.val - o.val, self.der - o.der)\n\n    def __rsub__(self, other):\n        o = Dual._coerce(other)\n        return Dual(o.val - self.val, o.der - self.der)\n\n    # Multiplication\n    def __mul__(self, other):\n        o = Dual._coerce(other)\n        # (u, u') * (v, v') = (uv, u'v + uv')\n        return Dual(self.val * o.val, self.der * o.val + self.val * o.der)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    # True division\n    def __truediv__(self, other):\n        o = Dual._coerce(other)\n        # (u, u') / (v, v') = (u/v, (u'v - uv')/v^2)\n        denom = o.val * o.val\n        return Dual(self.val / o.val, (self.der * o.val - self.val * o.der) / denom)\n\n    def __rtruediv__(self, other):\n        o = Dual._coerce(other)\n        # o / self\n        denom = self.val * self.val\n        return Dual(o.val / self.val, (o.der * self.val - o.val * self.der) / denom)\n\n    # Unary negation\n    def __neg__(self):\n        return Dual(-self.val, -self.der)\n\n    # Power: support real (int/float) exponents, commonly used for integer exponents in polynomials\n    def __pow__(self, power):\n        if isinstance(power, (int, float)):\n            if power == 0:\n                # x**0 = 1, derivative 0\n                return Dual(1.0, 0.0)\n            # For real power p: d(x**p) = p * x**(p - 1) * dx\n            primal = self.val ** power\n            deriv = power * (self.val ** (power - 1.0)) * self.der\n            return Dual(primal, deriv)\n        else:\n            raise TypeError(\"Power must be a real number for Dual.__pow__\")\n\ndef poly(x):\n    # f(x) = 3x^5 - 2x^3 + 7x - 11\n    return 3 * (x ** 5) - 2 * (x ** 3) + 7 * x - 11\n\ndef poly_float(x):\n    return 3.0 * (x ** 5) - 2.0 * (x ** 3) + 7.0 * x - 11.0\n\ndef poly_derivative_float(x):\n    # f'(x) = 15x^4 - 6x^2 + 7\n    return 15.0 * (x ** 4) - 6.0 * (x ** 2) + 7.0\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [0.0, 1.0, -1.0, 2.5, 10.0]\n\n    results = []\n    for x in test_cases:\n        # Seed the independent variable: derivative w.r.t. x is 1\n        dx = Dual(x, 1.0)\n        y = poly(dx)              # Dual result: (value, derivative)\n        f_val = poly_float(x)     # Float value\n        f_der = poly_derivative_float(x)  # Analytical derivative\n\n        val_err = abs(y.val - f_val)\n        der_err = abs(y.der - f_der)\n\n        results.append([val_err, der_err])\n\n    # Final print statement in the exact required format: a single line list of [val_err, der_err] pairs.\n    # Format with reasonable precision for readability.\n    formatted = \"[\" + \",\".join(f\"[{val:.12g},{der:.12g}]\" for val, der in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Reverse mode automatic differentiation is the engine behind deep learning, but its mechanism can seem like magic. This exercise pulls back the curtain by having you act as the AD engine. You will first trace a function's evaluation forward to build a computational graph, then meticulously work backward, applying the chain rule at each step to propagate gradients from the output to the inputs. This pencil-and-paper practice builds a powerful intuition for the backward pass and the Vector-Jacobian Products (VJPs) that make efficient gradient computation possible .",
            "id": "3100431",
            "problem": "Consider reverse-mode automatic differentiation (AD) in the context of deep learning, where gradients of scalar loss functions with respect to parameters are computed efficiently by traversing a computational graph backward using the chain rule. The function of interest is the scalar map $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ with $y\\neq 0$. Using only elementary operations compatible with a computational graph (multiplication, sine, exponential, and division), construct a minimal set of intermediate variables that evaluates $f(x,y)$ and a tape that records the parent-child relationships for these operations. Then, using the principle of the chain rule for composite functions and the concept of Vector-Jacobian Product (VJP), derive the exact sequence of backward passes (VJP pulls) needed to obtain the gradient $\\nabla f(x,y)$ by hand. Your derivation should clearly identify the order of traversing the tape in reverse and the local contributions to the adjoints of the inputs at each step. Provide the final analytical expression for $\\nabla f(x,y)$ as a row vector. Do not round; the final answer must be an exact symbolic expression.",
            "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, objective, and contains sufficient information to derive a unique and meaningful solution. The task concerns the application of reverse-mode automatic differentiation (AD), a cornerstone algorithm in computational calculus and deep learning, to a differentiable function. The procedure is formalizable and aligns with established principles.\n\nWe are tasked with computing the gradient $\\nabla f(x,y)$ of the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ given by $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ for $y \\neq 0$, using the principles of reverse-mode AD. This involves a forward pass to construct a computational graph and evaluate the function, followed by a backward pass to propagate gradients.\n\nFirst, we decompose the function into a sequence of elementary operations. This sequence defines the computational graph, or \"tape\". Let the inputs be $v_1 = x$ and $v_2 = y$.\n\n**Forward Pass: Constructing the Computational Graph**\n\nThe evaluation of $f(x,y)$ can be represented by the following minimal set of intermediate variables:\n1.  $v_3 = v_1 \\cdot v_2 = x \\cdot y$\n2.  $v_4 = \\sin(v_3) = \\sin(xy)$\n3.  $v_5 = \\exp(v_1) = \\exp(x)$\n4.  $v_6 = \\frac{v_5}{v_2} = \\frac{\\exp(x)}{y}$\n5.  $v_7 = v_4 + v_6 = \\sin(xy) + \\frac{\\exp(x)}{y} = f(x,y)$\n\nThis sequence constitutes the forward pass. The tape records these operations and their dependencies: $(v_3, \\text{mul}, v_1, v_2)$, $(v_4, \\sin, v_3)$, $(v_5, \\exp, v_1)$, $(v_6, \\text{div}, v_5, v_2)$, $(v_7, \\text{add}, v_4, v_6)$.\n\n**Backward Pass: Gradient Computation using the Chain Rule**\n\nThe backward pass computes the partial derivatives of the final output $v_7$ with respect to each intermediate variable $v_i$, which are denoted as adjoints, $\\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\frac{\\partial v_7}{\\partial v_i}$. The process begins by initializing the adjoint of the output node to $1$, i.e., $\\bar{v}_7 = \\frac{\\partial v_7}{\\partial v_7} = 1$. All other adjoints are initialized to $0$. We then traverse the graph in reverse topological order.\n\nThe core principle is the chain rule. For an operation $v_k = g(v_i, v_j, \\dots)$, the adjoints of its parents are updated by accumulating the child's adjoint multiplied by the local partial derivative:\n$$ \\bar{v}_i = \\bar{v}_i + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_i} $$\n$$ \\bar{v}_j = \\bar{v}_j + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_j} $$\n... and so on. This operation is effectively a Vector-Jacobian Product (VJP) pull.\n\nLet's compute the adjoints in reverse order of the forward pass:\n\n1.  **Start:** Initialize adjoints: $\\bar{v}_1=0, \\bar{v}_2=0, \\bar{v}_3=0, \\bar{v}_4=0, \\bar{v}_5=0, \\bar{v}_6=0$.\n    Set the seed for the backward pass: $\\bar{v}_7 = 1$.\n\n2.  **Node $v_7 = v_4 + v_6$:**\n    The parents are $v_4$ and $v_6$.\n    Local partial derivatives: $\\frac{\\partial v_7}{\\partial v_4} = 1$, $\\frac{\\partial v_7}{\\partial v_6} = 1$.\n    Update parent adjoints:\n    $\\bar{v}_4 \\leftarrow \\bar{v}_4 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_4} = 0 + 1 \\cdot 1 = 1$.\n    $\\bar{v}_6 \\leftarrow \\bar{v}_6 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_6} = 0 + 1 \\cdot 1 = 1$.\n    Current state: $\\bar{v}_4=1, \\bar{v}_6=1$.\n\n3.  **Node $v_6 = \\frac{v_5}{v_2}$:**\n    The parents are $v_5$ and $v_2$.\n    Local partial derivatives: $\\frac{\\partial v_6}{\\partial v_5} = \\frac{1}{v_2}$, $\\frac{\\partial v_6}{\\partial v_2} = -\\frac{v_5}{v_2^2}$.\n    Update parent adjoints:\n    $\\bar{v}_5 \\leftarrow \\bar{v}_5 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_5} = 0 + 1 \\cdot \\frac{1}{v_2} = \\frac{1}{y}$.\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_2} = 0 + 1 \\cdot \\left(-\\frac{v_5}{v_2^2}\\right) = -\\frac{\\exp(x)}{y^2}$.\n    Current state: $\\bar{v}_5 = \\frac{1}{y}$, $\\bar{v}_2=-\\frac{\\exp(x)}{y^2}$.\n\n4.  **Node $v_5 = \\exp(v_1)$:**\n    The parent is $v_1$.\n    Local partial derivative: $\\frac{\\partial v_5}{\\partial v_1} = \\exp(v_1)$.\n    Update parent adjoint:\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_5 \\cdot \\frac{\\partial v_5}{\\partial v_1} = 0 + \\frac{1}{y} \\cdot \\exp(v_1) = \\frac{\\exp(x)}{y}$.\n    Current state: $\\bar{v}_1 = \\frac{\\exp(x)}{y}$.\n\n5.  **Node $v_4 = \\sin(v_3)$:**\n    The parent is $v_3$.\n    Local partial derivative: $\\frac{\\partial v_4}{\\partial v_3} = \\cos(v_3)$.\n    Update parent adjoint:\n    $\\bar{v}_3 \\leftarrow \\bar{v}_3 + \\bar{v}_4 \\cdot \\frac{\\partial v_4}{\\partial v_3} = 0 + 1 \\cdot \\cos(v_3) = \\cos(xy)$.\n    Current state: $\\bar{v}_3 = \\cos(xy)$.\n\n6.  **Node $v_3 = v_1 \\cdot v_2$:**\n    The parents are $v_1$ and $v_2$. Note that $v_1$ and $v_2$ have already received gradients from other paths; we accumulate the new contributions.\n    Local partial derivatives: $\\frac{\\partial v_3}{\\partial v_1} = v_2$, $\\frac{\\partial v_3}{\\partial v_2} = v_1$.\n    Update parent adjoints:\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_1} = \\frac{\\exp(x)}{y} + \\cos(xy) \\cdot v_2 = \\frac{\\exp(x)}{y} + y \\cos(xy)$.\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_2} = -\\frac{\\exp(x)}{y^2} + \\cos(xy) \\cdot v_1 = -\\frac{\\exp(x)}{y^2} + x \\cos(xy)$.\n\nThe process terminates as we have computed the adjoints for all input nodes.\nThe final gradients are the final values of the adjoints of the input variables:\n$$ \\frac{\\partial f}{\\partial x} = \\bar{v}_1 = y \\cos(xy) + \\frac{\\exp(x)}{y} $$\n$$ \\frac{\\partial f}{\\partial y} = \\bar{v}_2 = x \\cos(xy) - \\frac{\\exp(x)}{y^2} $$\n\nThe gradient vector $\\nabla f(x,y)$ is the row vector of these partial derivatives:\n$$ \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} y \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2} \\end{pmatrix} $$\nThis derivation rigorously follows the mechanical steps of reverse-mode automatic differentiation.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\ny \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A robust AD system must not only respect the laws of calculus but also the realities of computer arithmetic. This practice delves into the crucial concept of numerical stability by examining the simple reciprocal function, $y = 1/x$. You will implement and contrast a 'naive' derivative calculation with a 'stabilized' one, empirically testing how they behave with inputs extremely close to zero. This exercise provides a clear, practical demonstration of why the choice between algebraically equivalent formulas is critical for building accurate and reliable numerical tools .",
            "id": "3207130",
            "problem": "Implement a small automatic differentiation system for the scalar reciprocal function using both forward mode and reverse mode. Starting only from the following foundational bases: the definition of the derivative as the linear map given by the chain rule, the dual-number model of forward mode with an infinitesimal $\\varepsilon$ satisfying $\\varepsilon^2 = 0$, and the adjoint (also called sensitivity) propagation viewpoint of reverse mode, derive numerically stable update rules for the reciprocal node and measure gradient accuracy across a range of inputs including values near zero.\n\nYour tasks are:\n\n1) Derive a forward mode rule for the reciprocal node, where the primal is $y = 1/x$. Use the dual-number definition that a forward mode value is represented as $x + \\dot{x}\\,\\varepsilon$ and that composition follows the chain rule. From this base, derive an update for the tangent $\\dot{y}$ in terms of the primal outputs rather than recomputing powers of the inputs. This should yield a formulation that reuses the computed $y$ to form a numerically stabilized update.\n\n2) Derive a reverse mode rule for the reciprocal node. In reverse mode, define the adjoint (sensitivity) $\\bar{y}$ for $y$ and show how to propagate it to the input sensitivity $\\bar{x}$ through a pullback that depends on the primal values. Express the update using the primal output $y$.\n\n3) Implement four rules in a program:\n- Forward mode naive reciprocal: propagate $\\dot{y}$ directly as a function of $x$.\n- Forward mode stabilized reciprocal: propagate $\\dot{y}$ using only already computed primal outputs where possible.\n- Reverse mode naive reciprocal: propagate $\\bar{x}$ directly as a function of $x$.\n- Reverse mode stabilized reciprocal: propagate $\\bar{x}$ using only already computed primal outputs where possible.\n\n4) Verification method. For each $x$ in a given set, compute the analytical derivative using the fundamental definition of calculus for the reciprocal node, and compare it to the derivative produced by each of the four automatic differentiation implementations. Use relative error, defined for each $x$ as\n$$\n\\mathrm{rel\\_err}(x) \\;=\\; \\frac{\\lvert g_{\\mathrm{AD}}(x) - g_{\\mathrm{exact}}(x)\\rvert}{\\lvert g_{\\mathrm{exact}}(x)\\rvert},\n$$\nwhenever the denominator is finite and nonzero. Ignore any $x$ that would produce a non-finite exact derivative or a non-finite automatic differentiation derivative in the error aggregation.\n\n5) Test suite. Evaluate and aggregate the maximum relative error over each of the following three input sets, designed to cover a typical range, near-zero inputs, and boundary values close to the largest finite derivative that should not overflow in double precision:\n- Case A (general range): $[-123.45,\\,-0.0314159,\\,-10^{-8},\\,10^{-8},\\,0.0314159,\\,123.45]$.\n- Case B (near-zero but finite derivatives): $[1.2345\\times 10^{-120},\\,-4.321\\times 10^{-110},\\,7.89\\times 10^{-130},\\,-9.99\\times 10^{-140},\\,5.0\\times 10^{-150},\\,-8.0\\times 10^{-150}]$.\n- Case C (boundary near overflow of the derivative but still finite): $[1.5\\times 10^{-154},\\,-1.5\\times 10^{-154},\\,2.2\\times 10^{-154},\\,-2.2\\times 10^{-154}]$.\n\nFor each case, compute four aggregated metrics:\n- Maximum relative error for forward mode naive.\n- Maximum relative error for forward mode stabilized.\n- Maximum relative error for reverse mode naive.\n- Maximum relative error for reverse mode stabilized.\n\n6) Final output format. Your program should produce a single line of output containing all twelve results, ordered by cases A, B, C, each contributing the four metrics above, as a comma-separated list enclosed in square brackets. The order is:\n$[\\mathrm{A\\_FwdNaive},\\mathrm{A\\_FwdStab},\\mathrm{A\\_RevNaive},\\mathrm{A\\_RevStab},\\mathrm{B\\_FwdNaive},\\mathrm{B\\_FwdStab},\\mathrm{B\\_RevNaive},\\mathrm{B\\_RevStab},\\mathrm{C\\_FwdNaive},\\mathrm{C\\_FwdStab},\\mathrm{C\\_RevNaive},\\mathrm{C\\_RevStab}]$.\nAs an example of the formatting only, a list like $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8,r_9,r_{10},r_{11},r_{12}]$ is acceptable. Your program must not read input, and must compute and print these twelve values using double precision arithmetic. No physical units are involved. All angles, if any, must be in radians; however, no trigonometric functions are required in this task.",
            "solution": "The problem requires the derivation and implementation of four different automatic differentiation (AD) rules for the scalar reciprocal function, $y = f(x) = 1/x$. These rules are categorized as \"naive\" and \"stabilized\" for both forward and reverse modes of AD. The objective is to compare their numerical accuracy, particularly for inputs that challenge the limits of double-precision floating-point arithmetic.\n\n### 1. Preliminaries: The Analytical Derivative\n\nThe function under consideration is the reciprocal function:\n$$\ny = f(x) = \\frac{1}{x} = x^{-1}\n$$\nThe analytical derivative, which serves as our ground truth for accuracy validation, is obtained by the power rule of differentiation:\n$$\ng_{\\mathrm{exact}}(x) = \\frac{dy}{dx} = -1 \\cdot x^{-2} = -\\frac{1}{x^2}\n$$\nThe core of the problem is to evaluate how closely the AD methods can approximate this exact derivative, $g_{\\mathrm{exact}}(x)$.\n\n### 2. Forward Mode Automatic Differentiation\n\nForward mode AD is based on the propagation of derivatives forward through a computation graph. We use the dual number representation, where a number $v$ and its derivative $\\dot{v}$ with respect to an independent variable are combined into a single entity $v + \\dot{v}\\varepsilon$, with the property $\\varepsilon^2 = 0$.\n\nLet the input be represented by the dual number $x + \\dot{x}\\varepsilon$. We wish to find the corresponding dual number for the output, $y + \\dot{y}\\varepsilon$.\n$$\ny + \\dot{y}\\varepsilon = f(x + \\dot{x}\\varepsilon) = \\frac{1}{x + \\dot{x}\\varepsilon}\n$$\nTo separate the primal ($y$) and tangent ($\\dot{y}$) components, we perform an algebraic manipulation analogous to a first-order Taylor expansion:\n$$\n\\frac{1}{x + \\dot{x}\\varepsilon} = \\frac{1}{x(1 + \\frac{\\dot{x}}{x}\\varepsilon)} = \\frac{1}{x} \\left(1 + \\frac{\\dot{x}}{x}\\varepsilon\\right)^{-1}\n$$\nUsing the geometric series expansion $(1+u)^{-1} = 1 - u + u^2 - \\dots$ with $u = \\frac{\\dot{x}}{x}\\varepsilon$, and leveraging the property $\\varepsilon^2=0$ which truncates the series after the linear term:\n$$\n\\frac{1}{x} \\left(1 - \\frac{\\dot{x}}{x}\\varepsilon + \\mathcal{O}(\\varepsilon^2)\\right) = \\frac{1}{x} - \\frac{\\dot{x}}{x^2}\\varepsilon\n$$\nBy comparing this result with the form $y + \\dot{y}\\varepsilon$, we extract the update rules:\n- Primal update: $y = \\frac{1}{x}$\n- Tangent update: $\\dot{y} = -\\frac{\\dot{x}}{x^2}$\n\nFrom this, we derive the two required forward mode rules. To compute the derivative $f'(x)$, we set the seed tangent $\\dot{x}=1$.\n\n**Forward Mode Naive Rule:**\nThe rule is implemented by directly using the input primal value $x$ to compute the tangent.\n$$\n\\dot{y} = -\\frac{1}{x^2}\n$$\nThis rule requires computing $x^2$ and then performing a division.\n\n**Forward Mode Stabilized Rule:**\nThis rule is formulated to reuse the computed primal output $y = 1/x$. A simple substitution into the tangent update formula yields:\n$$\n\\dot{y} = -\\frac{\\dot{x}}{x^2} = -\\dot{x} \\left(\\frac{1}{x}\\right)^2 = -\\dot{x} y^2\n$$\nWith $\\dot{x}=1$, the rule becomes:\n$$\n\\dot{y} = -y^2\n$$\nThis variant first computes $y=1/x$ and then squares the result, avoiding a re-computation involving the input $x$.\n\n### 3. Reverse Mode Automatic Differentiation\n\nReverse mode AD propagates sensitivities, or adjoints, backward from the output of a computation to its inputs. The adjoint of a variable $v$, denoted $\\bar{v}$, is defined as the partial derivative of a final scalar objective function $L$ with respect to $v$, i.e., $\\bar{v} = \\frac{\\partial L}{\\partial v}$.\n\nFor a function $y=f(x)$, the chain rule for adjoints relates the input adjoint $\\bar{x}$ to the output adjoint $\\bar{y}$:\n$$\n\\bar{x} = \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{dy}{dx} = \\bar{y} \\frac{dy}{dx}\n$$\nThe term $\\frac{dy}{dx}$ is the local partial derivative of the node. In our case, $\\frac{dy}{dx} = -1/x^2$. Substituting this into the adjoint propagation rule gives:\n$$\n\\bar{x} = \\bar{y} \\left(-\\frac{1}{x^2}\\right)\n$$\nTo find the derivative of the function $f(x)$ itself, we can conceptualize the objective function as being the function output, $L=y$. In this setting, the seed adjoint is $\\bar{y} = \\frac{\\partial L}{\\partial y} = \\frac{\\partial y}{\\partial y} = 1$. The resulting input adjoint $\\bar{x}$ is then $\\frac{dL}{dx} = \\frac{dy}{dx}$.\n\n**Reverse Mode Naive Rule:**\nThe update is expressed directly in terms of the input primal $x$. With $\\bar{y}=1$:\n$$\n\\bar{x} = -\\frac{1}{x^2}\n$$\nThis computational rule is identical to the naive forward mode rule.\n\n**Reverse Mode Stabilized Rule:**\nThis rule leverages the primal value $y = 1/x$, which is computed during a mandatory forward pass that precedes the reverse pass in any reverse mode AD system.\n$$\n\\bar{x} = \\bar{y} \\left(-\\frac{1}{x^2}\\right) = -\\bar{y} \\left(\\frac{1}{x}\\right)^2 = -\\bar{y} y^2\n$$\nWith $\\bar{y}=1$, the rule becomes:\n$$\n\\bar{x} = -y^2\n$$\nThis computational rule is identical to the stabilized forward mode rule. The distinction lies in the conceptual framework of AD, but the floating-point operations performed are the same.\n\n### 4. Verification and Implementation\n\nThe four derived rules are implemented as distinct functions. Their numerical accuracy is evaluated against the analytical derivative $g_{\\mathrm{exact}}(x) = -1/x^2$. The metric for comparison is the relative error for each input value $x$:\n$$\n\\mathrm{rel\\_err}(x) = \\frac{\\lvert g_{\\mathrm{AD}}(x) - g_{\\mathrm{exact}}(x)\\rvert}{\\lvert g_{\\mathrm{exact}}(x)\\rvert}\n$$\nThe test protocol requires aggregating the maximum relative error across the specified sets of inputs for each of the four rules. Any input $x$ that results in a non-finite value for either the AD-computed derivative or the exact derivative is excluded from the error calculation, as is any case where the exact derivative is zero (which does not occur for finite, non-zero $x$). The implementation will use `np.double` to enforce double-precision arithmetic and `np.isfinite` for validity checks.\n\nThe distinction between the \"naive\" and \"stabilized\" implementations lies in the order of floating-point operations.\n- Naive: `g = -1.0 / (x * x)`\n- Stabilized: `y = 1.0 / x`, `g = -y * y`\nAlthough algebraically equivalent, these two sequences of operations can produce different results due to rounding errors, and the magnitude of this difference is what the experiment is designed to measure, a foundational concept in numerical analysis.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and compares four AD rules for the reciprocal function y = 1/x.\n    The rules are: forward naive, forward stabilized, reverse naive, and reverse stabilized.\n    Their numerical accuracy is tested against the analytical derivative on three sets of inputs.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = {\n        'A': np.array([-123.45, -0.0314159, -1e-8, 1e-8, 0.0314159, 123.45], dtype=np.double),\n        'B': np.array([1.2345e-120, -4.321e-110, 7.89e-130, -9.99e-140, 5.0e-150, -8.0e-150], dtype=np.double),\n        'C': np.array([1.5e-154, -1.5e-154, 2.2e-154, -2.2e-154], dtype=np.double)\n    }\n\n    # 1. Analytical derivative (ground truth)\n    def g_exact(x: np.double) -> np.double:\n        \"\"\"Computes the exact derivative of 1/x, which is -1/x^2.\"\"\"\n        return np.double(-1.0) / (x * x)\n\n    # 3. Implementations of the four AD rules\n    def forward_naive(x: np.double) -> np.double:\n        \"\"\"Forward mode, naive rule: computes derivative from input x.\"\"\"\n        x_dot = np.double(1.0)\n        # Derivative is computed directly as -x_dot / (x*x)\n        y_dot = -x_dot / (x * x)\n        return y_dot\n\n    def forward_stabilized(x: np.double) -> np.double:\n        \"\"\"Forward mode, stabilized rule: reuses primal output y.\"\"\"\n        x_dot = np.double(1.0)\n        # Primal computation\n        y = np.double(1.0) / x\n        # Derivative computed using primal output y: -x_dot * y^2\n        y_dot = -x_dot * y * y\n        return y_dot\n        \n    def reverse_naive(x: np.double) -> np.double:\n        \"\"\"Reverse mode, naive rule: computes adjoint update from input x.\"\"\"\n        y_bar = np.double(1.0)\n        # Adjoint update computed directly as -y_bar / (x*x)\n        x_bar = -y_bar / (x * x)\n        return x_bar\n\n    def reverse_stabilized(x: np.double) -> np.double:\n        \"\"\"Reverse mode, stabilized rule: reuses primal output y.\"\"\"\n        y_bar = np.double(1.0)\n        # Primal computation (from forward pass)\n        y = np.double(1.0) / x\n        # Adjoint update computed using primal output y: -y_bar * y^2\n        x_bar = -y_bar * y * y\n        return x_bar\n\n    # 4. Verification method\n    def rel_err(g_ad: np.double, g_e: np.double) -> np.double:\n        \"\"\"Computes relative error.\"\"\"\n        return np.abs(g_ad - g_e) / np.abs(g_e)\n\n    all_results = []\n    \n    # Process cases in the specified order A, B, C\n    case_order = ['A', 'B', 'C']\n    ad_funcs = [forward_naive, forward_stabilized, reverse_naive, reverse_stabilized]\n\n    for case_name in case_order:\n        inputs = test_cases[case_name]\n        # max_errors for [FwdNaive, FwdStab, RevNaive, RevStab]\n        max_errors = [np.double(0.0), np.double(0.0), np.double(0.0), np.double(0.0)]\n\n        for x_val in inputs:\n            g_e = g_exact(x_val)\n            \n            # Compute derivatives from all 4 methods\n            ad_results = [f(x_val) for f in ad_funcs]\n\n            # Validation step: ignore non-finite results or zero denominator\n            if not np.isfinite(g_e) or g_e == 0.0:\n                continue\n            if not all(np.isfinite(g) for g in ad_results):\n                continue\n            \n            # Calculate and update max relative errors for the current case\n            for i in range(4):\n                error = rel_err(ad_results[i], g_e)\n                if error > max_errors[i]:\n                    max_errors[i] = error\n        \n        all_results.extend(max_errors)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}