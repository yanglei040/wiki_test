{
    "hands_on_practices": [
        {
            "introduction": "The primary motivation for the Fast Fourier Transform (FFT) is its incredible computational efficiency compared to the direct calculation of the Discrete Fourier Transform (DFT). While complexity notations like $O(N^2)$ and $O(N \\log N)$ provide a high-level understanding, working through the numbers for a specific case can be truly illuminating. This practice challenges you to quantify this advantage precisely, comparing the number of floating-point operations required by each method to transform a moderately sized dataset ().",
            "id": "3282537",
            "problem": "A software engineer must decide whether to implement the Discrete Fourier Transform (DFT) directly or via the radix-2 Cooley–Tukey Fast Fourier Transform (FFT) for a dataset of size $N=1024$. The engineer measures cost solely by the number of floating-point multiplications, defined as follows:\n- A complex multiplication is implemented naïvely as $4$ real multiplications.\n- Precomputation time for twiddle factors is ignored.\n- No algebraic simplifications of special twiddle values (such as $1$, $-1$, $\\mathrm{i}$, $-\\mathrm{i}$) are applied; each appearance of a twiddle factor is treated as a general complex multiplication and counted accordingly.\n\nUsing only the definition of the DFT and the radix-2 decimation principle underlying the Cooley–Tukey FFT, determine the exact difference in the total number of real floating-point multiplications between the direct DFT computation of all $N$ outputs and the radix-2 Cooley–Tukey FFT computation of all $N$ outputs, for $N=1024$. Provide your final answer as an integer; no rounding is required.",
            "solution": "The problem will be validated by first extracting the given information and then assessing its scientific and logical integrity.\n\n### Step 1: Extract Givens\n- The dataset size is $N=1024$.\n- The cost metric is the total number of real floating-point multiplications.\n- A single complex multiplication is defined to cost $4$ real multiplications.\n- The time to precompute twiddle factors is ignored.\n- No algebraic simplifications for special twiddle factor values (e.g., $1$, $-1$, $\\mathrm{i}$, $-\\mathrm{i}$) are considered. Every twiddle factor multiplication is counted as a general complex multiplication.\n- The goal is to find the difference in the total number of real multiplications between a direct Discrete Fourier Transform (DFT) computation and a radix-2 Cooley–Tukey Fast Fourier Transform (FFT) computation.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard exercise in computational complexity analysis, comparing the performance of the DFT and FFT algorithms. This is a fundamental topic in numerical methods and signal processing. The assumptions provided, such as the naïve cost of complex multiplication and the disregard for special-case optimizations, are simplifying but are clearly stated and scientifically consistent for a theoretical analysis.\n- **Well-Posed:** The problem is well-posed. The input size $N=1024$ is a power of $2$ ($N=2^{10}$), which is a requirement for the standard radix-2 Cooley–Tukey algorithm. The cost function is explicitly defined. A unique, integer-valued solution is expected and can be determined from the provided information.\n- **Objective:** The problem statement is objective and uses precise, unambiguous language.\n- **Completeness and Consistency:** The problem is self-contained and provides all necessary data and definitions. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will now be derived.\n\n### Solution Derivation\n\nThe objective is to compute the difference in the number of real floating-point multiplications between the direct DFT and the radix-2 FFT for a signal of length $N=1024$.\n\n**1. Cost of the Direct DFT**\n\nThe Discrete Fourier Transform (DFT) of a sequence $x_n$ of length $N$ is defined by the set of $N$ complex numbers $X_k$:\n$$\nX_k = \\sum_{n=0}^{N-1} x_n e^{-i \\frac{2\\pi kn}{N}} \\quad \\text{for } k = 0, 1, \\dots, N-1\n$$\nLet's analyze the computational cost for a single output coefficient, $X_k$. The calculation involves a sum of $N$ terms. Each term in the sum is of the form $x_n \\cdot e^{-i \\frac{2\\pi kn}{N}}$. This is a product of two complex numbers (the input sample $x_n$, which is complex in the general case, and the twiddle factor $e^{-i \\frac{2\\pi kn}{N}}$). According to the problem statement, we must count each such operation as a general complex multiplication.\n\nTherefore, for each of the $N$ output coefficients $X_k$, we must perform $N$ complex multiplications. The total number of complex multiplications for the direct DFT is thus:\n$$\n\\text{Cost}_{\\text{complex, DFT}} = N \\times N = N^2\n$$\nThe problem specifies that one complex multiplication costs $4$ real multiplications. Thus, the total number of real multiplications for the direct DFT is:\n$$\n\\text{Cost}_{\\text{real, DFT}} = 4 \\times N^2\n$$\n\n**2. Cost of the Radix-2 Cooley–Tukey FFT**\n\nThe radix-2 Cooley–Tukey algorithm is a recursive method for computing the DFT. It works by decomposing a DFT of size $N$ into two DFTs of size $N/2$. Let $C_{\\text{complex}}(N)$ be the number of complex multiplications required for an FFT of size $N$. The recurrence relation is:\n$$\nC_{\\text{complex}}(N) = 2 \\cdot C_{\\text{complex}}(N/2) + (\\text{multiplications in the combination step})\n$$\nThe combination step, or \"butterfly\" stage, computes the final outputs from the outputs of the two sub-problems. For $k = 0, 1, \\dots, N/2 - 1$, the computations are:\n$$\nX_k = E_k + e^{-i \\frac{2\\pi k}{N}} O_k\n$$\n$$\nX_{k+N/2} = E_k - e^{-i \\frac{2\\pi k}{N}} O_k\n$$\nwhere $E_k$ and $O_k$ are the DFTs of the even- and odd-indexed parts of the input signal, respectively.\n\nThe multiplication occurs in the term $e^{-i \\frac{2\\pi k}{N}} O_k$. This multiplication must be performed for each value of $k$ from $0$ to $N/2 - 1$. This amounts to $N/2$ complex multiplications. The problem explicitly states that no simplifications are to be made for special values, so even for $k=0$ (where the twiddle factor is $1$), we must count it as a full complex multiplication.\n\nThus, the recurrence relation becomes:\n$$\nC_{\\text{complex}}(N) = 2 C_{\\text{complex}}(N/2) + \\frac{N}{2}\n$$\nThe base case for the recursion is a DFT of size $N=1$, which is simply the identity ($X_0 = x_0$) and requires $0$ multiplications. So, $C_{\\text{complex}}(1) = 0$.\n\nSince $N$ is a power of $2$, let $N = 2^m$. There are $m = \\log_2(N)$ stages of recursion. At each stage, a total of $N/2$ complex multiplications are performed across all butterfly operations. Therefore, the total number of complex multiplications for the FFT is:\n$$\nC_{\\text{complex, FFT}}(N) = (\\log_2 N) \\times \\frac{N}{2} = \\frac{N}{2} \\log_2(N)\n$$\nThe total number of real multiplications is $4$ times this amount:\n$$\n\\text{Cost}_{\\text{real, FFT}} = 4 \\times \\left(\\frac{N}{2} \\log_2(N)\\right) = 2N \\log_2(N)\n$$\n\n**3. Calculation of the Difference**\n\nWe are asked for the difference in the number of real multiplications between the direct DFT and the FFT. Let this difference be $\\Delta M$.\n$$\n\\Delta M = \\text{Cost}_{\\text{real, DFT}} - \\text{Cost}_{\\text{real, FFT}}\n$$\n$$\n\\Delta M = 4N^2 - 2N \\log_2(N)\n$$\nSubstitute the given value $N = 1024$. We first note that $1024 = 2^{10}$, so $\\log_2(1024) = 10$.\n$$\n\\Delta M = 4(1024)^2 - 2(1024)(10)\n$$\n$$\n\\Delta M = 4(1048576) - 20(1024)\n$$\n$$\n\\Delta M = 4194304 - 20480\n$$\n$$\n\\Delta M = 4173824\n$$\nAlternatively, we can factor the expression before computing:\n$$\n\\Delta M = 2N(2N - \\log_2(N))\n$$\n$$\n\\Delta M = 2(1024)(2(1024) - 10)\n$$\n$$\n\\Delta M = 2048(2048 - 10)\n$$\n$$\n\\Delta M = 2048 \\times 2038\n$$\n$$\n\\Delta M = (2040+8)(2040-2) = 2040^2 - 4080 + 16320 - 16 = 4161600 + 12240 - 16 = 4173840 - 16 = 4173824\n$$\nThe difference in the total number of real floating-point multiplications is $4,173,824$.",
            "answer": "$$\n\\boxed{4173824}\n$$"
        },
        {
            "introduction": "One of the most powerful applications of the FFT is its ability to perform convolutions with remarkable speed, a cornerstone of digital filtering and signal analysis. However, the convolution theorem for the DFT comes with a critical caveat: it corresponds to *circular* convolution, not the more commonly desired *linear* convolution. This hands-on exercise () guides you through an investigation of the \"wrap-around\" artifacts that arise and demonstrates the essential technique of zero-padding to correctly compute linear convolution using the FFT.",
            "id": "3282547",
            "problem": "You are asked to investigate how the periodicity assumption inherent in the Fast Fourier Transform (FFT) algorithm impacts convolution and how zero-padding can mitigate wrap-around artifacts. Work entirely in discrete time. Your program must implement the following, starting from core definitions.\n\nLet a finite-length, nonperiodic discrete-time signal be defined by $x[n] = n + 1$ for $n \\in \\{0,1,\\dots,N-1\\}$ and $x[n] = 0$ otherwise, with $N = 10$. Let a finite-length, nonperiodic averaging kernel be defined by $h[n] = \\frac{1}{M}$ for $n \\in \\{0,1,\\dots,M-1\\}$ and $h[n] = 0$ otherwise, with $M = 4$. Define the linear convolution $y_{\\mathrm{lin}}[n]$ by\n$$\ny_{\\mathrm{lin}}[n] = \\sum_{k=-\\infty}^{\\infty} x[k]\\,h[n-k],\n$$\nwhere $x[n]$ and $h[n]$ are taken to be zero outside their specified supports. This sum is finite and $y_{\\mathrm{lin}}[n]$ has length $N+M-1 = 13$.\n\nDefine the length-$L$ Discrete Fourier Transform (DFT) of a sequence $a[n]$ supported on $\\{0,1,\\dots,L-1\\}$ by\n$$\nA_L[m] = \\sum_{n=0}^{L-1} a[n]\\,e^{-2\\pi i \\frac{mn}{L}},\\quad m=0,1,\\dots,L-1,\n$$\nand its inverse by\n$$\na[n] = \\frac{1}{L}\\sum_{m=0}^{L-1} A_L[m]\\,e^{2\\pi i \\frac{mn}{L}}.\n$$\nFor a given length $L$, form zero-padded versions $x_L[n]$ and $h_L[n]$ by\n$$\nx_L[n] = \\begin{cases}\nx[n], 0\\le n\\le N-1\\\\\n0, \\text{otherwise}\n\\end{cases},\\quad\nh_L[n] = \\begin{cases}\nh[n], 0\\le n\\le M-1\\\\\n0, \\text{otherwise}\n\\end{cases},\n$$\nfor $n\\in\\{0,1,\\dots,L-1\\}$. Compute the length-$L$ circular convolution\n$$\ny_{\\mathrm{circ},L}[n] = \\sum_{k=0}^{L-1} x_L[k]\\,h_L[(n-k)\\bmod L],\\quad n=0,1,\\dots,L-1,\n$$\nvia pointwise multiplication in the DFT domain using the Fast Fourier Transform (FFT). It is a well-tested fact that the product of DFTs corresponds to circular convolution in the time domain.\n\nTo quantify wrap-around artifacts, define for each $L$ the error\n$$\nE(L) = \\max_{0\\le n \\le L-1} \\left| y_{\\mathrm{circ},L}[n] - y_{\\mathrm{lin},L}[n] \\right|,\n$$\nwhere $y_{\\mathrm{lin},L}[n]$ is the linear convolution $y_{\\mathrm{lin}}[n]$ either truncated to its first $L$ samples if $L \\le N+M-1$, or zero-padded with $L-(N+M-1)$ trailing zeros if $L \\ge N+M-1$. This common comparison length ensures a pointwise error over $L$ samples. The presence of wrap-around artifacts is indicated by $E(L)$ being significantly larger than numerical roundoff.\n\nYour tasks are:\n- Implement the above definitions and compute $E(L)$ for a test suite of padding lengths $L \\in \\{10,12,13,16,64\\}$.\n- Interpret the results to assess three padding strategies $L$: no padding $L=N$, minimal padding $L=N+M-1$, and a power-of-two padding $L=\\min\\{2^p: 2^p\\ge N+M-1\\}$, with an additional oversized padding $L=64$ to probe numerical stability.\n\nYour program should produce a single line of output containing the results as a comma-separated list of floating-point errors in the order of the test suite, enclosed in square brackets, for example, $[e_{10},e_{12},e_{13},e_{16},e_{64}]$ where each $e_L$ is the computed $E(L)$ for that $L$. No other output is permitted.\n\nEnsure that your implementation is self-contained, uses no user input, and adheres to the definitions above. Angles in complex exponentials are in radians by definition. There are no physical units involved. The target audience is advanced undergraduate students in numerical methods and scientific computing.",
            "solution": "The problem requires an investigation into the use of the Fast Fourier Transform (FFT) to compute the linear convolution of two discrete-time signals. The central principle being examined is the convolution theorem for the Discrete Fourier Transform (DFT), which states that the DFT of a circular convolution of two sequences is the pointwise product of their individual DFTs. Linear convolution can be correctly computed via this method only if the signals are zero-padded to a sufficient length to prevent time-domain aliasing, also known as wrap-around error. This exercise aims to demonstrate and quantify this effect.\n\nThe procedure is structured as follows:\n\nFirst, we define the input signals. The primary signal is a finite-length ramp sequence $x[n] = n + 1$ for $n \\in \\{0, 1, \\dots, N-1\\}$, with $N=10$, and $x[n]=0$ otherwise. The second signal is a finite-length averaging kernel $h[n] = \\frac{1}{M}$ for $n \\in \\{0, 1, \\dots, M-1\\}$, with $M=4$, and $h[n]=0$ otherwise.\n\nSecond, we compute the true linear convolution, denoted $y_{\\mathrm{lin}}[n]$, which serves as our ground truth. It is defined by the convolution sum:\n$$\ny_{\\mathrm{lin}}[n] = \\sum_{k=-\\infty}^{\\infty} x[k]\\,h[n-k]\n$$\nFor a signal of length $N$ and a kernel of length $M$, the resulting sequence $y_{\\mathrm{lin}}[n]$ has a length of $N+M-1$. In this specific case, the length is $10+4-1=13$. This computation is performed directly in the time domain.\n\nThird, for each specified transform length $L$ from the set $\\{10, 12, 13, 16, 64\\}$, we compute the length-$L$ circular convolution, $y_{\\mathrm{circ},L}[n]$. This is accomplished by leveraging the convolution theorem. The steps are:\n1.  Create length-$L$ versions of the input signals, $x_L[n]$ and $h_L[n]$, by zero-padding the original signals $x[n]$ and $h[n]$ to length $L$.\n2.  Compute the length-$L$ DFTs of the padded signals, $X_L[m] = \\text{DFT}\\{x_L[n]\\}$ and $H_L[m] = \\text{DFT}\\{h_L[n]\\}$, using the FFT algorithm for efficiency. The DFT is defined as $A_L[m] = \\sum_{n=0}^{L-1} a[n]\\,e^{-2\\pi i \\frac{mn}{L}}$.\n3.  Perform pointwise multiplication in the frequency domain: $Y_{\\mathrm{circ},L}[m] = X_L[m] \\cdot H_L[m]$.\n4.  Compute the inverse DFT of the product, $y_{\\mathrm{circ},L}[n] = \\text{IDFT}\\{Y_{\\mathrm{circ},L}[m]\\}$, to obtain the circular convolution result in the time domain. The inverse DFT is defined as $a[n] = \\frac{1}{L}\\sum_{m=0}^{L-1} A_L[m]\\,e^{2\\pi i \\frac{mn}{L}}$.\n\nFourth, we quantify the discrepancy between the circular and linear convolutions. A comparison signal, $y_{\\mathrm{lin},L}[n]$, is created from the ground-truth $y_{\\mathrm{lin}}[n]$ by either truncating it or padding it with zeros to match the length $L$. The error for a given length $L$ is then defined as the maximum absolute difference between the two results over all sample points:\n$$\nE(L) = \\max_{0\\le n \\le L-1} \\left| y_{\\mathrm{circ},L}[n] - y_{\\mathrm{lin},L}[n] \\right|\n$$\nA non-negligible value of $E(L)$ indicates the presence of wrap-around artifacts.\n\nThe theoretical basis for this analysis is that the circular convolution computed with length $L$ is equivalent to the linear convolution if and only if $L \\ge N+M-1$.\n- If $L  N+M-1$, the tail of the linear convolution result (which extends to index $N+M-2$) \"wraps around\" and adds to the initial samples of the circular convolution result, causing aliasing. Thus, for $L=10$ and $L=12$, we expect $E(L)$ to be significantly greater than zero.\n- If $L \\ge N+M-1$, there is sufficient padding to contain the entire linear convolution result without wrap-around. In this case, $y_{\\mathrm{circ},L}[n]$ will be identical to $y_{\\mathrm{lin},L}[n]$. Therefore, for $L=13$, $L=16$, and $L=64$, we expect the error $E(L)$ to be on the order of machine floating-point precision.\n\nThe test cases are chosen to illustrate key padding strategies: $L=10$ (insufficient padding), $L=13$ (minimal sufficient padding), and $L=16$ (power-of-two padding, often chosen for FFT efficiency). The $L=64$ case further confirms stability with oversized padding. The implementation calculates $E(L)$ for each of these cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes convolution error E(L) to demonstrate the effect of zero-padding.\n    \"\"\"\n    # Define signal and kernel parameters as per the problem statement.\n    N = 10\n    M = 4\n\n    # Define the discrete-time signal x[n] = n + 1.\n    x = np.arange(1, N + 1, dtype=float)\n\n    # Define the averaging kernel h[n] = 1/M.\n    h = np.ones(M, dtype=float) / M\n\n    # Compute the ground-truth linear convolution y_lin[n].\n    # The length of the result is N + M - 1 = 13.\n    L_lin = N + M - 1\n    y_lin = np.convolve(x, h)\n\n    # Define the test suite of padding lengths L.\n    test_Ls = [10, 12, 13, 16, 64]\n    \n    results = []\n\n    # Iterate through each padding length L to compute the error E(L).\n    for L in test_Ls:\n        # Step 1: Create the comparison linear convolution y_lin_L[n] of length L.\n        # This is done by truncating or padding y_lin to length L.\n        y_lin_L = np.zeros(L)\n        len_to_copy = min(L, L_lin)\n        y_lin_L[:len_to_copy] = y_lin[:len_to_copy]\n\n        # Step 2: Create zero-padded versions of x and h to length L.\n        x_L = np.zeros(L)\n        x_L[:N] = x\n        \n        h_L = np.zeros(L)\n        h_L[:M] = h\n\n        # Step 3: Compute circular convolution via FFT.\n        # This uses the convolution theorem: IDFT{DFT{x} * DFT{h}}.\n        X_L = np.fft.fft(x_L)\n        H_L = np.fft.fft(h_L)\n        Y_circ_L = X_L * H_L\n        y_circ_L = np.fft.ifft(Y_circ_L)\n\n        # Step 4: Calculate the error E(L) as the maximum absolute difference.\n        # np.abs handles the case where y_circ_L has a tiny imaginary part\n        # due to numerical precision.\n        error = np.max(np.abs(y_circ_L - y_lin_L))\n        results.append(error)\n\n    # Print the results in the specified single-line format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond its computational elegance, the FFT is a workhorse of practical signal processing, enabling us to view and manipulate signals in the frequency domain. A classic application is denoising, where unwanted noise can be identified and removed based on its frequency characteristics. In this problem (), you will implement a complete denoising pipeline: transforming a noisy signal, applying a targeted filter in the frequency domain, and transforming it back to see the result, providing direct experience with the power of spectral filtering.",
            "id": "3282556",
            "problem": "You are given a discrete-time real-valued signal of finite length and are asked to implement a denoising procedure based on the Discrete Fourier Transform (DFT). Your program must construct synthetic signals, add high-frequency disturbances, transform to the frequency domain via the DFT, set small-magnitude, high-frequency coefficients to zero, invert the transform, and quantify the denoising effect.\n\nBase your reasoning on the following fundamental definitions and facts.\n\n1. Discrete Fourier Transform (DFT) and inverse Discrete Fourier Transform (IDFT). For a sequence $\\{x_n\\}_{n=0}^{N-1}$ with $N \\in \\mathbb{N}$,\n$$\nX_k = \\sum_{n=0}^{N-1} x_n \\, e^{-2\\pi i \\, n k / N}, \\quad k = 0,1,\\dots,N-1,\n$$\nand the inverse transform reconstructs\n$$\nx_n = \\frac{1}{N} \\sum_{k=0}^{N-1} X_k \\, e^{2\\pi i \\, n k / N}, \\quad n = 0,1,\\dots,N-1.\n$$\n2. For a real-valued time-domain sequence, the DFT exhibits Hermitian symmetry. Energy is preserved between domains up to normalization (a consequence of Parseval’s relation), which justifies measuring distortion by mean-squared error in the time domain.\n\nDenoising operator to implement. Let $X_k$ denote the DFT of the noisy signal of length $N$. Define the symmetric normalized frequency radius\n$$\n\\rho(k) = \\frac{\\min\\{k,\\,N-k\\}}{N/2}, \\quad k = 0,1,\\dots,N-1.\n$$\nGiven parameters $\\gamma \\in [0,1]$ and $\\tau \\in [0,1]$, and $M = \\max_{0 \\le k \\le N-1} |X_k|$, set to zero every frequency coefficient satisfying both\n$$\n\\rho(k) \\ge \\gamma \\quad \\text{and} \\quad |X_k| \\le \\tau \\, M.\n$$\nThen compute the inverse DFT of the modified spectrum to obtain the denoised sequence. Angles in all trigonometric functions must be in radians.\n\nSignal model for testing. For each test case, the clean signal is a finite sum of sinusoids aligned with DFT bin indices, so that for amplitudes $A_j  0$ and integer bin indices $b_j \\in \\{0,1,\\dots,N-1\\}$,\n$$\nx^{(\\mathrm{clean})}_n = \\sum_{j} A_j \\, \\sin\\!\\left(2\\pi \\, \\frac{b_j}{N} \\, n\\right), \\quad n = 0,1,\\dots,N-1.\n$$\nNoise is a sum of high-frequency sinusoids defined the same way (with specified amplitudes and bin indices). The noisy signal is $x^{(\\mathrm{noisy})}_n = x^{(\\mathrm{clean})}_n + x^{(\\mathrm{noise})}_n$.\n\nQuality metric. Let\n$$\n\\mathrm{MSE}_{\\mathrm{before}} = \\frac{1}{N}\\sum_{n=0}^{N-1} \\left(x^{(\\mathrm{noisy})}_n - x^{(\\mathrm{clean})}_n\\right)^2,\n$$\n$$\n\\mathrm{MSE}_{\\mathrm{after}} = \\frac{1}{N}\\sum_{n=0}^{N-1} \\left(\\hat{x}_n - x^{(\\mathrm{clean})}_n\\right)^2,\n$$\nwhere $\\hat{x}_n$ is the denoised sequence obtained from the inverse DFT. To avoid division by zero in degenerate cases, define a small regularization $\\varepsilon = 10^{-12}$ and report the improvement factor\n$$\nI = \\frac{\\mathrm{MSE}_{\\mathrm{before}} + \\varepsilon}{\\mathrm{MSE}_{\\mathrm{after}} + \\varepsilon}.\n$$\n\nYour task. Implement a program that, for each test case below, constructs the clean and noisy signals, applies the denoising operator specified above using the provided $(\\gamma,\\tau)$, computes $I$, and reports the results rounded to six decimal places.\n\nTest suite. Use the following four cases. In each case the clean signal parameters are identical, and the noise, threshold, and high-frequency parameters vary. All amplitudes and parameters are dimensionless.\n\n- Clean signal for all cases: $N = 256$, amplitudes and bins\n  - $(A_1, b_1) = (1.0, 10)$,\n  - $(A_2, b_2) = (0.8, 30)$,\n  - $(A_3, b_3) = (0.6, 50)$.\n\n- Case $1$ (happy path): noise amplitudes and bins\n  - $(0.3, 100)$, $(0.25, 110)$, $(0.1, 90)$,\n  with $\\gamma = 0.7$, $\\tau = 0.5$.\n\n- Case $2$ (no-noise boundary): no noise components, with $\\gamma = 0.7$, $\\tau = 0.5$.\n\n- Case $3$ (strong high-frequency noise not flagged as “small”): noise amplitudes and bins\n  - $(1.2, 120)$, $(0.4, 100)$,\n  with $\\gamma = 0.7$, $\\tau = 0.6$.\n\n- Case $4$ (high-frequency threshold too close to Nyquist): noise amplitudes and bins\n  - $(0.3, 100)$, $(0.25, 110)$,\n  with $\\gamma = 0.99$, $\\tau = 0.5$.\n\nFinal output format. Your program should produce a single line of output containing the four improvement factors, rounded to six decimal places, aggregated as a comma-separated list enclosed in square brackets, with no spaces. For example: \"[v1,v2,v3,v4]\".",
            "solution": "The problem statement is formally evaluated and found to be valid. It is scientifically grounded in the principles of digital signal processing, well-posed with a complete and consistent set of parameters, and objective in its formulation. The task is to implement a specific frequency-domain denoising algorithm and quantify its performance on a series of well-defined test cases.\n\nThe solution proceeds by implementing the specified procedure for each test case. The core principle is that a signal composed of a few dominant sinusoids will have a sparse representation in the frequency domain, while noise is often more broadly distributed or located at high frequencies. The algorithm attempts to isolate and remove these noise components by transforming the signal into the frequency domain, setting specific coefficients to zero, and transforming back to the time domain.\n\nThe step-by-step procedure is as follows:\n\n1.  **Signal Synthesis**: For each test case, we construct three discrete-time signals of length $N=256$, sampled at indices $n = 0, 1, \\dots, N-1$.\n    -   The clean signal, $x^{(\\mathrm{clean})}_n$, is generated as a sum of sinusoids according to the formula $x^{(\\mathrm{clean})}_n = \\sum_{j} A_j \\, \\sin(2\\pi \\frac{b_j}{N} n)$, using the provided amplitudes $A_j$ and bin indices $b_j$.\n    -   The noise signal, $x^{(\\mathrm{noise})}_n$, is constructed similarly using its specified amplitude and bin parameters for the given case.\n    -   The signal to be processed, $x^{(\\mathrm{noisy})}_n$, is the sum of the clean and noise signals: $x^{(\\mathrm{noisy})}_n = x^{(\\mathrm{clean})}_n + x^{(\\mathrm{noise})}_n$.\n\n2.  **Frequency Transformation**: The noisy signal is transformed into the frequency domain using the Discrete Fourier Transform (DFT), yielding a complex-valued spectrum $X_k$:\n    $$\n    X_k = \\sum_{n=0}^{N-1} x^{(\\mathrm{noisy})}_n \\, e^{-2\\pi i \\, n k / N}, \\quad k = 0,1,\\dots,N-1.\n    $$\n    This is computationally implemented using the Fast Fourier Transform (FFT) algorithm.\n\n3.  **Frequency-Domain Denoising**: The denoising is performed directly on the complex spectrum $X_k$. A filtered spectrum, $\\hat{X}_k$, is created by applying a thresholding rule. For each frequency coefficient $X_k$, we evaluate two conditions:\n    -   **High-Frequency Condition**: The normalized frequency radius, $\\rho(k) = \\frac{\\min\\{k, N-k\\}}{N/2}$, must be greater than or equal to a given threshold $\\gamma$. This condition isolates frequencies that are far from the DC component ($k=0$).\n    -   **Small-Magnitude Condition**: The magnitude of the coefficient, $|X_k|$, must be less than or equal to a threshold proportional to the maximum magnitude in the entire spectrum, $M = \\max_{j} |X_j|$. The condition is $|X_k| \\le \\tau M$. This is intended to preserve strong signal components, even if they lie at high frequencies, while attenuating weak (presumed noise) components.\n\n    A coefficient $X_k$ is set to zero in the filtered spectrum $\\hat{X}_k$ if and only if both conditions are met. Otherwise, $\\hat{X}_k = X_k$.\n\n4.  **Signal Reconstruction**: The denoised time-domain signal, $\\hat{x}_n$, is obtained by applying the inverse DFT (IDFT) to the filtered spectrum $\\hat{X}_k$:\n    $$\n    \\hat{x}_n = \\frac{1}{N} \\sum_{k=0}^{N-1} \\hat{X}_k \\, e^{2\\pi i \\, n k / N}, \\quad n = 0,1,\\dots,N-1.\n    $$\n    Since the original signal $x^{(\\mathrm{noisy})}_n$ is real-valued, and the filtering rule preserves Hermitian symmetry (if $X_k$ is zeroed, so is its conjugate-symmetric counterpart $X_{N-k}$), the resulting $\\hat{x}_n$ must also be real. We take the real part of the IDFT's output to discard any minor imaginary components arising from numerical floating-point inaccuracies.\n\n5.  **Performance Evaluation**: The effectiveness of the denoising process is quantified by comparing the mean-squared error (MSE) before and after filtering.\n    -   The initial error is $\\mathrm{MSE}_{\\mathrm{before}} = \\frac{1}{N}\\sum_{n=0}^{N-1} \\left(x^{(\\mathrm{noisy})}_n - x^{(\\mathrm{clean})}_n\\right)^2 = \\frac{1}{N}\\sum_{n=0}^{N-1} \\left(x^{(\\mathrm{noise})}_n\\right)^2$.\n    -   The error after denoising is $\\mathrm{MSE}_{\\mathrm{after}} = \\frac{1}{N}\\sum_{n=0}^{N-1} \\left(\\hat{x}_n - x^{(\\mathrm{clean})}_n\\right)^2$.\n    -   The final metric is the improvement factor, $I$, defined as the ratio of these errors, regularized by a small constant $\\varepsilon = 10^{-12}$ to ensure numerical stability:\n    $$\n    I = \\frac{\\mathrm{MSE}_{\\mathrm{before}} + \\varepsilon}{\\mathrm{MSE}_{\\mathrm{after}} + \\varepsilon}.\n    $$\n    An improvement factor $I  1$ indicates successful denoising, $I \\approx 1$ indicates no significant change, and $I  1$ would indicate that the process added more error than it removed.\n\nThis complete procedure is applied to each of the four test cases provided, and the resulting improvement factors are calculated and reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs signals, applies a DFT-based denoising filter, and computes\n    the performance improvement for a suite of test cases.\n    \"\"\"\n    # Define constants and test parameters from the problem statement.\n    N = 256\n    epsilon = 1e-12\n    \n    clean_components = [\n        (1.0, 10),\n        (0.8, 30),\n        (0.6, 50)\n    ]\n\n    test_suite = [\n        {\n            # Case 1: Standard denoising scenario\n            \"noise_components\": [(0.3, 100), (0.25, 110), (0.1, 90)],\n            \"gamma\": 0.7,\n            \"tau\": 0.5\n        },\n        {\n            # Case 2: No noise, to test filter's effect on clean signal\n            \"noise_components\": [],\n            \"gamma\": 0.7,\n            \"tau\": 0.5\n        },\n        {\n            # Case 3: Strong high-frequency noise that exceeds magnitude threshold\n            \"noise_components\": [(1.2, 120), (0.4, 100)],\n            \"gamma\": 0.7,\n            \"tau\": 0.6\n        },\n        {\n            # Case 4: High-frequency threshold is too restrictive (close to Nyquist)\n            \"noise_components\": [(0.3, 100), (0.25, 110)],\n            \"gamma\": 0.99,\n            \"tau\": 0.5\n        }\n    ]\n\n    results = []\n\n    # Pre-calculate arrays that are common across all test cases for efficiency\n    n = np.arange(N)\n    k_indices = np.arange(N)\n    rho = np.minimum(k_indices, N - k_indices) / (N / 2.0)\n\n    # Generate the clean signal once, as it's the same for all cases\n    x_clean = np.zeros(N, dtype=float)\n    for A, b in clean_components:\n        x_clean += A * np.sin(2 * np.pi * b * n / N)\n\n    for case in test_suite:\n        # Unpack parameters for the current test case\n        noise_components = case['noise_components']\n        gamma = case['gamma']\n        tau = case['tau']\n\n        # Step 1: Synthesize the noise and noisy signals\n        x_noise = np.zeros(N, dtype=float)\n        if noise_components:\n            for A, b in noise_components:\n                x_noise += A * np.sin(2 * np.pi * b * n / N)\n        x_noisy = x_clean + x_noise\n\n        # Calculate the initial Mean Squared Error\n        mse_before = np.mean(x_noise**2)\n\n        # Step 2: Transform to the frequency domain\n        X_noisy = np.fft.fft(x_noisy)\n        M = np.max(np.abs(X_noisy))\n\n        # Step 3: Apply frequency-domain denoising\n        # Identify coefficients that meet both filtering conditions\n        high_freq_mask = rho = gamma\n        small_mag_mask = np.abs(X_noisy) = tau * M\n        zero_mask = high_freq_mask  small_mag_mask\n\n        # Create the filtered spectrum by setting identified coefficients to zero\n        X_filtered = X_noisy.copy()\n        X_filtered[zero_mask] = 0.0\n\n        # Step 4: Reconstruct the signal via Inverse DFT\n        # Use np.real to discard negligible imaginary parts from numerical error\n        x_denoised = np.real(np.fft.ifft(X_filtered))\n\n        # Step 5: Evaluate performance\n        # Calculate the Mean Squared Error after denoising\n        mse_after = np.mean((x_denoised - x_clean)**2)\n\n        # Compute the regularized improvement factor\n        improvement = (mse_before + epsilon) / (mse_after + epsilon)\n        results.append(f\"{improvement:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}