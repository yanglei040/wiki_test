{
    "hands_on_practices": [
        {
            "introduction": "Metropolis-Hastings 算法的核心在于决定是接受还是拒绝一个新提议的状态。这个决策由一个精心设计的接受概率 $ \\alpha $ 来控制，它确保了马尔可夫链最终会收敛到我们想要的目标分布。本练习将这个关键步骤独立出来，让你专注于计算目标密度的比率，从而深入理解 MCMC 算法的内在机制。",
            "id": "1371728",
            "problem": "一位数据科学家正在实现一个马尔可夫链蒙特卡洛 (MCMC) 模拟，以从参数 $x$ 的后验概率分布中抽取样本。目标分布 $\\pi(x)$ 与该参数负绝对值的指数成正比，即 $\\pi(x) \\propto \\exp(-|x|)$。\n\n该科学家使用 Metropolis 算法，其提议分布 $q(x'|x)$ 是对称的，即在给定当前状态 $x$ 的情况下提议新状态 $x'$ 的概率等于在给定 $x'$ 的情况下提议 $x$ 的概率（即，$q(x'|x) = q(x|x')$）。\n\n假设在模拟的某一步中，马尔可夫链的当前状态为 $x = 1.5$。算法接着提议转移到一个新的候选状态 $x' = 2.0$。\n\n计算这次特定转移的接受概率。您的答案应该是一个无量纲的实数。将您的最终答案四舍五入到四位有效数字。",
            "solution": "对于一个从 $x$ 到 $x'$ 的转移，当提议分布 $q(x'|x)=q(x|x')$ 是对称的时，Metropolis 接受概率为\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\n给定目标分布 $\\pi(x)\\propto \\exp(-|x|)$，该比率简化为\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\n当 $x=1.5$ 且 $x'=2.0$ 时，我们有 $|x|=1.5$ 和 $|x'|=2.0$，因此\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\n因此，\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\n数值上，$\\exp(-0.5)\\approx 0.6065$（四舍五入到四位有效数字）。",
            "answer": "$$\\boxed{0.6065}$$"
        },
        {
            "introduction": "理解了单个步骤的接受概率计算后，我们就可以构建一个完整的 MCMC 采样器了。本练习将引导你应用 Metropolis-Hastings 算法解决一个常见且重要的问题：从贝叶斯视角估计线性回归模型的参数。通过从头开始编写代码，你将体验到如何利用 MCMC 从数据中学习并量化参数的不确定性。",
            "id": "3250349",
            "problem": "给定一个由独立同分布误差假设指定的线性观测模型：对于数据集中的每个索引 $i$，标量响应 $y_i$ 由以下公式生成\n$$\ny_i \\;=\\; m\\,x_i \\;+\\; b \\;+\\; \\epsilon_i,\n$$\n其中误差项 $\\epsilon_i$ 服从均值为零、方差为 $\\sigma^2$ 的高斯分布，即 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$，且各误差项 $\\epsilon_i$ 之间相互独立。参数 $m$ 和 $b$ 是未知的。假设独立的先验高斯分布 $m \\sim \\mathcal{N}(0,\\tau_m^2)$ 和 $b \\sim \\mathcal{N}(0,\\tau_b^2)$，其先验标准差 $\\tau_m$ 和 $\\tau_b$ 已知。方差 $\\sigma^2$ 已知。\n\n从贝叶斯定理以及高斯似然和高斯先验密度的定义出发，推导在给定观测数据 $(x_i,y_i)$ 和已知超参数 $(\\sigma,\\tau_m,\\tau_b)$ 的条件下，$(m,b)$ 的未归一化后验密度对数的可实现表达式。然后，为这两个参数实现一个使用对称高斯随机游走提议的 Metropolis–Hastings 马尔可夫链蒙特卡洛 (MCMC) 算法：\n- 提议 $m^\\star = m + \\eta_m$，其中 $\\eta_m \\sim \\mathcal{N}(0,s_m^2)$。\n- 提议 $b^\\star = b + \\eta_b$，其中 $\\eta_b \\sim \\mathcal{N}(0,s_b^2)$。\n- 根据未归一化后验密度的比率，遵循 Metropolis–Hastings 规则接受或拒绝 $(m^\\star,b^\\star)$。\n\n使用这些基础，为以下测试套件近似 $(m,b)$ 的后验分布。在每种情况下，必须在您的程序内部使用所提供的数据生成种子以及指定的真实参数和噪声水平来人工生成观测响应 $y_i$。然后，为马尔可夫链使用一个单独的种子，运行 MCMC 采样器，并在丢弃预热期样本后报告 $m$ 和 $b$ 的后验均值。不需要进行稀疏化处理。\n\n假设所有情况的初始状态 $(m_0,b_0)$ 均为 $(0,0)$。所有情况均使用下面指定的相同的先验标准差 $\\tau_m=\\tau_b$。\n\n测试套件：\n- 情况 $1$ (理想情况)：\n  - 设计点：$x$ 是从 $-2.0$ 到 $2.0$ (含) 的 $n=20$ 个等间距值。\n  - 真实值：$m_{\\text{true}}=2.0$, $b_{\\text{true}}=-1.0$。\n  - 噪声标准差：$\\sigma=0.5$。\n  - 先验标准差：$\\tau_m=\\tau_b=10.0$。\n  - 提议标准差：$s_m=0.02$, $s_b=0.05$。\n  - 数据生成种子：$11$。\n  - MCMC 种子：$101$。\n  - 总迭代次数：$20000$。\n  - 预热期：$5000$。\n- 情况 $2$ (包含两个点且噪声较低的边界情况数据集)：\n  - 设计点：$x=[-1.0,\\,1.0]$，其中 $n=2$。\n  - 真实值：$m_{\\text{true}}=-0.5$, $b_{\\text{true}}=2.0$。\n  - 噪声标准差：$\\sigma=0.1$。\n  - 先验标准差：$\\tau_m=\\tau_b=10.0$。\n  - 提议标准差：$s_m=0.005$, $s_b=0.01$。\n  - 数据生成种子：$22$。\n  - MCMC 种子：$202$。\n  - 总迭代次数：$30000$。\n  - 预热期：$10000$。\n- 情况 $3$ (高噪声观测)：\n  - 设计点：$x$ 是从 $0.0$ 到 $5.0$ (含) 的 $n=30$ 个等间距值。\n  - 真实值：$m_{\\text{true}}=0.3$, $b_{\\text{true}}=4.0$。\n  - 噪声标准差：$\\sigma=5.0$。\n  - 先验标准差：$\\tau_m=\\tau_b=10.0$。\n  - 提议标准差：$s_m=0.05$, $s_b=0.20$。\n  - 数据生成种子：$33$。\n  - MCMC 种子：$303$。\n  - 总迭代次数：$25000$。\n  - 预热期：$8000$。\n\n实现和输出要求：\n- 严格按照描述实现 Metropolis–Hastings 采样器，使用您推导出的未归一化后验密度的对数。\n- 对于每种情况，仅使用预热期后的样本计算 $m$ 的后验均值和 $b$ 的后验均值。\n- 您的程序必须使用给定的数据生成种子以及指定的真实参数和噪声水平来生成人工的 $y$ 值，并且必须使用提供的 MCMC 种子进行采样。\n- 将每个后验均值四舍五入到恰好 $4$ 位小数。\n- 您的程序应生成单行输出，其中包含结果，格式为方括号内以逗号分隔的列表，顺序为 $[m^{(1)}_{\\text{mean}},b^{(1)}_{\\text{mean}},m^{(2)}_{\\text{mean}},b^{(2)}_{\\text{mean}},m^{(3)}_{\\text{mean}},b^{(3)}_{\\text{mean}}]$，其中上标表示情况编号。例如，输出格式必须是包含方括号和逗号分隔的十进制数的单行。\n\n注意：\n- 所有角度（如果存在）都将以弧度为单位；此处未使用角度。\n- 输出不需要物理单位。\n- 通过为伪随机数生成使用指定的种子，确保所有计算都是确定性的。",
            "solution": "该问题要求实现一个 Metropolis-Hastings 马尔可夫链蒙特卡洛 (MCMC) 算法，用于从简单线性回归模型的参数 $(m,b)$ 的后验分布中进行采样。解决方案需要两个主要部分：目标概率密度函数的推导和 MCMC 采样器的实现。\n\n### 未归一化对数后验密度的推导\n\n该问题设置在贝叶斯框架内。根据贝叶斯定理，在给定观测数据 $D = \\{(x_i, y_i)\\}_{i=1}^n$ 和已知超参数 $H = (\\sigma, \\tau_m, \\tau_b)$ 的条件下，参数 $(m,b)$ 的后验概率由下式给出：\n$$\np(m, b \\mid D, H) = \\frac{p(D \\mid m, b, \\sigma) p(m \\mid \\tau_m) p(b \\mid \\tau_b)}{p(D \\mid H)}\n$$\n其中 $p(D \\mid m, b, \\sigma)$ 是数据的似然， $p(m \\mid \\tau_m)$ 和 $p(b \\mid \\tau_b)$ 是参数的先验概率，而 $p(D \\mid H)$ 是边际似然或证据，它作为一个归一化常数。对于像 Metropolis-Hastings 这样的 MCMC 方法，归一化常数不是必需的。我们可以使用与分子成正比的未归一化后验：\n$$\np(m, b \\mid D, H) \\propto p(D \\mid m, b, \\sigma) p(m \\mid \\tau_m) p(b \\mid \\tau_b)\n$$\n\n**1. 似然函数**\n线性观测模型为 $y_i = m x_i + b + \\epsilon_i$，误差项 $\\epsilon_i$ 从均值为零、方差为 $\\sigma^2$ 的正态分布中抽取，即 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。这意味着每个响应变量 $y_i$ 也服从正态分布：\n$$\ny_i \\mid m, b, x_i, \\sigma \\sim \\mathcal{N}(m x_i + b, \\sigma^2)\n$$\n单个观测值 $(x_i, y_i)$ 的概率密度函数 (PDF) 为：\n$$\np(y_i \\mid m, b, x_i, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - (m x_i + b))^2}{2\\sigma^2}\\right)\n$$\n假设误差 $\\epsilon_i$ 是独立同分布 (i.i.d.) 的，整个数据集 $D$ 的似然是各个概率的乘积：\n$$\np(D \\mid m, b, \\sigma) = \\prod_{i=1}^n p(y_i \\mid m, b, x_i, \\sigma)\n$$\n\n**2. 先验分布**\n问题指定了参数 $m$ 和 $b$ 的独立高斯先验：\n$$\nm \\sim \\mathcal{N}(0, \\tau_m^2) \\implies p(m \\mid \\tau_m) = \\frac{1}{\\sqrt{2\\pi\\tau_m^2}} \\exp\\left(-\\frac{m^2}{2\\tau_m^2}\\right)\n$$\n$$\nb \\sim \\mathcal{N}(0, \\tau_b^2) \\implies p(b \\mid \\tau_b) = \\frac{1}{\\sqrt{2\\pi\\tau_b^2}} \\exp\\left(-\\frac{b^2}{2\\tau_b^2}\\right)\n$$\n由于其独立性，联合先验是各个先验的乘积：$p(m, b \\mid \\tau_m, \\tau_b) = p(m \\mid \\tau_m) p(b \\mid \\tau_b)$。\n\n**3. 未归一化的对数后验**\n为了数值稳定性和计算便利性，标准做法是使用后验密度的对数。我们记为 $\\mathcal{L}_{un}(m,b)$ 的未归一化后验的对数为：\n$$\n\\mathcal{L}_{un}(m,b) = \\log\\left[ p(D \\mid m, b, \\sigma) p(m \\mid \\tau_m) p(b \\mid \\tau_b) \\right] = \\log p(D \\mid m, b, \\sigma) + \\log p(m \\mid \\tau_m) + \\log p(b \\mid \\tau_b)\n$$\n我们来展开每一项，并舍去任何不依赖于 $m$ 或 $b$ 的加法常数：\n对数似然项为：\n$$\n\\log p(D \\mid m, b, \\sigma) = \\sum_{i=1}^n \\log p(y_i \\mid m, b, x_i, \\sigma) = \\sum_{i=1}^n \\left( \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\frac{(y_i - (m x_i + b))^2}{2\\sigma^2} \\right)\n$$\n忽略常数项 $\\sum_{i=1}^n \\log(1/\\sqrt{2\\pi\\sigma^2})$，我们得到：\n$$\n\\log p(D \\mid m, b, \\sigma) \\propto -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - m x_i - b)^2\n$$\n对数先验项为：\n$$\n\\log p(m \\mid \\tau_m) \\propto -\\frac{m^2}{2\\tau_m^2} \\quad \\text{和} \\quad \\log p(b \\mid \\tau_b) \\propto -\\frac{b^2}{2\\tau_b^2}\n$$\n将这些项组合起来，得到未归一化对数后验密度的最终表达式：\n$$\n\\mathcal{L}_{un}(m,b) = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - m x_i - b)^2 - \\frac{m^2}{2\\tau_m^2} - \\frac{b^2}{2\\tau_b^2}\n$$\n这个表达式就是我们需要在 MCMC 算法中计算的函数。第一项与负的残差平方和成正比，第二项和第三项是源于高斯先验的正则化项 (L2 正则化)。\n\n### Metropolis-Hastings 算法\n\nMetropolis-Hastings 算法用于生成一个样本序列，该序列构成一个马尔可夫链，其平稳分布是所需的目标后验分布 $p(m, b \\mid D, H)$。该算法按以下步骤进行：\n\n1.  **初始化**: 从初始状态 $(m_0, b_0)$ 开始链。问题指定 $(m_0, b_0) = (0,0)$。\n\n2.  **迭代**: 对于每一步 $t=1, 2, \\dots, N_{\\text{total}}$：\n    a.  **提议**: 给定当前状态 $(m_t, b_t)$，从一个提议分布 $Q((m^\\star, b^\\star) \\mid (m_t, b_t))$ 中提议一个新的状态 $(m^\\star, b^\\star)$。问题指定了一个对称高斯随机游走提议：\n        $$\n        m^\\star = m_t + \\eta_m, \\quad \\text{with} \\quad \\eta_m \\sim \\mathcal{N}(0, s_m^2)\n        $$\n        $$\n        b^\\star = b_t + \\eta_b, \\quad \\text{with} \\quad \\eta_b \\sim \\mathcal{N}(0, s_b^2)\n        $$\n    b.  **接受概率**: 计算接受概率 $\\alpha$，即移动到提议状态 $(m^\\star, b^\\star)$ 的概率：\n        $$\n        \\alpha = \\min\\left(1, \\frac{p(m^\\star, b^\\star \\mid D, H)}{p(m_t, b_t \\mid D, H)} \\cdot \\frac{Q((m_t, b_t) \\mid (m^\\star, b^\\star))}{Q((m^\\star, b^\\star) \\mid (m_t, b_t))}\\right)\n        $$\n        由于提议分布是对称的（即从状态 A 提议移动到 B 的概率与从 B 提议到 A 的概率相同），提议密度的比率 (Hastings 比率) 为 $1$。接受概率简化为 Metropolis 规则：\n        $$\n        \\alpha = \\min\\left(1, \\frac{p(m^\\star, b^\\star \\mid D, H)}{p(m_t, b_t \\mid D, H)}\\right)\n        $$\n        为避免数值下溢，这使用对数概率进行计算：\n        $$\n        \\log(\\text{ratio}) = \\mathcal{L}_{un}(m^\\star, b^\\star) - \\mathcal{L}_{un}(m_t, b_t)\n        $$\n        $$\n        \\alpha = \\min(1, \\exp(\\log(\\text{ratio})))\n        $$\n    c.  **接受或拒绝**: 从均匀分布 $U(0,1)$ 中生成一个随机数 $u$。如果 $u  \\alpha$，则接受提议，下一个状态是 $(m_{t+1}, b_{t+1}) = (m^\\star, b^\\star)$。否则，拒绝提议，链保持在当前状态，即 $(m_{t+1}, b_{t+1}) = (m_t, b_t)$。\n\n3.  **分析**: 在链运行 $N_{\\text{total}}$ 次迭代后，链的初始部分（预热期，$N_{\\text{burn-in}}$）被丢弃，以使链收敛到其平稳分布。然后通过计算预热期后样本的算术平均值来估计 $m$ 和 $b$ 的后验均值：\n    $$\n    \\hat{m}_{\\text{mean}} = \\frac{1}{N_{\\text{total}} - N_{\\text{burn-in}}} \\sum_{t=N_{\\text{burn-in}}+1}^{N_{\\text{total}}} m_t\n    $$\n    $$\n    \\hat{b}_{\\text{mean}} = \\frac{1}{N_{\\text{total}} - N_{\\text{burn-in}}} \\sum_{t=N_{\\text{burn-in}}+1}^{N_{\\text{total}}} b_t\n    $$\n此过程将应用于问题陈述中指定的三个测试用例中的每一个。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Metropolis-Hastings MCMC sampler for Bayesian linear regression\n    and applies it to three test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"n\": 20, \"x_range\": [-2.0, 2.0],\n            \"m_true\": 2.0, \"b_true\": -1.0, \"sigma\": 0.5,\n            \"tau_m\": 10.0, \"tau_b\": 10.0,\n            \"s_m\": 0.02, \"s_b\": 0.05,\n            \"data_seed\": 11, \"mcmc_seed\": 101,\n            \"total_iterations\": 20000, \"burn_in\": 5000\n        },\n        # Case 2 (boundary-size dataset with two points and low noise)\n        {\n            \"n\": 2, \"x_manual\": np.array([-1.0, 1.0]),\n            \"m_true\": -0.5, \"b_true\": 2.0, \"sigma\": 0.1,\n            \"tau_m\": 10.0, \"tau_b\": 10.0,\n            \"s_m\": 0.005, \"s_b\": 0.01,\n            \"data_seed\": 22, \"mcmc_seed\": 202,\n            \"total_iterations\": 30000, \"burn_in\": 10000\n        },\n        # Case 3 (noisy observations)\n        {\n            \"n\": 30, \"x_range\": [0.0, 5.0],\n            \"m_true\": 0.3, \"b_true\": 4.0, \"sigma\": 5.0,\n            \"tau_m\": 10.0, \"tau_b\": 10.0,\n            \"s_m\": 0.05, \"s_b\": 0.20,\n            \"data_seed\": 33, \"mcmc_seed\": 303,\n            \"total_iterations\": 25000, \"burn_in\": 8000\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Generate synthetic data\n        rng_data = np.random.default_rng(case[\"data_seed\"])\n        \n        if \"x_manual\" in case:\n            x = case[\"x_manual\"]\n        else:\n            x = np.linspace(case[\"x_range\"][0], case[\"x_range\"][1], case[\"n\"])\n            \n        noise = rng_data.normal(0, case[\"sigma\"], size=case[\"n\"])\n        y = case[\"m_true\"] * x + case[\"b_true\"] + noise\n\n        # Define the unnormalized log posterior function\n        def log_unnormalized_posterior(m, b, x, y, sigma, tau_m, tau_b):\n            residuals = y - (m * x + b)\n            sum_sq_residuals = np.sum(residuals**2)\n            log_likelihood = -0.5 * sum_sq_residuals / (sigma**2)\n            log_prior = -0.5 * (m**2 / tau_m**2 + b**2 / tau_b**2)\n            return log_likelihood + log_prior\n\n        # Initialize MCMC\n        rng_mcmc = np.random.default_rng(case[\"mcmc_seed\"])\n        m_curr, b_curr = 0.0, 0.0\n        \n        m_samples = np.zeros(case[\"total_iterations\"])\n        b_samples = np.zeros(case[\"total_iterations\"])\n        \n        log_post_curr = log_unnormalized_posterior(\n            m_curr, b_curr, x, y, case[\"sigma\"], case[\"tau_m\"], case[\"tau_b\"])\n\n        # Run MCMC sampler\n        for i in range(case[\"total_iterations\"]):\n            # Propose new parameters\n            m_prop = m_curr + rng_mcmc.normal(0, case[\"s_m\"])\n            b_prop = b_curr + rng_mcmc.normal(0, case[\"s_b\"])\n            \n            # Calculate log posterior of proposed parameters\n            log_post_prop = log_unnormalized_posterior(\n                m_prop, b_prop, x, y, case[\"sigma\"], case[\"tau_m\"], case[\"tau_b\"])\n            \n            # Calculate acceptance ratio\n            log_ratio = log_post_prop - log_post_curr\n            alpha = np.min([1.0, np.exp(log_ratio)])\n            \n            # Accept or reject\n            if rng_mcmc.uniform(0, 1)  alpha:\n                m_curr = m_prop\n                b_curr = b_prop\n                log_post_curr = log_post_prop\n            \n            m_samples[i] = m_curr\n            b_samples[i] = b_curr\n            \n        # Post-processing: discard burn-in and compute posterior means\n        post_burn_in_m = m_samples[case[\"burn_in\"]:]\n        post_burn_in_b = b_samples[case[\"burn_in\"]:]\n        \n        m_mean = np.mean(post_burn_in_m)\n        b_mean = np.mean(post_burn_in_b)\n        \n        results.extend([m_mean, b_mean])\n\n    # Format and print the final output\n    formatted_results = \",\".join([f\"{r:.4f}\" for r in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "MCMC 是一个算法家族，而不仅仅是 Metropolis-Hastings 算法。本练习将介绍一种强大的替代方法——Gibbs 采样，当模型的完全条件分布易于采样时，它尤其高效。我们将把它应用于一个有趣的问题：通过对像素的潜在“干净”状态进行采样来恢复一幅损坏的二值图像，这在统计物理和计算机视觉中被称为伊辛模型（Ising model）。",
            "id": "3250350",
            "problem": "实现一个完整的程序，该程序通过在成对马尔可夫随机场先验下进行吉布斯采样，对损坏的二值图像执行马尔可夫链蒙特卡洛（MCMC）去噪。从第一性原理出发，推导吉布斯更新所需的条件分布，然后实现采样器以近似每个像素的后验均值。\n\n从以下建模假设开始，这些是唯一允许的出发点：\n- 潜在的干净图像是一个由二元自旋 $x_{i,j} \\in \\{-1,+1\\}$ 组成的网格，排列在具有四邻域结构（上、下、左、右）的矩形晶格上。先验是一个伊辛型吉布斯分布\n$$\np(x) \\propto \\exp\\left(\\beta \\sum_{\\langle (i,j),(k,\\ell)\\rangle} x_{i,j}\\,x_{k,\\ell}\\right),\n$$\n其中，求和遍历网格内所有无序的相邻对 $\\langle (i,j),(k,\\ell)\\rangle$（无环绕），$\\beta \\ge 0$ 是一个固定的耦合参数。\n- 观测模型在给定 $x$ 的条件下像素间独立，每个像素以概率 $\\varepsilon \\in (0,1)$ 翻转：对于每个位置 $(i,j)$，观测值 $y_{i,j} \\in \\{-1,+1\\}$ 满足\n$$\n\\mathbb{P}(y_{i,j} = x_{i,j} \\mid x_{i,j}) = 1-\\varepsilon,\\quad \\mathbb{P}(y_{i,j} = -x_{i,j} \\mid x_{i,j}) = \\varepsilon.\n$$\n\n您的任务：\n1. 使用贝叶斯法则和上述唯一定义，推导单个像素 $x_{i,j}$ 在给定其在 $x$ 中的四邻域和观测值 $y$ 的情况下的全条件分布。用 $\\beta$、$\\varepsilon$、其在 $x$ 中当前邻居的和以及局部观测值 $y_{i,j}$ 来表示 $x_{i,j}$ 等于 $+1$ 的条件概率的封闭形式。清楚地说明您使用的任何代数变换和出现的任何充分统计量。\n2. 设计一个吉布斯采样器，通过从您在任务1中推导的条件分布中抽样来迭代更新每个 $x_{i,j}$。在一次完整扫描中对所有位置使用系统的光栅扫描。使用一个具有固定种子 $s$ 的伪随机数生成器，以确保扫描更新的可复现性。在 $B$ 次完整扫描的预热期后，再收集 $T$ 次完整扫描，累加每个位置的自旋值的运行总和，以近似后验均值。通过取每个位置累加和的符号来生成去噪估计 $\\hat{x}$；也就是说，如果累加和为正，则 $\\hat{x}_{i,j} = +1$，否则为 $\\hat{x}_{i,j} = -1$。平局可以任意处理；请使用一致的规则。\n3. 对于下面的每个测试用例，计算正确去噪像素的比例，定义为\n$$\n\\text{accuracy} = \\frac{1}{N\\cdot M} \\sum_{i=1}^{N} \\sum_{j=1}^{M} \\mathbf{1}\\{\\hat{x}_{i,j} = x^{\\star}_{i,j}\\},\n$$\n其中 $x^{\\star}$ 是提供的干净图像，$(N,M)$ 是网格尺寸。将准确率报告为 $[0,1]$ 内的实数，四舍五入到三位小数。\n\n实现细节和约束：\n- 网格邻域是四连通且无环绕的；只有有效的边界内邻居有贡献。\n- 吉布斯采样器在单个测试用例的所有更新中必须使用单一固定的伪随机数种子 $s$。\n- 不涉及角度；没有物理单位。将准确率输出为十进制数（无百分号）。\n- 程序必须是自包含的，并且不需要任何输入。\n\n测试套件：\n为以下四个测试用例提供结果。每个用例都指定了干净图像 $x^{\\star}$、观测图像 $y$、耦合参数 $\\beta$、翻转概率参数 $\\varepsilon$、预热期 $B$、采样扫描次数 $T$ 以及采样器种子 $s$。所有网格和值都在 $\\{-1,+1\\}$ 中。\n\n- 测试用例 1（理想情况，中等噪声和平滑）：\n  - 网格尺寸：$8 \\times 8$。\n  - 干净图像 $x^{\\star}$：除中心由行索引 $2$ 到 $5$ 和列索引 $2$ 到 $5$（零基索引）构成的 $4 \\times 4$ 块设置为 $+1$ 外，其余均为 $-1$。\n  - 观测图像 $y$：与 $x^{\\star}$ 相同，但在以下坐标（零基）处翻转：$(0,0)$、$(1,7)$、$(2,3)$、$(3,2)$、$(4,4)$、$(5,5)$、$(6,1)$、$(7,6)$。\n  - 参数：$\\beta = 0.8$，$\\varepsilon = 0.1$，$B = 300$，$T = 601$，种子 $s = 12345$。\n\n- 测试用例 2（边界情况，单像素）：\n  - 网格尺寸：$1 \\times 1$。\n  - 干净图像 $x^{\\star} = [+1]$。\n  - 观测图像 $y = [-1]$。\n  - 参数：$\\beta = 0.6$，$\\varepsilon = 0.3$，$B = 50$，$T = 501$，种子 $s = 7$。\n\n- 测试用例 3（较大网格，低平滑度，较高噪声）：\n  - 网格尺寸：$10 \\times 10$。\n  - 干净图像 $x^{\\star}$：所有位置均为 $+1$。\n  - 观测图像 $y$：与 $x^{\\star}$ 相同，但在所有满足 $(i + j) \\bmod 5 = 0$ 的位置 $(i,j)$（零基 $i,j$）处翻转。\n  - 参数：$\\beta = 0.4$，$\\varepsilon = 0.3$，$B = 300$，$T = 601$，种子 $s = 2024$。\n\n- 测试用例 4（边缘情况，无先验耦合）：\n  - 网格尺寸：$6 \\times 6$。\n  - 干净图像 $x^{\\star}$：左半部分（列 $0,1,2$）设置为 $-1$，右半部分（列 $3,4,5$）设置为 $+1$。\n  - 观测图像 $y$：与 $x^{\\star}$ 相同，但在所有满足 $(i \\cdot 6 + j) \\bmod 5 = 1$ 的位置 $(i,j)$（零基 $i,j$）处翻转。\n  - 参数：$\\beta = 0.0$，$\\varepsilon = 0.2$，$B = 200$，$T = 401$，种子 $s = 999$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含上述测试用例的四个准确率，按顺序排列，作为一个逗号分隔的列表，并用方括号括起来，例如 $[a_1,a_2,a_3,a_4]$，其中每个 $a_k$ 都按规定四舍五入到三位小数。不应打印其他任何输出。",
            "solution": "该问题陈述已经过验证，被认为是可靠、适定和完整的。所有必要的组成部分，包括概率模型、参数和评估标准，都得到了明确的规定。该问题基于统计物理学和贝叶斯推断在计算图像分析中的既定原理。\n\n任务是使用马尔可夫链蒙特卡洛（MCMC）方法，特别是吉布斯采样，对二值图像进行去噪。这需要推导单像素更新的条件分布，实现采样器，并在几个测试用例上评估其性能。\n\n**1. 吉布斯采样器条件分布的推导**\n\n令潜在的干净图像由自旋网格 $x = \\{x_{i,j}\\}$ 表示，其中每个 $x_{i,j} \\in \\{-1, +1\\}$。令观测到的噪声图像为 $y = \\{y_{i,j}\\}$，其中 $y_{i,j} \\in \\{-1, +1\\}$。\n\n该模型由一个先验分布 $p(x)$ 和一个似然函数 $p(y \\mid x)$ 定义。吉布斯采样的目标是从后验分布 $p(x \\mid y)$ 中抽取样本。根据贝叶斯法则，后验分布由下式给出：\n$$\np(x \\mid y) \\propto p(y \\mid x) p(x)\n$$\n其中比例常数是证据 $p(y)$，它不依赖于 $x$，在 MCMC 采样中可以忽略。\n\n干净图像 $x$ 的先验是一个网格上的 Ising 模型，由吉布斯分布给出：\n$$\np(x) \\propto \\exp\\left(\\beta \\sum_{\\langle (i,j),(k,\\ell)\\rangle} x_{i,j}\\,x_{k,\\ell}\\right)\n$$\n这里，$\\beta \\ge 0$ 是鼓励相邻像素具有相同自旋的耦合参数，求和遍历所有相邻像素对（四邻域结构，无环绕）。\n\n观测模型（似然）指出，每个观测像素 $y_{i,j}$ 是从相应的真实像素 $x_{i,j}$ 独立生成的，被翻转的概率为 $\\varepsilon$：\n$$\n\\mathbb{P}(y_{i,j} \\mid x_{i,j}) =\n\\begin{cases}\n1-\\varepsilon  \\text{if } y_{i,j} = x_{i,j} \\\\\n\\varepsilon  \\text{if } y_{i,j} = -x_{i,j}\n\\end{cases}\n$$\n这可以写成更紧凑的指数形式。注意当 $x_{i,j}$ 和 $y_{i,j}$ 相等时，$x_{i,j}y_{i,j}=+1$，不同时为 $-1$。我们可以将概率表示为：\n$$\n\\mathbb{P}(y_{i,j} \\mid x_{i,j}) = (1-\\varepsilon)^{\\frac{1+x_{i,j}y_{i,j}}{2}} \\varepsilon^{\\frac{1-x_{i,j}y_{i,j}}{2}}\n$$\n取对数：\n$$\n\\log \\mathbb{P}(y_{i,j} \\mid x_{i,j}) = \\frac{1+x_{i,j}y_{i,j}}{2}\\log(1-\\varepsilon) + \\frac{1-x_{i,j}y_{i,j}}{2}\\log(\\varepsilon) = \\frac{1}{2}x_{i,j}y_{i,j}\\log\\left(\\frac{1-\\varepsilon}{\\varepsilon}\\right) + C\n$$\n其中 $C$ 是一个不依赖于 $x_{i,j}$ 的常数。因此，我们可以写成：\n$$\n\\mathbb{P}(y_{i,j} \\mid x_{i,j}) \\propto \\exp(\\eta \\, x_{i,j} y_{i,j})\n$$\n其中我们定义参数 $\\eta = \\frac{1}{2}\\log\\left(\\frac{1-\\varepsilon}{\\varepsilon}\\right)$。\n由于给定 $x$ 时观测是独立的，完整的似然函数为：\n$$\np(y \\mid x) = \\prod_{(i,j)} \\mathbb{P}(y_{i,j} \\mid x_{i,j}) \\propto \\exp\\left(\\eta \\sum_{(i,j)} x_{i,j}y_{i,j}\\right)\n$$\n\n结合先验和似然，联合分布 $p(x, y)$ 与后验分布 $p(x \\mid y)$ 成正比：\n$$\np(x \\mid y) \\propto \\exp\\left(\\beta \\sum_{\\langle (i,j),(k,\\ell)\\rangle} x_{i,j}\\,x_{k,\\ell} + \\eta \\sum_{(i,j)} x_{i,j}y_{i,j}\\right)\n$$\n\n吉布斯采样需要单个自旋 $x_{i,j}$ 在给定所有其他自旋 $x_{\\setminus (i,j)}$ 和观测值 $y$ 的情况下的全条件分布。由于马尔可夫随机场的局部性，这个条件分布只依赖于 $x_{i,j}$ 的马尔可夫毯，它由 $x$ 中它的直接邻居（表示为 $\\mathcal{N}_{i,j}$）和相应的观测值 $y_{i,j}$ 组成。\n$$\np(x_{i,j} \\mid x_{\\setminus (i,j)}, y) = p(x_{i,j} \\mid x_{\\mathcal{N}_{i,j}}, y_{i,j})\n$$\n为了推导这个，我们分离出对数后验中所有涉及 $x_{i,j}$ 的项：\n$$\n\\log p(x \\mid y) \\ni \\beta \\sum_{(k,\\ell) \\in \\mathcal{N}_{i,j}} x_{i,j}\\,x_{k,\\ell} + \\eta \\, x_{i,j}y_{i,j} + \\text{const}\n$$\n这可以改写为：\n$$\nx_{i,j} \\left( \\beta \\sum_{(k,\\ell) \\in \\mathcal{N}_{i,j}} x_{k,\\ell} + \\eta \\, y_{i,j} \\right)\n$$\n令 $S_{i,j} = \\sum_{(k,\\ell) \\in \\mathcal{N}_{i,j}} x_{k,\\ell}$ 为位置 $(i,j)$ 邻居的自旋之和。这是先验影响的充分统计量。于是条件分布为：\n$$\np(x_{i,j} \\mid x_{\\setminus (i,j)}, y) \\propto \\exp\\left( x_{i,j} \\left( \\beta S_{i,j} + \\eta y_{i,j} \\right) \\right)\n$$\n令 $A_{i,j} = \\beta S_{i,j} + \\eta y_{i,j}$。因为 $x_{i,j}$ 只能取 $+1$ 或 $-1$：\n$$\n\\mathbb{P}(x_{i,j}=+1 \\mid \\dots) = \\frac{\\exp(+1 \\cdot A_{i,j})}{\\exp(+1 \\cdot A_{i,j}) + \\exp(-1 \\cdot A_{i,j})} = \\frac{\\exp(A_{i,j})}{\\exp(A_{i,j}) + \\exp(-A_{i,j})}\n$$\n为了数值稳定性，这可以用逻辑 sigmoid 函数表示：\n$$\n\\mathbb{P}(x_{i,j}=+1 \\mid \\dots) = \\frac{1}{1 + \\exp(-2A_{i,j})}\n$$\n代入 $A_{i,j}$ 和 $\\eta$ 的表达式，最终的封闭形式概率为：\n$$\n\\mathbb{P}(x_{i,j}=+1 \\mid x_{\\setminus (i,j)}, y) = \\frac{1}{1 + \\exp\\left(-2\\left[\\beta S_{i,j} + \\frac{y_{i,j}}{2}\\log\\left(\\frac{1-\\varepsilon}{\\varepsilon}\\right)\\right]\\right)}\n$$\n其中 $S_{i,j}$ 是像素 $(i,j)$ 的四个直接邻居的自旋之和。\n\n**2. 吉布斯采样器算法设计**\n\n吉布斯采样器按以下步骤从后验 $p(x \\mid y)$ 生成样本：\n\n1.  **初始化**：将潜在状态 $x^{(0)}$ 初始化为一个起始配置。一个常见且有效的选择是将其设置为观测到的噪声图像，$x^{(0)} = y$。这将马尔可夫链置于状态空间的一个高概率区域。\n2.  **迭代**：对于每次扫描 $t=1, 2, \\dots, B+T$：\n    a.  **系统扫描**：按固定顺序遍历每个像素位置 $(i,j)$，例如光栅扫描（逐行逐列）。\n    b.  **更新**：对于每个位置 $(i,j)$，通过从上面推导出的全条件分布中抽取一个新值来更新其自旋 $x_{i,j}$。\n        i.  计算邻居自旋之和，$S_{i,j} = \\sum_{(k,\\ell) \\in \\mathcal{N}_{i,j}} x_{k,\\ell}^{(t')}$，其中 $x^{(t')}$ 表示网格的当前状态。注意更新是“就地”完成的，因此对于光栅扫描，当前扫描中已经访问过的邻居将使用它们的新值。\n        ii. 计算 $A_{i,j} = \\beta S_{i,j} + \\eta y_{i,j}$。\n        iii. 计算条件概率 $p_{+} = \\mathbb{P}(x_{i,j}=+1 \\mid \\dots) = 1/(1 + \\exp(-2A_{i,j}))$。\n        iv. 从一个具有固定种子 $s$ 的生成器中抽取一个均匀随机数 $u \\sim U(0,1)$。\n        v. 设置新的自旋：如果 $u  p_{+}$，则 $x_{i,j} = +1$，否则 $x_{i,j} = -1$。\n3.  **预热期与采样**：\n    a.  丢弃前 $B$ 次扫描作为“预热期”。这允许马尔可夫链收敛到其平稳分布，即所期望的后验分布 $p(x \\mid y)$。\n    b.  预热期后，收集接下来 $T$ 次扫描的状态，$\\{x^{(B+1)}, x^{(B+2)}, \\dots, x^{(B+T)}\\}$。\n4.  **后验均值估计**：通过样本均值来近似每个自旋的后验均值 $\\mathbb{E}[x_{i,j} \\mid y]$：\n    $$\n    \\bar{x}_{i,j} = \\frac{1}{T} \\sum_{t=1}^{T} x_{i,j}^{(B+t)}\n    $$\n    在实现中，这是通过在 $T$ 次采样扫描中维护每个位置自旋的运行总和来实现的。\n5.  **去噪图像构建**：最终的去噪估计 $\\hat{x}$ 是通过取每个像素处累积和（或样本均值）的符号得到的。\n    $$\n    \\hat{x}_{i,j} = \\text{sign}(\\bar{x}_{i,j})\n    $$\n    如规定，如果和为正，$\\hat{x}_{i,j}=+1$；否则（如果为零或负），$\\hat{x}_{i,j}=-1$。\n\n**3. 准确率计算**\n\n对于每个测试用例，将最终的去噪图像 $\\hat{x}$ 与提供的基准干净图像 $x^{\\star}$进行比较。准确率是正确估计的像素的比例：\n$$\n\\text{accuracy} = \\frac{1}{NM} \\sum_{i=0}^{N-1} \\sum_{j=0}^{M-1} \\mathbf{1}\\{\\hat{x}_{i,j} = x^{\\star}_{i,j}\\}\n$$\n其中 $N \\times M$ 是网格尺寸，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。结果四舍五入到三位小数。\n\n这完成了解决该问题的理论框架和算法设计。实现将对每个指定的测试用例遵循此逻辑。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef create_test_cases():\n    \"\"\"Generates the four test cases as defined in the problem.\"\"\"\n    test_cases = []\n\n    # Test case 1\n    N1, M1 = 8, 8\n    x_star1 = np.full((N1, M1), -1, dtype=np.int8)\n    x_star1[2:6, 2:6] = 1\n    y1 = x_star1.copy()\n    flips1 = [(0, 0), (1, 7), (2, 3), (3, 2), (4, 4), (5, 5), (6, 1), (7, 6)]\n    for i, j in flips1:\n        y1[i, j] *= -1\n    case1 = {\n        \"x_star\": x_star1,\n        \"y\": y1,\n        \"beta\": 0.8,\n        \"epsilon\": 0.1,\n        \"B\": 300,\n        \"T\": 601,\n        \"seed\": 12345\n    }\n    test_cases.append(case1)\n\n    # Test case 2\n    x_star2 = np.array([[1]], dtype=np.int8)\n    y2 = np.array([[-1]], dtype=np.int8)\n    case2 = {\n        \"x_star\": x_star2,\n        \"y\": y2,\n        \"beta\": 0.6,\n        \"epsilon\": 0.3,\n        \"B\": 50,\n        \"T\": 501,\n        \"seed\": 7\n    }\n    test_cases.append(case2)\n\n    # Test case 3\n    N3, M3 = 10, 10\n    x_star3 = np.full((N3, M3), 1, dtype=np.int8)\n    y3 = x_star3.copy()\n    for i in range(N3):\n        for j in range(M3):\n            if (i + j) % 5 == 0:\n                y3[i, j] *= -1\n    case3 = {\n        \"x_star\": x_star3,\n        \"y\": y3,\n        \"beta\": 0.4,\n        \"epsilon\": 0.3,\n        \"B\": 300,\n        \"T\": 601,\n        \"seed\": 2024\n    }\n    test_cases.append(case3)\n\n    # Test case 4\n    N4, M4 = 6, 6\n    x_star4 = np.full((N4, M4), -1, dtype=np.int8)\n    x_star4[:, 3:] = 1\n    y4 = x_star4.copy()\n    for i in range(N4):\n        for j in range(M4):\n            if (i * M4 + j) % 5 == 1:\n                y4[i, j] *= -1\n    case4 = {\n        \"x_star\": x_star4,\n        \"y\": y4,\n        \"beta\": 0.0,\n        \"epsilon\": 0.2,\n        \"B\": 200,\n        \"T\": 401,\n        \"seed\": 999\n    }\n    test_cases.append(case4)\n\n    return test_cases\n\ndef gibbs_denoise(y, beta, epsilon, B, T, seed):\n    \"\"\"\n    Performs MCMC denoising using Gibbs sampling.\n    \"\"\"\n    N, M = y.shape\n    rng = np.random.default_rng(seed)\n\n    # Initialize latent state x with the observed image y\n    x = y.copy()\n\n    # Pre-calculate eta to avoid repeated computation\n    # Epsilon must be in (0,1) to avoid division by zero or log of zero.\n    # The problem statement guarantees epsilon in (0,1).\n    eta = 0.5 * np.log((1.0 - epsilon) / epsilon)\n\n    # Burn-in phase\n    for _ in range(B):\n        # Raster scan over all pixels\n        for i in range(N):\n            for j in range(M):\n                # Calculate sum of neighbors\n                S_ij = 0\n                if i > 0: S_ij += x[i - 1, j]\n                if i  N - 1: S_ij += x[i + 1, j]\n                if j > 0: S_ij += x[i, j - 1]\n                if j  M - 1: S_ij += x[i, j + 1]\n\n                # Calculate the argument of the exponential\n                A_ij = beta * S_ij + eta * y[i, j]\n\n                # Calculate conditional probability of x_ij being +1\n                p_plus = 1.0 / (1.0 + np.exp(-2.0 * A_ij))\n\n                # Sample new value for x_ij\n                if rng.random()  p_plus:\n                    x[i, j] = 1\n                else:\n                    x[i, j] = -1\n\n    # Sampling phase\n    x_sum = np.zeros_like(x, dtype=np.float64)\n    for _ in range(T):\n        # Raster scan over all pixels\n        for i in range(N):\n            for j in range(M):\n                # Calculate sum of neighbors\n                S_ij = 0\n                if i > 0: S_ij += x[i - 1, j]\n                if i  N - 1: S_ij += x[i + 1, j]\n                if j > 0: S_ij += x[i, j - 1]\n                if j  M - 1: S_ij += x[i, j + 1]\n                \n                # Calculate the argument of the exponential\n                A_ij = beta * S_ij + eta * y[i, j]\n\n                # Calculate conditional probability of x_ij being +1\n                p_plus = 1.0 / (1.0 + np.exp(-2.0 * A_ij))\n\n                # Sample new value for x_ij\n                if rng.random()  p_plus:\n                    x[i, j] = 1\n                else:\n                    x[i, j] = -1\n        \n        # Accumulate the sample after a full sweep\n        x_sum += x\n\n    # Construct the final estimate by taking the sign of the sum\n    x_hat = np.ones_like(x, dtype=np.int8)\n    x_hat[x_sum = 0] = -1\n\n    return x_hat\n\ndef solve():\n    \"\"\"\n    Main function to run the MCMC denoising for all test cases.\n    \"\"\"\n    test_cases = create_test_cases()\n    results = []\n\n    for case in test_cases:\n        x_star = case[\"x_star\"]\n        y = case[\"y\"]\n        beta = case[\"beta\"]\n        epsilon = case[\"epsilon\"]\n        B = case[\"B\"]\n        T = case[\"T\"]\n        seed = case[\"seed\"]\n\n        # Get the denoised image estimate\n        x_hat = gibbs_denoise(y, beta, epsilon, B, T, seed)\n\n        # Calculate accuracy\n        accuracy = np.mean(x_hat == x_star)\n        \n        # Round to three decimal places\n        rounded_accuracy = round(accuracy, 3)\n        results.append(f\"{rounded_accuracy:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}