{
    "hands_on_practices": [
        {
            "introduction": "Sensitivity analysis often begins with the fundamental tools of calculus. This practice grounds the concept in a classic microeconomic model of supply and demand . By analytically calculating the partial derivatives of the equilibrium price with respect to market parameters, you will gain a first-principles understanding of how to quantify a model's response to change and interpret its economic meaning.",
            "id": "3191079",
            "problem": "A computational economist is building a simple market model to study how estimates of responsiveness parameters affect the computed equilibrium price. The model uses linear supply and demand functions, where quantity supplied and quantity demanded depend on price. Let the supply be given by $Q_{s} = a + bP$ and the demand be given by $Q_{d} = c - dP$, where $Q_{s}$ and $Q_{d}$ are quantities, $P$ is price, and $a$, $b$, $c$, $d$ are strictly positive parameters representing baseline levels ($a$, $c$) and responsiveness coefficients ($b$, $d$). Assume $c > a$ so that the market-clearing price is positive.\n\nStarting from the definitions of market equilibrium (quantity supplied equals quantity demanded) and sensitivity analysis (change in a model output with respect to its parameters defined via partial derivatives), derive the equilibrium price $P^{*}$ as a function of $a$, $b$, $c$, and $d$. Then, from first principles, quantify the sensitivity of $P^{*}$ to the responsiveness coefficients $b$ and $d$ by computing the exact partial derivatives $\\frac{\\partial P^{*}}{\\partial b}$ and $\\frac{\\partial P^{*}}{\\partial d}$.\n\nExpress your final answer as two closed-form analytic expressions in terms of $a$, $b$, $c$, and $d$. No rounding is required, and no units should be included in the final expressions.",
            "solution": "The problem statement will first be subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe explicit information provided in the problem statement is as follows:\n- Supply function: $Q_{s} = a + bP$\n- Demand function: $Q_{d} = c - dP$\n- Variables: $Q_s$ (quantity supplied), $Q_d$ (quantity demanded), $P$ (price).\n- Parameters: $a, b, c, d$ are all strictly positive real numbers.\n- Interpretation of parameters: $a, c$ are baseline levels; $b, d$ are responsiveness coefficients.\n- Constraint: $c > a$.\n- Definition of market equilibrium: $Q_s = Q_d$.\n- Definition of sensitivity analysis: Quantified by partial derivatives of a model output with respect to its parameters.\n- Objective:\n    1. Derive the equilibrium price, denoted $P^*$, as a function of $a, b, c, d$.\n    2. Compute the sensitivities $\\frac{\\partial P^{*}}{\\partial b}$ and $\\frac{\\partial P^{*}}{\\partial d}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established criteria:\n- **Scientifically Grounded:** The problem uses a standard linear supply-demand model, a cornerstone of microeconomic theory and a common example in computational modeling. The concepts of market equilibrium and sensitivity analysis are fundamental and well-established principles. The setup is scientifically sound.\n- **Well-Posed:** The problem provides all necessary equations and constraints to determine a unique solution. The conditions $b > 0$ and $d > 0$ ensure that the supply curve has a positive slope and the demand curve has a negative slope, as is standard. The sum $b + d$ will appear in the denominator of the expression for price; since both are strictly positive, their sum is non-zero, preventing division by zero. The condition $c > a$ ensures that the resulting equilibrium price $P^*$ is positive, which is physically meaningful in this context. Therefore, a unique, stable, and meaningful solution exists.\n- **Objective:** The problem is stated using precise and unambiguous mathematical and economic language. There are no subjective claims or opinions.\n\n### Step 3: Verdict and Action\nThe problem is scientifically sound, well-posed, objective, and complete. It is a valid problem requiring a formal derivation. We may proceed with the solution.\n\n### Solution Derivation\nThe objective is to find the equilibrium price $P^*$ and then to compute its sensitivity with respect to the parameters $b$ and $d$.\n\nFirst, we determine the equilibrium price $P^*$. Market equilibrium occurs when the quantity supplied equals the quantity demanded, i.e., $Q_s = Q_d$. We substitute the given functional forms for supply and demand:\n$$a + bP = c - dP$$\nTo solve for the price $P$, we rearrange the equation to isolate $P$. We gather all terms involving $P$ on one side of the equation and the constant terms on the other.\n$$bP + dP = c - a$$\nFactoring out $P$ from the left-hand side gives:\n$$P(b + d) = c - a$$\nThe equilibrium price $P^*$ is then found by dividing by the term $(b+d)$. Since $b > 0$ and $d > 0$, it is guaranteed that $b+d > 0$, so this division is well-defined.\n$$P^{*} = \\frac{c - a}{b + d}$$\nThe problem specifies that $c > a$, which ensures that the numerator $c - a$ is positive. The denominator $b+d$ is also positive. Therefore, the equilibrium price $P^*$ is strictly positive, consistent with economic reality.\n\nNext, we quantify the sensitivity of the equilibrium price $P^*$ to the responsiveness coefficients $b$ and $d$. This is accomplished by computing the partial derivatives of $P^*$ with respect to each of these parameters.\n\nTo compute the partial derivative of $P^*$ with respect to $b$, denoted $\\frac{\\partial P^{*}}{\\partial b}$, we treat $a$, $c$, and $d$ as constants. We apply the quotient rule for differentiation to the expression for $P^*$:\n$$\\frac{\\partial P^{*}}{\\partial b} = \\frac{\\partial}{\\partial b} \\left( \\frac{c - a}{b + d} \\right)$$\nThe quotient rule states that for a function $f(x) = \\frac{u(x)}{v(x)}$, the derivative is $f'(x) = \\frac{u'(x)v(x) - u(x)v'(x)}{[v(x)]^2}$. In our case, the variable is $b$, the numerator is $u(b) = c - a$, and the denominator is $v(b) = b + d$.\nThe derivatives of $u$ and $v$ with respect to $b$ are:\n$$\\frac{\\partial u}{\\partial b} = \\frac{\\partial}{\\partial b}(c - a) = 0$$\n$$\\frac{\\partial v}{\\partial b} = \\frac{\\partial}{\\partial b}(b + d) = 1$$\nSubstituting these into the quotient rule formula:\n$$\\frac{\\partial P^{*}}{\\partial b} = \\frac{(0)(b + d) - (c - a)(1)}{(b + d)^2}$$\nThis simplifies to:\n$$\\frac{\\partial P^{*}}{\\partial b} = -\\frac{c - a}{(b + d)^2}$$\n\nSimilarly, to compute the partial derivative of $P^*$ with respect to $d$, denoted $\\frac{\\partial P^{*}}{\\partial d}$, we treat $a$, $b$, and $c$ as constants. We again apply the quotient rule to the expression for $P^*$:\n$$\\frac{\\partial P^{*}}{\\partial d} = \\frac{\\partial}{\\partial d} \\left( \\frac{c - a}{b + d} \\right)$$\nHere, the variable is $d$, with numerator $u(d) = c - a$ and denominator $v(d) = b + d$.\nThe derivatives of $u$ and $v$ with respect to $d$ are:\n$$\\frac{\\partial u}{\\partial d} = \\frac{\\partial}{\\partial d}(c - a) = 0$$\n$$\\frac{\\partial v}{\\partial d} = \\frac{\\partial}{\\partial d}(b + d) = 1$$\nSubstituting these into the quotient rule formula:\n$$\\frac{\\partial P^{*}}{\\partial d} = \\frac{(0)(b + d) - (c - a)(1)}{(b + d)^2}$$\nThis simplifies to:\n$$\\frac{\\partial P^{*}}{\\partial d} = -\\frac{c - a}{(b + d)^2}$$\n\nThe two sensitivities are therefore identical. Both derivatives are negative, which is expected. An increase in either the supply responsiveness ($b$) or the demand responsiveness ($d$) leads to a decrease in the equilibrium price, as a more elastic market reacts more strongly to price changes, thus driving the equilibrium price down.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{c-a}{(b+d)^{2}} & -\\frac{c-a}{(b+d)^{2}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Not all systems are described by smooth, continuous functions, and sensitivity analysis is just as crucial in discrete domains. This exercise takes you into the world of combinatorial optimization, using the classic zero-one knapsack problem as a case study . By implementing a dynamic programming solver, you will directly measure how perturbing a single item's weight affects the optimal value, a practical approach for analyzing algorithms and operational models.",
            "id": "3272426",
            "problem": "You are given the classical zero-one knapsack optimization problem and asked to perform a discrete sensitivity analysis of its dynamic programming solution with respect to changing the weight of a single item. The knapsack problem can be stated as follows: given item weights $w_j \\in \\mathbb{Z}_{\\ge 1}$, item values $v_j \\in \\mathbb{Z}_{\\ge 0}$ for $j \\in \\{0,1,\\dots,n-1\\}$, and capacity $C \\in \\mathbb{Z}_{\\ge 0}$, choose decision variables $x_j \\in \\{0,1\\}$ to maximize the total value subject to the capacity constraint. The target of this task is to quantify how the optimal value changes when a single item's weight is perturbed by an integer amount.\n\nFundamental base: Use Bellman's principle of optimality for dynamic programming derived from the definition of the zero-one knapsack problem. Start from the definition that the optimal value over a set of items and remaining capacity equals the best between excluding or including the current item when feasible. You must not assume any closed-form shortcuts.\n\nSensitivity definition: For a fixed test instance, let $V^\\star$ denote the optimal objective value of the original instance. Fix a target item index $i \\in \\{0,1,\\dots,n-1\\}$ and an integer perturbation $d$. Form the perturbed instance by changing only the weight of item $i$ from $w_i$ to $\\tilde{w}_i(d) = \\max(1, w_i + d)$ while keeping all other data unchanged. Let $V^\\star(d)$ be the optimal value of the perturbed instance. The discrete sensitivity for perturbation $d$ is $S(d) = V^\\star(d) - V^\\star$.\n\nEdge-case-handling requirement: If $w_i + d \\le 0$, you must clamp the perturbed weight to $\\tilde{w}_i(d) = 1$ to keep the problem well-defined over nonnegative integer capacities. All inputs use integer weights, values, and capacities.\n\nAlgorithmic requirement: Implement a dynamic programming solver for the zero-one knapsack problem that is faithful to the principle of optimality, using integer arithmetic. Do not use approximation schemes or greedy heuristics. You may implement either a one-dimensional or a two-dimensional dynamic programming array as long as it correctly computes the exact optimal value in $O(nC)$ time.\n\nOutput requirement: For each test case, compute the baseline optimal value $V^\\star$ and then the discrete sensitivities $S(d)$ for each specified perturbation $d$, in the given order. Aggregate the results over all test cases into a single flat list of integers in the exact order specified below. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[1,2,3]\").\n\nAngle unit: Not applicable.\n\nPhysical units: Not applicable.\n\nPercentages: Not applicable.\n\nTest suite to implement and evaluate:\n- Test case $1$: Weights $w = [3,4,5]$, Values $v = [30,50,60]$, Capacity $C = 8$, Perturb item index $i = 2$ (zero-based), Perturbations $D = [1,-1,3,-2]$. Output for this test case should be the baseline $V^\\star$ followed by $S(1)$, $S(-1)$, $S(3)$, $S(-2)$ in this order.\n- Test case $2$: Weights $w = [5,5,5]$, Values $v = [10,40,30]$, Capacity $C = 5$, Perturb item index $i = 1$, Perturbations $D = [1,-1,-5,5]$. Output for this test case should be the baseline $V^\\star$ followed by $S(1)$, $S(-1)$, $S(-5)$, $S(5)$.\n- Test case $3$: Weights $w = [2,2,3]$, Values $v = [3,7,9]$, Capacity $C = 3$, Perturb item index $i = 0$, Perturbations $D = [-1,1,3,-2]$. Output for this test case should be the baseline $V^\\star$ followed by $S(-1)$, $S(1)$, $S(3)$, $S(-2)$.\n- Test case $4$: Weights $w = [1,2,2,3]$, Values $v = [2,4,4,6]$, Capacity $C = 4$, Perturb item index $i = 3$, Perturbations $D = [1,-1,100,-2]$. Output for this test case should be the baseline $V^\\star$ followed by $S(1)$, $S(-1)$, $S(100)$, $S(-2)$.\n- Test case $5$: Weights $w = [4,6,5,3]$, Values $v = [7,12,10,6]$, Capacity $C = 10$, Perturb item index $i = 1$, Perturbations $D = [-1,1,-3,4]$. Output for this test case should be the baseline $V^\\star$ followed by $S(-1)$, $S(1)$, $S(-3)$, $S(4)$.\n\nFinal output format: Your program must print a single line consisting of a flat list with $25$ integers corresponding to the five numbers per test case, concatenated in the order of the test cases above, enclosed in square brackets and separated by commas with no spaces.",
            "solution": "The problem requires performing a discrete sensitivity analysis on the optimal value of the zero-one knapsack problem. This analysis involves perturbing the weight of a single item and calculating the resulting change in the maximum achievable value. The solution must be derived from first principles using dynamic programming, adhering to Bellman's principle of optimality.\n\n### 1. Dynamic Programming Formulation for the Zero-One Knapsack Problem\n\nThe zero-one knapsack problem is a classic combinatorial optimization problem. Given a set of $n$ items, indexed $j \\in \\{0, 1, \\dots, n-1\\}$, each with a weight $w_j \\in \\mathbb{Z}_{\\ge 1}$ and a value $v_j \\in \\mathbb{Z}_{\\ge 0}$, and a knapsack with capacity $C \\in \\mathbb{Z}_{\\ge 0}$, the objective is to select a subset of items that maximizes the total value without exceeding the capacity $C$.\n\nLet $dp(k, c)$ denote the maximum value that can be obtained using a subset of the first $k$ items (i.e., items with indices from $0$ to $k-1$) with a total capacity of $c$. According to Bellman's principle of optimality, an optimal solution can be constructed from optimal solutions to subproblems. For item $k-1$, we have two choices:\n\n1.  **Exclude item $k-1$**: The maximum value is determined by the optimal solution for the first $k-1$ items with the same capacity $c$. This is given by $dp(k-1, c)$.\n2.  **Include item $k-1$**: This is only feasible if its weight $w_{k-1}$ is less than or equal to the current capacity $c$. If included, it contributes its value $v_{k-1}$, and the remaining capacity $c - w_{k-1}$ must be optimally filled with items from the first $k-1$ items. The value for this choice is $v_{k-1} + dp(k-1, c - w_{k-1})$.\n\nThe optimal value $dp(k, c)$ is the maximum of these two choices. This yields the recurrence relation:\n$$\ndp(k, c) = \\begin{cases}\n    dp(k-1, c) & \\text{if } w_{k-1} > c \\\\\n    \\max(dp(k-1, c), v_{k-1} + dp(k-1, c - w_{k-1})) & \\text{if } w_{k-1} \\le c\n\\end{cases}\n$$\nfor $k = 1, \\dots, n$ and $c = 1, \\dots, C$. The base cases are $dp(0, c) = 0$ for all $c$, as no items yield zero value. The final optimal value for the entire problem is $dp(n, C)$.\n\nThis two-dimensional recurrence can be optimized to use a one-dimensional array, let's call it $DP$, of size $C+1$. $DP[c]$ will store the maximum value for a capacity of $c$. For each item $j \\in \\{0, 1, \\dots, n-1\\}$, we update this array. To ensure that each item is considered at most once (the \"zero-one\" property), the capacity loop must iterate downwards from $C$ to $w_j$. The update rule for item $j$ is:\n$$ \\text{For } c \\text{ from } C \\text{ down to } w_j, \\quad DP[c] \\leftarrow \\max(DP[c], v_j + DP[c - w_j]) $$\nAfter iterating through all items, $DP[C]$ will contain the optimal value for the problem. This approach has a time complexity of $O(nC)$.\n\n### 2. Sensitivity Analysis Algorithm\n\nThe core of the task is to analyze the sensitivity of the optimal value to changes in a single item's weight. Let $V^\\star$ be the optimal value for a given baseline instance $(w, v, C)$. We are asked to compute the sensitivity $S(d) = V^\\star(d) - V^\\star$, where $V^\\star(d)$ is the optimal value for a perturbed instance.\n\nThe perturbed instance is created by changing the weight of a specific item $i$ from $w_i$ to $\\tilde{w}_i(d) = w_i + d$. A crucial constraint is that weights must be positive integers, so the perturbed weight is defined as $\\tilde{w}_i(d) = \\max(1, w_i + d)$.\n\nThe overall procedure for each test case is as follows:\n\n1.  **Define a Knapsack Solver**: Implement a function, `solve_knapsack(weights, values, capacity)`, based on the one-dimensional dynamic programming algorithm described above. This function will take a set of weights, values, and a capacity, and return the maximum achievable value.\n\n2.  **Process Test Case**: For each given test case consisting of weights $w$, values $v$, capacity $C$, a target item index $i$, and a list of perturbations $D$:\n    a. **Compute Baseline**: Calculate the baseline optimal value by calling the solver on the original inputs: $V^\\star = \\text{solve\\_knapsack}(w, v, C)$.\n    b. **Iterate Through Perturbations**: For each perturbation $d$ in the list $D$:\n        i. Create a new list of weights $\\tilde{w}$ by copying the original weights $w$.\n        ii. Calculate the new weight for item $i$: $\\tilde{w}_i = \\max(1, w_i + d)$. Update this weight in the list $\\tilde{w}$.\n        iii. Compute the perturbed optimal value: $V^\\star(d) = \\text{solve\\_knapsack}(\\tilde{w}, v, C)$.\n        iv. Calculate the sensitivity: $S(d) = V^\\star(d) - V^\\star$.\n    c. **Collect Results**: Store $V^\\star$ and the sequence of calculated sensitivities $S(d)$ for the current test case.\n\n3.  **Aggregate and Format**: Concatenate the results from all test cases into a single flat list. The final output is this list, formatted as a comma-separated string enclosed in square brackets.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to perform sensitivity analysis on the knapsack problem\n    for a suite of test cases.\n    \"\"\"\n\n    def knapsack_solver(weights, values, capacity):\n        \"\"\"\n        Solves the zero-one knapsack problem using dynamic programming.\n\n        Args:\n            weights (list[int]): A list of item weights.\n            values (list[int]): A list of item values.\n            capacity (int): The knapsack capacity.\n\n        Returns:\n            int: The maximum value that can be achieved.\n        \"\"\"\n        num_items = len(weights)\n        # DP table stores the max value for each capacity from 0 to C.\n        dp_table = np.zeros(capacity + 1, dtype=np.int64)\n\n        for i in range(num_items):\n            weight = weights[i]\n            value = values[i]\n            # Iterate backwards to ensure each item is used at most once.\n            for c in range(capacity, weight - 1, -1):\n                dp_table[c] = max(dp_table[c], value + dp_table[c - weight])\n\n        return int(dp_table[capacity])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (weights, values, capacity, item_index_to_perturb, perturbations)\n        ([3, 4, 5], [30, 50, 60], 8, 2, [1, -1, 3, -2]),\n        ([5, 5, 5], [10, 40, 30], 5, 1, [1, -1, -5, 5]),\n        ([2, 2, 3], [3, 7, 9], 3, 0, [-1, 1, 3, -2]),\n        ([1, 2, 2, 3], [2, 4, 4, 6], 4, 3, [1, -1, 100, -2]),\n        ([4, 6, 5, 3], [7, 12, 10, 6], 10, 1, [-1, 1, -3, 4]),\n    ]\n\n    all_results = []\n\n    for w_orig, v_orig, C, i, D in test_cases:\n        case_results = []\n\n        # 1. Calculate the baseline optimal value\n        baseline_optimal_value = knapsack_solver(w_orig, v_orig, C)\n        case_results.append(baseline_optimal_value)\n\n        # 2. Calculate sensitivities for each perturbation\n        for d in D:\n            w_perturbed = list(w_orig)\n            \n            # Apply perturbation and clamp the weight to be at least 1\n            perturbed_weight = max(1, w_orig[i] + d)\n            w_perturbed[i] = perturbed_weight\n            \n            # Calculate perturbed optimal value\n            perturbed_optimal_value = knapsack_solver(w_perturbed, v_orig, C)\n\n            # Calculate and store the sensitivity\n            sensitivity = perturbed_optimal_value - baseline_optimal_value\n            case_results.append(sensitivity)\n\n        all_results.extend(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Modern scientific models are often complex computational pipelines where an end-to-end analytical derivative is intractable. This advanced practice explores the sensitivity of a Gaussian blur operation with respect to its standard deviation parameter, $\\sigma$ . You will implement a solution using the Discrete Fourier Transform and discover how the analytical derivative of a model's component—the filter kernel—can be leveraged to compute the sensitivity of the entire system's output.",
            "id": "3272432",
            "problem": "Consider a two-dimensional discrete image represented as a real-valued array $I \\in \\mathbb{R}^{n \\times n}$ with $n = 16$. A Gaussian blur with standard deviation $\\sigma > 0$ is defined by the convolution of $I$ with a two-dimensional, separable Gaussian kernel $K_{\\sigma}$ constructed from a one-dimensional Gaussian $g(x;\\sigma)$ along each axis. The one-dimensional Gaussian is given by the foundational definition\n$$\ng(x;\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right),\n$$\nand the two-dimensional kernel on a finite stencil is $K_{\\sigma}(x,y) = g(x;\\sigma)\\,g(y;\\sigma)$ evaluated on integer grid points $(x,y) \\in \\{-R,\\ldots,R\\} \\times \\{-R,\\ldots,R\\}$, where $R = 7$ is the fixed stencil half-width used for numerical approximation. This finite-support approximation is scientifically reasonable when $R$ is sufficiently large relative to $\\sigma$, and here it is fixed for reproducibility across all test cases.\n\nFor the discrete image, the convolution is performed using the two-dimensional Discrete Fourier Transform (DFT), which corresponds to convolution under periodic boundary conditions (circular convolution). Specifically, the blurred image $B_{\\sigma}$ is given by\n$$\nB_{\\sigma} = \\mathcal{F}^{-1}\\!\\left(\\mathcal{F}(I)\\,\\odot\\,\\mathcal{F}(K_{\\sigma}^{\\mathrm{pad}})\\right),\n$$\nwhere $\\mathcal{F}$ denotes the DFT, $\\mathcal{F}^{-1}$ its inverse, $\\odot$ denotes pointwise multiplication, and $K_{\\sigma}^{\\mathrm{pad}}$ is the kernel $K_{\\sigma}$ zero-padded to the image size $n \\times n$ and circularly shifted so that its center aligns with the zero shift in the DFT domain.\n\nDefine the sensitivity of the Gaussian-blurred image to $\\sigma$ as the ratio\n$$\nS(I,\\sigma) = \\frac{\\left\\lVert \\frac{\\partial B_{\\sigma}}{\\partial \\sigma} \\right\\rVert_{F}}{\\left\\lVert B_{\\sigma} \\right\\rVert_{F}},\n$$\nwhere $\\lVert\\cdot\\rVert_{F}$ denotes the Frobenius norm. If $\\lVert B_{\\sigma} \\rVert_{F} = 0$, define $S(I,\\sigma) = 0$ by convention to avoid division by zero. Using the product structure $K_{\\sigma}(x,y) = g(x;\\sigma)g(y;\\sigma)$ and the linearity of differentiation and convolution, the sensitivity can be computed by convolving $I$ with $\\frac{\\partial K_{\\sigma}}{\\partial \\sigma}$, which itself is obtained from the derivative of the one-dimensional Gaussian with respect to $\\sigma$:\n$$\n\\frac{\\partial g(x;\\sigma)}{\\partial \\sigma} = g(x;\\sigma)\\left(\\frac{x^{2}}{\\sigma^{3}} - \\frac{1}{\\sigma}\\right),\n$$\nand then applying the product rule for the separable two-dimensional kernel.\n\nYour task is to implement a program that:\n- Constructs $K_{\\sigma}$ and $\\frac{\\partial K_{\\sigma}}{\\partial \\sigma}$ on the finite stencil of size $(2R+1)\\times(2R+1)$ with $R=7$ for each given $\\sigma$.\n- Performs circular convolution using the DFT to obtain $B_{\\sigma}$ and $\\frac{\\partial B_{\\sigma}}{\\partial \\sigma}$.\n- Computes $S(I,\\sigma)$ for each test case, using the Frobenius norm.\n\nUse the following test suite of images and parameters:\n- Case $1$: $I$ is an impulse image with $I_{8,8} = 1$ and all other entries equal to $0$, with $\\sigma = 1.0$.\n- Case $2$: $I$ is a ramp image defined by $I_{i,j} = \\frac{i + j}{30}$ for indices $i,j \\in \\{0,\\ldots,15\\}$, with $\\sigma = 0.5$.\n- Case $3$: $I$ is a constant image with $I_{i,j} = 1$ for all $i,j$, with $\\sigma = 2.0$.\n- Case $4$: $I$ is a checkerboard image defined by $I_{i,j} = (-1)^{i+j}$, with $\\sigma = 3.0$.\n- Case $5$: $I$ is a random image generated by a pseudo-random number generator seeded with $0$ and entries independently drawn from the uniform distribution on $[0,1]$, with $\\sigma = 0.1$.\n\nAll images are of size $16 \\times 16$. Angles are not used. No physical units are involved. The output for each case must be a real number representing $S(I,\\sigma)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[s_{1},s_{2},s_{3},s_{4},s_{5}]$). Each $s_{k}$ must be a floating-point number. You may format each number to six decimal places for readability.",
            "solution": "The problem is valid. It presents a well-defined task in numerical sensitivity analysis applied to image processing. All concepts, including Gaussian blurring, convolution via the Discrete Fourier Transform (DFT), and the definition of sensitivity, are scientifically and mathematically sound. The problem is self-contained, providing all necessary parameters, formulas, and test cases for a unique, verifiable solution.\n\nThe objective is to compute the sensitivity $S(I, \\sigma)$ of a Gaussian-blurred image $B_{\\sigma}$ with respect to the Gaussian standard deviation $\\sigma$. The sensitivity is defined as the ratio of the Frobenius norm of the derivative of the blurred image with respect to $\\sigma$ to the Frobenius norm of the blurred image itself:\n$$\nS(I,\\sigma) = \\frac{\\left\\lVert \\frac{\\partial B_{\\sigma}}{\\partial \\sigma} \\right\\rVert_{F}}{\\left\\lVert B_{\\sigma} \\right\\rVert_{F}}\n$$\nwhere $\\lVert\\cdot\\rVert_{F}$ denotes the Frobenius norm. For a matrix $A \\in \\mathbb{R}^{n \\times n}$, the Frobenius norm is $\\lVert A \\rVert_{F} = \\sqrt{\\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} |A_{ij}|^2}$. By convention, if the denominator is zero, $S(I,\\sigma) = 0$.\n\nThe computation proceeds in several steps for each test case, which pair an input image $I \\in \\mathbb{R}^{n \\times n}$ (with $n=16$) with a parameter $\\sigma$.\n\nFirst, we construct the necessary convolution kernels. The blurring is performed with a separable two-dimensional Gaussian kernel $K_{\\sigma}$ defined on a discrete stencil. The kernel is constructed from the one-dimensional Gaussian function:\n$$\ng(x;\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)\n$$\nThe problem specifies a finite stencil with half-width $R=7$, so the discrete coordinates are $x, y \\in \\{-7, -6, \\ldots, 6, 7\\}$. The two-dimensional kernel is the outer product of the one-dimensional function evaluated at these coordinates: $K_{\\sigma}(x,y) = g(x;\\sigma)g(y;\\sigma)$.\n\nTo compute the numerator of the sensitivity expression, we need the derivative of the blurred image, $\\frac{\\partial B_{\\sigma}}{\\partial \\sigma}$. Due to the linearity of convolution and differentiation, this is equivalent to convolving the original image $I$ with the derivative of the kernel, $\\frac{\\partial K_{\\sigma}}{\\partial \\sigma}$. We first find the derivative of the one-dimensional Gaussian function with respect to $\\sigma$:\n$$\n\\frac{\\partial g(x;\\sigma)}{\\partial \\sigma} = g(x;\\sigma)\\left(\\frac{x^{2}}{\\sigma^{3}} - \\frac{1}{\\sigma}\\right)\n$$\nUsing the product rule for differentiation on the separable two-dimensional kernel $K_{\\sigma}(x,y) = g(x;\\sigma)g(y;\\sigma)$, we obtain its derivative:\n$$\n\\frac{\\partial K_{\\sigma}(x,y)}{\\partial \\sigma} = \\frac{\\partial g(x;\\sigma)}{\\partial \\sigma} g(y;\\sigma) + g(x;\\sigma) \\frac{\\partial g(y;\\sigma)}{\\partial \\sigma}\n$$\nBoth $K_{\\sigma}$ and $\\frac{\\partial K_{\\sigma}}{\\partial \\sigma}$ are constructed as $(2R+1) \\times (2R+1) = 15 \\times 15$ matrices.\n\nSecond, we perform the convolutions using the Discrete Fourier Transform (DFT). The convolution theorem states that convolution in the spatial domain is equivalent to pointwise multiplication in the frequency domain. The blurred image $B_{\\sigma}$ and its derivative $\\frac{\\partial B_{\\sigma}}{\\partial \\sigma}$ are thus computed as:\n$$\nB_{\\sigma} = \\mathcal{F}^{-1}\\!\\left(\\mathcal{F}(I)\\,\\odot\\,\\mathcal{F}(K_{\\sigma}^{\\mathrm{pad}})\\right)\n$$\n$$\n\\frac{\\partial B_{\\sigma}}{\\partial \\sigma} = I * \\frac{\\partial K_{\\sigma}}{\\partial \\sigma} = \\mathcal{F}^{-1}\\!\\left(\\mathcal{F}(I)\\,\\odot\\,\\mathcal{F}\\left(\\left[\\frac{\\partial K_{\\sigma}}{\\partial \\sigma}\\right]^{\\mathrm{pad}}\\right)\\right)\n$$\nHere, $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ represent the two-dimensional DFT and its inverse, respectively, and $\\odot$ is pointwise multiplication. The operation requires that the kernels be padded to match the image size of $n \\times n = 16 \\times 16$. The notation $K^{\\mathrm{pad}}$ signifies that the $15 \\times 15$ kernel is placed within a $16 \\times 16$ array of zeros and circularly shifted. This shift aligns the conceptual center of the kernel (at coordinate $(0,0)$ on the stencil) with the zero-frequency component in the DFT domain, which corresponds to the index $(0,0)$ of the array. This is a standard procedure for implementing convolution via DFT.\n\nThe overall algorithm is as follows:\n1.  For each test case, generate the $16 \\times 16$ image matrix $I$ and set the corresponding value of $\\sigma$.\n2.  Define the one-dimensional stencil coordinates from $x=-7$ to $x=7$.\n3.  Calculate the values of the one-dimensional Gaussian $g(x;\\sigma)$ and its derivative $\\frac{\\partial g(x;\\sigma)}{\\partial \\sigma}$ for all points on the stencil.\n4.  Construct the two-dimensional $15 \\times 15$ kernels $K_{\\sigma}$ and $\\frac{\\partial K_{\\sigma}}{\\partial \\sigma}$ using outer products of the one-dimensional arrays.\n5.  Create padded and shifted versions of these kernels, $K_{\\sigma}^{\\mathrm{pad}}$ and $[\\frac{\\partial K_{\\sigma}}{\\partial \\sigma}]^{\\mathrm{pad}}$, of size $16 \\times 16$.\n6.  Compute the two-dimensional DFT of the image $I$ and both padded kernels.\n7.  Perform pointwise multiplication in the frequency domain to obtain the spectra of $B_{\\sigma}$ and $\\frac{\\partial B_{\\sigma}}{\\partial \\sigma}$.\n8.  Compute the inverse two-dimensional DFT of these spectra to obtain the spatial-domain matrices $B_{\\sigma}$ and $\\frac{\\partial B_{\\sigma}}{\\partial \\sigma}$. The real part of the result is taken to discard negligible imaginary components arising from numerical inaccuracies.\n9.  Calculate the Frobenius norms $\\left\\lVert B_{\\sigma} \\right\\rVert_{F}$ and $\\left\\lVert \\frac{\\partial B_{\\sigma}}{\\partial \\sigma} \\right\\rVert_{F}$.\n10. Compute the sensitivity $S(I,\\sigma)$ as their ratio, handling the case where the denominator is zero.\n11. Collate the results from all test cases into the final output format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the sensitivity of a Gaussian-blurred image to the blur parameter sigma.\n    \"\"\"\n\n    def calculate_sensitivity(I, sigma, n, R):\n        \"\"\"\n        Calculates S(I, sigma) for a given image I and parameter sigma.\n        \n        Args:\n            I (np.ndarray): The input image (n x n).\n            sigma (float): The standard deviation of the Gaussian blur.\n            n (int): The dimension of the image.\n            R (int): The half-width of the kernel stencil.\n\n        Returns:\n            float: The computed sensitivity.\n        \"\"\"\n        # 1. Construct Kernels\n        k_size = 2 * R + 1\n        x_coords = np.arange(-R, R + 1, dtype=float)\n        \n        # 1D Gaussian function\n        # Check for sigma being very small to avoid overflow in exp, though not needed for test cases\n        if sigma  1e-9: \n            return 0.0\n        g_1d = (1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-x_coords**2 / (2 * sigma**2))\n        \n        # 1D Gaussian derivative with respect to sigma\n        dg_dsigma_1d = g_1d * (x_coords**2 / sigma**3 - 1 / sigma)\n\n        # 2D kernels from outer products, based on separability\n        K_sigma = np.outer(g_1d, g_1d)\n        dK_dsigma = np.outer(dg_dsigma_1d, g_1d) + np.outer(g_1d, dg_dsigma_1d)\n\n        # 2. Pad and shift kernels for DFT convolution\n        def pad_and_shift(kernel, n_pad):\n            \"\"\"Pads a kernel to size n_pad x n_pad and shifts it for FFT convolution.\"\"\"\n            padded = np.zeros((n_pad, n_pad), dtype=float)\n            padded[:k_size, :k_size] = kernel\n            # Roll to move kernel center (R, R) to (0, 0)\n            return np.roll(padded, (-R, -R), axis=(0, 1))\n\n        K_sigma_pad = pad_and_shift(K_sigma, n)\n        dK_dsigma_pad = pad_and_shift(dK_dsigma, n)\n        \n        # 3. Perform convolution via DFT\n        F_I = np.fft.fft2(I)\n        F_K = np.fft.fft2(K_sigma_pad)\n        F_dK = np.fft.fft2(dK_dsigma_pad)\n        \n        # 4. Get results in the spatial domain via inverse DFT\n        # The result should be real; np.real handles small imaginary parts from numerical error.\n        B_sigma = np.real(np.fft.ifft2(F_I * F_K))\n        dB_dsigma = np.real(np.fft.ifft2(F_I * F_dK))\n        \n        # 5. Calculate norms and sensitivity\n        norm_B = np.linalg.norm(B_sigma)  # Frobenius norm is the default for 2D arrays\n        norm_dB = np.linalg.norm(dB_dsigma)\n        \n        if norm_B == 0:\n            return 0.0\n        else:\n            return norm_dB / norm_B\n\n    # Define problem parameters and test cases\n    n = 16\n    R = 7\n\n    # Case 1: Impulse image\n    I1 = np.zeros((n, n), dtype=float)\n    I1[8, 8] = 1.0\n\n    # Case 2: Ramp image\n    ii, jj = np.ogrid[0:n, 0:n]\n    I2 = (ii + jj) / 30.0\n\n    # Case 3: Constant image\n    I3 = np.ones((n, n), dtype=float)\n\n    # Case 4: Checkerboard image\n    I4 = ((-1)**(ii + jj)).astype(float)\n\n    # Case 5: Random image\n    rng = np.random.default_rng(seed=0)\n    I5 = rng.uniform(0, 1, size=(n, n))\n\n    test_cases = [\n        (I1, 1.0),\n        (I2, 0.5),\n        (I3, 2.0),\n        (I4, 3.0),\n        (I5, 0.1),\n    ]\n\n    results = []\n    for I, sigma in test_cases:\n        sensitivity = calculate_sensitivity(I, sigma, n, R)\n        results.append(sensitivity)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}