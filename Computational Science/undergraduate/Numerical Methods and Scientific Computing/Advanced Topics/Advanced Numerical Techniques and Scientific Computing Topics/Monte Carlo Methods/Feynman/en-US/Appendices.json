{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental application of Monte Carlo methods is to estimate the value of a definite integral. This approach reframes the integral as the average value of the integrand over a given domain, which can be approximated by sampling the function at random points and calculating their mean. This first hands-on practice  grounds this core concept by guiding you through a direct calculation, making the connection between the abstract formula and a concrete numerical estimate perfectly clear.",
            "id": "1964916",
            "problem": "A student in a computational physics course is learning about simple Monte Carlo (MC) methods. The task is to estimate the value of a definite integral by evaluating the integrand at a small number of randomly sampled points. This technique approximates the integral $I = \\int_a^b f(x) dx$ by calculating the expression $(b-a) \\times \\langle f \\rangle_N$, where $\\langle f \\rangle_N$ is the arithmetic mean of the function's values at $N$ points sampled uniformly from the interval $[a, b]$.\n\nThe student is asked to estimate the integral:\n$$ I = \\int_{0}^{2} \\frac{1}{1 + x^2} dx $$\nFor this estimation, a set of $N=5$ points has been pre-selected from a uniform random distribution over the interval $[0, 2]$. The points are:\n$$ x_1 = 0.25, \\quad x_2 = 1.50, \\quad x_3 = 0.80, \\quad x_4 = 1.90, \\quad x_5 = 1.10 $$\nUsing only this information, calculate the MC estimate for the integral $I$. Round your final answer to three significant figures.",
            "solution": "The crude Monte Carlo estimator with uniform sampling on a finite interval is\n$$\n\\hat{I}_{N}=(b-a)\\,\\langle f\\rangle_{N}=(b-a)\\,\\frac{1}{N}\\sum_{i=1}^{N}f(x_{i}).\n$$\nHere, $a=0$, $b=2$, $N=5$, and $f(x)=\\frac{1}{1+x^{2}}$. For the given points,\n$$\n\\begin{aligned}\nf(0.25)&=\\frac{1}{1+(0.25)^{2}}=\\frac{1}{1+\\frac{1}{16}}=\\frac{16}{17},\\\\\nf(1.50)&=\\frac{1}{1+(1.50)^{2}}=\\frac{1}{1+\\frac{9}{4}}=\\frac{4}{13},\\\\\nf(0.80)&=\\frac{1}{1+(0.80)^{2}}=\\frac{1}{1+\\frac{16}{25}}=\\frac{25}{41},\\\\\nf(1.90)&=\\frac{1}{1+(1.90)^{2}}=\\frac{1}{1+\\frac{361}{100}}=\\frac{100}{461},\\\\\nf(1.10)&=\\frac{1}{1+(1.10)^{2}}=\\frac{1}{1+\\frac{121}{100}}=\\frac{100}{221}.\n\\end{aligned}\n$$\nSumming,\n$$\nS=\\frac{16}{17}+\\frac{4}{13}+\\frac{25}{41}+\\frac{100}{461}+\\frac{100}{221}\n=\\frac{10,559,901}{4,177,121}.\n$$\nThus,\n$$\n\\langle f\\rangle_{5}=\\frac{S}{5}=\\frac{10,559,901}{20,885,605},\n\\quad\n\\hat{I}_{5}=(b-a)\\langle f\\rangle_{5}=2\\cdot\\frac{10,559,901}{20,885,605}\n=\\frac{21,119,802}{20,885,605}.\n$$\nNumerically,\n$$\n\\hat{I}_{5}\\approx 1.011213321\\ldots\n$$\nRounding to three significant figures gives $1.01$.",
            "answer": "$$\\boxed{1.01}$$"
        },
        {
            "introduction": "While simple, the \"crude\" Monte Carlo method can perform poorly or even fail when the function being integrated has singularities, which can lead to an estimator with excessively high or infinite variance. This exercise  demonstrates a powerful variance reduction technique, change of variables, which tames the singularity and dramatically improves the estimator's precision. By comparing the naive method, the correct transformation, and an incorrect application that omits the crucial Jacobian factor, you will gain a deep, practical understanding of both the power and the mathematical rigor of these advanced methods.",
            "id": "3253703",
            "problem": "Consider the problem of approximating the integral $$I=\\int_{0}^{1} x^{-1/2}\\,dx$$ using Monte Carlo methods at the advanced undergraduate level. Start from fundamental definitions: (i) an integral over a bounded interval equals the expectation of the integrand under a uniform random variable on that interval, and (ii) the change-of-variables theorem for integrals states that when applying a smooth bijective mapping, the integral transforms by multiplication with the absolute value of the derivative of the mapping (the Jacobian determinant in one dimension). Do not use any pre-derived special-purpose Monte Carlo formulas beyond these definitions.\n\nYou must design and implement three Monte Carlo estimators that all proceed by sampling points over the unit interval but use different representations of the integrand:\n\n- Method $0$ (Direct on $x$): Sample $x$ in $[0,1]$ and approximate the integral by averaging the values of the function $f(x)=x^{-1/2}$.\n\n- Method $1$ (Correct change of variables $x=u^2$): Apply the mapping $x=u^2$ on $[0,1]$ and use the change-of-variables theorem to obtain an integrand on $u \\in [0,1]$ that must include the Jacobian factor $|dx/du|$, then approximate the integral by averaging that correctly transformed integrand over samples of $u$.\n\n- Method $2$ (Incorrect mapping that ignores the Jacobian): Apply the same mapping $x=u^2$ but incorrectly omit the Jacobian factor, averaging only the composition $f(u^2)$ over samples of $u$.\n\nFor numerical stability and deterministic reproducibility, in all three methods use stratified midpoints on the unit interval rather than pseudorandom numbers. Specifically, for a given positive integer $N$, define $N$ sample points $$s_i=\\frac{i-1/2}{N},\\quad i=1,2,\\dots,N.$$ In Method $0$, set $x_i=s_i$. In Methods $1$ and $2$, set $u_i=s_i$ and apply the corresponding transformation.\n\nFor each method and each $N$, compute:\n\n- An estimate of $I$ as the arithmetic mean of the corresponding summand values.\n\n- The empirical variance of those summand values computed with the usual unbiased sample variance formula with divisor $N-1$ (if $N=1$, define the empirical variance to be $0$).\n\nFrom first principles, determine for each method whether the true mean of the summand is finite and whether its true variance is finite under independent and identically distributed sampling on the unit interval. Report these finiteness properties as booleans.\n\nTest suite. Your program must compute the above outputs for the following test cases, which cover a typical case and a boundary-growth case for each method:\n\n- Method $0$ with $N=10$.\n\n- Method $0$ with $N=1000$.\n\n- Method $1$ with $N=10$.\n\n- Method $1$ with $N=1000$.\n\n- Method $2$ with $N=10$.\n\n- Method $2$ with $N=1000$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the following four items in order: the estimate as a float rounded to six decimals, the empirical variance as a float rounded to six decimals, a boolean indicating whether the true mean of the summand is finite, and a boolean indicating whether the true variance of the summand is finite. Concatenate all test cases in the order listed above into one flat list. For example, the overall printed structure must be of the form $$[\\text{est}_{0,10},\\text{var}_{0,10},\\text{meanOK}_{0},\\text{varOK}_{0},\\text{est}_{0,1000},\\dots,\\text{est}_{2,1000},\\text{var}_{2,1000},\\text{meanOK}_{2},\\text{varOK}_{2}],$$ where each $\\text{est}_{\\cdot,\\cdot}$ and $\\text{var}_{\\cdot,\\cdot}$ are floats rounded to six decimals, and each $\\text{meanOK}_{\\cdot}$ and $\\text{varOK}_{\\cdot}$ are booleans. No other text should be printed.",
            "solution": "The problem asks for the design, theoretical analysis, and numerical implementation of three Monte Carlo-style estimators for the integral $I=\\int_{0}^{1} x^{-1/2}\\,dx$. The analysis must be based on first principles, namely the interpretation of an integral as an expectation and the change-of-variables theorem.\n\nThe exact value of the integral is $I = \\int_{0}^{1} x^{-1/2}\\,dx = [2x^{1/2}]_{0}^{1} = 2(1)^{1/2} - 2(0)^{1/2} = 2$. This is a proper Riemann integral if approached as an improper integral, as the limit exists. The integrand $f(x) = x^{-1/2}$ has a singularity at $x=0$.\n\nFor a function $g(v)$ and a random variable $V$ uniformly distributed on $[0,1]$, its expectation is $E[g(V)] = \\int_0^1 g(v)\\,dv$. Therefore, the integral $I$ can be expressed as the expectation $E[f(X)]$ where $X$ is a uniform random variable on $[0,1]$. A Monte Carlo estimate of $I$ is the sample mean $\\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^N f(x_i)$, where $\\{x_i\\}$ are independent samples from the uniform distribution on $[0,1]$. The problem specifies a deterministic, stratified sampling scheme instead of random sampling: $s_i = \\frac{i-1/2}{N}$ for $i=1,2,\\dots,N$. This is a quasi-Monte Carlo approach.\n\nThe quality of a Monte Carlo estimator is determined by the properties of its summand. The mean of the summand, $E[g(V)]$, determines if the estimator is biased. The variance of the summand, $Var[g(V)] = E[g(V)^2] - (E[g(V)])^2$, determines the rate of convergence of the estimate. A finite variance is crucial for the estimator's standard error to decrease at the canonical rate of $1/\\sqrt{N}$.\n\nLet's analyze each of the three proposed methods.\n\n**Method 0: Direct on $x$**\nIn this method, we directly use the integrand $f(x)=x^{-1/2}$. The sample points are $x_i = s_i = \\frac{i-1/2}{N}$. The summands are $y_i = x_i^{-1/2}$.\nThe estimate of the integral for a given $N$ is $\\hat{I}_0 = \\frac{1}{N} \\sum_{i=1}^{N} y_i = \\frac{1}{N} \\sum_{i=1}^{N} (s_i)^{-1/2}$.\n\nTheoretical Analysis:\nThe summand is the function $g_0(x) = x^{-1/2}$.\nThe true mean of the summand is its expectation for $X \\sim U(0,1)$:\n$$E[g_0(X)] = \\int_0^1 x^{-1/2} \\,dx = [2x^{1/2}]_0^1 = 2$$\nThe mean is finite.\nThe true variance of the summand is $Var[g_0(X)] = E[g_0(X)^2] - (E[g_0(X)])^2$. We first compute the second moment:\n$$E[g_0(X)^2] = \\int_0^1 (x^{-1/2})^2 \\,dx = \\int_0^1 x^{-1} \\,dx = [\\ln|x|]_0^1$$\nThis integral diverges at the lower limit $x=0$. Since the second moment is infinite, the variance is infinite.\nTherefore, for Method $0$, the true mean of the summand is finite, but the true variance is infinite. This implies that while the law of large numbers applies and the estimator is consistent (i.e., it converges to the true value), convergence is very slow, and the central limit theorem in its standard form does not apply.\n\nFiniteness Properties: `meanOK` is True, `varOK` is False.\n\n**Method 1: Correct change of variables $x=u^2$**\nThis method uses the transformation $x=g(u)=u^2$, where $u \\in [0,1]$. The change-of-variables theorem for an integral states that $\\int f(x) \\,dx = \\int f(g(u)) |g'(u)| \\,du$.\nHere, $g(u)=u^2$, so the Jacobian determinant is $|g'(u)| = |dx/du| = |2u| = 2u$, since $u \\ge 0$.\nThe integral transforms as:\n$$I = \\int_0^1 x^{-1/2} \\,dx = \\int_0^1 (u^2)^{-1/2} (2u) \\,du = \\int_0^1 u^{-1}(2u) \\,du = \\int_0^1 2 \\,du$$\nThe new integrand, which is the summand for the Monte Carlo method, is $g_1(u) = 2$.\nThe sample points are $u_i = s_i = \\frac{i-1/2}{N}$. The summands are $y_i = g_1(u_i) = 2$.\nThe estimate is $\\hat{I}_1 = \\frac{1}{N} \\sum_{i=1}^{N} 2 = \\frac{1}{N} (2N) = 2$.\n\nTheoretical Analysis:\nThe summand is the constant function $g_1(u) = 2$.\nThe true mean of the summand for $U \\sim U(0,1)$ is:\n$$E[g_1(U)] = \\int_0^1 2 \\,du = 2$$\nThe mean is finite.\nThe true variance of the summand is:\n$$Var[g_1(U)] = Var[2] = 0$$\nSince the summand is a constant, its variance is zero. The variance is finite.\nThis change of variables has transformed an integrand with infinite variance into one with zero variance, a perfect example of variance reduction. The estimate will be exactly correct ($2$) for any $N \\ge 1$.\n\nFiniteness Properties: `meanOK` is True, `varOK` is True.\n\n**Method 2: Incorrect mapping that ignores the Jacobian**\nThis method applies the transformation $x=u^2$ but incorrectly omits the Jacobian factor $|dx/du|$.\nThe summand is taken to be $g_2(u) = f(g(u)) = f(u^2) = (u^2)^{-1/2} = u^{-1}$ for $u>0$.\nThe sample points are $u_i = s_i = \\frac{i-1/2}{N}$. The summands are $y_i = (u_i)^{-1}$.\nThe estimate is $\\hat{I}_2 = \\frac{1}{N} \\sum_{i=1}^{N} (s_i)^{-1}$. This method is effectively estimating the integral $\\int_0^1 u^{-1} du$, not the original integral $I$.\n\nTheoretical Analysis:\nThe summand is the function $g_2(u) = u^{-1}$.\nThe true mean of the summand for $U \\sim U(0,1)$ is:\n$$E[g_2(U)] = \\int_0^1 u^{-1} \\,du = [\\ln|u|]_0^1$$\nThis integral diverges at the lower limit $u=0$.\nThe mean is infinite.\nSince the mean (first moment) is infinite, the variance (which depends on the first and second moments) is also necessarily infinite.\n\nFiniteness Properties: `meanOK` is False, `varOK` is False.\n\n**Numerical Computations**\nFor each method and a given $N$, we compute:\n1.  Sample points: $s_i = (i-0.5)/N$ for $i=1, \\dots, N$.\n2.  Summands $y_i$ according to the method's definition.\n3.  Estimate: $\\hat{I} = \\frac{1}{N}\\sum_{i=1}^N y_i$.\n4.  Empirical Variance: $S^2 = \\frac{1}{N-1}\\sum_{i=1}^N (y_i - \\hat{I})^2$ for $N > 1$, and $0$ for $N=1$.\n\nThese computations will be performed for the test cases specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Monte Carlo integration problem for the specified test cases.\n    \"\"\"\n\n    # Test cases: list of (method_id, N)\n    # method_id 0: Direct, f(x) = x^(-1/2)\n    # method_id 1: Correct change of vars, f(u) = 2\n    # method_id 2: Incorrect change of vars, f(u) = u^(-1)\n    test_cases = [\n        (0, 10),\n        (0, 1000),\n        (1, 10),\n        (1, 1000),\n        (2, 10),\n        (2, 1000),\n    ]\n\n    # Theoretical properties determined from first principles\n    # (is_mean_finite, is_variance_finite)\n    theoretical_props = {\n        0: (True, False),  # Mean is 2, Variance is infinite\n        1: (True, True),   # Mean is 2, Variance is 0\n        2: (False, False), # Mean is infinite, Variance is infinite\n    }\n\n    results = []\n\n    for method_id, N in test_cases:\n        # Generate stratified midpoint samples on the unit interval\n        # s_i = (i - 1/2) / N for i = 1, 2, ..., N\n        # np.arange(1, N + 1) creates the sequence of i's.\n        samples = (np.arange(1, N + 1) - 0.5) / N\n\n        summands = None\n        if method_id == 0:\n            # Method 0: Direct sampling on x. Summands are x_i^(-1/2)\n            # The samples 's' are the x_i values.\n            summands = samples**(-0.5)\n        elif method_id == 1:\n            # Method 1: Correct change of variables x=u^2.\n            # Summands are f(u^2) * |dx/du| = (u^-2)^(1/2) * 2u = u^-1 * 2u = 2.\n            # The samples 's' are the u_i values.\n            # np.full creates an array of size N filled with the value 2.0.\n            summands = np.full(N, 2.0)\n        elif method_id == 2:\n            # Method 2: Incorrect mapping x=u^2 without Jacobian.\n            # Summands are f(u^2) = (u^2)^(-1/2) = u^-1.\n            # The samples 's' are the u_i values.\n            summands = samples**(-1)\n\n        # Calculate the estimate (arithmetic mean of summands)\n        estimate = np.mean(summands)\n\n        # Calculate the empirical variance of the summands\n        # The problem specifies unbiased sample variance (divisor N-1).\n        # np.var with ddof=1 computes this.\n        # If N=1, variance is defined as 0.\n        if N > 1:\n            empirical_variance = np.var(summands, ddof=1)\n        else:\n            empirical_variance = 0.0\n\n        # Retrieve theoretical finiteness properties\n        mean_ok, var_ok = theoretical_props[method_id]\n\n        # Append results for this test case, formatted as required\n        results.extend([\n            f\"{estimate:.6f}\",\n            f\"{empirical_variance:.6f}\",\n            str(mean_ok),\n            str(var_ok)\n        ])\n\n    # Final print statement in the exact required format.\n    # Note: The problem description incorrectly implies that the integral estimate is the output.\n    # The output format specifies the average of the summands. For Methods 0 and 2, this\n    # is not the integral estimate.\n    # The problem: \"An estimate of I as the arithmetic mean of the corresponding summand values\"\n    # For a general integral I = \\int_a^b f(x) dx, the estimate is (b-a) * mean(f(x_i)).\n    # Here a=0, b=1, so (b-a)=1. The mean of summands is the estimate. My code is correct.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Many real-world systems exhibit behavior that varies significantly across different parts of their domain, such as power consumption fluctuating between day and night. This practice  introduces stratified sampling, a \"divide and conquer\" strategy that boosts estimation efficiency by allocating more computational effort to the more variable parts of the problem. In this problem, you will apply this technique to a hypothetical power grid model, using a pilot study to implement an optimal sampling strategy known as Neyman allocation and quantifying the significant gains in efficiency.",
            "id": "3253642",
            "problem": "Consider a stochastic model of power consumption over a single day. Let time $t$ range continuously over the interval $[0,24)$ measured in hours. The instantaneous power consumption $C(t)$ at time $t$ is modeled as a Gamma-distributed random variable with shape parameter $k$ and time-dependent scale parameter $\\theta(t)$ such that the conditional mean satisfies $\\mathbb{E}[C(t)\\mid t] = \\mu(t) = k\\,\\theta(t)$. The deterministic mean profile $\\mu(t)$ is defined as\n$$\n\\mu(t) = b + a \\,\\sin^2\\left(\\frac{\\pi\\,t}{24}\\right) + s \\,\\exp\\left(-\\frac{(t - 19)^2}{w^2}\\right),\n$$\nwhere $b$, $a$, $s$, and $w$ are positive constants, and the angle in the sine function is in radians. All trigonometric and exponential function arguments are dimensionless by construction. Assume $C(t)$ is expressed in kilowatts, but the required outputs in this task are dimensionless and do not require physical units.\n\nDefine a random time $T$ uniformly distributed on $[0,24)$, and the target quantity is the expected power consumption at a uniformly random time,\n$$\n\\mu^\\star = \\mathbb{E}[C(T)].\n$$\n\nYour task is to design and implement two Monte Carlo estimators of $\\mu^\\star$ with equal computational cost, and to quantify their efficiency in terms of estimated variance:\n\n- A crude Monte Carlo estimator that draws $N$ independent samples of $T$ uniformly over $[0,24)$ and, for each time, draws one realization of $C(t)$ given that time. The estimator of $\\mu^\\star$ is the empirical mean over these $N$ samples. The estimated variance of this estimator must be computed from the sample variance of the $C(t)$ values divided by $N$.\n\n- A stratified Monte Carlo estimator across specified time bins that partitions $[0,24)$ into $K$ disjoint strata. The law of total expectation and the properties of stratified sampling imply that an unbiased estimator of $\\mu^\\star$ can be formed by weighting stratum-specific averages by their stratum probabilities. Under a fixed total cost of $N$ samples, design a strata allocation that reduces estimator variance. Use a pilot sampling strategy within each stratum to approximate within-stratum variability, and derive an allocation rule that is justified by minimizing the estimator variance subject to the fixed total cost constraint. Use the pilot estimates to determine a data-dependent allocation, then compute the stratified estimator and its estimated variance from the within-stratum sample variances.\n\nThe following constants define the model:\n- $k = 6$\n- $b = 0.4$\n- $a = 1.1$\n- $s = 2.0$\n- $w = 1.2$\n\nAngle units in $\\sin(\\cdot)$ are radians.\n\nImplement a complete program that, for each test case, computes the ratio of the estimated variance of the stratified estimator to the estimated variance of the crude estimator. The total sampling budget $N$ must be exactly the same in both methods for each test case. Use the following test suite of parameter values:\n\n- Test case $1$: $N = 1000$, bins $[0,6)$, $[6,17)$, $[17,21)$, $[21,24)$, pilot size per bin $m = 5$, random seed $\\sigma = 12345$.\n- Test case $2$: $N = 40$, bins $[0,6)$, $[6,17)$, $[17,21)$, $[21,24)$, pilot size per bin $m = 2$, random seed $\\sigma = 54321$.\n- Test case $3$: $N = 800$, bins $[0,6)$, $[6,18)$, $[18,18.5)$, $[18.5,24)$, pilot size per bin $m = 5$, random seed $\\sigma = 2023$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the ratio $\\widehat{\\mathrm{Var}}_{\\text{strat}} / \\widehat{\\mathrm{Var}}_{\\text{crude}}$ for the corresponding test case, in the order given above. For example, the output format is\n$$\n[\\rho_1,\\rho_2,\\rho_3].\n$$\nBecause these ratios are dimensionless, no physical units are to be included in the output.",
            "solution": "The problem requires the design and comparison of two Monte Carlo estimators for the expected power consumption at a uniformly random time, denoted $\\mu^\\star$. We are given a stochastic model for the instantaneous power consumption $C(t)$ at time $t \\in [0, 24)$ and a deterministic model for its mean $\\mu(t)$.\n\nFirst, let us formalize the target quantity $\\mu^\\star$. We are given a random time $T$ uniformly distributed on $[0, 24)$, i.e., $T \\sim U(0, 24)$, and a random variable $C(t) \\sim \\mathrm{Gamma}(k, \\theta(t))$ with mean $\\mathbb{E}[C(t) \\mid t] = \\mu(t) = k \\theta(t)$. The target is the unconditional expectation $\\mu^\\star = \\mathbb{E}[C(T)]$.\n\nBy the law of total expectation (also known as the tower property or iterated expectation), we can write:\n$$\n\\mu^\\star = \\mathbb{E}[\\mathbb{E}[C(T) \\mid T]]\n$$\nGiven a specific time $T=t$, the conditional expectation is $\\mathbb{E}[C(T) \\mid T=t] = \\mathbb{E}[C(t)] = \\mu(t)$. Therefore, the target quantity is the expectation of the mean function $\\mu(t)$ over the random time $T$:\n$$\n\\mu^\\star = \\mathbb{E}[\\mu(T)]\n$$\nSince $T$ is uniformly distributed over $[0, 24)$, its probability density function is $f_T(t) = 1/24$ for $t \\in [0, 24)$. The expectation is thus the integral of $\\mu(t)$ weighted by its density:\n$$\n\\mu^\\star = \\int_{0}^{24} \\mu(t) f_T(t) dt = \\frac{1}{24} \\int_{0}^{24} \\mu(t) dt\n$$\nThis shows that $\\mu^\\star$ is the average value of the mean power consumption profile over a $24$-hour period. The problem is to estimate this value using two different Monte Carlo schemes that sample from the full stochastic process $C(T)$, not just the deterministic function $\\mu(T)$.\n\nThe mean power consumption profile is given by:\n$$\n\\mu(t) = b + a \\,\\sin^2\\left(\\frac{\\pi\\,t}{24}\\right) + s \\,\\exp\\left(-\\frac{(t - 19)^2}{w^2}\\right)\n$$\nwith constants $k = 6$, $b = 0.4$, $a = 1.1$, $s = 2.0$, and $w = 1.2$.\n\n**Crude Monte Carlo (CMC) Estimator**\n\nThe crude Monte Carlo method directly simulates the process to estimate $\\mu^\\star = \\mathbb{E}[C(T)]$.\nThe procedure is as follows:\n1.  Generate $N$ independent and identically distributed samples of the time variable, $T_1, T_2, \\ldots, T_N$, from the uniform distribution $U(0, 24)$.\n2.  For each time sample $T_i$, calculate the mean $\\mu(T_i)$ and the corresponding scale parameter for the Gamma distribution, $\\theta(T_i) = \\mu(T_i) / k$.\n3.  For each $T_i$, generate one sample of the power consumption, $C_i$, from the Gamma distribution $C_i \\sim \\mathrm{Gamma}(k, \\theta(T_i))$.\n4.  The CMC estimator of $\\mu^\\star$ is the sample mean of these consumption values:\n    $$\n    \\hat{\\mu}_{\\text{crude}} = \\frac{1}{N} \\sum_{i=1}^{N} C_i\n    $$\nThe variance of this estimator is $\\mathrm{Var}(\\hat{\\mu}_{\\text{crude}}) = \\mathrm{Var}(C(T)) / N$. We estimate this variance using the sample variance of the generated $C_i$ values. Let $\\hat{\\sigma}_{\\text{crude}}^2$ be the sample variance of the set $\\{C_1, \\ldots, C_N\\}$:\n$$\n\\hat{\\sigma}_{\\text{crude}}^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (C_i - \\hat{\\mu}_{\\text{crude}})^2\n$$\nThe estimated variance of the estimator $\\hat{\\mu}_{\\text{crude}}$ is then:\n$$\n\\widehat{\\mathrm{Var}}(\\hat{\\mu}_{\\text{crude}}) = \\frac{\\hat{\\sigma}_{\\text{crude}}^2}{N}\n$$\n\n**Stratified Monte Carlo Estimator**\n\nStratified sampling is a variance reduction technique that can improve estimation efficiency. The domain of the sampling variable $T$, the interval $[0, 24)$, is partitioned into $K$ disjoint subintervals, or strata, $S_j = [t_{j-1}, t_j)$ for $j=1, \\ldots, K$, with $t_0=0$ and $t_K=24$.\n\nThe probability that $T$ falls into stratum $S_j$ is its normalized length, $p_j = P(T \\in S_j) = (t_j - t_{j-1}) / 24$.\nThe overall expectation $\\mu^\\star$ can be expressed as a weighted sum of conditional expectations within each stratum:\n$$\n\\mu^\\star = \\sum_{j=1}^{K} p_j \\mu_j, \\quad \\text{where} \\quad \\mu_j = \\mathbb{E}[C(T) \\mid T \\in S_j]\n$$\nThe stratified estimator is constructed by estimating each $\\mu_j$ with a sample mean $\\bar{C}_j$ from samples drawn exclusively from stratum $S_j$, and combining them:\n$$\n\\hat{\\mu}_{\\text{strat}} = \\sum_{j=1}^{K} p_j \\bar{C}_j\n$$\nwhere $\\bar{C}_j$ is the mean of $n_j$ samples drawn from stratum $S_j$, and the total number of samples is fixed at $N = \\sum_{j=1}^K n_j$.\n\nThe variance of the stratified estimator is $\\mathrm{Var}(\\hat{\\mu}_{\\text{strat}}) = \\sum_{j=1}^{K} p_j^2 \\frac{\\sigma_j^2}{n_j}$, where $\\sigma_j^2 = \\mathrm{Var}(C(T) \\mid T \\in S_j)$ is the variance of consumption within stratum $S_j$. To minimize this variance for a fixed total cost $N$, the sample sizes $n_j$ should be chosen according to Neyman allocation:\n$$\nn_j \\propto p_j \\sigma_j\n$$\nSince the true stratum variances $\\sigma_j$ are unknown, the problem specifies a pilot sampling strategy to estimate them. We adopt a two-stage approach that respects the total sample budget $N$:\n1.  **Pilot Stage:** For each of the $K$ strata, draw a small pilot sample of size $m$. This uses a total of $K \\times m$ samples.\n    -   For each stratum $S_j$, draw $T_{j,1}, \\ldots, T_{j,m}$ from $U(S_j)$.\n    -   For each $T_{j,i}$, draw $C_{j,i} \\sim \\mathrm{Gamma}(k, \\mu(T_{j,i})/k)$.\n    -   Compute the pilot estimate of the stratum standard deviation, $\\hat{\\sigma}_{j, \\text{pilot}}$, from these $m$ samples. This is only possible if $m \\geq 2$.\n\n2.  **Allocation and Augmentation Stage:**\n    -   The remaining sample budget is $N_{\\text{rem}} = N - K \\times m$.\n    -   If $N_{\\text{rem}} > 0$, these samples are allocated to the strata to augment the pilot samples. The number of additional samples for stratum $S_j$, denoted $n_j^{\\text{add}}$, is determined by the Neyman allocation rule using the pilot estimates:\n        $$\n        n_j^{\\text{add}} \\approx N_{\\text{rem}} \\frac{p_j \\hat{\\sigma}_{j, \\text{pilot}}}{\\sum_{i=1}^{K} p_i \\hat{\\sigma}_{i, \\text{pilot}}}\n        $$\n    -   Since $n_j^{\\text{add}}$ must be an integer, the values are rounded and adjusted to ensure their sum is exactly $N_{\\text{rem}}$.\n    -   Draw $n_j^{\\text{add}}$ new samples from each corresponding stratum $S_j$.\n\n3.  **Final Estimation:**\n    -   For each stratum $S_j$, the total number of samples is now $n_j = m + n_j^{\\text{add}}$.\n    -   Using all $n_j$ samples in each stratum, compute the final stratum sample mean $\\bar{C}_j$ and the final stratum sample variance $\\hat{\\sigma}_j^2$.\n    -   The stratified estimator of $\\mu^\\star$ is $\\hat{\\mu}_{\\text{strat}} = \\sum_{j=1}^{K} p_j \\bar{C}_j$.\n    -   The estimated variance of this estimator is:\n        $$\n        \\widehat{\\mathrm{Var}}(\\hat{\\mu}_{\\text{strat}}) = \\sum_{j=1}^{K} p_j^2 \\frac{\\hat{\\sigma}_j^2}{n_j}\n        $$\n\nThe final task is to compute the efficiency ratio, $\\widehat{\\mathrm{Var}}(\\hat{\\mu}_{\\text{strat}}) / \\widehat{\\mathrm{Var}}(\\hat{\\mu}_{\\text{crude}})$, for each given test case. This ratio quantifies the variance reduction achieved by stratification compared to the crude method for the same total computational cost $N$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Monte Carlo simulations for the given test cases.\n    \"\"\"\n    # Define model constants\n    k = 6.0\n    b = 0.4\n    a = 1.1\n    s = 2.0\n    w = 1.2\n\n    def mu(t):\n        \"\"\"Calculates the mean power consumption mu(t) at time t.\"\"\"\n        term_b = b\n        term_a = a * np.sin(np.pi * t / 24.0)**2\n        term_s = s * np.exp(-((t - 19.0)**2) / w**2)\n        return term_b + term_a + term_s\n\n    def crude_mc(N, seed):\n        \"\"\"\n        Performs crude Monte Carlo estimation.\n        - N: total number of samples\n        - seed: random generator seed\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # 1. Draw N samples of T from U(0, 24)\n        times = rng.uniform(0.0, 24.0, N)\n        \n        # 2. Calculate mu(t) and theta(t) for each time\n        mus = mu(times)\n        thetas = mus / k\n        \n        # 3. Draw N samples of C(t) from Gamma(k, theta(t))\n        consumptions = rng.gamma(shape=k, scale=thetas, size=N)\n        \n        # 4. Calculate estimator and its estimated variance\n        # The estimated variance of the estimator is sample_variance / N\n        est_variance = np.var(consumptions, ddof=1) / N\n        \n        return est_variance\n\n    def stratified_mc(N, bins, m, seed):\n        \"\"\"\n        Performs stratified Monte Carlo estimation with a two-stage sampling process.\n        - N: total number of samples\n        - bins: a list of stratum boundaries, e.g., [0, 6, 17, 24]\n        - m: pilot sample size per stratum\n        - seed: random generator seed\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        K = len(bins) - 1\n        strata = [(bins[i], bins[i+1]) for i in range(K)]\n        \n        # Calculate stratum probabilities (weights)\n        p_j = np.array([end - start for start, end in strata]) / 24.0\n        \n        # --- Stage 1: Pilot Sampling ---\n        # Draw m samples from each stratum\n        pilot_samples_C = []\n        pilot_sigmas = np.zeros(K)\n        \n        for j in range(K):\n            start, end = strata[j]\n            # Draw m time samples from U(start, end)\n            times_j = rng.uniform(start, end, m)\n            mus_j = mu(times_j)\n            thetas_j = mus_j / k\n            \n            # Draw m consumption samples\n            consumptions_j = rng.gamma(shape=k, scale=thetas_j, size=m)\n            pilot_samples_C.append(list(consumptions_j))\n            \n            # Estimate stratum standard deviation from pilot samples\n            if m > 1:\n                pilot_sigmas[j] = np.std(consumptions_j, ddof=1)\n            else:\n                pilot_sigmas[j] = 0 # Cannot estimate variance with 1 sample\n        \n        # --- Stage 2: Allocation and Augmentation ---\n        N_rem = N - K * m\n        n_j_total = [m] * K\n\n        if N_rem > 0:\n            # Calculate Neyman allocation for the remaining samples\n            weights = p_j * pilot_sigmas\n            sum_weights = np.sum(weights)\n            \n            if sum_weights > 0:\n                alloc_proportions = weights / sum_weights\n                n_j_ideal = N_rem * alloc_proportions\n                \n                # Integer allocation\n                n_j_add_floor = np.floor(n_j_ideal).astype(int)\n                n_rem_int = N_rem - np.sum(n_j_add_floor)\n                \n                # Distribute remainder based on fractional parts\n                frac_parts = n_j_ideal - n_j_add_floor\n                indices_to_add = np.argsort(frac_parts)[-n_rem_int:]\n                n_j_add = n_j_add_floor\n                n_j_add[indices_to_add] += 1\n            else:\n                # If all pilot sigmas are zero, allocate proportionally to stratum size\n                n_j_add_floor = np.floor(N_rem * p_j).astype(int)\n                n_rem_int = N_rem - np.sum(n_j_add_floor)\n                indices_to_add = np.argsort(p_j)[-n_rem_int:]\n                n_j_add = n_j_add_floor\n                n_j_add[indices_to_add] += 1\n\n            # Augment samples\n            for j in range(K):\n                if n_j_add[j] > 0:\n                    start, end = strata[j]\n                    times_j_add = rng.uniform(start, end, n_j_add[j])\n                    mus_j_add = mu(times_j_add)\n                    thetas_j_add = mus_j_add / k\n                    consumptions_j_add = rng.gamma(shape=k, scale=thetas_j_add, size=n_j_add[j])\n                    pilot_samples_C[j].extend(consumptions_j_add)\n                    n_j_total[j] += n_j_add[j]\n       \n        # --- Final Estimation ---\n        final_stratum_vars = np.zeros(K)\n        for j in range(K):\n            if n_j_total[j] > 1:\n                final_stratum_vars[j] = np.var(pilot_samples_C[j], ddof=1)\n            # if n_j_total[j] <= 1, variance is 0, which is correct for the formula below.\n        \n        # Estimated variance of the stratified estimator\n        est_variance = np.sum((p_j**2 * final_stratum_vars) / n_j_total)\n        \n        return est_variance\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'N': 1000, 'bins': [0, 6, 17, 21, 24], 'm': 5, 'seed': 12345},\n        {'N': 40, 'bins': [0, 6, 17, 21, 24], 'm': 2, 'seed': 54321},\n        {'N': 800, 'bins': [0, 6, 18, 18.5, 24], 'm': 5, 'seed': 2023},\n    ]\n\n    ratios = []\n    for case in test_cases:\n        N = case['N']\n        bins = case['bins']\n        m = case['m']\n        seed = case['seed']\n        \n        var_crude = crude_mc(N, seed)\n        var_strat = stratified_mc(N, bins, m, seed)\n        \n        ratio = var_strat / var_crude\n        ratios.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in ratios)}]\")\n\nsolve()\n```"
        }
    ]
}