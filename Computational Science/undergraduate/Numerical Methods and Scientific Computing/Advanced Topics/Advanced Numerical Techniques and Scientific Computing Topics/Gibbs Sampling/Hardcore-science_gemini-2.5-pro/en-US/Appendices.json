{
    "hands_on_practices": [
        {
            "introduction": "Before we can run a Gibbs sampler, we must first derive the full conditional distributions for each variable, which serve as the engine of the algorithm. This exercise  provides practice in this essential first step, starting from a joint probability distribution common in statistical physics and machine learning. Mastering this analytical skill is fundamental to setting up a Gibbs sampler for custom probabilistic models where the conditionals are not immediately obvious.",
            "id": "1363738",
            "problem": "Consider a simplified probabilistic model for a system with two interacting components, represented by binary random variables $X$ and $Y$. Each variable can take a value in the set $\\{0, 1\\}$. The joint probability mass function of the system is described by an energy-based model, which is common in statistical physics and machine learning:\n$$P(X=x, Y=y) = \\frac{1}{Z} \\exp\\left(\\alpha xy + \\beta(x+y)\\right)$$\nIn this model, $\\alpha$ and $\\beta$ are positive, real-valued parameters that represent the strength of the interaction between the components and the influence of an external field, respectively. The term $Z$ is the normalization constant, also known as the partition function, which is the sum of $\\exp\\left(\\alpha xy + \\beta(x+y)\\right)$ over all possible states $(x, y)$ to ensure the probabilities sum to one.\n\nGibbs sampling is an algorithm used to draw samples from such a distribution by iteratively sampling from its conditional distributions. A crucial step in this process is the ability to compute these conditional probabilities.\n\nYour task is to derive an expression for the conditional probability of the component $X$ being in state 1, given that the component $Y$ is in state $y$. Derive a closed-form expression for $P(X=1 | Y=y)$, where $y \\in \\{0, 1\\}$. The expression should be given in terms of the parameters $\\alpha$, $\\beta$, and the variable $y$.",
            "solution": "We are given the joint distribution\n$$P(X=x, Y=y) = \\frac{1}{Z} \\exp\\left(\\alpha x y + \\beta(x+y)\\right),$$\nwith $x,y \\in \\{0,1\\}$. The conditional probability $P(X=1 \\mid Y=y)$ is defined by\n$$P(X=1 \\mid Y=y) = \\frac{P(X=1, Y=y)}{\\sum_{x \\in \\{0,1\\}} P(X=x, Y=y)}.$$\nSubstituting the given joint mass function and noting that the normalization constant $Z$ cancels between numerator and denominator, we obtain\n$$P(X=1 \\mid Y=y) = \\frac{\\exp\\left(\\alpha \\cdot 1 \\cdot y + \\beta(1+y)\\right)}{\\exp\\left(\\alpha \\cdot 1 \\cdot y + \\beta(1+y)\\right) + \\exp\\left(\\alpha \\cdot 0 \\cdot y + \\beta(0+y)\\right)}.$$\nCompute each exponential:\n- Numerator: $\\exp\\left(\\alpha y + \\beta + \\beta y\\right)$.\n- Denominator terms: $\\exp\\left(\\alpha y + \\beta + \\beta y\\right)$ and $\\exp\\left(\\beta y\\right)$.\nThus,\n$$P(X=1 \\mid Y=y) = \\frac{\\exp\\left(\\alpha y + \\beta + \\beta y\\right)}{\\exp\\left(\\alpha y + \\beta + \\beta y\\right) + \\exp\\left(\\beta y\\right)}.$$\nFactor $\\exp(\\beta y)$ from numerator and denominator:\n$$P(X=1 \\mid Y=y) = \\frac{\\exp(\\beta y)\\exp\\left(\\alpha y + \\beta\\right)}{\\exp(\\beta y)\\left[\\exp\\left(\\alpha y + \\beta\\right) + 1\\right]}.$$\nCancel the common factor $\\exp(\\beta y)$ to get\n$$P(X=1 \\mid Y=y) = \\frac{\\exp\\left(\\alpha y + \\beta\\right)}{1 + \\exp\\left(\\alpha y + \\beta\\right)} = \\frac{1}{1 + \\exp\\left(-\\alpha y - \\beta\\right)}.$$\nThis gives the required closed-form expression in terms of $\\alpha$, $\\beta$, and $y \\in \\{0,1\\}$.",
            "answer": "$$\\boxed{\\frac{1}{1+\\exp\\left(-\\alpha y-\\beta\\right)}}$$"
        },
        {
            "introduction": "With the conditional distributions defined, the core of the Gibbs sampler is an iterative process of drawing samples from them in sequence. This problem  guides you through a single, complete iteration of the algorithm for a bivariate system involving both a continuous and a discrete variable. By manually performing the sampling steps using the inverse transform method, you will gain a concrete, hands-on understanding of the sampler's mechanics in action.",
            "id": "1920320",
            "problem": "Consider a two-dimensional random vector $(X, Y)$ whose joint probability distribution is defined by the following full conditional distributions:\n- The conditional distribution of $X$ given $Y=y$ is an Exponential distribution with a rate parameter of $y$. The probability density function is given by $p(x|y) = y \\exp(-yx)$ for $x > 0$.\n- The conditional distribution of $Y$ given $X=x$ is a Poisson distribution with a mean parameter of $x$. The probability mass function is given by $p(y=k|x) = \\frac{x^k \\exp(-x)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nYou are tasked with performing one full iteration of a Gibbs sampler. Starting from the initial state $(x^{(0)}, y^{(0)}) = (2, 3)$, you will generate a new state $(x^{(1)}, y^{(1)})$. The procedure for the iteration is as follows: first, draw a sample for $x^{(1)}$ from the distribution $p(x|y^{(0)})$, and then, using this new value $x^{(1)}$, draw a sample for $y^{(1)}$ from the distribution $p(y|x^{(1)})$.\n\nTo generate the necessary random variates, you must use the inverse transform sampling method. Use the following random numbers, which are drawn from a Uniform(0,1) distribution:\n- For generating $x^{(1)}$, use the uniform random number $u_x = 0.600$.\n- For generating $y^{(1)}$, use the uniform random number $u_y = 0.750$.\n\nWhat are the numerical values for the new state $(x^{(1)}, y^{(1)})$? The value for $x^{(1)}$ must be rounded to four significant figures.",
            "solution": "We perform one Gibbs update using inverse transform sampling.\n\n1) Sample $x^{(1)}$ from $p(x \\mid y^{(0)}=3)$.\nFor an Exponential rate $y$, the conditional CDF is\n$$\nF(x \\mid y)=1-\\exp(-yx), \\quad x>0.\n$$\nInverse transform uses $u_{x}=F(x \\mid y)$, hence\n$$\nx^{(1)}=F^{-1}(u_{x})=-\\frac{1}{y^{(0)}}\\ln\\!\\bigl(1-u_{x}\\bigr).\n$$\nWith $y^{(0)}=3$ and $u_{x}=0.600$,\n$$\nx^{(1)}=-\\frac{1}{3}\\ln(1-0.600)=-\\frac{1}{3}\\ln(0.4)=\\frac{1}{3}\\ln(2.5)\\approx 0.3054302439.\n$$\nRounded to four significant figures: $x^{(1)}=0.3054$.\n\n2) Sample $y^{(1)}$ from $p(y \\mid x^{(1)})$ using $u_{y}=0.750$.\nFor a Poisson mean $x$, the pmf is\n$$\np(y=k \\mid x)=\\frac{x^{k}\\exp(-x)}{k!}, \\quad k\\in\\{0,1,2,\\dots\\}.\n$$\nInverse transform for a discrete distribution selects the smallest $k$ such that $F(k \\mid x)=\\sum_{j=0}^{k}p(j \\mid x)\\ge u_{y}$.\n\nWith $x=x^{(1)}=\\frac{1}{3}\\ln(2.5)$, compute\n$$\np(0 \\mid x)=\\exp(-x)=\\exp\\!\\Bigl(-\\tfrac{1}{3}\\ln(2.5)\\Bigr)=2.5^{-1/3}\\approx 0.7368.\n$$\nSince $p(0 \\mid x)=0.7368<0.750$, continue to $k=1$:\n$$\np(1 \\mid x)=x\\exp(-x)=x\\,2.5^{-1/3}\\approx 0.30543\\times 0.7368\\approx 0.2250.\n$$\nThen\n$$\nF(1 \\mid x)=p(0 \\mid x)+p(1 \\mid x)\\approx 0.7368+0.2250=0.9618>0.750,\n$$\nso the smallest $k$ with $F(k \\mid x)\\ge 0.750$ is $k=1$. Therefore $y^{(1)}=1$.\n\nThus, the new state is $(x^{(1)},y^{(1)})=(0.3054,1)$ with $x^{(1)}$ rounded to four significant figures.",
            "answer": "$$\\boxed{\\begin{pmatrix}0.3054 & 1\\end{pmatrix}}$$"
        },
        {
            "introduction": "While powerful, the Gibbs sampler is not a universally perfect tool; practitioners must understand its limitations. This hypothetical exercise  is designed to illustrate a classic pitfall where high correlation between variables causes the sampler to converge very slowly, a phenomenon known as poor mixing. By tracing the sampler's path in this carefully constructed scenario, you can develop an intuition for why and when the algorithm might struggle to explore the target distribution efficiently.",
            "id": "1363745",
            "problem": "In a high-precision manufacturing process for optical components, two geometric parameters of a lens, $x_1$ and $x_2$, are critical for its performance. These parameters represent normalized deviations from the ideal design specifications. Due to the physics of the fabrication process, these parameters are not independent. An analysis of production data reveals that their joint probability distribution can be modeled by an unnormalized density function $f(x_1, x_2)$ given by:\n$$f(x_1, x_2) \\propto \\exp \\left( -\\frac{1}{2(1-\\rho^2)} (x_1^2 - 2\\rho x_1 x_2 + x_2^2) \\right)$$\nFor this specific process, the correlation coefficient is found to be $\\rho = 0.99$. The mode of the distribution is at $(0, 0)$, which corresponds to a perfect component.\n\nTo simulate the process variations, you are asked to use a Gibbs sampler. You start from an initial state $(x_1^{(0)}, x_2^{(0)}) = (-4.0, -4.1)$, which represents a component at the edge of the acceptable quality range.\n\nYour task is to determine the state of the sampler, $(x_1^{(2)}, x_2^{(2)})$, after two full iterations. A full iteration consists of updating $x_1$ first, and then updating $x_2$. To make the calculation deterministic, you must assume that at each sampling step, the new value drawn for a variable is equal to the mean of its conditional distribution.\n\nCalculate the coordinates of the state $(x_1^{(2)}, x_2^{(2)})$. Report both coordinates in your final answer, rounded to four significant figures.",
            "solution": "We recognize the given unnormalized joint density\n$$\nf(x_{1},x_{2}) \\propto \\exp\\left(-\\frac{1}{2(1-\\rho^{2})}\\left(x_{1}^{2}-2\\rho x_{1}x_{2}+x_{2}^{2}\\right)\\right)\n$$\nas the kernel of a bivariate normal distribution with mean vector $(0,0)$, unit variances, and correlation coefficient $\\rho$. To derive the full conditional distributions, complete the square in $x_{1}$:\n$$\nx_{1}^{2}-2\\rho x_{1}x_{2}+x_{2}^{2}=(x_{1}-\\rho x_{2})^{2}+(1-\\rho^{2})x_{2}^{2}.\n$$\nHence, conditional on $x_{2}$,\n$$\nf(x_{1}\\mid x_{2}) \\propto \\exp\\left(-\\frac{1}{2(1-\\rho^{2})}(x_{1}-\\rho x_{2})^{2}\\right),\n$$\nwhich is the kernel of a normal distribution with\n$$\nx_{1}\\mid x_{2} \\sim \\mathcal{N}\\!\\left(\\rho x_{2},\\,1-\\rho^{2}\\right).\n$$\nBy symmetry, we also have\n$$\nx_{2}\\mid x_{1} \\sim \\mathcal{N}\\!\\left(\\rho x_{1},\\,1-\\rho^{2}\\right).\n$$\n\nThe Gibbs sampler updates $x_{1}$ first using $x_{2}$, then updates $x_{2}$ using the new $x_{1}$. Under the deterministic rule that each draw equals the conditional mean, the updates are\n$$\nx_{1}^{(t+1)}=\\rho\\,x_{2}^{(t)},\\qquad x_{2}^{(t+1)}=\\rho\\,x_{1}^{(t+1)}.\n$$\nCombining these,\n$$\nx_{2}^{(t+1)}=\\rho^{2}\\,x_{2}^{(t)},\\qquad x_{1}^{(t+1)}=\\rho\\,x_{2}^{(t)}.\n$$\nStarting from $(x_{1}^{(0)},x_{2}^{(0)})=(-4.0,-4.1)$ and using $\\rho=0.99$, the first full iteration yields\n$$\nx_{1}^{(1)}=\\rho\\,x_{2}^{(0)}=0.99\\times(-4.1)=-4.059,\\qquad\nx_{2}^{(1)}=\\rho\\,x_{1}^{(1)}=0.99\\times(-4.059)=-4.01841.\n$$\nThe second full iteration then gives\n$$\nx_{1}^{(2)}=\\rho\\,x_{2}^{(1)}=0.99\\times(-4.01841)=-3.9782259,\\qquad\nx_{2}^{(2)}=\\rho\\,x_{1}^{(2)}=0.99\\times(-3.9782259)=-3.938443641.\n$$\nRounding each coordinate to four significant figures:\n$$\nx_{1}^{(2)}\\approx -3.978,\\qquad x_{2}^{(2)}\\approx -3.938.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}-3.978 & -3.938\\end{pmatrix}}$$"
        }
    ]
}