{
    "hands_on_practices": [
        {
            "introduction": "The fundamental principle of Monte Carlo integration is to approximate an integral by averaging the function's value at randomly sampled points within the integration domain. This first exercise provides a concrete, hands-on application of this core idea. By calculating the total mass of an agent within a complex sub-region of a cube, you will see how to handle irregular integration boundaries and apply the Monte Carlo estimator from first principles, using a small, traceable number of sample points .",
            "id": "1376816",
            "problem": "A materials scientist is studying a new alloy created in a cubical mold of side length 1 meter. The coordinate system is aligned with the mold, such that it occupies the region defined by $0 \\le x \\le 1$, $0 \\le y \\le 1$, and $0 \\le z \\le 1$. The concentration of a special hardening agent, $C$, is found to be non-zero only in a specific sub-region of the mold defined by the inequalities $0 \\le z \\le y \\le x \\le 1$. Within this sub-region, the concentration at a point $(x,y,z)$ is described by the function $C(x,y,z) = k x y z$, where $k$ is a constant. Outside this region, the concentration is zero. The total mass of the agent in the mold is given by the integral of the concentration function over the entire volume of the 1-meter-cubed mold.\n\nTo estimate this total mass, an automated measurement system probes the concentration at a set of $N=5$ sample points, which are assumed to be representative of a uniform random sampling within the unit cube. The coordinates of these five points are:\n$P_1 = (0.8, 0.7, 0.2)$\n$P_2 = (0.9, 0.5, 0.6)$\n$P_3 = (0.6, 0.8, 0.3)$\n$P_4 = (0.5, 0.4, 0.3)$\n$P_5 = (0.7, 0.9, 0.8)$\n\nGiven the concentration constant $k = 4.8 \\text{ kg/m}^6$, calculate the numerical estimate for the total mass of the hardening agent in the mold based on this set of five sample points. Express your answer in kilograms (kg) and round to three significant figures.",
            "solution": "The total mass is the volume integral of the concentration over the unit cube:\n$$\nM=\\iiint_{[0,1]^{3}} C(x,y,z)\\,dV.\n$$\nWith uniform random sampling over a domain of volume $V=1$, the Monte Carlo estimator with $N$ samples $\\{P_{i}\\}_{i=1}^{N}$ is\n$$\n\\widehat{M}=\\frac{V}{N}\\sum_{i=1}^{N} C(P_{i})=\\frac{1}{N}\\sum_{i=1}^{N} C(P_{i}).\n$$\nHere $C(x,y,z)=k\\,x y z$ if $0 \\le z \\le y \\le x \\le 1$ and $C=0$ otherwise. Evaluate the indicator $0 \\le z \\le y \\le x \\le 1$ for each sample:\n- $P_{1}=(0.8,0.7,0.2)$: $0.2 \\le 0.7 \\le 0.8 \\le 1$ is true, so contributes $k\\cdot 0.8\\cdot 0.7\\cdot 0.2=0.112\\,k$.\n- $P_{2}=(0.9,0.5,0.6)$: $0.6 \\le 0.5$ is false, contributes $0$.\n- $P_{3}=(0.6,0.8,0.3)$: $0.8 \\le 0.6$ is false, contributes $0$.\n- $P_{4}=(0.5,0.4,0.3)$: $0.3 \\le 0.4 \\le 0.5 \\le 1$ is true, contributes $k\\cdot 0.5\\cdot 0.4\\cdot 0.3=0.06\\,k$.\n- $P_{5}=(0.7,0.9,0.8)$: $0.9 \\le 0.7$ is false, contributes $0$.\nTherefore\n$$\n\\sum_{i=1}^{5} C(P_{i})=(0.112+0.06)\\,k=0.172\\,k,\n$$\nand\n$$\n\\widehat{M}=\\frac{1}{5}\\cdot 0.172\\,k=0.0344\\,k.\n$$\nSubstituting $k=4.8$ gives\n$$\n\\widehat{M}=0.0344\\times 4.8=0.16512,\n$$\nwhich, rounded to three significant figures, is $0.165$ kilograms.",
            "answer": "$$\\boxed{0.165}$$"
        },
        {
            "introduction": "A key advantage of Monte Carlo integration is its robustness. Unlike deterministic methods like Simpson's rule, whose accuracy depends on the smoothness of the integrand, Monte Carlo's convergence rate of $O(N^{-1/2})$ is largely unaffected by kinks or discontinuities. This coding exercise  puts this theory into practice, asking you to compare the performance of Monte Carlo against Simpson's rule on a function with a non-differentiable point. This comparison will provide crucial insight into why Monte Carlo is often the preferred method for high-dimensional or poorly-behaved integrands.",
            "id": "3253419",
            "problem": "You are to implement and analyze two numerical estimators for the integral $$I = \\int_{0}^{1} \\lvert x - 0.3 \\rvert \\, dx,$$ focusing on how the non-differentiable point (the kink) at $$x = 0.3$$ affects convergence and variance. Work entirely within purely mathematical terms: there are no physical units, and no angles. Your program must be self-contained and produce the specified outputs without any user input.\n\nFundamental base to be used:\n- The identification of deterministic integrals with expectations: for a random variable $$U \\sim \\mathrm{Uniform}(0,1),$$ $$\\mathbb{E}[f(U)] = \\int_{0}^{1} f(x) \\, dx.$$\n- Properties of the Monte Carlo (MC) estimator under independence.\n- The composite Simpson’s rule for numerical quadrature on a uniform partition with an even number of subintervals.\n\nDefine $$f(x) = \\lvert x - 0.3 \\rvert.$$ Consider the Monte Carlo estimator with $$N$$ independent and identically distributed samples $$U_1, U_2, \\dots, U_N \\sim \\mathrm{Uniform}(0,1)$$ and the composite Simpson’s rule using $$m$$ uniform subintervals on $$[0,1],$$ where $$m$$ is an even integer. The focus is on how the kink at $$x = 0.3$$ changes error behavior.\n\nTasks your program must perform:\n1. Compute the exact value of $$I$$ analytically from first principles.\n2. For the Monte Carlo estimator:\n   - Using a fixed pseudorandom number generator seed per test case, draw $$N$$ samples $$U_i$$ from $$\\mathrm{Uniform}(0,1)$$ and compute the sample mean $$\\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(U_i).$$\n   - Compute the sample variance of the observed $$f(U_i)$$ values using the population convention (divide by $$N$$), so it remains defined for $$N = 1.$$\n   - Compute the theoretical standard error $$\\sqrt{\\operatorname{Var}(f(U))/N}$$ by analytically deriving $$\\operatorname{Var}(f(U)).$$\n3. For the composite Simpson’s rule:\n   - Implement composite Simpson’s rule on $$[0,1]$$ with $$m$$ even subintervals and uniform spacing $$h = 1/m.$$ Use the standard weights $$\\frac{h}{3} \\left[f(x_0) + 4 \\sum f(x_{2j+1}) + 2 \\sum f(x_{2j}) + f(x_m)\\right]$$ where $$x_k = k h.$$\n   - Compute the Simpson approximation $$S_m$$ and its absolute error with respect to the exact $$I.$$\n4. For both methods, report the absolute error with respect to the exact $$I.$$\n5. Use the following test suite to assess different scenarios:\n   - Case $$1$$ (happy path, kink aligned with a mesh node): $$N = 100,$$ $$m = 10,$$ seed $$= 12345.$$\n   - Case $$2$$ (happy path, kink not aligned with nodes): $$N = 100,$$ $$m = 12,$$ seed $$= 54321.$$\n   - Case $$3$$ (boundary condition for Monte Carlo and Simpson): $$N = 1,$$ $$m = 2,$$ seed $$= 7.$$\n   - Case $$4$$ (large sample size and fine mesh, kink aligned with a node): $$N = 10000,$$ $$m = 100,$$ seed $$= 2025.$$\n   All $$m$$ are even as required by composite Simpson’s rule.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list of seven floats in the following order:\n- $$\\hat{I}_N$$ (Monte Carlo estimate),\n- the sample variance of $$f(U_i)$$ using division by $$N,$$\n- the theoretical standard error $$\\sqrt{\\operatorname{Var}(f(U))/N},$$\n- $$S_m$$ (composite Simpson’s estimate),\n- the exact $$I,$$\n- the absolute error of Monte Carlo $$\\lvert \\hat{I}_N - I \\rvert,$$\n- the absolute error of Simpson $$\\lvert S_m - I \\rvert.$$\n\nThus, the final output format is a single line representing a list of lists, for example $$[[\\ldots],[\\ldots],[\\ldots],[\\ldots]].$$",
            "solution": "The problem is subjected to validation before proceeding.\n\n### Step 1: Extract Givens\n- Integral to evaluate: $$I = \\int_{0}^{1} \\lvert x - 0.3 \\rvert \\, dx$$.\n- Function definition: $$f(x) = \\lvert x - 0.3 \\rvert$$.\n- Random variable for Monte Carlo: $$U \\sim \\mathrm{Uniform}(0,1)$$.\n- Monte Carlo estimator: $$\\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(U_i)$$, with $$U_i \\sim \\mathrm{Uniform}(0,1)$$ i.i.d.\n- Sample variance definition: Use population convention (division by $$N$$).\n- Theoretical standard error: $$\\sqrt{\\operatorname{Var}(f(U))/N}$$.\n- Composite Simpson's rule: Use $$m$$ even subintervals, spacing $$h = 1/m$$, points $$x_k = k h$$, and the formula $$S_m = \\frac{h}{3} \\left[f(x_0) + 4 \\sum_{j=0}^{m/2-1} f(x_{2j+1}) + 2 \\sum_{j=1}^{m/2-1} f(x_{2j}) + f(x_m)\\right]$$.\n- Test Cases:\n  - Case 1: $$N = 100$$, $$m = 10$$, seed $$= 12345$$.\n  - Case 2: $$N = 100$$, $$m = 12$$, seed $$= 54321$$.\n  - Case 3: $$N = 1$$, $$m = 2$$, seed $$= 7$$.\n  - Case 4: $$N = 10000$$, $$m = 100$$, seed $$= 2025$$.\n- Required outputs for each case: A list of seven floating-point numbers: $$\\hat{I}_N$$, sample variance of $$f(U_i)$$, theoretical standard error, $$S_m$$, exact $$I$$, absolute error of Monte Carlo $$\\lvert \\hat{I}_N - I \\rvert$$, absolute error of Simpson $$\\lvert S_m - I \\rvert$$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded**: The problem is a standard exercise in numerical analysis, comparing a stochastic method (Monte Carlo) with a deterministic quadrature rule (Simpson's rule). The function $$f(x) = \\lvert x - 0.3 \\rvert$$ is a classic example used to illustrate the behavior of numerical methods on non-smooth functions. All principles invoked are fundamental to mathematics and scientific computing.\n- **Well-Posed**: The problem is fully specified. The integral is well-defined. The formulas for both estimators are explicitly given. All parameters ($$N, m$$, seeds) for each test case are provided. The definition of sample variance is clarified to avoid ambiguity.\n- **Objective**: The problem is stated in precise, mathematical language, free of any subjectivity or bias.\n\nThe problem does not exhibit any of the invalidity flaws. It is a well-defined, formalizable, and verifiable problem in the field of numerical methods.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Design of the Solution\n\nThe solution involves three main components: analytical calculation of exact quantities, implementation of the Monte Carlo estimator, and implementation of the composite Simpson's rule.\n\n#### 1. Analytical Calculations\n\nFirst, we must compute the exact value of the integral $$I$$ and the theoretical variance of $$f(U)$$ for an accurate assessment of the numerical methods.\n\n**Exact Value of the Integral $$I$$**\nThe integrand $$f(x) = \\lvert x - 0.3 \\rvert$$ has a non-differentiable point at $$x = 0.3$$. To integrate it, we split the domain at this point:\n$$I = \\int_{0}^{1} \\lvert x - 0.3 \\rvert \\, dx = \\int_{0}^{0.3} -(x - 0.3) \\, dx + \\int_{0.3}^{1} (x - 0.3) \\, dx$$\n$$I = \\left[ 0.3x - \\frac{x^2}{2} \\right]_{0}^{0.3} + \\left[ \\frac{x^2}{2} - 0.3x \\right]_{0.3}^{1}$$\n$$I = \\left( 0.3^2 - \\frac{0.3^2}{2} \\right) - 0 + \\left( \\frac{1^2}{2} - 0.3 \\cdot 1 \\right) - \\left( \\frac{0.3^2}{2} - 0.3^2 \\right)$$\n$$I = \\left( \\frac{0.09}{2} \\right) + (0.5 - 0.3) - \\left( -\\frac{0.09}{2} \\right) = 0.045 + 0.2 + 0.045 = 0.29$$\nSo, the exact value of the integral is $$I = 0.29$$.\n\n**Theoretical Variance of $$f(U)$$, where $$U \\sim \\mathrm{Uniform}(0,1)$$.**\nThe variance is given by $$\\operatorname{Var}(f(U)) = \\mathbb{E}[f(U)^2] - (\\mathbb{E}[f(U)])^2$$.\nWe already know $$\\mathbb{E}[f(U)] = \\int_{0}^{1} f(x) \\, dx = I = 0.29$$.\nWe compute the second moment $$\\mathbb{E}[f(U)^2]$$:\n$$\\mathbb{E}[f(U)^2] = \\int_{0}^{1} f(x)^2 \\, dx = \\int_{0}^{1} (\\lvert x - 0.3 \\rvert)^2 \\, dx = \\int_{0}^{1} (x - 0.3)^2 \\, dx$$\n$$\\mathbb{E}[f(U)^2] = \\int_{0}^{1} (x^2 - 0.6x + 0.09) \\, dx = \\left[ \\frac{x^3}{3} - 0.3x^2 + 0.09x \\right]_{0}^{1}$$\n$$\\mathbb{E}[f(U)^2] = \\left( \\frac{1}{3} - 0.3 + 0.09 \\right) - 0 = \\frac{1}{3} - 0.21 = \\frac{100 - 63}{300} = \\frac{37}{300}$$\nNow, we can find the variance:\n$$\\operatorname{Var}(f(U)) = \\frac{37}{300} - (0.29)^2 = \\frac{37}{300} - 0.0841 = \\frac{37}{300} - \\frac{841}{10000} = \\frac{3700 - 2523}{30000} = \\frac{1177}{30000}$$\nThis is the exact theoretical variance.\n\n#### 2. Monte Carlo Integration\n\nThe Monte Carlo method estimates the integral $$I = \\mathbb{E}[f(U)]$$ by the sample mean of $$N$$ function evaluations at random points $$U_i$$ drawn from the relevant distribution.\n- **Estimator**: $$\\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} f(U_i)$$.\n- **Sample Variance**: The problem specifies using the population formula (dividing by $$N$$): $$\\hat{\\sigma}^2_N = \\frac{1}{N} \\sum_{i=1}^{N} (f(U_i) - \\hat{I}_N)^2$$. This is the `ddof=0` convention in NumPy.\n- **Theoretical Standard Error**: The Central Limit Theorem suggests the error of the Monte Carlo estimator is approximately normally distributed with standard deviation $$\\sigma / \\sqrt{N}$$, where $$\\sigma^2 = \\operatorname{Var}(f(U))$$. This quantity, $$\\sqrt{\\operatorname{Var}(f(U))/N}$$, is the theoretical standard error of the mean.\nFor each test case, we will use the given seed to generate $$N$$ samples, compute the estimate $$\\hat{I}_N$$, its sample variance, the theoretical standard error, and the absolute error $$\\lvert \\hat{I}_N - I \\rvert$$.\n\n#### 3. Composite Simpson's Rule\n\nThis is a deterministic quadrature rule that approximates the integrand with piecewise quadratic polynomials.\n- **Formula**: $$S_m = \\frac{h}{3} \\sum_{j=0}^{m/2-1} \\left( f(x_{2j}) + 4f(x_{2j+1}) + f(x_{2j+2}) \\right)$$, where $$h=1/m$$.\n- **Convergence Analysis**: The standard error bound for composite Simpson's rule is of order $$O(h^4)$$ or $$O(m^{-4})$$, but this relies on the integrand being four times continuously differentiable. Our function $$f(x) = \\lvert x-0.3 \\rvert$$ is not differentiable at $$x=0.3$$. Its fourth derivative is undefined there. Consequently, the convergence rate will be significantly slower than $$O(m^{-4})$$. The error is dominated by the approximation over the subinterval(s) containing the kink.\n- **Specific Cases**:\n  - When $$m=10$$, $$h=0.1$$, the kink at $$x=0.3$$ is a grid point ($$x_3$$). The error will be confined to the single application of Simpson's rule over the interval $$[x_2, x_4] = [0.2, 0.4]$$, which straddles the kink.\n  - When $$m=12$$, $$h=1/12$$, the kink at $$x=0.3$$ is not a grid point. It lies within the interval $$[x_3, x_4] = [0.25, 0.33\\dots]$$. The error will arise from the approximation over the interval $$[x_2, x_4]$$ which contains this kink.\nFor each test case, we construct the uniform grid with $$m+1$$ points, evaluate $$f(x)$$ at these points, apply the summation formula to get $$S_m$$, and compute the absolute error $$\\lvert S_m - I \\rvert$$.\n\nThese steps will be implemented in Python for each of the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes Monte Carlo and Simpson's rule estimators for a\n    given integral with a non-differentiable point.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, m, seed)\n        (100, 10, 12345),\n        (100, 12, 54321),\n        (1, 2, 7),\n        (10000, 100, 2025)\n    ]\n\n    # Analytical values derived in the solution\n    exact_I = 0.29\n    # Theoretical variance Var(f(U)) = 1177 / 30000\n    theoretical_variance = 1177.0 / 30000.0\n\n    # The function to integrate: f(x) = |x - 0.3|\n    def f(x):\n        return np.abs(x - 0.3)\n\n    results = []\n    for N, m, seed in test_cases:\n        # --- Monte Carlo Method ---\n        rng = np.random.default_rng(seed)\n        samples = rng.uniform(0, 1, N)\n        f_samples = f(samples)\n\n        # Monte Carlo estimate\n        I_hat_N = np.mean(f_samples)\n        \n        # Sample variance (population convention, divide by N)\n        # np.var default is ddof=0, which is the population variance.\n        sample_var = np.var(f_samples)\n        \n        # Theoretical standard error of the mean\n        theo_std_err = np.sqrt(theoretical_variance / N)\n        \n        # Absolute error of Monte Carlo estimate\n        mc_abs_err = np.abs(I_hat_N - exact_I)\n\n        # --- Composite Simpson's Rule ---\n        h = 1.0 / m\n        # Generate m+1 points from 0 to 1\n        x_grid = np.linspace(0, 1, m + 1)\n        y_grid = f(x_grid)\n\n        # Simpson's formula: S_m = (h/3) * [f(x_0) + 4*f(x_1) + 2*f(x_2) + ... + f(x_m)]\n        # We can implement this with slicing for efficiency.\n        # Sum of odd-indexed terms: y[1], y[3], ..., y[m-1]\n        sum_odd = np.sum(y_grid[1:-1:2])\n        # Sum of even-indexed terms (excluding endpoints): y[2], y[4], ..., y[m-2]\n        sum_even = np.sum(y_grid[2:-1:2])\n        \n        S_m = (h / 3.0) * (y_grid[0] + y_grid[-1] + 4 * sum_odd + 2 * sum_even)\n\n        # Absolute error of Simpson's estimate\n        simpson_abs_err = np.abs(S_m - exact_I)\n\n        # Collect the seven required values\n        case_result = [\n            I_hat_N,\n            sample_var,\n            theo_std_err,\n            S_m,\n            exact_I,\n            mc_abs_err,\n            simpson_abs_err\n        ]\n        results.append(case_result)\n\n    # Format the final output as a string representing a list of lists.\n    # The default str() for a list includes spaces, which we need to remove\n    # to strictly match the requested format.\n    case_strings = []\n    for res in results:\n        # Using repr() to get a standard representation, can also use f-strings\n        res_str = f\"[{','.join(repr(val) for val in res)}]\"\n        case_strings.append(res_str)\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Many problems in science and engineering require integrating over an infinite domain, a task where simple Monte Carlo methods fail. This advanced practice demonstrates a powerful and sophisticated solution: a hybrid adaptive strategy. You will implement a method that combines standard Monte Carlo for a central region with importance sampling for the infinite tails, using a proposal distribution tailored to the integrand's decay . This capstone exercise showcases the flexibility of Monte Carlo techniques, guiding you through a pilot study to optimize the algorithm and efficiently estimate one of the most famous integrals in mathematics, $\\int_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx$, from the ground up.",
            "id": "3253443",
            "problem": "Implement a complete, runnable program that estimates the integral of the function $x \\mapsto e^{-x^{2}}$ over the entire real line by stochastic simulation. The integral of interest is the value of\n$$\nI \\equiv \\int_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx.\n$$\nYou must use a two-part Monte Carlo (Monte Carlo (MC)) strategy in which the real line is split at a symmetric threshold $A > 0$ into a central region and two tails. The central part is $\\int_{-A}^{A} e^{-x^{2}} \\, dx$ and the tail part is $\\int_{|x|>A} e^{-x^{2}} \\, dx$. The central integral must be estimated by simple Monte Carlo using a uniform distribution over the interval $[-A, A]$. The tail integral must be estimated by importance sampling using an exponential proposal on $[A, \\infty)$ with a rate parameter that depends linearly on $A$. The threshold $A$ must be chosen adaptively from a prescribed grid by a pilot study that estimates, for each candidate $A$, the variance contributions of the central and tail estimators and predicts the minimal variance achievable by optimally allocating the main sample between the two parts under a fixed total sample budget.\n\nStart from the following fundamentals only:\n- The definition of an integral as an expectation under a probability density, and the unbiased sample mean estimator for an expectation.\n- The construction of an importance sampling estimator using a proposal density and the associated unbiasedness under absolute continuity.\n- The variance of an independent sample mean and the additivity of variance for independent sums.\n\nYou must not rely on any closed-form antiderivative or special-function identity for the integrand. The algorithm should be designed from first principles using the above foundations and should justify each step in terms of unbiasedness and variance control. The program must carry out these steps:\n1. For a given candidate $A$, define a central estimator by sampling $U$ from the uniform distribution on $[-A, A]$ and using the sample mean of the transformed variable $G = (2A)\\, e^{-U^{2}}$.\n2. For a given candidate $A$, define a tail estimator by importance sampling the positive tail with proposal density $q_{A}(y) = \\lambda_{A} \\, e^{-\\lambda_{A}(y - A)}$ for $y \\ge A$ and reflecting by symmetry to cover both tails. Here $\\lambda_{A} = \\rho A$ with a given constant $\\rho > 0$. With a draw $Y$ from $q_{A}$, define the transformed variable $H = 2 \\, e^{-Y^{2}} / q_{A}(Y)$; the sample mean of $H$ is an unbiased estimator for the total contribution of both tails.\n3. For a grid of candidate thresholds $A$ and a fixed pilot sample size $n_{0}$ for each part, estimate for each $A$ the per-draw variance of $G$ and $H$ using a common set of random seeds across $A$ to ensure the pilot stage consumes a fixed number of random variates independent of the grid size. Use these pilot variance estimates and the total main-sample budget $N_{\\text{main}}$ to determine, for each $A$, the sample allocation that minimizes the predicted variance of the sum of the two independent estimators under the constraint $n_{\\text{center}} + n_{\\text{tail}} = N_{\\text{main}}$. Select the $A$ that yields the smallest predicted variance. If ties occur, choose the smallest $A$ among the minimizers.\n4. With the selected $A$, run the main sampling with the optimal integer allocation $n_{\\text{center}}$ and $n_{\\text{tail}}$ (each at least $1$) to obtain the final point estimate $\\widehat{I}$ as the sum of the two sample means. Compute the estimated standard error $\\widehat{\\mathrm{se}}$ by combining the sample variances of $G$ and $H$ computed on the main samples, divided by their respective sample sizes, and then taking the square root.\n5. The implementation must ensure that the total number of generated base random variates equals $N_{\\text{total}} = 2 n_{0} + N_{\\text{main}}$, where $n_{0}$ variates are used for the central pilot, $n_{0}$ variates are used for the tail pilot, and $N_{\\text{main}}$ variates are used across both parts in the main stage. Use common random numbers in the pilot stage as follows: generate a single vector of $n_{0}$ uniform variates on $[0,1]$ and transform it to $[-A, A]$ for each candidate $A$ to obtain the central pilot draws; generate a single vector of $n_{0}$ exponential variates with unit rate and transform it to the tail proposal for each candidate $A$ to obtain the tail pilot draws.\n\nInput parameters for each test case are fully specified below; your program must hard-code these test cases, perform the adaptive selection and estimation for each case, and print the required outputs exactly as specified. No user input is allowed.\n\nTest suite:\n- Case $1$: seed $= 123456$, $N_{\\text{total}} = 100000$, pilot size $n_{0} = 2000$, candidate grid for $A$ is linearly spaced from $0.5$ to $2.5$ with $9$ points (inclusive), and $\\rho = 1.25$.\n- Case $2$: seed $= 2024$, $N_{\\text{total}} = 8000$, pilot size $n_{0} = 1000$, candidate grid for $A$ is linearly spaced from $0.2$ to $1.4$ with $7$ points (inclusive), and $\\rho = 1.0$.\n- Case $3$: seed $= 77$, $N_{\\text{total}} = 40000$, pilot size $n_{0} = 1500$, candidate grid for $A$ is linearly spaced from $0.8$ to $3.0$ with $12$ points (inclusive), and $\\rho = 0.75$.\n\nImplementation details and constraints:\n- Use double precision arithmetic.\n- Use only the pseudo-random number generator initialized with the given seed to ensure reproducibility. For the exponential proposal on $[A, \\infty)$ with rate $\\lambda_{A}$, you may generate a unit-rate exponential $E$ and map it as $Y = A + E / \\lambda_{A}$; in this case, $q_{A}(Y) = \\lambda_{A} \\, e^{-\\lambda_{A}(Y - A)} = \\lambda_{A} \\, e^{-E}$ and the transformed variable can equivalently be written as $H = \\frac{2}{\\lambda_{A}} \\, \\exp\\!\\left(-\\left(A + \\frac{E}{\\lambda_{A}}\\right)^{2} + E \\right)$.\n- For each candidate $A$, define $N_{\\text{main}} = N_{\\text{total}} - 2 n_{0}$, which is guaranteed to be positive in the provided test suite. Use the pilot per-draw variance estimates of $G$ and $H$ to predict the minimal achievable variance at that $A$ under optimal allocation. Then select $A$ adaptively as described above, compute the corresponding optimal allocation, and perform the main sampling.\n- Compute the final standard error as $\\widehat{\\mathrm{se}} = \\sqrt{\\widehat{\\sigma}_{G}^{2}/n_{\\text{center}} + \\widehat{\\sigma}_{H}^{2}/n_{\\text{tail}}}$, where $\\widehat{\\sigma}_{G}^{2}$ and $\\widehat{\\sigma}_{H}^{2}$ are the unbiased sample variances of $G$ and $H$ computed on the main samples.\n\nAnswer specification and output format:\n- For each test case, output a list containing $5$ values in this exact order: the point estimate $\\widehat{I}$, the estimated standard error $\\widehat{\\mathrm{se}}$, the selected threshold $A$, the integer $n_{\\text{center}}$, and the integer $n_{\\text{tail}}$.\n- Express the three floating-point outputs rounded to exactly $6$ decimal places. The two allocation counts must be printed as integers.\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list of lists, for example $[[v_{11},v_{12},v_{13},v_{14},v_{15}],[v_{21},v_{22},v_{23},v_{24},v_{25}],[v_{31},v_{32},v_{33},v_{34},v_{35}]]$, where each $v_{ij}$ is a number as specified above.",
            "solution": "The problem requires the estimation of the Gaussian integral $I \\equiv \\int_{-\\infty}^{\\infty} e^{-x^{2}} \\, dx$ using a two-stage adaptive Monte Carlo method. The solution is developed from first principles, namely the representation of an integral as a statistical expectation and the use of sample means as unbiased estimators.\n\n### Part 1: Integral Decomposition and Estimator Construction\n\nThe integral $I$ over the entire real line $(-\\infty, \\infty)$ is partitioned at a symmetric threshold $A > 0$ into a central integral and a tail integral.\n$$\nI = \\int_{-A}^{A} e^{-x^{2}} \\, dx + \\int_{|x|>A} e^{-x^{2}} \\, dx = I_{\\text{center}}(A) + I_{\\text{tail}}(A)\n$$\nThese two components are estimated independently and their estimates are summed.\n\n**Central Integral Estimator:**\nThe central integral $I_{\\text{center}}(A)$ is estimated using simple Monte Carlo integration. We can express the integral as the expectation of a random variable. Let $U$ be a random variable uniformly distributed on the interval $[-A, A]$, with probability density function (PDF) $p(u) = 1/(2A)$ for $u \\in [-A, A]$. The integral is then:\n$$\nI_{\\text{center}}(A) = \\int_{-A}^{A} e^{-x^{2}} \\, dx = \\int_{-A}^{A} \\left( (2A) e^{-x^{2}} \\right) \\frac{1}{2A} \\, dx = E_{U \\sim \\text{Unif}[-A, A]} [G(U)]\n$$\nwhere $G(U) = (2A)e^{-U^{2}}$. An unbiased estimator for $I_{\\text{center}}(A)$ is the sample mean of $n_{\\text{center}}$ independent draws of $G$:\n$$\n\\widehat{I}_{\\text{center}} = \\frac{1}{n_{\\text{center}}} \\sum_{i=1}^{n_{\\text{center}}} G_i, \\quad \\text{where } G_i = (2A)e^{-U_i^2} \\text{ and } U_i \\sim \\text{Unif}[-A, A]\n$$\n\n**Tail Integral Estimator:**\nThe tail integral is $I_{\\text{tail}}(A) = \\int_{|x|>A} e^{-x^2} \\, dx = 2 \\int_{A}^{\\infty} e^{-x^2} \\, dx$ by symmetry. This integral is estimated using importance sampling, which is suitable for integrands that decay rapidly. We introduce a proposal distribution with PDF $q_{A}(y)$ on the domain $[A, \\infty)$. The integral can be written as:\n$$\nI_{\\text{tail}}(A) = 2 \\int_{A}^{\\infty} \\frac{e^{-y^2}}{q_A(y)} q_A(y) \\, dy = E_{Y \\sim q_A} \\left[ \\frac{2 e^{-Y^2}}{q_A(Y)} \\right]\n$$\nThe problem specifies a shifted exponential proposal density $q_A(y) = \\lambda_A e^{-\\lambda_A(y-A)}$ for $y \\ge A$, where the rate $\\lambda_A$ is a linear function of the threshold, $\\lambda_A = \\rho A$, for a given constant $\\rho > 0$. An unbiased estimator for $I_{\\text{tail}}(A)$ is the sample mean of $n_{\\text{tail}}$ independent draws of the random variable $H(Y) = 2e^{-Y^2}/q_A(Y)$:\n$$\n\\widehat{I}_{\\text{tail}} = \\frac{1}{n_{\\text{tail}}} \\sum_{i=1}^{n_{\\text{tail}}} H_i, \\quad \\text{where } H_i = \\frac{2e^{-Y_i^2}}{q_A(Y_i)} \\text{ and } Y_i \\sim q_A\n$$\nFor practical generation of samples $Y_i$, we can use inverse transform sampling. If $U' \\sim \\text{Unif}[0,1]$, then $E = -\\ln(U')$ is a standard exponential variate (rate $1$). A draw $Y$ from $q_A$ can be obtained as $Y = A + E/\\lambda_A$. Substituting this into the expression for $H$ yields a more convenient form for computation:\n$$\nH = \\frac{2e^{-(A+E/\\lambda_A)^2}}{\\lambda_A e^{-\\lambda_A((A+E/\\lambda_A)-A)}} = \\frac{2e^{-(A+E/\\lambda_A)^2}}{\\lambda_A e^{-E}} = \\frac{2}{\\lambda_A} \\exp\\left(-\\left(A + \\frac{E}{\\lambda_A}\\right)^2 + E\\right)\n$$\nwhere $E \\sim \\text{Exp}(1)$.\n\n### Part 2: Variance Minimization and Optimal Allocation\n\nThe total integral estimate is $\\widehat{I} = \\widehat{I}_{\\text{center}} + \\widehat{I}_{\\text{tail}}$. Since the two estimators are independent, the variance of the total estimate is the sum of their variances:\n$$\n\\text{Var}(\\widehat{I}) = \\text{Var}(\\widehat{I}_{\\text{center}}) + \\text{Var}(\\widehat{I}_{\\text{tail}}) = \\frac{\\sigma_G^2(A)}{n_{\\text{center}}} + \\frac{\\sigma_H^2(A)}{n_{\\text{tail}}}\n$$\nwhere $\\sigma_G^2(A) = \\text{Var}(G)$ and $\\sigma_H^2(A) = \\text{Var}(H)$. Our goal is to choose the sample sizes $n_{\\text{center}}$ and $n_{\\text{tail}}$ to minimize this variance, subject to a fixed total budget for the main simulation, $n_{\\text{center}} + n_{\\text{tail}} = N_{\\text{main}}$.\n\nUsing the method of Lagrange multipliers or simple substitution, the optimal allocation is found to be proportional to the standard deviations of the respective estimators:\n$$\nn_{\\text{center}}^* = N_{\\text{main}} \\frac{\\sigma_G(A)}{\\sigma_G(A) + \\sigma_H(A)}, \\quad n_{\\text{tail}}^* = N_{\\text{main}} \\frac{\\sigma_H(A)}{\\sigma_G(A) + \\sigma_H(A)}\n$$\nSubstituting these optimal (real-valued) allocations back into the variance formula gives the minimum achievable variance for a given $A$:\n$$\nV_{\\text{min}}(A) = \\frac{(\\sigma_G(A) + \\sigma_H(A))^2}{N_{\\text{main}}}\n$$\n\n### Part 3: Adaptive Selection of the Threshold $A$\n\nThe optimal threshold $A$ is unknown. We select it adaptively from a prescribed grid of candidate values. This is accomplished in a two-stage procedure.\n\n**Stage 1: Pilot Study**\nA pilot study is run to estimate the standard deviations $\\sigma_G(A)$ and $\\sigma_H(A)$ for each candidate $A$ in the grid.\n1. A small pilot sample of size $n_0$ is drawn for both the central and tail parts.\n2. To ensure a fair comparison across different values of $A$, Common Random Numbers (CRN) are used. A single set of $n_0$ base uniform variates on $[0,1]$ is generated for the central part, and a single set of $n_0$ base standard exponential variates is generated for the tail part.\n3. For each $A$ in the grid, these base random numbers are transformed to generate samples of $G$ and $H$, respectively.\n4. The sample variances $\\widehat{\\sigma}_{G, \\text{pilot}}^2(A)$ and $\\widehat{\\sigma}_{H, \\text{pilot}}^2(A)$ are computed from these pilot samples.\n5. For each $A$, we then predict the minimum achievable variance $V_{\\text{min}}(A)$ under optimal allocation by using the estimated standard deviations $\\widehat{\\sigma}_{G, \\text{pilot}}(A)$ and $\\widehat{\\sigma}_{H, \\text{pilot}}(A)$. Minimizing $V_{\\text{min}}(A)$ is equivalent to minimizing the sum of the estimated standard deviations, $\\widehat{\\sigma}_{G, \\text{pilot}}(A) + \\widehat{\\sigma}_{H, \\text{pilot}}(A)$.\n6. The candidate $A$ that yields the minimum predicted variance is selected as the optimal threshold, $A^*$. Ties are broken by choosing the smallest $A$.\n\n**Stage 2: Main Simulation**\n1. Using the selected $A^*$ and the corresponding pilot standard deviation estimates $\\widehat{\\sigma}_G(A^*)$ and $\\widehat{\\sigma}_H(A^*)$, the optimal integer sample allocations $n_{\\text{center}}$ and $n_{\\text{tail}}$ are determined by rounding the ideal fractional allocations while ensuring their sum is $N_{\\text{main}}$ and each is at least $1$.\n2. New, independent random samples of size $n_{\\text{center}}$ and $n_{\\text{tail}}$ are generated for the central and tail estimators, respectively.\n3. The final estimate $\\widehat{I}$ is calculated as the sum of the means of the main samples for $G$ and $H$.\n4. The standard error of the final estimate, $\\widehat{\\text{se}}(\\widehat{I})$, is computed using the sample variances from the main simulation:\n   $$\n   \\widehat{\\text{se}} = \\sqrt{\\frac{\\widehat{\\sigma}_{G, \\text{main}}^2}{n_{\\text{center}}} + \\frac{\\widehat{\\sigma}_{H, \\text{main}}^2}{n_{\\text{tail}}}}\n   $$\n   where $\\widehat{\\sigma}_{G, \\text{main}}^2$ and $\\widehat{\\sigma}_{H, \\text{main}}^2$ are the unbiased sample variances of the main samples for $G$ and $H$. This entire procedure ensures that the total number of random variates used is $2n_0$ for the pilot stage plus $N_{\\text{main}}$ for the main stage, totaling $N_{\\text{total}}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(case_params):\n    \"\"\"\n    Processes a single test case for the adaptive Monte Carlo integration.\n    \"\"\"\n    seed = case_params['seed']\n    N_total = case_params['N_total']\n    n_0 = case_params['n_0']\n    A_grid_start, A_grid_stop, A_grid_num = case_params['A_grid_params']\n    A_grid = np.linspace(A_grid_start, A_grid_stop, A_grid_num)\n    rho = case_params['rho']\n\n    rng = np.random.default_rng(seed)\n    N_main = N_total - 2 * n_0\n\n    # 1. Pilot Study with Common Random Numbers (CRN)\n    # Generate base random numbers once for the pilot stage.\n    pilot_central_uniforms = rng.uniform(size=n_0)  # For U in [-A, A]\n    pilot_tail_exponentials = rng.exponential(size=n_0)  # For E ~ Exp(1)\n\n    pilot_results = []\n    for A in A_grid:\n        lambda_A = rho * A\n\n        # Central estimator pilot calculations\n        U_pilot = A * (2 * pilot_central_uniforms - 1)\n        G_pilot = (2 * A) * np.exp(-U_pilot**2)\n        var_G = np.var(G_pilot, ddof=1)\n\n        # Tail estimator pilot calculations\n        # H = (2/lambda_A) * exp(-(A + E/lambda_A)^2 + E)\n        H_pilot = (2 / lambda_A) * np.exp(-(A + pilot_tail_exponentials / lambda_A)**2 + pilot_tail_exponentials)\n        var_H = np.var(H_pilot, ddof=1)\n        \n        if not np.isfinite(var_G) or not np.isfinite(var_H):\n            total_std_dev = np.inf\n            std_G, std_H = np.inf, np.inf\n        else:\n            std_G = np.sqrt(var_G)\n            std_H = np.sqrt(var_H)\n            total_std_dev = std_G + std_H\n\n        pilot_results.append({'total_std': total_std_dev, 'A': A, 'std_G': std_G, 'std_H': std_H})\n\n    # 2. Select Optimal A\n    # Find A that minimizes the predicted total standard deviation.\n    # The sort first by 'total_std', then by 'A' for tie-breaking.\n    best_pilot_run = sorted(pilot_results, key=lambda x: (x['total_std'], x['A']))[0]\n    A_opt = best_pilot_run['A']\n    std_G_opt = best_pilot_run['std_G']\n    std_H_opt = best_pilot_run['std_H']\n\n    # 3. Determine Main Sample Allocation\n    # Calculate ideal allocation and round to nearest integer.\n    if std_G_opt + std_H_opt == 0:\n        n_center = N_main // 2\n    else:\n        n_center_ideal = N_main * std_G_opt / (std_G_opt + std_H_opt)\n        n_center = int(np.round(n_center_ideal))\n\n    # Enforce allocation constraints: n_center, n_tail >= 1\n    if n_center  1:\n        n_center = 1\n    elif n_center > N_main - 1:\n        n_center = N_main - 1\n    n_tail = N_main - n_center\n\n    # 4. Main Monte Carlo Simulation\n    # Generate new independent random numbers for the main run.\n    \n    # Central part\n    U_main = rng.uniform(low=-A_opt, high=A_opt, size=n_center)\n    G_main = (2 * A_opt) * np.exp(-U_main**2)\n    I_hat_center = np.mean(G_main)\n    # Unbiased sample variance. If sample size is 1, variance is not estimable from sample, so we take it as 0.\n    var_G_main = np.var(G_main, ddof=1) if n_center > 1 else 0.0\n\n    # Tail part\n    lambda_A_opt = rho * A_opt\n    E_main = rng.exponential(size=n_tail)\n    H_main = (2 / lambda_A_opt) * np.exp(-(A_opt + E_main / lambda_A_opt)**2 + E_main)\n    I_hat_tail = np.mean(H_main)\n    var_H_main = np.var(H_main, ddof=1) if n_tail > 1 else 0.0\n\n    # 5. Combine Results\n    I_hat = I_hat_center + I_hat_tail\n    \n    # Final standard error computed from main sample variances\n    se_hat = np.sqrt(var_G_main / n_center + var_H_main / n_tail)\n\n    return [I_hat, se_hat, A_opt, n_center, n_tail]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'seed': 123456, 'N_total': 100000, 'n_0': 2000, 'A_grid_params': (0.5, 2.5, 9), 'rho': 1.25},\n        {'seed': 2024, 'N_total': 8000, 'n_0': 1000, 'A_grid_params': (0.2, 1.4, 7), 'rho': 1.0},\n        {'seed': 77, 'N_total': 40000, 'n_0': 1500, 'A_grid_params': (0.8, 3.0, 12), 'rho': 0.75}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case)\n        all_results.append(result)\n\n    # Format the output as specified\n    result_strings = []\n    for res in all_results:\n        # res[0:3] are floats, res[3:5] are ints\n        formatted_list = [\n            f\"{res[0]:.6f}\",\n            f\"{res[1]:.6f}\",\n            f\"{res[2]:.6f}\",\n            str(res[3]),\n            str(res[4])\n        ]\n        result_strings.append(f\"[{','.join(formatted_list)}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}