{
    "hands_on_practices": [
        {
            "introduction": "This first practice is foundational. We will build a pseudo-arclength continuation method from the ground up, focusing on the predictor-corrector scheme. This exercise is crucial for understanding how to numerically trace a solution curve even through turning points, a scenario where simpler methods fail. By implementing the tangent prediction and the augmented Newton corrector, you will gain hands-on experience with the core mechanics of robust path-following algorithms. ",
            "id": "3282840",
            "problem": "Consider the parameterized nonlinear system $F(x,\\lambda)=0$ with $x\\in\\mathbb{R}^2$, $\\lambda\\in\\mathbb{R}$, and $F:\\mathbb{R}^2\\times\\mathbb{R}\\to\\mathbb{R}^2$ defined by\n$$\nF_1(x,\\lambda) = x_1^3 - x_1 + x_2,\\quad F_2(x,\\lambda) = x_2 + \\lambda - x_1^2.\n$$\nThe Jacobian matrix of $F$ with respect to $x$ at $(x,\\lambda)$ is denoted by $\\dfrac{\\partial F}{\\partial x}(x,\\lambda)$, and the partial derivative with respect to $\\lambda$ is denoted by $\\dfrac{\\partial F}{\\partial \\lambda}(x,\\lambda)$. Your task is to implement a continuation method using a predictor-corrector scheme that traces a solution curve of triples $(x,\\lambda)$ satisfying $F(x,\\lambda)=0$. The method must explicitly use the Jacobian to compute local linearizations and perform corrections.\n\nFundamental base and constraints:\n- Use the definition of the Jacobian matrix and linearization of a nonlinear system to construct a local tangent direction to the solution manifold at a valid solution $(x,\\lambda)$. The tangent direction is conceptually defined by the requirement that movement along it preserves the first-order validity of the constraint $F(x,\\lambda)=0$.\n- Use a correction step based on Newton’s method applied to an augmented system that enforces both the original constraints and a scalar constraint defining motion along a chosen tangent direction.\n- Compute any required null-space direction using a numerically stable approach grounded in linear algebra and the Jacobian.\n\nAlgorithmic requirements:\n- Implement a predictor step that advances an approximate solution along a locally computed tangent direction using a step size chosen in advance.\n- Implement a corrector step that applies Newton’s method to an augmented system built from the original constraints and a scalar arclength-like constraint that fixes the motion to the predicted hyperplane defined by the tangent.\n- Use the Jacobian $\\dfrac{\\partial F}{\\partial x}(x,\\lambda)$ and $\\dfrac{\\partial F}{\\partial \\lambda}(x,\\lambda)$ to assemble the linear systems required by both the tangent computation and the Newton corrector.\n- For robustness, if a direct solve of the Newton system fails due to near-singularity, use a least-squares alternative consistent with the linearization.\n\nTest suite:\nImplement your program to run the following three test cases, each specified by an initial solution $(x_0,\\lambda_0)$ that satisfies $F(x_0,\\lambda_0)=0$, a step size $\\Delta s$, a number of continuation steps $N$, and Newton solver parameters (tolerance $\\varepsilon$ and maximum iterations $k_{\\max}$). Angles do not appear in this problem, so no angle unit is required. There are no physical units in this problem.\n\n- Case $1$ (general happy path):\n  - Initial solution: $x_0 = (0.0, 0.0)$, $\\lambda_0 = 0.0$.\n  - Step size: $\\Delta s = 0.2$.\n  - Number of steps: $N = 12$.\n  - Newton tolerance: $\\varepsilon = 10^{-12}$.\n  - Maximum Newton iterations: $k_{\\max} = 20$.\n\n- Case $2$ (near a turning point where local parameter continuation would fail):\n  - Initial solution: $x_0 = (0.3, 0.273)$, $\\lambda_0 = -0.183$.\n  - Step size: $\\Delta s = 0.1$.\n  - Number of steps: $N = 20$.\n  - Newton tolerance: $\\varepsilon = 10^{-12}$.\n  - Maximum Newton iterations: $k_{\\max} = 20$.\n\n- Case $3$ (approaching another turning point):\n  - Initial solution: $x_0 = (-0.8, -0.288)$, $\\lambda_0 = 0.928$.\n  - Step size: $\\Delta s = 0.1$.\n  - Number of steps: $N = 15$.\n  - Newton tolerance: $\\varepsilon = 10^{-12}$.\n  - Maximum Newton iterations: $k_{\\max} = 25$.\n\nAnswer specification:\n- For each case, run the continuation method for exactly $N$ predictor-corrector steps and return the final triple $(x_{\\text{end}},\\lambda_{\\text{end}})$.\n- Express each final triple as a list of three real numbers `[x_1_end, x_2_end, lambda_end]`, where each number is rounded to six decimal places.\n- Your program should produce a single line of output containing the results of all three cases as a comma-separated list enclosed in square brackets, with each case’s result itself being a bracketed comma-separated list. For example, the format must be exactly like `[[a_1,a_2,a_3],[b_1,b_2,b_3],[c_1,c_2,c_3]]` where each $a_i$, $b_i$, and $c_i$ is a float rounded to six decimals.\n\nScientific realism and derivation expectations:\n- Begin from the core definitions of the Jacobian matrix and linearization of $F(x,\\lambda)=0$ to justify the predictor step based on a tangent direction at a solution point.\n- Justify the corrector step by constructing an augmented system that enforces the original constraints and a scalar constraint defining motion within a hyperplane orthogonal to the tangent-based prediction.\n- Derive the need to use stable linear algebra (such as Singular Value Decomposition (SVD)) to obtain a reliable tangent direction even near folds, and explain how the augmented Newton step overcomes singularities of $\\dfrac{\\partial F}{\\partial x}(x,\\lambda)$ at turning points.",
            "solution": "The problem requires the implementation of a predictor-corrector continuation method to trace the solution curve of a parameterized nonlinear system. The system is defined as $F(x,\\lambda)=0$, where $x \\in \\mathbb{R}^2$, $\\lambda \\in \\mathbb{R}$, and the function $F:\\mathbb{R}^2\\times\\mathbb{R}\\to\\mathbb{R}^2$ is given by\n$$\nF(x,\\lambda) = \\begin{pmatrix} F_1(x,\\lambda) \\\\ F_2(x,\\lambda) \\end{pmatrix} = \\begin{pmatrix} x_1^3 - x_1 + x_2 \\\\ x_2 + \\lambda - x_1^2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe set of solutions $(x_1, x_2, \\lambda)$ constitutes a one-dimensional manifold (a curve) in $\\mathbb{R}^3$. The objective is to numerically follow this curve. To facilitate this, we define a unified state vector $y = (x_1, x_2, \\lambda)^T \\in \\mathbb{R}^3$, allowing the system to be written compactly as $F(y)=0$, with $F: \\mathbb{R}^3 \\to \\mathbb{R}^2$.\n\nThe foundation of the continuation method rests upon the local geometric properties of the solution curve. By differentiating the identity $F(y(s))=0$ with respect to an arclength parameter $s$, we obtain the condition $DF(y(s))\\frac{dy}{ds} = 0$, where $DF(y)$ is the Jacobian of $F$ with respect to $y$. This implies that the tangent vector to the curve, $\\tau = \\frac{dy}{ds}$, must lie in the null space of the Jacobian $DF(y)$. The Jacobian for the given system is a $2 \\times 3$ matrix:\n$$\nDF(y) = \\left[ \\frac{\\partial F}{\\partial x} \\middle| \\frac{\\partial F}{\\partial \\lambda} \\right] = \\begin{pmatrix} \\frac{\\partial F_1}{\\partial x_1} & \\frac{\\partial F_1}{\\partial x_2} & \\frac{\\partial F_1}{\\partial \\lambda} \\\\ \\frac{\\partial F_2}{\\partial x_1} & \\frac{\\partial F_2}{\\partial x_2} & \\frac{\\partial F_2}{\\partial \\lambda} \\end{pmatrix} = \\begin{pmatrix} 3x_1^2 - 1 & 1 & 0 \\\\ -2x_1 & 1 & 1 \\end{pmatrix}.\n$$\nAssuming $DF(y)$ has full rank (rank $2$), its null space is one-dimensional, uniquely defining the tangent direction at point $y$.\n\nThe algorithm proceeds in two main steps: a predictor step and a corrector step.\n\n**Predictor Step**:\nGiven a known solution point $y_k$ on the curve, the predictor step extrapolates to an approximate next point $y_{k+1}^*$ by advancing a fixed distance $\\Delta s$ along the tangent direction $\\tau_k$.\n$$\ny_{k+1}^* = y_k + \\Delta s \\cdot \\tau_k.\n$$\nThe tangent vector $\\tau_k$ is computed as the normalized basis vector of the null space of $DF(y_k)$. A numerically robust method to find this vector is the Singular Value Decomposition (SVD). For a matrix $DF(y_k) = U \\Sigma V^T$, the last column of the orthogonal matrix $V$ (corresponding to the smallest singular value) provides a basis for the null space. Thus, we set $\\tau_k$ to be this vector. To ensure the continuation method traverses the curve in a consistent direction, the sign of $\\tau_k$ is chosen such that its dot product with the previous tangent vector, $\\tau_{k-1}$, is non-negative: $\\tau_k^T \\tau_{k-1} \\ge 0$. For the initial step ($k=0$), a conventional choice is to orient $\\tau_0$ such that its $\\lambda$ component is non-negative, if possible.\n\n**Corrector Step**:\nThe predicted point $y_{k+1}^*$ generally does not satisfy $F(y_{k+1}^*)=0$. The corrector step refines this prediction to find a true solution point $y_{k+1}$ in its vicinity. To ensure a locally unique solution, we impose an additional constraint. In the pseudo-arclength continuation method, this constraint forces the final point $y_{k+1}$ to lie on the hyperplane that passes through $y_{k+1}^*$ and is orthogonal to the tangent vector $\\tau_k$. The mathematical form of this constraint is:\n$$\n\\tau_k^T (y_{k+1} - y_{k+1}^*) = 0.\n$$\nThis equation, combined with the original system $F(y_{k+1})=0$, forms an augmented, square system of $3$ equations for the $3$ unknowns in $y_{k+1}$. Let $y \\equiv y_{k+1}$, we solve the system $G(y)=0$:\n$$\nG(y) = \\begin{pmatrix} F(y) \\\\ \\tau_k^T (y - y_{k+1}^*) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis system is solved using Newton's method, with the predicted point $y_{k+1}^*$ as the initial guess. The Newton iteration is defined by $y^{(j+1)} = y^{(j)} - [DG(y^{(j)})]^{-1} G(y^{(j)})$, where $DG(y)$ is the Jacobian of the augmented system:\n$$\nDG(y) = \\begin{pmatrix} DF(y) \\\\ \\tau_k^T \\end{pmatrix} = \\begin{pmatrix} 3x_1^2 - 1 & 1 & 0 \\\\ -2x_1 & 1 & 1 \\\\ (\\tau_k)_1 & (\\tau_k)_2 & (\\tau_k)_3 \\end{pmatrix}.\n$$\nA crucial feature of this formulation is that the augmented Jacobian $DG(y)$ is generally non-singular even at turning points of the solution curve where the state-space Jacobian, $\\frac{\\partial F}{\\partial x}$, becomes singular. This allows the method to trace the curve through folds where simpler methods (like natural parameter continuation) would fail. As per the problem's robustness requirement, the linear system for the Newton step, $DG(y^{(j)}) \\Delta y = -G(y^{(j)})$, is solved using a least-squares approach to handle potential numerical instabilities or ill-conditioning. The iterative process continues until the Euclidean norm of the residual, $\\|G(y^{(j)})\\|_2$, is smaller than a given tolerance $\\varepsilon$. The converged point is accepted as the next point on the solution curve, $y_{k+1}$. This predictor-corrector cycle is repeated for the specified number of steps, $N$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a predictor-corrector continuation method to trace solution curves\n    of a parameterized nonlinear system F(x, lambda) = 0.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general happy path)\n        {'y0': [0.0, 0.0, 0.0], 'ds': 0.2, 'N': 12, 'tol': 1e-12, 'k_max': 20},\n        # Case 2 (near a turning point)\n        {'y0': [0.3, 0.273, -0.183], 'ds': 0.1, 'N': 20, 'tol': 1e-12, 'k_max': 20},\n        # Case 3 (approaching another turning point)\n        {'y0': [-0.8, -0.288, 0.928], 'ds': 0.1, 'N': 15, 'tol': 1e-12, 'k_max': 25},\n    ]\n\n    def F_func(y):\n        \"\"\"Computes the value of the nonlinear system F(y).\"\"\"\n        x1, x2, lam = y\n        F1 = x1**3 - x1 + x2\n        F2 = x2 + lam - x1**2\n        return np.array([F1, F2])\n\n    def DF_func(y):\n        \"\"\"Computes the 2x3 Jacobian matrix DF(y).\"\"\"\n        x1, _, _ = y\n        return np.array([\n            [3 * x1**2 - 1, 1, 0],\n            [-2 * x1, 1, 1]\n        ])\n\n    def run_continuation(y0, ds, N, tol, k_max):\n        \"\"\"\n        Executes the continuation algorithm for a single test case.\n\n        Args:\n            y0 (list): Initial solution [x1, x2, lambda].\n            ds (float): Step size along the curve.\n            N (int): Number of continuation steps.\n            tol (float): Tolerance for Newton's method convergence.\n            k_max (int): Maximum iterations for Newton's method.\n\n        Returns:\n            np.ndarray: The final solution point after N steps.\n        \"\"\"\n        y_current = np.array(y0, dtype=float)\n        # To maintain direction, we store the previous tangent.\n        # An initial orientation that favors increasing lambda helps start the process.\n        tau_prev = np.array([0.0, 0.0, 1.0])\n\n        for _ in range(N):\n            # --- Predictor Step ---\n            DF_current = DF_func(y_current)\n            \n            # Use Singular Value Decomposition (SVD) to find the null space of the 2x3 Jacobian.\n            # The null space is 1D and its basis vector is our tangent.\n            # For DF = U * Sigma * Vh, the last row of Vh (V transpose) is the normalized vector \n            # spanning the null space.\n            _, _, Vh = np.linalg.svd(DF_current)\n            tau_current = Vh[-1, :]\n            \n            # Orient the tangent vector to prevent the path from reversing.\n            if np.dot(tau_current, tau_prev)  0:\n                tau_current = -tau_current\n            tau_prev = tau_current\n\n            # Predict the next point along the tangent.\n            y_pred = y_current + ds * tau_current\n\n            # --- Corrector Step ---\n            # Use Newton's method to solve the augmented system G(y) = 0.\n            # G(y) = [F(y); tau^T * (y - y_pred)]\n            y_j = y_pred.copy()\n            \n            for _ in range(k_max):\n                F_val = F_func(y_j)\n                g_val = np.dot(tau_current, y_j - y_pred)\n                G_val = np.append(F_val, g_val)\n                \n                if np.linalg.norm(G_val)  tol:\n                    break  # Convergence achieved\n\n                # Jacobian of the augmented system DG(y)\n                DF_j = DF_func(y_j)\n                DG_j = np.vstack([DF_j, tau_current])\n\n                # Solve the linear system DG * delta_y = -G for the Newton step.\n                # np.linalg.lstsq is used for robustness as requested, handling near-singular cases.\n                delta_y = np.linalg.lstsq(DG_j, -G_val, rcond=None)[0]\n                \n                y_j += delta_y\n            \n            y_current = y_j  # Update point for the next continuation step\n        \n        return y_current\n\n    results = []\n    for case in test_cases:\n        y0, ds, N, tol, k_max = case['y0'], case['ds'], case['N'], case['tol'], case['k_max']\n        final_y = run_continuation(y0, ds, N, tol, k_max)\n        \n        # Format the result as a string with 6 decimal places.\n        rounded_y_str = [f\"{val:.6f}\" for val in final_y]\n        results.append(f\"[{','.join(rounded_y_str)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having built a core continuation engine, we now address a more complex feature of nonlinear systems: bifurcations, where multiple solution branches intersect. This practice guides you through implementing a branch-switching algorithm at a simple pitchfork bifurcation. You will learn how to use a perturbed auxiliary system to \"jump\" from a known solution branch onto a new one, a powerful technique for exploring the full solution landscape of a system. ",
            "id": "3217736",
            "problem": "You are asked to implement a branch-switching algorithm at a simple pitchfork bifurcation point by using a perturbed auxiliary system within the framework of continuation methods. Consider the nonlinear algebraic equation in two variables $u$ and $\\lambda$ given by the scalar residual\n$$\nF(u,\\lambda) = \\lambda\\,u - u^3,\n$$\nwhich has the symmetry $u \\mapsto -u$ and exhibits a simple pitchfork bifurcation at the point $(u,\\lambda)=(0,0)$. Your task is to write a complete, runnable program that:\n- Implements pseudo-arclength continuation to follow solution curves of $F(u,\\lambda)=0$.\n- Performs branch-switching at the bifurcation point by solving a perturbed auxiliary system with a small target value $u=\\sigma$ to seed the nontrivial branch.\n- Continues along the selected nontrivial branch and computes the corresponding solution value $u$ at prescribed target parameter values $\\lambda_{\\text{target}}$.\n\nThe derivation must start from foundational numerical analysis principles and core definitions:\n- A continuation method is a systematic procedure to trace solution manifolds of nonlinear equations by stepping along a curve of solutions and correcting by solving locally well-posed problems.\n- A pseudo-arclength method augments the nonlinear system with a linearized arclength constraint to regularize the corrector step even when the Jacobian with respect to the original variables becomes nearly singular.\n- A branch-switching event near a symmetry-breaking pitchfork can be induced by imposing an auxiliary constraint that perturbs the symmetric equilibrium, for example by fixing the state variable $u$ to a small nonzero target $u=\\sigma$.\n\nYou must implement the following algorithmic components:\n1. Construct the perturbed auxiliary system\n$$\nG(u,\\lambda;\\sigma) = \\begin{bmatrix}\nF(u,\\lambda)\\\\\nu - \\sigma\n\\end{bmatrix} = \\begin{bmatrix}\n\\lambda\\,u - u^3\\\\\nu - \\sigma\n\\end{bmatrix},\n$$\nand solve it by Newton’s method to obtain a seed $(u_0,\\lambda_0)$ on a nontrivial branch. The sign of $\\sigma$ selects the branch.\n2. For continuation, given a current solution $(u_k,\\lambda_k)$, compute a unit tangent $(t_u, t_\\lambda)$ to the solution curve using the fundamental orthogonality condition that the tangent lies in the null space of the gradient of $F$. Use a predictor $(u_{\\text{pred}},\\lambda_{\\text{pred}})=(u_k,\\lambda_k) + \\Delta s\\,(t_u,t_\\lambda)$ with fixed arclength step $\\Delta s$. Then apply a Newton corrector to solve the augmented system\n$$\nH(u,\\lambda) = \\begin{bmatrix}\nF(u,\\lambda)\\\\\nt_u (u - u_{\\text{pred}}) + t_\\lambda (\\lambda - \\lambda_{\\text{pred}})\n\\end{bmatrix} = \\mathbf{0},\n$$\nusing the $2\\times 2$ Jacobian composed of the partial derivatives of $F$ and the tangent components. Use a normalization for the tangent and an orientation that drives $t_\\lambda0$ to increase $\\lambda$.\n3. Once you have progressed along the branch, refine the solution $u$ at a prescribed $\\lambda_{\\text{target}}$ by solving $F(u,\\lambda_{\\text{target}})=0$ with Newton’s method starting from the latest continuation point; keep the sign of $u$ consistent with the chosen branch encoded by $\\sigma$.\n\nNumerical requirements:\n- Use a fixed predictor step $\\Delta s = 0.05$ in arclength units.\n- Use Newton’s method with a stopping tolerance of $10^{-12}$ on the norm of the correction, and a maximum of $12$ iterations for each corrector and auxiliary solve.\n- Ensure the tangent orientation satisfies $t_\\lambda0$ whenever possible so that continuation proceeds toward larger values of $\\lambda$.\n- Round the final $u$ values to six decimal places in the output.\n\nTest suite and output specification:\n- Your program must run four test cases specified by $(\\sigma, \\lambda_{\\text{target}})$ pairs, which probe different facets including the positive and negative branches and proximity to the bifurcation:\n    1. $(\\sigma, \\lambda_{\\text{target}}) = (0.01, 0.25)$.\n    2. $(\\sigma, \\lambda_{\\text{target}}) = (-0.02, 0.49)$.\n    3. $(\\sigma, \\lambda_{\\text{target}}) = (0.001, 0.0004)$.\n    4. $(\\sigma, \\lambda_{\\text{target}}) = (-0.0015, 0.0009)$.\n- For each test case, perform branch-switching at $(u,\\lambda)=(0,0)$ by solving the perturbed auxiliary system to obtain $(u_0,\\lambda_0)$, then use pseudo-arclength continuation steps until you have progressed to or beyond $\\lambda_{\\text{target}}$, and finally refine $u$ at $\\lambda=\\lambda_{\\text{target}}$ with a single-variable Newton method.\n- Your program should produce a single line of output containing the four resulting $u$ values, rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example: \"[u1,u2,u3,u4]\". No extra text or spaces are permitted.\n\nAngles are not used, and there are no physical units. All numeric results must be printed as plain decimal floats.",
            "solution": "The problem requires the implementation of a branch-switching algorithm for the nonlinear equation $F(u,\\lambda) = \\lambda u - u^3 = 0$, which exhibits a pitchfork bifurcation at the origin $(u,\\lambda) = (0,0)$. The solution curve consists of a trivial branch $u=0$ for all $\\lambda$, and two nontrivial branches $u = \\pm\\sqrt{\\lambda}$ for $\\lambda \\ge 0$. The goal is to compute points on one of the nontrivial branches selected by a parameter $\\sigma$. The specified algorithm consists of three main stages: seeding the branch, continuation along the branch, and refinement to a target parameter value.\n\n**1. Branch-Switching by a Perturbed Auxiliary System (Seeding)**\n\nThe core challenge at a bifurcation point, such as $(u,\\lambda)=(0,0)$, is that multiple solution branches intersect, making the choice of direction ambiguous. Standard continuation methods would typically follow the trivial branch $u=0$. To switch to a nontrivial branch, we introduce a perturbation. The problem specifies constructing an auxiliary system $G(u, \\lambda; \\sigma)$ that breaks the symmetry of the original problem. This is achieved by imposing a constraint that the solution must pass through a point with a small, non-zero $u$ value, $u=\\sigma$.\n\nThe system is defined as:\n$$\nG(u,\\lambda;\\sigma) = \\begin{bmatrix}\nF(u,\\lambda)\\\\\nu - \\sigma\n\\end{bmatrix} = \\begin{bmatrix}\n\\lambda u - u^3\\\\\nu - \\sigma\n\\end{bmatrix} = \\mathbf{0}\n$$\nThe solution to this system provides a starting point $(u_0, \\lambda_0)$ for the continuation process. By inspection, the solution is $u_0 = \\sigma$ and $\\lambda_0\\sigma - \\sigma^3 = 0$, which for $\\sigma \\ne 0$ yields $\\lambda_0 = \\sigma^2$. So, the seed point is $(u_0, \\lambda_0) = (\\sigma, \\sigma^2)$.\n\nThough solvable analytically, the problem requires using Newton's method. For a state vector $\\mathbf{x} = [u, \\lambda]^T$, the iterative scheme is:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - [J_G(\\mathbf{x}_k)]^{-1} G(\\mathbf{x}_k; \\sigma)\n$$\nwhere $J_G$ is the Jacobian matrix of $G$:\n$$\nJ_G(u, \\lambda) = \\begin{bmatrix}\n\\frac{\\partial F}{\\partial u}  \\frac{\\partial F}{\\partial \\lambda} \\\\\n1  0\n\\end{bmatrix} = \\begin{bmatrix}\n\\lambda - 3u^2  u \\\\\n1  0\n\\end{bmatrix}\n$$\nA robust initial guess for this Newton solve is $(\\sigma, 0)$, which is close to the exact solution $(\\sigma, \\sigma^2)$ for small $\\sigma$.\n\n**2. Pseudo-Arclength Continuation**\n\nOnce a point $(u_k, \\lambda_k)$ on the desired branch is found, we trace the solution curve using a predictor-corrector scheme. The pseudo-arclength method is employed to handle the problem of parameterization, particularly near turning points where the parameter $\\lambda$ would no longer be a good local coordinate for the curve.\n\n**2.1. Predictor Step**\nFirst, we compute a tangent vector $(t_u, t_\\lambda)$ to the solution curve at the current point $(u_k, \\lambda_k)$. The solution curve is a level set of $F(u,\\lambda)=0$, so the tangent vector must be orthogonal to the gradient vector $\\nabla F = (\\frac{\\partial F}{\\partial u}, \\frac{\\partial F}{\\partial \\lambda})$. A vector orthogonal to $(\\frac{\\partial F}{\\partial u}, \\frac{\\partial F}{\\partial \\lambda})$ is given by $(\\frac{\\partial F}{\\partial \\lambda}, -\\frac{\\partial F}{\\partial u})$. For our problem, this is:\n$$\n(t_u, t_\\lambda)_{\\text{unnormalized}} \\propto (u_k, -(\\lambda_k - 3u_k^2)) = (u_k, 3u_k^2 - \\lambda_k)\n$$\nTo ensure consistent direction along the path, this vector is normalized to unit length, and its orientation is fixed such that $t_\\lambda  0$, forcing the continuation to proceed towards increasing $\\lambda$ values whenever possible.\n\nWith the unit tangent $(t_u, t_\\lambda)$, a predictor point $(u_{\\text{pred}}, \\lambda_{\\text{pred}})$ is calculated by taking a step of size $\\Delta s$ from $(u_k, \\lambda_k)$ in the tangent direction:\n$$\n(u_{\\text{pred}}, \\lambda_{\\text{pred}}) = (u_k, \\lambda_k) + \\Delta s (t_u, t_\\lambda)\n$$\nwhere $\\Delta s = 0.05$ is the specified arclength step size.\n\n**2.2. Corrector Step**\nThe predictor point lies on the tangent line and is generally not on the solution curve itself. The corrector step finds a nearby point on the curve. This is formulated as solving an augmented system $H(u, \\lambda) = \\mathbf{0}$ that consists of the original equation and a constraint that the correction occurs in a direction perpendicular to the tangent vector, passing through the predictor point:\n$$\nH(u,\\lambda) = \\begin{bmatrix}\nF(u,\\lambda)\\\\\nt_u (u - u_{\\text{pred}}) + t_\\lambda (\\lambda - \\lambda_{\\text{pred}})\n\\end{bmatrix} = \\mathbf{0}\n$$\nThis system is solved using Newton's method, with $(u_{\\text{pred}}, \\lambda_{\\text{pred}})$ as the initial guess. The Jacobian of this system, $J_H$, is:\n$$\nJ_H(u, \\lambda) = \\begin{bmatrix}\n\\frac{\\partial F}{\\partial u}  \\frac{\\partial F}{\\partial \\lambda} \\\\\nt_u  t_\\lambda\n\\end{bmatrix} = \\begin{bmatrix}\n\\lambda - 3u^2  u \\\\\nt_u  t_\\lambda\n\\end{bmatrix}\n$$\nThis Jacobian is non-singular at regular points and also at simple turning points, which makes the pseudo-arclength method robust. The solution of this Newton iteration gives the next point on the curve, $(u_{k+1}, \\lambda_{k+1})$. This predictor-corrector cycle is repeated until the parameter value $\\lambda$ has crossed the target value, i.e., $\\lambda_{k+1} \\ge \\lambda_{\\text{target}}$.\n\n**3. Solution Refinement**\n\nThe final continuation point, say $(u_c, \\lambda_c)$, has $\\lambda_c \\ge \\lambda_{\\text{target}}$ but generally $\\lambda_c \\ne \\lambda_{\\text{target}}$. To find the precise solution $u$ at $\\lambda = \\lambda_{\\text{target}}$, we solve the original scalar equation with the parameter fixed:\n$$\nf(u) = F(u, \\lambda_{\\text{target}}) = \\lambda_{\\text{target}} u - u^3 = 0\n$$\nThis is a root-finding problem for a single variable $u$. It is solved using Newton's method for a scalar function:\n$$\nu_{m+1} = u_m - \\frac{f(u_m)}{f'(u_m)} = u_m - \\frac{\\lambda_{\\text{target}} u_m - u_m^3}{\\lambda_{\\text{target}} - 3u_m^2}\n$$\nThe initial guess for this iteration, $u_0$, is taken from the last computed continuation point, $u_c$. This ensures that the iteration converges to the correct root on the branch being traced (i.e., the one with the same sign as $\\sigma$). The converged value of $u$ is the final answer for the given test case.\n\nThe implementation will follow these three stages for each $(\\sigma, \\lambda_{\\text{target}})$ pair provided in the test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants for the numerical methods\nTOL = 1e-12\nMAX_ITER = 12\nDS = 0.05\n\ndef F_func(u, lam):\n    \"\"\"The residual function F(u, lambda).\"\"\"\n    return lam * u - u**3\n\ndef dF_du(u, lam):\n    \"\"\"Partial derivative of F with respect to u.\"\"\"\n    return lam - 3 * u**2\n\ndef dF_dlam(u, lam):\n    \"\"\"Partial derivative of F with respect to lambda.\"\"\"\n    return u\n\ndef newton_2d(func, jac, x0, tol, max_iter):\n    \"\"\"Solves a 2D system f(x)=0 using Newton's method.\"\"\"\n    x = np.array(x0, dtype=np.float64)\n    for _ in range(max_iter):\n        J = jac(x)\n        F_val = func(x)\n        if np.abs(np.linalg.det(J))  1e-20:\n            return x, False\n        delta = np.linalg.solve(J, -F_val)\n        x += delta\n        if np.linalg.norm(delta)  tol:\n            return x, True\n    return x, False\n\ndef newton_1d(func, deriv, x0, tol, max_iter):\n    \"\"\"Solves a 1D equation f(x)=0 using Newton's method.\"\"\"\n    x = np.float64(x0)\n    for _ in range(max_iter):\n        f_val = func(x)\n        df_val = deriv(x)\n        if abs(df_val)  1e-20:\n            return x, False\n        delta = -f_val / df_val\n        x += delta\n        if abs(delta)  tol:\n            return x, True\n    return x, False\n\ndef trace_branch(sigma, lambda_target, ds):\n    \"\"\"\n    Performs branch-switching, pseudo-arclength continuation, and refinement.\n    \"\"\"\n    # Stage 1: Branch Switching (Seeding)\n    def G_seed(x):\n        u, lam = x\n        return np.array([F_func(u, lam), u - sigma], dtype=np.float64)\n\n    def J_G_seed(x):\n        u, lam = x\n        return np.array([\n            [dF_du(u, lam), dF_dlam(u, lam)],\n            [1.0, 0.0]\n        ], dtype=np.float64)\n\n    seed_guess = np.array([sigma, 0.0])\n    seed_point, converged = newton_2d(G_seed, J_G_seed, seed_guess, TOL, MAX_ITER)\n    if not converged:\n        raise RuntimeError(f\"Seeding Newton failed for sigma={sigma}\")\n    \n    u_k, lambda_k = seed_point\n\n    # Stage 2: Pseudo-Arclength Continuation\n    while lambda_k  lambda_target:\n        # --- Predictor Step ---\n        tu_un = dF_dlam(u_k, lambda_k)\n        tlam_un = -dF_du(u_k, lambda_k)\n        \n        if tlam_un  0:\n            tu_un *= -1.0\n            tlam_un *= -1.0\n            \n        norm = np.sqrt(tu_un**2 + tlam_un**2)\n        if norm  1e-15:\n            raise RuntimeError(\"Tangent norm is zero during continuation.\")\n        t_u = tu_un / norm\n        t_lambda = tlam_un / norm\n\n        u_pred = u_k + ds * t_u\n        lambda_pred = lambda_k + ds * t_lambda\n        \n        # --- Corrector Step ---\n        def H_corr(x):\n            u, lam = x\n            return np.array([\n                F_func(u, lam),\n                t_u * (u - u_pred) + t_lambda * (lam - lambda_pred)\n            ], dtype=np.float64)\n        \n        def J_H_corr(x):\n            u, lam = x\n            return np.array([\n                [dF_du(u, lam), dF_dlam(u, lam)],\n                [t_u, t_lambda]\n            ], dtype=np.float64)\n            \n        corr_guess = np.array([u_pred, lambda_pred])\n        next_point, converged = newton_2d(H_corr, J_H_corr, corr_guess, TOL, MAX_ITER)\n        \n        if not converged:\n            raise RuntimeError(f\"Corrector Newton failed for target lambda={lambda_target}\")\n            \n        u_k, lambda_k = next_point\n    \n    # Stage 3: Refinement\n    def f_refine(u):\n        return F_func(u, lambda_target)\n        \n    def df_refine(u):\n        return dF_du(u, lambda_target)\n        \n    refine_guess = u_k\n    final_u, converged = newton_1d(f_refine, df_refine, refine_guess, TOL, MAX_ITER)\n    \n    if not converged:\n        raise RuntimeError(f\"Refinement Newton failed for target lambda={lambda_target}\")\n        \n    return final_u\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (sigma, lambda_target)\n        (0.01, 0.25),\n        (-0.02, 0.49),\n        (0.001, 0.0004),\n        (-0.0015, 0.0009),\n    ]\n\n    results = []\n    for sigma, lambda_target in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        u_final = trace_branch(sigma, lambda_target, DS)\n        results.append(f\"{u_final:.6f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice demonstrates the true power and versatility of continuation methods by applying them to a problem arising from a differential equation. You will first discretize a nonlinear boundary value problem (BVP) into a large system of algebraic equations using finite differences. Then, you will apply a natural parameter continuation method to trace the solution of this BVP as a key parameter changes, linking the abstract algorithm to tangible physical or engineering models. ",
            "id": "3217870",
            "problem": "Consider the one-dimensional boundary value problem (BVP) for the unknown function $u(x)$ on the closed interval $[0,1]$ defined by the differential equation\n$$u''(x) + \\lambda\\,u(x) + u(x)^3 = \\sin(\\pi x),\\quad x \\in (0,1),$$\ntogether with homogeneous Dirichlet boundary conditions\n$$u(0) = 0,\\quad u(1) = 0.$$\nThe parameter $\\lambda$ appears explicitly in the differential equation and serves as the continuation parameter. The right-hand side uses radians for the argument of the sine function. The goal is to construct a numerical solution for this BVP at various target values of $\\lambda$ using a continuation method in the parameter $\\lambda$.\n\nStarting from a base parameter value $\\lambda_0 = 0$, implement a parameter continuation scheme that incrementally changes $\\lambda$ from $\\lambda_0$ to a specified target value $\\lambda_{\\text{end}}$. At each intermediate parameter value, solve the discretized nonlinear system using Newton's method. The computational approach must be derived from fundamental and widely accepted numerical principles:\n\n- Use a uniform grid on $[0,1]$ with $N$ interior points. Denote the grid spacing by $h = \\frac{1}{N+1}$ and the interior grid points by $x_i = i\\,h$ for $i = 1,2,\\dots,N$.\n- Approximate the second derivative $u''(x)$ with the standard three-point centered finite difference formula: for interior points,\n$$u''(x_i) \\approx \\frac{u_{i-1} - 2\\,u_i + u_{i+1}}{h^2},$$\nwhere $u_i \\approx u(x_i)$ and boundary values are $u_0 = 0$ and $u_{N+1} = 0$.\n- Formulate the discrete residual vector $\\mathbf{F}(\\mathbf{u},\\lambda)$ of length $N$ with entries\n$$F_i(\\mathbf{u},\\lambda) = \\frac{u_{i-1} - 2\\,u_i + u_{i+1}}{h^2} + \\lambda\\,u_i + u_i^3 - \\sin(\\pi x_i),\\quad i = 1,\\dots,N.$$\n- Use Newton's method to solve $\\mathbf{F}(\\mathbf{u},\\lambda) = \\mathbf{0}$ at each continuation step, with the Jacobian matrix entries derived from the finite difference stencil and the derivative of the nonlinearity:\n$$\\frac{\\partial F_i}{\\partial u_{i}} = -\\frac{2}{h^2} + \\lambda + 3\\,u_i^2,\\quad \\frac{\\partial F_i}{\\partial u_{i-1}} = \\frac{1}{h^2},\\quad \\frac{\\partial F_i}{\\partial u_{i+1}} = \\frac{1}{h^2},$$\nand zeros elsewhere. Implement a damping or backtracking strategy if necessary to ensure robust Newton convergence.\n\nThe continuation scheme must be of the natural parameter type: take uniform steps in $\\lambda$ from $\\lambda_0 = 0$ to the target $\\lambda_{\\text{end}}$, using the converged solution at the previous step as the initial guess for Newton's method at the next step.\n\nFor each test case below, after reaching the target parameter $\\lambda_{\\text{end}}$ and obtaining the converged discrete solution $\\mathbf{u}$, report the value of the solution at the midpoint $x = \\frac{1}{2}$. Use linear interpolation between neighboring grid points to approximate $u\\!\\left(\\frac{1}{2}\\right)$ if $\\frac{1}{2}$ is not exactly a grid point. Express the final reported value rounded to $6$ decimal places. There are no physical units in this problem. Angles appearing in $\\sin(\\pi x)$ must be treated in radians.\n\nImplement the program to solve the following test suite:\n\n- Test case $1$: $\\lambda_{\\text{end}} = 0$, $N = 100$, number of continuation steps $= 10$.\n- Test case $2$: $\\lambda_{\\text{end}} = 4$, $N = 120$, number of continuation steps $= 20$.\n- Test case $3$: $\\lambda_{\\text{end}} = -4$, $N = 120$, number of continuation steps $= 20$.\n- Test case $4$: $\\lambda_{\\text{end}} = 9$, $N = 150$, number of continuation steps $= 30$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_k$ is the value of $u\\!\\left(\\frac{1}{2}\\right)$ for the corresponding test case, rounded to $6$ decimal places.",
            "solution": "The problem requires solving a nonlinear boundary value problem (BVP) using a natural parameter continuation method. The BVP is defined by the differential equation $u''(x) + \\lambda u(x) + u(x)^3 = \\sin(\\pi x)$ on $[0,1]$ with boundary conditions $u(0)=0$ and $u(1)=0$.\n\nFirst, the continuous problem is converted into a system of nonlinear algebraic equations via discretization. We use a uniform grid with $N$ interior points $x_i = ih$ for $i=1,\\dots,N$, where $h=1/(N+1)$ is the grid spacing. The second derivative $u''(x_i)$ is approximated using the second-order centered finite difference formula:\n$$\nu''(x_i) \\approx \\frac{u(x_{i-1}) - 2u(x_i) + u(x_{i+1})}{h^2} = \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2}\n$$\nwhere $u_i \\approx u(x_i)$ is the discrete solution at grid point $x_i$. The boundary conditions imply $u_0=0$ and $u_{N+1}=0$. Substituting this approximation into the differential equation for each interior grid point yields a system of $N$ nonlinear equations, $\\mathbf{F}(\\mathbf{u}, \\lambda) = \\mathbf{0}$, where $\\mathbf{u} = [u_1, u_2, \\dots, u_N]^T$ is the vector of unknown solution values. The $i$-th equation is:\n$$\nF_i(\\mathbf{u}, \\lambda) = \\frac{u_{i-1} - 2u_i + u_{i+1}}{h^2} + \\lambda u_i + u_i^3 - \\sin(\\pi x_i) = 0.\n$$\nThe natural parameter continuation method is then applied. We start from a known simple case, $\\lambda_0 = 0$, and solve for $\\mathbf{u}$. The initial guess for this first solve is typically the zero vector, $\\mathbf{u}=\\mathbf{0}$. We then proceed by taking small, uniform steps in the parameter $\\lambda$ until we reach the target value $\\lambda_{\\text{end}}$. For each intermediate parameter value $\\lambda_k$, we solve the nonlinear system $\\mathbf{F}(\\mathbf{u}, \\lambda_k) = \\mathbf{0}$ using Newton's method. A key aspect of continuation is that the converged solution $\\mathbf{u}_{k-1}$ from the previous step $\\lambda_{k-1}$ provides an excellent initial guess for the Newton solver at the current step $\\lambda_k$, which ensures robust convergence.\n\nThe core of the method is the Newton iteration for solving $\\mathbf{F}(\\mathbf{u}, \\lambda) = \\mathbf{0}$. Given a guess $\\mathbf{u}^{(j)}$, the next iterate $\\mathbf{u}^{(j+1)}$ is found by solving the linear system:\n$$\nJ(\\mathbf{u}^{(j)}) \\Delta \\mathbf{u} = -\\mathbf{F}(\\mathbf{u}^{(j)})\n$$\nand updating $\\mathbf{u}^{(j+1)} = \\mathbf{u}^{(j)} + \\Delta \\mathbf{u}$. Here, $J$ is the $N \\times N$ Jacobian matrix of $\\mathbf{F}$ with respect to $\\mathbf{u}$. Due to the local nature of the finite difference stencil, the Jacobian is a sparse, tridiagonal matrix. Its non-zero entries are:\n$$\nJ_{i,j} = \\frac{\\partial F_i}{\\partial u_j} = \\begin{cases}\n1/h^2  \\text{if } j = i-1 \\\\\n-2/h^2 + \\lambda + 3u_i^2  \\text{if } j = i \\\\\n1/h^2  \\text{if } j = i+1 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nAfter the continuation process reaches $\\lambda_{\\text{end}}$ and finds the final solution vector $\\mathbf{u}_{\\text{final}}$, we determine the value at $x=0.5$. If the midpoint coincides with a grid point, the value is taken directly. Otherwise, linear interpolation between the two adjacent grid points is used to approximate $u(0.5)$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_residual(u, lam, N, h_sq, rhs_forcing):\n    \"\"\"\n    Calculates the residual vector F(u, lambda) for the discretized BVP.\n\n    The residual F represents the system of N nonlinear equations F_i = 0.\n    \"\"\"\n    # Pad the solution vector u with boundary conditions u(0)=0 and u(1)=0.\n    u_padded = np.concatenate(([0.0], u, [0.0]))\n    \n    # Approximate the second derivative u_xx using a centered finite difference stencil.\n    laplacian_u = (u_padded[:-2] - 2 * u_padded[1:-1] + u_padded[2:]) / h_sq\n    \n    # Assemble the full residual vector F_i = u_xx + lambda*u_i + u_i^3 - f_i.\n    F = laplacian_u + lam * u + u**3 - rhs_forcing\n    return F\n\ndef calculate_jacobian(u, lam, N, h_sq):\n    \"\"\"\n    Calculates the Jacobian matrix J of the residual vector F with respect to u.\n\n    The Jacobian is a tridiagonal matrix required for Newton's method.\n    \"\"\"\n    # The main diagonal of the Jacobian: dF_i/du_i = -2/h^2 + lambda + 3*u_i^2.\n    diag_vals = -2.0 / h_sq + lam + 3.0 * u**2\n    \n    # The off-diagonal entries are constant: dF_i/du_{i-1} = dF_i/du_{i+1} = 1/h^2.\n    off_diag_val = 1.0 / h_sq\n    \n    # Construct the tridiagonal Jacobian matrix using numpy's diag function.\n    J = np.diag(diag_vals)\n    if N > 1:\n        off_diag_entries = np.full(N - 1, off_diag_val)\n        J += np.diag(off_diag_entries, k=1)\n        J += np.diag(off_diag_entries, k=-1)\n        \n    return J\n\ndef newton_solver(u_guess, lam, N, h, rhs_forcing, tol=1e-9, max_iter=50):\n    \"\"\"\n    Solves the nonlinear system F(u, lambda) = 0 using Newton's method.\n    \"\"\"\n    u = u_guess.copy()\n    h_sq = h * h\n    \n    for _ in range(max_iter):\n        # 1. Calculate the residual vector F(u_k)\n        F = calculate_residual(u, lam, N, h_sq, rhs_forcing)\n        \n        # 2. Check for convergence using the infinity norm of the residual.\n        if np.linalg.norm(F, np.inf)  tol:\n            return u\n            \n        # 3. Calculate the Jacobian matrix J(u_k)\n        J = calculate_jacobian(u, lam, N, h_sq)\n        \n        # 4. Solve the linear system J * delta_u = -F for the update step.\n        delta_u = np.linalg.solve(J, -F)\n        \n        # 5. Update the solution vector: u_{k+1} = u_k + delta_u.\n        u += delta_u\n        \n    # Assumption: Newton's method converges for the given parameters.\n    return u\n\ndef perform_continuation(lambda_end, N, num_steps):\n    \"\"\"\n    Implements natural parameter continuation to solve the BVP at a target lambda.\n    \"\"\"\n    lambda_start = 0.0\n    \n    if np.isclose(lambda_end, lambda_start):\n        # If lambda does not change, only one solve is needed at lambda_start.\n        lambda_sequence = [lambda_start]\n    else:\n        # Create a sequence of lambda values from start to end.\n        # `num_steps` implies `num_steps + 1` total points in the sequence.\n        lambda_sequence = np.linspace(lambda_start, lambda_end, num_steps + 1)\n\n    # Set up the computational grid and the forcing term sin(pi*x).\n    h = 1.0 / (N + 1)\n    x_grid = np.linspace(h, 1.0 - h, N)\n    rhs_forcing = np.sin(np.pi * x_grid)\n    \n    # The initial guess for the very first step (at lambda_start=0) is the zero vector.\n    u = np.zeros(N)\n    \n    # Iterate through the lambda sequence. The converged solution from the previous\n    # step serves as the initial guess for the current step.\n    for lam in lambda_sequence:\n        u = newton_solver(u, lam, N, h, rhs_forcing)\n        \n    return u\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda_end, N, num_steps)\n        (0.0, 100, 10),\n        (4.0, 120, 20),\n        (-4.0, 120, 20),\n        (9.0, 150, 30),\n    ]\n\n    results = []\n    for lambda_end, N, num_steps in test_cases:\n        # Run the continuation method to get the final solution vector at lambda_end.\n        u_final = perform_continuation(lambda_end, N, num_steps)\n        \n        # Interpolate the solution at the midpoint x = 0.5.\n        # The ideal 0-based array index for x=0.5 is j_ideal = 0.5/h - 1.\n        h = 1.0 / (N + 1)\n        j_ideal = 0.5 * (N + 1) - 1.0\n        j_low = int(np.floor(j_ideal))\n\n        # Check if x=0.5 falls exactly on a grid point.\n        if np.isclose(j_ideal, j_low):\n            # No interpolation needed.\n            u_midpoint = u_final[j_low]\n        else:\n            # Perform linear interpolation between the two bracketing points.\n            j_high = j_low + 1\n            fractional_part = j_ideal - j_low\n            u_low_val = u_final[j_low]\n            u_high_val = u_final[j_high]\n            u_midpoint = (1.0 - fractional_part) * u_low_val + fractional_part * u_high_val\n        \n        # Format the result to 6 decimal places and add to the list.\n        results.append(f\"{u_midpoint:.6f}\")\n    \n    # Print the final output in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}