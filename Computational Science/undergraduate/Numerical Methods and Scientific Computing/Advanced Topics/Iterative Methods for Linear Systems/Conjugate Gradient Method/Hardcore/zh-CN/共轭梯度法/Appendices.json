{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的消化方式是亲自动手计算。这个练习将引导你完成共轭梯度法（CG）的单次迭代计算。通过处理一个简单的 $2 \\times 2$ 线性系统 ，你将具体操作计算残差、搜索方向和步长，从而更新解，直观地理解算法的核心机制。",
            "id": "1393666",
            "problem": "考虑线性方程组 $Ax=b$，其中矩阵 $A$ 和向量 $b$ 由下式给出：\n$$\nA = \\begin{pmatrix} 2  -1 \\\\ -1  3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}\n$$\n矩阵 $A$ 是对称正定的。\n\n从初始猜测 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，应用一次共轭梯度法的迭代来求得第一次更新后的解 $x_1$。\n\n将您的答案表示为一个行矩阵，其中包含 $x_1$ 的分量，以精确分数形式表示。",
            "solution": "我们对一个对称正定矩阵 $A$ 从 $x_{0}$ 开始应用共轭梯度法。标准的第一次迭代公式是：\n$$\nr_{0} = b - A x_{0}, \\quad p_{0} = r_{0}, \\quad \\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}}, \\quad x_{1} = x_{0} + \\alpha_{0} p_{0}.\n$$\n给定 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 和 $b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}$，计算\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}, \\quad p_{0} = r_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}.\n$$\n接下来，计算 $A p_{0}$：\n$$\nA p_{0} = \\begin{pmatrix} 2  -1 \\\\ -1  3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 4 + (-1)(-3) \\\\ (-1) \\cdot 4 + 3 \\cdot (-3) \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ -13 \\end{pmatrix}.\n$$\n计算标量积：\n$$\nr_{0}^{T} r_{0} = 4^{2} + (-3)^{2} = 16 + 9 = 25, \\quad p_{0}^{T} A p_{0} = r_{0}^{T} (A p_{0}) = 4 \\cdot 11 + (-3) \\cdot (-13) = 44 + 39 = 83.\n$$\n因此，\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}} = \\frac{25}{83}.\n$$\n更新解：\n$$\nx_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{25}{83} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} \\frac{100}{83} \\\\ -\\frac{75}{83} \\end{pmatrix}.\n$$\n表示为行矩阵，$x_1$ 的分量是 $\\begin{pmatrix} \\frac{100}{83}  -\\frac{75}{83} \\end{pmatrix}$。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{100}{83}  -\\frac{75}{83} \\end{pmatrix}}$$"
        },
        {
            "introduction": "共轭梯度法的收敛性理论严格依赖于系数矩阵的对称正定性。但是，当这一核心假设不被满足时会发生什么？这个编程练习旨在通过实践探索这一问题。通过将标准的CG算法应用于非对称矩阵 ，你将亲眼观察到算法收敛行为可能出现的剧烈震荡甚至不收敛，从而深刻理解对称性假设的基石作用。",
            "id": "2382427",
            "problem": "给定方阵、右侧向量、初始向量、容差和最大允许迭代次数。对于每种情况，从 $x_0$ 开始，通过标准的无预处理共轭梯度迭代法求解线性系统 $A x = b$ 来生成序列，并根据三个量化标准评估其行为。在整个过程中使用欧几里得向量范数。\n\n设 $A \\in \\mathbb{R}^{n \\times n}$，$b \\in \\mathbb{R}^{n}$，$x_0 \\in \\mathbb{R}^{n}$。定义初始残差 $r_0 = b - A x_0$ 和初始搜索方向 $p_0 = r_0$。对于迭代索引 $k = 0, 1, 2, \\dots$，定义\n$$\n\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k},\n\\quad\nx_{k+1} = x_k + \\alpha_k p_k,\n\\quad\nr_{k+1} = r_k - \\alpha_k A p_k,\n\\quad\n\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k},\n\\quad\np_{k+1} = r_{k+1} + \\beta_{k+1} p_k.\n$$\n如果在任何迭代中出现 $p_k^\\top A p_k = 0$，则声明为计算中断并停止。在每一步中，跟踪相对残差范数\n$$\n\\rho_k = \\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2}.\n$$\n当 $\\rho_k \\le \\tau$ 或迭代次数达到 $k_{\\max}$ 或检测到计算中断时，停止迭代。如果对于所有连续的对，都有 $\\rho_{k+1} \\le \\rho_k + \\varepsilon$，其中 $\\varepsilon = 10^{-12}$，则定义序列 $\\{\\rho_k\\}$ 表现出单调非增行为。\n\n对于下面的每个测试用例，按以下顺序生成一个包含四个条目的结果列表：\n1. 一个布尔值，指示是否在达到 $k_{\\max}$ 次迭代之前且未发生计算中断的情况下满足了停止条件 $\\rho_k \\le \\tau$。\n2. 一个整数，等于实际执行的迭代次数（从 $x_k$ 更新到 $x_{k+1}$ 的次数）。\n3. 一个浮点数，等于最终的相对残差范数 $\\rho_{\\text{final}}$，四舍五入到六位小数。\n4. 一个布尔值，指示序列 $\\{\\rho_k\\}$ 是否根据上述定义是单调非增的。\n\n测试套件（按所列顺序，严格使用这些数据）：\n\n- 情况 1（对称正定参考）：\n  - 维度 $n = 10$。\n  - 矩阵 $A_1$ 的元素为 $(A_1)_{ii} = 2$（对于 $i = 1, \\dots, n$），$(A_1)_{i,i+1} = (A_1)_{i+1,i} = -1$（对于 $i = 1, \\dots, n-1$），其他位置为零。\n  - 右侧向量 $b_1$ 的分量为 $(b_1)_i = 1$（对于 $i = 1, \\dots, n$）。\n  - 初始向量 $x_{0,1}$ 的分量为 $(x_{0,1})_i = 0$（对于 $i = 1, \\dots, n$）。\n  - 容差 $\\tau_1 = 10^{-10}$。\n  - 最大迭代次数 $k_{\\max,1} = 10$。\n\n- 情况 2（轻度非对称）：\n  - 维度 $n = 10$。\n  - 设 $S$ 为严格斜对称矩阵，其元素为 $S_{i,i+1} = 1$ 和 $S_{i+1,i} = -1$（对于 $i = 1, \\dots, n-1$），其他位置为零。\n  - 矩阵 $A_2 = A_1 + \\gamma S$，其中 $\\gamma = 0.1$。\n  - 右侧向量 $b_2 = b_1$。\n  - 初始向量 $x_{0,2} = x_{0,1}$。\n  - 容差 $\\tau_2 = 10^{-10}$。\n  - 最大迭代次数 $k_{\\max,2} = 10$。\n\n- 情况 3（强度非对称，上三角非正规）：\n  - 维度 $n = 10$。\n  - 矩阵 $A_3$ 的元素为 $(A_3)_{ii} = 2$（对于 $i = 1, \\dots, n$），$(A_3)_{ij} = 1$（对于 $i  j$），以及 $(A_3)_{ij} = 0$（对于 $i > j$）。\n  - 右侧向量 $b_3 = b_1$。\n  - 初始向量 $x_{0,3} = x_{0,1}$。\n  - 容差 $\\tau_3 = 10^{-10}$。\n  - 最大迭代次数 $k_{\\max,3} = 10$。\n\n- 情况 4（边界情况，最小维度）：\n  - 维度 $n = 1$。\n  - 矩阵 $A_4 = [2]$。\n  - 右侧向量 $b_4 = [1]$。\n  - 初始向量 $x_{0,4} = [0]$。\n  - 容差 $\\tau_4 = 10^{-14}$。\n  - 最大迭代次数 $k_{\\max,4} = 1$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果作为一个子列表，顺序与上文相同。格式必须为：\n\"[[bool,int,float,bool],[bool,int,float,bool],[bool,int,float,bool],[bool,int,float,bool]]\"\n对于浮点数，四舍五入到六位小数。输出中不涉及物理单位，不使用角度，也不出现百分比。",
            "solution": "所述问题是计算物理和数值线性代数领域一个有效且适定的练习。它具有科学依据，没有矛盾之处，并提供了进行求解所需的所有必要信息。任务是实现标准的共轭梯度（CG）算法，并在一系列明确定义的测试用例上评估其性能。\n\n共轭梯度法是一种迭代算法，设计用于求解形如 $A x = b$ 的大型稀疏线性方程组，其中矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定（SPD）的。该方法的有效性基于迭代地构造一组 $A$-正交（或共轭）的搜索方向 $\\{p_k\\}_{k=0}^{n-1}$，这些方向满足 $p_i^\\top A p_j = 0$（对于 $i \\neq j$）。这种 $A$-正交性保证了在每一步中，误差在不断扩大的 Krylov 子空间 $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ 上以 $A$-范数最小化。一个关键的推论是残差序列 $\\{r_k\\}$ 是相互正交的，即 $r_i^\\top r_j = 0$（对于 $i \\neq j$）。在精确算术中，此过程保证在至多 $n$ 次迭代内收敛到精确解。\n\n当矩阵 $A$ 不是对称的（如问题中的情况 2 和 3），CG 方法的理论基础便不再有效。搜索方向的 $A$-正交性和残差的正交性等性质均会丧失。因此，不保证收敛。残差范数 $\\lVert r_k \\rVert_2$ 可能会表现出不稳定的非单调行为，算法可能无法收敛甚至发散。所提供的测试用例旨在展示这一原理：\n- 情况 1：矩阵 $A_1$ 是一个对称正定矩阵（离散一维拉普拉斯算子），代表了 CG 方法的理想应用场景。\n- 情况 2：矩阵 $A_2$ 是 $A_1$ 的一个轻度非对称扰动。CG 方法可能仍然收敛，但其理想的性能特征，如残差的单调减少，可能会丧失。\n- 情况 3：矩阵 $A_3$ 是一个强非对称、非正规的上三角矩阵。在此应用正式的 CG 算法预计会产生较差的结果，从而展示该方法的局限性。\n- 情况 4：一个平凡的 $n=1$ 的情况，必须在单步内收敛到精确解。\n\n实现将严格遵循问题陈述中提供的算法。从初始猜测 $x_0$ 开始，我们计算初始残差 $r_0 = b - A x_0$ 和初始搜索方向 $p_0 = r_0$。对于 $k = 0, 1, 2, \\dots$ 的迭代过程涉及以下计算：\n$$\n\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k},\n\\quad\nx_{k+1} = x_k + \\alpha_k p_k,\n\\quad\nr_{k+1} = r_k - \\alpha_k A p_k,\n\\quad\n\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k},\n\\quad\np_{k+1} = r_{k+1} + \\beta_{k+1} p_k.\n$$\n迭代基于三个条件终止：\n1.  **收敛**：相对残差范数 $\\rho_k = \\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2}$ 降至指定容差 $\\tau$ 以下。\n2.  **最大迭代次数**：迭代次数达到允许的最大值 $k_{\\max}$。\n3.  **计算中断**：$\\alpha_k$ 表达式中的分母 $p_k^\\top A p_k$ 变为零。对于 SPD 矩阵，这仅在 $p_k=0$ 时发生，意味着已找到解。对于一般矩阵，这可能在 $p_k \\neq 0$ 时发生，构成算法的致命性中断。\n\n通过在每个测试用例上执行算法的轨迹，将确定四个指定的量化指标：\n1.  一个布尔值，指示是否在 $k_{\\max}$ 限制内且未发生计算中断的情况下实现了收敛（通过 $\\tau$ 条件）。\n2.  执行的总迭代次数。\n3.  最终的相对残差范数 $\\rho_{\\text{final}}$，四舍五入到六位小数。\n4.  一个布尔值，指示相对残差范数序列 $\\{\\rho_k\\}$ 是否为单调非增，其定义为对所有 $k$ 均有 $\\rho_{k+1} \\le \\rho_k + \\varepsilon$，其中容差 $\\varepsilon = 10^{-12}$ 用于考虑微小的浮点波动。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the CG solver, and print results.\n    \"\"\"\n\n    def cg_solver(A, b, x0, tau, k_max):\n        \"\"\"\n        Implements the Conjugate Gradient algorithm as specified in the problem.\n\n        Args:\n            A (np.ndarray): The square matrix.\n            b (np.ndarray): The right-hand side vector.\n            x0 (np.ndarray): The initial guess vector.\n            tau (float): The tolerance for the relative residual norm.\n            k_max (int): The maximum number of iterations.\n\n        Returns:\n            list: A list containing the four required result metrics.\n        \"\"\"\n        epsilon_monotonicity = 1e-12\n        # A small number to check against for floating point zero\n        machine_zero = 1e-40\n\n        x = np.copy(x0).astype(float)\n        r = b - A @ x\n        p = np.copy(r)\n\n        norm_r0 = np.linalg.norm(r)\n\n        # If the initial guess is already the solution\n        if norm_r0  machine_zero:\n            return [True, 0, 0.0, True]\n\n        rho_history = [1.0]\n        \n        num_iterations = 0\n        converged_by_tau = False\n        breakdown = False\n\n        r_T_r = r.T @ r\n\n        for k in range(k_max):\n            # Calculate alpha_k\n            Ap = A @ p\n            p_T_Ap = p.T @ Ap\n            \n            # Breakdown condition\n            if abs(p_T_Ap)  machine_zero:\n                breakdown = True\n                break\n\n            alpha_k = r_T_r / p_T_Ap\n\n            # Update solution and residual\n            x = x + alpha_k * p\n            r_next = r - alpha_k * Ap\n            \n            num_iterations += 1\n            \n            # Check stopping condition based on relative residual norm\n            rho_k_plus_1 = np.linalg.norm(r_next) / norm_r0\n            rho_history.append(rho_k_plus_1)\n            \n            if rho_k_plus_1 = tau:\n                converged_by_tau = True\n                r = r_next # Finalize r for correct rho reporting\n                break\n\n            # Update search direction\n            r_next_T_r_next = r_next.T @ r_next\n            \n            # Robustness check to avoid division by zero if r becomes zero\n            if r_T_r  machine_zero:\n                converged_by_tau = True # Implicitly converged\n                r = r_next\n                break\n\n            beta_k_plus_1 = r_next_T_r_next / r_T_r\n            p = r_next + beta_k_plus_1 * p\n            \n            # Update residual for the next iteration\n            r = r_next\n            r_T_r = r_next_T_r_next\n\n        # Post-processing after the loop\n        final_rho = rho_history[-1]\n        \n        is_converged_output = converged_by_tau and not breakdown\n        \n        is_monotone = True\n        for i in range(len(rho_history) - 1):\n            if rho_history[i+1] > rho_history[i] + epsilon_monotonicity:\n                is_monotone = False\n                break\n                \n        return [is_converged_output, num_iterations, round(final_rho, 6), is_monotone]\n\n    # --- Define Test Cases ---\n    \n    # Common parameters for cases 1-3\n    n = 10\n    b_common = np.ones(n, dtype=float)\n    x0_common = np.zeros(n, dtype=float)\n    \n    # Case 1: Symmetric positive definite\n    A1 = np.diag(np.full(n, 2.0)) + np.diag(np.full(n - 1, -1.0), k=1) + np.diag(np.full(n - 1, -1.0), k=-1)\n    case1 = (A1, b_common, x0_common, 1e-10, 10)\n\n    # Case 2: Mildly non-symmetric\n    gamma = 0.1\n    S = np.diag(np.ones(n - 1), k=1) - np.diag(np.ones(n - 1), k=-1)\n    A2 = A1 + gamma * S\n    case2 = (A2, b_common, x0_common, 1e-10, 10)\n\n    # Case 3: Strongly non-symmetric, upper triangular\n    A3 = np.diag(np.full(n, 2.0)) + np.triu(np.ones((n, n)), k=1)\n    case3 = (A3, b_common, x0_common, 1e-10, 10)\n    \n    # Case 4: Minimal dimension (scalar)\n    A4 = np.array([[2.0]])\n    b4 = np.array([1.0])\n    x04 = np.array([0.0])\n    case4 = (A4, b4, x04, 1e-14, 1)\n    \n    test_cases = [case1, case2, case3, case4]\n\n    results = []\n    for A, b, x0, tau, kmax in test_cases:\n        result = cg_solver(A, b, x0, tau, kmax)\n        results.append(result)\n\n    # Format output as specified: \"[[...],[...],...]\"\n    # The str() on a list gives a string representation \"[...]\"\n    # Joining these with a comma and enclosing in brackets produces the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "共轭梯度法最强大的特性之一是其“免矩阵”（matrix-free）的能力。只要我们能够计算矩阵与向量的乘积 $Av$，就可以求解线性系统，而无需显式地存储矩阵 $A$。这种方法对于矩阵过于庞大而无法存入内存的大规模科学计算问题至关重要。这个练习  将指导你实现一个真正意义上的免矩阵CG求解器，让你体会该方法在解决实际问题中的巨大威力。",
            "id": "3216688",
            "problem": "您需要设计并实现一个用于对称正定（SPD）线性系统的无矩阵共轭梯度（CG）求解器，该求解器仅使用一个实现矩阵-向量乘积 $v \\mapsto Av$ 的函数，而不显式构建矩阵 $A$。您的求解器必须从第一性原理出发：将求解 SPD 矩阵 $A$ 的 $Ax=b$ 问题视为最小化一个严格凸的二次目标函数，该目标函数的梯度是残差，并推导出一个算法，该算法迭代地寻找相对于 $A$ 相互共轭的方向，并沿着这些方向执行线搜索。您的实现必须严格只使用由回调函数提供的 $Ap$ 形式的乘积。初始猜测必须是零向量。停止规则必须使用基于初始残差范数的相对残差准则：在第一个满足 $\\lVert r_k \\rVert_2 \\le \\text{tol} \\cdot \\lVert r_0 \\rVert_2$ 的迭代 $k$ 处停止，其中 $r_k = b - Ax_k$，$r_0$ 是初始残差。如果 $\\lVert r_0 \\rVert_2 = 0$，求解器必须立即返回，迭代次数为零。求解器还必须遵守用户指定的最大迭代次数。\n\n您的程序必须将您的求解器应用于以下四个测试用例，每个用例都纯粹通过矩阵-向量乘积来描述。在每个测试中，还需从一个已知的精确解 $x_{\\star}$ 构建右端项 $b = A x_{\\star}$，以便您可以衡量计算解的误差。在所有三角函数表达式中，使用以弧度度量的角。\n\n测试用例 1（一维泊松算子）：令 $n = 10$。通过对任意 $x \\in \\mathbb{R}^n$ 的作用来隐式定义 $A$，产生 $y = Ax \\in \\mathbb{R}^n$，其分量由 $y_i = 2 x_i - x_{i-1} - x_{i+1}$ 给出，并约定 $x_0 = 0$ 和 $x_{n+1} = 0$。这对应于主对角线为 $2$、次对角线为 $-1$ 的标准三对角算子（狄利克雷边界条件）。通过 $x_{\\star,i} = \\sin\\!\\left(\\pi i/(n+1)\\right)$ 定义精确解，其中 $i = 1,2,\\dots,n$，并设置 $b = A x_{\\star}$。使用容差 $\\text{tol} = 10^{-12}$ 和最大迭代次数 $n$。报告所用的迭代次数和最大绝对误差 $\\max_i |x_i - x_{\\star,i}|$。\n\n测试用例 2（零右端项）：重用测试用例 1 中相同的算子 $A$ 和大小 $n = 10$。令 $b$ 为 $\\mathbb{R}^n$ 中的零向量，令 $x_{\\star}$ 为零向量。使用容差 $\\text{tol} = 10^{-12}$ 和最大迭代次数 $n$。报告迭代次数和最大绝对误差。\n\n测试用例 3（单位算子）：令 $n = 50$。对于所有 $x \\in \\mathbb{R}^n$，定义 $A$ 为 $Ax = x$。令精确解为 $x_{\\star,i} = \\sin(i)$，其中 $i = 1,2,\\dots,n$，并设置 $b = A x_{\\star} = x_{\\star}$。使用容差 $\\text{tol} = 10^{-12}$ 和最大迭代次数 $n$。报告迭代次数和最大绝对误差。\n\n测试用例 4（二维泊松算子）：令 $n_x = 20$ 和 $n_y = 20$，并定义 $N = n_x n_y$。将向量 $x \\in \\mathbb{R}^N$ 表示为行主序的数组 $X \\in \\mathbb{R}^{n_y \\times n_x}$。通过带有齐次狄利克雷边界条件的五点拉普拉斯算子隐式定义 $A$：对于内部索引 $(i,j)$，其中 $1 \\le i \\le n_y$ 和 $1 \\le j \\le n_x$，定义 $(AX)_{i,j} = 4 X_{i,j} - X_{i-1,j} - X_{i+1,j} - X_{i,j-1} - X_{i,j+1}$，并约定网格外的任何邻居贡献为零。然后，$(AX)$ 被展平回 $\\mathbb{R}^N$ 中的一个向量。取 $x_{\\star}$ 为 $\\mathbb{R}^N$ 中全为1的向量，并设置 $b = A x_{\\star}$。使用容差 $\\text{tol} = 10^{-8}$ 和最大迭代次数 $N$。报告迭代次数和最大绝对误差。\n\n您的程序必须实现一个单一的求解器，该求解器通过接收一个计算 $v \\mapsto Av$ 的可调用对象、一个右端项 $b$、一个容差和一个最大迭代次数来处理上述所有情况，并返回近似解和迭代次数。对于每个测试，按指定顺序计算报告的数值对：首先是迭代次数，然后是最大绝对误差。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素是对应于测试用例 1 到 4 的一个双元素列表。例如：[[i1,e1],[i2,e2],[i3,e3],[i4,e4]]，其中 $i\\ell$ 是迭代次数，$e\\ell$ 是测试用例 $\\ell$ 的最大绝对误差。",
            "solution": "此问题表述无误。它要求设计并实现一个无矩阵（matrix-free）共轭梯度（CG）求解器，该求解器从第一性原理推导得出，并将其应用于四个来自数值线性代数的、定义明确且科学上合理的测试用例。问题陈述是自洽、一致的，并提供了所有必要的参数和规范。\n\n共轭梯度法是一种求解线性方程组 $Ax=b$ 的迭代算法，其中矩阵 $A$ 是对称正定（SPD）的。问题指定求解器应为“无矩阵”的，这意味着它不能显式构造矩阵 $A$。相反，它必须仅依赖于一个函数，该函数为任意给定的向量 $v$ 计算矩阵-向量乘积 $v \\mapsto Av$。\n\n方程组 $Ax=b$ 的解等价于严格凸二次目标函数的唯一最小化子：\n$$f(x) = \\frac{1}{2}x^\\top A x - b^\\top x$$\n该函数的梯度 $\\nabla f(x) = Ax - b$ 是残差向量 $r = b - Ax$ 的负值。在解 $x_{\\star}$ 处，梯度为零，即 $Ax_{\\star} - b = 0$。\n\nCG 方法生成一系列收敛于 $x_{\\star}$ 的近似值 $x_0, x_1, \\dots$。从一个初始猜测 $x_0$ 开始，每个后续的迭代值通过沿一个选定的搜索方向 $p_k$ 移动来找到：\n$$x_{k+1} = x_k + \\alpha_k p_k$$\n步长 $\\alpha_k$ 的选择是为了最小化沿方向 $p_k$ 的目标函数。这是一个一维最小化问题，通过将 $f(x_k + \\alpha p_k)$ 对 $\\alpha$ 的导数设为零来解决：\n$$\\frac{d}{d\\alpha} f(x_k + \\alpha p_k) = \\nabla f(x_{k+1})^\\top p_k = (A(x_k + \\alpha_k p_k) - b)^\\top p_k = (Ax_k - b + \\alpha_k Ap_k)^\\top p_k = (-r_k + \\alpha_k A p_k)^\\top p_k = 0$$\n求解 $\\alpha_k$ 得到最优步长：\n$$\\alpha_k = \\frac{r_k^\\top p_k}{p_k^\\top A p_k}$$\n\nCG方法的核心创新在于搜索方向 $p_k$ 的选择。它们被选择为相互 $A$-共轭的，即满足以下条件：\n$$p_i^\\top A p_j = 0 \\quad \\text{for} \\quad i \\neq j$$\n此属性确保在第 $k$ 步沿方向 $p_k$ 执行的最小化不会干扰先前步骤中已实现的最小化。\n\n搜索方向是迭代构造的。第一个搜索方向 $p_0$ 取为初始残差 $r_0 = b - Ax_0$，这是从 $x_0$ 出发的最速下降方向。每个后续方向 $p_{k+1}$ 由新的残差 $r_{k+1}$ 构造，方法是使其与前一个方向 $p_k$ $A$-共轭。这通过设置以下方式实现：\n$$p_{k+1} = r_{k+1} + \\beta_k p_k$$\n选择系数 $\\beta_k$ 以强制实现 $A$-共轭性，$p_{k+1}^\\top A p_k = 0$。代入 $p_{k+1}$ 的表达式得到：\n$$(r_{k+1} + \\beta_k p_k)^\\top A p_k = r_{k+1}^\\top A p_k + \\beta_k p_k^\\top A p_k = 0 \\implies \\beta_k = -\\frac{r_{k+1}^\\top A p_k}{p_k^\\top A p_k}$$\n通过涉及残差和搜索方向属性的代数操作，可以简化 $\\beta_k$ 和 $\\alpha_k$ 的表达式，从而得到一个高效的算法。一个关键属性是残差是相互正交的，即对于 $i \\ne j$ 有 $r_i^\\top r_j = 0$。这导出了关系式 $p_k^\\top r_k = r_k^\\top r_k$ 以及一个计算上更简便的 $\\beta_k$ 公式。\n\n完整的算法按以下步骤进行，从指定的初始猜测 $x_0 = 0$ 开始：\n\n1.  初始化：\n    $k = 0$\n    $x_0 = 0$\n    $r_0 = b - Ax_0 = b$\n    $p_0 = r_0$\n    $\\rho_0 = r_0^\\top r_0$\n\n2.  检查平凡解：如果 $\\lVert r_0 \\rVert_2 = \\sqrt{\\rho_0} = 0$，则解为 $x_0 = 0$。以 $0$ 次迭代终止。否则，计算停止容差阈值 $\\tau = \\text{tol} \\cdot \\lVert r_0 \\rVert_2$。\n\n3.  迭代 $k = 0, 1, 2, \\dots, \\text{max\\_iter}-1$：\n    a. 计算矩阵-向量乘积：$v_k = A p_k$。\n    b. 计算步长：$\\alpha_k = \\frac{\\rho_k}{p_k^\\top v_k}$。\n    c. 更新解：$x_{k+1} = x_k + \\alpha_k p_k$。\n    d. 更新残差：$r_{k+1} = r_k - \\alpha_k v_k$。\n    e. 计算新残差的范数平方：$\\rho_{k+1} = r_{k+1}^\\top r_{k+1}$。\n    f. 检查收敛性：如果 $\\sqrt{\\rho_{k+1}} \\le \\tau$，终止并返回 $x_{k+1}$ 和迭代次数 $k+1$。\n    g. 更新搜索方向：\n       i. $\\beta_k = \\frac{\\rho_{k+1}}{\\rho_k}$\n       ii. $p_{k+1} = r_{k+1} + \\beta_k p_k$\n    h. 为下一次迭代做准备：$\\rho_k \\leftarrow \\rho_{k+1}$。\n\n4.  如果循环完成但未收敛，返回当前解 $x_{\\text{max\\_iter}}$ 和计数 $\\text{max\\_iter}$。\n\n该算法每次迭代仅需一次矩阵-向量乘积，使其非常适用于无矩阵应用。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(matvec, b, tol, max_iter):\n    \"\"\"\n    Implements a matrix-free Conjugate Gradient (CG) solver.\n\n    Solves the symmetric positive-definite linear system Ax = b using the CG\n    method. The matrix A is implicitly provided via a function `matvec` that\n    computes the matrix-vector product Av.\n\n    Args:\n        matvec (callable): A function that takes a vector v and returns Av.\n        b (np.ndarray): The right-hand side vector of the linear system.\n        tol (float): The relative tolerance for the residual norm stopping criterion.\n        max_iter (int): The maximum number of iterations allowed.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The approximate solution vector x.\n            - int: The number of iterations performed.\n    \"\"\"\n    x = np.zeros_like(b, dtype=float)\n    r = b.copy()  # Since x_0 is zero, r_0 = b - A*0 = b\n    p = r.copy()\n\n    rs_old_sq = np.dot(r, r)\n    r0_norm = np.sqrt(rs_old_sq)\n\n    if r0_norm == 0.0:\n        return x, 0\n\n    stop_norm = tol * r0_norm\n\n    for i in range(max_iter):\n        Ap = matvec(p)\n        alpha = rs_old_sq / np.dot(p, Ap)\n\n        x += alpha * p\n        r -= alpha * Ap\n\n        rs_new_sq = np.dot(r, r)\n\n        if np.sqrt(rs_new_sq) = stop_norm:\n            return x, i + 1\n\n        p = r + (rs_new_sq / rs_old_sq) * p\n        rs_old_sq = rs_new_sq\n\n    return x, max_iter\n\ndef solve():\n    \"\"\"\n    Sets up and runs the four test cases for the CG solver.\n    \"\"\"\n    # Test case 1: 1D Poisson operator\n    n1 = 10\n    def matvec1(x):\n        y = np.zeros_like(x)\n        y[0] = 2.0 * x[0] - x[1]\n        y[-1] = 2.0 * x[-1] - x[-2]\n        y[1:-1] = 2.0 * x[1:-1] - x[:-2] - x[2:]\n        return y\n    idx1 = np.arange(1, n1 + 1)\n    x_star1 = np.sin(np.pi * idx1 / (n1 + 1))\n    b1 = matvec1(x_star1)\n    tol1 = 1e-12\n    max_iter1 = n1\n\n    # Test case 2: Zero right-hand side\n    n2 = 10\n    def matvec2(x): # Same operator as case 1\n        y = np.zeros_like(x)\n        y[0] = 2.0 * x[0] - x[1]\n        y[-1] = 2.0 * x[-1] - x[-2]\n        y[1:-1] = 2.0 * x[1:-1] - x[:-2] - x[2:]\n        return y\n    x_star2 = np.zeros(n2)\n    b2 = np.zeros(n2)\n    tol2 = 1e-12\n    max_iter2 = n2\n\n    # Test case 3: Identity operator\n    n3 = 50\n    def matvec3(x):\n        return x\n    idx3 = np.arange(1, n3 + 1)\n    x_star3 = np.sin(idx3)\n    b3 = x_star3.copy()  # Since A is identity, b = x_star\n    tol3 = 1e-12\n    max_iter3 = n3\n\n    # Test case 4: 2D Poisson operator\n    nx4, ny4 = 20, 20\n    N4 = nx4 * ny4\n    def matvec4(x_flat):\n        X = x_flat.reshape((ny4, nx4))\n        Y = 4.0 * X\n        Y[1:, :] -= X[:-1, :]    # Subtract contribution from neighbor above\n        Y[:-1, :] -= X[1:, :]   # Subtract contribution from neighbor below\n        Y[:, 1:] -= X[:, :-1]   # Subtract contribution from neighbor left\n        Y[:, :-1] -= X[:, 1:]   # Subtract contribution from neighbor right\n        return Y.flatten()\n    x_star4 = np.ones(N4)\n    b4 = matvec4(x_star4)\n    tol4 = 1e-8\n    max_iter4 = N4\n\n    test_cases = [\n        (matvec1, b1, tol1, max_iter1, x_star1),\n        (matvec2, b2, tol2, max_iter2, x_star2),\n        (matvec3, b3, tol3, max_iter3, x_star3),\n        (matvec4, b4, tol4, max_iter4, x_star4),\n    ]\n\n    results = []\n    for matvec, b, tol, max_iter, x_star in test_cases:\n        x_sol, iters = conjugate_gradient(matvec, b, tol, max_iter)\n        error = np.max(np.abs(x_sol - x_star))\n        results.append([iters, error])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}