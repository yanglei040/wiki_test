{
    "hands_on_practices": [
        {
            "introduction": "The true power of iterative methods is realized when dealing with large, sparse systems, where storing the full matrix is impractical. This first practice gets to the heart of scientific computing by tasking you with implementing the Conjugate Gradient (CG) algorithm using the Compressed Sparse Row (CSR) format. Mastering the sparse matrix-vector product is the foundational skill for building efficient iterative solvers for real-world problems. ",
            "id": "3244695",
            "problem": "You are tasked with implementing an iterative solver for sparse linear systems that operates directly on a matrix stored in Compressed Sparse Row (CSR) format. The focus is on correctly implementing the matrix-vector product for the CSR structure, and then using it within an iterative method to solve a symmetric positive definite linear system. The goal is to solve systems of the form $A x = b$ where $A$ is a large sparse matrix, and to report the quality of the computed solution via residual norms.\n\nFundamental base for the derivation: Start from the definition of a linear system $A x = b$ on a finite-dimensional real vector space, assume $A$ is symmetric positive definite, and use Euclidean inner products and norms. Use the well-tested facts that minimizing a strictly convex quadratic functional yields the unique solution, and that Krylov subspace methods can be constructed to generate search directions that are conjugate with respect to $A$.\n\nYou must implement:\n- A representation of a sparse matrix in Compressed Sparse Row (CSR) format, defined by three arrays: $data$, $indices$, and $indptr$, where $indptr$ demarcates rows, $indices$ holds column indices for each nonzero, and $data$ holds corresponding nonzero values.\n- A function to compute the matrix-vector product $y = A x$ using only the CSR representation, without converting $A$ to any dense structure or using any external sparse linear algebra library function.\n- An iterative solver that uses the matrix-vector product to solve $A x = b$ for $x$, with termination based on the relative residual norm criterion $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$, where $r_k = b - A x_k$, and $\\varepsilon$ is a prescribed tolerance. Use the Conjugate Gradient (CG) method, which is appropriate for symmetric positive definite matrices.\n\nConstraints:\n- You must not form $A$ as a dense array or use any dense matrix multiplication to compute $A x$. All multiplications must be performed via the CSR matrix-vector product that you implement.\n- Your implementation should be numerically stable for the provided test cases and should handle sparse patterns correctly.\n- Use an initial guess $x_0 = 0$ unless specified otherwise.\n- Use the Euclidean two-norm $\\|\\cdot\\|_2$ for residuals and the relative residual criterion $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$ for stopping.\n\nTest suite:\nImplement the following three test cases inside your program. For each case, compute the final relative residual norm $\\|r_k\\|_2 / \\|b\\|_2$ upon termination.\n\n- Test Case $1$ (Happy path, structured symmetric positive definite system): Let $A$ be the $5$-point finite difference discretization of the two-dimensional Laplacian on an interior grid of size $3 \\times 3$ with Dirichlet boundary conditions. For each interior point, set the diagonal entry to $4$ and the entries corresponding to its valid up, down, left, and right neighbors to $-1$. This yields an $n \\times n$ matrix with $n = 9$. Let $b$ be the $n$-vector with all entries equal to $1$. Use tolerance $\\varepsilon = 10^{-10}$ and maximum iterations $k_{\\max} = 1000$.\n\n- Test Case $2$ (Boundary condition, purely diagonal symmetric positive definite system): Let $A$ be diagonal with entries $[2, 3, 5, 7]$, so $n = 4$. Let $b = [1, 2, 3, 4]$. Use tolerance $\\varepsilon = 10^{-12}$ and maximum iterations $k_{\\max} = 100$.\n\n- Test Case $3$ (Ill-conditioned symmetric positive definite system): Let $A$ be diagonal with entries $[10^{-6}, 10^{-3}, 1, 10^{3}, 10^{6}]$, so $n = 5$. Let $b = [1, 1, 1, 1, 1]$. Use tolerance $\\varepsilon = 10^{-12}$ and maximum iterations $k_{\\max} = 200$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example, $[result_1,result_2,result_3]$, where each $result_i$ is the final relative residual norm for Test Case $i$. The outputs must be real numbers (floating-point values). No additional text should be printed.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in numerical linear algebra, requiring the implementation of the Conjugate Gradient algorithm for solving a sparse symmetric positive definite linear system, with the matrix stored in Compressed Sparse Row (CSR) format. All parameters and test cases are clearly defined and constitute a standard exercise in scientific computing.\n\nThe task is to solve the linear system of equations $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a sparse, symmetric, and positive definite (SPD) matrix, $x \\in \\mathbb{R}^n$ is the unknown vector, and $b \\in \\mathbb{R}^n$ is a given vector.\n\nThe solution to this system is equivalent to finding the unique minimizer of the quadratic functional $f(x) = \\frac{1}{2} x^T A x - x^T b$. The gradient of this functional is $\\nabla f(x) = A x - b$, which is zero at the solution. Since $A$ is SPD, the Hessian of $f(x)$, which is $A$, is positive definite, confirming that $f(x)$ is strictly convex and has a unique minimum.\n\nIterative methods, particularly Krylov subspace methods, are well-suited for large sparse systems. The Conjugate Gradient (CG) method is the canonical choice for SPD systems. It constructs a sequence of search directions $\\{p_k\\}$ that are conjugate with respect to $A$ (i.e., $p_i^T A p_j = 0$ for $i \\ne j$), which guarantees convergence to the exact solution in at most $n$ iterations in exact arithmetic.\n\nThe core of any iterative method for sparse systems is an efficient matrix-vector product. We first define the Compressed Sparse Row (CSR) format and the corresponding matrix-vector product.\n\nA sparse matrix $A$ is represented in CSR format by three arrays:\n1.  `data`: A real-valued array containing all the non-zero elements of $A$ in row-major order.\n2.  `indices`: An integer array storing the column index for each corresponding value in the `data` array.\n3.  `indptr` (index pointer): An integer array of size $n+1$. The non-zero elements of row $i$ are stored in `data` from index `indptr[i]` up to (but not including) index `indptr[i+1]`. The number of non-zero elements in row $i$ is thus `indptr[i+1] - indptr[i]`.\n\nThe matrix-vector product $y = A x$ is computed as follows. For each row $i$ from $0$ to $n-1$, the component $y_i$ is given by the dot product of the $i$-th row of $A$ with $x$. In CSR format, this is:\n$$ y_i = \\sum_{k=\\text{indptr}[i]}^{\\text{indptr}[i+1]-1} \\text{data}[k] \\cdot x_{\\text{indices}[k]} $$\nThis computation avoids storing or operating on the zero elements of $A$.\n\nThe Conjugate Gradient algorithm proceeds as follows:\n1.  Initialize:\n    -   Set an initial guess $x_0$. Per the problem, $x_0 = 0$.\n    -   Compute the initial residual: $r_0 = b - A x_0$. Since $x_0 = 0$, $r_0 = b$.\n    -   Set the initial search direction: $p_0 = r_0$.\n    -   Calculate the squared norm of the initial residual: $\\rho_0 = r_0^T r_0$.\n    -   Calculate the norm of $b$: $\\|b\\|_2$.\n2.  Iterate for $k = 0, 1, 2, \\dots$ until convergence or maximum iterations:\n    a.  Compute the matrix-vector product: $v_k = A p_k$.\n    b.  Calculate the step size: $\\alpha_k = \\frac{\\rho_k}{p_k^T v_k}$.\n    c.  Update the solution vector: $x_{k+1} = x_k + \\alpha_k p_k$.\n    d.  Update the residual: $r_{k+1} = r_k - \\alpha_k v_k$.\n    e.  Check for convergence: The relative residual norm is checked against a tolerance $\\varepsilon$. If $\\|r_{k+1}\\|_2 / \\|b\\|_2 \\le \\varepsilon$, the algorithm terminates.\n    f.  Calculate the squared norm of the new residual: $\\rho_{k+1} = r_{k+1}^T r_{k+1}$.\n    g.  Compute the improvement factor for the search direction: $\\beta_k = \\frac{\\rho_{k+1}}{\\rho_k}$.\n    h.  Update the search direction: $p_{k+1} = r_{k+1} + \\beta_k p_k$.\n    i.  Set $\\rho_k \\leftarrow \\rho_{k+1}$ for the next iteration.\n\nThis procedure will be applied to the three specified test cases.\n\nTest Case $1$: The matrix $A$ is the $9 \\times 9$ matrix from a $5$-point stencil discretization of the 2D Laplacian on a $3 \\times 3$ grid. A grid point $(i, j)$ with $i, j \\in \\{0, 1, 2\\}$ is mapped to a single index $k = 3i + j$. For each row $k$, the diagonal entry $A_{k,k}$ is $4$, and entries corresponding to valid North, South, East, and West neighbors are $-1$. The vector $b$ is a vector of all ones. This matrix is known to be SPD.\n\nTest Case $2$: The matrix $A$ is a diagonal $4 \\times 4$ matrix with all diagonal entries positive, $A = \\text{diag}(2, 3, 5, 7)$. This is trivially SPD. The vector $b$ is $[1, 2, 3, 4]^T$.\n\nTest Case $3$: The matrix $A$ is a diagonal $5 \\times 5$ matrix, $A = \\text{diag}(10^{-6}, 10^{-3}, 1, 10^3, 10^6)$. It is SPD, but its condition number $\\kappa_2(A) = \\frac{\\max(\\lambda_i)}{\\min(\\lambda_i)} = \\frac{10^6}{10^{-6}} = 10^{12}$ is very large. This tests the solver's performance on an ill-conditioned problem. The vector $b$ is a vector of all ones.\n\nThe implementation will construct these matrices in CSR format and apply the CG solver, reporting the final relative residual norm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to set up and run the test cases for the Conjugate Gradient solver.\n    \"\"\"\n\n    def generate_laplacian_csr(grid_size):\n        \"\"\"\n        Generates the CSR representation of the 5-point Laplacian on a 2D grid.\n        \"\"\"\n        n = grid_size * grid_size\n        data = []\n        indices = []\n        indptr = [0]\n        \n        for k in range(n):\n            i, j = k // grid_size, k % grid_size\n            \n            # Temporary storage for one row's non-zero entries\n            row_entries = {}\n            \n            # Diagonal element\n            row_entries[k] = 4.0\n            \n            # Neighbor connections\n            # North\n            if i > 0:\n                neighbor_idx = (i - 1) * grid_size + j\n                row_entries[neighbor_idx] = -1.0\n            # South\n            if i  grid_size - 1:\n                neighbor_idx = (i + 1) * grid_size + j\n                row_entries[neighbor_idx] = -1.0\n            # West\n            if j > 0:\n                neighbor_idx = i * grid_size + (j - 1)\n                row_entries[neighbor_idx] = -1.0\n            # East\n            if j  grid_size - 1:\n                neighbor_idx = i * grid_size + (j + 1)\n                row_entries[neighbor_idx] = -1.0\n\n            # Sort by column index and append to CSR lists\n            sorted_cols = sorted(row_entries.keys())\n            for col in sorted_cols:\n                data.append(row_entries[col])\n                indices.append(col)\n            indptr.append(len(data))\n            \n        return np.array(data, dtype=float), np.array(indices, dtype=int), np.array(indptr, dtype=int), n\n\n    def csr_matvec(n, data, indices, indptr, x):\n        \"\"\"\n        Computes the matrix-vector product y = A*x for a matrix in CSR format.\n        \"\"\"\n        y = np.zeros(n, dtype=float)\n        for i in range(n):\n            row_sum = 0.0\n            start = indptr[i]\n            end = indptr[i+1]\n            for k in range(start, end):\n                j = indices[k]\n                val = data[k]\n                row_sum += val * x[j]\n            y[i] = row_sum\n        return y\n\n    def conjugate_gradient(n, data, indices, indptr, b, tol, max_iter):\n        \"\"\"\n        Solves A*x = b using the Conjugate Gradient method for a CSR matrix.\n        \"\"\"\n        x = np.zeros(n, dtype=float)\n        # For x_0 = 0, the initial residual r_0 is b.\n        r = b.copy()\n        p = r.copy()\n        rs_old = np.dot(r, r)\n        norm_b = np.linalg.norm(b)\n\n        if norm_b == 0.0:\n            return 0.0\n\n        rel_res = np.sqrt(rs_old) / norm_b\n        if rel_res  tol:\n            return rel_res\n\n        for k in range(max_iter):\n            Ap = csr_matvec(n, data, indices, indptr, p)\n            \n            p_dot_Ap = np.dot(p, Ap)\n            # If p_dot_Ap is zero or negative, the matrix might not be SPD\n            # or we might have found the exact solution.\n            if p_dot_Ap = 0:\n                break\n                \n            alpha = rs_old / p_dot_Ap\n            x += alpha * p\n            r -= alpha * Ap\n            \n            rs_new = np.dot(r, r)\n            rel_res = np.sqrt(rs_new) / norm_b\n\n            if rel_res  tol:\n                break\n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n        \n        # After loop, compute final residual for verification\n        final_residual_norm = np.linalg.norm(b - csr_matvec(n, data, indices, indptr, x))\n        final_rel_res = final_residual_norm / norm_b if norm_b > 0 else 0.0\n\n        return final_rel_res\n\n    # === Define the test cases from the problem statement. ===\n\n    # Test Case 1: 2D Laplacian on 3x3 grid\n    data1, indices1, indptr1, n1 = generate_laplacian_csr(grid_size=3)\n    b1 = np.ones(n1, dtype=float)\n    tol1 = 1e-10\n    max_iter1 = 1000\n\n    # Test Case 2: Diagonal system\n    n2 = 4\n    data2 = np.array([2.0, 3.0, 5.0, 7.0])\n    indices2 = np.array([0, 1, 2, 3], dtype=int)\n    indptr2 = np.array([0, 1, 2, 3, 4], dtype=int)\n    b2 = np.array([1.0, 2.0, 3.0, 4.0])\n    tol2 = 1e-12\n    max_iter2 = 100\n\n    # Test Case 3: Ill-conditioned diagonal system\n    n3 = 5\n    data3 = np.array([1e-6, 1e-3, 1.0, 1e3, 1e6])\n    indices3 = np.array([0, 1, 2, 3, 4], dtype=int)\n    indptr3 = np.array([0, 1, 2, 3, 4, 5], dtype=int)\n    b3 = np.ones(n3, dtype=float)\n    tol3 = 1e-12\n    max_iter3 = 200\n    \n    test_cases = [\n        (n1, data1, indices1, indptr1, b1, tol1, max_iter1),\n        (n2, data2, indices2, indptr2, b2, tol2, max_iter2),\n        (n3, data3, indices3, indptr3, b3, tol3, max_iter3),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, data, indices, indptr, b, tol, max_iter = case\n        final_rel_res = conjugate_gradient(n, data, indices, indptr, b, tol, max_iter)\n        results.append(final_rel_res)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the Conjugate Gradient method is remarkably effective, its guarantees of success are built on a firm mathematical foundation: the system matrix must be symmetric and positive definite (SPD). This exercise challenges you to explore what happens when this condition is not met. By implementing CG and applying it to a symmetric *indefinite* matrix, you will witness the algorithm's breakdown firsthand, gaining a crucial understanding of its operational boundaries. ",
            "id": "3244707",
            "problem": "Consider the task of solving linear systems of the form $A x = b$ where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and $b \\in \\mathbb{R}^{n}$. The Conjugate Gradient (CG) method is traditionally applied to symmetric positive definite matrices. Its well-posedness is grounded in the minimization of the strictly convex quadratic functional $\\varphi(x) = \\tfrac{1}{2} x^{T} A x - b^{T} x$ when $A$ is symmetric positive definite. In that case, the search directions are chosen to be mutually $A$-conjugate and the step length along each search direction is selected to enforce orthogonality of the next residual to the current search direction. For symmetric but indefinite matrices, the quadratic functional is not strictly convex, and certain curvature quantities can become non-positive, rendering the usual CG step length undefined.\n\nYour task is to implement the unpreconditioned Conjugate Gradient (CG) iteration for symmetric matrices starting from first principles and to detect the precise iteration at which the method becomes ill-defined due to a non-positive curvature quantity. The implementation must:\n\n- Start from the zero initial guess $x_{0} = 0$ for each test case.\n- Use the standard CG update logic derived from minimizing the quadratic functional along $A$-conjugate directions for symmetric positive definite matrices.\n- At iteration $k$, before computing the step length, evaluate the curvature quantity $p_{k}^{T} A p_{k}$. If $p_{k}^{T} A p_{k} \\le 0$, the algorithm must immediately stop and report a failure at iteration $k$.\n- Use the Euclidean norm $\\|r_{k}\\|_{2}$ of the residual $r_{k} = b - A x_{k}$ as the stopping criterion for successful convergence.\n- Use the tolerance $\\varepsilon = 10^{-10}$ for the residual norm convergence test.\n- Use a maximum of $m = n$ iterations for a system of dimension $n$.\n\nReturn value specification per test case:\n\n- If the method converges successfully (that is, $\\|r_{k}\\|_{2} \\le \\varepsilon$ for some $k$ without encountering $p_{k}^{T} A p_{k} \\le 0$), return the positive integer equal to the number of completed iterations $k$ at the first satisfaction of the tolerance.\n- If the method encounters $p_{k}^{T} A p_{k} \\le 0$ at iteration $k$, return the negative integer $-(k+1)$, which uniquely encodes the zero-based iteration index of failure (this avoids ambiguity with $-0$).\n\nTest suite:\n\n- Test case $1$ (symmetric positive definite, expected successful convergence): \n  - $A_{1} = \\begin{bmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{bmatrix}$,\n  - $b_{1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$,\n  - $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\varepsilon = 10^{-10}$, $m = 3$.\n- Test case $2$ (symmetric indefinite, division by zero curvature at the start):\n  - $A_{2} = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}$,\n  - $b_{2} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$,\n  - $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\varepsilon = 10^{-10}$, $m = 2$.\n- Test case $3$ (symmetric indefinite, later negative curvature):\n  - $A_{3} = \\begin{bmatrix} 2  0 \\\\ 0  -1 \\end{bmatrix}$,\n  - $b_{3} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$,\n  - $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\varepsilon = 10^{-10}$, $m = 2$.\n\nAngle units and physical units do not apply in this problem.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly three integers corresponding to the three test cases, in order. For example, a valid output could be $[a,b,c]$ where $a$, $b$, and $c$ are integers as defined above for each test case.",
            "solution": "The user-provided problem is a well-posed and scientifically grounded exercise in numerical linear algebra, requiring the implementation of the Conjugate Gradient (CG) method with a specific modification to handle symmetric indefinite matrices. The problem is valid and can be solved as stated.\n\nThe problem asks for an implementation of the unpreconditioned Conjugate Gradient (CG) method to solve the linear system $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric matrix. The standard CG method is derived for symmetric positive definite (SPD) matrices, where it is guaranteed to converge. The method's foundation lies in the iterative minimization of the quadratic functional $\\varphi(x) = \\frac{1}{2} x^T A x - b^T x$. The gradient of this functional is $\\nabla\\varphi(x) = A x - b = -r(x)$, where $r(x)$ is the residual. For an SPD matrix $A$, $\\varphi(x)$ is a strictly convex paraboloid with a unique global minimum at the solution $x$ where $\\nabla\\varphi(x) = 0$.\n\nThe CG algorithm generates a sequence of iterates $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is the search direction and $\\alpha_k$ is the step length. The search directions are constructed to be $A$-conjugate (or $A$-orthogonal), meaning $p_i^T A p_j = 0$ for $i \\neq j$. The step length $\\alpha_k$ is chosen to minimize $\\varphi(x_{k+1})$ along the direction $p_k$, leading to the formula:\n$$\n\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}\n$$\nThe quantity $p_k^T A p_k$ in the denominator is the curvature of $\\varphi(x)$ in the direction $p_k$. For an SPD matrix $A$, this term is equivalent to the squared $A$-norm of the search direction, $\\|p_k\\|_A^2$, which is strictly greater than $0$ for any non-zero $p_k$. This positivity guarantees that $\\alpha_k$ is well-defined and positive, ensuring a descent step that reduces the value of $\\varphi(x)$.\n\nHowever, if $A$ is symmetric but indefinite, it may have non-positive eigenvalues. Consequently, the quadratic functional $\\varphi(x)$ is no longer convex and may have saddle points. The curvature term $p_k^T A p_k$ can become zero or negative during the iteration.\n\\begin{itemize}\n    \\item If $p_k^T A p_k = 0$, the step length $\\alpha_k$ is undefined, causing a division by zero.\n    \\item If $p_k^T A p_k  0$, the step length $\\alpha_k$ becomes negative. The update step moves in a direction of increasing $\\varphi(x)$, violating the minimization principle of CG.\n\\end{itemize}\nIn either case, the standard CG algorithm breaks down. The problem requires us to detect this failure precisely at the iteration $k$ where $p_k^T A p_k \\le 0$.\n\nThe algorithm to be implemented is as follows, starting with $x_0 = 0$:\n\n1.  Initialize:\n    $k = 0$\n    $x_0 = 0$\n    $r_0 = b - A x_0 = b$\n    $p_0 = r_0$\n    $\\rho_0 = r_0^T r_0$\n\n2.  Iterate for $k = 0, 1, 2, \\dots, m-1$ where $m=n$ is the maximum number of iterations:\n    a. Compute the matrix-vector product $v_k = A p_k$.\n    b. **Failure Detection:** Compute the curvature $\\gamma_k = p_k^T v_k$. If $\\gamma_k \\le 0$, the method has failed. Stop and return the value $-(k+1)$.\n    c. Compute the step length: $\\alpha_k = \\rho_k / \\gamma_k$.\n    d. Update the solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n    e. Update the residual: $r_{k+1} = r_k - \\alpha_k v_k$.\n    f. **Convergence Check:** Compute the Euclidean norm of the new residual, $\\|r_{k+1}\\|_2$. If $\\|r_{k+1}\\|_2 \\le \\varepsilon = 10^{-10}$, the method has converged. Stop and return the number of completed iterations, $k+1$.\n    g. Prepare for the next iteration:\n        i.  Compute the squared norm of the new residual: $\\rho_{k+1} = r_{k+1}^T r_{k+1}$.\n        ii. Compute the coefficient for the search direction update: $\\beta_k = \\rho_{k+1} / \\rho_k$.\n        iii. Update the search direction: $p_{k+1} = r_{k+1} + \\beta_k p_k$.\n        iv. Update the squared residual norm for the next step: $\\rho_k \\leftarrow \\rho_{k+1}$.\n\nThis procedure will be applied to each of the three test cases provided. For the first case (SPD matrix), we expect successful convergence. For the second and third cases (indefinite matrices), we expect a failure due to non-positive curvature at a specific iteration. The return values are encoded as positive integers for success and negative integers for failure, as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the Conjugate Gradient method\n    with a breakdown check for symmetric indefinite matrices.\n    \"\"\"\n\n    def conjugate_gradient_solver(A, b, tolerance, max_iterations):\n        \"\"\"\n        Implements the unpreconditioned Conjugate Gradient (CG) method.\n\n        Args:\n            A (np.ndarray): A symmetric n x n matrix.\n            b (np.ndarray): A vector of size n.\n            tolerance (float): The convergence tolerance for the residual norm.\n            max_iterations (int): The maximum number of iterations.\n\n        Returns:\n            int: A positive integer k for successful convergence in k iterations,\n                 or a negative integer -(k+1) for failure at zero-based iteration k.\n        \"\"\"\n        # Start with the zero initial guess x_0 = 0.\n        x = np.zeros_like(b, dtype=float)\n        \n        # Initial residual r_0 = b - A*x_0 = b.\n        r = b.copy()\n        \n        # Initial search direction p_0 = r_0.\n        p = r.copy()\n        \n        # Squared norm of the initial residual.\n        rs_old = np.dot(r, r)\n\n        # Check for immediate convergence (if b is close to zero vector).\n        if np.sqrt(rs_old) = tolerance:\n            # Per problem, return a positive integer. 0 is not positive.\n            # This case will not be triggered by the test suite.\n            # Returning 0 would conventionally mean 0 iterations.\n            return 0\n\n        # Main iteration loop.\n        for k in range(max_iterations):\n            # Compute the matrix-vector product A*p_k.\n            Ap = np.dot(A, p)\n            \n            # Evaluate the curvature quantity p_k^T * A * p_k.\n            p_T_Ap = np.dot(p, Ap)\n            \n            # If curvature is non-positive, CG breakdown occurs.\n            if p_T_Ap = 0:\n                return -(k + 1)\n            \n            # Calculate step length alpha_k.\n            alpha = rs_old / p_T_Ap\n            \n            # Update solution: x_{k+1} = x_k + alpha_k * p_k.\n            x += alpha * p\n            \n            # Update residual: r_{k+1} = r_k - alpha_k * A*p_k.\n            r -= alpha * Ap\n            \n            # Calculate the squared norm of the new residual.\n            rs_new = np.dot(r, r)\n            \n            # Check for convergence using the Euclidean norm of the new residual.\n            if np.sqrt(rs_new) = tolerance:\n                return k + 1\n            \n            # Update search direction: p_{k+1} = r_{k+1} + beta_k * p_k, where beta_k = rs_new / rs_old.\n            p = r + (rs_new / rs_old) * p\n            \n            # Update the squared residual norm for the next iteration.\n            rs_old = rs_new\n            \n        # If the method reaches max_iterations without converging, this is also a failure.\n        # This path should not be taken for the specified test cases.\n        # A conventional return for this state might be e.g., -(max_iterations + 1).\n        # We assume the problem constraints ensure one of the explicit conditions is met.\n        return -(max_iterations + 1) # Placeholder for non-convergence.\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"A\": np.array([[4, 1, 0], [1, 3, 1], [0, 1, 2]], dtype=float),\n            \"b\": np.array([1, 2, 3], dtype=float),\n        },\n        # Test case 2\n        {\n            \"A\": np.array([[1, 0], [0, -1]], dtype=float),\n            \"b\": np.array([1, 1], dtype=float),\n        },\n        # Test case 3\n        {\n            \"A\": np.array([[2, 0], [0, -1]], dtype=float),\n            \"b\": np.array([1, 1], dtype=float),\n        },\n    ]\n\n    tolerance = 1e-10\n    results = []\n\n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        n = A.shape[0]\n        max_iter = n\n        result = conjugate_gradient_solver(A, b, tolerance, max_iter)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Krylov subspace methods, like Conjugate Gradient, are guaranteed to find the exact solution in at most $n$ steps for an $n \\times n$ system, but often they converge much faster. This practice explores an ideal scenario known as a \"lucky breakdown,\" where the method finds the exact solution in a single step. By analyzing a carefully constructed system where the initial residual is an eigenvector of the matrix, you will uncover the deep connection between Krylov subspaces, eigenvalues, and the remarkable efficiency of these algorithms. ",
            "id": "3244757",
            "problem": "Consider the symmetric, sparse matrix $A \\in \\mathbb{R}^{4 \\times 4}$ and right-hand side $b \\in \\mathbb{R}^{4}$ given by\n$$\nA = \\begin{pmatrix}\n7  0  0  0 \\\\\n0  4  -1  0 \\\\\n0  -1  3  -1 \\\\\n0  0  -1  5\n\\end{pmatrix}, \n\\quad\nb = \\begin{pmatrix}\n1 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nLet the initial guess be $x_{0} = \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0\\end{pmatrix}$, and define the initial residual $r_{0} = b - A x_{0} = b$. Consider the Lanczos process applied to $A$ starting from the normalized residual $q_{1} = r_{0} / \\|r_{0}\\|$, and recall that the Lanczos algorithm constructs an orthonormal basis for the Krylov subspace using a three-term recurrence with scalars $\\alpha_{k}$ and $\\beta_{k}$ that tridiagonalize $A$ in this basis.\n\nUsing fundamental definitions of the Krylov subspace and the Lanczos recurrence, determine whether the process exhibits a \"lucky breakdown\" at the first step and explain why such a breakdown leads to early exact solution in Krylov methods. Then, for the Conjugate Gradient (CG) method, starting from $x_{0}$ with the initial search direction equal to $r_{0}$, compute the unique scalar $\\eta$ such that the one-step update $x_{1} = x_{0} + \\eta r_{0}$ satisfies $A x_{1} = b$ exactly.\n\nYour final answer must be the single value of $\\eta$ (no units). If you choose to express $\\eta$ as a decimal, it must be exact or provided in a closed-form analytic expression; rounding is not required for this problem.",
            "solution": "The problem requires us to analyze the solution process for a linear system $Ax=b$ using a Krylov subspace method. We are given the symmetric matrix $A \\in \\mathbb{R}^{4 \\times 4}$, the vector $b \\in \\mathbb{R}^{4}$, and the initial guess $x_0 \\in \\mathbb{R}^{4}$:\n$$\nA = \\begin{pmatrix}\n7  0  0  0 \\\\\n0  4  -1  0 \\\\\n0  -1  3  -1 \\\\\n0  0  -1  5\n\\end{pmatrix}, \n\\quad\nb = \\begin{pmatrix}\n1 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix},\n\\quad\nx_{0} = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\n\nFirst, we validate the problem. The matrix $A$ is symmetric. We can check if it is positive definite. $A$ is block diagonal, with a $1 \\times 1$ block $(7)$ and a $3 \\times 3$ block. The eigenvalue of the first block is $7$. For the second block, $A' = \\begin{pmatrix} 4  -1  0 \\\\ -1  3  -1 \\\\ 0  -1  5 \\end{pmatrix}$, its leading principal minors are $4  0$, $4(3)-(-1)^2 = 11  0$, and $\\det(A') = 4(15-1) - (-1)(-5) = 56-5 = 51  0$. Since all leading principal minors are positive, $A'$ is positive definite. Therefore, $A$ is symmetric and positive definite (SPD), which is a standard condition for the convergence of the Conjugate Gradient method. The problem is well-posed and scientifically grounded.\n\nThe first step is to compute the initial residual $r_0$:\n$$\nr_0 = b - A x_0\n$$\nSince $x_0$ is the zero vector, $A x_0$ is also the zero vector. Thus,\n$$\nr_0 = b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n\nThe problem asks us to consider the Lanczos process and determine if a \"lucky breakdown\" occurs. The Lanczos process generates an orthonormal basis $\\{q_1, q_2, \\dots\\}$ for the Krylov subspaces $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$. The first vector $q_1$ is the normalized initial residual.\nThe norm of $r_0$ is $\\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 0^2 + 0^2} = 1$.\nSo, the first Lanczos vector is:\n$$\nq_1 = \\frac{r_0}{\\|r_0\\|_2} = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe Lanczos algorithm uses a three-term recurrence. Let's compute the vector $A q_1$:\n$$\nA q_1 = A r_0 = \\begin{pmatrix} 7  0  0  0 \\\\ 0  4  -1  0 \\\\ 0  -1  3  -1 \\\\ 0  0  -1  5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 7 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nWe observe that $A q_1 = 7 q_1$. This means that $q_1$ (and $r_0$) is an eigenvector of $A$ with eigenvalue $\\lambda=7$.\n\nThe Lanczos recurrence relation is given by $\\beta_{k+1} q_{k+1} = A q_k - \\alpha_k q_k - \\beta_k q_{k-1}$, with $\\beta_1 q_0 = 0$.\nFor the first step ($k=1$):\n$$\n\\beta_2 q_2 = A q_1 - \\alpha_1 q_1.\n$$\nThe coefficient $\\alpha_1$ is defined as $\\alpha_1 = q_1^T A q_1$.\n$$\n\\alpha_1 = q_1^T (7 q_1) = 7 q_1^T q_1 = 7 \\|q_1\\|_2^2 = 7(1)^2 = 7.\n$$\nNow we compute the unnormalized next vector:\n$$\n\\beta_2 q_2 = A q_1 - \\alpha_1 q_1 = 7 q_1 - 7 q_1 = 0.\n$$\nThe scalar $\\beta_{k+1}$ is the norm of the vector on the right-hand side. Thus,\n$$\n\\beta_2 = \\|A q_1 - \\alpha_1 q_1\\|_2 = \\|0\\|_2 = 0.\n$$\nA breakdown in the Lanczos process occurs when $\\beta_{k+1} = 0$ for some $k  n$. When this happens, the Krylov subspace $\\mathcal{K}_k(A, q_1)$ is an invariant subspace of $A$. In this case, since $\\beta_2=0$, the process terminates at the first step. This is termed a \"lucky breakdown\" because it implies that the exact solution of the linear system lies within the affine space $x_0 + \\mathcal{K}_1(A, r_0)$. This happens because the initial residual $r_0$ is an eigenvector of $A$. The minimal polynomial of $A$ with respect to $r_0$ has degree $1$, so the CG method is guaranteed to find the exact solution in one step.\n\nNow, we compute the scalar $\\eta$ such that the one-step update $x_1 = x_0 + \\eta r_0$ satisfies $A x_1 = b$ exactly.\nWe substitute the expression for $x_1$ into the linear system:\n$$\nA(x_0 + \\eta r_0) = b.\n$$\nUsing the linearity of the matrix-vector product, we get:\n$$\nA x_0 + \\eta A r_0 = b.\n$$\nSince $x_0 = 0$, we have $A x_0 = 0$. The equation simplifies to:\n$$\n\\eta A r_0 = b.\n$$\nWe have already computed $A r_0$:\n$$\nA r_0 = \\begin{pmatrix} 7 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nWe also know that $b = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nSubstituting these into our equation for $\\eta$:\n$$\n\\eta \\begin{pmatrix} 7 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis vector equation is equivalent to the scalar equation:\n$$\n7 \\eta = 1.\n$$\nSolving for $\\eta$, we find:\n$$\n\\eta = \\frac{1}{7}.\n$$\nThe problem statement mentions the Conjugate Gradient (CG) method. Let's verify this result using the CG formulas. In the first step of CG, the search direction is $p_0 = r_0$. The step size $\\alpha_0$ is computed as:\n$$\n\\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0} = \\frac{r_0^T r_0}{r_0^T A r_0}.\n$$\nWe compute the numerator and denominator:\n$$\nr_0^T r_0 = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1.\n$$\n$$\nr_0^T A r_0 = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix} \\begin{pmatrix} 7 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 7.\n$$\nTherefore, the CG step size is:\n$$\n\\alpha_0 = \\frac{1}{7}.\n$$\nThe updated solution is $x_1 = x_0 + \\alpha_0 p_0 = x_0 + \\frac{1}{7}r_0$. This matches the form given in the problem, with $\\eta = \\alpha_0 = \\frac{1}{7}$. The one-step update with this $\\eta$ yields the exact solution because the initial residual is an eigenvector of the matrix $A$.",
            "answer": "$$\n\\boxed{\\frac{1}{7}}\n$$"
        }
    ]
}