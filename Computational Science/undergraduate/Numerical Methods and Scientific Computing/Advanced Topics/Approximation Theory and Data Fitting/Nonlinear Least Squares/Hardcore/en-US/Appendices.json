{
    "hands_on_practices": [
        {
            "introduction": "Nonlinear least squares problems are solved iteratively, refining an initial guess until it converges to a solution. This first practice breaks down the most fundamental iterative scheme, the Gauss-Newton method, into a single, manageable step. By calculating the Jacobian matrix and solving the resulting normal equations for one update, you will gain a concrete understanding of the mechanics that drive many NLLS solvers. ",
            "id": "2191241",
            "problem": "An experimental physicist is studying the radioactive decay of a newly synthesized isotope. The activity of the sample, which is the number of decays per second, is measured at several time points. The data collected are as follows:\n\n- At time $t=0.0$ hours, the activity is $95$ Becquerels (Bq).\n- At time $t=1.0$ hour, the activity is $35$ Bq.\n- At time $t=2.0$ hours, the activity is $13$ Bq.\n\nThe theoretical model for the activity $A(t)$ as a function of time $t$ is given by the exponential decay law:\n$$ A(t; C, \\lambda) = C e^{-\\lambda t} $$\nwhere $C$ is the initial activity at $t=0$ and $\\lambda$ is the decay constant in units of inverse hours.\n\nTo estimate the parameters $C$ and $\\lambda$ from the experimental data, the physicist decides to use the Gauss-Newton method for non-linear least squares. The goal is to find the parameter vector $\\mathbf{x} = (C, \\lambda)^T$ that minimizes the sum of the squared differences between the model's predictions and the measured data.\n\nThe Gauss-Newton algorithm iteratively refines the parameter estimates. Starting with an initial guess $\\mathbf{x}_0$, the update for the parameters, $\\Delta\\mathbf{x}$, is found by solving the linear system known as the normal equations:\n$$ (J^T J) \\Delta\\mathbf{x} = -J^T \\mathbf{f} $$\nwhere $\\mathbf{f}$ is the vector of residuals (model prediction minus measured data) and $J$ is the Jacobian matrix of the model function with respect to the parameters, both evaluated at the current guess. The new estimate is then $\\mathbf{x}_1 = \\mathbf{x}_0 + \\Delta\\mathbf{x}$.\n\nStarting with the initial guess $\\mathbf{x}_0 = (C_0, \\lambda_0) = (100.0, 0.900)$, perform exactly one iteration of the Gauss-Newton method to find the updated parameter estimates $\\mathbf{x}_1 = (C_1, \\lambda_1)$.\n\nProvide the numerical values for the updated parameters $C_1$ and $\\lambda_1$. Round your final answers to three significant figures.",
            "solution": "We model the activity by $A(t;C,\\lambda)=C\\exp(-\\lambda t)$. For data $(t_{i},y_{i})$ with $(t_{0},y_{0})=(0,95)$, $(t_{1},y_{1})=(1,35)$, and $(t_{2},y_{2})=(2,13)$, the residuals at $(C,\\lambda)$ are\n$$\nr_{i}(C,\\lambda)=C\\exp(-\\lambda t_{i})-y_{i}.\n$$\nThe Jacobian of the residual vector with respect to $(C,\\lambda)$ has rows\n$$\n\\left[\\frac{\\partial r_{i}}{\\partial C},\\frac{\\partial r_{i}}{\\partial \\lambda}\\right]=\\left[\\exp(-\\lambda t_{i}),-Ct_{i}\\exp(-\\lambda t_{i})\\right].\n$$\nAt the initial guess $\\mathbf{x}_{0}=(C_{0},\\lambda_{0})=(100,0.9)$, define\n$$\na_{0}=\\exp(-0.9\\cdot 0)=1,\\quad a_{1}=\\exp(-0.9)=0.4065696597405991,\\quad a_{2}=\\exp(-1.8)=0.16529888822158653,\n$$\nand note $a_{1}^{2}=a_{2}$, $a_{2}^{2}=\\exp(-3.6)$, and $a_{2}^{3}=\\exp(-5.4)$. The model values are\n$$\nA(t_{0})=100a_{0}=100,\\quad A(t_{1})=100a_{1}=40.65696597405991,\\quad A(t_{2})=100a_{2}=16.529888822158653,\n$$\nso the residual vector $\\mathbf{f}$ (model minus data) is\n$$\n\\mathbf{f}=\\begin{pmatrix}100-95\\\\ 100a_{1}-35\\\\ 100a_{2}-13\\end{pmatrix}=\\begin{pmatrix}5\\\\ 5.65696597405991\\\\ 3.529888822158653\\end{pmatrix}.\n$$\nThe Jacobian at $\\mathbf{x}_{0}$ is\n$$\nJ=\\begin{pmatrix}\na_{0} & -C_{0}\\cdot 0\\cdot a_{0}\\\\\na_{1} & -C_{0}\\cdot 1\\cdot a_{1}\\\\\na_{2} & -C_{0}\\cdot 2\\cdot a_{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1 & 0\\\\\na_{1} & -100a_{1}\\\\\na_{2} & -200a_{2}\n\\end{pmatrix}.\n$$\nCompute $J^{T}J$ and $J^{T}\\mathbf{f}$. Using $a_{1}^{2}=a_{2}$,\n$$\nJ^{T}J=\\begin{pmatrix}\n1+a_{1}^{2}+a_{2}^{2} & a_{1}(-100a_{1})+a_{2}(-200a_{2})\\\\\na_{1}(-100a_{1})+a_{2}(-200a_{2}) & (100a_{1})^{2}+(200a_{2})^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1+a_{2}+a_{2}^{2} & -100a_{2}-200a_{2}^{2}\\\\\n-100a_{2}-200a_{2}^{2} & 10000a_{2}+40000a_{2}^{2}\n\\end{pmatrix}.\n$$\nNumerically,\n$$\nJ^{T}J=\\begin{pmatrix}\n1.1926226106688791 & -21.994633311617165\\\\\n-21.994633311617165 & 2745.937780107568\n\\end{pmatrix}.\n$$\nNext,\n$$\nJ^{T}\\mathbf{f}=\\begin{pmatrix}\na_{0}r_{0}+a_{1}r_{1}+a_{2}r_{2}\\\\\n0\\cdot r_{0}+(-100a_{1})r_{1}+(-200a_{2})r_{2}\n\\end{pmatrix}.\n$$\nUsing $r_{1}=100a_{1}-35$ and $r_{2}=100a_{2}-13$, the first component simplifies to\n$5+87a_{2}+100a_{2}^{2}-35a_{1}$,\nand the second to\n$3500a_{1}-7400a_{2}-20000a_{2}^{2}$.\nNumerically,\n$$\nJ^{T}\\mathbf{f}=\\begin{pmatrix}7.883437429086315\\\\ -346.69241269349484\\end{pmatrix}.\n$$\nThe Gauss-Newton normal equations are $(J^{T}J)\\Delta\\mathbf{x}=-J^{T}\\mathbf{f}$. Let $M=J^{T}J$ and $\\mathbf{b}=-J^{T}\\mathbf{f}=\\begin{pmatrix}-7.883437429086315\\\\ 346.69241269349484\\end{pmatrix}$. Solve $M\\Delta\\mathbf{x}=\\mathbf{b}$ by Cramer’s rule. The determinant simplifies to\n$$\n\\det(M)=(1+a_{2}+a_{2}^{2})(10000a_{2}+40000a_{2}^{2})-(100a_{2}+200a_{2}^{2})^{2}=10000a_{2}+40000a_{2}^{2}+10000a_{2}^{3},\n$$\nwhich numerically is\n$$\n\\det(M)=2791.1035895336945.\n$$\nFor $\\Delta C$, the numerator is\n$N_{1}=b_{1}M_{22}-b_{2}M_{12}=(10000a_{2}+40000a_{2}^{2})b_{1}+(100a_{2}+200a_{2}^{2})b_{2}$,\nwhich simplifies (using the expressions for $b_{1}$ and $b_{2}$) to\n$N_{1}=700000\\,a_{2}^{2}a_{1}-50000\\,a_{2}-330000\\,a_{2}^{2}-1000000\\,a_{2}^{3}$.\nNumerically,\n$$\nN_{1}=-14022.056184528922,\\qquad \\Delta C=\\frac{N_{1}}{\\det(M)}\\approx -5.0238394.\n$$\nFor $\\Delta\\lambda$, the numerator is\n$N_{2}=M_{11}b_{2}-M_{12}b_{1}=(1+a_{2}+a_{2}^{2})b_{2}+(100a_{2}+200a_{2}^{2})b_{1}$,\nwhich simplifies to\n$N_{2}=-3500a_{1}+3500a_{2}^{2}a_{1}+6900a_{2}+17700a_{2}^{2}$.\nNumerically,\n$$\nN_{2}=240.07989483777672,\\qquad \\Delta\\lambda=\\frac{N_{2}}{\\det(M)}\\approx 0.0860161.\n$$\nUpdate the parameters:\n$$\nC_{1}=C_{0}+\\Delta C=100-5.0238394\\approx 94.9762,\\quad \\lambda_{1}=\\lambda_{0}+\\Delta\\lambda=0.9+0.0860161\\approx 0.986016.\n$$\nRounded to three significant figures,\n$$\nC_{1}\\approx 95.0,\\qquad \\lambda_{1}\\approx 0.986.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}95.0 & 0.986\\end{pmatrix}}$$"
        },
        {
            "introduction": "It can be tempting to linearize a nonlinear model with a transformation, like taking a logarithm, to use simpler linear regression. This practice explores the crucial trade-offs of this approach versus tackling the nonlinear problem directly. You will implement and compare both methods to see how the underlying error structure of your data—whether additive or multiplicative—determines the most accurate and appropriate fitting strategy. ",
            "id": "3256693",
            "problem": "Consider the nonlinear model $y=\\alpha x^{\\beta}$, where $x>0$ and $\\alpha>0$. You will examine two noise models and two estimation strategies for $(\\alpha,\\beta)$: nonlinear least squares (NLS) in the original variables and ordinary least squares (OLS) after a logarithmic transformation (log-log regression). The two noise models are: additive noise, where $y_i=\\alpha x_i^{\\beta}+\\varepsilon_i$, and multiplicative noise, where $y_i=\\alpha x_i^{\\beta}e^{\\varepsilon_i}$. The additive noise $\\varepsilon_i$ is assumed to be independent and identically distributed with zero mean; the multiplicative noise is modeled as $e^{\\varepsilon_i}$ with $\\varepsilon_i$ independent and identically distributed with zero mean. Use natural logarithms for all log operations.\n\nStarting from the general definition of nonlinear least squares for parameters $\\theta=(\\alpha,\\beta)$ that minimize $\\sum_{i=1}^{n} r_i(\\theta)^2$, where $r_i(\\theta)$ denotes the residual for datum $i$, derive the Gauss–Newton method update by forming a first-order Taylor approximation of the residuals and solving the associated least squares linear problem. The derivation must begin with the residual definition $r_i(\\theta)$ in the original, non-transformed variables and proceed by defining the Jacobian matrix of residuals with respect to the parameters. The algorithm must implement a practical update with a well-defined stopping criterion that ensures convergence to a local minimizer when the initial guess is near the solution.\n\nImplement two estimators:\n- An NLS estimator in the original variables for the additive-noise model $y_i=\\alpha x_i^{\\beta}+\\varepsilon_i$, using Gauss–Newton with a damping or backtracking strategy to ensure descent. Parameterize $\\alpha$ as $e^{a}$ with $a\\in\\mathbb{R}$ to enforce $\\alpha>0$.\n- A log-log OLS estimator for the multiplicative-noise model $y_i=\\alpha x_i^{\\beta}e^{\\varepsilon_i}$, fitting $\\log(y_i)=\\log(\\alpha)+\\beta\\log(x_i)+\\varepsilon_i$ by ordinary least squares. Only use data points with $y_i>0$ (since the logarithm is undefined for nonpositive values). If fewer than two points satisfy $y_i>0$, fall back to a reasonable default initial guess when initializing NLS.\n\nFor both estimators, compute the parameter estimates $(\\widehat{\\alpha},\\widehat{\\beta})$ and evaluate their accuracy using the error metric\n$$\nE=\\sqrt{\\left(\\log(\\widehat{\\alpha})-\\log(\\alpha_{\\text{true}})\\right)^2+\\left(\\widehat{\\beta}-\\beta_{\\text{true}}\\right)^2}.\n$$\nThis error metric compares the estimates in the $(\\log(\\alpha),\\beta)$ parameterization to ensure a scale-consistent measure. The logarithm is natural.\n\nGenerate synthetic observations using a fixed pseudorandom generator seed and the following test suite. For each case, $x$ values are predefined, and you must simulate $y$ according to the model and noise type indicated. In the multiplicative noise cases, set $\\varepsilon_i\\sim\\mathcal{N}(0,\\sigma_{\\log}^2)$ and use $y_i=\\alpha x_i^{\\beta}e^{\\varepsilon_i}$. In the additive noise cases, set $\\varepsilon_i\\sim\\mathcal{N}(0,\\sigma^2)$ and use $y_i=\\alpha x_i^{\\beta}+\\varepsilon_i$. Use a single fixed seed of $42$ for all random draws to ensure reproducibility.\n\nTest suite parameter sets:\n1. Multiplicative noise (moderate): $\\alpha_{\\text{true}}=2.5$, $\\beta_{\\text{true}}=1.2$, $\\sigma_{\\log}=0.25$, $x$ on a logarithmic grid from $0.1$ to $100$ with $80$ points (inclusive).\n2. Additive noise (moderate): $\\alpha_{\\text{true}}=5.0$, $\\beta_{\\text{true}}=0.5$, $\\sigma=0.5$, $x$ on a uniform grid from $0.2$ to $10.0$ with $80$ points (inclusive).\n3. Multiplicative noise (small): $\\alpha_{\\text{true}}=1.0$, $\\beta_{\\text{true}}=2.0$, $\\sigma_{\\log}=0.05$, $x$ on a logarithmic grid from $1.0$ to $100.0$ with $60$ points (inclusive).\n4. Additive noise (small): $\\alpha_{\\text{true}}=3.0$, $\\beta_{\\text{true}}=1.1$, $\\sigma=0.05$, $x$ on a logarithmic grid from $0.1$ to $50.0$ with $60$ points (inclusive).\n5. Multiplicative noise (heavy): $\\alpha_{\\text{true}}=10.0$, $\\beta_{\\text{true}}=0.3$, $\\sigma_{\\log}=0.6$, $x$ on a logarithmic grid from $0.5$ to $50.0$ with $40$ points (inclusive).\n\nFor each test case, compute two estimates: one by NLS in the original variables and the other by log-log OLS. Then evaluate which method is better for that case by the error metric $E$. A method is considered better if it achieves a strictly smaller error $E$ than the other method.\n\nFinal output format specification:\n- For each test case $k$ in the order listed above, produce a boolean indicating whether the method that theoretically matches the noise model achieves lower error than the alternative method. That is, output $\\text{True}$ for cases $1$, $3$, and $5$ if the log-log OLS error is lower than the NLS error, and output $\\text{True}$ for cases $2$ and $4$ if the NLS error is lower than the log-log OLS error. Output $\\text{False}$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,True,False]\").",
            "solution": "The problem requires the derivation and implementation of two estimators for the parameters $(\\alpha, \\beta)$ of the nonlinear model $y = \\alpha x^{\\beta}$. The first estimator is based on Nonlinear Least Squares (NLS) for an additive noise model, implemented with the Gauss-Newton algorithm. The second is an Ordinary Least Squares (OLS) estimator applied to a logarithmic transformation of the model, which is appropriate for multiplicative noise.\n\n### Derivation of the Gauss-Newton Method ###\n\nThe NLS estimator is designed for the additive noise model, $y_i = \\alpha x_i^{\\beta} + \\varepsilon_i$. The goal of least squares is to find the parameter values that minimize the sum of squared residuals. The residual for the $i$-th data point is the difference between the observed value $y_i$ and the model's prediction, $f(x_i; \\theta)$.\n\nThe parameter vector is $\\theta = (a, \\beta)$, where we use the reparameterization $\\alpha = e^a$ to enforce the constraint $\\alpha > 0$. The model function is thus $f(x_i; a, \\beta) = e^a x_i^{\\beta}$.\nThe residuals are given by:\n$$\nr_i(a, \\beta) = y_i - f(x_i; a, \\beta) = y_i - e^a x_i^{\\beta}\n$$\nThe objective function to be minimized is the sum of squared residuals, $S(\\theta)$:\n$$\nS(a, \\beta) = \\sum_{i=1}^{n} r_i(a, \\beta)^2 = \\|\\mathbf{r}(a, \\beta)\\|_2^2\n$$\nwhere $\\mathbf{r}$ is the vector of residuals $[r_1, r_2, \\dots, r_n]^T$.\n\nThe Gauss-Newton algorithm is an iterative procedure for solving nonlinear least squares problems. Starting from an initial guess $\\theta_k = (a_k, \\beta_k)$, it finds a step $\\Delta\\theta = (\\Delta a, \\Delta \\beta)$ to update the estimate to $\\theta_{k+1} = \\theta_k + \\Delta\\theta$. The step is found by linearizing the residual vector $\\mathbf{r}$ around $\\theta_k$ using a first-order Taylor expansion:\n$$\n\\mathbf{r}(\\theta_k + \\Delta\\theta) \\approx \\mathbf{r}(\\theta_k) + \\mathbf{J}(\\theta_k) \\Delta\\theta\n$$\nHere, $\\mathbf{J}(\\theta_k)$ is the Jacobian matrix of the residual vector $\\mathbf{r}$ with respect to the parameters $\\theta$, evaluated at $\\theta_k$. The algorithm then solves the linear least squares problem of minimizing the norm of the approximated residuals:\n$$\n\\min_{\\Delta\\theta} \\|\\mathbf{r}(\\theta_k) + \\mathbf{J}(\\theta_k) \\Delta\\theta\\|_2^2\n$$\nThe solution to this linear problem, $\\Delta\\theta$, is given by the normal equations:\n$$\n(\\mathbf{J}_k^T \\mathbf{J}_k) \\Delta\\theta = -\\mathbf{J}_k^T \\mathbf{r}_k\n$$\nwhere $\\mathbf{J}_k = \\mathbf{J}(\\theta_k)$ and $\\mathbf{r}_k = \\mathbf{r}(\\theta_k)$.\n\nTo form the Jacobian matrix $\\mathbf{J}$, we compute the partial derivatives of each residual $r_i$ with respect to the parameters $a$ and $\\beta$:\n\\begin{enumerate}\n    \\item Derivative with respect to $a$:\n    $$\n    \\frac{\\partial r_i}{\\partial a} = \\frac{\\partial}{\\partial a} (y_i - e^a x_i^\\beta) = - \\frac{\\partial}{\\partial a} (e^a x_i^\\beta) = -e^a x_i^\\beta\n    $$\n    \\item Derivative with respect to $\\beta$:\n    $$\n    \\frac{\\partial r_i}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} (y_i - e^a x_i^\\beta) = -e^a \\frac{\\partial}{\\partial \\beta} (x_i^\\beta) = -e^a x_i^\\beta \\ln(x_i)\n    $$\n\\end{enumerate}\nThe Jacobian $\\mathbf{J}$ is an $n \\times 2$ matrix, where the $i$-th row is given by:\n$$\n\\mathbf{J}_i(a, \\beta) = \\left[ \\frac{\\partial r_i}{\\partial a}, \\frac{\\partial r_i}{\\partial \\beta} \\right] = \\left[ -e^a x_i^\\beta, -e^a x_i^\\beta \\ln(x_i) \\right]\n$$\nAt each iteration $k$, the step $\\Delta\\theta_k$ is computed by solving the $2 \\times 2$ linear system of normal equations. A more numerically stable approach is to directly solve the linear least squares problem for $\\Delta\\theta_k$:\n$$\n\\mathbf{J}_k \\Delta\\theta_k \\approx -\\mathbf{r}_k\n$$\nTo ensure convergence, a full step $\\Delta\\theta_k$ may not be taken. Instead, a damped update is used:\n$$\n\\theta_{k+1} = \\theta_k + \\gamma_k \\Delta\\theta_k\n$$\nThe damping factor (or step size) $\\gamma_k \\in (0, 1]$ is chosen via a backtracking line search to satisfy the descent condition $S(\\theta_{k+1})  S(\\theta_k)$. The iterative process continues until a stopping criterion, such as the norm of the update step $\\|\\gamma_k \\Delta\\theta_k\\|$ falling below a specified tolerance, is met. The final parameters are $(\\widehat{\\alpha}, \\widehat{\\beta}) = (e^{\\widehat{a}}, \\widehat{\\beta})$.\n\n### Log-Log OLS Estimator ###\n\nThe log-log OLS estimator is suited for the multiplicative noise model, $y_i = \\alpha x_i^{\\beta}e^{\\varepsilon_i}$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\log}^2)$. Taking the natural logarithm of both sides linearizes the model:\n$$\n\\ln(y_i) = \\ln(\\alpha x_i^{\\beta}e^{\\varepsilon_i}) = \\ln(\\alpha) + \\beta\\ln(x_i) + \\varepsilon_i\n$$\nThis transformed equation is linear in the parameters $\\ln(\\alpha)$ and $\\beta$. Let $z_i = \\ln(y_i)$, $c = \\ln(\\alpha)$, and $w_i = \\ln(x_i)$. The model becomes:\n$$\nz_i = c + \\beta w_i + \\varepsilon_i\n$$\nThis is a standard linear regression model. The parameters $(c, \\beta)$ can be estimated by minimizing the sum of squared errors in the log domain, using Ordinary Least Squares (OLS). The OLS solution provides estimates $(\\hat{c}, \\hat{\\beta})$, from which we obtain the estimate for $\\alpha$ as $\\widehat{\\alpha} = e^{\\hat{c}}$. This method is only valid for data points where $y_i > 0$.\n\n### Implementation Strategy ###\n\nFor each test case, synthetic data $(x_i, y_i)$ are generated according to the specified model (additive or multiplicative noise).\nBoth estimators are applied to the generated data:\n\\begin{enumerate}\n    \\item The log-log OLS estimator is computed first. It provides the parameter estimates $(\\widehat{\\alpha}_{\\text{OLS}}, \\widehat{\\beta}_{\\text{OLS}})$ and also serves as a high-quality initial guess for the NLS algorithm. If OLS fails (due to fewer than two positive $y_i$ values), a default guess is used.\n    \\item The NLS estimator is then run using the Gauss-Newton method with backtracking line search, starting from the OLS-derived or default initial guess, to obtain $(\\widehat{\\alpha}_{\\text{NLS}}, \\widehat{\\beta}_{\\text{NLS}})$.\n\\end{enumerate}\nThe accuracy of each estimate $(\\widehat{\\alpha}, \\widehat{\\beta})$ is evaluated using the error metric $E$:\n$$\nE = \\sqrt{(\\ln(\\widehat{\\alpha}) - \\ln(\\alpha_{\\text{true}}))^2 + (\\widehat{\\beta} - \\beta_{\\text{true}})^2}\n$$\nFinally, for each test case, we determine if the estimation method that theoretically matches the noise model (OLS for multiplicative, NLS for additive) yields a strictly lower error $E$ than the alternative method.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef ols_estimator(x, y):\n    \"\"\"\n    Computes parameter estimates for y = alpha * x**beta using log-log OLS.\n    Returns (alpha_hat, beta_hat) or (None, None) if estimation is not possible.\n    \"\"\"\n    # Filter for positive y values, as log is undefined otherwise\n    positive_mask = y > 0\n    if np.sum(positive_mask)  2:\n        return None, None  # Not enough data for regression\n\n    x_log = np.log(x[positive_mask])\n    y_log = np.log(y[positive_mask])\n\n    # OLS for: log(y) = log(alpha) + beta * log(x)\n    # This is a linear model Z = A*p, where Z=y_log, p=[log(alpha), beta]\n    A = np.vstack([np.ones_like(x_log), x_log]).T\n    \n    # Solve the linear least squares problem\n    p, _, _, _ = np.linalg.lstsq(A, y_log, rcond=None)\n    \n    log_alpha_hat, beta_hat = p[0], p[1]\n    alpha_hat = np.exp(log_alpha_hat)\n    \n    return alpha_hat, beta_hat\n\ndef nls_estimator(x, y, initial_guess, max_iter=100, tol=1e-8):\n    \"\"\"\n    Computes parameter estimates for y = alpha * x**beta + eps using NLS.\n    Uses Gauss-Newton with backtracking line search.\n    \"\"\"\n    alpha_0, beta_0 = initial_guess\n    # Work with theta = [log(alpha), beta] to enforce alpha > 0\n    a_k = np.log(alpha_0) if alpha_0 > 0 else 0.0\n    b_k = beta_0\n    \n    for _ in range(max_iter):\n        alpha_k = np.exp(a_k)\n        y_pred = alpha_k * (x ** b_k)\n        \n        # Residuals\n        r = y - y_pred\n        S_k = np.sum(r**2)\n        \n        # Jacobian matrix w.r.t. parameters [a, beta]\n        # d(r)/da = -exp(a)*x^b = -y_pred\n        # d(r)/db = -exp(a)*x^b*log(x) = -y_pred*log(x)\n        J = np.vstack([-y_pred, -y_pred * np.log(x)]).T\n        \n        # Solve J*delta_theta = -r using least squares\n        try:\n            delta_theta, _, _, _ = np.linalg.lstsq(J, -r, rcond=None)\n        except np.linalg.LinAlgError:\n            # Fails if Jacobian is singular, break iteration\n            break\n            \n        # Backtracking line search\n        gamma = 1.0\n        for _ in range(10): # Max 10 backtracking steps\n            a_next, b_next = a_k + gamma * delta_theta[0], b_k + gamma * delta_theta[1]\n            y_pred_next = np.exp(a_next) * (x ** b_next)\n            S_next = np.sum((y - y_pred_next)**2)\n            \n            if S_next  S_k:\n                a_k, b_k = a_next, b_next\n                break\n            gamma /= 2.0\n        else: # Line search failed to find a better point\n            break\n            \n        # Check for convergence\n        if np.linalg.norm(gamma * delta_theta)  tol:\n            break\n            \n    alpha_hat = np.exp(a_k)\n    beta_hat = b_k\n    \n    return alpha_hat, beta_hat\n    \ndef calculate_error(alpha_hat, beta_hat, alpha_true, beta_true):\n    \"\"\"\n    Calculates the error metric E.\n    \"\"\"\n    log_alpha_hat = np.log(alpha_hat)\n    log_alpha_true = np.log(alpha_true)\n    error = np.sqrt((log_alpha_hat - log_alpha_true)**2 + (beta_hat - beta_true)**2)\n    return error\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'id': 1, 'noise_type': 'multiplicative', 'alpha_true': 2.5, 'beta_true': 1.2, 'noise_param': 0.25,\n         'x_grid': 'log', 'x_range': (0.1, 100), 'n_points': 80},\n        {'id': 2, 'noise_type': 'additive', 'alpha_true': 5.0, 'beta_true': 0.5, 'noise_param': 0.5,\n         'x_grid': 'uniform', 'x_range': (0.2, 10.0), 'n_points': 80},\n        {'id': 3, 'noise_type': 'multiplicative', 'alpha_true': 1.0, 'beta_true': 2.0, 'noise_param': 0.05,\n         'x_grid': 'log', 'x_range': (1.0, 100.0), 'n_points': 60},\n        {'id': 4, 'noise_type': 'additive', 'alpha_true': 3.0, 'beta_true': 1.1, 'noise_param': 0.05,\n         'x_grid': 'log', 'x_range': (0.1, 50.0), 'n_points': 60},\n        {'id': 5, 'noise_type': 'multiplicative', 'alpha_true': 10.0, 'beta_true': 0.3, 'noise_param': 0.6,\n         'x_grid': 'log', 'x_range': (0.5, 50.0), 'n_points': 40},\n    ]\n\n    rng = np.random.default_rng(42)\n    results = []\n    \n    for case in test_cases:\n        # Generate data\n        if case['x_grid'] == 'log':\n            x = np.logspace(np.log10(case['x_range'][0]), np.log10(case['x_range'][1]), case['n_points'])\n        else: # uniform\n            x = np.linspace(case['x_range'][0], case['x_range'][1], case['n_points'])\n            \n        y_true = case['alpha_true'] * (x ** case['beta_true'])\n        \n        if case['noise_type'] == 'multiplicative':\n            epsilon = rng.normal(0, case['noise_param'], case['n_points'])\n            y_obs = y_true * np.exp(epsilon)\n        else: # additive\n            epsilon = rng.normal(0, case['noise_param'], case['n_points'])\n            y_obs = y_true + epsilon\n\n        # OLS estimator (for multiplicative model  NLS initial guess)\n        alpha_ols, beta_ols = ols_estimator(x, y_obs)\n\n        # NLS estimator (for additive model)\n        if alpha_ols is None:\n            # OLS failed, use a neutral default guess for NLS\n            initial_guess = (1.0, 1.0)\n        else:\n            initial_guess = (alpha_ols, beta_ols)\n            \n        alpha_nls, beta_nls = nls_estimator(x, y_obs, initial_guess)\n\n        # Evaluate errors\n        error_ols = calculate_error(alpha_ols, beta_ols, case['alpha_true'], case['beta_true']) if alpha_ols is not None else float('inf')\n        error_nls = calculate_error(alpha_nls, beta_nls, case['alpha_true'], case['beta_true'])\n        \n        # Compare methods\n        if case['noise_type'] == 'multiplicative':\n            # Matching method is OLS\n            is_matching_method_better = error_ols  error_nls\n        else:\n            # Matching method is NLS\n            is_matching_method_better = error_nls  error_ols\n            \n        results.append(is_matching_method_better)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the Gauss-Newton method is powerful, it can fail if the problem is ill-conditioned. The Levenberg-Marquardt (LM) algorithm provides a robust solution by adaptively blending the speed of Gauss-Newton with the stability of gradient descent. In this capstone exercise, you will build a complete LM solver from the ground up, implementing the automatic damping control that makes it a workhorse of scientific and engineering data analysis. ",
            "id": "3256843",
            "problem": "You are tasked with implementing a complete, self-contained program that solves a nonlinear least squares fitting problem using the Levenberg–Marquardt algorithm, where the damping parameter is automatically adjusted based on the ratio of actual to predicted decrease in the sum of squares. The derivation and implementation must begin from the fundamental definition of the nonlinear least squares objective and proceed to an algorithm that is scientifically and numerically sound.\n\nBegin from the core definition of the nonlinear least squares objective. Let the parameter vector be $p \\in \\mathbb{R}^m$, the independent variable vector be $x \\in \\mathbb{R}^n$, the observed data (dependent variable) be $y \\in \\mathbb{R}^n$, and the model be a function $f:\\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}^n$ that predicts the data from parameters. Define the residual vector $r(p) \\in \\mathbb{R}^n$ by $r(p) = y - f(x,p)$ and the objective function by $F(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2$. Define the Jacobian matrix $J(p) \\in \\mathbb{R}^{n \\times m}$ with entries $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$. Your program must use central finite differences to approximate the Jacobian numerically at every iteration.\n\nAt each iteration, construct a local quadratic model of $F(p)$ by a second-order approximation around a current parameter $p$ using $r(p)$ and $J(p)$, and compute a parameter increment that balances between the Gauss–Newton direction and a gradient-descent direction by solving a damped normal equation. The algorithm must decide whether to accept or reject the proposed step using a gain ratio that compares the actual decrease in $F(p)$ to the predicted decrease in the local model used to compute the step. The damping parameter must be adjusted automatically based on this gain ratio: reduce the damping when the step is productive and increase it when the step is unproductive. Use a termination criterion that stops the iterations when either the infinity norm of the gradient is small, the parameter update is small, or the change in the objective function is negligible.\n\nThe angle quantity $\\phi$ used in the sinusoidal model must be treated and computed in radians.\n\nYour program must implement the above for the following three test cases, each with a specific model, dataset, and initial parameter guess. For each test case, return the fitted parameter vector as a list of floats rounded to six decimal places.\n\nTest Case 1 (general convergence):\n- Model: $f(x,p) = p_1 \\exp(p_2 x) + p_3$, with parameters $p = [p_1,p_2,p_3]$.\n- Data: $x$ defined by $x_i = 0.0 + i\\Delta$ for $i=0,1,\\dots,30$ and $\\Delta = 0.1$, so $x \\in [0,3.0]$ with $31$ points. Observations defined by $y = 2.0\\exp(-0.7 x) + 0.5 + 0.01\\sin(5x)$.\n- Initial parameter: $p^{(0)} = [1.0,-0.2,0.0]$.\n\nTest Case 2 (steep nonlinearity):\n- Model: logistic curve $f(x,p) = \\frac{p_1}{1 + \\exp(-p_2(x - p_3))}$, with parameters $p = [p_1,p_2,p_3]$.\n- Data: $x$ defined by $x_i = -1.5 + i\\Delta$ for $i=0,1,\\dots,14$ and $\\Delta = \\frac{2.0 - (-1.5)}{14}$, so $x \\in [-1.5,2.0]$ with $15$ points. Observations defined by $y = \\frac{1.5}{1 + \\exp(-3.0(x - 0.5))} + 0.02\\cos(3x)$.\n- Initial parameter: $p^{(0)} = [1.0,1.0,0.0]$.\n\nTest Case 3 (near-degeneracy in sensitivity):\n- Model: sinusoid with fixed angular frequency $\\omega = 2.5$ radians per unit of $x$, $f(x,p) = p_1 \\cos(\\omega x + p_2) + p_3$, with parameters $p = [p_1,p_2,p_3]$ and $\\phi = p_2$ in radians.\n- Data: $x$ defined by $x_i = 0.0 + i\\Delta$ for $i=0,1,\\dots,20$ and $\\Delta = 0.1$, so $x \\in [0,2.0]$ with $21$ points. Observations defined by $y = 0.8\\cos(2.5 x + 0.4) - 0.05 + 0.01\\sin(7x)$.\n- Initial parameter: $p^{(0)} = [0.5,0.0,0.0]$.\n\nAlgorithmic requirements:\n- Use central finite differences for the Jacobian with a perturbation step proportional to $\\sqrt{\\varepsilon}(1 + |p_j|)$, where $\\varepsilon$ is machine precision for double-precision floating-point arithmetic.\n- Construct and solve a damped normal equation to obtain a parameter increment at each iteration.\n- Compute the actual decrease in the objective function and the predicted decrease in the local model used in the step computation. Use their ratio to decide acceptance of the step and to adapt the damping parameter.\n- Terminate when any standard smallness criterion is met: the infinity norm of the gradient is small, the parameter change is small, or the change in the objective function is small. Also include a maximum iteration cap to ensure the algorithm halts.\n- The angle $\\phi$ in the sinusoidal model must be computed in radians throughout.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the fitted parameter values rounded to six decimal places. The final output format must be exactly of the form:\n\"[[p11,p12,p13],[p21,p22,p23],[p31,p32,p33]]\"\nwhere $p1k$, $p2k$, and $p3k$ are the fitted parameters for test cases $1$, $2$, and $3$, respectively, expressed as decimal numbers.",
            "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded, and computationally feasible task from the field of numerical methods. It requires the implementation of the Levenberg-Marquardt algorithm for nonlinear least squares, a standard and important technique in scientific computing. All necessary components, including the objective function, algorithmic procedures, test models, datasets, and initial conditions, are provided and internally consistent.\n\nHerein, a complete solution is derived from first principles.\n\n### 1. The Nonlinear Least Squares Problem\n\nThe core of the problem is to find the set of parameters that best fits a model to a set of observed data. Let the parameter vector be $p \\in \\mathbb{R}^m$, the vector of independent variables be $x \\in \\mathbb{R}^n$, and the corresponding observed data vector be $y \\in \\mathbb{R}^n$. A model function, $f(x, p)$, maps the parameters $p$ and independent variables $x$ to a set of predicted data points. The goal is to find the parameter vector $p$ that minimizes the sum of the squared differences between the observed data $y$ and the predicted data $f(x,p)$.\n\nThis is formally expressed by defining a residual vector $r(p) \\in \\mathbb{R}^n$ as:\n$$\nr(p) = y - f(x, p)\n$$\nThe objective is to minimize the scalar objective function $F(p)$, defined as one-half of the squared Euclidean norm of the residual vector:\n$$\nF(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2 = \\frac{1}{2} \\sum_{i=1}^{n} r_i(p)^2\n$$\nSince the model $f(x,p)$ is generally a nonlinear function of the parameters $p$, this is a nonlinear least squares problem, which must be solved using iterative methods.\n\n### 2. Iterative Solution via Local Approximation\n\nThe Levenberg-Marquardt algorithm is an iterative procedure that finds a local minimum of $F(p)$. Starting from an initial guess $p^{(0)}$, the algorithm generates a sequence of parameter vectors $p^{(1)}, p^{(2)}, \\dots$ that progressively reduce the value of $F(p)$. Each iteration computes a step vector $h$ that updates the current parameter vector: $p_{k+1} = p_k + h$.\n\nThe step $h$ is found by forming a local quadratic model of the objective function $F(p)$ around the current point $p_k$. This model is constructed by first linearizing the residual vector $r(p_k+h)$ using a first-order Taylor series expansion:\n$$\nr(p_k + h) \\approx r(p_k) + J(p_k)h\n$$\nwhere $J(p_k)$ is the Jacobian matrix of the residual vector $r$ evaluated at $p_k$. The entries of the Jacobian are given by $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$. Since $r(p) = y - f(x,p)$, this is equivalent to $J_{ij}(p) = -\\frac{\\partial f_i(x,p)}{\\partial p_j}$.\n\nSubstituting this linear approximation of the residual into the objective function $F(p_k+h)$ yields a local quadratic model $L(h)$:\n$$\nF(p_k + h) = \\frac{1}{2}\\lVert r(p_k+h) \\rVert_2^2 \\approx \\frac{1}{2}\\lVert r(p_k) + J(p_k)h \\rVert_2^2 = L(h)\n$$\nExpanding the squared norm gives:\n$$\nL(h) = \\frac{1}{2} (r_k + J_k h)^T (r_k + J_k h) = \\frac{1}{2} (r_k^T r_k + 2h^T J_k^T r_k + h^T J_k^T J_k h)\n$$\n$$\nL(h) = F(p_k) + h^T (J_k^T r_k) + \\frac{1}{2} h^T (J_k^T J_k) h\n$$\nwhere $r_k = r(p_k)$ and $J_k = J(p_k)$.\n\n### 3. The Gauss-Newton and Levenberg-Marquardt Steps\n\nThe step $h$ is chosen to minimize this quadratic model $L(h)$. Setting the gradient of $L(h)$ with respect to $h$ to zero gives:\n$$\n\\nabla_h L(h) = J_k^T r_k + (J_k^T J_k) h = 0\n$$\nThis leads to the **Gauss-Newton normal equations**:\n$$\n(J_k^T J_k) h_{gn} = -J_k^T r_k\n$$\nThe step $h_{gn}$ is the Gauss-Newton step. This method works well when the matrix $J_k^T J_k$ is well-conditioned. However, if $J_k^T J_k$ is singular or ill-conditioned, the step can be excessively large and unproductive.\n\nThe **Levenberg-Marquardt algorithm** addresses this issue by introducing a damping parameter $\\lambda \\ge 0$. The step $h_{lm}$ is found by solving a modified, or damped, normal equation:\n$$\n(J_k^T J_k + \\lambda I) h_{lm} = -J_k^T r_k\n$$\nwhere $I$ is the identity matrix. The term $\\lambda I$ is a regularization term.\n- If $\\lambda$ is small, the step $h_{lm}$ approximates the Gauss-Newton step $h_{gn}$.\n- If $\\lambda$ is large, the term $\\lambda I$ dominates $J_k^T J_k$, and the equation approximates $\\lambda I h_{lm} \\approx -J_k^T r_k$, which means $h_{lm} \\approx -\\frac{1}{\\lambda} (J_k^T r_k)$. Since the gradient of the objective function is $\\nabla F(p_k) = J_k^T r_k$, the step becomes a small step in the direction of steepest descent.\n\nThus, the parameter $\\lambda$ adaptively blends the fast-converging Gauss-Newton method with the robust but slower gradient descent method.\n\n### 4. Algorithm Implementation Details\n\n#### Numerical Jacobian Approximation\nThe Jacobian matrix $J_k$ is computed at each iteration using **central finite differences**. For each parameter $p_j$ (the $j$-th component of the vector $p$), a small perturbation $\\delta_j$ is calculated:\n$$\n\\delta_j = \\sqrt{\\varepsilon} (1 + |p_j|)\n$$\nwhere $\\varepsilon$ is the machine precision for double-precision floating-point numbers. The $j$-th column of the Jacobian, which represents the sensitivity of the residuals to the parameter $p_j$, is then approximated as:\n$$\nJ_{\\cdot, j}(p_k) = \\frac{\\partial r(p_k)}{\\partial p_j} \\approx \\frac{r(p_k + \\delta_j e_j) - r(p_k - \\delta_j e_j)}{2\\delta_j} = -\\frac{f(x, p_k + \\delta_j e_j) - f(x, p_k - \\delta_j e_j)}{2\\delta_j}\n$$\nwhere $e_j$ is the $j$-th standard basis vector.\n\n#### Damping Parameter Adaptation\nThe success of a proposed step $h=h_{lm}$ is evaluated using a **gain ratio**, $\\rho$. This ratio compares the actual reduction in the objective function to the reduction predicted by the local quadratic model $L(h)$.\n- **Actual reduction**: $\\Delta F_{actual} = F(p_k) - F(p_k + h)$\n- **Predicted reduction**: $\\Delta F_{pred} = L(0) - L(h) = -h^T J_k^T r_k - \\frac{1}{2}h^T (J_k^T J_k) h$\n\nThe gain ratio is $\\rho = \\frac{\\Delta F_{actual}}{\\Delta F_{pred}}$. The adaptation strategy is as follows:\n1.  If $\\rho$ is positive and significant (e.g., $\\rho > 10^{-4}$), the step is \"productive\". It is accepted ($p_{k+1} = p_k + h$), and the damping $\\lambda$ is decreased to move closer to the faster Gauss-Newton method for the next iteration. A common update rule is $\\lambda \\leftarrow \\lambda \\cdot \\max(1/3, 1-(2\\rho-1)^3)$.\n2.  If $\\rho$ is small or negative, the step is \"unproductive\". It is rejected ($p_{k+1} = p_k$), and the damping $\\lambda$ is increased to move towards the more robust gradient descent direction. The algorithm then re-computes the step $h$ with the larger $\\lambda$ at the same iteration $k$. A common rule is $\\lambda \\leftarrow \\lambda \\cdot v$, where $v$ is a multiplicative factor, typically starting at $v=2$ and increasing on successive rejections.\n\nAn initial value for $\\lambda$ is chosen based on the scale of the problem, for instance, $\\lambda_0 = \\tau \\cdot \\max_{ii}((J_0^T J_0)_{ii})$ with a small constant $\\tau$ (e.g., $10^{-4}$).\n\n#### Termination Criteria\nThe iterative process is terminated when one of the following conditions is met, indicating that a satisfactory solution has been found:\n1.  **Small Gradient**: The magnitude of the gradient of the objective function is close to zero: $\\lVert J_k^T r_k \\rVert_{\\infty}  \\epsilon_g$.\n2.  **Small Parameter Update**: The relative change in the parameter vector is negligible: $\\lVert p_{k+1} - p_k \\rVert_2  \\epsilon_p (\\lVert p_k \\rVert_2 + \\epsilon_p)$.\n3.  **Small Objective Function Change**: The relative change in the objective function value is negligible: $|F(p_{k+1}) - F(p_k)|  \\epsilon_f (F(p_k) + \\epsilon_f)$.\n4.  **Maximum Iterations**: A predefined maximum number of iterations, $k_{max}$, is reached to prevent an infinite loop.\n\nHere, $\\epsilon_g, \\epsilon_p, \\epsilon_f$ are small tolerance values (e.g., $10^{-8}$).\n\n### 5. Summary of the Levenberg-Marquardt Algorithm\nThe complete algorithm is as follows:\n\n1.  **Initialization**:\n    - Choose an initial parameter guess $p_0$.\n    - Set tolerances $\\epsilon_g, \\epsilon_p, \\epsilon_f$ and maximum iterations $k_{max}$.\n    - Compute initial residual $r_0 = y - f(x, p_0)$ and objective $F_0 = \\frac{1}{2}r_0^T r_0$.\n    - Compute initial Jacobian $J_0$ and gradient $g_0 = J_0^T r_0$.\n    - Initialize damping parameter $\\lambda_0$ and increase factor $v$.\n    - Set iteration counter $k = 0$.\n\n2.  **Main Loop**: While termination criteria are not met:\n    a. Solve the damped normal equations $(J_k^T J_k + \\lambda_k I) h = -g_k$ for the step $h$.\n    b. Evaluate the proposed new parameter vector $p_{new} = p_k + h$.\n    c. Compute the gain ratio $\\rho = \\frac{F(p_k) - F(p_{new})}{\\Delta F_{pred}}$.\n    d. **If $\\rho > \\epsilon_{\\rho}$**:\n        i. Accept the step: $p_{k+1} = p_{new}$.\n        ii. Update objective $F_{k+1}$, residual $r_{k+1}$, Jacobian $J_{k+1}$, and gradient $g_{k+1}$.\n        iii. Decrease damping: $\\lambda_{k+1} = \\lambda_k \\cdot \\max(1/3, 1-(2\\rho-1)^3)$ and reset $v=2$.\n        iv. Check termination criteria on step size and objective change.\n        v. Increment $k \\leftarrow k+1$.\n    e. **Else ($\\rho \\le \\epsilon_{\\rho}$)**:\n        i. Reject the step. Parameters are not updated.\n        ii. Increase damping: $\\lambda_k \\leftarrow \\lambda_k \\cdot v$, and then $v \\leftarrow 2v$.\n        iii. Repeat from step 2a with the new $\\lambda_k$.\n\n3.  **Termination**: Return the final parameter vector $p_k$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef levenberg_marquardt(model_func, p_init, x_data, y_data, max_iter=100, tol_g=1e-8, tol_p=1e-8, tol_f=1e-8):\n    \"\"\"\n    Implements the Levenberg-Marquardt algorithm for nonlinear least squares.\n\n    Args:\n        model_func: The model function f(x, p).\n        p_init: Initial guess for the parameters.\n        x_data: Independent variable data.\n        y_data: Dependent variable data (observations).\n        max_iter: Maximum number of iterations.\n        tol_g: Tolerance for the infinity norm of the gradient.\n        tol_p: Tolerance for the parameter update step.\n        tol_f: Tolerance for the change in the objective function.\n\n    Returns:\n        The fitted parameter vector.\n    \"\"\"\n    p_k = np.array(p_init, dtype=float)\n    m = len(p_k)\n    eps = np.finfo(np.float64).eps\n\n    # Initial evaluation\n    r_k = y_data - model_func(x_data, p_k)\n    F_k = 0.5 * np.dot(r_k, r_k)\n\n    # Jacobian calculation function\n    def calculate_jacobian(p):\n        J = np.zeros((len(x_data), m))\n        for j in range(m):\n            delta = np.sqrt(eps) * (1.0 + np.abs(p[j]))\n            p_plus = p.copy()\n            p_minus = p.copy()\n            p_plus[j] += delta\n            p_minus[j] -= delta\n            \n            # Central difference for jacobian of residual r = y - f\n            # dr/dp = -df/dp\n            # Using (f(p - d) - f(p + d)) / 2d to get -df/dp\n            f_plus = model_func(x_data, p_plus)\n            f_minus = model_func(x_data, p_minus)\n\n            J[:, j] = (f_minus - f_plus) / (2.0 * delta)\n        return J\n\n    J_k = calculate_jacobian(p_k)\n    g_k = np.dot(J_k.T, r_k)\n\n    if np.linalg.norm(g_k, np.inf)  tol_g:\n        return p_k\n\n    # Initial damping parameter\n    A = np.dot(J_k.T, J_k)\n    tau = 1e-4\n    mu = tau * np.max(np.diag(A))\n    nu = 2.0\n    \n    k = 0\n    while k  max_iter:\n        \n        while True:\n            H_lm = A + mu * np.identity(m)\n            try:\n                h = np.linalg.solve(H_lm, -g_k)\n            except np.linalg.LinAlgError:\n                mu *= nu\n                nu *= 2.0\n                continue\n\n            p_new = p_k + h\n            r_new = y_data - model_func(x_data, p_new)\n            F_new = 0.5 * np.dot(r_new, r_new)\n\n            # Predicted reduction\n            # pred_red = - (h^T * g_k + 0.5 * h^T * A * h)\n            pred_red = - (np.dot(h, g_k) + 0.5 * np.dot(h, np.dot(A, h)))\n            \n            # Gain ratio\n            actual_red = F_k - F_new\n            \n            if pred_red > 0: # Avoid division by zero or negative\n                rho = actual_red / pred_red\n            else:\n                rho = -1.0\n\n            if rho > 0:  # Step is accepted\n                # Check termination criteria on parameter and function change\n                p_norm = np.linalg.norm(p_k)\n                if np.linalg.norm(h)  tol_p * (p_norm + tol_p):\n                    return p_new\n                \n                F_norm = F_k\n                if abs(actual_red)  tol_f * (F_norm + tol_f):\n                    return p_new\n\n                p_k = p_new\n                F_k = F_new\n                r_k = r_new\n                \n                J_k = calculate_jacobian(p_k)\n                A = np.dot(J_k.T, J_k)\n                g_k = np.dot(J_k.T, r_k)\n\n                # Check termination on gradient\n                if np.linalg.norm(g_k, np.inf)  tol_g:\n                    return p_k\n\n                mu = mu * max(1/3.0, 1 - (2*rho - 1)**3)\n                nu = 2.0\n                break # Exit inner loop and start next iteration\n            else: # Step is rejected\n                mu *= nu\n                nu *= 2.0\n        \n        k += 1\n\n    return p_k\n\n\ndef solve():\n    \"\"\"\n    Defines and solves the three test cases specified in the problem.\n    \"\"\"\n    \n    # Test Case 1: Exponential Model\n    def model1(x, p):\n        return p[0] * np.exp(p[1] * x) + p[2]\n    \n    x1 = np.linspace(0.0, 3.0, 31)\n    y1 = 2.0 * np.exp(-0.7 * x1) + 0.5 + 0.01 * np.sin(5 * x1)\n    p_init1 = [1.0, -0.2, 0.0]\n    \n    # Test Case 2: Logistic Model\n    def model2(x, p):\n        return p[0] / (1.0 + np.exp(-p[1] * (x - p[2])))\n        \n    x2 = np.linspace(-1.5, 2.0, 15)\n    y2 = 1.5 / (1.0 + np.exp(-3.0 * (x2 - 0.5))) + 0.02 * np.cos(3 * x2)\n    p_init2 = [1.0, 1.0, 0.0]\n    \n    # Test Case 3: Sinusoidal Model\n    def model3(x, p):\n        omega = 2.5\n        return p[0] * np.cos(omega * x + p[1]) + p[2]\n        \n    x3 = np.linspace(0.0, 2.0, 21)\n    y3 = 0.8 * np.cos(2.5 * x3 + 0.4) - 0.05 + 0.01 * np.sin(7 * x3)\n    p_init3 = [0.5, 0.0, 0.0]\n\n    test_cases = [\n        (model1, p_init1, x1, y1),\n        (model2, p_init2, x2, y2),\n        (model3, p_init3, x3, y3),\n    ]\n\n    results = []\n    for model_func, p_init, x_data, y_data in test_cases:\n        p_fit = levenberg_marquardt(model_func, p_init, x_data, y_data)\n        rounded_p = [round(p, 6) for p in p_fit]\n        results.append(str(rounded_p).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}