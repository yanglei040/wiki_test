{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we will start with a direct calculation that demystifies the core Tikhonov regularization formula. By working with a simple and transparent two-dimensional system, you will gain a concrete intuition for how the regularization parameter, $\\lambda$, directly manipulates the components of the solution vector. This exercise  strips away complexity to reveal the fundamental mechanism of balancing data fidelity and solution stability.",
            "id": "2223140",
            "problem": "In many scientific and engineering applications, one encounters linear systems of equations of the form $Ax=b$ that are \"ill-conditioned.\" This means that small changes in the measurement vector $b$ can lead to very large changes in the solution vector $x$. Tikhonov regularization is a common method to stabilize the solution in such cases. The regularized solution, denoted by $x_{\\lambda}$, is found by solving a modified optimization problem, which leads to the formula:\n$$x_{\\lambda} = (A^T A + \\lambda^2 I)^{-1} A^T b$$\nHere, $A$ is the system matrix, $b$ is the measurement vector, $I$ is the identity matrix, and $\\lambda > 0$ is a user-chosen regularization parameter that controls the trade-off between fitting the data and stabilizing the solution.\n\nConsider a simple two-dimensional system characterized by the matrix $A$ and measurement vector $b$ given below:\n$$A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0.1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nThe small entry in matrix $A$ makes this system sensitive to noise and thus a good candidate for regularization. Your task is to investigate the behavior of the regularized solution $x_{\\lambda}$.\n\nDetermine the specific value of the regularization parameter $\\lambda$ for which the two components of the solution vector $x_{\\lambda}$ are equal. Provide your answer as a real number rounded to three significant figures.",
            "solution": "We use the Tikhonov formula for the regularized solution, as defined in the main text:\n$$x_{\\lambda}=(A^{T}A+\\lambda^2 I)^{-1}A^{T}b$$\nFor the given diagonal matrix $A=\\begin{pmatrix}1 & 0 \\\\ 0 & 0.1 \\end{pmatrix}$ and vector $b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, we have $A^{T}=A$, so\n$$A^{T}A=\\begin{pmatrix}1&0\\\\0&\\frac{1}{100}\\end{pmatrix}, \\quad A^{T}b=\\begin{pmatrix}1\\\\ \\frac{1}{5}\\end{pmatrix}$$\nThus,\n$$A^{T}A+\\lambda^2 I=\\begin{pmatrix}1+\\lambda^2&0\\\\0&\\lambda^2+\\frac{1}{100}\\end{pmatrix}$$\nThe solution vector $x_{\\lambda}$ is:\n$$x_{\\lambda}=(A^{T}A+\\lambda^2 I)^{-1}A^{T}b = \\begin{pmatrix}\\frac{1}{1+\\lambda^2}&0\\\\0&\\frac{1}{\\lambda^2+\\frac{1}{100}}\\end{pmatrix}\\begin{pmatrix}1\\\\ \\frac{1}{5}\\end{pmatrix}=\\begin{pmatrix}\\frac{1}{1+\\lambda^2}\\\\ \\frac{\\frac{1}{5}}{\\lambda^2+\\frac{1}{100}}\\end{pmatrix}$$\nWe seek $\\lambda$ such that the two components are equal:\n$$\\frac{1}{1+\\lambda^2}=\\frac{\\frac{1}{5}}{\\lambda^2+\\frac{1}{100}}$$\nCross-multiplying gives:\n$$\\lambda^2+\\frac{1}{100}=\\frac{1}{5}(1+\\lambda^2)$$\nMultiplying by $100$:\n$$100\\lambda^2+1=20(1+\\lambda^2)$$\n$$100\\lambda^2+1=20+20\\lambda^2$$\n$$80\\lambda^2=19$$\n$$\\lambda^2=\\frac{19}{80}$$\nSince $\\lambda > 0$, we take the square root:\n$$\\lambda = \\sqrt{\\frac{19}{80}} \\approx 0.4873397...$$\nNumerically, rounding to three significant figures gives $\\lambda = 0.487$.",
            "answer": "$$\\boxed{0.487}$$"
        },
        {
            "introduction": "Having seen how to compute a regularized solution for a given $\\lambda$, a critical practical question arises: how do we choose an appropriate value for $\\lambda$? This practice  introduces a powerful and widely used heuristic called the discrepancy principle. You will learn to calibrate the regularization strength by matching the solution's residual error to the estimated noise level in the measurements, a crucial step in applying regularization to real-world data.",
            "id": "2223147",
            "problem": "In the study of inverse problems, one frequently encounters linear systems of the form $Ax=b$ that are ill-posed. A common example arises when trying to determine a set of causal parameters $x$ from a set of observed effects $b$. For such systems, the matrix $A$ is typically ill-conditioned or singular, meaning that small amounts of noise in the measurement vector $b$ can lead to large, physically meaningless oscillations in the solution vector $x$.\n\nTikhonov regularization is a widely used technique to address this issue. Instead of solving $Ax=b$ directly, one seeks a solution $x_{\\lambda}$ that minimizes a composite objective function involving both the residual norm and the solution norm:\n$$ \\min_{x} \\left( \\|Ax - b\\|_2^2 + \\lambda^2 \\|x\\|_2^2 \\right) $$\nHere, $\\lambda > 0$ is the regularization parameter, which controls the trade-off between fitting the data and penalizing the magnitude of the solution.\n\nA crucial part of this process is choosing an appropriate value for $\\lambda$. The discrepancy principle provides a heuristic for this choice. It posits that if the noise level in the data vector $b$ is known or can be estimated as $\\delta = \\|b - b_{\\text{true}}\\|_2$, then a good choice for $\\lambda$ is one that makes the residual norm of the regularized solution equal to this noise level.\n\nConsider an ill-posed system described by the matrix $A$ and the measured data vector $b$:\n$$ A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} $$\nThe noise level in the measurement $b$ has been estimated to be $\\delta = 3\\sqrt{2}$.\n\nDetermine the value of the regularization parameter $\\lambda$ by applying the discrepancy principle, which in this case means finding the value of $\\lambda > 0$ that satisfies the equation $\\|A x_{\\lambda} - b\\|_2 = \\delta$. Express your final answer as an exact, closed-form analytic expression.",
            "solution": "We seek the Tikhonov-regularized solution\n$$\nx_{\\lambda}=\\arg\\min_{x}\\left(\\|Ax-b\\|_{2}^{2}+\\lambda^{2}\\|x\\|_{2}^{2}\\right),\n$$\nwhich satisfies the normal equation\n$$\n(A^{T}A+\\lambda^{2}I)x_{\\lambda}=A^{T}b.\n$$\nFor the given matrix and vector,\n$$\nA=\\begin{pmatrix}1&1\\\\1&1\\end{pmatrix},\\quad b=\\begin{pmatrix}5\\\\3\\end{pmatrix},\n$$\nwe compute\n$$\nA^{T}A=\\begin{pmatrix}2&2\\\\2&2\\end{pmatrix},\\quad A^{T}b=Ab=\\begin{pmatrix}8\\\\8\\end{pmatrix}.\n$$\nLet $M= A^{T}A+\\lambda^{2}I=2J+\\lambda^{2}I$, where $J$ is the $2\\times 2$ all-ones matrix. The vector $u=(1,1)^{T}$ is an eigenvector of $J$ with $J u=2u$, hence $M u=(4+\\lambda^{2})u$, while any vector orthogonal to $u$ is an eigenvector with eigenvalue $\\lambda^{2}$. Since $A^{T}b$ is proportional to $u$, we obtain\n$$\nx_{\\lambda}=\\frac{8}{4+\\lambda^{2}}\\,u=\\frac{8}{4+\\lambda^{2}}\\begin{pmatrix}1\\\\1\\end{pmatrix}.\n$$\nThe residual is\n$$\nr_{\\lambda}=Ax_{\\lambda}-b=Ax_{\\lambda}-b=\\frac{16}{4+\\lambda^{2}}\\begin{pmatrix}1\\\\1\\end{pmatrix}-\\begin{pmatrix}5\\\\3\\end{pmatrix}.\n$$\nDefine $t=\\dfrac{16}{4+\\lambda^{2}}$. Then\n$$\nr_{\\lambda}=\\begin{pmatrix}t-5\\\\ t-3\\end{pmatrix},\\quad \\|r_{\\lambda}\\|_{2}^{2}=(t-5)^{2}+(t-3)^{2}=2t^{2}-16t+34.\n$$\nThe discrepancy principle requires $\\|r_{\\lambda}\\|_{2}=\\delta=3\\sqrt{2}$, i.e., $\\|r_{\\lambda}\\|_{2}^{2} = 18$.\n$$\n2t^{2}-16t+34=18 \\quad\\Longrightarrow\\quad 2t^{2}-16t+16=0 \\quad\\Longrightarrow\\quad t^{2}-8t+8=0.\n$$\nSolving the quadratic gives\n$$\nt=4\\pm 2\\sqrt{2}.\n$$\nSince $t=\\dfrac{16}{4+\\lambda^{2}}$ satisfies $0<t\\leq 4$ for $\\lambda > 0$, the admissible root is $t=4-2\\sqrt{2}$. Hence\n$$\n\\frac{16}{4+\\lambda^{2}}=4-2\\sqrt{2}\\quad\\Longrightarrow\\quad 4+\\lambda^{2}=\\frac{16}{4-2\\sqrt{2}}.\n$$\nRationalizing the denominator,\n$$\n\\frac{16}{4-2\\sqrt{2}}=\\frac{8}{2-\\sqrt{2}}=\\frac{8(2+\\sqrt{2})}{(2-\\sqrt{2})(2+\\sqrt{2})}=\\frac{8(2+\\sqrt{2})}{2}=4(2+\\sqrt{2})=8+4\\sqrt{2}.\n$$\nTherefore,\n$$\n\\lambda^{2}=8+4\\sqrt{2}-4=4+4\\sqrt{2}=4(1+\\sqrt{2}),\n$$\nso the regularization parameter is\n$$\n\\lambda=2\\sqrt{1+\\sqrt{2}}.\n$$",
            "answer": "$$\\boxed{2\\sqrt{1+\\sqrt{2}}}$$"
        },
        {
            "introduction": "This capstone practice moves from calculating single solutions to implementing a complete computational workflow for exploring the Tikhonov \"regularization path\". You will develop a numerically robust algorithm using the Singular Value Decomposition (SVD) to compute solutions across a wide range of $\\lambda$ values. This exercise  bridges theory and practice by having you empirically verify key behaviors—such as coefficient shrinkage and convergence properties—across several distinct types of linear systems, solidifying your understanding of how regularization performs in diverse scenarios.",
            "id": "3200591",
            "problem": "You are given linear systems with noisy observations where a vector of observations $b \\in \\mathbb{R}^m$ is approximately modeled by a matrix $A \\in \\mathbb{R}^{m \\times n}$ and an unknown coefficient vector $x^\\star \\in \\mathbb{R}^n$ via $b \\approx A x^\\star$. To stabilize estimation when $A$ is ill-conditioned or rank-deficient, consider the Tikhonov-regularized objective\n$$\nJ_\\lambda(x) = \\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. The statistical ridge regression path is obtained by varying $\\lambda$ and observing how the coefficients shrink.\n\nStarting from the fundamental definition above and well-tested facts about linear algebra (including the existence and properties of the Singular Value Decomposition (SVD) and the Moore–Penrose pseudoinverse), derive a numerically stable algorithm to compute, for a logarithmically spaced grid of regularization strengths $\\lambda$, the corresponding minimizers $x_\\lambda$ of $J_\\lambda(x)$, and track coefficient shrinkage along this path. Do not rely on any shortcut formulas presented in this problem statement.\n\nYour program must implement the algorithm and apply it to the following test suite. For reproducibility, all random draws must use the specified seeds and standard normal distributions as indicated.\n\n- Test case $1$ (well-conditioned, overdetermined):\n  - Dimensions: $m = 80$, $n = 20$.\n  - Random seed: $42$.\n  - Construction:\n    - Draw $A \\in \\mathbb{R}^{80 \\times 20}$ with independent standard normal entries.\n    - Draw $x^\\text{true} \\in \\mathbb{R}^{20}$ with independent standard normal entries.\n    - Draw $\\epsilon \\in \\mathbb{R}^{80}$ with independent standard normal entries, then scale by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n- Test case $2$ (underdetermined, wide design):\n  - Dimensions: $m = 50$, $n = 100$.\n  - Random seed: $123$.\n  - Construction:\n    - Draw $A \\in \\mathbb{R}^{50 \\times 100}$, $x^\\text{true} \\in \\mathbb{R}^{100}$, and $\\epsilon \\in \\mathbb{R}^{50}$ as independent standard normal, with $\\epsilon$ scaled by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n- Test case $3$ (rank-deficient design):\n  - Dimensions: $m = 40$, $n = 40$.\n  - Random seed: $7$.\n  - Construction:\n    - Draw $A \\in \\mathbb{R}^{40 \\times 40}$ with independent standard normal entries.\n    - Impose exact column duplication to force rank deficiency: set the column at index $10$ equal to the column at index $5$, and the column at index $15$ equal to the column at index $5$.\n    - Draw $x^\\text{true} \\in \\mathbb{R}^{40}$ and $\\epsilon \\in \\mathbb{R}^{40}$ as independent standard normal, with $\\epsilon$ scaled by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n- Test case $4$ (highly ill-conditioned design with correlated columns):\n  - Dimensions: $m = 60$, $n = 20$.\n  - Random seed: $314$.\n  - Construction:\n    - Draw $G \\in \\mathbb{R}^{60 \\times 20}$ with independent standard normal entries.\n    - Create correlated and ill-conditioned columns by scaling: for column index $j \\in \\{0,1,\\dots,19\\}$, set the $j$-th column of $A$ to be the $j$-th column of $G$ multiplied by $10^{-\\frac{j}{3}}$.\n    - Draw $x^\\text{true} \\in \\mathbb{R}^{20}$ and $\\epsilon \\in \\mathbb{R}^{60}$ as independent standard normal, with $\\epsilon$ scaled by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n\nFor each test case, use a logarithmically spaced grid of $60$ values\n$$\n\\lambda \\in \\{\\lambda_k \\mid \\lambda_k = 10^{\\ell_k},\\ \\ell_k \\text{ equispaced in } [-10,6]\\},\n$$\nthat is, from $10^{-10}$ up to $10^{6}$.\n\nFor each test case, compute the regularization path $\\{x_{\\lambda_k}\\}$ and evaluate the following three boolean properties:\n\n1. Monotone shrinkage of the coefficient Euclidean norm: the sequence $\\{\\lVert x_{\\lambda_k} \\rVert_2\\}_{k=1}^{60}$ is nonincreasing within a small numerical tolerance, meaning $\\lVert x_{\\lambda_{k}} \\rVert_2 \\le \\lVert x_{\\lambda_{k-1}} \\rVert_2 + \\tau$ for all $k$, where $\\tau = 10^{-12} \\cdot \\max(1, \\lVert x_{\\lambda_{k-1}} \\rVert_2)$.\n2. Near-agreement at small regularization with the minimum-norm least squares solution: let $x_{\\mathrm{lsq}}$ be the Moore–Penrose pseudoinverse solution minimizing $\\lVert A x - b \\rVert_2$ with minimum $\\lVert x \\rVert_2$. Check that the relative difference between $x_{\\lambda_{\\min}}$ and $x_{\\mathrm{lsq}}$ satisfies\n$$\n\\frac{\\lVert x_{\\lambda_{\\min}} - x_{\\mathrm{lsq}} \\rVert_2}{\\max(1,\\lVert x_{\\mathrm{lsq}} \\rVert_2)} < 10^{-6}.\n$$\n3. Near-zero coefficients at very large regularization: check that\n$$\n\\frac{\\lVert x_{\\lambda_{\\max}} \\rVert_2}{\\max(1,\\lVert x_{\\mathrm{lsq}} \\rVert_2)} < 10^{-6}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries grouped per test case in the order described above and flattened across all test cases. Concretely, the output must be a list of $12$ boolean values\n$$\n[\\text{m1\\_t1},\\text{m2\\_t1},\\text{m3\\_t1},\\ \\text{m1\\_t2},\\text{m2\\_t2},\\text{m3\\_t2},\\ \\text{m1\\_t3},\\text{m2\\_t3},\\text{m3\\_t3},\\ \\text{m1\\_t4},\\text{m2\\_t4},\\text{m3\\_t4}],\n$$\nwhere, for test case $i$, $\\text{m1\\_t}i$ is the monotone-shrinkage check, $\\text{m2\\_t}i$ is the small-$\\lambda$ agreement check, and $\\text{m3\\_t}i$ is the large-$\\lambda$ near-zero check.",
            "solution": "The user-provided problem is a valid exercise in computational science, specifically on the topic of Tikhonov regularization. It requires the derivation and implementation of a numerically stable algorithm to compute the regularization path for a set of well-defined test cases and to verify key theoretical properties of the solutions. The problem is scientifically grounded, well-posed, objective, and contains all necessary information for its resolution.\n\n### Derivation of the Numerically Stable Algorithm\n\nThe Tikhonov-regularized objective function is given by:\n$$\nJ_\\lambda(x) = \\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$, and $\\lambda \\ge 0$ is the regularization parameter. The function $J_\\lambda(x)$ is convex in $x$. For $\\lambda > 0$, it is strictly convex, guaranteeing a unique minimizer. The minimizer $x_\\lambda$ can be found by setting the gradient of $J_\\lambda(x)$ with respect to $x$ to zero.\n\nFirst, we expand the objective function:\n$$\nJ_\\lambda(x) = (A x - b)^T (A x - b) + \\lambda^2 x^T x = x^T A^T A x - 2 b^T A x + b^T b + \\lambda^2 x^T I x\n$$\nThe gradient with respect to $x$ is:\n$$\n\\nabla_x J_\\lambda(x) = 2 A^T A x - 2 A^T b + 2 \\lambda^2 I x\n$$\nSetting the gradient to zero, $\\nabla_x J_\\lambda(x) = 0$, gives the Tikhonov normal equations:\n$$\n(A^T A + \\lambda^2 I) x = A^T b\n$$\nThe solution is formally $x_\\lambda = (A^T A + \\lambda^2 I)^{-1} A^T b$. However, directly forming the matrix $A^T A$ is numerically unstable, especially if $A$ is ill-conditioned. The condition number of $A^T A$ is the square of the condition number of $A$, which can lead to significant loss of precision.\n\nA numerically stable algorithm is derived using the Singular Value Decomposition (SVD) of $A$. Let the SVD of $A$ be:\n$$\nA = U \\Sigma V^T\n$$\nHere, $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices ($U^T U = I_m$, $V^T V = I_n$), and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix containing the non-negative singular values $\\sigma_i$ in decreasing order. Let $k = \\min(m, n)$. The singular values are $(\\Sigma)_{ii} = \\sigma_i$ for $i=1, \\dots, k$.\n\nWe substitute the SVD into the first term of the objective function. Using the property that the Euclidean norm is invariant under orthogonal transformations (i.e., $\\lVert U z \\rVert_2 = \\lVert z \\rVert_2$), we have:\n$$\n\\lVert A x - b \\rVert_2^2 = \\lVert U \\Sigma V^T x - b \\rVert_2^2 = \\lVert U^T (U \\Sigma V^T x - b) \\rVert_2^2 = \\lVert \\Sigma V^T x - U^T b \\rVert_2^2\n$$\nLet's introduce a change of variables. Define $y = V^T x$ and $c = U^T b$. Since $V$ is orthogonal, $x = V y$, and the norm is preserved: $\\lVert x \\rVert_2^2 = \\lVert V y \\rVert_2^2 = \\lVert y \\rVert_2^2$. The objective function transforms into a simpler form in terms of $y$:\n$$\nJ_\\lambda(y) = \\lVert \\Sigma y - c \\rVert_2^2 + \\lambda^2 \\lVert y \\rVert_2^2\n$$\nThis form is separable. We can write it as a sum over the components of $y$ and $c$:\n$$\nJ_\\lambda(y) = \\sum_{i=1}^m \\left( (\\Sigma y)_i - c_i \\right)^2 + \\lambda^2 \\sum_{j=1}^n y_j^2\n$$\nTaking into account the structure of $\\Sigma$:\n- For $i \\le k = \\min(m, n)$, $(\\Sigma y)_i = \\sigma_i y_i$.\n- For $i > k$, $(\\Sigma y)_i = 0$.\nThe objective function becomes:\n$$\nJ_\\lambda(y) = \\sum_{i=1}^k (\\sigma_i y_i - c_i)^2 + \\sum_{i=k+1}^m c_i^2 + \\lambda^2 \\left( \\sum_{i=1}^k y_i^2 + \\sum_{i=k+1}^n y_i^2 \\right)\n$$\nThe term $\\sum_{i=k+1}^m c_i^2$ is a constant with respect to $y$. To minimize $J_\\lambda(y)$, we can minimize the remaining parts independently for each component $y_i$.\nFor $i \\in \\{1, \\dots, k\\}$: we minimize $(\\sigma_i y_i - c_i)^2 + \\lambda^2 y_i^2$. Setting the derivative with respect to $y_i$ to zero gives:\n$$\n2(\\sigma_i y_i - c_i)\\sigma_i + 2\\lambda^2 y_i = 0 \\implies (\\sigma_i^2 + \\lambda^2) y_i = \\sigma_i c_i\n$$\nThe solution for $y_i$ is:\n$$\ny_i = \\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}\n$$\nThis formula is well-defined even if $\\sigma_i = 0$, in which case $y_i=0$ (for $\\lambda > 0$).\nFor $i \\in \\{k+1, \\dots, n\\}$ (this case only occurs if $n>m$, so $k=m$): we minimize $\\lambda^2 y_i^2$. For $\\lambda > 0$, the minimum is achieved at $y_i = 0$.\n\nThus, the solution vector $y_\\lambda \\in \\mathbb{R}^n$ has components:\n$$\n(y_\\lambda)_i =\n\\begin{cases}\n\\frac{\\sigma_i (U^T b)_i}{\\sigma_i^2 + \\lambda^2} & \\text{for } 1 \\le i \\le k \\\\\n0 & \\text{for } k < i \\le n\n\\end{cases}\n$$\nFinally, we transform the solution $y_\\lambda$ back to the original variable $x_\\lambda$ using $x = V y$:\n$$\nx_\\lambda = V y_\\lambda = \\sum_{i=1}^k (y_\\lambda)_i v_i\n$$\nwhere $v_i$ are the columns of $V$ (or the rows of $V^T$).\n\nThis leads to the following numerically stable algorithm:\n1.  Compute the full SVD of $A = U \\Sigma V^T$.\n2.  Compute the transformed vector $c = U^T b$.\n3.  For each desired $\\lambda$ in the grid:\n    a. Initialize an $n$-dimensional zero vector $y_\\lambda$.\n    b. For $i=1, \\dots, k=\\min(m,n)$, compute $(y_\\lambda)_i = \\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}$.\n    c. Compute the solution $x_\\lambda = V y_\\lambda$.\nThis algorithm avoids the formation of $A^T A$ and relies on the robust SVD computation, making it numerically superior. It correctly handles rank-deficient and ill-conditioned matrices.\n\n### Verification of Properties\n\nThe problem requires checking three properties:\n1.  **Monotone shrinkage of norm**: We must verify that $\\lVert x_\\lambda \\rVert_2$ is a non-increasing function of $\\lambda$. Analytically, $\\lVert x_\\lambda \\rVert_2^2 = \\lVert y_\\lambda \\rVert_2^2 = \\sum_{i=1}^k \\left(\\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}\\right)^2$. Each term in the sum is a non-increasing function of $\\lambda \\ge 0$, so their sum is also non-increasing. Thus, the property is expected to hold up to numerical precision.\n\n2.  **Small $\\lambda$ limit**: As $\\lambda \\to 0$, $x_\\lambda$ should approach the minimum-norm least-squares solution, $x_{\\mathrm{lsq}} = A^+ b$, where $A^+$ is the Moore-Penrose pseudoinverse of $A$. From our derivation, as $\\lambda \\to 0$:\n    $$\n    (y_\\lambda)_i \\to\n    \\begin{cases}\n    c_i / \\sigma_i & \\text{if } \\sigma_i > 0 \\\\\n    0 & \\text{if } \\sigma_i = 0\n    \\end{cases}\n    $$\n    This limiting vector is precisely $\\Sigma^+ c$. Thus, $x_0 = V \\Sigma^+ c = V \\Sigma^+ U^T b = A^+ b$. The property is expected to hold.\n\n3.  **Large $\\lambda$ limit**: As $\\lambda \\to \\infty$, the denominator $\\sigma_i^2 + \\lambda^2$ grows, driving each $(y_\\lambda)_i \\to 0$. Consequently, $y_\\lambda \\to 0$ and $x_\\lambda = V y_\\lambda \\to 0$. The coefficients are expected to shrink to zero.\n\nThe implementation will generate the test cases, apply the SVD-based algorithm, and verify these three properties empirically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization problem for four test cases\n    and verifies theoretical properties of the regularization path.\n    \"\"\"\n    test_cases_params = [\n        # (m, n, seed)\n        (80, 20, 42),\n        (50, 100, 123),\n        (40, 40, 7),\n        (60, 20, 314),\n    ]\n\n    all_results = []\n\n    for i, params in enumerate(test_cases_params):\n        m, n, seed = params\n        rng = np.random.default_rng(seed)\n\n        if i == 0:  # Case 1: well-conditioned, overdetermined\n            A = rng.standard_normal((m, n))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 1:  # Case 2: underdetermined\n            A = rng.standard_normal((m, n))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 2:  # Case 3: rank-deficient\n            A = rng.standard_normal((m, n))\n            A[:, 10] = A[:, 5]\n            A[:, 15] = A[:, 5]\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 3:  # Case 4: ill-conditioned\n            G = rng.standard_normal((m, n))\n            A = np.zeros_like(G)\n            for j in range(n):\n                A[:, j] = G[:, j] * (10**(-j / 3.0))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n\n        # Regularization path calculation\n        lambda_grid = np.logspace(-10, 6, 60)\n        \n        # SVD-based solution\n        U, s, Vt = svd(A, full_matrices=True)\n        k = len(s)\n        V = Vt.T\n        c = U.T @ b\n\n        x_path = []\n        for lam in lambda_grid:\n            y = np.zeros(n)\n            y[:k] = (s * c[:k]) / (s**2 + lam**2)\n            x_lam = V @ y\n            x_path.append(x_lam)\n\n        x_path_norms = [np.linalg.norm(x) for x in x_path]\n\n        # Verification checks\n        \n        # 1. Monotone shrinkage of the coefficient Euclidean norm\n        is_monotone = True\n        for k_idx in range(1, len(x_path_norms)):\n            norm_prev = x_path_norms[k_idx - 1]\n            norm_curr = x_path_norms[k_idx]\n            # Since lambda grid is increasing, norm should be non-increasing\n            tolerance = 1e-12 * max(1, norm_prev)\n            if norm_curr > norm_prev + tolerance:\n                is_monotone = False\n                break\n        \n        # 2. Near-agreement with least squares at small lambda\n        x_lsq = np.linalg.pinv(A) @ b\n        norm_x_lsq = np.linalg.norm(x_lsq)\n        x_lambda_min = x_path[0]\n        \n        rel_diff_small_lambda = np.linalg.norm(x_lambda_min - x_lsq) / max(1, norm_x_lsq)\n        agrees_at_small_lambda = rel_diff_small_lambda  1e-6\n        \n        # 3. Near-zero coefficients at very large lambda\n        x_lambda_max = x_path[-1]\n        norm_x_lambda_max = np.linalg.norm(x_lambda_max)\n        \n        ratio_large_lambda = norm_x_lambda_max / max(1, norm_x_lsq)\n        zero_at_large_lambda = ratio_large_lambda  1e-6\n\n        all_results.extend([is_monotone, agrees_at_small_lambda, zero_at_large_lambda])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r).lower() for r in all_results)}]\")\n\nsolve()\n```"
        }
    ]
}