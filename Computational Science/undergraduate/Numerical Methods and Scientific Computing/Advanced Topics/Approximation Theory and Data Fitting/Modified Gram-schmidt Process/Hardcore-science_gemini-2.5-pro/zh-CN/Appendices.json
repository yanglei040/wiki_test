{
    "hands_on_practices": [
        {
            "introduction": "理论上，经典格拉姆-施密特（CGS）和修正格拉姆-施密特（MGS）过程都能生成一组标准正交基。然而，在实际的浮点运算中，它们的表现却大相径庭。本练习旨在通过一个受“特征脸”启发的图像重建任务，直观地揭示 MGS 在数值稳定性方面的优势。 通过对一组高度相似（即，数值上近似线性相关）的向量应用这两种算法，您将直接观察并量化 MGS 相对于 CGS 的优越性，从而深刻理解为何在科学计算中 MGS 是更受青睐的选择。",
            "id": "3252976",
            "problem": "给定一个“特征脸”（eigenfaces）设定的简化框架，其中每张人脸图像表示为实向量空间 $\\mathbb{R}^d$ 中的一个列向量。令 $A \\in \\mathbb{R}^{d \\times m}$ 表示以 $m$ 个人脸向量为列的数据矩阵。通过对 $A$ 的列求平均值来定义平均人脸向量 $\\mu \\in \\mathbb{R}^d$，并定义均值中心化数据矩阵 $M = A - \\mu \\mathbf{1}^\\top$，其中 $\\mathbf{1} \\in \\mathbb{R}^m$ 表示全一向量。考虑使用两种不同的正交规范化（orthonormalization）过程为 $M$ 的列空间构造一个标准正交基（orthonormal basis）：经典 Gram-Schmidt（CGS）过程和修正 Gram-Schmidt（MGS）过程。这两种过程都基于 $\\mathbb{R}^d$ 上的标准欧几里得内积进行操作，旨在生成列式标准正交基。\n\n从内积空间、正交性和标准正交基的基本定义出发，实现经典 Gram-Schmidt（CGS）过程和修正 Gram-Schmidt（MGS）过程，为 $M$ 的列空间生成标准正交基。利用标准正交基允许构造到子空间的正交投影这一性质，对于每个基，通过将 $M$ 投影到前 $k$ 个基向量的张成空间（span）上，然后加回平均向量 $\\mu$，计算原始数据的线性重构。通过相对 Frobenius 范数重构误差来量化重构质量，该误差定义为残差的 Frobenius 范数除以原始矩阵的 Frobenius 范数。\n\n你的程序必须生成模拟“非常相似的人脸”的合成数据集，并以科学合理的方式对一个共同的基础向量进行微小扰动。使用确定性随机数生成以保证可复现性。对于每个数据集，计算均值中心化矩阵 $M$，使用 CGS 和 MGS 构造标准正交基，并对每个指定的 $k$，计算 CGS 基和 MGS 基的相对 Frobenius 重构误差。对于每个数据集和每个 $k$，报告误差之差 $\\mathrm{err}_{\\mathrm{CGS}} - \\mathrm{err}_{\\mathrm{MGS}}$。\n\n使用以下测试套件。在所有情况下，令 $d = 64$，这样人脸向量就对应于大小为 $8 \\times 8$ 的展平图像。令 $k$ 的取值范围为 $\\{1, 2, 3\\}$。\n\n- 测试用例 1（通用、中等相似度人脸）：\n  - 参数：$d = 64$，$m = 10$，噪声标准差 $\\sigma = 10^{-3}$，随机种子 $42$，重复标志设置为 false。\n- 测试用例 2（极其相似的人脸，接近重复的列以测试数值稳定性）：\n  - 参数：$d = 64$，$m = 10$，噪声标准差 $\\sigma = 10^{-8}$，随机种子 $7$，重复标志设置为 true（一半的列被构造成与前面某一列几乎重复）。\n- 测试用例 3（边界情况，图像少且相似度高，可能存在有效低秩）：\n  - 参数：$d = 64$，$m = 4$，噪声标准差 $\\sigma = 10^{-6}$，随机种子 $99$，重复标志设置为 true。\n\n科学真实性要求：\n- 基础人脸向量必须是 $8 \\times 8$ 网格上的一个平滑、非平凡的模式，例如图像中心二维高斯分布与平缓水平梯度的混合，并缩放到单位 $\\ell_2$ 范数。\n- 每张人脸通过对基础向量应用小的全局强度缩放和偏置，并添加标准差为 $\\sigma$ 的独立高斯噪声来生成。\n- 当重复标志为真时，通过与一个接近 $1$ 的系数形成线性组合并添加尺度为 $\\sigma$ 的噪声，将一半的列构造成与前面某一列几乎重复。\n\n误差度量：\n- 对于选定的 $k \\in \\{1, 2, 3\\}$ 和一个标准正交基 $Q \\in \\mathbb{R}^{d \\times r}$（其中 $r \\leq m$），通过将 $M$ 投影到 $Q$ 的前 $k$ 列的张成空间上，然后加回 $\\mu$ 来定义 $A$ 的重构。计算相对 Frobenius 范数重构误差，即 $A$ 与其重构之差的 Frobenius 范数除以 $A$ 的 Frobenius 范数。\n\n实现约束：\n- 实现 CGS 和 MGS 以从 $M$ 生成标准正交基。在正交规范化过程中，如果候选向量的欧几里得范数低于一个很小的阈值，则应将其丢弃，以避免除以在有限精度下会被视为零的数。使用一个适合双精度算术的固定阈值。\n- 仅使用 $\\mathbb{R}^d$ 的欧几里得内积结构以及标准正交基和正交投影的性质。不要使用主成分分析（PCA）或奇异值分解（SVD）。\n\n最终输出格式：\n- 你的程序应生成单行输出，包含一个逗号分隔的三个项目的列表，其中每个项目对应一个测试用例，并且其本身是一个包含三个浮点数的列表 $[\\Delta_1, \\Delta_2, \\Delta_3]$，其中 $\\Delta_k = \\mathrm{err}_{\\mathrm{CGS}}(k) - \\mathrm{err}_{\\mathrm{MGS}}(k)$，对于 $k \\in \\{1,2,3\\}$。输出行必须具有确切的格式：\n  - 示例结构：$[[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3]]$。\n\n不涉及物理单位。不使用角度。不得使用百分比；误差必须表示为十进制浮点数。程序必须是自包含的，不需要用户输入，并且仅依赖于指定的运行时环境。",
            "solution": "我们从欧几里得内积空间和正交投影的基本定义开始。设 $\\mathbb{R}^d$ 配备其标准内积 $\\langle x, y \\rangle = \\sum_{i=1}^d x_i y_i$，并定义欧几里得范数 $\\|x\\|_2 = \\sqrt{\\langle x, x \\rangle}$。$\\mathbb{R}^d$ 中的一组向量 $\\{q_1, q_2, \\dots, q_r\\}$ 是标准正交的，如果对于 $i \\neq j$ 有 $\\langle q_i, q_j \\rangle = 0$，且对于所有 $i$ 有 $\\|q_i\\|_2 = 1$。给定一个标准正交集 $\\{q_1, \\dots, q_r\\}$，到 $\\mathrm{span}\\{q_1, \\dots, q_r\\}$ 上的正交投影 $P$ 将向量 $x \\in \\mathbb{R}^d$ 映射到该子空间中使 $\\|x - y\\|_2$ 在子空间中的所有 $y$ 上最小化的唯一向量；该投影的特征是 $x - P x$ 与该子空间正交。对于一个标准正交基矩阵 $Q \\in \\mathbb{R}^{d \\times r}$，其列为 $\\{q_i\\}$，向量 $x$ 的投影可以使用内积系数 $\\langle q_i, x \\rangle$ 和标准正交关系来构造。\n\n在此基础上，我们考虑为均值中心化矩阵 $M = A - \\mu \\mathbf{1}^\\top$ 的列空间构造标准正交基，其中 $A \\in \\mathbb{R}^{d \\times m}$ 是数据矩阵，$\\mu \\in \\mathbb{R}^d$ 是 $A$ 的列的平均向量。我们比较两种过程：\n\n- 经典 Gram-Schmidt（CGS）：从概念上讲，在第 $j$ 步，我们通过使用与原始输入向量的内积，移除第 $j$ 个输入向量在先前构造的标准正交向量上的投影，从而形成其正交分量，然后如果结果的范数足够大，则对其进行归一化。在精确算术中，这会产生一个与输入向量张成相同子空间的标准正交集。在浮点运算中，当输入向量接近线性相关时，CGS 可能会失去正交性，因为减去几乎相等的量会加剧舍入误差，并且使用与原始向量计算的内积并不能减轻累积的数值相关性。\n- 修正 Gram-Schmidt（MGS）：在第 $j$ 步，我们沿着每个先前构造的标准正交向量，迭代地移除当前工作向量的分量，每次减法后都更新工作向量，然后如果最终结果的范数足够大，则对其进行归一化。在浮点运算中，MGS 在数值上更稳定，因为它以一种减少舍入误差增长并更好地保持所构造基向量之间正交性的方式执行重正交化。\n\n这两种过程都依赖于内积空间中正交性和归一化的核心定义。基于投影的重构利用了这样一个性质：对于一个标准正交基，最小二乘意义下的最佳近似是通过投影到所选基向量的张成空间上来获得的。具体来说，如果 $Q \\in \\mathbb{R}^{d \\times r}$ 具有标准正交的列，并且我们选择 $k \\leq r$，则将 $M$ 投影到 $Q$ 的前 $k$ 列的张成空间上会产生子空间中的一个近似 $\\widehat{M}$。然后，重构的数据矩阵是 $\\widehat{A} = \\mu \\mathbf{1}^\\top + \\widehat{M}$，重构质量通过相对 Frobenius 范数误差来量化\n$$\n\\mathrm{err} = \\frac{\\|A - \\widehat{A}\\|_F}{\\|A\\|_F},\n$$\n其中 $\\|\\cdot\\|_F$ 表示 Frobenius 范数 $\\|X\\|_F = \\sqrt{\\sum_{i,j} X_{ij}^2}$。\n\n算法设计：\n- 生成模拟“非常相似的人脸”的合成数据集。在 $8 \\times 8$ 网格上使用一个平滑、非平凡的模式（例如，图像中心点的二维高斯分布加上一个平缓的水平梯度）构造一个基础人脸向量，然后将其缩放至单位 $\\ell_2$ 范数。每个脸部向量通过对基础向量施加小的全局强度缩放和偏置，并添加标准差为 $\\sigma$ 的独立高斯噪声来获得。当重复标志为真时，通过将某一列乘以一个接近 $1$ 的系数并添加尺度为 $\\sigma$ 的小高斯噪声，创建一半的列使其成为先前某列的近似重复。这会产生几乎线性相关的列，从而对数值稳定性构成压力。\n- 通过对 $A$ 的列求平均来计算平均向量 $\\mu$，并形成 $M = A - \\mu \\mathbf{1}^\\top$。\n- 实现 CGS 和 MGS 以从 $M$ 生成标准正交基。为确保有限精度算术中的科学真实性，如果候选向量的范数低于一个很小的阈值（例如，双精度下机器精度数量级的容差），则丢弃该向量以避免不稳定的归一化。\n- 对于每个 $k \\in \\{1, 2, 3\\}$，通过将 $M$ 投影到 $Q$ 的前 $k$ 个基向量的张成空间上（对于 CGS 和 MGS），然后加回均值 $\\mu$，来重构 $\\widehat{A}$，并计算相对 Frobenius 重构误差。如果可用的基向量少于 $k$ 个，则使用所有可用的基向量。\n- 报告每个 $k$ 和每个测试用例的差异 $\\Delta_k = \\mathrm{err}_{\\mathrm{CGS}}(k) - \\mathrm{err}_{\\mathrm{MGS}}(k)$。\n\n为什么这个比较有意义：\n- 在精确算术中，CGS 和 MGS 都生成张成相同子空间的标准正交基，并且对于固定的 $k$，到前 $k$ 个基向量张成空间的正交投影仅依赖于子空间，而不依赖于特定的标准正交基。然而，在浮点运算中，当向量接近线性相关时，CGS 可能会遭受正交性损失，导致基不完全标准正交，从而使投影偏离理想情况。已知 MGS 可以减轻这种影响。因此，在非常相似的人脸数据集上，相对于 CGS，MGS 通常会为较小的 $k$ 产生更准确的重构，而差异 $\\Delta_k$ 提供了对这种稳定性差距的定量评估。\n\n测试套件覆盖范围：\n- 测试用例 1 测试了一个通用的“理想路径”，使用中等相似度的人脸（$d = 64$，$m = 10$，$\\sigma = 10^{-3}$），其中 CGS 和 MGS 的表现应该相当，但由于有限精度，可能会出现微小差异。\n- 测试用例 2 引入了极其相似的人脸和近似重复的列（$d = 64$，$m = 10$，$\\sigma = 10^{-8}$，重复为真），通过强烈的正交性损失风险来对 CGS 进行压力测试，并突显 MGS 的稳定性。\n- 测试用例 3 提供了一个边界情况，图像少且相似度高（$d = 64$，$m = 4$，$\\sigma = 10^{-6}$，重复为真），可能产生一个有效低秩的 $M$，使得请求 $k=3$ 超过了可靠基向量的数量，从而确保实现能够正确处理截断的基。\n\n程序将三个测试用例的结果汇总到单行输出中，其格式严格为 $[[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3],[\\Delta_1,\\Delta_2,\\Delta_3]]$，其中每个内部列表对应一个测试用例，条目对应于 $k \\in \\{1,2,3\\}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_base_face_vector(d: int) -> np.ndarray:\n    \"\"\"\n    Create a smooth, nontrivial base face vector over an 8x8 grid:\n    a mixture of a 2D Gaussian centered at the image midpoint and a gentle horizontal gradient.\n    The resulting vector is scaled to unit l2 norm.\n    \"\"\"\n    side = int(np.sqrt(d))\n    if side * side != d:\n        raise ValueError(\"d must be a perfect square for an 8x8 (or side x side) image grid.\")\n    x = np.linspace(0.0, 1.0, side)\n    y = np.linspace(0.0, 1.0, side)\n    xx, yy = np.meshgrid(x, y, indexing=\"xy\")\n    # 2D Gaussian centered near (0.5, 0.5) with moderate spread\n    sigma = 0.18\n    gaussian = np.exp(-((xx - 0.5)**2 + (yy - 0.5)**2) / (2.0 * sigma**2))\n    # Gentle horizontal gradient\n    gradient = 0.3 * xx\n    base = gaussian + gradient\n    base_vec = base.reshape(d).astype(np.float64)\n    # Normalize to unit l2 norm\n    norm = np.linalg.norm(base_vec)\n    if norm == 0.0:\n        return base_vec\n    return base_vec / norm\n\n\ndef generate_faces(d: int, m: int, sigma: float, seed: int, duplicates: bool) -> np.ndarray:\n    \"\"\"\n    Generate a synthetic face dataset A of shape (d, m) with \"very similar faces\":\n    - Start from a smooth base face vector.\n    - Apply small global intensity scaling and bias.\n    - Add independent Gaussian noise with standard deviation sigma.\n    - If duplicates is True, make half of the columns near duplicates of an earlier column.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    base = make_base_face_vector(d)\n    A = np.zeros((d, m), dtype=np.float64)\n\n    # Generate m faces with small global variations and noise.\n    for j in range(m):\n        gain = rng.uniform(-0.05, 0.05)  # small scaling\n        bias = rng.uniform(-0.02, 0.02)  # small bias\n        noise = rng.normal(loc=0.0, scale=sigma, size=d)\n        # Add a tiny structured variation correlated with horizontal gradient to avoid triviality\n        side = int(np.sqrt(d))\n        xx = np.linspace(0.0, 1.0, side)\n        grad_x = np.repeat(xx[np.newaxis, :], side, axis=0).reshape(d)\n        structured = 0.01 * bias * grad_x\n        A[:, j] = base * (1.0 + gain) + bias + structured + noise\n\n    if duplicates and m >= 2:\n        # Make half the columns (from index m//2 onward) near duplicates of earlier ones.\n        # Use a coefficient close to 1 and add small noise of scale sigma.\n        base_idx = 0\n        coeff = 0.999999\n        for j in range(m // 2, m):\n            noise = rng.normal(loc=0.0, scale=sigma, size=d)\n            A[:, j] = coeff * A[:, base_idx] + noise\n            # Cycle base_idx to duplicate different earlier columns if possible\n            base_idx = (base_idx + 1) % (m // 2 if m // 2 > 0 else 1)\n\n    return A\n\n\ndef mean_center_columns(A: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Compute the mean vector mu and mean-centered matrix M = A - mu * 1^T.\n    \"\"\"\n    mu = np.mean(A, axis=1)\n    M = A - mu[:, None]\n    return mu, M\n\n\ndef cgs_orthonormal_basis(M: np.ndarray, tol: float = 1e-12) -> np.ndarray:\n    \"\"\"\n    Classical Gram-Schmidt (CGS) to form an orthonormal basis of the column space of M.\n    At step j, compute inner products with the original column M[:, j] for all existing basis vectors,\n    subtract the combined projection once, then normalize if norm exceeds tol.\n    \"\"\"\n    d, m = M.shape\n    Q_cols = []\n    for j in range(m):\n        a_j = M[:, j].copy()\n        # Compute projection coefficients using the original a_j\n        coeffs = [float(np.dot(q, a_j)) for q in Q_cols]\n        # Subtract combined projection\n        v = a_j.copy()\n        for coeff, q in zip(coeffs, Q_cols):\n            v -= coeff * q\n        nrm = np.linalg.norm(v)\n        if nrm > tol:\n            q_new = v / nrm\n            Q_cols.append(q_new)\n        # If nrm = tol, discard as numerically zero\n    if len(Q_cols) == 0:\n        return np.zeros((d, 0), dtype=np.float64)\n    return np.column_stack(Q_cols)\n\n\ndef mgs_orthonormal_basis(M: np.ndarray, tol: float = 1e-12) -> np.ndarray:\n    \"\"\"\n    Modified Gram-Schmidt (MGS) to form an orthonormal basis of the column space of M.\n    At step j, iteratively remove components of the working vector along each existing basis vector,\n    updating the working vector after each subtraction, then normalize if norm exceeds tol.\n    \"\"\"\n    d, m = M.shape\n    Q_cols = []\n    for j in range(m):\n        v = M[:, j].copy()\n        for i in range(len(Q_cols)):\n            alpha = float(np.dot(Q_cols[i], v))\n            v -= alpha * Q_cols[i]\n        nrm = np.linalg.norm(v)\n        if nrm > tol:\n            q_new = v / nrm\n            Q_cols.append(q_new)\n        # If nrm = tol, discard as numerically zero\n    if len(Q_cols) == 0:\n        return np.zeros((d, 0), dtype=np.float64)\n    return np.column_stack(Q_cols)\n\n\ndef reconstruction_error_relative(A: np.ndarray, Q: np.ndarray, mu: np.ndarray, k: int) -> float:\n    \"\"\"\n    Compute the relative Frobenius norm reconstruction error when projecting onto the span\n    of the first k columns of Q (assumed columns are orthonormal), then adding back mu.\n    If fewer than k basis vectors are available, use all available vectors.\n    \"\"\"\n    M = A - mu[:, None]\n    if Q.shape[1] == 0 or k == 0:\n        M_hat = np.zeros_like(M)\n    else:\n        kk = min(k, Q.shape[1])\n        Qk = Q[:, :kk]\n        # Orthogonal projection onto span(Qk)\n        M_hat = Qk @ (Qk.T @ M)\n    A_hat = mu[:, None] + M_hat\n    num = np.linalg.norm(A - A_hat, ord='fro')\n    den = np.linalg.norm(A, ord='fro')\n    # Avoid division by zero (shouldn't happen with nontrivial data)\n    return float(num / den) if den != 0.0 else 0.0\n\n\ndef run_case(d: int, m: int, sigma: float, seed: int, duplicates: bool, k_list: list[int]) -> list[float]:\n    \"\"\"\n    Generate dataset, compute CGS and MGS bases, and return a list of differences\n    err_CGS(k) - err_MGS(k) for each k in k_list.\n    \"\"\"\n    A = generate_faces(d=d, m=m, sigma=sigma, seed=seed, duplicates=duplicates)\n    mu, M = mean_center_columns(A)\n    Q_cgs = cgs_orthonormal_basis(M, tol=1e-12)\n    Q_mgs = mgs_orthonormal_basis(M, tol=1e-12)\n\n    diffs = []\n    for k in k_list:\n        err_cgs = reconstruction_error_relative(A, Q_cgs, mu, k)\n        err_mgs = reconstruction_error_relative(A, Q_mgs, mu, k)\n        diffs.append(err_cgs - err_mgs)\n    return diffs\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: general, moderately similar faces\n        {\"d\": 64, \"m\": 10, \"sigma\": 1e-3, \"seed\": 42, \"duplicates\": False},\n        # Test case 2: extremely similar faces, near-duplicate columns\n        {\"d\": 64, \"m\": 10, \"sigma\": 1e-8, \"seed\": 7, \"duplicates\": True},\n        # Test case 3: boundary case, few images, high similarity\n        {\"d\": 64, \"m\": 4, \"sigma\": 1e-6, \"seed\": 99, \"duplicates\": True},\n    ]\n    k_list = [1, 2, 3]\n\n    results = []\n    for case in test_cases:\n        diffs = run_case(\n            d=case[\"d\"], m=case[\"m\"], sigma=case[\"sigma\"],\n            seed=case[\"seed\"], duplicates=case[\"duplicates\"],\n            k_list=k_list\n        )\n        results.append(diffs)\n\n    # Final print statement in the exact required format.\n    # Print a single line: a list of three lists, each with three floats.\n    print(f\"[{','.join('[' + ','.join(map(str, res)) + ']' for res in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理解了 MGS 的数值稳定性后，我们将其应用于解决一个经典的数值分析问题：多项式插值。直接使用范德蒙德矩阵求解插值问题是出了名的不稳定，尤其是在节点密集或分布不均时。 本练习将指导您利用 MGS 对条件数很差的范德蒙德矩阵进行稳定的 $QR$ 分解，从而将一个不稳定的线性系统转化为两个易于求解的稳定系统（一个涉及正交矩阵，另一个涉及上三角矩阵），最终精确地计算出插值多项式的系数。这展示了 MGS 作为一种强大工具，在解决其他领域的数值问题中的关键作用。",
            "id": "3253110",
            "problem": "给定一组节点和相应的函数值，请计算以单项式基表示的唯一插值多项式的系数。预期的数值策略是根据给定节点构建范德蒙矩阵，并使用改进的 Gram-Schmidt (MGS) 过程，关于标准离散内积将其列正交化。然后，利用得到的正交-三角分解来稳定地计算插值系数，而无需显式地求任何矩阵的逆。\n\n使用的基础理论：\n- 两个实列向量 $u \\in \\mathbb{R}^m$ 和 $v \\in \\mathbb{R}^m$ 的离散内积是 $\\langle u, v \\rangle = \\sum_{i=1}^m u_i v_i$。\n- 对于一个次数至多为 $n-1$ 的多项式，在节点 $x_0, x_1, \\dots, x_{m-1}$ 上的范德蒙矩阵的元素为 $V_{i,j} = x_i^j$，其中 $i = 0, \\dots, m-1$ 且 $j = 0, \\dots, n-1$。\n- 如果节点互异且 $m = n$，则范德蒙矩阵是非奇异的，并且存在一个唯一的插值多项式 $p(x) = \\sum_{j=0}^{n-1} c_j x^j$ 满足所有 $i$ 的 $p(x_i) = y_i$。\n- 改进的 Gram-Schmidt (MGS) 过程通过使用离散内积迭代地减去投影，从一个线性无关集中构造一个标准正交向量集，这比经典的 Gram-Schmidt 在数值上更稳定。\n\n你的任务：\n1. 实现改进的 Gram-Schmidt (MGS) 过程，以将给定实矩阵 $V \\in \\mathbb{R}^{m \\times n}$ 的列关于离散内积进行标准正交化，生成一个标准正交矩阵 $Q \\in \\mathbb{R}^{m \\times n}$ 和一个上三角矩阵 $R \\in \\mathbb{R}^{n \\times n}$，使得 $V = Q R$。\n2. 对于每个测试用例，使用给定的节点 $x_0, \\dots, x_{n-1}$ 和单项式基 $\\{1, x, x^2, \\dots, x^{n-1}\\}$ 构建方阵范德蒙矩阵 $V \\in \\mathbb{R}^{n \\times n}$。\n3. 使用你的 MGS 分解计算唯一插值多项式 $p(x) = \\sum_{j=0}^{n-1} c_j x^j$ 的单项式系数 $c_0, \\dots, c_{n-1}$，该多项式满足测试用例中所有节点的 $p(x_i) = y_i$。通过利用 MGS 产生的正交性并结合适当的三角求解来完成此操作；不要显式地对任何矩阵求逆。\n4. 按升幂顺序报告系数向量，即 $[c_0, c_1, \\dots, c_{n-1}]$。将每个系数四舍五入到 $10$ 位小数，并将结果打印为包含列表的列表的单行。使用标准十进制表示法（无科学记数法），且不包含任何额外文本。\n\n角度单位和物理单位不适用于此任务。所有输入和输出都是无单位的实数。\n\n测试套件：\n为以下四个插值数据集提供你的程序的输出。在每种情况下，节点都是互异的，并定义一个方形范德蒙系统。\n- 情况 A (理想二次曲线)：节点 $x = [-1.0, 0.0, 1.0]$，值 $y = [-2.0, 2.0, 4.0]$。\n- 情况 B (带混合符号的三次曲线)：节点 $x = [0.0, 1.0, 2.0, 3.0]$，值 $y = [1.0, 0.0, -3.0, -20.0]$。\n- 情况 C (轻度病态几何)：节点 $x = [0.0, 0.0001, 0.0002]$，值 $y = [1.0, 1.00020003, 1.00040012]$。\n- 情况 D (边界情况，常数多项式)：节点 $x = [7.5]$，值 $y = [4.2]$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个以逗号分隔的列表的列表形式的结果，每个内部列表按上述用例顺序给出相应测试用例的四舍五入系数。例如，输出必须具有以下形状\n$[[c^{(A)}_0, \\dots], [c^{(B)}_0, \\dots], [c^{(C)}_0, \\dots], [c^{(D)}_0, \\dots]]$\n并且必须作为类似 Python 的列表字面量精确地打印在一行上，每个系数四舍五入到 $10$ 位小数。",
            "solution": "用户提供的问题已经过分析并被确定为**有效**。这是一个数值线性代数中的适定问题，它基于已建立的数学原理，并包含获得唯一解所需的所有必要信息。\n\n任务是找到插值多项式 $p(x) = \\sum_{j=0}^{n-1} c_j x^j$ 的系数 $c = [c_0, c_1, \\dots, c_{n-1}]^T$。给定一组 $n$ 个互异节点 $\\{x_i\\}_{i=0}^{n-1}$ 和相应的值 $\\{y_i\\}_{i=0}^{n-1}$，插值条件 $p(x_i) = y_i$（对于 $i=0, \\dots, n-1$）构成一个线性方程组。该系统可以用矩阵形式表示为 $Vc = y$，其中 $V$ 是范德蒙矩阵，$c$ 是未知系数向量，$y$ 是函数值向量。\n\n对于单项式基 $\\{1, x, x^2, \\dots, x^{n-1}\\}$，范德蒙矩阵 $V \\in \\mathbb{R}^{n \\times n}$ 的元素为 $V_{ij} = x_i^j$，其中 $i, j \\in \\{0, 1, \\dots, n-1\\}$。该线性方程组为：\n$$\n\\begin{pmatrix}\nx_0^0  x_0^1  \\dots  x_0^{n-1} \\\\\nx_1^0  x_1^1  \\dots  x_1^{n-1} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\nx_{n-1}^0  x_{n-1}^1  \\dots  x_{n-1}^{n-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nc_0 \\\\\nc_1 \\\\\n\\vdots \\\\\nc_{n-1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\ny_0 \\\\\ny_1 \\\\\n\\vdots \\\\\ny_{n-1}\n\\end{pmatrix}\n$$\n通过求 $V$ 的逆来直接解这个方程组在数值上是不稳定的，特别是当节点彼此靠近时，$V$ 会变得病态。一种更稳健的方法，如题目所指定，是使用 $V$ 的 QR 分解。题目要求使用改进的 Gram-Schmidt (MGS) 算法来计算此分解，因为它比经典版本的数值稳定性更优。\n\nMGS 算法产生一个分解 $V = QR$，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是一个具有标准正交列的矩阵，而 $R \\in \\mathbb{R}^{n \\times n}$ 是一个上三角矩阵。$Q$ 的列构成了 $V$ 的列空间的一个标准正交基。$Q$ 的列的标准正交性意味着 $Q^T Q = I$，其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n步骤如下：\n1.  **改进的 Gram-Schmidt (MGS) 分解**：设 $V$ 的列表示为 $v_0, v_1, \\dots, v_{n-1}$。我们迭代计算 $Q$ 的列（表示为 $q_0, q_1, \\dots, q_{n-1}$）和 $R$ 的元素。\n    对于 $j = 0, \\dots, n-1$：\n    a. $R$ 的对角元素是当前向量的范数：$R_{jj} = ||v_j||_2$。\n    b. $Q$ 的第 $j$ 列是归一化后的向量：$q_j = v_j / R_{jj}$。\n    c. 对于所有后续的向量 $v_k$（其中 $k > j$），我们减去在 $q_j$ 上的投影。投影系数是 $R_{jk} = \\langle q_j, v_k \\rangle = q_j^T v_k$。\n    d. 剩余的向量被更新：$v_k \\leftarrow v_k - R_{jk} q_j$ 对于 $k = j+1, \\dots, n-1$。\n    MGS 的关键特性是，步骤 (d) 中的投影使用新正交化的向量 $q_j$ 来更新剩余的向量，这减少了影响经典 Gram-Schmidt 过程的正交性损失。\n\n2.  **求解方程组**：我们将 $V = QR$ 代入线性系统 $Vc = y$ 中：\n    $$QRc = y$$\n    从左侧乘以 $Q^T$ 得：\n    $$Q^T Q R c = Q^T y$$\n    由于 $Q^T Q = I$，系统简化为：\n    $$Rc = Q^T y$$\n    令 $z = Q^T y$。系统变为 $Rc = z$，这是一个上三角系统：\n    $$\n    \\begin{pmatrix}\n    R_{00}  R_{01}  \\dots  R_{0,n-1} \\\\\n    0  R_{11}  \\dots  R_{1,n-1} \\\\\n    \\vdots  \\vdots  \\ddots  \\vdots \\\\\n    0  0  \\dots  R_{n-1,n-1}\n    \\end{pmatrix}\n    \\begin{pmatrix}\n    c_0 \\\\\n    c_1 \\\\\n    \\vdots \\\\\n    c_{n-1}\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    z_0 \\\\\n    z_1 \\\\\n    \\vdots \\\\\n    z_{n-1}\n    \\end{pmatrix}\n    $$\n\n3.  **回代**：这个三角系统可以通过使用回代法高效地求解 $c$，从最后一行开始向上进行。\n    $$c_{n-1} = \\frac{z_{n-1}}{R_{n-1,n-1}}$$\n    并且对于 $i = n-2, \\dots, 0$：\n    $$c_i = \\frac{1}{R_{ii}} \\left( z_i - \\sum_{j=i+1}^{n-1} R_{ij} c_j \\right)$$\n    此过程无需计算任何矩阵的逆即可得到所需的系数向量 $c$，从而提供了一个数值稳定的解。\n\n按照要求，最终系数被四舍五入到 10 位小数。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes interpolating polynomial coefficients using MGS QR factorization.\n    \"\"\"\n\n    def modified_gram_schmidt(A):\n        \"\"\"\n        Computes the QR factorization of a matrix A using the Modified Gram-Schmidt algorithm.\n\n        Args:\n            A (np.ndarray): A matrix of shape (m, n) with linearly independent columns.\n\n        Returns:\n            Q (np.ndarray): A matrix of shape (m, n) with orthonormal columns.\n            R (np.ndarray): An upper triangular matrix of shape (n, n) such that A = QR.\n        \"\"\"\n        m, n = A.shape\n        Q = np.zeros((m, n), dtype=float)\n        R = np.zeros((n, n), dtype=float)\n        \n        # Use a copy of A for the vectors that are iteratively modified.\n        v = A.copy().astype(float)\n\n        for j in range(n):\n            # Calculate the norm of the j-th vector for the diagonal of R.\n            norm_vj = np.linalg.norm(v[:, j])\n            \n            # Since nodes are distinct, V is non-singular and norm will not be zero.\n            R[j, j] = norm_vj\n            # Normalize the vector to get the j-th column of Q.\n            Q[:, j] = v[:, j] / R[j, j]\n            \n            # Orthogonalize remaining vectors against the new orthonormal vector q_j.\n            for k in range(j + 1, n):\n                # Calculate the projection coefficient, which is an element of R.\n                R[j, k] = np.dot(Q[:, j], v[:, k])\n                # Subtract the projection from the k-th vector.\n                v[:, k] = v[:, k] - R[j, k] * Q[:, j]\n                \n        return Q, R\n\n    def back_substitution(R, z):\n        \"\"\"\n        Solves the upper triangular system Rc = z for c.\n\n        Args:\n            R (np.ndarray): An upper triangular square matrix.\n            z (np.ndarray): A vector.\n\n        Returns:\n            np.ndarray: The solution vector c.\n        \"\"\"\n        n = R.shape[0]\n        c = np.zeros(n, dtype=float)\n        for i in range(n - 1, -1, -1):\n            # Calculate the sum of R_ij * c_j for j > i\n            s = np.dot(R[i, i + 1:], c[i + 1:])\n            # R is non-singular, so R[i, i] is non-zero.\n            c[i] = (z[i] - s) / R[i, i]\n        return c\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path quadratic\n        {'x': [-1.0, 0.0, 1.0], 'y': [-2.0, 2.0, 4.0]},\n        # Case B: cubic with mixed signs\n        {'x': [0.0, 1.0, 2.0, 3.0], 'y': [1.0, 0.0, -3.0, -20.0]},\n        # Case C: mildly ill-conditioned geometry\n        {'x': [0.0, 0.0001, 0.0002], 'y': [1.0, 1.00020003, 1.00040012]},\n        # Case D: boundary, constant polynomial\n        {'x': [7.5], 'y': [4.2]},\n    ]\n    \n    all_results = []\n    \n    for case in test_cases:\n        x_nodes = np.array(case['x'], dtype=float)\n        y_values = np.array(case['y'], dtype=float)\n        \n        n = len(x_nodes)\n        \n        # 1. Construct the Vandermonde matrix.\n        #    increasing=True corresponds to the basis {1, x, x^2, ...}.\n        V = np.vander(x_nodes, N=n, increasing=True)\n        \n        # 2. Perform MGS QR factorization.\n        Q, R = modified_gram_schmidt(V)\n        \n        # 3. Compute z = Q^T * y.\n        z = Q.T @ y_values\n        \n        # 4. Solve Rc = z using back substitution.\n        c = back_substitution(R, z)\n        \n        # 5. Round coefficients to 10 decimal places and store.\n        rounded_c = [round(val, 10) for val in c]\n        all_results.append(rounded_c)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists of floats.\n    print(all_results)\n\nsolve()\n```"
        },
        {
            "introduction": "在之前实践的基础上，我们可以利用 MGS 分解来应对更高级的数值挑战。本练习将探讨如何运用 MGS 来分析矩阵自身的性质，特别是其条件数 $\\kappa_2(A)$，这是一个衡量矩阵对输入误差敏感度的重要指标。 您将把 MGS 的 $QR$ 分解与幂法及反幂法等迭代算法相结合，以估算矩阵的最大和最小奇异值，进而计算出条件数。这个练习综合了多个核心数值概念，揭示了由 MGS 提供的稳定分解在矩阵分析和诊断中的深远效用，标志着对数值线性代数工具的深入理解。",
            "id": "3253047",
            "problem": "你需要实现一个数值程序来估计列满秩矩阵 $A$ 的2-范数条件数 $\\kappa_2(A)$，方法是使用修正的Gram-Schmidt (MGS) 过程构建一个薄QR分解，然后通过此分解求解最小二乘问题。对于列满秩矩阵 $A$，2-范数条件数定义为 $\\kappa_2(A) = \\|A\\|_2 \\,\\|A^\\dagger\\|_2$，其中 $A^\\dagger$ 表示 $A$ 的伪逆；对于方形非奇异矩阵 $A$，这等于 $\\|A\\|_2 \\,\\|A^{-1}\\|_2$。谱范数是由向量2-范数诱导的算子范数：$\\|A\\|_2 = \\max_{x \\neq 0} \\frac{\\|Ax\\|_2}{\\|x\\|_2}$，其中对于任意向量 $x$，$\\|x\\|_2 = \\sqrt{x^\\top x}$。正交变换保持向量2-范数不变，因此对于一个薄QR分解 $A = QR$（其中 $Q$ 具有标准正交列，$R$ 为上三角矩阵），我们有 $\\|A\\|_2 = \\|R\\|_2$，并且 $A$ 和 $R$ 的非零奇异值相同。因此，$\\kappa_2(A)$ 等于 $R$ 的最大奇异值与最小奇异值之比。这些奇异值是对称正定矩阵 $R^\\top R$ 的极端特征值的平方根。\n\n你必须使用的基本原理：\n- 向量2-范数和诱导算子范数的定义。\n- 2-范数的正交不变性。\n- 使用修正的Gram-Schmidt (MGS) 算法为列满秩矩阵 $A$ 生成薄QR分解 $A = QR$。\n- 通过QR分解求解最小二乘：对于任意 $b$，$\\|Ax - b\\|_2$ 的最小化子可以通过求解 $R x = Q^\\top b$ 得到。\n- 使用幂迭代法估计对称正定矩阵的最大特征值，以及使用反幂迭代法（通过求解线性系统实现）估计最小特征值。\n\n你的程序必须：\n1. 实现修正的Gram-Schmidt (MGS) 算法，为 $m \\ge n$ 且列满秩的 $m \\times n$ 矩阵计算薄QR分解 $A = QR$。\n2. 使用因子 $R$，通过对 $R^\\top R$ 应用幂迭代法来估计最大奇异值 $\\sigma_{\\max}(A)$。\n3. 使用因子 $R$，通过对 $R^\\top R$ 应用反幂迭代法来估计最小奇异值 $\\sigma_{\\min}(A)$。不要显式地构建 $(R^\\top R)^{-1}$；而是在每次迭代中求解与 $R$ 和 $R^\\top$ 相关的线性系统（这些求解是通过QR分解解决最小二乘问题的核心）。具体来说，要将 $(R^\\top R)^{-1}$ 应用于一个向量 $v$，先通过前向代入法求解 $R^\\top t = v$，然后通过反向代入法求解 $R z = t$。\n4. 计算估计值 $\\widehat{\\kappa}_2(A) = \\widehat{\\sigma}_{\\max}(A)/\\widehat{\\sigma}_{\\min}(A)$。\n5. 在两次幂迭代中，都使用 $10^{-12}$ 的相对向量变化作为收敛容差，并设置最大迭代次数为 $1000$ 次。在每一步中，都对迭代向量进行2-范数归一化。\n\n测试套件：\n为以下矩阵计算 $\\widehat{\\kappa}_2(A)$。所有条目都是无量纲的。每个矩阵都已明确给出；你必须在程序中将它们硬编码为NumPy数组。\n\n- 情况1（高矩阵，一般情况，$m=5$, $n=3$）：\n  $$\n  A_1 =\n  \\begin{bmatrix}\n  1  2  3 \\\\\n  4  5  6 \\\\\n  7  8  10 \\\\\n  2  3  4 \\\\\n  1  0  1\n  \\end{bmatrix}.\n  $$\n- 情况2（方阵，病态上三角矩阵，$m=n=3$）：\n  $$\n  A_2 =\n  \\begin{bmatrix}\n  1  1  1 \\\\\n  0  1  1 \\\\\n  0  0  10^{-4}\n  \\end{bmatrix}.\n  $$\n- 情况3（方阵，对角缩放，$m=n=4$）：\n  $$\n  A_3 =\n  \\mathrm{diag}\\!\\left(10,\\;1,\\;10^{-1},\\;10^{-2}\\right).\n  $$\n- 情况4（高矩阵，标准正交列，$m=5$, $n=3$）：\n  $$\n  A_4 =\n  \\begin{bmatrix}\n  1  0  0 \\\\\n  0  1  0 \\\\\n  0  0  1 \\\\\n  0  0  0 \\\\\n  0  0  0\n  \\end{bmatrix}.\n  $$\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含四个估计的条件数，格式为方括号内的逗号分隔列表（例如，`\"[v_1,v_2,v_3,v_4]\"`），其中每个 $v_i$ 是一个四舍五入到八位小数的浮点数。\n- 不涉及角度；不需要角度单位。不涉及物理单位。\n\n你的实现不能为任何最小二乘求解显式地构建正规方程 $A^\\top A$，并且必须使用修正的Gram-Schmidt (MGS) QR分解路径来解决反幂迭代步骤中出现的最小二乘问题。",
            "solution": "该问题要求实现一种数值方法，用于估计给定一组列满秩矩阵的2-范数条件数 $\\kappa_2(A)$。解决方案必须遵循基于修正的Gram-Schmidt (MGS) 分解和迭代特征值估计算法的特定算法路径。\n\n一个 $m \\times n$ 且 $m \\ge n$ 的列满秩矩阵 $A$ 的2-范数条件数定义为 $\\kappa_2(A) = \\|A\\|_2 \\|A^\\dagger\\|_2$，其中 $\\|A\\|_2$ 是谱范数（由向量2-范数诱导的算子范数），$A^\\dagger = (A^\\top A)^{-1}A^\\top$ 是 $A$ 的伪逆。$A$ 的谱范数是其最大奇异值 $\\sigma_{\\max}(A)$，而其伪逆的谱范数是其最小奇异值的倒数，即 $\\|A^\\dagger\\|_2 = 1/\\sigma_{\\min}(A)$。因此，条件数是 $A$ 的最大奇异值与最小奇异值之比：\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}\n$$\n\n该指定方法的核心是首先计算 $A$ 的一个薄QR分解，即 $A=QR$，其中 $Q$ 是一个 $m \\times n$ 的矩阵，其列是标准正交的（$Q^\\top Q=I_n$），而 $R$ 是一个 $n \\times n$ 的可逆上三角矩阵。由于 $Q$ 是一个正交变换（当视为从 $\\mathbb{R}^n$到 $\\mathbb{R}^m$ 的一个子空间的映射时），它保持2-范数不变，这意味着对于任何向量 $x \\in \\mathbb{R}^n$，都有 $\\|Ax\\|_2 = \\|QRx\\|_2 = \\|Rx\\|_2$。因此，$A$ 的奇异值与 $R$ 的奇异值相同。这将问题简化为计算方形上三角矩阵 $R$ 的条件数：\n$$\n\\kappa_2(A) = \\kappa_2(R) = \\frac{\\sigma_{\\max}(R)}{\\sigma_{\\min}(R)}\n$$\n\n$R$ 的奇异值是对称正定矩阵 $R^\\top R$ 的特征值的平方根。设 $\\lambda_{\\max}(R^\\top R)$ 和 $\\lambda_{\\min}(R^\\top R)$ 分别表示 $R^\\top R$ 的最大和最小特征值。那么，\n$$\n\\sigma_{\\max}(R) = \\sqrt{\\lambda_{\\max}(R^\\top R)} \\quad \\text{且} \\quad \\sigma_{\\min}(R) = \\sqrt{\\lambda_{\\min}(R^\\top R)}\n$$\n因此，条件数的估计值为：\n$$\n\\widehat{\\kappa}_2(A) = \\sqrt{\\frac{\\widehat{\\lambda}_{\\max}(R^\\top R)}{\\widehat{\\lambda}_{\\min}(R^\\top R)}}\n$$\n\n整个算法分为三个主要阶段：\n\n**阶段1：修正的Gram-Schmidt (MGS) QR分解**\n薄QR分解 $A=QR$ 使用修正的Gram-Schmidt算法计算，该算法以其相比经典版本更优的数值稳定性而著称。给定 $A$ 的列向量为 $\\{a_1, a_2, \\dots, a_n\\}$，MGS生成 $Q$ 的标准正交列向量 $\\{q_1, q_2, \\dots, q_n\\}$ 和 $R$ 的元素如下。设 $v_j$ 表示正在被正交化的向量。初始时，对于 $j=1, \\dots, n$，设置 $v_j = a_j$。\n对于 $i = 1, \\dots, n$:\n1. 归一化当前向量 $v_i$：$r_{ii} = \\|v_i\\|_2$ 且 $q_i = v_i / r_{ii}$。\n2. 将所有后续向量 $\\{v_{i+1}, \\dots, v_n\\}$ 与新形成的 $q_i$ 正交化：\n   对于 $j = i+1, \\dots, n$:\n   $r_{ij} = q_i^\\top v_j$\n   $v_j = v_j - r_{ij} q_i$\n\n**阶段2：通过幂迭代法估计 $\\lambda_{\\max}(R^\\top R)$**\n矩阵的最大特征值可以使用幂迭代法来估计。对于矩阵 $M = R^\\top R$，算法如下：\n1. 初始化一个单位范数的随机向量 $v_0$。\n2. 对于 $k = 0, 1, 2, \\dots$:\n   a. 计算 $w_{k+1} = M v_k = (R^\\top R) v_k$。\n   b. 特征值估计为 $\\lambda_{k+1} = \\|w_{k+1}\\|_2$。\n   c. 更新特征向量估计：$v_{k+1} = w_{k+1} / \\lambda_{k+1}$。\n3. 当向量 $v_k$ 的变化足够小，即 $\\|v_{k+1} - v_k\\|_2  \\epsilon$（对于给定的容差 $\\epsilon=10^{-12}$），或达到最大迭代次数（$1000$）时，迭代终止。最终的 $\\lambda_k$ 就是我们的估计值 $\\widehat{\\lambda}_{\\max}(R^\\top R)$。\n\n**阶段3：通过反幂迭代法估计 $\\lambda_{\\min}(R^\\top R)$**\n$M=R^\\top R$ 的最小特征值通过反幂迭代法找到，这相当于对 $M^{-1}$ 应用幂迭代法。将 $M^{-1}$ 应用于向量 $v$ 意味着求解线性系统 $M z = v$，即 $(R^\\top R) z = v$。按照规定，该系统被高效求解，无需显式地构建 $M$ 或其逆矩阵。使用三角因子 $R^\\top$ 和 $R$ 分两步找到解：\n1. 令 $t = Rz$。系统变为 $R^\\top t = v$。由于 $R^\\top$ 是下三角矩阵，$t$ 可以通过前向代入法求得。\n2. 在已知 $t$ 的情况下，求解 $Rz = t$ 以得到 $z$。由于 $R$ 是上三角矩阵，$z$ 可以通过反向代入法求得。\n\n反幂迭代算法如下：\n1. 初始化一个单位范数的随机向量 $z_0$。\n2. 对于 $k = 0, 1, 2, \\dots$:\n   a. 使用上述两步代入过程求解 $(R^\\top R) w_{k+1} = z_k$ 得到 $w_{k+1}$。\n   b. $M^{-1}$ 的特征值估计为 $\\mu_{k+1} = \\|w_{k+1}\\|_2$。\n   c. 更新特征向量估计：$z_{k+1} = w_{k+1} / \\mu_{k+1}$。\n3. 迭代基于与幂迭代相同的收敛准则停止。$M$ 的最小特征值是 $M^{-1}$ 最大特征值的倒数，因此 $\\widehat{\\lambda}_{\\min}(R^\\top R) = 1/\\mu$，其中 $\\mu$ 是最终收敛的值。\n\n最后，通过结合阶段2和阶段3的结果计算估计的条件数。实现将包括用于前向和反向代入的辅助函数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef forward_substitution(L, b):\n    \"\"\"\n    Solves the linear system Lx = b for x, where L is a lower triangular matrix.\n    \"\"\"\n    n = L.shape[0]\n    x = np.zeros(n, dtype=float)\n    for i in range(n):\n        sum_lx = np.dot(L[i, :i], x[:i])\n        if L[i, i] == 0:\n            # This case should not occur for full-rank matrices.\n            raise ValueError(\"Matrix is singular.\")\n        x[i] = (b[i] - sum_lx) / L[i, i]\n    return x\n\ndef back_substitution(U, b):\n    \"\"\"\n    Solves the linear system Ux = b for x, where U is an upper triangular matrix.\n    \"\"\"\n    n = U.shape[0]\n    x = np.zeros(n, dtype=float)\n    for i in range(n - 1, -1, -1):\n        sum_ux = np.dot(U[i, i + 1:], x[i + 1:])\n        if U[i, i] == 0:\n            # This case should not occur for full-rank matrices.\n            raise ValueError(\"Matrix is singular.\")\n        x[i] = (b[i] - sum_ux) / U[i, i]\n    return x\n\ndef modified_gram_schmidt(A):\n    \"\"\"\n    Computes the thin QR factorization of A using Modified Gram-Schmidt.\n    A = QR, where Q has orthonormal columns and R is upper triangular.\n    \"\"\"\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    V = A.copy().astype(float)\n    \n    for i in range(n):\n        R[i, i] = np.linalg.norm(V[:, i], 2)\n        if R[i, i] == 0:\n            raise ValueError(\"Matrix does not have full column rank.\")\n        Q[:, i] = V[:, i] / R[i, i]\n        for j in range(i + 1, n):\n            R[i, j] = np.dot(Q[:, i].T, V[:, j])\n            V[:, j] = V[:, j] - R[i, j] * Q[:, i]\n            \n    return Q, R\n\ndef power_iteration(R, tol, max_iter):\n    \"\"\"\n    Estimates the largest eigenvalue of R^T R using power iteration.\n    \"\"\"\n    n = R.shape[0]\n    M = R.T @ R\n    \n    # Initialize with a fixed random vector for reproducibility\n    rng = np.random.default_rng(seed=42)\n    v = rng.random(n)\n    v /= np.linalg.norm(v)\n\n    eigenvalue = 0.0\n    for _ in range(max_iter):\n        v_old = v\n        w = M @ v\n        eigenvalue = np.linalg.norm(w)\n        v = w / eigenvalue\n        \n        if np.linalg.norm(v - v_old)  tol:\n            break\n            \n    return eigenvalue\n\ndef inverse_power_iteration(R, tol, max_iter):\n    \"\"\"\n    Estimates the smallest eigenvalue of R^T R using inverse power iteration.\n    \"\"\"\n    n = R.shape[0]\n    RT = R.T\n\n    # Initialize with a fixed random vector for reproducibility\n    rng = np.random.default_rng(seed=42)\n    v = rng.random(n)\n    v /= np.linalg.norm(v)\n\n    eigenvalue_inv = 0.0\n    for _ in range(max_iter):\n        v_old = v\n        # Solve (R^T R)w = v\n        # Step 1: Solve R^T t = v for t\n        t = forward_substitution(RT, v)\n        # Step 2: Solve R w = t for w\n        w = back_substitution(R, t)\n        \n        eigenvalue_inv = np.linalg.norm(w)\n        v = w / eigenvalue_inv\n\n        if np.linalg.norm(v - v_old)  tol:\n            break\n            \n    return 1.0 / eigenvalue_inv\n\ndef estimate_condition_number(A):\n    \"\"\"\n    Estimates the 2-norm condition number of matrix A.\n    \"\"\"\n    tol = 1e-12\n    max_iter = 1000\n    \n    _Q, R = modified_gram_schmidt(A)\n    \n    lambda_max = power_iteration(R, tol, max_iter)\n    lambda_min = inverse_power_iteration(R, tol, max_iter)\n    \n    if lambda_min == 0:\n        return float('inf')\n\n    condition_number = np.sqrt(lambda_max / lambda_min)\n    return condition_number\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute their condition numbers.\n    \"\"\"\n    A_1 = np.array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 10],\n        [2, 3, 4],\n        [1, 0, 1]\n    ], dtype=float)\n\n    A_2 = np.array([\n        [1, 1, 1],\n        [0, 1, 1],\n        [0, 0, 1e-4]\n    ], dtype=float)\n\n    A_3 = np.diag([10.0, 1.0, 1e-1, 1e-2])\n\n    A_4 = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [0, 0, 0],\n        [0, 0, 0]\n    ], dtype=float)\n\n    test_cases = [A_1, A_2, A_3, A_4]\n    \n    results = []\n    for A in test_cases:\n        kappa = estimate_condition_number(A)\n        results.append(f\"{kappa:.8f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}