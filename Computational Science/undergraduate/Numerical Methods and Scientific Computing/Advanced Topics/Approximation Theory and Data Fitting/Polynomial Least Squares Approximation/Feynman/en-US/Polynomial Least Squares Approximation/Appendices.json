{
    "hands_on_practices": [
        {
            "introduction": "Understanding the 'why' behind a method is as important as knowing 'how' to use it. This practice takes you back to first principles, guiding you to derive the equations for the best-fit polynomial by directly minimizing the sum of squared errors using calculus. This exercise  solidifies the foundational link between optimization and data fitting, revealing the origin of the famed normal equations.",
            "id": "3262996",
            "problem": "A laboratory is calibrating a sensor whose output is modeled by a quadratic polynomial. Four independent measurements are taken at input values $x \\in \\{0,1,1,2\\}$, producing outputs $y \\in \\{10,8,6,3\\}$ in the same order. The repeated input $x=1$ arises from an intentional replicate to reduce uncertainty; both replicate observations must be treated as distinct contributions to the fitting criterion. The model is a quadratic polynomial $p(x) = a x^{2} + b x + c$. The best approximation in the sense of the method of least squares is defined as the choice of coefficients $a$, $b$, and $c$ that minimizes the sum of squared residuals $\\sum (p(x_{i}) - y_{i})^{2}$ over the given data points.\n\nStarting only from the fundamental definition of least squares minimization and standard linear algebra facts, derive the necessary conditions for optimality by setting the gradient of the objective with respect to the coefficients equal to zero, carefully accounting for the fact that there are two measurements at $x=1$ with different outputs. Then solve the resulting system exactly to determine the leading coefficient $a$ of the best-fit quadratic polynomial.\n\nExpress your final answer exactly as a single number; do not round.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n-   Input values: $x \\in \\{0,1,1,2\\}$.\n-   Output values: $y \\in \\{10,8,6,3\\}$, corresponding to the input values in order.\n-   Data points $(x_i, y_i)$: $(0, 10)$, $(1, 8)$, $(1, 6)$, $(2, 3)$. The total number of measurements is $n=4$.\n-   Model: A quadratic polynomial $p(x) = a x^{2} + b x + c$.\n-   Objective: Minimize the sum of squared residuals, $S = \\sum_{i=1}^{4} (p(x_{i}) - y_{i})^{2}$, to find the best-fit coefficients $a$, $b$, and $c$.\n-   Task: Derive the necessary conditions for the minimum from first principles by setting the gradient of $S$ to zero, and then solve for the coefficient $a$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem describes a standard polynomial least squares fitting procedure, which is a fundamental and widely used technique in numerical methods, statistics, and data analysis. It is based on well-established mathematical principles.\n-   **Well-Posed:** The problem provides four data points to determine three unknown coefficients ($a, b, c$). The system is overdetermined, which is the standard scenario for a least squares fit. A unique solution exists if and only if the columns of the design matrix are linearly independent. The design matrix $A$ for this problem is given by\n    $$A = \\begin{pmatrix} x_1^2 & x_1 & 1 \\\\ x_2^2 & x_2 & 1 \\\\ x_3^2 & x_3 & 1 \\\\ x_4^2 & x_4 & 1 \\end{pmatrix} = \\begin{pmatrix} 0^2 & 0 & 1 \\\\ 1^2 & 1 & 1 \\\\ 1^2 & 1 & 1 \\\\ 2^2 & 2 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 4 & 2 & 1 \\end{pmatrix}$$\n    The columns are linearly independent, as shown by setting a linear combination to zero: $\\alpha_1 [0,1,1,4]^T + \\alpha_2 [0,1,1,2]^T + \\alpha_3 [1,1,1,1]^T = [0,0,0,0]^T$. The first row implies $\\alpha_3=0$. This reduces the system to $\\alpha_1+\\alpha_2=0$ and $4\\alpha_1+2\\alpha_2=0$, which yields $\\alpha_1=\\alpha_2=0$. Thus, the columns are linearly independent, the matrix $A^T A$ is invertible, and a unique solution exists. The problem is well-posed.\n-   **Objective:** The problem statement is clear, precise, and uses standard terminology. The instruction to treat the replicate measurements as distinct contributions is unambiguous.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, and objective. I will proceed with the solution.\n\n### Derivation and Solution\n\nThe model for the sensor's output is the quadratic polynomial $p(x) = a x^{2} + b x + c$. The data points are $(x_1, y_1) = (0, 10)$, $(x_2, y_2) = (1, 8)$, $(x_3, y_3) = (1, 6)$, and $(x_4, y_4) = (2, 3)$.\n\nThe objective is to find the coefficients $a$, $b$, and $c$ that minimize the sum of squared residuals, $S(a, b, c)$. The function $S$ is defined as:\n$$S(a, b, c) = \\sum_{i=1}^{4} (p(x_i) - y_i)^2$$\nSubstituting the given data and the polynomial model:\n$$S(a, b, c) = (a(0)^2 + b(0) + c - 10)^2 + (a(1)^2 + b(1) + c - 8)^2 + (a(1)^2 + b(1) + c - 6)^2 + (a(2)^2 + b(2) + c - 3)^2$$\n$$S(a, b, c) = (c - 10)^2 + (a + b + c - 8)^2 + (a + b + c - 6)^2 + (4a + 2b + c - 3)^2$$\n\nTo minimize $S(a,b,c)$, we must find the point where its gradient with respect to the coefficients is the zero vector. This gives the necessary conditions for optimality:\n$$\\frac{\\partial S}{\\partial a} = 0, \\quad \\frac{\\partial S}{\\partial b} = 0, \\quad \\frac{\\partial S}{\\partial c} = 0$$\n\nWe compute each partial derivative using the chain rule:\n\n1.  Partial derivative with respect to $a$:\n    $$\\frac{\\partial S}{\\partial a} = 0 + 2(a + b + c - 8)(1) + 2(a + b + c - 6)(1) + 2(4a + 2b + c - 3)(4) = 0$$\n    Dividing by $2$:\n    $$(a + b + c - 8) + (a + b + c - 6) + 4(4a + 2b + c - 3) = 0$$\n    $$(1+1+16)a + (1+1+8)b + (1+1+4)c = 8 + 6 + 12$$\n    $$18a + 10b + 6c = 26$$\n    $$9a + 5b + 3c = 13 \\quad (1)$$\n\n2.  Partial derivative with respect to $b$:\n    $$\\frac{\\partial S}{\\partial b} = 0 + 2(a + b + c - 8)(1) + 2(a + b + c - 6)(1) + 2(4a + 2b + c - 3)(2) = 0$$\n    Dividing by $2$:\n    $$(a + b + c - 8) + (a + b + c - 6) + 2(4a + 2b + c - 3) = 0$$\n    $$(1+1+8)a + (1+1+4)b + (1+1+2)c = 8 + 6 + 6$$\n    $$10a + 6b + 4c = 20$$\n    $$5a + 3b + 2c = 10 \\quad (2)$$\n\n3.  Partial derivative with respect to $c$:\n    $$\\frac{\\partial S}{\\partial c} = 2(c - 10)(1) + 2(a + b + c - 8)(1) + 2(a + b + c - 6)(1) + 2(4a + 2b + c - 3)(1) = 0$$\n    Dividing by $2$:\n    $$(c - 10) + (a + b + c - 8) + (a + b + c - 6) + (4a + 2b + c - 3) = 0$$\n    $$(1+1+4)a + (1+1+2)b + (1+1+1+1)c = 10 + 8 + 6 + 3$$\n    $$6a + 4b + 4c = 27 \\quad (3)$$\n\nWe now have a system of three linear equations, known as the normal equations, for the three unknown coefficients $a$, $b$, and $c$:\n1.  $9a + 5b + 3c = 13$\n2.  $5a + 3b + 2c = 10$\n3.  $6a + 4b + 4c = 27$\n\nThe problem requires solving for the leading coefficient $a$. We can use elimination to solve the system. Let's eliminate $c$.\nMultiply equation (2) by $3$ and equation (1) by $2$:\n$$2 \\times (1): \\quad 18a + 10b + 6c = 26$$\n$$3 \\times (2): \\quad 15a + 9b + 6c = 30$$\nSubtracting the second new equation from the first:\n$$(18a - 15a) + (10b - 9b) + (6c - 6c) = 26 - 30$$\n$$3a + b = -4 \\quad (4)$$\n\nNext, use equations (2) and (3) to eliminate $c$. Multiply equation (2) by $2$:\n$$2 \\times (2): \\quad 10a + 6b + 4c = 20$$\n$$(3): \\quad 6a + 4b + 4c = 27$$\nSubtracting the first new equation from the second:\n$$(6a - 10a) + (4b - 6b) + (4c - 4c) = 27 - 20$$\n$$-4a - 2b = 7 \\quad (5)$$\n\nNow we have a system of two equations for $a$ and $b$:\n4.  $3a + b = -4$\n5.  $-4a - 2b = 7$\n\nFrom equation (4), we can express $b$ in terms of $a$:\n$$b = -4 - 3a$$\nSubstitute this expression for $b$ into equation (5):\n$$-4a - 2(-4 - 3a) = 7$$\n$$-4a + 8 + 6a = 7$$\n$$2a = 7 - 8$$\n$$2a = -1$$\n$$a = -\\frac{1}{2}$$\n\nThe leading coefficient of the best-fit quadratic polynomial is exactly $-\\frac{1}{2}$.",
            "answer": "$$\\boxed{-\\frac{1}{2}}$$"
        },
        {
            "introduction": "Real-world datasets rarely consist of measurements with uniform precision. This practice introduces the powerful technique of weighted least squares, an essential extension for handling data of varying reliability. By assigning greater importance to more precise measurements , you will learn to formulate and solve a more robust fitting problem that better reflects the nature of experimental data.",
            "id": "3262884",
            "problem": "A laboratory collects $3$ scalar measurements of an underlying linear response at inputs $x_{1}=-1$, $x_{2}=0$, and $x_{3}=1$. The reported measurements are $y_{1}=0$, $y_{2}=1$, and $y_{3}=2$. The measurement errors are independent, zero-mean, and have known standard deviations $\\sigma_{1}=1$, $\\sigma_{2}=\\tfrac{1}{2}$, and $\\sigma_{3}=1$. You seek a polynomial least squares approximation with a linear polynomial $p(x)=c_{0}+c_{1}x$ that accounts for the varying reliability of the measurements by assigning higher weight to more reliable data. Use the diagonal matrix $W=\\mathrm{diag}\\!\\big(\\tfrac{1}{\\sigma_{1}},\\tfrac{1}{\\sigma_{2}},\\tfrac{1}{\\sigma_{3}}\\big)$ to define the objective as the Euclidean two-norm (L2) squared of the weighted residual, that is, minimize $\\|W(Ac-y)\\|_{2}^{2}$ where $A$ is the design matrix, $c=\\begin{pmatrix}c_{0}\\\\c_{1}\\end{pmatrix}$, and $y=\\begin{pmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\end{pmatrix}$.\n\nTasks:\n1) Write down the explicit matrices and vectors $A$, $W$, and $y$ associated with this problem.\n2) Starting from the fundamental principle of minimizing a differentiable scalar objective by setting its gradient with respect to $c$ to zero, derive the linear system that characterizes the minimizer.\n3) Solve for $c_{0}$ and $c_{1}$, and then evaluate the fitted polynomial at $x=2$.\n\nProvide the single final value of $p(2)$ as an exact number. Do not round.",
            "solution": "The problem is well-posed and scientifically grounded, representing a standard application of the weighted least squares method in numerical analysis. All data and conditions required for a unique solution are provided and are self-consistent.\n\n### Step 1: Problem Validation\n\n**Givens Extracted Verbatim:**\n*   Number of measurements: $3$.\n*   Inputs: $x_{1}=-1$, $x_{2}=0$, $x_{3}=1$.\n*   Measurements: $y_{1}=0$, $y_{2}=1$, $y_{3}=2$.\n*   Standard deviations: $\\sigma_{1}=1$, $\\sigma_{2}=\\tfrac{1}{2}$, $\\sigma_{3}=1$.\n*   Model: linear polynomial $p(x)=c_{0}+c_{1}x$.\n*   Weight Matrix: $W=\\mathrm{diag}\\!\\big(\\tfrac{1}{\\sigma_{1}},\\tfrac{1}{\\sigma_{2}},\\tfrac{1}{\\sigma_{3}}\\big)$.\n*   Objective function: minimize $\\|W(Ac-y)\\|_{2}^{2}$.\n*   Parameter vector: $c=\\begin{pmatrix}c_{0}\\\\c_{1}\\end{pmatrix}$.\n*   Measurement vector: $y=\\begin{pmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\end{pmatrix}$.\n\n**Validation Analysis:**\nThe problem is a well-defined mathematical exercise in weighted linear regression. It is scientifically sound, as the method of weighted least squares is a fundamental statistical technique for fitting models to data with non-uniform variance. The problem is self-contained, providing all necessary numerical values and functional forms. The inputs $x_i$ lead to a design matrix $A$ with linearly independent columns, ensuring that the resulting normal equations have a unique solution, thus making the problem well-posed. The terminology is precise and objective. The problem is valid.\n\n### Step 2: Solution Derivation\n\nThe problem requires finding the coefficients $c_0$ and $c_1$ of the linear polynomial $p(x) = c_0 + c_1x$ that minimizes the squared Euclidean norm of the weighted residual vector.\n\n#### Task 1: Explicit Matrices and Vectors\n\nFirst, we construct the matrices and vectors $A$, $W$, and $y$ from the given data.\n\nThe system of equations is $p(x_i) \\approx y_i$ for $i=1, 2, 3$:\n$$\n\\begin{cases}\nc_0 + c_1(-1) \\approx 0 \\\\\nc_0 + c_1(0) \\approx 1 \\\\\nc_0 + c_1(1) \\approx 2\n\\end{cases}\n$$\nThis can be written in matrix form as $Ac \\approx y$. The design matrix $A$ is formed by evaluating the basis functions ($1$ and $x$) at each input $x_i$:\n$$\nA = \\begin{pmatrix} 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{pmatrix}\n$$\nThe vector of measurements $y$ is:\n$$\ny = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\nThe weight matrix $W$ is given by $W=\\mathrm{diag}\\!\\big(\\tfrac{1}{\\sigma_{1}},\\tfrac{1}{\\sigma_{2}},\\tfrac{1}{\\sigma_{3}}\\big)$. With $\\sigma_{1}=1$, $\\sigma_{2}=\\tfrac{1}{2}$, and $\\sigma_{3}=1$, we have:\n$$\nW = \\mathrm{diag}\\!\\big(\\tfrac{1}{1},\\tfrac{1}{1/2},\\tfrac{1}{1}\\big) = \\mathrm{diag}(1, 2, 1) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n\n#### Task 2: Derivation of the Linear System\n\nThe objective is to minimize the scalar function $J(c) = \\|W(Ac-y)\\|_{2}^{2}$.\nUsing the definition of the Euclidean norm ($ \\|v\\|_2^2 = v^T v $), we can write the objective function as:\n$$\nJ(c) = \\big(W(Ac-y)\\big)^T \\big(W(Ac-y)\\big)\n$$\nUsing the transpose property $(XY)^T = Y^T X^T$:\n$$\nJ(c) = (Ac-y)^T W^T W (Ac-y)\n$$\nTo find the minimum, we compute the gradient of $J(c)$ with respect to $c$ and set it to zero. Let's expand the expression for $J(c)$:\n$$\nJ(c) = (c^T A^T - y^T) W^T W (Ac - y)\n$$\n$$\nJ(c) = c^T A^T W^T W A c - c^T A^T W^T W y - y^T W^T W A c + y^T W^T W y\n$$\nThe terms $c^T A^T W^T W y$ and $y^T W^T W A c$ are scalars, and they are transposes of each other. Therefore, they are equal.\n$$\nJ(c) = c^T (A^T W^T W A) c - 2 c^T (A^T W^T W y) + y^T W^T W y\n$$\nThe gradient with respect to the vector $c$ is:\n$$\n\\nabla_c J(c) = \\nabla_c \\Big( c^T (A^T W^T W A) c - 2 c^T (A^T W^T W y) + y^T W^T W y \\Big)\n$$\nUsing the matrix calculus identities $\\nabla_c(c^T M c) = 2Mc$ for a symmetric matrix $M$ and $\\nabla_c(c^T b) = b$:\n$$\n\\nabla_c J(c) = 2(A^T W^T W A) c - 2(A^T W^T W y)\n$$\nSetting the gradient to zero, $\\nabla_c J(c) = 0$:\n$$\n2(A^T W^T W A) c - 2(A^T W^T W y) = 0\n$$\n$$\n(A^T W^T W A) c = A^T W^T W y\n$$\nThis is the linear system, known as the normal equations for the weighted least squares problem, that characterizes the minimizer $c$.\n\n#### Task 3: Solving for $c_0$, $c_1$ and Evaluating $p(2)$\n\nWe now solve the system $(A^T W^T W A) c = A^T W^T W y$.\nSince $W$ is a real diagonal matrix, $W^T = W$. The system becomes $(A^T W^2 A) c = A^T W^2 y$.\nFirst, we compute the matrix $W^2$:\n$$\nW^2 = W^T W = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThis matrix contains the inverse variances $1/\\sigma_i^2$ on its diagonal, as expected for standard weighted least squares.\n\nNext, we calculate the matrix for the left-hand side, $A^T W^2 A$:\n$$\nA^T W^2 A = \\begin{pmatrix} 1 & 1 & 1 \\\\ -1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\end{pmatrix}\n$$\n$$\nA^T W^2 A = \\begin{pmatrix} 1 & 1 & 1 \\\\ -1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ 4 & 0 \\\\ 1 & 1 \\end{pmatrix}\n$$\n$$\nA^T W^2 A = \\begin{pmatrix} (1)(1)+(1)(4)+(1)(1) & (1)(-1)+(1)(0)+(1)(1) \\\\ (-1)(1)+(0)(4)+(1)(1) & (-1)(-1)+(0)(0)+(1)(1) \\end{pmatrix} = \\begin{pmatrix} 6 & 0 \\\\ 0 & 2 \\end{pmatrix}\n$$\nNow, we calculate the vector for the right-hand side, $A^T W^2 y$:\n$$\nA^T W^2 y = \\begin{pmatrix} 1 & 1 & 1 \\\\ -1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\nA^T W^2 y = \\begin{pmatrix} 1 & 1 & 1 \\\\ -1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\n$$\nA^T W^2 y = \\begin{pmatrix} (1)(0)+(1)(4)+(1)(2) \\\\ (-1)(0)+(0)(4)+(1)(2) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix}\n$$\nThe linear system to solve for $c = \\begin{pmatrix} c_0 \\\\ c_1 \\end{pmatrix}$ is:\n$$\n\\begin{pmatrix} 6 & 0 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} c_0 \\\\ c_1 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix}\n$$\nThis diagonal system yields the solution directly:\n$$\n6c_0 = 6 \\implies c_0 = 1\n$$\n$$\n2c_1 = 2 \\implies c_1 = 1\n$$\nThe fitted polynomial is $p(x) = c_0 + c_1 x = 1 + x$.\n\nFinally, we evaluate this polynomial at $x=2$:\n$$\np(2) = 1 + 2 = 3\n$$\nThe data points $(-1, 0)$, $(0, 1)$, and $(1, 2)$ are collinear and lie perfectly on the line $y=x+1$. Thus, the least squares fit, regardless of weighting, yields this exact line, resulting in zero residual error. Our derived coefficients $c_0=1$ and $c_1=1$ are consistent with this observation.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "A model that fits data perfectly is not always a good model, especially when used to predict outside the range of that data. This computational exercise serves as a crucial cautionary tale about the dangers of extrapolation. By fitting a high-degree polynomial to a simple periodic function , you will witness firsthand how a model can be highly accurate in-sample yet fail catastrophically out-of-sample, a vital lesson in responsible modeling.",
            "id": "3262865",
            "problem": "You will implement and analyze a polynomial least squares approximation to a periodic target function sampled without noise. The goal is to demonstrate, through computation, that a polynomial fit can be highly accurate on the training interval yet fail catastrophically when extrapolated outside that interval, thus failing to capture the functionâ€™s periodic nature.\n\nYou must base your derivation and algorithm on core principles of least squares approximation. Use the following well-tested facts and definitions as the foundational starting point:\n- The discrete least squares approximation seeks a function from a finite-dimensional space that minimizes the sum of squared residuals over given sample points.\n- For a polynomial model, the least squares problem can be written in matrix form using the Vandermonde matrix, which encodes monomial basis evaluations at the sample points.\n- Root Mean Squared Error (RMSE) over a set of points is the square root of the average of the squared residuals.\n\nTask:\n- Consider the target function $f(x)=\\sin(x)$ with $x$ in radians.\n- For each test case, generate a training dataset by uniform sampling of $N_{\\text{train}}$ points on the interval $[x_{\\min},x_{\\max}]$. Use $y_i=f(x_i)$ with no noise.\n- Fit a polynomial $p_d(x)$ of degree $d$ to this data in the discrete least squares sense, using a numerically stable method to solve the linear least squares problem defined by the Vandermonde matrix. Do not use the normal equations directly if a more stable method is available in your environment.\n- Compute the in-sample RMSE on the training points, denoted $\\text{RMSE}_{\\text{train}}=\\sqrt{\\frac{1}{N_{\\text{train}}}\\sum_{i=1}^{N_{\\text{train}}}\\left(p_d(x_i)-f(x_i)\\right)^2}$.\n- For evaluation of extrapolation behavior, define a testing interval $[u_{\\min},u_{\\max}]$, generate $N_{\\text{test}}$ points uniformly over it (choose $N_{\\text{test}}=200$), and compute the out-of-sample RMSE $\\text{RMSE}_{\\text{test}}$ analogously.\n- Use the following decision rule to determine whether the polynomial fit is simultaneously accurate in-sample and catastrophically failing out-of-sample:\n  - Define thresholds $\\varepsilon_{\\text{in}}=10^{-2}$ and $\\varepsilon_{\\text{out}}=5\\times 10^{-1}$.\n  - Output the integer $1$ if $\\text{RMSE}_{\\text{train}}\\le \\varepsilon_{\\text{in}}$ and $\\text{RMSE}_{\\text{test}}\\ge \\varepsilon_{\\text{out}}$; otherwise output the integer $0$.\n\nAngle unit: all trigonometric evaluations must use radians.\n\nTest suite:\n- Case $1$: $d=9$, $N_{\\text{train}}=64$, $[x_{\\min},x_{\\max}]=[0,2\\pi]$, $[u_{\\min},u_{\\max}]=[10\\pi,12\\pi]$.\n- Case $2$: $d=15$, $N_{\\text{train}}=100$, $[x_{\\min},x_{\\max}]=[0,2\\pi]$, $[u_{\\min},u_{\\max}]=[8\\pi,10\\pi]$.\n- Case $3$: $d=20$, $N_{\\text{train}}=120$, $[x_{\\min},x_{\\max}]=[0,2\\pi]$, $[u_{\\min},u_{\\max}]=[-12\\pi,-10\\pi]$.\n- Case $4$ (edge case to test the decision rule when the in-sample fit is not highly accurate): $d=1$, $N_{\\text{train}}=64$, $[x_{\\min},x_{\\max}]=[0,2\\pi]$, $[u_{\\min},u_{\\max}]=[8\\pi,10\\pi]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases. For example, the output should be of the form $[r_1,r_2,r_3,r_4]$ where each $r_k$ is either $0$ or $1$ as defined above.",
            "solution": "The problem requires the implementation and analysis of a polynomial least squares approximation to the periodic function $f(x) = \\sin(x)$. The central goal is to demonstrate that a polynomial model, while capable of achieving a high-accuracy fit on a specified training interval, is fundamentally unsuited for extrapolation due to its non-periodic nature. This leads to catastrophic failure when the model is evaluated far outside the domain of the training data.\n\nThe process involves fitting a polynomial of degree $d$ to a set of $N_{\\text{train}}$ noise-free samples of $f(x)$ on an interval $[x_{\\min}, x_{\\max}]$. The quality of the fit is assessed both on the training data (in-sample) and on a separate, non-overlapping test interval $[u_{\\min}, u_{\\max}]$ (out-of-sample). A decision rule is then applied to classify whether the fit is simultaneously good in-sample and poor out-of-sample.\n\nLet the polynomial model of degree $d$ be denoted by $p_d(x)$:\n$$\np_d(x) = \\sum_{j=0}^{d} c_j x^j = c_0 + c_1 x + c_2 x^2 + \\dots + c_d x^d\n$$\nThe task is to determine the vector of coefficients $\\mathbf{c} = [c_0, c_1, \\dots, c_d]^T$ that best fits the training data in the least squares sense. The training data consists of $N_{\\text{train}}$ pairs $(x_i, y_i)$, where the points $x_i$ are sampled uniformly from $[x_{\\min}, x_{\\max}]$ and $y_i = f(x_i) = \\sin(x_i)$.\n\nThe discrete least squares problem is to find the coefficient vector $\\mathbf{c}$ that minimizes the sum of squared residuals, $S$:\n$$\nS = \\sum_{i=1}^{N_{\\text{train}}} (p_d(x_i) - y_i)^2\n$$\nThis can be formulated as a linear algebra problem. Let $\\mathbf{y} = [y_1, y_2, \\dots, y_{N_{\\text{train}}}]^T$ be the vector of observed function values. The set of equations $p_d(x_i) = y_i$ for $i=1, \\dots, N_{\\text{train}}$ forms an overdetermined linear system $A\\mathbf{c} \\approx \\mathbf{y}$, where $A$ is the $N_{\\text{train}} \\times (d+1)$ Vandermonde matrix:\n$$\nA =\n\\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^d \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{N_{\\text{train}}} & x_{N_{\\text{train}}}^2 & \\dots & x_{N_{\\text{train}}}^d\n\\end{pmatrix}\n$$\nThe least squares problem is then equivalent to finding the vector $\\mathbf{c}$ that minimizes the Euclidean norm of the residual vector, $\\|\\mathbf{y} - A\\mathbf{c}\\|_2$.\n\nWhile this minimization problem can be solved formally via the normal equations, $A^TA\\mathbf{c} = A^T\\mathbf{y}$, this approach is numerically unstable. The condition number of the matrix $A^TA$ is the square of that of $A$, and Vandermonde matrices are notoriously ill-conditioned, especially for high degrees $d$. A numerically superior method, which will be employed here, is to use a matrix factorization such as the Singular Value Decomposition (SVD) of $A$. Modern numerical libraries provide robust linear least squares solvers based on such methods. We will use `numpy.linalg.lstsq` for this purpose.\n\nThe algorithmic procedure for each test case is as follows:\n1.  Generate $N_{\\text{train}}$ training points $x_i$ uniformly on $[x_{\\min}, x_{\\max}]$ and compute the corresponding target values $y_i = \\sin(x_i)$.\n2.  Construct the $N_{\\text{train}} \\times (d+1)$ Vandermonde matrix $A$ where $A_{ij} = x_i^j$ for $i=1, \\dots, N_{\\text{train}}$ and $j=0, \\dots, d$.\n3.  Solve the linear least squares system $A\\mathbf{c} \\approx \\mathbf{y}$ for the coefficient vector $\\mathbf{c}$.\n4.  Calculate the in-sample Root Mean Squared Error ($\\text{RMSE}_{\\text{train}}$). The predicted values on the training set are $\\hat{y}_i = p_d(x_i)$.\n    $$\n    \\text{RMSE}_{\\text{train}} = \\sqrt{\\frac{1}{N_{\\text{train}}}\\sum_{i=1}^{N_{\\text{train}}} (\\hat{y}_i - y_i)^2}\n    $$\n5.  Generate $N_{\\text{test}} = 200$ test points $u_k$ uniformly on the extrapolation interval $[u_{\\min}, u_{\\max}]$ and compute the true function values $v_k = \\sin(u_k)$.\n6.  Evaluate the polynomial $p_d(x)$ at the test points to get the predicted values $\\hat{v}_k = p_d(u_k)$.\n7.  Calculate the out-of-sample Root Mean Squared Error ($\\text{RMSE}_{\\text{test}}$):\n    $$\n    \\text{RMSE}_{\\text{test}} = \\sqrt{\\frac{1}{N_{\\text{test}}}\\sum_{k=1}^{N_{\\text{test}}} (\\hat{v}_k - v_k)^2}\n    $$\n8.  Apply the decision rule: if $\\text{RMSE}_{\\text{train}} \\le \\varepsilon_{\\text{in}}$ and $\\text{RMSE}_{\\text{test}} \\ge \\varepsilon_{\\text{out}}$, where $\\varepsilon_{\\text{in}} = 10^{-2}$ and $\\varepsilon_{\\text{out}} = 5 \\times 10^{-1}$, the result for the test case is $1$. Otherwise, the result is $0$.\n\nThis procedure is systematically applied to all specified test cases to determine the final output. The high-degree polynomial models are expected to fit the single cycle of the sine wave very well, but because polynomials are non-periodic and grow unboundedly as $|x| \\to \\infty$, they will diverge drastically from the periodic sine function on intervals far from the origin, leading to a large $\\text{RMSE}_{\\text{test}}$. The low-degree linear model (case $4$) is not flexible enough to capture the curvature of the sine function, resulting in a poor in-sample fit and failing the first condition of the decision rule.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes polynomial least squares approximation to demonstrate\n    the failure of extrapolation for non-periodic models of periodic functions.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: d=9, N_train=64, [x_min,x_max]=[0,2pi], [u_min,u_max]=[10pi,12pi]\n        {'d': 9, 'N_train': 64, 'x_range': [0, 2 * np.pi], 'u_range': [10 * np.pi, 12 * np.pi]},\n        # Case 2: d=15, N_train=100, [x_min,x_max]=[0,2pi], [u_min,u_max]=[8pi,10pi]\n        {'d': 15, 'N_train': 100, 'x_range': [0, 2 * np.pi], 'u_range': [8 * np.pi, 10 * np.pi]},\n        # Case 3: d=20, N_train=120, [x_min,x_max]=[0,2pi], [u_min,u_max]=[-12pi,-10pi]\n        {'d': 20, 'N_train': 120, 'x_range': [0, 2 * np.pi], 'u_range': [-12 * np.pi, -10 * np.pi]},\n        # Case 4: d=1, N_train=64, [x_min,x_max]=[0,2pi], [u_min,u_max]=[8pi,10pi]\n        {'d': 1, 'N_train': 64, 'x_range': [0, 2 * np.pi], 'u_range': [8 * np.pi, 10 * np.pi]},\n    ]\n\n    # Constants defined in the problem\n    N_test = 200\n    eps_in = 1e-2\n    eps_out = 0.5\n    \n    results = []\n\n    for case in test_cases:\n        d = case['d']\n        N_train = case['N_train']\n        x_min, x_max = case['x_range']\n        u_min, u_max = case['u_range']\n\n        # 1. Generate training data\n        x_train = np.linspace(x_min, x_max, N_train)\n        y_train = np.sin(x_train)\n\n        # 2. Construct the Vandermonde matrix for the training data\n        # The polynomial is p(x) = c_0 + c_1*x + ... + c_d*x^d\n        # increasing=True makes columns 1, x, x^2, ...\n        A_train = np.vander(x_train, N=d + 1, increasing=True)\n        \n        # 3. Solve for the polynomial coefficients c = [c_0, c_1, ..., c_d]\n        # np.linalg.lstsq provides a numerically stable SVD-based solution.\n        coeffs, _, _, _ = np.linalg.lstsq(A_train, y_train, rcond=None)\n\n        # 4. Compute in-sample RMSE\n        # Evaluate polynomial on training points\n        y_pred_train = A_train @ coeffs\n        rmse_train = np.sqrt(np.mean((y_pred_train - y_train)**2))\n\n        # 5. Generate test data\n        x_test = np.linspace(u_min, u_max, N_test)\n        y_test_true = np.sin(x_test)\n\n        # 6. Compute out-of-sample RMSE\n        # Evaluate polynomial on test points.\n        # np.polyval expects coefficients for descending powers (c_d, c_{d-1}, ...),\n        # so we reverse the 'coeffs' array.\n        y_pred_test = np.polyval(coeffs[::-1], x_test)\n        rmse_test = np.sqrt(np.mean((y_pred_test - y_test_true)**2))\n        \n        # 7. Apply the decision rule\n        if rmse_train = eps_in and rmse_test >= eps_out:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}