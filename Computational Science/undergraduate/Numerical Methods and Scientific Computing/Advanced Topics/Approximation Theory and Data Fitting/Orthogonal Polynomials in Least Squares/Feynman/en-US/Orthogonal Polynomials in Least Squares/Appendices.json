{
    "hands_on_practices": [
        {
            "introduction": "This first exercise grounds our study in the fundamental concept of least-squares approximation. We will determine the best possible constant function to approximate a given function, which illustrates the principle of minimizing the integrated squared error. This is equivalent to projecting the function onto the simplest possible polynomial spaceâ€”the space of constant functions, spanned by $p_0(x) = 1$. ",
            "id": "2192794",
            "problem": "In many applications of numerical analysis and signal processing, it is often useful to approximate a complex function with a simpler one. The quality of such an approximation is frequently measured using a least-squares criterion.\n\nConsider the function $f(x) = \\cos(x)$ defined on the interval $[0, \\pi/2]$. We wish to find the best possible approximation of $f(x)$ using a constant function, say $g(x) = c$. The best constant approximation in the continuous least-squares sense is the value of the constant $c$ that minimizes the integral of the squared error between $f(x)$ and $c$ over the given interval. This error, as a function of $c$, is defined by the integral:\n$$ E(c) = \\int_{0}^{\\pi/2} [f(x) - c]^2 \\, dx $$\nDetermine the value of this optimal constant $c$. Your answer should be a single closed-form analytic expression.",
            "solution": "We seek the constant $c$ minimizing the squared-error functional $E(c) = \\int_{0}^{\\pi/2} [\\cos(x) - c]^{2} \\, dx$. This is the continuous least-squares projection of $f(x)=\\cos(x)$ onto the subspace spanned by the constant function $1$ under the inner product $\\langle u, v \\rangle = \\int_{0}^{\\pi/2} u(x)v(x) \\, dx$. The minimizer satisfies the normal equation $\\int_{0}^{\\pi/2} [\\cos(x) - c]\\cdot 1 \\, dx = 0$. Equivalently, we differentiate $E(c)$ with respect to $c$ to obtain the same condition.\n\nDifferentiate $E(c)$:\n$$\nE'(c) = \\int_{0}^{\\pi/2} 2[\\cos(x) - c](-1) \\, dx = 2 \\int_{0}^{\\pi/2} [c - \\cos(x)] \\, dx.\n$$\nSetting $E'(c)=0$ gives\n$$\n0 = 2 \\left[ c \\int_{0}^{\\pi/2} dx - \\int_{0}^{\\pi/2} \\cos(x) \\, dx \\right].\n$$\nThus,\n$$\nc \\int_{0}^{\\pi/2} dx = \\int_{0}^{\\pi/2} \\cos(x) \\, dx.\n$$\nCompute the integrals:\n$$\n\\int_{0}^{\\pi/2} dx = \\frac{\\pi}{2}, \\qquad \\int_{0}^{\\pi/2} \\cos(x) \\, dx = \\left. \\sin(x) \\right|_{0}^{\\pi/2} = 1.\n$$\nTherefore,\n$$\nc \\cdot \\frac{\\pi}{2} = 1 \\quad \\Longrightarrow \\quad c = \\frac{2}{\\pi}.\n$$\nTo confirm it is a minimizer, compute the second derivative:\n$$\nE''(c) = 2 \\int_{0}^{\\pi/2} dx = 2 \\cdot \\frac{\\pi}{2} = \\pi > 0,\n$$\nso $c=\\frac{2}{\\pi}$ is the unique minimizer.",
            "answer": "$$\\boxed{\\frac{2}{\\pi}}$$"
        },
        {
            "introduction": "Building on the foundational concept of least-squares, this practice reveals the true power of orthogonality. By attempting to approximate a specific Legendre polynomial with a lower-degree polynomial, we discover how orthogonality can make complex calculations nearly trivial. This exercise highlights how understanding the underlying structure of orthogonal function spaces can provide deep insight and allow us to find solutions far more efficiently than by solving the standard normal equations. ",
            "id": "2192780",
            "problem": "Consider the function $f(x) = \\frac{3}{2}x^2 - \\frac{1}{2}$ defined on the interval $[-1, 1]$. We are interested in finding the best linear approximation to this function in the least-squares sense. This approximation is a polynomial of the form $L(x) = c_0 + c_1 x$, where the real coefficients $c_0$ and $c_1$ are chosen to minimize the squared error integral, defined as:\n\n$$E(c_0, c_1) = \\int_{-1}^{1} (f(x) - L(x))^2 \\, dx$$\n\nDetermine the linear polynomial $L(x)$ that best approximates $f(x)$ on the interval $[-1, 1]$. Express your answer as a polynomial in terms of $x$.",
            "solution": "We seek the best least-squares linear approximation $L(x)=c_{0}+c_{1}x$ to $f(x)=\\frac{3}{2}x^{2}-\\frac{1}{2}$ on $[-1,1]$ with respect to the inner product $\\langle g,h\\rangle=\\int_{-1}^{1}g(x)h(x)\\,dx$. The least-squares condition is that the residual $r(x)=f(x)-c_{0}-c_{1}x$ is orthogonal to the subspace spanned by $\\{1,x\\}$, which yields the normal equations\n$$\n\\int_{-1}^{1}\\left(f(x)-c_{0}-c_{1}x\\right)\\,dx=0,\\qquad \\int_{-1}^{1}x\\left(f(x)-c_{0}-c_{1}x\\right)\\,dx=0.\n$$\nExpanding these gives\n$$\n\\int_{-1}^{1}f(x)\\,dx - c_{0}\\int_{-1}^{1}1\\,dx - c_{1}\\int_{-1}^{1}x\\,dx=0,\n$$\n$$\n\\int_{-1}^{1}x f(x)\\,dx - c_{0}\\int_{-1}^{1}x\\,dx - c_{1}\\int_{-1}^{1}x^{2}\\,dx=0.\n$$\nCompute the needed integrals. First,\n$$\n\\int_{-1}^{1}f(x)\\,dx=\\int_{-1}^{1}\\left(\\frac{3}{2}x^{2}-\\frac{1}{2}\\right)\\,dx=\\frac{3}{2}\\int_{-1}^{1}x^{2}\\,dx-\\frac{1}{2}\\int_{-1}^{1}1\\,dx.\n$$\nUsing $\\int_{-1}^{1}x^{2}\\,dx=\\left[\\frac{x^{3}}{3}\\right]_{-1}^{1}=\\frac{2}{3}$ and $\\int_{-1}^{1}1\\,dx=2$, we get\n$$\n\\int_{-1}^{1}f(x)\\,dx=\\frac{3}{2}\\cdot\\frac{2}{3}-\\frac{1}{2}\\cdot 2=1-1=0.\n$$\nNext, since $f$ is even and $x$ is odd, $x f(x)$ is odd, hence\n$$\n\\int_{-1}^{1}x f(x)\\,dx=0.\n$$\nAlso, $\\int_{-1}^{1}x\\,dx=0$ and $\\int_{-1}^{1}x^{2}\\,dx=\\frac{2}{3}$ as above. Therefore, the normal equations reduce to\n$$\n0 - 2c_{0} - c_{1}\\cdot 0=0 \\quad\\Rightarrow\\quad c_{0}=0,\n$$\n$$\n0 - c_{0}\\cdot 0 - c_{1}\\cdot \\frac{2}{3}=0 \\quad\\Rightarrow\\quad c_{1}=0.\n$$\nHence the best least-squares linear approximation on $[-1,1]$ is\n$$\nL(x)=0.\n$$",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "This final practice shifts our focus from continuous functions to discrete data, a scenario frequently encountered in experimental science and engineering. We will explore how to generate a custom set of polynomials that are orthogonal with respect to a discrete inner product defined by data points. The exercise introduces the computationally elegant three-term recurrence relation, a cornerstone algorithm for building orthogonal bases in numerical applications. ",
            "id": "2192741",
            "problem": "In the numerical analysis of experimental data, it is often advantageous to work with a basis of functions that are orthogonal with respect to the data points. Consider a set of four experimental measurements at coordinates $\\{x_i\\} = \\{0, 1, 3, 6\\}$. We are interested in generating a sequence of monic polynomials $\\{p_k(x)\\}_{k=0}^\\infty$ that are orthogonal with respect to a discrete inner product.\n\nThe inner product of two real-valued functions, $f(x)$ and $g(x)$, over this set of points is defined as:\n$$ \\langle f, g \\rangle = \\sum_{i=1}^{4} f(x_i) g(x_i) $$\nThe sequence of orthogonal monic polynomials is generated by the three-term recurrence relation:\n$$ p_{k+1}(x) = (x - \\alpha_k) p_k(x) - \\beta_k p_{k-1}(x) $$\nfor $k \\ge 0$, with the initial conditions defined as $p_0(x) = 1$ and $p_{-1}(x) = 0$.\n\nYour task is to determine the exact value of the recurrence coefficient $\\alpha_1$. Express your final answer as a fraction in simplest form.",
            "solution": "We use the given discrete inner product\n$$\\langle f, g \\rangle = \\sum_{i=1}^{4} f(x_{i}) g(x_{i}), \\quad \\{x_{i}\\}=\\{0,1,3,6\\},$$\nand the monic three-term recurrence\n$$p_{k+1}(x)=(x-\\alpha_{k})p_{k}(x)-\\beta_{k}p_{k-1}(x), \\quad p_{0}(x)=1,\\; p_{-1}(x)=0.$$\n\nFor monic orthogonal polynomials, taking the inner product of the recurrence with $p_{k}$ yields\n$$\\langle p_{k+1},p_{k}\\rangle=\\langle (x-\\alpha_{k})p_{k},p_{k}\\rangle-\\beta_{k}\\langle p_{k-1},p_{k}\\rangle.$$\nBy orthogonality, $\\langle p_{k+1},p_{k}\\rangle=0$ and $\\langle p_{k-1},p_{k}\\rangle=0$, hence\n$$0=\\langle x p_{k},p_{k}\\rangle-\\alpha_{k}\\langle p_{k},p_{k}\\rangle,$$\nso\n$$\\alpha_{k}=\\frac{\\langle x p_{k},p_{k}\\rangle}{\\langle p_{k},p_{k}\\rangle}.$$\n\nFirst, compute $\\alpha_{0}$:\n$$\\alpha_{0}=\\frac{\\langle x p_{0},p_{0}\\rangle}{\\langle p_{0},p_{0}\\rangle}\n=\\frac{\\sum_{i=1}^{4} x_{i}}{\\sum_{i=1}^{4} 1}\n=\\frac{0+1+3+6}{4}\n=\\frac{10}{4}\n=\\frac{5}{2}.$$\nThus\n$$p_{1}(x)=x-\\alpha_{0}=x-\\frac{5}{2}.$$\n\nNow compute $\\alpha_{1}$ using\n$$\\alpha_{1}=\\frac{\\langle x p_{1},p_{1}\\rangle}{\\langle p_{1},p_{1}\\rangle}\n=\\frac{\\sum_{i=1}^{4} x_{i}\\,p_{1}(x_{i})^{2}}{\\sum_{i=1}^{4} p_{1}(x_{i})^{2}}.$$\nEvaluate $p_{1}(x)$ at the data points:\n$$p_{1}(0)=-\\frac{5}{2},\\quad p_{1}(1)=-\\frac{3}{2},\\quad p_{1}(3)=\\frac{1}{2},\\quad p_{1}(6)=\\frac{7}{2}.$$\nSquares:\n$$p_{1}(0)^{2}=\\frac{25}{4},\\quad p_{1}(1)^{2}=\\frac{9}{4},\\quad p_{1}(3)^{2}=\\frac{1}{4},\\quad p_{1}(6)^{2}=\\frac{49}{4}.$$\nDenominator:\n$$\\langle p_{1},p_{1}\\rangle=\\frac{25}{4}+\\frac{9}{4}+\\frac{1}{4}+\\frac{49}{4}=\\frac{84}{4}=21.$$\nNumerator:\n$$\\langle x p_{1},p_{1}\\rangle=0\\cdot\\frac{25}{4}+1\\cdot\\frac{9}{4}+3\\cdot\\frac{1}{4}+6\\cdot\\frac{49}{4}\n=\\frac{9}{4}+\\frac{3}{4}+\\frac{294}{4}=\\frac{306}{4}=\\frac{153}{2}.$$\nTherefore\n$$\\alpha_{1}=\\frac{\\frac{153}{2}}{21}=\\frac{153}{42}=\\frac{51}{14}.$$\nThis fraction is already in simplest form.",
            "answer": "$$\\boxed{\\frac{51}{14}}$$"
        }
    ]
}