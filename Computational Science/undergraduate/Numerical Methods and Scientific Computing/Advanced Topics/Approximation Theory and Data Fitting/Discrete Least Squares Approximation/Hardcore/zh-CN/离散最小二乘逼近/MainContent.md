## 引言
在科学研究与工程实践的广阔天地中，我们几乎总是面对着从离散、带有噪声的观测数据中提取规律和构建模型的挑战。无论是追踪一颗行星的轨迹，分析一段[金融时间序列](@entry_id:139141)，还是处理一张医学图像，核心问题都是相通的：如何找到一个数学函数，能够“最佳”地拟合这些数据点？离散[最小二乘逼近](@entry_id:148277)正是应对这一根本问题的基石性方法，它以其简洁的数学形式和深刻的物理内涵，成为了数据科学、数值计算乃至整个现代科技领域中不可或缺的工具。

然而，仅仅了解其“最小化[误差平方和](@entry_id:149299)”的表层概念是远远不够的。这背后隐藏着丰富的线性代数理论、深刻的几何直觉以及关乎成败的数值计算细节。本文旨在系统性地剖析离散最小二乘法，解决从“是什么”到“怎么做”再到“如何做好”的全链条问题。我们将带领读者穿越理论的迷雾，掌握从数据到模型之间的桥梁。

在接下来的内容中，我们将分三个章节逐步深入：
*   在**原理与机制**一章，我们将从最小二乘法的基本原理和几何诠释出发，推导出经典的正规方程，并重点探讨为何在实际计算中必须转向QR分解和[奇异值分解](@entry_id:138057)（SVD）等更为数值稳健的方法。
*   在**应用与跨学科联系**一章，我们将走出纯粹的数学理论，探索最小二乘法如何在物理学、信号处理、[计算机视觉](@entry_id:138301)和金融学等多个前沿领域中大放异彩，展示其解决真实世界问题的强大能力。
*   最后，在**动手实践**部分，您将有机会通过解决精心设计的编程问题，亲手实现和比较不同的算法，将理论知识转化为牢固的实践技能。

通过本文的学习，您将不仅掌握离散最小二乘法的计算方法，更能深刻理解其背后的数学思想，从而在未来的学术研究或工程项目中充满信心地应用这一强大工具。

## 原理与机制

本章旨在深入探讨离散[最小二乘逼近](@entry_id:148277)的数学原理与计算机制。我们将从其核心思想——最小化[误差平方和](@entry_id:149299)——出发，逐步构建起该方法的完整理论框架。内容将涵盖如何将一个实际问题转化为标准的线性代数形式，通过几何投影的视角理解其内在机理，并详细比较几种关键的数值求解策略，包括经典的[正规方程](@entry_id:142238)法以及更为稳健的基于矩阵分解（如QR和SVD）的方法。此外，我们还将讨论[最小二乘法](@entry_id:137100)的一些重要变体，如加权最小二乘和总体最小二乘，并分析解的稳定性和近似过程中可能出现的现象。

### 最小二乘法的基本原理

在科学研究和工程实践中，我们常常需要从一组带有测量误差的数据中提取规律，即寻找一个数学模型来描述数据点之间的关系。离散[最小二乘逼近](@entry_id:148277)为此提供了一个基础而强大的框架。其核心思想非常直观：**最佳的模型，是使其预测值与实际观测值之间的差异最小化的模型**。

那么，如何量化这种“差异”呢？最小二乘法选择的度量标准是**[残差平方和](@entry_id:174395) (Sum of Squared Residuals, SSR)**。对于第 $i$ 个数据点，其观测值为 $y_i$，模型基于[自变量](@entry_id:267118) $\mathbf{x}_i$ 给出的预测值为 $f(\mathbf{x}_i; \boldsymbol{\beta})$，其中 $\boldsymbol{\beta}$ 是模型的待定参数。残差 $r_i$ 定义为观测值与预测值之差：

$r_i = y_i - f(\mathbf{x}_i; \boldsymbol{\beta})$

最小二乘法的目标，就是寻找一组最优的参数 $\boldsymbol{\beta}$，使得所有数据点的[残差平方和](@entry_id:174395) $S$ 达到最小：

$S(\boldsymbol{\beta}) = \sum_{i=1}^{m} r_i^2 = \sum_{i=1}^{m} (y_i - f(\mathbf{x}_i; \boldsymbol{\beta}))^2 \to \min$

其中 $m$ 是数据点的总数。选择平方和作为目标函数，不仅因为其在数学上处理方便（例如，函数光滑可导），更重要的是，在高斯误差的假设下，[最小二乘解](@entry_id:152054)等价于**最大似然估计**，这为其提供了坚实的统计学基础。

### [线性模型](@entry_id:178302)的矩阵表述

当模型函数 $f(\mathbf{x}_i; \boldsymbol{\beta})$ 相对于待定参数 $\boldsymbol{\beta}$ 是线性的时候，问题便转化为**线性最小二乘问题**。值得注意的是，这里的“线性”指的是参数的线性，模型本身对于自变量 $\mathbf{x}_i$ 可以是高度[非线性](@entry_id:637147)的。例如，一个 $k$ 次[多项式模型](@entry_id:752298) $p_k(x) = c_0 + c_1 x + \dots + c_k x^k$ 对于其系数 $[c_0, \dots, c_k]$ 是线性的。

线性模型的普适性使得我们可以将[最小二乘问题](@entry_id:164198)统一表示为矩阵形式。考虑一个拥有 $p$ 个待定参数 $\beta_1, \dots, \beta_p$ 的模型。对于 $m$ 个数据点，我们可以写出 $m$ 个方程：

$f(\mathbf{x}_1; \boldsymbol{\beta}) \approx y_1$
$f(\mathbf{x}_2; \boldsymbol{\beta}) \approx y_2$
$\vdots$
$f(\mathbf{x}_m; \boldsymbol{\beta}) \approx y_m$

这构成了一个超定[方程组](@entry_id:193238)（通常 $m > p$），我们可以将其写为简洁的矩阵形式：

$A \boldsymbol{\beta} \approx \mathbf{y}$

其中：
- $\boldsymbol{\beta} = [\beta_1, \beta_2, \dots, \beta_p]^T$ 是 $(p \times 1)$ 的**参数矢量 (parameter vector)**。
- $\mathbf{y} = [y_1, y_2, \dots, y_m]^T$ 是 $(m \times 1)$ 的**观测矢量 (observation vector)**。
- $A$ 是一个 $(m \times p)$ 的矩阵，被称为**[设计矩阵](@entry_id:165826) (design matrix)**。它的每一行对应一个数据点，每一列[对应模](@entry_id:200367)型的一个[基函数](@entry_id:170178)在所有数据点上的取值。

[设计矩阵](@entry_id:165826) $A$ 的构建是应用最小二乘法的关键一步。例如，在拟合三维空间中的一个平面 $z = ax + by + c$ 时 ，参数矢量为 $\boldsymbol{\beta} = [a, b, c]^T$。对于第 $i$ 个数据点 $(x_i, y_i, z_i)$，模型方程为 $ax_i + by_i + c = z_i$。将所有数据点的方程整合起来，[设计矩阵](@entry_id:165826) $A$ 和观测矢量 $\mathbf{y}$ 分别为：

$A = \begin{pmatrix} x_1 & y_1 & 1 \\ x_2 & y_2 & 1 \\ \vdots & \vdots & \vdots \\ x_m & y_m & 1 \end{pmatrix}, \quad \mathbf{y} = \begin{pmatrix} z_1 \\ z_2 \\ \vdots \\ z_m \end{pmatrix}$

[设计矩阵](@entry_id:165826)可以容纳各种类型的变量。在一个更复杂的房地产价格预测模型中 ，价格可能被建模为房屋面积（连续变量）、卧室数量（整数变量）和地理位置（[分类变量](@entry_id:637195)）的[线性组合](@entry_id:154743)。对于[分类变量](@entry_id:637195)，我们通常使用**[独热编码](@entry_id:170007) (one-hot encoding)**。例如，若位置有“市区”、“郊区”、“乡村”三类，我们可以选择“乡村”为基准，引入两个[指示变量](@entry_id:266428) $I_{\text{urban}}$ 和 $I_{\text{suburban}}$。此时，模型 $f = \beta_0 + \beta_1 \cdot \text{面积} + \beta_2 \cdot \text{卧室数} + \gamma_{\text{urban}} I_{\text{urban}} + \gamma_{\text{suburban}} I_{\text{suburban}}$ 对应的[设计矩阵](@entry_id:165826)的一行将包含 `[1, 面积, 卧室数, 是否市区, 是否郊区]` 等值。

在矩阵形式下，残差矢量为 $\mathbf{r} = \mathbf{y} - A\boldsymbol{\beta}$。最小二乘的[目标函数](@entry_id:267263) $S(\boldsymbol{\beta})$ 就是残差矢量的欧几里得范数的平方：

$\min_{\boldsymbol{\beta}} S(\boldsymbol{\beta}) = \min_{\boldsymbol{\beta}} \|\mathbf{y} - A\boldsymbol{\beta}\|_2^2$

### [最小二乘问题](@entry_id:164198)的几何诠释

矩阵表述为我们提供了强大的几何直觉。向量 $\mathbf{y}$ 存在于 $m$ 维的观测空间 $\mathbb{R}^m$ 中。[设计矩阵](@entry_id:165826) $A$ 的所有列向量张成一个[子空间](@entry_id:150286)，称为 $A$ 的**列空间 (column space)**，记作 $\text{Col}(A)$。任何形如 $A\boldsymbol{\beta}$ 的向量都位于这个列空间内。

因此，[最小二乘问题](@entry_id:164198) $\min \|\mathbf{y} - A\boldsymbol{\beta}\|_2$ 在几何上等价于：**在 $A$ 的[列空间](@entry_id:156444)中，寻找一个向量 $\hat{\mathbf{y}} = A\boldsymbol{\beta}$，使其与观测向量 $\mathbf{y}$ 的距离最近**。

根据线性代数的基本原理，这个最近的向量正是 $\mathbf{y}$ 在[子空间](@entry_id:150286) $\text{Col}(A)$ 上的**正交投影 (orthogonal projection)**。当 $\hat{\mathbf{y}}$ 是 $\mathbf{y}$ 的[正交投影](@entry_id:144168)时，残差向量 $\mathbf{r} = \mathbf{y} - \hat{\mathbf{y}}$ 必须与 $\text{Col}(A)$ 中的任何向量都正交。这意味着残差向量 $\mathbf{r}$ 位于 $\text{Col}(A)$ 的正交补空间中。

这个[正交性条件](@entry_id:168905)是[最小二乘解](@entry_id:152054)的核心。一个向量与整个[子空间](@entry_id:150286)正交，等价于它与该[子空间](@entry_id:150286)的任意一组[基向量](@entry_id:199546)都正交。由于 $A$ 的列向量构成了 $\text{Col}(A)$ 的一组[生成集](@entry_id:156303)，所以[正交性条件](@entry_id:168905)可以写为：

$A^T \mathbf{r} = \mathbf{0}$

这个关系揭示了[最小二乘解](@entry_id:152054)的本质。它将一个观测向量 $\mathbf{y}$ 分解为两个相互正交的部分：一个是位于模型能够描述的空间（$A$ 的[列空间](@entry_id:156444)）中的投影分量 $\hat{\mathbf{y}} = A\boldsymbol{\beta}$，另一个是模型无法解释的残差分量 $\mathbf{r}$ 。根据勾股定理，我们有 $\|\mathbf{y}\|_2^2 = \|A\boldsymbol{\beta}\|_2^2 + \|\mathbf{r}\|_2^2$。

### 求解方法一：正规方程

基于上述的[正交性条件](@entry_id:168905) $A^T \mathbf{r} = \mathbf{0}$，我们可以推导出求解参数 $\boldsymbol{\beta}$ 的代数方法。将 $\mathbf{r} = \mathbf{y} - A\boldsymbol{\beta}$ 代入，得到：

$A^T (\mathbf{y} - A\boldsymbol{\beta}) = \mathbf{0}$

整理后可得：

$(A^T A) \boldsymbol{\beta} = A^T \mathbf{y}$

这个[方程组](@entry_id:193238)被称为**[正规方程](@entry_id:142238) (Normal Equations)**。它是一个 $p \times p$ 的[线性方程组](@entry_id:148943)。如果矩阵 $A$ 的列是[线性无关](@entry_id:148207)的（即 $A$ 是**列满秩**的），那么矩阵 $A^T A$ 就是对称正定的，并且是可逆的。在这种情况下，存在唯一的[最小二乘解](@entry_id:152054)：

$\boldsymbol{\beta} = (A^T A)^{-1} A^T \mathbf{y}$

正规方程提供了一种直接、优雅的解析解法，在理论分析中非常重要。然而，在实际的数值计算中，直接求解正规方程可能会遇到严重问题。

### 求解方法二：[数值稳定性](@entry_id:146550)与矩阵分解

#### 正规方程的数值缺陷

尽管[正规方程](@entry_id:142238)在理论上很完美，但在浮点数运算的计算机上，其数值表现可能非常糟糕。主要问题在于计算[叉积](@entry_id:156672)矩阵 $A^T A$ 的过程会恶化问题的**条件数 (condition number)**。一个矩阵的条件数 $\kappa(A)$ 衡量了其输出对输入的微小变化的敏感程度，值越大表示问题越“病态”。

形成 $A^T A$ 的操作会将原矩阵 $A$ 的条件数平方，即 $\kappa(A^T A) = \kappa(A)^2$ 。如果 $A$ 本身就是病态的（例如，其列向量近似[线性相关](@entry_id:185830)），那么 $A^T A$ 的条件数可能会大到超出计算机[浮点精度](@entry_id:138433)的表示范围。例如，若 $\kappa(A) \approx 10^8$，在[双精度](@entry_id:636927)[浮点数](@entry_id:173316)下（[机器精度](@entry_id:756332) $\epsilon_{\text{mach}} \approx 10^{-16}$）求解是可行的。但 $\kappa(A^T A) \approx 10^{16}$，此时 $A^T A$ 在数值上已与奇异矩阵无异，求解[正规方程](@entry_id:142238)将导致巨大的误差。

一个经典的例子是当[设计矩阵](@entry_id:165826)的列向量几乎平行时 。假设 $A$ 的两列 $c_1$ 和 $c_2$ 非常接近，即 $c_2 \approx c_1$。在计算 $A^T A$ 的元素时，对角线上的元素 $c_1^T c_1$ 和 $c_2^T c_2$ 会非常接近，而离对角[线元](@entry_id:196833)素 $c_1^T c_2$ 也与它们大小相仿。在浮点运算中，计算 $c_2^T c_2$ 过程中包含的微小差异信息可能会因为与大数值相加而被“吞噬”（称为**[灾难性抵消](@entry_id:146919)**或**大数吃小数**），导致计算出的 $A^T A$ 矩阵变为奇异矩阵，从而无法求解。

因此，在严肃的[科学计算](@entry_id:143987)中，应避免直接形成并求解正规方程。取而代之的是使用更为数值稳定的矩阵分解方法。

#### 基于[QR分解](@entry_id:139154)的稳定求解

[QR分解](@entry_id:139154)是将矩阵 $A$ 分解为一个列[正交矩阵](@entry_id:169220) $Q$ 和一个[上三角矩阵](@entry_id:150931) $R$ 的乘积，即 $A = QR$。其中 $Q$ 满足 $Q^T Q = I$。将此分解代入最小二乘的目标函数：

$\|\mathbf{y} - A\boldsymbol{\beta}\|_2^2 = \|\mathbf{y} - QR\boldsymbol{\beta}\|_2^2$

由于[正交变换](@entry_id:155650)不改变向量的欧几里得范数，我们可以用 $Q^T$ 左乘内部的向量：

$\|Q^T(\mathbf{y} - QR\boldsymbol{\beta})\|_2^2 = \|Q^T\mathbf{y} - (Q^T Q)R\boldsymbol{\beta}\|_2^2 = \|Q^T\mathbf{y} - R\boldsymbol{\beta}\|_2^2$

因为 $R$ 是[上三角矩阵](@entry_id:150931)，这个问题变得非常容易求解。令 $\mathbf{d} = Q^T\mathbf{y}$，我们只需解一个上三角[方程组](@entry_id:193238) $R\boldsymbol{\beta} = \mathbf{d}$，这可以通过**[回代](@entry_id:146909) (back substitution)** 高效完成。

QR方法的核心优势在于它直接对矩阵 $A$ 进行操作，避免了计算 $A^T A$。整个过程只涉及数值稳定的正交变换（如[Householder变换](@entry_id:168808)或[Givens旋转](@entry_id:167475)），因此不会平方条件数。该方法的[条件数](@entry_id:145150)由 $R$ 决定，而 $\kappa(R) = \kappa(A)$，从而保证了更好的数值稳定性 。

#### 基于SVD的通用解法与[伪逆](@entry_id:140762)

对于解决最小二乘问题，**[奇异值分解](@entry_id:138057) (Singular Value Decomposition, SVD)** 是最强大、最富有洞察力的工具。任何 $m \times p$ 矩阵 $A$ 都可以分解为：

$A = U \Sigma V^T$

其中：
- $U$ 是一个 $m \times m$ 的[正交矩阵](@entry_id:169220)，其列为[左奇异向量](@entry_id:751233)。
- $V$ 是一个 $p \times p$ 的正交矩阵，其列为[右奇异向量](@entry_id:754365)。
- $\Sigma$ 是一个 $m \times p$ 的对角矩阵，其对角线上的元素 $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p \ge 0$ 称为**[奇异值](@entry_id:152907) (singular values)**。

SVD不仅能处理列满秩的情况，还能优雅地处理**[秩亏](@entry_id:754065) (rank-deficient)** 的情况，即当 $A$ 的某些列线性相关时。例如，在房价预测模型中，如果房屋面积和卧室数量高度相关（如 $s_i \approx 50 b_i$），或者某个[分类变量](@entry_id:637195)在数据中从未出现（导致其指示列全为零），[设计矩阵](@entry_id:165826)就会[秩亏](@entry_id:754065) 。

利用SVD，[最小二乘解](@entry_id:152054)可以表示为：

$\boldsymbol{\beta} = A^\dagger \mathbf{y}$

其中 $A^\dagger$ 是 $A$ 的**[穆尔-彭罗斯伪逆](@entry_id:147255) (Moore-Penrose pseudoinverse)**，其SVD形式为 $A^\dagger = V \Sigma^\dagger U^T$。$\Sigma^\dagger$ 是将 $\Sigma$ 的所有非零[奇异值](@entry_id:152907)取倒数后[转置](@entry_id:142115)得到的。

当矩阵 $A$ [秩亏](@entry_id:754065)时，最小二乘问题的解有无穷多个。SVD给出的解 $\boldsymbol{\beta} = A^\dagger \mathbf{y}$ 是所有解中欧几里得范数 $\|\boldsymbol{\beta}\|_2$ 最小的那个，称为**[最小范数解](@entry_id:751996)** 。这在实践中通常是最合乎物理意义或最稳健的解。

在数值计算中，由于[浮点误差](@entry_id:173912)，理论上为零的奇异值可能会表现为非常小的正数。因此，我们通常需要设定一个阈值 $\tau$ 来判断一个[奇异值](@entry_id:152907)是否“数值上为零”，从而确定矩阵的**[数值秩](@entry_id:752818)**。一个常用的阈值是 $\tau = \max(m, p) \cdot \sigma_1 \cdot \epsilon_{\text{mach}}$。SVD方法能够通过识别并忽略这些微小的奇异值，稳健地处理各种情况，包括满秩、[秩亏](@entry_id:754065)、甚至行数少于列数的[欠定系统](@entry_id:148701) 。

### 扩展与变体

#### [基函数](@entry_id:170178)的选择与问题[条件数](@entry_id:145150)

在[多项式拟合](@entry_id:178856)等问题中，设计[矩阵的条件数](@entry_id:150947)极大地依赖于所选择的**[基函数](@entry_id:170178) (basis functions)**。使用标准的**单项式基** $\{1, x, x^2, \dots, x^k\}$ 会产生一个[范德蒙矩阵](@entry_id:147747)。对于区间 $[-1, 1]$ 上的点，随着次数 $k$ 的增加，高次幂 $x^k$ 的函数图像会越来越相似，尤其是在端点附近，导致[设计矩阵](@entry_id:165826)的列向量高度线性相关。这使得[范德蒙矩阵](@entry_id:147747)成为臭名昭著的[病态矩阵](@entry_id:147408)，其条件数随 $k$ 指数增长。

相比之下，使用一组**正交多项式**，如**[切比雪夫多项式](@entry_id:145074) (Chebyshev polynomials)** 作为[基函数](@entry_id:170178)，可以极大地改善问题的条件数。虽然[切比雪夫多项式](@entry_id:145074)在离散点集上不完全正交，但它们的“近正交性”足以使[设计矩阵](@entry_id:165826)的列向量保持良好的线性无关性。这使得相应的[格拉姆矩阵](@entry_id:203297) $A^T A$ 近似为[对角矩阵](@entry_id:637782)，其[条件数](@entry_id:145150)接近于1，从而保证了数值求解的稳定性 。这个例子说明，明智地选择[基函数](@entry_id:170178)是成功应用最小二乘法的一项关键技术。

#### [加权最小二乘法](@entry_id:177517) (Weighted Least Squares, WLS)

标准的最小二乘法（也称[普通最小二乘法](@entry_id:137121), OLS）假定所有观测值的[误差方差](@entry_id:636041)相同。但在许多实际应用中，不同数据点的可靠性是不同的。例如，某些数据点可能是多次测量的平均值，其[标准误差](@entry_id:635378)比单次测量要小 。

**[加权最小二乘法 (WLS)](@entry_id:170850)** 通过为每个残差项分配一个权重 $w_i$ 来处理这种情况，目标函数变为：

$S_W(\boldsymbol{\beta}) = \sum_{i=1}^{m} w_i (y_i - f(\mathbf{x}_i; \boldsymbol{\beta}))^2 \to \min$

权重 $w_i$ 通常取为观测值[方差](@entry_id:200758)的倒数，即 $w_i = 1/\sigma_i^2$。这样，[方差](@entry_id:200758)小（更可靠）的数据点在总误差中的贡献更大，拟合结果会更倾向于穿过这些点。

WLS问题同样可以转化为求解一个修正后的[正规方程](@entry_id:142238)。令 $W$ 是一个对角矩阵，其对角元素为 $W_{ii} = w_i$。则WLS的解由以下方程给出：

$(A^T W A) \boldsymbol{\beta} = A^T W \mathbf{y}$

#### [总体最小二乘法](@entry_id:170210) (Total Least Squares, TLS)

[普通最小二乘法](@entry_id:137121)和[加权最小二乘法](@entry_id:177517)都假定自变量 $\mathbf{x}_i$ 是精确无误的，所有误差都存在于因变量 $y_i$ 中。然而，在某些情况下，所有变量都可能存在测量误差。

**[总体最小二乘法](@entry_id:170210) (TLS)** 是一种更通用的方法，它旨在最小化数据点到拟合直线（或[超平面](@entry_id:268044)）的**正交距离**的平方和。对于拟合一条直线 $ax+by+c=0$ ，其目标是最小化：

$S_{TLS}(a,b,c) = \sum_{i=1}^m \frac{(ax_i+by_i+c)^2}{a^2+b^2}$

在归一化约束 $a^2+b^2=1$ 下，问题可以被转化为一个**[特征值问题](@entry_id:142153)**。可以证明，最佳的法向量 $(a,b)$ 是数据点中心化后的散布矩阵 $M = \begin{pmatrix} S_{xx} & S_{xy} \\ S_{xy} & S_{yy} \end{pmatrix}$ 的最小特征值对应的[特征向量](@entry_id:151813)。TLS因此也被称为“变量含误差”(errors-in-variables)模型。

### 解的敏感性与近似现象

#### 解对数据扰动的敏感性

[最小二乘解](@entry_id:152054)的质量不仅取决于算法的[数值稳定性](@entry_id:146550)，还内在地受问题本身的条件数影响。假设观测数据 $\mathbf{y}$ 存在一个微小的扰动 $\Delta\mathbf{y}$，这会导致解产生一个变化 $\Delta\boldsymbol{\beta}$。那么，这个扰动最大会被放大多少倍呢？

可以证明，最坏情况下的放大因子由矩阵 $A$ 的最小奇异值 $\sigma_p$ 决定 ：

$\sup_{\Delta\mathbf{y} \ne 0} \frac{\|\Delta\boldsymbol{\beta}\|_2}{\|\Delta\mathbf{y}\|_2} = \|A^\dagger\|_2 = \frac{1}{\sigma_p}$

这个结果极具启发性。它表明，当 $\sigma_p$ 非常接近于零时（即矩阵 $A$ 近似[秩亏](@entry_id:754065)），解对数据的扰动会变得极其敏感。此时，即使数据有微小的噪声，解也可能发生剧烈变化。这为[条件数](@entry_id:145150) $\kappa_2(A) = \sigma_1/\sigma_p$ 提供了一个直观的物理解释：它直接关系到[最小二乘解](@entry_id:152054)在最坏情况下的相对误差界。

#### 模型失配：吉布斯现象

最后，必须认识到最小二乘法作为一种近似工具的局限性。当选择的模型类型与数据的内在结构不匹配时，即使是“最佳”的[最小二乘拟合](@entry_id:751226)也可能产生不理想的结果。

一个著名的例子是用光滑的全局多项式去逼近一个不连续的函数，如阶跃函数 。无论多项式的次数多高，在不连续点附近，拟合曲线总会出现**[吉布斯现象](@entry_id:138701) (Gibbs phenomenon)**——一种持续的、幅度不随次数增加而减小的[过冲](@entry_id:147201)和下冲。这说明，对于具有局部剧烈变化的函数，使用全局多项式作为[基函数](@entry_id:170178)可能不是一个好主意，可能需要考虑[分段多项式](@entry_id:634113)（样条）或其它具有更好局部适应性的[基函数](@entry_id:170178)。这提醒我们，在应用最小二乘法时，模型的选择与验证同样至关重要。