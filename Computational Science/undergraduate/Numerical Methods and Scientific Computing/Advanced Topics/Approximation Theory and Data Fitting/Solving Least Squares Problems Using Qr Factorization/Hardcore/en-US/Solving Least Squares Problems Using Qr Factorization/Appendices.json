{
    "hands_on_practices": [
        {
            "introduction": "Solving a standard least squares problem is a foundational task, but real-world data is often messy. This practice explores how QR factorization provides a numerically stable solution and investigates the crucial impact of data scaling on both the condition number of the problem and the interpretability of the results . By implementing a Householder QR-based solver, you will gain hands-on experience with the mechanics of the method and see why preprocessing data is a critical step in scientific computing.",
            "id": "3275437",
            "problem": "You are asked to design and implement a program that solves overdetermined linear least squares problems in the presence of mixed units across feature columns by using a numerically stable orthogonal-factorization approach. The program must explicitly construct and use a Householder-based decomposition to factor a real matrix with more rows than columns and then solve the associated upper-triangular system, rather than relying on black-box solvers.\n\nStart from the following foundational base: the least squares problem minimizes the squared Euclidean norm $\\,\\lVert A x - b \\rVert_2\\,$ for a real matrix $\\,A \\in \\mathbb{R}^{m \\times n}\\,$ with $\\,m \\ge n\\,$ and a real vector $\\,b \\in \\mathbb{R}^m\\,$; orthogonal transformations preserve the Euclidean norm; and any real matrix $\\,A\\,$ admits a factorization by successive orthogonal transformations that zero out subdiagonal entries to produce an upper-triangular factor. Your implementation must use these facts to design the algorithm.\n\nYou must address mixed units explicitly. Suppose a column of $\\,A\\,$ is measured in kilometers while another is in meters. To achieve consistent physical interpretation of the coefficients $\\,x\\,$, you should construct a diagonal scaling matrix $\\,D_{\\text{phys}}\\,$ that converts the kilometer-measured columns to meters (for instance, multiply a kilometer column by $\\,1000\\,$). In addition, to examine numerical stability independent of physical units, you should construct a column normalization scaling matrix $\\,S_{\\text{norm}}\\,$ that scales each column to unit $\\,\\ell_2\\,$ norm. Given a scaled matrix $\\,A S\\,$ and a solution $\\,y\\,$ that minimizes $\\,\\lVert A S \\, y - b \\rVert_2\\,$, the coefficient vector in the original mixed-unit coordinates is $\\,x = S y\\,$. Your program must evaluate the solution found under three scenarios: unscaled mixed units, physically consistent units, and column-normalized units, and compare the residual norms and conditioning across scenarios while mapping coefficients back to the same original coordinate system for interpretability.\n\nImplement the following:\n- Construct a Householder-based orthogonal factorization for a given $\\,A\\,$, apply the associated orthogonal transformations to $\\,b\\,$ to obtain $\\,Q^\\top b\\,$, and solve the resulting upper-triangular system $\\,R x = Q^\\top b\\,$ for the least squares solution $\\,x\\,$ where $\\,A = Q R\\,$ with $\\,Q\\,$ orthogonal and $\\,R\\,$ upper-triangular.\n- For physical scaling, define $\\,D_{\\text{phys}}\\,$ by multiplying each kilometer column by $\\,1000\\,$ to convert it to meters, and leave meter and bias columns unchanged. For normalization scaling, define $\\,S_{\\text{norm}} = \\operatorname{diag}(1/\\lVert a_1 \\rVert_2, \\dots, 1/\\lVert a_n \\rVert_2)\\,$, where $\\,a_j\\,$ is the $\\,j$-th column of $\\,A\\,$. The mapped coefficients in the original mixed-unit coordinates are $\\,x_{\\text{phys}} = D_{\\text{phys}} y_{\\text{phys}}\\,$ and $\\,x_{\\text{norm}} = S_{\\text{norm}} y_{\\text{norm}}\\,$.\n- Compute the residual norms $\\,\\lVert A x_{\\text{mix}} - b \\rVert_2\\,$, $\\,\\lVert A x_{\\text{phys}} - b \\rVert_2\\,$, and $\\,\\lVert A x_{\\text{norm}} - b \\rVert_2\\,$ for the three scenarios. Also compute the $\\,2$-norm condition numbers $\\,\\kappa_2(A)\\,$, $\\,\\kappa_2(A D_{\\text{phys}})\\,$, and $\\,\\kappa_2(A S_{\\text{norm}})\\,$. Finally, compute the relative differences $\\,\\delta_{\\text{phys}} = \\lVert x_{\\text{mix}} - x_{\\text{phys}} \\rVert_2 / \\lVert x_{\\text{mix}} \\rVert_2\\,$ and $\\,\\delta_{\\text{norm}} = \\lVert x_{\\text{mix}} - x_{\\text{norm}} \\rVert_2 / \\lVert x_{\\text{mix}} \\rVert_2\\,$.\n\nAll outputs must be dimensionless decimal numbers. Round all reported floats to $\\,6\\,$ decimal places.\n\nTest suite. Use the following three test cases. In each, the matrix $\\,A\\,$ has three columns: the first measured in meters, the second measured in kilometers, and the third is a bias column of ones. The vector $\\,b\\,$ is explicitly given.\n\nTest case $\\,1\\,$ (moderate conditioning, mixed units):\n$$\nA_1 = \\begin{bmatrix}\n120 & 0.2 & 1 \\\\\n340 & 0.5 & 1 \\\\\n560 & 0.9 & 1 \\\\\n780 & 1.3 & 1 \\\\\n910 & 1.6 & 1 \\\\\n1050 & 2.0 & 1\n\\end{bmatrix},\\quad\nb_1 = \\begin{bmatrix}\n356 \\\\\n778 \\\\\n1325 \\\\\n1859 \\\\\n2253.5 \\\\\n2751.5\n\\end{bmatrix}.\n$$\n\nTest case $\\,2\\,$ (extreme scale disparity across columns):\n$$\nA_2 = \\begin{bmatrix}\n0.4 & 150 & 1 \\\\\n0.6 & 300 & 1 \\\\\n0.9 & 450 & 1 \\\\\n1.2 & 600 & 1 \\\\\n1.5 & 750 & 1\n\\end{bmatrix},\\quad\nb_2 = \\begin{bmatrix}\n75999.6 \\\\\n151002.9 \\\\\n225999.35 \\\\\n301002.8 \\\\\n376002.25\n\\end{bmatrix}.\n$$\n\nTest case $\\,3\\,$ (near-collinearity of meter and kilometer columns, but full rank):\n$$\nA_3 = \\begin{bmatrix}\n960 & 0.95 & 1 \\\\\n1035 & 1.05 & 1 \\\\\n1220 & 1.20 & 1 \\\\\n1295 & 1.30 & 1 \\\\\n1500 & 1.50 & 1 \\\\\n1790 & 1.80 & 1 \\\\\n2105 & 2.10 & 1\n\\end{bmatrix},\\quad\nb_3 = \\begin{bmatrix}\n396 \\\\\n401 \\\\\n480 \\\\\n486 \\\\\n549 \\\\\n634 \\\\\n731\n\\end{bmatrix}.\n$$\n\nFor each test case $\\,i \\in \\{1,2,3\\}\\,$, let $\\,A_i\\,$ and $\\,b_i\\,$ be as above. Your program must, for each $\\,i\\,$:\n- Solve the least squares problem in mixed units to obtain $\\,x_{\\text{mix}}\\,$ via Householder-based orthogonal factorization.\n- Solve the least squares problem after physical scaling to obtain $\\,y_{\\text{phys}}\\,$ on $\\,A_i D_{\\text{phys}}\\,$ and then map $\\,x_{\\text{phys}} = D_{\\text{phys}} y_{\\text{phys}}\\,$.\n- Solve the least squares problem after column normalization to obtain $\\,y_{\\text{norm}}\\,$ on $\\,A_i S_{\\text{norm}}\\,$ and then map $\\,x_{\\text{norm}} = S_{\\text{norm}} y_{\\text{norm}}\\,$.\n- Compute and report the eight floats for that case in the order\n$$\n\\big[\n\\lVert A_i x_{\\text{mix}} - b_i \\rVert_2,\\;\n\\lVert A_i x_{\\text{phys}} - b_i \\rVert_2,\\;\n\\lVert A_i x_{\\text{norm}} - b_i \\rVert_2,\\;\n\\kappa_2(A_i),\\;\n\\kappa_2(A_i D_{\\text{phys}}),\\;\n\\kappa_2(A_i S_{\\text{norm}}),\\;\n\\delta_{\\text{phys}},\\;\n\\delta_{\\text{norm}}\n\\big].\n$$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces. The top-level list should contain three sublists corresponding to the three test cases, each sublist containing the eight rounded floats in the order specified. For example, the output must look like\n$$\n\\big[\\,[f_{11},f_{12},\\dots,f_{18}],\\,[f_{21},f_{22},\\dots,f_{28}],\\,[f_{31},f_{32},\\dots,f_{38}]\\,\\big],\n$$\nwhere each $\\,f_{jk}\\,$ is a decimal number formatted to $\\,6\\,$ decimal places. No other text may be printed.",
            "solution": "The problem requires the design and implementation of a numerically stable solver for overdetermined linear least squares problems, $\\min_{x} \\lVert A x - b \\rVert_2$, where $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, and $b \\in \\mathbb{R}^m$. The specified method is QR factorization using Householder transformations. A key aspect of the problem is to investigate the effects of preconditioning the matrix $A$ through scaling to address issues arising from mixed physical units and disparate column norms.\n\nThe foundational principle is that the Euclidean norm is invariant under orthogonal transformations. If $Q \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix (i.e., $Q^\\top Q = I$), then for any vector $z \\in \\mathbb{R}^m$, $\\lVert Q z \\rVert_2 = \\lVert z \\rVert_2$. We can leverage this property by factorizing the matrix $A$ into the product of an orthogonal matrix $Q$ and an upper-triangular matrix $R$, such that $A=QR$. The least squares problem can then be rewritten as:\n$$\n\\lVert A x - b \\rVert_2^2 = \\lVert Q R x - b \\rVert_2^2 = \\lVert Q^\\top (Q R x - b) \\rVert_2^2 = \\lVert R x - Q^\\top b \\rVert_2^2\n$$\nThe matrix $R \\in \\mathbb{R}^{m \\times n}$ has an upper-triangular structure. Since $m \\ge n$, it can be partitioned as:\n$$\nR = \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix}, \\quad \\text{where } R_1 \\in \\mathbb{R}^{n \\times n} \\text{ is upper-triangular.}\n$$\nSimilarly, the transformed vector $Q^\\top b$ can be partitioned as:\n$$\nQ^\\top b = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}, \\quad \\text{where } c_1 \\in \\mathbb{R}^n \\text{ and } c_2 \\in \\mathbb{R}^{m-n}.\n$$\nThe minimization problem is thus transformed into:\n$$\n\\min_{x} \\left\\lVert \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix} x - \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix} \\right\\rVert_2^2 = \\min_{x} \\left( \\lVert R_1 x - c_1 \\rVert_2^2 + \\lVert c_2 \\rVert_2^2 \\right)\n$$\nThe term $\\lVert c_2 \\rVert_2^2$ is independent of $x$. The minimum of the expression is achieved when $\\lVert R_1 x - c_1 \\rVert_2^2 = 0$. Assuming the columns of $A$ are linearly independent, $A$ has full rank, and $R_1$ is invertible. The unique least squares solution $x$ is therefore found by solving the square upper-triangular system:\n$$\nR_1 x = c_1\n$$\nThis system can be efficiently solved using back substitution. The squared norm of the final residual is $\\lVert A x - b \\rVert_2^2 = \\lVert c_2 \\rVert_2^2$.\n\nThe QR factorization is constructed using a sequence of Householder transformations. A Householder transformation is a reflection across a hyperplane and is represented by a matrix $H = I - 2 \\frac{v v^\\top}{v^\\top v}$ for a non-zero vector $v \\in \\mathbb{R}^k$. For any vector $z \\in \\mathbb{R}^k$, we can choose a vector $v$ such that $H z$ is a multiple of the standard basis vector $e_1 = [1, 0, \\dots, 0]^\\top$. Specifically, the vector $v$ is chosen as $v = z + \\alpha e_1$, where $\\alpha = \\text{sgn}(z_1) \\lVert z \\rVert_2$. The sign is chosen to avoid catastrophic cancellation when $z$ is nearly parallel to $e_1$.\n\nThe algorithm for QR factorization of $A$ proceeds column by column. For each column $j$ from $1$ to $n$:\n$1$. Consider the vector $z$ comprising the elements of column $j$ from the diagonal downwards, i.e., $A_{j:m, j}$.\n$2$. Construct the corresponding Householder vector $v_j$ and the transformation matrix $H_j$.\n$3$. Apply this transformation to the submatrix of $A$ from row $j$ to $m$ and column $j$ to $n$. This introduces zeros below the diagonal in column $j$. The same transformation must be applied to the corresponding elements of vector $b$.\nAfter $n$ steps, the matrix $A$ is transformed into the upper-triangular matrix $R$, and the vector $b$ is transformed into $Q^\\top b$. Formally, $R = H_n \\cdots H_2 H_1 A$ and $Q^\\top b = H_n \\cdots H_2 H_1 b$, where $Q = H_1 H_2 \\cdots H_n$. The algorithm does not require forming $Q$ explicitly.\n\nThe problem investigates three scenarios based on matrix scaling:\n$1$. **Mixed Units**: The problem is solved using the given matrix $A$. The solution is $x_{\\text{mix}}$.\n$2$. **Physical Scaling**: The matrix $A$ is pre-multiplied by a diagonal scaling matrix $D_{\\text{phys}}$. For the given problem, the second column is in kilometers while others are in meters or are dimensionless. To convert kilometers to meters, we set $D_{\\text{phys}} = \\text{diag}(1, 1000, 1)$. The least squares problem is solved for the scaled matrix $\\hat{A}_{\\text{phys}} = A D_{\\text{phys}}$ to find a solution $y_{\\text{phys}}$. The solution in the original, unscaled coordinates is recovered by $x_{\\text{phys}} = D_{\\text{phys}} y_{\\text{phys}}$.\n$3$. **Column Normalization**: The matrix $A$ is pre-multiplied by a diagonal matrix $S_{\\text{norm}} = \\text{diag}(1/\\lVert a_1 \\rVert_2, \\dots, 1/\\lVert a_n \\rVert_2)$, where $a_j$ is the $j$-th column of $A$. This scaling makes each column of the new matrix $\\hat{A}_{\\text{norm}} = A S_{\\text{norm}}$ have a unit $\\ell_2$-norm. The least squares problem is solved for $\\hat{A}_{\\text{norm}}$ to find $y_{\\text{norm}}$, and the original solution is recovered via $x_{\\text{norm}} = S_{\\text{norm}} y_{\\text{norm}}$.\n\nThe analysis requires computing several metrics for each scenario:\n- **Residual Norms**: $\\lVert A x - b \\rVert_2$ for $x \\in \\{x_{\\text{mix}}, x_{\\text{phys}}, x_{\\text{norm}}\\}$. In exact arithmetic, these would be identical, but in floating-point arithmetic, they may differ due to the varied numerical stability of the underlying problems.\n- **Condition Numbers**: The $2$-norm condition number, $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$, measures the sensitivity of the solution to perturbations in the data. We compute $\\kappa_2(A)$, $\\kappa_2(A D_{\\text{phys}})$, and $\\kappa_2(A S_{\\text{norm}})$. A lower condition number generally implies a more numerically stable problem.\n- **Relative Differences**: $\\delta_{\\text{phys}} = \\lVert x_{\\text{mix}} - x_{\\text{phys}} \\rVert_2 / \\lVert x_{\\text{mix}} \\rVert_2$ and $\\delta_{\\text{norm}} = \\lVert x_{\\text{mix}} - x_{\\text{norm}} \\rVert_2 / \\lVert x_{\\text{mix}} \\rVert_2$. These quantify the numerical deviation of the solutions obtained from scaled matrices compared to the unscaled one.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the least squares problems for the given test cases.\n    \"\"\"\n\n    def householder_qr_solve(A, b):\n        \"\"\"\n        Solves the least squares problem min ||Ax - b||_2 using Householder QR factorization.\n\n        Args:\n            A (np.ndarray): The matrix A (m x n).\n            b (np.ndarray): The vector b (m,).\n\n        Returns:\n            np.ndarray: The solution vector x (n,).\n        \"\"\"\n        m, n = A.shape\n        R = A.copy()\n        c = b.copy().flatten()\n\n        for j in range(n):\n            # Extract the j-th column from the diagonal down\n            z = R[j:, j].copy()\n            \n            # Compute the Householder vector v\n            # Use `np.copysign` to handle the sign of 0 correctly\n            norm_z = np.linalg.norm(z)\n            alpha = -np.copysign(norm_z, z[0]) if norm_z != 0 else 0.0\n            \n            v = z.copy()\n            v[0] -= alpha\n            \n            norm_v = np.linalg.norm(v)\n            if norm_v > 1e-12: # Check to avoid division by zero\n                v = v / norm_v  # Normalize v to a unit vector u\n\n                # Apply reflection to the remaining submatrix of R\n                sub_matrix_R = R[j:, j:]\n                R[j:, j:] -= 2 * np.outer(v, v.T @ sub_matrix_R)\n\n                # Apply reflection to the corresponding part of b\n                sub_c = c[j:]\n                c[j:] -= 2 * v * (v.T @ sub_c)\n\n        # Extract the upper triangular matrix R1 and the vector c1\n        R1 = R[:n, :n]\n        c1 = c[:n]\n\n        # Solve the upper triangular system R1 @ x = c1 using back substitution\n        x = np.zeros(n)\n        for i in range(n - 1, -1, -1):\n            s = c1[i] - np.dot(R1[i, i + 1:], x[i + 1:])\n            x[i] = s / R1[i, i]\n            \n        return x\n\n    test_cases = [\n        (\n            np.array([\n                [120., 0.2, 1.], [340., 0.5, 1.], [560., 0.9, 1.],\n                [780., 1.3, 1.], [910., 1.6, 1.], [1050., 2.0, 1.]\n            ]),\n            np.array([356., 778., 1325., 1859., 2253.5, 2751.5])\n        ),\n        (\n            np.array([\n                [0.4, 150., 1.], [0.6, 300., 1.], [0.9, 450., 1.],\n                [1.2, 600., 1.], [1.5, 750., 1.]\n            ]),\n            np.array([75999.6, 151002.9, 225999.35, 301002.8, 376002.25])\n        ),\n        (\n            np.array([\n                [960., 0.95, 1.], [1035., 1.05, 1.], [1220., 1.20, 1.],\n                [1295., 1.30, 1.], [1500., 1.50, 1.], [1790., 1.80, 1.],\n                [2105., 2.10, 1.]\n            ]),\n            np.array([396., 401., 480., 486., 549., 634., 731.])\n        )\n    ]\n\n    all_results = []\n    \n    for A, b in test_cases:\n        # Scenario 1: Mixed Units (unscaled)\n        x_mix = householder_qr_solve(A, b)\n        res_norm_mix = np.linalg.norm(A @ x_mix - b.flatten())\n        cond_mix = np.linalg.cond(A, 2)\n\n        # Scenario 2: Physical Scaling\n        # Column 2 is in km, convert to m by multiplying by 1000.\n        D_phys = np.diag([1.0, 1000.0, 1.0])\n        A_phys = A @ D_phys\n        y_phys = householder_qr_solve(A_phys, b)\n        x_phys = D_phys @ y_phys\n        res_norm_phys = np.linalg.norm(A @ x_phys - b.flatten())\n        cond_phys = np.linalg.cond(A_phys, 2)\n        \n        # Scenario 3: Column Normalization Scaling\n        col_norms = np.linalg.norm(A, axis=0)\n        S_norm = np.diag(1.0 / col_norms)\n        A_norm = A @ S_norm\n        y_norm = householder_qr_solve(A_norm, b)\n        x_norm = S_norm @ y_norm\n        res_norm_norm = np.linalg.norm(A @ x_norm - b.flatten())\n        cond_norm = np.linalg.cond(A_norm, 2)\n\n        # Relative differences in solutions\n        delta_phys = np.linalg.norm(x_mix - x_phys) / np.linalg.norm(x_mix)\n        delta_norm = np.linalg.norm(x_mix - x_norm) / np.linalg.norm(x_mix)\n\n        case_results = [\n            res_norm_mix, res_norm_phys, res_norm_norm,\n            cond_mix, cond_phys, cond_norm,\n            delta_phys, delta_norm\n        ]\n        \n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified.\n    formatted_cases = []\n    for case_res in all_results:\n        formatted_floats = [f\"{val:.6f}\" for val in case_res]\n        formatted_cases.append(f\"[{','.join(formatted_floats)}]\")\n    \n    print(f\"[{','.join(formatted_cases)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In many applications, from sensor networks to online machine learning, data arrives in a continuous stream. Re-computing a full factorization for each new data point would be prohibitively expensive. This exercise introduces an elegant solution using Givens rotations to incrementally update the QR factorization as new rows of data are added, allowing for an efficient \"on-the-fly\" solution to the least squares problem .",
            "id": "3275374",
            "problem": "You must implement an incremental least-squares solver for streaming rows using Givens rotations. The goal is to maintain an up-to-date factorization and solve the growing least-squares problem after all rows in each test case have been processed. You must produce a complete, runnable program that uses only the specified Python environment.\n\nFoundational base to use:\n- The least-squares problem is defined as minimizing the Euclidean norm $\\,\\lVert A x - b \\rVert_2\\,$ over $\\,x \\in \\mathbb{R}^n\\,$ for a given matrix $\\,A \\in \\mathbb{R}^{m \\times n}\\,$ and vector $\\,b \\in \\mathbb{R}^m\\,$.\n- Orthogonal transformations preserve the Euclidean norm, i.e., if $\\,Q \\in \\mathbb{R}^{m \\times m}\\,$ is orthogonal (so $\\,Q^\\top Q = I\\,$), then $\\,\\lVert A x - b \\rVert_2 = \\lVert Q^\\top (A x - b) \\rVert_2\\,$.\n- A Givens rotation is a $\\,2 \\times 2\\,$ orthogonal matrix that zeroes one selected component of a $\\,2$-vector; it preserves norms and orthogonality when embedded into higher dimensions.\n\nTask specification:\n- You will implement an incremental update that, when a new row $\\,a^{\\top} \\in \\mathbb{R}^n\\,$ of $\\,A\\,$ and a new scalar $\\,\\beta \\in \\mathbb{R}\\,$ of $\\,b\\,$ arrive, updates an existing upper-triangular factor $\\,R \\in \\mathbb{R}^{n \\times n}\\,$ and a transformed right-hand side vector $\\,c \\in \\mathbb{R}^n\\,$ so that, after processing all rows for a test case, solving $\\,R x = c\\,$ yields a least-squares solution for the full system. The update must be performed using Givens rotations that act only on the current $\\,R\\,$, the incoming row $\\,a^{\\top}\\,$, and the right-hand side accumulation.\n- Initialization: before any rows are processed for a test case, set $\\,R = 0 \\in \\mathbb{R}^{n \\times n}\\,$ and $\\,c = 0 \\in \\mathbb{R}^n\\,$.\n- Row update invariants to maintain:\n  - After incorporating $\\,k\\,$ rows, there exists an orthogonal matrix $\\,Q_k \\in \\mathbb{R}^{k \\times k}\\,$ such that $$Q_k^\\top A_k = \\begin{bmatrix} R_k \\\\ 0 \\end{bmatrix},$$ where $\\,A_k \\in \\mathbb{R}^{k \\times n}\\,$ is the matrix of the first $\\,k\\,$ rows and $\\,R_k \\in \\mathbb{R}^{n \\times n}\\,$ is upper triangular (with possible zero diagonal entries if rank-deficient).\n  - The vector $\\,c\\,$ equals the first $\\,n\\,$ entries of $\\,Q_k^\\top b_k\\,$, where $\\,b_k \\in \\mathbb{R}^k\\,$ collects the first $\\,k\\,$ entries of $\\,b$.\n- You must implement a triangular solve to compute $\\,x\\,$ from $\\,R x = c\\,$ after all rows of a test case have been processed. To handle potential rank deficiency, use a threshold $\\,\\tau = 10^{-12}\\,$: if $\\,\\lvert R_{ii} \\rvert \\le \\tau\\,$, set $\\,x_i = 0\\,$ and do not divide by $\\,R_{ii}\\,$ for that index in back substitution. This produces a least-squares solution with minimal residual norm.\n- For verification, for each test case also compute a reference least-squares solution using a stable batch method and compare residual norms.\n\nWhat to compute and output for each test case:\n- For each test case, after processing all rows incrementally and computing the solution $\\,x_{\\text{inc}}\\,$ by back substitution, compute the residual norm $\\,\\rho_{\\text{inc}} = \\lVert A x_{\\text{inc}} - b \\rVert_2$.\n- Also compute a reference solution $\\,x_{\\text{ref}}\\,$ by a stable batch method and its residual norm $\\,\\rho_{\\text{ref}} = \\lVert A x_{\\text{ref}} - b \\rVert_2$.\n- The scalar result for the test case is the absolute difference $\\,\\Delta = \\lvert \\rho_{\\text{inc}} - \\rho_{\\text{ref}} \\rvert$.\n\nStreaming update to implement with Givens rotations:\n- When a new row $\\,a^{\\top}\\,$ with right-hand side $\\,\\beta\\,$ arrives, form a working copy $\\,w \\leftarrow a\\,$ and a scalar accumulator $\\,\\gamma \\leftarrow \\beta$.\n- For each column index $\\,j = 0, 1, \\dots, n-1\\,$, compute a Givens rotation coefficients $\\,c_j, s_j\\,$ that act on the pair $\\,\\big(R_{jj}, w_j\\big)\\,$ to zero the second component. Apply the same rotation to the subrow $\\,R_{j, j:n}\\,$ together with $\\,w_{j:n}\\,$, and apply it to the pair $\\,\\big(c_j^{(\\text{rhs})}, \\gamma\\big)\\,$, where $\\,c_j^{(\\text{rhs})}\\,$ denotes the current $\\,j\\,$th entry of the vector $\\,c$. After finishing $\\,j = 0 \\dots n-1\\,$, discard $\\,w\\,$ and $\\,\\gamma\\,$. This preserves orthogonality and maintains $\\,R\\,$ as upper triangular.\n\nNumerical details and constraints:\n- Use the threshold $\\,\\tau = 10^{-12}\\,$ for singularity decisions in back substitution.\n- Use numerically stable computation of the Givens rotation from the pair $\\,(\\alpha, \\beta)\\,$ via $\\,r = \\sqrt{\\alpha^2 + \\beta^2}\\,$, $\\,c = \\alpha / r\\,$, $\\,s = \\beta / r\\,$ when $\\,r \\ne 0\\,$; otherwise take $\\,c = 1\\,$ and $\\,s = 0$.\n- All floating-point computations must be in double precision.\n\nTest suite:\nFor each test case, $\\,n\\,$ is the number of columns, and the data are provided as an ordered stream of rows that must be processed in the given order.\n\n- Test case 1 (tall, well-conditioned):\n  - $\\,n = 3$.\n  - Rows of $\\,A\\,$ and corresponding $\\,b\\,$ entries:\n    - $[2.0, -1.0, 0.0]$, $\\,1.0$\n    - $[-1.0, 2.0, -1.0]$, $\\,0.0$\n    - $[0.0, -1.0, 2.0]$, $\\,1.0$\n    - $[1.0, 0.0, -1.0]$, $\\,-1.0$\n    - $[3.0, 1.0, 0.0]$, $\\,4.0$\n- Test case 2 (exactly determined, nonsingular):\n  - $\\,n = 3$.\n  - Rows of $\\,A\\,$ and $\\,b$:\n    - $[1.0, 2.0, 3.0]$, $\\,7.0$\n    - $[0.0, -1.0, 4.0]$, $\\,3.0$\n    - $[2.0, 0.0, 1.0]$, $\\,5.0$\n- Test case 3 (rank-deficient columns: third column equals the sum of the first two):\n  - $\\,n = 3$.\n  - Rows of $\\,A\\,$ and $\\,b$:\n    - $[1.0, 2.0, 3.0]$, $\\,1.0$\n    - $[2.0, -1.0, 1.0]$, $\\,0.0$\n    - $[-1.0, 0.5, -0.5]$, $\\,1.0$\n    - $[3.0, 1.0, 4.0]$, $\\,2.0$\n- Test case 4 (underdetermined: fewer rows than columns):\n  - $\\,n = 3$.\n  - Rows of $\\,A\\,$ and $\\,b$:\n    - $[1.0, 0.0, 0.0]$, $\\,3.0$\n    - $[0.0, 1.0, 0.0]$, $\\,4.0$\n\nReference method:\n- For $\\,x_{\\text{ref}}\\,$, you may use a batch least-squares solver that returns a least-squares solution with minimal residual norm (for example, a method based on singular value decomposition).\n\nFinal output format:\n- Your program must print a single line containing a Python-style list of the four scalar results $\\,\\Delta\\,$, one per test case in the order above.\n- Each number must be formatted in scientific notation with exactly $\\,12\\,$ digits after the decimal point.\n- Example format (not the actual values): $\\,\\,[1.234000000000e-06,5.678900000000e-12,0.000000000000e+00,9.990000000000e-04]$.\n\nNo input will be provided to your program; all data must be hard-coded exactly as specified above. Your implementation must use only the allowed libraries and environment and must not print any extra text. The output must strictly match the specified single-line format.",
            "solution": "The problem requires the implementation of an incremental algorithm for solving the linear least-squares problem, which seeks to find a vector $x \\in \\mathbb{R}^n$ that minimizes the Euclidean norm of the residual, $\\lVert Ax - b \\rVert_2$, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$. The data $(A, b)$ is provided as a stream of rows, and the solution must be updated incrementally for each incoming row.\n\nThe foundation of the method lies in the property that orthogonal transformations preserve the Euclidean norm. If $Q \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix (i.e., $Q^\\top Q = I$), then $\\lVert y \\rVert_2 = \\lVert Q^\\top y \\rVert_2$ for any vector $y \\in \\mathbb{R}^m$. Applying this to the least-squares residual, we have:\n$$\n\\lVert Ax - b \\rVert_2 = \\lVert Q^\\top (Ax - b) \\rVert_2 = \\lVert (Q^\\top A)x - (Q^\\top b) \\rVert_2\n$$\nThe strategy is to choose $Q$ such that it simplifies the structure of $A$. Specifically, we use the QR factorization, where $A$ is decomposed into the product of an orthogonal matrix $Q$ and an upper triangular matrix. We compute an orthogonal $Q$ such that:\n$$\nQ^\\top A = \\begin{bmatrix} R \\\\ 0 \\end{bmatrix}\n$$\nwhere $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular and $0$ is a $(m-n) \\times n$ zero matrix (assuming $m \\ge n$). The same transformation applied to $b$ yields:\n$$\nQ^\\top b = \\begin{bmatrix} c \\\\ d \\end{bmatrix}\n$$\nwhere $c \\in \\mathbb{R}^n$ and $d \\in \\mathbb{R}^{m-n}$. The minimization problem then becomes:\n$$\n\\min_{x} \\left\\lVert \\begin{bmatrix} R \\\\ 0 \\end{bmatrix} x - \\begin{bmatrix} c \\\\ d \\end{bmatrix} \\right\\rVert_2^2 = \\min_{x} \\left\\lVert \\begin{bmatrix} Rx - c \\\\ -d \\end{bmatrix} \\right\\rVert_2^2 = \\min_{x} \\left( \\lVert Rx - c \\rVert_2^2 + \\lVert d \\rVert_2^2 \\right)\n$$\nThe term $\\lVert d \\rVert_2^2$ is independent of $x$. Therefore, to minimize the total sum, we must minimize $\\lVert Rx - c \\rVert_2^2$. If $R$ is nonsingular, the minimum is zero, achieved by solving the upper triangular system $Rx = c$. The minimum residual norm is $\\lVert d \\rVert_2$.\n\nFor an incremental solver, we do not form the full matrix $A$ at once. Instead, we maintain the factor $R \\in \\mathbb{R}^{n \\times n}$ and the transformed vector $c \\in \\mathbb{R}^n$. Initially, for an empty system ($m=0$), we have $R=0$ and $c=0$. When a new row, consisting of a vector $a^\\top \\in \\mathbb{R}^{1 \\times n}$ and a scalar $\\beta \\in \\mathbb{R}$, arrives, we need to update $R$ and $c$. This is equivalent to finding an orthogonal transformation that updates the factorization for the augmented system:\n$$\n\\begin{bmatrix} R_{\\text{old}} \\\\ a^\\top \\end{bmatrix} \\rightarrow \\begin{bmatrix} R_{\\text{new}} \\\\ 0 \\end{bmatrix} \\quad \\text{and} \\quad \\begin{bmatrix} c_{\\text{old}} \\\\ \\beta \\end{bmatrix} \\rightarrow \\begin{bmatrix} c_{\\text{new}} \\\\ \\gamma' \\end{bmatrix}\n$$\nThis transformation is accomplished using a sequence of $n$ Givens rotations. A Givens rotation is an orthogonal matrix that acts on a two-dimensional subspace to zero out a specific element of a vector. For each new row, we apply a series of rotations to annihilate each element of the new row vector, incorporating its information into the existing $R$ matrix and $c$ vector.\n\nThe algorithmic procedure for processing one new row $(a^\\top, \\beta)$ is as follows:\n1.  Initialize a working copy of the row vector $w \\leftarrow a$ and the right-hand side scalar $\\gamma \\leftarrow \\beta$.\n2.  For each column index $j=0, 1, \\dots, n-1$:\n    a. We aim to zero out the element $w_j$ by rotating it with the diagonal element $R_{jj}$. Let $\\alpha = R_{jj}$ and $\\beta_g = w_j$.\n    b. Compute the Givens rotation coefficients $(c_g, s_g)$. A numerically stable way to do this is to first compute $r = \\sqrt{\\alpha^2 + \\beta_g^2}$. If $r=0$, the rotation is the identity ($c_g=1, s_g=0$). Otherwise, $c_g = \\alpha/r$ and $s_g = \\beta_g/r$.\n    c. This rotation transforms the pair $(\\alpha, \\beta_g)$ to $(r, 0)$:\n    $$\n    \\begin{bmatrix} c_g & s_g \\\\ -s_g & c_g \\end{bmatrix} \\begin{pmatrix} R_{jj} \\\\ w_j \\end{pmatrix} = \\begin{pmatrix} \\sqrt{R_{jj}^2 + w_j^2} \\\\ 0 \\end{pmatrix}\n    $$\n    d. Apply this same rotation to the remainder of the rows, i.e., the sub-vectors $R_{j, j:n}$ and $w_{j:n}$, and to the right-hand side components $c_j$ and $\\gamma$:\n    $$\n    \\begin{pmatrix} R_{j, j:n}^{(\\text{new})} \\\\ w_{j:n}^{(\\text{new})} \\end{pmatrix} = \\begin{bmatrix} c_g & s_g \\\\ -s_g & c_g \\end{bmatrix} \\begin{pmatrix} R_{j, j:n}^{(\\text{old})} \\\\ w_{j:n}^{(\\text{old})} \\end{pmatrix}\n    $$\n    $$\n    \\begin{pmatrix} c_j^{(\\text{new})} \\\\ \\gamma^{(\\text{new})} \\end{pmatrix} = \\begin{bmatrix} c_g & s_g \\\\ -s_g & c_g \\end{bmatrix} \\begin{pmatrix} c_j^{(\\text{old})} \\\\ \\gamma^{(\\text{old})} \\end{pmatrix}\n    $$\n    After this step, $w_j$ becomes zero, and the upper triangular structure of $R$ is preserved. After iterating through all $j$, the entire vector $w$ is zeroed out, and the update is complete.\n\nAfter all $m$ rows of $(A,b)$ have been processed, the resulting system $Rx=c$ is solved for $x$ using back substitution. For potentially rank-deficient systems, the matrix $R$ may have zero or very small diagonal elements. A threshold $\\tau = 10^{-12}$ is used to handle this: if $|R_{ii}| \\le \\tau$, the corresponding column is treated as dependent, and we set $x_i = 0$ to select a particular least-squares solution.\n\nThe correctness of this incremental method is verified by comparing the norm of its residual, $\\rho_{\\text{inc}} = \\lVert A x_{\\text{inc}} - b \\rVert_2$, with the residual norm from a stable batch solver, $\\rho_{\\text{ref}} = \\lVert A x_{\\text{ref}} - b \\rVert_2$. For any least-squares problem, the minimum residual norm is unique, even if the solution vector $x$ is not. Therefore, the absolute difference $\\Delta = |\\rho_{\\text{inc}} - \\rho_{\\text{ref}}|$ must be close to machine precision for a correct implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests an incremental least-squares solver using Givens rotations.\n    \"\"\"\n\n    def solve_incremental_lsq(n, A_rows, b_vals, tau):\n        \"\"\"\n        Solves the least-squares problem incrementally using Givens rotations.\n        \n        Args:\n            n (int): The number of columns in matrix A.\n            A_rows (list of lists): The rows of matrix A.\n            b_vals (list): The entries of vector b.\n            tau (float): The threshold for singularity in back substitution.\n        \n        Returns:\n            np.ndarray: The least-squares solution vector x.\n        \"\"\"\n        R = np.zeros((n, n), dtype=np.float64)\n        c = np.zeros(n, dtype=np.float64)\n\n        for a_row, b_val in zip(A_rows, b_vals):\n            w = np.array(a_row, dtype=np.float64)\n            gamma = float(b_val)\n\n            for j in range(n):\n                alpha = R[j, j]\n                beta = w[j]\n\n                # Compute Givens rotation coefficients robustly\n                r = np.hypot(alpha, beta)\n                if r == 0.0:\n                    cg, sg = 1.0, 0.0\n                else:\n                    cg, sg = alpha / r, beta / r\n\n                # Apply rotation to R and w (from column j onwards)\n                # A temporary copy of the R sub-row is needed for the update.\n                R_j_sub = R[j, j:].copy()\n                w_sub = w[j:].copy()\n                \n                R[j, j:] = cg * R_j_sub + sg * w_sub\n                w[j:] = -sg * R_j_sub + cg * w_sub\n                \n                # Apply the same rotation to the right-hand side c and gamma\n                c_j_val = c[j]\n                c[j] = cg * c_j_val + sg * gamma\n                gamma = -sg * c_j_val + cg * gamma\n                \n        # After processing all rows, solve Rx = c by back substitution\n        x = np.zeros(n, dtype=np.float64)\n        for i in range(n - 1, -1, -1):\n            rhs = c[i] - np.dot(R[i, i+1:], x[i+1:])\n            \n            # Handle potential rank deficiency\n            if abs(R[i, i]) > tau:\n                x[i] = rhs / R[i, i]\n            else:\n                x[i] = 0.0\n                \n        return x\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            3, # n\n            [[2.0, -1.0, 0.0],\n             [-1.0, 2.0, -1.0],\n             [0.0, -1.0, 2.0],\n             [1.0, 0.0, -1.0],\n             [3.0, 1.0, 0.0]], # A_rows\n            [1.0, 0.0, 1.0, -1.0, 4.0]  # b_vals\n        ),\n        (\n            3, # n\n            [[1.0, 2.0, 3.0],\n             [0.0, -1.0, 4.0],\n             [2.0, 0.0, 1.0]], # A_rows\n            [7.0, 3.0, 5.0]  # b_vals\n        ),\n        (\n            3, # n\n            [[1.0, 2.0, 3.0],\n             [2.0, -1.0, 1.0],\n             [-1.0, 0.5, -0.5],\n             [3.0, 1.0, 4.0]], # A_rows\n            [1.0, 0.0, 1.0, 2.0]  # b_vals\n        ),\n        (\n            3, # n\n            [[1.0, 0.0, 0.0],\n             [0.0, 1.0, 0.0]], # A_rows\n            [3.0, 4.0]  # b_vals\n        )\n    ]\n\n    tau = 1e-12\n    results = []\n\n    for n, A_rows, b_vals in test_cases:\n        # Assemble full matrices for residual calculation and reference solver\n        A = np.array(A_rows, dtype=np.float64)\n        b = np.array(b_vals, dtype=np.float64)\n        \n        # Get incremental solution\n        x_inc = solve_incremental_lsq(n, A_rows, b_vals, tau)\n        \n        # Get reference solution using a stable batch method\n        # Use rcond=None to let NumPy determine rank based on machine precision\n        x_ref = np.linalg.lstsq(A, b, rcond=None)[0]\n\n        # Compute residual norms for both solutions\n        # The case m=0 (A is empty) is implicitly handled as loops won't run.\n        if A.shape[0] > 0:\n            rho_inc = np.linalg.norm(A @ x_inc - b)\n            rho_ref = np.linalg.norm(A @ x_ref - b)\n        else:\n            rho_inc = np.linalg.norm(-b)\n            rho_ref = np.linalg.norm(-b)\n\n        # Calculate the absolute difference of residual norms\n        delta = abs(rho_inc - rho_ref)\n        results.append(delta)\n\n    # Format the final output string exactly as specified\n    formatted_results = [f\"{r:.12e}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many optimization problems in science and engineering require finding a solution that not only best fits the data but also perfectly satisfies a set of linear constraints. This practice tackles the equality-constrained least squares (ECLS) problem, a powerful extension of the standard model. You will implement the null-space method, which uses QR factorization as a key building block to elegantly decompose the problem into a particular solution that satisfies the constraints and a homogeneous solution that minimizes the objective, demonstrating a versatile and robust problem-solving technique .",
            "id": "3275428",
            "problem": "You are asked to design and implement a complete, runnable program that solves equality-constrained Least Squares (LS) problems by orthogonal factorization and null-space reduction. The problem is to minimize the Euclidean norm objective $\\,\\|A x - b\\|_2\\,$ subject to the linear constraints $\\,C x = d\\,$. The program must construct a principled numerical solution using the following fundamental base:\n- The definition of the Euclidean norm $\\,\\|y\\|_2 = \\sqrt{y^\\top y}\\,$.\n- The definition of LS: for a given matrix $A$ and vector $b$, find $x$ that minimizes $\\,\\|A x - b\\|_2\\,$.\n- The definition of an orthogonal matrix $\\,Q\\,$ satisfying $\\,Q^\\top Q = I\\,$.\n- The existence of a Quantum-Reflector (QR) factorization of a real matrix $\\,M\\,$ into an orthogonal factor and an upper-triangular factor; apply this factorization to the transpose $\\,C^\\top\\,$ of the constraint matrix to derive an orthonormal basis for both the column space and its orthogonal complement.\n- The concept of the null space of $\\,C\\,$, denoted $\\,\\mathcal{N}(C) = \\{x : C x = 0\\}\\,$.\n\nYour program must:\n- Construct an algorithmic solution based on obtaining an orthonormal decomposition from a QR factorization of $C^\\top$, use it to parameterize the feasible set for $\\,C x = d\\,$, reduce the constrained LS problem to an unconstrained LS problem with a smaller number of variables, and solve that reduced problem using QR factorization again.\n- Avoid ad hoc formulas; the design must be grounded in the above fundamental definitions and properties.\n\nInput is fixed within the program; do not read from files or standard input. The program must run as-is and produce a single line of output.\n\nTest Suite:\nImplement the solver and evaluate it on the following four test cases. Each case defines $A$, $b$, $C$, and $d$ explicitly.\n\nCase 1 (happy path, overdetermined $A$, partially constrained):\nLet $A \\in \\mathbb{R}^{6 \\times 4}$, $b \\in \\mathbb{R}^{6}$, $C \\in \\mathbb{R}^{2 \\times 4}$, $d \\in \\mathbb{R}^{2}$ be\n$$\nA =\n\\begin{bmatrix}\n2 & -1 & 0 & 0 \\\\\n1 & 3 & -2 & 1 \\\\\n0 & 1 & 4 & -1 \\\\\n3 & 0 & 1 & 2 \\\\\n0 & 2 & 0 & 1 \\\\\n1 & 0 & -1 & 1\n\\end{bmatrix},\\quad\nb =\n\\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ -1 \\\\ 4\n\\end{bmatrix},\\quad\nC =\n\\begin{bmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & -1 & 2\n\\end{bmatrix},\\quad\nd =\n\\begin{bmatrix}\n1 \\\\ 0.5\n\\end{bmatrix}.\n$$\n\nCase 2 (no constraints, unconstrained LS):\nLet $A \\in \\mathbb{R}^{5 \\times 3}$, $b \\in \\mathbb{R}^{5}$, $C \\in \\mathbb{R}^{0 \\times 3}$, $\\,d \\in \\mathbb{R}^{0}\\,$ be\n$$\nA =\n\\begin{bmatrix}\n1 & 2 & 0 \\\\\n0 & 1 & 1 \\\\\n3 & -1 & 2 \\\\\n0 & 0 & 1 \\\\\n2 & 1 & 0\n\\end{bmatrix},\\quad\nb =\n\\begin{bmatrix}\n1 \\\\ 0 \\\\ 4 \\\\ -1 \\\\ 2\n\\end{bmatrix},\\quad\nC = \\text{an empty matrix with } 0 \\text{ rows and } 3 \\text{ columns},\\quad\nd = \\text{an empty vector}.\n$$\n\nCase 3 (fully constrained, $\\,m = n\\,$):\nLet $A \\in \\mathbb{R}^{4 \\times 3}$, $b \\in \\mathbb{R}^{4}$, $C \\in \\mathbb{R}^{3 \\times 3}$, $d \\in \\mathbb{R}^{3}$ be\n$$\nA =\n\\begin{bmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & -1 \\\\\n2 & -1 & 0 \\\\\n1 & 1 & 1\n\\end{bmatrix},\\quad\nb =\n\\begin{bmatrix}\n3 \\\\ -1 \\\\ 2 \\\\ 0\n\\end{bmatrix},\\quad\nC =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix},\\quad\nd =\n\\begin{bmatrix}\n0.5 \\\\ -1.0 \\\\ 2.0\n\\end{bmatrix}.\n$$\n\nCase 4 (single constraint, larger $\\,n\\,$):\nLet $A \\in \\mathbb{R}^{7 \\times 5}$, $b \\in \\mathbb{R}^{7}$, $C \\in \\mathbb{R}^{1 \\times 5}$, $d \\in \\mathbb{R}^{1}$ be\n$$\nA =\n\\begin{bmatrix}\n1 & 0 & 2 & -1 & 0 \\\\\n0 & 1 & 0 & 2 & -1 \\\\\n2 & -1 & 1 & 0 & 1 \\\\\n0 & 0 & 1 & 1 & 0 \\\\\n1 & 2 & -1 & 0 & 2 \\\\\n3 & 0 & 0 & -2 & 1 \\\\\n0 & 1 & 1 & 0 & 0\n\\end{bmatrix},\\quad\nb =\n\\begin{bmatrix}\n1 \\\\ 2 \\\\ 0 \\\\ -1 \\\\ 3 \\\\ 4 \\\\ 0.5\n\\end{bmatrix},\\quad\nC =\n\\begin{bmatrix}\n1 & 0 & -1 & 0 & 2\n\\end{bmatrix},\\quad\nd =\n\\begin{bmatrix}\n1.0\n\\end{bmatrix}.\n$$\n\nOutput specification:\nFor each test case, compute the solution vector $\\,x^\\star\\,$, the residual norm $\\,r = \\|A x^\\star - b\\|_2\\,$, and the constraint violation $\\,v = \\|C x^\\star - d\\|_2\\,$. Round all floats to six decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test caseâ€™s result is itself a list of floats ordered as $\\,[x_1,\\ldots,x_n,r,v]\\,$, for the corresponding case dimension $n$.\n\nFor example, the final output format must look like\n`[[x_1,x_2,...,x_n,r,v],[...],[...],[...]]`\nwith all floats rounded to six decimal places.\n\nAngle units and physical units are not applicable to this purely mathematical problem; do not include any units in the output.",
            "solution": "The problem is to find the vector $x \\in \\mathbb{R}^n$ that minimizes the Euclidean norm of the residual $\\|A x - b\\|_2$ subject to a set of linear equality constraints $C x = d$. Here, $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $C \\in \\mathbb{R}^{p \\times n}$, and $d \\in \\mathbb{R}^p$. This is an equality-constrained least squares (ECLS) problem. A robust and principled method to solve this is the null-space method, which transforms the constrained problem into a smaller, unconstrained one. The derivation proceeds as follows, based on the fundamental principles outlined in the problem statement.\n\nFirst, we characterize the set of all vectors $x$ that satisfy the constraint $C x = d$. This set is an affine subspace of $\\mathbb{R}^n$. Any solution $x$ can be expressed as the sum of a particular solution $x_p$ (which satisfies $C x_p = d$) and a homogeneous solution $x_h$ (which satisfies $C x_h = 0$). The homogeneous solution $x_h$ must lie in the null space of $C$, denoted $\\mathcal{N}(C)$.\n$$\nx = x_p + x_h, \\quad \\text{where } x_h \\in \\mathcal{N}(C)\n$$\n\nThe core of the method is to find an orthonormal basis for the null space $\\mathcal{N}(C)$ and a suitable particular solution $x_p$. The problem specifies using the QR factorization of the transpose of the constraint matrix, $C^\\top$. Let $C^\\top \\in \\mathbb{R}^{n \\times p}$ have the QR factorization:\n$$\nC^\\top = QR\n$$\nwhere $Q \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix ($Q^\\top Q = I_n$) and $R \\in \\mathbb{R}^{n \\times p}$ is an upper trapezoidal matrix. We assume that the constraint matrix $C$ has full row rank, meaning $\\text{rank}(C)=p$. This implies that $C^\\top$ has full column rank.\n\nWe partition the orthogonal matrix $Q$ into two blocks:\n$$\nQ = \\begin{bmatrix} Q_1 & Q_2 \\end{bmatrix}\n$$\nwhere $Q_1 \\in \\mathbb{R}^{n \\times p}$ and $Q_2 \\in \\mathbb{R}^{n \\times (n-p)}$. The columns of $Q_1$ form an orthonormal basis for the range of $C^\\top$, $\\mathcal{R}(C^\\top)$. The columns of $Q_2$ form an orthonormal basis for the orthogonal complement of $\\mathcal{R}(C^\\top)$, which is precisely the null space of $C$, $\\mathcal{N}(C)$. Therefore, any homogeneous solution $x_h$ can be written as a linear combination of the columns of $Q_2$:\n$$\nx_h = Q_2 z\n$$\nfor some vector of free parameters $z \\in \\mathbb{R}^{n-p}$.\n\nNext, we find a particular solution $x_p$. A convenient choice is the minimum-norm solution to $C x = d$, which lies entirely in the range of $C^\\top$. Thus, we can express it as $x_p = Q_1 y$ for some vector $y \\in \\mathbb{R}^p$. To find $y$, we substitute this into the constraint equation:\n$$\nC x_p = d \\implies C (Q_1 y) = d\n$$\nFrom the QR factorization, we have $C = (QR)^\\top = R^\\top Q^\\top$. The matrix $R$ can be partitioned as $R = \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix}$, where $R_1 \\in \\mathbb{R}^{p \\times p}$ is upper triangular and invertible (since $\\text{rank}(C)=p$). Substituting this into the previous equation gives:\n$$\nR^\\top Q^\\top (Q_1 y) = \\begin{bmatrix} R_1^\\top & 0 \\end{bmatrix} \\begin{bmatrix} Q_1^\\top \\\\ Q_2^\\top \\end{bmatrix} (Q_1 y) = R_1^\\top Q_1^\\top Q_1 y = d\n$$\nSince $Q_1^\\top Q_1 = I_p$, the equation simplifies to a lower-triangular system for $y$:\n$$\nR_1^\\top y = d\n$$\nThis system can be solved for $y$ using forward substitution. The particular solution is then $x_p = Q_1 y$.\n\nWith the complete parameterization $x = x_p + Q_2 z$, we substitute it back into the original least squares objective:\n$$\n\\min_{x} \\|A x - b\\|_2 \\quad \\implies \\quad \\min_{z} \\|A (x_p + Q_2 z) - b\\|_2\n$$\nRearranging the terms, we get:\n$$\n\\min_{z} \\| (A Q_2) z - (b - A x_p) \\|_2\n$$\nThis is an unconstrained least squares problem for the unknown vector $z \\in \\mathbb{R}^{n-p}$. Let $\\hat{A} = A Q_2$ and $\\hat{b} = b - A x_p$. The problem is now to find $z^\\star$ that minimizes $\\|\\hat{A} z - \\hat{b}\\|_2$.\n\nThis standard least squares problem is solved using its own QR factorization. Let the factorization of $\\hat{A}$ be $\\hat{A} = \\hat{Q} \\hat{R}$, where $\\hat{Q}$ is orthogonal and $\\hat{R}$ is upper trapezoidal. The objective function becomes:\n$$\n\\|\\hat{Q} \\hat{R} z - \\hat{b}\\|_2 = \\|\\hat{Q}^\\top(\\hat{Q} \\hat{R} z - \\hat{b})\\|_2 = \\|\\hat{R} z - \\hat{Q}^\\top \\hat{b}\\|_2\n$$\nLet $\\hat{R}_1$ be the upper triangular part of $\\hat{R}$ and let $c_1$ be the corresponding part of the vector $\\hat{c} = \\hat{Q}^\\top \\hat{b}$. The solution $z^\\star$ is found by solving the upper-triangular system $\\hat{R}_1 z = c_1$ using back substitution.\n\nFinally, the optimal solution $x^\\star$ for the original ECLS problem is reconstructed by combining the particular and homogeneous parts:\n$$\nx^\\star = x_p + Q_2 z^\\star\n$$\nThe residual norm is calculated as $r = \\|A x^\\star - b\\|_2$, and the constraint violation norm is $v = \\|C x^\\star - d\\|_2$. By construction, $v$ should be close to machine precision.\n\nThis principled, step-by-step reduction using orthogonal factorizations provides a numerically stable and accurate method for solving equality-constrained least squares problems. The algorithm gracefully handles edge cases, such as problems with no constraints ($p=0$) or problems where the solution is fully determined by the constraints ($p=n$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_ecls(A, b, C, d):\n    \"\"\"\n    Solves an equality-constrained least squares problem using the null-space method.\n\n    Problem: minimize ||A*x - b||_2 subject to C*x = d.\n\n    Args:\n        A (np.ndarray): Matrix A of size m x n.\n        b (np.ndarray): Vector b of size m.\n        C (np.ndarray): Constraint matrix C of size p x n.\n        d (np.ndarray): Constraint vector d of size p.\n\n    Returns:\n        tuple: A tuple containing:\n            - x_star (np.ndarray): The solution vector of size n.\n            - residual_norm (float): The final residual norm ||A*x_star - b||_2.\n            - constraint_violation (float): The constraint violation ||C*x_star - d||_2.\n    \"\"\"\n    m, n = A.shape\n    p = C.shape[0]\n\n    if p == 0:\n        # Case with no constraints: reduces to a standard least squares problem.\n        # This branch aligns with the general logic by setting up identity transformations.\n        xp = np.zeros(n)\n        Q2 = np.eye(n)\n        A_hat = A\n        b_hat = b\n    else:\n        # Perform QR factorization on the transpose of the constraint matrix C\n        # C is p x n, so C.T is n x p.\n        Q, R = np.linalg.qr(C.T, mode='complete')\n\n        # Partition Q and R\n        # Q is n x n, R is n x p\n        Q1 = Q[:, :p]\n        Q2 = Q[:, p:]\n        R1 = R[:p, :]\n\n        # Find the particular solution xp to C*x = d\n        # Solve the lower triangular system R1.T * y = d\n        y = np.linalg.solve(R1.T, d)\n        xp = Q1 @ y\n\n        # Form the reduced unconstrained least squares problem\n        # min ||(A*Q2)*z - (b - A*xp)||_2\n        A_hat = A @ Q2\n        b_hat = b - A @ xp\n\n    # Solve the reduced unconstrained LS problem for the coefficients z of the null space\n    k = A_hat.shape[1]  # This is n-p\n    if k > 0:\n        # Use QR factorization of A_hat to solve for z\n        Q_hat, R_hat = np.linalg.qr(A_hat, mode='reduced')\n\n        # Solve R_hat * z = Q_hat.T * b_hat\n        c_hat = Q_hat.T @ b_hat\n        z_star = np.linalg.solve(R_hat, c_hat)\n    else:\n        # Trivial case where the null space is empty (n=p)\n        z_star = np.array([])\n    \n    # Reconstruct the final solution\n    # Q2 @ z_star produces a vector of zeros if z_star is empty\n    x_star = xp + Q2 @ z_star\n\n    # Calculate final norms for verification and output\n    residual_norm = np.linalg.norm(A @ x_star - b)\n    if p > 0:\n        constraint_violation = np.linalg.norm(C @ x_star - d)\n    else:\n        constraint_violation = 0.0\n\n    return x_star, residual_norm, constraint_violation\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and print results in the specified format.\n    \"\"\"\n    # Case 1\n    A1 = np.array([\n        [2., -1., 0., 0.], [1., 3., -2., 1.], [0., 1., 4., -1.],\n        [3., 0., 1., 2.], [0., 2., 0., 1.], [1., 0., -1., 1.]\n    ])\n    b1 = np.array([1., 2., 3., 0., -1., 4.])\n    C1 = np.array([[1., 0., 1., 0.], [0., 1., -1., 2.]])\n    d1 = np.array([1., 0.5])\n\n    # Case 2\n    A2 = np.array([\n        [1., 2., 0.], [0., 1., 1.], [3., -1., 2.],\n        [0., 0., 1.], [2., 1., 0.]\n    ])\n    b2 = np.array([1., 0., 4., -1., 2.])\n    C2 = np.empty((0, 3))\n    d2 = np.empty(0)\n\n    # Case 3\n    A3 = np.array([\n        [1., 0., 2.], [0., 1., -1.], [2., -1., 0.], [1., 1., 1.]\n    ])\n    b3 = np.array([3., -1., 2., 0.])\n    C3 = np.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n    d3 = np.array([0.5, -1.0, 2.0])\n\n    # Case 4\n    A4 = np.array([\n        [1., 0., 2., -1., 0.], [0., 1., 0., 2., -1.], [2., -1., 1., 0., 1.],\n        [0., 0., 1., 1., 0.], [1., 2., -1., 0., 2.], [3., 0., 0., -2., 1.],\n        [0., 1., 1., 0., 0.]\n    ])\n    b4 = np.array([1., 2., 0., -1., 3., 4., 0.5])\n    C4 = np.array([[1., 0., -1., 0., 2.]])\n    d4 = np.array([1.0])\n\n    test_cases = [\n        (A1, b1, C1, d1),\n        (A2, b2, C2, d2),\n        (A3, b3, C3, d3),\n        (A4, b4, C4, d4),\n    ]\n\n    all_results = []\n    for A, b, C, d in test_cases:\n        x_star, r, v = solve_ecls(A, b, C, d)\n        case_result = list(x_star) + [r, v]\n        all_results.append(case_result)\n    \n    # Format the final output string exactly as specified.\n    list_strs = []\n    for case_res in all_results:\n        num_strs = [f\"{x:.6f}\" for x in case_res]\n        list_strs.append(f\"[{','.join(num_strs)}]\")\n    final_output = f\"[{','.join(list_strs)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}