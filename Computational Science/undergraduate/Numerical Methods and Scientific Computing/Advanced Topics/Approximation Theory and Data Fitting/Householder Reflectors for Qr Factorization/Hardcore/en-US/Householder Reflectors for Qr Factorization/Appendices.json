{
    "hands_on_practices": [
        {
            "introduction": "Understanding the QR factorization begins with mastering its core component: the Householder vector. This exercise  strips the problem down to its geometric essence in two dimensions. It focuses on how to construct the specific vector $v$ that defines a reflection to transform a given vector into a desired form, reinforcing the fundamental algebraic formula.",
            "id": "18019",
            "problem": "A Householder reflection is a linear transformation that describes a reflection about a hyperplane. In numerical linear algebra, Householder reflections are used to zero out selected entries of a vector, which is a key step in algorithms like QR factorization.\n\nA Householder matrix $P$ is defined as:\n$$P = I - 2 \\frac{vv^T}{v^T v}$$\nwhere $I$ is the identity matrix and $v$ is a non-zero vector called the Householder vector, which is normal to the hyperplane of reflection.\n\nConsider a column vector $x \\in \\mathbb{R}^n$. The goal is to find a Householder vector $v$ such that the transformation $Px$ results in a vector that is collinear with the first standard basis vector $e_1 = [1, 0, \\dots, 0]^T$. That is, $Px = \\alpha e_1$ for some scalar $\\alpha$.\n\nSince a reflection is an isometry, it preserves the Euclidean norm, so we must have $\\|Px\\| = \\|x\\|$. This implies $|\\alpha| = \\|x\\|$. To ensure numerical stability, a standard convention is to choose $\\alpha$ such that it has the opposite sign of the first component of $x$, i.e., $\\alpha = -\\text{sign}(x_1)\\|x\\|$.\n\nThe Householder vector $v$ that achieves this transformation is given by the difference between the original vector $x$ and its target image $y = \\alpha e_1$:\n$$v = x - y = x - \\alpha e_1$$\nThis can also be written as $v = x + \\text{sign}(x_1)\\|x\\| e_1$.\n\n**Problem:**\nFor a general 2-dimensional vector $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$ where $x_1 > 0$, derive an expression for the sum of the components of the unnormalized Householder vector $v = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}$ that transforms $x$ into a vector of the form $[k, 0]^T$. Your final answer should be an expression in terms of $x_1$ and $x_2$.",
            "solution": "We choose the reflection target\n$$y = \\alpha e_1,\\qquad \\alpha = -\\mathrm{sign}(x_1)\\|x\\| = -\\sqrt{x_1^2 + x_2^2}$$\nsince $x_1>0$. The unnormalized Householder vector is\n$$v = x - y = \\begin{bmatrix}x_1 - \\alpha \\\\ x_2\\end{bmatrix} \n= \\begin{bmatrix}x_1 + \\sqrt{x_1^2 + x_2^2} \\\\ x_2\\end{bmatrix}.$$\nTherefore, the sum of its components is\n$$S = v_1 + v_2 = x_1 + \\sqrt{x_1^2 + x_2^2} + x_2.$$",
            "answer": "$$\\boxed{x_1 + x_2 + \\sqrt{x_1^2 + x_2^2}}$$"
        },
        {
            "introduction": "Building upon the basic construction of the Householder vector, this practice  takes you through the first, crucial step of the QR factorization algorithm for a matrix. You will apply a Householder reflection to the first column of a matrix to introduce zeros, a procedure that is repeated to create the final upper triangular matrix $R$. This exercise provides concrete practice in applying the transformation and understanding its effect on the other columns of the matrix.",
            "id": "17992",
            "problem": "### Problem Statement\n\nThe QR factorization of a matrix $A$ is a decomposition $A=QR$, where $Q$ is an orthogonal matrix and $R$ is an upper triangular matrix. One method to achieve this is through a series of Householder reflections.\n\nA Householder reflection is a linear transformation defined by a matrix $H = I - 2 \\frac{\\mathbf{v}\\mathbf{v}^T}{\\mathbf{v}^T\\mathbf{v}}$ for some non-zero vector $\\mathbf{v}$, where $I$ is the identity matrix. This transformation reflects any vector across the hyperplane orthogonal to $\\mathbf{v}$.\n\nTo begin the QR factorization of an $m \\times n$ matrix $A$, the first step is to apply a Householder transformation $H_1$ that maps the first column of $A$, denoted $\\mathbf{a}_1$, to a vector proportional to the standard basis vector $\\mathbf{e}_1 = (1, 0, \\dots, 0)^T$. This zeros out all entries below the first entry in the first column.\n\nThe required Householder matrix is $H_1 = I - 2 \\frac{\\mathbf{v}_1\\mathbf{v}_1^T}{\\mathbf{v}_1^T\\mathbf{v}_1}$, where the Householder vector $\\mathbf{v}_1$ is constructed as:\n$$\n\\mathbf{v}_1 = \\mathbf{a}_1 - \\alpha \\mathbf{e}_1\n$$\nand the scalar $\\alpha$ is given by:\n$$\n\\alpha = -\\text{sgn}(a_{11})\\|\\mathbf{a}_1\\|_2\n$$\nHere, $a_{11}$ is the first element of $\\mathbf{a}_1$, $\\|\\cdot\\|_2$ denotes the Euclidean norm, and the sign function $\\text{sgn}(x)$ is $1$ if $x \\ge 0$ and $-1$ if $x < 0$. After this first step, the transformed matrix is $A' = H_1 A$.\n\nGiven the matrix:\n$$\nA = \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 2 & 4 \\\\ 1 & 3 & 9 \\end{pmatrix}\n$$\nCalculate the entry $(A')_{2,2}$ of the matrix $A' = H_1 A$.",
            "solution": "We have $A\\in\\mathbb{R}^{3\\times3}$ with first column $\\mathbf{a}_1=(1,1,1)^T$.  We seek $H_1=I-2\\frac{\\mathbf{v}\\mathbf{v}^T}{\\mathbf{v}^T\\mathbf{v}}$ where\n$$\n\\alpha=-\\sgn(a_{11})\\|\\mathbf{a}_1\\|_2=-1\\cdot\\sqrt{1^2+1^2+1^2}=-\\sqrt3,\n$$\n$$\n\\mathbf{v}=\\mathbf{a}_1-\\alpha\\mathbf{e}_1\n=\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}-(-\\sqrt3)\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}\n=\\begin{pmatrix}1+\\sqrt3\\\\1\\\\1\\end{pmatrix}.\n$$\n\nNext, let $\\mathbf{a}_2=(1,2,3)^T$ be the second column of $A$.  Then\n$$\n\\mathbf{v}^T\\mathbf{v}=(1+\\sqrt3)^2+1^2+1^2\n=1+2\\sqrt3+3+1+1=6+2\\sqrt3,\n$$\n$$\n\\mathbf{v}^T\\mathbf{a}_2=(1+\\sqrt3)\\cdot1+1\\cdot2+1\\cdot3\n=6+\\sqrt3.\n$$\nThus\n$$\nH_1\\mathbf{a}_2\n=\\mathbf{a}_2-2\\,\\frac{\\mathbf{v}(\\mathbf{v}^T\\mathbf{a}_2)}{\\mathbf{v}^T\\mathbf{v}}\n=\\begin{pmatrix}1\\\\2\\\\3\\end{pmatrix}\n-2\\,\\frac{6+\\sqrt3}{6+2\\sqrt3}\\begin{pmatrix}1+\\sqrt3\\\\1\\\\1\\end{pmatrix}.\n$$\nSimplify the scalar:\n$$\n2\\,\\frac{6+\\sqrt3}{6+2\\sqrt3}\n=\\frac{12+2\\sqrt3}{6+2\\sqrt3}\n=\\frac{6+\\sqrt3}{3+\\sqrt3}\n=\\frac{(6+\\sqrt3)(3-\\sqrt3)}{6}\n=\\frac{15-3\\sqrt3}{6}\n=\\frac{5-\\sqrt3}{2}.\n$$\nHence\n$$\n(H_1\\mathbf{a}_2)_2\n=2-\\frac{5-\\sqrt3}{2}\\cdot1\n=\\frac{4-(5-\\sqrt3)}{2}\n=\\frac{\\sqrt3-1}{2}.\n$$\nTherefore the $(2,2)$ entry of $A'=H_1A$ is $\\displaystyle\\frac{\\sqrt3-1}{2}$.",
            "answer": "$$\\boxed{\\frac{\\sqrt3-1}{2}}$$"
        },
        {
            "introduction": "Why are Householder reflectors the preferred method for QR factorization in high-quality numerical software? This advanced practice  provides the answer through a computational experiment. By comparing the backward error of Householder QR with the more intuitive Classical Gram-Schmidt method on a set of ill-conditioned matrices, you will gain a hands-on appreciation for the superior numerical stability that makes Householder transformations so essential in scientific computing.",
            "id": "3240095",
            "problem": "Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ be a real matrix. The orthogonal-triangular factorization (QR) of $\\mathbf{A}$ seeks matrices $\\mathbf{Q} \\in \\mathbb{R}^{m \\times m}$ and $\\mathbf{R} \\in \\mathbb{R}^{m \\times n}$ such that $\\mathbf{Q}$ is orthogonal, meaning $\\mathbf{Q}^{\\top} \\mathbf{Q} = \\mathbf{I}$, and $\\mathbf{R}$ is upper triangular in its leading $n \\times n$ block, with $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$. A Householder reflector is a matrix of the form $\\mathbf{H} = \\mathbf{I} - 2 \\mathbf{u} \\mathbf{u}^{\\top}$, where $\\mathbf{u} \\in \\mathbb{R}^{m}$ is any unit vector, and satisfies $\\mathbf{H}^{\\top} = \\mathbf{H}$ and $\\mathbf{H}^{2} = \\mathbf{I}$; it reflects vectors across the hyperplane orthogonal to $\\mathbf{u}$. Classical Gram–Schmidt (CGS) orthogonalization constructs an orthonormal basis by sequentially projecting and subtracting components along previously formed basis vectors. Both approaches are widely studied in numerical methods and scientific computing.\n\nYou will construct a family of matrices $\\mathbf{A}(\\epsilon)$ with columns that are nearly collinear, and then compare the backward error of the Householder-based orthogonal-triangular factorization (QR) and Classical Gram–Schmidt (CGS) as $\\epsilon \\to 0$. Use the following foundational definitions:\n- The Euclidean norm of a vector $\\mathbf{x}$ is $\\lVert \\mathbf{x} \\rVert_{2} = \\sqrt{\\sum_{i} x_{i}^{2}}$.\n- The Frobenius norm of a matrix $\\mathbf{M}$ is $\\lVert \\mathbf{M} \\rVert_{F} = \\sqrt{\\sum_{i,j} M_{ij}^{2}}$.\n- The backward error of a computed factorization $(\\mathbf{Q}, \\mathbf{R})$ for $\\mathbf{A}$ is the relative Frobenius norm of the residual, defined as\n$$\n\\mathrm{err}_{\\mathrm{bwd}}(\\mathbf{A}, \\mathbf{Q}, \\mathbf{R}) = \\frac{\\lVert \\mathbf{A} - \\mathbf{Q} \\mathbf{R} \\rVert_{F}}{\\lVert \\mathbf{A} \\rVert_{F}}.\n$$\n\nConstruct the family $\\mathbf{A}(\\epsilon) \\in \\mathbb{R}^{7 \\times 4}$ as follows. Let\n$$\n\\mathbf{u}_{\\mathrm{raw}} = [1, 2, 3, 4, 5, 6, 7]^{\\top}, \\quad \\mathbf{d}_{2,\\mathrm{raw}} = [7, 6, 5, 4, 3, 2, 1]^{\\top},\n$$\n$$\n\\mathbf{d}_{3,\\mathrm{raw}} = [1, 0, 1, 0, 1, 0, 1]^{\\top}, \\quad \\mathbf{d}_{4,\\mathrm{raw}} = [0, 1, 0, 1, 0, 1, 0]^{\\top}.\n$$\nDefine $\\mathbf{u} = \\mathbf{u}_{\\mathrm{raw}} / \\lVert \\mathbf{u}_{\\mathrm{raw}} \\rVert_{2}$. For $k \\in \\{2,3,4\\}$, define\n$$\n\\mathbf{d}_{k} = \\mathbf{d}_{k,\\mathrm{raw}} - (\\mathbf{u}^{\\top} \\mathbf{d}_{k,\\mathrm{raw}}) \\mathbf{u},\n\\quad\n\\mathbf{d}_{k} \\leftarrow \\frac{\\mathbf{d}_{k}}{\\lVert \\mathbf{d}_{k} \\rVert_{2}},\n$$\nso that each $\\mathbf{d}_{k}$ is orthogonal to $\\mathbf{u}$ and has unit norm. For a given $\\epsilon \\ge 0$, set the columns of $\\mathbf{A}(\\epsilon)$ to\n$$\n\\mathbf{a}_{1}(\\epsilon) = \\mathbf{u}, \\quad\n\\mathbf{a}_{2}(\\epsilon) = \\mathbf{u} + \\epsilon \\mathbf{d}_{2}, \\quad\n\\mathbf{a}_{3}(\\epsilon) = \\mathbf{u} + \\epsilon^{2} \\mathbf{d}_{3}, \\quad\n\\mathbf{a}_{4}(\\epsilon) = \\mathbf{u} - \\epsilon \\mathbf{d}_{4},\n$$\nand let $\\mathbf{A}(\\epsilon) = [\\mathbf{a}_{1}(\\epsilon)\\ \\mathbf{a}_{2}(\\epsilon)\\ \\mathbf{a}_{3}(\\epsilon)\\ \\mathbf{a}_{4}(\\epsilon)]$. This construction ensures the columns are nearly collinear for small $\\epsilon$ and become exactly collinear when $\\epsilon = 0$.\n\nTasks:\n1. Implement Householder-based orthogonal-triangular factorization (QR) by successive application of Householder reflectors to transform $\\mathbf{A}(\\epsilon)$ into an upper-triangular $\\mathbf{R}$ while accumulating the orthogonal $\\mathbf{Q}$ as the product of the reflectors. Do not call any built-in QR routines.\n2. Implement Classical Gram–Schmidt (CGS) to compute $\\mathbf{Q}$ and $\\mathbf{R}$, with the convention that if a column becomes the zero vector in the process, you set the corresponding diagonal entry in $\\mathbf{R}$ to $0$ and the corresponding column in $\\mathbf{Q}$ to the zero vector.\n3. For each $\\epsilon$ in the test suite below, compute $\\mathrm{err}_{\\mathrm{bwd}}(\\mathbf{A}(\\epsilon), \\mathbf{Q}_{\\mathrm{HQR}}(\\epsilon), \\mathbf{R}_{\\mathrm{HQR}}(\\epsilon))$ for Householder-based QR and $\\mathrm{err}_{\\mathrm{bwd}}(\\mathbf{A}(\\epsilon), \\mathbf{Q}_{\\mathrm{CGS}}(\\epsilon), \\mathbf{R}_{\\mathrm{CGS}}(\\epsilon))$ for Classical Gram–Schmidt.\n\nTest Suite:\n- $\\epsilon_{1} = 10^{-1}$ (general case),\n- $\\epsilon_{2} = 10^{-4}$ (moderately small),\n- $\\epsilon_{3} = 10^{-8}$ (near double-precision rounding scale),\n- $\\epsilon_{4} = 10^{-12}$ (very close to collinearity),\n- $\\epsilon_{5} = 0$ (exact collinearity, rank-deficient boundary).\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry corresponds to one $\\epsilon$ in the order listed and is itself a two-element list $[\\mathrm{err}_{\\mathrm{HQR}}, \\mathrm{err}_{\\mathrm{CGS}}]$ of floating-point numbers. For example, the output must have the form\n$$\n[\\,[e_{1,\\mathrm{HQR}}, e_{1,\\mathrm{CGS}}],\\,[e_{2,\\mathrm{HQR}}, e_{2,\\mathrm{CGS}}],\\,[e_{3,\\mathrm{HQR}}, e_{3,\\mathrm{CGS}}],\\,[e_{4,\\mathrm{HQR}}, e_{4,\\mathrm{CGS}}],\\,[e_{5,\\mathrm{HQR}}, e_{5,\\mathrm{CGS}}]\\,].\n$$\nNo physical units or angles are involved in this problem; numerical values should be printed as standard floating-point decimals.",
            "solution": "The core of the problem is to investigate the numerical stability of two algorithms for computing the QR factorization of a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$. The factorization is given by $\\mathbf{A} = \\mathbf{Q} \\mathbf{R}$, where $\\mathbf{Q} \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix ($\\mathbf{Q}^{\\top} \\mathbf{Q} = \\mathbf{I}$) and $\\mathbf{R} \\in \\mathbb{R}^{m \\times n}$ is an upper triangular matrix (meaning all entries $R_{ij}$ with $i > j$ are zero). We will compare a method based on Householder reflectors, known for its excellent numerical stability, with the Classical Gram-Schmidt procedure, which is known to be numerically unstable. The comparison metric is the backward error, $\\mathrm{err}_{\\mathrm{bwd}} = \\frac{\\lVert \\mathbf{A} - \\mathbf{Q} \\mathbf{R} \\rVert_{F}}{\\lVert \\mathbf{A} \\rVert_{F}}$, which measures how close the product of the computed factors is to the original matrix, relative to the size of the original matrix.\n\nFirst, we construct the family of test matrices $\\mathbf{A}(\\epsilon) \\in \\mathbb{R}^{7 \\times 4}$ as specified. The columns of $\\mathbf{A}(\\epsilon)$ become nearly linearly dependent as $\\epsilon \\to 0$, creating an ill-conditioned problem that challenges the stability of numerical algorithms.\n\nThe specified raw vectors are:\n$$\n\\mathbf{u}_{\\mathrm{raw}} = [1, 2, 3, 4, 5, 6, 7]^{\\top}, \\quad \\mathbf{d}_{2,\\mathrm{raw}} = [7, 6, 5, 4, 3, 2, 1]^{\\top}\n$$\n$$\n\\mathbf{d}_{3,\\mathrm{raw}} = [1, 0, 1, 0, 1, 0, 1]^{\\top}, \\quad \\mathbf{d}_{4,\\mathrm{raw}} = [0, 1, 0, 1, 0, 1, 0]^{\\top}\n$$\nWe first normalize $\\mathbf{u}_{\\mathrm{raw}}$ to obtain a unit vector $\\mathbf{u} = \\mathbf{u}_{\\mathrm{raw}} / \\lVert \\mathbf{u}_{\\mathrm{raw}} \\rVert_{2}$.\nThen, for each $\\mathbf{d}_{k,\\mathrm{raw}}$ ($k \\in \\{2,3,4\\}$), we make it orthogonal to $\\mathbf{u}$ and normalize it. This is a Gram-Schmidt step:\n1.  Project $\\mathbf{d}_{k,\\mathrm{raw}}$ onto $\\mathbf{u}$: $\\mathrm{proj}_{\\mathbf{u}}(\\mathbf{d}_{k,\\mathrm{raw}}) = (\\mathbf{u}^{\\top} \\mathbf{d}_{k,\\mathrm{raw}}) \\mathbf{u}$.\n2.  Subtract the projection to get a vector orthogonal to $\\mathbf{u}$: $\\mathbf{d}_{k, \\perp} = \\mathbf{d}_{k,\\mathrm{raw}} - (\\mathbf{u}^{\\top} \\mathbf{d}_{k,\\mathrm{raw}}) \\mathbf{u}$.\n3.  Normalize to get the final unit vector: $\\mathbf{d}_{k} = \\mathbf{d}_{k, \\perp} / \\lVert \\mathbf{d}_{k, \\perp} \\rVert_{2}$.\n\nThe columns of $\\mathbf{A}(\\epsilon)$ are then defined as:\n$$\n\\mathbf{a}_{1}(\\epsilon) = \\mathbf{u}, \\quad\n\\mathbf{a}_{2}(\\epsilon) = \\mathbf{u} + \\epsilon \\mathbf{d}_{2}, \\quad\n\\mathbf{a}_{3}(\\epsilon) = \\mathbf{u} + \\epsilon^{2} \\mathbf{d}_{3}, \\quad\n\\mathbf{a}_{4}(\\epsilon) = \\mathbf{u} - \\epsilon \\mathbf{d}_{4}\n$$\nThe matrix is $\\mathbf{A}(\\epsilon) = [\\mathbf{a}_{1}(\\epsilon)\\ \\mathbf{a}_{2}(\\epsilon)\\ \\mathbf{a}_{3}(\\epsilon)\\ \\mathbf{a}_{4}(\\epsilon)]$.\n\nNext, we detail the two factorization algorithms.\n\n**1. Householder QR Factorization**\n\nThis method uses a sequence of Householder reflectors to introduce zeros below the diagonal of the matrix $\\mathbf{A}$, transforming it into the upper triangular matrix $\\mathbf{R}$. A Householder reflector for a non-zero vector $\\mathbf{v}$ is given by $\\mathbf{H} = \\mathbf{I} - 2\\frac{\\mathbf{v}\\mathbf{v}^\\top}{\\mathbf{v}^\\top\\mathbf{v}}$. It reflects any vector across the hyperplane orthogonal to $\\mathbf{v}$.\n\nThe algorithm proceeds for $k = 1, \\dots, n$:\n1.  Let the current matrix be $\\mathbf{A}^{(k-1)}$ (with $\\mathbf{A}^{(0)}=\\mathbf{A}$). We focus on the $k$-th column. Let $\\mathbf{x}$ be the vector corresponding to the sub-column from the diagonal downwards, i.e., $\\mathbf{x} = \\mathbf{A}^{(k-1)}[k-1:m, k-1]$.\n2.  We construct a reflector $\\mathbf{H}'_k$ that transforms $\\mathbf{x}$ into a vector parallel to $\\mathbf{e}_1 = [1, 0, \\dots, 0]^\\top$. Specifically, $\\mathbf{H}'_k \\mathbf{x} = \\alpha \\mathbf{e}_1$, where $\\alpha = -\\mathrm{sgn}(x_1) \\lVert \\mathbf{x} \\rVert_2$. The sign is chosen to avoid catastrophic cancellation when forming the reflector vector $\\mathbf{v}_k = \\mathbf{x} - \\alpha \\mathbf{e}_1$.\n3.  This reflector $\\mathbf{H}'_k$ of size $(m-k+1) \\times (m-k+1)$ is embedded into an $m \\times m$ matrix $\\mathbf{H}_k = \\begin{pmatrix} \\mathbf{I}_{k-1} & \\mathbf{0} \\\\ \\mathbf{0} & \\mathbf{H}'_k \\end{pmatrix}$.\n4.  The matrix is updated: $\\mathbf{A}^{(k)} = \\mathbf{H}_k \\mathbf{A}^{(k-1)}$. This zeros out the entries below the diagonal in the $k$-th column.\nAfter $n$ steps, we have $\\mathbf{R} = \\mathbf{A}^{(n)} = \\mathbf{H}_n \\cdots \\mathbf{H}_1 \\mathbf{A}$.\nFrom this, $\\mathbf{A} = (\\mathbf{H}_1 \\cdots \\mathbf{H}_n)\\mathbf{R}$, since $\\mathbf{H}_k^{-1} = \\mathbf{H}_k$.\nThe orthogonal matrix is thus $\\mathbf{Q} = \\mathbf{H}_1 \\cdots \\mathbf{H}_n$.\nTo compute $\\mathbf{Q}$, one can start with $\\mathbf{Q} = \\mathbf{I}_{m \\times m}$ and successively apply the reflectors from the right: $\\mathbf{Q} \\leftarrow \\mathbf{Q} \\mathbf{H}_k$ for $k=1, \\dots, n$.\n\n**2. Classical Gram-Schmidt (CGS) QR Factorization**\n\nCGS builds the columns of $\\mathbf{Q}$ sequentially. For each column $\\mathbf{a}_j$ of $\\mathbf{A}$, it subtracts its projections onto the previously computed orthonormal vectors $\\mathbf{q}_1, \\dots, \\mathbf{q}_{j-1}$, and then normalizes the result.\n\nThe algorithm proceeds for $j = 1, \\dots, n$:\n1.  Initialize an orthogonal component vector $\\mathbf{v}_j = \\mathbf{a}_j$.\n2.  For $i = 1, \\dots, j-1$, project $\\mathbf{a}_j$ onto $\\mathbf{q}_i$ and subtract:\n    *   $R_{ij} = \\mathbf{q}_i^{\\top} \\mathbf{a}_j$. (Note: CGS projects the original vector $\\mathbf{a}_j$, not the intermediate vector $\\mathbf{v}_j$. This is the source of its instability).\n    *   $\\mathbf{v}_j \\leftarrow \\mathbf{v}_j - R_{ij} \\mathbf{q}_i$.\n3.  Compute the norm of the resulting vector: $R_{jj} = \\lVert \\mathbf{v}_j \\rVert_2$.\n4.  Normalize to find the next orthonormal vector: $\\mathbf{q}_j = \\mathbf{v}_j / R_{jj}$.\nIf at any point $R_{jj}$ becomes zero (or numerically indistinguishable from zero), it signifies that $\\mathbf{a}_j$ is linearly dependent on the preceding columns. Following the problem specification, we set $R_{jj}=0$ and the corresponding column $\\mathbf{q}_j$ to the zero vector. A consequence is that the computed $\\mathbf{Q}_{\\mathrm{CGS}}$ is not guaranteed to have orthonormal columns, especially for ill-conditioned or rank-deficient matrices. The backward error will quantify this failure. This procedure naturally produces a reduced QR factorization, where $\\mathbf{Q} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{R} \\in \\mathbb{R}^{n \\times n}$, which is sufficient for calculating the backward error.\n\n**Comparison and Expected Results**\nFor small $\\epsilon$, the columns of $\\mathbf{A}(\\epsilon)$ are nearly parallel. In CGS, when computing $\\mathbf{q}_j$, the subtraction $\\mathbf{v}_j \\leftarrow \\mathbf{v}_j - R_{ij} \\mathbf{q}_i$ involves two nearly identical vectors. This leads to catastrophic cancellation, where most significant digits are lost. The resulting $\\mathbf{v}_j$ has large relative error, and the computed vectors $\\{\\mathbf{q}_j\\}$ will lose their orthogonality. This loss of orthogonality degrades the quality of the factorization, leading to a large backward error.\n\nHouseholder QR operates on whole columns and rows with orthogonal transformations. These transformations preserve the Frobenius norm of the matrix and are numerically backward stable. Therefore, even for ill-conditioned matrices, the computed $\\mathbf{Q}_{\\mathrm{HQR}}$ remains very close to a true orthogonal matrix, and the backward error $\\mathrm{err}_{\\mathrm{bwd}}(\\mathbf{A}, \\mathbf{Q}_{\\mathrm{HQR}}, \\mathbf{R}_{\\mathrm{HQR}})$ remains small, typically on the order of machine epsilon.\n\nFor $\\epsilon = 0$, the matrix $\\mathbf{A}(0)$ is rank-1. Both algorithms should handle this limiting case gracefully according to their definitions, ideally yielding a small backward error. However, for $\\epsilon \\approx 10^{-8}$ (near the square root of machine epsilon for double precision), CGS is expected to perform very poorly, while Householder QR should remain robust across all tested values of $\\epsilon$.\nThe subsequent code implements these algorithms and computes the backward errors for the specified test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_A(epsilon: float) -> np.ndarray:\n    \"\"\"\n    Constructs the matrix A(epsilon) as specified in the problem.\n    \"\"\"\n    m, n = 7, 4\n    \n    u_raw = np.array([1, 2, 3, 4, 5, 6, 7], dtype=float)\n    d_raw_list = [\n        np.array([7, 6, 5, 4, 3, 2, 1], dtype=float),\n        np.array([1, 0, 1, 0, 1, 0, 1], dtype=float),\n        np.array([0, 1, 0, 1, 0, 1, 0], dtype=float)\n    ]\n    \n    u = u_raw / np.linalg.norm(u_raw)\n    \n    d_list = []\n    for d_raw in d_raw_list:\n        d_ortho = d_raw - (u.T @ d_raw) * u\n        d_norm = np.linalg.norm(d_ortho)\n        d_list.append(d_ortho / d_norm)\n        \n    a1 = u\n    a2 = u + epsilon * d_list[0]\n    a3 = u + epsilon**2 * d_list[1]\n    a4 = u - epsilon * d_list[2]\n    \n    A = np.zeros((m, n))\n    A[:, 0] = a1\n    A[:, 1] = a2\n    A[:, 2] = a3\n    A[:, 3] = a4\n    \n    return A\n\ndef householder_qr(A: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Performs QR factorization using Householder reflectors.\n    Returns Q (m x m) and R (m x n).\n    \"\"\"\n    m, n = A.shape\n    Q = np.identity(m)\n    R = A.copy()\n\n    for j in range(n):\n        # Extract the vector to be reflected\n        x = R[j:, j]\n        norm_x = np.linalg.norm(x)\n\n        # Numerically stable choice for alpha\n        alpha = -np.copysign(norm_x, x[0]) if norm_x != 0 else 0\n        \n        v = x.copy()\n        v[0] -= alpha\n        \n        norm_v = np.linalg.norm(v)\n\n        # If the reflector vector is zero, the transformation is identity\n        if norm_v < 1e-16:\n            continue\n\n        # Normalize the reflector vector to get u\n        u = v / norm_v\n        u = u.reshape(-1, 1) # Ensure u is a column vector\n\n        # Apply the reflection to the relevant submatrix of R\n        R_sub = R[j:, j:]\n        R[j:, j:] = R_sub - 2 * u @ (u.T @ R_sub)\n\n        # Apply the reflection to the relevant columns of Q (from the right)\n        Q_sub = Q[:, j:]\n        Q[:, j:] = Q_sub - 2 * (Q_sub @ u) @ u.T\n    \n    # Return Q and the R part of the final matrix\n    return Q, R[:m, :n]\n\ndef cgs_qr(A: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Performs QR factorization using Classical Gram-Schmidt.\n    Returns Q (m x n, reduced) and R (n x n, reduced).\n    \"\"\"\n    m, n = A.shape\n    Q = np.zeros((m, n))\n    R = np.zeros((n, n))\n\n    for j in range(n):\n        v = A[:, j].copy()\n        \n        # Subtract projections onto previous q vectors\n        for i in range(j):\n            # CGS uses projection of a_j, not the intermediate v\n            R[i, j] = Q[:, i].T @ A[:, j]\n            v -= R[i, j] * Q[:, i]\n            \n        norm_v = np.linalg.norm(v)\n\n        if norm_v < 1e-16: # Handle linear dependence as per problem spec\n            R[j, j] = 0\n            # Q[:, j] is already zero\n        else:\n            R[j, j] = norm_v\n            Q[:, j] = v / norm_v\n\n    return Q, R\n\ndef solve():\n    \"\"\"\n    Main function to run the comparison and print results.\n    \"\"\"\n    # Test suite of epsilon values\n    test_suite = [10**-1, 10**-4, 10**-8, 10**-12, 0.0]\n    \n    results = []\n    \n    for epsilon in test_suite:\n        A = construct_A(epsilon)\n        norm_A = np.linalg.norm(A, 'fro')\n\n        # Householder QR\n        Q_hqr, R_hqr = householder_qr(A)\n        residual_hqr = A - Q_hqr @ R_hqr\n        err_hqr = np.linalg.norm(residual_hqr, 'fro') / norm_A if norm_A > 0 else 0.0\n        \n        # Classical Gram-Schmidt QR\n        Q_cgs, R_cgs = cgs_qr(A)\n        residual_cgs = A - Q_cgs @ R_cgs\n        err_cgs = np.linalg.norm(residual_cgs, 'fro') / norm_A if norm_A > 0 else 0.0\n\n        results.append([err_hqr, err_cgs])\n\n    # Format the output as specified\n    formatted_results = [f\"[{hqr:.16e}, {cgs:.16e}]\" for hqr, cgs in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}