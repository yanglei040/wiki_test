{
    "hands_on_practices": [
        {
            "introduction": "本练习探讨了主成分分析（PCA）的一个基本方面，而PCA是潜因子模型的基石。我们将研究当数据在分析前未进行中心化时会发生什么，揭示第一主成分如何可能被数据的均值（位置）而非其方差结构（形状）所主导。理解这一区别对于正确解释潜因子和确保分析捕捉到预期的变异来源至关重要。",
            "id": "3137645",
            "problem": "考虑一个向量的随机样本 $x_1, x_2, \\dots, x_n \\in \\mathbb{R}^p$，它们独立同分布地从一个均值向量为 $\\mu \\in \\mathbb{R}^p$、协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{p \\times p}$ 的分布中抽取。令 $X \\in \\mathbb{R}^{n \\times p}$ 为数据矩阵，其第 $i$ 行为 $x_i^\\top$。假设主成分分析（PCA）不是在中心化数据上执行，而是直接在经验二阶矩矩阵 $M_n = \\frac{1}{n} X^\\top X$ 上执行。在 $n$ 很大且 $\\Sigma$ 是良态的假设下，评估当 $X$ 未中心化时第一主成分的行为，并找出能够将位置效应（由 $\\mu$ 引起）与形状效应（由 $\\Sigma$ 引起）分离开的有效技术。\n\n下列哪个陈述是正确的？\n\nA. 如果数据未中心化，且位置向量 $\\mu$ 的范数相对于协方差谱足够大，那么 $M_n$ 的主特征向量倾向于与 $\\mu$ 的方向对齐，因此第一主成分主要捕获的是均值效应。\n\nB. 将每个特征标准化为单位方差，而不减去特征均值，可以消除 $\\mu$ 对第一主成分的影响，从而确保 PCA 仅反映协方差结构。\n\nC. 在 PCA 之前通过减去样本均值来中心化每个特征，可以分离形状效应，因为中心化经验协方差矩阵的特征向量反映的是关于均值的最大方差方向，而不是整体位置。\n\nD. 将每个观测值投影到样本均值方向的正交补空间上，然后在该子空间中执行 PCA，即使单个特征的均值非零，也能分离位置和形状效应，因为投影消除了沿均值方向的分量。",
            "solution": "在进行求解之前，对问题陈述的有效性进行严格评估。\n\n### 步骤 1：提取已知条件\n- 向量的随机样本 $x_1, x_2, \\dots, x_n \\in \\mathbb{R}^p$。\n- 样本是独立同分布的（i.i.d.）。\n- 分布的均值向量为 $\\mu \\in \\mathbb{R}^p$。\n- 分布的协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{p \\times p}$。\n- $X \\in \\mathbb{R}^{n \\times p}$ 是数据矩阵，其第 $i$ 行为 $x_i^\\top$。\n- 主成分分析（PCA）在经验二阶矩矩阵 $M_n = \\frac{1}{n} X^\\top X$ 上执行。\n- 假设：$n$ 很大，且 $\\Sigma$ 是良态的。\n- 任务是评估在未中心化数据上第一主成分的行为，并找出能够将位置效应（来自 $\\mu$）与形状效应（来自 $\\Sigma$）分离开的有效技术。\n\n### 步骤 2：使用已知条件进行验证\n- **科学性：**该问题是在多元分析和 PCA 的既定数学和统计框架内提出的。所有术语（$\\mu$、$\\Sigma$、二阶矩矩阵、PCA）都是标准术语。对未中心化数据进行 PCA 分析是一个众所周知的话题。该问题在科学上是合理的。\n- **适定性：**该问题是适定的（well-posed）。$n$ 很大的假设允许使用大数定律进行渐近分析，从而为所研究的矩阵导出一个确定性极限。问题要求评估行为并识别有效方法，这可以根据所涉及矩阵的数学性质来回答。\n- **客观性：**语言是形式化和客观的。诸如“足够大的范数”和“倾向于对齐”之类的术语在渐近分析和扰动分析的背景下是标准的，并且可以被数学上精确地定义。\n\n### 步骤 3：结论和行动\n问题陈述是有效的。这是统计学习中一个标准的、适定的问题。下面将推导解答。\n\n### 解答推导\n执行 PCA 所用的矩阵是经验二阶矩矩阵 $M_n = \\frac{1}{n} X^\\top X$。矩阵 $X^\\top X$ 可以写成对观测值的求和：$X^\\top X = \\sum_{i=1}^n x_i x_i^\\top$。\n因此，$M_n = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^\\top$。\n\n由于向量 $x_i$ 是独立同分布的，根据大数定律，当样本量 $n$ 变得很大时，样本均值 $M_n$ 依概率收敛于 $x x^\\top$ 的期望值：\n$$M_n \\xrightarrow{p} \\mathbb{E}[x x^\\top]$$\n协方差矩阵 $\\Sigma$ 定义为 $\\Sigma = \\mathbb{E}[(x-\\mu)(x-\\mu)^\\top]$。展开此表达式得到：\n$$ \\Sigma = \\mathbb{E}[x x^\\top - x \\mu^\\top - \\mu x^\\top + \\mu \\mu^\\top] $$\n根据期望的线性性质，并注意到 $\\mathbb{E}[x] = \\mu$：\n$$ \\Sigma = \\mathbb{E}[x x^\\top] - \\mathbb{E}[x]\\mu^\\top - \\mu \\mathbb{E}[x^\\top] + \\mu \\mu^\\top = \\mathbb{E}[x x^\\top] - \\mu \\mu^\\top - \\mu \\mu^\\top + \\mu \\mu^\\top = \\mathbb{E}[x x^\\top] - \\mu \\mu^\\top $$\n重新整理得到总体二阶矩矩阵：\n$$ \\mathbb{E}[x x^\\top] = \\Sigma + \\mu \\mu^\\top $$\n因此，对于大的 $n$，对未中心化数据进行 PCA 近似等同于寻找矩阵 $M = \\Sigma + \\mu \\mu^\\top$ 的特征向量和特征值。矩阵 $M$ 是总体协方差矩阵 $\\Sigma$（捕获形状）和一个秩为 1 的矩阵 $\\mu \\mu^\\top$（捕获相对于原点的位置）之和。\n\n主成分是 $M$ 的特征向量。项 $\\mu \\mu^\\top$ 是一个秩为 1 的矩阵。它唯一的非零特征值是 $\\|\\mu\\|^2 = \\mu^\\top \\mu$，对应的特征向量在 $\\mu$ 的方向上。所有与 $\\mu$ 正交的向量都在其零空间中（特征值为 0）。\n\n### 逐项分析\n\n**A. 如果数据未中心化，且位置向量 $\\mu$ 的范数相对于协方差谱足够大，那么 $M_n$ 的主特征向量倾向于与 $\\mu$ 的方向对齐，因此第一主成分主要捕获的是均值效应。**\n\n如前所述，对于大的 $n$，$M_n$ 趋近于 $M = \\Sigma + \\mu \\mu^\\top$。如果 $\\mu$ 的范数足够大，特别是如果 $\\|\\mu\\|^2$ 远大于 $\\Sigma$ 的最大特征值（即 $\\|\\mu\\|^2 \\gg \\lambda_{\\max}(\\Sigma)$），那么秩为 1 的矩阵 $\\mu \\mu^\\top$ 将在和中占主导地位。根据矩阵扰动理论，$M$ 的主特征向量将非常接近主导项 $\\mu \\mu^\\top$ 的主特征向量。$\\mu \\mu^\\top$ 的主特征向量是 $\\mu$ 的任何非零标量倍。因此，未中心化数据的第一主成分将近似地与均值向量 $\\mu$ 的方向对齐。该成分捕获了数据云中心相对于原点的“位置”，这正是“均值效应”。\n\n**结论：正确。**\n\n**B. 将每个特征标准化为单位方差，而不减去特征均值，可以消除 $\\mu$ 对第一主成分的影响，从而确保 PCA 仅反映协方差结构。**\n\n将特征 $j$ 标准化为单位方差意味着将其乘以一个因子 $1/\\sigma_j$，其中 $\\sigma_j^2$ 是该特征的方差。令 $x'_i$ 为此缩放后的观测向量。新向量的均值为 $\\mu' = \\mathbb{E}[x'_i]$。如果原始均值 $\\mu$ 非零，则新均值 $\\mu'$ 通常也非零。PCA 将在缩放后数据的二阶矩矩阵上执行，对于大的 $n$，该矩阵近似于 $\\Sigma' + \\mu'(\\mu')^\\top$，其中 $\\Sigma'$ 是缩放后数据的协方差矩阵。由于与均值相关的项 $\\mu'(\\mu')^\\top$ 仍然存在，均值 $\\mu$ 的影响（尽管被重新缩放了）并未被消除。分析仍然混合了位置和形状效应。\n\n**结论：不正确。**\n\n**C. 在 PCA 之前通过减去样本均值来中心化每个特征，可以分离形状效应，因为中心化经验协方差矩阵的特征向量反映的是关于均值的最大方差方向，而不是整体位置。**\n\n这描述了 PCA 的标准流程。令 $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ 为样本均值。中心化数据意味着将每个 $x_i$ 替换为 $\\tilde{x}_i = x_i - \\bar{x}$。然后 PCA 在样本协方差矩阵 $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{i=1}^n \\tilde{x}_i \\tilde{x}_i^\\top$ 上执行。对于大的 $n$，$\\bar{x} \\to \\mu$ 且 $\\hat{\\Sigma} \\to \\Sigma$。因此，PCA 实际上是在总体协方差矩阵 $\\Sigma$ 上执行的。根据定义，$\\Sigma$ 的特征向量是数据分布围绕其均值 $\\mu$ 的最大方差方向。这种分析纯粹关注数据的“形状”（协方差结构），通过将原点移至数据的质心来移除了“位置”方面。该陈述正确地描述了为什么中心化是使用 PCA 研究协方差结构的关键第一步。\n\n**结论：正确。**\n\n**D. 将每个观测值投影到样本均值方向的正交补空间上，然后在该子空间中执行 PCA，即使单个特征的均值非零，也能分离位置和形状效应，因为投影消除了沿均值方向的分量。**\n\n令 $\\bar{x}$ 为样本均值。对于 $\\bar{x} \\neq 0$，令 $u = \\bar{x}/\\|\\bar{x}\\|$ 为其方向上的单位向量。到该方向的正交补空间上的投影矩阵是 $P_{\\perp} = I - u u^\\top$。一个投影后的观测值是 $x'_i = P_{\\perp} x_i$。让我们分析这个新数据集 $\\{x'_i\\}$ 的性质。当 $n \\to \\infty$ 时，$\\bar{x} \\to \\mu$，所以 $P_{\\perp}$ 趋近于 $P_{\\mu,\\perp} = I - \\frac{\\mu \\mu^\\top}{\\|\\mu\\|^2}$（假设 $\\mu \\neq 0$）。一个投影数据点的期望值为 $\\mathbb{E}[x'_i] \\to P_{\\mu,\\perp} \\mathbb{E}[x_i] = P_{\\mu,\\perp} \\mu = (I - \\frac{\\mu \\mu^\\top}{\\|\\mu\\|^2}) \\mu = \\mu - \\mu = 0$。投影后的数据在原点处中心化。PCA 在这个新数据的协方差矩阵上执行，即 $\\text{Cov}(x'_i) = \\mathbb{E}[x'_i (x'_i)^\\top] \\to P_{\\mu,\\perp} \\mathbb{E}[x x^\\top] P_{\\mu,\\perp} = P_{\\mu,\\perp} (\\Sigma + \\mu \\mu^\\top) P_{\\mu,\\perp}$。由于 $P_{\\mu,\\perp} \\mu = 0$，这简化为 $P_{\\mu,\\perp} \\Sigma P_{\\mu,\\perp}$。分析是在投影到与 $\\mu$ 正交的子空间上的协方差矩阵 $\\Sigma$ 上进行的。该过程通过投影掉位置效应来成功地将其与形状效应（剩余子空间内的协方差结构）分离开来。\n\n**结论：正确。**",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "潜因子模型本身存在可识别性问题，这意味着多组不同的因子可以同样好地表示数据。本练习深入探讨了这个问题最常见的形式：排列模糊性，即潜因子可以被任意重新排序。通过检验因子分解的代数性质，我们将理解为何存在这种模糊性，并探索用于对齐不同模型拟合结果中因子的原则性方法，这是比较结果和评估模型稳定性的关键一步。",
            "id": "3137743",
            "problem": "考虑统计学习中的一个潜因子模型，其中数据矩阵 $X \\in \\mathbb{R}^{n \\times m}$ 被一个秩为 $k$ 的分解 $X \\approx W H^\\top$ 近似，其中 $W \\in \\mathbb{R}^{n \\times k}$ 且 $H \\in \\mathbb{R}^{m \\times k}$。潜维度为 $k$，$W$ 和 $H$ 的列代表 $k$ 个潜因子。一个置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$ 是一个方的二元矩阵，其每行每列都恰好有一个元素为 $1$，其余元素均为 $0$。矩阵 $A$ 的弗罗贝尼乌斯范数 $\\lVert A \\rVert_F$ 定义为其元素平方和的平方根，向量 $a, b \\in \\mathbb{R}^d$ 之间的标准内积是 $\\langle a, b \\rangle = a^\\top b$。\n\n分解算法的重复运行可能会产生因子对 $(W_1, H_1)$ 和 $(W_2, H_2)$，它们都能很好地拟合 $X$，但其列存在一个置换关系（以及其他小的数值差异）。为了从第一性原理出发，论证置换模糊性下的可识别性问题以及跨运行对齐因子的问题，请评估以下陈述：\n\nA. 对于任何置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$，都有 $(W P)(H P)^\\top = W H^\\top$。\n\nB. 一种在两次运行中匹配 $k$ 个因子的原则性方法是选择使 $\\lVert W_1 - W_2 P \\rVert_F^2$ 最小化的 $P \\in \\mathbb{R}^{k \\times k}$，这等价于在一个 $k \\times k$ 的成本矩阵上解决一个线性分配问题，该矩阵的 $(i, j)$ 项度量了 $W_1$ 的第 $i$ 列与 $W_2$ 的第 $j$ 列之间的差异。\n\nC. 将 $W_1$ 和 $W_2$ 的列按其 $\\ell_2$ 范数排序，并按此排序顺序进行匹配，只要列是不同的，就总能恢复正确的置换。\n\nD. 如果 $W_1$ 和 $W_2$ 的每一列都具有单位 $\\ell_2$ 范数，那么在 $\\{1, \\dots, k\\}$ 的所有置换 $\\pi$ 上最大化 $\\sum_{i=1}^k \\langle w^{(1)}_i, w^{(2)}_{\\pi(i)} \\rangle$ 等价于在置换矩阵 $P$ 上最小化 $\\lVert W_1 - W_2 P \\rVert_F^2$，其中 $w^{(r)}_i$ 表示 $W_r$ 的第 $i$ 列。\n\nE. 对 $W$ 的列进行置换，同时对 $H$ 的行进行置换，会使 $W H^\\top$ 保持不变。\n\n哪些陈述是正确的？",
            "solution": "问题陈述已经过验证，被认为是科学上可靠、定义良好且客观的。它提出了统计学习中关于潜因子模型可识别性的一个标准情景。我们逐一分析每个陈述。\n\n设数据矩阵为 $X \\in \\mathbb{R}^{n \\times m}$，通过因子矩阵 $W \\in \\mathbb{R}^{n \\times k}$ 和 $H \\in \\mathbb{R}^{m \\times k}$ 近似为 $X \\approx W H^\\top$。$W$ 的列表示为 $\\{w_i\\}_{i=1}^k$（其中 $w_i \\in \\mathbb{R}^n$），$H$ 的列表示为 $\\{h_i\\}_{i=1}^k$（其中 $h_i \\in \\mathbb{R}^m$）。该分解可以写成外积之和：$W H^\\top = \\sum_{i=1}^k w_i h_i^\\top$。一个置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$ 是一个正交矩阵，意味着它的列（和行）构成一个标准正交基。因此，$P^\\top P = P P^\\top = I_k$，其中 $I_k$ 是 $k \\times k$ 的单位矩阵。\n\n**陈述 A 的分析**\n\n该陈述断言对于任何置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$，都有 $(W P)(H P)^\\top = W H^\\top$。\n\n我们计算等式的左边。令 $W' = W P$ 和 $H' = H P$。我们关心的是乘积 $W' (H')^\\top$。\n使用矩阵乘积转置的性质 $(AB)^\\top = B^\\top A^\\top$，我们有：\n$$ (H P)^\\top = P^\\top H^\\top $$\n将此代入表达式中，得到：\n$$ (W P)(H P)^\\top = (W P)(P^\\top H^\\top) = W (P P^\\top) H^\\top $$\n如前所述，置换矩阵 $P$ 是一个正交矩阵，所以 $P P^\\top = I_k$。因此：\n$$ W (P P^\\top) H^\\top = W I_k H^\\top = W H^\\top $$\n左边与右边完全相同。这个恒等式展示了因子的置换模糊性：如果 $(W, H)$ 是一个有效的分解，那么 $(WP, HP)$ 是一个等价的分解，产生相同的近似，只是潜因子被置换了。\n\nA 的结论：**正确**。\n\n**陈述 B 的分析**\n\n该陈述提出了一种对齐来自两次不同运行的因子 $(W_1, H_1)$ 和 $(W_2, H_2)$ 的方法。该方法涉及寻找一个置换矩阵 $P$ 来最小化差异的弗罗贝尼乌斯范数 $\\lVert W_1 - W_2 P \\rVert_F^2$。该陈述声称这等价于一个线性分配问题。\n\n设 $w_i^{(1)}$ 是 $W_1$ 的第 $i$ 列，$w_j^{(2)}$ 是 $W_2$ 的第 $j$ 列。用一个置换矩阵 $P$ 右乘 $W_2$ 会置换 $W_2$ 的列。设作用于索引 $\\{1, \\dots, k\\}$ 上的置换为 $\\pi$。对于一个与 $\\pi$ 对应的合适的置换矩阵 $P$，置换后的矩阵 $W_2 P$ 的第 $i$ 列为 $w_{\\pi(i)}^{(2)}$。目标是找到能将 $W_1$ 的列与 $W_2$ 的列对齐的置换 $\\pi$。\n\n弗罗贝尼乌斯范数的平方是矩阵各列的 $\\ell_2$ 范数平方之和。我们可以将目标函数写为：\n$$ \\lVert W_1 - W_2 P \\rVert_F^2 = \\sum_{i=1}^k \\left\\lVert w_i^{(1)} - (W_2 P)_i \\right\\rVert_2^2 = \\sum_{i=1}^k \\left\\lVert w_i^{(1)} - w_{\\pi(i)}^{(2)} \\right\\rVert_2^2 $$\n我们寻求在所有可能的置换 $\\pi$ 上最小化这个和。让我们定义一个成本矩阵 $C \\in \\mathbb{R}^{k \\times k}$，其中条目 $C_{ij}$ 是将 $W_1$ 的第 $i$ 列与 $W_2$ 的第 $j$ 列匹配的成本：\n$$ C_{ij} = \\left\\lVert w_i^{(1)} - w_j^{(2)} \\right\\rVert_2^2 $$\n对于给定的置换 $\\pi$，总成本为 $\\sum_{i=1}^k C_{i, \\pi(i)}$。在所有置换 $\\pi$ 上最小化这个总成本是线性分配问题（或二分图中的最小权完美匹配）的正式定义。\n\n因此，最小化 $\\lVert W_1 - W_2 P \\rVert_F^2$ 确实等价于解决一个以 $C$ 作为成本矩阵的线性分配问题，其中每个条目度量了 $W_1$ 的一列和 $W_2$ 的一列之间的差异（平方欧几里得距离）。\n\nB 的结论：**正确**。\n\n**陈述 C 的分析**\n\n这个陈述提出了一种解决对齐问题的启发式方法：将 $W_1$ 和 $W_2$ 的列按其 $\\ell_2$ 范数排序，并按此排序顺序进行匹配。该陈述声称这将“总是”恢复正确的置换。“正确”的置换是解决 B 中描述的分配问题的那个。我们可以通过构造一个反例来检验这个说法。\n\n设 $k=2$ 且维度 $n$ 至少为 $2$。考虑以下因子矩阵：\n$$ W_1 = \\begin{bmatrix} w_1^{(1)}  w_2^{(1)} \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  2 \\\\ \\vdots  \\vdots \\end{bmatrix} $$\n$$ W_2 = \\begin{bmatrix} w_1^{(2)}  w_2^{(2)} \\end{bmatrix} = \\begin{bmatrix} 1.5  0 \\\\ 0  1.4 \\\\ \\vdots  \\vdots \\end{bmatrix} $$\n（假设其余条目为零）。这些列是不同的。\n$\\ell_2$ 范数是：\n$$ \\lVert w_1^{(1)} \\rVert_2 = 1, \\quad \\lVert w_2^{(1)} \\rVert_2 = 2 $$\n$$ \\lVert w_1^{(2)} \\rVert_2 = 1.5, \\quad \\lVert w_2^{(2)} \\rVert_2 = 1.4 $$\n排序启发式方法的步骤如下：\n- 按范数排序的 $W_1$ 的列是 $(w_1^{(1)}, w_2^{(1)})$。\n- 按范数排序的 $W_2$ 的列是 $(w_2^{(2)}, w_1^{(2)})$。\n该启发式方法将 $w_1^{(1)}$ 与 $w_2^{(2)}$ 匹配，将 $w_2^{(1)}$ 与 $w_1^{(2)}$ 匹配。这对应于置换 $\\pi(1)=2, \\pi(2)=1$。成本是：\n$$ \\text{Cost}_{\\text{heuristic}} = \\lVert w_1^{(1)} - w_2^{(2)} \\rVert_2^2 + \\lVert w_2^{(1)} - w_1^{(2)} \\rVert_2^2 = \\left\\lVert \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1.4 \\end{pmatrix} \\right\\rVert_2^2 + \\left\\lVert \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix} \\right\\rVert_2^2 $$\n$$ = (1^2 + (-1.4)^2) + ((-1.5)^2 + 2^2) = (1 + 1.96) + (2.25 + 4) = 2.96 + 6.25 = 9.21 $$\n现在，让我们计算恒等置换 $\\pi(1)=1, \\pi(2)=2$ 的成本：\n$$ \\text{Cost}_{\\text{identity}} = \\lVert w_1^{(1)} - w_1^{(2)} \\rVert_2^2 + \\lVert w_2^{(1)} - w_2^{(2)} \\rVert_2^2 = \\left\\lVert \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix} \\right\\rVert_2^2 + \\left\\lVert \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1.4 \\end{pmatrix} \\right\\rVert_2^2 $$\n$$ = (-0.5)^2 + (0.6)^2 = 0.25 + 0.36 = 0.61 $$\n由于 $\\text{Cost}_{\\text{identity}}  \\text{Cost}_{\\text{heuristic}}$，最优（正确）的置换是恒等置换。排序启发式方法未能找到它。“总是”这个词使得该陈述为假。\n\nC 的结论：**不正确**。\n\n**陈述 D 的分析**\n\n这个陈述声称，在 $W_1$ 和 $W_2$ 中所有因子列都具有单位 $\\ell_2$ 范数的条件下，两个优化问题是等价的。\n第一个问题是在置换矩阵 $P$ 上最小化 $\\lVert W_1 - W_2 P \\rVert_F^2$。如 B 的分析所示，这等价于在置换 $\\pi$ 上最小化 $\\sum_{i=1}^k \\lVert w_i^{(1)} - w_{\\pi(i)}^{(2)} \\rVert_2^2$。\n让我们展开范数的平方项：\n$$ \\lVert w_i^{(1)} - w_{\\pi(i)}^{(2)} \\rVert_2^2 = \\langle w_i^{(1)} - w_{\\pi(i)}^{(2)}, w_i^{(1)} - w_{\\pi(i)}^{(2)} \\rangle $$\n$$ = \\langle w_i^{(1)}, w_i^{(1)} \\rangle - 2 \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle + \\langle w_{\\pi(i)}^{(2)}, w_{\\pi(i)}^{(2)} \\rangle $$\n$$ = \\lVert w_i^{(1)} \\rVert_2^2 + \\lVert w_{\\pi(i)}^{(2)} \\rVert_2^2 - 2 \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle $$\n我们希望最小化的总和是：\n$$ \\sum_{i=1}^k \\left( \\lVert w_i^{(1)} \\rVert_2^2 + \\lVert w_{\\pi(i)}^{(2)} \\rVert_2^2 - 2 \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle \\right) $$\n给定条件 $\\lVert w_i^{(1)} \\rVert_2 = 1$ 和 $\\lVert w_j^{(2)} \\rVert_2 = 1$ 对所有 $i, j$ 成立：\n$$ \\sum_{i=1}^k \\left( 1^2 + 1^2 - 2 \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle \\right) = \\sum_{i=1}^k \\left( 2 - 2 \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle \\right) $$\n$$ = 2k - 2 \\sum_{i=1}^k \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle $$\n在 $\\pi$ 上最小化此表达式等价于最大化依赖于 $\\pi$ 的项，即 $\\sum_{i=1}^k \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle$。\n这与陈述中描述的第二个问题相匹配。等价性成立。\n\nD 的结论：**正确**。\n\n**陈述 E 的分析**\n\n这个陈述声称“对 $W$ 的列进行置换，同时对 $H$ 的行进行置换，会使 $W H^\\top$ 保持不变。”\n\n让我们将这些操作形式化。\n“对 $W$ 的列进行置换”：此操作对应于右乘一个置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$。新矩阵是 $W' = W P$。\n“对 $H$ 的行进行置换”：此操作对应于左乘一个置换矩阵 $Q \\in \\mathbb{R}^{m \\times m}$。新矩阵是 $H' = Q H$。\n\n现在，让我们计算新的乘积矩阵 $(W')(H')^\\top$：\n$$ (W P)(Q H)^\\top = (W P)(H^\\top Q^\\top) = W P H^\\top Q^\\top $$\n该陈述声称这等于 $W H^\\top$。要使其普遍成立，我们需要 $W P H^\\top Q^\\top = W H^\\top$。这不是一个恒等式。对于一个非零分解，这将意味着 $P H^\\top Q^\\top = H^\\top$，这对于任意的 $H$、$P$ 和 $Q$ 并不成立。\n正如 A 的分析所示，该模型的基本置换不变性是 $(WP)(HP)^\\top = WH^\\top$。这涉及到用相同的置换来置换 $W$ 的列和 $H$ 的列。该陈述错误地提到了置换 $H$ 的行。按其字面意思，该陈述在数学上是错误的。\n\nE 的结论：**不正确**。\n\n**总结**\n- 陈述 A 是正确的。\n- 陈述 B 是正确的。\n- 陈述 C 是不正确的。\n- 陈述 D 是正确的。\n- 陈述 E 是不正确的。\n\n正确的陈述是 A、B 和 D。",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "从理论走向应用，这个动手编程练习解决了因子分析中两个最实际的挑战：选择最优的潜因子数量（$k$）和为提高可解释性选择合适的因子旋转。您将通过自助法（bootstrap）重采样来实现一个稳健的、数据驱动的流程，以评估因子解的稳定性。本练习将因子估计、旋转和对齐等概念综合成一个完整的流程，用于构建和验证一个可靠的潜因子模型。",
            "id": "3137686",
            "problem": "考虑一个由带加性噪声的潜因子模型建模的矩形数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$。潜因子模型假设存在一个得分矩阵 $Z \\in \\mathbb{R}^{n \\times k}$ 和一个载荷矩阵 $W \\in \\mathbb{R}^{p \\times k}$，使得 $X \\approx Z W^{\\top} + E$，其中 $E$ 是一个噪声矩阵。因子维度 $k$ 是未知的，并且由于旋转模糊性，在没有约束的情况下因子解是不可识别的：对于任何满足 $R^{\\top} R = I_k$ 的正交矩阵 $R \\in \\mathbb{R}^{k \\times k}$，对 $(Z R, W R)$ 都会产生相同的近似 $Z W^{\\top}$。\n\n目标是检验在 bootstrap 重抽样下因子的稳定性，以选择潜因子维度 $k$ 和旋转类型。考虑两种旋转类型：不旋转和 varimax 旋转。对于每个候选的 $k$，您将对 $X$ 的行和列进行 bootstrap 重抽样，在重抽样数据上重新拟合因子，将重抽样得到的载荷与在完整数据上计算的参考解对齐，并计算一个稳定性得分。然后，使用一个原则性规则，利用跨 bootstrap 样本的稳定性来选择 $k$ 和旋转类型。\n\n您的程序必须实现以下基于基本定义的组件：\n\n- 数据标准化：给定 $X$，通过将每列中心化至均值为零并缩放至单位方差来构建 $\\tilde{X}$，即对于每个特征索引 $j \\in \\{1,\\dots,p\\}$，设置 $\\tilde{X}_{:,j} = (X_{:,j} - \\mu_j)/\\sigma_j$，其中 $\\mu_j$ 是第 $j$ 列的均值，$\\sigma_j$ 是其标准差。如果某列的方差为零，则保持不变以避免除以零。\n\n- 通过截断奇异值分解 (SVD) 进行因子估计：对于给定的 $k$，通过 SVD 计算 $\\tilde{X}$ 的秩为 $k$ 的近似，$\\tilde{X} \\approx U_k \\Sigma_k V_k^{\\top}$，其中 $U_k \\in \\mathbb{R}^{n \\times k}$，$\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ 是具有非负奇异值的对角矩阵，而 $V_k \\in \\mathbb{R}^{p \\times k}$。将 $k$ 个因子的载荷矩阵定义为 $L_k = V_k \\Sigma_k$。对于标准化数据，此选择与主成分分析 (PCA) 的载荷是一致的。根据指定的旋转类型对 $L_k$ 应用旋转：可以是不旋转（单位旋转），也可以是正交 varimax 旋转，后者旨在通过在正交性约束下最大化每个因子上载荷平方的方差，使 $L_k$ 的列更具可解释性。\n\n- Bootstrap 重抽样：对于固定的 $k$ 和选定的旋转类型，执行 $B$ 次 bootstrap 复制。每次复制 $b \\in \\{1, \\dots, B\\}$ 通过有放回地独立重抽样 $\\tilde{X}$ 的 $n$ 行和 $p$ 列，形成 $\\tilde{X}^{(b)} \\in \\mathbb{R}^{n \\times p}$。使用指定的旋转类型，在 $\\tilde{X}^{(b)}$ 上拟合秩为 $k$ 的因子载荷 $L_k^{(b)}$。\n\n- 通过对齐的绝对相关性进行稳定性评分：对于每次 bootstrap 复制 $b$，通过解决一个匹配问题，将 $L_k^{(b)}$ 与全数据参考载荷 $L_k^{\\text{ref}}$（在 $\\tilde{X}$ 上使用相同的 $k$ 和旋转类型计算）对齐，该问题将 $L_k^{(b)}$ 的 $k$ 列与 $L_k^{\\text{ref}}$ 的 $k$ 列配对。具体来说，对于每对因子索引 $(j, \\ell)$，计算 $L_k^{(b)}$ 中第 $j$ 个 bootstrap 载荷向量与 $L_k^{\\text{ref}}$ 中第 $\\ell$ 个参考载荷向量之间的绝对 Pearson 相关性，该计算仅限于复制 $b$ 的重抽样列索引，如果列被多次重抽样，则包括重复项。将此值表示为 $c_{j\\ell}^{(b)} \\in [0,1]$。构建一个 $k \\times k$ 矩阵 $C^{(b)}$，其元素为 $c_{j\\ell}^{(b)}$。找到一个 $\\{1,\\dots,k\\}$ 的排列 $\\pi^{(b)}$，使得 $\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$ 最大化（等价于用成本 $-C^{(b)}$ 解决一个线性分配问题）。将复制 $b$ 的稳定性得分定义为匹配的绝对相关性的平均值，$s^{(b)} = \\frac{1}{k}\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$。\n\n- 聚合与选择：对于每个候选的 $k$ 和旋转类型，计算平均稳定性 $\\bar{s}_{k,r} = \\frac{1}{B}\\sum_{b=1}^{B} s^{(b)}$ 和均值标准误 $\\text{se}_{k,r} = \\sqrt{\\frac{1}{B(B-1)}\\sum_{b=1}^{B} (s^{(b)} - \\bar{s}_{k,r})^2}$。对于每个 $k$，选择使 $\\bar{s}_{k,r}$ 最大化的旋转类型 $r_k$。然后使用单标准误规则选择潜因子维度 $k^{\\star}$：令 $\\bar{s}_{\\max} = \\max_k \\bar{s}_{k, r_k}$，并令 $k_{\\max}$ 为实现此最大值的 $k$，其对应的标准误为 $\\text{se}_{k_{\\max}, r_{k_{\\max}}}$。选择 $k^{\\star}$ 为满足 $\\bar{s}_{k, r_k} \\ge \\bar{s}_{\\max} - \\text{se}_{k_{\\max}, r_{k_{\\max}}}$ 的最小 $k$。最后，选择旋转类型 $r^{\\star} = r_{k^{\\star}}$。\n\n实现要求：\n\n- 仅使用奇异值分解和正交 varimax 旋转进行因子估计，如上所述。使用绝对 Pearson 相关性进行对齐。通过匈牙利方法进行最优分配。\n\n- 每个 $(k, \\text{rotation})$ 设置使用 $B$ 次 bootstrap 复制。\n\n- 候选的潜因子维度是集合 $\\{1,2,3,4,5\\}$ 中的整数。\n\n- 旋转类型为 \"none\" 和 \"varimax\"。在输出中，将旋转类型编码为整数：$0$ 表示 \"none\"，$1$ 表示 \"varimax\"。\n\n测试套件规范：\n\n为可复现性，使用固定的随机种子构建三个合成测试用例。对于每个用例，根据 $X = Z W^{\\top} + \\sigma \\cdot N$ 生成 $X$，其中 $Z$ 和 $W$ 的规定如下，$N$ 具有独立同分布的标准正态项。生成后，在拟合因子之前对列进行标准化以获得 $\\tilde{X}$。\n\n- 测试用例 1：设 $n = 120$，$p = 20$，$k_{\\text{true}} = 3$，$\\sigma = 0.5$。构建一个具有块结构的 $W \\in \\mathbb{R}^{20 \\times 3}$：对于特征索引 1 到 7，设置因子 1 上的强载荷；对于索引 8 到 14，设置因子 2 上的强载荷；对于索引 15 到 20，设置因子 3 上的强载荷。向 $W$ 中添加少量高斯噪声。从独立同分布的标准正态分布中抽取 $Z \\in \\mathbb{R}^{120 \\times 3}$。\n\n- 测试用例 2：设 $n = 100$，$p = 15$，$k_{\\text{true}} = 1$，$\\sigma = 0.8$。构建 $W \\in \\mathbb{R}^{15 \\times 1}$，其随机项在一个子集上被放大，以偏向一个主导因子。从独立同分布的标准正态分布中抽取 $Z \\in \\mathbb{R}^{100 \\times 1}$。\n\n- 测试用例 3：设 $n = 100$，$p = 18$，并通过将 $Z W^{\\top}$ 设置为零矩阵来生成纯噪声，$\\sigma = 1.0$。即，$X = \\sigma \\cdot N$，其中 $N$ 具有独立同分布的标准正态项。\n\n对于每个测试用例，设置一个固定的随机种子以确保可复现性。\n\n您的程序必须：\n\n- 实现上述过程，为所有 $k \\in \\{1,2,3,4,5\\}$ 和两种旋转类型计算 $\\bar{s}_{k,r}$ 和 $\\text{se}_{k,r}$。\n\n- 根据所描述的单标准误规则和旋转选择来选择 $k^{\\star}$ 和 $r^{\\star}$。\n\n- 对于每个测试用例，返回对 $[k^{\\star}, \\text{rotation\\_code}]$，其中 $\\text{rotation\\_code} \\in \\{0,1\\}$ 对旋转类型进行编码。\n\n最终输出格式：\n\n您的程序应生成一行输出，其中包含三个测试用例的结果，形式为用方括号括起来的逗号分隔列表，其中每个元素是相应测试用例的列表 $[k^{\\star}, \\text{rotation\\_code}]$，按测试用例 1、2 和 3 的顺序排列。例如，格式必须与 $[[k_1,r_1],[k_2,r_2],[k_3,r_3]]$ 完全一样。\n\n将 bootstrap 复制次数设置为 $B = 40$，并仅使用指定的整数输出。此问题不涉及物理单位或角度。所有数值输出必须是整数。",
            "solution": "该问题要求实现一个全面的统计程序，为一个因子模型选择潜因子数量 $k$ 和合适的旋转类型。在心理计量学、金融学和生物信息学等领域，这是一项常见且重要的任务，在这些领域中，潜在的未观测结构是从高维数据中推断出来的。该选择基于 bootstrap 重抽样下的模型稳定性原则。一个在数据的不同扰动版本中始终揭示相同因子结构的模型被认为是更可靠的。\n\n整个过程可以分解为以下逻辑步骤，它们构成了计算解决方案的基础：\n\n1.  **模型设定与数据准备**：\n    数据由一个矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 给出，其中有 $n$ 个观测值和 $p$ 个特征。潜因子模型假定 $X \\approx ZW^{\\top} + E$，其中 $Z \\in \\mathbb{R}^{n \\times k}$ 是因子得分，$W \\in \\mathbb{R}^{p \\times k}$ 是因子载荷，对于未知数量的因子 $k$。在分析之前，数据矩阵 $X$ 的每一列都被标准化为均值为 $0$，标准差为 $1$，得到 $\\tilde{X}$。这确保了所有特征对所解释的方差贡献均等。方差为零的列保持不变。对于标准化的数据矩阵，主成分分析 (PCA) 和因子分析密切相关，我们可以使用奇异值分解 (SVD) 进行因子估计。\n\n2.  **通过 SVD 进行因子估计**：\n    对于给定的因子数量 $k$，我们使用截断 SVD 计算标准化数据矩阵 $\\tilde{X}$ 的秩为 $k$ 的近似：$\\tilde{X} \\approx U_k \\Sigma_k V_k^{\\top}$。在这里，$U_k \\in \\mathbb{R}^{n \\times k}$ 和 $V_k \\in \\mathbb{R}^{p \\times k}$ 具有正交规范列，而 $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ 是前 $k$ 个奇异值的对角矩阵。遵循 PCA 中的一个常见惯例，因子载荷矩阵 $L_k \\in \\mathbb{R}^{p \\times k}$ 定义为 $L_k = V_k \\Sigma_k$。$L_k$ 的每一列代表一个因子，其元素代表原始 $p$ 个特征在该因子上的权重。\n\n3.  **因子旋转**：\n    解 $Z W^{\\top}$ 对正交旋转是不变的，这意味着对于任何正交矩阵 $R$，$(ZR)(WR)^{\\top} = ZW^{\\top}$。这种旋转模糊性意味着通过 SVD 估计的因子不是唯一的，可能不易解释。为了解决这个问题，采用了因子旋转技术。该问题考虑了两种选择：\n    *   **不旋转**：按原样使用 SVD 导出的载荷 $L_k$。这些是主成分，它们是正交的，并按其解释的方差量排序。\n    *   **Varimax 旋转**：这是一种正交旋转方法，旨在简化因子结构。它寻找一个旋转矩阵 $T$，使得旋转后的载荷 $L_k^{\\text{rot}} = L_k T$ 具有最大的“简单结构”。这是通过最大化 varimax 准则来实现的，该准则是每个因子载荷平方的方差之和。一个高的值表明，每个原始特征倾向于在少数因子上有高载荷，而在其他因子上的载荷接近于零，从而增强可解释性。\n\n4.  **Bootstrap 稳定性评估**：\n    选择程序的核心是评估当数据轻微扰动时，估计的因子结构的稳定性。这是通过 bootstrap 重抽样来完成的。对于每个候选的 $k$ 和每种旋转类型，重复以下过程 $B$ 次：\n    *   **重抽样**：通过从标准化的完整数据集 $\\tilde{X}$ 中有放回地重抽样行（观测值）和列（特征），创建一个 bootstrap 数据集 $\\tilde{X}^{(b)}$。\n    *   **重新估计**：将一个具有 $k$ 个因子的因子模型拟合到 bootstrap 数据集 $\\tilde{X}^{(b)}$ 上，产生一组新的载荷 $L_k^{(b)}$（与参考模型具有相同的旋转类型）。请注意，由于 $\\tilde{X}^{(b)}$ 的列是从一个已经标准化的矩阵中重抽样的，它们的均值和方差通常不会是 $0$ 和 $1$，因此需要对 $\\tilde{X}^{(b)}$ 重新应用标准化。\n    *   **对齐与评分**：bootstrap 解中的因子 $L_k^{(b)}$ 必须与参考解中的因子 $L_k^{\\text{ref}}$（在完整数据集 $\\tilde{X}$ 上计算）对齐。SVD 算法按奇异值对因子进行排序，这种排序在 bootstrap 样本中可能会改变。为了解决这个问题，需要解决一个匹配问题。我们构建一个 $k \\times k$ 矩阵 $C^{(b)}$，其中条目 $c_{j\\ell}^{(b)}$ 是 $L_k^{(b)}$ 的第 $j$ 个因子与 $L_k^{\\text{ref}}$ 的第 $\\ell$ 个因子之间的绝对 Pearson 相关性。该相关性是在 bootstrap 载荷向量和限制在重抽样特征索引上的参考载荷向量之间计算的。通过识别最大化匹配对相关性总和 $\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$ 的因子排列 $\\pi^{(b)}$ 来找到最优匹配。这是一个线性分配问题，可以通过匈牙利方法有效解决。该复制的稳定性得分 $s^{(b)}$ 是这些最大匹配相关性的平均值：$s^{(b)} = \\frac{1}{k}\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$。\n\n5.  **聚合与模型选择**：\n    经过 $B$ 次 bootstrap 复制后，我们为每对 $(k, \\text{旋转类型})$ 获得 $B$ 个稳定性得分。\n    *   **聚合**：对于每个 $(k, r)$，我们计算平均稳定性得分 $\\bar{s}_{k,r}$ 及其标准误 $\\text{se}_{k,r}$。\n    *   **选择规则**：最终模型通过一个两步过程选择。首先，对于每个候选维度 $k$，我们选择产生最高平均稳定性 $\\bar{s}_{k,r}$ 的旋转类型 $r_k$。这为每个 $k$ 提供了一个最佳稳定性得分序列。其次，我们对这个序列应用“单标准误规则”来选择最终维度 $k^{\\star}$。该规则在模型性能（稳定性）与简约性之间取得平衡。它选择稳定性不显著差于最稳定模型的最简单模型（最小的 $k$）。具体来说，如果 $k_{\\max}$ 是具有绝对最高稳定性 $\\bar{s}_{\\max}$ 的维度，我们选择 $k^{\\star}$ 作为其稳定性 $\\bar{s}_{k, r_k}$ 在最大值的一个标准误范围内的最小 $k$：$\\bar{s}_{k, r_k} \\ge \\bar{s}_{\\max} - \\text{se}_{k_{\\max}, r_{k_{\\max}}}$。最终的旋转类型 $r^{\\star}$ 则是在第一步中为 $k^{\\star}$ 选择的类型，即 $r^{\\star} = r_{k^{\\star}}$。\n\n这种系统化的、数据驱动的方法论为选择潜因子模型的关键超参数提供了一个稳健的框架，避免了武断的决定，并将选择建立在推断结构的经验稳定性之上。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\nfrom scipy.optimize import linear_sum_assignment\n\ndef generate_test_data(case_id, seed):\n    \"\"\"Generates synthetic data for the three test cases.\"\"\"\n    rng = np.random.default_rng(seed)\n    if case_id == 1:\n        n, p, k_true, sigma = 120, 20, 3, 0.5\n        W = np.zeros((p, k_true))\n        W[0:7, 0] = 1.0\n        W[7:14, 1] = 1.0\n        W[14:20, 2] = 1.0\n        W += rng.normal(0, 0.1, size=W.shape)\n        Z = rng.normal(size=(n, k_true))\n        X = Z @ W.T + sigma * rng.normal(size=(n, p))\n    elif case_id == 2:\n        n, p, k_true, sigma = 100, 15, 1, 0.8\n        W = rng.normal(size=(p, k_true))\n        amplified_indices = rng.choice(p, size=int(p / 2), replace=False)\n        W[amplified_indices, :] *= 3.0\n        Z = rng.normal(size=(n, k_true))\n        X = Z @ W.T + sigma * rng.normal(size=(n, p))\n    elif case_id == 3:\n        n, p, sigma = 100, 18, 1.0\n        X = sigma * rng.normal(size=(n, p))\n    else:\n        raise ValueError(\"Invalid case_id\")\n    return X\n\ndef standardize_data(X):\n    \"\"\"Standardizes columns of X to mean 0 and variance 1.\"\"\"\n    X_std = X.copy()\n    mu = np.mean(X_std, axis=0)\n    sigma = np.std(X_std, axis=0)\n    non_zero_std_mask = sigma > 1e-10\n    X_std[:, non_zero_std_mask] = (X_std[:, non_zero_std_mask] - mu[non_zero_std_mask]) / sigma[non_zero_std_mask]\n    return X_std\n\ndef varimax(L, tol=1e-8, max_iter=100):\n    \"\"\"Orthogonal varimax rotation using SVD.\"\"\"\n    p, k = L.shape\n    if k  2:\n        return L\n    \n    h = np.sqrt(np.sum(L**2, axis=1, keepdims=True))\n    h[h == 0] = 1.0\n    L_norm = L / h\n    \n    T = np.eye(k)\n    \n    for _ in range(max_iter):\n        L_rot = L_norm @ T\n        g = np.sum(L_rot**2, axis=0) / p\n        B = (1/p) * L_rot.T @ (L_rot**3 - L_rot @ np.diag(g))\n        \n        try:\n            U, _, Vt = svd(B)\n            R = U @ Vt\n        except np.linalg.LinAlgError:\n            break\n        \n        if np.sum((R - np.eye(k))**2)  tol:\n            break\n            \n        T = T @ R\n    \n    L_rotated_normalized = L_norm @ T\n    L_final = L_rotated_normalized * h\n    return L_final\n\ndef pearson_correlation(x, y):\n    \"\"\"Computes the Pearson correlation coefficient, handling zero variance.\"\"\"\n    mean_x, mean_y = np.mean(x), np.mean(y)\n    std_x, std_y = np.std(x), np.std(y)\n    \n    if std_x == 0 or std_y == 0:\n        return 0.0\n    \n    cov = np.mean((x - mean_x) * (y - mean_y))\n    return cov / (std_x * std_y)\n\ndef get_stability_scores(X, k, rotation_type, B, rng):\n    \"\"\"Computes stability scores for a given k and rotation type.\"\"\"\n    X_std = standardize_data(X)\n    n, p = X_std.shape\n    \n    try:\n        U, s, Vt = svd(X_std, full_matrices=False)\n        L_ref_unrotated = Vt[:k, :].T @ np.diag(s[:k])\n    except np.linalg.LinAlgError:\n        return np.array([])\n    \n    if rotation_type == 'varimax':\n        L_ref = varimax(L_ref_unrotated)\n    else:\n        L_ref = L_ref_unrotated\n        \n    scores = []\n    for _ in range(B):\n        row_indices = rng.choice(n, n, replace=True)\n        col_indices = rng.choice(p, p, replace=True)\n        \n        X_boot_raw = X_std[row_indices, :][:, col_indices]\n        X_boot_std = standardize_data(X_boot_raw)\n        \n        try:\n            U_b, s_b, Vt_b = svd(X_boot_std, full_matrices=False)\n            \n            num_sv = len(s_b)\n            if num_sv  k:\n                s_b_padded = np.zeros(k)\n                s_b_padded[:num_sv] = s_b\n                s_b = s_b_padded\n                Vt_b_padded = np.zeros((k, p))\n                Vt_b_padded[:Vt_b.shape[0], :] = Vt_b[:num_sv, :]\n                Vt_b = Vt_b_padded\n\n            L_boot_unrotated = Vt_b[:k, :].T @ np.diag(s_b[:k])\n            \n            if rotation_type == 'varimax':\n                L_boot = varimax(L_boot_unrotated)\n            else:\n                L_boot = L_boot_unrotated\n        except np.linalg.LinAlgError:\n            continue\n            \n        cost_matrix = np.zeros((k, k))\n        for j in range(k):\n            for l in range(k):\n                vec_boot = L_boot[:, j]\n                vec_ref_resampled = L_ref[col_indices, l]\n                corr = pearson_correlation(vec_boot, vec_ref_resampled)\n                cost_matrix[j, l] = np.abs(corr)\n\n        row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n        \n        max_corrs = cost_matrix[row_ind, col_ind]\n        score = np.mean(max_corrs) if k > 0 else 1.0\n        scores.append(score)\n        \n    return np.array(scores)\n\ndef solve():\n    test_specs = [\n        {'case_id': 1, 'seed': 1},\n        {'case_id': 2, 'seed': 2},\n        {'case_id': 3, 'seed': 3}\n    ]\n    \n    B = 40\n    candidate_ks = [1, 2, 3, 4, 5]\n    rotation_types = ['none', 'varimax']\n    final_results = []\n\n    for spec in test_specs:\n        X = generate_test_data(spec['case_id'], spec['seed'])\n        rng = np.random.default_rng(spec['seed'])\n        \n        stability_means = np.zeros((len(candidate_ks), len(rotation_types)))\n        stability_ses = np.zeros((len(candidate_ks), len(rotation_types)))\n\n        for i, k in enumerate(candidate_ks):\n            for j, r_type in enumerate(rotation_types):\n                scores = get_stability_scores(X, k, r_type, B, rng)\n                if len(scores) > 1:\n                    stability_means[i, j] = np.mean(scores)\n                    stability_ses[i, j] = np.std(scores, ddof=1) / np.sqrt(len(scores))\n                else: \n                    stability_means[i, j] = 0\n                    stability_ses[i, j] = 0\n\n        best_rotation_indices = np.argmax(stability_means, axis=1)\n        best_stabilities = np.array([stability_means[i, best_rotation_indices[i]] for i in range(len(candidate_ks))])\n        best_ses = np.array([stability_ses[i, best_rotation_indices[i]] for i in range(len(candidate_ks))])\n        \n        if np.all(best_stabilities == 0):\n            k_star = 1\n            r_star_code = 0\n        else:\n            k_max_idx = np.argmax(best_stabilities)\n            s_max = best_stabilities[k_max_idx]\n            se_at_max = best_ses[k_max_idx]\n            \n            threshold = s_max - se_at_max\n            \n            eligible_k_indices = np.where(best_stabilities >= threshold)[0]\n            k_star_idx = eligible_k_indices[0] if len(eligible_k_indices) > 0 else 0\n            \n            k_star = candidate_ks[k_star_idx]\n            r_star_code = best_rotation_indices[k_star_idx]\n\n        final_results.append([k_star, int(r_star_code)])\n        \n    print(f\"[{','.join(map(str, final_results))}]\".replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}