{
    "hands_on_practices": [
        {
            "introduction": "在解释潜因子模型的结果时，一个核心挑战是其固有的模糊性——我们可以交换因子（及其对应的载荷）而得到一个在数学上等效的解。本练习将从第一性原理出发，引导您理解这种置换模糊性，并介绍一种原则性的方法——线性分配问题——来对齐不同分析得出的因子。这对于比较模型或评估稳定性至关重要 。",
            "id": "3137743",
            "problem": "在统计学习中，考虑一个隐因子模型，其中数据矩阵 $X \\in \\mathbb{R}^{n \\times m}$ 由一个秩为 $k$ 的分解 $X \\approx W H^\\top$ 近似，其中 $W \\in \\mathbb{R}^{n \\times k}$ 且 $H \\in \\mathbb{R}^{m \\times k}$。隐维度为 $k$，$W$ 和 $H$ 的列代表 $k$ 个隐因子。一个置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$ 是一个方的二元矩阵，其每行每列都恰好有一个元素为 $1$，其余元素均为 $0$。矩阵 $A$ 的弗罗贝尼乌斯范数 $\\lVert A \\rVert_F$ 定义为其所有元素平方和的平方根，向量 $a, b \\in \\mathbb{R}^d$ 之间的标准内积为 $\\langle a, b \\rangle = a^\\top b$。\n\n重复运行一个分解算法可能会产生因子对 $(W_1, H_1)$ 和 $(W_2, H_2)$，它们都能很好地拟合 $X$，但其列存在一个置换关系（以及其他小的数值差异）。为了从第一性原理出发，推断置换模糊性下的可识别性问题以及跨次运行对齐因子的问题，请评估以下陈述：\n\nA. 对于任意置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$，有 $(W P)(H P)^\\top = W H^\\top$。\n\nB. 一种在两次运行中匹配 $k$ 个因子的原则性方法是，选择一个 $P \\in \\mathbb{R}^{k \\times k}$ 来最小化 $\\lVert W_1 - W_2 P \\rVert_F^2$，这等价于在一个 $k \\times k$ 的成本矩阵上解决一个线性分配问题，该成本矩阵的 $(i, j)$ 项衡量了 $W_1$ 的第 $i$ 列与 $W_2$ 的第 $j$ 列之间的差异。\n\nC. 将 $W_1$ 和 $W_2$ 的列按其 $\\ell_2$ 范数排序，并根据这个排序顺序进行匹配，只要列是不同的，总能恢复正确的置换。\n\nD. 如果 $W_1$ 和 $W_2$ 的每一列都具有单位 $\\ell_2$ 范数，那么在 $\\{1, \\dots, k\\}$ 的所有置换 $\\pi$ 上最大化 $\\sum_{i=1}^k \\langle w^{(1)}_i, w^{(2)}_{\\pi(i)} \\rangle$ 等价于在置换矩阵 $P$ 上最小化 $\\lVert W_1 - W_2 P \\rVert_F^2$，其中 $w^{(r)}_i$ 表示 $W_r$ 的第 $i$ 列。\n\nE. 置换 $W$ 的列同时置换 $H$ 的行，会使 $W H^\\top$ 保持不变。\n\n哪些陈述是正确的？",
            "solution": "问题陈述已经过验证，被认为是科学上合理的、问题定义良好的和客观的。它提出了统计学习中关于隐因子模型可识别性的一个标准情景。我们接下来分析每个陈述。\n\n设数据矩阵为 $X \\in \\mathbb{R}^{n \\times m}$，由 $X \\approx W H^\\top$ 近似，其中因子矩阵为 $W \\in \\mathbb{R}^{n \\times k}$ 和 $H \\in \\mathbb{R}^{m \\times k}$。$W$ 的列表示为 $\\{w_i\\}_{i=1}^k$，其中 $w_i \\in \\mathbb{R}^n$，$H$ 的列表示为 $\\{h_i\\}_{i=1}^k$，其中 $h_i \\in \\mathbb{R}^m$。该分解可以写成外积之和：$W H^\\top = \\sum_{i=1}^k w_i h_i^\\top$。一个置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$ 是一个正交矩阵，意味着它的列（和行）构成一个标准正交基。因此，$P^\\top P = P P^\\top = I_k$，其中 $I_k$ 是 $k \\times k$ 的单位矩阵。\n\n**陈述 A 的分析**\n\n该陈述断言，对于任意置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$，有 $(W P)(H P)^\\top = W H^\\top$。\n\n我们计算等式的左侧。设 $W' = W P$ 和 $H' = H P$。我们关心乘积 $W' (H')^\\top$。\n使用矩阵乘积转置的性质，$(AB)^\\top = B^\\top A^\\top$，我们有：\n$$ (H P)^\\top = P^\\top H^\\top $$\n将此代入表达式中得到：\n$$ (W P)(H P)^\\top = (W P)(P^\\top H^\\top) = W (P P^\\top) H^\\top $$\n如前所述，置换矩阵 $P$ 是一个正交矩阵，所以 $P P^\\top = I_k$。因此：\n$$ W (P P^\\top) H^\\top = W I_k H^\\top = W H^\\top $$\n等式左侧与右侧相同。这个恒等式展示了因子的置换模糊性：如果 $(W, H)$ 是一个有效的分解，那么 $(WP, HP)$ 也是一个等价的分解，产生相同的近似，其中隐因子被置换了。\n\n对 A 的判断：**正确**。\n\n**陈述 B 的分析**\n\n该陈述提出了一种对齐来自两次不同运行的因子 $(W_1, H_1)$ 和 $(W_2, H_2)$ 的方法。该方法涉及找到一个置换矩阵 $P$ 来最小化差异的弗罗贝尼乌斯范数 $\\lVert W_1 - W_2 P \\rVert_F^2$。该陈述声称这等价于一个线性分配问题。\n\n设 $w_i^{(1)}$ 为 $W_1$ 的第 $i$ 列，$w_j^{(2)}$ 为 $W_2$ 的第 $j$ 列。用置换矩阵 $P$ 右乘 $W_2$ 会置换 $W_2$ 的列。设索引 $\\{1, \\dots, k\\}$ 上的置换为 $\\pi$。对于一个与 $\\pi$ 对应的合适的置换矩阵 $P$，置换后的矩阵 $W_2 P$ 的第 $i$ 列为 $w_{\\pi(i)}^{(2)}$。目标是找到能将 $W_1$ 的列与 $W_2$ 的列对齐的置换 $\\pi$。\n\n弗罗贝尼乌斯范数的平方是矩阵各列的 $\\ell_2$ 范数平方之和。我们可以将目标函数写为：\n$$ \\lVert W_1 - W_2 P \\rVert_F^2 = \\sum_{i=1}^k \\left\\lVert w_i^{(1)} - (W_2 P)_i \\right\\rVert_2^2 = \\sum_{i=1}^k \\left\\lVert w_i^{(1)} - w_{\\pi(i)}^{(2)} \\right\\rVert_2^2 $$\n我们寻求在所有可能的置换 $\\pi$ 上最小化这个和。让我们定义一个成本矩阵 $C \\in \\mathbb{R}^{k \\times k}$，其中条目 $C_{ij}$ 是将 $W_1$ 的第 $i$ 列与 $W_2$ 的第 $j$ 列匹配的成本：\n$$ C_{ij} = \\left\\lVert w_i^{(1)} - w_j^{(2)} \\right\\rVert_2^2 $$\n对于给定的置换 $\\pi$，总成本为 $\\sum_{i=1}^k C_{i, \\pi(i)}$。在所有置换 $\\pi$ 上最小化此总成本是线性分配问题（或二部图中的最小权重完美匹配）的正式定义。\n\n因此，最小化 $\\lVert W_1 - W_2 P \\rVert_F^2$ 确实等价于用如上定义的成本矩阵 $C$ 解决一个线性分配问题，其中每个条目衡量了 $W_1$ 的一列和 $W_2$ 的一列之间的差异（欧几里得距离的平方）。\n\n对 B 的判断：**正确**。\n\n**陈述 C 的分析**\n\n该陈述提出了一种解决对齐问题的启发式方法：根据 $\\ell_2$ 范数对 $W_1$ 和 $W_2$ 的列进行排序，并按照这个排序顺序进行匹配。该陈述声称这“总是”能恢复正确的置换。“正确”的置换是指解决 B 中描述的分配问题的置换。我们可以通过构造一个反例来检验这个说法。\n\n设 $k=2$ 且维度 $n$ 至少为 $2$。考虑以下因子矩阵：\n$$ W_1 = \\begin{bmatrix} w_1^{(1)}  w_2^{(1)} \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  2 \\\\ \\vdots  \\vdots \\end{bmatrix} $$\n$$ W_2 = \\begin{bmatrix} w_1^{(2)}  w_2^{(2)} \\end{bmatrix} = \\begin{bmatrix} 1.5  0 \\\\ 0  1.4 \\\\ \\vdots  \\vdots \\end{bmatrix} $$\n（假设其余条目为零）。这些列是不同的。\n$\\ell_2$ 范数是：\n$$ \\lVert w_1^{(1)} \\rVert_2 = 1, \\quad \\lVert w_2^{(1)} \\rVert_2 = 2 $$\n$$ \\lVert w_1^{(2)} \\rVert_2 = 1.5, \\quad \\lVert w_2^{(2)} \\rVert_2 = 1.4 $$\n排序启发式方法的步骤如下：\n- 按范数排序的 $W_1$ 的列是 $(w_1^{(1)}, w_2^{(1)})$。\n- 按范数排序的 $W_2$ 的列是 $(w_2^{(2)}, w_1^{(2)})$。\n该启发式方法将 $w_1^{(1)}$ 与 $w_2^{(2)}$ 匹配，将 $w_2^{(1)}$ 与 $w_1^{(2)}$ 匹配。这对应于置换 $\\pi(1)=2, \\pi(2)=1$。成本为：\n$$ \\text{Cost}_{\\text{heuristic}} = \\lVert w_1^{(1)} - w_2^{(2)} \\rVert_2^2 + \\lVert w_2^{(1)} - w_1^{(2)} \\rVert_2^2 = \\left\\lVert \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1.4 \\end{pmatrix} \\right\\rVert_2^2 + \\left\\lVert \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix} \\right\\rVert_2^2 $$\n$$ = (1^2 + (-1.4)^2) + ((-1.5)^2 + 2^2) = (1 + 1.96) + (2.25 + 4) = 2.96 + 6.25 = 9.21 $$\n现在，我们来计算单位置换 $\\pi(1)=1, \\pi(2)=2$ 的成本：\n$$ \\text{Cost}_{\\text{identity}} = \\lVert w_1^{(1)} - w_1^{(2)} \\rVert_2^2 + \\lVert w_2^{(1)} - w_2^{(2)} \\rVert_2^2 = \\left\\lVert \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix} \\right\\rVert_2^2 + \\left\\lVert \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1.4 \\end{pmatrix} \\right\\rVert_2^2 $$\n$$ = (-0.5)^2 + (0.6)^2 = 0.25 + 0.36 = 0.61 $$\n由于 $\\text{Cost}_{\\text{identity}}  \\text{Cost}_{\\text{heuristic}}$，最优（正确）的置换是单位置换。排序启发式方法未能找到它。“总是”这个词使得该陈述为假。\n\n对 C 的判断：**不正确**。\n\n**陈述 D 的分析**\n\n该陈述声称，在 $W_1$ 和 $W_2$ 的所有因子列都具有单位 $\\ell_2$ 范数的条件下，两个优化问题是等价的。\n第一个问题是在置换矩阵 $P$ 上最小化 $\\lVert W_1 - W_2 P \\rVert_F^2$。如 B 的分析所示，这等价于在置换 $\\pi$ 上最小化 $\\sum_{i=1}^k \\lVert w_i^{(1)} - w_{\\pi(i)}^{(2)} \\rVert_2^2$。\n让我们展开范数的平方项：\n$$ \\lVert w_i^{(1)} - w_{\\pi(i)}^{(2)} \\rVert_2^2 = \\langle w_i^{(1)} - w_{\\pi(i)}^{(2)}, w_i^{(1)} - w_{\\pi(i)}^{(2)} \\rangle $$\n$$ = \\langle w_i^{(1)}, w_i^{(1)} \\rangle - 2 \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle + \\langle w_{\\pi(i)}^{(2)}, w_{\\pi(i)}^{(2)} \\rangle $$\n$$ = \\lVert w_i^{(1)} \\rVert_2^2 + \\lVert w_{\\pi(i)}^{(2)} \\rVert_2^2 - 2 \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle $$\n我们希望最小化的总和是：\n$$ \\sum_{i=1}^k \\left( \\lVert w_i^{(1)} \\rVert_2^2 + \\lVert w_{\\pi(i)}^{(2)} \\rVert_2^2 - 2 \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle \\right) $$\n给定条件 $\\lVert w_i^{(1)} \\rVert_2 = 1$ 和 $\\lVert w_j^{(2)} \\rVert_2 = 1$ 对所有 $i, j$ 成立：\n$$ \\sum_{i=1}^k \\left( 1^2 + 1^2 - 2 \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle \\right) = \\sum_{i=1}^k \\left( 2 - 2 \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle \\right) $$\n$$ = 2k - 2 \\sum_{i=1}^k \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle $$\n在 $\\pi$ 上最小化此表达式等价于最大化依赖于 $\\pi$ 的项，即 $\\sum_{i=1}^k \\langle w_i^{(1)}, w_{\\pi(i)}^{(2)} \\rangle$。\n这与陈述中描述的第二个问题相符。等价性成立。\n\n对 D 的判断：**正确**。\n\n**陈述 E 的分析**\n\n该陈述声称“置换 $W$ 的列同时置换 $H$ 的行，会使 $W H^\\top$ 保持不变”。\n\n让我们将这些操作形式化。\n“置换 $W$ 的列”：此操作对应于右乘一个置换矩阵 $P \\in \\mathbb{R}^{k \\times k}$。新矩阵为 $W' = W P$。\n“置换 $H$ 的行”：此操作对应于左乘一个置换矩阵 $Q \\in \\mathbb{R}^{m \\times m}$。新矩阵为 $H' = Q H$。\n\n现在，让我们计算新的乘积矩阵 $(W')(H')^\\top$：\n$$ (W P)(Q H)^\\top = (W P)(H^\\top Q^\\top) = W P H^\\top Q^\\top $$\n该陈述声称这等于 $W H^\\top$。要使其在一般情况下成立，我们需要 $W P H^\\top Q^\\top = W H^\\top$。这不是一个恒等式。对于一个非零分解，这将意味着 $P H^\\top Q^\\top = H^\\top$，这对于任意的 $H$、$P$ 和 $Q$ 并不成立。\n该模型的基本置换不变性，如分析 A 所示，是 $(WP)(HP)^\\top = WH^\\top$。这涉及用相同的置换来置换 $W$ 的列和 $H$ 的列。该陈述错误地提到了置换 $H$ 的行。按其字面意思，该陈述在数学上是错误的。\n\n对 E 的判断：**不正确**。\n\n**总结**\n- 陈述 A 是正确的。\n- 陈述 B 是正确的。\n- 陈述 C 是不正确的。\n- 陈述 D 是正确的。\n- 陈述 E 是不正确的。\n\n正确的陈述是 A、B 和 D。",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "在使用因子模型时，一个关键的实践问题是如何选择潜因子的数量 $k$。因子太少可能导致模型不准确，而因子太多则可能过拟合数据中的噪声。基于对齐因子的概念，本练习将实现一种强大的、由数据驱动的技术，即通过自助法重采样（bootstrap resampling）来评估因子的稳定性，从而为选择最佳因子数和旋转方法提供一个稳健的准则 。",
            "id": "3137686",
            "problem": "考虑一个由带有加性噪声的潜因子模型建模的矩形数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$。潜因子模型假设存在一个得分矩阵 $Z \\in \\mathbb{R}^{n \\times k}$ 和一个载荷矩阵 $W \\in \\mathbb{R}^{p \\times k}$，使得 $X \\approx Z W^{\\top} + E$，其中 $E$ 是一个噪声矩阵。因子维度 $k$ 是未知的，并且由于旋转模糊性，在没有约束的情况下因子解是不可识别的：对于任何满足 $R^{\\top} R = I_k$ 的正交矩阵 $R \\in \\mathbb{R}^{k \\times k}$，因子对 $(Z R, W R)$ 会产生相同的近似 $Z W^{\\top}$。\n\n目标是检验因子在自助法重采样下的稳定性，以选择潜因子维度 $k$ 和旋转类型。考虑两种旋转类型：不旋转和方差最大化（varimax）旋转。对于每个候选维度 $k$，您将对 $X$ 的行和列进行自助法重采样，在重采样的数据上重新拟合因子，将重采样的载荷与在完整数据上计算的参考解对齐，并计算一个稳定性得分。然后，使用一个有原则的规则，根据自助法样本间的稳定性来选择 $k$ 和旋转类型。\n\n您的程序必须基于基本定义实现以下组件：\n\n- 数据标准化：给定 $X$，通过将每列中心化至零均值并缩放到单位方差来构建 $\\tilde{X}$，即对于每个特征索引 $j \\in \\{1,\\dots,p\\}$，设置 $\\tilde{X}_{:,j} = (X_{:,j} - \\mu_j)/\\sigma_j$，其中 $\\mu_j$ 是列 $j$ 的均值，$\\sigma_j$ 是其标准差。如果某列的方差为零，则保持不变以避免除以零。\n\n- 通过截断奇异值分解（SVD）进行因子估计：对于给定的 $k$，通过 SVD 计算 $\\tilde{X}$ 的秩为 $k$ 的近似，$\\tilde{X} \\approx U_k \\Sigma_k V_k^{\\top}$，其中 $U_k \\in \\mathbb{R}^{n \\times k}$，$\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ 是具有非负奇异值的对角矩阵，且 $V_k \\in \\mathbb{R}^{p \\times k}$。将 $k$ 个因子的载荷矩阵定义为 $L_k = V_k \\Sigma_k$。这一选择与标准化数据的主成分分析（PCA）载荷是一致的。根据指定的旋转类型对 $L_k$ 应用旋转：可以是不旋转（单位变换）或正交的方差最大化（varimax）旋转，后者旨在通过在正交约束下最大化每个因子上载荷平方的方差，来使 $L_k$ 的列更具可解释性。\n\n- 自助法重采样：对于固定的 $k$ 和选定的旋转类型，执行 $B$ 次自助法重复。每次重复 $b \\in \\{1, \\dots, B\\}$ 从 $\\tilde{X}$ 中独立地有放回地重采样 $n$ 行和 $p$ 列，形成 $\\tilde{X}^{(b)} \\in \\mathbb{R}^{n \\times p}$。在 $\\tilde{X}^{(b)}$ 上使用指定的旋转拟合秩为 $k$ 的因子载荷 $L_k^{(b)}$。\n\n- 通过对齐的绝对相关性进行稳定性评分：对于每次自助法重复 $b$，通过解决一个匹配问题，将 $L_k^{(b)}$ 的 $k$ 列与 $L_k^{\\text{ref}}$ 的 $k$ 列配对，从而将 $L_k^{(b)}$ 与全数据参考载荷 $L_k^{\\text{ref}}$（在 $\\tilde{X}$ 上使用相同的 $k$ 和旋转类型计算）对齐。具体来说，对于每对因子索引 $(j, \\ell)$，计算 $L_k^{(b)}$ 中第 $j$ 个自助法载荷向量与 $L_k^{\\text{ref}}$ 中第 $\\ell$ 个参考载荷向量之间的绝对皮尔逊相关性，计算范围限定于重复 $b$ 的重采样列索引，如果列被多次重采样则包括重复。将此值表示为 $c_{j\\ell}^{(b)} \\in [0,1]$。构建一个元素为 $c_{j\\ell}^{(b)}$ 的 $k \\times k$ 矩阵 $C^{(b)}$。找到 $\\{1,\\dots,k\\}$ 的一个排列 $\\pi^{(b)}$，该排列最大化 $\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$（等价于解决一个成本为 $-C^{(b)}$ 的线性分配问题）。将重复 $b$ 的稳定性得分定义为匹配的绝对相关性的平均值，$s^{(b)} = \\frac{1}{k}\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$。\n\n- 聚合与选择：对于每个候选的 $k$ 和旋转类型，计算平均稳定性 $\\bar{s}_{k,r} = \\frac{1}{B}\\sum_{b=1}^{B} s^{(b)}$ 和均值标准误 $\\text{se}_{k,r} = \\sqrt{\\frac{1}{B(B-1)}\\sum_{b=1}^{B} (s^{(b)} - \\bar{s}_{k,r})^2}$。对于每个 $k$，选择能最大化 $\\bar{s}_{k,r}$ 的旋转类型 $r_k$。然后，使用一倍标准误规则选择潜因子维度 $k^{\\star}$：令 $\\bar{s}_{\\max} = \\max_k \\bar{s}_{k, r_k}$，并令 $k_{\\max}$ 为达到此最大值时的 $k$，其对应的标准误为 $\\text{se}_{k_{\\max}, r_{k_{\\max}}}$。选择 $k^{\\star}$ 为满足 $\\bar{s}_{k, r_k} \\ge \\bar{s}_{\\max} - \\text{se}_{k_{\\max}, r_{k_{\\max}}}$ 的最小 $k$。最后，选择旋转类型 $r^{\\star} = r_{k^{\\star}}$。\n\n实现要求：\n\n- 仅使用奇异值分解和正交方差最大化旋转进行因子估计，如上所述。使用绝对皮尔逊相关性进行对齐。通过匈牙利方法进行最优分配。\n\n- 每个 $(k, \\text{rotation})$ 设置使用 $B$ 次自助法重复。\n\n- 候选潜因子维度是集合 $\\{1,2,3,4,5\\}$ 中的整数。\n\n- 旋转类型为 \"none\" 和 \"varimax\"。在输出中，将旋转类型编码为整数：$0$ 代表 \"none\"，$1$ 代表 \"varimax\"。\n\n测试套件规范：\n\n构建三个具有固定随机种子的合成测试用例以确保可复现性。对于每个用例，根据 $X = Z W^{\\top} + \\sigma \\cdot N$ 生成 $X$，其中 $Z$ 和 $W$ 的规定如下，$N$ 具有独立同分布的标准正态分布条目。生成后，在拟合因子之前对列进行标准化以获得 $\\tilde{X}$。\n\n- 测试用例 1：令 $n = 120$, $p = 20$, $k_{\\text{true}} = 3$, $\\sigma = 0.5$。构建一个具有块状结构的 $W \\in \\mathbb{R}^{20 \\times 3}$：对于特征索引 1 到 7，在因子 1 上设置强载荷；对于索引 8 到 14，在因子 2 上设置强载荷；对于索引 15 到 20，在因子 3 上设置强载荷。向 $W$ 中添加少量高斯噪声。从独立同分布的标准正态分布中抽取 $Z \\in \\mathbb{R}^{120 \\times 3}$。\n\n- 测试用例 2：令 $n = 100$, $p = 15$, $k_{\\text{true}} = 1$, $\\sigma = 0.8$。构建一个 $W \\in \\mathbb{R}^{15 \\times 1}$，其随机条目在一个子集上被放大，以支持一个主导因子。从独立同分布的标准正态分布中抽取 $Z \\in \\mathbb{R}^{100 \\times 1}$。\n\n- 测试用例 3：令 $n = 100$, $p = 18$，并通过将 $Z W^{\\top}$ 设置为零矩阵来生成纯噪声，其中 $\\sigma = 1.0$。即，$X = \\sigma \\cdot N$，其中 $N$ 具有独立同分布的标准正态分布条目。\n\n对于每个测试用例，设置固定的随机种子以确保可复现性。\n\n您的程序必须：\n\n- 实现上述过程，为所有 $k \\in \\{1,2,3,4,5\\}$ 和两种旋转类型计算 $\\bar{s}_{k,r}$ 和 $\\text{se}_{k,r}$。\n\n- 根据所述的一倍标准误规则和旋转选择来选择 $k^{\\star}$ 和 $r^{\\star}$。\n\n- 对于每个测试用例，返回一个数对 $[k^{\\star}, \\text{rotation\\_code}]$，其中 $\\text{rotation\\_code} \\in \\{0,1\\}$ 对旋转类型进行编码。\n\n最终输出格式：\n\n您的程序应生成单行输出，其中包含三个测试用例的结果，格式为方括号括起来的逗号分隔列表，每个元素是对应测试用例的列表 $[k^{\\star}, \\text{rotation\\_code}]$，并按测试用例 1、2、3 的顺序排列。例如，格式必须与 $[[k_1,r_1],[k_2,r_2],[k_3,r_3]]$ 完全一样。\n\n将自助法重复次数设置为 $B = 40$，并仅使用指定的整数输出。本问题不涉及物理单位或角度。所有数字输出都必须是整数。",
            "solution": "该问题要求实现一个全面的统计程序，用于为因子模型选择潜因子数量 $k$ 和适当的旋转类型。在心理测量学、金融学和生物信息学等领域，这是一项常见而重要的任务，这些领域需要从高维数据中推断潜在的未观测结构。该选择基于自助法重采样下的模型稳定性原则。一个在数据的不同扰动版本中能一致地揭示相同因子结构的模型被认为是更可靠的。\n\n整个过程可以分解为以下几个逻辑步骤，这些步骤构成了计算解决方案的基础：\n\n1.  **模型设定与数据准备**：\n    数据由一个矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 给出，包含 $n$ 个观测值和 $p$ 个特征。潜因子模型假设 $X \\approx ZW^{\\top} + E$，其中 $Z \\in \\mathbb{R}^{n \\times k}$ 是因子得分，$W \\in \\mathbb{R}^{p \\times k}$ 是因子载荷，因子数量 $k$ 未知。在分析之前，数据矩阵 $X$ 的每一列都被标准化，使其均值为 $0$，标准差为 $1$，得到 $\\tilde{X}$。这确保了所有特征对所解释的方差贡献相等。方差为零的列保持不变。对于标准化数据矩阵，主成分分析（PCA）和因子分析密切相关，我们可以使用奇异值分解（SVD）进行因子估计。\n\n2.  **通过 SVD 进行因子估计**：\n    对于给定的因子数量 $k$，我们使用截断 SVD 计算标准化数据矩阵 $\\tilde{X}$ 的秩为 $k$ 的近似：$\\tilde{X} \\approx U_k \\Sigma_k V_k^{\\top}$。在这里，$U_k \\in \\mathbb{R}^{n \\times k}$ 和 $V_k \\in \\mathbb{R}^{p \\times k}$ 的列是标准正交的，而 $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$ 是包含前 $k$ 个最大奇异值的对角矩阵。遵循 PCA 中的一个常见约定，因子载荷矩阵 $L_k \\in \\mathbb{R}^{p \\times k}$ 定义为 $L_k = V_k \\Sigma_k$。$L_k$ 的每一列代表一个因子，其元素代表原始 $p$ 个特征在该因子上的权重。\n\n3.  **因子旋转**：\n    解 $Z W^{\\top}$ 对正交旋转是不变的，即对于任何正交矩阵 $R$，都有 $(ZR)(WR)^{\\top} = ZW^{\\top}$。这种旋转模糊性意味着通过 SVD 估计的因子不是唯一的，并且可能不易解释。为了解决这个问题，采用了因子旋转技术。本问题考虑了两种选择：\n    *   **不旋转**：直接使用 SVD 导出的载荷 $L_k$。这些是主成分，它们是正交的，并按其解释的方差大小排序。\n    *   **方差最大化（Varimax）旋转**：这是一种旨在简化因子结构的正交旋转方法。它寻找一个旋转矩阵 $T$，使得旋转后的载荷 $L_k^{\\text{rot}} = L_k T$ 具有最大的“简单结构”。这是通过最大化方差最大化准则来实现的，该准则即为每个因子上载荷平方的方差之和。一个较高的准则值表明，每个原始特征都倾向于在少数几个因子上有高载荷，而在其他因子上的载荷接近于零，从而增强了可解释性。\n\n4.  **自助法稳定性评估**：\n    选择程序的核心是评估当数据受到轻微扰动时，估计出的因子结构的稳定性。这通过自助法重采样来完成。对于每个候选的 $k$ 和每种旋转类型，以下过程重复 $B$ 次：\n    *   **重采样**：通过从标准化后的完整数据集 $\\tilde{X}$ 中有放回地重采样行（观测值）和列（特征），创建一个自助法数据集 $\\tilde{X}^{(b)}$。\n    *   **重新估计**：在自助法数据集 $\\tilde{X}^{(b)}$ 上拟合一个包含 $k$ 个因子的因子模型，得到一组新的载荷 $L_k^{(b)}$（使用与参考解相同的旋转类型）。注意，由于 $\\tilde{X}^{(b)}$ 的列是从一个已经标准化的矩阵中重采样得到的，它们的均值和方差通常不再是 $0$ 和 $1$，因此需要对 $\\tilde{X}^{(b)}$ 重新进行标准化。\n    *   **对齐与评分**：自助法解 $L_k^{(b)}$ 中的因子必须与参考解 $L_k^{\\text{ref}}$（在完整数据集 $\\tilde{X}$ 上计算）中的因子对齐。SVD 算法按奇异值对因子进行排序，但这种排序在自助法样本中可能会改变。为解决此问题，需要解决一个匹配问题。我们构建一个 $k \\times k$ 矩阵 $C^{(b)}$，其中条目 $c_{j\\ell}^{(b)}$ 是 $L_k^{(b)}$ 的第 $j$ 个因子与 $L_k^{\\text{ref}}$ 的第 $\\ell$ 个因子之间的绝对皮尔逊相关性。该相关性是在自助法载荷向量和仅限于重采样特征索引的参考载荷向量之间计算的。通过找到最大化匹配对相关性之和 $\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$ 的因子排列 $\\pi^{(b)}$ 来找到最优匹配。这是一个线性分配问题，可以通过匈牙利方法高效解决。该次重复的稳定性得分 $s^{(b)}$ 是这些最大匹配相关性的平均值：$s^{(b)} = \\frac{1}{k}\\sum_{j=1}^{k} c_{j,\\pi^{(b)}(j)}^{(b)}$。\n\n5.  **聚合与模型选择**：\n    经过 $B$ 次自助法重复后，我们为每对 $(k, \\text{旋转类型})$ 获得了 $B$ 个稳定性得分。\n    *   **聚合**：对于每个 $(k, r)$，我们计算平均稳定性得分 $\\bar{s}_{k,r}$ 及其标准误 $\\text{se}_{k,r}$。\n    *   **选择规则**：最终模型通过一个两步过程选出。首先，对于每个候选维度 $k$，我们选择能产生最高平均稳定性 $\\bar{s}_{k,r}$ 的旋转类型 $r_k$。这样就得到了一个对应每个 $k$ 的最佳稳定性得分序列。其次，我们对这个序列应用“一倍标准误规则”来选择最终维度 $k^{\\star}$。该规则在模型性能（稳定性）和简约性之间取得平衡。它选择稳定性不显著差于最稳定模型的那个最简单模型（最小的 $k$）。具体来说，如果 $k_{\\max}$ 是具有绝对最高稳定性 $\\bar{s}_{\\max}$ 的维度，我们选择 $k^{\\star}$ 为稳定性 $\\bar{s}_{k, r_k}$ 在最大值的一个标准误范围内的最小 $k$：$\\bar{s}_{k, r_k} \\ge \\bar{s}_{\\max} - \\text{se}_{k_{\\max}, r_{k_{\\max}}}$。最终的旋转类型 $r^{\\star}$ 则是在第一步中为 $k^{\\star}$ 选择的类型，即 $r^{\\star} = r_{k^{\\star}}$。\n\n这种系统性的、数据驱动的方法为选择潜因子模型的关键超参数提供了一个稳健的框架，避免了任意决策，并将选择建立在所推断结构的经验稳定性之上。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\nfrom scipy.optimize import linear_sum_assignment\n\ndef generate_test_data(case_id, seed):\n    \"\"\"Generates synthetic data for the three test cases.\"\"\"\n    rng = np.random.default_rng(seed)\n    if case_id == 1:\n        n, p, k_true, sigma = 120, 20, 3, 0.5\n        W = np.zeros((p, k_true))\n        W[0:7, 0] = 1.0\n        W[7:14, 1] = 1.0\n        W[14:20, 2] = 1.0\n        W += rng.normal(0, 0.1, size=W.shape)\n        Z = rng.normal(size=(n, k_true))\n        X = Z @ W.T + sigma * rng.normal(size=(n, p))\n    elif case_id == 2:\n        n, p, k_true, sigma = 100, 15, 1, 0.8\n        W = rng.normal(size=(p, k_true))\n        amplified_indices = rng.choice(p, size=int(p / 2), replace=False)\n        W[amplified_indices, :] *= 3.0\n        Z = rng.normal(size=(n, k_true))\n        X = Z @ W.T + sigma * rng.normal(size=(n, p))\n    elif case_id == 3:\n        n, p, sigma = 100, 18, 1.0\n        X = sigma * rng.normal(size=(n, p))\n    else:\n        raise ValueError(\"Invalid case_id\")\n    return X\n\ndef standardize_data(X):\n    \"\"\"Standardizes columns of X to mean 0 and variance 1.\"\"\"\n    X_std = X.copy()\n    mu = np.mean(X_std, axis=0)\n    sigma = np.std(X_std, axis=0)\n    non_zero_std_mask = sigma > 1e-10\n    X_std[:, non_zero_std_mask] = (X_std[:, non_zero_std_mask] - mu[non_zero_std_mask]) / sigma[non_zero_std_mask]\n    return X_std\n\ndef varimax(L, tol=1e-8, max_iter=100):\n    \"\"\"Orthogonal varimax rotation using SVD.\"\"\"\n    p, k = L.shape\n    if k  2:\n        return L\n    \n    h = np.sqrt(np.sum(L**2, axis=1, keepdims=True))\n    h[h == 0] = 1.0\n    L_norm = L / h\n    \n    T = np.eye(k)\n    \n    for _ in range(max_iter):\n        L_rot = L_norm @ T\n        g = np.sum(L_rot**2, axis=0) / p\n        B = (1/p) * L_rot.T @ (L_rot**3 - L_rot @ np.diag(g))\n        \n        try:\n            U, _, Vt = svd(B)\n            R = U @ Vt\n        except np.linalg.LinAlgError:\n            break\n        \n        if np.sum((R - np.eye(k))**2)  tol:\n            break\n            \n        T = T @ R\n    \n    L_rotated_normalized = L_norm @ T\n    L_final = L_rotated_normalized * h\n    return L_final\n\ndef pearson_correlation(x, y):\n    \"\"\"Computes the Pearson correlation coefficient, handling zero variance.\"\"\"\n    mean_x, mean_y = np.mean(x), np.mean(y)\n    std_x, std_y = np.std(x), np.std(y)\n    \n    if std_x == 0 or std_y == 0:\n        return 0.0\n    \n    cov = np.mean((x - mean_x) * (y - mean_y))\n    return cov / (std_x * std_y)\n\ndef get_stability_scores(X, k, rotation_type, B, rng):\n    \"\"\"Computes stability scores for a given k and rotation type.\"\"\"\n    X_std = standardize_data(X)\n    n, p = X_std.shape\n    \n    try:\n        U, s, Vt = svd(X_std, full_matrices=False)\n        L_ref_unrotated = Vt[:k, :].T @ np.diag(s[:k])\n    except np.linalg.LinAlgError:\n        return np.array([])\n    \n    if rotation_type == 'varimax':\n        L_ref = varimax(L_ref_unrotated)\n    else:\n        L_ref = L_ref_unrotated\n        \n    scores = []\n    for _ in range(B):\n        row_indices = rng.choice(n, n, replace=True)\n        col_indices = rng.choice(p, p, replace=True)\n        \n        X_boot_raw = X_std[row_indices, :][:, col_indices]\n        X_boot_std = standardize_data(X_boot_raw)\n        \n        try:\n            U_b, s_b, Vt_b = svd(X_boot_std, full_matrices=False)\n            \n            num_sv = len(s_b)\n            if num_sv  k:\n                s_b_padded = np.zeros(k)\n                s_b_padded[:num_sv] = s_b\n                s_b = s_b_padded\n                Vt_b_padded = np.zeros((k, p))\n                Vt_b_padded[:Vt_b.shape[0], :] = Vt_b[:num_sv, :]\n                Vt_b = Vt_b_padded\n\n            L_boot_unrotated = Vt_b[:k, :].T @ np.diag(s_b[:k])\n            \n            if rotation_type == 'varimax':\n                L_boot = varimax(L_boot_unrotated)\n            else:\n                L_boot = L_boot_unrotated\n        except np.linalg.LinAlgError:\n            continue\n            \n        cost_matrix = np.zeros((k, k))\n        for j in range(k):\n            for l in range(k):\n                vec_boot = L_boot[:, j]\n                vec_ref_resampled = L_ref[col_indices, l]\n                corr = pearson_correlation(vec_boot, vec_ref_resampled)\n                cost_matrix[j, l] = np.abs(corr)\n\n        row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n        \n        max_corrs = cost_matrix[row_ind, col_ind]\n        score = np.mean(max_corrs) if k > 0 else 1.0\n        scores.append(score)\n        \n    return np.array(scores)\n\ndef solve():\n    test_specs = [\n        {'case_id': 1, 'seed': 1},\n        {'case_id': 2, 'seed': 2},\n        {'case_id': 3, 'seed': 3}\n    ]\n    \n    B = 40\n    candidate_ks = [1, 2, 3, 4, 5]\n    rotation_types = ['none', 'varimax']\n    final_results = []\n\n    for spec in test_specs:\n        X = generate_test_data(spec['case_id'], spec['seed'])\n        rng = np.random.default_rng(spec['seed'])\n        \n        stability_means = np.zeros((len(candidate_ks), len(rotation_types)))\n        stability_ses = np.zeros((len(candidate_ks), len(rotation_types)))\n\n        for i, k in enumerate(candidate_ks):\n            for j, r_type in enumerate(rotation_types):\n                scores = get_stability_scores(X, k, r_type, B, rng)\n                if len(scores) > 1:\n                    stability_means[i, j] = np.mean(scores)\n                    stability_ses[i, j] = np.std(scores, ddof=1) / np.sqrt(len(scores))\n                else: \n                    stability_means[i, j] = 0\n                    stability_ses[i, j] = 0\n\n        best_rotation_indices = np.argmax(stability_means, axis=1)\n        best_stabilities = np.array([stability_means[i, best_rotation_indices[i]] for i in range(len(candidate_ks))])\n        best_ses = np.array([stability_ses[i, best_rotation_indices[i]] for i in range(len(candidate_ks))])\n        \n        if np.all(best_stabilities == 0):\n            k_star = 1\n            r_star_code = 0\n        else:\n            k_max_idx = np.argmax(best_stabilities)\n            s_max = best_stabilities[k_max_idx]\n            se_at_max = best_ses[k_max_idx]\n            \n            threshold = s_max - se_at_max\n            \n            eligible_k_indices = np.where(best_stabilities >= threshold)[0]\n            k_star_idx = eligible_k_indices[0] if len(eligible_k_indices) > 0 else 0\n            \n            k_star = candidate_ks[k_star_idx]\n            r_star_code = best_rotation_indices[k_star_idx]\n\n        final_results.append([k_star, int(r_star_code)])\n        \n    print(f\"[[{final_results[0][0]},{final_results[0][1]}],[{final_results[1][0]},{final_results[1][1]}],[{final_results[2][0]},{final_results[2][1]}]]\")\n\nsolve()\n```"
        },
        {
            "introduction": "标准的因子模型是无监督的，其目标仅在于解释特征数据 $X$ 内部的变异。然而，在许多科学应用中，我们希望发现的潜因子不仅能重构数据，还能预测某个外部响应变量 $y$。这项高级练习将引导您推导并实现一个监督因子模型（supervised factor model）的算法，并探索如何通过一个耦合参数 $\\lambda$ 来控制监督信号的影响，从而识别出与响应变量相关的特定潜因子 。",
            "id": "3137661",
            "problem": "考虑一个监督潜在因子模型，该模型联合解释一个数据矩阵和一个响应。令 $X \\in \\mathbb{R}^{n \\times d}$ 为数据矩阵，$y \\in \\mathbb{R}^{n}$ 为响应向量，$Z \\in \\mathbb{R}^{n \\times k}$ 为潜在得分，$W \\in \\mathbb{R}^{d \\times k}$ 为载荷，$\\beta \\in \\mathbb{R}^{k}$ 为连接潜在得分与响应的回归向量。目标是通过分析耦合强度如何影响估计的潜在预测器与真实响应关联的潜在方向的对齐度，来研究在存在监督耦合的情况下潜在因子的可识别性。\n\n从基本定义出发，考虑最小化正则化联合最小二乘目标函数\n$$\n\\mathcal{L}(Z,W,\\beta) \\;=\\; \\lVert X - Z W^\\top \\rVert_F^2 \\;+\\; \\lambda \\lVert y - Z \\beta \\rVert_2^2 \\;+\\; \\alpha \\lVert W \\rVert_F^2 \\;+\\; \\alpha \\lVert \\beta \\rVert_2^2 \\;+\\; \\gamma \\lVert Z \\rVert_F^2,\n$$\n其中 $\\lambda \\ge 0$ 控制监督耦合的强度，$\\alpha  0$ 和 $\\gamma  0$ 是为确保数值稳定性而设置的小岭惩罚项，$\\lVert \\cdot \\rVert_F$ 表示弗罗贝尼乌斯范数，$\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。假设数据生成过程的形式为\n$$\nX \\;=\\; Z_{\\text{true}} W_{\\text{true}}^\\top \\;+\\; \\varepsilon_X, \\quad y \\;=\\; Z_{\\text{true}} \\beta_{\\text{true}} \\;+\\; \\varepsilon_y,\n$$\n其中 $\\varepsilon_X$ 和 $\\varepsilon_y$ 是独立高斯噪声，并且 $Z_{\\text{true}}$ 的第一列是通过 $\\beta_{\\text{true}}$ 驱动响应的方向。\n\n您的任务是：\n- 通过将梯度设为零，从第一性原理推导出当其他变量块固定时，最小化 $\\mathcal{L}(Z,W,\\beta)$ 的每个变量块 $W$、$\\beta$ 和 $Z$ 的闭式正规方程。\n- 基于这些正规方程设计一个交替最小化算法，该算法依次更新 $W$、$\\beta$ 和 $Z$，直到收敛或达到固定的迭代次数。每个测试用例使用 $Z$ 的单个固定随机初始化，以分离耦合强度 $\\lambda$ 的影响。\n- 定义对齐度量如下。令 $s \\in \\mathbb{R}^{n}$ 为真实的响应关联潜在得分向量，其值等于 $Z_{\\text{true}}$ 的第一列。令估计的潜在子空间为最终 $Z$ 的列空间。对齐度定义为\n$$\n\\text{align}(Z, s) \\;=\\; \\frac{\\lVert P_Z s \\rVert_2}{\\lVert s \\rVert_2},\n$$\n其中 $P_Z$ 是到 $Z$ 的列空间上的正交投影算子。该值范围在 $[0,1]$ 内，$1$ 表示完美恢复了真实的响应关联方向。\n- 对于每个测试用例，在耦合强度的离散网格\n$$\n\\Lambda \\;=\\; [\\,0.0,\\, 0.05,\\, 0.1,\\, 0.5,\\, 1.0,\\, 2.0,\\, 5.0,\\, 10.0\\,]\n$$\n上进行搜索，并返回满足 $\\text{align}(Z, s) \\ge \\tau$ 的最小 $\\lambda \\in \\Lambda$，其中阈值 $\\tau = 0.9$。如果没有这样的 $\\lambda$ 达到阈值，则返回 $-1.0$。\n\n每个测试用例的数据生成细节：\n- 从独立标准正态分布中抽取 $Z_{\\text{true}} \\in \\mathbb{R}^{n \\times k}$ 的元素。\n- 从独立正态分布中抽取 $W_{\\text{true}} \\in \\mathbb{R}^{d \\times k}$ 的元素，其方差经过缩放以保持数值稳定性。\n- 从独立正态分布中抽取 $\\varepsilon_X$（方差为 $\\sigma_X^2$）和 $\\varepsilon_y$（方差为 $\\sigma_y^2$）的元素。\n- 如下文所述设置 $\\beta_{\\text{true}}$；其第一个坐标通过 $Z_{\\text{true}}$ 的第一列决定监督的强度。\n- 每个测试用例使用给定的固定伪随机种子。\n\n测试套件：\n- 测试用例 1：seed $= 42$, $n = 200$, $d = 60$, $k = 3$, $\\sigma_X = 0.25$, $\\sigma_y = 0.25$, $\\alpha = 10^{-3}$, $\\gamma = 10^{-3}$, $\\beta_{\\text{true}} = (2.0, 0.0, 0.0)$。\n- 测试用例 2：seed $= 7$, $n = 200$, $d = 60$, $k = 3$, $\\sigma_X = 0.5$, $\\sigma_y = 0.5$, $\\alpha = 10^{-3}$, $\\gamma = 10^{-3}$, $\\beta_{\\text{true}} = (0.2, 0.0, 0.0)$。\n- 测试用例 3：seed $= 123$, $n = 200$, $d = 60$, $k = 3$, $\\sigma_X = 0.25$, $\\sigma_y = 1.0$, $\\alpha = 10^{-3}$, $\\gamma = 10^{-3}$, $\\beta_{\\text{true}} = (0.0, 0.0, 0.0)$。\n- 测试用例 4：seed $= 99$, $n = 200$, $d = 60$, $k = 1$, $\\sigma_X = 0.25$, $\\sigma_y = 0.25$, $\\alpha = 10^{-3}$, $\\gamma = 10^{-3}$, $\\beta_{\\text{true}} = (2.0)$。\n\n您的程序必须实现交替最小化算法，为每个 $\\lambda \\in \\Lambda$ 计算对齐度，选择达到 $\\text{align}(Z, s) \\ge \\tau$ 的最小 $\\lambda$，并在一行中将四个测试用例的结果输出为方括号括起来的逗号分隔列表（例如，$[0.5,-1.0,2.0,0.0]$）。不涉及物理单位或角度单位。每个测试用例的输出必须是单个浮点数，用 $-1.0$ 表示没有 $\\lambda \\in \\Lambda$ 达到阈值。",
            "solution": "用户提供了一个关于监督潜在因子模型的适定统计学习问题。任务是推导一个交替最小化算法的更新规则，实现该算法，并用它来找到能确保估计的潜在空间与已知的真实潜在方向充分对齐的最小监督耦合强度 $\\lambda$。\n\n### 1. 模型与目标函数\n\n该问题围绕着为数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$、响应向量 $y \\in \\mathbb{R}^{n}$、潜在得分 $Z \\in \\mathbb{R}^{n \\times k}$、载荷 $W \\in \\mathbb{R}^{d \\times k}$ 和回归向量 $\\beta \\in \\mathbb{R}^{k}$ 最小化正则化联合最小二乘目标函数 $\\mathcal{L}(Z,W,\\beta)$。目标函数为：\n$$\n\\mathcal{L}(Z,W,\\beta) \\;=\\; \\lVert X - Z W^\\top \\rVert_F^2 \\;+\\; \\lambda \\lVert y - Z \\beta \\rVert_2^2 \\;+\\; \\alpha \\lVert W \\rVert_F^2 \\;+\\; \\alpha \\lVert \\beta \\rVert_2^2 \\;+\\; \\gamma \\lVert Z \\rVert_F^2\n$$\n在此，$\\lambda \\ge 0$ 是监督强度，而 $\\alpha  0$ 和 $\\gamma  0$ 是岭正则化参数。范数分别为弗罗贝尼乌斯范数 $\\lVert \\cdot \\rVert_F$ 和欧几里得范数 $\\lVert \\cdot \\rVert_2$。\n\n### 2. 交替最小化正规方程的推导\n\n优化是通过交替最小化 $\\mathcal{L}$ 对某一变量块（$W$、$\\beta$ 或 $Z$）的函数值，同时保持其他变量块固定来进行的。这需要通过将相应梯度设置为零来推导每个子问题的闭式解。\n\n#### 2.1. 载荷 $W$ 的更新\n固定 $Z$ 和 $\\beta$，我们最小化 $\\mathcal{L}$ 中依赖于 $W$ 的项：\n$$\n\\mathcal{L}_W(W) = \\lVert X - Z W^\\top \\rVert_F^2 + \\alpha \\lVert W \\rVert_F^2\n$$\n为求最小值，我们计算关于 $W$ 的梯度并将其设为零。使用矩阵导数恒等式 $\\nabla_A \\lVert M-BA^\\top\\rVert_F^2 = -2(M-BA^\\top)^\\top B$ 和 $\\nabla_A \\lVert A \\rVert_F^2 = 2A$，我们得到：\n$$\n\\nabla_W \\mathcal{L}_W = -2(X - ZW^\\top)^\\top Z + 2\\alpha W = -2(X^\\top Z - W Z^\\top Z) + 2\\alpha W = 0\n$$\n整理各项以求解 $W$：\n$$\n-X^\\top Z + W(Z^\\top Z) + \\alpha W = 0 \\implies W(Z^\\top Z + \\alpha I_k) = X^\\top Z\n$$\n其中 $I_k$ 是 $k \\times k$ 单位矩阵。这给出了 $W$ 的正规方程：\n$$\nW = (X^\\top Z) (Z^\\top Z + \\alpha I_k)^{-1}\n$$\n\n#### 2.2. 回归向量 $\\beta$ 的更新\n固定 $Z$ 和 $W$，我们最小化 $\\mathcal{L}$ 中依赖于 $\\beta$ 的项：\n$$\n\\mathcal{L}_\\beta(\\beta) = \\lambda \\lVert y - Z \\beta \\rVert_2^2 + \\alpha \\lVert \\beta \\rVert_2^2\n$$\n这是一个标准的岭回归问题。梯度为：\n$$\n\\nabla_\\beta \\mathcal{L}_\\beta = \\lambda \\nabla_\\beta (y - Z\\beta)^\\top(y - Z\\beta) + \\alpha \\nabla_\\beta \\beta^\\top\\beta = \\lambda (-2Z^\\top (y - Z\\beta)) + 2\\alpha\\beta\n$$\n将梯度设为零：\n$$\n-\\lambda Z^\\top y + \\lambda (Z^\\top Z)\\beta + \\alpha\\beta = 0 \\implies (\\lambda Z^\\top Z + \\alpha I_k) \\beta = \\lambda Z^\\top y\n$$\n由此得到 $\\beta$ 的正规方程为：\n$$\n\\beta = (\\lambda Z^\\top Z + \\alpha I_k)^{-1} (\\lambda Z^\\top y)\n$$\n\n#### 2.3. 潜在得分 $Z$ 的更新\n固定 $W$ 和 $\\beta$，我们最小化 $\\mathcal{L}$ 中依赖于 $Z$ 的项：\n$$\n\\mathcal{L}_Z(Z) = \\lVert X - Z W^\\top \\rVert_F^2 + \\lambda \\lVert y - Z \\beta \\rVert_2^2 + \\gamma \\lVert Z \\rVert_F^2\n$$\n使用恒等式 $\\nabla_A \\lVert M-AW \\rVert_F^2 = -2(M-AW)W^\\top$、$\\nabla_A \\lVert y-Ab\\rVert_2^2 = -2(y-Ab)b^\\top$ 和 $\\nabla_A \\lVert A \\rVert_F^2=2A$：\n$$\n\\nabla_Z \\mathcal{L}_Z = -2(X - ZW^\\top)W + \\lambda (-2(y - Z\\beta)\\beta^\\top) + 2\\gamma Z = 0\n$$\n整理各项以求解 $Z$：\n$$\n-XW + Z(W^\\top W) - \\lambda y \\beta^\\top + \\lambda Z(\\beta\\beta^\\top) + \\gamma Z = 0\n$$\n$$\nZ(W^\\top W + \\lambda \\beta\\beta^\\top + \\gamma I_k) = XW + \\lambda y\\beta^\\top\n$$\n$Z$ 的正规方程为：\n$$\nZ = (XW + \\lambda y \\beta^\\top) (W^\\top W + \\lambda \\beta\\beta^\\top + \\gamma I_k)^{-1}\n$$\n\n### 3. 算法与评估\n\n对于每个测试用例，总体算法流程如下：\n1.  **数据生成**：使用指定的参数和随机种子生成 $Z_{\\text{true}}$、$W_{\\text{true}}$、$\\varepsilon_X$ 和 $\\varepsilon_y$。构建观测矩阵 $X = Z_{\\text{true}} W_{\\text{true}}^\\top + \\varepsilon_X$ 和响应向量 $y = Z_{\\text{true}} \\beta_{\\text{true}} + \\varepsilon_y$。真实的响应关联潜在得分向量为 $s = Z_{\\text{true}}[:, 0]$。\n2.  **初始化**：对于每个测试用例，使用指定的种子生成一个单一固定的随机潜在得分矩阵 $Z_{init}$。\n3.  **参数搜索**：遍历网格 $\\Lambda = [\\,0.0,\\, 0.05,\\, 0.1,\\, 0.5,\\, 1.0,\\, 2.0,\\, 5.0,\\, 10.0\\,]$ 中的每个耦合强度 $\\lambda$。\n4.  **交替最小化**：对于每个 $\\lambda$，初始化 $Z = Z_{init}$ 并执行固定次数（例如 $100$ 次）的以下更新迭代：\n    a. 使用推导的方程更新 $W$。\n    b. 使用推导的方程更新 $\\beta$。\n    c. 使用推导的方程更新 $Z$。\n5.  **对齐度计算**：迭代结束后，计算由 $Z$ 的列表示的最终估计潜在空间与真实得分向量 $s$ 的对齊度。这是通过计算 $\\text{align}(Z, s) = \\lVert P_Z s \\rVert_2 / \\lVert s \\rVert_2$ 来完成的，其中 $P_Z = Q_Z Q_Z^\\top$ 是到 $Z$ 的列空间上的正交投影算子，$Q_Z$ 是该空间的一个标准正交基（通过 QR 分解获得）。\n6.  **最小 $\\lambda$ 选择**：如果对齐度达到或超过阈值 $\\tau=0.9$，则将当前的 $\\lambda$ 记录为该测试用例的结果，并终止对该用例的搜索。如果遍历 $\\Lambda$ 结束后仍未达到阈值，则结果为 $-1.0$。\n\n最终输出是每个测试用例的最小 $\\lambda$ 值的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, n, d, k, sigma_X, sigma_y, alpha, gamma, beta_true)\n        (42, 200, 60, 3, 0.25, 0.25, 1e-3, 1e-3, (2.0, 0.0, 0.0)),\n        (7, 200, 60, 3, 0.5, 0.5, 1e-3, 1e-3, (0.2, 0.0, 0.0)),\n        (123, 200, 60, 3, 0.25, 1.0, 1e-3, 1e-3, (0.0, 0.0, 0.0)),\n        (99, 200, 60, 1, 0.25, 0.25, 1e-3, 1e-3, (2.0,)),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_test_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef solve_test_case(params):\n    \"\"\"\n    Solves for a single test case.\n    \"\"\"\n    seed, n, d, k, sigma_X, sigma_y, alpha, gamma, beta_true_tuple = params\n    beta_true = np.array(beta_true_tuple, dtype=float).reshape(-1, 1)\n\n    # --- Data Generation ---\n    rng = np.random.default_rng(seed)\n    Z_true = rng.standard_normal(size=(n, k))\n    W_true = rng.normal(scale=1.0/np.sqrt(d), size=(d, k))\n    eps_X = rng.normal(scale=sigma_X, size=(n, d))\n    eps_y = rng.normal(scale=sigma_y, size=(n, 1))\n\n    X = Z_true @ W_true.T + eps_X\n    y = Z_true @ beta_true + eps_y\n\n    s = Z_true[:, 0]\n    \n    # --- Algorithm Parameters ---\n    LAMBDA_GRID = [0.0, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n    ALIGNMENT_THRESHOLD = 0.9\n    NUM_ITER = 100\n\n    # --- Single fixed random initialization for Z ---\n    Z_init = rng.standard_normal(size=(n, k))\n\n    # --- Search over lambda grid ---\n    for lam in LAMBDA_GRID:\n        Z = Z_init.copy()\n        W = np.zeros((d, k))\n        beta = np.zeros((k, 1))\n\n        # --- Alternating Minimization ---\n        for _ in range(NUM_ITER):\n            # Update W\n            # W = (X^T Z) (Z^T Z + alpha I_k)^-1\n            # Solve (Z^T Z + alpha I) W^T = Z^T X for W^T\n            A_W = Z.T @ Z + alpha * np.identity(k)\n            b_W = Z.T @ X\n            W_T = np.linalg.solve(A_W, b_W)\n            W = W_T.T\n\n            # Update beta\n            # beta = (lambda Z^T Z + alpha I_k)^-1 (lambda Z^T y)\n            # Solve (lambda Z^T Z + alpha I) beta = lambda Z^T y\n            A_beta = lam * (Z.T @ Z) + alpha * np.identity(k)\n            b_beta = lam * (Z.T @ y)\n            beta = np.linalg.solve(A_beta, b_beta)\n\n            # Update Z\n            # Z = (XW + lambda y beta^T) (W^T W + lambda beta beta^T + gamma I_k)^-1\n            # Solve (W^T W + ...)^T Z = (XW + ...)^T for Z\n            # Equivalently, solve B Z^T = A^T for Z^T where Z = A B^-1\n            A_Z_term1 = W.T @ W\n            A_Z_term2 = lam * (beta @ beta.T)\n            A_Z_term3 = gamma * np.identity(k)\n            A_Z = A_Z_term1 + A_Z_term2 + A_Z_term3\n\n            b_Z_term1 = X @ W\n            b_Z_term2 = lam * (y @ beta.T)\n            b_Z = b_Z_term1 + b_Z_term2\n            \n            # Solve A_Z^T Z^T = B_Z^T which is A_Z Z^T = B_Z^T\n            Z_T = np.linalg.solve(A_Z, b_Z.T)\n            Z = Z_T.T\n\n        # --- Alignment Calculation ---\n        # Compute orthonormal basis for the column space of Z\n        Q, _ = np.linalg.qr(Z, mode='reduced')\n        \n        # Project s onto the subspace spanned by Q and compute its norm\n        projected_s_norm = np.linalg.norm(Q.T @ s)\n        s_norm = np.linalg.norm(s)\n\n        alignment = 0.0 if s_norm == 0 else projected_s_norm / s_norm\n        \n        # --- Check Threshold ---\n        if alignment >= ALIGNMENT_THRESHOLD:\n            return lam\n\n    return -1.0\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}