{
    "hands_on_practices": [
        {
            "introduction": "许多矩阵补全方法的核心思想是最小化矩阵的核范数。这个练习将引导你实现奇异值阈值（Singular Value Thresholding, SVT）算法，这是解决此类问题的基石方法之一 。通过动手实现并观察正则化参数 $\\lambda$ 如何影响迭代过程中矩阵秩的变化，你将直观地理解该算法是如何逐步恢复出低秩结构的。",
            "id": "3168247",
            "problem": "要求您设计并分析一种使用核范数正则化器的矩阵补全近端方法。您的目标是实现一个由近端点原理驱动的迭代格式，并研究正则化权重的选择如何影响矩阵秩在迭代过程中的演变。您编写的程序必须通过奇异值分解（SVD）机制计算核范数的近端算子，并评估不同参数值下的秩收缩轨迹。\n\n起点与定义：\n- 设 $M \\in \\mathbb{R}^{m \\times n}$ 是一个部分观测矩阵，其关联的观测掩码为 $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$。定义线性观测算子 $P_{\\Omega}:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{m \\times n}$ 为：若 $(i,j)\\in\\Omega$，则 $(P_{\\Omega}(X))_{ij} = X_{ij}$；否则 $(P_{\\Omega}(X))_{ij}=0$。定义互补算子 $P_{\\Omega^c}(X) = X - P_{\\Omega}(X)$。矩阵 $X$ 的核范数，记为 $\\|X\\|_*$，是其所有奇异值的和。\n- 对于一个真、闭、凸函数 $f:\\mathbb{R}^{m \\times n}\\to\\mathbb{R}\\cup\\{+\\infty\\}$，其在点 $Y$ 处且参数为 $t>0$ 的近端算子定义为\n$$\n\\operatorname{prox}_{t f}(Y) = \\arg\\min_{X\\in\\mathbb{R}^{m\\times n}} \\left\\{ f(X) + \\frac{1}{2t}\\|X - Y\\|_F^2 \\right\\}.\n$$\n- 考虑复合目标函数 $F(X) = \\frac{1}{2}\\|P_{\\Omega}(X - M)\\|_F^2 + \\lambda \\|X\\|_*$，其中 $\\lambda>0$。\n\n任务：\n- 仅使用上述定义以及凸函数和 SVD 的标准性质，推导出一个迭代格式。该格式在每次迭代 $k$ 中，首先对观测到的元素强制执行数据一致性，然后在得到的点上应用核范数的近端算子。具体来说，对于一个固定的 $\\lambda$，从 $X^0 = 0$ 开始，对 $k=0,1,2,\\dots,K-1$ 执行以下两个步骤：\n  1. 数据一致性注入，通过设置 $Y^k_{ij} = M_{ij}$（如果 $(i,j)\\in\\Omega$）和 $Y^k_{ij} = X^k_{ij}$（否则）来形成 $Y^k$（即 $Y^k = P_{\\Omega}(M) + P_{\\Omega^c}(X^k)$）。\n  2. 近端步骤，通过从第一性原理推导出的奇异值阈值化方法计算 $X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$。\n- 在每次迭代 $k$ 中，计算 $X^{k}$ 的秩，其定义为严格大于一个固定数值容差的奇异值的数量（您必须选择并说明一个合理的容差）。\n\n研究内容：\n- 实现上述迭代格式，并对于每个 $\\lambda$ 的选择，记录秩的序列 $\\{\\operatorname{rank}(X^{1}), \\operatorname{rank}(X^{2}), \\dots, \\operatorname{rank}(X^{K})\\}$，其中 $K$ 是总迭代次数。\n\n数据生成协议（确定性）：\n- 设置 $m=20$ 和 $n=15$。\n- 使用正交因子和奇异值构建一个秩 $r_{\\text{true}}=3$ 的基准低秩矩阵 $X_{\\text{true}}\\in\\mathbb{R}^{20\\times 15}$。使用固定的奇异值 $10$、$5$ 和 $3$，并通过对具有固定伪随机种子的高斯随机矩阵应用 Gram–Schmidt 过程（通过标准的 QR 分解）来确定性地生成正交列。\n- 使用相同的固定伪随机种子，以概率 $p=0.5$ 独立采样每个元素作为观测值，从而生成观测掩码。\n- 对于 $(i,j)\\in\\Omega$，设置 $M_{ij} = X_{\\text{true},ij} + \\varepsilon_{ij}$ 来形成观测矩阵 $M$，其中 $\\varepsilon_{ij}$ 是均值为 $0$、标准差为 $0.01$ 的独立高斯噪声；对于 $(i,j)\\notin\\Omega$，设置 $M_{ij}=0$。噪声生成使用相同的固定种子。\n\n测试套件：\n- 使用 $K=12$ 次迭代和秩容差 $10^{-8}$。\n- 评估 $\\lambda$ 的三个值的秩轨迹：$\\lambda=0.0$（边界情况）、$\\lambda=1.0$（中等正则化）和 $\\lambda=5.0$（强正则化）。\n\n要求的最终输出格式：\n- 您的程序必须生成单行输出，其中包含三个秩轨迹（对应三个 $\\lambda$ 值），格式为方括号括起来的逗号分隔列表的列表，例如 $[[r_{1,1},\\dots,r_{1,12}],[r_{2,1},\\dots,r_{2,12}],[r_{3,1},\\dots,r_{3,12}]]$，其中 $r_{j,k}$ 是一个整数秩。不应打印任何其他文本。\n\n注：\n- 此问题不涉及角度和物理单位。\n- 实现必须是自包含的，并通过使用固定的伪随机种子来确保确定性。\n- 您必须通过奇异值阈值化来计算 $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}$，该方法需从近端算子的定义和 SVD 的性质推导得出；不要假设或使用任何预先提供的公式。",
            "solution": "已分析并验证用户提供的问题陈述。\n\n### 步骤1：提取已知信息\n- **矩阵与算子**：\n    - $M \\in \\mathbb{R}^{m \\times n}$: 一个部分观测矩阵。\n    - $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$: 一个观测掩码。\n    - $P_{\\Omega}(X)$: 线性算子，若 $(i,j)\\in\\Omega$ 则 $(P_{\\Omega}(X))_{ij} = X_{ij}$，否则为 $0$。\n    - $P_{\\Omega^c}(X) = X - P_{\\Omega}(X)$。\n    - $\\|X\\|_*$: 矩阵 $X$ 的核范数，即其奇异值之和。\n- **近端算子定义**：\n    - 对于一个真、闭、凸函数 $f$，$\\operatorname{prox}_{t f}(Y) = \\arg\\min_{X\\in\\mathbb{R}^{m \\times n}} \\left\\{ f(X) + \\frac{1}{2t}\\|X - Y\\|_F^2 \\right\\}$。\n- **目标函数**：\n    - $F(X) = \\frac{1}{2}\\|P_{\\Omega}(X - M)\\|_F^2 + \\lambda \\|X\\|_*$，其中 $\\lambda>0$。\n- **迭代格式**：\n    - 初始化：$X^0 = 0$。\n    - 对于 $k=0,1,2,\\dots,K-1$：\n        1. **数据一致性注入**：$Y^k = P_{\\Omega}(M) + P_{\\Omega^c}(X^k)$。\n        2. **近端步骤**：$X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$。\n- **任务**：\n    - 在每次迭代中计算秩序列 $\\{\\operatorname{rank}(X^{1}), \\operatorname{rank}(X^{2}), \\dots, \\operatorname{rank}(X^{K})\\}$。秩定义为大于指定容差的奇异值的数量。\n- **数据生成协议**：\n    - 矩阵维度：$m=20$, $n=15$。\n    - 基准秩：$r_{\\text{true}}=3$。\n    - 基准奇异值：$\\{10, 5, 3\\}$。\n    - 基准矩阵构造：$X_{\\text{true}} = U \\operatorname{diag}(\\{10,5,3\\}) V^T$，其中 $U$ 和 $V$ 是通过对使用固定种子的高斯随机矩阵进行 QR 分解生成的具有正交列的矩阵。\n    - 观测概率：每个条目为 $p=0.5$，使用固定种子。\n    - 观测矩阵：$M = P_{\\Omega}(X_{\\text{true}} + \\varepsilon)$，其中 $\\varepsilon$ 是使用固定种子生成的均值为 $0$、标准差为 $0.01$ 的高斯噪声。\n- **测试套件**：\n    - 迭代次数：$K=12$。\n    - 秩容差：$10^{-8}$。\n    - 正则化参数：$\\lambda \\in \\{0.0, 1.0, 5.0\\}$。\n- **输出格式**：\n    - 单行 `[[r_{1,1},\\dots,r_{1,12}],[r_{2,1},\\dots,r_{2,12}],[r_{3,1},\\dots,r_{3,12}]]`。\n\n### 步骤2：使用提取的已知信息进行验证\n- **科学依据**：该问题植根于凸优化这一成熟领域，特别是用于矩阵补全的近端方法。核范数、近端算子和奇异值分解（SVD）等概念是该领域的基础。所描述的算法是核范数最小化的一个著名迭代格式。\n- **适定性**：问题定义清晰。它提供了所有必要的参数、一个明确的迭代公式、一个确定性的数据生成协议（通过固定种子）以及精确的输出格式。这确保了可以计算出唯一且有意义的解。\n- **客观性**：问题以形式化的数学语言陈述，没有歧义、主观性或基于观点的论断。\n\n问题陈述是自包含、一致的，并遵循既定的科学原理。未发现任何缺陷。\n\n### 步骤3：结论与行动\n问题是**有效的**。将提供完整解决方案。\n\n### 求解推导与方法\n\n该迭代格式的核心是计算核范数的近端算子，$X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$。问题陈述中的符号表示，这可以解释为函数 $f(X) = \\lambda \\|X\\|_*$ 在参数 $t=1$ 时的近端算子。根据所提供的近端算子定义，这对应于求解以下优化问题：\n$$\nX^{k+1} = \\arg\\min_{X \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|X\\|_* + \\frac{1}{2}\\|X - Y^k\\|_F^2 \\right\\}\n$$\n该问题有一个基于 $Y^k$ 的奇异值分解（SVD）的闭式解。设 $Y^k$ 的 SVD 为 $Y^k = U \\Sigma V^T$，其中 $U \\in \\mathbb{R}^{m \\times r}$ 和 $V \\in \\mathbb{R}^{n \\times r}$ 是具有正交列的矩阵，$\\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_r)$ 是由正奇异值 $\\sigma_i > 0$ 构成的对角矩阵，且 $r = \\operatorname{rank}(Y^k)$。\n\n核范数和弗罗贝尼乌斯范数是酉不变的。这意味着对于任意正交矩阵 $A$ 和 $B$，都有 $\\|Z\\|_* = \\|A^T Z B\\|_*$ 和 $\\|Z\\|_F = \\|A^T Z B\\|_F$。将此性质应用于 $A=U$ 和 $B=V$，优化问题可以根据变换后的变量 $\\hat{X} = U^T X V$ 重写为：\n$$\n\\arg\\min_{\\hat{X} \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|\\hat{X}\\|_* + \\frac{1}{2}\\|\\hat{X} - U^T Y^k V\\|_F^2 \\right\\}\n$$\n由于 $Y^k = U \\Sigma V^T$，我们有 $U^T Y^k V = \\Sigma$。问题简化为：\n$$\n\\arg\\min_{\\hat{X} \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|\\hat{X}\\|_* + \\frac{1}{2}\\|\\hat{X} - \\Sigma\\|_F^2 \\right\\}\n$$\n可以证明（例如，通过 von Neumann 迹不等式），对于一个固定的对角矩阵 $\\Sigma$，当 $\\hat{X}$ 也是一个对角矩阵时，可以达到最小值。设 $\\hat{X} = D = \\operatorname{diag}(d_1, d_2, \\dots, d_r)$。$D$ 的奇异值就是 $|d_i|$。为了最小化 $\\|\\hat{X}\\|_*$，我们应选择 $d_i \\ge 0$。问题随后解耦为一组独立的标量最小化问题，每个奇异值对应一个：\n$$\n\\min_{d_i \\ge 0} \\left\\{ \\lambda d_i + \\frac{1}{2}(d_i - \\sigma_i)^2 \\right\\} \\quad \\text{for } i=1, \\dots, r\n$$\n目标函数是关于 $d_i$ 的一个简单二次函数。其关于 $d_i$ 的导数是 $\\lambda + d_i - \\sigma_i$。令导数为零得到一个候选解 $d_i = \\sigma_i - \\lambda$。由于我们要求 $d_i \\ge 0$，最优解是 $d_i = \\max(0, \\sigma_i - \\lambda)$。这个操作被称为软阈值算子，记为 $S_\\lambda(\\sigma_i) = (\\sigma_i - \\lambda)_+$.\n\n因此，最优对角矩阵是 $\\hat{X}_{\\text{opt}} = S_\\lambda(\\Sigma) = \\operatorname{diag}(S_\\lambda(\\sigma_1), \\dots, S_\\lambda(\\sigma_r))$。为了找到最终解 $X^{k+1}$，我们变换回原始坐标系：\n$$\nX^{k+1} = U \\hat{X}_{\\text{opt}} V^T = U S_\\lambda(\\Sigma) V^T\n$$\n这个过程被称为奇异值阈值化（SVT）。\n\n完整的算法如下：\n1.  **初始化**：根据指定的确定性协议生成基准矩阵 $X_{\\text{true}}$、观测矩阵 $M$ 和观测掩码 $\\Omega$。设置 $X^0 = 0$。\n2.  **迭代**：对于每个指定的 $\\lambda$ 值，以及 $k=0, \\dots, K-1$：\n    a.  **数据更新**：形成矩阵 $Y^k = M + P_{\\Omega^c}(X^k)$。此步骤保留了来自 $M$ 的已知条目，同时对未知条目使用当前估计值 $X^k$。\n    b.  **SVD**：计算 $Y^k$ 的 SVD：$Y^k = U \\Sigma V^T$。\n    c.  **阈值化**：对奇异值应用软阈值算子：$\\Sigma_{\\text{thresh}} = S_\\lambda(\\Sigma)$，其中对角线条目为 $(\\Sigma_{\\text{thresh}})_{ii} = \\max(0, \\Sigma_{ii} - \\lambda)$。\n    d.  **重构**：形成下一个迭代值 $X^{k+1} = U \\Sigma_{\\text{thresh}} V^T$。\n    e.  **秩计算**：计算 $\\operatorname{rank}(X^{k+1})$，其值为 $\\Sigma_{\\text{thresh}}$ 中大于容差 $10^{-8}$ 的对角线条目的数量。\n3.  **存储结果**：对于每个 $\\lambda$，存储计算出的秩序列 $\\{\\operatorname{rank}(X^1), \\dots, \\operatorname{rank}(X^{12})\\}$。最终输出将是一个包含这三个序列的列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the proximal point algorithm for matrix completion and\n    evaluates rank trajectories for different regularization weights.\n    \"\"\"\n    # Parameters from the problem statement\n    m, n = 20, 15\n    r_true = 3\n    singular_values_true = np.array([10.0, 5.0, 3.0])\n    p_obs = 0.5\n    noise_std = 0.01\n    K = 12\n    rank_tol = 1e-8\n    lambda_vals = [0.0, 1.0, 5.0]\n    seed = 0\n\n    # Data generation protocol (deterministic)\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct ground-truth low-rank matrix X_true\n    # Generate random matrices and use QR to get orthonormal columns\n    U_rand = rng.standard_normal((m, r_true))\n    U, _ = np.linalg.qr(U_rand)\n    \n    V_rand = rng.standard_normal((n, r_true))\n    V, _ = np.linalg.qr(V_rand)\n    V_t = V.T\n    \n    X_true = U @ np.diag(singular_values_true) @ V_t\n\n    # 2. Generate observation mask Omega\n    omega_mask = rng.random((m, n))  p_obs\n    omega_mask_c = ~omega_mask\n\n    # 3. Form the observed matrix M\n    noise = rng.normal(loc=0.0, scale=noise_std, size=(m, n))\n    M = np.zeros((m, n))\n    M[omega_mask] = (X_true + noise)[omega_mask]\n\n    all_rank_trajectories = []\n\n    # Investigation loop over lambda values\n    for lambda_val in lambda_vals:\n        rank_trajectory = []\n        X = np.zeros((m, n))\n\n        # Iterative scheme\n        for _ in range(K):\n            # 1. Data-consistency injection\n            Y = M + X * omega_mask_c\n\n            # 2. Proximal step\n            # Compute SVD of Y\n            try:\n                U_svd, s_svd, Vh_svd = np.linalg.svd(Y, full_matrices=False)\n            except np.linalg.LinAlgError:\n                # Handle cases where SVD might fail, though unlikely here\n                s_svd = np.array([])\n                U_svd = np.zeros((m, 0))\n                Vh_svd = np.zeros((0, n))\n\n            # Apply soft-thresholding to singular values\n            s_thresh = np.maximum(0, s_svd - lambda_val)\n            \n            # Reconstruct the matrix X_next\n            # Create a diagonal matrix of the correct size for multiplication\n            Sigma_thresh = np.zeros((len(s_thresh), len(s_thresh)))\n            np.fill_diagonal(Sigma_thresh, s_thresh)\n            \n            X_next = U_svd @ Sigma_thresh @ Vh_svd\n\n            # Compute rank of X_next\n            rank = np.sum(s_thresh > rank_tol)\n            rank_trajectory.append(rank)\n\n            # Update X for the next iteration\n            X = X_next\n            \n        all_rank_trajectories.append(rank_trajectory)\n\n    # Final print statement in the exact required format\n    inner_strs = [f\"[{','.join(map(str, r))}]\" for r in all_rank_trajectories]\n    final_output = f\"[{','.join(inner_strs)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "虽然核范数正则化非常普遍，但它并非唯一的选择，在某些条件下也未必是最佳选择。本练习将探索另一种强大的正则化方法——最大范数（max-norm）正则化 。通过模拟生成具有“尖峰”特征（即非均匀性）的数据并比较两种方法的泛化误差，你将学会如何批判性地评估不同正则化器对特定数据结构的适用性，从而做出更优的模型选择。",
            "id": "3145784",
            "problem": "您必须编写一个完整、可运行的程序，模拟在不同奇异向量尖峰度水平下，使用两种不同正则化器的矩阵补全过程，并通过在观测条目与未观测条目上进行训练/测试分割，评估哪种正则化器能更好地控制过拟合。其教学背景是统计学习中的矩阵分解与矩阵补全。该程序必须实现以下设置并产生指定的输出。\n\n从以下核心定义和原则开始。考虑一个秩为 $r$ 的目标矩阵 $X^{\\star} \\in \\mathbb{R}^{m \\times n}$。其一部分条目在带有加性噪声的情况下被观测到，目标是从这些带噪观测中估计 $X^{\\star}$。观测索引集表示为 $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$，经验风险是在观测条目上的均方误差。一种常见的方法是采用低秩矩阵分解 $X = U V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times r}$，$V \\in \\mathbb{R}^{n \\times r}$，然后通过正则化来控制复杂度。考虑以下两种正则化器：\n\n1. 迹范数（也称核范数）正则化，通过一个基于分解的惩罚项来实现，该惩罚项控制 $U$ 和 $V$ 的弗罗贝尼乌斯范数。\n\n2. 最大范数正则化，通过约束条件来实现，该约束条件控制 $U$ 和 $V$ 的最大行$\\ell_{2}$范数。\n\n程序必须通过构建 $X^{\\star} = A B^{\\top}$ 来模拟 $X^{\\star}$ 奇异向量中的尖峰度，其中 $A \\in \\mathbb{R}^{m \\times r}$ 和 $B \\in \\mathbb{R}^{n \\times r}$ 是从随机矩阵生成的，其行范数经过缩放以引入预设的尖峰度水平。设尖峰度参数 $s \\in [0,1]$ 控制行相干性。具体来说，构建一个权重向量 $w^{A} \\in \\mathbb{R}^{m}$，其中 $w^{A}_{1} = 1 + s \\cdot (m - 1)$ 且对于所有 $i \\in \\{2,\\dots,m\\}$ 有 $w^{A}_{i} = 1$，然后将其归一化，使其算术平均值为 $1$，即用 $w^{A} \\cdot \\frac{m}{\\sum_{i=1}^{m} w^{A}_{i}}$ 替换 $w^{A}$。类似地，构建 $w^{B} \\in \\mathbb{R}^{n}$，其中 $w^{B}_{1} = 1 + s \\cdot (n - 1)$ 且对于所有 $j \\in \\{2,\\dots,n\\}$ 有 $w^{B}_{j} = 1$，然后将其归一化，使其算术平均值为 $1$。通过从标准正态分布中独立抽取每个条目来生成 $A$ 和 $B$，并将 $A$ 的行乘以 $\\sqrt{w^{A}_{i}}$，$B$ 的行乘以 $\\sqrt{w^{B}_{j}}$。最后，缩放 $X^{\\star}$ 使其 $\\lVert X^{\\star} \\rVert_{F}$ 等于 $\\sqrt{m n}$。\n\n观测值的生成方式如下。对于给定的观测比例 $p \\in (0,1)$，采样一个掩码 $W \\in \\{0,1\\}^{m \\times n}$，其条目为独立的参数为 $p$ 的伯努利分布（即 $W_{ij} \\sim \\mathrm{Bernoulli}(p)$）。设噪声标准差为 $\\sigma  0$。观测数据矩阵 $Y$ 定义为 $Y_{ij} = W_{ij} \\cdot \\left( X^{\\star}_{ij} + \\varepsilon_{ij} \\right)$，其中 $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^{2})$ 独立同分布。在训练中，通过与 $W$ 进行逐元素相乘，只有 $W_{ij} = 1$ 的条目才被包含在损失中。\n\n您必须实现并训练两个形式为 $U V^{\\top}$ 且秩为固定值 $r$ 的估计器 $X_{\\mathrm{trace}}$ 和 $X_{\\mathrm{max}}$，使用基于梯度的优化方法：\n\n- 迹范数代理估计器：最小化观测条目上的平方误差总和，外加一个与 $\\lVert U \\rVert_{F}^{2} + \\lVert V \\rVert_{F}^{2}$ 成比例的惩罚项，使用固定的正则化参数 $\\lambda  0$ 以及固定的学习率和迭代次数。\n\n- 最大范数代理估计器：最小化观测条目上的平方误差总和，但需满足 $U$ 和 $V$ 的最大行$\\ell_{2}$范数受固定参数 $\\alpha  0$ 限制的约束条件。实现方法为：在观测条目上对平方误差执行梯度步骤，并在每步之后，将 $U$ 和 $V$ 的每一行投影到以原点为中心、半径为 $\\alpha$ 的闭$\\ell_{2}$球上。\n\n为确保公平比较，在每个测试案例开始时，为两个估计器使用相同的随机矩阵初始化 $U$ 和 $V$。训练后，计算每个估计器的泛化误差，即在未观测条目（即掩码 $W$ 的补集）上的均方误差，使用无噪声的 $X^{\\star}$ 作为真实值。将一个测试案例的最终决策定义为一个布尔值，当且仅当最大范数代理估计器获得的泛化误差严格小于迹范数代理估计器时，该值为真。\n\n测试套件。您的程序必须无需任何外部输入，运行以下三个测试案例，每个案例都由整数和浮点数完全指定：\n\n- 案例 1：$(m,n,r,s,p,\\sigma) = (30,30,2,0.0,0.4,0.1)$，随机种子为 $42$。\n\n- 案例 2：$(m,n,r,s,p,\\sigma) = (30,30,2,0.9,0.4,0.1)$，随机种子为 $43$。\n\n- 案例 3：$(m,n,r,s,p,\\sigma) = (30,30,2,0.9,0.1,0.1)$，随机种子为 $44$。\n\n在所有测试案例中均使用相同的固定超参数：正则化强度 $\\lambda = 0.08$，最大行范数界限 $\\alpha = 0.9$，一个恒定的学习率（选择一个合理的较小正值），以及固定的迭代次数（选择一个合理的正整数，大到足以在实践中确保收敛）。确保优化过程中的数值稳定性。\n\n最终输出格式。您的程序应产生单行输出，其中包含三个案例的布尔值列表，指明在每个案例中最大范数代理估计器的泛化误差是否严格小于迹范数代理估计器。该行必须是一个用方括号括起来、不含空格的逗号分隔列表，例如 `[True,False,True]`。不应打印任何其他文本。",
            "solution": "该问题要求对两种用于低秩矩阵补全的正则化方法进行比较分析：一种是迹范数代理方法，另一种是最大范数代理方法。该分析通过一项模拟研究进行，研究中改变了真实矩阵奇异向量的“尖峰度”以及观测条目的比例。\n\n### 1. 数据生成模型\n\n模拟的基础是生成一个具有受控属性的、秩为 $r$ 的合成真实矩阵 $X^{\\star} \\in \\mathbb{R}^{m \\times n}$。\n\n**1.1. 尖峰奇异向量的构建**\n奇异向量的结构由一个尖峰度参数 $s \\in [0,1]$ 控制。$s=0$ 的值对应于非尖峰情况，此时行范数期望是均匀的；而 $s  0$ 则会引入一个尖峰，使得因子矩阵的第一行的范数大于其他行。行范数的这种不均匀性是矩阵补全理论中一个关键概念——非均匀相干性的一个代理指标。\n\n目标矩阵构建为 $X^{\\star} = A B^{\\top}$，其中 $A \\in \\mathbb{R}^{m \\times r}$ 且 $B \\in \\mathbb{R}^{n \\times r}$。过程如下：\n\n1.  **定义权重向量**：定义两个权重向量 $w^{A} \\in \\mathbb{R}^{m}$ 和 $w^{B} \\in \\mathbb{R}^{n}$，以控制矩阵 $A$ 和 $B$ 的行范数。对于尖峰度参数 $s$，未归一化的权重为：\n    $$\n    w^{A}_{1} = 1 + s \\cdot (m - 1), \\quad w^{A}_{i} = 1 \\text{ for } i \\in \\{2,\\dots,m\\}\n    $$\n    $$\n    w^{B}_{1} = 1 + s \\cdot (n - 1), \\quad w^{B}_{j} = 1 \\text{ for } j \\in \\{2,\\dots,n\\}\n    $$\n\n2.  **归一化权重**：然后将这些向量进行归一化，使其算术平均值为 $1$。$w^A$ 的归一化因子为 $\\frac{m}{\\sum_{i=1}^{m} w^{A}_{i}}$，$w^B$ 也类似。其和为 $\\sum_{i=1}^{m} w^{A}_{i} = (1 + s(m-1)) + (m-1) = m + s(m-1)$。因此，归一化后的权重为：\n    $$\n    w^{A}_{\\text{norm}} = w^A \\cdot \\frac{m}{m + s(m-1)}\n    $$\n    $$\n    w^{B}_{\\text{norm}} = w^B \\cdot \\frac{n}{n + s(n-1)}\n    $$\n\n3.  **生成并缩放因子**：首先从标准正态分布中抽取矩阵 $A$ 和 $B$ 的元素（称之为 $\\tilde{A}$ 和 $\\tilde{B}$），然后将每一行乘以对应归一化权重的平方根来生成它们：\n    $$\n    A_{i,:} = \\tilde{A}_{i,:} \\cdot \\sqrt{w^{A}_{\\text{norm},i}} \\quad \\text{for } i=1,\\dots,m\n    $$\n    $$\n    B_{j,:} = \\tilde{B}_{j,:} \\cdot \\sqrt{w^{B}_{\\text{norm},j}} \\quad \\text{for } j=1,\\dots,n\n    $$\n    现在，$A$ 的第 $i$ 行的期望平方 $\\ell_2$-范数与 $w^{A}_{\\text{norm},i}$ 成正比。\n\n4.  **构建并归一化目标矩阵**：真实矩阵为 $X^{\\star} = AB^{\\top}$。最后，对 $X^{\\star}$进行缩放，使其弗罗贝尼乌斯范数为 $\\sqrt{mn}$，这确保了其元素的均方根为 $1$。\n    $$\n    X^{\\star} \\leftarrow X^{\\star} \\cdot \\frac{\\sqrt{mn}}{\\lVert X^{\\star} \\rVert_{F}}\n    $$\n\n**1.2. 观测模型**\n$X^{\\star}$ 的一部分条目被观测到，并受到加性高斯噪声的干扰。\n- 生成一个观测掩码 $W \\in \\{0,1\\}^{m \\times n}$，其中每个元素 $W_{ij}$ 是一个独立的伯努利随机变量，参数为 $p$，即 $W_{ij} \\sim \\mathrm{Bernoulli}(p)$。$W_{ij}=1$ 表示一个观测条目。\n- 观测数据矩阵 $Y \\in \\mathbb{R}^{m \\times n}$ 的形成方式是，取 $X^{\\star}$ 中与掩码 $W$ 对应的条目，并加上独立同分布的噪声 $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$。问题定义 $Y_{ij} = W_{ij} \\cdot (X^{\\star}_{ij} + \\varepsilon_{ij})$。为了优化，更方便的做法是定义一个完整的含噪矩阵 $Y_{full} = X^{\\star} + \\mathcal{E}$，其中 $\\mathcal{E}$ 是噪声项矩阵，然后使用掩码 $W$ 来选择用于损失函数的条目。\n\n### 2. 估计器与优化算法\n\n目标是使用低秩分解 $X = UV^{\\top}$ 来估计 $X^{\\star}$，其中 $U \\in \\mathbb{R}^{m \\times r}$ 且 $V \\in \\mathbb{R}^{n \\times r}$。比较了两种正则化方案。两者都使用基于梯度的优化方法进行训练，学习率固定为 $\\eta=0.005$，迭代次数为 $2000$ 次。选择这些超参数是考虑到问题规模的合理性。\n\n**2.1. 迹范数代理估计器 ($X_{\\mathrm{trace}}$)**\n矩阵的迹范数是其奇异值之和。一个常见的用于秩最小化的凸松弛是最小化迹范数。对于分解 $X=UV^\\top$，惩罚项 $\\lambda(\\lVert U \\rVert_F^2 + \\lVert V \\rVert_F^2)$ 是迹范数惩罚 $\\lambda \\lVert X \\rVert_*$ 的一个非凸代理。需要最小化的目标函数是在观测条目上的正则化平方误差和：\n$$\n\\mathcal{L}_{\\mathrm{trace}}(U, V) = \\frac{1}{2}\\left\\| W \\odot (UV^{\\top} - Y_{full}) \\right\\|_{F}^{2} + \\frac{\\lambda}{2} (\\left\\| U \\right\\|_{F}^{2} + \\left\\| V \\right\\|_{F}^{2})\n$$\n其中 $\\odot$ 表示逐元素（哈达玛）积，$\\lambda=0.08$ 是正则化参数。\n\n优化使用梯度下降法进行。目标函数关于 $U$ 和 $V$ 的梯度为：\n$$\n\\nabla_{U} \\mathcal{L}_{\\mathrm{trace}} = (W \\odot (UV^{\\top} - Y_{full})) V + \\lambda U\n$$\n$$\n\\nabla_{V} \\mathcal{L}_{\\mathrm{trace}} = (W \\odot (UV^{\\top} - Y_{full}))^{\\top} U + \\lambda V\n$$\n更新规则为 $U \\leftarrow U - \\eta \\nabla_{U} \\mathcal{L}_{\\mathrm{trace}}$ 和 $V \\leftarrow V - \\eta \\nabla_{V} \\mathcal{L}_{\\mathrm{trace}}$。\n\n**2.2. 最大范数代理估计器 ($X_{\\mathrm{max}}$)**\n最大范数正则化通过约束因子矩阵 $U$ 和 $V$ 的最大行$\\ell_2$-范数来控制模型的复杂度。这种方法已被证明是有效的，特别是在数据表现出不均匀性（尖峰度）时。优化问题是：\n$$\n\\text{minimize} \\quad \\mathcal{L}(U, V) = \\frac{1}{2}\\left\\| W \\odot (UV^{\\top} - Y_{full}) \\right\\|_{F}^{2}\n$$\n$$\n\\text{subject to} \\quad \\max_{i} \\|U_{i,:}\\|_{2} \\le \\alpha \\quad \\text{and} \\quad \\max_{j} \\|V_{j,:}\\|_{2} \\le \\alpha\n$$\n其中 $U_{i,:}$ 是 $U$ 的第 $i$ 行，$V_{j,:}$ 是 $V$ 的第 $j$ 行，$\\alpha=0.9$ 是最大行范数界限。\n\n这个约束问题使用投影梯度下降法（PGD）求解。每次迭代包括两个步骤：\n1.  **梯度步**：沿着无约束损失函数 $\\mathcal{L}(U,V)$ 的负梯度方向更新 $U$ 和 $V$。设中间更新为 $\\tilde{U}$ 和 $\\tilde{V}$。\n    $$\n    \\nabla_{U} \\mathcal{L} = (W \\odot (UV^{\\top} - Y_{full})) V \\quad \\Rightarrow \\quad \\tilde{U} = U - \\eta \\nabla_{U} \\mathcal{L}\n    $$\n    $$\n    \\nabla_{V} \\mathcal{L} = (W \\odot (UV^{\\top} - Y_{full}))^{\\top} U \\quad \\Rightarrow \\quad \\tilde{V} = V - \\eta \\nabla_{V} \\mathcal{L}\n    $$\n2.  **投影步**：将 $\\tilde{U}$ 和 $\\tilde{V}$ 的每一行投影回半径为 $\\alpha$ 的闭$\\ell_2$球上。对于 $\\tilde{U}$ 的每一行 $u_i$：\n    $$\n    U_{i,:} \\leftarrow u_i \\cdot \\min\\left(1, \\frac{\\alpha}{\\|u_i\\|_2}\\right)\n    $$\n    对 $\\tilde{V}$ 的每一行 $v_j$ 应用相同的投影。如果一行的范数为零，它将保持为零。\n\n### 3. 评估与比较\n\n每个估计器的性能通过其泛化误差来衡量，定义为在*未观测*条目集（表示为 $\\Omega^c = \\{(i,j) | W_{ij}=0\\}$）上的均方误差（MSE）。对于一个估计矩阵 $\\hat{X}=UV^\\top$，泛化误差为：\n$$\n\\text{MSE}_{\\text{unobs}}(\\hat{X}) = \\frac{\\sum_{(i,j) \\in \\Omega^c} (\\hat{X}_{ij} - X^{\\star}_{ij})^2}{|\\Omega^c|} = \\frac{\\left\\| (1-W) \\odot (\\hat{X} - X^{\\star}) \\right\\|_F^2}{\\sum_{i,j}(1-W_{ij})}\n$$\n对于每个测试案例，最终决策是一个布尔值，如果最大范数估计器获得的泛化误差严格小于迹范数估计器，即 $\\text{MSE}_{\\text{unobs}}(X_{\\mathrm{max}})  \\text{MSE}_{\\text{unobs}}(X_{\\mathrm{trace}})$，则为 `True`，否则为 `False`。理论结果表明，最大范数正则化对于尖峰度应该更具鲁棒性，而本模拟正是为此设计的测试。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the matrix completion simulation for all test cases.\n    \"\"\"\n    \n    # Define fixed hyperparameters for the optimization algorithms.\n    # A learning rate of 0.005 and 2000 iterations were chosen as reasonable\n    # values for convergence for the given problem size and parameters.\n    LEARNING_RATE = 0.005\n    ITERATIONS = 2000\n    LAMBDA_REG = 0.08  # Regularization for trace-norm surrogate\n    ALPHA_BOUND = 0.9   # Max row-norm bound for max-norm surrogate\n\n    test_cases = [\n        # (m, n, r, s, p, sigma, seed)\n        (30, 30, 2, 0.0, 0.4, 0.1, 42),\n        (30, 30, 2, 0.9, 0.4, 0.1, 43),\n        (30, 30, 2, 0.9, 0.1, 0.1, 44),\n    ]\n\n    results = []\n    \n    for m, n, r, s, p, sigma, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate Ground Truth Matrix X_star\n        # Generate weight vectors w_A and w_B for spikiness\n        w_A = np.ones(m)\n        w_A[0] = 1 + s * (m - 1)\n        w_A *= m / np.sum(w_A)\n        \n        w_B = np.ones(n)\n        w_B[0] = 1 + s * (n - 1)\n        w_B *= n / np.sum(w_B)\n\n        # Generate factor matrices A and B and scale rows\n        A = rng.standard_normal(size=(m, r)) * np.sqrt(w_A[:, np.newaxis])\n        B = rng.standard_normal(size=(n, r)) * np.sqrt(w_B[:, np.newaxis])\n        \n        X_star = A @ B.T\n        \n        # Normalize X_star to have Frobenius norm sqrt(mn)\n        norm_X_star = np.linalg.norm(X_star, 'fro')\n        if norm_X_star > 1e-9: # Avoid division by zero\n            X_star *= np.sqrt(m * n) / norm_X_star\n\n        # 2. Generate Observed Data\n        # Generate mask W for observed entries\n        W = rng.binomial(1, p, size=(m, n)).astype(np.float64)\n        \n        # Generate noise and the full noisy matrix Y_full\n        noise = rng.normal(0, sigma, size=(m, n))\n        Y_full = X_star + noise\n\n        # 3. Initialize Factor Matrices U and V for optimization\n        U_init = rng.standard_normal(size=(m, r))\n        V_init = rng.standard_normal(size=(n, r))\n\n        # 4. Train Trace-Norm Surrogate Estimator\n        U_trace, V_trace = U_init.copy(), V_init.copy()\n        for _ in range(ITERATIONS):\n            X_pred = U_trace @ V_trace.T\n            grad_loss_part = W * (X_pred - Y_full)\n            \n            grad_U = grad_loss_part @ V_trace + LAMBDA_REG * U_trace\n            grad_V = grad_loss_part.T @ U_trace + LAMBDA_REG * V_trace\n            \n            U_trace -= LEARNING_RATE * grad_U\n            V_trace -= LEARNING_RATE * grad_V\n\n        # 5. Train Max-Norm Surrogate Estimator\n        U_max, V_max = U_init.copy(), V_init.copy()\n        for _ in range(ITERATIONS):\n            X_pred = U_max @ V_max.T\n            grad_loss_part = W * (X_pred - Y_full)\n\n            # Gradient step (without regularization term)\n            grad_U = grad_loss_part @ V_max\n            grad_V = grad_loss_part.T @ U_max\n            \n            U_max -= LEARNING_RATE * grad_U\n            V_max -= LEARNING_RATE * grad_V\n\n            # Projection step\n            row_norms_U = np.linalg.norm(U_max, axis=1)\n            scales_U = np.minimum(1.0, ALPHA_BOUND / (row_norms_U + 1e-9))\n            U_max *= scales_U[:, np.newaxis]\n\n            row_norms_V = np.linalg.norm(V_max, axis=1)\n            scales_V = np.minimum(1.0, ALPHA_BOUND / (row_norms_V + 1e-9))\n            V_max *= scales_V[:, np.newaxis]\n\n        # 6. Evaluate Generalization Error\n        W_unobs = 1 - W\n        num_unobs = np.sum(W_unobs)\n\n        if num_unobs == 0:\n            # This case should not happen with p  1 and the given sizes,\n            # but is handled for robustness.\n            mse_trace, mse_max = 0.0, 0.0\n        else:\n            X_trace = U_trace @ V_trace.T\n            error_trace = np.sum(W_unobs * (X_trace - X_star)**2)\n            mse_trace = error_trace / num_unobs\n            \n            X_max = U_max @ V_max.T\n            error_max = np.sum(W_unobs * (X_max - X_star)**2)\n            mse_max = error_max / num_unobs\n        \n        # 7. Compare and store result\n        results.append(mse_max  mse_trace)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\".lower())\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "现实世界的数据往往伴随着丰富的辅助信息，善用这些信息可以显著提升模型性能。这个练习将以一个引人入胜的交通流量预测问题为例，演示如何将图结构信息融入矩阵分解模型 。你将实现一个图正则化矩阵补全算法，并亲眼见证利用道路网络拓扑信息如何能有效提高模型在关键任务（如预测交通高峰时刻）上的准确性，尤其是在数据稀疏的情况下。",
            "id": "3145702",
            "problem": "给定一个源于交通传感器网络的矩阵补全任务。考虑一个传感器-时间数据矩阵 $X \\in \\mathbb{R}^{n \\times T}$，其缺失条目由一个二元掩码 $M \\in \\{0,1\\}^{n \\times T}$ 指示，其中 $M_{i,t} = 1$ 表示 $X_{i,t}$ 是观测值，而 $M_{i,t} = 0$ 表示它是缺失值。您将使用一个低秩分解模型来补全该矩阵，并评估通过图正则化项引入道路网络拓扑对峰值时间预测准确性的影响。\n\n基本原理和定义：\n- 通过低秩分解进行矩阵补全，假设 $X \\approx U V^{\\top}$，其中对于选定的秩 $r$，有 $U \\in \\mathbb{R}^{n \\times r}$ 和 $V \\in \\mathbb{R}^{T \\times r}$。\n- 传感器上的道路网络拓扑由一个无向加权邻接矩阵 $W \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ 编码，其中 $W_{i,j} = W_{j,i}$ 且 $W_{i,i} = 0$。度矩阵为 $D = \\mathrm{diag}(W \\mathbf{1})$，图拉普拉斯算子为 $L = D - W$。\n- 矩阵补全问题被描述为最小化以下带惩罚的经验风险\n$$\n\\min_{U,V} \\;\\; \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2} + \\|V\\|_{F}^{2}\\right),\n$$\n其中 $\\odot$ 表示哈达玛积，$\\lambda_{\\mathrm{U}} \\ge 0$ 控制与图拓扑的对齐程度，$\\lambda  0$ 是一个岭正则化项。当 $\\lambda_{\\mathrm{U}} = 0$ 时，该问题简化为没有图正则化的标准低秩矩阵分解。\n\n任务：\n- 实现一个交替最小化求解器，对于固定的 $r$、$\\lambda_{\\mathrm{U}}$ 和 $\\lambda$，交替更新 $U$ 和 $V$，直到达到固定的迭代次数。使用下面指定的确定性初始化和确定性数据生成。\n- 补全矩阵得到 $\\widehat{X} = U V^{\\top}$ 后，对于每个传感器 $i \\in \\{1,\\dots,n\\}$，将其峰值时间索引定义为：对于真实值矩阵是 $p_i^{\\star} = \\arg\\max_{t \\in \\{1,\\dots,T\\}} X_{i,t}$，对于补全矩阵是 $\\widehat{p}_i = \\arg\\max_{t \\in \\{1,\\dots,T\\}} \\widehat{X}_{i,t}$。如果出现平局，则取最小的索引（$\\arg\\max$ 的默认行为）。\n- 将总峰值索引误差定义为 $E(\\widehat{X}; X) = \\sum_{i=1}^{n} \\left| \\widehat{p}_i - p_i^{\\star} \\right|$。\n- 对于下方的每个测试用例，计算图正则化分解相对于非正则化基线的峰值预测改进，结果为整数：\n$$\n\\Delta = E\\big(\\widehat{X}_{\\text{baseline}}; X\\big) - E\\big(\\widehat{X}_{\\text{graph}}; X\\big).\n$$\n正数 $\\Delta$ 表示图正则化提高了峰值预测的准确性。\n\n数据生成（确定性且通用）：\n- 维度：$n = 5$，$T = 8$，$r = 2$。\n- 真实值因子：\n  - 传感器因子 $U^{\\star} \\in \\mathbb{R}^{n \\times r}$：令位置 $p_i = \\frac{i-1}{n-1}$，其中 $i = 1,\\dots,n$。将第一列设置为 $U^{\\star}_{i,1} = p_i$，第二列对所有 $i$ 设置为 $U^{\\star}_{i,2} = 0.5$。\n  - 时间因子 $V^{\\star} \\in \\mathbb{R}^{T \\times r}$：令时间索引 $\\tau_t = t-1$，其中 $t = 1,\\dots,T$。设置 $V^{\\star}_{t,1} = \\exp\\!\\left(-\\frac{1}{2} \\left(\\frac{\\tau_t - 5}{1.0}\\right)^{2}\\right)$ 和 $V^{\\star}_{t,2} = 0.2 \\cdot \\frac{\\tau_t}{T}$。\n- 带微小噪声的真实值矩阵：$X = U^{\\star} (V^{\\star})^{\\top} + \\epsilon$，其中 $\\epsilon$ 的条目是使用种子为 $s_{\\mathrm{truth}} = 1$ 的伪随机数生成器从均值为零、标准差为 $0.01$ 的高斯分布中独立抽取的。\n- 基础观测掩码 $M^{\\mathrm{base}} \\in \\{0,1\\}^{n \\times T}$：使用种子为 $s_{\\mathrm{mask}} = 0$ 的伪随机数生成器在 $\\left[0,1\\right)$ 中抽取独立的均匀随机数，如果抽到的数不小于 $0.4$，则设置 $M^{\\mathrm{base}}_{i,t} = 1$，否则设置 $M^{\\mathrm{base}}_{i,t} = 0$（因此期望的观测比例为 $0.6$）。\n- 算法的确定性初始化：在每次运行开始时，使用种子为 $s_{\\mathrm{init}} = 123$ 的伪随机数生成器，用均值为零、标准差为 $0.1$ 的高斯分布条目独立地初始化 $U$ 和 $V$。\n\n道路网络图：\n- $n$ 个传感器上的路径图：$W \\in \\mathbb{R}^{n \\times n}$，其中对于 $i \\in \\{1,\\dots,n-1\\}$ 有 $W_{i,i+1} = W_{i+1,i} = 1$，其余 $W_{i,j} = 0$。\n- 空图：$W = 0$（因此 $L = 0$）。\n\n算法设置：\n- 使用 $\\lambda = 0.1$，迭代次数 $N_{\\mathrm{iter}} = 50$。\n- 对于基线模型，设置 $\\lambda_{\\mathrm{U}} = 0$；对于图正则化运行，设置 $\\lambda_{\\mathrm{U}} = 5.0$。\n\n测试套件：\n- 情况 1（标准情况）：$W$ 等于路径图，$M = M^{\\mathrm{base}}$。\n- 情况 2（冷启动传感器）：$W$ 等于路径图，$M$ 等于 $M^{\\mathrm{base}}$，但将整个第三行设置为零，即对于所有 $t \\in \\{1,\\dots,T\\}$，$M_{3,t} = 0$。\n- 情况 3（完全观测）：$W$ 等于路径图，$M$ 是形状为 $n \\times T$ 的全一矩阵。\n- 情况 4（无图信息）：$W = 0$，$M = M^{\\mathrm{base}}$。\n\n程序要求：\n- 实现一个交替最小化求解器，在更新 $V$ 和 $U$ 之间交替进行：\n  - $V$ 的更新必须为每个时间索引求解一个岭正则化加权最小二乘问题，其权重来自 $M$ 的相应列。\n  - $U$ 的更新必须在给定 $V$ 的情况下精确最小化关于 $U$ 的目标函数，这需要通过图拉普拉斯算子 $L$ 求解一个耦合所有传感器潜向量的线性系统。\n- 对于每个测试用例，从相同的初始化开始运行基线模型和图正则化求解器，补全矩阵得到 $\\widehat{X}$，计算总峰值索引误差 $E(\\widehat{X}; X)$，并报告整数改进值 $\\Delta$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来，按情况 1 到 4 的顺序排列结果，例如 $\\left[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4\\right]$。",
            "solution": "该问题要求实现一种基于低秩分解的图正则化矩阵补全算法。目标是评估图正则化项对模拟传感器网络中预测峰值时间事件准确性的影响。解决方案通过交替最小化方法找到。\n\n需要最小化的目标函数是带惩罚的经验风险：\n$$\nJ(U, V) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2} + \\|V\\|_{F}^{2}\\right)\n$$\n这里，$X \\in \\mathbb{R}^{n \\times T}$ 是数据矩阵，$M \\in \\{0,1\\}^{n \\times T}$ 是观测掩码，$U \\in \\mathbb{R}^{n \\times r}$ 和 $V \\in \\mathbb{R}^{T \\times r}$ 是潜因子，$L \\in \\mathbb{R}^{n \\times n}$ 是编码传感器拓扑的图拉普拉斯算子，$\\lambda_{\\mathrm{U}}, \\lambda$ 是正则化参数。项 $\\mathrm{tr}\\!\\left(U^{\\top} L U\\right)$ 促使相连传感器的潜因子相似。\n\n该优化问题在 $U$ 和 $V$ 上不是联合凸的。但是，它是双凸的，即对于固定的 $V$，问题在 $U$ 上是凸的；对于固定的 $U$，问题在 $V$ 上是凸的。这种结构启发我们采用交替最小化方法，即迭代地求解一个因子矩阵，同时保持另一个因子矩阵固定。\n\n### 交替最小化算法\n\n我们初始化 $U$ 和 $V$，然后在固定的迭代次数内交替执行两个主要步骤。\n\n#### 1. 更新 V (固定 U)\n对于固定的 $U$，目标函数中依赖于 $V$ 的项是：\n$$\nJ(V) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda}{2} \\|V\\|_{F}^{2} = \\frac{1}{2} \\sum_{t=1}^{T} \\sum_{i=1}^{n} M_{i,t} (X_{i,t} - \\mathbf{u}_i^{\\top} \\mathbf{v}_t)^2 + \\frac{\\lambda}{2} \\sum_{t=1}^{T} \\|\\mathbf{v}_t\\|_2^2\n$$\n其中 $\\mathbf{u}_i$ 是 $U$ 的第 $i$ 行，$\\mathbf{v}_t$ 是 $V$ 的第 $t$ 行。该问题按时间索引 $t$ 解耦。对于每个 $t \\in \\{1, \\dots, T\\}$，我们最小化：\n$$\nJ_t(\\mathbf{v}_t) = \\frac{1}{2} \\sum_{i=1}^{n} M_{i,t} (X_{i,t} - \\mathbf{u}_i^{\\top} \\mathbf{v}_t)^2 + \\frac{\\lambda}{2} \\|\\mathbf{v}_t\\|_2^2\n$$\n这是一个岭正则化加权最小二乘问题。令 $M_t = \\mathrm{diag}(M_{1,t}, \\dots, M_{n,t})$ 为时间 $t$ 的权重对角矩阵。$\\mathbf{v}_t$ 的问题等价于最小化 $\\frac{1}{2}\\|\\sqrt{M_t}(\\mathbf{x}_t - U\\mathbf{v}_t)\\|_2^2 + \\frac{\\lambda}{2}\\|\\mathbf{v}_t\\|_2^2$，其中 $\\mathbf{x}_t$ 是 $X$ 的第 $t$ 列。对 $\\mathbf{v}_t$ 求梯度并令其为零，得到正规方程：\n$$\n\\left( U^{\\top} M_t U + \\lambda I_r \\right) \\mathbf{v}_t = U^{\\top} M_t \\mathbf{x}_t\n$$\n其中 $I_r$ 是 $r \\times r$ 的单位矩阵。通过求解这个 $r \\times r$ 的线性系统可以找到更新后的 $\\mathbf{v}_t$。这可以更明确地表示为：令 $\\mathcal{I}_t = \\{ i \\mid M_{i,t} = 1 \\}$ 为在时间 $t$ 观测到的传感器集合。令 $U_{\\mathcal{I}_t}$ 为 $U$ 的行索引在 $\\mathcal{I}_t$ 中的子矩阵。方程为：\n$$\n\\left( U_{\\mathcal{I}_t}^{\\top} U_{\\mathcal{I}_t} + \\lambda I_r \\right) \\mathbf{v}_t = U_{\\mathcal{I}_t}^{\\top} \\mathbf{x}_{\\mathcal{I}_t, t}\n$$\n\n#### 2. 更新 U (固定 V)\n对于固定的 $V$，目标函数中依赖于 $U$ 的项是：\n$$\nJ(U) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\|U\\|_{F}^{2}\n$$\n与 V-更新不同，该问题不能按传感器索引 $i$ 解耦，因为图正则化项 $\\mathrm{tr}(U^{\\top} L U) = \\sum_{i,j} L_{ij} \\mathbf{u}_i^{\\top} \\mathbf{u}_j$ 耦合了 $U$ 的各行。我们必须同时求解 $U$ 的所有行。对单行 $\\mathbf{u}_i$ 求梯度并令其为零，得到：\n$$\n\\left( \\sum_{t=1}^{T} M_{i,t} \\mathbf{v}_t \\mathbf{v}_t^{\\top} + \\lambda I_r \\right) \\mathbf{u}_i + \\lambda_{\\mathrm{U}} \\sum_{j=1}^{n} L_{ij} \\mathbf{u}_j = \\sum_{t=1}^{T} M_{i,t} X_{i,t} \\mathbf{v}_t\n$$\n这给出了一个包含 $n$ 个向量 $\\mathbf{u}_1, \\dots, \\mathbf{u}_n$ 的 $n$ 个耦合线性方程组。我们可以将其表示为单个大型线性系统。令 $\\mathbf{u} = \\mathrm{vec}(U) \\in \\mathbb{R}^{nr}$ 是通过堆叠 $U$ 的行得到的向量。该系统形如 $\\mathcal{A} \\mathbf{u} = \\mathbf{b}$，其中 $\\mathcal{A}$ 是一个 $nr \\times nr$ 的块矩阵，$\\mathbf{b}$ 是一个长度为 $nr$ 的向量。$\\mathcal{A}$ 的第 $(i,j)$ 个块（一个 $r \\times r$ 矩阵）由下式给出：\n$$\n\\mathcal{A}_{ij} = \\delta_{ij} \\left( \\sum_{t=1}^{T} M_{i,t} \\mathbf{v}_t \\mathbf{v}_t^{\\top} + \\lambda I_r \\right) + \\lambda_{\\mathrm{U}} L_{ij} I_r\n$$\n其中 $\\delta_{ij}$ 是克罗内克δ。向量 $\\mathbf{b}$ 的第 $i$ 个块是 $\\mathbf{b}_i = \\sum_{t=1}^{T} M_{i,t} X_{i,t} \\mathbf{v}_t$。矩阵 $\\mathcal{A}$ 是对称且正定的（因为 $L$ 是半正定的且 $\\lambda  0$），这确保了 $U$ 的唯一解。通过求解这个 $nr \\times nr$ 的线性系统并重塑结果向量，即可获得更新后的 $U$。\n\n### 评估\n在运行交替最小化固定次数的迭代后，补全的矩阵为 $\\widehat{X} = UV^{\\top}$。通过比较预测的峰值时间索引 $\\widehat{p}_i = \\arg\\max_t \\widehat{X}_{i,t}$ 与真实值的峰值时间索引 $p_i^{\\star} = \\arg\\max_t X_{i,t}$ 来评估性能。总峰值索引误差为 $E(\\widehat{X}; X) = \\sum_{i=1}^{n} | \\widehat{p}_i - p_i^{\\star} |$。最终报告的指标是图正则化带来的改进 $\\Delta$，定义为基线模型（$\\lambda_{\\mathrm{U}}=0$）和图正则化模型（$\\lambda_{\\mathrm{U}}0$）误差之差：$\\Delta = E_{\\text{baseline}} - E_{\\text{graph}}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\n#   - name: scipy\n#     version: 1.11.4\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the matrix completion task for all test cases.\n    \"\"\"\n    # Problem Parameters\n    n, T, r = 5, 8, 2\n    lambda_reg = 0.1\n    n_iter = 50\n    lambda_U_graph = 5.0\n    s_truth, s_mask, s_init = 1, 0, 123\n\n    def alternating_minimization(X, M, L, lambda_U, n, T, r, lambda_reg, n_iter, seed_init):\n        \"\"\"\n        Implements the alternating minimization solver for graph-regularized matrix completion.\n        \"\"\"\n        rng_init = np.random.default_rng(seed=seed_init)\n        U = rng_init.normal(0, 0.1, size=(n, r))\n        V = rng_init.normal(0, 0.1, size=(T, r))\n\n        for _ in range(n_iter):\n            # 1. V-update (holding U fixed)\n            # This part solves a separate ridge regression problem for each time point t.\n            for t in range(T):\n                observed_sensors = M[:, t]\n                if not np.any(observed_sensors):\n                    # No observations at this time point, impute with zero vector\n                    V[t, :] = 0.0\n                    continue\n                \n                U_obs = U[observed_sensors, :]\n                X_obs = X[observed_sensors, t]\n                \n                # A_t = U_obs.T @ U_obs + lambda * I\n                A_t = U_obs.T.dot(U_obs) + lambda_reg * np.identity(r)\n                # b_t = U_obs.T @ X_obs_t\n                b_t = U_obs.T.dot(X_obs)\n                \n                # Solve A_t v_t = b_t\n                V[t, :] = np.linalg.solve(A_t, b_t)\n\n            # 2. U-update (holding V fixed)\n            # This part solves one large linear system for all of U.\n            A_u = lambda_U * np.kron(L, np.identity(r))\n            b_u = np.zeros(n * r)\n\n            for i in range(n):\n                observed_times = M[i, :]\n                if not np.any(observed_times):\n                    # Cold-start sensor: no observations\n                    C_i = lambda_reg * np.identity(r)\n                    b_i = np.zeros(r)\n                else:\n                    V_obs = V[observed_times, :]\n                    X_obs = X[i, observed_times]\n                    \n                    # C_i = V_obs.T @ V_obs + lambda * I\n                    C_i = V_obs.T.dot(V_obs) + lambda_reg * np.identity(r)\n                    # b_i = V_obs.T @ X_obs_i\n                    b_i = V_obs.T.dot(X_obs)\n                \n                # Fill the block matrix A_u and vector b_u\n                A_u[i*r:(i+1)*r, i*r:(i+1)*r] += C_i\n                b_u[i*r:(i+1)*r] = b_i\n            \n            # Solve A_u u = b_u and reshape to get U\n            u_vec = np.linalg.solve(A_u, b_u)\n            U = u_vec.reshape((n, r))\n        \n        return U.dot(V.T)\n\n    # --- Data Generation ---\n    # Ground truth factors U_star, V_star\n    p_i = np.linspace(0, 1, n)\n    U_star = np.zeros((n, r))\n    U_star[:, 0] = p_i\n    U_star[:, 1] = 0.5\n    \n    tau_t = np.arange(T, dtype=float)\n    V_star = np.zeros((T, r))\n    V_star[:, 0] = np.exp(-0.5 * ((tau_t - 5.0) / 1.0)**2)\n    V_star[:, 1] = 0.2 * tau_t / T\n    \n    # Ground truth matrix X with Gaussian noise\n    rng_truth = np.random.default_rng(seed=s_truth)\n    epsilon = rng_truth.normal(0, 0.01, size=(n, T))\n    X_true = U_star.dot(V_star.T) + epsilon\n    \n    # Base observation mask\n    rng_mask = np.random.default_rng(seed=s_mask)\n    M_base = rng_mask.uniform(0, 1, size=(n, T)) >= 0.4\n    \n    # --- Graph and Test Case Definitions ---\n    # Path graph\n    W_path = np.zeros((n, n))\n    for i in range(n - 1):\n        W_path[i, i + 1] = 1\n        W_path[i + 1, i] = 1\n    \n    # Null graph\n    W_null = np.zeros((n, n))\n\n    def get_laplacian(W):\n        D = np.diag(np.sum(W, axis=1))\n        return D - W\n\n    L_path = get_laplacian(W_path)\n    L_null = get_laplacian(W_null)\n\n    # Function to modify mask for Case 2\n    def mod_m_case2(m):\n        m_mod = m.copy()\n        m_mod[2, :] = 0  # Sensor 3 is a cold-start sensor\n        return m_mod\n\n    test_cases = [\n        {'L': L_path, 'M': M_base},\n        {'L': L_path, 'M': mod_m_case2(M_base)},\n        {'L': L_path, 'M': np.ones((n, T), dtype=bool)},\n        {'L': L_null, 'M': M_base},\n    ]\n\n    p_star = np.argmax(X_true, axis=1)\n    results = []\n\n    # --- Main Loop ---\n    for case in test_cases:\n        M, L = case['M'], case['L']\n        \n        # Run baseline model (lambda_U = 0)\n        X_hat_base = alternating_minimization(X_true, M, L, lambda_U=0.0, \n                                              n=n, T=T, r=r, lambda_reg=lambda_reg, \n                                              n_iter=n_iter, seed_init=s_init)\n        p_hat_base = np.argmax(X_hat_base, axis=1)\n        E_base = np.sum(np.abs(p_hat_base - p_star))\n\n        # Run graph-regularized model\n        X_hat_graph = alternating_minimization(X_true, M, L, lambda_U=lambda_U_graph,\n                                               n=n, T=T, r=r, lambda_reg=lambda_reg, \n                                               n_iter=n_iter, seed_init=s_init)\n        p_hat_graph = np.argmax(X_hat_graph, axis=1)\n        E_graph = np.sum(np.abs(p_hat_graph - p_star))\n\n        delta = E_base - E_graph\n        results.append(int(delta))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}