{
    "hands_on_practices": [
        {
            "introduction": "To truly master matrix completion, it is essential to move from theory to implementation. This first practice focuses on the cornerstone algorithm for nuclear norm minimization: Iterative Singular Value Thresholding (SVT). In this exercise (), you will implement this elegant and powerful method, which operates by repeatedly filling in the missing entries and then \"denoising\" the matrix by shrinking its singular values. This hands-on task will build a strong intuition for how the regularization parameter $\\lambda$ directly controls the rank of the solution, providing a practical feel for the trade-off between data fidelity and low-rank structure.",
            "id": "3168247",
            "problem": "You are asked to design and analyze a proximal method for matrix completion that uses the nuclear norm regularizer. Your goal is to implement an iterative scheme driven by the proximal point principle and then investigate how the choice of the regularization weight affects the evolution of matrix rank across iterations. The program you produce must compute the proximal operator of the nuclear norm via Singular Value Decomposition (SVD) mechanics and evaluate rank shrinkage trajectories for different parameter values.\n\nStarting point and definitions:\n- Let $M \\in \\mathbb{R}^{m \\times n}$ be a partially observed matrix with an associated observation mask $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$. Define the linear observation operator $P_{\\Omega}:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{m \\times n}$ by $(P_{\\Omega}(X))_{ij} = X_{ij}$ if $(i,j)\\in\\Omega$ and $(P_{\\Omega}(X))_{ij}=0$ otherwise. Define the complementary operator $P_{\\Omega^c}(X) = X - P_{\\Omega}(X)$. The nuclear norm of a matrix $X$, denoted $\\|X\\|_*$, is the sum of its singular values.\n- The proximal operator of a proper, closed, convex function $f:\\mathbb{R}^{m \\times n}\\to\\mathbb{R}\\cup\\{+\\infty\\}$ at a point $Y$ with parameter $t>0$ is defined by\n$$\n\\operatorname{prox}_{t f}(Y) = \\arg\\min_{X\\in\\mathbb{R}^{m\\times n}} \\left\\{ f(X) + \\frac{1}{2t}\\|X - Y\\|_F^2 \\right\\}.\n$$\n- Consider the composite objective $F(X) = \\frac{1}{2}\\|P_{\\Omega}(X - M)\\|_F^2 + \\lambda \\|X\\|_*$, where $\\lambda>0$.\n\nTask:\n- Using only the above definitions and standard properties of convex functions and SVD, derive an iterative scheme that, at each iteration $k$, first enforces data consistency on the observed entries and then applies the proximal operator of the nuclear norm at the resulting point. Concretely, for a fixed $\\lambda$, starting from $X^0 = 0$ and for $k=0,1,2,\\dots,K-1$, perform the following two steps:\n  1. Data-consistency injection to form $Y^k$ by setting $Y^k_{ij} = M_{ij}$ if $(i,j)\\in\\Omega$ and $Y^k_{ij} = X^k_{ij}$ otherwise (i.e., $Y^k = P_{\\Omega}(M) + P_{\\Omega^c}(X^k)$).\n  2. Proximal step to obtain $X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$, computed via singular value thresholding derived from first principles.\n- At each iteration $k$, compute the rank of $X^{k}$ as the number of singular values strictly greater than a fixed numerical tolerance (you must choose and state a sensible tolerance).\n\nInvestigation:\n- Implement the above iterative scheme and, for each choice of $\\lambda$, record the sequence of ranks $\\{\\operatorname{rank}(X^{1}), \\operatorname{rank}(X^{2}), \\dots, \\operatorname{rank}(X^{K})\\}$, where $K$ is the total number of iterations.\n\nData generation protocol (deterministic):\n- Set $m=20$ and $n=15$.\n- Construct a ground-truth low-rank matrix $X_{\\text{true}}\\in\\mathbb{R}^{20\\times 15}$ of rank $r_{\\text{true}}=3$ using orthonormal factors and singular values. Use fixed singular values $10$, $5$, and $3$, and generate orthonormal columns deterministically by applying the Gramâ€“Schmidt procedure (via a standard QR factorization) to random Gaussian matrices with a fixed pseudorandom seed.\n- Generate the observation mask by independently sampling each entry as observed with probability $p=0.5$, using the same fixed pseudorandom seed.\n- Form the observed matrix $M$ by setting $M_{ij} = X_{\\text{true},ij} + \\varepsilon_{ij}$ for $(i,j)\\in\\Omega$, where $\\varepsilon_{ij}$ are independent Gaussian noises with mean $0$ and standard deviation $0.01$, and $M_{ij}=0$ for $(i,j)\\notin\\Omega$. Use the same fixed seed for noise generation.\n\nTest suite:\n- Use $K=12$ iterations and rank tolerance $10^{-8}$.\n- Evaluate the rank trajectory for three values of $\\lambda$: $\\lambda=0.0$ (boundary case), $\\lambda=1.0$ (moderate regularization), and $\\lambda=5.0$ (strong regularization).\n\nRequired final output format:\n- Your program must produce a single line of output containing the three rank trajectories (for the three $\\lambda$ values) as a comma-separated list of lists enclosed in square brackets, for example, $[[r_{1,1},\\dots,r_{1,12}],[r_{2,1},\\dots,r_{2,12}],[r_{3,1},\\dots,r_{3,12}]]$, where $r_{j,k}$ is an integer rank. No other text should be printed.\n\nNotes:\n- Angles and physical units are not involved in this problem.\n- The implementation must be self-contained and deterministic by using a fixed pseudorandom seed.\n- You must compute $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}$ via singular value thresholding derived from the definition of the proximal operator and properties of SVD; do not assume or use any pre-provided formula.",
            "solution": "The user-provided problem statement has been analyzed and validated.\n\n### Step 1: Extract Givens\n- **Matrices and Operators**:\n    - $M \\in \\mathbb{R}^{m \\times n}$: A partially observed matrix.\n    - $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$: An observation mask.\n    - $P_{\\Omega}(X)$: Linear operator where $(P_{\\Omega}(X))_{ij} = X_{ij}$ if $(i,j)\\in\\Omega$ and $0$ otherwise.\n    - $P_{\\Omega^c}(X) = X - P_{\\Omega}(X)$.\n    - $\\|X\\|_*$: The nuclear norm of matrix $X$, which is the sum of its singular values.\n- **Proximal Operator Definition**:\n    - For a proper, closed, convex function $f$, $\\operatorname{prox}_{t f}(Y) = \\arg\\min_{X\\in\\mathbb{R}^{m \\times n}} \\left\\{ f(X) + \\frac{1}{2t}\\|X - Y\\|_F^2 \\right\\}$.\n- **Objective Function**:\n    - $F(X) = \\frac{1}{2}\\|P_{\\Omega}(X - M)\\|_F^2 + \\lambda \\|X\\|_*$, with $\\lambda>0$.\n- **Iterative Scheme**:\n    - Initialization: $X^0 = 0$.\n    - For $k=0,1,2,\\dots,K-1$:\n        1. **Data-consistency injection**: $Y^k = P_{\\Omega}(M) + P_{\\Omega^c}(X^k)$.\n        2. **Proximal step**: $X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$.\n- **Task**:\n    - Compute the sequence of ranks $\\{\\operatorname{rank}(X^{1}), \\operatorname{rank}(X^{2}), \\dots, \\operatorname{rank}(X^{K})\\}$ at each iteration. Rank is defined as the number of singular values greater than a specified tolerance.\n- **Data Generation Protocol**:\n    - Matrix dimensions: $m=20$, $n=15$.\n    - Ground-truth rank: $r_{\\text{true}}=3$.\n    - Ground-truth singular values: $\\{10, 5, 3\\}$.\n    - Ground-truth matrix construction: $X_{\\text{true}} = U \\operatorname{diag}(\\{10,5,3\\}) V^T$, where $U$ and $V$ have orthonormal columns generated via QR factorization of random Gaussian matrices using a fixed seed.\n    - Observation probability: $p=0.5$ for each entry, using a fixed seed.\n    - Observed matrix: $M = P_{\\Omega}(X_{\\text{true}} + \\varepsilon)$, where $\\varepsilon$ is Gaussian noise with mean $0$ and standard deviation $0.01$, generated using a fixed seed.\n- **Test Suite**:\n    - Number of iterations: $K=12$.\n    - Rank tolerance: $10^{-8}$.\n    - Regularization parameters: $\\lambda \\in \\{0.0, 1.0, 5.0\\}$.\n- **Output Format**:\n    - A single line `[[r_{1,1},\\dots,r_{1,12}],[r_{2,1},\\dots,r_{2,12}],[r_{3,1},\\dots,r_{3,12}]]$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is rooted in the established field of convex optimization, specifically proximal methods for matrix completion. The concepts of nuclear norm, proximal operators, and singular value decomposition (SVD) are fundamental to this area. The algorithm described is a well-known iterative scheme for nuclear norm minimization.\n- **Well-Posed**: The problem is clearly defined. It provides all necessary parameters, an explicit iterative formula, a deterministic data generation protocol (by fixing seeds), and a precise format for the output. This ensures that a unique and meaningful solution can be computed.\n- **Objective**: The problem is stated in formal mathematical language, free of ambiguity, subjectivity, or opinion-based claims.\n\nThe problem statement is self-contained, consistent, and adheres to established scientific principles. There are no identified flaws.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation and Method\n\nThe core of the iterative scheme is the computation of the proximal operator of the nuclear norm, $X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$. The problem statement's notation suggests interpreting this as the proximal operator of the function $f(X) = \\lambda \\|X\\|_*$ with parameter $t=1$. Following the provided definition of a proximal operator, this corresponds to solving the following optimization problem:\n$$\nX^{k+1} = \\arg\\min_{X \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|X\\|_* + \\frac{1}{2}\\|X - Y^k\\|_F^2 \\right\\}\n$$\nThis problem has a closed-form solution based on the Singular Value Decomposition (SVD) of $Y^k$. Let the SVD of $Y^k$ be $Y^k = U \\Sigma V^T$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ are matrices with orthonormal columns, $\\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_r)$ is a diagonal matrix of positive singular values $\\sigma_i > 0$, and $r = \\operatorname{rank}(Y^k)$.\n\nThe nuclear norm and the Frobenius norm are unitarily invariant. This means for any orthonormal matrices $A$ and $B$, $\\|Z\\|_* = \\|A^T Z B\\|_*$ and $\\|Z\\|_F = \\|A^T Z B\\|_F$. Applying this property with $A=U$ and $B=V$, the optimization problem can be rewritten in terms of a transformed variable $\\hat{X} = U^T X V$:\n$$\n\\arg\\min_{\\hat{X} \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|\\hat{X}\\|_* + \\frac{1}{2}\\|\\hat{X} - U^T Y^k V\\|_F^2 \\right\\}\n$$\nSince $Y^k = U \\Sigma V^T$, we have $U^T Y^k V = \\Sigma$. The problem simplifies to:\n$$\n\\arg\\min_{\\hat{X} \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|\\hat{X}\\|_* + \\frac{1}{2}\\|\\hat{X} - \\Sigma\\|_F^2 \\right\\}\n$$\nIt can be shown (e.g., via von Neumann's trace inequality) that for a fixed diagonal matrix $\\Sigma$, the minimum is achieved when $\\hat{X}$ is also a diagonal matrix. Let $\\hat{X} = D = \\operatorname{diag}(d_1, d_2, \\dots, d_r)$. The singular values of $D$ are simply $|d_i|$. To minimize $\\|\\hat{X}\\|_*$, we should choose $d_i \\ge 0$. The problem then decouples into a set of independent scalar minimization problems, one for each singular value:\n$$\n\\min_{d_i \\ge 0} \\left\\{ \\lambda d_i + \\frac{1}{2}(d_i - \\sigma_i)^2 \\right\\} \\quad \\text{for } i=1, \\dots, r\n$$\nThe objective is a simple quadratic in $d_i$. Its derivative with respect to $d_i$ is $\\lambda + d_i - \\sigma_i$. Setting the derivative to zero gives a candidate solution $d_i = \\sigma_i - \\lambda$. Since we require $d_i \\ge 0$, the optimal solution is $d_i = \\max(0, \\sigma_i - \\lambda)$. This operation is known as the soft-thresholding operator, denoted $S_\\lambda(\\sigma_i) = (\\sigma_i - \\lambda)_+$.\n\nThus, the optimal diagonal matrix is $\\hat{X}_{\\text{opt}} = S_\\lambda(\\Sigma) = \\operatorname{diag}(S_\\lambda(\\sigma_1), \\dots, S_\\lambda(\\sigma_r))$. To find the final solution $X^{k+1}$, we transform back to the original coordinate system:\n$$\nX^{k+1} = U \\hat{X}_{\\text{opt}} V^T = U S_\\lambda(\\Sigma) V^T\n$$\nThis procedure is known as Singular Value Thresholding (SVT).\n\nThe complete algorithm is as follows:\n1.  **Initialize**: Generate the ground truth matrix $X_{\\text{true}}$, the observed matrix $M$, and the observation mask $\\Omega$ according to the specified deterministic protocol. Set $X^0 = 0$.\n2.  **Iterate**: For each specified value of $\\lambda$, and for $k=0, \\dots, K-1$:\n    a.  **Data Update**: Form the matrix $Y^k = M + P_{\\Omega^c}(X^k)$. This step preserves the known entries from $M$ while using the current estimate $X^k$ for the unknown entries.\n    b.  **SVD**: Compute the SVD of $Y^k_c$: $Y^k = U \\Sigma V^T$.\n    c.  **Thresholding**: Apply the soft-thresholding operator to the singular values: $\\Sigma_{\\text{thresh}} = S_\\lambda(\\Sigma)$, where the diagonal entries are $(\\Sigma_{\\text{thresh}})_{ii} = \\max(0, \\Sigma_{ii} - \\lambda)$.\n    d.  **Reconstruction**: Form the next iterate $X^{k+1} = U \\Sigma_{\\text{thresh}} V^T$.\n    e.  **Rank Calculation**: Compute $\\operatorname{rank}(X^{k+1})$ as the number of diagonal entries in $\\Sigma_{\\text{thresh}}$ that are greater than the tolerance $10^{-8}$.\n3.  **Store Results**: For each $\\lambda$, store the sequence of computed ranks $\\{\\operatorname{rank}(X^1), \\dots, \\operatorname{rank}(X^{12})\\}$. The final output will be a list containing these three sequences.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the proximal point algorithm for matrix completion and\n    evaluates rank trajectories for different regularization weights.\n    \"\"\"\n    # Parameters from the problem statement\n    m, n = 20, 15\n    r_true = 3\n    singular_values_true = np.array([10.0, 5.0, 3.0])\n    p_obs = 0.5\n    noise_std = 0.01\n    K = 12\n    rank_tol = 1e-8\n    lambda_vals = [0.0, 1.0, 5.0]\n    seed = 0\n\n    # Data generation protocol (deterministic)\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct ground-truth low-rank matrix X_true\n    # Generate random matrices and use QR to get orthonormal columns\n    U_rand = rng.standard_normal((m, r_true))\n    U, _ = np.linalg.qr(U_rand)\n    \n    V_rand = rng.standard_normal((n, r_true))\n    V, _ = np.linalg.qr(V_rand)\n    V_t = V.T\n    \n    X_true = U @ np.diag(singular_values_true) @ V_t\n\n    # 2. Generate observation mask Omega\n    omega_mask = rng.random((m, n)) < p_obs\n    omega_mask_c = ~omega_mask\n\n    # 3. Form the observed matrix M\n    noise = rng.normal(loc=0.0, scale=noise_std, size=(m, n))\n    M = np.zeros((m, n))\n    M[omega_mask] = (X_true + noise)[omega_mask]\n\n    all_rank_trajectories = []\n\n    # Investigation loop over lambda values\n    for lambda_val in lambda_vals:\n        rank_trajectory = []\n        X = np.zeros((m, n))\n\n        # Iterative scheme\n        for _ in range(K):\n            # 1. Data-consistency injection\n            Y = M + X * omega_mask_c\n\n            # 2. Proximal step\n            # Compute SVD of Y\n            try:\n                U_svd, s_svd, Vh_svd = np.linalg.svd(Y, full_matrices=False)\n            except np.linalg.LinAlgError:\n                # Handle cases where SVD might fail, though unlikely here\n                s_svd = np.array([])\n                U_svd = np.zeros((m, 0))\n                Vh_svd = np.zeros((0, n))\n\n            # Apply soft-thresholding to singular values\n            s_thresh = np.maximum(0, s_svd - lambda_val)\n            \n            # Reconstruct the matrix X_next\n            # Create a diagonal matrix of the correct size for multiplication\n            Sigma_thresh = np.zeros((len(s_thresh), len(s_thresh)))\n            np.fill_diagonal(Sigma_thresh, s_thresh)\n            \n            X_next = U_svd @ Sigma_thresh @ Vh_svd\n\n            # Compute rank of X_next\n            rank = np.sum(s_thresh > rank_tol)\n            rank_trajectory.append(rank)\n\n            # Update X for the next iteration\n            X = X_next\n            \n        all_rank_trajectories.append(rank_trajectory)\n\n    # Final print statement in the exact required format\n    inner_strs = [f\"[{','.join(map(str, r))}]\" for r in all_rank_trajectories]\n    final_output = f\"[{','.join(inner_strs)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Standard matrix completion models are powerful, but what if we have additional information about our data? This practice explores how to incorporate such \"side information\" to build more accurate and robust models. Using the motivating example of a traffic sensor network (), you will implement a graph-regularized factorization model where the road network topology guides the solution. By adding a penalty based on the graph Laplacian, you will encourage the latent features of adjacent sensors to be similar, effectively sharing information across the network and improving predictions, especially in challenging \"cold-start\" scenarios where a sensor has no data at all.",
            "id": "3145702",
            "problem": "You are given a matrix completion task motivated by traffic sensor networks. Consider a sensor-by-time data matrix $X \\in \\mathbb{R}^{n \\times T}$ with missing entries indicated by a binary mask $M \\in \\{0,1\\}^{n \\times T}$, where $M_{i,t} = 1$ means $X_{i,t}$ is observed and $M_{i,t} = 0$ means it is missing. You will complete the matrix using a low-rank factorization model and evaluate how incorporating road-network topology through a graph regularizer affects the accuracy of peak-time predictions.\n\nFundamental base and definitions:\n- Matrix completion via low-rank factorization assumes $X \\approx U V^{\\top}$ with $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{T \\times r}$ for a chosen rank $r$.\n- The road network topology over sensors is encoded by an undirected weighted adjacency matrix $W \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$, with $W_{i,j} = W_{j,i}$ and $W_{i,i} = 0$. The degree matrix is $D = \\mathrm{diag}(W \\mathbf{1})$, and the graph Laplacian is $L = D - W$.\n- The completion is posed as minimizing the penalized empirical risk\n$$\n\\min_{U,V} \\;\\; \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2} + \\|V\\|_{F}^{2}\\right),\n$$\nwhere $\\odot$ denotes the Hadamard product, $\\lambda_{\\mathrm{U}} \\ge 0$ controls alignment with the graph topology, and $\\lambda > 0$ is a ridge regularizer. When $\\lambda_{\\mathrm{U}} = 0$, this reduces to standard low-rank matrix factorization without graph regularization.\n\nTask:\n- Implement an alternating minimization solver that, for fixed $r$, $\\lambda_{\\mathrm{U}}$, and $\\lambda$, alternately updates $U$ and $V$ until a fixed number of iterations is reached. Use deterministic initialization and deterministic data generation as specified below.\n- After completing the matrix to obtain $\\widehat{X} = U V^{\\top}$, define for each sensor $i \\in \\{1,\\dots,n\\}$ its peak time index as $p_i^{\\star} = \\arg\\max_{t \\in \\{1,\\dots,T\\}} X_{i,t}$ for the ground-truth matrix, and $\\widehat{p}_i = \\arg\\max_{t \\in \\{1,\\dots,T\\}} \\widehat{X}_{i,t}$ for the completed matrix. In case of ties, take the smallest index (the default behavior of $\\arg\\max$).\n- Define the total peak-index error as $E(\\widehat{X}; X) = \\sum_{i=1}^{n} \\left| \\widehat{p}_i - p_i^{\\star} \\right|$.\n- For each test case below, compute the improvement in peak prediction obtained by graph-regularized factorization over the non-regularized baseline as an integer:\n$$\n\\Delta = E\\big(\\widehat{X}_{\\text{baseline}}; X\\big) - E\\big(\\widehat{X}_{\\text{graph}}; X\\big).\n$$\nPositive $\\Delta$ indicates that graph regularization improves peak prediction accuracy.\n\nData generation (deterministic and universal):\n- Dimensions: $n = 5$, $T = 8$, $r = 2$.\n- Ground truth factors:\n  - Sensor factors $U^{\\star} \\in \\mathbb{R}^{n \\times r}$: Let positions $p_i = \\frac{i-1}{n-1}$ for $i = 1,\\dots,n$. Set the first column as $U^{\\star}_{i,1} = p_i$ and the second column as $U^{\\star}_{i,2} = 0.5$ for all $i$.\n  - Time factors $V^{\\star} \\in \\mathbb{R}^{T \\times r}$: Let time indices $\\tau_t = t-1$ for $t = 1,\\dots,T$. Set $V^{\\star}_{t,1} = \\exp\\!\\left(-\\frac{1}{2} \\left(\\frac{\\tau_t - 5}{1.0}\\right)^{2}\\right)$ and $V^{\\star}_{t,2} = 0.2 \\cdot \\frac{\\tau_t}{T}$.\n- Ground truth matrix with small noise: $X = U^{\\star} (V^{\\star})^{\\top} + \\epsilon$, where $\\epsilon$ has independent entries drawn from a zero-mean Gaussian with standard deviation $0.01$ using a pseudo-random number generator with seed $s_{\\mathrm{truth}} = 1$.\n- Base observation mask $M^{\\mathrm{base}} \\in \\{0,1\\}^{n \\times T}$: draw independent uniform random numbers in $\\left[0,1\\right)$ with a pseudo-random number generator seed $s_{\\mathrm{mask}} = 0$, and set $M^{\\mathrm{base}}_{i,t} = 1$ if the draw is at least $0.4$ and $M^{\\mathrm{base}}_{i,t} = 0$ otherwise (so the expected observed fraction is $0.6$).\n- Deterministic initialization for the algorithm: initialize $U$ and $V$ with independent zero-mean Gaussian entries with standard deviation $0.1$ using pseudo-random number generator seed $s_{\\mathrm{init}} = 123$ at the start of each run.\n\nRoad network graphs:\n- Path graph on $n$ sensors: $W \\in \\mathbb{R}^{n \\times n}$ with $W_{i,i+1} = W_{i+1,i} = 1$ for $i \\in \\{1,\\dots,n-1\\}$ and $W_{i,j} = 0$ otherwise.\n- Null graph: $W = 0$ (so $L = 0$).\n\nAlgorithmic settings:\n- Use $\\lambda = 0.1$, iteration count $N_{\\mathrm{iter}} = 50$.\n- For the baseline, set $\\lambda_{\\mathrm{U}} = 0$; for the graph-regularized run, set $\\lambda_{\\mathrm{U}} = 5.0$.\n\nTest suite:\n- Case $1$ (happy path): $W$ equals the path graph, $M = M^{\\mathrm{base}}$.\n- Case $2$ (cold-start sensor): $W$ equals the path graph, $M$ equals $M^{\\mathrm{base}}$ but with the entire third row set to zero, i.e., $M_{3,t} = 0$ for all $t \\in \\{1,\\dots,T\\}$.\n- Case $3$ (fully observed): $W$ equals the path graph, $M$ is the all-ones matrix of shape $n \\times T$.\n- Case $4$ (no graph information): $W = 0$, $M = M^{\\mathrm{base}}$.\n\nProgram requirements:\n- Implement an alternating minimization solver that alternates between updating $V$ and $U$:\n  - The $V$-update must solve, for each time index, a ridge-regularized weighted least squares problem with weights from the corresponding column of $M$.\n  - The $U$-update must minimize the objective exactly with respect to $U$ given $V$, which entails solving a linear system coupling all sensor latent vectors via the graph Laplacian $L$.\n- For each test case, run the baseline and the graph-regularized solver starting from the same initialization, complete the matrix to obtain $\\widehat{X}$, compute the total peak-index error $E(\\widehat{X}; X)$, and report the integer improvement $\\Delta$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $4$, for example, $\\left[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4\\right]$.",
            "solution": "The problem requires the implementation of a graph-regularized matrix completion algorithm based on low-rank factorization. The goal is to evaluate the impact of a graph regularizer on the accuracy of predicting peak-time events in a simulated sensor network. The solution is found by alternating minimization.\n\nThe objective function to be minimized is the penalized empirical risk:\n$$\nJ(U, V) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2} + \\|V\\|_{F}^{2}\\right)\n$$\nHere, $X \\in \\mathbb{R}^{n \\times T}$ is the data matrix, $M \\in \\{0,1\\}^{n \\times T}$ is the observation mask, $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{T \\times r}$ are the latent factors, $L \\in \\mathbb{R}^{n \\times n}$ is the graph Laplacian encoding sensor topology, and $\\lambda_{\\mathrm{U}}, \\lambda$ are regularization parameters. The term $\\mathrm{tr}\\!\\left(U^{\\top} L U\\right)$ encourages the latent factors of connected sensors to be similar.\n\nThe optimization problem is not jointly convex in $U$ and $V$. However, it is biconvex, meaning the problem is convex in $U$ for a fixed $V$, and convex in $V$ for a fixed $U$. This structure motivates an alternating minimization approach, where we iteratively solve for one factor matrix while keeping the other fixed.\n\n### Alternating Minimization Algorithm\n\nWe initialize $U$ and $V$ and then alternate between two main steps for a fixed number of iterations.\n\n#### 1. Update V (with U fixed)\nFor a fixed $U$, the objective function terms depending on $V$ are:\n$$\nJ(V) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda}{2} \\|V\\|_{F}^{2} = \\frac{1}{2} \\sum_{t=1}^{T} \\sum_{i=1}^{n} M_{i,t} (X_{i,t} - \\mathbf{u}_i^{\\top} \\mathbf{v}_t)^2 + \\frac{\\lambda}{2} \\sum_{t=1}^{T} \\|\\mathbf{v}_t\\|_2^2\n$$\nwhere $\\mathbf{u}_i$ is the $i$-th row of $U$ and $\\mathbf{v}_t$ is the $t$-th row of $V$. This problem decouples by time index $t$. For each $t \\in \\{1, \\dots, T\\}$, we minimize:\n$$\nJ_t(\\mathbf{v}_t) = \\frac{1}{2} \\sum_{i=1}^{n} M_{i,t} (X_{i,t} - \\mathbf{u}_i^{\\top} \\mathbf{v}_t)^2 + \\frac{\\lambda}{2} \\|\\mathbf{v}_t\\|_2^2\n$$\nThis is a ridge-regularized weighted least squares problem. Let $M_t = \\mathrm{diag}(M_{1,t}, \\dots, M_{n,t})$ be a diagonal matrix of weights for time $t$. The problem for $\\mathbf{v}_t$ is equivalent to minimizing $\\frac{1}{2}\\|\\sqrt{M_t}(\\mathbf{x}_t - U\\mathbf{v}_t)\\|_2^2 + \\frac{\\lambda}{2}\\|\\mathbf{v}_t\\|_2^2$, where $\\mathbf{x}_t$ is the $t$-th column of $X$. Taking the gradient with respect to $\\mathbf{v}_t$ and setting it to zero gives the normal equations:\n$$\n\\left( U^{\\top} M_t U + \\lambda I_r \\right) \\mathbf{v}_t = U^{\\top} M_t \\mathbf{x}_t\n$$\nwhere $I_r$ is the $r \\times r$ identity matrix. The updated $\\mathbf{v}_t$ is found by solving this $r \\times r$ linear system. This can be expressed more explicitly as: let $\\mathcal{I}_t = \\{ i \\mid M_{i,t} = 1 \\}$ be the set of observed sensors at time $t$. Let $U_{\\mathcal{I}_t}$ be the submatrix of $U$ with rows indexed by $\\mathcal{I}_t$. The equation is:\n$$\n\\left( U_{\\mathcal{I}_t}^{\\top} U_{\\mathcal{I}_t} + \\lambda I_r \\right) \\mathbf{v}_t = U_{\\mathcal{I}_t}^{\\top} \\mathbf{x}_{\\mathcal{I}_t, t}\n$$\n\n#### 2. Update U (with V fixed)\nFor a fixed $V$, the objective function terms depending on $U$ are:\n$$\nJ(U) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\|U\\|_{F}^{2}\n$$\nUnlike the V-update, this problem does not decouple by sensor index $i$ because the graph regularizer $\\mathrm{tr}(U^{\\top} L U) = \\sum_{i,j} L_{ij} \\mathbf{u}_i^{\\top} \\mathbf{u}_j$ couples the rows of $U$. We must solve for all rows of $U$ simultaneously. Taking the gradient with respect to a single row $\\mathbf{u}_i$ and setting it to zero yields:\n$$\n\\left( \\sum_{t=1}^{T} M_{i,t} \\mathbf{v}_t \\mathbf{v}_t^{\\top} + \\lambda I_r \\right) \\mathbf{u}_i + \\lambda_{\\mathrm{U}} \\sum_{j=1}^{n} L_{ij} \\mathbf{u}_j = \\sum_{t=1}^{T} M_{i,t} X_{i,t} \\mathbf{v}_t\n$$\nThis gives a system of $n$ coupled linear equations for the $n$ vectors $\\mathbf{u}_1, \\dots, \\mathbf{u}_n$. We can express this as a single large linear system. Let $\\mathbf{u} = \\mathrm{vec}(U) \\in \\mathbb{R}^{nr}$ be the vector obtained by stacking the rows of $U$. The system takes the form $\\mathcal{A} \\mathbf{u} = \\mathbf{b}$, where $\\mathcal{A}$ is an $nr \\times nr$ block matrix and $\\mathbf{b}$ is a vector of length $nr$. The $(i,j)$-th block of $\\mathcal{A}$ (an $r \\times r$ matrix) is given by:\n$$\n\\mathcal{A}_{ij} = \\delta_{ij} \\left( \\sum_{t=1}^{T} M_{i,t} \\mathbf{v}_t \\mathbf{v}_t^{\\top} + \\lambda I_r \\right) + \\lambda_{\\mathrm{U}} L_{ij} I_r\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. The $i$-th block of the vector $\\mathbf{b}$ is $\\mathbf{b}_i = \\sum_{t=1}^{T} M_{i,t} X_{i,t} \\mathbf{v}_t$. The matrix $\\mathcal{A}$ is symmetric and positive definite (since $L$ is positive semi-definite and $\\lambda > 0$), ensuring a unique solution for $U$. The updated $U$ is obtained by solving this $nr \\times nr$ linear system and reshaping the resulting vector.\n\n### Evaluation\nAfter running the alternating minimization for a fixed number of iterations, the completed matrix is $\\widehat{X} = UV^{\\top}$. The performance is evaluated by comparing the predicted peak time indices, $\\widehat{p}_i = \\arg\\max_t \\widehat{X}_{i,t}$, against the ground-truth peak time indices, $p_i^{\\star} = \\arg\\max_t X_{i,t}$. The total peak-index error is $E(\\widehat{X}; X) = \\sum_{i=1}^{n} | \\widehat{p}_i - p_i^{\\star} |$. The final reported metric is the improvement $\\Delta$ from graph regularization, defined as the difference in error between a baseline model ($\\lambda_{\\mathrm{U}}=0$) and the graph-regularized model ($\\lambda_{\\mathrm{U}}>0$): $\\Delta = E_{\\text{baseline}} - E_{\\text{graph}}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\n#   - name: scipy\n#     version: 1.11.4\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the matrix completion task for all test cases.\n    \"\"\"\n    # Problem Parameters\n    n, T, r = 5, 8, 2\n    lambda_reg = 0.1\n    n_iter = 50\n    lambda_U_graph = 5.0\n    s_truth, s_mask, s_init = 1, 0, 123\n\n    def alternating_minimization(X, M, L, lambda_U, n, T, r, lambda_reg, n_iter, seed_init):\n        \"\"\"\n        Implements the alternating minimization solver for graph-regularized matrix completion.\n        \"\"\"\n        rng_init = np.random.default_rng(seed=seed_init)\n        U = rng_init.normal(0, 0.1, size=(n, r))\n        V = rng_init.normal(0, 0.1, size=(T, r))\n\n        for _ in range(n_iter):\n            # 1. V-update (holding U fixed)\n            # This part solves a separate ridge regression problem for each time point t.\n            for t in range(T):\n                observed_sensors = M[:, t]\n                if not np.any(observed_sensors):\n                    # No observations at this time point, impute with zero vector\n                    V[t, :] = 0.0\n                    continue\n                \n                U_obs = U[observed_sensors, :]\n                X_obs = X[observed_sensors, t]\n                \n                # A_t = U_obs.T @ U_obs + lambda * I\n                A_t = U_obs.T.dot(U_obs) + lambda_reg * np.identity(r)\n                # b_t = U_obs.T @ X_obs_t\n                b_t = U_obs.T.dot(X_obs)\n                \n                # Solve A_t v_t = b_t\n                V[t, :] = np.linalg.solve(A_t, b_t)\n\n            # 2. U-update (holding V fixed)\n            # This part solves one large linear system for all of U.\n            A_u = lambda_U * np.kron(L, np.identity(r))\n            b_u = np.zeros(n * r)\n\n            for i in range(n):\n                observed_times = M[i, :]\n                if not np.any(observed_times):\n                    # Cold-start sensor: no observations\n                    C_i = lambda_reg * np.identity(r)\n                    b_i = np.zeros(r)\n                else:\n                    V_obs = V[observed_times, :]\n                    X_obs = X[i, observed_times]\n                    \n                    # C_i = V_obs.T @ V_obs + lambda * I\n                    C_i = V_obs.T.dot(V_obs) + lambda_reg * np.identity(r)\n                    # b_i = V_obs.T @ X_obs_i\n                    b_i = V_obs.T.dot(X_obs)\n                \n                # Fill the block matrix A_u and vector b_u\n                A_u[i*r:(i+1)*r, i*r:(i+1)*r] += C_i\n                b_u[i*r:(i+1)*r] = b_i\n            \n            # Solve A_u u = b_u and reshape to get U\n            u_vec = np.linalg.solve(A_u, b_u)\n            U = u_vec.reshape((n, r))\n        \n        return U.dot(V.T)\n\n    # --- Data Generation ---\n    # Ground truth factors U_star, V_star\n    p_i = np.linspace(0, 1, n)\n    U_star = np.zeros((n, r))\n    U_star[:, 0] = p_i\n    U_star[:, 1] = 0.5\n    \n    tau_t = np.arange(T, dtype=float)\n    V_star = np.zeros((T, r))\n    V_star[:, 0] = np.exp(-0.5 * ((tau_t - 5.0) / 1.0)**2)\n    V_star[:, 1] = 0.2 * tau_t / T\n    \n    # Ground truth matrix X with Gaussian noise\n    rng_truth = np.random.default_rng(seed=s_truth)\n    epsilon = rng_truth.normal(0, 0.01, size=(n, T))\n    X_true = U_star.dot(V_star.T) + epsilon\n    \n    # Base observation mask\n    rng_mask = np.random.default_rng(seed=s_mask)\n    M_base = rng_mask.uniform(0, 1, size=(n, T)) >= 0.4\n    \n    # --- Graph and Test Case Definitions ---\n    # Path graph\n    W_path = np.zeros((n, n))\n    for i in range(n - 1):\n        W_path[i, i + 1] = 1\n        W_path[i + 1, i] = 1\n    \n    # Null graph\n    W_null = np.zeros((n, n))\n\n    def get_laplacian(W):\n        D = np.diag(np.sum(W, axis=1))\n        return D - W\n\n    L_path = get_laplacian(W_path)\n    L_null = get_laplacian(W_null)\n\n    # Function to modify mask for Case 2\n    def mod_m_case2(m):\n        m_mod = m.copy()\n        m_mod[2, :] = 0  # Sensor 3 is a cold-start sensor\n        return m_mod\n\n    test_cases = [\n        {'L': L_path, 'M': M_base},\n        {'L': L_path, 'M': mod_m_case2(M_base)},\n        {'L': L_path, 'M': np.ones((n, T), dtype=bool)},\n        {'L': L_null, 'M': M_base},\n    ]\n\n    p_star = np.argmax(X_true, axis=1)\n    results = []\n\n    # --- Main Loop ---\n    for case in test_cases:\n        M, L = case['M'], case['L']\n        \n        # Run baseline model (lambda_U = 0)\n        X_hat_base = alternating_minimization(X_true, M, L, lambda_U=0.0, \n                                              n=n, T=T, r=r, lambda_reg=lambda_reg, \n                                              n_iter=n_iter, seed_init=s_init)\n        p_hat_base = np.argmax(X_hat_base, axis=1)\n        E_base = np.sum(np.abs(p_hat_base - p_star))\n\n        # Run graph-regularized model\n        X_hat_graph = alternating_minimization(X_true, M, L, lambda_U=lambda_U_graph,\n                                               n=n, T=T, r=r, lambda_reg=lambda_reg, \n                                               n_iter=n_iter, seed_init=s_init)\n        p_hat_graph = np.argmax(X_hat_graph, axis=1)\n        E_graph = np.sum(np.abs(p_hat_graph - p_star))\n\n        delta = E_base - E_graph\n        results.append(int(delta))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The choice of regularizer can have a profound impact on model performance, and the standard nuclear norm is not always the optimal choice. This advanced practice () delves into a comparative study between the trace-norm and the max-norm regularizers. You will design a simulation to generate data where the underlying signal has \"spiky\" singular vectors, a situation where information is concentrated in a few specific rows or columns. By comparing the generalization error of both methods, you will gain a practical understanding of why the max-norm can be more robust and lead to better predictions in the face of such non-uniform data structures.",
            "id": "3145784",
            "problem": "You must write a complete, runnable program that simulates matrix completion under two different regularizers in the presence of varying levels of singular vector spikiness and evaluates which regularizer better controls overfitting, using a train/test split on observed versus unobserved entries. The educational context is statistical learning with matrix factorization and matrix completion. The program must implement the following setting and produce the specified outputs.\n\nStart from the following core definitions and principles. Consider a target matrix $X^{\\star} \\in \\mathbb{R}^{m \\times n}$ of rank $r$. A subset of its entries is observed with additive noise, and the goal is to estimate $X^{\\star}$ from the noisy observations. The observed index set is denoted $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$, and the empirical risk is the mean squared error on the observed entries. A common approach is to employ low-rank matrix factorization $X = U V^{\\top}$ with $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ and then regularize to control complexity. Two regularizers are considered:\n\n1. Trace-norm (also known as nuclear norm) regularization, operationalized through a factorization-based penalty that controls the Frobenius norms of $U$ and $V$.\n\n2. Max-norm regularization, operationalized through constraints that control the maximum row $\\ell_{2}$ norms of $U$ and $V$.\n\nThe program must simulate spikiness in the singular vectors of $X^{\\star}$ by constructing $X^{\\star}$ as $X^{\\star} = A B^{\\top}$, where $A \\in \\mathbb{R}^{m \\times r}$ and $B \\in \\mathbb{R}^{n \\times r}$ are generated from random matrices whose row norms are scaled to induce a prescribed level of spikiness. Let a spikiness parameter $s \\in [0,1]$ control row-coherence. Specifically, construct a weight vector $w^{A} \\in \\mathbb{R}^{m}$ with $w^{A}_{1} = 1 + s \\cdot (m - 1)$ and $w^{A}_{i} = 1$ for all $i \\in \\{2,\\dots,m\\}$, and then normalize it so that its arithmetic mean equals $1$, i.e., replace $w^{A}$ by $w^{A} \\cdot \\frac{m}{\\sum_{i=1}^{m} w^{A}_{i}}$. Similarly, construct $w^{B} \\in \\mathbb{R}^{n}$ with $w^{B}_{1} = 1 + s \\cdot (n - 1)$ and $w^{B}_{j} = 1$ for all $j \\in \\{2,\\dots,n\\}$, then normalize it to have arithmetic mean equal to $1$. Generate $A$ and $B$ by drawing each entry independently from a standard normal distribution and scaling the rows of $A$ by $\\sqrt{w^{A}_{i}}$ and the rows of $B$ by $\\sqrt{w^{B}_{j}}$. Finally, scale $X^{\\star}$ so that $\\lVert X^{\\star} \\rVert_{F}$ equals $\\sqrt{m n}$.\n\nObservations are generated as follows. For a given observation fraction $p \\in (0,1)$, sample a mask $W \\in \\{0,1\\}^{m \\times n}$ with independent Bernoulli entries of parameter $p$ (i.e., $W_{ij} \\sim \\mathrm{Bernoulli}(p)$). Let the noise standard deviation be $\\sigma > 0$. The observed data matrix is $Y$ defined by $Y_{ij} = W_{ij} \\cdot \\left( X^{\\star}_{ij} + \\varepsilon_{ij} \\right)$ where $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^{2})$ independently. For training, only entries with $W_{ij} = 1$ are included in the loss via elementwise multiplication by $W$.\n\nYou must implement and train two estimators $X_{\\mathrm{trace}}$ and $X_{\\mathrm{max}}$ of the form $U V^{\\top}$ with fixed rank $r$ using gradient-based optimization:\n\n- Trace-norm surrogate estimator: minimize the sum of squared errors on observed entries plus a penalty that scales with $\\lVert U \\rVert_{F}^{2} + \\lVert V \\rVert_{F}^{2}$, using a fixed regularization parameter $\\lambda > 0$ and fixed learning rate and number of iterations.\n\n- Max-norm surrogate estimator: minimize the sum of squared errors on observed entries subject to the constraints that the maximum row $\\ell_{2}$ norms of $U$ and $V$ are bounded by a fixed parameter $\\alpha > 0$. Implement this by performing gradient steps on the squared error over observed entries and, after each step, projecting each row of $U$ and $V$ onto the closed $\\ell_{2}$ ball of radius $\\alpha$ centered at the origin.\n\nInitialize $U$ and $V$ for both estimators with the same random matrices at the start of each test case to ensure a fair comparison. After training, compute the generalization error for each estimator as the mean squared error on the unobserved entries, i.e., on the complement of the mask $W$, using the noise-free $X^{\\star}$ as ground truth. Define the final decision for a test case to be a boolean that is true if and only if the max-norm surrogate estimator achieves strictly smaller generalization error than the trace-norm surrogate estimator.\n\nTest suite. Your program must run the following three test cases without any external input, each fully specified by integers and floats:\n\n- Case $1$: $(m,n,r,s,p,\\sigma) = (30,30,2,0.0,0.4,0.1)$ with a seed for randomness equal to $42$.\n\n- Case $2$: $(m,n,r,s,p,\\sigma) = (30,30,2,0.9,0.4,0.1)$ with a seed for randomness equal to $43$.\n\n- Case $3$: $(m,n,r,s,p,\\sigma) = (30,30,2,0.9,0.1,0.1)$ with a seed for randomness equal to $44$.\n\nUse the same fixed hyperparameters across all test cases: regularization strength $\\lambda = 0.08$, max-row-norm bound $\\alpha = 0.9$, a constant learning rate (choose a reasonable small positive value), and a fixed number of iterations (choose a reasonable positive integer large enough to ensure convergence in practice). Ensure numerical stability in the optimization.\n\nFinal output format. Your program should produce a single line of output containing the list of booleans for the three cases, indicating for each case whether the max-norm surrogate estimator has strictly smaller generalization error than the trace-norm surrogate estimator. The line must be a comma-separated list enclosed in square brackets with no spaces, for example, $[{\\tt True},{\\tt False},{\\tt True}]$. No other text should be printed.",
            "solution": "The problem requires a comparative analysis of two regularization methods for low-rank matrix completion: a trace-norm surrogate and a max-norm surrogate. The analysis is performed through a simulation study that varies the \"spikiness\" of the singular vectors of the ground truth matrix and the fraction of observed entries.\n\n### 1. Data Generation Model\n\nThe foundation of the simulation is the generation of a synthetic ground truth matrix $X^{\\star} \\in \\mathbb{R}^{m \\times n}$ with rank $r$ and controlled properties.\n\n**1.1. Spiky Singular Vector Construction**\nThe structure of the singular vectors is controlled by a spikiness parameter $s \\in [0,1]$. A value of $s=0$ corresponds to a non-spiky case where row norms are expected to be uniform, while $s > 0$ introduces a spike, making the first row of the factor matrices have a larger norm than the others. This non-uniformity in row norms is a a proxy for non-uniform coherence, a key concept in matrix completion theory.\n\nThe target matrix is constructed as $X^{\\star} = A B^{\\top}$, where $A \\in \\mathbb{R}^{m \\times r}$ and $B \\in \\mathbb{R}^{n \\times r}$. The process is as follows:\n\n1.  **Define Weight Vectors**: Two weight vectors, $w^{A} \\in \\mathbb{R}^{m}$ and $w^{B} \\in \\mathbb{R}^{n}$, are defined to control the row norms of matrices $A$ and $B$. For a spikiness parameter $s$, the unnormalized weights are:\n    $$\n    w^{A}_{1} = 1 + s \\cdot (m - 1), \\quad w^{A}_{i} = 1 \\text{ for } i \\in \\{2,\\dots,m\\}\n    $$\n    $$\n    w^{B}_{1} = 1 + s \\cdot (n - 1), \\quad w^{B}_{j} = 1 \\text{ for } j \\in \\{2,\\dots,n\\}\n    $$\n\n2.  **Normalize Weights**: These vectors are then normalized to have an arithmetic mean of $1$. The normalization factor for $w^A$ is $\\frac{m}{\\sum_{i=1}^{m} w^{A}_{i}}$, and similarly for $w^B$. The sum is $\\sum_{i=1}^{m} w^{A}_{i} = (1 + s(m-1)) + (m-1) = m + s(m-1)$. Thus, the normalized weights are:\n    $$\n    w^{A}_{\\text{norm}} = w^A \\cdot \\frac{m}{m + s(m-1)}\n    $$\n    $$\n    w^{B}_{\\text{norm}} = w^B \\cdot \\frac{n}{n + s(n-1)}\n    $$\n\n3.  **Generate and Scale Factors**: Matrices $A$ and $B$ are generated by first drawing their entries from a standard normal distribution, let's call them $\\tilde{A}$ and $\\tilde{B}$. Then, each row is scaled by the square root of the corresponding normalized weight:\n    $$\n    A_{i,:} = \\tilde{A}_{i,:} \\cdot \\sqrt{w^{A}_{\\text{norm},i}} \\quad \\text{for } i=1,\\dots,m\n    $$\n    $$\n    B_{j,:} = \\tilde{B}_{j,:} \\cdot \\sqrt{w^{B}_{\\text{norm},j}} \\quad \\text{for } j=1,\\dots,n\n    $$\n    The expected squared $\\ell_2$-norm of row $i$ of $A$ is now proportional to $w^{A}_{\\text{norm},i}$.\n\n4.  **Construct and Normalize Target Matrix**: The ground truth matrix is $X^{\\star} = AB^{\\top}$. Finally, $X^{\\star}$ is scaled to have a Frobenius norm of $\\sqrt{mn}$, which ensures that the root mean square of its entries is $1$.\n    $$\n    X^{\\star} \\leftarrow X^{\\star} \\cdot \\frac{\\sqrt{mn}}{\\lVert X^{\\star} \\rVert_{F}}\n    $$\n\n**1.2. Observation Model**\nA subset of the entries of $X^{\\star}$ is observed, corrupted by additive Gaussian noise.\n- An observation mask $W \\in \\{0,1\\}^{m \\times n}$ is generated, where each entry $W_{ij}$ is an independent Bernoulli random variable with parameter $p$, i.e., $W_{ij} \\sim \\mathrm{Bernoulli}(p)$. $W_{ij}=1$ indicates an observed entry.\n- The observed data matrix $Y \\in \\mathbb{R}^{m \\times n}$ is formed by taking the entries of $X^{\\star}$ corresponding to the mask $W$ and adding independent and identically distributed noise $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2)$. The problem defines $Y_{ij} = W_{ij} \\cdot (X^{\\star}_{ij} + \\varepsilon_{ij})$. For optimization, it is more convenient to define the full noisy matrix $Y_{full} = X^{\\star} + \\mathcal{E}$, where $\\mathcal{E}$ is the matrix of noise terms, and then use the mask $W$ to select entries for the loss function.\n\n### 2. Estimators and Optimization Algorithms\n\nThe goal is to estimate $X^{\\star}$ using a low-rank factorization $X = UV^{\\top}$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$. Two regularization schemes are compared. Both are trained using gradient-based methods with a fixed learning rate $\\eta=0.005$ and for $2000$ iterations. These hyperparameters are chosen to be reasonable for the problem scale.\n\n**2.1. Trace-Norm Surrogate Estimator ($X_{\\mathrm{trace}}$)**\nThe trace norm of a matrix is the sum of its singular values. A common convex relaxation for rank minimization is to minimize the trace norm. For a factorization $X=UV^\\top$, the penalty $\\lambda(\\lVert U \\rVert_F^2 + \\lVert V \\rVert_F^2)$ serves as a non-convex surrogate for the trace norm penalty $\\lambda \\lVert X \\rVert_*$. The objective function to minimize is the regularized sum of squared errors on the observed entries:\n$$\n\\mathcal{L}_{\\mathrm{trace}}(U, V) = \\frac{1}{2}\\left\\| W \\odot (UV^{\\top} - Y_{full}) \\right\\|_{F}^{2} + \\frac{\\lambda}{2} (\\left\\| U \\right\\|_{F}^{2} + \\left\\| V \\right\\|_{F}^{2})\n$$\nwhere $\\odot$ denotes the element-wise (Hadamard) product and $\\lambda=0.08$ is the regularization parameter.\n\nOptimization is performed using gradient descent. The gradients of the objective function with respect to $U$ and $V$ are:\n$$\n\\nabla_{U} \\mathcal{L}_{\\mathrm{trace}} = (W \\odot (UV^{\\top} - Y_{full})) V + \\lambda U\n$$\n$$\n\\nabla_{V} \\mathcal{L}_{\\mathrm{trace}} = (W \\odot (UV^{\\top} - Y_{full}))^{\\top} U + \\lambda V\n$$\nThe update rules are $U \\leftarrow U - \\eta \\nabla_{U} \\mathcal{L}_{\\mathrm{trace}}$ and $V \\leftarrow V - \\eta \\nabla_{V} \\mathcal{L}_{\\mathrm{trace}}$.\n\n**2.2. Max-Norm Surrogate Estimator ($X_{\\mathrm{max}}$)**\nMax-norm regularization controls the complexity of the model by constraining the maximum $\\ell_2$-norm of the rows of the factor matrices $U$ and $V$. This has been shown to be effective, particularly when data exhibits non-uniformity (spikiness). The optimization problem is:\n$$\n\\text{minimize} \\quad \\mathcal{L}(U, V) = \\frac{1}{2}\\left\\| W \\odot (UV^{\\top} - Y_{full}) \\right\\|_{F}^{2}\n$$\n$$\n\\text{subject to} \\quad \\max_{i} \\|U_{i,:}\\|_{2} \\le \\alpha \\quad \\text{and} \\quad \\max_{j} \\|V_{j,:}\\|_{2} \\le \\alpha\n$$\nwhere $U_{i,:}$ is the $i$-th row of $U$, $V_{j,:}$ is the $j$-th row of $V$, and $\\alpha=0.9$ is the max-row-norm bound.\n\nThis constrained problem is solved using Projected Gradient Descent (PGD). Each iteration consists of two steps:\n1.  **Gradient Step**: Update $U$ and $V$ by taking a step in the negative gradient direction of the unconstrained loss function $\\mathcal{L}(U,V)$. Let the intermediate updates be $\\tilde{U}$ and $\\tilde{V}$.\n    $$\n    \\nabla_{U} \\mathcal{L} = (W \\odot (UV^{\\top} - Y_{full})) V \\quad \\Rightarrow \\quad \\tilde{U} = U - \\eta \\nabla_{U} \\mathcal{L}\n    $$\n    $$\n    \\nabla_{V} \\mathcal{L} = (W \\odot (UV^{\\top} - Y_{full}))^{\\top} U \\quad \\Rightarrow \\quad \\tilde{V} = V - \\eta \\nabla_{V} \\mathcal{L}\n    $$\n2.  **Projection Step**: Project each row of $\\tilde{U}$ and $\\tilde{V}$ back onto a closed $\\ell_2$ ball of radius $\\alpha$. For each row $u_i$ of $\\tilde{U}$:\n    $$\n    U_{i,:} \\leftarrow u_i \\cdot \\min\\left(1, \\frac{\\alpha}{\\|u_i\\|_2}\\right)\n    $$\n    The same projection is applied to each row $v_j$ of $\\tilde{V}$. If a row's norm is zero, it remains zero.\n\n### 3. Evaluation and Comparison\n\nThe performance of each estimator is measured by its generalization error, defined as the Mean Squared Error (MSE) on the set of *unobserved* entries, denoted $\\Omega^c = \\{(i,j) | W_{ij}=0\\}$. For an estimated matrix $\\hat{X}=UV^\\top$, the generalization error is:\n$$\n\\text{MSE}_{\\text{unobs}}(\\hat{X}) = \\frac{\\sum_{(i,j) \\in \\Omega^c} (\\hat{X}_{ij} - X^{\\star}_{ij})^2}{|\\Omega^c|} = \\frac{\\left\\| (1-W) \\odot (\\hat{X} - X^{\\star}) \\right\\|_F^2}{\\sum_{i,j}(1-W_{ij})}\n$$\nFor each test case, the final decision is a boolean value which is `True` if the max-norm estimator achieves a strictly smaller generalization error than the trace-norm estimator, i.e., $\\text{MSE}_{\\text{unobs}}(X_{\\mathrm{max}}) < \\text{MSE}_{\\text{unobs}}(X_{\\mathrm{trace}})$, and `False` otherwise. Theoretical results suggest that max-norm regularization should be more robust to spikiness, which the simulation is designed to test.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the matrix completion simulation for all test cases.\n    \"\"\"\n    \n    # Define fixed hyperparameters for the optimization algorithms.\n    # A learning rate of 0.005 and 2000 iterations were chosen as reasonable\n    # values for convergence for the given problem size and parameters.\n    LEARNING_RATE = 0.005\n    ITERATIONS = 2000\n    LAMBDA_REG = 0.08  # Regularization for trace-norm surrogate\n    ALPHA_BOUND = 0.9   # Max row-norm bound for max-norm surrogate\n\n    test_cases = [\n        # (m, n, r, s, p, sigma, seed)\n        (30, 30, 2, 0.0, 0.4, 0.1, 42),\n        (30, 30, 2, 0.9, 0.4, 0.1, 43),\n        (30, 30, 2, 0.9, 0.1, 0.1, 44),\n    ]\n\n    results = []\n    \n    for m, n, r, s, p, sigma, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate Ground Truth Matrix X_star\n        # Generate weight vectors w_A and w_B for spikiness\n        w_A = np.ones(m)\n        w_A[0] = 1 + s * (m - 1)\n        w_A *= m / np.sum(w_A)\n        \n        w_B = np.ones(n)\n        w_B[0] = 1 + s * (n - 1)\n        w_B *= n / np.sum(w_B)\n\n        # Generate factor matrices A and B and scale rows\n        A = rng.standard_normal(size=(m, r)) * np.sqrt(w_A[:, np.newaxis])\n        B = rng.standard_normal(size=(n, r)) * np.sqrt(w_B[:, np.newaxis])\n        \n        X_star = A @ B.T\n        \n        # Normalize X_star to have Frobenius norm sqrt(mn)\n        norm_X_star = np.linalg.norm(X_star, 'fro')\n        if norm_X_star > 1e-9: # Avoid division by zero\n            X_star *= np.sqrt(m * n) / norm_X_star\n\n        # 2. Generate Observed Data\n        # Generate mask W for observed entries\n        W = rng.binomial(1, p, size=(m, n)).astype(np.float64)\n        \n        # Generate noise and the full noisy matrix Y_full\n        noise = rng.normal(0, sigma, size=(m, n))\n        Y_full = X_star + noise\n\n        # 3. Initialize Factor Matrices U and V for optimization\n        U_init = rng.standard_normal(size=(m, r))\n        V_init = rng.standard_normal(size=(n, r))\n\n        # 4. Train Trace-Norm Surrogate Estimator\n        U_trace, V_trace = U_init.copy(), V_init.copy()\n        for _ in range(ITERATIONS):\n            X_pred = U_trace @ V_trace.T\n            grad_loss_part = W * (X_pred - Y_full)\n            \n            grad_U = grad_loss_part @ V_trace + LAMBDA_REG * U_trace\n            grad_V = grad_loss_part.T @ U_trace + LAMBDA_REG * V_trace\n            \n            U_trace -= LEARNING_RATE * grad_U\n            V_trace -= LEARNING_RATE * grad_V\n\n        # 5. Train Max-Norm Surrogate Estimator\n        U_max, V_max = U_init.copy(), V_init.copy()\n        for _ in range(ITERATIONS):\n            X_pred = U_max @ V_max.T\n            grad_loss_part = W * (X_pred - Y_full)\n\n            # Gradient step (without regularization term)\n            grad_U = grad_loss_part @ V_max\n            grad_V = grad_loss_part.T @ U_max\n            \n            U_max -= LEARNING_RATE * grad_U\n            V_max -= LEARNING_RATE * grad_V\n\n            # Projection step\n            row_norms_U = np.linalg.norm(U_max, axis=1)\n            scales_U = np.minimum(1.0, ALPHA_BOUND / (row_norms_U + 1e-9))\n            U_max *= scales_U[:, np.newaxis]\n\n            row_norms_V = np.linalg.norm(V_max, axis=1)\n            scales_V = np.minimum(1.0, ALPHA_BOUND / (row_norms_V + 1e-9))\n            V_max *= scales_V[:, np.newaxis]\n\n        # 6. Evaluate Generalization Error\n        W_unobs = 1 - W\n        num_unobs = np.sum(W_unobs)\n\n        if num_unobs == 0:\n            # This case should not happen with p < 1 and the given sizes,\n            # but is handled for robustness.\n            mse_trace, mse_max = 0.0, 0.0\n        else:\n            X_trace = U_trace @ V_trace.T\n            error_trace = np.sum(W_unobs * (X_trace - X_star)**2)\n            mse_trace = error_trace / num_unobs\n            \n            X_max = U_max @ V_max.T\n            error_max = np.sum(W_unobs * (X_max - X_star)**2)\n            mse_max = error_max / num_unobs\n        \n        # 7. Compare and store result\n        results.append(mse_max < mse_trace)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}