{
    "hands_on_practices": [
        {
            "introduction": "在实际应用中，驱动变量往往被限制在一个固定的区间内，例如考试分数的范围是 $0$ 到 $100$。本练习旨在通过编程实践，让您从头开始构建一个局部线性回归断点设计（RDD）估计量。您将亲身体验当断点从数据支持的中心移动到边缘时，由于数据截断导致估计窗口不对称，估计量的偏差会如何变化，从而加深对边界效应如何影响估计准确性的理解。",
            "id": "3168448",
            "problem": "您接到的任务是，算法性地评估在边界附近对驱动变量进行截断如何影响局部线性回归断点 (RD) 估计量的偏差。从一个基本定义出发：在条件结果函数连续的情况下，断点处的 RD 估计对象是观测结果在该断点处极限条件均值的跳跃。然后，设计并实现一个算法，该算法仅使用单边局部线性回归从第一性原理构建估计量。\n\n数据生成设置：\n- 驱动变量定义在一个紧区间上，具体为 $X \\in [0,100]$。您必须将该定义域视为在边界处被截断，这意味着对于 $x \\lt 0$ 或 $x \\gt 100$ 不存在观测值。\n- 生成一个确定性的、密集的驱动变量值网格：在 $[0,100]$ 上生成 $M$ 个等距点，其中 $M = 10001$，因此网格间距为 $0.01$，网格为 $\\{0,0.01,0.02,\\dots,100\\}$。\n- 定义潜在结果均值函数 $m_0(x)$ 和 $m_1(x)$ 如下：\n  $$m_0(x) = 2 + 0.04\\,x + 0.002\\,x^2,\\quad m_1(x) = m_0(x) + \\tau,$$\n  其中恒定的处理效应为 $\\tau = 3$。\n- 对于给定的断点 $c$，将观测结果确定性地定义为：\n  $$Y = m_0(X) + \\tau \\cdot \\mathbf{1}\\{X \\ge c\\},$$\n  不添加任何随机噪声。\n\n待实现的估计量：\n- 对于指定的带宽 $h \\gt 0$，在断点 $c$ 的两侧各构建一个单边局部线性估计量，仅使用窗口 $|X - c| \\le h$ 内且位于 $c$ 各自一侧的观测值。\n- 使用三角核函数：\n  $$K(u) = \\max\\{0, 1 - |u|\\},\\quad u = \\frac{X - c}{h}.$$\n- 对于左侧，使用所有满足 $X \\lt c$ 和 $|X - c| \\le h$ 的点；对于右侧，使用所有满足 $X \\ge c$ 和 $|X - c| \\le h$ 的点。\n- 在每一侧，拟合一个形式如下的加权最小二乘线性回归：\n  $$Y = \\alpha + \\beta\\,(X - c),$$\n  其中权重为 $K\\big((X - c)/h\\big)$。从左侧拟合中提取拟合截距 $\\widehat{\\alpha}_-$，从右侧拟合中提取 $\\widehat{\\alpha}_+$。\n- 将 RD 估计值定义为：\n  $$\\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_-.$$\n- 将给定 $(c,h)$ 的估计量偏差定义为：\n  $$\\mathrm{bias}(c,h) = \\widehat{\\tau}_{\\mathrm{RD}} - \\tau.$$\n\n科学目标：\n- 算法性地展示断点 $c$ 靠近边界（例如，接近 $100$）如何因一侧的非对称定义域以及 $m_0(x)$ 和 $m_1(x)$ 的曲率而增加绝对偏差。\n- 由于没有噪声且网格是密集和确定性的，因此产生的偏差是 $(c,h)$ 和数据生成过程的确定性函数。\n\n测试套件：\n- 对 $(c,h)$ 使用以下五个测试用例：\n  1. $(c,h) = (50,10)$，一个定义域内的内部案例，用作基线。\n  2. $(c,h) = (95,10)$，靠近上边界，以说明偏差的增加。\n  3. $(c,h) = (98,10)$，更靠近上边界，以强调边界效应。\n  4. $(c,h) = (95,5)$，与案例2断点相同，但带宽更小，以评估因曲率暴露减少而带来的偏差减小。\n  5. $(c,h) = (5,10)$，靠近下边界，以说明另一侧的边界效应。\n\n答案规格：\n- 对于每个测试用例，计算偏差 $\\mathrm{bias}(c,h)$，结果为一个实数。\n- 您的程序必须生成单行输出，其中包含五个结果，以逗号分隔并用方括号括起，例如：$[\\text{结果}_1,\\text{结果}_2,\\text{结果}_3,\\text{结果}_4,\\text{结果}_5]$，每个数字都打印为十进制浮点值。不涉及单位。不使用角度。如果您选择四舍五入，请对所有输出统一保留相同的小数位数。\n- 您的程序必须是一个完整、可运行的实现，完全按照规定执行上述计算，并且只输出所需的单行结果。不允许用户输入，也不得使用外部文件。实现语言另行指定，必须严格遵守。",
            "solution": "该问题要求设计并实现一个算法，用以计算局部线性回归断点 (RD) 估计量的偏差。分析的重点是当断点接近数据定义域的边界时，偏差如何受到影响。整个过程是确定性的，使用密集的点网格和无噪声的结果函数。\n\n首先，我们根据提供的规格形式化问题设置。驱动变量（表示为 $X$）在区间 $[0, 100]$ 上具有一个紧支集。我们在一个确定性的网格上进行操作，该网格包含此区间内的 $M = 10001$ 个等距点，从 $X=0$ 开始到 $X=100$ 结束。潜在结果均值函数由下式给出：\n$$m_0(x) = 2 + 0.04\\,x + 0.002\\,x^2$$\n$$m_1(x) = m_0(x) + \\tau$$\n其中真实处理效应 $\\tau$ 是一个等于 $3$ 的常数。对于给定的断点 $c$，观测结果 $Y$ 被确定性地定义为：\n$$Y = m_0(X) + \\tau \\cdot \\mathbf{1}\\{X \\ge c\\}$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。RD 估计对象是条件期望函数在断点处的跳跃，在这个无噪声的设置中，它恰好等于 $\\tau = m_1(c) - m_0(c) = 3$。\n\n任务是实现一个单边局部线性 RD 估计量。这涉及到在断点 $c$ 的两侧分别执行加权最小二乘 (WLS) 回归。对于给定的带宽 $h  0$，用于估计的数据被限制在窗口 $|X - c| \\le h$ 内。待拟合的回归模型是：\n$$Y = \\alpha + \\beta\\,(X - c)$$\n权重由三角核函数 $K(u) = \\max\\{0, 1 - |u|\\}$ 提供，其中 $u = (X - c)/h$。\n\n对于断点的右侧，我们对所有满足 $c \\le X_i \\le \\min(c+h, 100)$ 的数据点 $X_i$ 最小化加权残差平方和：\n$$ \\min_{\\alpha_+, \\beta_+} \\sum_{i: c \\le X_i \\le c+h} K\\left(\\frac{X_i - c}{h}\\right) \\left[ Y_i - \\left(\\alpha_+ + \\beta_+ (X_i - c)\\right) \\right]^2 $$\n这个 WLS 问题的解给出了估计的截距 $\\widehat{\\alpha}_+$。类似地，对于左侧，我们使用满足 $\\max(c-h, 0) \\le X_i  c$ 的数据点 $X_i$ 来找到截距 $\\widehat{\\alpha}_-$。注意左侧的严格不等式 $X_i  c$ 以及与 $0$ 和 $100$ 处定义域边界的相互作用。\n\n系数向量 $\\mathbf{b} = [\\alpha, \\beta]^T$ 的 WLS 估计量可以表示为矩阵形式：\n$$ \\widehat{\\mathbf{b}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} (\\mathbf{X}^T \\mathbf{W} \\mathbf{y}) $$\n在这里，$\\mathbf{y}$ 是局部窗口内结果值的向量，$\\mathbf{X}$ 是设计矩阵，其第一列为全1，第二列为中心化的变量 $(X_i - c)$，$\\mathbf{W}$ 是一个对角矩阵，其对角线元素为核权重 $K((X_i-c)/h)$。估计的截距 $\\widehat{\\alpha}$ 是向量 $\\widehat{\\mathbf{b}}$ 的第一个元素。\n\n一旦我们从左侧和右侧回归中计算出截距 $\\widehat{\\alpha}_-$ 和 $\\widehat{\\alpha}_+$，处理效应的 RD 估计值为：\n$$ \\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_- $$\n对于给定的 $(c,h)$ 对，该估计量的偏差计算如下：\n$$ \\mathrm{bias}(c,h) = \\widehat{\\tau}_{\\mathrm{RD}} - \\tau $$\n\n科学目标是展示当 $c$ 接近定义域边界时，这个偏差如何变化。局部线性估计量的偏差主要源于线性模型与真实的潜在函数之间的不匹配，在本例中，该函数具有非零曲率 ($m_0''(x) = 0.004$)。对于一个具有对称窗口 $[c-h, c+h]$ 的内部点 $c$，$\\widehat{\\alpha}_+$ 和 $\\widehat{\\alpha}_-$ 中的偏差倾向于具有相似的结构，并在计算它们的差值时部分抵消。然而，当 $c$ 靠近边界时（例如，$c=95$ 且 $h=10$），其中一个估计窗口会被截断（例如，右侧窗口变为 $[95, 100]$ 而不是 $[95, 105]$）。这种截断会产生一个非对称的有效核，从而破坏了偏差项的对称性。抵消作用不再有效，导致 $\\widehat{\\tau}_{\\mathrm{RD}}$ 的整体偏差更大。减小带宽 $h$ 会缩短估计窗口，使线性近似在更小的范围内更准确，从而即使在边界情况下也能减小曲率引起的偏差的大小。\n\n算法通过迭代每个测试用例 $(c,h)$ 来进行：\n1.  在 $X_i \\in [0, 100]$ 的网格上生成完整的观测结果值 $Y_i$ 集合。\n2.  对于给定的 $(c,h)$，对左侧执行 WLS 程序，使用来自 $[\\max(0, c-h), c)$ 的数据，以获得 $\\widehat{\\alpha}_-$。\n3.  对右侧执行 WLS 程序，使用来自 $[c, \\min(100, c+h)]$ 的数据，以获得 $\\widehat{\\alpha}_+$。\n4.  计算 $\\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_-$。\n5.  计算并存储偏差 $\\widehat{\\tau}_{\\mathrm{RD}} - 3$。\n对所有五个提供的测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of a local linear Regression Discontinuity (RD) estimator\n    for several test cases, demonstrating boundary effects.\n    \"\"\"\n\n    # Define constants from the problem statement\n    M = 10001\n    TAU = 3.0\n\n    # Generate the deterministic grid for the running variable X\n    X_grid = np.linspace(0, 100, M)\n\n    def m0(x):\n        \"\"\"\n        Defines the potential outcome mean function for the control group.\n        \"\"\"\n        return 2.0 + 0.04 * x + 0.002 * x**2\n\n    def triangular_kernel(u):\n        \"\"\"\n        Defines the triangular kernel function.\n        \"\"\"\n        return np.maximum(0, 1 - np.abs(u))\n\n    def local_linear_fit(X_grid, Y_values, c, h, side):\n        \"\"\"\n        Performs a one-sided local linear regression and returns the\n        estimated intercept.\n\n        Args:\n            X_grid (np.ndarray): The grid of running variable values.\n            Y_values (np.ndarray): The corresponding observed outcome values.\n            c (float): The cutoff point.\n            h (float): The bandwidth.\n            side (str): 'left' or 'right' of the cutoff.\n\n        Returns:\n            float: The estimated intercept (alpha_hat).\n        \"\"\"\n        # Select data points based on side, bandwidth, and support [0, 100]\n        if side == 'right':\n            mask = (X_grid >= c)  (X_grid = c + h)\n        elif side == 'left':\n            mask = (X_grid  c)  (X_grid >= c - h)\n        else:\n            raise ValueError(\"Side must be 'left' or 'right'\")\n        \n        X_local = X_grid[mask]\n        Y_local = Y_values[mask]\n\n        if len(X_local)  2:\n            # Not enough points for a linear fit. This case is not expected\n            # with the problem's dense grid.\n            return np.nan\n\n        # Recenter the running variable\n        X_centered = X_local - c\n        \n        # Calculate kernel weights based on u = (X - c) / h\n        u = X_centered / h\n        weights = triangular_kernel(u)\n        \n        # Set up the weighted least squares problem: Y = alpha + beta * (X - c)\n        # Construct the design matrix.\n        design_matrix = np.vstack([np.ones(len(X_centered)), X_centered]).T\n\n        # Solve the normal equations: (X'WX)b = X'Wy\n        # This is more efficient than creating a diagonal weight matrix.\n        XT_W_X = design_matrix.T @ (weights[:, np.newaxis] * design_matrix)\n        XT_W_y = design_matrix.T @ (weights * Y_local)\n\n        try:\n            # Solve for the coefficient vector [alpha, beta]\n            coeffs = np.linalg.solve(XT_W_X, XT_W_y)\n            alpha_hat = coeffs[0]\n        except np.linalg.LinAlgError:\n            # This would occur if the matrix is singular.\n            alpha_hat = np.nan\n            \n        return alpha_hat\n\n    def compute_bias(c, h, X_grid, m0_func, tau_val):\n        \"\"\"\n        Computes the RD estimator bias for a given (c, h) pair.\n        \"\"\"\n        # Generate the observed outcome Y based on the cutoff c\n        Y_values = m0_func(X_grid) + tau_val * (X_grid >= c)\n        \n        # Get intercept estimates from the left and right sides\n        alpha_hat_plus = local_linear_fit(X_grid, Y_values, c, h, side='right')\n        alpha_hat_minus = local_linear_fit(X_grid, Y_values, c, h, side='left')\n        \n        # Compute the RD estimate of the treatment effect\n        tau_hat_rd = alpha_hat_plus - alpha_hat_minus\n        \n        # Compute the final bias\n        bias = tau_hat_rd - tau_val\n        return bias\n\n    # Define the test suite of (c, h) pairs\n    test_cases = [\n        (50, 10),\n        (95, 10),\n        (98, 10),\n        (95, 5),\n        (5, 10),\n    ]\n\n    results = []\n    for c, h in test_cases:\n        bias_result = compute_bias(c, h, X_grid, m0, TAU)\n        results.append(bias_result)\n\n    # Format the output as a comma-separated list in brackets,\n    # with consistent decimal formatting.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "回归断点设计的核心假设是，我们在断点处观察到的结果跳变是由该断点处的处理分配所引起的。本练习将引导您探讨一个对RDD有效性的关键威胁：政策断点与实际处理生效点之间的不匹配。通过推导并计算由此产生的偏差，您将直接了解到，在错误的断点上进行RDD分析可能会完全无法识别出真正的处理效应。",
            "id": "3168533",
            "problem": "考虑一个断点回归设计 (Regression Discontinuity Design, RDD)，其定义为在断点处，观测结果的条件期望的右极限与左极限之差。设驱动变量为 $X \\in \\mathbb{R}$，其密度在断点 $c \\in \\mathbb{R}$ 附近连续可微。假设观测结果 $Y$ 由以下公式生成：\n$$\nY = f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon,\n$$\n其中 $f:\\mathbb{R}\\to\\mathbb{R}$ 在 $c$ 处连续，$\\tau \\in \\mathbb{R}$ 是一个恒定的处理效应，$\\Delta  0$ 是政策断点 $c$ 与实际处理开始点 $c+\\Delta$ 之间的一个非零偏移，$\\varepsilon$ 是一个均值为零的扰动项，满足 $\\mathbb{E}[\\varepsilon \\mid X] = 0$，且 $\\mathbb{1}\\{\\cdot\\}$ 是指示函数。在断点 $c$ 处不存在操纵行为（即 $X$ 的密度在 $c$ 处是连续的）。\n\n你的任务是：\n\n1) 使用 RDD 估计量 (estimand) 的定义\n$$\n\\theta_{\\text{RDD}}(c) = \\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x],\n$$\n在给定的数据生成过程下，推导 $\\theta_{\\text{RDD}}(c)$，并表达其相对于处理效应参数 $\\tau$ 的偏差。\n\n2) 考虑一个使用均匀核和对称带宽 $h0$ 的局部常数 RDD 估计器 (estimator)。也就是说，该估计器计算的是右侧窗口 $[c, c+h]$ 内 $Y$ 的样本均值与左侧窗口 $[c-h, c)$ 内 $Y$ 的样本均值之差。在大样本、 $X$ 在 $c$ 处的密度连续、以及假设 $f(x)$ 在邻域 $[c-h, c+h]$ 内近似为常数（即，对于 $x \\in [c-h, c+h]$，$f(x) \\approx f(c)$）的条件下，推导该估计器的概率极限及其相对于 $\\tau$ 的偏差，将其表示为 $\\tau$、$\\Delta$ 和 $h$ 的函数。\n\n3) 实现一个程序，为以下参数值测试套件计算在任务2中推导出的偏差函数。对于每种情况，取 $c=0$（此选择不影响你推导出的偏差表达式），并以实数（浮点数）形式返回偏差：\n- 情况A（理想路径）：$(\\tau, \\Delta, h) = (2.0, 0.3, 1.0)$。\n- 情况B（边界情况）：$(\\tau, \\Delta, h) = (1.0, 1.0, 1.0)$。\n- 情况C（右侧窗口无处理个体的边缘情况）：$(\\tau, \\Delta, h) = (1.5, 1.2, 1.0)$。\n- 情况D（近乎对齐）：$(\\tau, \\Delta, h) = (0.75, 10^{-6}, 0.5)$。\n- 情况E（负处理效应）：$(\\tau, \\Delta, h) = (-1.5, 0.25, 0.5)$。\n\n此问题中没有物理单位。所有输出必须表示为实数。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中 $r_i$ 是对上述第 $i$ 种情况计算出的偏差。",
            "solution": "该问题要求分析一个断点回归设计（RDD），其中处理分配相对于政策断点发生了偏移。我们必须首先推导理论上的 RDD 估计量及其偏差，然后推导一个实用的局部常数估计器的概率极限和偏差，最后实现一个程序来计算该估计器在特定参数值下的偏差。\n\n**任务1：RDD 估计量及其偏差的推导**\n\nRDD 估计量定义为结果 $Y$ 的条件期望在断点 $c$ 处的跳跃：\n$$\n\\theta_{\\text{RDD}}(c) = \\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x]\n$$\n数据生成过程由 $Y = f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon$ 给出。我们首先求 $Y$ 在给定 $X=x$ 下的条件期望。利用期望的线性和假设 $\\mathbb{E}[\\varepsilon \\mid X] = 0$，我们有：\n$$\n\\mathbb{E}[Y \\mid X = x] = \\mathbb{E}[f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon \\mid X = x]\n$$\n$$\n\\mathbb{E}[Y \\mid X = x] = f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\} + \\mathbb{E}[\\varepsilon \\mid X = x]\n$$\n$$\n\\mathbb{E}[Y \\mid X = x] = f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}\n$$\n现在，我们计算在 $x=c$ 处的右极限和左极限。\n\n对于右极限，$x$ 从上方趋近于 $c$，因此 $x > c$。由于问题陈述 $\\Delta > 0$，对于足够接近 $c$ 的 $x$ 值（特别是在区间 $(c, c+\\Delta)$ 内），条件 $x \\ge c+\\Delta$ 为假。因此，指示函数 $\\mathbb{1}\\{x \\ge c + \\Delta\\}$ 为 $0$。又鉴于 $f(x)$ 在 $c$ 处连续，该极限为：\n$$\n\\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] = \\lim_{x \\downarrow c} (f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}) = f(c) + \\tau \\cdot 0 = f(c)\n$$\n对于左极限，$x$ 从下方趋近于 $c$，因此 $x  c$。由于 $\\Delta > 0$，总有 $x  c  c+\\Delta$ 成立。因此，条件 $x \\ge c+\\Delta$ 为假，指示函数 $\\mathbb{1}\\{x \\ge c + \\Delta\\}$ 为 $0$。该极限为：\n$$\n\\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x] = \\lim_{x \\uparrow c} (f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}) = f(c) + \\tau \\cdot 0 = f(c)\n$$\n将这些极限代入 RDD 估计量的定义中，得到：\n$$\n\\theta_{\\text{RDD}}(c) = f(c) - f(c) = 0\n$$\n该估计量相对于真实处理效应 $\\tau$ 的偏差定义为 $\\text{Bias} = \\theta_{\\text{RDD}}(c) - \\tau$。因此，偏差为：\n$$\n\\text{Bias} = 0 - \\tau = -\\tau\n$$\n这个结果是直观的：RDD 估计量寻找在 $c$ 处的断点，但条件均值函数中的实际断点发生在 $c+\\Delta$ 处。由于 $f(x)$ 在 $c$ 处的连续性，没有检测到跳跃，导致估计值为 $0$，偏差为 $-\\tau$。\n\n**任务2：估计器的概率极限和偏差的推导**\n\n我们考虑一个局部常数 RDD 估计器，它是 $Y$ 在窗口 $[c, c+h)$ 和 $[c-h, c)$ 内样本均值的差。在大样本中（当 $n \\to \\infty$ 时），该估计器的概率极限（plim），记为 $\\hat{\\theta}_h$，是真实条件期望在这些窗口上的差值：\n$$\n\\text{plim}_{n \\to \\infty} \\hat{\\theta}_h = \\mathbb{E}[Y \\mid c \\le X  c+h] - \\mathbb{E}[Y \\mid c-h \\le X  c]\n$$\n我们分别分析每一项。对于左侧窗口 $[c-h, c)$，任何 $X$ 的值都满足 $X  c  c+\\Delta$（因为 $\\Delta > 0$）。因此，对于此窗口中的所有 $X$，$\\mathbb{1}\\{X \\ge c + \\Delta\\} = 0$。\n$$\n\\mathbb{E}[Y \\mid c-h \\le X  c] = \\mathbb{E}[f(X) + \\tau \\cdot 0 + \\varepsilon \\mid c-h \\le X  c] = \\mathbb{E}[f(X) \\mid c-h \\le X  c]\n$$\n使用 $f(x) \\approx f(c)$ 对于 $x \\in [c-h, c+h]$ 的假设，上式可简化为：\n$$\n\\mathbb{E}[Y \\mid c-h \\le X  c] \\approx f(c)\n$$\n对于右侧窗口 $[c, c+h)$，指示函数不总是为零。\n$$\n\\mathbb{E}[Y \\mid c \\le X  c+h] = \\mathbb{E}[f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon \\mid c \\le X  c+h]\n$$\n使用近似 $f(x) \\approx f(c)$ 和迭代期望定律，上式变为：\n$$\n\\mathbb{E}[Y \\mid c \\le X  c+h] \\approx f(c) + \\tau \\cdot \\mathbb{E}[\\mathbb{1}\\{X \\ge c + \\Delta\\} \\mid c \\le X  c+h]\n$$\n指示函数的期望是条件概率 $P(X \\ge c+\\Delta \\mid c \\le X  c+h)$。这可以写作：\n$$\nP(X \\ge c+\\Delta \\mid c \\le X  c+h) = \\frac{P((X \\ge c+\\Delta) \\cap (c \\le X  c+h))}{P(c \\le X  c+h)}\n$$\n分子是两个区间交集的概率，即 $P(c+\\Delta \\le X  c+h)$。只有当 $c+\\Delta  c+h$（即 $\\Delta  h$）时，这个区间才非空。如果 $\\Delta \\ge h$，则概率为 $0$。\n设 $g(x)$ 为 $X$ 的概率密度函数（PDF）。由于 $g(x)$ 在 $c$ 处是连续的，对于一个小的带宽 $h$，我们可以近似认为密度在区间 $[c, c+h)$ 上是常数，即 $g(x) \\approx g(c)$。\n分子的概率是 $\\int_{c+\\Delta}^{c+h} g(x) dx \\approx g(c) \\cdot ((c+h)-(c+\\Delta)) = g(c) \\cdot (h-\\Delta)$，当 $\\Delta  h$ 时。如果 $\\Delta \\ge h$，则为 $0$。这可以写作 $g(c) \\cdot \\max(0, h-\\Delta)$。\n分母的概率是 $\\int_c^{c+h} g(x) dx \\approx g(c) \\cdot ((c+h)-c) = g(c) \\cdot h$。\n所以，条件概率是：\n$$\nP(X \\ge c+\\Delta \\mid c \\le X  c+h) \\approx \\frac{g(c) \\cdot \\max(0, h-\\Delta)}{g(c) \\cdot h} = \\frac{\\max(0, h-\\Delta)}{h}\n$$\n估计器的概率极限则为：\n$$\n\\text{plim}_{n \\to \\infty} \\hat{\\theta}_h \\approx \\left( f(c) + \\tau \\frac{\\max(0, h-\\Delta)}{h} \\right) - f(c) = \\tau \\frac{\\max(0, h-\\Delta)}{h}\n$$\n渐近偏差是此概率极限与真实参数 $\\tau$ 之间的差：\n$$\n\\text{Bias} = \\text{plim}_{n \\to \\infty} \\hat{\\theta}_h - \\tau \\approx \\tau \\frac{\\max(0, h-\\Delta)}{h} - \\tau = \\tau \\left( \\frac{\\max(0, h-\\Delta)}{h} - 1 \\right)\n$$\n我们可以简化这个表达式。注意 $\\max(0, h-\\Delta) = h - \\min(h, \\Delta)$。\n$$\n\\text{Bias} \\approx \\tau \\left( \\frac{h-\\min(h, \\Delta)}{h} - 1 \\right) = \\tau \\left( 1 - \\frac{\\min(h, \\Delta)}{h} - 1 \\right) = -\\tau \\frac{\\min(h, \\Delta)}{h}\n$$\n这是偏差的最终表达式。它取决于真实效应 $\\tau$、政策错位 $\\Delta$ 和估计器的带宽 $h$。该公式对两种情况均有效：\n1. 如果 $\\Delta  h$，则 $\\min(h, \\Delta) = \\Delta$，所以偏差 $\\approx -\\tau \\frac{\\Delta}{h}$。\n2. 如果 $\\Delta \\ge h$，则 $\\min(h, \\Delta) = h$，所以偏差 $\\approx -\\tau \\frac{h}{h} = -\\tau$。\n\n**任务3：偏差的数值计算**\n\n我们现在将实现一个程序，使用推导出的公式 $\\text{Bias} = -\\tau \\frac{\\min(h, \\Delta)}{h}$ 来计算所提供测试用例的偏差，其中 $c=0$。$c$ 的值不影响偏差公式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of a local-constant RDD estimator with a misaligned cutoff.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (tau, Delta, h)\n    test_cases = [\n        # Case A (happy path)\n        (2.0, 0.3, 1.0),\n        # Case B (boundary)\n        (1.0, 1.0, 1.0),\n        # Case C (edge with no treated in right window)\n        (1.5, 1.2, 1.0),\n        # Case D (near alignment)\n        (0.75, 1e-6, 0.5),\n        # Case E (negative treatment effect)\n        (-1.5, 0.25, 0.5),\n    ]\n\n    def calculate_bias(tau: float, delta: float, h: float) -> float:\n        \"\"\"\n        Calculates the asymptotic bias of the local-constant RDD estimator.\n\n        The formula for the bias is derived as:\n        Bias = -tau * min(h, delta) / h\n\n        Args:\n            tau: The true treatment effect.\n            delta: The shift between the policy cutoff and the treatment start.\n            h: The symmetric bandwidth of the estimator.\n\n        Returns:\n            The calculated bias as a float.\n        \"\"\"\n        # The derivation shows that the bandwidth h must be positive.\n        # The problem statement specifies h > 0, so no division by zero error.\n        bias = -tau * min(h, delta) / h\n        return bias\n\n    results = []\n    for case in test_cases:\n        tau_val, delta_val, h_val = case\n        result = calculate_bias(tau_val, delta_val, h_val)\n        results.append(result)\n\n    # Format the final output string as a comma-separated list in brackets.\n    # The map(str, ...) converts each float result to its string representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "这个高级练习将带您从识别问题转向优化解决方案。在回归断点设计中，核函数（kernel function）的选择会直接影响估计量的偏差-方差权衡，进而决定其整体性能。通过为不同的核函数（如三角核和Epanechnikov核）推导渐近均方误差（Mean Squared Error, MSE），您将定量地分析这种权衡，并深入了解估计量具体设定的选择如何影响其最终表现。",
            "id": "3168499",
            "problem": "您需要在一个单边局部多项式回归断点设计（RD）中，推导并实现两种核函数的原则性比较。考虑在断点 $c$ 处的一个精确回归断点设计（RD），其中处理效应是条件期望函数 $m_{+}(x)$ 和 $m_{-}(x)$ 在 $c$ 右侧和左侧的跳跃 $\\tau = m_{+}(c) - m_{-}(c)$。假设在 $c$ 的每一侧都使用单边局部线性回归（多项式阶数为1），并使用共同的带宽 $h$。令归一化的处理变量坐标在右侧为 $u_{+} = (x - c)/h$，在左侧为 $u_{-} = (c - x)/h$，因此单边核函数的支撑集为 $[0,1]$。\n\n从以下基本基础开始：\n- 加权最小二乘法（WLS）将局部多项式估计量定义为最小化加权残差平方和的截距和斜率的最小化器，其中权重由应用于距断点的归一化距离的核函数 $K(u)$ 给出。\n- $m_{+}(x)$ 和 $m_{-}(x)$ 的平滑性意味着在 $x=c$ 附近的二阶泰勒展开是有效的，这产生了在边界处使用局部线性回归时决定渐近偏差的主导项。\n- 在局部多项式回归的统计学习中，标准抽样、独立性和正则性条件下，大数定律和中心极限定理意味着由核加权单项式构建的设计矩阵收敛于其总体矩矩阵。这些矩是通过在单边支撑集上对 $u$ 的幂函数积分 $K(u)$ 和 $K(u)^2$ 来定义的。\n\n您的任务是：\n1. 为任何支撑集在 $[0,1]$ 上的核函数 $K(u)$ 定义单边核矩矩阵：\n   - 令 $p(u) = [1, u]^{\\top}$。\n   - 定义矩矩阵 $S = \\int_{0}^{1} K(u) p(u) p(u)^{\\top} \\, du$，其元素为 $S_{11} = \\int_{0}^{1} K(u) \\, du$, $S_{12}=S_{21} = \\int_{0}^{1} K(u) u \\, du$, $S_{22} = \\int_{0}^{1} K(u) u^{2} \\, du$。\n   - 定义偏差加载向量 $R = \\int_{0}^{1} K(u) u^{2} p(u) \\, du$，即 $R = \\left[\\int_{0}^{1} K(u) u^{2} \\, du, \\int_{0}^{1} K(u) u^{3} \\, du \\right]^{\\top}$。\n   - 定义方差加载矩阵 $\\Xi = \\int_{0}^{1} K(u)^{2} p(u) p(u)^{\\top} \\, du$，其元素为 $\\Xi_{11} = \\int_{0}^{1} K(u)^{2} \\, du$, $\\Xi_{12}=\\Xi_{21} = \\int_{0}^{1} K(u)^{2} u \\, du$, $\\Xi_{22} = \\int_{0}^{1} K(u)^{2} u^{2} \\, du$。\n2. 使用上述定义，推导在 $c$ 的每一侧的单边局部线性截距的主导阶渐近偏差和方差，用 $S$、$R$ 和 $\\Xi$ 表示。然后推导 RD 估计量 $\\hat{\\tau}$（作为右侧和左侧截距之差）的相应量。明确展示断点处的曲率如何通过二阶导数 $m_{+}''(c)$ 和 $m_{-}''(c)$ 进入公式。\n3. 将推导应用于两种特定的核函数：\n   - 三角核函数：当 $u \\in [0,1]$ 时，$K(u) = 1 - u$，否则 $K(u)=0$。\n   - Epanechnikov 核函数：当 $u \\in [0,1]$ 时，$K(u) = \\frac{3}{4}(1 - u^{2})$，否则 $K(u)=0$。\n   精确计算所需的核积分，构建 $S$、$R$ 和 $\\Xi$，并评估当两侧使用相同的核函数和带宽时，适用于 RD 估计量的标量偏差和方差常数。\n4. 在断点处曲率非零的情况下，量化并比较两种核函数下 $\\hat{\\tau}$ 的均方误差（MSE）。该 MSE 通过曲率差 $\\Delta_{2} = m_{+}''(c) - m_{-}''(c) \\neq 0$、共同带宽 $h$、样本量 $n$、处理变量在 $c$ 处的左右密度 $g_{+}(c)$ 和 $g_{-}(c)$ 以及左右噪声方差 $\\sigma_{+}^{2}$ 和 $\\sigma_{-}^{2}$ 来表示。您的程序必须实现推导出的公式来计算 $MSE_{\\text{tri}}$ 和 $MSE_{\\text{epa}}$，然后对下面列出的每个测试用例输出 $MSE_{\\text{tri}} - MSE_{\\text{epa}}$。\n5. 使用以下测试套件。对于每个测试用例，参数以元组 $(n, h, g_{+}(c), g_{-}(c), \\sigma_{+}, \\sigma_{-}, \\Delta_{2})$ 的形式提供。不涉及物理单位。不涉及角度。最终答案为实数。\n   - 案例 1（平衡，中等带宽，中等曲率）：$(10000, 0.2, 1.0, 1.0, 1.0, 1.0, 1.5)$。\n   - 案例 2（平衡，小带宽，较强曲率）：$(10000, 0.05, 1.0, 1.0, 1.0, 1.0, 2.0)$。\n   - 案例 3（平衡密度，异方差两侧）：$(5000, 0.15, 0.8, 0.8, 1.0, 2.0, 1.0)$。\n   - 案例 4（不平衡密度，等噪声，负曲率差）：$(8000, 0.12, 1.0, 0.5, 1.2, 1.2, -1.2)$。\n   - 案例 5（平衡密度，大曲率，中等带宽）：$(5000, 0.25, 0.9, 0.9, 1.0, 1.0, 5.0)$。\n   - 案例 6（平衡密度，极大带宽，大曲率）：$(3000, 0.5, 1.0, 1.0, 1.0, 1.0, 5.0)$。\n6. 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个结果必须是浮点数，四舍五入到六位小数，并按顺序对应于上述测试用例。例如，输出可能看起来像 $[0.001234,0.000567,-0.000890,0.000012,0.000345,-0.001234]$。",
            "solution": "我们从精确回归断点设计（RD）中边界处局部多项式回归的基本定义开始，介绍推导和算法。\n\n设置和定义。在断点 $c$ 处的精确 RD 具有条件期望 $m_{+}(x)$（对于 $x \\ge c$）和 $m_{-}(x)$（对于 $x  c$）。我们感兴趣的参数是 $\\tau = m_{+}(c) - m_{-}(c)$。我们使用两个单边局部线性回归的差来估计 $\\tau$。对于右侧，定义 $u = (x - c)/h$ 并使用单边核函数 $K(u)$ 将其限制在 $u \\in [0,1]$；对于左侧，定义 $u = (c - x)/h$，同样有 $u \\in [0,1]$。在每一侧，我们拟合一个局部线性回归（多项式阶数为1）。令 $p(u) = [1, u]^{\\top}$。记局部设计矩矩阵为\n$$\nS = \\int_{0}^{1} K(u) p(u) p(u)^{\\top} \\, du\n= \\begin{bmatrix}\n\\mu_{0}  \\mu_{1} \\\\\n\\mu_{1}  \\mu_{2}\n\\end{bmatrix},\n$$\n其中 $\\mu_{j} = \\int_{0}^{1} K(u) u^{j} \\, du$。偏差加载向量为\n$$\nR = \\int_{0}^{1} K(u) u^{2} p(u) \\, du\n= \\begin{bmatrix}\n\\mu_{2} \\\\ \\mu_{3}\n\\end{bmatrix},\n$$\n其中 $\\mu_{3} = \\int_{0}^{1} K(u) u^{3} \\, du$。方差加载矩阵为\n$$\n\\Xi = \\int_{0}^{1} K(u)^{2} p(u) p(u)^{\\top} \\, du\n= \\begin{bmatrix}\n\\kappa_{0}  \\kappa_{1} \\\\\n\\kappa_{1}  \\kappa_{2}\n\\end{bmatrix},\n$$\n其中 $\\kappa_{j} = \\int_{0}^{1} K(u)^{2} u^{j} \\, du$。这些积分源于将大数定律和中心极限定理应用于加权回归量和误差。\n\n单边局部线性截距及其渐近性。设给定一侧 $m(c)$ 的单边局部线性估计量为截距 $\\hat{\\alpha}$，它来自权重为 $K(u)$、回归量为 $p(u)$ 的加权最小二乘解。在平滑性假设下，我们使用二阶泰勒展开\n$$\nm(c + h u) = m(c) + m'(c) h u + \\tfrac{1}{2} m''(c) h^{2} u^{2} + o(h^{2})\n$$\n（用于右侧），类似地\n$$\nm(c - h u) = m(c) - m'(c) h u + \\tfrac{1}{2} m''(c) h^{2} u^{2} + o(h^{2})\n$$\n（用于左侧）。单边局部线性截距的主导渐近偏差是由曲率项 $\\tfrac{1}{2} m''(c) h^{2}$ 与设计矩阵相互作用驱动的。标准的局部多项式理论表明\n$$\n\\text{Bias}(\\hat{\\alpha}) = \\tfrac{1}{2} h^{2} m''(c) \\cdot B_{K} + o(h^{2}), \\quad \\text{其中} \\quad B_{K} = e_{1}^{\\top} S^{-1} R,\n$$\n其中 $e_{1} = [1, 0]^{\\top}$。主导渐近方差为\n$$\n\\text{Var}(\\hat{\\alpha}) = \\frac{\\sigma^{2}}{n h g(c)} \\cdot V_{K} + o\\!\\left(\\frac{1}{n h}\\right), \\quad \\text{其中} \\quad V_{K} = e_{1}^{\\top} S^{-1} \\Xi S^{-1} e_{1}.\n$$\n此处，$n$ 是样本量，$h$ 是带宽，$g(c)$ 是相关一侧处理变量在 $c$ 处的密度，$\\sigma^{2}$ 是该侧的误差方差。这些表达式由 WLS 正规方程、矩向 $S$、$R$ 和 $\\Xi$ 的收敛以及局部多项式回归中的标准方差传播得出。\n\nRD 估计量作为单边截距之差。RD 估计量是右侧和左侧截距之差，$\\hat{\\tau} = \\hat{\\alpha}_{+} - \\hat{\\alpha}_{-}$。其主导渐近偏差是两个单边偏差之差：\n$$\n\\text{Bias}(\\hat{\\tau}) = \\tfrac{1}{2} h^{2} \\left( m_{+}''(c) - m_{-}''(c) \\right) B_{K} + o(h^{2})\n= \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} + o(h^{2}),\n$$\n其中 $\\Delta_{2} = m_{+}''(c) - m_{-}''(c)$ 是在断点处的曲率差。两个单边截距估计量使用不相交的样本（在 $c$ 的两侧），因此主导方差相加：\n$$\n\\text{Var}(\\hat{\\tau}) = \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} V_{K} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} V_{K} + o\\!\\left(\\frac{1}{n h}\\right)\n= V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} \\right) + o\\!\\left(\\frac{1}{n h}\\right).\n$$\n因此，$\\hat{\\tau}$ 的主导均方误差（MSE）为\n$$\n\\text{MSE}(\\hat{\\tau}) \\approx \\left( \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} \\right)^{2} + V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} \\right).\n$$\n\n特定核函数的积分。我们现在为两种核函数计算 $S$、$R$ 和 $\\Xi$。\n\n1. 三角核函数 $K(u) = 1 - u$ on $[0,1]$：\n   - 矩 $\\mu_{j} = \\int_{0}^{1} (1 - u) u^{j} \\, du = \\frac{1}{(j+1)(j+2)}$，因此\n     $$\n     \\mu_{0} = \\tfrac{1}{2}, \\quad \\mu_{1} = \\tfrac{1}{6}, \\quad \\mu_{2} = \\tfrac{1}{12}, \\quad \\mu_{3} = \\tfrac{1}{20}.\n     $$\n   - 核函数平方的矩 $\\kappa_{j} = \\int_{0}^{1} (1 - u)^{2} u^{j} \\, du\n     = \\frac{1}{j+1} - \\frac{2}{j+2} + \\frac{1}{j+3}$，因此\n     $$\n     \\kappa_{0} = \\tfrac{1}{3}, \\quad \\kappa_{1} = \\tfrac{1}{12}, \\quad \\kappa_{2} = \\tfrac{1}{30}.\n     $$\n   - 构建 $S = \\begin{bmatrix} \\tfrac{1}{2}  \\tfrac{1}{6} \\\\ \\tfrac{1}{6}  \\tfrac{1}{12} \\end{bmatrix}$，$R = \\begin{bmatrix} \\tfrac{1}{12} \\\\ \\tfrac{1}{20} \\end{bmatrix}$，和 $\\Xi = \\begin{bmatrix} \\tfrac{1}{3}  \\tfrac{1}{12} \\\\ \\tfrac{1}{12}  \\tfrac{1}{30} \\end{bmatrix}$。\n   - 对 $S$ 求逆并评估常数：\n     $$\n     S^{-1} = \\begin{bmatrix} 6  -12 \\\\ -12  36 \\end{bmatrix}, \\quad\n     B_{\\text{tri}} = e_{1}^{\\top} S^{-1} R = 6 \\cdot \\tfrac{1}{12} - 12 \\cdot \\tfrac{1}{20} = -\\tfrac{1}{10},\n     $$\n     $$\n     V_{\\text{tri}} = e_{1}^{\\top} S^{-1} \\Xi S^{-1} e_{1} = \\tfrac{24}{5}.\n     $$\n\n2. Epanechnikov 核函数 $K(u) = \\frac{3}{4}(1 - u^{2})$ on $[0,1]$：\n   - 矩 $\\mu_{j} = \\frac{3}{4}\\left( \\frac{1}{j+1} - \\frac{1}{j+3} \\right)$，因此\n     $$\n     \\mu_{0} = \\tfrac{1}{2}, \\quad \\mu_{1} = \\tfrac{3}{16}, \\quad \\mu_{2} = \\tfrac{1}{10}, \\quad \\mu_{3} = \\tfrac{1}{16}.\n     $$\n   - 核函数平方的矩使用 $K(u)^{2} = \\frac{9}{16}(1 - 2u^{2} + u^{4})$，所以\n     $$\n     \\kappa_{j} = \\frac{9}{16}\\left( \\frac{1}{j+1} - \\frac{2}{j+3} + \\frac{1}{j+5} \\right),\n     $$\n     因此\n     $$\n     \\kappa_{0} = \\tfrac{3}{10}, \\quad \\kappa_{1} = \\tfrac{3}{32}, \\quad \\kappa_{2} = \\tfrac{3}{70}.\n     $$\n   - 构建 $S = \\begin{bmatrix} \\tfrac{1}{2}  \\tfrac{3}{16} \\\\ \\tfrac{3}{16}  \\tfrac{1}{10} \\end{bmatrix}$，$R = \\begin{bmatrix} \\tfrac{1}{10} \\\\ \\tfrac{1}{16} \\end{bmatrix}$，和 $\\Xi = \\begin{bmatrix} \\tfrac{3}{10}  \\tfrac{3}{32} \\\\ \\tfrac{3}{32}  \\tfrac{3}{70} \\end{bmatrix}$。\n   - 对 $S$ 求逆并评估常数：\n     $$\n     S^{-1} = \\begin{bmatrix} \\tfrac{128}{19}  -\\tfrac{240}{19} \\\\ -\\tfrac{240}{19}  \\tfrac{640}{19} \\end{bmatrix}, \\quad\n     B_{\\text{epa}} = e_{1}^{\\top} S^{-1} R = \\tfrac{128}{19} \\cdot \\tfrac{1}{10} - \\tfrac{240}{19} \\cdot \\tfrac{1}{16} = -\\tfrac{11}{95},\n     $$\n     $$\n     V_{\\text{epa}} = e_{1}^{\\top} S^{-1} \\Xi S^{-1} e_{1} = \\left(\\tfrac{128}{19}\\right)^{2} \\cdot \\tfrac{3}{10}\n     + 2 \\cdot \\left(\\tfrac{128}{19}\\right)\\left(-\\tfrac{240}{19}\\right) \\cdot \\tfrac{3}{32}\n     + \\left(-\\tfrac{240}{19}\\right)^{2} \\cdot \\tfrac{3}{70},\n     $$\n     其数值计算结果约等于 $V_{\\text{epa}} \\approx 4.498$。\n\n$\\hat{\\tau}$ 的均方误差（MSE）。结合偏差和方差，\n$$\n\\text{MSE}_{K}(\\hat{\\tau}) \\approx \\left( \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} \\right)^{2}\n+ V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} \\right),\n$$\n对于核函数 $K \\in \\{\\text{triangular}, \\text{Epanechnikov}\\}$。因为 $B_{\\text{tri}}$ 和 $B_{\\text{epa}}$ 都是负数，偏差的符号取决于 $\\Delta_{2}$，但 MSE 取决于偏差的平方和方差之和。通常，Epanechnikov 核函数具有较小的方差常数，而三角核函数具有较小幅度的偏差常数。\n\n算法设计。程序：\n- 对三角核函数和 Epanechnikov 核函数，使用闭式积分计算 $j \\in \\{0,1,2,3\\}$ 或 $\\{0,1,2\\}$ 的 $\\mu_{j}$ 和 $\\kappa_{j}$。\n- 构建 $S$、$R$ 和 $\\Xi$，对 $S$ 求逆，并评估 $B_{K}$ 和 $V_{K}$。\n- 对于每个测试用例 $(n, h, g_{+}, g_{-}, \\sigma_{+}, \\sigma_{-}, \\Delta_{2})$，计算\n  $$\n  \\text{MSE}_{K} = \\left( \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} \\right)^{2}\n  + V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}} + \\frac{\\sigma_{-}^{2}}{n h g_{-}} \\right),\n  $$\n  然后输出四舍五入到六位小数的 $\\text{MSE}_{\\text{tri}} - \\text{MSE}_{\\text{epa}}$。\n该方法将局部多项式 RD 理论与显式的核矩计算相结合，以量化在断点处非零曲率下由核函数选择引起的均方误差差异。",
            "answer": "```python\n# language: Python 3.12\n# libraries: numpy==1.23.5, scipy==1.11.4 (not used)\nimport numpy as np\n\ndef kernel_moments_triangular():\n    # Triangular kernel K(u) = 1 - u on [0,1]\n    # mu_j = ∫_0^1 (1 - u) u^j du = 1/((j+1)(j+2))\n    mu0 = 1.0 / (1 * 2)\n    mu1 = 1.0 / (2 * 3)\n    mu2 = 1.0 / (3 * 4)\n    mu3 = 1.0 / (4 * 5)\n    # kappa_j = ∫_0^1 (1 - u)^2 u^j du = 1/(j+1) - 2/(j+2) + 1/(j+3)\n    def kappa(j):\n        return 1.0 / (j + 1) - 2.0 / (j + 2) + 1.0 / (j + 3)\n    k0 = kappa(0)\n    k1 = kappa(1)\n    k2 = kappa(2)\n    return (mu0, mu1, mu2, mu3), (k0, k1, k2)\n\ndef kernel_moments_epanechnikov():\n    # Epanechnikov kernel K(u) = (3/4)(1 - u^2) on [0,1]\n    # mu_j = (3/4)[1/(j+1) - 1/(j+3)]\n    def mu(j):\n        return (3.0 / 4.0) * (1.0 / (j + 1) - 1.0 / (j + 3))\n    mu0 = mu(0)\n    mu1 = mu(1)\n    mu2 = mu(2)\n    mu3 = mu(3)\n    # kappa_j = ∫_0^1 K(u)^2 u^j du = (9/16)[1/(j+1) - 2/(j+3) + 1/(j+5)]\n    def kappa(j):\n        return (9.0 / 16.0) * (1.0 / (j + 1) - 2.0 / (j + 3) + 1.0 / (j + 5))\n    k0 = kappa(0)\n    k1 = kappa(1)\n    k2 = kappa(2)\n    return (mu0, mu1, mu2, mu3), (k0, k1, k2)\n\ndef bias_variance_constants(mus, kappas):\n    # Build S, R, Xi and compute B_K and V_K\n    mu0, mu1, mu2, mu3 = mus\n    k0, k1, k2 = kappas\n    S = np.array([[mu0, mu1],\n                  [mu1, mu2]], dtype=float)\n    R = np.array([mu2, mu3], dtype=float)\n    Xi = np.array([[k0, k1],\n                   [k1, k2]], dtype=float)\n    S_inv = np.linalg.inv(S)\n    e1 = np.array([1.0, 0.0], dtype=float)\n    B = float(e1 @ S_inv @ R)\n    V = float(e1 @ S_inv @ Xi @ S_inv @ e1)\n    return B, V\n\ndef mse_rd(n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2, kernel):\n    # Compute MSE for RD estimator using given kernel\n    if kernel == 'triangular':\n        mus, kappas = kernel_moments_triangular()\n    elif kernel == 'epanechnikov':\n        mus, kappas = kernel_moments_epanechnikov()\n    else:\n        raise ValueError(\"Unsupported kernel\")\n    B, V = bias_variance_constants(mus, kappas)\n    bias = 0.5 * (h ** 2) * delta2 * B\n    var_part = V * ((sigma_plus ** 2) / (n * h * g_plus) + (sigma_minus ** 2) / (n * h * g_minus))\n    mse = bias ** 2 + var_part\n    return mse\n\ndef solve():\n    # Define the test cases as specified in the problem statement:\n    # Each case: (n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2)\n    test_cases = [\n        (10000, 0.2, 1.0, 1.0, 1.0, 1.0, 1.5),      # Case 1\n        (10000, 0.05, 1.0, 1.0, 1.0, 1.0, 2.0),     # Case 2\n        (5000, 0.15, 0.8, 0.8, 1.0, 2.0, 1.0),      # Case 3\n        (8000, 0.12, 1.0, 0.5, 1.2, 1.2, -1.2),     # Case 4\n        (5000, 0.25, 0.9, 0.9, 1.0, 1.0, 5.0),      # Case 5\n        (3000, 0.5, 1.0, 1.0, 1.0, 1.0, 5.0),       # Case 6\n    ]\n\n    results = []\n    for n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2 in test_cases:\n        mse_tri = mse_rd(n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2, 'triangular')\n        mse_epa = mse_rd(n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2, 'epanechnikov')\n        diff = mse_tri - mse_epa\n        results.append(diff)\n\n    # Round to six decimals and print in the exact required format\n    formatted = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```"
        }
    ]
}