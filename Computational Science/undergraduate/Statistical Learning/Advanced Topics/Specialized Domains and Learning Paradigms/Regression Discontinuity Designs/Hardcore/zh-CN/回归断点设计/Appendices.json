{
    "hands_on_practices": [
        {
            "introduction": "断点回归设计的核心识别假设是，我们感兴趣的干预效应恰好发生在政策断点处。然而，在现实世界中，政策的实施可能与官方断点存在延迟或偏差。这个练习旨在通过一个假设情景，揭示当实际处理起始点与政策断点不一致时会发生什么，帮助您理解这种错位如何从根本上影响因果效应的识别，并导致估计结果产生严重偏差 。通过推导和计算这种偏差，您将更深刻地认识到验证 RDD 核心假设的重要性。",
            "id": "3168533",
            "problem": "考虑一个回归断点设计（RDD），其定义为在断点处观测结果的条件期望的右极限与左极限之差。设驱动变量为 $X \\in \\mathbb{R}$，其密度在断点 $c \\in \\mathbb{R}$ 附近连续可微。假设观测结果 $Y$ 由以下公式生成：\n$$\nY = f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon,\n$$\n其中 $f:\\mathbb{R}\\to\\mathbb{R}$ 在 $c$ 处连续，$\\tau \\in \\mathbb{R}$ 是一个恒定的处理效应，$\\Delta > 0$ 是政策断点 $c$ 与实际处理开始点 $c+\\Delta$ 之间的一个非零偏移，$\\varepsilon$ 是一个均值为零的擾動項，满足 $\\mathbb{E}[\\varepsilon \\mid X] = 0$，$\\mathbb{1}\\{\\cdot\\}$ 是指示函数。在断点 $c$ 处不存在操纵（即 $X$ 的密度在 $c$ 处是连续的）。\n\n你的任务是：\n\n1) 使用RDD估计量的定义\n$$\n\\theta_{\\text{RDD}}(c) = \\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x],\n$$\n在给定的数据生成过程下，推导 $\\theta_{\\text{RDD}}(c)$，并表达其相对于处理效应参数 $\\tau$ 的偏差。\n\n2) 考虑一个使用均匀核和对称带宽 $h>0$ 的局部常数RDD估计量。即，该估计量计算的是右侧窗口 $[c, c+h]$ 内 $Y$ 的样本均值与左侧窗口 $[c-h, c)$ 内 $Y$ 的样本均值之差。在大样本、 $X$ 在 $c$ 处的密度连续以及 $f(x)$ 在邻域 $[c-h, c+h]$ 内近似为常数（即，对于 $x \\in [c-h, c+h]$，有 $f(x) \\approx f(c)$）的假设下，推导该估计量的概率极限及其相对于 $\\tau$ 的偏差，并将其表示为 $\\tau$、$\\Delta$ 和 $h$ 的函数。\n\n3) 实现一个程序，为下面这组参数测试套件计算在任务2中推导出的偏差函数。对于每种情况，取 $c=0$（此选择不影响你推导的偏差表达式），并以实数（浮点数）形式返回偏差：\n- 情况A（理想路径）：$(\\tau, \\Delta, h) = (2.0, 0.3, 1.0)$。\n- 情况B（边界情况）：$(\\tau, \\Delta, h) = (1.0, 1.0, 1.0)$。\n- 情况C（右窗口内无处理对象的边缘情况）：$(\\tau, \\Delta, h) = (1.5, 1.2, 1.0)$。\n- 情况D（近乎对齐）：$(\\tau, \\Delta, h) = (0.75, 10^{-6}, 0.5)$。\n- 情况E（负处理效应）：$(\\tau, \\Delta, h) = (-1.5, 0.25, 0.5)$。\n\n本问题中没有物理单位。所有输出都必须表示为实数。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中 $r_i$ 是按上述顺序计算的第 $i$ 种情况的偏差。",
            "solution": "该问题要求分析一个处理分配与政策断点存在偏移的回归断点设计（RDD）。我们必须首先推导理论上的RDD估计量及其偏差，然后推导一个实用的局部常数估计量的概率极限和偏差，最后实现一个程序来计算该估计量在特定参数值下的偏差。\n\n**任务1：RDD估计量及其偏差的推导**\n\nRDD估计量定义为结果 $Y$ 的条件期望在断点 $c$ 处的跳跃：\n$$\n\\theta_{\\text{RDD}}(c) = \\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] - \\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x]\n$$\n数据生成过程由 $Y = f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon$ 给出。我们首先求在 $X=x$ 条件下 $Y$ 的条件期望。利用期望的线性和 $\\mathbb{E}[\\varepsilon \\mid X] = 0$ 的假设，我们有：\n$$\n\\mathbb{E}[Y \\mid X = x] = \\mathbb{E}[f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon \\mid X = x]\n$$\n$$\n\\mathbb{E}[Y \\mid X = x] = f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\} + \\mathbb{E}[\\varepsilon \\mid X = x]\n$$\n$$\n\\mathbb{E}[Y \\mid X = x] = f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}\n$$\n现在，我们计算在 $x=c$ 处的右极限和左极限。\n\n对于右极限，$x$ 从上方趋近于 $c$，因此 $x > c$。由于题目说明 $\\Delta > 0$，对于足够接近 $c$ 的 $x$ 值（具体来说，在区间 $(c, c+\\Delta)$ 内），条件 $x \\ge c+\\Delta$ 不成立。因此，指示函数 $\\mathbb{1}\\{x \\ge c + \\Delta\\}$ 为 $0$。考虑到 $f(x)$ 在 $c$ 处是连续的，该极限为：\n$$\n\\lim_{x \\downarrow c} \\mathbb{E}[Y \\mid X = x] = \\lim_{x \\downarrow c} (f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}) = f(c) + \\tau \\cdot 0 = f(c)\n$$\n对于左极限，$x$ 从下方趋近于 $c$，因此 $x  c$。由于 $\\Delta > 0$，总是有 $x  c  c+\\Delta$。因此，条件 $x \\ge c+\\Delta$ 不成立，指示函数 $\\mathbb{1}\\{x \\ge c + \\Delta\\}$ 为 $0$。该极限为：\n$$\n\\lim_{x \\uparrow c} \\mathbb{E}[Y \\mid X = x] = \\lim_{x \\uparrow c} (f(x) + \\tau \\,\\mathbb{1}\\{x \\ge c + \\Delta\\}) = f(c) + \\tau \\cdot 0 = f(c)\n$$\n将这些极限代入RDD估计量的定义中，得到：\n$$\n\\theta_{\\text{RDD}}(c) = f(c) - f(c) = 0\n$$\n该估计量相对于真实处理效应 $\\tau$ 的偏差定义为 $\\text{Bias} = \\theta_{\\text{RDD}}(c) - \\tau$。因此，偏差为：\n$$\n\\text{Bias} = 0 - \\tau = -\\tau\n$$\n这个结果是直观的：RDD估计量寻找在 $c$ 处的断点，但条件均值函数中的实际断点发生在 $c+\\Delta$ 处。由于 $f(x)$ 在 $c$ 处的连续性，没有检测到跳跃，导致估计值为 $0$，偏差为 $-\\tau$。\n\n**任务2：估计量的概率极限和偏差的推导**\n\n我们考虑一个局部常数RDD估计量，它是 $Y$ 在窗口 $[c, c+h]$ 和 $[c-h, c)$ 内样本均值之差。在大样本中（当 $n \\to \\infty$ 时），该估计量（记为 $\\hat{\\theta}_h$）的概率极限（plim）是这些窗口上真实条件期望的差：\n$$\n\\text{plim}_{n \\to \\infty} \\hat{\\theta}_h = \\mathbb{E}[Y \\mid c \\le X  c+h] - \\mathbb{E}[Y \\mid c-h \\le X  c]\n$$\n我们来分别分析每一项。对于左侧窗口 $[c-h, c)$，任何 $X$ 的值都满足 $X  c  c+\\Delta$ （因为 $\\Delta > 0$）。因此，对于此窗口中的所有 $X$，$\\mathbb{1}\\{X \\ge c + \\Delta\\} = 0$。\n$$\n\\mathbb{E}[Y \\mid c-h \\le X  c] = \\mathbb{E}[f(X) + \\tau \\cdot 0 + \\varepsilon \\mid c-h \\le X  c] = \\mathbb{E}[f(X) \\mid c-h \\le X  c]\n$$\n使用 $f(x) \\approx f(c)$ for $x \\in [c-h, c+h]$ 的假设，这可以简化为：\n$$\n\\mathbb{E}[Y \\mid c-h \\le X  c] \\approx f(c)\n$$\n对于右侧窗口 $[c, c+h]$，指示函数不总是零。\n$$\n\\mathbb{E}[Y \\mid c \\le X  c+h] = \\mathbb{E}[f(X) + \\tau \\,\\mathbb{1}\\{X \\ge c + \\Delta\\} + \\varepsilon \\mid c \\le X  c+h]\n$$\n使用近似 $f(x) \\approx f(c)$ 和全期望定律，这变为：\n$$\n\\mathbb{E}[Y \\mid c \\le X  c+h] \\approx f(c) + \\tau \\cdot \\mathbb{E}[\\mathbb{1}\\{X \\ge c + \\Delta\\} \\mid c \\le X  c+h]\n$$\n指示函数的期望是条件概率 $P(X \\ge c+\\Delta \\mid c \\le X  c+h)$。这可以写成：\n$$\nP(X \\ge c+\\Delta \\mid c \\le X  c+h) = \\frac{P((X \\ge c+\\Delta) \\cap (c \\le X  c+h))}{P(c \\le X  c+h)}\n$$\n分子是两个区间交集的概率，即 $P(\\max(c, c+\\Delta) \\le X  c+h) = P(c+\\Delta \\le X  c+h)$。只有当 $c+\\Delta  c+h$（即 $\\Delta  h$）时，这个区间才非空。如果 $\\Delta \\ge h$，则该概率为 $0$。\n设 $g(x)$ 为 $X$ 的概率密度函数（PDF）。由于 $g(x)$ 在 $c$ 处连续，对于一个小的带宽 $h$，我们可以近似认为密度在区间 $[c, c+h]$ 上是常数，即 $g(x) \\approx g(c)$。\n对于 $\\Delta  h$，分子中的概率为 $\\int_{c+\\Delta}^{c+h} g(x) dx \\approx g(c) \\cdot ((c+h)-(c+\\Delta)) = g(c) \\cdot (h-\\Delta)$。如果 $\\Delta \\ge h$，则为 $0$。这可以写成 $g(c) \\cdot \\max(0, h-\\Delta)$。\n分母中的概率为 $\\int_c^{c+h} g(x) dx \\approx g(c) \\cdot ((c+h)-c) = g(c) \\cdot h$。\n所以，条件概率为：\n$$\nP(X \\ge c+\\Delta \\mid c \\le X  c+h) \\approx \\frac{g(c) \\cdot \\max(0, h-\\Delta)}{g(c) \\cdot h} = \\frac{\\max(0, h-\\Delta)}{h}\n$$\n该估计量的概率极限则为：\n$$\n\\text{plim}_{n \\to \\infty} \\hat{\\theta}_h \\approx \\left( f(c) + \\tau \\frac{\\max(0, h-\\Delta)}{h} \\right) - f(c) = \\tau \\frac{\\max(0, h-\\Delta)}{h}\n$$\n渐近偏差是此概率极限与真实参数 $\\tau$ 之间的差值：\n$$\n\\text{Bias} = \\text{plim}_{n \\to \\infty} \\hat{\\theta}_h - \\tau \\approx \\tau \\frac{\\max(0, h-\\Delta)}{h} - \\tau = \\tau \\left( \\frac{\\max(0, h-\\Delta)}{h} - 1 \\right)\n$$\n我们可以简化这个表达式。注意 $\\max(0, h-\\Delta) = h - \\min(h, \\Delta)$。\n$$\n\\text{Bias} \\approx \\tau \\left( \\frac{h-\\min(h, \\Delta)}{h} - 1 \\right) = \\tau \\left( 1 - \\frac{\\min(h, \\Delta)}{h} - 1 \\right) = -\\tau \\frac{\\min(h, \\Delta)}{h}\n$$\n这是偏差的最终表达式。它取决于真实效应 $\\tau$、政策错位 $\\Delta$ 和估计量的带宽 $h$。这个公式对两种情况都有效：\n1. 如果 $\\Delta  h$，则 $\\min(h, \\Delta) = \\Delta$，所以 Bias $\\approx -\\tau \\frac{\\Delta}{h}$。\n2. 如果 $\\Delta \\ge h$，则 $\\min(h, \\Delta) = h$，所以 Bias $\\approx -\\tau \\frac{h}{h} = -\\tau$。\n\n**任务3：偏差的数值计算**\n\n我们现在将实现一个程序，使用推导出的公式 $\\text{Bias} = -\\tau \\frac{\\min(h, \\Delta)}{h}$ 来为提供的测试用例计算偏差，并设 $c=0$。$c$ 的值不影响偏差公式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of a local-constant RDD estimator with a misaligned cutoff.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (tau, Delta, h)\n    test_cases = [\n        # Case A (happy path)\n        (2.0, 0.3, 1.0),\n        # Case B (boundary)\n        (1.0, 1.0, 1.0),\n        # Case C (edge with no treated in right window)\n        (1.5, 1.2, 1.0),\n        # Case D (near alignment)\n        (0.75, 1e-6, 0.5),\n        # Case E (negative treatment effect)\n        (-1.5, 0.25, 0.5),\n    ]\n\n    def calculate_bias(tau: float, delta: float, h: float) -> float:\n        \"\"\"\n        Calculates the asymptotic bias of the local-constant RDD estimator.\n\n        The formula for the bias is derived as:\n        Bias = -tau * min(h, delta) / h\n\n        Args:\n            tau: The true treatment effect.\n            delta: The shift between the policy cutoff and the treatment start.\n            h: The symmetric bandwidth of the estimator.\n\n        Returns:\n            The calculated bias as a float.\n        \"\"\"\n        # The derivation shows that the bandwidth h must be positive.\n        # The problem statement specifies h > 0, so no division by zero error.\n        bias = -tau * min(h, delta) / h\n        return bias\n\n    results = []\n    for case in test_cases:\n        tau_val, delta_val, h_val = case\n        result = calculate_bias(tau_val, delta_val, h_val)\n        results.append(result)\n\n    # Format the final output string as a comma-separated list in brackets.\n    # The map(str, ...) converts each float result to its string representation.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "即使断点设置正确，我们数据的内在结构也可能给估计带来挑战。本练习将引导您探讨一种常见的实际问题：“边界效应”。当断点非常接近驱动变量的取值范围的端点（例如，最高分或最低分附近）时，断点一侧的数据会变得稀疏或被截断。这个动手实践将通过编程模拟，清晰地展示这种数据不对称性如何扭曲局部回归的估计，从而系统性地影响最终的 RDD 估计值，这对于在实际研究中避免错误结论至关重要 。",
            "id": "3168448",
            "problem": "您的任务是，通过算法评估处置变量在边界附近的截断如何影响局部线性回归断点 (RD) 估计量的偏差。首先从基本定义出发：在条件结果函数连续的前提下，断点处的 RD 估计目标是观测结果在该断点处极限条件均值的跳跃值。然后，设计并实现一个算法，该算法仅使用单边局部线性回归，从基本原理出发构建此估计量。\n\n数据生成设置：\n- 处置变量定义在一个紧区间上，具体为 $X \\in [0,100]$。您必须将该支撑域视为在边界处被截断，即不存在 $x \\lt 0$ 或 $x \\gt 100$ 的观测值。\n- 生成一个确定性的、密集的处置变量值网格：在 $[0,100]$ 上生成 $M$ 个等距点，其中 $M = 10001$，因此网格间距为 $0.01$，网格为 $\\{0,0.01,0.02,\\dots,100\\}$。\n- 定义潜在结果均值函数 $m_0(x)$ 和 $m_1(x)$ 如下\n  $$m_0(x) = 2 + 0.04\\,x + 0.002\\,x^2,\\quad m_1(x) = m_0(x) + \\tau,$$\n  其中恒定的处理效应为 $\\tau = 3$。\n- 对于给定的断点 $c$，将观测结果确定性地定义为\n  $$Y = m_0(X) + \\tau \\cdot \\mathbf{1}\\{X \\ge c\\},$$\n  且不添加任何随机噪声。\n\n需要实现的估计量：\n- 对于指定的带宽 $h \\gt 0$，在断点 $c$ 的两侧分别构建一个单边局部线性估计量，仅使用窗口 $|X - c| \\le h$ 内且位于 $c$ 的相应一侧的观测值。\n- 使用三角核函数\n  $$K(u) = \\max\\{0, 1 - |u|\\},\\quad u = \\frac{X - c}{h}.$$\n- 对于左侧，使用所有满足 $X \\lt c$ 和 $|X - c| \\le h$ 的点；对于右侧，使用所有满足 $X \\ge c$ 和 $|X - c| \\le h$ 的点。\n- 在每一侧，拟合一个形式为\n  $$Y = \\alpha + \\beta\\,(X - c),$$\n  的加权最小二乘线性回归，其中权重为 $K\\big((X - c)/h\\big)$。从左侧拟合中提取拟合截距 $\\widehat{\\alpha}_-$，从右侧拟合中提取拟合截距 $\\widehat{\\alpha}_+$。\n- 将 RD 估计值定义为\n  $$\\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_-.$$\n- 将给定 $(c,h)$ 的估计量偏差定义为\n  $$\\mathrm{bias}(c,h) = \\widehat{\\tau}_{\\mathrm{RD}} - \\tau.$$\n\n科学目标：\n- 通过算法展示，由于一侧的非对称支撑域以及 $m_0(x)$ 和 $m_1(x)$ 中的曲率，断点 $c$ 与边界（例如，接近 $100$）的接近程度如何增加绝对偏差。\n- 因为没有噪声且网格是密集的、确定性的，所以最终的偏差是 $(c,h)$ 和数据生成过程的确定性函数。\n\n测试套件：\n- 使用以下五个 $(c,h)$ 的测试用例：\n  1. $(c,h) = (50,10)$，一个支撑域内的内部案例，用作基线。\n  2. $(c,h) = (95,10)$，靠近上边界，以说明偏差的增加。\n  3. $(c,h) = (98,10)$，更靠近上边界，以强调边界效应。\n  4. $(c,h) = (95,5)$，与案例 2 的断点相同，但带宽更小，以评估因曲率暴露减少而带来的偏差降低。\n  5. $(c,h) = (5,10)$，靠近下边界，以说明另一侧的边界效应。\n\n答案规范：\n- 对每个测试用例，计算偏差 $\\mathrm{bias}(c,h)$，结果为一个实数。\n- 您的程序必须生成单行输出，其中包含五个结果，形式为逗号分隔的列表，并用方括号括起来，例如 $[\\text{结果}_1,\\text{结果}_2,\\text{结果}_3,\\text{结果}_4,\\text{结果}_5]$，每个数字都打印为十进制浮点值。不涉及单位。不使用角度。如果选择四舍五入，请对所有输出统一保留固定的小数位数。\n\n您的程序必须是一个完整、可运行的实现，它完全按照规定执行上述计算，并且只输出所需的单行内容。不允许用户输入，也不得使用外部文件。实现语言将另行指定，且必须严格遵守。",
            "solution": "该问题要求设计并实现一个算法，用以计算局部线性回归断点 (RD) 估计量的偏差。分析的重点是当断点接近数据支撑域的边界时，偏差如何受到影响。整个过程是确定性的，使用了一个密集的点网格和无噪声的结果函数。\n\n首先，我们根据提供的规范将问题设置形式化。处置变量（记为 $X$）在区间 $[0, 100]$ 上有一个紧支撑域。我们操作于一个确定性网格上，该网格包含此区间内的 $M = 10001$ 个等距点，从 $X=0$ 开始到 $X=100$ 结束。潜在结果均值函数由下式给出\n$$m_0(x) = 2 + 0.04\\,x + 0.002\\,x^2$$\n$$m_1(x) = m_0(x) + \\tau$$\n其中真实处理效应 $\\tau$ 是一个等于 $3$ 的常数。对于给定的断点 $c$，观测结果 $Y$ 被确定性地定义为：\n$$Y = m_0(X) + \\tau \\cdot \\mathbf{1}\\{X \\ge c\\}$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。RD 估计目标是条件期望函数在断点处的跳跃值，在这个无噪声的环境中，该值恰好为 $\\tau = m_1(c) - m_0(c) = 3$。\n\n任务是实现一个单边局部线性 RD 估计量。这涉及到在断点 $c$ 的两侧分别执行一次加权最小二乘 (WLS) 回归。对于给定的带宽 $h  0$，用于估计的数据被限制在窗口 $|X - c| \\le h$ 内。要拟合的回归模型是：\n$$Y = \\alpha + \\beta\\,(X - c)$$\n权重由三角核函数 $K(u) = \\max\\{0, 1 - |u|\\}$ 提供，其中 $u = (X - c)/h$。\n\n对于断点的右侧，我们对所有满足 $c \\le X_i \\le \\min(c+h, 100)$ 的数据点 $X_i$ 最小化其加权残差平方和：\n$$ \\min_{\\alpha_+, \\beta_+} \\sum_{i: c \\le X_i \\le c+h} K\\left(\\frac{X_i - c}{h}\\right) \\left[ Y_i - \\left(\\alpha_+ + \\beta_+ (X_i - c)\\right) \\right]^2 $$\n这个 WLS 问题的解给出了估计的截距 $\\widehat{\\alpha}_+$。类似地，对于左侧，我们使用满足 $\\max(0, c-h) \\le X_i  c$ 的数据点 $X_i$ 来找到截距 $\\widehat{\\alpha}_-$。请注意左侧的严格不等式 $X_i  c$ 以及与 $0$ 和 $100$ 处支撑域边界的相互作用。\n\n系数向量 $\\mathbf{b} = [\\alpha, \\beta]^T$ 的 WLS 估计量可以用矩阵形式表示为：\n$$ \\widehat{\\mathbf{b}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} (\\mathbf{X}^T \\mathbf{W} \\mathbf{y}) $$\n在这里，$\\mathbf{y}$ 是局部窗口内结果值的向量，$\\mathbf{X}$ 是设计矩阵，其第一列为 1，第二列为中心化后的变量 $(X_i - c)$，$\\mathbf{W}$ 是一个包含核权重 $K((X_i-c)/h)$ 的对角矩阵。估计的截距 $\\widehat{\\alpha}$ 是向量 $\\widehat{\\mathbf{b}}$ 的第一个元素。\n\n一旦我们从左侧和右侧回归中计算出截距 $\\widehat{\\alpha}_-$ 和 $\\widehat{\\alpha}_+$，处理效应的 RD 估计值为：\n$$ \\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_- $$\n然后，对于给定的 $(c,h)$ 对，该估计量的偏差计算如下：\n$$ \\mathrm{bias}(c,h) = \\widehat{\\tau}_{\\mathrm{RD}} - \\tau $$\n\n科学目标是展示当 $c$ 接近支撑域边界时，该偏差如何变化。局部线性估计量的偏差主要归因于线性模型与真实基础函数之间的不匹配，在本例中，该函数具有非零曲率（$m_0''(x) = 0.004$）。对于一个具有对称窗口 $[c-h, c+h]$ 的内部点 $c$，$\\widehat{\\alpha}_+$ 和 $\\widehat{\\alpha}_-$ 中的偏差倾向于具有相似的结构，并且在计算它们的差值时会部分抵消。然而，当 $c$ 接近边界时（例如，$c=95$ 且 $h=10$），其中一个估计窗口会被截断（例如，右侧窗口变为 $[95, 100]$ 而不是 $[95, 105]$）。这种截断会产生一个非对称的有效核，从而破坏了偏差项的对称性。抵消作用不再有效，导致 $\\widehat{\\tau}_{\\mathrm{RD}}$ 的整体偏差更大。减小带宽 $h$ 会缩短估计窗口，使得线性近似在更小的范围内更加准确，从而减小了由曲率引起的偏差的大小，即使在边界情况下也是如此。\n\n该算法通过迭代每个测试用例 $(c,h)$ 来进行：\n1. 在 $X_i \\in [0, 100]$ 的网格上生成全套结果值 $Y_i$。\n2. 对于给定的 $(c,h)$，对左侧执行 WLS 过程，使用来自 $[\\max(0, c-h), c)$ 的数据，以获得 $\\widehat{\\alpha}_-$。\n3. 对右侧执行 WLS 过程，使用来自 $[c, \\min(100, c+h)]$ 的数据，以获得 $\\widehat{\\alpha}_+$。\n4. 计算 $\\widehat{\\tau}_{\\mathrm{RD}} = \\widehat{\\alpha}_+ - \\widehat{\\alpha}_-$。\n5. 计算并存储偏差 $\\widehat{\\tau}_{\\mathrm{RD}} - 3$。\n对所有五个提供的测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias of a local linear Regression Discontinuity (RD) estimator\n    for several test cases, demonstrating boundary effects.\n    \"\"\"\n\n    # Define constants from the problem statement\n    M = 10001\n    TAU = 3.0\n\n    # Generate the deterministic grid for the running variable X\n    X_grid = np.linspace(0, 100, M)\n\n    def m0(x):\n        \"\"\"\n        Defines the potential outcome mean function for the control group.\n        \"\"\"\n        return 2.0 + 0.04 * x + 0.002 * x**2\n\n    def triangular_kernel(u):\n        \"\"\"\n        Defines the triangular kernel function.\n        \"\"\"\n        return np.maximum(0, 1 - np.abs(u))\n\n    def local_linear_fit(X_grid, Y_values, c, h, side):\n        \"\"\"\n        Performs a one-sided local linear regression and returns the\n        estimated intercept.\n\n        Args:\n            X_grid (np.ndarray): The grid of running variable values.\n            Y_values (np.ndarray): The corresponding observed outcome values.\n            c (float): The cutoff point.\n            h (float): The bandwidth.\n            side (str): 'left' or 'right' of the cutoff.\n\n        Returns:\n            float: The estimated intercept (alpha_hat).\n        \"\"\"\n        # Select data points based on side, bandwidth, and support [0, 100]\n        if side == 'right':\n            mask = (X_grid >= c)  (X_grid = c + h)\n        elif side == 'left':\n            mask = (X_grid  c)  (X_grid >= c - h)\n        else:\n            raise ValueError(\"Side must be 'left' or 'right'\")\n        \n        X_local = X_grid[mask]\n        Y_local = Y_values[mask]\n\n        if len(X_local)  2:\n            # Not enough points for a linear fit. This case is not expected\n            # with the problem's dense grid.\n            return np.nan\n\n        # Recenter the running variable\n        X_centered = X_local - c\n        \n        # Calculate kernel weights based on u = (X - c) / h\n        u = X_centered / h\n        weights = triangular_kernel(u)\n        \n        # Set up the weighted least squares problem: Y = alpha + beta * (X - c)\n        # Construct the design matrix.\n        design_matrix = np.vstack([np.ones(len(X_centered)), X_centered]).T\n\n        # Solve the normal equations: (X'WX)b = X'Wy\n        # This is more efficient than creating a diagonal weight matrix.\n        XT_W_X = design_matrix.T @ (weights[:, np.newaxis] * design_matrix)\n        XT_W_y = design_matrix.T @ (weights * Y_local)\n\n        try:\n            # Solve for the coefficient vector [alpha, beta]\n            coeffs = np.linalg.solve(XT_W_X, XT_W_y)\n            alpha_hat = coeffs[0]\n        except np.linalg.LinAlgError:\n            # This would occur if the matrix is singular.\n            alpha_hat = np.nan\n            \n        return alpha_hat\n\n    def compute_bias(c, h, X_grid, m0_func, tau_val):\n        \"\"\"\n        Computes the RD estimator bias for a given (c, h) pair.\n        \"\"\"\n        # Generate the observed outcome Y based on the cutoff c\n        Y_values = m0_func(X_grid) + tau_val * (X_grid >= c)\n        \n        # Get intercept estimates from the left and right sides\n        alpha_hat_plus = local_linear_fit(X_grid, Y_values, c, h, side='right')\n        alpha_hat_minus = local_linear_fit(X_grid, Y_values, c, h, side='left')\n        \n        # Compute the RD estimate of the treatment effect\n        tau_hat_rd = alpha_hat_plus - alpha_hat_minus\n        \n        # Compute the final bias\n        bias = tau_hat_rd - tau_val\n        return bias\n\n    # Define the test suite of (c, h) pairs\n    test_cases = [\n        (50, 10),\n        (95, 10),\n        (98, 10),\n        (95, 5),\n        (5, 10),\n    ]\n\n    results = []\n    for c, h in test_cases:\n        bias_result = compute_bias(c, h, X_grid, m0, TAU)\n        results.append(bias_result)\n\n    # Format the output as a comma-separated list in brackets,\n    # with consistent decimal formatting.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了 RDD 的基本假设和常见陷阱之后，我们可以进一步优化估计器的性能。这个更高级的练习将带您深入局部多项式回归的核心部件——核函数（kernel function）。不同的核函数对断点附近的数据点赋予不同的权重，从而影响估计的偏差和方差。通过精确推导和量化比较两种常用核函数（三角核与 Epanechnikov 核）的均方误差（Mean Squared Error, MSE），您将学会如何在偏差与方差之间进行权衡，为选择最优的 RDD 估计策略打下坚实的理论基础 。",
            "id": "3168499",
            "problem": "要求您在一个单边局部多项式回归断点设计 (RD) 中，对两个核函数进行基于原理的比较，并推导和实现该比较。考虑一个在断点 $c$ 处的清晰回归断点设计 (RD)，其中处理效应是条件期望函数 $m_{+}(x)$ 和 $m_{-}(x)$ 在 $c$ 右侧和左侧的跳跃 $\\tau = m_{+}(c) - m_{-}(c)$。假设在 $c$ 的两侧均使用单边局部线性回归（1阶多项式），并采用共同的带宽 $h$。令右侧的标准化驱动变量坐标为 $u_{+} = (x - c)/h$，左侧为 $u_{-} = (c - x)/h$，这样单边核函数的支集就在 $[0,1]$ 上。\n\n从以下基本出发点开始：\n- 加权最小二乘法 (WLS) 将局部多项式估计量定义为通过最小化加權殘差平方和得到的截距项和斜率项的取值，其中权重由应用于与断点的标准化距离的核函数 $K(u)$ 给出。\n- $m_{+}(x)$ 和 $m_{-}(x)$ 的光滑性意味着在 $x=c$ 附近的二阶泰勒展开是成立的，从而产生决定在边界处使用局部线性回归时渐近偏差的主阶项。\n- 在局部多项式回归的统计学习中，标准抽样、独立性和正则性条件下，大数定律和中心极限定理意味着由核函数加权的单项式构建的设计矩阵收敛于其总体矩矩阵。这些矩是通过在单边支集上对 $u$ 的幂次积分 $K(u)$ 和 $K(u)^2$ 来定义的。\n\n您的任务是：\n1. 为任何支集在 $[0,1]$ 上的核函数 $K(u)$ 定义单边核矩矩阵：\n   - 令 $p(u) = [1, u]^{\\top}$。\n   - 定义矩矩阵 $S = \\int_{0}^{1} K(u) p(u) p(u)^{\\top} \\, du$，其元素为 $S_{11} = \\int_{0}^{1} K(u) \\, du$，$S_{12}=S_{21} = \\int_{0}^{1} K(u) u \\, du$，$S_{22} = \\int_{0}^{1} K(u) u^{2} \\, du$。\n   - 定义偏差加载向量 $R = \\int_{0}^{1} K(u) u^{2} p(u) \\, du$，即 $R = \\left[\\int_{0}^{1} K(u) u^{2} \\, du, \\int_{0}^{1} K(u) u^{3} \\, du \\right]^{\\top}$。\n   - 定义方差加载矩阵 $\\Xi = \\int_{0}^{1} K(u)^{2} p(u) p(u)^{\\top} \\, du$，其元素为 $\\Xi_{11} = \\int_{0}^{1} K(u)^{2} \\, du$，$\\Xi_{12}=\\Xi_{21} = \\int_{0}^{1} K(u)^{2} u \\, du$，$\\Xi_{22} = \\int_{0}^{1} K(u)^{2} u^{2} \\, du$。\n2. 使用以上定义，用 $S$、$R$ 和 $\\Xi$ 推导出 $c$ 两侧单边局部线性截距的主阶渐近偏差和方差，然后推导出 RD 估计量 $\\hat{\\tau}$（作为左右截距之差）的相应量。明确展示断点处的曲率是如何通过二阶导数 $m_{+}''(c)$ 和 $m_{-}''(c)$ 引入的。\n3. 将推导特化到两种核函数：\n   - 三角核：$K(u) = 1 - u$，对于 $u \\in [0,1]$，否则 $K(u)=0$。\n   - Epanechnikov 核：$K(u) = \\frac{3}{4}(1 - u^{2})$，对于 $u \\in [0,1]$，否则 $K(u)=0$。\n   精确计算所需的核积分，组装 $S$、$R$ 和 $\\Xi$，并计算当两侧使用相同核函数和带宽时适用于 RD 估计量的标量偏差和方差常数。\n4. 在断点处曲率非零的情况下，量化和比较两种核函数下 $\\hat{\\tau}$ 的均方误差 (MSE)。该 MSE 通过曲率差异 $\\Delta_{2} = m_{+}''(c) - m_{-}''(c) \\neq 0$、共同带宽 $h$、样本量 $n$、驱动变量在 $c$ 处的左右密度 $g_{+}(c)$ 和 $g_{-}(c)$，以及左右噪声方差 $\\sigma_{+}^{2}$ 和 $\\sigma_{-}^{2}$ 来表示。您的程序必须实现推導出的公式来计算 $MSE_{\\text{tri}}$ 和 $MSE_{\\text{epa}}$，然后为下面列出的每个测试用例输出 $MSE_{\\text{tri}} - MSE_{\\text{epa}}$。\n5. 使用以下测试套件。对于每个测试用例，参数以元组 $(n, h, g_{+}(c), g_{-}(c), \\sigma_{+}, \\sigma_{-}, \\Delta_{2})$ 的形式提供。不涉及物理单位。不涉及角度。最终答案为实数。\n   - 案例 1 (平衡，中等带宽，中等曲率)：$(10000, 0.2, 1.0, 1.0, 1.0, 1.0, 1.5)$。\n   - 案例 2 (平衡，小带宽，较强曲率)：$(10000, 0.05, 1.0, 1.0, 1.0, 1.0, 2.0)$。\n   - 案例 3 (密度平衡，异方差两侧)：$(5000, 0.15, 0.8, 0.8, 1.0, 2.0, 1.0)$。\n   - 案例 4 (密度不平衡，噪声相等，负曲率差异)：$(8000, 0.12, 1.0, 0.5, 1.2, 1.2, -1.2)$。\n   - 案例 5 (密度平衡，大曲率，中等带宽)：$(5000, 0.25, 0.9, 0.9, 1.0, 1.0, 5.0)$。\n   - 案例 6 (密度平衡，极大带宽，大曲率)：$(3000, 0.5, 1.0, 1.0, 1.0, 1.0, 5.0)$。\n6. 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个结果必须是四舍五入到六位小数的浮点数，并按顺序对应上述测试用例。例如，输出可能看起来像 $[0.001234,0.000567,-0.000890,0.000012,0.000345,-0.001234]$。",
            "solution": "我们从清晰回归断点设计 (RD) 中边界处局部多项式回归的基本定义开始，介绍其推导过程和算法。\n\n设置和定义。一个在断点 $c$ 处的清晰 RD 具有条件期望 $m_{+}(x)$（对于 $x \\ge c$）和 $m_{-}(x)$（对于 $x  c$）。感兴趣的参数是 $\\tau = m_{+}(c) - m_{-}(c)$。我们使用两个单边局部线性回归的差来估计 $\\tau$。对于右侧，定义 $u = (x - c)/h$ 并使用单边核函数 $K(u)$ 将其限制在 $u \\in [0,1]$；对于左侧，定义 $u = (c - x)/h$，同样有 $u \\in [0,1]$。在每一侧，我们拟合一个局部线性回归（1阶多项式）。令 $p(u) = [1, u]^{\\top}$。记局部设计矩矩阵为\n$$\nS = \\int_{0}^{1} K(u) p(u) p(u)^{\\top} \\, du\n= \\begin{bmatrix}\n\\mu_{0}  \\mu_{1} \\\\\n\\mu_{1}  \\mu_{2}\n\\end{bmatrix},\n$$\n其中 $\\mu_{j} = \\int_{0}^{1} K(u) u^{j} \\, du$。偏差加载向量\n$$\nR = \\int_{0}^{1} K(u) u^{2} p(u) \\, du\n= \\begin{bmatrix}\n\\mu_{2} \\\\ \\mu_{3}\n\\end{bmatrix},\n$$\n其中 $\\mu_{3} = \\int_{0}^{1} K(u) u^{3} \\, du$。方差加载矩阵\n$$\n\\Xi = \\int_{0}^{1} K(u)^{2} p(u) p(u)^{\\top} \\, du\n= \\begin{bmatrix}\n\\kappa_{0}  \\kappa_{1} \\\\\n\\kappa_{1}  \\kappa_{2}\n\\end{bmatrix},\n$$\n其中 $\\kappa_{j} = \\int_{0}^{1} K(u)^{2} u^{j} \\, du$。这些积分源于将大数定律和中心极限定理应用于加权回归量和误差。\n\n单边局部线性截距及其渐近性质。令给定一侧上 $m(c)$ 的单边局部线性估计量为通过权重为 $K(u)$、回归量为 $p(u)$ 的加权最小二乘法求解得到的截距 $\\hat{\\alpha}$。在光滑性条件下，我们使用二阶泰勒展开\n$$\nm(c + h u) = m(c) + m'(c) h u + \\tfrac{1}{2} m''(c) h^{2} u^{2} + o(h^{2})\n$$\n对于右侧，类似地\n$$\nm(c - h u) = m(c) - m'(c) h u + \\tfrac{1}{2} m''(c) h^{2} u^{2} + o(h^{2})\n$$\n对于左侧。单边局部线性截距的主阶渐近偏差是由曲率项 $\\tfrac{1}{2} m''(c) h^{2}$ 与设计的相互作用驱动的。标准的局部多项式理论表明\n$$\n\\text{Bias}(\\hat{\\alpha}) = \\tfrac{1}{2} h^{2} m''(c) \\cdot B_{K} + o(h^{2}), \\quad \\text{其中} \\quad B_{K} = e_{1}^{\\top} S^{-1} R,\n$$\n其中 $e_{1} = [1, 0]^{\\top}$。主阶渐近方差是\n$$\n\\text{Var}(\\hat{\\alpha}) = \\frac{\\sigma^{2}}{n h g(c)} \\cdot V_{K} + o\\!\\left(\\frac{1}{n h}\\right), \\quad \\text{其中} \\quad V_{K} = e_{1}^{\\top} S^{-1} \\Xi S^{-1} e_{1}.\n$$\n此处，$n$ 是样本量，$h$ 是带宽，$g(c)$ 是驱动变量在 $c$ 处相应一侧的密度，$\\sigma^{2}$ 是该侧的误差方差。这些表达式源于 WLS 正规方程组、向 $S$、$R$ 和 $\\Xi$ 的矩收敛，以及局部多项式回归中的标准方差传播。\n\n作为单边截距之差的 RD 估计量。RD 估计量是左右截距之差，$\\hat{\\tau} = \\hat{\\alpha}_{+} - \\hat{\\alpha}_{-}$。其主阶渐近偏差是两个单边偏差之差：\n$$\n\\text{Bias}(\\hat{\\tau}) = \\tfrac{1}{2} h^{2} \\left( m_{+}''(c) - m_{-}''(c) \\right) B_{K} + o(h^{2})\n= \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} + o(h^{2}),\n$$\n其中 $\\Delta_{2} = m_{+}''(c) - m_{-}''(c)$ 是断点处的曲率差异。两个单边截距估计量使用不相交的样本（在 $c$ 的两侧），因此主阶方差相加：\n$$\n\\text{Var}(\\hat{\\tau}) = \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} V_{K} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} V_{K} + o\\!\\left(\\frac{1}{n h}\\right)\n= V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} \\right) + o\\!\\left(\\frac{1}{n h}\\right).\n$$\n因此，$\\hat{\\tau}$ 的主阶均方误差 (MSE) 是\n$$\n\\text{MSE}(\\hat{\\tau}) \\approx \\left( \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} \\right)^{2} + V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} \\right).\n$$\n\n特定于核函数的积分。我们现在为两种核函数计算 $S$、$R$ 和 $\\Xi$。\n\n1. 三角核 $K(u) = 1 - u$ on $[0,1]$:\n   - 矩 $\\mu_{j} = \\int_{0}^{1} (1 - u) u^{j} \\, du = \\frac{1}{(j+1)(j+2)}$，因此\n     $$\n     \\mu_{0} = \\tfrac{1}{2}, \\quad \\mu_{1} = \\tfrac{1}{6}, \\quad \\mu_{2} = \\tfrac{1}{12}, \\quad \\mu_{3} = \\tfrac{1}{20}.\n     $$\n   - 核函数平方的矩 $\\kappa_{j} = \\int_{0}^{1} (1 - u)^{2} u^{j} \\, du\n     = \\frac{1}{j+1} - \\frac{2}{j+2} + \\frac{1}{j+3}$，因此\n     $$\n     \\kappa_{0} = \\tfrac{1}{3}, \\quad \\kappa_{1} = \\tfrac{1}{12}, \\quad \\kappa_{2} = \\tfrac{1}{30}.\n     $$\n   - 组装 $S = \\begin{bmatrix} \\tfrac{1}{2}  \\tfrac{1}{6} \\\\ \\tfrac{1}{6}  \\tfrac{1}{12} \\end{bmatrix}$，$R = \\begin{bmatrix} \\tfrac{1}{12} \\\\ \\tfrac{1}{20} \\end{bmatrix}$，以及 $\\Xi = \\begin{bmatrix} \\tfrac{1}{3}  \\tfrac{1}{12} \\\\ \\tfrac{1}{12}  \\tfrac{1}{30} \\end{bmatrix}$。\n   - 对 $S$ 求逆并计算常数：\n     $$\n     S^{-1} = \\begin{bmatrix} 6  -12 \\\\ -12  36 \\end{bmatrix}, \\quad\n     B_{\\text{tri}} = e_{1}^{\\top} S^{-1} R = 6 \\cdot \\tfrac{1}{12} - 12 \\cdot \\tfrac{1}{20} = -\\tfrac{1}{10},\n     $$\n     $$\n     V_{\\text{tri}} = e_{1}^{\\top} S^{-1} \\Xi S^{-1} e_{1} = \\tfrac{24}{5}.\n     $$\n\n2. Epanechnikov 核 $K(u) = \\frac{3}{4}(1 - u^{2})$ on $[0,1]$:\n   - 矩 $\\mu_{j} = \\frac{3}{4}\\left( \\frac{1}{j+1} - \\frac{1}{j+3} \\right)$，因此\n     $$\n     \\mu_{0} = \\tfrac{1}{2}, \\quad \\mu_{1} = \\tfrac{3}{16}, \\quad \\mu_{2} = \\tfrac{1}{10}, \\quad \\mu_{3} = \\tfrac{1}{16}.\n     $$\n   - 核函数平方的矩使用 $K(u)^{2} = \\frac{9}{16}(1 - 2u^{2} + u^{4})$，所以\n     $$\n     \\kappa_{j} = \\frac{9}{16}\\left( \\frac{1}{j+1} - \\frac{2}{j+3} + \\frac{1}{j+5} \\right),\n     $$\n     因此\n     $$\n     \\kappa_{0} = \\tfrac{3}{10}, \\quad \\kappa_{1} = \\tfrac{3}{32}, \\quad \\kappa_{2} = \\tfrac{3}{70}.\n     $$\n   - 组装 $S = \\begin{bmatrix} \\tfrac{1}{2}  \\tfrac{3}{16} \\\\ \\tfrac{3}{16}  \\tfrac{1}{10} \\end{bmatrix}$，$R = \\begin{bmatrix} \\tfrac{1}{10} \\\\ \\tfrac{1}{16} \\end{bmatrix}$，以及 $\\Xi = \\begin{bmatrix} \\tfrac{3}{10}  \\tfrac{3}{32} \\\\ \\tfrac{3}{32}  \\tfrac{3}{70} \\end{bmatrix}$。\n   - 对 $S$ 求逆并计算常数：\n     $$\n     S^{-1} = \\begin{bmatrix} \\tfrac{128}{19}  -\\tfrac{240}{19} \\\\ -\\tfrac{240}{19}  \\tfrac{640}{19} \\end{bmatrix}, \\quad\n     B_{\\text{epa}} = e_{1}^{\\top} S^{-1} R = \\tfrac{128}{19} \\cdot \\tfrac{1}{10} - \\tfrac{240}{19} \\cdot \\tfrac{1}{16} = -\\tfrac{11}{95},\n     $$\n     $$\n     V_{\\text{epa}} = e_{1}^{\\top} S^{-1} \\Xi S^{-1} e_{1} = \\left(\\tfrac{128}{19}\\right)^{2} \\cdot \\tfrac{3}{10}\n     + 2 \\cdot \\left(\\tfrac{128}{19}\\right)\\left(-\\tfrac{240}{19}\\right) \\cdot \\tfrac{3}{32}\n     + \\left(-\\tfrac{240}{19}\\right)^{2} \\cdot \\tfrac{3}{70},\n     $$\n     其数值计算结果约等于 $V_{\\text{epa}} \\approx 4.498$。\n\n$\\hat{\\tau}$ 的均方误差 (MSE)。结合偏差和方差，\n$$\n\\text{MSE}_{K}(\\hat{\\tau}) \\approx \\left( \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} \\right)^{2}\n+ V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}(c)} + \\frac{\\sigma_{-}^{2}}{n h g_{-}(c)} \\right),\n$$\n对于核函数 $K \\in \\{\\text{triangular}, \\text{Epanechnikov}\\}$。由于 $B_{\\text{tri}}$ 和 $B_{\\text{epa}}$ 均为负数，偏差的符号取决于 $\\Delta_{2}$，但 MSE 取决于偏差的平方和方差之和。通常，Epanechnikov 核具有较小的方差常数，而三角核具有绝对值更小的偏差常数。\n\n算法设计。该程序：\n- 使用闭式积分计算三角核和 Epanechnikov 核的 $\\mu_{j}$（对于 $j \\in \\{0,1,2,3\\}$）和 $\\kappa_{j}$（对于 $j \\in \\{0,1,2\\}$）。\n- 构建 $S$、$R$ 和 $\\Xi$，对 $S$ 求逆，并计算 $B_{K}$ 和 $V_{K}$。\n- 对于每个测试用例 $(n, h, g_{+}, g_{-}, \\sigma_{+}, \\sigma_{-}, \\Delta_{2})$，计算\n  $$\n  \\text{MSE}_{K} = \\left( \\tfrac{1}{2} h^{2} \\Delta_{2} B_{K} \\right)^{2}\n  + V_{K} \\left( \\frac{\\sigma_{+}^{2}}{n h g_{+}} + \\frac{\\sigma_{-}^{2}}{n h g_{-}} \\right),\n  $$\n  然后输出 $\\text{MSE}_{\\text{tri}} - \\text{MSE}_{\\text{epa}}$，并四舍五入到六位小数。\n该方法将局部多项式 RD 理论与显式的核矩计算相结合，以量化在断点处曲率非零时因核函数选择而导致的均方误差差异。",
            "answer": "```python\n# language: Python 3.12\n# libraries: numpy==1.23.5, scipy==1.11.4 (not used)\nimport numpy as np\n\ndef kernel_moments_triangular():\n    # Triangular kernel K(u) = 1 - u on [0,1]\n    # mu_j = ∫_0^1 (1 - u) u^j du = 1/((j+1)(j+2))\n    mu0 = 1.0 / (1 * 2)\n    mu1 = 1.0 / (2 * 3)\n    mu2 = 1.0 / (3 * 4)\n    mu3 = 1.0 / (4 * 5)\n    # kappa_j = ∫_0^1 (1 - u)^2 u^j du = 1/(j+1) - 2/(j+2) + 1/(j+3)\n    def kappa(j):\n        return 1.0 / (j + 1) - 2.0 / (j + 2) + 1.0 / (j + 3)\n    k0 = kappa(0)\n    k1 = kappa(1)\n    k2 = kappa(2)\n    return (mu0, mu1, mu2, mu3), (k0, k1, k2)\n\ndef kernel_moments_epanechnikov():\n    # Epanechnikov kernel K(u) = (3/4)(1 - u^2) on [0,1]\n    # mu_j = (3/4)[1/(j+1) - 1/(j+3)]\n    def mu(j):\n        return (3.0 / 4.0) * (1.0 / (j + 1) - 1.0 / (j + 3))\n    mu0 = mu(0)\n    mu1 = mu(1)\n    mu2 = mu(2)\n    mu3 = mu(3)\n    # kappa_j = ∫_0^1 K(u)^2 u^j du = (9/16)[1/(j+1) - 2/(j+3) + 1/(j+5)]\n    def kappa(j):\n        return (9.0 / 16.0) * (1.0 / (j + 1) - 2.0 / (j + 3) + 1.0 / (j + 5))\n    k0 = kappa(0)\n    k1 = kappa(1)\n    k2 = kappa(2)\n    return (mu0, mu1, mu2, mu3), (k0, k1, k2)\n\ndef bias_variance_constants(mus, kappas):\n    # Build S, R, Xi and compute B_K and V_K\n    mu0, mu1, mu2, mu3 = mus\n    k0, k1, k2 = kappas\n    S = np.array([[mu0, mu1],\n                  [mu1, mu2]], dtype=float)\n    R = np.array([mu2, mu3], dtype=float)\n    Xi = np.array([[k0, k1],\n                   [k1, k2]], dtype=float)\n    S_inv = np.linalg.inv(S)\n    e1 = np.array([1.0, 0.0], dtype=float)\n    B = float(e1 @ S_inv @ R)\n    V = float(e1 @ S_inv @ Xi @ S_inv @ e1)\n    return B, V\n\ndef mse_rd(n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2, kernel):\n    # Compute MSE for RD estimator using given kernel\n    if kernel == 'triangular':\n        mus, kappas = kernel_moments_triangular()\n    elif kernel == 'epanechnikov':\n        mus, kappas = kernel_moments_epanechnikov()\n    else:\n        raise ValueError(\"Unsupported kernel\")\n    B, V = bias_variance_constants(mus, kappas)\n    bias = 0.5 * (h ** 2) * delta2 * B\n    var_part = V * ((sigma_plus ** 2) / (n * h * g_plus) + (sigma_minus ** 2) / (n * h * g_minus))\n    mse = bias ** 2 + var_part\n    return mse\n\ndef solve():\n    # Define the test cases as specified in the problem statement:\n    # Each case: (n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2)\n    test_cases = [\n        (10000, 0.2, 1.0, 1.0, 1.0, 1.0, 1.5),      # Case 1\n        (10000, 0.05, 1.0, 1.0, 1.0, 1.0, 2.0),     # Case 2\n        (5000, 0.15, 0.8, 0.8, 1.0, 2.0, 1.0),      # Case 3\n        (8000, 0.12, 1.0, 0.5, 1.2, 1.2, -1.2),     # Case 4\n        (5000, 0.25, 0.9, 0.9, 1.0, 1.0, 5.0),      # Case 5\n        (3000, 0.5, 1.0, 1.0, 1.0, 1.0, 5.0),       # Case 6\n    ]\n\n    results = []\n    for n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2 in test_cases:\n        mse_tri = mse_rd(n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2, 'triangular')\n        mse_epa = mse_rd(n, h, g_plus, g_minus, sigma_plus, sigma_minus, delta2, 'epanechnikov')\n        diff = mse_tri - mse_epa\n        results.append(diff)\n\n    # Round to six decimals and print in the exact required format\n    formatted = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```"
        }
    ]
}