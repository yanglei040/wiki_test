## Applications and Interdisciplinary Connections

Having established the theoretical foundations and mechanics of Regression Discontinuity Designs (RDD) in the preceding chapter, we now turn to their practical implementation and broader scientific relevance. The true value of a statistical method is revealed not in its abstract properties but in its capacity to answer substantive questions across a range of disciplines. This chapter explores the remarkable versatility of RDD, demonstrating its application in fields from public policy and ecology to medicine and the behavioral sciences.

Our exploration is structured to move from core applications to more advanced extensions and, finally, to situate RDD within the wider landscape of [causal inference](@entry_id:146069). We will see how the fundamental logic of RDD—the identification of causal effects through a discontinuity in treatment assignment—provides a powerful and transparent framework for research in settings where randomized controlled trials are infeasible, unethical, or otherwise impractical. Throughout, we will emphasize the critical importance of careful design, rigorous validation, and a deep understanding of the context in which the discontinuity arises.

### Core Applications: Policy Evaluation and Scientific Discovery

The canonical application of RDD is in the evaluation of policies, programs, and interventions that utilize an eligibility threshold. The sharp, often arbitrary nature of these cutoffs provides a compelling "as-if" randomized experiment for individuals or units located in a narrow band around the threshold.

In **Public Policy and Economics**, RDD is a cornerstone of modern empirical analysis. Many social programs, tax laws, and regulations are defined by explicit thresholds based on income, age, firm size, or geographic location. For instance, consider a regulation that applies only to firms with more than 50 employees. A [regression discontinuity design](@entry_id:634606), using the number of employees as the running variable, can estimate the causal effect of this policy on outcomes like firm profitability, investment, or employment growth. By comparing firms just above and just below the 50-employee threshold, one can isolate the policy's impact, assuming that firms on either side are, on average, similar in all other relevant aspects. The [local linear regression](@entry_id:635822) framework provides a direct method to estimate the magnitude of the discontinuity in the outcome at the threshold, which corresponds to the local average [treatment effect](@entry_id:636010) of the policy .

The field of **Healthcare and Medicine** offers numerous applications, as clinical decisions are frequently guided by thresholds on diagnostic tests or risk scores. For example, a hospital's protocol might mandate admission to an intensive care unit (ICU) if a patient's clinical risk score exceeds a specific value. RDD can be used to estimate the causal effect of ICU admission on patient outcomes like mortality or length of stay for these marginal patients. An analyst would model mortality as a function of the risk score, allowing for a discontinuity at the triage cutoff. The credibility of such a design is enhanced by performing robustness checks, such as including additional baseline risk-adjustment covariates in the [local regression](@entry_id:637970) models. If the estimated [treatment effect](@entry_id:636010) remains stable after including such covariates, it strengthens the causal interpretation of the findings .

RDD is not limited to human-centric systems. In **Ecology and Environmental Science**, researchers can exploit sharp spatial or temporal boundaries, both natural and anthropogenic, as sources of identification.
-   A **spatial RDD** can be used to quantify ecological "[edge effects](@entry_id:183162)." Consider a sharp boundary between a forest and a pasture. The signed distance to this boundary serves as a continuous running variable. The abrupt change in habitat type at the boundary constitutes the treatment. By measuring a microclimatic variable, such as understory air temperature, at various distances on both sides of the boundary, one can estimate the causal effect of the forest edge on [microclimate](@entry_id:195467). Such spatial applications often require advanced statistical techniques, such as using spatial [heteroskedasticity](@entry_id:136378) and autocorrelation consistent (HAC) standard errors to account for spatial dependence in the data, and "donut" regressions that exclude observations very close to the boundary to mitigate concerns about measurement error in the running variable .
-   A **temporal RDD** can leverage policy changes that occur at a specific point in time. For example, to study the impact of anthropogenic noise on wildlife, researchers could analyze acoustic data recorded before and after the implementation of a nightly heavy-truck curfew on a road adjacent to a nature reserve. Using time (e.g., minutes from the 22:00 curfew) as the running variable, a discontinuity in measures of sound pressure level or in the detection probability of animal vocalizations at the curfew time would provide an estimate of the causal effect of the reduction in truck traffic .

Finally, in the **Behavioral and Social Sciences**, RDD is a creative tool for understanding how labels, awards, and status changes affect human behavior. Many online platforms and educational systems incorporate "gamification" elements with arbitrary thresholds. For instance, a [citizen science](@entry_id:183342) platform might confer "expert" status on a user once they submit 500 verified observations. By treating the number of observations as the running variable and the 500-observation mark as the cutoff, researchers can estimate the causal impact of receiving this expert status. Outcomes of interest could include subsequent changes in user behavior, such as the geographic breadth of their sampling effort (e.g., mean pairwise distance between observations) or their degree of taxonomic specialization. The estimated jumps in these behavioral metrics at the cutoff reveal how the status change causally influences user engagement .

### Advanced Designs and Extensions

While the sharp RDD is powerful, its core logic can be extended to accommodate more complex real-world scenarios. Two important extensions are the Difference-in-Discontinuities design and the Fuzzy Regression Discontinuity design.

The **Difference-in-Discontinuities (DiD-RDD)** design is a hybrid method that combines the logic of RDD with that of a [difference-in-differences](@entry_id:636293) approach. It is particularly useful in two common situations. First, it can account for a pre-existing discontinuity at the cutoff that is unrelated to the treatment of interest. For example, a policy might be introduced at a threshold where the outcome variable already exhibited a jump. A simple post-policy RDD would conflate the [treatment effect](@entry_id:636010) with this baseline jump. A DiD-RDD resolves this by collecting data from both pre-policy and post-policy periods. It estimates the jump at the cutoff in both periods separately and then calculates the difference between the post-policy jump and the pre-policy jump. This second difference isolates the causal effect attributable solely to the new policy . Second, the DiD-RDD framework can be adapted to situations where the policy cutoff itself changes over time. By estimating the discontinuity relative to the time-varying cutoff in each period and then differencing these estimates, one can assess how the [treatment effect](@entry_id:636010) itself evolves over time .

The **Fuzzy Regression Discontinuity (FRD)** design is employed when crossing the cutoff does not deterministically assign treatment but merely alters the probability of receiving it. This is common in programs with voluntary enrollment. For example, a Payments for Ecosystem Services (PES) program might make land parcels eligible for a conservation contract if a "conservation priority score" exceeds a cutoff. However, since enrollment is voluntary, not all eligible parcels will participate. In this scenario, the probability of treatment jumps at the cutoff from zero (for ineligible parcels) to some value between zero and one (for eligible parcels).

The FRD is best understood as a local [instrumental variable](@entry_id:137851) (IV) analysis. The instrument is the assignment to treatment based on the cutoff rule (i.e., an indicator for being above the threshold). The first stage of the analysis estimates the jump in the probability of treatment uptake at the cutoff. The second stage estimates the jump in the outcome at the cutoff (the "reduced form"). The causal effect is the ratio of the reduced-form jump to the first-stage jump. This approach relies on the standard IV assumptions, translated into the RDD context: instrument relevance (a non-zero jump in treatment probability), the [exclusion restriction](@entry_id:142409) (continuity of [potential outcomes](@entry_id:753644) at the cutoff), and monotonicity (no "defiers"—individuals who would take up treatment only if deemed ineligible) .

### Implementation and Methodological Considerations

The credibility of any RDD study rests on careful implementation and rigorous validation. Two areas deserve special attention: the choice of smoothing parameters and the integrity of the running variable.

A crucial step in implementing a local polynomial RDD is the selection of the **bandwidth** ($h$) and the **polynomial order** ($p$). The bandwidth defines the local neighborhood around the cutoff used for estimation, creating a fundamental trade-off: a wider bandwidth uses more data, reducing variance, but risks introducing bias if the underlying regression function is not truly linear. The polynomial order governs the flexibility of the local approximation. These parameters should not be chosen arbitrarily. Data-driven procedures, such as those based on [leave-one-out cross-validation](@entry_id:633953), provide a principled way to select the $(h, p)$ pair that minimizes an estimate of the [mean squared error](@entry_id:276542). Such methods formalize the bias-variance trade-off and improve the objectivity and [reproducibility](@entry_id:151299) of the results .

Perhaps the most critical aspect of an RDD is the **integrity of the running variable** and the cutoff. The entire design hinges on the assumption that the cutoff rule is sharp, known, and not precisely manipulable by the units of observation. A common pitfall can occur if the running variable itself is a composite score constructed by the researcher. For example, if a researcher uses machine learning to create a weighted score $S = w^{\top} Z - c$ from a set of features $Z$ and then uses $S$ as the running variable, any misspecification can invalidate the design. If the learned weights $w$ are not aligned with the true data-generating process, or if the intercept $c$ shifts the effective cutoff away from the true discontinuity, the mixing of treated and control units around the researcher's chosen cutoff will smooth out the very jump the study aims to detect. This can lead to severe bias, often attenuating the estimated effect towards zero. This highlights a crucial lesson: RDD is a design for analyzing known, pre-existing assignment rules, not a tool for discovering them post-hoc .

Throughout the analysis, a suite of **validity checks** should be standard practice. These include testing for a discontinuity in the density of the running variable at the cutoff (e.g., with a McCrary test) to check for non-random sorting of units, and verifying that pre-determined baseline covariates are continuous across the cutoff. Passing these tests provides empirical support for the core assumption that units are "as-if" randomized in a local neighborhood of the cutoff .

### Interdisciplinary Connections and the Broader Context of Causal Inference

Regression discontinuity is not an isolated technique but a member of a broader family of [quasi-experimental methods](@entry_id:636714). Understanding its relationship to other methods clarifies its unique strengths and the specific questions it is poised to answer.

A fundamental distinction in quantitative analysis is that between **prediction and causation**. A predictive algorithm, such as an ARIMA model for time-series forecasting, is optimized to find statistical patterns and correlations that minimize forecast error. It may perform this task exceptionally well without providing any insight into the underlying causal mechanisms. A [causal inference](@entry_id:146069) algorithm like RDD, by contrast, is not primarily designed for prediction. Its goal is to use a credible source of exogenous variation—the treatment assignment rule—to isolate and estimate the impact of an intervention. These two types of algorithms serve different purposes and should not be conflated; a model that predicts well is not necessarily a valid model for causal effects, and vice versa .

RDD shares a deep conceptual connection with other quasi-experimental designs, particularly **Mendelian Randomization (MR)**. In MR, the random allocation of genetic variants at conception is used as an [instrumental variable](@entry_id:137851) for a modifiable exposure (e.g., cholesterol levels) to estimate its causal effect on a disease outcome. The "as-if" randomization in MR is analogous to the "as-if" [randomization](@entry_id:198186) at the cutoff in RDD. The core assumptions of the two methods are parallel:
-   **Relevance:** In MR, the genetic variant must be associated with the exposure. In fuzzy RDD, being above the cutoff must be associated with receiving treatment.
-   **Exclusion Restriction:** In MR, this is the "no [horizontal pleiotropy](@entry_id:269508)" assumption—the gene must affect the outcome only through the exposure of interest. In RDD, this is the assumption of continuity of [potential outcomes](@entry_id:753644) at the cutoff—there are no other reasons for the outcome to jump at the threshold besides the treatment.
-   **Independence:** In MR, the gene must be independent of confounders of the exposure-outcome relationship. In RDD, this is guaranteed locally by the continuity assumption.

Both methods are vulnerable to analogous failures. Population stratification can violate the independence assumption in MR, while strategic sorting of individuals around the cutoff can violate the local [randomization](@entry_id:198186) assumption in RDD. In both cases, checking for balance in baseline covariates is a key diagnostic .

Finally, it is useful to contrast RDD with methods based on **conditional ignorability**, such as matching or [inverse probability](@entry_id:196307) weighting. These methods aim to achieve causal inference by statistically controlling for a rich set of observed pre-treatment covariates, under the assumption that no unmeasured confounders remain. A well-designed matching study, for instance, can provide credible causal estimates but relies on this strong, often untestable ignorability assumption . RDD is often considered more credible because its primary assumption—continuity of [potential outcomes](@entry_id:753644) at the cutoff—is more localized and can be partially assessed through placebo tests and covariate balance checks. These [quasi-experimental methods](@entry_id:636714) are essential because in many domains, such as [human genetics](@entry_id:261875) or large-scale environmental science, direct randomization of the "treatment" or exposure is ethically or practically impossible .

### Chapter Summary

The Regression Discontinuity Design is a powerful and widely applicable tool for [causal inference](@entry_id:146069). Its conceptual transparency, which stems from isolating a causal effect at a known and arbitrary cutoff, has led to its adoption across the natural and social sciences. We have seen its utility in evaluating the impact of economic policies, medical triage rules, environmental boundaries, and behavioral nudges.

Furthermore, the basic RDD framework can be flexibly extended to handle dynamic settings and imperfect compliance through Difference-in-Discontinuities and Fuzzy RDD variants. As a key member of the modern toolkit for causal inference, RDD shares deep connections with other methods like Mendelian Randomization and stands as a compelling alternative to designs based on less testable assumptions. The successful application of RDD, however, is not automatic. It demands a rigorous approach to design, a clear-eyed assessment of its core assumptions, and a commitment to transparent diagnostic testing. When these conditions are met, RDD provides one of the most credible paths to estimating causal effects from observational data.