{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the log-rank test, it is essential to look beyond the standard software output and grasp its combinatorial foundation. At its core, the test evaluates whether the observed pattern of events is unusual under the null hypothesis that group labels are meaningless. This practice guides you through an exact permutation test on a small dataset, revealing the fundamental mechanism of comparing observed outcomes to a null distribution generated by all possible group assignments .",
            "id": "1962134",
            "problem": "In a small clinical trial, a new treatment (Group A) is being compared against a control (Group B). A total of six subjects are enrolled in the study, with three randomly assigned to Group A and three to Group B. The study follows subjects over time, recording either the time until a specific adverse event occurs or the time of their last follow-up if they leave the study without the event (censoring).\n\nThe combined data for all six subjects are as follows, where time is measured in months and the status is either 'Event' or 'Censored':\n- Subject 1: (3, Event)\n- Subject 2: (5, Event)\n- Subject 3: (6, Censored)\n- Subject 4: (8, Event)\n- Subject 5: (10, Censored)\n- Subject 6: (12, Event)\n\nThe observed assignment of subjects to the groups was:\n- Group A (Treatment): Subjects with outcomes (5, Event), (6, Censored), and (12, Event).\n- Group B (Control): Subjects with outcomes (3, Event), (8, Event), and (10, Censored).\n\nTo test the null hypothesis that there is no difference in the event-time distributions between the two groups, you will perform an exact log-rank test. This involves calculating the numerator of the log-rank statistic, $U = \\sum_{j} (O_j - E_j)$, for every possible assignment of subjects to the groups.\n\nCalculate the two-sided p-value for the observed data by constructing the exact permutation distribution of the statistic $U$. Round your final answer to four significant figures.",
            "solution": "We order the six subjects by time: $t_{1}=3$ (Event), $t_{2}=5$ (Event), $t_{3}=6$ (Censored), $t_{4}=8$ (Event), $t_{5}=10$ (Censored), $t_{6}=12$ (Event). The log-rank numerator at each event time $j$ is $O_{j}-E_{j}$, where $O_{j}$ is the number of Group A events and $E_{j}=d_{j}\\,n_{Aj}/n_{j}$ with $d_{j}$ events at time $j$, $n_{j}$ at risk just before $j$, and $n_{Aj}$ Group A at risk just before $j$. There are $d_{j}=1$ at times $t_{1},t_{2},t_{4},t_{6}$; censorings at $t_{3},t_{5}$ contribute zero.\n\nLet $a_{i}\\in\\{0,1\\}$ indicate if subject $i$ is in Group A, with $\\sum_{i=1}^{6}a_{i}=3$. Then:\n- At $t_{1}=3$: $n_{1}=6$, $n_{A1}=3$, $E_{1}=1\\cdot(3/6)=\\frac{1}{2}$, $O_{1}=a_{1}$, so contribution is $a_{1}-\\frac{1}{2}$.\n- At $t_{2}=5$: $n_{2}=5$, $n_{A2}=3-a_{1}$, $E_{2}=\\frac{3-a_{1}}{5}$, $O_{2}=a_{2}$, so contribution is $a_{2}-\\frac{3-a_{1}}{5}$.\n- At $t_{3}=6$ (censoring): no contribution.\n- At $t_{4}=8$: $n_{3}=3$, $n_{A3}=3-a_{1}-a_{2}-a_{3}$, $E_{3}=\\frac{3-a_{1}-a_{2}-a_{3}}{3}$, $O_{3}=a_{4}$, so contribution is $a_{4}-\\frac{3-a_{1}-a_{2}-a_{3}}{3}$.\n- At $t_{5}=10$ (censoring): no contribution.\n- At $t_{6}=12$: $n_{4}=1$, $E_{4}=a_{6}$, $O_{4}=a_{6}$, so contribution is $0$.\n\nTherefore,\n$$\nU=(a_{1}-\\tfrac{1}{2})+\\Bigl(a_{2}-\\tfrac{3-a_{1}}{5}\\Bigr)+\\Bigl(a_{4}-\\tfrac{3-a_{1}-a_{2}-a_{3}}{3}\\Bigr),\n$$\nwhich simplifies to\n$$\nU=\\tfrac{23}{15}a_{1}+\\tfrac{4}{3}a_{2}+\\tfrac{1}{3}a_{3}+a_{4}-\\tfrac{21}{10}.\n$$\n\nThe observed assignment is Group A: $(5,\\text{Event})$, $(6,\\text{Censored})$, $(12,\\text{Event})$, i.e., $(a_{1},a_{2},a_{3},a_{4},a_{5},a_{6})=(0,1,1,0,0,1)$. Thus\n$$\nU_{\\text{obs}}=\\tfrac{4}{3}\\cdot 1+\\tfrac{1}{3}\\cdot 1-\\tfrac{21}{10}=\\tfrac{5}{3}-\\tfrac{21}{10}=-\\tfrac{13}{30}.\n$$\n\nTo form the exact permutation distribution, enumerate all $\\binom{6}{3}=20$ assignments. Because $U$ depends only on $(a_{1},a_{2},a_{3},a_{4})$ and $\\sum_{i=1}^{6}a_{i}=3$, the allowable counts among the first four are $k\\in\\{1,2,3\\}$. For each subset $S\\subset\\{1,2,3,4\\}$ with $1\\leq |S|\\leq 3$, $U(S)=\\sum_{i\\in S}w_{i}-c$ with weights $w_{1}=\\tfrac{23}{15}$, $w_{2}=\\tfrac{4}{3}$, $w_{3}=\\tfrac{1}{3}$, $w_{4}=1$, and $c=\\tfrac{21}{10}$. The values (with multiplicities across all 20 assignments) are:\n- For $|S|=1$: $-\\tfrac{17}{30}$, $-\\tfrac{23}{30}$, $-\\tfrac{53}{30}$, $-\\tfrac{33}{30}$ (each count $1$).\n- For $|S|=2$: $\\tfrac{23}{30}$, $-\\tfrac{7}{30}$, $\\tfrac{13}{30}$, $-\\tfrac{13}{30}$, $\\tfrac{7}{30}$, $-\\tfrac{23}{30}$ (each count $2$).\n- For $|S|=3$: $\\tfrac{33}{30}$, $\\tfrac{53}{30}$, $\\tfrac{23}{30}$, $\\tfrac{17}{30}$ (each count $1$).\n\nCollecting, the full multiset of $U$ has counts: $\\tfrac{53}{30}(1)$, $\\tfrac{33}{30}(1)$, $\\tfrac{23}{30}(3)$, $\\tfrac{17}{30}(1)$, $\\tfrac{13}{30}(2)$, $\\tfrac{7}{30}(2)$, $-\\tfrac{7}{30}(2)$, $-\\tfrac{13}{30}(2)$, $-\\tfrac{17}{30}(1)$, $-\\tfrac{23}{30}(3)$, $-\\tfrac{33}{30}(1)$, $-\\tfrac{53}{30}(1)$.\n\nWith $|U_{\\text{obs}}|=\\tfrac{13}{30}$, the two-sided p-value is the proportion with $|U|\\geq \\tfrac{13}{30}$. The only values with $|U|\\tfrac{13}{30}$ are $\\pm\\tfrac{7}{30}$ (total count $4$). Hence the p-value is\n$$\np=\\frac{20-4}{20}=\\frac{16}{20}=0.8.\n$$\n\nRounded to four significant figures, the p-value is $0.8000$.",
            "answer": "$$\\boxed{0.8000}$$"
        },
        {
            "introduction": "A key feature that distinguishes survival analysis is the presence of censored data, where an event has not been observed for a subject. Simply ignoring these subjects or treating their censoring times as event times can lead to severely biased conclusions. This problem pits the log-rank test against a naive Wilcoxon rank-sum test to demonstrate how mishandling censored data can distort results, underscoring the critical importance of the log-rank methodology .",
            "id": "1962146",
            "problem": "In a clinical trial, a new therapeutic agent (Group A) is compared against a standard treatment (Group B) for its effect on patient survival time. Sixteen patients are enrolled and randomly assigned to one of the two groups, with eight patients in each group. The primary endpoint is the time (in months) until a specific event occurs. Some patients are lost to follow-up or do not experience the event by the end of the observation period; these are recorded as censored observations.\n\nThe observed times for the two groups are as follows, where a `+` symbol indicates a censored observation:\n- **Group A (New Agent):** 21, 26, 31, 35+, 35+, 35+, 35+, 35+\n- **Group B (Standard Treatment):** 5, 10, 15, 18, 22, 27, 32, 38\n\nA data analyst proposes two different methods to test the null hypothesis that there is no difference in the survival distributions between the two groups.\n1. A log-rank test, which is the standard method for analyzing such time-to-event data with censoring.\n2. A \"naive\" Wilcoxon rank-sum test (also known as the Mann-Whitney U test) applied *only* to the subset of patients who experienced the event (i.e., non-censored observations).\n\nYour task is to compute the test statistic for each approach. Let $Z$ be the test statistic for the log-rank test, calculated as $Z = (\\sum(O_A - E_A)) / \\sqrt{\\sum V}$, where $O_A$ are the observed events in Group A, $E_A$ are the expected events, and $V$ is the variance. Let $U$ be the test statistic for the Wilcoxon rank-sum test, defined as the minimum of the two possible $U$ statistics calculated for each group.\n\nCalculate the value of the pair $(Z, U)$. For the final answer, express $Z$ rounded to three significant figures, and $U$ as an exact integer.",
            "solution": "The problem asks for two statistical test values based on the provided survival data for two groups, A and B.\n\n**Part 1: Naive Wilcoxon Rank-Sum Test (Mann-Whitney U Test)**\n\nThis test is performed only on the subjects who experienced the event. We first extract the event times for each group.\n- Group A event times: {21, 26, 31}. The sample size of event-experiencing subjects is $n_A = 3$.\n- Group B event times: {5, 10, 15, 18, 22, 27, 32, 38}. The sample size of event-experiencing subjects is $n_B = 8$.\n\nNext, we pool all event times and rank them from smallest to largest. The total number of observations in this naive analysis is $N = n_A + n_B = 3 + 8 = 11$.\n\n| Time | Group | Rank |\n|------|-------|------|\n| 5    | B     | 1    |\n| 10   | B     | 2    |\n| 15   | B     | 3    |\n| 18   | B     | 4    |\n| 21   | A     | 5    |\n| 22   | B     | 6    |\n| 26   | A     | 7    |\n| 27   | B     | 8    |\n| 31   | A     | 9    |\n| 32   | B     | 10   |\n| 38   | B     | 11   |\n\nNow, we calculate the sum of ranks for each group.\n- Sum of ranks for Group A ($R_A$): $5 + 7 + 9 = 21$.\n- Sum of ranks for Group B ($R_B$): $1 + 2 + 3 + 4 + 6 + 8 + 10 + 11 = 45$.\nAs a check, the total sum of ranks should be $\\frac{N(N+1)}{2} = \\frac{11(12)}{2} = 66$. And indeed, $R_A + R_B = 21 + 45 = 66$.\n\nThe Mann-Whitney U statistics for each group are calculated as:\n$U_A = R_A - \\frac{n_A(n_A+1)}{2} = 21 - \\frac{3(3+1)}{2} = 21 - 6 = 15$.\n$U_B = R_B - \\frac{n_B(n_B+1)}{2} = 45 - \\frac{8(8+1)}{2} = 45 - 36 = 9$.\nAs a check, $U_A + U_B = 15 + 9 = 24$, which should equal $n_A n_B = 3 \\times 8 = 24$. The calculation is correct.\n\nThe test statistic $U$ is the minimum of $U_A$ and $U_B$.\n$U = \\min(15, 9) = 9$.\n\n**Part 2: Log-Rank Test**\n\nThis test uses all data, including censored observations, to compare the survival distributions. The null hypothesis is that the survival functions of the two groups are identical. We calculate the observed events ($O_A$), expected events ($E_A$), and variance ($V$) at each distinct event time.\n\nThe data is:\n- Group A: {21, 26, 31, 35+, 35+, 35+, 35+, 35+} ($N_A=8$)\n- Group B: {5, 10, 15, 18, 22, 27, 32, 38} ($N_B=8$)\nThe distinct event times are: 5, 10, 15, 18, 21, 22, 26, 27, 31, 32, 38.\n\nWe construct a table to keep track of the calculations at each event time $t_j$.\n- $n_{Aj}$: number of subjects at risk in Group A just before $t_j$.\n- $n_{Bj}$: number of subjects at risk in Group B just before $t_j$.\n- $N_j = n_{Aj} + n_{Bj}$: total number at risk.\n- $d_j$: total number of events at $t_j$. For this dataset, $d_j=1$ for all event times.\n- $d_{Aj}$: number of events in Group A at $t_j$.\n- $O_{Aj} = d_{Aj}$: observed events in Group A.\n- $E_{Aj} = d_j \\frac{n_{Aj}}{N_j}$: expected events in Group A under $H_0$.\n- $V_j = \\frac{n_{Aj} n_{Bj} d_j (N_j - d_j)}{N_j^2 (N_j-1)}$: variance of $O_{Aj}$ under $H_0$.\n\n| $t_j$ | $n_{Aj}$ | $n_{Bj}$ | $N_j$ | $d_j$ | $d_{Aj}$ | $O_{Aj}-E_{Aj}$ | $V_j$ |\n|-------|----------|----------|-------|-------|----------|-----------------------|-----------------------|\n| 5     | 8        | 8        | 16    | 1     | 0        | $0 - 8/16 = -0.5000$  | $\\frac{8 \\cdot 8 \\cdot 1 \\cdot 15}{16^2 \\cdot 15} = 0.2500$ |\n| 10    | 8        | 7        | 15    | 1     | 0        | $0 - 8/15 \\approx -0.5333$ | $\\frac{8 \\cdot 7 \\cdot 1 \\cdot 14}{15^2 \\cdot 14} \\approx 0.2489$ |\n| 15    | 8        | 6        | 14    | 1     | 0        | $0 - 8/14 \\approx -0.5714$ | $\\frac{8 \\cdot 6 \\cdot 1 \\cdot 13}{14^2 \\cdot 13} \\approx 0.2449$ |\n| 18    | 8        | 5        | 13    | 1     | 0        | $0 - 8/13 \\approx -0.6154$ | $\\frac{8 \\cdot 5 \\cdot 1 \\cdot 12}{13^2 \\cdot 12} \\approx 0.2367$ |\n| 21    | 8        | 4        | 12    | 1     | 1        | $1 - 8/12 \\approx 0.3333$  | $\\frac{8 \\cdot 4 \\cdot 1 \\cdot 11}{12^2 \\cdot 11} \\approx 0.2222$ |\n| 22    | 7        | 4        | 11    | 1     | 0        | $0 - 7/11 \\approx -0.6364$ | $\\frac{7 \\cdot 4 \\cdot 1 \\cdot 10}{11^2 \\cdot 10} \\approx 0.2314$ |\n| 26    | 7        | 3        | 10    | 1     | 1        | $1 - 7/10 = 0.3000$   | $\\frac{7 \\cdot 3 \\cdot 1 \\cdot 9}{10^2 \\cdot 9} = 0.2100$ |\n| 27    | 6        | 3        | 9     | 1     | 0        | $0 - 6/9 \\approx -0.6667$ | $\\frac{6 \\cdot 3 \\cdot 1 \\cdot 8}{9^2 \\cdot 8} \\approx 0.2222$ |\n| 31    | 6        | 2        | 8     | 1     | 1        | $1 - 6/8 = 0.2500$    | $\\frac{6 \\cdot 2 \\cdot 1 \\cdot 7}{8^2 \\cdot 7} = 0.1875$ |\n| 32    | 5        | 2        | 7     | 1     | 0        | $0 - 5/7 \\approx -0.7143$ | $\\frac{5 \\cdot 2 \\cdot 1 \\cdot 6}{7^2 \\cdot 6} \\approx 0.2041$ |\n| 38    | 0        | 1        | 1     | 1     | 0        | $0 - 0/1 = 0$         | $0$ (since $N_j-1=0$) |\nNote: At $t=35$, 5 subjects from Group A are censored, so for the event at $t=38$, $n_{Aj}=0$.\n\nNow we sum the columns for $O_{Aj}-E_{Aj}$ and $V_j$.\n$\\sum(O_{Aj}-E_{Aj}) \\approx -0.5000 - 0.5333 - 0.5714 - 0.6154 + 0.3333 - 0.6364 + 0.3000 - 0.6667 + 0.2500 - 0.7143 + 0 = -3.3542$\n\n$\\sum V_j \\approx 0.2500 + 0.2489 + 0.2449 + 0.2367 + 0.2222 + 0.2314 + 0.2100 + 0.2222 + 0.1875 + 0.2041 + 0 = 2.2579$\n\nThe log-rank test statistic $Z$ is:\n$Z = \\frac{\\sum(O_{Aj}-E_{Aj})}{\\sqrt{\\sum V_j}} = \\frac{-3.3542}{\\sqrt{2.2579}} \\approx \\frac{-3.3542}{1.50263} \\approx -2.2322$\n\nRounding to three significant figures, $Z = -2.23$.\n\nThe pair of values $(Z, U)$ is $(-2.23, 9)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} -2.23  9 \\end{pmatrix}}$$"
        },
        {
            "introduction": "The standard log-rank test is optimally powerful under the proportional hazards assumption, where the hazard ratio between groups remains constant over time. This coding challenge invites you to investigate the test's sensitivity when this crucial assumption does not hold, using a scenario with an oscillating hazard ratio . By implementing both the standard test and its weighted variants, you will gain practical insight into diagnosing and addressing non-proportional hazards, a common challenge in real-world data analysis.",
            "id": "3185128",
            "problem": "You are asked to write a complete, runnable program that investigates how the classical log-rank test responds to deviations from proportional hazards. Consider two groups, indexed by $g \\in \\{1,2\\}$, starting follow-up at time $0$. Group $1$ has constant hazard $\\lambda_1(t) = \\lambda_0$ and group $2$ deviates from proportional hazards according to $\\lambda_2(t) = \\lambda_0 \\{1 + \\sin(\\omega t)\\}$, where $\\omega \\ge 0$ is a frequency parameter. Assume independent right censoring with censoring times drawn from a continuous distribution and independence between survival and censoring. All times are dimensionless.\n\nFrom fundamental definitions, recall the cumulative hazard $\\Lambda_g(t) = \\int_0^t \\lambda_g(u) \\, du$ for group $g$, and the survival function $S_g(t) = \\exp\\{-\\Lambda_g(t)\\}$. Generate survival times in each group by inversion using a uniform random variable $U \\sim \\text{Uniform}(0,1)$ and solving for $T_g$ such that $\\Lambda_g(T_g) = -\\log U$; apply independent censoring by observing $X_g = \\min(T_g, C_g)$ with event indicator $\\Delta_g = \\mathbf{1}\\{T_g \\le C_g\\}$.\n\nConstruct the log-rank test statistic from first principles. At each distinct event time $t_j$ across both groups, compute the risk set sizes and observed events in each group using the counting-process definition of at-risk and event indicators. Under the null hypothesis of equal hazards, compute the expected number of events in group $2$ at $t_j$ as the number of total events at $t_j$ multiplied by the proportion of the risk set belonging to group $2$. Form the standardized test statistic with an appropriate variance derived from the variability of the group allocation of events at $t_j$ under the null. Use the central limit theorem to obtain a two-sided $p$-value for the log-rank test.\n\nAlso construct two weighted tests using the same numerator and variance but multiplied by weights $w(t_j)$ defined through the pooled Kaplan–Meier estimator. Specifically, use Fleming–Harrington weights of the form $w(t_j) = S_{\\text{pooled}}(t_j^-)^p \\{1 - S_{\\text{pooled}}(t_j^-)\\}^q$, where $S_{\\text{pooled}}(t_j^-)$ is the pooled Kaplan–Meier survival immediately before $t_j$ and $(p,q)$ are nonnegative integers. Compare:\n- the standard log-rank test, which corresponds to $(p,q) = (0,0)$,\n- an early-weighted test with $(p,q) = (1,0)$,\n- a late-weighted test with $(p,q) = (0,1)$.\n\nParameter specification for the simulation:\n- Baseline hazard: $\\lambda_0 = 0.2$.\n- Sample sizes: $n_1 = 200$ and $n_2 = 200$.\n- Censoring distribution: $C_g \\sim \\text{Uniform}(0,12)$ independently for each subject.\n- Random seed: use a fixed seed equal to $42$ to ensure deterministic results.\n\nTest suite (frequency values):\n- $\\omega \\in \\{0.0, 0.5, 2.0, 6.0\\}$.\n\nFor each value of $\\omega$, simulate one dataset for group $2$ and reuse the same fixed dataset for group $1$ across all $\\omega$ to isolate the effect of the frequency modulation. Compute three two-sided $p$-values: the standard log-rank $p$-value, the Fleming–Harrington $(p,q) = (1,0)$ $p$-value, and the Fleming–Harrington $(p,q) = (0,1)$ $p$-value.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$[p_{\\text{LR}}(\\omega_1), p_{\\text{FH}(1,0)}(\\omega_1), p_{\\text{FH}(0,1)}(\\omega_1), p_{\\text{LR}}(\\omega_2), p_{\\text{FH}(1,0)}(\\omega_2), p_{\\text{FH}(0,1)}(\\omega_2), \\ldots]$,\nwhere each $p$-value is a float. Use the four-test suite values for $\\omega$ given above.",
            "solution": "The user wants to solve a problem in statistical learning, specifically to implement the log-rank test and its weighted variants (Fleming-Harrington tests) to assess their behavior when the proportional hazards assumption is violated.\n\n### Step 1: Extract Givens\n- **Groups**: Two groups, $g \\in \\{1,2\\}$.\n- **Hazard Functions**:\n    - Group 1: $\\lambda_1(t) = \\lambda_0$ (constant hazard).\n    - Group 2: $\\lambda_2(t) = \\lambda_0 \\{1 + \\sin(\\omega t)\\}$ (time-varying hazard).\n- **Definitions**:\n    - Cumulative Hazard: $\\Lambda_g(t) = \\int_0^t \\lambda_g(u) \\, du$.\n    - Survival Function: $S_g(t) = \\exp\\{-\\Lambda_g(t)\\}$.\n- **Data Generation**:\n    - Survival times $T_g$ are generated by inversion: $\\Lambda_g(T_g) = -\\log U$, where $U \\sim \\text{Uniform}(0,1)$.\n    - Censoring times $C_g$ are from an independent continuous distribution.\n    - Observed data: $X_g = \\min(T_g, C_g)$ and $\\Delta_g = \\mathbf{1}\\{T_g \\le C_g\\}$.\n- **Test Statistics**:\n    - Log-rank test statistic to be constructed from first principles.\n    - Null hypothesis $H_0$: equal hazards.\n    - Expected events in group $2$ at time $t_j$: $E_{2j} = d_j \\frac{Y_{2j}}{Y_j}$, where $d_j$ is total events and $Y_{2j}/Y_j$ is the proportion at risk in group $2$.\n    - Standardized test statistic uses a variance derived from the hypergeometric distribution of event allocation at $t_j$.\n    - A two-sided $p$-value is obtained using the central limit theorem (normal approximation).\n- **Weighted Tests**:\n    - Fleming-Harrington family with weights $w(t_j) = S_{\\text{pooled}}(t_j^-)^p \\{1 - S_{\\text{pooled}}(t_j^-)\\}^q$.\n    - $S_{\\text{pooled}}(t_j^-)$ is the pooled Kaplan-Meier survival estimate just before time $t_j$.\n    - Comparison of three tests:\n        1.  Standard log-rank: $(p,q) = (0,0)$.\n        2.  Early-weighted (Peto-Peto): $(p,q) = (1,0)$.\n        3.  Late-weighted: $(p,q) = (0,1)$.\n- **Simulation Parameters**:\n    - Baseline hazard: $\\lambda_0 = 0.2$.\n    - Sample sizes: $n_1 = 200$, $n_2 = 200$.\n    - Censoring distribution: $C_g \\sim \\text{Uniform}(0,12)$.\n    - Random seed: $42$.\n- **Test Suite**:\n    - Frequency values for group 2: $\\omega \\in \\{0.0, 0.5, 2.0, 6.0\\}$.\n- **Simulation Protocol**:\n    - Group 1 data is generated once and reused.\n    - For each $\\omega$, a new dataset for group 2 is simulated.\n- **Output Format**:\n    - A single line `[...]` containing a comma-separated list of $12$ floating-point $p$-values, ordered as $[p_{\\text{LR}}(\\omega_1), p_{\\text{FH}(1,0)}(\\omega_1), p_{\\text{FH}(0,1)}(\\omega_1), \\ldots]$ for the four values of $\\omega$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is firmly rooted in the theory of survival analysis, a major topic in biostatistics and statistical learning. The log-rank test, Kaplan-Meier estimation, hazard functions, and Fleming-Harrington tests are all standard, well-established concepts. The use of a sinusoidal term to model deviation from proportional hazards is a common method for studying the properties of such tests.\n2.  **Well-Posed**: The problem is specified with mathematical precision. All parameters, distributions, and procedures are clearly defined. The task is to perform a specific simulation and compute specific quantities, leading to a unique numerical result given the fixed random seed.\n3.  **Objective**: The problem statement is written in formal, objective, and unambiguous scientific language. There are no subjective elements.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined, scientifically sound problem in computational statistics. I will proceed with providing a complete solution.\n\n### Principle-Based Design of the Solution\n\nThe solution will be implemented in Python, adhering to the specified libraries. The core logic follows the principles of survival analysis counting processes.\n\n1.  **Survival Time Generation**:\n    - **Group 1**: The hazard $\\lambda_1(t) = \\lambda_0$ leads to a cumulative hazard $\\Lambda_1(t) = \\lambda_0 t$. Inverting the survival function gives $T_1 = -\\frac{\\log U}{\\lambda_0}$, where $U \\sim \\text{Uniform}(0,1)$. This corresponds to sampling from an Exponential($\\lambda_0$) distribution.\n    - **Group 2**: The hazard is $\\lambda_2(t) = \\lambda_0(1 + \\sin(\\omega t))$. The cumulative hazard is $\\Lambda_2(t) = \\int_0^t \\lambda_2(u)du = \\lambda_0 (t - \\frac{1}{\\omega}\\cos(\\omega t) + \\frac{1}{\\omega})$. For $\\omega=0$, this reduces to $\\Lambda_2(t) = \\lambda_0 t$. For $\\omega  0$, generating a survival time $T_2$ requires solving the transcendental equation $\\Lambda_2(T_2) = -\\log U$ for $T_2$. This is a root-finding problem that can be efficiently solved using a numerical method like Brent's method, available in `scipy.optimize.root_scalar`.\n\n2.  **Censoring**: For each subject, an observed time $X_g = \\min(T_g, C_g)$ and an event indicator $\\Delta_g = \\mathbf{1}\\{T_g \\le C_g\\}$ are created, where $C_g \\sim \\text{Uniform}(0, 12)$.\n\n3.  **Test Statistic Calculation**: The core of the solution is the calculation of the weighted log-rank statistics. This is best handled by processing the data in chronological order of observed times.\n    - **Data Aggregation**: Data from both groups are combined into a single dataset, sorted by observation time.\n    - **Iterative Calculation**: We iterate through the unique time points present in the sorted data. At each unique time $t_j$:\n        a. **Risk Sets**: The number of subjects at risk just before $t_j$ in each group ($Y_{1j}, Y_{2j}$) are tracked. These are initialized to $n_1$ and $n_2$ and are decremented as subjects are observed (either event or censoring).\n        b. **Events**: The number of events at $t_j$ in each group ($d_{1j}, d_{2j}$) are counted.\n        c. **Pooled Survival (for weights)**: The pooled Kaplan-Meier estimate $S_{\\text{pooled}}(t_j^-)$ is maintained. It is the survival probability just before time $t_j$, calculated from the combined data of both groups. It is updated after processing the events at each time point.\n        d. **Statistic Components**: If there are events at $t_j$ ($d_j = d_{1j} + d_{2j}  0$), we compute the contribution to the test statistics.\n            - The \"Observed minus Expected\" term for group 2 is $O_{2j} - E_{2j} = d_{2j} - d_j \\frac{Y_{2j}}{Y_{1j}+Y_{2j}}$.\n            - The variance of this term, under the null hypothesis (hypergeometric model), is $\\text{Var}(d_{2j}) = \\frac{Y_{1j} Y_{2j} d_j (Y_j - d_j)}{Y_j^2 (Y_j - 1)}$, where $Y_j = Y_{1j}+Y_{2j}$.\n        e. **Accumulation**: The weighted terms $w(t_j)(O_{2j}-E_{2j})$ and $w(t_j)^2 \\text{Var}(d_{2j})$ are accumulated for the numerator ($Z$) and variance ($V$) of each of the three tests. The weights $w(t_j)$ are:\n            - $(p_1, q_1)=(0,0) \\Rightarrow w=1$ (standard log-rank)\n            - $(p_2, q_2)=(1,0) \\Rightarrow w = S_{\\text{pooled}}(t_j^-)$\n            - $(p_3, q_3)=(0,1) \\Rightarrow w = 1 - S_{\\text{pooled}}(t_j^-)$\n    - **Finalization**: After iterating through all event times, the final standardized statistic for each test is $U = Z / \\sqrt{V}$. If $V=0$, the statistic is $0$.\n    - **p-value**: The two-sided p-value is computed as $2 \\times (1 - \\Phi(|U|))$, where $\\Phi$ is the standard normal CDF. This is obtained using `scipy.stats.norm.sf`.\n\n4.  **Simulation Loop**: The entire process is encapsulated in a loop over the specified $\\omega$ values. The Group 1 dataset is generated once before the loop, and the random number generator state is preserved, allowing for new, independent random numbers to be drawn for Group 2 in each iteration. This isolates the effect of $\\omega$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import root_scalar\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and print the results.\n    \"\"\"\n    # Parameter specification\n    LAMBDA0 = 0.2\n    N1 = 200\n    N2 = 200\n    CENSOR_MAX = 12.0\n    SEED = 42\n    OMEGAS = [0.0, 0.5, 2.0, 6.0]\n\n    # Initialize a deterministic random number generator\n    rng = np.random.default_rng(SEED)\n\n    def generate_survival_time_g2(n, omega):\n        \"\"\"\n        Generates survival times for group 2 using the inverse transform method.\n        For omega  0, this requires numerical root-finding.\n        \"\"\"\n        if omega == 0.0:\n            # When omega is 0, hazard is constant, same as group 1\n            U = rng.uniform(size=n)\n            return -np.log(U) / LAMBDA0\n\n        U = rng.uniform(size=n)\n        log_U = np.log(U)\n        \n        times = np.zeros(n)\n        for i in range(n):\n            # Define the function f(t) = Lambda_2(t) + log(U) = 0\n            f = lambda t: LAMBDA0 * (t - (np.cos(omega * t) - 1.0) / omega) + log_U[i]\n            \n            # Initial guess for the root, based on the average hazard\n            t_guess = -log_U[i] / LAMBDA0\n            \n            # Use a robust numerical solver to find the root T_2\n            # A large bracket is used to ensure the root is found.\n            sol = root_scalar(f, bracket=[0, 200], x0=t_guess, method='brentq')\n            times[i] = sol.root\n            \n        return times\n\n    def compute_all_p_values(times, statuses, groups):\n        \"\"\"\n        Calculates the p-values for the standard log-rank test and two\n        Fleming-Harrington weighted tests ((1,0) and (0,1)).\n        \"\"\"\n        # Sort data chronologically. A stable sort is used for consistency.\n        sort_idx = np.argsort(times, kind='mergesort')\n        times = times[sort_idx]\n        statuses = statuses[sort_idx]\n        groups = groups[sort_idx]\n\n        # Initialize numerators (Z) and variances (V) for the three tests\n        Z_00, Z_10, Z_01 = 0.0, 0.0, 0.0\n        V_00, V_10, V_01 = 0.0, 0.0, 0.0\n        \n        # Initialize pooled Kaplan-Meier survival estimate\n        S_pooled = 1.0\n\n        # Get unique time points and their first indices in the sorted array\n        unique_times, first_indices = np.unique(times, return_index=True)\n\n        n1_total = np.sum(groups == 1)\n        n2_total = np.sum(groups == 2)\n        \n        # Initial numbers at risk\n        Y1, Y2 = n1_total, n2_total\n\n        # Iterate through unique time points\n        for i, t in enumerate(unique_times):\n            Y_total = Y1 + Y2\n            \n            # Identify all subjects with observation at the current time t\n            if i  len(unique_times) - 1:\n                mask_t = np.arange(first_indices[i], first_indices[i+1])\n            else:\n                mask_t = np.arange(first_indices[i], len(times))\n\n            # Count total events (d) at time t\n            d = np.sum(statuses[mask_t])\n            \n            if d  0:\n                # Count events in group 1 (d1) and group 2 (d2)\n                d1 = np.sum(statuses[mask_t]  (groups[mask_t] == 1))\n                d2 = d - d1\n\n                if Y_total  0:\n                    # Expected events in group 2 under H0\n                    E2 = d * Y2 / Y_total\n                    O_minus_E = d2 - E2\n                    \n                    # Hypergeometric variance of d2\n                    if Y_total  1:\n                        Var = (Y1 * Y2 * d * (Y_total - d)) / (Y_total**2 * (Y_total - 1))\n                    else:\n                        Var = 0.0\n                    \n                    # Weights are based on S_pooled(t-), which is the current S_pooled value\n                    w_10 = S_pooled\n                    w_01 = 1.0 - S_pooled\n\n                    # Accumulate numerators and variances for each test\n                    Z_00 += O_minus_E\n                    V_00 += Var\n                    \n                    Z_10 += w_10 * O_minus_E\n                    V_10 += w_10**2 * Var\n                    \n                    Z_01 += w_01 * O_minus_E\n                    V_01 += w_01**2 * Var\n                \n                # Update pooled KM survival for the next time point\n                if Y_total  0:\n                    S_pooled *= (1.0 - d / Y_total)\n\n            # Update numbers at risk by removing all subjects observed at time t\n            w1 = np.sum(groups[mask_t] == 1)\n            w2 = len(mask_t) - w1\n            Y1 -= w1\n            Y2 -= w2\n            \n        test_results = []\n        for Z, V in [(Z_00, V_00), (Z_10, V_10), (Z_01, V_01)]:\n            if V  1e-9: # Avoid division by a very small number\n                U = Z / np.sqrt(V)\n                p_value = 2 * norm.sf(np.abs(U))\n            else: # If variance is zero, no evidence against H0\n                p_value = 1.0\n            test_results.append(p_value)\n            \n        return test_results\n\n    # --- Main Simulation Logic ---\n    \n    # Generate the fixed dataset for group 1 once\n    T1 = -np.log(rng.uniform(size=N1)) / LAMBDA0\n    C1 = rng.uniform(0, CENSOR_MAX, size=N1)\n    X1 = np.minimum(T1, C1)\n    D1 = (T1 = C1).astype(int)\n\n    all_p_values = []\n    # Loop over each frequency parameter omega\n    for omega in OMEGAS:\n        # Simulate a new dataset for group 2 for the current omega\n        T2 = generate_survival_time_g2(N2, omega)\n        C2 = rng.uniform(0, CENSOR_MAX, size=N2)\n        X2 = np.minimum(T2, C2)\n        D2 = (T2 = C2).astype(int)\n        \n        # Combine data from both groups\n        times = np.concatenate((X1, X2))\n        statuses = np.concatenate((D1, D2))\n        groups = np.concatenate((np.ones(N1, dtype=int), np.full(N2, 2, dtype=int)))\n        \n        # Compute the three p-values for the combined dataset\n        p_vals = compute_all_p_values(times, statuses, groups)\n        all_p_values.extend(p_vals)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, all_p_values))}]\")\n\nsolve()\n```"
        }
    ]
}