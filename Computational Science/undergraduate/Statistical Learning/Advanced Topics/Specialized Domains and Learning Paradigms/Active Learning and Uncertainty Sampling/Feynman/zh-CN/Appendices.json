{
    "hands_on_practices": [
        {
            "introduction": "掌握主动学习的第一步是亲手实现其核心算法。本练习将指导你为三种常见的不确定性抽样策略——最小置信度、边缘抽样和熵抽样——编写代码。通过在一个构造的数据集上应用这些方法，你将直观地理解它们如何以不同方式量化模型的不确定性，并观察到这如何导致它们选择不同的数据点进行标注 。",
            "id": "3095122",
            "problem": "给定一个用于多类别分类的基于池的主动学习场景，其中具有固定参数 $\\hat{\\theta}$ 的概率分类器为每个未标记的输入 $x$ 提供一个在有限标签集 $\\mathcal{Y}$ 上的类别预测分布 $p_{\\hat{\\theta}}(y \\mid x)$。您的任务是实现三种不确定性采样策略——最低置信度（least-confidence）、边际（margin）和熵（entropy）采样——并确保它们在一个适当构建的池上产生不同的选择。解决方案必须是一个完整的、可运行的程序，该程序评估指定的测试套件并以确切的格式打印所请求的输出。\n\n基本基础和定义：\n- 对于每个未标记的输入 $x$，分类器输出一个类别概率向量 $p_{\\hat{\\theta}}(y \\mid x)$（其中 $y \\in \\mathcal{Y}$），该向量满足对所有 $y$ 都有 $p_{\\hat{\\theta}}(y \\mid x) \\ge 0$ 且 $\\sum_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) = 1$。\n- 令 $p_{(1)}(x) \\ge p_{(2)}(x) \\ge \\cdots$ 表示 $p_{\\hat{\\theta}}(y \\mid x)$ 按降序排列的概率。\n- 对于 $x$ 的最低置信度不确定性由 $\\max_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) = p_{(1)}(x)$ 来衡量。最低置信度采样选择池中具有最小 $p_{(1)}(x)$ 值的 $x$。\n- 对于 $x$ 的边际不确定性由前两个最高概率之差 $p_{(1)}(x) - p_{(2)}(x)$ 来衡量。边际采样选择池中具有最小 $p_{(1)}(x) - p_{(2)}(x)$ 值的 $x$。\n- 对于 $x$ 的熵不确定性由香农熵 $H\\!\\left[Y \\mid x, \\hat{\\theta}\\right] = - \\sum_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) \\log p_{\\hat{\\theta}}(y \\mid x)$ 给出，其中 $\\log$ 表示自然对数。熵采样选择池中具有最大 $H\\!\\left[Y \\mid x, \\hat{\\theta}\\right]$ 值的 $x$。\n- 平局必须通过选择 0-基索引下最小的索引来确定性地解决。\n\n编程要求：\n- 对于每个测试用例，您将得到一个有限的池 $\\mathcal{U} = \\{x_i\\}_{i=0}^{n-1}$，由预测的类别概率向量矩阵 $\\left( p_{\\hat{\\theta}}(y \\mid x_i) \\right)$ 表示。每一行对应一个 $x_i$，并且每行之和在浮点精度范围内为 $1$。\n- 对于每个测试用例，计算：\n  $1.$ 最低置信度采样选择的索引（基于 0-基索引）以及该索引对应的 $p_{(1)}(x)$ 值。\n  $2.$ 边际采样选择的索引以及该索引对应的 $\\left(p_{(1)}(x) - p_{(2)}(x)\\right)$ 值。\n  $3.$ 熵采样选择的索引以及该索引对应的 $H\\!\\left[Y \\mid x, \\hat{\\theta}\\right]$ 值。\n  $4.$ 三个选择中的不同索引的数量（即包含三个所选索引的集合的基数）。\n- 在计算熵时，使用约定 $0 \\cdot \\log 0 = 0$。$\\log$ 使用自然对数。\n- 所有浮点输出必须四舍五入到恰好 $6$ 位小数。\n- 任何选择标准的平局都必须通过选择 0-基索引下最小的索引来解决。\n\n测试套件：\n- 测试用例 $\\mathbf{1}$ (理想路径：三个标准选择了不同的输入):\n  - 包含四个未标记输入（行）和三个类别（列）的池：\n    - $x_0$: $\\left[0.39,\\; 0.38,\\; 0.23\\right]$\n    - $x_1$: $\\left[0.41,\\; 0.295,\\; 0.295\\right]$\n    - $x_2$: $\\left[0.500,\\; 0.495,\\; 0.005\\right]$\n    - $x_3$: $\\left[0.70,\\; 0.20,\\; 0.10\\right]$\n- 测试用例 $\\mathbf{2}$ (边界情况：池内所有标准完全平局；通过最小索引决胜):\n  - 包含三个未标记输入和三个类别的池：\n    - $x_0$: $\\left[\\frac{1}{3},\\; \\frac{1}{3},\\; \\frac{1}{3}\\right]$\n    - $x_1$: $\\left[\\frac{1}{3},\\; \\frac{1}{3},\\; \\frac{1}{3}\\right]$\n    - $x_2$: $\\left[\\frac{1}{3},\\; \\frac{1}{3},\\; \\frac{1}{3}\\right]$\n- 测试用例 $\\mathbf{3}$ (边缘情况：存在零概率；近确定性预测):\n  - 包含四个未标记输入和三个类别的池：\n    - $x_0$: $\\left[0.99,\\; 0.01,\\; 0.00\\right]$\n    - $x_1$: $\\left[0.60,\\; 0.40,\\; 0.00\\right]$\n    - $x_2$: $\\left[0.49,\\; 0.49,\\; 0.02\\right]$\n    - $x_3$: $\\left[0.55,\\; 0.45,\\; 0.00\\right]$\n\n最终输出规范：\n- 对于每个测试用例，按顺序生成以下七个值作为扁平序列：\n  - $\\text{LC\\_idx}$, 在 $\\text{LC\\_idx}$ 处的 $p_{(1)}$，\n  - $\\text{M\\_idx}$, 在 $\\text{M\\_idx}$ 处的 $\\left(p_{(1)} - p_{(2)}\\right)$，\n  - $\\text{ENT\\_idx}$, 在 $\\text{ENT\\_idx}$ 处的 $H$，\n  - $\\text{distinct\\_count}$。\n- 将所有测试用例的结果按测试用例的顺序汇总到一个列表中。您的程序应生成单行输出，其中包含一个用方括号括起来、以逗号分隔的结果列表，所有浮点数四舍五入到恰好 $6$ 位小数。例如，输出必须具有 $\\left[\\text{v}_0,\\text{v}_1,\\ldots,\\text{v}_{m-1}\\right]$ 的形式，其中每个 $\\text{v}_j$ 是一个整数或一个恰有 $6$ 位小数的浮点数。",
            "solution": "问题陈述已经过严格审查，并被确定为 **有效**。它在科学上基于统计学习的原理，特别是在主动学习方面。最低置信度、边际和熵采样的定义是标准且正确的。该问题是适定的（well-posed），提供了所有必要的数据、约束和确定性的平局决胜规则，这确保了每个测试用例都存在唯一且可计算的解。输入是自洽的，目标是可形式化和客观的。\n\n任务是为多类别分类器在基于池的主动学习设置中实现并比较三种不同的不确定性采样策略。给定一个未标记数据点池 $\\mathcal{U}$，每个数据点由一个关于类别集合 $\\mathcal{Y}$ 的预测概率向量 $p_{\\hat{\\theta}}(y \\mid x)$ 表示，我们必须选择信息量最大的单个点来查询其真实标签。“信息量”或“不确定性”由三种不同的度量标准来量化。\n\n设 $P$ 是一个维度为 $n \\times k$ 的矩阵，其中 $n$ 是池中数据点的数量，$k = |\\mathcal{Y}|$ 是类别的数量。每行 $P_i$ 对应于概率向量 $p_{\\hat{\\theta}}(y \\mid x_i)$。目标是根据以下每个标准找到对应于最不确定数据点的索引 $i^*$。\n\n**1. 最低置信度采样 (Least-Confidence Sampling)**\n\n该策略基于分类器对其最可能预测的置信度。当最可能类别的概率最小时，不确定性被认为是最大的。对于输入 $x$ 的最置信预测由 $p_{(1)}(x) = \\max_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x)$ 给出。最低置信度采样策略从池 $\\mathcal{U}$ 中选择使该值最小化的数据点 $x^*$。\n\n因此，选择索引 $i^*_{LC}$ 为：\n$$\ni^*_{LC} = \\arg\\min_{i \\in \\{0, \\dots, n-1\\}} p_{(1)}(x_i)\n$$\n如果出现平局，则选择最小的索引 $i$。相关的不确定性值为 $p_{(1)}(x_{i^*_{LC}})$。\n\n**2. 边际采样 (Margin Sampling)**\n\n该策略通过考虑两个最可能类别之间的模糊性来改进最低置信度采样。第一和第二最可能类别 $p_{(1)}(x)$ 和 $p_{(2)}(x)$ 之间的小边际表明分类器难以区分它们，从而表示高度不确定性。边际定义为 $p_{(1)}(x) - p_{(2)}(x)$。边际采样选择具有最小边际的数据点 $x^*$。\n\n选择索引 $i^*_{M}$ 为：\n$$\ni^*_{M} = \\arg\\min_{i \\in \\{0, \\dots, n-1\\}} \\left( p_{(1)}(x_i) - p_{(2)}(x_i) \\right)\n$$\n同样，平局通过选择最小索引来解决。相关的不确定性值是边际本身，$p_{(1)}(x_{i^*_{M}}) - p_{(2)}(x_{i^*_{M}})$。\n\n**3. 熵采样 (Entropy Sampling)**\n\n这是三种度量中最全面的，因为它考虑了整个概率分布。它使用香农熵作为不确定性的度量。一个尖锐的概率分布（一个概率高，其他概率低）具有低熵，表示低不确定性。相反，接近均匀的分布具有高熵，表示高不确定性。对于给定输入 $x$ 的熵为：\n$$\nH[Y \\mid x, \\hat{\\theta}] = - \\sum_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) \\log p_{\\hat{\\theta}}(y \\mid x)\n$$\n其中 $\\log$ 是自然对数，并使用 $0 \\cdot \\log 0 = 0$ 的约定。熵采样选择具有最大熵的数据点 $x^*$。\n\n选择索引 $i^*_{ENT}$ 为：\n$$\ni^*_{ENT} = \\arg\\max_{i \\in \\{0, \\dots, n-1\\}} H[Y \\mid x_i, \\hat{\\theta}]\n$$\n平局通过选择最小索引来解决。相关的不确定性值是熵 $H[Y \\mid x_{i^*_{ENT}}, \\hat{\\theta}]$。\n\n**计算流程**\n\n对于由一个 $n \\times k$ 概率矩阵 $P$ 表示的每个测试用例：\n1.  **最低置信度：** 对于每一行，找到最大概率。然后，找到具有这些最大值中最小值的行的索引。\n2.  **边际：** 对于每一行，将概率按降序排序以找到 $p_{(1)}$ 和 $p_{(2)}$。计算它们的差值。然后，找到具有最小差值的行的索引。\n3.  **熵：** 对于每一行，计算香农熵，通过确保形如 $0 \\cdot \\log 0$ 的项对总和的贡献为 $0$ 来正确处理 $p=0$ 的情况。然后，找到具有最大熵的行的索引。\n4.  **不同索引：** 收集三个结果索引（$i^*_{LC}$、$i^*_{M}$、$i^*_{ENT}$）并计算此集合中唯一值的数量。\n5.  **格式化：** 所有浮点结果都四舍五入到 $6$ 位小数，并与整数索引和计数一起整理成一个扁平列表，以符合问题规范。\n\n实现将利用 `numpy` 库，其 `argmin` 和 `argmax` 函数通过返回极值的第一个遇到的索引，固有地满足了指定的平局决胜规则。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements least-confidence, margin, and entropy uncertainty sampling\n    for a series of test cases and formats the output as specified.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        [\n            [0.39, 0.38, 0.23],\n            [0.41, 0.295, 0.295],\n            [0.500, 0.495, 0.005],\n            [0.70, 0.20, 0.10],\n        ],\n        # Test case 2\n        [\n            [1/3, 1/3, 1/3],\n            [1/3, 1/3, 1/3],\n            [1/3, 1/3, 1/3],\n        ],\n        # Test case 3\n        [\n            [0.99, 0.01, 0.00],\n            [0.60, 0.40, 0.00],\n            [0.49, 0.49, 0.02],\n            [0.55, 0.45, 0.00],\n        ]\n    ]\n\n    results = []\n    for case in test_cases:\n        p_matrix = np.array(case, dtype=np.float64)\n\n        # 1. Least-Confidence Sampling\n        # The goal is to find the point with the smallest most-confident prediction.\n        p1_values = np.max(p_matrix, axis=1)\n        lc_idx = np.argmin(p1_values)\n        lc_val = p1_values[lc_idx]\n\n        # 2. Margin Sampling\n        # The goal is to find the point with the smallest difference between the\n        # top two predictions.\n        # Sort probabilities in descending order for each row.\n        sorted_p = -np.sort(-p_matrix, axis=1)\n        p1 = sorted_p[:, 0]\n        p2 = sorted_p[:, 1]\n        margin_values = p1 - p2\n        m_idx = np.argmin(margin_values)\n        m_val = margin_values[m_idx]\n\n        # 3. Entropy Sampling\n        # The goal is to find the point with the highest Shannon entropy.\n        # We must handle the 0 * log(0) = 0 case.\n        # Create a temporary array where p=0 is replaced by p=1.\n        # This makes log(p)=0, so p*log(p) becomes 0, satisfying the convention.\n        p_for_log = p_matrix.copy()\n        p_for_log[p_for_log == 0] = 1.0\n        entropy_values = -np.sum(p_matrix * np.log(p_for_log), axis=1)\n        ent_idx = np.argmax(entropy_values)\n        ent_val = entropy_values[ent_idx]\n\n        # 4. Count of distinct indices\n        distinct_count = len({lc_idx, m_idx, ent_idx})\n\n        # Append results for the current test case.\n        results.extend([\n            lc_idx, lc_val,\n            m_idx, m_val,\n            ent_idx, ent_val,\n            distinct_count\n        ])\n\n    def format_val(v):\n        \"\"\"Formats an integer or float according to problem specs.\"\"\"\n        if isinstance(v, (int, np.integer)):\n            return str(v)\n        else:  # float or np.floating\n            return f\"{v:.6f}\"\n\n    # Format the final list of results for printing.\n    formatted_results = [format_val(r) for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "虽然不确定性抽样很有效，但它倾向于选择彼此高度相似的冗余数据点，尤其是在批处理模式下。本练习介绍了一种更高级的技术——行列式点过程 (Determinantal Point Process, DPP)，它优雅地解决了这个问题，通过在选择中同时考虑数据点的不确定性（质量）和多样性。你将实现一个基于 DPP 的贪心算法，以构建一个既包含高度不确定又具有良好代表性的数据批次 。",
            "id": "3095129",
            "problem": "给定一个由特征向量表示的有限未标记物品池。考虑一个定义在索引子集 $S \\subseteq \\{0,1,\\dots,N-1\\}$ 上的行列式点过程（Determinantal Point Process, DPP），其概率与一个核矩阵的主子矩阵的行列式成正比。具体来说，核定义如下\n$$\nK_{ij} \\;=\\; \\alpha \\,\\mathrm{sim}(x_i, x_j) \\;+\\; \\beta \\, u(x_i)\\,\\delta_{ij},\n$$\n其中 $\\alpha \\ge 0$ 和 $\\beta \\ge 0$ 是标量权重，$x_i \\in \\mathbb{R}^d$ 是物品 $i$ 的特征向量，$u(x_i) \\ge 0$ 是物品 $i$ 的不确定性分数，$\\delta_{ij}$ 是克罗内克δ函数，$\\mathrm{sim}(x_i,x_j)$ 是一个半正定相似度函数。令相似度为高斯径向基函数 (RBF)\n$$\n\\mathrm{sim}(x_i,x_j) \\;=\\; \\exp\\!\\big(-\\gamma \\,\\lVert x_i - x_j\\rVert_2^2\\big),\n$$\n其逆长度尺度参数为 $\\gamma > 0$。\n\n从半正定核、主子矩阵的行列式以及DPP概率规则的定义出发，推导出一个有原则的算法。该算法在给定 $(\\alpha,\\beta,\\gamma)$、一个固定的批大小 $k$、特征向量 $x_i$ 和不确定性 $u(x_i)$ 的情况下，返回一个大小为 $k$ 的子集 $S$，该子集能最大化概率（等价于最大化 $\\det(K_S)$）。在任何决策点出现平局时，必须通过选择最小的索引来打破平局。你的算法必须在科学上是合理的，并且必须从基本定义逻辑推导得出，而不是依赖未经证实的启发式方法。使用标准的双精度算术。\n\n使用以下物品池和不确定性分数（这些在所有测试用例中都是固定的）：\n- 物品数量 $N = 6$，特征维度 $d = 2$。\n- 特征向量 $X \\in \\mathbb{R}^{6 \\times 2}$：\n$$\nX = \\begin{bmatrix}\n0  & 0 \\\\\n0  & 0 \\\\\n100  & 100 \\\\\n100  & 100 \\\\\n-100  & -100 \\\\\n-100  & -100\n\\end{bmatrix}.\n$$\n- 不确定性分数 $u \\in \\mathbb{R}^6$：\n$$\nu = \\begin{bmatrix}\n0.6 \\\\ 0.1 \\\\ 0.9 \\\\ 0.2 \\\\ 0.8 \\\\ 0.3\n\\end{bmatrix}.\n$$\n\n你的程序必须为每个测试用例精确地构建如上定义的 $K$，然后按你推导的算法选择的顺序输出所选索引。\n\n测试套件：\n- 案例 1：$\\alpha = 1.0$, $\\beta = 1.0$, $\\gamma = 0.5$, $k = 3$。\n- 案例 2：$\\alpha = 0.0$, $\\beta = 1.0$, $\\gamma = 0.5$, $k = 3$。\n- 案例 3：$\\alpha = 1.0$, $\\beta = 0.0$, $\\gamma = 0.5$, $k = 3$。\n- 案例 4：$\\alpha = 0.5$, $\\beta = 1.5$, $\\gamma = 0.5$, $k = 1$。\n- 案例 5：$\\alpha = 0.7$, $\\beta = 0.3$, $\\gamma = 0.5$, $k = 6$。\n\n答案类型和最终输出格式：\n- 对于每个测试用例，答案是对应于所选索引的整数列表，按其被选择的顺序列出。\n- 你的程序应该生成单行输出，包含五个测试用例的结果，格式为一个逗号分隔的索引列表的列表，每个内部列表也用逗号分隔，且没有空格。例如，输出的形状应为 $$[[i_1,i_2,\\dots],[j_1,\\dots],\\dots]$$ 并打印为单行。",
            "solution": "所述问题在科学上是有根据的、适定的且完整的。它描述了基于行列式点过程（DPPs）的成熟理论的一项统计学习任务。核矩阵 $K$、相似度函数 $\\mathrm{sim}(x_i, x_j)$ 以及概率规则 $P(S) \\propto \\det(K_S)$ 的定义都是标准的。所提供的数据和参数足以构建和实现一个有效的算法。目标是找到一个大小为 $k$ 的固定子集 $S$，使得 $\\det(K_S)$ 最大化。\n\n核矩阵 $K$ 定义为 $K_{ij} = \\alpha \\,\\mathrm{sim}(x_i, x_j) + \\beta \\, u(x_i)\\,\\delta_{ij}$。相似度函数 $\\mathrm{sim}(x_i, x_j) = \\exp(-\\gamma \\,\\lVert x_i - x_j\\rVert_2^2)$ 是一个高斯RBF核，已知它是半正定的。将此相似度矩阵表示为 $A$。项 $\\beta u(x_i)\\delta_{ij}$ 可以写成一个对角矩阵 $D$，其中 $D_{ii} = \\beta u(x_i)$。由于 $\\beta \\ge 0$ 和 $u(x_i) \\ge 0$，矩阵 $D$ 是半正定的。完整的核是 $K = \\alpha A + D$。给定 $\\alpha \\ge 0$，矩阵 $\\alpha A$ 也是半正定的。两个半正定矩阵的和是半正定的，因此 $K$ 保证是半正定的。这确保了对于任何索引子集 $S$，其主子矩阵 $K_S$ 也是半正定的，因此 $\\det(K_S) \\ge 0$。\n\n找到最大化 $\\det(K_S)$ 的 $k$ 元素子集 $S$ 的问题在一般情况下是NP难的。然而，对于半正定矩阵 $K$，函数 $f(S) = \\log \\det(K_S)$ 是子模的。此性质为一个有原则的贪心算法提供了理论依据，该算法能提供强大的近似保证，并且是解决此类优化任务的标准方法。问题要求推导出这样一个有原则的算法。\n\n贪心算法以迭代方式构建集合 $S$。从空集 $S_0 = \\emptyset$ 开始，它在 $k$ 步中每次添加一个元素。在第 $m$ 步（$1 \\le m \\le k$），已选定集合 $S_{m-1}$ 后，算法从剩余候选集合 $C \\setminus S_{m-1}$ 中选择下一个元素 $j$，该元素能提供最大的边际增益。目标是最大化 $\\det(K_{S_{m-1} \\cup \\{j\\}})$。\n\n让我们分析目标函数。利用分块矩阵行列式的性质，对于任何 $j \\notin S$，我们有：\n$$\n\\det(K_{S \\cup \\{j\\}}) = \\det(K_S) \\cdot \\left( K_{jj} - K_{j, S} K_S^{-1} K_{S, j} \\right)\n$$\n其中 $K_S$ 是索引在 $S$ 中的主子矩阵，$K_{j,S}$ 是物品 $j$ 与 $S$ 中物品之间的核值的行向量，而 $K_{S,j}$ 是其转置。此公式在 $K_S$ 可逆时有效。由于在第 $m$ 步 $\\det(K_S)$ 是固定的，最大化 $\\det(K_{S_{m-1} \\cup \\{j\\}})$ 等价于最大化边际增益项 $K_{jj} - K_{j, S_{m-1}} K_{S_{m-1}}^{-1} K_{S_{m-1}, j}$。然而，为了避免计算矩阵的逆并处理 $K_S$ 可能为奇异矩阵的情况，最直接的方法是为每个候选 $j$ 计算 $\\det(K_{S_{m-1} \\cup \\{j\\}})$，并选择产生最大值的那个。\n\n这引出了以下算法：\n\n1.  **初始化**：给定参数 $(\\alpha, \\beta, \\gamma)$、特征向量 $\\{x_i\\}_{i=0}^{N-1}$ 和不确定性 $\\{u_i\\}_{i=0}^{N-1}$：\n    a. 构建 $N \\times N$ 核矩阵 $K$。对于每对索引 $(i, j)$：\n    $$\n    K_{ij} \\;=\\; \\alpha \\exp\\!\\big(-\\gamma \\,\\lVert x_i - x_j\\rVert_2^2\\big) \\;+\\; \\beta \\, u_i\\,\\delta_{ij}\n    $$\n    b. 将所选索引的集合初始化为空列表，$S = []$。\n    c. 初始化可用索引的集合，$C = \\{0, 1, \\dots, N-1\\}$。\n\n2.  **迭代选择**：对于从 $1$到 $k$ 的 $m$：\n    a. 对于每个候选索引 $j \\in C \\setminus S$：\n        i. 形成一个临时索引集 $S' = S \\cup \\{j\\}$。\n        ii. 构建子矩阵 $K_{S'}$。\n        iii. 计算行列式 $\\det(K_{S'})$。\n    b. 识别出最大化此行列式的候选索引 $j^*$。\n    c. 如果行列式值出现平局，选择索引值最小的候选者。\n    d. 将 $j^*$ 添加到所选索引列表 $S$ 中。\n\n3.  **输出**：返回所选索引的有序列表 $S$。\n\n由于有平局打破规则，该算法是确定性的，并提供了一个源于DPP和子模优化基本性质的有原则的、循序渐进的程序。这就是需要实现的程序。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the DPP selection problem for the given test cases.\n    \"\"\"\n    \n    # Fixed data for all test cases\n    X = np.array([\n        [0.0, 0.0],\n        [0.0, 0.0],\n        [100.0, 100.0],\n        [100.0, 100.0],\n        [-100.0, -100.0],\n        [-100.0, -100.0]\n    ], dtype=np.float64)\n    \n    u = np.array([0.6, 0.1, 0.9, 0.2, 0.8, 0.3], dtype=np.float64)\n    N = X.shape[0]\n\n    test_cases = [\n        # (alpha, beta, gamma, k)\n        (1.0, 1.0, 0.5, 3),\n        (0.0, 1.0, 0.5, 3),\n        (1.0, 0.0, 0.5, 3),\n        (0.5, 1.5, 0.5, 1),\n        (0.7, 0.3, 0.5, 6),\n    ]\n\n    results = []\n    for alpha, beta, gamma, k in test_cases:\n        selected_indices = run_dpp_greedy_selection(X, u, N, alpha, beta, gamma, k)\n        results.append(selected_indices)\n\n    # Format the final output string to be a single line with no spaces.\n    # e.g., [[1,2],[3,4]]\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\ndef run_dpp_greedy_selection(X, u, N, alpha, beta, gamma, k):\n    \"\"\"\n    Implements the greedy algorithm to select a k-sized subset that maximizes\n    the determinant of the DPP kernel submatrix.\n\n    Args:\n        X (np.ndarray): Feature vectors of size (N, d).\n        u (np.ndarray): Uncertainty scores of size (N,).\n        N (int): Total number of items.\n        alpha (float): Weight for the similarity term.\n        beta (float): Weight for the uncertainty term.\n        gamma (float): Inverse length-scale for the RBF kernel.\n        k (int): Number of items to select.\n\n    Returns:\n        list: A list of selected indices in the order of selection.\n    \"\"\"\n\n    # 1. Construct the full N x N kernel matrix K\n    dist_sq = np.sum((X[:, np.newaxis, :] - X[np.newaxis, :, :]) ** 2, axis=-1)\n    sim_matrix = np.exp(-gamma * dist_sq)\n    kernel_matrix = alpha * sim_matrix + beta * np.diag(u)\n\n    selected_indices = []\n    available_indices = list(range(N))\n\n    # 2. Iteratively select k items\n    for _ in range(k):\n        max_det = -1.0\n        best_next_idx = -1\n\n        # Iterate through available candidates in increasing order of index\n        for candidate_idx in available_indices:\n            \n            # Form the temporary set of indices for determinant calculation\n            temp_indices = selected_indices + [candidate_idx]\n            \n            # Extract submatrix and compute its determinant.\n            # np.ix_ is used for convenient submatrix extraction.\n            sub_k = kernel_matrix[np.ix_(temp_indices, temp_indices)]\n            \n            # Use np.linalg.slogdet for better numerical stability with small determinants,\n            # though det() is fine here. We need the actual determinant value.\n            current_det = np.linalg.det(sub_k)\n\n            # Check for a new maximum determinant. Tie-breaking is handled\n            # implicitly by checking for strictly greater values and iterating\n            # through candidates in increasing index order.\n            if current_det > max_det:\n                max_det = current_det\n                best_next_idx = candidate_idx\n        \n        # Add the best index to the selection and remove it from candidates\n        if best_next_idx != -1:\n            selected_indices.append(best_next_idx)\n            available_indices.remove(best_next_idx)\n\n    return selected_indices\n\nsolve()\n\n```"
        },
        {
            "introduction": "实现了主动学习策略后，评估其有效性是至关重要的一步。这个最终练习将重点放在量化主动学习相对于被动学习所带来的“标签节省”上。你将使用一个假设的实验数据集，通过拟合学习曲线模型 $R(n)=\\alpha n^{-\\beta}+\\gamma$ 来估计达到相同目标性能水平需要多少标签，从而具体衡量主动学习的效率优势 。",
            "id": "3095092",
            "problem": "在监督式统计学习中，预期泛化风险的学习曲线通常可以通过经验幂律加平台模型 $R(n)=\\alpha n^{-\\beta}+\\gamma$ 很好地近似，其中 $R(n)$ 表示在有 $n$ 个标记样本时的预期风险，$\\alpha \\ge 0$ 是一个尺度参数，$\\beta > 0$ 是衰减率，$\\gamma \\ge 0$ 是不可约平台项。考虑比较被动采样（从数据分布中抽取独立同分布的标签）和采用不确定性采样的​​主动学习（一种优先标记当前模型最不确定输入的策略）。我们假设两种策略都对不同标签计数的风险产生带噪声的测量值，并且我们通过非线性最小二乘法来拟合学习曲线模型的参数。\n\n从良定学习问题中预期风险 $R(n)$ 随 $n$ 非递增这一基本基础出发，并且 $R(n)$ 随 $n$ 增加而减少的趋势通常由经验模型 $R(n)=\\alpha n^{-\\beta}+\\gamma$ 捕获，推导出一个表达式，用于计算对于给定的目标误差阈值 $\\epsilon$ 使得 $R(n)\\le \\epsilon$ 的最小标签计数 $n_{\\text{req}}$。使用此表达式估算被动采样和主动采样为达到相同目标 $\\epsilon$ 所需的整数标签节省量。\n\n你的任务是编写一个完整的、可运行的程序，该程序：\n- 通过有界非线性最小二乘法，将模型参数 $(\\alpha,\\beta,\\gamma)$ 与每种策略测得的 $(n,R)$ 对进行拟合。\n- 计算每种策略满足 $R(n)\\le \\epsilon$ 的最小整数标签计数 $n_{\\text{req}}$。\n- 使用以下约定计算标签节省量 $S = n_{\\text{req}}^{\\text{passive}} - n_{\\text{req}}^{\\text{active}}$：\n    - 如果某个策略的 $\\epsilon \\le \\gamma$，则该策略无法达到目标，且 $n_{\\text{req}}=+\\infty$。\n    - 如果两种策略都得出 $n_{\\text{req}}=+\\infty$，则 $S$ 返回 $\\mathrm{NaN}$。\n    - 如果恰好有一种策略得出 $n_{\\text{req}}=+\\infty$，当被动策略无法达到目标时返回 $+\\infty$，当主动策略无法达到目标时返回 $-\\infty$。\n    - 否则，返回 $S$ 作为有限整数差。\n- 当 $n_{\\text{req}}$ 为有限值时，使用向上取整函数确保其为整数标签计数。\n\n使用以下测量的 $(n,R)$ 对和目标 $\\epsilon$ 值的测试套件。每对以 $(n,R)$ 形式给出，$n$ 的单位是标签数，$R$ 和 $\\epsilon$ 是无量纲的。对于每种情况，为主动和被动测量分别拟合模型，然后按规定计算标签节省量。\n\n测试用例 1（理想情况，主动学习更具样本效率）：\n- 主动学习测量值：$(20,0.154)$, $(50,0.119)$, $(100,0.104)$, $(200,0.095)$, $(500,0.088)$, $(1000,0.085)$。\n- 被动采样测量值：$(20,0.322)$, $(50,0.242)$, $(100,0.199)$, $(200,0.171)$, $(500,0.145)$, $(1000,0.132)$。\n- 目标：$\\epsilon=0.12$。\n\n测试用例 2（边界情况，$\\epsilon$ 接近主动学习的平台但低于被动采样的平台）：\n- 主动学习测量值：$(50,0.117)$, $(100,0.094)$, $(200,0.0795)$, $(500,0.067)$, $(1000,0.061)$, $(2000,0.057)$。\n- 被动采样测量值：$(50,0.236)$, $(100,0.190)$, $(200,0.158)$, $(500,0.129)$, $(1000,0.115)$, $(2000,0.105)$。\n- 目标：$\\epsilon=0.055$。\n\n测试用例 3（边缘情况，目标低于两个平台）：\n- 主动学习测量值：$(50,0.151)$, $(100,0.129)$, $(200,0.115)$, $(500,0.102)$, $(1000,0.096)$。\n- 被动采样测量值：$(50,0.190)$, $(100,0.160)$, $(200,0.139)$, $(500,0.121)$, $(1000,0.112)$。\n- 目标：$\\epsilon=0.07$。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3]”）。每个结果必须是相应测试用例的标签节省量 $S$，如果有限则表示为整数，或按上文规定表示为 $inf$、$-inf$ 或 $nan$。不应打印任何附加文本。",
            "solution": "该问题陈述经评估具有科学依据、问题良定且客观。它没有矛盾、歧义或不可靠的前提。该任务是统计学习领域内非线性回归和基于模型的推断的标准应用。\n\n预期泛化风险 $R(n)$ 作为标记样本数量 $n$ 的函数的学习曲线由以下经验模型给出：\n$$R(n) = \\alpha n^{-\\beta} + \\gamma$$\n参数的约束条件为 $\\alpha \\ge 0$，$\\beta > 0$ 和 $\\gamma \\ge 0$。参数 $\\alpha$ 是一个尺度因子，$\\beta$ 是学习率，$\\gamma$ 是不可约误差，即当 $n \\to \\infty$ 时的渐近风险。\n\n主要目标是推导出一个表达式，用于计算达到目标风险水平 $\\epsilon$ 所需的最小整数样本数 $n_{\\text{req}}$。这需要找到满足 $R(n) \\le \\epsilon$ 的最小整数 $n$。\n\n由于 $R(n)$ 是 $n$ 的一个非递增函数（对于 $\\alpha, \\beta > 0$），我们可以通过设置 $R(n) = \\epsilon$ 来找到边界情况：\n$$\\alpha n^{-\\beta} + \\gamma = \\epsilon$$\n我们继续解这个关于 $n$ 的方程：\n$$\\alpha n^{-\\beta} = \\epsilon - \\gamma$$\n为了使解存在，两边的符号必须一致。左侧的 $\\alpha n^{-\\beta}$ 是非负的，因为 $\\alpha \\ge 0$ 且 $n > 0$。因此，我们必须有 $\\epsilon - \\gamma \\ge 0$，这意味着 $\\epsilon \\ge \\gamma$。\n\n情况 1：目标误差无法达到。\n如果 $\\epsilon \\le \\gamma$，目标误差 $\\epsilon$ 小于或等于不可约误差 $\\gamma$。对于任何非平凡的学习过程（$\\alpha>0$），风险 $R(n) = \\alpha n^{-\\beta} + \\gamma$ 将始终严格大于 $\\gamma$，因此也大于 $\\epsilon$。在这种情况下，目标误差是无法达到的。根据问题的规定，所需的样本数量是无限的：\n$$n_{\\text{req}} = +\\infty \\quad \\text{if} \\quad \\epsilon \\le \\gamma$$\n\n情况 2：目标误差可以达到。\n如果 $\\epsilon > \\gamma$，有限的样本量可以达到目标。我们可以继续求解 $n$。假设 $\\alpha > 0$：\n$$n^{-\\beta} = \\frac{\\epsilon - \\gamma}{\\alpha}$$\n取两边的倒数：\n$$n^{\\beta} = \\frac{\\alpha}{\\epsilon - \\gamma}$$\n将两边都取 $1/\\beta$ 次幂：\n$$n = \\left( \\frac{\\alpha}{\\epsilon - \\gamma} \\right)^{1/\\beta}$$\n这个 $n$ 的值是达到 $R(n) = \\epsilon$ 所需的精确实数样本量。由于 $R(n)$ 是非递增的，任何大于或等于此值的样本量也将满足 $R(n) \\le \\epsilon$ 的标准。问题要求的是*最小整数*标签计数，这可以通过应用向上取整函数找到。为确保样本数至少为 $1$，我们取结果和 $1$ 的最大值。\n$$n_{\\text{req}} = \\max\\left(1, \\left\\lceil \\left( \\frac{\\alpha}{\\epsilon - \\gamma} \\right)^{1/\\beta} \\right\\rceil\\right) \\quad \\text{if} \\quad \\epsilon > \\gamma$$\n\n解决该问题的计算步骤如下：\n1.  对于每个测试用例中的每种采样策略（主动和被动），通过将函数 $R(n)$ 拟合到所提供的 $(n, R)$ 数据点来确定模型参数 $(\\alpha, \\beta, \\gamma)$。这是使用有界非线性最小二乘法完成的，特别是使用 `scipy.optimize.curve_fit` 函数。参数的界限被设定为遵守约束条件 $\\alpha \\ge 0$，$\\beta > 0$ 和 $\\gamma \\ge 0$。为 $\\beta$ 使用一个小的正下界（例如，$10^{-9}$）以确保数值稳定性并遵守严格不等式。\n2.  一旦找到了某个策略的最优参数 $(\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\gamma})$，就使用推导出的表达式为给定的目标风险 $\\epsilon$ 计算所需的样本量 $n_{\\text{req}}$。\n3.  然后，根据为处理有限、无限（$+\\infty, -\\infty$）和未定义（$\\mathrm{NaN}$）结果提供的特定规则，计算标签节省量 $S = n_{\\text{req}}^{\\text{passive}} - n_{\\text{req}}^{\\text{active}}$。\n\n这个完整的、有原则的方法被应用于所提供的每个测试用例。\n\n提供的数据是：\n测试用例 1：\n- 主动学习测量值：$(20, 0.154)$, $(50, 0.119)$, $(100, 0.104)$, $(200, 0.095)$, $(500, 0.088)$, $(1000, 0.085)$。\n- 被动采样测量值：$(20, 0.322)$, $(50, 0.242)$, $(100, 0.199)$, $(200, 0.171)$, $(500, 0.145)$, $(1000, 0.132)$。\n- 目标：$\\epsilon=0.12$。\n\n测试用例 2：\n- 主动学习测量值：$(50, 0.117)$, $(100, 0.094)$, $(200, 0.0795)$, $(500, 0.067)$, $(1000, 0.061)$, $(2000, 0.057)$。\n- 被动采样测量值：$(50, 0.236)$, $(100, 0.190)$, $(200, 0.158)$, $(500, 0.129)$, $(1000, 0.115)$, $(2000, 0.105)$。\n- 目标：$\\epsilon=0.055$。\n\n测试用例 3：\n- 主动学习测量值：$(50, 0.151)$, $(100, 0.129)$, $(200, 0.115)$, $(500, 0.102)$, $(1000, 0.096)$。\n- 被动采样测量值：$(50, 0.190)$, $(100, 0.160)$, $(200, 0.139)$, $(500, 0.121)$, $(1000, 0.112)$。\n- 目标：$\\epsilon=0.07$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef solve():\n    \"\"\"\n    Solves the learning curve analysis problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"active\": {\n                \"n\": np.array([20, 50, 100, 200, 500, 1000]),\n                \"R\": np.array([0.154, 0.119, 0.104, 0.095, 0.088, 0.085]),\n            },\n            \"passive\": {\n                \"n\": np.array([20, 50, 100, 200, 500, 1000]),\n                \"R\": np.array([0.322, 0.242, 0.199, 0.171, 0.145, 0.132]),\n            },\n            \"epsilon\": 0.12,\n        },\n        {\n            \"active\": {\n                \"n\": np.array([50, 100, 200, 500, 1000, 2000]),\n                \"R\": np.array([0.117, 0.094, 0.0795, 0.067, 0.061, 0.057]),\n            },\n            \"passive\": {\n                \"n\": np.array([50, 100, 200, 500, 1000, 2000]),\n                \"R\": np.array([0.236, 0.190, 0.158, 0.129, 0.115, 0.105]),\n            },\n            \"epsilon\": 0.055,\n        },\n        {\n            \"active\": {\n                \"n\": np.array([50, 100, 200, 500, 1000]),\n                \"R\": np.array([0.151, 0.129, 0.115, 0.102, 0.096]),\n            },\n            \"passive\": {\n                \"n\": np.array([50, 100, 200, 500, 1000]),\n                \"R\": np.array([0.190, 0.160, 0.139, 0.121, 0.112]),\n            },\n            \"epsilon\": 0.07,\n        },\n    ]\n\n    def learning_curve_model(n, alpha, beta, gamma):\n        \"\"\"Power-law-plus-plateau model for learning curves.\"\"\"\n        return alpha * n**(-beta) + gamma\n\n    def get_n_req(n_data, r_data, epsilon):\n        \"\"\"\n        Fits the learning curve model and calculates the required number of samples.\n        \"\"\"\n        # Heuristic initial guesses for parameters\n        gamma_guess = r_data[-1] if len(r_data) > 0 else 0\n        beta_guess = 0.5\n        alpha_guess = (r_data[0] - gamma_guess) * (n_data[0]**beta_guess) if len(r_data) > 0 else 1\n        p0 = [alpha_guess, beta_guess, gamma_guess]\n\n        # Parameter bounds: alpha >= 0, beta > 0, gamma >= 0\n        bounds = ([0, 1e-9, 0], [np.inf, np.inf, np.inf])\n\n        try:\n            params, _ = curve_fit(\n                learning_curve_model,\n                n_data,\n                r_data,\n                p0=p0,\n                bounds=bounds,\n                maxfev=5000\n            )\n        except RuntimeError:\n            # If fit fails, assume it's impossible to determine n_req\n            return np.inf\n\n        alpha, beta, gamma = params\n\n        # If target epsilon is below or at the irreducible error plateau, it's unreachable.\n        if epsilon = gamma:\n            return np.inf\n\n        # Handle the case where the numerator or the entire base might be problematic\n        base = alpha / (epsilon - gamma)\n        if base = 0:\n             # This can happen due to numerical precision issues if alpha is near zero\n             # or epsilon is extremely close to gamma. Treat as unreachable.\n            return np.inf\n\n        # Calculate required sample size n, ensuring it's at least 1.\n        n_calc = base**(1 / beta)\n        n_required = np.ceil(n_calc)\n        \n        return max(1, n_required)\n\n    results = []\n    for case in test_cases:\n        n_req_active = get_n_req(case[\"active\"][\"n\"], case[\"active\"][\"R\"], case[\"epsilon\"])\n        n_req_passive = get_n_req(case[\"passive\"][\"n\"], case[\"passive\"][\"R\"], case[\"epsilon\"])\n\n        is_active_inf = np.isinf(n_req_active)\n        is_passive_inf = np.isinf(n_req_passive)\n\n        if is_active_inf and is_passive_inf:\n            savings = np.nan\n        elif is_passive_inf:\n            savings = np.inf\n        elif is_active_inf:\n            savings = -np.inf\n        else:\n            savings = int(n_req_passive - n_req_active)\n        \n        results.append(savings)\n\n    def format_result(val):\n        if np.isnan(val):\n            return \"nan\"\n        elif np.isposinf(val):\n            return \"inf\"\n        elif np.isneginf(val):\n            return \"-inf\"\n        else:\n            return str(int(val))\n\n    formatted_results = [format_result(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}