{
    "hands_on_practices": [
        {
            "introduction": "现代推荐系统正从预测用户评分转向更实用的任务：预测用户的偏好排序。这个实践将指导你从零开始构建一个完整的成对排序模型。你将学习如何将原始的评分数据转化为偏序约束，并使用铰链损失（hinge loss）和随机梯度下降（SGD）来学习用户和物品的隐因子表示，这是构建现代协同过滤系统的基本功。",
            "id": "3110042",
            "problem": "要求您实现一个成对排序协同过滤算法，该算法强制执行从观察到的用户评分中派生出的偏序约束。背景是推荐系统中的统计学习。您将使用一个隐因子模型和一个成对铰链损失函数，从观察到的评分中学习用户和物品的嵌入向量，然后评估学习到的模型在多大程度上满足观察到的排序约束。\n\n实现一个程序，对每个测试用例，从基本原理出发执行以下操作：\n- 将用户-物品评分矩阵中的条目视为序数偏好的观测值。缺失条目编码为 $0$ 且不携带任何信息。\n- 对于每个用户 $u$，构建所有满足 $R_{u j} > R_{u k}$ 的有序物品对 $(j,k)$，其中 $R_{u j}$ 表示用户 $u$ 对物品 $j$ 的观察评分。每个这样的三元组 $(u,j,k)$ 构成一个严格偏序约束，模型的目标是通过 $\\hat{R}_{u j} > \\hat{R}_{u k}$ 来满足它。\n- 使用一个 $k$ 维隐因子模型，其预测分数为 $\\hat{R}_{u i} = p_u^\\top q_i$，其中 $p_u \\in \\mathbb{R}^k$ 是用户 $u$ 的隐向量，$q_i \\in \\mathbb{R}^k$ 是物品 $i$ 的隐向量。使用来自小标准差的零均值正态分布的独立样本来初始化所有隐向量。\n- 将经验风险定义为带有固定正间隔 $\\gamma$ 和 $\\ell_2$ 正则化的成对铰链损失。也就是说，对于每个约束 $(u,j,k)$，其损失贡献为 $\\max\\left(0,\\ \\gamma - \\left(p_u^\\top q_j - p_u^\\top q_k\\right)\\right)$，并且总损失加上 $\\frac{\\lambda}{2}\\left(\\lVert P\\rVert_F^2 + \\lVert Q\\rVert_F^2\\right)$ 用于正则化，其中 $P$ 和 $Q$ 分别是用户因子和物品因子堆叠而成的矩阵。\n- 使用随机梯度下降（SGD）优化参数，其中，仅当相关的间隔违例为正时，才应用铰链项的次梯度，而 $\\ell_2$ 正则化在整个训练过程中都应用。确保在每个轮次中使用固定的随机种子来随机化约束的顺序，以使结果可复现。必须明确地实现随机梯度下降（SGD）；不要依赖外部优化例程。\n- 训练结束后，计算排序准确率，即在测试用例的所有已构建约束中，被满足的约束所占的比例。一个约束 $(u,j,k)$ 被满足当且仅当 $\\hat{R}_{u j} - \\hat{R}_{u k} > 0$。如果一个用户没有贡献任何约束（例如，由于只有一个评分或评分相同），则其对总分母贡献零个约束。如果一个测试用例产生零个总约束，则按照惯例将准确率定义为 $1.0$，以避免除以零。预测中的平局，即 $\\hat{R}_{u j} - \\hat{R}_{u k} = 0$，应被视为未满足。\n\n您的程序必须处理以下三个测试用例。在每个用例中，评分为集合 $\\{1,2,3,4,5\\}$ 中的整数，缺失条目编码为 $0$。对所有测试用例使用相同的超参数：隐维度 $k = 3$，学习率 $\\eta = 0.05$，正则化系数 $\\lambda = 0.01$，间隔 $\\gamma = 1.0$，轮次数 $T = 200$，以及用于初始化和每轮约束排序的随机种子 $s = 42$。\n\n测试用例 1：\n- 形状为 $3 \\times 4$ 的评分矩阵 $R^{(1)}$：\n  - 第 0 行：$[5,3,0,1]$\n  - 第 1 行：$[0,4,2,0]$\n  - 第 2 行：$[2,0,5,4]$\n\n测试用例 2：\n- 形状为 $3 \\times 4$ 的评分矩阵 $R^{(2)}$：\n  - 第 0 行：$[3,3,1,0]$ (评分相等的物品之间不形成约束)\n  - 第 1 行：$[0,0,5,0]$ (单个评分不产生约束)\n  - 第 2 行：$[4,4,4,4]$ (所有评分相等不产生约束)\n\n测试用例 3：\n- 形状为 $4 \\times 5$ 的评分矩阵 $R^{(3)}$：\n  - 第 0 行：$[1,0,4,0,2]$\n  - 第 1 行：$[0,5,0,3,0]$\n  - 第 2 行：$[2,0,0,0,1]$\n  - 第 3 行：$[0,4,1,0,0]$\n\n需要遵守的实现细节：\n- 对于每个用户 $u$，仅使用满足 $R_{u j} > R_{u k}$ 的物品对 $(j,k)$ 从 $R$ 构建约束集；不要从评分相等的项创建约束。\n- 使用指定的隐因子模型和带有间隔 $\\gamma$ 和系数为 $\\lambda$ 的 $\\ell_2$ 正则化的铰链损失函数。\n- 使用SGD实现训练，对约束集进行 $T$ 次完整遍历。使用固定的种子 $s$ 来初始化参数并在每个轮次中打乱约束顺序，以确保可复现性。\n- 对每个测试用例，计算最终的排序准确率，结果为 $[0,1]$ 区间内的浮点数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、由逗号分隔的三个浮点数列表，按测试用例的顺序排列，每个浮点数格式化到小数点后恰好四位（例如，$[0.9750,0.5000,1.0000]$）。",
            "solution": "用户-物品评分矩阵被视为序数偏好数据的来源。该问题要求使用隐因子模型实现一个成对排序协同过滤算法。模型的参数是通过随机梯度下降（SGD）优化一个特定的目标函数来学习的。\n\n### 1. 模型与目标函数\n\n该模型的核心是学习每个用户和物品的隐因子表示。对于一个用户 $u$ 和一个物品 $i$，它们各自的隐向量是 $p_u \\in \\mathbb{R}^k$ 和 $q_i \\in \\mathbb{R}^k$，其中 $k$ 是隐空间的维度。用户 $u$ 对物品 $i$ 的预测偏好分数由点积给出：\n$$\n\\hat{R}_{ui} = p_u^\\top q_i\n$$\n学习过程不直接预测评分值，而是旨在保持数据中观察到的偏好相对顺序。对于每个用户 $u$，每当观察到的一对物品 $(j,k)$ 的评分严格满足 $R_{uj} > R_{uk}$ 时，就生成一个严格偏序约束 $(u,j,k)$。所有此类约束的集合表示为 $\\mathcal{C}$。\n\n模型通过最小化一个由带有 $\\ell_2$ 正则化的成对铰链损失定义的经验风险函数来进行训练。对于每个约束 $(u,j,k) \\in \\mathcal{C}$，模型应预测 $\\hat{R}_{uj} > \\hat{R}_{uk}$。损失函数惩罚对此排序的违反，特别是在 $\\hat{R}_{uj} - \\hat{R}_{uk}$ 不大于指定间隔 $\\gamma$ 时。需要最小化的总目标函数 $L$ 是：\n$$\nL(P, Q) = \\sum_{(u,j,k) \\in \\mathcal{C}} \\max\\left(0, \\gamma - (\\hat{R}_{uj} - \\hat{R}_{uk})\\right) + \\frac{\\lambda}{2}\\left(\\lVert P \\rVert_F^2 + \\lVert Q \\rVert_F^2\\right)\n$$\n这里，$P$ 和 $Q$ 分别是通过堆叠所有用户向量 $p_u$ 和物品向量 $q_i$ 形成的矩阵。项 $\\lVert \\cdot \\rVert_F^2$ 代表弗罗贝尼乌斯范数的平方，即 $\\sum_u \\lVert p_u \\rVert_2^2 + \\sum_i \\lVert q_i \\rVert_2^2$。超参数 $\\lambda > 0$ 控制正则化的强度，通过惩罚大的参数值来防止过拟合。间隔给定为 $\\gamma=1.0$。\n\n### 2. 算法实现\n\n#### 2.1. 约束生成\n首先，从输入评分矩阵 $R$ 中提取偏好约束集 $\\mathcal{C}$。对于每个用户 $u$，我们识别出他们已评分的所有物品（评分非零）。然后，对于这些已评分物品的每个有序对，比如物品 $j$ 和物品 $k$，如果 $R_{uj} > R_{uk}$，则将三元组 $(u,j,k)$ 添加到 $\\mathcal{C}$ 中。平局（$R_{uj} = R_{uk}$）和缺失评分不贡献约束。\n\n#### 2.2. 参数初始化\n初始化大小为 $N_u \\times k$ 的用户因子矩阵 $P$ 和大小为 $N_i \\times k$ 的物品因子矩阵 $Q$。$N_u$ 是用户数量，$N_i$ 是物品数量。每个条目都从一个零均值正态分布中抽取，该分布具有一个小的标准差（$0.1$ 是一个合适的选择）。使用一个固定的随机种子（$s=42$）来确保可复现性。\n\n#### 2.3. 随机梯度下降（SGD）优化\n通过在训练数据上迭代固定轮次（$T=200$），来优化模型参数 $P$ 和 $Q$。在每个轮次中，约束集 $\\mathcal{C}$ 被随机打乱（使用相同的带种子的随机数生成器）以确保无偏更新。然后，算法遍历 $\\mathcal{C}$ 中的每个约束 $(u,j,k)$ 并执行一次 SGD 更新。\n\n对于单个约束 $(u,j,k)$，计算目标函数关于相关参数 $p_u$、$q_j$ 和 $q_k$ 的梯度。设 $x_{ujk} = p_u^\\top q_j - p_u^\\top q_k$。指示函数 $\\mathbb{I}[\\gamma - x_{ujk} > 0]$ 在间隔被违反时为 $1$，否则为 $0$。梯度为：\n$$\n\\nabla_{p_u} L = \\lambda p_u + \\mathbb{I}[\\gamma - x_{ujk} > 0] \\cdot (q_k - q_j)\n$$\n$$\n\\nabla_{q_j} L = \\lambda q_j + \\mathbb{I}[\\gamma - x_{ujk} > 0] \\cdot (-p_u)\n$$\n$$\n\\nabla_{q_k} L = \\lambda q_k + \\mathbb{I}[\\gamma - x_{ujk} > 0] \\cdot (p_u)\n$$\n然后，通过沿负梯度方向迈出一小步来更新参数，步长由学习率 $\\eta=0.05$ 缩放：\n$$\np_u \\leftarrow p_u - \\eta \\nabla_{p_u} L\n$$\n$$\nq_j \\leftarrow q_j - \\eta \\nabla_{q_j} L\n$$\n$$\nq_k \\leftarrow q_k - \\eta \\nabla_{q_k} L\n$$\n为确保正确更新，该步骤中所有梯度计算都必须使用步骤开始时的 $p_u$、$q_j$ 和 $q_k$ 的值。\n\n### 3. 评估\n经过 $T$ 轮训练后，使用排序准确率来评估模型的性能。该指标是 $\\mathcal{C}$ 中被学习模型正确满足的约束的比例。如果物品 $j$ 的预测分数严格大于物品 $k$ 的预测分数，即 $\\hat{R}_{uj} - \\hat{R}_{uk} > 0$，则认为约束 $(u,j,k)$ 被满足。平局（$\\hat{R}_{uj} - \\hat{R}_{uk} = 0$）被计为未满足。准确率计算如下：\n$$\n\\text{准确率} = \\frac{|\\{(u,j,k) \\in \\mathcal{C} \\mid p_u^\\top q_j - p_u^\\top q_k > 0\\}|}{|\\mathcal{C}|}\n$$\n如果一个测试用例生成了零个约束（$|\\mathcal{C}| = 0$），则按惯例将准确率定义为 $1.0$。",
            "answer": "```python\nimport numpy as np\n\ndef run_pairwise_ranking(R, k, eta, lamb, gamma, T, seed):\n    \"\"\"\n    Implements and evaluates the pairwise-ranking collaborative filtering algorithm.\n\n    Args:\n        R (np.ndarray): The user-item rating matrix.\n        k (int): The number of latent dimensions.\n        eta (float): The learning rate for SGD.\n        lamb (float): The regularization coefficient.\n        gamma (float): The margin for the hinge loss.\n        T (int): The number of training epochs.\n        seed (int): The random seed for reproducibility.\n\n    Returns:\n        float: The final ranking accuracy.\n    \"\"\"\n    num_users, num_items = R.shape\n\n    # Step 1: Construct partial order constraints from the rating matrix.\n    # A constraint is a tuple (user_id, preferred_item_id, less_preferred_item_id).\n    constraints = []\n    for u in range(num_users):\n        rated_items_indices = np.where(R[u, :] > 0)[0]\n        # Generate pairs of items rated by the same user\n        for i in range(len(rated_items_indices)):\n            for j in range(len(rated_items_indices)):\n                if i == j:\n                    continue\n                item_j = rated_items_indices[i]\n                item_k = rated_items_indices[j]\n                # Add constraint if a strict preference is observed\n                if R[u, item_j] > R[u, item_k]:\n                    constraints.append((u, item_j, item_k))\n\n    # If there are no constraints, accuracy is 1.0 by definition.\n    if not constraints:\n        return 1.0\n\n    # Step 2: Initialize parameters (user and item latent factor matrices).\n    # The same RNG is used for initialization and shuffling for reproducibility.\n    rng = np.random.default_rng(seed)\n    std_dev = 0.1  # A small standard deviation for initialization\n    P = rng.normal(loc=0.0, scale=std_dev, size=(num_users, k))\n    Q = rng.normal(loc=0.0, scale=std_dev, size=(num_items, k))\n\n    # Step 3: Train using Stochastic Gradient Descent (SGD).\n    for epoch in range(T):\n        rng.shuffle(constraints)  # Shuffle constraints for each epoch\n        for u, j, k_item in constraints:\n            # Get current factors. Make copies to ensure simultaneous updates.\n            p_u = P[u, :].copy()\n            q_j = Q[j, :].copy()\n            q_k = Q[k_item, :].copy()\n\n            # Calculate the difference in predicted scores\n            pred_diff = np.dot(p_u, q_j) - np.dot(p_u, q_k)\n\n            # Check for margin violation to determine the hinge loss gradient part\n            if gamma - pred_diff > 0:\n                grad_pu_hinge = q_k - q_j\n                grad_qj_hinge = -p_u\n                grad_qk_hinge = p_u\n            else:\n                grad_pu_hinge = 0.0\n                grad_qj_hinge = 0.0\n                grad_qk_hinge = 0.0\n            \n            # Update parameters by taking a step against the combined gradient\n            # (regularization + hinge loss)\n            P[u, :] -= eta * (lamb * p_u + grad_pu_hinge)\n            Q[j, :] -= eta * (lamb * q_j + grad_qj_hinge)\n            Q[k_item, :] -= eta * (lamb * q_k + grad_qk_hinge)\n\n    # Step 4: Evaluate ranking accuracy on the set of constraints.\n    satisfied_count = 0\n    for u, j, k_item in constraints:\n        # A constraint is satisfied if the predicted preference order matches the observed one.\n        pred_diff = np.dot(P[u, :], Q[j, :]) - np.dot(P[u, :], Q[k_item, :])\n        if pred_diff > 0:\n            satisfied_count += 1\n            \n    accuracy = satisfied_count / len(constraints)\n    return accuracy\n\ndef solve():\n    \"\"\"\n    Main function to run the algorithm on all test cases and print the results.\n    \"\"\"\n    # Define hyperparameters as specified in the problem\n    k = 3\n    eta = 0.05\n    lamb = 0.01  # `lambda` is a keyword, so `lamb` is used\n    gamma = 1.0\n    T = 200\n    seed = 42\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        np.array([\n            [5, 3, 0, 1],\n            [0, 4, 2, 0],\n            [2, 0, 5, 4]\n        ], dtype=np.float64),\n        np.array([\n            [3, 3, 1, 0],\n            [0, 0, 5, 0],\n            [4, 4, 4, 4]\n        ], dtype=np.float64),\n        np.array([\n            [1, 0, 4, 0, 2],\n            [0, 5, 0, 3, 0],\n            [2, 0, 0, 0, 1],\n            [0, 4, 1, 0, 0]\n        ], dtype=np.float64)\n    ]\n\n    results = []\n    for R_case in test_cases:\n        accuracy = run_pairwise_ranking(R_case, k, eta, lamb, gamma, T, seed)\n        # Format the result to exactly four decimal places\n        results.append(f\"{accuracy:.4f}\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在你构建了像上一个练习中的排序模型之后，理解其学习过程的数学原理至关重要。这个练习将带你深入探讨模型优化的核心引擎：梯度计算。你将为贝叶斯个性化排序（Bayesian Personalized Ranking, BPR）的损失函数推导梯度，并通过一个具体的数值例子来验证你的结果，从而加深对基于梯度的优化方法如何驱动模型参数学习的理解。",
            "id": "3110073",
            "problem": "一个推荐系统使用贝叶斯个性化排序 (BPR) 算法，该算法使用逻辑斯蒂链接对成对偏好进行建模：对于一个用户 $i$ 和项目 $j$、$k$，用户 $i$ 偏好项目 $j$ 胜过项目 $k$ 的概率是 $p\\big((i,j,k)\\big) = \\sigma\\!\\big(\\Delta_{ijk}\\big)$，其中 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$ 且 $\\Delta_{ijk} = u_i^{\\top}(v_j - v_k)$。训练目标是正则化的负对数似然\n$$\n\\mathcal{L}(U,V) = - \\sum_{(i,j,k) \\in \\mathcal{S}} \\ln\\!\\big(\\sigma(\\Delta_{ijk})\\big) + \\frac{\\lambda}{2} \\sum_{i} \\|u_i\\|_2^2 + \\frac{\\lambda}{2} \\sum_{\\ell} \\|v_{\\ell}\\|_2^2,\n$$\n其中 $U = \\{u_i\\}$ 是用户嵌入，$V = \\{v_{\\ell}\\}$ 是项目嵌入，$\\lambda > 0$ 是一个正则化系数，$\\mathcal{S}$ 是观测到的用户-项目正负对的集合。\n\n从这个定义和逻辑斯蒂函数的性质出发，推导对于一个固定用户 $i$ 的梯度 $\\nabla_{u_i} \\mathcal{L}$，它应是 $\\{(j,k) \\in \\mathcal{S}_i\\}$、用户嵌入 $u_i$ 以及项目嵌入 $v_j$ 和 $v_k$ 的函数。然后，对于一个具体的实例，其中嵌入维度 $d = 3$，用户 $i$ 有两个观测对 $\\mathcal{S}_i = \\{(j_1,k_1),(j_2,k_2)\\}$，正则化系数 $\\lambda = 0.1$，且\n$$\nu_i = \\begin{pmatrix} 0.5 \\\\ -1.0 \\\\ 0.75 \\end{pmatrix}, \\quad\nv_{j_1} = \\begin{pmatrix} 0.2 \\\\ 0.0 \\\\ -0.1 \\end{pmatrix}, \\quad\nv_{k_1} = \\begin{pmatrix} -0.3 \\\\ 0.4 \\\\ 0.0 \\end{pmatrix},\n$$\n$$\nv_{j_2} = \\begin{pmatrix} 1.0 \\\\ -0.5 \\\\ 0.25 \\end{pmatrix}, \\quad\nv_{k_2} = \\begin{pmatrix} -0.5 \\\\ 0.2 \\\\ 0.1 \\end{pmatrix},\n$$\n计算欧几里得范数 $\\|\\nabla_{u_i} \\mathcal{L}\\|_2$。将你的最终数值答案四舍五入到四位有效数字。将你的最终答案表示为一个无单位的纯数。",
            "solution": "该问题要求首先推导贝叶斯个性化排序 (BPR) 损失函数关于用户嵌入的梯度，然后针对一个具体的数值实例计算该梯度的欧几里得范数。\n\nBPR 目标函数如下所示：\n$$\n\\mathcal{L}(U,V) = - \\sum_{(i,j,k) \\in \\mathcal{S}} \\ln\\!\\big(\\sigma(\\Delta_{ijk})\\big) + \\frac{\\lambda}{2} \\sum_{i} \\|u_i\\|_2^2 + \\frac{\\lambda}{2} \\sum_{\\ell} \\|v_{\\ell}\\|_2^2\n$$\n其中 $\\Delta_{ijk} = u_i^{\\top}(v_j - v_k)$ 且 $\\sigma(x) = (1 + \\exp(-x))^{-1}$。\n\n我们被要求找出对于一个固定用户 $i$ 的梯度 $\\nabla_{u_i} \\mathcal{L}$。当对特定用户嵌入 $u_i$ 求导时，我们只需考虑 $\\mathcal{L}$ 中依赖于 $u_i$ 的项。这些是涉及用户 $i$ 的偏好项和针对 $u_i$ 的正则化项。令 $\\mathcal{S}_i$ 为用户 $i$ 的观测对集合，即 $\\mathcal{S}_i = \\{(j,k) | (i,j,k) \\in \\mathcal{S}\\}$。\n\n损失函数中依赖于 $u_i$ 的部分，记作 $\\mathcal{L}_i$，是：\n$$\n\\mathcal{L}_i(u_i) = - \\sum_{(j,k) \\in \\mathcal{S}_i} \\ln\\!\\big(\\sigma(u_i^{\\top}(v_j - v_k))\\big) + \\frac{\\lambda}{2} \\|u_i\\|_2^2\n$$\n梯度为 $\\nabla_{u_i} \\mathcal{L} = \\nabla_{u_i} \\mathcal{L}_i(u_i)$。我们可以分别计算每一项的梯度。\n\n正则化项的梯度是一个标准结果：\n$$\n\\nabla_{u_i} \\left(\\frac{\\lambda}{2} \\|u_i\\|_2^2\\right) = \\nabla_{u_i} \\left(\\frac{\\lambda}{2} u_i^{\\top}u_i\\right) = \\lambda u_i\n$$\n\n对于负对数似然项，我们关注单个三元组 $(i,j,k)$。我们使用链式法则来求 $\\ln(\\sigma(\\Delta_{ijk}))$ 的梯度。\n$$\n\\nabla_{u_i} \\ln(\\sigma(\\Delta_{ijk})) = \\frac{d}{d\\sigma} \\ln(\\sigma) \\cdot \\frac{d\\sigma}{d\\Delta_{ijk}} \\cdot \\nabla_{u_i} \\Delta_{ijk}\n$$\n导数如下：\n$1$. $\\frac{d}{d\\sigma} \\ln(\\sigma) = \\frac{1}{\\sigma(\\Delta_{ijk})}$。\n$2$. 逻辑斯蒂函数的导数是 $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$。因此，$\\frac{d\\sigma}{d\\Delta_{ijk}} = \\sigma(\\Delta_{ijk})(1 - \\sigma(\\Delta_{ijk}))$。\n$3$. $\\Delta_{ijk}$ 是 $u_i$ 的线性函数。对于 $\\Delta_{ijk} = u_i^{\\top}(v_j - v_k)$，其梯度为 $\\nabla_{u_i} \\Delta_{ijk} = v_j - v_k$。\n\n结合这些结果：\n$$\n\\nabla_{u_i} \\ln(\\sigma(\\Delta_{ijk})) = \\frac{1}{\\sigma(\\Delta_{ijk})} \\cdot \\sigma(\\Delta_{ijk})(1 - \\sigma(\\Delta_{ijk})) \\cdot (v_j - v_k) = (1 - \\sigma(\\Delta_{ijk}))(v_j - v_k)\n$$\n使用性质 $1 - \\sigma(x) = \\frac{1+\\exp(-x)-1}{1+\\exp(-x)} = \\frac{\\exp(-x)}{1+\\exp(-x)} = \\frac{1}{\\exp(x)+1} = \\sigma(-x)$，我们可以写出：\n$$\n\\nabla_{u_i} \\ln(\\sigma(\\Delta_{ijk})) = \\sigma(-\\Delta_{ijk})(v_j - v_k)\n$$\n现在，我们可以写出完整损失 $\\mathcal{L}_i$ 的梯度：\n$$\n\\nabla_{u_i} \\mathcal{L}_i(u_i) = \\nabla_{u_i} \\left( - \\sum_{(j,k) \\in \\mathcal{S}_i} \\ln(\\sigma(\\Delta_{ijk})) + \\frac{\\lambda}{2} \\|u_i\\|_2^2 \\right)\n$$\n$$\n\\nabla_{u_i} \\mathcal{L} = - \\sum_{(j,k) \\in \\mathcal{S}_i} \\sigma(-\\Delta_{ijk})(v_j - v_k) + \\lambda u_i\n$$\n这是关于用户嵌入 $u_i$ 的梯度的通用表达式。\n\n接下来，我们为所提供的具体实例计算这个梯度。\n参数如下：\n$\\lambda = 0.1$。\n$u_i = \\begin{pmatrix} 0.5 \\\\ -1.0 \\\\ 0.75 \\end{pmatrix}$。\n用户 $i$ 的配对集合是 $\\mathcal{S}_i = \\{(j_1,k_1),(j_2,k_2)\\}$，所以求和中有两项。\n项目嵌入如下：\n$v_{j_1} = \\begin{pmatrix} 0.2 \\\\ 0.0 \\\\ -0.1 \\end{pmatrix}, v_{k_1} = \\begin{pmatrix} -0.3 \\\\ 0.4 \\\\ 0.0 \\end{pmatrix}$。\n$v_{j_2} = \\begin{pmatrix} 1.0 \\\\ -0.5 \\\\ 0.25 \\end{pmatrix}, v_{k_2} = \\begin{pmatrix} -0.5 \\\\ 0.2 \\\\ 0.1 \\end{pmatrix}$。\n\n梯度是：\n$$\n\\nabla_{u_i} \\mathcal{L} = -\\sigma(-\\Delta_{ij_1k_1})(v_{j_1} - v_{k_1}) - \\sigma(-\\Delta_{ij_2k_2})(v_{j_2} - v_{k_2}) + \\lambda u_i\n$$\n\n对于第一对 $(j_1,k_1)$：\n$v_{j_1} - v_{k_1} = \\begin{pmatrix} 0.2 - (-0.3) \\\\ 0.0 - 0.4 \\\\ -0.1 - 0.0 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ -0.4 \\\\ -0.1 \\end{pmatrix}$。\n$\\Delta_{ij_1k_1} = u_i^{\\top}(v_{j_1} - v_{k_1}) = (0.5)(0.5) + (-1.0)(-0.4) + (0.75)(-0.1) = 0.25 + 0.4 - 0.075 = 0.575$。\n系数是 $\\sigma(-\\Delta_{ij_1k_1}) = \\sigma(-0.575) = \\frac{1}{1 + \\exp(0.575)} \\approx 0.360093$。\n\n对于第二对 $(j_2,k_2)$：\n$v_{j_2} - v_{k_2} = \\begin{pmatrix} 1.0 - (-0.5) \\\\ -0.5 - 0.2 \\\\ 0.25 - 0.1 \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ -0.7 \\\\ 0.15 \\end{pmatrix}$。\n$\\Delta_{ij_2k_2} = u_i^{\\top}(v_{j_2} - v_{k_2}) = (0.5)(1.5) + (-1.0)(-0.7) + (0.75)(0.15) = 0.75 + 0.7 + 0.1125 = 1.5625$。\n系数是 $\\sigma(-\\Delta_{ij_2k_2}) = \\sigma(-1.5625) = \\frac{1}{1 + \\exp(1.5625)} \\approx 0.173291$。\n\n正则化项是：\n$\\lambda u_i = 0.1 \\begin{pmatrix} 0.5 \\\\ -1.0 \\\\ 0.75 \\end{pmatrix} = \\begin{pmatrix} 0.05 \\\\ -0.1 \\\\ 0.075 \\end{pmatrix}$。\n\n现在我们组装梯度向量，记作 $g = \\nabla_{u_i} \\mathcal{L}$：\n$$\ng = -0.360093 \\begin{pmatrix} 0.5 \\\\ -0.4 \\\\ -0.1 \\end{pmatrix} - 0.173291 \\begin{pmatrix} 1.5 \\\\ -0.7 \\\\ 0.15 \\end{pmatrix} + \\begin{pmatrix} 0.05 \\\\ -0.1 \\\\ 0.075 \\end{pmatrix}\n$$\n梯度向量 $g = (g_1, g_2, g_3)^{\\top}$ 的分量是：\n$g_1 = - (0.360093)(0.5) - (0.173291)(1.5) + 0.05 = -0.1800465 - 0.2599365 + 0.05 = -0.389983$。\n$g_2 = - (0.360093)(-0.4) - (0.173291)(-0.7) - 0.1 = 0.1440372 + 0.1213037 - 0.1 = 0.1653409$。\n$g_3 = - (0.360093)(-0.1) - (0.173291)(0.15) + 0.075 = 0.0360093 - 0.02599365 + 0.075 = 0.08501565$。\n\n所以，梯度向量约等于：\n$$\n\\nabla_{u_i} \\mathcal{L} \\approx \\begin{pmatrix} -0.389983 \\\\ 0.165341 \\\\ 0.085016 \\end{pmatrix}\n$$\n最后，我们计算其欧几里得范数 $\\|\\nabla_{u_i} \\mathcal{L}\\|_2$：\n$$\n\\|\\nabla_{u_i} \\mathcal{L}\\|_2 = \\sqrt{g_1^2 + g_2^2 + g_3^2}\n$$\n$$\n\\|\\nabla_{u_i} \\mathcal{L}\\|_2 \\approx \\sqrt{(-0.389983)^2 + (0.165341)^2 + (0.085016)^2}\n$$\n$$\n\\|\\nabla_{u_i} \\mathcal{L}\\|_2 \\approx \\sqrt{0.152087 + 0.027338 + 0.007228} = \\sqrt{0.186653} \\approx 0.4320335\n$$\n使用更高精度的中间计算得出 $\\sqrt{0.186652072} \\approx 0.432032489$。四舍五入到四位有效数字得到 $0.4320$。",
            "answer": "$$\\boxed{0.4320}$$"
        },
        {
            "introduction": "一个模型的优劣取决于我们如何评估它，而天真的评估指标可能会产生误导。本章的最后一个实践聚焦于推荐系统评估中的一个关键现实问题：曝光偏差（exposure bias）。你将学习并实现一种强大的反事实评估技术——逆倾向得分（Inverse Propensity Scoring, IPS），来校正离线评估中的偏差，从而更准确地衡量推荐算法的真实性能。",
            "id": "3110057",
            "problem": "您的任务是设计一个程序，使用协同过滤评估原则来评估在位置相关的曝光偏差下的排名推荐。该场景包含多个用户，每个用户都会看到一个项目的排名列表。每个位置都有被看到的倾向性，只有被曝光的项目才可能被点击。目标是推导并实现一个曝光感知的排名质量估计量，该估计量能校正非均匀曝光，并将其与忽略曝光偏差的朴素估计量进行比较。\n\n从以下基本定义开始：\n\n- 折扣累积增益 (DCG) 通过对位置折扣后的增益求和来评估一个排名列表。对于一个列表长度为 $L$ 的用户、一个单调增益函数 $g(\\cdot)$ 和一个位置折扣 $d(r)$，$\\mathrm{DCG}$ 是对位置 $r \\in \\{1,\\dots,L\\}$ 的求和。使用标准的二元相关性增益 $g(y_r) = y_r$ 和对数折扣 $d(r) = 1/\\log_2(1 + r)$，其中 $\\log_2(\\cdot)$ 表示以 2 为底的对数。\n- 列表长度为 $L$ 时的归一化折扣累积增益 (NDCG) 是 $\\mathrm{DCG}$ 除以理想 DCG，其中理想 DCG (记作 $\\mathrm{IDCG}$) 是通过将项目按其真实相关性标签 $y_r \\in \\{0,1\\}$ 降序排序所能达到的 DCG。\n\n在离线协同过滤评估中，观测到的点击会受到曝光的影响。令 $e_r \\in \\{0,1\\}$ 表示位置 $r$ 是否被曝光，令 $c_r \\in \\{0,1\\}$ 为观测到的点击。假设点击遵循确定性关系 $c_r = e_r \\cdot y_r$（无额外噪声）。令 $\\hat{\\pi}_r \\in (0,1]$ 为位置 $r$ 处曝光的估计倾向性。在非均匀曝光下，使用通用的逆倾向性得分原则：对于任何对已曝光实例的求和，当曝光是独立的且建模正确时，通过 $1/\\hat{\\pi}_r$ 加权会得到目标策略下相应期望的无偏估计量。\n\n您的任务：\n\n1. 使用逆倾向性权重 $1/\\hat{\\pi}_r$ 推导位置折扣增益总和的曝光感知估计量，然后通过理想 DCG 进行归一化，以获得逆倾向性得分归一化折扣累积增益 (IPS-NDCG)。同时，定义忽略曝光偏差的朴素 NDCG，它直接使用观测到的点击而无需倾向性权重。\n2. 在用户级别实现这两个指标，并通过平均每个用户的 NDCG 值来跨用户聚合，为每个测试用例的每种方法生成一个单一的分数。\n3. 对于每个测试用例，计算 IPS-NDCG 和朴素 NDCG 之间的差值（IPS 减去朴素值）。您的程序应将这些差值输出为浮点数。\n\n测试套件规范：\n\n- 使用以下四个测试用例。在每个用例中，列表已经按排名顺序排列，因此位置 $r = 1,2,\\dots$ 遵循给定的数组。对于每个用户，都提供了真实相关性 $y$、曝光倾向性 $\\hat{\\pi}$ 和实际曝光 $e$ 的数组。观测到的点击定义为 $c_r = e_r \\cdot y_r$。所有数字必须严格按照给定的值处理。\n\n- 测试用例 $1$（单个用户，列表长度 $5$）：\n  - 真实相关性 $y$：$[0,1,1,1,1]$\n  - 倾向性 $\\hat{\\pi}$：$[0.95,0.8,0.5,0.2,0.05]$\n  - 曝光 $e$：$[1,1,1,0,0]$\n\n- 测试用例 $2$（两个用户，每个列表长度 $4$）：\n  - 用户 A：\n    - 真实相关性 $y^{(A)}$：$[1,0,1,0]$\n    - 倾向性 $\\hat{\\pi}^{(A)}$：$[0.9,0.7,0.4,0.2]$\n    - 曝光 $e^{(A)}$：$[1,1,0,0]$\n  - 用户 B：\n    - 真实相关性 $y^{(B)}$：$[0,1,1,1]$\n    - 倾向性 $\\hat{\\pi}^{(B)}$：$[0.95,0.6,0.3,0.1]$\n    - 曝光 $e^{(B)}$：$[1,1,1,0]$\n\n- 测试用例 $3$（单个用户，列表长度 $4$；未观测到点击的边缘情况）：\n  - 真实相关性 $y$：$[0,0,0,1]$\n  - 倾向性 $\\hat{\\pi}$：$[0.9,0.7,0.5,0.01]$\n  - 曝光 $e$：$[1,1,1,0]$\n\n- 测试用例 $4$（单个用户，列表长度 $5$；深层位置存在强位置偏差）：\n  - 真实相关性 $y$：$[0,0,1,0,1]$\n  - 倾向性 $\\hat{\\pi}$：$[0.98,0.9,0.4,0.2,0.02]$\n  - 曝光 $e$：$[1,1,1,0,1]$\n\n最终输出格式规范：\n\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个元素对应一个测试用例，是该测试用例中（如果适用）跨用户平均的浮点数差值 $\\Delta = \\mathrm{IPS}\\text{-}\\mathrm{NDCG} - \\mathrm{naïve}\\text{-}\\mathrm{NDCG}$。例如，包含四个测试用例的输出必须类似于 `[0.1234, -0.0567, 0.0000, 0.9876]`。",
            "solution": "该问题要求推导并实现两个排名评估指标：一个朴素的归一化折扣累积增益 (NDCG) 和一个使用逆倾向性得分 (IPS-NDCG) 的曝光感知版本，以评估在位置相关的曝光偏差下排名推荐的质量。\n\n### 步骤 1：排名指标的正式推导\n\n我们已知折扣累积增益 (DCG) 及其归一化版本 NDCG 的基本定义。设一个用户的排名列表长度为 $L$。列表中的位置由 $r \\in \\{1, 2, \\dots, L\\}$ 表示。\n\n增益函数是二元相关性 $g(y_r) = y_r$，其中 $y_r \\in \\{0, 1\\}$ 是位置 $r$ 处项目的真实相关性。\n位置折扣函数是 $d(r) = 1/\\log_2(1+r)$，其中 $\\log_2$ 是以 2 为底的对数。\n\n在离线评估场景中，我们不直接观测 $y_r$。相反，我们观测到点击 $c_r \\in \\{0, 1\\}$，它取决于相关性 $y_r$ 和该位置是否被曝光（由 $e_r \\in \\{0, 1\\}$ 指示）。点击被确定性地建模为 $c_r = e_r \\cdot y_r$。位置 $r$ 的估计曝光概率由倾向性得分 $\\hat{\\pi}_r \\in (0, 1]$ 给出。\n\n**理想折扣累积增益 (IDCG)**\n\n朴素 NDCG 和 IPS-NDCG 都由相同的量——理想折扣累积增益 (IDCG) 进行归一化。IDCG 代表了呈现给用户的项目集合可能获得的最大 DCG。它是通过将项目按其真实相关性 $y_r$ 降序排列，然后计算此理想排名的 DCG 来得到的。设 $y^{\\text{ideal}}$ 是真实相关性标签 $y$ 按降序排序后的向量。\n\n$$\n\\mathrm{IDCG} = \\sum_{r=1}^{L} d(r) \\cdot y^{\\text{ideal}}_r = \\sum_{r=1}^{L} \\frac{y^{\\text{ideal}}_r}{\\log_2(1+r)}\n$$\n\n如果所有项目都没有相关性（即 $\\sum y_r = 0$），则 $\\mathrm{IDCG} = 0$。在这种情况下，任何 NDCG 通常都定义为 $0$。\n\n**朴素 NDCG**\n\n朴素方法忽略曝光偏差，将观测到的点击 $c_r$ 视为真实的相关性标签。\n\n朴素折扣累积增益，$\\mathrm{DCG}_{\\text{naïve}}$，是折扣后观测点击的总和：\n$$\n\\mathrm{DCG}_{\\text{naïve}} = \\sum_{r=1}^{L} d(r) \\cdot c_r = \\sum_{r=1}^{L} \\frac{c_r}{\\log_2(1+r)}\n$$\n朴素归一化折扣累积增益，$\\mathrm{NDCG}_{\\text{naïve}}$，是该值经 IDCG 归一化后的结果：\n$$\n\\mathrm{NDCG}_{\\text{naïve}} = \\frac{\\mathrm{DCG}_{\\text{naïve}}}{\\mathrm{IDCG}}\n$$\n\n**逆倾向性得分 NDCG (IPS-NDCG)**\n\n为了校正曝光偏差，我们使用逆倾向性得分 (IPS) 原理。目标是获得真实 DCG 的无偏估计，即 $\\sum_{r=1}^{L} d(r) y_r$。这个总和可以通过对观测到的交互所产生的增益，用其观测（曝光）倾向性的倒数进行加权来估计。\n\n位置 $r$ 增益的基于 IPS 的估计量是 $\\frac{e_r y_r}{\\hat{\\pi}_r} = \\frac{c_r}{\\hat{\\pi}_r}$。如果倾向性模型是正确的（即 $\\hat{\\pi}_r$ 是真实的曝光概率 $P(e_r=1)$），那么这个估计量对于真实增益 $y_r$ 是无偏的。\n\n基于 IPS 的折扣累积增益，$\\mathrm{DCG}_{\\text{IPS}}$，是这些经倾向性加权的折扣增益的总和：\n$$\n\\mathrm{DCG}_{\\text{IPS}} = \\sum_{r=1}^{L} d(r) \\cdot \\frac{c_r}{\\hat{\\pi}_r} = \\sum_{r=1}^{L} \\frac{c_r}{\\hat{\\pi}_r \\log_2(1+r)}\n$$\n那么，IPS 归一化折扣累积增益，$\\mathrm{NDCG}_{\\text{IPS}}$，是：\n$$\n\\mathrm{NDCG}_{\\text{IPS}} = \\frac{\\mathrm{DCG}_{\\text{IPS}}}{\\mathrm{IDCG}}\n$$\n\n### 步骤 2：聚合与最终计算\n\n对于涉及多个用户的测试用例，指标是为每个用户单独计算，然后取平均值。设 $N$ 为一个测试用例中的用户数。最终报告的分数是：\n$$\n\\overline{\\mathrm{NDCG}}_{\\text{naïve}} = \\frac{1}{N} \\sum_{u=1}^{N} \\mathrm{NDCG}_{\\text{naïve}}^{(u)}\n$$\n$$\n\\overline{\\mathrm{NDCG}}_{\\text{IPS}} = \\frac{1}{N} \\sum_{u=1}^{N} \\mathrm{NDCG}_{\\text{IPS}}^{(u)}\n$$\n每个测试用例所需的输出是差值 $\\Delta$：\n$$\n\\Delta = \\overline{\\mathrm{NDCG}}_{\\text{IPS}} - \\overline{\\mathrm{NDCG}}_{\\text{naïve}}\n$$\n该过程将应用于四个指定的测试用例中的每一个。实现将使用数值数组来表示向量 $y$、$c$、$e$ 和 $\\hat{\\pi}$，并为提高效率执行向量化计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of evaluating ranked recommendations under position-dependent exposure bias.\n    Computes the difference between IPS-NDCG and naïve-NDCG for a suite of test cases.\n    \"\"\"\n\n    def calculate_ndcgs(y: np.ndarray, pi: np.ndarray, e: np.ndarray):\n        \"\"\"\n        Calculates naïve NDCG and IPS-NDCG for a single user's ranked list.\n\n        Args:\n            y (np.ndarray): Array of true relevance labels (0 or 1).\n            pi (np.ndarray): Array of estimated exposure propensities.\n            e (np.ndarray): Array of realized exposures (0 or 1).\n\n        Returns:\n            tuple[float, float]: A tuple containing (naïve_ndcg, ips_ndcg).\n        \"\"\"\n        L = len(y)\n        if L == 0:\n            return 0.0, 0.0\n\n        # Calculate observed clicks\n        c = e * y\n\n        # Calculate position discounts: d(r) = 1 / log2(r + 1)\n        # For positions r=1,...,L, we need log2(2),...,log2(L+1)\n        ranks = np.arange(1, L + 1)\n        discounts = 1.0 / np.log2(ranks + 1)\n\n        # Calculate naive DCG\n        dcg_naive = np.sum(c * discounts)\n\n        # Calculate IPS-based DCG\n        # Add a small epsilon to pi to avoid division by zero, although problem states pi > 0\n        dcg_ips = np.sum((c / pi) * discounts)\n\n        # Calculate Ideal DCG (IDCG)\n        y_ideal = np.sort(y)[::-1]\n        idcg = np.sum(y_ideal * discounts)\n\n        if idcg == 0:\n            # If there are no relevant items, NDCG is 0\n            return 0.0, 0.0\n\n        ndcg_naive = dcg_naive / idcg\n        ndcg_ips = dcg_ips / idcg\n\n        return ndcg_naive, ndcg_ips\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (single user)\n        [\n            {\n                \"y\": np.array([0, 1, 1, 1, 1]),\n                \"pi\": np.array([0.95, 0.8, 0.5, 0.2, 0.05]),\n                \"e\": np.array([1, 1, 1, 0, 0]),\n            }\n        ],\n        # Test Case 2 (two users)\n        [\n            { # User A\n                \"y\": np.array([1, 0, 1, 0]),\n                \"pi\": np.array([0.9, 0.7, 0.4, 0.2]),\n                \"e\": np.array([1, 1, 0, 0]),\n            },\n            { # User B\n                \"y\": np.array([0, 1, 1, 1]),\n                \"pi\": np.array([0.95, 0.6, 0.3, 0.1]),\n                \"e\": np.array([1, 1, 1, 0]),\n            }\n        ],\n        # Test Case 3 (single user, no clicks)\n        [\n            {\n                \"y\": np.array([0, 0, 0, 1]),\n                \"pi\": np.array([0.9, 0.7, 0.5, 0.01]),\n                \"e\": np.array([1, 1, 1, 0]),\n            }\n        ],\n        # Test Case 4 (single user, strong bias)\n        [\n            {\n                \"y\": np.array([0, 0, 1, 0, 1]),\n                \"pi\": np.array([0.98, 0.9, 0.4, 0.2, 0.02]),\n                \"e\": np.array([1, 1, 1, 0, 1]),\n            }\n        ],\n    ]\n\n    results = []\n    for case_data in test_cases:\n        num_users = len(case_data)\n        total_naive_ndcg = 0.0\n        total_ips_ndcg = 0.0\n\n        for user_data in case_data:\n            naive_ndcg, ips_ndcg = calculate_ndcgs(\n                user_data[\"y\"], user_data[\"pi\"], user_data[\"e\"]\n            )\n            total_naive_ndcg += naive_ndcg\n            total_ips_ndcg += ips_ndcg\n        \n        avg_naive_ndcg = total_naive_ndcg / num_users\n        avg_ips_ndcg = total_ips_ndcg / num_users\n        \n        difference = avg_ips_ndcg - avg_naive_ndcg\n        results.append(difference)\n\n    # Format the final output string as [res1,res2,res3,res4]\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}