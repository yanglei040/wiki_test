{
    "hands_on_practices": [
        {
            "introduction": "One of the most fundamental tasks when working with a Hidden Markov Model is to evaluate how well it explains a given sequence of observations. This exercise will guide you through calculating the probability of an observation sequence, which is a cornerstone skill for model comparison and validation. You will apply the Forward Algorithm, a classic dynamic programming technique that efficiently computes this likelihood by summing the probabilities of all possible hidden state paths that could have generated the observed data .",
            "id": "1306030",
            "problem": "An industrial sensor is used to monitor the quality of components on an assembly line. The sensor's status, which is not directly observable, can be modeled as a system with two hidden states: 'Operational' and 'Malfunctioning'. At each time step, which corresponds to the inspection of one component, the sensor either remains in its current state or transitions to the other. The sensor outputs one of two signals for each component it inspects: 'Pass' or 'Defect'.\n\nThe behavior of this system can be described by a Hidden Markov Model (HMM) defined by the following parameters:\n\nLet the set of hidden states be $S = \\{S_{\\text{Op}}, S_{\\text{Mal}}\\}$, where $S_{\\text{Op}}$ represents the 'Operational' state and $S_{\\text{Mal}}$ represents the 'Malfunctioning' state.\n\n1.  **Initial State Probabilities ($\\pi$):** At the beginning of the observation period (time $t=1$), the probabilities of the sensor being in each state are:\n    *   $P(S_{\\text{Op}}) = 0.95$\n    *   $P(S_{\\text{Mal}}) = 0.05$\n\n2.  **State Transition Probability Matrix ($A$):** This matrix gives the probability of transitioning from one state to another between consecutive time steps.\n    $$\n    A = \n    \\begin{pmatrix}\n    P(S_{t+1}=S_{\\text{Op}} | S_t=S_{\\text{Op}})  P(S_{t+1}=S_{\\text{Mal}} | S_t=S_{\\text{Op}}) \\\\\n    P(S_{t+1}=S_{\\text{Op}} | S_t=S_{\\text{Mal}})  P(S_{t+1}=S_{\\text{Mal}} | S_t=S_{\\text{Mal}})\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    0.98  0.02 \\\\\n    0.15  0.85\n    \\end{pmatrix}\n    $$\n\n3.  **Observation Emission Probability Matrix ($B$):** This matrix gives the probability of observing a particular signal given the sensor's current hidden state.\n    $$\n    B = \n    \\begin{pmatrix}\n    P(\\text{'Pass'} | S_{\\text{Op}})  P(\\text{'Defect'} | S_{\\text{Op}}) \\\\\n    P(\\text{'Pass'} | S_{\\text{Mal}})  P(\\text{'Defect'} | S_{\\text{Mal}})\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    0.99  0.01 \\\\\n    0.20  0.80\n    \\end{pmatrix}\n    $$\n\nAn engineer observes the sequence of signals from the sensor for three consecutive components: ('Pass', 'Pass', 'Defect').\n\nCalculate the total probability of observing this specific sequence. Round your final answer to four significant figures.",
            "solution": "We model the observation sequence using the forward algorithm. Let $\\alpha_{t}(s) = P(o_{1:t}, S_{t}=s)$, where $o_{1:t}$ denotes the observations from time $1$ to $t$, and $S_{t}$ is the hidden state at time $t$. The recursions are:\n$$\n\\alpha_{1}(s) = \\pi(s)\\, b_{s}(o_{1}), \\quad\n\\alpha_{t+1}(s') = \\left[\\sum_{s \\in S} \\alpha_{t}(s)\\, a_{s \\to s'}\\right] b_{s'}(o_{t+1}),\n$$\nand the total probability of the observation sequence $o_{1:3} = (\\text{Pass}, \\text{Pass}, \\text{Defect})$ is\n$$\nP(o_{1:3}) = \\sum_{s \\in S} \\alpha_{3}(s).\n$$\n\nInitial step for $t=1$ with $o_{1}=\\text{Pass}$:\n$$\n\\alpha_{1}(S_{\\text{Op}}) = \\pi(S_{\\text{Op}})\\, b_{S_{\\text{Op}}}(\\text{Pass}) = 0.95 \\cdot 0.99 = 0.9405,\n$$\n$$\n\\alpha_{1}(S_{\\text{Mal}}) = \\pi(S_{\\text{Mal}})\\, b_{S_{\\text{Mal}}}(\\text{Pass}) = 0.05 \\cdot 0.20 = 0.01.\n$$\n\nInduction for $t=2$ with $o_{2}=\\text{Pass}$:\n$$\n\\alpha_{2}(S_{\\text{Op}}) = \\left[\\alpha_{1}(S_{\\text{Op}})\\, a_{\\text{Op}\\to\\text{Op}} + \\alpha_{1}(S_{\\text{Mal}})\\, a_{\\text{Mal}\\to\\text{Op}}\\right] b_{S_{\\text{Op}}}(\\text{Pass})\n= \\left[0.9405 \\cdot 0.98 + 0.01 \\cdot 0.15\\right] \\cdot 0.99\n= 0.92319 \\cdot 0.99 = 0.9139581,\n$$\n$$\n\\alpha_{2}(S_{\\text{Mal}}) = \\left[\\alpha_{1}(S_{\\text{Op}})\\, a_{\\text{Op}\\to\\text{Mal}} + \\alpha_{1}(S_{\\text{Mal}})\\, a_{\\text{Mal}\\to\\text{Mal}}\\right] b_{S_{\\text{Mal}}}(\\text{Pass})\n= \\left[0.9405 \\cdot 0.02 + 0.01 \\cdot 0.85\\right] \\cdot 0.20\n= 0.02731 \\cdot 0.20 = 0.005462.\n$$\n\nInduction for $t=3$ with $o_{3}=\\text{Defect}$:\n$$\n\\alpha_{3}(S_{\\text{Op}}) = \\left[\\alpha_{2}(S_{\\text{Op}})\\, a_{\\text{Op}\\to\\text{Op}} + \\alpha_{2}(S_{\\text{Mal}})\\, a_{\\text{Mal}\\to\\text{Op}}\\right] b_{S_{\\text{Op}}}(\\text{Defect})\n= \\left[0.9139581 \\cdot 0.98 + 0.005462 \\cdot 0.15\\right] \\cdot 0.01\n= 0.896498238 \\cdot 0.01 = 0.00896498238,\n$$\n$$\n\\alpha_{3}(S_{\\text{Mal}}) = \\left[\\alpha_{2}(S_{\\text{Op}})\\, a_{\\text{Op}\\to\\text{Mal}} + \\alpha_{2}(S_{\\text{Mal}})\\, a_{\\text{Mal}\\to\\text{Mal}}\\right] b_{S_{\\text{Mal}}}(\\text{Defect})\n= \\left[0.9139581 \\cdot 0.02 + 0.005462 \\cdot 0.85\\right] \\cdot 0.80\n= 0.022921862 \\cdot 0.80 = 0.0183374896.\n$$\n\nTotal probability of the sequence is\n$$\nP(\\text{Pass}, \\text{Pass}, \\text{Defect}) = \\alpha_{3}(S_{\\text{Op}}) + \\alpha_{3}(S_{\\text{Mal}}) = 0.00896498238 + 0.0183374896 = 0.02730247198.\n$$\n\nRounding to four significant figures gives $0.02730$.",
            "answer": "$$\\boxed{0.02730}$$"
        },
        {
            "introduction": "After determining the likelihood of an observation sequence, a common next step is to infer what was happening \"behind the scenes.\" This practice focuses on the decoding problem: finding the single most likely sequence of hidden states that produced the observations. You will use the Viterbi algorithm, another powerful dynamic programming method, to trace this optimal path through the hidden states, providing a clear and practical understanding of how to uncover the unobservable processes modeled by an HMM .",
            "id": "1305999",
            "problem": "A biologist is studying the health of a specific, sensitive plant species on a remote, unmonitored island. The plant's health, observed daily, can be categorized as either `Fresh` or `Wilted`. The biologist hypothesizes that the plant's state is primarily determined by the unobserved atmospheric condition of the previous day, which can be either `Dry` or `Humid`.\n\nThis system is modeled as a Hidden Markov Model (HMM) with the following parameters:\n\n1.  **Initial State Probabilities**: The probability of the weather state on the first day of observation.\n    *   $P(\\text{Day 1 is Dry}) = 0.7$\n    *   $P(\\text{Day 1 is Humid}) = 0.3$\n\n2.  **Transition Probabilities**: The probability of transitioning from one weather state to another on consecutive days.\n    *   $P(\\text{Today is Dry} | \\text{Yesterday was Dry}) = 0.8$\n    *   $P(\\text{Today is Humid} | \\text{Yesterday was Dry}) = 0.2$\n    *   $P(\\text{Today is Dry} | \\text{Yesterday was Humid}) = 0.4$\n    *   $P(\\text{Today is Humid} | \\text{Yesterday was Humid}) = 0.6$\n\n3.  **Emission Probabilities**: The probability of observing the plant's condition given the weather state of that day.\n    *   $P(\\text{Plant is Wilted} | \\text{Weather is Dry}) = 0.9$\n    *   $P(\\text{Plant is Fresh} | \\text{Weather is Dry}) = 0.1$\n    *   $P(\\text{Plant is Wilted} | \\text{Weather is Humid}) = 0.2$\n    *   $P(\\text{Plant is Fresh} | \\text{Weather is Humid}) = 0.8$\n\nThe biologist receives a report of the plant's condition over a three-day period: `Fresh`, `Wilted`, `Wilted`.\n\nDetermine the most likely sequence of weather states (Day 1, Day 2, Day 3) that would produce this sequence of observations.\n\nA. Humid, Dry, Dry\n\nB. Humid, Humid, Dry\n\nC. Dry, Dry, Dry\n\nD. Humid, Dry, Humid\n\nE. Dry, Humid, Dry",
            "solution": "We model the hidden weather states as $S=\\{\\text{D},\\text{H}\\}$ for Dry and Humid. The observation sequence over three days is $O=(\\text{Fresh},\\text{Wilted},\\text{Wilted})$. Initial probabilities are $\\pi(\\text{D})=0.7$, $\\pi(\\text{H})=0.3$. Transition probabilities are $a_{\\text{DD}}=0.8$, $a_{\\text{DH}}=0.2$, $a_{\\text{HD}}=0.4$, $a_{\\text{HH}}=0.6$. Emission probabilities are $b_{\\text{D}}(\\text{Fresh})=0.1$, $b_{\\text{D}}(\\text{Wilted})=0.9$, $b_{\\text{H}}(\\text{Fresh})=0.8$, $b_{\\text{H}}(\\text{Wilted})=0.2$.\n\nUse the Viterbi algorithm. Define\n$$\n\\delta_{t}(s)=\\max_{s_{1},\\ldots,s_{t-1}} P(s_{1},\\ldots,s_{t-1},s_{t}=s,\\; o_{1},\\ldots,o_{t}),\n$$\nwith recursion\n$$\n\\delta_{1}(s)=\\pi(s)\\,b_{s}(o_{1}),\\quad \\delta_{t}(s)=b_{s}(o_{t})\\max_{s'}\\left[\\delta_{t-1}(s')\\,a_{s's}\\right],\n$$\nand backpointers $\\psi_{t}(s)=\\arg\\max_{s'}\\left[\\delta_{t-1}(s')\\,a_{s's}\\right]$.\n\nInitialization for $t=1$ with $o_{1}=\\text{Fresh}$:\n$$\n\\delta_{1}(\\text{D})=\\pi(\\text{D})\\,b_{\\text{D}}(\\text{Fresh})=0.7\\cdot 0.1=0.07,\\quad\n\\delta_{1}(\\text{H})=\\pi(\\text{H})\\,b_{\\text{H}}(\\text{Fresh})=0.3\\cdot 0.8=0.24.\n$$\n\nInduction for $t=2$ with $o_{2}=\\text{Wilted}$:\n- For state D:\n$$\n\\delta_{2}(\\text{D})=b_{\\text{D}}(\\text{Wilted})\\max\\{\\delta_{1}(\\text{D})a_{\\text{DD}},\\,\\delta_{1}(\\text{H})a_{\\text{HD}}\\}\n=0.9\\max\\{0.07\\cdot 0.8,\\,0.24\\cdot 0.4\\}=0.9\\cdot 0.096=0.0864,\n$$\nso $\\psi_{2}(\\text{D})=\\text{H}$.\n- For state H:\n$$\n\\delta_{2}(\\text{H})=b_{\\text{H}}(\\text{Wilted})\\max\\{\\delta_{1}(\\text{D})a_{\\text{DH}},\\,\\delta_{1}(\\text{H})a_{\\text{HH}}\\}\n=0.2\\max\\{0.07\\cdot 0.2,\\,0.24\\cdot 0.6\\}=0.2\\cdot 0.144=0.0288,\n$$\nso $\\psi_{2}(\\text{H})=\\text{H}$.\n\nInduction for $t=3$ with $o_{3}=\\text{Wilted}$:\n- For state D:\n$$\n\\delta_{3}(\\text{D})=b_{\\text{D}}(\\text{Wilted})\\max\\{\\delta_{2}(\\text{D})a_{\\text{DD}},\\,\\delta_{2}(\\text{H})a_{\\text{HD}}\\}\n=0.9\\max\\{0.0864\\cdot 0.8,\\,0.0288\\cdot 0.4\\}=0.9\\cdot 0.06912=0.062208,\n$$\nso $\\psi_{3}(\\text{D})=\\text{D}$.\n- For state H:\n$$\n\\delta_{3}(\\text{H})=b_{\\text{H}}(\\text{Wilted})\\max\\{\\delta_{2}(\\text{D})a_{\\text{DH}},\\,\\delta_{2}(\\text{H})a_{\\text{HH}}\\}\n=0.2\\max\\{0.0864\\cdot 0.2,\\,0.0288\\cdot 0.6\\}=0.2\\cdot 0.01728=0.003456,\n$$\nwith a tie in the argmax; the backpointer choice here does not affect the final most likely path.\n\nTermination and backtrace:\n$$\n\\max\\{\\delta_{3}(\\text{D}),\\delta_{3}(\\text{H})\\}=0.062208 \\text{ at state D}.\n$$\nBacktracking: at $t=3$ the state is D; $\\psi_{3}(\\text{D})=\\text{D}$ gives state D at $t=2$; $\\psi_{2}(\\text{D})=\\text{H}$ gives state H at $t=1$. Thus, the most likely hidden sequence is Humid, Dry, Dry, which corresponds to option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The elegance of algorithms like the Forward and Viterbi can sometimes obscure the computational challenges they overcome. This problem demystifies the need for such efficiency by asking you to solve the evaluation problem in two ways: first by the brute-force method of enumerating every single possible hidden path, and then by using the much more efficient Forward Algorithm. This direct comparison  offers a visceral understanding of exponential complexity and powerfully illustrates the indispensable role of dynamic programming in making HMMs practical for sequences of any meaningful length.",
            "id": "3128474",
            "problem": "A discrete hidden Markov model (HMM) has hidden state set $\\{X, Y\\}$ and observation alphabet $\\{A, B\\}$. The initial state distribution is given by $\\pi(X) = 0.6$ and $\\pi(Y) = 0.4$. The state transition probabilities are $a_{XX} = 0.8$, $a_{XY} = 0.2$, $a_{YX} = 0.3$, and $a_{YY} = 0.7$. The emission probabilities are $b_X(A) = 0.5$, $b_X(B) = 0.5$, $b_Y(A) = 0.2$, and $b_Y(B) = 0.8$. Consider the observation sequence of length $3$, $(A, B, A)$.\n\nUsing only the foundational definitions of conditional independence in hidden Markov models (namely, the first-order Markov property for hidden states and conditional independence of observations given the current state), do the following:\n1. Compute the probability of the observation sequence $(A, B, A)$ by explicitly enumerating all hidden state sequences of length $3$ and summing their joint probabilities.\n2. Independently verify the same probability using the forward algorithm derived from those definitions, and determine, for this instance, the number of scalar multiplications and additions required by (i) full enumeration of all hidden paths and (ii) the forward algorithm.\n\nReport only the probability of $(A, B, A)$ for this HMM as your final numeric answer, rounded to four significant figures.",
            "solution": "The foundational assumptions for a hidden Markov model are:\n1.  **Markov Property**: The probability of the current hidden state depends only on the previous hidden state. $P(q_t|q_{t-1}, q_{t-2}, \\dots, q_1) = P(q_t|q_{t-1})$.\n2.  **Output Independence**: The probability of the current observation depends only on the current hidden state. $P(O_t|q_t, q_{t-1}, \\dots, q_1, O_{t-1}, \\dots, O_1) = P(O_t|q_t)$.\n\nThese assumptions allow the joint probability of an observation sequence $O=(O_1, \\dots, O_T)$ and a hidden state sequence $Q=(q_1, \\dots, q_T)$ to be expressed as:\n$$P(O,Q) = P(q_1) \\prod_{t=2}^{T} P(q_t|q_{t-1}) \\prod_{t=1}^{T} P(O_t|q_t) = \\pi_{q_1} b_{q_1}(O_1) \\prod_{t=2}^{T} a_{q_{t-1}q_t} b_{q_t}(O_t)$$\n\nThe probability of the observation sequence $O$ is obtained by summing this joint probability over all possible hidden state sequences $Q$:\n$$P(O) = \\sum_{\\text{all } Q} P(O,Q)$$\n\nWe are asked to compute $P(O=(A,B,A))$ for the given HMM.\n\n**Part 1: Computation by Full Enumeration**\nWith $N=2$ states and a sequence length of $T=3$, there are $N^T = 2^3 = 8$ possible hidden state sequences $Q = (q_1, q_2, q_3)$. We calculate the joint probability $P(O, Q)$ for each sequence and sum the results. The observation sequence is $O=(A,B,A)$.\n\nThe formula for each path is $P(O,Q) = \\pi_{q_1} b_{q_1}(A) a_{q_1q_2} b_{q_2}(B) a_{q_2q_3} b_{q_3}(A)$.\n\n1.  $Q = (X,X,X)$: $P(O,Q) = \\pi_X b_X(A) a_{XX} b_X(B) a_{XX} b_X(A) = (0.6)(0.5)(0.8)(0.5)(0.8)(0.5) = 0.048$\n2.  $Q = (X,X,Y)$: $P(O,Q) = \\pi_X b_X(A) a_{XX} b_X(B) a_{XY} b_Y(A) = (0.6)(0.5)(0.8)(0.5)(0.2)(0.2) = 0.0048$\n3.  $Q = (X,Y,X)$: $P(O,Q) = \\pi_X b_X(A) a_{XY} b_Y(B) a_{YX} b_X(A) = (0.6)(0.5)(0.2)(0.8)(0.3)(0.5) = 0.0072$\n4.  $Q = (X,Y,Y)$: $P(O,Q) = \\pi_X b_X(A) a_{XY} b_Y(B) a_{YY} b_Y(A) = (0.6)(0.5)(0.2)(0.8)(0.7)(0.2) = 0.00672$\n5.  $Q = (Y,X,X)$: $P(O,Q) = \\pi_Y b_Y(A) a_{YX} b_X(B) a_{XX} b_X(A) = (0.4)(0.2)(0.3)(0.5)(0.8)(0.5) = 0.0048$\n6.  $Q = (Y,X,Y)$: $P(O,Q) = \\pi_Y b_Y(A) a_{YX} b_X(B) a_{XY} b_Y(A) = (0.4)(0.2)(0.3)(0.5)(0.2)(0.2) = 0.00048$\n7.  $Q = (Y,Y,X)$: $P(O,Q) = \\pi_Y b_Y(A) a_{YY} b_Y(B) a_{YX} b_X(A) = (0.4)(0.2)(0.7)(0.8)(0.3)(0.5) = 0.00672$\n8.  $Q = (Y,Y,Y)$: $P(O,Q) = \\pi_Y b_Y(A) a_{YY} b_Y(B) a_{YY} b_Y(A) = (0.4)(0.2)(0.7)(0.8)(0.7)(0.2) = 0.006272$\n\nSumming these probabilities:\n$P(O) = 0.048 + 0.0048 + 0.0072 + 0.00672 + 0.0048 + 0.00048 + 0.00672 + 0.006272 = 0.084992$.\n\n**Part 2: Verification using the Forward Algorithm**\nThe forward algorithm provides a more efficient method derived from the same first principles. The forward variable $\\alpha_t(i)$ is defined as the joint probability of observing the first $t$ observations and being in state $S_i$ at time $t$: $\\alpha_t(i) = P(O_1, \\dots, O_t, q_t=S_i)$.\n\n- **Initialization ($t=1$):** Observation is $O_1=A$.\n$\\alpha_1(X) = \\pi_X b_X(O_1) = (0.6)(0.5) = 0.3$\n$\\alpha_1(Y) = \\pi_Y b_Y(O_1) = (0.4)(0.2) = 0.08$\n\n- **Recursion ($t=2$):** Observation is $O_2=B$.\nThe recursive step is $\\alpha_t(j) = \\left[ \\sum_{i=1}^{N} \\alpha_{t-1}(i) a_{ij} \\right] b_j(O_t)$.\n$\\alpha_2(X) = [\\alpha_1(X) a_{XX} + \\alpha_1(Y) a_{YX}] b_X(O_2) = [(0.3)(0.8) + (0.08)(0.3)] (0.5) = [0.24 + 0.024](0.5) = (0.264)(0.5) = 0.132$\n$\\alpha_2(Y) = [\\alpha_1(X) a_{XY} + \\alpha_1(Y) a_{YY}] b_Y(O_2) = [(0.3)(0.2) + (0.08)(0.7)] (0.8) = [0.06 + 0.056](0.8) = (0.116)(0.8) = 0.0928$\n\n- **Recursion ($t=3$):** Observation is $O_3=A$.\n$\\alpha_3(X) = [\\alpha_2(X) a_{XX} + \\alpha_2(Y) a_{YX}] b_X(O_3) = [(0.132)(0.8) + (0.0928)(0.3)] (0.5) = [0.1056 + 0.02784](0.5) = (0.13344)(0.5) = 0.06672$\n$\\alpha_3(Y) = [\\alpha_2(X) a_{XY} + \\alpha_2(Y) a_{YY}] b_Y(O_3) = [(0.132)(0.2) + (0.0928)(0.7)] (0.2) = [0.0264 + 0.06496](0.2) = (0.09136)(0.2) = 0.018272$\n\n- **Termination:**\nThe final probability is the sum of the final forward variables:\n$P(O) = \\alpha_3(X) + \\alpha_3(Y) = 0.06672 + 0.018272 = 0.084992$.\nThe result matches the one obtained from full enumeration, which serves as a verification.\n\n**Part 3: Complexity Comparison**\n\n- **(i) Full Enumeration:**\n  For each of the $N^T$ possible hidden state sequences, the calculation of its joint probability involves $(T-1)$ transition probability multiplications and $T$ emission probability multiplications, for a total of $2T-1$ multiplications per sequence. The sum of these $N^T$ probabilities requires $N^T-1$ additions.\n  For $N=2$ and $T=3$:\n  - Multiplications: $N^T (2T-1) = 2^3 (2 \\cdot 3 - 1) = 8 \\times 5 = 40$.\n  - Additions: $N^T - 1 = 2^3 - 1 = 7$.\n\n- **(ii) Forward Algorithm:**\n  The complexity of the forward algorithm is dominated by the recursive step, which is performed $T-1$ times. In each step, for each of the $N$ states, we compute a sum of $N$ terms (requiring $N$ multiplications and $N-1$ additions) and then perform one more multiplication. This gives $N(N)$ multiplications and $N(N-1)$ additions per time step, plus $N$ multiplications for the emission probabilities. The initialization step requires $N$ multiplications. The termination step requires $N-1$ additions. The total complexity is on the order of $O(N^2 T)$.\n  For $N=2$ and $T=3$:\n  - Multiplications:\n    - $t=1$: $N=2$ multiplications.\n    - $t=2$: $N(N) + N = 2(2) + 2 = 6$ multiplications.\n    - $t=3$: $N(N) + N = 2(2) + 2 = 6$ multiplications.\n    - Total: $2 + 6 + 6 = 14$.\n  - Additions:\n    - $t=1$: $0$ additions.\n    - $t=2$: $N(N-1) = 2(2-1) = 2$ additions.\n    - $t=3$: $N(N-1) = 2(2-1) = 2$ additions.\n    - Termination: $N-1 = 1$ addition.\n    - Total: $2 + 2 + 1 = 5$.\n\nThe forward algorithm is significantly more efficient, especially for longer sequences, as it avoids the exponential complexity of enumerating all paths.\n\nThe probability of the observation sequence $(A, B, A)$ is $0.084992$. Rounding to four significant figures gives $0.08499$.",
            "answer": "$$\\boxed{0.08499}$$"
        }
    ]
}