{
    "hands_on_practices": [
        {
            "introduction": "In theory, policy iteration guarantees improvement by acting greedily with respect to a policy's true value function. But what happens when we can only work with an approximation of the values, as is common in practice? This exercise  demonstrates a critical failure mode where a seemingly 'improved' policy based on a simple linear approximation actually performs much worse, revealing the subtle dangers of combining approximation with policy improvement.",
            "id": "3169919",
            "problem": "Consider the following finite Markov Decision Process (MDP) with discount factor $\\gamma=\\frac{9}{10}$. The state space is $\\{s_0,s_1,s_2,s_{\\mathrm{T}}\\}$, where $s_{\\mathrm{T}}$ is terminal. The start state is $s_0$. The dynamics and rewards are deterministic and defined as follows:\n- In $s_0$, there are two actions. Action $A$ yields immediate reward $0$ and transitions to $s_1$. Action $B$ yields immediate reward $1$ and transitions to $s_2$.\n- In $s_1$, there is a single available action that yields immediate reward $10$ and transitions to $s_{\\mathrm{T}}$.\n- In $s_2$, there is a single available action that yields immediate reward $0$ and transitions to $s_{\\mathrm{T}}$.\n- In $s_{\\mathrm{T}}$, the episode terminates and no further rewards are collected.\n\nLet the initial policy $\\pi_0$ choose action $A$ in $s_0$ (the only choices in $s_1$ and $s_2$ are implicit). Perform policy evaluation for $\\pi_0$ using linear function approximation (LFA) with feature map $\\phi:\\{s_0,s_1,s_2,s_{\\mathrm{T}}\\}\\to\\mathbb{R}$ given by $\\phi(s)=1$ for $s\\in\\{s_0,s_1,s_2\\}$ and $\\phi(s_{\\mathrm{T}})=0$, and approximator $\\hat{v}(s)=w\\,\\phi(s)$ with parameter $w\\in\\mathbb{R}$. Define the fitted parameter $w^{\\star}$ to minimize the mean squared error to the true state values of $\\pi_0$ over the nonterminal set $\\{s_0,s_1,s_2\\}$ with uniform weighting, that is, $w^{\\star}\\in\\arg\\min_{w}\\frac{1}{3}\\sum_{s\\in\\{s_0,s_1,s_2\\}}\\left(V^{\\pi_0}(s)-w\\right)^{2}$.\n\nUsing this fitted $\\hat{v}$, define the greedy policy improvement at $s_0$ by computing approximate action-values via one-step lookahead,\n$$\\hat{q}(s_0,a)\\;=\\;r(s_0,a)\\;+\\;\\gamma\\,\\hat{v}\\!\\left(s^{\\prime}\\right),$$\nwhere $s^{\\prime}$ is the deterministic next state under action $a\\in\\{A,B\\}$. Let $\\pi_{\\mathrm{greedy}}$ choose the action in $\\{A,B\\}$ that maximizes $\\hat{q}(s_0,a)$. Compute the true performance change at the start state induced by this greedy improvement,\n$$\\Delta\\;=\\;V^{\\pi_{\\mathrm{greedy}}}(s_0)\\;-\\;V^{\\pi_0}(s_0),$$\nand report its exact value as a single real number. Do not round; provide the exact value.",
            "solution": "The user wants to compute the performance change at the start state after one step of policy improvement using linear function approximation. The process involves several steps: first, evaluating the initial policy $\\pi_0$; second, fitting the function approximator to these true values; third, using the approximator to perform a greedy policy update; fourth, evaluating the new policy; and finally, computing the difference in value at the start state.\n\nThe state space is $\\mathcal{S}=\\{s_0,s_1,s_2,s_{\\mathrm{T}}\\}$, where $s_{\\mathrm{T}}$ is a terminal state. The start state is $s_0$. The discount factor is $\\gamma=\\frac{9}{10}$. The dynamics and rewards are deterministic.\n\nFirst, we compute the true state-value function, $V^{\\pi_0}(s)$, for the initial policy $\\pi_0$. The policy $\\pi_0$ selects action $A$ in state $s_0$. In states $s_1$ and $s_2$, the actions are deterministic. The value of the terminal state is always $V(s_{\\mathrm{T}})=0$.\n\nThe value of state $s_1$ under any policy is determined by the single available action, which yields a reward of $10$ and transitions to $s_{\\mathrm{T}}$:\n$$V^{\\pi_0}(s_1) = R(s_1) + \\gamma V^{\\pi_0}(s_{\\mathrm{T}}) = 10 + \\gamma \\cdot 0 = 10$$\n\nThe value of state $s_2$ under any policy is determined by the single available action, which yields a reward of $0$ and transitions to $s_{\\mathrm{T}}$:\n$$V^{\\pi_0}(s_2) = R(s_2) + \\gamma V^{\\pi_0}(s_{\\mathrm{T}}) = 0 + \\gamma \\cdot 0 = 0$$\n\nUnder policy $\\pi_0$, action $A$ is taken in state $s_0$. This yields a reward of $0$ and transitions to state $s_1$. Therefore, the value of the start state $s_0$ is:\n$$V^{\\pi_0}(s_0) = R(s_0, A) + \\gamma V^{\\pi_0}(s_1) = 0 + \\frac{9}{10} \\cdot 10 = 9$$\n\nSo, the true values for the nonterminal states under policy $\\pi_0$ are $V^{\\pi_0}(s_0)=9$, $V^{\\pi_0}(s_1)=10$, and $V^{\\pi_0}(s_2)=0$.\n\nNext, we find the parameter $w^{\\star}$ for the linear function approximator $\\hat{v}(s) = w\\,\\phi(s)$, where the feature map is $\\phi(s)=1$ for $s\\in\\{s_0,s_1,s_2\\}$ and $\\phi(s_{\\mathrm{T}})=0$. This implies $\\hat{v}(s)=w$ for all nonterminal states. The parameter $w^{\\star}$ is found by minimizing the mean squared error (MSE) over the nonterminal states:\n$$J(w) = \\frac{1}{3}\\sum_{s\\in\\{s_0,s_1,s_2\\}} \\left(V^{\\pi_0}(s)-\\hat{v}(s)\\right)^{2} = \\frac{1}{3}\\left(\\left(V^{\\pi_0}(s_0)-w\\right)^{2} + \\left(V^{\\pi_0}(s_1)-w\\right)^{2} + \\left(V^{\\pi_0}(s_2)-w\\right)^{2}\\right)$$\nSubstituting the true values:\n$$J(w) = \\frac{1}{3}\\left(\\left(9-w\\right)^{2} + \\left(10-w\\right)^{2} + \\left(0-w\\right)^{2}\\right)$$\nTo find the minimum, we set the derivative with respect to $w$ to zero:\n$$\\frac{dJ}{dw} = \\frac{1}{3}\\left(2(9-w)(-1) + 2(10-w)(-1) + 2w\\right) = 0$$\n$$-2(9-w) - 2(10-w) + 2w = 0$$\n$$-(9-w) - (10-w) + w = 0$$\n$$-9 + w - 10 + w + w = 0$$\n$$3w - 19 = 0$$\n$$w^{\\star} = \\frac{19}{3}$$\nThis result is expected, as the value of $w$ that minimizes the sum of squared errors is the mean of the target values: $w^{\\star} = \\frac{9+10+0}{3} = \\frac{19}{3}$.\n\nThe approximate value function is thus $\\hat{v}(s) = \\frac{19}{3}$ for $s \\in \\{s_0,s_1,s_2\\}$, and $\\hat{v}(s_{\\mathrm{T}})=0$.\n\nNow, we perform policy improvement by defining a new greedy policy $\\pi_{\\mathrm{greedy}}$ based on the approximate action-values $\\hat{q}(s_0,a)$ at the start state $s_0$. The formula for the action-value is $\\hat{q}(s_0,a) = r(s_0,a) + \\gamma\\,\\hat{v}(s^{\\prime})$.\n\nFor action $A$, the reward is $r(s_0,A)=0$ and the next state is $s^{\\prime}=s_1$:\n$$\\hat{q}(s_0,A) = 0 + \\gamma \\hat{v}(s_1) = \\frac{9}{10} \\cdot \\frac{19}{3} = \\frac{3 \\cdot 19}{10} = \\frac{57}{10} = 5.7$$\n\nFor action $B$, the reward is $r(s_0,B)=1$ and the next state is $s^{\\prime}=s_2$:\n$$\\hat{q}(s_0,B) = 1 + \\gamma \\hat{v}(s_2) = 1 + \\frac{9}{10} \\cdot \\frac{19}{3} = 1 + \\frac{57}{10} = \\frac{10}{10} + \\frac{57}{10} = \\frac{67}{10} = 6.7$$\n\nSince $\\hat{q}(s_0,B) > \\hat{q}(s_0,A)$, the greedy policy $\\pi_{\\mathrm{greedy}}$ selects action $B$ in state $s_0$:\n$$\\pi_{\\mathrm{greedy}}(s_0) = \\arg\\max_{a\\in\\{A,B\\}} \\hat{q}(s_0,a) = B$$\n\nNext, we compute the true value of the start state under this new policy, $V^{\\pi_{\\mathrm{greedy}}}(s_0)$. With policy $\\pi_{\\mathrm{greedy}}$, action $B$ is taken at $s_0$, which gives an immediate reward of $1$ and transitions to $s_2$. The value of state $s_2$ is unchanged and remains $V^{\\pi_{\\mathrm{greedy}}}(s_2)=0$.\n$$V^{\\pi_{\\mathrm{greedy}}}(s_0) = R(s_0, B) + \\gamma V^{\\pi_{\\mathrm{greedy}}}(s_2) = 1 + \\frac{9}{10} \\cdot 0 = 1$$\n\nFinally, we compute the performance change $\\Delta$:\n$$\\Delta = V^{\\pi_{\\mathrm{greedy}}}(s_0) - V^{\\pi_0}(s_0)$$\nSubstituting the calculated values:\n$$\\Delta = 1 - 9 = -8$$\nThe \"greedy\" update based on the function approximator has resulted in a policy that is substantially worse than the initial policy. This is a known issue in approximate reinforcement learning, where the error in the value function approximation can mislead the policy improvement step.",
            "answer": "$$\\boxed{-8}$$"
        },
        {
            "introduction": "Policy gradient methods offer a powerful approach by directly optimizing a policy's parameters, but the path to the optimal policy is not always straightforward. The optimization landscape can be riddled with local optima that trap naive gradient ascent algorithms. Through this hands-on problem , you will analyze such a landscape and discover how an agent's initial bias can prevent it from discovering a much better strategy, a core challenge in exploration.",
            "id": "3169897",
            "problem": "Consider the following Markov Decision Process (MDP) construction and analysis task grounded in first principles of reinforcement learning. A Markov Decision Process (MDP) is specified by a set of states, a set of actions, a transition kernel, a reward function, and a discount factor. You are given the following MDP with two states and episodic horizon.\n\n- States: The process starts in state $s_0$. If action transitions to state $s_1$, the episode continues; otherwise, it terminates immediately.\n- Actions at $s_0$: The agent chooses one of two actions, $a \\in \\{\\text{left}, \\text{right}\\}$. If $a=\\text{left}$, the episode terminates with immediate reward $2$. If $a=\\text{right}$, the episode transitions deterministically to $s_1$ with immediate reward $0$.\n- Actions at $s_1$: The agent chooses one of two actions, $b \\in \\{\\text{safe}, \\text{risky}\\}$. If $b=\\text{safe}$, the episode terminates with reward $0$. If $b=\\text{risky}$, the episode terminates with reward $10$ with probability $1/2$ and reward $-1$ with probability $1/2$.\n- Discount factor: The discount factor is $\\gamma=1$.\n\nThe agent uses a stationary, stochastic, differentiable policy parameterized by two real scalars $\\theta_1$ and $\\theta_2$, with independent logistic parameterizations at each state:\n- At $s_0$, the probability of choosing $\\text{right}$ is $\\pi(\\text{right}\\mid s_0)=\\sigma(\\theta_1)$ and $\\pi(\\text{left}\\mid s_0)=1-\\sigma(\\theta_1)$.\n- At $s_1$, the probability of choosing $\\text{risky}$ is $\\pi(\\text{risky}\\mid s_1)=\\sigma(\\theta_2)$ and $\\pi(\\text{safe}\\mid s_1)=1-\\sigma(\\theta_2)$.\n\nHere $\\sigma(x)$ denotes the logistic function defined by $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$.\n\nUsing only the foundational definitions of expected return and the chain rule for differentiation, and without invoking any specialized policy gradient theorems, do the following:\n1. Derive the expected episodic return $J(\\theta_1,\\theta_2)$ from the start state $s_0$ in terms of $\\sigma(\\theta_1)$ and $\\sigma(\\theta_2)$.\n2. Compute the partial derivatives $\\frac{\\partial J}{\\partial \\theta_1}$ and $\\frac{\\partial J}{\\partial \\theta_2}$ explicitly via the chain rule and properties of $\\sigma(x)$.\n3. Analyze the stationary points of $J(\\theta_1,\\theta_2)$ and describe conditions under which naive policy gradient ascent, starting from an initialization with $\\sigma(\\theta_1)$ small, can converge to a suboptimal local maximum that avoids $s_1$. Provide intuition for the coupling between $\\theta_1$ and $\\theta_2$ that creates this behavior.\n4. As a concrete, calculational deliverable, determine the exact value of $\\sigma(\\theta_2)$ that makes $\\frac{\\partial J}{\\partial \\theta_1}=0$ at a finite value of $\\theta_1$ (i.e., not relying on saturation $\\sigma(\\theta_1)\\in\\{0,1\\}$). Express your final answer as a single simplified fraction or closed-form expression. No rounding is required.",
            "solution": "The problem statement is a well-defined exercise in reinforcement learning and is scientifically grounded, well-posed, and objective. It contains no invalidating flaws. We may proceed with a solution.\n\nThe problem asks for an analysis of a simple Markov Decision Process (MDP) using first principles. We are given the structure of the MDP, the rewards, and a parameterized policy. We will derive the expected return, compute its gradients, and analyze its stationary points to understand a potential pitfall of policy gradient methods.\n\nLet $\\sigma_1 = \\sigma(\\theta_1)$ and $\\sigma_2 = \\sigma(\\theta_2)$ for notational simplicity. The policy is defined as:\n- At state $s_0$: $\\pi(\\text{right}\\mid s_0) = \\sigma_1$ and $\\pi(\\text{left}\\mid s_0) = 1-\\sigma_1$.\n- At state $s_1$: $\\pi(\\text{risky}\\mid s_1) = \\sigma_2$ and $\\pi(\\text{safe}\\mid s_1) = 1-\\sigma_2$.\n\nThe discount factor is $\\gamma=1$. All episodes terminate in at most two steps.\n\n**1. Derivation of the Expected Episodic Return $J(\\theta_1, \\theta_2)$**\n\nThe expected return $J(\\theta_1, \\theta_2)$ is the value of the start state $s_0$, which we can denote as $V^{\\pi}(s_0)$. We can calculate this by considering the possible trajectories starting from $s_0$.\n\nThere are two initial choices from $s_0$:\n- Action 'left': This action is taken with probability $1-\\sigma_1$. The episode terminates immediately with a reward of $2$. The contribution to the total expected return from this path is $(1-\\sigma_1) \\times 2$.\n- Action 'right': This action is taken with probability $\\sigma_1$. The agent receives an immediate reward of $0$ and transitions to state $s_1$. The total return for this path is the immediate reward plus the discounted value of the subsequent state. Since $\\gamma=1$, this is $0 + 1 \\times V^{\\pi}(s_1)$.\n\nFirst, we must compute the expected return from state $s_1$, $V^{\\pi}(s_1)$. From $s_1$, the agent can take two actions:\n- Action 'safe': Taken with probability $1-\\sigma_2$. The episode terminates with a reward of $0$.\n- Action 'risky': Taken with probability $\\sigma_2$. The episode terminates with a reward of $10$ with probability $1/2$ or a reward of $-1$ with probability $1/2$. The expected reward for this action is $10 \\times \\frac{1}{2} + (-1) \\times \\frac{1}{2} = 5 - \\frac{1}{2} = 4.5$.\n\nTherefore, the value of state $s_1$ under policy $\\pi$ is:\n$$V^{\\pi}(s_1) = (1-\\sigma_2) \\times 0 + \\sigma_2 \\times (4.5) = 4.5\\sigma_2$$\n\nNow we can write the full expression for $J(\\theta_1, \\theta_2) = V^{\\pi}(s_0)$:\n$$J(\\theta_1, \\theta_2) = (1-\\sigma_1) \\times 2 + \\sigma_1 \\times (0 + \\gamma V^{\\pi}(s_1))$$\nSubstituting $\\gamma=1$ and the expression for $V^{\\pi}(s_1)$:\n$$J(\\theta_1, \\theta_2) = 2(1-\\sigma_1) + \\sigma_1 (4.5\\sigma_2)$$\n$$J(\\theta_1, \\theta_2) = 2 - 2\\sigma_1 + 4.5\\sigma_1\\sigma_2$$\nFactoring out $\\sigma_1$:\n$$J(\\theta_1, \\theta_2) = 2 + \\sigma_1(4.5\\sigma_2 - 2)$$\n\n**2. Computation of Partial Derivatives**\n\nTo compute the partial derivatives with respect to $\\theta_1$ and $\\theta_2$, we use the chain rule. We first recall the derivative of the logistic function $\\sigma(x)$: $\\frac{d\\sigma(x)}{dx} = \\sigma(x)(1-\\sigma(x))$.\n\nPartial derivative with respect to $\\theta_1$:\n$$\\frac{\\partial J}{\\partial \\theta_1} = \\frac{\\partial J}{\\partial \\sigma_1} \\frac{\\partial \\sigma_1}{\\partial \\theta_1}$$\nFrom the expression for $J$, we find $\\frac{\\partial J}{\\partial \\sigma_1}$:\n$$\\frac{\\partial J}{\\partial \\sigma_1} = \\frac{\\partial}{\\partial \\sigma_1} (2 - 2\\sigma_1 + 4.5\\sigma_1\\sigma_2) = -2 + 4.5\\sigma_2$$\nThe derivative of $\\sigma_1 = \\sigma(\\theta_1)$ is:\n$$\\frac{\\partial \\sigma_1}{\\partial \\theta_1} = \\sigma(\\theta_1)(1-\\sigma(\\theta_1)) = \\sigma_1(1-\\sigma_1)$$\nCombining these gives:\n$$\\frac{\\partial J}{\\partial \\theta_1} = (4.5\\sigma_2 - 2) \\sigma_1(1-\\sigma_1)$$\n\nPartial derivative with respect to $\\theta_2$:\n$$\\frac{\\partial J}{\\partial \\theta_2} = \\frac{\\partial J}{\\partial \\sigma_2} \\frac{\\partial \\sigma_2}{\\partial \\theta_2}$$\nFrom the expression for $J$, we find $\\frac{\\partial J}{\\partial \\sigma_2}$:\n$$\\frac{\\partial J}{\\partial \\sigma_2} = \\frac{\\partial}{\\partial \\sigma_2} (2 - 2\\sigma_1 + 4.5\\sigma_1\\sigma_2) = 4.5\\sigma_1$$\nThe derivative of $\\sigma_2 = \\sigma(\\theta_2)$ is:\n$$\\frac{\\partial \\sigma_2}{\\partial \\theta_2} = \\sigma(\\theta_2)(1-\\sigma(\\theta_2)) = \\sigma_2(1-\\sigma_2)$$\nCombining these gives:\n$$\\frac{\\partial J}{\\partial \\theta_2} = 4.5 \\sigma_1 \\sigma_2(1-\\sigma_2)$$\n\n**3. Analysis of Stationary Points and Suboptimal Convergence**\n\nStationary points occur where the gradient is zero, i.e., $\\frac{\\partial J}{\\partial \\theta_1} = 0$ and $\\frac{\\partial J}{\\partial \\theta_2} = 0$.\n\nFrom $\\frac{\\partial J}{\\partial \\theta_1} = (4.5\\sigma_2 - 2) \\sigma_1(1-\\sigma_1) = 0$, we have three possibilities:\n(a) $\\sigma_1 = 0$ (corresponding to $\\theta_1 \\to -\\infty$)\n(b) $\\sigma_1 = 1$ (corresponding to $\\theta_1 \\to +\\infty$)\n(c) $4.5\\sigma_2 - 2 = 0 \\implies \\sigma_2 = \\frac{2}{4.5} = \\frac{4}{9}$\n\nFrom $\\frac{\\partial J}{\\partial \\theta_2} = 4.5 \\sigma_1 \\sigma_2(1-\\sigma_2) = 0$, we have three possibilities:\n(d) $\\sigma_1 = 0$\n(e) $\\sigma_2 = 0$ (corresponding to $\\theta_2 \\to -\\infty$)\n(f) $\\sigma_2 = 1$ (corresponding to $\\theta_2 \\to +\\infty$)\n\nStationary points are combinations of these conditions satisfying both equations.\n- If $\\sigma_1=0$, both equations are satisfied for any value of $\\sigma_2$. This forms a line of stationary points. The expected return is $J = 2 + 0 \\times (4.5\\sigma_2-2) = 2$. This corresponds to the deterministic policy of always choosing 'left' at $s_0$.\n- If $\\sigma_1=1$, the first equation is satisfied. For the second equation to hold, we need $\\sigma_2=0$ or $\\sigma_2=1$.\n  - $(\\sigma_1, \\sigma_2) = (1, 0)$: $J = 2 + 1(4.5(0)-2) = 0$. This is a local minimum.\n  - $(\\sigma_1, \\sigma_2) = (1, 1)$: $J = 2 + 1(4.5(1)-2) = 4.5$. This is the global maximum.\n- If $\\sigma_1 \\in (0,1)$ and $\\sigma_2 \\in (0,1)$, then for the gradient to be zero we need $4.5\\sigma_2 - 2 = 0$ (from eq. 1) and $4.5\\sigma_1 = 0$ (from eq. 2). The second condition implies $\\sigma_1=0$, which contradicts $\\sigma_1 \\in (0,1)$. Thus, there are no interior stationary points.\n\nThe global maximum return is $4.5$, achieved when the agent always goes 'right' and then chooses 'risky'. However, there is a suboptimal local maximum (a line of stationary points) with a return of $2$.\n\nThe behavior of policy gradient ascent depends critically on the initial parameterization, particularly $\\theta_2$. The gradient for $\\theta_1$ is $\\frac{\\partial J}{\\partial \\theta_1} \\propto (4.5\\sigma_2 - 2)$.\n- If initially $\\sigma_2 > 4/9$, then $4.5\\sigma_2 - 2 > 0$. The gradient $\\frac{\\partial J}{\\partial \\theta_1}$ is positive. Policy gradient ascent will increase $\\theta_1$, leading to an increase in $\\sigma_1$. This encourages exploration of state $s_1$, allowing the agent to learn about the high potential reward and eventually converge towards the optimal policy $(\\sigma_1, \\sigma_2) = (1,1)$.\n- If initially $\\sigma_2  4/9$, then $4.5\\sigma_2 - 2  0$. The gradient $\\frac{\\partial J}{\\partial \\theta_1}$ is negative. Policy gradient ascent will decrease $\\theta_1$, causing $\\sigma_1$ to shrink towards $0$. This means the agent becomes less likely to visit $s_1$. As $\\sigma_1 \\to 0$, the gradient for $\\theta_2$, which is proportional to $\\sigma_1$, also vanishes. Learning for $\\theta_2$ effectively stops. The agent converges to the policy $\\sigma_1=0$, getting a stable but suboptimal return of $2$. It gets trapped because its initial policy at $s_1$ is not promising enough to justify exploring that state over the guaranteed reward of $2$.\n\nThe coupling between $\\theta_1$ and $\\theta_2$ arises because the gradient for $\\theta_2$ depends on the probability of reaching $s_1$ (i.e., on $\\sigma_1$), and the gradient for $\\theta_1$ depends on the expected value of being in $s_1$ (i.e., on $\\sigma_2$). This creates a \"chicken-and-egg\" problem characteristic of exploration challenges in reinforcement learning.\n\n**4. Final Calculational Deliverable**\n\nWe need to find the value of $\\sigma(\\theta_2)$ that makes $\\frac{\\partial J}{\\partial \\theta_1}=0$ for a finite value of $\\theta_1$. A finite $\\theta_1$ means that $\\sigma(\\theta_1) = \\sigma_1$ is not at its saturation points of $0$ or $1$. Thus, $\\sigma_1(1-\\sigma_1) \\neq 0$.\n\nThe expression for the partial derivative is:\n$$\\frac{\\partial J}{\\partial \\theta_1} = (4.5\\sigma_2-2) \\sigma_1(1-\\sigma_1)$$\nFor this to be zero with $\\sigma_1 \\in (0,1)$, the term $(4.5\\sigma_2-2)$ must be zero.\n$$4.5\\sigma_2 - 2 = 0$$\nSolving for $\\sigma_2 = \\sigma(\\theta_2)$:\n$$4.5\\sigma_2 = 2$$\n$$\\frac{9}{2}\\sigma_2 = 2$$\n$$\\sigma_2 = 2 \\times \\frac{2}{9} = \\frac{4}{9}$$\nThis is the threshold value. At this specific value for the policy at state $s_1$, the agent at state $s_0$ is perfectly indifferent between the guaranteed reward of $2$ from choosing 'left' and the expected reward from choosing 'right', since $V^\\pi(s_1) = 4.5 \\times (4/9) = 2$.",
            "answer": "$$\\boxed{\\frac{4}{9}}$$"
        },
        {
            "introduction": "While many reinforcement learning methods are iterative, an entirely different and powerful perspective exists when the environment's model is known: linear programming. This practice  guides you to reframe the search for an optimal policy as a global optimization problem using state-action occupancy measures. You will also explore the elegant connection between this formulation and the Bellman equations through the principle of duality.",
            "id": "3169918",
            "problem": "Consider a discounted, infinite-horizon Markov Decision Process (MDP) with a finite state set and deterministic action sets. The Markov Decision Process (MDP) is specified by the tuple $\\left(\\mathcal{S}, \\{\\mathcal{A}(s)\\}_{s \\in \\mathcal{S}}, P, r, \\gamma, d_0\\right)$, where $\\mathcal{S}$ is the finite set of states, $\\mathcal{A}(s)$ is the finite set of actions available at state $s$, $P(s' \\mid s,a)$ is the transition kernel, $r(s,a)$ is the bounded reward function, $\\gamma \\in (0,1)$ is the discount factor, and $d_0$ is the initial state distribution. The discounted occupancy measure $\\rho^{\\pi}(s,a)$ for a policy $\\pi$ is defined by\n$$\n\\rho^{\\pi}(s,a) \\;=\\; (1-\\gamma)\\sum_{t=0}^{\\infty}\\gamma^{t}\\,\\Pr^{\\pi}(s_t = s, a_t = a).\n$$\nThe initial distribution is $d_0(s_0) = 1$ and $d_0(s_1) = 0$, with $\\mathcal{S} = \\{s_0, s_1\\}$. The action sets are $\\mathcal{A}(s_0)=\\{a_0,a_1\\}$ and $\\mathcal{A}(s_1)=\\{b_0,b_1\\}$. The dynamics and rewards are:\n- At $s_0$: taking $a_0$ yields reward $r(s_0,a_0)=1$ and transitions to $s_1$ with probability $P(s_1 \\mid s_0,a_0)=1$; taking $a_1$ yields reward $r(s_0,a_1)=0$ and transitions to $s_0$ with probability $P(s_0 \\mid s_0,a_1)=1$.\n- At $s_1$: taking $b_0$ yields reward $r(s_1,b_0)=2$ and transitions to $s_1$ with probability $P(s_1 \\mid s_1,b_0)=1$; taking $b_1$ yields reward $r(s_1,b_1)=-1$ and transitions to $s_0$ with probability $P(s_0 \\mid s_1,b_1)=1$.\n\nTasks:\n- Using the definition of the discounted occupancy measure and conservation of probability flow under the Markov property, formulate the primal linear programming (LP) problem that optimizes over variables $\\rho(s,a)$ to maximize the scaled expected discounted return. Write the objective and all constraints explicitly specialized to the above MDP, including nonnegativity constraints.\n- Derive the dual LP by introducing one dual variable per state, and show how the dual constraints recover a form of the optimality condition on the state value function.\n- Compute the optimal value at the initial state, $V^{*}(s_0)$, as a closed-form analytic expression in $\\gamma$. Provide only the final expression for $V^{*}(s_0)$ as your answer. No rounding is required.",
            "solution": "The problem requires formulating the primal and dual linear programming (LP) representations for a given Markov Decision Process (MDP) and then computing the optimal state value function $V^{*}(s_0)$ at the initial state $s_0$. We will address each part in sequence.\n\nFirst, we formulate the primal LP. The variables of this LP are the discounted occupancy measures $\\rho(s,a)$ for each state-action pair $(s,a)$. The objective is to maximize the scaled expected total discounted reward, which is given by the sum of rewards weighted by their respective occupancy measures. The constraints enforce the conservation of state occupancy probability flow.\n\nThe objective function to maximize is:\n$$\n\\sum_{s \\in \\mathcal{S}} \\sum_{a \\in \\mathcal{A}(s)} r(s,a) \\rho(s,a)\n$$\nIn our specific case, this is:\n$$\nZ_P = r(s_0, a_0)\\rho(s_0, a_0) + r(s_0, a_1)\\rho(s_0, a_1) + r(s_1, b_0)\\rho(s_1, b_0) + r(s_1, b_1)\\rho(s_1, b_1)\n$$\nSubstituting the given reward values $r(s_0,a_0)=1$, $r(s_0,a_1)=0$, $r(s_1,b_0)=2$, and $r(s_1,b_1)=-1$:\n$$\nZ_P = 1 \\cdot \\rho(s_0, a_0) + 0 \\cdot \\rho(s_0, a_1) + 2 \\cdot \\rho(s_1, b_0) - 1 \\cdot \\rho(s_1, b_1) = \\rho(s_0, a_0) + 2\\rho(s_1, b_0) - \\rho(s_1, b_1)\n$$\nThe constraints are derived from the Bellman flow equations for occupancy measures, which state that for any state $s'$, the total occupancy of leaving $s'$ is equal to the initial occupancy plus the total discounted occupancy flowing in from all other states. The general form is:\n$$\n\\sum_{a \\in \\mathcal{A}(s')} \\rho(s', a) = (1-\\gamma) d_0(s') + \\gamma \\sum_{s \\in \\mathcal{S}} \\sum_{a \\in \\mathcal{A}(s)} P(s' \\mid s, a) \\rho(s, a)\n$$\nFor our MDP, we have two states $s_0$ and $s_1$.\nFor state $s_0$:\nGiven $d_0(s_0)=1$, $P(s_0 \\mid s_0,a_1)=1$, and $P(s_0 \\mid s_1,b_1)=1$. All other transitions to $s_0$ have probability $0$.\n$$\n\\rho(s_0, a_0) + \\rho(s_0, a_1) = (1-\\gamma)(1) + \\gamma (P(s_0 \\mid s_0, a_1) \\rho(s_0, a_1) + P(s_0 \\mid s_1, b_1) \\rho(s_1, b_1))\n$$\n$$\n\\rho(s_0, a_0) + \\rho(s_0, a_1) = 1-\\gamma + \\gamma (\\rho(s_0, a_1) + \\rho(s_1, b_1))\n$$\nRearranging gives the first constraint:\n$$\n\\rho(s_0, a_0) + (1-\\gamma)\\rho(s_0, a_1) - \\gamma \\rho(s_1, b_1) = 1-\\gamma\n$$\nFor state $s_1$:\nGiven $d_0(s_1)=0$, $P(s_1 \\mid s_0,a_0)=1$, and $P(s_1 \\mid s_1,b_0)=1$. All other transitions to $s_1$ have probability $0$.\n$$\n\\rho(s_1, b_0) + \\rho(s_1, b_1) = (1-\\gamma)(0) + \\gamma (P(s_1 \\mid s_0, a_0) \\rho(s_0, a_0) + P(s_1 \\mid s_1, b_0) \\rho(s_1, b_0))\n$$\n$$\n\\rho(s_1, b_0) + \\rho(s_1, b_1) = \\gamma (\\rho(s_0, a_0) + \\rho(s_1, b_0))\n$$\nRearranging gives the second constraint:\n$$\n-\\gamma \\rho(s_0, a_0) + (1-\\gamma)\\rho(s_1, b_0) + \\rho(s_1, b_1) = 0\n$$\nFinally, occupancy measures must be non-negative: $\\rho(s,a) \\geq 0$ for all $(s,a)$.\n\nThe complete primal LP is:\nMaximize $\\rho(s_0, a_0) + 2\\rho(s_1, b_0) - \\rho(s_1, b_1)$\nSubject to:\n1. $\\rho(s_0, a_0) + (1-\\gamma)\\rho(s_0, a_1) - \\gamma \\rho(s_1, b_1) = 1-\\gamma$\n2. $-\\gamma \\rho(s_0, a_0) + (1-\\gamma)\\rho(s_1, b_0) + \\rho(s_1, b_1) = 0$\n3. $\\rho(s_0, a_0) \\geq 0$, $\\rho(s_0, a_1) \\geq 0$, $\\rho(s_1, b_0) \\geq 0$, $\\rho(s_1, b_1) \\geq 0$\n\nNext, we derive the dual LP. We introduce one dual variable for each of the two equality constraints in the primal, denoted by $V(s_0)$ and $V(s_1)$. The dual objective is to minimize a linear combination of these variables, with coefficients given by the right-hand side of the primal constraints:\n$$\nZ_D = (1-\\gamma)V(s_0) + 0 \\cdot V(s_1) = (1-\\gamma)V(s_0)\n$$\nThe dual constraints are formed by ensuring that for each primal variable $\\rho(s,a)$, the inner product of its coefficient vector in the primal constraints with the vector of dual variables is greater than or equal to its coefficient in the primal objective.\n\nFor $\\rho(s_0, a_0)$: $1 \\cdot V(s_0) - \\gamma \\cdot V(s_1) \\geq 1$\nFor $\\rho(s_0, a_1)$: $(1-\\gamma)V(s_0) + 0 \\cdot V(s_1) \\geq 0 \\implies V(s_0) \\geq 0$ (since $1-\\gamma > 0$)\nFor $\\rho(s_1, b_0)$: $0 \\cdot V(s_0) + (1-\\gamma)V(s_1) \\geq 2 \\implies V(s_1) \\geq \\frac{2}{1-\\gamma}$\nFor $\\rho(s_1, b_1)$: $-\\gamma V(s_0) + 1 \\cdot V(s_1) \\geq -1$\n\nThe dual LP is:\nMinimize $(1-\\gamma)V(s_0)$\nSubject to:\n1. $V(s_0) - \\gamma V(s_1) \\geq 1$\n2. $V(s_0) \\geq 0$\n3. $V(s_1) \\geq \\frac{2}{1-\\gamma}$\n4. $V(s_1) - \\gamma V(s_0) \\geq -1$\n\nThese dual constraints recover a form of the optimality condition. Let's rewrite them using the rewards and transition probabilities:\n1. $V(s_0) \\geq 1 + \\gamma V(s_1) = r(s_0, a_0) + \\gamma \\sum_{s'} P(s'|s_0, a_0)V(s')$\n2. $V(s_0) \\geq 0 = r(s_0, a_1) + \\gamma V(s_0) - \\gamma V(s_0) \\iff (1-\\gamma)V(s_0) \\geq 0$, which is $V(s_0) \\geq r(s_0,a_1)+\\gamma \\sum_{s'} P(s'|s_0, a_1)V(s')$\n3. $(1-\\gamma)V(s_1) \\geq 2 \\iff V(s_1) \\geq 2 + \\gamma V(s_1)$, which is $V(s_1) \\geq r(s_1, b_0) + \\gamma \\sum_{s'} P(s'|s_1, b_0)V(s')$\n4. $V(s_1) \\geq -1 + \\gamma V(s_0)$, which is $V(s_1) \\geq r(s_1, b_1) + \\gamma \\sum_{s'} P(s'|s_1, b_1)V(s')$\nEach constraint is of the form $V(s) \\geq r(s,a) + \\gamma \\sum_{s'} P(s'|s,a)V(s')$. For each state $s$, the set of constraints for all actions $a \\in \\mathcal{A}(s)$ is equivalent to $V(s) \\geq \\max_{a \\in \\mathcal{A}(s)}\\{r(s,a) + \\gamma \\sum_{s'} P(s'|s,a)V(s')\\}$. This is precisely the condition $V \\geq T^*V$, where $T^*$ is the Bellman optimality operator. The dual LP minimizes a weighted sum of the $V(s)$ values subject to this condition, and its solution yields the optimal state-value function $V^*$.\n\nFinally, we compute $V^*(s_0)$. The optimal value function $V^*$ is the unique fixed point of the Bellman optimality operator, satisfying $V^* = T^*V^*$. We write the Bellman optimality equations for our MDP:\n$$\nV^*(s_0) = \\max \\left\\{ r(s_0,a_0) + \\gamma V^*(s_1), \\ r(s_0,a_1) + \\gamma V^*(s_0) \\right\\} = \\max \\left\\{ 1 + \\gamma V^*(s_1), \\ \\gamma V^*(s_0) \\right\\}\n$$\n$$\nV^*(s_1) = \\max \\left\\{ r(s_1,b_0) + \\gamma V^*(s_1), \\ r(s_1,b_1) + \\gamma V^*(s_0) \\right\\} = \\max \\left\\{ 2 + \\gamma V^*(s_1), \\ -1 + \\gamma V^*(s_0) \\right\\}\n$$\nConsider the equation for $V^*(s_1)$. The term $2 + \\gamma V^*(s_1)$ represents the value of taking action $b_0$ and staying in state $s_1$. If this is the optimal choice, then $V^*(s_1) = 2 + \\gamma V^*(s_1)$, which implies $(1-\\gamma)V^*(s_1) = 2$, or $V^*(s_1) = \\frac{2}{1-\\gamma}$. This value corresponds to receiving a reward of $2$ at every step, discounted appropriately.\nLet's analyze the conditions under which each action is optimal.\nAt state $s_1$, the action $b_0$ is optimal if $2 + \\gamma V^*(s_1) \\geq -1 + \\gamma V^*(s_0)$, which is equivalent to $V^*(s_1) = \\frac{2}{1-\\gamma}$. This holds if $\\frac{2}{1-\\gamma} \\geq -1 + \\gamma V^*(s_0)$.\nAt state $s_0$, the action $a_0$ is optimal if $1 + \\gamma V^*(s_1) \\geq \\gamma V^*(s_0)$. Since $V^*(s_0)$ must be positive (as reward $1$ is achievable), we know $V^*(s_0)  \\gamma V^*(s_0)$. Therefore, the optimal action at $s_0$ is $a_0$ if $1 + \\gamma V^*(s_1) \\geq \\gamma V^*(s_0)$. If $a_0$ is optimal, then $V^*(s_0) = 1 + \\gamma V^*(s_1)$.\n\nLet's assume the optimal policy is $\\pi^*(s_0) = a_0$ and $\\pi^*(s_1) = b_0$. Then we have the system:\n$$\nV^*(s_0) = 1 + \\gamma V^*(s_1)\n$$\n$$\nV^*(s_1) = 2 + \\gamma V^*(s_1) \\implies V^*(s_1) = \\frac{2}{1-\\gamma}\n$$\nSubstituting the value of $V^*(s_1)$ into the equation for $V^*(s_0)$:\n$$\nV^*(s_0) = 1 + \\gamma \\left(\\frac{2}{1-\\gamma}\\right) = \\frac{1-\\gamma+2\\gamma}{1-\\gamma} = \\frac{1+\\gamma}{1-\\gamma}\n$$\nWe must verify that this solution is consistent with our assumptions.\nAssumption $\\pi^*(s_1)=b_0$ requires $\\frac{2}{1-\\gamma} \\geq -1 + \\gamma V^*(s_0)$. Substituting $V^*(s_0) = \\frac{1+\\gamma}{1-\\gamma}$:\n$$\n\\frac{2}{1-\\gamma} \\geq -1 + \\gamma \\frac{1+\\gamma}{1-\\gamma} = \\frac{-(1-\\gamma) + \\gamma(1+\\gamma)}{1-\\gamma} = \\frac{-1+2\\gamma+\\gamma^2}{1-\\gamma}\n$$\nSince $1-\\gamma  0$, this simplifies to $2 \\geq -1+2\\gamma+\\gamma^2$, or $\\gamma^2+2\\gamma-3 \\leq 0$. Factoring gives $(\\gamma+3)(\\gamma-1) \\leq 0$. For $\\gamma \\in (0,1)$, $\\gamma+3  0$ and $\\gamma-1  0$, so the inequality holds. The assumption is consistent.\nAssumption $\\pi^*(s_0)=a_0$ requires $1 + \\gamma V^*(s_1) \\geq \\gamma V^*(s_0)$. We have $V^*(s_0) = 1 + \\gamma V^*(s_1)$, so equality holds, and the assumption is consistent.\nThus, the calculated values are correct. The optimal value at the initial state is $V^*(s_0)$.",
            "answer": "$$\\boxed{\\frac{1+\\gamma}{1-\\gamma}}$$"
        }
    ]
}