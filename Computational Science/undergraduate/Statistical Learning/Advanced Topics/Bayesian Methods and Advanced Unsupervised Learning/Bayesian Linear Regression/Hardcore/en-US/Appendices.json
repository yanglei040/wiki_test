{
    "hands_on_practices": [
        {
            "introduction": "We begin with a foundational exercise that clarifies the mechanics of posterior derivation in an idealized setting. By using a design matrix with orthonormal columns, the algebraic complexity is stripped away, allowing you to focus on the essential interaction between the prior and the likelihood. This practice  beautifully illustrates how the independence of predictors translates into independent posterior beliefs about their corresponding weights.",
            "id": "3103067",
            "problem": "Consider Bayesian linear regression (BLR), where a design matrix $X \\in \\mathbb{R}^{N \\times D}$ maps a weight vector $w \\in \\mathbb{R}^{D}$ to predicted targets via a linear model. The observed target vector is $t \\in \\mathbb{R}^{N}$. Assume a Gaussian likelihood with noise precision $\\beta > 0$ and an independent zero-mean Gaussian prior over coefficients with precision $\\alpha > 0$.\n\nConstruct a specific dataset with orthonormal columns as follows: let $N=3$ and $D=2$, and take\n$$\nX \\;=\\;\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix},\n\\qquad\nt \\;=\\;\n\\begin{pmatrix}\n4 \\\\ -1 \\\\ 2\n\\end{pmatrix}.\n$$\nVerify that the columns of $X$ are orthonormal. Then, starting from the definitions of the Gaussian likelihood $p(t \\mid w, X, \\beta)$ and the Gaussian prior $p(w \\mid \\alpha)$, and applying Bayes' theorem $p(w \\mid t, X, \\alpha, \\beta) \\propto p(t \\mid w, X, \\beta)\\,p(w \\mid \\alpha)$, derive the posterior distribution over $w$. In your derivation, complete the square to identify both the posterior mean and the posterior covariance matrix, and simplify the posterior covariance using the orthonormality of the columns of $X$.\n\nExplain, based on your derivation, how orthogonality of the columns of $X$ decouples the posterior distributions of the individual coefficients in $w$.\n\nFinally, with $\\alpha = 2$ and $\\beta = 3$, evaluate the exact posterior covariance matrix for this dataset. Report the posterior covariance matrix as your final answer. No rounding is required.",
            "solution": "The problem is valid as it is a standard exercise in Bayesian linear regression, is scientifically and mathematically sound, and provides all necessary information for a unique solution.\n\nFirst, we validate that the columns of the matrix $X$ are orthonormal. Let the columns be denoted by $x_1$ and $x_2$.\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\qquad x_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nOrthonormality requires that the inner product $x_i^T x_j = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta. We check the inner products:\n$$\nx_1^T x_1 = (1)(1) + (0)(0) + (0)(0) = 1\n$$\n$$\nx_2^T x_2 = (0)(0) + (1)(1) + (0)(0) = 1\n$$\n$$\nx_1^T x_2 = (1)(0) + (0)(1) + (0)(0) = 0\n$$\nSince $x_1^T x_1 = 1$, $x_2^T x_2 = 1$, and $x_1^T x_2 = 0$, the columns are orthonormal. A more compact way to verify this is to compute $X^T X$:\n$$\nX^T X = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2\n$$\nSince $X^T X = I_D$ where $D=2$ is the number of columns, the columns of $X$ are, by definition, orthonormal.\n\nNext, we derive the posterior distribution over the weights $w$. The model is specified by a Gaussian likelihood and a Gaussian prior.\nThe likelihood is $p(t \\mid w, X, \\beta) = \\mathcal{N}(t \\mid Xw, \\beta^{-1}I_N)$, where $I_N$ is the $N \\times N$ identity matrix. The probability density function is:\n$$\np(t \\mid w, X, \\beta) \\propto \\exp\\left( -\\frac{\\beta}{2} (t - Xw)^T (t - Xw) \\right)\n$$\nThe prior on the weights is $p(w \\mid \\alpha) = \\mathcal{N}(w \\mid 0, \\alpha^{-1}I_D)$, where $I_D$ is the $D \\times D$ identity matrix. Its probability density function is:\n$$\np(w \\mid \\alpha) \\propto \\exp\\left( -\\frac{\\alpha}{2} w^T w \\right)\n$$\nAccording to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(w \\mid t, X, \\alpha, \\beta) \\propto p(t \\mid w, X, \\beta) \\, p(w \\mid \\alpha)\n$$\nWe work with the logarithm of the posterior, as this simplifies the product of exponentials into a sum of their arguments:\n$$\n\\ln p(w \\mid t, X, \\alpha, \\beta) = -\\frac{\\beta}{2} (t - Xw)^T (t - Xw) - \\frac{\\alpha}{2} w^T w + \\text{const}\n$$\nWe expand the quadratic term:\n$$\n(t - Xw)^T (t - Xw) = t^T t - t^T Xw - w^T X^T t + w^T X^T Xw = t^T t - 2w^T X^T t + w^T X^T Xw\n$$\nSubstituting this back into the log-posterior expression:\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{\\beta}{2} (t^T t - 2w^T X^T t + w^T X^T Xw) - \\frac{\\alpha}{2} w^T w + \\text{const}\n$$\nWe group terms involving $w$:\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{1}{2} w^T (\\beta X^T X + \\alpha I_D) w + \\beta w^T X^T t - \\frac{\\beta}{2} t^T t + \\text{const}\n$$\nDropping terms not dependent on $w$:\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{1}{2} w^T (\\beta X^T X + \\alpha I_D) w + \\beta w^T X^T t + \\text{const'}\n$$\nThis is a quadratic form in $w$, which implies the posterior is a Gaussian distribution, $p(w \\mid t, \\dots) = \\mathcal{N}(w \\mid m_N, S_N)$. The log-density of a general multivariate Gaussian $\\mathcal{N}(w \\mid m_N, S_N)$ is:\n$$\n\\ln \\mathcal{N}(w \\mid m_N, S_N) = -\\frac{1}{2} (w - m_N)^T S_N^{-1} (w - m_N) + \\text{const} = -\\frac{1}{2} (w^T S_N^{-1} w - 2w^T S_N^{-1} m_N + m_N^T S_N^{-1} m_N) + \\text{const}\n$$\n$$\n\\ln \\mathcal{N}(w \\mid m_N, S_N) = -\\frac{1}{2} w^T S_N^{-1} w + w^T S_N^{-1} m_N + \\text{const''}\n$$\nBy comparing the coefficients of the quadratic and linear terms in $w$ between our expression for the log-posterior and the general log-Gaussian density, we can identify the posterior precision matrix (inverse covariance) $S_N^{-1}$ and the mean $m_N$.\nComparing the quadratic terms ($w^T(\\cdot)w$):\n$$\nS_N^{-1} = \\beta X^T X + \\alpha I_D\n$$\nThus, the posterior covariance matrix is:\n$$\nS_N = (\\beta X^T X + \\alpha I_D)^{-1}\n$$\nComparing the linear terms ($w^T(\\cdot)$):\n$$\nS_N^{-1} m_N = \\beta X^T t \\implies m_N = S_N (\\beta X^T t) = \\beta (\\beta X^T X + \\alpha I_D)^{-1} X^T t\n$$\nThese are the general expressions for the posterior mean and covariance in Bayesian linear regression.\n\nNow, we use the property that the columns of $X$ are orthonormal, which means $X^T X = I_D$. Substituting this into the expressions for $S_N$ and $m_N$:\nFor the posterior covariance $S_N$:\n$$\nS_N = (\\beta I_D + \\alpha I_D)^{-1} = ((\\beta + \\alpha) I_D)^{-1} = \\frac{1}{\\alpha + \\beta} I_D\n$$\nFor the posterior mean $m_N$:\n$$\nm_N = \\beta \\left(\\frac{1}{\\alpha + \\beta} I_D\\right) X^T t = \\frac{\\beta}{\\alpha + \\beta} X^T t\n$$\nThe orthonormality of the columns of $X$ significantly simplifies the posterior parameters. The posterior covariance matrix $S_N = \\frac{1}{\\alpha + \\beta} I_D$ is a diagonal matrix. In a multivariate Gaussian distribution, a diagonal covariance matrix signifies that the random variables are uncorrelated. For Gaussian variables, being uncorrelated is equivalent to being statistically independent.\nThe joint posterior density is:\n$$\np(w \\mid t, \\dots) \\propto \\exp\\left(-\\frac{1}{2} (w-m_N)^T S_N^{-1} (w-m_N)\\right) = \\exp\\left(-\\frac{\\alpha+\\beta}{2} (w-m_N)^T (w-m_N)\\right)\n$$\n$$\n= \\exp\\left(-\\frac{\\alpha+\\beta}{2} \\sum_{j=1}^D (w_j - m_{N,j})^2\\right) = \\prod_{j=1}^D \\exp\\left(-\\frac{\\alpha+\\beta}{2} (w_j - m_{N,j})^2\\right)\n$$\nThis shows that the joint posterior $p(w \\mid t, \\dots)$ factorizes into a product of individual posteriors for each coefficient $w_j$: $p(w_1, \\dots, w_D \\mid t, \\dots) = \\prod_{j=1}^D p(w_j \\mid t, \\dots)$. This factorization is the \"decoupling\" of the posterior distributions of the coefficients. Each $w_j$ has a univariate Gaussian posterior distribution with mean $m_{N,j}$ and variance $1/(\\alpha+\\beta)$. The orthonormality of the features ensures that learning about one weight $w_j$ provides no information about any other weight $w_k$ ($k \\neq j$) beyond what is already known from the prior.\n\nFinally, we evaluate the exact posterior covariance matrix for the given dataset with $\\alpha = 2$ and $\\beta = 3$. We use the simplified formula derived from the orthonormality of $X$'s columns. The dimensionality of the weight vector is $D=2$.\n$$\nS_N = \\frac{1}{\\alpha + \\beta} I_D = \\frac{1}{2 + 3} I_2 = \\frac{1}{5} I_2\n$$\nWriting this as a matrix:\n$$\nS_N = \\frac{1}{5} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{5} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{5} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Real-world data is rarely as clean as in our first exercise; predictors are often correlated. This practice explores the important challenge of multicollinearity, where the design matrix is nearly singular. You will use singular value decomposition (SVD) as a conceptual tool to understand how posterior uncertainty can become unmanageably large for certain parameter combinations and, crucially, how a well-designed prior can provide necessary regularization .",
            "id": "3103139",
            "problem": "Consider a linear model with Gaussian noise, where the response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ and the design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ satisfy $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$. In Bayesian linear regression, suppose a zero-mean Gaussian prior is placed on the parameter vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$, with covariance $\\boldsymbol{\\Sigma}_{0}$ that is positive definite.\n\nAssume the singular value decomposition (SVD) of the design matrix is $\\mathbf{X} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^{\\top}$, where $\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{V} \\in \\mathbb{R}^{p \\times p}$ are orthogonal, and $\\mathbf{S} \\in \\mathbb{R}^{n \\times p}$ contains nonnegative singular values $s_{1} \\geq s_{2} \\geq \\cdots \\geq s_{p} \\geq 0$ on its diagonal (padding with zeros if $n \\neq p$). Consider the case where $\\mathbf{X}^{\\top}\\mathbf{X}$ is nearly singular, so that $s_{p} \\approx 0$ and the corresponding right singular vector $\\mathbf{v}_{p}$ represents a weakly identified direction in the parameter space.\n\nYou are asked to reason from the Gaussian likelihood and Gaussian prior to determine how the posterior covariance behaves along the principal directions of $\\mathbf{X}^{\\top}\\mathbf{X}$ and to propose a prior covariance design that regularizes the weakly identified directions without distorting well-identified ones. Which option below is correct?\n\nA. In the limit of a diffuse prior, the posterior variance along $\\mathbf{v}_{i}$ is large when $s_{i}$ is small, potentially diverging as $s_{i} \\to 0$. To control this, choose a prior covariance aligned with $\\mathbf{V}$, namely $\\boldsymbol{\\Sigma}_{0} = \\mathbf{V}\\,\\mathrm{diag}(\\alpha_{1},\\ldots,\\alpha_{p})\\,\\mathbf{V}^{\\top}$, and set smaller $\\alpha_{i}$ for directions with smaller $s_{i}$ to shrink most strongly along weakly identified directions.\n\nB. With an isotropic prior covariance $\\boldsymbol{\\Sigma}_{0} = \\tau^{2}\\mathbf{I}_{p}$ for some $\\tau^{2} > 0$, the posterior variance is identical across all directions regardless of the values of $s_{i}$, so aligning the prior with principal components is unnecessary.\n\nC. To regularize weakly identified directions of $\\boldsymbol{\\beta}$, align the prior covariance with $\\mathbf{U}$ by taking $\\boldsymbol{\\Sigma}_{0} = \\mathbf{U}\\,\\mathrm{diag}(\\alpha_{1},\\ldots,\\alpha_{n})\\,\\mathbf{U}^{\\top}$ and set smaller $\\alpha_{i}$ for smaller $s_{i}$.\n\nD. Increasing the prior variance along directions with small $s_{i}$ (i.e., taking larger $\\alpha_{i}$ when $s_{i}$ is small) reduces posterior variance along those directions because a broader prior enhances certainty about parameters in weakly identified directions.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Model**: Linear model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$.\n- **Response**: $\\mathbf{y} \\in \\mathbb{R}^{n}$.\n- **Design Matrix**: $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$.\n- **Parameter Vector**: $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$.\n- **Noise Distribution**: $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$. This defines the likelihood.\n- **Prior Distribution**: $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}_{0})$, where $\\boldsymbol{\\Sigma}_{0}$ is positive definite.\n- **SVD of X**: $\\mathbf{X} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^{\\top}$, with $\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{V} \\in \\mathbb{R}^{p \\times p}$ being orthogonal matrices, and $\\mathbf{S} \\in \\mathbb{R}^{n \\times p}$ containing singular values $s_{1} \\geq s_{2} \\geq \\cdots \\geq s_{p} \\geq 0$.\n- **Condition**: $\\mathbf{X}^{\\top}\\mathbf{X}$ is nearly singular, meaning $s_{p} \\approx 0$.\n- **Weakly Identified Direction**: The right singular vector $\\mathbf{v}_{p}$ (the $p$-th column of $\\mathbf{V}$) corresponds to the small singular value $s_{p}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard setup for Bayesian linear regression. The concepts used, such as Gaussian likelihood, Gaussian prior, singular value decomposition (SVD), and the relationship between singular values and matrix singularity, are fundamental and correctly stated in statistics and linear algebra. The principal directions of $\\mathbf{X}^{\\top}\\mathbf{X}$ are indeed its eigenvectors, which are the columns of $\\mathbf{V}$ from the SVD of $\\mathbf{X}$. The problem is well-posed, as it asks for a standard analysis of the posterior distribution and the design of a prior for regularization, a core topic in statistical learning. The language is precise and objective. The problem is self-contained and free from scientific or logical inconsistencies.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. The derivation of the solution can now proceed.\n\n### Derivation of the Posterior Distribution\n\nThe likelihood for $\\boldsymbol{\\beta}$, given $\\mathbf{y}$ and $\\sigma^2$, is derived from the noise distribution:\n$$ p(\\mathbf{y}|\\boldsymbol{\\beta}, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^{\\top}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\right) $$\nThe prior on $\\boldsymbol{\\beta}$ is:\n$$ p(\\boldsymbol{\\beta}|\\boldsymbol{\\Sigma}_0) = (2\\pi)^{-p/2}|\\boldsymbol{\\Sigma}_0|^{-1/2} \\exp\\left(-\\frac{1}{2}\\boldsymbol{\\beta}^{\\top}\\boldsymbol{\\Sigma}_{0}^{-1}\\boldsymbol{\\beta}\\right) $$\nAccording to Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior:\n$$ p(\\boldsymbol{\\beta}|\\mathbf{y}, \\sigma^2, \\boldsymbol{\\Sigma}_0) \\propto p(\\mathbf{y}|\\boldsymbol{\\beta}, \\sigma^2) p(\\boldsymbol{\\beta}|\\boldsymbol{\\Sigma}_0) $$\nThe exponent of the posterior distribution is the sum of the exponents of the likelihood and prior (ignoring constants):\n$$ -\\frac{1}{2\\sigma^2}(\\mathbf{y}^{\\top}\\mathbf{y} - 2\\mathbf{y}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\beta}^{\\top}\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta}) - \\frac{1}{2}\\boldsymbol{\\beta}^{\\top}\\boldsymbol{\\Sigma}_{0}^{-1}\\boldsymbol{\\beta} $$\n$$ = -\\frac{1}{2} \\left[ \\boldsymbol{\\beta}^{\\top}\\left(\\frac{1}{\\sigma^2}\\mathbf{X}^{\\top}\\mathbf{X} + \\boldsymbol{\\Sigma}_{0}^{-1}\\right)\\boldsymbol{\\beta} - \\frac{2}{\\sigma^2}\\mathbf{y}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} \\right] + \\text{const.} $$\nThis is a quadratic form in $\\boldsymbol{\\beta}$, which implies the posterior is also Gaussian, $\\boldsymbol{\\beta}|\\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{post}}, \\boldsymbol{\\Sigma}_{\\text{post}})$. By completing the square or comparing with the general form of the multivariate Gaussian density, we can identify the posterior precision matrix (the inverse of the covariance matrix) as:\n$$ \\boldsymbol{\\Sigma}_{\\text{post}}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{X}^{\\top}\\mathbf{X} + \\boldsymbol{\\Sigma}_{0}^{-1} $$\nThe posterior covariance matrix is therefore:\n$$ \\boldsymbol{\\Sigma}_{\\text{post}} = \\left(\\frac{1}{\\sigma^2}\\mathbf{X}^{\\top}\\mathbf{X} + \\boldsymbol{\\Sigma}_{0}^{-1}\\right)^{-1} $$\n\n### Analysis of Posterior Covariance along Principal Directions\n\nThe principal directions of $\\mathbf{X}^{\\top}\\mathbf{X}$ are its eigenvectors, which are the columns of $\\mathbf{V}$, denoted by $\\mathbf{v}_i$. We use the SVD of $\\mathbf{X}$ to analyze $\\mathbf{X}^{\\top}\\mathbf{X}$:\n$$ \\mathbf{X}^{\\top}\\mathbf{X} = (\\mathbf{U S V}^{\\top})^{\\top}(\\mathbf{U S V}^{\\top}) = \\mathbf{V S}^{\\top}\\mathbf{U}^{\\top}\\mathbf{U S V}^{\\top} = \\mathbf{V} (\\mathbf{S}^{\\top}\\mathbf{S}) \\mathbf{V}^{\\top} $$\nwhere $\\mathbf{S}^{\\top}\\mathbf{S}$ is a $p \\times p$ diagonal matrix with entries $s_i^2$. Therefore, $\\mathbf{X}^{\\top}\\mathbf{X} = \\mathbf{V} \\mathrm{diag}(s_1^2, \\dots, s_p^2) \\mathbf{V}^{\\top}$.\n\nThe problem suggests designing a prior covariance aligned with these directions. Such a prior has the form $\\boldsymbol{\\Sigma}_0 = \\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{V}^{\\top}$, where $\\boldsymbol{\\Lambda}$ is a diagonal matrix of prior variances along the directions $\\mathbf{v}_i$. Let $\\boldsymbol{\\Lambda} = \\mathrm{diag}(\\alpha_1, \\dots, \\alpha_p)$.\nThe inverse of the prior covariance is $\\boldsymbol{\\Sigma}_0^{-1} = \\mathbf{V} \\mathrm{diag}(\\alpha_1^{-1}, \\dots, \\alpha_p^{-1}) \\mathbf{V}^{\\top}$.\n\nSubstituting these into the posterior precision matrix formula:\n$$ \\boldsymbol{\\Sigma}_{\\text{post}}^{-1} = \\frac{1}{\\sigma^2} \\mathbf{V} \\mathrm{diag}(s_i^2) \\mathbf{V}^{\\top} + \\mathbf{V} \\mathrm{diag}(\\alpha_i^{-1}) \\mathbf{V}^{\\top} $$\n$$ \\boldsymbol{\\Sigma}_{\\text{post}}^{-1} = \\mathbf{V} \\left( \\frac{1}{\\sigma^2}\\mathrm{diag}(s_i^2) + \\mathrm{diag}(\\alpha_i^{-1}) \\right) \\mathbf{V}^{\\top} = \\mathbf{V} \\mathrm{diag}\\left(\\frac{s_i^2}{\\sigma^2} + \\frac{1}{\\alpha_i}\\right) \\mathbf{V}^{\\top} $$\nThe posterior covariance matrix is the inverse of this expression:\n$$ \\boldsymbol{\\Sigma}_{\\text{post}} = \\left(\\mathbf{V} \\mathrm{diag}\\left(\\frac{s_i^2}{\\sigma^2} + \\frac{1}{\\alpha_i}\\right) \\mathbf{V}^{\\top}\\right)^{-1} = \\mathbf{V} \\mathrm{diag}\\left(\\left(\\frac{s_i^2}{\\sigma^2} + \\frac{1}{\\alpha_i}\\right)^{-1}\\right) \\mathbf{V}^{\\top} $$\nThe posterior variance of the parameter component along the direction $\\mathbf{v}_j$ is given by $\\mathrm{Var}(\\mathbf{v}_j^{\\top}\\boldsymbol{\\beta} | \\mathbf{y}) = \\mathbf{v}_j^{\\top} \\boldsymbol{\\Sigma}_{\\text{post}} \\mathbf{v}_j$. This simplifies to the $j$-th diagonal element of the diagonal matrix:\n$$ \\mathrm{Var}_j^{\\text{post}} = \\left(\\frac{s_j^2}{\\sigma^2} + \\frac{1}{\\alpha_j}\\right)^{-1} $$\nNow we can analyze the behavior:\n1.  **Weakly identified directions ($s_j \\approx 0$):** The posterior variance becomes $\\mathrm{Var}_j^{\\text{post}} \\approx \\left(\\frac{1}{\\alpha_j}\\right)^{-1} = \\alpha_j$. The posterior variance is controlled by the prior variance $\\alpha_j$. To regularize this direction and prevent high variance, we must choose a **small** value for $\\alpha_j$. This enforces a strong prior belief that the component of $\\boldsymbol{\\beta}$ in this direction is close to $0$.\n2.  **Well-identified directions ($s_j \\gg 0$):** The term $s_j^2/\\sigma^2$ is large. The posterior variance becomes $\\mathrm{Var}_j^{\\text{post}} \\approx \\left(\\frac{s_j^2}{\\sigma^2}\\right)^{-1} = \\frac{\\sigma^2}{s_j^2}$. The posterior variance is small and determined primarily by the data. To ensure the prior does not distort this data-driven inference, we should choose a large $\\alpha_j$, which corresponds to a diffuse (uninformative) prior for this component.\n\n### Option-by-Option Analysis\n\n**A. In the limit of a diffuse prior, the posterior variance along $\\mathbf{v}_{i}$ is large when $s_{i}$ is small, potentially diverging as $s_{i} \\to 0$. To control this, choose a prior covariance aligned with $\\mathbf{V}$, namely $\\boldsymbol{\\Sigma}_{0} = \\mathbf{V}\\,\\mathrm{diag}(\\alpha_{1},\\ldots,\\alpha_{p})\\,\\mathbf{V}^{\\top}$, and set smaller $\\alpha_{i}$ for directions with smaller $s_{i}$ to shrink most strongly along weakly identified directions.**\n- **First statement:** A diffuse prior corresponds to $\\alpha_j \\to \\infty$ for all $j$. In this case, $1/\\alpha_j \\to 0$. The posterior variance is $\\mathrm{Var}_j^{\\text{post}} = \\sigma^2/s_j^2$. As $s_j \\to 0$, this variance diverges. This statement is correct.\n- **Proposed prior:** The form $\\boldsymbol{\\Sigma}_{0} = \\mathbf{V}\\,\\mathrm{diag}(\\alpha_{1},\\ldots,\\alpha_{p})\\,\\mathbf{V}^{\\top}$ is correctly aligned with the principal directions of $\\mathbf{X}^{\\top}\\mathbf{X}$.\n- **Regularization strategy:** As derived above, for a weakly identified direction with small $s_j$, the posterior variance is $\\approx \\alpha_j$. To control it (i.e., keep it small), we must choose a small $\\alpha_j$. \"Shrinking most strongly\" means applying a strong prior, which corresponds to a small prior variance. Thus, setting smaller $\\alpha_i$ for smaller $s_i$ is the correct strategy.\n- **Verdict:** **Correct**.\n\n**B. With an isotropic prior covariance $\\boldsymbol{\\Sigma}_{0} = \\tau^{2}\\mathbf{I}_{p}$ for some $\\tau^{2} > 0$, the posterior variance is identical across all directions regardless of the values of $s_{i}$, so aligning the prior with principal components is unnecessary.**\n- This corresponds to setting $\\alpha_j = \\tau^2$ for all $j=1,\\dots,p$ in our general formula. The posterior variance along direction $\\mathbf{v}_j$ is $\\mathrm{Var}_j^{\\text{post}} = \\left(\\frac{s_j^2}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1}$. This value explicitly depends on $s_j$. For example, if $s_1 > s_p$, then $\\mathrm{Var}_1^{\\text{post}} < \\mathrm{Var}_p^{\\text{post}}$. The statement that the posterior variance is identical across all directions is false. This form of prior corresponds to ridge regression, which regularizes all directions equally, but the resulting posterior variances are not equal.\n- **Verdict:** **Incorrect**.\n\n**C. To regularize weakly identified directions of $\\boldsymbol{\\beta}$, align the prior covariance with $\\mathbf{U}$ by taking $\\boldsymbol{\\Sigma}_{0} = \\mathbf{U}\\,\\mathrm{diag}(\\alpha_{1},\\ldots,\\alpha_{n})\\,\\mathbf{U}^{\\top}$ and set smaller $\\alpha_{i}$ for smaller $s_{i}$.**\n- The parameter vector $\\boldsymbol{\\beta}$ is in $\\mathbb{R}^p$, so its covariance matrix $\\boldsymbol{\\Sigma}_0$ must be a $p \\times p$ matrix. The matrix $\\mathbf{U}$ is $n \\times n$. The expression $\\mathbf{U}\\,\\mathrm{diag}(\\alpha_{1},\\ldots,\\alpha_{n})\\,\\mathbf{U}^{\\top}$ defines an $n \\times n$ matrix. This is a dimensional mismatch. Conceptually, the principal directions for the parameter space $\\mathbb{R}^p$ are given by the columns of $\\mathbf{V}$, not $\\mathbf{U}$, which spans the data space $\\mathbb{R}^n$.\n- **Verdict:** **Incorrect**.\n\n**D. Increasing the prior variance along directions with small $s_{i}$ (i.e., taking larger $\\alpha_{i}$ when $s_{i}$ is small) reduces posterior variance along those directions because a broader prior enhances certainty about parameters in weakly identified directions.**\n- As derived for weakly identified directions ($s_j \\approx 0$), the posterior variance is approximately equal to the prior variance, $\\mathrm{Var}_j^{\\text{post}} \\approx \\alpha_j$. Increasing the prior variance $\\alpha_j$ would therefore *increase* the posterior variance. A broader prior (larger variance) represents *less* prior information and thus *less* certainty. The claim that it \"reduces posterior variance\" and \"enhances certainty\" is a direct contradiction of the principles of Bayesian inference.\n- **Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "One of the great strengths of the Bayesian paradigm is its coherent framework for model comparison. This final practice moves beyond parameter inference within a single model to the task of selecting between different models. Through a hands-on coding exercise, you will compute and compare the marginal likelihood for models with Gaussian versus heavy-tailed Student's-$t$ noise, quantifying the evidence in favor of a model that is more robust to outliers .",
            "id": "3103065",
            "problem": "Consider Bayesian linear regression (BLR) for a single coefficient parameter. Let there be $n$ observations, a design vector $x \\in \\mathbb{R}^n$, a response vector $y \\in \\mathbb{R}^n$, and a scalar weight $w \\in \\mathbb{R}$. The BLR model assumes a prior distribution for $w$ and a noise model for the residuals. The marginal likelihood (also called model evidence) is defined as the integral of the likelihood multiplied by the prior over the parameter space, namely $p(y \\mid x) = \\int p(y \\mid x, w) \\, p(w) \\, dw$. The goal is to evaluate the sensitivity of the marginal likelihood to outliers under Gaussian noise and then propose and assess a robust alternative that uses a heavy-tailed noise model.\n\nUse the following foundational base:\n- The definition of Bayesian linear regression: $y_i = x_i w + \\varepsilon_i$ for $i = 1, \\dots, n$.\n- The Gaussian (normal) distribution with probability density function $p(z) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\!\\left(-\\dfrac{(z - \\mu)^2}{2 \\sigma^2}\\right)$.\n- The Student's $t$ distribution with degrees of freedom $\\nu$, location $\\mu$, and scale $s$, with probability density function $p(z) = \\dfrac{\\Gamma\\!\\left(\\dfrac{\\nu + 1}{2}\\right)}{\\Gamma\\!\\left(\\dfrac{\\nu}{2}\\right) \\sqrt{\\nu \\pi} s} \\left(1 + \\dfrac{1}{\\nu} \\left(\\dfrac{z - \\mu}{s}\\right)^2 \\right)^{-\\frac{\\nu + 1}{2}}$.\n- The Gaussian prior for the weight: $p(w) = \\mathcal{N}(w \\mid 0, \\tau^2)$, with variance parameter $\\tau^2 > 0$.\n\nTasks:\n1. Under Gaussian noise with known variance $\\sigma^2$, treat the likelihood as $p(y \\mid x, w) = \\prod_{i=1}^n \\mathcal{N}\\!\\left(y_i \\mid x_i w, \\sigma^2\\right)$ and the prior as $p(w) = \\mathcal{N}\\!\\left(w \\mid 0, \\tau^2\\right)$. Compute the marginal likelihood $p(y \\mid x)$ by integrating out $w$. You must start from the definition $p(y \\mid x) = \\int p(y \\mid x, w) \\, p(w) \\, dw$ and use valid linear algebra identities to evaluate the integral.\n2. Propose a robust alternative by replacing the Gaussian noise with a heavy-tailed Student's $t$ noise model with degrees of freedom $\\nu$ and scale $s$: $p(y \\mid x, w) = \\prod_{i=1}^n \\mathrm{StudentT}_\\nu\\!\\left(y_i \\mid x_i w, s\\right)$. Keep the same Gaussian prior $p(w) = \\mathcal{N}\\!\\left(w \\mid 0, \\tau^2\\right)$. Compute the marginal likelihood $p(y \\mid x)$ by numerically integrating $p(y \\mid x, w) \\, p(w)$ over $w \\in \\mathbb{R}$.\n3. Quantify the gain in model evidence due to robustness by computing the difference in log marginal likelihoods, defined as $\\Delta = \\log p_{\\mathrm{StudentT}}(y \\mid x) - \\log p_{\\mathrm{Gaussian}}(y \\mid x)$.\n\nUse the following test suite with specified parameters to evaluate the sensitivity to outliers and the robustness gains. For each case, $x$ and $y$ are fixed arrays, and the hyperparameters are scalars:\n\n- Case A (baseline, no outlier):\n  - $x = [0.0, \\; 0.5, \\; 1.0, \\; 1.5, \\; 2.0]$\n  - $y = [0.0, \\; 1.0, \\; 2.0, \\; 3.0, \\; 4.0]$\n  - $\\sigma = 1.0$, $\\tau = 5.0$, $\\nu = 3$, $s = 1.0$\n- Case B (single outlier):\n  - $x = [0.0, \\; 0.5, \\; 1.0, \\; 1.5, \\; 2.0]$\n  - $y = [0.0, \\; 1.0, \\; 2.0, \\; 3.0, \\; 12.0]$\n  - $\\sigma = 1.0$, $\\tau = 5.0$, $\\nu = 3$, $s = 1.0$\n- Case C (boundary condition, heavy tails approach Gaussian):\n  - $x = [0.0, \\; 0.5, \\; 1.0, \\; 1.5, \\; 2.0]$\n  - $y = [0.0, \\; 1.0, \\; 2.0, \\; 3.0, \\; 12.0]$\n  - $\\sigma = 1.0$, $\\tau = 5.0$, $\\nu = 100$, $s = 1.0$\n\nFor each case, compute $\\Delta$ as a real number. Your program should produce a single line of output containing the three $\\Delta$ values corresponding to Cases A, B, and C, as a comma-separated list enclosed in square brackets (for example, $[\\Delta_A,\\Delta_B,\\Delta_C]$). No physical units or angles apply. The numerical output should be deterministically computed from the provided inputs without any randomness and expressed as floating-point numbers.",
            "solution": "The user-provided problem has been analyzed and validated. It is scientifically sound, self-contained, well-posed, and objective. The tasks are clearly defined and relate directly to the core concepts of Bayesian model comparison in statistical learning. There are no identifiable flaws.\n\n### 1. Marginal Likelihood for the Gaussian Noise Model\n\nThe first task is to compute the marginal likelihood $p(y \\mid x)$ for a Bayesian linear regression model with Gaussian noise and a Gaussian prior on the weight parameter $w$.\n\nThe model is defined as $y_i = x_i w + \\varepsilon_i$, where the noise terms $\\varepsilon_i$ are independent and identically distributed according to a Gaussian distribution, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. In vector form, this is $y = xw + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nThe likelihood of the data $y$ given the weight $w$ is a multivariate Gaussian distribution:\n$$p(y \\mid x, w) = \\mathcal{N}(y \\mid xw, \\sigma^2 I_n)$$\n\nThe prior distribution for the scalar weight $w$ is also Gaussian:\n$$p(w) = \\mathcal{N}(w \\mid 0, \\tau^2)$$\n\nThe marginal likelihood, or model evidence, $p(y \\mid x)$, is obtained by integrating the product of the likelihood and the prior over all possible values of $w$:\n$$p(y \\mid x) = \\int_{-\\infty}^{\\infty} p(y \\mid x, w) p(w) \\, dw$$\n\nThis integral can be solved analytically. Since $y$ is a linear function of the Gaussian random variable $w$ plus independent Gaussian noise $\\varepsilon$, the resulting distribution for $y$ is also Gaussian. We can find its parameters by computing its mean and covariance.\n\nThe mean of $y$ is:\n$$E[y] = E[xw + \\varepsilon] = E[xw] + E[\\varepsilon] = x E[w] + 0$$\nSince the prior mean $E[w]$ is $0$, the marginal mean of $y$ is also $0$.\n$$E[y] = x \\cdot 0 = 0$$\n\nThe covariance of $y$ is:\n$$\\text{Cov}(y) = \\text{Cov}(xw + \\varepsilon)$$\nBecause $w$ and $\\varepsilon$ are independent, the covariance of their sum is the sum of their covariances:\n$$\\text{Cov}(y) = \\text{Cov}(xw) + \\text{Cov}(\\varepsilon)$$\nThe covariance of the noise is given as $\\text{Cov}(\\varepsilon) = \\sigma^2 I_n$.\nThe covariance of the term $xw$ is:\n$$\\text{Cov}(xw) = E[(xw)(xw)^T] - E[xw]E[xw]^T = E[w^2 xx^T] - (x E[w])(x E[w])^T$$\nSince $E[w]=0$, $E[w^2] = \\text{Var}(w) + (E[w])^2 = \\tau^2 + 0^2 = \\tau^2$.\nSo, $\\text{Cov}(xw) = xx^T E[w^2] = \\tau^2 xx^T$.\n\nCombining these results, the covariance matrix of $y$, denoted $\\Sigma_y$, is:\n$$\\Sigma_y = \\tau^2 xx^T + \\sigma^2 I_n$$\n\nThus, the marginal distribution of $y$ is a multivariate Gaussian with a mean of $0$ and covariance $\\Sigma_y$:\n$$p(y \\mid x) = \\mathcal{N}(y \\mid 0, \\Sigma_y)$$\nThe log-probability density is given by:\n$$\\log p(y \\mid x) = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det(\\Sigma_y) - \\frac{1}{2} y^T \\Sigma_y^{-1} y$$\n\nTo evaluate this, we need the determinant and inverse of $\\Sigma_y$. The matrix $\\Sigma_y$ has the form $cI + uv^T$. We can use the matrix determinant lemma and the Sherman-Morrison formula.\nLet $c=\\sigma^2$, $u=\\tau x$, and $v=\\tau x$.\n\nThe determinant is:\n$$\\det(\\Sigma_y) = \\det(\\sigma^2 I_n + (\\tau x)(\\tau x)^T) = (\\sigma^2)^n \\left(1 + \\frac{1}{\\sigma^2}(\\tau x)^T(\\tau x)\\right) = (\\sigma^2)^n \\left(1 + \\frac{\\tau^2}{\\sigma^2}x^T x\\right) = (\\sigma^2)^{n-1} (\\sigma^2 + \\tau^2 x^T x)$$\n\nThe inverse is:\n$$\\Sigma_y^{-1} = (\\sigma^2 I_n + \\tau^2 xx^T)^{-1} = \\frac{1}{\\sigma^2}I_n - \\frac{\\frac{\\tau^2}{\\sigma^4}xx^T}{1+\\frac{\\tau^2}{\\sigma^2}x^Tx} = \\frac{1}{\\sigma^2}I_n - \\frac{\\tau^2 xx^T}{\\sigma^2(\\sigma^2 + \\tau^2 x^T x)}$$\n\nThe quadratic term becomes:\n$$y^T \\Sigma_y^{-1} y = y^T \\left( \\frac{1}{\\sigma^2}I_n - \\frac{\\tau^2 xx^T}{\\sigma^2(\\sigma^2 + \\tau^2 x^T x)} \\right) y = \\frac{y^T y}{\\sigma^2} - \\frac{\\tau^2 (y^T x)(x^T y)}{\\sigma^2(\\sigma^2 + \\tau^2 x^T x)} = \\frac{y^T y}{\\sigma^2} - \\frac{\\tau^2 (x^T y)^2}{\\sigma^2(\\sigma^2 + \\tau^2 x^T x)}$$\n\nSubstituting these into the expression for the log marginal likelihood gives the final analytical formula used for computation.\n\n### 2. Marginal Likelihood for the Student's $t$ Noise Model\n\nThe second task replaces the Gaussian noise model with a heavy-tailed Student's $t$ distribution, while retaining the Gaussian prior on $w$.\n\nThe likelihood is now a product of Student's $t$ probability densities:\n$$p(y \\mid x, w) = \\prod_{i=1}^n \\mathrm{StudentT}_\\nu(y_i \\mid x_i w, s)$$\nwhere $\\mathrm{StudentT}_\\nu(z \\mid \\mu, s)$ is the PDF given in the problem statement. The prior remains $p(w) = \\mathcal{N}(w \\mid 0, \\tau^2)$.\n\nThe marginal likelihood is again the integral over $w$:\n$$p_{\\mathrm{StudentT}}(y \\mid x) = \\int_{-\\infty}^{\\infty} \\left( \\prod_{i=1}^n \\mathrm{StudentT}_\\nu(y_i \\mid x_i w, s) \\right) \\mathcal{N}(w \\mid 0, \\tau^2) \\, dw$$\n\nThis combination of a Student's $t$ likelihood and a Gaussian prior is not conjugate, meaning the integral does not have a simple closed-form solution. Therefore, it must be evaluated numerically.\n\nLet the integrand be $f(w) = p(y \\mid x, w) p(w)$. To avoid numerical underflow, it is standard practice to work with the logarithm of the integrand, $\\log f(w) = \\log p(y \\mid x, w) + \\log p(w)$. This function can have a very sharp peak and decay rapidly, which poses a challenge for standard numerical quadrature methods.\n\nTo ensure a stable and accurate computation, we adopt the following strategy:\n1.  Find the mode of the integrand, $w_{map}$, by numerically minimizing the negative log-integrand, $-\\log f(w)$. This value $w_{map}$ is the maximum a posteriori (MAP) estimate of $w$.\n2.  Let $L_{max} = \\log f(w_{map})$ be the maximum value of the log-integrand.\n3.  Rewrite the integral by factoring out the peak value:\n    $$p(y \\mid x) = \\int_{-\\infty}^{\\infty} e^{\\log f(w)} dw = \\int_{-\\infty}^{\\infty} e^{\\log f(w) - L_{max} + L_{max}} dw = e^{L_{max}} \\int_{-\\infty}^{\\infty} e^{\\log f(w) - L_{max}} dw$$\n4.  The new integrand, $g(w) = e^{\\log f(w) - L_{max}}$, is numerically stable. Its maximum value is $1$ (at $w=w_{map}$), and it smoothly decays to $0$. This function is suitable for a standard numerical integration routine like `scipy.integrate.quad`.\n5.  The final log marginal likelihood is calculated as:\n    $$\\log p_{\\mathrm{StudentT}}(y \\mid x) = L_{max} + \\log \\left( \\int_{-\\infty}^{\\infty} g(w) \\, dw \\right)$$\n\n### 3. Quantifying the Gain in Model Evidence\n\nThe final task is to compute the difference in log marginal likelihoods:\n$$\\Delta = \\log p_{\\mathrm{StudentT}}(y \\mid x) - \\log p_{\\mathrm{Gaussian}}(y \\mid x)$$\n\nThis quantity, also known as the log Bayes factor, quantifies the evidence in favor of the Student's $t$ model relative to the Gaussian model. A positive value for $\\Delta$ indicates that the data are better explained by the model with Student's $t$ noise. This is expected in cases with outliers, as the heavy tails of the Student's $t$ distribution can accommodate extreme values with a smaller penalty to the overall likelihood compared to the Gaussian distribution. The computation is performed for three distinct test cases to observe this effect.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, norm\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize_scalar\n\ndef log_marginal_likelihood_gaussian(x: np.ndarray, y: np.ndarray, sigma: float, tau: float) -> float:\n    \"\"\"\n    Computes the log marginal likelihood for a Bayesian linear regression model\n    with a single weight, Gaussian likelihood, and Gaussian prior.\n    \n    Args:\n        x (np.ndarray): Design vector of shape (n,).\n        y (np.ndarray): Response vector of shape (n,).\n        sigma (float): Standard deviation of the Gaussian noise.\n        tau (float): Standard deviation of the Gaussian prior on the weight.\n\n    Returns:\n        float: The log marginal likelihood, log p(y|x).\n    \"\"\"\n    n = len(x)\n    sigma2 = sigma**2\n    tau2 = tau**2\n    \n    xTx = np.dot(x, x)\n    yTy = np.dot(y, y)\n    xTy = np.dot(x, y)\n    \n    # Log-determinant of the marginal covariance matrix Sigma_y\n    # det(Sigma_y) = (sigma^2)^(n-1) * (sigma^2 + tau^2 * x^T*x)\n    # The term sigma2 + tau2 * xTx could be zero or negative if parameters are not positive.\n    # The problem constraints ensure this is not an issue.\n    log_det_sigma_y = (n - 1) * np.log(sigma2) + np.log(sigma2 + tau2 * xTx)\n    \n    # Quadratic form term y^T * Sigma_y^-1 * y\n    # y^T*Sigma_y^-1*y = y^T*y/sigma^2 - (tau^2 * (y^T*x)^2) / (sigma^2 * (sigma^2 + tau^2 * x^T*x))\n    quad_term = yTy / sigma2 - (tau2 * xTy**2) / (sigma2 * (sigma2 + tau2 * xTx))\n    \n    # Log marginal likelihood: log p(y|x) = -n/2*log(2*pi) - 0.5*log(det(Sigma_y)) - 0.5*y^T*Sigma_y^-1*y\n    log_p = -n / 2.0 * np.log(2 * np.pi) - 0.5 * log_det_sigma_y - 0.5 * quad_term\n    \n    return log_p\n\ndef log_marginal_likelihood_student_t(x: np.ndarray, y: np.ndarray, nu: float, s: float, tau: float) -> float:\n    \"\"\"\n    Computes the log marginal likelihood for a model with Student's t likelihood\n    and Gaussian prior using numerical integration.\n    \n    Args:\n        x (np.ndarray): Design vector of shape (n,).\n        y (np.ndarray): Response vector of shape (n,).\n        nu (float): Degrees of freedom for the Student's t distribution.\n        s (float): Scale parameter for the Student's t distribution.\n        tau (float): Standard deviation of the Gaussian prior on the weight.\n\n    Returns:\n        float: The log marginal likelihood, log p(y|x).\n    \"\"\"\n    \n    # Log of the integrand p(y|x,w)p(w)\n    def log_integrand(w: float) -> float:\n        log_likelihood = np.sum(t.logpdf(y, df=nu, loc=x * w, scale=s))\n        log_prior = norm.logpdf(w, loc=0, scale=tau)\n        return log_likelihood + log_prior\n\n    # To improve numerical stability, find the peak of the integrand (MAP estimate of w)\n    # by minimizing the negative log-integrand.\n    res = minimize_scalar(lambda w: -log_integrand(w))\n    w_map = res.x\n    log_max_val = log_integrand(w_map)\n    \n    # Define the scaled integrand for numerical integration.\n    # integrand = exp(log_integrand(w) - log_max_val)\n    def scaled_integrand(w: float) -> float:\n        log_val = log_integrand(w) - log_max_val\n        # np.exp can handle large negative inputs, but we can be explicit.\n        if log_val < -700: # Threshold for underflow\n             return 0.0\n        return np.exp(log_val)\n\n    # Numerically integrate the scaled function.\n    integral_val, _ = quad(scaled_integrand, -np.inf, np.inf)\n    \n    # Reconstruct the log marginal likelihood from the scaled integral.\n    # The integral of a positive function must be positive.\n    # A non-positive result would indicate a numerical error.\n    if integral_val <= 0:\n        return -np.inf\n    \n    return log_max_val + np.log(integral_val)\n\ndef calculate_delta(x_list: list, y_list: list, sigma: float, tau: float, nu: float, s: float) -> float:\n    \"\"\"\n    Calculates the difference in log marginal likelihoods between the Student's t\n    and Gaussian models.\n    \"\"\"\n    x = np.array(x_list)\n    y = np.array(y_list)\n\n    log_p_gaussian = log_marginal_likelihood_gaussian(x, y, sigma, tau)\n    log_p_student_t = log_marginal_likelihood_student_t(x, y, nu, s, tau)\n\n    delta = log_p_student_t - log_p_gaussian\n    return delta\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A: Baseline, no outlier\n        {\n            \"x\": [0.0, 0.5, 1.0, 1.5, 2.0],\n            \"y\": [0.0, 1.0, 2.0, 3.0, 4.0],\n            \"sigma\": 1.0, \"tau\": 5.0, \"nu\": 3, \"s\": 1.0\n        },\n        # Case B: Single outlier\n        {\n            \"x\": [0.0, 0.5, 1.0, 1.5, 2.0],\n            \"y\": [0.0, 1.0, 2.0, 3.0, 12.0],\n            \"sigma\": 1.0, \"tau\": 5.0, \"nu\": 3, \"s\": 1.0\n        },\n        # Case C: Boundary condition, heavy tails approach Gaussian\n        {\n            \"x\": [0.0, 0.5, 1.0, 1.5, 2.0],\n            \"y\": [0.0, 1.0, 2.0, 3.0, 12.0],\n            \"sigma\": 1.0, \"tau\": 5.0, \"nu\": 100, \"s\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        delta = calculate_delta(\n            x_list=case[\"x\"],\n            y_list=case[\"y\"],\n            sigma=case[\"sigma\"],\n            tau=case[\"tau\"],\n            nu=case[\"nu\"],\n            s=case[\"s\"]\n        )\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}