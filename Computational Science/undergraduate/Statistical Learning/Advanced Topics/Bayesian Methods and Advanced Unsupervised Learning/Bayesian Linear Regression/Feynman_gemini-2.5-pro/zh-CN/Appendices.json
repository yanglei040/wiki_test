{
    "hands_on_practices": [
        {
            "introduction": "贝叶斯线性回归的核心是推断模型权重 $w$ 的后验分布。这个练习将引导你通过一个具体的例子，完整地推导后验均值和协方差。通过一个具有正交特征的特殊数据集，本练习将揭示一个深刻的见解：当特征在数学上是正交的，我们对不同权重的后验信念在统计上是相互独立的，这极大地简化了模型的计算和解释。",
            "id": "3103067",
            "problem": "考虑贝叶斯线性回归（BLR），其中设计矩阵 $X \\in \\mathbb{R}^{N \\times D}$ 通过线性模型将权重向量 $w \\in \\mathbb{R}^{D}$ 映射到预测目标。观测到的目标向量为 $t \\in \\mathbb{R}^{N}$。假设高斯似然的噪声精度为 $\\beta > 0$，并且系数服从精度为 $\\alpha > 0$ 的独立零均值高斯先验。\n\n按如下方式构造一个具有标准正交列的特定数据集：令 $N=3$ 和 $D=2$，取\n$$\nX \\;=\\;\n\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix},\n\\qquad\nt \\;=\\;\n\\begin{pmatrix}\n4 \\\\ -1 \\\\ 2\n\\end{pmatrix}.\n$$\n验证 $X$ 的列是标准正交的。然后，从高斯似然 $p(t \\mid w, X, \\beta)$ 和高斯先验 $p(w \\mid \\alpha)$ 的定义出发，并应用贝叶斯定理 $p(w \\mid t, X, \\alpha, \\beta) \\propto p(t \\mid w, X, \\beta)\\,p(w \\mid \\alpha)$，推导 $w$ 的后验分布。在你的推导中，通过配方法来确定后验均值和后验协方差矩阵，并利用 $X$ 列的标准正交性简化后验协方差。\n\n根据你的推导，解释 $X$ 列的正交性如何解耦 $w$ 中各个系数的后验分布。\n\n最后，在 $\\alpha = 2$ 和 $\\beta = 3$ 的情况下，计算该数据集的精确后验协方差矩阵。将后验协方差矩阵作为你的最终答案报告。不需要四舍五入。",
            "solution": "该问题是有效的，因为它是贝叶斯线性回归中的一个标准练习，在科学上和数学上都是合理的，并为得出唯一解提供了所有必要信息。\n\n首先，我们验证矩阵 $X$ 的列是标准正交的。令这些列表示为 $x_1$ 和 $x_2$。\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\qquad x_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n标准正交性要求内积 $x_i^T x_j = \\delta_{ij}$，其中 $\\delta_{ij}$ 是克罗内克 δ。我们检查内积：\n$$\nx_1^T x_1 = (1)(1) + (0)(0) + (0)(0) = 1\n$$\n$$\nx_2^T x_2 = (0)(0) + (1)(1) + (0)(0) = 1\n$$\n$$\nx_1^T x_2 = (1)(0) + (0)(1) + (0)(0) = 0\n$$\n由于 $x_1^T x_1 = 1$，$x_2^T x_2 = 1$，且 $x_1^T x_2 = 0$，这些列是标准正交的。一个更简洁的验证方法是计算 $X^T X$：\n$$\nX^T X = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2\n$$\n由于 $X^T X = I_D$，其中 $D=2$ 是列数，根据定义，$X$ 的列是标准正交的。\n\n接下来，我们推导权重 $w$ 的后验分布。该模型由一个高斯似然和一个高斯先验指定。\n似然函数为 $p(t \\mid w, X, \\beta) = \\mathcal{N}(t \\mid Xw, \\beta^{-1}I_N)$，其中 $I_N$ 是 $N \\times N$ 的单位矩阵。其概率密度函数为：\n$$\np(t \\mid w, X, \\beta) \\propto \\exp\\left( -\\frac{\\beta}{2} (t - Xw)^T (t - Xw) \\right)\n$$\n权重的先验为 $p(w \\mid \\alpha) = \\mathcal{N}(w \\mid 0, \\alpha^{-1}I_D)$，其中 $I_D$ 是 $D \\times D$ 的单位矩阵。其概率密度函数为：\n$$\np(w \\mid \\alpha) \\propto \\exp\\left( -\\frac{\\alpha}{2} w^T w \\right)\n$$\n根据贝叶斯定理，后验分布与似然函数和先验分布的乘积成正比：\n$$\np(w \\mid t, X, \\alpha, \\beta) \\propto p(t \\mid w, X, \\beta) \\, p(w \\mid \\alpha)\n$$\n我们处理后验的对数，因为这能将指数的乘积简化为其参数的和：\n$$\n\\ln p(w \\mid t, X, \\alpha, \\beta) = -\\frac{\\beta}{2} (t - Xw)^T (t - Xw) - \\frac{\\alpha}{2} w^T w + \\text{const}\n$$\n我们展开二次项：\n$$\n(t - Xw)^T (t - Xw) = t^T t - t^T Xw - w^T X^T t + w^T X^T Xw = t^T t - 2w^T X^T t + w^T X^T Xw\n$$\n将此代回对数后验表达式：\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{\\beta}{2} (t^T t - 2w^T X^T t + w^T X^T Xw) - \\frac{\\alpha}{2} w^T w + \\text{const}\n$$\n我们对包含 $w$ 的项进行分组：\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{1}{2} w^T (\\beta X^T X + \\alpha I_D) w + \\beta w^T X^T t - \\frac{\\beta}{2} t^T t + \\text{const}\n$$\n舍去不依赖于 $w$ 的项：\n$$\n\\ln p(w \\mid t, \\dots) = -\\frac{1}{2} w^T (\\beta X^T X + \\alpha I_D) w + \\beta w^T X^T t + \\text{const'}\n$$\n这是一个关于 $w$ 的二次型，这意味着后验是一个高斯分布，$p(w \\mid t, \\dots) = \\mathcal{N}(w \\mid m_N, S_N)$。一个通用的多元高斯分布 $\\mathcal{N}(w \\mid m_N, S_N)$ 的对数密度为：\n$$\n\\ln \\mathcal{N}(w \\mid m_N, S_N) = -\\frac{1}{2} (w - m_N)^T S_N^{-1} (w - m_N) + \\text{const} = -\\frac{1}{2} (w^T S_N^{-1} w - 2w^T S_N^{-1} m_N + m_N^T S_N^{-1} m_N) + \\text{const}\n$$\n$$\n\\ln \\mathcal{N}(w \\mid m_N, S_N) = -\\frac{1}{2} w^T S_N^{-1} w + w^T S_N^{-1} m_N + \\text{const''}\n$$\n通过比较我们的对数后验表达式和通用对数高斯密度表达式中 $w$ 的二次项和线性项的系数，我们可以确定后验精度矩阵（协方差的逆）$S_N^{-1}$ 和均值 $m_N$。\n比较二次项 ($w^T(\\cdot)w$):\n$$\nS_N^{-1} = \\beta X^T X + \\alpha I_D\n$$\n因此，后验协方差矩阵为：\n$$\nS_N = (\\beta X^T X + \\alpha I_D)^{-1}\n$$\n比较线性项 ($w^T(\\cdot)$):\n$$\nS_N^{-1} m_N = \\beta X^T t \\implies m_N = S_N (\\beta X^T t) = \\beta (\\beta X^T X + \\alpha I_D)^{-1} X^T t\n$$\n这些是贝叶斯线性回归中后验均值和协方差的一般表达式。\n\n现在，我们利用 $X$ 的列是标准正交的这一性质，即 $X^T X = I_D$。将此代入 $S_N$ 和 $m_N$ 的表达式中：\n对于后验协方差 $S_N$：\n$$\nS_N = (\\beta I_D + \\alpha I_D)^{-1} = ((\\beta + \\alpha) I_D)^{-1} = \\frac{1}{\\alpha + \\beta} I_D\n$$\n对于后验均值 $m_N$：\n$$\nm_N = \\beta \\left(\\frac{1}{\\alpha + \\beta} I_D\\right) X^T t = \\frac{\\beta}{\\alpha + \\beta} X^T t\n$$\n$X$ 列的标准正交性显著简化了后验参数。后验协方差矩阵 $S_N = \\frac{1}{\\alpha + \\beta} I_D$ 是一个对角矩阵。在多元高斯分布中，对角协方差矩阵表示随机变量是不相关的。对于高斯变量，不相关等价于统计独立。\n联合后验密度为：\n$$\np(w \\mid t, \\dots) \\propto \\exp\\left(-\\frac{1}{2} (w-m_N)^T S_N^{-1} (w-m_N)\\right) = \\exp\\left(-\\frac{\\alpha+\\beta}{2} (w-m_N)^T (w-m_N)\\right)\n$$\n$$\n= \\exp\\left(-\\frac{\\alpha+\\beta}{2} \\sum_{j=1}^D (w_j - m_{N,j})^2\\right) = \\prod_{j=1}^D \\exp\\left(-\\frac{\\alpha+\\beta}{2} (w_j - m_{N,j})^2\\right)\n$$\n这表明联合后验 $p(w \\mid t, \\dots)$ 可以分解为每个系数 $w_j$ 的单个后验的乘积：$p(w_1, \\dots, w_D \\mid t, \\dots) = \\prod_{j=1}^D p(w_j \\mid t, \\dots)$。这种分解就是系数后验分布的“解耦”。每个 $w_j$ 都有一个单变量高斯后验分布，其均值为 $m_{N,j}$，方差为 $1/(\\alpha+\\beta)$。特征的标准正交性确保了学习一个权重 $w_j$ 不会提供关于任何其他权重 $w_k$ ($k \\neq j$) 的任何信息，超出了先验已知的信息。\n\n最后，我们针对给定的数据集，在 $\\alpha = 2$ 和 $\\beta = 3$ 的情况下，计算精确的后验协方差矩阵。我们使用从 $X$ 列的标准正交性推导出的简化公式。权重向量的维度是 $D=2$。\n$$\nS_N = \\frac{1}{\\alpha + \\beta} I_D = \\frac{1}{2 + 3} I_2 = \\frac{1}{5} I_2\n$$\n将其写成矩阵形式：\n$$\nS_N = \\frac{1}{5} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  \\frac{1}{5} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{5}  0 \\\\ 0  \\frac{1}{5} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在解决了基本概念之后，我们来关注一个实际应用中的主要挑战：计算成本，尤其是在特征数量 $p$ 远大于样本数量 $n$ 时。这个练习介绍了一个强大的线性代数工具——Woodbury 恒等式，它可以显著改变后验协方差矩阵的计算方式。通过这个练习，你将理解如何根据数据维度 $p$ 和 $n$ 的相对大小，策略性地选择计算公式，从而在“宽数据”场景下实现数量级的计算效率提升。",
            "id": "3103080",
            "problem": "考虑一个具有高斯似然和零均值各向同性高斯先验的贝叶斯线性回归问题。具体来说，设 $y \\in \\mathbb{R}^{n}$，$X \\in \\mathbb{R}^{n \\times p}$，以及 $w \\in \\mathbb{R}^{p}$，其中\n- $y \\mid w \\sim \\mathcal{N}(Xw, \\sigma^{2} I_{n})$，对于某个噪声方差 $\\sigma^{2} \\gt 0$，\n- $w \\sim \\mathcal{N}(0, \\alpha^{-1} I_{p})$，对于某个精度 $\\alpha \\gt 0$。\n\n根据高斯共轭性，后验概率 $p(w \\mid y)$ 是一个高斯分布，其协方差矩阵为 $S_{N} \\in \\mathbb{R}^{p \\times p}$。一种标准形式涉及到对一个 $p \\times p$ 矩阵求逆。请找出一个不采用该直接形式的 $S_{N}$ 的等价表达式，该表达式避免了直接对 $p \\times p$ 矩阵求逆，然后解释在 $p \\gg n$ 和 $n \\gg p$ 两种情况下其计算上的意义。假设使用稠密线性代数，朴素矩阵求逆的成本主要由矩阵维度的三次方时间决定。\n\n哪个选项是正确的？\n\nA. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\alpha^{-1} X^{\\top}\\, \\bigl(\\sigma^{2} I_{n} + X \\alpha^{-1} X^{\\top}\\bigr)^{-1}\\, X \\,\\alpha^{-1}$；当 $p \\gg n$ 时，这种形式是有利的，因为它将一个 $p \\times p$ 矩阵的求逆替换为了一个 $n \\times n$ 矩阵的求逆（成本为 $\\mathcal{O}(n^{3})$ 而不是 $\\mathcal{O}(p^{3})$）。当 $n \\gg p$ 时，原始的 $p \\times p$ 形式更可取。\n\nB. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\alpha^{-1} X^{\\top}\\, \\bigl(\\sigma^{-2} I_{n} + X \\alpha^{-1} X^{\\top}\\bigr)^{-1}\\, X \\,\\alpha^{-1}$；当 $n \\gg p$ 时，这种形式是有利的，因为它将一个 $n \\times n$ 矩阵的求逆替换为了一个 $p \\times p$ 矩阵的求逆，所以当 $p \\gg n$ 时应使用原始形式。\n\nC. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\alpha^{-2} X^{\\top}\\, \\bigl(I_{n} + (\\alpha \\sigma^{2})^{-1} X X^{\\top}\\bigr)^{-1}\\, X$；这完全消除了矩阵求逆，因此无论 $p \\gg n$ 还是 $n \\gg p$，形成 $S_{N}$ 的时间都是 $\\mathcal{O}(n p^{2})$。\n\nD. $S_{N} = \\alpha^{-1} I_{p} \\;-\\; \\sigma^{-2} X^{\\top}\\, \\bigl(I_{n} + (\\alpha \\sigma^{2})^{-1} X X^{\\top}\\bigr)^{-1}\\, X \\,\\alpha^{-1}$；这个表达式在数值上总是比对 $p \\times p$ 矩阵求逆更稳定，因此无论 $p$ 和 $n$ 的关系如何，都应始终被优先选择。",
            "solution": "问题陈述是贝叶斯线性回归的标准表述，具有科学依据、适定且客观。它包含了得出唯一解所需的所有信息。\n\n首先，我们将推导后验协方差矩阵 $S_N$ 的标准表达式。权重 $w$ 在给定数据 $y$ 下的后验概率由贝叶斯定理给出：\n$$p(w \\mid y) \\propto p(y \\mid w) p(w)$$\n似然 $p(y \\mid w)$ 以高斯分布的形式给出：\n$$p(y \\mid w) = \\mathcal{N}(y \\mid Xw, \\sigma^2 I_n) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - Xw)^\\top(y - Xw)\\right)$$\n先验 $p(w)$ 也是一个高斯分布：\n$$p(w) = \\mathcal{N}(w \\mid 0, \\alpha^{-1} I_p) \\propto \\exp\\left(-\\frac{1}{2\\alpha^{-1}} w^\\top w\\right) = \\exp\\left(-\\frac{\\alpha}{2} w^\\top w\\right)$$\n因此，后验概率与这两个指数函数的乘积成正比。在忽略常数项的情况下，后验分布的指数部分为：\n$$L(w) = -\\frac{1}{2\\sigma^2} (y - Xw)^\\top(y - Xw) - \\frac{\\alpha}{2} w^\\top w$$\n为了找到后验分布的形式（由于共轭性已知其为高斯分布），我们分析指数部分中关于 $w$ 的二次项和一次项：\n$$L(w) = -\\frac{1}{2\\sigma^2} (y^\\top y - 2y^\\top Xw + w^\\top X^\\top Xw) - \\frac{\\alpha}{2} w^\\top w$$\n$$L(w) = \\frac{1}{\\sigma^2} y^\\top Xw - \\frac{1}{2} w^\\top \\left(\\frac{1}{\\sigma^2} X^\\top X\\right) w - \\frac{1}{2} w^\\top (\\alpha I_p) w + \\text{const.}$$\n$$L(w) = -\\frac{1}{2} w^\\top \\left(\\alpha I_p + \\frac{1}{\\sigma^2} X^\\top X\\right) w + \\left(\\frac{1}{\\sigma^2} X^\\top y\\right)^\\top w + \\text{const.}$$\n对于 $w$ 的一个通用多元高斯分布，其对数概率形式为 $-\\frac{1}{2} (w-\\mu)^\\top \\Sigma^{-1} (w-\\mu) + \\text{const.}$，展开后为 $-\\frac{1}{2} w^\\top \\Sigma^{-1} w + \\mu^\\top \\Sigma^{-1} w + \\text{const.}$。\n通过比较二次项，我们可以确定后验协方差矩阵 $S_N$ 的逆：\n$$S_N^{-1} = \\alpha I_p + \\frac{1}{\\sigma^2} X^\\top X$$\n所以，后验协方差为：\n$$S_N = \\left(\\alpha I_p + \\frac{1}{\\sigma^2} X^\\top X\\right)^{-1}$$\n这是标准形式，它需要对一个 $p \\times p$ 的矩阵求逆，其中 $p$ 是特征的数量。\n\n为了找到一个替代表达式，我们使用 Woodbury 矩阵恒等式，其内容如下：\n$$(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}$$\n我们可以将我们表达式 $S_N^{-1}$ 中的项与形式 $A+UCV$ 进行匹配：\n令 $A = \\alpha I_p$，$U = X^\\top$，$C = \\frac{1}{\\sigma^2} I_n$，以及 $V = X$。\n相应的逆项是：\n$A^{-1} = (\\alpha I_p)^{-1} = \\alpha^{-1} I_p$。\n$C^{-1} = (\\frac{1}{\\sigma^2} I_n)^{-1} = \\sigma^2 I_n$。\n\n将这些代入 Woodbury 恒等式：\n$$S_N = (\\alpha I_p)^{-1} - (\\alpha I_p)^{-1} X^\\top \\left( (\\frac{1}{\\sigma^2} I_n)^{-1} + X (\\alpha I_p)^{-1} X^\\top \\right)^{-1} X (\\alpha I_p)^{-1}$$\n$$S_N = \\alpha^{-1} I_p - (\\alpha^{-1} I_p) X^\\top \\left( \\sigma^2 I_n + X (\\alpha^{-1} I_p) X^\\top \\right)^{-1} X (\\alpha^{-1} I_p)$$\n$$S_N = \\alpha^{-1} I_p - \\alpha^{-1} X^\\top \\left( \\sigma^2 I_n + \\alpha^{-1} X X^\\top \\right)^{-1} \\alpha^{-1} X$$\n由于 $\\alpha^{-1}$ 是一个标量，它可以自由移动。我们可以将乘以中间表达式的两个 $\\alpha^{-1}$ 项组合在一起：\n$$S_N = \\alpha^{-1} I_p - \\alpha^{-2} X^\\top \\left( \\sigma^2 I_n + \\alpha^{-1} X X^\\top \\right)^{-1} X$$\n这种替代形式需要对一个 $n \\times n$ 的矩阵 $(\\sigma^2 I_n + \\alpha^{-1} X X^\\top)$ 求逆，其中 $n$ 是数据点的数量。\n\n现在我们来分析计算成本。假设一个 $m \\times k$ 矩阵和一个 $k \\times l$ 矩阵的朴素矩阵乘法成本为 $\\mathcal{O}(mkl)$，一个 $m \\times m$ 矩阵的求逆成本为 $\\mathcal{O}(m^3)$。\n- 原始形式 $S_N = (\\alpha I_p + \\sigma^{-2} X^\\top X)^{-1}$ 需要计算 $X^\\top X$（成本为 $\\mathcal{O}(np^2)$）并对一个 $p \\times p$ 矩阵求逆（成本为 $\\mathcal{O}(p^3)$）。总成本主要由 $\\mathcal{O}(np^2 + p^3)$ 决定。\n- 替代形式需要计算 $XX^\\top$（成本为 $\\mathcal{O}(pn^2)$）并对一个 $n \\times n$ 矩阵求逆（成本为 $\\mathcal{O}(n^3)$）。总成本主要由 $\\mathcal{O}(pn^2 + n^3)$ 决定。\n\n让我们比较在两种指定情况下的成本：\n1.  **情况 $p \\gg n$（特征多，样本少）：**\n    - 原始形式的成本主要由 $\\mathcal{O}(p^3)$ 决定。\n    - 替代形式的成本主要由 $\\mathcal{O}(n^3)$ 决定（因为 $p > n$，$pn^2$ 小于 $p^3$，且 $n^3$ 远小于 $p^3$）。\n    - 由于 $p \\gg n$，我们有 $p^3 \\gg n^3$。因此，替代形式在计算上更有优势。它将一个大的 $p \\times p$ 矩阵求逆替换为一个小得多的 $n \\times n$ 矩阵求逆。\n2.  **情况 $n \\gg p$（特征少，样本多）：**\n    - 原始形式的成本在 $n$ 非常大时主要由形成 $X^\\top X$ 的 $\\mathcal{O}(np^2)$ 决定，但求逆的成本是 $\\mathcal{O}(p^3)$。\n    - 替代形式的成本主要由 $\\mathcal{O}(n^3)$ 决定。\n    - 由于 $n \\gg p$，我们有 $n^3 \\gg p^3$。在这种情况下，需要 $\\mathcal{O}(p^3)$ 求逆的原始形式远比需要 $\\mathcal{O}(n^3)$ 求逆的替代形式效率高得多。\n\n现在我们来评估每个选项。\n\nA. $S_N$ 的表达式是 $S_N = \\alpha^{-1}I_p - \\alpha^{-1}X^\\top(\\sigma^2I_n + (\\alpha^{-1}X)X^\\top)^{-1}(\\alpha^{-1}X) = \\alpha^{-1}I_p - \\alpha^{-2}X^\\top(\\sigma^2I_n + \\alpha^{-1}XX^\\top)^{-1}X$。这与我们推导的公式相符。\n- 计算分析也是正确的：对于 $p \\gg n$，替代形式更好，因为它涉及 $\\mathcal{O}(n^3)$ 的求逆而不是 $\\mathcal{O}(p^3)$。对于 $n \\gg p$，具有 $\\mathcal{O}(p^3)$ 求逆的原始形式更可取。\n- **结论：正确。**\n\nB. 公式不正确；它在逆矩阵内部包含了 $\\sigma^{-2}I_n$，而正确的应该是 $\\sigma^2I_n$。\n- 计算推理是反的。替代形式在 $p \\gg n$ 时有优势，而不是 $n \\gg p$。它用一个 $n \\times n$ 的逆替换一个 $p \\times p$ 的逆，而不是反过来。\n- **结论：不正确。**\n\nC. 公式不正确。从我们推导的表达式中提出因子 $\\sigma^2$ 得到 $S_N = \\alpha^{-1} I_p - \\alpha^{-2}\\sigma^{-2} X^\\top (I_n + (\\alpha \\sigma^2)^{-1} XX^\\top)^{-1} X$。该选项缺少了 $\\sigma^{-2}$ 这个因子。\n- 声称这“完全消除了矩阵求逆”是错误的。项 $(\\dots)^{-1}$ 清楚地表示一个 $n \\times n$ 矩阵的求逆。成本分析也是不正确的。\n- **结论：不正确。**\n\nD. 这个可以写成 $S_N = \\alpha^{-1} I_p - \\alpha^{-1}\\sigma^{-2} X^\\top (\\dots)^{-1} X$ 的公式是不正确的。如对 C 的分析所示，它应该是 $S_N = \\alpha^{-1} I_p - \\alpha^{-2}\\sigma^{-2} X^\\top (\\dots)^{-1} X$。它缺少了一个因子 $\\alpha^{-1}$。\n- 声称它“无论 $p$ 和 $n$ 的关系如何都应始终被优先选择”是错误的，正如我们的计算成本分析所证明的（$n \\gg p$ 使原始形式更好）。关于数值稳定性的说法没有根据，并且通常不成立。\n- **结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "现实世界的数据往往充满噪声，甚至包含一些可能严重误导模型的异常值。这个练习将展示贝叶斯方法如何应对这一挑战，即通过构建更稳健的概率模型（例如，使用重尾的 Student-t 分布代替高斯分布）。你将学习如何计算和比较不同模型的边缘似然（或称模型证据），从而量化地判断哪个模型能更好地解释含异常值的数据，这体现了贝叶斯模型选择和稳健性设计的核心思想。",
            "id": "3103065",
            "problem": "考虑单个系数参数的贝叶斯线性回归（BLR）。设有 $n$ 个观测值，一个设计向量 $x \\in \\mathbb{R}^n$，一个响应向量 $y \\in \\mathbb{R}^n$，以及一个标量权重 $w \\in \\mathbb{R}$。BLR模型假设了 $w$ 的先验分布以及残差的噪声模型。边缘似然（也称为模型证据）定义为似然函数乘以先验在参数空间上的积分，即 $p(y \\mid x) = \\int p(y \\mid x, w) \\, p(w) \\, dw$。目标是评估在高斯噪声下边缘似然对异常值的敏感性，然后提出并评估一个使用重尾噪声模型的稳健替代方案。\n\n使用以下基础知识：\n- 贝叶斯线性回归的定义：$y_i = x_i w + \\varepsilon_i$，$i = 1, \\dots, n$。\n- 高斯（正态）分布，其概率密度函数为 $p(z) = \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\!\\left(-\\dfrac{(z - \\mu)^2}{2 \\sigma^2}\\right)$。\n- 学生t分布，其自由度为 $\\nu$，位置为 $\\mu$，尺度为 $s$，概率密度函数为 $p(z) = \\dfrac{\\Gamma\\!\\left(\\dfrac{\\nu + 1}{2}\\right)}{\\Gamma\\!\\left(\\dfrac{\\nu}{2}\\right) \\sqrt{\\nu \\pi} s} \\left(1 + \\dfrac{1}{\\nu} \\left(\\dfrac{z - \\mu}{s}\\right)^2 \\right)^{-\\frac{\\nu + 1}{2}}$。\n- 权重的高斯先验：$p(w) = \\mathcal{N}(w \\mid 0, \\tau^2)$，方差参数 $\\tau^2 > 0$。\n\n任务：\n1. 在已知方差 $\\sigma^2$ 的高斯噪声下，将似然视为 $p(y \\mid x, w) = \\prod_{i=1}^n \\mathcal{N}\\!\\left(y_i \\mid x_i w, \\sigma^2\\right)$，先验视为 $p(w) = \\mathcal{N}\\!\\left(w \\mid 0, \\tau^2\\right)$。通过对 $w$ 进行积分消元来计算边缘似然 $p(y \\mid x)$。您必须从定义 $p(y \\mid x) = \\int p(y \\mid x, w) \\, p(w) \\, dw$ 开始，并使用有效的线性代数恒等式来计算该积分。\n2. 提出一个稳健的替代方案，用自由度为 $\\nu$、尺度为 $s$ 的重尾学生t噪声模型替换高斯噪声：$p(y \\mid x, w) = \\prod_{i=1}^n \\mathrm{StudentT}_\\nu\\!\\left(y_i \\mid x_i w, s\\right)$。保持相同的高斯先验 $p(w) = \\mathcal{N}\\!\\left(w \\mid 0, \\tau^2\\right)$。通过对 $w \\in \\mathbb{R}$ 上的 $p(y \\mid x, w) \\, p(w)$ 进行数值积分，计算边缘似然 $p(y \\mid x)$。\n3. 通过计算对数边缘似然的差异来量化由稳健性带来的模型证据增益，定义为 $\\Delta = \\log p_{\\mathrm{StudentT}}(y \\mid x) - \\log p_{\\mathrm{Gaussian}}(y \\mid x)$。\n\n使用以下具有指定参数的测试套件来评估对异常值的敏感性和稳健性增益。对于每种情况，$x$ 和 $y$ 是固定的数组，超参数是标量：\n\n- 情况A（基线，无异常值）：\n  - $x = [0.0, \\; 0.5, \\; 1.0, \\; 1.5, \\; 2.0]$\n  - $y = [0.0, \\; 1.0, \\; 2.0, \\; 3.0, \\; 4.0]$\n  - $\\sigma = 1.0$, $\\tau = 5.0$, $\\nu = 3$, $s = 1.0$\n- 情况B（单个异常值）：\n  - $x = [0.0, \\; 0.5, \\; 1.0, \\; 1.5, \\; 2.0]$\n  - $y = [0.0, \\; 1.0, \\; 2.0, \\; 3.0, \\; 12.0]$\n  - $\\sigma = 1.0$, $\\tau = 5.0$, $\\nu = 3$, $s = 1.0$\n- 情况C（边界条件，重尾分布接近高斯分布）：\n  - $x = [0.0, \\; 0.5, \\; 1.0, \\; 1.5, \\; 2.0]$\n  - $y = [0.0, \\; 1.0, \\; 2.0, \\; 3.0, \\; 12.0]$\n  - $\\sigma = 1.0$, $\\tau = 5.0$, $\\nu = 100$, $s = 1.0$\n\n对于每种情况，计算 $\\Delta$ 作为一个实数。您的程序应生成单行输出，其中包含对应于情况A、B和C的三个 $\\Delta$ 值，形式为用逗号分隔并用方括号括起来的列表（例如，$[\\Delta_A,\\Delta_B,\\Delta_C]$）。不适用任何物理单位或角度。数值输出应根据所提供的输入确定性地计算，不含任何随机性，并表示为浮点数。",
            "solution": "用户提供的问题已经过分析和验证。该问题在科学上是合理的、自洽的、适定的和客观的。任务定义清晰，并直接关系到统计学习中贝叶斯模型比较的核心概念。没有发现可识别的缺陷。\n\n### 1. 高斯噪声模型的边缘似然\n\n第一个任务是为一个具有高斯噪声和权重参数 $w$ 上的高斯先验的贝叶斯线性回归模型计算边缘似然 $p(y \\mid x)$。\n\n模型定义为 $y_i = x_i w + \\varepsilon_i$，其中噪声项 $\\varepsilon_i$ 是独立同分布的，服从高斯分布，$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。以向量形式表示，即为 $y = xw + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，$I_n$ 是 $n \\times n$ 的单位矩阵。\n\n给定权重 $w$ 的数据 $y$ 的似然是一个多元高斯分布：\n$$p(y \\mid x, w) = \\mathcal{N}(y \\mid xw, \\sigma^2 I_n)$$\n\n标量权重 $w$ 的先验分布也是高斯分布：\n$$p(w) = \\mathcal{N}(w \\mid 0, \\tau^2)$$\n\n边缘似然或模型证据 $p(y \\mid x)$，是通过将似然与先验的乘积在 $w$ 的所有可能值上积分得到的：\n$$p(y \\mid x) = \\int_{-\\infty}^{\\infty} p(y \\mid x, w) p(w) \\, dw$$\n\n这个积分可以解析求解。由于 $y$ 是高斯随机变量 $w$ 的线性函数加上独立的高斯噪声 $\\varepsilon$，因此 $y$ 的结果分布也是高斯分布。我们可以通过计算其均值和协方差来找到其参数。\n\n$y$的均值是：\n$$E[y] = E[xw + \\varepsilon] = E[xw] + E[\\varepsilon] = x E[w] + 0$$\n由于先验均值 $E[w]$ 为 $0$，因此 $y$ 的边缘均值也为 $0$。\n$$E[y] = x \\cdot 0 = 0$$\n\n$y$的协方差是：\n$$\\text{Cov}(y) = \\text{Cov}(xw + \\varepsilon)$$\n因为 $w$ 和 $\\varepsilon$ 是独立的，所以它们和的协方差等于它们协方差的和：\n$$\\text{Cov}(y) = \\text{Cov}(xw) + \\text{Cov}(\\varepsilon)$$\n噪声的协方差给出为 $\\text{Cov}(\\varepsilon) = \\sigma^2 I_n$。\n项 $xw$ 的协方差是：\n$$\\text{Cov}(xw) = E[(xw)(xw)^T] - E[xw]E[xw]^T = E[w^2 xx^T] - (x E[w])(x E[w])^T$$\n由于 $E[w]=0$，$E[w^2] = \\text{Var}(w) + (E[w])^2 = \\tau^2 + 0^2 = \\tau^2$。\n所以，$\\text{Cov}(xw) = xx^T E[w^2] = \\tau^2 xx^T$。\n\n综合这些结果，$y$ 的协方差矩阵，记作 $\\Sigma_y$，是：\n$$\\Sigma_y = \\tau^2 xx^T + \\sigma^2 I_n$$\n\n因此，$y$ 的边缘分布是一个均值为 $0$、协方差为 $\\Sigma_y$ 的多元高斯分布：\n$$p(y \\mid x) = \\mathcal{N}(y \\mid 0, \\Sigma_y)$$\n对数概率密度由下式给出：\n$$\\log p(y \\mid x) = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det(\\Sigma_y) - \\frac{1}{2} y^T \\Sigma_y^{-1} y$$\n\n为了计算它，我们需要 $\\Sigma_y$ 的行列式和逆。矩阵 $\\Sigma_y$ 的形式为 $cI + uv^T$。我们可以使用矩阵行列式引理和 Sherman-Morrison 公式。\n令 $c=\\sigma^2$, $u=\\tau x$, $v=\\tau x$。\n\n行列式为：\n$$\\det(\\Sigma_y) = \\det(\\sigma^2 I_n + (\\tau x)(\\tau x)^T) = (\\sigma^2)^n \\left(1 + \\frac{1}{\\sigma^2}(\\tau x)^T(\\tau x)\\right) = (\\sigma^2)^n \\left(1 + \\frac{\\tau^2}{\\sigma^2}x^T x\\right) = (\\sigma^2)^{n-1} (\\sigma^2 + \\tau^2 x^T x)$$\n\n逆矩阵为：\n$$\\Sigma_y^{-1} = (\\sigma^2 I_n + \\tau^2 xx^T)^{-1} = \\frac{1}{\\sigma^2}I_n - \\frac{\\frac{\\tau^2}{\\sigma^4}xx^T}{1+\\frac{\\tau^2}{\\sigma^2}x^Tx} = \\frac{1}{\\sigma^2}I_n - \\frac{\\tau^2 xx^T}{\\sigma^2(\\sigma^2 + \\tau^2 x^T x)}$$\n\n二次项变为：\n$$y^T \\Sigma_y^{-1} y = y^T \\left( \\frac{1}{\\sigma^2}I_n - \\frac{\\tau^2 xx^T}{\\sigma^2(\\sigma^2 + \\tau^2 x^T x)} \\right) y = \\frac{y^T y}{\\sigma^2} - \\frac{\\tau^2 (y^T x)(x^T y)}{\\sigma^2(\\sigma^2 + \\tau^2 x^T x)} = \\frac{y^T y}{\\sigma^2} - \\frac{\\tau^2 (x^T y)^2}{\\sigma^2(\\sigma^2 + \\tau^2 x^T x)}$$\n\n将这些代入对数边缘似然的表达式中，得到用于计算的最终解析公式。\n\n### 2. 学生t噪声模型的边缘似然\n\n第二个任务用重尾的学生t分布替换高斯噪声模型，同时保留 $w$ 的高斯先验。\n\n似然现在是学生t概率密度的乘积：\n$$p(y \\mid x, w) = \\prod_{i=1}^n \\mathrm{StudentT}_\\nu(y_i \\mid x_i w, s)$$\n其中 $\\mathrm{StudentT}_\\nu(z \\mid \\mu, s)$ 是问题陈述中给出的概率密度函数（PDF）。先验保持为 $p(w) = \\mathcal{N}(w \\mid 0, \\tau^2)$。\n\n边缘似然同样是关于 $w$ 的积分：\n$$p_{\\mathrm{StudentT}}(y \\mid x) = \\int_{-\\infty}^{\\infty} \\left( \\prod_{i=1}^n \\mathrm{StudentT}_\\nu(y_i \\mid x_i w, s) \\right) \\mathcal{N}(w \\mid 0, \\tau^2) \\, dw$$\n\n这种学生t似然和高斯先验的组合不是共轭的，这意味着该积分没有简单的闭式解。因此，必须进行数值计算。\n\n设被积函数为 $f(w) = p(y \\mid x, w) p(w)$。为避免数值下溢，标准做法是处理被积函数的对数，即 $\\log f(w) = \\log p(y \\mid x, w) + \\log p(w)$。这个函数可能有一个非常尖锐的峰值并迅速衰减，这对标准的数值求积方法构成了挑战。\n\n为确保计算的稳定性和准确性，我们采用以下策略：\n1.  通过数值最小化负对数被积函数 $-\\log f(w)$ 来找到被积函数的众数 $w_{map}$。这个值 $w_{map}$ 是 $w$ 的最大后验（MAP）估计。\n2.  令 $L_{max} = \\log f(w_{map})$ 为对数被积函数的最大值。\n3.  通过提出峰值来重写积分：\n    $$p(y \\mid x) = \\int_{-\\infty}^{\\infty} e^{\\log f(w)} dw = \\int_{-\\infty}^{\\infty} e^{\\log f(w) - L_{max} + L_{max}} dw = e^{L_{max}} \\int_{-\\infty}^{\\infty} e^{\\log f(w) - L_{max}} dw$$\n4.  新的被积函数 $g(w) = e^{\\log f(w) - L_{max}}$ 在数值上是稳定的。其最大值为 $1$（在 $w=w_{map}$ 处），并平滑地衰减到 $0$。该函数适用于像 `scipy.integrate.quad` 这样的标准数值积分程序。\n5.  最终的对数边缘似然计算如下：\n    $$\\log p_{\\mathrm{StudentT}}(y \\mid x) = L_{max} + \\log \\left( \\int_{-\\infty}^{\\infty} g(w) \\, dw \\right)$$\n\n### 3. 量化模型证据的增益\n\n最后的任务是计算对数边缘似然的差异：\n$$\\Delta = \\log p_{\\mathrm{StudentT}}(y \\mid x) - \\log p_{\\mathrm{Gaussian}}(y \\mid x)$$\n\n这个量，也称为对数贝叶斯因子，量化了支持学生t模型相对于高斯模型的证据。$\\Delta$ 的正值表明，使用学生t噪声的模型能更好地解释数据。这在有异常值的情况下是预期的，因为与高斯分布相比，学生t分布的重尾可以容纳极端值，而对总似然的惩罚较小。为观察此效应，对三个不同的测试用例进行了计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, norm\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize_scalar\n\ndef log_marginal_likelihood_gaussian(x: np.ndarray, y: np.ndarray, sigma: float, tau: float) -> float:\n    \"\"\"\n    Computes the log marginal likelihood for a Bayesian linear regression model\n    with a single weight, Gaussian likelihood, and Gaussian prior.\n    \n    Args:\n        x (np.ndarray): Design vector of shape (n,).\n        y (np.ndarray): Response vector of shape (n,).\n        sigma (float): Standard deviation of the Gaussian noise.\n        tau (float): Standard deviation of the Gaussian prior on the weight.\n\n    Returns:\n        float: The log marginal likelihood, log p(y|x).\n    \"\"\"\n    n = len(x)\n    sigma2 = sigma**2\n    tau2 = tau**2\n    \n    xTx = np.dot(x, x)\n    yTy = np.dot(y, y)\n    xTy = np.dot(x, y)\n    \n    # Log-determinant of the marginal covariance matrix Sigma_y\n    # det(Sigma_y) = (sigma^2)^(n-1) * (sigma^2 + tau^2 * x^T*x)\n    # The term sigma2 + tau2 * xTx could be zero or negative if parameters are not positive.\n    # The problem constraints ensure this is not an issue.\n    log_det_sigma_y = (n - 1) * np.log(sigma2) + np.log(sigma2 + tau2 * xTx)\n    \n    # Quadratic form term y^T * Sigma_y^-1 * y\n    # y^T*Sigma_y^-1*y = y^T*y/sigma^2 - (tau^2 * (y^T*x)^2) / (sigma^2 * (sigma^2 + tau^2 * x^T*x))\n    quad_term = yTy / sigma2 - (tau2 * xTy**2) / (sigma2 * (sigma2 + tau2 * xTx))\n    \n    # Log marginal likelihood: log p(y|x) = -n/2*log(2*pi) - 0.5*log(det(Sigma_y)) - 0.5*y^T*Sigma_y^-1*y\n    log_p = -n / 2.0 * np.log(2 * np.pi) - 0.5 * log_det_sigma_y - 0.5 * quad_term\n    \n    return log_p\n\ndef log_marginal_likelihood_student_t(x: np.ndarray, y: np.ndarray, nu: float, s: float, tau: float) -> float:\n    \"\"\"\n    Computes the log marginal likelihood for a model with Student's t likelihood\n    and Gaussian prior using numerical integration.\n    \n    Args:\n        x (np.ndarray): Design vector of shape (n,).\n        y (np.ndarray): Response vector of shape (n,).\n        nu (float): Degrees of freedom for the Student's t distribution.\n        s (float): Scale parameter for the Student's t distribution.\n        tau (float): Standard deviation of the Gaussian prior on the weight.\n\n    Returns:\n        float: The log marginal likelihood, log p(y|x).\n    \"\"\"\n    \n    # Log of the integrand p(y|x,w)p(w)\n    def log_integrand(w: float) -> float:\n        log_likelihood = np.sum(t.logpdf(y, df=nu, loc=x * w, scale=s))\n        log_prior = norm.logpdf(w, loc=0, scale=tau)\n        return log_likelihood + log_prior\n\n    # To improve numerical stability, find the peak of the integrand (MAP estimate of w)\n    # by minimizing the negative log-integrand.\n    res = minimize_scalar(lambda w: -log_integrand(w))\n    w_map = res.x\n    log_max_val = log_integrand(w_map)\n    \n    # Define the scaled integrand for numerical integration.\n    # integrand = exp(log_integrand(w) - log_max_val)\n    def scaled_integrand(w: float) -> float:\n        log_val = log_integrand(w) - log_max_val\n        # np.exp can handle large negative inputs, but we can be explicit.\n        if log_val  -700: # Threshold for underflow\n             return 0.0\n        return np.exp(log_val)\n\n    # Numerically integrate the scaled function.\n    integral_val, _ = quad(scaled_integrand, -np.inf, np.inf)\n    \n    # Reconstruct the log marginal likelihood from the scaled integral.\n    # The integral of a positive function must be positive.\n    # A non-positive result would indicate a numerical error.\n    if integral_val = 0:\n        return -np.inf\n    \n    return log_max_val + np.log(integral_val)\n\ndef calculate_delta(x_list: list, y_list: list, sigma: float, tau: float, nu: float, s: float) - float:\n    \"\"\"\n    Calculates the difference in log marginal likelihoods between the Student's t\n    and Gaussian models.\n    \"\"\"\n    x = np.array(x_list)\n    y = np.array(y_list)\n\n    log_p_gaussian = log_marginal_likelihood_gaussian(x, y, sigma, tau)\n    log_p_student_t = log_marginal_likelihood_student_t(x, y, nu, s, tau)\n\n    delta = log_p_student_t - log_p_gaussian\n    return delta\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A: Baseline, no outlier\n        {\n            \"x\": [0.0, 0.5, 1.0, 1.5, 2.0],\n            \"y\": [0.0, 1.0, 2.0, 3.0, 4.0],\n            \"sigma\": 1.0, \"tau\": 5.0, \"nu\": 3, \"s\": 1.0\n        },\n        # Case B: Single outlier\n        {\n            \"x\": [0.0, 0.5, 1.0, 1.5, 2.0],\n            \"y\": [0.0, 1.0, 2.0, 3.0, 12.0],\n            \"sigma\": 1.0, \"tau\": 5.0, \"nu\": 3, \"s\": 1.0\n        },\n        # Case C: Boundary condition, heavy tails approach Gaussian\n        {\n            \"x\": [0.0, 0.5, 1.0, 1.5, 2.0],\n            \"y\": [0.0, 1.0, 2.0, 3.0, 12.0],\n            \"sigma\": 1.0, \"tau\": 5.0, \"nu\": 100, \"s\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        delta = calculate_delta(\n            x_list=case[\"x\"],\n            y_list=case[\"y\"],\n            sigma=case[\"sigma\"],\n            tau=case[\"tau\"],\n            nu=case[\"nu\"],\n            s=case[\"s\"]\n        )\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}