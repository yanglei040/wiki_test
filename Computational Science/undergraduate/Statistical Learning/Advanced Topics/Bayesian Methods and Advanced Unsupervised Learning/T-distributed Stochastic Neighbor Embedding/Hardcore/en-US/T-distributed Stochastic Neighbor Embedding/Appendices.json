{
    "hands_on_practices": [
        {
            "introduction": "The core of t-SNE is an optimization process that arranges points in a low-dimensional space to reflect their high-dimensional neighborhoods. This arrangement is driven by a delicate balance of attractive and repulsive forces. This practice challenges you to derive these forces from first principles for a general kernel, providing a deep understanding of t-SNE's mechanics, and then analyze why the specific choice of a heavy-tailed Student-$t$ kernel is crucial for creating clear visualizations. ",
            "id": "3179628",
            "problem": "Consider the t-distributed stochastic neighbor embedding (t-SNE) method, whose objective can be written in terms of the Kullback–Leibler divergence (KL) between high-dimensional neighborhood probabilities and low-dimensional kernel-induced probabilities. Let $y_i \\in \\mathbb{R}^d$ denote low-dimensional embeddings and $r_{ij} = \\lVert y_i - y_j \\rVert$ denote pairwise distances. Suppose similarities in the low-dimensional space are defined by a differentiable, radial kernel $g(r)$ so that\n$$\nq_{ij} = \\frac{g(r_{ij})}{\\sum_{k \\ne \\ell} g(r_{k\\ell})},\n$$\nand the objective is\n$$\nC(y) = \\sum_{i \\ne j} p_{ij} \\log\\left(\\frac{p_{ij}}{q_{ij}}\\right),\n$$\nwhere $p_{ij}$ are fixed, symmetric high-dimensional similarities satisfying $\\sum_{i \\ne j} p_{ij} = 1$. Starting from these core definitions, derive the negative gradient (“force”) $F_i = -\\frac{\\partial C}{\\partial y_i}$ for a general differentiable kernel $g(r)$ in terms of $p_{ij}$, $q_{ij}$, $g'(r)$, and the unit direction $u_{ij} = \\frac{y_i - y_j}{r_{ij}}$. Then specialize your expression to the far-neighbor regime in which $p_{ij} + p_{ji} \\approx 0$ and extract the radial repulsive magnitude $M(r)$ as a function of $r$.\n\nNext, consider two specific kernels:\n- The Student-$t$ kernel with one degree of freedom, defined by $g_{\\mathrm{t}}(r) = (1 + r^2)^{-1}$.\n- A proposed Laplace kernel variant, defined by $g_{\\mathrm{L}}(r) = \\exp(-r)$.\n\nFrom first principles, determine the far-distance asymptotic behavior of the repulsive magnitude $M_{\\mathrm{t}}(r)$ and $M_{\\mathrm{L}}(r)$ as $r \\to \\infty$, and assess whether the Laplace kernel’s tail shape alleviates the crowding problem relative to the Student-$t$ kernel. Your analysis must proceed from the KL objective and the definition of $q_{ij}$ above; do not assume or use pre-stated gradient formulas for t-SNE.\n\nYour program must implement the following test suite, each test producing a single fundamental-type result (boolean or float):\n\n- Define the force-profile ratio $R(r)$ by\n$$\nR(r) = \\frac{\\left|g'_{\\mathrm{L}}(r)\\right|}{\\left|g'_{\\mathrm{t}}(r)\\right|},\n$$\nwhich is proportional to the ratio of far-neighbor repulsive magnitudes if the normalization $\\sum_{k \\ne \\ell} g(r_{k\\ell})$ is treated as a constant factor independent of a single distant pair. Compute $R(r)$ for $r$ values in the set $\\{10^{-8}, 1, 5, 50\\}$ and return each as a float.\n\n- Return a boolean indicating whether $R(50)  10^{-6}$.\n\n- Compute the float\n$$\n\\Delta = \\int_{10}^{20} \\left|g'_{\\mathrm{L}}(r)\\right| \\, dr \\;-\\; \\int_{10}^{20} \\left|g'_{\\mathrm{t}}(r)\\right| \\, dr,\n$$\nusing numerical integration, and return $\\Delta$.\n\n- Return a boolean indicating whether $\\Delta  0$.\n\n- For the threshold $\\tau = 10^{-3}$, compute the radii $r^\\ast_{\\mathrm{L}}$ and $r^\\ast_{\\mathrm{t}}$ at which $|g'_{\\mathrm{L}}(r)| = \\tau$ and $|g'_{\\mathrm{t}}(r)| = \\tau$, respectively, and return the float ratio $r^\\ast_{\\mathrm{L}} / r^\\ast_{\\mathrm{t}}$.\n\n- Return a boolean indicating whether $r^\\ast_{\\mathrm{L}}  r^\\ast_{\\mathrm{t}}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[result1,result2,result3,result4,result5,result6,result7,result8,result9]”). No user input is required, and no physical units apply in this problem.",
            "solution": "The analysis of the problem proceeds in two main stages. First, we perform a formal derivation of the gradient of the t-SNE objective function for a general kernel. Second, we analyze this gradient for two specific kernels and evaluate their properties as requested.\n\n### Step 1: General Gradient Derivation\n\nThe objective function to be minimized is the Kullback-Leibler (KL) divergence between high-dimensional similarities $p_{ij}$ and low-dimensional similarities $q_{ij}$:\n$$\nC(y) = \\sum_{i \\ne j} p_{ij} \\log\\left(\\frac{p_{ij}}{q_{ij}}\\right)\n$$\nwhere $y_i \\in \\mathbb{R}^d$ are the low-dimensional embeddings. The high-dimensional similarities $p_{ij}$ are given as fixed, symmetric ($p_{ij}=p_{ji}$), and normalized such that $\\sum_{i \\ne j} p_{ij} = 1$. The low-dimensional similarities $q_{ij}$ are defined using a differentiable radial kernel $g(r)$:\n$$\nq_{ij} = \\frac{g(r_{ij})}{\\sum_{k \\ne \\ell} g(r_{k\\ell})} = \\frac{g(r_{ij})}{Z}\n$$\nwhere $r_{ij} = \\lVert y_i - y_j \\rVert$ and $Z = \\sum_{k \\ne \\ell} g(r_{k\\ell})$ is the normalization constant.\n\nWe can rewrite the cost function $C(y)$ by separating the terms involving $y$:\n$$\nC(y) = \\sum_{i \\ne j} p_{ij} \\log p_{ij} - \\sum_{i \\ne j} p_{ij} \\log q_{ij}\n$$\nThe first term is constant with respect to $y_i$. Substituting the definition of $q_{ij}$:\n$$\nC(y) = \\text{const} - \\sum_{i \\ne j} p_{ij} \\log \\left( \\frac{g(r_{ij})}{Z} \\right) = \\text{const} - \\sum_{i \\ne j} p_{ij} \\log g(r_{ij}) + \\left(\\sum_{i \\ne j} p_{ij}\\right) \\log Z\n$$\nSince $\\sum_{i \\ne j} p_{ij} = 1$, we have:\n$$\nC(y) = \\text{const} - \\sum_{i \\ne j} p_{ij} \\log g(r_{ij}) + \\log \\left( \\sum_{k \\ne \\ell} g(r_{k\\ell}) \\right)\n$$\nWe seek the negative gradient, or \"force\", $F_i = -\\frac{\\partial C}{\\partial y_i}$. We compute the gradient $\\frac{\\partial C}{\\partial y_i}$:\n$$\n\\frac{\\partial C}{\\partial y_i} = -\\sum_{k \\ne \\ell} p_{k\\ell} \\frac{1}{g(r_{k\\ell})} \\frac{\\partial g(r_{k\\ell})}{\\partial y_i} + \\frac{1}{Z} \\frac{\\partial Z}{\\partial y_i}\n$$\nThe derivative of a function of a pairwise distance $f(r_{k\\ell})$ with respect to $y_i$ is:\n$$\n\\frac{\\partial f(r_{k\\ell})}{\\partial y_i} = f'(r_{k\\ell}) \\frac{\\partial r_{k\\ell}}{\\partial y_i} = f'(r_{k\\ell}) \\frac{y_k - y_\\ell}{r_{k\\ell}} (\\delta_{ik} - \\delta_{i\\ell}) = f'(r_{k\\ell}) u_{k\\ell} (\\delta_{ik} - \\delta_{i\\ell})\n$$\nwhere $u_{k\\ell} = (y_k - y_\\ell) / r_{k\\ell}$ is the unit direction vector.\n\nApplying this, the sum in the first term of $\\frac{\\partial C}{\\partial y_i}$ is non-zero only when $k=i$ or $\\ell=i$:\n$$\n-\\sum_{k \\ne \\ell} p_{k\\ell} \\frac{g'(r_{k\\ell})}{g(r_{k\\ell})} u_{k\\ell} (\\delta_{ik} - \\delta_{i\\ell}) = -\\sum_{j \\ne i} p_{ij} \\frac{g'(r_{ij})}{g(r_{ij})} u_{ij} - \\sum_{j \\ne i} p_{ji} \\frac{g'(r_{ji})}{g(r_{ji})} (-u_{ji})\n$$\nUsing $r_{ij}=r_{ji}$ and $u_{ij}=-u_{ji}$, the second part becomes $-\\sum_{j \\ne i} p_{ji} \\frac{g'(r_{ij})}{g(r_{ij})} u_{ij}$. Combining these gives:\n$$\n-\\sum_{j \\ne i} (p_{ij} + p_{ji}) \\frac{g'(r_{ij})}{g(r_{ij})} u_{ij}\n$$\nSimilarly, the derivative of the normalization constant $Z$ is:\n$$\n\\frac{\\partial Z}{\\partial y_i} = \\sum_{k \\ne \\ell} g'(r_{k\\ell}) u_{k\\ell} (\\delta_{ik} - \\delta_{i\\ell}) = \\sum_{j \\ne i} g'(r_{ij})u_{ij} - \\sum_{j \\ne i} g'(r_{ji})u_{ji} = \\sum_{j \\ne i} (g'(r_{ij})u_{ij} - g'(r_{ij})(-u_{ij})) = 2\\sum_{j \\ne i} g'(r_{ij}) u_{ij}\n$$\nCombining all parts, the gradient is:\n$$\n\\frac{\\partial C}{\\partial y_i} = -\\sum_{j \\ne i} (p_{ij} + p_{ji}) \\frac{g'(r_{ij})}{g(r_{ij})} u_{ij} + \\frac{2}{Z} \\sum_{j \\ne i} g'(r_{ij}) u_{ij}\n$$\nRearranging the sum:\n$$\n\\frac{\\partial C}{\\partial y_i} = \\sum_{j \\ne i} \\left( \\frac{2 g'(r_{ij})}{Z} - (p_{ij} + p_{ji}) \\frac{g'(r_{ij})}{g(r_{ij})} \\right) u_{ij}\n$$\nThe force $F_i = -\\frac{\\partial C}{\\partial y_i}$ is therefore:\n$$\nF_i = \\sum_{j \\ne i} \\left( (p_{ij} + p_{ji}) \\frac{g'(r_{ij})}{g(r_{ij})} - \\frac{2 g'(r_{ij})}{Z} \\right) u_{ij}\n$$\nThis expression separates the force on point $y_i$ into a sum of pairwise contributions from other points $y_j$. The term proportional to $(p_{ij}+p_{ji})$ represents attraction, while the term involving $Z$ represents repulsion. Note that $g(r)$ is typically a decreasing function, so $g'(r)  0$. The attractive term pulls $y_i$ towards $y_j$ (opposite to $u_{ij}$), and the repulsive term pushes $y_i$ away from $y_j$ (along $u_{ij}$).\n\n### Step 2: Far-Neighbor Repulsive Magnitude\n\nIn the far-neighbor regime for a pair $(i, j)$, the high-dimensional similarity is negligible, i.e., $p_{ij} + p_{ji} \\approx 0$. In this case, the attractive part of the force contribution from $y_j$ vanishes. The force simplifies to a purely repulsive component:\n$$\nF_{i \\leftarrow j}^{\\text{far}} \\approx \\left( - \\frac{2 g'(r_{ij})}{Z} \\right) u_{ij}\n$$\nThe radial repulsive magnitude is the magnitude of this force contribution:\n$$\nM(r) = \\left| - \\frac{2 g'(r)}{Z} \\right| = \\frac{2 |g'(r)|}{\\sum_{k \\ne \\ell} g(r_{k\\ell})}\n$$\nWhen analyzing the functional form of the repulsion, $Z$ is treated as a constant factor for a given embedding configuration. Thus, the shape of the repulsive force profile is determined by $|g'(r)|$.\n\n### Step 3: Analysis of Specific Kernels\n\nWe now analyze the two specified kernels.\n\n1.  **Student-$t$ kernel**: $g_{\\mathrm{t}}(r) = (1 + r^2)^{-1}$\n    The derivative is $g'_{\\mathrm{t}}(r) = -2r(1 + r^2)^{-2}$.\n    The repulsive magnitude's functional form is $M_{\\mathrm{t}}(r) \\propto |g'_{\\mathrm{t}}(r)| = 2r(1 + r^2)^{-2}$.\n    As $r \\to \\infty$, the asymptotic behavior is $M_{\\mathrm{t}}(r) \\propto r(r^2)^{-2} = r \\cdot r^{-4} = r^{-3}$. This is an algebraic (power-law) decay.\n\n2.  **Laplace kernel**: $g_{\\mathrm{L}}(r) = \\exp(-r)$\n    The derivative is $g'_{\\mathrm{L}}(r) = -\\exp(-r)$.\n    The repulsive magnitude's functional form is $M_{\\mathrm{L}}(r) \\propto |g'_{\\mathrm{L}}(r)| = \\exp(-r)$.\n    As $r \\to \\infty$, this shows an exponential decay.\n\n**Assessment of the Crowding Problem**:\nThe crowding problem in SNE arises when there is insufficient repulsive force to separate points that are far apart in the high-dimensional space. t-SNE mitigates this by using the Student-$t$ kernel, whose algebraic decay ($r^{-3}$) provides a \"heavier tail\" and thus a longer-range repulsive force compared to the exponential decay of a Gaussian kernel used in original SNE.\n\nComparing the Laplace kernel to the Student-$t$ kernel, we find that the repulsive magnitude $M_{\\mathrm{L}}(r) \\propto e^{-r}$ decays exponentially, which is significantly faster than the algebraic decay of $M_{\\mathrm{t}}(r) \\propto r^{-3}$. An exponential decay constitutes a much shorter-range force. Consequently, the Laplace kernel would provide a much weaker repulsion between distant points than the Student-$t$ kernel. This would exacerbate the crowding problem, not alleviate it. The Student-$t$ kernel is superior in this regard due to its heavier-tailed repulsive forces.\n\n### Step 4: Numerical Computations\n\nThe problem requires a series of numerical calculations based on these kernels.\n\n- **Kernels and derivatives**:\n  $|g'_{\\mathrm{t}}(r)| = 2r(1 + r^2)^{-2}$\n  $|g'_{\\mathrm{L}}(r)| = \\exp(-r)$\n\n- **Force-profile ratio**: $R(r) = |g'_{\\mathrm{L}}(r)| / |g'_{\\mathrm{t}}(r)| = \\frac{\\exp(-r)}{2r(1+r^2)^{-2}} = \\frac{\\exp(-r)(1+r^2)^2}{2r}$.\n\n- **Integral difference**: $\\Delta = \\int_{10}^{20} \\exp(-r) \\, dr - \\int_{10}^{20} 2r(1+r^2)^{-2} \\, dr$. These integrals can be computed numerically or analytically.\n  $\\int \\exp(-r) dr = -\\exp(-r)$\n  $\\int 2r(1+r^2)^{-2} dr = -(1+r^2)^{-1}$ (using substitution $u=1+r^2$)\n  $\\Delta = [-\\exp(-r)]_{10}^{20} - [-(1+r^2)^{-1}]_{10}^{20} = (e^{-10} - e^{-20}) - (\\frac{1}{101} - \\frac{1}{401})$.\n\n- **Radii at threshold $\\tau = 10^{-3}$**:\n  $r^\\ast_{\\mathrm{L}}$: Solve $|g'_{\\mathrm{L}}(r)| = e^{-r} = 10^{-3} \\implies r = -\\ln(10^{-3}) = 3\\ln(10)$.\n  $r^\\ast_{\\mathrm{t}}$: Solve $|g'_{\\mathrm{t}}(r)| = \\frac{2r}{(1+r^2)^2} = 10^{-3}$. This requires a numerical root-finding method.\n\nThese calculations are implemented in the Python code below.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Performs the calculations specified in the problem statement.\n    \"\"\"\n\n    # Define the absolute values of the kernel derivatives\n    def abs_g_prime_t(r):\n        \"\"\"Absolute derivative of the Student-t kernel.\"\"\"\n        return 2 * r / ((1 + r**2)**2)\n\n    def abs_g_prime_l(r):\n        \"\"\"Absolute derivative of the Laplace kernel.\"\"\"\n        return np.exp(-r)\n\n    # Task 1: Compute the force-profile ratio R(r) for specific r values\n    def force_profile_ratio(r):\n        \"\"\"Computes R(r) = |g'_L(r)| / |g'_t(r)|.\"\"\"\n        if r == 0:\n            return np.inf  # The formula has 1/r dependency\n        return abs_g_prime_l(r) / abs_g_prime_t(r)\n\n    r_values = [1e-8, 1, 5, 50]\n    r_results = [force_profile_ratio(r) for r in r_values]\n\n    # Task 2: Check if R(50)  1e-6\n    r50_check = bool(r_results[-1]  1e-6)\n\n    # Task 3: Compute the integral difference Delta\n    integral_l, _ = quad(abs_g_prime_l, 10, 20)\n    integral_t, _ = quad(abs_g_prime_t, 10, 20)\n    delta = integral_l - integral_t\n\n    # Task 4: Check if Delta  0\n    delta_check = bool(delta  0)\n\n    # Task 5: Compute the radii ratio r_star_L / r_star_t\n    tau = 1e-3\n\n    # For Laplace kernel: exp(-r) = tau = r = -log(tau)\n    r_star_l = -np.log(tau)\n\n    # For Student-t kernel: 2r / (1+r^2)^2 = tau. Find root of f(r) = 0.\n    def f_t(r):\n        return abs_g_prime_t(r) - tau\n    \n    # Bracket the root. f(10)  0, f(15)  0 based on preliminary analysis.\n    sol_t = root_scalar(f_t, bracket=[1, 20], method='brentq')\n    r_star_t = sol_t.root\n\n    radii_ratio = r_star_l / r_star_t\n\n    # Task 6: Check if r_star_L  r_star_t\n    radii_ratio_check = bool(r_star_l  r_star_t)\n    \n    # Consolidate all results\n    all_results = r_results + [r50_check, delta, delta_check, radii_ratio, radii_ratio_check]\n    \n    # Format and print the final output\n    # Using a mix of float formatting for readability where appropriate\n    formatted_results = []\n    for item in all_results:\n        if isinstance(item, bool):\n            formatted_results.append(str(item).lower())\n        else:\n            # Use general format for large/small numbers, and float for others\n            if abs(item)  1e6 or (abs(item)  1e-4 and item != 0):\n                 formatted_results.append(f\"{item:.10e}\")\n            else:\n                 formatted_results.append(f\"{item:.10f}\")\n\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While the full t-SNE gradient calculation is exact, it becomes computationally prohibitive for large datasets, as it requires considering all pairs of points. This exercise guides you through a common solution in modern machine learning: mini-batch approximation. You will design an unbiased gradient estimator and empirically analyze the trade-offs in terms of bias and variance, gaining insight into the practical challenges of scaling up neighborhood embedding methods. ",
            "id": "3179603",
            "problem": "You are asked to design and analyze a mini-batch approximation scheme for the gradient of the t-distributed Stochastic Neighbor Embedding (t-SNE) objective. The work must begin from fundamental definitions and facts in statistical learning. Specifically, start from the definitions of pairwise similarities in the high-dimensional input space and the low-dimensional embedding space, and the Kullback–Leibler divergence (KL) objective that t-SNE minimizes. Build a principled mini-batch estimator for the gradient by sampling subsets of points, and analyze the bias and variance of the one-step reduction in the objective achieved by this estimator.\n\nThe fundamental base you are permitted to use consists of the following definitions and facts:\n\n- High-dimensional pairwise conditional similarities are defined as $p_{j \\mid i} = \\frac{\\exp\\!\\left(-\\frac{\\lVert x_i - x_j \\rVert^2}{2 \\sigma_i^2}\\right)}{\\sum_{k \\neq i} \\exp\\!\\left(-\\frac{\\lVert x_i - x_k \\rVert^2}{2 \\sigma_i^2}\\right)}$, where $x_i \\in \\mathbb{R}^D$ is a data point, $\\sigma_i  0$ is chosen such that the Shannon entropy of $p_{\\cdot \\mid i}$ equals a target entropy specified by a given perplexity, and $D$ is the input dimension. The perplexity target is enforced by binary search on $\\sigma_i$ to match the desired entropy.\n\n- The symmetrized joint similarities in the high-dimensional space are $p_{ij} = \\frac{p_{i \\mid j} + p_{j \\mid i}}{2N}$ for $i \\neq j$, where $N$ is the number of points, and $p_{ii} = 0$, with $\\sum_{i \\neq j} p_{ij} = 1$.\n\n- Low-dimensional pairwise similarities are computed from the Student $t$-kernel as $t_{ij} = \\left(1 + \\lVert y_i - y_j \\rVert^2\\right)^{-1}$ for $i \\neq j$, with $y_i \\in \\mathbb{R}^m$ the embedding, and $t_{ii} = 0$. These are normalized to $q_{ij} = \\frac{t_{ij}}{\\sum_{a \\neq b} t_{ab}}$ with $q_{ii} = 0$ and $\\sum_{i \\neq j} q_{ij} = 1$.\n\n- The t-SNE objective is the Kullback–Leibler divergence (KL) $KL(P \\parallel Q) = \\sum_{i \\neq j} p_{ij} \\log\\!\\left(\\frac{p_{ij}}{q_{ij}}\\right)$, where $P = [p_{ij}]$ and $Q = [q_{ij}]$.\n\nFrom this base, you must:\n\n1. Derive the gradient of the t-SNE objective with respect to the embedding positions $y_i$, by applying calculus to the above definitions. Provide a complete derivation starting from $KL(P \\parallel Q)$ and the dependence of $q_{ij}$ on $y_i$ via $t_{ij}$ and its normalization. Do not introduce any formulas that are not derived from the above fundamental definitions.\n\n2. Design a mini-batch gradient approximation scheme that, for each point index $i$, uses a subset of other indices to form an unbiased estimator of the full gradient. Your estimator must be constructed by sampling, without replacement, a subset of size $B$ from $\\{j \\in \\{1, \\dots, N\\} \\mid j \\neq i\\}$ and appropriately scaling the partial sum so that its expectation equals the full sum over $j \\neq i$.\n\n3. Use the estimator in a single-step update $y_i' = y_i - \\eta \\widehat{g}_i$ for all $i$, where $\\eta > 0$ is a small learning rate and $\\widehat{g}_i$ is your mini-batch gradient estimator at $y_i$. Define the one-step objective change $\\Delta = KL(P \\parallel Q(Y')) - KL(P \\parallel Q(Y))$, where $Q(Y)$ denotes the $Q$ computed from the current embedding $Y$ and $Q(Y')$ from the updated embedding.\n\n4. Analyze the bias and variance of the one-step objective change under your mini-batch scheme. Specifically, for fixed data $X$, fixed initial embedding $Y$, fixed $P$ computed from $X$, and fixed learning rate $\\eta$, define the bias as $b = \\mathbb{E}[\\Delta_{\\text{mb}}] - \\Delta_{\\text{full}}$ and the variance as $v = \\operatorname{Var}(\\Delta_{\\text{mb}})$, where $\\Delta_{\\text{mb}}$ is the one-step change using the mini-batch gradient and $\\Delta_{\\text{full}}$ is the one-step change using the full gradient. Estimate $b$ and $v$ empirically by repeated random mini-batch sampling.\n\nImplementation requirements:\n\n- Construct $P$ from a synthetic dataset with $N$ points in input dimension $D$, formed as a mixture of two Gaussian clusters with well-separated means. Use $m = 2$ for the embedding dimension. Initialize $Y$ by drawing from a zero-mean isotropic Gaussian with small variance. Fix a small learning rate $\\eta$.\n\n- For numerical stability, when computing $KL(P \\parallel Q)$, treat only off-diagonal terms ($i \\neq j$), and clip $p_{ij}$ and $q_{ij}$ away from zero by a small positive constant.\n\n- Your program must implement the entire pipeline end-to-end: data generation, $P$ construction via binary search on $\\sigma_i$ to match a target perplexity, gradient derivation implementation, mini-batch estimator construction, one-step updates, and empirical bias-variance estimation by repeated sampling.\n\nTest suite:\n\nUse the following parameter sets to test different facets of your scheme. Each test case must estimate the bias and variance by repeated sampling.\n\n- Case $1$ (edge case, maximal stochasticity): $N = 32$, $D = 10$, perplexity target $= 10$, batch size $B = 1$, number of mini-batch replicates $R = 128$, learning rate $\\eta = 0.1$.\n\n- Case $2$ (moderate batch size): $N = 32$, $D = 10$, perplexity target $= 10$, batch size $B = 8$, number of mini-batch replicates $R = 128$, learning rate $\\eta = 0.1$.\n\n- Case $3$ (larger batch size): $N = 32$, $D = 10$, perplexity target $= 10$, batch size $B = 16$, number of mini-batch replicates $R = 128$, learning rate $\\eta = 0.1$.\n\n- Case $4$ (full batch, boundary condition): $N = 32$, $D = 10$, perplexity target $= 10$, batch size $B = 31$ (i.e., all other points), number of mini-batch replicates $R = 64$, learning rate $\\eta = 0.1$.\n\nFor each case, compute the sample mean of $\\Delta_{\\text{mb}}$ over $R$ replicates, compute the corresponding bias estimate as the sample mean minus the full-gradient one-step change $\\Delta_{\\text{full}}$, and compute the sample variance of $\\Delta_{\\text{mb}}$. Your program’s final output must be a single line containing a list of lists, one per test case, in the exact format\n\"[[b1,v1],[b2,v2],[b3,v3],[b4,v4]]\"\nwhere $b_k$ and $v_k$ are floating-point numbers for case $k$.\n\nNo physical units are involved in this problem. There are no angles or percentages required.\n\nYour final answer must be a complete, runnable program that performs all computations and prints only the final result line, in the specified format.",
            "solution": "The task is to derive the gradient of the t-SNE objective function, design a mini-batch approximation for this gradient, and empirically analyze the bias and variance of the one-step objective change resulting from this approximation.\n\n### 1. Gradient Derivation\n\nThe t-SNE objective is the Kullback-Leibler (KL) divergence between the high-dimensional joint similarity distribution $P$ and the low-dimensional one $Q$:\n$$ C = KL(P \\parallel Q) = \\sum_{i \\neq j} p_{ij} \\log\\left(\\frac{p_{ij}}{q_{ij}}\\right) $$\nWe seek the gradient of $C$ with respect to the embedding coordinates $y_l \\in \\mathbb{R}^m$. The term $\\sum_{i \\neq j} p_{ij} \\log p_{ij}$ is constant with respect to $Y = \\{y_i\\}_{i=1}^N$. Thus, we only need to differentiate the second term:\n$$ C = \\text{const} - \\sum_{i \\neq j} p_{ij} \\log q_{ij} $$\nThe gradient with respect to a single embedding point $y_l$ is:\n$$ \\nabla_{y_l} C = - \\sum_{i \\neq j} p_{ij} \\frac{1}{q_{ij}} \\nabla_{y_l} q_{ij} $$\nThe low-dimensional similarities $q_{ij}$ are defined as $q_{ij} = \\frac{t_{ij}}{Z}$, where $t_{ij} = (1 + \\lVert y_i - y_j \\rVert^2)^{-1}$ and $Z = \\sum_{a \\neq b} t_{ab}$. The gradient of $q_{ij}$ with respect to $y_l$ is found using the quotient rule:\n$$ \\nabla_{y_l} q_{ij} = \\frac{\\nabla_{y_l} t_{ij}}{Z} - \\frac{t_{ij} \\nabla_{y_l} Z}{Z^2} = \\frac{q_{ij}}{t_{ij}} \\nabla_{y_l} t_{ij} - \\frac{q_{ij}}{Z} \\nabla_{y_l} Z $$\nSubstituting this back into the gradient of $C$:\n$$ \\nabla_{y_l} C = - \\sum_{i \\neq j} p_{ij} \\left( \\frac{\\nabla_{y_l} t_{ij}}{t_{ij}} - \\frac{\\nabla_{y_l} Z}{Z} \\right) = - \\sum_{i \\neq j} p_{ij} \\frac{\\nabla_{y_l} t_{ij}}{t_{ij}} + \\left(\\sum_{i \\neq j} p_{ij}\\right) \\frac{\\nabla_{y_l} Z}{Z} $$\nGiven that $\\sum_{i \\neq j} p_{ij} = 1$, this simplifies to:\n$$ \\nabla_{y_l} C = - \\sum_{i \\neq j} \\frac{p_{ij}}{t_{ij}} \\nabla_{y_l} t_{ij} + \\frac{\\nabla_{y_l} Z}{Z} $$\nNow we find the gradients of $t_{ij}$ and $Z$. The term $\\nabla_{y_l} t_{ij}$ is non-zero only if $l=i$ or $l=j$:\n$$ \\nabla_{y_l} t_{ij} = \\begin{cases} -2(1 + \\lVert y_i - y_j \\rVert^2)^{-2}(y_i - y_j) = -2t_{ij}^2(y_i - y_j)  \\text{if } l=i \\\\ -2(1 + \\lVert y_i - y_j \\rVert^2)^{-2}(y_j - y_i) = -2t_{ij}^2(y_j - y_i)  \\text{if } l=j \\\\ 0  \\text{otherwise} \\end{cases} $$\nThe first term in $\\nabla_{y_l} C$ involves a sum over all pairs $(i,j)$, but only pairs where $l \\in \\{i, j\\}$ contribute:\n$$ \\sum_{i \\neq j} \\frac{p_{ij}}{t_{ij}} \\nabla_{y_l} t_{ij} = \\sum_{j \\neq l} \\frac{p_{lj}}{t_{lj}} \\nabla_{y_l} t_{lj} + \\sum_{i \\neq l} \\frac{p_{il}}{t_{il}} \\nabla_{y_l} t_{il} $$\nUsing the derivative of $t_{ij}$ and the symmetry $p_{ij}=p_{ji}, t_{ij}=t_{ji}$:\n$$ = \\sum_{j \\neq l} \\frac{p_{lj}}{t_{lj}}(-2 t_{lj}^2 (y_l - y_j)) + \\sum_{i \\neq l} \\frac{p_{il}}{t_{il}}(-2 t_{il}^2 (y_l - y_i)) = -4 \\sum_{j \\neq l} p_{lj} t_{lj} (y_l - y_j) $$\nNext, we find $\\nabla_{y_l} Z$:\n$$ \\nabla_{y_l} Z = \\sum_{a \\neq b} \\nabla_{y_l} t_{ab} = \\sum_{j \\neq l} \\nabla_{y_l} t_{lj} + \\sum_{i \\neq l} \\nabla_{y_l} t_{il} = -4 \\sum_{j \\neq l} t_{lj}^2 (y_l - y_j) $$\nSubstituting these results back into the expression for $\\nabla_{y_l} C$:\n$$ \\nabla_{y_l} C = - \\left(-4 \\sum_{j \\neq l} p_{lj} t_{lj} (y_l - y_j)\\right) + \\frac{-4 \\sum_{j \\neq l} t_{lj}^2 (y_l - y_j)}{Z} $$\n$$ = 4 \\sum_{j \\neq l} p_{lj} t_{lj} (y_l - y_j) - 4 \\sum_{j \\neq l} \\frac{t_{lj}}{Z} t_{lj} (y_l - y_j) $$\nUsing $q_{lj} = t_{lj}/Z$ and renaming the index $l$ to $i$ for convention, we arrive at the final gradient expression for point $y_i$:\n$$ g_i = \\nabla_{y_i} C = 4 \\sum_{j \\neq i} (p_{ij} - q_{ij}) t_{ij} (y_i - y_j) $$\n\n### 2. Mini-Batch Gradient Estimator Design\n\nThe full gradient $g_i$ requires a sum over all $N-1$ other points. To create a mini-batch estimator, we sample a small subset of these points. Let $J_i = \\{j \\in \\{1, \\dots, N\\} \\mid j \\neq i\\}$ be the set of indices of other points. We sample a subset $S_i \\subset J_i$ of size $B$ without replacement.\n\nThe full gradient is a sum $g_i = \\sum_{j \\in J_i} \\mathbf{v}_{ij}$, where $\\mathbf{v}_{ij} = 4(p_{ij}-q_{ij})t_{ij}(y_i - y_j)$. An unbiased estimator for this sum can be constructed by scaling the partial sum over the mini-batch $S_i$:\n$$ \\widehat{g}_i = \\frac{|J_i|}{|S_i|} \\sum_{j \\in S_i} \\mathbf{v}_{ij} = \\frac{N-1}{B} \\sum_{j \\in S_i} \\mathbf{v}_{ij} $$\nTo confirm this is unbiased, we take its expectation over the random sampling of $S_i$. Let $\\delta_j$ be an indicator variable such that $\\delta_j=1$ if $j \\in S_i$ and $0$ otherwise. The probability that any given point $j \\in J_i$ is included in the sample is $P(j \\in S_i) = \\frac{B}{N-1}$.\n$$ \\mathbb{E}[\\widehat{g}_i] = \\mathbb{E}\\left[\\frac{N-1}{B} \\sum_{j \\in J_i} \\delta_j \\mathbf{v}_{ij}\\right] = \\frac{N-1}{B} \\sum_{j \\in J_i} \\mathbb{E}[\\delta_j] \\mathbf{v}_{ij} = \\frac{N-1}{B} \\sum_{j \\in J_i} P(j \\in S_i) \\mathbf{v}_{ij} $$\n$$ = \\frac{N-1}{B} \\sum_{j \\in J_i} \\frac{B}{N-1} \\mathbf{v}_{ij} = \\sum_{j \\in J_i} \\mathbf{v}_{ij} = g_i $$\nThe estimator is indeed unbiased. Note that for this to hold, the quantities $p_{ij}$, $q_{ij}$, and $t_{ij}$ used in the computation must be based on the full set of points, not just those in the mini-batch.\n\n### 3. One-Step Change Analysis\n\nWe analyze the one-step change in the objective function, $\\Delta = KL(P \\parallel Q(Y')) - KL(P \\parallel Q(Y))$. The full-gradient update is $Y' = Y - \\eta G$, where $G$ is the matrix of full gradients $g_i$. The mini-batch update is $\\widehat{Y} = Y - \\eta \\widehat{G}$, where $\\widehat{G}$ is the matrix of mini-batch gradient estimators $\\widehat{g}_i$. The corresponding objective changes are $\\Delta_{\\text{full}}$ and $\\Delta_{\\text{mb}}$.\n\nThe bias and variance of the one-step change are defined as:\n- Bias: $b = \\mathbb{E}[\\Delta_{\\text{mb}}] - \\Delta_{\\text{full}}$\n- Variance: $v = \\operatorname{Var}(\\Delta_{\\text{mb}})$\n\nWe estimate these quantities empirically. For a fixed initial state $(X, P, Y)$, we first compute $\\Delta_{\\text{full}}$ once. Then, we repeat the mini-batch sampling process $R$ times. In each replicate $r=1, \\dots, R$, we sample independent mini-batches $\\{S_i^{(r)}\\}_{i=1}^N$, compute the gradient matrix $\\widehat{G}^{(r)}$, the updated embedding $\\widehat{Y}^{(r)}$, and the resulting change $\\Delta^{(r)}_{\\text{mb}}$.\n\nThe empirical estimates for bias and variance are:\n- $\\hat{b} = \\bar{\\Delta}_{\\text{mb}} - \\Delta_{\\text{full}}$, where $\\bar{\\Delta}_{\\text{mb}} = \\frac{1}{R} \\sum_{r=1}^R \\Delta^{(r)}_{\\text{mb}}$\n- $\\hat{v} = \\frac{1}{R-1} \\sum_{r=1}^R (\\Delta^{(r)}_{\\text{mb}} - \\bar{\\Delta}_{\\text{mb}})^2$\n\nThe bias arises from the non-linearity of the objective function. A first-order Taylor expansion suggests the bias is zero, but a second-order expansion reveals a bias term proportional to $\\eta^2$ and the variance of the gradient estimator, i.e., $b \\approx \\frac{1}{2}\\eta^2 \\operatorname{Tr}(H \\operatorname{Cov}(\\widehat{G}))$, where $H$ is the Hessian of the objective. As the batch size $B$ approaches $N-1$, the variance of the gradient estimator approaches zero, and so should the bias and variance of $\\Delta_{\\text{mb}}$. In the limit $B=N-1$, the estimator is exact, so $b$ and $v$ must be zero.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main function to run the t-SNE mini-batch analysis for all test cases.\n    \"\"\"\n    # Use a fixed random seed for reproducibility of the entire process.\n    RNG = np.random.default_rng(seed=42)\n    CLIP_VAL = 1e-12\n\n    def h_beta(D_row, beta=1.0):\n        \"\"\"\n        Computes conditional probabilities P_j|i and shannon entropy H for beta = 1/(2*sigma^2).\n        Uses natural log for entropy. D_row are squared distances to other points.\n        \"\"\"\n        log_P = -D_row * beta\n        # Numerically stable softmax\n        shift = np.max(log_P)\n        log_sum_P = shift + np.log(np.sum(np.exp(log_P - shift)))\n        P = np.exp(log_P - log_sum_P)\n        H = -np.sum(P * np.log(P + CLIP_VAL))\n        return H, P\n\n    def search_sigma_beta(D_row, perplexity, tol=1e-5, max_iter=50):\n        \"\"\"\n        Binary search for beta that produces the target perplexity.\n        \"\"\"\n        log_perplexity = np.log(perplexity)\n        beta_min, beta_max = -np.inf, np.inf\n        beta = 1.0\n\n        for _ in range(max_iter):\n            H, _ = h_beta(D_row, beta)\n            H_diff = H - log_perplexity\n            if np.abs(H_diff)  tol:\n                break\n            if H_diff  0:\n                beta_min = beta\n                beta = (beta + beta_max) / 2.0 if beta_max != np.inf else beta * 2.0\n            else:\n                beta_max = beta\n                beta = (beta + beta_min) / 2.0 if beta_min != -np.inf else beta / 2.0\n        \n        _, P_row = h_beta(D_row, beta)\n        return P_row\n\n    def compute_P_matrix(X, perplexity):\n        \"\"\"Computes the symmetrized high-dimensional similarity matrix P.\"\"\"\n        N = X.shape[0]\n        D_sq = squareform(pdist(X, 'sqeuclidean'))\n        P = np.zeros((N, N))\n        \n        for i in range(N):\n            D_row = np.delete(D_sq[i, :], i)\n            P_row = search_sigma_beta(D_row, perplexity)\n            P[i, np.arange(N) != i] = P_row\n            \n        P = (P + P.T) / (2 * N)\n        return np.maximum(P, CLIP_VAL)\n\n    def compute_Q_matrix_and_T(Y):\n        \"\"\"Computes the low-dimensional similarity matrix Q and the unnormalized T.\"\"\"\n        D_sq_Y = squareform(pdist(Y, 'sqeuclidean'))\n        t_matrix = 1.0 / (1.0 + D_sq_Y)\n        np.fill_diagonal(t_matrix, 0)\n        \n        sum_t = np.sum(t_matrix)\n        Q = t_matrix / (sum_t if sum_t  0 else CLIP_VAL)\n        return np.maximum(Q, CLIP_VAL), t_matrix\n\n    def kl_divergence(P, Q):\n        \"\"\"Computes the KL divergence KL(P || Q).\"\"\"\n        return np.sum(P * np.log(P / Q))\n        \n    def tsne_gradient(P, Q, Y, t_matrix):\n        \"\"\"Computes the full gradient of the t-SNE objective.\"\"\"\n        N, m = Y.shape\n        grad = np.zeros((N, m))\n        PQ_T = (P - Q) * t_matrix\n        \n        for i in range(N):\n            Y_diff = Y[i, :] - Y\n            grad[i, :] = 4 * np.sum(PQ_T[i, :, np.newaxis] * Y_diff, axis=0)\n        return grad\n\n    def tsne_gradient_mb(P, Q, Y, t_matrix, B):\n        \"\"\"Computes the mini-batch gradient estimator.\"\"\"\n        N, m = Y.shape\n        grad_mb = np.zeros((N, m))\n        indices = np.arange(N)\n        \n        for i in range(N):\n            other_indices = np.delete(indices, i)\n            S_i = RNG.choice(other_indices, size=B, replace=False)\n            \n            PQ_T_i = (P[i, S_i] - Q[i, S_i]) * t_matrix[i, S_i]\n            Y_diff_i = Y[i, :] - Y[S_i, :]\n            \n            grad_i_sum = np.sum(PQ_T_i[:, np.newaxis] * Y_diff_i, axis=0)\n            grad_mb[i, :] = 4 * ((N - 1) / B) * grad_i_sum\n\n        return grad_mb\n\n    def analyze_case(params):\n        \"\"\"Runs the full analysis for a single set of parameters.\"\"\"\n        N, D, perplexity, B, R, eta = params\n        \n        mean_vec = np.zeros(D)\n        mean_vec[0] = 5.0\n        X1 = RNG.standard_normal((N // 2, D)) + mean_vec\n        X2 = RNG.standard_normal((N - N // 2, D)) - mean_vec\n        X = np.vstack((X1, X2))\n        \n        P = compute_P_matrix(X, perplexity)\n        \n        m = 2\n        Y = RNG.standard_normal((N, m)) * 1e-4\n\n        Q_old, t_matrix_old = compute_Q_matrix_and_T(Y)\n        kl_old = kl_divergence(P, Q_old)\n        \n        G_full = tsne_gradient(P, Q_old, Y, t_matrix_old)\n        Y_new_full = Y - eta * G_full\n        Q_new_full, _ = compute_Q_matrix_and_T(Y_new_full)\n        delta_full = kl_divergence(P, Q_new_full) - kl_old\n        \n        delta_mbs = np.zeros(R)\n        for r in range(R):\n            G_mb = tsne_gradient_mb(P, Q_old, Y, t_matrix_old, B)\n            Y_new_mb = Y - eta * G_mb\n            Q_new_mb, _ = compute_Q_matrix_and_T(Y_new_mb)\n            delta_mbs[r] = kl_divergence(P, Q_new_mb) - kl_old\n            \n        mean_delta_mb = np.mean(delta_mbs)\n        bias = mean_delta_mb - delta_full\n        variance = np.var(delta_mbs, ddof=1) if R  1 else 0.0\n\n        # For the full batch case, bias and variance are analytically zero.\n        if B == N - 1:\n            bias, variance = 0.0, 0.0\n\n        return [bias, variance]\n    \n    test_cases = [\n        (32, 10, 10, 1, 128, 0.1),\n        (32, 10, 10, 8, 128, 0.1),\n        (32, 10, 10, 16, 128, 0.1),\n        (32, 10, 10, 31, 64, 0.1),\n    ]\n\n    results = [analyze_case(params) for params in test_cases]\n    \n    output_str = '[' + ','.join([f'[{b},{v}]' for b, v in results]) + ']'\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A t-SNE visualization can be powerfully insightful, but how do we quantify its faithfulness to the original data? This practice moves beyond visual intuition by implementing a \"trustworthiness\" metric that assesses the preservation of local neighborhoods for each individual point. By computing this score, you will learn to create diagnostic tools that identify potentially misleading regions in an embedding, fostering a critical approach to interpreting dimensionality reduction results. ",
            "id": "3179626",
            "problem": "You are given two Euclidean spaces: a high-dimensional data space and a low-dimensional embedding space arising from t-distributed stochastic neighbor embedding (t-SNE). For a finite set of points indexed by $i \\in \\{0,1,\\dots,N-1\\}$, let $X = \\{x_i \\in \\mathbb{R}^D\\}$ denote the data space coordinates and $Y = \\{y_i \\in \\mathbb{R}^d\\}$ denote the embedding space coordinates. For a chosen neighborhood size $k \\in \\{1,2,\\dots,N-1\\}$, define the $k$-nearest neighbor set in the data space for point $i$ as $N_k^X(i)$ and in the embedding space as $N_k^Y(i)$, using the Euclidean norm. The Euclidean distance between two points $a,b \\in \\mathbb{R}^m$ is $\\|a-b\\|_2 = \\sqrt{\\sum_{j=1}^m (a_j-b_j)^2}$. The $k$-nearest neighbor set for point $i$ is obtained by sorting all indices $j \\neq i$ by increasing distance $\\|x_i - x_j\\|_2$ (respectively $\\|y_i - y_j\\|_2$), breaking any exact distance ties by smaller index $j$, and then taking the first $k$ indices. For each point $i$, define the per-point neighbor preservation fraction\n$$\np_i(k) \\;=\\; \\frac{\\left|\\,N_k^X(i) \\,\\cap\\, N_k^Y(i)\\,\\right|}{k}.\n$$\nThis quantity $p_i(k)$ lies in $[0,1]$ and serves as a trustworthiness indicator at the level of individual points for a given $k$.\n\nYour task is to write a complete program that, for each specified test case, computes the vector of per-point neighbor preservation fractions $[p_0(k),p_1(k),\\dots,p_{N-1}(k)]$ and identifies indices whose local neighborhoods were not preserved beyond a diagnostic tolerance. Given a threshold $\\tau \\in [0,1]$, define the set of harmed points as those indices $i$ with $p_i(k)  \\tau$. This can be used to diagnose regions harmed by mechanisms such as early exaggeration or approximation in t-distributed stochastic neighbor embedding (t-SNE), which are known to potentially disrupt local neighborhoods for some points even when global structure appears reasonable.\n\nUse only the foundational definitions above; do not rely on prepackaged neighbor-preservation or trustworthiness functions. Implement Euclidean distances, deterministic tie-breaking by smaller index, and strict set intersection as stated. For each test case, output two items: the list $[p_0(k),\\dots,p_{N-1}(k)]$ with each value rounded to three decimal places, and the sorted list of harmed indices $\\{i : p_i(k)  \\tau\\}$.\n\nTest suite. Use the following four test cases. In each case, $X$ and $Y$ are given explicitly.\n\n- Test case $1$ (happy path: monotone scaling in $Y$ preserves neighbors):\n  - $N = 6$, $D = 1$, $d = 1$, $k = 2$, $\\tau = 1.0$.\n  - $X^{(1)} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 3 \\\\ 7 \\\\ 12 \\\\ 18 \\end{bmatrix}$.\n  - $Y^{(1)} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 6 \\\\ 14 \\\\ 24 \\\\ 36 \\end{bmatrix}$.\n- Test case $2$ (diagnosing a locally harmed region due to exaggeration-like distortion in $Y$):\n  - $N = 6$, $D = 1$, $d = 1$, $k = 2$, $\\tau = 0.75$.\n  - $X^{(2)} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 3 \\\\ 7 \\\\ 12 \\\\ 18 \\end{bmatrix}$.\n  - $Y^{(2)} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 20 \\\\ 14 \\\\ 24 \\\\ 36 \\end{bmatrix}$.\n- Test case $3$ (approximation-like perturbation that flips nearest neighbors with $k=1$):\n  - $N = 5$, $D = 2$, $d = 2$, $k = 1$, $\\tau = 1.0$.\n  - $X^{(3)} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.9  0.05 \\\\ 1.8  0.2 \\\\ 3.0  5.0 \\\\ 3.1  5.05 \\end{bmatrix}$.\n  - $Y^{(3)} = \\begin{bmatrix} 0.0  0.0 \\\\ 0.9  0.05 \\\\ 0.3  0.02 \\\\ 3.0  5.0 \\\\ 3.1  5.05 \\end{bmatrix}$.\n- Test case $4$ (boundary: $k = N-1$ makes every point’s neighbor set be all other points):\n  - $N = 4$, $D = 2$, $d = 2$, $k = 3$, $\\tau = 1.0$.\n  - $X^{(4)} = \\begin{bmatrix} 0  0 \\\\ 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$.\n  - $Y^{(4)} = \\begin{bmatrix} 1  1 \\\\ 0  1 \\\\ 1  0 \\\\ 0  0 \\end{bmatrix}$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list: the first element is the list $[p_0(k),\\dots,p_{N-1}(k)]$ in the order of point indices, rounded to three decimal places, and the second element is the sorted list of harmed indices with $p_i(k)  \\tau$. For example, the overall structure must be of the form\n$[[[p^{(1)}_0,\\dots,p^{(1)}_{N-1}],[\\text{harmed}^{(1)}]], [[p^{(2)}_0,\\dots],[\\text{harmed}^{(2)}]], [[p^{(3)}_0,\\dots],[\\text{harmed}^{(3)}]], [[p^{(4)}_0,\\dots],[\\text{harmed}^{(4)}]]]$.",
            "solution": "The problem requires the computation of a per-point local neighborhood preservation metric for a t-SNE-style embedding. This metric, denoted $p_i(k)$, quantifies the fraction of the $k$-nearest neighbors of a point $x_i$ in a high-dimensional data space $X$ that are also among the $k$-nearest neighbors of its corresponding point $y_i$ in a low-dimensional embedding space $Y$. The objective is to implement a program to calculate the vector of these preservation fractions, $[p_0(k), p_1(k), \\ldots, p_{N-1}(k)]$, and to identify a set of \"harmed\" points for which this preservation fraction falls below a specified tolerance threshold $\\tau$.\n\nThe problem is well-posed and scientifically grounded. All terms are defined with mathematical precision. The data space coordinates are given by the set $X = \\{x_i \\in \\mathbb{R}^D\\}_{i=0}^{N-1}$ and the embedding space coordinates by $Y = \\{y_i \\in \\mathbb{R}^d\\}_{i=0}^{N-1}$. The distance metric is the standard Euclidean norm, $\\|a-b\\|_2 = \\sqrt{\\sum_{j=1}^m (a_j-b_j)^2}$. A crucial detail is the deterministic tie-breaking rule: when two points are equidistant from a reference point, the point with the smaller index is considered closer. This ensures that the $k$-nearest neighbor sets, $N_k^X(i)$ and $N_k^Y(i)$, are uniquely defined for any point $i$ and any neighborhood size $k \\in \\{1, 2, \\ldots, N-1\\}$.\n\nThe per-point neighbor preservation fraction is defined as:\n$$\np_i(k) \\;=\\; \\frac{\\left|\\,N_k^X(i) \\,\\cap\\, N_k^Y(i)\\,\\right|}{k}\n$$\nThis value ranges from $0$ (no neighbors preserved) to $1$ (all $k$ neighbors preserved). The set of harmed points is then defined as all indices $i$ such that $p_i(k)  \\tau$.\n\nThe algorithmic procedure to compute the required outputs for each test case is as follows. For each point $i$ from $0$ to $N-1$:\n\n1.  **Construct Neighbor Lists**:\n    -   For the data space $X$, create a list of tuples $(d_{ij}^X, j)$ for all $j \\neq i$, where $d_{ij}^X = \\|x_i - x_j\\|_2$.\n    -   For the embedding space $Y$, create a list of tuples $(d_{ij}^Y, j)$ for all $j \\neq i$, where $d_{ij}^Y = \\|y_i - y_j\\|_2$.\n\n2.  **Sort and Determine k-NN Sets**:\n    -   Sort the list from the $X$ space based on a primary key of distance $d_{ij}^X$ (ascending) and a secondary key of index $j$ (ascending) to enforce the tie-breaking rule. The set of the first $k$ indices from this sorted list constitutes the $k$-nearest neighbor set $N_k^X(i)$.\n    -   Perform the identical sorting procedure for the list from the $Y$ space to determine the set $N_k^Y(i)$.\n\n3.  **Calculate Preservation Fraction**:\n    -   Compute the intersection of the two sets: $I_i = N_k^X(i) \\cap N_k^Y(i)$.\n    -   Calculate the preservation fraction $p_i(k) = |I_i| / k$.\n\n4.  **Identify Harmed Points**:\n    -   After computing $p_i(k)$ for all $i \\in \\{0, \\ldots, N-1\\}$, compare each $p_i(k)$ to the threshold $\\tau$. The set of harmed indices is $\\{i \\mid p_i(k)  \\tau\\}$.\n\nThis algorithm is applied to each of the four provided test cases.\n\n-   **Test Case 1** ($N=6, k=2, \\tau=1.0$): The embedding $Y^{(1)}$ is a linear scaling of the data $X^{(1)}$, i.e., $y_i = 2x_i$. This transformation preserves the ordering of all inter-point distances. Consequently, for every point $i$, the $k$-nearest neighbors are identical in both spaces: $N_k^X(i) = N_k^Y(i)$. This results in a perfect preservation fraction $p_i(k) = 1.0$ for all $i$. No points are harmed as $p_i(k)$ is not less than $\\tau=1.0$.\n\n-   **Test Case 2** ($N=6, k=2, \\tau=0.75$): The coordinate $y_2=20$ is a significant distortion from the original data's structure, where $x_2=3$ is close to $x_1=1$ and $x_0=0$. This displacement moves point $2$ far away from points $0$ and $1$ in the embedding space $Y$, disrupting their local neighborhoods. Specifically, for point $i=2$, its neighbors in $X$ are $\\{0,1\\}$ but become $\\{3,4\\}$ in $Y$, yielding $p_2(2)=0.0$. Similar disruptions affect the neighborhoods of points $0$ and $1$, leading to $p_0(2)=0.5$ and $p_1(2)=0.5$. The remaining points $3, 4, 5$ are far from the perturbation and retain their neighborhoods, so $p_i(2)=1.0$ for $i \\in \\{3,4,5\\}$. Indices $0, 1, 2$ are harmed as their $p_i(k)$ values are below $\\tau=0.75$.\n\n-   **Test Case 3** ($N=5, k=1, \\tau=1.0$): This case illustrates a local scrambling. In space $X$, the points with indices $0, 1, 2$ form a chain where $1$ is the nearest neighbor of $0$ and $2$, and $0$ is the nearest neighbor of $1$. In space $Y$, point $y_2$ is moved to be very close to $y_0$. This breaks the original neighborhood structure. The nearest neighbor of $y_0$ becomes $y_2$ (not $y_1$), the nearest neighbor of $y_1$ becomes $y_2$ (not $y_0$), and the nearest neighbor of $y_2$ becomes $y_0$ (not $y_1$). In all three cases, the single nearest neighbor is not preserved, so $p_0(1)=p_1(1)=p_2(1)=0.0$. The pair of points $(3,4)$ is distant from this scramble and their mutual nearest-neighbor relationship is preserved, so $p_3(1)=p_4(1)=1.0$. The harmed indices are $\\{0, 1, 2\\}$ as their preservation is less than $\\tau=1.0$.\n\n-   **Test Case 4** ($N=4, k=3, \\tau=1.0$): This is a boundary case where $k=N-1=3$. For any point $i$, its set of neighbors is trivially the set of all other points in the dataset. That is, $N_3^X(i) = N_3^Y(i) = \\{0, 1, 2, 3\\} \\setminus \\{i\\}$. The intersection is perfect by definition. Therefore, $p_i(3) = 3/3 = 1.0$ for all $i \\in \\{0,1,2,3\\}$. No points are harmed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_k_nearest_neighbors(points, i, k):\n    \"\"\"\n    Finds the k-nearest neighbors for point i in the given set of points.\n\n    Args:\n        points (np.ndarray): An NxD or Nxd array of points.\n        i (int): The index of the reference point.\n        k (int): The number of neighbors to find.\n\n    Returns:\n        set: A set of indices of the k nearest neighbors.\n    \"\"\"\n    N = points.shape[0]\n    distances = []\n    current_point = points[i]\n\n    for j in range(N):\n        if i == j:\n            continue\n        other_point = points[j]\n        # Euclidean distance calculation as per the problem definition\n        dist = np.linalg.norm(current_point - other_point)\n        # Store as a tuple of (distance, index) for sorting\n        distances.append((dist, j))\n\n    # Sort by distance (primary key) and then by index (secondary key for tie-breaking)\n    distances.sort(key=lambda x: (x[0], x[1]))\n\n    # Extract the indices of the first k neighbors\n    neighbors = {d[1] for d in distances[:k]}\n    return neighbors\n\ndef solve():\n    \"\"\"\n    Solves the neighbor preservation problem for the four given test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 6, \"k\": 2, \"tau\": 1.0,\n            \"X\": np.array([[0.0], [1.0], [3.0], [7.0], [12.0], [18.0]]),\n            \"Y\": np.array([[0.0], [2.0], [6.0], [14.0], [24.0], [36.0]])\n        },\n        {\n            \"N\": 6, \"k\": 2, \"tau\": 0.75,\n            \"X\": np.array([[0.0], [1.0], [3.0], [7.0], [12.0], [18.0]]),\n            \"Y\": np.array([[0.0], [2.0], [20.0], [14.0], [24.0], [36.0]])\n        },\n        {\n            \"N\": 5, \"k\": 1, \"tau\": 1.0,\n            \"X\": np.array([[0.0, 0.0], [0.9, 0.05], [1.8, 0.2], [3.0, 5.0], [3.1, 5.05]]),\n            \"Y\": np.array([[0.0, 0.0], [0.9, 0.05], [0.3, 0.02], [3.0, 5.0], [3.1, 5.05]])\n        },\n        {\n            \"N\": 4, \"k\": 3, \"tau\": 1.0,\n            \"X\": np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"Y\": np.array([[1.0, 1.0], [0.0, 1.0], [1.0, 0.0], [0.0, 0.0]])\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        k = case[\"k\"]\n        tau = case[\"tau\"]\n        X = case[\"X\"]\n        Y = case[\"Y\"]\n\n        p_values = []\n        harmed_indices = []\n\n        for i in range(N):\n            # Find k-nearest neighbors in both spaces\n            neighbors_X = get_k_nearest_neighbors(X, i, k)\n            neighbors_Y = get_k_nearest_neighbors(Y, i, k)\n\n            # Calculate the size of the intersection\n            intersection_size = len(neighbors_X.intersection(neighbors_Y))\n            \n            # Calculate preservation fraction p_i(k)\n            if k  0:\n                p_i_k = intersection_size / k\n            else:\n                p_i_k = 1.0 # By convention for k=0, though k = 1 in this problem\n            \n            p_values.append(p_i_k)\n\n            # Check if the point is harmed\n            if p_i_k  tau:\n                harmed_indices.append(i)\n        \n        # Round p_values to three decimal places\n        rounded_p_values = [round(p, 3) for p in p_values]\n        \n        # Assemble the result for this test case\n        # harmed_indices is naturally sorted as the loop iterates from i=0 to N-1\n        case_result = [rounded_p_values, harmed_indices]\n        all_results.append(case_result)\n\n    # Format the final output string to match the exact requirement (no spaces)\n    # The default str() representation of lists is used, and then spaces are removed.\n    final_output_str = str(all_results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}