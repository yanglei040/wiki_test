{
    "hands_on_practices": [
        {
            "introduction": "The Expectation-Maximization (EM) algorithm can seem abstract at first. The best way to grasp its mechanics is to perform a step-by-step calculation. This first exercise  guides you through a single, complete iteration of the EM algorithm for a simple two-component Gaussian Mixture Model. By manually calculating the responsibilities in the E-step and then using them to update the model parameters in the M-step, you will build a concrete understanding of how EM iteratively refines its estimates to fit the data.",
            "id": "1960172",
            "problem": "A researcher is analyzing a dataset of measurements, which is believed to be sampled from a mixture of two distinct populations, A and B. They decide to model the data using a two-component Gaussian Mixture Model (GMM). The probability density function (PDF) for a single data point $x$ is given by:\n$$p(x | \\theta) = \\pi_A \\mathcal{N}(x | \\mu_A, \\sigma_A^2) + \\pi_B \\mathcal{N}(x | \\mu_B, \\sigma_B^2)$$\nwhere $\\pi_A$ and $\\pi_B$ are the mixing proportions (with $\\pi_A + \\pi_B = 1$), and $\\mathcal{N}(x | \\mu, \\sigma^2)$ is the Gaussian PDF:\n$$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n\nThe dataset consists of the following five measurements: $\\{4.0, 4.5, 5.0, 8.0, 9.0\\}$.\nFor simplicity, the variances of the two components are assumed to be known and fixed at $\\sigma_A^2 = \\sigma_B^2 = 1.0$.\n\nThe researcher initializes the model parameters for the Expectation-Maximization (EM) algorithm as follows:\n- Initial means: $\\mu_A^{(0)} = 4.2$ and $\\mu_B^{(0)} = 8.8$.\n- Initial mixing proportions: $\\pi_A^{(0)} = 0.5$ and $\\pi_B^{(0)} = 0.5$.\n\nYour task is to perform a single, full iteration of the EM algorithm, which consists of one Expectation step (E-step) followed by one Maximization step (M-step), to compute the updated parameters.\n\nCalculate the updated value for the mean of the first component, $\\mu_A^{(1)}$. Report your answer as a real number, rounded to four significant figures.",
            "solution": "We apply one Expectation-Maximization (EM) iteration for the two-component Gaussian Mixture Model with fixed variances $\\sigma_{A}^{2}=\\sigma_{B}^{2}=1$ and initial parameters $\\mu_{A}^{(0)}=4.2$, $\\mu_{B}^{(0)}=8.8$, $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}=0.5$.\n\nE-step (responsibilities):\nFor each data point $x_{i}$, the responsibility of component $A$ is\n$$\nr_{iA} \\equiv \\gamma_{iA} = \\frac{\\pi_{A}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{A}^{(0)},1)}{\\pi_{A}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{A}^{(0)},1) + \\pi_{B}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{B}^{(0)},1)}.\n$$\nWith $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}$ and equal variances, the common factors cancel:\n$$\nr_{iA}=\\frac{\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}}{2}\\right)}{\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}}{2}\\right)+\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{B}^{(0)})^{2}}{2}\\right)}=\\frac{1}{1+\\exp\\!\\left(\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}-(x_{i}-\\mu_{B}^{(0)})^{2}}{2}\\right)}.\n$$\nFor the dataset $\\{4.0,4.5,5.0,8.0,9.0\\}$ and $(\\mu_{A}^{(0)},\\mu_{B}^{(0)})=(4.2,8.8)$:\n- For $x_{1}=4.0$: $(x_{1}-\\mu_{A}^{(0)})^{2}=0.04$, $(x_{1}-\\mu_{B}^{(0)})^{2}=23.04$, so\n$$\nr_{1A}=\\frac{1}{1+\\exp(-11.5)}\\approx 0.9999898625.\n$$\n- For $x_{2}=4.5$: $(x_{2}-\\mu_{A}^{(0)})^{2}=0.09$, $(x_{2}-\\mu_{B}^{(0)})^{2}=18.49$, so\n$$\nr_{2A}=\\frac{1}{1+\\exp(-9.2)}\\approx 0.9998989606.\n$$\n- For $x_{3}=5.0$: $(x_{3}-\\mu_{A}^{(0)})^{2}=0.64$, $(x_{3}-\\mu_{B}^{(0)})^{2}=14.44$, so\n$$\nr_{3A}=\\frac{1}{1+\\exp(-6.9)}\\approx 0.9989932309.\n$$\n- For $x_{4}=8.0$: $(x_{4}-\\mu_{A}^{(0)})^{2}=14.44$, $(x_{4}-\\mu_{B}^{(0)})^{2}=0.64$, so\n$$\nr_{4A}=\\frac{1}{1+\\exp(6.9)}=1-r_{3A}\\approx 0.0010067691.\n$$\n- For $x_{5}=9.0$: $(x_{5}-\\mu_{A}^{(0)})^{2}=23.04$, $(x_{5}-\\mu_{B}^{(0)})^{2}=0.04$, so\n$$\nr_{5A}=\\frac{1}{1+\\exp(11.5)}=1-r_{1A}\\approx 0.0000101375.\n$$\nNote $r_{1A}+r_{5A}=1$ and $r_{3A}+r_{4A}=1$ by symmetry.\n\nM-step (update mean of component A):\nWith fixed variance, the updated mean is the responsibility-weighted average:\n$$\n\\mu_{A}^{(1)}=\\frac{\\sum_{i=1}^{5} r_{iA} x_{i}}{\\sum_{i=1}^{5} r_{iA}}.\n$$\nCompute the denominator using symmetry:\n$$\n\\sum_{i=1}^{5} r_{iA}=(r_{1A}+r_{5A})+(r_{3A}+r_{4A})+r_{2A}=2+r_{2A}\\approx 2.9998989606.\n$$\nCompute the numerator; group symmetric pairs:\n$$\n\\sum_{i=1}^{5} r_{iA} x_{i}=(r_{1A}\\cdot 4.0 + r_{5A}\\cdot 9.0) + (r_{3A}\\cdot 5.0 + r_{4A}\\cdot 8.0) + r_{2A}\\cdot 4.5.\n$$\nUse $r_{5A}=1-r_{1A}$ and $r_{4A}=1-r_{3A}$:\n$$\nr_{1A}\\cdot 4.0 + r_{5A}\\cdot 9.0 = 9 - 5 r_{1A},\\quad\nr_{3A}\\cdot 5.0 + r_{4A}\\cdot 8.0 = 8 - 3 r_{3A}.\n$$\nThus\n$$\n\\sum_{i=1}^{5} r_{iA} x_{i} = 17 - 5 r_{1A} - 3 r_{3A} + 4.5 r_{2A} \\approx 13.5026163178.\n$$\nTherefore,\n$$\n\\mu_{A}^{(1)}=\\frac{13.5026163178}{2.9998989606}\\approx 4.501023693.\n$$\nRounded to four significant figures, the updated mean is $4.501$.",
            "answer": "$$\\boxed{4.501}$$"
        },
        {
            "introduction": "While often introduced with Gaussian mixtures, the EM algorithm is a general framework applicable to a wide variety of models with latent variables. This practice  challenges you to apply EM to a mixture of exponential distributions. You will first derive the M-step update rule for the exponential rate parameter from the expected complete-data log-likelihood, reinforcing the theoretical foundation of the algorithm, and then apply your result to a numerical example.",
            "id": "3119716",
            "problem": "Consider a finite mixture of exponential distributions with $K$ components. The observed data are $x_{1},\\dots,x_{N}$, each $x_{n} \\in \\mathbb{R}_{+}$, assumed independent and identically distributed (i.i.d.) from the mixture model with parameters $\\{\\pi_{k}\\}_{k=1}^{K}$ and $\\{\\lambda_{k}\\}_{k=1}^{K}$, where $\\pi_{k} \\ge 0$, $\\sum_{k=1}^{K} \\pi_{k} = 1$, and $\\lambda_{k} > 0$. The exponential probability density function is $p(x \\mid \\lambda) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$. Introduce latent indicator variables $z_{n,k} \\in \\{0,1\\}$ satisfying $\\sum_{k=1}^{K} z_{n,k} = 1$. Starting from Bayes’ rule and the complete-data likelihood $p(\\{x_{n}\\},\\{z_{n,k}\\}\\mid \\{\\pi_{k}\\},\\{\\lambda_{k}\\})$, derive the expectation-maximization (EM) responsibilities $\\gamma_{n,k} = \\mathbb{E}[z_{n,k} \\mid x_{n}]$ and the maximization-step (M-step) update for the exponential rates $\\lambda_{k}$ by maximizing the expected complete-data log-likelihood with respect to $\\lambda_{k}$.\n\nThen, apply your derivations to the following specific instance. Let $K=2$, $N=4$, data $x_{1} = 0.2$, $x_{2} = 0.6$, $x_{3} = 1.0$, $x_{4} = 2.0$. Suppose the current parameters at iteration $t$ are $\\pi_{1}^{(t)} = 0.7$, $\\pi_{2}^{(t)} = 0.3$, $\\lambda_{1}^{(t)} = 2.0$, $\\lambda_{2}^{(t)} = 0.5$. Compute the responsibilities $\\gamma_{n,1}$ for $n=1,2,3,4$, and then compute the updated rate $\\lambda_{1}^{(t+1)}$ using your M-step expression. Round your final numerical answer to four significant figures. Report only the value of $\\lambda_{1}^{(t+1)}$.",
            "solution": "The problem asks for the derivation of the expectation-maximization (EM) algorithm update equations for a mixture of exponential distributions and the application of these equations to a specific numerical example.\n\nFirst, we derive the general equations. The model is a mixture of $K$ exponential distributions. The observed data are $\\mathbf{x} = \\{x_1, \\dots, x_N\\}$, with each $x_n \\in \\mathbb{R}_{+}$. The parameters are $\\theta = \\{\\{\\pi_k\\}_{k=1}^K, \\{\\lambda_k\\}_{k=1}^K\\}$. The probability density function (PDF) for a single exponential distribution with rate $\\lambda_k$ is $p(x | \\lambda_k) = \\lambda_k \\exp(-\\lambda_k x)$. The latent variables $z_{n,k} \\in \\{0,1\\}$ indicate which component generated data point $x_n$, with $\\sum_{k=1}^K z_{n,k} = 1$. The prior probability of $z_{n,k}=1$ is the mixing coefficient $\\pi_k$.\n\nThe complete-data likelihood, given the observed data $\\mathbf{x}$ and the latent variables $\\mathbf{z} = \\{z_{n,k}\\}$, is:\n$$p(\\mathbf{x}, \\mathbf{z} \\mid \\theta) = \\prod_{n=1}^N p(x_n, \\mathbf{z}_n \\mid \\theta) = \\prod_{n=1}^N \\prod_{k=1}^K [p(x_n \\mid z_{n,k}=1, \\theta) p(z_{n,k}=1 \\mid \\theta)]^{z_{n,k}}$$\n$$p(\\mathbf{x}, \\mathbf{z} \\mid \\theta) = \\prod_{n=1}^N \\prod_{k=1}^K [\\pi_k p(x_n \\mid \\lambda_k)]^{z_{n,k}}$$\nSubstituting the exponential PDF, we get:\n$$p(\\mathbf{x}, \\mathbf{z} \\mid \\theta) = \\prod_{n=1}^N \\prod_{k=1}^K [\\pi_k \\lambda_k \\exp(-\\lambda_k x_n)]^{z_{n,k}}$$\nThe complete-data log-likelihood is:\n$$\\ln p(\\mathbf{x}, \\mathbf{z} \\mid \\theta) = \\sum_{n=1}^N \\sum_{k=1}^K z_{n,k} (\\ln \\pi_k + \\ln \\lambda_k - \\lambda_k x_n)$$\n\nThe EM algorithm proceeds in two steps: Expectation (E-step) and Maximization (M-step).\n\n**E-step: Derivation of Responsibilities**\nIn the E-step, we compute the posterior probability of the latent variables given the data and the current parameter estimates $\\theta^{(t)} = \\{\\pi_k^{(t)}, \\lambda_k^{(t)}\\}$. This posterior probability is the responsibility, $\\gamma_{n,k}$.\n$$\\gamma_{n,k} = p(z_{n,k}=1 \\mid x_n, \\theta^{(t)}) = \\mathbb{E}[z_{n,k} \\mid x_n, \\theta^{(t)}]$$\nUsing Bayes' rule:\n$$\\gamma_{n,k} = \\frac{p(x_n \\mid z_{n,k}=1, \\theta^{(t)}) p(z_{n,k}=1 \\mid \\theta^{(t)})}{\\sum_{j=1}^K p(x_n \\mid z_{n,j}=1, \\theta^{(t)}) p(z_{n,j}=1 \\mid \\theta^{(t)})} = \\frac{\\pi_k^{(t)} p(x_n \\mid \\lambda_k^{(t)})}{\\sum_{j=1}^K \\pi_j^{(t)} p(x_n \\mid \\lambda_j^{(t)})}$$\nSubstituting the exponential PDF, we obtain the expression for the responsibilities:\n$$\\gamma_{n,k} = \\frac{\\pi_k^{(t)} \\lambda_k^{(t)} \\exp(-\\lambda_k^{(t)} x_n)}{\\sum_{j=1}^K \\pi_j^{(t)} \\lambda_j^{(t)} \\exp(-\\lambda_j^{(t)} x_n)}$$\n\n**M-step: Derivation of the Update for $\\lambda_k$**\nIn the M-step, we maximize the expected complete-data log-likelihood, denoted $Q(\\theta \\mid \\theta^{(t)})$, with respect to the parameters $\\theta$.\n$$Q(\\theta \\mid \\theta^{(t)}) = \\mathbb{E}_{\\mathbf{z}\\mid\\mathbf{x}, \\theta^{(t)}}[\\ln p(\\mathbf{x}, \\mathbf{z} \\mid \\theta)] = \\sum_{n=1}^N \\sum_{k=1}^K \\mathbb{E}[z_{n,k}] (\\ln \\pi_k + \\ln \\lambda_k - \\lambda_k x_n)$$\nSince $\\mathbb{E}[z_{n,k}] = \\gamma_{n,k}$, we have:\n$$Q(\\theta \\mid \\theta^{(t)}) = \\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{n,k} (\\ln \\pi_k + \\ln \\lambda_k - \\lambda_k x_n)$$\nTo find the update for $\\lambda_k$, we maximize $Q$ with respect to $\\lambda_k$. We differentiate $Q$ with respect to $\\lambda_k$ and set the derivative to zero. We only need to consider terms involving $\\lambda_k$:\n$$\\frac{\\partial Q}{\\partial \\lambda_k} = \\frac{\\partial}{\\partial \\lambda_k} \\sum_{n=1}^N \\gamma_{n,k} (\\ln \\lambda_k - \\lambda_k x_n) = \\sum_{n=1}^N \\gamma_{n,k} \\left( \\frac{1}{\\lambda_k} - x_n \\right) = 0$$\nSolving for $\\lambda_k$:\n$$\\frac{1}{\\lambda_k} \\sum_{n=1}^N \\gamma_{n,k} = \\sum_{n=1}^N \\gamma_{n,k} x_n$$\nThis gives the M-step update equation for the rate parameter $\\lambda_k$:\n$$\\lambda_k^{(t+1)} = \\frac{\\sum_{n=1}^N \\gamma_{n,k}}{\\sum_{n=1}^N \\gamma_{n,k} x_n}$$\n\nNow we apply these derivations to the specific instance.\nThe given data are $K=2$, $N=4$, $x_1=0.2$, $x_2=0.6$, $x_3=1.0$, $x_4=2.0$.\nThe parameters at iteration $t$ are $\\pi_1^{(t)} = 0.7$, $\\pi_2^{(t)} = 0.3$, $\\lambda_1^{(t)} = 2.0$, $\\lambda_2^{(t)} = 0.5$.\n\nFirst, we compute the responsibilities $\\gamma_{n,1}$ for $n=1,2,3,4$ using the derived E-step formula.\nFor a generic data point $x_n$, let the numerator terms be $A_n = \\pi_1^{(t)} \\lambda_1^{(t)} \\exp(-\\lambda_1^{(t)} x_n)$ and $B_n = \\pi_2^{(t)} \\lambda_2^{(t)} \\exp(-\\lambda_2^{(t)} x_n)$. Then $\\gamma_{n,1} = A_n / (A_n + B_n)$.\n\nFor $x_1 = 0.2$:\n$A_1 = 0.7 \\times 2.0 \\times \\exp(-2.0 \\times 0.2) = 1.4 \\exp(-0.4) \\approx 0.93845$\n$B_1 = 0.3 \\times 0.5 \\times \\exp(-0.5 \\times 0.2) = 0.15 \\exp(-0.1) \\approx 0.13573$\n$\\gamma_{1,1} = \\frac{0.93845}{0.93845 + 0.13573} \\approx 0.87365$\n\nFor $x_2 = 0.6$:\n$A_2 = 1.4 \\exp(-2.0 \\times 0.6) = 1.4 \\exp(-1.2) \\approx 0.42167$\n$B_2 = 0.15 \\exp(-0.5 \\times 0.6) = 0.15 \\exp(-0.3) \\approx 0.11112$\n$\\gamma_{2,1} = \\frac{0.42167}{0.42167 + 0.11112} \\approx 0.79143$\n\nFor $x_3 = 1.0$:\n$A_3 = 1.4 \\exp(-2.0 \\times 1.0) = 1.4 \\exp(-2.0) \\approx 0.18947$\n$B_3 = 0.15 \\exp(-0.5 \\times 1.0) = 0.15 \\exp(-0.5) \\approx 0.09098$\n$\\gamma_{3,1} = \\frac{0.18947}{0.18947 + 0.09098} \\approx 0.67563$\n\nFor $x_4 = 2.0$:\n$A_4 = 1.4 \\exp(-2.0 \\times 2.0) = 1.4 \\exp(-4.0) \\approx 0.02564$\n$B_4 = 0.15 \\exp(-0.5 \\times 2.0) = 0.15 \\exp(-1.0) \\approx 0.05518$\n$\\gamma_{4,1} = \\frac{0.02564}{0.02564 + 0.05518} \\approx 0.31726$\n\nNext, we compute the updated parameter $\\lambda_1^{(t+1)}$ using the M-step formula.\nNumerator: Sum of responsibilities for component $1$.\n$$\\sum_{n=1}^4 \\gamma_{n,1} \\approx 0.87365 + 0.79143 + 0.67563 + 0.31726 \\approx 2.65797$$\nDenominator: Weighted sum of data points for component $1$.\n$$\\sum_{n=1}^4 \\gamma_{n,1} x_n \\approx (0.87365 \\times 0.2) + (0.79143 \\times 0.6) + (0.67563 \\times 1.0) + (0.31726 \\times 2.0)$$\n$$\\sum_{n=1}^4 \\gamma_{n,1} x_n \\approx 0.17473 + 0.47486 + 0.67563 + 0.63452 \\approx 1.95974$$\nFinally, we compute the updated rate $\\lambda_1^{(t+1)}$:\n$$\\lambda_1^{(t+1)} = \\frac{\\sum_{n=1}^4 \\gamma_{n,1}}{\\sum_{n=1}^4 \\gamma_{n,1} x_n} \\approx \\frac{2.65797}{1.95974} \\approx 1.356294$$\nRounding the final answer to four significant figures gives $1.356$.",
            "answer": "$$\\boxed{1.356}$$"
        },
        {
            "introduction": "A crucial theoretical property of EM is that it guarantees the likelihood of the observed data will not decrease at each iteration. However, this does not guarantee convergence to a local maximum; the algorithm can get stuck at other stationary points, like a saddle point. This advanced coding exercise  demonstrates this exact behavior, tasking you with implementing EM for a carefully chosen scenario where it converges to a saddle point. By numerically approximating the Hessian matrix at the fixed point, you will verify its nature, gaining deep insight into the convergence properties and potential pitfalls of the EM algorithm.",
            "id": "3119715",
            "problem": "You will construct and analyze an instance of the Expectation–Maximization algorithm (EM) that converges to a fixed point that is not a strict local maximum of the observed-data log-likelihood. Work in a one-dimensional, two-component Gaussian mixture model with known, common variance and equal mixture weights. You must start from fundamental definitions and implement the Expectation–Maximization algorithm in code to verify two properties: the monotonicity of the observed-data log-likelihood across iterations, and the local curvature of the observed-data log-likelihood at the EM fixed point.\n\nModel and notation:\n- Observations are real numbers denoted by $x_1,\\dots,x_N \\in \\mathbb{R}$.\n- There are two latent classes denoted by $z_n \\in \\{1,2\\}$ for $n \\in \\{1,\\dots,N\\}$.\n- The class-conditional densities are Gaussian with common known variance $\\sigma^2 > 0$ and unknown means $\\mu_1,\\mu_2 \\in \\mathbb{R}$.\n- The mixture weights are fixed and equal, that is $p(z_n=1)=p(z_n=2)=\\tfrac{1}{2}$.\n- The observed-data log-likelihood for parameters $(\\mu_1,\\mu_2)$ is\n$$\n\\ell(\\mu_1,\\mu_2) \\triangleq \\sum_{n=1}^N \\log\\!\\Big(\\tfrac{1}{2}\\,\\phi(x_n \\,|\\, \\mu_1,\\sigma^2) + \\tfrac{1}{2}\\,\\phi(x_n \\,|\\, \\mu_2,\\sigma^2)\\Big),\n$$\nwhere $\\phi(x\\,|\\,\\mu,\\sigma^2)$ is the Gaussian probability density function with mean $\\mu$ and variance $\\sigma^2$.\n\nRequirements:\n1. Starting only from the definitions of the mixture model, Bayes rule, and the expected complete-data log-likelihood, derive the Expectation–Maximization update rules for $(\\mu_1,\\mu_2)$ in this model with fixed equal weights and fixed variance. Implement these updates to produce a sequence $\\{(\\mu_1^{(t)},\\mu_2^{(t)})\\}_{t=0}^{T}$.\n2. Numerically verify the monotonicity of the observed-data log-likelihood sequence $\\{\\ell(\\mu_1^{(t)},\\mu_2^{(t)})\\}_{t=0}^{T}$; allow a small numerical tolerance when checking nondecrease.\n3. Compute a numerical approximation of the $2 \\times 2$ Hessian matrix of $\\ell(\\mu_1,\\mu_2)$ at the limit point $(\\mu_1^{(\\infty)},\\mu_2^{(\\infty)})$ of the EM sequence using central finite differences. Classify the limit point as a strict local maximum, strict local minimum, or a saddle point by examining the signs of the eigenvalues of the Hessian.\n4. Use this to provide a counterexample in which the EM algorithm converges to a fixed point that is a saddle point and not a strict local maximum, while the observed-data log-likelihood sequence is monotonic.\n\nTest suite:\nImplement your program to run EM on the following three cases. For each case, run until convergence or a maximum of $1000$ iterations, with stopping tolerance $\\varepsilon = 10^{-10}$ on the Euclidean change $\\sqrt{(\\Delta \\mu_1)^2 + (\\Delta \\mu_2)^2}$. Use a monotonicity tolerance of $\\tau = 10^{-12}$ when checking that $\\ell^{(t+1)} - \\ell^{(t)} \\ge -\\tau$ for all $t$.\n- Case 1 (counterexample target): data $x = [-3,-2,-1,1,2,3]$, variance $\\sigma^2 = 1.0$, initialization $(\\mu_1^{(0)},\\mu_2^{(0)})=(0.0,0.0)$.\n- Case 2 (asymmetric initialization): same data and variance as Case 1, initialization $(\\mu_1^{(0)},\\mu_2^{(0)})=(0.2,-0.2)$.\n- Case 3 (edge case with concentrated data): data $x = [0,0,0,0,0,0,0,0]$, variance $\\sigma^2 = 1.0$, initialization $(\\mu_1^{(0)},\\mu_2^{(0)})=(0.0,0.0)$.\n\nFor each case, your program must:\n- Run the Expectation–Maximization algorithm.\n- Check whether the observed-data log-likelihood sequence is monotonic nondecreasing within tolerance $\\tau$.\n- Approximate the Hessian of $\\ell$ at the EM limit point and determine if it has both positive and negative eigenvalues (saddle), both negative (strict local maximum), or both positive (strict local minimum).\n\nFinal output specification:\n- For each case, output a boolean defined as follows: output $\\,\\mathrm{True}\\,$ if and only if the EM sequence is monotonic nondecreasing within tolerance and the limit point is classified as a saddle point (that is, the Hessian has eigenvalues of opposite signs beyond a numerical threshold). Otherwise, output $\\,\\mathrm{False}\\,$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the three cases in order, for example $[r_1,r_2,r_3]$ where each $r_i$ is either $\\,\\mathrm{True}\\,$ or $\\,\\mathrm{False}\\,$.\n- No physical units or angle units are involved in this problem.\n\nYour implementation must be entirely self-contained and runnable without user input. The numerical classification of the Hessian should use a finite-difference step size that is small but numerically stable, and should treat eigenvalues with magnitude below a small threshold as effectively zero when determining the sign pattern.",
            "solution": "The user has provided a problem that requires the derivation and implementation of the Expectation–Maximization (EM) algorithm for a specific Gaussian Mixture Model (GMM). The goal is to demonstrate a known property of the EM algorithm: its ability to converge to a saddle point of the observed-data log-likelihood function, rather than a strict local maximum.\n\n### Step 1: Extract Givens\n- **Model**: A one-dimensional, two-component Gaussian mixture model.\n- **Observations**: $x_1, \\dots, x_N \\in \\mathbb{R}$.\n- **Latent Variables**: $z_n \\in \\{1,2\\}$ for each observation $x_n$.\n- **Mixture Weights**: Fixed and equal, $p(z_n=1) = p(z_n=2) = \\frac{1}{2}$.\n- **Class-Conditional Densities**: Gaussian, $\\phi(x \\,|\\, \\mu_k, \\sigma^2)$, with known common variance $\\sigma^2 > 0$ and unknown means $\\mu_1, \\mu_2$.\n- **Observed-Data Log-Likelihood**: $\\ell(\\mu_1,\\mu_2) \\triangleq \\sum_{n=1}^N \\log\\!\\Big(\\frac{1}{2}\\,\\phi(x_n \\,|\\, \\mu_1,\\sigma^2) + \\frac{1}{2}\\,\\phi(x_n \\,|\\, \\mu_2,\\sigma^2)\\Big)$.\n- **Task**:\n    1. Derive the EM update rules for $\\mu_1$ and $\\mu_2$.\n    2. Implement the EM algorithm.\n    3. Verify the monotonicity of the observed-data log-likelihood sequence, $\\{\\ell(\\mu_1^{(t)},\\mu_2^{(t)})\\}_{t=0}^{T}$.\n    4. Compute the Hessian of $\\ell(\\mu_1,\\mu_2)$ at the limit point $(\\mu_1^{(\\infty)},\\mu_2^{(\\infty)})$ via central finite differences and classify the point (strict local maximum, strict local minimum, or saddle point) by its eigenvalues.\n    5. Find a case where EM converges to a saddle point.\n- **Test Cases**:\n    - **Case 1**: Data $x = [-3,-2,-1,1,2,3]$, variance $\\sigma^2 = 1.0$, initialization $(\\mu_1^{(0)},\\mu_2^{(0)})=(0.0,0.0)$.\n    - **Case 2**: Data $x = [-3,-2,-1,1,2,3]$, variance $\\sigma^2 = 1.0$, initialization $(\\mu_1^{(0)},\\mu_2^{(0)})=(0.2,-0.2)$.\n    - **Case 3**: Data $x = [0,0,0,0,0,0,0,0]$, variance $\\sigma^2 = 1.0$, initialization $(\\mu_1^{(0)},\\mu_2^{(0)})=(0.0,0.0)$.\n- **Numerical Parameters**:\n    - Max iterations: $1000$.\n    - Convergence tolerance: $\\varepsilon = 10^{-10}$ (Euclidean change in $(\\mu_1, \\mu_2)$).\n    - Monotonicity tolerance: $\\tau = 10^{-12}$.\n- **Final Output**: For each case, a boolean value indicating if the log-likelihood sequence is monotonic nondecreasing and the limit point is a saddle point. The results are to be in a list, e.g., `[r_1,r_2,r_3]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It deals with standard concepts from statistical learning, specifically the Expectation-Maximization algorithm for Gaussian Mixture Models. The premise that EM can converge to a saddle point is a well-documented aspect of the algorithm's behavior, making the investigation valid and non-trivial. The problem is well-posed, providing all necessary data, model specifications, initial conditions, and convergence criteria. The language is precise and objective. There are no contradictions, ambiguities, or reliance on pseudoscience. The problem is a formal exercise in mathematical statistics and numerical implementation.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation and Methodology\n\n#### Derivation of the Expectation–Maximization Update Rules\nThe EM algorithm aims to find maximum likelihood estimates for parameters when the model involves latent variables. It iteratively applies two steps: an Expectation (E) step and a Maximization (M) step.\n\nLet the parameters at iteration $t$ be $(\\mu_1^{(t)}, \\mu_2^{(t)})$. The complete-data log-likelihood, assuming we know the latent class assignments $z_n \\in \\{1,2\\}$ for each observation $x_n$, is:\n$$ \\ell_c(\\mu_1, \\mu_2; X, Z) = \\sum_{n=1}^N \\log p(x_n, z_n | \\mu_1, \\mu_2) $$\nWe can write this as $\\sum_{n=1}^N \\sum_{k=1}^2 \\mathbb{I}(z_n=k) \\log [p(x_n | z_n=k, \\mu_k) p(z_n=k)]$. Since the mixture weights $p(z_n=k)=\\frac{1}{2}$ are fixed, maximizing with respect to $\\mu_k$ is equivalent to maximizing:\n$$ \\sum_{n=1}^N \\sum_{k=1}^2 \\mathbb{I}(z_n=k) \\log \\phi(x_n | \\mu_k, \\sigma^2) $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function.\n\n**E-Step**: We compute the expectation of the complete-data log-likelihood with respect to the conditional distribution of the latent variables $Z$ given the observed data $X$ and the current parameter estimates $(\\mu_1^{(t)}, \\mu_2^{(t)})$. This involves calculating the posterior probability that observation $x_n$ belongs to class $k$, often called the responsibility, denoted $\\gamma_{nk}^{(t)}$.\n$$ \\gamma_{nk}^{(t)} \\triangleq E[\\mathbb{I}(z_n=k) | X, \\mu_1^{(t)}, \\mu_2^{(t)}] = p(z_n=k | x_n, \\mu_1^{(t)}, \\mu_2^{(t)}) $$\nUsing Bayes' rule:\n$$ \\gamma_{nk}^{(t)} = \\frac{p(x_n | z_n=k, \\mu_k^{(t)}) p(z_n=k)}{\\sum_{j=1}^2 p(x_n | z_n=j, \\mu_j^{(t)}) p(z_n=j)} = \\frac{\\frac{1}{2} \\phi(x_n | \\mu_k^{(t)}, \\sigma^2)}{\\frac{1}{2} \\phi(x_n | \\mu_1^{(t)}, \\sigma^2) + \\frac{1}{2} \\phi(x_n | \\mu_2^{(t)}, \\sigma^2)} $$\nFor $k=1$, this simplifies to:\n$$ \\gamma_{n1}^{(t)} = \\frac{\\phi(x_n | \\mu_1^{(t)}, \\sigma^2)}{\\phi(x_n | \\mu_1^{(t)}, \\sigma^2) + \\phi(x_n | \\mu_2^{(t)}, \\sigma^2)} $$\nAnd since there are only two components, $\\gamma_{n2}^{(t)} = 1 - \\gamma_{n1}^{(t)}$.\n\n**M-Step**: We maximize the expected complete-data log-likelihood (the $Q$-function) with respect to the parameters $(\\mu_1, \\mu_2)$ to find the updated estimates $(\\mu_1^{(t+1)}, \\mu_2^{(t+1)})$.\n$$ Q(\\mu_1, \\mu_2 | \\mu_1^{(t)}, \\mu_2^{(t)}) = \\sum_{n=1}^N \\sum_{k=1}^2 \\gamma_{nk}^{(t)} \\log \\phi(x_n | \\mu_k, \\sigma^2) $$\nThe term $\\log \\phi(x_n | \\mu_k, \\sigma^2) = -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x_n - \\mu_k)^2}{2\\sigma^2}$. To maximize $Q$, we take the partial derivative with respect to each $\\mu_k$ and set it to zero.\n$$ \\frac{\\partial Q}{\\partial \\mu_k} = \\sum_{n=1}^N \\gamma_{nk}^{(t)} \\frac{\\partial}{\\partial \\mu_k} \\left( - \\frac{(x_n - \\mu_k)^2}{2\\sigma^2} \\right) = \\sum_{n=1}^N \\gamma_{nk}^{(t)} \\frac{x_n - \\mu_k}{\\sigma^2} = 0 $$\nSolving for $\\mu_k$:\n$$ \\mu_k \\sum_{n=1}^N \\gamma_{nk}^{(t)} = \\sum_{n=1}^N \\gamma_{nk}^{(t)} x_n \\implies \\mu_k^{(t+1)} = \\frac{\\sum_{n=1}^N \\gamma_{nk}^{(t)} x_n}{\\sum_{n=1}^N \\gamma_{nk}^{(t)}} $$\nThis defines the update rule for the means. The algorithm iterates between the E-step and M-step until convergence.\n\n#### Verification Methodology\n1.  **Monotonicity of Log-Likelihood**: A key property of the EM algorithm is that it guarantees the observed-data log-likelihood sequence is non-decreasing, i.e., $\\ell(\\mu_1^{(t+1)}, \\mu_2^{(t+1)}) \\ge \\ell(\\mu_1^{(t)}, \\mu_2^{(t)})$. We will verify this numerically by computing $\\ell$ at each iteration and checking that $\\ell^{(t+1)} - \\ell^{(t)} \\ge -\\tau$ for a small numerical tolerance $\\tau = 10^{-12}$.\n\n2.  **Hessian Calculation and Limit Point Classification**: Upon convergence to a fixed point $(\\mu_1^*, \\mu_2^*)$, we analyze the local curvature of the log-likelihood surface by computing its Hessian matrix, $H$.\n    $$ H = \\begin{pmatrix} \\frac{\\partial^2 \\ell}{\\partial \\mu_1^2} & \\frac{\\partial^2 \\ell}{\\partial \\mu_1 \\partial \\mu_2} \\\\ \\frac{\\partial^2 \\ell}{\\partial \\mu_2 \\partial \\mu_1} & \\frac{\\partial^2 \\ell}{\\partial \\mu_2^2} \\end{pmatrix} $$\n    We will approximate the second partial derivatives numerically using the central finite difference method with a small step size $h$:\n    - $H_{11} \\approx \\frac{\\ell(\\mu_1^*+h, \\mu_2^*) - 2\\ell(\\mu_1^*, \\mu_2^*) + \\ell(\\mu_1^*-h, \\mu_2^*)}{h^2}$\n    - $H_{22} \\approx \\frac{\\ell(\\mu_1^*, \\mu_2^*+h) - 2\\ell(\\mu_1^*, \\mu_2^*) + \\ell(\\mu_1^*, \\mu_2^*-h)}{h^2}$\n    - $H_{12} = H_{21} \\approx \\frac{\\ell(\\mu_1^*+h, \\mu_2^*+h) - \\ell(\\mu_1^*+h, \\mu_2^*-h) - \\ell(\\mu_1^*-h, \\mu_2^*+h) + \\ell(\\mu_1^*-h, \\mu_2^*-h)}{4h^2}$\n\n    The nature of the fixed point is determined by the signs of the eigenvalues of $H$:\n    - **Strict Local Maximum**: Both eigenvalues are negative.\n    - **Strict Local Minimum**: Both eigenvalues are positive.\n    - **Saddle Point**: The eigenvalues have opposite signs (one positive, one negative).\n\n#### Analysis of Test Cases\n- **Case 1**: The data $x = [-3,-2,-1,1,2,3]$ is symmetric around $0$, and the initial means are $(\\mu_1^{(0)}, \\mu_2^{(0)}) = (0,0)$. Due to this symmetry, the responsibilities for each point will be exactly $\\gamma_{n1} = \\gamma_{n2} = 1/2$. The updated means will be the overall sample mean, which is $\\bar{x}=0$. Thus, the parameters remain at $(0,0)$, which is a fixed point. A theoretical calculation of the Hessian at this point reveals one positive and one negative eigenvalue, identifying it as a saddle point. The log-likelihood remains constant, so the monotonicity condition is met. This case is designed to be the required counterexample.\n\n- **Case 2**: The same symmetric data is used, but the initialization $(\\mu_1^{(0)}, \\mu_2^{(0)}) = (0.2, -0.2)$ breaks the symmetry. The EM algorithm will iterate away from the saddle point at $(0,0)$ and converge to one of the two symmetric local maxima, which are located near $(\\mu, -\\mu)$ for some $\\mu>0$. At these maxima, the Hessian is negative definite. The log-likelihood will be strictly increasing until convergence. Thus, the limit point will not be a saddle point.\n\n- **Case 3**: The data consists of eight points at $x=0$, and the initialization is $(0,0)$. As in Case 1, the algorithm will not move from $(0,0)$. However, with all data concentrated at the mean, the point $(\\mu_1, \\mu_2) = (0,0)$ represents a single Gaussian perfectly fitting the data. This is the global maximum of the likelihood surface. The Hessian at this point will be negative definite. Therefore, the limit point is a local maximum, not a saddle point.\n\nThe implementation will execute these steps for each case and produce the specified boolean output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem by running EM on three test cases and analyzing the results.\n    \"\"\"\n    test_cases = [\n        {'data': np.array([-3.0, -2.0, -1.0, 1.0, 2.0, 3.0]), 'variance': 1.0, 'init_means': (0.0, 0.0)},\n        {'data': np.array([-3.0, -2.0, -1.0, 1.0, 2.0, 3.0]), 'variance': 1.0, 'init_means': (0.2, -0.2)},\n        {'data': np.array([0.0] * 8), 'variance': 1.0, 'init_means': (0.0, 0.0)}\n    ]\n    \n    # Numerical parameters from the problem statement\n    max_iter = 1000\n    conv_tol = 1e-10\n    mono_tol = 1e-12\n    fd_step = 1e-5\n    eig_thresh = 1e-8\n\n    results = []\n    for case in test_cases:\n        x = case['data']\n        var = case['variance']\n        mu1_init, mu2_init = case['init_means']\n\n        # Run the EM algorithm\n        final_means, log_likelihoods = run_em(x, var, mu1_init, mu2_init, max_iter, conv_tol)\n        \n        # Check for monotonicity\n        is_monotonic = check_monotonicity(log_likelihoods, mono_tol)\n        \n        # Classify the limit point\n        point_type = classify_limit_point(final_means, x, var, fd_step, eig_thresh)\n        is_saddle = (point_type == 'saddle')\n        \n        # The final result for the case is True if both conditions are met\n        results.append(is_monotonic and is_saddle)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(lambda b: 'True' if b else 'False', results))}]\")\n\n\ndef gaussian_pdf(x, mean, variance):\n    \"\"\"\n    Computes the probability density function of a Gaussian distribution.\n    This is equivalent to scipy.stats.norm.pdf(x, loc=mean, scale=sqrt(variance)).\n    \"\"\"\n    return norm.pdf(x, loc=mean, scale=np.sqrt(variance))\n\ndef observed_log_likelihood(params, x, variance):\n    \"\"\"\n    Computes the observed-data log-likelihood for the GMM.\n    \"\"\"\n    mu1, mu2 = params\n    \n    # Mixture weights are 0.5 each\n    likelihoods = 0.5 * gaussian_pdf(x, mu1, variance) + 0.5 * gaussian_pdf(x, mu2, variance)\n    \n    # To prevent log(0), add a small epsilon where likelihoods are zero\n    # This scenario is unlikely with Gaussian components unless variance is zero.\n    likelihoods[likelihoods == 0] = np.finfo(float).eps\n    \n    return np.sum(np.log(likelihoods))\n\ndef run_em(x, variance, mu1_init, mu2_init, max_iter, conv_tol):\n    \"\"\"\n    Runs the Expectation-Maximization algorithm for the 2-component GMM.\n    \"\"\"\n    mu1, mu2 = mu1_init, mu2_init\n    log_likelihoods = [observed_log_likelihood((mu1, mu2), x, variance)]\n\n    for i in range(max_iter):\n        # E-step: Compute responsibilities\n        pdf1 = gaussian_pdf(x, mu1, variance)\n        pdf2 = gaussian_pdf(x, mu2, variance)\n        \n        denominator = pdf1 + pdf2\n        # Handle cases where both components have zero probability for a data point\n        denominator[denominator == 0] = 1.0  # Avoid division by zero; resp will be 0 anyway.\n        \n        gamma1 = pdf1 / denominator\n        gamma2 = 1.0 - gamma1\n\n        # M-step: Update means\n        gamma1_sum = np.sum(gamma1)\n        gamma2_sum = np.sum(gamma2)\n\n        mu1_new = np.sum(gamma1 * x) / gamma1_sum if gamma1_sum > 0 else mu1\n        mu2_new = np.sum(gamma2 * x) / gamma2_sum if gamma2_sum > 0 else mu2\n        \n        # Store new log-likelihood\n        log_likelihoods.append(observed_log_likelihood((mu1_new, mu2_new), x, variance))\n\n        # Check for convergence\n        change = np.sqrt((mu1_new - mu1)**2 + (mu2_new - mu2)**2)\n        if change  conv_tol:\n            mu1, mu2 = mu1_new, mu2_new\n            break\n            \n        mu1, mu2 = mu1_new, mu2_new\n    \n    return (mu1, mu2), log_likelihoods\n\ndef check_monotonicity(log_likelihoods, tolerance):\n    \"\"\"\n    Checks if the log-likelihood sequence is non-decreasing within a tolerance.\n    \"\"\"\n    for i in range(len(log_likelihoods) - 1):\n        if log_likelihoods[i+1]  log_likelihoods[i] - tolerance:\n            return False\n    return True\n\ndef classify_limit_point(final_means, x, variance, h, eig_thresh):\n    \"\"\"\n    Approximates the Hessian and classifies the point based on its eigenvalues.\n    \"\"\"\n    mu1_star, mu2_star = final_means\n    \n    # Central point\n    ell_0 = observed_log_likelihood(final_means, x, variance)\n    \n    # Hessian element H_11\n    ell_p1 = observed_log_likelihood((mu1_star + h, mu2_star), x, variance)\n    ell_m1 = observed_log_likelihood((mu1_star - h, mu2_star), x, variance)\n    h11 = (ell_p1 - 2 * ell_0 + ell_m1) / (h**2)\n    \n    # Hessian element H_22\n    ell_p2 = observed_log_likelihood((mu1_star, mu2_star + h), x, variance)\n    ell_m2 = observed_log_likelihood((mu1_star, mu2_star - h), x, variance)\n    h22 = (ell_p2 - 2 * ell_0 + ell_m2) / (h**2)\n    \n    # Hessian element H_12\n    ell_pp = observed_log_likelihood((mu1_star + h, mu2_star + h), x, variance)\n    ell_pm = observed_log_likelihood((mu1_star + h, mu2_star - h), x, variance)\n    ell_mp = observed_log_likelihood((mu1_star - h, mu2_star + h), x, variance)\n    ell_mm = observed_log_likelihood((mu1_star - h, mu2_star - h), x, variance)\n    h12 = (ell_pp - ell_pm - ell_mp + ell_mm) / (4 * h**2)\n    \n    hessian = np.array([[h11, h12], [h12, h22]])\n    \n    # Compute eigenvalues. Use eigvalsh for symmetric matrices.\n    eigenvalues = np.linalg.eigvalsh(hessian)\n    \n    # Classify the point\n    n_pos = np.sum(eigenvalues > eig_thresh)\n    n_neg = np.sum(eigenvalues  -eig_thresh)\n\n    if n_pos == 0 and n_neg == 2:\n        return 'local_max'\n    elif n_pos == 2 and n_neg == 0:\n        return 'local_min'\n    elif n_pos == 1 and n_neg == 1:\n        return 'saddle'\n    else:\n        return 'degenerate'\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}