{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握一种算法，最好的方法就是亲手实现它。本练习将指导您从零开始构建局部线性嵌入（LLE）算法，并通过实践揭示其一个基本特性：对数据缩放的敏感性。通过这个练习，您将探索 LLE 如何对各向异性的特征缩放做出反应，并理解为何像特征标准化这样的预处理步骤在实践中至关重要。",
            "id": "3141684",
            "problem": "实现一个完整的程序，从第一性原理出发，为给定的点云构建局部线性嵌入 (LLE)，并用它来定量比较不同输入缩放下的嵌入结果。您的程序不得依赖任何外部机器学习库。它必须仅使用线性代数、欧几里得几何和特征分解。请遵循以下纯数学术语进行操作。\n\n用作基本基础的定义：\n- 给定一个点云，包含 $N$ 个样本，每个样本在 $D$ 维空间中，排列成一个矩阵 $X \\in \\mathbb{R}^{N \\times D}$。一个样本的 $k$ 个最近邻是那些欧几里得距离最小的样本（距离相等时可通过索引顺序解决）。对于任意向量 $x,y \\in \\mathbb{R}^{D}$，欧几里得距离为 $\\lVert x - y \\rVert_{2}$。\n- 对于每个样本 $x_i \\in \\mathbb{R}^{D}$ 及其邻域索引 $\\mathcal{N}(i)$，定义局部最小二乘重构问题，以找到重构权重 $w_{ij}$，使得在权重和为一的约束下，平方误差最小化。使用由邻域差异构建的局部数据矩阵来表述此问题。\n- 从局部重构中汇编一个全局代价函数，并通过求解一个由权重导出的特征分解问题，来恢复一个低维嵌入 $Y \\in \\mathbb{R}^{N \\times d}$。使用与最小特征值相关联的非平凡特征向量（排除与平凡常数模式相关联的那个）。\n- 比较两个仅在刚性变换和均匀缩放意义下定义的嵌入的标准工具是正交 Procrustes 分析。为比较 $Y_1, Y_2 \\in \\mathbb{R}^{N \\times d}$，将每个嵌入中心化至零均值，将每个嵌入归一化至单位弗罗贝尼乌斯范数，通过奇异值分解 (SVD) 计算最优正交对齐，并报告对齐后残差平方和作为差异度。这将产生一个非负实数；值越小表示越接近。\n\n待探索的不变性概念：\n- 均匀全局缩放：将 $X$ 替换为 $s X$（其中 $s \\in \\mathbb{R}_{+}$ 是一个标量），会将所有成对距离乘以相同的因子。研究这如何影响邻域集、局部重构和最终的嵌入。\n- 各向异性特征缩放：将 $X$ 替换为 $X A$（其中 $A \\in \\mathbb{R}^{D \\times D}$ 是一个对角线上具有不等正项的对角矩阵），会对每个特征进行不同尺度的缩放。研究这如何影响邻域集、局部重构和最终的嵌入。\n- 逐特征标准化：通过使用在 $N$ 个样本上逐特征计算的经验均值和方差，将每个特征标准化为零均值和单位方差，从而将 $X$ 转换为 $\\tilde{X}$。\n\n您的实现的算法要求：\n- 在完整数据集上使用精确的欧几里得距离实现 $k$-最近邻。\n- 对每个点 $x_i$，构建并求解从其 $k$ 个邻居重构 $x_i$ 的约束最小二乘问题，权重需满足和为一的约束。为确保局部格拉姆矩阵病态时的数值稳定性，添加一个与其迹乘以一个微小标量 $\\epsilon$ 成正比的吉洪诺夫 (Tikhonov) 正则化项。\n- 从权重中汇编全局矩阵，并将维度为 $d$ 的嵌入计算为由重构算子产生的对称半正定矩阵的 $d$ 个最小非平凡特征值所对应的特征向量。\n- 使用线性代数和奇异值分解实现上述 Procrustes 对齐。\n\n使用以下固定的数据和参数作为测试套件。所有角度都是无量纲的；没有物理单位。所有数值必须被视为精确的标量。\n\n所有测试的共享参数：\n- 邻居数 $k = 8$。\n- 目标嵌入维度 $d = 2$。\n- 正则化参数 $\\epsilon = 10^{-3}$，通过将单位矩阵乘以局部格拉姆矩阵的迹与 $\\epsilon$ 的乘积来使用。\n- 每个布尔测试的 Procrustes 差异度阈值如下所示。\n\n数据集：\n1) 三维空间中的二维弯曲流形。\n- 构建一个 $m = 11$ 的规则网格，使得 $u$ 和 $v$ 各自取遍 $[0,1]$ 区间内的 $m$ 个等间距值，即 $\\{0, \\frac{1}{m-1}, \\ldots, 1\\}$。将所有 $N = m^2$ 个点对 $(u,v)$ 按字典序堆叠成一个矩阵 $U \\in \\mathbb{R}^{N \\times 2}$。\n- 通过 $x = \\begin{bmatrix} u & v & u^2 + v^2 \\end{bmatrix}$ 将每个 $(u,v)$ 映射到 $\\mathbb{R}^{3}$ 中的一个点，形成 $X_{\\text{base}} \\in \\mathbb{R}^{N \\times 3}$。\n- 定义三个变换后的版本：\n  a) 全局缩放：$X_{\\text{glob}} = s X_{\\text{base}}$，其中 $s = 7$。\n  b) 强各向异性特征缩放：$X_{\\text{aniso}} = X_{\\text{base}} A$，其中 $A = \\mathrm{diag}(100, 0.01, 50)$。\n  c) 逐特征标准化：$\\mathrm{Std}(X)$ 表示逐特征标准化为零均值和单位方差。计算 $X_{\\text{std-base}} = \\mathrm{Std}(X_{\\text{base}})$ 和 $X_{\\text{std-aniso}} = \\mathrm{Std}(X_{\\text{aniso}})$。\n\n2) 三维空间中近似平衡的线性流形。\n- 使用与上面相同的 $U$，通过 $x = \\begin{bmatrix} u & v & \\frac{u+v}{2} \\end{bmatrix}$ 定义 $X_{\\text{bal}} \\in \\mathbb{R}^{N \\times 3}$ 及其标准化版本 $X_{\\text{std-bal}} = \\mathrm{Std}(X_{\\text{bal}})$。\n\n对每个数据集 $X$，计算 LLE 嵌入 $Y \\in \\mathbb{R}^{N \\times d}$。\n\n待计算的定量比较：\n- 令 $\\Delta(\\cdot,\\cdot)$ 表示相同数据集大小的两个嵌入之间的 Procrustes 差异度。\n- 计算以下五个布尔测试结果：\n  1) $b_1$：在弯曲流形上对均匀全局缩放的不变性。要求 $b_1 = [\\Delta(Y_{\\text{base}}, Y_{\\text{glob}})  10^{-6}]$。\n  2) $b_2$：在弯曲流形上对各向异性特征缩放的敏感性。要求 $b_2 = [\\Delta(Y_{\\text{base}}, Y_{\\text{aniso}})  2 \\times 10^{-2}]$。\n  3) $b_3$：在弯曲流形上，标准化可在小残差范围内抵消各向异性。要求 $b_3 = [\\Delta(Y_{\\text{std-base}}, Y_{\\text{std-aniso}})  2 \\times 10^{-2}]$。\n  4) $b_4$：对于弯曲流形，与强各向异性缩放的输入相比，对原始输入进行标准化能改善其与原始嵌入的接近程度。要求 $b_4 = [\\Delta(Y_{\\text{base}}, Y_{\\text{std-base}})  \\Delta(Y_{\\text{base}}, Y_{\\text{aniso}})]$。\n  5) $b_5$：在近似平衡的线性流形上，标准化不会显著改变嵌入。要求 $b_5 = [\\Delta(Y_{\\text{bal}}, Y_{\\text{std-bal}})  5 \\times 10^{-3}]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含五个布尔结果，按 $[b_1, b_2, b_3, b_4, b_5]$ 的顺序以逗号分隔的列表形式，并用方括号括起来。例如，一个有效的输出行应类似于 $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{True}]$，但包含的是您程序计算出的实际布尔值。\n\n注意事项：\n- 所有计算必须是确定性的。不要使用随机性。\n- 问题中不涉及角度。没有物理单位。",
            "solution": "用户要求从第一性原理出发实现局部线性嵌入 (LLE) 算法，以研究其对不同特征缩放变换的敏感性。该实现必须遵守一组特定的算法和参数定义。\n\n该过程分为三个主要部分：LLE 算法本身，一种比较嵌入几何形状的方法 (Procrustes 分析)，以及一系列针对生成数据集的定量测试。\n\n**1. 局部线性嵌入 (LLE) 算法**\n\nLLE 算法通过保留局部线性重构属性，为高维数据集构建一个低维嵌入。对于给定的数据集 $X \\in \\mathbb{R}^{N \\times D}$，该算法分三步进行。\n\n**第 1 步：邻域识别**\n对于每个数据点 $x_i$，我们识别其 $k$ 个最近邻。邻域 $\\mathcal{N}(i)$ 由 $k$ 个具有最小欧几里得距离 $\\lVert x_i - x_j \\rVert_2$ 的点 $x_j$ ($j \\neq i$) 组成。为确保确定性结果，任何距离相等的情况都通过优先选择索引 $j$ 较小的点来解决。\n\n**第 2 步：局部重构权重计算**\nLLE 的核心假设是每个点 $x_i$ 都可以由其邻居的线性组合很好地近似。我们寻求一组重构权重 $W_{ij}$，以最小化总重构误差，同时满足两个约束条件：如果 $x_j$ 不是 $x_i$ 的邻居，则 $W_{ij} = 0$；并且每个点的权重总和必须为 1，即 $\\sum_{j \\in \\mathcal{N}(i)} W_{ij} = 1$。这导致了针对每个点 $x_i$ 的约束最小二乘问题：\n$$ \\min_{W} \\left\\| x_i - \\sum_{j \\in \\mathcal{N}(i)} W_{ij} x_j \\right\\|^2 \\quad \\text{subject to} \\quad \\sum_{j \\in \\mathcal{N}(i)} W_{ij} = 1 $$\n该问题对输入数据的旋转、平移和均匀缩放具有不变性。通过代入约束条件，目标可以重写为找到点 $x_i$ 的权重 $w \\in \\mathbb{R}^k$ 以最小化：\n$$ \\left\\| \\sum_{j=1}^k w_j (x_i - x_{\\eta_j}) \\right\\|^2 = w^T G_i w $$\n其中 $\\{x_{\\eta_j}\\}_{j=1}^k$ 是 $x_i$ 的邻居，而 $G_i$ 是局部格拉姆矩阵。$G_i$ 的元素为 $(G_i)_{jk} = (x_i-x_{\\eta_j})^T(x_i-x_{\\eta_k})$。最优权重 $w$ 的解可以通过求解线性系统 $G_i z = \\mathbf{1}$（其中 $\\mathbf{1}$ 是全一向量）得到，然后进行归一化 $w = z / \\sum z_l$。\n\n为了处理病态的格拉姆矩阵，添加了一个吉洪诺夫 (Tikhonov) 正则化项。问题指定添加一个与格拉姆矩阵的迹成比例的项，这使得正则化本身具有尺度不变性。修改后的格拉姆矩阵 $G'_i$ 是：\n$$ G'_i = G_i + \\epsilon \\cdot \\mathrm{tr}(G_i) \\cdot I_k $$\n其中 $I_k$ 是 $k \\times k$ 的单位矩阵，$\\epsilon = 10^{-3}$ 是一个微小标量。然后我们求解 $G'_i z = \\mathbf{1}$ 以得到 $z$ 来找到权重。\n\n**第 3 步：全局嵌入计算**\n在计算出 $N \\times N$ 的权重矩阵 $W$（其中对于非邻居 $W_{ij}=0$）之后，我们寻求一个能够保留这些局部重构权重的低维嵌入 $Y \\in \\mathbb{R}^{N \\times d}$。这通过最小化嵌入代价函数来实现：\n$$ \\Phi(Y) = \\sum_{i=1}^N \\left\\| y_i - \\sum_{j=1}^N W_{ij} y_j \\right\\|^2 $$\n这种二次型可以表示为 $\\Phi(Y) = \\mathrm{Tr}(Y^T M Y)$，其中 $M = (I-W)^T(I-W)$。矩阵 $M$ 是对称半正定的。为了在最小化 $\\Phi(Y)$ 的同时避免平凡的坍缩解（$Y=0$），我们对 $Y$ 施加中心化和单位协方差约束。解由 $M$ 的最小特征值对应的特征向量给出。$M$ 的最小特征值总是零（或数值上接近零），其对应的特征向量是全一向量。这代表了一个平凡的常数嵌入，应被舍弃。所需的 $d$ 维嵌入 $Y$ 由与接下来 $d$ 个最小特征值（即第 2 到第 $d+1$ 个特征值）相关联的特征向量构成。\n\n**2. 用于嵌入比较的正交 Procrustes 分析**\n\n为了定量比较两个仅在刚性变换（旋转、反射）和均匀缩放意义下定义的嵌入 $Y_1, Y_2 \\in \\mathbb{R}^{N \\times d}$，我们使用正交 Procrustes 分析。差异度 $\\Delta(Y_1, Y_2)$ 是最优对齐后的最小平方差之和。其步骤如下：\n1.  **中心化**：平移 $Y_1$ 和 $Y_2$，使其质心位于原点。\n2.  **归一化**：缩放两个中心化后的矩阵，使其弗罗贝尼乌斯范数均为单位一，即 $\\|Y\\|_F = \\sqrt{\\sum_{i,j} Y_{ij}^2} = 1$。\n3.  **对齐**：使用矩阵 $C = Y_1^T Y_2$ 的奇异值分解 (SVD) 来找到能最小化 $\\|Y_1 R - Y_2\\|_F^2$ 的最优正交矩阵 $R$。如果 $C = U \\Sigma V^T$，则最优旋转是 $R=UV^T$。\n4.  **计算差异度**：最小残差平方和，即 Procrustes 差异度，可以高效地计算为 $\\Delta = \\|Y_1\\|_F^2 + \\|Y_2\\|_F^2 - 2 \\sum_i \\sigma_i$，其中 $\\sigma_i$ 是 $C$ 的奇异值。归一化后，这简化为 $\\Delta = 2(1 - \\sum_i \\sigma_i)$。较小的差异度表示嵌入之间具有更高的相似性。\n\n**3. 数据集和定量测试**\n\n该分析在两个基础数据集的几种变换上进行，使用的参数为 $k=8$，$d=2$ 和 $\\epsilon=10^{-3}$。这些测试旨在探究 LLE 的已知属性：\n- $b_1$：测试对均匀全局缩放的不变性。理论上 LLE 是不变的，所以这应该为真。\n- $b_2$：测试对各向异性特征缩放的敏感性。这是 LLE 的一个已知弱点，所以差异度应该很大。\n- $b_3$：测试逐特征标准化是否能抵消各向异性的影响。对 $X_{\\text{base}}$ 和 $X_{\\text{aniso}}$ 的特征进行标准化会得到相同的数据集，因此它们的嵌入应该是相同的，差异度接近于零。\n- $b_4$：比较标准化与强各向异性缩放的效果。标准化预计是一种干扰性小得多的变换。\n- $b_5$：检验标准化在近似平衡的线性流形上的效果。预计变化会很小，导致较小的差异度。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh, svd\n\ndef solve():\n    \"\"\"\n    Implements Locally Linear Embedding (LLE) from first principles and uses it to\n    compare embeddings under different input scalings.\n    \"\"\"\n\n    # -----------------------------------------------------\n    # Algorithm Implementation based on Problem Description\n    # -----------------------------------------------------\n\n    def standardize(X):\n        \"\"\"Standardizes each feature of X to have zero mean and unit variance.\"\"\"\n        mean = np.mean(X, axis=0)\n        std = np.std(X, axis=0)\n        # Avoid division by zero for constant features\n        std[np.isclose(std, 0)] = 1.0\n        return (X - mean) / std\n\n    def find_neighbors(X, k):\n        \"\"\"Finds the k-nearest neighbors for each point in X.\"\"\"\n        N = X.shape[0]\n        # Calculate squared Euclidean distances to avoid sqrt\n        sum_X_sq = np.sum(X**2, axis=1)\n        dist_sq_matrix = np.add.outer(sum_X_sq, sum_X_sq) - 2 * (X @ X.T)\n        dist_sq_matrix[dist_sq_matrix  0] = 0.0 # for numerical stability\n        \n        # Argsort on distances to find nearest neighbors\n        # kind='stable' ensures ties are broken by index order as required\n        neighbor_indices = np.argsort(dist_sq_matrix, axis=1, kind='stable')\n        \n        # Return indices of k-nearest neighbors.\n        # Index 0 is the point itself, so we take from 1 to k+1.\n        return neighbor_indices[:, 1:k+1]\n\n    def compute_weights(X, neighbor_indices, k, epsilon):\n        \"\"\"Computes the LLE reconstruction weights for each point.\"\"\"\n        N = X.shape[0]\n        W = np.zeros((N, N))\n        \n        for i in range(N):\n            neighbors = neighbor_indices[i]\n            \n            # Local data matrix of neighbor differences\n            C_i = X[i] - X[neighbors]  # Uses broadcasting\n            \n            # Local Gram matrix\n            G_i = C_i @ C_i.T\n            \n            # Tikhonov regularization as specified\n            trace_G = np.trace(G_i)\n            reg_val = epsilon * trace_G\n            G_i += reg_val * np.identity(k)\n            \n            # Solve for weights by solving G_i * z = 1\n            ones = np.ones(k)\n            try:\n                z = np.linalg.solve(G_i, ones)\n            except np.linalg.LinAlgError:\n                # Fallback to pseudo-inverse if singular despite regularization\n                z = np.linalg.pinv(G_i) @ ones\n\n            # Normalize weights to sum to 1\n            w_i = z / np.sum(z)\n            \n            # Populate sparse weight matrix W\n            W[i, neighbors] = w_i\n            \n        return W\n\n    def compute_embedding(W, N, d):\n        \"\"\"Computes the low-dimensional embedding from the weight matrix.\"\"\"\n        # Construct the cost matrix M = (I-W)^T(I-W)\n        I = np.identity(N)\n        M = (I - W).T @ (I - W)\n        \n        # Find eigenvectors for the d smallest non-trivial eigenvalues.\n        # eigh returns eigenvalues in ascending order.\n        # We need eigenvectors for indices 1 up to and including d.\n        # This corresponds to the 2nd to (d+1)-th smallest eigenvalues.\n        _, eigvecs = eigh(M, subset_by_index=[1, d])\n        \n        return eigvecs\n\n    def procrustes_disparity(Y1, Y2):\n        \"\"\"Computes the orthogonal Procrustes disparity between two embeddings.\"\"\"\n        # 1. Center the embeddings to have zero mean.\n        Y1_c = Y1 - np.mean(Y1, axis=0)\n        Y2_c = Y2 - np.mean(Y2, axis=0)\n\n        # 2. Normalize to unit Frobenius norm.\n        norm1 = np.linalg.norm(Y1_c, 'fro')\n        norm2 = np.linalg.norm(Y2_c, 'fro')\n        Y1_norm = Y1_c / norm1 if norm1 > 1e-10 else Y1_c\n        Y2_norm = Y2_c / norm2 if norm2 > 1e-10 else Y2_c\n\n        # 3. Compute optimal alignment via SVD of the cross-covariance matrix.\n        M = Y1_norm.T @ Y2_norm\n        _, s, _ = svd(M)\n        \n        # 4. Compute disparity using the singular values.\n        # Disparity = ||Y1_norm||^2 + ||Y2_norm||^2 - 2 * tr(Sigma)\n        #           = 1 + 1 - 2 * sum(s) = 2 * (1 - sum(s))\n        disparity = 2.0 - 2.0 * np.sum(s)\n        return disparity\n\n    def run_lle(X, k, d, epsilon):\n        \"\"\"A wrapper function to run the full LLE pipeline.\"\"\"\n        neighbor_indices = find_neighbors(X, k)\n        W = compute_weights(X, neighbor_indices, k, epsilon)\n        N = X.shape[0]\n        Y = compute_embedding(W, N, d)\n        return Y\n\n    # ------------------\n    # Main Logic\n    # ------------------\n    \n    # Shared parameters from the problem description\n    k = 8\n    d = 2\n    epsilon = 1e-3\n    m = 11\n\n    # 1. Generate Datasets\n    u_vals = np.linspace(0.0, 1.0, m)\n    v_vals = np.linspace(0.0, 1.0, m)\n    uu, vv = np.meshgrid(u_vals, v_vals, indexing='ij')\n    U = np.stack([uu.ravel(), vv.ravel()], axis=1)\n\n    # Curved manifold datasets\n    X_base = np.column_stack((U[:, 0], U[:, 1], U[:, 0]**2 + U[:, 1]**2))\n    X_glob = 7.0 * X_base\n    A = np.diag([100.0, 0.01, 50.0])\n    X_aniso = X_base @ A\n    X_std_base = standardize(X_base)\n    X_std_aniso = standardize(X_aniso)\n\n    # Balanced linear manifold datasets\n    X_bal = np.column_stack((U[:, 0], U[:, 1], (U[:, 0] + U[:, 1]) / 2.0))\n    X_std_bal = standardize(X_bal)\n\n    datasets = {\n        \"base\": X_base, \"glob\": X_glob, \"aniso\": X_aniso,\n        \"std-base\": X_std_base, \"std-aniso\": X_std_aniso,\n        \"bal\": X_bal, \"std-bal\": X_std_bal,\n    }\n\n    # 2. Compute LLE Embeddings for all datasets\n    embeddings = {name: run_lle(X, k, d, epsilon) for name, X in datasets.items()}\n\n    # 3. Perform Quantitative Comparisons\n    Y_base, Y_glob, Y_aniso = embeddings[\"base\"], embeddings[\"glob\"], embeddings[\"aniso\"]\n    Y_std_base, Y_std_aniso = embeddings[\"std-base\"], embeddings[\"std-aniso\"]\n    Y_bal, Y_std_bal = embeddings[\"bal\"], embeddings[\"std-bal\"]\n\n    # Test 1: Invariance to uniform global scaling\n    d1 = procrustes_disparity(Y_base, Y_glob)\n    b1 = bool(d1  1e-6)\n\n    # Test 2: Sensitivity to anisotropic feature scaling\n    d2 = procrustes_disparity(Y_base, Y_aniso)\n    b2 = bool(d2 > 2e-2)\n\n    # Test 3: Standardization cancels anisotropy\n    d3 = procrustes_disparity(Y_std_base, Y_std_aniso)\n    b3 = bool(d3  2e-2)\n\n    # Test 4: Standardization vs. Anisotropic Scaling\n    d4_std = procrustes_disparity(Y_base, Y_std_base)\n    b4 = bool(d4_std  d2)\n    \n    # Test 5: Standardization on a balanced manifold\n    d5 = procrustes_disparity(Y_bal, Y_std_bal)\n    b5 = bool(d5  5e-3)\n\n    results = [b1, b2, b3, b4, b5]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "理论算法通常假设数据是理想的，但现实世界的数据集常常包含重复点等退化情况。本练习将深入探讨 LLE 的一个关键故障模式：当数据点完全相同时会发生什么。您将亲手验证重复点如何导致局部协方差矩阵的奇异性，并探索两种实用的补救措施——数据抖动（jittering）和对角加载（diagonal loading），从而学习如何增强算法的稳健性。",
            "id": "3141726",
            "problem": "给定一个数据矩阵 $X \\in \\mathbb{R}^{N \\times D}$，其行向量为 $x_{i} \\in \\mathbb{R}^{D}$。对于每个索引 $i$，考虑在欧几里得范数下 $x_i$ 的 $K$ 个最近邻，若距离相等则按行索引升序排序。对于选定的邻居 $j$，通过堆叠行向量 $x_{j} - x_{i}$ 来定义邻域差异矩阵 $Z_{i} \\in \\mathbb{R}^{K \\times D}$。局部线性嵌入的构建过程将局部协方差 $C_{i} \\in \\mathbb{R}^{K \\times K}$ 构建为 $C_{i} = Z_{i} Z_{i}^{\\top}$。您将分析 $X$ 中的完全重复行如何使 $C_i$ 退化，并评估两种基于抖动的修正方法。\n\n基本原理：使用以下核心定义和事实。\n- $v \\in \\mathbb{R}^{D}$ 的欧几里得范数是 $\\lVert v \\rVert_{2} = \\sqrt{\\sum_{d=1}^{D} v_{d}^{2}}$。\n- 对于任何矩阵 $A$，秩满足 $\\mathrm{rank}(A A^{\\top}) = \\mathrm{rank}(A)$，且 $A A^{\\top}$ 是对称半正定矩阵。\n- 对于对称半正定矩阵 $M$，其特征值为实数且非负。当 $\\lambda_{\\min}(M)  0$ 时，二范数条件数为 $\\kappa_{2}(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$。\n- 如果两个数据点完全重复，它们的欧几里得距离为 $0$，相减得到零向量。\n\n您必须从这些原则和定义出发，实现以下内容，不得使用本问题陈述中作为提示给出的任何快捷公式。\n\n计算规范。\n- 最近邻：对于给定的 $i$，计算所有 $j \\neq i$ 的距离 $\\lVert x_{j} - x_{i} \\rVert_{2}$，先按距离升序排序，再按 $j$ 升序排序，并取前 $K$ 个索引。\n- 局部协方差：通过堆叠所选邻居的 $x_{j} - x_{i}$ 形成 $Z_{i}$，然后设置 $C_{i} = Z_{i} Z_{i}^{\\top}$。\n- 奇异性检测：设 $\\tau = 10^{-12}$。如果 $C_i$ 的最小特征值小于或等于 $\\tau$，则判定其为奇异矩阵，否则为非奇异矩阵。\n\n待评估的抖动策略。\n- $Z_i$ 中的各向同行抖动：在形成 $C_i$ 之前，向 $Z_i$ 的每个元素添加均值为 $0$、标准差为 $\\varepsilon$ 的独立高斯噪声。为保证可复现性，使用 $\\varepsilon = 10^{-3}$ 和固定的随机种子 $0$。\n- $C_i$ 的对角加载（岭）：给定目标条件数 $\\kappa_{\\mathrm{target}}  1$，找到最小的非负标量 $r_{\\min}$，使得 $C_{i} + r I$ 的二范数条件数至多为 $\\kappa_{\\mathrm{target}}$。将 $r_{\\min}$ 表示为四舍五入到 $6$ 位小数的数值。\n\n测试套件。\n使用以下固定的数据集和参数。所有点都在 $\\mathbb{R}^{4}$ 中。\n- 情况 $1$（理想情况，无重复点）：$X_{A}$ 有 $N = 6$ 行，\n  $x_{0} = [0, 0, 0, 0]$,\n  $x_{1} = [1, 0, 0, 0]$,\n  $x_{2} = [0, 1, 0, 0]$,\n  $x_{3} = [0, 0, 1, 0]$,\n  $x_{4} = [0, 0, 0, 1]$,\n  $x_{5} = [1, 1, 1, 1]$。\n  在 $i = 5$ 和 $K = 4$ 的条件下进行评估。输出一个布尔值，指示 $C_{5}$ 在阈值为 $\\tau$ 的奇异性规则下是否为非奇异。\n- 情况 $2$（单个完全重复的邻居）：$X_{B}$ 通过追加一个 $x_{5}$ 的副本扩展了 $X_{A}$，因此 $N = 7$ 且\n  $x_{6} = [1, 1, 1, 1]$。\n  在 $i = 5$ 和 $K = 4$ 的条件下进行评估。根据相同的规则，输出一个布尔值，指示 $C_{5}$ 是否为奇异。\n- 情况 $3$（所有邻居与中心点相同）：$X_{C}$ 有 $N = 5$ 行，全部等于 $[0, 0, 0, 0]$。在 $i = 0$ 和 $K = 4$ 的条件下进行评估。在 $Z_0$ 中应用各向同行抖动（使用 $\\varepsilon = 10^{-3}$ 和种子 $0$），从抖动后的 $Z_0$ 形成 $C_0$，并根据相同的规则输出一个布尔值，指示得到的 $C_0$ 是否为非奇异。\n- 情况 $4$（为达到目标条件数需要进行岭回归）：重用 $X_{B}$，在 $i = 5$ 和 $K = 4$ 的条件下评估。设 $\\kappa_{\\mathrm{target}} = 10^{6}$。计算最小的非负 $r_{\\min}$，使得 $C_{5} + r I$ 的二范数条件数至多为 $\\kappa_{\\mathrm{target}}$。将此 $r_{\\min}$ 作为浮点数输出，并四舍五入到 $6$ 位小数。\n\n最终输出格式。\n您的程序应生成单行输出，按顺序包含情况 1 到 4 的结果，格式为方括号内的逗号分隔列表（例如，$[b_{1},b_{2},b_{3},r]$，其中 $b_{1}$、$b_{2}$ 和 $b_{3}$ 是布尔值，$r$ 是一个四舍五入到 6 位小数的浮点数）。不应打印任何其他文本。",
            "solution": "该问题要求分析局部线性嵌入（LLE）算法中使用的局部协方差矩阵 $C_i$。我们将研究其性质，特别是由于数据点重复而可能导致的奇异性，并评估两种用于数值稳定性的方法：抖动和对角加载（岭正则化）。\n\n核心流程如下。给定一个数据矩阵 $X \\in \\mathbb{R}^{N \\times D}$ 和一个关注点 $x_i$，我们首先确定其 $K$ 个最近邻。邻居的确定方法是，根据欧几里得距离 $\\lVert x_j - x_i \\rVert_2$ 对所有其他点 $x_j$（$j \\neq i$）进行升序排序。距离上的任何相等情况都通过按行索引 $j$ 升序排序来解决。\n\n从这 $K$ 个邻居（其索引我们表示为 $j_1, \\dots, j_K$）中，我们构造邻域差异矩阵 $Z_i \\in \\mathbb{R}^{K \\times D}$。$Z_i$ 的第 $k$ 行是向量 $x_{j_k} - x_i$。\n\n然后，局部协方差矩阵 $C_i \\in \\mathbb{R}^{K \\times K}$ 被定义为格拉姆矩阵 $C_i = Z_i Z_i^\\top$。$C_i$ 的每个元素 $(a, b)$ 是第 $a$ 个和第 $b$ 个差分向量的点积：$(C_i)_{ab} = (x_{j_a} - x_i) \\cdot (x_{j_b} - x_i)$。\n\n根据其构造，$C_i$ 是对称半正定的。其秩由 $Z_i$ 的秩决定，即由邻域差异向量 $\\{x_{j_k} - x_i\\}_{k=1}^K$ 张成的向量空间的维度。$Z_i$ 的秩至多为 $\\min(K, D)$。如果秩小于 $K$，则 $K \\times K$ 矩阵 $C_i$ 是奇异的，这意味着它至少有一个零特征值。出于数值计算的目的，如果其最小特征值 $\\lambda_{\\min}(C_i)$ 满足 $\\lambda_{\\min}(C_i) \\le \\tau$，我们将 $C_i$ 分类为奇异矩阵，其中阈值给定为 $\\tau = 10^{-12}$。\n\n### 情况 1：无重复点（非奇异情况）\n给定数据集 $X_A \\in \\mathbb{R}^{6 \\times 4}$，要求我们评估点 $x_5 = [1, 1, 1, 1]$ 在 $K=4$ 时的 $C_5$。\n\n首先，我们计算从 $x_5$ 到所有其他点 $x_j$（$j \\in \\{0, 1, 2, 3, 4\\}$）的欧几里得距离的平方：\n- $\\lVert x_0 - x_5 \\rVert_2^2 = \\lVert [-1, -1, -1, -1] \\rVert_2^2 = 4$\n- $\\lVert x_1 - x_5 \\rVert_2^2 = \\lVert [0, -1, -1, -1] \\rVert_2^2 = 3$\n- $\\lVert x_2 - x_5 \\rVert_2^2 = \\lVert [-1, 0, -1, -1] \\rVert_2^2 = 3$\n- $\\lVert x_3 - x_5 \\rVert_2^2 = \\lVert [-1, -1, 0, -1] \\rVert_2^2 = 3$\n- $\\lVert x_4 - x_5 \\rVert_2^2 = \\lVert [-1, -1, -1, 0] \\rVert_2^2 = 3$\n\n四个最小的距离是相同的（$\\sqrt{3}$），对应于点 $x_1, x_2, x_3, x_4$。由于索引已经是升序排列，这些就是 $K=4$ 个最近邻。邻域差异矩阵 $Z_5$ 由行向量 $x_j - x_5$ 构成：\n$$Z_5 = \\begin{pmatrix} 0  -1  -1  -1 \\\\ -1  0  -1  -1 \\\\ -1  -1  0  -1 \\\\ -1  -1  -1  0 \\end{pmatrix}$$\n局部协方差矩阵 $C_5 = Z_5 Z_5^\\top$ 为：\n$$C_5 = \\begin{pmatrix} 3  2  2  2 \\\\ 2  3  2  2 \\\\ 2  2  3  2 \\\\ 2  2  2  3 \\end{pmatrix}$$\n该矩阵的形式为 $2J + I$，其中 $J$ 是全一矩阵，$I$ 是单位矩阵。一个形式为 $aJ+bI$ 的 $k \\times k$ 矩阵的特征值为 $a \\cdot k + b$（重数为 $1$）和 $b$（重数为 $k-1$）。对于 $C_5$，我们有 $k=4$，$a=2$，$b=1$。特征值为 $2 \\cdot 4 + 1 = 9$ 和 $1$（重数为 $3$）。\n最小特征值为 $\\lambda_{\\min}(C_5) = 1$。由于 $1  \\tau=10^{-12}$，该矩阵是非奇异的。此情况的结果是 `True`。\n\n### 情况 2：单个完全重复的邻居（奇异情况）\n数据集 $X_B$ 是在 $X_A$ 的基础上增加了 $x_6 = x_5 = [1, 1, 1, 1]$。我们重新评估 $x_5$ 在 $K=4$ 时的情形。\n从 $x_5$ 到其重复点 $x_6$ 的距离是 $\\lVert x_6 - x_5 \\rVert_2 = 0$。这是可能的最小距离，所以 $x_6$ 是最近的邻居。其余三个邻居是 $x_1, x_2, x_3$，每个距离都是 $\\sqrt{3}$。\n邻域差异矩阵 $Z_5$ 是：\n$$Z_5 = \\begin{pmatrix} x_6-x_5 \\\\ x_1-x_5 \\\\ x_2-x_5 \\\\ x_3-x_5 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  0 \\\\ 0  -1  -1  -1 \\\\ -1  0  -1  -1 \\\\ -1  -1  0  -1 \\end{pmatrix}$$\n由于 $Z_5$ 的第一行是零向量，所以这些行是线性相关的。因此，$\\mathrm{rank}(Z_5)  4$。格拉姆矩阵 $C_5 = Z_5 Z_5^\\top$ 因此必须是奇异的，其秩为 $\\mathrm{rank}(C_5) = \\mathrm{rank}(Z_5)  4$。这意味着它的最小特征值恰好为 $\\lambda_{\\min}(C_5)=0$。由于 $0 \\le \\tau=10^{-12}$，该矩阵是奇异的。此情况的结果是 `True`。\n\n### 情况 3：为正则化进行抖动\n数据集 $X_C$ 包含 $N=5$ 个相同的点 $[0, 0, 0, 0]$。我们评估 $x_0$ 在 $K=4$ 时的情形。\n邻居是 $x_1, x_2, x_3, x_4$。到每个点的距离都是 $0$。差分向量 $x_j - x_0$ 都是零向量。初始的邻域差异矩阵 $Z_0$ 是一个 $4 \\times 4$ 的零矩阵：\n$$Z_0 = \\mathbf{0}_{4 \\times 4}$$\n该矩阵是秩为 $0$ 的最大奇异矩阵。为了修正这个问题，我们应用各向同性抖动。一个噪声矩阵，其元素独立地从高斯分布 $\\mathcal{N}(0, \\varepsilon^2)$（其中 $\\varepsilon = 10^{-3}$）中抽取，被加到 $Z_0$ 上形成一个新的矩阵 $Z'_0$。由于 $Z'_0$ 的元素是从一个连续概率分布中抽取的，该矩阵将以概率 $1$ 可逆。其秩将为 $4$。\n新的协方差矩阵 $C_0 = Z'_0 (Z'_0)^\\top$ 也将有秩 $4$，因此是非奇异的。其最小特征值 $\\lambda_{\\min}(C_0)$ 将严格为正。考虑到噪声的非零方差，几乎可以肯定 $\\lambda_{\\min}(C_0)$ 将大于小容差 $\\tau = 10^{-12}$。因此，抖动后的矩阵是非奇异的。此情况的结果是 `True`。\n\n### 情况 4：为正则化进行对角加载\n我们重用情况 2 中的奇异矩阵 $C_5$。其特征值为 $\\{0, 1, 1, 7\\}$。我们需要找到最小的非负标量 $r_{\\min}$，使得正则化矩阵 $C'_5 = C_5 + rI$ 的条件数 $\\kappa_2(C'_5) \\le \\kappa_{\\mathrm{target}}$，其中 $\\kappa_{\\mathrm{target}} = 10^6$。\n$C'_5$ 的特征值是通过将 $C_5$ 的特征值平移 $r$ 得到的。对于 $r \\ge 0$，它们是 $\\{r, 1+r, 1+r, 7+r\\}$。最小和最大特征值分别是 $\\lambda'_{\\min} = r$ 和 $\\lambda'_{\\max} = 7+r$。\n条件数（对于 $r  0$）是 $\\kappa_2(C'_5) = \\frac{\\lambda'_{\\max}}{\\lambda'_{\\min}} = \\frac{7+r}{r}$。\n我们解不等式：\n$$\\frac{7+r}{r} \\le \\kappa_{\\mathrm{target}}$$\n因为 $r$ 必须为正（否则 $\\kappa_2$ 未定义或为无穷大），我们可以两边乘以 $r$：\n$$7 + r \\le r \\cdot \\kappa_{\\mathrm{target}} \\implies 7 \\le r (\\kappa_{\\mathrm{target}} - 1)$$\n这给出了 $r$ 的条件：\n$$r \\ge \\frac{7}{\\kappa_{\\mathrm{target}} - 1}$$\n$r$ 可以取的最小非负值为 $r_{\\min} = \\frac{7}{\\kappa_{\\mathrm{target}} - 1}$。代入 $\\kappa_{\\mathrm{target}} = 10^6$：\n$$r_{\\min} = \\frac{7}{10^6 - 1} = \\frac{7}{999999} \\approx 7.000007000007 \\times 10^{-6}$$\n将此值四舍五入到 6 位小数得到 $0.000007$。",
            "answer": "```python\nimport numpy as np\n\ndef find_neighbors_and_form_CZ(X, i, K):\n    \"\"\"\n    Finds K nearest neighbors for X[i] and computes C_i and Z_i.\n    Ties are broken by ascending row index.\n    \"\"\"\n    x_i = X[i, :]\n    num_points = X.shape[0]\n    \n    distances = []\n    for j in range(num_points):\n        if i == j:\n            continue\n        dist = np.linalg.norm(X[j, :] - x_i)\n        distances.append((dist, j))\n    \n    # Sort by distance, then by index for tie-breaking\n    distances.sort()\n    \n    # Get the K nearest neighbor indices\n    neighbor_indices = [j for dist, j in distances[:K]]\n    \n    # Form the Z_i matrix. The shape should be K x D\n    Z_i = np.zeros((K, X.shape[1]))\n    for k, j in enumerate(neighbor_indices):\n        Z_i[k, :] = X[j, :] - x_i\n    \n    # Form the C_i matrix\n    C_i = Z_i @ Z_i.T\n    \n    return C_i, Z_i\n\ndef solve():\n    \"\"\"\n    Solves the four test cases specified in the problem.\n    \"\"\"\n    # Define datasets\n    X_A = np.array([\n        [0., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 1., 1., 1.]\n    ], dtype=float)\n\n    X_B = np.array([\n        [0., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]\n    ], dtype=float)\n\n    X_C = np.zeros((5, 4), dtype=float)\n    \n    tau = 1e-12\n    results = []\n\n    # Case 1: Happy path, no duplicates. Check for nonsingular.\n    C5_A, _ = find_neighbors_and_form_CZ(X_A, i=5, K=4)\n    eigenvalues_A = np.linalg.eigh(C5_A)[0]\n    lambda_min_A = eigenvalues_A[0]\n    results.append(lambda_min_A > tau)\n\n    # Case 2: Single exact duplicate neighbor. Check for singular.\n    C5_B, _ = find_neighbors_and_form_CZ(X_B, i=5, K=4)\n    eigenvalues_B = np.linalg.eigh(C5_B)[0]\n    lambda_min_B = eigenvalues_B[0]\n    results.append(lambda_min_B = tau)\n\n    # Case 3: All neighbors identical. Jitter and check for nonsingular.\n    epsilon = 1e-3\n    seed = 0\n    _, Z0_C = find_neighbors_and_form_CZ(X_C, i=0, K=4)\n    \n    np.random.seed(seed)\n    noise = np.random.normal(scale=epsilon, size=Z0_C.shape)\n    Z0_jittered = Z0_C + noise\n    \n    C0_jittered = Z0_jittered @ Z0_jittered.T\n    eigenvalues_C = np.linalg.eigh(C0_jittered)[0]\n    lambda_min_C = eigenvalues_C[0]\n    results.append(lambda_min_C > tau)\n    \n    # Case 4: Ridge needed for a target condition number.\n    kappa_target = 10**6\n    # Re-use eigenvalues from Case 2.\n    lambda_min_B_case4 = eigenvalues_B[0]\n    lambda_max_B_case4 = eigenvalues_B[-1]\n    \n    # We need to find the smallest r = 0 such that:\n    # (lambda_max + r) / (lambda_min + r) = kappa_target\n    # This leads to: r = (lambda_max - kappa_target * lambda_min) / (kappa_target - 1)\n    if kappa_target > 1:\n        r_min_candidate = (lambda_max_B_case4 - kappa_target * lambda_min_B_case4) / (kappa_target - 1)\n        r_min = max(0.0, r_min_candidate)\n    else:\n        # This case is not relevant for the problem, but for completeness:\n        # If target condition number = 1, it's generally impossible unless a matrix is scalar multiple of identity.\n        r_min = np.inf \n\n    results.append(r_min)\n    \n    # Format final output string as per requirements\n    formatted_results = []\n    for item in results:\n        if isinstance(item, bool):\n            formatted_results.append(str(item))\n        elif isinstance(item, float):\n            # Format the float to 6 decimal places for the output string\n            formatted_results.append(f\"{item:.6f}\")\n        else:\n            formatted_results.append(str(item))\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在拥有了正确的算法并处理了数据退化问题之后，我们还需要关注实现的最后但至关重要的一环：数值稳定性。在计算机浮点运算中，数学上等价的公式可能产生截然不同的结果。本练习将引导您比较两种用于构建 LLE 关键矩阵 $M$ 的代数等价公式，通过量化分析，您将揭示灾难性抵消（catastrophic cancellation）的危害，并体会到选择数值稳定计算方法的重要性。",
            "id": "3141701",
            "problem": "你的任务是通过计算来检验两种代数上等价的局部线性嵌入（Locally Linear Embedding, LLE）矩阵构造方法所带来的舍入误差影响。在 LLE 中，需要构建一个权重矩阵 $W \\in \\mathbb{R}^{n \\times n}$，其行向量 $w_i^\\top$ 包含的权重可以从每个数据点 $x_i \\in \\mathbb{R}^d$ 的 $k$ 个最近邻点重构该点，并满足权重之和为 $1$ 的仿射约束。用于寻找低维嵌入的标准类图拉普拉斯矩阵构造为 $M = (I - W)^\\top (I - W)$，其中 $I$ 是 $n \\times n$ 的单位矩阵。利用矩阵分配律和转置法则，这个乘积可以展开为一个代数上等价的表达式 $M = I + W^\\top W - W - W^\\top$。虽然这两个表达式在精确算术中是数学上恒等的，但在浮点运算中它们可能表现出不同的舍入误差行为。\n\n允许使用的基本原理包括：矩阵乘法的分配律和结合律、转置法则 $(AB)^\\top = B^\\top A^\\top$、欧几里得范数及其弗罗贝尼乌斯变体，以及单个算术运算会引入一个量级为机器精度的相对误差的标准浮点舍入模型。除了这些基础知识外，你不能假设任何未经证明的性质。\n\n实现一个程序，该程序：\n- 通过求解具有 $k$ 个最近邻的局部线性重构问题来从数据构造 $W$，使用 Tikhonov 风格的正则化来稳定奇异的局部协方差。\n- 构造 $M$ 的两个版本：乘积形式 $M_{\\text{prod}} = (I - W)^\\top (I - W)$ 和展开形式 $M_{\\text{exp}} = I + W^\\top W - W - W^\\top$。\n- 使用可量化的指标比较它们的数值行为。\n\n权重构造细节：\n- 对于每个点 $x_i \\in \\mathbb{R}^d$，通过欧几里得距离找到其 $k$ 个最近邻点（不包括其自身）。\n- 令 $Z \\in \\mathbb{R}^{k \\times d}$ 收集所选邻点与 $x_i$ 的差值 $x_{j} - x_i$。构造局部协方差 $C = Z Z^\\top \\in \\mathbb{R}^{k \\times k}$。\n- 对 $C$ 进行正则化：$C \\leftarrow C + \\lambda \\operatorname{trace}(C) I_k$，其中 $\\lambda  0$ 是给定的标量， $I_k$ 是 $k \\times k$ 的单位矩阵。\n- 求解 $C w = \\mathbf{1}$ 得到 $w \\in \\mathbb{R}^k$，然后归一化 $w \\leftarrow w / (\\mathbf{1}^\\top w)$ 以强制施加仿射约束 $\\mathbf{1}^\\top w = 1$。将这些权重放入 $W$ 的第 $i$ 行中对应邻点索引的位置。\n\n对每种情况需要计算的指标：\n- 两个计算出的矩阵之间差异的弗罗贝尼乌斯范数：$e_F = \\lVert M_{\\text{prod}} - M_{\\text{exp}} \\rVert_F$。\n- 每种形式的对称性残差：$r_{\\text{sym,prod}} = \\lVert M_{\\text{prod}} - M_{\\text{prod}}^\\top \\rVert_F / \\lVert M_{\\text{prod}} \\rVert_F$ 和 $r_{\\text{sym,exp}} = \\lVert M_{\\text{exp}} - M_{\\text{exp}}^\\top \\rVert_F / \\lVert M_{\\text{exp}} \\rVert_F$，约定如果分母为 $0$ 则返回 $0$。\n- 对称化矩阵 $\\tfrac{1}{2}(M + M^\\top)$ 对每种形式的最小特征值，分别表示为 $\\lambda_{\\min,\\text{prod}}$ 和 $\\lambda_{\\min,\\text{exp}}$。\n\n测试套件：\n在以下 $3$ 种情况下实现并评估上述内容。所有随机元素必须使用指定的种子以确保确定性输出。\n\n- 情况 1（良态，近乎均匀的流形）：\n  - $n = 64$，$d = 2$，$k = 8$，$\\lambda = 10^{-3}$。\n  - 数据：带噪声的单位圆上的点。令 $\\theta_j = 2\\pi j / n + \\eta_j$，对于 $j = 0, \\ldots, n-1$，其中 $\\eta_j$ 是使用随机种子 $0$ 独立抽取的均值为 $0$、标准差为 $10^{-3}$ 的高斯分布。设 $x_j = (\\cos \\theta_j, \\sin \\theta_j)$。\n\n- 情况 2（近乎共线的邻域）：\n  - $n = 80$，$d = 2$，$k = 10$，$\\lambda = 10^{-12}$。\n  - 数据：沿一条近乎直线的点，带有微小噪声。令 $t_j = j/(n-1)$，对于 $j = 0, \\ldots, n-1$。定义 $x_j = (t_j, 10^{-6} t_j + \\nu_j)$，其中 $\\nu_j$ 是使用随机种子 $1$ 独立抽取的均值为 $0$、标准差为 $10^{-8}$ 的高斯分布。\n\n- 情况 3（高维各向异性缩放）：\n  - $n = 120$，$d = 10$，$k = 12$，$\\lambda = 10^{-3}$。\n  - 数据：使用随机种子 $2$ 抽取 $n \\times d$ 的独立标准正态样本，然后将第 $i$ 列乘以 $10^{i-1}$，对于 $i = 1, \\ldots, d$。\n\n对于每种情况，按以下顺序计算指标：\n- $e_F$\n- $r_{\\text{sym,prod}}$\n- $r_{\\text{sym,exp}}$\n- $\\lambda_{\\min,\\text{prod}}$\n- $\\lambda_{\\min,\\text{exp}}$\n\n你的程序应该生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来，结果按所述的案例和指标排序，即：\n$[e_F^{(1)}, r_{\\text{sym,prod}}^{(1)}, r_{\\text{sym,exp}}^{(1)}, \\lambda_{\\min,\\text{prod}}^{(1)}, \\lambda_{\\min,\\text{exp}}^{(1)}, e_F^{(2)}, r_{\\text{sym,prod}}^{(2)}, r_{\\text{sym,exp}}^{(2)}, \\lambda_{\\min,\\text{prod}}^{(2)}, \\lambda_{\\min,\\text{exp}}^{(2)}, e_F^{(3)}, r_{\\text{sym,prod}}^{(3)}, r_{\\text{sym,exp}}^{(3)}, \\lambda_{\\min,\\text{prod}}^{(3)}, \\lambda_{\\min,\\text{exp}}^{(3)}]$。\n\n所有输出必须是纯实数（浮点数），不带单位。本问题不涉及角度和物理单位。代码必须完全自包含，不接受任何外部输入。",
            "solution": "该问题要求通过计算来研究局部线性嵌入（LLE）矩阵 $M$ 的两种代数等价表达式的数值稳定性。任务的核心是使用给定的数据和方法构建这个矩阵，然后用一组定义的指标来量化这两种形式之间的数值差异。\n\nLLE 矩阵 $M \\in \\mathbb{R}^{n \\times n}$ 的两种表达式是：\n1.  乘积形式：$M_{\\text{prod}} = (I - W)^\\top (I - W)$\n2.  展开形式：$M_{\\text{exp}} = I + W^\\top W - W - W^\\top$\n\n其中，$I$ 是 $n \\times n$ 的单位矩阵，$W \\in \\mathbb{R}^{n \\times n}$ 是编码数据内部局部线性关系的权重矩阵。在精确算术中，这两种形式是恒等的。然而，在浮点运算中，它们的行为可能显著不同。乘积形式通常更稳定。展开形式涉及到可能是大的、几乎相等的矩阵（$I + W^\\top W$ 和 $W + W^\\top$）的减法，这是灾难性抵消的典型场景，可能导致精度的严重损失。\n\n解决方案通过对每个提供的测试案例执行一系列明确定义的步骤来实现。\n\n### 步骤 1：权重矩阵的构建\n权重矩阵 $W$ 是逐行构建的。对于 $n$ 个数据点中的每个数据点 $x_i \\in \\mathbb{R}^d$：\n\n1.  **邻点识别**：通过计算与所有其他点 $x_j$（其中 $j \\neq i$）的欧几里得距离平方 $d(x_i, x_j)^2 = \\lVert x_i - x_j \\rVert_2^2$，并选择距离最小的 $k$ 个点，来确定 $x_i$ 的 $k$ 个最近邻。设这些邻点的索引集合为 $\\mathcal{N}_i$。\n\n2.  **局部协方差**：形成一个矩阵 $Z_i \\in \\mathbb{R}^{k \\times d}$，其中每一行都是一个邻点与 $x_i$ 之间的向量差。也就是说，$Z_i$ 的行是 $(x_j - x_i)$，对于所有 $j \\in \\mathcal{N}_i$。然后计算局部协方差（或格拉姆）矩阵 $C_i = Z_i Z_i^\\top$。这个 $k \\times k$ 的矩阵概括了 $x_i$ 邻域的几何形状。\n\n3.  **正则化**：如果 $x_i$ 的邻点以某种方式共线或共面，导致 $C_i$ 是奇异或病态的，那么求解重构权重会变得不稳定。为了缓解这个问题，应用了 Tikhonov 正则化。正则化后的协方差矩阵是 $C_{i, \\text{reg}} = C_i + \\lambda \\operatorname{trace}(C_i) I_k$，其中 $\\lambda  0$ 是一个小的正则化参数，$I_k$ 是 $k \\times k$ 的单位矩阵。这一加法确保了矩阵是正定的，因此是可逆的。\n\n4.  **权重计算**：点 $x_i$ 的重构权重（用向量 $w_i \\in \\mathbb{R}^k$ 表示）通过求解线性系统 $C_{i, \\text{reg}} w_i = \\mathbf{1}$ 得到，其中 $\\mathbf{1}$ 是一个包含 $k$ 个 1 的向量。该系统源于在权重和为一的约束下最小化重构误差 $\\lVert x_i - \\sum_{j \\in \\mathcal{N}_i} w_{ij} x_j \\rVert_2^2$。\n\n5.  **归一化**：问题指定了一个仿射变换（权重和为 1）。上一步的解 $w_i$ 并不严格强制这一点，因此需要对其进行归一化：$w_i \\leftarrow w_i / (\\mathbf{1}^\\top w_i)$。这确保了 $\\sum_j w_{ij} = 1$。\n\n6.  **$W$ 的组装**：隐式地形成一个稀疏的 $n \\times n$ 矩阵 $W$。对于每个点 $i$，计算出的归一化权重被放置在 $W$ 的第 $i$ 行，位于与 $\\mathcal{N}_i$ 中邻点索引相对应的列。也就是说，如果第 $m$ 个邻点是点 $j$，则 $W_{ij} = (w_i)_m$，否则 $W_{ij} = 0$。\n\n### 步骤 2：$M_{\\text{prod}}$ 和 $M_{\\text{exp}}$ 的构建\n一旦权重矩阵 $W$ 计算完毕，就使用标准的矩阵运算来构建 LLE 矩阵 $M$ 的两种形式：\n- $M_{\\text{prod}}$ 计算为 $(I - W)^\\top (I - W)$。这涉及一次矩阵减法和一次矩阵乘法。从数值角度看，这种构建方法是稳健的，并且（在浮点误差范围内）保证了结果是半正定对称的。\n- $M_{\\text{exp}}$ 计算为 $I + W^\\top W - W - W^\\top$。这涉及多次加法和减法。正是这种形式容易出现数值不稳定性。\n\n### 步骤 3：数值指标的评估\n使用五个指标来量化 $M_{\\text{prod}}$ 和 $M_{\\text{exp}}$ 的数值属性：\n\n1.  **差异的弗罗贝尼乌斯范数 ($e_F$)**：计算为 $e_F = \\lVert M_{\\text{prod}} - M_{\\text{exp}} \\rVert_F$。该指标直接衡量两个计算出的矩阵之间差异的大小。非零值是浮点不精确性的直接后果。\n\n2.  **对称性残差 ($r_{\\text{sym}$)**：对每种形式计算为 $r_{\\text{sym, M}} = \\lVert M - M^\\top \\rVert_F / \\lVert M \\rVert_F$。理论上，$M_{\\text{prod}}$ 和 $M_{\\text{exp}}$ 都应该是对称的。这个指标量化了由于数值误差导致的对称性损失。值越小表示对这种结构性质的保持得越好。\n\n3.  **对称化矩阵的最小特征值 ($\\lambda_{\\min}$)**：对每种形式计算为 $\\lambda_{\\min, M} = \\lambda_{\\min}\\left(\\frac{1}{2}(M + M^\\top)\\right)$。LLE 矩阵 $M$ 应该是半正定的，意味着其特征值必须为非负。$M_{\\text{exp}}$ 中的灾难性抵消可能导致计算出的矩阵不是半正定的。对矩阵进行对称化确保了其特征值为实数，从而可以进行有意义的检查。负的最小特征值是数值失效的强烈指标，因为它意味着与矩阵相关的二次型不是凸的，这与 LLE 的理论相悖。\n\n这些步骤被系统地应用于三个测试案例中的每一个，这些案例旨在探测数值方法在不同条件下的表现：一个行为良好的流形、一个近乎退化的点配置，以及一个高维各向异性数据集。然后，将结果汇总并按要求格式化。实现利用 `NumPy` 进行高效的数组操作，并使用 `SciPy` 中稳健的线性代数求解器。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve, eigvalsh\n\ndef solve_problem():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Well-conditioned, nearly uniform manifold\n        {'n': 64, 'd': 2, 'k': 8, 'lambda_reg': 1e-3, 'seed': 0, 'case_id': 1},\n        # Case 2: Nearly collinear neighborhoods\n        {'n': 80, 'd': 2, 'k': 10, 'lambda_reg': 1e-12, 'seed': 1, 'case_id': 2},\n        # Case 3: High-dimensional anisotropic scaling\n        {'n': 120, 'd': 10, 'k': 12, 'lambda_reg': 1e-3, 'seed': 2, 'case_id': 3},\n    ]\n\n    all_results = []\n    for params in test_cases:\n        case_results = run_case(params)\n        all_results.extend(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef generate_data(n, d, seed, case_id):\n    \"\"\"\n    Generates synthetic data for a given test case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    if case_id == 1:\n        # Noisy circle\n        eta = rng.normal(0, 1e-3, size=n)\n        theta = 2 * np.pi * np.arange(n) / n + eta\n        X = np.stack([np.cos(theta), np.sin(theta)], axis=1)\n        return X\n    \n    elif case_id == 2:\n        # Nearly straight line with tiny noise\n        t = np.linspace(0, 1, n)\n        nu = rng.normal(0, 1e-8, size=n)\n        X = np.stack([t, 1e-6 * t + nu], axis=1)\n        return X\n\n    elif case_id == 3:\n        # High-dimensional anisotropic scaled data\n        A = rng.standard_normal(size=(n, d))\n        scaling_factors = 10.0**np.arange(d)\n        X = A * scaling_factors\n        return X\n    \n    raise ValueError(\"Invalid case_id\")\n\ndef construct_weight_matrix(X, n, k, lambda_reg):\n    \"\"\"\n    Constructs the LLE weight matrix W.\n    \"\"\"\n    W = np.zeros((n, n))\n    I_k = np.identity(k)\n    ones_k = np.ones(k)\n    \n    for i in range(n):\n        # Calculate squared Euclidean distances from point i\n        dist_sq = np.sum((X - X[i, :])**2, axis=1)\n        \n        # Get indices of k nearest neighbors (excluding self)\n        neighbor_indices = np.argsort(dist_sq)[1 : k + 1]\n        \n        # Form the neighborhood matrix Z (k x d)\n        neighbors = X[neighbor_indices, :]\n        Z = neighbors - X[i, :]\n        \n        # Form the local covariance matrix C (k x k)\n        C = Z @ Z.T\n        \n        # Regularize C\n        trace_C = np.trace(C)\n        if trace_C > 0:\n            reg_term = lambda_reg * trace_C\n            C_reg = C + reg_term * I_k\n        else:\n            C_reg = C\n        \n        # Solve C_reg * w = 1 for weights w\n        w = solve(C_reg, ones_k, assume_a='pos')\n        \n        # Normalize weights to sum to 1\n        sum_w = np.sum(w)\n        if sum_w != 0:\n            w_norm = w / sum_w\n        else:\n            # Fallback for the unlikely case of sum(w) = 0\n            w_norm = np.ones(k) / k\n        \n        # Place weights into the matrix W\n        W[i, neighbor_indices] = w_norm\n        \n    return W\n\ndef run_case(params):\n    \"\"\"\n    Executes one full test case and computes all metrics.\n    \"\"\"\n    n, d, k, lambda_reg, seed, case_id = params['n'], params['d'], params['k'], params['lambda_reg'], params['seed'], params['case_id']\n    \n    X = generate_data(n, d, seed, case_id)\n    W = construct_weight_matrix(X, n, k, lambda_reg)\n    \n    I = np.identity(n)\n    \n    # Form M using product form\n    A = I - W\n    M_prod = A.T @ A\n    \n    # Form M using expanded form\n    M_exp = I + W.T @ W - W - W.T\n    \n    # --- Compute Metrics ---\n    \n    # 1. Frobenius norm of the disagreement\n    e_F = np.linalg.norm(M_prod - M_exp, 'fro')\n    \n    # 2. Symmetry residual for product form\n    norm_M_prod = np.linalg.norm(M_prod, 'fro')\n    r_sym_prod = np.linalg.norm(M_prod - M_prod.T, 'fro') / norm_M_prod if norm_M_prod > 0 else 0.0\n        \n    # 3. Symmetry residual for expanded form\n    norm_M_exp = np.linalg.norm(M_exp, 'fro')\n    r_sym_exp = np.linalg.norm(M_exp - M_exp.T, 'fro') / norm_M_exp if norm_M_exp > 0 else 0.0\n        \n    # 4. Minimum eigenvalue of symmetrized product form\n    M_prod_sym = 0.5 * (M_prod + M_prod.T)\n    eigvals_prod = eigvalsh(M_prod_sym)\n    lambda_min_prod = eigvals_prod[0] # eigvalsh returns sorted eigenvalues\n    \n    # 5. Minimum eigenvalue of symmetrized expanded form\n    M_exp_sym = 0.5 * (M_exp + M_exp.T)\n    eigvals_exp = eigvalsh(M_exp_sym)\n    lambda_min_exp = eigvals_exp[0]\n    \n    return [e_F, r_sym_prod, r_sym_exp, lambda_min_prod, lambda_min_exp]\n\n# Execute the main function\nsolve_problem()\n```"
        }
    ]
}