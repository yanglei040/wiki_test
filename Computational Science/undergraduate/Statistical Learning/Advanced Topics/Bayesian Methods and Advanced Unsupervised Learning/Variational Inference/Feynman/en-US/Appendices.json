{
    "hands_on_practices": [
        {
            "introduction": "The heart of mean-field variational inference is the iterative optimization of each factor in the approximate posterior distribution. This first exercise provides a focused look at the core mechanic: deriving a single coordinate ascent update in a challenging, non-conjugate model. By working through the Bayesian logistic regression example , you will gain essential practice in applying the general update rule and learn how to use auxiliary bounds to handle terms that make the model analytically intractable.",
            "id": "691486",
            "problem": "Consider a Bayesian logistic regression model for a binary classification task. We are given a single data point $(x, t)$, where $x=(x_1, x_2)^T \\in \\mathbb{R}^2$ is a feature vector and $t \\in \\{-1, 1\\}$ is the corresponding class label. The likelihood of the label is given by $p(t|w,x) = \\sigma(t w^T x)$, where $w=(w_1, w_2)^T$ is the weight vector and $\\sigma(z) = (1+e^{-z})^{-1}$ is the logistic sigmoid function.\n\nA zero-mean isotropic Gaussian prior with precision $\\alpha > 0$ is placed on the weights:\n$$p(w) = \\mathcal{N}(w | 0, \\alpha^{-1}I)$$\nwhere $I$ is the $2 \\times 2$ identity matrix.\n\nThe true posterior distribution $p(w|x,t)$ is intractable. We approximate it using variational inference with a factorized Gaussian posterior approximation (mean-field):\n$$q(w) = q_1(w_1)q_2(w_2) = \\mathcal{N}(w_1|\\mu_1, v_1)\\mathcal{N}(w_2|\\mu_2, v_2)$$\n\nTo handle the non-conjugacy of the likelihood and prior, the log-likelihood term in the Evidence Lower Bound (ELBO) is approximated. The term $\\log \\sigma(z)$ is lower-bounded by a quadratic function involving a variational parameter $\\xi \\in \\mathbb{R}$:\n$$ \\log \\sigma(z) \\ge \\log \\sigma(\\xi) + \\frac{1}{2}(z - \\xi) - \\lambda(\\xi)(z^2 - \\xi^2) $$\nwhere the function $\\lambda(\\xi)$ is defined as:\n$$ \\lambda(\\xi) = \\frac{\\tanh(\\xi/2)}{4\\xi} $$\n\nThe variational parameters are updated iteratively. Consider a single update step for the distribution $q_1(w_1)$. Assuming the parameters for the other factor ($\\mu_2$) and the local bound ($\\xi$) are fixed, find the expression for the optimal mean $\\mu_1$ that maximizes the ELBO. Your answer should be expressed in terms of $x_1, x_2, t, \\alpha, \\xi$, and $\\mu_2$.",
            "solution": "1. Bound the log-likelihood term:\n$$\\log\\sigma(tw^T x)\\ge\\log\\sigma(\\xi)+\\frac12\\,t(w_1x_1+w_2x_2)-\\lambda(\\xi)\\bigl[t^2(w_1x_1+w_2x_2)^2-\\xi^2\\bigr].$$\n\n2. Log-prior:\n$$\\log p(w)=-\\frac12\\alpha(w_1^2+w_2^2)+\\text{const}.$$\n\n3. Form the variational exponent for $w_1$:\n$$\n\\mathbb{E}_{q_2}[\\log p(w)+\\log\\sigma(tw^T x)]\n=\\;-\\tfrac12\\,\\alpha\\,w_1^2\n+\\underbrace{\\tfrac12\\,t\\,x_1\\,w_1}_{\\text{from linear term}}\n-\\lambda(\\xi)\\,\\mathbb{E}_{q_2}[(w_1x_1+w_2x_2)^2]\n+\\text{const}.\n$$\n\n4. Compute $\\mathbb{E}_{q_2}[(w_1x_1+w_2x_2)^2]$:\n$$\n(w_1x_1+\\mu_2x_2)^2+v_2x_2^2\n=w_1^2x_1^2+2w_1x_1\\mu_2x_2+\\text{const}.\n$$\n\n5. Collect quadratic and linear terms in $w_1$:\n- Quadratic: $-\\tfrac12(\\alpha+2\\lambda(\\xi)x_1^2)w_1^2$.\n- Linear: $(\\tfrac12t x_1-2\\lambda(\\xi)x_1x_2\\mu_2)w_1$.\n\n6. Read off Gaussian parameters:\n$$\n\\text{precision}=\\alpha+2\\lambda(\\xi)x_1^2,\\quad\n\\text{nat.\\ mean}=\\tfrac12t x_1-2\\lambda(\\xi)x_1x_2\\mu_2\n$$\nso\n$$\n\\mu_1=\\frac{\\tfrac12t x_1-2\\lambda(\\xi)x_1x_2\\mu_2}{\\alpha+2\\lambda(\\xi)x_1^2}.\n$$",
            "answer": "$$\\boxed{\\frac{\\tfrac12\\,t\\,x_1 \\;-\\;2\\,\\lambda(\\xi)\\,x_1\\,x_2\\,\\mu_2}{\\alpha+2\\,\\lambda(\\xi)\\,x_1^2}}$$"
        },
        {
            "introduction": "Building on the skill of deriving individual updates, this practice challenges you to assemble them into a complete algorithm for a common unsupervised learning problem. You will implement the full coordinate ascent variational inference (CAVI) algorithm for a Gaussian Mixture Model (GMM), a classic application of VI . This exercise will solidify your understanding of how the updates for different latent variables work in concert and introduce you to important practical considerations like model identifiability and the \"label switching\" phenomenon.",
            "id": "3191998",
            "problem": "Consider a one-dimensional, two-component Gaussian mixture model with latent component indicators and unknown component means. Let there be $K=2$ components and $N$ scalar observations $\\{x_n\\}_{n=1}^N$. The generative model is defined by the following well-tested construction:\n- Latent indicators $z_n \\in \\{1,2\\}$ are drawn independently from a categorical distribution with mixing weights $\\pi = (\\pi_1,\\pi_2)$, where $\\pi_k \\in (0,1)$ and $\\pi_1 + \\pi_2 = 1$.\n- Conditional on $z_n = k$ and the component mean $\\mu_k$, each observation $x_n$ is drawn from a Gaussian distribution with known variance $\\sigma^2$, that is $x_n \\mid (z_n=k,\\mu_k) \\sim \\mathcal{N}(\\mu_k,\\sigma^2)$.\n- The unknown component means have independent Gaussian priors $\\mu_k \\sim \\mathcal{N}(m_{0k},\\tau_{0k}^{-1})$, where $\\tau_{0k} > 0$ is the prior precision and $m_{0k}$ is the prior mean.\n\nUsing the Evidence Lower Bound (ELBO) definition from variational inference and the mean-field factorization $q(z,\\mu) = \\left(\\prod_{n=1}^N q(z_n)\\right)\\left(\\prod_{k=1}^2 q(\\mu_k)\\right)$, derive from first principles the coordinate ascent variational updates for:\n1. The responsibilities $r_{nk} = q(z_n = k)$ for $n \\in \\{1,\\dots,N\\}$ and $k \\in \\{1,2\\}$.\n2. The variational factors $q(\\mu_k)$, including closed-form expressions for their means and precisions.\n\nBase your derivation on the foundational ELBO identity $\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(x,z,\\mu)] - \\mathbb{E}_q[\\log q(z,\\mu)]$ and the optimal mean-field factor update rule $q^*(\\theta_i) \\propto \\exp\\left(\\mathbb{E}_{q(\\theta_{\\setminus i})}[\\log p(x,\\theta)]\\right)$ for each factor. Do not use shortcut formulas beyond those definitions.\n\nAfter deriving the updates, implement a deterministic coordinate ascent algorithm that:\n- Initializes responsibilities uniformly, $r_{nk} = 1/2$ for all $n,k$.\n- Alternates between updating the parameters of $q(\\mu_k)$ and updating the responsibilities $r_{nk}$.\n- Stops when the sum of absolute differences in responsibilities across all $n,k$ between successive iterations is at most a tolerance $T$ or upon reaching a maximum number of iterations.\n\nUse the following test suite to assess label switching implications and a boundary case. For each test, run the algorithm twice if needed and compare the outcomes according to the description. Use tolerance $T = 10^{-8}$ for comparisons of responsibilities.\n\n- Test $1$ (happy path, label invariance under full permutation):\n  - Observations: $x = [\\, -2.2,\\, -1.9,\\, -1.7,\\, 1.5,\\, 1.8,\\, 2.2 \\,]$.\n  - Known variance: $\\sigma^2 = 0.25$.\n  - Mixing weights: $\\pi = [\\, 0.5,\\, 0.5 \\,]$.\n  - Prior means: $m_0 = [\\, -2.0,\\, 2.0 \\,]$.\n  - Prior precisions: $\\tau_0 = [\\, 1.0,\\, 1.0 \\,]$.\n  Run the algorithm to obtain responsibilities $R^{(A)}$. Then run it again with the labels fully permuted, that is mixing weights and prior means swapped: $\\pi' = [\\, 0.5,\\, 0.5 \\,]$, $m_0' = [\\, 2.0,\\, -2.0 \\,]$, $\\tau_0' = [\\, 1.0,\\, 1.0 \\,]$, producing $R^{(B)}$. Check whether $R^{(A)}$ equals $R^{(B)}$ up to a column permutation, within tolerance $T$. The result should be a boolean.\n\n- Test $2$ (edge case, partial permutation that breaks invariance):\n  - Observations: $x = [\\, -2.2,\\, -1.9,\\, -1.7,\\, 1.5,\\, 1.8,\\, 2.2 \\,]$.\n  - Known variance: $\\sigma^2 = 0.25$.\n  - Mixing weights: $\\pi = [\\, 0.7,\\, 0.3 \\,]$.\n  - Prior means: $m_0 = [\\, -2.0,\\, 2.0 \\,]$.\n  - Prior precisions: $\\tau_0 = [\\, 1.0,\\, 1.0 \\,]$.\n  Run the algorithm to obtain responsibilities $R^{(C)}$. Then run it again with prior means swapped but mixing weights kept in the original order (a partial permutation): $m_0' = [\\, 2.0,\\, -2.0 \\,]$, $\\pi' = [\\, 0.7,\\, 0.3 \\,]$, producing $R^{(D)}$. Determine whether $R^{(C)}$ differs from $R^{(D)}$ beyond the tolerance $T$ even after a column permutation. The result should be a boolean indicating non-invariance.\n\n- Test $3$ (boundary condition, indistinguishable components):\n  - Observations: $x = [\\, -1.0,\\, 1.0 \\,]$.\n  - Known variance: $\\sigma^2 = 1.0$.\n  - Mixing weights: $\\pi = [\\, 0.5,\\, 0.5 \\,]$.\n  - Prior means: $m_0 = [\\, 0.0,\\, 0.0 \\,]$.\n  - Prior precisions: $\\tau_0 = [\\, 100.0,\\, 100.0 \\,]$.\n  Run the algorithm once and compute the maximum absolute deviation of the responsibilities from $0.5$ across all data points and components. The result should be a float.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\,\\text{result1},\\text{result2},\\text{result3}\\,]$), where $\\text{result1}$ and $\\text{result2}$ are booleans for Tests $1$ and $2$, and $\\text{result3}$ is a float for Test $3$.",
            "solution": "The user-provided problem is valid. It is a well-posed problem in statistical learning, specifically variational inference, with a clear and consistent model definition, a specific algorithmic task, and verifiable test cases. All necessary data and parameters are provided.\n\nThe problem is to derive and implement a coordinate ascent variational inference (CAVI) algorithm for a two-component Gaussian mixture model (GMM). The derivation will be based on the mean-field approximation and the general form of the optimal variational factor.\n\n### 1. Model Specification\n\nLet the observed data be a set of $N$ scalars $\\{x_n\\}_{n=1}^N$. The latent variables are the component means $\\mu = \\{\\mu_1, \\mu_2\\}$ and the component assignments $z = \\{z_n\\}_{n=1}^N$, where each $z_n \\in \\{1, 2\\}$. For convenience, we represent $z_n$ using a one-hot encoding, where $z_{nk}=1$ if the $n$-th observation is from component $k$, and $z_{nk}=0$ otherwise.\n\nThe generative process is as follows:\n- The prior for each component mean is an independent Gaussian:\n$$p(\\mu_k) = \\mathcal{N}(\\mu_k | m_{0k}, \\tau_{0k}^{-1})$$\nwhere $m_{0k}$ is the prior mean and $\\tau_{0k} > 0$ is the prior precision.\n- The latent component indicators $z_n$ are drawn from a categorical distribution with known mixing weights $\\pi = (\\pi_1, \\pi_2)$:\n$$p(z_n | \\pi) = \\prod_{k=1}^2 \\pi_k^{z_{nk}}$$\n- Each observation $x_n$ is drawn from a Gaussian distribution conditioned on its assigned component $k$ and the corresponding mean $\\mu_k$, with a known variance $\\sigma^2$:\n$$p(x_n | z_n, \\mu) = \\prod_{k=1}^2 \\mathcal{N}(x_n | \\mu_k, \\sigma^2)^{z_{nk}}$$\n\nThe full joint probability distribution over all variables (observed and latent) is given by:\n$$p(x, z, \\mu) = p(\\mu) p(z | \\pi) p(x | z, \\mu) = \\left(\\prod_{k=1}^2 p(\\mu_k)\\right) \\left(\\prod_{n=1}^N p(z_n | \\pi)\\right) \\left(\\prod_{n=1}^N p(x_n | z_n, \\mu)\\right)$$\nThe logarithm of the joint distribution is:\n$$\\log p(x, z, \\mu) = \\sum_{k=1}^2 \\log p(\\mu_k) + \\sum_{n=1}^N \\sum_{k=1}^2 z_{nk} \\log \\pi_k + \\sum_{n=1}^N \\sum_{k=1}^2 z_{nk} \\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)$$\n\n### 2. Variational Inference Setup\n\nWe use a mean-field variational family to approximate the true posterior $p(z, \\mu | x)$. The variational distribution $q(z, \\mu)$ factorizes as:\n$$q(z, \\mu) = q(z)q(\\mu) = \\left(\\prod_{n=1}^N q(z_n)\\right) \\left(\\prod_{k=1}^2 q(\\mu_k)\\right)$$\nwhere each $q(z_n)$ is a categorical distribution over $K=2$ components, characterized by probabilities $r_{nk} = q(z_n=k)$, and each $q(\\mu_k)$ is a distribution over the mean of component $k$.\n\nThe coordinate ascent algorithm iteratively optimizes each factor $q(\\theta_i)$ while holding the others fixed. The optimal form for a factor $q^*(\\theta_i)$ is given by:\n$$\\log q^*(\\theta_i) = \\mathbb{E}_{q(\\theta_{\\setminus i})}[\\log p(x, \\theta)] + \\mathrm{constant}$$\nwhere $\\theta_i$ is one of the latent variables ($z_n$ or $\\mu_k$) and $\\theta_{\\setminus i}$ denotes all other latent variables.\n\n### 3. Derivation of Update for $q(\\mu_k)$\n\nTo find the optimal form for $q(\\mu_k)$, we apply the general update rule. The logarithm of the optimal distribution $q^*(\\mu_k)$ is proportional to the expectation of the log-joint probability with respect to all other factors, $q(z)$ and $q(\\mu_{j \\neq k})$:\n$$\\log q^*(\\mu_k) = \\mathbb{E}_{q(z)}[\\log p(x, z, \\mu)] + \\mathrm{constant}$$\nWe only need to consider terms in $\\log p(x, z, \\mu)$ that depend on $\\mu_k$:\n$$\\log q^*(\\mu_k) = \\mathbb{E}_{q(z)}\\left[\\sum_{n=1}^N z_{nk} \\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)\\right] + \\log \\mathcal{N}(\\mu_k | m_{0k}, \\tau_{0k}^{-1}) + \\mathrm{const}$$\nThe expectation $\\mathbb{E}_{q(z)}[z_{nk}]$ is the responsibility $r_{nk}$. Let $\\tau = 1/\\sigma^2$ be the known data precision.\n$$\\log q^*(\\mu_k) = \\sum_{n=1}^N r_{nk} \\left(-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{\\tau}{2}(x_n - \\mu_k)^2\\right) + \\left(-\\frac{1}{2} \\log(2\\pi\\tau_{0k}^{-1}) - \\frac{\\tau_{0k}}{2}(\\mu_k - m_{0k})^2\\right) + \\mathrm{const}$$\nTo identify the form of $q^*(\\mu_k)$, we collect terms involving $\\mu_k$:\n$$\\log q^*(\\mu_k) = -\\frac{\\tau}{2} \\sum_{n=1}^N r_{nk}(x_n^2 - 2x_n\\mu_k + \\mu_k^2) - \\frac{\\tau_{0k}}{2}(\\mu_k^2 - 2\\mu_k m_{0k} + m_{0k}^2) + \\mathrm{const}$$\nGrouping terms by powers of $\\mu_k$:\n- Term with $\\mu_k^2$: $-\\frac{1}{2}\\mu_k^2 \\left(\\tau_{0k} + \\tau \\sum_{n=1}^N r_{nk}\\right)$\n- Term with $\\mu_k$: $\\mu_k \\left(\\tau_{0k}m_{0k} + \\tau \\sum_{n=1}^N r_{nk}x_n\\right)$\nThis is a quadratic form in $\\mu_k$, which is characteristic of the logarithm of a Gaussian density. A Gaussian $\\mathcal{N}(\\mu | m, \\lambda^{-1})$ has a log-density of the form $-\\frac{\\lambda}{2}\\mu^2 + \\lambda m \\mu + \\mathrm{const}$.\nBy matching coefficients, we find that $q^*(\\mu_k)$ is a Gaussian distribution $\\mathcal{N}(\\mu_k | m_k, \\tau_k^{-1})$ with precision $\\tau_k$ and mean $m_k$:\n$$\\tau_k = \\tau_{0k} + \\tau \\sum_{n=1}^N r_{nk}$$\n$$m_k = \\frac{\\tau_{0k}m_{0k} + \\tau \\sum_{n=1}^N r_{nk}x_n}{\\tau_k}$$\nThese are the update equations for the parameters of the variational distribution $q(\\mu_k)$.\n\n### 4. Derivation of Update for $q(z_n)$\n\nSimilarly, we find the optimal form for $q(z_n)$ by taking the expectation of the log-joint with respect to all other factors, $\\{q(z_j)\\}_{j \\neq n}$ and $\\{q(\\mu_k)\\}_{k=1}^2$:\n$$\\log q^*(z_n) = \\mathbb{E}_{q(\\mu)}[\\log p(x, z, \\mu)] + \\mathrm{constant}$$\nWe collect terms that depend on $z_n$:\n$$\\log q^*(z_n) = \\sum_{k=1}^2 z_{nk} \\log \\pi_k + \\sum_{k=1}^2 z_{nk} \\mathbb{E}_{q(\\mu_k)}[\\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)] + \\mathrm{const}$$\nThis implies that $q^*(z_n)$ is a categorical distribution. The log-probability for component $k$ is:\n$$\\log q^*(z_n=k) \\equiv \\log r_{nk} \\propto \\log \\pi_k + \\mathbb{E}_{q(\\mu_k)}[\\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)]$$\nLet's expand the expectation term:\n$$\\mathbb{E}_{q(\\mu_k)}[\\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)] = \\mathbb{E}_{q(\\mu_k)}\\left[-\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{\\tau}{2}(x_n - \\mu_k)^2\\right]$$\n$$= -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{\\tau}{2} \\mathbb{E}_{q(\\mu_k)}[x_n^2 - 2x_n\\mu_k + \\mu_k^2]$$\n$$= -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{\\tau}{2} (x_n^2 - 2x_n\\mathbb{E}[\\mu_k] + \\mathbb{E}[\\mu_k^2])$$\nFrom our derived form of $q(\\mu_k) = \\mathcal{N}(\\mu_k | m_k, \\tau_k^{-1})$, we have:\n$$\\mathbb{E}[\\mu_k] = m_k$$\n$$\\mathbb{E}[\\mu_k^2] = \\mathrm{Var}[\\mu_k] + (\\mathbb{E}[\\mu_k])^2 = \\tau_k^{-1} + m_k^2$$\nSubstituting these into the expression for $\\log q^*(z_n=k)$ and dropping terms constant with respect to $k$ (like $-\\frac{1}{2}\\log(2\\pi\\sigma^2)$ and $-\\frac{\\tau}{2}x_n^2$):\n$$\\log \\tilde{\\rho}_{nk} \\propto \\log \\pi_k + \\tau x_n \\mathbb{E}[\\mu_k] - \\frac{\\tau}{2} \\mathbb{E}[\\mu_k^2]$$\n$$\\log \\tilde{\\rho}_{nk} = \\log \\pi_k + \\tau x_n m_k - \\frac{\\tau}{2}(m_k^2 + \\tau_k^{-1})$$\nThe responsibilities $r_{nk} = q(z_n=k)$ are obtained by normalizing the exponentiated values:\n$$r_{nk} = \\frac{\\exp(\\log \\tilde{\\rho}_{nk})}{\\sum_{j=1}^2 \\exp(\\log \\tilde{\\rho}_{nj})}$$\n\n### 5. Algorithm Summary\n\nThe coordinate ascent variational inference (CAVI) algorithm proceeds as follows:\n1.  **Initialize**: Initialize responsibilities $r_{nk}$ (e.g., uniformly as $r_{nk} = 1/2$).\n2.  **Iterate** until convergence:\n    a. **Update for $q(\\mu)$ (M-like step)**: For each component $k=1,2$, update the parameters $m_k$ and $\\tau_k$ of the variational distribution $q(\\mu_k)$ using the current responsibilities $r_{nk}$:\n       $$\\tau_k \\leftarrow \\tau_{0k} + \\tau \\sum_{n=1}^N r_{nk}$$\n       $$m_k \\leftarrow \\frac{\\tau_{0k}m_{0k} + \\tau \\sum_{n=1}^N r_{nk}x_n}{\\tau_k}$$\n    b. **Update for $q(z)$ (E-like step)**: For each data point $n=1,\\dots,N$, update the responsibilities $r_{nk}$ using the updated parameters of $q(\\mu)$:\n       $$\\log \\tilde{\\rho}_{nk} \\leftarrow \\log \\pi_k + \\tau x_n m_k - \\frac{\\tau}{2}(m_k^2 + \\tau_k^{-1})$$\n       $$r_{nk} \\leftarrow \\frac{\\exp(\\log \\tilde{\\rho}_{nk})}{\\sum_{j=1}^2 \\exp(\\log \\tilde{\\rho}_{nj})}$$\n    c. **Check for convergence**: Stop if the change in responsibilities is below a tolerance $T$ or a maximum number of iterations is reached.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the variational inference problem for the provided test cases.\n    \"\"\"\n\n    def run_cavi(x, sigma_sq, pi, m0, tau0, tol, max_iter=100):\n        \"\"\"\n        Runs the Coordinate Ascent Variational Inference algorithm for a GMM.\n\n        Args:\n            x (np.ndarray): 1D array of observations.\n            sigma_sq (float): Known variance of the Gaussian components.\n            pi (np.ndarray): 1D array of mixing weights.\n            m0 (np.ndarray): 1D array of prior means for component means.\n            tau0 (np.ndarray): 1D array of prior precisions for component means.\n            tol (float): Convergence tolerance for responsibilities.\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            np.ndarray: A (N, K) array of final responsibilities.\n        \"\"\"\n        N = x.shape[0]\n        K = pi.shape[0]\n        tau = 1.0 / sigma_sq\n\n        # Initialize responsibilities uniformly\n        r_nk = np.full((N, K), 1.0 / K)\n\n        for i in range(max_iter):\n            r_nk_old = r_nk.copy()\n\n            # M-step: Update q(mu_k)\n            # Sum of responsibilities for each component\n            N_k = np.sum(r_nk, axis=0) # Shape (K,)\n            # Update variational precision for mu_k\n            tau_k = tau0 + tau * N_k # Shape (K,)\n            # Update variational mean for mu_k\n            # r_nk.T is (K,N), x is (N,). (r_nk.T @ x) is sum(r_nk * x) for each k\n            sum_r_x = r_nk.T @ x # Shape (K,)\n            m_k = (tau0 * m0 + tau * sum_r_x) / tau_k # Shape (K,)\n\n            # E-step: Update q(z_n), i.e., the responsibilities r_nk\n            E_mu_k_sq = 1.0 / tau_k + m_k**2 # Shape (K,)\n            \n            # Use broadcasting for efficiency. x[:, np.newaxis] is (N,1)\n            # m_k and E_mu_k_sq are (K,) which broadcasts to (N,K)\n            log_rho_nk = np.log(pi) + tau * x[:, np.newaxis] * m_k - (tau / 2) * E_mu_k_sq\n            \n            # Log-sum-exp trick for numerical stability\n            log_rho_nk_max = np.max(log_rho_nk, axis=1, keepdims=True)\n            log_rho_nk_stable = log_rho_nk - log_rho_nk_max\n            rho_nk = np.exp(log_rho_nk_stable)\n            \n            # Normalize to get responsibilities\n            r_nk = rho_nk / np.sum(rho_nk, axis=1, keepdims=True)\n\n            # Check for convergence\n            diff = np.sum(np.abs(r_nk - r_nk_old))\n            if diff < tol:\n                break\n        \n        return r_nk\n\n    T = 10**-8\n    results = []\n\n    # Test 1: Happy path, label invariance under full permutation\n    x1 = np.array([-2.2, -1.9, -1.7, 1.5, 1.8, 2.2])\n    sigma_sq1 = 0.25\n    pi_A = np.array([0.5, 0.5])\n    m0_A = np.array([-2.0, 2.0])\n    tau0_1 = np.array([1.0, 1.0])\n    R_A = run_cavi(x1, sigma_sq1, pi_A, m0_A, tau0_1, T)\n\n    pi_B = np.array([0.5, 0.5]) # Same pi\n    m0_B = np.array([2.0, -2.0]) # Swapped means\n    tau0_B = np.array([1.0, 1.0])\n    R_B = run_cavi(x1, sigma_sq1, pi_B, m0_B, tau0_B, T)\n\n    # Check for equality up to column permutation (label switching)\n    is_invariant = np.allclose(R_A, R_B, atol=T) or np.allclose(R_A, R_B[:, ::-1], atol=T)\n    results.append(is_invariant)\n\n    # Test 2: Edge case, partial permutation that breaks invariance\n    x2 = np.array([-2.2, -1.9, -1.7, 1.5, 1.8, 2.2])\n    sigma_sq2 = 0.25\n    pi_C = np.array([0.7, 0.3])\n    m0_C = np.array([-2.0, 2.0])\n    tau0_2 = np.array([1.0, 1.0])\n    R_C = run_cavi(x2, sigma_sq2, pi_C, m0_C, tau0_2, T)\n    \n    pi_D = np.array([0.7, 0.3]) # Unswapped pi\n    m0_D = np.array([2.0, -2.0]) # Swapped means\n    tau0_D = np.array([1.0, 1.0])\n    R_D = run_cavi(x2, sigma_sq2, pi_D, m0_D, tau0_D, T)\n\n    # Check if results are different even after accounting for label switching\n    is_non_invariant = not (np.allclose(R_C, R_D, atol=T) or np.allclose(R_C, R_D[:, ::-1], atol=T))\n    results.append(is_non_invariant)\n\n    # Test 3: Boundary condition, indistinguishable components\n    x3 = np.array([-1.0, 1.0])\n    sigma_sq3 = 1.0\n    pi_3 = np.array([0.5, 0.5])\n    m0_3 = np.array([0.0, 0.0])\n    tau0_3 = np.array([100.0, 100.0])\n    R_3 = run_cavi(x3, sigma_sq3, pi_3, m0_3, tau0_3, T)\n    max_dev = np.max(np.abs(R_3 - 0.5))\n    results.append(max_dev)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While CAVI is powerful, its requirement to process the entire dataset at each iteration makes it slow for modern, large-scale problems. This final practice introduces Stochastic Variational Inference (SVI), a scalable approach that uses noisy but computationally cheap gradients from data minibatches. You will investigate the fundamental trade-off at the heart of SVI—balancing gradient variance against computational cost—by deriving an adaptive strategy for selecting the minibatch size .",
            "id": "3192033",
            "problem": "Consider a univariate normal-normal model in statistical learning where observations are independent and identically distributed. The observed data are $\\{y_i\\}_{i=1}^N$ with $y_i \\mid \\mu \\sim \\mathcal{N}(\\mu,\\sigma^2)$ for a known variance $\\sigma^2$, and the prior is $\\mu \\sim \\mathcal{N}(\\mu_0,\\tau_0^2)$. Let the variational family be $q(\\mu)=\\mathcal{N}(m,v)$ with variational parameters $m$ and $v$. The optimization objective is the Evidence Lower Bound (ELBO), denoted by $\\mathcal{L}(m,v)$, and we use Stochastic Variational Inference (SVI) to perform gradient ascent on $\\mathcal{L}(m,v)$.\n\nStarting from the fundamental definition of the ELBO,\n$$\n\\mathcal{L}(m,v) = \\mathbb{E}_{q(\\mu)}[\\log p(y,\\mu)] - \\mathbb{E}_{q(\\mu)}[\\log q(\\mu)],\n$$\nwhere $p(y,\\mu)=p(y\\mid\\mu)p(\\mu)$ and $\\log p(y\\mid\\mu) = -\\frac{N}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i-\\mu)^2$, and $\\log p(\\mu) = -\\frac{1}{2}\\log(2\\pi\\tau_0^2) - \\frac{1}{2\\tau_0^2}(\\mu-\\mu_0)^2$, derive the gradient with respect to the variational mean $m$, denoted $\\nabla_m \\mathcal{L}(m,v)$, relying only on basic properties of expectations of normal random variables and linearity of differentiation under the expectation sign. Do not provide or use any shortcut formulas beyond these foundations in your derivation.\n\nDefine a minibatch of size $b$ by sampling indices uniformly at random with replacement from $\\{1,\\dots,N\\}$. Construct an unbiased estimator of $\\nabla_m \\mathcal{L}(m,v)$ using such a minibatch. Then, starting from the unbiasedness requirement and independence of sampled indices, derive the variance of this minibatch-based gradient estimator as a function of $b$, assuming variance is generated only by the data-likelihood term and the prior contribution is deterministic with respect to minibatch sampling.\n\nTo investigate the trade-off between convergence and computational cost, let the wall-clock time per iteration be modeled by a linear function $t(b)=t_0+t_1 b$ with constants $t_0>0$ and $t_1>0$. Define the signal-to-noise ratio (SNR) for the minibatch gradient estimator as\n$$\n\\mathrm{SNR}(b)=\\frac{\\left(\\mathbb{E}[\\hat{g}_b]\\right)^2}{\\mathrm{Var}(\\hat{g}_b)},\n$$\nwhere $\\hat{g}_b$ is the unbiased estimator of $\\nabla_m \\mathcal{L}(m,v)$ at a current parameter $m$. Propose an adaptive batching strategy that, at a given iterate $m$, selects the smallest batch size $b$ that achieves a user-specified SNR threshold $\\rho>0$, subject to the constraint $1 \\le b \\le N$.\n\nYour task is to implement a complete, runnable program that:\n- Generates synthetic data for each test case by drawing $y_i \\sim \\mathcal{N}(\\mu_{\\mathrm{true}},\\sigma^2)$ independently for $i=1,\\dots,N$, using the provided random seed for reproducibility.\n- Computes the exact gradient $\\nabla_m \\mathcal{L}(m,v)$ at the given initial value of $m$ (note that the gradient with respect to $m$ does not depend on $v$ in this model).\n- Computes the per-sample contribution variance of the data-likelihood term to the gradient at the given $m$ across the full dataset.\n- Applies your proposed adaptive batching rule to select $b$ that meets the SNR threshold $\\rho$ and clips the result to the interval $[1,N]$.\n- Reports the selected batch sizes as integers for all test cases.\n\nTest Suite:\n- Case $1$ (happy path): $N=512$, $\\sigma^2=1.0$, $\\mu_0=0.0$, $\\tau_0^2=10.0$, $\\mu_{\\mathrm{true}}=1.0$, $m_{\\mathrm{init}}=-2.0$, $\\rho=5.0$, $t_0=10^{-4}$, $t_1=10^{-6}$, $\\text{seed}=123$.\n- Case $2$ (boundary condition: high SNR near optimum): $N=64$, $\\sigma^2=2.0$, $\\mu_0=0.0$, $\\tau_0^2=5.0$, $\\mu_{\\mathrm{true}}=0.5$, $m_{\\mathrm{init}}=0.45$, $\\rho=50.0$, $t_0=10^{-4}$, $t_1=10^{-6}$, $\\text{seed}=321$.\n- Case $3$ (edge case: large dataset, far from optimum): $N=1024$, $\\sigma^2=0.5$, $\\mu_0=0.0$, $\\tau_0^2=20.0$, $\\mu_{\\mathrm{true}}=-1.5$, $m_{\\mathrm{init}}=3.0$, $\\rho=1.0$, $t_0=10^{-4}$, $t_1=10^{-6}$, $\\text{seed}=999$.\n\nAll quantities have no physical units and should be treated as dimensionless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is the selected adaptive batch size $b$ (an integer) for the corresponding test case, in the order of Cases $1$, $2$, and $3$.",
            "solution": "The user-provided problem is assessed to be valid as it is scientifically grounded, self-contained, well-posed, and objective. It presents a standard problem in statistical learning that can be solved through rigorous mathematical derivation and implementation.\n\nThe solution proceeds in four stages:\n1.  Derivation of the gradient of the Evidence Lower Bound (ELBO) with respect to the variational mean parameter $m$.\n2.  Construction of an unbiased stochastic gradient estimator using minibatches.\n3.  Derivation of the variance of this stochastic estimator.\n4.  Formulation of the adaptive batching rule based on the Signal-to-Noise Ratio (SNR).\n\n### 1. Derivation of the ELBO Gradient $\\nabla_m \\mathcal{L}(m,v)$\n\nThe Evidence Lower Bound (ELBO) for the variational distribution $q(\\mu) = \\mathcal{N}(m,v)$ is given by:\n$$\n\\mathcal{L}(m,v) = \\mathbb{E}_{q(\\mu)}[\\log p(y,\\mu)] - \\mathbb{E}_{q(\\mu)}[\\log q(\\mu)]\n$$\nwhere $y = \\{y_i\\}_{i=1}^N$. We seek to find the gradient with respect to the variational mean, $\\nabla_m \\mathcal{L}(m,v)$.\n\nA key property for a location-family distribution like the Normal distribution $q(\\mu; m, v)$ is that the gradient with respect to the location parameter $m$ can be interchanged with the expectation operator. This can be seen via the reparameterization trick, where $\\mu = m + \\sqrt{v}\\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,1)$. The gradient becomes $\\nabla_m \\mathbb{E}_{\\epsilon}[f(m+\\sqrt{v}\\epsilon)] = \\mathbb{E}_{\\epsilon}[\\nabla_m f(m+\\sqrt{v}\\epsilon)] = \\mathbb{E}_{\\epsilon}[f'(\\mu)|_{\\mu=m+\\sqrt{v}\\epsilon} \\cdot 1] = \\mathbb{E}_{q(\\mu)}[\\nabla_{\\mu}f(\\mu)]$. Applying this property, we get:\n$$\n\\nabla_m \\mathcal{L}(m,v) = \\mathbb{E}_{q(\\mu)}[\\nabla_{\\mu}\\log p(y,\\mu)] - \\mathbb{E}_{q(\\mu)}[\\nabla_{\\mu}\\log q(\\mu)]\n$$\nLet's analyze the second term. The score function of the variational distribution is $\\nabla_{\\mu}\\log q(\\mu)$:\n$$\n\\log q(\\mu) = \\log\\left(\\frac{1}{\\sqrt{2\\pi v}}\\exp\\left(-\\frac{(\\mu-m)^2}{2v}\\right)\\right) = -\\frac{1}{2}\\log(2\\pi v) - \\frac{(\\mu-m)^2}{2v}\n$$\n$$\n\\nabla_{\\mu}\\log q(\\mu) = \\frac{d}{d\\mu}\\left(-\\frac{(\\mu-m)^2}{2v}\\right) = -\\frac{2(\\mu-m)}{2v} = -\\frac{\\mu-m}{v}\n$$\nThe expectation of the score function is:\n$$\n\\mathbb{E}_{q(\\mu)}[\\nabla_{\\mu}\\log q(\\mu)] = \\mathbb{E}_{q(\\mu)}\\left[-\\frac{\\mu-m}{v}\\right] = -\\frac{1}{v}(\\mathbb{E}_{q(\\mu)}[\\mu] - m) = -\\frac{1}{v}(m - m) = 0\n$$\nThis is a general result: the expected score of a distribution with respect to its own parameters is zero. Thus, the ELBO gradient simplifies to:\n$$\n\\nabla_m \\mathcal{L}(m,v) = \\mathbb{E}_{q(\\mu)}[\\nabla_{\\mu}\\log p(y,\\mu)]\n$$\nThe joint log-probability is $\\log p(y,\\mu) = \\log p(y|\\mu) + \\log p(\\mu)$. We differentiate each part with respect to $\\mu$:\n$$\n\\log p(y|\\mu) = \\sum_{i=1}^N \\log p(y_i|\\mu) = \\sum_{i=1}^N \\left(-\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y_i-\\mu)^2}{2\\sigma^2}\\right)\n$$\n$$\n\\nabla_{\\mu} \\log p(y|\\mu) = \\sum_{i=1}^N \\frac{d}{d\\mu} \\left(-\\frac{(y_i-\\mu)^2}{2\\sigma^2}\\right) = \\sum_{i=1}^N \\frac{2(y_i-\\mu)}{2\\sigma^2} = \\frac{1}{\\sigma^2}\\sum_{i=1}^N(y_i-\\mu)\n$$\nAnd for the prior:\n$$\n\\log p(\\mu) = -\\frac{1}{2}\\log(2\\pi\\tau_0^2) - \\frac{(\\mu-\\mu_0)^2}{2\\tau_0^2}\n$$\n$$\n\\nabla_{\\mu} \\log p(\\mu) = \\frac{d}{d\\mu} \\left(-\\frac{(\\mu-\\mu_0)^2}{2\\tau_0^2}\\right) = -\\frac{2(\\mu-\\mu_0)}{2\\tau_0^2} = -\\frac{\\mu-\\mu_0}{\\tau_0^2}\n$$\nCombining these and taking the expectation with respect to $q(\\mu)$, where $\\mathbb{E}_{q(\\mu)}[\\mu] = m$:\n$$\n\\nabla_m \\mathcal{L}(m,v) = \\mathbb{E}_{q(\\mu)}\\left[\\frac{1}{\\sigma^2}\\sum_{i=1}^N(y_i-\\mu) - \\frac{\\mu-\\mu_0}{\\tau_0^2}\\right]\n$$\nBy linearity of expectation:\n$$\n\\nabla_m \\mathcal{L}(m,v) = \\frac{1}{\\sigma^2}\\sum_{i=1}^N(y_i-\\mathbb{E}_{q(\\mu)}[\\mu]) - \\frac{\\mathbb{E}_{q(\\mu)}[\\mu]-\\mu_0}{\\tau_0^2}\n$$\n$$\n\\nabla_m \\mathcal{L}(m,v) = \\frac{1}{\\sigma^2}\\left(\\sum_{i=1}^N y_i - Nm\\right) + \\frac{\\mu_0-m}{\\tau_0^2}\n$$\nAs noted in the problem, this gradient, which we denote $g_m$, does not depend on the variational variance $v$.\n\n### 2. Stochastic Gradient Estimator\n\nThe full gradient $g_m$ can be written as the sum of a data-dependent term and a prior term:\n$$\ng_m = \\left(\\sum_{i=1}^N \\frac{y_i-m}{\\sigma^2}\\right) + \\left(\\frac{\\mu_0-m}{\\tau_0^2}\\right)\n$$\nIn Stochastic Variational Inference (SVI), the sum over the entire dataset is approximated. Let a minibatch $S$ be a set of $b$ indices drawn uniformly at random with replacement from $\\{1, \\dots, N\\}$. An unbiased estimator for the sum $\\sum_{i=1}^N f(y_i)$ is $\\frac{N}{b}\\sum_{j \\in S} f(y_j)$. Applying this to the data-dependent part of the gradient, we construct the stochastic gradient estimator $\\hat{g}_b$:\n$$\n\\hat{g}_b = \\frac{N}{b}\\sum_{j \\in S} \\frac{y_j-m}{\\sigma^2} + \\frac{\\mu_0-m}{\\tau_0^2}\n$$\nThis estimator is unbiased because the expectation over the random sampling of the minibatch $S$ is:\n$$\n\\mathbb{E}_{S}[\\hat{g}_b] = \\mathbb{E}_{S}\\left[\\frac{N}{b}\\sum_{j \\in S} \\frac{y_j-m}{\\sigma^2}\\right] + \\frac{\\mu_0-m}{\\tau_0^2} = \\frac{N}{b} \\sum_{k=1}^b \\mathbb{E}_{j_k}\\left[\\frac{y_{j_k}-m}{\\sigma^2}\\right] + \\frac{\\mu_0-m}{\\tau_0^2}\n$$\nwhere $j_k$ are the i.i.d. indices in the minibatch. The expectation for a single draw is $\\mathbb{E}_{j_k}[\\dots] = \\frac{1}{N}\\sum_{i=1}^N \\frac{y_i-m}{\\sigma^2}$.\n$$\n\\mathbb{E}_{S}[\\hat{g}_b] = \\frac{N}{b} \\cdot b \\cdot \\left(\\frac{1}{N}\\sum_{i=1}^N \\frac{y_i-m}{\\sigma^2}\\right) + \\frac{\\mu_0-m}{\\tau_0^2} = g_m\n$$\n\n### 3. Variance of the Stochastic Estimator\n\nThe variance of $\\hat{g}_b$ arises from the random sampling of the minibatch. The prior term is deterministic with respect to this sampling.\n$$\n\\mathrm{Var}(\\hat{g}_b) = \\mathrm{Var}\\left(\\frac{N}{b}\\sum_{j \\in S} \\frac{y_j-m}{\\sigma^2}\\right) = \\left(\\frac{N}{b}\\right)^2 \\mathrm{Var}\\left(\\sum_{j \\in S} \\frac{y_j-m}{\\sigma^2}\\right)\n$$\nSince the indices in $S$ are drawn i.i.d., the variance of the sum is the sum of variances:\n$$\n\\mathrm{Var}\\left(\\sum_{j \\in S} \\frac{y_j-m}{\\sigma^2}\\right) = b \\cdot \\mathrm{Var}_{\\text{single sample}}\\left(\\frac{y_j-m}{\\sigma^2}\\right)\n$$\nLet's define $h_i = \\frac{y_i-m}{\\sigma^2}$ as the gradient contribution from a single data point. The variance of a single draw from the set $\\{h_1, \\dots, h_N\\}$ is the population variance of these values, which we denote $V_h$:\n$$\nV_h = \\mathrm{Var}_{j \\sim \\mathrm{Unif}(\\{1..N\\})}(h_j) = \\frac{1}{N}\\sum_{i=1}^N h_i^2 - \\left(\\frac{1}{N}\\sum_{i=1}^N h_i\\right)^2\n$$\nCombining these results, the variance of the stochastic gradient estimator is:\n$$\n\\mathrm{Var}(\\hat{g}_b) = \\left(\\frac{N}{b}\\right)^2 (b \\cdot V_h) = \\frac{N^2}{b}V_h\n$$\n\n### 4. Adaptive Batching Strategy\n\nThe Signal-to-Noise Ratio (SNR) for the estimator $\\hat{g}_b$ is defined as:\n$$\n\\mathrm{SNR}(b) = \\frac{(\\mathbb{E}[\\hat{g}_b])^2}{\\mathrm{Var}(\\hat{g}_b)} = \\frac{g_m^2}{\\frac{N^2}{b}V_h} = \\frac{b g_m^2}{N^2 V_h}\n$$\nThe adaptive batching rule requires finding the smallest integer batch size $b$ such that $\\mathrm{SNR}(b) \\ge \\rho$ for a given threshold $\\rho > 0$, subject to $1 \\le b \\le N$.\nWe set up the inequality:\n$$\n\\frac{b g_m^2}{N^2 V_h} \\ge \\rho\n$$\nAssuming $g_m^2 > 0$ and $V_h > 0$, we solve for $b$:\n$$\nb \\ge \\rho \\frac{N^2 V_h}{g_m^2}\n$$\nThe smallest integer $b$ satisfying this condition is found by taking the ceiling of the right-hand side. Let $b_{req} = \\rho \\frac{N^2 V_h}{g_m^2}$. Then the smallest integer is $\\lceil b_{req} \\rceil$.\nFinally, we apply the constraints $1 \\le b \\le N$ by clipping the result:\n$$\nb_{\\text{adaptive}} = \\min(N, \\max(1, \\lceil b_{req} \\rceil))\n$$\nThis formula provides the batch size to be implemented. If $g_m=0$, the SNR is $0$ (for $V_h>0$), so any positive $\\rho$ cannot be satisfied. In this case, $b_{req}$ is infinite, and clipping to $N$ provides the largest possible batch size in an attempt to achieve the highest possible SNR. If $V_h=0$, all per-sample gradient contributions are identical, noise is zero, and $b=1$ is sufficient.\n\nThe implementation will compute $g_m$ and $V_h$ from the data and parameters, then use the formula for $b_{\\text{adaptive}}$ to determine the batch size for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # The parameters t_0 and t_1 are specified in the problem but are not needed\n    # for the adaptive batch size calculation, which is based solely on the SNR threshold rho.\n    test_cases = [\n        # (N, sigma^2, mu_0, tau_0^2, mu_true, m_init, rho, seed)\n        (512, 1.0, 0.0, 10.0, 1.0, -2.0, 5.0, 123),  # Case 1\n        (64, 2.0, 0.0, 5.0, 0.5, 0.45, 50.0, 321),    # Case 2\n        (1024, 0.5, 0.0, 20.0, -1.5, 3.0, 1.0, 999),  # Case 3\n    ]\n\n    results = []\n    for case in test_cases:\n        N, sigma2, mu0, tau02, mu_true, m_init, rho, seed = case\n        result = _calculate_adaptive_batch_size(N, sigma2, mu0, tau02, mu_true, m_init, rho, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _calculate_adaptive_batch_size(N, sigma2, mu0, tau02, mu_true, m_init, rho, seed):\n    \"\"\"\n    Calculates the adaptive batch size for a single test case.\n\n    Args:\n        N (int): Number of data points.\n        sigma2 (float): Known variance of the likelihood.\n        mu0 (float): Mean of the prior.\n        tau02 (float): Variance of the prior.\n        mu_true (float): True mean for data generation.\n        m_init (float): Initial value for the variational mean parameter m.\n        rho (float): Target Signal-to-Noise Ratio (SNR).\n        seed (int): Random seed for data generation.\n\n    Returns:\n        int: The computed adaptive batch size, clipped to [1, N].\n    \"\"\"\n    # 1. Generate synthetic data using the provided random seed for reproducibility.\n    rng = np.random.default_rng(seed)\n    sigma = np.sqrt(sigma2)\n    y = rng.normal(loc=mu_true, scale=sigma, size=N)\n\n    # 2. Compute the per-sample gradient contributions from the data-likelihood term.\n    # h_i = (y_i - m) / sigma^2\n    h = (y - m_init) / sigma2\n\n    # 3. Compute the exact full gradient of the ELBO w.r.t. m.\n    # g_m = sum(h_i) + (mu_0 - m) / tau_0^2\n    grad_likelihood_term = np.sum(h)\n    grad_prior_term = (mu0 - m_init) / tau02\n    g_m = grad_likelihood_term + grad_prior_term\n\n    # 4. Compute the population variance of the per-sample gradient contributions.\n    # V_h = Var({h_i})\n    # np.var calculates the population variance by default (ddof=0).\n    V_h = np.var(h)\n\n    # 5. Calculate the required batch size b_req to meet the SNR threshold rho.\n    # b_req = rho * (N^2 * V_h) / g_m^2\n    g_m_squared = g_m**2\n    if g_m_squared < 1e-12:  # Handle numerical instability for near-zero gradients.\n        # If the gradient is zero, SNR is zero (for V_h > 0). Any rho > 0 is\n        # unachievable. We select the max batch size to get the best possible SNR.\n        b_req = np.inf\n    elif V_h == 0:\n        # If V_h is zero, the gradient estimator has zero variance.\n        # The SNR is infinite. The smallest batch size is sufficient.\n        b_req = 0.0\n    else:\n        b_req = rho * (N**2 * V_h) / g_m_squared\n\n    # 6. The smallest integer batch size is the ceiling of b_req.\n    b_adaptive = math.ceil(b_req)\n\n    # 7. Clip the result to the valid range [1, N] as per the problem constraints.\n    b_final = int(np.clip(b_adaptive, 1, N))\n\n    return b_final\n\nsolve()\n```"
        }
    ]
}