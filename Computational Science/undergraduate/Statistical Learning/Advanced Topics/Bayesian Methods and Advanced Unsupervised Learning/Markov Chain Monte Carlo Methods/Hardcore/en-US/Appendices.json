{
    "hands_on_practices": [
        {
            "introduction": "The Metropolis-Hastings algorithm is a cornerstone of MCMC methods. Its ingenuity lies in a simple probabilistic rule for deciding whether to accept or reject a proposed move, which guarantees that the chain will eventually explore the target probability distribution. This first exercise focuses directly on this core mechanism . By calculating the acceptance probability $\\alpha$ for a specific move, you will gain a concrete understanding of how the algorithm balances exploration with the need to favor regions of higher probability.",
            "id": "1371728",
            "problem": "A data scientist is implementing a Markov Chain Monte Carlo (MCMC) simulation to draw samples from a posterior probability distribution for a parameter $x$. The target distribution, $\\pi(x)$, is proportional to the exponential of the negative absolute value of the parameter, such that $\\pi(x) \\propto \\exp(-|x|)$.\n\nThe scientist uses the Metropolis algorithm with a symmetric proposal distribution $q(x'|x)$, where the probability of proposing a new state $x'$ given the current state $x$ is equal to the probability of proposing $x$ given $x'$ (i.e., $q(x'|x) = q(x|x')$).\n\nSuppose that at a certain step in the simulation, the current state of the chain is $x = 1.5$. The algorithm then proposes a move to a new candidate state $x' = 2.0$.\n\nCalculate the acceptance probability for this specific move. Your answer should be a dimensionless real number. Round your final answer to four significant figures.",
            "solution": "The Metropolis acceptance probability for a move from $x$ to $x'$ with a symmetric proposal $q(x'|x)=q(x|x')$ is\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\nGiven the target distribution $\\pi(x)\\propto \\exp(-|x|)$, the ratio simplifies to\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\nWith $x=1.5$ and $x'=2.0$, we have $|x|=1.5$ and $|x'|=2.0$, so\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\nTherefore,\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\nNumerically, $\\exp(-0.5)\\approx 0.6065$ when rounded to four significant figures.",
            "answer": "$$\\boxed{0.6065}$$"
        },
        {
            "introduction": "While the Metropolis algorithm provides a general framework for MCMC, the Gibbs sampler offers a powerful alternative, especially for multivariate distributions. The key to Gibbs sampling is not an acceptance-rejection step, but rather the ability to sample each variable from its full conditional distribution—its distribution given the current values of all other variables. This practice focuses on this essential prerequisite . By deriving the conditional distributions for a simple bivariate target, you'll master the foundational skill needed to construct a Gibbs sampler for more complex models.",
            "id": "1932854",
            "problem": "A Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for generating a sequence of observations that approximate a target multivariate probability distribution, which is especially useful when direct sampling is difficult. A crucial step in implementing a Gibbs sampler is to derive the full conditional distribution for each variable in the model.\n\nConsider a bivariate random vector $(X, Y)$ with a joint probability density function (PDF), $p(x, y)$, that is uniform over a triangular region $\\mathcal{R}$ in the $xy$-plane. The region $\\mathcal{R}$ is defined by the inequalities $x>0$, $y>0$, and $x+y < 1$. The joint PDF is therefore:\n$$\np(x, y) =\n\\begin{cases}\n2 & \\text{if } (x,y) \\in \\mathcal{R} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nTo construct a Gibbs sampler for this distribution, we need the full conditional distributions, $p(x|y)$ and $p(y|x)$.\n\nWhich of the following options correctly identifies these two conditional distributions? Note that $\\text{Uniform}(a,b)$ denotes the uniform distribution on the interval $(a,b)$.\n\nA: $p(x|y)$ is Uniform$(0, 1-y)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(0, 1-x)$ for $x \\in (0,1)$.\n\nB: $p(x|y)$ is Uniform$(0, 1)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(0, 1)$ for $x \\in (0,1)$.\n\nC: $p(x|y)$ is Uniform$(0, y)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(0, x)$ for $x \\in (0,1)$.\n\nD: The conditional distributions are not uniform.\n\nE: $p(x|y)$ is Uniform$(y, 1)$ for $y \\in (0,1)$, and $p(y|x)$ is Uniform$(x, 1)$ for $x \\in (0,1)$.",
            "solution": "We are given a joint density $p(x,y)$ that is uniform over the region $\\mathcal{R}=\\{(x,y): x>0,\\ y>0,\\ x+y<1\\}$ with $p(x,y)=2$ on $\\mathcal{R}$ and $0$ otherwise. To obtain the full conditional distributions, we use the definition of conditionals via the marginals.\n\nFirst, compute the marginal density of $Y$. For a fixed $y$, the admissible $x$ values are those satisfying $0<x<1-y$, provided $0<y<1$. Therefore,\n$$\np_{Y}(y)=\\int_{-\\infty}^{\\infty} p(x,y)\\,dx=\\int_{0}^{1-y} 2\\,dx=2(1-y), \\quad \\text{for } 0<y<1,\n$$\nand $p_{Y}(y)=0$ otherwise.\n\nThen the conditional density of $X$ given $Y=y$ is\n$$\np_{X\\mid Y}(x\\mid y)=\\frac{p(x,y)}{p_{Y}(y)}=\\frac{2}{2(1-y)}=\\frac{1}{1-y},\n$$\nvalid exactly on the support where $p(x,y)>0$, namely $0<x<1-y$ (and $0<y<1$). This is the density of a Uniform distribution on $(0,1-y)$:\n$$\nX\\mid Y=y \\sim \\text{Uniform}(0,1-y), \\quad \\text{for } 0<y<1.\n$$\n\nBy symmetry, compute the marginal of $X$:\n$$\np_{X}(x)=\\int_{-\\infty}^{\\infty} p(x,y)\\,dy=\\int_{0}^{1-x} 2\\,dy=2(1-x), \\quad \\text{for } 0<x<1,\n$$\nand $p_{X}(x)=0$ otherwise.\n\nThen the conditional density of $Y$ given $X=x$ is\n$$\np_{Y\\mid X}(y\\mid x)=\\frac{p(x,y)}{p_{X}(x)}=\\frac{2}{2(1-x)}=\\frac{1}{1-x},\n$$\nvalid on $0<y<1-x$ (and $0<x<1$). This is Uniform on $(0,1-x)$:\n$$\nY\\mid X=x \\sim \\text{Uniform}(0,1-x), \\quad \\text{for } 0<x<1.\n$$\n\nTherefore, the correct option is that both conditionals are uniform on the respective truncated intervals determined by the constraint $x+y<1$, which corresponds to option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Beyond constructing a valid sampler, a crucial aspect of applied MCMC is ensuring its efficiency—how quickly it explores the target distribution. A common issue is high autocorrelation between samples, which slows down convergence. This advanced hands-on practice challenges you to tackle this problem directly through reparameterization, a powerful technique in Bayesian modeling . By implementing and comparing two Gibbs samplers for a hierarchical model, you will empirically measure and understand how a 'non-centered' parameterization can break posterior dependencies and significantly improve sampler performance.",
            "id": "3144797",
            "problem": "Consider a hierarchical normal model where observed group-level data are modeled with latent group means and a population-level mean. The objective is to analyze the effect of reparameterization on Markov chain mixing and quantify differences in autocorrelation for the population mean parameter by comparing a centered parameterization against a non-centered parameterization. The comparison must be performed using Markov Chain Monte Carlo (MCMC), specifically Gibbs sampling, for a scenario with known observational variance and known group-level variance.\n\nFoundations:\n- Use Bayes' theorem and normal-normal conjugacy to derive conditional distributions.\n- Use the definition of a Markov chain and the concept of Markov Chain Monte Carlo (MCMC), where the chain transitions are constructed to have a target posterior distribution as the stationary distribution.\n- Gibbs sampling is a special case of MCMC where one samples each variable from its full conditional distribution in sequence.\n- Autocorrelation at lag one is defined as the correlation between successive samples of a stationary time series.\n\nModel specification:\n- For each group index $j \\in \\{1, \\dots, J\\}$, the observed data $y_j$ are modeled as $y_j \\mid \\mu_j \\sim \\mathcal{N}(\\mu_j, \\sigma^2)$, where $\\sigma^2$ is known.\n- The latent group mean has the hierarchical prior $\\mu_j \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2)$, where $\\tau^2$ is known.\n- The population mean has a prior $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$, with $m_0$ and $s_0^2$ known.\n\nTwo parameterizations:\n- Centered parameterization: directly parameterize with $\\mu$ and $\\{\\mu_j\\}_{j=1}^J$.\n- Non-centered parameterization: reparameterize by introducing $\\eta_j \\sim \\mathcal{N}(0,1)$ and set $\\mu_j = \\mu + \\tau \\eta_j$, thereby parameterizing with $\\mu$ and $\\{\\eta_j\\}_{j=1}^J$.\n\nTask:\n- Derive the full conditional distributions required for Gibbs sampling under both parameterizations from Bayes' theorem and the properties of normal distributions. Do not use shortcut formulas; start from the fundamental definitions of the likelihood and priors.\n- Implement two Gibbs samplers corresponding to the centered and the non-centered parameterizations.\n- For each sampler, collect samples of the population mean $\\mu$ and compute the lag-$1$ autocorrelation coefficient for the post-burn-in $\\mu$ chain. For a time series $\\{x_t\\}_{t=1}^N$, the lag-$1$ autocorrelation is defined as\n$$\n\\rho_1 = \\frac{\\sum_{t=1}^{N-1} (x_t - \\bar{x})(x_{t+1} - \\bar{x})}{\\sum_{t=1}^{N} (x_t - \\bar{x})^2},\n$$\nwhere $\\bar{x}$ is the sample mean of $\\{x_t\\}_{t=1}^N$.\n- Quantify the difference in autocorrelation by computing $\\Delta = \\rho_{1,\\text{centered}} - \\rho_{1,\\text{non-centered}}$ for each test case. A positive $\\Delta$ indicates higher autocorrelation in the centered parameterization compared to the non-centered parameterization, and a negative $\\Delta$ indicates the reverse.\n\nTest suite:\nUse the following cases, each specified by $(J, \\{y_j\\}_{j=1}^J, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed})$, with all quantities given explicitly as numbers.\n\n- Case $1$ (weak data, expect non-centered to mix better): $J = 8$, $\\{y_j\\} = [5, 7, 3, -2, 0, 4, 6, 8]$, $\\sigma = 5$, $\\tau = 1$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 123$.\n- Case $2$ (strong data, expect centered to mix better): $J = 8$, $\\{y_j\\} = [5, 7, 3, -2, 0, 4, 6, 8]$, $\\sigma = 1$, $\\tau = 5$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 456$.\n- Case $3$ (boundary case with a single group): $J = 1$, $\\{y_j\\} = [3]$, $\\sigma = 5$, $\\tau = 2$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 789$.\n- Case $4$ (many groups, moderate information): $J = 50$, $\\{y_j\\}$ defined deterministically by $y_j = 0.3 \\times (j - 25.5)$ for $j = 1, \\dots, 50$, $\\sigma = 2$, $\\tau = 2$, $m_0 = 0$, $s_0 = 10$, $N_{\\text{iter}} = 10000$, $N_{\\text{burn}} = 2000$, $\\text{seed} = 987$.\n\nRequirements:\n- Implement both Gibbs samplers and compute $\\Delta$ for each case.\n- Use the seeds provided to ensure deterministic outputs.\n- There are no physical units involved in this problem.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the four cases, for example, $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$.",
            "solution": "The user requests a comparison of centered and non-centered parameterizations for a hierarchical normal model using Gibbs sampling. The primary goal is to analyze the difference in Markov chain Monte Carlo (MCMC) mixing properties by computing the lag-$1$ autocorrelation of the chain for the population mean parameter, $\\mu$.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Model:**\n    -   Data likelihood: $y_j \\mid \\mu_j \\sim \\mathcal{N}(\\mu_j, \\sigma^2)$ for $j \\in \\{1, \\dots, J\\}$, with $\\sigma^2$ known.\n    -   Hierarchical prior for group means: $\\mu_j \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2)$, with $\\tau^2$ known.\n    -   Prior for population mean: $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$, with $m_0$ and $s_0^2$ known.\n-   **Parameterizations:**\n    -   Centered: Parameters are $\\mu$ and $\\{\\mu_j\\}_{j=1}^J$.\n    -   Non-centered: Reparameterize with $\\eta_j \\sim \\mathcal{N}(0,1)$ such that $\\mu_j = \\mu + \\tau \\eta_j$. Parameters are $\\mu$ and $\\{\\eta_j\\}_{j=1}^J$.\n-   **Task:**\n    1.  Derive full conditional distributions for Gibbs sampling under both parameterizations.\n    2.  Implement two Gibbs samplers.\n    3.  Compute the lag-$1$ autocorrelation $\\rho_1$ for the post-burn-in chain of $\\mu$ for each sampler. The formula is $\\rho_1 = \\frac{\\sum_{t=1}^{N-1} (x_t - \\bar{x})(x_{t+1} - \\bar{x})}{\\sum_{t=1}^{N} (x_t - \\bar{x})^2}$.\n    4.  Compute the difference $\\Delta = \\rho_{1,\\text{centered}} - \\rho_{1,\\text{non-centered}}$ for each test case.\n-   **Test Suite:**\n    -   Case $1$: $(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (8, [5, 7, 3, -2, 0, 4, 6, 8], 5, 1, 0, 10, 10000, 2000, 123)$.\n    -   Case $2$: $(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (8, [5, 7, 3, -2, 0, 4, 6, 8], 1, 5, 0, 10, 10000, 2000, 456)$.\n    -   Case $3$: $(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (1, [3], 5, 2, 0, 10, 10000, 2000, 789)$.\n    -   Case $4$: $(J, \\{y_j\\}, \\sigma, \\tau, m_0, s_0, N_{\\text{iter}}, N_{\\text{burn}}, \\text{seed}) = (50, \\{0.3 \\times (j - 25.5)\\}_{j=1}^{50}, 2, 2, 0, 10, 10000, 2000, 987)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard exercise in computational Bayesian statistics, focused on MCMC efficiency.\n-   **Scientific Grounding**: The hierarchical normal model, Gibbs sampling, and the concept of reparameterization to improve mixing are fundamental topics in Bayesian statistics and statistical learning. All models and formulas are based on established probability theory and statistics.\n-   **Well-Posedness**: The problem provides all necessary data, parameters, and a deterministic procedure (including random seeds) to arrive at a unique, meaningful numerical result for each test case.\n-   **Objectivity**: The language is precise and technical. There are no subjective elements.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of Full Conditional Distributions\n\nThe core of a Gibbs sampler is to iteratively draw samples for each parameter from its full conditional distribution, which is the distribution of that parameter conditioned on all other parameters and the data. We derive these distributions for both parameterizations using Bayes' theorem, which states that the posterior is proportional to the likelihood times the prior, $p(\\theta|D) \\propto p(D|\\theta)p(\\theta)$.\n\n#### 1. Centered Parameterization\n\nThe parameters are $\\theta_C = (\\mu, \\mu_1, \\dots, \\mu_J)$. The joint posterior distribution is:\n$$p(\\mu, \\{\\mu_j\\}_{j=1}^J | \\{y_j\\}_{j=1}^J) \\propto p(\\{y_j\\} | \\{\\mu_j\\}) p(\\{\\mu_j\\} | \\mu) p(\\mu)$$\n$$p(\\mu, \\{\\mu_j\\} | \\{y_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(y_j | \\mu_j, \\sigma^2) \\right) \\left( \\prod_{j=1}^J \\mathcal{N}(\\mu_j | \\mu, \\tau^2) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n\n**Full Conditional for $\\mu$:**\nThe distribution of $\\mu$ conditional on all other parameters only depends on those parameters on which it is a parent in the graphical model, which are the $\\{\\mu_j\\}$.\n$$p(\\mu | \\{\\mu_j\\}, \\{y_j\\}) \\propto p(\\mu | \\{\\mu_j\\}) \\propto p(\\{\\mu_j\\} | \\mu) p(\\mu)$$\n$$p(\\mu | \\{\\mu_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(\\mu_j | \\mu, \\tau^2) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n$$p(\\mu | \\{\\mu_j\\}) \\propto \\exp\\left( -\\frac{1}{2\\tau^2} \\sum_{j=1}^J (\\mu_j - \\mu)^2 \\right) \\exp\\left( -\\frac{1}{2s_0^2} (\\mu - m_0)^2 \\right)$$\nExpanding the quadratic terms in $\\mu$ in the exponent:\n$$-\\frac{1}{2}\\left[ \\mu^2 \\left(\\frac{J}{\\tau^2} + \\frac{1}{s_0^2}\\right) - 2\\mu \\left(\\frac{1}{\\tau^2}\\sum_{j=1}^J \\mu_j + \\frac{m_0}{s_0^2}\\right) + \\text{const} \\right]$$\nThis is the kernel of a normal distribution $\\mathcal{N}(\\mu | M_{\\mu}, V_{\\mu})$. By completing the square or matching terms, we find the posterior variance $V_{\\mu}$ and mean $M_{\\mu}$:\n$$V_{\\mu} = \\left(\\frac{J}{\\tau^2} + \\frac{1}{s_0^2}\\right)^{-1}$$\n$$M_{\\mu} = V_{\\mu} \\left(\\frac{\\sum_{j=1}^J \\mu_j}{\\tau^2} + \\frac{m_0}{s_0^2}\\right)$$\nSo, the full conditional is $\\mu | \\{\\mu_j\\} \\sim \\mathcal{N}(M_{\\mu}, V_{\\mu})$.\n\n**Full Conditional for $\\mu_j$:**\nDue to conditional independence in the model, the full conditional for $\\mu_j$ only depends on $y_j$ and $\\mu$.\n$$p(\\mu_j | \\mu, \\{\\mu_{k \\neq j}\\}, \\{y_k\\}) \\propto p(y_j | \\mu_j) p(\\mu_j | \\mu)$$\n$$p(\\mu_j | \\mu, y_j) \\propto \\mathcal{N}(y_j|\\mu_j, \\sigma^2) \\mathcal{N}(\\mu_j|\\mu, \\tau^2)$$\n$$p(\\mu_j | \\mu, y_j) \\propto \\exp\\left( -\\frac{(y_j - \\mu_j)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{(\\mu_j - \\mu)^2}{2\\tau^2} \\right)$$\nExpanding the quadratic terms in $\\mu_j$ in the exponent:\n$$-\\frac{1}{2}\\left[ \\mu_j^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right) - 2\\mu_j \\left(\\frac{y_j}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right) + \\text{const} \\right]$$\nThis is the kernel of a normal distribution $\\mathcal{N}(\\mu_j | M_j, V_j)$ with variance $V_j$ and mean $M_j$:\n$$V_j = \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1}$$\n$$M_j = V_j \\left(\\frac{y_j}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right)$$\nThe full conditional is $\\mu_j | \\mu, y_j \\sim \\mathcal{N}(M_j, V_j)$ for each $j \\in \\{1, \\dots, J\\}$.\n\n#### 2. Non-Centered Parameterization\n\nThe parameters are $\\theta_{NC} = (\\mu, \\eta_1, \\dots, \\eta_J)$, with the mapping $\\mu_j = \\mu + \\tau \\eta_j$. The likelihood is now $y_j | \\mu, \\eta_j \\sim \\mathcal{N}(\\mu + \\tau\\eta_j, \\sigma^2)$. The priors are $\\eta_j \\sim \\mathcal{N}(0, 1)$ and $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$. The joint posterior is:\n$$p(\\mu, \\{\\eta_j\\} | \\{y_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(y_j | \\mu + \\tau\\eta_j, \\sigma^2) \\right) \\left( \\prod_{j=1}^J \\mathcal{N}(\\eta_j | 0, 1) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n\n**Full Conditional for $\\mu$:**\n$$p(\\mu | \\{\\eta_j\\}, \\{y_j\\}) \\propto \\left( \\prod_{j=1}^J \\mathcal{N}(y_j | \\mu + \\tau\\eta_j, \\sigma^2) \\right) \\mathcal{N}(\\mu | m_0, s_0^2)$$\n$$p(\\mu | \\{\\eta_j\\}, \\{y_j\\}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^J (y_j - \\tau\\eta_j - \\mu)^2 \\right) \\exp\\left( -\\frac{(\\mu - m_0)^2}{2s_0^2} \\right)$$\nThis is a standard normal-normal conjugate update where we have $J$ observations $y'_j = y_j - \\tau\\eta_j$, each with mean $\\mu$ and known variance $\\sigma^2$. The full conditional for $\\mu$ is $\\mathcal{N}(\\mu | M_{\\mu, nc}, V_{\\mu, nc})$ with:\n$$V_{\\mu, nc} = \\left(\\frac{J}{\\sigma^2} + \\frac{1}{s_0^2}\\right)^{-1}$$\n$$M_{\\mu, nc} = V_{\\mu, nc} \\left(\\frac{\\sum_{j=1}^J (y_j - \\tau\\eta_j)}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right)$$\n\n**Full Conditional for $\\eta_j$:**\n$$p(\\eta_j | \\mu, \\{\\eta_{k \\neq j}\\}, \\{y_k\\}) \\propto p(y_j | \\mu, \\eta_j) p(\\eta_j)$$\n$$p(\\eta_j | \\mu, y_j) \\propto \\mathcal{N}(y_j|\\mu + \\tau\\eta_j, \\sigma^2) \\mathcal{N}(\\eta_j|0, 1)$$\n$$p(\\eta_j | \\mu, y_j) \\propto \\exp\\left( -\\frac{(y_j - \\mu - \\tau\\eta_j)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{\\eta_j^2}{2} \\right)$$\nExpanding the quadratic terms in $\\eta_j$ in the exponent:\n$$-\\frac{1}{2}\\left[ \\eta_j^2 \\left(\\frac{\\tau^2}{\\sigma^2} + 1\\right) - 2\\eta_j \\frac{\\tau(y_j - \\mu)}{\\sigma^2} + \\text{const} \\right]$$\nThis is the kernel of a normal distribution $\\mathcal{N}(\\eta_j | M_{\\eta_j}, V_{\\eta_j})$ with:\n$$V_{\\eta_j} = \\left(\\frac{\\tau^2}{\\sigma^2} + 1\\right)^{-1}$$\n$$M_{\\eta_j} = V_{\\eta_j} \\left(\\frac{\\tau(y_j - \\mu)}{\\sigma^2}\\right)$$\nThe full conditional is $\\eta_j | \\mu, y_j \\sim \\mathcal{N}(M_{\\eta_j}, V_{\\eta_j})$ for each $j \\in \\{1, \\dots, J\\}$.\n\n### MCMC Performance and Implementation\n\nThe two parameterizations are mathematically equivalent but can have vastly different computational properties.\n-   **Centered Parameterization:** The parameters $\\mu$ and $\\{\\mu_j\\}$ are often highly correlated in the posterior, especially when data is sparse ($\\sigma^2$ is large) and the hierarchical variance is small ($\\tau^2$ is small). This dependency means a draw for $\\mu$ strongly constrains the subsequent draws for $\\{\\mu_j\\}$, and vice-versa. This \"stickiness\" causes the Gibbs sampler to move slowly through the parameter space, resulting in high autocorrelation between successive samples.\n-   **Non-Centered Parameterization:** This approach aims to break the posterior dependency by sampling $\\mu$ and a set of independent standard normal parameters $\\{\\eta_j\\}$. The dependency is moved from the priors into the likelihood. This often reduces posterior correlation between the blocks of parameters being sampled, leading to faster mixing and lower autocorrelation, particularly in the sparse data scenarios mentioned above.\n-   Conversely, when data is informative ($\\sigma^2$ is small), the centered parameterization can perform well because the likelihood for each $y_j$ identifies $\\mu_j$ precisely, reducing its dependence on $\\mu$. The non-centered parameterization can struggle in this regime because a strong constraint $y_j \\approx \\mu + \\tau\\eta_j$ can induce high posterior correlation between $\\mu$ and $\\eta_j$.\n\nThe implementation will consist of two Gibbs sampler functions, one for each parameterization. Each function will iteratively sample from the derived full conditional distributions for $N_{\\text{iter}}$ steps. After discarding the first $N_{\\text{burn}}$ samples (the burn-in period), the lag-$1$ autocorrelation of the chain for $\\mu$ will be computed. The final result for each test case is the difference $\\Delta$ between the autocorrelations from the two samplers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_autocorr_lag1(chain):\n    \"\"\"\n    Computes the lag-1 autocorrelation for a given MCMC chain.\n    \"\"\"\n    if len(chain) < 2:\n        return np.nan\n    \n    x_bar = np.mean(chain)\n    dev = chain - x_bar\n    denominator = np.dot(dev, dev)\n    \n    if denominator == 0:\n        return 0.0\n        \n    numerator = np.dot(dev[:-1], dev[1:])\n    return numerator / denominator\n\ndef gibbs_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed):\n    \"\"\"\n    Gibbs sampler for the centered parameterization.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    y = np.array(y)\n    \n    sigma2 = sigma**2\n    tau2 = tau**2\n    s02 = s0**2\n    \n    # Initialize parameters\n    mu = float(m0)\n    mu_j = np.copy(y)\n    \n    mu_samples = np.zeros(N_iter)\n    \n    # Pre-compute conditional posterior variances (constant throughout sampling)\n    V_j = 1.0 / (1.0/sigma2 + 1.0/tau2)\n    V_mu = 1.0 / (J/tau2 + 1.0/s02)\n    \n    std_j = np.sqrt(V_j)\n    std_mu = np.sqrt(V_mu)\n\n    # Gibbs sampling loop\n    for i in range(N_iter):\n        # Update mu (population mean)\n        M_mu = V_mu * (np.sum(mu_j)/tau2 + m0/s02)\n        mu = rng.normal(loc=M_mu, scale=std_mu)\n        mu_samples[i] = mu\n\n        # Update mu_j (group means)\n        M_j = V_j * (y/sigma2 + mu/tau2)\n        mu_j = rng.normal(loc=M_j, scale=std_j)\n        \n    post_burn_in_samples = mu_samples[N_burn:]\n    return calculate_autocorr_lag1(post_burn_in_samples)\n\ndef gibbs_non_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed):\n    \"\"\"\n    Gibbs sampler for the non-centered parameterization.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    y = np.array(y)\n    \n    sigma2 = sigma**2\n    tau2 = tau**2\n    s02 = s0**2\n    \n    # Initialize parameters\n    mu = float(m0)\n    eta_j = np.zeros(J)\n    \n    mu_samples = np.zeros(N_iter)\n\n    # Pre-compute conditional posterior variances (constant throughout sampling)\n    V_mu_nc = 1.0 / (J/sigma2 + 1.0/s02)\n    V_eta_j = 1.0 / (tau2/sigma2 + 1.0)\n    \n    std_mu_nc = np.sqrt(V_mu_nc)\n    std_eta_j = np.sqrt(V_eta_j)\n\n    # Gibbs sampling loop\n    for i in range(N_iter):\n        # Update mu\n        M_mu_nc = V_mu_nc * (np.sum(y - tau * eta_j)/sigma2 + m0/s02)\n        mu = rng.normal(loc=M_mu_nc, scale=std_mu_nc)\n        mu_samples[i] = mu\n\n        # Update eta_j\n        M_eta_j = V_eta_j * (tau * (y - mu) / sigma2)\n        eta_j = rng.normal(loc=M_eta_j, scale=std_eta_j)\n        \n    post_burn_in_samples = mu_samples[N_burn:]\n    return calculate_autocorr_lag1(post_burn_in_samples)\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute the difference in autocorrelation.\n    \"\"\"\n    y_case4 = [0.3 * (j - 25.5) for j in range(1, 51)]\n\n    test_cases = [\n        # (J, {y_j}, sigma, tau, m0, s0, N_iter, N_burn, seed)\n        (8, [5, 7, 3, -2, 0, 4, 6, 8], 5, 1, 0, 10, 10000, 2000, 123),\n        (8, [5, 7, 3, -2, 0, 4, 6, 8], 1, 5, 0, 10, 10000, 2000, 456),\n        (1, [3], 5, 2, 0, 10, 10000, 2000, 789),\n        (50, y_case4, 2, 2, 0, 10, 10000, 2000, 987),\n    ]\n\n    results = []\n    for case in test_cases:\n        J, y, sigma, tau, m0, s0, N_iter, N_burn, seed = case\n        \n        # Run centered sampler\n        rho_centered = gibbs_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed)\n        \n        # Run non-centered sampler\n        rho_non_centered = gibbs_non_centered(J, y, sigma, tau, m0, s0, N_iter, N_burn, seed)\n        \n        # Compute the difference\n        delta_rho = rho_centered - rho_non_centered\n        results.append(delta_rho)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}