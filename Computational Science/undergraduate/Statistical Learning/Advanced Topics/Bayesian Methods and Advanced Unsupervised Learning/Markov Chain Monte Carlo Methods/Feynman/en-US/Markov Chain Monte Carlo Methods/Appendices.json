{
    "hands_on_practices": [
        {
            "introduction": "The Metropolis algorithm's elegance lies in its acceptance rule, which ensures the resulting chain of samples converges to the desired target distribution. This first exercise breaks down the core mechanism into a single calculation, focusing on the acceptance probability $\\alpha$. By working through a concrete example , you will see firsthand how the relative likelihood of the current and proposed states dictates the chain's movement, forming the fundamental building block of MCMC exploration.",
            "id": "1371728",
            "problem": "A data scientist is implementing a Markov Chain Monte Carlo (MCMC) simulation to draw samples from a posterior probability distribution for a parameter $x$. The target distribution, $\\pi(x)$, is proportional to the exponential of the negative absolute value of the parameter, such that $\\pi(x) \\propto \\exp(-|x|)$.\n\nThe scientist uses the Metropolis algorithm with a symmetric proposal distribution $q(x'|x)$, where the probability of proposing a new state $x'$ given the current state $x$ is equal to the probability of proposing $x$ given $x'$ (i.e., $q(x'|x) = q(x|x')$).\n\nSuppose that at a certain step in the simulation, the current state of the chain is $x = 1.5$. The algorithm then proposes a move to a new candidate state $x' = 2.0$.\n\nCalculate the acceptance probability for this specific move. Your answer should be a dimensionless real number. Round your final answer to four significant figures.",
            "solution": "The Metropolis acceptance probability for a move from $x$ to $x'$ with a symmetric proposal $q(x'|x)=q(x|x')$ is\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\nGiven the target distribution $\\pi(x)\\propto \\exp(-|x|)$, the ratio simplifies to\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\nWith $x=1.5$ and $x'=2.0$, we have $|x|=1.5$ and $|x'|=2.0$, so\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\nTherefore,\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\nNumerically, $\\exp(-0.5)\\approx 0.6065$ when rounded to four significant figures.",
            "answer": "$$\\boxed{0.6065}$$"
        },
        {
            "introduction": "While the acceptance rule is fixed, the way we propose new states is a critical design choice that dramatically affects sampler performance. This exercise explores the practical consequences of choosing the \"step size\" or width of a random-walk proposal distribution . Understanding the trade-off between making small, safe steps that are often accepted and large, ambitious jumps that are often rejected is key to tuning an efficient MCMC sampler that explores the parameter space effectively.",
            "id": "1932810",
            "problem": "An analyst is using a Markov Chain Monte Carlo (MCMC) method, specifically the Random Walk Metropolis algorithm, to generate samples from a continuous, unimodal target probability density function $\\pi(x)$ defined on the real line. The algorithm proceeds as follows: given the current state of the chain is $x^{(t)}$, a new candidate state $x'$ is proposed from a symmetric proposal distribution $q(x'|x^{(t)})$. For this specific implementation, the proposal is generated by adding a random perturbation: $x' = x^{(t)} + \\epsilon$, where $\\epsilon$ is drawn from a Normal distribution with mean 0 and standard deviation $\\sigma$, i.e., $\\epsilon \\sim N(0, \\sigma^2)$. The proposed state $x'$ is then accepted as the next state, $x^{(t+1)} = x'$, with probability $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. If the proposal is rejected, the chain remains at the current state, i.e., $x^{(t+1)} = x^{(t)}$.\n\nThe analyst is experimenting with two different settings for the proposal distribution's standard deviation:\n1.  A very narrow proposal distribution, where $\\sigma = \\sigma_{small}$ is a very small positive number.\n2.  A very wide proposal distribution, where $\\sigma = \\sigma_{large}$ is a very large positive number.\n\nAssume the chain has been initialized in a region of non-negligible probability (e.g., near the mode of $\\pi(x)$). Which of the following statements most accurately describes the expected behavior of the MCMC chain in these two scenarios?\n\nA. The narrow proposal ($\\sigma_{small}$) will lead to a low acceptance rate and slow exploration. The wide proposal ($\\sigma_{large}$) will lead to a high acceptance rate and fast exploration.\n\nB. Both proposal widths will lead to similar acceptance rates, but the chain using the wide proposal ($\\sigma_{large}$) will explore the state space much more quickly than the chain using the narrow proposal ($\\sigma_{small}$).\n\nC. The narrow proposal ($\\sigma_{small}$) will result in a very high acceptance rate, but the chain will explore the state space very slowly. The wide proposal ($\\sigma_{large}$) will result in a very low acceptance rate, causing the chain to remain stuck at the same state for many iterations.\n\nD. The narrow proposal ($\\sigma_{small}$) will lead to a high acceptance rate and fast exploration. The wide proposal ($\\sigma_{large}$) will lead to a low acceptance rate and slow exploration.\n\nE. The width of the proposal distribution has a negligible effect on the algorithm's performance; both the acceptance rate and the speed of exploration are primarily determined by the properties of the target distribution $\\pi(x)$.",
            "solution": "The problem asks us to analyze the behavior of the Random Walk Metropolis algorithm, concentrating on how the width (standard deviation $\\sigma$) of the proposal distribution $N(0, \\sigma^2)$ affects the chain's acceptance rate and its ability to explore the state space of a target distribution $\\pi(x)$.\n\nThe acceptance probability for a proposed move from $x^{(t)}$ to $x'$ is given by $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. Let's analyze the two cases separately.\n\n**Case 1: Narrow Proposal Distribution ($\\sigma = \\sigma_{small}$)**\n\nWhen the standard deviation $\\sigma_{small}$ is very small, the random perturbation $\\epsilon$ drawn from $N(0, \\sigma_{small}^2)$ will also be very small in magnitude. This means the proposed state, $x' = x^{(t)} + \\epsilon$, will be very close to the current state $x^{(t)}$.\n\nSince the target distribution $\\pi(x)$ is continuous, if $x'$ is very close to $x^{(t)}$, then the value of the probability density at these two points, $\\pi(x')$ and $\\pi(x^{(t)})$, will be very similar.\nConsequently, the ratio $\\frac{\\pi(x')}{\\pi(x^{(t)})}$ will be very close to 1.\n\nThe acceptance probability is $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. Since the ratio is close to 1, the acceptance probability $\\alpha$ will be very high (close to 1). This means that almost every proposed move will be accepted.\n\nHowever, since each accepted move is just a tiny step away from the previous position, the chain moves through the state space very slowly. This is an inefficient way to explore the full range of the target distribution $\\pi(x)$, especially the tails. The chain exhibits high autocorrelation, meaning successive samples are highly dependent, and it takes a very large number of iterations to obtain a representative set of independent samples. This constitutes very slow exploration.\n\n**Case 2: Wide Proposal Distribution ($\\sigma = \\sigma_{large}$)**\n\nWhen the standard deviation $\\sigma_{large}$ is very large, the random perturbation $\\epsilon$ will often be large in magnitude. This means the proposed state $x' = x^{(t)} + \\epsilon$ is likely to be very far from the current state $x^{(t)}$.\n\nWe are given that the target distribution $\\pi(x)$ is unimodal and the chain starts in a region of high probability (near the mode). When the chain is in such a region, $\\pi(x^{(t)})$ is relatively large. A large jump from $x^{(t)}$ is very likely to land in a region far out in the tails of the distribution, where the probability density is extremely low. Thus, it is highly probable that $\\pi(x') \\ll \\pi(x^{(t)})$.\n\nIn this scenario, the ratio $\\frac{\\pi(x')}{\\pi(x^{(t)})}$ will be very close to 0.\n\nThe acceptance probability is $\\alpha(x', x^{(t)}) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x^{(t)})}\\right)$. Since the ratio is very small, the acceptance probability $\\alpha$ will also be very small. This means that the vast majority of proposed moves will be rejected.\n\nWhen a proposal is rejected, the chain does not move: $x^{(t+1)} = x^{(t)}$. Therefore, for a wide proposal distribution, the chain will remain stuck at the same state for many consecutive iterations, waiting for a rare proposal that happens to land in another high-probability region. This is also a very inefficient way to explore the state space.\n\n**Conclusion and Evaluation of Options:**\n\n-   A narrow proposal gives a **high acceptance rate** but **slow exploration**.\n-   A wide proposal gives a **low acceptance rate** and **slow exploration** (because the chain gets stuck).\n\nLet's evaluate the given options based on this analysis:\n\n-   **A:** Incorrect. It claims a wide proposal leads to a high acceptance rate.\n-   **B:** Incorrect. It claims both have similar acceptance rates.\n-   **C:** This statement correctly captures both behaviors. The narrow proposal leads to a high acceptance rate but slow exploration. The wide proposal leads to a very low acceptance rate, causing the chain to get stuck. This is the most accurate description.\n-   **D:** Incorrect. It claims a narrow proposal leads to fast exploration.\n-   **E:** Incorrect. The width of the proposal distribution is a critical tuning parameter that profoundly affects the algorithm's performance.\n\nTherefore, the most accurate statement is C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "An efficient sampler is not always a correct one, and one of the most significant challenges in MCMC is ensuring the sampler explores all important regions of the target distribution. This is especially true when the distribution has multiple modes (peaks of high probability) separated by valleys of low probability. This practice problem  presents a classic scenario where a seemingly well-behaved sampler gets trapped in a single mode, failing to provide a complete picture of the target. This exercise is vital for developing the diagnostic skills needed to identify when an MCMC chain has failed to converge properly.",
            "id": "1932795",
            "problem": "A data scientist is employing a Markov chain Monte Carlo (MCMC) method to draw samples from a complex, one-dimensional target probability density function, $\\pi(x)$. The target distribution is known to be a symmetric bimodal distribution, specifically an equal-weight mixture of two Gaussian distributions. The density is proportional to the sum of two Gaussian probability density functions:\n$$ \\pi(x) \\propto \\exp\\left(-\\frac{(x - \\mu_A)^2}{2\\sigma_{mode}^2}\\right) + \\exp\\left(-\\frac{(x - \\mu_B)^2}{2\\sigma_{mode}^2}\\right) $$\nThe parameters are given as $\\mu_A = -10$, $\\mu_B = 10$, and $\\sigma_{mode} = 1$. This structure results in two narrow, well-separated probability modes centered at $x=-10$ and $x=10$, with a region of extremely low probability density between them.\n\nThe scientist uses a random-walk Metropolis algorithm. At each step, a new state $x'$ is proposed from a Gaussian distribution centered at the current state $x_t$, i.e., $x' \\sim N(x_t, \\sigma_{step}^2)$. Seeking to achieve a high acceptance rate, the scientist chooses a very small step size variance, setting $\\sigma_{step} = 0.1$. The MCMC chain is initialized at the peak of one of the modes, $x_0 = -10$, and is run for $N=10^6$ iterations.\n\nWhich of the following statements most accurately describes the behavior of the MCMC sampler and the statistical properties of the resulting sample set $\\{x_1, x_2, \\dots, x_N\\}$?\n\nA. The samples will be distributed around the true mean of the target distribution, which is $x=0$. The sample mean will be close to 0, but the sample variance will be large (greater than 100), accurately reflecting the significant separation between the two modes.\n\nB. The acceptance rate of proposed moves will be very low (close to 0) because the step size is not well-tuned to the overall scale of the target distribution. The chain will remain at or very near its initial position, $x_0 = -10$.\n\nC. The acceptance rate of proposed moves will be very high (close to 1). The generated samples will thoroughly explore the region corresponding to the mode at $x=-10$, but the chain will fail to transition to the other mode at $x=10$. The sample mean will be approximately $-10$.\n\nD. The sampler will efficiently explore the entire state space. The chain will frequently jump back and forth between the two modes, and the histogram of the samples will correctly form two distinct peaks centered at $x=-10$ and $x=10$.\n\nE. The sampler will behave like a simple random walk, causing the samples to diffuse away from the starting point. The final collection of samples will be approximately uniformly distributed over a wide interval centered at $x=-10$.",
            "solution": "We model the target as an equal-weight mixture of two Gaussian densities with common standard deviation $\\sigma_{mode}$ and means $\\mu_{A}$ and $\\mu_{B}$. Up to proportionality,\n$$\n\\pi(x) \\propto \\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right) + \\exp\\!\\left(-\\frac{(x-\\mu_{B})^{2}}{2\\sigma_{mode}^{2}}\\right).\n$$\nWith a random-walk Metropolis sampler using a symmetric Gaussian proposal $q(x' \\mid x)=\\mathcal{N}(x,\\sigma_{step}^{2})$, the Metropolisâ€“Hastings acceptance probability at state $x_{t}$ for a proposal $x'$ is\n$$\n\\alpha(x_{t},x')=\\min\\!\\left(1,\\frac{\\pi(x')}{\\pi(x_{t})}\\right).\n$$\n\nInitialize at $x_{0}=\\mu_{A}$. Because the modes are well separated, when $x$ is near $\\mu_{A}$ the contribution from the $\\mu_{B}$-component in $\\pi(x)$ is negligible relative to that from the $\\mu_{A}$-component. For a small proposal increment $\\epsilon:=x'-x$ with $|\\epsilon| \\ll \\sigma_{mode}$, the dominant-ratio approximation gives\n$$\n\\frac{\\pi(x')}{\\pi(x)} \\approx \\frac{\\exp\\!\\left(-\\frac{(x'-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right)}{\\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right)}=\\exp\\!\\left(-\\frac{(x'-\\mu_{A})^{2}-(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right).\n$$\nAt $x\\approx\\mu_{A}$, this simplifies for small $\\epsilon$ to\n$$\n\\frac{\\pi(x')}{\\pi(x)} \\approx \\exp\\!\\left(-\\frac{\\epsilon^{2}}{2\\sigma_{mode}^{2}}\\right),\n$$\nso the acceptance probability is close to $1$ when $\\sigma_{step} \\ll \\sigma_{mode}$ because typical $|\\epsilon|$ is on the order of $\\sigma_{step}$. Hence the acceptance rate is very high while the chain explores the vicinity of the starting mode.\n\nA direct jump from the neighborhood of $\\mu_{A}$ to the neighborhood of $\\mu_{B}$ in one proposal requires a displacement of order $|\\mu_{B}-\\mu_{A}|$. Under a Gaussian proposal with variance $\\sigma_{step}^{2}$, the probability of such a jump is of order\n$$\n\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{2\\sigma_{step}^{2}}\\right),\n$$\nwhich is negligible when $|\\mu_{B}-\\mu_{A}| \\gg \\sigma_{step}$.\n\nCrossing the low-density region via many small accepted steps is also overwhelmingly unlikely within a finite run because the stationary density in the valley is exponentially smaller than at the peak. At the midpoint $x^{\\star}=(\\mu_{A}+\\mu_{B})/2$, the target density is\n$$\n\\pi(x^{\\star}) \\propto 2\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{8\\sigma_{mode}^{2}}\\right),\n$$\nwhile near $x=\\mu_{A}$ it is $\\pi(\\mu_{A}) \\propto 1$ (the other component there is negligible). Thus the ratio\n$$\n\\frac{\\pi(x^{\\star})}{\\pi(\\mu_{A})} \\approx 2\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{8\\sigma_{mode}^{2}}\\right)\n$$\nis exponentially small when $|\\mu_{B}-\\mu_{A}| \\gg \\sigma_{mode}$. This implies an exponentially large expected time to reach the valley or the other mode, on the order of the inverse of this ratio, which far exceeds the given $N$ when the separation is large and the proposal is very local.\n\nTherefore, with $\\sigma_{step}$ chosen very small relative to $\\sigma_{mode}$ and with well-separated modes, the chain has a very high acceptance rate, thoroughly explores the local basin around the starting mode at $x=\\mu_{A}$, essentially never transitions to the other mode within the run, and yields a sample mean approximately equal to $\\mu_{A}$. Among the options, this corresponds to statement C.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}