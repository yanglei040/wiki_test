{
    "hands_on_practices": [
        {
            "introduction": "理论知识告诉我们，高斯混合模型（GMM）使用贝叶斯决策论进行聚类。但是，这在实践中究竟意味着什么？本练习将通过一个简化的场景，让你亲手推导并分析决策边界，直观地理解混合权重（即先验概率）如何影响聚类结果。通过这个练习，你将看到决策边界并不仅仅位于两个簇的均值中点，而是会向权重较小的簇偏移，这揭示了GMM决策的深刻统计内涵。",
            "id": "3122648",
            "problem": "考虑一个在 一维 ($1$-D) 特征空间中用于聚类的、包含 $2$ 个组分的高斯混合模型 (GMM)。设该模型有两个组分均值 $\\mu_1$ 和 $\\mu_2$，两个组分具有相同的协方差（在一维情况下等于方差）$\\sigma^2$，以及混合权重（先验）$\\pi_1$ 和 $\\pi_2$，满足 $\\pi_1 + \\pi_2 = 1$ 且对于每个组分索引 $k \\in \\{1,2\\}$ 都有 $\\pi_k \\in (0,1)$。数据生成密度为 $p(x) = \\pi_1 \\,\\mathcal{N}(x \\mid \\mu_1, \\sigma^2) + \\pi_2 \\,\\mathcal{N}(x \\mid \\mu_2, \\sigma^2)$，其中 $\\mathcal{N}(x \\mid \\mu, \\sigma^2)$ 表示一维高斯密度。贝叶斯决策规则将点 $x$ 分配给具有较大后验概率的组分，该后验概率正比于 $\\pi_k \\,\\mathcal{N}(x \\mid \\mu_k, \\sigma^2)$。两个组分之间的决策边界是后验概率相等的位置 $x^\\star$。\n\n您的任务是：\n- 仅从高斯密度和贝叶斯决策规则的定义出发，从第一性原理推导由 $\\pi_1 \\,\\mathcal{N}(x^\\star \\mid \\mu_1, \\sigma^2) = \\pi_2 \\,\\mathcal{N}(x^\\star \\mid \\mu_2, \\sigma^2)$ 定义的决策边界 $x^\\star$ 的方程，并求解该方程，用 $\\mu_1$、$\\mu_2$、$\\sigma^2$、$\\pi_1$ 和 $\\pi_2$ 表示 $x^\\star$。\n- 定义中点 $m = (\\mu_1 + \\mu_2)/2$ 和位移 $\\Delta = x^\\star - m$。分析 $\\Delta$ 的符号如何依赖于混合权重，并将 $\\Delta$ 的符号与决策边界是向 $\\mu_1$ 移动还是向 $\\mu_2$ 移动联系起来。\n\n实现一个程序，对于每个测试用例，计算三元组 $[x^\\star, \\Delta, s]$，其中 $x^\\star$ 是决策边界位置，$\\Delta$ 是相对于中点的位移， $s$ 是一个整数符号指示器，定义如下：\n- 如果 $\\Delta > 0$（边界向 $\\mu_2$ 移动），则 $s = 1$\n- 如果 $\\Delta  0$（边界向 $\\mu_1$ 移动），则 $s = -1$\n- 如果 $\\Delta = 0$（无移动），则 $s = 0$\n\n使用以下测试套件，该套件保持协方差相同并改变混合权重，以探究决策边界的移动：\n- 测试用例 1：$\\mu_1 = -1$, $\\mu_2 = 1$, $\\sigma^2 = 0.25$, $\\pi_1 = 0.5$, $\\pi_2 = 0.5$。\n- 测试用例 2：$\\mu_1 = -1$, $\\mu_2 = 1$, $\\sigma^2 = 0.25$, $\\pi_1 = 0.7$, $\\pi_2 = 0.3$。\n- 测试用例 3：$\\mu_1 = -1$, $\\mu_2 = 1$, $\\sigma^2 = 0.25$, $\\pi_1 = 0.01$, $\\pi_2 = 0.99$。\n\n您的程序应生成单行输出，其中包含结果，格式为用方括号括起来的逗号分隔列表，每个测试用例一个三元组，按顺序排列：$[[x^\\star_1,\\Delta_1,s_1],[x^\\star_2,\\Delta_2,s_2],[x^\\star_3,\\Delta_3,s_3]]$。不应打印任何其他文本。",
            "solution": "用户提供了一个基于统计学习理论的有效且适定的问题。任务是为二维一维高斯混合模型（GMM）推导决策边界并分析其性质。\n\n### 第 1 部分：决策边界 $x^\\star$ 的推导\n\n问题将决策边界 $x^\\star$ 定义为两个组分的后验概率相等的点。在高斯混合模型（GMM）的背景下，组分 $k$ 的后验概率正比于先验概率（混合权重）$\\pi_k$ 与类条件密度 $\\mathcal{N}(x \\mid \\mu_k, \\sigma^2)$ 的乘积。因此，决策边界 $x^\\star$ 满足以下方程：\n$$\n\\pi_1 \\,\\mathcal{N}(x^\\star \\mid \\mu_1, \\sigma^2) = \\pi_2 \\,\\mathcal{N}(x^\\star \\mid \\mu_2, \\sigma^2)\n$$\n一维高斯概率密度函数 (PDF) 由下式给出：\n$$\n\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n$$\n将此定义代入边界方程，得到：\n$$\n\\pi_1 \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2}\\right) = \\pi_2 \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\\right)\n$$\n归一化常数 $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ 为正且在等式两边共有，因此可以消去：\n$$\n\\pi_1 \\exp\\left(-\\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2}\\right) = \\pi_2 \\exp\\left(-\\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\\right)\n$$\n为了求解 $x^\\star$，我们对等式两边取自然对数。这是一个保持等式成立的单调变换。\n$$\n\\ln\\left( \\pi_1 \\exp\\left(-\\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2}\\right) \\right) = \\ln\\left( \\pi_2 \\exp\\left(-\\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\\right) \\right)\n$$\n使用对数的性质 $\\ln(ab) = \\ln(a) + \\ln(b)$ 和 $\\ln(e^z) = z$，我们得到：\n$$\n\\ln(\\pi_1) - \\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2} = \\ln(\\pi_2) - \\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\n$$\n重新排列各项，将对数部分和二次项部分分组：\n$$\n\\ln(\\pi_1) - \\ln(\\pi_2) = \\frac{(x^\\star-\\mu_1)^2}{2\\sigma^2} - \\frac{(x^\\star-\\mu_2)^2}{2\\sigma^2}\n$$\n使用 $\\ln(a) - \\ln(b) = \\ln(a/b)$ 并将两边同乘以 $2\\sigma^2$：\n$$\n2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) = (x^\\star-\\mu_1)^2 - (x^\\star-\\mu_2)^2\n$$\n我们展开右侧的二次项：\n$$\n(x^\\star)^2 - 2x^\\star\\mu_1 + \\mu_1^2 - \\left((x^\\star)^2 - 2x^\\star\\mu_2 + \\mu_2^2\\right)\n$$\n$$\n= (x^\\star)^2 - 2x^\\star\\mu_1 + \\mu_1^2 - (x^\\star)^2 + 2x^\\star\\mu_2 - \\mu_2^2\n$$\n$(x^\\star)^2$ 项相互抵消，留下一个关于 $x^\\star$ 的线性方程：\n$$\n= 2x^\\star(\\mu_2 - \\mu_1) + (\\mu_1^2 - \\mu_2^2)\n$$\n使用平方差公式 $\\mu_1^2 - \\mu_2^2 = (\\mu_1 - \\mu_2)(\\mu_1 + \\mu_2) = -(\\mu_2 - \\mu_1)(\\mu_1 + \\mu_2)$，上式简化为：\n$$\n= 2x^\\star(\\mu_2 - \\mu_1) - (\\mu_2 - \\mu_1)(\\mu_1 + \\mu_2)\n$$\n将其代回我们的方程：\n$$\n2\\sigma^2 \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) = 2x^\\star(\\mu_2 - \\mu_1) - (\\mu_2 - \\mu_1)(\\mu_1 + \\mu_2)\n$$\n假设均值不同 ($\\mu_1 \\neq \\mu_2$)，我们可以将等式两边除以 $2(\\mu_2 - \\mu_1)$：\n$$\n\\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) = x^\\star - \\frac{\\mu_1 + \\mu_2}{2}\n$$\n求解 $x^\\star$ 得出决策边界的最终表达式：\n$$\nx^\\star = \\frac{\\mu_1 + \\mu_2}{2} + \\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right)\n$$\n\n### 第 2 部分：位移 $\\Delta$ 的分析\n\n问题定义了中点 $m = (\\mu_1 + \\mu_2)/2$ 和位移 $\\Delta = x^\\star - m$。使用我们为 $x^\\star$ 推导出的表达式：\n$$\n\\Delta = \\left( \\frac{\\mu_1 + \\mu_2}{2} + \\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) \\right) - \\frac{\\mu_1 + \\mu_2}{2}\n$$\n$$\n\\Delta = \\frac{\\sigma^2}{\\mu_2 - \\mu_1} \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right)\n$$\n$\\Delta$ 的符号决定了决策边界相对于中点 $m$ 的移动。由于方差 $\\sigma^2$ 始终为正，$\\Delta$ 的符号取决于 $(\\mu_2 - \\mu_1)$ 和 $\\ln(\\pi_1/\\pi_2)$ 的符号。\n对于给定的测试用例，$\\mu_1 = -1$ 且 $\\mu_2 = 1$，因此 $\\mu_2 - \\mu_1 = 2 > 0$。在这种情况下，$\\Delta$ 的符号完全由 $\\ln(\\pi_1/\\pi_2)$ 的符号决定。\n\n- 如果 $\\pi_1 = \\pi_2$，那么 $\\pi_1/\\pi_2 = 1$ 且 $\\ln(1) = 0$。这使得 $\\Delta = 0$。决策边界 $x^\\star$ 恰好在中点 $m$ 处。这对应于符号指示器 $s=0$。\n- 如果 $\\pi_1 > \\pi_2$，那么 $\\pi_1/\\pi_2 > 1$ 且 $\\ln(\\pi_1/\\pi_2) > 0$。这使得 $\\Delta > 0$。边界 $x^\\star = m + \\Delta$ 从中点向正方向移动，即朝向 $\\mu_2$。这对应于 $s=1$。组分 1 的较高先验概率将其均值 $\\mu_1$ 一侧的边界推开。\n- 如果 $\\pi_1  \\pi_2$，那么 $\\pi_1/\\pi_2  1$ 且 $\\ln(\\pi_1/\\pi_2)  0$。这使得 $\\Delta  0$。边界 $x^\\star = m + \\Delta$ 从中点向负方向移动，即朝向 $\\mu_1$。这对应于 $s=-1$。组分 2 的较高先验概率将其均值 $\\mu_2$ 一侧的边界推开。\n\n这些结果与问题陈述中提供的符号指示器 $s$ 的定义一致。现在可以实现推导出的公式来解决这些测试用例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (mu1, mu2, sigma_sq, pi1, pi2)\n        (-1.0, 1.0, 0.25, 0.5, 0.5),\n        (-1.0, 1.0, 0.25, 0.7, 0.3),\n        (-1.0, 1.0, 0.25, 0.01, 0.99),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu1, mu2, sigma_sq, pi1, pi2 = case\n\n        # Calculate the midpoint m\n        m = (mu1 + mu2) / 2.0\n        \n        # The decision boundary x_star is derived from the condition:\n        # pi1 * N(x|mu1, sigma_sq) = pi2 * N(x|mu2, sigma_sq)\n        # This simplifies to the following equation, as derived in the solution:\n        # x_star = m + (sigma_sq / (mu2 - mu1)) * log(pi1 / pi2)\n        \n        # We can assume mu1 != mu2, as per the test cases.\n        # The priors pi_k are in (0,1), so division by pi2 is safe.\n        log_ratio = np.log(pi1 / pi2)\n        \n        x_star = m + (sigma_sq / (mu2 - mu1)) * log_ratio\n        \n        # The displacement delta is defined as x_star - m.\n        delta = x_star - m\n        \n        # The sign indicator s is the sign of delta.\n        # np.sign(x) returns 1.0 for x>0, -1.0 for x0, and 0.0 for x=0.\n        # We cast the result to an integer.\n        s = int(np.sign(delta))\n        \n        # Store the computed triple [x_star, delta, s].\n        results.append([x_star, delta, s])\n\n    # The required output format is a single line with no spaces:\n    # [[x_1,d_1,s_1],[x_2,d_2,s_2],[x_3,d_3,s_3]]\n    # Converting the list of lists to a string and removing spaces achieves this.\n    final_output = str(results).replace(\" \", \"\")\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "高斯混合模型的一个强大之处在于其“软聚类”特性，它为每个数据点提供了分配到各个簇的概率（即“责任”）。但我们如何量化这种分配的不确定性呢？本练习将引入信息论中的香农熵，作为一个强大的工具来衡量聚类分配的模糊程度。通过计算每个数据点责任的熵，你将学会识别模型最不确定的区域，这些区域通常对应于簇的重叠部分。",
            "id": "3122627",
            "problem": "您的任务是实现一个程序，该程序使用每个数据点的分量责任（component responsibilities）的 Shannon 熵来量化高斯混合模型聚类中的分配不确定性。请从基本原理开始：使用 Bayes 定理从高斯混合模型中获取后验分配概率（责任），并使用离散分布的 Shannon 熵定义。实现必须是数值稳定且科学合理的。\n\n假设一个在 $d$ 维空间中具有 $K$ 个分量的高斯混合模型。设混合模型具有非负的混合比例 $\\pi_k$（满足 $\\sum_{k=1}^K \\pi_k = 1$）、分量均值 $\\mu_k \\in \\mathbb{R}^d$ 以及正定协方差矩阵 $\\Sigma_k \\in \\mathbb{R}^{d \\times d}$，其中 $k \\in \\{1,\\dots,K\\}$。分量 $k$ 的多元正态概率密度函数 (PDF) 为\n$$\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu_k)^\\top \\Sigma_k^{-1} (x-\\mu_k)\\right).\n$$\n使用 Bayes 定理，将每个数据点 $x_i$ 的后验分配概率（责任）$r_{ik}$ 构建为给定 $x_i$ 时分量 $k$ 的后验概率，该概率由分量先验 $\\pi_k$ 和分量似然 $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ 推导得出。对于每个点 $x_i$，使用自然对数计算其在 $K$ 个分量上的分配分布的 Shannon 熵，\n$$\nH(x_i) = -\\sum_{k=1}^K r_{ik} \\log r_{ik}.\n$$\n释义：当多个分量在 $x_i$ 处具有可比较的后验概率时，会出现较大的 $H(x_i)$，这表明存在重叠；当单个分量占主导地位时，会出现较小的 $H(x_i)$。\n\n对于下方的每个测试用例，计算熵 $H(x_i)$ 超过给定阈值 $\\tau$ 的点的比例。将此比例表示为小数（而非百分比）。您的程序必须按照所述方式，通过 Bayes 定理使用多元正态 PDF 来实现责任计算，使用自然对数计算熵，并汇总所有测试用例的最终结果。\n\n测试套件规范：\n- 用例 1（一维，两个分量分离良好）：\n  - $d = 1$， $K = 2$， $\\pi = [0.5, 0.5]$。\n  - $\\mu_1 = -3$， $\\mu_2 = 3$。\n  - $\\Sigma_1 = [0.5]$， $\\Sigma_2 = [0.5]$。\n  - 点 $x_i \\in \\{-4, -3, -2, 2, 3, 4\\}$。\n  - 阈值 $\\tau = 0.3$。\n- 用例 2（一维，两个分量中度重叠）：\n  - $d = 1$， $K = 2$， $\\pi = [0.5, 0.5]$。\n  - $\\mu_1 = 0$， $\\mu_2 = 1$。\n  - $\\Sigma_1 = [1]$， $\\Sigma_2 = [1]$。\n  - 点 $x_i \\in \\{-0.2, 0.5, 0.8, 1.5\\}$。\n  - 阈值 $\\tau = 0.5$。\n- 用例 3（二维，沿一个轴重叠）：\n  - $d = 2$， $K = 2$， $\\pi = [0.5, 0.5]$。\n  - $\\mu_1 = [0, 0]$， $\\mu_2 = [3, 0]$。\n  - $\\Sigma_1 = \\mathrm{diag}([1, 4])$， $\\Sigma_2 = \\mathrm{diag}([1, 4])$。\n  - 点 $x_i \\in \\{[0, 0], [1.5, 0], [3, 0], [1.5, 2], [1.5, -2]\\}$。\n  - 阈值 $\\tau = 0.4$。\n- 用例 4（二维，作为相同分量边界条件的均匀责任）：\n  - $d = 2$， $K = 3$， $\\pi = [\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$。\n  - $\\mu_1 = [0, 0]$， $\\mu_2 = [0, 0]$， $\\mu_3 = [0, 0]$。\n  - $\\Sigma_1 = I$， $\\Sigma_2 = I$， $\\Sigma_3 = I$，其中 $I$ 是 $2 \\times 2$ 单位矩阵。\n  - 点 $x_i \\in \\{[2, 2], [0, 0], [-1, 1], [3, -3]\\}$。\n  - 阈值 $\\tau = 1.0$。\n\n输出要求：\n- 对于每个用例，计算 $H(x_i)  \\tau$ 的点的比例。\n- 您的程序应生成单行输出，其中包含这四个比例，形式为逗号分隔的列表并用方括号括起（例如 `[r_1, r_2, r_3, r_4]`），其中每个 $r_j$ 是一个小数。\n\n假设和约束：\n- 所有 `log` 计算均使用自然对数。\n- 通过使用一个小的正常数对责任进行裁剪以避免其为 $0$，从而确保熵计算中的数值稳定性，避免出现 $\\log 0$。\n- 不涉及物理单位。",
            "solution": "该问题要求实现一个函数，用于计算高斯混合模型 (GMM) 中数据点的分配不确定性，该不确定性由分量责任的 Shannon 熵来量化。解决方案必须从第一性原理推导，包括 Bayes 定理和多元正态分布的定义。\n\n首先，我们建立理论基础。GMM 是一个概率模型，它假设所有数据点都由有限数量的、参数未知的高斯分布的混合生成。对于一个在 $d$ 维空间中有 $K$ 个分量的 GMM，该模型由混合比例 $\\pi_k$、分量均值 $\\mu_k \\in \\mathbb{R}^d$ 和分量协方差矩阵 $\\Sigma_k \\in \\mathbb{R}^{d \\times d}$ 定义，其中 $k \\in \\{1, \\dots, K\\}$。混合比例是非负的，并且总和为一：$\\sum_{k=1}^K \\pi_k = 1$。每个协方差矩阵 $\\Sigma_k$ 都要求是正定的。\n\n在点 $x \\in \\mathbb{R}^d$ 处，单个多元正态分量 $k$ 的概率密度函数 (PDF) 由以下公式给出：\n$$\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu_k)^\\top \\Sigma_k^{-1} (x-\\mu_k)\\right)\n$$\n在这里，$|\\Sigma_k|$ 是协方差矩阵 $\\Sigma_k$ 的行列式，而项 $(x-\\mu_k)^\\top \\Sigma_k^{-1} (x-\\mu_k)$ 是点 $x$ 和均值 $\\mu_k$ 之间的马氏距离（Mahalanobis distance）的平方。\n\n问题的核心是为每个数据点 $x_i$ 确定它由分量 $k$ 生成的后验概率。这个后验概率被称为责任（responsibility），记作 $r_{ik}$。使用 Bayes 定理，责任的计算公式如下：\n$$\nr_{ik} = P(k \\mid x_i) = \\frac{P(x_i \\mid k) P(k)}{P(x_i)} = \\frac{\\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)}\n$$\n在此表达式中，$\\pi_k$ 作为分量 $k$ 的先验概率 $P(k)$，而 $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ 是似然 $P(x_i \\mid k)$。分母 $\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)$ 是数据点 $x_i$ 的边际概率，通常称为证据（evidence），它作为一个归一化常数，确保 $\\sum_{k=1}^K r_{ik} = 1$。\n\n直接计算似然和证据在数值上可能不稳定，尤其是在高维空间中或当指数项导致下溢时。一种标准且稳健的技术是使用对数概率。我们定义点 $x_i$ 和分量 $k$ 的对数加权似然为 $L_{ik}$：\n$$\nL_{ik} = \\log\\left(\\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\right) = \\log(\\pi_k) + \\log\\left(\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\right)\n$$\n对数 PDF 为：\n$$\n\\log\\left(\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\right) = -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma_k| - \\frac{1}{2}(x_i-\\mu_k)^\\top \\Sigma_k^{-1} (x_i-\\mu_k)\n$$\n责任可以用 $L_{ik}$ 表示为 $r_{ik} = \\frac{\\exp(L_{ik})}{\\sum_{j=1}^K \\exp(L_{ij})}$。为了防止指数函数中的数值上溢/下溢，我们使用 log-sum-exp 技巧。设 $L_{\\text{max}, i} = \\max_{j} L_{ij}$。那么，\n$$\nr_{ik} = \\frac{\\exp(L_{ik} - L_{\\text{max}, i})}{\\sum_{j=1}^K \\exp(L_{ij} - L_{\\text{max}, i})}\n$$\n这种计算在数值上是稳定的，因为现在最大的指数是 $0$。\n\n一旦为数据点 $x_i$ 计算出责任 $\\{r_{ik}\\}_{k=1}^K$，它们就构成了在各分量上的一个离散概率分布。这种分配的不确定性由 Shannon 熵来量化，使用自然对数计算：\n$$\nH(x_i) = -\\sum_{k=1}^K r_{ik} \\log r_{ik}\n$$\n当责任是均匀的（例如，对所有 $k$ 都有 $r_{ik} = 1/K$）时，熵最大化，表明不确定性高。当一个分量的责任为 $1$ 而所有其他分量的责任都为 $0$ 时，熵最小化（为 $0$），表明分配是确定的。在计算中，如果 $r_{ik}=0$，则项 $r_{ik} \\log r_{ik}$ 取为 $0$，这对应于极限 $\\lim_{p \\to 0^+} p \\log p = 0$。\n\n最后一步是处理每个给定的测试用例。对于每个用例，我们遍历所有提供的数据点 $\\{x_i\\}$。对于每个点，我们计算其熵 $H(x_i)$。然后，我们统计有多少个点的熵大于指定的阈值 $\\tau$。该用例的结果是这类点相对于总点数的比例。\n\n实现将遵循这一逻辑。我们将创建一个函数，用于在给定 GMM 参数的情况下计算一组点的熵。该函数将为每个点和分量计算对数加权似然，应用 log-sum-exp 技巧来求得责任，然后计算每个点的责任分布的 Shannon 熵。主脚本将对每个测试用例使用此函数，并计算所需的比例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes assignment uncertainty in GMM clustering using Shannon entropy.\n    \"\"\"\n\n    def compute_entropies(X, pis, mus, sigmas):\n        \"\"\"\n        Computes the Shannon entropy of responsibilities for each data point.\n\n        Args:\n            X (np.ndarray): Data points, shape (N, d).\n            pis (np.ndarray): Mixing proportions, shape (K,).\n            mus (np.ndarray): Component means, shape (K, d).\n            sigmas (np.ndarray): Component covariance matrices, shape (K, d, d).\n\n        Returns:\n            np.ndarray: Shannon entropy for each data point, shape (N,).\n        \"\"\"\n        N, d = X.shape\n        K = len(pis)\n\n        # log_weighted_likelihoods will store log(pi_k * N(x_i | mu_k, sigma_k))\n        log_weighted_likelihoods = np.zeros((N, K))\n\n        for k in range(K):\n            pi_k = pis[k]\n            mu_k = mus[k]\n            sigma_k = sigmas[k]\n            \n            # Pre-compute inverse and log-determinant of covariance matrix\n            inv_sigma_k = np.linalg.inv(sigma_k)\n            det_sigma_k = np.linalg.det(sigma_k)\n            log_det_sigma_k = np.log(det_sigma_k)\n\n            # Log of the normalization constant of the multivariate normal PDF\n            log_norm_const = -0.5 * (d * np.log(2. * np.pi) + log_det_sigma_k)\n\n            # Compute Mahalanobis distance for all points to the k-th center\n            X_minus_mu = X - mu_k\n            # (X_minus_mu @ inv_sigma_k) gives shape (N, d)\n            # The element-wise product followed by sum is equivalent to (x-mu)T.inv(S).(x-mu) for each row\n            mahalanobis_sq = np.sum((X_minus_mu @ inv_sigma_k) * X_minus_mu, axis=1)\n\n            # Log-likelihood for each point for component k\n            log_pdf = log_norm_const - 0.5 * mahalanobis_sq\n            \n            # Log of the joint probability P(x, k) = P(x|k)P(k)\n            log_weighted_likelihoods[:, k] = np.log(pi_k) + log_pdf\n\n        # Log-sum-exp trick for numerical stability to compute responsibilities\n        log_sum_exp_offset = np.max(log_weighted_likelihoods, axis=1, keepdims=True)\n        exp_terms = np.exp(log_weighted_likelihoods - log_sum_exp_offset)\n        sum_exp_terms = np.sum(exp_terms, axis=1, keepdims=True)\n        \n        responsibilities = exp_terms / sum_exp_terms\n        \n        # Compute Shannon entropy, handling the case where r_ik = 0.\n        # The term r*log(r) -> 0 as r -> 0.\n        # np.where is used to avoid log(0) which results in -inf, and 0*-inf = nan.\n        entropy_terms = np.where(responsibilities > 0, responsibilities * np.log(responsibilities), 0)\n        entropies = -np.sum(entropy_terms, axis=1)\n        \n        return entropies\n\n    test_cases = [\n        # Case 1 (one-dimensional, well-separated)\n        {\n            \"points\": np.array([[-4.], [-3.], [-2.], [2.], [3.], [4.]]),\n            \"pis\": np.array([0.5, 0.5]),\n            \"mus\": np.array([[-3.], [3.]]),\n            \"sigmas\": np.array([[[0.5]], [[0.5]]]),\n            \"tau\": 0.3\n        },\n        # Case 2 (one-dimensional, overlapping)\n        {\n            \"points\": np.array([[-0.2], [0.5], [0.8], [1.5]]),\n            \"pis\": np.array([0.5, 0.5]),\n            \"mus\": np.array([[0.], [1.]]),\n            \"sigmas\": np.array([[[1.]], [[1.]]]),\n            \"tau\": 0.5\n        },\n        # Case 3 (two-dimensional, overlap along one axis)\n        {\n            \"points\": np.array([[0., 0.], [1.5, 0.], [3., 0.], [1.5, 2.], [1.5, -2.]]),\n            \"pis\": np.array([0.5, 0.5]),\n            \"mus\": np.array([[0., 0.], [3., 0.]]),\n            \"sigmas\": np.array([[[1., 0.], [0., 4.]], [[1., 0.], [0., 4.]]]),\n            \"tau\": 0.4\n        },\n        # Case 4 (two-dimensional, identical components)\n        {\n            \"points\": np.array([[2., 2.], [0., 0.], [-1., 1.], [3., -3.]]),\n            \"pis\": np.array([1./3., 1./3., 1./3.]),\n            \"mus\": np.array([[0., 0.], [0., 0.], [0., 0.]]),\n            \"sigmas\": np.array([[[1., 0.], [0., 1.]], [[1., 0.], [0., 1.]], [[1., 0.], [0., 1.]]]),\n            \"tau\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"points\"]\n        pis = case[\"pis\"]\n        mus = case[\"mus\"]\n        sigmas = case[\"sigmas\"]\n        tau = case[\"tau\"]\n\n        entropies = compute_entropies(X, pis, mus, sigmas)\n        \n        num_points_above_threshold = np.sum(entropies > tau)\n        fraction = num_points_above_threshold / len(X)\n        results.append(fraction)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在实际应用中，一个至关重要的问题是：我们是否有足够的数据来可靠地区分不同的簇，尤其当它们非常接近时？本练习将引导你设计一个计算实验来回答这个问题。你将使用贝叶斯信息准则（BIC）作为模型选择的依据，通过模拟和重复检验，探究在不同簇间距下，可靠地识别出两个组分所需的最小样本量。这个过程将加深你对模型复杂性、数据量和统计显著性之间权衡的理解。",
            "id": "3122550",
            "problem": "您需要设计并实现一个完整的、可运行的程序，该程序使用贝叶斯信息准则（BIC）来经验性地量化在高斯混合模型中可靠分离两个间距很近的组分所需的样本量。背景是统计学习，重点是用于聚类的高斯混合模型。在您的推导和算法设计中，应使用的基本依据是高斯模型的最大似然估计以及作为对数边际似然的大样本近似的贝叶斯信息准则。\n\n考虑一个在$d$维空间中的双组分多元高斯分布混合模型，其参数包括组分均值$\\mu_1,\\mu_2 \\in \\mathbb{R}^d$，正定的组分协方差矩阵$\\Sigma_1,\\Sigma_2 \\in \\mathbb{R}^{d \\times d}$，以及混合权重$\\pi_1,\\pi_2 \\in (0,1)$且$\\pi_1 + \\pi_2 = 1$。数据由从该混合模型中独立抽取的$n$个样本组成。这些组分几乎相同，即欧几里得距离$\\|\\mu_1 - \\mu_2\\|$远小于由$\\sqrt{\\lambda_{\\max}(\\Sigma)}$量化的协方差主导尺度，其中$\\lambda_{\\max}(\\Sigma)$表示协方差矩阵的最大特征值。\n\n您的程序必须为每个测试用例执行以下任务：\n\n1.  从一个双组分高斯混合模型中模拟$n$个$\\mathbb{R}^d$空间中的独立样本。该模型具有相等的混合权重$\\pi_1 = \\pi_2 = \\frac{1}{2}$，相同的协方差矩阵$\\Sigma_1 = \\Sigma_2 = \\Sigma$，以及沿第一个坐标轴分离的均值$\\mu_1 = \\left(-\\frac{\\delta}{2}, 0, \\dots, 0\\right)$和$\\mu_2 = \\left(\\frac{\\delta}{2}, 0, \\dots, 0\\right)$，其中$\\delta = \\|\\mu_1 - \\mu_2\\|$是分离幅度。\n\n2.  通过最大似然法为每个模拟数据集拟合两个竞争模型：\n    -   一个单一多元高斯模型，其均值$\\mu$和协方差$\\Sigma$从完整数据集中估计。\n    -   一个双组分高斯混合模型，其参数$(\\pi_1,\\pi_2,\\mu_1,\\mu_2,\\Sigma_1,\\Sigma_2)$使用期望最大化（EM）算法进行估计。\n\n3.  为每个拟合的模型计算贝叶斯信息准则（BIC）。对于一个具有最大化对数似然$\\ell(\\hat{\\theta})$和$p$个自由参数、拟合于$n$个观测值的模型，BIC定义为$ \\mathrm{BIC} = -2 \\, \\ell(\\hat{\\theta}) + p \\, \\log(n) $。对于$d$维的单一高斯模型，参数数量为$p_{\\text{single}} = d + \\frac{d(d+1)}{2}$。对于$d$维的双组分全协方差高斯混合模型，参数数量为$p_{\\text{mix}} = 2d + 2 \\cdot \\frac{d(d+1)}{2} + (2-1) = 2d + d(d+1) + 1$。\n\n4.  如果双组分模型的BIC严格小于单一高斯模型的BIC，即$\\mathrm{BIC}_{\\text{mix}}  \\mathrm{BIC}_{\\text{single}}$，则定义为双组分模型的一次“成功”。对于给定的$n$，重复模拟和拟合$T$次，以估计成功的比例。如果成功比例至少达到可靠性阈值$\\tau$（以小数表示），则声明$n$在该测试用例下实现了可靠分离。\n\n5.  对于每个测试用例，在一个固定的候选样本量集合中搜索，找到实现可靠分离的最小$n$。如果没有任何候选值能实现可靠分离，则该测试用例返回$-1$。\n\n使用以下测试套件来探测不同程度的组分邻近度，并覆盖从非常困难到较容易的分离情形。所有量必须完全按照规定处理。\n\n-   所有用例的通用设置：维度$d = 2$，协方差矩阵$\\Sigma = s^2 I_2$，其中$s = 1.0$，因此$\\sqrt{\\lambda_{\\max}(\\Sigma)} = s = 1.0$，相等的混合权重$\\pi_1 = \\pi_2 = \\frac{1}{2}$，试验次数$T = 12$，可靠性阈值$\\tau = 0.75$，候选样本量$n \\in \\{60, 120, 240, 360\\}$。\n-   用例 A（非常小的分离）：$\\delta = 0.10$。\n-   用例 B（小分离）：$\\delta = 0.20$。\n-   用例 C（相对于$s$的中等分离）：$\\delta = 0.30$。\n\n您的程序必须为双组分混合模型实现一个统计上稳健的期望最大化过程，稳健地计算两个模型的对数似然，应用上述参数数量计算BIC，并执行重复试验的可靠性评估。允许使用数值稳定性措施，例如在协方差估计中添加小的对角正则化项。不允许使用外部数据或用户输入。\n\n最终输出格式：您的程序应生成单行输出，其中包含三个用例实现可靠分离的最小样本量，格式为一个由方括号括起来的、用逗号分隔的列表，例如$[n_A,n_B,n_C]$，其中每个$n_\\cdot$是来自候选集的整数，如果没有任何候选值满足可靠性阈值，则为$-1$。",
            "solution": "该问题要求进行一项经验性研究，探讨需要多大的样本量$n$才能使用贝叶斯信息准则（BIC）可靠地区分一个双组分高斯混合模型（GMM）与一个更简单的单组分高斯模型。问题的设定是混合模型的组分间距很近，这使得模型选择任务变得不简单。\n\n该方法论涉及一系列计算实验。对于由特定组分均值间距$\\delta$定义的每个测试用例，我们在给定的候选集$\\{60, 120, 240, 360\\}$中搜索最小的样本量，该样本量能让更复杂的双组分模型在$T=12$次独立试验中以至少$\\tau = 0.75$的可靠性被BIC所偏好。\n\n**1. 数据模拟**\n\n在每次试验中，我们在一个$d=2$维空间中模拟一个包含$n$个点的数据集。数据从一个具有以下参数的双组分GMM中抽取：\n- 混合权重：$\\pi_1 = \\pi_2 = 0.5$。\n- 组分均值：$\\mu_1 = \\left(-\\frac{\\delta}{2}, 0\\right)^T$ 和 $\\mu_2 = \\left(\\frac{\\delta}{2}, 0\\right)^T$。参数$\\delta$控制组分之间的分离程度。\n- 组分协方差：$\\Sigma_1 = \\Sigma_2 = \\Sigma = s^2 I_2$，其中$s=1.0$，$I_2$是$2 \\times 2$的单位矩阵。这意味着组分是各向同性且相同的球形组分。\n\n为生成大小为$n$的样本，大约有$n/2$个点从高斯分布$\\mathcal{N}(\\mu_1, \\Sigma)$中抽取，另外$n/2$个点从$\\mathcal{N}(\\mu_2, \\Sigma)$中抽取。在任何给定的试验中，来自每个组分的确切点数由$n$次概率为$0.5$的独立伯努利试验决定。\n\n**2. 模型拟合与评估**\n\n使用最大似然估计（MLE）将两个模型拟合到每个模拟数据集上。\n\n**模型A：单一多元高斯模型**\n将一个单一高斯分布$\\mathcal{N}(\\mu, \\Sigma_{\\text{single}})$拟合到包含$n$个点的整个数据集上。MLE参数是样本均值$\\hat{\\mu}$和样本协方差矩阵$\\hat{\\Sigma}_{\\text{single}}$：\n$$\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n$$\n$$\n\\hat{\\Sigma}_{\\text{single}} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{\\mu})(x_i - \\hat{\\mu})^T\n$$\n该模型的最大化对数似然$\\ell_{\\text{single}}$可以使用闭式表达式计算：\n$$\n\\ell_{\\text{single}} = -\\frac{n}{2} \\left( d \\log(2\\pi) + \\log|\\det(\\hat{\\Sigma}_{\\text{single}})| + d \\right)\n$$\n在维度$d=2$下，该模型的自由参数数量为$p_{\\text{single}} = d + \\frac{d(d+1)}{2} = 2 + \\frac{2(3)}{2} = 5$。\n\n**模型B：双组分高斯混合模型**\n将一个双组分GMM拟合到数据上。由于MLE没有闭式解，参数$\\theta = \\{\\pi_1, \\pi_2, \\mu_1, \\mu_2, \\Sigma_1, \\Sigma_2\\}$使用期望最大化（EM）算法进行估计。\n\n- **初始化**：EM算法的性能对初始参数的选择敏感。为获得稳健的初始化，我们运行几次简单的k-均值聚类算法（$k=2$）。将得到的簇均值、比例和协方差作为EM算法的起点。\n\n- **期望步骤（E-Step）**：给定当前参数估计$\\theta^{(t)}$，计算数据点$x_i$由组分$k$生成的后验概率（或称“责任”）：\n$$\n\\gamma_{ik}^{(t+1)} = \\frac{\\pi_k^{(t)} \\mathcal{N}(x_i | \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{j=1}^2 \\pi_j^{(t)} \\mathcal{N}(x_i | \\mu_j^{(t)}, \\Sigma_j^{(t)})}\n$$\n\n- **最大化步骤（M-Step）**：使用计算出的责任更新参数，以最大化期望对数似然：\n$$\nN_k^{(t+1)} = \\sum_{i=1}^n \\gamma_{ik}^{(t+1)}\n$$\n$$\n\\pi_k^{(t+1)} = \\frac{N_k^{(t+1)}}{n}\n$$\n$$\n\\mu_k^{(t+1)} = \\frac{1}{N_k^{(t+1)}} \\sum_{i=1}^n \\gamma_{ik}^{(t+1)} x_i\n$$\n$$\n\\Sigma_k^{(t+1)} = \\frac{1}{N_k^{(t+1)}} \\sum_{i=1}^n \\gamma_{ik}^{(t+1)} (x_i - \\mu_k^{(t+1)})(x_i - \\mu_k^{(t+1)})^T\n$$\n为确保数值稳定性并防止奇异性，在每次更新后向协方差矩阵添加一个小的正则化项（$\\epsilon I_d$，其中$\\epsilon = 10^{-6}$）。\n\n- **收敛**：迭代执行E步骤和M步骤，直到总对数似然$\\ell_{\\text{mix}} = \\sum_{i=1}^n \\log\\left( \\sum_{k=1}^2 \\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k) \\right)$的变化量小于一个小的容差。\n\n在维度$d=2$下，这个全协方差GMM的自由参数数量为$p_{\\text{mix}} = 2d + 2 \\cdot \\frac{d(d+1)}{2} + (2-1) = 4 + 6 + 1 = 11$。\n\n**3. 通过BIC进行模型选择**\n\n使用贝叶斯信息准则比较这两个拟合的模型。对于一个有$p$个参数和最大化对数似然$\\ell$的模型，BIC为：\n$$\n\\mathrm{BIC} = -2 \\ell + p \\log(n)\n$$\nBIC值较低的模型更优。因此，如果$\\mathrm{BIC}_{\\text{mix}}  \\mathrm{BIC}_{\\text{single}}$，则认为双组分GMM“成功”。该条件可以表示为对数似然增益的一个下界：\n$$\n2(\\ell_{\\text{mix}} - \\ell_{\\text{single}}) > (p_{\\text{mix}} - p_{\\text{single}})\\log(n)\n$$\n右侧代表了对GMM增加的复杂度的惩罚，GMM比单一高斯模型多了$\\Delta p = p_{\\text{mix}} - p_{\\text{single}} = 6$个参数。\n\n**4. 经验性搜索最小样本量**\n\n对于每个测试用例（即每个$\\delta$值），程序按升序遍历候选样本量$n \\in \\{60, 120, 240, 360\\}$。对于每个$n$，它执行$T=12$次模拟与拟合试验。它计算GMM成功的次数（即$\\mathrm{BIC}_{\\text{mix}}  \\mathrm{BIC}_{\\text{single}}$）。如果成功比例至少达到可靠性阈值$\\tau=0.75$（即12次中至少成功9次），则认为该样本量$n$足以实现可靠分离。程序返回满足此标准的第一个（因此是最小的）$n$。如果没有候选$n$满足阈值，则返回$-1$。对三个指定的$\\delta$值中的每一个重复此整个过程。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import multivariate_normal\nfrom scipy.special import logsumexp\n\ndef fit_gmm_em(data, d, k, max_iter=100, tol=1e-4):\n    \"\"\"\n    Fits a k-component Gaussian Mixture Model using the EM algorithm.\n    \"\"\"\n    n, _ = data.shape\n    reg_cov = 1e-6\n\n    # Initialization using k-means\n    try:\n        # 1. Randomly initialize centers\n        centers = data[np.random.choice(n, k, replace=False)]\n        \n        # 2. A few k-means iterations\n        for _ in range(10):\n            distances = np.linalg.norm(data[:, np.newaxis, :] - centers[np.newaxis, :, :], axis=2)\n            labels = np.argmin(distances, axis=1)\n            \n            new_centers = np.array([data[labels == j].mean(axis=0) if np.sum(labels == j) > 0 else centers[j] for j in range(k)])\n\n            if np.allclose(centers, new_centers):\n                break\n            centers = new_centers\n\n        mus = centers\n        labels = np.argmin(np.linalg.norm(data[:, np.newaxis, :] - mus, axis=2), axis=1) # final assignment\n        \n        pis = np.array([np.mean(labels == j) for j in range(k)])\n        \n        sigmas = []\n        for j in range(k):\n            cluster_points = data[labels == j]\n            if len(cluster_points)  d:\n                # Not enough points, use global covariance\n                cov_j = np.cov(data, rowvar=False, bias=True)\n            else:\n                cov_j = np.cov(cluster_points, rowvar=False, bias=True)\n            sigmas.append(cov_j + reg_cov * np.identity(d))\n        sigmas = np.array(sigmas)\n\n    except (ValueError, np.linalg.LinAlgError):\n        return -np.inf\n\n    ll_old = -np.inf\n    for _ in range(max_iter):\n        # E-step\n        try:\n            log_probs = np.zeros((n, k))\n            for j in range(k):\n                log_probs[:, j] = np.log(pis[j]) + multivariate_normal.logpdf(data, mean=mus[j], cov=sigmas[j])\n        except (ValueError, np.linalg.LinAlgError):\n            return -np.inf\n\n        log_sum_probs = logsumexp(log_probs, axis=1)\n        log_responsibilities = log_probs - log_sum_probs[:, np.newaxis]\n        responsibilities = np.exp(log_responsibilities)\n\n        # M-step\n        nk = np.sum(responsibilities, axis=0)\n        \n        # Check for collapsed components\n        if np.any(nk  1e-9):\n            return -np.inf\n\n        pis = nk / n\n        mus = np.dot(responsibilities.T, data) / nk[:, np.newaxis]\n        \n        for j in range(k):\n            diff = data - mus[j]\n            weighted_diff = diff * np.sqrt(responsibilities[:, j])[:, np.newaxis]\n            sigmas[j] = (weighted_diff.T @ weighted_diff) / nk[j] + reg_cov * np.identity(d)\n        \n        # Convergence check\n        ll_new = np.sum(log_sum_probs)\n        if ll_new - ll_old  tol and ll_new > -np.inf:\n            break\n        ll_old = ll_new\n\n    if not np.isfinite(ll_old):\n        return -np.inf\n        \n    return ll_old\n\ndef log_likelihood_single_gaussian(data, d):\n    \"\"\"\n    Computes the maximized log-likelihood for a single Gaussian model.\n    \"\"\"\n    n, _ = data.shape\n    if n  2:\n        return -np.inf\n    \n    mean = np.mean(data, axis=0)\n    cov = np.cov(data, rowvar=False, bias=True)\n    cov += 1e-6 * np.identity(d) # Regularization\n\n    try:\n        sign, logdet = np.linalg.slogdet(cov)\n        if sign = 0:\n            return -np.inf\n    except np.linalg.LinAlgError:\n        return -np.inf\n\n    ll = -0.5 * n * (d * np.log(2 * np.pi) + logdet + d)\n    return ll\n\ndef find_minimal_n(delta, d, cov_matrix, T, tau, n_candidates):\n    \"\"\"\n    Searches for the minimal sample size n that achieves reliable separation.\n    \"\"\"\n    mu1 = np.zeros(d)\n    mu1[0] = -delta / 2\n    mu2 = np.zeros(d)\n    mu2[0] = delta / 2\n    \n    p_single = d + d * (d + 1) / 2\n    p_mix = 1 + 2 * d + d * (d + 1) # for k=2 components\n\n    for n in sorted(n_candidates):\n        win_count = 0\n        for _ in range(T):\n            # 1. Simulate data\n            n1 = np.random.binomial(n, 0.5)\n            n2 = n - n1\n            data1 = np.random.multivariate_normal(mu1, cov_matrix, size=n1)\n            data2 = np.random.multivariate_normal(mu2, cov_matrix, size=n2)\n            data = np.vstack((data1, data2)) if n1 > 0 and n2 > 0 else (data1 if n1 > 0 else data2)\n            np.random.shuffle(data)\n\n            # 2. Fit single Gaussian model\n            ll_single = log_likelihood_single_gaussian(data, d)\n            if not np.isfinite(ll_single):\n                continue\n            bic_single = -2 * ll_single + p_single * np.log(n)\n\n            # 3. Fit two-component GMM\n            ll_mix = fit_gmm_em(data, d, k=2)\n            if not np.isfinite(ll_mix):\n                continue\n            bic_mix = -2 * ll_mix + p_mix * np.log(n)\n\n            # 4. Compare BIC\n            if bic_mix  bic_single:\n                win_count += 1\n        \n        # 5. Check reliability\n        win_fraction = win_count / T\n        if win_fraction >= tau:\n            return n\n            \n    return -1\n\ndef solve():\n    # Set a seed for reproducibility.\n    np.random.seed(42)\n\n    # Common settings from problem statement\n    d = 2\n    s = 1.0\n    cov_matrix = np.identity(d) * (s**2)\n    T = 12\n    tau = 0.75\n    n_candidates = [60, 120, 240, 360]\n    \n    # Test cases\n    test_cases = [\n        {'name': 'Case A', 'delta': 0.10},\n        {'name': 'Case B', 'delta': 0.20},\n        {'name': 'Case C', 'delta': 0.30},\n    ]\n\n    results = []\n    for case in test_cases:\n        delta = case['delta']\n        min_n = find_minimal_n(delta, d, cov_matrix, T, tau, n_candidates)\n        results.append(min_n)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}