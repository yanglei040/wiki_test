{
    "hands_on_practices": [
        {
            "introduction": "核主成分分析 (KPCA) 的一个核心操作是将新数据点投影到已学习的主成分上。这个练习将指导你手动完成这一过程，让你深入理解样本外扩展 (out-of-sample extension) 的内部机制。通过处理一个具体的核矩阵，你将亲身体验如何在特征空间中正确地进行数据中心化，并计算出新数据点的坐标。",
            "id": "3136620",
            "problem": "考虑一个包含 $n=3$ 个点的训练集，其核函数 $k(\\cdot,\\cdot)$ 是对称半正定核。令 $K \\in \\mathbb{R}^{3 \\times 3}$ 表示未中心化的格拉姆矩阵(Gram matrix)，其元素为 $K_{ij} = k(x_i, x_j)$，由下式给出\n$$\nK \\;=\\; \\begin{pmatrix}\n2  1  1 \\\\\n1  2  1 \\\\\n1  1  2\n\\end{pmatrix}.\n$$\n令 $H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ 为中心化矩阵，其中 $\\mathbf{1} \\in \\mathbb{R}^3$ 是全为1的向量。中心化的格拉姆矩阵为 $K_c = H K H$。已知 $K_c$ 的非零特征对为\n$$\n\\lambda_1 \\,=\\, 1,\\quad u_1 \\,=\\, \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix},\\qquad\n\\lambda_2 \\,=\\, 1,\\quad u_2 \\,=\\, \\frac{1}{\\sqrt{6}}\\begin{pmatrix}1\\\\1\\\\-2\\end{pmatrix},\n$$\n其中 $u_1$ 和 $u_2$ 是标准正交的，剩余的特征值等于 $0$，其对应的特征向量与 $\\mathbf{1}$ 成比例。\n\n观测到一个新点 $x_{\\star}$，其与训练点的核函数求值结果如下：\n$$\nk_{\\star} \\;=\\; \\begin{pmatrix}\nk(x_1, x_{\\star}) \\\\ k(x_2, x_{\\star}) \\\\ k(x_3, x_{\\star})\n\\end{pmatrix} \\;=\\; \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\n\n使用核主成分分析 (KPCA)，在标准的样本外扩展下，计算新点 $x_{\\star}$ 的第一主坐标 $z_1(x_{\\star})$。该扩展正确地考虑了使用训练集进行的特征空间中心化。将最终答案表示为单个精确值。不需要四舍五入，也不涉及单位。",
            "solution": "本题的任务是使用核主成分分析 (KPCA) 计算新数据点 $x_{\\star}$ 的第一主坐标 $z_1(x_{\\star})$。该坐标是 $x_{\\star}$ 的中心化特征向量在特征空间中第一主成分方向上的投影。\n\n令 $\\Phi: \\mathcal{X} \\to \\mathcal{F}$ 是与核函数 $k$ 相关联的特征映射，使得 $k(x, y) = \\Phi(x)^{\\top}\\Phi(y)$。训练数据点为 $\\{x_1, x_2, x_3\\}$。训练数据在特征空间中的均值为 $\\bar{\\Phi} = \\frac{1}{n} \\sum_{i=1}^n \\Phi(x_i)$。\n\n新点 $x_{\\star}$ 的第 $k$ 个主坐标 $z_k(x_{\\star})$，是其中心化特征向量 $\\tilde{\\Phi}(x_{\\star}) = \\Phi(x_{\\star}) - \\bar{\\Phi}$ 在第 $k$ 个主成分方向 $v_k$ 上的投影。\n$v_k$ 是特征空间中的单位向量，可以表示为 $v_k = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{i=1}^n (\\alpha^{(k)})_i \\tilde{\\Phi}(x_i)$，其中 $\\lambda_k$ 和 $\\alpha^{(k)}$ 分别是中心化格拉姆矩阵 $K_c$ 的特征值和单位范数特征向量。\n\n因此，投影为：\n$$ z_k(x_{\\star}) = \\langle v_k, \\tilde{\\Phi}(x_{\\star}) \\rangle = \\frac{1}{\\sqrt{\\lambda_k}} \\sum_{i=1}^n (\\alpha^{(k)})_i \\langle \\tilde{\\Phi}(x_i), \\tilde{\\Phi}(x_{\\star}) \\rangle $$\n将内积 $\\langle \\tilde{\\Phi}(x_i), \\tilde{\\Phi}(x_{\\star}) \\rangle$ 展开，并利用 $\\alpha^{(k)}$ 与全1向量正交的性质（即 $\\sum_i (\\alpha^{(k)})_i = 0$，因为 $\\lambda_k \\neq 0$）进行化简，可得到用于计算的表达式：\n$$ z_k(x_{\\star}) = \\frac{1}{\\sqrt{\\lambda_k}} \\left( \\sum_{i=1}^n (\\alpha^{(k)})_i k(x_i, x_{\\star}) - \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^n (\\alpha^{(k)})_i K_{ij} \\right) $$\n这个表达式可以使用矩阵和向量表示法紧凑地写出。令 $k_{\\star}$ 为列向量，其元素为 $(k_{\\star})_i = k(x_i, x_{\\star})$，并令 $\\mathbf{1}$ 为全为1的列向量。\n$$ z_k(x_{\\star}) = \\frac{1}{\\sqrt{\\lambda_k}} \\left( (\\alpha^{(k)})^{\\top} k_{\\star} - \\frac{1}{n} (\\alpha^{(k)})^{\\top} K \\mathbf{1} \\right) $$\n我们已知以下值：\n$n=3$。\n$\\lambda_1 = 1$。\n$K_c$ 的第一个单位范数特征向量是 $\\alpha^{(1)} = u_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}$。\n新点的核函数求值结果为 $k_{\\star} = \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix}$。\n未中心化的格拉姆矩阵是 $K = \\begin{pmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{pmatrix}$。\n\n我们继续计算 $z_1(x_{\\star})$ 表达式中的两项。\n\n第一项：$(\\alpha^{(1)})^{\\top} k_{\\star}$\n$$ (\\alpha^{(1)})^{\\top} k_{\\star} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  -1  0\\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} (1 \\cdot 3 + (-1) \\cdot 1 + 0 \\cdot 1) = \\frac{1}{\\sqrt{2}}(3 - 1) = \\frac{2}{\\sqrt{2}} = \\sqrt{2} $$\n\n第二项：$\\frac{1}{n} (\\alpha^{(1)})^{\\top} K \\mathbf{1}$\n首先，我们计算乘积 $K\\mathbf{1}$：\n$$ K\\mathbf{1} = \\begin{pmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2+1+1 \\\\ 1+2+1 \\\\ 1+1+2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 4 \\\\ 4 \\end{pmatrix} = 4\\mathbf{1} $$\n现在，我们计算 $(\\alpha^{(1)})^{\\top} K \\mathbf{1}$：\n$$ (\\alpha^{(1)})^{\\top} (4\\mathbf{1}) = 4 (\\alpha^{(1)})^{\\top} \\mathbf{1} = 4 \\left( \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  -1  0\\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{4}{\\sqrt{2}}(1 \\cdot 1 - 1 \\cdot 1 + 0 \\cdot 1) = \\frac{4}{\\sqrt{2}}(0) = 0 $$\n这一项为零是符合预期的。特征向量 $\\alpha^{(1)}$ 对应于 $K_c = HKH$ 的一个非零特征值 $\\lambda_1=1$。这意味着 $\\alpha^{(1)}$ 位于 $H$ 的列空间中，而 $H$ 的列空间是与向量 $\\mathbf{1}$ 正交的向量空间。因此，点积 $(\\alpha^{(1)})^{\\top} \\mathbf{1}$ 必须为零。\n\n所以，$z_1(x_{\\star})$ 表达式中的第二项是 $\\frac{1}{3} \\cdot 0 = 0$。\n\n最后，我们将这些值代回 $z_1(x_{\\star})$ 的方程中：\n$$ z_1(x_{\\star}) = \\frac{1}{\\sqrt{1}} (\\sqrt{2} - 0) = \\sqrt{2} $$\n$x_{\\star}$ 的第一主坐标是 $\\sqrt{2}$。",
            "answer": "$$\\boxed{\\sqrt{2}}$$"
        },
        {
            "introduction": "理论知识的最终目的是解决实际问题，而降噪是 KPCA 的一个经典应用。在这个练习中，你将探索如何利用 KPCA 识别数据中的非线性结构（一个抛物线），并将其用于去除噪声。此过程的核心挑战在于“原像问题”(pre-image problem)，即如何将特征空间中的降噪后结果映射回原始数据空间，你将通过一个迭代算法来解决这个问题。",
            "id": "3136635",
            "problem": "您必须实现一个完整的程序，该程序构建一个示例数据集，其中核主成分分析 (KPCA) 的第一主成分对应于一个已知的非线性函数，然后使用预像近似来恢复该函数。该已知非线性函数为平方函数，定义为 $f(x) = x^{2}$。数据集由二维点 $(x,y)$ 构成，其中 $y = f(x) + \\varepsilon$，$\\varepsilon$ 为加性高斯噪声。您将使用径向基函数 (RBF) 核，并在 KPCA 中保留单个主成分，这将产生一条预期会遵循 $y \\approx f(x)$ 图像的一维非线性主曲线。然后，您将计算每个训练点投影到该第一主成分上之后的预像，以获得 $y \\approx f(x)$ 的去噪估计。\n\n基本出发点和定义：\n- 核主成分分析 (KPCA) 是在数据集 $\\{x_{i}\\}_{i=1}^{n}$ 上，通过由特征映射 $\\phi(\\cdot)$ 导出的半正定核 $k(x,z) = \\langle \\phi(x), \\phi(z) \\rangle$ 以及中心化核矩阵 $K_{c}$ 来定义的。中心化核由 $K_{c} = H K H$ 给出，其中 $H = I_{n} - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{\\top}$ 且 $K_{ij} = k(x_{i}, x_{j})$。\n- 径向基函数 (RBF) 核定义为 $k(x,z) = \\exp\\!\\left(-\\frac{\\lVert x - z \\rVert^{2}}{2\\sigma^{2}}\\right)$，其中 $\\sigma > 0$ 是带宽参数。\n- KPCA 的主成分通过求解特征值问题 $K_{c} v_{\\ell} = \\lambda_{\\ell} v_{\\ell}$ 获得，并进行排序以使 $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{n} \\ge 0$，其中 $v_{\\ell}$ 被归一化为单位欧几里得范数。对于一个训练点 $x_{k}$，其在主成分 $\\ell$ 上的坐标为 $y_{\\ell}(x_{k}) = \\sqrt{\\lambda_{\\ell}}\\, v_{\\ell,k}$。\n- 训练点 $x_{k}$ 在特征空间中的秩为 $m$ 的重构可以写为 $\\hat{\\Phi}(x_{k}) = \\sum_{i=1}^{n} a_{i}^{(k)} \\Phi_{c}(x_{i})$，其中 $a_{i}^{(k)} = \\sum_{\\ell=1}^{m} v_{\\ell,i} v_{\\ell,k}$。在本问题中，您必须使用 $m = 1$，因此 $a_{i}^{(k)} = v_{1,i} v_{1,k}$。\n- 预像问题旨在寻找一个 $\\hat{x}$，使得 $\\phi(\\hat{x})$ 在特征空间中尽可能接近 $\\hat{\\Phi}(x_{k})$。对于 RBF 核，一个经过充分测试的预像不动点近似方法是\n$$\n\\hat{x}^{(t+1)} = \\frac{\\sum_{j=1}^{n} \\gamma_{j}^{(t)} x_{j}}{\\sum_{j=1}^{n} \\gamma_{j}^{(t)}}, \n\\quad \\text{其中} \\quad \n\\gamma_{j}^{(t)} = a_{j}^{(k)} \\exp\\!\\left(-\\frac{\\lVert \\hat{x}^{(t)} - x_{j} \\rVert^{2}}{2\\sigma^{2}}\\right).\n$$\n您应将 $\\hat{x}^{(0)} = x_{k}$ 作为初始值，并迭代直至收敛。\n\n程序要求：\n1. 数据生成：对于每个测试用例，生成 $n$ 个独立样本 $x_{i} \\sim \\mathrm{Uniform}([-1,1])$，设置 $y_{i}^{\\mathrm{true}} = f(x_{i}) = x_{i}^{2}$，并观测 $y_{i} = y_{i}^{\\mathrm{true}} + \\varepsilon_{i}$，其中 $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{noise}}^{2})$。构建点 $X_{i} = (x_{i}, y_{i}) \\in \\mathbb{R}^{2}$。\n2. 核计算：在二维输入 $X_{i} \\in \\mathbb{R}^{2}$ 上使用 RBF 核 $k(x,z) = \\exp\\!\\left(-\\frac{\\lVert x - z \\rVert^{2}}{2\\sigma^{2}}\\right)$。计算 $n \\times n$ 的核矩阵 $K$ 并将其中心化以获得 $K_{c}$。\n3. KPCA：求解 $K_{c} v_{\\ell} = \\lambda_{\\ell} v_{\\ell}$ 并选择具有最大特征值 $\\lambda_{1}$ 的第一主成分 $(\\lambda_{1}, v_{1})$。使用 $m=1$。\n4. 重构系数：对于每个训练索引 $k \\in \\{1,\\dots,n\\}$，形成 $a^{(k)} \\in \\mathbb{R}^{n}$，其条目为 $a_{i}^{(k)} = v_{1,i} v_{1,k}$。\n5. 预像近似：对于每个 $k$，通过不动点迭代计算 $\\hat{x}_{k} \\in \\mathbb{R}^{2}$\n$$\n\\hat{x}_{k}^{(t+1)} = \\frac{\\sum_{j=1}^{n} \\gamma_{j}^{(t)} X_{j}}{\\sum_{j=1}^{n} \\gamma_{j}^{(t)}}, \n\\quad \\gamma_{j}^{(t)} = a_{j}^{(k)} \\exp\\!\\left(-\\frac{\\lVert \\hat{x}_{k}^{(t)} - X_{j} \\rVert^{2}}{2\\sigma^{2}}\\right),\n$$\n在 $\\hat{x}_{k}^{(0)} = X_{k}$ 处初始化，迭代直到 $\\lVert \\hat{x}_{k}^{(t+1)} - \\hat{x}_{k}^{(t)} \\rVert_{2}  \\varepsilon$ 或达到最大迭代次数 $T$。使用容差 $\\varepsilon = 10^{-8}$ 和 $T = 200$。\n6. 函数恢复与评估：使用 $\\hat{x}_{k}$ 的第二个坐标作为 $\\hat{y}_{k}$，即您对 $f(x_{k})$ 的估计。计算去噪前和去噪后的均方根误差 (RMSE)：\n$$\n\\mathrm{RMSE}_{\\mathrm{before}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} \\left(y_{k} - y_{k}^{\\mathrm{true}}\\right)^{2}}, \n\\quad \n\\mathrm{RMSE}_{\\mathrm{after}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} \\left(\\hat{y}_{k} - y_{k}^{\\mathrm{true}}\\right)^{2}}.\n$$\n\n测试套件：\n在以下三个测试用例上运行您的程序，每个用例由 $(n, \\sigma, \\sigma_{\\mathrm{noise}}, s)$ 指定，其中 $s$ 是用于 $x_{i}$ 采样和高斯噪声的随机种子。\n- 用例 1：$(n, \\sigma, \\sigma_{\\mathrm{noise}}, s) = (80, 0.15, 0.05, 0)$。\n- 用例 2 (边界情况，无噪声)：$(n, \\sigma, \\sigma_{\\mathrm{noise}}, s) = (60, 0.12, 0.0, 1)$。\n- 用例 3 (更小带宽，更高噪声)：$(n, \\sigma, \\sigma_{\\mathrm{noise}}, s) = (80, 0.05, 0.10, 2)$。\n\n最终输出格式：\n- 对于每个用例，输出保留 $6$ 位小数的配对 $[\\mathrm{RMSE}_{\\mathrm{before}}, \\mathrm{RMSE}_{\\mathrm{after}}]$。\n- 您的程序应生成单行输出，其中包含一个以逗号分隔的列表形式的结果，并用方括号括起来，例如 `[[a,b],[c,d],[e,f]]`，不含空格。\n\n角度单位、物理单位和百分比不适用于此问题；所有量均为无量纲实数。请遵循上述精确定义，不使用任何未从所述基础中推导出的特定启发式方法或捷径，以确保科学真实性。",
            "solution": "用户提供了一个在统计学习领域内定义明确的计算问题。任务是实现核主成分分析 (KPCA) 以对一个合成数据集进行去噪，并评估其性能。该问题具有科学依据，内部一致，并包含解决问题所需的所有必要信息。我将按步骤进行求解。\n\n核心目标是证明 KPCA 能够识别非线性数据流形——在本例中为抛物线——并且通过将含噪数据投影到这个学习到的流形上，然后在原始空间中重构这些点（预像问题），我们可以有效地降低噪声。\n\n### 1. 数据生成\n\n第一步是根据问题规范生成一个合成数据集。对于每个测试用例，我们给定数据点数量 $n$、噪声标准差 $\\sigma_{\\mathrm{noise}}$ 以及用于可复现性的随机种子 $s$。我们从区间 $[-1, 1]$ 上的均匀分布中生成 $n$ 个独立样本 $x_i$。真实的底层函数是 $f(x) = x^2$。“真实”数据点为 $(x_i, y_i^{\\mathrm{true}})$，其中 $y_i^{\\mathrm{true}} = x_i^2$。然后我们通过添加高斯噪声来创建观测数据集：观测点为 $X_i = (x_i, y_i) \\in \\mathbb{R}^2$，其中 $y_i = y_i^{\\mathrm{true}} + \\varepsilon_i$，且 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{noise}}^2)$。该数据集 $\\{X_i\\}_{i=1}^n$ 形成一个含噪的抛物线。\n\n### 2. 核矩阵构建与中心化\n\nKPCA 在一个由核函数隐式定义的高维特征空间中运行。指定的核是径向基函数 (RBF) 核：\n$$k(x, z) = \\exp\\left(-\\frac{\\lVert x - z \\rVert^2}{2\\sigma^2}\\right)$$\n其中 $\\sigma$ 是带宽参数。我们计算 $n \\times n$ 的格拉姆矩阵 $K$，其中每个元素 $K_{ij} = k(X_i, X_j)$，针对我们在 $\\mathbb{R}^2$ 中的 $n$ 个数据点。\n\nPCA 要求数据以原点为中心。在特征空间中，这是通过对核矩阵进行中心化来实现的。中心化核矩阵 $K_c$ 由下式给出：\n$$K_c = H K H$$\n这里，$H=I_n - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top$，$I_n$ 是 $n \\times n$ 的单位矩阵，$\\mathbf{1}$ 是一个包含 $n$ 个 1 的列向量。此操作确保映射到特征空间的数据 $\\phi(X_i)$ 具有零均值。\n\n### 3. 核主成分提取\n\n特征空间中的主成分通过求解中心化核矩阵的特征值问题来找到：\n$$K_c v_\\ell = \\lambda_\\ell v_\\ell$$\n特征值 $\\lambda_\\ell$ 与每个主成分所捕获的方差成正比，而特征向量 $v_\\ell$（归一化为 $\\|\\cdot\\|_2=1$）则包含了根据映射后的数据点定义主轴的系数。我们将特征值按降序排序，$\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$。由于我们的数据主要沿着一条一维曲线（抛物线）分布，我们预期对应于最大特征值 $\\lambda_1$ 的第一主成分将捕获这一主要结构。因此，问题指导我们只使用第一主成分，这意味着我们只保留特征向量 $v_1$ 和特征值 $\\lambda_1$。\n\n### 4. 通过不动点迭代进行预像重构\n\n在将数据投影到特征空间中的第一主成分上之后，我们的目标是找到在原始输入空间中对应的点。这就是“预像问题”。给定特征空间中的一个投影，我们在输入空间中寻找一个点 $\\hat{x}$，使得 $\\phi(\\hat{x})$ 接近这个投影。问题指定了一种不动点迭代方法来近似这个预像。\n\n一个点 $\\phi_c(X_k)$ 在第一主成分上的投影由其重构给出，这涉及到系数 $a_i^{(k)}$。对于秩为 1 的重构 ($m=1$)，这些系数是：\n$$a_i^{(k)} = v_{1,i} v_{1,k}$$\n其中 $v_{1,i}$ 是第一特征向量 $v_1$ 的第 $i$ 个元素。这些系数定义了如何对所有训练点的中心化特征向量 $\\phi_c(X_i)$进行加权，以形成 $\\phi_c(X_k)$ 的重构。\n\n对于每个训练点 $X_k$，我们寻求其预像 $\\hat{x}_k$。我们用原始数据点 $\\hat{x}_k^{(0)} = X_k$ 初始化搜索，并使用以下更新规则进行迭代（$t=0, 1, 2, \\dots$）：\n$$\\hat{x}_k^{(t+1)} = \\frac{\\sum_{j=1}^{n} \\gamma_j^{(t)} X_j}{\\sum_{j=1}^{n} \\gamma_j^{(t)}}$$\n权重 $\\gamma_j^{(t)}$ 定义为：\n$$\\gamma_j^{(t)} = a_j^{(k)} \\exp\\left(-\\frac{\\lVert \\hat{x}_k^{(t)} - X_j \\rVert^2}{2\\sigma^2}\\right)$$\n这个迭代是一个加权平均更新。权重 $\\gamma_j^{(t)}$ 结合了两种效应：来自 PCA 投影的结构信息（通过 $a_j^{(k)}$）和基于局部性的加权（通过 RBF 核项），后者给予了靠近当前预像估计 $\\hat{x}_k^{(t)}$ 的训练点 $X_j$ 更大的影响。迭代持续进行，直到估计值的变化可以忽略不计，即 $\\lVert \\hat{x}_k^{(t+1)} - \\hat{x}_k^{(t)} \\rVert_2  \\varepsilon = 10^{-8}$，或者达到最大迭代次数 $T=200$。\n\n### 5. 去噪性能评估\n\n对每个 $X_k$ 进行预像计算的结果是一个去噪后的点 $\\hat{x}_k = (\\hat{x}_{k,1}, \\hat{x}_{k,2})$。第二个坐标，我们可以称之为 $\\hat{y}_k = \\hat{x}_{k,2}$，是我们对原始含噪 $y_k$ 的去噪估计。\n\n为了量化性能，我们计算了去噪过程前后 $y$ 坐标的均方根误差 (RMSE)。“去噪前”的 RMSE 衡量了观测到的含噪数据相对于真实函数的误差：\n$$\\mathrm{RMSE}_{\\mathrm{before}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} (y_k - y_k^{\\mathrm{true}})^2}$$\n“去噪后”的 RMSE 衡量了我们重构估计的误差：\n$$\\mathrm{RMSE}_{\\mathrm{after}} = \\sqrt{\\frac{1}{n}\\sum_{k=1}^{n} (\\hat{y}_k - y_k^{\\mathrm{true}})^2}$$\n$\\mathrm{RMSE}_{\\mathrm{after}}  \\mathrm{RMSE}_{\\mathrm{before}}$ 表明去噪是成功的。我们将为指定的三个测试用例执行这整个过程。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom scipy.spatial.distance import pdist, squareform\n\ndef run_kpca_denoising(n, sigma, sigma_noise, seed):\n    \"\"\"\n    Performs KPCA-based denoising for a single test case.\n\n    Args:\n        n (int): Number of data points.\n        sigma (float): RBF kernel bandwidth.\n        sigma_noise (float): Standard deviation of Gaussian noise.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        list: A list containing [RMSE_before, RMSE_after].\n    \"\"\"\n    # 1. Data generation\n    rng = np.random.default_rng(seed)\n    x_coords = rng.uniform(-1, 1, n)\n    y_true = x_coords**2\n    noise = rng.normal(0, sigma_noise, n)\n    y_coords = y_true + noise\n    X = np.vstack((x_coords, y_coords)).T  # n x 2 matrix\n\n    # 6. Evaluation (before)\n    # The noise added is (y_coords - y_true), so RMSE_before is just the RMS of the noise\n    rmse_before = np.sqrt(np.mean((y_coords - y_true)**2))\n\n    # 2. Kernel computation\n    sq_dists = squareform(pdist(X, 'sqeuclidean'))\n    K = np.exp(-sq_dists / (2 * sigma**2))\n\n    # Center the kernel matrix\n    H = np.eye(n) - np.ones((n, n)) / n\n    Kc = H @ K @ H\n\n    # 3. KPCA (Eigen-decomposition)\n    # eigh returns eigenvalues in ascending order\n    eigenvalues, eigenvectors = eigh(Kc)\n    \n    # Select the first principal component (corresponding to the largest eigenvalue)\n    # lambda_1 = eigenvalues[-1] # Not directly needed for pre-image\n    v1 = eigenvectors[:, -1]\n\n    # 5. Pre-image approximation\n    X_hat = np.zeros_like(X)\n    T_max = 200\n    epsilon = 1e-8\n    \n    for k in range(n):\n        # 4. Reconstruction coefficients for point k\n        a_k = v1 * v1[k]\n\n        # Initialize pre-image iteration\n        x_hat_t = X[k, :] \n        \n        for _ in range(T_max):\n            # Compute distances from current estimate to all training points\n            dists_sq_t = np.sum((x_hat_t - X)**2, axis=1)\n            \n            # Compute gamma weights\n            gamma_t = a_k * np.exp(-dists_sq_t / (2 * sigma**2))\n            \n            # Compute next estimate\n            numerator = np.sum(gamma_t[:, np.newaxis] * X, axis=0)\n            denominator = np.sum(gamma_t)\n            \n            # Add small epsilon for numerical stability\n            if np.abs(denominator)  1e-12:\n                denominator += 1e-12\n                \n            x_hat_t_plus_1 = numerator / denominator\n            \n            # Check for convergence\n            if np.linalg.norm(x_hat_t_plus_1 - x_hat_t)  epsilon:\n                x_hat_t = x_hat_t_plus_1\n                break\n                \n            x_hat_t = x_hat_t_plus_1\n        \n        X_hat[k, :] = x_hat_t\n\n    # 6. Evaluation (after)\n    y_hat = X_hat[:, 1]\n    rmse_after = np.sqrt(np.mean((y_hat - y_true)**2))\n\n    return [round(rmse_before, 6), round(rmse_after, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, sigma_noise, seed)\n        (80, 0.15, 0.05, 0),\n        (60, 0.12, 0.0, 1),\n        (80, 0.05, 0.10, 2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, sigma, sigma_noise, seed = case\n        result = run_kpca_denoising(n, sigma, sigma_noise, seed)\n        results.append(result)\n\n    # Format output string manually to avoid spaces\n    # str(list) adds spaces, e.g., '[0.1, 0.2]'. We need '[0.1,0.2]'.\n    results_str = [str(r).replace(\" \", \"\") for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "KPCA 的强大能力也伴随着风险：选择不当的核函数可能会让模型学习到噪声而非真实的信号。本练习将引导你扮演数据侦探的角色，通过编写代码来诊断 KPCA 模型是否捕获了伪结构。你将学习使用“核矩阵对齐”(kernel alignment)等高级诊断工具，来判断主成分的有效性，从而培养在实践中审慎选择和评估模型的关键能力。",
            "id": "3136607",
            "problem": "您的任务是分析，当核选择不当时，核主成分分析（Kernel Principal Components Analysis, KPCA）找到的首个主成分是否会被噪声主导。该分析必须基于主成分分析（Principal Component Analysis, PCA）和再生核希尔伯特空间（Reproducing Kernel Hilbert Space, RKHS）理论的第一性原理进行，并实现为一个完整的、可运行的程序。\n\n从以下基础出发：\n- PCA 寻求使数据方差最大化的标准正交方向，这些方向由协方差矩阵的特征向量给出。\n- 在 KPCA 中，数据通过与半正定核 $k(\\cdot, \\cdot)$ 相关联的特征映射 $\\Phi(\\cdot)$ 映射到特征空间，然后通过核技巧在该特征空间中执行 PCA。\n- Gram 矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 的元素为 $K_{ij} = k(x_i, x_j)$，在特征空间中进行中心化对应于通过 $K_c = H K H$ 对 Gram 矩阵进行中心化，其中 $H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top$。\n\n问题目标：\n1. 通过计算给定核的中心化 Gram 矩阵 $K_c$，并通过特征分解获得其特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n$，来实现 KPCA。在解释碎石图时，仅使用谱的非负部分：定义 $\\lambda_i^{+} = \\max(\\lambda_i, 0)$ 并计算首个主成分占比\n$$\ns = \\frac{\\max_i \\lambda_i^{+}}{\\sum_{i=1}^n \\lambda_i^{+}},\n$$\n约定当 $\\sum_i \\lambda_i^{+} = 0$ 时，$s = 0$。\n2. 计算用于 KPCA 的核与从生成数据的潜在标量 $z$ 导出的基准“信号核”$K_{\\text{sig}}$ 之间的中心化对齐度。使用中心化核上的归一化 Frobenius 内积：\n$$\nA = \\frac{\\langle K_c, K_{\\text{sig},c} \\rangle_F}{\\|K_c\\|_F \\, \\|K_{\\text{sig},c}\\|_F}\n$$\n其中 $K_{\\text{sig},c} = H K_{\\text{sig}} H$，$\\langle A, B \\rangle_F = \\mathrm{trace}(A^\\top B)$，且 $\\|A\\|_F = \\sqrt{\\langle A, A \\rangle_F}$。如果分母中的任一范数为零，则定义 $A = 0$。\n3. 根据以下规则诊断 KPCA 的首个主成分是否捕获了噪声：如果 $s \\ge \\tau_s$ 且 $A \\le \\tau_A$，则声明“已捕获噪声”，其中阈值 $\\tau_s = 0.6$ 和 $\\tau_A = 0.2$。\n\n数据模型：\n- 对于每个测试用例，生成 $n$ 个独立的潜在变量 $z_i \\sim \\mathrm{Uniform}([-1, 1])$。\n- 观测值为二维向量 $x_i \\in \\mathbb{R}^2$，构造如下\n$$\nx_i = \\begin{bmatrix} z_i \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\epsilon^{(x)}_i \\\\ \\epsilon^{(y)}_i \\end{bmatrix},\n$$\n其中 $\\epsilon^{(x)}_i \\sim \\mathcal{N}(0, \\sigma_x^2)$ 和 $\\epsilon^{(y)}_i \\sim \\mathcal{N}(0, \\sigma_y^2)$ 相互独立。\n- 基准信号核是潜在变量上的线性核：$K_{\\text{sig}}(i,j) = z_i z_j$。\n\n在观测数据上要测试的核：\n- 线性核：$k(x, x') = x^\\top x'$。\n- 多项式核：$k(x, x') = (x^\\top x' + c)^d$，其中次数 $d \\in \\mathbb{N}$，偏移量 $c \\in \\mathbb{R}$。\n- 径向基函数（RBF）：$k(x, x') = \\exp\\big(-\\gamma \\|x - x'\\|_2^2\\big)$，其中带宽参数 $\\gamma  0$。\n\n测试套件：\n提供一个单一程序，用于生成数据并对以下四个测试用例评估诊断。在每个用例中，使用固定的随机种子以保证可复现性。\n\n- 用例 1（理想情况，信号主导且使用适当的核）：\n    - $n = 200$, $\\sigma_x = 0.05$, $\\sigma_y = 0.20$, 核：线性核。\n- 用例 2（非理想情况，不当核放大了噪声）：\n    - $n = 200$, $\\sigma_x = 0.05$, $\\sigma_y = 1.00$, 核：多项式核，参数 $d = 5$ 和 $c = 1.0$。\n- 用例 3（非理想情况，RBF 核过窄导致近似对角占主导）：\n    - $n = 200$, $\\sigma_x = 0.05$, $\\sigma_y = 0.50$, 核：RBF 核，参数 $\\gamma = 15.0$。\n- 用例 4（边缘情况，RBF 核过宽导致近似常数核）：\n    - $n = 60$, $\\sigma_x = 0.05$, $\\sigma_y = 1.00$, 核：RBF 核，参数 $\\gamma = 0.001$。\n\n所需输出：\n- 对每个用例，计算上文定义的首个主成分占比 $s$ 和中心化对齐度 $A$。\n- 对每个用例，输出一个布尔值，指示“已捕获噪声”条件是否成立：$s \\ge 0.6$ 且 $A \\le 0.2$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[True,False,True,False]”）。\n\n此问题不涉及物理单位或角度。所有结果均为无量纲。",
            "solution": "该问题要求对核主成分分析（Kernel Principal Components Analysis, KPCA）进行分析，以确定当选择不当的核时，其首个主成分是否可能被噪声主导。这将通过从第一性原理实现 KPCA，将其应用于具有已知信号和噪声结构的合成数据集，并评估一个特定的诊断规则来完成。该分析涉及统计学习、线性代数和再生核希尔伯特空间（Reproducing Kernel Hilbert Space, RKHS）理论中的概念。\n\n每个测试用例的总体流程如下：\n1.  生成一个包含 $n$ 个二维点 $x_i \\in \\mathbb{R}^2$ 的合成数据集。数据由一维潜在信号 $z_i$ 和加性高斯噪声构成。\n2.  在观测数据 $x_i$ 上，使用指定的核函数（线性、多项式或 RBF）构建 Gram 矩阵 $K$。\n3.  将 Gram 矩阵 $K$ 中心化以获得 $K_c$，这等同于在特征空间中对数据进行中心化。\n4.  在潜在变量 $z_i$ 上使用线性核计算基准信号 Gram 矩阵 $K_{\\text{sig}}$，并将其中心化以得到 $K_{\\text{sig},c}$。\n5.  计算两个诊断指标：\n    a. 首个主成分占比 $s$，它衡量特征空间中最大主成分的主导程度。\n    b. 中心化对齐度 $A$，它衡量所选核捕获的结构（$K_c$）与基准信号结构（$K_{\\text{sig},c}$）之间的相似性。\n6.  应用诊断规则：如果首个主成分高度主导（$s \\ge 0.6$）但核与真实信号的对齐度很差（$A \\le 0.2$），我们则声明首个主成分捕获了噪声。\n\n**1. KPCA 的理论基础**\n\n主成分分析（PCA）旨在识别一组能够捕获数据集中最大方差的标准正交轴。在 KPCA 中，这是对通过映射 $\\Phi: \\mathbb{R}^d \\to \\mathcal{F}$ 映射到高维特征空间 $\\mathcal{F}$ 的数据点执行的。假设数据已中心化为 $\\Phi(x_i) - \\bar{\\Phi}$，该空间中的协方差矩阵为 $C_{\\Phi} = \\frac{1}{n} \\sum_{i=1}^n (\\Phi(x_i) - \\bar{\\Phi})(\\Phi(x_i) - \\bar{\\Phi})^\\top$。\n\nKPCA 的核心是“核技巧”，它避免了对 $\\Phi(x_i)$ 的显式计算。特征空间中的内积由一个核函数给出：$k(x_i, x_j) = \\langle \\Phi(x_i), \\Phi(x_j) \\rangle_{\\mathcal{F}}$。在 $\\mathcal{F}$ 中的 PCA 特征值问题 $C_{\\Phi} v = \\lambda v$ 可以被重构为 $n \\times n$ Gram 矩阵 $K$ 上的特征值问题，其中 $K_{ij} = k(x_i, x_j)$。\n\n在 $\\mathcal{F}$ 中对数据进行中心化是通过变换 Gram 矩阵来实现的。中心化 Gram 矩阵为 $K_c = H K H$，其中 $H = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top$ 是几何中心化矩阵，$I$ 是 $n \\times n$ 的单位矩阵，$\\mathbf{1}$ 是一个包含 $n$ 个 1 的列向量。$K_c$ 的特征值 $\\lambda'_i$ 与特征空间协方差矩阵 $C_{\\Phi}$ 的特征值 $\\lambda_i$ 成正比（具体来说，$\\lambda'_i = n \\lambda_i$）。问题陈述将 $K_c$ 的特征值记为 $\\lambda_i$，我们将遵循此用法。\n\n**2. 数据生成与核矩阵**\n\n对于每个具有参数 $n$、$\\sigma_x$ 和 $\\sigma_y$ 的测试用例，我们按以下方式生成数据：\n-   潜在信号：$z_i \\sim \\mathrm{Uniform}([-1, 1])$，其中 $i = 1, \\dots, n$。\n-   观测数据：$x_i = \\begin{bmatrix} z_i \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\epsilon^{(x)}_i \\\\ \\epsilon^{(y)}_i \\end{bmatrix}$，其中 $\\epsilon^{(x)}_i \\sim \\mathcal{N}(0, \\sigma_x^2)$ 且 $\\epsilon^{(y)}_i \\sim \\mathcal{N}(0, \\sigma_y^2)$。第一个维度是信号加噪声，而第二个维度是纯噪声。\n\n从观测数据 $X = \\{x_1, \\dots, x_n\\}$，我们根据指定的核计算 Gram 矩阵 $K$：\n-   **线性核**：$K_{ij} = x_i^\\top x_j$。\n-   **多项式核**：$K_{ij} = (x_i^\\top x_j + c)^d$。\n-   **RBF 核**：$K_{ij} = \\exp(-\\gamma \\|x_i - x_j\\|_2^2)$。\n\n基准信号核是潜在变量上的线性核：$K_{\\text{sig},ij} = z_i z_j$。该矩阵代表了我们旨在恢复的理想协方差结构。\n\n**3. 诊断指标**\n\n**首个主成分占比 ($s$)：**\n该指标量化了方差在首个主成分中的集中程度。计算中心化 Gram 矩阵 $K_c$ 后，我们求其特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n$。由于 $K_c$ 是对称的，其所有特征值均为实数。我们只考虑谱的非负部分，定义 $\\lambda_i^+ = \\max(\\lambda_i, 0)$。占比 $s$ 为：\n$$\ns = \\frac{\\max_i \\lambda_i^{+}}{\\sum_{i=1}^n \\lambda_i^{+}}\n$$\n高 $s$ 值表示有一个成分主导了特征空间中的方差。我们约定如果分母为零，则 $s=0$。\n\n**中心化对齐度 ($A$)：**\n该指标在考虑中心化后，衡量用于分析的核（$K$）与真实基础信号结构（$K_{\\text{sig}}$）的符合程度。它是中心化矩阵 $K_c$ 和 $K_{\\text{sig},c}$（被视为 $\\mathbb{R}^{n \\times n}$ 中的向量）之间的余弦相似度，使用 Frobenius 内积计算：\n$$\nA = \\frac{\\langle K_c, K_{\\text{sig},c} \\rangle_F}{\\|K_c\\|_F \\, \\|K_{\\text{sig},c}\\|_F}\n$$\n其中 $\\langle A, B \\rangle_F = \\mathrm{trace}(A^\\top B)$ 且 $\\|A\\|_F = \\sqrt{\\langle A, A \\rangle_F}$。对齐度 $A$ 的取值范围在 $[-1, 1]$ 内。接近 1 的值表示高度相似。我们约定如果分母中的任一范数为零，则 $A=0$。\n\n**4. 测试用例分析**\n\n-   **用例 1（线性核，低噪声）：** 线性核适用于数据的线性信号结构。我们预期会有较高的对齐度 $A$。噪声水平较低，因此信号应是方差的主要来源。首个主成分占比 $s$ 将会很显著，但不会过高，因为仍然存在噪声。预计条件 $s \\ge 0.6$ 且 $A \\le 0.2$ 不会成立。\n\n-   **用例 2（多项式核，高噪声）：** 噪声 $\\sigma_y$ 很大，并且使用了高阶（$d=5$）多项式核。该核展开式包含 $(x_i^\\top x_j + c)^d = (x_{i1} x_{j1} + x_{i2} x_{j2} + c)^d$ 等项。其中 $x_{i2}x_{j2}$ 项仅涉及噪声。将其提升到 5 次方会放大与噪声相关的结构，使其在核矩阵中占主导地位。这将导致与信号核的对齐度 $A$ 较低。如果这个噪声结构本身是低秩的（例如，被少数几个大的噪声值主导），它可能会在 $K_c$ 中产生一个大的首个特征值，从而得到一个高的 $s$。该用例旨在触发“已捕获噪声”的诊断。\n\n-   **用例 3（窄 RBF 核）：** 较大的带宽参数 $\\gamma=15.0$ 使得 RBF 核 $\\exp(-\\gamma \\|x_i - x_j\\|^2)$ 高度局部化。只有当点对 $(x_i, x_j)$ 非常接近时，核函数值才不可忽略。这种接近性可能由噪声而非信号引起。如果偶然有少数数据点形成一个与其他点分离的紧密簇，核矩阵将在该块中以较大的值反映这一点。这会产生一个对应于此伪簇的主导特征向量，导致高的 $s$。由于这种结构是由噪声驱动的，与信号的对齐度 $A$ 将会很低。这是另一个预期会触发诊断的场景。\n\n-   **用例 4（宽 RBF 核）：** 非常小的 $\\gamma=0.001$ 使核对距离不敏感。$k(x_i, x_j) \\approx 1 - \\gamma \\|x_i - x_j\\|^2$。Gram 矩阵 $K$ 变得接近一个全为 1 的秩一矩阵，$K \\approx \\mathbf{1}\\mathbf{1}^\\top$。中心化后，$K_c = H K H \\approx H (\\mathbf{1}\\mathbf{1}^\\top) H = \\mathbf{0}$，因为 $H\\mathbf{1} = \\mathbf{0}$。$K_c$ 的特征值将非常接近于零，导致一个小的 $\\|K_c\\|_F$ 以及一个可能很小或不稳定的 $s$。对齐度 $A$ 很可能也很小。预计不会触发诊断，但该用例测试了此极限情况下的数值行为。该问题被正确地识别为一个边缘情况。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the KPCA noise analysis on the specified test cases.\n    \"\"\"\n    \n    # Define test cases as a list of dictionaries.\n    test_cases = [\n        {\n            \"name\": \"Case 1: Happy path (linear kernel)\",\n            \"n\": 200, \"sigma_x\": 0.05, \"sigma_y\": 0.20,\n            \"kernel_type\": \"linear\", \"kernel_params\": {},\n            \"seed\": 0\n        },\n        {\n            \"name\": \"Case 2: Bad path (inappropriate polynomial kernel)\",\n            \"n\": 200, \"sigma_x\": 0.05, \"sigma_y\": 1.00,\n            \"kernel_type\": \"poly\", \"kernel_params\": {\"d\": 5, \"c\": 1.0},\n            \"seed\": 1\n        },\n        {\n            \"name\": \"Case 3: Bad path (narrow RBF kernel)\",\n            \"n\": 200, \"sigma_x\": 0.05, \"sigma_y\": 0.50,\n            \"kernel_type\": \"rbf\", \"kernel_params\": {\"gamma\": 15.0},\n            \"seed\": 2\n        },\n        {\n            \"name\": \"Case 4: Edge case (wide RBF kernel)\",\n            \"n\": 60, \"sigma_x\": 0.05, \"sigma_y\": 1.00,\n            \"kernel_type\": \"rbf\", \"kernel_params\": {\"gamma\": 0.001},\n            \"seed\": 3\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        result = run_kpca_diagnostic(\n            n=case[\"n\"],\n            sigma_x=case[\"sigma_x\"],\n            sigma_y=case[\"sigma_y\"],\n            kernel_type=case[\"kernel_type\"],\n            kernel_params=case[\"kernel_params\"],\n            seed=case[\"seed\"]\n        )\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_kpca_diagnostic(n, sigma_x, sigma_y, kernel_type, kernel_params, seed):\n    \"\"\"\n    Performs the KPCA diagnostic for a single case.\n    \"\"\"\n    # Set seed for reproducibility of this specific case\n    np.random.seed(seed)\n    \n    # 1. Data Generation\n    z = np.random.uniform(-1, 1, n)\n    eps_x = np.random.normal(0, sigma_x, n)\n    eps_y = np.random.normal(0, sigma_y, n)\n    \n    x_coords = z + eps_x\n    y_coords = eps_y\n    X = np.stack([x_coords, y_coords], axis=1)\n\n    # 2. Kernel Matrix Construction\n    K = compute_kernel_matrix(X, kernel_type, kernel_params)\n\n    # 3. Signal Kernel Construction\n    K_sig = np.outer(z, z)\n\n    # 4. Centering\n    H = np.eye(n) - (1/n) * np.ones((n, n))\n    K_c = H @ K @ H\n    K_sig_c = H @ K_sig @ H\n\n    # 5. Compute Top-Component Share (s)\n    eigenvalues = np.linalg.eigh(K_c)[0]\n    eigenvalues_plus = np.maximum(eigenvalues, 0)\n    sum_eig = np.sum(eigenvalues_plus)\n    \n    if sum_eig > 1e-12: # Use a tolerance for floating point comparison\n        max_eig = np.max(eigenvalues_plus)\n        s = max_eig / sum_eig\n    else:\n        s = 0.0\n\n    # 6. Compute Centered Alignment (A)\n    norm_Kc_F = np.linalg.norm(K_c, 'fro')\n    norm_K_sig_c_F = np.linalg.norm(K_sig_c, 'fro')\n    \n    denominator = norm_Kc_F * norm_K_sig_c_F\n    if denominator > 1e-12:\n        # Since Kc and K_sig_c are symmetric, tr(A^T B) = sum(A * B)\n        inner_prod_F = np.sum(K_c * K_sig_c)\n        A = inner_prod_F / denominator\n    else:\n        A = 0.0\n\n    # 7. Apply Diagnostic Rule\n    tau_s = 0.6\n    tau_A = 0.2\n    noise_captured = (s >= tau_s) and (A = tau_A)\n    \n    return noise_captured\n\ndef compute_kernel_matrix(X, kernel_type, params):\n    \"\"\"\n    Computes the Gram matrix for the given data and kernel.\n    \"\"\"\n    if kernel_type == \"linear\":\n        return X @ X.T\n    \n    elif kernel_type == \"poly\":\n        d = params[\"d\"]\n        c = params[\"c\"]\n        return (X @ X.T + c)**d\n    \n    elif kernel_type == \"rbf\":\n        gamma = params[\"gamma\"]\n        # Compute pairwise squared Euclidean distances without scipy\n        # (a-b)^2 = a^2 - 2ab + b^2\n        sq_norms = np.sum(X**2, axis=1)\n        dist_sq = sq_norms[:, np.newaxis] + sq_norms[np.newaxis, :] - 2 * (X @ X.T)\n        # Ensure non-negativity due to potential float errors\n        dist_sq = np.maximum(dist_sq, 0)\n        return np.exp(-gamma * dist_sq)\n    \n    else:\n        raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n\nsolve()\n```"
        }
    ]
}