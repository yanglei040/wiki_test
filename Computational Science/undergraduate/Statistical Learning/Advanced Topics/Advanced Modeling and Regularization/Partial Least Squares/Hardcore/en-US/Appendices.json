{
    "hands_on_practices": [
        {
            "introduction": "To truly master Partial Least Squares, it is invaluable to move beyond software packages and work through its mechanics by hand. This exercise guides you through a step-by-step calculation of a two-component PLS model on a small, well-defined dataset. By performing the core operations of covariance maximization, score generation, and iterative deflation yourself, you will build a concrete intuition for how PLS constructs its latent variables to model the relationship between predictors $X$ and a response $y$. ",
            "id": "3156334",
            "problem": "Consider a centered data matrix $X \\in \\mathbb{R}^{4 \\times 3}$ and a centered response vector $y \\in \\mathbb{R}^{4}$ given by\n$$\nX = \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n-1 & 0 & -1 \\\\\n0 & -1 & 0\n\\end{pmatrix}, \n\\quad\ny = \\begin{pmatrix}\n4 \\\\\n2 \\\\\n-4 \\\\\n-2\n\\end{pmatrix}.\n$$\nYou will perform two-component Partial Least Squares (PLS) regression in the single-response setting (PLS1). Begin from the fundamental definition that PLS constructs latent scores $t = X w$ such that the covariance between $t$ and $y$ is maximized subject to the constraint $\\|w\\|_{2} = 1$, and that deflation proceeds by orthogonal projection of $X$ and $y$ onto the orthogonal complement of the span of the current score. Derive everything symbolically from these principles without invoking shortcut formulas.\n\nTasks:\n1. Using the covariance-maximization principle and unit-norm constraint, derive the first weight vector $w_{1}$ and the first score $t_{1} = X w_{1}$.\n2. Derive the predictor loading $p_{1}$ and the response loading $q_{1}$ by orthogonal projection onto $t_{1}$, and construct the deflated matrices $X_{2} = X - t_{1} p_{1}^{\\top}$ and $y_{2} = y - q_{1} t_{1}$.\n3. Apply the same principles to the deflated pair $(X_{2}, y_{2})$ to derive the second weight $w_{2}$ and score $t_{2} = X_{2} w_{2}$. Confirm the orthogonality of scores by showing $t_{1}^{\\top} t_{2} = 0$.\n4. Assemble the weight matrix $W = [w_{1}, w_{2}]$, loading matrix $P = [p_{1}, p_{2}]$, and response-loading vector $q = \\begin{pmatrix} q_{1} \\\\ q_{2} \\end{pmatrix}$. Using least squares in the latent space, express the PLS regression coefficients as a closed-form analytic expression in terms of $W$, $P$, and $q$. Compute the second coefficient $\\hat{\\beta}_{2}$.\n\nAnswer specification:\n- Your final reported quantity must be the single scalar $\\hat{\\beta}_{2}$.\n- Express the answer as an exact value.",
            "solution": "The user-provided problem is evaluated as valid based on the required criteria. It is scientifically grounded in the field of statistical learning, is well-posed, and is stated using objective and precise mathematical language. All necessary data and definitions are provided.\n\nThe problem asks for a two-component Partial Least Squares (PLS) regression based on the NIPALS algorithm, derived from first principles.\n\n### Step 1: Validation\n- **Givens**:\n    - Centered data matrix $X \\in \\mathbb{R}^{4 \\times 3}$:\n    $$X = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & -1 \\\\ 0 & -1 & 0 \\end{pmatrix}$$\n    - Centered response vector $y \\in \\mathbb{R}^{4}$:\n    $$y = \\begin{pmatrix} 4 \\\\ 2 \\\\ -4 \\\\ -2 \\end{pmatrix}$$\n    - PLS is defined by maximizing the covariance between scores $t = Xw$ and response $y$, subject to $\\|w\\|_{2} = 1$.\n    - Deflation is performed by orthogonal projection onto the orthogonal complement of the current score's span.\n\n- **Verdict**: The problem is valid.\n\n### Step 2: Solution\n\n#### Task 1: First PLS Component\nThe first weight vector $w_{1}$ is found by maximizing the sample covariance between $t_1 = Xw_1$ and $y$. Since $X$ and $y$ are centered, this is equivalent to maximizing $(t_1^{\\top} y)^2$ or simply $t_1^{\\top} y$ (assuming positive correlation), subject to $\\|w_1\\|_{2} = 1$. The objective function to maximize is $(Xw_1)^{\\top}y = w_1^{\\top}X^{\\top}y$. Using a Lagrange multiplier for the constraint $w_1^{\\top}w_1 = 1$, we find that $w_1$ must be proportional to $X^{\\top}y$. The unit-norm constraint dictates that $w_1 = \\frac{X^{\\top}y}{\\|X^{\\top}y\\|_{2}}$.\n\nFirst, compute $X^{\\top}y$:\n$$X^{\\top}y = \\begin{pmatrix} 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1 \\\\ 1 & 0 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 2 \\\\ -4 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1(4) + 0(2) + (-1)(-4) + 0(-2) \\\\ 0(4) + 1(2) + 0(-4) + (-1)(-2) \\\\ 1(4) + 0(2) + (-1)(-4) + 0(-2) \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 4 \\\\ 8 \\end{pmatrix}$$\n\nNext, compute the norm $\\|X^{\\top}y\\|_{2}$:\n$$\\|X^{\\top}y\\|_{2} = \\sqrt{8^2 + 4^2 + 8^2} = \\sqrt{64 + 16 + 64} = \\sqrt{144} = 12$$\n\nThe first weight vector $w_1$ is:\n$$w_1 = \\frac{1}{12} \\begin{pmatrix} 8 \\\\ 4 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 1/3 \\\\ 2/3 \\end{pmatrix}$$\n\nThe first score vector $t_1$ is $t_1 = Xw_1$:\n$$t_1 = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & -1 \\\\ 0 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} 2/3 \\\\ 1/3 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 1(\\frac{2}{3}) + 0(\\frac{1}{3}) + 1(\\frac{2}{3}) \\\\ 0(\\frac{2}{3}) + 1(\\frac{1}{3}) + 0(\\frac{2}{3}) \\\\ -1(\\frac{2}{3}) + 0(\\frac{1}{3}) - 1(\\frac{2}{3}) \\\\ 0(\\frac{2}{3}) - 1(\\frac{1}{3}) + 0(\\frac{2}{3}) \\end{pmatrix} = \\begin{pmatrix} 4/3 \\\\ 1/3 \\\\ -4/3 \\\\ -1/3 \\end{pmatrix}$$\n\n#### Task 2: Deflation\nThe predictor loading $p_1$ and response loading $q_1$ are found by regressing $X$ and $y$ on $t_1$.\n$$p_1 = \\frac{X^{\\top}t_1}{t_1^{\\top}t_1}, \\quad q_1 = \\frac{y^{\\top}t_1}{t_1^{\\top}t_1}$$\nFirst, compute the required inner products:\n$$t_1^{\\top}t_1 = (\\frac{4}{3})^2 + (\\frac{1}{3})^2 + (-\\frac{4}{3})^2 + (-\\frac{1}{3})^2 = \\frac{16}{9} + \\frac{1}{9} + \\frac{16}{9} + \\frac{1}{9} = \\frac{34}{9}$$\n$$X^{\\top}t_1 = \\begin{pmatrix} 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1 \\\\ 1 & 0 & -1 & 0 \\end{pmatrix} \\begin{pmatrix} 4/3 \\\\ 1/3 \\\\ -4/3 \\\\ -1/3 \\end{pmatrix} = \\begin{pmatrix} 4/3 + 4/3 \\\\ 1/3 + 1/3 \\\\ 4/3 + 4/3 \\end{pmatrix} = \\begin{pmatrix} 8/3 \\\\ 2/3 \\\\ 8/3 \\end{pmatrix}$$\n$$y^{\\top}t_1 = \\begin{pmatrix} 4 & 2 & -4 & -2 \\end{pmatrix} \\begin{pmatrix} 4/3 \\\\ 1/3 \\\\ -4/3 \\\\ -1/3 \\end{pmatrix} = \\frac{16}{3} + \\frac{2}{3} + \\frac{16}{3} + \\frac{2}{3} = \\frac{36}{3} = 12$$\nNow, compute the loadings:\n$$p_1 = \\frac{1}{34/9} \\begin{pmatrix} 8/3 \\\\ 2/3 \\\\ 8/3 \\end{pmatrix} = \\frac{9}{34} \\frac{1}{3} \\begin{pmatrix} 8 \\\\ 2 \\\\ 8 \\end{pmatrix} = \\frac{3}{34} \\begin{pmatrix} 8 \\\\ 2 \\\\ 8 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 12 \\\\ 3 \\\\ 12 \\end{pmatrix}$$\n$$q_1 = \\frac{12}{34/9} = \\frac{12 \\times 9}{34} = \\frac{108}{34} = \\frac{54}{17}$$\nDeflate $X$ and $y$:\n$$X_2 = X - t_1 p_1^{\\top} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ -1 & 0 & -1 \\\\ 0 & -1 & 0 \\end{pmatrix} - \\frac{1}{17} \\begin{pmatrix} 4/3 \\\\ 1/3 \\\\ -4/3 \\\\ -1/3 \\end{pmatrix} \\begin{pmatrix} 12 & 3 & 12 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 1 & -4 & 1 \\\\ -4 & 16 & -4 \\\\ -1 & 4 & -1 \\\\ 4 & -16 & 4 \\end{pmatrix}$$\n$$y_2 = y - q_1 t_1 = \\begin{pmatrix} 4 \\\\ 2 \\\\ -4 \\\\ -2 \\end{pmatrix} - \\frac{54}{17} \\begin{pmatrix} 4/3 \\\\ 1/3 \\\\ -4/3 \\\\ -1/3 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 2 \\\\ -4 \\\\ -2 \\end{pmatrix} - \\frac{18}{17} \\begin{pmatrix} 4 \\\\ 1 \\\\ -4 \\\\ -1 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 68-72 \\\\ 34-18 \\\\ -68+72 \\\\ -34+18 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} -4 \\\\ 16 \\\\ 4 \\\\ -16 \\end{pmatrix}$$\n\n#### Task 3: Second PLS Component\nThe second weight $w_2$ is found by applying the same principle to the deflated data $(X_2, y_2)$. $w_2$ is proportional to $X_2^{\\top}y_2$.\n$$X_2^{\\top}y_2 = \\frac{1}{17} \\begin{pmatrix} 1 & -4 & -1 & 4 \\\\ -4 & 16 & 4 & -16 \\\\ 1 & -4 & -1 & 4 \\end{pmatrix} \\frac{1}{17} \\begin{pmatrix} -4 \\\\ 16 \\\\ 4 \\\\ -16 \\end{pmatrix} = \\frac{1}{289} \\begin{pmatrix} -4-64-4-64 \\\\ 16+256+16+256 \\\\ -4-64-4-64 \\end{pmatrix} = \\frac{1}{289} \\begin{pmatrix} -136 \\\\ 544 \\\\ -136 \\end{pmatrix} = \\frac{136}{289} \\begin{pmatrix} -1 \\\\ 4 \\\\ -1 \\end{pmatrix}$$\nSo, $w_2$ is in the direction of $\\begin{pmatrix} -1 & 4 & -1 \\end{pmatrix}^{\\top}$. Normalizing this vector:\n$$\\| \\begin{pmatrix} -1 & 4 & -1 \\end{pmatrix}^{\\top} \\|_{2} = \\sqrt{(-1)^2 + 4^2 + (-1)^2} = \\sqrt{1+16+1} = \\sqrt{18} = 3\\sqrt{2}$$\n$$w_2 = \\frac{1}{3\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 4 \\\\ -1 \\end{pmatrix}$$\nThe second score vector is $t_2 = X_2 w_2$:\n$$t_2 = \\frac{1}{17} \\begin{pmatrix} 1 & -4 & 1 \\\\ -4 & 16 & -4 \\\\ -1 & 4 & -1 \\\\ 4 & -16 & 4 \\end{pmatrix} \\frac{1}{3\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 4 \\\\ -1 \\end{pmatrix} = \\frac{1}{51\\sqrt{2}} \\begin{pmatrix} -1-16-1 \\\\ 4+64+4 \\\\ 1+16+1 \\\\ -4-64-4 \\end{pmatrix} = \\frac{1}{51\\sqrt{2}} \\begin{pmatrix} -18 \\\\ 72 \\\\ 18 \\\\ -72 \\end{pmatrix} = \\frac{18}{51\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 4 \\\\ 1 \\\\ -4 \\end{pmatrix} = \\frac{6}{17\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 4 \\\\ 1 \\\\ -4 \\end{pmatrix}$$\nTo confirm orthogonality, we compute $t_1^{\\top}t_2$:\n$$t_1^{\\top}t_2 = \\left(\\frac{1}{3} \\begin{pmatrix} 4 \\\\ 1 \\\\ -4 \\\\ -1 \\end{pmatrix}\\right)^{\\top} \\left(\\frac{6}{17\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 4 \\\\ 1 \\\\ -4 \\end{pmatrix}\\right) = \\frac{2}{17\\sqrt{2}} (4(-1) + 1(4) + (-4)(1) + (-1)(-4)) = \\frac{2}{17\\sqrt{2}}(-4+4-4+4) = 0$$\nThe scores are orthogonal.\n\n#### Task 4: Regression Coefficients\nWe assemble the weight matrix $W=[w_1, w_2]$, loading matrix $P=[p_1, p_2]$, and response-loading vector $q = \\begin{pmatrix} q_1 \\\\ q_2 \\end{pmatrix}$.\n$$W = \\begin{pmatrix} 2/3 & -1/(3\\sqrt{2}) \\\\ 1/3 & 4/(3\\sqrt{2}) \\\\ 2/3 & -1/(3\\sqrt{2}) \\end{pmatrix}$$\nWe need $p_2$ and $q_2$. As $t_1$ and $t_2$ are orthogonal, we can use the shortcut $p_2 = \\frac{X^{\\top}t_2}{t_2^{\\top}t_2}$ and $q_2 = \\frac{y^{\\top}t_2}{t_2^{\\top}t_2}$.\n$$t_2^{\\top}t_2 = \\left(\\frac{6}{17\\sqrt{2}}\\right)^2 ((-1)^2 + 4^2 + 1^2 + (-4)^2) = \\frac{36}{289 \\times 2} (34) = \\frac{36 \\times 34}{578} = \\frac{1224}{578} = \\frac{36}{17}$$\n$$X^{\\top}t_2 = \\begin{pmatrix} 1 & 0 & -1 & 0 \\\\ 0 & 1 & 0 & -1 \\\\ 1 & 0 & -1 & 0 \\end{pmatrix} \\frac{6}{17\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 4 \\\\ 1 \\\\ -4 \\end{pmatrix} = \\frac{6}{17\\sqrt{2}} \\begin{pmatrix} -2 \\\\ 8 \\\\ -2 \\end{pmatrix} = \\frac{12}{17\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 4 \\\\ -1 \\end{pmatrix}$$\n$$p_2 = \\frac{12/(17\\sqrt{2})}{36/17} \\begin{pmatrix} -1 \\\\ 4 \\\\ -1 \\end{pmatrix} = \\frac{12}{17\\sqrt{2}} \\frac{17}{36} \\begin{pmatrix} -1 \\\\ 4 \\\\ -1 \\end{pmatrix} = \\frac{1}{3\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 4 \\\\ -1 \\end{pmatrix} = \\frac{\\sqrt{2}}{6} \\begin{pmatrix} -1 \\\\ 4 \\\\ -1 \\end{pmatrix}$$\n$$P = \\begin{pmatrix} 12/17 & -\\sqrt{2}/6 \\\\ 3/17 & 2\\sqrt{2}/3 \\\\ 12/17 & -\\sqrt{2}/6 \\end{pmatrix}$$\n$$y^{\\top}t_2 = \\begin{pmatrix} 4 & 2 & -4 & -2 \\end{pmatrix} \\frac{6}{17\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 4 \\\\ 1 \\\\ -4 \\end{pmatrix} = \\frac{6}{17\\sqrt{2}}(-4+8-4+8) = \\frac{6 \\times 8}{17\\sqrt{2}} = \\frac{48}{17\\sqrt{2}}$$\n$$q_2 = \\frac{48/(17\\sqrt{2})}{36/17} = \\frac{48}{17\\sqrt{2}} \\frac{17}{36} = \\frac{48}{36\\sqrt{2}} = \\frac{4}{3\\sqrt{2}} = \\frac{2\\sqrt{2}}{3}$$\n$$q = \\begin{pmatrix} 54/17 \\\\ 2\\sqrt{2}/3 \\end{pmatrix}$$\n\nThe PLS regression model is $y \\approx Tq$. The relationship between scores $T$ and predictors $X$ is $T = XR$, where $R$ is a \"weight\" matrix relating $X$ to the orthogonal scores $T$. It can be shown that $R=W(P^\\top W)^{-1}$. The regression coefficients are then $\\hat{\\beta} = Rq = W(P^\\top W)^{-1}q$.\n\nCompute $U = P^{\\top}W$:\n$$P^{\\top}W = \\begin{pmatrix} 12/17 & 3/17 & 12/17 \\\\ -\\sqrt{2}/6 & 2\\sqrt{2}/3 & -\\sqrt{2}/6 \\end{pmatrix} \\begin{pmatrix} 2/3 & -1/(3\\sqrt{2}) \\\\ 1/3 & 4/(3\\sqrt{2}) \\\\ 2/3 & -1/(3\\sqrt{2}) \\end{pmatrix} = \\begin{pmatrix} 1 & -4/(17\\sqrt{2}) \\\\ 0 & 1 \\end{pmatrix}$$\nThe identities $p_1^{\\top}w_1=1$, $p_2^{\\top}w_1=0$, and $p_2^{\\top}w_2=1$ were used here.\nNow find the inverse $U^{-1}$:\n$$U^{-1} = \\begin{pmatrix} 1 & 4/(17\\sqrt{2}) \\\\ 0 & 1 \\end{pmatrix}$$\nNext, compute $c = U^{-1}q$:\n$$c = \\begin{pmatrix} 1 & 4/(17\\sqrt{2}) \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 54/17 \\\\ 2\\sqrt{2}/3 \\end{pmatrix} = \\begin{pmatrix} 54/17 + (4/(17\\sqrt{2}))(2\\sqrt{2}/3) \\\\ 2\\sqrt{2}/3 \\end{pmatrix} = \\begin{pmatrix} 54/17 + 8/51 \\\\ 2\\sqrt{2}/3 \\end{pmatrix} = \\begin{pmatrix} 162/51 + 8/51 \\\\ 2\\sqrt{2}/3 \\end{pmatrix} = \\begin{pmatrix} 10/3 \\\\ 2\\sqrt{2}/3 \\end{pmatrix}$$\nFinally, compute $\\hat{\\beta} = Wc$:\n$$\\hat{\\beta} = \\begin{pmatrix} 2/3 & -1/(3\\sqrt{2}) \\\\ 1/3 & 4/(3\\sqrt{2}) \\\\ 2/3 & -1/(3\\sqrt{2}) \\end{pmatrix} \\begin{pmatrix} 10/3 \\\\ 2\\sqrt{2}/3 \\end{pmatrix} = \\begin{pmatrix} (2/3)(10/3) - (1/(3\\sqrt{2}))(2\\sqrt{2}/3) \\\\ (1/3)(10/3) + (4/(3\\sqrt{2}))(2\\sqrt{2}/3) \\\\ (2/3)(10/3) - (1/(3\\sqrt{2}))(2\\sqrt{2}/3) \\end{pmatrix} = \\begin{pmatrix} 20/9 - 2/9 \\\\ 10/9 + 8/9 \\\\ 20/9 - 2/9 \\end{pmatrix} = \\begin{pmatrix} 18/9 \\\\ 18/9 \\\\ 18/9 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$$\nThe second coefficient, $\\hat{\\beta}_2$, is the second component of this vector.\n$$\\hat{\\beta}_2 = 2$$",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "After building a PLS model, the crucial next step is to interpret its findings. A key question is: which predictors are most important for explaining the response? This practice introduces the Variable Importance in Projection (VIP) score, a widely used metric that quantifies the influence of each predictor. By deriving and calculating VIP scores, you will learn how PLS combines a variable's contribution to the latent components with each component's relevance in explaining the response variance. ",
            "id": "3156331",
            "problem": "A dataset has $p=4$ standardized predictors collected in the matrix $X \\in \\mathbb{R}^{n \\times 4}$ and a centered scalar response $y \\in \\mathbb{R}^{n}$. A two-component Partial Least Squares (PLS) regression model has been fit using the standard NIPALS procedure on the standardized predictors. In PLS, each latent component is defined by a weight vector $w_k \\in \\mathbb{R}^{4}$, a score vector $t_k = X w_k$, and a loading $c_k \\in \\mathbb{R}$ for the response such that the component $k$ explains a portion of the response’s sum of squares. Let $\\mathrm{SSY}_k$ denote the response sum of squares explained by component $k$, and let $\\mathrm{SSY}_{1:K} = \\sum_{k=1}^{K} \\mathrm{SSY}_k$ denote the total response sum of squares explained by the $K$ components.\n\nThe fitted model yields the following unit-norm weight vectors for the two components:\n$$\nw_1 = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix},\n\\qquad\nw_2 = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\\\ 0 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix},\n\\qquad\n\\|w_1\\|^2 = 1, \\quad \\|w_2\\|^2 = 1.\n$$\nThe response variance contributions are\n$$\n\\mathrm{SSY}_1 = 3, \\qquad \\mathrm{SSY}_2 = 1, \\qquad \\mathrm{SSY}_{1:2} = 4.\n$$\n\nStarting from the principles of Partial Least Squares (PLS)—namely, that each component projects $X$ onto $t_k = X w_k$ to maximize covariance with $y$, and that each component explains a fraction of the response’s sum of squares—derive an analytical expression for the Variable Importance in Projection (VIP) for each predictor $j$ in terms of $p$, the response variance fractions carried by the components, and the predictor’s squared contributions within each component. Then compute the VIP for each predictor $j \\in \\{1,2,3,4\\}$ for this model, in exact form.\n\nFinally, based on your computed VIP values, interpret which predictors most strongly drive the fitted response $\\hat{y} = X \\hat{\\beta}$ in this two-component PLS model. Provide the VIP values in a single row vector. Do not round your final VIP values.",
            "solution": "The problem is valid as it is scientifically grounded in the established statistical methodology of Partial Least Squares (PLS), is well-posed with sufficient information for a unique solution, and is stated in objective, formal language. All givens are consistent and relevant.\n\nThe Variable Importance in Projection (VIP) score is a metric that summarizes the importance of each predictor in a PLS model. It accounts for both the influence of a predictor on the formation of each latent component and the importance of each component in explaining the response variable's variance.\n\nLet us first derive the analytical expression for the VIP score. A PLS model with $K$ components for a predictor matrix $X \\in \\mathbb{R}^{n \\times p}$ and a response vector $y \\in \\mathbb{R}^{n}$ is constructed. The model identifies a set of latent variables (scores) $T = [t_1, t_2, \\dots, t_K]$, where each score vector $t_k \\in \\mathbb{R}^n$ is a linear combination of the original predictors: $t_k = Xw_k$. The vector $w_k \\in \\mathbb{R}^p$ is the weight vector for the $k$-th component and is constrained to have unit norm, i.e., $\\|w_k\\|^2=1$.\n\nThe importance of a predictor $j$ can be quantified by its contribution to forming the latent variables. This is represented by the squared weight, $w_{jk}^2$, for each component $k$.\nThe importance of a component $k$ is determined by how much of the response variance it explains. This is given by the fraction of the total explained sum of squares of $y$ that is attributable to component $k$. Let $\\mathrm{SSY}_k$ be the sum of squares of the response explained by component $k$, and let $\\mathrm{SSY}_{1:K} = \\sum_{l=1}^{K} \\mathrm{SSY}_l$ be the total sum of squares explained by all $K$ components. The importance of component $k$ is then the ratio $\\frac{\\mathrm{SSY}_k}{\\mathrm{SSY}_{1:K}}$.\n\nThe VIP score for predictor $j$ aggregates these two aspects. It is a weighted sum of the squared weights $w_{jk}^2$ across all components, where the weights for the sum are the importance of each component:\n$$ \\sum_{k=1}^{K} \\left( \\frac{\\mathrm{SSY}_k}{\\mathrm{SSY}_{1:K}} \\right) w_{jk}^2 $$\nThis term represents the average contribution of predictor $j$ to the explained variance of $y$. By convention, this sum is scaled by the total number of predictors, $p$, to ensure that the average of the squared VIP scores across all predictors is equal to $1$. This provides a standardized basis for comparison.\n$$ \\frac{1}{p} \\sum_{j=1}^{p} \\mathrm{VIP}_j^2 = 1 $$\nThis requires that $\\mathrm{VIP}_j^2 = p \\times (\\text{average contribution})$. Taking the square root gives the final definition of the VIP score for predictor $j$:\n$$ \\mathrm{VIP}_j = \\sqrt{ p \\sum_{k=1}^{K} \\left( \\frac{\\mathrm{SSY}_k}{\\mathrm{SSY}_{1:K}} \\right) w_{jk}^2 } $$\n\nNow, we compute the VIP values for the given problem.\nThe provided data are:\n- Number of predictors, $p=4$.\n- Number of components, $K=2$.\n- Response sum of squares for component 1: $\\mathrm{SSY}_1 = 3$.\n- Response sum of squares for component 2: $\\mathrm{SSY}_2 = 1$.\n- Total response sum of squares explained: $\\mathrm{SSY}_{1:2} = \\mathrm{SSY}_1 + \\mathrm{SSY}_2 = 3 + 1 = 4$.\n- Weight vector for component 1: $w_1 = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 1/2 \\\\ 1/2 \\end{pmatrix}$.\n- Weight vector for component 2: $w_2 = \\begin{pmatrix} \\sqrt{3}/2 \\\\ 0 \\\\ 1/2 \\\\ 0 \\end{pmatrix}$.\n\nThe squared weights are:\n- For component 1: $w_{11}^2 = \\frac{1}{4}$, $w_{21}^2 = \\frac{1}{4}$, $w_{31}^2 = \\frac{1}{4}$, $w_{41}^2 = \\frac{1}{4}$.\n- For component 2: $w_{12}^2 = \\frac{3}{4}$, $w_{22}^2 = 0$, $w_{32}^2 = \\frac{1}{4}$, $w_{42}^2 = 0$.\n\nThe component importance weights are:\n- For component 1: $\\frac{\\mathrm{SSY}_1}{\\mathrm{SSY}_{1:2}} = \\frac{3}{4}$.\n- For component 2: $\\frac{\\mathrm{SSY}_2}{\\mathrm{SSY}_{1:2}} = \\frac{1}{4}$.\n\nNow we can calculate the VIP for each predictor $j \\in \\{1, 2, 3, 4\\}$ using the formula for $K=2$:\n$$ \\mathrm{VIP}_j = \\sqrt{p \\left[ \\left( \\frac{\\mathrm{SSY}_1}{\\mathrm{SSY}_{1:2}} \\right) w_{j1}^2 + \\left( \\frac{\\mathrm{SSY}_2}{\\mathrm{SSY}_{1:2}} \\right) w_{j2}^2 \\right]} $$\n\nFor predictor $j=1$:\n$$ \\mathrm{VIP}_1 = \\sqrt{4 \\left[ \\left(\\frac{3}{4}\\right) w_{11}^2 + \\left(\\frac{1}{4}\\right) w_{12}^2 \\right]} = \\sqrt{4 \\left[ \\left(\\frac{3}{4}\\right) \\left(\\frac{1}{4}\\right) + \\left(\\frac{1}{4}\\right) \\left(\\frac{3}{4}\\right) \\right]} = \\sqrt{4 \\left[ \\frac{3}{16} + \\frac{3}{16} \\right]} = \\sqrt{4 \\left(\\frac{6}{16}\\right)} = \\sqrt{\\frac{24}{16}} = \\sqrt{\\frac{3}{2}} = \\frac{\\sqrt{6}}{2} $$\n\nFor predictor $j=2$:\n$$ \\mathrm{VIP}_2 = \\sqrt{4 \\left[ \\left(\\frac{3}{4}\\right) w_{21}^2 + \\left(\\frac{1}{4}\\right) w_{22}^2 \\right]} = \\sqrt{4 \\left[ \\left(\\frac{3}{4}\\right) \\left(\\frac{1}{4}\\right) + \\left(\\frac{1}{4}\\right) (0) \\right]} = \\sqrt{4 \\left[ \\frac{3}{16} \\right]} = \\sqrt{\\frac{12}{16}} = \\sqrt{\\frac{3}{4}} = \\frac{\\sqrt{3}}{2} $$\n\nFor predictor $j=3$:\n$$ \\mathrm{VIP}_3 = \\sqrt{4 \\left[ \\left(\\frac{3}{4}\\right) w_{31}^2 + \\left(\\frac{1}{4}\\right) w_{32}^2 \\right]} = \\sqrt{4 \\left[ \\left(\\frac{3}{4}\\right) \\left(\\frac{1}{4}\\right) + \\left(\\frac{1}{4}\\right) \\left(\\frac{1}{4}\\right) \\right]} = \\sqrt{4 \\left[ \\frac{3}{16} + \\frac{1}{16} \\right]} = \\sqrt{4 \\left(\\frac{4}{16}\\right)} = \\sqrt{1} = 1 $$\n\nFor predictor $j=4$:\n$$ \\mathrm{VIP}_4 = \\sqrt{4 \\left[ \\left(\\frac{3}{4}\\right) w_{41}^2 + \\left(\\frac{1}{4}\\right) w_{42}^2 \\right]} = \\sqrt{4 \\left[ \\left(\\frac{3}{4}\\right) \\left(\\frac{1}{4}\\right) + \\left(\\frac{1}{4}\\right) (0) \\right]} = \\sqrt{4 \\left[ \\frac{3}{16} \\right]} = \\sqrt{\\frac{12}{16}} = \\sqrt{\\frac{3}{4}} = \\frac{\\sqrt{3}}{2} $$\n\nThe computed VIP scores are:\n- $\\mathrm{VIP}_1 = \\frac{\\sqrt{6}}{2} \\approx 1.225$\n- $\\mathrm{VIP}_2 = \\frac{\\sqrt{3}}{2} \\approx 0.866$\n- $\\mathrm{VIP}_3 = 1$\n- $\\mathrm{VIP}_4 = \\frac{\\sqrt{3}}{2} \\approx 0.866$\n\nInterpretation:\nA common rule of thumb is that predictors with a VIP score greater than $1$ are considered the most influential in the model. Based on the calculated values, predictor $1$ ($\\mathrm{VIP}_1 \\approx 1.225$) is the most important variable for explaining the response $y$. Predictor $3$ ($\\mathrm{VIP}_3 = 1$) is also considered important. Predictors $2$ and $4$ share the same lower VIP score ($\\mathrm{VIP}_2 = \\mathrm{VIP}_4 \\approx 0.866$), indicating they have a lesser, but still notable, influence on the model compared to a threshold of $0.8$. Therefore, the order of importance of the predictors in driving the fitted response is $X_1 > X_3 > X_2 = X_4$.\n\nThe final answer is the row vector of the VIP scores for predictors $1$ through $4$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sqrt{6}}{2} & \\frac{\\sqrt{3}}{2} & 1 & \\frac{\\sqrt{3}}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A celebrated strength of PLS is its robust performance in the presence of high multicollinearity among predictors, a common challenge in fields like chemometrics and genomics. This advanced exercise puts that strength to the test by comparing PLS with another powerful regularization method, the Elastic Net. By implementing both algorithms and applying them to data with a built-in group of correlated features, you will explore how PLS implicitly handles collinearity and contrast its predictive performance with a method that explicitly balances lasso and ridge penalties. ",
            "id": "3156249",
            "problem": "You must write a complete, runnable program that, from first principles, compares Partial Least Squares (PLS) to Elastic Net on synthetic datasets with highly correlated features. The goal is to examine whether PLS latent factors exhibit a grouping behavior in the estimated coefficients comparable to Elastic Net, and to quantify prediction performance differences across a grid of regularization strengths.\n\nFoundational base and constraints:\n\n- Use the definition of Partial Least Squares (PLS) as finding latent directions in the feature space that maximize covariance with the response. Implement univariate-response PLS via iterative deflation: at each component, project features onto a weight vector aligned with the covariance between features and response, extract a score, and deflate both features and response along this score. Do not use any pre-packaged machine learning routines.\n- Use the definition of Elastic Net (EN) as the minimizer of the convex objective\n  $$\n  \\min_{\\beta \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2n}\\lVert y - X\\beta \\rVert_2^2 + \\lambda\\left(\\alpha \\lVert \\beta \\rVert_1 + \\frac{1-\\alpha}{2}\\lVert \\beta \\rVert_2^2\\right),\n  $$\n  where $X \\in \\mathbb{R}^{n\\times p}$, $y \\in \\mathbb{R}^n$, $\\lambda \\ge 0$, and $\\alpha \\in [0,1]$. Implement the solution path across a grid of $\\lambda$ using coordinate descent derived from this objective. Do not use any pre-packaged machine learning routines.\n\nProblem setup:\n\n- Data generation. For each test case, generate features $X \\in \\mathbb{R}^{n \\times p}$ with a designated correlated group of size $g$ using a single latent factor $z \\sim \\mathcal{N}(0,1)$ so that, for $j \\in \\{1,\\dots,g\\}$,\n  $$\n  x_j = z + \\eta_j,\n  $$\n  with $\\eta_j \\sim \\mathcal{N}(0,\\sigma_\\eta^2)$ chosen so that the pairwise correlation within the group is approximately $\\rho$. For a target within-group correlation $\\rho \\in (0,1)$ and $\\operatorname{Var}(z)=1$, pick $\\sigma_\\eta^2 = \\frac{1}{\\rho} - 1$. For $j \\in \\{g+1,\\dots,p\\}$, let $x_j \\sim \\mathcal{N}(0,1)$ i.i.d. The true coefficient vector $\\beta^\\star \\in \\mathbb{R}^p$ has entries $\\beta^\\star_j = \\frac{1}{g}$ for $j \\le g$ and $\\beta^\\star_j = 0$ otherwise. Generate the response as\n  $$\n  y = X \\beta^\\star + \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma_y^2),\n  $$\n  with $\\sigma_y = 0.3$. Use independent training and test sets for performance evaluation. All randomness must be reproducible with a fixed seed $12345$.\n\n- Preprocessing. For both PLS and Elastic Net, standardize the features by centering each column of $X$ to zero mean and scaling to unit variance using the training data statistics. Center the training response $y$ to zero mean. Use these training statistics to standardize the test data for prediction. Recover the intercept for predictions on the original scale as needed.\n\n- PLS modeling. Implement univariate-response PLS with $K$ components using deflation. Construct the coefficient vector in the standardized feature space as\n  $$\n  \\hat\\beta_{\\text{PLS}} = W \\left(P^\\top W\\right)^{-1} q,\n  $$\n  where $W \\in \\mathbb{R}^{p \\times K}$ collects the weight vectors, $P \\in \\mathbb{R}^{p \\times K}$ the loading vectors, and $q \\in \\mathbb{R}^{K}$ the regression scalars obtained at each deflation step.\n\n- Elastic Net modeling. Implement coordinate descent updates derived from the first-order optimality conditions for the stated objective. For feature $j$, with residual $r = y - X\\beta$, define\n  $$\n  \\rho_j = \\frac{1}{n} x_j^\\top \\left(r + x_j \\beta_j\\right),\n  $$\n  and update\n  $$\n  \\beta_j \\leftarrow \\frac{\\operatorname{soft}(\\rho_j,\\lambda\\alpha)}{\\frac{1}{n}\\lVert x_j \\rVert_2^2 + \\lambda(1-\\alpha)},\n  $$\n  where $\\operatorname{soft}(u,t) = \\operatorname{sign}(u)\\max(|u|-t,0)$. Use warm starts along a decreasing grid of $\\lambda$ values. For $\\alpha = 0$, this reduces to ridge-like updates with $\\operatorname{soft}(u,0)=u$.\n\nEvaluation metrics:\n\n- Grouping mimicry. Focus on the designated correlated group $G = \\{1,\\dots,g\\}$. For a given coefficient vector $\\hat\\beta$, form $v = \\left(|\\hat\\beta_j|\\right)_{j \\in G}$. Define the cosine similarity between two such vectors $v^{(1)}$ and $v^{(2)}$ as\n  $$\n  \\cos\\left(v^{(1)},v^{(2)}\\right) = \\frac{\\langle v^{(1)},v^{(2)} \\rangle}{\\lVert v^{(1)} \\rVert_2 \\lVert v^{(2)} \\rVert_2}.\n  $$\n  Let $\\hat\\beta_{\\text{PLS}}$ be the PLS coefficients in the standardized space, and let $\\hat\\beta_{\\text{EN}}(\\lambda^\\star)$ be the Elastic Net coefficients at the $\\lambda^\\star$ minimizing the test Mean Squared Error (MSE) over the grid. Declare that PLS mimics the Elastic Net grouping effect if the cosine similarity between the absolute coefficient vectors on $G$ is at least $\\gamma = 0.9$:\n  $$\n  \\cos\\left(|\\hat\\beta_{\\text{PLS}}|_G,\\;|\\hat\\beta_{\\text{EN}}(\\lambda^\\star)|_G\\right) \\ge 0.9.\n  $$\n  If either vector is numerically zero on $G$, treat mimicry as false.\n\n- Prediction gaps. For each $\\lambda$ in the grid, compute the test MSE difference (gap)\n  $$\n  \\Delta(\\lambda) = \\operatorname{MSE}_{\\text{EN}}(\\lambda) - \\operatorname{MSE}_{\\text{PLS}}.\n  $$\n  Report the minimum gap over the grid and the average gap across the grid.\n\nTest suite:\n\nImplement your program to run the following four test cases, each parameterized by a tuple $(\\rho, \\alpha, K, n_{\\text{train}}, n_{\\text{test}}, p, g)$:\n\n- Case $1$: $(0.95, 0.5, 1, 140, 60, 12, 8)$.\n- Case $2$: $(0.95, 0.0, 1, 140, 60, 12, 8)$.\n- Case $3$: $(0.6, 1.0, 1, 140, 60, 12, 8)$.\n- Case $4$: $(0.95, 0.5, 2, 40, 40, 12, 8)$.\n\nLambda grid specification:\n\n- For $\\alpha \\in (0,1]$, compute $\\lambda_{\\max} = \\max_j \\left|\\frac{1}{n} x_j^\\top y\\right| / \\alpha$ using standardized training $X$ and centered $y$. Construct a grid of $50$ values logarithmically spaced from $\\lambda_{\\max}$ down to $\\lambda_{\\min} = 10^{-3}\\lambda_{\\max}$.\n- For $\\alpha = 0$, construct a grid of $50$ values logarithmically spaced from $\\lambda_{\\max}^{(0)} = 10.0$ down to $\\lambda_{\\min}^{(0)} = 0.01$.\n\nRequired outputs:\n\n- For each case, output a list with three entries: $[\\text{mimic}, \\min\\_\\text{gap}, \\text{avg\\_gap}]$, where $\\text{mimic}$ is a boolean, and $\\min\\_\\text{gap}$ and $\\text{avg\\_gap}$ are floats computed as defined above.\n- Your program should produce a single line of output containing the results for all four cases as a comma-separated list enclosed in square brackets, with each case’s triple enclosed in its own square brackets. For example: $[[\\text{True},-0.0123,0.0456],[\\text{False},0.1001,0.2002],[\\text{True},-0.0050,0.0100],[\\text{True},-0.0200,0.0001]]$.\n- No user input is allowed; all values and random seeds are fixed in the program.\n\nImplementation rules:\n\n- Programming language: Python $3.12$.\n- Libraries: only NumPy (version $1.23.5$) and SciPy (version $1.11.4$) from the Python ecosystem, plus the Python standard library. SciPy is optional and not required.\n- Do not use any machine learning libraries or external datasets.",
            "solution": "The user's request is a well-defined computational problem in statistical learning, requiring the implementation and comparison of two regression methods, Partial Least Squares (PLS) and Elastic Net (EN), from first principles.\n\n### Step 1: Extract Givens\n- **Problem Statement**: Compare PLS and Elastic Net on synthetic data with highly correlated features, examining coefficient grouping and prediction performance.\n- **PLS Algorithm**: Implement univariate-response PLS via iterative deflation. The coefficient vector is given by $\\hat\\beta_{\\text{PLS}} = W (P^\\top W)^{-1} q$, where $W$, $P$, and $q$ are matrices of weights, loadings, and regression scalars, respectively.\n- **Elastic Net Algorithm**: Implement coordinate descent to minimize the objective function $\\frac{1}{2n}\\lVert y - X\\beta \\rVert_2^2 + \\lambda(\\alpha \\lVert \\beta \\rVert_1 + \\frac{1-\\alpha}{2}\\lVert \\beta \\rVert_2^2)$. The update rule is $\\beta_j \\leftarrow \\frac{\\operatorname{soft}(\\rho_j,\\lambda\\alpha)}{\\frac{1}{n}\\lVert x_j \\rVert_2^2 + \\lambda(1-\\alpha)}$, with $\\rho_j = \\frac{1}{n} x_j^\\top (r + x_j \\beta_j)$.\n- **Data Generation**: Features $X \\in \\mathbb{R}^{n \\times p}$ are generated with a correlated group of size $g$. For $j \\leq g$, $x_j = z + \\eta_j$ where $z \\sim \\mathcal{N}(0,1)$ and $\\eta_j \\sim \\mathcal{N}(0,\\sigma_\\eta^2)$ with $\\sigma_\\eta^2 = \\frac{1}{\\rho} - 1$. For $j > g$, $x_j \\sim \\mathcal{N}(0,1)$. The true coefficient vector is $\\beta^\\star_j = 1/g$ for $j \\leq g$ and $0$ otherwise. The response is $y = X \\beta^\\star + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_y^2)$ and $\\sigma_y = 0.3$. A fixed random seed of $12345$ is required for reproducibility.\n- **Preprocessing**: Standardize features (zero mean, unit variance) and center the response, using statistics from the training set.\n- **Evaluation Metrics**:\n    - **Grouping Mimicry**: Boolean, true if $\\cos(\\lvert\\hat\\beta_{\\text{PLS}}\\rvert_G, \\lvert\\hat\\beta_{\\text{EN}}(\\lambda^\\star)\\rvert_G) \\geq 0.9$, where $G$ is the set of correlated features and $\\lambda^\\star$ is the EN parameter yielding the minimum test Mean Squared Error (MSE).\n    - **Prediction Gaps**: $\\min(\\operatorname{MSE}_{\\text{EN}}(\\lambda) - \\operatorname{MSE}_{\\text{PLS}})$ and $\\text{mean}(\\operatorname{MSE}_{\\text{EN}}(\\lambda) - \\operatorname{MSE}_{\\text{PLS}})$ over the $\\lambda$ grid.\n- **Test Cases**: Four specific tuples of $(\\rho, \\alpha, K, n_{\\text{train}}, n_{\\text{test}}, p, g)$ are provided.\n- **Lambda Grid**: For $\\alpha \\in (0,1]$, a log-spaced grid of $50$ values from $\\lambda_{\\max} = \\max_j |\\frac{1}{n} x_j^\\top y| / \\alpha$ down to $10^{-3}\\lambda_{\\max}$. For $\\alpha=0$, a log-spaced grid from $10.0$ to $0.01$.\n- **Implementation Constraints**: Python $3.12$ with NumPy $1.23.5$ and SciPy $1.11.4$. No pre-packaged machine learning libraries.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is grounded in established principles of statistical learning and numerical optimization. PLS and Elastic Net are standard regression techniques. The data generation process is a common and valid method for simulating correlated predictors. The formulas for the algorithms, data generation, and evaluation are all standard and correct.\n- **Well-Posed**: The problem is fully specified. All necessary parameters, data generation procedures, algorithmic details, and evaluation criteria are explicitly defined. This structure ensures that a unique and meaningful solution can be computed.\n- **Objective**: The problem is stated in precise, formal, and unbiased mathematical language.\n\nThe problem does not violate any of the invalidity criteria. It is a scientifically sound, well-posed, objective, and non-trivial computational task.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. Proceeding to the solution.\n\n### Algorithmic Solution\nThe solution is structured as a single Python script that executes the complete pipeline for all test cases.\n\n1.  **Data Generation**: A function `_generate_data` implements the specified procedure. It takes dimensions, correlation $\\rho$, noise level $\\sigma_y$, and a random number generator as input, and returns a feature matrix $X$ and response vector $y$.\n2.  **Preprocessing**: Data is standardized by centering to mean $0$ and scaling to unit variance. The mean and standard deviation from the training set are stored and applied to the test set to prevent data leakage. The response vector $y$ is centered using the training mean.\n3.  **Partial Least Squares (PLS)**: The `_pls_fit` function implements the iterative deflation (NIPALS-like) algorithm.\n    - For each of the $K$ components:\n        - The weight vector $w_k$ is computed as the normalized covariance vector, $w_k \\propto X_k^\\top y_k$.\n        - The score vector $t_k = X_k w_k$ is calculated.\n        - The loading vector $p_k = X_k^\\top t_k / (t_k^\\top t_k)$ and scalar $q_k = y_k^\\top t_k / (t_k^\\top t_k)$ are computed.\n        - The feature matrix and response vector are \"deflated\": $X_{k+1} = X_k - t_k p_k^\\top$ and $y_{k+1} = y_k - t_k q_k$.\n        - The vectors $w_k, p_k$ and scalar $q_k$ are stored.\n    - After $K$ iterations, the final coefficient vector is assembled using the formula $\\hat\\beta_{\\text{PLS}} = W (P^\\top W)^{-1} q$, where a pseudo-inverse is used for numerical stability.\n4.  **Elastic Net**: The `_elastic_net_cd` function implements coordinate descent.\n    - It iterates through each feature $j=1, \\dots, p$, updating its coefficient $\\beta_j$.\n    - The update is based on the partial residual $r_{\\text{partial},j} = y - \\sum_{k \\neq j} X_k \\beta_k$. The term $\\rho_j = \\frac{1}{n} x_j^\\top r_{\\text{partial},j}$ is calculated.\n    - The new coefficient $\\beta_j$ is computed using the soft-thresholding operator and the formula provided in the problem statement.\n    - The algorithm uses warm starts, where the solution for the previous $\\lambda$ value is used as the starting point for the current $\\lambda$, which improves convergence speed.\n5.  **Main Execution Loop**: The main `run_case` function orchestrates the entire process for a single test case.\n    - It calls the data generation and preprocessing steps.\n    - It fits the PLS model and computes its test MSE.\n    - It defines the appropriate a grid of $\\lambda$ values for the Elastic Net model.\n    - It iterates through the $\\lambda$ grid, fitting an Elastic Net model at each step, and computes the corresponding test MSE.\n    - It identifies the best Elastic Net model (at $\\lambda^\\star$) as the one with the lowest test MSE.\n    - It calculates the two required evaluation metrics: grouping mimicry (cosine similarity) and prediction gaps ($\\min$ and average difference in MSE).\n6.  **Final Output**: The main `solve` function sets up the test cases and the random number generator, calls `run_case` for each, and formats the collected results into the exact string format required by the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the comparison between PLS and Elastic Net\n    on synthetic datasets as specified in the problem.\n    \"\"\"\n\n    def _soft_threshold(u, t):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(u) * np.maximum(np.abs(u) - t, 0.0)\n\n    def _generate_data(n, p, g, rho, sigma_y, rng):\n        \"\"\"Generates synthetic data according to the problem specification.\"\"\"\n        z = rng.normal(0.0, 1.0, size=(n, 1))\n        \n        # Ensure rho is not 1 to avoid division by zero\n        if np.isclose(rho, 1.0):\n            sigma_eta_sq = 0.0\n        else:\n            sigma_eta_sq = 1.0 / rho - 1.0\n        \n        sigma_eta = np.sqrt(max(0.0, sigma_eta_sq))\n        \n        X = np.zeros((n, p))\n        if g > 0:\n            eta = rng.normal(0.0, sigma_eta, size=(n, g))\n            X[:, :g] = z + eta\n        if p > g:\n            X[:, g:] = rng.normal(0.0, 1.0, size=(n, p - g))\n        \n        beta_star = np.zeros(p)\n        if g > 0:\n            beta_star[:g] = 1.0 / g\n            \n        epsilon = rng.normal(0.0, sigma_y, size=n)\n        y = X @ beta_star + epsilon\n        \n        return X, y\n\n    def _pls_fit(X, y, K):\n        \"\"\"Implements univariate PLS with K components using iterative deflation.\"\"\"\n        n, p = X.shape\n        X_k = X.copy()\n        y_k = y.copy()\n        \n        W = np.zeros((p, K))\n        P = np.zeros((p, K))\n        q = np.zeros(K)\n        \n        for k in range(K):\n            w_k = X_k.T @ y_k\n            norm_w_k = np.linalg.norm(w_k)\n            w_k = w_k / norm_w_k if not np.isclose(norm_w_k, 0.0) else np.zeros_like(w_k)\n            \n            t_k = X_k @ w_k\n            dot_t_t = np.dot(t_k, t_k)\n            \n            p_k = X_k.T @ t_k / dot_t_t if not np.isclose(dot_t_t, 0.0) else np.zeros_like(w_k)\n            q_k = np.dot(y_k, t_k) / dot_t_t if not np.isclose(dot_t_t, 0.0) else 0.0\n            \n            X_k -= np.outer(t_k, p_k)\n            y_k -= t_k * q_k\n            \n            W[:, k] = w_k\n            P[:, k] = p_k\n            q[k] = q_k\n            \n        try:\n            # Using inv() as specified by the formula in the problem.\n            PTW_inv = np.linalg.inv(P.T @ W)\n            beta_pls = W @ PTW_inv @ q\n        except np.linalg.LinAlgError:\n            return np.zeros(p)\n            \n        return beta_pls\n\n    def _elastic_net_cd(X, y, alpha, lambda_val, tol=1e-7, max_iter=1000, initial_beta=None):\n        \"\"\"Implements coordinate descent for Elastic Net.\"\"\"\n        n, p = X.shape\n        beta = np.zeros(p) if initial_beta is None else initial_beta.copy()\n        x_j_norm_sq_over_n = np.sum(X**2, axis=0) / n\n        r = y - X @ beta\n        \n        for _ in range(max_iter):\n            max_change = 0.0\n            for j in range(p):\n                beta_j_old = beta[j]\n                r += X[:, j] * beta_j_old\n                rho_j = np.dot(X[:, j], r) / n\n                \n                soft_rho = _soft_threshold(rho_j, lambda_val * alpha)\n                denominator = x_j_norm_sq_over_n[j] + lambda_val * (1.0 - alpha)\n                \n                beta[j] = soft_rho / denominator if denominator > 1e-10 else 0.0\n                \n                r -= X[:, j] * beta[j]\n                \n                change = np.abs(beta_j_old - beta[j])\n                if change > max_change:\n                    max_change = change\n            if max_change  tol:\n                break\n        return beta\n\n    def run_case(params, rng):\n        rho, alpha, K, n_train, n_test, p, g = params\n        sigma_y = 0.3\n        \n        X_train_raw, y_train_raw = _generate_data(n_train, p, g, rho, sigma_y, rng)\n        X_test_raw, y_test = _generate_data(n_test, p, g, rho, sigma_y, rng)\n        \n        y_train_mean = np.mean(y_train_raw)\n        y_train_centered = y_train_raw - y_train_mean\n        \n        X_train_mean = np.mean(X_train_raw, axis=0)\n        X_train_std = np.std(X_train_raw, axis=0)\n        X_train_std[np.isclose(X_train_std, 0.0)] = 1.0\n        \n        X_train_std_mat = (X_train_raw - X_train_mean) / X_train_std\n        X_test_std_mat = (X_test_raw - X_train_mean) / X_train_std\n        \n        beta_pls_std = _pls_fit(X_train_std_mat, y_train_centered, K)\n        y_pred_pls = (X_test_std_mat @ beta_pls_std) + y_train_mean\n        mse_pls = np.mean((y_test - y_pred_pls)**2)\n        \n        n_lambdas = 50\n        if alpha > 0.0:\n            lambda_max_val = np.max(np.abs(X_train_std_mat.T @ y_train_centered / n_train)) / alpha\n            lambdas = np.logspace(np.log10(lambda_max_val), np.log10(lambda_max_val * 1e-3), n_lambdas)\n        else: # alpha = 0 (Ridge)\n            lambdas = np.logspace(np.log10(10.0), np.log10(0.01), n_lambdas)\n\n        mses_en = []\n        betas_en = []\n        beta_en_current = np.zeros(p)\n        \n        for lam in lambdas:\n            beta_en_std = _elastic_net_cd(X_train_std_mat, y_train_centered, alpha, lam, initial_beta=beta_en_current)\n            betas_en.append(beta_en_std)\n            beta_en_current = beta_en_std\n            y_pred_en = (X_test_std_mat @ beta_en_std) + y_train_mean\n            mses_en.append(np.mean((y_test - y_pred_en)**2))\n\n        mses_en = np.array(mses_en)\n        best_lambda_idx = np.argmin(mses_en)\n        beta_en_star_std = betas_en[best_lambda_idx]\n        \n        v_pls = np.abs(beta_pls_std[:g]) if g > 0 else np.array([])\n        v_en = np.abs(beta_en_star_std[:g]) if g > 0 else np.array([])\n        \n        mimic = False\n        if g > 0:\n            norm_pls = np.linalg.norm(v_pls)\n            norm_en = np.linalg.norm(v_en)\n            if not (np.isclose(norm_pls, 0.0) or np.isclose(norm_en, 0.0)):\n                cosine_sim = np.dot(v_pls, v_en) / (norm_pls * norm_en)\n                if cosine_sim >= 0.9:\n                    mimic = True\n        \n        gaps = mses_en - mse_pls\n        min_gap = np.min(gaps)\n        avg_gap = np.mean(gaps)\n        \n        return [mimic, min_gap, avg_gap]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.95, 0.5, 1, 140, 60, 12, 8),  # Case 1\n        (0.95, 0.0, 1, 140, 60, 12, 8),  # Case 2\n        (0.6, 1.0, 1, 140, 60, 12, 8),   # Case 3\n        (0.95, 0.5, 2, 40, 40, 12, 8),    # Case 4\n    ]\n\n    rng = np.random.default_rng(12345)\n    results = []\n    for case_params in test_cases:\n        result = run_case(case_params, rng)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res in results:\n        # Use repr() for bool to get 'True'/'False' and standard float formatting\n        formatted_results.append(f\"[{repr(res[0])},{res[1]},{res[2]}]\")\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}