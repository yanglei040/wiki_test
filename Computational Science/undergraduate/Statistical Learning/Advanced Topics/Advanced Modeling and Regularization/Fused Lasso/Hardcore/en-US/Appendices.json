{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the fused lasso, we must first see it in action. The core mechanism of the model is its ability to create piecewise-constant solutions by merging adjacent coefficients as the penalty parameter, $\\lambda$, is increased. This exercise  provides an essential hands-on experience in deriving the complete solution path for a small dataset. By manually calculating the critical values of $\\lambda$ where fusions occur, you will build a concrete intuition for how the algorithm navigates the trade-off between data fidelity and signal simplicity.",
            "id": "3122167",
            "problem": "Consider one-dimensional fused lasso (total variation denoising) on a chain of length $n=6$. Given data $y \\in \\mathbb{R}^{6}$ with entries $y = (2, 0, 3, 4, 1, 5)$, define the fused lasso estimator $\\hat{\\beta}(\\lambda) \\in \\mathbb{R}^{6}$ for tuning parameter $\\lambda \\ge 0$ as the solution of the convex optimization problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^{6}} \\;\\; \\frac{1}{2}\\sum_{i=1}^{6} (\\beta_{i} - y_{i})^{2} \\;+\\; \\lambda \\sum_{i=2}^{6} |\\beta_{i} - \\beta_{i-1}|.\n$$\nStarting from fundamental optimality conditions for convex functions with nonsmooth penalties (subgradient optimality or Karush–Kuhn–Tucker conditions), derive by hand the exact piecewise-linear solution path $\\hat{\\beta}(\\lambda)$ as $\\lambda$ increases from $0$ to a value large enough that all coordinates have fused to a constant. In your derivation, explicitly:\n- Identify the segmentation of $\\{1,\\dots,6\\}$ into contiguous fused blocks on each interval of $\\lambda$ where the active signs of differences remain constant.\n- For each such interval, express the blockwise constant values of $\\hat{\\beta}(\\lambda)$ in terms of the block means and $\\lambda$.\n- Determine all critical values of $\\lambda$ at which adjacent blocks fuse.\n\nYour final reported answer must be the ordered list of all distinct positive critical values of $\\lambda$ (in increasing order) at which fusions occur. Give exact values; no rounding. Report the list as a single row vector. No units are required.",
            "solution": "The problem asks for the solution path of the one-dimensional fused lasso estimator $\\hat{\\beta}(\\lambda)$ for a given data vector $y \\in \\mathbb{R}^6$. The estimator is the solution to the convex optimization problem:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{6}} L(\\beta) = \\min_{\\beta \\in \\mathbb{R}^{6}} \\left( \\frac{1}{2}\\sum_{i=1}^{6} (\\beta_{i} - y_{i})^{2} \\;+\\; \\lambda \\sum_{i=2}^{6} |\\beta_{i} - \\beta_{i-1}| \\right)\n$$\nwhere $y = (2, 0, 3, 4, 1, 5)$ and $\\lambda \\ge 0$.\n\nThe objective function $L(\\beta)$ is convex. The Karush-Kuhn-Tucker (KKT) or subgradient optimality conditions state that a vector $\\hat{\\beta}$ is a minimizer if and only if the zero vector is in the subdifferential of $L(\\beta)$ at $\\hat{\\beta}$. The subdifferential of $L(\\beta)$ is given by $\\partial L(\\beta) = (\\beta - y) + \\lambda \\partial P(\\beta)$, where $P(\\beta)=\\sum_{i=2}^{6} |\\beta_{i} - \\beta_{i-1}|$.\n\nLet us introduce dual variables $u_i$ associated with each difference. The optimality conditions can be expressed a primal-dual system. Let $u_1=0$ and $u_{n+1}=u_7=0$.\nThe conditions are:\n1. $\\hat{\\beta}_i - y_i = u_{i+1} - u_i$ for $i=1, \\dots, 6$.\n2. $u_i \\in \\lambda \\cdot \\partial|\\hat{\\beta}_i - \\hat{\\beta}_{i-1}|$ for $i=2, \\dots, 6$. This means:\n   - If $\\hat{\\beta}_i - \\hat{\\beta}_{i-1} \\neq 0$, then $u_i = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_i - \\hat{\\beta}_{i-1})$.\n   - If $\\hat{\\beta}_i - \\hat{\\beta}_{i-1} = 0$, then $|u_i| \\le \\lambda$.\n\nSumming the first condition from $i=1$ to $k$, we get $\\sum_{i=1}^k (\\hat{\\beta}_i - y_i) = u_{k+1} - u_1 = u_{k+1}$. This provides a way to find the dual variables from the primal solution: $u_{k+1} = \\sum_{i=1}^k (\\hat{\\beta}_i - y_i)$ for $k=1, \\dots, 6$. The condition $u_7=0$ implies $\\sum_{i=1}^6 (\\hat{\\beta}_i - y_i) = 0$, so $\\sum_{i=1}^6 \\hat{\\beta}_i = \\sum_{i=1}^6 y_i$.\n\nWe trace the solution path $\\hat{\\beta}(\\lambda)$ by increasing $\\lambda$ from $0$.\n\n**Interval 1: $\\lambda \\in [0, \\lambda_1]$**\nFor $\\lambda=0$, the solution is $\\hat{\\beta}(0) = y = (2, 0, 3, 4, 1, 5)$.\nThe differences are:\n$\\beta_2 - \\beta_1 = -2$\n$\\beta_3 - \\beta_2 = 3$\n$\\beta_4 - \\beta_3 = 1$\n$\\beta_5 - \\beta_4 = -3$\n$\\beta_6 - \\beta_5 = 4$\nFor $\\lambda > 0$ and sufficiently small, the signs of the differences remain the same. The active signs are $s = (\\text{sign}(\\beta_2-\\beta_1), \\dots, \\text{sign}(\\beta_6-\\beta_5)) = (-1, 1, 1, -1, 1)$.\nThis implies $u_i = \\lambda s_i$ for $i=2, \\dots, 6$:\n$u_2 = -\\lambda$, $u_3 = \\lambda$, $u_4 = \\lambda$, $u_5 = -\\lambda$, $u_6 = \\lambda$. With $u_1=0$ and $u_7=0$.\n\nUsing $\\hat{\\beta}_i = y_i + u_{i+1} - u_i$, we find the piecewise-linear path:\n$\\hat{\\beta}_1(\\lambda) = y_1 + u_2 - u_1 = 2 - \\lambda - 0 = 2 - \\lambda$\n$\\hat{\\beta}_2(\\lambda) = y_2 + u_3 - u_2 = 0 + \\lambda - (-\\lambda) = 2\\lambda$\n$\\hat{\\beta}_3(\\lambda) = y_3 + u_4 - u_3 = 3 + \\lambda - \\lambda = 3$\n$\\hat{\\beta}_4(\\lambda) = y_4 + u_5 - u_4 = 4 - \\lambda - \\lambda = 4 - 2\\lambda$\n$\\hat{\\beta}_5(\\lambda) = y_5 + u_6 - u_5 = 1 + \\lambda - (-\\lambda) = 1 + 2\\lambda$\n$\\hat{\\beta}_6(\\lambda) = y_6 + u_7 - u_6 = 5 + 0 - \\lambda = 5 - \\lambda$\n\nThis path is valid as long as the signs of the differences are preserved.\n$\\beta_2 - \\beta_1 = 2\\lambda - (2 - \\lambda) = 3\\lambda - 2 < 0 \\implies \\lambda < 2/3$\n$\\beta_3 - \\beta_2 = 3 - 2\\lambda > 0 \\implies \\lambda < 3/2$\n$\\beta_4 - \\beta_3 = (4 - 2\\lambda) - 3 = 1 - 2\\lambda > 0 \\implies \\lambda < 1/2$\n$\\beta_5 - \\beta_4 = (1 + 2\\lambda) - (4 - 2\\lambda) = 4\\lambda - 3 < 0 \\implies \\lambda < 3/4$\n$\\beta_6 - \\beta_5 = (5 - \\lambda) - (1 + 2\\lambda) = 4 - 3\\lambda > 0 \\implies \\lambda < 4/3$\n\nThe first difference to become zero determines the first critical value of $\\lambda$. The tightest constraint is $\\lambda < 1/2$. Thus, the first fusion event occurs at $\\lambda_1 = 1/2$, where $\\beta_4 - \\beta_3 = 0$.\n\n**Interval 2: $\\lambda \\in [1/2, \\lambda_2]$**\nAt $\\lambda_1 = 1/2$, indices $3$ and $4$ fuse. For $\\lambda > 1/2$, we have the block partition $\\{\\{1\\}, \\{2\\}, \\{3,4\\}, \\{5\\}, \\{6\\}\\}$.\nThe condition $\\beta_3 = \\beta_4$ implies $|u_4| \\le \\lambda$. The other differences remain nonzero, so $u_2 = -\\lambda, u_3 = \\lambda, u_5 = -\\lambda, u_6 = \\lambda$.\nWe have $\\hat{\\beta}_3 = y_3 + u_4 - u_3 = 3+u_4-\\lambda$ and $\\hat{\\beta}_4 = y_4 + u_5 - u_4 = 4-\\lambda-u_4$.\nSetting $\\hat{\\beta}_3 = \\hat{\\beta}_4$ gives $3+u_4-\\lambda = 4-\\lambda-u_4 \\implies 2u_4=1 \\implies u_4 = 1/2$.\nThe condition $|u_4| \\le \\lambda$ becomes $1/2 \\le \\lambda$, which is consistent with $\\lambda > \\lambda_1$.\nThe common value for the block $\\{3,4\\}$ is $\\hat{\\beta}_{3,4}(\\lambda) = 3 + 1/2 - \\lambda = 3.5 - \\lambda$. This is equal to $\\frac{y_3+y_4}{2} + \\frac{u_5-u_3}{2} = \\frac{3+4}{2} + \\frac{-\\lambda-\\lambda}{2} = 3.5 - \\lambda$.\nThe other coefficients are as before: $\\hat{\\beta}_1(\\lambda) = 2-\\lambda$, $\\hat{\\beta}_2(\\lambda) = 2\\lambda$, $\\hat{\\beta}_5(\\lambda) = 1+2\\lambda$, $\\hat{\\beta}_6(\\lambda) = 5-\\lambda$.\nThe segmentation is $\\{1\\}, \\{2\\}, \\{3,4\\}, \\{5\\}, \\{6\\}$.\nThe solution path is $\\hat{\\beta}(\\lambda) = (2-\\lambda, 2\\lambda, 3.5-\\lambda, 3.5-\\lambda, 1+2\\lambda, 5-\\lambda)$.\nChecking the differences between blocks:\n$\\beta_2 - \\beta_1 = 3\\lambda - 2 < 0 \\implies \\lambda < 2/3$\n$\\beta_3 - \\beta_2 = 3.5 - 3\\lambda > 0 \\implies \\lambda < 3.5/3 = 7/6$\n$\\beta_5 - \\beta_4 = 3\\lambda - 2.5 < 0 \\implies \\lambda < 2.5/3 = 5/6$\n$\\beta_6 - \\beta_5 = 4 - 3\\lambda > 0 \\implies \\lambda < 4/3$\n\nThe tightest constraint is $\\lambda < 2/3$. The next fusion is at $\\lambda_2 = 2/3$, where indices $1$ and $2$ fuse.\n\n**Interval 3: $\\lambda \\in [2/3, \\lambda_3]$**\nAt $\\lambda_2 = 2/3$, indices $1$ and $2$ fuse. The block partition becomes $\\{\\{1,2\\}, \\{3,4\\}, \\{5\\}, \\{6\\}\\}$.\nThis implies $|u_2| \\le \\lambda$ and $|u_4|=1/2 \\le \\lambda$. The active signs between blocks imply $u_3=\\lambda$, $u_5=-\\lambda$, $u_6=\\lambda$.\nWe use the general formula for a block $B$: $\\hat{\\beta}_B(\\lambda) = \\frac{1}{|B|}\\sum_{i \\in B} y_i + \\frac{u_{\\max(B)+1} - u_{\\min(B)}}{|B|}$.\n$\\hat{\\beta}_{\\{1,2\\}}(\\lambda) = \\frac{y_1+y_2}{2} + \\frac{u_3-u_1}{2} = \\frac{2+0}{2} + \\frac{\\lambda-0}{2} = 1 + \\lambda/2$.\n$\\hat{\\beta}_{\\{3,4\\}}(\\lambda) = \\frac{y_3+y_4}{2} + \\frac{u_5-u_3}{2} = \\frac{3+4}{2} + \\frac{-\\lambda-\\lambda}{2} = 3.5 - \\lambda$.\n$\\hat{\\beta}_5(\\lambda) = y_5 + u_6 - u_5 = 1 + \\lambda - (-\\lambda) = 1 + 2\\lambda$.\n$\\hat{\\beta}_6(\\lambda) = y_6 + u_7 - u_6 = 5 - \\lambda$.\nChecking differences between blocks:\n$\\beta_3 - \\beta_2 = (3.5 - \\lambda) - (1 + \\lambda/2) = 2.5 - 1.5\\lambda > 0 \\implies \\lambda < 2.5/1.5 = 5/3$.\n$\\beta_5 - \\beta_4 = (1 + 2\\lambda) - (3.5 - \\lambda) = 3\\lambda - 2.5 < 0 \\implies \\lambda < 2.5/3 = 5/6$.\n$\\beta_6 - \\beta_5 = (5 - \\lambda) - (1 + 2\\lambda) = 4 - 3\\lambda > 0 \\implies \\lambda < 4/3$.\n\nThe tightest constraint is $\\lambda < 5/6$. The next fusion is at $\\lambda_3 = 5/6$, where block $\\{3,4\\}$ fuses with $\\{5\\}$.\n\n**Interval 4: $\\lambda \\in [5/6, \\lambda_4]$**\nAt $\\lambda_3 = 5/6$, block $\\{3,4\\}$ fuses with index $5$. The partition becomes $\\{\\{1,2\\}, \\{3,4,5\\}, \\{6\\}\\}$.\nThis implies $|u_2|, |u_4|, |u_5| \\le \\lambda$. The active signs imply $u_3=\\lambda, u_6=\\lambda$.\n$\\hat{\\beta}_{\\{1,2\\}}(\\lambda) = \\frac{y_1+y_2}{2} + \\frac{u_3-u_1}{2} = 1 + \\lambda/2$.\n$\\hat{\\beta}_{\\{3,4,5\\}}(\\lambda) = \\frac{y_3+y_4+y_5}{3} + \\frac{u_6-u_3}{3} = \\frac{3+4+1}{3} + \\frac{\\lambda-\\lambda}{3} = 8/3$.\n$\\hat{\\beta}_6(\\lambda) = y_6 + u_7 - u_6 = 5 - \\lambda$.\nChecking differences between blocks:\n$\\beta_3 - \\beta_2 = 8/3 - (1+\\lambda/2) = 5/3 - \\lambda/2 > 0 \\implies \\lambda < 10/3$.\n$\\beta_6 - \\beta_5 = (5-\\lambda) - 8/3 = 7/3 - \\lambda > 0 \\implies \\lambda < 7/3$.\n\nThe tightest constraint is $\\lambda < 7/3$. The next fusion is at $\\lambda_4 = 7/3$, where block $\\{3,4,5\\}$ fuses with $\\{6\\}$.\n\n**Interval 5: $\\lambda \\in [7/3, \\lambda_5]$**\nAt $\\lambda_4 = 7/3$, block $\\{3,4,5\\}$ fuses with index $6$. The partition is $\\{\\{1,2\\}, \\{3,4,5,6\\}\\}$.\nThe active sign between blocks implies $u_3=\\lambda$.\n$\\hat{\\beta}_{\\{1,2\\}}(\\lambda) = \\frac{y_1+y_2}{2} + \\frac{u_3-u_1}{2} = 1 + \\lambda/2$.\n$\\hat{\\beta}_{\\{3,4,5,6\\}}(\\lambda) = \\frac{y_3+y_4+y_5+y_6}{4} + \\frac{u_7-u_3}{4} = \\frac{3+4+1+5}{4} + \\frac{0-\\lambda}{4} = 13/4 - \\lambda/4$.\nChecking the difference between the two blocks:\n$\\beta_3 - \\beta_2 = (13/4 - \\lambda/4) - (1 + \\lambda/2) = 9/4 - 3\\lambda/4 > 0 \\implies 9 > 3\\lambda \\implies \\lambda < 3$.\n\nThe final fusion occurs at $\\lambda_5 = 3$, where the two remaining blocks merge.\n\n**For $\\lambda \\ge 3$**\nAll coefficients are fused into a single block $\\{1, \\dots, 6\\}$. The solution is constant across all indices and equals the global mean of the data:\n$\\hat{\\beta}_i(\\lambda) = \\bar{y} = \\frac{1}{6}\\sum_{i=1}^6 y_i = \\frac{2+0+3+4+1+5}{6} = \\frac{15}{6} = 2.5$.\nAs a check, at $\\lambda=3$:\n$\\hat{\\beta}_{\\{1,2\\}}(3) = 1 + 3/2 = 2.5$.\n$\\hat{\\beta}_{\\{3,4,5,6\\}}(3) = 13/4 - 3/4 = 10/4 = 2.5$.\nThe solution is continuous at the critical value, as expected.\n\nThe positive, distinct critical values of $\\lambda$ at which fusions occur are, in increasing order:\n$\\lambda_1 = 1/2$\n$\\lambda_2 = 2/3$\n$\\lambda_3 = 5/6$\n$\\lambda_4 = 7/3$\n$\\lambda_5 = 3$\n\nThe ordered list is $(\\frac{1}{2}, \\frac{2}{3}, \\frac{5}{6}, \\frac{7}{3}, 3)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} & \\frac{2}{3} & \\frac{5}{6} & \\frac{7}{3} & 3\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond its mechanical operation, it is crucial to understand the statistical consequences of applying the fused lasso penalty. While the model is excellent for identifying abrupt changes, or \"jumps,\" in a signal, the regularization process is not without cost; it introduces a systematic bias in the estimates. This problem  uses a simplified two-segment model to help you quantify this effect. Deriving a closed-form expression for the bias in the estimated jump magnitude reveals how the penalty shrinks the estimate and how this shrinkage depends on $\\lambda$ and the data structure.",
            "id": "3122180",
            "problem": "Consider the one-dimensional fused least absolute shrinkage and selection operator (fused lasso) denoising problem for a piecewise-constant signal with exactly two constant segments. Let there be $m_1$ consecutive observations from the first segment with true level $\\theta_1$ followed by $m_2$ consecutive observations from the second segment with true level $\\theta_2$, where the true jump magnitude is $\\Delta = \\theta_2 - \\theta_1$ and satisfies $\\Delta > 0$. Observations follow $y_i = \\theta_i + \\varepsilon_i$ with $\\varepsilon_i$ independent and identically distributed (i.i.d.) $\\mathcal{N}(0,\\sigma^2)$ noise. The fused lasso estimator $\\hat{\\beta} = (\\hat{\\beta}_1,\\ldots,\\hat{\\beta}_{m_1+m_2})$ solves the optimization problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^{m_1+m_2}} \\;\\; \\frac{1}{2}\\sum_{i=1}^{m_1+m_2} (y_i - \\beta_i)^2 \\;+\\; \\lambda \\sum_{i=2}^{m_1+m_2} |\\beta_i - \\beta_{i-1}|,\n$$\nwith tuning parameter $\\lambda > 0$. Assume we restrict attention to estimators that are constant within each segment, i.e., $\\beta_i = \\mu_1$ for $i \\in \\{1,\\ldots,m_1\\}$ and $\\beta_i = \\mu_2$ for $i \\in \\{m_1+1,\\ldots,m_1+m_2\\}$. Denote the sample means within each segment by $\\bar{y}_1 = \\frac{1}{m_1}\\sum_{i=1}^{m_1} y_i$ and $\\bar{y}_2 = \\frac{1}{m_2}\\sum_{i=m_1+1}^{m_1+m_2} y_i$, and define the estimated jump magnitude $J_{\\text{hat}} = \\mu_2 - \\mu_1$.\n\nUsing only foundational definitions and optimality principles of convex estimation, derive a closed-form expression for $J_{\\text{hat}}$ as a function of $\\bar{y}_1$, $\\bar{y}_2$, $\\lambda$, $m_1$, and $m_2$. Then, under a high signal-to-noise ratio regime in which $\\Delta > 0$ and the probability that $J_{\\text{hat}}$ is shrunk to zero is negligible, use your expression to obtain a first-order approximation for the expected estimated jump magnitude $\\mathbb{E}[J_{\\text{hat}}]$ in terms of $\\Delta$, $\\lambda$, $m_1$, and $m_2$.\n\nProvide your final answer as a single closed-form analytic expression for $\\mathbb{E}[J_{\\text{hat}}]$ in terms of $\\Delta$, $\\lambda$, $m_1$, and $m_2$.",
            "solution": "The problem is to derive a closed-form expression for the estimated jump magnitude in a simplified fused lasso setting and then find its expected value under a high signal-to-noise ratio assumption.\n\nFirst, we address the optimization problem under the specified constraint. The fused lasso objective function is given by\n$$\nL(\\beta) = \\frac{1}{2}\\sum_{i=1}^{m_1+m_2} (y_i - \\beta_i)^2 \\;+\\; \\lambda \\sum_{i=2}^{m_1+m_2} |\\beta_i - \\beta_{i-1}|\n$$\nWe are given the constraint that the solution is piecewise constant with two segments, corresponding to the true underlying structure. This means the solution vector $\\beta$ is of the form $\\beta_i = \\mu_1$ for $i \\in \\{1, \\dots, m_1\\}$ and $\\beta_i = \\mu_2$ for $i \\in \\{m_1+1, \\dots, m_1+m_2\\}$. We can substitute this structure into the objective function $L(\\beta)$ to obtain a new objective function in terms of $\\mu_1$ and $\\mu_2$.\n\nThe sum-of-squares term becomes:\n$$\n\\frac{1}{2}\\sum_{i=1}^{m_1} (y_i - \\mu_1)^2 + \\frac{1}{2}\\sum_{i=m_1+1}^{m_1+m_2} (y_i - \\mu_2)^2\n$$\nFor a fixed set of observations $\\{y_i\\}$, minimizing this term with respect to $\\mu_1$ and $\\mu_2$ is equivalent to minimizing\n$$\n\\frac{m_1}{2}(\\mu_1 - \\bar{y}_1)^2 + \\frac{m_2}{2}(\\mu_2 - \\bar{y}_2)^2\n$$\nplus terms that do not depend on $\\mu_1$ or $\\mu_2$. Here, $\\bar{y}_1 = \\frac{1}{m_1}\\sum_{i=1}^{m_1} y_i$ and $\\bar{y}_2 = \\frac{1}{m_2}\\sum_{i=m_1+1}^{m_1+m_2} y_i$ are the sample means of the two segments.\n\nThe penalty term $\\lambda \\sum_{i=2}^{m_1+m_2} |\\beta_i - \\beta_{i-1}|$ simplifies significantly under the constraint. The differences $\\beta_i - \\beta_{i-1}$ are zero for all $i$ except for $i=m_1+1$. At this point, $\\beta_{m_1+1} = \\mu_2$ and $\\beta_{m_1} = \\mu_1$. Thus, the penalty term reduces to $\\lambda|\\mu_2 - \\mu_1|$.\n\nCombining these, the optimization problem becomes a two-dimensional problem in $\\mu_1$ and $\\mu_2$:\n$$\n\\min_{\\mu_1, \\mu_2 \\in \\mathbb{R}} \\;\\; F(\\mu_1, \\mu_2) = \\frac{m_1}{2}(\\mu_1 - \\bar{y}_1)^2 + \\frac{m_2}{2}(\\mu_2 - \\bar{y}_2)^2 + \\lambda |\\mu_2 - \\mu_1|\n$$\nThis is a convex optimization problem. The minimum $(\\hat{\\mu}_1, \\hat{\\mu}_2)$ is found by setting the subgradient of $F(\\mu_1, \\mu_2)$ to zero. The subgradient equations are:\n$$\n\\frac{\\partial F}{\\partial \\mu_1}: \\quad m_1(\\hat{\\mu}_1 - \\bar{y}_1) - \\lambda s = 0\n$$\n$$\n\\frac{\\partial F}{\\partial \\mu_2}: \\quad m_2(\\hat{\\mu}_2 - \\bar{y}_2) + \\lambda s = 0\n$$\nwhere $s$ is an element of the subdifferential of the absolute value function at $\\hat{\\mu}_2 - \\hat{\\mu}_1$. Specifically, $s = \\text{sign}(\\hat{\\mu}_2 - \\hat{\\mu}_1)$ if $\\hat{\\mu}_2 \\neq \\hat{\\mu}_1$, and $s \\in [-1, 1]$ if $\\hat{\\mu}_2 = \\hat{\\mu}_1$.\n\nFrom these equations, we can express $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$:\n$$\n\\hat{\\mu}_1 = \\bar{y}_1 + \\frac{\\lambda s}{m_1}\n$$\n$$\n\\hat{\\mu}_2 = \\bar{y}_2 - \\frac{\\lambda s}{m_2}\n$$\nThe estimated jump magnitude is $J_{\\text{hat}} = \\hat{\\mu}_2 - \\hat{\\mu}_1$. Subtracting the two equations gives:\n$$\nJ_{\\text{hat}} = (\\bar{y}_2 - \\bar{y}_1) - \\lambda s \\left(\\frac{1}{m_1} + \\frac{1}{m_2}\\right)\n$$\nLet $D = \\bar{y}_2 - \\bar{y}_1$ and $C = \\lambda \\left(\\frac{1}{m_1} + \\frac{1}{m_2}\\right)$. The equation becomes $J_{\\text{hat}} = D - sC$. We analyze this based on the value of $s$:\n1.  If $J_{\\text{hat}} > 0$, we must have $s=1$. The solution is $J_{\\text{hat}} = D - C$. This case is self-consistent only if $D-C > 0$, i.e., $D > C$.\n2.  If $J_{\\text{hat}} < 0$, we must have $s=-1$. The solution is $J_{\\text{hat}} = D + C$. This case is self-consistent only if $D+C < 0$, i.e., $D < -C$.\n3.  If $J_{\\text{hat}} = 0$, we have $0 = D - sC$, which implies $s = D/C$ (assuming $C>0$). This requires $s \\in [-1, 1]$, which is equivalent to $|D/C| \\le 1$, or $|D| \\le C$.\n\nCombining these three cases yields a single expression for $J_{\\text{hat}}$:\n$$\nJ_{\\text{hat}} = \n\\begin{cases}\nD - C & \\text{if } D > C \\\\\n0 & \\text{if } |D| \\le C \\\\\nD + C & \\text{if } D < -C\n\\end{cases}\n$$\nThis is the soft-thresholding function applied to $D$ with threshold $C$. It can be written compactly as $J_{\\text{hat}} = \\text{sign}(D)(|D|-C)_+$, where $(x)_+ = \\max(x,0)$. Substituting back the expressions for $D$ and $C$, we have:\n$$\nJ_{\\text{hat}} = \\text{sign}(\\bar{y}_2 - \\bar{y}_1) \\left( |\\bar{y}_2 - \\bar{y}_1| - \\lambda\\left(\\frac{1}{m_1} + \\frac{1}{m_2}\\right) \\right)_+\n$$\nThis is the closed-form expression for the estimated jump magnitude.\n\nNext, we find the first-order approximation for the expected value $\\mathbb{E}[J_{\\text{hat}}]$. We are given the model $y_i = \\theta_i + \\varepsilon_i$ with $\\varepsilon_i \\sim \\text{i.i.d.} \\mathcal{N}(0,\\sigma^2)$. The sample means are random variables:\n$$\n\\bar{y}_1 = \\frac{1}{m_1}\\sum_{i=1}^{m_1} (\\theta_1 + \\varepsilon_i) = \\theta_1 + \\bar{\\varepsilon}_1 \\sim \\mathcal{N}(\\theta_1, \\sigma^2/m_1)\n$$\n$$\n\\bar{y}_2 = \\frac{1}{m_2}\\sum_{i=m_1+1}^{m_1+m_2} (\\theta_2 + \\varepsilon_i) = \\theta_2 + \\bar{\\varepsilon}_2 \\sim \\mathcal{N}(\\theta_2, \\sigma^2/m_2)\n$$\nThe difference $D = \\bar{y}_2 - \\bar{y}_1$ is also a normal random variable, as it is a linear combination of independent normal variables. Its mean and variance are:\n$$\n\\mathbb{E}[D] = \\mathbb{E}[\\bar{y}_2] - \\mathbb{E}[\\bar{y}_1] = \\theta_2 - \\theta_1 = \\Delta\n$$\n$$\n\\text{Var}(D) = \\text{Var}(\\bar{y}_2) + \\text{Var}(\\bar{y}_1) = \\frac{\\sigma^2}{m_2} + \\frac{\\sigma^2}{m_1} = \\sigma^2\\left(\\frac{1}{m_1} + \\frac{1}{m_2}\\right)\n$$\nThe problem states we are in a high signal-to-noise ratio regime where $\\Delta > 0$ and the probability that $J_{\\text{hat}}$ is shrunk to zero is negligible. $J_{\\text{hat}}=0$ if $|D| \\le C$. The condition $P(|D| \\le C) \\approx 0$ with $\\mathbb{E}[D] = \\Delta > 0$ implies that the distribution of $D$ is concentrated in the region $D > C$.\n\nIn this regime, with probability nearly $1$, we have $D > C$. The expression for $J_{\\text{hat}}$ thus simplifies to the first case:\n$$\nJ_{\\text{hat}} \\approx D - C = (\\bar{y}_2 - \\bar{y}_1) - \\lambda\\left(\\frac{1}{m_1} + \\frac{1}{m_2}\\right)\n$$\nThis approximation neglects the non-linear behavior of the thresholding function near the origin, which is justified by the high SNR assumption. This is the first-order approximation.\n\nWe now take the expectation of this approximate expression. Since expectation is a linear operator:\n$$\n\\mathbb{E}[J_{\\text{hat}}] \\approx \\mathbb{E}\\left[(\\bar{y}_2 - \\bar{y}_1) - \\lambda\\left(\\frac{1}{m_1} + \\frac{1}{m_2}\\right)\\right]\n$$\n$$\n\\mathbb{E}[J_{\\text{hat}}] \\approx \\mathbb{E}[\\bar{y}_2 - \\bar{y}_1] - \\lambda\\left(\\frac{1}{m_1} + \\frac{1}{m_2}\\right)\n$$\nUsing the result that $\\mathbb{E}[\\bar{y}_2 - \\bar{y}_1] = \\Delta$, we arrive at the final expression for the expected estimated jump magnitude:\n$$\n\\mathbb{E}[J_{\\text{hat}}] \\approx \\Delta - \\lambda\\left(\\frac{1}{m_1} + \\frac{1}{m_2}\\right)\n$$\nThis shows that under this simplified model, the fused lasso estimator for the jump is biased. The bias is negative, meaning it shrinks the estimate toward zero, and its magnitude is equal to the threshold of the soft-thresholding operator.",
            "answer": "$$\\boxed{\\Delta - \\lambda \\left(\\frac{1}{m_1} + \\frac{1}{m_2}\\right)}$$"
        },
        {
            "introduction": "The behavior of the fused lasso estimator is a result of a delicate balance between the data fidelity term and the total variation penalty. A fascinating question is how this balance shifts when the structure of the input data changes. This thought experiment  explores this interplay by asking you to predict a somewhat counter-intuitive outcome: the effect of duplicating a data point on the fusion process. By analyzing this simple case from first principles, you will develop a much deeper intuition for how data distribution and redundancy influence the model's tendency to fuse coefficients.",
            "id": "3122140",
            "problem": "Consider the one-dimensional Fused Lasso Signal Approximator (FLSA), which estimates a sequence of parameters $\\beta = (\\beta_{1}, \\dots, \\beta_{n})$ from observations $y = (y_{1}, \\dots, y_{n})$ by solving\n$$\n\\min_{\\beta \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\sum_{i=1}^{n} (y_{i} - \\beta_{i})^{2} \\;+\\; \\lambda \\sum_{i=1}^{n-1} |\\beta_{i+1} - \\beta_{i}|,\n$$\nwhere $\\lambda \\ge 0$ is the regularization strength and $|\\cdot|$ denotes the absolute value. You will investigate how duplicating an index (creating a cloned data point with the same observed value and placed adjacently in the chain) affects the tendency of FLSA to fuse adjacent coefficients.\n\nCompare the following two cases.\n\nCase I (no duplication): $y^{(1)} = (0, 3)$ with $n=2$.\n\nCase II (duplicate the second index): $y^{(2)} = (0, 3, 3)$ with $n=3$.\n\nFor each case, define a “local fusion across the boundary” as the event that the estimated coefficients on the left and right sides of the boundary are equal. Concretely, in Case I it means $\\beta_{1} = \\beta_{2}$. In Case II it means $\\beta_{1} = \\beta_{2} = \\beta_{3}$ (i.e., the leftmost coefficient equals the common value on the duplicated right side).\n\nLet $\\lambda^{\\star}$ denote the smallest regularization strength for which this local fusion across the boundary is achieved in the optimal solution. Based only on first principles of the FLSA objective above (no additional formulas are given), which statement about the effect of duplicating the index is correct?\n\nA. Duplicating the index increases $\\lambda^{\\star}$, so duplicates weaken local fusion across that boundary (it becomes harder to fuse across it).\n\nB. Duplicating the index decreases $\\lambda^{\\star}$, so duplicates strengthen local fusion across that boundary (it becomes easier to fuse across it).\n\nC. Duplicating the index has no effect on $\\lambda^{\\star}$ if the duplicate has the same observed value, so the fusion tendency across the boundary is unchanged.\n\nD. Duplicating the index forces immediate fusion across the boundary for any $\\lambda > 0$ because the extra total variation penalty imposes stronger equality.",
            "solution": "The problem asks to compare the critical regularization strength $\\lambda^{\\star}$ for achieving local fusion in two cases involving the one-dimensional Fused Lasso Signal Approximator (FLSA). The objective function is given by:\n$$\nL(\\beta) = \\frac{1}{2}\\sum_{i=1}^{n} (y_{i} - \\beta_{i})^{2} \\;+\\; \\lambda \\sum_{i=1}^{n-1} |\\beta_{i+1} - \\beta_{i}|\n$$\nThis function is convex, and for $\\lambda \\geq 0$, a unique minimizer $\\hat{\\beta}$ exists. The optimality conditions (Karush-Kuhn-Tucker, or KKT) state that the zero vector must be in the subgradient of $L(\\beta)$ at the solution $\\hat{\\beta}$.\n\nThe subgradient of $L(\\beta)$ with respect to a component $\\beta_k$ is given by:\n- For $k=1$: $\\frac{\\partial L}{\\partial \\beta_1} = (\\beta_1 - y_1) - \\lambda g_1$\n- For $k \\in \\{2, \\dots, n-1\\}$: $\\frac{\\partial L}{\\partial \\beta_k} = (\\beta_k - y_k) + \\lambda (g_{k-1} - g_k)$\n- For $k=n$: $\\frac{\\partial L}{\\partial \\beta_n} = (\\beta_n - y_n) + \\lambda g_{n-1}$\nwhere $g_i \\in \\partial|\\beta_{i+1}-\\beta_i|$. This means $g_i = \\text{sign}(\\beta_{i+1}-\\beta_i)$ if $\\beta_{i+1} \\neq \\beta_i$, and $g_i \\in [-1, 1]$ if $\\beta_{i+1} = \\beta_i$.\n\nAt an optimal solution $\\hat{\\beta}$, these subgradients must be zero. The KKT conditions are:\n1. $\\hat{\\beta}_1 - y_1 - \\lambda g_1 = 0$\n2. $\\hat{\\beta}_k - y_k + \\lambda (g_{k-1} - g_k) = 0$ for $k=2, \\dots, n-1$\n3. $\\hat{\\beta}_n - y_n + \\lambda g_{n-1} = 0$\n\nWe will now apply these first principles to find $\\lambda^{\\star}$ for each case.\n\n### Case I Analysis\nHere, $n=2$ and $y^{(1)} = (0, 3)$. The objective function is:\n$$\nL(\\beta_1, \\beta_2) = \\frac{1}{2}(\\beta_1 - 0)^2 + \\frac{1}{2}(\\beta_2 - 3)^2 + \\lambda |\\beta_2 - \\beta_1|\n$$\nThe KKT conditions are:\n1. $\\beta_1 - 0 - \\lambda g_1 = 0$\n2. $\\beta_2 - 3 + \\lambda g_1 = 0$\nwhere $g_1 \\in \\partial|\\beta_2-\\beta_1|$.\n\nFor small $\\lambda > 0$, we expect the solution $\\hat{\\beta}$ to be close to $y$, so $\\hat{\\beta}_1 \\approx 0$ and $\\hat{\\beta}_2 \\approx 3$. This suggests $\\hat{\\beta}_2 > \\hat{\\beta}_1$. In this unfused regime, we must have $g_1 = \\text{sign}(\\beta_2 - \\beta_1) = 1$. The KKT conditions simplify to:\n1. $\\beta_1 - \\lambda = 0 \\implies \\beta_1 = \\lambda$\n2. $\\beta_2 - 3 + \\lambda = 0 \\implies \\beta_2 = 3 - \\lambda$\n\nThis solution is valid as long as the initial assumption $\\beta_2 > \\beta_1$ holds.\n$3 - \\lambda > \\lambda \\implies 3 > 2\\lambda \\implies \\lambda < 1.5$.\n\nAt the boundary $\\lambda = 1.5$, we find $\\beta_1 = 1.5$ and $\\beta_2 = 3 - 1.5 = 1.5$. Thus, $\\beta_1 = \\beta_2$. This is the point of fusion.\nThe problem defines $\\lambda^{\\star}$ as the smallest regularization strength for which local fusion is achieved. Therefore, for Case I, $\\lambda_I^{\\star} = 1.5$.\n\nFor completeness, let's verify the solution for $\\lambda \\ge 1.5$. In the fused regime, $\\beta_1 = \\beta_2 = \\beta_c$. The KKT conditions become:\n1. $\\beta_c - 0 - \\lambda g_1 = 0$\n2. $\\beta_c - 3 + \\lambda g_1 = 0$\nwhere $g_1$ can be any value in $[-1, 1]$. Adding the two equations yields $2\\beta_c - 3 = 0$, which gives the fused value $\\beta_c = 1.5$. Substituting this into the first equation gives $1.5 - \\lambda g_1 = 0$, so $g_1 = 1.5/\\lambda$. The condition for this to be a valid subgradient is $g_1 \\in [-1, 1]$, which means $|1.5/\\lambda| \\le 1$. Since $\\lambda \\ge 0$, this requires $\\lambda \\ge 1.5$. This confirms that fusion occurs for all $\\lambda \\ge 1.5$.\n\n### Case II Analysis\nHere, $n=3$ and $y^{(2)} = (0, 3, 3)$. The objective function is:\n$$\nL(\\beta_1, \\beta_2, \\beta_3) = \\frac{1}{2}(\\beta_1 - 0)^2 + \\frac{1}{2}(\\beta_2 - 3)^2 + \\frac{1}{2}(\\beta_3 - 3)^2 + \\lambda (|\\beta_2 - \\beta_1| + |\\beta_3 - \\beta_2|)\n$$\nThe KKT conditions are:\n1. $\\beta_1 - 0 - \\lambda g_1 = 0$\n2. $\\beta_2 - 3 + \\lambda(g_1 - g_2) = 0$\n3. $\\beta_3 - 3 + \\lambda g_2 = 0$\nwhere $g_1 \\in \\partial|\\beta_2-\\beta_1|$ and $g_2 \\in \\partial|\\beta_3-\\beta_2|$.\n\nFirst, let's analyze the relationship between $\\beta_2$ and $\\beta_3$. Since $y_2=y_3=3$, there is no data evidence to distinguish between them. Any penalty $\\lambda > 0$ on $|\\beta_3 - \\beta_2|$ will drive them together. We can formally prove that $\\beta_2 = \\beta_3$ for any $\\lambda > 0$. If we assume $\\beta_2 \\neq \\beta_3$, we would have $g_2 = \\text{sign}(\\beta_3-\\beta_2) \\in \\{-1, 1\\}$. But this leads to a contradiction within the KKT system, as shown by algebraic manipulation of equations (2) and (3). Thus, for any $\\lambda > 0$, the solution must have $\\beta_2 = \\beta_3$ and consequently $g_2 \\in [-1, 1]$. This is the fusion on the \"duplicated right side\".\n\nThe problem defines \"local fusion across the boundary\" as the event $\\beta_1 = \\beta_2 = \\beta_3$. We need to find the smallest $\\lambda$ for this to happen.\nGiven $\\beta_2 = \\beta_3$, let's analyze the regime where $\\beta_1 \\neq \\beta_2$. As in Case I, we expect $\\beta_1 \\approx 0$ and $\\beta_2 \\approx 3$, so we assume $\\beta_2 > \\beta_1$, which implies $g_1 = 1$. The KKT conditions become:\n1. $\\beta_1 - \\lambda = 0 \\implies \\beta_1 = \\lambda$\n2. $\\beta_2 - 3 + \\lambda(1 - g_2) = 0$\n3. $\\beta_2 - 3 + \\lambda g_2 = 0$\n\nFrom equation (3), we have $\\lambda g_2 = 3 - \\beta_2$. Substituting this into (2) gives $\\beta_2 - 3 + \\lambda - (3 - \\beta_2) = 0$, which simplifies to $2\\beta_2 - 6 + \\lambda = 0$. This yields $\\beta_2 = 3 - \\lambda/2$.\nThis solution is valid as long as our assumption $\\beta_2 > \\beta_1$ holds:\n$3 - \\lambda/2 > \\lambda \\implies 3 > 3\\lambda/2 \\implies 6 > 3\\lambda \\implies \\lambda < 2$.\n\nAt the boundary $\\lambda = 2$, we find $\\beta_1 = 2$ and $\\beta_2 = 3 - 2/2 = 2$. Since we already know $\\beta_2 = \\beta_3$, at this point we have $\\beta_1 = \\beta_2 = \\beta_3 = 2$. This is the point of full fusion as defined in the problem.\nTherefore, for Case II, $\\lambda_{II}^{\\star} = 2$.\n\nAgain, for completeness, we verify the fully fused solution $\\beta_1 = \\beta_2 = \\beta_3 = \\beta_c$ for $\\lambda \\ge 2$. Summing the three KKT equations yields $\\beta_1 + \\beta_2 + \\beta_3 - (y_1+y_2+y_3) = 0$, so $3\\beta_c - (0+3+3) = 0$, which gives $\\beta_c = 2$.\nSubstituting $\\beta_c=2$ into the KKT system:\n1. $2 - 0 - \\lambda g_1 = 0 \\implies g_1 = 2/\\lambda$\n3. $2 - 3 + \\lambda g_2 = 0 \\implies g_2 = 1/\\lambda$\nThese are valid subgradients provided $|g_1| \\le 1$ and $|g_2| \\le 1$.\n$|2/\\lambda| \\le 1 \\implies \\lambda \\ge 2$.\n$|1/\\lambda| \\le 1 \\implies \\lambda \\ge 1$.\nBoth conditions must hold, so we must have $\\lambda \\ge 2$. This confirms that full fusion occurs for $\\lambda \\ge 2$.\n\n### Comparison and Conclusion\nWe have derived the critical regularization strengths from first principles:\n-   Case I: $\\lambda_I^{\\star} = 1.5$\n-   Case II: $\\lambda_{II}^{\\star} = 2$\n\nWe observe that $\\lambda_{II}^{\\star} > \\lambda_I^{\\star}$. This means that a stronger regularization penalty is required to achieve fusion across the boundary when the data point on the right side is duplicated. Duplicating the index has made it harder to fuse the leftmost coefficient with the coefficients on its right.\n\nNow, we evaluate the given options:\n\nA. Duplicating the index increases $\\lambda^{\\star}$, so duplicates weaken local fusion across that boundary (it becomes harder to fuse across it).\nOur analysis shows that $\\lambda^{\\star}$ increases from $1.5$ to $2$. A higher $\\lambda^{\\star}$ signifies that a stronger penalty is needed to cause fusion, meaning fusion is \"weaker\" or \"harder to achieve\". This statement is consistent with our findings.\n**Verdict: Correct.**\n\nB. Duplicating the index decreases $\\lambda^{\\star}$, so duplicates strengthen local fusion across that boundary (it becomes easier to fuse across it).\nOur analysis shows that $\\lambda^{\\star}$ increases, not decreases.\n**Verdict: Incorrect.**\n\nC. Duplicating the index has no effect on $\\lambda^{\\star}$ if the duplicate has the same observed value, so the fusion tendency across the boundary is unchanged.\nOur analysis shows that $\\lambda^{\\star}$ changes from $1.5$ to $2$.\n**Verdict: Incorrect.**\n\nD. Duplicating the index forces immediate fusion across the boundary for any $\\lambda > 0$ because the extra total variation penalty imposes stronger equality.\nOur analysis shows that fusion across the boundary (i.e., $\\beta_1 = \\beta_2$) only happens at $\\lambda = 2$. It is the fusion between the duplicated points ($\\beta_2 = \\beta_3$) that occurs for any $\\lambda > 0$, not the fusion across the boundary of interest.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}