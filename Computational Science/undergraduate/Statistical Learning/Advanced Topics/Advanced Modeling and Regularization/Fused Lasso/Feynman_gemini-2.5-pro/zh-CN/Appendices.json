{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握融合套索，没有什么比一步步追踪其解的路径更有效的方法了。本练习将通过手动计算一个小数据集的完整解路径，让你深入融合套索的核心机制，揭示分段常数解是如何随着正则化参数 $\\lambda$ 的变化而逐步形成的。通过解决这个问题，你将直接应用最优性条件（KKT条件），并确切地看到相邻系数何时以及为何会“融合”成一个单一的块，从而建立坚实的基础直觉。",
            "id": "3122167",
            "problem": "考虑在长度为 $n=6$ 的链上的一维融合套索（全变分降噪）问题。给定数据 $y \\in \\mathbb{R}^{6}$，其条目为 $y = (2, 0, 3, 4, 1, 5)$，将融合套索估计量 $\\hat{\\beta}(\\lambda) \\in \\mathbb{R}^{6}$（对于调整参数 $\\lambda \\ge 0$）定义为以下凸优化问题的解：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{6}} \\;\\; \\frac{1}{2}\\sum_{i=1}^{6} (\\beta_{i} - y_{i})^{2} \\;+\\; \\lambda \\sum_{i=2}^{6} |\\beta_{i} - \\beta_{i-1}|.\n$$\n从带有非光滑惩罚项的凸函数的基本最优性条件（次梯度最优性或 Karush–Kuhn–Tucker 条件）出发，手动推导当 $\\lambda$ 从 0 增加到一个足够大的值（使得所有坐标都融合为一个常数）时的精确分段线性解路径 $\\hat{\\beta}(\\lambda)$。在你的推导中，明确地：\n- 在每个差值的活跃符号保持不变的 $\\lambda$ 区间上，确定 $\\{1,\\dots,6\\}$ 到连续融合块的分割。\n- 对于每个这样的区间，用块均值和 $\\lambda$ 表示 $\\hat{\\beta}(\\lambda)$ 的块内常数值。\n- 确定所有使得相邻块发生融合的 $\\lambda$ 临界值。\n\n你最终报告的答案必须是所有发生融合的不同正 $\\lambda$ 临界值的有序列表（按升序排列）。给出精确值，不要四舍五入。将该列表报告为单个行向量。不需要单位。",
            "solution": "问题要求解给定数据向量 $y \\in \\mathbb{R}^6$ 的一维融合套索估计量 $\\hat{\\beta}(\\lambda)$ 的解路径。该估计量是以下凸优化问题的解：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{6}} L(\\beta) = \\min_{\\beta \\in \\mathbb{R}^{6}} \\left( \\frac{1}{2}\\sum_{i=1}^{6} (\\beta_{i} - y_{i})^{2} \\;+\\; \\lambda \\sum_{i=2}^{6} |\\beta_{i} - \\beta_{i-1}| \\right)\n$$\n其中 $y = (2, 0, 3, 4, 1, 5)$ 且 $\\lambda \\ge 0$。\n\n目标函数 $L(\\beta)$ 是凸函数。Karush-Kuhn-Tucker (KKT) 或次梯度最优性条件表明，向量 $\\hat{\\beta}$ 是一个最小化子，当且仅当零向量位于 $L(\\beta)$ 在 $\\hat{\\beta}$ 处的次微分中。$L(\\beta)$ 的次微分由 $\\partial L(\\beta) = (\\beta - y) + \\lambda \\partial P(\\beta)$ 给出，其中 $P(\\beta)=\\sum_{i=2}^{6} |\\beta_{i} - \\beta_{i-1}|$。\n\n让我们引入与每个差值相关联的对偶变量 $u_i$。最优性条件可以表示为一个原始-对偶系统。令 $u_1=0$ 且 $u_{n+1}=u_7=0$。\n这些条件是：\n1. 对 $i=1, \\dots, 6$，有 $\\hat{\\beta}_i - y_i = u_{i+1} - u_i$。\n2. 对 $i=2, \\dots, 6$，有 $u_i \\in \\lambda \\cdot \\partial|\\hat{\\beta}_i - \\hat{\\beta}_{i-1}|$。这意味着：\n   - 如果 $\\hat{\\beta}_i - \\hat{\\beta}_{i-1} \\neq 0$，则 $u_i = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_i - \\hat{\\beta}_{i-1})$。\n   - 如果 $\\hat{\\beta}_i - \\hat{\\beta}_{i-1} = 0$，则 $|u_i| \\le \\lambda$。\n\n将第一个条件从 $i=1$ 到 $k$ 求和，我们得到 $\\sum_{i=1}^k (\\hat{\\beta}_i - y_i) = u_{k+1} - u_1 = u_{k+1}$。这提供了一种从原始解中找到对偶变量的方法：对 $k=1, \\dots, 6$，有 $u_{k+1} = \\sum_{i=1}^k (\\hat{\\beta}_i - y_i)$。条件 $u_7=0$ 意味着 $\\sum_{i=1}^6 (\\hat{\\beta}_i - y_i) = 0$，所以 $\\sum_{i=1}^6 \\hat{\\beta}_i = \\sum_{i=1}^6 y_i$。\n\n我们通过将 $\\lambda$ 从 0 开始增加来追踪解路径 $\\hat{\\beta}(\\lambda)$。\n\n**区间 1: $\\lambda \\in [0, \\lambda_1]$**\n对于 $\\lambda=0$，解为 $\\hat{\\beta}(0) = y = (2, 0, 3, 4, 1, 5)$。\n差值为：\n$\\beta_2 - \\beta_1 = -2$\n$\\beta_3 - \\beta_2 = 3$\n$\\beta_4 - \\beta_3 = 1$\n$\\beta_5 - \\beta_4 = -3$\n$\\beta_6 - \\beta_5 = 4$\n对于 $\\lambda > 0$ 且足够小的情况，差值的符号保持不变。活跃符号为 $s = (\\text{sign}(\\beta_2-\\beta_1), \\dots, \\text{sign}(\\beta_6-\\beta_5)) = (-1, 1, 1, -1, 1)$。\n这意味着对 $i=2, \\dots, 6$，有 $u_i = \\lambda s_i$：\n$u_2 = -\\lambda$, $u_3 = \\lambda$, $u_4 = \\lambda$, $u_5 = -\\lambda$, $u_6 = \\lambda$。其中 $u_1=0$ 且 $u_7=0$。\n\n使用 $\\hat{\\beta}_i = y_i + u_{i+1} - u_i$，我们找到分段线性路径：\n$\\hat{\\beta}_1(\\lambda) = y_1 + u_2 - u_1 = 2 - \\lambda - 0 = 2 - \\lambda$\n$\\hat{\\beta}_2(\\lambda) = y_2 + u_3 - u_2 = 0 + \\lambda - (-\\lambda) = 2\\lambda$\n$\\hat{\\beta}_3(\\lambda) = y_3 + u_4 - u_3 = 3 + \\lambda - \\lambda = 3$\n$\\hat{\\beta}_4(\\lambda) = y_4 + u_5 - u_4 = 4 - \\lambda - \\lambda = 4 - 2\\lambda$\n$\\hat{\\beta}_5(\\lambda) = y_5 + u_6 - u_5 = 1 + \\lambda - (-\\lambda) = 1 + 2\\lambda$\n$\\hat{\\beta}_6(\\lambda) = y_6 + u_7 - u_6 = 5 + 0 - \\lambda = 5 - \\lambda$\n\n只要差值的符号保持不变，该路径就有效。\n$\\beta_2 - \\beta_1 = 2\\lambda - (2 - \\lambda) = 3\\lambda - 2 < 0 \\implies \\lambda < 2/3$\n$\\beta_3 - \\beta_2 = 3 - 2\\lambda > 0 \\implies \\lambda < 3/2$\n$\\beta_4 - \\beta_3 = (4 - 2\\lambda) - 3 = 1 - 2\\lambda > 0 \\implies \\lambda < 1/2$\n$\\beta_5 - \\beta_4 = (1 + 2\\lambda) - (4 - 2\\lambda) = 4\\lambda - 3 < 0 \\implies \\lambda < 3/4$\n$\\beta_6 - \\beta_5 = (5 - \\lambda) - (1 + 2\\lambda) = 4 - 3\\lambda > 0 \\implies \\lambda < 4/3$\n\n第一个变为零的差值决定了 $\\lambda$ 的第一个临界值。最紧的约束是 $\\lambda < 1/2$。因此，第一个融合事件发生在 $\\lambda_1 = 1/2$ 处，此时 $\\beta_4 - \\beta_3 = 0$。\n\n**区间 2: $\\lambda \\in [1/2, \\lambda_2]$**\n在 $\\lambda_1 = 1/2$ 处，索引 3 和 4 融合。对于 $\\lambda > 1/2$，我们有块划分 $\\{\\{1\\}, \\{2\\}, \\{3,4\\}, \\{5\\}, \\{6\\}\\}$。\n条件 $\\beta_3 = \\beta_4$ 意味着 $|u_4| \\le \\lambda$。其他差值保持非零，因此 $u_2 = -\\lambda, u_3 = \\lambda, u_5 = -\\lambda, u_6 = \\lambda$。\n我们有 $\\hat{\\beta}_3 = y_3 + u_4 - u_3 = 3+u_4-\\lambda$ 和 $\\hat{\\beta}_4 = y_4 + u_5 - u_4 = 4-\\lambda-u_4$。\n令 $\\hat{\\beta}_3 = \\hat{\\beta}_4$ 得到 $3+u_4-\\lambda = 4-\\lambda-u_4 \\implies 2u_4=1 \\implies u_4 = 1/2$。\n条件 $|u_4| \\le \\lambda$ 变为 $1/2 \\le \\lambda$，这与 $\\lambda > \\lambda_1$ 一致。\n块 $\\{3,4\\}$ 的共同值为 $\\hat{\\beta}_{3,4}(\\lambda) = 3 + 1/2 - \\lambda = 3.5 - \\lambda$。这等于 $\\frac{y_3+y_4}{2} + \\frac{u_5-u_3}{2} = \\frac{3+4}{2} + \\frac{-\\lambda-\\lambda}{2} = 3.5 - \\lambda$。\n其他系数与之前相同：$\\hat{\\beta}_1(\\lambda) = 2-\\lambda$，$\\hat{\\beta}_2(\\lambda) = 2\\lambda$，$\\hat{\\beta}_5(\\lambda) = 1+2\\lambda$，$\\hat{\\beta}_6(\\lambda) = 5-\\lambda$。\n分割为 $\\{1\\}, \\{2\\}, \\{3,4\\}, \\{5\\}, \\{6\\}$。\n解路径为 $\\hat{\\beta}(\\lambda) = (2-\\lambda, 2\\lambda, 3.5-\\lambda, 3.5-\\lambda, 1+2\\lambda, 5-\\lambda)$。\n检查块之间的差值：\n$\\beta_2 - \\beta_1 = 3\\lambda - 2 < 0 \\implies \\lambda < 2/3$\n$\\beta_3 - \\beta_2 = 3.5 - 3\\lambda > 0 \\implies \\lambda < 3.5/3 = 7/6$\n$\\beta_5 - \\beta_4 = 3\\lambda - 2.5 < 0 \\implies \\lambda < 2.5/3 = 5/6$\n$\\beta_6 - \\beta_5 = 4 - 3\\lambda > 0 \\implies \\lambda < 4/3$\n\n最紧的约束是 $\\lambda < 2/3$。下一次融合发生在 $\\lambda_2 = 2/3$ 处，此时索引 1 和 2 融合。\n\n**区间 3: $\\lambda \\in [2/3, \\lambda_3]$**\n在 $\\lambda_2 = 2/3$ 处，索引 1 和 2 融合。块划分变为 $\\{\\{1,2\\}, \\{3,4\\}, \\{5\\}, \\{6\\}\\}$。\n这意味着 $|u_2| \\le \\lambda$ 且 $|u_4|=1/2 \\le \\lambda$。块间的活跃符号意味着 $u_3=\\lambda$, $u_5=-\\lambda$, $u_6=\\lambda$。\n我们使用块 $B$ 的通用公式：$\\hat{\\beta}_B(\\lambda) = \\frac{1}{|B|}\\sum_{i \\in B} y_i + \\frac{u_{\\max(B)+1} - u_{\\min(B)}}{|B|}$。\n$\\hat{\\beta}_{\\{1,2\\}}(\\lambda) = \\frac{y_1+y_2}{2} + \\frac{u_3-u_1}{2} = \\frac{2+0}{2} + \\frac{\\lambda-0}{2} = 1 + \\lambda/2$。\n$\\hat{\\beta}_{\\{3,4\\}}(\\lambda) = \\frac{y_3+y_4}{2} + \\frac{u_5-u_3}{2} = \\frac{3+4}{2} + \\frac{-\\lambda-\\lambda}{2} = 3.5 - \\lambda$。\n$\\hat{\\beta}_5(\\lambda) = y_5 + u_6 - u_5 = 1 + \\lambda - (-\\lambda) = 1 + 2\\lambda$。\n$\\hat{\\beta}_6(\\lambda) = y_6 + u_7 - u_6 = 5 - \\lambda$。\n检查块之间的差值：\n$\\beta_3 - \\beta_2 = (3.5 - \\lambda) - (1 + \\lambda/2) = 2.5 - 1.5\\lambda > 0 \\implies \\lambda < 2.5/1.5 = 5/3$。\n$\\beta_5 - \\beta_4 = (1 + 2\\lambda) - (3.5 - \\lambda) = 3\\lambda - 2.5 < 0 \\implies \\lambda < 2.5/3 = 5/6$。\n$\\beta_6 - \\beta_5 = (5 - \\lambda) - (1 + 2\\lambda) = 4 - 3\\lambda > 0 \\implies \\lambda < 4/3$。\n\n最紧的约束是 $\\lambda < 5/6$。下一次融合发生在 $\\lambda_3 = 5/6$ 处，此时块 $\\{3,4\\}$ 与 $\\{5\\}$ 融合。\n\n**区间 4: $\\lambda \\in [5/6, \\lambda_4]$**\n在 $\\lambda_3 = 5/6$ 处，块 $\\{3,4\\}$ 与索引 5 融合。划分变为 $\\{\\{1,2\\}, \\{3,4,5\\}, \\{6\\}\\}$。\n这意味着 $|u_2|, |u_4|, |u_5| \\le \\lambda$。活跃符号意味着 $u_3=\\lambda, u_6=\\lambda$。\n$\\hat{\\beta}_{\\{1,2\\}}(\\lambda) = \\frac{y_1+y_2}{2} + \\frac{u_3-u_1}{2} = 1 + \\lambda/2$。\n$\\hat{\\beta}_{\\{3,4,5\\}}(\\lambda) = \\frac{y_3+y_4+y_5}{3} + \\frac{u_6-u_3}{3} = \\frac{3+4+1}{3} + \\frac{\\lambda-\\lambda}{3} = 8/3$。\n$\\hat{\\beta}_6(\\lambda) = y_6 + u_7 - u_6 = 5 - \\lambda$。\n检查块之间的差值：\n$\\beta_3 - \\beta_2 = 8/3 - (1+\\lambda/2) = 5/3 - \\lambda/2 > 0 \\implies \\lambda < 10/3$。\n$\\beta_6 - \\beta_5 = (5-\\lambda) - 8/3 = 7/3 - \\lambda > 0 \\implies \\lambda < 7/3$。\n\n最紧的约束是 $\\lambda < 7/3$。下一次融合发生在 $\\lambda_4 = 7/3$ 处，此时块 $\\{3,4,5\\}$ 与 $\\{6\\}$ 融合。\n\n**区间 5: $\\lambda \\in [7/3, \\lambda_5]$**\n在 $\\lambda_4 = 7/3$ 处，块 $\\{3,4,5\\}$ 与索引 6 融合。划分为 $\\{\\{1,2\\}, \\{3,4,5,6\\}\\}$。\n块间的活跃符号意味着 $u_3=\\lambda$。\n$\\hat{\\beta}_{\\{1,2\\}}(\\lambda) = \\frac{y_1+y_2}{2} + \\frac{u_3-u_1}{2} = 1 + \\lambda/2$。\n$\\hat{\\beta}_{\\{3,4,5,6\\}}(\\lambda) = \\frac{y_3+y_4+y_5+y_6}{4} + \\frac{u_7-u_3}{4} = \\frac{3+4+1+5}{4} + \\frac{0-\\lambda}{4} = 13/4 - \\lambda/4$。\n检查两个块之间的差值：\n$\\beta_3 - \\beta_2 = (13/4 - \\lambda/4) - (1 + \\lambda/2) = 9/4 - 3\\lambda/4 > 0 \\implies 9 > 3\\lambda \\implies \\lambda < 3$。\n\n最终的融合发生在 $\\lambda_5 = 3$ 处，此时剩下的两个块合并。\n\n**对于 $\\lambda \\ge 3$**\n所有系数都融合到一个块 $\\{1, \\dots, 6\\}$ 中。解在所有索引上都是常数，等于数据的全局均值：\n$\\hat{\\beta}_i(\\lambda) = \\bar{y} = \\frac{1}{6}\\sum_{i=1}^6 y_i = \\frac{2+0+3+4+1+5}{6} = \\frac{15}{6} = 2.5$。\n作为检验，在 $\\lambda=3$ 时：\n$\\hat{\\beta}_{\\{1,2\\}}(3) = 1 + 3/2 = 2.5$。\n$\\hat{\\beta}_{\\{3,4,5,6\\}}(3) = 13/4 - 3/4 = 10/4 = 2.5$。\n解在临界值处是连续的，正如预期。\n\n发生融合的正的、不同的 $\\lambda$ 临界值按升序排列为：\n$\\lambda_1 = 1/2$\n$\\lambda_2 = 2/3$\n$\\lambda_3 = 5/6$\n$\\lambda_4 = 7/3$\n$\\lambda_5 = 3$\n\n有序列表为 $(\\frac{1}{2}, \\frac{2}{3}, \\frac{5}{6}, \\frac{7}{3}, 3)$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} & \\frac{2}{3} & \\frac{5}{6} & \\frac{7}{3} & 3\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在了解了融合是如何发生的之后，一个自然的问题是什么因素会影响这个过程，特别是数据保真度项与惩罚项之间的权衡。这个思想实验通过一个简单而富有洞察力的场景来探讨这种平衡：当我们复制一个数据点时会发生什么？ 通过比较有无重复数据点时的融合行为，你将对最小二乘项（数据保真度）和总变分惩罚项如何相互作用以形成最终解有更深的直觉，并理解数据在每个位置的“权重”在抵抗融合过程中扮演的关键角色。",
            "id": "3122140",
            "problem": "考虑一维融合套索信号近似器 (FLSA)，它通过求解以下问题，从观测值 $y = (y_{1}, \\dots, y_{n})$ 估计参数序列 $\\beta = (\\beta_{1}, \\dots, \\beta_{n})$：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\sum_{i=1}^{n} (y_{i} - \\beta_{i})^{2} \\;+\\; \\lambda \\sum_{i=1}^{n-1} |\\beta_{i+1} - \\beta_{i}|,\n$$\n其中 $\\lambda \\ge 0$ 是正则化强度，$|\\cdot|$ 表示绝对值。您将研究复制一个索引（即创建一个具有相同观测值并放置在链中相邻位置的克隆数据点）如何影响 FLSA 融合相邻系数的趋势。\n\n比较以下两种情况。\n\n情况 I（无复制）：$y^{(1)} = (0, 3)$，其中 $n=2$。\n\n情况 II（复制第二个索引）：$y^{(2)} = (0, 3, 3)$，其中 $n=3$。\n\n对于每种情况，将“跨边界的局部融合”定义为边界左右两侧的估计系数相等的事件。具体来说，在情况 I 中，这意味着 $\\beta_{1} = \\beta_{2}$。在情况 II 中，这意味着 $\\beta_{1} = \\beta_{2} = \\beta_{3}$（即，最左边的系数等于复制的右侧的共同值）。\n\n令 $\\lambda^{\\star}$ 表示在最优解中实现这种跨边界局部融合的最小正则化强度。仅基于上述 FLSA 目标函数的第一性原理（没有给出额外的公式），关于复制索引效果的哪个陈述是正确的？\n\nA. 复制索引会增加 $\\lambda^{\\star}$，因此复制会削弱跨该边界的局部融合（融合变得更难）。\n\nB. 复制索引会减小 $\\lambda^{\\star}$，因此复制会加强跨该边界的局部融合（融合变得更容易）。\n\nC. 如果复制的数据点具有相同的观测值，则复制索引对 $\\lambda^{\\star}$ 没有影响，因此跨边界的融合趋势不变。\n\nD. 对于任何 $\\lambda > 0$，复制索引会强制立即跨边界融合，因为额外的总变差惩罚项施加了更强的相等性约束。",
            "solution": "问题要求比较在一维融合套索信号近似器 (FLSA) 的两种情况下实现局部融合所需的临界正则化强度 $\\lambda^{\\star}$。目标函数由下式给出：\n$$\nL(\\beta) = \\frac{1}{2}\\sum_{i=1}^{n} (y_{i} - \\beta_{i})^{2} \\;+\\; \\lambda \\sum_{i=1}^{n-1} |\\beta_{i+1} - \\beta_{i}|\n$$\n该函数是凸函数，对于 $\\lambda \\geq 0$，存在唯一的最小化子 $\\hat{\\beta}$。最优性条件（Karush-Kuhn-Tucker，或 KKT）指出，在解 $\\hat{\\beta}$ 处，$L(\\beta)$ 的次梯度必须包含零向量。\n\n关于分量 $\\beta_k$ 的 $L(\\beta)$ 的次梯度由下式给出：\n- 对于 $k=1$：$\\frac{\\partial L}{\\partial \\beta_1} = (\\beta_1 - y_1) - \\lambda g_1$\n- 对于 $k \\in \\{2, \\dots, n-1\\}$：$\\frac{\\partial L}{\\partial \\beta_k} = (\\beta_k - y_k) + \\lambda (g_{k-1} - g_k)$\n- 对于 $k=n$：$\\frac{\\partial L}{\\partial \\beta_n} = (\\beta_n - y_n) + \\lambda g_{n-1}$\n其中 $g_i \\in \\partial|\\beta_{i+1}-\\beta_i|$。这意味着如果 $\\beta_{i+1} \\neq \\beta_i$，则 $g_i = \\text{sign}(\\beta_{i+1}-\\beta_i)$；如果 $\\beta_{i+1} = \\beta_i$，则 $g_i \\in [-1, 1]$。\n\n在最优解 $\\hat{\\beta}$ 处，这些次梯度必须为零。KKT 条件是：\n1. $\\hat{\\beta}_1 - y_1 - \\lambda g_1 = 0$\n2. $\\hat{\\beta}_k - y_k + \\lambda (g_{k-1} - g_k) = 0$ for $k=2, \\dots, n-1$\n3. $\\hat{\\beta}_n - y_n + \\lambda g_{n-1} = 0$\n\n我们现在将应用这些第一性原理来为每种情况找到 $\\lambda^{\\star}$。\n\n### 情况 I 分析\n这里，$n=2$ 且 $y^{(1)} = (0, 3)$。目标函数是：\n$$\nL(\\beta_1, \\beta_2) = \\frac{1}{2}(\\beta_1 - 0)^2 + \\frac{1}{2}(\\beta_2 - 3)^2 + \\lambda |\\beta_2 - \\beta_1|\n$$\nKKT 条件是：\n1. $\\beta_1 - 0 - \\lambda g_1 = 0$\n2. $\\beta_2 - 3 + \\lambda g_1 = 0$\n其中 $g_1 \\in \\partial|\\beta_2-\\beta_1|$。\n\n对于较小的 $\\lambda > 0$，我们期望解 $\\hat{\\beta}$ 接近于 $y$，因此 $\\hat{\\beta}_1 \\approx 0$ 且 $\\hat{\\beta}_2 \\approx 3$。这表明 $\\hat{\\beta}_2 > \\hat{\\beta}_1$。在这种未融合的状态下，我们必须有 $g_1 = \\text{sign}(\\beta_2 - \\beta_1) = 1$。KKT 条件简化为：\n1. $\\beta_1 - \\lambda = 0 \\implies \\beta_1 = \\lambda$\n2. $\\beta_2 - 3 + \\lambda = 0 \\implies \\beta_2 = 3 - \\lambda$\n\n只要初始假设 $\\beta_2 > \\beta_1$ 成立，该解就有效。\n$3 - \\lambda > \\lambda \\implies 3 > 2\\lambda \\implies \\lambda < 1.5$。\n\n在边界 $\\lambda = 1.5$ 处，我们得到 $\\beta_1 = 1.5$ 且 $\\beta_2 = 3 - 1.5 = 1.5$。因此，$\\beta_1 = \\beta_2$。这就是融合点。\n问题将 $\\lambda^{\\star}$ 定义为实现局部融合的最小正则化强度。因此，对于情况 I，$\\lambda_I^{\\star} = 1.5$。\n\n为完整起见，我们验证 $\\lambda \\ge 1.5$ 时的解。在融合状态下，$\\beta_1 = \\beta_2 = \\beta_c$。KKT 条件变为：\n1. $\\beta_c - 0 - \\lambda g_1 = 0$\n2. $\\beta_c - 3 + \\lambda g_1 = 0$\n其中 $g_1$ 可以是 $[-1, 1]$ 中的任何值。将两个方程相加得到 $2\\beta_c - 3 = 0$，这给出融合值 $\\beta_c = 1.5$。将此代入第一个方程得到 $1.5 - \\lambda g_1 = 0$，所以 $g_1 = 1.5/\\lambda$。使其成为有效次梯度的条件是 $g_1 \\in [-1, 1]$，这意味着 $|1.5/\\lambda| \\le 1$。由于 $\\lambda \\ge 0$，这要求 $\\lambda \\ge 1.5$。这证实了对于所有 $\\lambda \\ge 1.5$，都会发生融合。\n\n### 情况 II 分析\n这里，$n=3$ 且 $y^{(2)} = (0, 3, 3)$。目标函数是：\n$$\nL(\\beta_1, \\beta_2, \\beta_3) = \\frac{1}{2}(\\beta_1 - 0)^2 + \\frac{1}{2}(\\beta_2 - 3)^2 + \\frac{1}{2}(\\beta_3 - 3)^2 + \\lambda (|\\beta_2 - \\beta_1| + |\\beta_3 - \\beta_2|)\n$$\nKKT 条件是：\n1. $\\beta_1 - 0 - \\lambda g_1 = 0$\n2. $\\beta_2 - 3 + \\lambda(g_1 - g_2) = 0$\n3. $\\beta_3 - 3 + \\lambda g_2 = 0$\n其中 $g_1 \\in \\partial|\\beta_2-\\beta_1|$ 且 $g_2 \\in \\partial|\\beta_3-\\beta_2|$。\n\n首先，我们来分析 $\\beta_2$ 和 $\\beta_3$ 之间的关系。由于 $y_2=y_3=3$，没有数据证据可以区分它们。任何对 $|\\beta_3 - \\beta_2|$ 的惩罚项 $\\lambda > 0$ 都会使它们趋于相等。我们可以形式上证明，对于任何 $\\lambda > 0$，$\\beta_2 = \\beta_3$。如果我们假设 $\\beta_2 \\neq \\beta_3$，我们将得到 $g_2 = \\text{sign}(\\beta_3-\\beta_2) \\in \\{-1, 1\\}$。但这通过对 KKT 系统中的方程 (2) 和 (3) 进行代数操作会导致矛盾。因此，对于任何 $\\lambda > 0$，解必须有 $\\beta_2 = \\beta_3$，因此 $g_2 \\in [-1, 1]$。这就是在“复制的右侧”上的融合。\n\n问题将“跨边界的局部融合”定义为事件 $\\beta_1 = \\beta_2 = \\beta_3$。我们需要找到使之发生的最小 $\\lambda$。\n给定 $\\beta_2 = \\beta_3$，我们来分析 $\\beta_1 \\neq \\beta_2$ 的状态。与情况 I 一样，我们期望 $\\beta_1 \\approx 0$ 且 $\\beta_2 \\approx 3$，所以我们假设 $\\beta_2 > \\beta_1$，这意味着 $g_1 = 1$。KKT 条件变为：\n1. $\\beta_1 - \\lambda = 0 \\implies \\beta_1 = \\lambda$\n2. $\\beta_2 - 3 + \\lambda(1 - g_2) = 0$\n3. $\\beta_2 - 3 + \\lambda g_2 = 0$\n\n从方程 (3)，我们有 $\\lambda g_2 = 3 - \\beta_2$。将其代入 (2) 得到 $\\beta_2 - 3 + \\lambda - (3 - \\beta_2) = 0$，简化为 $2\\beta_2 - 6 + \\lambda = 0$。这得出 $\\beta_2 = 3 - \\lambda/2$。\n只要我们的假设 $\\beta_2 > \\beta_1$ 成立，该解就有效：\n$3 - \\lambda/2 > \\lambda \\implies 3 > 3\\lambda/2 \\implies 6 > 3\\lambda \\implies \\lambda < 2$。\n\n在边界 $\\lambda = 2$ 处，我们得到 $\\beta_1 = 2$ 且 $\\beta_2 = 3 - 2/2 = 2$。由于我们已经知道 $\\beta_2 = \\beta_3$，此时我们有 $\\beta_1 = \\beta_2 = \\beta_3 = 2$。这就是问题中定义的完全融合点。因此，对于情况 II，$\\lambda_{II}^{\\star} = 2$。\n\n再次，为完整起见，我们验证 $\\lambda \\ge 2$ 时的完全融合解 $\\beta_1 = \\beta_2 = \\beta_3 = \\beta_c$。将三个 KKT 方程相加得到 $\\beta_1 + \\beta_2 + \\beta_3 - (y_1+y_2+y_3) = 0$，所以 $3\\beta_c - (0+3+3) = 0$，这给出 $\\beta_c = 2$。\n将 $\\beta_c=2$ 代入 KKT 系统：\n1. $2 - 0 - \\lambda g_1 = 0 \\implies g_1 = 2/\\lambda$\n3. $2 - 3 + \\lambda g_2 = 0 \\implies g_2 = 1/\\lambda$\n只要 $|g_1| \\le 1$ 和 $|g_2| \\le 1$，这些就是有效的次梯度。\n$|2/\\lambda| \\le 1 \\implies \\lambda \\ge 2$。\n$|1/\\lambda| \\le 1 \\implies \\lambda \\ge 1$。\n两个条件都必须满足，所以我们必须有 $\\lambda \\ge 2$。这证实了对于 $\\lambda \\ge 2$，会发生完全融合。\n\n### 比较与结论\n我们已经从第一性原理推导出了临界正则化强度：\n-   情况 I: $\\lambda_I^{\\star} = 1.5$\n-   情况 II: $\\lambda_{II}^{\\star} = 2$\n\n我们观察到 $\\lambda_{II}^{\\star} > \\lambda_I^{\\star}$。这意味着当右侧的数据点被复制时，需要更强的正则化惩罚才能实现跨边界的融合。复制索引使得最左边的系数更难与其右边的系数融合。\n\n现在，我们评估给出的选项：\n\n**A. 复制索引会增加 $\\lambda^{\\star}$，因此复制会削弱跨该边界的局部融合（融合变得更难）。**\n我们的分析表明 $\\lambda^{\\star}$ 从 1.5 增加到 2。更高的 $\\lambda^{\\star}$ 表示需要更强的惩罚才能引起融合，这意味着融合“更弱”或“更难实现”。这个陈述与我们的发现一致。\n**结论：正确。**\n\n**B. 复制索引会减小 $\\lambda^{\\star}$，因此复制会加强跨该边界的局部融合（融合变得更容易）。**\n我们的分析表明 $\\lambda^{\\star}$ 增加，而不是减少。\n**结论：错误。**\n\n**C. 如果复制的数据点具有相同的观测值，则复制索引对 $\\lambda^{\\star}$ 没有影响，因此跨边界的融合趋势不变。**\n我们的分析表明 $\\lambda^{\\star}$ 从 1.5 变为 2。\n**结论：错误。**\n\n**D. 对于任何 $\\lambda > 0$，复制索引会强制立即跨边界融合，因为额外的总变差惩罚项施加了更强的相等性约束。**\n我们的分析表明，跨边界的融合（即 $\\beta_1 = \\beta_2$）仅在 $\\lambda = 2$ 时发生。对于任何 $\\lambda > 0$ 发生的是复制点之间的融合（$\\beta_2 = \\beta_3$），而不是我们关心的跨边界融合。\n**结论：错误。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "手动计算虽然富有启发性，但对于现实世界的问题并不可行，我们需要一种高效的算法来求解。这个练习将引导你从理论走向实践，通过推导并编写一个基于交替方向乘子法（ADMM）的融合套索求解器。 你将学习到为何像坐标下降这样的朴素方法效率低下，以及变量分裂技术如何将复杂问题分解为可管理的部分，从而巩固你对现代凸优化技术的理解。",
            "id": "3122231",
            "problem": "给定一个在统计学习中出现的凸优化任务，称为融合最小绝对收缩和选择算子（fused lasso）。融合套索通过平衡数据保真度、元素级稀疏性和分段恒定性来估计参数向量 $\\beta$。设 $\\mathbf{y} \\in \\mathbb{R}^{n}$ 为响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 为设计矩阵，并令 $D \\in \\mathbb{R}^{(p-1) \\times p}$ 为一阶差分算子，其定义为 $(D \\beta)_{i} = \\beta_{i+1} - \\beta_{i}$，对于 $i \\in \\{1,\\dots,p-1\\}$。考虑以下目标函数\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2}\\|\\mathbf{y} - X \\beta\\|_{2}^{2} \\;+\\; \\lambda_{\\mathrm{b}} \\|\\beta\\|_{1} \\;+\\; \\lambda_{\\mathrm{f}} \\|D \\beta\\|_{1},\n$$\n其中 $\\lambda_{\\mathrm{b}} \\ge 0$ 控制元素级稀疏性，$\\lambda_{\\mathrm{f}} \\ge 0$ 通过一阶差分控制相邻坐标的融合。\n\n任务：\n- 从凸优化和近端算子的一阶原理出发，推导一个块坐标下降方案，该方案引入增广变量来解耦非光滑项。你的推导必须从凸性、次梯度和元素级绝对值函数的近端算子的基本定义开始，并且必须证明每一步的合理性。具体来说，引入一个辅助变量 $z \\in \\mathbb{R}^{p-1}$ 来表示 $z = D \\beta$ 和第二个辅助变量 $w \\in \\mathbb{R}^{p}$ 来表示 $w = \\beta$，并推导关于 $\\beta$、$w$ 和 $z$ 的交替最小化更新步骤，以及相应的缩放对偶变量。不要通过引用现成的结果来跳过推导步骤；推导更新是什么，为什么它们是有效的，以及它们是如何从一个增广公式中产生的。\n- 解释为什么当 $\\lambda_{\\mathrm{f}} > 0$ 时，直接对 $\\beta$ 进行朴素坐标下降对融合套索是无效的，重点关注由 $D$ 在非光滑项中引起的耦合及其对坐标可分离性和收敛性的影响。\n\n实现要求：\n- 将推导出的方案实现为一个在指定环境中完整且可运行的程序，用于为提供的测试套件求解融合套索问题。你的实现必须为每个测试显式构造 $D$，对 $\\beta$、$w$ 和 $z$ 以及相应的缩放对偶变量执行块坐标更新，并通过精确求解由二次项产生的线性系统来解决 $\\beta$-子问题。你的实现必须接受通用的 $X$，但在下面的测试套件中，你将把 $X$ 设置为适当维度的单位矩阵 $I$。\n- 对于数值停止条件，使用交替方向框架中的原始和对偶残差范数，并在两者均低于一个容差时终止。在最终输出中将每个估计系数四舍五入到 $4$ 位小数。\n\n测试套件：\n- 情况 1（通用“理想路径”）：$n = 6$, $p = 6$, $X = I$, $\\mathbf{y} = (1.0, 2.0, 2.5, 2.0, 1.5, 1.0)$, $\\lambda_{\\mathrm{b}} = 0.1$, $\\lambda_{\\mathrm{f}} = 0.5$, 惩罚参数 $\\rho_{1} = 1.0$, $\\rho_{2} = 1.0$, 最大迭代次数 $1000$, 容差 $10^{-6}$。\n- 情况 2（边界情况，无融合）：$n = 4$, $p = 4$, $X = I$, $\\mathbf{y} = (-1.0, 0.0, 3.0, -2.0)$, $\\lambda_{\\mathrm{b}} = 0.5$, $\\lambda_{\\mathrm{f}} = 0.0$, 惩罚参数 $\\rho_{1} = 1.0$, $\\rho_{2} = 1.0$, 最大迭代次数 $1000$, 容差 $10^{-6}$。\n- 情况 3（边界情况，无元素级稀疏性）：$n = 6$, $p = 6$, $X = I$, $\\mathbf{y} = (0.0, 0.1, -0.1, 3.0, 3.2, 2.9)$, $\\lambda_{\\mathrm{b}} = 0.0$, $\\lambda_{\\mathrm{f}} = 1.0$, 惩罚参数 $\\rho_{1} = 1.0$, $\\rho_{2} = 1.0$, 最大迭代次数 $1000$, 容差 $10^{-6}$。\n- 情况 4（边缘情况，常数信号）：$n = 5$, $p = 5$, $X = I$, $\\mathbf{y} = (1.0, 1.0, 1.0, 1.0, 1.0)$, $\\lambda_{\\mathrm{b}} = 0.0$, $\\lambda_{\\mathrm{f}} = 2.0$, 惩罚参数 $\\rho_{1} = 1.0$, $\\rho_{2} = 1.0$, 最大迭代次数 $1000$, 容差 $10^{-6}$。\n\n输出规范：\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表的结果，其中每个案例的估计 $\\hat{\\beta}$ 表示为一个用方括号括起来且没有空格的逗号分隔列表，并且每个系数四舍五入到 $4$ 位小数。例如，最终输出格式必须是 $[[b_{1,1},\\dots,b_{1,p}], [b_{2,1},\\dots,b_{2,p}], [b_{3,1},\\dots,b_{3,p}], [b_{4,1},\\dots,b_{4,p}]]$ 的形式，但字符串中任何地方都不能有空格。",
            "solution": "该问题是有效的。它描述了一个定义明确的凸优化问题，即融合套索（fused lasso），这是统计学习和信号处理中的一种标准技术。该问题具有科学依据，是客观的，并且为理论推导和数值实现都提供了完整的规范。\n\n### 朴素坐标下降的无效性\n\n首先，我们讨论为什么当融合惩罚参数 $\\lambda_{\\mathrm{f}} > 0$ 时，朴素的坐标级最小化不适用于融合套索目标函数。目标函数为：\n$$\nf(\\beta) = \\frac{1}{2}\\|\\mathbf{y} - X \\beta\\|_{2}^{2} \\;+\\; \\lambda_{\\mathrm{b}} \\|\\beta\\|_{1} \\;+\\; \\lambda_{\\mathrm{f}} \\|D \\beta\\|_{1}\n$$\n坐标下降是一种优化策略，它一次只沿一个坐标方向迭代地最小化目标函数，同时保持所有其他坐标固定。为了更新第 $j$ 个坐标 $\\beta_j$，需要求解一维优化问题：\n$$\n\\min_{\\beta_j \\in \\mathbb{R}} f(\\beta_1, \\dots, \\beta_{j-1}, \\beta_j, \\beta_{j+1}, \\dots, \\beta_p)\n$$\n让我们分析 $f(\\beta)$ 中依赖于 $\\beta_j$ 的项：\n1.  数据保真项 $\\frac{1}{2}\\|\\mathbf{y} - X \\beta\\|_{2}^{2}$ 是关于 $\\beta_j$ 的二次函数。\n2.  元素级稀疏项 $\\lambda_{\\mathrm{b}} \\|\\beta\\|_{1} = \\lambda_{\\mathrm{b}} \\sum_{i=1}^{p} |\\beta_i|$ 贡献了 $\\lambda_{\\mathrm{b}} |\\beta_j|$。该项是可分离的，意味着涉及 $\\beta_j$ 的部分独立于所有其他 $\\beta_i$ ($i \\ne j$)。\n3.  融合项 $\\lambda_{\\mathrm{f}} \\|D \\beta\\|_{1} = \\lambda_{\\mathrm{f}} \\sum_{i=1}^{p-1} |\\beta_{i+1} - \\beta_i|$ 是困难的根源。对于一个内部坐标 $j$ ($1 < j < p$)，$\\beta_j$ 出现在两个绝对值项中：$|\\beta_j - \\beta_{j-1}|$ 和 $|\\beta_{j+1} - \\beta_j|$。\n\n因此，更新 $\\beta_j$ 的子问题形式为：\n$$\n\\min_{\\beta_j} \\left( \\frac{1}{2}a_j \\beta_j^2 - b_j \\beta_j + \\lambda_{\\mathrm{b}}|\\beta_j| + \\lambda_{\\mathrm{f}}|\\beta_j - \\beta_{j-1}| + \\lambda_{\\mathrm{f}}|\\beta_{j+1} - \\beta_j| \\right)\n$$\n其中 $a_j$ 和 $b_j$ 来自二次数据保真项。这是一个一维广义套索问题。虽然可解，但它不像标准套索中使用的软阈值算子那样具有简单的闭式解。$\\beta_j$ 与其邻居 $\\beta_{j-1}$ 和 $\\beta_{j+1}$ 在不可微的 $\\ell_1$ 范数内的耦合，阻止了问题在坐标上是可分离的。与能够更优雅地处理这种结构化不可分离性的方法相比，这种复杂性使得朴素坐标下降效率低下且难以实现。\n\n### 通过变量分裂推导块坐标下降\n\n为了克服不可分离性，我们引入辅助变量来解耦目标函数中的各项。该策略允许我们使用交替最小化方案，特别是交替方向乘子法（ADMM）。\n\n**1. 问题重构**\n我们引入两个辅助变量，$w \\in \\mathbb{R}^{p}$ 和 $z \\in \\mathbb{R}^{p-1}$，并用约束条件重构问题：\n$$\n\\min_{\\beta, w, z} \\;\\; \\frac{1}{2}\\|\\mathbf{y} - X \\beta\\|_{2}^{2} \\;+\\; \\lambda_{\\mathrm{b}} \\|w\\|_{1} \\;+\\; \\lambda_{\\mathrm{f}} \\|z\\|_{1}\n$$\n约束条件为 $w = \\beta$ 和 $z = D \\beta$。\n\n**2. 增广拉格朗日量**\n我们为这个约束问题构造增广拉格朗日量。设 $u \\in \\mathbb{R}^{p}$ 和 $v \\in \\mathbb{R}^{p-1}$ 分别是与约束 $w = \\beta$ 和 $z = D\\beta$ 相关联的拉格朗日对偶变量。增广拉格朗日量为：\n$$\nL_{\\rho_1, \\rho_2}(\\beta, w, z, u, v) = f(\\beta, w, z) + u^T(\\beta - w) + v^T(D\\beta - z) + \\frac{\\rho_1}{2}\\|\\beta - w\\|_2^2 + \\frac{\\rho_2}{2}\\|D\\beta - z\\|_2^2\n$$\n其中 $f(\\beta, w, z) = \\frac{1}{2}\\|\\mathbf{y} - X \\beta\\|_{2}^{2} + \\lambda_{\\mathrm{b}} \\|w\\|_{1} + \\lambda_{\\mathrm{f}} \\|z\\|_{1}$，$\\rho_1, \\rho_2 > 0$ 是惩罚参数。\n\n为方便起见，我们使用缩放对偶形式。通过配方法，我们可以重写对偶项和惩罚项。设缩放对偶变量为 $u_s = u/\\rho_1$ 和 $v_s = v/\\rho_2$。\n$$\nu^T(\\beta - w) + \\frac{\\rho_1}{2}\\|\\beta - w\\|_2^2 = \\frac{\\rho_1}{2}\\|\\beta - w + u/\\rho_1\\|_2^2 - \\frac{\\rho_1}{2}\\|u/\\rho_1\\|_2^2 = \\frac{\\rho_1}{2}\\|\\beta - w + u_s\\|_2^2 - \\frac{\\rho_1}{2}\\|u_s\\|_2^2\n$$\n类似的变换适用于涉及 $z, v, \\rho_2$ 的项。忽略相对于原始变量（$\\beta, w, z$）为常数的项，缩放形式的增广拉格朗日量的最小化部分为：\n$$\nL(\\beta, w, z, u_s, v_s) = \\frac{1}{2}\\|\\mathbf{y} - X \\beta\\|_{2}^{2} + \\lambda_{\\mathrm{b}}\\|w\\|_{1} + \\lambda_{\\mathrm{f}}\\|z\\|_{1} + \\frac{\\rho_1}{2}\\|\\beta - w + u_s\\|_2^2 + \\frac{\\rho_2}{2}\\|D\\beta - z + v_s\\|_2^2\n$$\n\n**3. ADMM 更新步骤**\nADMM 通过迭代地最小化 $L$ 关于每个原始变量块（$\\beta$, $w$, $z$）来进行，然后更新对偶变量。设 $k$ 为迭代索引。\n\n**a) $\\beta$-更新：** 我们最小化 $L$ 关于 $\\beta$，同时将其他变量固定在第 $k$ 次迭代的值。\n$$\n\\beta^{k+1} = \\arg\\min_{\\beta} \\left( \\frac{1}{2}\\|\\mathbf{y} - X \\beta\\|_{2}^{2} + \\frac{\\rho_1}{2}\\|\\beta - w^k + u_s^k\\|_2^2 + \\frac{\\rho_2}{2}\\|D\\beta - z^k + v_s^k\\|_2^2 \\right)\n$$\n这是关于 $\\beta$ 的严格凸二次函数的最小化。我们通过将关于 $\\beta$ 的梯度设为零来找到最小化点：\n$$\n\\nabla_{\\beta} L = X^T(X\\beta - \\mathbf{y}) + \\rho_1(\\beta - w^k + u_s^k) + \\rho_2 D^T(D\\beta - z^k + v_s^k) = 0\n$$\n重新整理各项以求解 $\\beta$：\n$$\n(X^T X + \\rho_1 I + \\rho_2 D^T D) \\beta = X^T \\mathbf{y} + \\rho_1(w^k - u_s^k) + \\rho_2 D^T(z^k - v_s^k)\n$$\n这是一个 $A\\beta = b$ 形式的线性系统，其中矩阵 $A = X^T X + \\rho_1 I + \\rho_2 D^T D$ 是对称正定的，因此是可逆的。对于测试用例，$X$ 是单位矩阵 $I$，因此 $A = (1+\\rho_1)I + \\rho_2 D^T D$。\n\n**b) $w$-更新：** 我们最小化 $L$ 关于 $w$，将 $\\beta$ 保持在其新值 $\\beta^{k+1}$，其他变量保持在第 $k$ 次迭代的值。\n$$\nw^{k+1} = \\arg\\min_{w} \\left( \\lambda_{\\mathrm{b}}\\|w\\|_{1} + \\frac{\\rho_1}{2}\\|\\beta^{k+1} - w + u_s^k\\|_2^2 \\right)\n$$\n这是 $\\ell_1$-范数的近端算子的定义。该问题在 $w$ 的各个分量上是可分离的。对于每个分量 $w_i$，我们求解 $\\min_{w_i} \\lambda_{\\mathrm{b}}|w_i| + \\frac{\\rho_1}{2}(w_i - a_i)^2$，其中 $a = \\beta^{k+1} + u_s^k$。解由软阈值算子 $S_{\\kappa}(a)$ 给出：\n$$\nw^{k+1} = S_{\\lambda_{\\mathrm{b}}/\\rho_1}(\\beta^{k+1} + u_s^k)\n$$\n软阈值函数 $S_{\\kappa}(a_i) = \\text{sign}(a_i) \\max(|a_i| - \\kappa, 0)$ 是从一阶最优性条件（次梯度等于零）推导出来的。$g(w_i) = \\lambda_{\\mathrm{b}}|w_i| + \\frac{\\rho_1}{2}(w_i - a_i)^2$ 的次梯度 $\\partial g(w_i)$ 是 $\\lambda_{\\mathrm{b}}\\partial|w_i| + \\rho_1(w_i - a_i)$。令其包含 $0$ 即可得到分段解。\n\n**c) $z$-更新：** $z$ 的更新在结构上与 $w$ 的更新相同。\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\lambda_{\\mathrm{f}}\\|z\\|_{1} + \\frac{\\rho_2}{2}\\|D\\beta^{k+1} - z + v_s^k\\|_2^2 \\right)\n$$\n解是软阈值算子的另一次应用：\n$$\nz^{k+1} = S_{\\lambda_{\\mathrm{f}}/\\rho_2}(D\\beta^{k+1} + v_s^k)\n$$\n\n**d) 对偶变量更新：** 最后，我们使用梯度上升步骤更新缩放的对偶变量。\n$$\nu_s^{k+1} = u_s^k + \\beta^{k+1} - w^{k+1}\n$$\n$$\nv_s^{k+1} = v_s^k + D\\beta^{k+1} - z^{k+1}\n$$\n这些更新是约束 $\\beta-w=0$ 和 $D\\beta-z=0$ 的原始残差。\n\n**4. 停止准则**\n当原始和对偶残差均低于指定的容差 $\\epsilon > 0$ 时，算法终止。\n-   **原始残差**：这些衡量了对原始可行性约束 $w=\\beta$ 和 $z=D\\beta$ 的违反程度。\n    $$ r_w^{k+1} = \\beta^{k+1} - w^{k+1}, \\quad r_z^{k+1} = D\\beta^{k+1} - z^{k+1} $$\n    原始残差范数为 $\\sqrt{\\|r_w^{k+1}\\|_2^2 + \\|r_z^{k+1}\\|_2^2}$。\n-   **对偶残差**：这衡量了对对偶可行性/平稳性条件的违反程度。从 $\\beta$ 的平稳性条件推导，对偶残差为：\n    $$ s^{k+1} = -\\rho_1(w^{k+1}-w^k) - \\rho_2 D^T(z^{k+1}-z^k) $$\n    当 $\\|(r_w^{k+1}, r_z^{k+1})\\|_2 < \\epsilon$ 和 $\\|s^{k+1}\\|_2 < \\epsilon$ 都成立时，算法停止。\n\n这种 ADMM 方案有效地将复杂的融合套索问题分解为一系列更简单的子问题：一个线性系统求解和两次软阈值算子的应用。\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the fused lasso problem.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"y\": np.array([1.0, 2.0, 2.5, 2.0, 1.5, 1.0]),\n            \"lambda_b\": 0.1,\n            \"lambda_f\": 0.5,\n            \"rho1\": 1.0,\n            \"rho2\": 1.0,\n            \"max_iter\": 1000,\n            \"tol\": 1e-6,\n        },\n        {\n            \"y\": np.array([-1.0, 0.0, 3.0, -2.0]),\n            \"lambda_b\": 0.5,\n            \"lambda_f\": 0.0,\n            \"rho1\": 1.0,\n            \"rho2\": 1.0,\n            \"max_iter\": 1000,\n            \"tol\": 1e-6,\n        },\n        {\n            \"y\": np.array([0.0, 0.1, -0.1, 3.0, 3.2, 2.9]),\n            \"lambda_b\": 0.0,\n            \"lambda_f\": 1.0,\n            \"rho1\": 1.0,\n            \"rho2\": 1.0,\n            \"max_iter\": 1000,\n            \"tol\": 1e-6,\n        },\n        {\n            \"y\": np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"lambda_b\": 0.0,\n            \"lambda_f\": 2.0,\n            \"rho1\": 1.0,\n            \"rho2\": 1.0,\n            \"max_iter\": 1000,\n            \"tol\": 1e-6,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        p = len(case[\"y\"])\n        n = p\n        # In the test suite, X is the identity matrix of appropriate dimension.\n        X = np.identity(n)\n        \n        beta_hat = fused_lasso_admm(\n            y=case[\"y\"],\n            X=X,\n            lambda_b=case[\"lambda_b\"],\n            lambda_f=case[\"lambda_f\"],\n            rho1=case[\"rho1\"],\n            rho2=case[\"rho2\"],\n            max_iter=case[\"max_iter\"],\n            tol=case[\"tol\"],\n        )\n        \n        # Format the result for the specific case\n        rounded_beta = [f\"{b:.4f}\" for b in beta_hat]\n        results.append(f\"[{','.join(rounded_beta)}]\")\n\n    # Print the final output string, which will be the answer\n    return f\"[{','.join(results)}]\"\n\n\ndef soft_threshold(x, kappa):\n    \"\"\"\n    Soft-thresholding operator.\n    \"\"\"\n    return np.sign(x) * np.maximum(np.abs(x) - kappa, 0)\n\n\ndef fused_lasso_admm(y, X, lambda_b, lambda_f, rho1, rho2, max_iter, tol):\n    \"\"\"\n    Solves the fused lasso problem using ADMM.\n    min 1/2 ||y - X*beta||_2^2 + lambda_b * ||w||_1 + lambda_f * ||z||_1\n    s.t. beta = w, D*beta = z\n    \"\"\"\n    n, p = X.shape\n    \n    # Construct the first-order difference operator D\n    D = np.zeros((p - 1, p))\n    if p > 1:\n        for i in range(p - 1):\n            D[i, i] = -1\n            D[i, i + 1] = 1\n\n    # Initialize variables\n    beta = np.zeros(p)\n    w = np.zeros(p)\n    z = np.zeros(p - 1)\n    u_s = np.zeros(p) # scaled dual for beta - w = 0\n    v_s = np.zeros(p - 1) # scaled dual for D*beta - z = 0\n\n    # Pre-compute matrices for the beta-update\n    XtX = X.T @ X\n    DtD = D.T @ D\n    \n    # The matrix for the linear system in the beta update\n    # In this problem setting X=I, so XtX = I\n    A = XtX + rho1 * np.identity(p) + rho2 * DtD\n    A_inv = np.linalg.inv(A)\n\n    Xty = X.T @ y\n\n    for k in range(max_iter):\n        # Store old values for residual calculation\n        w_old = w.copy()\n        z_old = z.copy()\n\n        # 1. beta-update: Solve the linear system A*beta = b\n        b = Xty + rho1 * (w - u_s) + rho2 * D.T @ (z - v_s)\n        beta = A_inv @ b\n\n        # 2. w-update: Soft-thresholding\n        w = soft_threshold(beta + u_s, lambda_b / rho1)\n\n        # 3. z-update: Soft-thresholding\n        z = soft_threshold(D @ beta + v_s, lambda_f / rho2)\n\n        # 4. Dual variable updates (scaled form)\n        r_w = beta - w\n        r_z = (D @ beta) - z\n        u_s += r_w\n        v_s += r_z\n\n        # Stopping criteria\n        # Primal residual norm\n        primal_res_norm = np.linalg.norm(np.concatenate([r_w, r_z]))\n        \n        # Dual residual norm\n        s_w = -rho1 * (w - w_old)\n        s_z = -rho2 * (z - z_old)\n        dual_res_vec = s_w + D.T @ s_z\n        dual_res_norm = np.linalg.norm(dual_res_vec)\n\n        # Using standard tolerance checks from Boyd's ADMM book\n        eps_pri = np.sqrt(p + p - 1) * tol + tol * max(np.linalg.norm(np.concatenate([beta, D@beta])), np.linalg.norm(np.concatenate([w,z])))\n        eps_dual = np.sqrt(p) * tol + tol * np.linalg.norm(np.concatenate([rho1*u_s, rho2*D.T@v_s]))\n        \n        if primal_res_norm  eps_pri and dual_res_norm  eps_dual:\n            break\n            \n    return beta\n```",
            "answer": "```\n[[1.0714,2.0714,2.0714,2.0714,1.4048,0.9048],[-0.5000,0.0000,2.5000,-1.5000],[0.0000,0.0000,0.0000,3.0333,3.0333,3.0333],[1.0000,1.0000,1.0000,1.0000,1.0000]]\n```"
        }
    ]
}