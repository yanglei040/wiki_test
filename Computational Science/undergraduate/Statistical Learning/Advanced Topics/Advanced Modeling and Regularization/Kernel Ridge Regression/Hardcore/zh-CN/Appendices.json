{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的检验方式就是亲自动手计算。本练习将引导你从零开始，为一个简单的一维数据集构建核岭回归（KRR）预测器 。通过这个过程，你将加深对表示定理（Representer Theorem）的理解，并掌握如何通过求解线性系统来确定对偶系数。此外，本练习还进一步要求你计算预测函数对输入的导数，这不仅揭示了KRR模型产生平滑函数的特性，也为在更高级的应用（如优化问题）中使用模型梯度奠定了基础。",
            "id": "3136849",
            "problem": "考虑在一维输入上进行核岭回归，训练数据为 $\\{(x_1, y_1), (x_2, y_2)\\}$，其中 $x_1 = 0$，$x_2 = 1$，$y_1 = 1$，$y_2 = -1$。设假设空间是与高斯核 $k(x, x') = \\exp\\!\\big(-(x - x')^{2}\\big)$ 和正则化参数 $\\lambda = \\frac{1}{2}$ 相关联的再生核希尔伯特空间 (RKHS)。核岭回归估计量定义为在 RKHS 中所有函数 $f$ 中最小化以下目标函数的解\n$$\n\\sum_{i=1}^{2} \\big(y_i - f(x_i)\\big)^{2} + \\lambda \\|f\\|_{\\mathcal{H}}^{2}.\n$$\n利用 RKHS 的表示定理 (Representer Theorem) 的基本陈述和一阶最优性条件，推导估计量的函数形式以及用核矩阵表示的系数所对应的线性系统。然后，通过对核函数应用标准微积分，推导核岭回归预测器 $f(x)$ 关于输入 $x$ 的导数 $\\frac{d}{dx} f(x)$。最后，对于给定的数据和参数，在点 $x^{\\star} = \\frac{1}{2}$ 处计算该导数的值。将你的最终答案表示为一个仅包含有理数和指数的单一精确闭式解析表达式。不要进行数值近似。",
            "solution": "首先验证问题以确保其科学上成立、适定且客观。\n\n### 步骤1：提取已知条件\n- 训练数据点：$(x_1, y_1) = (0, 1)$ 和 $(x_2, y_2) = (1, -1)$。\n- 输入向量：$\\mathbf{x} \\in \\mathbb{R}^1$。\n- 核函数：$k(x, x') = \\exp(-(x - x')^2)$。\n- 正则化参数：$\\lambda = \\frac{1}{2}$。\n- 要最小化的目标函数：$J(f) = \\sum_{i=1}^{2} (y_i - f(x_i))^2 + \\lambda \\|f\\|_{\\mathcal{H}}^2$。\n- 任务：推导估计量 $f(x)$ 的函数形式、系数的线性系统、导数 $\\frac{d}{dx}f(x)$，并在 $x^{\\star} = \\frac{1}{2}$ 处计算该导数。\n\n### 步骤2：使用提取的已知条件进行验证\n- **科学基础：** 该问题基于核岭回归（KRR），这是机器学习和统计学中一种标准且基础的技术。使用高斯核和再生核希尔伯特空间（RKHS）框架是公认的。\n- **适定性：** 目标函数是平方误差项和范数平方惩罚项之和。对于 $\\lambda > 0$，此函数是严格凸的，因为对于不同点的核矩阵 $K$ 是正定的，使得 $K + \\lambda I$ 也是正定且可逆的。这保证了唯一、稳定且有意义的解的存在。\n- **客观性：** 该问题以精确的数学语言陈述，没有任何主观或模糊的术语。\n\n### 步骤3：结论与行动\n问题是有效的，因为它是自洽的、科学上合理的并且是适定的。将提供完整的解答。\n\n### 求解推导\n核岭回归（KRR）估计量 $f(x)$ 是在再生核希尔伯特空间（RKHS）$\\mathcal{H}$ 中最小化正则化最小二乘目标函数的函数：\n$$\nJ(f) = \\sum_{i=1}^{2} (y_i - f(x_i))^2 + \\lambda \\|f\\|_{\\mathcal{H}}^2\n$$\n根据表示定理 (Representer Theorem)，最小化子 $f$ 可以表示为以训练数据点为中心的核函数的线性组合：\n$$\nf(x) = \\sum_{i=1}^{2} \\alpha_i k(x_i, x)\n$$\n其中 $\\alpha_i$ 是待定系数。\n\n$f$ 在 RKHS 中的范数由下式给出：\n$$\n\\|f\\|_{\\mathcal{H}}^2 = \\left\\langle \\sum_{i=1}^{2} \\alpha_i k(x_i, \\cdot), \\sum_{j=1}^{2} \\alpha_j k(x_j, \\cdot) \\right\\rangle_{\\mathcal{H}} = \\sum_{i=1}^{2} \\sum_{j=1}^{2} \\alpha_i \\alpha_j k(x_i, x_j) = \\boldsymbol{\\alpha}^T K \\boldsymbol{\\alpha}\n$$\n其中 $K$ 是 $2 \\times 2$ 的格拉姆矩阵（或核矩阵），其元素为 $K_{ij} = k(x_i, x_j)$，且 $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2)^T$。\n\n在训练点上的预测值为 $f(x_i) = \\sum_{j=1}^{2} \\alpha_j k(x_j, x_i) = (K\\boldsymbol{\\alpha})_i$。令 $\\mathbf{y} = (y_1, y_2)^T$。目标函数可以重写为矩阵形式：\n$$\nJ(\\boldsymbol{\\alpha}) = (\\mathbf{y} - K\\boldsymbol{\\alpha})^T(\\mathbf{y} - K\\boldsymbol{\\alpha}) + \\lambda \\boldsymbol{\\alpha}^T K \\boldsymbol{\\alpha}\n$$\n为了找到最小化 $J(\\boldsymbol{\\alpha})$ 的最优系数 $\\boldsymbol{\\alpha}$，我们计算 $J$ 相对于 $\\boldsymbol{\\alpha}$ 的梯度并令其为零。\n$$\n\\nabla_{\\boldsymbol{\\alpha}} J(\\boldsymbol{\\alpha}) = -2 K^T (\\mathbf{y} - K\\boldsymbol{\\alpha}) + 2 \\lambda K \\boldsymbol{\\alpha}\n$$\n由于核矩阵 $K$ 是对称的（$K^T=K$），这可以简化为：\n$$\n\\nabla_{\\boldsymbol{\\alpha}} J(\\boldsymbol{\\alpha}) = -2 K (\\mathbf{y} - K\\boldsymbol{\\alpha}) + 2 \\lambda K \\boldsymbol{\\alpha} = -2 K \\mathbf{y} + 2 K^2 \\boldsymbol{\\alpha} + 2 \\lambda K \\boldsymbol{\\alpha}\n$$\n令梯度为零：\n$$\n-2 K \\mathbf{y} + 2 K^2 \\boldsymbol{\\alpha} + 2 \\lambda K \\boldsymbol{\\alpha} = \\mathbf{0} \\implies K( (K + \\lambda I)\\boldsymbol{\\alpha} - \\mathbf{y} ) = \\mathbf{0}\n$$\n由于高斯核对不同的点生成正定核矩阵，因此 $K$ 是可逆的。因此，我们可以左乘 $K^{-1}$ 以获得系数的线性系统：\n$$\n(K + \\lambda I)\\boldsymbol{\\alpha} = \\mathbf{y}\n$$\n现在，我们代入给定的数据。训练点为 $x_1 = 0$ 和 $x_2 = 1$。核函数为 $k(x, x') = \\exp(-(x-x')^2)$。核矩阵 $K$ 为：\n$$\nK = \\begin{pmatrix} k(x_1, x_1) & k(x_1, x_2) \\\\ k(x_2, x_1) & k(x_2, x_2) \\end{pmatrix} = \\begin{pmatrix} k(0, 0) & k(0, 1) \\\\ k(1, 0) & k(1, 1) \\end{pmatrix}\n$$\n$K_{11} = k(0, 0) = \\exp(-(0-0)^2) = \\exp(0) = 1$。\n$K_{12} = k(0, 1) = \\exp(-(0-1)^2) = \\exp(-1)$。\n$K_{21} = k(1, 0) = \\exp(-(1-0)^2) = \\exp(-1)$。\n$K_{22} = k(1, 1) = \\exp(-(1-1)^2) = \\exp(0) = 1$。\n所以，$K = \\begin{pmatrix} 1 & \\exp(-1) \\\\ \\exp(-1) & 1 \\end{pmatrix}$。\n\n响应向量为 $\\mathbf{y} = (1, -1)^T$，正则化参数为 $\\lambda = \\frac{1}{2}$。线性系统变为：\n$$\n\\left( \\begin{pmatrix} 1 & \\exp(-1) \\\\ \\exp(-1) & 1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} \\frac{3}{2} & \\exp(-1) \\\\ \\exp(-1) & \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n为了求解 $\\boldsymbol{\\alpha}$，我们对矩阵 $A = K + \\lambda I$ 求逆：\n$$\n\\det(A) = \\left(\\frac{3}{2}\\right)^2 - (\\exp(-1))^2 = \\frac{9}{4} - \\exp(-2)\n$$\n$$\nA^{-1} = \\frac{1}{\\frac{9}{4} - \\exp(-2)} \\begin{pmatrix} \\frac{3}{2} & -\\exp(-1) \\\\ -\\exp(-1) & \\frac{3}{2} \\end{pmatrix}\n$$\n现在，我们求解 $\\boldsymbol{\\alpha} = A^{-1}\\mathbf{y}$：\n$$\n\\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\end{pmatrix} = \\frac{1}{\\frac{9}{4} - \\exp(-2)} \\begin{pmatrix} \\frac{3}{2} & -\\exp(-1) \\\\ -\\exp(-1) & \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\frac{9}{4} - \\exp(-2)} \\begin{pmatrix} \\frac{3}{2} + \\exp(-1) \\\\ -\\exp(-1) - \\frac{3}{2} \\end{pmatrix}\n$$\n我们可以简化 $\\alpha_1$：\n$$\n\\alpha_1 = \\frac{\\frac{3}{2} + \\exp(-1)}{\\frac{9}{4} - \\exp(-2)} = \\frac{\\frac{3}{2} + \\exp(-1)}{(\\frac{3}{2} - \\exp(-1))(\\frac{3}{2} + \\exp(-1))} = \\frac{1}{\\frac{3}{2} - \\exp(-1)} = \\frac{1}{\\frac{3\\exp(1) - 2}{2\\exp(1)}} = \\frac{2\\exp(1)}{3\\exp(1) - 2}\n$$\n而 $\\alpha_2 = -\\alpha_1 = -\\frac{2\\exp(1)}{3\\exp(1) - 2}$。\n\nKRR 预测器为 $f(x) = \\alpha_1 k(0, x) + \\alpha_2 k(1, x) = \\alpha_1 \\exp(-x^2) + \\alpha_2 \\exp(-(1-x)^2)$。\n我们需要求它关于 $x$ 的导数：\n$$\n\\frac{d}{dx}f(x) = \\alpha_1 \\frac{d}{dx}\\exp(-x^2) + \\alpha_2 \\frac{d}{dx}\\exp(-(1-x)^2)\n$$\n使用链式法则：\n$$\n\\frac{d}{dx}f(x) = \\alpha_1 (-2x \\exp(-x^2)) + \\alpha_2 (-2(1-x)(-1) \\exp(-(1-x)^2))\n$$\n$$\n\\frac{d}{dx}f(x) = -2x \\alpha_1 \\exp(-x^2) + 2(1-x) \\alpha_2 \\exp(-(1-x)^2)\n$$\n最后，我们在点 $x^{\\star} = \\frac{1}{2}$ 处计算该导数的值：\n$$\n\\frac{d}{dx}f(x)\\bigg|_{x=1/2} = -2\\left(\\frac{1}{2}\\right)\\alpha_1 \\exp\\left(-\\left(\\frac{1}{2}\\right)^2\\right) + 2\\left(1 - \\frac{1}{2}\\right)\\alpha_2 \\exp\\left(-\\left(1 - \\frac{1}{2}\\right)^2\\right)\n$$\n$$\n= -\\alpha_1 \\exp\\left(-\\frac{1}{4}\\right) + 2\\left(\\frac{1}{2}\\right)\\alpha_2 \\exp\\left(-\\left(\\frac{1}{2}\\right)^2\\right)\n$$\n$$\n= -\\alpha_1 \\exp\\left(-\\frac{1}{4}\\right) + \\alpha_2 \\exp\\left(-\\frac{1}{4}\\right) = (\\alpha_2 - \\alpha_1)\\exp\\left(-\\frac{1}{4}\\right)\n$$\n代入 $\\alpha_2 = -\\alpha_1$：\n$$\n(-\\alpha_1 - \\alpha_1)\\exp\\left(-\\frac{1}{4}\\right) = -2\\alpha_1 \\exp\\left(-\\frac{1}{4}\\right)\n$$\n现在，代入 $\\alpha_1$ 的值：\n$$\n-2 \\left( \\frac{2\\exp(1)}{3\\exp(1) - 2} \\right) \\exp\\left(-\\frac{1}{4}\\right) = -\\frac{4\\exp(1)\\exp(-1/4)}{3\\exp(1) - 2}\n$$\n$$\n= -\\frac{4\\exp(1 - 1/4)}{3\\exp(1) - 2} = -\\frac{4\\exp(3/4)}{3\\exp(1) - 2}\n$$\n这就是最终的精确闭式解析表达式。",
            "answer": "$$\\boxed{-\\frac{4\\exp(3/4)}{3\\exp(1) - 2}}$$"
        },
        {
            "introduction": "在机器学习中，高效的模型选择至关重要。本练习  探究了核岭回归的一个优美而强大的理论特性：留一法交叉验证（LOOCV）的闭式解。你将推导出一个仅利用单次模型拟合结果就能精确计算所有留一法预测值的公式，而无需重新训练模型$n$次。这个练习深刻揭示了线性平滑器（linear smoothers）的代数结构，并完美展示了理论洞察力如何转化为巨大的计算效率提升。",
            "id": "3136836",
            "problem": "考虑一个训练样本 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$，一个半正定核函数 $k(\\cdot, \\cdot)$，其格拉姆矩阵 (Gram matrix) $K \\in \\mathbb{R}^{n \\times n}$ 定义为 $K_{ij} = k(x_{i}, x_{j})$，以及一个正则化参数 $\\lambda > 0$。核岭回归 (Kernel Ridge Regression, KRR) 定义为惩罚经验风险的最小化器：\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\big(y_{i} - f(x_{i})\\big)^{2} + \\lambda \\,\\|f\\|_{\\mathcal{H}}^{2},\n$$\n其中 $\\mathcal{H}$ 表示由 $k(\\cdot, \\cdot)$ 导出的再生核希尔伯特空间 (Reproducing Kernel Hilbert Space, RKHS)。一个经过充分验证的事实是，在训练输入处的拟合值是响应向量的线性函数，因此存在一个称为帽子矩阵 (hat matrix) 的矩阵 $S_{\\lambda} \\in \\mathbb{R}^{n \\times n}$，使得拟合值向量为 $\\hat{\\boldsymbol{y}} = S_{\\lambda} \\boldsymbol{y}$。令 $s_{ii}$ 表示 $S_{\\lambda}$ 的第 $i$ 个对角线元素，令 $\\hat{y}_{i}$ 表示 $\\hat{\\boldsymbol{y}}$ 的第 $i$ 个元素。\n\n对于每个索引 $i \\in \\{1, \\dots, n\\}$，将留一法 (leave-one-out) 拟合值 $\\hat{y}_{-i}$ 定义为在除数据点 $(x_{i}, y_{i})$ 之外的所有数据上训练 KRR 后，在 $x_{i}$ 处的预测值。仅从上述基本定义和估计器相对于 $\\boldsymbol{y}$ 的线性性质出发，推导出一个关于 $\\hat{y}_{i}$、$y_{i}$ 和 $s_{ii}$ 的 $\\hat{y}_{-i}$ 的闭式解析表达式。你的最终答案必须是单个闭式表达式。",
            "solution": "我们从所述的基本前提开始：核岭回归 (KRR) 在训练输入上产生的拟合值是响应向量 $\\boldsymbol{y} \\in \\mathbb{R}^{n}$ 的线性函数。因此，存在一个仅依赖于输入 $\\{x_{i}\\}_{i=1}^{n}$ 和 $\\lambda$ 的矩阵 $S_{\\lambda} \\in \\mathbb{R}^{n \\times n}$，使得\n$$\n\\hat{\\boldsymbol{y}} = S_{\\lambda} \\boldsymbol{y}.\n$$\n令 $s_{ij}$ 表示 $S_{\\lambda}$ 的 $(i,j)$ 元素，$s_{ii}$ 表示其在索引 $i$ 处的对角线元素。根据线性性质，第 $i$ 个拟合值可以写成\n$$\n\\hat{y}_{i} = \\sum_{j=1}^{n} s_{ij} y_{j} = s_{ii} y_{i} + \\sum_{j \\neq i} s_{ij} y_{j}.\n$$\n\n现在考虑留一法的情形，其中数据点 $(x_{i}, y_{i})$ 被移除，KRR 在余下的 $n-1$ 个数据点上进行训练。用 $\\hat{y}_{-i}$ 表示这个留一法拟合在 $x_{i}$ 处的预测值。由于估计器对 $\\boldsymbol{y}$ 是线性的，且目标函数是二次的，因此增加一个残差恰好为零的观测值不会改变最小化器：如果一个观测值在给定的拟合 $f$ 下满足 $y_{i} = f(x_{i})$，那么它对经验损失和梯度的贡献在该拟合下都为零，因此包含或排除该观测值不会改变最小化器。\n\n利用这一点，我们考虑在全部 $n$ 个点上进行训练，但将第 $i$ 个响应替换为一个变量 $t \\in \\mathbb{R}$。根据线性性质，\n$$\n\\hat{y}_{i}(t) = s_{ii} t + \\sum_{j \\neq i} s_{ij} y_{j}.\n$$\n选择 $t$ 等于留一法预测值 $\\hat{y}_{-i}$。在此选择下，观测值 $(x_{i}, t)$ 在留一法拟合产生的拟合函数处的残差将为零，因此包含它不会改变最小化器。因此，在用 $t$ 替换后的全数据训练下，在 $x_{i}$ 处的拟合值必须等于 $t$ 本身：\n$$\nt = \\hat{y}_{i}(t) = s_{ii} t + \\sum_{j \\neq i} s_{ij} y_{j}.\n$$\n求解这个关于 $t$ 的标量线性方程，得到\n$$\nt = \\frac{\\sum_{j \\neq i} s_{ij} y_{j}}{1 - s_{ii}}.\n$$\n最后，将关于 $j \\neq i$ 的和用 $\\hat{y}_{i}$ 和 $y_{i}$ 表示：\n$$\n\\sum_{j \\neq i} s_{ij} y_{j} = \\hat{y}_{i} - s_{ii} y_{i}.\n$$\n将此代入 $t$ 的表达式，得到所求的留一法预测值的闭式公式：\n$$\n\\hat{y}_{-i} = \\frac{\\hat{y}_{i} - s_{ii} y_{i}}{1 - s_{ii}}.\n$$\n该表达式仅依赖于索引 $i$ 处的全数据拟合值、索引 $i$ 处的观测响应以及帽子矩阵 $S_{\\lambda}$ 的第 $i$ 个对角线元素。",
            "answer": "$$\\boxed{\\frac{\\hat{y}_{i} - s_{ii} y_{i}}{1 - s_{ii}}}$$"
        },
        {
            "introduction": "理论上的完美公式在有限精度的计算机上可能表现不佳。这个编码练习  将带你直面在实现核方法时一个核心的实际挑战：数值稳定性。你将通过编程实验，研究核矩阵的条件数如何影响求解的精度，并探索通过添加微小的“扰动”（jitter）来稳定求解过程的实用技术，同时权衡其对解的偏差所造成的影响。这项实践有助于你建立从理论到可靠软件实现之间的桥梁。",
            "id": "3136815",
            "problem": "要求您编写一个完整、可运行的程序，研究通过 Cholesky 分解求解核岭回归正规方程的数值条件和稳定性，并提出能够稳定求解器而不过度引入偏差的最小对角抖动量级。该问题必须完全用纯数学和数值线性代数的术语来解决，最终输出必须是单行的浮点抖动值列表，每个测试用例一个值。\n\n基本定义和事实：\n- 一个对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定 (SPD) 的，当且仅当其所有特征值都严格为正。对于一个 SPD 矩阵 $A$，Cholesky 分解 $A = L L^{\\top}$ 存在且唯一，其中 $L$ 是具有正对角元的下三角矩阵。\n- 一个 SPD 矩阵 $A$ 的 $2$-范数条件数是 $\\kappa_2(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$，其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别是 $A$ 的最大和最小特征值。\n- 在核岭回归中，求解对偶系数可归结为求解一个形式为 $(K + \\lambda I)\\alpha = y$ 的线性系统，其中 $K$ 是核 Gram 矩阵，$\\lambda \\ge 0$ 是一个正则化参数，$I$ 是单位矩阵，$y$ 是目标向量。\n- 添加一个小的单位矩阵倍数 $j I$（称为抖动），其中 $j \\ge 0$，可以通过增加 $\\lambda_{\\min}$ 来改善数值稳定性，但过大的 $j$ 会使解产生偏差，因为它改变了所求解的系统。\n\n您的任务：\n- 对于下述每个测试用例，使用指定带宽 $\\sigma > 0$ 的高斯径向基函数核 $k(\\mathbf{x}, \\mathbf{z}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{z}\\|_2^2}{2\\sigma^2}\\right)$ 构建一个核矩阵 $K$。\n- 对于给定的基础正则化 $\\lambda \\ge 0$，构建 $M(\\lambda) = K + \\lambda I$。\n- 从一个可复现的正态分布中生成一个真实解向量 $\\alpha_{\\mathrm{true}} \\in \\mathbb{R}^n$，计算 $y = M(\\lambda)\\,\\alpha_{\\mathrm{true}}$，然后对于一个抖动值网格 $j$，考虑微扰系统 $A(j) = M(\\lambda) + j I$。\n- 对于网格中的每个 $j$，执行以下所有操作：\n  1. 如果 $A(j)$ 不是 SPD（即，如果其最小特征值不严格为正），则跳过此 $j$。\n  2. 计算基于特征值的条件数 $\\kappa_2(A(j))$。\n  3. 使用基于特征分解的求解器计算 $A(j)\\,\\alpha = y$ 的参考解 $\\alpha_{\\mathrm{eig}}(j)$，该求解器对于 SPD 矩阵是良态的。\n  4. 使用 Cholesky 分解和三角求解计算 $A(j)\\,\\alpha = y$ 的基于 Cholesky 的解 $\\alpha_{\\mathrm{chol}}(j)$。\n  5. 将 Cholesky 求解的相对数值误差量化为 $e_{\\mathrm{num}}(j) = \\|\\alpha_{\\mathrm{chol}}(j) - \\alpha_{\\mathrm{eig}}(j)\\|_2 / \\|\\alpha_{\\mathrm{eig}}(j)\\|_2$。\n  6. 将添加抖动引入的相对偏差量化为 $e_{\\mathrm{bias}}(j) = \\|\\alpha_{\\mathrm{eig}}(j) - \\alpha_{\\mathrm{true}}\\|_2 / \\|\\alpha_{\\mathrm{true}}\\|_2$。\n- 从网格中选择最小的抖动 $j^\\star$，该抖动同时满足数值稳定性和偏差标准\n  $$e_{\\mathrm{num}}(j^\\star) \\le \\tau_{\\mathrm{num}} \\quad \\text{和} \\quad e_{\\mathrm{bias}}(j^\\star) \\le \\tau_{\\mathrm{bias}},$$\n  容差为 $\\tau_{\\mathrm{num}} = 10^{-8}$ 和 $\\tau_{\\mathrm{bias}} = 10^{-3}$。\n- 如果网格上没有 $j$ 满足这两个标准，则在满足 $e_{\\mathrm{bias}}(j) \\le \\tau_{\\mathrm{bias}}$ 的那些 $j$ 中，选择使 $e_{\\mathrm{num}}(j)$ 最小的一个；如果也没有一个满足偏差标准，则选择使 Cholesky 成功且 $A(j)$ 为 SPD 并且最小化 $e_{\\mathrm{bias}}(j)$ 的那个 $j$。\n\n测试套件：\n- 通用设置：\n  - 使用高斯核 $k(\\mathbf{x}, \\mathbf{z}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}-\\mathbf{z}\\|_2^2}{2\\sigma^2}\\right)$。\n  - 所有描述的范数均使用欧几里得 $2$-范数。\n  - 抖动网格为 $\\{0\\} \\cup \\{10^k : k \\in \\{-16,-15,\\dots,-2\\}\\}$。\n  - 对于特征值检查，仅当其数值计算出的最小特征值严格为正时，才将 $A(j)$ 视为 SPD。\n- 用例 1（病态但 SPD，旨在要求非零抖动）：\n  - 维度 $n = 25$，输入为 $\\mathbf{x}_i = i/(n-1)$，其中 $i \\in \\{0,1,\\dots,n-1\\}$，视为一维列向量。\n  - 带宽 $\\sigma = 10.0$。\n  - 正则化 $\\lambda = 0$。\n  - 真实解：使用固定的种子 $42$ 从 $\\mathcal{N}(0,1)^n$ 中抽取 $\\alpha_{\\mathrm{true}}$。\n- 用例 2（因重复点而半正定，Cholesky 在零抖动时失败；需要正抖动）：\n  - 从 $25$ 个输入 $\\mathbf{x}_i = i/24$（$i \\in \\{0,1,\\dots,24\\}$）开始，然后附加索引为 $10$ 到 $14$ 的输入的 $5$ 个副本，得到总共 $n = 30$ 个点，所有点均为一维列向量。\n  - 带宽 $\\sigma = 0.2$。\n  - 正则化 $\\lambda = 0$。\n  - 真实解：使用固定的种子 $7$ 从 $\\mathcal{N}(0,1)^n$ 中抽取 $\\alpha_{\\mathrm{true}}$。\n- 用例 3（良态，零抖动应已足够）：\n  - 维度 $n = 60$，输入使用固定的种子 $123$ 从 $\\mathrm{Uniform}[0,1]$ 中独立抽取，作为一维列向量。\n  - 带宽 $\\sigma = 2.0$。\n  - 正则化 $\\lambda = 10^{-12}$。\n  - 真实解：使用固定的种子 $99$ 从 $\\mathcal{N}(0,1)^n$ 中抽取 $\\alpha_{\\mathrm{true}}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例对应一个浮点数，即该用例选择的抖动 $j^\\star$，按用例 1、2、3 的顺序排列。例如，输出应类似于 $[j_1,j_2,j_3]$。\n\n不涉及任何物理单位或角度。所有答案都是无量纲的实数。给定指定的种子和输入，输出值必须是确定性的。不要打印任何额外的文本。",
            "solution": "用户旨在为三个不同的测试用例确定添加到核岭回归矩阵 $M(\\lambda) = K + \\lambda I$ 的最优对角抖动 $j^\\star$。最优抖动 $j^\\star$ 是指定网格中的最小值，它能确保基于 Cholesky 的线性求解器的数值稳定性，同时将引入的解偏差保持在给定容差内。该问题需要基于线性代数和数值分析原理进行系统的数值研究。\n\n每个测试用例的总体流程如下：\n1.  **系统设置**：构建数据点 $\\mathbf{x}_i$、高斯 RBF 核矩阵 $K$、基础回归矩阵 $M(\\lambda) = K + \\lambda I$ 和真实解向量 $\\alpha_{\\mathrm{true}}$。然后，目标向量被合成为 $y = M(\\lambda) \\alpha_{\\mathrm{true}}$。\n2.  **迭代分析**：对于网格 $\\{0\\} \\cup \\{10^k : k \\in \\{-16, \\dots, -2\\}\\}$ 中的每个抖动值 $j$，构建微扰矩阵 $A(j) = M(\\lambda) + j I$。\n3.  **验证与求解**：\n    a. 我们首先通过计算 $A(j)$ 的特征值并确保最小特征值 $\\lambda_{\\min}(A(j)) > 0$ 来验证其是否为对称正定 (SPD)。如果 $A(j)$ 不是 SPD，则它不适用于 Cholesky 分解，我们舍弃该 $j$ 值。\n    b. 对于一个有效的 SPD 矩阵 $A(j)$，我们计算系统 $A(j)\\alpha = y$ 的两个解：\n        i. **参考解 $\\alpha_{\\mathrm{eig}}(j)$**：这是使用特征分解计算的。如果 $A(j) = VDV^\\top$，其中 $V$ 是特征向量的正交矩阵，$D$ 是特征值的对角矩阵，则解为 $\\alpha_{\\mathrm{eig}}(j) = V D^{-1} V^\\top y$。该方法对于 SPD 矩阵是数值稳定的，并作为我们准确性的基准。\n        ii. **Cholesky 解 $\\alpha_{\\mathrm{chol}}(j)$**：该解的计算方法是先找到 Cholesky 分解 $A(j) = LL^\\top$，然后解两个三角系统：$Lz = y$（前向替换）后跟 $L^\\top \\alpha = z$（后向替换）。该方法计算效率高，但如果 $A(j)$ 是病态的（即具有大的条件数 $\\kappa_2(A) = \\lambda_{\\max}/\\lambda_{\\min}$），则可能数值不稳定。如果对于一个理论上是 SPD 的矩阵，Cholesky 分解失败，这表明存在严重的病态，该 $j$ 值将被舍弃。\n4.  **误差量化**：为每个有效的 $j$ 计算两个误差度量：\n    a. **数值误差**：$e_{\\mathrm{num}}(j) = \\frac{\\|\\alpha_{\\mathrm{chol}}(j) - \\alpha_{\\mathrm{eig}}(j)\\|_2}{\\|\\alpha_{\\mathrm{eig}}(j)\\|_2}$。这衡量了基于 Cholesky 的求解器相对于稳定的基于特征分解的求解器的不准确性。高值表明病态正在降低 Cholesky 解的质量。\n    b. **偏差误差**：$e_{\\mathrm{bias}}(j) = \\frac{\\|\\alpha_{\\mathrm{eig}}(j) - \\alpha_{\\mathrm{true}}\\|_2}{\\|\\alpha_{\\mathrm{true}}\\|_2}$。这衡量了抖动微扰系统 $A(j)\\alpha = y$ 的解与原始未微扰问题的真实解 $\\alpha_{\\mathrm{true}}$ 的偏离程度。添加抖动 $j>0$ 内在地改变了我们正在解决的问题，从而引入了偏差。\n5.  **最优抖动选择**：最优抖动 $j^\\star$ 是基于一个分层准则选择的，旨在找到实现数值稳定性而不过度引入偏差的最小可能微扰。\n    a. **主要准则**：找到同时满足 $e_{\\mathrm{num}}(j) \\le \\tau_{\\mathrm{num}}$ 和 $e_{\\mathrm{bias}}(j) \\le \\tau_{\\mathrm{bias}}$ 的最小 $j$，其中容差给定为 $\\tau_{\\mathrm{num}} = 10^{-8}$ 和 $\\tau_{\\mathrm{bias}} = 10^{-3}$。\n    b. **次要准则（如果主要准则失败）**：如果没有 $j$ 满足这两个条件，我们放宽数值稳定性要求。在所有满足偏差约束 $e_{\\mathrm{bias}}(j) \\le \\tau_{\\mathrm{bias}}$ 的 $j$ 中，我们选择使数值误差 $e_{\\mathrm{num}}(j)$ 最小的一个。\n    c. **第三准则（如果次要准则也失败）**：如果没有 $j$ 满足偏差约束，我们优先找到一个解，无论其偏差多大。在所有使矩阵 $A(j)$ 为 SPD 且 Cholesky 求解器成功的 $j$ 中，我们选择使偏差误差 $e_{\\mathrm{bias}}(j)$ 最小的一个。\n\n此过程将应用于三个测试用例中的每一个，这些用例旨在探测不同情景：一个病态但理论上是 SPD 的矩阵、一个由重复数据产生的奇异矩阵，以及一个良态矩阵。\n\n**用例 1**：$K$ 是用大带宽 $\\sigma=10.0$ 和 $\\lambda=0$ 生成的。这会创建一个病态的核矩阵，其最小特征值非常接近于零，使得 $A(0)=K$ 对于 Cholesky 分解在数值上具有挑战性。预计需要一个小的非零抖动来“提升”这些小特征值并稳定计算。\n\n**用例 2**：输入数据包含重复点，且 $\\lambda=0$。这使得核矩阵 $K$ 在数学上是奇异的（即，它至少有一个特征值恰好为零），因此是半正定的，而不是正定的。$A(0)=K$ 的 Cholesky 分解保证会失败。严格需要一个正的抖动 $j>0$ 来使矩阵 $A(j) = K + jI$ 成为正定矩阵。\n\n**用例 3**：矩阵是用一个小的非零正则化 $\\lambda = 10^{-12}$ 和唯一的数据点构成的。所得到的矩阵 $A(0) = K + \\lambda I$ 预计是良态且严格正定的。因此，我们预期不需要额外的抖动，$j^\\star=0$ 将满足主要准则。\n\n实现将使用 `numpy` 进行线性代数运算，并使用 `scipy.linalg` 中专门的、鲁棒的求解器，如 `cholesky` 和 `solve_triangular`。通过为所有随机数生成使用固定的种子来确保可复现性。最终输出将是一个列表，包含为每个用例确定的 $j^\\star$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular, eigh\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    \n    test_cases = [\n        {\n            'case_id': 1,\n            'n': 25,\n            'data_gen': lambda n, rng: np.linspace(0, 1, n).reshape(-1, 1),\n            'sigma': 10.0,\n            'lambda': 0.0,\n            'seed_alpha': 42,\n            'seed_data': None,\n        },\n        {\n            'case_id': 2,\n            'n_base': 25,\n            'data_gen': lambda n_base, rng: np.concatenate([\n                np.linspace(0, 1, n_base).reshape(-1, 1),\n                np.linspace(0, 1, n_base).reshape(-1, 1)[10:15]\n            ]),\n            'sigma': 0.2,\n            'lambda': 0.0,\n            'seed_alpha': 7,\n            'seed_data': None,\n        },\n        {\n            'case_id': 3,\n            'n': 60,\n            'data_gen': lambda n, rng: rng.uniform(0, 1, n).reshape(-1, 1),\n            'sigma': 2.0,\n            'lambda': 1e-12,\n            'seed_alpha': 99,\n            'seed_data': 123,\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        j_star = process_case(case_params)\n        results.append(j_star)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef rbf_kernel(X, sigma):\n    \"\"\"\n    Computes the Gaussian RBF kernel matrix.\n    k(x, z) = exp(-||x-z||_2^2 / (2*sigma^2))\n    \n    Args:\n        X (np.ndarray): Data matrix of shape (n, d).\n        sigma (float): Bandwidth of the RBF kernel.\n\n    Returns:\n        np.ndarray: The kernel matrix K of shape (n, n).\n    \"\"\"\n    sq_dists = cdist(X, X, 'sqeuclidean')\n    return np.exp(-sq_dists / (2 * sigma**2))\n\ndef process_case(params):\n    \"\"\"\n    Processes a single test case to find the optimal jitter.\n    \"\"\"\n    # Tolerances\n    tau_num = 1e-8\n    tau_bias = 1e-3\n\n    # Generate data\n    data_rng = np.random.default_rng(params['seed_data']) if params['seed_data'] is not None else None\n    if params['case_id'] == 2:\n        X = params['data_gen'](params['n_base'], data_rng)\n        n = X.shape[0]\n    else:\n        n = params['n']\n        X = params['data_gen'](n, data_rng)\n\n    # Generate ground truth alpha\n    alpha_rng = np.random.default_rng(params['seed_alpha'])\n    alpha_true = alpha_rng.normal(0, 1, n)\n\n    # System setup\n    K = rbf_kernel(X, params['sigma'])\n    M_lambda = K + params['lambda'] * np.identity(n)\n    y = M_lambda @ alpha_true\n\n    # Jitter analysis\n    jitter_grid = [0.0] + [10.0**k for k in range(-16, -1)]\n    j_results = []\n\n    for j in jitter_grid:\n        A_j = M_lambda + j * np.identity(n)\n\n        # 1. Check if SPD via eigenvalues\n        try:\n            eigenvalues, V = eigh(A_j)\n            if np.min(eigenvalues) = 0:\n                continue\n        except np.linalg.LinAlgError:\n            continue\n\n        # 3. Reference solution (eigen-decomposition)\n        D_inv = np.diag(1.0 / eigenvalues)\n        alpha_eig = V @ (D_inv @ (V.T @ y))\n        \n        # 4. Cholesky solution\n        try:\n            L = cholesky(A_j, lower=True)\n            z = solve_triangular(L, y, lower=True, check_finite=False)\n            alpha_chol = solve_triangular(L.T, z, lower=False, check_finite=False)\n        except np.linalg.LinAlgError:\n            continue\n\n        # 5. Numerical error\n        norm_alpha_eig = np.linalg.norm(alpha_eig)\n        e_num = np.linalg.norm(alpha_chol - alpha_eig) / norm_alpha_eig if norm_alpha_eig > 0 else 0.0\n\n        # 6. Bias error\n        norm_alpha_true = np.linalg.norm(alpha_true)\n        e_bias = np.linalg.norm(alpha_eig - alpha_true) / norm_alpha_true if norm_alpha_true > 0 else 0.0\n\n        j_results.append({'j': j, 'e_num': e_num, 'e_bias': e_bias})\n    \n    # Selection logic\n    # Rule 1: Smallest j satisfying both criteria\n    s1_candidates = [r for r in j_results if r['e_num'] = tau_num and r['e_bias'] = tau_bias]\n    if s1_candidates:\n        return min(r['j'] for r in s1_candidates)\n\n    # Rule 2: Among those satisfying bias, find j that minimizes num_error\n    s2_candidates = [r for r in j_results if r['e_bias'] = tau_bias]\n    if s2_candidates:\n        # min will pick the first one in case of a tie in e_num, which corresponds to smaller j\n        best_choice = min(s2_candidates, key=lambda r: r['e_num'])\n        return best_choice['j']\n\n    # Rule 3: Among all successful, find j that minimizes bias_error\n    if j_results:\n        # min will pick the first one in case of a tie in e_bias, which corresponds to smaller j\n        best_choice = min(j_results, key=lambda r: r['e_bias'])\n        return best_choice['j']\n        \n    # Should not be reached in this problem\n    return float('nan')\n\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}