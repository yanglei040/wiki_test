{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a hands-on entry point into optimizing a stacking ensemble for a probabilistic classification task. You will discover that the definition of an \"optimal\" ensemble is not universal; it depends directly on the chosen performance metric. By implementing and comparing the weight vectors derived from minimizing two different proper scoring rules—the Brier score and logarithmic loss—you will gain practical insight into how the objective function shapes the final model .",
            "id": "3175480",
            "problem": "You are given a binary probabilistic classification stacking task. There are $M$ base probabilistic classifiers that produce predicted probabilities for $N$ labeled samples. A stacked predictor forms a convex mixture of base predictions. Let $y_i \\in \\{0,1\\}$ denote the true label for sample $i \\in \\{1,\\dots,N\\}$, and let $p_{ij} \\in [0,1]$ denote the predicted probability from base model $j \\in \\{1,\\dots,M\\}$ for sample $i$. Let $w = (w_1,\\dots,w_M)$ be the stacking weights. The mixed predicted probability for sample $i$ is defined as\n$$\n\\hat{p}_i(w) = \\sum_{j=1}^M w_j\\, p_{ij},\n$$\nsubject to the convexity constraints\n$$\nw_j \\ge 0 \\text{ for all } j \\in \\{1,\\dots,M\\}, \\quad \\sum_{j=1}^M w_j = 1.\n$$\n\nTwo strictly proper scoring rules will be used to evaluate the predictions:\n1. The logarithmic loss (also called negative log-likelihood for Bernoulli outcomes),\n$$\n\\ell_{\\mathrm{log}}(y_i,\\hat{p}_i) = -\\left[y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i)\\right],\n$$\nwith the convention that to avoid $\\log(0)$ we apply a clipping $\\hat{p}_i \\leftarrow \\min\\{\\max\\{\\hat{p}_i,\\epsilon\\},1-\\epsilon\\}$ for a fixed $\\epsilon = 10^{-15}$ before evaluating the logarithm.\n2. The Brier score,\n$$\n\\ell_{\\mathrm{brier}}(y_i,\\hat{p}_i) = \\left(y_i - \\hat{p}_i\\right)^2.\n$$\n\nYour task is to compute, for each provided test case, the stacking weight vector $w$ that minimizes the empirical risk under each scoring rule, that is,\n$$\n\\min_{w \\in \\mathbb{R}^M} \\frac{1}{N}\\sum_{i=1}^N \\ell_{\\mathrm{log}}(y_i,\\hat{p}_i(w)) \\quad \\text{subject to } w_j \\ge 0, \\ \\sum_{j=1}^M w_j = 1,\n$$\nand\n$$\n\\min_{w \\in \\mathbb{R}^M} \\frac{1}{N}\\sum_{i=1}^N \\ell_{\\mathrm{brier}}(y_i,\\hat{p}_i(w)) \\quad \\text{subject to } w_j \\ge 0, \\ \\sum_{j=1}^M w_j = 1.\n$$\nThen compare the resulting weight vectors by reporting the $L_1$ difference $\\|w^{\\mathrm{brier}} - w^{\\mathrm{log}}\\|_1 = \\sum_{j=1}^M |w^{\\mathrm{brier}}_j - w^{\\mathrm{log}}_j|$.\n\nStart from fundamental definitions of empirical risk minimization, convex combinations on the probability simplex, and the definitions of the two strictly proper scoring rules above. Derive the necessary gradients or conditions for optimality from these bases and implement a numerically stable solver that respects the constraints.\n\nUse the following fixed test suite. Each case specifies $N$, $M$, the labels $y$, and the base-model prediction matrix $P$ whose $i$-th row is $(p_{i1},\\dots,p_{iM})$. The models are ordered as $m_1, m_2, m_3$ within each case.\n\nTest case $1$ (general case, $N=8$, $M=3$):\n- $y = [0,1,0,1,0,1,0,1]$.\n- $m_1$ predictions: $[0.3,0.7,0.4,0.6,0.3,0.65,0.35,0.7]$.\n- $m_2$ predictions: $[0.2,0.8,0.3,0.7,0.4,0.5,0.2,0.85]$.\n- $m_3$ predictions: $[0.1,0.9,0.2,0.8,0.2,0.8,0.3,0.7]$.\n\nTest case $2$ (two identical models, $N=4$, $M=3$):\n- $y = [0,0,1,1]$.\n- $m_1$ predictions: $[0.1,0.2,0.8,0.9]$.\n- $m_2$ predictions: $[0.1,0.2,0.8,0.9]$.\n- $m_3$ predictions: $[0.2,0.2,0.7,0.85]$.\n\nTest case $3$ (near-perfect model present, $N=6$, $M=3$):\n- $y = [1,0,1,0,1,0]$.\n- $m_1$ predictions: $[0.6,0.4,0.55,0.45,0.65,0.35]$.\n- $m_2$ predictions: $[0.3,0.7,0.4,0.6,0.35,0.65]$.\n- $m_3$ predictions: $[0.99,0.01,0.98,0.02,0.97,0.03]$.\n\nTest case $4$ (probability extremes to test stability, $N=4$, $M=3$):\n- $y = [1,1,0,0]$.\n- $m_1$ predictions: $[0.0,0.1,1.0,0.9]$.\n- $m_2$ predictions: $[0.2,0.0,0.8,1.0]$.\n- $m_3$ predictions: $[0.05,0.05,0.95,0.95]$.\n\nImplementation details and requirements:\n- For logarithmic loss, apply the clipping with $\\epsilon = 10^{-15}$ to $\\hat{p}_i(w)$ inside both the objective and its derivatives to ensure numerical stability.\n- Enforce the constraints $w_j \\ge 0$ and $\\sum_{j=1}^M w_j = 1$ exactly in your optimization routine.\n- For each test case, return three items: the Brier-optimal weight vector $w^{\\mathrm{brier}}$, the log-loss-optimal weight vector $w^{\\mathrm{log}}$, and the $L_1$ difference between them.\n- Round all reported real numbers to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list of lists without spaces. For each test case, output a list with three items: the list of $M$ Brier weights, the list of $M$ log-loss weights, and the scalar $L_1$ difference. For example, a valid overall format is\n$[[[w_{1,1},w_{1,2},w_{1,3}],[v_{1,1},v_{1,2},v_{1,3}],d_1],[[w_{2,1},w_{2,2},w_{2,3}],[v_{2,1},v_{2,2},v_{2,3}],d_2],[[w_{3,1},w_{3,2},w_{3,3}],[v_{3,1},v_{3,2},v_{3,3}],d_3],[[w_{4,1},w_{4,2},w_{4,3}],[v_{4,1},v_{4,2},v_{4,3}],d_4]]$,\nwhere each $w_{k,j}$, $v_{k,j}$, and $d_k$ is printed with exactly $6$ decimal places.",
            "solution": "The problem requires finding the optimal weight vector $w \\in \\mathbb{R}^M$ for a stacking ensemble of $M$ probabilistic classifiers. The optimal weights are those that minimize the empirical risk, evaluated over $N$ samples, under two different scoring rules: the Brier score and the logarithmic loss. The weight vector must satisfy the convexity constraints $w_j \\ge 0$ for all $j \\in \\{1,\\dots,M\\}$ and $\\sum_{j=1}^M w_j = 1$. This set of constraints defines the standard $(M-1)$-simplex, a compact convex set denoted by $\\Delta^{M-1}$.\n\nLet $y \\in \\{0,1\\}^N$ be the vector of true labels, and let $P$ be the $N \\times M$ matrix where $P_{ij} = p_{ij}$ is the predicted probability from base model $j$ for sample $i$. The stacked prediction for sample $i$ is a convex combination of the base predictions:\n$$\n\\hat{p}_i(w) = \\sum_{j=1}^M w_j P_{ij}\n$$\nIn vector notation, the vector of stacked predictions for all $N$ samples is $\\hat{p}(w) = Pw$.\n\nThe task is to solve two independent constrained optimization problems.\n\n### 1. Minimization of the Brier Score\n\nThe Brier score for a single prediction is $\\ell_{\\mathrm{brier}}(y_i, \\hat{p}_i) = (y_i - \\hat{p}_i)^2$. The empirical risk is the average Brier score over all samples:\n$$\nR_{\\mathrm{brier}}(w) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{p}_i(w))^2 = \\frac{1}{N} \\|y - Pw\\|_2^2\n$$\nThe optimization problem is:\n$$\n\\min_{w \\in \\Delta^{M-1}} R_{\\mathrm{brier}}(w)\n$$\nThis is a convex optimization problem. To solve it using a gradient-based method, we first expand the objective function:\n$$\nR_{\\mathrm{brier}}(w) = \\frac{1}{N} (y - Pw)^T(y - Pw) = \\frac{1}{N} (y^Ty - 2y^TPw + w^TP^TPw)\n$$\nThis is a quadratic function of $w$. This specific type of problem is known as a Quadratic Program (QP). The gradient of the objective function with respect to $w$ is required for the optimization algorithm. Using the rules of vector calculus, we find the gradient:\n$$\n\\nabla_w R_{\\mathrm{brier}}(w) = \\frac{1}{N} \\nabla_w (y^Ty - 2y^TPw + w^TP^TPw) = \\frac{1}{N} (-2P^Ty + 2P^TPw) = \\frac{2}{N} (P^TPw - P^Ty)\n$$\nThis can also be expressed in terms of the prediction vector $\\hat{p} = Pw$ as $\\nabla_w R_{\\mathrm{brier}}(w) = \\frac{2}{N} P^T(Pw - y) = -\\frac{2}{N} P^T(y - \\hat{p})$.\n\n### 2. Minimization of the Logarithmic Loss\n\nThe logarithmic loss for a single prediction is $\\ell_{\\mathrm{log}}(y_i, \\hat{p}_i) = -[y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i)]$. The empirical risk is the average log-loss:\n$$\nR_{\\mathrm{log}}(w) = \\frac{1}{N} \\sum_{i=1}^N -\\left[y_i \\log(\\hat{p}_i(w)) + (1-y_i)\\log(1-\\hat{p}_i(w))\\right]\n$$\nFor numerical stability, the predicted probabilities $\\hat{p}_i$ are clipped to the range $[\\epsilon, 1-\\epsilon]$ before being passed to the logarithm, where $\\epsilon=10^{-15}$. Let $\\hat{p}_c(w) = \\text{clip}(\\hat{p}(w), \\epsilon, 1-\\epsilon)$. The objective function is $R_{\\mathrm{log}}(w)$ with $\\hat{p}_i(w)$ replaced by $\\hat{p}_{ic}(w)$. This function is convex. The optimization problem is:\n$$\n\\min_{w \\in \\Delta^{M-1}} R_{\\mathrm{log}}(w)\n$$\nTo find the gradient, we differentiate with respect to each component $w_k$:\n$$\n\\frac{\\partial R_{\\mathrm{log}}}{\\partial w_k} = \\frac{1}{N} \\sum_{i=1}^N -\\left[ \\frac{y_i}{\\hat{p}_{ic}} \\frac{\\partial \\hat{p}_{ic}}{\\partial w_k} - \\frac{1-y_i}{1-\\hat{p}_{ic}} \\frac{\\partial \\hat{p}_{ic}}{\\partial w_k} \\right]\n$$\nAssuming the optimal solution lies in the interior of the clipping range for all samples, $\\frac{\\partial \\hat{p}_{ic}}{\\partial w_k} = \\frac{\\partial \\hat{p}_i}{\\partial w_k} = P_{ik}$. The derivative becomes:\n$$\n\\frac{\\partial R_{\\mathrm{log}}}{\\partial w_k} = -\\frac{1}{N} \\sum_{i=1}^N \\left( \\frac{y_i}{\\hat{p}_{ic}} - \\frac{1-y_i}{1-\\hat{p}_{ic}} \\right) P_{ik}\n$$\nIn vector form, the gradient is:\n$$\n\\nabla_w R_{\\mathrm{log}}(w) = -\\frac{1}{N} P^T \\left( \\frac{y}{\\hat{p}_c} - \\frac{1-y}{1-\\hat{p}_c} \\right)\n$$\nwhere the divisions are performed element-wise. The clipping of $\\hat{p}$ to $\\hat{p}_c$ is crucial for preventing division by zero and ensuring the gradient is well-defined.\n\n### 3. Numerical Solution Strategy\n\nBoth problems are instances of convex optimization with linear constraints. A suitable algorithm for this is Sequential Least Squares Programming (SLSQP), which can handle both equality and inequality constraints. We use the implementation provided in `scipy.optimize.minimize`.\n\nThe optimization problem is configured as follows:\n- **Objective Function**: $R_{\\mathrm{brier}}(w)$ or $R_{\\mathrm{log}}(w)$.\n- **Jacobian (Gradient)**: $\\nabla_w R_{\\mathrm{brier}}(w)$ or $\\nabla_w R_{\\mathrm{log}}(w)$.\n- **Constraints**:\n    1. Equality constraint: $\\sum_{j=1}^M w_j - 1 = 0$.\n    2. Inequality constraints (bounds): $w_j \\ge 0$ for all $j=1, \\dots, M$.\n- **Initial Guess**: A uniform weight vector, $w_j = 1/M$ for all $j$, is a reasonable starting point.\n\nThis setup allows for a numerically stable and efficient computation of the optimal weight vectors $w^{\\mathrm{brier}}$ and $w^{\\mathrm{log}}$. The final step is to compute the $L_1$ difference $\\|w^{\\mathrm{brier}} - w^{\\mathrm{log}}\\|_1 = \\sum_{j=1}^M |w^{\\mathrm{brier}}_j - w^{\\mathrm{log}}_j|$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the stacking optimization problems for all test cases.\n    \"\"\"\n    epsilon = 1e-15\n\n    test_cases = [\n        # Test case 1 (general case, N=8, M=3)\n        {\n            \"y\": np.array([0, 1, 0, 1, 0, 1, 0, 1]),\n            \"P\": np.array([\n                [0.3, 0.2, 0.1],\n                [0.7, 0.8, 0.9],\n                [0.4, 0.3, 0.2],\n                [0.6, 0.7, 0.8],\n                [0.3, 0.4, 0.2],\n                [0.65, 0.5, 0.8],\n                [0.35, 0.2, 0.3],\n                [0.7, 0.85, 0.7]\n            ])\n        },\n        # Test case 2 (two identical models, N=4, M=3)\n        {\n            \"y\": np.array([0, 0, 1, 1]),\n            \"P\": np.array([\n                [0.1, 0.1, 0.2],\n                [0.2, 0.2, 0.2],\n                [0.8, 0.8, 0.7],\n                [0.9, 0.9, 0.85]\n            ])\n        },\n        # Test case 3 (near-perfect model present, N=6, M=3)\n        {\n            \"y\": np.array([1, 0, 1, 0, 1, 0]),\n            \"P\": np.array([\n                [0.6, 0.3, 0.99],\n                [0.4, 0.7, 0.01],\n                [0.55, 0.4, 0.98],\n                [0.45, 0.6, 0.02],\n                [0.65, 0.35, 0.97],\n                [0.35, 0.65, 0.03]\n            ])\n        },\n        # Test case 4 (probability extremes, N=4, M=3)\n        {\n            \"y\": np.array([1, 1, 0, 0]),\n            \"P\": np.array([\n                [0.0, 0.2, 0.05],\n                [0.1, 0.0, 0.05],\n                [1.0, 0.8, 0.95],\n                [0.9, 1.0, 0.95]\n            ])\n        }\n    ]\n\n    def solve_weights(y, P, loss_type, epsilon):\n        \"\"\"\n        Solves for the optimal stacking weights for a given loss function.\n        \"\"\"\n        N, M = P.shape\n        w0 = np.full(M, 1 / M)\n\n        if loss_type == 'brier':\n            def objective(w):\n                phat = P @ w\n                return np.mean((y - phat)**2)\n\n            def jacobian(w):\n                phat = P @ w\n                return (2 / N) * P.T @ (phat - y)\n        elif loss_type == 'log':\n            def objective(w):\n                phat = P @ w\n                phat_clipped = np.clip(phat, epsilon, 1 - epsilon)\n                return -np.mean(y * np.log(phat_clipped) + (1 - y) * np.log(1 - phat_clipped))\n\n            def jacobian(w):\n                phat = P @ w\n                phat_clipped = np.clip(phat, epsilon, 1 - epsilon)\n                grad_term = y / phat_clipped - (1 - y) / (1 - phat_clipped)\n                return -(1 / N) * P.T @ grad_term\n        else:\n            raise ValueError(\"Invalid loss_type specified.\")\n\n        constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n        bounds = tuple((0, None) for _ in range(M))\n        \n        result = minimize(\n            fun=objective,\n            x0=w0,\n            method='SLSQP',\n            jac=jacobian,\n            bounds=bounds,\n            constraints=constraints,\n            tol=1e-12\n        )\n        # It's possible for weights to be very small negative numbers due to precision.\n        # Clip at 0 and re-normalize to strictly satisfy constraints.\n        w_opt = np.maximum(0, result.x)\n        w_opt /= np.sum(w_opt)\n        return w_opt\n\n    results = []\n    for case in test_cases:\n        y, P = case[\"y\"], case[\"P\"]\n        \n        w_brier = solve_weights(y, P, 'brier', epsilon)\n        w_log = solve_weights(y, P, 'log', epsilon)\n        \n        l1_diff = np.sum(np.abs(w_brier - w_log))\n        \n        # Format results to 6 decimal places as strings\n        w_brier_str = '[' + ','.join([f\"{x:.6f}\" for x in w_brier]) + ']'\n        w_log_str = '[' + ','.join([f\"{x:.6f}\" for x in w_log]) + ']'\n        l1_diff_str = f\"{l1_diff:.6f}\"\n        \n        case_result_str = f\"[{w_brier_str},{w_log_str},{l1_diff_str}]\"\n        results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    final_output = '[' + ','.join(results) + ']'\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "After learning to compute stacking weights, a crucial next step is to understand the stability and uniqueness of the solution. This exercise delves into the important theoretical issue of weight identifiability, which arises when base models are correlated. You will use fundamental principles of linear algebra to explore why standard least squares can yield non-unique weights and discover how regularization techniques provide a robust remedy to this problem .",
            "id": "3175491",
            "problem": "A prediction stacking system uses $M$ base learners. For a dataset with $n$ observations, let $Z \\in \\mathbb{R}^{n \\times M}$ collect the out-of-sample predictions of the base learners in its columns, and let $w \\in \\mathbb{R}^{M}$ be the stacking weights. The stacked predictor is the linear map $\\hat{y}(w) = Z w \\in \\mathbb{R}^{n}$. Suppose $\\operatorname{rank}(Z) = r$ with $r < M$.\n\nStarting only from the definitions of linear prediction, the column space and null space of a matrix, the Rank–Nullity Theorem, the Moore–Penrose pseudoinverse, and the properties of convex optimization, answer the following:\n\n1. Derive a necessary and sufficient condition on $w_{1}$ and $w_{2}$ for $\\hat{y}(w_{1}) = \\hat{y}(w_{2})$. Using this, characterize the entire set of weights that yield the same predictions as a given reference weight $w_{0}$. Your characterization must be explicit and parameterized using fundamental linear-algebraic objects associated with $Z$.\n\n2. Consider the empirical risk minimization problem with squared error and a separable regularizer,\n$$\n\\min_{w \\in \\mathbb{R}^{M}} \\; \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\, \\Omega(w),\n$$\nwith $\\lambda \\ge 0$. For each of the following choices, reason about identifiability of $w$ and of the predictions $\\hat{y} = Z w$:\n   - (i) No regularization: $\\lambda = 0$ with $\\Omega(w) \\equiv 0$.\n   - (ii) Ridge regularization: $\\Omega(w) = \\|w\\|_{2}^{2}$ with $\\lambda > 0$.\n   - (iii) Least Absolute Shrinkage and Selection Operator (LASSO): $\\Omega(w) = \\|w\\|_{1}$ with $\\lambda > 0$.\nIn each case, justify whether the optimizer $w^{\\star}$ is unique, whether the predictions $\\hat{y}^{\\star} = Z w^{\\star}$ are unique, and how the solution relates to the linear-algebraic decomposition of $\\mathbb{R}^{M}$ induced by $Z$.\n\n3. Let $\\mathcal{W}(w_{0})$ denote the full set of weight vectors that produce exactly the same predictions as an arbitrary reference $w_{0} \\in \\mathbb{R}^{M}$. What is the dimension of $\\mathcal{W}(w_{0})$ expressed in terms of $M$ and $r$? Report this dimension as your final answer as a closed-form expression in $M$ and $r$.\n\nNo numerical rounding is required for this problem. Provide the final answer as instructed, and do not include any units.",
            "solution": "This problem investigates the identifiability of parameters and predictions in a linear stacking model, particularly when the base learners' predictions are collinear. The validation of the problem statement is as follows.\n\nGivens extracted from the problem statement:\n- The number of base learners is $M$.\n- The number of observations is $n$.\n- The matrix of out-of-sample predictions is $Z \\in \\mathbb{R}^{n \\times M}$.\n- The stacking weights vector is $w \\in \\mathbb{R}^{M}$.\n- The stacked predictor is defined as the linear map $\\hat{y}(w) = Z w \\in \\mathbb{R}^{n}$.\n- The rank of the prediction matrix is $\\operatorname{rank}(Z) = r$.\n- It is given that $r < M$.\n- The solution must be derived starting from the definitions of linear prediction, column space ($\\mathcal{C}(Z)$), null space ($\\mathcal{N}(Z)$), the Rank–Nullity Theorem, the Moore–Penrose pseudoinverse, and properties of convex optimization.\n- The empirical risk minimization problem is $\\min_{w \\in \\mathbb{R}^{M}} \\; \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\, \\Omega(w)$ with $\\lambda \\ge 0$.\n- Three cases for the regularizer $\\Omega(w)$ are specified: (i) $\\Omega(w) = 0$, (ii) $\\Omega(w) = \\|w\\|_{2}^{2}$ with $\\lambda > 0$, and (iii) $\\Omega(w) = \\|w\\|_{1}$ with $\\lambda > 0$.\n\nThe problem is scientifically grounded, well-posed, and objective. It is a standard, formal problem in statistical learning and optimization theory. The premises are mathematically consistent and the questions are well-defined, leading to a unique and meaningful solution based on established mathematical principles. The condition $r < M$ is crucial and represents a common practical scenario where base learners are correlated. The problem is therefore deemed valid.\n\nPart 1: Characterization of equivalent weights.\n\nTwo weight vectors $w_{1} \\in \\mathbb{R}^{M}$ and $w_{2} \\in \\mathbb{R}^{M}$ produce the same stacked predictions if and only if $\\hat{y}(w_{1}) = \\hat{y}(w_{2})$. By the definition of the predictor, this equality is:\n$$\nZ w_{1} = Z w_{2}\n$$\nRearranging the terms, we get:\n$$\nZ w_{1} - Z w_{2} = 0\n$$\nUsing the distributive property of matrix multiplication, this becomes:\n$$\nZ (w_{1} - w_{2}) = 0\n$$\nThis equation states that the vector difference $(w_{1} - w_{2})$ is mapped to the zero vector by the linear transformation $Z$. By definition, the set of all such vectors constitutes the null space (or kernel) of the matrix $Z$, denoted $\\mathcal{N}(Z)$. Therefore, the necessary and sufficient condition for $w_{1}$ and $w_{2}$ to yield the same predictions is that their difference must lie in the null space of $Z$:\n$$\n(w_{1} - w_{2}) \\in \\mathcal{N}(Z)\n$$\nNow, we characterize the entire set of weights, denoted $\\mathcal{W}(w_{0})$, that yield the same predictions as a given reference weight vector $w_{0}$. A vector $w$ belongs to $\\mathcal{W}(w_{0})$ if and only if $\\hat{y}(w) = \\hat{y}(w_{0})$. From our derived condition, this is equivalent to:\n$$\n(w - w_{0}) \\in \\mathcal{N}(Z)\n$$\nThis means there exists some vector $v \\in \\mathcal{N}(Z)$ such that $w - w_{0} = v$. Solving for $w$, we find that any element of $\\mathcal{W}(w_{0})$ can be expressed as:\n$$\nw = w_{0} + v \\quad \\text{for some } v \\in \\mathcal{N}(Z)\n$$\nThis explicitly characterizes the set $\\mathcal{W}(w_{0})$ as the set of all vectors formed by adding any vector from the null space of $Z$ to the reference vector $w_{0}$. This set is an affine subspace of $\\mathbb{R}^{M}$, which can be written compactly as:\n$$\n\\mathcal{W}(w_{0}) = w_{0} + \\mathcal{N}(Z) = \\{ w_{0} + v \\mid v \\in \\mathcal{N}(Z) \\}\n$$\n\nPart 2: Identifiability analysis for regularized empirical risk minimization.\n\nThe general objective function is $L(w) = \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\, \\Omega(w)$. The first term, the squared error loss, is a convex function of $w$. Its Hessian matrix is $\\nabla_{w}^{2} \\left(\\frac{1}{2n} \\| y - Z w \\|_{2}^{2}\\right) = \\frac{1}{n} Z^{T} Z$. Since $\\operatorname{rank}(Z) = r < M$, the matrix $Z^{T} Z \\in \\mathbb{R}^{M \\times M}$ also has rank $r$. This means $Z^{T} Z$ is positive semi-definite but not positive definite. Its null space is identical to the null space of $Z$, $\\mathcal{N}(Z^{T}Z) = \\mathcal{N}(Z)$, which is non-trivial because $r < M$. Consequently, the squared error loss term is convex but not strictly convex.\n\n(i) No regularization ($\\lambda = 0$):\nThe problem is the standard Ordinary Least Squares (OLS) problem: $\\min_{w} \\frac{1}{2n} \\| y - Z w \\|_{2}^{2}$.\n- Uniqueness of $w^{\\star}$: Since the objective function is not strictly convex, the optimizer $w^{\\star}$ is not unique. If $w^{\\star}$ is a minimizer, then for any non-zero vector $v \\in \\mathcal{N}(Z)$, the vector $w^{\\star} + v$ is also a minimizer. This is because $Z(w^{\\star} + v) = Z w^{\\star} + Zv = Z w^{\\star}$, so the value of the objective function remains unchanged. The set of all solutions is the affine subspace $w^{\\star}_{p} + \\mathcal{N}(Z)$, where $w^{\\star}_{p} = Z^{+}y$ is the particular solution given by the Moore-Penrose pseudoinverse. Thus, $w^{\\star}$ is **not unique**.\n- Uniqueness of $\\hat{y}^{\\star}$: The predictions are $\\hat{y}^{\\star} = Z w^{\\star}$. Let $w^{\\star}_{1}$ and $w^{\\star}_{2}$ be two distinct optimal weight vectors. From Part 1, we know that $w^{\\star}_{2} - w^{\\star}_{1} \\in \\mathcal{N}(Z)$. Therefore, $Z w^{\\star}_{1} = Z w^{\\star}_{2}$. This shows that all optimal weight vectors lead to the same prediction vector. The unique prediction $\\hat{y}^{\\star}$ is the orthogonal projection of the target vector $y$ onto the column space of $Z$, $\\mathcal{C}(Z)$. This can be written as $\\hat{y}^{\\star} = Z Z^{+} y$. Thus, the prediction $\\hat{y}^{\\star}$ is **unique**.\n\n(ii) Ridge regularization ($\\Omega(w) = \\|w\\|_{2}^{2}$, $\\lambda > 0$):\nThe objective function is $L(w) = \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\|w\\|_{2}^{2}$.\n- Uniqueness of $w^{\\star}$: The Hessian of the objective function is $\\nabla^{2} L(w) = \\frac{1}{n} Z^{T} Z + 2 \\lambda I$. The matrix $\\frac{1}{n} Z^{T} Z$ is positive semi-definite. The matrix $2 \\lambda I$ is positive definite because $\\lambda > 0$. The sum of a positive semi-definite matrix and a positive definite matrix is positive definite. A function with a positive definite Hessian is strictly convex. A strictly convex function defined on $\\mathbb{R}^{M}$ has at most one global minimum. Since the function is also coercive (i.e., $L(w) \\to \\infty$ as $\\|w\\|_{2} \\to \\infty$), a unique minimizer exists. Therefore, the optimizer $w^{\\star}$ is **unique**.\n- Uniqueness of $\\hat{y}^{\\star}$: Since the optimizer $w^{\\star}$ is unique, the resulting prediction $\\hat{y}^{\\star} = Z w^{\\star}$ is also necessarily **unique**. The Ridge penalty effectively resolves the ambiguity caused by the non-trivial null space $\\mathcal{N}(Z)$ by selecting the solution (among those that might otherwise have similar loss values) with the smallest L2-norm.\n\n(iii) LASSO regularization ($\\Omega(w) = \\|w\\|_{1}$, $\\lambda > 0$):\nThe objective function is $L(w) = \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\|w\\|_{1}$.\n- Uniqueness of $w^{\\star}$: The objective is a sum of the convex squared error term and the convex L1-norm term. The sum is therefore convex. However, neither term is guaranteed to be strictly convex, and their sum is not strictly convex in general. The condition $r < M$ implies that there exist linear dependencies among the columns of $Z$. This can lead to non-unique solutions for $w^{\\star}$. For example, if two columns are identical, $Z_{i} = Z_{j}$, then the loss term depends only on the sum $w_{i} + w_{j}$, while the penalty term is $\\lambda (|w_{i}|+|w_{j}|)$. For a fixed sum $w_{i} + w_{j} = c$, the penalty is minimized by setting one of $w_i$ or $w_j$ to $c$ and the other to $0$, leading to multiple solutions. Since $\\operatorname{rank}(Z) < M$, there is always a non-zero $v \\in \\mathcal{N}(Z)$, and conditions for non-uniqueness can be met. Thus, the optimizer $w^{\\star}$ is **not unique** in general.\n- Uniqueness of $\\hat{y}^{\\star}$: To analyze the uniqueness of the prediction $\\hat{y} = Z w$, we can re-frame the optimization problem in terms of $\\hat{y}$. Let $\\hat{y}$ be a vector in the column space of $Z$, $\\mathcal{C}(Z)$. For any such $\\hat{y}$, the set of weights that produce it is an affine subspace $\\{w \\mid Zw = \\hat{y}\\}$. The problem can be expressed as:\n$$\n\\min_{\\hat{y} \\in \\mathcal{C}(Z)} \\left( \\frac{1}{2n} \\|y - \\hat{y}\\|_{2}^{2} + \\lambda \\min_{w: Zw=\\hat{y}} \\|w\\|_{1} \\right)\n$$\nLet's analyze the new objective function in terms of $\\hat{y}$. The term $f(\\hat{y}) = \\frac{1}{2n} \\|y - \\hat{y}\\|_{2}^{2}$ is a strictly convex function of $\\hat{y}$. The term $g(\\hat{y}) = \\min_{w: Zw=\\hat{y}} \\|w\\|_{1}$ is a convex function of $\\hat{y}$ (it can be shown to be a norm on $\\mathcal{C}(Z)$). The sum of a strictly convex function ($f(\\hat{y})$) and a convex function ($\\lambda g(\\hat{y})$) is strictly convex. Therefore, the optimization problem for $\\hat{y}$ has a unique solution. The prediction $\\hat{y}^{\\star}$ is **unique**.\n\nPart 3: Dimension of the set of equivalent weights.\n\nFrom Part 1, the set of all weight vectors that produce the same predictions as a reference vector $w_{0}$ is the affine subspace $\\mathcal{W}(w_{0}) = w_{0} + \\mathcal{N}(Z)$. The dimension of an affine subspace is defined as the dimension of the corresponding linear subspace, which in this case is the null space $\\mathcal{N}(Z)$.\nWe need to find $\\operatorname{dim}(\\mathcal{N}(Z))$. The problem directs us to use the Rank–Nullity Theorem. For any matrix $Z \\in \\mathbb{R}^{n \\times M}$, the theorem states:\n$$\n\\operatorname{rank}(Z) + \\operatorname{nullity}(Z) = M\n$$\nwhere $\\operatorname{rank}(Z)$ is the dimension of the column space (and row space) and $\\operatorname{nullity}(Z) = \\operatorname{dim}(\\mathcal{N}(Z))$ is the dimension of the null space.\nWe are given that $\\operatorname{rank}(Z) = r$ and the number of columns of $Z$ (which corresponds to the dimension of the domain of the linear map) is $M$. Substituting these values into the theorem:\n$$\nr + \\operatorname{dim}(\\mathcal{N}(Z)) = M\n$$\nSolving for the dimension of the null space, we get:\n$$\n\\operatorname{dim}(\\mathcal{N}(Z)) = M - r\n$$\nTherefore, the dimension of the set $\\mathcal{W}(w_{0})$ is $M - r$.",
            "answer": "$$\n\\boxed{M - r}\n$$"
        },
        {
            "introduction": "This advanced practice challenges you to build a more sophisticated and robust stacking ensemble for regression. The standard approach implicitly uses an Ordinary Least Squares (OLS) meta-learner, which is only optimal if the error variance is constant (homoskedastic). In this exercise, you will address the common real-world scenario of heteroskedasticity by implementing a Weighted Least Squares (WLS) meta-learner, thereby improving the ensemble's performance by giving more influence to more reliable predictions .",
            "id": "3175509",
            "problem": "Implement a stacking ensemble with a meta-learner trained by Weighted Least Squares (WLS) using inverse variance weights proportional to $1/\\hat{\\sigma}^2(x)$ on synthetic heteroskedastic data, and evaluate it against an unweighted meta-learner trained by Ordinary Least Squares (OLS). Your program must be fully deterministic and produce a single line of output containing a list of booleans as specified below.\n\nBase setting and fundamental definitions to use:\n- A stacking ensemble combines $M$ base predictors. Given a training set $\\{(x_i, y_i)\\}_{i=1}^n$, define the level-$1$ design matrix $Z \\in \\mathbb{R}^{n \\times M}$ whose columns are the out-of-fold predictions of the $M$ base learners. The meta-learner is a linear model that predicts $\\hat{y} = \\beta_0 + \\sum_{j=1}^M \\beta_j z_{j}$.\n- Ordinary Least Squares (OLS) estimates the meta-parameters $\\beta$ by minimizing $\\sum_{i=1}^n (y_i - \\beta_0 - z_i^\\top \\beta)^2$.\n- Under independent Gaussian noise with observation-specific variance $\\sigma_i^2$, Maximum Likelihood Estimation leads to Weighted Least Squares (WLS), which minimizes $\\sum_{i=1}^n w_i (y_i - \\beta_0 - z_i^\\top \\beta)^2$, where $w_i \\propto 1/\\sigma_i^2$.\n- To avoid target leakage in stacking, the level-$1$ matrix $Z$ must be built from out-of-fold predictions obtained via $K$-fold Cross-Validation (CV), where $K \\in \\mathbb{N}$ and $K \\ge 2$.\n\nSynthetic data generation to be used:\n- Input features $x$ are sampled independently from a uniform distribution on $[-3, 3]$.\n- The mean function is $f(x) = 1.5 \\sin(1.2 x) + 0.3 x$.\n- The heteroskedastic noise standard deviation is $\\sigma(x) = 0.3 + 0.7 \\lvert x \\rvert / 3$, and the homoskedastic case uses $\\sigma(x) \\equiv 0.6$. Responses are $y = f(x) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2(x))$.\n- Base learners for stacking:\n  - Base $1$: linear regression on features $[1, x]$.\n  - Base $2$: cubic regression on features $[1, x, x^2, x^3]$.\n- Build out-of-fold predictions for the meta-learner using $K = 5$ folds.\n\nVariance model for WLS weights:\n- Estimate the conditional variance via a log-linear model. First, fit a mean model $\\tilde{f}(x)$ as a cubic polynomial on the full training data and compute residuals $r_i = y_i - \\tilde{f}(x_i)$. Then regress $\\log(r_i^2 + \\epsilon)$ on features $[1, \\lvert x_i \\rvert]$ using OLS to obtain coefficients $\\gamma$. The estimated variance is $\\hat{\\sigma}^2(x) = \\exp(\\gamma_0 + \\gamma_1 \\lvert x \\rvert)$, with a floor $v_{\\min} > 0$ applied as $\\hat{\\sigma}^2(x) \\leftarrow \\max(\\hat{\\sigma}^2(x), v_{\\min})$. Use $\\epsilon = 10^{-8}$ and $v_{\\min} = 10^{-6}$ unless stated otherwise in a test.\n- Weights for WLS are proportional to $1/\\hat{\\sigma}^2(x)$; do not normalize across samples, since scaling all weights by a common positive scalar leaves the WLS solution unchanged.\n\nEvaluation protocol:\n- For each test case below, generate an independent training and test set with the specified random seed. Training size is $n_{\\text{train}}$ and test size is $n_{\\text{test}}$. Use the stacking procedure described to produce test predictions via:\n  - OLS meta-learner trained on out-of-fold level-$1$ training predictions.\n  - WLS meta-learner trained with either oracle weights $w_i \\propto 1/\\sigma^2(x_i)$ or estimated weights $w_i \\propto 1/\\hat{\\sigma}^2(x_i)$ as specified.\n- Report test Mean Squared Error (MSE) averaged over the test set, $ \\text{MSE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_i - \\hat{y}_i)^2$.\n\nTest suite to implement and the required booleans to output:\n- Test $1$ (heteroskedastic, oracle and estimated weights):\n  - Parameters: $n_{\\text{train}} = 800$, $n_{\\text{test}} = 5000$, seed = 123.\n  - Data: heteroskedastic.\n  - Compute three MSEs on the test set: $\\text{MSE}_{\\text{OLS}}$, $\\text{MSE}_{\\text{WLS-oracle}}$ (using $w_i \\propto 1/\\sigma^2(x_i)$ with known $\\sigma(x)$), and $\\text{MSE}_{\\text{WLS-est}}$ (using $w_i \\propto 1/\\hat{\\sigma}^2(x_i)$).\n  - Output two booleans:\n    - $b_1$: whether $\\text{MSE}_{\\text{WLS-oracle}} < \\text{MSE}_{\\text{OLS}}$.\n    - $b_2$: whether $\\text{MSE}_{\\text{WLS-est}} < \\text{MSE}_{\\text{OLS}}$.\n- Test $2$ (homoskedastic, estimated weights approximate constant):\n  - Parameters: $n_{\\text{train}} = 3000$, $n_{\\text{test}} = 5000$, seed = 456.\n  - Data: homoskedastic with $\\sigma(x) \\equiv 0.6$.\n  - Compute $\\text{MSE}_{\\text{OLS}}$ and $\\text{MSE}_{\\text{WLS-est}}$.\n  - Output one boolean:\n    - $b_3$: whether $\\lvert \\text{MSE}_{\\text{WLS-est}} - \\text{MSE}_{\\text{OLS}} \\rvert \\le \\tau$, with tolerance $\\tau = 10^{-2}$.\n- Test $3$ (stability under extreme but positive weights):\n  - Parameters: $n_{\\text{train}} = 800$, $n_{\\text{test}} = 5000$, seed = 321.\n  - Data: heteroskedastic.\n  - Modify the estimated variance before inversion by multiplying it by $c = 10^{-6}$, and use the very small floor $v_{\\min} = 10^{-12}$ when computing $1/\\hat{\\sigma}^2(x)$. Train the WLS meta-learner with these weights and compute the test MSE.\n  - Output one boolean:\n    - $b_4$: whether all fitted meta-parameters and the resulting test MSE are finite real numbers.\n- Test $4$ (misspecified weights harm performance):\n  - Parameters: $n_{\\text{train}} = 800$, $n_{\\text{test}} = 5000$, seed = 789.\n  - Data: heteroskedastic.\n  - Construct misspecified weights proportional to $\\hat{\\sigma}^2(x)$ (instead of its reciprocal) and train the WLS meta-learner using these misspecified weights. Compute $\\text{MSE}_{\\text{WLS-bad}}$ and compare to $\\text{MSE}_{\\text{OLS}}$.\n  - Output one boolean:\n    - $b_5$: whether $\\text{MSE}_{\\text{OLS}} \\le \\text{MSE}_{\\text{WLS-bad}}$.\n- Test $5$ (invariance of WLS to global rescaling of weights):\n  - Use the same data generation as in Test $1$ (heteroskedastic, seed = 123, $n_{\\text{train}} = 800$, $n_{\\text{test}} = 5000$).\n  - Compute test predictions using oracle weights $w_i \\propto 1/\\sigma^2(x_i)$ and again with rescaled weights $w_i' = c \\cdot w_i$ with $c = 7$. Measure the maximum absolute difference between the two sets of test predictions.\n  - Output one boolean:\n    - $b_6$: whether the maximum absolute difference is less than $\\delta = 10^{-10}$.\n\nImplementation requirements:\n- Build out-of-fold level-$1$ predictions using $K = 5$ folds.\n- Use only linear algebra and basic numerical operations; do not use external machine learning libraries.\n- All random number generation must be controlled by the specified seeds for reproducibility.\n- Angles do not appear; there are no physical units.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[b_1, b_2, b_3, b_4, b_5, b_6]$.\n\nYour program must be a single, complete, runnable script that produces exactly one line of output of the specified format, with no additional text.",
            "solution": "The user wants to implement a stacking ensemble and evaluate the performance of a meta-learner trained with Weighted Least Squares (WLS) against one trained with Ordinary Least Squares (OLS). This analysis will be conducted on synthetic data exhibiting heteroskedasticity.\n\n### 1. Problem Validation\n\nThe problem statement has been meticulously reviewed according to the specified validation protocol.\n\n-   **Step 1: Givens Extraction**: All parameters, models, and procedures have been extracted verbatim. This includes the synthetic data generation process, the definitions of the base learners, the out-of-fold prediction methodology ($K=5$ cross-validation), the formulation of OLS and WLS meta-learners, the specific model for estimating observation variance, and the detailed suite of five tests with their corresponding evaluation criteria.\n-   **Step 2: Validation against Criteria**: The problem is determined to be valid.\n    -   It is **scientifically grounded** in the established principles of statistical learning, specifically ensemble methods and regression under non-constant variance.\n    -   It is **well-posed**, with all variables, functions, and procedures unambiguously defined, ensuring a unique and reproducible outcome for each test case given the specified random seeds.\n    -   It is **objective**, using precise mathematical language and quantitative evaluation metrics (Mean Squared Error), free of any subjective or opinion-based statements.\n-   **Step 3: Verdict**: The problem is **valid**. No flaws were identified. I will proceed with a full solution.\n\n### 2. Methodological Framework\n\nThe core of the problem is to compare two estimation strategies for a linear meta-learner within a stacking ensemble framework. The ensemble combines predictions from two base learners.\n\n**2.1. Data Generation**\n\nSynthetic data is generated to control the underlying true model and noise structure. For a set of $n$ samples:\n-   Features $x_i$ are drawn from a uniform distribution: $x_i \\sim U[-3, 3]$.\n-   The true underlying function is $f(x) = 1.5 \\sin(1.2 x) + 0.3 x$.\n-   The response $y_i$ is generated by adding Gaussian noise to the true function value: $y_i = f(x_i) + \\varepsilon_i$.\n-   The noise $\\varepsilon_i$ is drawn from $\\mathcal{N}(0, \\sigma^2(x_i))$, where the standard deviation $\\sigma(x)$ can be either:\n    -   **Heteroskedastic**: $\\sigma(x) = 0.3 + 0.7 \\lvert x \\rvert / 3$. The noise variance depends on the input $x$.\n    -   **Homoskedastic**: $\\sigma(x) \\equiv 0.6$. The noise variance is constant.\n\n**2.2. Stacking Ensemble Structure**\n\nThe ensemble consists of two levels: base learners (level-0) and a meta-learner (level-1).\n\n-   **Base Learners**:\n    -   Learner 1: A simple linear regression model, using features $[1, x]$.\n    -   Learner 2: A cubic polynomial regression model, using features $[1, x, x^2, x^3]$.\n\n-   **Level-1 Data Generation**: To prevent target leakage, the training data for the meta-learner is constructed using out-of-fold predictions. The training set of size $n_{\\text{train}}$ is split into $K=5$ folds. For each fold, the base learners are trained on the other $K-1$ folds and then used to make predictions on the held-out fold. The collection of these predictions for all $n_{\\text{train}}$ samples forms the level-1 design matrix $\\mathbf{Z} \\in \\mathbb{R}^{n_{\\text{train}} \\times 2}$. The $i$-th row of $\\mathbf{Z}$, denoted $\\mathbf{z}_i$, contains the out-of-fold predictions of the two base learners for the training sample $(x_i, y_i)$.\n\n**2.3. Meta-Learner**\n\nThe meta-learner is a linear model that combines the predictions from the base learners to produce the final prediction $\\hat{y}$. It includes an intercept term $\\beta_0$:\n$$\n\\hat{y}_i = \\beta_0 + \\beta_1 z_{i1} + \\beta_2 z_{i2} = \\mathbf{z}_{\\text{aug}, i}^T \\boldsymbol{\\beta}\n$$\nwhere $\\mathbf{z}_{\\text{aug}, i} = [1, z_{i1}, z_{i2}]^T$ is the augmented level-1 feature vector and $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^T$ is the vector of meta-parameters.\n\n-   **Ordinary Least Squares (OLS)**: This approach assumes homoskedastic errors and finds $\\boldsymbol{\\beta}$ by minimizing the sum of squared residuals:\n    $$\n    \\text{RSS} = \\sum_{i=1}^{n_{\\text{train}}} (y_i - \\mathbf{z}_{\\text{aug}, i}^T \\boldsymbol{\\beta})^2\n    $$\n    The solution is given by the normal equations: $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{Z}_{\\text{aug}}^T \\mathbf{Z}_{\\text{aug}})^{-1} \\mathbf{Z}_{\\text{aug}}^T \\mathbf{y}$.\n\n-   **Weighted Least Squares (WLS)**: This approach is optimal when errors are independent but have non-constant variance (heteroskedasticity). It finds $\\boldsymbol{\\beta}$ by minimizing the weighted sum of squared residuals:\n    $$\n    \\text{WRSS} = \\sum_{i=1}^{n_{\\text{train}}} w_i (y_i - \\mathbf{z}_{\\text{aug}, i}^T \\boldsymbol{\\beta})^2\n    $$\n    where weights $w_i$ are ideally inversely proportional to the error variance, $w_i \\propto 1/\\sigma_i^2$. The solution is given by the weighted normal equations: $\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}} = (\\mathbf{Z}_{\\text{aug}}^T \\mathbf{W} \\mathbf{Z}_{\\text{aug}})^{-1} \\mathbf{Z}_{\\text{aug}}^T \\mathbf{W} \\mathbf{y}$, where $\\mathbf{W}$ is a diagonal matrix with diagonal entries $W_{ii} = w_i$.\n\n**2.4. WLS Weight Estimation**\n\nSince the true variance $\\sigma^2(x)$ is typically unknown in practice, it must be estimated. The protocol specifies a log-linear model for the variance:\n1.  Fit a cubic polynomial model $\\tilde{f}(x)$ to the full training data $(x_i, y_i)$ to get an estimate of the conditional mean.\n2.  Compute the squared residuals $r_i^2 = (y_i - \\tilde{f}(x_i))^2$.\n3.  Model the logarithm of the squared residuals. Fit a linear model to predict $\\log(r_i^2 + \\epsilon)$ from features $[1, \\lvert x_i \\rvert]$, where $\\epsilon=10^{-8}$ ensures the argument of the logarithm is positive. This yields coefficients $\\boldsymbol{\\gamma} = [\\gamma_0, \\gamma_1]^T$.\n4.  The estimated variance is then $\\hat{\\sigma}^2(x) = \\exp(\\gamma_0 + \\gamma_1 \\lvert x \\rvert)$.\n5.  A floor $v_{\\min} = 10^{-6}$ is applied to prevent extremely small variance estimates: $\\hat{\\sigma}^2(x) \\leftarrow \\max(\\hat{\\sigma}^2(x), v_{\\min})$.\n6.  The estimated weights for WLS are $w_i = 1/\\hat{\\sigma}^2(x_i)$.\n\n**2.5. Evaluation**\n\nFor each test case, an independent test set of size $n_{\\text{test}}$ is generated. The base learners, trained on the *full* training set, are used to generate the test level-1 matrix $\\mathbf{Z}_{\\text{test}}$. The OLS and WLS meta-learners, trained on the training level-1 data, are then used to make final predictions on $\\mathbf{Z}_{\\text{test}}$. Performance is measured by the Mean Squared Error (MSE) on the test set:\n$$\n\\text{MSE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{test}, i} - \\hat{y}_{\\text{test}, i})^2\n$$\nThe specific numerical comparisons outlined in the five tests are then performed to generate the required boolean outputs. This procedure robustly tests the efficacy of WLS under various conditions, including ideal (oracle weights), practical (estimated weights), and pathological (misspecified weights) scenarios.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the suite of tests for stacking with WLS meta-learner.\n    \"\"\"\n\n    def fit_linear_model(X, y, weights=None):\n        \"\"\"\n        Fits a linear model using OLS or WLS.\n        \n        Args:\n            X (np.ndarray): Design matrix.\n            y (np.ndarray): Target vector.\n            weights (np.ndarray, optional): Weights for WLS. If None, performs OLS.\n        \n        Returns:\n            np.ndarray: Fitted coefficients.\n        \"\"\"\n        if weights is None:\n            # OLS\n            coeffs = np.linalg.lstsq(X, y, rcond=None)[0]\n        else:\n            # WLS: transform to an equivalent OLS problem\n            sqrt_w = np.sqrt(weights)\n            X_w = sqrt_w[:, np.newaxis] * X\n            y_w = sqrt_w * y\n            coeffs = np.linalg.lstsq(X_w, y_w, rcond=None)[0]\n        return coeffs\n\n    def predict_linear_model(X, coeffs):\n        \"\"\"Predicts using a fitted linear model.\"\"\"\n        return X @ coeffs\n\n    def create_design_matrix(x, degree):\n        \"\"\"Creates a polynomial design matrix [1, x, x^2, ..., x^degree].\"\"\"\n        return np.vander(x, N=degree + 1, increasing=True)\n\n    def generate_data(n_samples, seed, heteroskedastic):\n        \"\"\"Generates synthetic data based on the problem specification.\"\"\"\n        rng = np.random.default_rng(seed)\n        x = rng.uniform(-3, 3, n_samples)\n        f_x = 1.5 * np.sin(1.2 * x) + 0.3 * x\n        \n        if heteroskedastic:\n            sigma_x = 0.3 + 0.7 * np.abs(x) / 3\n        else:\n            sigma_x = np.full_like(x, 0.6)\n            \n        epsilon = rng.normal(0, sigma_x, n_samples)\n        y = f_x + epsilon\n        return x, y, sigma_x\n\n    def get_out_of_fold_predictions(x_train, y_train, K):\n        \"\"\"\n        Generates level-1 data (out-of-fold predictions) using K-fold CV.\n        \n        Returns:\n            np.ndarray: Level-1 design matrix Z.\n        \"\"\"\n        n_train = len(y_train)\n        Z_train = np.zeros((n_train, 2))\n        \n        # Use deterministic fold splits\n        fold_indices = np.array_split(np.arange(n_train), K)\n\n        for k in range(K):\n            val_idx = fold_indices[k]\n            train_idx = np.concatenate([fold_indices[j] for j in range(K) if j != k])\n\n            x_fold_train, y_fold_train = x_train[train_idx], y_train[train_idx]\n            x_fold_val = x_train[val_idx]\n\n            # Base Learner 1: Linear\n            X1_fold_train = create_design_matrix(x_fold_train, 1)\n            coeffs1 = fit_linear_model(X1_fold_train, y_fold_train)\n            X1_fold_val = create_design_matrix(x_fold_val, 1)\n            Z_train[val_idx, 0] = predict_linear_model(X1_fold_val, coeffs1)\n\n            # Base Learner 2: Cubic\n            X2_fold_train = create_design_matrix(x_fold_train, 3)\n            coeffs2 = fit_linear_model(X2_fold_train, y_fold_train)\n            X2_fold_val = create_design_matrix(x_fold_val, 3)\n            Z_train[val_idx, 1] = predict_linear_model(X2_fold_val, coeffs2)\n            \n        return Z_train\n\n    def estimate_variance_params(x_train, y_train, epsilon):\n        \"\"\"\n        Estimates the parameters of the log-linear variance model.\n        \n        Returns:\n            np.ndarray: Coefficients gamma.\n        \"\"\"\n        # 1. Fit mean model (cubic poly)\n        X_mean = create_design_matrix(x_train, 3)\n        mean_coeffs = fit_linear_model(X_mean, y_train)\n        y_pred_mean = predict_linear_model(X_mean, mean_coeffs)\n        \n        # 2. Compute residuals\n        residuals = y_train - y_pred_mean\n        \n        # 3. Regress log(r^2 + eps) on [1, |x|]\n        log_sq_res = np.log(residuals**2 + epsilon)\n        X_var = create_design_matrix(np.abs(x_train), 1)\n        gamma = fit_linear_model(X_var, log_sq_res)\n        \n        return gamma\n\n    def get_estimated_variance(x, gamma, v_min):\n        \"\"\"Calculates estimated variance for given x and gamma.\"\"\"\n        X_var = create_design_matrix(np.abs(x), 1)\n        log_var = predict_linear_model(X_var, gamma)\n        var_est = np.exp(log_var)\n        return np.maximum(var_est, v_min)\n\n\n    results = []\n\n    # --- Test 1 & 5: Heteroskedastic, Oracle vs. Estimated Weights ---\n    n_train, n_test, seed = 800, 5000, 123\n    x_train, y_train, sigma_train = generate_data(n_train, seed, heteroskedastic=True)\n    x_test, y_test, sigma_test = generate_data(n_test, seed + 1, heteroskedastic=True)\n\n    # Level-1 data\n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n\n    # Base learners trained on full training data for test predictions\n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    \n    Z_test = np.zeros((n_test, 2))\n    Z_test[:, 0] = predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full)\n    Z_test[:, 1] = predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n\n    # OLS meta-learner\n    beta_ols = fit_linear_model(Z_train_aug, y_train)\n    y_pred_ols = predict_linear_model(Z_test_aug, beta_ols)\n    mse_ols = np.mean((y_test - y_pred_ols)**2)\n\n    # WLS with Oracle weights\n    weights_oracle = 1 / sigma_train**2\n    beta_wls_oracle = fit_linear_model(Z_train_aug, y_train, weights=weights_oracle)\n    y_pred_wls_oracle = predict_linear_model(Z_test_aug, beta_wls_oracle)\n    mse_wls_oracle = np.mean((y_test - y_pred_wls_oracle)**2)\n    \n    # WLS with Estimated weights\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-6)\n    weights_est = 1 / var_est_train\n    beta_wls_est = fit_linear_model(Z_train_aug, y_train, weights=weights_est)\n    y_pred_wls_est = predict_linear_model(Z_test_aug, beta_wls_est)\n    mse_wls_est = np.mean((y_test - y_pred_wls_est)**2)\n\n    b1 = mse_wls_oracle  mse_ols\n    b2 = mse_wls_est  mse_ols\n    results.extend([b1, b2])\n\n    # --- Test 5: Invariance to weight scaling ---\n    weights_oracle_scaled = 7.0 * weights_oracle\n    beta_wls_oracle_scaled = fit_linear_model(Z_train_aug, y_train, weights=weights_oracle_scaled)\n    y_pred_wls_oracle_scaled = predict_linear_model(Z_test_aug, beta_wls_oracle_scaled)\n    max_diff_test5 = np.max(np.abs(y_pred_wls_oracle - y_pred_wls_oracle_scaled))\n    b6 = max_diff_test5  1e-10\n\n    # --- Test 2: Homoskedastic case ---\n    n_train, n_test, seed = 3000, 5000, 456\n    x_train, y_train, _ = generate_data(n_train, seed, heteroskedastic=False)\n    x_test, y_test, _ = generate_data(n_test, seed + 1, heteroskedastic=False)\n    \n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n    \n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    Z_test = np.c_[\n        predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full),\n        predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    ]\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n\n    beta_ols = fit_linear_model(Z_train_aug, y_train)\n    y_pred_ols = predict_linear_model(Z_test_aug, beta_ols)\n    mse_ols_t2 = np.mean((y_test - y_pred_ols)**2)\n\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-6)\n    weights_est = 1 / var_est_train\n    beta_wls_est = fit_linear_model(Z_train_aug, y_train, weights=weights_est)\n    y_pred_wls_est = predict_linear_model(Z_test_aug, beta_wls_est)\n    mse_wls_est_t2 = np.mean((y_test - y_pred_wls_est)**2)\n\n    b3 = np.abs(mse_wls_est_t2 - mse_ols_t2) = 1e-2\n    results.append(b3)\n\n    # --- Test 3: Stability under extreme weights ---\n    n_train, n_test, seed = 800, 5000, 321\n    x_train, y_train, _ = generate_data(n_train, seed, heteroskedastic=True)\n    x_test, y_test, _ = generate_data(n_test, seed + 1, heteroskedastic=True)\n\n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-12)\n    \n    # Modify variance and weights\n    var_est_mod = var_est_train * 1e-6\n    weights_extreme = 1 / var_est_mod\n    \n    beta_wls_extreme = fit_linear_model(Z_train_aug, y_train, weights=weights_extreme)\n    \n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    Z_test = np.c_[\n        predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full),\n        predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    ]\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n\n    y_pred_wls_extreme = predict_linear_model(Z_test_aug, beta_wls_extreme)\n    mse_wls_extreme = np.mean((y_test - y_pred_wls_extreme)**2)\n\n    b4 = np.all(np.isfinite(beta_wls_extreme)) and np.isfinite(mse_wls_extreme)\n    results.append(b4)\n\n    # --- Test 4: Misspecified weights ---\n    n_train, n_test, seed = 800, 5000, 789\n    x_train, y_train, _ = generate_data(n_train, seed, heteroskedastic=True)\n    x_test, y_test, _ = generate_data(n_test, seed + 1, heteroskedastic=True)\n    \n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n\n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    Z_test = np.c_[\n        predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full),\n        predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    ]\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n    \n    beta_ols = fit_linear_model(Z_train_aug, y_train)\n    y_pred_ols = predict_linear_model(Z_test_aug, beta_ols)\n    mse_ols_t4 = np.mean((y_test - y_pred_ols)**2)\n\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-6)\n    weights_bad = var_est_train  # Proportional to variance, not inverse\n    \n    beta_wls_bad = fit_linear_model(Z_train_aug, y_train, weights=weights_bad)\n    y_pred_wls_bad = predict_linear_model(Z_test_aug, beta_wls_bad)\n    mse_wls_bad = np.mean((y_test - y_pred_wls_bad)**2)\n    \n    b5 = mse_ols_t4 = mse_wls_bad\n    results.append(b5)\n    \n    # Add Test 5 result at the end\n    results.append(b6)\n    \n    # Final output formatting: [b1, b2, b3, b4, b5, b6]\n    print(f\"[{','.join(map(str, [r.lower() if isinstance(r, bool) else r for r in results]))}]\")\n\nsolve()\n```"
        }
    ]
}