{
    "hands_on_practices": [
        {
            "introduction": "本节的第一个实践是一项基础的编程练习。它将指导你为一个概率分类问题构建一个堆叠集成模型。核心要点在于理解元学习器损失函数的选择（例如布里尔分数或对数损失等严格固有评分规则）如何影响最终的权重，并进而影响模型的预测结果 。",
            "id": "3175480",
            "problem": "给定一个二元概率分类的堆叠（stacking）任务。有 $M$ 个基础概率分类器，它们为 $N$ 个已标记的样本生成预测概率。一个堆叠预测器构成了基础预测的一个凸组合（convex mixture）。设 $y_i \\in \\{0,1\\}$ 表示样本 $i \\in \\{1,\\dots,N\\}$ 的真实标签，设 $p_{ij} \\in [0,1]$ 表示基础模型 $j \\in \\{1,\\dots,M\\}$ 对样本 $i$ 的预测概率。设 $w = (w_1,\\dots,w_M)$ 为堆叠权重。样本 $i$ 的混合预测概率定义为\n$$\n\\hat{p}_i(w) = \\sum_{j=1}^M w_j\\, p_{ij},\n$$\n满足以下凸性约束条件\n$$\nw_j \\ge 0 \\text{ for all } j \\in \\{1,\\dots,M\\}, \\quad \\sum_{j=1}^M w_j = 1.\n$$\n\n将使用两种严格正常评分规则（strictly proper scoring rules）来评估预测：\n1. 对数损失（logarithmic loss），也称为伯努利分布结果的负对数似然（negative log-likelihood），\n$$\n\\ell_{\\mathrm{log}}(y_i,\\hat{p}_i) = -\\left[y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i)\\right],\n$$\n约定为避免 $\\log(0)$，在计算对数之前，我们会对 $\\hat{p}_i$ 应用一个固定的 $\\epsilon = 10^{-15}$ 进行裁剪：$\\hat{p}_i \\leftarrow \\min\\{\\max\\{\\hat{p}_i,\\epsilon\\},1-\\epsilon\\}$。\n2. Brier 分数，\n$$\n\\ell_{\\mathrm{brier}}(y_i,\\hat{p}_i) = \\left(y_i - \\hat{p}_i\\right)^2.\n$$\n\n您的任务是为每个给定的测试用例，计算在每种评分规则下使经验风险最小化的堆叠权重向量 $w$，即\n$$\n\\min_{w \\in \\mathbb{R}^M} \\frac{1}{N}\\sum_{i=1}^N \\ell_{\\mathrm{log}}(y_i,\\hat{p}_i(w)) \\quad \\text{subject to } w_j \\ge 0, \\ \\sum_{j=1}^M w_j = 1,\n$$\n和\n$$\n\\min_{w \\in \\mathbb{R}^M} \\frac{1}{N}\\sum_{i=1}^N \\ell_{\\mathrm{brier}}(y_i,\\hat{p}_i(w)) \\quad \\text{subject to } w_j \\ge 0, \\ \\sum_{j=1}^M w_j = 1.\n$$\n然后，通过报告 $L_1$ 差异 $\\|w^{\\mathrm{brier}} - w^{\\mathrm{log}}\\|_1 = \\sum_{j=1}^M |w^{\\mathrm{brier}}_j - w^{\\mathrm{log}}_j|$ 来比较得到的权重向量。\n\n从经验风险最小化、概率单纯形上的凸组合以及上述两种严格正常评分规则的基本定义出发。基于这些基础推导出必要的最优性梯度或条件，并实现一个满足约束条件的数值稳定求解器。\n\n请使用以下固定的测试套件。每个案例指定了 $N$、$M$、标签 $y$ 以及基础模型预测矩阵 $P$，其第 $i$ 行为 $(p_{i1},\\dots,p_{iM})$。在每个案例中，模型的排序为 $m_1, m_2, m_3$。\n\n测试用例 1（一般情况，$N=8$，$M=3$）：\n- $y = [0,1,0,1,0,1,0,1]$。\n- $m_1$ 预测值：$[0.3,0.7,0.4,0.6,0.3,0.65,0.35,0.7]$。\n- $m_2$ 预测值：$[0.2,0.8,0.3,0.7,0.4,0.5,0.2,0.85]$。\n- $m_3$ 预测值：$[0.1,0.9,0.2,0.8,0.2,0.8,0.3,0.7]$。\n\n测试用例 2（两个相同模型，$N=4$，$M=3$）：\n- $y = [0,0,1,1]$。\n- $m_1$ 预测值：$[0.1,0.2,0.8,0.9]$。\n- $m_2$ 预测值：$[0.1,0.2,0.8,0.9]$。\n- $m_3$ 预测值：$[0.2,0.2,0.7,0.85]$。\n\n测试用例 3（存在近乎完美的模型，$N=6$，$M=3$）：\n- $y = [1,0,1,0,1,0]$。\n- $m_1$ 预测值：$[0.6,0.4,0.55,0.45,0.65,0.35]$。\n- $m_2$ 预测值：$[0.3,0.7,0.4,0.6,0.35,0.65]$。\n- $m_3$ 预测值：$[0.99,0.01,0.98,0.02,0.97,0.03]$。\n\n测试用例 4（测试稳定性的概率极值，$N=4$，$M=3$）：\n- $y = [1,1,0,0]$。\n- $m_1$ 预测值：$[0.0,0.1,1.0,0.9]$。\n- $m_2$ 预测值：$[0.2,0.0,0.8,1.0]$。\n- $m_3$ 预测值：$[0.05,0.05,0.95,0.95]$。\n\n实现细节和要求：\n- 对于对数损失，在目标函数及其导数中对 $\\hat{p}_i(w)$ 应用 $\\epsilon = 10^{-15}$ 的裁剪，以确保数值稳定性。\n- 在您的优化程序中，严格执行约束条件 $w_j \\ge 0$ 和 $\\sum_{j=1}^M w_j = 1$。\n- 对每个测试用例，返回三项内容：Brier 最优权重向量 $w^{\\mathrm{brier}}$、对数损失最优权重向量 $w^{\\mathrm{log}}$ 以及它们之间的 $L_1$ 差异。\n- 将所有报告的实数四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的结果，格式为无空格、逗号分隔的列表的列表。对于每个测试用例，输出一个包含三项的列表：$M$ 个 Brier 权重的列表、$M$ 个对数损失权重的列表以及标量 $L_1$ 差异。例如，一个有效的整体格式是\n$[[[w_{1,1},w_{1,2},w_{1,3}],[v_{1,1},v_{1,2},v_{1,3}],d_1],[[w_{2,1},w_{2,2},w_{2,3}],[v_{2,1},v_{2,2},v_{2,3}],d_2],[[w_{3,1},w_{3,2},w_{3,3}],[v_{3,1},v_{3,2},v_{3,3}],d_3],[[w_{4,1},w_{4,2},w_{4,3}],[v_{4,1},v_{4,2},v_{4,3}],d_4]]$,\n其中每个 $w_{k,j}$、$v_{k,j}$ 和 $d_k$ 都精确打印到 $6$ 位小数。",
            "solution": "该问题要求为一个由 $M$ 个概率分类器组成的堆叠（stacking）集成寻找最优权重向量 $w \\in \\mathbb{R}^M$。最优权重是指在两种不同的评分规则（Brier 分数和对数损失）下，使在 $N$ 个样本上评估的经验风险最小化的权重。权重向量必须满足凸性约束条件 $w_j \\ge 0$（对于所有 $j \\in \\{1,\\dots,M\\}$）以及 $\\sum_{j=1}^M w_j = 1$。这组约束定义了标准的 $(M-1)$-单纯形（simplex），这是一个紧凸集，记为 $\\Delta^{M-1}$。\n\n设 $y \\in \\{0,1\\}^N$ 为真实标签向量，设 $P$ 为 $N \\times M$ 矩阵，其中 $P_{ij} = p_{ij}$ 是基础模型 $j$ 对样本 $i$ 的预测概率。样本 $i$ 的堆叠预测是基础预测的凸组合：\n$$\n\\hat{p}_i(w) = \\sum_{j=1}^M w_j P_{ij}\n$$\n用向量表示法，所有 $N$ 个样本的堆叠预测向量为 $\\hat{p}(w) = Pw$。\n\n任务是解决两个独立的约束优化问题。\n\n### 1. Brier 分数最小化\n\n单个预测的 Brier 分数为 $\\ell_{\\mathrm{brier}}(y_i, \\hat{p}_i) = (y_i - \\hat{p}_i)^2$。经验风险是所有样本的平均 Brier 分数：\n$$\nR_{\\mathrm{brier}}(w) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{p}_i(w))^2 = \\frac{1}{N} \\|y - Pw\\|_2^2\n$$\n优化问题是：\n$$\n\\min_{w \\in \\Delta^{M-1}} R_{\\mathrm{brier}}(w)\n$$\n这是一个凸优化问题。为了使用基于梯度的方法求解，我们首先展开目标函数：\n$$\nR_{\\mathrm{brier}}(w) = \\frac{1}{N} (y - Pw)^T(y - Pw) = \\frac{1}{N} (y^Ty - 2y^TPw + w^TP^TPw)\n$$\n这是一个关于 $w$ 的二次函数。这类特定问题被称为二次规划（Quadratic Program, QP）。优化算法需要目标函数关于 $w$ 的梯度。利用向量微积分的法则，我们求得梯度：\n$$\n\\nabla_w R_{\\mathrm{brier}}(w) = \\frac{1}{N} \\nabla_w (y^Ty - 2y^TPw + w^TP^TPw) = \\frac{1}{N} (-2P^Ty + 2P^TPw) = \\frac{2}{N} (P^TPw - P^Ty)\n$$\n这也可以用预测向量 $\\hat{p} = Pw$ 表示为 $\\nabla_w R_{\\mathrm{brier}}(w) = \\frac{2}{N} P^T(Pw - y) = -\\frac{2}{N} P^T(y - \\hat{p})$。\n\n### 2. 对数损失最小化\n\n单个预测的对数损失为 $\\ell_{\\mathrm{log}}(y_i, \\hat{p}_i) = -[y_i \\log(\\hat{p}_i) + (1-y_i)\\log(1-\\hat{p}_i)]$。经验风险是平均对数损失：\n$$\nR_{\\mathrm{log}}(w) = \\frac{1}{N} \\sum_{i=1}^N -\\left[y_i \\log(\\hat{p}_i(w)) + (1-y_i)\\log(1-\\hat{p}_i(w))\\right]\n$$\n为了数值稳定性，预测概率 $\\hat{p}_i$ 在传入对数函数之前被裁剪到 $[\\epsilon, 1-\\epsilon]$ 范围内，其中 $\\epsilon=10^{-15}$。令 $\\hat{p}_c(w) = \\text{clip}(\\hat{p}(w), \\epsilon, 1-\\epsilon)$。目标函数是 $R_{\\mathrm{log}}(w)$，其中 $\\hat{p}_i(w)$ 被替换为 $\\hat{p}_{ic}(w)$。这个函数是凸函数。优化问题是：\n$$\n\\min_{w \\in \\Delta^{M-1}} R_{\\mathrm{log}}(w)\n$$\n为求梯度，我们对每个分量 $w_k$ 求导：\n$$\n\\frac{\\partial R_{\\mathrm{log}}}{\\partial w_k} = \\frac{1}{N} \\sum_{i=1}^N -\\left[ \\frac{y_i}{\\hat{p}_{ic}} \\frac{\\partial \\hat{p}_{ic}}{\\partial w_k} - \\frac{1-y_i}{1-\\hat{p}_{ic}} \\frac{\\partial \\hat{p}_{ic}}{\\partial w_k} \\right]\n$$\n假设最优解对所有样本都位于裁剪范围的内部，则 $\\frac{\\partial \\hat{p}_{ic}}{\\partial w_k} = \\frac{\\partial \\hat{p}_i}{\\partial w_k} = P_{ik}$。导数变为：\n$$\n\\frac{\\partial R_{\\mathrm{log}}}{\\partial w_k} = -\\frac{1}{N} \\sum_{i=1}^N \\left( \\frac{y_i}{\\hat{p}_{ic}} - \\frac{1-y_i}{1-\\hat{p}_{ic}} \\right) P_{ik}\n$$\n以向量形式表示，梯度为：\n$$\n\\nabla_w R_{\\mathrm{log}}(w) = -\\frac{1}{N} P^T \\left( \\frac{y}{\\hat{p}_c} - \\frac{1-y}{1-\\hat{p}_c} \\right)\n$$\n其中除法是逐元素执行的。将 $\\hat{p}$ 裁剪为 $\\hat{p}_c$ 对于防止除以零并确保梯度有明确定义至关重要。\n\n### 3. 数值求解策略\n\n两个问题都是带线性约束的凸优化实例。一个适合的算法是序列最小二乘规划（Sequential Least Squares Programming, SLSQP），它可以处理等式和不等式约束。我们使用 `scipy.optimize.minimize` 中提供的实现。\n\n优化问题的配置如下：\n- **目标函数**：$R_{\\mathrm{brier}}(w)$ 或 $R_{\\mathrm{log}}(w)$。\n- **雅可比矩阵（梯度）**：$\\nabla_w R_{\\mathrm{brier}}(w)$ 或 $\\nabla_w R_{\\mathrm{log}}(w)$。\n- **约束条件**：\n    1. 等式约束：$\\sum_{j=1}^M w_j - 1 = 0$。\n    2. 不等式约束（边界）：$w_j \\ge 0$（对于所有 $j=1, \\dots, M$）。\n- **初始猜测**：一个均匀的权重向量，即对所有 $j$ 都有 $w_j = 1/M$，是一个合理的起点。\n\n这个设置可以实现对最优权重向量 $w^{\\mathrm{brier}}$ 和 $w^{\\mathrm{log}}$ 的数值稳定且高效的计算。最后一步是计算 $L_1$ 差异 $\\|w^{\\mathrm{brier}} - w^{\\mathrm{log}}\\|_1 = \\sum_{j=1}^M |w^{\\mathrm{brier}}_j - w^{\\mathrm{log}}_j|$。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the stacking optimization problems for all test cases.\n    \"\"\"\n    epsilon = 1e-15\n\n    test_cases = [\n        # Test case 1 (general case, N=8, M=3)\n        {\n            \"y\": np.array([0, 1, 0, 1, 0, 1, 0, 1]),\n            \"P\": np.array([\n                [0.3, 0.2, 0.1],\n                [0.7, 0.8, 0.9],\n                [0.4, 0.3, 0.2],\n                [0.6, 0.7, 0.8],\n                [0.3, 0.4, 0.2],\n                [0.65, 0.5, 0.8],\n                [0.35, 0.2, 0.3],\n                [0.7, 0.85, 0.7]\n            ])\n        },\n        # Test case 2 (two identical models, N=4, M=3)\n        {\n            \"y\": np.array([0, 0, 1, 1]),\n            \"P\": np.array([\n                [0.1, 0.1, 0.2],\n                [0.2, 0.2, 0.2],\n                [0.8, 0.8, 0.7],\n                [0.9, 0.9, 0.85]\n            ])\n        },\n        # Test case 3 (near-perfect model present, N=6, M=3)\n        {\n            \"y\": np.array([1, 0, 1, 0, 1, 0]),\n            \"P\": np.array([\n                [0.6, 0.3, 0.99],\n                [0.4, 0.7, 0.01],\n                [0.55, 0.4, 0.98],\n                [0.45, 0.6, 0.02],\n                [0.65, 0.35, 0.97],\n                [0.35, 0.65, 0.03]\n            ])\n        },\n        # Test case 4 (probability extremes, N=4, M=3)\n        {\n            \"y\": np.array([1, 1, 0, 0]),\n            \"P\": np.array([\n                [0.0, 0.2, 0.05],\n                [0.1, 0.0, 0.05],\n                [1.0, 0.8, 0.95],\n                [0.9, 1.0, 0.95]\n            ])\n        }\n    ]\n\n    def solve_weights(y, P, loss_type, epsilon):\n        \"\"\"\n        Solves for the optimal stacking weights for a given loss function.\n        \"\"\"\n        N, M = P.shape\n        w0 = np.full(M, 1 / M)\n\n        if loss_type == 'brier':\n            def objective(w):\n                phat = P @ w\n                return np.mean((y - phat)**2)\n\n            def jacobian(w):\n                phat = P @ w\n                return (2 / N) * P.T @ (phat - y)\n        elif loss_type == 'log':\n            def objective(w):\n                phat = P @ w\n                phat_clipped = np.clip(phat, epsilon, 1 - epsilon)\n                return -np.mean(y * np.log(phat_clipped) + (1 - y) * np.log(1 - phat_clipped))\n\n            def jacobian(w):\n                phat = P @ w\n                phat_clipped = np.clip(phat, epsilon, 1 - epsilon)\n                grad_term = y / phat_clipped - (1 - y) / (1 - phat_clipped)\n                return -(1 / N) * P.T @ grad_term\n        else:\n            raise ValueError(\"Invalid loss_type specified.\")\n\n        constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n        bounds = tuple((0, None) for _ in range(M))\n        \n        result = minimize(\n            fun=objective,\n            x0=w0,\n            method='SLSQP',\n            jac=jacobian,\n            bounds=bounds,\n            constraints=constraints,\n            tol=1e-12\n        )\n        # It's possible for weights to be very small negative numbers due to precision.\n        # Clip at 0 and re-normalize to strictly satisfy constraints.\n        w_opt = np.maximum(0, result.x)\n        w_opt /= np.sum(w_opt)\n        return w_opt\n\n    results = []\n    for case in test_cases:\n        y, P = case[\"y\"], case[\"P\"]\n        \n        w_brier = solve_weights(y, P, 'brier', epsilon)\n        w_log = solve_weights(y, P, 'log', epsilon)\n        \n        l1_diff = np.sum(np.abs(w_brier - w_log))\n        \n        # Format results to 6 decimal places as strings\n        w_brier_str = '[' + ','.join([f\"{x:.6f}\" for x in w_brier]) + ']'\n        w_log_str = '[' + ','.join([f\"{x:.6f}\" for x in w_log]) + ']'\n        l1_diff_str = f\"{l1_diff:.6f}\"\n        \n        case_result_str = f\"[{w_brier_str},{w_log_str},{l1_diff_str}]\"\n        results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    final_output = '[' + ','.join(results) + ']'\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "在学习了如何计算堆叠权重后，这个实践将探讨一个关键的理论问题：当基础模型之间存在相关性时会发生什么？本练习将运用线性代数知识，阐释为何权重可能变得不可唯一识别，并展示像岭回归(Ridge)和LASSO这样的正则化技术如何提供一种有原则的方法来获得唯一且稳定的解 。",
            "id": "3175491",
            "problem": "一个预测堆叠系统使用 $M$ 个基学习器。对于一个有 $n$ 个观测值的数据集，令 $Z \\in \\mathbb{R}^{n \\times M}$ 的各列收集基学习器的样本外预测，并令 $w \\in \\mathbb{R}^{M}$ 为堆叠权重。堆叠预测器是线性映射 $\\hat{y}(w) = Z w \\in \\mathbb{R}^{n}$。假设 $\\operatorname{rank}(Z) = r$ 且 $r  M$。\n\n仅从线性预测、矩阵的列空间和零空间、秩-零度定理、Moore–Penrose 伪逆以及凸优化的性质出发，回答以下问题：\n\n1. 推导 $\\hat{y}(w_{1}) = \\hat{y}(w_{2})$ 对 $w_{1}$ 和 $w_{2}$ 的一个充分必要条件。利用此条件，刻画出与给定参考权重 $w_{0}$ 产生相同预测的全部权重集合。你的刻画必须是显式的，并使用与 $Z$ 相关的基本线性代数对象进行参数化。\n\n2. 考虑带有平方误差和可分离正则化项的经验风险最小化问题，\n$$\n\\min_{w \\in \\mathbb{R}^{M}} \\; \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\, \\Omega(w),\n$$\n其中 $\\lambda \\ge 0$。对于以下每种选择，论证 $w$ 的可识别性以及预测 $\\hat{y} = Z w$ 的可识别性：\n   - (i) 无正则化：$\\lambda = 0$ 且 $\\Omega(w) \\equiv 0$。\n   - (ii) Ridge 正则化：$\\Omega(w) = \\|w\\|_{2}^{2}$ 且 $\\lambda  0$。\n   - (iii) 最小绝对收缩和选择算子 (LASSO)：$\\Omega(w) = \\|w\\|_{1}$ 且 $\\lambda  0$。\n在每种情况下，论证优化器 $w^{\\star}$ 是否唯一，预测 $\\hat{y}^{\\star} = Z w^{\\star}$ 是否唯一，以及解如何与由 $Z$ 导出的 $\\mathbb{R}^{M}$ 的线性代数分解相关联。\n\n3. 令 $\\mathcal{W}(w_{0})$ 表示与任意参考 $w_{0} \\in \\mathbb{R}^{M}$ 产生完全相同预测的权重向量全集。$\\mathcal{W}(w_{0})$ 的维度用 $M$ 和 $r$ 表示是什么？将此维度作为你的最终答案，以 $M$ 和 $r$ 的封闭形式表达式报告。\n\n本题无需数值四舍五入。按要求提供最终答案，不要包含任何单位。",
            "solution": "第 1 部分：等价权重的刻画。\n\n两个权重向量 $w_{1} \\in \\mathbb{R}^{M}$ 和 $w_{2} \\in \\mathbb{R}^{M}$ 产生相同的堆叠预测，当且仅当 $\\hat{y}(w_{1}) = \\hat{y}(w_{2})$。根据预测器的定义，此等式为：\n$$\nZ w_{1} = Z w_{2}\n$$\n整理各项，我们得到：\n$$\nZ w_{1} - Z w_{2} = 0\n$$\n利用矩阵乘法的分配律，这变为：\n$$\nZ (w_{1} - w_{2}) = 0\n$$\n该方程表明，向量差 $(w_{1} - w_{2})$ 被线性变换 $Z$ 映射到零向量。根据定义，所有这类向量的集合构成了矩阵 $Z$ 的零空间（或核），记为 $\\mathcal{N}(Z)$。因此，$w_{1}$ 和 $w_{2}$ 产生相同预测的充分必要条件是它们的差必须位于 $Z$ 的零空间中：\n$$\n(w_{1} - w_{2}) \\in \\mathcal{N}(Z)\n$$\n现在，我们来刻画与给定参考权重向量 $w_{0}$ 产生相同预测的全部权重集合，记为 $\\mathcal{W}(w_{0})$。一个向量 $w$ 属于 $\\mathcal{W}(w_{0})$ 当且仅当 $\\hat{y}(w) = \\hat{y}(w_{0})$。根据我们推导出的条件，这等价于：\n$$\n(w - w_{0}) \\in \\mathcal{N}(Z)\n$$\n这意味着存在某个向量 $v \\in \\mathcal{N}(Z)$ 使得 $w - w_{0} = v$。解出 $w$，我们发现 $\\mathcal{W}(w_{0})$ 的任何元素都可以表示为：\n$$\nw = w_{0} + v \\quad \\text{其中 } v \\in \\mathcal{N}(Z)\n$$\n这明确地将集合 $\\mathcal{W}(w_{0})$ 刻画为通过将 $Z$ 的零空间中的任何向量加到参考向量 $w_{0}$ 上而形成的所有向量的集合。这个集合是 $\\mathbb{R}^{M}$ 的一个仿射子空间，可以紧凑地写作：\n$$\n\\mathcal{W}(w_{0}) = w_{0} + \\mathcal{N}(Z) = \\{ w_{0} + v \\mid v \\in \\mathcal{N}(Z) \\}\n$$\n\n第 2 部分：正则化经验风险最小化的可识别性分析。\n\n一般目标函数为 $L(w) = \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\, \\Omega(w)$。第一项，即平方误差损失，是 $w$ 的一个凸函数。其 Hessian 矩阵为 $\\nabla_{w}^{2} \\left(\\frac{1}{2n} \\| y - Z w \\|_{2}^{2}\\right) = \\frac{1}{n} Z^{T} Z$。由于 $\\operatorname{rank}(Z) = r  M$，矩阵 $Z^{T} Z \\in \\mathbb{R}^{M \\times M}$ 的秩也为 $r$。这意味着 $Z^{T} Z$ 是半正定的，但不是正定的。其零空间与 $Z$ 的零空间相同，即 $\\mathcal{N}(Z^{T}Z) = \\mathcal{N}(Z)$，由于 $r  M$，这是一个非平凡的零空间。因此，平方误差损失项是凸的，但不是严格凸的。\n\n(i) 无正则化 ($\\lambda = 0$)：\n该问题是标准的普通最小二乘 (OLS) 问题：$\\min_{w} \\frac{1}{2n} \\| y - Z w \\|_{2}^{2}$。\n- $w^{\\star}$ 的唯一性：由于目标函数不是严格凸的，优化器 $w^{\\star}$ 不是唯一的。如果 $w^{\\star}$ 是一个最小化器，那么对于任何非零向量 $v \\in \\mathcal{N}(Z)$，向量 $w^{\\star} + v$ 也是一个最小化器。这是因为 $Z(w^{\\star} + v) = Z w^{\\star} + Zv = Z w^{\\star}$，所以目标函数的值保持不变。所有解的集合是仿射子空间 $w^{\\star}_{p} + \\mathcal{N}(Z)$，其中 $w^{\\star}_{p} = Z^{+}y$ 是由 Moore-Penrose 伪逆给出的特解。因此，$w^{\\star}$ **不是唯一的**。\n- $\\hat{y}^{\\star}$ 的唯一性：预测为 $\\hat{y}^{\\star} = Z w^{\\star}$。设 $w^{\\star}_{1}$ 和 $w^{\\star}_{2}$ 是两个不同的最优权重向量。从第 1 部分我们知道 $w^{\\star}_{2} - w^{\\star}_{1} \\in \\mathcal{N}(Z)$。因此，$Z w^{\\star}_{1} = Z w^{\\star}_{2}$。这表明所有最优权重向量都导致相同的预测向量。唯一的预测 $\\hat{y}^{\\star}$ 是目标向量 $y$ 在 $Z$ 的列空间 $\\mathcal{C}(Z)$ 上的正交投影。这可以写作 $\\hat{y}^{\\star} = Z Z^{+} y$。因此，预测 $\\hat{y}^{\\star}$ 是 **唯一的**。\n\n(ii) Ridge 正则化 ($\\Omega(w) = \\|w\\|_{2}^{2}$，$\\lambda  0$)：\n目标函数为 $L(w) = \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\|w\\|_{2}^{2}$。\n- $w^{\\star}$ 的唯一性：目标函数的 Hessian 矩阵为 $\\nabla^{2} L(w) = \\frac{1}{n} Z^{T} Z + 2 \\lambda I$。矩阵 $\\frac{1}{n} Z^{T} Z$ 是半正定的。由于 $\\lambda  0$，矩阵 $2 \\lambda I$ 是正定的。一个半正定矩阵和一个正定矩阵的和是正定的。Hessian 矩阵为正定的函数是严格凸函数。定义在 $\\mathbb{R}^{M}$ 上的严格凸函数至多有一个全局最小值。由于该函数也是强制的（即当 $\\|w\\|_{2} \\to \\infty$ 时，$L(w) \\to \\infty$），因此存在一个唯一的最小化器。因此，优化器 $w^{\\star}$ 是 **唯一的**。\n- $\\hat{y}^{\\star}$ 的唯一性：由于优化器 $w^{\\star}$ 是唯一的，因此得到的预测 $\\hat{y}^{\\star} = Z w^{\\star}$ 也必然是 **唯一的**。Ridge 惩罚项通过选择具有最小 L2-范数的解（在那些可能具有相似损失值的解中），有效地解决了由非平凡零空间 $\\mathcal{N}(Z)$ 引起的歧义。\n\n(iii) LASSO 正则化 ($\\Omega(w) = \\|w\\|_{1}$，$\\lambda  0$)：\n目标函数为 $L(w) = \\frac{1}{2n} \\| y - Z w \\|_{2}^{2} + \\lambda \\|w\\|_{1}$。\n- $w^{\\star}$ 的唯一性：目标函数是一个凸的平方误差项和一个凸的 L1-范数项之和。因此，该和是凸的。然而，无法保证任何一项是严格凸的，它们的和通常也不是严格凸的。条件 $r  M$ 意味着 $Z$ 的列之间存在线性依赖关系。这可能导致 $w^{\\star}$ 的解不唯一。例如，如果两列相同，$Z_{i} = Z_{j}$，那么损失项仅取决于和 $w_{i} + w_{j}$，而惩罚项为 $\\lambda (|w_{i}|+|w_{j}|)$。对于固定的和 $w_{i} + w_{j} = c$，当将 $w_i$ 或 $w_j$ 中的一个设为 $c$ 而另一个设为 $0$ 时，惩罚项最小化，这导致了多个解。由于 $\\operatorname{rank}(Z)  M$，总存在一个非零的 $v \\in \\mathcal{N}(Z)$，并且可以满足不唯一性的条件。因此，优化器 $w^{\\star}$ 通常 **不是唯一的**。\n- $\\hat{y}^{\\star}$ 的唯一性：为了分析预测 $\\hat{y} = Z w$ 的唯一性，我们可以根据 $\\hat{y}$ 重新构建优化问题。设 $\\hat{y}$ 是 $Z$ 的列空间 $\\mathcal{C}(Z)$ 中的一个向量。对于任何这样的 $\\hat{y}$，产生它的权重集合是一个仿射子空间 $\\{w \\mid Zw = \\hat{y}\\}$。该问题可以表示为：\n$$\n\\min_{\\hat{y} \\in \\mathcal{C}(Z)} \\left( \\frac{1}{2n} \\|y - \\hat{y}\\|_{2}^{2} + \\lambda \\min_{w: Zw=\\hat{y}} \\|w\\|_{1} \\right)\n$$\n我们来分析关于 $\\hat{y}$ 的新目标函数。项 $f(\\hat{y}) = \\frac{1}{2n} \\|y - \\hat{y}\\|_{2}^{2}$ 是 $\\hat{y}$ 的一个严格凸函数。项 $g(\\hat{y}) = \\min_{w: Zw=\\hat{y}} \\|w\\|_{1}$ 是 $\\hat{y}$ 的一个凸函数（可以证明它是 $\\mathcal{C}(Z)$ 上的一个范数）。一个严格凸函数（$f(\\hat{y})$）与一个凸函数（$\\lambda g(\\hat{y})$）的和是严格凸的。因此，关于 $\\hat{y}$ 的优化问题有唯一解。预测 $\\hat{y}^{\\star}$ 是 **唯一的**。\n\n第 3 部分：等价权重集合的维度。\n\n从第 1 部分可知，产生与参考向量 $w_{0}$ 相同预测的所有权重向量的集合是仿射子空间 $\\mathcal{W}(w_{0}) = w_{0} + \\mathcal{N}(Z)$。仿射子空间的维度定义为相应线性子空间的维度，在此情况下即为零空间 $\\mathcal{N}(Z)$ 的维度。\n我们需要求 $\\operatorname{dim}(\\mathcal{N}(Z))$。题目指导我们使用秩-零度定理。对于任何矩阵 $Z \\in \\mathbb{R}^{n \\times M}$，该定理陈述：\n$$\n\\operatorname{rank}(Z) + \\operatorname{nullity}(Z) = M\n$$\n其中 $\\operatorname{rank}(Z)$ 是列空间（和行空间）的维度，而 $\\operatorname{nullity}(Z) = \\operatorname{dim}(\\mathcal{N}(Z))$ 是零空间的维度。\n我们已知 $\\operatorname{rank}(Z) = r$，并且 $Z$ 的列数（对应于线性映射的定义域维度）为 $M$。将这些值代入定理中：\n$$\nr + \\operatorname{dim}(\\mathcal{N}(Z)) = M\n$$\n求解零空间的维度，我们得到：\n$$\n\\operatorname{dim}(\\mathcal{N}(Z)) = M - r\n$$\n因此，集合 $\\mathcal{W}(w_{0})$ 的维度是 $M - r$。",
            "answer": "$$\n\\boxed{M - r}\n$$"
        },
        {
            "introduction": "最后的这项实践旨在解决一个常见的真实世界数据挑战：误差方差不恒定（即异方差性）。它展示了如何通过用加权最小二乘法(WLS)替代标准的普通最小二乘法(OLS)来增强堆叠元学习器，从而通过赋予更可靠的预测更大的影响力来提升模型性能。这项练习体现了堆叠框架的适应性和强大功能 。",
            "id": "3175509",
            "problem": "实现一个堆叠集成，其元学习器在合成的异方差数据上通过加权最小二乘法 (WLS) 进行训练，使用的权重是与方差倒数 $1/\\hat{\\sigma}^2(x)$ 成正比的逆方差权重。然后，将其与一个通过普通最小二乘法 (OLS) 训练的无权重元学习器进行评估比较。你的程序必须是完全确定性的，并生成一行包含如下指定的布尔值列表的输出。\n\n使用的基本设置和基础定义：\n- 堆叠集成结合了 $M$ 个基学习器。给定一个训练集 $\\{(x_i, y_i)\\}_{i=1}^n$，定义第一层设计矩阵 $Z \\in \\mathbb{R}^{n \\times M}$，其列是 $M$ 个基学习器的折外预测。元学习器是一个线性模型，用于预测 $\\hat{y} = \\beta_0 + \\sum_{j=1}^M \\beta_j z_{j}$。\n- 普通最小二乘法 (OLS) 通过最小化 $\\sum_{i=1}^n (y_i - \\beta_0 - z_i^\\top \\beta)^2$ 来估计元参数 $\\beta$。\n- 在具有特定于观测值的方差 $\\sigma_i^2$ 的独立高斯噪声下，最大似然估计会导出加权最小二乘法 (WLS)，该方法最小化 $\\sum_{i=1}^n w_i (y_i - \\beta_0 - z_i^\\top \\beta)^2$，其中 $w_i \\propto 1/\\sigma_i^2$。\n- 为避免在堆叠中出现目标泄漏，第一层矩阵 $Z$ 必须由通过 $K$ 折交叉验证 (CV) 获得的折外预测构建，其中 $K \\in \\mathbb{N}$ 且 $K \\ge 2$。\n\n使用的合成数据生成方法：\n- 输入特征 $x$ 从 $[-3, 3]$ 上的均匀分布中独立采样。\n- 均值函数为 $f(x) = 1.5 \\sin(1.2 x) + 0.3 x$。\n- 异方差噪声标准差为 $\\sigma(x) = 0.3 + 0.7 \\lvert x \\rvert / 3$，同方差情况使用 $\\sigma(x) \\equiv 0.6$。响应为 $y = f(x) + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2(x))$。\n- 用于堆叠的基学习器：\n  - 基学习器 1：对特征 $[1, x]$ 进行线性回归。\n  - 基学习器 2：对特征 $[1, x, x^2, x^3]$ 进行立方回归。\n- 使用 $K = 5$ 折为元学习器构建折外预测。\n\n用于 WLS 权重的方差模型：\n- 通过对数线性模型估计条件方差。首先，在完整训练数据上拟合一个三次多项式作为均值模型 $\\tilde{f}(x)$，并计算残差 $r_i = y_i - \\tilde{f}(x_i)$。然后，使用 OLS 将 $\\log(r_i^2 + \\epsilon)$ 对特征 $[1, \\lvert x_i \\rvert]$ 进行回归以获得系数 $\\gamma$。估计的方差为 $\\hat{\\sigma}^2(x) = \\exp(\\gamma_0 + \\gamma_1 \\lvert x \\rvert)$，并应用一个下限 $v_{\\min}  0$：$\\hat{\\sigma}^2(x) \\leftarrow \\max(\\hat{\\sigma}^2(x), v_{\\min})$。除非在测试中另有说明，否则使用 $\\epsilon = 10^{-8}$ 和 $v_{\\min} = 10^{-6}$。\n- WLS 的权重与 $1/\\hat{\\sigma}^2(x)$ 成正比；不要在样本间进行归一化，因为用一个共同的正标量缩放所有权重不会改变 WLS 的解。\n\n评估协议：\n- 对于下文的每个测试用例，使用指定的随机种子生成独立的训练集和测试集。训练集大小为 $n_{\\text{train}}$，测试集大小为 $n_{\\text{test}}$。使用所述的堆叠程序，通过以下方式生成测试预测：\n  - 在折外第一层训练预测上训练的 OLS 元学习器。\n  - 使用指定的神谕权重 $w_i \\propto 1/\\sigma^2(x_i)$ 或估计权重 $w_i \\propto 1/\\hat{\\sigma}^2(x_i)$ 训练的 WLS 元学习器。\n- 报告在测试集上平均的测试均方误差 (MSE)，$ \\text{MSE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_i - \\hat{y}_i)^2$。\n\n要实现的测试套件以及需要输出的布尔值：\n- 测试 $1$ (异方差，神谕权重和估计权重)：\n  - 参数：$n_{\\text{train}} = 800$，$n_{\\text{test}} = 5000$，种子 $= 123$。\n  - 数据：异方差。\n  - 在测试集上计算三个 MSE：$\\text{MSE}_{\\text{OLS}}$、$\\text{MSE}_{\\text{WLS-oracle}}$ (使用已知 $\\sigma(x)$ 的权重 $w_i \\propto 1/\\sigma^2(x_i)$) 和 $\\text{MSE}_{\\text{WLS-est}}$ (使用权重 $w_i \\propto 1/\\hat{\\sigma}^2(x_i)$)。\n  - 输出两个布尔值：\n    - $b_1$: $\\text{MSE}_{\\text{WLS-oracle}}  \\text{MSE}_{\\text{OLS}}$ 是否成立。\n    - $b_2$: $\\text{MSE}_{\\text{WLS-est}}  \\text{MSE}_{\\text{OLS}}$ 是否成立。\n- 测试 $2$ (同方差，估计权重近似为常数)：\n  - 参数：$n_{\\text{train}} = 3000$，$n_{\\text{test}} = 5000$，种子 $= 456$。\n  - 数据：同方差，$\\sigma(x) \\equiv 0.6$。\n  - 计算 $\\text{MSE}_{\\text{OLS}}$ 和 $\\text{MSE}_{\\text{WLS-est}}$。\n  - 输出一个布尔值：\n    - $b_3$: $\\lvert \\text{MSE}_{\\text{WLS-est}} - \\text{MSE}_{\\text{OLS}} \\rvert \\le \\tau$ 是否成立，容差 $\\tau = 10^{-2}$。\n- 测试 $3$ (在极端但为正的权重下的稳定性)：\n  - 参数：$n_{\\text{train}} = 800$，$n_{\\text{test}} = 5000$，种子 $= 321$。\n  - 数据：异方差。\n  - 在求倒数前，将估计的方差乘以 $c = 10^{-6}$，并在计算 $1/\\hat{\\sigma}^2(x)$ 时使用一个非常小的下限 $v_{\\min} = 10^{-12}$。使用这些权重训练 WLS 元学习器并计算测试 MSE。\n  - 输出一个布尔值：\n    - $b_4$: 所有拟合的元参数和最终的测试 MSE 是否都是有限实数。\n- 测试 $4$ (错误设定的权重损害性能)：\n  - 参数：$n_{\\text{train}} = 800$，$n_{\\text{test}} = 5000$，种子 $= 789$。\n  - 数据：异方差。\n  - 构造与 $\\hat{\\sigma}^2(x)$ (而非其倒数) 成正比的错误设定的权重，并使用这些权重训练 WLS 元学习器。计算 $\\text{MSE}_{\\text{WLS-bad}}$ 并与 $\\text{MSE}_{\\text{OLS}}$ 进行比较。\n  - 输出一个布尔值：\n    - $b_5$: $\\text{MSE}_{\\text{OLS}} \\le \\text{MSE}_{\\text{WLS-bad}}$ 是否成立。\n- 测试 $5$ (WLS 对权重的全局重新缩放的不变性)：\n  - 使用与测试 1 相同的数据生成方式 (异方差，种子 $= 123$，$n_{\\text{train}} = 800$，$n_{\\text{test}} = 5000$)。\n  - 使用神谕权重 $w_i \\propto 1/\\sigma^2(x_i)$ 计算测试预测，然后再次使用重新缩放的权重 $w_i' = c \\cdot w_i$ (其中 $c=7$) 进行计算。测量两组测试预测之间的最大绝对差。\n  - 输出一个布尔值：\n    - $b_6$: 最大绝对差是否小于 $\\delta = 10^{-10}$。\n\n实现要求：\n- 使用 $K = 5$ 折构建折外第一层预测。\n- 仅使用线性代数和基础数值运算；不使用外部机器学习库。\n- 所有随机数生成必须由指定的种子控制，以确保可复现性。\n- 不涉及角度；没有物理单位。\n- 最终输出格式：你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[b_1, b_2, b_3, b_4, b_5, b_6]$。\n\n你的程序必须是一个单一、完整、可运行的脚本，它只生成一行指定格式的输出，不含任何额外文本。",
            "solution": "### 方法论框架\n\n问题的核心是在一个堆叠集成框架内，比较一个线性元学习器的两种估计策略。该集成结合了两个基学习器的预测。\n\n**1. 数据生成**\n\n生成合成数据以控制底层的真实模型和噪声结构。对于一组 $n$ 个样本：\n-   特征 $x_i$ 从一个均匀分布中抽取：$x_i \\sim U[-3, 3]$。\n-   真实的底层函数是 $f(x) = 1.5 \\sin(1.2 x) + 0.3 x$。\n-   响应 $y_i$ 是通过向真实函数值添加高斯噪声生成的：$y_i = f(x_i) + \\varepsilon_i$。\n-   噪声 $\\varepsilon_i$ 从 $\\mathcal{N}(0, \\sigma^2(x_i))$ 中抽取，其中标准差 $\\sigma(x)$ 可以是：\n    -   **异方差**：$\\sigma(x) = 0.3 + 0.7 \\lvert x \\rvert / 3$。噪声方差取决于输入 $x$。\n    -   **同方差**：$\\sigma(x) \\equiv 0.6$。噪声方差是恒定的。\n\n**2. 堆叠集成结构**\n\n该集成包含两个层次：基学习器（第 0 层）和元学习器（第 1 层）。\n\n-   **基学习器**：\n    -   学习器 1：一个简单的线性回归模型，使用特征 $[1, x]$。\n    -   学习器 2：一个三次多项式回归模型，使用特征 $[1, x, x^2, x^3]$。\n\n-   **第 1 层数据生成**：为防止目标泄漏，元学习器的训练数据是通过折外预测构建的。大小为 $n_{\\text{train}}$ 的训练集被分成 $K=5$ 个折。对于每个折，基学习器在其他 $K-1$ 个折上进行训练，然后用于对留出的折进行预测。所有 $n_{\\text{train}}$ 个样本的这些预测的集合构成了第一层设计矩阵 $\\mathbf{Z} \\in \\mathbb{R}^{n_{\\text{train}} \\times 2}$。$\\mathbf{Z}$ 的第 $i$ 行，表示为 $\\mathbf{z}_i$，包含了两个基学习器对训练样本 $(x_i, y_i)$ 的折外预测。\n\n**3. 元学习器**\n\n元学习器是一个线性模型，它结合基学习器的预测来产生最终预测 $\\hat{y}$。它包含一个截距项 $\\beta_0$：\n$$\n\\hat{y}_i = \\beta_0 + \\beta_1 z_{i1} + \\beta_2 z_{i2} = \\mathbf{z}_{\\text{aug}, i}^T \\boldsymbol{\\beta}\n$$\n其中 $\\mathbf{z}_{\\text{aug}, i} = [1, z_{i1}, z_{i2}]^T$ 是增广的第一层特征向量，$\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^T$ 是元参数向量。\n\n-   **普通最小二乘法 (OLS)**：此方法假设误差是同方差的，并通过最小化残差平方和来找到 $\\boldsymbol{\\beta}$：\n    $$\n    \\text{RSS} = \\sum_{i=1}^{n_{\\text{train}}} (y_i - \\mathbf{z}_{\\text{aug}, i}^T \\boldsymbol{\\beta})^2\n    $$\n    解由正规方程给出：$\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{Z}_{\\text{aug}}^T \\mathbf{Z}_{\\text{aug}})^{-1} \\mathbf{Z}_{\\text{aug}}^T \\mathbf{y}$。\n\n-   **加权最小二乘法 (WLS)**：当误差是独立的但具有非恒定方差（异方差性）时，此方法是最优的。它通过最小化加权残差平方和来找到 $\\boldsymbol{\\beta}$：\n    $$\n    \\text{WRSS} = \\sum_{i=1}^{n_{\\text{train}}} w_i (y_i - \\mathbf{z}_{\\text{aug}, i}^T \\boldsymbol{\\beta})^2\n    $$\n    其中权重 $w_i$ 理想情况下与误差方差成反比，$w_i \\propto 1/\\sigma_i^2$。解由加权正规方程给出：$\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}} = (\\mathbf{Z}_{\\text{aug}}^T \\mathbf{W} \\mathbf{Z}_{\\text{aug}})^{-1} \\mathbf{Z}_{\\text{aug}}^T \\mathbf{W} \\mathbf{y}$，其中 $\\mathbf{W}$ 是一个对角矩阵，其对角线元素为 $W_{ii} = w_i$。\n\n**4. WLS 权重估计**\n\n由于真实方差 $\\sigma^2(x)$ 在实践中通常是未知的，因此必须对其进行估计。该协议为方差指定了一个对数线性模型：\n1.  拟合一个三次多项式模型 $\\tilde{f}(x)$ 到完整训练数据 $(x_i, y_i)$ 以获得条件均值的估计。\n2.  计算平方残差 $r_i^2 = (y_i - \\tilde{f}(x_i))^2$。\n3.  对平方残差的对数进行建模。拟合一个线性模型，用特征 $[1, \\lvert x_i \\rvert]$ 来预测 $\\log(r_i^2 + \\epsilon)$，其中 $\\epsilon=10^{-8}$ 确保对数的参数为正。这将产生系数 $\\boldsymbol{\\gamma} = [\\gamma_0, \\gamma_1]^T$。\n4.  那么估计的方差为 $\\hat{\\sigma}^2(x) = \\exp(\\gamma_0 + \\gamma_1 \\lvert x \\rvert)$。\n5.  应用一个下限 $v_{\\min} = 10^{-6}$ 来防止极小的方差估计：$\\hat{\\sigma}^2(x) \\leftarrow \\max(\\hat{\\sigma}^2(x), v_{\\min})$。\n6.  WLS 的估计权重为 $w_i = 1/\\hat{\\sigma}^2(x_i)$。\n\n**5. 评估**\n\n对于每个测试用例，都会生成一个大小为 $n_{\\text{test}}$ 的独立测试集。在*完整*训练集上训练的基学习器，被用来生成测试集的第一层矩阵 $\\mathbf{Z}_{\\text{test}}$。然后，在训练集的第一层数据上训练的 OLS 和 WLS 元学习器，被用来对 $\\mathbf{Z}_{\\text{test}}$ 进行最终预测。性能通过测试集上的均方误差 (MSE) 来衡量：\n$$\n\\text{MSE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{test}, i} - \\hat{y}_{\\text{test}, i})^2\n$$\n然后执行五个测试中概述的具体数值比较，以生成所需的布尔输出。该程序稳健地测试了 WLS 在各种条件下的功效，包括理想（神谕权重）、实际（估计权重）和病态（错误设定的权重）场景。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main function to run the suite of tests for stacking with WLS meta-learner.\n    \"\"\"\n\n    def fit_linear_model(X, y, weights=None):\n        \"\"\"\n        Fits a linear model using OLS or WLS.\n        \n        Args:\n            X (np.ndarray): Design matrix.\n            y (np.ndarray): Target vector.\n            weights (np.ndarray, optional): Weights for WLS. If None, performs OLS.\n        \n        Returns:\n            np.ndarray: Fitted coefficients.\n        \"\"\"\n        if weights is None:\n            # OLS\n            coeffs = np.linalg.lstsq(X, y, rcond=None)[0]\n        else:\n            # WLS: transform to an equivalent OLS problem\n            sqrt_w = np.sqrt(weights)\n            X_w = sqrt_w[:, np.newaxis] * X\n            y_w = sqrt_w * y\n            coeffs = np.linalg.lstsq(X_w, y_w, rcond=None)[0]\n        return coeffs\n\n    def predict_linear_model(X, coeffs):\n        \"\"\"Predicts using a fitted linear model.\"\"\"\n        return X @ coeffs\n\n    def create_design_matrix(x, degree):\n        \"\"\"Creates a polynomial design matrix [1, x, x^2, ..., x^degree].\"\"\"\n        return np.vander(x, N=degree + 1, increasing=True)\n\n    def generate_data(n_samples, seed, heteroskedastic):\n        \"\"\"Generates synthetic data based on the problem specification.\"\"\"\n        rng = np.random.default_rng(seed)\n        x = rng.uniform(-3, 3, n_samples)\n        f_x = 1.5 * np.sin(1.2 * x) + 0.3 * x\n        \n        if heteroskedastic:\n            sigma_x = 0.3 + 0.7 * np.abs(x) / 3\n        else:\n            sigma_x = np.full_like(x, 0.6)\n            \n        epsilon = rng.normal(0, sigma_x, n_samples)\n        y = f_x + epsilon\n        return x, y, sigma_x\n\n    def get_out_of_fold_predictions(x_train, y_train, K):\n        \"\"\"\n        Generates level-1 data (out-of-fold predictions) using K-fold CV.\n        \n        Returns:\n            np.ndarray: Level-1 design matrix Z.\n        \"\"\"\n        n_train = len(y_train)\n        Z_train = np.zeros((n_train, 2))\n        \n        # Use deterministic fold splits\n        fold_indices = np.array_split(np.arange(n_train), K)\n\n        for k in range(K):\n            val_idx = fold_indices[k]\n            train_idx = np.concatenate([fold_indices[j] for j in range(K) if j != k])\n\n            x_fold_train, y_fold_train = x_train[train_idx], y_train[train_idx]\n            x_fold_val = x_train[val_idx]\n\n            # Base Learner 1: Linear\n            X1_fold_train = create_design_matrix(x_fold_train, 1)\n            coeffs1 = fit_linear_model(X1_fold_train, y_fold_train)\n            X1_fold_val = create_design_matrix(x_fold_val, 1)\n            Z_train[val_idx, 0] = predict_linear_model(X1_fold_val, coeffs1)\n\n            # Base Learner 2: Cubic\n            X2_fold_train = create_design_matrix(x_fold_train, 3)\n            coeffs2 = fit_linear_model(X2_fold_train, y_fold_train)\n            X2_fold_val = create_design_matrix(x_fold_val, 3)\n            Z_train[val_idx, 1] = predict_linear_model(X2_fold_val, coeffs2)\n            \n        return Z_train\n\n    def estimate_variance_params(x_train, y_train, epsilon):\n        \"\"\"\n        Estimates the parameters of the log-linear variance model.\n        \n        Returns:\n            np.ndarray: Coefficients gamma.\n        \"\"\"\n        # 1. Fit mean model (cubic poly)\n        X_mean = create_design_matrix(x_train, 3)\n        mean_coeffs = fit_linear_model(X_mean, y_train)\n        y_pred_mean = predict_linear_model(X_mean, mean_coeffs)\n        \n        # 2. Compute residuals\n        residuals = y_train - y_pred_mean\n        \n        # 3. Regress log(r^2 + eps) on [1, |x|]\n        log_sq_res = np.log(residuals**2 + epsilon)\n        X_var = create_design_matrix(np.abs(x_train), 1)\n        gamma = fit_linear_model(X_var, log_sq_res)\n        \n        return gamma\n\n    def get_estimated_variance(x, gamma, v_min):\n        \"\"\"Calculates estimated variance for given x and gamma.\"\"\"\n        X_var = create_design_matrix(np.abs(x), 1)\n        log_var = predict_linear_model(X_var, gamma)\n        var_est = np.exp(log_var)\n        return np.maximum(var_est, v_min)\n\n\n    results = []\n\n    # --- Test 1  5: Heteroskedastic, Oracle vs. Estimated Weights  Scaling Invariance ---\n    n_train, n_test, seed = 800, 5000, 123\n    x_train, y_train, sigma_train = generate_data(n_train, seed, heteroskedastic=True)\n    x_test, y_test, sigma_test = generate_data(n_test, seed + 1, heteroskedastic=True)\n\n    # Level-1 data\n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n\n    # Base learners trained on full training data for test predictions\n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    \n    Z_test = np.zeros((n_test, 2))\n    Z_test[:, 0] = predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full)\n    Z_test[:, 1] = predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n\n    # OLS meta-learner\n    beta_ols = fit_linear_model(Z_train_aug, y_train)\n    y_pred_ols = predict_linear_model(Z_test_aug, beta_ols)\n    mse_ols = np.mean((y_test - y_pred_ols)**2)\n\n    # WLS with Oracle weights\n    weights_oracle = 1 / sigma_train**2\n    beta_wls_oracle = fit_linear_model(Z_train_aug, y_train, weights=weights_oracle)\n    y_pred_wls_oracle = predict_linear_model(Z_test_aug, beta_wls_oracle)\n    mse_wls_oracle = np.mean((y_test - y_pred_wls_oracle)**2)\n    \n    # WLS with Estimated weights\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-6)\n    weights_est = 1 / var_est_train\n    beta_wls_est = fit_linear_model(Z_train_aug, y_train, weights=weights_est)\n    y_pred_wls_est = predict_linear_model(Z_test_aug, beta_wls_est)\n    mse_wls_est = np.mean((y_test - y_pred_wls_est)**2)\n\n    b1 = mse_wls_oracle  mse_ols\n    b2 = mse_wls_est  mse_ols\n    results.extend([b1, b2])\n\n    # --- Test 5: Invariance to weight scaling ---\n    weights_oracle_scaled = 7.0 * weights_oracle\n    beta_wls_oracle_scaled = fit_linear_model(Z_train_aug, y_train, weights=weights_oracle_scaled)\n    y_pred_wls_oracle_scaled = predict_linear_model(Z_test_aug, beta_wls_oracle_scaled)\n    max_diff_test5 = np.max(np.abs(y_pred_wls_oracle - y_pred_wls_oracle_scaled))\n    b6 = max_diff_test5  1e-10\n\n    # --- Test 2: Homoskedastic case ---\n    n_train, n_test, seed = 3000, 5000, 456\n    x_train, y_train, _ = generate_data(n_train, seed, heteroskedastic=False)\n    x_test, y_test, _ = generate_data(n_test, seed + 1, heteroskedastic=False)\n    \n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n    \n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    Z_test = np.c_[\n        predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full),\n        predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    ]\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n\n    beta_ols = fit_linear_model(Z_train_aug, y_train)\n    y_pred_ols = predict_linear_model(Z_test_aug, beta_ols)\n    mse_ols_t2 = np.mean((y_test - y_pred_ols)**2)\n\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-6)\n    weights_est = 1 / var_est_train\n    beta_wls_est = fit_linear_model(Z_train_aug, y_train, weights=weights_est)\n    y_pred_wls_est = predict_linear_model(Z_test_aug, beta_wls_est)\n    mse_wls_est_t2 = np.mean((y_test - y_pred_wls_est)**2)\n\n    b3 = np.abs(mse_wls_est_t2 - mse_ols_t2) = 1e-2\n    results.append(b3)\n\n    # --- Test 3: Stability under extreme weights ---\n    n_train, n_test, seed = 800, 5000, 321\n    x_train, y_train, _ = generate_data(n_train, seed, heteroskedastic=True)\n    x_test, y_test, _ = generate_data(n_test, seed + 1, heteroskedastic=True)\n\n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-12)\n    \n    # Modify variance and weights\n    var_est_mod = var_est_train * 1e-6\n    weights_extreme = 1 / var_est_mod\n    \n    beta_wls_extreme = fit_linear_model(Z_train_aug, y_train, weights=weights_extreme)\n    \n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    Z_test = np.c_[\n        predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full),\n        predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    ]\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n\n    y_pred_wls_extreme = predict_linear_model(Z_test_aug, beta_wls_extreme)\n    mse_wls_extreme = np.mean((y_test - y_pred_wls_extreme)**2)\n\n    b4 = np.all(np.isfinite(beta_wls_extreme)) and np.isfinite(mse_wls_extreme)\n    results.append(b4)\n\n    # --- Test 4: Misspecified weights ---\n    n_train, n_test, seed = 800, 5000, 789\n    x_train, y_train, _ = generate_data(n_train, seed, heteroskedastic=True)\n    x_test, y_test, _ = generate_data(n_test, seed + 1, heteroskedastic=True)\n    \n    Z_train = get_out_of_fold_predictions(x_train, y_train, K=5)\n    Z_train_aug = np.c_[np.ones(n_train), Z_train]\n\n    coeffs1_full = fit_linear_model(create_design_matrix(x_train, 1), y_train)\n    coeffs2_full = fit_linear_model(create_design_matrix(x_train, 3), y_train)\n    Z_test = np.c_[\n        predict_linear_model(create_design_matrix(x_test, 1), coeffs1_full),\n        predict_linear_model(create_design_matrix(x_test, 3), coeffs2_full)\n    ]\n    Z_test_aug = np.c_[np.ones(n_test), Z_test]\n    \n    beta_ols = fit_linear_model(Z_train_aug, y_train)\n    y_pred_ols = predict_linear_model(Z_test_aug, beta_ols)\n    mse_ols_t4 = np.mean((y_test - y_pred_ols)**2)\n\n    gamma_est = estimate_variance_params(x_train, y_train, epsilon=1e-8)\n    var_est_train = get_estimated_variance(x_train, gamma_est, v_min=1e-6)\n    weights_bad = var_est_train  # Proportional to variance, not inverse\n    \n    beta_wls_bad = fit_linear_model(Z_train_aug, y_train, weights=weights_bad)\n    y_pred_wls_bad = predict_linear_model(Z_test_aug, beta_wls_bad)\n    mse_wls_bad = np.mean((y_test - y_pred_wls_bad)**2)\n    \n    b5 = mse_ols_t4 = mse_wls_bad\n    results.append(b5)\n    \n    # Add Test 5 result at the end\n    results.append(b6)\n    \n    # Final output formatting: [b1, b2, b3, b4, b5, b6]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}