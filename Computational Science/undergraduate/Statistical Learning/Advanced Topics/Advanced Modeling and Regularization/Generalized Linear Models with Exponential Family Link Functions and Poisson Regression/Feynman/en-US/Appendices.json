{
    "hands_on_practices": [
        {
            "introduction": "After fitting a statistical model, a crucial next step is to assess its goodness-of-fit. This practice introduces Pearson residuals, a fundamental diagnostic tool for Generalized Linear Models. By deriving these residuals from first principles for a Poisson model, you will learn how to check one of its core assumptions—that the variance is equal to the mean—and how to spot common problems like overdispersion .",
            "id": "3124037",
            "problem": "A laboratory studies the count of spontaneous cellular events over fixed time windows. The counts are modeled using a Generalized Linear Model (GLM) for the Poisson distribution with the canonical log link. Suppose the response variables $Y_{i}$ are conditionally independent and each follows a Poisson distribution with mean $\\mu_{i}$.\n\n1) Starting from the exponential family form and the GLM variance function, derive the Pearson residual for a Poisson GLM in terms of $y_{i}$ and $\\mu_{i}$. Your derivation must begin with the exponential family representation and proceed by identifying the variance function.\n\n2) Explain the interpretation of the Pearson residuals in this setting and describe what qualitative pattern in a residuals-versus-fitted plot would diagnose variance misfit relative to the Poisson variance assumption. Be explicit about how overdispersion or underdispersion would appear.\n\n3) A simple study is conducted with $n=6$ independent observations and a single covariate $x_{i}\\in\\{0,1,2,3,4,5\\}$. A Poisson GLM with log link is fitted, yielding estimated coefficients $\\hat{\\beta}_{0}=1$ and $\\hat{\\beta}_{1}=0.25$, so that the fitted means are $\\hat{\\mu}_{i}=\\exp\\!\\big(\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{i}\\big)$. The observed counts are\n- for $x_{1}=0$, $y_{1}=1$,\n- for $x_{2}=1$, $y_{2}=6$,\n- for $x_{3}=2$, $y_{3}=2$,\n- for $x_{4}=3$, $y_{4}=10$,\n- for $x_{5}=4$, $y_{5}=9$,\n- for $x_{6}=5$, $y_{6}=13$.\n\nCompute the Pearson dispersion estimate\n$$\n\\hat{\\phi}_{P}\\;=\\;\\frac{1}{n-p}\\sum_{i=1}^{n} r_{i}^{2},\n$$\nwhere $r_{i}$ are the Pearson residuals you derived in part (1) and $p=2$ is the number of fitted coefficients (intercept and slope). Report the numerical value of $\\hat{\\phi}_{P}$. Round your answer to four significant figures. Do not include units.",
            "solution": "We proceed from the foundational properties of the exponential family and the structure of Generalized Linear Models (GLMs).\n\n1) Derivation of the Pearson residual for a Poisson GLM.\n\nA one-parameter exponential family has densities of the form\n$$\nf(y\\mid \\theta,\\phi)\\;=\\;\\exp\\!\\left(\\frac{y\\theta-b(\\theta)}{a(\\phi)}+c(y,\\phi)\\right),\n$$\nwhere $\\theta$ is the canonical parameter, $\\phi$ is a scale (dispersion) parameter, and $a(\\cdot)$, $b(\\cdot)$, $c(\\cdot)$ are known functions. Fundamental properties of exponential families yield\n$$\n\\mathbb{E}[Y]\\;=\\;b'(\\theta),\\qquad \\operatorname{Var}(Y)\\;=\\;a(\\phi)\\,b''(\\theta).\n$$\nIn a GLM, we link the mean $\\mu=\\mathbb{E}[Y]$ to predictors via a link function $g(\\mu)=\\eta=x^{\\top}\\beta$, and write the variance in the form\n$$\n\\operatorname{Var}(Y)\\;=\\;\\phi\\,V(\\mu),\n$$\nwhere $V(\\mu)$ is the variance function.\n\nFor the Poisson distribution, the probability mass function is\n$$\n\\mathbb{P}(Y=y)\\;=\\;\\exp\\!\\big(y\\ln\\mu - \\mu - \\ln(y!)\\big),\n$$\nso in canonical form we may take $\\theta=\\ln\\mu$, $a(\\phi)=1$, $b(\\theta)=\\exp(\\theta)$, and $c(y,\\phi)=-\\ln(y!)$. Then\n$$\nb'(\\theta)\\;=\\;\\exp(\\theta)\\;=\\;\\mu,\\qquad b''(\\theta)\\;=\\;\\exp(\\theta)\\;=\\;\\mu,\n$$\nand hence\n$$\n\\operatorname{Var}(Y)\\;=\\;a(\\phi)\\,b''(\\theta)\\;=\\;1\\cdot \\mu\\;=\\;\\mu.\n$$\nTherefore the variance function for the Poisson family is $V(\\mu)=\\mu$, and the dispersion is $\\phi=1$ by construction of the Poisson GLM.\n\nThe Pearson residual is defined as the raw residual standardized by the model-implied standard deviation,\n$$\nr_{i}\\;=\\;\\frac{y_{i}-\\mu_{i}}{\\sqrt{\\operatorname{Var}(Y_{i})}}\\;=\\;\\frac{y_{i}-\\mu_{i}}{\\sqrt{\\phi\\,V(\\mu_{i})}}.\n$$\nFor the Poisson case with $\\phi=1$ and $V(\\mu_{i})=\\mu_{i}$, this specializes to\n$$\nr_{i}\\;=\\;\\frac{y_{i}-\\mu_{i}}{\\sqrt{\\mu_{i}}}.\n$$\n\n2) Interpretation and residuals-versus-fitted diagnostic for variance misfit.\n\nBecause $r_{i}$ standardizes the deviation $y_{i}-\\mu_{i}$ by the model-implied standard deviation $\\sqrt{\\mu_{i}}$, under a correctly specified Poisson GLM with adequate sample size and a well-fitted mean structure, we expect $r_{i}$ to be approximately centered at $0$ with unit variance, that is,\n$$\n\\mathbb{E}[r_{i}]\\approx 0,\\qquad \\operatorname{Var}(r_{i})\\approx 1,\n$$\nand no systematic pattern when plotted against fitted means $\\hat{\\mu}_{i}$ (or the linear predictor $\\hat{\\eta}_{i}$). A residuals-versus-fitted plot that shows increasing spread of $r_{i}$ as $\\hat{\\mu}_{i}$ increases (a funnel opening upwards) suggests overdispersion relative to the Poisson variance assumption $V(\\mu)=\\mu$; the empirical variance exceeds the mean, inflating the spread of residuals at larger fitted values. Conversely, a decreasing spread (a funnel narrowing) suggests underdispersion, with empirical variance smaller than the Poisson mean. Systematic curvature in the residual mean trend indicates mean-structure misspecification, whereas a systematic change in the vertical spread across $\\hat{\\mu}_{i}$ indicates variance misfit.\n\n3) Computation of the Pearson dispersion estimate $\\hat{\\phi}_{P}$.\n\nWe are given $n=6$, $p=2$, $\\hat{\\beta}_{0}=1$, $\\hat{\\beta}_{1}=0.25$, $x_{i}\\in\\{0,1,2,3,4,5\\}$, and observed counts $(y_{1},\\dots,y_{6})=(1,6,2,10,9,13)$. The fitted means are\n$$\n\\hat{\\mu}_{i}\\;=\\;\\exp\\!\\big(\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{i}\\big)\\;=\\;\\exp\\!\\big(1+0.25\\,x_{i}\\big).\n$$\nWriting $\\exp(1)=e$, the fitted means and their square roots are\n$$\n\\begin{aligned}\n&\\hat{\\mu}_{1}=\\exp(1+0.25\\cdot 0)=\\exp(1)=e,\\quad &&\\sqrt{\\hat{\\mu}_{1}}=\\exp(0.5)=e^{1/2},\\\\\n&\\hat{\\mu}_{2}=\\exp(1+0.25\\cdot 1)=\\exp(1.25),\\quad &&\\sqrt{\\hat{\\mu}_{2}}=\\exp(0.625),\\\\\n&\\hat{\\mu}_{3}=\\exp(1+0.25\\cdot 2)=\\exp(1.5),\\quad &&\\sqrt{\\hat{\\mu}_{3}}=\\exp(0.75),\\\\\n&\\hat{\\mu}_{4}=\\exp(1+0.25\\cdot 3)=\\exp(1.75),\\quad &&\\sqrt{\\hat{\\mu}_{4}}=\\exp(0.875),\\\\\n&\\hat{\\mu}_{5}=\\exp(1+0.25\\cdot 4)=\\exp(2),\\quad &&\\sqrt{\\hat{\\mu}_{5}}=\\exp(1),\\\\\n&\\hat{\\mu}_{6}=\\exp(1+0.25\\cdot 5)=\\exp(2.25),\\quad &&\\sqrt{\\hat{\\mu}_{6}}=\\exp(1.125).\n\\end{aligned}\n$$\nThe Pearson residuals are $r_{i}=(y_{i}-\\hat{\\mu}_{i})/\\sqrt{\\hat{\\mu}_{i}}$. Using exact-exponential forms where convenient and numerical evaluation for the final calculation:\n- For $i=1$: $r_{1}=(1-e)/e^{1/2}=e^{-1/2}-e^{1/2}\\approx -1.042190611$.\n- For $i=2$: $r_{2}=\\big(6-\\exp(1.25)\\big)/\\exp(0.625)\\approx 1.343391721$.\n- For $i=3$: $r_{3}=\\big(2-\\exp(1.5)\\big)/\\exp(0.75)\\approx -1.172450069$.\n- For $i=4$: $r_{4}=\\big(10-\\exp(1.75)\\big)/\\exp(0.875)\\approx 1.769745$.\n- For $i=5$: $r_{5}=\\big(9-\\exp(2)\\big)/\\exp(1)=9/e-e\\approx 0.5926331421$.\n- For $i=6$: $r_{6}=\\big(13-\\exp(2.25)\\big)/\\exp(1.125)\\approx 1.1402652$.\n\nNow compute the Pearson dispersion estimate with $n-p=6-2=4$:\n$$\n\\hat{\\phi}_{P}\\;=\\;\\frac{1}{4}\\sum_{i=1}^{6} r_{i}^{2}\\;\\approx\\;\\frac{1}{4}\\Big(1.042190611^{2}+1.343391721^{2}+1.172450069^{2}+1.769745^{2}+0.5926331421^{2}+1.1402652^{2}\\Big).\n$$\nEvaluating the squares and summing,\n$$\n\\sum_{i=1}^{6} r_{i}^{2}\\;\\approx\\;9.04892,\n$$\nso\n$$\n\\hat{\\phi}_{P}\\;\\approx\\;\\frac{9.04892}{4}\\;=\\;2.26223.\n$$\nRounded to four significant figures, the value is $2.262$. This value substantially exceeds $1$, which is consistent with overdispersion relative to the Poisson variance assumption and aligns with an anticipated upward-opening funnel in a residuals-versus-fitted plot.",
            "answer": "$$\\boxed{2.262}$$"
        },
        {
            "introduction": "Beyond the variance assumption, a GLM's validity also hinges on the correct specification of the mean structure—specifically, the linear relationship between predictors and the transformed mean. This hands-on coding exercise guides you through implementing and using component-plus-residual (CPR) plots, a powerful technique for diagnosing non-linearity . By building the diagnostic from scratch, you will gain a deeper intuition for how to validate and refine the functional form of your model.",
            "id": "3124117",
            "problem": "You are asked to implement, from first principles, a verification procedure for the linearity of the log-mean in predictors in a Poisson Generalized Linear Model (GLM) using component-plus-residual plots. You must build your method upon the exponential family representation of the Poisson distribution and the definition of generalized linear models with a log link. Your implementation must be a complete program that performs the following tasks without any user input.\n\nLet the response be a count variable $Y \\in \\{0,1,2,\\dots\\}$ such that $Y \\mid \\boldsymbol{x} \\sim \\text{Poisson}(\\mu)$, where the mean $\\mu$ satisfies the GLM relation $\\eta = g(\\mu) = \\log(\\mu)$ and $\\eta = \\boldsymbol{x}^{\\top}\\boldsymbol{\\beta}$ for coefficient vector $\\boldsymbol{\\beta}$. The Poisson distribution belongs to the exponential family with canonical link given by the log function, and the model is to be estimated by Iteratively Reweighted Least Squares, derived from the score equations of the exponential family and the link function. You must not assume any shortcut formulas beyond these foundational definitions in your code design.\n\nYour program must:\n\n- Implement a Poisson GLM with log link fitted by Iteratively Reweighted Least Squares from first principles, using only linear algebra and the definitions implied by the exponential family and GLM framework.\n- After fitting the model, construct a component-plus-residual diagnostic for a designated predictor $x_j$ by computing, for each observation $i$, a component-plus-residual ordinate that is appropriate for the Poisson log-link model. Use this to assess whether the relationship between the linear predictor and $x_j$ is linear.\n- Operationalize the linearity check by fitting an ordinary least squares model of the component-plus-residual ordinate versus $1$, $x_j$, and $x_j^2$, and extract the coefficient multiplying $x_j^2$. A value near $0$ indicates approximate linearity in $x_j$, whereas a value with a non-negligible magnitude indicates curvature.\n- For each test case described below, output the estimated quadratic coefficient as a floating-point number.\n\nYou must use the following test suite, generating synthetic data in each case with the specified seeds to ensure reproducibility. In all cases, include an intercept, the target predictor $x$, and a nuisance predictor $w$ in the fitted model (but do not include any nonlinear terms in the fitted GLM; the nonlinearity is only for data generation in specified cases). Let $\\boldsymbol{x}_i = (1, x_i, w_i)^{\\top}$, and let $n$ denote the sample size.\n\nTest Case A (happy path, linear truth):\n- Sample size $n = 1000$.\n- Set coefficients $\\beta_0 = 1.0$, $\\beta_1 = 0.5$, $\\beta_2 = -0.25$.\n- Generate $x_i \\sim \\text{Uniform}([-2,2])$ and $w_i \\sim \\mathcal{N}(0,1)$ independently, using random seed $12345$.\n- Generate responses with $\\eta_i = \\beta_0 + \\beta_1 x_i + \\beta_2 w_i$ and $\\mu_i = \\exp(\\eta_i)$, then $Y_i \\sim \\text{Poisson}(\\mu_i)$ with seed $12345$.\n\nTest Case B (detectable curvature in $x$):\n- Sample size $n = 1000$.\n- Set coefficients $\\beta_0 = 1.0$, $\\beta_1 = 0.0$, $\\gamma = 0.6$, $\\beta_2 = -0.25$.\n- Generate $x_i \\sim \\text{Uniform}([-2,2])$ and $w_i \\sim \\mathcal{N}(0,1)$ independently, using random seed $23456$.\n- Generate responses with $\\eta_i = \\beta_0 + \\beta_1 x_i + \\gamma x_i^2 + \\beta_2 w_i$ and $\\mu_i = \\exp(\\eta_i)$, then $Y_i \\sim \\text{Poisson}(\\mu_i)$ with seed $23456$.\n\nTest Case C (boundary case: no $x$ effect, linear in $w$ only):\n- Sample size $n = 1000$.\n- Set coefficients $\\beta_0 = 1.0$, $\\beta_1 = 0.0$, $\\beta_2 = 0.2$.\n- Generate $x_i \\sim \\text{Uniform}([-2,2])$ and $w_i \\sim \\mathcal{N}(0,1)$ independently, using random seed $34567$.\n- Generate responses with $\\eta_i = \\beta_0 + \\beta_1 x_i + \\beta_2 w_i$ and $\\mu_i = \\exp(\\eta_i)$, then $Y_i \\sim \\text{Poisson}(\\mu_i)$ with seed $34567$.\n\nFor each test case, fit the Poisson GLM with predictors $(1, x, w)$ only (no quadratic terms) via Iteratively Reweighted Least Squares. Then, for the predictor $x$, compute a component-plus-residual ordinate appropriate for the Poisson log-link model and fit an ordinary least squares regression of this ordinate on $1$, $x$, and $x^2$. Extract the estimated coefficient on $x^2$.\n\nYour program should produce a single line of output containing a comma-separated list of the three estimated quadratic coefficients, corresponding to Test Cases A, B, and C, in that order, rounded to at most six decimal places, and enclosed in square brackets. For example, the output must look like $[a,b,c]$ where $a$, $b$, and $c$ are the requested floating-point numbers.\n\nNo physical units or angles are involved. All outputs are pure numbers without any unit annotation. All random generation must follow the stated seeds exactly. No external input is permitted, and the program must be executable as given.",
            "solution": "The user requests the implementation of a statistical diagnostic procedure to verify the linearity assumption in a Poisson Generalized Linear Model (GLM). This requires building the model fitting algorithm, Iteratively Reweighted Least Squares (IRLS), from first principles, and then constructing and analyzing a component-plus-residual (CPR) diagnostic.\n\n### 1. The Poisson GLM and its Exponential Family Representation\n\nA Generalized Linear Model consists of three components: a random component (the distribution of the response), a systematic component (the linear predictor), and a link function. For the specified Poisson GLM:\n\n*   **Random Component**: The response variable $Y_i$ for observation $i$ is assumed to follow a Poisson distribution, $Y_i \\sim \\text{Poisson}(\\mu_i)$, where $\\mu_i = E[Y_i]$. The probability mass function (PMF) is $P(Y_i=y_i) = \\frac{e^{-\\mu_i}\\mu_i^{y_i}}{y_i!}$.\n*   **Systematic Component**: The linear predictor $\\eta_i$ is a linear function of the predictors $\\boldsymbol{x}_i$, i.e., $\\eta_i = \\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta}$, where $\\boldsymbol{\\beta}$ is the vector of coefficients.\n*   **Link Function**: The link function $g$ connects the mean $\\mu_i$ to the linear predictor $\\eta_i$. The problem specifies the logarithm as the link function, so $\\eta_i = g(\\mu_i) = \\log(\\mu_i)$. This implies $\\mu_i = \\exp(\\eta_i)$.\n\nThe Poisson distribution is a member of the exponential family. Its PMF can be expressed in the canonical form $f(y|\\theta, \\phi) = \\exp\\left(\\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y, \\phi)\\right)$:\n$$P(Y=y;\\mu) = \\frac{\\mu^y e^{-\\mu}}{y!} = \\exp(y\\log(\\mu) - \\mu - \\log(y!))$$\nBy comparison, we identify the components:\n*   The canonical parameter is $\\theta = \\log(\\mu)$.\n*   The function $b(\\theta)$ is $\\mu = e^{\\theta}$.\n*   The dispersion parameter $a(\\phi)$ is $1$.\n*   $c(y, \\phi) = -\\log(y!)$.\n\nThe canonical link function is the one that equates the canonical parameter to the linear predictor, $\\theta = \\eta$. For the Poisson distribution, this means $\\log(\\mu) = \\eta$, which matches the specified log link.\n\n### 2. Model Fitting: Iteratively Reweighted Least Squares (IRLS)\n\nThe coefficients $\\boldsymbol{\\beta}$ are estimated by maximizing the log-likelihood function. The log-likelihood for $n$ independent observations is:\n$$l(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\log P(Y_i=y_i) = \\sum_{i=1}^n (y_i \\log(\\mu_i) - \\mu_i - \\log(y_i!))$$\nSince $\\mu_i = \\exp(\\boldsymbol{x}_i^{\\top}\\boldsymbol{\\beta})$, the estimating equations (obtained by setting the gradient of $l(\\boldsymbol{\\beta})$ to zero) are nonlinear in $\\boldsymbol{\\beta}$. We solve them using a Newton-Raphson-based algorithm known as a Fisher scoring or IRLS.\n\nThe score vector $\\boldsymbol{U}(\\boldsymbol{\\beta})$ (the gradient of the log-likelihood) is:\n$$\\boldsymbol{U}(\\boldsymbol{\\beta}) = \\frac{\\partial l}{\\partial \\boldsymbol{\\beta}} = \\sum_{i=1}^n \\frac{\\partial l_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\boldsymbol{\\beta}} = \\sum_{i=1}^n \\left(\\frac{y_i - \\mu_i}{\\mu_i}\\right) (\\mu_i) (\\boldsymbol{x}_i) = \\sum_{i=1}^n (y_i-\\mu_i)\\boldsymbol{x}_i = \\boldsymbol{X}^{\\top}(\\boldsymbol{y}-\\boldsymbol{\\mu})$$\nThe Hessian matrix $\\boldsymbol{H}(\\boldsymbol{\\beta})$ (second derivative) is:\n$$\\boldsymbol{H}(\\boldsymbol{\\beta}) = \\frac{\\partial^2 l}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^{\\top}} = -\\sum_{i=1}^n \\mu_i \\boldsymbol{x}_i \\boldsymbol{x}_i^{\\top} = -\\boldsymbol{X}^{\\top}\\boldsymbol{W}\\boldsymbol{X}$$\nwhere $\\boldsymbol{W}$ is a diagonal matrix with $W_{ii} = \\mu_i$. The Fisher Information matrix is $\\boldsymbol{I}(\\boldsymbol{\\beta}) = -E[\\boldsymbol{H}(\\boldsymbol{\\beta})] = \\boldsymbol{X}^{\\top}\\boldsymbol{W}\\boldsymbol{X}$.\n\nThe Newton-Raphson update at iteration $t$ is:\n$$\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + \\boldsymbol{I}(\\boldsymbol{\\beta}^{(t)})^{-1} \\boldsymbol{U}(\\boldsymbol{\\beta}^{(t)}) = \\boldsymbol{\\beta}^{(t)} + (\\boldsymbol{X}^{\\top}\\boldsymbol{W}^{(t)}\\boldsymbol{X})^{-1} \\boldsymbol{X}^{\\top}(\\boldsymbol{y}-\\boldsymbol{\\mu}^{(t)})$$\nThis can be rearranged into the IRLS form. We define an \"adjusted\" or \"working\" response vector $\\boldsymbol{z}^{(t)}$:\n$$z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)})g'(\\mu_i^{(t)}) = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}}$$\nWith this definition, the update for $\\boldsymbol{\\beta}$ becomes the solution to a weighted least squares problem:\n$$\\boldsymbol{\\beta}^{(t+1)} = (\\boldsymbol{X}^{\\top}\\boldsymbol{W}^{(t)}\\boldsymbol{X})^{-1}\\boldsymbol{X}^{\\top}\\boldsymbol{W}^{(t)}\\boldsymbol{z}^{(t)}$$\nThe algorithm proceeds as follows:\n1.  Initialize $\\boldsymbol{\\beta}^{(0)}$, for instance, to $\\boldsymbol{0}$.\n2.  Iterate for $t=0, 1, 2, \\dots$ until convergence:\n    a. Compute $\\boldsymbol{\\eta}^{(t)} = \\boldsymbol{X}\\boldsymbol{\\beta}^{(t)}$ and $\\boldsymbol{\\mu}^{(t)} = \\exp(\\boldsymbol{\\eta}^{(t)})$.\n    b. Compute the weights $W_{ii}^{(t)} = \\mu_i^{(t)}$ and the working responses $z_i^{(t)} = \\eta_i^{(t)} + (y_i - \\mu_i^{(t)})/\\mu_i^{(t)}$.\n    c. Solve the weighted least squares equation for $\\boldsymbol{\\beta}^{(t+1)}$.\n3.  The final converged vector $\\hat{\\boldsymbol{\\beta}}$ is the maximum likelihood estimate.\n\n### 3. Component-Plus-Residual (CPR) Diagnostic\n\nThe CPR plot is a diagnostic tool used to assess the functional form of a predictor in a regression model. For a predictor $x_j$, the plot displays the \"component-plus-residual\" ordinate against $x_j$. If the assumed linear relationship is correct, the plot should exhibit a linear trend.\n\nFor a GLM, the relationship is linear on the scale of the link function. The component-plus-residual ordinate for predictor $x_j$ at observation $i$ is defined as the sum of the fitted linear component and a residual term:\n$$C_i^{(j)} = \\hat{\\beta}_j x_{ij} + \\text{residual}_i$$\nThe appropriate residual is the *working residual* from the final IRLS iteration, which represents the remaining error on the linear predictor scale:\n$$r_{W,i} = z_i - \\hat{\\eta}_i = \\frac{y_i - \\hat{\\mu}_i}{\\hat{\\mu}_i}$$\nThus, the CPR ordinate is:\n$$C_i^{(j)} = \\hat{\\beta}_j x_{ij} + \\frac{y_i - \\hat{\\mu}_i}{\\hat{\\mu}_i}$$\nThis quantity isolates the relationship between the response (adjusted for other predictors) and $x_j$.\n\n### 4. Quantitative Assessment of Linearity\n\nTo automate the visual inspection of the CPR plot, we can fit a regression model to it. The problem specifies fitting a quadratic model using Ordinary Least Squares (OLS):\n$$C_i^{(j)} = \\gamma_0 + \\gamma_1 x_{ij} + \\gamma_2 x_{ij}^2 + \\epsilon_i$$\nThe magnitude of the estimated coefficient $\\hat{\\gamma}_2$ serves as a quantitative measure of nonlinearity. A value of $\\hat{\\gamma}_2$ near zero supports the linearity assumption for predictor $x_j$. A non-negligible value indicates that a quadratic (or other nonlinear) term might be needed in the model. The OLS estimate for $\\boldsymbol{\\gamma} = (\\gamma_0, \\gamma_1, \\gamma_2)^\\top$ is found by solving the normal equations:\n$$\\hat{\\boldsymbol{\\gamma}} = (\\boldsymbol{Z}_j^\\top \\boldsymbol{Z}_j)^{-1}\\boldsymbol{Z}_j^\\top \\boldsymbol{C}^{(j)}$$\nwhere $\\boldsymbol{Z}_j$ is the design matrix with columns $[1, x_j, x_j^2]$, and $\\boldsymbol{C}^{(j)}$ is the vector of CPR ordinates.\n\nThe following Python program implements this entire procedure for the three specified test cases. It generates synthetic data, fits the Poisson GLM using the derived IRLS algorithm, computes the CPR ordinates for the predictor $x$, and finally estimates the quadratic coefficient $\\gamma_2$ for each case.",
            "answer": "```python\nimport numpy as np\n\ndef fit_poisson_glm(X, y, max_iter=50, tol=1e-8):\n    \"\"\"\n    Fits a Poisson GLM with a log link using Iteratively Reweighted Least Squares (IRLS).\n    \n    The derivation is from first principles based on the exponential family form\n    of the Poisson distribution and Newton-Raphson (Fisher scoring) updates.\n\n    Args:\n        X (np.ndarray): Design matrix (n_samples, n_features).\n        y (np.ndarray): Response vector (n_samples,).\n        max_iter (int): Maximum number of iterations for the IRLS algorithm.\n        tol (float): Convergence tolerance for the norm of the change in the coefficient vector.\n        \n    Returns:\n        np.ndarray: Estimated coefficient vector beta.\n    \"\"\"\n    # Initialize coefficients to zeros\n    beta = np.zeros(X.shape[1])\n    \n    for _ in range(max_iter):\n        # Step a: Calculate current linear predictor, mean, and weights\n        eta = X @ beta\n        mu = np.exp(eta)\n        \n        # The weight for observation i is mu_i, derived from the Fisher Information matrix\n        w = mu\n        \n        # Step b: Calculate the working response z\n        # z = eta + (y - mu) * g'(mu), where g'(mu) = 1/mu for the log link\n        z = eta + (y - mu) / mu\n        \n        # Store old beta for convergence check\n        beta_old = beta.copy()\n        \n        # Step c: Solve the weighted least squares problem for the new beta\n        # beta_new = inv(X.T @ W @ X) @ X.T @ W @ z\n        # This is best implemented using np.linalg.solve for numerical stability,\n        # and by using broadcasting for efficient computation of matrix products involving W.\n        XtW = X.T * w  # Broadcasting w to the rows of X.T\n        XtWX = XtW @ X\n        XtWz = XtW @ z\n        \n        try:\n            beta = np.linalg.solve(XtWX, XtWz)\n        except np.linalg.LinAlgError:\n            # This is unlikely with the problem's data but is good practice.\n            # If the matrix is singular, return the last stable coefficients.\n            return beta_old\n            \n        # Check for convergence\n        if np.linalg.norm(beta - beta_old) < tol:\n            break\n            \n    return beta\n\ndef run_case(n, true_params, seed, target_predictor_idx=1):\n    \"\"\"\n    Runs a single test case: generates data, fits the GLM, and computes the diagnostic.\n    \n    Args:\n        n (int): Sample size.\n        true_params (dict): Dictionary of parameters for data generation.\n        seed (int): Random seed for reproducibility of both predictors and response.\n        target_predictor_idx (int): Column index in X of the predictor to test.\n        \n    Returns:\n        float: The estimated quadratic coefficient from the CPR diagnostic regression.\n    \"\"\"\n    # 1. Generate data using the specified seed to ensure reproducibility.\n    rng = np.random.default_rng(seed)\n    \n    x = rng.uniform(-2, 2, size=n)\n    w = rng.normal(0, 1, size=n)\n    \n    # Construct design matrix for the GLM to be fitted (linear terms only)\n    X = np.c_[np.ones(n), x, w]\n    \n    # Generate true linear predictor (eta_true) based on the case's data generating process\n    b0 = true_params.get('b0', 0.0)\n    b1 = true_params.get('b1', 0.0)\n    b2 = true_params.get('b2', 0.0)\n    gamma_true = true_params.get('gamma', 0.0) # For non-linear case\n    \n    eta_true = b0 + b1 * x + gamma_true * x**2 + b2 * w\n    mu_true = np.exp(eta_true)\n    y = rng.poisson(mu_true)\n    \n    # 2. Fit the specified Poisson GLM\n    beta_hat = fit_poisson_glm(X, y)\n    \n    # 3. Compute the Component-Plus-Residual (CPR) ordinate\n    # Calculate final fitted values from the model\n    mu_hat = np.exp(X @ beta_hat)\n    \n    # Isolate the target predictor variable and its estimated coefficient\n    x_j = X[:, target_predictor_idx]\n    beta_j_hat = beta_hat[target_predictor_idx]\n    \n    # CPR ordinate = (linear component for x_j) + (working residual)\n    cpr_ordinate = beta_j_hat * x_j + (y - mu_hat) / mu_hat\n    \n    # 4. Perform the diagnostic OLS regression: CPR ordinate ~ 1 + x_j + x_j^2\n    # Construct the design matrix for this diagnostic regression\n    Z = np.c_[np.ones(n), x_j, x_j**2]\n    \n    # Solve for the coefficients of the diagnostic regression\n    gamma_hat = np.linalg.solve(Z.T @ Z, Z.T @ cpr_ordinate)\n    \n    # 5. Extract and return the quadratic coefficient (gamma_2)\n    quad_coeff = gamma_hat[2]\n    \n    return quad_coeff\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite, then prints the results in the required format.\n    \"\"\"\n    # Define the three test cases as specified in the problem statement.\n    test_cases = [\n        # Case A: Linear relationship (happy path)\n        {'n': 1000, 'params': {'b0': 1.0, 'b1': 0.5, 'b2': -0.25}, 'seed': 12345},\n        # Case B: Quadratic relationship (detectable curvature)\n        {'n': 1000, 'params': {'b0': 1.0, 'b1': 0.0, 'gamma': 0.6, 'b2': -0.25}, 'seed': 23456},\n        # Case C: No relationship with x (boundary case)\n        {'n': 1000, 'params': {'b0': 1.0, 'b1': 0.0, 'b2': 0.2}, 'seed': 34567},\n    ]\n\n    results = []\n    for case in test_cases:\n        # For all cases, the target predictor 'x' is at index 1 of the design matrix X.\n        quad_coeff = run_case(\n            n=case['n'],\n            true_params=case['params'],\n            seed=case['seed'],\n            target_predictor_idx=1\n        )\n        # Round the result to at most six decimal places as requested.\n        results.append(round(quad_coeff, 6))\n\n    # Format the final output as a single-line, comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world data rarely perfectly match our model's assumptions. This exercise explores a scenario where the mean is correctly specified but the variance structure is wrong, a common situation in practice. You will learn to derive and apply the sandwich estimator, a cornerstone of robust inference that provides valid standard errors for your parameter estimates even when the model's variance assumption is violated . This practice demonstrates how to draw reliable conclusions from imperfect models.",
            "id": "3124048",
            "problem": "You are given a scenario in statistical learning involving a Generalized Linear Model (GLM) where the true data-generating process is Poisson with an identity mean function, but inference is performed using a Gaussian linear model with an identity link. This setting illustrates how link misspecification can yield a correct mean while using the wrong variance, and how inference must be corrected via sandwich estimators.\n\nAssume independent observations $\\{(x_i, y_i)\\}_{i=1}^n$ where $y_i$ follows a Poisson distribution with conditional mean $E[y_i \\mid x_i] = \\mu_i$, and suppose the true mean is linear, $\\mu_i = \\beta_0 + \\beta_1 x_i$, with $\\mu_i > 0$ for all $i$. Consider fitting a Gaussian linear model with identity link to $\\{(x_i, y_i)\\}$, which yields the correct mean specification but assumes constant variance. In contrast, the Poisson distribution implies variance equal to the mean at each $i$. You will design, derive, and compute the correct asymptotic covariance for the least-squares estimator using a sandwich estimator.\n\nUse the following core definitions and facts as the fundamental base:\n- The Poisson distribution belongs to the exponential family and satisfies $E[y_i \\mid x_i] = \\mu_i$ and $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i$.\n- A Generalized Linear Model (GLM) links the mean $\\mu_i$ to the linear predictor via a link function, and the variance is given by a variance function times a dispersion parameter. For Poisson, the variance function is $V(\\mu_i) = \\mu_i$ and the dispersion parameter is $1$.\n- The Gaussian linear model with identity link assumes $\\operatorname{Var}(y_i \\mid x_i) = \\sigma^2$, a constant, which can be misspecified if the true variance is not constant.\n\nYour program must, for each test case, derive from first principles the asymptotic covariance of the least-squares estimator under heteroskedasticity induced by the Poisson variance, and compare it to the misspecified homoskedastic covariance. The heteroskedastic-robust covariance must be constructed as a sandwich estimator using the score-based M-estimation framework. The misspecified homoskedastic covariance must be constructed from the Gaussian model with identity link using a constant variance level. To make the comparison numerically definite and fully reproducible, define the misspecified constant variance level as the average of the true variances, namely $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n \\mu_i$.\n\nFor each test case:\n1. Treat the design matrix as $X \\in \\mathbb{R}^{n \\times 2}$ with first column equal to $1$ and second column equal to $x_i$.\n2. Compute the heteroskedastic-robust sandwich covariance matrix for $(\\beta_0,\\beta_1)$ using the Poisson variance structure.\n3. Compute the misspecified homoskedastic covariance matrix for $(\\beta_0,\\beta_1)$ using $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n \\mu_i$.\n4. Extract the standard errors for the intercept and slope from both covariance matrices.\n5. Report, for each test case, the ratio of robust to homoskedastic standard errors for the intercept and the slope, in that order, as two floats.\n\nTest Suite:\n- Case A (happy path, moderate means): $n = 6$, $x = [0, 1, 2, 3, 4, 5]$, $\\beta_0 = 2.0$, $\\beta_1 = 0.5$.\n- Case B (boundary, small means but strictly positive): $n = 5$, $x = [-3, -2, -1, 0, 1]$, $\\beta_0 = 0.7$, $\\beta_1 = 0.2$.\n- Case C (strong heteroskedasticity, large spread in means): $n = 6$, $x = [0, 5, 10, 15, 20, 25]$, $\\beta_0 = 1.0$, $\\beta_1 = 0.8$.\n\nYour program must compute the ratios for all three cases and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is a two-element list in the order [intercept_ratio, slope_ratio]. For example, the output format must be exactly like:\n[[rA0,rA1],[rB0,rB1],[rC0,rC1]]\nwhere each $r$ is a float. No additional text should be printed. Angles do not appear in this task, and no physical units are involved. Express all numeric output as decimal floats.",
            "solution": "The problem requires the derivation and computation of two types of covariance matrices for an Ordinary Least Squares (OLS) estimator under a specific scenario of model misspecification. The true data generating process is a Poisson regression model with an identity link function, while the fitted model is a Gaussian linear model, also with an identity link. This setup implies that the mean function is correctly specified, but the variance structure is not.\n\nFirst, we formalize the problem. We have independent observations $\\{(x_i, y_i)\\}_{i=1}^n$. The true model for $y_i$ conditional on the predictor $x_i$ is given by:\n1.  **True Mean**: $E[y_i \\mid x_i] = \\mu_i$. The link is identity, so the mean is linear in parameters: $\\mu_i = \\beta_0 + \\beta_1 x_i$. In matrix notation, using the design matrix $X$ with rows $x_i^T = [1, x_i]$, we have $E[y \\mid X] = X\\beta$.\n2.  **True Distribution**: $y_i \\sim \\text{Poisson}(\\mu_i)$. A key property of the Poisson distribution is that the variance equals the mean, so $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i$.\n\nThe model being fitted is a Gaussian linear model, which corresponds to OLS. The estimator for the coefficient vector $\\beta = [\\beta_0, \\beta_1]^T$ is:\n$$ \\hat{\\beta} = (X^T X)^{-1} X^T y $$\nSince the mean function is correctly specified ($E[y \\mid X] = X\\beta$), the OLS estimator $\\hat{\\beta}$ is unbiased:\n$$ E[\\hat{\\beta}] = E[(X^T X)^{-1} X^T y] = (X^T X)^{-1} X^T E[y] = (X^T X)^{-1} X^T (X\\beta) = \\beta $$\n\nThe core of the problem lies in the covariance of $\\hat{\\beta}$. The general formula for the covariance of a linear transformation of a random vector $y$ is $\\operatorname{Cov}(Ay) = A \\operatorname{Cov}(y) A^T$. Applying this to $\\hat{\\beta}$:\n$$ \\operatorname{Cov}(\\hat{\\beta}) = \\operatorname{Cov}((X^T X)^{-1} X^T y) = (X^T X)^{-1} X^T \\operatorname{Cov}(y) X (X^T X)^{-1} $$\nHere, $\\operatorname{Cov}(y)$ is the covariance matrix of the response vector $y$. Since observations are independent, this is a diagonal matrix where the diagonal entries are the individual variances, $\\operatorname{Var}(y_i)$. Let's denote this matrix by $\\Omega = \\text{diag}(\\operatorname{Var}(y_1), \\dots, \\operatorname{Var}(y_n))$.\n\n**1. Heteroskedasticity-Robust (Sandwich) Covariance**\n\nUnder the true Poisson data-generating process, the variance is heteroskedastic: $\\operatorname{Var}(y_i \\mid x_i) = \\mu_i$. The true covariance matrix of $y$ is therefore $\\Omega = \\text{diag}(\\mu_1, \\mu_2, \\dots, \\mu_n)$. Substituting this into the general formula gives the correct, robust covariance matrix for $\\hat{\\beta}$:\n$$ \\Sigma_{\\text{robust}} = (X^T X)^{-1} (X^T \\Omega X) (X^T X)^{-1} $$\nThis is the \"sandwich\" estimator of the covariance. The matrices $(X^T X)^{-1}$ form the \"bread,\" and the middle term $X^T \\Omega X$ is the \"meat.\" This form arises from M-estimation theory, where the OLS estimator is derived from the estimating equation $\\sum_{i=1}^n x_i(y_i - x_i^T\\beta) = 0$. The robust covariance formula correctly accounts for the fact that the variance of the residuals $(y_i - \\mu_i)$ depends on $i$.\n\n**2. Misspecified Homoskedastic Covariance**\n\nThe standard Gaussian linear model incorrectly assumes homoskedasticity, i.e., constant variance $\\operatorname{Var}(y_i \\mid x_i) = \\sigma^2$ for all $i$. Under this assumption, $\\operatorname{Cov}(y) = \\sigma^2 I_n$, where $I_n$ is the $n \\times n$ identity matrix. The covariance formula simplifies to:\n$$ \\Sigma_{\\text{misspecified}} = (X^T X)^{-1} X^T (\\sigma^2 I_n) X (X^T X)^{-1} = \\sigma^2 (X^T X)^{-1} (X^T X) (X^T X)^{-1} = \\sigma^2 (X^T X)^{-1} $$\nTo make the comparison concrete, the problem specifies that the constant variance level $\\sigma^2$ should be taken as the average of the true variances:\n$$ \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n \\operatorname{Var}(y_i) = \\frac{1}{n} \\sum_{i=1}^n \\mu_i $$\n\n**3. Calculation and Comparison**\n\nFor each test case, the procedure is as follows:\n- Given the true parameters $\\beta_0$ and $\\beta_1$ and the vector of predictors $x$, we first construct the design matrix $X$.\n- We then compute the true conditional means $\\mu = X\\beta$.\n- The robust covariance matrix, $\\Sigma_{\\text{robust}}$, is computed using $\\Omega = \\text{diag}(\\mu_i)$. The \"meat\" matrix can be calculated as $X^T \\Omega X = \\sum_{i=1}^n \\mu_i x_i x_i^T$, where $x_i$ is the $i$-th row of $X$.\n- The misspecified homoskedastic covariance matrix, $\\Sigma_{\\text{homo}} = \\sigma^2 (X^T X)^{-1}$, is computed using $\\sigma^2 = \\text{mean}(\\mu)$.\n- The standard errors (SE) for the intercept and slope are the square roots of the diagonal elements of these respective covariance matrices. For example, $\\text{SE}_{\\text{robust}}(\\hat{\\beta}_0) = \\sqrt{(\\Sigma_{\\text{robust}})_{11}}$.\n- Finally, we compute the ratio of the robust SE to the homoskedastic SE for both the intercept ($\\beta_0$) and the slope ($\\beta_1$). This ratio indicates the extent to which the naive OLS standard errors are incorrect. A ratio greater than $1$ means the naive SE underestimates the true uncertainty, while a ratio less than $1$ means it overestimates it.\n\nThe implementation will perform these matrix calculations for each of the three test cases provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the formatted result.\n    \"\"\"\n    test_cases = [\n        # Case A (happy path, moderate means)\n        {'n': 6, 'x': np.array([0., 1., 2., 3., 4., 5.]), 'beta0': 2.0, 'beta1': 0.5},\n        # Case B (boundary, small means but strictly positive)\n        {'n': 5, 'x': np.array([-3., -2., -1., 0., 1.]), 'beta0': 0.7, 'beta1': 0.2},\n        # Case C (strong heteroskedasticity, large spread in means)\n        {'n': 6, 'x': np.array([0., 5., 10., 15., 20., 25.]), 'beta0': 1.0, 'beta1': 0.8},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n, x_data, beta0, beta1 = case['n'], case['x'], case['beta0'], case['beta1']\n        \n        # 1. Construct the design matrix X and coefficient vector beta.\n        X = np.vstack([np.ones(n), x_data]).T\n        beta = np.array([beta0, beta1])\n\n        # 2. Compute the true conditional means mu_i.\n        mu = X @ beta\n\n        # 3. Compute the inverse of X^T X, a common term.\n        XTX_inv = np.linalg.inv(X.T @ X)\n\n        # 4. Compute the heteroskedastic-robust sandwich covariance matrix.\n        # Omega is the diagonal matrix of true variances (mu_i).\n        # The 'meat' of the sandwich is X^T * Omega * X.\n        # This can be computed efficiently without forming the large Omega matrix.\n        # (X.T * mu) @ X is an efficient way to write X.T @ diag(mu) @ X\n        meat = (X.T * mu) @ X\n        Sigma_robust = XTX_inv @ meat @ XTX_inv\n\n        # 5. Compute the misspecified homoskedastic covariance matrix.\n        # sigma^2 is defined as the average of the true variances.\n        sigma2 = np.mean(mu)\n        Sigma_homo = sigma2 * XTX_inv\n\n        # 6. Extract the standard errors (sqrt of diagonal elements).\n        se_robust = np.sqrt(np.diag(Sigma_robust))\n        se_homo = np.sqrt(np.diag(Sigma_homo))\n\n        # 7. Compute the ratios of robust to homoskedastic standard errors.\n        ratios = se_robust / se_homo\n        \n        # Format the result for this case as a string '[r0,r1]'\n        formatted_case_result = f\"[{ratios[0]},{ratios[1]}]\"\n        all_results.append(formatted_case_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}