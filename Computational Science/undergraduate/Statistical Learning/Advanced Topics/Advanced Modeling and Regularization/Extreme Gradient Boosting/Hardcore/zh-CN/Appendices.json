{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握XGBoost，我们必须从其核心决策机制开始：模型如何决定在哪里以及是否进行分裂。这个练习将引导你从第一性原理出发，推导决定树节点分裂的“增益”公式，并理解正则化参数（特别是复杂度惩罚项 $\\gamma$）如何影响这一决策。这是理解XGBoost学习过程的基础。",
            "id": "3120320",
            "problem": "考虑极端梯度提升（XGBoost），该算法通过使用围绕当前预测的二阶泰勒展开来最小化正则化训练目标，从而拟合决策树。对于权重为 $w$ 的单个叶节点，假设每次迭代对目标的近似贡献为\n$$\\sum_{i \\in \\text{leaf}} \\big( g_i w + \\tfrac{1}{2} h_i w^2 \\big) + \\tfrac{\\lambda}{2} w^2 + \\gamma,$$\n其中，$g_i$ 和 $h_i$ 分别表示在数据点 $i$ 处损失函数关于预测值的一阶和二阶导数，$\\lambda$ 是叶节点权重的 $\\ell_2$ 正则化系数，$\\gamma$ 是每个叶节点的复杂度惩罚项。令 $G = \\sum_{i \\in \\text{leaf}} g_i$ 和 $H = \\sum_{i \\in \\text{leaf}} h_i$ 表示该叶节点上的一阶和二阶导数之和。\n\n假设一个候选分裂将一个节点上的数据划分为一个左子节点（其导数和为 $(G_L, H_L)$）和一个右子节点（其导数和为 $(G_R, H_R)$），而父节点的导数和为 $(G_P, H_P)$，满足 $G_P = G_L + G_R$ 和 $H_P = H_L + H_R$。假设 $H_L  0$, $H_R  0$ 且 $\\lambda  0$。\n\n从上述正则化目标出发，且不使用任何预先记下的公式，推导出一个精确的解析表达式，用于计算叶节点惩罚项 $\\gamma$ 的最小值，使得执行该分裂所带来的净目标减少量为非正值（即不发生分裂）。然后，使用具体数值 $G_L = 7$, $H_L = 14$, $G_R = -3$, $H_R = 6$ 和 $\\lambda = 1$ 计算该最小 $\\gamma$ 值。将最终的最小 $\\gamma$ 数值四舍五入到四位有效数字。你的最终答案必须是一个实数。",
            "solution": "用户希望我按照以下步骤解决问题：\n1.  **验证问题陈述：** 我将提取所有给定信息，并根据验证标准进行检查。该问题定义明确，在统计学习理论（特别是 XGBoost 的机制）中有科学依据，并且提供了所有必要的信息。这是一个有效的问题。\n2.  **推导最小 gamma 的解析表达式。**\n3.  **用给定的数值计算该表达式。**\n\n以下是详细的解题过程：\n\n### 步骤 1：问题验证\n\n**1.1. 提取已知条件：**\n-   权重为 $w$ 的单个叶节点每次迭代的近似目标贡献为：$\\sum_{i \\in \\text{leaf}} \\big( g_i w + \\tfrac{1}{2} h_i w^2 \\big) + \\tfrac{\\lambda}{2} w^2 + \\gamma$。\n-   $g_i$: 在数据点 $i$ 处损失函数关于预测值的一阶导数。\n-   $h_i$: 在数据点 $i$ 处损失函数关于预测值的二阶导数。\n-   $\\lambda$: 叶节点权重的 $\\ell_2$ 正则化系数。\n-   $\\gamma$: 每个叶节点的复杂度惩罚项。\n-   $G = \\sum_{i \\in \\text{leaf}} g_i$。\n-   $H = \\sum_{i \\in \\text{leaf}} h_i$。\n-   一个分裂创建一个左子节点（其导数和为 $(G_L, H_L)$）和一个右子节点（其导数和为 $(G_R, H_R)$）。\n-   父节点的导数和为 $(G_P, H_P)$，其中 $G_P = G_L + G_R$ 和 $H_P = H_L + H_R$。\n-   假设：$H_L  0$, $H_R  0$ 且 $\\lambda  0$。\n-   用于计算的数值：$G_L = 7$, $H_L = 14$, $G_R = -3$, $H_R = 6$ 和 $\\lambda = 1$。\n\n**1.2. 使用提取的已知条件进行验证：**\n-   **科学依据：** 该问题是极端梯度提升（XGBoost）理论中的一个标准推导，XGBoost 是机器学习中一个广泛使用且成熟的算法。其目标函数、泰勒展开和正则化项都是 XGBoost 公式中的典型组成部分。\n-   **定义明确：** 该问题提供了所有必要的定义、约束和数据，以推导出唯一且有意义的解。问题清晰明确。\n-   **客观性：** 该问题以精确的数学语言陈述，没有主观性。\n\n**1.3. 结论与行动：**\n问题有效。我将继续推导解决方案。\n\n### 步骤 2：推导与求解\n\n问题要求解出使得分裂不发生的 $\\gamma$ 的最小值。这种情况发生在分裂后的总目标函数大于或等于不进行分裂时的目标函数。我们必须首先找到给定树结构下最小化的目标值。\n\n设 $\\tilde{\\mathcal{L}}$ 为正在添加的新树的总目标函数。对于一棵有 $T$ 个叶节点的树，这是每个叶节点贡献的总和：\n$$ \\tilde{\\mathcal{L}} = \\sum_{j=1}^{T} \\left[ \\sum_{i \\in I_j} \\left( g_i w_j + \\frac{1}{2} h_i w_j^2 \\right) + \\frac{\\lambda}{2} w_j^2 + \\gamma \\right] $$\n其中 $I_j$ 是叶节点 $j$ 中的数据点集合，$w_j$ 是叶节点 $j$ 的权重。\n\n使用给定的导数和 $G_j = \\sum_{i \\in I_j} g_i$ 和 $H_j = \\sum_{i \\in I_j} h_i$，我们可以将单个叶节点 $j$ 的目标重写为：\n$$ \\tilde{\\mathcal{L}}_j = G_j w_j + \\frac{1}{2} H_j w_j^2 + \\frac{\\lambda}{2} w_j^2 + \\gamma = G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 + \\gamma $$\n树的总目标是：\n$$ \\tilde{\\mathcal{L}} = \\sum_{j=1}^{T} \\left[ G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right] + T\\gamma $$\n对于一个固定的树结构（即固定的叶节点），这个目标是关于叶节点权重 $w_j$ 的独立二次函数的和。我们可以通过最小化每个叶节点对目标的贡献来找到其最优权重 $w_j^*$。我们对 $w_j$ 求导并令其为 $0$：\n$$ \\frac{\\partial \\tilde{\\mathcal{L}}_j}{\\partial w_j} = G_j + (H_j + \\lambda) w_j = 0 $$\n已知 $H_L  0$、$H_R  0$ 且 $\\lambda  0$，因此对于任何叶节点都有 $H_j  0$，从而 $H_j + \\lambda  0$。因此，存在唯一的最小值。最优权重 $w_j^*$ 为：\n$$ w_j^* = -\\frac{G_j}{H_j + \\lambda} $$\n将此最优权重代回叶节点 $j$ 的目标函数，得到该叶节点的最小目标值：\n$$ \\tilde{\\mathcal{L}}_j^* = G_j \\left( -\\frac{G_j}{H_j + \\lambda} \\right) + \\frac{1}{2} (H_j + \\lambda) \\left( -\\frac{G_j}{H_j + \\lambda} \\right)^2 + \\gamma $$\n$$ \\tilde{\\mathcal{L}}_j^* = -\\frac{G_j^2}{H_j + \\lambda} + \\frac{1}{2} \\frac{G_j^2}{H_j + \\lambda} + \\gamma = -\\frac{1}{2} \\frac{G_j^2}{H_j + \\lambda} + \\gamma $$\n这个值通常被称为叶节点的“分数”。\n\n现在，我们比较两种情况下的总目标值：\n1.  **不分裂：** 父节点被视为一个单独的叶节点。此时 $T=1$。目标为：\n    $$ \\tilde{\\mathcal{L}}_{\\text{no-split}} = -\\frac{1}{2} \\frac{G_P^2}{H_P + \\lambda} + \\gamma $$\n2.  **分裂：** 父节点被分裂为左（$L$）和右（$R$）两个叶节点。此时 $T=2$。总目标是两个新叶节点分数的总和：\n    $$ \\tilde{\\mathcal{L}}_{\\text{split}} = \\left( -\\frac{1}{2} \\frac{G_L^2}{H_L + \\lambda} + \\gamma \\right) + \\left( -\\frac{1}{2} \\frac{G_R^2}{H_R + \\lambda} + \\gamma \\right) = -\\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} \\right) + 2\\gamma $$\n如果分裂能减少总目标函数，即 $\\tilde{\\mathcal{L}}_{\\text{split}}  \\tilde{\\mathcal{L}}_{\\text{no-split}}$，则分裂是有益的。分裂带来的“增益”（Gain）通常定义为分裂所带来的损失部分的减少量，即 $\\text{Gain} = \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_P^2}{H_P + \\lambda} \\right)$。只有当 $\\text{Gain} > \\gamma$ 时才会分裂。\n\n问题要求“净目标减少量为非正值”，即 $\\tilde{\\mathcal{L}}_{\\text{no-split}} - \\tilde{\\mathcal{L}}_{\\text{split}} \\le 0$。\n$$ \\left( -\\frac{1}{2} \\frac{G_P^2}{H_P + \\lambda} + \\gamma \\right) - \\left( -\\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} \\right) + 2\\gamma \\right) \\le 0 $$\n$$ \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_P^2}{H_P + \\lambda} \\right) - \\gamma \\le 0 $$\n重新整理以求解 $\\gamma$：\n$$ \\gamma \\ge \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_P^2}{H_P + \\lambda} \\right) $$\n确保分裂不发生的 $\\gamma$ 的最小值是使不等式变为等式的值。使用 $G_P = G_L + G_R$ 和 $H_P = H_L + H_R$：\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right) $$\n这就是所求的解析表达式。\n\n现在我们用给定的数值计算这个表达式：$G_L = 7$, $H_L = 14$, $G_R = -3$, $H_R = 6$ 和 $\\lambda = 1$。\n首先，计算父节点的导数和：\n$$ G_P = G_L + G_R = 7 + (-3) = 4 $$\n$$ H_P = H_L + H_R = 14 + 6 = 20 $$\n将这些值代入 $\\gamma_{\\text{min}}$ 的表达式中：\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{7^2}{14 + 1} + \\frac{(-3)^2}{6 + 1} - \\frac{4^2}{20 + 1} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{49}{15} + \\frac{9}{7} - \\frac{16}{21} \\right) $$\n为了对分数求和，我们找到一个公分母，即 $\\text{lcm}(15, 7, 21) = 105$。\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{49 \\times 7}{15 \\times 7} + \\frac{9 \\times 15}{7 \\times 15} - \\frac{16 \\times 5}{21 \\times 5} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{343}{105} + \\frac{135}{105} - \\frac{80}{105} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{343 + 135 - 80}{105} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{1}{2} \\left( \\frac{478 - 80}{105} \\right) = \\frac{1}{2} \\left( \\frac{398}{105} \\right) $$\n$$ \\gamma_{\\text{min}} = \\frac{199}{105} $$\n其小数值约为 $1.895238...$。四舍五入到四位有效数字得到 $1.895$。",
            "answer": "$$\\boxed{1.895}$$"
        },
        {
            "introduction": "在理解了分裂的基本机理后，我们将探讨一个常见的实际问题：当正则化过强时会发生什么？通过分析一个所有候选分裂都因增益不足而被拒绝的案例，你将学会诊断由正则化引起的欠拟合问题，并思考如何通过调整超参数来解决它。这个练习连接了理论计算与实际模型调优。",
            "id": "3120315",
            "problem": "您正在一个二分类任务上训练一个极端梯度提升 (XGBoost) 分类器，这是一种梯度提升决策树 (GBDT) 的形式，使用逻辑损失。训练使用目标函数的二阶近似，并包括标准的树正则化：一个由参数 $\\lambda$ 控制的叶子权重上的 $\\ell_2$ 惩罚，以及一个由参数 $\\gamma$ 控制的每次分裂的最小损失降低阈值。考虑第一个提升轮次，从一个恒定得分 $f(x) = 0$ 开始（因此对于每个训练实例，初始类别概率为 $p_i = 0.5$），样本权重均匀，并假设在这一轮中最大深度为 $d_{\\max} = 1$。\n\n给定数据集，其特征为 $x_1, x_2 \\in \\{0,1\\}$，标签为 $y \\in \\{0,1\\}$：\n- $i = 1$: $(x_1, x_2, y) = (0, 0, 0)$\n- $i = 2$: $(x_1, x_2, y) = (0, 0, 0)$\n- $i = 3$: $(x_1, x_2, y) = (0, 1, 1)$\n- $i = 4$: $(x_1, x_2, y) = (0, 1, 0)$\n- $i = 5$: $(x_1, x_2, y) = (1, 0, 1)$\n- $i = 6$: $(x_1, x_2, y) = (1, 0, 0)$\n- $i = 7$: $(x_1, x_2, y) = (1, 1, 1)$\n- $i = 8$: $(x_1, x_2, y) = (1, 1, 1)$\n\n这一轮的超参数为 $\\lambda = 4$ 和 $\\gamma = 0.25$。假设没有其他约束是激活的（例如，最小叶子节点权重阈值足够小，不会阻止分裂）。\n\n仅使用带二阶近似的梯度提升和树正则化的核心定义（不要假设任何快捷公式），从第一性原理出发，对候选节点上的聚合一阶和二阶导数进行推理，以决定在初始化时，对 $x_1$ 或 $x_2$ 的任何单次分裂是否能通过正则化阈值。然后，回答以下关于如何在此设置中检测和纠正可能出现的欠拟合的多项选择题。\n\n哪些陈述是正确的？\n\nA. 将 $\\gamma$ 降低到由原始分裂增益决定的阈值以下（例如，在此数据集中使 $\\gamma  0.2$）或减小 $\\lambda$ 将会增加净分裂增益并可能使分裂得以发生；这直接解决了由过强正则化引起的欠拟合问题。\n\nB. 在保持 $\\gamma = 0.25$ 的同时增加提升轮次 $T$ 的数量，将可靠地克服欠拟合，因为即使每棵单独的树都没有分裂，集成模型也可以累积改进。\n\nC. 在这里，一种检测欠拟合的实用方法是监控训练目标和验证指标：如果几轮过后，两者都在基线附近停滞不前，并且每棵树都只有一个叶子节点（没有接受任何分裂），这表明是由正则化阈值引起的欠拟合。\n\nD. 将 $x_1$ 和 $x_2$ 标准化为零均值和单位方差，其本身将在不改变 $\\lambda$ 或 $\\gamma$ 的情况下，根据二阶准则增加分裂增益并克服 $\\gamma$ 阈值。\n\nE. 在保持 $\\gamma = 0.25$ 和 $\\lambda = 4$ 的同时，将最大深度 $d_{\\max}$ 从 1 增加到 3，可以保证第一次分裂被接受，因为更深的树有更多的叶子，从而抵消了分裂惩罚。",
            "solution": "用户要求对问题陈述进行批判性验证，然后从第一性原理推导解答，并对所有提供的选项进行评估。\n\n### 问题验证\n\n**步骤1：提取给定信息**\n-   **模型:** 极端梯度提升 (XGBoost) 分类器，一种梯度提升决策树 (GBDT) 的形式。\n-   **任务:** 二分类。\n-   **损失函数:** 逻辑损失。\n-   **近似方法:** 目标函数的二阶近似。\n-   **正则化:** 一个由 $\\lambda$ 控制的叶子权重上的 $\\ell_2$ 惩罚，以及一个每次分裂的最小损失降低阈值 $\\gamma$。\n-   **初始条件:** 第一个提升轮次从恒定得分 $f(x) = 0$ 开始。这意味着所有实例的初始预测概率为 $p_i = 0.5$。样本权重是均匀的。\n-   **树约束:** 第一轮的最大深度为 $d_{\\max} = 1$。\n-   **数据集:** $N=8$ 个实例，特征为 $x_1, x_2 \\in \\{0, 1\\}$，标签为 $y \\in \\{0, 1\\}$。\n    -   $i=1: (x_1, x_2, y) = (0, 0, 0)$\n    -   $i=2: (x_1, x_2, y) = (0, 0, 0)$\n    -   $i=3: (x_1, x_2, y) = (0, 1, 1)$\n    -   $i=4: (x_1, x_2, y) = (0, 1, 0)$\n    -   $i=5: (x_1, x_2, y) = (1, 0, 1)$\n    -   $i=6: (x_1, x_2, y) = (1, 0, 0)$\n    -   $i=7: (x_1, x_2, y) = (1, 1, 1)$\n    -   $i=8: (x_1, x_2, y) = (1, 1, 1)$\n-   **超参数:** $\\lambda = 4$ 和 $\\gamma = 0.25$。\n-   **假设:** 没有其他约束（例如，最小叶子节点权重）是激活的。\n\n**步骤2：使用提取的给定信息进行验证**\n-   **科学依据:** 该问题牢固地植根于梯度提升机的理论，特别是 XGBoost 算法。逻辑损失、二阶泰勒近似以及 $\\ell_2$ 和 $\\gamma$ 正则化的使用是该模型的标准和基本组成部分。该问题在科学上是合理的。\n-   **良构性:** 问题提供了计算第一棵树分裂增益所需的所有必要信息：数据集、初始预测、损失函数以及所有相关超参数。问题是具体的，并允许一个唯一的、可推导的答案。\n-   **客观性:** 问题以精确、客观和技术性的语言陈述，没有歧义或主观论断。\n-   **完整性和一致性:** 设置是自成体系且内部一致的。没有缺失的数据点或矛盾的约束。\n-   **可行性:** 数据集和超参数值是人工合成的，但对于机器学习课程中的一个示例问题来说完全现实。所需的计算是可行的。\n\n**步骤3：结论和行动**\n问题陈述是有效的。这是一个统计学习领域中良构的、有科学依据的问题。我将继续进行解答。\n\n### 解答推导\n\n核心任务是确定第一棵树中是否可以进行任何分裂。这需要计算所有可能分裂的分裂增益，并将其与阈值 $\\gamma$ 进行比较。\n\n**1. 一阶和二阶导数（梯度和海森值）**\n损失函数是二分类的逻辑损失：$L(y_i, \\hat{y}_i) = -[y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)]$，其中 $p_i = \\sigma(\\hat{y}_i) = 1 / (1 + e^{-\\hat{y}_i})$ 并且 $\\hat{y}_i$ 是原始的对数几率得分。\n\n损失函数关于原始得分 $\\hat{y}_i$ 的一阶和二阶导数是：\n-   一阶导数（梯度），$g_i = \\frac{\\partial L}{\\partial \\hat{y}_i} = p_i - y_i$。\n-   二阶导数（海森值），$h_i = \\frac{\\partial^2 L}{\\partial \\hat{y}_i^2} = p_i (1-p_i)$。\n\n在第一轮开始时（$t=1$），对所有实例 $i$，模型预测为 $\\hat{y}_i^{(0)} = 0$。\n因此，对所有 $i$，初始概率为 $p_i = \\sigma(0) = 1 / (1+e^0) = 0.5$。\n梯度为 $g_i = 0.5 - y_i$。\n海森值为 $h_i = 0.5 \\times (1 - 0.5) = 0.25$，对所有 $i$ 均成立。\n\n让我们为数据集中的每个实例计算梯度 $g_i$：\n-   $i=1, y=0: g_1 = 0.5 - 0 = 0.5$\n-   $i=2, y=0: g_2 = 0.5 - 0 = 0.5$\n-   $i=3, y=1: g_3 = 0.5 - 1 = -0.5$\n-   $i=4, y=0: g_4 = 0.5 - 0 = 0.5$\n-   $i=5, y=1: g_5 = 0.5 - 1 = -0.5$\n-   $i=6, y=0: g_6 = 0.5 - 0 = 0.5$\n-   $i=7, y=1: g_7 = 0.5 - 1 = -0.5$\n-   $i=8, y=1: g_8 = 0.5 - 1 = -0.5$\n\n**2. 分裂增益计算**\n分裂的增益定义为目标函数的减少量。对于一个父节点 $I$ 分裂成一个左子节点 $I_L$ 和一个右子节点 $I_R$，其增益为：\n$$ \\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_I^2}{H_I + \\lambda} \\right] $$\n其中 $G = \\sum_{i \\in \\text{Node}} g_i$ 且 $H = \\sum_{i \\in \\text{Node}} h_i$。如果 $\\text{Gain} > \\gamma$，则接受分裂。\n\n首先，我们计算根节点的总和，该节点包含所有实例 $I = \\{1, 2, ..., 8\\}$：\n-   $G_I = \\sum_{i=1}^8 g_i = (0.5+0.5-0.5+0.5-0.5+0.5-0.5-0.5) = 4 \\times 0.5 + 4 \\times (-0.5) = 2 - 2 = 0$。\n-   $H_I = \\sum_{i=1}^8 h_i = 8 \\times 0.25 = 2$。\n\n现在，我们评估两种可能的分裂（在 $x_1$ 和 $x_2$上）。\n\n**在 $x_1$ 上分裂（阈值为 $0.5$）：**\n-   左子节点 $I_L$（$x_1=0$ 的实例）：$\\{1, 2, 3, 4\\}$。\n    -   $G_L = g_1+g_2+g_3+g_4 = 0.5+0.5-0.5+0.5 = 1$。\n    -   $H_L = 4 \\times 0.25 = 1$。\n-   右子节点 $I_R$（$x_1=1$ 的实例）：$\\{5, 6, 7, 8\\}$。\n    -   $G_R = g_5+g_6+g_7+g_8 = -0.5+0.5-0.5-0.5 = -1$。\n    -   $H_R = 4 \\times 0.25 = 1$。\n\n这个分裂的增益是：\n$$ \\text{Gain}(x_1) = \\frac{1}{2} \\left[ \\frac{1^2}{1 + 4} + \\frac{(-1)^2}{1 + 4} - \\frac{0^2}{2 + 4} \\right] = \\frac{1}{2} \\left[ \\frac{1}{5} + \\frac{1}{5} - 0 \\right] = \\frac{1}{2} \\times \\frac{2}{5} = \\frac{1}{5} = 0.2 $$\n我们必须检查 $\\text{Gain}(x_1) > \\gamma$ 是否成立。这里，$0.2 > 0.25$ 是 **假的**。这个分裂被拒绝。\n\n**在 $x_2$ 上分裂（阈值为 $0.5$）：**\n-   左子节点 $I_L$（$x_2=0$ 的实例）：$\\{1, 2, 5, 6\\}$。\n    -   $G_L = g_1+g_2+g_5+g_6 = 0.5+0.5-0.5+0.5 = 1$。\n    -   $H_L = 4 \\times 0.25 = 1$。\n-   右子节点 $I_R$（$x_2=1$ 的实例）：$\\{3, 4, 7, 8\\}$。\n    -   $G_R = g_3+g_4+g_7+g_8 = -0.5+0.5-0.5-0.5 = -1$。\n    -   $H_R = 4 \\times 0.25 = 1$。\n\n这个分裂的增益是：\n$$ \\text{Gain}(x_2) = \\frac{1}{2} \\left[ \\frac{1^2}{1 + 4} + \\frac{(-1)^2}{1 + 4} - \\frac{0^2}{2 + 4} \\right] = \\frac{1}{2} \\left[ \\frac{1}{5} + \\frac{1}{5} - 0 \\right] = \\frac{1}{5} = 0.2 $$\n我们必须检查 $\\text{Gain}(x_2) > \\gamma$ 是否成立。同样，$0.2 > 0.25$ 是 **假的**。这个分裂也被拒绝。\n\n**结论:** 在第一个提升轮次，没有分裂被接受，因为任何可能分裂的增益（$0.2$）都没有超过正则化阈值 $\\gamma=0.25$。模型由于正则化过强而欠拟合。第一棵树将是一个只有根节点的树桩。\n\n### 逐项分析选项\n\n**A. 将 $\\gamma$ 降低到由原始分裂增益决定的阈值以下（例如，在此数据集中使 $\\gamma  0.2$）或减小 $\\lambda$ 将会增加净分裂增益并可能使分裂得以发生；这直接解决了由过强正则化引起的欠拟合问题。**\n-   接受分裂的条件是 $\\text{Gain} > \\gamma$。在我们的例子中，最佳增益是 $0.2$。如果我们设置 $\\gamma  0.2$ （例如 $\\gamma=0.15$），那么条件 $0.2 > 0.15$ 将为真，分裂将被接受。这使得模型能够学习。\n-   增益公式 $\\text{Gain} = \\frac{1}{2} [ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_I^2}{H_I + \\lambda} ]$ 表明增益是 $\\lambda$ 的递减函数（因为 $\\lambda$ 在正项的分母中）。减小 $\\lambda$ 会增加增益。例如，如果我们将 $\\lambda$ 减小到 $1$，增益将变为 $\\frac{1}{2} [\\frac{1^2}{1+1} + \\frac{(-1)^2}{1+1} - 0] = \\frac{1}{2} [ 0.5+0.5 ] = 0.5$。由于 $0.5 > 0.25$，分裂将被接受。\n-   两种操作（降低 $\\gamma$ 或减小 $\\lambda$）都可以使先前被拒绝的分裂得以发生，从而让模型更好地拟合数据，因此解决了由正则化引起的欠拟合问题。该陈述与 XGBoost 的机制完全一致。\n-   结论：**正确**。\n\n**B. 在保持 $\\gamma = 0.25$ 的同时增加提升轮次 $T$ 的数量，将可靠地克服欠拟合，因为即使每棵单独的树都没有分裂，集成模型也可以累积改进。**\n-   如果第一棵树没有分裂，它只是一个根节点。这个节点的最优权重是 $w_1^* = -G_I / (H_I + \\lambda) = -0 / (2+4) = 0$。第一棵树是 $f_1(x)=0$。\n-   模型更新为 $\\hat{y}_i^{(1)} = \\hat{y}_i^{(0)} + \\eta f_1(x_i) = 0 + \\eta \\cdot 0 = 0$。预测值不发生改变。\n-   对于第二轮（$t=2$），梯度 $g_i$ 和海森值 $h_i$ 是基于 $\\hat{y}_i^{(1)}=0$ 计算的。它们将与第一轮中的完全相同。\n-   因此，第二棵树的增益计算将是相同的，同样不会有分裂发生。第二棵树也将是 $f_2(x)=0$。\n-   这个过程会无限重复。模型预测永远不会脱离初始状态。如果每棵新树的贡献都为零，集成模型就无法累积改进。\n-   结论：**不正确**。\n\n**C. 在这里，一种检测欠拟合的实用方法是监控训练目标和验证指标：如果几轮过后，两者都在基线附近停滞不前，并且每棵树都只有一个叶子节点（没有接受任何分裂），这表明是由正则化阈值引起的欠拟合。**\n-   正如在对B的分析中所示，如果没有分裂发生，模型就永远不会学习。预测值保持在基线水平（$p_i=0.5$）。\n-   这意味着训练损失将恒定在其初始值，例如 $\\text{logloss} = -\\ln(0.5) \\approx 0.693$。任何验证指标（准确率、AUC等）也将恒定在其基线值。\n-   观察性能指标的这种停滞现象，再结合检查所构建的树并发现它们都是单节点“树桩”，正是这种特定失败模式的典型诊断特征：由正则化参数（$\\lambda$、$\\gamma$、`min_child_weight`等）过于严格而导致的欠拟合。\n-   结论：**正确**。\n\n**D. 将 $x_1$ 和 $x_2$ 标准化为零均值和单位方差，其本身将在不改变 $\\lambda$ 或 $\\gamma$ 的情况下，根据二阶准则增加分裂增益并克服 $\\gamma$ 阈值。**\n-   决策树算法，包括 XGBoost，对单个特征的单调变换是不变的。标准化就是这样一种变换。\n-   特征 $x_1$ 和 $x_2$ 是二元的，取值于 $\\{0, 1\\}$。对此类特征进行分裂，例如 $x_1  c$（对于任何 $0  c  1$），会将所有 $x_1=0$ 的实例分到一边，所有 $x_1=1$ 的实例分到另一边。标准化后，特征值会改变，但分裂仍然会将完全相同的实例集分开。\n-   由于增益计算仅取决于分裂后子节点中的梯度和海森值总和（$G_L, H_L, G_R, H_R$），而这些总和仅取决于哪些实例被分配到哪个子节点，所以增益对于特征的单调变换是不变的。\n-   结论：**不正确**。\n\n**E. 在保持 $\\gamma = 0.25$ 和 $\\lambda = 4$ 的同时，将最大深度 $d_{\\max}$ 从 1 增加到 3，可以保证第一次分裂被接受，因为更深的树有更多的叶子，从而抵消了分裂惩罚。**\n-   XGBoost采用贪心算法逐层构建树。如果根节点（深度为0）无法找到一个增益大于 $\\gamma$ 的有效分裂，那么树的生长就会立即停止。无论 `max_depth` 设置得有多大，都无法进行第一次分裂。\n-   增加 `max_depth` 只是为树的生长提供了更大的潜力，但它并不能强制接受一个本身不满足增益条件的初始分裂。\n-   结论：**不正确**。",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "在掌握了分析和调试XGBoost分裂过程之后，我们更进一步，从分析转向构建。这个练习要求你修改核心的分裂逻辑，以强制实现单调性约束，这在许多领域（如金融风控、医疗诊断）中对于模型的可解释性和可靠性至关重要。通过这个实践，你将体会到XGBoost框架的灵活性和可扩展性。 ",
            "id": "3120326",
            "problem": "要求您以 eXtreme Gradient Boosting (XGBoost) 的风格实现一个带约束的加性树模型，该模型使用平方误差的二阶代理进行训练，同时对选定特征强制施加单调偏依赖。目标是确保对于一个指定的特征索引子集，模型的偏导数是非负的，这一点通过前向有限差分进行数值验证。您的程序必须是一个完整、可运行的程序，能够构建合成数据、训练模型，并在一个小网格上通过有限差分来验证单调性。\n\n此任务的基本基础包括以下经过充分检验的定义和事实：\n- 提升加性模型形式为 $f(\\mathbf{x}) = \\sum_{m=1}^{M} \\eta\\, h_{m}(\\mathbf{x})$，其中 $\\eta$ 是学习率，$h_{m}$ 是通过最小化训练目标的二阶代理以贪婪方式添加的弱学习器。\n- 对于平方误差目标 $L = \\tfrac{1}{2}\\sum_{i=1}^{n}(y_{i} - f(\\mathbf{x}_{i}))^{2}$，每个样本相对于当前预测的一阶导数是 $g_{i} = \\partial L/\\partial f(\\mathbf{x}_{i})$，二阶导数是 $h_{i} = \\partial^{2} L/\\partial f(\\mathbf{x}_{i})^{2}$。\n- eXtreme Gradient Boosting (XGBoost) 使用目标函数的二阶泰勒展开，并对叶节点权重施加 $L_{2}$ 正则化，选择能够最大化近似目标函数减少量的分裂点。\n\n模型设计要求：\n- 使用决策桩（深度为一的树）作为弱学习器 $h_{m}(\\mathbf{x})$。\n- 在每个提升轮次中，考虑形式为 $x_{j} \\le \\tau$ 与 $x_{j}  \\tau$ 的分裂，其中 $j$ 遍历所有特征，$\\tau$ 是一个候选阈值。候选阈值必须选择为训练集中该特征排序后连续不同值的中点。\n- 对于每个候选分裂，计算能最小化带叶节点权重 $L_{2}$ 正则化的平方误差二阶代理的最优叶节点权重，同时受限于下述的单调性约束。根据所选叶节点权重下的实际目标减少量，从所有特征和阈值中选择唯一的最佳分裂。\n- 对选定特征强制施加单调非递减的偏依赖。对于任何在受约束特征索引 $j$ 上进行分裂的决策桩，必须强制要求右叶节点的预测值不小于左叶节点的预测值，即区域 $\\{x_{j}  \\tau\\}$ 的叶节点权重必须大于或等于区域 $\\{x_{j} \\le \\tau\\}$ 的叶节点权重。如果无约束最优解违反此不等式，则将两个叶节点权重替换为一个在等式约束下能够最小化代理函数的共同值。这保证了每个决策桩在受约束特征上是单調非递減的，并且这些决策桩的和在这些特征上也是单調非递減的。\n- 使用 $L_{2}$ 正则化，其正则化参数为 $\\lambda$，学习率为 $\\eta$，以及一个分裂复杂度参数 $\\gamma$，该参数会从每次分裂的目标减少量中减去。仅当计算出的减少量严格大于 $0$ 时才接受分裂。\n\n数值验证要求：\n- 对于每个测试用例和每个受约束的特征索引 $j$，使用前向有限差分数值验证偏依赖是单调非递减的。将所有非 $j$ 特征固定在其样本均值上，构建一个包含 $N$ 个点的均匀网格，该网格跨越训练集中 $x_{j}$ 的经验范围，并检查对于沿 $x_{j}$ 的所有连续网格点 $\\mathbf{x}^{(k)}$ 和 $\\mathbf{x}^{(k+1)}$，是否有 $f(\\mathbf{x}^{(k+1)}) - f(\\mathbf{x}^{(k)}) \\ge 0$ 成立，允许一个小的容差 $\\varepsilon$。三角函数中使用的角度必须是弧度。\n\n数据生成和测试套件：\n- 对于每个测试，您必须生成一个合成数据集 $\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$，其中 $\\mathbf{x}_{i} \\in [0,1]^{d}$ 是独立均匀采样的，$y_{i}$ 由指定的带噪声的单调或非单调关系生成。标准差为 $\\sigma$ 的独立高斯噪声被添加到 $y_{i}$ 中。\n- 训练超参数包括：提升轮次数量 $M$、学习率 $\\eta$、$L_{2}$ 正则化参数 $\\lambda$、分裂复杂度 $\\gamma$ 以及用于有限差分的网格大小 $N$。请完全按照每个测试用例提供的 $\\eta$、$\\lambda$、$\\gamma$ 和 $M$ 值。在所有测试用例中，使用 $N = 25$ 和容差 $\\varepsilon = 10^{-12}$。\n\n提供以下测试套件（三角函数中的角度为弧度）：\n1. “顺利路径”用例，测试单个单调特征：\n   - $n = 120$, $d = 3$, seed $= 0$。\n   - $\\mathbf{x}_{i} \\sim \\mathrm{Uniform}([0,1]^{3})$。\n   - $y_{i} = 2.0\\,x_{i,0} + 0.4\\,\\sin(2\\pi\\,x_{i,1}) + 0.3\\,x_{i,2} + \\varepsilon_{i}$，其中 $\\varepsilon_{i} \\sim \\mathcal{N}(0, 0.05^{2})$。\n   - 单调特征集 $J = \\{0\\}$。\n   - $M = 30$, $\\eta = 0.3$, $\\lambda = 1.0$, $\\gamma = 0.0$。\n2. 多个单调特征：\n   - $n = 160$, $d = 4$, seed $= 1$。\n   - $\\mathbf{x}_{i} \\sim \\mathrm{Uniform}([0,1]^{4})$。\n   - $y_{i} = 1.5\\,x_{i,0} + 0.8\\,x_{i,2} + 0.2\\,\\cos(2\\pi\\,x_{i,1}) + 0.1\\,x_{i,3}^{2} + \\varepsilon_{i}$，其中 $\\varepsilon_{i} \\sim \\mathcal{N}(0, 0.05^{2})$。\n   - 单调特征集 $J = \\{0, 2\\}$。\n   - $M = 30$, $\\eta = 0.3$, $\\lambda = 1.0$, $\\gamma = 0.0$。\n3. 矛盾的单调性约束（确保约束机制能阻止有害的分裂）：\n   - $n = 140$, $d = 2$, seed $= 2$。\n   - $\\mathbf{x}_{i} \\sim \\mathrm{Uniform}([0,1]^{2})$。\n   - $y_{i} = -1.0\\,x_{i,1} + 0.3\\,x_{i,0} + \\varepsilon_{i}$，其中 $\\varepsilon_{i} \\sim \\mathcal{N}(0, 0.05^{2})$。\n   - 单调特征集 $J = \\{1\\}$。\n   - $M = 30$, $\\eta = 0.3$, $\\lambda = 1.0$, $\\gamma = 0.0$。\n4. 信号基本恒定的边界情况：\n   - $n = 100$, $d = 2$, seed $= 3$。\n   - $\\mathbf{x}_{i} \\sim \\mathrm{Uniform}([0,1]^{2})$。\n   - $y_{i} = 0.0 + \\varepsilon_{i}$，其中 $\\varepsilon_{i} \\sim \\mathcal{N}(0, 0.05^{2})$。\n   - 单调特征集 $J = \\{0\\}$。\n   - $M = 30$, $\\eta = 0.3$, $\\lambda = 1.0$, $\\gamma = 0.0$。\n\n程序输出规范：\n- 您的程序必须生成一行输出，其中包含一个方括号括起来的逗号分隔列表，每个测试用例对应一个布尔值，顺序与上面列出的相同。例如，形如 $[\\text{true},\\text{true},\\text{false},\\text{true}]$ 的输出是可以接受的，但您必须使用 Python 的小写布尔字面量（`true`, `false`）。布尔值表示在该测试用例中，所有受约束特征是否都通过了单调性有限差分检查。",
            "solution": "该问题陈述已经过验证，被认为是可靠的。它描述了统计学习中的一个良构任务，为实现带约束的梯度提升算法提供了一套完整且一致的要求。其科学基础是已建立的梯度树提升理论，所有参数和过程都得到了形式化和客观的规定。\n\n该任务是实现一个自定义的梯度提升模型，使用决策桩作为弱学习器，在平方误差损失函数的二阶近似上进行训练。一个关键特性是在指定的特征子集上强制执行单调性约束。\n\n该模型是一个加性集成的形式：\n$$\nf(\\mathbf{x}) = \\sum_{m=1}^{M} \\eta h_m(\\mathbf{x})\n$$\n其中 $f(\\mathbf{x})$ 是对输入向量 $\\mathbf{x}$ 的最终预测，$M$ 是提升轮数，$\\eta$ 是学习率，$h_m(\\mathbf{x})$ 是在第 $m$ 次迭代中添加的弱学习器（一个决策桩）。模型以贪婪方式构建。在每次迭代 $m$ 中，我们试图找到一个决策桩 $h_m$ 来最小化训练目标。设 $f^{(m-1)}(\\mathbf{x}_i)$ 是模型经过 $m-1$ 轮后对数据点 $i$ 的预测。模型更新为 $f^{(m)}(\\mathbf{x}_i) = f^{(m-1)}(\\mathbf{x}_i) + \\eta h_m(\\mathbf{x}_i)$。\n\n目标函数是正则化的平方误差损失。为了找到最优的 $h_m$，我们围绕当前预测 $f^{(m-1)}(\\mathbf{x}_i)$ 对损失函数 $L = \\sum_{i=1}^n \\frac{1}{2}(y_i - f(\\mathbf{x}_i))^2$ 进行二阶泰勒展开。去掉常数项，第 $m$ 步为树 $h_m$ 最小化的目标变为：\n$$\n\\tilde{\\mathcal{L}}^{(m)} = \\sum_{i=1}^{n} \\left[ g_i h_m(\\mathbf{x}_i) + \\frac{1}{2} h_i h_m(\\mathbf{x}_i)^2 \\right] + \\Omega(h_m)\n$$\n其中，$g_i = \\frac{\\partial L}{\\partial f^{(m-1)}} = f^{(m-1)}(\\mathbf{x}_i) - y_i$ 是梯度，$h_i = \\frac{\\partial^2 L}{(\\partial f^{(m-1)})^2} = 1$ 是平方误差损失的海森值（Hessian）。正则化项是 $\\Omega(h_m) = \\gamma + \\frac{1}{2}\\lambda \\sum_{j=1}^{T} w_j^2$，其中 $T$ 是树中的叶节点数，$w_j$ 是叶节点 $j$ 的权重，$\\lambda$ 是 $L_2$ 正则化参数，$\\gamma$ 是分裂复杂度代价。由于我们使用决策桩，所以 $T=2$。\n\n一个决策桩将一个节点的实例集 $I$ 划分为一个左集合 $I_L$ 和一个右集合 $I_R$。函数 $h_m(\\mathbf{x})$ 对于 $\\mathbf{x} \\in I_L$取值为 $w_L$，对于 $\\mathbf{x} \\in I_R$ 取值为 $w_R$。目标函数可以通过按叶节点对实例进行分组来重写：\n$$\n\\tilde{\\mathcal{L}}^{(m)} = \\left[ w_L \\sum_{i \\in I_L} g_i + \\frac{1}{2}w_L^2 \\left(\\sum_{i \\in I_L} h_i \\right) \\right] + \\left[ w_R \\sum_{i \\in I_R} g_i + \\frac{1}{2}w_R^2 \\left(\\sum_{i \\in I_R} h_i \\right) \\right] + \\frac{1}{2}\\lambda(w_L^2+w_R^2)\n$$\n设 $G_L = \\sum_{i \\in I_L} g_i$，$H_L = \\sum_{i \\in I_L} h_i$，右叶节点同理。目标函数是：\n$$\n\\tilde{\\mathcal{L}}^{(m)} = \\left[ G_L w_L + \\frac{1}{2}(H_L+\\lambda)w_L^2 \\right] + \\left[ G_R w_R + \\frac{1}{2}(H_R+\\lambda)w_R^2 \\right]\n$$\n这是关于 $w_L$ 和 $w_R$ 的二次函数。无约束的最优权重通过将偏导数设为零得到：\n$$\nw_L^* = -\\frac{G_L}{H_L+\\lambda} \\quad \\text{和} \\quad w_R^* = -\\frac{G_R}{H_R+\\lambda}\n$$\n该分裂对应的最小目标值为 $\\tilde{\\mathcal{L}}^* = -\\frac{1}{2}\\left(\\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda}\\right)$。分裂的质量通过目标函数的减少量（或称为增益）来衡量。无约束分裂的增益为：\n$$\n\\text{Gain} = \\frac{1}{2}\\left[\\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right]\n$$\n只有当此增益减去 $\\gamma$ 后严格为正时，才接受该分裂。\n\n问题的核心在于对指定集合 $J$ 中的特征 $j$ 施加单调性约束。对于在此类特征上分裂的决策桩，我们必须确保 $w_R \\ge w_L$。其逻辑如下：\n1. 计算无约束的最优权重 $w_L^*$ 和 $w_R^*$。\n2. 如果 $w_R^* \\ge w_L^*$，则约束自然满足。我们使用无约束的权重并如上所示计算增益。\n3. 如果 $w_R^*  w_L^*$，则违反了约束。我们必须在等式约束 $w_L = w_R = w$ 下找到最优权重。问题变为在此新约束下最小化 $\\tilde{\\mathcal{L}}^{(m)}$。目标函数变为：\n    $$\n    \\tilde{\\mathcal{L}}_{\\text{constrained}}(w) = (G_L+G_R)w + \\frac{1}{2}(H_L+H_R)w^2 + \\frac{1}{2}\\lambda(w^2+w^2) = (G_L+G_R)w + \\frac{1}{2}(H_L+H_R+2\\lambda)w^2\n    $$\n    最优的共同权重是：\n    $$\n    w_c = -\\frac{G_L+G_R}{H_L+H_R+2\\lambda}\n    $$\n    对于这些约束权重，实际的目标减少量需要与父节点（未分裂）的目标值 $-\\frac{1}{2} \\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}$进行比较。这种情况下，分裂后的目标值为 $-\\frac{1}{2}\\frac{(G_L+G_R)^2}{H_L+H_R+2\\lambda}$。由于$\\lambda > 0$，分裂后的目标值会比未分裂时差，导致负的或零的增益，从而有效地阻止了这种违反约束的分裂被选中。通过最大化（增益 - $\\gamma$），从所有特征和所有有效阈值中选择最佳分裂。\n\n训练算法如下：\n1. 对所有 $i=1, ..., n$，初始化预测值 $f^{(0)}(\\mathbf{x}_i) = 0$。\n2. 对于 $m = 1, ..., M$：\n    a. 计算梯度 $g_i = f^{(m-1)}(\\mathbf{x}_i) - y_i$ 和海森值 $h_i=1$。\n    b. 通过遍历所有可能性并计算增益来找到最佳分裂（特征、阈值），并应用单调性逻辑。通过首先对每个特征的值进行排序，使用了一种高效的 $O(n \\log n)$ 方法。\n    c. 如果最佳分裂的（增益 - $\\gamma$）大于 $0$，则创建决策桩 $h_m$ 并更新预测：$f^{(m)}(\\mathbf{x}_i) = f^{(m-1)}(\\mathbf{x}_i) + \\eta h_m(\\mathbf{x}_i)$。\n    d. 如果没有分裂的（增益 - $\\gamma$）为正，则此轮不添加任何树。\n\n最后，对于每个受约束的特征 $j \\in J$，通过构建一个跨越该特征范围的 $N=25$ 个点的网格（同时将其他特征保持在其均值），并检查模型的输出在连续网格点上是否单调非递减（允许一个容差 $\\varepsilon$），来对训练后模型的单调性进行数值验证。",
            "answer": "```python\nimport numpy as np\n\nclass MonotonicStumpBooster:\n    \"\"\"\n    A simplified gradient boosting model with decision stumps that enforces\n    monotonicity constraints on selected features.\n    \"\"\"\n\n    class Stump:\n        \"\"\"Represents a single decision stump.\"\"\"\n        def __init__(self, feature_idx, threshold, w_left, w_right):\n            self.feature_idx = feature_idx\n            self.threshold = threshold\n            self.w_left = w_left\n            self.w_right = w_right\n\n        def predict(self, X):\n            \"\"\"Predicts leaf weights for input data X.\"\"\"\n            predictions = np.full(X.shape[0], self.w_left)\n            if self.feature_idx is not None:\n                mask = X[:, self.feature_idx] > self.threshold\n                predictions[mask] = self.w_right\n            return predictions\n\n    def __init__(self, M, eta, lambda_reg, gamma, monotone_features):\n        self.M = M\n        self.eta = eta\n        self.lambda_reg = lambda_reg\n        self.gamma = gamma\n        self.monotone_features = set(monotone_features)\n        self.stumps = []\n        self.initial_prediction = 0.0\n\n    def fit(self, X, y):\n        \"\"\"Trains the model on data X and labels y.\"\"\"\n        n_samples, _ = X.shape\n        # Predictions start from 0 for simplicity and to match common implementations.\n        self.initial_prediction = 0.0 \n        current_predictions = np.full(n_samples, self.initial_prediction)\n\n        for _ in range(self.M):\n            gradients = current_predictions - y\n            hessians = np.ones(n_samples) # For squared error\n\n            best_stump_params = self._find_best_split(X, gradients, hessians)\n\n            if best_stump_params:\n                stump = self.Stump(*best_stump_params)\n                self.stumps.append(stump)\n                current_predictions += self.eta * stump.predict(X)\n\n    def _find_best_split(self, X, g, h):\n        \"\"\"Finds the best stump by iterating over all features and thresholds.\"\"\"\n        n_samples, n_features = X.shape\n        best_gain = 0.0\n        best_split = None\n\n        G_total = np.sum(g)\n        H_total = np.sum(h)\n        \n        parent_score = -0.5 * (G_total**2) / (H_total + self.lambda_reg)\n\n        for j in range(n_features):\n            feature_values = X[:, j]\n            sorted_indices = np.argsort(feature_values)\n            g_sorted, h_sorted, f_sorted = g[sorted_indices], h[sorted_indices], feature_values[sorted_indices]\n            \n            G_left, H_left = 0.0, 0.0\n            \n            for i in range(n_samples - 1):\n                G_left += g_sorted[i]\n                H_left += h_sorted[i]\n\n                if f_sorted[i] == f_sorted[i+1]:\n                    continue\n                \n                threshold = (f_sorted[i] + f_sorted[i+1]) / 2.0\n                G_right, H_right = G_total - G_left, H_total - H_left\n                \n                if H_left  1e-6 or H_right  1e-6:\n                    continue\n\n                w_left_unc = -G_left / (H_left + self.lambda_reg)\n                w_right_unc = -G_right / (H_right + self.lambda_reg)\n                \n                w_left, w_right = w_left_unc, w_right_unc\n                \n                is_constrained = j in self.monotone_features\n                if is_constrained and w_right_unc  w_left_unc:\n                    w_common = -(G_left + G_right) / (H_left + H_right + 2 * self.lambda_reg)\n                    w_left, w_right = w_common, w_common\n                    # The objective for this constrained split. Sum of objectives for two leaves.\n                    split_score = (G_left + G_right) * w_common + 0.5 * (H_left + H_right + 2 * self.lambda_reg) * w_common**2\n                else:\n                    # Unconstrained or constraint-satisfying split\n                    split_score = -0.5 * (G_left**2 / (H_left + self.lambda_reg) +\n                                      G_right**2 / (H_right + self.lambda_reg))\n\n                gain = parent_score - split_score # Reduction in objective\n                final_gain = gain - self.gamma\n                if final_gain > best_gain:\n                    best_gain = final_gain\n                    best_split = (j, threshold, w_left, w_right)\n\n        return best_split\n    \n    def predict(self, X):\n        \"\"\"Makes predictions for new data X.\"\"\"\n        predictions = np.full(X.shape[0], self.initial_prediction)\n        for stump in self.stumps:\n            predictions += self.eta * stump.predict(X)\n        return predictions\n\ndef run_test_case(params):\n    \"\"\"\n    Generates data, trains the model, and verifies monotonicity for a single test case.\n    \"\"\"\n    n, d, seed, y_func_str, sigma, J, M, eta, lambda_reg, gamma = params\n    \n    rng = np.random.default_rng(seed)\n    X_train = rng.uniform(0, 1, size=(n, d))\n    \n    # Eval y_func_str requires access to np\n    y_true = eval(y_func_str, {\"np\": np, \"x\": X_train.T})\n    noise = rng.normal(0, sigma, size=n)\n    y_train = y_true + noise\n\n    # Train model\n    model = MonotonicStumpBooster(M=M, eta=eta, lambda_reg=lambda_reg, gamma=gamma, monotone_features=J)\n    model.fit(X_train, y_train)\n\n    # Verify monotonicity\n    N = 25\n    tolerance = 1e-12\n    is_monotone = True\n\n    if not J:\n        return True\n\n    for j in J:\n        means = np.mean(X_train, axis=0)\n        grid_j = np.linspace(np.min(X_train[:, j]), np.max(X_train[:, j]), N)\n        \n        X_test = np.tile(means, (N, 1))\n        X_test[:, j] = grid_j\n        \n        y_pred = model.predict(X_test)\n        \n        diffs = np.diff(y_pred)\n        if not np.all(diffs >= -tolerance):\n            is_monotone = False\n            break\n    \n    return is_monotone\n\ndef solve():\n    test_cases = [\n        (120, 3, 0, '2.0*x[0] + 0.4*np.sin(2*np.pi*x[1]) + 0.3*x[2]', 0.05, {0}, 30, 0.3, 1.0, 0.0),\n        (160, 4, 1, '1.5*x[0] + 0.8*x[2] + 0.2*np.cos(2*np.pi*x[1]) + 0.1*x[3]**2', 0.05, {0, 2}, 30, 0.3, 1.0, 0.0),\n        (140, 2, 2, '-1.0*x[1] + 0.3*x[0]', 0.05, {1}, 30, 0.3, 1.0, 0.0),\n        (100, 2, 3, '0.0 * x[0]', 0.05, {0}, 30, 0.3, 1.0, 0.0)\n    ]\n\n    results = []\n    for case in test_cases:\n        # A small correction to the last test case's y_func to avoid numpy warnings\n        # with plain '0.0', ensuring it depends on 'x'.\n        if case[2] == 3:\n            y_func_str = '0.0 * x[0]'\n            case = (case[0], case[1], case[2], y_func_str, *case[4:])\n\n        result = run_test_case(case)\n        results.append(str(result).lower()) # convert Python bool to lowercase string\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}