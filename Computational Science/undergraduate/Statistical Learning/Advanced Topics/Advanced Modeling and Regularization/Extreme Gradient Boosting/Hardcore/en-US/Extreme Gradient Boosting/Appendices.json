{
    "hands_on_practices": [
        {
            "introduction": "This exercise peels back the layers of the XGBoost algorithm to reveal its optimization engine. By deriving the gradient and Hessian for the multi-class objective from first principles , you will gain a concrete understanding of how the model quantifies error at each step. Applying these concepts to a small dataset will solidify your grasp of how individual trees are constructed to correct the residual errors of the ensemble.",
            "id": "3120267",
            "problem": "Consider eXtreme Gradient Boosting (XGBoost), which optimizes a regularized objective by fitting decision trees to second-order Taylor expansions of the loss. For multi-class classification with $K$ classes, XGBoost uses the softmax model with class scores $f_{ik}$ for instance $i$ and class $k$, and probabilities defined by the softmax transform $p_{ik} = \\frac{\\exp(f_{ik})}{\\sum_{j=1}^{K} \\exp(f_{ij})}$. The per-instance loss is the negative log-likelihood of the true class under the softmax probabilities. \n\nStarting only from these definitions, derive expressions for the per-instance gradient $g_{ik}$ and the per-instance diagonal second derivative (the diagonal of the Hessian) $h_{ik}$ of the loss with respect to the score $f_{ik}$.\n\nThen, test your derivation on the following $3$-class toy dataset. There are $N=3$ training instances in a single leaf, with class labels\n$$\ny_1 = 1,\\quad y_2 = 1,\\quad y_3 = 3.\n$$\nAssume the current boosting iteration uses zero initialization for all class scores, so $f_{ik} = 0$ for all $i$ and $k$. Let the $\\ell_2$ regularization parameter be $\\lambda = \\frac{1}{2}$.\n\nUsing the second-order leaf-weight solution employed by XGBoost for class $k=2$, compute the optimal leaf weight\n$$\nw^{\\ast} = -\\frac{\\sum_{i=1}^{N} g_{i2}}{\\sum_{i=1}^{N} h_{i2} + \\lambda}.\n$$\n\nReport the value of $w^{\\ast}$ as a single number. No rounding is required.",
            "solution": "We begin from the fundamental definitions for multi-class logistic (softmax) modeling. For an instance $i$ with scores $\\mathbf{f}_i = (f_{i1}, \\dots, f_{iK})$, the softmax probability for class $k$ is\n$$\np_{ik} = \\frac{\\exp(f_{ik})}{\\sum_{j=1}^{K} \\exp(f_{ij})}.\n$$\nThe negative log-likelihood (cross-entropy) loss for instance $i$ and true class $y_i$ is\n$$\nL_i(\\mathbf{f}_i) = -\\ln\\left(p_{i, y_i}\\right) = -\\sum_{k=1}^{K} \\mathbf{1}[y_i = k] \\ln(p_{ik}),\n$$\nwhere $\\mathbf{1}[\\cdot]$ is the indicator function.\n\nWe derive the gradient $\\frac{\\partial L_i}{\\partial f_{ik}}$ using the chain rule. First note that $L_i = -\\sum_{k} y_{ik} \\ln(p_{ik})$ where $y_{ik} = \\mathbf{1}[y_i=k]$. The derivative of $\\ln(p_{ik})$ with respect to $f_{il}$ uses the softmax derivative. The softmax Jacobian is well known and can be derived as follows:\n$$\np_{ik} = \\frac{\\exp(f_{ik})}{Z_i},\\quad \\text{with}\\quad Z_i = \\sum_{j=1}^{K} \\exp(f_{ij}).\n$$\nThen\n$$\n\\frac{\\partial p_{ik}}{\\partial f_{il}} = \\frac{\\exp(f_{ik})}{Z_i}\\left(\\delta_{kl} - \\frac{\\exp(f_{il})}{Z_i}\\right) = p_{ik}(\\delta_{kl} - p_{il}),\n$$\nwhere $\\delta_{kl}$ is the Kronecker delta. Consequently,\n$$\n\\frac{\\partial \\ln(p_{ik})}{\\partial f_{il}} = \\frac{1}{p_{ik}}\\frac{\\partial p_{ik}}{\\partial f_{il}} = \\delta_{kl} - p_{il}.\n$$\nThus, the gradient of $L_i$ with respect to $f_{il}$ is\n$$\n\\frac{\\partial L_i}{\\partial f_{il}} = -\\sum_{k=1}^{K} y_{ik}\\left(\\delta_{kl} - p_{il}\\right) = -y_{il} + \\left(\\sum_{k=1}^{K} y_{ik}\\right)p_{il}.\n$$\nSince $\\sum_{k=1}^{K} y_{ik} = 1$, we have\n$$\n\\frac{\\partial L_i}{\\partial f_{il}} = p_{il} - y_{il} = p_{il} - \\mathbf{1}[y_i=l].\n$$\nRelabeling the index $l$ as $k$ for clarity, the per-instance gradient is\n$$\ng_{ik} = \\frac{\\partial L_i}{\\partial f_{ik}} = p_{ik} - \\mathbf{1}[y_i = k].\n$$\n\nNext, we derive the Hessian. The Hessian with respect to $\\mathbf{f}_i$ is the matrix with entries\n$$\n\\frac{\\partial^2 L_i}{\\partial f_{ik}\\,\\partial f_{il}} = \\frac{\\partial}{\\partial f_{il}}\\left(p_{ik} - y_{ik}\\right) = \\frac{\\partial p_{ik}}{\\partial f_{il}} = p_{ik}\\left(\\delta_{kl} - p_{il}\\right).\n$$\nTherefore, the full Hessian for instance $i$ is\n$$\nH_i = \\operatorname{diag}(\\mathbf{p}_i) - \\mathbf{p}_i \\mathbf{p}_i^{\\top},\n$$\nwhere $\\mathbf{p}_i = (p_{i1},\\dots,p_{iK})$. The diagonal entries are\n$$\n\\frac{\\partial^2 L_i}{\\partial f_{ik}^2} = p_{ik}(1 - p_{ik}).\n$$\nIn XGBoostâ€™s per-class tree construction, the second-order term used for class $k$ is the diagonal element for that class, so the per-instance diagonal second derivative is\n$$\nh_{ik} = p_{ik}(1 - p_{ik}).\n$$\n\nWe now test on the specified $3$-class dataset with $N=3$ instances and labels $y_1=1$, $y_2=1$, $y_3=3$. The current scores are $f_{ik}=0$ for all $i$ and $k$. Under this initialization,\n$$\np_{ik} = \\frac{\\exp(0)}{\\exp(0)+\\exp(0)+\\exp(0)} = \\frac{1}{3}\\quad \\text{for all } i,k.\n$$\n\nCompute the per-instance gradient for class $k=2$:\n- For instance $i=1$, $y_1=1 \\neq 2$, so $\\mathbf{1}[y_1=2]=0$ and $g_{12} = \\frac{1}{3} - 0 = \\frac{1}{3}$.\n- For instance $i=2$, $y_2=1 \\neq 2$, so $g_{22} = \\frac{1}{3} - 0 = \\frac{1}{3}$.\n- For instance $i=3$, $y_3=3 \\neq 2$, so $g_{32} = \\frac{1}{3} - 0 = \\frac{1}{3}$.\n\nAggregate gradient for class $2$ over the leaf:\n$$\nG_2 = \\sum_{i=1}^{3} g_{i2} = \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 1.\n$$\n\nCompute the per-instance diagonal second derivative for class $k=2$:\n$$\nh_{i2} = p_{i2}(1 - p_{i2}) = \\frac{1}{3}\\left(1 - \\frac{1}{3}\\right) = \\frac{1}{3}\\cdot \\frac{2}{3} = \\frac{2}{9}.\n$$\nAggregate Hessian for class $2$ over the leaf:\n$$\nH_2 = \\sum_{i=1}^{3} h_{i2} = 3 \\cdot \\frac{2}{9} = \\frac{6}{9} = \\frac{2}{3}.\n$$\n\nWith $\\lambda = \\frac{1}{2}$, the second-order optimal leaf weight for class $2$ is\n$$\nw^{\\ast} = -\\frac{G_2}{H_2 + \\lambda} = -\\frac{1}{\\frac{2}{3} + \\frac{1}{2}} = -\\frac{1}{\\frac{4}{6} + \\frac{3}{6}} = -\\frac{1}{\\frac{7}{6}} = -\\frac{6}{7}.\n$$\n\nThis is a single exact rational number, so no rounding is required.",
            "answer": "$$\\boxed{-\\frac{6}{7}}$$"
        },
        {
            "introduction": "A powerful model is only useful if it can learn, and sometimes, aggressive regularization can bring training to a halt. This practice problem  simulates a classic case of underfitting, where the model fails to make even a single split. By calculating the split gain and comparing it against the regularization penalty, you will learn to diagnose this critical issue and reason about the hyperparameters that control the learning process.",
            "id": "3120315",
            "problem": "You are training an eXtreme Gradient Boosting (XGBoost) classifier, a form of gradient boosting decision trees (GBDT), on a binary classification task with logistic loss. The training uses a second-order approximation of the objective and includes standard tree regularization: an $\\ell_2$ penalty on leaf weights controlled by parameter $\\lambda$ and a per-split minimum loss reduction threshold controlled by parameter $\\gamma$. Consider the first boosting round starting from a constant score $f(x) = 0$ (so the initial class probability is $p_i = 0.5$ for every training instance), uniform sample weights, and assume the maximum depth is $d_{\\max} = 1$ at this round.\n\nYou are given the dataset with features $x_1, x_2 \\in \\{0,1\\}$ and labels $y \\in \\{0,1\\}$:\n- $i = 1$: $(x_1, x_2, y) = (0, 0, 0)$\n- $i = 2$: $(x_1, x_2, y) = (0, 0, 0)$\n- $i = 3$: $(x_1, x_2, y) = (0, 1, 1)$\n- $i = 4$: $(x_1, x_2, y) = (0, 1, 0)$\n- $i = 5$: $(x_1, x_2, y) = (1, 0, 1)$\n- $i = 6$: $(x_1, x_2, y) = (1, 0, 0)$\n- $i = 7$: $(x_1, x_2, y) = (1, 1, 1)$\n- $i = 8$: $(x_1, x_2, y) = (1, 1, 1)$\n\nHyperparameters at this round are $\\lambda = 4$ and $\\gamma = 0.25$. Assume no other constraints are active (for example, the minimum child weight threshold is small enough that it does not block splits).\n\nUsing only the core definitions of gradient boosting with second-order approximation and tree regularization (do not assume any shortcut formulas), reason from first principles about the aggregated first and second derivatives on candidate nodes to decide whether any single split on $x_1$ or $x_2$ can pass the regularization threshold at initialization. Then, answer the following multiple-choice question about how to detect and remedy possible underfitting in this setting.\n\nWhich statements are true?\n\nA. Lowering $\\gamma$ below a threshold determined by the raw split improvement (for example, making $\\gamma  0.2$ in this dataset) or decreasing $\\lambda$ will increase the net split gain and can enable splits; this directly addresses underfitting caused by overly strong regularization.\n\nB. Increasing the number of boosting rounds $T$ while keeping $\\gamma = 0.25$ will reliably overcome the underfitting, because the ensemble can accumulate improvements even if every individual tree has no splits.\n\nC. A practical way to detect the underfitting here is to monitor the training objective and validation metric: if both plateau near the baseline after several rounds and each tree has only a single leaf (no accepted splits), this indicates underfitting induced by regularization thresholds.\n\nD. Standardizing $x_1$ and $x_2$ to zero mean and unit variance will, by itself, increase the split gain under the second-order criterion and overcome the $\\gamma$ threshold without changing $\\lambda$ or $\\gamma$.\n\nE. Increasing the maximum depth $d_{\\max}$ from $1$ to $3$ while keeping $\\gamma = 0.25$ and $\\lambda = 4$ guarantees that the first split will be accepted, because deeper trees have more leaves and therefore offset the split penalty.",
            "solution": "The user requires a critical validation of the problem statement, followed by a first-principles derivation of the solution and an evaluation of all provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Model:** eXtreme Gradient Boosting (XGBoost) classifier, a form of gradient boosting decision trees (GBDT).\n-   **Task:** Binary classification.\n-   **Loss Function:** Logistic loss.\n-   **Approximation Method:** Second-order approximation of the objective function.\n-   **Regularization:** An $\\ell_2$ penalty on leaf weights controlled by $\\lambda$, and a per-split minimum loss reduction threshold $\\gamma$.\n-   **Initial Conditions:** The first boosting round begins from a constant score $f(x) = 0$. This implies initial predicted probabilities are $p_i = 0.5$ for all instances. Sample weights are uniform.\n-   **Tree Constraint:** Maximum depth $d_{\\max} = 1$ for the first round.\n-   **Dataset:** $N=8$ instances with features $x_1, x_2 \\in \\{0, 1\\}$ and labels $y \\in \\{0, 1\\}$.\n    -   $i=1: (x_1, x_2, y) = (0, 0, 0)$\n    -   $i=2: (x_1, x_2, y) = (0, 0, 0)$\n    -   $i=3: (x_1, x_2, y) = (0, 1, 1)$\n    -   $i=4: (x_1, x_2, y) = (0, 1, 0)$\n    -   $i=5: (x_1, x_2, y) = (1, 0, 1)$\n    -   $i=6: (x_1, x_2, y) = (1, 0, 0)$\n    -   $i=7: (x_1, x_2, y) = (1, 1, 1)$\n    -   $i=8: (x_1, x_2, y) = (1, 1, 1)$\n-   **Hyperparameters:** $\\lambda = 4$ and $\\gamma = 0.25$.\n-   **Assumption:** No other constraints (e.g., minimum child weight) are active.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding:** The problem is firmly rooted in the theory of gradient boosting machines, specifically the XGBoost algorithm. The use of logistic loss, second-order Taylor approximation, and $\\ell_2$ and $\\gamma$ regularization are standard and fundamental components of this model. The problem is scientifically sound.\n-   **Well-Posedness:** The problem provides all necessary information to compute the split gain for the first tree: the dataset, initial predictions, loss function, and all relevant hyperparameters. The question is specific and allows for a unique, derivable answer.\n-   **Objectivity:** The problem is stated in precise, objective, and technical language, free from ambiguity or subjective claims.\n-   **Completeness and Consistency:** The setup is self-contained and internally consistent. There are no missing data points or contradictory constraints.\n-   **Feasibility:** The dataset and hyperparameter values are synthetic but entirely realistic for an example problem in a machine learning course. The calculations required are feasible.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. It is a well-posed, scientifically grounded problem in statistical learning. I will proceed with the solution.\n\n### Solution Derivation\n\nThe core task is to determine if any split can be made in the first tree. This requires calculating the split gain for all possible splits and comparing it to the threshold $\\gamma$.\n\n**1. First and Second Derivatives (Gradients and Hessians)**\nThe loss function is the logistic loss for binary classification: $L(y_i, \\hat{y}_i) = -[y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)]$, where $p_i = \\sigma(\\hat{y}_i) = 1 / (1 + e^{-\\hat{y}_i})$ and $\\hat{y}_i$ is the raw log-odds score.\n\nThe first and second derivatives of the loss with respect to the raw score $\\hat{y}_i$ are:\n-   First derivative (gradient), $g_i = \\frac{\\partial L}{\\partial \\hat{y}_i} = p_i - y_i$.\n-   Second derivative (Hessian), $h_i = \\frac{\\partial^2 L}{\\partial \\hat{y}_i^2} = p_i (1-p_i)$.\n\nAt the start of the first round ($t=1$), the model prediction is $\\hat{y}_i^{(0)} = 0$ for all instances $i$.\nThus, the initial probability is $p_i = \\sigma(0) = 1 / (1+e^0) = 0.5$ for all $i$.\nThe gradients are $g_i = 0.5 - y_i$.\nThe Hessians are $h_i = 0.5 \\times (1 - 0.5) = 0.25$ for all $i$.\n\nLet's compute the gradient $g_i$ for each instance in the dataset:\n-   $i=1, y=0: g_1 = 0.5 - 0 = 0.5$\n-   $i=2, y=0: g_2 = 0.5 - 0 = 0.5$\n-   $i=3, y=1: g_3 = 0.5 - 1 = -0.5$\n-   $i=4, y=0: g_4 = 0.5 - 0 = 0.5$\n-   $i=5, y=1: g_5 = 0.5 - 1 = -0.5$\n-   $i=6, y=0: g_6 = 0.5 - 0 = 0.5$\n-   $i=7, y=1: g_7 = 0.5 - 1 = -0.5$\n-   $i=8, y=1: g_8 = 0.5 - 1 = -0.5$\n\n**2. Split Gain Calculation**\nThe gain of a split is defined as the reduction in the objective function. For a split of a parent node $I$ into a left child $I_L$ and a right child $I_R$, the gain is:\n$$ \\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_I^2}{H_I + \\lambda} \\right] $$\nwhere $G = \\sum_{i \\in \\text{Node}} g_i$ and $H = \\sum_{i \\in \\text{Node}} h_i$. A split is accepted if $\\text{Gain}  \\gamma$.\n\nFirst, we compute the sums for the root node, which contains all instances $I = \\{1, 2, ..., 8\\}$:\n-   $G_I = \\sum_{i=1}^8 g_i = (0.5+0.5-0.5+0.5-0.5+0.5-0.5-0.5) = 4 \\times 0.5 + 4 \\times (-0.5) = 2 - 2 = 0$.\n-   $H_I = \\sum_{i=1}^8 h_i = 8 \\times 0.25 = 2$.\n\nNow, we evaluate the two possible splits (on $x_1$ and $x_2$).\n\n**Split on $x_1$ (at threshold $0.5$):**\n-   Left child $I_L$ (instances with $x_1=0$): $\\{1, 2, 3, 4\\}$.\n    -   $G_L = g_1+g_2+g_3+g_4 = 0.5+0.5-0.5+0.5 = 1$.\n    -   $H_L = 4 \\times 0.25 = 1$.\n-   Right child $I_R$ (instances with $x_1=1$): $\\{5, 6, 7, 8\\}$.\n    -   $G_R = g_5+g_6+g_7+g_8 = -0.5+0.5-0.5-0.5 = -1$.\n    -   $H_R = 4 \\times 0.25 = 1$.\n\nThe gain for this split is:\n$$ \\text{Gain}(x_1) = \\frac{1}{2} \\left[ \\frac{1^2}{1 + 4} + \\frac{(-1)^2}{1 + 4} - \\frac{0^2}{2 + 4} \\right] = \\frac{1}{2} \\left[ \\frac{1}{5} + \\frac{1}{5} - 0 \\right] = \\frac{1}{2} \\times \\frac{2}{5} = \\frac{1}{5} = 0.2 $$\nWe must check if $\\text{Gain}(x_1)  \\gamma$. Here, $0.2  0.25$ is **false**. This split is rejected.\n\n**Split on $x_2$ (at threshold $0.5$):**\n-   Left child $I_L$ (instances with $x_2=0$): $\\{1, 2, 5, 6\\}$.\n    -   $G_L = g_1+g_2+g_5+g_6 = 0.5+0.5-0.5+0.5 = 1$.\n    -   $H_L = 4 \\times 0.25 = 1$.\n-   Right child $I_R$ (instances with $x_2=1$): $\\{3, 4, 7, 8\\}$.\n    -   $G_R = g_3+g_4+g_7+g_8 = -0.5+0.5-0.5-0.5 = -1$.\n    -   $H_R = 4 \\times 0.25 = 1$.\n\nThe gain for this split is:\n$$ \\text{Gain}(x_2) = \\frac{1}{2} \\left[ \\frac{1^2}{1 + 4} + \\frac{(-1)^2}{1 + 4} - \\frac{0^2}{2 + 4} \\right] = \\frac{1}{2} \\left[ \\frac{1}{5} + \\frac{1}{5} - 0 \\right] = \\frac{1}{5} = 0.2 $$\nWe must check if $\\text{Gain}(x_2)  \\gamma$. Again, $0.2  0.25$ is **false**. This split is also rejected.\n\n**Conclusion:** At the first boosting round, no split is accepted because the gain from any possible split ($0.2$) does not exceed the regularization threshold $\\gamma=0.25$. The model is underfitting due to overly strong regularization. The first tree will be a stump with only a root node.\n\n### Option-by-Option Analysis\n\n**A. Lowering $\\gamma$ below a threshold determined by the raw split improvement (for example, making $\\gamma  0.2$ in this dataset) or decreasing $\\lambda$ will increase the net split gain and can enable splits; this directly addresses underfitting caused by overly strong regularization.**\n-   The condition for accepting a split is $\\text{Gain}  \\gamma$. In our case, the best gain is $0.2$. If we set $\\gamma  0.2$ (e.g., $\\gamma=0.15$), then the condition $0.2  0.15$ would be true, and the split would be accepted. This allows the model to learn.\n-   The gain formula, $\\text{Gain} = \\frac{1}{2} [ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G_I^2}{H_I + \\lambda} ]$, shows that gain is a decreasing function of $\\lambda$ (since $\\lambda$ is in the denominator of positive terms). Decreasing $\\lambda$ will increase the gain. For instance, if we decrease $\\lambda$ to $1$, the gain would become $\\frac{1}{2} [\\frac{1^2}{1+1} + \\frac{(-1)^2}{1+1} - 0] = \\frac{1}{2} [ 0.5+0.5 ] = 0.5$. Since $0.5  0.25$, the split would be accepted.\n-   Both actions (lowering $\\gamma$ or decreasing $\\lambda$) can enable splits that were previously rejected, allowing the model to fit the data better and thus addressing underfitting caused by regularization. The statement is entirely consistent with the mechanics of XGBoost.\n-   Verdict: **Correct**.\n\n**B. Increasing the number of boosting rounds $T$ while keeping $\\gamma = 0.25$ will reliably overcome the underfitting, because the ensemble can accumulate improvements even if every individual tree has no splits.**\n-   If the first tree has no splits, it is just a root node. The optimal weight for this node is $w_1^* = -G_I / (H_I + \\lambda) = -0 / (2+4) = 0$. The first tree is $f_1(x)=0$.\n-   The model update is $\\hat{y}_i^{(1)} = \\hat{y}_i^{(0)} + \\eta f_1(x_i) = 0 + \\eta \\cdot 0 = 0$. The predictions do not change.\n-   For the second round ($t=2$), the gradients $g_i$ and Hessians $h_i$ are calculated based on $\\hat{y}_i^{(1)}=0$. These will be identical to the ones in the first round.\n-   Consequently, the gain calculation for the second tree will be identical, and again no splits will be made. The second tree will also be $f_2(x)=0$.\n-   This process repeats indefinitely. The model predictions never move from the initial state. The ensemble cannot accumulate improvements if each new tree adds zero contribution.\n-   Verdict: **Incorrect**.\n\n**C. A practical way to detect the underfitting here is to monitor the training objective and validation metric: if both plateau near the baseline after several rounds and each tree has only a single leaf (no accepted splits), this indicates underfitting induced by regularization thresholds.**\n-   As shown in the analysis for B, if no splits are made, the model never learns. The predictions remain at the baseline ($p_i=0.5$).\n-   This means the training loss will be constant at its initial value, e.g., $\\text{logloss} = -\\ln(0.5) \\approx 0.693$. Any validation metric (accuracy, AUC, etc.) will also be constant at its baseline value.\n-   Observing this plateau in performance metrics, combined with inspecting the built trees and finding they are all single-node \"stumps\", is the classic diagnostic signature for this exact failure mode: underfitting caused by regularization parameters ($\\lambda$, $\\gamma$, `min_child_weight`, etc.) being too restrictive.\n-   Verdict: **Correct**.\n\n**D. Standardizing $x_1$ and $x_2$ to zero mean and unit variance will, by itself, increase the split gain under the second-order criterion and overcome the $\\gamma$ threshold without changing $\\lambda$ or $\\gamma$.**\n-   Decision tree algorithms, including XGBoost, are invariant to monotonic transformations of individual features. Standardization is one such transformation.\n-   The features $x_1$ and $x_2$ are binary, taking values in $\\{0, 1\\}$. A split on such a feature, e.g., $x_1  c$ (for any $0c1$), partitions the dataset into a group where $x_1=0$ and a group where $x_1=1$.\n-   Standardizing a binary $\\{0,1\\}$ feature with equal counts transforms it to a $\\{-1, 1\\}$ feature. A split on the standardized feature, e.g., $x'_1  0$, would partition the dataset into a group where $x'_1=-1$ and one where $x'_1=1$. This is the *exact same partition of instances* as before.\n-   The split gain calculation depends only on the set of instances in the left and right child nodes ($I_L$ and $I_R$), not on the feature values themselves. Since the partition of instances is identical, the sums of gradients and Hessians ($G_L, H_L, G_R, H_R$) are identical, and thus the calculated gain is identical.\n-   Standardization will not change the split gain.\n-   Verdict: **Incorrect**.\n\n**E. Increasing the maximum depth $d_{\\max}$ from $1$ to $3$ while keeping $\\gamma = 0.25$ and $\\lambda = 4$ guarantees that the first split will be accepted, because deeper trees have more leaves and therefore offset the split penalty.**\n-   The `max_depth` parameter is a stopping condition. The XGBoost algorithm first calculates the gain for all possible splits at a given node. It then checks if the best gain is greater than $\\gamma$. Only if this condition is met is the split performed. After the split is made, the algorithm checks if the new nodes have reached the max_depth.\n-   The decision to make the *first split* at the root node is based solely on the gain calculation for that split. It is entirely independent of the `max_depth` parameter.\n-   If the gain for the root split ($0.2$) is insufficient to overcome $\\gamma$ ($0.25$), no split will be made, and the tree construction stops. The value of `max_depth` is irrelevant because the first level of the tree could not even be built. The potential to grow deeper does not retroactively approve a split with insufficient gain.\n-   Verdict: **Incorrect**.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "Beyond predictive accuracy, a robust model should exhibit stable behavior. This advanced coding exercise  introduces a powerful method for probing model stability by applying bootstrap resampling to the split-finding process. By creating a confidence interval for a learned split threshold, you will develop a deeper intuition for model uncertainty and learn a practical technique for evaluating the reliability of your model's internal structure.",
            "id": "3120252",
            "problem": "You will investigate the stability of the learned split threshold in a one-dimensional decision stump built using the Extreme Gradient Boosting (XGBoost) principle and quantify uncertainty via bootstrap resampling. The task is to implement a complete program that fits a single-depth tree stump in the first iteration of Extreme Gradient Boosting under the logistic loss, records the learned threshold, and constructs a nonparametric confidence interval for that threshold across bootstrap resamples.\n\nUse the following foundational base:\n- Binary labels are encoded as $y \\in \\{0,1\\}$ and the model uses the logistic loss for a score function $f(x)$, where $p(x) = \\sigma(f(x))$ with $\\sigma(\\cdot)$ denoting the logistic sigmoid. The loss on one observation is $-\\left(y \\log p + (1-y)\\log(1-p)\\right)$.\n- Extreme Gradient Boosting (XGBoost) builds trees by minimizing a second-order Taylor approximation of the regularized objective around the current scores $f(x)$, using the first and second derivatives with respect to $f(x)$ and employing $\\ell_2$ regularization with parameter $\\lambda$ on the leaf weights and a leaf-splitting penalty $\\gamma$.\n- A decision stump has exactly two leaves induced by a scalar threshold $t$ on a single feature $x$.\n\nRequirements:\n1) Derive from first principles (starting from the logistic loss and the second-order Taylor approximation of the regularized objective) the expressions needed to evaluate the quality of any candidate stump split of the form $x \\le t$ versus $x  t$, using aggregated first- and second-derivative terms over the left and right partitions and the regularization parameters $\\lambda$ and $\\gamma$. Start from the current model scores $f(x) \\equiv 0$ for all training points so that $p(x) = \\sigma(0)$ for all $x$.\n2) Implement a procedure that, given a dataset of pairs $(x_i,y_i)$ with $x_i \\in \\mathbb{R}$ and $y_i \\in \\{0,1\\}$, computes the best threshold $t$ for the first stump by:\n   - Sorting the data by $x$,\n   - Considering candidate splits only between strictly increasing adjacent $x$ values,\n   - Evaluating the second-order-approximate, regularized objective reduction for each candidate, and\n   - Selecting the $t$ that maximizes that reduction, subject to a minimum child-weight constraint defined as a lower bound on the sum of second derivatives in each child.\n   If no valid split exists under the constraint, use the midpoint between the minimum and maximum observed $x$ values as a fallback threshold.\n3) To assess stability, implement bootstrap resampling: for a given dataset of size $n$, draw $B$ bootstrap samples of size $n$ with replacement, refit the stump on each bootstrap sample exactly as above, and record the threshold $\\{t^{(b)}\\}_{b=1}^B$.\n4) Compute the empirical $95$-level two-sided confidence interval for $t$ as the $2.5$ and $97.5$ quantiles of $\\{t^{(b)}\\}_{b=1}^B$.\n5) Use a single, fixed pseudo-random seed $12345$ to ensure deterministic results.\n6) For each dataset below, generate the data synthetically as follows:\n   - Sample $x_i \\sim \\mathcal{N}(0,1)$ independently.\n   - Given a true threshold $\\theta^\\star$, set $y_i = 1$ if $x_i  \\theta^\\star$ and $y_i = 0$ otherwise.\n   - Independently for each $i$, flip the label with probability $p_{\\text{flip}}$ by replacing $y_i$ with $1-y_i$.\n   All probabilities must be represented as decimals in $[0,1]$.\n7) Hyperparameters for the split selection must be: $\\lambda = 1.0$, $\\gamma = 0.0$, and minimum child-weight (sum of second derivatives) equal to $10^{-6}$. Work at the initial boosting iteration with $f(x)\\equiv 0$ only.\n8) Test suite. Run your procedure on the following three datasets, each with $B=400$ bootstraps:\n   - Test A (happy path): $n=300$, $\\theta^\\star = 0.25$, $p_{\\text{flip}} = 0.0$.\n   - Test B (noisier): $n=200$, $\\theta^\\star = -0.20$, $p_{\\text{flip}} = 0.30$.\n   - Test C (small sample): $n=40$, $\\theta^\\star = 0.00$, $p_{\\text{flip}} = 0.10$.\n9) Final output format. Your program should produce a single line of output containing the confidence intervals for the three tests as a comma-separated list of three two-element lists, each list being $[L,U]$ with $L$ the lower bound and $U$ the upper bound. Round each bound to six decimal places. The exact required format is:\n   [[L_A,U_A],[L_B,U_B],[L_C,U_C]]\nwhere $L_A$ and $U_A$ are the bounds for Test A, $L_B$ and $U_B$ for Test B, and $L_C$ and $U_C$ for Test C. No additional text should be printed.\n\nNotes:\n- Angles or physical units are not involved.\n- All numeric probabilities and quantiles must be treated as decimals (not percentages).\n- The algorithm must be implemented purely in terms of the definitions above; no heuristic shortcuts are allowed beyond what is implied by the second-order approximation with $\\ell_2$ regularization and the leaf-penalty.",
            "solution": "The problem requires the implementation of a procedure to determine a bootstrap confidence interval for the optimal split threshold of a decision stump. This stump represents the first tree in an Extreme Gradient Boosting (XGBoost) model trained for binary classification using the logistic loss. The solution is structured in two parts: first, a derivation of the theoretical foundations from first principles, and second, an algorithmic implementation based on this theory.\n\n### Part 1: Theoretical Derivation\n\nThe core of the XGBoost algorithm is the iterative minimization of a regularized objective function. At each boosting iteration $m$, a new tree, denoted by its score function $f_m(x)$, is added to the model. The objective function at this step is approximated by a second-order Taylor expansion of the loss function around the predictions from the previous $m-1$ trees, $\\hat{y}^{(m-1)}$. The objective for the new tree $f_m$ is:\n$$ \\mathcal{O}^{(m)} \\approx \\sum_{i=1}^{n} \\left[ L(y_i, \\hat{y}_i^{(m-1)}) + g_i f_m(x_i) + \\frac{1}{2} h_i f_m^2(x_i) \\right] + \\Omega(f_m) $$\nHere, $L(y_i, \\hat{y})$ is the loss function for a single observation $(x_i, y_i)$, and $g_i$ and $h_i$ are the first and second derivatives of the loss with respect to the raw score prediction $\\hat{y}$, evaluated at the current model's prediction $\\hat{y}_i^{(m-1)}$:\n$$ g_i = \\left[\\frac{\\partial L(y_i, \\hat{y})}{\\partial \\hat{y}}\\right]_{\\hat{y}=\\hat{y}_i^{(m-1)}}, \\quad h_i = \\left[\\frac{\\partial^2 L(y_i, \\hat{y})}{\\partial \\hat{y}^2}\\right]_{\\hat{y}=\\hat{y}_i^{(m-1)}} $$\nThe term $L(y_i, \\hat{y}_i^{(m-1)})$ is a constant with respect to the new tree $f_m$ and can be omitted from the optimization. The regularization term $\\Omega(f_m)$ penalizes model complexity:\n$$ \\Omega(f_m) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^{T} w_j^2 $$\nwhere $T$ is the number of leaves in the tree, $w_j$ is the score (weight) assigned to leaf $j$, and $\\gamma$ and $\\lambda$ are regularization parameters.\n\nFor a fixed tree structure with $T$ leaves, the function $f_m(x_i)$ simply returns the weight $w_j$ of the leaf $j$ into which the instance $x_i$ falls. Let $I_j = \\{i | x_i \\text{ is in leaf } j\\}$. The objective function can be rewritten by grouping terms by leaf:\n$$ \\mathcal{O}^{(m)} \\approx \\sum_{j=1}^{T} \\left[ \\left(\\sum_{i \\in I_j} g_i\\right) w_j + \\frac{1}{2} \\left(\\sum_{i \\in I_j} h_i + \\lambda\\right) w_j^2 \\right] + \\gamma T $$\nLet $G_j = \\sum_{i \\in I_j} g_i$ and $H_j = \\sum_{i \\in I_j} h_i$. The objective becomes a sum of quadratic functions in $w_j$:\n$$ \\mathcal{O}^{(m)} \\approx \\sum_{j=1}^{T} \\left[ G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right] + \\gamma T $$\nFor a fixed tree structure, the optimal weight $w_j^*$ for each leaf $j$ that minimizes this quadratic is found by setting its derivative with respect to $w_j$ to zero:\n$$ \\frac{\\partial}{\\partial w_j} \\left[ G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 \\right] = G_j + (H_j + \\lambda) w_j = 0 \\implies w_j^* = -\\frac{G_j}{H_j + \\lambda} $$\nSubstituting this optimal weight back into the objective gives the minimum objective value for a given tree structure:\n$$ \\mathcal{O}^* = -\\frac{1}{2} \\sum_{j=1}^{T} \\frac{G_j^2}{H_j + \\lambda} + \\gamma T $$\nThe goal of the tree construction algorithm is to find a tree structure that minimizes this quantity. This is a greedy process. Starting from a single leaf, we seek a split that results in the largest reduction of the objective function. The reduction, or \"gain\", from splitting a parent leaf $P$ into a left leaf $L$ and a right leaf $R$ is:\n$$ \\text{Gain} = \\mathcal{O}^*_P - (\\mathcal{O}^*_L + \\mathcal{O}^*_R) $$\n$$ \\text{Gain} = \\left(-\\frac{1}{2} \\frac{G_P^2}{H_P + \\lambda} + \\gamma \\cdot 1\\right) - \\left(-\\frac{1}{2} \\frac{G_L^2}{H_L + \\lambda} - \\frac{1}{2} \\frac{G_R^2}{H_R + \\lambda} + \\gamma \\cdot 2\\right) $$\nNoting that $G_P = G_L + G_R$ and $H_P = H_L + H_R$, the gain simplifies to:\n$$ \\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right] - \\gamma $$\nTo find the best split for a given node, we must evaluate this gain for all possible candidate splits. For any split of a particular node, the term $\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}$ (which relates to the parent node's score) and the penalty $\\gamma$ are constant. Therefore, maximizing the gain is equivalent to maximizing the score:\n$$ \\text{Score} = \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} $$\nThis is the expression used to evaluate the quality of a candidate split.\n\nFor this specific problem, we use the logistic loss for a binary classification problem with labels $y \\in \\{0,1\\}$. The predicted probability is $p = \\sigma(f(x)) = (1+e^{-f(x)})^{-1}$, and the loss is $L(y, f) = -[y \\log p + (1-y)\\log(1-p)]$. The derivatives with respect to the raw score $f(x)$ are:\n$$ g = \\frac{\\partial L}{\\partial f} = p - y $$\n$$ h = \\frac{\\partial^2 L}{\\partial f^2} = \\frac{\\partial p}{\\partial f} = p(1-p) $$\nThe problem specifies that we start at the first boosting iteration with an initial model of $f_0(x) \\equiv 0$. This implies an initial probability prediction of $p_i = \\sigma(0) = 0.5$ for all data points $i$. The gradients and Hessians for the first tree are therefore:\n$$ g_i = 0.5 - y_i $$\n$$ h_i = 0.5 \\times (1 - 0.5) = 0.25 $$\nThese expressions for $g_i$ and $h_i$ are used to compute the aggregated sums $G_L, H_L, G_R, H_R$ for each candidate split.\n\n### Part 2: Algorithm and Implementation\n\nThe procedure to find the optimal split and its confidence interval is as follows:\n\n1.  **Data Simulation**: For each test case, a dataset of size $n$ is generated. Feature values $x_i$ are drawn from a standard normal distribution $\\mathcal{N}(0,1)$. True labels are set as $y_i = 1$ if $x_i  \\theta^\\star$ and $y_i = 0$ otherwise. These labels are then flipped with probability $p_{\\text{flip}}$ to introduce noise.\n\n2.  **Optimal Threshold Search**: A function is implemented to find the best split threshold $t$ for a given dataset $(x, y)$.\n    a. First, the gradients $g_i = 0.5 - y_i$ and Hessians $h_i = 0.25$ are computed for all points.\n    b. The data points $(x_i, y_i)$ are sorted based on their $x_i$ value. This enables an efficient linear scan over all possible split points.\n    c. The algorithm iterates through adjacent sorted $x$ values. A candidate split is defined between each pair $(x_{(i)}, x_{(i+1)})$ where $x_{(i)}  x_{(i+1)}$. The threshold is the midpoint $t = (x_{(i)} + x_{(i+1)})/2$.\n    d. For each candidate split, the data is partitioned into a left set ($x \\le t$) and a right set ($x  t$). The sums of gradients and Hessians ($G_L, H_L, G_R, H_R$) are calculated.\n    e. A split is considered valid only if the sum of Hessians in both resulting child nodes is at least the minimum child weight, i.e., $H_L \\ge 10^{-6}$ and $H_R \\ge 10^{-6}$.\n    f. For each valid split, the score $\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda}$ is computed with $\\lambda=1.0$.\n    g. The threshold $t$ that maximizes this score is selected as the optimal threshold for the dataset.\n    h. If no valid split can be found (e.g., all $x$ values in the sample are identical), a fallback threshold equal to the midpoint of the minimum and maximum observed $x$ values is used.\n\n3.  **Bootstrap Resampling**: To assess the stability of the learned threshold, this process is repeated on $B=400$ bootstrap samples. For each test case's generated dataset, $B$ new datasets of size $n$ are created by sampling with replacement from the original. The optimal threshold is computed for each of these bootstrap samples, yielding a distribution of $400$ thresholds $\\{t^{(b)}\\}_{b=1}^{400}$.\n\n4.  **Confidence Interval Calculation**: The empirical $95\\%$ confidence interval for the threshold is constructed by finding the $2.5$ and $97.5$ percentiles of the sorted bootstrap thresholds.\n\nThis entire procedure is executed for each of the three test cases specified, using a fixed pseudo-random number generator seed of $12345$ for deterministic and reproducible results. The final output consists of the computed confidence intervals for each test case, formatted to six decimal places.",
            "answer": "```python\nimport numpy as np\n\ndef find_best_threshold(x, y, lambda_, min_child_weight):\n    \"\"\"\n    Finds the best split threshold for a 1D decision stump in XGBoost.\n    \"\"\"\n    n = len(x)\n    if n  2:\n        return (np.min(x) + np.max(x)) / 2.0 if n  0 else 0.0\n\n    # At the first iteration, f(x)=0, so p=0.5\n    # g = p - y, h = p * (1-p)\n    g = 0.5 - y\n    h_val = 0.25\n\n    # Sort data by feature x for efficient scanning\n    sorted_indices = np.argsort(x)\n    x_sorted = x[sorted_indices]\n    g_sorted = g[sorted_indices]\n    \n    # Initialize with fallback threshold\n    fallback_threshold = (x_sorted[0] + x_sorted[-1]) / 2.0\n    best_threshold = fallback_threshold\n    best_score = -1.0\n    found_valid_split = False\n\n    # Calculate total sums\n    G_total = np.sum(g_sorted)\n    H_total = n * h_val\n\n    # Efficiently scan for the best split\n    G_L, H_L = 0.0, 0.0\n    for i in range(n - 1):\n        G_L += g_sorted[i]\n        H_L += h_val\n\n        # Consider split only between unique x values\n        if x_sorted[i]  x_sorted[i+1]:\n            G_R = G_total - G_L\n            H_R = H_total - H_L\n\n            # Check min_child_weight constraint\n            if H_L = min_child_weight and H_R = min_child_weight:\n                found_valid_split = True\n                score = (G_L**2 / (H_L + lambda_)) + (G_R**2 / (H_R + lambda_))\n                if score  best_score:\n                    best_score = score\n                    best_threshold = (x_sorted[i] + x_sorted[i+1]) / 2.0\n    \n    return best_threshold\n\ndef run_test_case(n, theta_star, p_flip, lambda_, min_child_weight, B, rng):\n    \"\"\"\n    Generates data, runs bootstrap, and computes CI for one test case.\n    \"\"\"\n    # 1. Generate synthetic data\n    x = rng.normal(loc=0.0, scale=1.0, size=n)\n    y = (x  theta_star).astype(int)\n    \n    # Apply label noise\n    flip_mask = rng.random(size=n)  p_flip\n    y[flip_mask] = 1 - y[flip_mask]\n\n    # 2. Bootstrap resampling\n    bootstrap_thresholds = []\n    base_indices = np.arange(n)\n    for _ in range(B):\n        bootstrap_indices = rng.choice(base_indices, size=n, replace=True)\n        x_boot = x[bootstrap_indices]\n        y_boot = y[bootstrap_indices]\n        \n        threshold = find_best_threshold(x_boot, y_boot, lambda_, min_child_weight)\n        bootstrap_thresholds.append(threshold)\n\n    # 3. Compute 95% confidence interval\n    lower_bound = np.quantile(bootstrap_thresholds, 0.025)\n    upper_bound = np.quantile(bootstrap_thresholds, 0.975)\n    \n    return [lower_bound, upper_bound]\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final result.\n    \"\"\"\n    # Global parameters\n    SEED = 12345\n    B = 400\n    LAMBDA = 1.0\n    GAMMA = 0.0  # Note: gamma does not affect split selection, only gain value\n    MIN_CHILD_WEIGHT = 1e-6\n    \n    rng = np.random.default_rng(SEED)\n\n    # Test cases from the problem statement\n    test_cases = [\n        {'n': 300, 'theta_star': 0.25, 'p_flip': 0.0},   # Test A\n        {'n': 200, 'theta_star': -0.20, 'p_flip': 0.30}, # Test B\n        {'n': 40, 'theta_star': 0.00, 'p_flip': 0.10},   # Test C\n    ]\n\n    results = []\n    for case in test_cases:\n        ci = run_test_case(\n            n=case['n'],\n            theta_star=case['theta_star'],\n            p_flip=case['p_flip'],\n            lambda_=LAMBDA,\n            min_child_weight=MIN_CHILD_WEIGHT,\n            B=B,\n            rng=rng\n        )\n        results.append(ci)\n\n    # Format output to match the specified format\n    formatted_intervals = [f\"[{l:.6f},{u:.6f}]\" for l, u in results]\n    final_output = f\"[{','.join(formatted_intervals)}]\"\n\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}