{
    "hands_on_practices": [
        {
            "introduction": "Before tracing the intricate solution path of a regularized model, it's essential to identify its starting point. This exercise guides you through deriving $\\lambda_{\\max}$, the critical value of the regularization parameter above which all coefficients are guaranteed to be zero . Understanding how to calculate this threshold from first principles using the Karush-Kuhn-Tucker (KKT) conditions is a foundational skill for both theoretical analysis and the efficient implementation of path-following algorithms.",
            "id": "3126744",
            "problem": "Consider the linear model with grouped predictors in the Group Least Absolute Shrinkage and Selection Operator (group lasso) framework. The data consist of a response vector $Y \\in \\mathbb{R}^{n}$ and a design matrix $X \\in \\mathbb{R}^{n \\times p}$ whose columns are partitioned into $G$ disjoint groups. Denote the parameter vector by $\\beta = (\\beta_{1}^{\\top}, \\dots, \\beta_{G}^{\\top})^{\\top}$ where each $\\beta_{g} \\in \\mathbb{R}^{p_{g}}$ corresponds to group $g$ with $p_{g}$ columns, and $X = [X_{1} \\,\\, X_{2} \\,\\, \\dots \\,\\, X_{G}]$ with $X_{g} \\in \\mathbb{R}^{n \\times p_{g}}$. The group lasso estimator minimizes the convex objective\n$$\n\\frac{1}{2n} \\left\\| Y - \\sum_{g=1}^{G} X_{g} \\beta_{g} \\right\\|_{2}^{2} \\;+\\; \\lambda \\sum_{g=1}^{G} w_{g} \\left\\| \\beta_{g} \\right\\|_{2},\n$$\nwhere $\\lambda \\geq 0$ is the regularization parameter and $w_{g} > 0$ are given group weights.\n\nStarting from first principles of convex optimization, specifically the subgradient optimality (Karush-Kuhn-Tucker (KKT) conditions) for nonsmooth convex functions and the Euclidean norm subdifferential, derive a closed-form expression for the smallest regularization value, denoted $\\lambda_{\\max}$, such that the zero vector $\\hat{\\beta} = 0$ satisfies the optimality conditions of the group lasso problem. Then, for the concrete dataset below, compute the numerical value of $\\lambda_{\\max}$.\n\nUse $n = 5$, $G = 3$, and the following $Y$ and grouped columns of $X$:\n- $Y = (2, \\,-1, \\,0, \\,3, \\,-2)^{\\top}$.\n- Group $g=1$ has $p_{1} = 2$ columns: \n$$\nx_{1} = (1, \\,0, \\,1, \\,0, \\,1)^{\\top}, \\quad x_{2} = (0, \\,1, \\,0, \\,1, \\,0)^{\\top},\n$$\nso $X_{1} = [x_{1} \\,\\, x_{2}]$.\n- Group $g=2$ has $p_{2} = 1$ column:\n$$\nx_{3} = (1, \\,1, \\,1, \\,1, \\,1)^{\\top},\n$$\nso $X_{2} = [x_{3}]$.\n- Group $g=3$ has $p_{3} = 1$ column:\n$$\nx_{4} = (2, \\,-1, \\,0, \\,1, \\,2)^{\\top},\n$$\nso $X_{3} = [x_{4}]$.\n\nLet the group weights be $w_{1} = \\sqrt{2}$, $w_{2} = 1$, and $w_{3} = 1$. Your final answer must be a single real number or an exact closed-form expression equal to $\\lambda_{\\max}$ for this dataset. No rounding is required. Express the final answer without units.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the established theory of regularized linear models (group lasso) and convex optimization. The problem is well-posed, with a complete and consistent set of givens, allowing for the derivation of a unique, meaningful solution for $\\lambda_{\\max}$. The terminology is objective and formal.\n\nThe group lasso estimator $\\hat{\\beta}$ is the solution to the convex optimization problem:\n$$\n\\min_{\\beta} L(\\beta) = \\min_{\\beta_{1}, \\dots, \\beta_{G}} \\left\\{ \\frac{1}{2n} \\left\\| Y - \\sum_{g=1}^{G} X_{g} \\beta_{g} \\right\\|_{2}^{2} \\;+\\; \\lambda \\sum_{g=1}^{G} w_{g} \\left\\| \\beta_{g} \\right\\|_{2} \\right\\}\n$$\nThe objective function $L(\\beta)$ is the sum of a differentiable convex function (the least-squares loss) and a non-differentiable but convex function (the group lasso penalty). The Karush-Kuhn-Tucker (KKT) conditions provide necessary and sufficient conditions for a point $\\hat{\\beta}$ to be a minimizer. These conditions state that the zero vector must belong to the subdifferential of the objective function at the minimum, i.e., $0 \\in \\partial L(\\hat{\\beta})$.\n\nThe subdifferential of $L(\\beta)$ is given by:\n$$\n\\partial L(\\beta) = \\nabla \\left( \\frac{1}{2n} \\left\\| Y - X\\beta \\right\\|_{2}^{2} \\right) + \\lambda \\sum_{g=1}^{G} w_{g} \\partial \\left\\| \\beta_{g} \\right\\|_{2}\n$$\nThe gradient of the least-squares term is:\n$$\n\\nabla \\left( \\frac{1}{2n} \\left\\| Y - X\\beta \\right\\|_{2}^{2} \\right) = -\\frac{1}{n} X^{\\top} (Y - X\\beta)\n$$\nWe are interested in finding $\\lambda_{\\max}$, the smallest value of $\\lambda \\ge 0$ for which the solution is the zero vector, $\\hat{\\beta} = 0$. Substituting $\\hat{\\beta}=0$ into the gradient term gives:\n$$\n-\\frac{1}{n} X^{\\top} (Y - X \\cdot 0) = -\\frac{1}{n} X^{\\top} Y\n$$\nThe subdifferential of the Euclidean norm $\\|\\beta_g\\|_2$ at $\\beta_g = 0$ is the closed unit ball in $\\mathbb{R}^{p_g}$:\n$$\n\\partial \\|\\beta_g\\|_2 |_{\\beta_g = 0} = \\{ v_g \\in \\mathbb{R}^{p_g} \\mid \\|v_g\\|_2 \\le 1 \\}\n$$\nFor $\\hat{\\beta}=0$ to be the solution, the KKT condition $0 \\in \\partial L(0)$ must hold. This means there must exist subgradients $v_g$ with $\\|v_g\\|_2 \\le 1$ for each group $g=1, \\dots, G$ such that:\n$$\n0 = -\\frac{1}{n} X^{\\top}Y + \\lambda \\begin{pmatrix} w_1 v_1 \\\\ \\vdots \\\\ w_G v_G \\end{pmatrix}\n$$\nThis vector equation can be decomposed into a set of equations for each group:\n$$\n0 = -\\frac{1}{n} X_g^{\\top}Y + \\lambda w_g v_g \\quad \\text{for } g = 1, \\dots, G\n$$\nRearranging for $v_g$ gives:\n$$\nv_g = \\frac{X_g^{\\top}Y}{n \\lambda w_g}\n$$\nThe condition for these subgradients to be valid is $\\|v_g\\|_2 \\le 1$. Applying this constraint:\n$$\n\\left\\| \\frac{X_g^{\\top}Y}{n \\lambda w_g} \\right\\|_{2} \\le 1\n$$\nSince $\\lambda > 0$ and $w_g > 0$, we can write:\n$$\n\\frac{\\|X_g^{\\top}Y\\|_2}{n \\lambda w_g} \\le 1\n$$\nThis implies that for each group $g$, $\\lambda$ must satisfy:\n$$\n\\lambda \\ge \\frac{\\|X_g^{\\top}Y\\|_2}{n w_g}\n$$\nFor $\\hat{\\beta}=0$ to be a valid solution, this inequality must hold for all groups simultaneously. Therefore, $\\lambda$ must be greater than or equal to the maximum of these lower bounds. The smallest value of $\\lambda$ for which $\\hat{\\beta}=0$ is a solution is this maximum value. This defines $\\lambda_{\\max}$:\n$$\n\\lambda_{\\max} = \\max_{g \\in \\{1, \\dots, G\\}} \\left\\{ \\frac{\\|X_g^{\\top}Y\\|_2}{n w_g} \\right\\}\n$$\nNow, we compute this value for the given dataset.\nThe givens are:\n$n = 5$\n$Y = (2, -1, 0, 3, -2)^{\\top}$\nGroup $1$: $p_1=2$, $w_1 = \\sqrt{2}$, $X_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$\nGroup $2$: $p_2=1$, $w_2 = 1$, $X_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$\nGroup $3$: $p_3=1$, $w_3 = 1$, $X_3 = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix}$\n\nWe calculate the term $\\frac{\\|X_g^{\\top}Y\\|_2}{n w_g}$ for each group.\n\nFor group $g=1$:\n$X_1^{\\top}Y = \\begin{pmatrix} 1 & 0 & 1 & 0 & 1 \\\\ 0 & 1 & 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1(2)+0(-1)+1(0)+0(3)+1(-2) \\\\ 0(2)+1(-1)+0(0)+1(3)+0(-2) \\end{pmatrix} = \\begin{pmatrix} 2-2 \\\\ -1+3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$\n$\\|X_1^{\\top}Y\\|_2 = \\sqrt{0^2 + 2^2} = 2$\nThe term for group $1$ is $\\frac{\\|X_1^{\\top}Y\\|_2}{n w_1} = \\frac{2}{5\\sqrt{2}} = \\frac{\\sqrt{2}}{5}$.\n\nFor group $g=2$:\n$X_2^{\\top}Y = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ -2 \\end{pmatrix} = 2 - 1 + 0 + 3 - 2 = 2$\nSince this group has only one predictor, the result is a scalar. $\\|X_2^{\\top}Y\\|_2 = |2| = 2$.\nThe term for group $2$ is $\\frac{\\|X_2^{\\top}Y\\|_2}{n w_2} = \\frac{2}{5 \\cdot 1} = \\frac{2}{5}$.\n\nFor group $g=3$:\n$X_3^{\\top}Y = \\begin{pmatrix} 2 & -1 & 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ -2 \\end{pmatrix} = 2(2) + (-1)(-1) + 0(0) + 1(3) + 2(-2) = 4 + 1 + 0 + 3 - 4 = 4$\nThis group also has one predictor, so the result is a scalar. $\\|X_3^{\\top}Y\\|_2 = |4| = 4$.\nThe term for group $3$ is $\\frac{\\|X_3^{\\top}Y\\|_2}{n w_3} = \\frac{4}{5 \\cdot 1} = \\frac{4}{5}$.\n\nFinally, we find $\\lambda_{\\max}$ by taking the maximum of these values:\n$$\n\\lambda_{\\max} = \\max\\left\\{ \\frac{\\sqrt{2}}{5}, \\frac{2}{5}, \\frac{4}{5} \\right\\}\n$$\nTo compare the values, we note that $\\sqrt{2} < 2 < 4$. Therefore, $\\frac{\\sqrt{2}}{5} < \\frac{2}{5} < \\frac{4}{5}$.\nThe maximum value is $\\frac{4}{5}$.",
            "answer": "$$\n\\boxed{\\frac{4}{5}}\n$$"
        },
        {
            "introduction": "Once the regularization parameter $\\lambda$ falls below $\\lambda_{\\max}$, the model begins selecting variables, but in what order? This practice explores the dynamics of the Group Lasso solution path in a controlled setting with an orthogonal design matrix . By working directly with the KKT optimality conditions, you will analytically determine the exact sequence in which variable groups become active, providing deep insight into the group-wise selection mechanism at the heart of the method.",
            "id": "3126799",
            "problem": "Consider the Group Lasso estimator in a noise-free linear model. Let the design matrix be $X \\in \\mathbb{R}^{6 \\times 6}$ with $X = I_{6}$, where $I_{6}$ is the $6 \\times 6$ identity matrix. Partition the coefficient vector $\\beta \\in \\mathbb{R}^{6}$ into three non-overlapping groups: $G_{1} = \\{1\\}$, $G_{2} = \\{2,3\\}$, and $G_{3} = \\{4,5,6\\}$. Let the observed response be $y \\in \\mathbb{R}^{6}$ with $y = (5, 1, -2, 3, 0, 0)^{\\top}$. Consider the Group Lasso optimization problem\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{6}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\sum_{g=1}^{3} \\| \\beta_{G_{g}} \\|_{2} \\right\\},\n$$\nwhere $\\lambda \\geq 0$ is the regularization parameter and $\\| \\cdot \\|_{2}$ denotes the Euclidean norm. Assume unit weights for all groups.\n\nStarting from the fundamental definition of optimality via the Karush–Kuhn–Tucker (KKT) conditions, and without invoking any pre-derived solution formulas, analytically determine $\\hat{\\beta}(\\lambda)$ as a function of $\\lambda$ and verify the KKT conditions for each regime of $\\lambda$ that leads to distinct sparsity patterns. Use the orthogonality of $X$ and the group structure to reason from first principles.\n\nExplicitly:\n- Derive the KKT conditions for this problem and use them to characterize when a group $G_{g}$ is active (nonzero) or inactive (zero).\n- Compute the piecewise form of $\\hat{\\beta}(\\lambda)$ as $\\lambda$ decreases from a large value to $0$, and use this to determine the order in which the groups $G_{1}$, $G_{2}$, and $G_{3}$ enter the model.\n- Identify and state the exact values of the critical regularization parameters at which each group becomes active.\n\nFinally, report the exact value of the second critical regularization parameter at which the second group to enter the model becomes active. Express your final answer as an exact value. No rounding is required, and no units are involved.",
            "solution": "The problem is valid. It is a well-posed optimization problem from the field of statistical learning, with all necessary data and definitions provided. It is scientifically grounded, objective, and self-contained.\n\nThe Group Lasso optimization problem is given by\n$$ \\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{6}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\sum_{g=1}^{3} \\| \\beta_{G_{g}} \\|_{2} \\right\\} $$\nThe design matrix is the identity matrix, $X = I_{6}$. This simplifies the residual sum of squares (RSS) term:\n$$ \\frac{1}{2} \\| y - I_{6} \\beta \\|_{2}^{2} = \\frac{1}{2} \\| y - \\beta \\|_{2}^{2} = \\frac{1}{2} \\sum_{i=1}^{6} (y_i - \\beta_i)^2 $$\nThe groups are non-overlapping. This allows the objective function to be decomposed into a sum of independent optimization problems for each group $G_g$:\n$$ L(\\beta) = \\sum_{g=1}^{3} \\left( \\frac{1}{2} \\| y_{G_g} - \\beta_{G_g} \\|_{2}^{2} + \\lambda \\| \\beta_{G_g} \\|_{2} \\right) $$\nwhere $\\beta_{G_g}$ and $y_{G_g}$ are the subvectors of $\\beta$ and $y$ corresponding to the indices in group $G_g$. We can find the overall minimum $\\hat{\\beta}$ by minimizing each group's objective function separately.\n\nLet $L_g(\\beta_{G_g}) = \\frac{1}{2} \\| y_{G_g} - \\beta_{G_g} \\|_{2}^{2} + \\lambda \\| \\beta_{G_g} \\|_{2}$. The term $\\frac{1}{2} \\| y_{G_g} - \\beta_{G_g} \\|_{2}^{2}$ is differentiable, while $\\lambda \\| \\beta_{G_g} \\|_{2}$ is convex but not differentiable at $\\beta_{G_g}=0$. The Karush–Kuhn–Tucker (KKT) conditions provide the necessary and sufficient conditions for optimality for this convex problem. The subgradient of $L_g$ must contain zero at the minimizer $\\hat{\\beta}_{G_g}$:\n$$ \\nabla_{\\beta_{G_g}} \\left( \\frac{1}{2} \\| y_{G_g} - \\hat{\\beta}_{G_g} \\|_{2}^{2} \\right) + \\lambda \\cdot \\partial (\\| \\hat{\\beta}_{G_g} \\|_{2}) \\ni 0 $$\nThe gradient of the RSS term with respect to $\\beta_{G_g}$ is $-(y_{G_g} - \\hat{\\beta}_{G_g})$. The subdifferential of the Euclidean norm, $\\partial (\\| \\beta_{G_g} \\|_{2})$, is:\n$$ \\partial (\\| \\beta_{G_g} \\|_{2}) = \\begin{cases} \\{ \\frac{\\beta_{G_g}}{\\|\\beta_{G_g}\\|_{2}} \\} & \\text{if } \\beta_{G_g} \\neq 0 \\\\ \\{ v_g \\in \\mathbb{R}^{|G_g|} : \\|v_g\\|_{2} \\le 1 \\} & \\text{if } \\beta_{G_g} = 0 \\end{cases} $$\nSo the KKT condition for group $g$ is:\n$$ -(y_{G_g} - \\hat{\\beta}_{G_g}) + \\lambda v_g = 0 \\quad \\text{where } v_g \\in \\partial (\\| \\hat{\\beta}_{G_g} \\|_{2}) $$\nWe analyze two cases for the solution $\\hat{\\beta}_{G_g}$:\n\nCase 1: The group is active, $\\hat{\\beta}_{G_g} \\neq 0$.\nIn this case, $v_g = \\frac{\\hat{\\beta}_{G_g}}{\\|\\hat{\\beta}_{G_g}\\|_{2}}$. The KKT condition becomes:\n$$ -(y_{G_g} - \\hat{\\beta}_{G_g}) + \\lambda \\frac{\\hat{\\beta}_{G_g}}{\\|\\hat{\\beta}_{G_g}\\|_{2}} = 0 $$\nRearranging the terms to solve for $y_{G_g}$:\n$$ y_{G_g} = \\hat{\\beta}_{G_g} + \\lambda \\frac{\\hat{\\beta}_{G_g}}{\\|\\hat{\\beta}_{G_g}\\|_{2}} = \\hat{\\beta}_{G_g} \\left( 1 + \\frac{\\lambda}{\\|\\hat{\\beta}_{G_g}\\|_{2}} \\right) $$\nThis shows that $\\hat{\\beta}_{G_g}$ must be collinear with $y_{G_g}$. Taking the Euclidean norm of both sides:\n$$ \\|y_{G_g}\\|_{2} = \\left\\| \\hat{\\beta}_{G_g} \\left( 1 + \\frac{\\lambda}{\\|\\hat{\\beta}_{G_g}\\|_{2}} \\right) \\right\\|_{2} = \\|\\hat{\\beta}_{G_g}\\|_{2} \\left( 1 + \\frac{\\lambda}{\\|\\hat{\\beta}_{G_g}\\|_{2}} \\right) = \\|\\hat{\\beta}_{G_g}\\|_{2} + \\lambda $$\nSolving for the norm of the estimated coefficients:\n$$ \\|\\hat{\\beta}_{G_g}\\|_{2} = \\|y_{G_g}\\|_{2} - \\lambda $$\nSince norm must be positive, this solution is valid only if $\\|y_{G_g}\\|_{2} - \\lambda > 0$, which means $\\lambda < \\|y_{G_g}\\|_{2}$. If this condition holds, we can substitute $\\|\\hat{\\beta}_{G_g}\\|_{2}$ back into the equation for $y_{G_g}$:\n$$ y_{G_g} = \\hat{\\beta}_{G_g} \\left( 1 + \\frac{\\lambda}{\\|y_{G_g}\\|_{2} - \\lambda} \\right) = \\hat{\\beta}_{G_g} \\left( \\frac{\\|y_{G_g}\\|_{2} - \\lambda + \\lambda}{\\|y_{G_g}\\|_{2} - \\lambda} \\right) = \\hat{\\beta}_{G_g} \\left( \\frac{\\|y_{G_g}\\|_{2}}{\\|y_{G_g}\\|_{2} - \\lambda} \\right) $$\nSolving for $\\hat{\\beta}_{G_g}$:\n$$ \\hat{\\beta}_{G_g} = y_{G_g} \\left( \\frac{\\|y_{G_g}\\|_{2} - \\lambda}{\\|y_{G_g}\\|_{2}} \\right) = \\left( 1 - \\frac{\\lambda}{\\|y_{G_g}\\|_{2}} \\right) y_{G_g} $$\nThis is the block soft-thresholding operator.\n\nCase 2: The group is inactive, $\\hat{\\beta}_{G_g} = 0$.\nIn this case, the KKT condition is $-y_{G_g} + \\lambda v_g = 0$ for some subgradient vector $v_g$ with $\\|v_g\\|_2 \\le 1$. This implies $y_{G_g} = \\lambda v_g$. Taking the norm of both sides:\n$$ \\|y_{G_g}\\|_2 = \\|\\lambda v_g\\|_2 = \\lambda \\|v_g\\|_2 $$\nSince $\\|v_g\\|_2 \\le 1$, this holds if and only if $\\|y_{G_g}\\|_2 \\le \\lambda$.\n\nSummary of group activity: A group $G_g$ becomes active (i.e., $\\hat{\\beta}_{G_g}$ becomes non-zero) if and only if $\\lambda < \\|y_{G_g}\\|_2$. The value $\\lambda_{crit}^{(g)} = \\|y_{G_g}\\|_2$ is the critical regularization parameter for group $G_g$.\n\nNow, we compute these critical values for the given data:\n$y = (5, 1, -2, 3, 0, 0)^{\\top}$, $G_1 = \\{1\\}$, $G_2 = \\{2,3\\}$, $G_3 = \\{4,5,6\\}$.\n\nFor Group $G_1 = \\{1\\}$:\n$y_{G_1} = (5)$.\n$\\|y_{G_1}\\|_2 = |5| = 5$.\nSo, $\\lambda_{crit}^{(1)} = 5$.\n\nFor Group $G_2 = \\{2,3\\}$:\n$y_{G_2} = (1, -2)^{\\top}$.\n$\\|y_{G_2}\\|_2 = \\sqrt{1^2 + (-2)^2} = \\sqrt{1+4} = \\sqrt{5}$.\nSo, $\\lambda_{crit}^{(2)} = \\sqrt{5}$.\n\nFor Group $G_3 = \\{4,5,6\\}$:\n$y_{G_3} = (3, 0, 0)^{\\top}$.\n$\\|y_{G_3}\\|_2 = \\sqrt{3^2 + 0^2 + 0^2} = 3$.\nSo, $\\lambda_{crit}^{(3)} = 3$.\n\nTo determine the order in which groups enter the model as $\\lambda$ decreases from a large value, we sort the critical parameters in descending order:\nThe critical values are $5$, $3$, and $\\sqrt{5} \\approx 2.236$.\nThe largest critical value is $5$, corresponding to Group $G_1$.\nThe second largest is $3$, corresponding to Group $G_3$.\nThe smallest is $\\sqrt{5}$, corresponding to Group $G_2$.\n\nThe solution path is as follows:\n- For $\\lambda \\ge 5$, all groups are inactive ($\\hat{\\beta}=0$).\n- At $\\lambda=5$, Group $G_1$ becomes active. This is the first critical parameter, and $G_1$ is the first group to enter the model.\n- For $3 \\le \\lambda < 5$, only Group $G_1$ is active.\n- At $\\lambda=3$, Group $G_3$ becomes active. This is the second critical parameter, and $G_3$ is the second group to enter the model.\n- For $\\sqrt{5} \\le \\lambda < 3$, Groups $G_1$ and $G_3$ are active.\n- At $\\lambda=\\sqrt{5}$, Group $G_2$ becomes active. This is the third critical parameter, and $G_2$ is the third group to enter the model.\n- For $0 \\le \\lambda < \\sqrt{5}$, all groups are active.\n\nThe problem asks for the exact value of the second critical regularization parameter at which the second group to enter the model becomes active. Based on our analysis, the second group to enter is $G_3$, and this occurs when $\\lambda$ decreases past the value $3$.\n\nThus, the second critical parameter is $3$.\n\nLet's compute the solution $\\hat{\\beta}(\\lambda)$ for the regime $3 \\le \\lambda < 5$ to explicitly verify this.\n- Group $G_1$ is active: $\\hat{\\beta}_{G_1} = \\hat{\\beta}_1 = (1 - \\lambda/5) y_1 = (1 - \\lambda/5) \\cdot 5 = 5-\\lambda$.\n- Group $G_2$ is inactive: $\\|y_{G_2}\\|_2=\\sqrt{5} < 3 \\le \\lambda$. So $\\hat{\\beta}_{G_2}=(0,0)^{\\top}$.\n- Group $G_3$ is inactive: $\\|y_{G_3}\\|_2=3 \\le \\lambda$. So $\\hat{\\beta}_{G_3}=(0,0,0)^{\\top}$.\nAt the boundary $\\lambda=3$: $\\hat{\\beta}_1=2$, and all other coefficients are zero. For any $\\lambda$ just below $3$, say $\\lambda = 3-\\epsilon$ for a small $\\epsilon > 0$, Group $G_3$ becomes active:\n- $\\hat{\\beta}_{G_3} = (1 - \\lambda/3) y_{G_3} = (1 - (3-\\epsilon)/3)(3,0,0)^{\\top} = (\\epsilon/3)(3,0,0)^{\\top} = (\\epsilon,0,0)^{\\top} \\neq 0$.\nThis confirms that $\\lambda=3$ is indeed the critical value at which the second group ($G_3$) becomes active.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "Theoretical exercises provide clarity, but the ultimate test of a model is its performance under realistic, and often imperfect, conditions. This computational practice addresses a crucial applied question: how sensitive is Group Lasso to the way we define the groups ? You will implement a proximal gradient solver and run simulations to quantify the loss in estimation and prediction accuracy when the predefined groups are misaligned with the true signal structure, highlighting the interplay between model assumptions and practical outcomes.",
            "id": "3126821",
            "problem": "You are asked to investigate the effect of mis-specified groups on the Group Lasso estimator in a linear regression setting by simulation, and to quantify the resulting performance loss. Work in the following purely mathematical framework.\n\nConsider the linear model with no intercept: given a design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ and coefficients $\\boldsymbol{\\beta}^{\\star} \\in \\mathbb{R}^{p}$, the response is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$, where noise $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$. The Group Lasso estimator $\\widehat{\\boldsymbol{\\beta}}$ solves the convex optimization problem\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_{2}^{2} \\;+\\; \\lambda \\sum_{g=1}^{m} w_{g} \\lVert \\boldsymbol{\\beta}_{G_{g}} \\rVert_{2},\n$$\nwhere $\\{G_{1},\\dots,G_{m}\\}$ is a partition of $\\{1,\\dots,p\\}$ into $m$ groups, $\\boldsymbol{\\beta}_{G_{g}}$ denotes the subvector of $\\boldsymbol{\\beta}$ indexed by group $G_{g}$, and $w_{g} = \\sqrt{|G_{g}|}$ are the group weights.\n\nYour task is to write a complete program that:\n- Simulates data $\\mathbf{X}$ and $\\mathbf{y}$ according to the model above with a contiguous true support of $\\boldsymbol{\\beta}^{\\star}$ that can be either wholly contained within one group (“correctly specified groups”) or split across two adjacent groups (“mis-specified groups”).\n- Solves the Group Lasso optimization by a principled first-order method that separates the smooth loss from the non-smooth penalty and uses an appropriate proximity operator for the group penalty.\n- Quantifies the performance loss due to mis-specified groups, compared to correctly specified groups, on both coefficient estimation error and prediction error.\n\nFundamental base you may assume:\n- The definitions of the Euclidean norm and the basic properties of convex functions and subgradients.\n- The gradient of the quadratic loss $\\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_{2}^{2}$ with respect to $\\boldsymbol{\\beta}$ equals $\\frac{1}{n}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})$.\n- The notion that for a composite convex objective with a Lipschitz-continuous gradient term and a separable non-smooth term, a proximal-gradient iteration is appropriate.\n\nData-generating process:\n- Construct $\\mathbf{X}$ with correlated columns as follows. Let $\\rho \\in [0,1)$ and draw $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$ with independent standard normal entries. Define columns recursively by\n$$\n\\mathbf{x}_{1} = \\mathbf{z}_{1}, \\quad \\mathbf{x}_{j} = \\rho \\,\\mathbf{x}_{j-1} + \\sqrt{1-\\rho^{2}}\\,\\mathbf{z}_{j} \\;\\; \\text{for} \\;\\; j \\in \\{2,\\dots,p\\}.\n$$\nThen center each column to have empirical mean $0$ and scale to have empirical second moment $1$, i.e., enforce $\\frac{1}{n}\\sum_{i=1}^{n} x_{ij}^{2} = 1$ for each $j$.\n- Let $\\boldsymbol{\\beta}^{\\star}$ have a single contiguous nonzero block of length $L$, starting at index $s$ (indices are $0$-based), with each nonzero equal to a fixed magnitude $b_{0} > 0$, and all other entries equal to $0$.\n- Generate $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$ independently of $\\mathbf{X}$.\n\nGroup specifications:\n- Mis-specified groups: uniform adjacent groups of fixed size $g_{\\text{mis}} = 5$ formed as $G^{\\text{mis}}_{1} = \\{0,1,2,3,4\\}$, $G^{\\text{mis}}_{2} = \\{5,6,7,8,9\\}$, and so on, with the last group possibly smaller if $p$ is not a multiple of $5$.\n- Correctly specified groups: form groups of base size $g_{\\text{cor}} = 5$ but modify the partition so that the entire true nonzero block $\\{s, s+1, \\dots, s+L-1\\}$ appears as a single group $G^{\\text{cor}}_{k}$ in its entirety (even if this deviates from the uniform partition), and the remaining indices are partitioned into adjacent groups of size at most $5$ without splitting any index.\n\nPerformance metrics to compute for each test case:\n- Estimation error: $E(\\widehat{\\boldsymbol{\\beta}}) = \\lVert \\widehat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$.\n- In-sample prediction error: $P(\\widehat{\\boldsymbol{\\beta}}) = \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$.\n- Performance loss due to mis-specification on each metric is defined as the difference between the mis-specified value and the correctly specified value.\n\nAlgorithmic requirements:\n- Use a proximal-gradient method with a constant step size based on a valid Lipschitz constant $L_{\\nabla}$ of the gradient of the quadratic loss. You must compute $L_{\\nabla}$ from $\\mathbf{X}$ and $n$ and then choose a step size $\\tau$ satisfying $0 < \\tau \\le 1/L_{\\nabla}$.\n- Use the exact proximity operator for the weighted group $\\ell_{2}$ penalty $\\sum_{g} w_{g} \\lVert \\boldsymbol{\\beta}_{G_{g}} \\rVert_{2}$ with weights $w_{g} = \\sqrt{|G_{g}|}$.\n- Initialize at $\\boldsymbol{0}$ and iterate until a relative tolerance is met or a maximum number of iterations is reached.\n\nTest suite:\nImplement and run your program on the following five test cases. Each tuple is $(\\text{seed}, n, p, \\rho, s, L, b_{0}, \\sigma, \\lambda)$, with all indices $0$-based.\n- Case $1$: $(0, 120, 30, 0.3, 6, 6, 2.0, 0.5, 0.2)$.\n- Case $2$: $(1, 120, 30, 0.3, 6, 6, 0.5, 0.5, 0.2)$.\n- Case $3$: $(2, 120, 30, 0.3, 6, 6, 3.0, 0.0, 0.1)$.\n- Case $4$: $(3, 60, 80, 0.7, 12, 6, 2.0, 0.5, 0.25)$.\n- Case $5$ (boundary, no split under mis-specified grouping): $(4, 120, 30, 0.3, 5, 5, 2.0, 0.5, 0.2)$.\n\nImplementation details:\n- For each case, construct $\\mathbf{X}$, $\\boldsymbol{\\beta}^{\\star}$, and $\\mathbf{y}$ as specified, with the given random seed controlling all randomness.\n- Solve the Group Lasso twice on the same data: once using the mis-specified groups and once using the correctly specified groups (with weights $w_{g} = \\sqrt{|G_{g}|}$ in both).\n- Compute $E(\\widehat{\\boldsymbol{\\beta}})$ and $P(\\widehat{\\boldsymbol{\\beta}})$ for both runs, and then compute the performance losses as:\n$$\n\\Delta E = E(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - E(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}}), \\quad \\Delta P = P(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - P(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}}).\n$$\n\nFinal output format:\n- Your program should produce a single line of output containing a list of length $5$, where each element is a two-element list $[\\Delta E, \\Delta P]$ in this order for the corresponding test case. The floats should be printed with exactly $6$ digits after the decimal point. For example: $[[0.123456,0.234567],[\\dots],\\dots]$.\n- No additional text should be printed.\n\nAngle units do not appear in this problem. There are no physical units to report. All percentages, if any, must be represented as decimals, but none are needed here.",
            "solution": "The problem requires a simulation study to quantify the performance loss of the Group Lasso estimator when the predefined groups are mis-specified relative to the true structure of the underlying signal. This involves generating synthetic data, implementing a robust solver for the Group Lasso optimization problem, and comparing the results under two different grouping assumptions.\n\nThe linear model is given by $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\boldsymbol{\\beta}^{\\star} \\in \\mathbb{R}^{p}$ is the true coefficient vector, and $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$ is a vector of i.i.d. Gaussian noise.\n\nThe Group Lasso estimator, $\\widehat{\\boldsymbol{\\beta}}$, is found by solving the following convex optimization problem:\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}} \\;\\; \\underbrace{\\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_{2}^{2}}_{f(\\boldsymbol{\\beta})} \\;+\\; \\underbrace{\\lambda \\sum_{g=1}^{m} w_{g} \\lVert \\boldsymbol{\\beta}_{G_{g}} \\rVert_{2}}_{g(\\boldsymbol{\\beta})}\n$$\nThis objective function is a composite of a smooth, differentiable loss term $f(\\boldsymbol{\\beta})$ (the mean squared error) and a non-smooth, convex penalty term $g(\\boldsymbol{\\beta})$. The penalty term encourages sparsity at the group level, meaning entire subvectors $\\boldsymbol{\\beta}_{G_{g}}$ are driven to zero. The groups $\\{G_{1},\\dots,G_{m}\\}$ form a partition of the variable indices $\\{1,\\dots,p\\}$, and the weights are set to $w_{g} = \\sqrt{|G_{g}|}$ to counteract the dependency of the $\\ell_2$-norm on group size.\n\n**Data Generation**\nThe simulation begins by constructing the data $(\\mathbf{X}, \\mathbf{y}, \\boldsymbol{\\beta}^{\\star})$ according to the specified process for each test case.\n1.  **Design Matrix $\\mathbf{X}$**: To introduce multicollinearity, a common feature in real-world data, the columns of $\\mathbf{X}$ are generated using an autoregressive process. First, a matrix $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$ is drawn with independent $\\mathcal{N}(0, 1)$ entries. The columns of $\\mathbf{X}$, denoted $\\mathbf{x}_{j}$, are then defined as $\\mathbf{x}_{1} = \\mathbf{z}_{1}$ and $\\mathbf{x}_{j} = \\rho \\,\\mathbf{x}_{j-1} + \\sqrt{1-\\rho^{2}}\\,\\mathbf{z}_{j}$ for $j=2,\\dots,p$. This creates a Toeplitz-like correlation structure where adjacent columns are more highly correlated. Finally, each column is centered to have zero mean and scaled such that its empirical second moment is unity, i.e., $\\frac{1}{n}\\lVert\\mathbf{x}_j\\rVert_2^2 = 1$.\n2.  **True Coefficients $\\boldsymbol{\\beta}^{\\star}$**: The true signal is structured to have group sparsity. The vector $\\boldsymbol{\\beta}^{\\star}$ is zero everywhere except for a single contiguous block of length $L$ starting at index $s$, where the non-zero entries all have the value $b_0$.\n3.  **Response Vector $\\mathbf{y}$**: The response is generated via the linear model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$, with noise $\\boldsymbol{\\varepsilon}$ drawn from an i.i.d. Gaussian distribution with variance $\\sigma^2$.\n\n**Group Specifications**\nThe core of the investigation lies in comparing two group partitions:\n1.  **Mis-specified Groups ($G^{\\text{mis}}$)**: A simple, uniform partition of indices into adjacent groups of a fixed size $g_{\\text{mis}}=5$. This structure is agnostic to the true support of $\\boldsymbol{\\beta}^{\\star}$ and thus may split the non-zero block across multiple groups.\n2.  **Correctly Specified Groups ($G^{\\text{cor}}$)**: An \"oracle\" partition where the non-zero block $\\{s, s+1, \\dots, s+L-1\\}$ is known and defined as a single group. The remaining indices (those not in the support) are partitioned into adjacent groups of size at most $5$. This represents the ideal scenario where the group structure assumed by the model matches the true underlying structure of the signal.\n\n**Optimization via Proximal Gradient Method**\nThe composite nature of the Group Lasso objective function makes it amenable to a proximal gradient method (also known as Iterative Shrinkage-Thresholding Algorithm or ISTA). The algorithm iteratively performs a gradient descent step on the smooth part $f(\\boldsymbol{\\beta})$ followed by a proximal mapping step corresponding to the non-smooth part $g(\\boldsymbol{\\beta})$. The update rule for iteration $k+1$ is:\n$$\n\\boldsymbol{\\beta}^{(k+1)} = \\text{prox}_{\\tau g}\\left(\\boldsymbol{\\beta}^{(k)} - \\tau \\nabla f(\\boldsymbol{\\beta}^{(k)})\\right)\n$$\n- **Gradient Step**: The gradient of the loss term is $\\nabla f(\\boldsymbol{\\beta}) = \\frac{1}{n}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})$.\n- **Step Size $\\tau$**: For convergence, the step size must satisfy $0 < \\tau \\le 1/L_{\\nabla}$, where $L_{\\nabla}$ is the Lipschitz constant of $\\nabla f$. The constant $L_{\\nabla}$ is the largest eigenvalue of the Hessian $\\nabla^2 f(\\boldsymbol{\\beta}) = \\frac{1}{n}\\mathbf{X}^{\\top}\\mathbf{X}$, which can be computed as $L_{\\nabla} = \\frac{1}{n} \\sigma_{\\max}^2(\\mathbf{X})$. A step size of $\\tau = 1/L_{\\nabla}$ is chosen.\n- **Proximal Step**: The proximity operator for the Group Lasso penalty $g(\\boldsymbol{\\beta}) = \\sum_g \\lambda w_g \\lVert\\boldsymbol{\\beta}_{G_g}\\rVert_2$ is separable across the groups. For a single group $G_g$, the operator is block soft-thresholding:\n$$\n\\text{prox}_{\\tau\\lambda w_g \\lVert \\cdot \\rVert_2}(\\mathbf{u}) = \\mathbf{u} \\left(1 - \\frac{\\tau\\lambda w_g}{\\lVert \\mathbf{u} \\rVert_2}\\right)_{+}\n$$\nwhere $(\\cdot)_+ = \\max(0, \\cdot)$. This operation shrinks the group subvector $\\mathbf{u}$ towards the origin, setting it to exactly zero if its norm is below the threshold $\\tau\\lambda w_g$. The overall proximal operator applies this block-wise to the corresponding subvectors of its argument. The algorithm is initialized with $\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}$ and iterates until the relative change in $\\boldsymbol{\\beta}$ falls below a specified tolerance.\n\n**Performance Evaluation**\nFor each test case, the Group Lasso problem is solved twice: once with the mis-specified groups to obtain $\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}$ and once with the correctly specified groups for $\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}}$. The performance is evaluated using two metrics:\n1.  **Estimation Error**: $E(\\widehat{\\boldsymbol{\\beta}}) = \\lVert \\widehat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$, which measures how close the estimated coefficients are to the true coefficients.\n2.  **In-sample Prediction Error**: $P(\\widehat{\\boldsymbol{\\beta}}) = \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$, which measures the error in predicting the noiseless signal component.\n\nThe performance loss due to mis-specification is then calculated as the difference in these metrics: $\\Delta E = E(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - E(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}})$ and $\\Delta P = P(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - P(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}})$. A positive loss indicates that the mis-specified grouping leads to worse performance, as expected.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Simulates the effect of mis-specified groups on the Group Lasso estimator.\n\n    For each test case, it:\n    1. Generates correlated data (X, y) based on a true sparse signal (beta_star).\n    2. Defines two group structures: 'mis-specified' (uniform) and 'correctly specified' (oracle).\n    3. Solves the Group Lasso problem for both group structures using a proximal gradient method.\n    4. Computes estimation and prediction errors for both solutions.\n    5. Calculates the performance loss (mis-specified error - correct error).\n    6. Formats and prints the results as specified.\n    \"\"\"\n    \n    # Constants for the proximal gradient solver\n    MAX_ITER = 10000\n    TOL = 1e-6\n\n    def generate_data(seed, n, p, rho, s, L, b0, sigma):\n        \"\"\"Generates data (X, y, beta_star) according to the problem specification.\"\"\"\n        rng = np.random.default_rng(seed)\n\n        # Generate correlated design matrix X\n        Z = rng.standard_normal((n, p))\n        X = np.zeros((n, p))\n        X[:, 0] = Z[:, 0]\n        for j in range(1, p):\n            X[:, j] = rho * X[:, j - 1] + np.sqrt(1 - rho**2) * Z[:, j]\n\n        # Center and scale X to have mean 0 and second moment 1\n        X -= X.mean(axis=0, keepdims=True)\n        scales = np.sqrt(np.mean(X**2, axis=0))\n        scales[scales == 0] = 1.0  # Avoid division by zero\n        X /= scales\n\n        # Generate true coefficient vector beta_star\n        beta_star = np.zeros(p)\n        if L > 0:\n            beta_star[s : s + L] = b0\n\n        # Generate response vector y\n        epsilon = rng.standard_normal(n) * sigma\n        y = X @ beta_star + epsilon\n\n        return X, y, beta_star\n\n    def generate_mis_groups(p, group_size=5):\n        \"\"\"Generates mis-specified groups (uniform partition).\"\"\"\n        groups = []\n        for i in range(0, p, group_size):\n            groups.append(list(range(i, min(i + group_size, p))))\n        return groups\n\n    def generate_cor_groups(p, s, L, group_size=5):\n        \"\"\"Generates correctly specified groups (aligning with true support).\"\"\"\n        if L == 0:\n            return generate_mis_groups(p, group_size)\n\n        groups = []\n        \n        def partition_indices(indices, size):\n            return [indices[i:i + size] for i in range(0, len(indices), size) if indices[i:i+size]]\n\n        # Indices before the support block\n        pre_indices = list(range(0, s))\n        if pre_indices:\n            groups.extend(partition_indices(pre_indices, group_size))\n\n        # The support block as a single group\n        groups.append(list(range(s, s + L)))\n\n        # Indices after the support block\n        post_indices = list(range(s + L, p))\n        if post_indices:\n            groups.extend(partition_indices(post_indices, group_size))\n            \n        return groups\n\n    def group_lasso_solver(X, y, lambda_reg, groups):\n        \"\"\"Solves the Group Lasso problem using a proximal gradient method.\"\"\"\n        n, p = X.shape\n        \n        # Calculate Lipschitz constant and step size\n        C = (X.T @ X) / n\n        eigenvalues = linalg.eigvalsh(C)\n        L_nabla = eigenvalues[-1] if len(eigenvalues) > 0 else 1.0\n        \n        tau = 1.0 / L_nabla if L_nabla > 0 else 1.0\n        \n        # Pre-compute weights and thresholds for the proximity operator\n        weights = np.array([np.sqrt(len(g)) for g in groups if g])\n        \n        # Initialize beta and start iteration\n        beta = np.zeros(p)\n        for _ in range(MAX_ITER):\n            beta_old = beta.copy()\n            \n            # Gradient descent step on the smooth part\n            grad = (X.T @ (X @ beta - y)) / n\n            z = beta - tau * grad\n            \n            # Proximal step on the non-smooth part (group-wise soft-thresholding)\n            beta_new = np.zeros(p)\n            group_idx_counter = 0\n            for i, g in enumerate(groups):\n                if not g: continue # Skip empty groups\n                threshold = tau * lambda_reg * weights[group_idx_counter]\n                group_idx_counter += 1\n                \n                z_g = z[g]\n                norm_z_g = np.linalg.norm(z_g)\n                \n                if norm_z_g > threshold:\n                    factor = 1 - threshold / norm_z_g\n                    beta_new[g] = z_g * factor\n                else:\n                    beta_new[g] = 0.0\n            \n            beta = beta_new\n            \n            # Check for convergence\n            rel_diff = np.linalg.norm(beta - beta_old) / (np.linalg.norm(beta_old) + 1e-8)\n            if rel_diff  TOL:\n                break\n                \n        return beta\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 120, 30, 0.3, 6, 6, 2.0, 0.5, 0.2),\n        (1, 120, 30, 0.3, 6, 6, 0.5, 0.5, 0.2),\n        (2, 120, 30, 0.3, 6, 6, 3.0, 0.0, 0.1),\n        (3, 60, 80, 0.7, 12, 6, 2.0, 0.5, 0.25),\n        (4, 120, 30, 0.3, 5, 5, 2.0, 0.5, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, n, p, rho, s, L, b0, sigma, lambda_reg = case\n        \n        X, y, beta_star = generate_data(seed, n, p, rho, s, L, b0, sigma)\n        \n        # Run for mis-specified groups\n        groups_mis = generate_mis_groups(p, group_size=5)\n        beta_hat_mis = group_lasso_solver(X, y, lambda_reg, groups_mis)\n        \n        # Run for correctly specified groups\n        groups_cor = generate_cor_groups(p, s, L, group_size=5)\n        beta_hat_cor = group_lasso_solver(X, y, lambda_reg, groups_cor)\n        \n        # Calculate performance metrics\n        y_true_signal = X @ beta_star\n\n        E_mis = np.sum((beta_hat_mis - beta_star)**2)\n        P_mis = np.sum((X @ beta_hat_mis - y_true_signal)**2) / n\n        \n        E_cor = np.sum((beta_hat_cor - beta_star)**2)\n        P_cor = np.sum((X @ beta_hat_cor - y_true_signal)**2) / n\n\n        # Calculate performance loss\n        delta_E = E_mis - E_cor\n        delta_P = P_mis - P_cor\n        \n        results.append([delta_E, delta_P])\n\n    # Final print statement in the exact required format.\n    output_str_parts = []\n    for dE, dP in results:\n        dE_str = f\"{dE:.6f}\"\n        dP_str = f\"{dP:.6f}\"\n        output_str_parts.append(f\"[{dE_str},{dP_str}]\")\n        \n    final_output = f\"[{','.join(output_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}