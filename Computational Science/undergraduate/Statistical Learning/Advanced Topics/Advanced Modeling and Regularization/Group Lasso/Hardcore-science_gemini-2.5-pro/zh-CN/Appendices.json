{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握组套索（Group Lasso），理解其变量选择的核心机制至关重要。本练习  通过一个简化的无噪声正交设计场景，剥离了问题的复杂性，让你可以专注于基本原理。通过运用Karush-Kuhn-Tucker (KKT) 最优性条件，你将从第一性原理出发，推导出精确的解路径，并亲眼观察随着正则化参数 $\\lambda$ 的变化，不同的系数组是如何被激活的。",
            "id": "3126799",
            "problem": "考虑一个无噪声线性模型中的组套索（Group Lasso）估计量。设设计矩阵为 $X \\in \\mathbb{R}^{6 \\times 6}$ 且 $X = I_{6}$，其中 $I_{6}$ 是 $6 \\times 6$ 的单位矩阵。将系数向量 $\\beta \\in \\mathbb{R}^{6}$ 划分为三个不重叠的组：$G_{1} = \\{1\\}$、$G_{2} = \\{2,3\\}$ 和 $G_{3} = \\{4,5,6\\}$。设观测响应为 $y \\in \\mathbb{R}^{6}$，其值为 $y = (5, 1, -2, 3, 0, 0)^{\\top}$。考虑组套索优化问题\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{6}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\sum_{g=1}^{3} \\| \\beta_{G_{g}} \\|_{2} \\right\\},\n$$\n其中 $\\lambda \\geq 0$ 是正则化参数，$\\| \\cdot \\|_{2}$ 表示欧几里得范数。假设所有组的权重均为单位权重。\n\n从通过 Karush-Kuhn-Tucker (KKT) 条件定义的最优性基本定义出发，并且不使用任何预先推导的解公式，解析地确定 $\\hat{\\beta}(\\lambda)$ 作为 $\\lambda$ 的函数，并为导致不同稀疏模式的每个 $\\lambda$ 范围验证 KKT 条件。利用 $X$ 的正交性和组结构，从第一性原理出发进行推理。\n\n具体要求：\n- 推导此问题的 KKT 条件，并用它们来刻画一个组 $G_{g}$ 何时是活跃的（非零）或不活跃的（零）。\n- 当 $\\lambda$ 从一个大值减小到 $0$ 时，计算 $\\hat{\\beta}(\\lambda)$ 的分段形式，并用它来确定组 $G_{1}$、$G_{2}$ 和 $G_{3}$ 进入模型的顺序。\n- 识别并陈述每个组变得活跃时的临界正则化参数的精确值。\n\n最后，报告第二个进入模型的组变得活跃时的第二个临界正则化参数的精确值。将最终答案表示为精确值。无需四舍五入，也不涉及单位。",
            "solution": "该问题是有效的。这是一个来自统计学习领域的良定优化问题，提供了所有必要的数据和定义。它具有科学依据，是客观且自洽的。\n\n组套索优化问题由下式给出\n$$ \\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{6}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\sum_{g=1}^{3} \\| \\beta_{G_{g}} \\|_{2} \\right\\} $$\n设计矩阵是单位矩阵 $X = I_{6}$。这简化了残差平方和 (RSS) 项：\n$$ \\frac{1}{2} \\| y - I_{6} \\beta \\|_{2}^{2} = \\frac{1}{2} \\| y - \\beta \\|_{2}^{2} = \\frac{1}{2} \\sum_{i=1}^{6} (y_i - \\beta_i)^2 $$\n各组是不重叠的。这使得目标函数可以分解为针对每个组 $G_g$ 的独立优化问题之和：\n$$ L(\\beta) = \\sum_{g=1}^{3} \\left( \\frac{1}{2} \\| y_{G_g} - \\beta_{G_g} \\|_{2}^{2} + \\lambda \\| \\beta_{G_g} \\|_{2} \\right) $$\n其中 $\\beta_{G_g}$ 和 $y_{G_g}$ 是 $\\beta$ 和 $y$ 中对应于组 $G_g$ 索引的子向量。我们可以通过分别最小化每个组的目标函数来找到全局最小值 $\\hat{\\beta}$。\n\n设 $L_g(\\beta_{G_g}) = \\frac{1}{2} \\| y_{G_g} - \\beta_{G_g} \\|_{2}^{2} + \\lambda \\| \\beta_{G_g} \\|_{2}$。项 $\\frac{1}{2} \\| y_{G_g} - \\beta_{G_g} \\|_{2}^{2}$ 是可微的，而 $\\lambda \\| \\beta_{G_g} \\|_{2}$ 是凸的，但在 $\\beta_{G_g}=0$ 处不可微。Karush-Kuhn-Tucker (KKT) 条件为此凸问题提供了最优性的充分必要条件。在最小化点 $\\hat{\\beta}_{G_g}$ 处，$L_g$ 的次梯度必须包含零：\n$$ \\nabla_{\\beta_{G_g}} \\left( \\frac{1}{2} \\| y_{G_g} - \\hat{\\beta}_{G_g} \\|_{2}^{2} \\right) + \\lambda \\cdot \\partial (\\| \\hat{\\beta}_{G_g} \\|_{2}) \\ni 0 $$\nRSS 项关于 $\\beta_{G_g}$ 的梯度是 $-(y_{G_g} - \\hat{\\beta}_{G_g})$。欧几里得范数 $\\partial (\\| \\beta_{G_g} \\|_{2})$ 的次微分是：\n$$ \\partial (\\| \\beta_{G_g} \\|_{2}) = \\begin{cases} \\{ \\frac{\\beta_{G_g}}{\\|\\beta_{G_g}\\|_{2}} \\}  \\text{若 } \\beta_{G_g} \\neq 0 \\\\ \\{ v_g \\in \\mathbb{R}^{|G_g|} : \\|v_g\\|_{2} \\le 1 \\}  \\text{若 } \\beta_{G_g} = 0 \\end{cases} $$\n因此，组 $g$ 的 KKT 条件是：\n$$ -(y_{G_g} - \\hat{\\beta}_{G_g}) + \\lambda v_g = 0 \\quad \\text{其中 } v_g \\in \\partial (\\| \\hat{\\beta}_{G_g} \\|_{2}) $$\n我们分析解 $\\hat{\\beta}_{G_g}$ 的两种情况：\n\n情况1：组是活跃的，$\\hat{\\beta}_{G_g} \\neq 0$。\n在这种情况下，$v_g = \\frac{\\hat{\\beta}_{G_g}}{\\|\\hat{\\beta}_{G_g}\\|_{2}}$。KKT 条件变为：\n$$ -(y_{G_g} - \\hat{\\beta}_{G_g}) + \\lambda \\frac{\\hat{\\beta}_{G_g}}{\\|\\hat{\\beta}_{G_g}\\|_{2}} = 0 $$\n重新整理各项以求解 $y_{G_g}$：\n$$ y_{G_g} = \\hat{\\beta}_{G_g} + \\lambda \\frac{\\hat{\\beta}_{G_g}}{\\|\\hat{\\beta}_{G_g}\\|_{2}} = \\hat{\\beta}_{G_g} \\left( 1 + \\frac{\\lambda}{\\|\\hat{\\beta}_{G_g}\\|_{2}} \\right) $$\n这表明 $\\hat{\\beta}_{G_g}$ 必须与 $y_{G_g}$ 共线。对两边取欧几里得范数：\n$$ \\|y_{G_g}\\|_{2} = \\left\\| \\hat{\\beta}_{G_g} \\left( 1 + \\frac{\\lambda}{\\|\\hat{\\beta}_{G_g}\\|_{2}} \\right) \\right\\|_{2} = \\|\\hat{\\beta}_{G_g}\\|_{2} \\left( 1 + \\frac{\\lambda}{\\|\\hat{\\beta}_{G_g}\\|_{2}} \\right) = \\|\\hat{\\beta}_{G_g}\\|_{2} + \\lambda $$\n求解估计系数的范数：\n$$ \\|\\hat{\\beta}_{G_g}\\|_{2} = \\|y_{G_g}\\|_{2} - \\lambda $$\n由于范数必须为正，此解仅在 $\\|y_{G_g}\\|_{2} - \\lambda > 0$ 时有效，即 $\\lambda  \\|y_{G_g}\\|_{2}$。如果此条件成立，我们可以将 $\\|\\hat{\\beta}_{G_g}\\|_{2}$ 代回到 $y_{G_g}$ 的方程中：\n$$ y_{G_g} = \\hat{\\beta}_{G_g} \\left( 1 + \\frac{\\lambda}{\\|y_{G_g}\\|_{2} - \\lambda} \\right) = \\hat{\\beta}_{G_g} \\left( \\frac{\\|y_{G_g}\\|_{2} - \\lambda + \\lambda}{\\|y_{G_g}\\|_{2} - \\lambda} \\right) = \\hat{\\beta}_{G_g} \\left( \\frac{\\|y_{G_g}\\|_{2}}{\\|y_{G_g}\\|_{2} - \\lambda} \\right) $$\n求解 $\\hat{\\beta}_{G_g}$：\n$$ \\hat{\\beta}_{G_g} = y_{G_g} \\left( \\frac{\\|y_{G_g}\\|_{2} - \\lambda}{\\|y_{G_g}\\|_{2}} \\right) = \\left( 1 - \\frac{\\lambda}{\\|y_{G_g}\\|_{2}} \\right) y_{G_g} $$\n这就是块软阈值算子。\n\n情况2：组是不活跃的，$\\hat{\\beta}_{G_g} = 0$。\n在这种情况下，KKT 条件是 $-y_{G_g} + \\lambda v_g = 0$，其中 $v_g$ 是某个满足 $\\|v_g\\|_2 \\le 1$ 的次梯度向量。这意味着 $y_{G_g} = \\lambda v_g$。对两边取范数：\n$$ \\|y_{G_g}\\|_2 = \\|\\lambda v_g\\|_2 = \\lambda \\|v_g\\|_2 $$\n由于 $\\|v_g\\|_2 \\le 1$，此式成立当且仅当 $\\|y_{G_g}\\|_2 \\le \\lambda$。\n\n组活跃性总结：一个组 $G_g$ 变得活跃（即 $\\hat{\\beta}_{G_g}$ 变为非零）当且仅当 $\\lambda  \\|y_{G_g}\\|_2$。值 $\\lambda_{crit}^{(g)} = \\|y_{G_g}\\|_2$ 是组 $G_g$ 的临界正则化参数。\n\n现在，我们为给定的数据计算这些临界值：\n$y = (5, 1, -2, 3, 0, 0)^{\\top}$，$G_1 = \\{1\\}$，$G_2 = \\{2,3\\}$，$G_3 = \\{4,5,6\\}$。\n\n对于组 $G_1 = \\{1\\}$：\n$y_{G_1} = (5)$。\n$\\|y_{G_1}\\|_2 = |5| = 5$。\n所以，$\\lambda_{crit}^{(1)} = 5$。\n\n对于组 $G_2 = \\{2,3\\}$：\n$y_{G_2} = (1, -2)^{\\top}$。\n$\\|y_{G_2}\\|_2 = \\sqrt{1^2 + (-2)^2} = \\sqrt{1+4} = \\sqrt{5}$。\n所以，$\\lambda_{crit}^{(2)} = \\sqrt{5}$。\n\n对于组 $G_3 = \\{4,5,6\\}$：\n$y_{G_3} = (3, 0, 0)^{\\top}$。\n$\\|y_{G_3}\\|_2 = \\sqrt{3^2 + 0^2 + 0^2} = 3$。\n所以，$\\lambda_{crit}^{(3)} = 3$。\n\n为了确定当 $\\lambda$ 从一个大值减小时各组进入模型的顺序，我们将临界参数按降序排列：\n1. $\\lambda_{crit}^{(1)} = 5$ (对于组 $G_1$)\n2. $\\lambda_{crit}^{(3)} = 3$ (对于组 $G_3$)\n3. $\\lambda_{crit}^{(2)} = \\sqrt{5} \\approx 2.236$ (对于组 $G_2$)\n\n解路径如下：\n- 对于 $\\lambda \\ge 5$，所有组都不活跃 ($\\hat{\\beta}=0$)。\n- 在 $\\lambda=5$ 时，组 $G_1$ 变得活跃。这是第一个临界参数，且 $G_1$ 是第一个进入模型的组。\n- 对于 $3 \\le \\lambda  5$，只有组 $G_1$ 是活跃的。\n- 在 $\\lambda=3$ 时，组 $G_3$ 变得活跃。这是第二个临界参数，且 $G_3$ 是第二个进入模型的组。\n- 对于 $\\sqrt{5} \\le \\lambda  3$，组 $G_1$ 和 $G_3$ 是活跃的。\n- 在 $\\lambda=\\sqrt{5}$ 时，组 $G_2$ 变得活跃。这是第三个临界参数，且 $G_2$ 是第三个进入模型的组。\n- 对于 $0 \\le \\lambda  \\sqrt{5}$，所有组都是活跃的。\n\n问题要求的是第二个进入模型的组变得活跃时的第二个临界正则化参数的精确值。根据我们的分析，第二个进入模型的组是 $G_3$，这发生在 $\\lambda$ 减小到经过值 $3$ 时。\n\n因此，第二个临界参数是 $3$。\n\n让我们计算在 $3 \\le \\lambda  5$ 范围内的解 $\\hat{\\beta}(\\lambda)$ 来明确验证这一点。\n- 组 $G_1$ 是活跃的：$\\hat{\\beta}_{G_1} = \\hat{\\beta}_1 = (1 - \\lambda/5) y_1 = (1 - \\lambda/5) \\cdot 5 = 5-\\lambda$。\n- 组 $G_2$ 是不活跃的：$\\|y_{G_2}\\|_2=\\sqrt{5}  3 \\le \\lambda$。所以 $\\hat{\\beta}_{G_2}=(0,0)^{\\top}$。\n- 组 $G_3$ 是不活跃的：$\\|y_{G_3}\\|_2=3 \\le \\lambda$。所以 $\\hat{\\beta}_{G_3}=(0,0,0)^{\\top}$。\n在边界 $\\lambda=3$ 处：$\\hat{\\beta}_1=2$，所有其他系数为零。对于任何略小于 $3$ 的 $\\lambda$，例如对于一个小的 $\\epsilon > 0$ 设 $\\lambda = 3-\\epsilon$，组 $G_3$ 变得活跃：\n- $\\hat{\\beta}_{G_3} = (1 - \\lambda/3) y_{G_3} = (1 - (3-\\epsilon)/3)(3,0,0)^{\\top} = (\\epsilon/3)(3,0,0)^{\\top} = (\\epsilon,0,0)^{\\top} \\neq 0$。\n这证实了 $\\lambda=3$ 确实是第二个组 ($G_3$) 变得活跃的临界值。",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "高效计算组套索的解通常需要沿一系列 $\\lambda$ 值追踪解路径。一个关键的起始步骤是确定起点：即保证所有系数为零的最小正则化值 $\\lambda_{\\max}$。本练习  将引导你从第一性原理出发，推导 $\\lambda_{\\max}$ 的封闭形式表达式，这个值对于初始化任何路径跟踪算法都至关重要。",
            "id": "3126744",
            "problem": "考虑一个带有分组预测变量的线性模型，该模型在组最小绝对收缩和选择算子 (group lasso) 框架下。数据由一个响应向量 $Y \\in \\mathbb{R}^{n}$ 和一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 组成，该矩阵的列被划分为 $G$ 个不相交的组。将参数向量表示为 $\\beta = (\\beta_{1}^{\\top}, \\dots, \\beta_{G}^{\\top})^{\\top}$，其中每个 $\\beta_{g} \\in \\mathbb{R}^{p_{g}}$ 对应于具有 $p_{g}$ 列的组 $g$，并且 $X = [X_{1} \\,\\, X_{2} \\,\\, \\dots \\,\\, X_{G}]$，其中 $X_{g} \\in \\mathbb{R}^{n \\times p_{g}}$。组 lasso 估计量最小化以下凸目标函数\n$$\n\\frac{1}{2n} \\left\\| Y - \\sum_{g=1}^{G} X_{g} \\beta_{g} \\right\\|_{2}^{2} \\;+\\; \\lambda \\sum_{g=1}^{G} w_{g} \\left\\| \\beta_{g} \\right\\|_{2},\n$$\n其中 $\\lambda \\geq 0$ 是正则化参数，$w_{g}  0$ 是给定的组权重。\n\n从凸优化的基本原理出发，特别是非光滑凸函数的次梯度最优性（Karush-Kuhn-Tucker (KKT) 条件）和欧几里得范数次微分，推导出一个闭式表达式，用于计算最小的正则化值 $\\lambda_{\\max}$，使得零向量 $\\hat{\\beta} = 0$ 满足组 lasso 问题的最优性条件。然后，对于下面给出的具体数据集，计算 $\\lambda_{\\max}$ 的数值。\n\n使用 $n = 5$，$G = 3$，以及以下的 $Y$ 和 $X$ 的分组列：\n- $Y = (2, \\,-1, \\,0, \\,3, \\,-2)^{\\top}$。\n- 组 $g=1$ 有 $p_{1} = 2$ 列：\n$$\nx_{1} = (1, \\,0, \\,1, \\,0, \\,1)^{\\top}, \\quad x_{2} = (0, \\,1, \\,0, \\,1, \\,0)^{\\top},\n$$\n所以 $X_{1} = [x_{1} \\,\\, x_{2}]$。\n- 组 $g=2$ 有 $p_{2} = 1$ 列：\n$$\nx_{3} = (1, \\,1, \\,1, \\,1, \\,1)^{\\top},\n$$\n所以 $X_{2} = [x_{3}]$。\n- 组 $g=3$ 有 $p_{3} = 1$ 列：\n$$\nx_{4} = (2, \\,-1, \\,0, \\,1, \\,2)^{\\top},\n$$\n所以 $X_{3} = [x_{4}]$。\n\n设组权重为 $w_{1} = \\sqrt{2}$，$w_{2} = 1$ 和 $w_{3} = 1$。您的最终答案必须是此数据集的 $\\lambda_{\\max}$ 的单个实数或精确的闭式表达式。无需四舍五入。最终答案无需单位。",
            "solution": "该问题经评估有效。它在科学上基于正则化线性模型（组 lasso）和凸优化的既定理论。该问题是适定的，具有完整且一致的给定条件，从而可以为 $\\lambda_{\\max}$ 推导出唯一且有意义的解。术语客观且正式。\n\n组 lasso 估计量 $\\hat{\\beta}$ 是以下凸优化问题的解：\n$$\n\\min_{\\beta} L(\\beta) = \\min_{\\beta_{1}, \\dots, \\beta_{G}} \\left\\{ \\frac{1}{2n} \\left\\| Y - \\sum_{g=1}^{G} X_{g} \\beta_{g} \\right\\|_{2}^{2} \\;+\\; \\lambda \\sum_{g=1}^{G} w_{g} \\left\\| \\beta_{g} \\right\\|_{2} \\right\\}\n$$\n目标函数 $L(\\beta)$ 是一个可微凸函数（最小二乘损失）和一个不可微但凸的函数（组 lasso 惩罚项）之和。Karush-Kuhn-Tucker (KKT) 条件为点 $\\hat{\\beta}$ 成为最小值点提供了必要和充分条件。这些条件表明，零向量必须属于目标函数在最小值点处的次微分，即 $0 \\in \\partial L(\\hat{\\beta})$。\n\n$L(\\beta)$ 的次微分由下式给出：\n$$\n\\partial L(\\beta) = \\nabla \\left( \\frac{1}{2n} \\left\\| Y - X\\beta \\right\\|_{2}^{2} \\right) + \\lambda \\sum_{g=1}^{G} w_{g} \\partial \\left\\| \\beta_{g} \\right\\|_{2}\n$$\n最小二乘项的梯度为：\n$$\n\\nabla \\left( \\frac{1}{2n} \\left\\| Y - X\\beta \\right\\|_{2}^{2} \\right) = -\\frac{1}{n} X^{\\top} (Y - X\\beta)\n$$\n我们感兴趣的是找到 $\\lambda_{\\max}$，即当解为零向量 $\\hat{\\beta} = 0$ 时 $\\lambda \\ge 0$ 的最小值。将 $\\hat{\\beta}=0$ 代入梯度项，得到：\n$$\n-\\frac{1}{n} X^{\\top} (Y - X \\cdot 0) = -\\frac{1}{n} X^{\\top} Y\n$$\n欧几里得范数 $\\|\\beta_g\\|_2$ 在 $\\beta_g = 0$ 处的次微分是 $\\mathbb{R}^{p_g}$ 中的闭单位球：\n$$\n\\partial \\|\\beta_g\\|_2 |_{\\beta_g = 0} = \\{ v_g \\in \\mathbb{R}^{p_g} \\mid \\|v_g\\|_2 \\le 1 \\}\n$$\n为使 $\\hat{\\beta}=0$ 成为解，KKT 条件 $0 \\in \\partial L(0)$ 必须成立。这意味着对于每个组 $g=1, \\dots, G$，必须存在次梯度 $v_g$ 满足 $\\|v_g\\|_2 \\le 1$，使得：\n$$\n0 = -\\frac{1}{n} X^{\\top}Y + \\lambda \\begin{pmatrix} w_1 v_1 \\\\ \\vdots \\\\ w_G v_G \\end{pmatrix}\n$$\n这个向量方程可以分解为每个组的一组方程：\n$$\n0 = -\\frac{1}{n} X_g^{\\top}Y + \\lambda w_g v_g \\quad \\text{for } g = 1, \\dots, G\n$$\n整理得到 $v_g$：\n$$\nv_g = \\frac{X_g^{\\top}Y}{n \\lambda w_g}\n$$\n这些次梯度有效的条件是 $\\|v_g\\|_2 \\le 1$。应用此约束：\n$$\n\\left\\| \\frac{X_g^{\\top}Y}{n \\lambda w_g} \\right\\|_{2} \\le 1\n$$\n因为 $\\lambda  0$ 且 $w_g  0$，我们可以写出：\n$$\n\\frac{\\|X_g^{\\top}Y\\|_2}{n \\lambda w_g} \\le 1\n$$\n这意味着对于每个组 $g$，$\\lambda$ 必须满足：\n$$\n\\lambda \\ge \\frac{\\|X_g^{\\top}Y\\|_2}{n w_g}\n$$\n为使 $\\hat{\\beta}=0$ 成为一个有效解，这个不等式必须对所有组同时成立。因此，$\\lambda$ 必须大于或等于这些下界的最大值。使得 $\\hat{\\beta}=0$ 成为解的最小 $\\lambda$ 值就是这个最大值。这定义了 $\\lambda_{\\max}$：\n$$\n\\lambda_{\\max} = \\max_{g \\in \\{1, \\dots, G\\}} \\left\\{ \\frac{\\|X_g^{\\top}Y\\|_2}{n w_g} \\right\\}\n$$\n现在，我们为给定的数据集计算这个值。\n给定条件是：\n$n = 5$\n$Y = (2, -1, 0, 3, -2)^{\\top}$\n组 1：$p_1=2$, $w_1 = \\sqrt{2}$, $X_1 = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  0 \\\\ 0  1 \\\\ 1  0 \\end{pmatrix}$\n组 2：$p_2=1$, $w_2 = 1$, $X_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$\n组 3：$p_3=1$, $w_3 = 1$, $X_3 = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix}$\n\n我们为每个组计算 $\\frac{\\|X_g^{\\top}Y\\|_2}{n w_g}$ 这一项。\n\n对于组 $g=1$：\n$X_1^{\\top}Y = \\begin{pmatrix} 1  0  1  0  1 \\\\ 0  1  0  1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1(2)+0(-1)+1(0)+0(3)+1(-2) \\\\ 0(2)+1(-1)+0(0)+1(3)+0(-2) \\end{pmatrix} = \\begin{pmatrix} 2-2 \\\\ -1+3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$\n$\\|X_1^{\\top}Y\\|_2 = \\sqrt{0^2 + 2^2} = 2$\n组 1 的项是 $\\frac{\\|X_1^{\\top}Y\\|_2}{n w_1} = \\frac{2}{5\\sqrt{2}} = \\frac{\\sqrt{2}}{5}$。\n\n对于组 $g=2$：\n$X_2^{\\top}Y = \\begin{pmatrix} 1  1  1  1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ -2 \\end{pmatrix} = 2 - 1 + 0 + 3 - 2 = 2$\n由于该组只有一个预测变量，结果是一个标量。$\\|X_2^{\\top}Y\\|_2 = |2| = 2$。\n组 2 的项是 $\\frac{\\|X_2^{\\top}Y\\|_2}{n w_2} = \\frac{2}{5 \\cdot 1} = \\frac{2}{5}$。\n\n对于组 $g=3$：\n$X_3^{\\top}Y = \\begin{pmatrix} 2  -1  0  1  2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\\\ -2 \\end{pmatrix} = 2(2) + (-1)(-1) + 0(0) + 1(3) + 2(-2) = 4 + 1 + 0 + 3 - 4 = 4$\n该组也只有一个预测变量，所以结果是一个标量。$\\|X_3^{\\top}Y\\|_2 = |4| = 4$。\n组 3 的项是 $\\frac{\\|X_3^{\\top}Y\\|_2}{n w_3} = \\frac{4}{5 \\cdot 1} = \\frac{4}{5}$。\n\n最后，我们通过取这些值的最大值来找到 $\\lambda_{\\max}$：\n$$\n\\lambda_{\\max} = \\max\\left\\{ \\frac{\\sqrt{2}}{5}, \\frac{2}{5}, \\frac{4}{5} \\right\\}\n$$\n为了比较这些值，我们注意到 $\\sqrt{2}  2  4$。因此，$\\frac{\\sqrt{2}}{5}  \\frac{2}{5}  \\frac{4}{5}$。\n最大值是 $\\frac{4}{5}$。",
            "answer": "$$\n\\boxed{\\frac{4}{5}}\n$$"
        },
        {
            "introduction": "组套索的有效性取决于一个重要假设：预定义的分组能够反映数据的真实潜在结构。这个动手模拟练习  将挑战你研究当这一假设被违反时会发生什么。通过实现一个近端梯度求解器，并比较在“正确指定”与“错误指定”分组结构下的模型性能，你将量化性能损失，并对分组定义的实际影响获得更深刻的理解。",
            "id": "3126821",
            "problem": "要求您通过模拟研究在线性回归设定中，错误指定的分组对组套索（Group Lasso）估计量的影响，并量化由此产生的性能损失。请在以下纯数学框架内进行操作。\n\n考虑一个不含截距项的线性模型：给定一个设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 和系数 $\\boldsymbol{\\beta}^{\\star} \\in \\mathbb{R}^{p}$，响应为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$，其中噪声 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$。组套索估计量 $\\widehat{\\boldsymbol{\\beta}}$ 通过求解以下凸优化问题得到\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_{2}^{2} \\;+\\; \\lambda \\sum_{g=1}^{m} w_{g} \\lVert \\boldsymbol{\\beta}_{G_{g}} \\rVert_{2},\n$$\n其中 $\\{G_{1},\\dots,G_{m}\\}$ 是将 $\\{1,\\dots,p\\}$ 划分为 $m$ 个组的一个划分，$\\boldsymbol{\\beta}_{G_{g}}$ 表示由组 $G_{g}$ 索引的 $\\boldsymbol{\\beta}$ 的子向量，而 $w_{g} = \\sqrt{|G_{g}|}$ 是组权重。\n\n您的任务是编写一个完整的程序，该程序能够：\n- 根据上述模型模拟数据 $\\mathbf{X}$ 和 $\\mathbf{y}$，其中 $\\boldsymbol{\\beta}^{\\star}$ 的真实支撑集是连续的，并且可以完全包含在一个分组内（“正确指定的分组”），或者被分割到两个相邻的分组中（“错误指定的分组”）。\n- 通过一种有原则的一阶方法求解组套索优化问题，该方法将光滑损失项与非光滑惩罚项分离，并对组惩罚项使用合适的邻近算子。\n- 在系数估计误差和预测误差两方面，量化由于分组错误指定（与正确指定的分组相比）所导致的性能损失。\n\n您可以假定的基本知识：\n- 欧几里得范数的定义以及凸函数和次梯度的基本性质。\n- 二次损失 $\\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_{2}^{2}$ 相对于 $\\boldsymbol{\\beta}$ 的梯度等于 $\\frac{1}{n}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})$。\n- 对于一个包含 Lipschitz 连续梯度项和可分离非光滑项的复合凸目标函数，使用邻近梯度迭代是合适的。\n\n数据生成过程：\n- 按如下方式构造具有相关列的 $\\mathbf{X}$。令 $\\rho \\in [0,1)$ 并从独立的标准正态分布中抽取 $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$。递归地定义各列：\n$$\n\\mathbf{x}_{1} = \\mathbf{z}_{1}, \\quad \\mathbf{x}_{j} = \\rho \\,\\mathbf{x}_{j-1} + \\sqrt{1-\\rho^{2}}\\,\\mathbf{z}_{j} \\;\\; \\text{for} \\;\\; j \\in \\{2,\\dots,p\\}.\n$$\n然后将每列中心化，使其经验均值为 0，并进行缩放，使其经验二阶矩为 1，即对每个 $j$ 强制满足 $\\frac{1}{n}\\sum_{i=1}^{n} x_{ij}^{2} = 1$。\n- 设 $\\boldsymbol{\\beta}^{\\star}$ 具有一个长度为 $L$ 的单个连续非零块，从索引 $s$ 开始（索引从 0 开始），其中每个非零元素的大小固定为 $b_{0}  0$，所有其他元素均为 0。\n- 生成 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$，其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$ 且与 $\\mathbf{X}$ 独立。\n\n分组设定：\n- 错误指定的分组：大小固定为 $g_{\\text{mis}} = 5$ 的均匀相邻分组，形式为 $G^{\\text{mis}}_{1} = \\{0,1,2,3,4\\}$，$G^{\\text{mis}}_{2} = \\{5,6,7,8,9\\}$，以此类推。如果 $p$ 不是 $5$ 的倍数，最后一个组可能会更小。\n- 正确指定的分组：以基本大小 $g_{\\text{cor}} = 5$ 形成分组，但修改该划分，使得整个真实的非零块 $\\{s, s+1, \\dots, s+L-1\\}$ 作为一个单独的组 $G^{\\text{cor}}_{k}$ 完整出现（即使这会偏离均匀划分），而剩余的索引则被划分为大小最多为 5 的相邻分组，且不分割任何索引。\n\n每个测试用例需要计算的性能指标：\n- 估计误差：$E(\\widehat{\\boldsymbol{\\beta}}) = \\lVert \\widehat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$。\n- 样本内预测误差：$P(\\widehat{\\boldsymbol{\\beta}}) = \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$。\n- 每个指标上由于错误指定分组导致的性能损失定义为错误指定值与正确指定值之间的差值。\n\n算法要求：\n- 使用邻近梯度法，步长为常数，该步长基于二次损失梯度的有效 Lipschitz 常数 $L_{\\nabla}$。您必须根据 $\\mathbf{X}$ 和 $n$ 计算 $L_{\\nabla}$，然后选择满足 $0  \\tau \\le 1/L_{\\nabla}$ 的步长 $\\tau$。\n- 对加权组 $\\ell_{2}$ 惩罚项 $\\sum_{g} w_{g} \\lVert \\boldsymbol{\\beta}_{G_{g}} \\rVert_{2}$ 使用精确的邻近算子，其中权重为 $w_{g} = \\sqrt{|G_{g}|}$。\n- 从 $\\boldsymbol{0}$ 开始初始化，并进行迭代，直到满足相对容差或达到最大迭代次数。\n\n测试套件：\n在以下五个测试用例上实现并运行您的程序。每个元组为 $(\\text{seed}, n, p, \\rho, s, L, b_{0}, \\sigma, \\lambda)$，所有索引均从 0 开始。\n- 案例 1：$(0, 120, 30, 0.3, 6, 6, 2.0, 0.5, 0.2)$。\n- 案例 2：$(1, 120, 30, 0.3, 6, 6, 0.5, 0.5, 0.2)$。\n- 案例 3：$(2, 120, 30, 0.3, 6, 6, 3.0, 0.0, 0.1)$。\n- 案例 4：$(3, 60, 80, 0.7, 12, 6, 2.0, 0.5, 0.25)$。\n- 案例 5（边界情况，在错误指定的分组下没有发生分割）：$(4, 120, 30, 0.3, 5, 5, 2.0, 0.5, 0.2)$。\n\n实现细节：\n- 对于每个案例，按照规定构造 $\\mathbf{X}$、$\\boldsymbol{\\beta}^{\\star}$ 和 $\\mathbf{y}$，并使用给定的随机种子控制所有随机过程。\n- 在相同的数据上求解组套索问题两次：一次使用错误指定的分组，另一次使用正确指定的分组（两次均使用权重 $w_{g} = \\sqrt{|G_{g}|}$）。\n- 为两次运行计算 $E(\\widehat{\\boldsymbol{\\beta}})$ 和 $P(\\widehat{\\boldsymbol{\\beta}})$，然后按下式计算性能损失：\n$$\n\\Delta E = E(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - E(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}}), \\quad \\Delta P = P(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - P(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}}).\n$$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个长度为 5 的列表，每个元素是对应测试用例的一个双元素列表 $[\\Delta E, \\Delta P]$（按此顺序）。浮点数应精确到小数点后 6 位。例如：$[[0.123456,0.234567],[\\dots],\\dots]$。\n- 不应打印任何额外文本。\n\n本问题不涉及角度单位。没有需要报告的物理单位。所有百分比（如有）必须表示为小数，但此处不需要。",
            "solution": "该问题要求进行一项模拟研究，以量化当预定义的分组与潜在信号的真实结构不匹配（即错误指定）时，组套索（Group Lasso）估计量的性能损失。这包括生成合成数据，为组套索优化问题实现一个鲁棒的求解器，并在两种不同的分组假设下比较结果。\n\n线性模型由 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$ 给出，其中 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\boldsymbol{\\beta}^{\\star} \\in \\mathbb{R}^{p}$ 是真实的系数向量，$\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$ 是一个独立同分布的高斯噪声向量。\n\n组套索估计量 $\\widehat{\\boldsymbol{\\beta}}$ 通过求解以下凸优化问题得到：\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}} \\;\\; \\underbrace{\\frac{1}{2n}\\lVert \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\rVert_{2}^{2}}_{f(\\boldsymbol{\\beta})} \\;+\\; \\underbrace{\\lambda \\sum_{g=1}^{m} w_{g} \\lVert \\boldsymbol{\\beta}_{G_{g}} \\rVert_{2}}_{g(\\boldsymbol{\\beta})}\n$$\n该目标函数由一个光滑、可微的损失项 $f(\\boldsymbol{\\beta})$（均方误差）和一个非光滑、凸的惩罚项 $g(\\boldsymbol{\\beta})$ 复合而成。惩罚项鼓励组级别的稀疏性，即促使整个子向量 $\\boldsymbol{\\beta}_{G_{g}}$ 变为零。分组 $\\{G_{1},\\dots,G_{m}\\}$ 构成了变量索引 $\\{1,\\dots,p\\}$ 的一个划分，权重设置为 $w_{g} = \\sqrt{|G_{g}|}$ 以抵消 $\\ell_2$-范数对组大小的依赖性。\n\n**数据生成**\n模拟首先根据每个测试用例的指定过程构建数据 $(\\mathbf{X}, \\mathbf{y}, \\boldsymbol{\\beta}^{\\star})$。\n1.  **设计矩阵 $\\mathbf{X}$**：为了引入多重共线性（这是真实世界数据的一个常见特征），$\\mathbf{X}$ 的列是使用自回归过程生成的。首先，从独立的 $\\mathcal{N}(0, 1)$ 分布中抽取一个矩阵 $\\mathbf{Z} \\in \\mathbb{R}^{n \\times p}$。然后，$\\mathbf{X}$ 的列（表示为 $\\mathbf{x}_{j}$）被定义为 $\\mathbf{x}_{1} = \\mathbf{z}_{1}$ 和 $\\mathbf{x}_{j} = \\rho \\,\\mathbf{x}_{j-1} + \\sqrt{1-\\rho^{2}}\\,\\mathbf{z}_{j}$ (对于 $j=2,\\dots,p$)。这会产生一个类托普利茨（Toeplitz-like）相关结构，其中相邻列的相关性更高。最后，对每列进行中心化以使其均值为零，并进行缩放以使其经验二阶矩为 1，即 $\\frac{1}{n}\\lVert\\mathbf{x}_j\\rVert_2^2 = 1$。\n2.  **真实系数 $\\boldsymbol{\\beta}^{\\star}$**：真实信号被构造成具有组稀疏性。向量 $\\boldsymbol{\\beta}^{\\star}$ 在各处均为零，除了一个从索引 $s$ 开始、长度为 $L$ 的单个连续块，其中非零项的值均为 $b_0$。\n3.  **响应向量 $\\mathbf{y}$**：响应是通过线性模型 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^{\\star} + \\boldsymbol{\\varepsilon}$ 生成的，其中噪声 $\\boldsymbol{\\varepsilon}$ 从方差为 $\\sigma^2$ 的独立同分布高斯分布中抽取。\n\n**分组设定**\n研究的核心在于比较两种分组划分：\n1.  **错误指定的分组 ($G^{\\text{mis}}$)**：将索引简单、均匀地划分为大小固定为 $g_{\\text{mis}}=5$ 的相邻分组。这种结构与 $\\boldsymbol{\\beta}^{\\star}$ 的真实支撑集无关，因此可能会将非零块分割到多个分组中。\n2.  **正确指定的分组 ($G^{\\text{cor}}$)**：一个“神谕式”的划分，其中非零块 $\\{s, s+1, \\dots, s+L-1\\}$ 是已知的，并被定义为一个单独的组。剩余的索引（即不在支撑集中的索引）被划分为大小最多为 5 的相邻分组。这代表了理想情景，即模型所假定的组结构与信号的真实潜在结构相匹配。\n\n**通过邻近梯度法进行优化**\n组套索目标函数的复合性质使其适用于邻近梯度法（也称为迭代收缩阈值算法或 ISTA）。该算法迭代地对光滑部分 $f(\\boldsymbol{\\beta})$ 执行梯度下降步，然后对非光滑部分 $g(\\boldsymbol{\\beta})$ 执行相应的邻近映射步。第 $k+1$ 次迭代的更新规则是：\n$$\n\\boldsymbol{\\beta}^{(k+1)} = \\text{prox}_{\\tau g}\\left(\\boldsymbol{\\beta}^{(k)} - \\tau \\nabla f(\\boldsymbol{\\beta}^{(k)})\\right)\n$$\n- **梯度步**：损失项的梯度为 $\\nabla f(\\boldsymbol{\\beta}) = \\frac{1}{n}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})$。\n- **步长 $\\tau$**：为保证收敛，步长必须满足 $0  \\tau \\le 1/L_{\\nabla}$，其中 $L_{\\nabla}$ 是 $\\nabla f$ 的 Lipschitz 常数。该常数是海森矩阵 $\\nabla^2 f(\\boldsymbol{\\beta}) = \\frac{1}{n}\\mathbf{X}^{\\top}\\mathbf{X}$ 的最大特征值，可以计算为 $L_{\\nabla} = \\frac{1}{n} \\sigma_{\\max}^2(\\mathbf{X})$。选择步长为 $\\tau = 1/L_{\\nabla}$。\n- **邻近步**：组套索惩罚项 $g(\\boldsymbol{\\beta}) = \\sum_g \\lambda w_g \\lVert\\boldsymbol{\\beta}_{G_g}\\rVert_2$ 的邻近算子在各个组之间是可分离的。对于单个组 $G_g$，该算子是块软阈值操作：\n$$\n\\text{prox}_{\\tau\\lambda w_g \\lVert \\cdot \\rVert_2}(\\mathbf{u}) = \\mathbf{u} \\left(1 - \\frac{\\tau\\lambda w_g}{\\lVert \\mathbf{u} \\rVert_2}\\right)_{+}\n$$\n其中 $(\\cdot)_+ = \\max(0, \\cdot)$。此操作将组子向量 $\\mathbf{u}$ 向原点收缩，如果其范数低于阈值 $\\tau\\lambda w_g$，则将其精确地设置为零。整体的邻近算子将其参数的相应子向量逐块地应用此操作。该算法以 $\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}$ 初始化，并迭代直到 $\\boldsymbol{\\beta}$ 的相对变化低于指定的容差。\n\n**性能评估**\n对于每个测试用例，组套索问题被求解两次：一次使用错误指定的分组得到 $\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}$，另一次使用正确指定的分组得到 $\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}}$。性能通过两个指标进行评估：\n1.  **估计误差**：$E(\\widehat{\\boldsymbol{\\beta}}) = \\lVert \\widehat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$，它衡量估计系数与真实系数的接近程度。\n2.  **样本内预测误差**：$P(\\widehat{\\boldsymbol{\\beta}}) = \\frac{1}{n} \\lVert \\mathbf{X}\\widehat{\\boldsymbol{\\beta}} - \\mathbf{X}\\boldsymbol{\\beta}^{\\star} \\rVert_{2}^{2}$，它衡量在预测无噪声信号分量时的误差。\n\n然后，由于分组错误指定而导致的性能损失计算为这些指标的差值：$\\Delta E = E(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - E(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}})$ 和 $\\Delta P = P(\\widehat{\\boldsymbol{\\beta}}_{\\text{mis}}) - P(\\widehat{\\boldsymbol{\\beta}}_{\\text{cor}})$。正的损失值表明错误指定的分组导致了更差的性能，这与预期相符。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Simulates the effect of mis-specified groups on the Group Lasso estimator.\n\n    For each test case, it:\n    1. Generates correlated data (X, y) based on a true sparse signal (beta_star).\n    2. Defines two group structures: 'mis-specified' (uniform) and 'correctly specified' (oracle).\n    3. Solves the Group Lasso problem for both group structures using a proximal gradient method.\n    4. Computes estimation and prediction errors for both solutions.\n    5. Calculates the performance loss (mis-specified error - correct error).\n    6. Formats and prints the results as specified.\n    \"\"\"\n    \n    # Constants for the proximal gradient solver\n    MAX_ITER = 10000\n    TOL = 1e-6\n\n    def generate_data(seed, n, p, rho, s, L, b0, sigma):\n        \"\"\"Generates data (X, y, beta_star) according to the problem specification.\"\"\"\n        rng = np.random.default_rng(seed)\n\n        # Generate correlated design matrix X\n        Z = rng.standard_normal((n, p))\n        X = np.zeros((n, p))\n        X[:, 0] = Z[:, 0]\n        for j in range(1, p):\n            X[:, j] = rho * X[:, j - 1] + np.sqrt(1 - rho**2) * Z[:, j]\n\n        # Center and scale X to have mean 0 and second moment 1\n        X -= X.mean(axis=0, keepdims=True)\n        scales = np.sqrt(np.mean(X**2, axis=0))\n        scales[scales == 0] = 1.0  # Avoid division by zero\n        X /= scales\n\n        # Generate true coefficient vector beta_star\n        beta_star = np.zeros(p)\n        if L  0:\n            beta_star[s : s + L] = b0\n\n        # Generate response vector y\n        epsilon = rng.standard_normal(n) * sigma\n        y = X @ beta_star + epsilon\n\n        return X, y, beta_star\n\n    def generate_mis_groups(p, group_size=5):\n        \"\"\"Generates mis-specified groups (uniform partition).\"\"\"\n        groups = []\n        for i in range(0, p, group_size):\n            groups.append(list(range(i, min(i + group_size, p))))\n        return groups\n\n    def generate_cor_groups(p, s, L, group_size=5):\n        \"\"\"Generates correctly specified groups (aligning with true support).\"\"\"\n        if L == 0:\n            return generate_mis_groups(p, group_size)\n\n        groups = []\n        \n        def partition_indices(indices, size):\n            return [indices[i:i + size] for i in range(0, len(indices), size)]\n\n        # Indices before the support block\n        pre_indices = list(range(0, s))\n        if pre_indices:\n            groups.extend(partition_indices(pre_indices, group_size))\n\n        # The support block as a single group\n        groups.append(list(range(s, s + L)))\n\n        # Indices after the support block\n        post_indices = list(range(s + L, p))\n        if post_indices:\n            groups.extend(partition_indices(post_indices, group_size))\n            \n        return groups\n\n    def group_lasso_solver(X, y, lambda_reg, groups):\n        \"\"\"Solves the Group Lasso problem using a proximal gradient method.\"\"\"\n        n, p = X.shape\n        \n        # Calculate Lipschitz constant and step size\n        C = (X.T @ X) / n\n        eigenvalues = linalg.eigvalsh(C)\n        L_nabla = eigenvalues[-1] if len(eigenvalues)  0 else 1.0\n        \n        tau = 1.0 / L_nabla if L_nabla  0 else 1.0\n        \n        # Pre-compute weights and thresholds for the proximity operator\n        weights = np.array([np.sqrt(len(g)) for g in groups])\n        thresholds = tau * lambda_reg * weights\n        \n        # Initialize beta and start iteration\n        beta = np.zeros(p)\n        for _ in range(MAX_ITER):\n            beta_old = beta.copy()\n            \n            # Gradient descent step on the smooth part\n            grad = (X.T @ (X @ beta - y)) / n\n            z = beta - tau * grad\n            \n            # Proximal step on the non-smooth part (group-wise soft-thresholding)\n            beta_new = np.zeros(p)\n            for i, g in enumerate(groups):\n                z_g = z[g]\n                norm_z_g = np.linalg.norm(z_g)\n                \n                if norm_z_g  thresholds[i]:\n                    factor = 1 - thresholds[i] / norm_z_g\n                    beta_new[g] = z_g * factor\n                else:\n                    beta_new[g] = 0.0\n            \n            beta = beta_new\n            \n            # Check for convergence\n            rel_diff = np.linalg.norm(beta - beta_old) / (np.linalg.norm(beta_old) + 1e-8)\n            if rel_diff  TOL:\n                break\n                \n        return beta\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 120, 30, 0.3, 6, 6, 2.0, 0.5, 0.2),\n        (1, 120, 30, 0.3, 6, 6, 0.5, 0.5, 0.2),\n        (2, 120, 30, 0.3, 6, 6, 3.0, 0.0, 0.1),\n        (3, 60, 80, 0.7, 12, 6, 2.0, 0.5, 0.25),\n        (4, 120, 30, 0.3, 5, 5, 2.0, 0.5, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, n, p, rho, s, L, b0, sigma, lambda_reg = case\n        \n        X, y, beta_star = generate_data(seed, n, p, rho, s, L, b0, sigma)\n        \n        # Run for mis-specified groups\n        groups_mis = generate_mis_groups(p, group_size=5)\n        beta_hat_mis = group_lasso_solver(X, y, lambda_reg, groups_mis)\n        \n        # Run for correctly specified groups\n        groups_cor = generate_cor_groups(p, s, L, group_size=5)\n        beta_hat_cor = group_lasso_solver(X, y, lambda_reg, groups_cor)\n        \n        # Calculate performance metrics\n        y_true_signal = X @ beta_star\n\n        E_mis = np.sum((beta_hat_mis - beta_star)**2)\n        P_mis = np.sum((X @ beta_hat_mis - y_true_signal)**2) / n\n        \n        E_cor = np.sum((beta_hat_cor - beta_star)**2)\n        P_cor = np.sum((X @ beta_hat_cor - y_true_signal)**2) / n\n\n        # Calculate performance loss\n        delta_E = E_mis - E_cor\n        delta_P = P_mis - P_cor\n        \n        results.append([delta_E, delta_P])\n\n    # Final print statement in the exact required format.\n    output_str_parts = []\n    for dE, dP in results:\n        dE_str = f\"{dE:.6f}\"\n        dP_str = f\"{dP:.6f}\"\n        output_str_parts.append(f\"[{dE_str},{dP_str}]\")\n        \n    final_output = f\"[{','.join(output_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}