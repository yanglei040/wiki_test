{
    "hands_on_practices": [
        {
            "introduction": "要想真正理解动量法的工作原理，没有什么比亲自动手算一遍更有效了。第一个练习将引导你使用经典动量算法，手动计算优化过程的最初几个步骤。通过将更新规则应用于一个简单的二次函数，你将具体地感受到“速度”矢量是如何累积历史梯度并引导参数走向最优解的。",
            "id": "2187765",
            "problem": "一个优化算法被用来最小化一个成本函数 $f(x, y)$，该函数依赖于两个参数 $x$ 和 $y$。成本函数由以下公式给出：\n$$f(x, y) = x^2 + 2y^2$$\n\n所选的算法是经典动量法。在步骤 $t$ 时，参数向量 $w_t = (x_t, y_t)$ 和速度向量 $v_t$ 的更新规则如下：\n1. $v_t = \\gamma v_{t-1} + \\eta \\nabla f(w_{t-1})$\n2. $w_t = w_{t-1} - v_t$\n\n其中 $\\nabla f(w_{t-1})$ 是在先前参数向量 $w_{t-1}$ 处计算的成本函数的梯度。\n\n优化从初始参数向量 $w_0 = (x_0, y_0) = (4, 2)$ 开始。初始速度向量为 $v_0 = (0, 0)$。算法的超参数设置为学习率 $\\eta = 0.1$ 和动量参数 $\\gamma = 0.9$。\n\n你的任务是计算经过两个完整更新步骤后的参数向量 $w_2 = (x_2, y_2)$。将你的答案表示为一个包含两个分量的行向量。",
            "solution": "成本函数为 $f(x,y)=x^{2}+2y^{2}$，所以其梯度为\n$$\n\\nabla f(x,y)=\\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n=\\begin{pmatrix} 2x \\\\ 4y \\end{pmatrix}.\n$$\n动量更新规则为 $v_{t}=\\gamma v_{t-1}+\\eta\\,\\nabla f(w_{t-1})$ 和 $w_{t}=w_{t-1}-v_{t}$，其中 $w_{0}=\\begin{pmatrix}4\\\\2\\end{pmatrix}$，$v_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$，$\\eta=\\frac{1}{10}$，以及 $\\gamma=\\frac{9}{10}$。\n\n第一步 $t=1$：\n$$\n\\nabla f(w_{0})=\\begin{pmatrix}2\\cdot 4\\\\4\\cdot 2\\end{pmatrix}\n=\\begin{pmatrix}8\\\\8\\end{pmatrix},\\qquad\nv_{1}=\\frac{9}{10}\\begin{pmatrix}0\\\\0\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}8\\\\8\\end{pmatrix}\n=\\begin{pmatrix}\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix},\n$$\n$$\nw_{1}=w_{0}-v_{1}=\\begin{pmatrix}4\\\\2\\end{pmatrix}-\\begin{pmatrix}\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{16}{5}\\\\\\frac{6}{5}\\end{pmatrix}.\n$$\n\n第二步 $t=2$：\n$$\n\\nabla f(w_{1})=\\begin{pmatrix}2\\cdot \\frac{16}{5}\\\\4\\cdot \\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{32}{5}\\\\\\frac{24}{5}\\end{pmatrix},\n$$\n$$\nv_{2}=\\frac{9}{10}\\begin{pmatrix}\\frac{4}{5}\\\\\\frac{4}{5}\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}\\frac{32}{5}\\\\\\frac{24}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{36}{50}\\\\\\frac{36}{50}\\end{pmatrix}+\\begin{pmatrix}\\frac{32}{50}\\\\\\frac{24}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{68}{50}\\\\\\frac{60}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{34}{25}\\\\\\frac{6}{5}\\end{pmatrix},\n$$\n$$\nw_{2}=w_{1}-v_{2}=\\begin{pmatrix}\\frac{16}{5}\\\\\\frac{6}{5}\\end{pmatrix}-\\begin{pmatrix}\\frac{34}{25}\\\\\\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{80}{25}-\\frac{34}{25}\\\\0\\end{pmatrix}\n=\\begin{pmatrix}\\frac{46}{25}\\\\0\\end{pmatrix}.\n$$\n因此，经过两个完整更新步骤后，参数向量为行向量 $\\begin{pmatrix}\\frac{46}{25}  0\\end{pmatrix}$。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{46}{25}  0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "在经典动量法的基础上，涅斯捷罗夫加速梯度（Nesterov Accelerated Gradient, NAG）引入了一个虽细微但强大的改变：它在一个“前瞻”位置计算梯度。这项练习将挑战你通过分析一段给定的代码来识别其实现的究竟是哪种算法。这不再是简单的计算，而是考验你对这两种重要优化方法之间关键差异的深入理解。",
            "id": "2187801",
            "problem": "在机器学习领域，迭代优化算法通过更新参数矢量 $w$ 来最小化损失函数 $L(w)$。两种流行的基于动量的方法是经典动量法（Classical Momentum）和 Nesterov 加速梯度（Nesterov Accelerated Gradient, NAG）。\n\n动量法的核心思想是加速在持续下降方向上的移动，并抑制振荡。这是通过在更新规则中加入一个“速度”项 $v$ 来实现的。更新依赖于学习率 $\\eta$ 和动量系数 $\\gamma$。\n\n这两种方法之间的关键区别在于计算损失函数梯度的*位置*。\n- **经典动量法（Classical Momentum）**：在当前参数位置 $w_t$ 计算梯度。\n- **Nesterov 加速梯度（NAG）**：它首先沿先前速度的方向进行“前瞻”一步，然后在这个前瞻位置计算梯度，以进行更精准的修正。\n\n考虑以下用于优化算法单次更新步骤的伪代码。函数 `compute_gradient_at(point)` 计算损失函数 $L$ 在给定 `point` 处的梯度。\n\n```\nfunction update_step(w, v, eta, gamma):\n    // w: current parameter vector\n    // v: current velocity vector\n    // eta: learning rate\n    // gamma: momentum coefficient\n\n    // Step 1: Calculate a temporary, projected position\n    w_projected = w - gamma * v\n\n    // Step 2: Compute the gradient at the projected position\n    grad = compute_gradient_at(w_projected)\n\n    // Step 3: Update the velocity vector\n    v_new = gamma * v + eta * grad\n\n    // Step 4: Update the parameter vector\n    w_new = w - v_new\n\n    // Return the updated parameter and velocity vectors\n    return w_new, v_new\n```\n\n该伪代码实现了哪种优化算法？\n\nA. 带动量的简单梯度下降（经典动量法）\n\nB. Nesterov 加速梯度 (NAG)\n\nC. 简单梯度下降\n\nD. Adagrad\n\nE. RMSprop",
            "solution": "给定一个更新方案，其参数为 $w$ 和速度 $v$，学习率为 $\\eta$，动量系数为 $\\gamma$。伪代码执行以下步骤：\n1) 投影位置：\n$$w_{\\text{proj}}=w-\\gamma v.$$\n2) 在投影位置的梯度：\n$$g=\\nabla L(w_{\\text{proj}})=\\nabla L(w-\\gamma v).$$\n3) 速度更新：\n$$v_{\\text{new}}=\\gamma v+\\eta g.$$\n4) 参数更新：\n$$w_{\\text{new}}=w-v_{\\text{new}}=w-(\\gamma v+\\eta g).$$\n\n经典动量法使用\n$$v_{t+1}=\\gamma v_{t}+\\eta \\nabla L(w_{t}),\\qquad w_{t+1}=w_{t}-v_{t+1},$$\n也就是说，梯度是在当前点 $w_{t}$ 处计算的。\n\nNesterov 加速梯度首先计算一个前瞻点\n$$\\tilde{w}_{t}=w_{t}-\\gamma v_{t},$$\n然后在该点计算梯度，\n$$g_{t}=\\nabla L(\\tilde{w}_{t})=\\nabla L(w_{t}-\\gamma v_{t}),$$\n并进行更新\n$$v_{t+1}=\\gamma v_{t}+\\eta g_{t},\\qquad w_{t+1}=w_{t}-v_{t+1}.$$\n\n将这些公式与伪代码进行比较，我们发现它与 Nesterov 的过程完全一致：梯度是在前瞻位置 $w-\\gamma v$ 计算的，然后应用标准的动量式速度和参数更新。因此，该伪代码实现的算法是 Nesterov 加速梯度。\n\n它不是简单梯度下降，因为存在速度项；它不是经典动量法，因为梯度不是在 $w$ 处计算的；它也不是 Adagrad 或 RMSprop，因为没有使用累积平方梯度进行逐参数的自适应缩放。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "为什么说 NAG 的“前瞻”步骤是一种改进？最后一个练习将通过一个优化器“冲过”最小值的特定场景，为你提供一个清晰的分析性解答。你将推导 NAG 在此情况下的更新步骤，并从数学上看到它在预测点计算梯度的方式如何使其能够做出更智能的修正。这项练习揭示了 NAG 的理论优势，解释了它在实践中为何通常更稳健、收敛更快。",
            "id": "2187789",
            "problem": "考虑一个机器学习模型，其参数正通过一个迭代优化算法进行更新。在步骤 $t$，优化器的状态由参数向量 $\\theta_t$ 和累积的动量向量 $v_t$ 描述。优化器的目标是最小化一个损失函数 $J(\\theta)$。更新过程由学习率 $\\eta  0$ 和动量系数 $\\gamma \\in (0, 1)$ 控制。\n\n两种常见算法，标准动量（Standard Momentum, SM）和 Nesterov 加速梯度（Nesterov Accelerated Gradient, NAG）的更新规则如下：\n\n- **标准动量 (SM):**\n  1. 计算梯度：$g_t = \\nabla J(\\theta_t)$\n  2. 更新动量：$v_{t+1} = \\gamma v_t + \\eta g_t$\n  3. 更新参数：$\\theta_{t+1} = \\theta_t - v_{t+1}$\n\n- **Nesterov 加速梯度 (NAG):**\n  1. 计算前瞻位置：$\\theta_{lookahead} = \\theta_t - \\gamma v_t$\n  2. 计算前瞻位置的梯度：$g_{lookahead} = \\nabla J(\\theta_{lookahead})$\n  3. 更新动量：$v_{t+1} = \\gamma v_t + \\eta g_{lookahead}$\n  4. 更新参数：$\\theta_{t+1} = \\theta_t - v_{t+1}$\n\n现在，考虑一个优化器可能已经越过了一个局部最小值点的特定情况。在当前位置 $\\theta_t$ 的梯度指向与动量向量完全相反的方向，即 $\\nabla J(\\theta_t) = -c v_t$，其中 $c$ 为某个正常数。\n\n为了分析 NAG 的行为，假设在 $\\theta_t$ 的局部邻域内，损失函数的梯度近似线性变化。也就是说，对于一个小的位移向量 $d$，梯度可以近似为 $\\nabla J(\\theta_t + d) \\approx \\nabla J(\\theta_t) + H d$，其中海森矩阵 $H$ 是一个常数正定矩阵。在本分析中，假设此关系是精确的，并将海森矩阵简化为 $H = \\lambda I$，其中 $\\lambda$ 是一个表示局部曲率的正标量常数，而 $I$ 是单位矩阵。因此，该近似变为一个等式：$\\nabla J(\\theta_t + d) = \\nabla J(\\theta_t) + \\lambda d$。\n\n你的任务是在这些条件下，确定 Nesterov 加速梯度 (NAG) 算法单步的参数更新向量 $\\Delta \\theta = \\theta_{t+1} - \\theta_t$。用符号表达式表示你的答案，该表达式应包含 $\\eta$、$\\gamma$、$c$、$\\lambda$ 和动量向量 $v_t$。",
            "solution": "我们已知 Nesterov 加速梯度 (NAG) 的更新步骤：\n1) 使用当前动量计算前瞻位置：$\\theta_{\\text{lookahead}} = \\theta_{t} - \\gamma v_{t}$。\n2) 计算前瞻位置的梯度：$g_{\\text{lookahead}} = \\nabla J(\\theta_{\\text{lookahead}})$。\n3) 更新动量：$v_{t+1} = \\gamma v_{t} + \\eta g_{\\text{lookahead}}$。\n4) 更新参数：$\\theta_{t+1} = \\theta_{t} - v_{t+1}$。\n\n我们假设梯度的局部线性模型，其海森矩阵为 $H = \\lambda I$，即 $\\nabla J(\\theta_{t} + d) = \\nabla J(\\theta_{t}) + \\lambda d$，并且已知当前梯度与动量方向相反，$\\nabla J(\\theta_{t}) = - c v_{t}$，其中 $c > 0$。\n\n首先，计算前瞻点的梯度。从 $\\theta_{t}$ 到前瞻点的位移是 $d = \\theta_{\\text{lookahead}} - \\theta_{t} = - \\gamma v_{t}$。使用线性梯度模型，\n$$\ng_{\\text{lookahead}} = \\nabla J(\\theta_{\\text{lookahead}}) = \\nabla J(\\theta_{t}) + \\lambda d = - c v_{t} + \\lambda(- \\gamma v_{t}) = -\\left(c + \\gamma \\lambda\\right) v_{t}.\n$$\n使用 NAG 规则更新动量：\n$$\nv_{t+1} = \\gamma v_{t} + \\eta g_{\\text{lookahead}} = \\gamma v_{t} + \\eta \\left[-\\left(c + \\gamma \\lambda\\right) v_{t}\\right] = \\left[\\gamma - \\eta \\left(c + \\gamma \\lambda\\right)\\right] v_{t}.\n$$\n更新参数并构成参数更新向量 $\\Delta \\theta = \\theta_{t+1} - \\theta_{t}$：\n$$\n\\theta_{t+1} = \\theta_{t} - v_{t+1} \\quad \\Longrightarrow \\quad \\Delta \\theta = - v_{t+1} = - \\left[\\gamma - \\eta \\left(c + \\gamma \\lambda\\right)\\right] v_{t} = \\left[\\eta \\left(c + \\gamma \\lambda\\right) - \\gamma\\right] v_{t}.\n$$\n因此，在所述假设下，NAG 的单步参数更新为\n$$\n\\Delta \\theta = \\left[\\eta \\left(c + \\gamma \\lambda\\right) - \\gamma\\right] v_{t}.\n$$",
            "answer": "$$\\boxed{\\left[\\eta\\left(c+\\gamma\\lambda\\right)-\\gamma\\right]v_{t}}$$"
        }
    ]
}