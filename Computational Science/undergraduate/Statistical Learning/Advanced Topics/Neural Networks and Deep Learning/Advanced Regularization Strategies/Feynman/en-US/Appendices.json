{
    "hands_on_practices": [
        {
            "introduction": "Standard ridge regression applies an isotropic, or uniform, penalty to all coefficients. This practice generalizes that idea by introducing a custom null-space regularizer, $\\mu \\lVert Aw \\rVert_2^2$, which allows you to penalize specific linear combinations of weights based on prior knowledge. By implementing this more flexible regularization scheme , you will learn to encode structural assumptions directly into the objective function and observe the trade-off between imposing a correct versus an incorrect model constraint on out-of-sample prediction error.",
            "id": "3096585",
            "problem": "You are given a linear prediction model with parameter vector $w \\in \\mathbb{R}^d$, input matrix $X \\in \\mathbb{R}^{n \\times d}$, and responses $y \\in \\mathbb{R}^n$. Consider the quadratic objective that combines empirical risk minimization with two regularizers: an isotropic $\\ell_2$ penalty and a null-space penalty that enforces linear invariances via a matrix $A \\in \\mathbb{R}^{k \\times d}$. The objective is\n$$\nJ(w) \\;=\\; \\frac{1}{n}\\,\\lVert y - Xw \\rVert_2^2 \\;+\\; \\lambda\\,\\lVert w \\rVert_2^2 \\;+\\; \\mu\\,\\lVert Aw \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ and $\\mu \\ge 0$ are hyperparameters. The null-space penalty encourages $w$ to lie close to the null space of $A$, thereby penalizing deviations from specified linear invariances.\n\nYour tasks are:\n- Starting only from the fundamental definitions of least squares minimization and matrix calculus, derive the normal equations for the minimizer $w^\\star$ of $J(w)$, and obtain $w^\\star$ in closed form as a linear system solution. Then derive the associated hat matrix $H$ that maps the training response $y$ to its fitted values $\\hat{y} = X w^\\star$ and the effective degrees of freedom $\\mathrm{df} = \\mathrm{trace}(H)$.\n- Implement a program that solves for $w^\\star$ and computes the effective degrees of freedom and the out-of-sample mean squared error on a provided test set for several specified hyperparameter and constraint-matrix cases.\n\nUse the following fixed data for training and testing:\n- Training design matrix $X_{\\mathrm{train}} \\in \\mathbb{R}^{6 \\times 3}$,\n$$\nX_{\\mathrm{train}} \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & -1 \\\\\n2 & 1 & 0 \\\\\n1 & -1 & 1 \\\\\n3 & 0 & -2 \\\\\n0 & 2 & 1\n\\end{pmatrix}.\n$$\n- Training responses $y_{\\mathrm{train}} \\in \\mathbb{R}^{6}$,\n$$\ny_{\\mathrm{train}} \\;=\\;\n\\begin{pmatrix}\n0.1 \\\\\n2.8 \\\\\n6.2 \\\\\n-1.0 \\\\\n7.7 \\\\\n3.1\n\\end{pmatrix}.\n$$\n- Test design matrix $X_{\\mathrm{test}} \\in \\mathbb{R}^{4 \\times 3}$,\n$$\nX_{\\mathrm{test}} \\;=\\;\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 2 \\\\\n2 & -1 & 1 \\\\\n1 & 0 & -1\n\\end{pmatrix}.\n$$\n- Noise-free test responses $y_{\\mathrm{test}} \\in \\mathbb{R}^{4}$, generated by a ground-truth parameter $w_{\\mathrm{true}} \\in \\mathbb{R}^3$ with components $w_{\\mathrm{true}} = (2,\\,2,\\,-1)$:\n$$\ny_{\\mathrm{test}} \\;=\\; X_{\\mathrm{test}}\\, w_{\\mathrm{true}} \\;=\\;\n\\begin{pmatrix}\n4 \\\\\n0 \\\\\n1 \\\\\n3\n\\end{pmatrix}.\n$$\n\nFor all test cases use $\\lambda = 0.1$. Define the following test suite of constraint settings, each specified by a pair $(\\mu, A)$:\n- Case $1$: $\\mu = 0$, $A = \\begin{pmatrix} 0 & 0 & 0 \\end{pmatrix}$.\n- Case $2$: $\\mu = 50$, $A = \\begin{pmatrix} 1 & -1 & 0 \\end{pmatrix}$, which encourages the invariance $w_1 = w_2$ by penalizing deviations from the null space defined by $Aw = 0$.\n- Case $3$: $\\mu = 50$, $A = \\begin{pmatrix} 0 & 1 & 1 \\end{pmatrix}$, which encourages the invariance $w_2 = -w_3$ by penalizing deviations from the null space defined by $Aw = 0$.\n\nFor each case:\n- Let $n = 6$ and $d = 3$. Form the symmetric positive definite matrix\n$$\nM \\;=\\; \\frac{1}{n} X_{\\mathrm{train}}^\\top X_{\\mathrm{train}} \\;+\\; \\lambda I_d \\;+\\; \\mu\\, A^\\top A,\n$$\nand the right-hand side\n$$\nb \\;=\\; \\frac{1}{n} X_{\\mathrm{train}}^\\top y_{\\mathrm{train}}.\n$$\n- Compute $w^\\star$ by solving the linear system $M\\,w^\\star = b$.\n- Compute the hat matrix\n$$\nH \\;=\\; \\frac{1}{n}\\, X_{\\mathrm{train}}\\, M^{-1}\\, X_{\\mathrm{train}}^\\top,\n$$\nand the effective degrees of freedom $\\mathrm{df} = \\mathrm{trace}(H)$.\n- Compute the test mean squared error\n$$\n\\mathrm{MSE}_{\\mathrm{test}} \\;=\\; \\frac{1}{m}\\, \\lVert X_{\\mathrm{test}}\\, w^\\star - y_{\\mathrm{test}} \\rVert_2^2,\n$$\nwhere $m = 4$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should be ordered as $[\\mathrm{MSE}_1, \\mathrm{df}_1, \\mathrm{MSE}_2, \\mathrm{df}_2, \\mathrm{MSE}_3, \\mathrm{df}_3]$, where index $i$ corresponds to Case $i$ above.\n- Round all floating-point outputs to $6$ decimal places in the printed line.\n- The required final output is a single line, for example $[r_1,r_2,r_3,r_4,r_5,r_6]$ with the specified order.\n\nNote: Angles are not involved and no physical units are required. All numerical quantities should be computed as real numbers in standard floating-point arithmetic. The answer for each test case must be reported as floats as specified above.",
            "solution": "We begin from the fundamental setup of linear least squares and matrix calculus. Let $X \\in \\mathbb{R}^{n \\times d}$, $y \\in \\mathbb{R}^n$, and $w \\in \\mathbb{R}^d$. Consider the objective\n$$\nJ(w) \\;=\\; \\frac{1}{n}\\,\\lVert y - Xw \\rVert_2^2 \\;+\\; \\lambda\\,\\lVert w \\rVert_2^2 \\;+\\; \\mu\\,\\lVert Aw \\rVert_2^2,\n$$\nwith $\\lambda \\ge 0$, $\\mu \\ge 0$, and $A \\in \\mathbb{R}^{k \\times d}$. This objective equals the sum of a strictly convex quadratic (provided regularization ensures positive definiteness) and nonnegative penalties. The unique minimizer arises from setting the gradient to zero.\n\nUsing the well-tested matrix calculus identity for the gradient of a quadratic, namely $\\nabla_w \\lVert y - Xw \\rVert_2^2 = -2 X^\\top (y - Xw)$, and $\\nabla_w \\lVert w \\rVert_2^2 = 2 w$, and $\\nabla_w \\lVert A w \\rVert_2^2 = 2 A^\\top A w$, we compute the gradient of $J(w)$:\n$$\n\\nabla_w J(w) \\;=\\; -\\frac{2}{n} X^\\top (y - Xw) \\;+\\; 2 \\lambda w \\;+\\; 2 \\mu A^\\top A w.\n$$\nSetting $\\nabla_w J(w) = 0$ and dividing by $2$ yields the normal equations\n$$\n\\left(\\frac{1}{n} X^\\top X + \\lambda I_d + \\mu A^\\top A\\right) w \\;=\\; \\frac{1}{n} X^\\top y.\n$$\nDefine the symmetric positive definite matrix\n$$\nM \\;=\\; \\frac{1}{n} X^\\top X + \\lambda I_d + \\mu A^\\top A,\n$$\nand the right-hand side\n$$\nb \\;=\\; \\frac{1}{n} X^\\top y.\n$$\nThen the unique minimizer is obtained by solving the linear system\n$$\nM w^\\star \\;=\\; b.\n$$\nNo shortcut formula beyond the linear system solution is necessary; a numerically stable way is to use a linear solver rather than an explicit matrix inverse.\n\nNext, for the fitted values $\\hat{y} = X w^\\star$, substitute $w^\\star = M^{-1} b$ to get\n$$\n\\hat{y} \\;=\\; X M^{-1} \\left( \\frac{1}{n} X^\\top y \\right) \\;=\\; \\left( \\frac{1}{n} X M^{-1} X^\\top \\right) y.\n$$\nTherefore, the hat matrix is\n$$\nH \\;=\\; \\frac{1}{n} X M^{-1} X^\\top.\n$$\nThe effective degrees of freedom, a standard measure of model flexibility for linear smoothers, is given by\n$$\n\\mathrm{df} \\;=\\; \\mathrm{trace}(H).\n$$\nThe null-space penalty modifies $M$ by adding the positive semidefinite matrix $\\mu A^\\top A$. This shrinks components of $w$ in directions orthogonal to the null space of $A$, effectively reducing variance in those directions. When the ground-truth parameter $w_{\\mathrm{true}}$ lies near the null space of $A$, this penalty improves generalization by reducing variance without much additional bias. Conversely, if $w_{\\mathrm{true}}$ significantly violates the constraint $A w = 0$, the penalty introduces bias that can harm out-of-sample performance.\n\nAlgorithmic procedure to compute the requested quantities for each test case:\n- Input $X_{\\mathrm{train}} \\in \\mathbb{R}^{n \\times d}$, $y_{\\mathrm{train}} \\in \\mathbb{R}^n$, $X_{\\mathrm{test}} \\in \\mathbb{R}^{m \\times d}$, $y_{\\mathrm{test}} \\in \\mathbb{R}^{m}$, hyperparameters $\\lambda$ and $\\mu$, and constraint matrix $A \\in \\mathbb{R}^{k \\times d}$.\n- Form $S = \\frac{1}{n} X_{\\mathrm{train}}^\\top X_{\\mathrm{train}}$, $M = S + \\lambda I_d + \\mu A^\\top A$, and $b = \\frac{1}{n} X_{\\mathrm{train}}^\\top y_{\\mathrm{train}}$.\n- Solve $M w^\\star = b$ using a linear solver to obtain $w^\\star$.\n- Compute $C = M^{-1} X_{\\mathrm{train}}^\\top$ without explicit inversion by solving $M C = X_{\\mathrm{train}}^\\top$; equivalently, $C$ is the solution of a matrix right-hand side. Then $H = \\frac{1}{n} X_{\\mathrm{train}} C$ and $\\mathrm{df} = \\mathrm{trace}(H)$.\n- Compute $\\mathrm{MSE}_{\\mathrm{test}} = \\frac{1}{m} \\lVert X_{\\mathrm{test}} w^\\star - y_{\\mathrm{test}} \\rVert_2^2$.\n- Round the floating-point outputs to $6$ decimal places and print in the specified order $[\\mathrm{MSE}_1, \\mathrm{df}_1, \\mathrm{MSE}_2, \\mathrm{df}_2, \\mathrm{MSE}_3, \\mathrm{df}_3]$.\n\nFor the provided test suite:\n- Case $1$ uses $\\mu = 0$ and $A = (0, 0, 0)$, which reduces to ridge regression with $\\lambda = 0.1$. This gives a baseline $\\mathrm{df}$ less than $d = 3$ and a certain out-of-sample $\\mathrm{MSE}_{\\mathrm{test}}$.\n- Case $2$ uses $\\mu = 50$ and $A = (1, -1, 0)$, strongly encouraging $w_1 \\approx w_2$. Since the ground truth satisfies $w_1 = w_2$, this penalty should reduce estimator variance along the $w_1 - w_2$ direction and often improve $\\mathrm{MSE}_{\\mathrm{test}}$, while further reducing $\\mathrm{df}$.\n- Case $3$ uses $\\mu = 50$ and $A = (0, 1, 1)$, strongly encouraging $w_2 \\approx - w_3$, which conflicts with $w_{\\mathrm{true}}$ where $w_2 + w_3 = 1$. This introduces bias, potentially inflating $\\mathrm{MSE}_{\\mathrm{test}}$, with a similar reduction in $\\mathrm{df}$ due to the strong penalty.\n\nThe specified program implements exactly these computations in a numerically stable manner using linear system solves, ensures the results are aggregated in the requested format, and rounds to $6$ decimal places as required.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    # Fixed training data (n=6, d=3)\n    X_train = np.array([\n        [1.0, 0.0, 2.0],\n        [0.0, 1.0, -1.0],\n        [2.0, 1.0, 0.0],\n        [1.0, -1.0, 1.0],\n        [3.0, 0.0, -2.0],\n        [0.0, 2.0, 1.0]\n    ], dtype=float)\n    y_train = np.array([0.1, 2.8, 6.2, -1.0, 7.7, 3.1], dtype=float)\n\n    # Fixed test data (m=4, d=3)\n    X_test = np.array([\n        [1.0, 1.0, 0.0],\n        [0.0, 1.0, 2.0],\n        [2.0, -1.0, 1.0],\n        [1.0, 0.0, -1.0]\n    ], dtype=float)\n    # Ground-truth w_true = (2, 2, -1), so y_test is noise-free\n    y_test = np.array([4.0, 0.0, 1.0, 3.0], dtype=float)\n\n    n, d = X_train.shape\n    m = X_test.shape[0]\n    lam = 0.1  # lambda\n\n    # Test suite of cases: (mu, A)\n    test_cases = [\n        (0.0, np.array([[0.0, 0.0, 0.0]], dtype=float)),     # Case 1\n        (50.0, np.array([[1.0, -1.0, 0.0]], dtype=float)),   # Case 2\n        (50.0, np.array([[0.0, 1.0, 1.0]], dtype=float)),    # Case 3\n    ]\n\n    # Precompute S and b components\n    Xt = X_train.T\n    S = (Xt @ X_train) / n\n    b = (Xt @ y_train) / n\n    I = np.eye(d)\n\n    results = []\n\n    for mu, A in test_cases:\n        # Form M = S + lam*I + mu * A^T A\n        AT_A = A.T @ A\n        M = S + lam * I + mu * AT_A\n\n        # Solve M w = b\n        w_star = np.linalg.solve(M, b)\n\n        # Compute H = (1/n) * X * M^{-1} * X^T without explicit inverse:\n        # Solve M * C = X^T -> C = M^{-1} X^T\n        C = np.linalg.solve(M, Xt)  # shape (d, n)\n        H = (X_train @ C) / n\n        df = np.trace(H)\n\n        # Test MSE\n        y_pred_test = X_test @ w_star\n        mse_test = np.mean((y_pred_test - y_test) ** 2)\n\n        # Append rounded results in the specified order\n        results.extend([mse_test, df])\n\n    # Format results to 6 decimal places, no spaces\n    formatted = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While some regularization methods apply \"soft\" penalties to guide parameters, there are situations where domain knowledge provides exact, non-negotiable constraints that a model must satisfy. This practice moves from soft penalties to hard constraints by exploring equality-constrained ridge regression, where the goal is to minimize the regularized loss subject to a linear system of equations $C\\beta = d$. By deriving the solution using the Karush-Kuhn-Tucker (KKT) framework and implementing it numerically , you will gain experience in constrained optimization and learn to quantify the performance cost of enforcing such constraints.",
            "id": "3096634",
            "problem": "You are given a linear regression setting with ridge regularization. Let $X \\in \\mathbb{R}^{n \\times p}$ be a fixed design matrix and $y \\in \\mathbb{R}^{n}$ be a fixed response vector. For any coefficient vector $\\beta \\in \\mathbb{R}^{p}$ and regularization strength $\\lambda \\ge 0$, define the penalized training risk (ridge objective) as\n$$\nJ_{\\lambda}(\\beta) \\;=\\; \\frac{1}{n}\\,\\lVert y - X \\beta \\rVert_2^2 \\;+\\; \\lambda \\,\\lVert \\beta \\rVert_2^2.\n$$\nYou will explore equality-constrained ridge regression where some coefficients are fixed by domain knowledge. Specifically, suppose you are given a constraint matrix $C \\in \\mathbb{R}^{m \\times p}$ and a constraint vector $d \\in \\mathbb{R}^{m}$, and you must solve\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{n}\\,\\lVert y - X \\beta \\rVert_2^2 \\;+\\; \\lambda \\,\\lVert \\beta \\rVert_2^2\n\\quad \\text{subject to} \\quad C \\beta \\;=\\; d.\n$$\n\nTask A (derivation from first principles): Starting from the definition of $J_{\\lambda}(\\beta)$, the normal equations for the unconstrained ridge minimizer, and the Karush–Kuhn–Tucker (KKT) conditions for equality-constrained problems, derive the expression for the constrained ridge minimizer in terms of $X$, $y$, $\\lambda$, $C$, and $d$. Your derivation must begin with the gradient conditions of the Lagrangian and proceed via linear algebraic reasoning without assuming any closed-form result.\n\nTask B (quantifying error degradation): Define the error degradation of imposing the constraints relative to the unconstrained ridge solution as\n$$\n\\Delta_{\\lambda}(C,d) \\;=\\; J_{\\lambda}(\\beta_{\\text{constr}}) \\;-\\; J_{\\lambda}(\\beta_{\\text{unconstr}}),\n$$\nwhere $\\beta_{\\text{unconstr}}$ is the unconstrained ridge minimizer and $\\beta_{\\text{constr}}$ is the constrained ridge minimizer for the same $X$, $y$, and $\\lambda$. Compute $\\Delta_{\\lambda}(C,d)$ for each test case below.\n\nUse the following fixed data:\n$$\nn \\;=\\; 6,\\quad p \\;=\\; 3,\\quad\nX \\;=\\;\n\\begin{bmatrix}\n1 & 0 & 2\\\\\n0 & 1 & 1\\\\\n1 & 1 & 0\\\\\n2 & 1 & 1\\\\\n0 & 2 & 1\\\\\n1 & 0 & 0\n\\end{bmatrix},\\quad\ny \\;=\\;\n\\begin{bmatrix}\n3\\\\\n1\\\\\n2\\\\\n4\\\\\n2\\\\\n1\n\\end{bmatrix}.\n$$\n\nTest suite (each test case is a tuple $(\\lambda, C, d)$):\n1. General case with no constraints:\n   - $\\lambda \\;=\\; \\tfrac{1}{2}$,\n   - $C$ is the $0 \\times 3$ zero-row matrix (no constraints),\n   - $d$ is the empty vector in $\\mathbb{R}^{0}$.\n2. Constraint exactly matching the unconstrained ridge coefficient for the first coordinate at $\\lambda \\;=\\; \\tfrac{1}{2}$:\n   - $\\lambda \\;=\\; \\tfrac{1}{2}$,\n   - $C \\;=\\; \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}$,\n   - $d \\;=\\; \\begin{bmatrix} \\tfrac{47}{49} \\end{bmatrix}$.\n3. Single incorrect constraint at the same $\\lambda$:\n   - $\\lambda \\;=\\; \\tfrac{1}{2}$,\n   - $C \\;=\\; \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}$,\n   - $d \\;=\\; \\begin{bmatrix} 2 \\end{bmatrix}$.\n4. High regularization with a large incorrect constraint:\n   - $\\lambda \\;=\\; 2$,\n   - $C \\;=\\; \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}$,\n   - $d \\;=\\; \\begin{bmatrix} 3 \\end{bmatrix}$.\n5. Boundary case with no regularization and multiple constraints:\n   - $\\lambda \\;=\\; 0$,\n   - $C \\;=\\; \\begin{bmatrix} 0 & 1 & 0\\\\ 0 & 0 & 1 \\end{bmatrix}$,\n   - $d \\;=\\; \\begin{bmatrix} 0\\\\ 0 \\end{bmatrix}$.\n\nYour program must:\n- Implement a numerically stable solver for both the unconstrained ridge solution and the equality-constrained ridge solution based solely on linear algebraic operations.\n- For each test case, compute the scalar value $\\Delta_{\\lambda}(C,d)$.\n- Round each resulting scalar to $6$ decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite, for example $[a_1,a_2,a_3,a_4,a_5]$, where each $a_i$ is a decimal with exactly $6$ digits after the decimal point.\n\nNo physical units are involved. All angles, if any appear, should be interpreted in radians. Percentages, if any appear, must be expressed as decimals or fractions, not with a percentage sign. The final output format must be exactly one line as described above.",
            "solution": "The problem is assessed to be **valid**. It is a well-posed, self-contained, and scientifically grounded problem in the field of statistical learning, specifically focusing on equality-constrained optimization. All provided data and definitions are clear, consistent, and formalizable.\n\n### Task A: Derivation of the Constrained Ridge Minimizer\n\nThe optimization problem is to find the coefficient vector $\\beta \\in \\mathbb{R}^{p}$ that minimizes the ridge objective function\n$$\nJ_{\\lambda}(\\beta) \\;=\\; \\frac{1}{n}\\,\\lVert y - X \\beta \\rVert_2^2 \\;+\\; \\lambda \\,\\lVert \\beta \\rVert_2^2\n$$\nsubject to a set of linear equality constraints given by $C \\beta = d$, where $C \\in \\mathbb{R}^{m \\times p}$ and $d \\in \\mathbb{R}^{m}$.\n\nThis is a convex optimization problem, as the objective function $J_{\\lambda}(\\beta)$ is strictly convex for $\\lambda > 0$ (and convex for $\\lambda=0$) and the constraints are affine. The Karush–Kuhn–Tucker (KKT) conditions are therefore necessary and sufficient for optimality. We begin by forming the Lagrangian, introducing a vector of Lagrange multipliers $\\nu \\in \\mathbb{R}^{m}$:\n$$\n\\mathcal{L}(\\beta, \\nu) = J_{\\lambda}(\\beta) + \\nu^T (C\\beta - d)\n$$\nThe KKT conditions consist of stationarity with respect to the primal variables $\\beta$ and primal feasibility.\n\n**1. Stationarity Condition**\nThe gradient of the Lagrangian with respect to $\\beta$ must be zero at the optimal point $(\\beta_{\\text{constr}}, \\nu^*)$:\n$$\n\\nabla_{\\beta} \\mathcal{L}(\\beta_{\\text{constr}}, \\nu^*) = 0\n$$\nFirst, we compute the gradient of $J_{\\lambda}(\\beta)$. The objective function can be expanded as:\n$$\nJ_{\\lambda}(\\beta) = \\frac{1}{n}(y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) + \\lambda \\beta^T \\beta\n$$\nTaking the gradient with respect to $\\beta$ yields:\n$$\n\\nabla_{\\beta} J_{\\lambda}(\\beta) = \\frac{1}{n}(-2X^T y + 2X^T X \\beta) + 2\\lambda\\beta = \\frac{2}{n}(X^T X \\beta - X^T y) + 2\\lambda\\beta\n$$\nThe gradient of the full Lagrangian is:\n$$\n\\nabla_{\\beta} \\mathcal{L}(\\beta, \\nu) = \\nabla_{\\beta} J_{\\lambda}(\\beta) + \\nabla_{\\beta}(\\nu^T C\\beta) = \\frac{2}{n}(X^T X \\beta - X^T y) + 2\\lambda\\beta + C^T\\nu\n$$\nSetting the gradient to zero gives the stationarity equation:\n$$\n\\frac{2}{n}(X^T X \\beta - X^T y) + 2\\lambda\\beta + C^T\\nu = 0\n$$\nTo simplify, we multiply the entire equation by $\\frac{n}{2}$:\n$$\n(X^T X \\beta - X^T y) + n\\lambda\\beta + \\frac{n}{2}C^T\\nu = 0\n$$\nLet's define a rescaled vector of Lagrange multipliers $\\mu = \\frac{n}{2}\\nu$. Rearranging the terms, we get:\n$$\n(X^T X + n\\lambda I_p)\\beta + C^T\\mu = X^T y\n$$\nwhere $I_p$ is the $p \\times p$ identity matrix.\n\n**2. Primal Feasibility Condition**\nThe solution must satisfy the original equality constraints:\n$$\nC\\beta = d\n$$\n\n**3. The KKT System**\nCombining the stationarity and primal feasibility conditions, we obtain a system of $p+m$ linear equations in $p+m$ variables (the elements of $\\beta$ and $\\mu$). This system can be expressed in a block matrix form:\n$$\n\\begin{bmatrix}\nX^T X + n\\lambda I_p & C^T \\\\\nC & 0_{m \\times m}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta \\\\\n\\mu\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nX^T y \\\\\nd\n\\end{bmatrix}\n$$\nwhere $0_{m \\times m}$ is the $m \\times m$ matrix of zeros.\n\nThe solution to this linear system provides the constrained minimizer $\\beta_{\\text{constr}}$ (the first $p$ components of the solution vector) and the associated Lagrange multipliers $\\mu$. This formulation is directly translatable to a numerical algorithm. For the unconstrained case ($m=0$), the $C$ and $d$ components vanish, correctly reducing the system to the standard normal equations for ridge regression:\n$$\n(X^T X + n\\lambda I_p)\\beta_{\\text{unconstr}} = X^T y\n$$\n\n### Task B: Numerical Computation of Error Degradation\n\nTo quantify the error degradation $\\Delta_{\\lambda}(C,d) = J_{\\lambda}(\\beta_{\\text{constr}}) - J_{\\lambda}(\\beta_{\\text{unconstr}})$, we proceed as follows for each test case.\nThe fixed data are $n=6, p=3$, with\n$$\nX = \\begin{bmatrix}\n1 & 0 & 2\\\\\n0 & 1 & 1\\\\\n1 & 1 & 0\\\\\n2 & 1 & 1\\\\\n0 & 2 & 1\\\\\n1 & 0 & 0\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix} 3\\\\ 1\\\\ 2\\\\ 4\\\\ 2\\\\ 1 \\end{bmatrix}\n$$\nFrom this, we pre-compute $X^T X$ and $X^T y$:\n$$\nX^T X = \\begin{bmatrix} 7 & 3 & 4 \\\\ 3 & 7 & 4 \\\\ 4 & 4 & 7 \\end{bmatrix}, \\quad\nX^T y = \\begin{bmatrix} 14 \\\\ 11 \\\\ 13 \\end{bmatrix}\n$$\n\nFor each test case $(\\lambda, C, d)$:\n1.  **Compute $\\beta_{\\text{unconstr}}$**: Solve $(X^T X + n\\lambda I_p)\\beta = X^T y$.\n2.  **Compute $\\beta_{\\text{constr}}$**: Solve the KKT system derived above.\n3.  **Compute Objectives**: Evaluate $J_{\\lambda}(\\beta_{\\text{unconstr}})$ and $J_{\\lambda}(\\beta_{\\text{constr}})$.\n4.  **Compute Degradation**: Calculate the difference $\\Delta_{\\lambda}(C,d)$.\n\nThe numerical calculations for each test case are performed by the provided Python script.\n- **Case 1**: No constraints. $\\beta_{\\text{constr}} = \\beta_{\\text{unconstr}}$, so $\\Delta_{\\lambda}(C,d) = 0$.\n- **Case 2**: The constraint is $C\\beta = d$, where $C=\\begin{bmatrix}1 & 0 & 0\\end{bmatrix}$ and $d = \\begin{bmatrix}47/49\\end{bmatrix}$. The unconstrained solution for $\\lambda=1/2$ has $\\beta_1 = 47/49$, which already satisfies the constraint. Thus, $\\beta_{\\text{constr}} = \\beta_{\\text{unconstr}}$ and $\\Delta_{\\lambda}(C,d) = 0$.\n- **Cases 3, 4, 5**: The constraints are active and do not match the unconstrained solution. The degradation $\\Delta_{\\lambda}(C,d)$ will be positive, representing the increase in the objective function value due to the imposition of non-optimal constraints. The script calculates these values numerically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the equality-constrained ridge regression problem for a suite of test cases.\n    It derives the constrained and unconstrained solutions and computes the error degradation.\n    \"\"\"\n    #\n    # Define fixed data from the problem statement\n    #\n    n = 6\n    p = 3\n    X = np.array([\n        [1., 0., 2.],\n        [0., 1., 1.],\n        [1., 1., 0.],\n        [2., 1., 1.],\n        [0., 2., 1.],\n        [1., 0., 0.]\n    ])\n    y = np.array([3., 1., 2., 4., 2., 1.])\n\n    #\n    # Define the test suite as a list of tuples (lambda, C, d)\n    #\n    test_cases = [\n        # Case 1: General case with no constraints\n        (0.5, np.empty((0, p)), np.empty(0)),\n        \n        # Case 2: Constraint exactly matching the unconstrained ridge coefficient\n        (0.5, np.array([[1., 0., 0.]]), np.array([47./49.])),\n        \n        # Case 3: Single incorrect constraint\n        (0.5, np.array([[1., 0., 0.]]), np.array([2.])),\n        \n        # Case 4: High regularization with a large incorrect constraint\n        (2.0, np.array([[0., 0., 1.]]), np.array([3.])),\n        \n        # Case 5: Boundary case with no regularization and multiple constraints\n        (0.0, np.array([[0., 1., 0.], [0., 0., 1.]]), np.array([0., 0.]))\n    ]\n\n    #\n    # Reusable functions based on the derived theory\n    #\n    def solve_ridge(X, y, lam, C, d):\n        \"\"\"\n        Solves for the ridge regression coefficients, handling both unconstrained\n        and equality-constrained cases.\n        \"\"\"\n        n_obs, n_features = X.shape\n        m_constraints = C.shape[0]\n\n        XTX = X.T @ X\n        XTy = X.T @ y\n\n        if m_constraints == 0:\n            # Unconstrained case: solve normal equations\n            # (X'X + n*lambda*I) * beta = X'y\n            A = XTX + n_obs * lam * np.identity(n_features)\n            beta = np.linalg.solve(A, XTy)\n            return beta\n        \n        # Constrained case: solve KKT system\n        # [[X'X + n*lambda*I, C'], [C, 0]] * [beta, mu] = [X'y, d]\n        K_11 = XTX + n_obs * lam * np.identity(n_features)\n        K_12 = C.T\n        K_21 = C\n        K_22 = np.zeros((m_constraints, m_constraints))\n        \n        K = np.block([[K_11, K_12], [K_21, K_22]])\n        \n        # Ensure d is a 1D array for concatenation\n        b_vec = np.concatenate((XTy, d.flatten()))\n        \n        try:\n            solution_vec = np.linalg.solve(K, b_vec)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse for singular KKT matrix (e.g., if lambda=0 and X'X is singular)\n            solution_vec = np.linalg.pinv(K) @ b_vec\n\n        beta = solution_vec[:n_features]\n        return beta\n\n    def compute_objective(X, y, beta, lam):\n        \"\"\"\n        Computes the value of the ridge objective function J_lambda(beta).\n        \"\"\"\n        n_obs = X.shape[0]\n        residual = y - X @ beta\n        risk = (1/n_obs) * np.dot(residual, residual)\n        regularization = lam * np.dot(beta, beta)\n        return risk + regularization\n\n    results = []\n    \n    #\n    # Process each test case\n    #\n    for lam, C, d in test_cases:\n        # Unconstrained solution\n        beta_unconstr = solve_ridge(X, y, lam, np.empty((0, p)), np.empty(0))\n        j_unconstr = compute_objective(X, y, beta_unconstr, lam)\n\n        # Constrained solution\n        beta_constr = solve_ridge(X, y, lam, C, d)\n        j_constr = compute_objective(X, y, beta_constr, lam)\n        \n        # Calculate error degradation Delta\n        delta = j_constr - j_unconstr\n        results.append(delta)\n\n    #\n    # Format and print the final output in the required single-line format\n    #\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In many real-world systems, relationships are not static but evolve over time, requiring models whose parameters can adapt. This exercise tackles this challenge by applying Laplacian regularization to a time-varying linear model, introducing a penalty term $\\lambda \\sum_{t} \\lVert w_t - w_{t-1} \\rVert_2^2$ that encourages coefficients to change smoothly over time. By implementing a solver for the resulting large-scale, structured linear system , you will learn how to use regularization to model dynamic processes and detect abrupt \"regime shifts\" when the underlying system parameters change significantly.",
            "id": "3096605",
            "problem": "Consider a time-varying linear regression model indexed by discrete times $t \\in \\{1,\\dots,T\\}$. At each time $t$, we observe a design matrix $X_t \\in \\mathbb{R}^{n_t \\times p}$ and a response vector $y_t \\in \\mathbb{R}^{n_t}$. Assume the data are generated by a linear model with additive Gaussian noise: $y_t = X_t w_t^\\star + \\varepsilon_t$, where $w_t^\\star \\in \\mathbb{R}^p$ is the true coefficient vector at time $t$ and $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2 I_{n_t})$ are independent across times and samples. We wish to estimate a sequence of coefficients $\\{w_t\\}_{t=1}^T$ by solving a least-squares problem regularized to encourage temporal smoothness in the coefficients via a Laplacian smoothing penalty. Specifically, define the empirical risk\n$$\n\\mathcal{R}(w_1,\\dots,w_T) = \\sum_{t=1}^T \\lVert y_t - X_t w_t \\rVert_2^2 + \\lambda \\sum_{t=2}^T \\lVert w_t - w_{t-1} \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter. The penalty couples adjacent times and encourages small discrete temporal gradients, while allowing changes when the data demand them. A regime shift is defined to occur at time index $t \\in \\{2,\\dots,T\\}$ when the Euclidean norm of the discrete coefficient difference satisfies $\\lVert \\widehat{w}_t - \\widehat{w}_{t-1} \\rVert_2 \\ge \\tau$ for a user-chosen threshold $\\tau > 0$, where $\\widehat{w}_t$ are the estimated coefficients.\n\nYour task is to implement a program that, for each test case described below, generates synthetic data under the Gaussian model, computes the regularized estimator by solving the optimization problem above using first-order optimality conditions derived from the Gaussian likelihood and the given penalty, and then detects regime shifts by thresholding the norms of the estimated discrete temporal gradients.\n\nStart from the following fundamental base:\n- The Gaussian log-likelihood under independent, identically distributed noise is proportional to the negative of the sum of squared residuals.\n- The regularized estimator is obtained by minimizing the sum of squared residuals plus a smoothness penalty.\n- The first-order optimality condition for minimizing a differentiable convex function requires the gradient to be zero at the minimizer.\n- The gradient of a sum is the sum of gradients, and the gradient of a squared Euclidean norm is linear in its argument.\n\nBy applying these principles, derive the linear system characterizing the minimizer and use it algorithmically to obtain the estimates $\\widehat{w}_1,\\dots,\\widehat{w}_T$. Then compute the discrete differences $d_t = \\lVert \\widehat{w}_t - \\widehat{w}_{t-1} \\rVert_2$ for $t \\in \\{2,\\dots,T\\}$, and declare a regime shift at time $t$ if and only if $d_t \\ge \\tau$. The final output for each test case is the list of detected regime shift indices (integers in $\\{2,\\dots,T\\}$), sorted in increasing order.\n\nData-generation protocol for each test case:\n- Fix a pseudorandom seed $s$.\n- For each $t \\in \\{1,\\dots,T\\}$, draw $X_t$ with independent standard normal entries, i.e., each entry is distributed as $\\mathcal{N}(0,1)$.\n- Construct the true coefficient sequence $\\{w_t^\\star\\}$ to be piecewise-constant according to the specified change times and regime coefficient vectors.\n- Draw noise $\\varepsilon_t$ with independent entries distributed as $\\mathcal{N}(0,\\sigma^2)$, and set $y_t = X_t w_t^\\star + \\varepsilon_t$.\n- Solve for $\\{\\widehat{w}_t\\}$ by minimizing $\\mathcal{R}$.\n- Compute and threshold $d_t$ to detect regime shifts.\n\nYour program must implement the above procedure for the following test suite. Each case is independent and must use its own seed. For each case, $n_t = n$ for all $t$.\n\n- Case A (happy path with clear regime shifts):\n  - $s = 31415$, $T = 12$, $p = 3$, $n = 60$, $\\sigma = 0.1$, $\\lambda = 8.0$, $\\tau = 0.9$.\n  - Change times: $[5, 9]$ meaning changes at $t = 5$ and $t = 9$.\n  - Regime coefficient vectors in order: $w^{(1)} = [1.0, 0.0, 0.0]^\\top$, $w^{(2)} = [1.0, 1.5, 0.0]^\\top$, $w^{(3)} = [0.0, 1.5, -1.0]^\\top$.\n\n- Case B (oversmoothing edge case with a small shift):\n  - $s = 27182$, $T = 12$, $p = 3$, $n = 60$, $\\sigma = 0.2$, $\\lambda = 50.0$, $\\tau = 0.4$.\n  - Change times: $[7]$ meaning a change at $t = 7$ only.\n  - Regime coefficient vectors in order: $w^{(1)} = [0.0, 0.0, 0.0]^\\top$, $w^{(2)} = [0.0, 0.25, 0.0]^\\top$.\n\n- Case C (no shift baseline):\n  - $s = 16180$, $T = 10$, $p = 4$, $n = 60$, $\\sigma = 0.1$, $\\lambda = 5.0$, $\\tau = 0.5$.\n  - Change times: $[]$ meaning no changes.\n  - Regime coefficient vectors in order: $w^{(1)} = [0.5, -0.5, 0.0, 0.0]^\\top$.\n\n- Case D (boundary-adjacent shifts):\n  - $s = 14142$, $T = 8$, $p = 2$, $n = 60$, $\\sigma = 0.1$, $\\lambda = 8.0$, $\\tau = 1.0$.\n  - Change times: $[2, 7]$ meaning changes at $t = 2$ and $t = 7$.\n  - Regime coefficient vectors in order: $w^{(1)} = [-1.0, 0.5]^\\top$, $w^{(2)} = [1.0, 0.5]^\\top$, $w^{(3)} = [1.0, -1.5]^\\top$.\n\nInterpretation of change times: Let the change times be $[c_1, c_2, \\dots, c_K]$ with $K \\ge 0$. Then $w_t^\\star = w^{(1)}$ for $t \\in \\{1,\\dots,c_1 - 1\\}$, $w_t^\\star = w^{(2)}$ for $t \\in \\{c_1,\\dots,c_2 - 1\\}$, and so on, with $w_t^\\star = w^{(K+1)}$ for $t \\in \\{c_K,\\dots,T\\}$. If $K = 0$, then $w_t^\\star = w^{(1)}$ for all $t$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, with no spaces, enclosed in square brackets, where the $k$-th inner list is the ascending list of detected change indices for the $k$-th test case. For example, if the four cases produce detected change lists $L_1, L_2, L_3, L_4$, the output must be exactly $[L_1,L_2,L_3,L_4]$ with each $L_j$ printed as $[i_1,i_2,\\dots]$ and with no spaces anywhere in the line.",
            "solution": "The problem is valid. It is a well-posed problem in statistical learning, grounded in the principles of regularized linear regression and convex optimization. All parameters and procedures are specified, allowing for a unique and verifiable solution.\n\nThe task is to estimate a sequence of coefficient vectors $\\{w_t\\}_{t=1}^T$ in a time-varying linear model by minimizing a regularized objective function, and then to identify regime shifts based on the estimated coefficients.\n\nThe objective function to minimize is the regularized empirical risk:\n$$\n\\mathcal{R}(w_1,\\dots,w_T) = \\sum_{t=1}^T \\lVert y_t - X_t w_t \\rVert_2^2 + \\lambda \\sum_{t=2}^T \\lVert w_t - w_{t-1} \\rVert_2^2\n$$\nThe parameters $\\{w_1, \\dots, w_T\\}$, each in $\\mathbb{R}^p$, are the variables of the optimization problem. The function $\\mathcal{R}$ is a sum of squared norms, making it a quadratic function of the concatenated vector of all coefficients. As it is a sum of convex functions, $\\mathcal{R}$ is convex. Under the given data generation protocol where $n_t > p$ and the entries of $X_t$ are from a continuous distribution, the matrices $X_t^\\top X_t$ are positive definite with probability $1$. This guarantees that $\\mathcal{R}$ is strictly convex, and thus a unique minimizer $\\{\\widehat{w}_1, \\dots, \\widehat{w}_T\\}$ exists.\n\nThe minimizer is found by setting the gradient of $\\mathcal{R}$ with respect to each $w_k$ to zero. This is the first-order optimality condition. Let's compute the partial derivative of $\\mathcal{R}$ with respect to a single coefficient vector $w_k$ for $k \\in \\{1,\\dots,T\\}$.\n\nThe gradient of the least-squares data-fitting term with respect to $w_k$ is:\n$$\n\\frac{\\partial}{\\partial w_k} \\left( \\sum_{t=1}^T \\lVert y_t - X_t w_t \\rVert_2^2 \\right) = \\frac{\\partial}{\\partial w_k} \\lVert y_k - X_k w_k \\rVert_2^2 = 2(X_k^\\top X_k w_k - X_k^\\top y_k)\n$$\n\nThe gradient of the temporal smoothing penalty depends on whether $w_k$ is at a boundary ($k=1$ or $k=T$) or in the interior ($k \\in \\{2, \\dots, T-1\\}$).\n\nFor an interior time point, $k \\in \\{2, \\dots, T-1\\}$, $w_k$ appears in two penalty terms:\n$$\n\\frac{\\partial}{\\partial w_k} \\left( \\lambda \\lVert w_k - w_{k-1} \\rVert_2^2 + \\lambda \\lVert w_{k+1} - w_k \\rVert_2^2 \\right) = 2\\lambda(w_k - w_{k-1}) - 2\\lambda(w_{k+1} - w_k) = 2\\lambda(2w_k - w_{k-1} - w_{k+1})\n$$\n\nFor the first time point, $k=1$:\n$$\n\\frac{\\partial}{\\partial w_1} \\left( \\lambda \\lVert w_2 - w_1 \\rVert_2^2 \\right) = -2\\lambda(w_2 - w_1) = 2\\lambda(w_1 - w_2)\n$$\n\nFor the last time point, $k=T$:\n$$\n\\frac{\\partial}{\\partial w_T} \\left( \\lambda \\lVert w_T - w_{T-1} \\rVert_2^2 \\right) = 2\\lambda(w_T - w_{T-1})\n$$\n\nSetting the total partial derivative $\\frac{\\partial \\mathcal{R}}{\\partial w_k}$ to the zero vector and dividing by $2$ gives a system of linear equations:\n\nFor $k=1$:\n$$(X_1^\\top X_1) w_1 - X_1^\\top y_1 + \\lambda (w_1 - w_2) = 0 \\implies (X_1^\\top X_1 + \\lambda I_p) w_1 - \\lambda I_p w_2 = X_1^\\top y_1$$\n\nFor $k \\in \\{2, \\dots, T-1\\}$:\n$$(X_k^\\top X_k) w_k - X_k^\\top y_k + \\lambda (2w_k - w_{k-1} - w_{k+1}) = 0 \\implies -\\lambda I_p w_{k-1} + (X_k^\\top X_k + 2\\lambda I_p) w_k - \\lambda I_p w_{k+1} = X_k^\\top y_k$$\n\nFor $k=T$:\n$$(X_T^\\top X_T) w_T - X_T^\\top y_T + \\lambda (w_T - w_{T-1}) = 0 \\implies -\\lambda I_p w_{T-1} + (X_T^\\top X_T + \\lambda I_p) w_T = X_T^\\top y_T$$\n\nThis set of $T$ coupled vector equations can be formulated as a single large linear system $\\mathcal{H} W = \\mathcal{B}$, where $W = [w_1^\\top, \\dots, w_T^\\top]^\\top \\in \\mathbb{R}^{Tp}$ is the concatenation of all coefficient vectors. The right-hand side is $\\mathcal{B} = [(X_1^\\top y_1)^\\top, \\dots, (X_T^\\top y_T)^\\top]^\\top \\in \\mathbb{R}^{Tp}$. The system matrix $\\mathcal{H} \\in \\mathbb{R}^{Tp \\times Tp}$ is a symmetric block-tridiagonal matrix:\n$$\n\\mathcal{H} = \\begin{pmatrix}\nA_1 & C & & \\\\\nC & A_2 & C & \\\\\n& \\ddots & \\ddots & \\ddots \\\\\n& & C & A_{T-1} & C \\\\\n& & & C & A_T\n\\end{pmatrix}\n$$\nwhere the blocks are $p \\times p$ matrices defined as:\n- Diagonal blocks:\n  - $A_1 = X_1^\\top X_1 + \\lambda I_p$\n  - $A_k = X_k^\\top X_k + 2\\lambda I_p$ for $k \\in \\{2, \\dots, T-1\\}$\n  - $A_T = X_T^\\top X_T + \\lambda I_p$\n- Off-diagonal blocks:\n  - $C = -\\lambda I_p$\n\nThe solution algorithm is as follows:\n1.  For each test case, generate the data $(X_t, y_t)$ for $t=1, \\dots, T$ according to the specified protocol. This involves using the provided pseudorandom seed, dimensions, noise level, and constructing the true piecewise-constant coefficient sequence $w_t^\\star$.\n2.  Construct the matrix $\\mathcal{H}$ and the vector $\\mathcal{B}$ as derived above.\n3.  Solve the linear system $\\mathcal{H} W = \\mathcal{B}$ to obtain the flattened vector of estimated coefficients $W = \\widehat{W}$.\n4.  Reshape $\\widehat{W}$ into the sequence of estimated coefficient vectors $\\{\\widehat{w}_t\\}_{t=1}^T$.\n5.  For each time $t \\in \\{2, \\dots, T\\}$, compute the discrete temporal difference norm $d_t = \\lVert \\widehat{w}_t - \\widehat{w}_{t-1} \\rVert_2$.\n6.  A regime shift is detected at time $t$ if $d_t \\ge \\tau$. The final output for the test case is the sorted list of all such $t$.\nThis procedure is implemented for each of the provided test cases.",
            "answer": "```python\nimport numpy as np\n\ndef process_case(s, T, p, n, sigma, lambda_reg, tau, change_times, regime_vectors):\n    \"\"\"\n    Generates synthetic data, solves the regularized regression problem,\n    and detects regime shifts for a single test case.\n    \"\"\"\n    # 1. Setup random number generator for reproducibility\n    rng = np.random.default_rng(s)\n\n    # 2. Generate the true piecewise-constant coefficient sequence w_star\n    w_star_T = np.zeros((T, p))\n    # Use searchsorted to find which regime each time t belongs to.\n    # The problem uses 1-based indexing for time, so we check against np.arange(1, T + 1).\n    regime_indices = np.searchsorted(change_times, np.arange(1, T + 1), side='right')\n    for t_idx in range(T):\n        regime_idx = regime_indices[t_idx]\n        w_star_T[t_idx, :] = regime_vectors[regime_idx]\n\n    # 3. Generate synthetic data (X_t, y_t) for t=1...T\n    X_T = []\n    y_T = []\n    for t_idx in range(T):\n        X_t = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n        epsilon_t = rng.normal(loc=0.0, scale=sigma, size=n)\n        y_t = X_t @ w_star_T[t_idx, :] + epsilon_t\n        X_T.append(X_t)\n        y_T.append(y_t)\n\n    # 4. Assemble the block-tridiagonal system matrix H and the vector B\n    total_dim = T * p\n    H = np.zeros((total_dim, total_dim))\n    B = np.zeros(total_dim)\n    I_p = np.eye(p)\n\n    # Populate diagonal blocks of H and the vector B\n    for t_idx in range(T):\n        t = t_idx + 1 # 1-based time index\n        start, end = t_idx * p, (t_idx + 1) * p\n        \n        # RHS vector B\n        B[start:end] = X_T[t_idx].T @ y_T[t_idx]\n        \n        # Diagonal blocks of H\n        XtX = X_T[t_idx].T @ X_T[t_idx]\n        if t == 1 or t == T:\n            H[start:end, start:end] = XtX + lambda_reg * I_p\n        else: # 2 <= t <= T-1\n            H[start:end, start:end] = XtX + 2 * lambda_reg * I_p\n\n    # Populate off-diagonal blocks of H\n    off_diag_block = -lambda_reg * I_p\n    for t_idx in range(T - 1):\n        start1, end1 = t_idx * p, (t_idx + 1) * p\n        start2, end2 = (t_idx + 1) * p, (t_idx + 2) * p\n        H[start1:end1, start2:end2] = off_diag_block\n        H[start2:end2, start1:end1] = off_diag_block\n        \n    # 5. Solve the linear system H * W = B\n    W_hat_flat = np.linalg.solve(H, B)\n    W_hat_T = W_hat_flat.reshape((T, p))\n    \n    # 6. Detect regime shifts by thresholding norms of differences\n    shifts = []\n    for t_idx in range(1, T): # Corresponds to t=2...T\n        w_curr = W_hat_T[t_idx, :]\n        w_prev = W_hat_T[t_idx - 1, :]\n        diff_norm = np.linalg.norm(w_curr - w_prev)\n        if diff_norm >= tau:\n            # Append the 1-based time index\n            shifts.append(t_idx + 1)\n            \n    return shifts\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'s': 31415, 'T': 12, 'p': 3, 'n': 60, 'sigma': 0.1, 'lambda_reg': 8.0, 'tau': 0.9,\n         'change_times': [5, 9], \n         'regime_vectors': [np.array([1.0, 0.0, 0.0]), np.array([1.0, 1.5, 0.0]), np.array([0.0, 1.5, -1.0])]},\n        # Case B\n        {'s': 27182, 'T': 12, 'p': 3, 'n': 60, 'sigma': 0.2, 'lambda_reg': 50.0, 'tau': 0.4,\n         'change_times': [7], \n         'regime_vectors': [np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.25, 0.0])]},\n        # Case C\n        {'s': 16180, 'T': 10, 'p': 4, 'n': 60, 'sigma': 0.1, 'lambda_reg': 5.0, 'tau': 0.5,\n         'change_times': [], \n         'regime_vectors': [np.array([0.5, -0.5, 0.0, 0.0])]},\n        # Case D\n        {'s': 14142, 'T': 8, 'p': 2, 'n': 60, 'sigma': 0.1, 'lambda_reg': 8.0, 'tau': 1.0,\n         'change_times': [2, 7], \n         'regime_vectors': [np.array([-1.0, 0.5]), np.array([1.0, 0.5]), np.array([1.0, -1.5])]},\n    ]\n    \n    results = []\n    for case in test_cases:\n        detected_shifts = process_case(**case)\n        results.append(detected_shifts)\n    \n    # Format the output string precisely as required: [[...],[...],...] with no spaces.\n    inner_parts = []\n    for res_list in results:\n        # For each list, create the string representation like '[5,9]'\n        inner_str = '[' + ','.join(map(str, res_list)) + ']'\n        inner_parts.append(inner_str)\n    \n    final_output = '[' + ','.join(inner_parts) + ']'\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}