{
    "hands_on_practices": [
        {
            "introduction": "To truly understand weight decay, we must first analyze its fundamental effect on the learning process. This practice invites you to derive the equilibrium point for a weight vector being trained with gradient descent and weight decay. By simplifying the scenario to assume a constant gradient from the loss function, we can isolate and precisely quantify how the decay term counteracts the update, revealing the stable point where these two opposing forces find their balance .",
            "id": "3169493",
            "problem": "You are training a single layer of a neural network with parameter vector $w \\in \\mathbb{R}^{d}$ using gradient descent (GD) with weight decay on the penalized objective $J(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}$, where $\\lambda > 0$ is the weight decay coefficient. Assume that across iterations the gradient of the unregularized loss is constant and equal to a fixed vector $g \\in \\mathbb{R}^{d}$, and that the learning rate is a constant $\\eta > 0$. Starting only from the definition of the gradient descent update applied to $J(w)$, and from the fact that the gradient of the squared Euclidean norm is the identity scaled by its coefficient, derive the fixed point $w^{\\star}$ (the value that the iterates converge to if they converge) under the stated assumptions. You may assume $\\eta$ and $\\lambda$ are such that the iteration converges for any initialization. Express your final answer as a closed-form analytic expression in terms of $g$ and $\\lambda$ only. Do not introduce any numerical values.",
            "solution": "The user has requested the derivation of the fixed point for a gradient descent update with weight decay under specific assumptions. The problem is first validated for correctness and solvability.\n\n### Step 1: Extract Givens\n- The parameter vector is $w \\in \\mathbb{R}^{d}$.\n- The optimization algorithm is gradient descent (GD).\n- The penalized objective function is $J(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}$.\n- The weight decay coefficient is $\\lambda > 0$.\n- The gradient of the unregularized loss is a constant vector: $\\nabla L(w) = g \\in \\mathbb{R}^{d}$.\n- The learning rate is a constant $\\eta > 0$.\n- The gradient of the regularization term is given by the statement \"the gradient of the squared Euclidean norm is the identity scaled by its coefficient\", which implies $\\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right) = \\lambda w$.\n- It is assumed that the GD iteration converges to a fixed point, $w^{\\star}$, for any initialization.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard theoretical analysis of L2 regularization (weight decay) in machine learning. The assumption that the gradient of the loss function, $\\nabla L(w)$, is a constant vector $g$ is a simplification used to make the analysis tractable. This is a common modeling choice in optimization theory, representing the behavior of the algorithm on a simple quadratic objective or in a local region where the gradient is approximately constant. All provided terms ($\\lambda$, $\\eta$) are well-defined. The objective is to find the fixed point of the iterative process, which is a mathematically well-posed problem. The problem is self-contained, scientifically sound within its theoretical context, and free of contradictions or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of the Fixed Point\nThe process of gradient descent updates the parameter vector $w$ at each iteration $t$ according to the rule:\n$$\nw^{(t+1)} = w^{(t)} - \\eta \\nabla J(w^{(t)})\n$$\nwhere $w^{(t)}$ is the parameter vector at iteration $t$, and $\\eta$ is the learning rate.\n\nThe objective function to be minimized is given as:\n$$\nJ(w) = L(w) + \\frac{\\lambda}{2}\\|w\\|^{2}\n$$\nTo apply the update rule, we first need to compute the gradient of $J(w)$ with respect to $w$. Using the linearity of the gradient operator, we have:\n$$\n\\nabla J(w) = \\nabla L(w) + \\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right)\n$$\nThe problem statement provides two key pieces of information for this gradient calculation:\n1. The gradient of the unregularized loss is a constant vector $g$:\n$$\n\\nabla L(w) = g\n$$\n2. The gradient of the regularization term, $\\frac{\\lambda}{2}\\|w\\|^{2}$, is $\\lambda w$. This is consistent with standard vector calculus, as the gradient of the squared Euclidean norm $\\|w\\|^{2} = w^T w$ is $2w$, so $\\nabla \\left(\\frac{\\lambda}{2}\\|w\\|^{2}\\right) = \\frac{\\lambda}{2}(2w) = \\lambda w$.\n\nSubstituting these into the expression for $\\nabla J(w)$, we get:\n$$\n\\nabla J(w) = g + \\lambda w\n$$\nNote that the gradient of the total objective function is not constant, as it depends on the current parameter vector $w$.\n\nNow, we can write the specific gradient descent update rule for this problem by substituting this gradient into the general update equation:\n$$\nw^{(t+1)} = w^{(t)} - \\eta (g + \\lambda w^{(t)})\n$$\nA fixed point of this iteration, denoted as $w^{\\star}$, is a value for which the iteration no longer changes. That is, if $w^{(t)} = w^{\\star}$, then $w^{(t+1)}$ will also be equal to $w^{\\star}$. We find this point by setting $w^{(t+1)} = w^{(t)} = w^{\\star}$ in the update equation:\n$$\nw^{\\star} = w^{\\star} - \\eta (g + \\lambda w^{\\star})\n$$\nWe can now solve this equation for $w^{\\star}$. Subtracting $w^{\\star}$ from both sides gives:\n$$\n0 = - \\eta (g + \\lambda w^{\\star})\n$$\nSince the learning rate $\\eta$ is given as a positive constant ($\\eta > 0$), we can divide both sides by $-\\eta$ without affecting the equality:\n$$\n0 = g + \\lambda w^{\\star}\n$$\nThis equation reveals that the fixed point is the point at which the gradient of the objective function is the zero vector, which is the definition of a critical point of $J(w)$.\n\nTo isolate $w^{\\star}$, we rearrange the equation:\n$$\n\\lambda w^{\\star} = -g\n$$\nFinally, since the weight decay coefficient $\\lambda$ is also given as a positive constant ($\\lambda > 0$), we can divide by $\\lambda$:\n$$\nw^{\\star} = -\\frac{1}{\\lambda} g\n$$\nThis is the closed-form expression for the fixed point of the gradient descent iteration under the stated assumptions. The result depends only on the constant gradient of the loss, $g$, and the weight decay coefficient, $\\lambda$, as required. The learning rate $\\eta$ affects the rate of convergence to this fixed point but not its value. The assumption that the iteration converges ensures that $w^{\\star}$ is the limit of the sequence $\\{w^{(t)}\\}$.",
            "answer": "$$\n\\boxed{-\\frac{g}{\\lambda}}\n$$"
        },
        {
            "introduction": "Building on our understanding of the basic mechanism, we now explore a critical aspect for any practitioner: the interaction between weight decay and data preprocessing. This exercise examines how scaling input features impacts the regularization effect. You will discover that to maintain an equivalent regularization objective, the weight decay coefficient $\\lambda$ must be adjusted in response to feature scaling, a result that underscores the importance of data normalization as a standard practice in machine learning .",
            "id": "3169486",
            "problem": "Consider a single-layer linear neuron (a one-layer neural network) with parameter vector $w \\in \\mathbb{R}^{d}$ that maps inputs $x \\in \\mathbb{R}^{d}$ to predictions $f_{w}(x) = w^{\\top} x$. The network is trained on a dataset $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ by minimizing the empirical risk with Mean Squared Error (MSE) and $\\ell_{2}$ weight decay. Formally, the objective is\n$$\nJ(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times d}$ is the design matrix with rows $x_{i}^{\\top}$, $y \\in \\mathbb{R}^{n}$ is the target vector, $\\lambda > 0$ is the weight decay coefficient, and $\\|\\cdot\\|$ denotes the Euclidean norm. Suppose every input is rescaled by a positive scalar $\\alpha > 0$, so that $x_{i} \\mapsto x_{i}' = \\alpha x_{i}$ for all $i$, and the rescaled design matrix becomes $X' = \\alpha X$. To maintain identical predictions for the original inputs $x$, we simultaneously reparameterize the weights as $w' = \\frac{1}{\\alpha} w$, so that $f_{w'}(x') = f_{w}(x)$ for all $x$.\n\nDefine the rescaled objective\n$$\nJ(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2} + \\frac{\\lambda'}{2} \\|w'\\|^{2},\n$$\nwhere $\\lambda' > 0$ is the new weight decay coefficient applied after the input rescaling. Starting from the fundamental definitions of empirical risk minimization with MSE and $\\ell_{2}$ regularization, determine the expression for $\\lambda'$ in terms of $\\alpha$ and $\\lambda$ that makes the total objective value invariant under the joint transformation $(x_{i}, w, \\lambda) \\mapsto (x_{i}', w', \\lambda')$, in the sense that\n$$\nJ(w; X, y, \\lambda) = J\\!\\left(\\frac{1}{\\alpha} w; \\alpha X, y, \\lambda'\\right)\n$$\nfor all $w$, $X$, and $y$.\n\nYour final answer must be a single closed-form analytic expression in terms of $\\alpha$ and $\\lambda$. No rounding is required.",
            "solution": "The problem will first be validated according to the established criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Model: A single-layer linear neuron with parameters $w \\in \\mathbb{R}^{d}$ and inputs $x \\in \\mathbb{R}^{d}$.\n-   Prediction function: $f_{w}(x) = w^{\\top} x$.\n-   Dataset: $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$, with design matrix $X \\in \\mathbb{R}^{n \\times d}$ and target vector $y \\in \\mathbb{R}^{n}$.\n-   Original objective function: $J(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2}$, where $\\lambda > 0$ and $\\|\\cdot\\|$ is the Euclidean norm.\n-   Input rescaling: $x_{i} \\mapsto x_{i}' = \\alpha x_{i}$ for a positive scalar $\\alpha > 0$. The rescaled design matrix is $X' = \\alpha X$.\n-   Weight reparameterization: $w \\mapsto w' = \\frac{1}{\\alpha} w$. The problem states this ensures $f_{w'}(x') = f_{w}(x)$.\n-   Rescaled objective function: $J(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2} + \\frac{\\lambda'}{2} \\|w'\\|^{2}$, where $\\lambda' > 0$.\n-   Invariance condition: Find the expression for $\\lambda'$ such that $J(w; X, y, \\lambda) = J\\!\\left(\\frac{1}{\\alpha} w; \\alpha X, y, \\lambda'\\right)$ holds for all $w$, $X$, and $y$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a standard exercise in statistical learning, specifically analyzing the properties of ridge regression (linear regression with $\\ell_{2}$ regularization). All concepts are fundamental to machine learning theory.\n-   **Well-Posed:** The problem provides a clear objective and a specific invariance condition. It requests a uniquely determined relationship between the parameters, which can be found through direct algebraic manipulation.\n-   **Objective:** The problem is stated using formal mathematical language and definitions, with no subjective or ambiguous terms.\n-   **Completeness and Consistency:** All necessary variables, functions, and transformations are explicitly defined. There are no contradictions in the setup. The condition $f_{w'}(x') = f_{w}(x)$ is consistent with the definitions of $x'$ and $w'$, as ${w'}^{\\top} x' = (\\frac{1}{\\alpha}w)^{\\top}(\\alpha x) = \\frac{1}{\\alpha} w^{\\top} \\alpha x = w^{\\top} x$.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, objective, and internally consistent. The solution process may proceed.\n\n### Solution\n\nThe goal is to determine the expression for $\\lambda'$ in terms of $\\alpha$ and $\\lambda$ that ensures the objective function's value remains invariant under the specified transformations. The invariance condition is:\n$$\nJ(w; X, y, \\lambda) = J\\!\\left(w'; X', y, \\lambda'\\right)\n$$\nwhere $w' = \\frac{1}{\\alpha} w$ and $X'$ is the matrix whose rows are $x_{i}'^{\\top} = (\\alpha x_{i})^{\\top}$.\n\nLet's analyze the rescaled objective function, $J(w'; X', y, \\lambda')$, by substituting the expressions for $x_{i}'$ and $w'$.\nThe rescaled objective is composed of two parts: the Mean Squared Error (MSE) term and the regularization term.\n\nFirst, consider the MSE term:\n$$\n\\text{MSE}' = \\frac{1}{n} \\sum_{i=1}^{n} \\left({w'}^{\\top} x_{i}' - y_{i}\\right)^{2}\n$$\nSubstitute $w' = \\frac{1}{\\alpha} w$ and $x_{i}' = \\alpha x_{i}$:\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} w\\right)^{\\top} (\\alpha x_{i})\n$$\nUsing the property of the transpose $(c a)^{\\top} = c a^{\\top}$ for a scalar $c$ and a vector $a$:\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} w^{\\top}\\right) (\\alpha x_{i})\n$$\nSince scalar multiplication is commutative and associative:\n$$\n{w'}^{\\top} x_{i}' = \\left(\\frac{1}{\\alpha} \\cdot \\alpha\\right) (w^{\\top} x_{i}) = 1 \\cdot (w^{\\top} x_{i}) = w^{\\top} x_{i}\n$$\nThis confirms that the prediction for a given data point remains unchanged after the transformation, as stated in the problem.\nTherefore, the MSE term in the rescaled objective is identical to the original MSE term:\n$$\n\\text{MSE}' = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} = \\text{MSE}\n$$\n\nNext, consider the regularization term in the rescaled objective:\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\|w'\\|^{2}\n$$\nSubstitute $w' = \\frac{1}{\\alpha} w$:\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\left\\|\\frac{1}{\\alpha} w\\right\\|^{2}\n$$\nUsing the property of the Euclidean norm $\\|c v\\| = |c| \\|v\\|$ for a scalar $c$ and vector $v$:\n$$\n\\left\\|\\frac{1}{\\alpha} w\\right\\|^{2} = \\left(\\left|\\frac{1}{\\alpha}\\right| \\|w\\|\\right)^{2} = \\left(\\frac{1}{\\alpha}\\right)^{2} \\|w\\|^{2} = \\frac{1}{\\alpha^2} \\|w\\|^{2}\n$$\nThe problem states $\\alpha > 0$, so $|\\frac{1}{\\alpha}| = \\frac{1}{\\alpha}$.\nSubstituting this back into the expression for $\\text{Reg}'$:\n$$\n\\text{Reg}' = \\frac{\\lambda'}{2} \\left(\\frac{1}{\\alpha^2} \\|w\\|^{2}\\right) = \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\nNow, we can write the full rescaled objective function in terms of the original variables:\n$$\nJ(w'; X', y, \\lambda') = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\nThe invariance condition requires this to be equal to the original objective function for all $w$, $X$, and $y$:\n$$\nJ(w; X, y, \\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2}\n$$\nSetting the two expressions for the total objective equal:\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda}{2} \\|w\\|^{2} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(w^{\\top} x_{i} - y_{i}\\right)^{2} + \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\nThe MSE terms are identical and thus cancel out, leaving the equality for the regularization terms:\n$$\n\\frac{\\lambda}{2} \\|w\\|^{2} = \\frac{\\lambda'}{2\\alpha^2} \\|w\\|^{2}\n$$\nThis equality must hold for any parameter vector $w \\in \\mathbb{R}^d$. For any non-trivial case where $w$ is not the zero vector (i.e., $\\|w\\|^2 \\neq 0$), we can divide both sides by $\\frac{1}{2}\\|w\\|^2$:\n$$\n\\lambda = \\frac{\\lambda'}{\\alpha^2}\n$$\nSolving for $\\lambda'$ yields the desired relationship:\n$$\n\\lambda' = \\alpha^2 \\lambda\n$$\nThis result shows that to maintain an equivalent objective function under an input feature scaling of $\\alpha$ and a corresponding weight rescaling of $1/\\alpha$, the weight decay coefficient $\\lambda$ must be scaled by $\\alpha^2$.",
            "answer": "$$\\boxed{\\alpha^{2} \\lambda}$$"
        },
        {
            "introduction": "Weight decay, a form of $\\ell_2$ regularization, is not the only method for controlling model complexity. A popular alternative is $\\ell_1$ regularization, which is known for promoting sparse solutions. This exercise presents a hypothetical scenario with correlated features to contrast the behavior of these two powerful techniques. By analyzing the predictive performance of the sparse solution from $\\ell_1$ versus the dense solution from weight decay, you will gain insight into the \"grouping effect\" of $\\ell_2$ regularization and appreciate why it can sometimes lead to more stable and accurate models .",
            "id": "3169449",
            "problem": "Consider a single-output neural network with one fully connected layer and identity activation, so that the prediction is given by $\\hat{y} = \\mathbf{w}^{\\top}\\mathbf{x}$ for a weight vector $\\mathbf{w} \\in \\mathbb{R}^{2}$ and input $\\mathbf{x} \\in \\mathbb{R}^{2}$. The data are generated according to a linear model with Gaussian covariates: $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$, where $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with correlation parameter $\\rho \\in [-1, 1]$, and the label satisfies $y = \\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon$, where $\\boldsymbol{\\theta} = (t, t)^{\\top}$ for some $t \\neq 0$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ is independent noise. Two regularization schemes are applied during training:\n- $\\ell_{1}$ regularization (also called Lasso) with penalty parameter producing a sparse solution $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$ for some $a > 0$.\n- $\\ell_{2}$ regularization (weight decay) with penalty parameter producing a non-sparse solution $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ for the same $a > 0$.\nNote that $\\|\\mathbf{w}^{(1)}\\|_{1} = \\|\\mathbf{w}^{(2)}\\|_{1} = 2a$, but $\\mathbf{w}^{(1)}$ is sparser than $\\mathbf{w}^{(2)}$. Assume the test inputs follow the same distribution as the training inputs, and performance is evaluated by expected Mean Squared Error (MSE), defined as $\\mathbb{E}\\big[(y - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\big]$.\n\nWhich option correctly identifies a case in which $\\ell_{1}$ regularization outperforms $\\ell_{2}$ in sparsity, yet $\\ell_{2}$ (weight decay) yields strictly better predictive performance, and explains this outcome in terms of feature correlation and estimator stability?\n\nA. When the features are positively correlated with $0 \\le \\rho < 1$ and the true signal is spread across both inputs ($\\boldsymbol{\\theta} = (t, t)^{\\top}$ with $t \\neq 0$), the weight-decay solution $\\mathbf{w}^{(2)} = (a, a)^{\\top}$ achieves strictly lower expected MSE than the sparse solution $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$ for any $a > 0$, while $\\mathbf{w}^{(1)}$ remains more sparse. This occurs because $\\ell_{2}$ distributes weight across correlated directions, reducing variance and stabilizing the estimator, whereas $\\ell_{1}$ arbitrarily drops one correlated input, increasing bias and instability.\n\nB. For any correlation level with $0 \\le \\rho < 1$, $\\ell_{1}$ regularization always yields lower expected MSE than $\\ell_{2}$ because it removes redundant features; correlation does not affect estimator stability, and sparsity directly translates into better predictive accuracy.\n\nC. Only in the case of perfectly correlated features ($\\rho = 1$) does weight decay outperform $\\ell_{1}$ in predictive performance; otherwise, both methods have equal expected MSE because they regularize the same total magnitude $\\|\\mathbf{w}\\|_{1} = 2a$.\n\nD. When the features are strongly negatively correlated (e.g., $\\rho \\approx -0.9$) and the true signal is spread across both inputs ($\\boldsymbol{\\theta} = (t, t)^{\\top}$), the sparse solution $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$ is more stable than the weight-decay solution $\\mathbf{w}^{(2)} = (a, a)^{\\top}$, yielding lower expected MSE because dropping one negatively correlated feature reduces harmful variance; $\\ell_{2}$ retains both inputs and therefore overfits.",
            "solution": "The problem requires a comparison of the expected Mean Squared Error (MSE) for two different regularized estimators, $\\mathbf{w}^{(1)}$ and $\\mathbf{w}^{(2)}$, in a linear regression setting with correlated features.\n\n### 1. Formulation of the Expected MSE\nThe expected MSE for an estimator $\\mathbf{w}$ is defined as $\\mathbb{E}\\big[(y - \\mathbf{w}^{\\top}\\mathbf{x})^{2}\\big]$, where the expectation is taken over the data distribution. Given the true model $y = \\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon$ with $\\mathbb{E}[\\varepsilon]=0$ and $\\varepsilon$ independent of $\\mathbf{x}$, the MSE can be decomposed:\n$$\n\\begin{aligned}\n\\text{MSE}(\\mathbf{w}) = \\mathbb{E}\\left[\\left((\\boldsymbol{\\theta}^{\\top}\\mathbf{x} + \\varepsilon) - \\mathbf{w}^{\\top}\\mathbf{x}\\right)^{2}\\right] \\\\\n= \\mathbb{E}\\left[\\left((\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x} + \\varepsilon\\right)^{2}\\right] \\\\\n= \\mathbb{E}\\left[\\left((\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right)^2 + 2\\varepsilon(\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x} + \\varepsilon^2\\right] \\\\\n= \\mathbb{E}\\left[\\left((\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right)^2\\right] + 2\\mathbb{E}[\\varepsilon]\\mathbb{E}\\left[(\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\mathbf{x}\\right] + \\mathbb{E}[\\varepsilon^2] \\\\\n= (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\Sigma(\\boldsymbol{\\theta} - \\mathbf{w}) + \\sigma^2\n\\end{aligned}\n$$\nTo compare the estimators, we only need to compare the risk term $R(\\mathbf{w}) = (\\boldsymbol{\\theta} - \\mathbf{w})^{\\top}\\Sigma(\\boldsymbol{\\theta} - \\mathbf{w})$, as $\\sigma^2$ is a constant irreducible error. We are given $\\boldsymbol{\\theta} = (t, t)^{\\top}$ and $\\Sigma = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$.\n\n### 2. Risk Calculation for the $\\ell_1$ Solution\nFor the sparse solution $\\mathbf{w}^{(1)} = (2a, 0)^{\\top}$, the difference vector is $\\boldsymbol{\\theta} - \\mathbf{w}^{(1)} = (t - 2a, t)^{\\top}$.\n$$\n\\begin{aligned}\nR(\\mathbf{w}^{(1)}) = (t - 2a, t) \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} \\begin{pmatrix} t - 2a \\\\ t \\end{pmatrix} \\\\\n= (t-2a)^2 + 2\\rho t(t-2a) + t^2 \\\\\n= t^2 - 4at + 4a^2 + 2\\rho t^2 - 4a\\rho t + t^2 \\\\\n= 4a^2 - 4at(1+\\rho) + 2t^2(1+\\rho)\n\\end{aligned}\n$$\n\n### 3. Risk Calculation for the $\\ell_2$ Solution\nFor the weight decay solution $\\mathbf{w}^{(2)} = (a, a)^{\\top}$, the difference vector is $\\boldsymbol{\\theta} - \\mathbf{w}^{(2)} = (t - a, t - a)^{\\top}$.\n$$\n\\begin{aligned}\nR(\\mathbf{w}^{(2)}) = (t - a, t - a) \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} \\begin{pmatrix} t - a \\\\ t - a \\end{pmatrix} \\\\\n= (t-a)^2(1 + \\rho + \\rho + 1) = 2(t-a)^2(1+\\rho) \\\\\n= 2(t^2 - 2at + a^2)(1+\\rho) \\\\\n= 2a^2(1+\\rho) - 4at(1+\\rho) + 2t^2(1+\\rho)\n\\end{aligned}\n$$\n\n### 4. Comparison and Conclusion\nWe compare the two risk terms to see when $R(\\mathbf{w}^{(2)})  R(\\mathbf{w}^{(1)})$.\n$$\n2a^2(1+\\rho) - 4at(1+\\rho) + 2t^2(1+\\rho)  4a^2 - 4at(1+\\rho) + 2t^2(1+\\rho)\n$$\nCanceling common terms on both sides, the inequality simplifies to:\n$$\n2a^2(1+\\rho)  4a^2\n$$\nSince $a > 0$, we can divide by $2a^2$:\n$$\n1 + \\rho  2 \\implies \\rho  1\n$$\nThis shows that for any correlation $\\rho$ strictly less than 1, the weight decay solution $\\mathbf{w}^{(2)}$ yields a strictly lower expected MSE.\n\n**Evaluating the options:**\n- **Option A:** Correctly states that for $0 \\le \\rho  1$, the $\\ell_2$ solution has lower MSE. Its explanation is also correct: $\\ell_2$ regularization has a \"grouping effect,\" meaning it tends to shrink the coefficients of correlated features together rather than setting one to zero. This is advantageous when the true model depends on both features, as is the case here. The $\\ell_1$ solution arbitrarily selects one feature, which is less stable and introduces bias.\n- **Option B:** Incorrectly claims $\\ell_1$ is always better.\n- **Option C:** Incorrectly states that $\\ell_2$ only outperforms at $\\rho = 1$. In fact, at $\\rho = 1$, their performance is identical.\n- **Option D:** Incorrectly claims $\\ell_1$ is better for negative correlation. Our derivation shows $\\ell_2$ is better for all $\\rho  1$, including negative values.\n\nTherefore, Option A provides the correct analysis and conclusion.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}