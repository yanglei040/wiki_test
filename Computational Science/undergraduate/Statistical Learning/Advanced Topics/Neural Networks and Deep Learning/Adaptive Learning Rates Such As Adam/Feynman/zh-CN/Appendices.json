{
    "hands_on_practices": [
        {
            "introduction": "Adam优化器的一个核心优势是其对特征缩放的近似不变性。本练习将通过一个精心设计的课程学习场景，让你直观地体验并验证这一特性，即当数据特征的尺度发生剧烈变化时，Adam如何比标准梯度下降（SGD）更快地适应并收敛。通过这个实践，你将深刻理解Adam中的第二动量估计$v_t$是如何通过逐坐标归一化更新步长，从而实现这种强大的自适应能力的。",
            "id": "3096105",
            "problem": "你需要编写一个完整、可运行的程序，在一个训练课程（curriculum）下实证评估自适应矩估计（Adaptive Moment Estimation, Adam）相对于普通随机梯度下降（Stochastic Gradient Descent, SGD）的尺度不变性（scale invariance）和适应速度（adaptation speed）。该课程在训练过程中会逐渐改变单个特征的尺度。实验的核心是使用平方损失的纯线性回归。此测试要求你从基本原理出发实现这两种优化器，并测量在每次尺度变化后，每种优化器达到目标损失所需进行的梯度步数。\n\n数据集和模型规范：\n- 生成一个包含 $N$ 个独立样本和 $d$ 个特征的合成数据集。固定随机种子为 $42$ 以确保结果的确定性。\n- 设 $N = 512$ 且 $d = 2$。\n- 特征矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 的条目从标准正态分布中独立采样。\n- 设真实参数向量为 $w_{\\text{true}} = [\\,2.0,\\,-3.0\\,]^{\\top}$。\n- 使用带加性噪声的线性模型生成目标 $y \\in \\mathbb{R}^{N}$：$y = X w_{\\text{true}} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I)$，$\\sigma = 0.1$。\n- 预测函数为 $\\hat{y}(w) = X_{\\text{scaled}} w$，其中 $X_{\\text{scaled}}$ 是通过将 $X$ 的第二个特征列乘以一个正常量 $s$ 得到的。也就是说，如果 $X = [\\,x^{(1)},\\,x^{(2)}\\,]$，那么对于一个选定的 $s$，有 $X_{\\text{scaled}} = [\\,x^{(1)},\\, s \\cdot x^{(2)}\\,]$。\n- 损失函数为均方误差（Mean Squared Error, MSE）$L(w; X_{\\text{scaled}}, y) = \\frac{1}{N}\\sum_{i=1}^{N}\\left(\\hat{y}_{i}(w) - y_{i}\\right)^{2}$。\n\n课程与评估协议：\n- 一个课程是指一个有限的正尺度因子序列 $\\{s_{0}, s_{1}, \\dots, s_{P-1}\\}$，在一个给定优化器的单次训练运行中，这些因子会按顺序应用于第二个特征。优化器在不同阶段之间会从其当前状态继续；也就是说，当尺度变为下一个 $s_{p}$ 时，参数向量 $w$ 和任何优化器内部状态都不会重置。\n- 对于每个阶段 $p \\in \\{0,1,\\dots,P-1\\}$，通过将 $X$ 的第二个特征乘以 $s_{p}$ 来设置当前的设计矩阵为 $X_{\\text{scaled}}^{(p)}$，然后使用固定的 $X_{\\text{scaled}}^{(p)}$ 进行至多 $T$ 次梯度步骤。\n- 定义目标 MSE 阈值为 $\\tau = c \\cdot \\sigma^{2}$，其中 $c = 2.0$。具体来说，当 $\\sigma = 0.1$ 时，$\\tau = 2.0 \\cdot (0.1)^{2}$。\n- 阶段 $p$ 的适应步数定义为在该阶段内，MSE 首次降至最多为 $\\tau$ 所需的最小梯度步数（如果阶段开始时 MSE 已不大于 $\\tau$，则计为零），或者如果 MSE 在该阶段内从未降至不大于 $\\tau$，则等于 $T$。\n- 对于一个课程，将平均适应步数定义为该课程中所有阶段的每阶段适应步数的算术平均值。\n\n需要从基本原理实现的算法：\n- 对于普通随机梯度下降（SGD），使用固定的学习率 $\\alpha_{\\text{sgd}}$ 和 MSE 的全批量梯度。$L$ 相对于 $w$ 的全批量梯度为\n$$\n\\nabla L(w) \\;=\\; \\frac{2}{N}\\,X_{\\text{scaled}}^{\\top}\\left(X_{\\text{scaled}}w - y\\right).\n$$\n- 对于自适应矩估计（Adam），设计其更新规则，使用梯度的一阶和二阶矩的指数移动平均以及偏差校正，使得每个坐标的更新幅度对特征的重新缩放近似不变。你必须推导并实现对梯度及其元素平方使用带有衰减参数的指数移动平均，并在形成每个参数的归一化步长之前进行偏差校正。Adam 的任何内部状态（如矩估计或时间索引）在课程阶段之间不得重置。\n\n超参数：\n- SGD 使用 $\\alpha_{\\text{sgd}} = 0.001$。\n- Adam 使用学习率 $\\alpha_{\\text{adam}} = 0.01$，一阶矩衰减 $\\beta_{1} = 0.9$，二阶矩衰减 $\\beta_{2} = 0.999$，以及数值稳定性常数 $\\varepsilon_{\\text{adam}} = 10^{-8}$。\n- 在所有步骤中均使用全批量梯度。\n- 对于每个课程阶段，最多允许 $T$ 步，其中 $T$ 由每个测试用例指定。\n\n测试套件：\n- 所有测试用例共享由上述 $X$ 和 $y$ 构建的相同数据集。\n- 每个测试用例指定一个课程和每阶段的步数预算 $T$。\n- 测试用例如下：\n    1. Case A (顺利情况)：尺度为 $\\left[\\,1.0,\\,10.0,\\,100.0\\,\\right]$，$T = 400$。\n    2. Case B (先缩小后放大)：尺度为 $\\left[\\,1.0,\\,0.1,\\,10.0\\,\\right]$，$T = 400$。\n    3. Case C (边界情况：无尺度变化)：尺度为 $\\left[\\,1.0\\,\\right]$，$T = 200$。\n\n所需输出：\n- 对于每个测试用例，计算比率\n$$\nr \\;=\\; \\frac{\\text{SGD 在课程上的平均适应步数}}{\\text{Adam 在课程上的平均适应步数}}。\n$$\n- 将三个比率按顺序 $\\left[\\text{Case A},\\text{Case B},\\text{Case C}\\right]$ 聚合为单行。\n- 你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，$\\left[\\text{resultA},\\text{resultB},\\text{resultC}\\right]$）。每个元素必须是一个实数。\n\n科学真实性与推导基础：\n- 从线性回归的 MSE 定义、全批量梯度表达式以及带偏差校正的指数移动平均定义开始。推导通过根二阶矩估计进行逐坐标归一化如何使得更新幅度对相应特征的常数重缩放近似不变，并在 Adam 中实现这一点。不要在阶段之间重置优化器状态，以正确测量突然重缩放后的适应速度。确保你的实现是数值稳定的，并且在固定的种​​子下实验是确定性的。",
            "solution": "该问题要求在一个线性回归任务上，对普通全批量梯度下降（Gradient Descent, GD）和自适应矩估计（Adaptive Moment Estimation, Adam）进行实证比较。比较的重点是适应速度，通过在其中一个特征的尺度发生突变后，达到目标损失所需的优化步数来衡量。这是一个设计精良的数值实验，旨在突显 Adam 的尺度不变性特性。问题陈述具有科学依据，内部一致，并为获得确定性和可复现的结果提供了所有必要的参数。\n\n我们首先将数学设定形式化。数据集包含 $N=512$ 个样本，每个样本有 $d=2$ 个特征。特征矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 的条目从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取。目标变量 $y \\in \\mathbb{R}^{N}$ 由一个带加性高斯噪声的线性模型生成：\n$$\ny = X w_{\\text{true}} + \\varepsilon\n$$\n其中，真实参数向量为 $w_{\\text{true}} = [2.0, -3.0]^{\\top}$，噪声为 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$，且 $\\sigma = 0.1$。为了可复现性，数据生成使用固定的随机种子 $42$。\n\n预测模型是一个线性函数 $\\hat{y}(w) = X_{\\text{scaled}} w$，其中 $X_{\\text{scaled}}$ 是通过将其第二个特征列乘以一个因子 $s > 0$ 从 $X$ 派生而来。如果 $X = [x^{(1)}, x^{(2)}]$，则 $X_{\\text{scaled}} = [x^{(1)}, s \\cdot x^{(2)}]$。目标是找到最小化均方误差（MSE）损失的参数向量 $w \\in \\mathbb{R}^d$：\n$$\nL(w; X_{\\text{scaled}}, y) = \\frac{1}{N} \\| X_{\\text{scaled}} w - y \\|_2^2 = \\frac{1}{N} \\sum_{i=1}^{N} \\left( (X_{\\text{scaled}} w)_i - y_i \\right)^2\n$$\n最小化此损失的最优参数 $w^*$ 取决于尺度 $s$。它们满足 $X_{\\text{scaled}}^\\top (X_{\\text{scaled}} w^* - y) = 0$。在无噪声的情况下，这意味着 $X_{\\text{scaled}} w^* \\approx X w_{\\text{true}}$，这在 $w_1^* = w_{\\text{true},1}$ 且 $s \\cdot w_2^* = w_{\\text{true},2}$ 时成立。因此，目标参数向量近似为 $w^*(s) = [2.0, -3.0/s]^\\top$。\n\n为最小化损失，我们使用基于梯度的优化方法。MSE 损失相对于 $w$ 的梯度由下式给出：\n$$\n\\nabla_w L(w) = \\frac{2}{N} X_{\\text{scaled}}^{\\top} (X_{\\text{scaled}} w - y)\n$$\n两种优化算法都使用这个梯度，并从基本原理实现。\n\n首先，我们考虑普通梯度下降（GD），问题中称之为 SGD，但使用的是全批量梯度。其更新规则为：\n$$\nw_{t+1} = w_t - \\alpha_{\\text{gd}} \\nabla_w L(w_t)\n$$\n其中 $w_t$ 是第 $t$ 步的参数向量，$\\alpha_{\\text{gd}} = 0.001$ 是固定的学习率。GD 的性能对特征的缩放高度敏感。梯度 $\\nabla_w L(w)$ 的分量依赖于尺度 $s$。第二个分量 $\\frac{\\partial L}{\\partial w_2}$ 受 $s$ 的影响很大。大的 $s$ 会使这个梯度分量变得非常大，导致振荡和不稳定，除非 $\\alpha_{\\text{gd}}$ 非常小。相反，小的 $s$ 会导致梯度分量消失，从而减慢 $w_2$ 的学习速度。\n\n其次，我们实现 Adam 优化器。Adam 维护过去梯度的指数衰减平均（一阶矩）和过去梯度平方的指数衰减平均（二阶矩）。令 $g_t = \\nabla_w L(w_{t-1})$ 为第 $t$ 步的梯度。矩估计更新如下：\n$$\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n$$\n$$\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n$$\n其中 $g_t^2$ 表示逐元素平方，$\\beta_1 = 0.9$ 是一阶矩衰减率，$\\beta_2 = 0.999$ 是二阶矩衰减率。这些是有偏估计，尤其是在初始步骤中。Adam 通过计算以下方式来校正这种偏差：\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} \\quad \\text{和} \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n$$\n最终的参数更新为：\n$$\nw_t = w_{t-1} - \\alpha_{\\text{adam}} \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon_{\\text{adam}}}\n$$\n学习率为 $\\alpha_{\\text{adam}} = 0.01$，还有一个小的稳定性常数 $\\varepsilon_{\\text{adam}} = 10^{-8}$。Adam 适应性的关键在于 $\\sqrt{\\hat{v}_t}$ 这一项，它对每个参数坐标的更新进行归一化。如果某个参数 $w_j$ 的梯度分量持续很大（例如，由于特征尺度 $s$ 很大），其对应的二阶矩估计 $v_{t,j}$ 也会很大。除以 $\\sqrt{\\hat{v}_{t,j}}$ 会有效地减小该参数的步长。反之，如果梯度很小，步长则会有效地增大。这种机制使得 Adam 的更新步长近似于对特征尺度不变，使其能够在损失函数的地形发生变化时快速适应。\n\n评估协议涉及一个尺度课程 $\\{s_0, s_1, \\dots, s_{P-1}\\}$。对于每个尺度 $s_p$，优化器最多运行 $T$ 步。优化器的状态（GD 的 $w$；Adam 的 $w, m, v, t$）在各个阶段之间保持不变。我们测量每个阶段内使 MSE 降至阈值 $\\tau = c \\cdot \\sigma^2 = 2.0 \\cdot (0.1)^2 = 0.02$ 以下所需的步数。如果在一个阶段开始时损失已经低于 $\\tau$，则该阶段的步数为 $0$。如果在 $T$ 步内未达到 $\\tau$，则步数为 $T$。性能指标是课程中所有阶段的平均适应步数。最后，我们计算 GD 与 Adam 的平均步数之比。\n\n以下代码实现了整个过程，包括数据生成、两种从基本原理出发的优化器，以及针对三个测试用例的指定课程评估。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment, evaluate optimizers, and print the result.\n    \"\"\"\n    \n    # 1. Dataset and Model Specification\n    N, d = 512, 2\n    w_true = np.array([2.0, -3.0])\n    sigma = 0.1\n    random_seed = 42\n\n    def generate_data(N, d, w_true, sigma, seed):\n        \"\"\"Generates synthetic dataset based on problem specifications.\"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.standard_normal(size=(N, d))\n        noise = rng.normal(loc=0.0, scale=sigma, size=N)\n        y = X @ w_true + noise\n        return X, y\n\n    X, y = generate_data(N, d, w_true, sigma, random_seed)\n\n    # 2. Loss and Gradient Functions\n    def mse_loss(w, X_scaled, y):\n        \"\"\"Computes the Mean Squared Error.\"\"\"\n        error = X_scaled @ w - y\n        return np.mean(error**2)\n\n    def gradient(w, X_scaled, y):\n        \"\"\"Computes the full-batch gradient of the MSE loss.\"\"\"\n        N_samples = X_scaled.shape[0]\n        error = X_scaled @ w - y\n        return (2 / N_samples) * X_scaled.T @ error\n\n    # 3. Experiment Runner\n    def run_experiment(optimizer_type, X_base, y_base, curriculum, T, hyperparams):\n        \"\"\"\n        Runs an optimization experiment for a given optimizer and curriculum.\n        Returns the average adaptation steps.\n        \"\"\"\n        w = np.zeros(d)\n        tau = 2.0 * (sigma**2)\n        \n        # Optimizer state\n        if optimizer_type == 'adam':\n            alpha, beta1, beta2, epsilon = hyperparams\n            m = np.zeros(d)\n            v = np.zeros(d)\n            t_global = 0\n        else: # gd\n            alpha, = hyperparams\n\n        phase_adaptation_steps = []\n\n        for s in curriculum:\n            X_scaled = X_base.copy()\n            X_scaled[:, 1] *= s\n\n            # Check loss at the beginning of the phase\n            initial_loss = mse_loss(w, X_scaled, y_base)\n            if initial_loss <= tau:\n                phase_adaptation_steps.append(0)\n                continue\n\n            steps_taken_in_phase = T\n            for step in range(1, T + 1):\n                grad = gradient(w, X_scaled, y_base)\n\n                if optimizer_type == 'gd':\n                    w -= alpha * grad\n                else:  # adam\n                    t_global += 1\n                    m = beta1 * m + (1 - beta1) * grad\n                    v = beta2 * v + (1 - beta2) * (grad**2)\n                    \n                    # Bias correction\n                    m_hat = m / (1 - beta1**t_global)\n                    v_hat = v / (1 - beta2**t_global)\n                    \n                    w -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n                \n                current_loss = mse_loss(w, X_scaled, y_base)\n                if current_loss <= tau:\n                    steps_taken_in_phase = step\n                    break\n            \n            phase_adaptation_steps.append(steps_taken_in_phase)\n\n        return np.mean(phase_adaptation_steps)\n\n    # 4. Hyperparameters and Test Suite\n    hyperparams_gd = (0.001,)\n    hyperparams_adam = (0.01, 0.9, 0.999, 1e-8)\n\n    test_cases = [\n        {'name': 'Case A', 'scales': [1.0, 10.0, 100.0], 'T': 400},\n        {'name': 'Case B', 'scales': [1.0, 0.1, 10.0], 'T': 400},\n        {'name': 'Case C', 'scales': [1.0], 'T': 200},\n    ]\n\n    ratios = []\n    for case in test_cases:\n        curriculum = case['scales']\n        T = case['T']\n\n        avg_steps_gd = run_experiment('gd', X, y, curriculum, T, hyperparams_gd)\n        avg_steps_adam = run_experiment('adam', X, y, curriculum, T, hyperparams_adam)\n\n        if avg_steps_adam == 0.0:\n            # If Adam takes 0 steps, it's infinitely efficient. Ratio is 1 if GD also takes 0.\n            ratio = 1.0 if avg_steps_gd == 0.0 else np.inf\n        else:\n            ratio = avg_steps_gd / avg_steps_adam\n        \n        ratios.append(ratio)\n    \n    # 5. Final Output\n    print(f\"[{','.join(map(str, ratios))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "尽管自适应方法非常强大，但它们并非万能药。在某些非凸优化问题中，Adam的自适应机制有时会适得其反，导致其收敛到次优的局部最小值。本练习将引导你构建一个巧妙的一维非凸问题，在该问题中，Adam会陷入一个较差的解，而简单的SGD却能成功“逃脱”并找到更好的解。这个反直觉的例子揭示了第二动量$v_t$在面对稀疏但信息量大的梯度时可能带来的陷阱，从而加深你对Adam行为局限性的认识。",
            "id": "3096082",
            "problem": "我们要求你构建并分析一个非凸玩具优化问题，用以说明自适应矩估计 (Adam) 如何收敛到一个次优盆地，而随机梯度下降 (SGD) 则能逃逸，其定性原因与二阶矩累积量 $v_t$ 引起的方向变化有关。你的工作必须以一个完整的、可运行的程序形式呈现，该程序在同一个确定性非凸目标上实现 SGD 和 Adam，并在一个小型测试套件上比较它们的行为。\n\n从以下基本出发点开始：使用源自负梯度方向的迭代一阶更新，对一维参数 $x$ 最小化一个可微标量目标函数 $L$。使用指数移动平均的标准定义来实现 Adam 的自适应缩放，此处无需写出其闭式更新公式；你的实现应与 Adam 的广泛实践（包括偏差修正）保持一致。对于 SGD，使用固定的学习率。\n\n将非凸目标函数定义为\n$$\nL(x) = (x^2 - 1)^2 - c\\,x,\n$$\n其中倾斜参数 $c = 0.3$。其精确梯度为\n$$\n\\nabla L(x) = 4x(x^2 - 1) - c.\n$$\n为了模拟一个具有结构化变异性、能够产生方向变化的确定性小批量梯度估计器，我们在精确梯度上添加一个周期性扰动序列。对于每个迭代索引 $t \\in \\{0,1,2,\\dots\\}$，定义周期 $p = 10$ 和一个扰动\n$$\n\\delta_t = \\begin{cases}\n-a,  \\text{if } t \\bmod p \\neq p-1,\\\\\n+b,  \\text{if } t \\bmod p = p-1,\n\\end{cases}\n$$\n使得梯度估计器为 $g_t(x) = \\nabla L(x) + \\delta_t$。直观上，如果 $b$ 相对于 $a$ 足够大，那么许多小的负向推动和偶尔大的正向尖峰会在一个周期内产生正的平均值，然而 Adam 的 $v_t$ 会削弱尖峰的影响并改变净方向。\n\n在此 $g_t(x)$ 上实现这两种优化器，具体细节如下：\n- 使用 $T$ 次迭代，并根据每个测试用例的规定初始化 $x_0$。\n- 对于 SGD，使用恒定的步长 $\\eta$。\n- 对于 Adam，使用步长 $\\alpha$、一阶矩参数 $\\beta_1$、二阶矩参数 $\\beta_2$ 和数值稳定器 $\\varepsilon$。包括针对一阶矩和二阶矩的标准偏差修正。\n\n对于每次运行，根据最终迭代值 $x_T$ 的符号将其归类到一个盆地：如果 $x_T < 0$，则为左盆地（对于给定的 $c$ 是次优的）；如果 $x_T > 0$，则为右盆地（对于给定的 $c$ 是更优的盆地）。对于每个测试用例，输出一个布尔值，指示 Adam 是否最终进入左盆地而 SGD 最终进入右盆地。\n\n测试套件：\n使用以下四组参数来测试该行为的不同方面。在所有情况下，使用 $c = 0.3$，周期 $p = 10$，噪声 $a = 0.03$，尖峰 $b = 4.0$，以及如上定义的扰动。\n\n- 用例 1（理想路径）：$x_0 = 0.1$，$T = 4000$，SGD：$\\eta = 0.01$，Adam：$\\alpha = 0.01$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$，$\\varepsilon = 10^{-8}$。\n- 用例 2（从一个看似平稳的区域开始）：$x_0 = 0.0$，$T = 4000$，SGD：$\\eta = 0.01$，Adam：$\\alpha = 0.01$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$，$\\varepsilon = 10^{-8}$。\n- 用例 3（从略为负值的位置开始）：$x_0 = -0.05$，$T = 4000$，SGD：$\\eta = 0.01$，Adam：$\\alpha = 0.01$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$，$\\varepsilon = 10^{-8}$。\n- 用例 4（改变 $v_t$ 记忆的边界条件）：$x_0 = 0.1$，$T = 4000$，SGD：$\\eta = 0.01$，Adam：$\\alpha = 0.01$，$\\beta_1 = 0.9$，$\\beta_2 = 0.9$，$\\varepsilon = 10^{-8}$。\n\n最终输出规范：\n你的程序应生成单行输出，其中包含四个测试用例的布尔结果，按顺序排列，形式为用方括号括起来的逗号分隔列表（例如，`[True,False,True,True]`）。不应打印任何其他文本。在此问题中，所有数字都没有单位。不使用角度。不使用百分比；任何分数量都以上述指定的小数形式出现。",
            "solution": "该问题是有效的。它提出了一个定义明确的数值优化计算任务，该任务基于既定的原理和算法。其目标是构建并分析一个特定场景，在该场景中，Adam 优化器收敛到次优解，而标准的随机梯度下降 (SGD) 找到了更好的解。这是一个已知的现象，该问题提供了一个精确的、确定性的框架来复现它。所有参数和条件都已明确说明，使得该问题自成一体、无歧义且可通过计算验证。\n\n问题的核心在于非凸目标函数 $L(x)$ 与一个特殊构造的梯度估计器 $g_t(x)$ 之间的相互作用。\n\n目标函数是一个倾斜的双阱势：\n$$\nL(x) = (x^2 - 1)^2 - c\\,x\n$$\n其中倾斜参数 $c = 0.3$。该函数有两个局部最小值。项 $-c\\,x$（其中 $c>0$）使得正 $x$ 处的最小值为全局最小值（“更优的”盆地），而负 $x$ 处的最小值为次优局部最小值（“左侧”盆地）。任务是观察一个从 $x=0$ 附近开始的优化器，是否能够导航到 $x>0$ 的更优盆地。该函数的梯度是：\n$$\n\\nabla L(x) = 4x(x^2 - 1) - c\n$$\n\n优化器不使用真实梯度，而是使用一个扰动梯度 $g_t(x) = \\nabla L(x) + \\delta_t$。扰动 $\\delta_t$ 是确定性的，周期为 $p=10$。在每 10 次迭代中，有 9 次它提供一个小的负向推动 $\\delta_t = -a = -0.03$。在第 10 次迭代中，它提供一个大的正向尖峰 $\\delta_t = +b = 4.0$。一个周期内的平均扰动为 $\\frac{9(-a) + b}{p} = \\frac{9(-0.03) + 4.0}{10} = 0.373$，该值为正。\n\n我们将实现并比较两种一阶优化算法：\n\n1.  **随机梯度下降 (SGD)**：更新规则是在负梯度方向上迈出简单的一步，学习率 $\\eta$ 固定：\n    $$\n    x_{t+1} = x_t - \\eta g_t(x_t)\n    $$\n    对于 SGD，更新方向完全由当前梯度估计 $g_t(x_t)$ 的符号决定。虽然单次更新可能指向不同方向，但多次迭代的累积效应决定了轨迹。将一个周期内的位移相加（对于小的 $x$），九次向右的推动的净效应超过了一次向左的推动，从而将 SGD 引导向右盆地（$x>0$）。\n\n2.  **自适应矩估计 (Adam)**：Adam 基于梯度的的一阶矩和二阶矩估计，为每个参数自适应地调整学习率。在第 $t$ 次迭代中，更新涉及以下步骤：\n    -   计算梯度估计：$g_t = g_t(x_{t-1})$\n    -   更新有偏一阶矩（均值）：$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n    -   更新有偏二阶矩（非中心方差）：$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n    -   计算偏差修正后的矩：$\\hat{m}_t = m_t / (1 - \\beta_1^t)$ 和 $\\hat{v}_t = v_t / (1 - \\beta_2^t)$\n    -   更新参数：$x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}$\n\n在这个问题中，Adam 行为的关键在于二阶矩累积量 $v_t$。大的正向梯度尖峰（即 $\\delta_t = +b$ 时的 $g_t$）会产生一个非常大的 $g_t^2$ 值。这会使 $v_t$ 膨胀。由于 $\\beta_2$ 的值很高（例如 0.999），这种膨胀会持续很多次后续迭代，导致有效学习率 $\\alpha/(\\sqrt{\\hat{v}_t} + \\varepsilon)$ 变得非常小。与尖峰相对应的更新本身会将 $x$ 向左推。随后的九次更新（通常会将 $x$ 向右推）的幅度被严重抑制。因此，单次大的向左推动主导了九次微小的向右推动，导致 Adam 收敛到次优的左盆地（$x<0$）。\n\n用例 4 通过设置 $\\beta_2=0.9$ 来检验这个假设。由于二阶矩的记忆时间更短，$v_t$ 在尖峰过后会衰减得更快。学习率的抑制是短暂的，这使得随后的向右推动能产生更大的效果。这应该足以让 Adam 克服单次向左的推动并逃逸到右盆地，就像 SGD 一样。\n\n该程序将实现这两种算法，对四个指定的测试用例进行运行，并为每个用例确定条件（Adam 收敛到左盆地且 SGD 收敛到右盆地）是否满足。这需要为每个优化器模拟 $T=4000$ 步，然后对最终参数值 $x_T$ 的符号进行分类。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves the optimization problem for the given test suite.\n    \"\"\"\n\n    # --- Problem Definitions ---\n\n    def grad_L(x, c):\n        \"\"\"Computes the exact gradient of the objective function L(x).\"\"\"\n        return 4.0 * x * (x**2 - 1.0) - c\n\n    def delta_t(t, p, a, b):\n        \"\"\"Computes the deterministic perturbation at iteration index t.\"\"\"\n        if (t % p) == (p - 1):\n            return b\n        else:\n            return -a\n\n    def run_sgd(x0, T, eta, c, p, a, b):\n        \"\"\"Runs the SGD optimization.\"\"\"\n        x = float(x0)\n        for t in range(T):\n            g_t = grad_L(x, c) + delta_t(t, p, a, b)\n            x -= eta * g_t\n        return x\n\n    def run_adam(x0, T, alpha, beta1, beta2, epsilon, c, p, a, b):\n        \"\"\"Runs the Adam optimization.\"\"\"\n        x = float(x0)\n        m = 0.0\n        v = 0.0\n        \n        # Use iterative updates for powers to maintain numerical stability for large T\n        beta1_power = 1.0\n        beta2_power = 1.0\n\n        for t in range(T):\n            g_t = grad_L(x, c) + delta_t(t, p, a, b)\n            \n            m = beta1 * m + (1.0 - beta1) * g_t\n            v = beta2 * v + (1.0 - beta2) * (g_t**2)\n            \n            # Iteration number is t+1 (1-indexed)\n            beta1_power *= beta1\n            beta2_power *= beta2\n\n            m_hat = m / (1.0 - beta1_power)\n            v_hat = v / (1.0 - beta2_power)\n            \n            update = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n            x -= update\n            \n        return x\n\n    # --- Test Execution ---\n    \n    # Common parameters from the problem statement\n    c_param = 0.3\n    p_param = 10\n    a_param = 0.03\n    b_param = 4.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: (happy path)\n        {'x0': 0.1, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_eps': 1e-8},\n        # Case 2: (starting at a stationary-looking region)\n        {'x0': 0.0, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_eps': 1e-8},\n        # Case 3: (slightly negative start)\n        {'x0': -0.05, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_eps': 1e-8},\n        # Case 4: (boundary condition altering v_t memory)\n        {'x0': 0.1, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.9, 'adam_eps': 1e-8},\n    ]\n\n    results = []\n    for case in test_cases:\n        # Run SGD simulation\n        x_sgd = run_sgd(case['x0'], case['T'], case['sgd_eta'], c_param, p_param, a_param, b_param)\n        \n        # Run Adam simulation\n        x_adam = run_adam(case['x0'], case['T'], case['adam_alpha'], case['adam_beta1'], case['adam_beta2'], case['adam_eps'], c_param, p_param, a_param, b_param)\n        \n        # Classify basins based on the sign of the final iterate\n        adam_in_left_basin = x_adam < 0\n        sgd_in_right_basin = x_sgd > 0\n        \n        # Determine if the specified condition is met\n        condition_met = adam_in_left_basin and sgd_in_right_basin\n        results.append(condition_met)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "在了解了Adam的优点和潜在缺陷之后，是时候将它投入到一个更接近实际应用的场景中了：稀疏恢复。本练习要求你在一个带有光滑$L_1$正则化的线性回归任务中，公平地比较Adam和SGD在识别真实稀疏解方面的效率。通过在相同的梯度计算预算下进行这场“竞赛”，你将学会如何设计和评估优化算法的实证性能，并探索自适应学习率在加速发现模型结构方面的实际效果。",
            "id": "3096054",
            "problem": "要求您在给定相同梯度计算预算的情况下，通过实证评估自适应矩估计 (Adam) 是否比随机梯度下降 (SGD) 更能加速稀疏恢复。该比较必须在一个合成线性回归任务上进行，该任务使用一个明确指定的 $\\ell_1$ 正则化器的光滑近似。\n\n从以下基础出发：\n- 经验风险最小化旨在选择参数 $\\mathbf{w} \\in \\mathbb{R}^d$ 以最小化样本平均损失加上正则化项。\n- 线性模型的平方误差损失是一个经过充分检验的目标函数：对于数据 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ 和响应 $\\mathbf{y} \\in \\mathbb{R}^n$，经验均方误差为 $\\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2$。\n- 稀疏性可以通过 $\\ell_1$ 型惩罚来促进，但为了能够使用基于梯度的算法，您必须使用 $\\ell_1$ 的光滑近似。\n\n您的程序必须实现以下实验：\n1. 为每个测试用例生成一个合成数据集：\n   - 抽取一个设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，其条目为独立的标准正态分布，然后将每列缩放至单位 $\\ell_2$ 范数。\n   - 通过均匀随机选择 $K$ 个索引并赋予从量级为 $1$ 且符号随机的分布中抽取的非零值，构建一个 $K$-稀疏的真实参数向量 $\\mathbf{w}^\\star \\in \\mathbb{R}^d$。其余条目为零。\n   - 生成响应 $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^\\star + \\boldsymbol{\\varepsilon}$，其中 $\\boldsymbol{\\varepsilon}$ 的条目为独立的、均值为 $0$、标准差为 $\\sigma$ 的高斯分布。\n2. 定义目标函数\n   $$L(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\lambda \\sum_{i=1}^d \\sqrt{w_i^2 + \\beta^2},$$\n   其中惩罚项是使用函数 $w \\mapsto \\sqrt{w^2 + \\beta^2}$ 和平滑参数 $\\beta > 0$ 对 $\\|\\mathbf{w}\\|_1$ 的光滑近似。\n3. 使用批量大小为 $b$ 的小批量随机梯度从零向量初始化更新 $\\mathbf{w}$：\n   - 对于 SGD：在每次迭代 $t$，使用小批量梯度估计更新 $\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{SGD}} \\cdot \\widehat{\\nabla} L(\\mathbf{w}_t)$。\n   - 对于 Adam：在每次迭代 $t$，使用标准 Adam 规则和步长 $\\eta_{\\text{Adam}}$ 及固定的超参数进行更新。不要改变相对于 SGD 的梯度预算；为了公平起见，两种方法必须使用相同的批量大小 $b$ 执行相同次数的迭代 $T$，并使用相同的小批量序列。\n4. 定义在迭代 $t$ 时的估计支撑集为 $S_t = \\{ i \\in \\{1,\\dots,d\\} : |w_{t,i}| \\ge \\theta \\}$，真实支撑集为 $S^\\star = \\{ i : w^\\star_i \\ne 0 \\}$。\n5. 定义最早恢复时间 $t_{\\text{SGD}}$（对于 Adam 相应为 $t_{\\text{Adam}}$）为使得 $S_t = S^\\star$ 的最小 $t \\in \\{1,2,\\dots,T\\}$。如果预算内不存在这样的 $t$，则将 $t$ 设置为 $+\\infty$。\n6. 您的程序必须根据以下规则判断 Adam 是否在相同预算下加速了稀疏恢复：\n   - 如果至少有一种方法在预算内精确恢复了支撑集，当 $t_{\\text{Adam}} \\le t_{\\text{SGD}}$ 时，则声明 Adam 加速了恢复。\n   - 如果两种方法都未能在预算内精确恢复，则计算最终迭代 $t=T$ 时的支撑集并比较其 F1 分数，其中对于集合 $A$ 和 $B$，F1 分数定义为\n     $$\\text{F1}(A,B) = \\frac{2|A \\cap B|}{2|A \\cap B| + |A \\setminus B| + |B \\setminus A|}.$$\n     在这种情况下，如果 $\\text{F1}(S_T^{\\text{Adam}}, S^\\star) > \\text{F1}(S_T^{\\text{SGD}}, S^\\star)$，则声明 Adam 加速了恢复。\n7. 每个测试用例的随机数生成必须是确定性的。每个用例使用固定的种子，以确保结果是可复现的。SGD 和 Adam 的小批量序列必须相同。\n\n实现必须遵循基本定义，不得依赖任何没有根据的捷径。特别地，要从第一性原理推导光滑 $\\ell_1$ 惩罚项的梯度。\n\n测试套件：\n- 用例 1：$(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (50,200,5,0.05,0.05,10^{-3},20,0.05,0.01,400,0.05)$\n- 用例 2：$(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (60,120,8,0.2,0.1,10^{-2},30,0.03,0.01,600,0.08)$\n- 用例 3：$(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (40,100,4,0,0.02,10^{-3},25,0.04,0.01,300,0.04)$\n- 用例 4：$(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (80,200,10,0.15,0.15,5\\times10^{-2},40,0.02,0.005,800,0.1)$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[True,False,True,False]”），其中每个布尔值对应于上述顺序中相应测试用例的判定结果。",
            "solution": "用户问题是一项实证研究，旨在比较 Adam 优化器与随机梯度下降 (SGD) 在稀疏恢复任务上的性能。该任务被表述为一个带有 $\\ell_1$ 正则化器光滑近似的线性回归问题。\n\n### 步骤 1：问题验证\n\n**1.1. 提取给定条件**\n\n- **目标函数：** $L(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\lambda \\sum_{i=1}^d \\sqrt{w_i^2 + \\beta^2}$，参数为 $\\mathbf{w} \\in \\mathbb{R}^d$。\n- **合成数据生成：**\n    - 设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ 来自独立的 $\\mathcal{N}(0,1)$ 条目，列被缩放至单位 $\\ell_2$ 范数。\n    - 真实参数向量 $\\mathbf{w}^\\star \\in \\mathbb{R}^d$ 是 $K$-稀疏的。$K$ 个非零位置是均匀随机选择的。非零值的大小量级为 1 且符号随机。\n    - 响应向量 $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^\\star + \\boldsymbol{\\varepsilon}$，噪声 $\\boldsymbol{\\varepsilon}$ 的条目是来自 $\\mathcal{N}(0, \\sigma^2)$ 的独立同分布随机变量。\n- **优化算法：**\n    - 小批量随机梯度，批量大小为 $b$，总迭代次数为 $T$。\n    - 初始化：$\\mathbf{w}_0 = \\mathbf{0}$。\n    - SGD 更新：$\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{SGD}} \\cdot \\widehat{\\nabla} L(\\mathbf{w}_t)$。\n    - Adam 更新：使用步长 $\\eta_{\\text{Adam}}$ 的标准 Adam 规则。\n    - 两种方法使用相同的小批量序列。\n- **评估标准：**\n    - 真实支撑集：$S^\\star = \\{ i : w^\\star_i \\ne 0 \\}$。\n    - 在迭代 $t$ 时的估计支撑集：$S_t = \\{ i : |w_{t,i}| \\ge \\theta \\}$。\n    - 最早恢复时间 $t_{\\text{opt}}$：使得 $S_t = S^\\star$ 的最小 $t \\in \\{1,\\dots,T\\}$。如果不存在这样的 $t$，则 $t_{\\text{opt}} = +\\infty$。\n- **比较规则：**\n    - 如果至少有一种方法恢复了支撑集（即 $\\min(t_{\\text{SGD}}, t_{\\text{Adam}}) < +\\infty$）：若 $t_{\\text{Adam}} \\le t_{\\text{SGD}}$，则 Adam 加速了恢复。\n    - 如果两种方法都未恢复：若 $\\text{F1}(S_T^{\\text{Adam}}, S^\\star) > \\text{F1}(S_T^{\\text{SGD}}, S^\\star)$，则 Adam 加速了恢复。\n    - F1 分数：$\\text{F1}(A,B) = \\frac{2|A \\cap B|}{2|A \\cap B| + |A \\setminus B| + |B \\setminus A|}$。\n- **可复现性：** 每个测试用例必须使用固定的随机种子。\n- **测试用例：** 四个用例，具有特定的参数集 $(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta)$。\n\n**1.2. 验证与结论**\n\n- **科学基础：** 该问题牢固地植根于统计学习、优化和稀疏方法的既定原则。所有组成部分（线性回归、$\\ell_1$ 正则化、SGD、Adam）都是标准的。对不可微的 $\\ell_1$ 范数使用光滑近似是一种众所周知的技术。\n- **适定性：** 问题是适定的。目标明确定义，由于固定的种子，实验过程是确定性的，并且比较指标是明确的，涵盖了所有可能的结果。每个测试用例都存在唯一且有意义的解。\n- **客观性：** 问题以精确、定量的术语陈述，没有主观或模糊的语言。\n\n该问题没有表现出任何缺陷，如科学上不合理、信息缺失或逻辑矛盾。“量级为 1”这一术语是合成数据生成中的标准惯例，可解释为从某个围绕 1 的均匀分布中抽样，并不会使问题无效。\n\n**结论：问题是有效的。**\n\n### 步骤 2：解法推导与算法设计\n\n问题的核心是使用两种不同的基于梯度的优化器最小化目标函数 $L(\\mathbf{w})$，并比较它们在识别真实参数向量 $\\mathbf{w}^\\star$ 的稀疏支撑集方面的有效性。\n\n**2.1. 目标函数与梯度**\n\n目标函数为\n$$L(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\lambda \\sum_{i=1}^d \\sqrt{w_i^2 + \\beta^2}$$\n该函数是数据保真项 $L_{\\text{data}}(\\mathbf{w})$ 和正则化项 $L_{\\text{reg}}(\\mathbf{w})$ 的和。为了应用基于梯度的优化器，我们必须计算梯度 $\\nabla L(\\mathbf{w}) = \\nabla L_{\\text{data}}(\\mathbf{w}) + \\nabla L_{\\text{reg}}(\\mathbf{w})$。\n\n数据保真项（均方误差）的梯度是：\n$$ \\nabla L_{\\text{data}}(\\mathbf{w}) = \\frac{1}{n} \\mathbf{X}^T (\\mathbf{X}\\mathbf{w} - \\mathbf{y}) $$\n正则化项的梯度通过对每个分量 $w_j$（对于 $j \\in \\{1, \\dots, d\\}$）求导得到：\n$$ \\frac{\\partial}{\\partial w_j} L_{\\text{reg}}(\\mathbf{w}) = \\lambda \\frac{\\partial}{\\partial w_j} \\sqrt{w_j^2 + \\beta^2} = \\lambda \\frac{1}{2\\sqrt{w_j^2 + \\beta^2}} (2w_j) = \\frac{\\lambda w_j}{\\sqrt{w_j^2 + \\beta^2}} $$\n正则化项的完整梯度是一个向量，其每个分量由上述表达式给出。这可以用逐元素操作紧凑地写出：\n$$ \\nabla L_{\\text{reg}}(\\mathbf{w}) = \\lambda \\frac{\\mathbf{w}}{\\sqrt{\\mathbf{w}^2 + \\beta^2}} $$\n其中平方和平方根是逐元素应用的。由于 $\\beta > 0$，分母始终严格为正，确保梯度是良定义的。\n\n**2.2. 随机小批量梯度**\n\n在实践中，对于大的 $n$，我们使用在数据的小批量上计算的梯度随机近似。设 $I \\subset \\{1, \\dots, n\\}$ 是大小为 $b$ 的索引集，定义一个小批量。随机梯度 $\\widehat{\\nabla} L(\\mathbf{w})$ 为：\n$$ \\widehat{\\nabla} L(\\mathbf{w}) = \\frac{1}{b} \\mathbf{X}_I^T (\\mathbf{X}_I \\mathbf{w} - \\mathbf{y}_I) + \\lambda \\frac{\\mathbf{w}}{\\sqrt{\\mathbf{w}^2 + \\beta^2}} $$\n其中 $\\mathbf{X}_I$ 和 $\\mathbf{y}_I$ 分别是 $\\mathbf{X}$ 的行和 $\\mathbf{y}$ 的条目，对应于 $I$ 中的索引。梯度的正则化部分是使用完整的参数向量 $\\mathbf{w}$ 计算的，不是随机的。\n\n**2.3. 优化算法**\n\n两种算法都从 $\\mathbf{w}_0 = \\mathbf{0}$ 开始，运行 $T$ 次迭代。在每次迭代 $t=0, 1, \\dots, T-1$ 中，它们使用相同的小批量计算随机梯度 $\\mathbf{g}_t = \\widehat{\\nabla} L(\\mathbf{w}_t)$。\n\n- **随机梯度下降 (SGD)：** 更新规则是沿负梯度方向的一个简单步骤：\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{SGD}} \\cdot \\mathbf{g}_t $$\n\n- **自适应矩估计 (Adam)：** Adam 维护梯度及其平方的移动平均值。设 $\\beta_1=0.9$，$\\beta_2=0.999$ 和 $\\epsilon=10^{-8}$ 为标准超参数。\n    1.  初始化一阶矩向量 $\\mathbf{m}_0 = \\mathbf{0}$ 和二阶矩向量 $\\mathbf{v}_0 = \\mathbf{0}$。\n    2.  更新有偏矩估计：\n    $$ \\mathbf{m}_{t+1} = \\beta_1 \\mathbf{m}_t + (1 - \\beta_1) \\mathbf{g}_t $$\n    $$ \\mathbf{v}_{t+1} = \\beta_2 \\mathbf{v}_t + (1 - \\beta_2) \\mathbf{g}_t^2 \\quad \\text{(逐元素平方)} $$\n    3.  计算无偏矩估计：\n    $$ \\hat{\\mathbf{m}}_{t+1} = \\frac{\\mathbf{m}_{t+1}}{1 - \\beta_1^{t+1}} $$\n    $$ \\hat{\\mathbf{v}}_{t+1} = \\frac{\\mathbf{v}_{t+1}}{1 - \\beta_2^{t+1}} $$\n    4.  更新参数：\n    $$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{Adam}} \\frac{\\hat{\\mathbf{m}}_{t+1}}{\\sqrt{\\hat{\\mathbf{v}}_{t+1}} + \\epsilon} $$\n\n**2.4. 实验流程与评估**\n\n对于每个测试用例，执行以下流程：\n1.  **设置：** 使用特定的随机种子以确保可复现性。\n2.  **数据生成：** 按规定生成合成数据集 $(\\mathbf{X}, \\mathbf{y})$ 和真实稀疏向量 $\\mathbf{w}^\\star$。记录真实支撑集 $S^\\star$。\n3.  **迭代：** SGD 和 Adam 并行运行 $T$ 次迭代。在每次迭代 $t$ 中，使用相同的小批量为两种更新计算梯度。\n4.  **支撑集恢复跟踪：** 每次参数更新（产生 $\\mathbf{w}_{t+1}$）后，通过用 $\\theta$ 对权重的绝对值进行阈值处理来计算估计支撑集 $S_{t+1}$。将其与真实支撑集 $S^\\star$ 进行比较。如果它们匹配，则记录恢复时间（迭代次数 $t+1$）。这对两个优化器都进行。初始恢复时间设置为 $+\\infty$。\n5.  **比较：** 在 $T$ 次迭代后，根据指定的规则做出最终决定：\n    - 如果至少有一个优化器找到了支撑集，则恢复时间更短（对于 Adam 允许相等）的那个获胜。\n    - 如果都没有找到支撑集，则使用它们最终估计支撑集相对于真实支撑集的 F1 分数来比较它们的性能。F1 分数更高的优化器获胜。\n对于一个估计支撑集 $S$ 和真实支撑集 $S^\\star$，F1 分数计算如下：\n$$ \\text{F1}(S, S^\\star) = \\frac{2|S \\cap S^\\star|}{|S| + |S^\\star|} $$\n这与问题陈述中给出的公式是等价的。返回一个布尔值，指示 Adam 是否被认为相对于 SGD 加速了恢复。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (50, 200, 5, 0.05, 0.05, 1e-3, 20, 0.05, 0.01, 400, 0.05),\n        (60, 120, 8, 0.2, 0.1, 1e-2, 30, 0.03, 0.01, 600, 0.08),\n        (40, 100, 4, 0, 0.02, 1e-3, 25, 0.04, 0.01, 300, 0.04),\n        (80, 200, 10, 0.15, 0.15, 5e-2, 40, 0.02, 0.005, 800, 0.1),\n    ]\n\n    results = []\n    for i, params in enumerate(test_cases):\n        decision = solve_one_case(params, seed=i)\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_one_case(params, seed):\n    \"\"\"\n    Executes the experiment for a single set of parameters.\n    \"\"\"\n    d, n, K, sigma, lamb, beta, b, eta_sgd, eta_adam, T, theta = params\n    \n    # Use the case index as a seed for reproducibility\n    np.random.seed(seed)\n\n    # 1. Generate synthetic dataset\n    X = np.random.randn(n, d)\n    X /= np.linalg.norm(X, axis=0, keepdims=True)\n\n    w_star = np.zeros(d)\n    support_indices = np.random.choice(d, K, replace=False)\n    signs = np.random.choice([-1, 1], K)\n    magnitudes = np.random.uniform(0.5, 1.5, K)\n    w_star[support_indices] = signs * magnitudes\n    \n    S_star_set = set(support_indices)\n\n    noise = np.random.randn(n) * sigma\n    y = X @ w_star + noise\n\n    # Pre-generate mini-batch indices for all T iterations to ensure fairness\n    mini_batch_indices = [np.random.choice(n, b, replace=False) for _ in range(T)]\n\n    # Initialize parameters for both optimizers\n    w_sgd = np.zeros(d)\n    w_adam = np.zeros(d)\n    \n    # Adam-specific parameters\n    m = np.zeros(d)\n    v = np.zeros(d)\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 1e-8\n\n    t_sgd_recovery = np.inf\n    t_adam_recovery = np.inf\n\n    for t in range(T):\n        # Get the same mini-batch for both optimizers\n        batch_idx = mini_batch_indices[t]\n        X_b, y_b = X[batch_idx], y[batch_idx]\n\n        # --- SGD Step ---\n        grad_data_sgd = (1/b) * X_b.T @ (X_b @ w_sgd - y_b)\n        grad_reg_sgd = lamb * w_sgd / np.sqrt(w_sgd**2 + beta**2)\n        grad_sgd = grad_data_sgd + grad_reg_sgd\n        w_sgd -= eta_sgd * grad_sgd\n        \n        # --- Adam Step ---\n        grad_data_adam = (1/b) * X_b.T @ (X_b @ w_adam - y_b)\n        grad_reg_adam = lamb * w_adam / np.sqrt(w_adam**2 + beta**2)\n        grad_adam = grad_data_adam + grad_reg_adam\n\n        m = beta1 * m + (1 - beta1) * grad_adam\n        v = beta2 * v + (1 - beta2) * (grad_adam**2)\n        \n        # Bias correction\n        m_hat = m / (1 - beta1**(t + 1))\n        v_hat = v / (1 - beta2**(t + 1))\n        \n        w_adam -= eta_adam * m_hat / (np.sqrt(v_hat) + epsilon)\n\n        # --- Evaluation at iteration t+1 ---\n        # Check for SGD recovery\n        if np.isinf(t_sgd_recovery):\n            S_sgd_current = set(np.where(np.abs(w_sgd) >= theta)[0])\n            if S_sgd_current == S_star_set:\n                t_sgd_recovery = t + 1\n        \n        # Check for Adam recovery\n        if np.isinf(t_adam_recovery):\n            S_adam_current = set(np.where(np.abs(w_adam) >= theta)[0])\n            if S_adam_current == S_star_set:\n                t_adam_recovery = t + 1\n\n    # Final comparison logic\n    if min(t_sgd_recovery, t_adam_recovery) < np.inf:\n        return t_adam_recovery <= t_sgd_recovery\n    else:\n        # Neither recovered, compare F1 scores at the final iteration\n        S_T_sgd = set(np.where(np.abs(w_sgd) >= theta)[0])\n        S_T_adam = set(np.where(np.abs(w_adam) >= theta)[0])\n        \n        def f1_score(A, B):\n            intersection_size = len(A.intersection(B))\n            if len(A) == 0 and len(B) == 0:\n                return 1.0\n            denominator = len(A) + len(B)\n            if denominator == 0:\n                return 0.0\n            \n            return 2.0 * intersection_size / denominator\n\n        f1_sgd = f1_score(S_T_sgd, S_star_set)\n        f1_adam = f1_score(S_T_adam, S_star_set)\n        \n        return f1_adam > f1_sgd\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}