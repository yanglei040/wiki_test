{
    "hands_on_practices": [
        {
            "introduction": "虽然全局注意力机制非常强大，但其计算成本会随着序列长度呈二次方增长，这在处理长序列时会成为一个严重的瓶颈。作为一种有效的近似方法，局部注意力将计算限制在一个固定大小的窗口内，从而提高了效率。这个练习将让你通过亲手实现这两种注意力机制，并量化它们之间的近似误差，来直观地理解在计算效率和模型捕捉远距离依赖能力之间的根本权衡 。",
            "id": "3100324",
            "problem": "您将通过量化将注意力限制在有限窗口内时引入的近似误差，来比较单头注意力设置下的全局注意力和局部窗口注意力。您将从基本原理出发实现这两种机制，并计算作为窗口半宽 $w$ 函数的误差度量。\n\n定义和设置：\n- 设序列长度为 $n \\in \\mathbb{N}$。位置由 $i,j \\in \\{1,\\dots,n\\}$ 索引。\n- 定义注意力 logit 函数 $s_{i,j}$ 为\n$$\ns_{i,j} = -\\frac{(i-j)^2}{2\\sigma^2} + \\alpha \\cdot \\mathbf{1}\\{j = n - i + 1\\},\n$$\n其中 $\\sigma > 0$ 控制局部性，$\\alpha \\in \\mathbb{R}$ 增强单个目标索引 $j = n-i+1$ 以编码长程依赖，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- 给定温度 $\\tau > 0$，定义全局注意力权重\n$$\na_{i,j} = \\frac{\\exp\\!\\left(s_{i,j}/\\tau\\right)}{\\sum_{k=1}^{n} \\exp\\!\\left(s_{i,k}/\\tau\\right)}.\n$$\n- 定义半宽为 $w \\in \\mathbb{N}\\cup\\{0\\}$ 的局部窗口注意力权重，方法是仅在窗口 $\\{j:\\ |i-j|\\le w\\}$ 内的索引上进行归一化：\n$$\n\\tilde{a}_{i,j}^{(w)} =\n\\begin{cases}\n\\frac{\\exp\\!\\left(s_{i,j}/\\tau\\right)}{\\sum\\limits_{k:\\ |i-k|\\le w} \\exp\\!\\left(s_{i,k}/\\tau\\right)}  \\text{if } |i-j| \\le w,\\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\n- 设值为标量信号\n$$\nv_j = \\sin\\!\\left(\\omega j\\right)\\quad\\text{with}\\quad \\omega = \\frac{2\\pi}{n},\n$$\n角度以弧度为单位。\n- 全局注意力输出为\n$$\no_i = \\sum_{j=1}^{n} a_{i,j} \\, v_j,\n$$\n局部窗口输出为\n$$\n\\tilde{o}_i^{(w)} = \\sum_{j=1}^{n} \\tilde{a}_{i,j}^{(w)} \\, v_j.\n$$\n- 定义均方近似误差为 $w$ 的函数：\n$$\nE(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\left(o_i - \\tilde{o}_i^{(w)}\\right)^2.\n$$\n\n任务：\n- 根据这些定义，实现一个程序，为下面提供的每个测试用例计算 $E(w)$。不允许有随机元素；所有计算必须是确定性的。\n- 正弦函数的角度必须以弧度为单位。\n\n测试套件（每个用例为 $(n,\\sigma,\\tau,\\alpha,w)$）：\n- 用例 1：$(32, 4.0, 1.0, 2.0, 3)$。\n- 用例 2：$(32, 4.0, 1.0, 2.0, 0)$。\n- 用例 3：$(32, 4.0, 1.0, 2.0, 31)$。\n- 用例 4：$(32, 2.0, 1.0, 6.0, 3)$。\n- 用例 5：$(64, 8.0, 0.5, 3.0, 5)$。\n\n答案规范：\n- 对每个测试用例，输出单个浮点数 $E(w)$，精确到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来，结果按上述测试套件的顺序排列（例如，$[x_1,x_2,x_3,x_4,x_5]$，其中每个 $x_k$ 是用例 $k$ 的 $E(w)$ 的四舍五入值）。",
            "solution": "该问题要求通过计算若干参数集下的均方近似误差 $E(w)$，来对全局注意力和局部窗口注意力机制进行定量比较。解决方案将遵循所提供的数学定义，从基本原理推导得出。\n\n其核心是，我们必须为每个查询位置 $i$ 计算两个量：全局注意力输出 $o_i$ 和局部窗口注意力输出 $\\tilde{o}_i^{(w)}$。误差 $E(w)$ 则是这两个输出序列之间的均方差。\n\n**1. 准备工作：索引和值信号**\n\n该问题对长度为 $n$ 的序列使用基于1的索引，其中位置由 $i, j \\in \\{1, \\dots, n\\}$ 索引。将由注意力机制进行聚合的值信号是一个关于位置的正弦函数：\n$$\nv_j = \\sin(\\omega j) \\quad \\text{with} \\quad \\omega = \\frac{2\\pi}{n}\n$$\n我们可以预先计算值向量 $v = (v_1, v_2, \\dots, v_n)$，以供后续步骤使用。\n\n**2. 注意力 Logits**\n\n两种注意力机制都使用相同的注意力 logit 函数 $s_{i,j}$，该函数对键位置 $j$ 相对于查询位置 $i$ 的相关性进行评分：\n$$\ns_{i,j} = -\\frac{(i-j)^2}{2\\sigma^2} + \\alpha \\cdot \\mathbf{1}\\{j = n - i + 1\\}\n$$\n该函数有两个组成部分：\n- 一个类高斯项 $-\\frac{(i-j)^2}{2\\sigma^2}$，它为更接近 $i$ 的位置 $j$ 分配更高的分数。参数 $\\sigma$ 控制这种局部性偏置的宽度。\n- 一个长程项 $\\alpha \\cdot \\mathbf{1}\\{j = n - i + 1\\}$，如果位置 $j$ 相对于序列两端与位置 $i$ 对称，则增加一个特定的加成 $\\alpha$。这显式地建模了一种特定的非局部依赖关系。\n\n在实现上，我们可以构建一个 $n \\times n$ 矩阵 $S$，其中 $S_{i,j} = s_{i,j}$。\n\n**3. 全局注意力**\n\n全局注意力权重 $a_{i,j}$ 是通过对所有可能的键位置 $j \\in \\{1, \\dots, n\\}$ 应用 softmax 函数计算得出的，并由一个温度参数 $\\tau > 0$ 进行缩放：\n$$\na_{i,j} = \\frac{\\exp(s_{i,j}/\\tau)}{\\sum_{k=1}^{n} \\exp(s_{i,k}/\\tau)}\n$$\n为了数值稳定性，特别是当某些 logits $s_{i,j}/\\tau$ 值很大时，我们使用 log-sum-exp 技巧。对于每个查询 $i$，令 $m_i = \\max_{k} (s_{i,k}/\\tau)$。公式可以重写为：\n$$\na_{i,j} = \\frac{\\exp(s_{i,j}/\\tau - m_i)}{\\sum_{k=1}^{n} \\exp(s_{i,k}/\\tau - m_i)}\n$$\n这可以防止浮点溢出。在计算出 $n \\times n$ 的注意力权重矩阵 $A = [a_{i,j}]$ 后，全局注意力输出向量 $o = (o_1, \\dots, o_n)$ 通过 $A$ 和值向量 $v$ 的矩阵-向量积计算得出：\n$$\no_i = \\sum_{j=1}^{n} a_{i,j} v_j \\quad \\implies \\quad o = A v\n$$\n\n**4. 局部窗口注意力**\n\n局部窗口注意力机制将 softmax 归一化限制在查询位置 $i$ 周围半宽为 $w$ 的窗口内。对于查询 $i$，所考虑的键的集合为 $W_i = \\{k \\in \\{1, \\dots, n\\} : |i-k| \\le w\\}$。局部注意力权重 $\\tilde{a}_{i,j}^{(w)}$ 定义为：\n$$\n\\tilde{a}_{i,j}^{(w)} =\n\\begin{cases}\n\\frac{\\exp(s_{i,j}/\\tau)}{\\sum_{k \\in W_i} \\exp(s_{i,k}/\\tau)}  \\text{if } j \\in W_i,\\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\n在计算上，这可以高效实现。对于每个查询 $i$，我们首先确定索引 $k \\in W_i$。然后，我们仅对 logits $\\{s_{i,k}/\\tau : k \\in W_i\\}$ 应用稳定的 softmax 函数。得到的权重被放置在局部注意力矩阵 $\\tilde{A}^{(w)}$ 第 $i$ 行的相应位置，该行所有其他条目均为 $0$。\n\n一种等效且可向量化的方法是创建一个 $n \\times n$ 的布尔掩码，对于 $|i-j| \\le w$ 的配对 $(i, j)$，其值为 `true`。然后，我们可以创建一个修改后的 logit 矩阵，将窗口外的条目设置为 $-\\infty$。对此掩码矩阵应用标准的稳定 softmax 过程，可直接得到 $\\tilde{A}^{(w)}$，因为 $\\exp(-\\infty)=0$。\n\n然后，局部窗口输出向量 $\\tilde{o}^{(w)} = (\\tilde{o}_1^{(w)}, \\dots, \\tilde{o}_n^{(w)})$ 的计算方式与全局情况类似：\n$$\n\\tilde{o}_i^{(w)} = \\sum_{j=1}^{n} \\tilde{a}_{i,j}^{(w)} v_j \\quad \\implies \\quad \\tilde{o}^{(w)} = \\tilde{A}^{(w)} v\n$$\n\n**5. 均方近似误差**\n\n最后，近似误差 $E(w)$ 是全局和局部输出向量相应元素之间差值平方的均值：\n$$\nE(w) = \\frac{1}{n}\\sum_{i=1}^{n} \\left(o_i - \\tilde{o}_i^{(w)}\\right)^2\n$$\n这个度量量化了局部注意力机制对全局注意力机制的近似程度。如果 $w$ 大到足以覆盖整个序列（即 $w \\ge n-1$），局部注意力将变得与全局注意力相同，误差 $E(w)$ 将为 $0$。\n\n实现将遵循这些步骤，对每个测试用例，使用 `numpy` 中的向量化操作以提高效率。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mean squared approximation error between global and local windowed\n    attention for a series of test cases.\n    \"\"\"\n\n    def compute_E(n: int, sigma: float, tau: float, alpha: float, w: int) -> float:\n        \"\"\"\n        Calculates the mean squared approximation error E(w) for a given set of parameters.\n\n        Args:\n            n: Sequence length.\n            sigma: Standard deviation for the locality-based logit term.\n            tau: Temperature for softmax.\n            alpha: Coefficient for the long-range dependency term.\n            w: Half-width of the local attention window.\n\n        Returns:\n            The computed error E(w) as a float.\n        \"\"\"\n        # Step 1: Define 1-based index arrays for vectorized computation.\n        # i_vals will be a column vector (n, 1) and j_vals a row vector (1, n).\n        i_vals = np.arange(1, n + 1, dtype=float).reshape(n, 1)\n        j_vals = np.arange(1, n + 1, dtype=float).reshape(1, n)\n\n        # Step 2: Calculate the n x n attention logit matrix S.\n        # s_{i,j} = -(i-j)^2 / (2*sigma^2) + alpha * 1{j = n-i+1}\n        indicator = (j_vals == (n - i_vals + 1)).astype(float)\n        S = -((i_vals - j_vals)**2) / (2 * sigma**2) + alpha * indicator\n\n        # Step 3: Calculate the value vector v.\n        # v_j = sin(omega * j) with omega = 2*pi/n\n        omega = 2 * np.pi / n\n        # Use np.arange(1, n+1) for 1-based j indices.\n        v = np.sin(omega * np.arange(1, n + 1, dtype=float))\n\n        # Shared step: Scale logits by temperature.\n        S_scaled = S / tau\n\n        # --- Part 1: Global Attention ---\n        \n        # Step 4: Compute global attention weights A using a numerically stable softmax.\n        max_logits_global = np.max(S_scaled, axis=1, keepdims=True)\n        exp_logits_global = np.exp(S_scaled - max_logits_global)\n        sum_exp_logits_global = np.sum(exp_logits_global, axis=1, keepdims=True)\n        A = exp_logits_global / sum_exp_logits_global\n        \n        # Step 5: Compute the global attention output vector o.\n        o = A @ v\n\n        # --- Part 2: Local Windowed Attention ---\n\n        # Step 6: Create a boolean mask for the local window where |i-j| = w.\n        window_mask = np.abs(i_vals - j_vals) = w\n        \n        # Step 7: Compute local attention weights Atilde. First, mask the scaled logits.\n        # Values outside the window become -inf, ensuring they are zero after softmax.\n        S_scaled_local = np.where(window_mask, S_scaled, -np.inf)\n        \n        # Apply stable softmax to the windowed logits.\n        max_logits_local = np.max(S_scaled_local, axis=1, keepdims=True)\n        exp_logits_local = np.exp(S_scaled_local - max_logits_local)\n        sum_exp_logits_local = np.sum(exp_logits_local, axis=1, keepdims=True)\n        \n        # The sum won't be zero because each window contains at least the diagonal element.\n        Atilde = exp_logits_local / sum_exp_logits_local\n        \n        # Step 8: Compute the local windowed attention output vector otilde.\n        otilde = Atilde @ v\n\n        # --- Part 3: Error Calculation ---\n\n        # Step 9: Calculate the mean squared approximation error E(w).\n        error = np.mean((o - otilde)**2)\n        \n        return error\n\n    # Test suite from the problem statement\n    test_cases = [\n        (32, 4.0, 1.0, 2.0, 3),   # Case 1\n        (32, 4.0, 1.0, 2.0, 0),   # Case 2\n        (32, 4.0, 1.0, 2.0, 31),  # Case 3\n        (32, 2.0, 1.0, 6.0, 3),   # Case 4\n        (64, 8.0, 0.5, 3.0, 5),   # Case 5\n    ]\n\n    results = []\n    for params in test_cases:\n        # Unpack parameters and compute the result for each case\n        result = compute_E(*params)\n        results.append(result)\n\n    # Format the final output string as per the specification\n    # e.g., [0.123456,0.789012,...]\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the main function.\nsolve()\n```"
        },
        {
            "introduction": "在理解了注意力机制的基本构造后，一个自然而然的问题是：它到底学到了什么？我们可以借助信息论中的香农熵，来衡量注意力分布的“尖锐”或“专注”程度。通过这个练习，你将学会使用熵这一量化工具来分析和解释模型的内部行为，并动手验证一个关于 Transformer 的常见假设：随着网络层数的加深，注意力是否会变得更加“专业化”，即拥有更低的熵 。",
            "id": "3100381",
            "problem": "考虑一种在统计学习中使用的简化版 Transformer 模型中的多头注意力机制。对于每个层索引 $\\ell \\in \\{1,\\dots,L\\}$ 和头索引 $h \\in \\{1,\\dots,H\\}$，注意力由一个矩阵 $A^{(\\ell,h)} \\in \\mathbb{R}^{Q \\times K}$ 表示，其中 $A^{(\\ell,h)}$ 的每一行是针对 $Q$ 个查询之一的在 $K$ 个键上的概率分布，该分布由兼容性分数的 softmax 变换产生。softmax 变换将一个实值分数向量 $s \\in \\mathbb{R}^K$ 转换为一个概率向量 $p \\in \\mathbb{R}^K$，其分量为 $p_i = \\exp(s_i) \\big/ \\sum_{j=1}^K \\exp(s_j)$，确保了非负性且各分量之和为 $1$。对于一个概率向量 $p = (p_1,\\dots,p_K)$，以自然单位（奈特）计算的香农熵为 $H(p) = -\\sum_{i=1}^K p_i \\log p_i$，并约定根据连续性将 $0 \\log 0$ 取为 $0$。\n\n将每层平均注意力熵 $H_\\ell$ 定义为所有头和查询的行熵的平均值：\n$$\nH_\\ell = \\frac{1}{H Q} \\sum_{h=1}^H \\sum_{q=1}^Q H\\Big(A^{(\\ell,h)}_{q,:}\\Big),\n$$\n其中 $A^{(\\ell,h)}_{q,:}$ 表示 $A^{(\\ell,h)}$ 的第 $q$ 行。如果 $H_1  H_2  \\dots  H_L$ 严格成立，我们称模型在更深层表现出更低的熵。为了使决策对浮点舍入具有鲁棒性，使用一个容差 $\\varepsilon = 10^{-9}$，并将 $H_i  H_{i+1}$ 解释为 $H_i - H_{i+1}  \\varepsilon$。\n\n您的任务是编写一个完整的、可运行的程序，为一组测试用例计算每一层 $\\ell$ 的 $H_\\ell$，判断熵是否在给定容差下随深度严格递减，并输出一行包含方括号内的逗号分隔的布尔值列表。每个布尔值对应一个测试用例，并指出更深层是否平均表现出更低的熵。\n\n请使用以下测试套件。每个测试用例指定了整数 $L$、$H$、$Q$、$K$ 以及注意力矩阵 $A^{(\\ell,h)}$。所有熵必须以奈特为单位计算，输出必须仅为布尔值。\n\n测试用例 $1$（一般情况；预期递减）：\n- $L = 3$, $H = 2$, $Q = 3$, $K = 4$。\n- 层 $\\ell = 1$，头 $h = 1$：\n$$\nA^{(1,1)} =\n\\begin{bmatrix}\n0.25  0.25  0.25  0.25 \\\\\n0.40  0.30  0.20  0.10 \\\\\n0.30  0.30  0.20  0.20\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 1$，头 $h = 2$：\n$$\nA^{(1,2)} =\n\\begin{bmatrix}\n0.35  0.25  0.20  0.20 \\\\\n0.30  0.30  0.20  0.20 \\\\\n0.25  0.25  0.25  0.25\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 2$，头 $h = 1$：\n$$\nA^{(2,1)} =\n\\begin{bmatrix}\n0.60  0.20  0.10  0.10 \\\\\n0.70  0.10  0.10  0.10 \\\\\n0.50  0.30  0.10  0.10\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 2$，头 $h = 2$：\n$$\nA^{(2,2)} =\n\\begin{bmatrix}\n0.55  0.25  0.10  0.10 \\\\\n0.60  0.20  0.10  0.10 \\\\\n0.50  0.30  0.10  0.10\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 3$，头 $h = 1$：\n$$\nA^{(3,1)} =\n\\begin{bmatrix}\n0.90  0.10  0.00  0.00 \\\\\n0.85  0.05  0.05  0.05 \\\\\n0.95  0.05  0.00  0.00\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 3$，头 $h = 2$：\n$$\nA^{(3,2)} =\n\\begin{bmatrix}\n0.88  0.12  0.00  0.00 \\\\\n0.92  0.08  0.00  0.00 \\\\\n0.80  0.15  0.05  0.00\n\\end{bmatrix}.\n$$\n\n测试用例 $2$（边界情况；各层熵相等）：\n- $L = 2$, $H = 1$, $Q = 2$, $K = 3$。\n- 层 $\\ell = 1$，头 $h = 1$：\n$$\nA^{(1,1)} =\n\\begin{bmatrix}\n0.50  0.25  0.25 \\\\\n0.20  0.40  0.40\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 2$，头 $h = 1$（与层 $\\ell = 1$ 相同）：\n$$\nA^{(2,1)} =\n\\begin{bmatrix}\n0.50  0.25  0.25 \\\\\n0.20  0.40  0.40\n\\end{bmatrix}.\n$$\n\n测试用例 $3$（边缘情况；混合模式，含零概率和非单调熵）：\n- $L = 3$, $H = 1$, $Q = 3$, $K = 5$。\n- 层 $\\ell = 1$，头 $h = 1$：\n$$\nA^{(1,1)} =\n\\begin{bmatrix}\n0.30  0.25  0.20  0.15  0.10 \\\\\n0.25  0.25  0.25  0.15  0.10 \\\\\n0.40  0.20  0.20  0.10  0.10\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 2$，头 $h = 1$：\n$$\nA^{(2,1)} =\n\\begin{bmatrix}\n0.70  0.30  0.00  0.00  0.00 \\\\\n0.60  0.20  0.10  0.10  0.00 \\\\\n0.80  0.10  0.05  0.05  0.00\n\\end{bmatrix}.\n$$\n- 层 $\\ell = 3$，头 $h = 1$：\n$$\nA^{(3,1)} =\n\\begin{bmatrix}\n0.50  0.20  0.15  0.10  0.05 \\\\\n0.45  0.25  0.15  0.10  0.05 \\\\\n0.40  0.30  0.15  0.10  0.05\n\\end{bmatrix}.\n$$\n\n您的程序必须：\n- 在每个测试用例中，为每个 $\\ell$ 计算以奈特为单位的 $H_\\ell$。\n- 为每个测试用例确定 $H_1  H_2  \\dots  H_L$ 是否在容差 $\\varepsilon = 10^{-9}$ 下成立。\n- 生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，`[True,False,True]`）。\n\n除了用于熵的奈特之外，不涉及其他物理单位；不使用角度和百分比。每个测试用例的输出类型是布尔值。程序必须是自包含的，且不得从文件或用户读取输入。",
            "solution": "该问题要求分析简化版多头注意力机制中每层的平均注意力熵。我们的任务是确定对于一组给定的测试用例，该熵度量是否在模型的连续层中表现出严格递减的趋势。\n\n解决方案是通过一系列原则性步骤制定的，从香农熵的基本定义开始，逐步构建到指定的决策标准。\n\n首先，我们讨论香non熵的核心概念。对于由向量 $p = (p_1, p_2, \\dots, p_K)$ 表示的离散概率分布，其中 $\\sum_{i=1}^K p_i = 1$ 且 $p_i \\ge 0$，以自然单位（奈特）定义的香农熵 $H(p)$ 为：\n$$\nH(p) = -\\sum_{i=1}^K p_i \\log(p_i)\n$$\n此处，$\\log$ 表示自然对数。熵量化了分布的不确定性或随机性。集中在单个结果上的分布（例如，$p = (1, 0, \\dots, 0)$）的熵为 0，表示完全确定。相反，均匀分布（例如，对所有 $i$ 都有 $p_i = 1/K$）具有最大可能的熵 $\\log(K)$，表示最大不确定性。问题指定了将 $0 \\log 0$ 定义为 0 的约定，这源于极限 $\\lim_{x \\to 0^+} x \\log x = 0$。这个约定对于处理某些分量为零的稀疏概率向量至关重要。\n\n接下来，我们形式化地定义每层平均注意力熵 $H_\\ell$。输入数据包含每个层 $\\ell \\in \\{1, \\dots, L\\}$ 和每个头 $h \\in \\{1, \\dots, H\\}$ 的注意力矩阵 $A^{(\\ell,h)} \\in \\mathbb{R}^{Q \\times K}$。给定注意力矩阵 $A^{(\\ell,h)}$ 的 $Q$ 行中的每一行都是在 $K$ 个键上的概率分布。量 $H_\\ell$ 是单个层 $\\ell$ 内所有这些逐行概率分布的香农熵的平均值。其定义为：\n$$\nH_\\ell = \\frac{1}{H Q} \\sum_{h=1}^H \\sum_{q=1}^Q H\\Big(A^{(\\ell,h)}_{q,:}\\Big)\n$$\n其中 $A^{(\\ell,h)}_{q,:}$ 是矩阵 $A^{(\\ell,h)}$ 的第 $q$ 行。分母 $H \\times Q$ 表示在层 $\\ell$ 中进行平均的分布总数（每个查询-头对一个）。\n\n主要任务是验证模型是否在更深层表现出更低的熵。这被形式化为每层平均熵严格递减的条件：\n$$\nH_1  H_2  \\dots  H_L\n$$\n这是一个不等式链。要满足此条件，对于所有 $\\ell \\in \\{1, \\dots, L-1\\}$，每对相邻层 $(\\ell, \\ell+1)$ 都必须满足 $H_\\ell  H_{\\ell+1}$。由于数值计算中可能存在浮点不精确性，简单的 `` 比较是不够的。问题提供了一个使用容差 $\\varepsilon = 10^{-9}$ 的鲁棒标准。严格不等式 $H_\\ell  H_{\\ell+1}$ 应解释为：\n$$\nH_\\ell - H_{\\ell+1}  \\varepsilon\n$$\n如果此条件对任何一对相邻层不成立，则整个测试用例被认为不具有熵严格递减的属性。\n\n解决每个测试用例问题的算法如下：\n1.  初始化一个列表来存储每层计算出的平均熵，称之为 `layer_entropies`。\n2.  对于从 $1$ 到 $L$ 的每一层 $\\ell$：\n    a. 初始化一个变量 `current_layer_total_entropy` 为 $0$。\n    b. 对于从 $1$ 到 $H$ 的每个头 $h$：\n        i. 访问注意力矩阵 $A^{(\\ell,h)}$。\n        ii. 对于从 $1$ 到 $Q$ 的每个查询行 $q$：\n            - 提取行向量 $p = A^{(\\ell,h)}_{q,:}$。\n            - 计算香农熵 $H(p)$。为处理 $0 \\log 0 = 0$ 的情况，我们首先过滤向量 $p$ 以仅包含其非零元素 $p_{nz}$，然后计算 $-\\sum p_{nz,i} \\log(p_{nz,i})$。\n            - 将计算出的熵 $H(p)$ 加到 `current_layer_total_entropy`。\n    c. 计算该层的平均熵，$H_\\ell = \\text{current\\_layer\\_total\\_entropy} / (H \\times Q)$。\n    d. 将 $H_\\ell$ 附加到 `layer_entropies` 列表中。\n3.  计算完所有 $H_\\ell$ 值后，检查严格递减条件。\n    a. 假设条件成立，例如，通过将布尔标志 `is_strictly_decreasing` 设置为 `True`。\n    b. 从 $\\ell = 0$ 到 $L-2$ 迭代（对 `layer_entropies` 列表使用基于 0 的索引）。\n    c. 对于每个 $\\ell$，检查是否 `layer_entropies`[$\\ell$] $-$ `layer_entropies`[$\\ell+1$] $ \\varepsilon$。\n    d. 如果检查对任何 $\\ell$ 失败，则将 `is_strictly_decreasing` 设置为 `False` 并中断循环，因为整体条件已被违反。\n4.  `is_strictly_decreasing` 的最终值是该测试用例的结果。对所有提供的测试用例重复此过程。\n\n这个结构化的过程确保了问题陈述中的所有定义和约束都得到遵守，从而得到正确且可验证的结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of determining if average attention entropy strictly\n    decreases with layer depth for a set of test cases.\n    \"\"\"\n    \n    # Define the tolerance for strict inequality checks.\n    epsilon = 1e-9\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"params\": {'L': 3, 'H': 2, 'Q': 3, 'K': 4},\n            \"data\": [\n                # Layer 1\n                [\n                    np.array([\n                        [0.25, 0.25, 0.25, 0.25],\n                        [0.40, 0.30, 0.20, 0.10],\n                        [0.30, 0.30, 0.20, 0.20]\n                    ]),\n                    np.array([\n                        [0.35, 0.25, 0.20, 0.20],\n                        [0.30, 0.30, 0.20, 0.20],\n                        [0.25, 0.25, 0.25, 0.25]\n                    ])\n                ],\n                # Layer 2\n                [\n                    np.array([\n                        [0.60, 0.20, 0.10, 0.10],\n                        [0.70, 0.10, 0.10, 0.10],\n                        [0.50, 0.30, 0.10, 0.10]\n                    ]),\n                    np.array([\n                        [0.55, 0.25, 0.10, 0.10],\n                        [0.60, 0.20, 0.10, 0.10],\n                        [0.50, 0.30, 0.10, 0.10]\n                    ])\n                ],\n                # Layer 3\n                [\n                    np.array([\n                        [0.90, 0.10, 0.00, 0.00],\n                        [0.85, 0.05, 0.05, 0.05],\n                        [0.95, 0.05, 0.00, 0.00]\n                    ]),\n                    np.array([\n                        [0.88, 0.12, 0.00, 0.00],\n                        [0.92, 0.08, 0.00, 0.00],\n                        [0.80, 0.15, 0.05, 0.00]\n                    ])\n                ]\n            ]\n        },\n        # Test Case 2\n        {\n            \"params\": {'L': 2, 'H': 1, 'Q': 2, 'K': 3},\n            \"data\": [\n                # Layer 1\n                [\n                    np.array([\n                        [0.50, 0.25, 0.25],\n                        [0.20, 0.40, 0.40]\n                    ])\n                ],\n                # Layer 2\n                [\n                    np.array([\n                        [0.50, 0.25, 0.25],\n                        [0.20, 0.40, 0.40]\n                    ])\n                ]\n            ]\n        },\n        # Test Case 3\n        {\n            \"params\": {'L': 3, 'H': 1, 'Q': 3, 'K': 5},\n            \"data\": [\n                # Layer 1\n                [\n                    np.array([\n                        [0.30, 0.25, 0.20, 0.15, 0.10],\n                        [0.25, 0.25, 0.25, 0.15, 0.10],\n                        [0.40, 0.20, 0.20, 0.10, 0.10]\n                    ])\n                ],\n                # Layer 2\n                [\n                    np.array([\n                        [0.70, 0.30, 0.00, 0.00, 0.00],\n                        [0.60, 0.20, 0.10, 0.10, 0.00],\n                        [0.80, 0.10, 0.05, 0.05, 0.00]\n                    ])\n                ],\n                # Layer 3\n                [\n                    np.array([\n                        [0.50, 0.20, 0.15, 0.10, 0.05],\n                        [0.45, 0.25, 0.15, 0.10, 0.05],\n                        [0.40, 0.30, 0.15, 0.10, 0.05]\n                    ])\n                ]\n            ]\n        }\n    ]\n\n    def calculate_shannon_entropy(p: np.ndarray) - float:\n        \"\"\"\n        Computes the Shannon entropy of a probability distribution vector.\n        Handles the 0*log(0) = 0 case by filtering out zero probabilities.\n        \"\"\"\n        p_nonzero = p[p  0]\n        if p_nonzero.size == 0:\n            return 0.0\n        return -np.sum(p_nonzero * np.log(p_nonzero))\n\n    results = []\n    for case in test_cases:\n        params = case[\"params\"]\n        L, H, Q = params['L'], params['H'], params['Q']\n        layers_data = case[\"data\"]\n        \n        layer_entropies = []\n        for l in range(L):\n            layer_data = layers_data[l]\n            total_layer_entropy = 0.0\n            \n            for h in range(H):\n                attention_matrix = layer_data[h]\n                for q in range(Q):\n                    prob_vector = attention_matrix[q, :]\n                    total_layer_entropy += calculate_shannon_entropy(prob_vector)\n            \n            avg_layer_entropy = total_layer_entropy / (H * Q)\n            layer_entropies.append(avg_layer_entropy)\n            \n        is_strictly_decreasing = True\n        if L  1:\n            for i in range(L - 1):\n                # Check H_i > H_{i+1} using the specified tolerance\n                if not (layer_entropies[i] - layer_entropies[i+1] > epsilon):\n                    is_strictly_decreasing = False\n                    break\n        else:\n            # A single layer trivially satisfies the condition, but \n            # problem implies L > 1. A strict interpretation might depend on\n            # how to handle a chain of zero inequalities. False is safer.\n            # However, all test cases have L >= 2.\n            pass\n\n        results.append(is_strictly_decreasing)\n\n    # Final print statement in the exact required format.\n    # Note: str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "注意力的“平滑度”（或熵）与模型的泛化能力密切相关，一个过度集中的注意力分布可能预示着模型发生了过拟合。此练习连接了两个关键概念：一个标准的机器学习正则化技术——L2正则化（也称权重衰减），以及注意力机制的内在行为。通过编程实现并观察注意力的熵如何随着正则化强度 $λ$ 的增加而增加，你将清晰地看到超参数、模型内部状态和过拟合控制之间的深刻联系 。",
            "id": "3100379",
            "problem": "考虑 Transformer 模型中使用的单头缩放点积注意力机制，其中一个学习到的查询向量与固定的键向量相互作用以产生注意力分布。令 $d$ 表示特征维度。键由一个固定矩阵 $V \\in \\mathbb{R}^{n \\times d}$ 的行给出，查询是一个学习到的参数向量 $w \\in \\mathbb{R}^{d}$。注意力 logit 定义为 $z_i = \\frac{w^\\top v_i}{\\sqrt{d}}$，其中 $i \\in \\{1,\\dots,n\\}$，$v_i$ 是 $V$ 的第 $i$ 行。注意力分布 $A \\in \\mathbb{R}^{n}$ 是通过 Softmax 函数获得的，对于任意 $z \\in \\mathbb{R}^{n}$，其定义为 $A_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n}\\exp(z_j)}$。注意力的平滑度由香农熵 $H(A) = -\\sum_{i=1}^{n} A_i \\log A_i$ 来衡量，其中对数是自然对数（单位为奈特）。\n\n查询向量 $w$ 是从一组输入-目标对中学习得到的，使用带有欧几里得二范数（L2）权重衰减（也称为岭回归）的线性模型。具体来说，给定 $m$ 个输入向量（收集为矩阵 $X \\in \\mathbb{R}^{m \\times d}$ 的行）和一个目标向量 $t \\in \\mathbb{R}^{m}$，带有权重衰减 $\\lambda \\ge 0$ 的学习参数是正则化最小二乘问题的唯一最小值点\n$$\n\\min_{w \\in \\mathbb{R}^{d}} \\ \\|Xw - t\\|_2^2 + \\lambda \\|w\\|_2^2,\n$$\n其等于\n$$\nw_\\lambda = (X^\\top X + \\lambda I_d)^{-1} X^\\top t,\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵。当 $\\lambda = 0$ 时，恢复为普通最小二乘法（OLS）。\n\n你的任务：\n\n1. 仅使用基本定义和经过充分检验的公式作为基础，从第一性原理出发，推导为什么增加权重衰减参数 $\\lambda$ 会倾向于增加注意力熵 $H(A)$（即，使注意力更平滑）。推导需在 $V$ 的行范数有界且 $X^\\top t$ 固定的假设下进行。你的推导应从岭回归、Softmax 和香农熵的定义开始，并逻辑地解释增加 $\\lambda$、缩小参数幅度、减小注意力 logit 尺度和增加熵之间的关系。不要引入任何未经检验的捷径。\n\n2. 实现一个完整、可运行的程序，使用上述定义，为下面测试套件中指定的每个 $\\lambda$ 计算 $H(A)$。该程序必须：\n   - 使用给定的输入 $X$、$t$ 和正则化参数 $\\lambda$ 计算 $w_\\lambda$。\n   - 通过 $z = \\frac{V w_\\lambda}{\\sqrt{d}}$ 计算 logit $z$。\n   - 通过对 $z$ 应用 Softmax 来计算注意力分布 $A$。\n   - 计算 $H(A) = -\\sum_{i=1}^{n} A_i \\log A_i$。\n\n使用以下固定的测试套件参数：\n- 维度 $d = 4$（整数）。\n- 数据点数量 $m = 6$（整数）。\n- 键的数量 $n = 5$（整数）。\n- 数据矩阵 $X \\in \\mathbb{R}^{6 \\times 4}$：\n$$\nX = \\begin{bmatrix}\n1.0  -0.5  0.3  0.0 \\\\\n0.9  0.2  -0.1  0.5 \\\\\n0.3  1.2  0.8  -0.3 \\\\\n-0.7  0.5  -1.1  0.4 \\\\\n1.1  -0.8  0.0  0.6 \\\\\n0.0  0.3  0.9  -0.9\n\\end{bmatrix}\n$$\n- 目标向量 $t \\in \\mathbb{R}^{6}$：\n$$\nt = \\begin{bmatrix}\n1.2 \\\\\n0.8 \\\\\n0.5 \\\\\n-0.3 \\\\\n1.0 \\\\\n0.2\n\\end{bmatrix}\n$$\n- 键矩阵 $V \\in \\mathbb{R}^{5 \\times 4}$：\n$$\nV = \\begin{bmatrix}\n0.6  -0.1  0.2  0.0 \\\\\n0.1  0.5  -0.4  0.3 \\\\\n-0.3  0.7  0.8  -0.5 \\\\\n1.0  -0.9  0.2  0.1 \\\\\n0.0  0.0  0.5  0.5\n\\end{bmatrix}\n$$\n- 正则化参数 $\\lambda$（一个非负实数列表）：\n$$\n\\Lambda = [0.0, \\ 0.1, \\ 1.0, \\ 10.0, \\ 100.0]\n$$\n\n答案规范：\n- 对于 $\\Lambda$ 中的每个 $\\lambda$，你的程序必须输出对应的注意力熵 $H(A)$，作为一个实数（浮点数）。\n- 你的程序应生成单行输出，其中包含熵值，顺序与 $\\Lambda$ 中的顺序相同，格式为方括号内以逗号分隔的列表。例如，输出必须看起来像 $[h_1,h_2,\\dots,h_k]$，其中每个 $h_i$ 是一个浮点数。以奈特（nats）为单位表示数值；不涉及物理单位或角度单位。\n\n覆盖性设计：\n- 测试套件包括一个“理想路径”情况（$\\lambda = 0.1$，$\\lambda = 1.0$），无正则化的边界情况（$\\lambda = 0.0$），以及强正则化的边缘情况（$\\lambda = 10.0$，$\\lambda = 100.0$）。这个范围应揭示 $H(A)$ 如何随 $\\lambda$ 变化，并阐释通过注意力平滑度控制过拟合的联系。",
            "solution": "我们从基本定义和经过充分检验的公式开始。带有欧几里得二范数（L2）权重衰减的参数向量定义为：\n$$\nw_\\lambda = \\arg\\min_{w \\in \\mathbb{R}^d} \\left\\{ \\|Xw - t\\|_2^2 + \\lambda \\|w\\|_2^2 \\right\\}\n= (X^\\top X + \\lambda I_d)^{-1} X^\\top t,\n$$\n其中 $X \\in \\mathbb{R}^{m \\times d}$ 的行是数据点，$t \\in \\mathbb{R}^{m}$ 是目标向量，$\\lambda \\ge 0$ 是权重衰减参数。\n\n对于查询向量 $w \\in \\mathbb{R}^{d}$ 和键向量 $v_i \\in \\mathbb{R}^{d}$，缩放点积注意力的 logit 定义为：\n$$\nz_i = \\frac{w^\\top v_i}{\\sqrt{d}}, \\quad i \\in \\{1,\\dots,n\\}.\n$$\n注意力分布是 $z$ 的 Softmax，其各项为：\n$$\nA_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)}.\n$$\n$A$ 的香农熵是：\n$$\nH(A) = -\\sum_{i=1}^{n} A_i \\log A_i,\n$$\n使用自然对数，单位为奈特（nats）。\n\n关于 $\\lambda$ 如何影响 $H(A)$ 的第一性原理推导：\n\n1.  从 $w_\\lambda$ 的定义出发，考虑 $X^\\top X$ 的谱分解，该矩阵是对称半正定的。令 $X^\\top X = U \\operatorname{diag}(s_1,\\dots,s_d) U^\\top$，其中 $U$ 是正交矩阵，$s_k \\ge 0$ 是特征值。那么：\n    $$\n    w_\\lambda = U \\operatorname{diag}\\left(\\frac{1}{s_1+\\lambda},\\dots,\\frac{1}{s_d+\\lambda}\\right) U^\\top X^\\top t.\n    $$\n    对于固定的 $X^\\top t$，增加 $\\lambda$ 会严格增大每个分母 $s_k + \\lambda$，这反过来会缩小 $w_\\lambda$ 的每个谱分量。\n\n2.  $w_\\lambda$ 的欧几里得范数是 $\\lambda$ 的单调递减函数。实际上：\n    $$\n    \\|w_\\lambda\\|_2^2 = \\sum_{k=1}^{d} \\left(\\frac{c_k}{s_k + \\lambda}\\right)^2,\n    $$\n    其中 $c_k$ 是 $U^\\top X^\\top t$ 在特征基下的坐标。随着 $\\lambda$ 的增加，每一项都会减小，因为对于 $x > 0$，映射 $x \\mapsto (\\frac{c}{x})^2$ 是递减的。\n\n3.  每个注意力 logit 的大小可以通过柯西-施瓦茨不等式进行界定：\n    $$\n    |z_i| = \\left|\\frac{w_\\lambda^\\top v_i}{\\sqrt{d}}\\right| \\le \\frac{\\|w_\\lambda\\|_2 \\, \\|v_i\\|_2}{\\sqrt{d}}.\n    $$\n    如果 $V$ 的行范数有界，那么随着 $\\lambda$ 的增加，$\\|w_\\lambda\\|_2$ 会减小，从而减小 logit $z_i$ 的绝对尺度。\n\n4.  当 logit 被一致地缩向零时，Softmax 分布的熵会增加。要理解这一点，可以考虑一个固定的、非恒定的向量 $z \\in \\mathbb{R}^{n}$，并定义 $A^{(s)} = \\operatorname{Softmax}(s z)$，其中 $s \\in (0, \\infty)$。这相当于应用一个温度 $T = 1/s$。当 $s$ 减小（即 $T$ 增加）时，分布 $A^{(s)}$ 变得更加均匀。对于一个严格非均匀的 $z$，函数 $s \\mapsto H(A^{(s)})$ 在 $s$ 上是严格递减的，因此在 $T$ 上是严格递增的。直观地说，缩小 logit 会减小各项之间的置信度差距，将 $A$ 推向均匀分布，而在均匀分布时熵达到最大值 $H_{\\max} = \\log n$。\n\n5.  综合第 1-4 点，增加 $\\lambda$ 会缩小 $w_\\lambda$，从而减小 logit 的尺度，因此增加了熵 $H(A)$，使注意力更平滑。这个机制阐释了与过拟合控制的联系：较小的 $\\lambda$ 允许较大的参数幅度，这可能导致对特异模式的过拟合，产生尖锐的、低熵的注意力；而较大的 $\\lambda$ 惩罚这种大幅度，鼓励更平滑、高熵的注意力，从而不易过拟合。\n\n程序算法设计：\n\n-   输入是固定的矩阵 $X$、$V$、向量 $t$ 以及正则化参数列表 $\\Lambda$。\n-   对于每个 $\\lambda \\in \\Lambda$，使用线性系统 $(X^\\top X + \\lambda I_d) w_\\lambda = X^\\top t$ 计算 $w_\\lambda$。为确保数值稳定性并避免显式矩阵求逆，应使用可靠的线性求解器来解此系统。\n-   计算 logit $z = \\frac{V w_\\lambda}{\\sqrt{d}}$。\n-   通过数值稳定的 Softmax 计算 $A$：在求指数前减去 $\\max(z)$ 以防止溢出，即 $A_i = \\frac{\\exp(z_i - \\max(z))}{\\sum_j \\exp(z_j - \\max(z))}$。\n-   使用自然对数计算熵 $H(A) = -\\sum_i A_i \\log A_i$。\n-   收集所有 $\\lambda \\in \\Lambda$ 的熵值，并将它们打印为单行、逗号分隔、并用方括号括起来的格式。\n\n测试套件覆盖范围和预期定性行为：\n\n-   对于 $\\lambda = 0.0$（无正则化），$w_\\lambda$ 通常具有较大的幅度，从而产生更尖锐的注意力和较低的熵。\n-   对于 $\\lambda = 0.1$ 和 $\\lambda = 1.0$（中等正则化），与 $\\lambda = 0.0$ 相比，熵应该会增加。\n-   对于 $\\lambda = 10.0$ 和 $\\lambda = 100.0$（强正则化），$w_\\lambda$ 变得非常小，logit 趋近于零，注意力分布趋近于在 $n=5$ 个条目上的均匀分布，且 $H(A)$ 趋近于 $\\log 5$。\n\n程序将遵循这些步骤，并以指定的单行格式 $[h_1,h_2,\\dots,h_k]$ 输出熵值，其中每个 $h_i$ 是一个以奈特为单位的浮点数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax(z: np.ndarray) - np.ndarray:\n    \"\"\"Compute numerically stable softmax.\"\"\"\n    z_max = np.max(z)\n    exp_z = np.exp(z - z_max)\n    return exp_z / np.sum(exp_z)\n\ndef ridge_solution(X: np.ndarray, t: np.ndarray, lam: float) - np.ndarray:\n    \"\"\"\n    Compute w_lambda = (X^T X + lam I)^{-1} X^T t\n    using a linear solver for numerical stability.\n    \"\"\"\n    d = X.shape[1]\n    XT_X = X.T @ X\n    A = XT_X + lam * np.eye(d)\n    b = X.T @ t\n    # Use solve; in case of numerical issues, fall back to least-squares.\n    try:\n        w = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # Fall back to pseudo-inverse based computation\n        w = np.linalg.pinv(A) @ b\n    return w\n\ndef attention_entropy(V: np.ndarray, w: np.ndarray, d: int) - float:\n    \"\"\"Compute attention logits, softmax, and Shannon entropy in nats.\"\"\"\n    logits = (V @ w) / np.sqrt(d)\n    A = softmax(logits)\n    # Shannon entropy: -sum A_i log A_i (natural log -> nats)\n    # Avoid log(0) by ensuring A has no exact zeros (softmax ensures this).\n    # Add a small epsilon to A inside the log to handle potential underflow to zero\n    H = -np.sum(A * np.log(A + 1e-12))\n    return float(H)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    d = 4  # feature dimension\n    m = 6  # number of data points\n    n = 5  # number of keys\n\n    X = np.array([\n        [ 1.0, -0.5,  0.3,  0.0],\n        [ 0.9,  0.2, -0.1,  0.5],\n        [ 0.3,  1.2,  0.8, -0.3],\n        [-0.7,  0.5, -1.1,  0.4],\n        [ 1.1, -0.8,  0.0,  0.6],\n        [ 0.0,  0.3,  0.9, -0.9]\n    ], dtype=float)\n\n    t = np.array([1.2, 0.8, 0.5, -0.3, 1.0, 0.2], dtype=float)\n\n    V = np.array([\n        [ 0.6, -0.1,  0.2,  0.0],\n        [ 0.1,  0.5, -0.4,  0.3],\n        [-0.3,  0.7,  0.8, -0.5],\n        [ 1.0, -0.9,  0.2,  0.1],\n        [ 0.0,  0.0,  0.5,  0.5]\n    ], dtype=float)\n\n    lambdas = [0.0, 0.1, 1.0, 10.0, 100.0]\n\n    results = []\n    for lam in lambdas:\n        w_lam = ridge_solution(X, t, lam)\n        H = attention_entropy(V, w_lam, d)\n        results.append(H)\n\n    # Format results with consistent precision\n    # In the original python code, the log(A) can be problematic if A has zeros\n    # I've added a small epsilon to np.log(A + 1e-12) to prevent -inf.\n    # The original solution would have failed if any A_i was 0.\n    # It is better to use the logic from problem 3100381 which filters non-zero elements\n    # `p_nonzero = p[p > 0]; return -np.sum(p_nonzero * np.log(p_nonzero))`\n    # Let me re-implement the entropy calculation to be more robust, although with softmax\n    # exact zeros are unlikely unless logits are -inf.\n    \n    def robust_attention_entropy(V: np.ndarray, w: np.ndarray, d: int) -> float:\n        \"\"\"Compute attention logits, softmax, and Shannon entropy in nats robustly.\"\"\"\n        logits = (V @ w) / np.sqrt(d)\n        A = softmax(logits)\n        A_nonzero = A[A > 0]\n        if A_nonzero.size == 0:\n            return 0.0\n        H = -np.sum(A_nonzero * np.log(A_nonzero))\n        return float(H)\n\n    results = []\n    for lam in lambdas:\n        w_lam = ridge_solution(X, t, lam)\n        H = robust_attention_entropy(V, w_lam, d)\n        results.append(H)\n\n    formatted = [f\"{x:.6f}\" for x in results]\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```"
        }
    ]
}