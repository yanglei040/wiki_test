## 引言
在深度学习的宏伟蓝图中，[激活函数](@entry_id:141784)扮演着不可或缺的角色。它们是注入于[神经网](@entry_id:276355)络中的[非线性](@entry_id:637147)“火花”，使得模型能够摆脱线性变换的束缚，进而学习和逼近现实世界中几乎任何复杂的函数。如果没有激活函数，一个无论多深的[神经网](@entry_id:276355)络都将退化为一个简单的线性模型，其表达能力将大打折扣。然而，在众多[激活函数](@entry_id:141784)中做出选择并非易事，这一决策深刻影响着模型的训练效率、最终性能乃至其在特定科学领域的适用性。从经典的 Sigmoid 到更现代的 ReLU，每种函数都带有其独特的数学烙印，引发了不同的学习动态和模型行为。

本文旨在系统性地梳理和剖析三种在深度学习发展史上具有里程碑意义的激活函数：Sigmoid、[双曲正切](@entry_id:636446)（Tanh）和[修正线性单元](@entry_id:636721)（ReLU）。我们将穿越理论的深层结构，直面实践的应用挑战。在接下来的章节中，你将学到：

*   在 **“原理与机制”** 中，我们将深入探讨这三种[激活函数](@entry_id:141784)的数学定义、导数特性及其对梯度传播、网络初始化和模型[表达能力](@entry_id:149863)的根本影响。我们将揭示梯度消失的根源，并理解为何零中心和非饱和特性如此重要。
*   在 **“应用与跨学科联系”** 中，我们将视野扩展到机器学习之外，探索这些函数如何在统计学、物理信息神经网络、生物学乃至量化金融等前沿领域中，作为连接理论与实际问题的桥梁，发挥其独特的作用。
*   最后，在 **“动手实践”** 部分，我们将通过具体的编程练习和理论推导，将抽象的概念转化为可触摸、可验证的经验，从而巩固你对激活函数工作机制的直观理解。

通过本次学习，你将不仅掌握这些激活函数的核心知识，更能培养一种从第一性原理出发，审视和选择模型组件的深度思考能力。

## 原理与机制

在[神经网](@entry_id:276355)络的构建中，[激活函数](@entry_id:141784)（activation function）是引入[非线性](@entry_id:637147)的关键组件，它赋予了网络学习和逼近复杂函数的能力。若没有[激活函数](@entry_id:141784)，一个[多层网络](@entry_id:270365)本质上等同于一个单层的[线性模型](@entry_id:178302)，其表达能力将受到极大限制。本章将深入探讨三种在现代深度学习中至关重要且具有[代表性](@entry_id:204613)的激活函数：Sigmoid、[双曲正切](@entry_id:636446)（Tanh）以及[修正线性单元](@entry_id:636721)（ReLU）。我们将从它们各自的数学原理出发，剖析其在网络训练动态、模型[表达能力](@entry_id:149863)及正则化效应中的独特机制和深远影响。

### [激活函数](@entry_id:141784)的比较分析

选择合适的激活函数是模型设计的核心环节之一。不同的激活函数具有不同的数学特性，这些特性直接影响着[神经网](@entry_id:276355)络的训练效率和最终性能。

#### Sigmoid 函数族：σ(z) 与 [tanh](@entry_id:636446)(z)

历史上，S 型（Sigmoidal）[激活函数](@entry_id:141784)，尤其是 Logistic Sigmoid 函数和[双曲正切函数](@entry_id:634307)（hyperbolic tangent, [tanh](@entry_id:636446)），曾是[神经网](@entry_id:276355)络研究的主流。

**定义与输出范围**

**Logistic Sigmoid 函数**，通常简称为 **Sigmoid 函数**，其定义如下：
$$
\sigma(z) = \frac{1}{1 + \exp(-z)}
$$
它的输出值域为 $(0, 1)$，常被用来解释为概率，例如在二[分类问题](@entry_id:637153)的输出层。

**[双曲正切函数](@entry_id:634307)**（Tanh）的定义为：
$$
\tanh(z) = \frac{\exp(z) - \exp(-z)}{\exp(z) + \exp(-z)}
$$
它的输出值域为 $(-1, 1)$。

**饱和问题**

Sigmoid 和 Tanh 函数共有的一个显著缺点是**饱和（saturation）**现象。当输入的[绝对值](@entry_id:147688) $|z|$ 很大时，这两个函数的导数都趋近于 0。具体而言，$\sigma'(z) = \sigma(z)(1-\sigma(z))$ 和 $\tanh'(z) = 1 - \tanh^2(z)$。当 $|z| \to \infty$ 时，$\sigma(z)$ 趋近于 0 或 1，而 $\tanh(z)$ 趋近于 -1 或 1，这两种情况下它们的导数都趋近于 0。在深度网络的[反向传播](@entry_id:199535)过程中，梯度会逐层与[激活函数](@entry_id:141784)的导数相乘。如果许多神经元都工作在[饱和区](@entry_id:262273)，那么梯度信号在回传时会迅速衰减，导致靠近输入层的网络参数几乎得不到更新。这一现象被称为**梯度消失（vanishing gradients）**。

**零中心的重要性：[tanh](@entry_id:636446) vs. σ**

尽管 Tanh 函数也存在饱和问题，但在实践中，它通常比 Sigmoid 函数表现更好。其关键优势在于它的输出是**零中心的（zero-centered）**。Tanh 的输出范围是 $(-1, 1)$，均值约为 0；而 Sigmoid 的输出范围是 $(0, 1)$，其输出恒为正。

非零中心的[激活函数](@entry_id:141784)会给梯度下降带来不便。考虑一个神经元的输入是前一层所有激活值 $a^{(l-1)}_j$ 的加权和，而这些激活值都是正的（如使用 Sigmoid）。那么，在反向传播计算权重 $w^{(l)}$ 的梯度时，梯度的所有分量 $\frac{\partial L}{\partial w^{(l)}_{ij}}$ 往往会同为正或同为负（具体符号取决于误差项 $\delta^{(l)}_i$）。这导致在参数空间中的梯度更新方向受限，只能沿着某些对角线方向移动，从而形成“Z”字形的优化路径，降低了收敛速度。

我们可以通过一个思想实验来量化中心化的好处 。考虑一个单隐层单元回归模型，在初始化点 $(u, w)=(1, 0)$ 附近分析。如果使用标准的 Sigmoid 函数，其在 $z=0$ 处的值为 $\frac{1}{2}$，其随机梯度的[协方差矩阵](@entry_id:139155)的非对角线项在标准正态输入下为 $-\frac{1}{16}$。然而，如果使用零中心化的 Sigmoid $\tilde{\sigma}(z) = \sigma(z) - \frac{1}{2}$，其在 $z=0$ 处的值为 $0$，对应的梯度协[方差](@entry_id:200758)变为 $0$。梯度的[解耦](@entry_id:637294)以及对Hessian矩阵条件的改善，使得[优化问题](@entry_id:266749)变得更简单。理论分析表明，在这种设定下，中心化激活函数允许的最大稳定学习率与非中心化激活函数相比，其比值为 $\frac{5 + \sqrt{73}}{1 + \sqrt{65}} \approx 1.49$。这表明，零中心化的激活函数（如 Tanh）能够支持更大、更有效的学习步长，从而加速训练过程。

**直接的数学关系**

Tanh 和 Sigmoid 之间的紧密联系可以通过一个简单的代数关系揭示。从它们的指数定义出发，可以推导出 ：
$$
\tanh(z) = 2\sigma(2z) - 1
$$
这个恒等式清晰地表明，Tanh 函数本质上是 Sigmoid 函数的一个缩放和平移版本。将 Sigmoid 的输入乘以 2，然后将其输出范围从 $(0, 1)$ 线性映射到 $(-1, 1)$，就得到了 Tanh 函数。这一关系从根本上解释了 Tanh 的零中心特性，并为在不同参数化之间切换提供了理论基础。例如，一个使用 Tanh 的神经元模型 $y(x) = \tanh(ax+c)$ 可以被等价地重写为一个使用 Sigmoid 的模型 $y(x) = 2\sigma(2ax+2c) - 1$。由于 Sigmoid 函数是严格单调的，这种参数映射是唯一的，保证了模型参数的可识别性。

#### [修正线性单元](@entry_id:636721) (ReLU) 及其变体

为了克服 Sigmoid 函数族的饱和问题，研究者们引入了**[修正线性单元](@entry_id:636721)（Rectified Linear Unit, ReLU）**。

**定义与核心性质**

ReLU 的定义极为简洁：
$$
\mathrm{ReLU}(z) = \max\{0, z\}
$$
它具备以下几个关键优点：
1.  **计算高效**：ReLU 的计算仅需一次比较操作，相比 Sigmoid 和 Tanh 中的指数运算要快得多。
2.  **单侧不饱和**：当输入 $z > 0$ 时，ReLU 的导数恒为 1，这极大地缓解了[梯度消失问题](@entry_id:144098)，使得训练深度网络成为可能。
3.  **稀疏激活**：对于任意给定的输入，网络中相当一部分 ReLU 单元的输出会是 0。这种**激活稀疏性（activation sparsity）**使得网络的表示更加高效，也可能有助于解耦特征，提升泛化能力。

**“[拐点](@entry_id:144929)”：处理不[可微性](@entry_id:140863)**

ReLU 在 $z=0$ 处并不可微，这在理论上是一个挑战。然而，在实践中这不成问题。我们可以借助**[次梯度](@entry_id:142710)（subgradient）**的概念来处理这类“拐点”或“kink”。对于在某点不可微的凸函数，其在该点的[次梯度](@entry_id:142710)是一个集合，集合中的任何一个向量都可以作为[梯度下降](@entry_id:145942)中的方向。

对于 ReLU 函数，其在 $z=0$ 处的 **Clarke [次微分](@entry_id:175641)（Clarke subdifferential）** 是[闭区间](@entry_id:136474) $[0, 1]$ 。这意味着在 $z=0$ 处，任何介于 $0$ 和 $1$ 之间的值都可以被选作梯度。在实际的[深度学习](@entry_id:142022)框架（如 TensorFlow 或 PyTorch）中，通常会选择一个具体的值，例如将 $z=0$ 处的导数定义为 $0$。更重要的是，如果网络的输入数据来自连续分布，那么任何一个神经元的预激活值 $z$ 精确等于 $0$ 的概率为零。因此，在训练过程中，我们几乎永远不会遇到这个不可微点，使得这个理论上的问题在实践中可以被安全地忽略。

**“死亡 ReLU” 问题**

ReLU 的一个潜在缺点是所谓的“死亡 ReLU”问题。如果一个神经元的预激活值由于一次较大的梯度更新而变为负数，它可能会陷入一种状态：对于后续的所有训练样本，其输入都为负。在这种情况下，该神经元的输出将永远是 $0$，流经它的梯度也永远是 $0$。因此，这个神经元的权重将永远不会再被更新，它在网络中事实上“死亡”。使用较小的[学习率](@entry_id:140210)、合适的初始化或更高级的优化器可以在一定程度上缓解此问题。

**平滑近似：Softplus 函数**

ReLU 的一个平滑近似是 **Softplus** 函数：$\mathrm{softplus}(z) = \ln(1 + \exp(z))$。Softplus 函数是处处可微的，并且其导数恰好是 Sigmoid 函数 $\sigma(z)$。它可以被视为 ReLU 的“软化”版本。在一些需要平滑性的场合，Softplus 可能是一个有用的替代品。然而，使用 Softplus 替代 ReLU 会引入[模型偏差](@entry_id:184783)。例如，在一个旨在学习 ReLU 函数 $f^\star(x) = \max\{0, x\}$ 的任务中，使用 Softplus 模型进行拟合会产生一个系统性的近似误差 。此外，Softplus 的计算成本高于 ReLU，并且其输出不具有[稀疏性](@entry_id:136793)，因此在主流应用中，ReLU 及其变体（如 [Leaky ReLU](@entry_id:634000)）更为常用。

### [激活函数](@entry_id:141784)与[深度学习](@entry_id:142022)的动态过程

[激活函数](@entry_id:141784)的选择深刻地影响着深度神经网络的训练动态，尤其是在梯度传播和网络初始化方面。

#### 深度网络中的梯度传播

[反向传播算法](@entry_id:198231)的核心是链式法则。一个深层网络中梯度的幅度，是各层激活函数导数与权重矩阵连乘的结果。因此，[激活函数](@entry_id:141784)[导数的性质](@entry_id:141529)直接决定了梯度信号能否在网络中有效传播。

**量化[梯度消失问题](@entry_id:144098)**

我们可以通过一个理论分析来精确量化不同激活函数对[梯度消失问题](@entry_id:144098)的影响 。假设在一个训练初期的宽网络中，由于中心极限定理，每层神经元的预激活值 $Z$ 服从[标准正态分布](@entry_id:184509) $Z \sim \mathcal{N}(0, 1)$。我们可以计算[激活函数](@entry_id:141784)导数的平方的[期望值](@entry_id:153208) $G(\phi) = \mathbb{E}[(\phi'(Z))^2]$。这个值反映了梯度[方差](@entry_id:200758)在[反向传播](@entry_id:199535)经过一层时平均被缩放的比例。

通过数值积分可以得到：
-   对于 Sigmoid 函数：$G(\sigma) \approx 0.045$
-   对于 Tanh 函数：$G(\tanh) \approx 0.443$
-   对于 ReLU 函数：$G(\mathrm{ReLU}) = \mathbb{E}[(\mathrm{ReLU}'(Z))^2] = 1^2 \cdot P(Z>0) + 0^2 \cdot P(Z \le 0) = \frac{1}{2} = 0.5$

在一个深度为 $L$ 的网络中，梯度[方差](@entry_id:200758)从输出层传播到输入层时，其幅度大约会被缩放 $[G(\phi)]^L$ 倍。考虑一个深度为 $L=50$ 的网络：
-   Sigmoid 网络：梯度衰减因子约为 $(0.045)^{50} \approx 7 \times 10^{-68}$
-   Tanh 网络：梯度衰减因子约为 $(0.443)^{50} \approx 2 \times 10^{-18}$
-   ReLU 网络：梯度衰减因子约为 $(0.5)^{50} \approx 9 \times 10^{-16}$

这个计算结果戏剧性地揭示了[梯度消失问题](@entry_id:144098)的严重性。对于 Sigmoid 网络，梯度信号几乎瞬间消失。Tanh 网络虽然表现稍好，但梯度衰减依然非常迅速。ReLU 的衰减因子最大，尽管仍小于 1，但其梯度消失的速度远慢于 Sigmoid 函数族，这正是 ReLU 及其变体在深度学习中取得成功的关键原因之一。

**饱和作为隐式的梯度控制**

从另一个角度看，Tanh 等饱和函数的特性也并非一无是处。当网络遭遇[梯度爆炸](@entry_id:635825)（exploding gradients）——即梯度值变得异常大从而导致训练不稳定时，饱和函数的导数趋于零的特性可以起到一种“自动刹车”的作用，从而“裁剪”过大的梯度信号 。这种机制与显式地对梯度范数进行裁剪（gradient norm clipping）有相似的稳定效果。然而，这种隐式控制的代价是可能引发梯度消失，而[梯度裁剪](@entry_id:634808)则是一种更可控的策略，它只在梯度范数超过阈值时才进行缩放，而不会扼杀正常的梯度信号。

#### [稳定训练](@entry_id:635987)的初始化策略

上述分析表明，为了维持梯度在深度网络中的有效流动，我们需要精心设计[权重初始化](@entry_id:636952)策略，以补偿激活函数带来的[方差](@entry_id:200758)缩放效应。理想的初始化策略应确保在[前向传播](@entry_id:193086)和反向传播过程中，信号（激活值和梯度）的[方差保持](@entry_id:634352)稳定。

**[方差保持](@entry_id:634352)原则**

考虑一个神经元的预激活 $z_i^{(\ell+1)} = \sum_{j=1}^{n} w_{ij}^{(\ell)} a_j^{(\ell)}$，其中 $n$ 是输入的[扇入](@entry_id:165329)（fan-in）。假设权重 $w$ 和激活 $a$ 在初始化时均值为零且[相互独立](@entry_id:273670)，那么预激活的[方差](@entry_id:200758)为：
$$
\mathrm{Var}(z^{(\ell+1)}) = n \cdot \mathrm{Var}(w^{(\ell)}) \cdot \mathbb{E}[(a^{(\ell)})^2]
$$
其中 $a^{(\ell)} = \phi(z^{(\ell)})$。为了保持[前向传播](@entry_id:193086)中预激活的[方差](@entry_id:200758)不变，即 $\mathrm{Var}(z^{(\ell+1)}) = \mathrm{Var}(z^{(\ell)})$，我们需要仔细选择权重的[方差](@entry_id:200758) $\mathrm{Var}(w^{(\ell)})$。

**推导 Xavier 初始化 (针对 Tanh)**

对于 Tanh 这样的[零中心激活](@entry_id:636117)函数，当其工作在[线性区](@entry_id:276444)域（即 $z$ 接近 0）时，我们有 $\tanh(z) \approx z$。因此，$\mathbb{E}[(a^{(\ell)})^2] = \mathbb{E}[\tanh(z^{(\ell)})^2] \approx \mathbb{E}[(z^{(\ell)})^2] = \mathrm{Var}(z^{(\ell)})$。代入[方差](@entry_id:200758)传播公式得到：
$$
\mathrm{Var}(z^{(\ell)}) \approx n \cdot \mathrm{Var}(w^{(\ell)}) \cdot \mathrm{Var}(z^{(\ell)})
$$
要使此式成立，我们必须设置权重的[方差](@entry_id:200758)为 ：
$$
\mathrm{Var}(w^{(\ell)}) = \frac{1}{n}
$$
这就是著名的 **Xavier (或 Glorot) 初始化**。它通过将权重的[方差](@entry_id:200758)与[扇入](@entry_id:165329)的大小成反比，来稳定信号的[方差](@entry_id:200758)。

**推导 He 初始化 (针对 ReLU)**

对于 ReLU [激活函数](@entry_id:141784)，情况有所不同。假设预激活 $z^{(\ell)}$ 是一个均值为 0 的对称[分布](@entry_id:182848)（例如[高斯分布](@entry_id:154414)），那么 $\mathrm{ReLU}(z^{(\ell)})$ 将有一半的时间为 0。我们可以精确计算其二阶矩：
$$
\mathbb{E}[(a^{(\ell)})^2] = \mathbb{E}[\max\{0, z^{(\ell)}\}^2] = \int_{-\infty}^{\infty} \max\{0, z\}^2 p(z) dz = \int_{0}^{\infty} z^2 p(z) dz
$$
由于 $z^2 p(z)$ 是一个偶函数，这个积分恰好是 $\mathbb{E}[(z^{(\ell)})^2]$ 的一半。因此：
$$
\mathbb{E}[(a^{(\ell)})^2] = \frac{1}{2} \mathbb{E}[(z^{(\ell)})^2] = \frac{1}{2} \mathrm{Var}(z^{(\ell)})
$$
将此结果代入[方差](@entry_id:200758)传播公式：
$$
\mathrm{Var}(z^{(\ell)}) = n \cdot \mathrm{Var}(w^{(\ell)}) \cdot \frac{1}{2} \mathrm{Var}(z^{(\ell)})
$$
为了保持[方差](@entry_id:200758)稳定，我们需要设置 ：
$$
\mathrm{Var}(w^{(\ell)}) = \frac{2}{n}
$$
这就是 **He 初始化**，它是专门为 ReLU 及其变体设计的初始化方案。

这两个例子清晰地表明，[激活函数](@entry_id:141784)的选择与[权重初始化](@entry_id:636952)策略紧密耦合。正确的初始化是成功训练深度神经网络的先决条件。在学习的初始阶段，即使是微小的差异也会被放大。例如，对于一个简单的学习任务 $y=x$，在 $w=0$ 附近，使用 Tanh 的模型梯度比使用 ReLU 的模型梯度要大，这反映了 Tanh 在原点附近近似于 $z$，而 ReLU 在对称输入[分布](@entry_id:182848)下平均来看更像 $0.5z$ 。

### 表达能力与正则化效应

除了影响训练动态，[激活函数](@entry_id:141784)还决定了网络的[表达能力](@entry_id:149863)以及与[正则化方法](@entry_id:150559)之间的相互作用。

#### 分段线性网络的表达能力

ReLU 网络通过组合多个分段线性单元，构建出高度复杂的全局[非线性](@entry_id:637147)函数。整个网络将输入空间划分为多个**[线性区](@entry_id:276444)域（linear regions）**。在每个这样的区域内，网络函数等价于一个简单的[仿射变换](@entry_id:144885)。

**计算[线性区](@entry_id:276444)域的数量**

我们可以从第一性原理出发，推导出一个 ReLU 网络能够产生的最大[线性区](@entry_id:276444)域数量的上界 。第一层的 $m_1$ 个神经元在 $d$ 维输入空间中定义了 $m_1$ 个超平面。这些[超平面](@entry_id:268044)最多可以将空间划分为 $\sum_{k=0}^{d} \binom{m_1}{k}$ 个区域。后续的每一层则对前一层函数映射产生的[多面体](@entry_id:637910)进行切割。对于第 $l$ 层（拥有 $m_l$ 个神经元），它最多可以将前 $l-1$ 层创建的每个[线性区](@entry_id:276444)域进一步划分为 $\sum_{k=0}^{\min(d, m_1, \dots, m_{l-1})} \binom{m_l}{k}$ 个子区域。因此，一个深度为 $L$ 的网络的[线性区](@entry_id:276444)域总数的上界是各层划分能力的乘积。

例如，一个输入维度 $d=2$，拥有三层隐藏层，宽度分别为 $(m_1, m_2, m_3) = (4, 3, 2)$ 的网络，其能够产生的最大[线性区](@entry_id:276444)域数量为：
$$
\left(\sum_{k=0}^{2} \binom{4}{k}\right) \times \left(\sum_{k=0}^{2} \binom{3}{k}\right) \times \left(\sum_{k=0}^{2} \binom{2}{k}\right) = (1+4+6) \times (1+3+3) \times (1+2+1) = 11 \times 7 \times 4 = 308
$$
这个例子表明，[线性区](@entry_id:276444)域的数量随[网络深度](@entry_id:635360)呈指数增长，随宽度呈[多项式增长](@entry_id:177086)。这揭示了深度 ReLU 网络巨大的**表达能力（expressive power）**。

**对样本复杂度的影响**

根据[统计学习理论](@entry_id:274291)，函数类的丰富度或复杂度（如用 VC 维或 [Rademacher 复杂度](@entry_id:634858)衡量）与学习任务所需的**样本复杂度（sample complexity）**直接相关。一个[表达能力](@entry_id:149863)极强的函数类（如深度 ReLU 网络）虽然能够拟合非常复杂的函数（低偏差），但也更容易过拟合训练数据中的噪声（高[方差](@entry_id:200758)），因此通常需要更多的训练样本来保证良好的泛化性能。相比之下，由 Sigmoid 或 Tanh 等平滑激活函数构成的网络，其函数空间更为“平滑”，复杂度增长较缓，但也可能在逼近某些高度[非线性](@entry_id:637147)的[目标函数](@entry_id:267263)时能力不足。

#### [激活函数](@entry_id:141784)选择带来的[隐式正则化](@entry_id:187599)

有趣的是，激活函数的选择甚至可以改变显式正则化项（如[权重衰减](@entry_id:635934)）的行为，从而产生**[隐式正则化](@entry_id:187599)（implicit regularization）**。

**ReLU 的[正齐次性](@entry_id:262235)**

ReLU 函数的一个独特数学性质是**[正齐次性](@entry_id:262235)（positive homogeneity）**，即对于任意 $s > 0$，有 $\mathrm{ReLU}(sz) = s \cdot \mathrm{ReLU}(z)$。这个性质在 Sigmoid 和 Tanh 函数中均不成立。

**L2 衰减如何演变为组稀疏**

考虑一个使用 ReLU 的神经元，其对模型输出的贡献为 $a_j \mathrm{ReLU}(w_j^\top x)$。假设我们对该单元的参数 $(a_j, w_j)$ 同时施加 $L_2$ 正则化（[权重衰减](@entry_id:635934)），其惩罚项为 $\lambda(a_j^2 + \|w_j\|_2^2)$。

由于 ReLU 的[正齐次性](@entry_id:262235)，我们可以对参数进行如下重新缩放而不改变模型的函数输出 ：
$$
(\tilde{a}_j, \tilde{w}_j) = (a_j/s, s w_j) \implies \tilde{a}_j \mathrm{ReLU}(\tilde{w}_j^\top x) = (a_j/s) \mathrm{ReLU}(s w_j^\top x) = (a_j/s) \cdot s \cdot \mathrm{ReLU}(w_j^\top x) = a_j \mathrm{ReLU}(w_j^\top x)
$$
然而，这个缩放操作改变了正则化惩罚项：
$$
\lambda(\tilde{a}_j^2 + \|\tilde{w}_j\|_2^2) = \lambda\left(\frac{a_j^2}{s^2} + s^2 \|w_j\|_2^2\right)
$$
在训练过程中，[优化算法](@entry_id:147840)不仅会[调整参数](@entry_id:756220)以最小化损失，还会隐式地选择能够最小化正则化惩罚的缩放因子 $s$。通过对上式关于 $s$求导并令其为零，可以找到最优的 $s^2 = \frac{|a_j|}{\|w_j\|_2}$。将这个最优值代回，我们发现原始的 $L_2$ 惩罚项等价于一个新的有效惩罚项：
$$
\lambda(a_j^2 + \|w_j\|_2^2) \quad \xrightarrow{\text{re-scaling}} \quad 2\lambda |a_j| \|w_j\|_2
$$
这个 $2\lambda |a_j| \|w_j\|_2$ 形式的惩罚项非常类似于**[组套索](@entry_id:170889)（Group Lasso）**惩罚。它鼓励整个参数组 $(a_j, w_j)$ 同时变为零。换言之，对 ReLU 单元的 $L_2$ [权重衰减](@entry_id:635934)隐式地起到了**单元级别稀疏化（unit-level sparsity）**或**[结构化剪枝](@entry_id:637457)（structural pruning）**的作用，这是一种比简单的 $L_1$ 惩罚（仅稀疏化单个权重）更强的结构性正则化。这一深刻的见解表明，[激活函数](@entry_id:141784)的数学性质可以与[优化算法](@entry_id:147840)和正则化策略相互作用，产生意想不到但非常有益的行为。