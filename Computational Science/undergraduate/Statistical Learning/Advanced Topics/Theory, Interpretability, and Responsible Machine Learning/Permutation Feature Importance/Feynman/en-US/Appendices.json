{
    "hands_on_practices": [
        {
            "introduction": "Data leakage is a critical pitfall in predictive modeling, where information from the target variable inadvertently contaminates the features, leading to overly optimistic performance estimates. Permutation Feature Importance (PFI) offers a robust, model-agnostic method to diagnose this problem by quantifying the impact of each feature on model error. This practice  guides you through implementing PFI from first principles to detect a synthetically generated \"leaky\" feature, giving you a practical tool for identifying suspiciously powerful predictors in your own models.",
            "id": "3156657",
            "problem": "You must write a complete, runnable program that constructs synthetic datasets with and without label leakage and uses Permutation Feature Importance (PFI) to detect suspicious features. The program must implement all computations from first principles, starting from empirical risk under the squared loss, and must not rely on any external machine learning libraries. The program will execute a small test suite of parameter settings and output a single line collecting the detection results.\n\nContext and fundamental base: begin from empirical risk minimization with squared loss. For a feature matrix $X \\in \\mathbb{R}^{n \\times d}$ and target vector $y \\in \\mathbb{R}^{n}$, the squared loss on a dataset is the empirical risk defined by the Mean Squared Error (MSE), which for predictions $\\hat{y}$ is $ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $. Fit a linear predictor by minimizing empirical squared loss via Ordinary Least Squares (OLS) or a numerically stable variant with a small $\\ell_2$ regularization term. Estimate PFI for a feature by quantifying the change in empirical risk when that single feature is randomly permuted while keeping the model fixed.\n\nData generation: for each test case, generate a training set with $n_{\\text{train}} = 800$ and a test set with $n_{\\text{test}} = 200$. Draw base features $X_1, X_2, X_3$ independently from a standard normal distribution $ \\mathcal{N}(0,1) $ for both training and test sets. Generate the target as $ y = 3 X_1 - 2 X_2 + \\epsilon $ with independent noise $ \\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) $ where $ \\sigma_\\epsilon = 0.5 $. Define the leakage feature $ X_{\\text{leak}} = y + \\delta $ where $ \\delta \\sim \\mathcal{N}(0, \\sigma_\\delta^2) $ and $ \\sigma_\\delta = \\gamma \\cdot \\operatorname{std}(y_{\\text{train}}) $. Here $ \\gamma \\ge 0 $ is a test-case parameter controlling the leakage strength. In the “no leak” condition, instead of $ X_{\\text{leak}} = y + \\delta $, construct $ X_{\\text{leak}} $ as an independent normal feature with zero mean and standard deviation equal to $ \\operatorname{std}(y_{\\text{train}}) $, independent of $ y $. Form the full feature matrix with columns $ [X_1, X_2, X_3, X_{\\text{leak}}] $.\n\nModel fitting: fit a linear model $ \\hat{y} = X w $ by minimizing empirical squared loss on the training set. Use a small $\\ell_2$ regularization (ridge) with coefficient $ \\lambda = 10^{-6} $ to ensure numerical stability. Compute the baseline MSE on the test set.\n\nPFI estimation: for each feature index $ j \\in \\{0,1,2,3\\} $, estimate its PFI by repeatedly permuting only column $ j $ of the test feature matrix while leaving the trained model unchanged, recomputing the test MSE, and averaging the increase in MSE relative to the baseline. Use $ B = 30 $ independent permutations per feature for averaging. All randomness must be controlled by a fixed seed in your program to ensure reproducibility.\n\nSuspicious feature flag: for the leakage detection, define two thresholds. Let $ \\Delta_j $ denote the estimated PFI for feature $ j $, and let $ \\Delta_{\\text{leak}} $ be the PFI of the leakage feature. Let $ \\Delta_{\\max,\\neg\\text{leak}} $ be the maximum PFI over all non-leak features. Declare the leakage feature suspicious if both conditions are met:\n- Absolute-risk criterion: $ \\Delta_{\\text{leak}} \\ge \\tau \\cdot \\text{MSE}_{\\text{baseline}} $,\n- Relative-dominance criterion: $ \\Delta_{\\text{leak}} \\ge \\alpha \\cdot \\Delta_{\\max,\\neg\\text{leak}} $.\nHere $ \\tau > 0 $ and $ \\alpha > 1 $ are test-case parameters.\n\nTest suite: run the program for the following four parameter sets, where each set is a tuple $ (\\gamma, \\text{has\\_leak}, \\tau, \\alpha) $:\n- Case $1$: $ (\\gamma = 0.0, \\text{has\\_leak} = 1, \\tau = 1.0, \\alpha = 2.0) $.\n- Case $2$: $ (\\gamma = 0.2, \\text{has\\_leak} = 1, \\tau = 1.0, \\alpha = 2.0) $.\n- Case $3$: $ (\\gamma = 4.0, \\text{has\\_leak} = 1, \\tau = 1.0, \\alpha = 2.0) $.\n- Case $4$: $ (\\gamma = 0.0, \\text{has\\_leak} = 0, \\tau = 1.0, \\alpha = 2.0) $.\n\nFor each case, output a boolean indicating whether the leakage feature is flagged as suspicious. Use a single fixed random seed across the entire program for reproducibility.\n\nFinal output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite cases (for example, $[r_1,r_2,r_3,r_4]$ where each $ r_i $ is a boolean). There must be no additional output or whitespace beyond this single line.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in statistical learning theory, well-posed with a complete and consistent set of definitions, and objective in its formulation. We will proceed to construct a solution.\n\nThe objective is to implement a procedure for detecting data leakage using Permutation Feature Importance (PFI) from fundamental principles. This involves data synthesis, model fitting, and importance estimation, culminating in a decision rule to flag suspicious features.\n\n### 1. Data Generation\n\nWe begin by synthesizing the datasets. For each test case, we generate a training set of size $n_{\\text{train}} = 800$ and a test set of size $n_{\\text{test}} = 200$. The process is as follows:\n\n- **Base Features**: Three independent base features, $X_1$, $X_2$, and $X_3$, are drawn from a standard normal distribution, $\\mathcal{N}(0,1)$. We generate $n_{\\text{train}} + n_{\\text{test}} = 1000$ samples for each and then partition them into training and testing sets. Let the base feature matrices be $X_{\\text{base, train}} \\in \\mathbb{R}^{800 \\times 3}$ and $X_{\\text{base, test}} \\in \\mathbb{R}^{200 \\times 3}$.\n\n- **Target Variable**: The target variable $y$ is a linear combination of the first two base features with added Gaussian noise. The relationship is given by:\n$$ y = 3 X_1 - 2 X_2 + \\epsilon $$\nwhere the noise term $\\epsilon$ is drawn from $\\mathcal{N}(0, \\sigma_\\epsilon^2)$ with $\\sigma_\\epsilon = 0.5$. The target vectors $y_{\\text{train}}$ and $y_{\\text{test}}$ are generated using the corresponding features and noise samples.\n\n- **Leakage Feature**: A fourth feature, $X_{\\text{leak}}$, is constructed under two different conditions specified by the `has_leak` parameter:\n    1.  **With Leakage (`has_leak = 1`)**: The feature is a direct, noisy copy of the target variable:\n        $$ X_{\\text{leak}} = y + \\delta $$\n        The noise $\\delta$ is drawn from a normal distribution $\\mathcal{N}(0, \\sigma_\\delta^2)$, where the standard deviation $\\sigma_\\delta$ is a function of the target variable's standard deviation in the training set and a leakage strength parameter $\\gamma$:\n        $$ \\sigma_\\delta = \\gamma \\cdot \\operatorname{std}(y_{\\text{train}}) $$\n        A smaller $\\gamma$ implies stronger leakage. $\\gamma=0$ represents perfect leakage where $X_{\\text{leak}} = y$.\n    2.  **Without Leakage (`has_leak = 0`)**: The feature is generated independently of the target $y$. It is drawn from a normal distribution with a mean of $0$ and a standard deviation equal to that of the training target variable:\n        $$ X_{\\text{leak}} \\sim \\mathcal{N}(0, (\\operatorname{std}(y_{\\text{train}}))^2) $$\n        This design ensures that the marginal distribution of $X_{\\text{leak}}$ has a similar scale to the target, making it a plausible but non-leaky feature.\n\n- **Final Feature Matrices**: The full feature matrices, $X_{\\text{train}}$ and $X_{\\text{test}}$, are formed by concatenating the base features with the leakage feature, resulting in matrices with four columns: $[X_1, X_2, X_3, X_{\\text{leak}}]$.\n\n### 2. Model Fitting: Ridge Regression\n\nA linear model, $\\hat{y} = Xw$, is fit to the training data. The weight vector $w \\in \\mathbb{R}^4$ is determined by minimizing the empirical squared loss with a small $\\ell_2$ regularization term (ridge regression) to ensure numerical stability. The objective function $J(w)$ on the training data is:\n$$ J(w) = \\frac{1}{n_{\\text{train}}} \\| y_{\\text{train}} - X_{\\text{train}}w \\|_2^2 + \\lambda \\|w\\|_2^2 $$\nwhere $\\lambda = 10^{-6}$ is the regularization coefficient. The closed-form solution for the optimal weight vector $w$ is found by setting the gradient $\\nabla_w J(w)$ to zero:\n$$ \\nabla_w J(w) = \\frac{2}{n_{\\text{train}}} X_{\\text{train}}^T (X_{\\text{train}}w - y_{\\text{train}}) + 2\\lambda w = 0 $$\n$$ (X_{\\text{train}}^T X_{\\text{train}} + n_{\\text{train}}\\lambda I)w = X_{\\text{train}}^T y_{\\text{train}} $$\nThis yields the solution:\n$$ w = (X_{\\text{train}}^T X_{\\text{train}} + n_{\\text{train}}\\lambda I)^{-1} X_{\\text{train}}^T y_{\\text{train}} $$\nwhere $I$ is the $4 \\times 4$ identity matrix.\n\n### 3. Permutation Feature Importance (PFI)\n\nPFI measures the importance of a feature by quantifying the drop in model performance when the feature's values are randomly shuffled. This shuffling breaks the relationship between the feature and the target variable.\n\n- **Baseline Score**: First, we compute the baseline Mean Squared Error (MSE) of the trained model on the unaltered test set:\n$$ \\text{MSE}_{\\text{baseline}} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{test},i} - (X_{\\text{test}}w)_i)^2 $$\n\n- **Permuted Score**: For each feature $j \\in \\{0, 1, 2, 3\\}$, we perform the following procedure $B=30$ times:\n    1.  Create a permuted test matrix $X_{\\text{test}}^{(j,b)}$ by randomly shuffling the $j$-th column of $X_{\\text{test}}$.\n    2.  Calculate the MSE with this permuted matrix: $\\text{MSE}^{(j,b)} = \\frac{1}{n_{\\text{test}}} \\| y_{\\text{test}} - X_{\\text{test}}^{(j,b)}w \\|_2^2$.\n\n- **PFI Calculation**: The PFI for feature $j$, denoted $\\Delta_j$, is the average increase in MSE across all permutations, relative to the baseline MSE:\n$$ \\Delta_j = \\left( \\frac{1}{B} \\sum_{b=1}^{B} \\text{MSE}^{(j,b)} \\right) - \\text{MSE}_{\\text{baseline}} $$\nA large positive $\\Delta_j$ indicates that the model relies heavily on feature $j$ for its predictions.\n\n### 4. Leakage Detection\n\nThe core of the task is to use the calculated PFI scores to flag the leakage feature ($j=3$) as suspicious. A feature is deemed suspicious if it is not only important but disproportionately so compared to other informative features. This is formalized through two criteria:\n\n-   **Absolute-Risk Criterion**: The PFI of the leakage feature, $\\Delta_{\\text{leak}} = \\Delta_3$, must exceed a threshold proportional to the baseline model error. This ensures the feature's impact is significant in absolute terms.\n    $$ \\Delta_{\\text{leak}} \\ge \\tau \\cdot \\text{MSE}_{\\text{baseline}} $$\n-   **Relative-Dominance Criterion**: The PFI of the leakage feature must be substantially larger than the maximum PFI of any non-leakage feature, $\\Delta_{\\max,\\neg\\text{leak}} = \\max(\\Delta_0, \\Delta_1, \\Delta_2)$. This identifies features that are suspiciously dominant.\n    $$ \\Delta_{\\text{leak}} \\ge \\alpha \\cdot \\Delta_{\\max,\\neg\\text{leak}} $$\n\nThe leakage feature is flagged if and only if both conditions are met. The parameters $\\tau$ and $\\alpha$ control the sensitivity of the detection logic and are provided for each test case. This dual-threshold approach helps distinguish truly leaky features from simply strong, legitimate predictors.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs synthetic datasets, fits a linear model, and uses Permutation\n    Feature Importance (PFI) to detect a suspicious leakage feature based on\n    pre-defined criteria. The implementation is from first principles.\n    \"\"\"\n    \n    # Set a single fixed random seed for reproducibility across the entire program.\n    np.random.seed(42)\n    rng = np.random.default_rng(42)\n\n    # Test suite parameters: (gamma, has_leak, tau, alpha)\n    test_cases = [\n        (0.0, 1, 1.0, 2.0),\n        (0.2, 1, 1.0, 2.0),\n        (4.0, 1, 1.0, 2.0),\n        (0.0, 0, 1.0, 2.0),\n    ]\n\n    results = []\n\n    for case_params in test_cases:\n        gamma, has_leak, tau, alpha = case_params\n\n        # --- 1. Data Generation ---\n        n_train = 800\n        n_test = 200\n        n_total = n_train + n_test\n        sigma_eps = 0.5\n\n        # Base features X1, X2, X3 from N(0,1)\n        X_base_all = rng.standard_normal(size=(n_total, 3))\n        X_base_train = X_base_all[:n_train, :]\n        X_base_test = X_base_all[n_train:, :]\n\n        # Target variable y = 3*X1 - 2*X2 + noise\n        epsilon = rng.normal(loc=0.0, scale=sigma_eps, size=n_total)\n        y_all = 3 * X_base_all[:, 0] - 2 * X_base_all[:, 1] + epsilon\n        y_train = y_all[:n_train]\n        y_test = y_all[n_train:]\n        \n        y_train_std = np.std(y_train)\n\n        # Leakage feature X_leak\n        if has_leak:\n            # X_leak = y + delta\n            sigma_delta = gamma * y_train_std\n            delta = rng.normal(loc=0.0, scale=sigma_delta, size=n_total)\n            X_leak_all = y_all + delta\n        else:\n            # X_leak is independent N(0, std(y_train)^2)\n            X_leak_all = rng.normal(loc=0.0, scale=y_train_std, size=n_total)\n        \n        X_leak_train = X_leak_all[:n_train].reshape(-1, 1)\n        X_leak_test = X_leak_all[n_train:].reshape(-1, 1)\n\n        # Final feature matrices\n        X_train = np.hstack([X_base_train, X_leak_train])\n        X_test = np.hstack([X_base_test, X_leak_test])\n        \n        # --- 2. Model Fitting (Ridge Regression) ---\n        lambda_reg = 1e-6\n        n_features = X_train.shape[1]\n        \n        XTX = X_train.T @ X_train\n        I = np.identity(n_features)\n        # w = (X'X + n_train * lambda * I)^-1 * X'y\n        weights = np.linalg.inv(XTX + n_train * lambda_reg * I) @ X_train.T @ y_train\n\n        # --- 3. PFI Estimation ---\n        B = 30  # Number of permutations\n        \n        # Baseline MSE on test set\n        y_pred_baseline = X_test @ weights\n        mse_baseline = np.mean((y_test - y_pred_baseline)**2)\n        \n        pfi_scores = []\n        for j in range(n_features):\n            permuted_mses = []\n            for _ in range(B):\n                X_test_permuted = X_test.copy()\n                col_to_permute = X_test_permuted[:, j].copy()\n                rng.shuffle(col_to_permute)\n                X_test_permuted[:, j] = col_to_permute\n                \n                y_pred_permuted = X_test_permuted @ weights\n                mse_permuted = np.mean((y_test - y_pred_permuted)**2)\n                permuted_mses.append(mse_permuted)\n            \n            mean_permuted_mse = np.mean(permuted_mses)\n            pfi_j = mean_permuted_mse - mse_baseline\n            pfi_scores.append(pfi_j)\n\n        # --- 4. Leakage Detection ---\n        pfi_leak_feature = pfi_scores[3]\n        # Handle case where all non-leak PFIs are negative\n        pfi_non_leak_scores = pfi_scores[:3]\n        pfi_max_non_leak = np.max(pfi_non_leak_scores) if pfi_non_leak_scores else 0.0\n\n        # Absolute-risk criterion\n        abs_risk_cond = (pfi_leak_feature >= tau * mse_baseline)\n        \n        # Relative-dominance criterion\n        rel_dom_cond = (pfi_leak_feature >= alpha * pfi_max_non_leak)\n        \n        is_suspicious = abs_risk_cond and rel_dom_cond\n        results.append(str(is_suspicious))\n\n    # Final output must be a single line in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The importance score assigned by PFI is not an intrinsic property of a feature but is fundamentally tied to the performance metric used for evaluation. This exercise  explores the crucial interplay between feature importance, the choice of evaluation metric—such as AUC, log-loss, or F1-score—and the common challenge of class imbalance. By observing how feature rankings change as class distributions become more skewed, you will develop a more nuanced understanding of how analytical context shapes the interpretation of feature importance.",
            "id": "3156641",
            "problem": "You must write a complete, runnable program that empirically examines how permutation feature importance responds to class imbalance in binary classification. The investigation must be conducted by training a probabilistic classifier on synthetic data with skewed class prior $p(y)$ and then evaluating permutation feature importance under three distinct metrics: Area Under the Receiver Operating Characteristic Curve (AUC), log-loss, and F1 score.\n\nStart from the following fundamental definitions.\n\n- Binary classification setup: Let $(\\mathbf{x}_i, y_i)$ for $i \\in \\{1,\\dots,n\\}$ be independent and identically distributed samples with features $\\mathbf{x}_i \\in \\mathbb{R}^d$ and binary label $y_i \\in \\{0,1\\}$, drawn from a joint distribution with class prior $p(y=1) = \\pi \\in (0,1)$.\n- Logistic regression model: A probabilistic classifier with parameters $\\mathbf{w} \\in \\mathbb{R}^{d+1}$ maps an augmented feature vector $\\tilde{\\mathbf{x}} = [1, \\mathbf{x}^\\top]^\\top$ to a predicted probability $\\hat{p}(y=1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\tilde{\\mathbf{x}})$, where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the logistic sigmoid. The regularized empirical risk with $\\ell_2$ penalty is the average negative log-likelihood plus a penalty term on the non-intercept coefficients.\n- Area Under the Receiver Operating Characteristic Curve (AUC): The AUC is the probability that a randomly chosen positive instance receives a higher score than a randomly chosen negative instance; it is a scalar in $[0,1]$.\n- Log-loss: The log-loss (cross-entropy) is the empirical mean of the negative log-likelihood, $-\\frac{1}{n}\\sum_{i=1}^n \\left[y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)\\right]$.\n- F1 score: The F1 score is the harmonic mean of precision and recall. Using a fixed threshold at $0.5$, define predicted labels $\\hat{y}_i = \\mathbb{I}[\\hat{p}_i \\ge 0.5]$, then precision is $\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$ and recall is $\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$, and F1 is $\\frac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$ with the convention that divisions by $0$ yield $0$.\n- Permutation Feature Importance (PFI): Given a trained model and a performance metric,\n  - For metrics to be maximized (such as AUC and F1), define importance for feature $j$ as $\\Delta_j = S(\\text{baseline}) - S(\\text{permuted } j)$, where $S$ denotes the metric computed on the unpermuted or permuted test set and model parameters are held fixed.\n  - For metrics to be minimized (such as log-loss), define importance as $\\Delta_j = L(\\text{permuted } j) - L(\\text{baseline})$, where $L$ denotes the loss. In both cases, larger $\\Delta_j$ indicates higher importance.\n\nData-generating process. Your program must generate synthetic data in $d = 3$ dimensions with conditional Gaussian features and a controllable class prior. For each sample, draw $y \\sim \\mathrm{Bernoulli}(\\pi)$, and then conditionally on $y$ draw $\\mathbf{x} \\mid y \\sim \\mathcal{N}(\\boldsymbol{\\mu}_y, \\mathbf{I}_3)$ with means\n- $\\boldsymbol{\\mu}_1 = [2.0,\\, 0.8,\\, 0.0]^\\top$ and\n- $\\boldsymbol{\\mu}_0 = [0.0,\\, 0.0,\\, 0.0]^\\top$,\nand identity covariance $\\mathbf{I}_3$. Use independent draws for training and testing.\n\nTraining. Fit a logistic regression model with an intercept term by minimizing the average negative log-likelihood plus an $\\ell_2$ penalty on the non-intercept coefficients with regularization strength $\\lambda = 1.0$. Use a numerically stable optimizer that converges reliably.\n\nEvaluation and permutation. For each test case, compute baseline AUC, log-loss, and F1 on the test set. Then compute PFI for each of the $3$ features under each metric by independently permuting the corresponding test feature column $K$ times and averaging the resulting changes, with $K = 5$. Use a fixed random seed per test case to ensure reproducibility.\n\nComparison across metrics. To summarize sensitivity with respect to class imbalance and metric choice, do both of the following for each test case:\n- Determine the index (zero-based) of the most important feature under each metric, that is, $\\arg\\max_j \\Delta_j$ for AUC, log-loss, and F1. Break ties by choosing the smallest index.\n- Compute the Spearman rank correlation between the $3$-dimensional PFI vectors for the pairs (AUC, log-loss), (AUC, F1), and (log-loss, F1). If a correlation is undefined due to zero variance in ranks, return $0.0$ for that correlation.\n\nTest suite. Your program must run the following three test cases, each specifying the class prior $\\pi$, the sample sizes, and the random seed:\n- Case $1$ (balanced): $\\pi = 0.5$, $n_{\\text{train}} = 4000$, $n_{\\text{test}} = 20000$, seed $= 123$.\n- Case $2$ (moderate imbalance): $\\pi = 0.2$, $n_{\\text{train}} = 4000$, $n_{\\text{test}} = 20000$, seed $= 456$.\n- Case $3$ (severe imbalance): $\\pi = 0.05$, $n_{\\text{train}} = 4000$, $n_{\\text{test}} = 20000$, seed $= 789$.\n\nAnswer specification and output format. For each case, produce a list of $6$ numbers in the following fixed order:\n- the index of the most important feature under AUC (integer in $\\{0,1,2\\}$),\n- the index of the most important feature under log-loss (integer in $\\{0,1,2\\}$),\n- the index of the most important feature under F1 (integer in $\\{0,1,2\\}$),\n- the Spearman rank correlation between AUC-PFI and log-loss-PFI (float),\n- the Spearman rank correlation between AUC-PFI and F1-PFI (float),\n- the Spearman rank correlation between log-loss-PFI and F1-PFI (float).\n\nAggregate the results for all three cases in order into a single flat list of length $18$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[a_1,a_2,\\dots,a_{18}]$). Floats must be printed rounded to exactly $6$ decimal places, and integers printed without decimals. There are no physical units involved. Angles are not used. Percentages, if any appear, must be expressed as decimals, but this problem does not require reporting any percentages explicitly.",
            "solution": "The user provided a valid problem statement.\n\n### 1. Problem Formulation and Objective\n\nThe objective is to empirically investigate the stability of Permutation Feature Importance (PFI) under varying degrees of class imbalance for a binary classification task. We will train a probabilistic classifier, specifically logistic regression, and evaluate PFI using three distinct performance metrics: Area Under the Receiver Operating Characteristic Curve (AUC), log-loss, and F1 score. The analysis will focus on how the ranking of features by importance and the correlation between these rankings change as the class prior probability $\\pi = p(y=1)$ becomes more skewed.\n\n### 2. Synthetic Data Generation\n\nThe data are generated in $d=3$ dimensions from a conditional Gaussian distribution. The process for each sample $(\\mathbf{x}, y)$ is as follows:\n1.  A binary label $y$ is drawn from a Bernoulli distribution: $y \\sim \\mathrm{Bernoulli}(\\pi)$.\n2.  Conditional on the label $y$, a feature vector $\\mathbf{x} \\in \\mathbb{R}^3$ is drawn from a multivariate normal distribution: $\\mathbf{x} \\mid y \\sim \\mathcal{N}(\\boldsymbol{\\mu}_y, \\mathbf{I}_3)$.\nThe means for the two classes are given as $\\boldsymbol{\\mu}_1 = [2.0, 0.8, 0.0]^\\top$ for class $y=1$ and $\\boldsymbol{\\mu}_0 = [0.0, 0.0, 0.0]^\\top$ for class $y=0$. The covariance matrix is the identity matrix $\\mathbf{I}_3$, indicating that the features are conditionally independent and have unit variance.\n\nThe difference in means, $\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0 = [2.0, 0.8, 0.0]^\\top$, dictates the inherent predictive power of the features. Feature $0$ has the largest separation, followed by feature $1$. Feature $2$ has no separation between class-conditional means and is thus an irrelevant or noise feature.\n\n### 3. Logistic Regression Model and Training\n\nWe employ a logistic regression model, which predicts the probability of the positive class $y=1$ given a feature vector $\\mathbf{x}$. The model uses an augmented feature vector $\\tilde{\\mathbf{x}} = [1, \\mathbf{x}^\\top]^\\top$ to incorporate an intercept term. The predicted probability is:\n$$\n\\hat{p}(y=1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\tilde{\\mathbf{x}})\n$$\nwhere $\\mathbf{w} \\in \\mathbb{R}^{d+1}$ is the vector of model parameters (weights) and $\\sigma(z) = (1 + e^{-z})^{-1}$ is the logistic sigmoid function.\n\nThe model is trained by minimizing a regularized empirical risk function. As specified, this function is the sum of the average negative log-likelihood (NLL) on the training set and an $\\ell_2$ penalty on the feature coefficients (excluding the intercept $w_0$). For a training set of size $n_{\\text{train}}$, the objective function $J(\\mathbf{w})$ to be minimized is:\n$$\nJ(\\mathbf{w}) = \\left( -\\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\left[ y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i) \\right] \\right) + \\lambda \\sum_{j=1}^{d} w_j^2\n$$\nwhere $\\hat{p}_i = \\sigma(\\mathbf{w}^\\top \\tilde{\\mathbf{x}}_i)$ and the regularization strength is $\\lambda = 1.0$. This objective function is convex, ensuring that a unique minimum exists. We can find the optimal parameters $\\mathbf{w}^*$ using a quasi-Newton method like L-BFGS, which requires the gradient of the objective function:\n$$\n\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} (\\hat{p}_i - y_i) \\tilde{\\mathbf{x}}_i + 2\\lambda \\mathbf{w}_{\\text{reg}}\n$$\nwhere $\\mathbf{w}_{\\text{reg}} = [0, w_1, \\dots, w_d]^\\top$ is a vector containing zeros for the intercept and the coefficient values for the penalty term.\n\n### 4. Performance Metrics and Permutation Feature Importance\n\nWe assess model performance and feature importance using three metrics:\n\n-   **AUC**: A rank-based metric that is insensitive to class imbalance and calibration. It measures the probability that a randomly chosen positive sample is ranked higher than a randomly chosen negative sample. An AUC of $0.5$ indicates no better than random guessing, while $1.0$ is perfect discrimination.\n-   **Log-loss**: A proper scoring rule that evaluates the quality of predicted probabilities. It is sensitive to both discrimination and calibration and is influenced by class imbalance. Lower values are better.\n-   **F1 score**: The harmonic mean of precision and recall, calculated on binarized predictions using a fixed threshold of $0.5$. This metric is highly sensitive to the choice of threshold and the class distribution.\n\nPermutation Feature Importance (PFI) for a feature $j$ is calculated by measuring the drop (or increase for loss functions) in model performance when the values of that feature in the test set are randomly shuffled. This shuffling breaks the relationship between the feature and the target variable. The procedure is:\n1.  Calculate the baseline performance metric, $S_{\\text{base}}$, on the original test set.\n2.  For each feature $j \\in \\{0, 1, 2\\}$:\n    a. For each of $K=5$ repetitions, create a copy of the test set and randomly permute the column corresponding to feature $j$.\n    b. Compute the performance metric $S_{\\text{perm}, k}$ on this permuted dataset.\n    c. Average the scores over the $K$ repetitions: $\\bar{S}_{\\text{perm}} = \\frac{1}{K} \\sum_{k=1}^K S_{\\text{perm}, k}$.\n3.  Compute the importance $\\Delta_j$:\n    -   For AUC and F1 (metrics to maximize): $\\Delta_j = S_{\\text{base}} - \\bar{S}_{\\text{perm}}$.\n    -   For Log-loss (a metric to minimize): $\\Delta_j = \\bar{S}_{\\text{perm}} - S_{\\text{base}}$.\n\nA larger $\\Delta_j$ value always indicates higher importance.\n\n### 5. Analysis and Algorithmic Implementation\n\nFor each test case, we perform two analyses to compare the PFI results across the three metrics:\n\n1.  **Most Important Feature**: We identify the index of the most important feature, $\\arg\\max_j \\Delta_j$, for each of the three PFI vectors. This reveals which feature each metric deems most impactful. Ties are broken by selecting the smallest index.\n2.  **Rank Correlation**: We compute the Spearman rank correlation coefficient $\\rho$ between the $3$-dimensional PFI vectors for the pairs (AUC, log-loss), (AUC, F1), and (log-loss, F1). A correlation of $1.0$ indicates that the metrics produce identical feature importance rankings. Lower values suggest disagreement. If the correlation is undefined (due to zero variance in ranks), it is reported as $0.0$.\n\nThe overall algorithm is implemented in a Python script as follows:\n-   A main loop iterates through the three test cases defined by $(\\pi, n_{\\text{train}}, n_{\\text{test}}, \\text{seed})$.\n-   For each case, a `numpy.random.default_rng` instance is created with the given seed to ensure reproducibility of data generation and permutations.\n-   Synthetic training and test datasets are generated.\n-   The logistic regression model is trained using `scipy.optimize.minimize` with the `L-BFGS-B` solver, which is provided with the objective function and its analytical gradient for efficiency and stability.\n-   PFI is computed for each feature and each metric by implementing the permutation procedure described above.\n-   The AUC is calculated robustly using the Wilcoxon-Mann-Whitney U-statistic formulation, implemented with `scipy.stats.rankdata`. Log-loss and F1 scores are implemented directly from their definitions, with care taken to handle numerical edge cases (e.g., `log(0)` or division by zero).\n-   The index of the most important feature and the Spearman correlations are computed. `scipy.stats.spearmanr` is used for correlation, with a check for `NaN` results.\n-   The six resulting values for each case are collected, formatted to the specified precision, and printed as a single comma-separated list.",
            "answer": "```python\nimport numpy as np\nimport scipy.optimize\nimport scipy.special\nimport scipy.stats\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation across all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (balanced)\n        {'pi': 0.5, 'n_train': 4000, 'n_test': 20000, 'seed': 123},\n        # Case 2 (moderate imbalance)\n        {'pi': 0.2, 'n_train': 4000, 'n_test': 20000, 'seed': 456},\n        # Case 3 (severe imbalance)\n        {'pi': 0.05, 'n_train': 4000, 'n_test': 20000, 'seed': 789},\n    ]\n\n    mu1 = np.array([2.0, 0.8, 0.0])\n    mu0 = np.array([0.0, 0.0, 0.0])\n    d = 3\n    lambda_reg = 1.0\n    K_permutations = 5\n    \n    all_results = []\n\n    for case in test_cases:\n        pi, n_train, n_test, seed = case['pi'], case['n_train'], case['n_test'], case['seed']\n        \n        # RNG for reproducibility within a test case\n        rng = np.random.default_rng(seed)\n\n        # 1. Data Generation\n        def generate_data(n_samples, pi_val, rng_instance):\n            y = rng_instance.binomial(1, pi_val, size=n_samples)\n            X = np.zeros((n_samples, d))\n            n_pos = np.sum(y)\n            n_neg = n_samples - n_pos\n            X[y == 1, :] = mu1 + rng_instance.standard_normal(size=(n_pos, d))\n            X[y == 0, :] = mu0 + rng_instance.standard_normal(size=(n_neg, d))\n            return X, y\n        \n        X_train, y_train = generate_data(n_train, pi, rng)\n        X_test, y_test = generate_data(n_test, pi, rng)\n\n        # Add intercept term\n        X_train_aug = np.hstack([np.ones((n_train, 1)), X_train])\n        \n        # 2. Logistic Regression Training\n        def log_reg_cost_grad(w, X, y, lambda_val):\n            n_samples = X.shape[0]\n            z = X @ w\n            \n            # Cost Function (Average NLL + L2 Penalty)\n            # Use numerically stable log-sum-exp trick: log(1+exp(z)) = logaddexp(0, z)\n            avg_nll = np.sum(np.logaddexp(0, z) - y * z) / n_samples\n            penalty = lambda_val * np.sum(w[1:]**2)\n            cost = avg_nll + penalty\n            \n            # Gradient\n            p_hat = scipy.special.expit(z)\n            grad_nll = X.T @ (p_hat - y) / n_samples\n            grad_pen = np.zeros_like(w)\n            grad_pen[1:] = 2 * lambda_val * w[1:]\n            grad = grad_nll + grad_pen\n            \n            return cost, grad\n\n        w_initial = np.zeros(d + 1)\n        res = scipy.optimize.minimize(\n            lambda w: log_reg_cost_grad(w, X_train_aug, y_train, lambda_reg),\n            w_initial,\n            method='L-BFGS-B',\n            jac=True\n        )\n        w_opt = res.x\n\n        def predict_proba(X, w):\n            X_aug = np.hstack([np.ones((X.shape[0], 1)), X])\n            return scipy.special.expit(X_aug @ w)\n\n        # 3. Metric Implementations\n        def compute_auc(y_true, y_pred):\n            n_pos = np.sum(y_true == 1)\n            n_neg = np.sum(y_true == 0)\n            if n_pos == 0 or n_neg == 0:\n                return 0.5\n            ranks = scipy.stats.rankdata(y_pred)\n            rank_sum_pos = np.sum(ranks[y_true == 1])\n            u_stat = rank_sum_pos - (n_pos * (n_pos + 1) / 2)\n            return u_stat / (n_pos * n_neg)\n\n        def compute_log_loss(y_true, p_hat):\n            epsilon = 1e-15\n            p_hat_clipped = np.clip(p_hat, epsilon, 1 - epsilon)\n            return -np.mean(y_true * np.log(p_hat_clipped) + (1 - y_true) * np.log(1 - p_hat_clipped))\n\n        def compute_f1(y_true, p_hat):\n            y_pred = (p_hat >= 0.5).astype(int)\n            tp = np.sum((y_true == 1)  (y_pred == 1))\n            fp = np.sum((y_true == 0)  (y_pred == 1))\n            fn = np.sum((y_true == 1)  (y_pred == 0))\n            \n            precision_den = tp + fp\n            recall_den = tp + fn\n            \n            precision = tp / precision_den if precision_den > 0 else 0.0\n            recall = tp / recall_den if recall_den > 0 else 0.0\n            \n            f1_den = precision + recall\n            f1 = 2 * precision * recall / f1_den if f1_den > 0 else 0.0\n            return f1\n            \n        metrics = {\n            'auc': {'func': compute_auc, 'type': 'maximize'},\n            'log_loss': {'func': compute_log_loss, 'type': 'minimize'},\n            'f1': {'func': compute_f1, 'type': 'maximize'}\n        }\n        \n        # 4. PFI Calculation\n        pfi_results = {}\n        p_test_base = predict_proba(X_test, w_opt)\n\n        for name, metric_info in metrics.items():\n            metric_func = metric_info['func']\n            baseline_score = metric_func(y_test, p_test_base)\n            importances = np.zeros(d)\n            \n            for j in range(d):\n                permuted_scores = np.zeros(K_permutations)\n                for k in range(K_permutations):\n                    X_test_perm = X_test.copy()\n                    rng.shuffle(X_test_perm[:, j])\n                    p_test_perm = predict_proba(X_test_perm, w_opt)\n                    permuted_scores[k] = metric_func(y_test, p_test_perm)\n                \n                avg_permuted_score = np.mean(permuted_scores)\n                \n                if metric_info['type'] == 'maximize':\n                    importances[j] = baseline_score - avg_permuted_score\n                else: # minimize\n                    importances[j] = avg_permuted_score - baseline_score\n            \n            pfi_results[name] = importances\n\n        # 5. Analysis\n        pfi_auc = pfi_results['auc']\n        pfi_log_loss = pfi_results['log_loss']\n        pfi_f1 = pfi_results['f1']\n        \n        argmax_auc = np.argmax(pfi_auc)\n        argmax_log_loss = np.argmax(pfi_log_loss)\n        argmax_f1 = np.argmax(pfi_f1)\n        \n        def safe_spearmanr(x, y):\n            corr, _ = scipy.stats.spearmanr(x, y)\n            return corr if not np.isnan(corr) else 0.0\n\n        corr_auc_logloss = safe_spearmanr(pfi_auc, pfi_log_loss)\n        corr_auc_f1 = safe_spearmanr(pfi_auc, pfi_f1)\n        corr_logloss_f1 = safe_spearmanr(pfi_log_loss, pfi_f1)\n        \n        case_results = [\n            argmax_auc, argmax_log_loss, argmax_f1,\n            corr_auc_logloss, corr_auc_f1, corr_logloss_f1\n        ]\n        all_results.extend(case_results)\n\n    # Final formatting and printing\n    formatted_results = []\n    for item in all_results:\n        if isinstance(item, (int, np.integer)):\n            formatted_results.append(str(item))\n        else:\n            formatted_results.append(f\"{item:.6f}\")\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "While standard PFI provides a useful global measure of a feature's importance, it can obscure crucial local effects and interactions. A feature's predictive power may change depending on the value of another feature, a phenomenon known as effect heterogeneity. This advanced practice  introduces Stratified PFI, a technique that computes importance within distinct data slices to uncover such conditional relationships, allowing you to move beyond global summaries and interpret complex models with interaction effects.",
            "id": "3156620",
            "problem": "You will implement stratified Permutation Feature Importance (PFI) to study the conditional importance of a feature across quantile-defined strata of another feature. The fundamental base for this task is the definition of expected prediction error under a specified loss and its empirical approximation. You will rely on the following ingredients.\n\n1. Loss and risk. Let the loss be the squared error loss defined by $L(y, \\hat{y}) = (y - \\hat{y})^2$. The expected prediction error (risk) is defined as $R = \\mathbb{E}[L(Y, \\hat{f}(X))]$, which we approximate with the empirical mean over a finite sample.\n\n2. Permutation Feature Importance (PFI). For a given trained predictor $\\hat{f}$ and a target feature $X_j$, the PFI for $X_j$ under a loss is defined as the increase in risk after breaking the association between $X_j$ and the response by randomly permuting $X_j$ values while leaving all other features unchanged, thereby preserving the marginal distribution of $X_j$. This procedure leverages the invariance of the joint distribution of all other features and the labels under the permutation of $X_j$. \n\n3. Stratified PFI. To analyze conditional importance, you will perform PFI within strata defined by quantile bins of another feature $X_k$. Specifically, you will:\n   - Partition the test data into $Q$ bins using empirical quantiles of $X_k$ at probabilities $0, \\frac{1}{Q}, \\ldots, 1$.\n   - Within each bin, compute the baseline empirical error and then the empirical error after permuting $X_j$ only within that bin (to preserve the conditional distribution of $X_k$ in that stratum).\n   - The stratified PFI for bin $b$ is the difference between the permuted and baseline empirical errors in that bin, averaged over multiple independent permutations.\n\nYour program must implement the following end-to-end pipeline:\n\nA. Data generation. For each test case, simulate independent and identically distributed samples $(X_j, X_k, Y)$ from the model\n$$\nY = \\alpha + a\\,X_j + b\\,X_k + c\\,(X_j X_k) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2),\n$$\nwith $(X_j, X_k)$ jointly Gaussian, zero-mean, unit-variance, and correlation $\\rho$. To generate a pair with the specified correlation $\\rho$, draw $Z_1, Z_2 \\sim \\mathcal{N}(0,1)$ independent, set $X_k = Z_2$ and $X_j = \\rho Z_2 + \\sqrt{1-\\rho^2}\\, Z_1$. Use $n_{\\text{train}}$ samples for training and $n_{\\text{test}}$ samples for testing.\n\nB. Model fitting. Fit a linear model $\\hat{f}$ by Ordinary Least Squares (OLS) with an intercept and the three raw features:\n$$\n\\phi(X_j, X_k) = \\big[1,\\; X_j,\\; X_k,\\; X_j X_k\\big].\n$$\nTrain on the training set to obtain the coefficient vector $\\hat{w}$ such that\n$$\n\\hat{f}(X_j, X_k) = \\hat{w}_0 + \\hat{w}_1 X_j + \\hat{w}_2 X_k + \\hat{w}_3 (X_j X_k).\n$$\n\nC. Stratified PFI within quantile bins of $X_k$ on the test set. For a fixed number of bins $Q$, compute the empirical quantiles of $X_k$ at probabilities $0, \\frac{1}{Q}, \\ldots, 1$ on the test set, forming $Q$ consecutive bins. In bin $q$, perform the following:\n   - Compute the baseline Mean Squared Error (MSE) in that bin:\n     $$\n     \\text{MSE}_{\\text{base}, q} = \\frac{1}{m_q} \\sum_{i \\in \\text{bin } q} \\big(Y_i - \\hat{f}(X_{j,i}, X_{k,i})\\big)^2,\n     $$\n     where $m_q$ is the number of test samples in bin $q$.\n   - For $R$ independent repetitions, randomly permute the $X_j$ values among the samples in bin $q$, recompute predictions using the permuted $X_j$ and unchanged $X_k$ within the model $\\hat{f}$, and record the permuted MSE in that bin, $\\text{MSE}_{\\text{perm}, q}^{(r)}$.\n   - The stratified PFI for bin $q$ is\n     $$\n     I_q = \\left(\\frac{1}{R} \\sum_{r=1}^R \\text{MSE}_{\\text{perm}, q}^{(r)} \\right) - \\text{MSE}_{\\text{base}, q}.\n     $$\n\nD. Output. For each test case, output the list $[I_1, I_2, \\ldots, I_Q]$ rounded to $3$ decimal places. Aggregate the outputs across all test cases into a single line, formatted as a Python-style list of lists. For example, the final line should look like $[[i_{1,1},\\ldots,i_{1,Q}], [i_{2,1},\\ldots,i_{2,Q}], \\ldots]$.\n\nImplementation details and constraints:\n\n- Use the squared error loss $L(y, \\hat{y}) = (y - \\hat{y})^2$.\n- Use $Q = 4$ bins defined by empirical quartiles of $X_k$ on the test set. Place the left edge inclusive and for the final bin include the right edge as well to cover all test points.\n- Use $R = 128$ independent permutations per bin.\n- Use $n_{\\text{train}} = 3000$ and $n_{\\text{test}} = 1500$.\n- For numeric stability, you may solve the OLS problem via a least-squares routine.\n- Randomness and reproducibility:\n  - For test case index $i \\in \\{0,1,2,3\\}$, use seeds $s_{\\text{train}} = 1234 + 100 i$ for generating the training data, $s_{\\text{test}} = 5678 + 100 i$ for generating the test data, and $s_{\\text{perm}} = 91011 + 100 i$ for permutation randomness.\n- Rounding and formatting:\n  - Round each bin-specific importance $I_q$ to $3$ decimal places before output.\n  - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no extra text.\n\nTest suite specification:\n\nProvide results for the following four test cases, each defined by $(\\alpha, a, b, c, \\sigma, \\rho)$:\n\n- Case $1$: $(\\alpha, a, b, c, \\sigma, \\rho) = (0.0, 1.0, 0.5, 1.0, 0.3, 0.4)$.\n- Case $2$: $(\\alpha, a, b, c, \\sigma, \\rho) = (0.0, 1.0, 0.5, 0.0, 0.3, 0.4)$.\n- Case $3$: $(\\alpha, a, b, c, \\sigma, \\rho) = (0.0, 1.0, 0.5, -1.0, 0.3, 0.4)$.\n- Case $4$: $(\\alpha, a, b, c, \\sigma, \\rho) = (0.0, 0.0, 1.0, 0.0, 0.3, 0.0)$.\n\nInstructions to the developer:\n\n- Implement the complete pipeline as described.\n- Do not read any input; all parameters are fixed as specified above.\n- The final output must be a single line exactly in the specified format, for example: $[[0.123,0.234,0.345,0.456],[\\ldots],\\ldots]$.",
            "solution": "The problem is valid. It is a well-posed, scientifically grounded task in computational statistics that requires implementing stratified Permutation Feature Importance (PFI). All parameters, models, and procedures are clearly and consistently defined.\n\nHerein, we detail the step-by-step procedure for computing the stratified PFI.\n\n### A. Data Generation\n\nThe first step is to simulate training and testing datasets. For each of the four test cases, specified by a parameter set $(\\alpha, a, b, c, \\sigma, \\rho)$, we generate two independent datasets: a training set of size $n_{\\text{train}} = 3000$ and a test set of size $n_{\\text{test}} = 1500$. Each sample consists of a triplet $(X_j, X_k, Y)$.\n\nThe features $(X_j, X_k)$ are drawn from a bivariate Gaussian distribution with zero means, unit variances, and a specified correlation $\\rho$. This is achieved by first generating two independent standard normal variables, $Z_1, Z_2 \\sim \\mathcal{N}(0,1)$, and then constructing the correlated variables as follows:\n$$\nX_k = Z_2\n$$\n$$\nX_j = \\rho Z_2 + \\sqrt{1-\\rho^2}\\, Z_1\n$$\nThis construction ensures that $\\mathbb{E}[X_j] = \\mathbb{E}[X_k] = 0$, $\\text{Var}[X_j] = \\text{Var}[X_k] = 1$, and $\\text{Cov}[X_j, X_k] = \\rho$.\n\nThe response variable $Y$ is generated from the true structural model, which includes a main effect for each feature and an interaction term:\n$$\nY = \\alpha + a\\,X_j + b\\,X_k + c\\,(X_j X_k) + \\varepsilon\n$$\nThe noise term $\\varepsilon$ is drawn from a normal distribution with zero mean and variance $\\sigma^2$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nTo ensure reproducibility, separate random number generators are initialized with distinct seeds for training data generation ($s_{\\text{train}}$), test data generation ($s_{\\text{test}}$), and the permutation procedure ($s_{\\text{perm}}$) for each test case indexed by $i \\in \\{0, 1, 2, 3\\}$.\n\n### B. Model Fitting\n\nThe next step is to fit a predictive model $\\hat{f}$ to the generated training data. The specified model is a linear regression model whose features include an intercept, the two raw predictors, and their interaction term. The feature vector is given by $\\phi(X_j, X_k) = [1, X_j, X_k, X_j X_k]$. The functional form of the model is:\n$$\n\\hat{f}(X_j, X_k) = \\hat{w}_0 + \\hat{w}_1 X_j + \\hat{w}_2 X_k + \\hat{w}_3 (X_j X_k)\n$$\nThe coefficient vector $\\hat{w} = [\\hat{w}_0, \\hat{w}_1, \\hat{w}_2, \\hat{w}_3]^T$ is determined using Ordinary Least Squares (OLS). This involves finding the $\\hat{w}$ that minimizes the sum of squared errors on the training set. The solution is obtained by solving the linear system $\\Phi_{\\text{train}}^T \\Phi_{\\text{train}} \\hat{w} = \\Phi_{\\text{train}}^T Y_{\\text{train}}$, where $\\Phi_{\\text{train}}$ is the $n_{\\text{train}} \\times 4$ design matrix constructed from the training data. For numerical stability, this is implemented using a least-squares solver such as `numpy.linalg.lstsq`.\n\n### C. Stratified Permutation Feature Importance (PFI)\n\nThe core of the task is to compute the PFI of feature $X_j$ conditional on the values of feature $X_k$. This is performed on the test set.\n\n1.  **Stratification:** The test data is partitioned into $Q=4$ disjoint bins (strata) based on the values of $X_k$. The bins are defined by the empirical quartiles of $X_k$ from the test set. Specifically, we find the quantile values corresponding to probabilities $\\{0.25, 0.50, 0.75\\}$, which serve as the boundaries for the four bins. This partitions the data into four groups of approximately equal size.\n\n2.  **PFI Calculation within Bins:** For each bin $q \\in \\{1, 2, 3, 4\\}$, we compute the importance score $I_q$ as follows:\n    *   **Baseline Error:** First, we calculate the baseline Mean Squared Error ($\\text{MSE}$) for the samples within bin $q$ using the original, unpermuted data. Let $S_q$ be the set of indices of samples in bin $q$, and let $m_q = |S_q|$ be the number of samples in the bin. The baseline error is:\n        $$\n        \\text{MSE}_{\\text{base}, q} = \\frac{1}{m_q} \\sum_{i \\in S_q} \\big(Y_i - \\hat{f}(X_{j,i}, X_{k,i})\\big)^2\n        $$\n    *   **Permuted Error:** Next, we measure the effect of breaking the association between $X_j$ and $Y$ *within the stratum*. This is done by repeatedly permuting the values of $X_j$ only among the samples in bin $q$. This process is repeated for $R=128$ independent permutations. For each permutation $r \\in \\{1, \\ldots, R\\}$, we obtain a permuted feature vector $X_{j, \\text{perm}}^{(r)}$ and calculate the corresponding MSE:\n        $$\n        \\text{MSE}_{\\text{perm}, q}^{(r)} = \\frac{1}{m_q} \\sum_{i \\in S_q} \\big(Y_i - \\hat{f}(X_{j, \\text{perm}, i}^{(r)}, X_{k,i})\\big)^2\n        $$\n    *   **Importance Score:** The permuted errors are averaged over the $R$ repetitions. The stratified PFI for bin $q$ is the increase in error, defined as the difference between the average permuted MSE and the baseline MSE:\n        $$\n        I_q = \\left(\\frac{1}{R} \\sum_{r=1}^R \\text{MSE}_{\\text{perm}, q}^{(r)} \\right) - \\text{MSE}_{\\text{base}, q}\n        $$\n\n### D. Output Generation\n\nThe above procedure is executed for each of the four specified test cases. For each case, the result is a list of four importance scores, $[I_1, I_2, I_3, I_4]$, where each $I_q$ is rounded to $3$ decimal places. The final output is an aggregation of these results into a single Python-style list of lists, formatted as a string.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the complete pipeline for stratified Permutation Feature Importance.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    Q = 4  # Number of quantile bins\n    R = 128  # Number of permutations per bin\n    n_train = 3000\n    n_test = 1500\n\n    test_cases = [\n        # Case 1: (alpha, a, b, c, sigma, rho)\n        (0.0, 1.0, 0.5, 1.0, 0.3, 0.4),\n        # Case 2: No interaction term\n        (0.0, 1.0, 0.5, 0.0, 0.3, 0.4),\n        # Case 3: Negative interaction term\n        (0.0, 1.0, 0.5, -1.0, 0.3, 0.4),\n        # Case 4: No effect from X_j and no correlation\n        (0.0, 0.0, 1.0, 0.0, 0.3, 0.0),\n    ]\n\n    all_results = []\n\n    for i, params in enumerate(test_cases):\n        alpha, a, b, c, sigma, rho = params\n\n        # --- A. Data Generation ---\n        # Set up random number generators for reproducibility\n        s_train = 1234 + 100 * i\n        s_test = 5678 + 100 * i\n        s_perm = 91011 + 100 * i\n        rng_train = np.random.default_rng(s_train)\n        rng_test = np.random.default_rng(s_test)\n        rng_perm = np.random.default_rng(s_perm)\n\n        # Generate training data\n        Z1_train = rng_train.normal(size=n_train)\n        Z2_train = rng_train.normal(size=n_train)\n        Xk_train = Z2_train\n        Xj_train = rho * Z2_train + np.sqrt(1 - rho**2) * Z1_train\n        eps_train = rng_train.normal(loc=0, scale=sigma, size=n_train)\n        Y_train = alpha + a * Xj_train + b * Xk_train + c * (Xj_train * Xk_train) + eps_train\n\n        # Generate test data\n        Z1_test = rng_test.normal(size=n_test)\n        Z2_test = rng_test.normal(size=n_test)\n        Xk_test = Z2_test\n        Xj_test = rho * Z2_test + np.sqrt(1 - rho**2) * Z1_test\n        eps_test = rng_test.normal(loc=0, scale=sigma, size=n_test)\n        Y_test = alpha + a * Xj_test + b * Xk_test + c * (Xj_test * Xk_test) + eps_test\n\n        # --- B. Model Fitting ---\n        # Construct design matrix for training: [1, Xj, Xk, Xj*Xk]\n        Phi_train = np.c_[np.ones(n_train), Xj_train, Xk_train, Xj_train * Xk_train]\n        \n        # Solve OLS to get model weights\n        w_hat, _, _, _ = np.linalg.lstsq(Phi_train, Y_train, rcond=None)\n\n        def predict(Xj, Xk, weights):\n            \"\"\"Makes predictions using the fitted linear model.\"\"\"\n            Phi = np.c_[np.ones(len(Xj)), Xj, Xk, Xj * Xk]\n            return Phi @ weights\n\n        def mean_squared_error(y_true, y_pred):\n            \"\"\"Computes the mean squared error.\"\"\"\n            return np.mean((y_true - y_pred)**2)\n\n        # --- C. Stratified PFI ---\n        # Define strata based on Xk_test quantiles\n        quantile_boundaries = np.quantile(Xk_test, [0.25, 0.5, 0.75])\n        bin_indices = np.digitize(Xk_test, bins=quantile_boundaries)\n\n        case_importances = []\n        for q in range(Q):\n            in_bin_mask = (bin_indices == q)\n            \n            # Skip if a bin is empty (unlikely for this data size)\n            if not np.any(in_bin_mask):\n                case_importances.append(0.0)\n                continue\n\n            Xj_bin = Xj_test[in_bin_mask]\n            Xk_bin = Xk_test[in_bin_mask]\n            Y_bin = Y_test[in_bin_mask]\n\n            # Compute baseline MSE in the bin\n            Y_pred_base = predict(Xj_bin, Xk_bin, w_hat)\n            mse_base = mean_squared_error(Y_bin, Y_pred_base)\n            \n            # Compute permuted MSEs over R repetitions\n            permuted_mses = []\n            for _ in range(R):\n                Xj_perm = rng_perm.permutation(Xj_bin)\n                Y_pred_perm = predict(Xj_perm, Xk_bin, w_hat)\n                mse_perm = mean_squared_error(Y_bin, Y_pred_perm)\n                permuted_mses.append(mse_perm)\n            \n            # Calculate importance score for the bin\n            avg_mse_perm = np.mean(permuted_mses)\n            importance_q = avg_mse_perm - mse_base\n            case_importances.append(round(importance_q, 3))\n        \n        all_results.append(case_importances)\n\n    # --- D. Output ---\n    # Format the results into a single string: [[...],[...],...]\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_string = f\"[{','.join(inner_strings)}]\"\n    print(final_string)\n\nsolve()\n```"
        }
    ]
}