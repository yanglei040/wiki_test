{
    "hands_on_practices": [
        {
            "introduction": "要想真正掌握特征重要性的量化方法，我们必须超越抽象的定义，亲手进行计算。本练习将指导您在一个小而明确的数据集上，计算两种主要的重要性度量——基于不纯度的方法和基于排列的方法 。通过从第一性原理出发，您将对这些方法的内在机制有更具体的理解，并观察到它们如何对哪个特征更重要得出不同的结论。",
            "id": "3121084",
            "problem": "考虑一个二元分类任务，其特征为 $X_1$、$X_2$，标签为 $Y \\in \\{0,1\\}$。一个深度为 $2$ 的手动构建的决策树定义如下：在根节点，按 $X_1 \\leq 0$ 进行分裂；左子节点（其中 $X_1=0$）按 $X_2 \\leq 0$ 进行分裂；右子节点（其中 $X_1=1$）是一个叶节点。每个叶节点根据到达该节点的训练样本中的多数类别进行预测。\n\n给定一个包含 $n=12$ 个观测值的合成数据集，其计数如下：\n- 三个观测值为 $(X_1,X_2,Y)=(0,0,1)$，\n- 一个观测值为 $(X_1,X_2,Y)=(0,1,0)$，\n- 三个观测值为 $(X_1,X_2,Y)=(1,0,1)$，\n- 五个观测值为 $(X_1,X_2,Y)=(1,1,1)$。\n\n任务：\n1. 从第一性原理出发，计算根节点按 $X_1$ 分裂和左子节点按 $X_2$ 分裂时的基尼不纯度减少量，并通过将每次分裂的不纯度减少量乘以到达该分裂节点的样本比例进行加权，从而形成基于不纯度的特征重要性。使用香农熵（自然对数）重复同样的推导。\n2. 将一个特征的置换重要性定义为：在保持训练好的决策树不变的情况下，将该特征的值在 $n$ 个样本中进行均匀随机置换时，分类准确率（在同一数据集上）的期望下降值。精确计算 $X_1$ 和 $X_2$ 的置换重要性（不要进行模拟；请解析地推导期望值）。\n3. 最后，计算 $X_1$ 和 $X_2$ 的置换重要性之差。\n\n将你的最终答案表示为一个最简分数。无需四舍五入。",
            "solution": "该问题被评估为有效，因为它科学地基于统计学习的原理，问题陈述清晰，具有明确和完整的数据集和定义，并以客观、正式的语言表达。没有矛盾、歧义或不合理的假设。我们可以开始求解。\n\n决策树结构和数据集如下：\n- 样本总数为 $n=12$。\n- 数据计数：\n  - $3$ 个样本：$(X_1=0, X_2=0, Y=1)$\n  - $1$ 个样本：$(X_1=0, X_2=1, Y=0)$\n  - $3$ 个样本：$(X_1=1, X_2=0, Y=1)$\n  - $5$ 个样本：$(X_1=1, X_2=1, Y=1)$\n- 树结构：\n  - 根节点：按 $X_1 \\leq 0$ 分裂。由于 $X_1$ 只取值 $0$ 和 $1$，这将样本分为 $X_1=0$ 与 $X_1=1$ 两组。\n  - 左子节点（对于 $X_1=0$）：按 $X_2 \\leq 0$ 分裂，将样本分为 $X_2=0$ 与 $X_2=1$ 两组。\n  - 右子节点（对于 $X_1=1$）：这是一个叶节点。\n- 叶节点的预测由该叶节点中样本的多数类别决定。\n\n让我们用训练数据填充决策树以确定叶节点的预测。\n- **根节点 (Node 0):** 包含所有 $n=12$ 个样本。\n  - 类别计数：$N_0 = 1$（对于 $Y=0$），$N_1 = 11$（对于 $Y=1$）。\n- **在根节点按 $X_1$ 分裂：**\n  - **左子节点 (Node 1, $X_1=0$):** 包含 $3+1=4$ 个样本。\n    - 类别计数：$N_0 = 1$，$N_1 = 3$。\n  - **右子节点 (Node 2, $X_1=1$):** 包含 $3+5=8$ 个样本。这是一个叶节点。\n    - 类别计数：$N_0 = 0$，$N_1 = 8$。多数类别为 $1$。所以，该叶节点预测 $\\hat{Y}=1$。\n- **在节点1按 $X_2$ 分裂：**\n  - **左-左子节点 (Node 3, $X_1=0, X_2=0$):** 包含 $3$ 个样本。这是一个叶节点。\n    - 类别计数：$N_0 = 0$，$N_1 = 3$。多数类别为 $1$。所以，该叶节点预测 $\\hat{Y}=1$。\n  - **左-右子节点 (Node 4, $X_1=0, X_2=1$):** 包含 $1$ 个样本。这是一个叶节点。\n    - 类别计数：$N_0 = 1$，$N_1 = 0$。多数类别为 $0$。所以，该叶节点预测 $\\hat{Y}=0$。\n\n固定的决策树的预测规则是：\n- 如果 $X_1=1$，预测 $\\hat{Y}=1$。\n- 如果 $X_1=0$ 且 $X_2=0$，预测 $\\hat{Y}=1$。\n- 如果 $X_1=0$ 且 $X_2=1$，预测 $\\hat{Y}=0$。\n\n**1. 基于不纯度的特征重要性**\n\n这部分按要求计算，但最终答案不使用它，最终答案关心的是置换重要性。\n\n**基尼不纯度 (Gini Impurity):** 对于一个类别比例为 $p_k$ 的节点，其基尼不纯度为 $I_G = 1 - \\sum_k p_k^2$。不纯度减少量为 $\\Delta I_G = I_{G,parent} - \\sum_{c \\in children} \\frac{N_c}{N_{parent}} I_{G,c}$。特征的重要性是该特征所有分裂的加权不纯度减少量之和：$Imp(F) = \\sum_{s \\in \\text{splits on } F} \\frac{N_s}{N} \\Delta I_G(s)$。\n\n- **根节点按 $X_1$ 分裂：**\n  - $I_G(\\text{Root}): p_0=\\frac{1}{12}, p_1=\\frac{11}{12} \\implies I_G(\\text{Root}) = 1 - ((\\frac{1}{12})^2 + (\\frac{11}{12})^2) = 1 - \\frac{1+121}{144} = \\frac{22}{144} = \\frac{11}{72}$。\n  - $I_G(\\text{Node 1, } X_1=0): N=4, p_0=\\frac{1}{4}, p_1=\\frac{3}{4} \\implies I_G(\\text{Node 1}) = 1 - ((\\frac{1}{4})^2 + (\\frac{3}{4})^2) = 1 - \\frac{1+9}{16} = \\frac{6}{16} = \\frac{3}{8}$。\n  - $I_G(\\text{Node 2, } X_1=1): N=8, p_0=0, p_1=1 \\implies I_G(\\text{Node 2}) = 1 - (0^2+1^2) = 0$。\n  - 不纯度减少量：$\\Delta I_G(\\text{root}) = \\frac{11}{72} - (\\frac{4}{12} \\cdot \\frac{3}{8} + \\frac{8}{12} \\cdot 0) = \\frac{11}{72} - \\frac{1}{8} = \\frac{11-9}{72} = \\frac{2}{72} = \\frac{1}{36}$。\n  - $X_1$ 的重要性：$Imp_G(X_1) = \\frac{12}{12} \\cdot \\Delta I_G(\\text{root}) = \\frac{1}{36}$。\n\n- **节点1按 $X_2$ 分裂：**\n  - $I_G(\\text{Node 1}) = \\frac{3}{8}$。\n  - $I_G(\\text{Node 3, } X_1=0, X_2=0): N=3, p_1=1 \\implies I_G(\\text{Node 3}) = 0$。\n  - $I_G(\\text{Node 4, } X_1=0, X_2=1): N=1, p_0=1 \\implies I_G(\\text{Node 4}) = 0$。\n  - 不纯度减少量：$\\Delta I_G(\\text{Node 1}) = \\frac{3}{8} - (\\frac{3}{4} \\cdot 0 + \\frac{1}{4} \\cdot 0) = \\frac{3}{8}$。\n  - $X_2$ 的重要性：$Imp_G(X_2) = \\frac{4}{12} \\cdot \\Delta I_G(\\text{Node 1}) = \\frac{1}{3} \\cdot \\frac{3}{8} = \\frac{1}{8}$。\n\n**香农熵 (Shannon Entropy):** 熵的定义为 $I_H = -\\sum_k p_k \\ln(p_k)$。\n- **根节点按 $X_1$ 分裂：**\n  - $I_H(\\text{Root}) = -(\\frac{1}{12}\\ln\\frac{1}{12} + \\frac{11}{12}\\ln\\frac{11}{12})$。\n  - $I_H(\\text{Node 1}) = -(\\frac{1}{4}\\ln\\frac{1}{4} + \\frac{3}{4}\\ln\\frac{3}{4})$。\n  - $I_H(\\text{Node 2}) = 0$。\n  - 信息增益：$\\Delta I_H(\\text{root}) = I_H(\\text{Root}) - \\frac{4}{12}I_H(\\text{Node 1})$。\n  - $X_1$ 的重要性：$Imp_H(X_1) = \\frac{12}{12} \\Delta I_H(\\text{root}) = I_H(\\text{Root}) - \\frac{1}{3}I_H(\\text{Node 1})$。\n\n- **节点1按 $X_2$ 分裂：**\n  - $I_H(\\text{Node 1}) = -(\\frac{1}{4}\\ln\\frac{1}{4} + \\frac{3}{4}\\ln\\frac{3}{4})$。\n  - $I_H(\\text{Node 3}) = 0$ and $I_H(\\text{Node 4}) = 0$。\n  - 信息增益：$\\Delta I_H(\\text{Node 1}) = I_H(\\text{Node 1})$。\n  - $X_2$ 的重要性：$Imp_H(X_2) = \\frac{4}{12} \\Delta I_H(\\text{Node 1}) = \\frac{1}{3} I_H(\\text{Node 1})$。\n\n**2. 置换重要性**\n\n特征 $X_j$ 的置换重要性定义为 $PI(X_j) = \\text{accuracy}_{\\text{original}} - E[\\text{accuracy}_{\\text{permuted}}]$。\n\n首先，我们计算在原始数据集上的基准准确率。\n- 对于 $(X_1=0, X_2=0, Y=1)$ ($3$ 个样本)：预测为 $\\hat{Y}=1$。正确。\n- 对于 $(X_1=0, X_2=1, Y=0)$ ($1$ 个样本)：预测为 $\\hat{Y}=0$。正确。\n- 对于 $(X_1=1, X_2=0, Y=1)$ ($3$ 个样本)：预测为 $\\hat{Y}=1$。正确。\n- 对于 $(X_1=1, X_2=1, Y=1)$ ($5$ 个样本)：预测为 $\\hat{Y}=1$。正确。\n所有 $12$ 个样本都被正确分类。因此，$\\text{accuracy}_{\\text{original}} = \\frac{12}{12} = 1$。\n\n置换后的期望准确率为 $E[\\text{accuracy}] = \\frac{1}{n} \\sum_{i=1}^{n} P(\\hat{y}_i = y_i)$。\n\n**$X_1$ 的置换重要性**\n$X_1$ 值的向量有 $4$ 个零和 $8$ 个一。当置换后，对于任何样本 $i$，其新值 $X'_{1,i}$ 的概率为 $P(X'_{1,i}=0) = \\frac{4}{12} = \\frac{1}{3}$ 和 $P(X'_{1,i}=1) = \\frac{8}{12} = \\frac{2}{3}$。每个样本的 $X_2$ 和 $Y$ 值保持不变。\n我们计算每个样本被正确预测的概率。\n- **具有 $X_2=0, Y=1$ 的 $6$ 个样本**（来自组 $(0,0,1)$ 和 $(1,0,1)$）：\n  - 预测取决于 $X'_{1,i}$。\n  - 如果 $X'_{1,i}=0$ (概率 $\\frac{1}{3}$)，预测 $\\hat{Y}=1$。正确。\n  - 如果 $X'_{1,i}=1$ (概率 $\\frac{2}{3}$)，预测 $\\hat{Y}=1$。正确。\n  - 预测总是正确的。$P(\\text{正确}) = 1$。\n- **具有 $X_2=1, Y=0$ 的 $1$ 个样本**：\n  - 如果 $X'_{1,i}=0$ (概率 $\\frac{1}{3}$)，预测 $\\hat{Y}=0$。正确。\n  - 如果 $X'_{1,i}=1$ (概率 $\\frac{2}{3}$)，预测 $\\hat{Y}=1$。不正确。\n  - $P(\\text{正确}) = \\frac{1}{3}$。\n- **具有 $X_2=1, Y=1$ 的 $5$ 个样本**：\n  - 如果 $X'_{1,i}=0$ (概率 $\\frac{1}{3}$)，预测 $\\hat{Y}=0$。不正确。\n  - 如果 $X'_{1,i}=1$ (概率 $\\frac{2}{3}$)，预测 $\\hat{Y}=1$。正确。\n  - $P(\\text{正确}) = \\frac{2}{3}$。\n\n期望的正确预测数量为 $6 \\cdot 1 + 1 \\cdot \\frac{1}{3} + 5 \\cdot \\frac{2}{3} = 6 + \\frac{1}{3} + \\frac{10}{3} = 6 + \\frac{11}{3} = \\frac{18+11}{3} = \\frac{29}{3}$。\n期望的准确率为 $E[\\text{accuracy}_{\\text{permuted } X_1}] = \\frac{29/3}{12} = \\frac{29}{36}$。\n$X_1$ 的置换重要性为 $PI(X_1) = 1 - \\frac{29}{36} = \\frac{7}{36}$。\n\n**$X_2$ 的置换重要性**\n$X_2$ 值的向量有 $3+3=6$ 个零和 $1+5=6$ 个一。当置换后，对于任何样本 $i$，其新值 $X'_{2,i}$ 的概率为 $P(X'_{2,i}=0) = \\frac{6}{12} = \\frac{1}{2}$ 和 $P(X'_{2,i}=1) = \\frac{6}{12} = \\frac{1}{2}$。每个样本的 $X_1$ 和 $Y$ 值保持不变。\n- **具有 $X_1=0$ 的 $4$ 个样本**：\n  - $3$ 个样本有 $Y=1$。如果 $X'_{2,i}=0$ (概率 $\\frac{1}{2}$)，预测为 $\\hat{Y}=1$；如果 $X'_{2,i}=1$ (概率 $\\frac{1}{2}$)，预测为 $\\hat{Y}=0$。$P(\\text{正确}) = \\frac{1}{2}$。\n  - $1$ 个样本有 $Y=0$。如果 $X'_{2,i}=0$ (概率 $\\frac{1}{2}$)，预测为 $\\hat{Y}=1$；如果 $X'_{2,i}=1$ (概率 $\\frac{1}{2}$)，预测为 $\\hat{Y}=0$。$P(\\text{正确}) = \\frac{1}{2}$。\n- **具有 $X_1=1$ 的 $8$ 个样本**：\n  - 所有 $8$ 个样本都有 $Y=1$。对于任何 $X_1=1$ 的样本，无论 $X_2$ 为何值，决策树都预测 $\\hat{Y}=1$。预测总是正确的。$P(\\text{正确}) = 1$。\n\n期望的正确预测数量为 $3 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{2} + 8 \\cdot 1 = \\frac{4}{2} + 8 = 2+8=10$。\n期望的准确率为 $E[\\text{accuracy}_{\\text{permuted } X_2}] = \\frac{10}{12} = \\frac{5}{6}$。\n$X_2$ 的置换重要性为 $PI(X_2) = 1 - \\frac{5}{6} = \\frac{1}{6}$。\n\n**3. 最终计算**\n\n最后的任务是计算 $X_1$ 和 $X_2$ 的置换重要性之差。\n$PI(X_1) - PI(X_2) = \\frac{7}{36} - \\frac{1}{6} = \\frac{7}{36} - \\frac{6}{36} = \\frac{1}{36}$。",
            "answer": "$$\\boxed{\\frac{1}{36}}$$"
        },
        {
            "introduction": "真实世界的数据很少是干净的，常常包含无信息或冗余的特征。本实践将从手动计算转向以编程方式实现随机森林，以研究基于不纯度的重要性分数在此类挑战性情景下的表现 。通过在包含恒定特征和完全相关特征的数据上测试算法，您将揭示这种重要性度量的关键局限性，并理解为何必须谨慎解释其结果。",
            "id": "3166180",
            "problem": "你需要从头开始实现一个用于二元分类的随机森林，该森林基于分类与回归树 (CART)，使用基尼不纯度作为分裂标准，并计算定义为平均不纯度下降的归一化特征重要性分数。你的实现必须处理某些特征是常数（零方差）的边界情况。然后，你将在三个指定的测试用例上运行该实现，并报告一个紧凑的数值摘要。\n\n定义和要求：\n- 决策树是递归生成的。在每个具有数据索引 $\\mathcal{I}$ 的内部节点，你必须：\n  1) 在每个节点上，独立地从 $d$ 个可用特征中随机（无放回）选择一个大小为 $m$ 的特征子集。\n  2) 对于每个选定的特征 $j$，考虑所有候选阈值，这些阈值由 $\\mathcal{I}$ 中样本在该特征上的排序后唯一值之间的中点构成。如果一个特征只有一个唯一值，则无法在此节点上进行分裂。\n  3) 对于一个候选阈值 $\\tau$，将左子节点定义为满足 $x_{ij} \\le \\tau$ 的索引集，右子节点定义为满足 $x_{ij} > \\tau$ 的索引集。计算此分裂的加权基尼不纯度，并选择能最大化不纯度下降的特征和阈值。如果最佳的不纯度下降为非正数，则将该节点设为叶节点。\n- 对于任何具有索引集 $\\mathcal{I}$ 和类别标签 $y_i \\in \\{0,1\\}$ 的节点，其基尼不纯度为 $G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2$，其中 $p_k$ 是 $\\mathcal{I}$ 中的类别比例。\n- 对于使用特征 $j$ 和阈值 $\\tau$ 将 $\\mathcal{I}$ 分裂为左子集 $\\mathcal{L}$ 和右子集 $\\mathcal{R}$，该分裂贡献的不纯度下降量为\n$$\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R}).$$\n- 一个包含 $T$ 棵树的随机森林是通过在原始训练集上进行大小为 $N$ 的 bootstrap 抽样（有放回抽样）来训练每棵树，并应用大小为 $m$ 的节点级随机特征子空间选择来构建的。\n- 对于单棵树，特征 $j$ 的重要性是所有使用特征 $j$ 进行最佳分裂的内部节点上，$\\Delta(\\mathcal{I}, j, \\tau)$ 乘以该节点样本数 $|\\mathcal{I}|$ 的总和。在所有树上聚合这个总和。设 $S$ 为所有特征的这些聚合后的、经样本加权的不纯度下降量的总和。如果 $S > 0$，则将特征 $j$ 的归一化重要性定义为 $I_j = \\frac{\\text{特征 } j \\text{ 的聚合下降量}}{S}$，使得 $\\sum_{j=1}^{d} I_j = 1$。如果 $S = 0$，则对所有 $j$ 定义 $I_j = 0$。\n- 除了当没有有效分裂能产生正的不纯度下降时的隐式停止条件外，对树的深度没有限制。\n\n实现约束：\n- 你必须完全按照上述规定，使用纯数值运算来实现该过程。不允许使用外部机器学习库。\n- 所有的随机性必须使用指定的种子以保证可复现。\n\n数据集和测试套件：\n你将在三个用例上运行你的实现。在每个用例中，按照规定构建特征矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 和标签 $y \\in \\{0,1\\}^N$，然后用指定的参数训练一个随机森林，并计算归一化的特征重要性 $\\{I_j\\}_{j=1}^d$。\n\n用例 1（包含一个信息特征、一个常数特征和一个噪声特征）：\n- 数据生成种子：$42$。\n- $N = 200$, $d = 3$.\n- 对 $i = 1,\\dots,200$ 独立生成 $x_{i1} \\sim \\mathcal{N}(0,1)$。\n- 对所有 $i$ 设置 $x_{i2} = 0$（一个常数特征）。\n- 对所有 $i$ 独立生成 $x_{i3} \\sim \\mathcal{N}(0,1)$。\n- 定义标签：如果 $x_{i1} > 0$ 则 $y_i = 1$，否则 $y_i = 0$。\n- 随机森林参数：树的数量 $T = 50$，节点级特征子集大小 $m = 2$，森林随机性种子 $2024$。\n- 计算归一化重要性 $(I_1, I_2, I_3)$。\n\n用例 2（一个完美复制的信息特征和一个噪声特征）：\n- 数据生成种子：$123$。\n- $N = 200$, $d = 3$.\n- 对 $i = 1,\\dots,200$ 独立生成 $x_{i1} \\sim \\mathcal{N}(0,1)$。\n- 对所有 $i$ 设置 $x_{i2} = x_{i1}$（一个完美复制）。\n- 独立生成 $x_{i3} \\sim \\mathcal{N}(0,1)$。\n- 定义标签：如果 $x_{i1} > 0$ 则 $y_i = 1$，否则 $y_i = 0$。\n- 随机森林参数：树的数量 $T = 50$，节点级特征子集大小 $m = 1$，森林随机性种子 $2025$。\n- 计算归一化重要性 $(I_1, I_2, I_3)$，并计算重复特征共享统计量 $S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}$。\n\n用例 3（所有特征均为常数）：\n- 数据生成种子：$7$。\n- $N = 100$, $d = 2$.\n- 对所有 $i$ 设置 $x_{i1} = 0$ 和 $x_{i2} = 3$（两个特征均为常数）。\n- 使用给定的种子，将标签 $y_i$ 生成为 $P(y_i = 1) = 0.5$ 的独立伯努利分布。\n- 随机森林参数：树的数量 $T = 10$，节点级特征子集大小 $m = 2$，森林随机性种子 $99$。\n- 计算归一化重要性 $(I_1, I_2)$ 及其和 $Q = I_1 + I_2$。\n\n数值报告：\n- 对于用例 1，报告三个浮点数 $I_1, I_2, I_3$。\n- 对于用例 2，报告四个浮点数 $S_{\\text{dup}}, I_1, I_2, I_3$。\n- 对于用例 3，报告三个浮点数 $Q, I_1, I_2$。\n- 将每个报告的浮点数四舍五入到六位小数。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含所有报告结果，以逗号分隔的列表形式用方括号括起来，顺序如下：\n$[I_1^{(1)}, I_2^{(1)}, I_3^{(1)}, S_{\\text{dup}}^{(2)}, I_1^{(2)}, I_2^{(2)}, I_3^{(2)}, Q^{(3)}, I_1^{(3)}, I_2^{(3)}]$,\n其中上标表示用例索引。没有物理单位。不使用角度。所有比例必须以小数形式报告，而不是百分比。",
            "solution": "用户指定了从第一性原理实现用于二元分类的随机森林分类器的任务。该实现必须遵循一套精确的定义，包括分类与回归树（CART）、基尼不纯度、节点分裂，以及一种基于平均不纯度下降计算特征重要性的特定方法。该实现将针对三个不同的测试用例进行验证，并且必须以特定格式报告一组数值结果。\n\n### 问题验证\n\n首先，我将对照要求的标准验证问题陈述。\n\n**步骤1：提取既定信息**\n\n- **算法：** 由 `T` 棵用于二元分类（`y \\in \\{0,1\\}`）的分类与回归树（CART）组成的随机森林。\n- **树的构建（递归）：**\n    - 在每个节点 `\\mathcal{I}`，从 `d` 个总特征中无放回地随机选择 `m` 个特征。\n    - 对每个选定的特征 `j`，找到最优分裂。\n    - **阈值 (`\\tau`):** 特征 `j` 在样本 `\\mathcal{I}` 中排序后的唯一值之间的中点。\n    - **分裂规则：** 左子节点 `\\mathcal{L} = \\{i \\in \\mathcal{I} | x_{ij} \\le \\tau\\}`，右子节点 `\\mathcal{R} = \\{i \\in \\mathcal{I} | x_{ij} > \\tau\\}`。\n    - **停止条件：** 如果最佳的不纯度下降为非正数，或无法进行有效分裂（例如，节点处的所有特征都是常数），则该节点成为叶节点。\n- **分裂标准（基尼不纯度）：**\n    - **节点 `\\mathcal{I}` 的不纯度：** `G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2`，其中 `p_k` 是类别 `k` 的比例。\n    - **不纯度下降（基尼增益）：** `\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R})`。选择使 `\\Delta` 最大化的分裂。\n- **集成方法（随机森林）：**\n    - `T` 棵树中的每一棵都在大小为 `N` 的训练数据的 `N` 个自举样本（有放回抽样）上进行训练。\n- **特征重要性：**\n    - 对于单棵树和特征 `j`，其重要性是在所有以 `j` 为最佳分裂特征的节点上 `|\\mathcal{I}| \\cdot \\Delta(\\mathcal{I}, j, \\tau)` 的总和。\n    - 特征 `j` 的总重要性是在所有 `T` 棵树上聚合的总和。\n    - **归一化：** 设 `S` 是所有特征聚合重要性的总和。如果 `S > 0`，则归一化重要性 `I_j` 为 `\\frac{\\text{j 的聚合下降量}}{S}`；如果 `S = 0`，则 `I_j = 0`。\n- **测试用例参数：**\n    - **用例1：** 数据种子=`42`，`N=200`，`d=3`。`x_1 \\sim \\mathcal{N}(0,1)`，`x_2=0`，`x_3 \\sim \\mathcal{N}(0,1)`。`y=1` 若 `x_1 > 0`，否则为 `0`。随机森林参数：`T=50`，`m=2`，森林种子=`2024`。报告：`(I_1, I_2, I_3)`。\n    - **用例2：** 数据种子=`123`，`N=200`，`d=3`。`x_1 \\sim \\mathcal{N}(0,1)`，`x_2=x_1`，`x_3 \\sim \\mathcal{N}(0,1)`。`y=1` 若 `x_1 > 0`，否则为 `0`。随机森林参数：`T=50`，`m=1`，森林种子=`2025`。报告：`(S_{\\text{dup}}, I_1, I_2, I_3)`，其中 `S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}`。\n    - **用例3：** 数据种子=`7`，`N=100`，`d=2`。`x_1=0`，`x_2=3`。`y \\sim \\text{Bernoulli}(0.5)`。随机森林参数：`T=10`，`m=2`，森林种子=`99`。报告：`(Q, I_1, I_2)`，其中 `Q = I_1 + I_2`。\n- **输出：** 单个逗号分隔的列表，包含10个浮点数，四舍五入到六位小数，并用方括号括起来。\n\n**步骤2：使用提取的既定信息进行验证**\n\n- **科学性：** 该问题描述了标准的随机森林算法。CART方法、基尼不纯度准则和基于平均不纯度下降（MDI）的特征重要性都是统计学习中公认的概念。所提供的定义在数学上和算法上都是正确的。\n- **适定性：** 问题被精确地指定。所有参数（`N`, `d`, `T`, `m`）、数据生成过程和随机种子都已提供，这确保了结果是唯一的、确定性的和可计算的。边界情况（常数特征、无正增益）也已明确定义。\n- **客观性：** 语言是正式、精确的，没有任何主观或模糊的术语。\n\n该问题不违反任何使其无效的标准。它是一个定义明确、科学合理的计算任务。\n\n**步骤3：结论与行动**\n\n该问题是**有效的**。我将继续进行实现和求解。\n\n### 算法实现\n\n解决方案将结构化为一组函数，共同实现指定的随机森林算法。\n\n1.  **`gini_impurity(y)`**：一个辅助函数，用于计算给定标签数组 `y` 的基尼不纯度。\n2.  **`find_best_split(X, y, idxs, feature_subset)`**：此函数遍历给定的特征子集。对于每个特征，它评估所有有效的阈值（唯一值的中点），以找到最大化基尼增益的分裂。它返回一个包含最佳分裂细节（特征索引、阈值、增益和子节点索引）的字典，如果找不到有效分裂则返回 `None`。\n3.  **`grow_tree(X, y, idxs, m, d, rng, tree_importances)`**：一个递归函数，用于构建单棵决策树。在每一步（节点），它检查停止条件（例如，纯节点）。如果不是叶节点，它会随机选择 `m` 个特征，调用 `find_best_split` 来确定最优分裂，记录对特征重要性的贡献（`|\\mathcal{I}| \\cdot \\Delta`），然后为左右子节点递归调用自身。\n4.  **`run_rf_case(X, y, T, m, forest_seed)`**：此函数为单个测试用例协调整个随机森林的训练。它使用森林种子初始化一个随机数生成器。然后迭代 `T` 次，每次创建一个数据的自举样本并使用 `grow_tree` 生长一棵树。它聚合所有树的特征重要性，并返回最终的归一化重要性分数。\n5.  **`solve()`**：主函数，为三个测试用例准备指定的数据，使用适当的参数调用 `run_rf_case`，计算所需的摘要统计量（`S_{\\text{dup}}`、`Q`），并以指定格式打印最终的组合结果。\n\n随机性通过 `numpy.random.default_rng` 进行仔细管理，以确保数据生成和算法所有随机方面（自举抽样和特征选择）的可复现性。特别注意了边界情况，例如处理常数特征（无法分裂）和仅在总不纯度下降为正时才进行重要性归一化。",
            "answer": "```python\nimport numpy as np\n\ndef gini_impurity(y):\n    \"\"\"Computes the Gini impurity of a set of labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    _, counts = np.unique(y, return_counts=True)\n    proportions = counts / n_samples\n    return 1.0 - np.sum(proportions**2)\n\ndef find_best_split(X, y, idxs, feature_subset):\n    \"\"\"Finds the best split for a node by iterating through features and thresholds.\"\"\"\n    X_node, y_node = X[idxs], y[idxs]\n    n_node = len(y_node)\n    \n    if n_node = 1:\n        return None\n\n    g_parent = gini_impurity(y_node)\n    best_gain = -1.0\n    best_split_info = None\n\n    for j in feature_subset:\n        feature_values = X_node[:, j]\n        unique_vals = np.unique(feature_values)\n\n        if len(unique_vals) = 1:\n            continue\n\n        thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n        for tau in thresholds:\n            left_mask = feature_values = tau\n            right_mask = ~left_mask\n\n            y_left, y_right = y_node[left_mask], y_node[right_mask]\n\n            if len(y_left) == 0 or len(y_right) == 0:\n                continue\n\n            g_left = gini_impurity(y_left)\n            g_right = gini_impurity(y_right)\n            n_left, n_right = len(y_left), len(y_right)\n            \n            w_left = n_left / n_node\n            w_right = n_right / n_node\n            \n            gain = g_parent - (w_left * g_left + w_right * g_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_split_info = {\n                    'feature': j, \n                    'threshold': tau, \n                    'gain': gain,\n                    'left_idxs': idxs[left_mask],\n                    'right_idxs': idxs[right_mask]\n                }\n                \n    return best_split_info\n\ndef grow_tree(X, y, idxs, m, d, rng, tree_importances):\n    \"\"\"Recursively grows a single decision tree and accumulates feature importances.\"\"\"\n    if len(np.unique(y[idxs])) == 1:\n        return\n\n    m_node = min(m, d)\n    feature_subset = rng.choice(d, size=m_node, replace=False)\n    \n    best_split = find_best_split(X, y, idxs, feature_subset)\n    \n    if best_split is None or best_split['gain'] = 0:\n        return\n\n    j = best_split['feature']\n    gain = best_split['gain']\n    n_samples = len(idxs)\n    tree_importances[j] += n_samples * gain\n    \n    left_idxs = best_split['left_idxs']\n    right_idxs = best_split['right_idxs']\n    \n    grow_tree(X, y, left_idxs, m, d, rng, tree_importances)\n    grow_tree(X, y, right_idxs, m, d, rng, tree_importances)\n\ndef run_rf_case(X, y, T, m, forest_seed):\n    \"\"\"Runs the Random Forest algorithm for a given case and computes feature importances.\"\"\"\n    N, d = X.shape\n    forest_rng = np.random.default_rng(forest_seed)\n    total_importances = np.zeros(d)\n\n    for _ in range(T):\n        bootstrap_idxs = forest_rng.choice(N, size=N, replace=True)\n        X_sample, y_sample = X[bootstrap_idxs], y[bootstrap_idxs]\n        \n        tree_importances = np.zeros(d)\n        \n        # Grow a tree on the bootstrap sample\n        initial_idxs = np.arange(len(y_sample))\n        grow_tree(X_sample, y_sample, initial_idxs, m, d, forest_rng, tree_importances)\n        \n        total_importances += tree_importances\n\n    S = np.sum(total_importances)\n    if S > 0:\n        normalized_importances = total_importances / S\n    else:\n        normalized_importances = np.zeros(d)\n        \n    return normalized_importances\n\ndef solve():\n    \"\"\"Generates data for three cases, runs the RF, and reports the results.\"\"\"\n    all_results = []\n\n    # Case 1\n    data_seed, N, d = 42, 200, 3\n    T, m, forest_seed = 50, 2, 2024\n    rng_case1 = np.random.default_rng(data_seed)\n    X1 = np.zeros((N, d))\n    X1[:, 0] = rng_case1.normal(size=N)\n    X1[:, 1] = 0.0\n    X1[:, 2] = rng_case1.normal(size=N)\n    y1 = (X1[:, 0] > 0).astype(int)\n    importances1 = run_rf_case(X1, y1, T, m, forest_seed)\n    all_results.extend(importances1)\n\n    # Case 2\n    data_seed, N, d = 123, 200, 3\n    T, m, forest_seed = 50, 1, 2025\n    rng_case2 = np.random.default_rng(data_seed)\n    X2 = np.zeros((N, d))\n    X2[:, 0] = rng_case2.normal(size=N)\n    X2[:, 1] = X2[:, 0]\n    X2[:, 2] = rng_case2.normal(size=N)\n    y2 = (X2[:, 0] > 0).astype(int)\n    importances2 = run_rf_case(X2, y2, T, m, forest_seed)\n    I1_2, I2_2, I3_2 = importances2\n    denom = I1_2 + I2_2 + I3_2\n    S_dup = (I1_2 + I2_2) / denom if denom > 0 else 0.0\n    all_results.extend([S_dup, I1_2, I2_2, I3_2])\n\n    # Case 3\n    data_seed, N, d = 7, 100, 2\n    T, m, forest_seed = 10, 2, 99\n    rng_case3 = np.random.default_rng(data_seed)\n    X3 = np.zeros((N, d))\n    X3[:, 0] = 0.0\n    X3[:, 1] = 3.0\n    y3 = rng_case3.integers(0, 2, size=N)\n    importances3 = run_rf_case(X3, y3, T, m, forest_seed)\n    I1_3, I2_3 = importances3\n    Q = I1_3 + I2_3\n    all_results.extend([Q, I1_3, I2_3])\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个特征的重要性得分并非一个固定不变的普适数值，而是产生它的模型的产物。本练习旨在探讨一个关键概念：特征重要性估计如何对树的超参数（如最大深度 $d_{\\max}$ 和最小叶节点样本数 $\\ell_{\\min}$）敏感 。通过系统地改变这些参数并观察其对重要性排名的影响，您将学会量化这种不稳定性，并构建一个更稳健的、经“超参数平均”的重要性估计。",
            "id": "3121059",
            "problem": "你的任务是实现一个确定性回归树学习器，以研究特征重要性估计对树超参数的敏感性，并通过对不同配置进行平均来构建一个超参数鲁棒的重要性估计。所有计算必须从第一性原理出发，使用经验风险最小化和轴对齐分裂来完成。\n\n定义和设置：\n- 考虑一个数据集，包含 $n$ 个样本，特征 $X \\in \\mathbb{R}^{n \\times p}$ 和目标 $y \\in \\mathbb{R}^{n}$。回归树使用轴对齐阈值递归地划分输入空间。在任何包含样本索引集 $S \\subseteq \\{1,\\dots,n\\}$ 的节点上，将不纯度定义为平方偏差之和\n$$\n\\mathcal{I}(S) \\triangleq \\sum_{i \\in S} \\left(y_i - \\bar{y}_S\\right)^2,\\quad \\bar{y}_S \\triangleq \\frac{1}{|S|}\\sum_{i \\in S} y_i.\n$$\n- 对特征 $j \\in \\{1,\\dots,p\\}$ 在阈值 $t \\in \\mathbb{R}$ 处进行分裂，会创建两个子集\n$$\nS_L(j,t) \\triangleq \\{i \\in S: X_{ij} \\le t\\}, \\quad S_R(j,t) \\triangleq \\{i \\in S: X_{ij}  t\\}.\n$$\n- 这样的分裂所实现的不纯度减少量为\n$$\n\\Delta \\mathcal{I}(S;j,t) \\triangleq \\mathcal{I}(S) - \\left(\\mathcal{I}(S_L(j,t)) + \\mathcal{I}(S_R(j,t))\\right).\n$$\n\n控制树的超参数：\n- 最大深度 $d_{\\max} \\in \\mathbb{N}$，其中根节点的深度为 $0$，深度为 $d$ 的节点只有在 $d  d_{\\max}$ 时才能分裂。\n- 每个叶节点的最小样本数 $\\ell_{\\min} \\in \\mathbb{N}$，其中一个提议的分裂 $(j,t)$ 只有在 $|S_L(j,t)| \\ge \\ell_{\\min}$ 和 $|S_R(j,t)| \\ge \\ell_{\\min}$ 时才是可接受的。\n- 最大特征数 $m \\in \\mathbb{N}$，它确定性地将在每次分裂时可以考虑的特征集限制为按索引升序的前 $m$ 个特征，即 $\\{1,2,\\dots,\\min(m,p)\\}$。特征选择中没有随机性。\n\n建树规则：\n- 在每个符合条件的节点，在所有使用特征 $j \\in \\{1,2,\\dots,\\min(m,p)\\}$ 和阈值 $t$（选自在 $S$ 中观察到的连续不同特征值之间的中点）的可接受的分裂 $(j,t)$ 中，选择使 $\\Delta \\mathcal{I}(S;j,t)$ 最大化的分裂。如果 $\\Delta \\mathcal{I}$ 出现平局，则通过选择最小的特征索引 $j$ 来打破平局，如果仍然平局，则选择最小的阈值 $t$。如果没有可接受的分裂产生严格为正的不纯度减少量，则停止分裂并声明为叶节点。\n\n特征重要性：\n- 对于一个固定的超参数配置 $\\theta = (d_{\\max}, \\ell_{\\min}, m)$，将特征 $j$ 的未归一化特征重要性定义为\n$$\n\\widehat{I}_j(\\theta) \\triangleq \\sum_{\\text{splits on } j} \\Delta \\mathcal{I}(S;j,t),\n$$\n即在所有对特征 $j$ 进行分裂的内部分裂节点上的不纯度减少量之和。\n- 归一化每棵树的重要性，以便在不同超参数之间进行比较：\n$$\n\\widetilde{I}_j(\\theta) \\triangleq \\begin{cases}\n\\frac{\\widehat{I}_j(\\theta)}{\\sum_{k=1}^p \\widehat{I}_k(\\theta)},   \\text{if } \\sum_{k=1}^p \\widehat{I}_k(\\theta)  0,\\\\\n0,   \\text{otherwise.}\n\\end{cases}\n$$\n- 给定一个有限的超参数配置集 $\\Theta$，通过对配置进行平均来定义超参数鲁棒的重要性，\n$$\n\\overline{I}_j \\triangleq \\frac{1}{|\\Theta|} \\sum_{\\theta \\in \\Theta} \\widetilde{I}_j(\\theta),\n$$\n并将敏感性定义为跨配置的标准差，\n$$\n\\sigma_j \\triangleq \\sqrt{\\frac{1}{|\\Theta|} \\sum_{\\theta \\in \\Theta} \\left(\\widetilde{I}_j(\\theta) - \\overline{I}_j\\right)^2 }.\n$$\n\n确定性和数值细节：\n- 不得有任何随机性；特征和阈值的选择按规定是确定性的。\n- 在节点 $S$ 处，特征 $j$ 的阈值候选是 $\\{X_{ij}: i \\in S\\}$ 的排序后唯一值之间的中点。\n- 所有改进和总和都使用有限精度算术精确计算。如果不纯度减少量 $\\Delta \\mathcal{I}(S;j,t)  0$，则将其视为严格为正。\n- 报告数字时，四舍五入至6位小数。\n\n测试套件：\n为以下三个测试用例实现上述功能。对每个测试用例，计算向量 $\\overline{I} = (\\overline{I}_1, \\dots, \\overline{I}_p)$ 和向量 $\\sigma = (\\sigma_1, \\dots, \\sigma_p)$。\n\n- 测试用例 1：\n  - 数据 $n = 12$, $p = 3$：\n    - 特征矩阵 $X \\in \\mathbb{R}^{12 \\times 3}$，其行 $(x_{i1}, x_{i2}, x_{i3})$ 由下式给出\n      - 对于 $i \\in \\{1,2,3,4\\}$：$x_{i1} = 0$, $x_{i2}$ 依次取 $\\{0,1,2,3\\}$, $x_{i3}$ 依次取 $\\{1,0,1,0\\}$。\n      - 对于 $i \\in \\{5,6,7,8\\}$：$x_{i1} = 1$, $x_{i2}$ 依次取 $\\{0,1,2,3\\}$, $x_{i3}$ 依次取 $\\{1,0,1,0\\}$。\n      - 对于 $i \\in \\{9,10,11,12\\}$：$x_{i1} = 2$, $x_{i2}$ 依次取 $\\{0,1,2,3\\}$, $x_{i3}$ 依次取 $\\{1,0,1,0\\}$。\n    - 目标向量 $y \\in \\mathbb{R}^{12}$ 定义为 $y_i = 3 \\cdot x_{i1} + 2 \\cdot \\mathbf{1}\\{x_{i2} \\ge 1.5\\} + 0.1 \\cdot x_{i3}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n  - 超参数网格 $\\Theta_1$：\n    - $d_{\\max} \\in \\{1,2,3\\}$,\n    - $\\ell_{\\min} \\in \\{1,2\\}$,\n    - $m \\in \\{1,2,3\\}$。\n\n- 测试用例 2：\n  - 数据 $n = 4$, $p = 2$：\n    - $X$ 的行为 $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$。\n    - $y$ 的条目为 $0, 1, 1, 0$。\n  - 超参数网格 $\\Theta_2$：\n    - $d_{\\max} \\in \\{1\\}$,\n    - $\\ell_{\\min} \\in \\{1,2\\}$,\n    - $m \\in \\{1,2\\}$。\n\n- 测试用例 3：\n  - 数据 $n = 6$, $p = 3$：\n    - $X$ 的行为 $(0,0,0), (1,0,0), (0,1,0), (1,1,0), (0,0,1), (1,0,1)$。\n    - $y$ 的条目为 $0, 0, 0, 0, 5, 5$。\n  - 超参数网格 $\\Theta_3$：\n    - $d_{\\max} \\in \\{1,2\\}$,\n    - $\\ell_{\\min} \\in \\{1,2\\}$,\n    - $m \\in \\{1,2,3\\}$。\n\n程序要求：\n- 实现上述具有分裂和打破平局规则的确定性回归树学习器。\n- 对每个测试用例，计算指定网格中所有 $\\theta$ 的 $\\widetilde{I}_j(\\theta)$，然后计算 $\\overline{I}_j$ 和 $\\sigma_j$。\n- 所有输出均四舍五入至6位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、长度为3（每个测试用例一个条目）的逗号分隔列表。每个条目本身必须是两个列表的列表：第一个是 $\\overline{I}$，第二个是 $\\sigma$，两者都按特征索引顺序排列。例如：\n\"[[[a11,a12,...],[s11,s12,...]],[[a21,a22,...],[s21,s22,...]],[[a31,a32,...],[s31,s32,...]]]\"\n- 所有数字必须四舍五入至6位小数。不应打印其他任何文本。",
            "solution": "用户在统计学习领域提供了一个定义明确的计算问题。任务是实现一个确定性回归树学习器，用它在一个超参数网格上评估特征重要性，然后计算特征重要性及其敏感性的鲁棒估计。\n\n### **问题验证**\n\n该问题陈述经过了严格验证，并被确定为**有效的**。\n\n1.  **科学基础**：该问题基于标准的CART（分类与回归树）算法。不纯度（平方偏差之和）、不纯度减少量、递归划分和特征重要性（MDI - 平均不纯度减少量）等概念都是机器学习领域中基础且成熟的概念。\n2.  **适定性**：问题以高度精确的方式指定，确保了唯一解。促成这一点的关键细节包括：\n    *   **确定性特征选择**：在每个节点，用于分裂的特征集被确定性地固定为前 `m` 个特征。\n    *   **确定性阈值选择**：阈值候选被明确定义为连续唯一特征值之间的中点。\n    *   **明确的分裂标准**：最佳分裂的选择基于最大化不纯度减少量。\n    *   **严格的打破平局规则**：提供了一个清晰的打破平局层级结构（首先按最小特征索引 `j`，然后按最小阈值 `t`），这保证了在任何节点都有唯一的最佳分裂。\n    *   **明确的停止标准**：一个节点成为叶节点的条件（`d_max`、`l_min`、无正增益）被明确说明。\n3.  **完整性和一致性**：所有必要的数据（`X`, `y`）、定义（不纯度、重要性）、超参数（`d_max`, `l_min`, `m`）和评估指标（`\\overline{I}_j`, `\\sigma_j`）都已提供。三个测试用例都已完全指定。没有矛盾之处。\n\n该问题是一个形式化且可解的算法任务，没有任何提示中列出的使其无效的缺陷。\n\n### **方法论与算法设计**\n\n解决方案的核心是实现一个 `DeterministicRegressionTree` 类，该类根据指定的规则构建树并计算特征重要性。每个测试用例的总体流程如下：\n\n1.  **遍历超参数网格**：对于每个测试用例，我们遍历指定网格 `\\Theta` 中的每个超参数配置 `\\theta = (d_{\\max}, \\ell_{\\min}, m)`。\n2.  **构建树并计算未归一化的重要性**：对于每个 `\\theta`，在数据集 `(X, y)` 上构建一个回归树。树的构建是迭代的，从根节点开始，使用一个待处理节点的队列进行。\n    *   **节点处理**：对于队列中的每个节点，我们检查停止标准（$d \\ge d_{\\max}$，$|S|  2\\ell_{\\min}$）。如果节点有资格进行分裂，我们搜索最佳分裂。\n    *   **寻找最佳分裂**：这是最关键的步骤。为了找到节点 `S` 的最佳分裂 `(j^*, t^*)`，我们遍历允许的特征 $j \\in \\{1, \\dots, \\min(m, p)\\}$ 及其对应的阈值候选。对于每个潜在的分裂，我们计算不纯度减少量 $\\Delta\\mathcal{I}(S; j, t)$。不纯度定义为 $\\mathcal{I}(S) = \\sum_{i \\in S} (y_i - \\bar{y}_S)^2$。为了数值稳定性和效率，这被计算为 $\\mathcal{I}(S) = \\sum_{i \\in S} y_i^2 - (\\sum_{i \\in S} y_i)^2 / |S|$。\n    *   **打破平局**：最佳分裂的选择必须严格遵守打破平局规则。目标是找到 `argmax_{j,t} \\Delta\\mathcal{I}`，在出现平局时优先选择较小的 `j`，然后是较小的 `t`。这是通过找到按字典序最大化元组 $(\\Delta\\mathcal{I}(S; j, t), -j, -t)$ 的分裂 $(j, t)$ 来实现的。\n    *   **更新重要性**：如果找到了一个具有严格正增益的有效分裂，其不纯度减少量 $\\Delta\\mathcal{I}$ 会被加到用于分裂的特征 `j` 的总未归一化重要性 $\\widehat{I}_j(\\theta)$ 中。然后将两个产生的子节点添加到处理队列中。\n3.  **归一化重要性**：为一个配置 $\\theta$ 完全构建树后，原始重要性 $\\widehat{I}_j(\\theta)$ 被归一化使其总和为一，得到 $\\widetilde{I}_j(\\theta)$。如果原始重要性的总和为零（即没有进行任何分裂），则归一化重要性全部为零。\n4.  **汇总结果**：将所有配置的归一化重要性 `\\{\\widetilde{I}(\\theta)\\}_{\\theta \\in \\Theta}` 收集起来。\n5.  **计算最终指标**：通过对 $\\Theta$ 中所有配置进行平均，为每个特征 `j` 计算超参数鲁棒的重要性 $\\overline{I}_j$（均值）和敏感性 $\\sigma_j$（标准差）。\n\n这个分步、确定性的过程在 Python 中使用 `numpy` 库进行数值计算。根据要求，所有结果都四舍五入到六位小数以用于最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fast_impurity(y: np.ndarray) -> float:\n    \"\"\"\n    Calculates impurity for a set of target values using the computationally\n    efficient formula: Var(y) * n = sum(y^2) - (sum(y))^2 / n.\n    \"\"\"\n    n = y.shape[0]\n    if n == 0:\n        return 0.0\n    # Use 64-bit floats for precision in sums\n    y_f64 = y.astype(np.float64)\n    sum_y = np.sum(y_f64)\n    sum_y_sq = np.sum(y_f64**2)\n    return sum_y_sq - (sum_y * sum_y) / n\n\nclass DeterministicRegressionTree:\n    \"\"\"\n    A deterministic regression tree learner built according to the problem's specifications.\n    It builds a tree and calculates feature importances based on impurity decrease.\n    \"\"\"\n\n    def __init__(self, d_max: int, l_min: int, m: int):\n        self.d_max = d_max\n        self.l_min = l_min\n        self.m = m\n        self.n = 0\n        self.p = 0\n        self.feature_importances_ = None\n\n    def fit(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"Builds the tree and computes feature importances.\"\"\"\n        self.n, self.p = X.shape\n        self.feature_importances_ = np.zeros(self.p, dtype=np.float64)\n        root_indices = np.arange(self.n)\n        \n        # Use a list as a queue for breadth-first tree construction\n        q = [(root_indices, 0)]  # (indices, depth)\n        \n        while q:\n            indices, depth = q.pop(0)\n\n            # Stopping Condition 1: Maximum depth reached\n            if depth >= self.d_max:\n                continue\n            \n            # Stopping Condition 2: Not enough samples to create a valid split\n            if indices.shape[0]  2 * self.l_min:\n                continue\n\n            # Stopping Condition 3: Node is pure (all targets are identical)\n            node_y = y[indices]\n            if np.all(node_y == node_y[0]):\n                continue\n            \n            # Find the best possible split for the current node\n            best_split = self._find_best_split(indices, X, y)\n            \n            if best_split is not None:\n                j, gain, left_indices, right_indices = best_split\n                \n                self.feature_importances_[j] += gain\n                \n                q.append((left_indices, depth + 1))\n                q.append((right_indices, depth + 1))\n\n    def _find_best_split(self, indices: np.ndarray, X: np.ndarray, y: np.ndarray):\n        \"\"\"\n        Finds the best split for a node to maximize impurity decrease,\n        respecting the specified deterministic tie-breaking rules.\n        \"\"\"\n        # We maximize (gain, -j, -t) lexicographically.\n        best_objective = (-1.0, 0.0, 0.0)\n        best_split_info = None\n\n        node_y = y[indices]\n        impurity_node = fast_impurity(node_y)\n        \n        if impurity_node  1e-12: # Effectively pure\n            return None\n\n        # Iterate through the first `m` features\n        for j in range(min(self.m, self.p)):\n            node_X_j = X[indices, j]\n            \n            unique_vals = np.unique(node_X_j)\n            if unique_vals.shape[0]  2:\n                continue\n            \n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n            \n            for t in thresholds:\n                left_mask = node_X_j = t\n                \n                n_left = np.sum(left_mask)\n                n_right = indices.shape[0] - n_left\n\n                if n_left  self.l_min or n_right  self.l_min:\n                    continue\n                \n                y_left = node_y[left_mask]\n                y_right = node_y[~left_mask]\n                impurity_left = fast_impurity(y_left)\n                impurity_right = fast_impurity(y_right)\n\n                gain = impurity_node - (impurity_left + impurity_right)\n                \n                current_objective = (gain, -j, -t)\n                \n                if current_objective > best_objective:\n                    best_objective = current_objective\n                    left_indices = indices[left_mask]\n                    right_indices = indices[~left_mask]\n                    best_split_info = (j, gain, left_indices, right_indices)\n        \n        # A split is only valid if it yields a strictly positive gain.\n        if best_objective[0] > 1e-12:\n            return best_split_info\n        else:\n            return None\n\ndef solve():\n    \"\"\"Main function to run all test cases and print the final result.\"\"\"\n    \n    # --- Test Case 1 Setup ---\n    X1 = np.zeros((12, 3))\n    y1 = np.zeros(12)\n    idx = 0\n    for x1_val in [0, 1, 2]:\n        for i, x2_val in enumerate([0, 1, 2, 3]):\n            x3_val = [1, 0, 1, 0][i]\n            X1[idx, :] = [x1_val, x2_val, x3_val]\n            y1[idx] = 3 * x1_val + 2 * (1 if x2_val >= 1.5 else 0) + 0.1 * x3_val\n            idx += 1\n    Theta1 = []\n    for d_max in [1, 2, 3]:\n        for l_min in [1, 2]:\n            for m in [1, 2, 3]:\n                Theta1.append((d_max, l_min, m))\n\n    # --- Test Case 2 Setup ---\n    X2 = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float64)\n    y2 = np.array([0, 1, 1, 0], dtype=np.float64)\n    Theta2 = []\n    for d_max in [1]:\n        for l_min in [1, 2]:\n            for m in [1, 2]:\n                Theta2.append((d_max, l_min, m))\n\n    # --- Test Case 3 Setup ---\n    X3 = np.array([[0,0,0], [1,0,0], [0,1,0], [1,1,0], [0,0,1], [1,0,1]], dtype=np.float64)\n    y3 = np.array([0, 0, 0, 0, 5, 5], dtype=np.float64)\n    Theta3 = []\n    for d_max in [1, 2]:\n        for l_min in [1, 2]:\n            for m in [1, 2, 3]:\n                Theta3.append((d_max, l_min, m))\n\n    test_cases = [\n        (X1, y1, Theta1),\n        (X2, y2, Theta2),\n        (X3, y3, Theta3),\n    ]\n\n    all_results = []\n    for X, y, Theta in test_cases:\n        p = X.shape[1]\n        all_norm_importances = []\n\n        for d_max, l_min, m in Theta:\n            tree = DeterministicRegressionTree(d_max=d_max, l_min=l_min, m=m)\n            tree.fit(X, y)\n            unnorm_I = tree.feature_importances_\n            \n            total_I = np.sum(unnorm_I)\n            if total_I > 0:\n                norm_I = unnorm_I / total_I\n            else:\n                norm_I = np.zeros(p)\n            all_norm_importances.append(norm_I)\n\n        importances_array = np.array(all_norm_importances)\n        mean_I = np.mean(importances_array, axis=0)\n        std_I = np.std(importances_array, axis=0)\n        \n        all_results.append((mean_I, std_I))\n\n    # Format the final output string exactly as specified\n    case_strings = []\n    for mean_I, std_I in all_results:\n        mean_list_str = [f\"{x:.6f}\" for x in mean_I]\n        std_list_str = [f\"{x:.6f}\" for x in std_I]\n        case_str = f\"[[{','.join(mean_list_str)}],[{','.join(std_list_str)}]]\"\n        case_strings.append(case_str)\n        \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```"
        }
    ]
}