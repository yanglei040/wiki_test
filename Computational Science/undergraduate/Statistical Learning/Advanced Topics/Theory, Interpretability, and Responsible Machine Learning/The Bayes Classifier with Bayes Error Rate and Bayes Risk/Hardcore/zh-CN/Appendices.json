{
    "hands_on_practices": [
        {
            "introduction": "本节的第一个练习将引导你直接应用贝叶斯风险的定义。在这个设想的场景中，我们直接给出了后验概率函数 $\\eta(x)$，这使得问题回归其核心：计算给定模型下的预期错误率。通过这个练习 ，你将巩固计算贝叶斯风险的基本公式，并将其与机器学习中无处不在的逻辑斯谛模型（logistic model）联系起来。",
            "id": "3180161",
            "problem": "考虑一个二元分类场景，其特征 $X \\in \\mathbb{R}$ 服从标准正态分布 $X \\sim \\mathcal{N}(0,1)$，类别 $Y=1$ 的后验概率函数由 $\\eta(x) = \\sigma(a x + b)$ 给出，其中 $\\sigma(z)$ 是 logistic sigmoid 函数，定义为 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$，且 $a \\in \\mathbb{R}$，$b \\in \\mathbb{R}$ 为参数。在 $0$-$1$ 损失下，贝叶斯分类器在点 $x$ 处预测后验概率较高的类别，而贝叶斯风险是使用此分类器时的期望错分概率。\n\n仅从 $0$-$1$ 损失下贝叶斯分类器和贝叶斯风险的核心定义出发，推导出贝叶斯风险作为关于 $X$ 的期望的适当表达式，并实现一个数值程序，以针对给定的 $\\eta(x)$ 和 $X \\sim \\mathcal{N}(0,1)$ 计算该风险。你的程序必须：\n- 使用在 $x \\in \\mathbb{R}$ 上关于标准正态密度的精确数值积分，来为每个参数对 $(a,b)$ 评估贝叶斯风险。\n- 生成浮点数输出，将贝叶斯风险值表示为小数（而非百分比）。\n- 将每个计算出的贝叶斯风险四舍五入到 $10$ 位小数，作为最终报告的值。\n\n测试套件：\n为以下参数对 $(a,b)$ 计算贝叶斯风险：\n- 情况 1：$(a,b) = (1.0, 0.0)$。\n- 情况 2：$(a,b) = (0.0, 0.0)$。\n- 情况 3：$(a,b) = (0.0, 2.0)$。\n- 情况 4：$(a,b) = (5.0, 0.0)$。\n- 情况 5：$(a,b) = (1.5, 2.0)$。\n- 情况 6：$(a,b) = (-1.5, -2.0)$。\n- 情况 7：$(a,b) = (-2.0, 1.0)$。\n\n最终输出格式：\n你的程序应生成单行输出，按顺序包含上述情况的结果，形式为方括号内以逗号分隔的列表（例如，$[r_1,r_2,r_3,r_4,r_5,r_6,r_7]$），其中每个 $r_i$ 是情况 $i$ 的贝叶斯风险，四舍五入到 $10$ 位小数。",
            "solution": "用户需要一个细致、基于原则的解决方案，从问题验证开始。\n\n### 步骤 1：提取已知条件\n- **场景**：二元分类。\n- **特征**：$X \\in \\mathbb{R}$。\n- **特征分布**：$X \\sim \\mathcal{N}(0,1)$（标准正态分布）。\n- **类别 $Y=1$ 的后验概率**：$\\eta(x) = P(Y=1|X=x) = \\sigma(ax+b)$。\n- **Logistic Sigmoid 函数**：$\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n- **参数**：$a \\in \\mathbb{R}$，$b \\in \\mathbb{R}$。\n- **损失函数**：$0-1$ 损失。\n- **贝叶斯分类器定义**：预测后验概率较高的类别。\n- **贝叶斯风险定义**：贝叶斯分类器的期望错分概率。\n- **任务**：推导贝叶斯风险的表达式并实现一个数值程序来计算它。\n- **数值方法**：在 $x \\in \\mathbb{R}$ 上关于标准正态密度的精确数值积分。\n- **测试用例**：为以下参数对 $(a,b)$ 进行计算：\n    1. $(1.0, 0.0)$\n    2. $(0.0, 0.0)$\n    3. $(0.0, 2.0)$\n    4. $(5.0, 0.0)$\n    5. $(1.5, 2.0)$\n    6. $(-1.5, -2.0)$\n    7. $(-2.0, 1.0)$\n- **输出要求**：浮点数值，四舍五入到 $10$ 位小数，格式为方括号内的逗号分隔列表。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学或事实的合理性**：该问题使用统计学习理论中标准、成熟的概念进行阐述，包括贝叶斯分类器、贝叶斯风险、后验概率、logistic sigmoid 函数和正态分布。该设置代表了一个经典的逻辑回归模型。该问题在科学上是合理的。\n2.  **非形式化或不相关**：该问题是一个形式化的数学和计算任务，直接关系到贝叶斯分类器及其相关风险这一特定主题。它不是隐喻性的或不相关的。\n3.  **不完整或矛盾的设置**：所有必要的组成部分都已提供：特征分布、后验概率的形式、损失函数以及待计算量的定义。该问题是自洽且一致的。\n4.  **不切实际或不可行**：该模型是统计学和机器学习中的标准模型。所需的计算涉及对一个表现良好的函数进行数值积分，这是一个标准且可行的数值任务。\n5.  **不适定或结构不良**：对于任何给定的参数 $(a,b)$，贝叶斯风险是一个唯一定义的量。该问题是适定的且结构清晰。\n6.  **伪深刻、琐碎或同义反复**：该问题需要从第一性原理进行正确推导和非平凡的数值计算。虽然某些测试用例（例如，$a=0$）会产生更简单的解析结果，但一般情况不会，需要真正应用数值方法。\n7.  **超出科学可验证性**：该推导可进行数学证明，数值结果可通过独立计算进行验证。\n\n### 步骤 3：结论和行动\n问题有效。将提供完整的解决方案。\n\n### 推导和方法\n\n目标是计算贝叶斯风险，它是在二元分类问题中期望 $0-1$ 损失的最小可能值。\n\n1.  **贝叶斯分类器规则**：$0-1$ 损失函数定义为 $L(y, \\hat{y}) = I(y \\neq \\hat{y})$，其中 $I(\\cdot)$ 是指示函数。目标是找到一个分类器 $g(x)$ 来最小化风险 $R(g) = E[L(Y, g(X))]$。这通过为每个 $x$ 最小化条件风险 $E[L(Y, g(X))|X=x]$ 来实现。\n\n    对于一个预测 $\\hat{y}$，在 $x$ 处的条件风险是：\n    $$ E[L(Y, \\hat{y})|X=x] = P(Y \\neq \\hat{y} | X=x) $$\n    如果我们预测类别 $1$（即 $\\hat{y}=1$），条件风险是 $P(Y=0|X=x) = 1 - P(Y=1|X=x) = 1 - \\eta(x)$。\n    如果我们预测类别 $0$（即 $\\hat{y}=0$），条件风险是 $P(Y=1|X=x) = \\eta(x)$。\n\n    贝叶斯分类器 $g^*(x)$ 通过选择具有较高后验概率的类别来最小化此条件风险：\n    $$ g^*(x) = \\begin{cases} 1  \\text{若 } \\eta(x) \\geq 1 - \\eta(x) \\\\ 0  \\text{若 } \\eta(x)  1 - \\eta(x) \\end{cases} $$\n    条件 $\\eta(x) \\geq 1 - \\eta(x)$ 可简化为 $\\eta(x) \\geq 0.5$。\n\n2.  **贝叶斯风险计算**：与贝叶斯分类器相关的风险是贝叶斯风险，记为 $R^*$。当使用贝叶斯分类器时，在点 $x$ 处的条件风险是较不可能类别的概率，即 $\\min\\{\\eta(x), 1-\\eta(x)\\}$。\n\n    贝叶斯风险是此条件风险在 $X$ 的分布上的期望：\n    $$ R^* = E_X[\\min\\{\\eta(X), 1-\\eta(X)\\}] $$\n    假定 $X$ 服从一个概率密度函数 (PDF) 为 $p(x)$ 的分布，则此期望由以下积分给出：\n    $$ R^* = \\int_{-\\infty}^{\\infty} \\min\\{\\eta(x), 1-\\eta(x)\\} p(x) dx $$\n\n3.  **应用于具体问题**：\n    - 特征 $X$ 服从标准正态分布，$X \\sim \\mathcal{N}(0,1)$，因此其 PDF 为 $p(x) = \\phi(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}$。\n    - 后验概率为 $\\eta(x) = \\sigma(ax+b) = \\frac{1}{1 + e^{-(ax+b)}}$。\n\n    我们需要简化项 $\\min\\{\\eta(x), 1-\\eta(x)\\}$。注意，logistic 函数 $\\sigma(z)$ 是单调递增的，当 $z0$ 时 $\\sigma(z)  0.5$，$\\sigma(0)=0.5$，当 $z>0$ 时 $\\sigma(z) > 0.5$。\n    因此，$\\eta(x) = \\sigma(ax+b) \\geq 0.5$ 当且仅当其参数 $ax+b \\geq 0$。\n\n    - 若 $ax+b \\geq 0$，则 $\\eta(x) \\geq 0.5$，所以 $\\min\\{\\eta(x), 1-\\eta(x)\\} = 1-\\eta(x)$。\n    - 若 $ax+b  0$，则 $\\eta(x)  0.5$，所以 $\\min\\{\\eta(x), 1-\\eta(x)\\} = \\eta(x)$。\n\n    我们可以使用恒等式 $1 - \\sigma(z) = 1 - \\frac{1}{1+e^{-z}} = \\frac{e^{-z}}{1+e^{-z}} = \\frac{1}{e^z+1} = \\sigma(-z)$。\n    所以，$1-\\eta(x) = 1-\\sigma(ax+b) = \\sigma(-(ax+b))$。\n    因此，条件风险可以紧凑地写为：\n    $$ \\min\\{\\eta(x), 1-\\eta(x)\\} = \\begin{cases} \\sigma(-(ax+b))  \\text{若 } ax+b \\geq 0 \\\\ \\sigma(ax+b)  \\text{若 } ax+b  0 \\end{cases} $$\n    这等价于 $\\sigma(-|ax+b|)$。贝叶斯风险的表达式变为：\n    $$ R^* = \\int_{-\\infty}^{\\infty} \\sigma(-|ax+b|) \\phi(x) dx $$\n    代入 $\\sigma$ 和 $\\phi$ 的函数形式：\n    $$ R^* = \\int_{-\\infty}^{\\infty} \\frac{1}{1 + e^{|ax+b|}} \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx $$\n\n4.  **数值评估**：该积分没有通用的闭式解析解，必须进行数值评估。我们将使用数值积分法来计算在域 $(-\\infty, \\infty)$ 上的定积分。Python 中的 `scipy.integrate.quad` 函数非常适合此目的。对于测试套件中提供的每一对参数 $(a,b)$，我们将定义被积函数 $f(x;a,b) = \\frac{\\phi(x)}{1 + e^{|ax+b|}}$ 并计算积分 $\\int_{-\\infty}^{\\infty} f(x;a,b) dx$。结果将按要求四舍五入到 $10$ 位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the Bayes risk for a binary classification problem with a normally\n    distributed feature and a logistic posterior probability function for\n    several sets of parameters.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 0.0),   # Case 1\n        (0.0, 0.0),   # Case 2\n        (0.0, 2.0),   # Case 3\n        (5.0, 0.0),   # Case 4\n        (1.5, 2.0),   # Case 5\n        (-1.5, -2.0), # Case 6\n        (-2.0, 1.0)   # Case 7\n    ]\n\n    results = []\n    \n    for a, b in test_cases:\n        # The Bayes risk R* is the expectation of the minimum of the posterior\n        # probabilities for the two classes, E[min(eta(X), 1-eta(X))], where\n        # X ~ N(0,1). This expectation is calculated by integrating the\n        # conditional risk over the distribution of X.\n        #\n        # Conditional risk at x: min(eta(x), 1-eta(x))\n        #   eta(x) = sigma(ax+b) = 1 / (1 + exp(-(ax+b)))\n        #   1-eta(x) = sigma(-(ax+b))\n        #   min(eta(x), 1-eta(x)) = sigma(-|ax+b|) = 1 / (1 + exp(|ax+b|))\n        #\n        # Distribution of X is standard normal, with PDF phi(x).\n        # Integrand for Bayes risk: (1 / (1 + exp(|ax+b|))) * phi(x)\n\n        # Define the integrand function for numerical integration.\n        # It takes x, a, and b as arguments.\n        def integrand(x, a_val, b_val):\n            # Calculate the conditional error probability\n            # sigma(-|ax+b|) = 1 / (1 + exp(|ax+b|))\n            conditional_risk = 1.0 / (1.0 + np.exp(np.abs(a_val * x + b_val)))\n            \n            # Multiply by the standard normal PDF\n            # norm.pdf(x, 0, 1) provides phi(x)\n            return conditional_risk * norm.pdf(x, loc=0, scale=1)\n\n        # Perform numerical integration over (-inf, +inf)\n        # quad returns a tuple (integral_value, error_estimate)\n        bayes_risk, _ = quad(integrand, -np.inf, np.inf, args=(a, b))\n        \n        # Round the result to 10 decimal places as specified\n        rounded_risk = round(bayes_risk, 10)\n        results.append(rounded_risk)\n\n    # Format the final output string as [r1,r2,...,r7]\n    # Using .10f format specifier to avoid scientific notation for small numbers\n    # and ensure trailing zeros if necessary.\n    formatted_results = [f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了从后验概率直接计算风险的方法后，下一个练习将带你更进一步。在这里，我们将从更基本的元素——类条件概率密度（以Beta分布为例）和先验概率——开始构建贝叶斯分类器，这更贴近于实际的建模过程。此外，这个练习  的一个关键部分是计算贝叶斯误差，并将其与理论上界（Bhattacharyya界和平凡界）进行比较，这对于理解分类器性能的理论极限至关重要。",
            "id": "3180189",
            "problem": "您需要实现一个程序，该程序使用 Beta 分布作为类条件密度，计算在单位区间上进行二元分类的贝叶斯错误率，并将其与两个界限进行比较。请基于贝叶斯决策理论的基本原理和 $0$-$1$ 损失进行计算。所有量都应以纯数学形式表示，不带任何物理单位。\n\n使用的定义如下：\n- 两个类别 $C_1$ 和 $C_2$ 的先验概率分别为 $\\pi_1$ 和 $\\pi_2$，满足 $\\pi_1 + \\pi_2 = 1$，其类条件密度 $f_1(x)$ 和 $f_2(x)$ 定义在 $[0,1]$ 上。\n- 贝叶斯分类器在每个 $x \\in [0,1]$ 处选择后验概率较大的类别。\n- 在 $0$-$1$ 损失下，贝叶斯风险等于贝叶斯错误率 $L^\\star$，即所有分类器中可能达到的最小错分概率。它等于以下积分：\n$$\nL^\\star \\;=\\; \\int_0^1 \\min\\!\\big(\\pi_1 f_1(x), \\pi_2 f_2(x)\\big)\\,dx.\n$$\n- Bhattacharyya 系数为：\n$$\n\\rho \\;=\\; \\int_0^1 \\sqrt{f_1(x) f_2(x)}\\,dx,\n$$\n而 Bhattacharyya 界给出：\n$$\nL^\\star \\;\\le\\; \\sqrt{\\pi_1 \\pi_2}\\;\\rho.\n$$\n- 一个平凡上界是 $L^\\star \\le \\min(\\pi_1,\\pi_2)$，它对应于一个总是预测多数类的常数分类器。\n\n在本问题中，类条件密度是定义在 $[0,1]$ 上的 Beta 分布，其各自的形状参数为 $(\\alpha_1,\\beta_1)$ 和 $(\\alpha_2,\\beta_2)$：\n$$\nf_k(x) \\;=\\; \\frac{x^{\\alpha_k - 1} (1-x)^{\\beta_k - 1}}{B(\\alpha_k,\\beta_k)},\\quad x\\in[0,1],\\quad \\alpha_k>0,\\;\\beta_k>0,\\quad k\\in\\{1,2\\},\n$$\n其中 $B(\\cdot,\\cdot)$ 是 Beta 函数。您必须通过在 $[0,1]$ 上进行数值积分来计算 $L^\\star$。您还必须通过数值积分计算 Bhattacharyya 系数 $\\rho$，然后报告 Bhattacharyya 上界 $\\sqrt{\\pi_1\\pi_2}\\,\\rho$。最后，计算平凡上界 $\\min(\\pi_1,\\pi_2)$。\n\n实现要求：\n- 对被积函数 $\\min(\\pi_1 f_1(x), \\pi_2 f_2(x))$ 和 $\\sqrt{f_1(x) f_2(x)}$ 实现 $[0,1]$ 上的数值积分，要求精度足够高，以产生至少精确到小数点后 $6$ 位的稳定结果。\n- 对于每个测试用例，输出一个包含三个浮点数的列表（四舍五入到小数点后 $6$ 位）：$[L^\\star, \\sqrt{\\pi_1\\pi_2}\\,\\rho, \\min(\\pi_1,\\pi_2)]$。\n\n测试套件：\n计算以下四组参数集的输出，每组参数的形式为 $(\\pi_1,\\alpha_1,\\beta_1,\\pi_2,\\alpha_2,\\beta_2)$。\n- 情况 $1$ (平衡先验，镜像形状)：$(0.5, 2, 5, 0.5, 5, 2)$。\n- 情况 $2$ (不等先验，中等重叠)：$(0.7, 2, 2, 0.3, 5, 5)$。\n- 情况 $3$ (相同分布，不等先验；边界情况)：$(0.6, 3, 4, 0.4, 3, 4)$。在这种情况下，精确计算时 $L^\\star$ 应等于 $\\min(0.6,0.4)$。\n- 情况 $4$ (平衡先验，高度集中在端点附近)：$(0.5, 0.5, 5, 0.5, 5, 0.5)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由列表组成的列表，用逗号分隔，每个内部列表按上述顺序对应一个测试用例。例如，精确格式应为：\n- $[[L^\\star_1, U_{B,1}, U_{T,1}], [L^\\star_2, U_{B,2}, U_{T,2}], [L^\\star_3, U_{B,3}, U_{T,3}], [L^\\star_4, U_{B,4}, U_{T,4}]]$\n其中 $U_{B,i} = \\sqrt{\\pi_{1,i}\\pi_{2,i}}\\,\\rho_i$ 且 $U_{T,i} = \\min(\\pi_{1,i},\\pi_{2,i})$，每个浮点值都四舍五入到小数点后 $6$ 位，并且不打印任何额外文本。",
            "solution": "用户提供的问题被评估为有效，因为它在贝叶斯决策理论中有科学依据，提法明确，并包含一套完整且一致的定义和参数。任务是计算在单位区间上的一个二元分类问题的贝叶斯错误率及其两个上界，其中类条件密度由 Beta 分布建模。解将通过数值积分获得。\n\n该问题涉及两个类别 $C_1$ 和 $C_2$，其先验概率分别为 $\\pi_1$ 和 $\\pi_2$，且 $\\pi_1 + \\pi_2 = 1$。类条件概率密度函数 $f_1(x)$ 和 $f_2(x)$ 定义在区间 $x \\in [0,1]$ 上。\n\n根据贝叶斯决策理论，对于给定的观测值 $x$，类别 $C_k$ 的后验概率由贝叶斯定理给出：\n$$\nP(C_k|x) = \\frac{\\pi_k f_k(x)}{f(x)} = \\frac{\\pi_k f_k(x)}{\\pi_1 f_1(x) + \\pi_2 f_2(x)}\n$$\n贝叶斯分类器通过将 $x$ 分配给后验概率最高的类别来最小化错分概率。这等价于选择使量 $\\pi_k f_k(x)$ 最大化的类别 $C_k$。\n\n最小错误概率，即贝叶斯错误率 $L^\\star$，是由贝叶斯分类器实现的。对于 $0$-$1$ 损失函数下的二元分类问题，贝叶斯风险等于贝叶斯错误率。在点 $x$ 处的错误是少数类的后验概率，即 $\\min(P(C_1|x), P(C_2|x))$。将此错误在所有可能的 $x$ 值上积分，得到总贝叶斯错误率：\n$$\nL^\\star = \\int_0^1 \\min(P(C_1|x), P(C_2|x)) f(x) \\,dx = \\int_0^1 \\min(\\pi_1 f_1(x), \\pi_2 f_2(x)) \\,dx\n$$\n这是要计算的第一个量。\n\n该问题要求与两个上界进行比较。第一个是 Bhattacharyya 界。它基于 Bhattacharyya 系数 $\\rho$，该系数衡量了两个概率分布之间的重叠程度：\n$$\n\\rho = \\int_0^1 \\sqrt{f_1(x) f_2(x)} \\,dx\n$$\n贝叶斯错误的 Bhattacharyya 界由下式给出：\n$$\nL^\\star \\le \\sqrt{\\pi_1 \\pi_2} \\, \\rho\n$$\n这是要计算的第二个量。\n\n第二个更简单的上界是平凡界，它是一个忽略数据 $x$ 并总是预测多数类的分类器的错误率。此类分类器的错误率是少数类的先验概率：\n$$\nL^\\star \\le \\min(\\pi_1, \\pi_2)\n$$\n这是要计算的第三个量。\n\n类条件密度被指定为具有形状参数 $(\\alpha_k, \\beta_k)$ 的 Beta 分布：\n$$\nf_k(x) = \\frac{x^{\\alpha_k - 1} (1-x)^{\\beta_k - 1}}{B(\\alpha_k, \\beta_k)}, \\quad x \\in [0,1], \\quad \\alpha_k > 0, \\beta_k > 0\n$$\n其中 $B(\\alpha_k, \\beta_k)$ 是 Beta 函数，它作为归一化常数，确保 $\\int_0^1 f_k(x)\\,dx = 1$。\n\n$L^\\star$ 和 $\\rho$ 的计算需要求解一维积分，其被积函数由 Beta 概率密度函数构成。由于这些积分通常没有封闭形式的解析解，因此必须进行数值积分。为此，一个标准的数值求积方法是合适的，例如 `scipy.integrate.quad` 提供的那个。当 Beta 分布的参数 $\\alpha_k$ 或 $\\beta_k$ 小于 1 时，该函数可以处理可能在区间 $[0,1]$ 端点处出现的可积奇点。\n\n对于由参数集 $(\\pi_1, \\alpha_1, \\beta_1, \\pi_2, \\alpha_2, \\beta_2)$ 指定的每个测试用例，步骤如下：\n1.  使用给定参数定义两个 Beta PDF 函数 $f_1(x)$ 和 $f_2(x)$。\n2.  在区间 $[0,1]$ 上对函数 $g(x) = \\min(\\pi_1 f_1(x), \\pi_2 f_2(x))$ 进行数值积分，以获得贝叶斯错误率 $L^\\star$。\n3.  在 $[0,1]$ 上对函数 $h(x) = \\sqrt{f_1(x) f_2(x)}$ 进行数值积分，以获得 Bhattacharyya 系数 $\\rho$。\n4.  计算 Bhattacharyya 界为 $\\sqrt{\\pi_1 \\pi_2} \\, \\rho$。\n5.  计算平凡界为 $\\min(\\pi_1, \\pi_2)$。\n6.  收集这三个值 $[L^\\star, \\sqrt{\\pi_1 \\pi_2} \\, \\rho, \\min(\\pi_1, \\pi_2)]$ 并按要求格式化。\n\n一个特殊情况是情况 3，其中 $(\\alpha_1, \\beta_1) = (\\alpha_2, \\beta_2)$，意味着 $f_1(x) = f_2(x)$。$L^\\star$ 的被积函数变为 $\\min(\\pi_1, \\pi_2) f_1(x)$。由于 $\\int_0^1 f_1(x) dx = 1$，贝叶斯错误率简化为 $L^\\star = \\min(\\pi_1, \\pi_2)$，这必须与平凡界相匹配。这为实现的正确性提供了一个有价值的检验。\n\n另一个有趣的情况是情况 1，其中分布是镜像的，$f_2(x) = f_1(1-x)$，且先验是平衡的，$\\pi_1=\\pi_2=0.5$。决策边界在 $x=0.5$ 处，根据对称性，贝叶斯错误为 $L^\\star = \\int_{0.5}^1 \\pi_1 f_1(x) dx$。\n\n实现将通过定义被积函数并使用数值积分例程来计算测试套件中提供的每组参数的定积分。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Computes the Bayes error rate, Bhattacharyya bound, and trivial bound\n    for a binary classification problem with Beta class-conditional densities.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (pi1, alpha1, beta1, pi2, alpha2, beta2)\n    test_cases = [\n        (0.5, 2, 5, 0.5, 5, 2),        # Case 1: balanced priors, mirror shapes\n        (0.7, 2, 2, 0.3, 5, 5),        # Case 2: unequal priors, moderate overlap\n        (0.6, 3, 4, 0.4, 3, 4),        # Case 3: identical distributions, unequal priors\n        (0.5, 0.5, 5, 0.5, 5, 0.5),    # Case 4: balanced priors, high concentration\n    ]\n\n    all_results = []\n    for case in test_cases:\n        pi1, a1, b1, pi2, a2, b2 = case\n\n        # Define the class-conditional density functions (PDFs) for the two Beta distributions.\n        # The scipy.stats.beta class provides an object representing the distribution.\n        # Its pdf method evaluates the probability density function.\n        f1 = beta(a=a1, b=b1).pdf\n        f2 = beta(a=a2, b=b2).pdf\n\n        # --- 1. Compute the Bayes error rate (L_star) ---\n        # The integrand is min(pi1*f1(x), pi2*f2(x)).\n        integrand_L_star = lambda x: np.minimum(pi1 * f1(x), pi2 * f2(x))\n        \n        # Perform numerical integration over the interval [0, 1].\n        # quad returns the integral result and an error estimate; we only need the result.\n        L_star, _ = quad(integrand_L_star, 0, 1, epsabs=1e-12, epsrel=1e-12)\n\n        # --- 2. Compute the Bhattacharyya bound ---\n        # First, compute the Bhattacharyya coefficient (rho).\n        # The integrand is sqrt(f1(x) * f2(x)).\n        integrand_rho = lambda x: np.sqrt(f1(x) * f2(x))\n        rho, _ = quad(integrand_rho, 0, 1, epsabs=1e-12, epsrel=1e-12)\n        \n        # The Bhattacharyya bound is sqrt(pi1 * pi2) * rho.\n        bhattacharyya_bound = np.sqrt(pi1 * pi2) * rho\n\n        # --- 3. Compute the trivial upper bound ---\n        # The trivial bound is the error of always guessing the majority class,\n        # which is the prior of the minority class.\n        trivial_bound = min(pi1, pi2)\n        \n        # Store the three computed values for this case.\n        results_for_case = [L_star, bhattacharyya_bound, trivial_bound]\n        all_results.append(results_for_case)\n\n    # --- Format the final output string as specified ---\n    # The output must be a single-line string representation of a list of lists,\n    # with each float rounded to 6 decimal places.\n    result_strings = []\n    for inner_list in all_results:\n        # Format each float to 6 decimal places.\n        formatted_numbers = [f\"{x:.6f}\" for x in inner_list]\n        # Create the string for the inner list, e.g., \"[0.123456,0.789012,0.345678]\"\n        result_strings.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the inner list strings into the final output string.\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这个实践练习旨在探索更复杂和贴近现实的分类情境。它通过一个高斯混合模型的例子，生动地展示了即使基础分布（高斯分布）很简单，它们的组合也可以产生复杂的、非凸的决策边界，这打破了我们基于简单模型的直觉。此外，该练习  明确要求你区分对称 $0$-$1$ 损失下的贝叶斯误差率和非对称损失下的贝叶斯风险，这在不同类型错误会带来不同后果的实际应用中至关重要。",
            "id": "3180228",
            "problem": "考虑一个统计学习中的二元分类问题，其特征为 $X \\in \\mathbb{R}$，标签为 $Y \\in \\{0,1\\}$。类的先验概率为 $P(Y=0)=\\pi_0$ 和 $P(Y=1)=\\pi_1$，其中 $\\pi_0,\\pi_1 \\in (0,1)$ 且 $\\pi_0+\\pi_1=1$。类条件分布规定如下：\n- 对于类别 $Y=1$，条件分布 $X \\mid Y=1$ 是两个正态分量的双峰混合分布：其概率密度函数 (PDF) 为\n$$\nf_1(x) \\equiv w \\,\\phi(x;\\mu_{1a},\\sigma_{1a}) + (1-w)\\,\\phi(x;\\mu_{1b},\\sigma_{1b}),\n$$\n其中 $w \\in (0,1)$ 是混合权重，$\\phi(x;\\mu,\\sigma)$ 表示均值为 $\\mu$、标准差为 $\\sigma$ 的正态概率密度函数，即\n$$\n\\phi(x;\\mu,\\sigma) \\equiv \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n$$\n- 对于类别 $Y=0$，条件分布 $X \\mid Y=0$ 是单峰正态分布，其 PDF 为\n$$\nf_0(x) \\equiv \\phi(x;\\mu_0,\\sigma_0).\n$$\n\n一个分类决策 $\\hat{Y}(x) \\in \\{0,1\\}$ 会根据以下 $2\\times 2$ 的损失设定产生损失：预测正确类别的损失为零，当真实类别为 $0$ 时预测为 $1$ 的损失为 $\\lambda_{10} > 0$（假阳性），当真实类别为 $1$ 时预测为 $0$ 的损失为 $\\lambda_{01} > 0$（假阴性）。Bayes 分类器被定义为在每个 $x$ 处最小化条件期望损失的决策规则，而 Bayes 风险是在 $(X,Y)$ 的联合分布下 Bayes 分类器的期望损失。\n\n任务：\n1. 从核心定义（用于后验概率的 Bayes 定理和条件风险最小化）出发，推导此设定下的 Bayes 决策规则。解释为什么当 $f_1$ 是双峰而 $f_0$ 是单峰时，用于预测类别 $1$ 的 Bayes 决策区域可以是 $\\mathbb{R}$ 中的一个非凸集（不相交区间的并集），并用原始量表示在对称 0-1 损失和非对称损失下表征类别 $1$ 决策区域的不等式。\n2. 实现一个数值方法来：\n   - 在对称 0-1 损失（即 $\\lambda_{10}=\\lambda_{01}=1$）下，确定预测类别 $1$ 的 Bayes 决策区域中不相交区间的数量。\n   - 确定该决策区域是否为非凸（当且仅当区间数量超过 $1$ 时，返回布尔值 $true$）。\n   - 在对称 0-1 损失下，计算 Bayes 错误率。将此错误率表示为十进制形式的实数（无百分号）。\n   - 在指定的非对称损失 $(\\lambda_{10},\\lambda_{01})$ 下，计算 Bayes 风险。将此风险表示为十进制形式的实数。\n\n使用纯粹的数学和算法推理。不要借助于无法从给定定义中推导出的公式。你的程序必须以数值方式执行计算，且不得需要任何外部输入。\n\n测试套件：\n对于每个测试用例，参数以元组形式给出\n$$\n(\\pi_0,\\pi_1,w,\\mu_{1a},\\sigma_{1a},\\mu_{1b},\\sigma_{1b},\\mu_0,\\sigma_0,\\lambda_{10},\\lambda_{01}),\n$$\n所有量均为实值，且 $\\pi_0+\\pi_1=1$，$w \\in (0,1)$，所有标准差均为严格正值。测试套件如下：\n\n- 情况 A（一般理想路径，具有清晰的双峰 $f_1$ 和一个更宽的单峰 $f_0$，对称损失）：\n$$\n(0.5,\\,0.5,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,1.0).\n$$\n- 情况 B（边界趋势，类别 $Y=0$ 具有主导性先验概率，对称损失）：\n$$\n(0.8,\\,0.2,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,1.0).\n$$\n- 情况 C（替代形状，具有更窄的 $f_0$ 和偏移的混合分量，对称损失）：\n$$\n(0.5,\\,0.5,\\,0.6,\\,-2.5,\\,0.5,\\,1.8,\\,0.5,\\,0.0,\\,0.7,\\,1.0,\\,1.0).\n$$\n- 情况 D（非对称损失，强调假阳性惩罚，形状与情况 A 相同）：\n$$\n(0.5,\\,0.5,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,3.0).\n$$\n\n算法输出规范：\n- 对于每个测试用例，生成一个形式为 $[N,B,E,R]$ 的结果列表，其中：\n  - $N$ 是一个整数，表示在对称 0-1 损失下，类别 $1$ 的 Bayes 决策区域中不相交区间的数量。\n  - $B$ 是一个布尔值，当且仅当在对称 0-1 损失下决策区域为非凸（即 $N>1$）时为 $true$。\n  - $E$ 是一个浮点数，表示在对称 0-1 损失下的 Bayes 错误率。\n  - $R$ 是一个浮点数，表示该测试用例在指定 $(\\lambda_{10},\\lambda_{01})$ 下的 Bayes 风险。\n- 你的程序应生成单行输出，包含四个情况的结果，格式为用方括号括起来的逗号分隔列表，例如：\n$$\n[\\,[N_1,B_1,E_1,R_1],\\,[N_2,B_2,E_2,R_2],\\,[N_3,B_3,E_3,R_3],\\,[N_4,B_4,E_4,R_4]\\,].\n$$\n不涉及物理单位或角度。所有概率和风险必须表示为小数（而非百分比）。",
            "solution": "该问题要求对一个具有一维特征 $X$ 的二元分类问题中的 Bayes 分类器进行理论和数值分析。\n\n### **第一部分：理论推导与分析**\n\n**1. 从条件风险最小化推导 Bayes 决策规则**\n\n设 $Y \\in \\{0, 1\\}$ 为真实类别标签，$\\hat{Y}(x) \\in \\{0, 1\\}$ 为观测值 $X=x$ 的预测类别。损失函数由一个矩阵 $L$ 给出，其中 $L(i, j)$ 是当真实类别为 $j$ 时预测为类别 $i$ 的损失。根据问题描述，我们有：\n- $L(0, 0) = 0$ (正确预测类别 $0$)\n- $L(1, 1) = 0$ (正确预测类别 $1$)\n- $L(1, 0) = \\lambda_{10} > 0$ (假阳性，第一类错误)\n- $L(0, 1) = \\lambda_{01} > 0$ (假阴性，第二类错误)\n\nBayes 分类器旨在为每个 $x$ 最小化条件期望损失（或条件风险）。在给定 $X=x$ 时预测类别 $k$ 的条件风险为：\n$$\nR(k | x) = \\mathbb{E}[L(k, Y) | X=x] = \\sum_{j=0}^{1} L(k, j) P(Y=j | X=x)\n$$\n我们为两种可能的决策 $\\hat{Y}(x)=0$ 和 $\\hat{Y}(x)=1$ 计算这个风险。\n\n预测类别 $0$ 的条件风险是：\n$$\nR(\\hat{Y}=0 | x) = L(0, 0)P(Y=0 | x) + L(0, 1)P(Y=1 | x) = 0 \\cdot P(Y=0 | x) + \\lambda_{01} P(Y=1 | x) = \\lambda_{01} P(Y=1 | x)\n$$\n预测类别 $1$ 的条件风险是：\n$$\nR(\\hat{Y}=1 | x) = L(1, 0)P(Y=0 | x) + L(1, 1)P(Y=1 | x) = \\lambda_{10} P(Y=0 | x) + 0 \\cdot P(Y=1 | x) = \\lambda_{10} P(Y=0 | x)\n$$\nBayes 决策规则是选择使该条件风险最小化的类别。因此，我们当且仅当以下条件成立时预测类别 $1$：\n$$R(\\hat{Y}=1 | x)  R(\\hat{Y}=0 | x)$$\n$$\\lambda_{10} P(Y=0 | x)  \\lambda_{01} P(Y=1 | x)$$\n这个不等式定义了决策准则。为了使其可操作，我们使用 Bayes 定理来表示后验概率 $P(Y=j|x)$。后验概率正比于似然乘以先验：\n$$\nP(Y=j | x) = \\frac{f(x|Y=j) P(Y=j)}{f(x)} = \\frac{f_j(x) \\pi_j}{f(x)}\n$$\n其中 $f_j(x)$ 是类别 $j$ 的类条件 PDF，$\\pi_j$ 是类别 $j$ 的先验概率，而 $f(x) = \\sum_{j=0}^{1} f_j(x) \\pi_j$ 是 $X$ 的边缘 PDF。\n\n将这些代入决策规则的不等式：\n$$\n\\lambda_{10} \\frac{f_0(x) \\pi_0}{f(x)}  \\lambda_{01} \\frac{f_1(x) \\pi_1}{f(x)}\n$$\n由于对所有 $x$ 都有 $f(x) > 0$，我们可以将不等式两边同乘以 $f(x)$ 而不改变不等号方向。这就得到了预测类别 $1$ 的决策规则的最终形式：\n$$\n\\lambda_{10} f_0(x) \\pi_0  \\lambda_{01} f_1(x) \\pi_1\n$$\n这可以表示为似然比检验。如果满足以下条件，我们预测类别 $1$：\n$$\n\\frac{f_1(x)}{f_0(x)} > \\frac{\\lambda_{10} \\pi_0}{\\lambda_{01} \\pi_1}\n$$\n类别 $1$ 的决策区域，记为 $\\mathcal{R}_1$，是集合 $\\{x \\in \\mathbb{R} \\mid \\lambda_{01} \\pi_1 f_1(x) > \\lambda_{10} \\pi_0 f_0(x)\\}$。\n\n对于对称 0-1 损失的特殊情况，我们有 $\\lambda_{10} = \\lambda_{01} = 1$。规则简化为：\n$$\n\\pi_1 f_1(x) > \\pi_0 f_0(x)\n$$\n这是最大后验概率 (MAP) 规则，它表明我们应该选择具有更高后验概率的类别。\n\n**2. 决策区域的非凸性**\n\n决策边界是使不等式两边相等的点 $x$：\n$$\n\\lambda_{01} \\pi_1 f_1(x) = \\lambda_{10} \\pi_0 f_0(x)\n$$\n我们定义一个函数 $g(x) = \\lambda_{01} \\pi_1 f_1(x) - \\lambda_{10} \\pi_0 f_0(x)$。类别 $1$ 的决策区域是 $g(x)>0$ 的地方。边界是 $g(x)=0$ 的根。\n\n在这个问题中，$f_0(x)$ 是一个单峰正态 PDF，$\\phi(x;\\mu_0,\\sigma_0)$。函数 $f_1(x)$ 是两个正态 PDF 的双峰混合，$w \\,\\phi(x;\\mu_{1a},\\sigma_{1a}) + (1-w)\\,\\phi(x;\\mu_{1b},\\sigma_{1b})$。因此，$g(x)$ 是三个高斯函数的加权和与差。\n\n单个高斯函数有一个峰值。高斯混合可以是多峰的。函数 $\\lambda_{01} \\pi_1 f_1(x)$ 可以有两个不同的峰（如果分量足够分离），而函数 $\\lambda_{10} \\pi_0 f_0(x)$ 只有一个峰。一个有两个驼峰的曲线可以多次与一个有一个驼峰的曲线相交。例如，如果双峰分布的两个峰相对于位于它们之间的单峰分布足够高，双峰曲线可能会先升到单峰曲线上方，然后在中间降到其下方，然后再一次升到其上方。这种情况会产生四个交点（$g(x)=0$ 的根）。\n\n设根为 $r_1  r_2  r_3  r_4$。$g(x)$ 的符号可能会在这些根之间交替变化。例如，如果对于大的 $|x|$，$g(x)$ 为负，那么类别 $1$ 的决策区域（其中 $g(x)>0$）可能是不相交的两个区间的并集，例如 $\\mathcal{R}_1 = (r_1, r_2) \\cup (r_3, r_4)$。不相交区间的并集是 $\\mathbb{R}$ 中的一个非凸集。因此，$f_1(x)$ 的双峰性是允许出现非凸决策区域的关键特征。\n\n**3. Bayes 错误率和 Bayes 风险**\n\n**Bayes 风险** $R$ 是分类器可能达到的最小期望损失，由 Bayes 决策规则 $\\hat{Y}_{Bayes}(x)$ 实现。\n$$\nR = \\mathbb{E}[L(\\hat{Y}_{Bayes}(X), Y)] = \\int_{-\\infty}^{\\infty} \\mathbb{E}[L(\\hat{Y}_{Bayes}(x), Y) | X=x] f(x) dx\n$$\n内部的期望是在 $x$ 处最优决策的条件风险，即 $\\min\\{R(\\hat{Y}=0 | x), R(\\hat{Y}=1 | x)\\}$。\n$$\nR = \\int_{-\\infty}^{\\infty} \\min\\{\\lambda_{01} P(Y=1|x), \\lambda_{10} P(Y=0|x)\\} f(x) dx\n$$\n使用关系 $P(Y=j|x)f(x) = \\pi_j f_j(x)$，上式简化为：\n$$\nR = \\int_{-\\infty}^{\\infty} \\min\\{\\lambda_{01} \\pi_1 f_1(x), \\lambda_{10} \\pi_0 f_0(x)\\} dx\n$$\n**Bayes 错误率** $E$ 是对称 0-1 损失（$\\lambda_{10} = \\lambda_{01} = 1$）下 Bayes 风险的一个特例。\n$$\nE = \\int_{-\\infty}^{\\infty} \\min\\{\\pi_1 f_1(x), \\pi_0 f_0(x)\\} dx\n$$\n这些积分可以进行数值计算。该过程涉及找到相应 $g(x)$ 函数的根以确定决策区域。设 $\\mathcal{R}_0$ 和 $\\mathcal{R}_1$ 分别是我们预测类别 $0$ 和 $1$ 的区域。积分可以重写为：\n- 对于错误率 $E$：$E = \\int_{\\mathcal{R}_1} \\pi_0 f_0(x) dx + \\int_{\\mathcal{R}_0} \\pi_1 f_1(x) dx$。\n- 对于风险 $R$：$R = \\int_{\\mathcal{R}'_1} \\lambda_{10} \\pi_0 f_0(x) dx + \\int_{\\mathcal{R}'_0} \\lambda_{01} \\pi_1 f_1(x) dx$，其中 $\\mathcal{R}'_0, \\mathcal{R}'_1$ 是非对称损失下的区域。\n\n### **第二部分：数值实现策略**\n\n每个测试用例的数值解法包括以下步骤：\n1.  **定义函数**：实现 PDF $f_0(x)$ 和 $f_1(x)$ 的 Python 函数，以及决策函数 $g_{sym}(x) = \\pi_1 f_1(x) - \\pi_0 f_0(x)$ 和 $g_{asym}(x) = \\lambda_{01} \\pi_1 f_1(x) - \\lambda_{10} \\pi_0 f_0(x)$。\n2.  **寻找根**：为了找到对称损失下不相交区间的数量，确定 $g_{sym}(x)=0$ 的所有实根。一个稳健的方法是扫描一个很宽的 $x$ 值范围以寻找符号变化，从而框定根的范围。然后，在每个框定的范围内使用像 `scipy.optimize.brentq` 这样的数值求解器来高精度地找到根。\n3.  **确定区间和 N, B**：排序后的根 $\\{r_1, r_2, \\dots, r_k\\}$ 将 $\\mathbb{R}$ 划分成 $k+1$ 个区间。通过在每个区间内的一个样本点上测试 $g_{sym}(x)$ 的符号，我们可以识别出构成决策区域 $\\mathcal{R}_1$ 的不相交区间集合。这些区间的数量即为 $N$，如果 $N>1$，则 $B$ 为 `True`。\n4.  **计算 Bayes 错误率 E**：利用对称情况下的根，将实数轴 $(-\\infty, \\infty)$ 划分为多个区间。对于每个区间，确定它属于 $\\mathcal{R}_0$ 还是 $\\mathcal{R}_1$。使用 `scipy.integrate.quad` 对相应的误差项（在 $\\mathcal{R}_1$ 上对 $\\pi_0 f_0(x)$积分，在 $\\mathcal{R}_0$ 上对 $\\pi_1 f_1(x)$ 积分）进行数值积分，并将结果求和。\n5.  **计算 Bayes 风险 R**：对非对称情况重复此过程。找到 $g_{asym}(x)=0$ 的根。使用这些新的根来定义非对称决策区域 $\\mathcal{R}'_0$ 和 $\\mathcal{R}'_1$。对相应的风险项（在 $\\mathcal{R}'_1$ 上对 $\\lambda_{10} \\pi_0 f_0(x)$ 积分，在 $\\mathcal{R}'_0$ 上对 $\\lambda_{01} \\pi_1 f_1(x)$ 积分）进行数值积分，并将结果求和以得到总 Bayes 风险 $R$。\n此过程是为所提供的测试套件实现的。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        (0.5, 0.5, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 1.0),\n        # Case B\n        (0.8, 0.2, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 1.0),\n        # Case C\n        (0.5, 0.5, 0.6, -2.5, 0.5, 1.8, 0.5, 0.0, 0.7, 1.0, 1.0),\n        # Case D\n        (0.5, 0.5, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 3.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(process_case(case))\n\n    # Format the final output string as a compact list of lists\n    # e.g., [[N1,B1,E1,R1],[N2,B2,E2,R2],...]\n    output_string = str(results).replace(\" \", \"\")\n    print(output_string)\n\ndef process_case(params):\n    \"\"\"\n    Computes [N, B, E, R] for a single test case.\n    \"\"\"\n    pi0, pi1, w, mu1a, sigma1a, mu1b, sigma1b, mu0, sigma0, lam10, lam01 = params\n\n    # 1. Define PDFs\n    def f1(x):\n        return w * norm.pdf(x, loc=mu1a, scale=sigma1a) + (1-w) * norm.pdf(x, loc=mu1b, scale=sigma1b)\n\n    def f0(x):\n        return norm.pdf(x, loc=mu0, scale=sigma0)\n\n    # 2. Analyze symmetric loss case for N, B, E\n    def g_sym(x):\n        return pi1 * f1(x) - pi0 * f0(x)\n\n    def find_roots(func):\n        # Determine a sufficiently wide range to find all relevant roots\n        all_mus = [mu1a, mu1b, mu0]\n        all_sigmas = [sigma1a, sigma1b, sigma0]\n        search_min = min(all_mus) - 15 * max(all_sigmas)\n        search_max = max(all_mus) + 15 * max(all_sigmas)\n        \n        roots = []\n        grid = np.linspace(search_min, search_max, 20001)\n        y_vals = func(grid)\n        \n        for i in range(len(grid) - 1):\n            if np.sign(y_vals[i]) != np.sign(y_vals[i+1]):\n                try:\n                    root = brentq(func, grid[i], grid[i+1])\n                    roots.append(root)\n                except (ValueError, RuntimeError):\n                    # In case of issues, we skip, as sign change is the main trigger.\n                    pass\n        return sorted(list(set(roots)))\n\n    roots_sym = find_roots(g_sym)\n    \n    # 3. Determine N and B\n    R1_sym_intervals = []\n    all_sym_points = [-np.inf] + roots_sym + [np.inf]\n    for i in range(len(all_sym_points) - 1):\n        a, b = all_sym_points[i], all_sym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n        if g_sym(test_point) > 0:\n            R1_sym_intervals.append((a, b))\n    \n    N = len(R1_sym_intervals)\n    B = N > 1\n\n    # 4. Compute Bayes Error Rate E (symmetric loss)\n    # E = integral over R of min(pi0*f0, pi1*f1)\n    # This is equivalent to integrating pi1*f1 over R0 and pi0*f0 over R1\n    error = 0.0\n    all_sym_points = [-np.inf] + roots_sym + [np.inf]\n    for i in range(len(all_sym_points) - 1):\n        a, b = all_sym_points[i], all_sym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n\n        if g_sym(test_point) > 0: # Predict 1, error is if Y=0\n            error += quad(lambda x: pi0 * f0(x), a, b)[0]\n        else: # Predict 0, error is if Y=1\n            error += quad(lambda x: pi1 * f1(x), a, b)[0]\n    E = error\n    \n    # 5. Compute Bayes Risk R (asymmetric loss)\n    def g_asym(x):\n        return lam01 * pi1 * f1(x) - lam10 * pi0 * f0(x)\n\n    roots_asym = find_roots(g_asym)\n    \n    risk = 0.0\n    all_asym_points = [-np.inf] + roots_asym + [np.inf]\n    for i in range(len(all_asym_points) - 1):\n        a, b = all_asym_points[i], all_asym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n\n        if g_asym(test_point) > 0: # Predict 1, risk is lambda_10 * P(Y=0)\n            risk += quad(lambda x: lam10 * pi0 * f0(x), a, b)[0]\n        else: # Predict 0, risk is lambda_01 * P(Y=1)\n            risk += quad(lambda x: lam01 * pi1 * f1(x), a, b)[0]\n    R = risk\n\n    return [N, B, E, R]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}