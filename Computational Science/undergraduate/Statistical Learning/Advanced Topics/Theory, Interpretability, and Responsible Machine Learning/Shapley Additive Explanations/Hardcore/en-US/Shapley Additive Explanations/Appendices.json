{
    "hands_on_practices": [
        {
            "introduction": "To begin, we explore how Shapley Additive Explanations (SHAP) operate in an idealized yet insightful scenario. This practice  examines an additive model with a non-linear, saturating feature effect, akin to a dose-response curve. By deriving the SHAP value analytically, you will see precisely how this method captures the changing importance of a feature as it moves from a linear response regime to a saturation plateau, a feat impossible for simple linear coefficients.",
            "id": "3173352",
            "problem": "Consider a two-feature regression model used to explain a dose-response effect with saturation. Let the prediction function be\n$$\nf(x_{1},x_{2}) \\;=\\; g(x_{1}) \\;+\\; b\\,x_{2} \\;+\\; c,\n$$\nwhere the dose-response component is saturating,\n$$\ng(x_{1}) \\;=\\; L\\bigl(1 - \\exp(-k\\,x_{1})\\bigr),\n$$\nwith constants $L>0$ and $k>0$. Assume features are statistically independent under the training data distribution, with $X_{1} \\sim \\mathrm{Exponential}(\\lambda)$ (rate $\\lambda>0$) and $X_{2}$ having a finite mean $\\mu_{2}$ and finite variance.\n\nUsing the definition of Shapley additive explanations (SHAP), where the Shapley value $\\phi_{i}$ for feature $i$ at an observation $x=(x_{1},x_{2})$ is defined as the average marginal contribution over all coalitions,\n$$\n\\phi_{i}(x) \\;=\\; \\sum_{S \\subseteq \\{1,2\\}\\setminus\\{i\\}} \\frac{|S|!\\,\\bigl(2-|S|-1\\bigr)!}{2!}\\,\\Bigl(\\,\\mathbb{E}\\bigl[f(X)\\,\\big|\\,X_{S}=x_{S},\\,X_{i}=x_{i}\\bigr]\\;-\\;\\mathbb{E}\\bigl[f(X)\\,\\big|\\,X_{S}=x_{S}\\bigr]\\,\\Bigr),\n$$\nderive a closed-form expression for the SHAP value $\\phi_{1}(x)$ as a function of $x_{1}$, $L$, $k$, and $\\lambda$ for this model and distributional setting. Your derivation must start from the above definition and use only the given model structure and distributional assumptions. Discuss, based on your derived expression, how $\\phi_{1}(x)$ evolves as $x_{1}$ moves from the approximately linear regime (small $x_{1}$) into the saturation plateau (large $x_{1}$), but report only the requested closed-form expression as your final answer. No numerical approximation is required, and no rounding is needed. Express your final answer as a single closed-form analytic expression in terms of $x_{1}$, $L$, $k$, and $\\lambda$.",
            "solution": "The user wants me to validate the problem and then, if it is valid, solve it.\n\n### Step 1: Extract Givens\nThe givens are:\n- Prediction function: $f(x_{1},x_{2}) = g(x_{1}) + b\\,x_{2} + c$\n- Dose-response component: $g(x_{1}) = L\\bigl(1 - \\exp(-k\\,x_{1})\\bigr)$\n- Constants: $L>0$, $k>0$, $\\lambda>0$, $b$, $c$\n- Feature distributions: Features are statistically independent. $X_{1} \\sim \\mathrm{Exponential}(\\lambda)$. $X_{2}$ has a finite mean $\\mu_{2}$ and finite variance.\n- SHAP definition for feature $i$:\n$$\n\\phi_{i}(x) \\;=\\; \\sum_{S \\subseteq \\{1,2\\}\\setminus\\{i\\}} \\frac{|S|!\\,\\bigl(2-|S|-1\\bigr)!}{2!}\\,\\Bigl(\\,\\mathbb{E}\\bigl[f(X)\\,\\big|\\,X_{S}=x_{S},\\,X_{i}=x_{i}\\bigr]\\;-\\;\\mathbb{E}\\bigl[f(X)\\,\\big|\\,X_{S}=x_{S}\\bigr]\\,\\Bigr)\n$$\n- Objective: Derive a closed-form expression for $\\phi_{1}(x)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is validated against the specified criteria:\n- **Scientifically Grounded**: The problem is well-grounded in the field of statistical learning and explainable artificial intelligence. The model is an additive model with a standard saturating component. The distributions are common, and the definition of Shapley values is the standard one for feature attributions.\n- **Well-Posed**: The problem is well-posed. It provides all necessary information—the model form, distributional assumptions, feature independence, and the specific definition of SHAP values—to derive a unique analytical solution.\n- **Objective**: The problem is stated using precise mathematical language, free of any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the flaws listed:\n1.  **Scientific or Factual Unsoundness**: None. The mathematical and statistical setup is sound.\n2.  **Non-Formalizable or Irrelevant**: None. The problem is a formal mathematical derivation directly relevant to the topic of SHAP.\n3.  **Incomplete or Contradictory Setup**: None. The assumptions of feature independence and the specified distributions are sufficient for the derivation. The properties of $X_2$ (finite mean) are all that is needed.\n4.  **Unrealistic or Infeasible**: None. The model structure and distributional choices are common in practice.\n5.  **Ill-Posed or Poorly Structured**: None. The question leads to a single, stable, and meaningful solution.\n6.  **Pseudo-Profound, Trivial, or Tautological**: None. The derivation requires a rigorous application of expectation and conditioning principles and is not trivial.\n7.  **Outside Scientific Verifiability**: None. The derivation is a mathematical procedure that is fully verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation.\n\nTo derive the Shapley value $\\phi_{1}(x)$ for feature $1$ at an observation $x=(x_{1}, x_{2})$, we use the provided definition. The set of all features is $\\{1, 2\\}$, so we are calculating $\\phi_{1}(x)$. The subsets $S$ of features excluding feature $1$ are subsets of $\\{2\\}$. Thus, the possible coalitions $S$ are the empty set $\\emptyset$ and the set $\\{2\\}$.\n\nThe SHAP formula for $\\phi_{1}(x)$ is the sum of contributions from these two coalitions:\n$$\n\\phi_{1}(x) = \\frac{0!(2-0-1)!}{2!}\\left(\\mathbb{E}[f(X)|X_1=x_1] - \\mathbb{E}[f(X)]\\right) + \\frac{1!(2-1-1)!}{2!}\\left(\\mathbb{E}[f(X)|X_1=x_1, X_2=x_2] - \\mathbb{E}[f(X)|X_2=x_2]\\right)\n$$\nThe combinatorial weights are:\n- For $S=\\emptyset$, $|S|=0$: $\\frac{0!1!}{2!} = \\frac{1 \\cdot 1}{2} = \\frac{1}{2}$.\n- For $S=\\{2\\}$, $|S|=1$: $\\frac{1!0!}{2!} = \\frac{1 \\cdot 1}{2} = \\frac{1}{2}$.\n\nSo, the expression for $\\phi_{1}(x)$ becomes:\n$$\n\\phi_{1}(x) = \\frac{1}{2}\\left(\\mathbb{E}[f(X)|X_1=x_1] - \\mathbb{E}[f(X)]\\right) + \\frac{1}{2}\\left(f(x_1, x_2) - \\mathbb{E}[f(X)|X_2=x_2]\\right)\n$$\nNote that $\\mathbb{E}[f(X)|X_1=x_1, X_2=x_2] = f(x_1, x_2)$ since all variables are specified.\n\nNext, we must calculate the four expectation terms. The model is $f(X_{1},X_{2}) = g(X_{1}) + b\\,X_{2} + c$, with $g(X_{1}) = L\\bigl(1 - \\exp(-k\\,X_{1})\\bigr)$. The features $X_{1}$ and $X_{2}$ are independent. $X_{1} \\sim \\mathrm{Exponential}(\\lambda)$ and $\\mathbb{E}[X_{2}] = \\mu_{2}$.\n\nFirst, we calculate the expectation of the non-linear component $g(X_{1})$:\n$$\n\\mathbb{E}[g(X_{1})] = \\int_{0}^{\\infty} g(x_{1}) p(x_{1}) dx_{1} = \\int_{0}^{\\infty} L\\bigl(1 - \\exp(-k\\,x_{1})\\bigr) \\lambda\\exp(-\\lambda\\,x_{1}) dx_{1}\n$$\n$$\n= L\\lambda \\int_{0}^{\\infty} \\left(\\exp(-\\lambda\\,x_{1}) - \\exp(-(k+\\lambda)x_{1})\\right) dx_{1}\n$$\n$$\n= L\\lambda \\left[ -\\frac{1}{\\lambda}\\exp(-\\lambda\\,x_{1}) - \\left(-\\frac{1}{k+\\lambda}\\exp(-(k+\\lambda)x_{1})\\right) \\right]_{0}^{\\infty}\n$$\n$$\n= L\\lambda \\left[ \\left(0 - 0\\right) - \\left(-\\frac{1}{\\lambda} + \\frac{1}{k+\\lambda}\\right) \\right] = L\\lambda \\left(\\frac{1}{\\lambda} - \\frac{1}{k+\\lambda}\\right)\n$$\n$$\n= L\\lambda \\left(\\frac{k+\\lambda-\\lambda}{\\lambda(k+\\lambda)}\\right) = L\\lambda \\left(\\frac{k}{\\lambda(k+\\lambda)}\\right) = \\frac{L k}{k+\\lambda}\n$$\n\nNow we compute the four expectation terms for the SHAP formula:\n1.  $\\mathbb{E}[f(X)]$: Using linearity of expectation and independence:\n    $$\n    \\mathbb{E}[f(X)] = \\mathbb{E}[g(X_{1}) + b\\,X_{2} + c] = \\mathbb{E}[g(X_{1})] + b\\,\\mathbb{E}[X_{2}] + c = \\frac{L k}{k+\\lambda} + b\\,\\mu_{2} + c\n    $$\n2.  $\\mathbb{E}[f(X)|X_1=x_1]$: Conditioning on $X_1=x_1$ and using independence:\n    $$\n    \\mathbb{E}[f(X)|X_1=x_1] = \\mathbb{E}[g(X_{1}) + b\\,X_{2} + c|X_1=x_1] = g(x_1) + b\\,\\mathbb{E}[X_{2}] + c = g(x_1) + b\\,\\mu_{2} + c\n    $$\n3.  $\\mathbb{E}[f(X)|X_2=x_2]$: Conditioning on $X_2=x_2$ and using independence:\n    $$\n    \\mathbb{E}[f(X)|X_2=x_2] = \\mathbb{E}[g(X_{1}) + b\\,X_{2} + c|X_2=x_2] = \\mathbb{E}[g(X_1)] + b\\,x_2 + c = \\frac{L k}{k+\\lambda} + b\\,x_2 + c\n    $$\n4.  $f(x_1, x_2) = g(x_1) + b\\,x_2 + c$\n\nNow we substitute these into the two terms of the $\\phi_{1}(x)$ expression.\n\nTerm 1 (coalition $S=\\emptyset$):\n$$\n\\frac{1}{2}\\left(\\mathbb{E}[f(X)|X_1=x_1] - \\mathbb{E}[f(X)]\\right) = \\frac{1}{2}\\left( (g(x_1) + b\\,\\mu_{2} + c) - (\\frac{L k}{k+\\lambda} + b\\,\\mu_{2} + c) \\right)\n$$\n$$\n= \\frac{1}{2}\\left( g(x_1) - \\frac{L k}{k+\\lambda} \\right)\n$$\n\nTerm 2 (coalition $S=\\{2\\}$):\n$$\n\\frac{1}{2}\\left(f(x_1, x_2) - \\mathbb{E}[f(X)|X_2=x_2]\\right) = \\frac{1}{2}\\left( (g(x_1) + b\\,x_2 + c) - (\\frac{L k}{k+\\lambda} + b\\,x_2 + c) \\right)\n$$\n$$\n= \\frac{1}{2}\\left( g(x_1) - \\frac{L k}{k+\\lambda} \\right)\n$$\nAs expected for an additive model with independent features, the contribution from feature $2$ (via $b$, $x_2$, $\\mu_2$) cancels out.\n\nFinally, we sum the two terms to get $\\phi_{1}(x)$:\n$$\n\\phi_{1}(x) = \\frac{1}{2}\\left( g(x_1) - \\frac{L k}{k+\\lambda} \\right) + \\frac{1}{2}\\left( g(x_1) - \\frac{L k}{k+\\lambda} \\right) = g(x_1) - \\frac{L k}{k+\\lambda}\n$$\nSubstituting the expression for $g(x_1)$:\n$$\n\\phi_{1}(x) = L\\bigl(1 - \\exp(-k\\,x_{1})\\bigr) - \\frac{L k}{k+\\lambda}\n$$\nThis expression represents the contribution of feature $1$ at value $x_1$ relative to its average contribution over the data distribution.\n\nThe problem also asks for a discussion of the behavior of $\\phi_{1}(x)$. The derived expression is $\\phi_{1}(x_{1}) = g(x_{1}) - \\mathbb{E}[g(X_{1})]$. The shape of $\\phi_{1}(x_{1})$ as a function of $x_{1}$ is identical to that of $g(x_{1})$ but shifted vertically downwards by a constant amount $\\mathbb{E}[g(X_{1})] = \\frac{L k}{k+\\lambda}$.\n- For small $x_{1}$ ($k\\,x_{1} \\ll 1$), we have $\\exp(-k\\,x_{1}) \\approx 1 - k\\,x_{1}$. So, $g(x_{1}) \\approx L(1 - (1 - k\\,x_{1})) = L k\\,x_{1}$. In this linear regime, $\\phi_{1}(x_{1}) \\approx L k\\,x_{1} - \\frac{L k}{k+\\lambda}$. The SHAP value starts at $\\phi_{1}(0) = -\\frac{L k}{k+\\lambda}$ and increases linearly with $x_{1}$. A zero dose has a negative SHAP value, indicating its effect is less than the average effect of the dose.\n- For large $x_{1}$ ($k\\,x_{1} \\gg 1$), we have $\\exp(-k\\,x_{1}) \\to 0$. So, $g(x_{1}) \\to L$. In this saturation plateau, $\\phi_{1}(x_{1}) \\to L - \\frac{L k}{k+\\lambda} = L\\left(1 - \\frac{k}{k+\\lambda}\\right) = \\frac{L\\lambda}{k+\\lambda}$. The SHAP value asymptotically approaches a constant positive value, representing the maximum impact of the dose relative to its average impact.\n\nThe final answer is the derived closed-form expression.",
            "answer": "$$\n\\boxed{L\\left(1 - \\exp(-k\\,x_{1})\\right) - \\frac{L k}{k + \\lambda}}\n$$"
        },
        {
            "introduction": "Building on the ideal case, we now confront a common practical challenge: feature engineering. Explanations are only as good as the model they interpret, and this exercise  demonstrates this vividly using the example of a cyclic feature like a calendar month. You will quantify the misleading attributions that arise from a naive ordinal encoding and see how a proper trigonometric encoding allows SHAP to produce balanced and sensible results that respect the feature's cyclical nature.",
            "id": "3173394",
            "problem": "You are asked to implement a reproducible experiment to demonstrate how Shapley Additive Explanations (SHAP) can mislead when used with an ordinal encoding of cyclic features, and how a cyclic sine-cosine encoding restores balanced attributions. The core object is a deterministic model that maps a calendar month to a scalar output. Use the following specifications.\n\n- Fundamental base: use the cooperative game theory definition of Shapley values and the conditional expectation semantics for missing features in Shapley Additive Explanations (SHAP). Specifically, the value function uses conditional expectations of the model output given a subset of observed features, and the Shapley value of a feature is the average marginal contribution across all possible subsets and permutations. Do not assume any ad hoc shortcut formulas; build the reasoning from the definitions and basic symmetries.\n\n- Dataset and target: construct the set of months $\\{1,2,\\dots,12\\}$. Associate each month $m$ with an angle $\\theta_m = 2\\pi m / 12$ measured in radians. Define a deterministic ground-truth model\n$$\nf(m) = A \\sin(\\theta_m) + B \\cos(\\theta_m),\n$$\nwith $A = 2$ and $B = 1$.\n\n- Two encodings and models:\n  1. Ordinal encoding: treat the month as a single integer feature $m$ and fit a least-squares linear model $g(m) = \\alpha m + \\beta$ to the ground-truth outputs $f(m)$ over $m \\in \\{1,\\dots,12\\}$. In the SHAP framework with a single feature, the Shapley value for an instance $m$ is the difference between the model output at $m$ and the baseline expectation under the empirical distribution of months, that is,\n  $$\n  \\phi_{\\text{ord}}(m) = g(m) - \\mathbb{E}[g(M)],\n  $$\n  where $M$ is uniformly distributed over $\\{1,\\dots,12\\}$.\n  2. Cyclic sine-cosine encoding: represent each month $m$ by two features $s_m = \\sin(\\theta_m)$ and $c_m = \\cos(\\theta_m)$ and use the linear model $h(s,c) = A s + B c$. For SHAP, use the conditional expectation semantics with the empirical uniform distribution over months. The Shapley values $\\phi_s(m)$ and $\\phi_c(m)$ should be computed from first principles via conditional expectations and marginal contributions of the coalitions $\\{\\}$, $\\{s\\}$, $\\{c\\}$, and $\\{s,c\\}$, leveraging the symmetries of $\\sin$ and $\\cos$ over the $12$ equally spaced angles.\n\n- Angle unit: all angles must be in radians.\n\n- Test suite: compute the following quantities to evaluate misinterpretations and attribution balance. Use the uniform empirical month distribution on $\\{1,\\dots,12\\}$ for all expectations.\n  1. Adjacency misinterpretation check across the circular boundary: compute the absolute SHAP gap for adjacent months at the boundary under the ordinal model,\n  $$\n  \\Delta_{\\text{ord}} = \\left|\\phi_{\\text{ord}}(12) - \\phi_{\\text{ord}}(1)\\right|,\n  $$\n  and compare to the cyclic model’s SHAP-sum gap,\n  $$\n  \\Delta_{\\text{cyc}} = \\left|(\\phi_s(12)+\\phi_c(12)) - (\\phi_s(1)+\\phi_c(1))\\right|.\n  $$\n  Output a boolean indicating whether $\\Delta_{\\text{ord}} > \\Delta_{\\text{cyc}}$.\n  2. Equal-sine attribution balance for months with the same sine but different cosine ($m=1$ and $m=5$): output two booleans,\n     - whether $\\phi_s(1)$ equals $\\phi_s(5)$ up to numerical tolerance $10^{-12}$, and\n     - whether $\\phi_c(1)$ and $\\phi_c(5)$ are equal in magnitude and opposite in sign up to numerical tolerance $10^{-12}$; that is, whether $\\phi_c(1)+\\phi_c(5)$ is numerically zero.\n  3. Explicit SHAP components at a sine-dominant month ($m=3$, angle $\\theta_3 = \\pi/2$): output $\\phi_s(3)$ and $\\phi_c(3)$ as floats.\n  4. Explicit SHAP components at a cosine-dominant month ($m=6$, angle $\\theta_6 = \\pi$): output $\\phi_s(6)$ and $\\phi_c(6)$ as floats.\n  5. SHAP-sum consistency: for months $m \\in \\{1,3,5,6,12\\}$, verify that $\\phi_s(m)+\\phi_c(m)$ equals $f(m)$ within tolerance $10^{-12}$ (recall the baseline expectation $\\mathbb{E}[f(M)]$ is zero by symmetry). Output a single boolean that is true if and only if this consistency holds for all five months.\n  6. Misinterpretation ratio for the adjacency boundary: output the float\n  $$\n  R = \\frac{\\Delta_{\\text{ord}}}{\\max(\\Delta_{\\text{cyc}}, 10^{-12})}.\n  $$\n\n- Final output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n  $$\n  [\\text{adjacency\\_boolean},\\ \\text{equal\\_sine\\_boolean},\\ \\text{cos\\_opposite\\_boolean},\\ \\phi_s(3),\\ \\phi_c(3),\\ \\phi_s(6),\\ \\phi_c(6),\\ \\text{sum\\_consistency\\_boolean},\\ R].\n  $$\nAll numeric outputs must be in standard floating-point format without units; all booleans must be standard logical values.",
            "solution": "The problem statement is validated as scientifically sound, well-posed, objective, and complete. It presents a rigorous exercise in applying the foundational principles of Shapley Additive Explanations (SHAP) to demonstrate a known issue with feature engineering for cyclic variables. We will proceed with a full solution.\n\nThe core of the problem is to compute and compare SHAP values for two different representations of a cyclic feature (a calendar month). The ground-truth model is a deterministic function of the month $m \\in \\{1, 2, \\dots, 12\\}$:\n$$\nf(m) = A \\sin(\\theta_m) + B \\cos(\\theta_m)\n$$\nwhere $A = 2$, $B = 1$, and $\\theta_m = 2\\pi m / 12$. The empirical distribution of months is uniform over $\\{1, \\dots, 12\\}$.\n\n### Ordinal Encoding Model Analysis\n\nFirst, we consider the ordinal encoding, where the month is treated as a single integer feature $m$. A simple linear model $g(m) = \\alpha m + \\beta$ is fitted to the ground-truth data $(m, f(m))$ for $m=1, \\dots, 12$ using ordinary least squares.\n\nThe coefficients $\\alpha$ and $\\beta$ are given by:\n$$\n\\alpha = \\frac{\\sum_{m=1}^{12} (m - \\bar{m})(f(m) - \\bar{f})}{\\sum_{m=1}^{12} (m - \\bar{m})^2}, \\quad \\beta = \\bar{f} - \\alpha \\bar{m}\n$$\nHere, $\\bar{m} = \\mathbb{E}[M] = \\frac{1}{12}\\sum_{m=1}^{12} m = 6.5$. The average of the ground-truth function $\\bar{f} = \\mathbb{E}[f(M)]$ is:\n$$\n\\bar{f} = \\frac{1}{12}\\sum_{m=1}^{12} \\left(A \\sin\\left(\\frac{2\\pi m}{12}\\right) + B \\cos\\left(\\frac{2\\pi m}{12}\\right)\\right) = 0\n$$\nThis is because the sums of sine and cosine over a complete cycle of equally spaced points are zero.\nWith $\\bar{f}=0$, the coefficients simplify to:\n$$\n\\alpha = \\frac{\\sum_{m=1}^{12} (m - 6.5)f(m)}{\\sum_{m=1}^{12} (m - 6.5)^2}, \\quad \\beta = -\\alpha \\bar{m} = -6.5\\alpha\n$$\nThe denominator is a standard sum, $\\sum_{m=1}^{12} (m - 6.5)^2 = 143$. The numerator can be computed symbolically:\n$$\n\\sum_{m=1}^{12} m f(m) = A \\sum_{m=1}^{12} m \\sin\\left(\\frac{2\\pi m}{12}\\right) + B \\sum_{m=1}^{12} m \\cos\\left(\\frac{2\\pi m}{12}\\right)\n$$\nUsing known finite trigonometric sum identities, $\\sum_{k=1}^{N} k \\sin(2\\pi k/N) = -N/2 \\cot(\\pi/N)$ and $\\sum_{k=1}^{N} k \\cos(2\\pi k/N) = N/2$. For $N=12$, $A=2$, and $B=1$:\n$$\n\\sum_{m=1}^{12} m f(m) = 2 \\left(-6 \\cot\\left(\\frac{\\pi}{12}\\right)\\right) + 1 \\left(\\frac{12}{2}\\right) = -12(2+\\sqrt{3}) + 6 = -24 - 12\\sqrt{3} + 6 = -18 - 12\\sqrt{3}\n$$\nSince $\\sum f(m)=0$, the numerator is $\\sum (m-6.5)f(m) = \\sum m f(m) = -18 - 12\\sqrt{3}$.\n$$\n\\alpha = \\frac{-18 - 12\\sqrt{3}}{143} \\approx -0.27122\n$$\nThe SHAP value for the single-feature ordinal model is $\\phi_{\\text{ord}}(m) = g(m) - \\mathbb{E}[g(M)]$.\n$$\n\\mathbb{E}[g(M)] = \\mathbb{E}[\\alpha M + \\beta] = \\alpha\\mathbb{E}[M] + \\beta = \\alpha\\bar{m} + \\beta = \\alpha\\bar{m} + (\\bar{f} - \\alpha\\bar{m}) = \\bar{f} = 0\n$$\nThus, $\\phi_{\\text{ord}}(m) = g(m) = \\alpha m + \\beta$. The adjacency gap at the circular boundary is:\n$$\n\\Delta_{\\text{ord}} = |\\phi_{\\text{ord}}(12) - \\phi_{\\text{ord}}(1)| = |(12\\alpha+\\beta) - (\\alpha+\\beta)| = |11\\alpha| = 11 \\frac{18 + 12\\sqrt{3}}{143} = \\frac{18 + 12\\sqrt{3}}{13} \\approx 2.9834\n$$\nThis large gap reflects the model's failure to understand that month $12$ is adjacent to month $1$.\n\n### Cyclic Sine-Cosine Encoding Model Analysis\n\nNext, we use a two-feature encoding: $s_m = \\sin(\\theta_m)$ and $c_m = \\cos(\\theta_m)$. The model is $h(s, c) = As + Bc$, which is the ground-truth model itself. We compute the SHAP values $\\phi_s(m)$ and $\\phi_c(m)$ for an instance $(s_m, c_m)$ from first principles.\n\nThe features (players) are $F=\\{s, c\\}$. The Shapley values are:\n$$\n\\phi_s(m) = \\frac{1}{2}\\left(v(\\{s\\}) - v(\\emptyset)\\right) + \\frac{1}{2}\\left(v(\\{s,c\\}) - v(\\{c\\})\\right)\n$$\n$$\n\\phi_c(m) = \\frac{1}{2}\\left(v(\\{c\\}) - v(\\emptyset)\\right) + \\frac{1}{2}\\left(v(\\{s,c\\}) - v(\\{s\\})\\right)\n$$\nThe value function $v(S)$ is the conditional expectation of the model output, where features in coalition $S$ are fixed to the instance's values, and other features are averaged over their conditional distribution. The distribution is uniform over the $12$ months.\n\n1.  $v(\\emptyset) = \\mathbb{E}[h(S,C)] = \\mathbb{E}[f(M)] = 0$, as shown before.\n2.  $v(\\{s,c\\}) = \\mathbb{E}[h(S,C) | s=s_m, c=c_m] = h(s_m, c_m) = A s_m + B c_m = f(m)$.\n3.  $v(\\{s\\}) = \\mathbb{E}[h(S,C) | s=s_m] = A s_m + B \\mathbb{E}[C | s=s_m]$. Due to the symmetries of the cosine function for angles with the same sine (i.e., $\\theta$ and $\\pi-\\theta$), we have $\\mathbb{E}[C | s=s_m] = 0$ for all $m$. Thus, $v(\\{s\\}) = A s_m$.\n4.  $v(\\{c\\}) = \\mathbb{E}[h(S,C) | c=c_m] = A \\mathbb{E}[S | c=c_m] + B c_m$. Due to the symmetries of the sine function for angles with the same cosine (i.e., $\\theta$ and $2\\pi-\\theta$), we have $\\mathbb{E}[S | c=c_m] = 0$ for all $m$. Thus, $v(\\{c\\}) = B c_m$.\n\nSubstituting these into the Shapley formulas:\n$$\n\\phi_s(m) = \\frac{1}{2}(A s_m - 0) + \\frac{1}{2}((A s_m + B c_m) - B c_m) = A s_m\n$$\n$$\n\\phi_c(m) = \\frac{1}{2}(B c_m - 0) + \\frac{1}{2}((A s_m + B c_m) - A s_m) = B c_m\n$$\nThe SHAP values are simply the individual terms of the linear model: $\\phi_s(m) = 2 \\sin(\\theta_m)$ and $\\phi_c(m) = \\cos(\\theta_m)$.\nThe sum of SHAP attributions is $\\phi_s(m) + \\phi_c(m) = A s_m + B c_m = f(m)$. This matches the SHAP property $\\sum \\phi_i = f(x) - \\mathbb{E}[f]$ since $\\mathbb{E}[f]=0$.\n\nThe adjacency gap for the cyclic model is:\n$$\n\\Delta_{\\text{cyc}} = |(\\phi_s(12)+\\phi_c(12)) - (\\phi_s(1)+\\phi_c(1))| = |f(12) - f(1)|\n$$\n$$\nf(12) = 2\\sin(2\\pi) + \\cos(2\\pi) = 1\n$$\n$$\nf(1) = 2\\sin(\\pi/6) + \\cos(\\pi/6) = 2(1/2) + \\sqrt{3}/2 = 1+\\sqrt{3}/2\n$$\n$$\n\\Delta_{\\text{cyc}} = |1 - (1+\\sqrt{3}/2)| = \\sqrt{3}/2 \\approx 0.8660\n$$\n\n### Test Suite Evaluation\n\nWe now compute the required quantities.\n\n1.  **Adjacency misinterpretation**: $\\Delta_{\\text{ord}} \\approx 2.9834$ and $\\Delta_{\\text{cyc}} \\approx 0.8660$. Since $2.9834 > 0.8660$, the output is `True`. The ordinal model creates a large, artificial discontinuity at the year boundary.\n2.  **Equal-sine attribution**: For $m=1$ ($\\theta=\\pi/6$) and $m=5$ ($\\theta=5\\pi/6$), $\\sin(\\theta_1)=\\sin(\\theta_5)=1/2$.\n    $\\phi_s(1) = A s_1 = 2(1/2) = 1$. $\\phi_s(5) = A s_5 = 2(1/2) = 1$. They are equal, so the output is `True`.\n3.  **Opposite-cosine attribution**: For $m=1, 5$, $\\cos(\\theta_1)=\\sqrt{3}/2$ and $\\cos(\\theta_5)=-\\sqrt{3}/2$.\n    $\\phi_c(1) = B c_1 = \\sqrt{3}/2$. $\\phi_c(5) = B c_5 = -\\sqrt{3}/2$. Their sum is $0$, so the output is `True`. The attributions correctly reflect the features' relationship.\n4.  **SHAP components at $m=3$**: $\\theta_3=\\pi/2$, so $s_3=1, c_3=0$.\n    $\\phi_s(3) = A s_3 = 2(1) = 2.0$.\n    $\\phi_c(3) = B c_3 = 1(0) = 0.0$.\n5.  **SHAP components at $m=6$**: $\\theta_6=\\pi$, so $s_6=0, c_6=-1$.\n    $\\phi_s(6) = A s_6 = 2(0) = 0.0$.\n    $\\phi_c(6) = B c_6 = 1(-1) = -1.0$.\n6.  **SHAP-sum consistency**: As shown, $\\phi_s(m) + \\phi_c(m) = f(m)$ for all $m$. This check will pass for all specified months. The output is `True`.\n7.  **Misinterpretation ratio**:\n    $$\n    R = \\frac{\\Delta_{\\text{ord}}}{\\Delta_{\\text{cyc}}} = \\frac{(18 + 12\\sqrt{3})/13}{\\sqrt{3}/2} = \\frac{2(18 + 12\\sqrt{3})}{13\\sqrt{3}} = \\frac{36\\sqrt{3} + 72}{39} = \\frac{12\\sqrt{3} + 24}{13} \\approx 3.4450\n    $$\n    The ordinal model's boundary gap is over $3.4$ times larger than the cyclic model's, quantifying the misinterpretation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a reproducible experiment on SHAP values for cyclic features.\n    \"\"\"\n    # Define constants from the problem statement\n    A = 2.0\n    B = 1.0\n    TOL = 1e-12\n\n    # Construct the base dataset\n    months = np.arange(1, 13)\n    thetas = 2 * np.pi * months / 12\n    f_vals = A * np.sin(thetas) + B * np.cos(thetas)\n\n    # --- Ordinal Model Analysis ---\n    # Fit a least-squares linear model g(m) = alpha*m + beta\n    alpha, beta = np.polyfit(months, f_vals, 1)\n\n    # The SHAP value is phi_ord(m) = g(m) - E[g(M)].\n    # E[g(M)] = E[alpha*M + beta] = alpha*E[M] + beta.\n    # Since beta = mean(f) - alpha*mean(m) and mean(f) is numerically ~0,\n    # E[g(M)] is also numerically ~0.\n    # Therefore, phi_ord(m) is approximately g(m).\n    phi_ord_12 = alpha * 12 + beta\n    phi_ord_1 = alpha * 1 + beta\n    delta_ord = np.abs(phi_ord_12 - phi_ord_1)\n\n    # --- Cyclic Sine-Cosine Model Analysis ---\n    # Features are s_m = sin(theta_m) and c_m = cos(theta_m)\n    # The model is h(s,c) = A*s + B*c.\n    # As derived in the solution, due to feature distribution symmetries,\n    # the SHAP values are phi_s(m) = A*s_m and phi_c(m) = B*c_m.\n    s_vals = np.sin(thetas)\n    c_vals = np.cos(thetas)\n\n    phi_s = A * s_vals\n    phi_c = B * c_vals\n    \n    # Month indices for numpy arrays (0-indexed)\n    m1_idx, m3_idx, m5_idx, m6_idx, m12_idx = 0, 2, 4, 5, 11\n\n    # SHAP sum for the cyclic model\n    phi_sum_cyclic_12 = phi_s[m12_idx] + phi_c[m12_idx]\n    phi_sum_cyclic_1 = phi_s[m1_idx] + phi_c[m1_idx]\n    delta_cyc = np.abs(phi_sum_cyclic_12 - phi_sum_cyclic_1)\n\n    # --- Test Suite Evaluation ---\n\n    # 1. Adjacency misinterpretation check\n    adjacency_boolean = delta_ord > delta_cyc\n\n    # 2. Equal-sine attribution balance for m=1 and m=5\n    equal_sine_boolean = np.isclose(phi_s[m1_idx], phi_s[m5_idx], atol=TOL)\n\n    # 3. Opposite-cosine attribution balance for m=1 and m=5\n    cos_opposite_boolean = np.isclose(phi_c[m1_idx] + phi_c[m5_idx], 0, atol=TOL)\n\n    # 4. Explicit SHAP components at m=3\n    phi_s_3 = phi_s[m3_idx]\n    phi_c_3 = phi_c[m3_idx]\n\n    # 5. Explicit SHAP components at m=6\n    phi_s_6 = phi_s[m6_idx]\n    phi_c_6 = phi_c[m6_idx]\n\n    # 6. SHAP-sum consistency\n    test_months_indices = [m1_idx, m3_idx, m5_idx, m6_idx, m12_idx]\n    shap_sums = phi_s[test_months_indices] + phi_c[test_months_indices]\n    f_vals_test = f_vals[test_months_indices]\n    sum_consistency_boolean = np.all(np.isclose(shap_sums, f_vals_test, atol=TOL))\n    \n    # 7. Misinterpretation ratio\n    R = delta_ord / max(delta_cyc, 1e-12)\n\n    # Assemble final results list\n    results = [\n        adjacency_boolean,\n        equal_sine_boolean,\n        cos_opposite_boolean,\n        phi_s_3,\n        phi_c_3,\n        phi_s_6,\n        phi_c_6,\n        sum_consistency_boolean,\n        R\n    ]\n\n    # Format and print the final output\n    # Booleans are lowercased for standard Python str() conversion\n    formatted_results = [str(r).lower() if isinstance(r, bool) else f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice delves into one of the most fundamental challenges in modeling: the map is not the territory. This experiment  sets up a synthetic world where the true, non-linear data-generating process is known, then asks you to fit a misspecified linear model to it. By comparing the SHAP values from the simple model to the \"ground-truth\" attributions from the true function, you will directly measure the discrepancy and gain a crucial understanding that SHAP provides a faithful explanation of your model, which itself may be an imperfect approximation of reality.",
            "id": "3173341",
            "problem": "You are given a synthetic setting to study Shapley Additive Explanations (SHAP) from first principles. A model produces a scalar prediction as a function of three independent input features. You must implement a complete, runnable program that (i) generates data from a known data-generating process, (ii) fits a misspecified linear model by ordinary least squares, (iii) computes exact ground-truth Shapley attributions for the true data-generating function using the interventional definition, (iv) computes exact Shapley attributions for the fitted linear model under the same interventional semantics, and (v) reports quantitative discrepancies between the two for a fixed test suite of inputs.\n\nFundamental base:\n- Cooperative game theory Shapley value for a function of features: For a model output function $f(\\mathbf{x})$ with feature index set $\\mathcal{M}=\\{1,\\dots,M\\}$ and an instance $\\mathbf{x}$, define the set function $v(S)=\\mathbb{E}[f(X)\\mid X_S=\\mathbf{x}_S]$, where the expectation is taken with respect to a specified background distribution and $S\\subseteq\\mathcal{M}$. The Shapley value for feature $i$ is\n$$\n\\phi_i(\\mathbf{x})=\\sum_{S\\subseteq \\mathcal{M}\\setminus\\{i\\}} \\frac{|S|!\\,(M-|S|-1)!}{M!}\\,\\Big(v(S\\cup\\{i\\})-v(S)\\Big).\n$$\nInterventional SHAP uses the product of marginal feature distributions as the background, so if features are independent in the data-generating process, the conditional expectations factorize accordingly.\n- Ordinary least squares (OLS) solves $\\min_{\\boldsymbol{\\beta}}\\sum_{n=1}^N \\big(y_n - \\beta_0 - \\sum_{j=1}^M \\beta_j x_{n,j}\\big)^2$, yielding the linear predictor $\\hat f(\\mathbf{x})=\\hat\\beta_0+\\sum_{j=1}^M \\hat\\beta_j x_j$.\n\nData-generating process:\n- Number of features $M=3$. All angles are in radians.\n- Feature distributions are mutually independent:\n  - $X_1\\sim \\mathcal{N}(\\mu_1,\\sigma_1^2)$ with $\\mu_1=1.0$ and $\\sigma_1=2.0$.\n  - $X_2\\sim \\mathrm{Uniform}(a_2,b_2)$ with $a_2=-1.0$ and $b_2=2.0$.\n  - $X_3\\sim \\mathrm{Exponential}(\\lambda_3)$ with rate $\\lambda_3=1.5$ (so $\\mathbb{E}[X_3]=1/\\lambda_3$).\n- True regression function and noisy outcomes:\n  - Let constants be $c=0.7$, $w_1=3.0$, $w_2=2.5$, $w_3=1.2$, and $w_{12}=-1.7$.\n  - Define the true conditional mean function $f^\\star(\\mathbf{x}) = c + w_1 x_1 + w_2 x_2^2 + w_3 \\sin(x_3) + w_{12} x_1 x_2$.\n  - Observations are $Y = f^\\star(\\mathbf{X}) + \\varepsilon$ with $\\varepsilon\\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$, independent of $\\mathbf{X}$, and $\\sigma_\\varepsilon=0.3$.\n- Fit the misspecified linear model $\\hat f(\\mathbf{x})=\\hat\\beta_0+\\hat\\beta_1 x_1+\\hat\\beta_2 x_2+\\hat\\beta_3 x_3$ by OLS on $N$ independent training samples with $N=20000$.\n\nGround-truth SHAP under the interventional background:\n- Use the interventional definition $v(S)=\\mathbb{E}[f^\\star(X)\\mid X_S=\\mathbf{x}_S]$ with the true independent feature distribution. This induces a baseline $f^\\star_0=\\mathbb{E}[f^\\star(X)]$ and attributions $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x})$ that sum to $f^\\star(\\mathbf{x})-f^\\star_0$.\n- You must derive and use closed-form expressions for $\\mathbb{E}[X_2^2]$ and $\\mathbb{E}[\\sin(X_3)]$ under the specified distributions. All angles are in radians.\n\nModel-based SHAP for the fitted linear model:\n- Under the same interventional background (product of marginals) used above, compute the exact SHAP vector $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x})$ for the fitted linear predictor $\\hat f(\\mathbf{x})$.\n\nDiscrepancy metric:\n- For any instance $\\mathbf{x}$, define the total absolute attribution discrepancy as\n$$\nD(\\mathbf{x})=\\sum_{i=1}^3 \\left|\\phi^{\\mathrm{lin}}_i(\\mathbf{x}) - \\phi^{\\mathrm{true}}_i(\\mathbf{x})\\right|.\n$$\n\nTest suite:\n- Use the following four test inputs, with $x_3$ specified in radians:\n  1. $\\mathbf{x}^{(1)}=\\big(\\mu_1,\\ \\tfrac{a_2+b_2}{2},\\ \\tfrac{1}{\\lambda_3}\\big)$.\n  2. $\\mathbf{x}^{(2)}=\\big(\\mu_1+\\sigma_1,\\ b_2,\\ \\tfrac{1}{\\lambda_3}+1.0\\big)$.\n  3. $\\mathbf{x}^{(3)}=\\big(\\mu_1-2\\sigma_1,\\ a_2,\\ \\tfrac{1}{\\lambda_3}+3.0\\big)$.\n  4. $\\mathbf{x}^{(4)}=\\big(\\mu_1+0.5,\\ 0.0,\\ \\arcsin(\\mathbb{E}[\\sin(X_3)])\\big)$, where $\\arcsin$ denotes the principal value in $[0,\\tfrac{\\pi}{2}]$.\n\nRequired computations and outputs:\n- Generate the training data with a fixed random seed to ensure determinism.\n- Fit the linear model by OLS.\n- For each test input $\\mathbf{x}^{(k)}$, compute the ground-truth interventional SHAP vector $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x}^{(k)})$ and the linear-model SHAP vector $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x}^{(k)})$ under the same interventional background.\n- For each test input, compute $D(\\mathbf{x}^{(k)})$ as defined above.\n\nFinal output format:\n- Your program should produce a single line of output containing the four discrepancy values as a comma-separated list enclosed in square brackets, in the order of the four test inputs, i.e., a string of the form $[d_1,d_2,d_3,d_4]$ where each $d_k$ is a floating-point number.",
            "solution": "The problem requires a quantitative comparison of Shapley Additive Explanations (SHAP) for a true, non-linear data-generating process and a misspecified linear model fitted to data from that process. The core of the task is to derive and implement the exact formulas for interventional SHAP values for both the true function and the linear approximation.\n\nFirst, we establish the theoretical groundwork by deriving the necessary expected values, which form the baseline for the SHAP calculations. Second, we derive the closed-form expressions for the ground-truth SHAP values, $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x})$, based on the true model function $f^\\star(\\mathbf{x})$. Third, we derive the corresponding SHAP values, $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x})$, for the fitted ordinary least squares (OLS) linear model, $\\hat{f}(\\mathbf{x})$. Finally, we outline the full computational procedure to obtain the specified discrepancy metric for the given test suite.\n\nThe interventional SHAP framework requires expectations over a background distribution, which is defined as the product of the independent marginal distributions of the features. The feature distributions are given as:\n- $X_1 \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$ with $\\mu_1=1.0$ and $\\sigma_1=2.0$.\n- $X_2 \\sim \\mathrm{Uniform}(a_2, b_2)$ with $a_2=-1.0$ and $b_2=2.0$.\n- $X_3 \\sim \\mathrm{Exponential}(\\lambda_3)$ with $\\lambda_3=1.5$.\nThe true data-generating function is $f^\\star(\\mathbf{x}) = c + w_1 x_1 + w_2 x_2^2 + w_3 \\sin(x_3) + w_{12} x_1 x_2$. The SHAP calculations require the expectations of the terms appearing in this function.\n\nThe first-order moments are:\n- $\\mathbb{E}[X_1] = \\mu_1 = 1.0$.\n- $\\mathbb{E}[X_2] = \\frac{a_2+b_2}{2} = \\frac{-1.0+2.0}{2} = 0.5$.\n- $\\mathbb{E}[X_3] = \\frac{1}{\\lambda_3} = \\frac{1}{1.5} = \\frac{2}{3}$.\nWe also need expectations of the non-linear terms in $f^\\star(\\mathbf{x})$:\n- $\\mathbb{E}[X_2^2]$: For a uniform distribution $\\mathrm{U}(a, b)$, the second moment is $\\mathbb{E}[X^2] = \\frac{1}{b-a} \\int_a^b x^2 dx = \\frac{b^2+ab+a^2}{3}$.\n$$ \\mathbb{E}[X_2^2] = \\frac{(2.0)^2 + (2.0)(-1.0) + (-1.0)^2}{3} = \\frac{4.0 - 2.0 + 1.0}{3} = \\frac{3.0}{3.0} = 1.0. $$\n- $\\mathbb{E}[\\sin(X_3)]$: For an exponential distribution with rate $\\lambda_3$, the PDF is $p(x) = \\lambda_3 e^{-\\lambda_3 x}$ for $x \\ge 0$. The expectation is computed via the Laplace transform of $\\sin(x)$:\n$$ \\mathbb{E}[\\sin(X_3)] = \\int_0^\\infty \\sin(x) \\lambda_3 e^{-\\lambda_3 x} dx = \\lambda_3 \\int_0^\\infty \\sin(x) e^{-\\lambda_3 x} dx. $$\nThe integral is the Laplace transform of $\\sin(t)$ with $a=1$, evaluated at $s=\\lambda_3$, which is $\\frac{1}{s^2+1}$.\n$$ \\mathbb{E}[\\sin(X_3)] = \\lambda_3 \\left( \\frac{1}{\\lambda_3^2+1} \\right) = \\frac{1.5}{1.5^2+1} = \\frac{1.5}{3.25} = \\frac{6}{13}. $$\nThese values are fundamental for calculating the baseline (expected) predictions for both models.\n\nThe SHAP values are defined by $\\phi_i(\\mathbf{x}) = \\sum_{S\\subseteq \\mathcal{M}\\setminus\\{i\\}} \\frac{|S|!\\,(M-|S|-1)!}{M!}\\,\\big(v(S\\cup\\{i\\})-v(S)\\big)$, where $v(S)=\\mathbb{E}[f^\\star(X)\\mid X_S=\\mathbf{x}_S]$. Due to feature independence, this simplifies to taking expectations over the features not in $S$. A key property of SHAP is additivity: if $f = g_1 + g_2$, then $\\phi_i(f) = \\phi_i(g_1) + \\phi_i(g_2)$. We decompose $f^\\star(\\mathbf{x})$ as follows:\n$$ f^\\star(\\mathbf{x}) = c + \\underbrace{w_1 x_1}_{f_1(x_1)} + \\underbrace{w_2 x_2^2}_{f_2(x_2)} + \\underbrace{w_3 \\sin(x_3)}_{f_3(x_3)} + \\underbrace{w_{12} x_1 x_2}_{f_{12}(x_1, x_2)}. $$\nWe compute the SHAP values for each component. For a function of a single variable $f_i(x_i)$, the only non-zero SHAP value is for feature $i$: $\\phi_i(f_i) = f_i(x_i) - \\mathbb{E}[f_i(X_i)]$. For the interaction term $g(x_1, x_2) = x_1 x_2$, the attributions for features $1$ and $2$ (in a $3$-feature space), derived from the Shapley formula, are:\n$$ \\phi_1(g) = \\frac{1}{2}(x_1-\\mathbb{E}[X_1])(x_2+\\mathbb{E}[X_2]), \\quad \\phi_2(g) = \\frac{1}{2}(x_2-\\mathbb{E}[X_2])(x_1+\\mathbb{E}[X_1]). $$\nCombining these results using additivity, we get the exact SHAP values for $f^\\star$:\n- $\\phi_1^{\\mathrm{true}}(\\mathbf{x}) = \\phi_1(f_1) + \\phi_1(f_{12}) = w_1(x_1 - \\mathbb{E}[X_1]) + \\frac{w_{12}}{2}(x_1 - \\mathbb{E}[X_1])(x_2 + \\mathbb{E}[X_2])$.\n- $\\phi_2^{\\mathrm{true}}(\\mathbf{x}) = \\phi_2(f_2) + \\phi_2(f_{12}) = w_2(x_2^2 - \\mathbb{E}[X_2^2]) + \\frac{w_{12}}{2}(x_2 - \\mathbb{E}[X_2])(x_1 + \\mathbb{E}[X_1])$.\n- $\\phi_3^{\\mathrm{true}}(\\mathbf{x}) = \\phi_3(f_3) = w_3(\\sin(x_3) - \\mathbb{E}[\\sin(X_3)])$.\nThese equations provide the ground-truth attributions against which the linear model's attributions will be compared.\n\nThe misspecified model is a linear function fitted by OLS: $\\hat{f}(\\mathbf{x}) = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 + \\hat\\beta_3 x_3$. The coefficients $(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2, \\hat\\beta_3)$ are estimated from $N=20000$ training samples. For a linear model with independent features, the interventional SHAP values have a simple, exact form. The total contribution $ \\hat{f}(\\mathbf{x}) - \\mathbb{E}[\\hat{f}(X)] $ is:\n$$ (\\hat\\beta_0 + \\sum_{j=1}^3 \\hat\\beta_j x_j) - (\\hat\\beta_0 + \\sum_{j=1}^3 \\hat\\beta_j \\mathbb{E}[X_j]) = \\sum_{j=1}^3 \\hat\\beta_j(x_j - \\mathbb{E}[X_j]). $$\nThis expression is already a sum of terms, each depending on a single feature. This unique decomposition gives the SHAP values directly:\n$$ \\phi_i^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_i(x_i - \\mathbb{E}[X_i]). $$\nSpecifically for our $3$-feature model:\n- $\\phi_1^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_1(x_1 - \\mathbb{E}[X_1])$.\n- $\\phi_2^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_2(x_2 - \\mathbb{E}[X_2])$.\n- $\\phi_3^{\\mathrm{lin}}(\\mathbf{x}) = \\hat\\beta_3(x_3 - \\mathbb{E}[X_3])$.\nThese formulas show that the linear model incorrectly assumes that the attribution for a feature is simply its centered value scaled by a single coefficient, ignoring any non-linearities or interactions present in the true function $f^\\star$.\n\nThe implementation proceeds as follows:\n$1$. **Data Generation**: Generate $N=20000$ samples from the specified independent distributions for $X_1$, $X_2$, and $X_3$.\n$2$. **True Outcomes**: Compute the true function values $f^\\star(\\mathbf{X})$ and add Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ with $\\sigma_\\varepsilon=0.3$ to get the observed outcomes $Y$.\n$3$. **OLS Fitting**: Form the design matrix $\\mathbf{X}_{\\mathrm{b}}$ with a column of ones for the intercept. Solve the normal equations $\\mathbf{X}_{\\mathrm{b}}^T \\mathbf{X}_{\\mathrm{b}} \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}_{\\mathrm{b}}^T Y$ to find the OLS coefficient vector $\\hat{\\boldsymbol{\\beta}} = (\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2, \\hat\\beta_3)^T$.\n$4$. **SHAP Calculation**: For each test input $\\mathbf{x}^{(k)}$ from the provided suite:\n    - Calculate the ground-truth SHAP vector $\\boldsymbol{\\phi}^{\\mathrm{true}}(\\mathbf{x}^{(k)})$ using the derived formulas and known parameters ($w_i$, etc.).\n    - Calculate the linear model's SHAP vector $\\boldsymbol{\\phi}^{\\mathrm{lin}}(\\mathbf{x}^{(k)})$ using the fitted coefficients $\\hat\\beta_i$.\n$5$. **Discrepancy Metric**: Compute the total absolute attribution discrepancy for each test input:\n$$ D(\\mathbf{x}^{(k)}) = \\sum_{i=1}^3 \\left|\\phi^{\\mathrm{lin}}_i(\\mathbf{x}^{(k)}) - \\phi^{\\mathrm{true}}_i(\\mathbf{x}^{(k)})\\right|. $$\nThe program will execute these steps and report the computed values of $D(\\mathbf{x}^{(k)})$ for the four test cases. A fixed random seed ensures deterministic and reproducible results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full procedure to calculate SHAP value discrepancies.\n    \"\"\"\n    # Use a fixed random seed for reproducibility\n    SEED = 0\n    rng = np.random.default_rng(SEED)\n\n    # ----------------------------------------------------------------------\n    # 1. Define constants and parameters from the problem statement\n    # ----------------------------------------------------------------------\n    # Data-generating process parameters\n    N = 20000\n    mu1, sigma1 = 1.0, 2.0\n    a2, b2 = -1.0, 2.0\n    lambda3 = 1.5\n    \n    # True function parameters\n    c = 0.7\n    w1 = 3.0\n    w2 = 2.5\n    w3 = 1.2\n    w12 = -1.7\n    \n    # Noise parameter\n    sigma_eps = 0.3\n\n    # ----------------------------------------------------------------------\n    # 2. Generate training data\n    # ----------------------------------------------------------------------\n    X1_train = rng.normal(mu1, sigma1, N)\n    X2_train = rng.uniform(a2, b2, N)\n    X3_train = rng.exponential(1.0 / lambda3, N)\n    X_train = np.stack([X1_train, X2_train, X3_train], axis=1)\n\n    f_star = c + w1*X1_train + w2*X2_train**2 + w3*np.sin(X3_train) + w12*X1_train*X2_train\n    Y_train = f_star + rng.normal(0, sigma_eps, N)\n\n    # ----------------------------------------------------------------------\n    # 3. Fit the misspecified linear model by OLS\n    # ----------------------------------------------------------------------\n    X_b = np.c_[np.ones(N), X_train]\n    # Solve X_b.T @ X_b @ beta_hat = X_b.T @ Y_train\n    beta_hat = np.linalg.solve(X_b.T @ X_b, X_b.T @ Y_train)\n    beta0_hat, beta1_hat, beta2_hat, beta3_hat = beta_hat\n\n    # ----------------------------------------------------------------------\n    # 4. Define background expectations and test suite\n    # ----------------------------------------------------------------------\n    E_X1 = mu1\n    E_X2 = (a2 + b2) / 2.0\n    E_X3 = 1.0 / lambda3\n    E_X2_sq = 1.0  # Derived as (b2^3 - a2^3) / (3*(b2-a2)) = 1.0\n    E_sinX3 = lambda3 / (lambda3**2 + 1.0) # Derived from Laplace transform\n\n    test_cases = [\n        (mu1, (a2 + b2) / 2.0, 1.0 / lambda3),\n        (mu1 + sigma1, b2, 1.0 / lambda3 + 1.0),\n        (mu1 - 2 * sigma1, a2, 1.0 / lambda3 + 3.0),\n        (mu1 + 0.5, 0.0, np.arcsin(E_sinX3))\n    ]\n\n    discrepancies = []\n    \n    # ----------------------------------------------------------------------\n    # 5. Calculate SHAP values and discrepancies for each test case\n    # ----------------------------------------------------------------------\n    for x_test in test_cases:\n        x1, x2, x3 = x_test\n\n        # --- Ground-truth SHAP values ---\n        phi1_true = w1 * (x1 - E_X1) + (w12 / 2.0) * (x1 - E_X1) * (x2 + E_X2)\n        phi2_true = w2 * (x2**2 - E_X2_sq) + (w12 / 2.0) * (x2 - E_X2) * (x1 + E_X1)\n        phi3_true = w3 * (np.sin(x3) - E_sinX3)\n        \n        # --- Linear model SHAP values ---\n        phi1_lin = beta1_hat * (x1 - E_X1)\n        phi2_lin = beta2_hat * (x2 - E_X2)\n        phi3_lin = beta3_hat * (x3 - E_X3)\n        \n        # --- Total absolute attribution discrepancy ---\n        D_x = (\n            np.abs(phi1_lin - phi1_true) + \n            np.abs(phi2_lin - phi2_true) + \n            np.abs(phi3_lin - phi3_true)\n        )\n        discrepancies.append(D_x)\n\n    # ----------------------------------------------------------------------\n    # 6. Format and print the final output\n    # ----------------------------------------------------------------------\n    print(f\"[{','.join(f'{d:.7f}' for d in discrepancies)}]\")\n\nsolve()\n\n```"
        }
    ]
}