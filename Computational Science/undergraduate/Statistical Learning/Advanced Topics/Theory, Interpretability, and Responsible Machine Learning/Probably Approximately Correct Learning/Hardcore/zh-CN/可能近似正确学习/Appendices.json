{
    "hands_on_practices": [
        {
            "introduction": "PAC 学习理论的核心是量化假设类的“复杂度”，其中 VC 维是最重要的度量之一。本练习将理论与实践相结合，要求你首先从第一性原理出发推导出常见假设类的 VC 维和断点，然后通过编写程序来经验性地验证“打散”现象在断点处是如何失效的 。这个过程将抽象的定义转化为具体的直觉，是真正掌握学习理论的关键一步。",
            "id": "3161849",
            "problem": "在可能近似正确（PAC）学习和 Vapnik–Chervonenkis（VC）维度的背景下，考虑四个假设类 $\\mathcal{H}$：实数线上的阈值、实数线上的区间、二维欧几里得空间中的线性分隔器，以及二维欧几里得空间中轴对齐的矩形。本问题的基础在于打散（shattering）、成长函数（growth function）、VC维度（VC dimension）和断点（breakpoint）的核心定义。定义如下：如果对于有限集 $S$ 的每一种标签分配，都存在一个假设 $h \\in \\mathcal{H}$ 能够实现该标签分配，则称假设类 $\\mathcal{H}$ 打散了该集合 $S$；成长函数 $m_{\\mathcal{H}}(m)$ 是 $\\mathcal{H}$ 在任何大小为 $m$ 的集合上能够实现的不同标签分配的最大数量；$\\mathcal{H}$ 的 Vapnik–Chervonenkis（VC）维度 $d$ 是存在一个被 $\\mathcal{H}$ 打散的大小为 $m$ 的集合的最大 $m$ 值；$\\mathcal{H}$ 的断点 $k$ 是使得 $m_{\\mathcal{H}}(m)  2^m$ 的最小整数 $m$，等价地，是指不存在大小为 $m$ 的集合可以被 $\\mathcal{H}$ 打散的最小 $m$ 值。从这些定义出发，通过第一性原理（不使用快捷公式）推导以下四个类别的理论断点，然后实现一个程序，以经验方式验证打散失败的转变点。\n\n假设类：\n- $\\mathbb{R}$ 上的阈值：$\\mathcal{H}_{\\mathrm{thr}} = \\{ h_t(x) = \\mathbf{1}[x \\ge t] : t \\in \\mathbb{R} \\}$。\n- $\\mathbb{R}$ 上的区间：$\\mathcal{H}_{\\mathrm{int}} = \\{ h_{a,b}(x) = \\mathbf{1}[a \\le x \\le b] : a \\le b,\\, a,b \\in \\mathbb{R} \\}$。\n- $\\mathbb{R}^2$ 上的线性分隔器：$\\mathcal{H}_{\\mathrm{lin}} = \\{ h_{w,b}(x) = \\mathbf{1}[w^\\top x + b \\ge 0] : w \\in \\mathbb{R}^2,\\, b \\in \\mathbb{R} \\}$。\n- $\\mathbb{R}^2$ 上轴对齐的矩形：$\\mathcal{H}_{\\mathrm{rect}} = \\{ h_A(x) = \\mathbf{1}[x \\in [a_x,b_x]\\times[a_y,b_y]] : a_x \\le b_x,\\, a_y \\le b_y,\\, a_x,a_y,b_x,b_y \\in \\mathbb{R} \\}$。\n\n任务：\n1. 仅使用上述基本定义，通过推理确定四个类别 $\\mathcal{H}_{\\mathrm{thr}}$、$\\mathcal{H}_{\\mathrm{int}}$、$\\mathcal{H}_{\\mathrm{lin}}$ 和 $\\mathcal{H}_{\\mathrm{rect}}$ 中每一个的 VC 维度 $d$ 和理论断点 $k$。\n2. 为每个类别构建具体的数据集，以在大小为 $m = d$ 和 $m = k$ 时经验性地揭示从打散到失败的转变。使用以下确定性数据集：\n   - 对于 $\\mathcal{H}_{\\mathrm{thr}}$：在 $m=d$ 时使用 $X^{\\mathrm{thr}}_d = [\\,0\\,]$；在 $m=k$ 时使用 $X^{\\mathrm{thr}}_k = [\\,0,\\,1\\,]$。\n   - 对于 $\\mathcal{H}_{\\mathrm{int}}$：在 $m=d$ 时使用 $X^{\\mathrm{int}}_d = [\\,0,\\,1\\,]$；在 $m=k$ 时使用 $X^{\\mathrm{int}}_k = [\\,0,\\,1,\\,2\\,]$。\n   - 对于 $\\mathcal{H}_{\\mathrm{lin}}$：在 $m=d$ 时使用 $X^{\\mathrm{lin}}_d = [\\,(0,0),\\,(1,0),\\,(0,1)\\,]$；在 $m=k$ 时使用 $X^{\\mathrm{lin}}_k = [\\,(0,0),\\,(1,0),\\,(1,1),\\,(0,1)\\,]$。\n   - 对于 $\\mathcal{H}_{\\mathrm{rect}}$：在 $m=d$ 时使用 $X^{\\mathrm{rect}}_d = [\\,(1,0),\\,(-1,0),\\,(0,1),\\,(0,-1)\\,]$；在 $m=k$ 时使用 $X^{\\mathrm{rect}}_k = [\\,(1,0),\\,(-1,0),\\,(0,1),\\,(0,-1),\\,(\\tfrac{1}{2},\\tfrac{1}{2})\\,]$。\n   所有坐标均采用标准欧几里得单位（无单位实数）。\n3. 对于每个类别 $\\mathcal{H}$ 和每个数据集 $X$，通过详尽枚举 $X$ 的所有 $2^{|X|}$ 种标签分配，并为每种标签分配判断是否存在 $\\mathcal{H}$ 中的假设能够实现它，来经验性地测试 $X$ 是否被 $\\mathcal{H}$ 打散。您的决策过程必须直接根据定义以及每个类别的几何或顺序结构来设计，不能调用外部的黑盒学习包。\n4. 根据每个类别在两个数据集 $(X_d, X_k)$ 上的经验结果，将经验断点 $\\hat{k}$ 定义为导致打散失败的最小测试大小 $m \\in \\{\\,|X_d|,\\,|X_k|\\,\\}$。如果两个测试集都被打散（对于正确选择的数据集，这不应发生），则将 $\\hat{k}$ 设置为测试大小中的较大者。\n5. 比较每个类别的理论断点 $k$ 和经验断点 $\\hat{k}$。\n\n测试套件和输出规范：\n- 使用上述指定的四个类别及其数据集作为测试套件。\n- 为每个类别分配一个整数标识符：$0$ 表示 $\\mathbb{R}$ 上的阈值，$1$ 表示 $\\mathbb{R}$ 上的区间，$2$ 表示 $\\mathbb{R}^2$ 上的线性分隔器，$3$ 表示 $\\mathbb{R}^2$ 上轴对齐的矩形。\n- 您的程序应为每个类别计算一个结果列表 $[\\,\\mathrm{id},\\,k,\\,S_d,\\,S_k,\\,\\hat{k},\\,A\\,]$，其中 $\\mathrm{id}$ 是整数标识符，$k$ 是理论断点，$S_d$ 是一个布尔值，指示 $X_d$ 是否被打散，$S_k$ 是一个布尔值，指示 $X_k$ 是否被打散，$\\hat{k}$ 是如上定义的经验断点，$A$ 是一个布尔值，说明 $k = \\hat{k}$ 是否成立。\n- 最终输出格式：您的程序应生成单行输出，其中包含四个类别的结果，形式为用方括号括起来的逗号分隔列表。每个类别的结果本身也必须是一个方括号列表。因此，总格式为 $[\\,[\\mathrm{id},k,S_d,S_k,\\hat{k},A],\\,[\\mathrm{id},k,S_d,S_k,\\hat{k},A],\\,[\\mathrm{id},k,S_d,S_k,\\hat{k},A],\\,[\\mathrm{id},k,S_d,S_k,\\hat{k},A]\\,]$。布尔值必须打印为 $True$ 或 $False$，整数以十进制数字表示。",
            "solution": "确定不同假设类的 Vapnik-Chervonenkis (VC) 维度和断点是统计学习理论中的一个基础练习。解决方案需要一个双管齐下的方法：从第一性原理进行理论推导，并通过计算进行经验验证。\n\n提供的定义是标准的：\n- 如果一个假设类 $\\mathcal{H}$ 能够实现集合 $S$ 中所有 $2^{|S|}$ 种可能的二元标签分配，则称 $\\mathcal{H}$ **打散**了集合 $S$。\n- **成长函数** $m_{\\mathcal{H}}(m)$ 是 $\\mathcal{H}$ 在任何 $m$ 个点组成的集合上能产生的二分（标签分配）的最大数量。\n- **VC 维度** $d = \\mathrm{VCdim}(\\mathcal{H})$ 是能被 $\\mathcal{H}$ 打散的最大集合的大小。形式上，$d = \\max \\{ m : m_{\\mathcal{H}}(m) = 2^m \\}$。\n- **断点** $k$ 是指不存在大小为 $m$ 的集合能被 $\\mathcal{H}$ 打散的最小整数 $m$。形式上，$k = \\min \\{ m : m_{\\mathcal{H}}(m)  2^m \\}$。\n由这些定义可以直接得出，断点 $k$ 等于 VC 维度加一，即 $k = d+1$。其逻辑是，$d$ 是*能被*打散的最大规模，因此 $d+1$ 必须是*不能被*打散的最小规模。我们将从第一性原理为每个类别推导 $d$ 和 $k$。\n\n### 1. 类别 $\\mathcal{H}_{\\mathrm{thr}}$: $\\mathbb{R}$ 上的阈值\n$\\mathcal{H}_{\\mathrm{thr}} = \\{ h_t(x) = \\mathbf{1}[x \\ge t] : t \\in \\mathbb{R} \\}$。此类中的假设将阈值 $t$ 右侧的所有点标记为正（$1$），左侧的所有点标记为负（$0$）。\n\n**理论分析 ($d$ 与 $k$)**\n- **VC 维度 ($d$):**\n  - **证明 $d \\ge 1$**：我们必须找到一个大小为 $m=1$ 的集合可以被打破。设 $S = \\{x_1\\}$。存在 $2^1=2$ 种标签分配：$\\{0\\}$ 和 $\\{1\\}$。\n    - 标签分配 $\\{1\\}$：我们需要 $\\mathbf{1}[x_1 \\ge t] = 1$，这要求 $x_1 \\ge t$。我们可以选择 $t = x_1$。\n    - 标签分配 $\\{0\\}$：我们需要 $\\mathbf{1}[x_1 \\ge t] = 0$，这要求 $x_1  t$。我们可以选择 $t = x_1 + \\epsilon$ 对于任意 $\\epsilon > 0$。\n    由于两种标签分配都可以实现，任何大小为 1 的集合都能被打破。因此，$d \\ge 1$。\n  - **证明 $d  2$**：我们必须证明*任何*大小为 $m=2$ 的集合都不能被打破。设 $S = \\{x_1, x_2\\}$ 为任意两个不同点的集合。不失一般性，假设 $x_1  x_2$。有 $2^2=4$ 种可能的标签分配：$(0,0), (0,1), (1,0), (1,1)$。\n    - 标签分配 $(0,0)$：选择 $t > x_2$。则 $x_1  t$ 和 $x_2  t$，所以 $h_t(x_1)=0, h_t(x_2)=0$。可实现。\n    - 标签分配 $(0,1)$：选择 $x_1  t \\le x_2$。则 $x_1  t$ 和 $x_2 \\ge t$，所以 $h_t(x_1)=0, h_t(x_2)=1$。可实现。\n    - 标签分配 $(1,1)$：选择 $t \\le x_1$。则 $x_1 \\ge t$ 和 $x_2 \\ge t$，所以 $h_t(x_1)=1, h_t(x_2)=1$。可实现。\n    - 标签分配 $(1,0)$：我们需要 $h_t(x_1)=1$ 和 $h_t(x_2)=0$。这意味着 $x_1 \\ge t$ 和 $x_2  t$。将这些结合起来得到 $x_2  t \\le x_1$，这与我们 $x_1  x_2$ 的假设相矛盾。这种标签分配是不可能实现的。\n  由于对于任何大小为 2 的集合，都存在一种无法实现的标签分配，所以没有大小为 2 的集合能被打破。因此，$d  2$。\n  - **结论**：从 $d \\ge 1$ 和 $d  2$ 可得，VC 维度为 $d=1$。\n\n- **断点 ($k$)**：断点 $k$ 是不能被打破的集合的最小大小 $m$。我们刚刚证明了 $m=2$ 是这样的最小大小。因此，断点为 $k=2$。这与 $k=d+1$ 一致。\n\n**经验验证逻辑**\n如果存在一个阈值 $t$ 使得对于所有 $(x_i, y_i)$ 都有 $\\mathbf{1}[x_i \\ge t] = y_i$，那么集合 $X$ 的一种标签分配 $y$ 就可以被 $\\mathcal{H}_{\\mathrm{thr}}$ 实现。设 $X_+ = \\{x_i | y_i=1\\}$ 和 $X_- = \\{x_i | y_i=0\\}$。该条件等价于找到一个 $t$，使得对于所有 $x_i \\in X_+$ 都有 $t \\le x_i$，且对于所有 $x_j \\in X_-$ 都有 $t > x_j$。这当且仅当 $\\max(X_-)  \\min(X_+)$ 时是可能的。（如果 $X_-$ 为空，我们定义 $\\max(X_-)=-\\infty$；如果 $X_+$ 为空，我们定义 $\\min(X_+)=\\infty$。）为了测试打散，我们生成所有 $2^{|X|}$ 种标签分配，并检查每种分配是否满足此条件。\n\n### 2. 类别 $\\mathcal{H}_{\\mathrm{int}}$: $\\mathbb{R}$ 上的区间\n$\\mathcal{H}_{\\mathrm{int}} = \\{ h_{a,b}(x) = \\mathbf{1}[a \\le x \\le b] : a \\le b,\\, a,b \\in \\mathbb{R} \\}$。此类中的假设将区间 $[a,b]$ 内的点标记为正。\n\n**理论分析 ($d$ 与 $k$)**\n- **VC 维度 ($d$):**\n  - **证明 $d \\ge 2$**：我们必须找到一个大小为 $m=2$ 的集合可以被打破。设 $S = \\{x_1, x_2\\}$ 且 $x_1  x_2$。\n    - 标签分配 $(0,0)$：选择与 $S$ 不相交的 $[a,b]$，例如 $b  x_1$。\n    - 标签分配 $(1,0)$：选择 $[a,b]$ 只包含 $x_1$，例如 $a=b=x_1$。\n    - 标签分配 $(0,1)$：选择 $[a,b]$ 只包含 $x_2$，例如 $a=b=x_2$。\n    - 标签分配 $(1,1)$：选择 $[a,b]$ 同时包含两者，例如 $a=x_1, b=x_2$。\n    所有 4 种标签分配都可实现。因此，$d \\ge 2$。\n  - **证明 $d  3$**：我们必须证明*任何*大小为 $m=3$ 的集合都不能被打破。设 $S = \\{x_1, x_2, x_3\\}$ 且 $x_1  x_2  x_3$。考虑标签分配 $(1,0,1)$。要实现这一点，我们需要一个单一区间 $[a,b]$，使得 $x_1 \\in [a,b]$, $x_3 \\in [a,b]$，并且 $x_2 \\notin [a,b]$。对于一个区间，如果它包含两个点 $x_1$ 和 $x_3$，它必须包含它们之间的所有点。由于 $x_1  x_2  x_3$，任何包含 $x_1$ 和 $x_3$ 的区间也必须包含 $x_2$。这与要求 $x_2$ 标记为 0 相矛盾。因此，标签分配 $(1,0,1)$ 是不可实现的。\n  由于对于任何大小为 3 的集合，都存在一种无法实现的标签分配，所以没有大小为 3 的集合能被打破。因此，$d  3$。\n  - **结论**：从 $d \\ge 2$ 和 $d  3$ 可得，VC 维度为 $d=2$。\n\n- **断点 ($k$)**：断点 $k$ 是不能被打破的集合的最小大小 $m$。我们已经证明了这个大小是 $m=3$。因此，断点为 $k=3$。这与 $k=d+1$ 一致。\n\n**经验验证逻辑**\n如果正例点在实数轴上是“连续”的，那么集合 $X$ 的一种标签分配 $y$ 就可以被 $\\mathcal{H}_{\\mathrm{int}}$ 实现。设 $X_+ = \\{x_i | y_i=1\\}$ 和 $X_- = \\{x_i | y_i=0\\}$。如果能找到一个区间 $[a,b]$ 包含所有 $X_+$ 但不包含任何 $X_-$，则该标签分配是可实现的。最紧凑的此类区间是 $[\\min(X_+), \\max(X_+)]$。当且仅当没有来自 $X_-$ 的点落入此区间内时，该标签分配是可实现的。（如果 $|X_+|\\le 1$，则总是可实现的）。\n\n### 3. 类别 $\\mathcal{H}_{\\mathrm{lin}}$: $\\mathbb{R}^2$ 上的线性分隔器\n$\\mathcal{H}_{\\mathrm{lin}} = \\{ h_{w,b}(x) = \\mathbf{1}[w^\\top x + b \\ge 0] : w \\in \\mathbb{R}^2,\\, b \\in \\mathbb{R} \\}$。此类中的假设是半平面。\n\n**理论分析 ($d$ 与 $k$)**\n- **VC 维度 ($d$):**\n  - **证明 $d \\ge 3$**：我们必须找到一个大小为 $m=3$ 的集合可以被打破。设 $S$ 是一个由 3 个不共线的点组成的集合，例如一个三角形的顶点。将这 3 个点划分为两个集合（正例和负例）的任何划分都可以用一条线实现。\n    - 标签分配 $(0,0,0)$ 或 $(1,1,1)$：是平凡的（例如，一条远离所有点的线）。\n    - 将一个点归为一类，另外两个点归为另一类的标签分配（例如 $(1,0,0)$）：可以画一条线将单个点与其他两个点分开。\n    由于所有 $2^3=8$ 种标签分配都可以实现，任何 3 个不共线的点组成的集合都可以被打破。因此，$d \\ge 3$。\n  - **证明 $d  4$**：我们必须证明*任何*大小为 $m=4$ 的集合都不能被打破。设 $S$ 是 $\\mathbb{R}^2$ 中任意 4 个点的集合。根据 Radon 定理，$\\mathbb{R}^2$ 中任意 $d+2=4$ 个点可以被划分为两个子集 $S_1$ 和 $S_2$，它们的凸包相交。\n    - 情况1：这些点形成一个凸四边形。设点按周边顺序为 $p_1,p_2,p_3,p_4$。考虑分别分配标签 $1,0,1,0$。正例点集为 $X_+ = \\{p_1,p_3\\}$，负例点集为 $X_- = \\{p_2,p_4\\}$。要存在一个线性分隔器， $X_+$ 和 $X_-$ 的凸包必须不相交。在这里，$\\mathrm{conv}(X_+)$ 是连接 $p_1$ 和 $p_3$ 的线段，$\\mathrm{conv}(X_-)$ 是连接 $p_2$ 和 $p_4$ 的线段。这些是四边形的对角线，它们必须相交。由于它们的凸包相交，没有线可以将它们分开。\n    - 情况2：一个点位于其他三个点形成的凸包（三角形）内部。设 $p_4$ 位于由 $p_1,p_2,p_3$ 形成的三角形内部。考虑给 $p_4$ 分配标签 0，给 $p_1,p_2,p_3$ 分配标签 1。任何包含 $p_1,p_2,p_3$ 的半平面都必须包含它们的凸包（即该三角形）。由于 $p_4$ 在三角形内部，它也必须在该半平面内，因此必须被标记为 1，这与要求相矛盾。\n    在任何一种情况下，都存在一种无法实现的标签分配。因此，没有大小为 4 的点集能被打破。因此，$d  4$。\n  - **结论**：从 $d \\ge 3$ 和 $d  4$ 可得，VC 维度为 $d=3$。\n\n- **断点 ($k$)**：断点是不能被打破的集合的最小大小 $m$。我们证明了这是 $m=4$。因此，断点为 $k=4$。\n\n**经验验证逻辑**\n如果正例点集 $X_+$ 可以通过一条线与负例点集 $X_-$ 分开，则该标签分配是可实现的。这当且仅当 $\\mathrm{conv}(X_+) \\cap \\mathrm{conv}(X_-) = \\emptyset$。对于测试集中少量点的情况，我们可以通过几何方式检查这一点。\n- 对于 $m=3$ 个不共线的点，任何划分为 $X_+$ 和 $X_-$ 都会导致不相交的凸包（一个点和一个线段，或者其中一个集合为空）。它们总是可分的。\n- 对于 $m=4$，我们必须检查划分。如果 $|X_+|=2, |X_-|=2$，我们检查两条线段是否相交。如果 $|X_+|=1, |X_-|=3$，我们检查一个点是否位于一个三角形内。\n\n### 4. 类别 $\\mathcal{H}_{\\mathrm{rect}}$: $\\mathbb{R}^2$ 上轴对齐的矩形\n$\\mathcal{H}_{\\mathrm{rect}} = \\{ h_A(x) = \\mathbf{1}[x \\in [a_x,b_x]\\times[a_y,b_y]] \\}$。假设是轴对齐的矩形。\n\n**理论分析 ($d$ 与 $k$)**\n- **VC 维度 ($d$):**\n  - **证明 $d \\ge 4$**：我们必须找到一个大小为 $m=4$ 的集合可以被打破。考虑集合 $S = \\{(1,0), (-1,0), (0,1), (0,-1)\\}$。这种形如菱形的点配置可以被打破。对于要被标记为正的任何点子集，我们都可以构造一个轴对齐的矩形，恰好包含该子集。例如，要实现对点 $((1,0), (-1,0), (0,1), (0,-1))$ 的标签分配 $(1,1,0,0)$，我们需要只包含前两个点。对于一个小的 $\\epsilon > 0$，矩形 $[-1,1] \\times [-\\epsilon, \\epsilon]$ 可以做到。最具有挑战性的情况通常是“棋盘”模式。让我们将 $(1,0)$ 和 $(0,1)$ 标记为正，将 $(-1,0)$ 和 $(0,-1)$ 标记为负。正例点的最小边界框是 $[0,1] \\times [0,1]$。这个矩形不包含负例点。因此，这种标签分配是可实现的。可以验证所有 16 种标签分配都是可能的。所以 $d \\ge 4$。\n  - **证明 $d  5$**：我们必须证明任何大小为 $m=5$ 的集合都不能被打破。给定任意 5 个点的集合 $S$，找到具有最小和最大 $x$ 和 $y$ 坐标的点。这些点定义了整个集合 $S$ 的一个最小边界框。在 $S$ 中必须至少有一个点不在这个“外边缘”上（即，它在 $S$ 的点中既没有最小/最大 $x$ 坐标，也没有最小/最大 $y$ 坐标）。让我们称这样的点为“内部”点。考虑一种标签分配，将所有非内部（“外部”）点分配为 1，将所有内部点分配为 0。为了让一个假设（一个轴对齐的矩形）将外部点分类为正，它必须包含所有这些点。任何这样的矩形因此必须包含这些外部点的最小边界框，这与整个集合 $S$ 的最小边界框相同。根据定义，这个框也包含所有内部点。因此，该假设会将内部点标记为 1，这与期望的标签 0 相矛盾。\n  因此，没有大小为 5 的集合可以被打破。所以，$d  5$。\n  - **结论**：从 $d \\ge 4$ 和 $d  5$ 可得，VC 维度为 $d=4$。\n\n- **断点 ($k$)**：断点是不能被打破的集合的最小大小 $m$。我们证明了这是 $m=5$。因此，断点为 $k=5$。\n\n**经验验证逻辑**\n如果存在一个轴对齐的矩形，它包含所有正例点 $X_+$ 而不包含任何负例点 $X_-$，那么集合 $X$ 的一种标签分配 $y$ 就是可实现的。最小的此类矩形是 $X_+$ 的最小轴对齐边界框，我们称之为 $R_{min}$。当且仅当 $X_-$ 中没有点包含在 $R_{min}$ 中时，该标签分配是可实现的。（如果 $X_+$ 为空，则总是可实现的）。\n\n### 理论结果总结\n| ID | 类别名称                | VC 维度 ($d$) | 断点 ($k$) |\n|----|---------------------------|--------------------|------------------|\n| 0  | $\\mathbb{R}$ 上的阈值      | $1$                | $2$              |\n| 1  | $\\mathbb{R}$ 上的区间       | $2$                | $3$              |\n| 2  | $\\mathbb{R}^2$ 上的线性分隔器 | $3$                | $4$              |\n| 3  | $\\mathbb{R}^2$ 上轴对齐的矩形 | $4$                | $5$              |\n\n经验验证现在将基于这些结果和为每个类别概述的逻辑进行。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives theoretical breakpoints and empirically verifies shattering for four hypothesis classes.\n    \"\"\"\n\n    # ------------------ Shattering Check Implementations ------------------ #\n\n    def check_threshold_shattering(X):\n        \"\"\"Checks if a 1D dataset X is shattered by H_thr.\"\"\"\n        m = len(X)\n        for i in range(2**m):\n            labels = [(i >> j)  1 for j in range(m)]\n            pos_points = [X[j] for j in range(m) if labels[j] == 1]\n            neg_points = [X[j] for j in range(m) if labels[j] == 0]\n\n            realizable = False\n            if not pos_points or not neg_points:\n                realizable = True\n            else:\n                if np.max(neg_points)  np.min(pos_points):\n                    realizable = True\n            \n            if not realizable:\n                return False\n        return True\n\n    def check_interval_shattering(X):\n        \"\"\"Checks if a 1D dataset X is shattered by H_int.\"\"\"\n        m = len(X)\n        X_sorted_indices = np.argsort(X.flatten())\n        \n        for i in range(2**m):\n            labels = np.array([(i >> j)  1 for j in range(m)])\n            labels_sorted = labels[X_sorted_indices]\n            \n            pos_indices = [j for j in range(m) if labels_sorted[j] == 1]\n            \n            realizable = False\n            if not pos_indices:\n                realizable = True\n            else:\n                # Find the first and last positive points in the sorted array\n                first_pos_idx = pos_indices[0]\n                last_pos_idx = pos_indices[-1]\n                # Check if all points between them are also positive\n                is_contiguous = all(labels_sorted[j] == 1 for j in range(first_pos_idx, last_pos_idx + 1))\n                if is_contiguous:\n                    realizable = True\n\n            if not realizable:\n                return False\n        return True\n\n    def check_linear_separator_shattering(X):\n        \"\"\"Checks if a 2D dataset X is shattered by H_lin.\"\"\"\n        m = X.shape[0]\n\n        def orientation(p, q, r):\n            # Helper for segment intersection and point-in-triangle\n            val = (q[1] - p[1]) * (r[0] - q[0]) - (q[0] - p[0]) * (r[1] - q[1])\n            if np.isclose(val, 0): return 0  # Collinear\n            return 1 if val > 0 else 2  # Clockwise or Counter-clockwise\n\n        def segments_intersect(p1, q1, p2, q2):\n            # Check if line segment p1q1 and p2q2 intersect.\n            o1 = orientation(p1, q1, p2)\n            o2 = orientation(p1, q1, q2)\n            o3 = orientation(p2, q2, p1)\n            o4 = orientation(p2, q2, q1)\n            if o1 != o2 and o3 != o4:\n                return True\n            # Collinear cases are not needed for the given problem sets as they are separable.\n            return False\n\n        def point_in_triangle(p, a, b, c):\n            # Check if point p is inside triangle abc using barycentric coordinates\n            # or orientation checks.\n            d1 = orientation(p, a, b)\n            d2 = orientation(p, b, c)\n            d3 = orientation(p, c, a)\n            has_neg = (d1 == 2) or (d2 == 2) or (d3 == 2)\n            has_pos = (d1 == 1) or (d2 == 1) or (d3 == 1)\n            return not (has_neg and has_pos) # True if all orientations are the same (or collinear)\n\n        for i in range(2**m):\n            labels = np.array([(i >> j)  1 for j in range(m)])\n            pos_points = X[labels == 1]\n            neg_points = X[labels == 0]\n\n            realizable = False\n            if pos_points.shape[0] == 0 or neg_points.shape[0] == 0:\n                realizable = True\n            elif m == 3: # 3 non-collinear points are always separable\n                realizable = True\n            elif m == 4:\n                # Based on Radon's theorem, we only need to check non-separability.\n                # Case 1: 2 vs 2 points (potential intersection of diagonals)\n                if pos_points.shape[0] == 2:\n                    if not segments_intersect(pos_points[0], pos_points[1], neg_points[0], neg_points[1]):\n                        realizable = True\n                # Case 2: 1 vs 3 points (point in triangle)\n                elif pos_points.shape[0] == 1:\n                    if not point_in_triangle(pos_points[0], neg_points[0], neg_points[1], neg_points[2]):\n                        realizable = True\n                elif neg_points.shape[0] == 1:\n                    if not point_in_triangle(neg_points[0], pos_points[0], pos_points[1], pos_points[2]):\n                        realizable = True\n\n            if not realizable:\n                return False\n        return True\n\n    def check_rectangle_shattering(X):\n        \"\"\"Checks if a 2D dataset X is shattered by H_rect.\"\"\"\n        m = X.shape[0]\n        for i in range(2**m):\n            labels = np.array([(i >> j)  1 for j in range(m)])\n            pos_points = X[labels == 1]\n            neg_points = X[labels == 0]\n            \n            realizable = False\n            if pos_points.shape[0] == 0:\n                realizable = True\n            else:\n                # Minimal bounding box of positive points\n                min_x, min_y = np.min(pos_points, axis=0)\n                max_x, max_y = np.max(pos_points, axis=0)\n                \n                # Check if any negative point is inside this box\n                if neg_points.shape[0] > 0:\n                    is_inside = np.any(\n                        (neg_points[:, 0] >= min_x)  (neg_points[:, 0] = max_x) \n                        (neg_points[:, 1] >= min_y)  (neg_points[:, 1] = max_y)\n                    )\n                    if not is_inside:\n                        realizable = True\n                else:\n                    realizable = True\n\n            if not realizable:\n                return False\n        return True\n\n    # ------------------ Test Suite and Main Logic ------------------ #\n    \n    test_suite = [\n        {\n            \"id\": 0, \"name\": \"Thresholds\", \"k_theory\": 2,\n            \"X_d\": np.array([0]), \n            \"X_k\": np.array([0, 1]),\n            \"checker\": check_threshold_shattering\n        },\n        {\n            \"id\": 1, \"name\": \"Intervals\", \"k_theory\": 3,\n            \"X_d\": np.array([0, 1]),\n            \"X_k\": np.array([0, 1, 2]),\n            \"checker\": check_interval_shattering\n        },\n        {\n            \"id\": 2, \"name\": \"Linear Separators\", \"k_theory\": 4,\n            \"X_d\": np.array([[0,0], [1,0], [0,1]]),\n            \"X_k\": np.array([[0,0], [1,0], [1,1], [0,1]]),\n            \"checker\": check_linear_separator_shattering\n        },\n        {\n            \"id\": 3, \"name\": \"Axis-Aligned Rectangles\", \"k_theory\": 5,\n            \"X_d\": np.array([[1,0], [-1,0], [0,1], [0,-1]]),\n            \"X_k\": np.array([[1,0], [-1,0], [0,1], [0,-1], [0.5, 0.5]]),\n            \"checker\": check_rectangle_shattering\n        },\n    ]\n\n    results = []\n    for case in test_suite:\n        k = case[\"k_theory\"]\n        checker = case[\"checker\"]\n        X_d, X_k = case[\"X_d\"], case[\"X_k\"]\n        m_d = X_d.shape[0] if X_d.ndim > 1 else X_d.size\n        m_k = X_k.shape[0] if X_k.ndim > 1 else X_k.size\n\n        # Empirically test shattering for m=d and m=k\n        S_d = checker(X_d)\n        S_k = checker(X_k)\n\n        # Determine empirical breakpoint k_hat\n        if not S_d:\n            k_hat = m_d\n        elif not S_k:\n            k_hat = m_k\n        else: # Both shattered (or other unexpected result)\n            k_hat = m_k \n\n        # Compare theoretical and empirical breakpoints\n        A = (k == k_hat)\n\n        results.append(f\"[{case['id']},{k},{S_d},{S_k},{k_hat},{A}]\")\n    \n    # Format and print the final output\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了如何定义复杂度后，下一步便是应用它来获得泛化保证，即确保模型在未见数据上也能表现良好。学习理论提供了多种工具来构建这样的保证，包括经典的 VC 界、Rademacher 复杂度假定和 PAC-Bayes 界，它们在严谨性和实用性上各有千秋。本练习是一个计算实验，你将通过编程实现并比较这些泛化界在同一数据集上的数值表现，从而亲身体会不同理论工具的“紧致度”及其在实践中的意义 。",
            "id": "3161842",
            "problem": "您必须编写一个完整、可运行的程序，该程序实现一个实验，在相同的数据生成过程和相同的假设类上比较三种泛化界。主题是统计学习，具体是可能近似正确学习。程序应针对每个指定的训练样本量，计算哪个界在数值上最紧。最紧的界定义为：将数据集上测得的经验量代入界的表达式后，得到的真实风险的最小数值上界。\n\n您必须使用的基础是一组核心定义，包括经验风险、真实风险、经验风险最小化 (ERM)、Vapnik–Chervonenkis 维度 (VC)、Rademacher 复杂度和可能近似正确-贝叶斯 (PAC-Bayes)，以及用于一致收敛和对称化的标准集中原理。在设计方法时，不要依赖任何捷径公式；从此处陈述的、适用于二元分类和有限假设类的标准定义和事实推导出每个界。\n\n实验设计：\n- 假设类：考虑实线上的阈值分类器，包含两种极性。令 $\\mathcal{H}$ 为在某个切点 $(k)$（其中 $k \\in \\{0,1,\\dots,m\\}$，$m$ 是当前测试用例的样本量）处分割 $\\mathbb{R}$ 的假设所组成的类。对于任何阈值索引 $k$，定义两个假设：$h_{k}^{+}$ 对前 $k$ 个排序后的点预测为 $-1$，对其余的 $m-k$ 个点预测为 $+1$；$h_{k}^{-}$ 对前 $k$ 个点预测为 $+1$，对其余的点预测为 $-1$。这样就得到一个大小为 $|\\mathcal{H}| = 2(m+1)$ 的有限假设类。实线上阈值分类器的 Vapnik–Chervonenkis 维度 (VC) 为 $d = 1$。\n\n- 数据分布和数据集构建：对于给定的样本量 $m$，构建一个确定性的数据集 $S_{m} = \\{(x_{i}, y_{i})\\}_{i=1}^{m}$，其中对于每个 $i \\in \\{1,\\dots,m\\}$，$x_{i} = \\frac{i}{m+1}$，标签 $y_{i} \\in \\{-1,+1\\}$ 由 $y_{i} = \\text{sign}(x_{i} - \\theta)$ 定义，其中 $\\theta = 0.6$。为避免平凡可分性并使某些样本量下的经验误差不为零，当 $i$ 可被 $4$ 整除时，通过翻转 $y_{i}$ 来确定性地引入标签噪声（即，如果 $4$ 整除 $i$，则用 $-y_{i}$ 替换 $y_{i}$）。这种构造确保了对于给定的 $m$，所有界都在相同的数据集上对相同的假设类 $\\mathcal{H}$ 进行评估。\n\n- 经验风险最小化 (ERM)：对于数据集 $S_{m}$，在 $0$-$1$ 损失下，找到使经验风险 $L_{S_{m}}(h)$ 最小化的假设 $h^{*} \\in \\mathcal{H}$。经验风险 $L_{S_{m}}(h)$ 定义为在 $S_{m}$ 上的平均分类误差。\n\n- 待比较的界：\n  1. 经典的 Vapnik–Chervonenkis (VC) 一致收敛界：使用 VC 维度 $d$，根据经验风险 $L_{S_{m}}(h)$、样本量 $m$ 和置信度参数 $\\delta \\in (0,1)$，推导真实风险 $L(h)$ 的一个泛化上界。\n  2. Rademacher 复杂度界：使用取值于 $\\{-1,+1\\}$ 的有限类 $\\mathcal{H}$，根据 $L_{S_{m}}(h)$、样本量 $m$、置信度参数 $\\delta$ 以及基于其基数 $|\\mathcal{H}|$ 的经验 Rademacher 复杂度上界（对于值域有界于 $[-1,1]$ 的函数），推导真实风险 $L(h)$ 的一个界。\n  3. 可能近似正确-贝叶斯 (PAC-Bayes) 界：使用 $\\mathcal{H}$ 上的均匀先验 $P$ 和一个集中在 ERM 假设 $h^{*}$ 上的点质量分布后验 $Q$，根据 $L_{S_m}(Q)$、Kullback–Leibler (KL) 散度 $\\mathrm{KL}(Q \\| P)$、样本量 $m$ 和置信度参数 $\\delta$，推导 Gibbs 分类器真实风险 $L(Q)$ 的一个上界。\n\n- 置信度参数：对所有测试用例使用 $\\delta = 0.05$。\n\n每个测试用例 $(m)$ 所需的计算步骤：\n1. 按照规定构建 $S_{m}$，并将 $\\mathcal{H}$ 作为在 $m+1$ 个切点位置上所有两种极性阈值分类器的集合。\n2. 计算在 $\\mathcal{H}$ 上使 $L_{S_{m}}(h)$ 最小化的 $h^{*}$，并记录 $L_{S_{m}}(h^{*})$。\n3. 使用您推导出的表达式，以及 $d = 1$、$|\\mathcal{H}| = 2(m+1)$ 和指定的 $\\delta$，计算三个界对真实风险的数值上界。\n4. 通过从三个界中选择最小的数值上界来确定哪个界最紧。\n\n测试套件：\n- 使用以下样本量：$m \\in \\{10, 50, 200, 1\\}$。\n- 对集合中的每个 $m$，执行上述步骤。\n\n最终输出规范：\n- 您的程序应生成单行输出，包含一个整数列表，每个测试用例对应一个整数，顺序与测试套件相同。每个整数指示哪个界最紧：使用 $0$ 表示经典的 Vapnik–Chervonenkis 界，$1$ 表示 Rademacher 复杂度界，$2$ 表示可能近似正确-贝叶斯界。\n- 该行必须是一个用方括号括起来的逗号分隔列表，例如 $[0,1,2,1]$。\n\n所有角度都是抽象的，不涉及物理单位。所有数值输出必须是纯十进制数。任何地方都不得使用百分比；任何分数都必须表示为小数。",
            "solution": "用户提供的问题已经过验证，被确定为统计学习理论领域中一个适定、有科学依据且客观的问题。它要求对三种标准的泛化界进行数值比较。我们现在将进行求解，这包括推导每个界的数学表达式，然后通过一个计算实验来评估它们的数值。\n\n令 $\\mathcal{D}$ 为一个未知的底层数据分布，其定义在 $\\mathcal{X} \\times \\mathcal{Y}$ 上，其中 $\\mathcal{X} = \\mathbb{R}$ 且 $\\mathcal{Y} = \\{-1, +1\\}$。一个假设是一个函数 $h: \\mathcal{X} \\to \\mathcal{Y}$。假设 $h$ 的性能由 $0-1$ 损失来衡量，即 $\\ell(h(x), y) = \\mathbf{1}_{h(x) \\neq y}$，其中 $\\mathbf{1}$ 是指示函数。$h$ 的真实风险（或泛化误差）是其在分布 $\\mathcal{D}$ 上的期望损失，由 $L(h) = \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[\\ell(h(x), y)]$ 给出。给定一个从 $\\mathcal{D}$ 中独立同分布抽取的、大小为 $m$ 的训练样本 $S_m = \\{(x_i, y_i)\\}_{i=1}^m$，经验风险是样本上的平均损失：$L_{S_m}(h) = \\frac{1}{m} \\sum_{i=1}^m \\ell(h(x_i), y_i)$。\n\n泛化界的目标是根据假设的经验风险 $L_{S_m}(h)$、样本量 $m$、假设类 $\\mathcal{H}$ 的性质以及一个置信度参数 $\\delta \\in (0,1)$，为该假设的真实风险 $L(h)$ 提供一个高概率上界。问题要求我们找到经验风险最小化 (ERM) 假设 $h^{*} = \\arg\\min_{h \\in \\mathcal{H}} L_{S_m}(h)$，然后比较其真实风险 $L(h^{*})$ 的三个不同上界。\n\n假设类 $\\mathcal{H}$ 是一维阈值分类器的集合，包含两种极性。对于大小为 $m$ 的数据集，阈值由 $m+1$ 个可能的切点定义。对于每个切点 $k \\in \\{0, 1, \\dots, m\\}$，存在两个假设 $h_k^+$ 和 $h_k^-$。这构成了一个大小为 $|\\mathcal{H}| = 2(m+1)$ 的有限假设类。该类的 Vapnik-Chervonenkis (VC) 维度为 $d=1$。置信度参数为 $\\delta = 0.05$。\n\n三个界的推导如下。\n\n**1. 经典的 Vapnik-Chervonenkis (VC) 界**\n\nVapnik 和 Chervonenkis 的理论基于假设类的 VC 维度（衡量其复杂度的指标）提供了泛化界。对于任何具有有限 VC 维度 $d$ 的假设类 $\\mathcal{H}$，一个标准的一致收敛结果表明，以至少 $1-\\delta$ 的概率（针对样本 $S_m$ 的选择），以下不等式对所有 $h \\in \\mathcal{H}$ 均成立：\n$$\nL(h) \\leq L_{S_m}(h) + \\sqrt{\\frac{8}{m} \\left( d \\log\\frac{2em}{d} + \\log\\frac{4}{\\delta} \\right)}\n$$\n该界是使用对称化论证（用第二个“影子样本”上的经验风险替换真实风险）和应用于增长函数 $\\tau_{\\mathcal{H}}(m)$ 的集中不等式（如 Hoeffding 或 Bernstein 不等式）推导出来的。增长函数 $\\tau_{\\mathcal{H}}(m)$ 可通过 Sauer-Shelah 引理使用 VC 维度进行限定：$\\tau_{\\mathcal{H}}(m) \\leq (\\frac{em}{d})^d$。\n\n由于此不等式对所有 $h \\in \\mathcal{H}$ 均成立，因此它也必然对 ERM 假设 $h^*$ 成立。给定 $d=1$。代入 $h=h^*$ 和 $d=1$，我们得到第一个界：\n$$\nL(h^*) \\leq L_{S_m}(h^*) + \\sqrt{\\frac{8}{m} \\left( \\log(2em) + \\log\\frac{4}{\\delta} \\right)}\n$$\n这就是 VC 界的表达式，我们将其记为界 0。\n\n**2. Rademacher 复杂度界**\n\nRademacher 复杂度提供了一种更依赖于数据的复杂度度量。对于一个样本 $S_m$，函数类 $F$ 的经验 Rademacher 复杂度为 $\\hat{\\mathfrak{R}}_S(F) = \\mathbb{E}_{\\sigma} [\\sup_{f \\in F} \\frac{1}{m} \\sum_{i=1}^m \\sigma_i f(x_i)]$，其中 $\\sigma_i$ 是独立的 Rademacher 随机变量（以 $1/2$ 的概率取值 $\\pm 1$）。（期望）Rademacher 复杂度是 $\\mathfrak{R}_m(F) = \\mathbb{E}_{S \\sim \\mathcal{D}^m}[\\hat{\\mathfrak{R}}_S(F)]$。\n\n对于使用 $0-1$ 损失的二元分类，一个标准定理通过假设类 $\\mathcal{H}$（其中假设映射到 $\\{-1, +1\\}$）的 Rademacher 复杂度将真实风险与经验风险联系起来。以至少 $1-\\delta$ 的概率，对所有 $h \\in \\mathcal{H}$：\n$$\nL(h) \\leq L_{S_m}(h) + 2\\mathfrak{R}_m(\\mathcal{H}) + \\sqrt{\\frac{\\log(1/\\delta)}{2m}}\n$$\n问题指定我们应使用基于 $\\mathcal{H}$ 的基数的 Rademacher 复杂度上界。对于有限类 $\\mathcal{H}$，Massart 引理提供了经验 Rademacher 复杂度的界。对于任何固定的样本 $S_m$，令 $A = \\{ (h(x_1), \\dots, h(x_m)) \\in \\mathbb{R}^m : h \\in \\mathcal{H} \\}$。由于 $h(x_i) \\in \\{-1, +1\\}$，任何 $A$ 中向量的欧几里得范数是 $\\|v\\|_2 = \\sqrt{m}$。Massart 引理给出：\n$$\n\\hat{\\mathfrak{R}}_S(\\mathcal{H}) \\leq \\frac{\\max_{v \\in A} \\|v\\|_2 \\sqrt{2\\log|A|}}{m} = \\frac{\\sqrt{m}\\sqrt{2\\log|A|}}{m} = \\sqrt{\\frac{2\\log|A|}{m}}\n$$\n由于 $|A| \\leq |\\mathcal{H}|$，我们有 $\\hat{\\mathfrak{R}}_S(\\mathcal{H}) \\leq \\sqrt{\\frac{2\\log|\\mathcal{H}|}{m}}$。由于这个上界不依赖于样本 $S$，它也为期望 Rademacher 复杂度提供了上界：$\\mathfrak{R}_m(\\mathcal{H}) \\leq \\sqrt{\\frac{2\\log|\\mathcal{H}|}{m}}$。\n\n将此结果代入主要的 Rademacher 不等式，并将其应用于 $h^*$，其中 $|\\mathcal{H}|=2(m+1)$，我们得到第二个界：\n$$\nL(h^*) \\leq L_{S_m}(h^*) + 2\\sqrt{\\frac{2\\log(2(m+1))}{m}} + \\sqrt{\\frac{\\log(1/\\delta)}{2m}}\n$$\n这就是 Rademacher 界的表达式，记为界 1。\n\n**3. 可能近似正确-贝叶斯 (PAC-Bayes) 界**\n\nPAC-Bayes 框架提供了关于假设类 $\\mathcal{H}$ 上“后验”分布 $Q$ 的风险的界。PAC-Bayes 定理的一种常见形式是，对于 $\\mathcal{H}$ 上的任何先验分布 $P$，以至少 $1-\\delta$ 的概率（针对样本 $S_m$），以下不等式对 $\\mathcal{H}$ 上的所有后验分布 $Q$ 均成立：\n$$\nL(Q) \\leq L_{S_m}(Q) + \\sqrt{\\frac{\\mathrm{KL}(Q \\| P) + \\log\\frac{2m}{\\delta}}{2m}}\n$$\n这里，$L(Q) = \\mathbb{E}_{h \\sim Q}[L(h)]$ 和 $L_{S_m}(Q) = \\mathbb{E}_{h \\sim Q}[L_{S_m}(h)]$ 分别是由 $Q$ 定义的 Gibbs 分类器的真实风险和经验风险。$\\mathrm{KL}(Q \\| P)$ 是 $Q$ 和 $P$ 之间的 Kullback-Leibler 散度。\n\n问题指定了一个均匀先验 $P$，因此对于所有 $h \\in \\mathcal{H}$，$P(h) = 1/|\\mathcal{H}|$。后验 $Q$ 是一个集中在 ERM 假设 $h^*$ 上的点质量分布，即 $Q(h^*)=1$ 且对于 $h \\ne h^*$ 有 $Q(h)=0$。对于这种 $Q$ 的选择：\n- Gibbs 分类器的风险是 $L(Q) = \\sum_h Q(h)L(h) = L(h^*)$。\n- Gibbs 分类器的经验风险是 $L_{S_m}(Q) = \\sum_h Q(h)L_{S_m}(h) = L_{S_m}(h^*)$。\n- KL 散度是 $\\mathrm{KL}(Q \\| P) = \\sum_h Q(h) \\log\\frac{Q(h)}{P(h)} = 1 \\cdot \\log\\frac{1}{1/|\\mathcal{H}|} = \\log|\\mathcal{H}|$。\n\n将这些结果代入 PAC-Bayes 不等式，并使用 $|\\mathcal{H}|=2(m+1)$，我们得到第三个界：\n$$\nL(h^*) \\leq L_{S_m}(h^*) + \\sqrt{\\frac{\\log(2(m+1)) + \\log\\frac{2m}{\\delta}}{2m}}\n$$\n这就是 PAC-Bayes 界的表达式，记为界 2。\n\n计算任务是，对于每个给定的样本量 $m$，使用经验确定的 $L_{S_m}(h^*)$ 来计算这三个上界的数值，并确定哪个界产生的值最小。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements an experiment to compare three generalization bounds (VC, Rademacher, PAC-Bayes)\n    for threshold classifiers on a deterministically generated dataset.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [10, 50, 200, 1]\n\n    results = []\n    for m in test_cases:\n        # Define constants for the current test case\n        delta = 0.05\n        d = 1  # VC dimension of 1D thresholds\n        \n        # Handle m=1 case where some log calculations might be problematic\n        if m == 0:\n            # Although not in the test suite, this avoids division by zero.\n            results.append(2) # Default to PAC-Bayes for trivial cases\n            continue\n        if m == 1 and d == 1:\n            # np.log(2*np.e*m/d) with m=1 becomes log(2e) which is fine.\n            # No division by zero for m=1\n            pass\n\n        # Step 1: Build the dataset S_m as specified.\n        # x_i = i/(m+1) for i in {1, ..., m} using 1-based indexing for definition.\n        x = np.array([(i_one_based) / (m + 1) for i_one_based in range(1, m + 1)])\n        theta = 0.6\n        y = np.sign(x - theta)\n        \n        # Introduce label noise deterministically.\n        # y_i is flipped if i is divisible by 4 (1-based index).\n        for i_one_based in range(1, m + 1):\n            if i_one_based % 4 == 0:\n                y[i_one_based - 1] *= -1\n\n        # Step 2: Find the ERM hypothesis h* and its empirical risk L_S_m(h*).\n        min_risk = float('inf')\n        \n        # The hypothesis class H consists of 2*(m+1) hypotheses.\n        # A hypothesis is defined by a cut k in {0, ..., m} and a polarity.\n        # k=0 means the cut is before the first point, k=m means after the last.\n        for k in range(m + 1):\n            # Hypothesis h_k^+: predicts -1 up to k, then +1.\n            pred_plus = np.array([-1.0] * k + [1.0] * (m - k))\n            errors_plus = np.sum(pred_plus != y)\n            risk_plus = errors_plus / m\n            if risk_plus  min_risk:\n                min_risk = risk_plus\n\n            # Hypothesis h_k^-: predicts +1 up to k, then -1.\n            pred_minus = np.array([1.0] * k + [-1.0] * (m - k))\n            errors_minus = np.sum(pred_minus != y)\n            risk_minus = errors_minus / m\n            if risk_minus  min_risk:\n                min_risk = risk_minus\n\n        R_emp = min_risk\n        \n        # Step 3: Compute the three bounds' numerical upper values.\n        H_card = 2 * (m + 1)\n        \n        # Bound 0: Classical Vapnik-Chervonenkis (VC)\n        # L(h) = L_emp(h) + sqrt( (8/m) * (d*log(2*e*m/d) + log(4/delta)) )\n        bound_vc_complexity = np.sqrt((8 / m) * (d * np.log(2 * np.e * m / d) + np.log(4 / delta)))\n        bound_vc = R_emp + bound_vc_complexity\n\n        # Bound 1: Rademacher complexity bound\n        # L(h) = L_emp(h) + 2*R_m(H) + sqrt(log(1/delta)/(2*m))\n        # with R_m(H) = sqrt(2*log(|H|)/m)\n        log_H_card = np.log(H_card)\n        rademacher_upper_bound = np.sqrt(2 * log_H_card / m)\n        bound_rad_complexity = 2 * rademacher_upper_bound + np.sqrt(np.log(1 / delta) / (2 * m))\n        bound_rad = R_emp + bound_rad_complexity\n        \n        # Bound 2: Probably Approximately Correct-Bayesian (PAC-Bayes)\n        # L(Q) = L_emp(Q) + sqrt((KL(Q||P) + log(2*m/delta)) / (2*m))\n        # For our specific P and Q, L(Q)=L(h*), L_emp(Q)=L_emp(h*), KL(Q||P)=log(|H|)\n        bound_pac_complexity = np.sqrt((log_H_card + np.log(2 * m / delta)) / (2 * m))\n        bound_pac = R_emp + bound_pac_complexity\n\n        # Step 4: Determine which bound is tightest (smallest numerical value).\n        bounds = [bound_vc, bound_rad, bound_pac]\n        tightest_index = np.argmin(bounds)\n        results.append(tightest_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "真实的机器学习系统通常是通过组合简单的构建模块来解决复杂问题的，一个典型的例子就是使用多个二元分类器来构建一个“一对多”（One-vs-Rest）多类别分类系统。一个自然而深刻的问题是：我们能否根据基础模块的复杂度（如 VC 维）来推断整个组合系统的复杂度？本练习要求你通过严谨的数学推导，为一个由线性分类器构成的 OvR 系统精确地计算其 VC 维，这项分析技能对于理解现代机器学习模型如何通过组合来扩展其表达能力至关重要 。",
            "id": "3192466",
            "problem": "考虑一个有 $K$ 个类别的多分类问题，该问题使用 $\\mathbb{R}^{d}$ 中的仿射线性分隔器，并采用“一对剩余”(One-vs-Rest, OvR) 归约方法。设二元假设类为\n$$\\mathcal{H} \\;=\\; \\left\\{\\, h_{\\mathbf{w},b}(x) \\;=\\; \\mathrm{sign}(\\mathbf{w}^{\\top} x + b) \\;:\\; \\mathbf{w}\\in\\mathbb{R}^{d},\\; b\\in\\mathbb{R} \\,\\right\\},$$\n其中 $x\\in\\mathbb{R}^{d}$ 且 $\\mathrm{sign}(\\cdot)\\in\\{-1,+1\\}$。OvR 方案构建了 $K$ 个二元问题，每个类别索引 $k\\in\\{1,\\dots,K\\}$ 对应一个，并从 $\\mathcal{H}$ 中学习 $K$ 个假设，记为 $\\{h_{k}\\}_{k=1}^{K}$。\n\n在扩展域 $\\mathcal{X}\\times\\{1,\\dots,K\\}$ 上定义诱导积类为\n$$\\mathcal{G} \\;=\\; \\left\\{\\, g(x,k) \\;=\\; h_{k}(x) \\;:\\; (h_{1},\\dots,h_{K}) \\in \\mathcal{H}^{K} \\,\\right\\},$$\n因此每个 $g\\in\\mathcal{G}$ 对输入 $x$ 输出与类别 $k$ 相关联的二元决策。\n\n从 Vapnik–Chervonenkis (VC) 维和组合增长函数的基本定义出发，推导 $\\mathcal{G}$ 的 VC 维关于 $d$ 和 $K$ 的精确表达式。然后，使用二元分类的标准一致收敛理论，解释为了对所有 $K$ 个诱导二元问题同时获得泛化保证，样本复杂度如何随推导出的 $\\mathcal{G}$ 的 VC 维以及准确度与置信度参数 $\\varepsilon$ 和 $\\delta$ 而变化。你的最终答案必须是 $\\mathcal{G}$ 的 VC 维作为 $d$ 和 $K$ 的函数的单个闭式解析表达式。",
            "solution": "该问题要求计算为“一对剩余”(OvR)多分类方案构建的假设类 $\\mathcal{G}$ 的 Vapnik-Chervonenkis (VC) 维，并将其与样本复杂度联系起来。\n\n### 第 1 步：问题验证\n\n**1.1. 提取已知条件**\n- **多分类设置**：$K$ 个类别，类别索引 $k \\in \\{1,\\dots,K\\}$。\n- **输入空间**：$x \\in \\mathbb{R}^{d}$。\n- **基础二元假设类**：$\\mathbb{R}^{d}$ 中的仿射线性分隔器，\n  $\\mathcal{H} = \\{ h_{\\mathbf{w},b}(x) = \\mathrm{sign}(\\mathbf{w}^{\\top} x + b) : \\mathbf{w}\\in\\mathbb{R}^{d}, b\\in\\mathbb{R} \\}$。输出在 $\\{-1,+1\\}$ 中。\n- **OvR 方案**：学习 $K$ 个假设 $\\{h_{k}\\}_{k=1}^{K}$，其中每个 $h_k \\in \\mathcal{H}$。\n- **诱导积类**：在扩展域 $\\mathcal{X}\\times\\{1,\\dots,K\\}$ 上定义，\n  $\\mathcal{G} = \\{ g(x,k) = h_{k}(x) : (h_{1},\\dots,h_{K}) \\in \\mathcal{H}^{K} \\}$。\n\n**1.2. 使用提取的已知条件进行验证**\n- **科学依据**：该问题位于统计学习理论的标准数学框架内。VC 维、仿射线性分隔器和 OvR 归约等概念是该领域公认的基础概念。\n- **适定性**：该问题定义了确定 $\\mathcal{G}$ 的 VC 维所需的所有必要组件——基础类 $\\mathcal{H}$、类别数 $K$ 以及诱导类 $\\mathcal{G}$ 的构造方法。问题具体，且有唯一的可推导答案。\n- **客观性**：该问题使用精确、形式化的数学语言陈述，没有任何主观性或模糊性。\n\n**1.3. 结论与行动**\n该问题在科学上是合理的、适定的、客观的且自洽的。这是一个统计学习理论中的有效问题。我将继续进行解答。\n\n### 第 2 步：VC 维的推导\n\n一个假设类的 VC 维是该类能打散的点的最大数量。我们需要找到能被 $\\mathcal{G}$ 打散的最大集合 $S = \\{(x_1, k_1), \\dots, (x_m, k_m)\\}$ 的大小 $m$。如果对于任意可能的标签 $(y_1, \\dots, y_m) \\in \\{-1, +1\\}^m$，都存在一个假设 $g \\in \\mathcal{G}$ 使得对所有 $i \\in \\{1, \\dots, m\\}$ 都有 $g(x_i, k_i) = y_i$，那么集合 $S$ 就被 $\\mathcal{G}$ 打散。\n\n一个假设 $g \\in \\mathcal{G}$ 由 $K$ 个独立假设的元组 $(h_1, \\dots, h_K)$ 指定，其中每个 $h_j \\in \\mathcal{H}$。$g$ 的定义属性是 $g(x_i, k_i) = h_{k_i}(x_i)$。\n\n首先，我们确定基础类 $\\mathcal{H}$ 的 VC 维。类 $\\mathcal{H}$ 是 $\\mathbb{R}^d$ 中仿射超平面的集合。一个仿射超平面由 $\\{x \\in \\mathbb{R}^d : \\mathbf{w}^\\top x + b = 0\\}$ 定义。通过将 $x$ 映射到 $(x, 1)$ 并使用权重向量 $(\\mathbf{w}, b)$，这等价于 $\\mathbb{R}^{d+1}$ 中的齐次超平面。$p$ 维空间中齐次超平面的 VC 维是 $p$。在这里，$p = d+1$。因此，$\\mathcal{H}$ 的 VC 维是一个已知结果：\n$$\n\\mathrm{VCdim}(\\mathcal{H}) = d+1\n$$\n\n现在，我们将通过建立一个下界和一个上界来确定 $\\mathrm{VCdim}(\\mathcal{G})$。\n\n**$\\mathrm{VCdim}(\\mathcal{G})$ 的下界**\n\n为了证明 $\\mathrm{VCdim}(\\mathcal{G}) \\geq K(d+1)$，我们必须构造一个大小为 $K(d+1)$ 且能被 $\\mathcal{G}$ 打散的集合。\n令 $D_{\\mathcal{H}} = \\mathrm{VCdim}(\\mathcal{H}) = d+1$。对于每个类别索引 $j \\in \\{1,\\dots,K\\}$，我们可以在 $\\mathbb{R}^d$ 中找到一个由 $D_{\\mathcal{H}}$ 个点组成的集合，我们称之为 $S'_j = \\{x_{j,1}, \\dots, x_{j, D_{\\mathcal{H}}}\\}$，这个集合可以被 $\\mathcal{H}$ 打散。我们可以选择这 $K$ 个点集，使它们彼此不相交。\n\n现在，在扩展域 $\\mathcal{X} \\times \\{1,\\dots,K\\}$ 中构造一个点集 $S$ 如下：\n$$\nS = \\bigcup_{j=1}^{K} \\{ (x, j) \\mid x \\in S'_j \\}\n$$\n这个集合的大小是 $|S| = \\sum_{j=1}^{K} |S'_j| = K \\cdot D_{\\mathcal{H}} = K(d+1)$。\n\n我们必须证明 $S$ 被 $\\mathcal{G}$ 打散。考虑 $S$ 中点的任意标签，记为 $\\{y_{j,i} \\in \\{-1,+1\\}\\}$，其中 $j \\in \\{1,\\dots,K\\}$ 且 $i \\in \\{1,\\dots,D_{\\mathcal{H}}\\}$。我们需要找到一个假设 $g \\in \\mathcal{G}$，它对应一个元组 $(h_1, \\dots, h_K) \\in \\mathcal{H}^K$，使得对于 $S$ 中的每个点 $(x_{j,i}, j)$，都有 $g(x_{j,i}, j) = y_{j,i}$。这个条件就是 $h_j(x_{j,i}) = y_{j,i}$。\n\n让我们独立地看对每个 $h_j$ 的要求。\n- 对于 $j=1$，我们需要找到一个 $h_1 \\in \\mathcal{H}$ 使得对所有 $i \\in \\{1,\\dots,D_{\\mathcal{H}}\\}$ 都有 $h_1(x_{1,i}) = y_{1,i}$。这是对集合 $S'_1$ 的一种标签。由于 $S'_1$ 被 $\\mathcal{H}$ 打散，这样的 $h_1$ 保证存在。\n- 对于 $j=2$，我们需要找到一个 $h_2 \\in \\mathcal{H}$ 使得对所有 $i \\in \\{1,\\dots,D_{\\mathcal{H}}\\}$ 都有 $h_2(x_{2,i}) = y_{2,i}$。由于 $S'_2$ 被 $\\mathcal{H}$ 打散，这样的 $h_2$ 保证存在。\n- ...\n- 对于 $j=K$，我们需要找到一个 $h_K \\in \\mathcal{H}$ 来实现 $S'_K$ 上的给定标签。由于 $S'_K$ 被打散，这样的 $h_K$ 存在。\n\n由于我们可以独立地从 $\\mathcal{H}$ 中选择每个 $h_j$ 来满足其对应点集 $S'_j$ 上的条件，我们可以构造元组 $(h_1, \\dots, h_K)$，它定义了一个 $g \\in \\mathcal{G}$，满足 $S$ 的全部标签。因为这对 $S$ 的任何任意标签都成立，所以集合 $S$ 被 $\\mathcal{G}$ 打散。\n因此，$\\mathrm{VCdim}(\\mathcal{G}) \\geq |S| = K(d+1)$。\n\n**$\\mathrm{VCdim}(\\mathcal{G})$ 的上界**\n\n设 $S = \\{(x_1, k_1), \\dots, (x_m, k_m)\\}$ 是 $\\mathcal{G}$ 的定义域中任意一个被 $\\mathcal{G}$ 打散的大小为 $m$ 的点集。我们根据类别索引 $k$ 对 $S$ 进行划分。对于每个 $j \\in \\{1,\\dots,K\\}$，定义 x 坐标的集合：\n$$\nS_j = \\{ x_i \\mid (x_i, k_i) \\in S \\text{ and } k_i = j \\}\n$$\n令 $m_j = |S_j|$。那么 $S$ 的总大小是 $m = \\sum_{j=1}^{K} m_j$。\n\n一个假设类 $\\mathcal{F}$ 可以在集合 $Z$ 上诱导的二分法数量由其增长函数 $\\Pi_{\\mathcal{F}}(Z)$ 表示。如果一个大小为 $m$ 的集合 $S$ 被打散，那么 $|\\Pi_{\\mathcal{G}}(S)| = 2^m$。\n\n由 $g=(h_1, \\dots, h_K) \\in \\mathcal{G}$ 在 $S$ 上诱导的二分法是标签向量 $(h_{k_1}(x_1), \\dots, h_{k_m}(x_m))$。对于一个点 $(x_i, k_i)$，其中 $k_i=j$，其标签仅取决于 $h_j$ 的选择。由于假设 $h_1, \\dots, h_K$ 是独立选择的，因此 $S$ 上所有可能二分法的集合是 $\\mathcal{H}$ 可以在每个子集 $S_j$ 上实现的二分法集合的笛卡尔积。\n因此，$\\mathcal{G}$ 在 $S$ 上的增长函数可以分解为：\n$$\n|\\Pi_{\\mathcal{G}}(S)| = \\prod_{j=1}^{K} |\\Pi_{\\mathcal{H}}(S_j)|\n$$\n如果 $S$ 被 $\\mathcal{G}$ 打散，我们必须有 $|\\Pi_{\\mathcal{G}}(S)| = 2^m = 2^{\\sum m_j} = \\prod_{j=1}^K 2^{m_j}$。\n结合这些事实得到：\n$$\n\\prod_{j=1}^{K} |\\Pi_{\\mathcal{H}}(S_j)| = \\prod_{j=1}^{K} 2^{m_j}\n$$\n根据 Sauer 引理，对于任何大小为 $m_j$ 的集合 $S_j$，我们有 $|\\Pi_{\\mathcal{H}}(S_j)| \\leq 2^{m_j}$。因此，上述等式只有在对所有 $j \\in \\{1,\\dots,K\\}$ 都有 $|\\Pi_{\\mathcal{H}}(S_j)| = 2^{m_j}$ 时才能成立。\n\n条件 $|\\Pi_{\\mathcal{H}}(S_j)| = 2^{m_j}$ 意味着集合 $S_j$ 必须被基础类 $\\mathcal{H}$ 打散。根据 VC 维的定义，任何大小超过 $\\mathrm{VCdim}(\\mathcal{H})$ 的集合都不能被打散。因此，对于每个 $j$，我们必须有：\n$$\nm_j = |S_j| \\leq \\mathrm{VCdim}(\\mathcal{H}) = d+1\n$$\n那么，被打散的集合 $S$ 的总大小受限于：\n$$\nm = \\sum_{j=1}^{K} m_j \\leq \\sum_{j=1}^{K} \\mathrm{VCdim}(\\mathcal{H}) = K(d+1)\n$$\n这表明任何被 $\\mathcal{G}$ 打散的集合的大小最多为 $K(d+1)$。根据 VC 维的定义，这意味着 $\\mathrm{VCdim}(\\mathcal{G}) \\leq K(d+1)$。\n\n**关于 VC 维的结论**\n结合下界和上界，我们证明了：\n$$\nK(d+1) \\leq \\mathrm{VCdim}(\\mathcal{G}) \\leq K(d+1)\n$$\n这就确定了 $\\mathcal{G}$ 的 VC 维的精确值。\n\n### 第 3 步：样本复杂度的缩放关系\n\n问题询问了对于一个同时的泛化保证，样本复杂度如何随推导出的 VC 维而变化。通过将 OvR 任务构建为在扩展域 $\\mathcal{X} \\times \\{1, \\dots, K\\}$ 上使用假设类 $\\mathcal{G}$ 的单个二元分类问题，我们可以直接应用 PAC 学习理论中的标准一致收敛界。\n\n对于一个 VC 维为 $d_{\\mathcal{F}}$ 的二元假设类 $\\mathcal{F}$，大小为 $m$ 的样本足以保证，以至少 $1-\\delta$ 的概率，对于所有 $h \\in \\mathcal{F}$，其真实误差 $R(h)$ 和经验误差 $\\hat{R}(h)$ 满足 $|R(h) - \\hat{R}(h)| \\leq \\varepsilon$。样本复杂度 $m$ 具有以下缩放行为（忽略 $m, \\varepsilon, d$ 中的对数因子）：\n$$\nm(\\varepsilon, \\delta) = O\\left( \\frac{d_{\\mathcal{F}} + \\log(1/\\delta)}{\\varepsilon^2} \\right)\n$$\n这个界在整个类 $\\mathcal{F}$ 上提供了一致的保证，确保无论学习到哪个假设（例如，通过经验风险最小化），其泛化误差都受到控制。\n\n在我们的情况下，学习问题是在假设类 $\\mathcal{G}$ 上定义的。一个在所有 $g \\in \\mathcal{G}$ 上一致的保证，也就是一个在 OvR 分类器 $(h_1, \\dots, h_K)$ 的所有可能元组上一致的保证。这可以解释为对所有 $K$ 个二元问题“同时”成立，即该保证适用于整个学习到的系统 $g = (h_1, \\dots, h_K)$。\n\n为了确定样本复杂度的缩放关系，我们将 $d_{\\mathcal{F}}$ 替换为 $\\mathrm{VCdim}(\\mathcal{G})$：\n$$\nm(\\varepsilon, \\delta) = O\\left( \\frac{\\mathrm{VCdim}(\\mathcal{G}) + \\log(1/\\delta)}{\\varepsilon^2} \\right) = O\\left( \\frac{K(d+1) + \\log(1/\\delta)}{\\varepsilon^2} \\right)\n$$\n因此，样本复杂度与类别数 $K$ 成线性关系，与输入维度 $d$ 成线性关系。它也表现出与准确度参数 $\\varepsilon$ 的标准缩放关系 $O(1/\\varepsilon^2)$，以及与置信度参数 $\\delta$ 的标准缩放关系 $O(\\log(1/\\delta))$。\n\n最终需要的答案是 $\\mathcal{G}$ 的 VC 维的闭式表达式。",
            "answer": "$$\\boxed{K(d+1)}$$"
        }
    ]
}