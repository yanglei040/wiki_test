{
    "hands_on_practices": [
        {
            "introduction": "仅仅计算出偏差指标的单一点估计值是不够的；我们还需要理解其统计不确定性。一个观测到的差异可能仅仅是由于抽样变异，而非系统中存在的真实偏差。这项实践  引入了影响函数（influence function）这一强大的理论工具，它使我们能够为公平性指标（如人口均等差异）构建置信区间。通过推导影响函数，您将学会如何从一个简单的点估计，发展到对公平性进行统计上稳健的评估。",
            "id": "3105484",
            "problem": "给定一个二元受保护属性 $A \\in \\{0,1\\}$ 和特征 $X \\in \\mathbb{R}^{d}$。考虑从一个未知的分布 $P$（在 $\\mathbb{R}^{d} \\times \\{0,1\\}$ 上）中独立同分布（i.i.d.）抽取的观测值 $(X_{i}, A_{i})$（其中 $i=1,\\dots,n$）。令 $s:\\mathbb{R}^{d} \\to [0,1]$ 为一个固定的、可测的评分规则。对于分布 $P$，人口均等差异定义为如下泛函：\n$$\n\\Delta(P) = \\mathbb{E}_{P}\\!\\left[s(X)\\mid A=0\\right] - \\mathbb{E}_{P}\\!\\left[s(X)\\mid A=1\\right].\n$$\n一种常见的偏见缓解诊断方法是为 $\\Delta(P)$ 提供一个置信区间（CI），该置信区间通过影响函数（IF）的渐近线性展开获得。影响函数定义为一个均值为零的函数 $\\varphi(Z)$（其中 $Z=(X,A)$），它等于参数 $\\Delta(P)$ 沿形式为 $P_{\\varepsilon} = (1-\\varepsilon)P + \\varepsilon Q$ 的污染路径在 $\\varepsilon=0$ 处求值的 Gâteaux 导数。\n\n从条件期望的定义和通过污染路径定义的影响函数出发，在一个非参数模型中推导泛函 $\\Delta(P)$ 的影响函数 $\\varphi(Z)$，其中除了期望为有限所需的（可）测性和（可）积性条件外，$P$ 是不受限制的。请用指示函数 $\\mathbb{1}\\{A=a\\}$、得分 $s(X)$、群体概率 $\\pi_{a} = \\mathbb{P}_{P}(A=a)$ 以及群体条件均值 $m_{a} = \\mathbb{E}_{P}[s(X)\\mid A=a]$（其中 $a \\in \\{0,1\\}$）将您的最终答案表示为单个闭式解析表达式。最终答案不应需要数值近似。",
            "solution": "该问题要求推导人口均等差异泛函 $\\Delta(P)$ 的影响函数。验证过程确认了该问题是适定的、有科学依据的，并包含了进行形式化推导所需的所有信息。\n\n人口均等差异定义为：\n$$\n\\Delta(P) = \\mathbb{E}_{P}\\!\\left[s(X)\\mid A=0\\right] - \\mathbb{E}_{P}\\!\\left[s(X)\\mid A=1\\right]\n$$\n让我们将特定组的条件均值表示为 $m_{a} = \\mathbb{E}_{P}[s(X)\\mid A=a]$，其中 $a \\in \\{0,1\\}$。那么该泛函可以写成：\n$$\n\\Delta(P) = m_{0} - m_{1}\n$$\n影响函数 $\\varphi(Z)$ 是通过泛函 $\\Delta(P)$ 相对于分布 $P$ 的污染的 Gâteaux 导数来定义的。具体来说，对于由狄拉克测度 $\\delta_{z}$ 表示的在点 $z = (x, a') \\in \\mathbb{R}^{d} \\times \\{0,1\\}$ 处的污染，影响函数 $\\varphi(z)$ 由下式给出：\n$$\n\\varphi(z) = \\frac{d}{d\\varepsilon} \\Delta(P_{\\varepsilon}) \\Big|_{\\varepsilon=0}\n$$\n其中 $P_{\\varepsilon} = (1-\\varepsilon)P + \\varepsilon \\delta_{z}$。\n\n由于微分是线性算子，$\\Delta(P)$ 的影响函数是其组成项 $m_{0}$ 和 $m_{1}$ 的影响函数之差：\n$$\n\\varphi_{\\Delta}(Z) = \\varphi_{m_{0}}(Z) - \\varphi_{m_{1}}(Z)\n$$\n我们将首先推导一个通用条件均值泛函 $m_{a}(P)$（其中 $a \\in \\{0,1\\}$）的影响函数。条件期望可以表示为无条件期望的比值：\n$$\nm_{a}(P) = \\frac{\\mathbb{E}_{P}[s(X)\\mathbb{1}\\{A=a\\}]}{\\mathbb{E}_{P}[\\mathbb{1}\\{A=a\\}]} = \\frac{\\mathbb{E}_{P}[s(X)\\mathbb{1}\\{A=a\\}]}{\\mathbb{P}_{P}(A=a)}\n$$\n我们定义分子泛函为 $N_{a}(P) = \\mathbb{E}_{P}[s(X)\\mathbb{1}\\{A=a\\}]$，分母泛函为 $D_{a}(P) = \\mathbb{P}_{P}(A=a) = \\pi_{a}$。因此，$m_{a}(P) = \\frac{N_{a}(P)}{D_{a}(P)}$。\n\n我们现在在受污染的分布 $P_{\\varepsilon}$ 上评估这些泛函：\n$N_{a}(P_{\\varepsilon}) = \\mathbb{E}_{P_{\\varepsilon}}[s(X)\\mathbb{1}\\{A=a\\}] = \\int s(x_{obs})\\mathbb{1}\\{a_{obs}=a\\} \\, dP_{\\varepsilon}(x_{obs}, a_{obs})$\n根据积分的线性性质，这变成：\n$$\nN_{a}(P_{\\varepsilon}) = (1-\\varepsilon)\\int s(x_{obs})\\mathbb{1}\\{a_{obs}=a\\} \\, dP(x_{obs}, a_{obs}) + \\varepsilon \\int s(x_{obs})\\mathbb{1}\\{a_{obs}=a\\} \\, d\\delta_{z}(x_{obs}, a_{obs})\n$$\n$$\nN_{a}(P_{\\varepsilon}) = (1-\\varepsilon)N_{a}(P) + \\varepsilon s(x)\\mathbb{1}\\{a'=a\\}\n$$\n类似地，对于分母：\n$$\nD_{a}(P_{\\varepsilon}) = \\mathbb{E}_{P_{\\varepsilon}}[\\mathbb{1}\\{A=a\\}] = (1-\\varepsilon)D_{a}(P) + \\varepsilon\\mathbb{1}\\{a'=a\\}\n$$\n在点 $z=(x,a')$ 处 $m_{a}$ 的影响函数是导数 $\\frac{d}{d\\varepsilon}m_{a}(P_{\\varepsilon})|_{\\varepsilon=0}$。我们使用除法法则进行微分：\n$$\n\\varphi_{m_{a}}(z) = \\frac{d}{d\\varepsilon} \\left(\\frac{N_{a}(P_{\\varepsilon})}{D_{a}(P_{\\varepsilon})}\\right) \\Bigg|_{\\varepsilon=0} = \\frac{\\frac{d N_{a}(P_{\\varepsilon})}{d\\varepsilon}D_{a}(P_{\\varepsilon}) - N_{a}(P_{\\varepsilon})\\frac{d D_{a}(P_{\\varepsilon})}{d\\varepsilon}}{[D_{a}(P_{\\varepsilon})]^2} \\Bigg|_{\\varepsilon=0}\n$$\n$N_{a}(P_{\\varepsilon})$ 和 $D_{a}(P_{\\varepsilon})$ 关于 $\\varepsilon$ 的导数是：\n$$\n\\frac{d N_{a}(P_{\\varepsilon})}{d\\varepsilon} = -N_{a}(P) + s(x)\\mathbb{1}\\{a'=a\\}\n$$\n$$\n\\frac{d D_{a}(P_{\\varepsilon})}{d\\varepsilon} = -D_{a}(P) + \\mathbb{1}\\{a'=a\\}\n$$\n在 $\\varepsilon=0$ 处求值，我们有 $P_{0}=P$，因此 $N_{a}(P_{0})=N_{a}(P)$ 且 $D_{a}(P_{0})=D_{a}(P)$。将这些代入除法法则表达式中，得到：\n$$\n\\varphi_{m_{a}}(z) = \\frac{[-N_{a}(P) + s(x)\\mathbb{1}\\{a'=a\\}]D_{a}(P) - N_{a}(P)[-D_{a}(P) + \\mathbb{1}\\{a'=a\\}]}{[D_{a}(P)]^{2}}\n$$\n展开分子：\n$$\n\\varphi_{m_{a}}(z) = \\frac{-N_{a}(P)D_{a}(P) + s(x)\\mathbb{1}\\{a'=a\\}D_{a}(P) + N_{a}(P)D_{a}(P) - N_{a}(P)\\mathbb{1}\\{a'=a\\}}{[D_{a}(P)]^{2}}\n$$\n项 $-N_{a}(P)D_{a}(P)$ 和 $+N_{a}(P)D_{a}(P)$ 相互抵消。\n$$\n\\varphi_{m_{a}}(z) = \\frac{s(x)\\mathbb{1}\\{a'=a\\}D_{a}(P) - N_{a}(P)\\mathbb{1}\\{a'=a\\}}{[D_{a}(P)]^{2}}\n$$\n提出因子 $\\mathbb{1}\\{a'=a\\}$ 并除以一个 $D_{a}(P)$：\n$$\n\\varphi_{m_{a}}(z) = \\frac{\\mathbb{1}\\{a'=a\\}}{D_{a}(P)} \\left( s(x) - \\frac{N_{a}(P)}{D_{a}(P)} \\right)\n$$\n回到原始符号，其中 $z$ 是随机变量 $Z$ 的一个通用实现 $(X,A)$，$D_a(P) = \\pi_a$，且 $\\frac{N_a(P)}{D_a(P)} = m_a$，我们得到条件均值 $m_a$ 的影响函数：\n$$\n\\varphi_{m_{a}}(Z) = \\frac{\\mathbb{1}\\{A=a\\}}{\\pi_{a}}(s(X) - m_{a})\n$$\n我们可以通过检查其在 $P$ 下的期望是否为零来验证这是一个有效的影响函数：\n$$\n\\mathbb{E}_{P}[\\varphi_{m_{a}}(Z)] = \\mathbb{E}_{P}\\left[\\frac{\\mathbb{1}\\{A=a\\}}{\\pi_{a}}(s(X) - m_{a})\\right] = \\frac{1}{\\pi_{a}} \\left( \\mathbb{E}_{P}[\\mathbb{1}\\{A=a\\}s(X)] - m_{a}\\mathbb{E}_{P}[\\mathbb{1}\\{A=a\\}] \\right)\n$$\n根据定义，$\\mathbb{E}_{P}[\\mathbb{1}\\{A=a\\}s(X)] = \\mathbb{P}_{P}(A=a)\\mathbb{E}_{P}[s(X)|A=a] = \\pi_{a}m_{a}$，并且 $\\mathbb{E}_{P}[\\mathbb{1}\\{A=a\\}] = \\pi_{a}$。\n$$\n\\mathbb{E}_{P}[\\varphi_{m_{a}}(Z)] = \\frac{1}{\\pi_{a}}(\\pi_{a}m_{a} - m_{a}\\pi_{a}) = 0\n$$\n均值为零的性质成立。现在，我们组合得到 $\\Delta(P)$ 的最终影响函数：\n$$\n\\varphi_{\\Delta}(Z) = \\varphi_{m_{0}}(Z) - \\varphi_{m_{1}}(Z) = \\frac{\\mathbb{1}\\{A=0\\}}{\\pi_{0}}(s(X) - m_{0}) - \\frac{\\mathbb{1}\\{A=1\\}}{\\pi_{1}}(s(X) - m_{1})\n$$\n这就是人口均等差异影响函数的最终解析表达式。",
            "answer": "$$\n\\boxed{\\frac{\\mathbb{1}\\{A=0\\}}{\\pi_{0}}(s(X) - m_{0}) - \\frac{\\mathbb{1}\\{A=1\\}}{\\pi_{1}}(s(X) - m_{1})}\n$$"
        },
        {
            "introduction": "“处理中”（in-processing）方法将公平性约束直接嵌入到学习算法的优化过程中，在模型训练阶段就主动进行偏差纠正。这项实践  介绍了一种直观的技术，称为“公平梯度裁剪”（fair gradient clipping）。您将通过一个具体的动手实验，学习如何修改标准梯度下降算法，以平衡不同群体在模型更新过程中的影响力，从而探索在公平性、模型准确性和训练稳定性之间存在的微妙权衡。",
            "id": "3105436",
            "problem": "本题要求您为二元逻辑回归形式化并实现一种称为“公平梯度裁剪”的偏差缓解技术，该技术在训练期间限制特定群体梯度贡献的主导作用。您将从基本的经验风险最小化原则和逻辑损失的定义出发，推导出梯度和一个裁剪规则，该规则强制群体贡献之间存在有界比率，然后实现一个采用此裁剪规则的全批量梯度下降过程。您的程序必须计算量化指标，以揭示作为单个超参数函数的偏差与稳定性之间的权衡。\n\n考虑一个二元分类数据集，其包含一个表示两个群体 $A \\in \\{0,1\\}$ 的敏感属性。设数据为 $\\{(x_i, y_i, a_i)\\}_{i=1}^N$，其中每个 $x_i \\in \\mathbb{R}^d$（包含一个显式的偏置坐标作为值为 1 的额外特征），$y_i \\in \\{0,1\\}$，且 $a_i \\in \\{0,1\\}$。设模型为 $f_\\theta(x) = \\sigma(\\theta^\\top x)$，其参数向量为 $\\theta \\in \\mathbb{R}^d$，逻辑链接函数为 $\\sigma(z) = 1/(1 + e^{-z})$。带有 $\\ell_2$ 正则化的经验风险为：\n$$\n\\mathcal{L}(\\theta) \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\Big( -y_i \\log \\sigma(\\theta^\\top x_i) \\;-\\; (1-y_i)\\log\\!\\big(1 - \\sigma(\\theta^\\top x_i)\\big) \\Big) \\;+\\; \\frac{\\lambda}{2}\\|\\theta\\|_2^2.\n$$\n根据逻辑损失的定义和链式法则，单样本梯度向量为：\n$$\ng_i(\\theta) \\;=\\; \\big(\\sigma(\\theta^\\top x_i) - y_i\\big)\\, x_i.\n$$\n令 $G_a$ 表示 $a_i = a$ 的样本索引集，$n_a = |G_a|$，因此 $n_0 + n_1 = N$。定义群体 $a$ 内的平均梯度为：\n$$\ng_a(\\theta) \\;=\\; \\frac{1}{n_a}\\sum_{i \\in G_a} g_i(\\theta),\n$$\n并定义按组加权的贡献向量为：\n$$\nc_a(\\theta) \\;=\\; \\frac{n_a}{N}\\, g_a(\\theta), \\quad a \\in \\{0,1\\}.\n$$\n未裁剪的数据梯度为 $c_0(\\theta) + c_1(\\theta)$，包含正则化项的完整梯度为 $c_0(\\theta) + c_1(\\theta) + \\lambda \\theta$。\n\n公平梯度裁剪通过限制两个群体贡献向量的欧几里得范数之比来施加一个优势上限。具体来说，对于给定的比率上限 $\\rho \\in [1,\\infty)$，在每个更新步骤中，您必须减小 $\\|c_0(\\theta)\\|_2$ 和 $\\|c_1(\\theta)\\|_2$ 中较大者的范数（不改变其方向），使得缩放后，较大范数不超过较小范数的 $\\rho$ 倍；较小的贡献保持不变。如果两个群体的主导作用均未超过因子 $\\rho$，则不进行缩放。用于更新的数据梯度是两个（可能经过缩放的）贡献向量之和。然后执行一次全批量梯度下降更新。\n\n从以上定义出发，推导出一个满足优势上限、同时保持方向且不改变非主导群体的缩放规则。实现以下训练和评估协议。\n\n1) 固定的数据集和设置。使用 $d=3$，其中偏置坐标作为第三个分量包含在 $x_i$ 中，其值恒为 $1$。设 $N=16$，包含以下样本：\n- 群体 $A=0$（$n_0 = 12$）：六个正样本，$x = (2,2,1)$，$y=1$；六个负样本，$x = (-2,-2,1)$，$y=0$。\n- 群体 $A=1$（$n_1 = 4$）：两个正样本，$x = (-2,2,1)$，$y=1$；两个负样本，$x = (2,-2,1)$，$y=0$。\n\n2) 训练常数。使用全批量梯度下降，学习率 $\\eta = 0.2$，迭代次数 $T = 200$，正则化系数 $\\lambda = 0.01$。初始化 $\\theta_0 = 0 \\in \\mathbb{R}^3$。在每次迭代中，根据当前的 $\\theta_t$ 计算群体贡献，应用指定的 $\\rho$ 进行公平梯度裁剪，加上正则化项，并通过 $\\theta_{t+1} = \\theta_t - \\eta \\,\\tilde{g}_t$ 更新 $\\theta$，其中 $\\tilde{g}_t$ 是裁剪后的数据梯度加上正则化梯度。在需要时使用一个小的数值常数 $\\varepsilon = 10^{-12}$ 以避免除以零。\n\n3) 训练后报告的指标：\n- 总体逻辑损失：所有 $N$ 个样本上未正则化的逻辑损失的平均值，\n$$\n\\bar{\\ell} \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\Big( -y_i \\log \\sigma(\\theta^\\top x_i) - (1-y_i)\\log(1-\\sigma(\\theta^\\top x_i)) \\Big).\n$$\n- 公平性差距：两个群体之间未正则化的平均逻辑损失的绝对差，\n$$\n\\Delta_{\\text{loss}} \\;=\\; \\big| \\; \\frac{1}{n_0}\\sum_{i \\in G_0} \\ell_i \\;-\\; \\frac{1}{n_1}\\sum_{i \\in G_1} \\ell_i \\; \\big|.\n$$\n- 稳定性失真：迭代过程中的相对失真的平均值，定义为未裁剪数据梯度与裁剪后数据梯度之间的相对失真，\n$$\nD \\;=\\; \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\big\\| \\,(c_0(\\theta_t)+c_1(\\theta_t)) \\;-\\; (\\tilde{c}_0(\\theta_t)+\\tilde{c}_1(\\theta_t)) \\,\\big\\|_2}{\\big\\|\\, c_0(\\theta_t)+c_1(\\theta_t) \\,\\big\\|_2 + \\varepsilon}.\n$$\n\n4) 测试套件。使用以下四个优势上限 $\\rho$ 的值运行上述过程：\n- 情况 1：$\\rho = 1.0$。\n- 情况 2：$\\rho = 1.5$。\n- 情况 3：$\\rho = 3.0$。\n- 情况 4：$\\rho = 10^9$（这近似于不进行裁剪）。\n\n5) 程序输出。您的程序必须生成单行输出，其中包含一个包含四个结果的列表，每个测试用例一个结果，每个结果都是列表 $[\\bar{\\ell}, \\Delta_{\\text{loss}}, D]$，其中每个值都四舍五入到 $6$ 位小数。要求的输出格式为单行形式：\n$[[\\bar{\\ell}_1,\\Delta_{\\text{loss},1},D_1],[\\bar{\\ell}_2,\\Delta_{\\text{loss},2},D_2],[\\bar{\\ell}_3,\\Delta_{\\text{loss},3},D_3],[\\bar{\\ell}_4,\\Delta_{\\text{loss},4},D_4]]$。\n\n6) 实现约束。代码必须完全自包含，无任何输入，并且必须实现您从优势上限定义中推导出的基于推导的裁剪规则。所有计算都必须以浮点数进行，并仔细处理任何除法或对数中的 $\\varepsilon$ 以避免未定义的值。不涉及物理单位。不涉及角度。任何时候您需要表示一个分数，在最终输出中必须如上所述以十进制数表示。",
            "solution": "用户提供的问题是有效的。它提出了一个在计算统计学和机器学习领域中定义明确的任务，该任务基于逻辑回归、基于梯度的优化和算法公平性的既定原则。所有数据、常数和程序步骤都以足够的精度进行了规定，从而能够得到一个唯一的、可验证的解决方案。该问题是自包含的、科学上合理的和客观的。\n\n### 1. 基于原则的设计：推导与算法\n\n问题的核心是在逻辑回归的标准梯度下降程序中实现一个公平的梯度裁剪机制。这要求将裁剪规则形式化，并将其整合到迭代优化算法中。\n\n#### 1.1. 模型和梯度定义\n\n模型是一个逻辑回归器，$f_\\theta(x) = \\sigma(\\theta^\\top x)$，其中 $\\sigma(z) = (1 + e^{-z})^{-1}$ 是 sigmoid 函数。参数 $\\theta \\in \\mathbb{R}^d$ 通过最小化经验风险来进行优化，该风险是所有 $N$ 个数据点的二元交叉熵损失之和，外加一个 $\\ell_2$ 正则化项：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\ell_i(\\theta) + \\frac{\\lambda}{2}\\|\\theta\\|_2^2, \\quad \\text{where} \\quad \\ell_i(\\theta) = -y_i \\log \\sigma(\\theta^\\top x_i) - (1-y_i)\\log(1 - \\sigma(\\theta^\\top x_i)).\n$$\n该损失函数的梯度是数据项和正则化项贡献的总和。数据项的梯度可以按组分解。单样本梯度由 $g_i(\\theta) = (\\sigma(\\theta^\\top x_i) - y_i) x_i$ 给出。对于群体 $a \\in \\{0,1\\}$，按组加权的贡献向量定义为 $c_a(\\theta) = \\frac{n_a}{N} g_a(\\theta) = \\frac{1}{N} \\sum_{i \\in G_a} g_i(\\theta)$，其中 $G_a$ 是群体 $a$ 中数据点的索引集。总数据梯度为 $g_{\\text{data}}(\\theta) = c_0(\\theta) + c_1(\\theta)$，完整梯度为 $g(\\theta) = c_0(\\theta) + c_1(\\theta) + \\lambda\\theta$。\n\n#### 1.2. 公平梯度裁剪规则的推导\n\n裁剪规则旨在限制一个群体的梯度贡献相对于另一个群体的主导作用。令 $n_{c0} = \\|c_0(\\theta)\\|_2$ 和 $n_{c1} = \\|c_1(\\theta)\\|_2$ 为群体贡献向量的欧几里得范数。对于给定的优势上限 $\\rho \\ge 1$，我们必须强制使裁剪后的向量 $\\tilde{c}_0(\\theta)$ 和 $\\tilde{c}_1(\\theta)$ 的范数满足 $\\|\\tilde{c}_{\\text{larger}}\\|_2 \\le \\rho \\cdot \\|\\tilde{c}_{\\text{smaller}}\\|_2$。裁剪操作会缩减范数较大的向量（同时保持其方向），而范数较小的向量保持不变。\n\n让我们将此规则形式化：\n\n1.  **情况 1：群体 0 的贡献占主导。**\n    如果 $n_{c0} > \\rho \\cdot n_{c1}$，则必须裁剪贡献 $c_0(\\theta)$。为保持其方向，裁剪后的向量 $\\tilde{c}_0(\\theta)$ 必须是原始向量的缩放版本，即 $\\tilde{c}_0(\\theta) = s \\cdot c_0(\\theta)$，其中 $s > 0$ 是一个标量。新范数需要满足 $\\|\\tilde{c}_0(\\theta)\\|_2 = \\rho \\cdot n_{c1}$。这意味着 $s \\cdot n_{c0} = \\rho \\cdot n_{c1}$，从而得出缩放因子 $s = \\frac{\\rho \\cdot n_{c1}}{n_{c0}}$。另一个贡献保持不变。因此：\n    $$\n    \\tilde{c}_0(\\theta) = c_0(\\theta) \\cdot \\frac{\\rho \\cdot \\|c_1(\\theta)\\|_2}{\\|c_0(\\theta)\\|_2}, \\quad \\tilde{c}_1(\\theta) = c_1(\\theta).\n    $$\n\n2.  **情况 2：群体 1 的贡献占主导。**\n    对称地，如果 $n_{c1} > \\rho \\cdot n_{c0}$，则必须裁剪贡献 $c_1(\\theta)$。遵循同样的逻辑：\n    $$\n    \\tilde{c}_1(\\theta) = c_1(\\theta) \\cdot \\frac{\\rho \\cdot \\|c_0(\\theta)\\|_2}{\\|c_1(\\theta)\\|_2}, \\quad \\tilde{c}_0(\\theta) = c_0(\\theta).\n    $$\n\n3.  **情况 3：无主导。**\n    如果两个群体的贡献均未超过对方的 $\\rho$ 倍（即 $n_{c0} \\le \\rho \\cdot n_{c1}$ 且 $n_{c1} \\le \\rho \\cdot n_{c0}$），则不应用裁剪：\n    $$\n    \\tilde{c}_0(\\theta) = c_0(\\theta), \\quad \\tilde{c}_1(\\theta) = c_1(\\theta).\n    $$\n为防止在实现中出现除以零的情况，特别是当某个范数为零时，分母将增加一个小的常数 $\\varepsilon$。例如，情况 1 中的缩放因子变为 $s = \\frac{\\rho \\cdot \\|c_1(\\theta)\\|_2}{\\|c_0(\\theta)\\|_2 + \\varepsilon}$。\n\n#### 1.3. 算法流程\n\n完整的算法将此裁剪规则集成到全批量梯度下降循环中。对于由 $\\rho$ 值定义的每个测试用例：\n\n1.  **初始化**：设置参数向量 $\\theta_0 = 0 \\in \\mathbb{R}^3$。初始化一个空列表以存储每次迭代的失真值。\n\n2.  **训练循环**：对于从 $0$ 到 $T-1$ 的每次迭代 $t$（共 $T=200$ 次迭代）：\n    a.  使用每个群体的所有数据点，计算当前的群体贡献向量 $c_0(\\theta_t)$ 和 $c_1(\\theta_t)$。\n    b.  计算它们的范数，$n_{c0} = \\|c_0(\\theta_t)\\|_2$ 和 $n_{c1} = \\|c_1(\\theta_t)\\|_2$。\n    c.  应用推导出的裁剪规则和给定的 $\\rho$，得到裁剪后的向量 $\\tilde{c}_0(\\theta_t)$ 和 $\\tilde{c}_1(\\theta_t)$。\n    d.  计算本次迭代的稳定性失真：\n        $$\n        d_t = \\frac{\\big\\| (c_0(\\theta_t)+c_1(\\theta_t)) - (\\tilde{c}_0(\\theta_t)+\\tilde{c}_1(\\theta_t)) \\big\\|_2}{\\big\\| c_0(\\theta_t)+c_1(\\theta_t) \\big\\|_2 + \\varepsilon}\n        $$\n        并存储它。\n    e.  构建用于更新的最终梯度：$\\tilde{g}_t = \\tilde{c}_0(\\theta_t) + \\tilde{c}_1(\\theta_t) + \\lambda\\theta_t$。\n    f.  更新参数：$\\theta_{t+1} = \\theta_t - \\eta \\tilde{g}_t$。\n\n3.  **评估**：在 $T$ 次迭代之后，使用最终的参数向量 $\\theta_T$ 来计算评估指标：\n    a.  **总体损失 $\\bar{\\ell}$**：计算 $N=16$ 个数据点中每个点的未正则化逻辑损失，并取其平均值。\n    b.  **公平性差距 $\\Delta_{\\text{loss}}$**：计算群体 0（超过 $n_0=12$ 个点）和群体 1（超过 $n_1=4$ 个点）的平均损失，并求这两个平均值之间的绝对差。\n    c.  **稳定性失真 $D$**：计算存储的 $T$ 个失真值 $d_t$ 的平均值。\n\n对每个指定的 $\\rho$ 值重复此过程，为每个测试用例生成一组指标 $[\\bar{\\ell}, \\Delta_{\\text{loss}}, D]$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates the fair gradient clipping algorithm for logistic regression.\n    \"\"\"\n    # 1. Fixed dataset and setup\n    d = 3\n    N = 16\n    n0, n1 = 12, 4\n    \n    # Define the four unique types of data points as numpy arrays\n    x0_pos = np.array([2.0, 2.0, 1.0])\n    x0_neg = np.array([-2.0, -2.0, 1.0])\n    x1_pos = np.array([-2.0, 2.0, 1.0])\n    x1_neg = np.array([2.0, -2.0, 1.0])\n    \n    # Counts for each type of point\n    counts = {'g0_pos': 6, 'g0_neg': 6, 'g1_pos': 2, 'g1_neg': 2}\n    \n    # 2. Training constants\n    eta = 0.2\n    T = 200\n    lambda_reg = 0.01\n    epsilon = 1e-12\n    \n    # 4. Test suite\n    test_cases = [1.0, 1.5, 3.0, 10**9]\n    \n    final_results = []\n\n    def sigma(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    for rho in test_cases:\n        theta = np.zeros(d)\n        distortions = []\n        \n        # Training loop\n        for _ in range(T):\n            # Calculate predictions for each point type\n            pred_0_pos = sigma(theta @ x0_pos)\n            pred_0_neg = sigma(theta @ x0_neg)\n            pred_1_pos = sigma(theta @ x1_pos)\n            pred_1_neg = sigma(theta @ x1_neg)\n            \n            # Calculate per-example gradients\n            g_0_pos = (pred_0_pos - 1.0) * x0_pos  # y=1\n            g_0_neg = (pred_0_neg - 0.0) * x0_neg  # y=0\n            g_1_pos = (pred_1_pos - 1.0) * x1_pos  # y=1\n            g_1_neg = (pred_1_neg - 0.0) * x1_neg  # y=0\n\n            # Calculate group contribution vectors c_a = (1/N) * sum_{i in G_a} g_i\n            c0 = (1 / N) * (counts['g0_pos'] * g_0_pos + counts['g0_neg'] * g_0_neg)\n            c1 = (1 / N) * (counts['g1_pos'] * g_1_pos + counts['g1_neg'] * g_1_neg)\n            \n            # Apply Fair Gradient Clipping\n            norm_c0 = np.linalg.norm(c0)\n            norm_c1 = np.linalg.norm(c1)\n            \n            c0_tilde, c1_tilde = c0, c1\n            if norm_c0 > rho * norm_c1:\n                scaling_factor = (rho * norm_c1) / (norm_c0 + epsilon)\n                c0_tilde = c0 * scaling_factor\n            elif norm_c1 > rho * norm_c0:\n                scaling_factor = (rho * norm_c0) / (norm_c1 + epsilon)\n                c1_tilde = c1 * scaling_factor\n            \n            # Calculate distortion for the current iteration\n            unclipped_data_grad = c0 + c1\n            clipped_data_grad = c0_tilde + c1_tilde\n            \n            distortion_numerator = np.linalg.norm(unclipped_data_grad - clipped_data_grad)\n            distortion_denominator = np.linalg.norm(unclipped_data_grad) + epsilon\n            distortions.append(distortion_numerator / distortion_denominator)\n\n            # Gradient update\n            reg_grad = lambda_reg * theta\n            total_grad = clipped_data_grad + reg_grad\n            theta = theta - eta * total_grad\n\n        # 3. Metrics calculation after training\n        final_theta = theta\n        \n        # Predictions with final theta\n        pred_0_pos = sigma(final_theta @ x0_pos)\n        pred_0_neg = sigma(final_theta @ x0_neg)\n        pred_1_pos = sigma(final_theta @ x1_pos)\n        pred_1_neg = sigma(final_theta @ x1_neg)\n        \n        # Individual losses (with epsilon for log stability)\n        loss_0_pos = -np.log(pred_0_pos + epsilon)\n        loss_0_neg = -np.log(1 - pred_0_neg + epsilon)\n        loss_1_pos = -np.log(pred_1_pos + epsilon)\n        loss_1_neg = -np.log(1 - pred_1_neg + epsilon)\n\n        # Overall loss (ell_bar)\n        total_loss = (counts['g0_pos'] * loss_0_pos + counts['g0_neg'] * loss_0_neg +\n                      counts['g1_pos'] * loss_1_pos + counts['g1_neg'] * loss_1_neg)\n        ell_bar = total_loss / N\n\n        # Fairness gap (Delta_loss)\n        avg_loss_g0 = (counts['g0_pos'] * loss_0_pos + counts['g0_neg'] * loss_0_neg) / n0\n        avg_loss_g1 = (counts['g1_pos'] * loss_1_pos + counts['g1_neg'] * loss_1_neg) / n1\n        delta_loss = abs(avg_loss_g0 - avg_loss_g1)\n\n        # Stability distortion (D)\n        D = np.mean(distortions)\n        \n        final_results.append([round(ell_bar, 6), round(delta_loss, 6), round(D, 6)])\n\n    # 5. Program output\n    print(f\"{final_results}\")\n\nsolve()\n```"
        },
        {
            "introduction": "当模型已经训练完毕或无法重新训练时，“后处理”（post-processing）技术提供了一种灵活的偏差缓解方案。这项实践  探讨了如何应用逐组温度缩放（per-group temperature scaling）来校准模型输出，以满足特定的公平性标准，例如实现不同群体间均等的真正例率（equal opportunity）。通过这个练习，您将理解如何调整模型预测，同时也会发现这种单调变换并不会改变模型固有的排序能力，即其ROC曲线保持不变。",
            "id": "3105464",
            "problem": "一个二元分类器对属于敏感群体 $g \\in \\{A,B\\}$ 的实例 $x$ 产生一个未校准的 logit 输出 $z(x,g) \\in \\mathbb{R}$。为了在改善校准的同时减轻偏差，您考虑采用按群体进行温度缩放的方法：为每个群体 $g$ 定义一个校准分数 $s(x,g) = \\sigma(z(x,g)/T_{g})$，其中 $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ 是逻辑S型函数（logistic sigmoid），$T_{g} > 0$ 是一个待选择的特定于群体的温度参数。一个全局决策规则在两个群体中以一个共同的水平 $\\tau \\in (0,1)$ 对校准分数进行阈值处理。\n\n仅使用接收者操作特征（ROC）和类条件决策率的基本定义来推理以下问题。在整个问题中，假设对于每个群体 $g$ 和标签 $y \\in \\{0,1\\}$，未校准 logit 的类条件分布是具有共同方差的高斯分布：$z \\mid (Y=y,G=g) \\sim \\mathcal{N}(\\mu_{g,y}, \\sigma^{2})$，其中 $\\sigma>0$ 已知。\n\n您的任务是：\n- 从 ROC 曲线的定义出发，即通过改变决策阈值可获得的 $(\\operatorname{FPR}, \\operatorname{TPR})$ 对的集合，从第一性原理论证，对于任何固定的群体 $g$，将 $z$ 替换为 $z/T_{g}$ 然后映射到 $s=\\sigma(z/T_{g})$ 是否会改变该群体的 ROC 曲线。使用所涉及变换的单调性来证明您的结论。\n- 对于一个固定的全局分数阈值 $\\tau \\in (0,1)$，根据标准正态累积分布函数 $\\Phi$ 和参数 $(\\mu_{g,1},\\mu_{g,0},\\sigma,T_{g},\\tau)$，推导出真阳性率 $\\operatorname{TPR}_{g}(\\tau,T_{g})$ 和假阳性率 $\\operatorname{FPR}_{g}(\\tau,T_{g})$ 的闭式表达式。您的推导必须从定义 $\\operatorname{TPR}_{g} = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=1,G=g)$ 和 $\\operatorname{FPR}_{g} = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=0,G=g)$ 开始。\n- 假设参数为 $\\mu_{A,1} = 1.2$，$\\mu_{B,1} = 0.8$，$\\mu_{A,0} = -0.2$，$\\mu_{B,0} = -0.3$，$\\sigma = 1$，$T_{B} = 1$，以及 $\\tau = 0.7$。确定唯一的 $T_{A} > 0$ 值，使得在固定阈值 $\\tau$ 下，两个群体的真阳性率相等，即求解 $\\operatorname{TPR}_{A}(\\tau,T_{A}) = \\operatorname{TPR}_{B}(\\tau,T_{B})$。将您最终的 $T_{A}$ 数值答案四舍五入到四位有效数字。\n\n重要提示：\n- 接收者操作特征（ROC）指的就是 Receiver Operating Characteristic (ROC)，曲线下面积（AUC）指的就是 Area Under the Curve (AUC)。\n- 您的最终答案必须是一个实数。按要求四舍五入到四位有效数字。不需要单位。",
            "solution": "首先根据指定标准对问题进行验证。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- 一个二元分类器对实例 $x$ 和敏感群体 $g \\in \\{A,B\\}$ 产生一个未校准的 logit 输出 $z(x,g) \\in \\mathbb{R}$。\n- 校准分数定义为 $s(x,g) = \\sigma(z(x,g)/T_{g})$，其中 $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ 是逻辑S型函数。\n- $T_{g} > 0$ 是一个特定于群体的温度参数。\n- 一个全局决策规则以一个共同的水平 $\\tau \\in (0,1)$ 对校准分数进行阈值处理。\n- logit 的类条件分布给定为 $z \\mid (Y=y,G=g) \\sim \\mathcal{N}(\\mu_{g,y}, \\sigma^{2})$，其中 $\\sigma>0$ 已知。\n- 任务1：确定对于一个群体 $g$，当分数从 $z$ 变换到 $s=\\sigma(z/T_{g})$ 时，其 ROC 曲线是否会改变。\n- 任务2：使用定义 $\\operatorname{TPR}_{g} = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=1,G=g)$ 和 $\\operatorname{FPR}_{g} = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=0,G=g)$ 推导出 $\\operatorname{TPR}_{g}(\\tau,T_{g})$ 和 $\\operatorname{FPR}_{g}(\\tau,T_{g})$ 的闭式表达式。\n- 任务3：给定参数 $\\mu_{A,1} = 1.2$，$\\mu_{B,1} = 0.8$，$\\mu_{A,0} = -0.2$，$\\mu_{B,0} = -0.3$，$\\sigma = 1$，$T_{B} = 1$ 和 $\\tau = 0.7$，求使 $\\operatorname{TPR}_{A}(\\tau,T_{A}) = \\operatorname{TPR}_{B}(\\tau,T_{B})$ 成立的 $T_{A} > 0$ 的值。\n- 最终的 $T_{A}$ 数值答案必须四舍五入到四位有效数字。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题牢固地定位于统计学习和算法公平性领域。温度缩放是模型校准的标准技术。使用高斯分布对 logits 建模是一种常见且有效的建模假设。所有概念，如 ROC 曲线、TPR 和 FPR，都是标准的且定义明确。\n- **良态性：** 问题提供了所有必要信息。问题在数学上是精确的，其结构导向一个唯一解。可以验证存在唯一的正值 $T_A$。\n- **客观性：** 问题以形式化的数学语言陈述，没有任何主观或模糊的术语。\n\n**步骤3：结论与行动**\n该问题是有效的，因为它具有科学依据、良态、客观且完整。我将继续构建解决方案。\n\n### 解\n\n**第1部分：ROC曲线的不变性**\n\n接收者操作特征（ROC）曲线定义为通过在分类器分数的整个范围内改变决策阈值而生成的所有可实现的（假阳性率，真阳性率）对，即 $(\\operatorname{FPR}, \\operatorname{TPR})$ 的集合。\n\n对于一个固定的群体 $g$，原始分数是 logit $z$。通过将 $z$ 与阈值 $\\theta_z \\in (-\\infty, \\infty)$ 进行比较来做出决策。新的分数是 $s = \\sigma(z/T_g)$。通过将 $s$ 与阈值 $\\tau_s \\in (0,1)$ 进行比较来做出决策。我们必须确定两个评分系统的 $(\\operatorname{FPR}, \\operatorname{TPR})$ 对集合是否相同。\n\n从 $z$ 到 $s$ 的变换是两个函数的复合：$f_1(z) = z/T_g$ 和 $f_2(u) = \\sigma(u)$。\n1.  由于 $T_g > 0$ 是一个正常数，函数 $f_1(z) = z/T_g$ 是一个简单的缩放，它是 $z$ 的严格单调递增函数。\n2.  逻辑S型函数 $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ 的导数为 $\\frac{d\\sigma}{du} = \\sigma(u)(1-\\sigma(u))$。由于对于所有 $u \\in \\mathbb{R}$，$\\sigma(u) \\in (0,1)$，所以导数是严格为正的。因此，$\\sigma(u)$ 也是 $u$ 的一个严格单调递增函数。\n\n两个严格单调递增函数的复合本身也是严格单调递增的。因此，校准分数 $s(z) = \\sigma(z/T_g)$ 是 logit $z$ 的一个严格单调函数。\n\n这种严格的单调性意味着在 $s$ 上的任何决策阈值与在 $z$ 上的等效决策阈值之间存在一一对应的关系。一个形如 $s \\ge \\tau_s$ 的决策规则等价于：\n$$ \\sigma(z/T_g) \\ge \\tau_s $$\n对两边应用同样是严格递增的反S型函数 $\\sigma^{-1}(v) = \\ln(\\frac{v}{1-v})$，不等式得以保持：\n$$ z/T_g \\ge \\sigma^{-1}(\\tau_s) $$\n由于 $T_g > 0$，我们可以乘以 $T_g$ 而不改变不等式方向：\n$$ z \\ge T_g \\sigma^{-1}(\\tau_s) $$\n令 $\\theta_z = T_g \\sigma^{-1}(\\tau_s)$。当阈值 $\\tau_s$ 扫过其整个范围 $(0,1)$ 时，其反函数 $\\sigma^{-1}(\\tau_s)$ 扫过范围 $(-\\infty, \\infty)$。由于 $T_g$ 是一个正常数，等效阈值 $\\theta_z$ 也扫过整个范围 $(-\\infty, \\infty)$。\n\n这表明，对于任何基于 $s$ 的阈值的可能分类规则，都存在一个基于 $z$ 的阈值的等效规则，该规则产生完全相同的正负预测集合。因此，对于任何用分数 $s$ 可实现的 $(\\operatorname{FPR}, \\operatorname{TPR})$ 对，用分数 $z$ 也可以实现相同的对，反之亦然。\n\n因此，所有可实现的 $(\\operatorname{FPR}, \\operatorname{TPR})$ 对的集合——即 ROC 曲线——对于两种评分函数是相同的。应用按群体的温度缩放和S型函数不会改变该群体的 ROC 曲线；它只是重新参数化了决策阈值。\n\n**第2部分：TPR 和 FPR 表达式的推导**\n\n我们为一个特定的群体 $g$、一个固定的全局阈值 $\\tau$ 和一个温度 $T_g$ 推导表达式。\n\n真阳性率定义为 $\\operatorname{TPR}_{g}(\\tau,T_{g}) = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=1,G=g)$。\n代入 $s(x,g)$ 的定义：\n$$ \\operatorname{TPR}_{g} = \\mathbb{P}\\left(\\sigma(z/T_{g}) \\ge \\tau \\mid Y=1,G=g\\right) $$\n正如在第1部分中确立的，这个不等式等价于：\n$$ \\operatorname{TPR}_{g} = \\mathbb{P}\\left(z \\ge T_{g} \\sigma^{-1}(\\tau) \\mid Y=1,G=g\\right) $$\n我们已知在条件 $(Y=1,G=g)$ 下，logit $z$ 服从正态分布：$z \\sim \\mathcal{N}(\\mu_{g,1}, \\sigma^2)$。为了评估该概率，我们将随机变量 $z$ 标准化。令 $Z_{std} = \\frac{z - \\mu_{g,1}}{\\sigma}$，其中 $Z_{std} \\sim \\mathcal{N}(0,1)$。\n不等式 $z \\ge T_{g} \\sigma^{-1}(\\tau)$ 可以重写为：\n$$ z - \\mu_{g,1} \\ge T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,1} $$\n$$ \\frac{z - \\mu_{g,1}}{\\sigma} \\ge \\frac{T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,1}}{\\sigma} $$\n所以，$\\operatorname{TPR}_{g} = \\mathbb{P}\\left(Z_{std} \\ge \\frac{T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,1}}{\\sigma}\\right)$。\n使用标准正态累积分布函数 $\\Phi(v) = \\mathbb{P}(Z_{std} \\le v)$ 和对称性质 $\\mathbb{P}(Z_{std} \\ge c) = \\mathbb{P}(Z_{std} \\le -c) = \\Phi(-c)$，我们得到：\n$$ \\operatorname{TPR}_{g}(\\tau,T_{g}) = \\Phi\\left(-\\frac{T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,1}}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu_{g,1} - T_{g} \\sigma^{-1}(\\tau)}{\\sigma}\\right) $$\n假阳性率的推导是类似的。$\\operatorname{FPR}_{g}(\\tau,T_{g}) = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=0,G=g)$。\n现在的条件是 $(Y=0,G=g)$，在此条件下 $z \\sim \\mathcal{N}(\\mu_{g,0}, \\sigma^2)$。\n决策规则仍然是 $z \\ge T_{g} \\sigma^{-1}(\\tau)$。\n相对于 $\\mu_{g,0}$ 进行标准化：\n$$ \\operatorname{FPR}_{g} = \\mathbb{P}\\left(\\frac{z - \\mu_{g,0}}{\\sigma} \\ge \\frac{T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,0}}{\\sigma} \\mid Y=0,G=g\\right) $$\n$$ \\operatorname{FPR}_{g}(\\tau,T_{g}) = \\Phi\\left(-\\frac{T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,0}}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu_{g,0} - T_{g} \\sigma^{-1}(\\tau)}{\\sigma}\\right) $$\n\n**第3部分：$T_A$ 的计算**\n\n我们的任务是找到 $T_A > 0$ 的值，使得 $\\operatorname{TPR}_{A}(\\tau,T_{A}) = \\operatorname{TPR}_{B}(\\tau,T_{B})$。使用在第2部分推导出的 $\\operatorname{TPR}_{g}$ 表达式：\n$$ \\Phi\\left(\\frac{\\mu_{A,1} - T_{A} \\sigma^{-1}(\\tau)}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu_{B,1} - T_{B} \\sigma^{-1}(\\tau)}{\\sigma}\\right) $$\n由于标准正态累积分布函数 $\\Phi$ 是一个严格递增函数，$\\Phi(a) = \\Phi(b)$ 意味着 $a = b$。因此，我们可以令 $\\Phi$ 的参数相等：\n$$ \\frac{\\mu_{A,1} - T_{A} \\sigma^{-1}(\\tau)}{\\sigma} = \\frac{\\mu_{B,1} - T_{B} \\sigma^{-1}(\\tau)}{\\sigma} $$\n乘以 $\\sigma$（因为 $\\sigma > 0$）：\n$$ \\mu_{A,1} - T_{A} \\sigma^{-1}(\\tau) = \\mu_{B,1} - T_{B} \\sigma^{-1}(\\tau) $$\n我们重新整理这个方程来解出 $T_A$：\n$$ T_{A} \\sigma^{-1}(\\tau) = \\mu_{A,1} - \\mu_{B,1} + T_{B} \\sigma^{-1}(\\tau) $$\n项 $\\sigma^{-1}(\\tau)$ 是 logit 函数：$\\sigma^{-1}(\\tau) = \\ln\\left(\\frac{\\tau}{1-\\tau}\\right)$。对于 $\\tau=0.7$，该值为 $\\ln\\left(\\frac{0.7}{0.3}\\right) = \\ln(7/3)$，不为零。因此，我们可以用它来除：\n$$ T_A = \\frac{\\mu_{A,1} - \\mu_{B,1}}{\\sigma^{-1}(\\tau)} + T_B $$\n现在，我们代入给定的数值：\n$\\mu_{A,1} = 1.2$\n$\\mu_{B,1} = 0.8$\n$T_{B} = 1$\n$\\tau = 0.7$\n\n首先，计算 $\\sigma^{-1}(\\tau)$：\n$$ \\sigma^{-1}(0.7) = \\ln\\left(\\frac{0.7}{1-0.7}\\right) = \\ln\\left(\\frac{0.7}{0.3}\\right) = \\ln\\left(\\frac{7}{3}\\right) $$\n现在，将此代入 $T_A$ 的表达式中：\n$$ T_A = \\frac{1.2 - 0.8}{\\ln(7/3)} + 1 $$\n$$ T_A = \\frac{0.4}{\\ln(7/3)} + 1 $$\n我们计算数值：\n$$ \\ln(7/3) \\approx 0.84729786038 $$\n$$ T_A \\approx \\frac{0.4}{0.84729786038} + 1 \\approx 0.472081015 + 1 = 1.472081015 $$\n问题要求答案四舍五入到四位有效数字。\n$$ T_A \\approx 1.472 $$\n该值为正，与约束 $T_A > 0$ 一致。",
            "answer": "$$\\boxed{1.472}$$"
        }
    ]
}