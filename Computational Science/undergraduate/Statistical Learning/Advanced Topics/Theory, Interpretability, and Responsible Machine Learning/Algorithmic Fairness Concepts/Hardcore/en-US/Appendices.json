{
    "hands_on_practices": [
        {
            "introduction": "One of the most direct ways to influence algorithmic fairness is by carefully curating the data used for training. This practice explores a pre-processing technique where we strategically sub-sample from different demographic groups to enforce a fairness criterion before a model is ever built. You will investigate how to achieve demographic parity—equal positive rates across groups in the training data—by deriving optimal sampling probabilities that respect a fixed budget, and quantify the statistical cost of this intervention in terms of estimation error . This exercise provides a concrete look at the trade-offs between achieving fairness at the data-level and preserving the statistical integrity of your dataset.",
            "id": "3098386",
            "problem": "Consider a binary classification setting with two demographic groups $g \\in \\{0,1\\}$ and a binary outcome $Y \\in \\{0,1\\}$. The population sizes are $N_{0}$ and $N_{1}$, and the true positive prevalences are $p_{0} = \\mathbb{P}(Y=1 \\mid g=0)$ and $p_{1} = \\mathbb{P}(Y=1 \\mid g=1)$. You construct a training dataset by sub-sampling independently within each group using the following label-dependent inclusion scheme: for group $g$, include each positive ($Y=1$) independently with probability $a_{g} \\in (0,1]$ and each negative ($Y=0$) independently with probability $b_{g} \\in (0,1]$. The expected number of included points from group $g$ is $N_{g}\\big(a_{g} p_{g} + b_{g} (1-p_{g})\\big)$.\n\nYou wish to achieve demographic parity in the training distribution in the sense of equalizing the expected positive prevalence across the two groups. That is, you require that the expected prevalence in the constructed sample for each group equals a common target level $\\tau \\in (0,1)$:\n$$\n\\frac{a_{g} p_{g}}{a_{g} p_{g} + b_{g} (1-p_{g})} \\;=\\; \\tau \\quad \\text{for } g \\in \\{0,1\\}.\n$$\nAdditionally, suppose you have a fixed expected sampling budget $B>0$ that constrains the total expected sample size:\n$$\nN_{0}\\big(a_{0} p_{0} + b_{0} (1-p_{0})\\big) + N_{1}\\big(a_{1} p_{1} + b_{1} (1-p_{1})\\big) \\;=\\; B.\n$$\n\nTo quantify the accuracy cost of sub-sampling, you will estimate the true group prevalences $p_{g}$ using the Horvitz–Thompson (HT) estimator, which in this setting uses inverse-probability weights for included positives:\n$$\n\\widehat{p}_{g} \\;=\\; \\frac{1}{N_{g}} \\sum_{i=1}^{N_{g}} \\frac{\\mathbf{1}\\{\\text{unit } i \\text{ is included}\\} \\cdot \\mathbf{1}\\{Y_{i}=1\\}}{a_{g}}.\n$$\nYou will measure fairness by the target equalized prevalence constraint and you will measure accuracy by the mean squared error (MSE) of the estimator of the prevalence difference $\\delta = p_{1} - p_{0}$:\n$$\n\\widehat{\\delta} \\;=\\; \\widehat{p}_{1} - \\widehat{p}_{0}.\n$$\n\nStarting from fundamental definitions of expectation and variance for independent Bernoulli sampling and the unbiasedness of the Horvitz–Thompson estimator, derive the optimal inclusion probabilities $a_{0}^{\\star}, a_{1}^{\\star}, b_{0}^{\\star}, b_{1}^{\\star}$ that minimize the mean squared error $\\mathbb{E}\\big[(\\widehat{\\delta} - \\delta)^{2}\\big]$ subject to the demographic parity constraint above and the budget constraint. Assume $B$ and $\\tau$ are such that the optimal inclusion probabilities lie in the interval $(0,1]$.\n\nExpress your final answers for $a_{0}^{\\star}, a_{1}^{\\star}, b_{0}^{\\star}, b_{1}^{\\star}$ in closed form as functions of $N_{0}, N_{1}, p_{0}, p_{1}, \\tau, B$. The final answer must be a single analytical expression, and if you choose to present multiple quantities, write them as a single row matrix using LaTeX’s $\\mathrm{pmatrix}$ environment. No numerical rounding is required.",
            "solution": "## PROBLEM VALIDATION\n\n### Step 1: Extract Givens\n- **Setting**: Binary classification with binary outcome $Y \\in \\{0, 1\\}$ and two demographic groups $g \\in \\{0, 1\\}$.\n- **Population parameters**:\n    - Group sizes: $N_{0}$, $N_{1}$.\n    - True positive prevalences: $p_{0} = \\mathbb{P}(Y=1 \\mid g=0)$ and $p_{1} = \\mathbb{P}(Y=1 \\mid g=1)$.\n- **Sampling scheme**: Independent sub-sampling within each group.\n    - Inclusion probability for positives ($Y=1$) in group $g$: $a_{g} \\in (0, 1]$.\n    - Inclusion probability for negatives ($Y=0$) in group $g$: $b_{g} \\in (0, 1]$.\n- **Expected sample size for group g**: $N_{g}\\big(a_{g} p_{g} + b_{g} (1-p_{g})\\big)$.\n- **Constraints**:\n    1.  **Demographic Parity Constraint**: The expected positive prevalence in the sampled data for each group must equal a target level $\\tau \\in (0,1)$:\n        $$ \\frac{a_{g}p_{g}}{a_{g} p_{g} + b_{g} (1-p_{g})} = \\tau \\quad \\text{for } g \\in \\{0,1\\}. $$\n    2.  **Budget Constraint**: The total expected sample size is fixed at $B > 0$:\n        $$ N_{0}\\big(a_{0} p_{0} + b_{0} (1-p_{0})\\big) + N_{1}\\big(a_{1} p_{1} + b_{1} (1-p_{1})\\big) = B. $$\n- **Estimator and Objective Function**:\n    - **Horvitz–Thompson (HT) estimator for $p_g$**:\n      $$ \\widehat{p}_{g} = \\frac{1}{N_{g}} \\sum_{i=1}^{N_{g}} \\frac{\\mathbf{1}\\{\\text{unit } i \\text{ is included}\\} \\cdot \\mathbf{1}\\{Y_{i}=1\\}}{a_{g}}. $$\n    - **Estimator for prevalence difference $\\delta = p_1 - p_0$**: $\\widehat{\\delta} = \\widehat{p}_{1} - \\widehat{p}_{0}$.\n    - **Objective**: Minimize the mean squared error (MSE) of $\\widehat{\\delta}$: $\\mathbb{E}\\big[(\\widehat{\\delta} - \\delta)^{2}\\big]$.\n- **Assumption**: The parameters $B$ and $\\tau$ are such that the optimal inclusion probabilities $a_{g}^{\\star}, b_{g}^{\\star}$ lie in the interval $(0, 1]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria:\n\n-   **Scientifically Grounded**: The problem is well-grounded in the fields of statistical learning and algorithmic fairness. It uses established concepts such as Horvitz–Thompson estimation for survey sampling, demographic parity as a fairness metric, and mean squared error as a standard measure of estimator accuracy. The setup is scientifically and mathematically sound.\n-   **Well-Posed**: The problem is a well-posed constrained optimization problem. It asks to minimize a well-defined objective function (MSE) subject to a set of equality constraints. The objective function will be shown to be a convex function of the optimization variables, and the constraints are linear in those variables, which ensures that a unique minimum exists under the given conditions.\n-   **Objective**: The problem is stated using precise, objective mathematical language. All terms are formally defined, and there are no subjective or opinion-based statements.\n-   **No Flaws**: The problem does not exhibit any of the listed invalidity flaws. It is not scientifically unsound, non-formalizable, incomplete, contradictory, unrealistic, or ill-posed. The assumption that the solution lies in $(0, 1]$ simplifies the analysis but does not render the problem trivial; it bypasses the need for explicit Karush-Kuhn-Tucker (KKT) analysis of the inequality constraints, focusing the challenge on the core optimization.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n## SOLUTION\n\nThe problem is to find the optimal inclusion probabilities $a_{0}^{\\star}, a_{1}^{\\star}, b_{0}^{\\star}, b_{1}^{\\star}$ that minimize the mean squared error (MSE) of the estimator $\\widehat{\\delta} = \\widehat{p}_1 - \\widehat{p}_0$, subject to demographic parity and budget constraints.\n\n### 1. Derive the Objective Function (MSE)\nThe mean squared error of an estimator $\\widehat{\\theta}$ for a parameter $\\theta$ is $\\mathrm{MSE}(\\widehat{\\theta}) = \\mathrm{Var}(\\widehat{\\theta}) + (\\mathbb{E}[\\widehat{\\theta}] - \\theta)^2$.\nFirst, we establish the unbiasedness of the estimator $\\widehat{p}_{g}$. Let $Z_{gi}$ be the indicator random variable that unit $i$ from group $g$ is included in the sample. Its expectation depends on the unit's (fixed) outcome $Y_{gi}$: $\\mathbb{E}[Z_{gi} | Y_{gi}=1] = a_g$ and $\\mathbb{E}[Z_{gi} | Y_{gi}=0] = b_g$.\nThe estimator for $p_g$ is $\\widehat{p}_{g} = \\frac{1}{N_{g}} \\sum_{i=1}^{N_{g}} \\frac{Z_{gi} \\cdot \\mathbf{1}\\{Y_{gi}=1\\}}{a_{g}}$.\nThe expectation of $\\widehat{p}_{g}$ is:\n$$ \\mathbb{E}[\\widehat{p}_{g}] = \\frac{1}{N_{g} a_{g}} \\sum_{i=1}^{N_{g}} \\mathbb{E}[Z_{gi} \\cdot \\mathbf{1}\\{Y_{gi}=1\\}] $$\nFor a unit $i$ with $Y_{gi}=0$, the term is $0$. For a unit $i$ with $Y_{gi}=1$, the term is $Z_{gi}$, and $\\mathbb{E}[Z_{gi}]=a_g$.\n$$ \\mathbb{E}[\\widehat{p}_{g}] = \\frac{1}{N_{g} a_{g}} \\sum_{i=1}^{N_{g}} \\mathbb{E}[Z_{gi}] \\mathbf{1}\\{Y_{gi}=1\\} = \\frac{1}{N_{g} a_{g}} \\sum_{i=1}^{N_{g}} a_g \\mathbf{1}\\{Y_{gi}=1\\} = \\frac{1}{N_{g}} \\sum_{i=1}^{N_{g}} \\mathbf{1}\\{Y_{gi}=1\\} $$\nThe sum $\\sum_{i=1}^{N_{g}} \\mathbf{1}\\{Y_{gi}=1\\}$ is the total number of positive instances in group $g$, which is $N_g p_g$. Thus,\n$$ \\mathbb{E}[\\widehat{p}_{g}] = \\frac{1}{N_{g}} (N_g p_g) = p_g $$\nThe estimator $\\widehat{p}_{g}$ is unbiased. Consequently, the estimator for the difference $\\widehat{\\delta} = \\widehat{p}_1 - \\widehat{p}_0$ is also unbiased, as $\\mathbb{E}[\\widehat{\\delta}] = \\mathbb{E}[\\widehat{p}_1] - \\mathbb{E}[\\widehat{p}_0] = p_1 - p_0 = \\delta$.\nThe MSE is therefore equal to the variance: $\\mathrm{MSE}(\\widehat{\\delta}) = \\mathrm{Var}(\\widehat{\\delta})$. Since the sampling is independent across the two groups, $\\mathrm{Var}(\\widehat{\\delta}) = \\mathrm{Var}(\\widehat{p}_1) + \\mathrm{Var}(\\widehat{p}_0)$.\n\nNext, we compute $\\mathrm{Var}(\\widehat{p}_{g})$. Since the units are sampled independently, the variance of the sum is the sum of the variances:\n$$ \\mathrm{Var}(\\widehat{p}_{g}) = \\mathrm{Var}\\left(\\frac{1}{N_{g}} \\sum_{i=1}^{N_{g}} \\frac{Z_{gi} \\mathbf{1}\\{Y_{gi}=1\\}}{a_{g}}\\right) = \\frac{1}{N_{g}^2 a_{g}^2} \\sum_{i=1}^{N_{g}} \\mathrm{Var}(Z_{gi} \\mathbf{1}\\{Y_{gi}=1\\}) $$\nFor a unit $i$ with $Y_{gi}=0$, the term $Z_{gi} \\mathbf{1}\\{Y_{gi}=1\\}$ is always $0$, so its variance is $0$.\nFor a unit $i$ with $Y_{gi}=1$, the term is $Z_{gi}$. Since $Y_{gi}=1$, $Z_{gi}$ is a Bernoulli random variable with success probability $a_g$. Its variance is $\\mathrm{Var}(Z_{gi}) = a_g(1-a_g)$.\nSo, $\\mathrm{Var}(Z_{gi} \\mathbf{1}\\{Y_{gi}=1\\}) = a_g(1-a_g) \\mathbf{1}\\{Y_{gi}=1\\}$.\nSumming over all units in group $g$:\n$$ \\sum_{i=1}^{N_{g}} \\mathrm{Var}(Z_{gi} \\mathbf{1}\\{Y_{gi}=1\\}) = \\sum_{i=1}^{N_{g}} a_g(1-a_g) \\mathbf{1}\\{Y_{gi}=1\\} = a_g(1-a_g) (N_g p_g) $$\nSubstituting this back into the variance expression for $\\widehat{p}_g$:\n$$ \\mathrm{Var}(\\widehat{p}_{g}) = \\frac{1}{N_{g}^2 a_{g}^2} [a_g(1-a_g) N_g p_g] = \\frac{p_g(1-a_g)}{N_g a_g} = \\frac{p_g}{N_g a_g} - \\frac{p_g}{N_g} $$\nThe objective function to minimize is the total MSE:\n$$ \\mathrm{MSE}(\\widehat{\\delta}) = \\mathrm{Var}(\\widehat{p}_{0}) + \\mathrm{Var}(\\widehat{p}_{1}) = \\frac{p_0(1-a_0)}{N_0 a_0} + \\frac{p_1(1-a_1)}{N_1 a_1} $$\nMinimizing this is equivalent to minimizing $\\frac{p_0}{N_0 a_0} + \\frac{p_1}{N_1 a_1}$, as the other terms are constants.\n\n### 2. Simplify the Constraints\nThe optimization variables are $a_0, a_1, b_0, b_1$. The objective function only depends on $a_0$ and $a_1$. We use the constraints to eliminate $b_0$ and $b_1$ and to constrain $a_0$ and $a_1$.\n\nFrom the demographic parity constraint for group $g$:\n$$ \\frac{a_{g} p_{g}}{a_{g} p_{g} + b_{g} (1-p_{g})} = \\tau $$\n$$ a_g p_g = \\tau \\big(a_g p_g + b_g(1-p_g)\\big) \\implies a_g p_g (1-\\tau) = \\tau b_g (1-p_g) $$\nThis allows us to express $b_g$ in terms of $a_g$:\n$$ b_g = a_g \\frac{p_g(1-\\tau)}{\\tau(1-p_g)} $$\nNow, we substitute this into the budget constraint. Let's first simplify the term for the expected number of samples from group $g$:\n$$ N_{g}\\big(a_{g} p_{g} + b_{g} (1-p_{g})\\big) = N_g \\frac{a_g p_g}{\\tau} $$\nThe budget constraint becomes:\n$$ N_{0} \\frac{a_0 p_0}{\\tau} + N_{1} \\frac{a_1 p_1}{\\tau} = B $$\nMultiplying by $\\tau$ gives a linear constraint on $a_0$ and $a_1$:\n$$ N_0 p_0 a_0 + N_1 p_1 a_1 = B \\tau $$\n\n### 3. Solve the Constrained Optimization Problem\nThe problem is now to minimize $f(a_0, a_1) = \\frac{p_0}{N_0 a_0} + \\frac{p_1}{N_1 a_1}$ subject to the constraint $N_0 p_0 a_0 + N_1 p_1 a_1 = B \\tau$.\nWe use the method of Lagrange multipliers. The Lagrangian is:\n$$ \\mathcal{L}(a_0, a_1, \\lambda) = \\frac{p_0}{N_0 a_0} + \\frac{p_1}{N_1 a_1} + \\lambda(N_0 p_0 a_0 + N_1 p_1 a_1 - B \\tau) $$\nTaking partial derivatives with respect to $a_0$ and $a_1$ and setting them to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial a_0} = -\\frac{p_0}{N_0 a_0^2} + \\lambda N_0 p_0 = 0 \\implies \\lambda = \\frac{1}{N_0^2 a_0^2} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial a_1} = -\\frac{p_1}{N_1 a_1^2} + \\lambda N_1 p_1 = 0 \\implies \\lambda = \\frac{1}{N_1^2 a_1^2} $$\nFrom these equations, we have $\\frac{1}{N_0^2 a_0^2} = \\frac{1}{N_1^2 a_1^2}$, which implies $N_0 a_0 = N_1 a_1$ (since $N_g, a_g > 0$).\nLet $N_0 a_0 = N_1 a_1 = K$ for some constant $K$. Then $a_0 = K/N_0$ and $a_1 = K/N_1$.\nSubstitute these into the budget constraint:\n$$ N_0 p_0 \\left(\\frac{K}{N_0}\\right) + N_1 p_1 \\left(\\frac{K}{N_1}\\right) = B \\tau $$\n$$ p_0 K + p_1 K = B \\tau \\implies K(p_0 + p_1) = B \\tau \\implies K = \\frac{B \\tau}{p_0 + p_1} $$\nNow we find the optimal values $a_0^{\\star}$ and $a_1^{\\star}$:\n$$ a_0^{\\star} = \\frac{K}{N_0} = \\frac{B \\tau}{N_0 (p_0 + p_1)} $$\n$$ a_1^{\\star} = \\frac{K}{N_1} = \\frac{B \\tau}{N_1 (p_0 + p_1)} $$\nThe problem statement guarantees these values are in $(0, 1]$.\n\n### 4. Determine Optimal $b_0^{\\star}$ and $b_1^{\\star}$\nUsing the relation $b_g = a_g \\frac{p_g(1-\\tau)}{\\tau(1-p_g)}$, we find the optimal values for $b_0$ and $b_1$:\n$$ b_0^{\\star} = a_0^{\\star} \\frac{p_0(1-\\tau)}{\\tau(1-p_0)} = \\frac{B \\tau}{N_0 (p_0 + p_1)} \\cdot \\frac{p_0(1-\\tau)}{\\tau(1-p_0)} = \\frac{B p_0 (1-\\tau)}{N_0 (1-p_0) (p_0 + p_1)} $$\n$$ b_1^{\\star} = a_1^{\\star} \\frac{p_1(1-\\tau)}{\\tau(1-p_1)} = \\frac{B \\tau}{N_1 (p_0 + p_1)} \\cdot \\frac{p_1(1-\\tau)}{\\tau(1-p_1)} = \\frac{B p_1 (1-\\tau)}{N_1 (1-p_1) (p_0 + p_1)} $$\nThese are the optimal inclusion probabilities that minimize the MSE of the prevalence difference estimator, subject to the fairness and budget constraints. The final expressions for the four parameters are:\n- $a_{0}^{\\star} = \\frac{B \\tau}{N_{0} (p_{0} + p_{1})}$\n- $a_{1}^{\\star} = \\frac{B \\tau}{N_{1} (p_{0} + p_{1})}$\n- $b_{0}^{\\star} = \\frac{B p_{0} (1 - \\tau)}{N_{0} (1 - p_{0}) (p_{0} + p_{1})}$\n- $b_{1}^{\\star} = \\frac{B p_{1} (1 - \\tau)}{N_{1} (1 - p_{1}) (p_{0} + p_{1})}$",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{B \\tau}{N_{0} (p_{0} + p_{1})} & \\frac{B \\tau}{N_{1} (p_{0} + p_{1})} & \\frac{B p_{0} (1 - \\tau)}{N_{0} (1 - p_{0}) (p_{0} + p_{1})} & \\frac{B p_{1} (1 - \\tau)}{N_{1} (1 - p_{1}) (p_{0} + p_{1})} \\end{pmatrix}} $$"
        },
        {
            "introduction": "Beyond modifying the data, we can embed fairness directly into the model's learning process. This exercise moves from pre-processing to in-processing, focusing on a regression setting where the goal is to ensure the model's prediction errors are balanced across different groups. You will mathematically derive how to adjust a standard learning algorithm, least squares, by assigning different weights to errors from each group . This hands-on derivation reveals the fundamental tension that can arise between overall predictive accuracy and fairness, especially when groups exhibit different underlying characteristics like noise levels ($\\sigma_a^2$), and demonstrates how weighted loss functions offer a principled tool to navigate this trade-off.",
            "id": "3098302",
            "problem": "A binary sensitive attribute $A \\in \\{0,1\\}$ partitions a population into two groups with probabilities $\\mathbb{P}(A=a)=p_{a}$, where $p_{0}, p_{1} \\in (0,1)$ and $p_{0}+p_{1}=1$. A single real-valued feature $X \\in \\mathbb{R}$ is distributed with $\\mathbb{E}[X \\mid A=a]=0$ and $\\operatorname{Var}(X \\mid A=a)=1$ for each $a \\in \\{0,1\\}$, and the outcome $Y \\in \\mathbb{R}$ follows the linear structural model $Y=\\beta_{A} X+\\varepsilon_{A}$, where $\\beta_{0}, \\beta_{1} \\in \\mathbb{R}$ are group-specific slopes, $\\mathbb{E}[\\varepsilon_{A} \\mid X, A]=0$, and $\\operatorname{Var}(\\varepsilon_{A} \\mid X, A=a)=\\sigma_{a}^{2}$ with $\\sigma_{0}^{2}, \\sigma_{1}^{2} > 0$. We fit a single shared linear predictor $\\hat{Y}_{\\theta}=\\theta X$ by minimizing the weighted squared loss $\\mathbb{E}[w(A)\\,(Y-\\theta X)^{2}]$ over $\\theta \\in \\mathbb{R}$, where $w(0)=w_{0}>0$ and $w(1)=w_{1}>0$ are fixed positive weights. To remove the scale indeterminacy of the weights, assume the normalization constraint $p_{0} w_{0}+p_{1} w_{1}=1$.\n\nDefine fairness in regression as equality of the group-conditional mean squared error: $\\mathbb{E}[(Y-\\hat{Y}_{\\theta})^{2} \\mid A=0]=\\mathbb{E}[(Y-\\hat{Y}_{\\theta})^{2} \\mid A=1]$. Starting from the core definitions of conditional expectation, mean squared error, and the first-order optimality condition for weighted least squares, derive the unique pair of normalized weights $(w_{0}, w_{1})$ that make the weighted least squares solution $\\theta$ satisfy the fairness criterion. Your derivation should explicitly connect how heteroscedastic noise (i.e., $\\sigma_{0}^{2} \\neq \\sigma_{1}^{2}$) creates an unavoidable trade-off between predictive accuracy and the fairness constraint when using a single shared predictor, and then show how an appropriate choice of weights remedies this by steering the fitted $\\theta$ to equalize the conditional mean squared errors.\n\nAssume $\\beta_{0} \\neq \\beta_{1}$ and that the parameters are such that the fairness-optimal shared slope lies strictly between the two group slopes, which guarantees positive weights under the stated normalization. Express your final answer as the closed-form analytic expressions for $w_{0}$ and $w_{1}$ in terms of $\\beta_{0}$, $\\beta_{1}$, $\\sigma_{0}^{2}$, $\\sigma_{1}^{2}$, $p_{0}$, and $p_{1}$. No numerical approximation is required, and no units are involved. Present your final answer as a single row matrix using the LaTeX $\\operatorname{pmatrix}$ environment.",
            "solution": "The problem requires the derivation of a unique pair of normalized weights, $(w_{0}, w_{1})$, that ensures a weighted least squares linear predictor satisfies a specific fairness criterion. The fairness criterion is defined as the equality of group-conditional mean squared errors for two groups, $A=0$ and $A=1$.\n\nThe derivation proceeds in four main steps:\n1.  Determine the optimal predictor slope $\\theta$ by minimizing the weighted squared loss.\n2.  Formulate the group-conditional mean squared error (MSE) for each group $a \\in \\{0,1\\}$.\n3.  Apply the fairness criterion (equality of group-conditional MSEs) to find the specific value of $\\theta$, denoted $\\theta_{F}$, that satisfies this constraint.\n4.  Equate the expression for $\\theta$ from step 1 with the value $\\theta_{F}$ from step 3, and solve the resulting system of equations for the weights $(w_{0}, w_{1})$ under the given normalization constraint.\n\n**Step 1: Weighted Least Squares Solution**\nThe objective is to find the slope $\\theta$ that minimizes the weighted squared loss function, $L(\\theta) = \\mathbb{E}[w(A)\\,(Y-\\theta X)^{2}]$. We can expand this expectation using the law of total expectation over the sensitive attribute $A$:\n$$L(\\theta) = \\sum_{a \\in \\{0,1\\}} \\mathbb{P}(A=a) \\mathbb{E}[w(A)\\,(Y-\\theta X)^{2} \\mid A=a]$$\nSubstituting the given probabilities $p_{a}$ and weights $w_{a}$:\n$$L(\\theta) = p_{0} w_{0} \\mathbb{E}[(Y-\\theta X)^{2} \\mid A=0] + p_{1} w_{1} \\mathbb{E}[(Y-\\theta X)^{2} \\mid A=1]$$\nFor a given group $A=a$, the structural model is $Y = \\beta_{a} X + \\varepsilon_{a}$. The error term within the expectation becomes:\n$$Y - \\theta X = (\\beta_{a} X + \\varepsilon_{a}) - \\theta X = (\\beta_{a} - \\theta)X + \\varepsilon_{a}$$\nThe squared error is $((\\beta_{a} - \\theta)X + \\varepsilon_{a})^{2} = (\\beta_{a}-\\theta)^{2}X^{2} + 2(\\beta_{a}-\\theta)X\\varepsilon_{a} + \\varepsilon_{a}^{2}$. We compute its conditional expectation:\n$$\\mathbb{E}[((\\beta_{a} - \\theta)X + \\varepsilon_{a})^{2} \\mid A=a] = (\\beta_{a}-\\theta)^{2}\\mathbb{E}[X^{2} \\mid A=a] + 2(\\beta_{a}-\\theta)\\mathbb{E}[X\\varepsilon_{a} \\mid A=a] + \\mathbb{E}[\\varepsilon_{a}^{2} \\mid A=a]$$\nUsing the provided conditions:\n- $\\mathbb{E}[X \\mid A=a]=0$ and $\\operatorname{Var}(X \\mid A=a)=1$, which implies $\\mathbb{E}[X^{2} \\mid A=a] = \\operatorname{Var}(X \\mid A=a) + (\\mathbb{E}[X \\mid A=a])^{2} = 1 + 0^{2} = 1$.\n- $\\mathbb{E}[\\varepsilon_{A} \\mid X, A]=0$. By the law of iterated expectations, $\\mathbb{E}[X\\varepsilon_{a} \\mid A=a] = \\mathbb{E}[\\mathbb{E}[X\\varepsilon_{a} \\mid X, A=a] \\mid A=a] = \\mathbb{E}[X \\mathbb{E}[\\varepsilon_{a} \\mid X, A=a] \\mid A=a] = \\mathbb{E}[X \\cdot 0 \\mid A=a] = 0$.\n- $\\operatorname{Var}(\\varepsilon_{A} \\mid X, A=a)=\\sigma_{a}^{2}$ and $\\mathbb{E}[\\varepsilon_{A} \\mid X, A=a] = 0$. This implies $\\mathbb{E}[\\varepsilon_{a}^{2} \\mid X, A=a] = \\sigma_{a}^{2}$. By iterated expectations, $\\mathbb{E}[\\varepsilon_{a}^{2} \\mid A=a] = \\mathbb{E}[\\mathbb{E}[\\varepsilon_{a}^{2} \\mid X, A=a] \\mid A=a] = \\mathbb{E}[\\sigma_{a}^{2} \\mid A=a] = \\sigma_{a}^{2}$.\n\nSubstituting these results back, the conditional expectation of the squared error for group $a$ is $(\\beta_{a} - \\theta)^{2} + \\sigma_{a}^{2}$. The loss function becomes:\n$$L(\\theta) = p_{0} w_{0} [(\\beta_{0} - \\theta)^{2} + \\sigma_{0}^{2}] + p_{1} w_{1} [(\\beta_{1} - \\theta)^{2} + \\sigma_{1}^{2}]$$\nTo find the minimum, we compute the derivative with respect to $\\theta$ and set it to zero (first-order optimality condition):\n$$\\frac{dL}{d\\theta} = p_{0} w_{0} [2(\\beta_{0} - \\theta)(-1)] + p_{1} w_{1} [2(\\beta_{1} - \\theta)(-1)] = 0$$\n$$-2 [p_{0} w_{0} (\\beta_{0} - \\theta) + p_{1} w_{1} (\\beta_{1} - \\theta)] = 0$$\n$$(p_{0} w_{0} + p_{1} w_{1})\\theta = p_{0} w_{0} \\beta_{0} + p_{1} w_{1} \\beta_{1}$$\nApplying the normalization constraint $p_{0} w_{0} + p_{1} w_{1} = 1$, we find the weighted least squares solution for $\\theta$:\n$$\\theta = p_{0} w_{0} \\beta_{0} + p_{1} w_{1} \\beta_{1}$$\n\n**Step 2: Group-Conditional Mean Squared Error**\nThe mean squared error for group $a$, $\\text{MSE}_{a}$, is defined as $\\mathbb{E}[(Y-\\hat{Y}_{\\theta})^{2} \\mid A=a]$. With $\\hat{Y}_{\\theta}=\\theta X$, this is precisely the conditional expectation we calculated in Step 1:\n$$\\text{MSE}_{a} = \\mathbb{E}[(Y - \\theta X)^{2} \\mid A=a] = (\\beta_{a} - \\theta)^{2} + \\sigma_{a}^{2}$$\nThis expression shows that the MSE for a group has two components: a bias term $(\\beta_{a}-\\theta)^{2}$ which measures the squared difference between the group-optimal slope $\\beta_a$ and the shared slope $\\theta$, and a variance term $\\sigma_{a}^{2}$ from the irreducible noise.\n\n**Step 3: The Fairness Constraint**\nThe fairness criterion requires $\\text{MSE}_{0} = \\text{MSE}_{1}$. Let $\\theta_{F}$ be the value of $\\theta$ that satisfies this criterion.\n$$(\\beta_{0} - \\theta_{F})^{2} + \\sigma_{0}^{2} = (\\beta_{1} - \\theta_{F})^{2} + \\sigma_{1}^{2}$$\nExpanding the squared terms:\n$$\\beta_{0}^{2} - 2\\beta_{0}\\theta_{F} + \\theta_{F}^{2} + \\sigma_{0}^{2} = \\beta_{1}^{2} - 2\\beta_{1}\\theta_{F} + \\theta_{F}^{2} + \\sigma_{1}^{2}$$\n$$2\\beta_{1}\\theta_{F} - 2\\beta_{0}\\theta_{F} = \\beta_{1}^{2} - \\beta_{0}^{2} + \\sigma_{1}^{2} - \\sigma_{0}^{2}$$\n$$2\\theta_{F}(\\beta_{1} - \\beta_{0}) = (\\beta_{1} - \\beta_{0})(\\beta_{1} + \\beta_{0}) + (\\sigma_{1}^{2} - \\sigma_{0}^{2})$$\nSince $\\beta_{0} \\neq \\beta_{1}$, we can divide by $2(\\beta_{1} - \\beta_{0})$:\n$$\\theta_{F} = \\frac{\\beta_{0} + \\beta_{1}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})}$$\nThis is the unique slope that equalizes the MSE between the two groups. If the noise is homoscedastic ($\\sigma_{0}^{2} = \\sigma_{1}^{2}$), then $\\theta_F$ is simply the average of the two group-specific slopes. However, with heteroscedastic noise ($\\sigma_{0}^{2} \\neq \\sigma_{1}^{2}$), $\\theta_F$ is shifted away from this average to compensate. For example, if group 1 is noisier ($\\sigma_{1}^{2} > \\sigma_{0}^{2}$) and has a higher slope ($\\beta_{1} > \\beta_{0}$), $\\theta_F$ must be larger than the average to decrease the bias term for group 1 (by bringing $\\theta_F$ closer to $\\beta_1$) and increase it for group 0, balancing the overall MSE. This illustrates the trade-off: achieving fairness requires choosing a slope $\\theta_F$ that is not necessarily optimal for either group individually, nor for the population as a whole (which would typically be a weighted average like $p_0\\beta_0+p_1\\beta_1$), in order to balance errors across groups. The weights $(w_0, w_1)$ are the mechanism to steer the WLS optimization toward this specific fairness-driven target $\\theta_F$.\n\n**Step 4: Solving for the Weights**\nWe now have a system of two linear equations for $w_{0}$ and $w_{1}$:\n1.  $\\theta_{F} = p_{0} w_{0} \\beta_{0} + p_{1} w_{1} \\beta_{1}$\n2.  $1 = p_{0} w_{0} + p_{1} w_{1}$\n\nFrom equation (2), we can write $p_{1} w_{1} = 1 - p_{0} w_{0}$. Substituting this into equation (1):\n$$\\theta_{F} = p_{0} w_{0} \\beta_{0} + (1 - p_{0} w_{0})\\beta_{1} = p_{0} w_{0} \\beta_{0} + \\beta_{1} - p_{0} w_{0} \\beta_{1}$$\n$$\\theta_{F} - \\beta_{1} = p_{0} w_{0} (\\beta_{0} - \\beta_{1})$$\n$$w_{0} = \\frac{\\theta_{F} - \\beta_{1}}{p_{0}(\\beta_{0} - \\beta_{1})} = \\frac{\\beta_{1} - \\theta_{F}}{p_{0}(\\beta_{1} - \\beta_{0})}$$\nSubstitute the expression for $\\theta_{F}$:\n$$w_{0} = \\frac{1}{p_{0}(\\beta_{1} - \\beta_{0})} \\left[ \\beta_{1} - \\left(\\frac{\\beta_{0} + \\beta_{1}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})}\\right) \\right]$$\n$$w_{0} = \\frac{1}{p_{0}(\\beta_{1} - \\beta_{0})} \\left[ \\frac{2\\beta_{1} - (\\beta_{0} + \\beta_{1})}{2} - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})} \\right]$$\n$$w_{0} = \\frac{1}{p_{0}(\\beta_{1} - \\beta_{0})} \\left[ \\frac{\\beta_{1} - \\beta_{0}}{2} - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})} \\right]$$\n$$w_{0} = \\frac{1}{2p_{0}} \\left( 1 - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right)$$\n\nSimilarly, we can solve for $w_{1}$ by substituting $p_{0}w_{0} = 1 - p_{1}w_{1}$ into equation (1):\n$$\\theta_{F} = (1 - p_{1} w_{1})\\beta_{0} + p_{1} w_{1} \\beta_{1} = \\beta_{0} + p_{1} w_{1}(\\beta_{1} - \\beta_{0})$$\n$$w_{1} = \\frac{\\theta_{F} - \\beta_{0}}{p_{1}(\\beta_{1} - \\beta_{0})}$$\nSubstitute the expression for $\\theta_{F}$:\n$$w_{1} = \\frac{1}{p_{1}(\\beta_{1} - \\beta_{0})} \\left[ \\left(\\frac{\\beta_{0} + \\beta_{1}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})}\\right) - \\beta_{0} \\right]$$\n$$w_{1} = \\frac{1}{p_{1}(\\beta_{1} - \\beta_{0})} \\left[ \\frac{\\beta_{1} - \\beta_{0}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})} \\right]$$\n$$w_{1} = \\frac{1}{2p_{1}} \\left( 1 + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right)$$\n\nThese are the required unique normalized weights. The assumption that $\\theta_F$ lies strictly between $\\beta_0$ and $\\beta_1$ ensures that the term $1 \\pm \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}}$ is positive, and thus $w_0, w_1 > 0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2p_{0}} \\left( 1 - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right) & \\frac{1}{2p_{1}} \\left( 1 + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A model that achieves fairness on its training data may not remain fair when deployed in the real world, a critical challenge in trustworthy AI. This problem tackles the crucial topic of auditing and the brittleness of fairness guarantees under distributional shifts, a phenomenon known as covariate shift where feature distributions $P(X \\mid A)$ differ between training and testing. Through a concrete numerical example, you will first verify how a classifier satisfying equalized odds on a training set can violate it on a test set . You will then implement a powerful statistical technique, importance-weighted auditing, to accurately estimate the model's real-world fairness performance using only the training data and information about the shift, highlighting a key skill for robustly evaluating algorithmic systems.",
            "id": "3098292",
            "problem": "Consider binary classification under Empirical Risk Minimization (ERM), with a binary sensitive attribute $A \\in \\{0,1\\}$, a binary feature $X \\in \\{0,1\\}$, and a binary label $Y \\in \\{0,1\\}$. The classifier is defined by $\\hat{Y}(X) = \\mathbf{1}\\{X=1\\}$ and is trained to minimize empirical $0$-$1$ loss. Let the training distribution be imbalanced across groups, with $P_{\\text{train}}(A=0) = 0.9$ and $P_{\\text{train}}(A=1) = 0.1$. Assume the label conditional is stable across domains and groups (covariate shift only), specified by $P(Y=1 \\mid X=1) = \\frac{3}{4}$ and $P(Y=1 \\mid X=0) = \\frac{1}{4}$, so that $P(Y=0 \\mid X=1) = \\frac{1}{4}$ and $P(Y=0 \\mid X=0) = \\frac{3}{4}$ for both training and test. On the training distribution, the covariate conditionals are group-balanced: $P_{\\text{train}}(X=1 \\mid A=0) = P_{\\text{train}}(X=1 \\mid A=1) = \\frac{1}{2}$. On the test distribution, there is group-specific covariate shift: $P_{\\text{test}}(X=1 \\mid A=0) = \\frac{3}{4}$ and $P_{\\text{test}}(X=1 \\mid A=1) = \\frac{1}{4}$. \n\nWe audit the fairness notion of equalized odds on the false positive rate (FPR), where for group $a \\in \\{0,1\\}$ the false positive rate is defined by $\\text{FPR}_{a} = P(\\hat{Y}=1 \\mid Y=0, A=a)$. First, reason from the definitions to verify that training equalized odds (equality of $\\text{FPR}_{0}$ and $\\text{FPR}_{1}$) holds, and that test equalized odds fails. Then, propose importance-weighted auditing under covariate shift by defining $w_{a}(x) = \\frac{P_{\\text{test}}(X=x \\mid A=a)}{P_{\\text{train}}(X=x \\mid A=a)}$, and using these weights to rewrite the test-domain FPR as a weighted conditional expectation on the training domain.\n\nCompute, explicitly from the given numbers, the importance-weighted false positive rate gap $\\text{FPR}_{0}^{\\text{iw}} - \\text{FPR}_{1}^{\\text{iw}}$ obtained by auditing on the training distribution with the weights $w_{a}(x)$. Express your final answer as a decimal or a fraction (no percent sign). No rounding is required.",
            "solution": "The problem will first be validated for correctness and solvability.\n\n### Step 1: Extract Givens\n- **Problem Type**: Binary classification.\n- **Sensitive Attribute**: $A \\in \\{0,1\\}$.\n- **Feature**: $X \\in \\{0,1\\}$.\n- **Label**: $Y \\in \\{0,1\\}$.\n- **Classifier**: $\\hat{Y}(X) = \\mathbf{1}\\{X=1\\}$.\n- **Training Group Distribution**: $P_{\\text{train}}(A=0) = 0.9$, $P_{\\text{train}}(A=1) = 0.1$.\n- **Stable Label Conditional**: $P(Y=1 \\mid X=1) = \\frac{3}{4}$ and $P(Y=1 \\mid X=0) = \\frac{1}{4}$. This implies $P(Y=0 \\mid X=1) = 1 - \\frac{3}{4} = \\frac{1}{4}$ and $P(Y=0 \\mid X=0) = 1 - \\frac{1}{4} = \\frac{3}{4}$. This conditional distribution is constant across domains and groups.\n- **Training Covariate Conditionals**: $P_{\\text{train}}(X=1 \\mid A=0) = \\frac{1}{2}$ and $P_{\\text{train}}(X=1 \\mid A=1) = \\frac{1}{2}$. This implies $P_{\\text{train}}(X=0 \\mid A=0) = \\frac{1}{2}$ and $P_{\\text{train}}(X=0 \\mid A=1) = \\frac{1}{2}$.\n- **Test Covariate Conditionals**: $P_{\\text{test}}(X=1 \\mid A=0) = \\frac{3}{4}$ and $P_{\\text{test}}(X=1 \\mid A=1) = \\frac{1}{4}$. This implies $P_{\\text{test}}(X=0 \\mid A=0) = \\frac{1}{4}$ and $P_{\\text{test}}(X=0 \\mid A=1) = \\frac{3}{4}$.\n- **Fairness Metric**: Equalized odds on the false positive rate (FPR).\n- **FPR Definition**: $\\text{FPR}_{a} = P(\\hat{Y}=1 \\mid Y=0, A=a)$.\n- **Importance Weights Definition**: $w_{a}(x) = \\frac{P_{\\text{test}}(X=x \\mid A=a)}{P_{\\text{train}}(X=x \\mid A=a)}$.\n- **Goal**: Verify training/test equalized odds, and then compute the importance-weighted FPR gap $\\text{FPR}_{0}^{\\text{iw}} - \\text{FPR}_{1}^{\\text{iw}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded within the field of statistical learning and algorithmic fairness. It uses standard definitions and concepts such as Empirical Risk Minimization (ERM), covariate shift, and equalized odds. The setup is entirely self-contained, providing all necessary probability distributions. The language is precise and objective. There are no contradictions, ambiguities, or unrealistic assumptions. The problem is well-posed and leads to a unique, stable solution.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe problem requires an analysis of the false positive rate (FPR) of a classifier under training and test distributions characterized by covariate shift. The classifier is given by $\\hat{Y}(X) = \\mathbf{1}\\{X=1\\}$, meaning it predicts a positive outcome ($\\hat{Y}=1$) if and only if the feature $X$ is $1$.\n\nThe false positive rate for a group $a \\in \\{0, 1\\}$ is defined as $\\text{FPR}_{a} = P(\\hat{Y}=1 \\mid Y=0, A=a)$. Given the classifier's definition, this is equivalent to $\\text{FPR}_{a} = P(X=1 \\mid Y=0, A=a)$. We will calculate this quantity for both the training and test distributions.\n\nUsing Bayes' theorem, we can express the FPR as:\n$$ \\text{FPR}_{a} = \\frac{P(Y=0 \\mid X=1, A=a) P(X=1 \\mid A=a)}{P(Y=0 \\mid A=a)} $$\nThe problem states that the label conditional is stable, i.e., $P(Y \\mid X, A) = P(Y \\mid X)$. Thus, the formula simplifies to:\n$$ \\text{FPR}_{a} = \\frac{P(Y=0 \\mid X=1) P(X=1 \\mid A=a)}{P(Y=0 \\mid A=a)} $$\nThe denominator, $P(Y=0 \\mid A=a)$, must be computed for each distribution (training and test) using the law of total probability:\n$$ P(Y=0 \\mid A=a) = \\sum_{x \\in \\{0,1\\}} P(Y=0 \\mid X=x, A=a) P(X=x \\mid A=a) $$\n$$ P_{\\text{dist}}(Y=0 \\mid A=a) = P(Y=0 \\mid X=0)P_{\\text{dist}}(X=0 \\mid A=a) + P(Y=0 \\mid X=1)P_{\\text{dist}}(X=1 \\mid A=a) $$\nwhere $P(Y=0 \\mid X=1) = \\frac{1}{4}$ and $P(Y=0 \\mid X=0) = \\frac{3}{4}$.\n\n**Part 1: Verification of Training and Test Equalized Odds**\n\n**Training Distribution:**\nFirst, we compute $P_{\\text{train}}(Y=0 \\mid A=a)$:\nFor $A=0$: $P_{\\text{train}}(Y=0 \\mid A=0) = (\\frac{3}{4})P_{\\text{train}}(X=0 \\mid A=0) + (\\frac{1}{4})P_{\\text{train}}(X=1 \\mid A=0) = (\\frac{3}{4})(\\frac{1}{2}) + (\\frac{1}{4})(\\frac{1}{2}) = \\frac{3}{8} + \\frac{1}{8} = \\frac{4}{8} = \\frac{1}{2}$.\nFor $A=1$: $P_{\\text{train}}(Y=0 \\mid A=1) = (\\frac{3}{4})P_{\\text{train}}(X=0 \\mid A=1) + (\\frac{1}{4})P_{\\text{train}}(X=1 \\mid A=1) = (\\frac{3}{4})(\\frac{1}{2}) + (\\frac{1}{4})(\\frac{1}{2}) = \\frac{1}{2}$.\n\nNow, we compute the training FPRs:\n$\\text{FPR}_{0}^{\\text{train}} = \\frac{P(Y=0 \\mid X=1) P_{\\text{train}}(X=1 \\mid A=0)}{P_{\\text{train}}(Y=0 \\mid A=0)} = \\frac{(\\frac{1}{4})(\\frac{1}{2})}{\\frac{1}{2}} = \\frac{1}{4}$.\n$\\text{FPR}_{1}^{\\text{train}} = \\frac{P(Y=0 \\mid X=1) P_{\\text{train}}(X=1 \\mid A=1)}{P_{\\text{train}}(Y=0 \\mid A=1)} = \\frac{(\\frac{1}{4})(\\frac{1}{2})}{\\frac{1}{2}} = \\frac{1}{4}$.\nSince $\\text{FPR}_{0}^{\\text{train}} = \\text{FPR}_{1}^{\\text{train}}$, equalized odds on the FPR holds on the training distribution.\n\n**Test Distribution:**\nFirst, we compute $P_{\\text{test}}(Y=0 \\mid A=a)$:\nFor $A=0$: $P_{\\text{test}}(Y=0 \\mid A=0) = (\\frac{3}{4})P_{\\text{test}}(X=0 \\mid A=0) + (\\frac{1}{4})P_{\\text{test}}(X=1 \\mid A=0) = (\\frac{3}{4})(\\frac{1}{4}) + (\\frac{1}{4})(\\frac{3}{4}) = \\frac{3}{16} + \\frac{3}{16} = \\frac{6}{16} = \\frac{3}{8}$.\nFor $A=1$: $P_{\\text{test}}(Y=0 \\mid A=1) = (\\frac{3}{4})P_{\\text{test}}(X=0 \\mid A=1) + (\\frac{1}{4})P_{\\text{test}}(X=1 \\mid A=1) = (\\frac{3}{4})(\\frac{3}{4}) + (\\frac{1}{4})(\\frac{1}{4}) = \\frac{9}{16} + \\frac{1}{16} = \\frac{10}{16} = \\frac{5}{8}$.\n\nNow, we compute the test FPRs:\n$\\text{FPR}_{0}^{\\text{test}} = \\frac{P(Y=0 \\mid X=1) P_{\\text{test}}(X=1 \\mid A=0)}{P_{\\text{test}}(Y=0 \\mid A=0)} = \\frac{(\\frac{1}{4})(\\frac{3}{4})}{\\frac{3}{8}} = \\frac{\\frac{3}{16}}{\\frac{3}{8}} = \\frac{1}{2}$.\n$\\text{FPR}_{1}^{\\text{test}} = \\frac{P(Y=0 \\mid X=1) P_{\\text{test}}(X=1 \\mid A=1)}{P_{\\text{test}}(Y=0 \\mid A=1)} = \\frac{(\\frac{1}{4})(\\frac{1}{4})}{\\frac{5}{8}} = \\frac{\\frac{1}{16}}{\\frac{5}{8}} = \\frac{1}{10}$.\nSince $\\text{FPR}_{0}^{\\text{test}} \\neq \\text{FPR}_{1}^{\\text{test}}$, equalized odds on the FPR fails on the test distribution.\n\n**Part 2: Importance-Weighted Auditing**\n\nThe goal of importance-weighted auditing is to estimate a test-domain quantity using the training-domain distribution. The importance-weighted FPR, $\\text{FPR}_{a}^{\\text{iw}}$, is an estimator for the test FPR, $\\text{FPR}_{a}^{\\text{test}}$. With full knowledge of the distributions, this theoretical quantity is exactly equal to $\\text{FPR}_{a}^{\\text{test}}$. We will demonstrate this through explicit computation.\n\nThe test FPR can be written as a ratio of expectations:\n$$ \\text{FPR}_{a}^{\\text{test}} = P_{\\text{test}}(\\hat{Y}=1 \\mid Y=0, A=a) = \\frac{P_{\\text{test}}(\\hat{Y}=1, Y=0 \\mid A=a)}{P_{\\text{test}}(Y=0 \\mid A=a)} = \\frac{E_{\\text{test}}[\\hat{Y}(X) \\mathbf{1}\\{Y=0\\} \\mid A=a]}{E_{\\text{test}}[\\mathbf{1}\\{Y=0\\} \\mid A=a]} $$\nUnder covariate shift, we can re-express these test-domain expectations as weighted training-domain expectations using the importance weights $w_{a}(x) = \\frac{P_{\\text{test}}(X=x \\mid A=a)}{P_{\\text{train}}(X=x \\mid A=a)}$. This gives the formula for the importance-weighted FPR:\n$$ \\text{FPR}_{a}^{\\text{iw}} = \\frac{E_{\\text{train}}[\\hat{Y}(X) \\mathbf{1}\\{Y=0\\} w_a(X) \\mid A=a]}{E_{\\text{train}}[\\mathbf{1}\\{Y=0\\} w_a(X) \\mid A=a]} $$\nFirst, we compute the weights:\nFor $A=0$: $w_0(1) = \\frac{P_{\\text{test}}(X=1 \\mid A=0)}{P_{\\text{train}}(X=1 \\mid A=0)} = \\frac{3/4}{1/2} = \\frac{3}{2}$. $w_0(0) = \\frac{P_{\\text{test}}(X=0 \\mid A=0)}{P_{\\text{train}}(X=0 \\mid A=0)} = \\frac{1/4}{1/2} = \\frac{1}{2}$.\nFor $A=1$: $w_1(1) = \\frac{P_{\\text{test}}(X=1 \\mid A=1)}{P_{\\text{train}}(X=1 \\mid A=1)} = \\frac{1/4}{1/2} = \\frac{1}{2}$. $w_1(0) = \\frac{P_{\\text{test}}(X=0 \\mid A=1)}{P_{\\text{train}}(X=0 \\mid A=1)} = \\frac{3/4}{1/2} = \\frac{3}{2}$.\n\nNow we compute the numerator and denominator for each group $a$. The expectation $E_{\\text{train}}[\\cdot \\mid A=a]$ is over the joint distribution $P_{\\text{train}}(X, Y \\mid A=a) = P(Y \\mid X) P_{\\text{train}}(X \\mid A=a)$.\n\n**For group $A=0$**:\nThe numerator is $N_0 = E_{\\text{train}}[\\mathbf{1}\\{X=1\\} \\mathbf{1}\\{Y=0\\} w_0(X) \\mid A=0]$. The only non-zero term is for $X=1, Y=0$.\n$N_0 = w_0(1) P_{\\text{train}}(X=1, Y=0 \\mid A=0) = w_0(1) P(Y=0 \\mid X=1) P_{\\text{train}}(X=1 \\mid A=0) = (\\frac{3}{2}) (\\frac{1}{4}) (\\frac{1}{2}) = \\frac{3}{16}$.\nThe denominator is $D_0 = E_{\\text{train}}[\\mathbf{1}\\{Y=0\\} w_0(X) \\mid A=0]$.\n$D_0 = w_0(0) P_{\\text{train}}(X=0, Y=0 \\mid A=0) + w_0(1) P_{\\text{train}}(X=1, Y=0 \\mid A=0)$\n$D_0 = w_0(0) P(Y=0 \\mid X=0) P_{\\text{train}}(X=0 \\mid A=0) + N_0 = (\\frac{1}{2}) (\\frac{3}{4}) (\\frac{1}{2}) + \\frac{3}{16} = \\frac{3}{16} + \\frac{3}{16} = \\frac{6}{16} = \\frac{3}{8}$.\nSo, $\\text{FPR}_{0}^{\\text{iw}} = \\frac{N_0}{D_0} = \\frac{3/16}{3/8} = \\frac{1}{2}$.\n\n**For group $A=1$**:\nThe numerator is $N_1 = E_{\\text{train}}[\\mathbf{1}\\{X=1\\} \\mathbf{1}\\{Y=0\\} w_1(X) \\mid A=1]$.\n$N_1 = w_1(1) P_{\\text{train}}(X=1, Y=0 \\mid A=1) = w_1(1) P(Y=0 \\mid X=1) P_{\\text{train}}(X=1 \\mid A=1) = (\\frac{1}{2}) (\\frac{1}{4}) (\\frac{1}{2}) = \\frac{1}{16}$.\nThe denominator is $D_1 = E_{\\text{train}}[\\mathbf{1}\\{Y=0\\} w_1(X) \\mid A=1]$.\n$D_1 = w_1(0) P_{\\text{train}}(X=0, Y=0 \\mid A=1) + w_1(1) P_{\\text{train}}(X=1, Y=0 \\mid A=1)$\n$D_1 = w_1(0) P(Y=0 \\mid X=0) P_{\\text{train}}(X=0 \\mid A=1) + N_1 = (\\frac{3}{2}) (\\frac{3}{4}) (\\frac{1}{2}) + \\frac{1}{16} = \\frac{9}{16} + \\frac{1}{16} = \\frac{10}{16} = \\frac{5}{8}$.\nSo, $\\text{FPR}_{1}^{\\text{iw}} = \\frac{N_1}{D_1} = \\frac{1/16}{5/8} = \\frac{1}{10}$.\n\nAs expected, $\\text{FPR}_{a}^{\\text{iw}} = \\text{FPR}_{a}^{\\text{test}}$ for both groups. The importance-weighted false positive rate gap is:\n$$ \\text{FPR}_{0}^{\\text{iw}} - \\text{FPR}_{1}^{\\text{iw}} = \\frac{1}{2} - \\frac{1}{10} = \\frac{5}{10} - \\frac{1}{10} = \\frac{4}{10} = \\frac{2}{5} $$\nAs a decimal, this is $0.4$.",
            "answer": "$$ \\boxed{\\frac{2}{5}} $$"
        }
    ]
}