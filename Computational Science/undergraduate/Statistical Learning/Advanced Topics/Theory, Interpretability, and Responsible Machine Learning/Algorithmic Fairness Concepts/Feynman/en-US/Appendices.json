{
    "hands_on_practices": [
        {
            "introduction": "Our exploration into the hands-on practice of algorithmic fairness begins with a fundamental question: how can we mathematically encode a fairness objective directly into a model's training process? This exercise guides you through deriving a solution for fair linear regression. You will discover how adjusting the learning objective with specific weights can steer a model to satisfy the fairness criterion of equal mean squared error across groups, revealing the inherent trade-offs that arise from differing data characteristics like heteroscedastic noise .",
            "id": "3098302",
            "problem": "A binary sensitive attribute $A \\in \\{0,1\\}$ partitions a population into two groups with probabilities $\\mathbb{P}(A=a)=p_{a}$, where $p_{0}, p_{1} \\in (0,1)$ and $p_{0}+p_{1}=1$. A single real-valued feature $X \\in \\mathbb{R}$ is distributed with $\\mathbb{E}[X \\mid A=a]=0$ and $\\operatorname{Var}(X \\mid A=a)=1$ for each $a \\in \\{0,1\\}$, and the outcome $Y \\in \\mathbb{R}$ follows the linear structural model $Y=\\beta_{A} X+\\varepsilon_{A}$, where $\\beta_{0}, \\beta_{1} \\in \\mathbb{R}$ are group-specific slopes, $\\mathbb{E}[\\varepsilon_{A} \\mid X, A]=0$, and $\\operatorname{Var}(\\varepsilon_{A} \\mid X, A=a)=\\sigma_{a}^{2}$ with $\\sigma_{0}^{2}, \\sigma_{1}^{2} > 0$. We fit a single shared linear predictor $\\hat{Y}_{\\theta}=\\theta X$ by minimizing the weighted squared loss $\\mathbb{E}[w(A)\\,(Y-\\theta X)^{2}]$ over $\\theta \\in \\mathbb{R}$, where $w(0)=w_{0}>0$ and $w(1)=w_{1}>0$ are fixed positive weights. To remove the scale indeterminacy of the weights, assume the normalization constraint $p_{0} w_{0}+p_{1} w_{1}=1$.\n\nDefine fairness in regression as equality of the group-conditional mean squared error: $\\mathbb{E}[(Y-\\hat{Y}_{\\theta})^{2} \\mid A=0]=\\mathbb{E}[(Y-\\hat{Y}_{\\theta})^{2} \\mid A=1]$. Starting from the core definitions of conditional expectation, mean squared error, and the first-order optimality condition for weighted least squares, derive the unique pair of normalized weights $(w_{0}, w_{1})$ that make the weighted least squares solution $\\theta$ satisfy the fairness criterion. Your derivation should explicitly connect how heteroscedastic noise (i.e., $\\sigma_{0}^{2} \\neq \\sigma_{1}^{2}$) creates an unavoidable trade-off between predictive accuracy and the fairness constraint when using a single shared predictor, and then show how an appropriate choice of weights remedies this by steering the fitted $\\theta$ to equalize the conditional mean squared errors.\n\nAssume $\\beta_{0} \\neq \\beta_{1}$ and that the parameters are such that the fairness-optimal shared slope lies strictly between the two group slopes, which guarantees positive weights under the stated normalization. Express your final answer as the closed-form analytic expressions for $w_{0}$ and $w_{1}$ in terms of $\\beta_{0}$, $\\beta_{1}$, $\\sigma_{0}^{2}$, $\\sigma_{1}^{2}$, $p_{0}$, and $p_{1}$. No numerical approximation is required, and no units are involved. Present your final answer as a single row matrix using the LaTeX $\\operatorname{pmatrix}$ environment.",
            "solution": "The problem requires the derivation of a unique pair of normalized weights, $(w_{0}, w_{1})$, that ensures a weighted least squares linear predictor satisfies a specific fairness criterion. The fairness criterion is defined as the equality of group-conditional mean squared errors for two groups, $A=0$ and $A=1$.\n\nThe derivation proceeds in four main steps:\n1.  Determine the optimal predictor slope $\\theta$ by minimizing the weighted squared loss.\n2.  Formulate the group-conditional mean squared error (MSE) for each group $a \\in \\{0,1\\}$.\n3.  Apply the fairness criterion (equality of group-conditional MSEs) to find the specific value of $\\theta$, denoted $\\theta_{F}$, that satisfies this constraint.\n4.  Equate the expression for $\\theta$ from step 1 with the value $\\theta_{F}$ from step 3, and solve the resulting system of equations for the weights $(w_{0}, w_{1})$ under the given normalization constraint.\n\n**Step 1: Weighted Least Squares Solution**\nThe objective is to find the slope $\\theta$ that minimizes the weighted squared loss function, $L(\\theta) = \\mathbb{E}[w(A)\\,(Y-\\theta X)^{2}]$. We can expand this expectation using the law of total expectation over the sensitive attribute $A$:\n$$L(\\theta) = \\sum_{a \\in \\{0,1\\}} \\mathbb{P}(A=a) \\mathbb{E}[w(A)\\,(Y-\\theta X)^{2} \\mid A=a]$$\nSubstituting the given probabilities $p_{a}$ and weights $w_{a}$:\n$$L(\\theta) = p_{0} w_{0} \\mathbb{E}[(Y-\\theta X)^{2} \\mid A=0] + p_{1} w_{1} \\mathbb{E}[(Y-\\theta X)^{2} \\mid A=1]$$\nFor a given group $A=a$, the structural model is $Y = \\beta_{a} X + \\varepsilon_{a}$. The error term within the expectation becomes:\n$$Y - \\theta X = (\\beta_{a} X + \\varepsilon_{a}) - \\theta X = (\\beta_{a} - \\theta)X + \\varepsilon_{a}$$\nThe squared error is $((\\beta_{a} - \\theta)X + \\varepsilon_{a})^{2} = (\\beta_{a}-\\theta)^{2}X^{2} + 2(\\beta_{a}-\\theta)X\\varepsilon_{a} + \\varepsilon_{a}^{2}$. We compute its conditional expectation:\n$$\\mathbb{E}[((\\beta_{a} - \\theta)X + \\varepsilon_{a})^{2} \\mid A=a] = (\\beta_{a}-\\theta)^{2}\\mathbb{E}[X^{2} \\mid A=a] + 2(\\beta_{a}-\\theta)\\mathbb{E}[X\\varepsilon_{a} \\mid A=a] + \\mathbb{E}[\\varepsilon_{a}^{2} \\mid A=a]$$\nUsing the provided conditions:\n- $\\mathbb{E}[X \\mid A=a]=0$ and $\\operatorname{Var}(X \\mid A=a)=1$, which implies $\\mathbb{E}[X^{2} \\mid A=a] = \\operatorname{Var}(X \\mid A=a) + (\\mathbb{E}[X \\mid A=a])^{2} = 1 + 0^{2} = 1$.\n- $\\mathbb{E}[\\varepsilon_{A} \\mid X, A]=0$. By the law of iterated expectations, $\\mathbb{E}[X\\varepsilon_{a} \\mid A=a] = \\mathbb{E}[\\mathbb{E}[X\\varepsilon_{a} \\mid X, A=a] \\mid A=a] = \\mathbb{E}[X \\mathbb{E}[\\varepsilon_{a} \\mid X, A=a] \\mid A=a] = \\mathbb{E}[X \\cdot 0 \\mid A=a] = 0$.\n- $\\operatorname{Var}(\\varepsilon_{A} \\mid X, A=a)=\\sigma_{a}^{2}$ and $\\mathbb{E}[\\varepsilon_{A} \\mid X, A=a] = 0$. This implies $\\mathbb{E}[\\varepsilon_{a}^{2} \\mid X, A=a] = \\sigma_{a}^{2}$. By iterated expectations, $\\mathbb{E}[\\varepsilon_{a}^{2} \\mid A=a] = \\mathbb{E}[\\mathbb{E}[\\varepsilon_{a}^{2} \\mid X, A=a] \\mid A=a] = \\mathbb{E}[\\sigma_{a}^{2} \\mid A=a] = \\sigma_{a}^{2}$.\n\nSubstituting these results back, the conditional expectation of the squared error for group $a$ is $(\\beta_{a} - \\theta)^{2} + \\sigma_{a}^{2}$. The loss function becomes:\n$$L(\\theta) = p_{0} w_{0} [(\\beta_{0} - \\theta)^{2} + \\sigma_{0}^{2}] + p_{1} w_{1} [(\\beta_{1} - \\theta)^{2} + \\sigma_{1}^{2}]$$\nTo find the minimum, we compute the derivative with respect to $\\theta$ and set it to zero (first-order optimality condition):\n$$\\frac{dL}{d\\theta} = p_{0} w_{0} [2(\\beta_{0} - \\theta)(-1)] + p_{1} w_{1} [2(\\beta_{1} - \\theta)(-1)] = 0$$\n$$-2 [p_{0} w_{0} (\\beta_{0} - \\theta) + p_{1} w_{1} (\\beta_{1} - \\theta)] = 0$$\n$$(p_{0} w_{0} + p_{1} w_{1})\\theta = p_{0} w_{0} \\beta_{0} + p_{1} w_{1} \\beta_{1}$$\nApplying the normalization constraint $p_{0} w_{0} + p_{1} w_{1} = 1$, we find the weighted least squares solution for $\\theta$:\n$$\\theta = p_{0} w_{0} \\beta_{0} + p_{1} w_{1} \\beta_{1}$$\n\n**Step 2: Group-Conditional Mean Squared Error**\nThe mean squared error for group $a$, $\\text{MSE}_{a}$, is defined as $\\mathbb{E}[(Y-\\hat{Y}_{\\theta})^{2} \\mid A=a]$. With $\\hat{Y}_{\\theta}=\\theta X$, this is precisely the conditional expectation we calculated in Step 1:\n$$\\text{MSE}_{a} = \\mathbb{E}[(Y - \\theta X)^{2} \\mid A=a] = (\\beta_{a} - \\theta)^{2} + \\sigma_{a}^{2}$$\nThis expression shows that the MSE for a group has two components: a bias term $(\\beta_{a}-\\theta)^{2}$ which measures the squared difference between the group-optimal slope $\\beta_a$ and the shared slope $\\theta$, and a variance term $\\sigma_{a}^{2}$ from the irreducible noise.\n\n**Step 3: The Fairness Constraint**\nThe fairness criterion requires $\\text{MSE}_{0} = \\text{MSE}_{1}$. Let $\\theta_{F}$ be the value of $\\theta$ that satisfies this criterion.\n$$(\\beta_{0} - \\theta_{F})^{2} + \\sigma_{0}^{2} = (\\beta_{1} - \\theta_{F})^{2} + \\sigma_{1}^{2}$$\nExpanding the squared terms:\n$$\\beta_{0}^{2} - 2\\beta_{0}\\theta_{F} + \\theta_{F}^{2} + \\sigma_{0}^{2} = \\beta_{1}^{2} - 2\\beta_{1}\\theta_{F} + \\theta_{F}^{2} + \\sigma_{1}^{2}$$\n$$2\\beta_{1}\\theta_{F} - 2\\beta_{0}\\theta_{F} = \\beta_{1}^{2} - \\beta_{0}^{2} + \\sigma_{1}^{2} - \\sigma_{0}^{2}$$\n$$2\\theta_{F}(\\beta_{1} - \\beta_{0}) = (\\beta_{1} - \\beta_{0})(\\beta_{1} + \\beta_{0}) + (\\sigma_{1}^{2} - \\sigma_{0}^{2})$$\nSince $\\beta_{0} \\neq \\beta_{1}$, we can divide by $2(\\beta_{1} - \\beta_{0})$:\n$$\\theta_{F} = \\frac{\\beta_{0} + \\beta_{1}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})}$$\nThis is the unique slope that equalizes the MSE between the two groups. If the noise is homoscedastic ($\\sigma_{0}^{2} = \\sigma_{1}^{2}$), then $\\theta_F$ is simply the average of the two group-specific slopes. However, with heteroscedastic noise ($\\sigma_{0}^{2} \\neq \\sigma_{1}^{2}$), $\\theta_F$ is shifted away from this average to compensate. For example, if group 1 is noisier ($\\sigma_{1}^{2} > \\sigma_{0}^{2}$) and has a higher slope ($\\beta_{1} > \\beta_{0}$), $\\theta_F$ must be larger than the average to decrease the bias term for group 1 (by bringing $\\theta_F$ closer to $\\beta_1$) and increase it for group 0, balancing the overall MSE. This illustrates the trade-off: achieving fairness requires choosing a slope $\\theta_F$ that is not necessarily optimal for either group individually, nor for the population as a whole (which would typically be a weighted average like $p_0\\beta_0+p_1\\beta_1$), in order to balance errors across groups. The weights $(w_0, w_1)$ are the mechanism to steer the WLS optimization toward this specific fairness-driven target $\\theta_F$.\n\n**Step 4: Solving for the Weights**\nWe now have a system of two linear equations for $w_{0}$ and $w_{1}$:\n1.  $\\theta_{F} = p_{0} w_{0} \\beta_{0} + p_{1} w_{1} \\beta_{1}$\n2.  $1 = p_{0} w_{0} + p_{1} w_{1}$\n\nFrom equation (2), we can write $p_{1} w_{1} = 1 - p_{0} w_{0}$. Substituting this into equation (1):\n$$\\theta_{F} = p_{0} w_{0} \\beta_{0} + (1 - p_{0} w_{0})\\beta_{1} = p_{0} w_{0} \\beta_{0} + \\beta_{1} - p_{0} w_{0} \\beta_{1}$$\n$$\\theta_{F} - \\beta_{1} = p_{0} w_{0} (\\beta_{0} - \\beta_{1})$$\n$$w_{0} = \\frac{\\theta_{F} - \\beta_{1}}{p_{0}(\\beta_{0} - \\beta_{1})} = \\frac{\\beta_{1} - \\theta_{F}}{p_{0}(\\beta_{1} - \\beta_{0})}$$\nSubstitute the expression for $\\theta_{F}$:\n$$w_{0} = \\frac{1}{p_{0}(\\beta_{1} - \\beta_{0})} \\left[ \\beta_{1} - \\left(\\frac{\\beta_{0} + \\beta_{1}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})}\\right) \\right]$$\n$$w_{0} = \\frac{1}{p_{0}(\\beta_{1} - \\beta_{0})} \\left[ \\frac{2\\beta_{1} - (\\beta_{0} + \\beta_{1})}{2} - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})} \\right]$$\n$$w_{0} = \\frac{1}{p_{0}(\\beta_{1} - \\beta_{0})} \\left[ \\frac{\\beta_{1} - \\beta_{0}}{2} - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})} \\right]$$\n$$w_{0} = \\frac{1}{2p_{0}} \\left( 1 - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right)$$\n\nSimilarly, we can solve for $w_{1}$ by substituting $p_{0}w_{0} = 1 - p_{1}w_{1}$ into equation (1):\n$$\\theta_{F} = (1 - p_{1} w_{1})\\beta_{0} + p_{1} w_{1} \\beta_{1} = \\beta_{0} + p_{1} w_{1}(\\beta_{1} - \\beta_{0})$$\n$$w_{1} = \\frac{\\theta_{F} - \\beta_{0}}{p_{1}(\\beta_{1} - \\beta_{0})}$$\nSubstitute the expression for $\\theta_{F}$:\n$$w_{1} = \\frac{1}{p_{1}(\\beta_{1} - \\beta_{0})} \\left[ \\left(\\frac{\\beta_{0} + \\beta_{1}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})}\\right) - \\beta_{0} \\right]$$\n$$w_{1} = \\frac{1}{p_{1}(\\beta_{1} - \\beta_{0})} \\left[ \\frac{\\beta_{1} - \\beta_{0}}{2} + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{2(\\beta_{1} - \\beta_{0})} \\right]$$\n$$w_{1} = \\frac{1}{2p_{1}} \\left( 1 + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right)$$\n\nThese are the required unique normalized weights. The assumption that $\\theta_F$ lies strictly between $\\beta_0$ and $\\beta_1$ ensures that the term $1 \\pm \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}}$ is positive, and thus $w_0, w_1 > 0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2p_{0}} \\left( 1 - \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right) & \\frac{1}{2p_{1}} \\left( 1 + \\frac{\\sigma_{1}^{2} - \\sigma_{0}^{2}}{(\\beta_{1} - \\beta_{0})^{2}} \\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A model that appears fair in the lab may exhibit significant bias when deployed in the real world, a critical challenge known as distribution shift. This practice problem provides a stark, numerical demonstration of how covariate shift between training and test data can invalidate fairness guarantees like equalized odds. By working through this scenario, you will not only diagnose the failure of a seemingly fair model but also learn to apply importance weighting, a powerful technique for auditing and estimating fairness on a target population using data from a different source distribution .",
            "id": "3098292",
            "problem": "Consider binary classification under Empirical Risk Minimization (ERM), with a binary sensitive attribute $A \\in \\{0,1\\}$, a binary feature $X \\in \\{0,1\\}$, and a binary label $Y \\in \\{0,1\\}$. The classifier is defined by $\\hat{Y}(X) = \\mathbf{1}\\{X=1\\}$ and is trained to minimize empirical $0$-$1$ loss. Let the training distribution be imbalanced across groups, with $P_{\\text{train}}(A=0) = 0.9$ and $P_{\\text{train}}(A=1) = 0.1$. Assume the label conditional is stable across domains and groups (covariate shift only), specified by $P(Y=1 \\mid X=1) = \\frac{3}{4}$ and $P(Y=1 \\mid X=0) = \\frac{1}{4}$, so that $P(Y=0 \\mid X=1) = \\frac{1}{4}$ and $P(Y=0 \\mid X=0) = \\frac{3}{4}$ for both training and test. On the training distribution, the covariate conditionals are group-balanced: $P_{\\text{train}}(X=1 \\mid A=0) = P_{\\text{train}}(X=1 \\mid A=1) = \\frac{1}{2}$. On the test distribution, there is group-specific covariate shift: $P_{\\text{test}}(X=1 \\mid A=0) = \\frac{3}{4}$ and $P_{\\text{test}}(X=1 \\mid A=1) = \\frac{1}{4}$. \n\nWe audit the fairness notion of equalized odds on the false positive rate (FPR), where for group $a \\in \\{0,1\\}$ the false positive rate is defined by $\\text{FPR}_{a} = P(\\hat{Y}=1 \\mid Y=0, A=a)$. First, reason from the definitions to verify that training equalized odds (equality of $\\text{FPR}_{0}$ and $\\text{FPR}_{1}$) holds, and that test equalized odds fails. Then, propose importance-weighted auditing under covariate shift by defining $w_{a}(x) = \\frac{P_{\\text{test}}(X=x \\mid A=a)}{P_{\\text{train}}(X=x \\mid A=a)}$, and using these weights to rewrite the test-domain FPR as a weighted conditional expectation on the training domain.\n\nCompute, explicitly from the given numbers, the importance-weighted false positive rate gap $\\text{FPR}_{0}^{\\text{iw}} - \\text{FPR}_{1}^{\\text{iw}}$ obtained by auditing on the training distribution with the weights $w_{a}(x)$. Express your final answer as a decimal or a fraction (no percent sign). No rounding is required.",
            "solution": "The solution requires analyzing the false positive rate (FPR) of the classifier $\\hat{Y}(X) = \\mathbf{1}\\{X=1\\}$ under both the training and test distributions. The FPR for a group $a$ is defined as $\\text{FPR}_{a} = P(\\hat{Y}=1 \\mid Y=0, A=a)$, which simplifies to $P(X=1 \\mid Y=0, A=a)$ given the classifier.\n\n**1. Verification of Equalized Odds on Training and Test Data**\n\nWe can express the FPR using Bayes' theorem. Due to the covariate shift assumption, the label conditional $P(Y \\mid X)$ is stable across domains and groups.\n$$ \\text{FPR}_{a} = P(X=1 \\mid Y=0, A=a) = \\frac{P(Y=0 \\mid X=1) P(X=1 \\mid A=a)}{P(Y=0 \\mid A=a)} $$\nThe denominator, $P_{\\text{dist}}(Y=0 \\mid A=a)$, is computed using the law of total probability for each distribution (train/test):\n$$ P_{\\text{dist}}(Y=0 \\mid A=a) = P(Y=0 \\mid X=0)P_{\\text{dist}}(X=0 \\mid A=a) + P(Y=0 \\mid X=1)P_{\\text{dist}}(X=1 \\mid A=a) $$\nWe are given $P(Y=0 \\mid X=1) = 1/4$ and $P(Y=0 \\mid X=0) = 3/4$.\n\n**On the training distribution:**\n- $P_{\\text{train}}(X=1 \\mid A=0) = 1/2$ and $P_{\\text{train}}(X=1 \\mid A=1) = 1/2$.\n- For both groups, $P_{\\text{train}}(Y=0 \\mid A=a) = (\\frac{3}{4})(\\frac{1}{2}) + (\\frac{1}{4})(\\frac{1}{2}) = \\frac{1}{2}$.\n- Therefore, the training FPRs are:\n  - $\\text{FPR}_{0}^{\\text{train}} = \\frac{(1/4)(1/2)}{1/2} = \\frac{1}{4}$\n  - $\\text{FPR}_{1}^{\\text{train}} = \\frac{(1/4)(1/2)}{1/2} = \\frac{1}{4}$\nSince $\\text{FPR}_{0}^{\\text{train}} = \\text{FPR}_{1}^{\\text{train}}$, equalized odds holds on the training data.\n\n**On the test distribution:**\n- $P_{\\text{test}}(X=1 \\mid A=0) = 3/4$ and $P_{\\text{test}}(X=1 \\mid A=1) = 1/4$.\n- $P_{\\text{test}}(Y=0 \\mid A=0) = (\\frac{3}{4})(\\frac{1}{4}) + (\\frac{1}{4})(\\frac{3}{4}) = \\frac{6}{16} = \\frac{3}{8}$.\n- $P_{\\text{test}}(Y=0 \\mid A=1) = (\\frac{3}{4})(\\frac{3}{4}) + (\\frac{1}{4})(\\frac{1}{4}) = \\frac{10}{16} = \\frac{5}{8}$.\n- The test FPRs are:\n  - $\\text{FPR}_{0}^{\\text{test}} = \\frac{(1/4)(3/4)}{3/8} = \\frac{3/16}{3/8} = \\frac{1}{2}$\n  - $\\text{FPR}_{1}^{\\text{test}} = \\frac{(1/4)(1/4)}{5/8} = \\frac{1/16}{5/8} = \\frac{1}{10}$\nSince $\\text{FPR}_{0}^{\\text{test}} \\neq \\text{FPR}_{1}^{\\text{test}}$, equalized odds fails on the test data due to covariate shift.\n\n**2. Importance-Weighted Auditing**\n\nTo estimate the test FPRs using the training data, we use importance weights $w_{a}(x) = \\frac{P_{\\text{test}}(X=x \\mid A=a)}{P_{\\text{train}}(X=x \\mid A=a)}$. The importance-weighted FPR is given by the ratio of weighted expectations:\n$$ \\text{FPR}_{a}^{\\text{iw}} = \\frac{E_{\\text{train}}[\\hat{Y}(X) \\mathbf{1}\\{Y=0\\} w_a(X) \\mid A=a]}{E_{\\text{train}}[\\mathbf{1}\\{Y=0\\} w_a(X) \\mid A=a]} $$\nFirst, compute the weights:\n- For $A=0$: $w_0(1) = \\frac{3/4}{1/2} = 3/2$ and $w_0(0) = \\frac{1/4}{1/2} = 1/2$.\n- For $A=1$: $w_1(1) = \\frac{1/4}{1/2} = 1/2$ and $w_1(0) = \\frac{3/4}{1/2} = 3/2$.\n\nNow, compute the importance-weighted FPRs:\n**For group $A=0$**:\n- Numerator: $N_0 = E_{\\text{train}}[\\mathbf{1}\\{X=1\\}\\mathbf{1}\\{Y=0\\}w_0(X) \\mid A=0] = P(Y=0|X=1) P_{\\text{train}}(X=1|A=0) w_0(1) = (\\frac{1}{4})(\\frac{1}{2})(\\frac{3}{2}) = \\frac{3}{16}$.\n- Denominator: $D_0 = E_{\\text{train}}[\\mathbf{1}\\{Y=0\\}w_0(X) \\mid A=0] = P(Y=0|X=0)P_{\\text{train}}(X=0|A=0)w_0(0) + N_0 = (\\frac{3}{4})(\\frac{1}{2})(\\frac{1}{2}) + \\frac{3}{16} = \\frac{3}{16} + \\frac{3}{16} = \\frac{6}{16} = \\frac{3}{8}$.\n- $\\text{FPR}_{0}^{\\text{iw}} = \\frac{N_0}{D_0} = \\frac{3/16}{3/8} = \\frac{1}{2}$.\n\n**For group $A=1$**:\n- Numerator: $N_1 = P(Y=0|X=1) P_{\\text{train}}(X=1|A=1) w_1(1) = (\\frac{1}{4})(\\frac{1}{2})(\\frac{1}{2}) = \\frac{1}{16}$.\n- Denominator: $D_1 = P(Y=0|X=0)P_{\\text{train}}(X=0|A=1)w_1(0) + N_1 = (\\frac{3}{4})(\\frac{1}{2})(\\frac{3}{2}) + \\frac{1}{16} = \\frac{9}{16} + \\frac{1}{16} = \\frac{10}{16} = \\frac{5}{8}$.\n- $\\text{FPR}_{1}^{\\text{iw}} = \\frac{N_1}{D_1} = \\frac{1/16}{5/8} = \\frac{1}{10}$.\n\nThe importance-weighted gap is:\n$$ \\text{FPR}_{0}^{\\text{iw}} - \\text{FPR}_{1}^{\\text{iw}} = \\frac{1}{2} - \\frac{1}{10} = \\frac{5}{10} - \\frac{1}{10} = \\frac{4}{10} = \\frac{2}{5} $$\nThis matches the gap calculated directly on the test distribution, demonstrating that importance weighting correctly audits for fairness under covariate shift.",
            "answer": "$$ \\boxed{\\frac{2}{5}} $$"
        },
        {
            "introduction": "Moving from auditing to proactive design, this final practice challenges you to build fairness directly into the structure of a more complex model: a decision tree. Decision trees make predictions by routing instances through a series of splits, and these paths can inadvertently segregate demographic groups, leading to biased outcomes. In this hands-on coding exercise, you will derive and implement a novel regularizer that penalizes decision splits that increase group disparity, encouraging the tree to learn fairer representations at every step of the decision-making process .",
            "id": "3098334",
            "problem": "You are given a decision path in a binary classification setting where a decision tree routes an instance from the root to a leaf. At each node along the path, the population is partitioned into a fixed set of demographic groups, and you have access to the group membership counts. The goal is to explore algorithmic fairness in tree-based models by designing and computing a regularizer that penalizes changes in group proportions along the path, enforcing that intermediate decision splits do not greatly skew the distribution of groups relative to their parent node.\n\nStart from the fundamental base of probability and convex analysis: empirical proportions derived from counts, the concept of statistical parity (comparing selection rates across groups), and the definition of convex penalties. The following definitions and requirements must be used to design the regularizer.\n\nDefinitions:\n- Let $k$ be the number of groups. At node $i$ along a path of length $T$ (with $T+1$ nodes indexed by $i=0,1,\\dots,T$), you observe the count vector $\\mathbf{c}^{(i)} \\in \\mathbb{N}_0^k$, where the $j$-th component $c^{(i)}_j$ is the number of individuals from group $j$ at node $i$.\n- Define smoothed empirical proportions for each node $i$ as $\\mathbf{p}^{(i)}$, computed from $\\mathbf{c}^{(i)}$ using Laplace smoothing with parameter $\\alpha \\ge 0$:\n$$\np^{(i)}_j \\triangleq \\frac{c^{(i)}_j + \\alpha}{\\sum_{m=1}^k c^{(i)}_m + k\\alpha}, \\quad j=1,\\dots,k,\n$$\nwhich ensures well-defined proportions even if some $c^{(i)}_j$ are zero. If any node has $\\sum_{m=1}^k c^{(i)}_m = 0$, you must take $\\alpha > 0$.\n\nRequirements for the regularizer:\n- The regularizer $R$ must depend only on consecutive differences $\\mathbf{p}^{(i+1)} - \\mathbf{p}^{(i)}$ along the path, encouraging stability of group proportions at splits.\n- $R$ must be symmetric with respect to permutations of group indices (no group is privileged).\n- $R$ must be convex in the differences to reflect increasing penalty for larger skew.\n- $R$ must be separable across groups (the contribution is a sum over groups).\n- $R$ must be additive along the path with nonnegative edge weights $\\{w_i\\}_{i=0}^{T-1}$, where $w_i$ quantifies how much the $i$-th splitâ€™s skew contributes to the overall penalty.\n- $R$ must be scale-invariant with respect to scaling all counts at a node by a positive constant, due to normalization into proportions.\n- To penalize endpoint skew at the leaf relative to the root (to reflect cumulative distortion after routing), include an endpoint term with coefficient $\\beta \\ge 0$ that compares $\\mathbf{p}^{(T)}$ to $\\mathbf{p}^{(0)}$.\n\nUnder the restriction to quadratic penalties in the differences and the above invariances, derive the canonical form of $R$ and then implement it. Your program must:\n- Compute smoothed proportions $\\mathbf{p}^{(i)}$ for each node using the given $\\alpha$.\n- Compute the final regularizer value $R$ for each provided test case, using the derived additive quadratic form with weights $\\{w_i\\}$ and endpoint coefficient $\\beta$, applied to the sequence $\\{\\mathbf{p}^{(i)}\\}_{i=0}^T$.\n\nTest suite and parameterization:\nFor each test case, you are given $k$, a list of node count vectors $\\{\\mathbf{c}^{(i)}\\}_{i=0}^T$, an edge-weight list $\\{w_i\\}_{i=0}^{T-1}$, and $(\\alpha,\\beta)$. Implement the regularizer that meets all the requirements above, and compute the resulting scalar value as a float.\n\nUse the following test cases:\n- Case $1$ (happy path, two groups, nonzero endpoint penalty):\n  - $k = 2$\n  - Counts along path: $[50,50] \\rightarrow [30,20] \\rightarrow [18,12]$\n  - Weights: $[1.0,0.5]$\n  - $\\alpha = 0.0$, $\\beta = 0.25$\n- Case $2$ (three groups, deeper path, positive smoothing):\n  - $k = 3$\n  - Counts along path: $[20,30,50] \\rightarrow [25,15,60] \\rightarrow [5,10,80] \\rightarrow [1,9,90]$\n  - Weights: $[1.0,1.0,1.0]$\n  - $\\alpha = 0.5$, $\\beta = 1.0$\n- Case $3$ (edge case: zero counts for one group at all nodes):\n  - $k = 2$\n  - Counts along path: $[0,100] \\rightarrow [0,50]$\n  - Weights: $[1.0]$\n  - $\\alpha = 1.0$, $\\beta = 0.0$\n- Case $4$ (boundary case: identical counts at each node should yield zero regularizer even with smoothing and endpoint penalty):\n  - $k = 2$\n  - Counts along path: $[10,30] \\rightarrow [10,30] \\rightarrow [10,30]$\n  - Weights: $[1.0,2.0]$\n  - $\\alpha = 1.0$, $\\beta = 0.5$\n- Case $5$ (scaling invariance check: counts scaled by a positive constant should yield the same regularizer as Case $1$):\n  - $k = 2$\n  - Counts along path: $[500,500] \\rightarrow [300,200] \\rightarrow [180,120]$\n  - Weights: $[1.0,0.5]$\n  - $\\alpha = 0.0$, $\\beta = 0.25$\n\nFinal output format:\n- Your program should produce a single line of output containing the regularizer values for Case $1$ through Case $5$ as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places (for example, $[0.123456,0.000000,0.500000]$).",
            "solution": "The problem requires the derivation and implementation of a regularizer, $R$, designed to penalize changes in demographic group proportions along a decision path in a tree-based model. The derivation must adhere to a set of specified principles.\n\n**1. Derivation of the Regularizer $R$**\n\nLet us construct the form of the regularizer $R$ by systematically incorporating the given requirements. The path consists of $T+1$ nodes, indexed by $i \\in \\{0, 1, \\dots, T\\}$, with corresponding group count vectors $\\mathbf{c}^{(i)} \\in \\mathbb{N}_0^k$ for $k$ groups. The smoothed empirical proportions at node $i$ are given by the vector $\\mathbf{p}^{(i)}$, where each component $p_j^{(i)}$ is defined as:\n$$\np^{(i)}_j \\triangleq \\frac{c^{(i)}_j + \\alpha}{\\sum_{m=1}^k c^{(i)}_m + k\\alpha}\n$$\nHere, $\\alpha \\ge 0$ is the Laplace smoothing parameter.\n\nThe requirements for $R$ are as follows:\n\n*   **Additivity and Edge Weights**: The regularizer must be additive along the path, with non-negative weights $\\{w_i\\}_{i=0}^{T-1}$ for each transition (edge) from node $i$ to $i+1$. It also includes a separate endpoint term with a coefficient $\\beta \\ge 0$. This structure implies a sum of terms:\n    $$\n    R = \\left( \\sum_{i=0}^{T-1} w_i \\cdot \\text{Penalty}(\\text{split } i \\to i+1) \\right) + \\beta \\cdot \\text{Penalty}(\\text{endpoint vs. root})\n    $$\n\n*   **Dependence on Differences**: The penalty for each split must depend only on the difference in proportions between consecutive nodes, $\\Delta \\mathbf{p}^{(i)} \\triangleq \\mathbf{p}^{(i+1)} - \\mathbf{p}^{(i)}$. The endpoint penalty must depend on the difference between the final leaf node proportions and the initial root node proportions, $\\mathbf{p}^{(T)} - \\mathbf{p}^{(0)}$.\n\n*   **Separability, Symmetry, Convexity, and Quadratic Form**: These constraints define the functional form of the penalty terms.\n    1.  **Separability across groups** implies that the penalty for a single split, let's call it $S_i(\\Delta \\mathbf{p}^{(i)})$, is a sum of functions of the individual components of the difference vector: $S_i(\\Delta \\mathbf{p}^{(i)}) = \\sum_{j=1}^k f_j ((\\Delta p^{(i)})_j)$.\n    2.  **Symmetry with respect to group indices** requires that all groups are treated equally, meaning the function $f_j$ must be the same for all groups $j$. Let's call this function $f$. Thus, $S_i(\\Delta \\mathbf{p}^{(i)}) = \\sum_{j=1}^k f((\\Delta p^{(i)})_j)$.\n    3.  The restriction to **quadratic penalties** dictates that $f(x)$ must be a quadratic function of its argument $x$. The simplest such non-negative function is $f(x) = C x^2$ for some constant $C \\geq 0$. This form also satisfies the **convexity** requirement.\n    Without loss of generality, we can set the constant $C=1$, as any scaling factor can be absorbed into the edge weights $w_i$ and the endpoint coefficient $\\beta$.\n\nCombining these points, the penalty for the split from node $i$ to $i+1$ is the sum of the squares of the component-wise differences in proportions:\n$$\n\\text{Penalty}(\\text{split } i \\to i+1) = \\sum_{j=1}^k (p_j^{(i+1)} - p_j^{(i)})^2 = \\|\\mathbf{p}^{(i+1)} - \\mathbf{p}^{(i)}\\|_2^2\n$$\nwhere $\\|\\cdot\\|_2^2$ denotes the squared Euclidean norm.\n\nApplying the same logic for the endpoint term, which compares $\\mathbf{p}^{(T)}$ to $\\mathbf{p}^{(0)}$, its form is:\n$$\n\\text{Penalty}(\\text{endpoint vs. root}) = \\sum_{j=1}^k (p_j^{(T)} - p_j^{(0)})^2 = \\|\\mathbf{p}^{(T)} - \\mathbf{p}^{(0)}\\|_2^2\n$$\n\n*   **Final Form**: Assembling these parts, we arrive at the canonical form of the regularizer $R$:\n    $$\n    R = \\sum_{i=0}^{T-1} w_i \\|\\mathbf{p}^{(i+1)} - \\mathbf{p}^{(i)}\\|_2^2 + \\beta \\|\\mathbf{p}^{(T)} - \\mathbf{p}^{(0)}\\|_2^2\n    $$\n    This expression satisfies all the structural requirements.\n\n*   **Scale Invariance**: The problem requires $R$ to be scale-invariant with respect to scaling all counts at a node by a positive constant. Let us examine the proportion formula. If $\\alpha = 0$, then $p^{(i)}_j = c^{(i)}_j / \\sum_m c^{(i)}_m$. If we scale all counts by $\\lambda > 0$, i.e., $\\mathbf{c}'^{(i)} = \\lambda \\mathbf{c}^{(i)}$, the new proportions are $p'^{(i)}_j = (\\lambda c^{(i)}_j) / \\sum_m (\\lambda c^{(i)}_m) = p^{(i)}_j$. In this case ($\\alpha=0$), the proportions are invariant, and thus $R$ is scale-invariant. However, if $\\alpha > 0$, the smoothed proportion $p^{(i)}_j = (\\lambda c^{(i)}_j + \\alpha) / (\\lambda \\sum_m c^{(i)}_m + k\\alpha)$ is not invariant to the scaling factor $\\lambda$. The smoothing term introduces a scale-dependent effect, which is a standard consequence of this regularization technique (pseudo-counts have a larger relative effect when true counts are small). The problem statement implicitly acknowledges this by only testing scale invariance (Case 5 vs. Case 1) in the scenario where $\\alpha = 0$. Our implementation will use the provided formula for proportions as defined, and the scale-invariance property will hold if and only if $\\alpha=0$.\n\n**2. Implementation Plan**\n\nThe implementation will consist of two main parts: a function to compute the smoothed proportions for a given count vector, and a main function to compute the regularizer $R$ over a path.\n\n1.  **Proportion Calculation**: For each node $i$, take the count vector $\\mathbf{c}^{(i)}$, the number of groups $k$, and the smoothing parameter $\\alpha$. Compute the denominator $D^{(i)} = \\sum_{m=1}^k c^{(i)}_m + k\\alpha$. The problem specification guarantees that $D^{(i)} \\neq 0$. Then, compute the proportion vector $\\mathbf{p}^{(i)}$ where $p_j^{(i)} = (c^{(i)}_j + \\alpha) / D^{(i)}$.\n\n2.  **Regularizer Calculation**:\n    a. Iterate through all nodes in the path to compute the sequence of proportion vectors $\\{\\mathbf{p}^{(0)}, \\mathbf{p}^{(1)}, \\dots, \\mathbf{p}^{(T)}\\}$.\n    b. Initialize a variable `total_penalty = 0`.\n    c. Loop from $i = 0$ to $T-1$: calculate the difference vector $\\Delta \\mathbf{p}^{(i)} = \\mathbf{p}^{(i+1)} - \\mathbf{p}^{(i)}$, compute its squared Euclidean norm $\\|\\Delta \\mathbf{p}^{(i)}\\|_2^2$, and add $w_i \\cdot \\|\\Delta \\mathbf{p}^{(i)}\\|_2^2$ to `total_penalty`.\n    d. Calculate the endpoint difference $\\Delta \\mathbf{p}_{\\text{end}} = \\mathbf{p}^{(T)} - \\mathbf{p}^{(0)}$, compute its squared Euclidean norm $\\|\\Delta \\mathbf{p}_{\\text{end}}\\|_2^2$, and add $\\beta \\cdot \\|\\Delta \\mathbf{p}_{\\text{end}}\\|_2^2$ to `total_penalty`.\n    e. The final value of `total_penalty` is the regularizer value $R$.\n\nThis procedure is directly implemented in the following Python code to solve the provided test cases.",
            "answer": "```python\nimport numpy as np\n\ndef compute_regularizer(k: int, all_counts: list[list[int]], weights: list[float], alpha: float, beta: float) -> float:\n    \"\"\"\n    Computes the fairness regularizer value based on the derived formula.\n\n    The regularizer is defined as:\n    R = sum_{i=0 to T-1} w_i * ||p^(i+1) - p^(i)||_2^2 + beta * ||p^(T) - p^(0)||_2^2\n    where p^(i) is the smoothed proportion vector at node i.\n\n    Args:\n        k: The number of demographic groups.\n        all_counts: A list of count vectors, where each vector c^(i) represents\n                    the counts of individuals from each group at node i.\n        weights: A list of non-negative edge weights {w_i} for each split.\n        alpha: The Laplace smoothing parameter.\n        beta: The coefficient for the endpoint penalty term.\n\n    Returns:\n        The scalar value of the regularizer R.\n    \"\"\"\n    \n    # Step 1: Compute smoothed proportions for all nodes in the path.\n    proportions = []\n    for counts_at_node in all_counts:\n        counts_arr = np.array(counts_at_node, dtype=float)\n        total_counts = np.sum(counts_arr)\n        \n        # Denominator for smoothed proportions. The problem constraints ensure this is not zero.\n        denominator = total_counts + k * alpha\n        \n        # Numerator with smoothing\n        numerator = counts_arr + alpha\n        \n        # Smoothed proportion vector p^(i)\n        p_i = numerator / denominator\n        proportions.append(p_i)\n    \n    T = len(proportions) - 1\n    total_penalty = 0.0\n\n    # Step 2: Compute the sum of weighted penalties for each split.\n    for i in range(T):\n        w_i = weights[i]\n        p_i = proportions[i]\n        p_i_plus_1 = proportions[i+1]\n        \n        # Difference vector p^(i+1) - p^(i)\n        diff = p_i_plus_1 - p_i\n        \n        # Squared L2 norm of the difference\n        squared_norm = np.sum(np.square(diff))\n        \n        total_penalty += w_i * squared_norm\n        \n    # Step 3: Compute and add the endpoint penalty.\n    if T >= 0 and beta > 0:\n        p_0 = proportions[0]\n        p_T = proportions[T]\n        \n        # Difference vector p^(T) - p^(0)\n        end_diff = p_T - p_0\n        \n        # Squared L2 norm of the endpoint difference\n        end_squared_norm = np.sum(np.square(end_diff))\n        \n        total_penalty += beta * end_squared_norm\n        \n    return total_penalty\n\ndef solve():\n    \"\"\"\n    Defines the test cases, computes the regularizer for each, and prints the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, two groups, nonzero endpoint penalty)\n        {\n            \"k\": 2,\n            \"counts\": [[50, 50], [30, 20], [18, 12]],\n            \"weights\": [1.0, 0.5],\n            \"params\": (0.0, 0.25)\n        },\n        # Case 2 (three groups, deeper path, positive smoothing)\n        {\n            \"k\": 3,\n            \"counts\": [[20, 30, 50], [25, 15, 60], [5, 10, 80], [1, 9, 90]],\n            \"weights\": [1.0, 1.0, 1.0],\n            \"params\": (0.5, 1.0)\n        },\n        # Case 3 (edge case: zero counts for one group at all nodes)\n        {\n            \"k\": 2,\n            \"counts\": [[0, 100], [0, 50]],\n            \"weights\": [1.0],\n            \"params\": (1.0, 0.0)\n        },\n        # Case 4 (boundary case: identical counts, should be zero)\n        {\n            \"k\": 2,\n            \"counts\": [[10, 30], [10, 30], [10, 30]],\n            \"weights\": [1.0, 2.0],\n            \"params\": (1.0, 0.5)\n        },\n        # Case 5 (scaling invariance check for alpha=0)\n        {\n            \"k\": 2,\n            \"counts\": [[500, 500], [300, 200], [180, 120]],\n            \"weights\": [1.0, 0.5],\n            \"params\": (0.0, 0.25)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k = case[\"k\"]\n        all_counts = case[\"counts\"]\n        weights = case[\"weights\"]\n        alpha, beta = case[\"params\"]\n        \n        result = compute_regularizer(k, all_counts, weights, alpha, beta)\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}