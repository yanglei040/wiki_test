{
    "hands_on_practices": [
        {
            "introduction": "Concentration inequalities are the bedrock of statistical learning theory, providing guarantees on how well an empirical average estimates its true mean. This first exercise provides a direct comparison between two fundamental tools: Hoeffding's inequality, which relies only on the range of the data, and Bernstein's inequality, which incorporates knowledge of the variance. By calculating the sample size savings, you will gain a concrete understanding of the value of information in statistical estimation .",
            "id": "3189962",
            "problem": "A data scientist is estimating the mean of a bounded loss in a binary classification task. Let $\\{X_{i}\\}_{i=1}^{n}$ be independent and identically distributed random variables with $X_{i} \\in [0,1]$ and $\\mathbb{E}[X_{i}] = \\mu$. The goal is to choose a sample size $n$ so that the empirical mean $\\hat{\\mu}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ satisfies $|\\hat{\\mu}_{n} - \\mu| \\leq \\varepsilon$ with probability at least $1 - \\delta$, where $\\varepsilon \\in (0,1)$ and $\\delta \\in (0,1)$ are given design parameters.\n\nTwo approaches are considered:\n\n1) A range-only approach that uses only the information that $X_{i} \\in [0,1]$.\n\n2) A variance-aware approach that additionally uses the knowledge that $\\operatorname{Var}(X_{i}) = \\sigma^{2}$ for a known $\\sigma^{2} \\in [0, \\tfrac{1}{4}]$.\n\nDefine $n_{\\text{H}}(\\varepsilon,\\delta)$ to be a sufficient sample size obtained under the range-only approach, and $n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})$ to be a sufficient sample size obtained under the variance-aware approach. Let the sample size savings factor be\n$$\nS(\\varepsilon,\\delta,\\sigma^{2}) \\equiv \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})}.\n$$\n\nUsing foundational concentration results for bounded independent random variables and the knowledge of variance when available, derive an exact simplified expression for $S(\\varepsilon,\\delta,\\sigma^{2})$ as a function of $\\varepsilon$ and $\\sigma^{2}$ only. Your final answer must be a single closed-form expression; no inequalities or implicit definitions are allowed. Do not provide a numerical approximation.",
            "solution": "The problem requires the derivation of the sample size savings factor $S(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})}$, where $n_{\\text{H}}$ and $n_{\\text{B}}$ are sufficient sample sizes derived from concentration inequalities. The goal is to find an expression for $S$ that depends only on $\\varepsilon$ and $\\sigma^{2}$. This will be achieved by applying the appropriate foundational concentration inequalities for each case and then computing the ratio of the derived sample sizes.\n\nFirst, we determine the sufficient sample size $n_{\\text{H}}(\\varepsilon,\\delta)$ for the range-only approach. We are given that $\\{X_{i}\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) random variables with $X_{i} \\in [0,1]$. Let $\\mathbb{E}[X_i] = \\mu$ and $\\hat{\\mu}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$. The appropriate tool for this scenario is Hoeffding's inequality, which bounds the deviation of a sum of bounded independent random variables from its expected value. For the sample mean, Hoeffding's inequality is stated as:\n$$P(|\\hat{\\mu}_{n} - \\mu| \\ge \\varepsilon) \\le 2 \\exp(-2n\\varepsilon^2)$$\nThis inequality holds because the range of each $X_i$ is $1-0=1$. We require the probability of the deviation being larger than $\\varepsilon$ to be at most $\\delta$:\n$$2 \\exp(-2n\\varepsilon^2) \\le \\delta$$\nTo find a sufficient sample size $n$, we solve this inequality for $n$:\n$$\\exp(-2n\\varepsilon^2) \\le \\frac{\\delta}{2}$$\n$$-2n\\varepsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right)$$\n$$2n\\varepsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right)$$\n$$n \\ge \\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\nTherefore, a sufficient sample size for the range-only approach is:\n$$n_{\\text{H}}(\\varepsilon,\\delta) = \\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n\nNext, we determine the sufficient sample size $n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})$ for the variance-aware approach. In addition to the previous conditions, we know that $\\operatorname{Var}(X_{i}) = \\sigma^{2}$. The foundational result that incorporates variance information for bounded variables is Bernstein's inequality. We apply it to the zero-mean variables $Y_i = X_i - \\mu$. We have $\\mathbb{E}[Y_i] = 0$ and $\\operatorname{Var}(Y_i) = \\operatorname{Var}(X_i) = \\sigma^2$. Since $X_i \\in [0,1]$ and $\\mu = \\mathbb{E}[X_i] \\in [0,1]$, the variable $Y_i$ is bounded as $|Y_i| = |X_i - \\mu| \\le \\max(\\mu, 1-\\mu) \\le 1$. We take the bound $M=1$. A common form of the two-sided Bernstein's inequality is:\n$$P\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right| \\ge \\varepsilon\\right) \\le 2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + M\\varepsilon/3)}\\right)$$\nSubstituting $|\\hat{\\mu}_{n} - \\mu|$ for $\\left|\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right|$ and $M=1$:\n$$P(|\\hat{\\mu}_{n} - \\mu| \\ge \\varepsilon) \\le 2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right)$$\nWe require this probability to be at most $\\delta$:\n$$2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right) \\le \\delta$$\nSolving for a sufficient sample size $n$:\n$$\\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right) \\le \\frac{\\delta}{2}$$\n$$-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)} \\le \\ln\\left(\\frac{\\delta}{2}\\right)$$\n$$\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)} \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right)$$\n$$n \\ge \\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\nTherefore, a sufficient sample size for the variance-aware approach is:\n$$n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n\nFinally, we compute the sample size savings factor $S(\\varepsilon,\\delta,\\sigma^{2})$ by taking the ratio of $n_{\\text{H}}$ and $n_{\\text{B}}$:\n$$S(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})} = \\frac{\\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)}{\\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)}$$\nThe terms $\\ln\\left(\\frac{2}{\\delta}\\right)$ and $\\varepsilon^2$ cancel, yielding an expression that depends only on $\\sigma^2$ and $\\varepsilon$, as required:\n$$S(\\sigma^{2},\\varepsilon) = \\frac{1/2}{2(\\sigma^2 + \\varepsilon/3)} = \\frac{1}{4(\\sigma^2 + \\varepsilon/3)}$$\nTo provide a simplified closed-form expression without fractions in the denominator, we can rewrite this as:\n$$S(\\sigma^{2},\\varepsilon) = \\frac{1}{4\\sigma^2 + \\frac{4\\varepsilon}{3}} = \\frac{1}{\\frac{12\\sigma^2 + 4\\varepsilon}{3}} = \\frac{3}{12\\sigma^2 + 4\\varepsilon} = \\frac{3}{4(3\\sigma^2 + \\varepsilon)}$$\nThis is the final simplified expression for the sample size savings factor.",
            "answer": "$$\n\\boxed{\\frac{3}{4(3\\sigma^2 + \\varepsilon)}}\n$$"
        },
        {
            "introduction": "While controlling the deviation of a single estimate is useful, machine learning requires guarantees that hold simultaneously over an entire family of hypotheses. This practice transitions from simple concentration to the concept of uniform convergence, using the classic example of threshold functions. You will compute the Vapnik-Chervonenkis (VC) dimension to measure the complexity of this class and apply the powerful Dvoretzky–Kiefer–Wolfowitz (DKW) inequality to determine a sufficient sample size for generalization .",
            "id": "3189954",
            "problem": "Consider the hypothesis class of monotone threshold functions on the real line, defined by $\\mathcal{H} = \\{ h_{t} : \\mathbb{R} \\to \\{0,1\\} \\mid h_{t}(x) = \\mathbf{1}\\{x \\le t\\},\\ t \\in \\mathbb{R} \\}$. Let $X_{1},\\dots,X_{n}$ be independent and identically distributed real-valued samples drawn from an arbitrary distribution $P$ on $\\mathbb{R}$ with cumulative distribution function $F$. For $h \\in \\mathcal{H}$, define the true risk $P(h) = \\mathbb{E}[h(X)]$ and the empirical risk $P_{n}(h) = \\frac{1}{n} \\sum_{i=1}^{n} h(X_{i})$.\n\nTasks:\n1. From first principles, compute the Vapnik–Chervonenkis (VC) dimension of $\\mathcal{H}$ by directly applying the definition of shattering.\n2. Let $\\epsilon \\in (0,1)$ and $\\delta \\in (0,1)$. Using only the exact-constant form of the Dvoretzky–Kiefer–Wolfowitz inequality as a foundational fact, derive the smallest integer sample size $n^{\\star}(\\epsilon,\\delta)$ such that, for every distribution $P$ on $\\mathbb{R}$, with probability at least $1-\\delta$ (over the draw of the sample), the uniform deviation bound\n$$\n\\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big| \\le \\epsilon\n$$\nholds simultaneously.\n3. By reducing the uniform deviation over $\\mathcal{H}$ to a deviation of a binomial proportion at a fixed threshold for a distribution with a continuous and strictly increasing cumulative distribution function at its median, argue that in any distribution-free sub-Gaussian tail bound of the form\n$$\n\\sup_{P} \\Pr\\!\\left( \\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big| > \\epsilon \\right) \\le K \\exp\\!\\big( - c\\, n \\epsilon^{2} \\big),\n$$\nthat holds for all $n$ sufficiently large and all $\\epsilon$ in a fixed interval $(0,\\epsilon_{0}]$, the largest possible exponent constant $c$ is a fixed numerical value. Determine this optimal $c$.\n\nReport your final answer as a single row matrix in the order: the VC dimension of $\\mathcal{H}$, the exact closed-form expression for $n^{\\star}(\\epsilon,\\delta)$, and the optimal exponent constant $c$. No rounding is required, and the answer must be an exact closed-form expression.",
            "solution": "This solution is presented in three parts, corresponding to the three tasks in the problem statement.\n\n**Task 1: VC dimension of $\\mathcal{H}$**\nThe Vapnik–Chervonenkis (VC) dimension of a hypothesis class $\\mathcal{H}$, denoted $\\mathrm{VCdim}(\\mathcal{H})$, is the cardinality of the largest set of points that $\\mathcal{H}$ can shatter. A set of points $\\{x_1, \\dots, x_m\\}$ is said to be shattered by $\\mathcal{H}$ if, for every possible labeling $(y_1, \\dots, y_m) \\in \\{0, 1\\}^m$, there exists a hypothesis $h \\in \\mathcal{H}$ such that $h(x_i) = y_i$ for all $i \\in \\{1, \\dots, m\\}$. The hypotheses in our class are $h_t(x) = \\mathbf{1}\\{x \\le t\\}$.\n\nFirst, we show that $\\mathrm{VCdim}(\\mathcal{H}) \\ge 1$. Consider any single point $\\{x_1\\}$. There are $2^1 = 2$ possible labelings: $\\{0\\}$ and $\\{1\\}$.\n*   To obtain the labeling $\\{1\\}$, we need $h_t(x_1) = 1$, which means $x_1 \\le t$. We can choose $t = x_1$.\n*   To obtain the labeling $\\{0\\}$, we need $h_t(x_1) = 0$, which means $x_1 > t$. We can choose $t = x_1 - 1$.\nSince both labelings are achievable, any single point can be shattered. Thus, $\\mathrm{VCdim}(\\mathcal{H}) \\ge 1$.\n\nNext, we show that $\\mathrm{VCdim}(\\mathcal{H}) < 2$. Consider any set of two distinct points, $\\{x_1, x_2\\}$. Without loss of generality, let $x_1 < x_2$. There are $2^2 = 4$ possible labelings: $(0,0), (0,1), (1,0), (1,1)$. Let's see if we can realize all of them.\n*   For $(0,0)$: We need $h_t(x_1)=0$ and $h_t(x_2)=0$. This means $x_1 > t$ and $x_2 > t$. We can choose any $t < x_1$, for example $t=x_1 - 1$.\n*   For $(1,1)$: We need $h_t(x_1)=1$ and $h_t(x_2)=1$. This means $x_1 \\le t$ and $x_2 \\le t$. We can choose any $t \\ge x_2$, for example $t=x_2$.\n*   For $(1,0)$: We need $h_t(x_1)=1$ and $h_t(x_2)=0$. This means $x_1 \\le t$ and $x_2 > t$. We can choose any $t$ such that $x_1 \\le t < x_2$, for example $t=x_1$.\n*   For $(0,1)$: We need $h_t(x_1)=0$ and $h_t(x_2)=1$. This means $x_1 > t$ and $x_2 \\le t$. This would imply $t < x_1$ and $t \\ge x_2$. Since we assumed $x_1 < x_2$, it is impossible to find such a $t$.\n\nBecause the labeling $(0,1)$ cannot be generated for any pair of points $\\{x_1, x_2\\}$ with $x_1 < x_2$, no set of size $2$ can be shattered by $\\mathcal{H}$. Therefore, the VC dimension is $1$.\n\n**Task 2: Sample size $n^{\\star}(\\epsilon,\\delta)$**\nThe uniform deviation that we want to bound is $\\sup_{h \\in \\mathcal{H}} |P(h) - P_n(h)|$. For the given hypothesis class $\\mathcal{H}$, the true risk and empirical risk are:\n$P(h_t) = \\mathbb{E}[\\mathbf{1}\\{X \\le t\\}] = \\Pr(X \\le t) = F(t)$, where $F$ is the CDF of the data-generating distribution $P$.\n$P_n(h_t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\} = \\hat{F}_n(t)$, where $\\hat{F}_n$ is the empirical CDF.\nThus, the uniform deviation is equivalent to the Kolmogorov-Smirnov distance between the true and empirical CDFs:\n$$ \\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big| = \\sup_{t \\in \\mathbb{R}} \\big| F(t) - \\hat{F}_n(t) \\big| $$\nThe problem asks to use the exact-constant form of the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality, which was established by Massart (1990). This inequality states that for any $n \\ge 1$ and any $\\epsilon > 0$,\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\le 2 \\exp(-2n\\epsilon^2). $$\nWe want to find the smallest integer $n^{\\star}(\\epsilon, \\delta)$ such that the probability of the complementary event is at least $1-\\delta$:\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| \\le \\epsilon \\right) \\ge 1 - \\delta. $$\nThis is equivalent to ensuring that the probability of the tail event is at most $\\delta$:\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\le \\delta. $$\nBy applying the DKW inequality, we can satisfy this condition by setting its upper bound to be less than or equal to $\\delta$:\n$ 2 \\exp(-2n\\epsilon^2) \\le \\delta $.\nWe now solve this inequality for $n$:\n$$ \\exp(-2n\\epsilon^2) \\le \\frac{\\delta}{2} $$\nTaking the natural logarithm of both sides:\n$$ -2n\\epsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right) $$\nMultiplying by $-1$ and reversing the inequality sign:\n$$ 2n\\epsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\left(\\frac{\\delta}{2}\\right)^{-1}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right) $$\nFinally, isolating $n$:\n$$ n \\ge \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) $$\nSince $n$ must be an integer, the smallest integer $n$ that satisfies this condition is the ceiling of the right-hand side.\n$$ n^{\\star}(\\epsilon,\\delta) = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil. $$\n\n**Task 3: Optimal exponent constant $c$**\nWe are asked for the largest possible constant $c$ such that for some constant $K$, the following bound holds for all distributions $P$, all sufficiently large $n$, and all $\\epsilon \\in (0, \\epsilon_0]$:\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{h \\in \\mathcal{H}} |P(h) - P_n(h)| > \\epsilon \\right) \\le K \\exp(-cn\\epsilon^2). $$\nThe left-hand side is $\\sup_{P} \\Pr\\!\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right)$. The DKW inequality with constant $c=2$ shows that such a bound exists. To prove that $c=2$ is the optimal (largest possible) value, we must establish a matching lower bound on the rate of decay.\n\nWe follow the hint to reduce the problem to the deviation of a binomial proportion. The supremum over all distributions must be greater than or equal to the probability for any single distribution. Furthermore, the supremum over all thresholds $t$ must be greater than or equal to the deviation at any single threshold $t_0$.\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\ge \\Pr_{P_0}\\!\\left( |F_{0}(t_0) - \\hat{F}_{n,0}(t_0)| > \\epsilon \\right) $$\nfor any specific distribution $P_0$ and threshold $t_0$.\n\nLet's choose $P_0$ to be the uniform distribution on $[0,1]$, i.e., $X \\sim U(0,1)$, for which the CDF is $F_0(t) = t$ for $t \\in [0,1]$. Let's choose the threshold $t_0 = 1/2$, which is the median of this distribution. At this point, $F_0(1/2) = 1/2$.\nThe empirical value is $\\hat{F}_{n,0}(1/2) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le 1/2\\}$. Let $Y_i = \\mathbf{1}\\{X_i \\le 1/2\\}$. Since $X_i \\sim U(0,1)$, the $Y_i$ are i.i.d. Bernoulli random variables with success probability $p=\\Pr(X_i \\le 1/2) = F_0(1/2) = 1/2$.\nThe sum $S_n = \\sum_{i=1}^n Y_i = n \\hat{F}_{n,0}(1/2)$ follows a binomial distribution, $S_n \\sim \\mathrm{Binomial}(n, 1/2)$.\n\nThe deviation at $t_0=1/2$ is $|\\hat{F}_{n,0}(1/2) - F_0(1/2)| = |\\frac{S_n}{n} - \\frac{1}{2}|$.\nThe large deviation behavior of this quantity is well-known. According to Sanov's theorem, the probability $\\Pr(|\\frac{S_n}{n} - \\frac{1}{2}| > \\epsilon)$ decays exponentially with rate given by the Kullback-Leibler (KL) divergence. For large $n$ and small $\\epsilon$,\n$$ \\Pr\\left(\\left|\\frac{S_n}{n} - \\frac{1}{2}\\right| > \\epsilon\\right) \\approx \\exp\\left(-n \\inf_{|q-1/2| \\ge \\epsilon} D_{KL}(q || 1/2)\\right), $$\nwhere $D_{KL}(q || p) = q \\ln(q/p) + (1-q)\\ln((1-q)/(1-p))$. The infimum is achieved at the boundary, e.g., at $q = 1/2 + \\epsilon$. Let's analyze the rate function $D_{KL}(1/2+\\epsilon || 1/2)$:\n$$ D_{KL}(1/2+\\epsilon || 1/2) = \\left(\\frac{1}{2}+\\epsilon\\right) \\ln\\left(\\frac{1/2+\\epsilon}{1/2}\\right) + \\left(\\frac{1}{2}-\\epsilon\\right) \\ln\\left(\\frac{1/2-\\epsilon}{1/2}\\right) $$\n$$ = \\left(\\frac{1}{2}+\\epsilon\\right) \\ln(1+2\\epsilon) + \\left(\\frac{1}{2}-\\epsilon\\right) \\ln(1-2\\epsilon) $$\nUsing the Taylor series expansion $\\ln(1+x) = x - x^2/2 + x^3/3 - \\dots$ for small $x$:\n$$ \\ln(1+2\\epsilon) = 2\\epsilon - \\frac{(2\\epsilon)^2}{2} + O(\\epsilon^3) = 2\\epsilon - 2\\epsilon^2 + O(\\epsilon^3) $$\n$$ \\ln(1-2\\epsilon) = -2\\epsilon - \\frac{(-2\\epsilon)^2}{2} + O(\\epsilon^3) = -2\\epsilon - 2\\epsilon^2 + O(\\epsilon^3) $$\nSubstituting these into the expression for the KL divergence:\n$$ D_{KL}(1/2+\\epsilon || 1/2) \\approx \\left(\\frac{1}{2}+\\epsilon\\right)(2\\epsilon - 2\\epsilon^2) + \\left(\\frac{1}{2}-\\epsilon\\right)(-2\\epsilon - 2\\epsilon^2) $$\n$$ = (\\epsilon - \\epsilon^2 + 2\\epsilon^2 - 2\\epsilon^3) + (-\\epsilon - \\epsilon^2 + 2\\epsilon^2 + 2\\epsilon^3) + O(\\epsilon^4) $$\n$$ = 2\\epsilon^2 + O(\\epsilon^4) $$\nFor small $\\epsilon$, the dominant term in the large deviation rate is $2\\epsilon^2$. This implies that for large $n$, there exists a constant $C'>0$ such that\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{t} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\ge \\Pr_{U(0,1)}\\!\\left( \\left|\\hat{F}_n(1/2) - 1/2\\right| > \\epsilon \\right) \\ge C' \\exp(-2n\\epsilon^2). $$\nIf we have an upper bound of the form $K \\exp(-cn\\epsilon^2)$, it must accommodate this lower bound. For the inequality $C' \\exp(-2n\\epsilon^2) \\le K \\exp(-cn\\epsilon^2)$ to hold for all large $n$, the exponential decay rate of the upper bound cannot be faster than that of the lower bound. This requires $-c \\ge -2$, which implies $c \\le 2$.\n\nSince the DKW inequality provides an upper bound with $c=2$, and our analysis of a specific case shows that no $c>2$ is possible, the largest possible value for the constant $c$ is $2$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil & 2 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The ultimate goal of learning theory is to guide the design of better algorithms. This final exercise applies the advanced tool of Rademacher complexity to analyze one of the most important choices in modern machine learning: the type of regularization to use. By comparing the complexity of hypothesis classes defined by $\\ell_1$ and $\\ell_2$ norm constraints, you will uncover the geometric reasons behind the effectiveness of sparsity-inducing regularization in high-dimensional spaces .",
            "id": "3189970",
            "problem": "Consider linear classification with real-valued scores of the form $f_{\\mathbf{w}}(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle$ on $\\mathbb{R}^d$. You are given a fixed sample $S = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ with $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\{-1, +1\\}$. Assume that for all $i \\in \\{1,\\dots,n\\}$, the feature vectors satisfy $\\|\\mathbf{x}_i\\|_2 \\le R_2$ and $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ for some known radii $R_2, R_\\infty > 0$. Consider the two hypothesis classes\n$\\mathcal{H}_2 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_2 \\le B_2\\}$ and $\\mathcal{H}_1 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_1 \\le B_1\\}$,\nwith radii $B_2, B_1 > 0$. Let $\\ell:\\mathbb{R}\\times\\{-1,+1\\}\\to[0,1]$ be a loss that is $1$-Lipschitz in its first argument. Uniform convergence for $\\ell \\circ \\mathcal{H}_p$ can be controlled via the empirical Rademacher complexity and concentration inequalities.\n\nWhich option correctly characterizes how the empirical Rademacher complexities of $\\mathcal{H}_2$ and $\\mathcal{H}_1$ scale with $n$, $d$, $R_2$, and $R_\\infty$, and correctly interprets the geometric reason for any dimension dependence?\n\nA. For any such sample, the empirical Rademacher complexity of $\\mathcal{H}_2$ satisfies $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_2) \\le \\frac{B_2 R_2}{\\sqrt{n}}$, while that of $\\mathcal{H}_1$ satisfies $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$. Thus, the $\\ell_1$ class incurs a $\\sqrt{\\log d}$ factor because its dual norm is $\\ell_\\infty$ (max over coordinates), whereas the $\\ell_2$ class has no explicit $d$ in its bound. Using $\\|\\mathbf{x}\\|_2 \\le \\sqrt{d}\\,\\|\\mathbf{x}\\|_\\infty$, if $B_1 \\approx B_2$ and $d$ is large, the $\\ell_1$ class can have a smaller complexity by about a factor $\\sqrt{d/\\log d}$.\n\nB. If $\\|\\mathbf{x}_i\\|_2 \\le R_2$, then $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2}{\\sqrt{n}}$ (dimension-free), because the dual norm of $\\ell_1$ is $\\ell_2$; consequently, $\\ell_1$ regularization eliminates any dependence on $d$.\n\nC. When $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$, the $\\ell_2$ class necessarily exhibits a $\\sqrt{\\log d}$ factor in its Rademacher complexity due to taking a maximum over coordinates, while the $\\ell_1$ class is dimension-free under the same condition.\n\nD. Without any bound on $\\|\\mathbf{x}_i\\|_\\infty$, the $\\ell_1$ and $\\ell_2$ classes admit the same empirical Rademacher complexity, up to universal constants, provided only that $\\|\\mathbf{x}_i\\|_2 \\le R_2$.",
            "solution": "We will proceed by first deriving the relevant bounds on the empirical Rademacher complexities and then evaluating each option.\n\nThe empirical Rademacher complexity of a function class $\\mathcal{F}$ over a sample $S = \\{z_1, \\dots, z_n\\}$ is defined as:\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i f(z_i) \\right] $$\nwhere $\\sigma_1, \\dots, \\sigma_n$ are independent Rademacher random variables, i.e., $\\mathbb{P}(\\sigma_i = 1) = \\mathbb{P}(\\sigma_i = -1) = 1/2$.\n\nIn this problem, the functions are compositions of a loss function $\\ell$ and a linear function class $\\mathcal{H}_p$. Let the full function class be $\\mathcal{L}_p = \\{(\\mathbf{x}, y) \\mapsto \\ell(\\langle \\mathbf{w}, \\mathbf{x} \\rangle, y) \\mid \\mathbf{w} \\in \\mathcal{H}_p \\}$. The sample is $S = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$.\nThe loss function $\\ell$ is $1$-Lipschitz in its first argument. By the Ledoux-Talagrand contraction inequality, the Rademacher complexity of the composite class $\\mathcal{L}_p$ is bounded by the Rademacher complexity of the underlying linear function class. Let $\\mathcal{F}_p = \\{\\mathbf{x} \\mapsto \\langle \\mathbf{w}, \\mathbf{x} \\rangle \\mid \\mathbf{w} \\in \\mathcal{H}_p \\}$. The contraction principle gives:\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{L}_p) \\le 1 \\cdot \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) $$\nWe compute $\\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p)$ based on the data points $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$:\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\mathbf{w} \\in \\mathcal{H}_p} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle \\right] $$\nBy linearity of the inner product, this can be written as:\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) = \\frac{1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\mathbf{w} \\in \\mathcal{H}_p} \\left\\langle \\mathbf{w}, \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\rangle \\right] $$\nThe supremum $\\sup_{\\|\\mathbf{w}\\|_p \\le B_p} \\langle \\mathbf{w}, \\mathbf{v} \\rangle$ is equal to $B_p \\|\\mathbf{v}\\|_{p^*}$, where $\\|\\cdot\\|_{p^*}$ is the dual norm to $\\|\\cdot\\|_p$. This gives the general formula:\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) = \\frac{B_p}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_{p^*} \\right] $$\n\nNow we analyze the two specific cases.\n\n**Case 1: The $\\ell_2$ class, $\\mathcal{H}_2 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_2 \\le B_2\\}$**\nHere, $p=2$, and the dual norm is also the $\\ell_2$ norm since $(p^*) = 2$.\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_2) = \\frac{B_2}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_2 \\right] $$\nUsing Jensen's inequality ($\\mathbb{E}[X] \\le \\sqrt{\\mathbb{E}[X^2]}$):\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_2 \\right] \\le \\sqrt{ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_2^2 \\right] } $$\nThe term inside the square root is:\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\langle \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i, \\sum_{j=1}^n \\sigma_j \\mathbf{x}_j \\right\\rangle \\right] = \\sum_{i=1}^n \\sum_{j=1}^n \\mathbb{E}[\\sigma_i \\sigma_j] \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle = \\sum_{i=1}^n \\|\\mathbf{x}_i\\|_2^2 $$\nThis uses the fact that $\\mathbb{E}[\\sigma_i \\sigma_j] = \\delta_{ij}$ because the $\\sigma_i$ are independent and have zero mean.\nGiven $\\|\\mathbf{x}_i\\|_2 \\le R_2$, we have $\\sum_{i=1}^n \\|\\mathbf{x}_i\\|_2^2 \\le n R_2^2$.\nThus,\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_2) \\le \\frac{B_2}{n} \\sqrt{n R_2^2} = \\frac{B_2 R_2}{\\sqrt{n}} $$\nThis provides a dimension-free bound for the complexity of $\\mathcal{H}_2$.\n\n**Case 2: The $\\ell_1$ class, $\\mathcal{H}_1 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_1 \\le B_1\\}$**\nHere, $p=1$, and the dual norm is the $\\ell_\\infty$ norm, $p^* = \\infty$.\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_1) = \\frac{B_1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_\\infty \\right] $$\nThe $\\ell_\\infty$ norm is the maximum absolute value of the components. Let $\\mathbf{z} = \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i$. Then $\\|\\mathbf{z}\\|_\\infty = \\max_{j \\in \\{1,\\dots,d\\}} |z_j| = \\max_{j \\in \\{1,\\dots,d\\}} \\left| \\sum_{i=1}^n \\sigma_i x_{i,j} \\right|$.\nThe quantity $\\mathbb{E}[\\max_j |\\sum_i \\sigma_i x_{i,j}|]$ can be bounded using standard results for the maximum of sub-Gaussian variables. A well-known bound (related to Massart's Lemma) is:\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\max_{j=1,\\dots,d} \\left| \\sum_{i=1}^n \\sigma_i x_{i,j} \\right| \\right] \\le \\sqrt{2 \\log(2d)} \\cdot \\max_{j=1,\\dots,d} \\sqrt{\\sum_{i=1}^n x_{i,j}^2} $$\nWe are given $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$, which implies $|x_{i,j}| \\le R_\\infty$ for all $i, j$. Therefore, for any $j$, $\\sum_{i=1}^n x_{i,j}^2 \\le \\sum_{i=1}^n R_\\infty^2 = n R_\\infty^2$.\nPlugging this into the bound:\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_\\infty \\right] \\le \\sqrt{2 \\log(2d)} \\cdot \\sqrt{n R_\\infty^2} = R_\\infty \\sqrt{n} \\sqrt{2 \\log(2d)} $$\nThis yields the complexity bound for $\\mathcal{H}_1$:\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_1) \\le \\frac{B_1}{n} \\left( R_\\infty \\sqrt{n} \\sqrt{2 \\log(2d)} \\right) = \\frac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}} $$\nThis bound exhibits a mild logarithmic dependence on the dimension $d$.\n\nNow, we evaluate the options.\n\n**A.** \"For any such sample, the empirical Rademacher complexity of $\\mathcal{H}_2$ satisfies $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_2) \\le \\frac{B_2 R_2}{\\sqrt{n}}$, while that of $\\mathcal{H}_1$ satisfies $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$. Thus, the $\\ell_1$ class incurs a $\\sqrt{\\log d}$ factor because its dual norm is $\\ell_\\infty$ (max over coordinates), whereas the $\\ell_2$ class has no explicit $d$ in its bound. Using $\\|\\mathbf{x}\\|_2 \\le \\sqrt{d}\\,\\|\\mathbf{x}\\|_\\infty$, if $B_1 \\approx B_2$ and $d$ is large, the $\\ell_1$ class can have a smaller complexity by about a factor $\\sqrt{d/\\log d}$.\"\n- The two bounds presented are precisely those derived above using the natural pairings of norm constraints ($B_2, R_2$ and $B_1, R_\\infty$). This part is **Correct**.\n- The explanation of the origin of the $\\sqrt{\\log d}$ factor is also **Correct**: it arises from bounding the maximum over $d$ coordinates inherent in the $\\ell_\\infty$ dual norm, whereas the $\\ell_2$ dual norm calculation avoids this, leading to a dimension-free bound (when data is bounded in $\\ell_2$).\n- The final comparison is a standard argument in high-dimensional statistics. If we assume the data is constrained by $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$, then the tightest available bound for $\\|\\mathbf{x}_i\\|_2$ is $R_2 = \\sqrt{d} R_\\infty$. The corresponding Rademacher bound for $\\mathcal{H}_2$ would then scale as $\\frac{B_2 (\\sqrt{d} R_\\infty)}{\\sqrt{n}}$, i.e., proportional to $\\sqrt{d}$. The bound for $\\mathcal{H}_1$ scales as $\\frac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$, i.e., proportional to $\\sqrt{\\log d}$. For large $d$, the ratio of the $\\mathcal{H}_2$ bound to the $\\mathcal{H}_1$ bound is of order $\\sqrt{d}/\\sqrt{\\log d}$. This implies that the complexity bound for the $\\ell_1$ class is significantly smaller. The statement is carefully phrased as \"can have a smaller complexity,\" which is supported by this analysis of the bounds. This part is **Correct**.\nTherefore, the entire option is correct.\n\n**B.** \"If $\\|\\mathbf{x}_i\\|_2 \\le R_2$, then $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2}{\\sqrt{n}}$ (dimension-free), because the dual norm of $\\ell_1$ is $\\ell_2$; consequently, $\\ell_1$ regularization eliminates any dependence on $d$.\"\n- The inequality $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2}{\\sqrt{n}}$ can be derived. $\\widehat{\\mathfrak{R}}_S(\\mathcal{F}_1) = \\frac{B_1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} [ \\| \\sum_i \\sigma_i \\mathbf{x}_i \\|_\\infty ]$. Since $\\|\\mathbf{v}\\|_\\infty \\le \\|\\mathbf{v}\\|_2$ for any vector $\\mathbf{v}$, we have $\\mathbb{E}[\\|\\cdot\\|_\\infty] \\le \\mathbb{E}[\\|\\cdot\\|_2]$. Using the result from the $\\mathcal{H}_2$ analysis, $\\mathbb{E}[\\| \\sum_i \\sigma_i \\mathbf{x}_i \\|_2] \\le R_2 \\sqrt{n}$. Thus, the inequality holds.\n- However, the justification \"because the dual norm of $\\ell_1$ is $\\ell_2$\" is factually **Incorrect**. The dual norm of $\\ell_1$ is $\\ell_\\infty$. The dual of $\\ell_2$ is $\\ell_2$. This fundamental error invalidates the reasoning.\n\n**C.** \"When $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$, the $\\ell_2$ class necessarily exhibits a $\\sqrt{\\log d}$ factor in its Rademacher complexity due to taking a maximum over coordinates, while the $\\ell_1$ class is dimension-free under the same condition.\"\n- This statement reverses the roles of the two classes. The $\\ell_2$ class complexity bound is related to the $\\ell_2$ dual norm, which does not involve a maximum over coordinates and does not introduce a $\\sqrt{\\log d}$ factor. Using $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ gives a bound for $\\mathcal{H}_2$ proportional to $\\sqrt{d}$ (since $\\|\\mathbf{x}\\|_2 \\le \\sqrt{d}\\|\\mathbf{x}\\|_\\infty$), not $\\sqrt{\\log d}$.\n- Conversely, it is the $\\ell_1$ class that has a complexity bound depending on $\\sqrt{\\log d}$ due to its $\\ell_\\infty$ dual norm, precisely under the condition $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$.\n- The entire statement is **Incorrect**.\n\n**D.** \"Without any bound on $\\|\\mathbf{x}_i\\|_\\infty$, the $\\ell_1$ and $\\ell_2$ classes admit the same empirical Rademacher complexity, up to universal constants, provided only that $\\|\\mathbf{x}_i\\|_2 \\le R_2$.\"\n- Under $\\|\\mathbf{x}_i\\|_2 \\le R_2$, we have the bounds $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_2) \\le \\frac{B_2 R_2}{\\sqrt{n}}$ and, as shown in the analysis of B, $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2}{\\sqrt{n}}$. While the upper bounds look similar, the actual complexities are $\\frac{B_2}{n}\\mathbb{E}[\\|\\sum\\sigma_i\\mathbf{x}_i\\|_2]$ and $\\frac{B_1}{n}\\mathbb{E}[\\|\\sum\\sigma_i\\mathbf{x}_i\\|_\\infty]$. The norms $\\|\\cdot\\|_2$ and $\\|\\cdot\\|_\\infty$ are not equivalent up to universal constants; their relationship depends on dimension $d$ ($1 \\le \\|\\mathbf{v}\\|_2/\\|\\mathbf{v}\\|_\\infty \\le \\sqrt{d}$). As demonstrated by counterexamples (e.g., data vectors being orthogonal vs. collinear), the ratio of the two complexities can depend on $n$ and the data structure, not just a universal constant. The claim is too strong and generally false. For instance, if we use the bound on $\\mathcal{H}_1$ that includes the dimension, derived from $\\|\\mathbf{x}_i\\|_\\infty \\le \\|\\mathbf{x}_i\\|_2 \\le R_2$, we get $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2 \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$, which explicitly differs from the $\\mathcal{H}_2$ bound by a $\\sqrt{\\log d}$ factor. Thus, the complexities are not the same. This option is **Incorrect**.\n\nBased on the analysis, only option A is fully correct.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}