{
    "hands_on_practices": [
        {
            "introduction": "本练习将引导您从第一性原理出发，计算两个基本假设类——单调合取（monotone conjunctions）和奇偶函数（parity functions）的VC维。通过比较它们相同的VC维和截然不同的实际学习特性，您将更深刻地理解VC维作为一种复杂度度量的强大之处及其局限性。这项实践对于掌握如何通过寻找被“打散”（shatter）的集合来确定VC维至关重要。",
            "id": "3138549",
            "problem": "考虑布尔超立方体 $\\{0,1\\}^{d}$，其中 $d \\in \\mathbb{N}$。定义单调合取假设类如下：对于每个子集 $S \\subseteq [d] := \\{1,2,\\dots,d\\}$，假设 $h_{S} : \\{0,1\\}^{d} \\to \\{0,1\\}$ 由 $h_{S}(x) = \\bigwedge_{i \\in S} x_{i}$ 给出，其中 $\\bigwedge$ 表示逻辑与。空合取 $S=\\varnothing$ 被解释为对所有 $x \\in \\{0,1\\}^{d}$ 都有 $h_{\\varnothing}(x)=1$。\n\n从 Vapnik–Chervonenkis (VC) 维的基础定义和打散的概念出发，并且只使用统计学习理论中成熟的结论（如增长函数和一致收敛论证），完成以下任务：\n\n1. 形式化地构建单调合取类 $\\mathcal{H}_{\\mathrm{mc}} := \\{ h_{S} : S \\subseteq [d] \\}$，并从第一性原理出发确定其 Vapnik–Chervonenkis (VC) 维 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}})$。你的推导必须通过一个显式打散集来证明一个匹配的下界，并通过基数或增长函数论证来证明一个上界。\n\n2. 定义奇偶性假设类 $\\mathcal{P} := \\{ g_{S} : S \\subseteq [d] \\}$，其中 $g_{S}(x) = \\bigoplus_{i \\in S} x_{i}$，$\\bigoplus$ 表示模 2 加法。使用相同的基础方法，在常见的准确度参数 $\\epsilon \\in (0,1)$ 和置信度参数 $\\delta \\in (0,1)$ 下，比较在可实现情况下，$\\mathcal{H}_{\\mathrm{mc}}$ 与 $\\mathcal{P}$ 的可能近似正确 (PAC) 学习的样本复杂度。将此比较表示为一个闭式比率 $R(d,\\epsilon,\\delta)$，该比率是根据一个标准的一致收敛 (VC) 界限（该界限相同地应用于两个类）所指示的最小充分样本量的比值；假设界限中的任何通用常数在两个类之间是共享的。\n\n3. 以第一性原理为基础，定性地讨论这些结构性差异如何与噪声敏感性相关联，具体涉及输入中的独立随机比特翻转，以及速率为 $\\eta \\in (0,1)$ 的随机分类噪声。\n\n你的最终答案必须是一个单行矩阵，包含 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}})$ 和比率 $R(d,\\epsilon,\\delta)$，并简化为闭式表达式。不需要四舍五入，最终答案中也不应包含单位。",
            "solution": "该问题陈述是计算学习理论中一个定义明确的练习。所有术语都有正式定义，任务明确，且其科学背景根植于 Vapnik–Chervonenkis (VC) 理论和可能近似正确 (PAC) 学习的基础原理。该问题是有效的，并且可以按所述方式解决。\n\n### 1. 单调合取（$\\mathcal{H}_{\\mathrm{mc}}$）的 VC 维\n\n一个假设类 $\\mathcal{H}$ 的 Vapnik–Chervonenkis (VC) 维，记作 $d_{\\mathrm{VC}}(\\mathcal{H})$，是能被 $\\mathcal{H}$ 打散的最大点集的大小。如果对于点集 $C$ 中点的每一种可能的二分（标记）方式，都存在 $\\mathcal{H}$ 中的一个假设能够实现该标记，那么集合 $C$ 就被 $\\mathcal{H}$ 打散。我们将通过证明匹配的下界和上界来确定 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}})$。\n\n#### 下界：$d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\ge d$\n\n为了建立一个为 $d$ 的下界，我们必须展示一个在 $\\{0,1\\}^{d}$ 中包含 $d$ 个点的集合，该集合能被 $\\mathcal{H}_{\\mathrm{mc}}$ 打散。令 $\\mathbf{1}$ 表示 $\\{0,1\\}^d$ 中全为 1 的向量，并令 $e_i$ 为标准基向量，其在第 $i$ 个位置为 1，其他位置为 0。我们假设集合 $C = \\{x^{(1)}, x^{(2)}, \\dots, x^{(d)}\\}$，其中 $x^{(i)} = \\mathbf{1} - e_i$。因此，每个向量 $x^{(i)}$ 在坐标 $i$ 处有一个唯一的 0，在其他所有位置都是 1。\n\n为了证明 $C$ 被打散，我们必须表明对于任意的标记向量 $y = (y_1, y_2, \\dots, y_d) \\in \\{0,1\\}^d$，都存在一个假设 $h_S \\in \\mathcal{H}_{\\mathrm{mc}}$，使得对所有 $i \\in \\{1, \\dots, d\\}$ 都有 $h_S(x^{(i)}) = y_i$。\n\n假设 $h_S$ 定义为 $h_S(x) = \\bigwedge_{j \\in S} x_j$。$h_S(x^{(i)})$ 的值由 $x^{(i)}$ 在属于集合 $S$ 的索引上的分量决定。根据构造，$x^{(i)}$ 向量的分量对于所有 $j \\neq i$ 都有 $(x^{(i)})_j=1$，且 $(x^{(i)})_i=0$。\n\n因此，当且仅当存在至少一个索引 $j \\in S$ 使得 $(x^{(i)})_j = 0$ 时，合取 $\\bigwedge_{j \\in S} (x^{(i)})_j$ 等于 0。这当且仅当索引 $i$（即 $x^{(i)}$ 中唯一 0 的位置）是 $S$ 的一个元素时发生。\n形式上：\n$$\nh_S(x^{(i)}) = 0 \\iff \\exists j \\in S \\text{ such that } (x^{(i)})_j=0 \\iff i \\in S\n$$\n等价地，$h_S(x^{(i)}) = 1 \\iff i \\notin S$。\n\n我们的目标是找到一个集合 $S$ 来生成所需的标记 $y$。我们要求对每个 $i \\in \\{1, \\dots, d\\}$ 都有 $h_S(x^{(i)}) = y_i$。基于上述逻辑，我们必须满足条件：\n$$\ny_i = 0 \\iff i \\in S\n$$\n这个条件唯一地定义了所需的集合 $S$。我们选择 $S = \\{i \\in \\{1, \\dots, d\\} \\mid y_i = 0\\}$。\n\n选择了这个 $S$ 后，我们来验证标记：\n- 如果我们需要用 $y_k=0$ 来标记 $x^{(k)}$，我们的构造将 $k$ 放入 $S$ 中。那么 $h_S(x^{(k)})=0$，这是正确的。\n- 如果我们需要用 $y_j=1$ 来标记 $x^{(j)}$，我们的构造确保 $j \\notin S$。那么 $h_S(x^{(j)})=1$，这也是正确的。\n\n因为对于 $2^d$ 种可能的标记 $y \\in \\{0,1\\}^d$ 中的任何一种，我们都可以构造出相应的假设 $h_S \\in \\mathcal{H}_{\\mathrm{mc}}$，所以大小为 $d$ 的集合 $C$ 被打散了。这证明了 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\ge d$。\n\n#### 上界：$d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\le d$\n\n学习理论中的一个标准结果指出，对于任何有限假设类 $\\mathcal{H}$，其 VC 维受其基数的对数限制：$d_{\\mathrm{VC}}(\\mathcal{H}) \\le \\log_2(|\\mathcal{H}|)$。\n\n单调合取类 $\\mathcal{H}_{\\mathrm{mc}} = \\{h_S : S \\subseteq [d]\\}$ 是通过选择坐标 $[d] = \\{1, 2, \\dots, d\\}$ 的一个子集来构建的。这样的子集数量为 $2^d$。每个不同的子集 $S$ 定义了一个唯一的假设函数。因此，该假设类的基数为 $|\\mathcal{H}_{\\mathrm{mc}}| = 2^d$。\n\n应用基数界限，我们得到：\n$$\nd_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\le \\log_2(|\\mathcal{H}_{\\mathrm{mc}}|) = \\log_2(2^d) = d\n$$\n这证明了任何大小为 $d+1$ 的集合都不能被打散，因此 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\le d$。\n\n结合下界和上界，我们得出结论，单调合取类的 VC 维恰好是 $d$。\n$$\nd_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) = d\n$$\n\n### 2. 样本复杂度比较\n\n为了比较 $\\mathcal{H}_{\\mathrm{mc}}$ 和奇偶性类 $\\mathcal{P}$ 的样本复杂度，我们首先需要确定 $\\mathcal{P}$ 的 VC 维。奇偶性假设类是 $\\mathcal{P} := \\{ g_{S} : S \\subseteq [d] \\}$，其中 $g_{S}(x) = \\bigoplus_{i \\in S} x_{i}$。\n\n#### 奇偶性类 ($\\mathcal{P}$) 的 VC 维\n\n与对 $\\mathcal{H}_{\\mathrm{mc}}$ 的分析类似，我们建立下界和上界。\n\n**下界：$d_{\\mathrm{VC}}(\\mathcal{P}) \\ge d$**\n我们展示一个能被 $\\mathcal{P}$ 打散的包含 $d$ 个点的集合。考虑标准基向量集合 $C = \\{e_1, e_2, \\dots, e_d\\}$。\n对于一个任意的标记 $y = (y_1, y_2, \\dots, y_d) \\in \\{0,1\\}^d$，我们必须找到一个集合 $S \\subseteq [d]$，使得对所有 $i \\in \\{1, \\dots, d\\}$ 都有 $g_S(e_i) = y_i$。\n假设 $g_S$ 应用于 $e_i$ 的结果是 $g_S(e_i) = \\bigoplus_{j \\in S} (e_i)_j$。由于只有当 $j=i$ 时 $(e_i)_j=1$ 且在其他情况下为 0，所以如果 $i \\in S$，模 2 加法的结果是 1，如果 $i \\notin S$，结果是 0。\n$$\ng_S(e_i) = 1 \\iff i \\in S\n$$\n为了实现标记 $y$，我们要求 $g_S(e_i) = y_i$，这意味着我们必须满足条件：\n$$\ny_i = 1 \\iff i \\in S\n$$\n这唯一地定义了所需的集合 $S = \\{i \\in \\{1, \\dots, d\\} \\mid y_i = 1\\}$。由于对于每个可能的标记向量 $y$ 都存在这样一个唯一的集合，所以集合 $C$ 被 $\\mathcal{P}$ 打散。因此，$d_{\\mathrm{VC}}(\\mathcal{P}) \\ge d$。\n\n**上界：$d_{\\mathrm{VC}}(\\mathcal{P}) \\le d$**\n奇偶性类 $\\mathcal{P}$ 也是由 $[d]$ 的所有子集定义的，所以其基数为 $|\\mathcal{P}| = 2^d$。应用与之前相同的基数界限：\n$$\nd_{\\mathrm{VC}}(\\mathcal{P}) \\le \\log_2(|\\mathcal{P}|) = \\log_2(2^d) = d\n$$\n我们得出结论，$d_{\\mathrm{VC}}(\\mathcal{P}) = d$。\n\n#### 样本量比率\n\n在可实现情况下，PAC 学习的样本复杂度由依赖于假设类的 VC 维、准确度参数 $\\epsilon$ 和置信度参数 $\\delta$ 的界限给出。一个常见的标准界限形式如下：\n$$\nN(\\mathcal{H}, \\epsilon, \\delta) \\ge \\frac{K}{\\epsilon} \\left(d_{\\mathrm{VC}}(\\mathcal{H}) \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\n$$\n其中 $K$ 是一个通用常数。问题要求我们将一个相同的标准界限应用于两个类。\n\n对于单调合取类 $\\mathcal{H}_{\\mathrm{mc}}$，最小充分样本量 $N_{\\mathrm{mc}}$ 由此界限确定，其中 $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) = d$：\n$$\nN_{\\mathrm{mc}}(d,\\epsilon,\\delta) = \\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\n$$\n对于奇偶性类 $\\mathcal{P}$，最小充分样本量 $N_{\\mathcal{P}}$ 由相同的界限确定，其中 $d_{\\mathrm{VC}}(\\mathcal{P}) = d$：\n$$\nN_{\\mathcal{P}}(d,\\epsilon,\\delta) = \\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\n$$\n由于两个类具有相同的 VC 维，任何仅依赖于 $d_{\\mathrm{VC}}$、$\\epsilon$ 和 $\\delta$ 的标准样本复杂度界限都将为两者产生相同的值。\n\n因此，比率 $R(d,\\epsilon,\\delta)$ 为：\n$$\nR(d,\\epsilon,\\delta) = \\frac{N_{\\mathrm{mc}}(d,\\epsilon,\\delta)}{N_{\\mathcal{P}}(d,\\epsilon,\\delta)} = \\frac{\\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)}{\\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)} = 1\n$$\n这表明，从由 VC 维衡量的信息论样本复杂度的角度来看，这两个类是无法区分的。\n\n### 3. 关于噪声敏感性的定性讨论\n\n相同的 VC 维和由此产生的样本复杂度界限掩盖了这两个类之间深刻的结构性差异，这些差异在考虑噪声时变得明显。\n\n**输入中的独立随机比特翻转：**\n考虑一个输入 $x$ 和一个由 $S$ 参数化的假设。令 $x'$ 是通过翻转单个比特 $x_k$ 形成的。\n\n- 对于**单调合取（$h_S$）**：输出 $h_S(x) = \\bigwedge_{i \\in S} x_i$ 仅在 $k \\in S$ 时受翻转 $x_k$ 的影响。如果 $k \\notin S$，该函数对翻转完全不敏感。如果 $k \\in S$，函数的输出可能会翻转。一个关键特征是其非均匀的敏感性。对于一个输入 $x$，如果 $h_S(x)=1$（即对于所有 $i \\in S$，都有 $x_i=1$），那么将任何一个这样的 $x_k$ 翻转为 0 都保证输出变为 0。然而，对于一个输入，如果 $h_S(x)=0$，单个比特翻转可能改变也可能不改变输出，这取决于是否存在其他比特 $x_j$（对于 $j \\in S$）也为 0。该函数对 $S$ 之外的属性翻转具有鲁棒性。\n\n- 对于**奇偶性（$g_S$）**：输出 $g_S(x) = \\bigoplus_{i \\in S} x_i$ 也仅在 $k \\in S$ 时受翻转 $x_k$ 的影响。然而，其效果截然不同。如果 $k \\in S$，将 $x_k$ 翻转为 $1-x_k$ *总是*会翻转函数的输出，因为 $g_S(x') = g_S(x) \\oplus 1$。这意味着对于*每个*输入点 $x$，奇偶性函数对任何相关属性的比特翻转都具有最大的敏感性。函数的输出会随着输入的变化而迅速振荡。\n\n**随机分类噪声 (RCN)：**\n这是指真实标签 $y$ 以一定概率 $\\eta$ 被翻转为 $1-y$ 的情况。\n\n- **单调合取**的结构意味着一种平滑性。如果一个输入 $x$ 被 $h_S$ 标记为 1，那么任何按位大于或等于 $x$ 的输入 $x'$（即 $x'_i \\ge x_i, \\forall i$）也将被标记为 1。这意味着在汉明距离上相近的点很可能共享相同的标签。学习算法可以利用这种局部相关性；例如，它可以将在一个统一标记的邻居“海洋”中具有意外标签的孤立点视为可能被噪声破坏的点。\n\n- **奇偶性**函数的结构没有这种平滑性。任何汉明距离为 1 的两个点（如果差异比特在 $S$ 中）都具有相反的标签。标签之间没有局部相关性。一个点及其最近的邻居提供了一种棋盘状的标签模式。在存在随机分类噪声的情况下，学习算法没有局部结构可以利用来区分真实标签和被翻转的标签。任何标记在局部看起来都是合理的。这使得在不可知（容忍噪声）的情况下，学习奇偶性在计算上非常困难，而这一点并未被只衡量信息论复杂度的 VC 维所捕捉。存在在噪声下学习合取的有效算法，而已知在有噪声的情况下学习奇偶性对于有效算法来说是计算上困难的。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nd  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "虽然VC维是分析二元分类的经典工具，但它不适用于实值函数。本练习探讨了一个拥有无限VC维但胖打散维度（fat-shattering dimension）有限的假设类，后者是更适合回归问题的复杂度度量。您将推导这种依赖于尺度$ \\gamma $的复杂度，并用它来为回归任务的泛化误差建立一个上界。",
            "id": "3138564",
            "problem": "考虑一个定义在输入空间上的实值假设类，该输入空间赋有一个来自再生核希尔伯特空间（RKHS）的特征映射。设输入域为 $\\mathcal{X} \\subset \\mathbb{R}^{d}$，配备高斯核 $k(x,x') = \\exp\\!\\big(-\\|x-x'\\|^{2}/(2\\sigma^{2})\\big)$，并设 $\\phi:\\mathcal{X}\\to\\mathcal{H}$ 是到 RKHS $\\mathcal{H}$ 的关联特征映射。已知高斯核是通用的，特别地，对于任何不同的点，其格拉姆矩阵（Gram matrix）是严格正定的。假设对于所有 $x \\in \\mathcal{X}$，都有 $\\|\\phi(x)\\|_{\\mathcal{H}} = \\sqrt{k(x,x)} = 1$。定义假设类\n$$\nH_{B} \\;=\\; \\{\\, f_{w}(x) = \\langle w, \\phi(x) \\rangle_{\\mathcal{H}} \\;:\\; \\|w\\|_{\\mathcal{H}} \\leq B \\,\\},\n$$\n其中 $B  0$ 是一个固定的半径。\n\n您将研究基于核心定义和经过充分检验的结果的三个方面：Vapnik–Chervonenkis (VC) 维度、尺度为 $\\gamma$ 的 fat-shattering，以及在绝对损失下的回归泛化率。对于二元分类，考虑符号类 $\\mathrm{sign}(H_{B}) = \\{\\, \\mathrm{sign}(f_{w}(x)) : f_{w}\\in H_{B} \\,\\}$。对于回归，假设输出 $y \\in [-1,1]$ 并使用绝对损失 $\\ell(y, f(x)) = |\\,y - f(x)\\,|$。\n\n任务：\n- 利用高斯 RKHS 的通用性和不同点的格拉姆矩阵的正定性，构建 $H_{B}$，使得二元类 $\\mathrm{sign}(H_{B})$ 具有无限的 Vapnik–Chervonenkis (VC) 维度，而实值类 $H_{B}$ 在任何固定尺度 $\\gamma  0$ 下具有有限的 fat-shattering 维度。\n- 从尺度为 $\\gamma$ 的 fat-shattering 的定义和希尔伯特空间中线性泛函的标准范数-间隔关系出发，推导出 $H_{B}$ 在尺度为 $\\gamma$ 时的 fat-shattering 维度的显式上界，用 $B$ 和 $\\gamma$ 表示。\n- 从经验风险和期望风险的定义、对称化原理以及线性函数类的 Rademacher 复杂度出发，推导出在绝对损失下 $H_{B}$ 上的期望一致泛化差距，用样本大小 $n$ 和半径 $B$ 表示。然后，确定保证此期望一致差距至多为 $\\gamma$ 的最小样本大小 $n$。\n\n您的最终答案必须是关于最小样本量 $n$ 的一个单一闭式表达式，用 $B$ 和 $\\gamma$ 表示。不需要单位。如果需要四舍五入，请说明四舍五入的有效数字；然而，预期的答案是精确的，不需要四舍五入。",
            "solution": "在尝试解决问题之前，对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- 输入空间: $\\mathcal{X} \\subset \\mathbb{R}^{d}$。\n- 核函数：高斯核, $k(x,x') = \\exp(-\\|x-x'\\|^{2}/(2\\sigma^{2}))$。\n- 特征映射: $\\phi:\\mathcal{X}\\to\\mathcal{H}$ 到一个再生核希尔伯特空间 (RKHS) $\\mathcal{H}$。\n- 核函数属性：通用性；对于不同的点，格拉姆矩阵是严格正定的。\n- 归一化: $\\|\\phi(x)\\|_{\\mathcal{H}} = \\sqrt{k(x,x)} = 1$ 对所有 $x \\in \\mathcal{X}$。\n- 假设类: $H_{B} = \\{\\, f_{w}(x) = \\langle w, \\phi(x) \\rangle_{\\mathcal{H}} \\;:\\; \\|w\\|_{\\mathcal{H}} \\leq B \\,\\}$。\n- 参数: $B  0$。\n- 二元假设类: $\\mathrm{sign}(H_{B}) = \\{\\, \\mathrm{sign}(f_{w}(x)) : f_{w}\\in H_{B} \\,\\}$。\n- 回归目标: $y \\in [-1,1]$。\n- 损失函数：绝对损失, $\\ell(y, f(x)) = |y - f(x)|$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题在统计学习理论的既定框架内是良定义的。它涉及 RKHS、核方法、Vapnik–Chervonenkis (VC) 维度、fat-shattering 维度和 Rademacher 复杂度等标准概念。所有已知条件都是一致的，并且足以进行所需的推导。\n- **科学依据：** 该问题基于机器学习理论的基本原理。高斯核和 RKHS 的性质被正确陈述。\n- **良定性：** 任务被明确指定，并能导出一个唯一、可推导的解。\n- **客观性：** 问题以精确的数学语言陈述，没有歧义或主观性。\n- **完整性与一致性：** 所提供的信息是自洽的，没有矛盾。条件 $\\|\\phi(x)\\|_{\\mathcal{H}} = 1$ 与高斯核的定义一致，因为 $k(x,x)=\\exp(-\\|x-x\\|^2/(2\\sigma^2)) = \\exp(0) = 1$。\n\n### 步骤 3：结论与行动\n该问题被判定为**有效**。将提供一个完整的、有理有据的解答。\n\n***\n\n### 解答推导\n\n解答过程将通过处理问题陈述中提出的三个任务来进行。\n\n**任务 1：VC 维度和 Fat-Shattering 维度分析**\n\n第一个任务是解释为什么二元类 $\\mathrm{sign}(H_{B})$ 具有无限的 VC 维度，而实值类 $H_{B}$ 在任何固定尺度 $\\gamma  0$ 下具有有限的 fat-shattering 维度。\n\n*   **$\\mathrm{sign}(H_{B})$ 的 Vapnik–Chervonenkis 维度**：\n    一个假设类的 VC 维度是该类可以打散（shatter）的点的最大数量 $n$。要打散一组点 $\\{x_1, \\dots, x_n\\}$，对于任意的标签 $(y_1, \\dots, y_n) \\in \\{-1, 1\\}^n$，必须存在一个函数 $f_w \\in H_B$ 使得对于所有 $i \\in \\{1, \\dots, n\\}$，都有 $\\mathrm{sign}(f_w(x_i)) = y_i$。\n    问题陈述指出，对于任何一组不同的点，其格拉姆矩阵是严格正定的。这意味着对于任何 $n$ 个不同的点 $\\{x_1, \\dots, x_n\\}$，特征向量 $\\{\\phi(x_1), \\dots, \\phi(x_n)\\}$ 在 $\\mathcal{H}$ 中是线性无关的。\n    由于这种线性无关性，对于任何标签 $(y_1, \\dots, y_n)$，我们可以找到一个向量 $w_0 \\in \\mathcal{H}$，使得插值条件 $\\langle w_0, \\phi(x_i) \\rangle_{\\mathcal{H}} = y_i$ 对所有 $i$ 都成立。这个 $w_0$ 可以构造为特征向量的线性组合，$w_0 = \\sum_{j=1}^n c_j \\phi(x_j)$，其中系数 $c_j$ 通过求解一个线性系统得到。\n    由于 $y_i \\in \\{-1, 1\\}$，$w_0$ 不可能是零向量，所以 $\\|w_0\\|_{\\mathcal{H}}  0$。我们定义一个新向量 $w = \\alpha w_0$，其中标量 $\\alpha  0$。该函数为 $f_w(x) = \\langle \\alpha w_0, \\phi(x) \\rangle_{\\mathcal{H}} = \\alpha \\langle w_0, \\phi(x) \\rangle_{\\mathcal{H}}$。\n    该函数在 $x_i$ 处的输出符号为 $\\mathrm{sign}(f_w(x_i)) = \\mathrm{sign}(\\alpha y_i) = y_i$。为确保 $f_w \\in H_B$，我们必须满足范数约束 $\\|w\\|_{\\mathcal{H}} \\le B$。这意味着 $\\|\\alpha w_0\\|_{\\mathcal{H}} = \\alpha \\|w_0\\|_{\\mathcal{H}} \\le B$。因为 $B0$ 且 $\\|w_0\\|_{\\mathcal{H}}0$，我们总可以取一个足够小的 $\\alpha  0$，例如 $\\alpha = B/\\|w_0\\|_{\\mathcal{H}}$，来满足这个约束。\n    这个过程对任何有限数量的点 $n$ 都有效。因此，任何有限点集都可以被 $\\mathrm{sign}(H_{B})$ 打散，这意味着其 VC 维度是无限的。\n\n*   **$H_{B}$ 的 Fat-Shattering 维度**：\n    由于范数约束 $\\|w\\|_{\\mathcal{H}} \\le B$，fat-shattering 维度是有限的。如果函数值可以被推到某个阈值之上或之下至少 $\\gamma$ 的距离，那么这组点就是被 $\\gamma$-打散的。对 $w$ 的范数约束限制了函数 $\\langle w, \\phi(x) \\rangle_{\\mathcal{H}}$ 的变化幅度，这反过来又限制了其 $\\gamma$-打散任意大集合的能力。下一个任务通过推导一个显式上界，为这种有限性提供了定量证明。\n\n**任务 2：Fat-Shattering 维度的上界**\n\n设 $\\mathrm{fat}_{H_B}(\\gamma)$ 为 $H_B$ 在尺度 $\\gamma$ 下的 fat-shattering 维度。设 $\\{x_1, \\dots, x_n\\}$ 是被 $H_B$ $\\gamma$-打散的一组 $n$ 个点。根据定义，存在实数（见证）$r_1, \\dots, r_n$，使得对于任意二元向量 $b = (b_1, \\dots, b_n) \\in \\{-1, 1\\}^n$，都存在一个函数 $f_{w_b} \\in H_B$（即 $\\|w_b\\|_{\\mathcal{H}} \\le B$），满足对于所有 $i \\in \\{1, \\dots, n\\}$：\n$b_i (f_{w_b}(x_i) - r_i) \\ge \\gamma$。\n这个不等式对于向量 $-b \\in \\{-1, 1\\}^n$ 也成立，所以存在一个 $w_{-b}$ 且 $\\|w_{-b}\\|_{\\mathcal{H}} \\le B$ 使得：\n$(-b_i) (f_{w_{-b}}(x_i) - r_i) \\ge \\gamma$。\n重写第二个不等式得到 $b_i (r_i - f_{w_{-b}}(x_i)) \\ge \\gamma$。\n将两个不等式相加得到：\n$b_i (f_{w_b}(x_i) - r_i) + b_i (r_i - f_{w_{-b}}(x_i)) \\ge 2\\gamma$。\n$b_i (f_{w_b}(x_i) - f_{w_{-b}}(x_i)) \\ge 2\\gamma$。\n代入 $f_w(x) = \\langle w, \\phi(x) \\rangle_{\\mathcal{H}}$：\n$b_i \\langle w_b - w_{-b}, \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge 2\\gamma$。\n令 $v_b = w_b - w_{-b}$。根据三角不等式，$\\|v_b\\|_{\\mathcal{H}} \\le \\|w_b\\|_{\\mathcal{H}} + \\|w_{-b}\\|_{\\mathcal{H}} \\le B + B = 2B$。\n对 $i=1, \\dots, n$ 求和该不等式：\n$\\sum_{i=1}^n b_i \\langle v_b, \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge \\sum_{i=1}^n 2\\gamma = 2n\\gamma$。\n根据内积的线性性质：\n$\\langle v_b, \\sum_{i=1}^n b_i \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge 2n\\gamma$。\n应用柯西-施瓦茨不等式：\n$\\|v_b\\|_{\\mathcal{H}} \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\ge \\langle v_b, \\sum_{i=1}^n b_i \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge 2n\\gamma$。\n使用 $\\|v_b\\|_{\\mathcal{H}}$ 的界：\n$2B \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\ge 2n\\gamma$。\n$B \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\ge n\\gamma$。\n这个不等式对任意 $b \\in \\{-1, 1\\}^n$ 都成立。我们可以对两边平方，然后对一个均匀随机选择的 $b$（其中每个 $b_i$ 是一个独立的 Rademacher 随机变量）取期望：\n$\\mathbb{E}_b \\left[ B^2 \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}}^2 \\right] \\ge n^2\\gamma^2$。\n$B^2 \\mathbb{E}_b \\left[ \\sum_{i=1}^n \\sum_{j=1}^n b_i b_j \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal{H}} \\right] \\ge n^2\\gamma^2$。\n根据期望的线性性质，并使用 $\\mathbb{E}[b_i b_j] = \\delta_{ij}$（克罗内克 δ）：\n$B^2 \\sum_{i=1}^n \\sum_{j=1}^n \\delta_{ij} \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal{H}} \\ge n^2\\gamma^2$。\n$B^2 \\sum_{i=1}^n \\langle \\phi(x_i), \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge n^2\\gamma^2$。\n$B^2 \\sum_{i=1}^n \\|\\phi(x_i)\\|_{\\mathcal{H}}^2 \\ge n^2\\gamma^2$。\n使用已知条件 $\\|\\phi(x)\\|_{\\mathcal{H}} = 1$：\n$B^2 n \\ge n^2\\gamma^2$。\n因为 $n0$，我们可以除以 $n$：$B^2 \\ge n\\gamma^2$。\n求解 $n$，我们得到任何 $\\gamma$-打散集的大小的上界：\n$n \\le \\frac{B^2}{\\gamma^2}$。\n因此，fat-shattering 维度是有限的，其界为 $\\mathrm{fat}_{H_B}(\\gamma) \\le \\frac{B^2}{\\gamma^2}$。\n\n**任务 3：泛化差距和所需样本量**\n\n目标是推导绝对损失的期望一致泛化差距，并找到保证该差距至多为 $\\gamma$ 的最小样本量 $n$。\n\n对于一个假设类 $H_B$ 和一个损失函数 $\\ell$，期望一致泛化差距为 $\\mathbb{E}_S[\\sup_{f \\in H_B} |R(f) - \\hat{R}_S(f)|]$，其中 $S$ 是大小为 $n$ 的样本，$R(f) = \\mathbb{E}_{X,Y}[\\ell(Y, f(X))]$ 是真实风险，$\\hat{R}_S(f) = \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i, f(x_i))$ 是经验风险。\n\n1.  **对称化：** 一个标准结果将此差距界定为损失类 $\\mathcal{L}_{H_B} = \\{(x,y) \\mapsto |y-f(x)| : f \\in H_B\\}$ 的 Rademacher 复杂度的两倍。\n    $\\mathbb{E}_S[\\sup_{f \\in H_B} |R(f) - \\hat{R}_S(f)|] \\le 2 \\mathfrak{R}_n(\\mathcal{L}_{H_B})$。\n    损失类的 Rademacher 复杂度为 $\\mathfrak{R}_n(\\mathcal{L}_{H_B}) = \\mathbb{E}_{S, \\epsilon} \\left[ \\sup_{f \\in H_B} \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i |y_i - f(x_i)| \\right]$。\n\n2.  **收缩原理：** 绝对损失函数 $\\ell(y, a) = |y-a|$ 关于其第二个参数 $a$ 是 $1$-Lipschitz 的。Ledoux-Talagrand 收缩原理指出，对于一个 $L$-Lipschitz 函数 $\\psi$，复合类 $\\psi \\circ H_B$ 的 Rademacher 复杂度由 $H_B$ 的 Rademacher 复杂度的 $L$ 倍所界定。这里，$L=1$。\n    $\\mathfrak{R}_n(\\mathcal{L}_{H_B}) \\le 1 \\cdot \\mathfrak{R}_n(H_B)$。\n\n3.  **$H_B$ 的 Rademacher 复杂度：** 现在我们计算原始假设类 $H_B$ 的 Rademacher 复杂度。\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\sup_{f \\in H_B} \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i f(x_i) \\right]$。\n    代入 $f(x_i) = \\langle w, \\phi(x_i) \\rangle_{\\mathcal{H}}$ 且 $\\|w\\|_{\\mathcal{H}} \\le B$：\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\sup_{\\|w\\|_{\\mathcal{H}} \\le B} \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i \\langle w, \\phi(x_i) \\rangle_{\\mathcal{H}} \\right]$。\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\frac{1}{n} \\sup_{\\|w\\|_{\\mathcal{H}} \\le B} \\left\\langle w, \\sum_{i=1}^n \\epsilon_i \\phi(x_i) \\right\\rangle_{\\mathcal{H}} \\right]$。\n    在球 $\\|w\\|_{\\mathcal{H}} \\le B$ 上 $\\langle w, v \\rangle$ 的上确界是 $B\\|v\\|_{\\mathcal{H}}$。因此：\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\frac{B}{n} \\left\\| \\sum_{i=1}^n \\epsilon_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\right]$。\n    使用詹森不等式 (Jensen's inequality)（$\\mathbb{E}[X] \\le \\sqrt{\\mathbb{E}[X^2]}$）：\n    $\\mathfrak{R}_n(H_B) \\le \\frac{B}{n} \\sqrt{\\mathbb{E}_{S, \\epsilon} \\left[ \\left\\| \\sum_{i=1}^n \\epsilon_i \\phi(x_i) \\right\\|_{\\mathcal{H}}^2 \\right]}$。\n    平方根内的项是 $\\mathbb{E}_{S, \\epsilon} \\left[ \\sum_{i,j} \\epsilon_i \\epsilon_j \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal{H}} \\right]$。首先对 $\\epsilon$ 取期望（对于固定的样本 $S$）并使用 $\\mathbb{E}_\\epsilon[\\epsilon_i\\epsilon_j]=\\delta_{ij}$：\n    $\\mathbb{E}_{\\epsilon} \\left[ \\dots \\right] = \\sum_{i=1}^n \\langle \\phi(x_i), \\phi(x_i) \\rangle_{\\mathcal{H}} = \\sum_{i=1}^n \\|\\phi(x_i)\\|_{\\mathcal{H}}^2$。\n    使用已知条件 $\\|\\phi(x_i)\\|_{\\mathcal{H}}=1$，这个和等于 $n$。结果与样本 $S$ 无关，因此对 $S$ 取期望不会改变它。\n    $\\mathfrak{R}_n(H_B) \\le \\frac{B}{n} \\sqrt{n} = \\frac{B}{\\sqrt{n}}$。\n\n4.  **最终界和样本量：** 综合以上步骤，期望一致泛化差距的界为：\n    $\\mathbb{E}_S[\\sup_{f \\in H_B} |R(f) - \\hat{R}_S(f)|] \\le 2 \\mathfrak{R}_n(H_B) \\le \\frac{2B}{\\sqrt{n}}$。\n    为保证此差距至多为 $\\gamma$，我们将上界设为小于或等于 $\\gamma$：\n    $\\frac{2B}{\\sqrt{n}} \\le \\gamma$。\n    求解 $n$：\n    $\\sqrt{n} \\ge \\frac{2B}{\\gamma} \\implies n \\ge \\left(\\frac{2B}{\\gamma}\\right)^2 = \\frac{4B^2}{\\gamma^2}$。\n    保证此界的最小样本量 $n$ 是 $\\frac{4B^2}{\\gamma^2}$。",
            "answer": "$$\\boxed{\\frac{4B^{2}}{\\gamma^{2}}}$$"
        },
        {
            "introduction": "模型的泛化能力不仅取决于假设类的复杂性，还与学习算法本身密切相关。本练习将视角从假设类转向算法的稳定性（algorithmic stability），展示了算法决策——例如在梯度下降中采用提前停止（early stopping）——如何扮演隐式正则化的角色。您将通过推导证明，泛化误差的上界直接取决于训练的迭代次数$T$，从而揭示算法行为对学习结果的深刻影响。",
            "id": "3138528",
            "problem": "设 $S = (z_{1}, \\dots, z_{n})$ 是一个大小为 $n$ 的数据集，其样本从样本空间 $\\mathcal{Z}$ 上的一个未知分布中独立同分布地抽取。考虑一个参数为 $w \\in \\mathbb{R}^{d}$ 的参数化预测器，通过最小化经验风险 $F_{S}(w) = \\frac{1}{n} \\sum_{i=1}^{n} f(w; z_{i})$ 进行训练，其中损失 $f(w; z)$ 满足以下性质：\n- 对于每个 $z \\in \\mathcal{Z}$，函数 $w \\mapsto f(w; z)$ 是凸且可微的。\n- 存在一个常数 $G  0$，使得对于所有 $w, w' \\in \\mathbb{R}^{d}$ 和所有 $z \\in \\mathcal{Z}$，有 $|f(w; z) - f(w'; z)| \\leq G \\|w - w'\\|$，即 $f(\\cdot; z)$ 关于 $w$ 对 $z$ 一致地是 $G$-Lipschitz 的。\n- 经验风险 $F_{S}(w)$ 关于 $w$ 是凸且 $L$-光滑的（对于某个 $L  0$），意味着其梯度是 $L$-Lipschitz 的：对于所有 $w, w' \\in \\mathbb{R}^{d}$，有 $\\|\\nabla F_{S}(w) - \\nabla F_{S}(w')\\| \\leq L \\|w - w'\\|$。\n\n使用确定性梯度下降法进行训练，固定步长为 $\\eta \\in (0, 2/L]$，从一个与数据无关的初始化 $w_{0} = 0$ 开始，生成迭代序列 $w_{t+1}(S) = w_{t}(S) - \\eta \\nabla F_{S}(w_{t}(S))$，其中 $t = 0, 1, 2, \\dots$。将经过 $T$ 次迭代后的早停解定义为 $w_{T}(S)$。\n\n设 $S'$ 是与 $S$ 仅有一个样本不同的任意数据集。如果在第 $T$ 次迭代时，学习算法满足\n$\\sup_{z \\in \\mathcal{Z}} |f(w_{T}(S); z) - f(w_{T}(S'); z)| \\leq \\beta_{T}$，\n则称该算法是参数为 $\\beta_{T}$ 的一致稳定算法。\n仅使用上述假设，推导在第 $T$ 次迭代时早停解的期望泛化误差\n$\\mathbb{E}_{S}\\big[ F(w_{T}(S)) - F_{S}(w_{T}(S)) \\big]$\n的一个闭式上界，该上界用 $n$、$G$、$\\eta$ 和 $T$ 表示，其中 $F(w) = \\mathbb{E}_{z}[f(w; z)]$ 表示总体风险。你的最终答案必须是单一的解析表达式。不需要四舍五入。",
            "solution": "用户希望推导期望泛化误差的一个上界。\n\n### 步骤 1：问题验证\n\n**1.1. 提取已知条件**\n- 数据集：$S = (z_{1}, \\dots, z_{n})$ 从样本空间 $\\mathcal{Z}$ 上的一个未知分布中独立同分布 (i.i.d.) 抽取。\n- 预测器参数：$w \\in \\mathbb{R}^{d}$。\n- 经验风险：$F_{S}(w) = \\frac{1}{n} \\sum_{i=1}^{n} f(w; z_{i})$。\n- 总体风险：$F(w) = \\mathbb{E}_{z}[f(w; z)]$。\n- 损失函数性质：\n    - 对于每个 $z \\in \\mathcal{Z}$，$w \\mapsto f(w; z)$ 是凸且可微的。\n    - 存在一个常数 $G  0$，使得 $f(\\cdot; z)$ 关于 $w$ 对 $z$ 一致地是 $G$-Lipschitz 的：对于所有 $w, w' \\in \\mathbb{R}^{d}$ 和所有 $z \\in \\mathcal{Z}$，有 $|f(w; z) - f(w'; z)| \\leq G \\|w - w'\\|$。\n- 经验风险性质：\n    - $F_{S}(w)$ 是凸的。\n    - $F_{S}(w)$ 关于 $w$ 是 $L$-光滑的（对于某个 $L  0$），意味着 $\\|\\nabla F_{S}(w) - \\nabla F_{S}(w')\\| \\leq L \\|w - w'\\|$。\n- 训练算法：确定性梯度下降。\n    - 初始化：$w_{0} = 0$。\n    - 迭代序列：$w_{t+1}(S) = w_{t}(S) - \\eta \\nabla F_{S}(w_{t}(S))$，其中 $t = 0, 1, 2, \\dots$。\n    - 步长：$\\eta \\in (0, 2/L]$。\n    - 早停解：$w_{T}(S)$。\n- 稳定性：如果对于任何与 $S$ 相差一个样本的数据集 $S'$，都有 $\\sup_{z \\in \\mathcal{Z}} |f(w_{T}(S); z) - f(w_{T}(S'); z)| \\leq \\beta_{T}$，则该算法是参数为 $\\beta_{T}$ 的一致稳定算法。\n- 目标：推导期望泛化误差 $\\mathbb{E}_{S}[ F(w_{T}(S)) - F_{S}(w_{T}(S)) ]$ 的一个闭式上界，用 $n, G, \\eta, T$ 表示。\n\n**1.2. 已知条件的验证**\n该问题在统计学习理论领域内是适定的且有科学依据的。所给的假设（凸性、损失的 Lipschitz 连续性、经验风险的光滑性）和算法（梯度下降）都是标准的。一致稳定性的概念及其与泛化能力的联系是学习理论中的一个基本课题。问题要求一个依赖于 $n, G, \\eta, T$ 但不依赖于 $L$ 的上界。参数 $L$ 用于定义步长 $\\eta$ 的有效范围，如下所示，其显式值对于最终的上界并非必需，因为它的作用是确保 GD 更新步骤的一个关键性质（非扩张性）。所有术语都定义清晰，设定内部一致。\n\n**1.3. 结论**\n该问题是有效的。我们开始求解。\n\n### 步骤 2：解题推导\n\n推导过程主要分为四个步骤。首先，我们将期望泛化误差与一致稳定性参数 $\\beta_T$ 联系起来。其次，我们将 $\\beta_T$ 与在相邻数据集上训练出的参数之间的几何距离 $\\|w_T(S) - w_T(S')\\|$ 联系起来。第三，我们推导这个距离的递推关系。最后，我们求解该递推关系并将结果结合起来，得到最终的上界。\n\n**第 1 部分：用稳定性界定泛化误差**\n\n设 $S = (z_1, \\dots, z_n)$ 为训练集。假设 $w_T(S)$ 的期望泛化误差由 $\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))]$ 给出。\n使用 $F(w)$ 和 $F_S(w)$ 的定义：\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] = \\mathbb{E}_{S}\\left[ \\mathbb{E}_{z'}[f(w_T(S); z')] - \\frac{1}{n}\\sum_{i=1}^{n}f(w_T(S); z_i) \\right]\n$$\n其中 $z'$ 是一个随机变量，与 $z_i$ 同分布且独立于 $S$。根据期望的线性性质以及 $z_i$ 是独立同分布的，我们可以将其写为：\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S}\\left[ \\mathbb{E}_{z'_i}[f(w_T(S); z'_i)] - f(w_T(S); z_i) \\right]\n$$\n其中每个 $z'_i$ 是 $z_i$ 的一个独立副本。设 $S^{(i)}$ 是将数据集 $S$ 中的第 $i$ 个样本 $z_i$ 替换为 $z'_i$ 后得到的数据集。$(S, z'_i)$ 的联合分布与 $(S^{(i)}, z_i)$ 的联合分布相同。因此，$\\mathbb{E}_{S, z'_i}[f(w_T(S); z_i)] = \\mathbb{E}_{S^{(i)}, z_i}[f(w_T(S^{(i)}); z'_i)]$。更简单地说，对于任何函数 $g$，都有 $\\mathbb{E}_{S, z'_i}[g(S, z_i)] = \\mathbb{E}_{S^{(i)}, z_i}[g(S^{(i)}, z'_i)]$。\n期望可以改写为：\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ f(w_T(S); z'_i) - f(w_T(S); z_i) \\right]\n$$\n我们在期望内部加上并减去 $f(w_T(S^{(i)}); z'_i)$：\n$$\n= \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ f(w_T(S); z'_i) - f(w_T(S^{(i)}); z'_i) + f(w_T(S^{(i)}); z'_i) - f(w_T(S); z_i) \\right]\n$$\n最后两项的期望相互抵消：$\\mathbb{E}_{S, z'_i}[f(w_T(S^{(i)}); z'_i)] = \\mathbb{E}_{S, z'_i}[f(w_T(S); z_i)]$。\n我们剩下：\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ f(w_T(S); z'_i) - f(w_T(S^{(i)}); z'_i) \\right]\n$$\n根据一致稳定性的定义，对于任意 $z$，有 $|f(w_T(S); z) - f(w_T(S^{(i)}); z)| \\leq \\beta_T$。因此：\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] \\leq \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ \\beta_T \\right] = \\beta_T\n$$\n所以，问题简化为求 $\\beta_T$ 的一个上界。\n\n**第 2 部分：用参数距离界定稳定性**\n\n设 $S$ 和 $S'$ 是仅相差一个样本的两个数据集。根据一致稳定性的定义，$\\beta_T = \\sup_{S, S', z} |f(w_T(S); z) - f(w_T(S'); z)|$。\n损失函数 $f(\\cdot; z)$ 关于其第一个参数 $w$ 是 $G$-Lipschitz 的。这意味着：\n$$\n|f(w_T(S); z) - f(w_T(S'); z)| \\leq G \\|w_T(S) - w_T(S')\\|\n$$\n该不等式对所有 $z$ 均成立，因此在对 $z$ 取上确界后，我们得到：\n$$\n\\sup_{z \\in \\mathcal{Z}} |f(w_T(S); z) - f(w_T(S'); z)| \\leq G \\|w_T(S) - w_T(S')\\|\n$$\n因此，$\\beta_T \\leq G \\sup_{S, S'} \\|w_T(S) - w_T(S')\\|$，其中上确界是对所有相邻数据集取的。我们的目标现在是界定 $\\|w_T(S) - w_T(S')\\|$。\n\n**第 3 部分：参数距离的递推关系**\n\n设 $\\delta_t = \\|w_t(S) - w_t(S')\\|$。初始条件为 $\\delta_0 = \\|w_0 - w_0\\| = \\|0 - 0\\| = 0$。\n梯度下降的迭代公式为：\n$w_{t+1}(S) = w_t(S) - \\eta \\nabla F_S(w_t(S))$\n$w_{t+1}(S') = w_t(S') - \\eta \\nabla F_{S'}(w_t(S'))$\n在第 $t+1$ 步的距离为：\n$$\n\\delta_{t+1} = \\|w_t(S) - w_t(S') - \\eta ( \\nabla F_S(w_t(S)) - \\nabla F_{S'}(w_t(S')) ) \\|\n$$\n我们在范数内部加上并减去 $\\eta \\nabla F_S(w_t(S'))$，并使用三角不等式：\n$$\n\\delta_{t+1} \\leq \\| w_t(S) - \\eta \\nabla F_S(w_t(S)) - (w_t(S') - \\eta \\nabla F_S(w_t(S'))) \\| + \\| \\eta (\\nabla F_S(w_t(S')) - \\nabla F_{S'}(w_t(S'))) \\|\n$$\n我们来分析第一项。对于凸的、$L$-光滑的函数 $F_S$ 和步长 $\\eta \\in (0, 2/L]$，算子 $T(w) = w - \\eta \\nabla F_S(w)$ 是非扩张的。这意味着 $\\|T(w) - T(w')\\| \\leq \\|w - w'\\|$。\n为了证明这一点，我们使用凸 $L$-光滑函数的梯度的余矫顽性：$\\langle \\nabla F_S(w) - \\nabla F_S(w'), w - w' \\rangle \\geq \\frac{1}{L}\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2$。\n$$\n\\|T(w) - T(w')\\|^2 = \\|(w-w') - \\eta(\\nabla F_S(w) - \\nabla F_S(w'))\\|^2 \\\\\n= \\|w-w'\\|^2 - 2\\eta\\langle w-w', \\nabla F_S(w) - \\nabla F_S(w') \\rangle + \\eta^2\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2 \\\\\n\\leq \\|w-w'\\|^2 - \\frac{2\\eta}{L}\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2 + \\eta^2\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2 \\\\\n= \\|w-w'\\|^2 + \\eta(\\eta - \\frac{2}{L})\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2\n$$\n由于 $\\eta \\in (0, 2/L]$，项 $\\eta(\\eta - 2/L) \\leq 0$。因此，$\\|T(w) - T(w')\\|^2 \\leq \\|w-w'\\|^2$，证明了非扩张性。\n将此性质应用于 $\\delta_{t+1}$ 不等式的第一项：\n$$\n\\| w_t(S) - \\eta \\nabla F_S(w_t(S)) - (w_t(S') - \\eta \\nabla F_S(w_t(S'))) \\| \\leq \\|w_t(S) - w_t(S')\\| = \\delta_t\n$$\n这简化了 $\\delta_{t+1}$ 的递推关系：\n$$\n\\delta_{t+1} \\leq \\delta_t + \\eta \\|\\nabla F_S(w_t(S')) - \\nabla F_{S'}(w_t(S')) \\|\n$$\n现在我们界定梯度差项。假设 $S'$ 与 $S$ 的不同之处在于用 $z'_k$ 替换了 $z_k$。\n$$\n\\nabla F_S(w) - \\nabla F_{S'}(w) = \\frac{1}{n} \\sum_{i=1}^n \\nabla f(w; z_i) - \\frac{1}{n} \\left( \\sum_{i \\neq k} \\nabla f(w; z_i) + \\nabla f(w; z'_k) \\right) = \\frac{1}{n}(\\nabla f(w; z_k) - \\nabla f(w; z'_k))\n$$\n由于 $f(\\cdot; z)$ 是 $G$-Lipschitz 的，其梯度范数必须由 $G$ 界定：$\\|\\nabla f(w; z)\\| \\leq G$。\n$$\n\\|\\nabla F_S(w) - \\nabla F_{S'}(w)\\| \\leq \\frac{1}{n} (\\|\\nabla f(w; z_k)\\| + \\|\\nabla f(w; z'_k)\\|) \\leq \\frac{1}{n}(G + G) = \\frac{2G}{n}\n$$\n这个界与 $w$ 无关。将其代入 $\\delta_{t+1}$ 的递推关系：\n$$\n\\delta_{t+1} \\leq \\delta_t + \\eta \\frac{2G}{n}\n$$\n\n**第 4 部分：求解递推关系并得出最终界**\n\n我们有递推关系 $\\delta_{t+1} \\leq \\delta_t + \\frac{2G\\eta}{n}$ 和初始条件 $\\delta_0 = 0$。将其展开 $T$ 步：\n$$\n\\delta_T \\leq \\delta_{T-1} + \\frac{2G\\eta}{n} \\leq \\delta_{T-2} + 2 \\cdot \\frac{2G\\eta}{n} \\leq \\dots \\leq \\delta_0 + T \\cdot \\frac{2G\\eta}{n}\n$$\n由于 $\\delta_0 = 0$，我们有 $\\delta_T = \\|w_T(S) - w_T(S')\\| \\leq \\frac{2GT\\eta}{n}$。\n现在，我们将其代回 $\\beta_T$ 的界：\n$$\n\\beta_T \\leq G \\sup_{S, S'} \\|w_T(S) - w_T(S')\\| \\leq G \\left( \\frac{2GT\\eta}{n} \\right) = \\frac{2G^2T\\eta}{n}\n$$\n最后，我们用它来界定期望泛化误差：\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] \\leq \\beta_T \\leq \\frac{2G^2T\\eta}{n}\n$$\n这就得到了一个用指定参数 $n, G, \\eta, T$ 表示的闭式上界。",
            "answer": "$$\n\\boxed{\\frac{2G^2T\\eta}{n}}\n$$"
        }
    ]
}