{
    "hands_on_practices": [
        {
            "introduction": "A central question in learning theory is what makes a class of functions learnable. The Vapnik-Chervonenkis (VC) dimension offers a foundational answer by measuring the combinatorial complexity of a hypothesis class. This exercise challenges you to compute the VC dimension from first principles for two classic function classes: monotone conjunctions and parity functions. By comparing their sample complexities, you will gain a deeper appreciation for both the power and the limitations of VC dimension as a measure of learnability, particularly when considering factors like noise sensitivity that are not directly captured by this measure .",
            "id": "3138549",
            "problem": "Consider the Boolean hypercube $\\{0,1\\}^{d}$ with $d \\in \\mathbb{N}$. Define a hypothesis class of monotone conjunctions as follows: for each subset $S \\subseteq [d] := \\{1,2,\\dots,d\\}$, the hypothesis $h_{S} : \\{0,1\\}^{d} \\to \\{0,1\\}$ is given by $h_{S}(x) = \\bigwedge_{i \\in S} x_{i}$, where $\\bigwedge$ denotes the logical AND. The empty conjunction $S=\\varnothing$ is interpreted as $h_{\\varnothing}(x)=1$ for all $x \\in \\{0,1\\}^{d}$.\n\nStarting from the foundational definition of Vapnik–Chervonenkis (VC) dimension and the notion of shattering, and using only well-tested facts from statistical learning theory such as the growth function and uniform convergence arguments, carry out the following tasks:\n\n1. Construct the monotone conjunction class $\\mathcal{H}_{\\mathrm{mc}} := \\{ h_{S} : S \\subseteq [d] \\}$ formally and determine its Vapnik–Chervonenkis (VC) dimension $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}})$ from first principles. Your derivation must justify both a matching lower bound via an explicit shattered set and an upper bound via a cardinality or growth-function argument.\n\n2. Define the parity hypothesis class $\\mathcal{P} := \\{ g_{S} : S \\subseteq [d] \\}$ with $g_{S}(x) = \\bigoplus_{i \\in S} x_{i}$, where $\\bigoplus$ denotes addition modulo $2$. Using the same foundational approach, compare the sample complexity of Probably Approximately Correct (PAC) learning in the realizable setting for $\\mathcal{H}_{\\mathrm{mc}}$ versus $\\mathcal{P}$ at common accuracy parameter $\\epsilon \\in (0,1)$ and confidence parameter $\\delta \\in (0,1)$. Express this comparison as a closed-form ratio $R(d,\\epsilon,\\delta)$ of the minimal sufficient sample sizes indicated by a standard uniform convergence (VC) bound applied identically to both classes; assume any universal constants in the bound are shared between the two classes.\n\n3. Discuss, in qualitative terms grounded in first principles, how these structural differences relate to noise sensitivity under independent random bit flips in the input, and under random classification noise at rate $\\eta \\in (0,1)$.\n\nYour final answer must be a single row matrix containing $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}})$ and the ratio $R(d,\\epsilon,\\delta)$, simplified to a closed-form expression. No rounding is required, and no units are to be included in the final answer.",
            "solution": "The problem statement is a well-posed exercise in computational learning theory. All terms are formally defined, the tasks are unambiguous, and the context is scientifically grounded in the foundational principles of Vapnik–Chervonenkis (VC) theory and Probably Approximately Correct (PAC) learning. The problem is valid and can be solved as stated.\n\n### 1. VC Dimension of Monotone Conjunctions ($\\mathcal{H}_{\\mathrm{mc}}$)\n\nThe Vapnik–Chervonenkis (VC) dimension of a hypothesis class $\\mathcal{H}$, denoted $d_{\\mathrm{VC}}(\\mathcal{H})$, is the size of the largest set of points that can be shattered by $\\mathcal{H}$. A set $C$ is shattered by $\\mathcal{H}$ if for every possible dichotomization (labeling) of the points in $C$, there exists a hypothesis in $\\mathcal{H}$ that realizes that labeling. We will establish $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}})$ by proving matching lower and upper bounds.\n\n#### Lower Bound: $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\ge d$\n\nTo establish a lower bound of $d$, we must exhibit a set of $d$ points in $\\{0,1\\}^{d}$ that is shattered by $\\mathcal{H}_{\\mathrm{mc}}$. Let $\\mathbf{1}$ denote the vector of all ones in $\\{0,1\\}^d$, and let $e_i$ be the standard basis vector with a $1$ in the $i$-th position and $0$s elsewhere. We posit the set $C = \\{x^{(1)}, x^{(2)}, \\dots, x^{(d)}\\}$ where $x^{(i)} = \\mathbf{1} - e_i$. Each vector $x^{(i)}$ thus has a single $0$ at coordinate $i$ and $1$s everywhere else.\n\nTo show that $C$ is shattered, we must demonstrate that for any arbitrary labeling vector $y = (y_1, y_2, \\dots, y_d) \\in \\{0,1\\}^d$, there exists a hypothesis $h_S \\in \\mathcal{H}_{\\mathrm{mc}}$ such that $h_S(x^{(i)}) = y_i$ for all $i \\in \\{1, \\dots, d\\}$.\n\nThe hypothesis $h_S$ is defined as $h_S(x) = \\bigwedge_{j \\in S} x_j$. The value of $h_S(x^{(i)})$ is determined by the components of $x^{(i)}$ at indices belonging to the set $S$. By construction, the vector $x^{(i)}$ has components $(x^{(i)})_j=1$ for all $j \\neq i$ and $(x^{(i)})_i=0$.\n\nTherefore, the conjunction $\\bigwedge_{j \\in S} (x^{(i)})_j$ is equal to $0$ if and only if there is at least one index $j \\in S$ for which $(x^{(i)})_j = 0$. This occurs if and only if the index $i$ (the position of the unique zero in $x^{(i)}$) is an element of $S$.\nFormally:\n$$\nh_S(x^{(i)}) = 0 \\iff \\exists j \\in S \\text{ such that } (x^{(i)})_j=0 \\iff i \\in S\n$$\nEquivalently, $h_S(x^{(i)}) = 1 \\iff i \\notin S$.\n\nOur goal is to find a set $S$ that generates the desired labeling $y$. We require $h_S(x^{(i)}) = y_i$ for each $i \\in \\{1, \\dots, d\\}$.\nBased on the logic above, we must satisfy the condition:\n$$\ny_i = 0 \\iff i \\in S\n$$\nThis condition uniquely defines the required set $S$. We choose $S = \\{i \\in \\{1, \\dots, d\\} \\mid y_i = 0\\}$.\n\nWith this choice of $S$, let us verify the labeling:\n- If we need to label $x^{(k)}$ with $y_k=0$, our construction places $k \\in S$. Then $h_S(x^{(k)})=0$, which is correct.\n- If we need to label $x^{(j)}$ with $y_j=1$, our construction ensures $j \\notin S$. Then $h_S(x^{(j)})=1$, which is also correct.\n\nSince for any of the $2^d$ possible labelings $y \\in \\{0,1\\}^d$ we can construct a corresponding hypothesis $h_S \\in \\mathcal{H}_{\\mathrm{mc}}$, the set $C$ of size $d$ is shattered. This establishes that $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\ge d$.\n\n#### Upper Bound: $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\le d$\n\nA standard result in learning theory states that for any finite hypothesis class $\\mathcal{H}$, its VC dimension is bounded by the logarithm of its cardinality: $d_{\\mathrm{VC}}(\\mathcal{H}) \\le \\log_2(|\\mathcal{H}|)$.\n\nThe class of monotone conjunctions, $\\mathcal{H}_{\\mathrm{mc}} = \\{h_S : S \\subseteq [d]\\}$, is constructed by choosing a subset of the coordinates $[d] = \\{1, 2, \\dots, d\\}$. The number of such subsets is $2^d$. Each distinct subset $S$ defines a unique hypothesis function. Therefore, the cardinality of the hypothesis class is $|\\mathcal{H}_{\\mathrm{mc}}| = 2^d$.\n\nApplying the cardinality bound, we get:\n$$\nd_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\le \\log_2(|\\mathcal{H}_{\\mathrm{mc}}|) = \\log_2(2^d) = d\n$$\nThis proves that no set of size $d+1$ can be shattered, and thus $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) \\le d$.\n\nCombining the lower and upper bounds, we conclude that the VC dimension of the monotone conjunctions class is precisely $d$.\n$$\nd_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) = d\n$$\n\n### 2. Sample Complexity Comparison\n\nTo compare the sample complexities of $\\mathcal{H}_{\\mathrm{mc}}$ and the parity class $\\mathcal{P}$, we first need to determine the VC dimension of $\\mathcal{P}$. The parity hypothesis class is $\\mathcal{P} := \\{ g_{S} : S \\subseteq [d] \\}$ with $g_{S}(x) = \\bigoplus_{i \\in S} x_{i}$.\n\n#### VC Dimension of Parity Class ($\\mathcal{P}$)\n\nSimilar to the analysis for $\\mathcal{H}_{\\mathrm{mc}}$, we establish lower and upper bounds.\n\n**Lower Bound: $d_{\\mathrm{VC}}(\\mathcal{P}) \\ge d$**\nWe exhibit a set of $d$ points that is shattered by $\\mathcal{P}$. Consider the set of standard basis vectors, $C = \\{e_1, e_2, \\dots, e_d\\}$.\nFor an arbitrary labeling $y = (y_1, y_2, \\dots, y_d) \\in \\{0,1\\}^d$, we must find a set $S \\subseteq [d]$ such that $g_S(e_i) = y_i$ for all $i \\in \\{1, \\dots, d\\}$.\nThe hypothesis $g_S$ applied to $e_i$ is $g_S(e_i) = \\bigoplus_{j \\in S} (e_i)_j$. Since $(e_i)_j=1$ only for $j=i$ and is $0$ otherwise, the sum modulo $2$ is $1$ if $i \\in S$ and $0$ if $i \\notin S$.\n$$\ng_S(e_i) = 1 \\iff i \\in S\n$$\nTo realize the labeling $y$, we require $g_S(e_i) = y_i$, which implies we must satisfy the condition:\n$$\ny_i = 1 \\iff i \\in S\n$$\nThis uniquely defines the required set $S = \\{i \\in \\{1, \\dots, d\\} \\mid y_i = 1\\}$. Since such a set exists and is unique for every possible labeling vector $y$, the set $C$ is shattered by $\\mathcal{P}$. Thus, $d_{\\mathrm{VC}}(\\mathcal{P}) \\ge d$.\n\n**Upper Bound: $d_{\\mathrm{VC}}(\\mathcal{P}) \\le d$**\nThe parity class $\\mathcal{P}$ is also defined by the set of all subsets of $[d]$, so its cardinality is $|\\mathcal{P}| = 2^d$. Applying the same cardinality bound as before:\n$$\nd_{\\mathrm{VC}}(\\mathcal{P}) \\le \\log_2(|\\mathcal{P}|) = \\log_2(2^d) = d\n$$\nWe conclude that $d_{\\mathrm{VC}}(\\mathcal{P}) = d$.\n\n#### Ratio of Sample Sizes\n\nThe sample complexity of PAC learning in the realizable setting is given by bounds that depend on the VC dimension of the hypothesis class, the accuracy parameter $\\epsilon$, and the confidence parameter $\\delta$. A common standard bound takes the form:\n$$\nN(\\mathcal{H}, \\epsilon, \\delta) \\ge \\frac{K}{\\epsilon} \\left(d_{\\mathrm{VC}}(\\mathcal{H}) \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\n$$\nwhere $K$ is a universal constant. The problem instructs us to apply an identical standard bound to both classes.\n\nFor the monotone conjunction class $\\mathcal{H}_{\\mathrm{mc}}$, the minimal sufficient sample size $N_{\\mathrm{mc}}$ is determined by this bound with $d_{\\mathrm{VC}}(\\mathcal{H}_{\\mathrm{mc}}) = d$:\n$$\nN_{\\mathrm{mc}}(d,\\epsilon,\\delta) = \\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\n$$\nFor the parity class $\\mathcal{P}$, the minimal sufficient sample size $N_{\\mathcal{P}}$ is determined by the same bound with $d_{\\mathrm{VC}}(\\mathcal{P}) = d$:\n$$\nN_{\\mathcal{P}}(d,\\epsilon,\\delta) = \\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)\n$$\nAs both classes have the same VC dimension, any standard sample complexity bound that depends solely on $d_{\\mathrm{VC}}$, $\\epsilon$, and $\\delta$ will yield the same value for both.\n\nThe ratio $R(d,\\epsilon,\\delta)$ is therefore:\n$$\nR(d,\\epsilon,\\delta) = \\frac{N_{\\mathrm{mc}}(d,\\epsilon,\\delta)}{N_{\\mathcal{P}}(d,\\epsilon,\\delta)} = \\frac{\\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)}{\\frac{K}{\\epsilon} \\left(d \\ln\\left(\\frac{1}{\\epsilon}\\right) + \\ln\\left(\\frac{1}{\\delta}\\right)\\right)} = 1\n$$\nThis reveals that from the perspective of information-theoretic sample complexity as measured by VC dimension, the two classes are indistinguishable.\n\n### 3. Qualitative Discussion on Noise Sensitivity\n\nThe identical VC dimensions and resulting sample complexity bounds mask profound structural differences between the two classes, which become apparent when considering noise.\n\n**Independent Random Bit Flips in Input:**\nConsider an input $x$ and a hypothesis parameterized by $S$. Let $x'$ be formed by flipping a single bit $x_k$.\n\n- For **monotone conjunctions ($h_S$)**: The output $h_S(x) = \\bigwedge_{i \\in S} x_i$ is affected by flipping $x_k$ only if $k \\in S$. If $k \\notin S$, the function is completely insensitive to the flip. If $k \\in S$, the function's output may flip. A key characteristic is its non-uniform sensitivity. For an input $x$ where $h_S(x)=1$ (i.e., all $x_i=1$ for $i \\in S$), a single flip of any such $x_k$ to $0$ guarantees a change in output to $0$. However, for an input where $h_S(x)=0$, a single bit flip may or may not change the output, depending on whether other bits $x_j$ (for $j \\in S$) are also $0$. The function is robust to flips in attributes outside $S$.\n\n- For **parity ($g_S$)**: The output $g_S(x) = \\bigoplus_{i \\in S} x_i$ is also only affected by flipping $x_k$ if $k \\in S$. However, the effect is dramatically different. If $k \\in S$, flipping $x_k$ to $1-x_k$ *always* flips the output of the function, since $g_S(x') = g_S(x) \\oplus 1$. This means the parity function has maximal sensitivity to a bit flip on any relevant attribute for *every* input point $x$. The function's output oscillates rapidly with changes in the input.\n\n**Random Classification Noise (RCN):**\nThis refers to a setting where the true label $y$ is flipped to $1-y$ with some probability $\\eta$.\n\n- The structure of **monotone conjunctions** implies a form of smoothness. If an input $x$ is labeled $1$ by $h_S$, then any input $x'$ that is bitwise greater than or equal to $x$ (i.e., $x'_i \\ge x_i, \\forall i$) will also be labeled $1$. This means points that are close in Hamming distance are likely to share the same label. A learning algorithm can exploit this local correlation; for instance, it can treat an isolated point with a surprising label in a \"sea\" of uniformly labeled neighbors as likely noise-corrupted.\n\n- The structure of the **parity** function has no such smoothness. Any two points that are Hamming distance $1$ apart (if the differing bit is in $S$) have opposite labels. There is no local correlation of labels. A point and its nearest neighbors provide a checkerboard-like pattern of labels. In the presence of RCN, a learning algorithm has no local structure to leverage to distinguish a true label from a flipped one. Any labeling appears locally plausible. This makes learning parity computationally very difficult in the agnostic (noise-tolerant) setting, a fact not captured by the VC dimension, which only measures information-theoretic complexity. Efficient algorithms for learning conjunctions under noise exist, while learning parity with noise is known to be computationally hard for efficient algorithms.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nd & 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While VC dimension is perfect for binary classification, it is insufficient for analyzing models that output continuous values, such as in regression. This practice introduces the concept of scale-sensitive complexity through the fat-shattering dimension, a more suitable measure for real-valued function classes. You will explore this concept within the powerful framework of a Reproducing Kernel Hilbert Space (RKHS) with a Gaussian kernel. This problem reveals a crucial insight: a hypothesis class can have an infinite VC dimension, yet still be learnable in a regression setting with meaningful generalization guarantees, demonstrating the importance of matching the complexity measure to the learning task .",
            "id": "3138564",
            "problem": "Consider a real-valued hypothesis class defined on an input space endowed with a feature map from a Reproducing Kernel Hilbert Space (RKHS). Let the input domain be $\\mathcal{X} \\subset \\mathbb{R}^{d}$ equipped with the Gaussian kernel $k(x,x') = \\exp\\!\\big(-\\|x-x'\\|^{2}/(2\\sigma^{2})\\big)$, and let $\\phi:\\mathcal{X}\\to\\mathcal{H}$ be the associated feature map into the RKHS $\\mathcal{H}$. It is known that the Gaussian kernel is universal, and in particular, for any distinct points, the Gram matrix is strictly positive definite. Assume that for all $x \\in \\mathcal{X}$, one has $\\|\\phi(x)\\|_{\\mathcal{H}} = \\sqrt{k(x,x)} = 1$. Define the hypothesis class\n$$\nH_{B} \\;=\\; \\{\\, f_{w}(x) = \\langle w, \\phi(x) \\rangle_{\\mathcal{H}} \\;:\\; \\|w\\|_{\\mathcal{H}} \\leq B \\,\\},\n$$\nwhere $B > 0$ is a fixed radius.\n\nYou will investigate three aspects grounded in core definitions and well-tested results: Vapnik–Chervonenkis (VC) dimension, fat-shattering at scale $\\gamma$, and regression generalization rates under an absolute loss. For binary classification, consider the sign class $\\mathrm{sign}(H_{B}) = \\{\\, \\mathrm{sign}(f_{w}(x)) : f_{w}\\in H_{B} \\,\\}$. For regression, assume outputs $y \\in [-1,1]$ and use the absolute loss $\\ell(y, f(x)) = |\\,y - f(x)\\,|$.\n\nTasks:\n- Using the universality of the Gaussian RKHS and the positive definiteness of Gram matrices for distinct points, formulate $H_{B}$ so that the binary class $\\mathrm{sign}(H_{B})$ has infinite Vapnik–Chervonenkis (VC) dimension, while the real-valued class $H_{B}$ has finite fat-shattering dimension at any fixed scale $\\gamma > 0$.\n- Starting from the definition of fat-shattering at scale $\\gamma$ and standard norm-margin relations for linear functionals in a Hilbert space, derive an explicit upper bound on the fat-shattering dimension of $H_{B}$ at scale $\\gamma$ in terms of $B$ and $\\gamma$.\n- Beginning from the definition of empirical risk and expected risk, the symmetrization principle, and the Rademacher complexity of linear function classes, derive the expected uniform generalization gap for absolute loss over $H_{B}$ in terms of the sample size $n$ and the radius $B$. Then, determine the smallest sample size $n$ that guarantees this expected uniform gap is at most $\\gamma$.\n\nYour final answer must be a single closed-form expression for the smallest sample size $n$ in terms of $B$ and $\\gamma$. No units are required. If you need to round, state your rounding in terms of significant figures; however, the intended answer is exact and does not require rounding.",
            "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- Input space: $\\mathcal{X} \\subset \\mathbb{R}^{d}$.\n- Kernel: Gaussian kernel, $k(x,x') = \\exp(-\\|x-x'\\|^{2}/(2\\sigma^{2}))$.\n- Feature map: $\\phi:\\mathcal{X}\\to\\mathcal{H}$ into a Reproducing Kernel Hilbert Space (RKHS) $\\mathcal{H}$.\n- Kernel properties: Universal; Gram matrix is strictly positive definite for distinct points.\n- Normalization: $\\|\\phi(x)\\|_{\\mathcal{H}} = \\sqrt{k(x,x)} = 1$ for all $x \\in \\mathcal{X}$.\n- Hypothesis class: $H_{B} = \\{\\, f_{w}(x) = \\langle w, \\phi(x) \\rangle_{\\mathcal{H}} \\;:\\; \\|w\\|_{\\mathcal{H}} \\leq B \\,\\}$.\n- Parameter: $B > 0$.\n- Binary hypothesis class: $\\mathrm{sign}(H_{B}) = \\{\\, \\mathrm{sign}(f_{w}(x)) : f_{w}\\in H_{B} \\,\\}$.\n- Regression targets: $y \\in [-1,1]$.\n- Loss function: Absolute loss, $\\ell(y, f(x)) = |y - f(x)|$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the established framework of statistical learning theory. It deals with standard concepts such as RKHS, kernel methods, Vapnik–Chervonenkis (VC) dimension, fat-shattering dimension, and Rademacher complexity. All givens are consistent and sufficient for the required derivations.\n- **Scientifically Grounded:** The problem is based on fundamental principles of machine learning theory. The properties of the Gaussian kernel and RKHS are correctly stated.\n- **Well-Posed:** The tasks are clearly specified and lead to a unique, derivable solution.\n- **Objective:** The problem is stated in precise mathematical language, free of ambiguity or subjectivity.\n- **Completeness and Consistency:** The information provided is self-contained and free from contradictions. The condition $\\|\\phi(x)\\|_{\\mathcal{H}} = 1$ is consistent with the definition of the Gaussian kernel, as $k(x,x)=\\exp(-\\|x-x\\|^2/(2\\sigma^2)) = \\exp(0) = 1$.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n***\n\n### Solution Derivation\n\nThe solution proceeds by addressing the three tasks presented in the problem statement.\n\n**Task 1: VC Dimension and Fat-Shattering Dimension Analysis**\n\nThe first task is to explain why the binary class $\\mathrm{sign}(H_{B})$ has infinite VC dimension, while the real-valued class $H_{B}$ has a finite fat-shattering dimension for any fixed scale $\\gamma > 0$.\n\n*   **Vapnik–Chervonenkis Dimension of $\\mathrm{sign}(H_{B})$**:\n    The VC dimension of a hypothesis class is the maximum number of points $n$ that can be shattered by the class. To shatter a set of points $\\{x_1, \\dots, x_n\\}$, for any labeling $(y_1, \\dots, y_n) \\in \\{-1, 1\\}^n$, there must exist a function $f_w \\in H_B$ such that $\\mathrm{sign}(f_w(x_i)) = y_i$ for all $i \\in \\{1, \\dots, n\\}$.\n    The problem states that the Gram matrix for any set of distinct points is strictly positive definite. This implies that for any set of $n$ distinct points $\\{x_1, \\dots, x_n\\}$, the feature vectors $\\{\\phi(x_1), \\dots, \\phi(x_n)\\}$ are linearly independent in $\\mathcal{H}$.\n    Because of this linear independence, for any labeling $(y_1, \\dots, y_n)$, we can find a vector $w_0 \\in \\mathcal{H}$ such that the interpolation condition $\\langle w_0, \\phi(x_i) \\rangle_{\\mathcal{H}} = y_i$ is satisfied for all $i$. This $w_0$ can be constructed as a linear combination of the feature vectors, $w_0 = \\sum_{j=1}^n c_j \\phi(x_j)$, where the coefficients $c_j$ are found by solving a linear system.\n    Since $y_i \\in \\{-1, 1\\}$, $w_0$ cannot be the zero vector, so $\\|w_0\\|_{\\mathcal{H}} > 0$. Let's define a new vector $w = \\alpha w_0$ for some scalar $\\alpha > 0$. The function is $f_w(x) = \\langle \\alpha w_0, \\phi(x) \\rangle_{\\mathcal{H}} = \\alpha \\langle w_0, \\phi(x) \\rangle_{\\mathcal{H}}$.\n    The sign of the function's output at $x_i$ is $\\mathrm{sign}(f_w(x_i)) = \\mathrm{sign}(\\alpha y_i) = y_i$. To ensure $f_w \\in H_B$, we must satisfy the norm constraint $\\|w\\|_{\\mathcal{H}} \\le B$. This means $\\|\\alpha w_0\\|_{\\mathcal{H}} = \\alpha \\|w_0\\|_{\\mathcal{H}} \\le B$. Since $B>0$ and $\\|w_0\\|_{\\mathcal{H}}>0$, we can always choose a sufficiently small $\\alpha > 0$, for instance $\\alpha = B/\\|w_0\\|_{\\mathcal{H}}$, to satisfy this constraint.\n    This procedure works for any finite number of points $n$. Therefore, any finite set of points can be shattered by $\\mathrm{sign}(H_{B})$, which means its VC dimension is infinite.\n\n*   **Fat-Shattering Dimension of $H_{B}$**:\n    The fat-shattering dimension is finite due to the norm constraint $\\|w\\|_{\\mathcal{H}} \\le B$. A set of points is $\\gamma$-shattered if the function values can be pushed above or below certain thresholds by at least $\\gamma$. The norm constraint on $w$ limits how much the function $\\langle w, \\phi(x) \\rangle_{\\mathcal{H}}$ can vary, which in turn limits its ability to $\\gamma$-shatter arbitrarily large sets. The next task provides a quantitative proof of this finiteness by deriving an explicit upper bound.\n\n**Task 2: Upper Bound on the Fat-Shattering Dimension**\n\nLet $\\mathrm{fat}_{H_B}(\\gamma)$ be the fat-shattering dimension of $H_B$ at scale $\\gamma$. Let $\\{x_1, \\dots, x_n\\}$ be a set of $n$ points that is $\\gamma$-shattered by $H_B$. By definition, there exist real numbers (witnesses) $r_1, \\dots, r_n$ such that for any binary vector $b = (b_1, \\dots, b_n) \\in \\{-1, 1\\}^n$, there exists a function $f_{w_b} \\in H_B$ (i.e., $\\|w_b\\|_{\\mathcal{H}} \\le B$) satisfying for all $i \\in \\{1, \\dots, n\\}$:\n$b_i (f_{w_b}(x_i) - r_i) \\ge \\gamma$.\nThis inequality also holds for the vector $-b \\in \\{-1, 1\\}^n$, so there is a $w_{-b}$ with $\\|w_{-b}\\|_{\\mathcal{H}} \\le B$ such that:\n$(-b_i) (f_{w_{-b}}(x_i) - r_i) \\ge \\gamma$.\nRewriting the second inequality gives $b_i (r_i - f_{w_{-b}}(x_i)) \\ge \\gamma$.\nSumming the two inequalities yields:\n$b_i (f_{w_b}(x_i) - r_i) + b_i (r_i - f_{w_{-b}}(x_i)) \\ge 2\\gamma$.\n$b_i (f_{w_b}(x_i) - f_{w_{-b}}(x_i)) \\ge 2\\gamma$.\nSubstituting $f_w(x) = \\langle w, \\phi(x) \\rangle_{\\mathcal{H}}$:\n$b_i \\langle w_b - w_{-b}, \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge 2\\gamma$.\nLet $v_b = w_b - w_{-b}$. By the triangle inequality, $\\|v_b\\|_{\\mathcal{H}} \\le \\|w_b\\|_{\\mathcal{H}} + \\|w_{-b}\\|_{\\mathcal{H}} \\le B + B = 2B$.\nSumming the inequality over $i=1, \\dots, n$:\n$\\sum_{i=1}^n b_i \\langle v_b, \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge \\sum_{i=1}^n 2\\gamma = 2n\\gamma$.\nBy linearity of the inner product:\n$\\langle v_b, \\sum_{i=1}^n b_i \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge 2n\\gamma$.\nApplying the Cauchy-Schwarz inequality:\n$\\|v_b\\|_{\\mathcal{H}} \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\ge \\langle v_b, \\sum_{i=1}^n b_i \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge 2n\\gamma$.\nUsing the bound on $\\|v_b\\|_{\\mathcal{H}}$:\n$2B \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\ge 2n\\gamma$.\n$B \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\ge n\\gamma$.\nThis inequality holds for any choice of $b \\in \\{-1, 1\\}^n$. We can square both sides and take the expectation over a uniformly random choice of $b$ (where each $b_i$ is an independent Rademacher random variable):\n$\\mathbb{E}_b \\left[ B^2 \\left\\| \\sum_{i=1}^n b_i \\phi(x_i) \\right\\|_{\\mathcal{H}}^2 \\right] \\ge n^2\\gamma^2$.\n$B^2 \\mathbb{E}_b \\left[ \\sum_{i=1}^n \\sum_{j=1}^n b_i b_j \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal{H}} \\right] \\ge n^2\\gamma^2$.\nBy linearity of expectation, and using $\\mathbb{E}[b_i b_j] = \\delta_{ij}$ (Kronecker delta):\n$B^2 \\sum_{i=1}^n \\sum_{j=1}^n \\delta_{ij} \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal{H}} \\ge n^2\\gamma^2$.\n$B^2 \\sum_{i=1}^n \\langle \\phi(x_i), \\phi(x_i) \\rangle_{\\mathcal{H}} \\ge n^2\\gamma^2$.\n$B^2 \\sum_{i=1}^n \\|\\phi(x_i)\\|_{\\mathcal{H}}^2 \\ge n^2\\gamma^2$.\nUsing the given condition $\\|\\phi(x)\\|_{\\mathcal{H}} = 1$:\n$B^2 n \\ge n^2\\gamma^2$.\nSince $n>0$, we can divide by $n$: $B^2 \\ge n\\gamma^2$.\nSolving for $n$, we find an upper bound on the size of any $\\gamma$-shattered set:\n$n \\le \\frac{B^2}{\\gamma^2}$.\nThus, the fat-shattering dimension is finite and bounded by $\\mathrm{fat}_{H_B}(\\gamma) \\le \\frac{B^2}{\\gamma^2}$.\n\n**Task 3: Generalization Gap and Required Sample Size**\n\nThe goal is to derive the expected uniform generalization gap for the absolute loss and find the minimal sample size $n$ to guarantee the gap is at most $\\gamma$.\n\nThe expected uniform generalization gap for a hypothesis class $H_B$ and a loss $\\ell$ is $\\mathbb{E}_S[\\sup_{f \\in H_B} |R(f) - \\hat{R}_S(f)|]$, where $S$ is a sample of size $n$, $R(f) = \\mathbb{E}_{X,Y}[\\ell(Y, f(X))]$ is the true risk, and $\\hat{R}_S(f) = \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i, f(x_i))$ is the empirical risk.\n\n1.  **Symmetrization:** A standard result bounds this gap by twice the Rademacher complexity of the loss class $\\mathcal{L}_{H_B} = \\{(x,y) \\mapsto |y-f(x)| : f \\in H_B\\}$.\n    $\\mathbb{E}_S[\\sup_{f \\in H_B} |R(f) - \\hat{R}_S(f)|] \\le 2 \\mathfrak{R}_n(\\mathcal{L}_{H_B})$.\n    The Rademacher complexity of the loss class is $\\mathfrak{R}_n(\\mathcal{L}_{H_B}) = \\mathbb{E}_{S, \\epsilon} \\left[ \\sup_{f \\in H_B} \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i |y_i - f(x_i)| \\right]$.\n\n2.  **Contraction Principle:** The absolute loss function $\\ell(y, a) = |y-a|$ is $1$-Lipschitz with respect to its second argument $a$. The Ledoux-Talagrand contraction principle states that for a $L$-Lipschitz function $\\psi$, the Rademacher complexity of the composed class $\\psi \\circ H_B$ is bounded by $L$ times the Rademacher complexity of $H_B$. Here, $L=1$.\n    $\\mathfrak{R}_n(\\mathcal{L}_{H_B}) \\le 1 \\cdot \\mathfrak{R}_n(H_B)$.\n\n3.  **Rademacher Complexity of $H_B$:** Now we compute the Rademacher complexity of the original hypothesis class $H_B$.\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\sup_{f \\in H_B} \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i f(x_i) \\right]$.\n    Substituting $f(x_i) = \\langle w, \\phi(x_i) \\rangle_{\\mathcal{H}}$ with $\\|w\\|_{\\mathcal{H}} \\le B$:\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\sup_{\\|w\\|_{\\mathcal{H}} \\le B} \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i \\langle w, \\phi(x_i) \\rangle_{\\mathcal{H}} \\right]$.\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\frac{1}{n} \\sup_{\\|w\\|_{\\mathcal{H}} \\le B} \\left\\langle w, \\sum_{i=1}^n \\epsilon_i \\phi(x_i) \\right\\rangle_{\\mathcal{H}} \\right]$.\n    The supremum of $\\langle w, v \\rangle$ over the ball $\\|w\\|_{\\mathcal{H}} \\le B$ is $B\\|v\\|_{\\mathcal{H}}$. Thus:\n    $\\mathfrak{R}_n(H_B) = \\mathbb{E}_{S, \\epsilon} \\left[ \\frac{B}{n} \\left\\| \\sum_{i=1}^n \\epsilon_i \\phi(x_i) \\right\\|_{\\mathcal{H}} \\right]$.\n    Using Jensen's inequality ($\\mathbb{E}[X] \\le \\sqrt{\\mathbb{E}[X^2]}$):\n    $\\mathfrak{R}_n(H_B) \\le \\frac{B}{n} \\sqrt{\\mathbb{E}_{S, \\epsilon} \\left[ \\left\\| \\sum_{i=1}^n \\epsilon_i \\phi(x_i) \\right\\|_{\\mathcal{H}}^2 \\right]}$.\n    The term inside the square root is $\\mathbb{E}_{S, \\epsilon} \\left[ \\sum_{i,j} \\epsilon_i \\epsilon_j \\langle \\phi(x_i), \\phi(x_j) \\rangle_{\\mathcal{H}} \\right]$. Taking the expectation over $\\epsilon$ first (for a fixed sample $S$) and using $\\mathbb{E}_\\epsilon[\\epsilon_i\\epsilon_j]=\\delta_{ij}$:\n    $\\mathbb{E}_{\\epsilon} \\left[ \\dots \\right] = \\sum_{i=1}^n \\langle \\phi(x_i), \\phi(x_i) \\rangle_{\\mathcal{H}} = \\sum_{i=1}^n \\|\\phi(x_i)\\|_{\\mathcal{H}}^2$.\n    Using the given condition $\\|\\phi(x_i)\\|_{\\mathcal{H}}=1$, this sum equals $n$. The result is independent of the sample $S$, so taking the expectation over $S$ does not change it.\n    $\\mathfrak{R}_n(H_B) \\le \\frac{B}{n} \\sqrt{n} = \\frac{B}{\\sqrt{n}}$.\n\n4.  **Final Bound and Sample Size:** Combining the steps, the expected uniform generalization gap is bounded by:\n    $\\mathbb{E}_S[\\sup_{f \\in H_B} |R(f) - \\hat{R}_S(f)|] \\le 2 \\mathfrak{R}_n(H_B) \\le \\frac{2B}{\\sqrt{n}}$.\n    To guarantee this gap is at most $\\gamma$, we set the upper bound to be less than or equal to $\\gamma$:\n    $\\frac{2B}{\\sqrt{n}} \\le \\gamma$.\n    Solving for $n$:\n    $\\sqrt{n} \\ge \\frac{2B}{\\gamma} \\implies n \\ge \\left(\\frac{2B}{\\gamma}\\right)^2 = \\frac{4B^2}{\\gamma^2}$.\n    The smallest sample size $n$ that guarantees this bound is $\\frac{4B^2}{\\gamma^2}$.",
            "answer": "$$\\boxed{\\frac{4B^{2}}{\\gamma^{2}}}$$"
        },
        {
            "introduction": "Generalization is not just a property of a hypothesis class, but also of the learning algorithm used to select a hypothesis. This practice shifts our focus to algorithmic stability, a framework for understanding how the algorithm itself controls complexity and prevents overfitting. You will analyze one of the most common and effective machine learning techniques: training a model with gradient descent and stopping early. By deriving a generalization bound that depends on the number of training iterations $T$, you will prove how early stopping acts as a form of implicit regularization, directly connecting a practical algorithmic choice to a rigorous theoretical guarantee .",
            "id": "3138528",
            "problem": "Let $S = (z_{1}, \\dots, z_{n})$ be a dataset of size $n$ drawn independently and identically distributed from an unknown distribution over a sample space $\\mathcal{Z}$. Consider a parametric predictor with parameter $w \\in \\mathbb{R}^{d}$ trained by minimizing the empirical risk $F_{S}(w) = \\frac{1}{n} \\sum_{i=1}^{n} f(w; z_{i})$, where the loss $f(w; z)$ satisfies the following properties:\n- For every $z \\in \\mathcal{Z}$, the function $w \\mapsto f(w; z)$ is convex and differentiable.\n- There exists a constant $G > 0$ such that for all $w, w' \\in \\mathbb{R}^{d}$ and all $z \\in \\mathcal{Z}$, $|f(w; z) - f(w'; z)| \\leq G \\|w - w'\\|$, that is, $f(\\cdot; z)$ is $G$-Lipschitz in $w$ uniformly over $z$.\n- The empirical risk $F_{S}(w)$ is convex and $L$-smooth in $w$ for some $L > 0$, meaning its gradient is $L$-Lipschitz: for all $w, w' \\in \\mathbb{R}^{d}$, $\\|\\nabla F_{S}(w) - \\nabla F_{S}(w')\\| \\leq L \\|w - w'\\|$.\n\nTrain using deterministic gradient descent with a fixed step size $\\eta \\in (0, 2/L]$ from a data-independent initialization $w_{0} = 0$, producing iterates $w_{t+1}(S) = w_{t}(S) - \\eta \\nabla F_{S}(w_{t}(S))$ for $t = 0, 1, 2, \\dots$. Define the early-stopped solution after $T$ iterations as $w_{T}(S)$.\n\nLet $S'$ be any dataset that differs from $S$ in exactly one example. The learning algorithm at iteration $T$ is said to be uniformly stable with parameter $\\beta_{T}$ if\n$\\sup_{z \\in \\mathcal{Z}} |f(w_{T}(S); z) - f(w_{T}(S'); z)| \\leq \\beta_{T}$.\nUsing only the above assumptions, derive a closed-form upper bound on the expected generalization error of the early-stopped solution at iteration $T$,\n$\\mathbb{E}_{S}\\big[ F(w_{T}(S)) - F_{S}(w_{T}(S)) \\big]$,\nin terms of $n$, $G$, $\\eta$, and $T$, where $F(w) = \\mathbb{E}_{z}[f(w; z)]$ denotes the population risk. Your final answer must be a single analytic expression. No rounding is required.",
            "solution": "The user wants to derive an upper bound on the expected generalization error.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- Dataset: $S = (z_{1}, \\dots, z_{n})$ drawn independently and identically distributed (i.i.d.) from an unknown distribution over $\\mathcal{Z}$.\n- Predictor parameter: $w \\in \\mathbb{R}^{d}$.\n- Empirical risk: $F_{S}(w) = \\frac{1}{n} \\sum_{i=1}^{n} f(w; z_{i})$.\n- Population risk: $F(w) = \\mathbb{E}_{z}[f(w; z)]$.\n- Loss function properties:\n    - For every $z \\in \\mathcal{Z}$, $w \\mapsto f(w; z)$ is convex and differentiable.\n    - There exists a constant $G > 0$ such that $f(\\cdot; z)$ is $G$-Lipschitz in $w$ uniformly over $z$: $|f(w; z) - f(w'; z)| \\leq G \\|w - w'\\|$ for all $w, w' \\in \\mathbb{R}^{d}$ and all $z \\in \\mathcal{Z}$.\n- Empirical risk properties:\n    - $F_{S}(w)$ is convex.\n    - $F_{S}(w)$ is $L$-smooth in $w$ for some $L > 0$, meaning $\\|\\nabla F_{S}(w) - \\nabla F_{S}(w')\\| \\leq L \\|w - w'\\|$.\n- Training algorithm: Deterministic gradient descent.\n    - Initialization: $w_{0} = 0$.\n    - Iterates: $w_{t+1}(S) = w_{t}(S) - \\eta \\nabla F_{S}(w_{t}(S))$ for $t = 0, 1, 2, \\dots$.\n    - Step size: $\\eta \\in (0, 2/L]$.\n    - Early-stopped solution: $w_{T}(S)$.\n- Stability: The algorithm is uniformly stable with parameter $\\beta_{T}$ if for any dataset $S'$ differing from $S$ in one example, $\\sup_{z \\in \\mathcal{Z}} |f(w_{T}(S); z) - f(w_{T}(S'); z)| \\leq \\beta_{T}$.\n- Objective: Derive a closed-form upper bound on the expected generalization error $\\mathbb{E}_{S}[ F(w_{T}(S)) - F_{S}(w_{T}(S)) ]$ in terms of $n$, $G$, $\\eta$, and $T$.\n\n**1.2. Validation of Givens**\nThe problem is well-posed and scientifically grounded within the field of statistical learning theory. The assumptions (convexity, Lipschitz continuity of the loss, smoothness of the empirical risk) and the algorithm (gradient descent) are standard. The concept of uniform stability and its connection to generalization is a fundamental topic in learning theory. The problem asks for an upper bound which depends on $n, G, \\eta, T$, but not on $L$. The parameter $L$ is used to define the valid range of the step size $\\eta$, and as will be shown, its explicit value is not needed for the final bound, as its role is to ensure a key property (non-expansiveness) of the GD update step. All terms are clearly defined and the setup is internally consistent.\n\n**1.3. Verdict**\nThe problem is valid. We proceed to the solution.\n\n### Step 2: Solution Derivation\n\nThe derivation proceeds in four main steps. First, we relate the expected generalization error to the uniform stability parameter $\\beta_T$. Second, we relate $\\beta_T$ to the geometric distance between the parameters trained on neighboring datasets, $\\|w_T(S) - w_T(S')\\|$. Third, we derive a recurrence relation for this distance. Finally, we solve the recurrence and combine the results to obtain the final bound.\n\n**Part 1: Bounding Generalization Error by Stability**\n\nLet $S = (z_1, \\dots, z_n)$ be the training set. The expected generalization error of the hypothesis $w_T(S)$ is given by $\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))]$.\nUsing the definitions of $F(w)$ and $F_S(w)$:\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] = \\mathbb{E}_{S}\\left[ \\mathbb{E}_{z'}[f(w_T(S); z')] - \\frac{1}{n}\\sum_{i=1}^{n}f(w_T(S); z_i) \\right]\n$$\nwhere $z'$ is a random variable with the same distribution as $z_i$ and is independent of $S$. By linearity of expectation and since the $z_i$ are i.i.d., we can write this as:\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S}\\left[ \\mathbb{E}_{z'_i}[f(w_T(S); z'_i)] - f(w_T(S); z_i) \\right]\n$$\nwhere each $z'_i$ is an independent copy of $z_i$. Let $S^{(i)}$ be the dataset $S$ with the $i$-th example $z_i$ replaced by $z'_i$. The joint distribution of $(S, z'_i)$ is identical to the joint distribution of $(S^{(i)}, z_i)$. Therefore, $\\mathbb{E}_{S, z'_i}[f(w_T(S); z_i)] = \\mathbb{E}_{S^{(i)}, z_i}[f(w_T(S^{(i)}); z'_i)]$. More simply, $\\mathbb{E}_{S, z'_i}[g(S, z_i)] = \\mathbb{E}_{S^{(i)}, z_i}[g(S^{(i)}, z'_i)]$ for any function $g$.\nThe expectation can be rewritten as:\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ f(w_T(S); z'_i) - f(w_T(S); z_i) \\right]\n$$\nWe add and subtract $f(w_T(S^{(i)}); z'_i)$ inside the expectation:\n$$\n= \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ f(w_T(S); z'_i) - f(w_T(S^{(i)}); z'_i) + f(w_T(S^{(i)}); z'_i) - f(w_T(S); z_i) \\right]\n$$\nThe expectations of the last two terms cancel: $\\mathbb{E}_{S, z'_i}[f(w_T(S^{(i)}); z'_i)] = \\mathbb{E}_{S, z'_i}[f(w_T(S); z_i)]$.\nWe are left with:\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ f(w_T(S); z'_i) - f(w_T(S^{(i)}); z'_i) \\right]\n$$\nBy the definition of uniform stability, $|f(w_T(S); z) - f(w_T(S^{(i)}); z)| \\leq \\beta_T$ for any $z$. Thus:\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] \\leq \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}_{S, z'_i}\\left[ \\beta_T \\right] = \\beta_T\n$$\nSo, the problem reduces to finding an upper bound for $\\beta_T$.\n\n**Part 2: Bounding Stability with Parameter Distance**\n\nLet $S$ and $S'$ be two datasets that differ in a single example. By the definition of uniform stability, $\\beta_T = \\sup_{S, S', z} |f(w_T(S); z) - f(w_T(S'); z)|$.\nThe loss function $f(\\cdot; z)$ is $G$-Lipschitz with respect to its first argument $w$. This means:\n$$\n|f(w_T(S); z) - f(w_T(S'); z)| \\leq G \\|w_T(S) - w_T(S')\\|\n$$\nThis inequality holds for all $z$, so upon taking the supremum over $z$, we get:\n$$\n\\sup_{z \\in \\mathcal{Z}} |f(w_T(S); z) - f(w_T(S'); z)| \\leq G \\|w_T(S) - w_T(S')\\|\n$$\nTherefore, $\\beta_T \\leq G \\sup_{S, S'} \\|w_T(S) - w_T(S')\\|$, where the supremum is over all adjacent datasets. Our goal is now to bound $\\|w_T(S) - w_T(S')\\|$.\n\n**Part 3: Recurrence for Parameter Distance**\n\nLet $\\delta_t = \\|w_t(S) - w_t(S')\\|$. The initial condition is $\\delta_0 = \\|w_0 - w_0\\| = \\|0 - 0\\| = 0$.\nThe gradient descent iterates are given by:\n$w_{t+1}(S) = w_t(S) - \\eta \\nabla F_S(w_t(S))$\n$w_{t+1}(S') = w_t(S') - \\eta \\nabla F_{S'}(w_t(S'))$\nThe distance at step $t+1$ is:\n$$\n\\delta_{t+1} = \\|w_t(S) - w_t(S') - \\eta ( \\nabla F_S(w_t(S)) - \\nabla F_{S'}(w_t(S')) ) \\|\n$$\nWe add and subtract $\\eta \\nabla F_S(w_t(S'))$ inside the norm and use the triangle inequality:\n$$\n\\delta_{t+1} \\leq \\| w_t(S) - \\eta \\nabla F_S(w_t(S)) - (w_t(S') - \\eta \\nabla F_S(w_t(S'))) \\| + \\| \\eta (\\nabla F_S(w_t(S')) - \\nabla F_{S'}(w_t(S'))) \\|\n$$\nLet's analyze the first term. The operator $T(w) = w - \\eta \\nabla F_S(w)$ is non-expansive for a convex, $L$-smooth function $F_S$ and step size $\\eta \\in (0, 2/L]$. This means $\\|T(w) - T(w')\\| \\leq \\|w - w'\\|$.\nTo prove this, we use the co-coercivity of the gradient of a convex $L$-smooth function: $\\langle \\nabla F_S(w) - \\nabla F_S(w'), w - w' \\rangle \\geq \\frac{1}{L}\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2$.\n$$\n\\|T(w) - T(w')\\|^2 = \\|(w-w') - \\eta(\\nabla F_S(w) - \\nabla F_S(w'))\\|^2 \\\\\n= \\|w-w'\\|^2 - 2\\eta\\langle w-w', \\nabla F_S(w) - \\nabla F_S(w') \\rangle + \\eta^2\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2 \\\\\n\\leq \\|w-w'\\|^2 - \\frac{2\\eta}{L}\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2 + \\eta^2\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2 \\\\\n= \\|w-w'\\|^2 + \\eta(\\eta - \\frac{2}{L})\\|\\nabla F_S(w) - \\nabla F_S(w')\\|^2\n$$\nSince $\\eta \\in (0, 2/L]$, the term $\\eta(\\eta - 2/L) \\leq 0$. Thus, $\\|T(w) - T(w')\\|^2 \\leq \\|w-w'\\|^2$, proving non-expansiveness.\nApplying this property to the first term of our inequality for $\\delta_{t+1}$:\n$$\n\\| w_t(S) - \\eta \\nabla F_S(w_t(S)) - (w_t(S') - \\eta \\nabla F_S(w_t(S'))) \\| \\leq \\|w_t(S) - w_t(S')\\| = \\delta_t\n$$\nThis simplifies the recurrence for $\\delta_{t+1}$:\n$$\n\\delta_{t+1} \\leq \\delta_t + \\eta \\|\\nabla F_S(w_t(S')) - \\nabla F_{S'}(w_t(S')) \\|\n$$\nNow we bound the gradient difference term. Suppose $S'$ differs from $S$ by replacing $z_k$ with $z'_k$.\n$$\n\\nabla F_S(w) - \\nabla F_{S'}(w) = \\frac{1}{n} \\sum_{i=1}^n \\nabla f(w; z_i) - \\frac{1}{n} \\left( \\sum_{i \\neq k} \\nabla f(w; z_i) + \\nabla f(w; z'_k) \\right) = \\frac{1}{n}(\\nabla f(w; z_k) - \\nabla f(w; z'_k))\n$$\nSince $f(\\cdot; z)$ is $G$-Lipschitz, its gradient norm must be bounded by $G$: $\\|\\nabla f(w; z)\\| \\leq G$.\n$$\n\\|\\nabla F_S(w) - \\nabla F_{S'}(w)\\| \\leq \\frac{1}{n} (\\|\\nabla f(w; z_k)\\| + \\|\\nabla f(w; z'_k)\\|) \\leq \\frac{1}{n}(G + G) = \\frac{2G}{n}\n$$\nThis bound is independent of $w$. Substituting it into the recurrence for $\\delta_{t+1}$:\n$$\n\\delta_{t+1} \\leq \\delta_t + \\eta \\frac{2G}{n}\n$$\n\n**Part 4: Solving the Recurrence and Final Bound**\n\nWe have the recurrence $\\delta_{t+1} \\leq \\delta_t + \\frac{2G\\eta}{n}$ with the initial condition $\\delta_0 = 0$. Unrolling this for $T$ steps:\n$$\n\\delta_T \\leq \\delta_{T-1} + \\frac{2G\\eta}{n} \\leq \\delta_{T-2} + 2 \\cdot \\frac{2G\\eta}{n} \\leq \\dots \\leq \\delta_0 + T \\cdot \\frac{2G\\eta}{n}\n$$\nSince $\\delta_0 = 0$, we have $\\delta_T = \\|w_T(S) - w_T(S')\\| \\leq \\frac{2GT\\eta}{n}$.\nNow, we substitute this back into the bound for $\\beta_T$:\n$$\n\\beta_T \\leq G \\sup_{S, S'} \\|w_T(S) - w_T(S')\\| \\leq G \\left( \\frac{2GT\\eta}{n} \\right) = \\frac{2G^2T\\eta}{n}\n$$\nFinally, we use this to bound the expected generalization error:\n$$\n\\mathbb{E}_{S}[F(w_T(S)) - F_S(w_T(S))] \\leq \\beta_T \\leq \\frac{2G^2T\\eta}{n}\n$$\nThis provides a closed-form upper bound in terms of the specified parameters $n, G, \\eta, T$.",
            "answer": "$$\n\\boxed{\\frac{2G^2T\\eta}{n}}\n$$"
        }
    ]
}