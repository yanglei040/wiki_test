{
    "hands_on_practices": [
        {
            "introduction": "Understanding how features work together is a cornerstone of accurate model interpretation. This practice explores this challenge using the classic \"XOR\" function, a perfect example of a pure interaction. By comparing different explanation methods, you will discover firsthand why simple global analyses can be misleading and how more advanced techniques correctly capture these crucial interactive effects. ",
            "id": "3132648",
            "problem": "You are given a binary interaction model defined on two features. Let $x = (x_{1}, x_{2})$ with each $x_{i} \\in \\{0,1\\}$. Define the target function as the exclusive-or interaction $f(x) = \\mathbf{1}[x_{1} \\oplus x_{2}]$, where $\\mathbf{1}[\\cdot]$ is the indicator function and $\\oplus$ denotes logical exclusive-or. Consider a background data-generating process where $X_{1} \\sim \\text{Bernoulli}(p_{1})$ and $X_{2} \\sim \\text{Bernoulli}(p_{2})$, independent of each other. Interpretability quantities must be computed with respect to this background, using the following fundamental definitions:\n- Partial Dependence Plot (PDP): For feature $i \\in \\{1,2\\}$, the partial dependence function is $PD_{i}(a) = \\mathbb{E}_{X_{-i}}[f(x) \\mid x_{i}=a]$, where the expectation is taken over the background distribution of $X_{-i}$, treating the other feature(s) as random.\n- Individual Conditional Expectation (ICE): For a fixed instance with $x_{-i}^{\\star}$, the individual conditional expectation curve for feature $i$ is $ICE_{i}(a \\mid x_{-i}^{\\star}) = f(x)$ with $x_{i}=a$ and $x_{-i}=x_{-i}^{\\star}$.\n- Shapley interaction index: For features $i$ and $j$ at point $x$, define the characteristic function of a coalition $S \\subseteq \\{1,2\\}$ as $v(S) = \\mathbb{E}[f(X) \\mid X_{S} = x_{S}]$ under an interventional background in which features outside $S$ are independently sampled from their marginals. The Shapley interaction value between features $1$ and $2$ at $x$ is the average second-order discrete difference\n$$\\phi_{1,2}^{\\mathrm{int}}(x) = \\sum_{S \\subseteq \\{1,2\\}\\setminus\\{1,2\\}} \\frac{|S|!\\,(2 - |S| - 2)!}{2\\,(2-1)!} \\big(v(S \\cup \\{1,2\\}) - v(S \\cup \\{1\\}) - v(S \\cup \\{2\\}) + v(S)\\big),$$\nwhich for $2$ features reduces to\n$$\\phi_{1,2}^{\\mathrm{int}}(x) = \\frac{1}{2}\\big(v(\\{1,2\\}) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)\\big).$$\nAll expectations above are with respect to the specified independent Bernoulli background.\n\nYour task: Write a complete program that constructs $f(x)$ and, for each specified test case $(p_{1}, p_{2})$, computes the following four quantities that together compare how Partial Dependence Plot (PDP), Individual Conditional Expectation (ICE), and the Shapley interaction index capture the model’s pure interaction.\n\nFor each test case $(p_{1}, p_{2})$, compute:\n1) The PDP main-effect magnitude for feature $1$, defined as $M_{1} = \\left|PD_{1}(1) - PD_{1}(0)\\right|$.\n2) The PDP main-effect magnitude for feature $2$, defined as $M_{2} = \\left|PD_{2}(1) - PD_{2}(0)\\right|$.\n3) An ICE heterogeneity metric for feature $1$, defined using two reference instances with $x_{2}^{\\star} \\in \\{0,1\\}$ as the mean absolute pointwise difference between the two ICE curves across $x_{1} \\in \\{0,1\\}$:\n$$H_{\\mathrm{ICE}} = \\frac{1}{2} \\sum_{a \\in \\{0,1\\}} \\left| ICE_{1}(a \\mid x_{2}^{\\star}=0) - ICE_{1}(a \\mid x_{2}^{\\star}=1) \\right|.$$\n4) The average absolute Shapley interaction magnitude across the $4$ input points $x \\in \\{0,1\\}^{2}$:\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4} \\sum_{x \\in \\{0,1\\}^{2}} \\left|\\phi_{1,2}^{\\mathrm{int}}(x)\\right|,$$\nwhere each $\\phi_{1,2}^{\\mathrm{int}}(x)$ uses the interventional background as defined above.\n\nUse the following test suite of background parameter values:\n- Case $1$: $(p_{1}, p_{2}) = (0.5, 0.5)$.\n- Case $2$: $(p_{1}, p_{2}) = (0.5, 0.8)$.\n- Case $3$: $(p_{1}, p_{2}) = (0.5, 1.0)$.\n\nYour program should output one result per test case, where each result is the list $[M_{1}, M_{2}, H_{\\mathrm{ICE}}, A_{\\mathrm{SHAP}}]$ in that order. The final output must be a single line containing a list of these three lists, printed as a comma-separated list enclosed in square brackets, for example:\n\"[[m_{1,1},m_{1,2},m_{1,3},m_{1,4}],[m_{2,1},m_{2,2},m_{2,3},m_{2,4}],[m_{3,1},m_{3,2},m_{3,3},m_{3,4}]]\".\n\nDesign notes for coverage:\n- Case $1$ tests the symmetric background where $p_{1}=p_{2}$ and both equal $0.5$, where a pure interaction should have $M_{1} = 0$ and $M_{2} = 0$, but nonzero interaction detected by ICE and Shapley interaction.\n- Case $2$ introduces marginal imbalance in $X_{2}$, showing that $M_{1}$ can become nonzero even without any additive main effect in $f$.\n- Case $3$ is an extreme skew for $X_{2}$ with $p_{2} = 1.0$ to test boundary behavior of these measures under a degenerate background distribution while keeping interventional expectations well-defined.\n\nNo physical units are involved. All returned quantities must be real numbers. The output format must be exactly a single line as described above. Do not print any explanatory text, only the required final list.",
            "solution": "The problem statement is formally evaluated and found to be valid. It is self-contained, scientifically grounded in the theory of statistical learning and model interpretability, and well-posed. All definitions and variables are stated with sufficient precision to allow for a unique and meaningful solution. We may therefore proceed with a formal derivation of the required quantities.\n\nThe target function is the exclusive-or (XOR) function on two binary features, $x_1, x_2 \\in \\{0, 1\\}$. It is given by $f(x_1, x_2) = \\mathbf{1}[x_1 \\oplus x_2]$. The explicit values of the function are:\n$$f(0, 0) = 0$$\n$$f(0, 1) = 1$$\n$$f(1, 0) = 1$$\n$$f(1, 1) = 0$$\nThe background data is generated by two independent Bernoulli random variables, $X_1 \\sim \\text{Bernoulli}(p_1)$ and $X_2 \\sim \\text{Bernoulli}(p_2)$. This implies $P(X_1=1) = p_1$, $P(X_1=0) = 1-p_1$, and similarly for $X_2$. We will now derive the analytical expressions for the four requested quantities.\n\n**1. PDP Main-Effect Magnitude for Feature 1 ($M_1$)**\n\nThe Partial Dependence Plot (PDP) function for feature $1$ at value $a \\in \\{0, 1\\}$ is defined as $PD_1(a) = \\mathbb{E}_{X_2}[f(a, X_2)]$. The expectation is taken over the background distribution of $X_2$.\n\nFor $a=1$:\n$$PD_1(1) = \\mathbb{E}_{X_2}[f(1, X_2)] = f(1, 0) \\cdot P(X_2=0) + f(1, 1) \\cdot P(X_2=1)$$\n$$PD_1(1) = 1 \\cdot (1-p_2) + 0 \\cdot p_2 = 1-p_2$$\nFor $a=0$:\n$$PD_1(0) = \\mathbb{E}_{X_2}[f(0, X_2)] = f(0, 0) \\cdot P(X_2=0) + f(0, 1) \\cdot P(X_2=1)$$\n$$PD_1(0) = 0 \\cdot (1-p_2) + 1 \\cdot p_2 = p_2$$\nThe PDP main-effect magnitude for feature $1$ is $M_1 = |PD_1(1) - PD_1(0)|$.\n$$M_1 = |(1-p_2) - p_2| = |1 - 2p_2|$$\n\n**2. PDP Main-Effect Magnitude for Feature 2 ($M_2$)**\n\nSimilarly, the PDP function for feature $2$ at value $a \\in \\{0, 1\\}$ is $PD_2(a) = \\mathbb{E}_{X_1}[f(X_1, a)]$.\n\nFor $a=1$:\n$$PD_2(1) = \\mathbb{E}_{X_1}[f(X_1, 1)] = f(0, 1) \\cdot P(X_1=0) + f(1, 1) \\cdot P(X_1=1)$$\n$$PD_2(1) = 1 \\cdot (1-p_1) + 0 \\cdot p_1 = 1-p_1$$\nFor $a=0$:\n$$PD_2(0) = \\mathbb{E}_{X_1}[f(X_1, 0)] = f(0, 0) \\cdot P(X_1=0) + f(1, 0) \\cdot P(X_1=1)$$\n$$PD_2(0) = 0 \\cdot (1-p_1) + 1 \\cdot p_1 = p_1$$\nThe PDP main-effect magnitude for feature $2$ is $M_2 = |PD_2(1) - PD_2(0)|$.\n$$M_2 = |(1-p_1) - p_1| = |1 - 2p_1|$$\n\n**3. ICE Heterogeneity Metric ($H_{\\mathrm{ICE}}$)**\n\nThe Individual Conditional Expectation (ICE) curve for feature $1$ at a fixed value $x_2^\\star$ is $ICE_1(a | x_2^\\star) = f(a, x_2^\\star)$. The heterogeneity metric $H_{\\mathrm{ICE}}$ compares the two possible ICE curves generated by setting $x_2^\\star=0$ and $x_2^\\star=1$.\n\nFor $x_2^\\star=0$, the ICE curve is $ICE_1(a|0) = f(a, 0)$.\n$$ICE_1(0|0) = f(0, 0) = 0$$\n$$ICE_1(1|0) = f(1, 0) = 1$$\nFor $x_2^\\star=1$, the ICE curve is $ICE_1(a|1) = f(a, 1)$.\n$$ICE_1(0|1) = f(0, 1) = 1$$\n$$ICE_1(1|1) = f(1, 1) = 0$$\nThe heterogeneity metric is defined as $H_{\\mathrm{ICE}} = \\frac{1}{2} \\sum_{a \\in \\{0,1\\}} |ICE_1(a | x_2^\\star=0) - ICE_1(a | x_2^\\star=1)|$.\n$$H_{\\mathrm{ICE}} = \\frac{1}{2} \\left[ |ICE_1(0|0) - ICE_1(0|1)| + |ICE_1(1|0) - ICE_1(1|1)| \\right]$$\n$$H_{\\mathrm{ICE}} = \\frac{1}{2} \\left[ |0 - 1| + |1 - 0| \\right] = \\frac{1}{2} (1 + 1) = 1$$\nThis metric is a constant value of $1$, indicating that the effect of feature $1$ is always perfectly inverted depending on the value of feature $2$, which is the definition of the XOR interaction. This measure is independent of the background distribution probabilities $p_1$ and $p_2$.\n\n**4. Average Absolute Shapley Interaction Magnitude ($A_{\\mathrm{SHAP}}$)**\n\nThe Shapley interaction value is $\\phi_{1,2}^{\\mathrm{int}}(x) = \\frac{1}{2} \\left(v(\\{1,2\\}) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)\\right)$. We first compute the terms of the characteristic function $v(S) = \\mathbb{E}[f(X) | X_S = x_S]$.\n$v(\\{1,2\\}) = f(x_1, x_2)$ as both features are fixed.\n$v(\\{1\\}) = \\mathbb{E}_{X_2}[f(x_1, X_2)] = PD_1(x_1)$. This is $p_2$ for $x_1=0$ and $1-p_2$ for $x_1=1$.\n$v(\\{2\\}) = \\mathbb{E}_{X_1}[f(X_1, x_2)] = PD_2(x_2)$. This is $p_1$ for $x_2=0$ and $1-p_1$ for $x_2=1$.\n$v(\\varnothing) = \\mathbb{E}[f(X_1, X_2)] = P(X_1=0, X_2=1) + P(X_1=1, X_2=0) = (1-p_1)p_2 + p_1(1-p_2)$.\n\nWe now compute $\\phi_{1,2}^{\\mathrm{int}}(x)$ for each of the four points $x \\in \\{0,1\\}^2$.\n- For $x=(0,0)$:\n$\\phi_{1,2}^{\\mathrm{int}}(0,0) = \\frac{1}{2} [f(0,0) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [0 - p_2 - p_1 + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[ -p_1 - p_2 + p_2 - p_1 p_2 + p_1 - p_1 p_2 ] = \\frac{1}{2}[-2p_1 p_2] = -p_1 p_2$.\n- For $x=(0,1)$:\n$\\phi_{1,2}^{\\mathrm{int}}(0,1) = \\frac{1}{2} [f(0,1) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [1 - p_2 - (1-p_1) + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[p_1 - p_2 + p_2 - p_1 p_2 + p_1 - p_1 p_2] = \\frac{1}{2}[2p_1 - 2p_1 p_2] = p_1(1-p_2)$.\n- For $x=(1,0)$:\n$\\phi_{1,2}^{\\mathrm{int}}(1,0) = \\frac{1}{2} [f(1,0) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [1 - (1-p_2) - p_1 + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[p_2 - p_1 + p_2 - p_1 p_2 + p_1 - p_1 p_2] = \\frac{1}{2}[2p_2 - 2p_1 p_2] = p_2(1-p_1)$.\n- For $x=(1,1)$:\n$\\phi_{1,2}^{\\mathrm{int}}(1,1) = \\frac{1}{2} [f(1,1) - v(\\{1\\}) - v(\\{2\\}) + v(\\varnothing)] = \\frac{1}{2} [0 - (1-p_2) - (1-p_1) + (1-p_1)p_2 + p_1(1-p_2)] = \\frac{1}{2}[-2+p_1+p_2 + p_2 - p_1 p_2 + p_1 - p_1 p_2] = \\frac{1}{2}[2p_1+2p_2-2 - 2p_1 p_2] = - (1 - p_1 - p_2 + p_1 p_2) = -(1-p_1)(1-p_2)$.\n\nThe average absolute Shapley interaction magnitude is $A_{\\mathrm{SHAP}} = \\frac{1}{4} \\sum_{x \\in \\{0,1\\}^2} |\\phi_{1,2}^{\\mathrm{int}}(x)|$. Since $p_1, p_2 \\in [0, 1]$, all terms are non-negative.\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4} \\left[ |-p_1 p_2| + |p_1(1-p_2)| + |p_2(1-p_1)| + |-(1-p_1)(1-p_2)| \\right]$$\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4} \\left[ p_1 p_2 + p_1(1-p_2) + p_2(1-p_1) + (1-p_1)(1-p_2) \\right]$$\nExpanding and simplifying the sum in the brackets:\n$$p_1 p_2 + p_1 - p_1 p_2 + p_2 - p_1 p_2 + 1 - p_1 - p_2 + p_1 p_2 = 1$$\nThus, the average absolute Shapley interaction magnitude is constant:\n$$A_{\\mathrm{SHAP}} = \\frac{1}{4}$$\n\n**Summary of Formulas and Computations for Test Cases**\n\nThe derived expressions are:\n1. $M_1 = |1 - 2p_2|$\n2. $M_2 = |1 - 2p_1|$\n3. $H_{\\mathrm{ICE}} = 1$\n4. $A_{\\mathrm{SHAP}} = 0.25$\n\nWe apply these to the given test cases:\n- **Case 1**: $(p_1, p_2) = (0.5, 0.5)$\n  - $M_1 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $M_2 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $H_{\\mathrm{ICE}} = 1.0$\n  - $A_{\\mathrm{SHAP}} = 0.25$\n  - Result: $[0.0, 0.0, 1.0, 0.25]$\n\n- **Case 2**: $(p_1, p_2) = (0.5, 0.8)$\n  - $M_1 = |1 - 2(0.8)| = |1 - 1.6| = 0.6$\n  - $M_2 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $H_{\\mathrm{ICE}} = 1.0$\n  - $A_{\\mathrm{SHAP}} = 0.25$\n  - Result: $[0.6, 0.0, 1.0, 0.25]$\n\n- **Case 3**: $(p_1, p_2) = (0.5, 1.0)$\n  - $M_1 = |1 - 2(1.0)| = |1 - 2| = 1.0$\n  - $M_2 = |1 - 2(0.5)| = |1 - 1| = 0.0$\n  - $H_{\\mathrm{ICE}} = 1.0$\n  - $A_{\\mathrm{SHAP}} = 0.25$\n  - Result: $[1.0, 0.0, 1.0, 0.25]$",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes interpretability quantities for the XOR function f(x1, x2) = x1 ^ x2.\n    \n    The quantities are:\n    1. M1: PDP main-effect magnitude for feature 1.\n    2. M2: PDP main-effect magnitude for feature 2.\n    3. H_ICE: ICE heterogeneity metric for feature 1.\n    4. A_SHAP: Average absolute Shapley interaction magnitude.\n\n    These are computed for three test cases of background distribution parameters (p1, p2).\n    \"\"\"\n    \n    # Test cases for (p1, p2)\n    test_cases = [\n        (0.5, 0.5), # Case 1\n        (0.5, 0.8), # Case 2\n        (0.5, 1.0), # Case 3\n    ]\n\n    all_results = []\n    \n    for p1, p2 in test_cases:\n        # Based on the analytical derivations:\n        # M_1 = |1 - 2*p2|\n        # M_2 = |1 - 2*p1|\n        # H_ICE = 1\n        # A_SHAP = 0.25\n        \n        # 1. PDP main-effect magnitude for feature 1\n        m1 = np.abs(1 - 2 * p2)\n        \n        # 2. PDP main-effect magnitude for feature 2\n        m2 = np.abs(1 - 2 * p1)\n        \n        # 3. ICE heterogeneity metric\n        # The metric is constant for the XOR function.\n        # H_ICE = 0.5 * (|f(0,0)-f(0,1)| + |f(1,0)-f(1,1)|)\n        # H_ICE = 0.5 * (|0-1| + |1-0|) = 0.5 * (1 + 1) = 1.0\n        h_ice = 1.0\n        \n        # 4. Average absolute Shapley interaction magnitude\n        # This metric is also constant for the XOR function under the given definition.\n        # The sum of absolute interaction values is:\n        # |p1*p2| + |p1*(1-p2)| + |p2*(1-p1)| + |(1-p1)*(1-p2)| = 1\n        # A_SHAP = (1/4) * 1 = 0.25\n        a_shap = 0.25\n        \n        # Store the results as a list of floating-point numbers\n        result = [float(m1), float(m2), float(h_ice), float(a_shap)]\n        all_results.append(result)\n\n    # Format the output string as specified in the problem statement.\n    # e.g., \"[[0.0,0.0,1.0,0.25],[0.6,0.0,1.0,0.25],[1.0,0.0,1.0,0.25]]\"\n    string_formatted_results = []\n    for res_list in all_results:\n        # Convert each inner list to its string representation and remove spaces\n        list_str = str(res_list).replace(\" \", \"\")\n        string_formatted_results.append(list_str)\n    \n    final_output = f\"[{','.join(string_formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world data is often messy, and one common issue is collinearity, where features are highly correlated. This poses a challenge for interpretability: if two features carry similar information, how do we fairly attribute credit for a model's prediction? This exercise demonstrates how SHAP values behave in the presence of collinearity and introduces the concept of grouped explanations as a robust way to analyze the collective impact of related features. ",
            "id": "3132668",
            "problem": "You are given a linear model, a Gaussian background distribution, and a specific instance to explain. Your task is to implement the feature attribution method based on SHapley Additive exPlanations (SHAP), compare standard feature-level attributions against a grouped attribution that treats a set of collinear features as a single player, and quantify fairness criteria under collinearity.\n\nConsider the following setup. Let the model be the linear function\n$$\nf(\\mathbf{x}) = b + \\mathbf{w}^\\top \\mathbf{x},\n$$\nwith parameter vector $\\mathbf{w} = (1, 1, 1/2)$ and bias $b = 0$. The instance to be explained is\n$\\mathbf{x} = (1, 1, 2)$.\nThe background distribution for inputs $\\mathbf{X} = (X_1, X_2, X_3)$ is the multivariate normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ with\n$\\boldsymbol{\\mu} = (0,0,0)$\nand covariance matrix\n$$\n\\boldsymbol{\\Sigma}(\\rho) = \\begin{pmatrix}\n1 & \\rho & 0\\\\\n\\rho & 1 & 0\\\\\n0 & 0 & 1\n\\end{pmatrix},\n$$\nfor a given correlation parameter $\\rho \\in [-1,1]$. The features $X_1$ and $X_2$ are correlated with correlation coefficient $\\rho$, and $X_3$ is independent of both.\n\nDefine the set of feature indices as $N = \\{1,2,3\\}$ and a value function for any subset $S \\subseteq N$ by\n$$\nv(S) = \\mathbb{E}\\big[f(\\mathbf{X}) \\,\\big|\\, \\mathbf{X}_S = \\mathbf{x}_S\\big],\n$$\nwhere $\\mathbf{X}_S$ and $\\mathbf{x}_S$ denote the subvectors of $\\mathbf{X}$ and $\\mathbf{x}$ indexed by $S$.\n\nStandard feature-level SHAP values $\\phi_i$ for $i \\in N$ are the Shapley values of the cooperative game $v(S)$ with players $N$. Grouped-SHAP for the coalition $G_1 = \\{1,2\\}$ treats the coalition as a single player and computes the Shapley value $\\phi_{G_1}$ in the quotient game with players $\\{G_1, \\{3\\}\\}$ and value function inherited from $v(S)$ by mapping each coalition of players to the union of their member feature indices.\n\nYour program must:\n- Implement the value function $v(S)$ exactly under the given Gaussian background using the conditional expectation of a multivariate normal distribution.\n- Compute the standard SHAP values $\\phi_1, \\phi_2, \\phi_3$ for the model and instance.\n- Compute the grouped Shapley value $\\phi_{G_1}$ for the coalition $G_1 = \\{1,2\\}$.\n- Quantify three diagnostics for each test case:\n  1. Additivity error: \n     $$\n     \\left|\\sum_{i \\in N} \\phi_i - \\big(f(\\mathbf{x}) - \\mathbb{E}[f(\\mathbf{X})]\\big)\\right|.\n     $$\n  2. Group consistency error:\n     $$\n     \\left| \\phi_{G_1} - (\\phi_1 + \\phi_2) \\right|.\n     $$\n  3. Symmetry deviation inside the collinear group:\n     $$\n     \\left| \\phi_1 - \\phi_2 \\right|.\n     $$\n\nUse the following test suite of correlation values:\n- $\\rho = 0.0$ (independent features),\n- $\\rho = 0.9$ (high collinearity),\n- $\\rho = 1.0$ (perfect collinearity).\n\nFor all test cases, take $\\mathbf{w} = (1,1,1/2)$, $b = 0$, $\\boldsymbol{\\mu} = (0,0,0)$, and $\\mathbf{x} = (1,1,2)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case (in the order of the test suite) and is itself a list of three floating-point numbers \n$$\n[\\text{additivity\\_error}, \\text{group\\_consistency\\_error}, \\text{symmetry\\_deviation}].\n$$\nFor example, the output format must be\n$$\n[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]],\n$$\nwith each $a_k, b_k, c_k$ being a floating-point number. No physical units or angles are involved in this problem, and no percentages should be used anywhere in the output.",
            "solution": "The user wants to solve a problem related to SHAP (SHapley Additive exPlanations) for a linear model with a Gaussian background distribution. The problem asks for the computation of standard and grouped SHAP values and an analysis of three diagnostic metrics under varying levels of collinearity.\n\n### **Problem Validation**\nThe problem statement is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of givens for a well-defined task in the field of machine learning interpretability. The underlying concepts—linear models, multivariate normal distributions, conditional expectation, and Shapley values—are standard in mathematics and statistics. The case of perfect correlation ($\\rho=1$) results in a singular covariance matrix, but the required conditional expectations remain well-defined, as the conditioning variables are consistent with the linear dependency. The problem is valid.\n\n### **Step-by-Step Solution**\n\nThe solution proceeds in four steps:\n1.  Derive the characteristic value function $v(S)$ for all feature coalitions $S$.\n2.  Compute the standard feature-level SHAP values $\\phi_1, \\phi_2, \\phi_3$.\n3.  Compute the grouped SHAP value $\\phi_{G_1}$ for the coalition $G_1 = \\{1,2\\}$.\n4.  Calculate the three specified diagnostic metrics for each value of $\\rho$.\n\n**1. Derivation of the Value Function $v(S)$**\n\nThe model is $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} = 1 \\cdot x_1 + 1 \\cdot x_2 + \\frac{1}{2} \\cdot x_3$, since $b=0$. The background distribution is $\\mathbf{X} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma}(\\rho))$. The value function is defined as the conditional expectation $v(S) = \\mathbb{E}[f(\\mathbf{X}) | \\mathbf{X}_S = \\mathbf{x}_S]$. By linearity of expectation:\n$$v(S) = \\mathbf{w}^\\top \\mathbb{E}[\\mathbf{X} | \\mathbf{X}_S = \\mathbf{x}_S]$$\nThe conditional expectation for a multivariate normal distribution with mean $\\boldsymbol{\\mu} = \\mathbf{0}$ is $\\mathbb{E}[\\mathbf{X}_{\\bar{S}} | \\mathbf{X}_S = \\mathbf{x}_S] = \\boldsymbol{\\Sigma}_{\\bar{S}S} \\boldsymbol{\\Sigma}_{SS}^{-1} \\mathbf{x}_S$, where $\\bar{S}=N\\setminus S$. The components of $\\mathbf{X}$ we condition on remain fixed at their values in $\\mathbf{x}=(1,1,2)$.\n\nWe compute $v(S)$ for all $S \\subseteq N = \\{1,2,3\\}$:\n-   $S = \\emptyset$: $v(\\emptyset) = \\mathbb{E}[f(\\mathbf{X})] = \\mathbf{w}^\\top \\mathbb{E}[\\mathbf{X}] = \\mathbf{w}^\\top \\boldsymbol{\\mu} = 0$.\n-   $S = \\{1\\}$: We need $\\mathbb{E}[X_2|X_1=x_1]$ and $\\mathbb{E}[X_3|X_1=x_1]$. Since $X_3$ is independent of $X_1$, $\\mathbb{E}[X_3|X_1=x_1]=\\mathbb{E}[X_3]=0$. The conditional mean of $X_2$ given $X_1=x_1$ is $\\mathbb{E}[X_2|X_1=x_1]=\\mu_2 + \\frac{\\sigma_{12}}{\\sigma_{11}}(x_1-\\mu_1) = 0 + \\frac{\\rho}{1}(x_1-0) = \\rho x_1$.\n    $v(\\{1\\}) = w_1x_1 + w_2(\\rho x_1) + w_3(0) = 1(1) + 1(\\rho \\cdot 1) = 1+\\rho$.\n-   $S = \\{2\\}$: By symmetry with feature $1$, $v(\\{2\\}) = w_2x_2 + w_1(\\rho x_2) = 1(1) + 1(\\rho \\cdot 1) = 1+\\rho$.\n-   $S = \\{3\\}$: $X_1, X_2$ are independent of $X_3$, so $\\mathbb{E}[X_1|X_3=x_3]=0$ and $\\mathbb{E}[X_2|X_3=x_3]=0$.\n    $v(\\{3\\}) = w_3x_3 = \\frac{1}{2}(2) = 1$.\n-   $S = \\{1,2\\}$: We need $\\mathbb{E}[X_3|X_1=x_1, X_2=x_2]$. As $X_3$ is independent of $(X_1, X_2)$, this is $\\mathbb{E}[X_3]=0$.\n    $v(\\{1,2\\}) = w_1x_1 + w_2x_2 = 1(1) + 1(1) = 2$. This holds even for $\\rho=1$, as we do not need to invert $\\boldsymbol{\\Sigma}_{\\{1,2\\},\\{1,2\\}}$.\n-   $S = \\{1,3\\}$: We need $\\mathbb{E}[X_2|X_1=x_1, X_3=x_3]$. Independence of $X_3$ from $(X_1, X_2)$ implies this is just $\\mathbb{E}[X_2|X_1=x_1]=\\rho x_1$.\n    $v(\\{1,3\\}) = w_1x_1 + w_3x_3 + w_2(\\rho x_1) = 1(1) + \\frac{1}{2}(2) + 1(\\rho \\cdot 1) = 2+\\rho$.\n-   $S = \\{2,3\\}$: By symmetry with $\\{1,3\\}$, $v(\\{2,3\\}) = 2+\\rho$.\n-   $S = \\{1,2,3\\}$: $v(\\{1,2,3\\}) = f(\\mathbf{x}) = 1(1)+1(1)+\\frac{1}{2}(2) = 3$.\n\n**2. Computation of Standard SHAP Values**\n\nThe Shapley value for a player $i$ in a game with player set $N$ is:\n$$\\phi_i(v) = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! (|N| - |S| - 1)!}{|N|!} [v(S \\cup \\{i\\}) - v(S)]$$\nFor $|N|=3$, the combinatorial weights are $1/3$ for $|S|=0$, $1/6$ for $|S|=1$, and $1/3$ for $|S|=2$.\n\n-   **$\\phi_1$**:\n    $\\phi_1 = \\frac{1}{3}[v(\\{1\\}) - v(\\emptyset)] + \\frac{1}{6}[v(\\{1,2\\}) - v(\\{2\\})] + \\frac{1}{6}[v(\\{1,3\\}) - v(\\{3\\})] + \\frac{1}{3}[v(\\{1,2,3\\}) - v(\\{2,3\\})]$\n    $\\phi_1 = \\frac{1}{3}(1+\\rho) + \\frac{1}{6}(2 - (1+\\rho)) + \\frac{1}{6}((2+\\rho) - 1) + \\frac{1}{3}(3 - (2+\\rho))$\n    $\\phi_1 = \\frac{1+\\rho}{3} + \\frac{1-\\rho}{6} + \\frac{1+\\rho}{6} + \\frac{1-\\rho}{3} = \\frac{2(1+\\rho)+(1-\\rho)+(1+\\rho)+2(1-\\rho)}{6} = \\frac{6}{6} = 1$.\n\n-   **$\\phi_2$**: The problem setup is symmetric with respect to features $1$ and $2$ ($w_1=w_2$, $x_1=x_2$, and their roles in the covariance matrix are exchangeable). The resulting value function $v(S)$ is also symmetric. By the symmetry axiom of Shapley values, $\\phi_1 = \\phi_2$. Thus, $\\phi_2 = 1$.\n\n-   **$\\phi_3$**:\n    $\\phi_3 = \\frac{1}{3}[v(\\{3\\}) - v(\\emptyset)] + \\frac{1}{6}[v(\\{1,3\\}) - v(\\{1\\})] + \\frac{1}{6}[v(\\{2,3\\}) - v(\\{2\\})] + \\frac{1}{3}[v(\\{1,2,3\\}) - v(\\{1,2\\})]$\n    $\\phi_3 = \\frac{1}{3}(1-0) + \\frac{1}{6}((2+\\rho) - (1+\\rho)) + \\frac{1}{6}((2+\\rho) - (1+\\rho)) + \\frac{1}{3}(3-2)$\n    $\\phi_3 = \\frac{1}{3}(1) + \\frac{1}{6}(1) + \\frac{1}{6}(1) + \\frac{1}{3}(1) = \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{3} = 1$.\n\nThe standard SHAP values are $(\\phi_1, \\phi_2, \\phi_3) = (1, 1, 1)$, independent of $\\rho$.\n\n**3. Computation of Grouped SHAP Value $\\phi_{G_1}$**\n\nWe consider a new game with two players: $P_1 = G_1 = \\{1,2\\}$ and $P_2 = \\{3\\}$. The value function $v'$ for this game is inherited from $v$:\n- $v'(\\emptyset) = v(\\emptyset) = 0$\n- $v'(\\{P_1\\}) = v(\\{1,2\\}) = 2$\n- $v'(\\{P_2\\}) = v(\\{3\\}) = 1$\n- $v'(\\{P_1, P_2\\}) = v(\\{1,2,3\\}) = 3$\n\nThe Shapley value for player $P_1$ in this 2-player game is:\n$$\\phi_{G_1} = \\frac{1}{2}[v'(\\{P_1\\}) - v'(\\emptyset)] + \\frac{1}{2}[v'(\\{P_1,P_2\\}) - v'(\\{P_2\\})]$$\n$$\\phi_{G_1} = \\frac{1}{2}(2 - 0) + \\frac{1}{2}(3 - 1) = 1 + 1 = 2$$\nThe grouped Shapley value $\\phi_{G_1}$ is $2$, also independent of $\\rho$.\n\n**4. Computation of Diagnostics**\nFor each test case $\\rho \\in \\{0.0, 0.9, 1.0\\}$, we compute the three metrics.\n\n1.  **Additivity error**: $|\\sum_{i \\in N} \\phi_i - (f(\\mathbf{x}) - \\mathbb{E}[f(\\mathbf{X})])|$.\n    The sum of SHAP values is $\\phi_1+\\phi_2+\\phi_3 = 1+1+1 = 3$. The total effect to explain is $f(\\mathbf{x}) - v(\\emptyset) = 3 - 0 = 3$. The efficiency axiom of Shapley values guarantees $\\sum \\phi_i = v(N) - v(\\emptyset)$.\n    The error is $|3 - 3| = 0$.\n\n2.  **Group consistency error**: $| \\phi_{G_1} - (\\phi_1 + \\phi_2) |$.\n    We have $\\phi_{G_1} = 2$ and $\\phi_1 + \\phi_2 = 1 + 1 = 2$.\n    The error is $|2 - 2| = 0$. This is zero because the game is additively separable. The feature set $\\{1,2\\}$ is statistically independent of $\\{3\\}$, and the linear model has no interaction terms between these sets. This implies $v(S \\cup T) = v(S) + v(T)$ for $S \\subseteq \\{1,2\\}$ and $T \\subseteq \\{3\\}$, which leads to this form of group consistency.\n\n3.  **Symmetry deviation**: $| \\phi_1 - \\phi_2 |$.\n    As noted, the problem is symmetric with respect to features $1$ and $2$. This forces their Shapley values to be equal.\n    The error is $|1 - 1| = 0$.\n\nAll three diagnostic metrics are $0$ for all values of $\\rho$. The specific level of collinearity between features $1$ and $2$ does not affect these particular diagnostics due to the problem's structural properties (independence from feature $3$ and symmetry between features $1$ and $2$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes SHAP-based diagnostics for a linear model with collinear features.\n    \n    The problem setup allows for an exact analytical solution, which is implemented\n    here. The diagnostic values are found to be zero in all cases due to the\n    structural properties of the problem:\n    1. Additivity Error is zero by the efficiency property of Shapley values.\n    2. Symmetry Deviation is zero due to the symmetric treatment of features 1 and 2\n       in the model, instance, and data distribution.\n    3. Group Consistency Error is zero because the game is additively separable,\n       a consequence of the feature set {1,2} being statistically independent of {3}.\n    \"\"\"\n    w = np.array([1.0, 1.0, 0.5])\n    x_inst = np.array([1.0, 1.0, 2.0])\n    # Bias b = 0 and background mean mu = (0,0,0) are constants in the problem.\n\n    test_rhos = [0.0, 0.9, 1.0]\n    \n    all_results = []\n    \n    for rho in test_rhos:\n        # 1. Define the value function v(S) based on analytical derivations.\n        #    v(S) = E[f(X) | X_S = x_S]\n        #    The keys are tuples of feature indices (1-based for clarity).\n        v = {}\n        v[()] = 0.0\n        v[(1,)] = (1.0 + rho) * x_inst[0]  \n        v[(2,)] = (1.0 + rho) * x_inst[1]  \n        v[(3,)] = w[2] * x_inst[2]          \n        v[(1, 2)] = w[0] * x_inst[0] + w[1] * x_inst[1]\n        v[(1, 3)] = w[0] * x_inst[0] + w[2] * x_inst[2] + w[1] * rho * x_inst[0]\n        v[(2, 3)] = w[1] * x_inst[1] + w[2] * x_inst[2] + w[0] * rho * x_inst[1]\n        v[(1, 2, 3)] = np.dot(w, x_inst)\n\n        # 2. Compute standard SHAP values for N=3 players.\n        #    The combinatorial weights are 1/3 for |S|=0, 1/6 for |S|=1, 1/3 for |S|=2.\n        \n        # phi_1 for feature 1\n        mc1_s0 = v[(1,)] - v[()]\n        mc1_s2 = v[(1, 2)] - v[(2,)]\n        mc1_s3 = v[(1, 3)] - v[(3,)]\n        mc1_s23 = v[(1, 2, 3)] - v[(2, 3)]\n        phi_1 = (1.0/3.0)*mc1_s0 + (1.0/6.0)*mc1_s2 + (1.0/6.0)*mc1_s3 + (1.0/3.0)*mc1_s23\n\n        # phi_2 for feature 2\n        mc2_s0 = v[(2,)] - v[()]\n        mc2_s1 = v[(1, 2)] - v[(1,)]\n        mc2_s3 = v[(2, 3)] - v[(3,)]\n        mc2_s13 = v[(1, 2, 3)] - v[(1, 3)]\n        phi_2 = (1.0/3.0)*mc2_s0 + (1.0/6.0)*mc2_s1 + (1.0/6.0)*mc2_s3 + (1.0/3.0)*mc2_s13\n        \n        # phi_3 for feature 3\n        mc3_s0 = v[(3,)] - v[()]\n        mc3_s1 = v[(1, 3)] - v[(1,)]\n        mc3_s2 = v[(2, 3)] - v[(2,)]\n        mc3_s12 = v[(1, 2, 3)] - v[(1, 2)]\n        phi_3 = (1.0/3.0)*mc3_s0 + (1.0/6.0)*mc3_s1 + (1.0/6.0)*mc3_s2 + (1.0/3.0)*mc3_s12\n\n        # 3. Compute grouped Shapley value phi_G1 for G1={1,2}.\n        #    This is a 2-player game with players P1=G1 and P2={3}.\n        v_prime = {\n            'empty': v[()],\n            'P1': v[(1, 2)],\n            'P2': v[(3,)],\n            'P12': v[(1, 2, 3)]\n        }\n        \n        phi_G1 = 0.5 * (v_prime['P1'] - v_prime['empty']) + 0.5 * (v_prime['P12'] - v_prime['P2'])\n        \n        # 4. Quantify diagnostics.\n        fx_minus_Ex = v[(1, 2, 3)] - v[()]\n        \n        additivity_error = abs((phi_1 + phi_2 + phi_3) - fx_minus_Ex)\n        group_consistency_error = abs(phi_G1 - (phi_1 + phi_2))\n        symmetry_deviation = abs(phi_1 - phi_2)\n        \n        case_results = [additivity_error, group_consistency_error, symmetry_deviation]\n        all_results.append(case_results)\n\n    # Format the final output string to match the problem specification exactly.\n    # The format is [[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]] with no spaces.\n    results_str = str(all_results).replace(\" \", \"\")\n    print(results_str)\n\nsolve()\n```"
        },
        {
            "introduction": "A model's explanation is not created in a vacuum; it is deeply tied to the entire modeling pipeline, including preprocessing steps. This exercise explores the profound impact of a common step—feature scaling—on both the model coefficients of a LASSO regression and the resulting SHAP explanations. By comparing two models with nearly identical predictive accuracy but different feature scales, you will see how preprocessing choices can lead to fundamentally different interpretations of feature importance. ",
            "id": "3132563",
            "problem": "You are given a supervised learning setting with an input matrix and a real-valued target. The goal is to construct two models that produce effectively identical test error while using differently scaled features, and then analyze how the Least Absolute Shrinkage and Selection Operator (LASSO) coefficient path and Shapley Additive Explanations (SHAP) attributions respond to feature scaling under the transformation $x_j'=\\alpha_j x_j$. Your analysis must be grounded in first principles of optimization and cooperative game theory.\n\nStart from the following fundamental bases:\n- The LASSO estimator solves the penalized empirical risk minimization problem for a linear model with coefficients $w\\in\\mathbb{R}^p$, training inputs $X\\in\\mathbb{R}^{n\\times p}$, and training targets $y\\in\\mathbb{R}^n$:\n$$\n\\min_{w\\in\\mathbb{R}^p}\\ \\frac{1}{2n}\\lVert y - Xw\\rVert_2^2\\ +\\ \\lambda \\lVert w\\rVert_1,\n$$\nwhere $\\lambda\\ge 0$ is the regularization parameter and $\\lVert\\cdot\\rVert_1$ denotes the $\\ell_1$ norm.\n- For coordinate descent on a convex quadratic with an $\\ell_1$ penalty, the optimal coordinate update uses the soft-thresholding operator $S(z,\\lambda)=\\mathrm{sign}(z)\\max\\{|z|-\\lambda,0\\}$ after isolating the partial residual along each feature.\n- For a linear model $f(x)=w^\\top x$ with zero-mean features and a baseline equal to the expectation, the SHAP (Shapley Additive Explanations) attribution for feature $j$ at input $x$ is given by the Shapley value $\\phi_j(x)=w_j\\,(x_j-\\mathbb{E}[X_j])$. Under centered features with $\\mathbb{E}[X_j]=0$, this simplifies to $\\phi_j(x)=w_j x_j$.\n\nYou must implement the following procedure for each test case:\n1. Data generation and centering:\n   - Fix the number of training samples $n_{\\text{train}}$, test samples $n_{\\text{test}}$, and features $p$.\n   - Draw $X_{\\text{train}}\\in\\mathbb{R}^{n_{\\text{train}}\\times p}$ and $X_{\\text{test}}\\in\\mathbb{R}^{n_{\\text{test}}\\times p}$ with independent standard normal entries.\n   - Center each column of $X_{\\text{train}}$ to zero mean and subtract the same training column means from $X_{\\text{test}}$ to align baselines.\n   - Draw a ground-truth coefficient vector $w^\\star\\in\\mathbb{R}^p$ with independent standard normal entries.\n   - Generate $y_{\\text{train}}=X_{\\text{train}}w^\\star+\\varepsilon_{\\text{train}}$ and $y_{\\text{test}}=X_{\\text{test}}w^\\star+\\varepsilon_{\\text{test}}$ with independent Gaussian noise of standard deviation $\\sigma$, and then center $y_{\\text{train}}$ by subtracting its mean.\n2. Feature scaling:\n   - Given a scaling vector $\\alpha\\in\\mathbb{R}^p$ with strictly positive entries, form scaled inputs $X'_{\\text{train}} = X_{\\text{train}}\\,\\mathrm{diag}(\\alpha)$ and $X'_{\\text{test}} = X_{\\text{test}}\\,\\mathrm{diag}(\\alpha)$, which is equivalent to $x_j'=\\alpha_j x_j$ for each feature $j$.\n3. LASSO path via coordinate descent:\n   - Implement coordinate descent to solve the LASSO objective for a grid of $K$ regularization values $\\lambda_k$, for $k=1,\\dots,K$, on both $(X_{\\text{train}},y_{\\text{train}})$ and $(X'_{\\text{train}},y_{\\text{train}})$. For each $\\lambda_k$, compute the coefficients $w^{(k)}$ (unscaled) and $w'^{(k)}$ (scaled) and the corresponding test mean squared errors $\\mathrm{MSE}^{(k)}$ and $\\mathrm{MSE}'^{(k)}$ on $X_{\\text{test}}$ and $X'_{\\text{test}}$ respectively.\n4. Matching test error across different scalings:\n   - Identify the unscaled model index $k^\\star$ that minimizes $\\mathrm{MSE}^{(k)}$.\n   - On the scaled path, select an index $k^\\dagger$ that minimizes $\\big|\\mathrm{MSE}'^{(k)}-\\mathrm{MSE}^{(k^\\star)}\\big|$.\n   - Report the absolute test error gap $\\Delta_{\\text{MSE}}=\\big|\\mathrm{MSE}^{(k^\\star)}-\\mathrm{MSE}'^{(k^\\dagger)}\\big|$, which quantifies how closely the two models match in test error.\n5. Coefficient alignment and SHAP comparison:\n   - Map the scaled coefficients back to the original feature space using $w^{\\text{equiv}}=\\mathrm{diag}(\\alpha)w'^{(k^\\dagger)}$ so that $w'^{(k^\\dagger)\\top}\\mathbf{x}' = w^{\\text{equiv}\\top}\\mathbf{x}$ for any $\\mathbf{x}$.\n   - Compute the Euclidean norm alignment error $\\Delta_w=\\lVert w^{(k^\\star)}-w^{\\text{equiv}}\\rVert_2$.\n   - For a fixed test point $x_0$ chosen as the first row of $X_{\\text{test}}$, compute SHAP attributions using the centered-baseline linear formula:\n     - Unscaled: $\\phi_j=w^{(k^\\star)}_j\\,x_{0j}$.\n     - Scaled: $\\phi'_j=w'^{(k^\\dagger)}_j\\,x'_{0j}$ with $x'_{0j}=\\alpha_j x_{0j}$.\n     - Report the maximum absolute SHAP difference across features $\\Delta_{\\text{SHAP}}=\\max_j |\\phi_j-\\phi'_j|$.\n6. Output formatting:\n   - For each test case, return the triple $[\\Delta_{\\text{MSE}},\\Delta_w,\\Delta_{\\text{SHAP}}]$ as floating-point values.\n\nYour program must implement the coordinate descent solver directly (no external machine learning libraries), run the above procedure on the specified test suite, and produce a single line of output containing the results as a comma-separated list of triples, enclosed in square brackets and without spaces.\n\nTest suite:\n- Case $1$ (happy path, uniform scaling): $n_{\\text{train}}=120$, $n_{\\text{test}}=80$, $p=5$, seed $=42$, $\\sigma=0.2$, $\\alpha=(2.0,2.0,2.0,2.0,2.0)$.\n- Case $2$ (nonuniform scaling): $n_{\\text{train}}=120$, $n_{\\text{test}}=80$, $p=5$, seed $=7$, $\\sigma=0.3$, $\\alpha=(1.0,0.5,2.0,0.25,4.0)$.\n- Case $3$ (edge case with one feature extremely scaled): $n_{\\text{train}}=120$, $n_{\\text{test}}=80$, $p=5$, seed $=11$, $\\sigma=0.1$, $\\alpha=(1.0,1.0,10^{-3},1.0,2.0)$.\n\nRegularization grid:\n- Use $K=60$ values $\\lambda_k$ logarithmically spaced between $10^{-3}$ and $1$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of three-element lists, with each element rounded to six decimal places, and with no spaces, for all test cases in order. For example: $[[a,b,c],[d,e,f],[g,h,i]]$.",
            "solution": "The problem is well-posed and scientifically grounded in the fields of statistical learning and optimization. It presents a detailed procedure to investigate the effect of feature scaling on the solutions produced by LASSO regularization and on the corresponding SHAP interpretability values. The core of the problem lies in the fact that the LASSO objective function is not invariant to arbitrary scaling of input features, which has direct consequences for both the model coefficients and any explanation methods derived from them. A minor notational ambiguity in the problem statement regarding matrix multiplication for feature scaling is resolved by adhering to the standard convention and the explicit definition provided. Specifically, \"$X_{\\text{train}}'=\\mathrm{diag}(\\alpha)\\,X_{\\text{train}}$\" is dimensionally inconsistent for $n_{\\text{train}} \\neq p$ and is interpreted as the column-wise scaling described by \"$x_j'=\\alpha_j x_j$\", which corresponds to the matrix operation $X_{\\text{train}}' = X_{\\text{train}}\\,\\mathrm{diag}(\\alpha)$. With this clarification, the problem is valid and can be solved as specified.\n\n### Theoretical Foundation\n\n**1. LASSO and Feature Scaling**\n\nThe LASSO optimization problem is given by:\n$$\n\\min_{w\\in\\mathbb{R}^p}\\ \\frac{1}{2n}\\lVert y - Xw\\rVert_2^2\\ +\\ \\lambda \\lVert w\\rVert_1\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is the input matrix, $y \\in \\mathbb{R}^n$ is the target vector, $w \\in \\mathbb{R}^p$ are the model coefficients, and $\\lambda \\geq 0$ is the regularization parameter.\n\nLet us consider a scaling of the features, where each feature $j$ is multiplied by a positive constant $\\alpha_j > 0$. Let $A = \\mathrm{diag}(\\alpha_1, \\dots, \\alpha_p)$. The scaled feature matrix is $X' = XA$. A linear model on these scaled features is parameterized by coefficients $w' \\in \\mathbb{R}^p$, making predictions as $\\hat{y} = X'w'$. The LASSO problem for the scaled data is:\n$$\n\\min_{w'\\in\\mathbb{R}^p}\\ \\frac{1}{2n}\\lVert y - X'w'\\rVert_2^2\\ +\\ \\lambda' \\lVert w'\\rVert_1\n$$\nTo compare this to the original problem, we can express the scaled model in terms of the original features. Let $w = Aw'$. This transformation implies $w_j = \\alpha_j w'_j$, and therefore the two models are equivalent in their predictions: $(X')w' = (XA)w' = X(Aw') = Xw$. Substituting $w' = A^{-1}w$ into the scaled objective function gives:\n$$\n\\min_{w\\in\\mathbb{R}^p}\\ \\frac{1}{2n}\\lVert y - Xw\\rVert_2^2\\ +\\ \\lambda' \\lVert A^{-1}w\\rVert_1\n$$\nThe penalty term becomes $\\lambda' \\lVert A^{-1}w\\rVert_1 = \\lambda' \\sum_{j=1}^{p} |w_j / \\alpha_j|$. The objective is now:\n$$\n\\min_{w\\in\\mathbb{R}^p}\\ \\frac{1}{2n}\\lVert y - Xw\\rVert_2^2\\ +\\ \\sum_{j=1}^{p} (\\lambda'/\\alpha_j) |w_j|\n$$\nThis is a form of weighted LASSO. It is evident that this objective is different from the original LASSO objective unless all $\\alpha_j$ are equal. If $\\alpha_j = \\alpha$ for all $j$, the penalty becomes $(\\lambda'/\\alpha) \\lVert w \\rVert_1$, which means the scaled problem is equivalent to the unscaled problem with a rescaled regularization parameter $\\tilde{\\lambda} = \\lambda'/\\alpha$. However, for non-uniform scaling, the solution path (the set of coefficients $w$ as $\\lambda$ varies) will be fundamentally different. Features with a larger scale factor $\\alpha_j$ receive a smaller effective penalty, promoting larger corresponding coefficients.\n\n**2. SHAP and Feature Scaling**\n\nFor a linear model $f(x)=w^\\top x$ with centered features ($\\mathbb{E}[X_j]=0$), the SHAP value for feature $j$ is $\\phi_j(x) = w_j x_j$. This represents the contribution of feature $j$ to the prediction for input $x$.\n\nFor the unscaled model with coefficients $w^{(k^\\star)}$, the SHAP value is $\\phi_j = w^{(k^\\star)}_j x_{0j}$.\nFor the scaled model, the input is $x'_{0j} = \\alpha_j x_{0j}$ and the coefficients are $w'^{(k^\\dagger)}$. The SHAP value is $\\phi'_j = w'^{(k^\\dagger)}_j x'_{0j}$.\n\nTo compare these, we use the equivalent coefficient in the original feature space, $w^{\\text{equiv}} = A w'^{(k^\\dagger)}$, which means $w^{\\text{equiv}}_j = \\alpha_j w'^{(k^\\dagger)}_j$. Substituting this into the expression for $\\phi'_j$:\n$$\n\\phi'_j = w'^{(k^\\dagger)}_j (\\alpha_j x_{0j}) = (\\alpha_j w'^{(k^\\dagger)}_j) x_{0j} = w^{\\text{equiv}}_j x_{0j}\n$$\nThe comparison of SHAP values, $|\\phi_j - \\phi'_j|$, is therefore equivalent to comparing $|(w^{(k^\\star)}_j - w^{\\text{equiv}}_j)x_{0j}|$. The SHAP values will only be identical if the original-space coefficients $w^{(k^\\star)}$ and $w^{\\text{equiv}}$ are identical. Since non-uniform scaling changes the LASSO solution, we expect $w^{(k^\\star)} \\neq w^{\\text{equiv}}$ and thus $\\Delta_{\\text{SHAP}} > 0$. This demonstrates that two models with nearly identical predictive error can have different local explanations if they were trained on differently scaled features.\n\n### Algorithmic Implementation\n\n**Coordinate Descent for LASSO**\n\nTo solve the LASSO problem, we implement cyclic coordinate descent. For each feature $j=1, \\dots, p$, we update the coefficient $w_j$ while keeping others fixed. The one-variable optimization problem for $w_j$ is to minimize:\n$$\nf(w_j) = \\frac{1}{2n}\\lVert r_j - X_{\\cdot,j}w_j\\rVert_2^2 + \\lambda|w_j|\n$$\nwhere $r_j = y - \\sum_{k \\neq j} X_{\\cdot,k}w_k$ is the partial residual. The solution to this subproblem is given by the soft-thresholding operator $S(z, \\gamma) = \\mathrm{sign}(z)\\max(|z|-\\gamma, 0)$:\n$$\nw_j^{\\text{new}} = \\frac{S\\left(\\frac{1}{n}X_{\\cdot,j}^\\top r_j, \\lambda\\right)}{\\frac{1}{n}\\lVert X_{\\cdot,j}\\rVert_2^2}\n$$\nFor computational efficiency, we pre-calculate $C = \\frac{1}{n}X^\\top X$ and $c = \\frac{1}{n}X^\\top y$. The term $\\frac{1}{n}X_{\\cdot,j}^\\top r_j$ can be written as $c_j - \\sum_{k \\neq j} C_{jk}w_k$. The update rule for $w_j$ in a cycle becomes:\n$$\nw_j^{\\text{new}} \\leftarrow \\frac{S\\left(c_j - \\sum_{k \\neq j} C_{jk}w_k^{\\text{old}}, \\lambda\\right)}{C_{jj}}\n$$\nWe cycle through all features $j=1, \\dots, p$ repeatedly until the coefficient vector converges. For the LASSO path calculation, we use a warm-start strategy, where the solution for a given $\\lambda_k$ is used as the initial guess for the next smaller value $\\lambda_{k+1}$.\n\n### Execution of the Procedure\n\nThe implementation proceeds precisely as defined in the problem description:\n\n1.  **Data Generation:** For each test case, we generate and center the training and test data ($X, y$) using the specified parameters and random seed.\n2.  **Feature Scaling:** The scaled data matrices $X'_{\\text{train}}$ and $X'_{\\text{test}}$ are created by multiplying each feature column $j$ by its corresponding scaling factor $\\alpha_j$.\n3.  **LASSO Path Construction:** Using the coordinate descent solver, we compute the coefficient paths $\\{w^{(k)}\\}_{k=1}^K$ and $\\{w'^{(k)}\\}_{k=1}^K$ for the unscaled and scaled data, respectively, over the specified grid of $\\lambda$ values. Test mean squared errors (MSE) are computed for each model in the paths.\n4.  **Error Matching:** We find the unscaled model with the minimum test MSE, $\\mathrm{MSE}^{(k^\\star)}$. We then find the scaled model whose test MSE, $\\mathrm{MSE}'^{(k^\\dagger)}$, is closest to this minimum value. The absolute difference $\\Delta_{\\text{MSE}}$ is recorded.\n5.  **Coefficient and SHAP Comparison:** The scaled coefficients $w'^{(k^\\dagger)}$ are transformed back to the original feature space via $w^{\\text{equiv}} = \\mathrm{diag}(\\alpha)w'^{(k^\\dagger)}$. We then compute the Euclidean distance $\\Delta_w = \\lVert w^{(k^\\star)} - w^{\\text{equiv}} \\rVert_2$. Finally, we compute the SHAP values $\\phi_j$ and $\\phi'_j$ for the first test sample and find the maximum absolute difference across all features, $\\Delta_{\\text{SHAP}} = \\max_j |\\phi_j - \\phi'_j|$.\n6.  **Output:** The triplet $[\\Delta_{\\text{MSE}}, \\Delta_w, \\Delta_{\\text{SHAP}}]$ for each case is collected and formatted as required.\n\nThis procedure rigorously quantifies how feature scaling, a common preprocessing step, affects LASSO's variable selection and coefficient estimation, and how these changes propagate to local feature attributions, even when overall model performance is matched.",
            "answer": "```python\nimport numpy as np\n# No other libraries are permitted as per the problem description.\n\ndef soft_threshold(z, lam):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - lam, 0.0)\n\ndef lasso_cd(X, y, lam, max_iter=1000, tol=1e-8, w_init=None):\n    \"\"\"\n    LASSO solver using coordinate descent.\n    \"\"\"\n    n, p = X.shape\n    \n    if w_init is None:\n        w = np.zeros(p)\n    else:\n        w = w_init.copy()\n\n    # Pre-compute parts of the objective function derivative.\n    C = (1/n) * X.T @ X\n    c = (1/n) * X.T @ y\n    a = np.diag(C)\n\n    for _ in range(max_iter):\n        w_prev = w.copy()\n        for j in range(p):\n            if a[j] == 0: continue\n\n            # Efficiently compute the argument for soft-thresholding\n            # This is equivalent to (1/n) * X_j^T * (y - sum_{k!=j} X_k w_k)\n            z_j = c[j] - (C[j, :] @ w - a[j] * w[j])\n            \n            # Update w_j\n            w[j] = soft_threshold(z_j, lam) / a[j]\n\n        # Check for convergence\n        if np.max(np.abs(w - w_prev)) < tol:\n            break\n            \n    return w\n\ndef run_analysis(n_train, n_test, p, seed, sigma, alpha, K, lambda_min, lambda_max):\n    \"\"\"\n    Runs the full analysis for a single test case.\n    \"\"\"\n    # 1. Data generation and centering\n    rng = np.random.default_rng(seed)\n    X_train = rng.standard_normal(size=(n_train, p))\n    X_test = rng.standard_normal(size=(n_test, p))\n\n    X_train_mean = X_train.mean(axis=0)\n    X_train -= X_train_mean\n    X_test -= X_train_mean\n\n    w_star = rng.standard_normal(size=p)\n    \n    noise_train = sigma * rng.standard_normal(size=n_train)\n    noise_test = sigma * rng.standard_normal(size=n_test)\n    \n    y_train = X_train @ w_star + noise_train\n    y_test = X_test @ w_star + noise_test\n\n    y_train_mean = y_train.mean()\n    y_train -= y_train_mean\n    # y_test is not centered to preserve the original prediction target\n\n    # 2. Feature scaling\n    alpha_vec = np.array(alpha)\n    X_train_prime = X_train * alpha_vec\n    X_test_prime = X_test * alpha_vec\n\n    # 3. LASSO path via coordinate descent\n    lambda_grid = np.logspace(np.log10(lambda_min), np.log10(lambda_max), K)\n    \n    ws_unscaled = []\n    mses_unscaled = []\n    w_unscaled_init = None\n    \n    ws_scaled = []\n    mses_scaled = []\n    w_scaled_init = None\n\n    # Iterate from large lambda to small for warm starts\n    for lam in reversed(lambda_grid):\n        # Unscaled model\n        w_unscaled = lasso_cd(X_train, y_train, lam=lam, w_init=w_unscaled_init)\n        y_pred_unscaled = X_test @ w_unscaled\n        mse_unscaled = np.mean((y_test - y_pred_unscaled)**2)\n        ws_unscaled.append(w_unscaled)\n        mses_unscaled.append(mse_unscaled)\n        w_unscaled_init = w_unscaled\n\n        # Scaled model\n        w_scaled = lasso_cd(X_train_prime, y_train, lam=lam, w_init=w_scaled_init)\n        y_pred_scaled = X_test_prime @ w_scaled\n        mse_scaled = np.mean((y_test - y_pred_scaled)**2)\n        ws_scaled.append(w_scaled)\n        mses_scaled.append(mse_scaled)\n        w_scaled_init = w_scaled\n\n    # Reverse results to match original lambda_grid order\n    ws_unscaled = ws_unscaled[::-1]\n    mses_unscaled = mses_unscaled[::-1]\n    ws_scaled = ws_scaled[::-1]\n    mses_scaled = mses_scaled[::-1]\n\n    # 4. Matching test error across different scalings\n    k_star = np.argmin(mses_unscaled)\n    mse_star = mses_unscaled[k_star]\n    \n    mse_diffs = np.abs(np.array(mses_scaled) - mse_star)\n    k_dagger = np.argmin(mse_diffs)\n    \n    Delta_MSE = np.abs(mse_star - mses_scaled[k_dagger])\n\n    # 5. Coefficient alignment and SHAP comparison\n    w_k_star = ws_unscaled[k_star]\n    w_prime_k_dagger = ws_scaled[k_dagger]\n\n    w_equiv = alpha_vec * w_prime_k_dagger\n    \n    Delta_w = np.linalg.norm(w_k_star - w_equiv)\n\n    x0 = X_test[0, :]\n    x0_prime = x0 * alpha_vec\n\n    phi = w_k_star * x0\n    phi_prime = w_prime_k_dagger * x0_prime\n    \n    Delta_SHAP = np.max(np.abs(phi - phi_prime))\n\n    return [Delta_MSE, Delta_w, Delta_SHAP]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: n_train, n_test, p, seed, sigma, alpha\n        (120, 80, 5, 42, 0.2, (2.0, 2.0, 2.0, 2.0, 2.0)),\n        # Case 2\n        (120, 80, 5, 7, 0.3, (1.0, 0.5, 2.0, 0.25, 4.0)),\n        # Case 3\n        (120, 80, 5, 11, 0.1, (1.0, 1.0, 10**-3, 1.0, 2.0)),\n    ]\n\n    K = 60\n    lambda_min = 1e-3\n    lambda_max = 1.0\n\n    results = []\n    for case in test_cases:\n        result = run_analysis(*case, K, lambda_min, lambda_max)\n        results.append(result)\n\n    # Format the output string precisely as required: [[a,b,c],[d,e,f],...]\n    # without any spaces.\n    outer_parts = []\n    for row in results:\n        inner_parts = [f\"{val:.6f}\" for val in row]\n        outer_parts.append(f\"[{','.join(inner_parts)}]\")\n    final_output_string = f\"[{','.join(outer_parts)}]\"\n\n    print(final_output_string)\n\nsolve()\n\n```"
        }
    ]
}