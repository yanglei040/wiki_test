{
    "hands_on_practices": [
        {
            "introduction": "Our first practice explores the core concept of algorithmic stability through a fundamental comparison: mean versus median. This exercise demonstrates how the choice of a statistical estimator impacts its robustness to outliers, a key aspect of stable learning . By programmatically quantifying how much the sample mean and median shift when a single data point is changed to an extreme value, you will build a concrete intuition for how different algorithms handle data contamination.",
            "id": "3098721",
            "problem": "You will analyze the algorithmic stability of two constant-function regression algorithms under heavy-tailed errors. Consider a dataset $S = \\{(x_j, y_j)\\}_{j=1}^n$ with inputs $x_j \\in \\mathbb{R}$ and outputs $y_j \\in \\mathbb{R}$. Define two learning algorithms that fit constant predictors $f_S(x) \\equiv c$ by minimizing empirical risk: (a) squared loss regression, which returns the sample mean, and (b) absolute loss regression (median or quantile regression at quantile $q = \\tfrac{1}{2}$), which returns the sample median. For a single-point replacement, define $S^{(i,y^\\star)}$ as the dataset obtained by replacing only the $i$-th response $y_i$ in $S$ by $y^\\star$, keeping all other pairs unchanged. For any fixed input $x_0 \\in \\mathbb{R}$, define the empirical point-replacement stability of an algorithm $A$ on $(S, i, y^\\star)$ as\n$$\n\\Delta_A(S,i,y^\\star) \\equiv \\bigl| f_S(x_0) - f_{S^{(i,y^\\star)}}(x_0) \\bigr|.\n$$\nBecause the predictors are constant functions, $\\Delta_A(S,i,y^\\star)$ does not depend on $x_0$.\n\nYour task is to write a program that, for a deterministic family of heavy-tailed datasets $S$, computes $\\Delta_{\\text{mean}}$ and $\\Delta_{\\text{median}}$ and verifies whether $\\Delta_{\\text{median}} \\le \\Delta_{\\text{mean}}$ for a set of specified test cases. You must construct $S$ without randomness using quantile grids of heavy-tailed distributions as follows.\n\n1. Cauchy distribution with location $0$ and scale $\\gamma > 0$: for $j \\in \\{0,1,\\dots,n-1\\}$ let\n$$\np_j = \\frac{j + \\tfrac{1}{2}}{n}, \\quad y_j = \\gamma \\cdot \\tan\\!\\bigl(\\pi(p_j - \\tfrac{1}{2})\\bigr).\n$$\n2. Pareto distribution with scale (minimum) $x_m > 0$ and shape $\\alpha > 0$: for $j \\in \\{0,1,\\dots,n-1\\}$ let\n$$\np_j = \\frac{j + \\tfrac{1}{2}}{n}, \\quad y_j = \\frac{x_m}{(1 - p_j)^{1/\\alpha}}.\n$$\n\nIn both cases, set $x_j \\equiv 0$ for all $j$ since inputs are irrelevant for constant predictors. The sample median for even $n$ must be defined as the average of the two central order statistics. For each test case, compute\n$$\n\\Delta_{\\text{mean}}(S,i,y^\\star) = \\bigl|\\bar{y}(S) - \\bar{y}(S^{(i,y^\\star)})\\bigr|, \\quad \n\\Delta_{\\text{median}}(S,i,y^\\star) = \\bigl|\\operatorname{med}(S) - \\operatorname{med}(S^{(i,y^\\star)})\\bigr|.\n$$\nReturn a boolean indicating whether $\\Delta_{\\text{median}} \\le \\Delta_{\\text{mean}}$.\n\nUse the following test suite of parameter values that together exercise typical, boundary, and edge scenarios. Each test case specifies the distribution family, the dataset size $n$, the heavy-tail parameters, the replacement index $i$ (zero-based), and the replacement value $y^\\star$.\n\n- Test case $1$ (symmetric heavy tail, odd $n$):\n  - Family: Cauchy\n  - Parameters: $n = 101$, $\\gamma = 1$\n  - Replacement: $i = 0$, $y^\\star = 10^6$.\n- Test case $2$ (symmetric heavy tail, even $n$):\n  - Family: Cauchy\n  - Parameters: $n = 100$, $\\gamma = 1$\n  - Replacement: $i = 99$, $y^\\star = -10^6$.\n- Test case $3$ (asymmetric heavy tail, odd $n$):\n  - Family: Pareto\n  - Parameters: $n = 99$, $x_m = 1$, $\\alpha = 1.5$\n  - Replacement: $i = 0$, $y^\\star = 10^9$.\n- Test case $4$ (asymmetric heavy tail, even $n$):\n  - Family: Pareto\n  - Parameters: $n = 50$, $x_m = 1$, $\\alpha = 2$\n  - Replacement: $i = 0$, $y^\\star = 10^6$.\n\nYour program must compute, for each test case, a boolean value indicating whether median regression is at least as stable as mean regression under the specified point replacement, that is whether\n$$\n\\Delta_{\\text{median}}(S,i,y^\\star) \\le \\Delta_{\\text{mean}}(S,i,y^\\star).\n$$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_k$ is a boolean value for test case $k$.\n\nNo physical units are involved. All numerical results are real numbers and boolean values without units. Angles are not used. Percentages are not used. The use of any randomness is forbidden; you must use the quantile grids exactly as specified to construct $S$ deterministically.",
            "solution": "The problem is evaluated to be **valid**. It is scientifically grounded in statistical learning theory, specifically algorithmic stability, and is well-posed with all necessary data and definitions provided. The task is to compare the stability of regression estimators based on the mean (squared loss) and the median (absolute loss) on deterministically generated heavy-tailed datasets. The problem is objective, formalizable, and computationally feasible. We will proceed with a full solution.\n\nOur objective is to compare the point-replacement stability of two constant regression predictors: the sample mean and the sample median. The stability of an algorithm $A$ is measured by the change in its output predictor when one point in the training set $S$ is altered. The stability measure is defined as:\n$$\n\\Delta_A(S,i,y^\\star) \\equiv \\bigl| f_S(x_0) - f_{S^{(i,y^\\star)}}(x_0) \\bigr|\n$$\nwhere $S^{(i,y^\\star)}$ is the dataset $S$ with the $i$-th response $y_i$ replaced by a new value $y^\\star$. Since the predictors are constant functions, $f_S(x) \\equiv c$, this simplifies to the absolute difference between the constant values fitted on the original and modified datasets.\n\nLet the dataset be $S = \\{y_j\\}_{j=0}^{n-1}$, comprising $n$ scalar responses. The inputs $x_j$ are irrelevant.\n\n**1. Stability of the Sample Mean**\n\nThe first algorithm, corresponding to minimizing the squared error loss $\\sum_{j=0}^{n-1} (y_j - c)^2$, produces the sample mean as its estimate:\n$$\nf_S(x) \\equiv \\bar{y}(S) = \\frac{1}{n} \\sum_{j=0}^{n-1} y_j\n$$\nThe modified dataset $S^{(i,y^\\star)}$ has elements $\\{y_0, \\dots, y_{i-1}, y^\\star, y_{i+1}, \\dots, y_{n-1}\\}$. The sample mean of this new dataset is:\n$$\n\\bar{y}(S^{(i,y^\\star)}) = \\frac{1}{n} \\left( \\left(\\sum_{j=0}^{n-1} y_j\\right) - y_i + y^\\star \\right) = \\frac{1}{n} \\sum_{j=0}^{n-1} y_j - \\frac{y_i}{n} + \\frac{y^\\star}{n} = \\bar{y}(S) + \\frac{y^\\star - y_i}{n}\n$$\nThe stability measure for the mean, $\\Delta_{\\text{mean}}$, is therefore:\n$$\n\\Delta_{\\text{mean}}(S,i,y^\\star) = \\bigl| \\bar{y}(S) - \\bar{y}(S^{(i,y^\\star)}) \\bigr| = \\left| \\bar{y}(S) - \\left( \\bar{y}(S) + \\frac{y^\\star - y_i}{n} \\right) \\right| = \\left| - \\frac{y^\\star - y_i}{n} \\right| = \\frac{|y_i - y^\\star|}{n}\n$$\nThis formula shows that the change in the mean is directly proportional to the magnitude of the perturbation $|y_i - y^\\star|$ and inversely proportional to the dataset size $n$. For a large perturbation (outlier), the change can be arbitrarily large.\n\n**2. Stability of the Sample Median**\n\nThe second algorithm, corresponding to minimizing the absolute error loss $\\sum_{j=0}^{n-1} |y_j - c|$, produces the sample median as its estimate:\n$$\nf_S(x) \\equiv \\operatorname{med}(S)\n$$\nThe sample median is calculated from the sorted data. Let $y_{(0)} \\le y_{(1)} \\le \\dots \\le y_{(n-1)}$ be the order statistics of the dataset $S$.\n- If $n$ is odd, the median is the central element, $\\operatorname{med}(S) = y_{((n-1)/2)}$.\n- If $n$ is even, the median is the average of the two central elements, $\\operatorname{med}(S) = \\frac{1}{2} (y_{(n/2 - 1)} + y_{(n/2)})$.\n\nUnlike the mean, the stability of the median, $\\Delta_{\\text{median}}$, does not have a simple closed-form expression. It must be computed algorithmically:\n1.  Generate the initial dataset $S = \\{y_j\\}_{j=0}^{n-1}$.\n2.  Calculate its median, $\\operatorname{med}(S)$.\n3.  Construct the modified dataset $S^{(i,y^\\star)}$ by replacing $y_i$ with $y^\\star$.\n4.  Calculate the median of the modified dataset, $\\operatorname{med}(S^{(i,y^\\star)})$. This requires re-sorting or finding the new central elements.\n5.  Compute the stability: $\\Delta_{\\text{median}}(S,i,y^\\star) = |\\operatorname{med}(S) - \\operatorname{med}(S^{(i,y^\\star)})|$.\n\nThe median is known to be a robust statistic. Its value depends on the rank of the data points, not their magnitude. Therefore, replacing a point $y_i$ with an extreme outlier $y^\\star$ typically causes a much smaller change in the median compared to the mean, as it might only shift the position of the median to an adjacent data point.\n\n**3. Dataset Generation**\n\nThe datasets are constructed deterministically using the quantile functions (inverse CDFs) of heavy-tailed distributions. This ensures the data has the desired distributional properties without introducing randomness.\nThe quantile grid points are $p_j = \\frac{j + 0.5}{n}$ for $j \\in \\{0, 1, \\dots, n-1\\}$.\n\n- **Cauchy Distribution:** Location $0$, scale $\\gamma > 0$. The quantile function is $F^{-1}(p) = \\gamma \\tan(\\pi(p - 0.5))$.\n  $$\n  y_j = \\gamma \\cdot \\tan\\!\\bigl(\\pi(p_j - \\tfrac{1}{2})\\bigr)\n  $$\n- **Pareto Distribution:** Scale (minimum) $x_m > 0$, shape $\\alpha > 0$. The quantile function is $F^{-1}(p) = x_m / (1-p)^{1/\\alpha}$.\n  $$\n  y_j = \\frac{x_m}{(1 - p_j)^{1/\\alpha}}\n  $$\nSince the quantile functions are monotonic and the $p_j$ values are ordered, the generated dataset $\\{y_j\\}$ is already sorted.\n\n**4. Computation for Each Test Case**\n\nFor each specified test case, we perform the following steps:\n1.  Define the parameters: distribution family, $n$, heavy-tail parameters ($\\gamma$ or $(x_m, \\alpha)$), replacement index $i$, and replacement value $y^\\star$.\n2.  Generate the initial sorted dataset $S = \\{y_j\\}_{j=0}^{n-1}$ using the corresponding formula.\n3.  Calculate the stability of the mean using the analytical formula: $\\Delta_{\\text{mean}} = \\frac{|y_i - y^\\star|}{n}$.\n4.  Calculate the stability of the median algorithmically:\n    a.  Compute the original median, $\\operatorname{med}(S)$.\n    b.  Create the modified dataset $S^{(i,y^\\star)}$ by replacing $y_i$.\n    c.  Compute the new median, $\\operatorname{med}(S^{(i,y^\\star)})$.\n    d.  Compute $\\Delta_{\\text{median}} = |\\operatorname{med}(S) - \\operatorname{med}(S^{(i,y^\\star)})|$.\n5.  Compare the stability values and determine if $\\Delta_{\\text{median}} \\le \\Delta_{\\text{mean}}$. The result is a boolean value.\n\nThis procedure is systematically applied to all test cases provided in the problem description to generate the final list of boolean results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and compares the algorithmic stability of mean and median regression\n    under point replacement for deterministically generated heavy-tailed datasets.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'family': 'Cauchy', 'n': 101, 'gamma': 1, 'xm': None, 'alpha': None, 'i': 0, 'y_star': 1e6},\n        {'family': 'Cauchy', 'n': 100, 'gamma': 1, 'xm': None, 'alpha': None, 'i': 99, 'y_star': -1e6},\n        {'family': 'Pareto', 'n': 99, 'gamma': None, 'xm': 1, 'alpha': 1.5, 'i': 0, 'y_star': 1e9},\n        {'family': 'Pareto', 'n': 50, 'gamma': None, 'xm': 1, 'alpha': 2, 'i': 0, 'y_star': 1e6},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Unpack parameters for the current test case\n        n = case['n']\n        i = case['i']\n        y_star = case['y_star']\n        \n        # Step 1: Generate the initial dataset S = {y_j}\n        # The quantile points p_j\n        p = (np.arange(n) + 0.5) / n\n        \n        if case['family'] == 'Cauchy':\n            gamma = case['gamma']\n            # Generate sorted data using the Cauchy quantile function\n            y = gamma * np.tan(np.pi * (p - 0.5))\n        elif case['family'] == 'Pareto':\n            xm = case['xm']\n            alpha = case['alpha']\n            # Generate sorted data using the Pareto quantile function\n            y = xm / (1 - p)**(1 / alpha)\n        else:\n            # This path should not be reached with the given test cases\n            raise ValueError(f\"Unknown distribution family: {case['family']}\")\n\n        # The data y is generated sorted, as it's based on an ordered sequence of quantiles p_j\n        # and monotonic quantile functions.\n        \n        # Step 2: Compute stability of the mean\n        # We use the analytical formula: Delta_mean = |y_i - y_star| / n\n        y_i = y[i]\n        delta_mean = np.abs(y_i - y_star) / n\n        \n        # Step 3: Compute stability of the median\n        # a. Calculate the median of the original dataset\n        med_s = np.median(y)\n        \n        # b. Create the modified dataset S^(i, y_star)\n        y_mod = y.copy()\n        y_mod[i] = y_star\n        \n        # c. Calculate the median of the modified dataset\n        # np.median internally sorts the array, which is necessary here.\n        med_s_mod = np.median(y_mod)\n        \n        # d. Compute the stability measure for the median\n        delta_median = np.abs(med_s - med_s_mod)\n        \n        # Step 4: Compare stabilities and record the boolean result\n        is_median_more_stable = (delta_median <= delta_mean)\n        results.append(is_median_more_stable)\n\n    # Final print statement in the exact required format.\n    # The str() of a boolean in Python is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the concept of robustness, our next exercise examines how to engineer stability directly into a regression model by choosing an appropriate loss function. We will compare the standard squared loss with the robust Huber loss, which is specifically designed to be less sensitive to large prediction errors . This hands-on comparison will illustrate the practical benefits of using robust loss functions to create models that are less affected by outliers, a crucial skill in real-world data analysis.",
            "id": "3098786",
            "problem": "Consider a supervised learning setting with scalar inputs and outputs, where the hypothesis class consists of one-dimensional linear predictors of the form $f_{\\mathbf{w}}(x) = w x$ with parameter $w \\in \\mathbb{R}$. Let $S = \\{(x_i, y_i)\\}_{i=1}^n$ be a training dataset. Define the empirical risk minimization (ERM) principle as minimizing the empirical risk $\\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f_{\\mathbf{w}}(x_i))$ over $w$, where $\\ell$ is a specified loss function. We will compare two loss functions: (i) the squared loss $\\ell_{\\text{sq}}(y, \\hat{y}) = (y - \\hat{y})^2$, and (ii) the Huber loss $\\ell_{\\text{Huber}, \\delta}(y, \\hat{y})$, parameterized by a threshold $\\delta > 0$, defined by\n$$\n\\ell_{\\text{Huber}, \\delta}(y, \\hat{y}) =\n\\begin{cases}\n\\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\le \\delta, \\\\\n\\delta \\left(|y - \\hat{y}| - \\frac{1}{2}\\delta\\right) & \\text{if } |y - \\hat{y}| > \\delta.\n\\end{cases}\n$$\nAlgorithmic stability in the sense of replace-one stability concerns the sensitivity of the learned predictor when a single training example is modified. For a given dataset $S$, let $f_S$ denote the ERM solution under a specified loss, and let $S^{(k \\to y_k')}$ denote the dataset formed by replacing a single response $y_k$ at index $k$ with $y_k'$, keeping all other $(x_i, y_i)$ unchanged. Let $f_{S^{(k \\to y_k')}}$ be the ERM solution on the modified dataset. For a fixed query point $x_\\star \\in \\mathbb{R}$, define the prediction shift due to the swap as\n$$\n\\Delta_{\\text{loss}}(S, k, y_k', x_\\star) = \\left| f_S(x_\\star) - f_{S^{(k \\to y_k')}}(x_\\star) \\right|.\n$$\nYour task is to implement a complete, runnable program that:\n- Constructs ERM solutions $f_S$ and $f_{S^{(k \\to y_k')}}$ for both the squared loss and the Huber loss (with a given threshold $\\delta$) for each specified dataset.\n- Computes the absolute prediction shift $\\Delta_{\\text{sq}}$ for the squared loss and $\\Delta_{\\text{Huber}}$ for the Huber loss at the given $x_\\star$.\n- Aggregates the results across all test cases into a single output line in the required format.\n\nStart from the foundational definitions above and produce numerically accurate solutions. Do not assume any closed-form formulas unless they follow from first principles for the stated setting.\n\nUse the following test suite of parameter values, covering a typical case with an extreme outlier, a mixed-sign design with a negative outlier, and a boundary condition where the replacement does not change the dataset:\n\n- Test Case 1 (typical outlier):\n  - $n = 5$,\n  - $\\mathbf{x} = (1,2,3,4,5)$,\n  - $\\mathbf{y} = (1,2,3,4,1000)$,\n  - replacement index $k = 5$ and $y_k' = 5$,\n  - Huber threshold $\\delta = 1.0$,\n  - query point $x_\\star = 2.0$.\n\n- Test Case 2 (mixed signs and negative outlier):\n  - $n = 4$,\n  - $\\mathbf{x} = (-2,-1,0.5,1.5)$,\n  - $\\mathbf{y} = (1,-200,0.5,-1)$,\n  - replacement index $k = 2$ and $y_k' = -2$,\n  - Huber threshold $\\delta = 0.5$,\n  - query point $x_\\star = -1.0$.\n\n- Test Case 3 (boundary: no change):\n  - $n = 3$,\n  - $\\mathbf{x} = (1,2,3)$,\n  - $\\mathbf{y} = (1,2,3)$,\n  - replacement index $k = 2$ and $y_k' = 2$,\n  - Huber threshold $\\delta = 1.0$,\n  - query point $x_\\star = 1.0$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order\n$$\n[\\Delta_{\\text{sq}}^{(1)}, \\Delta_{\\text{Huber}}^{(1)}, \\Delta_{\\text{sq}}^{(2)}, \\Delta_{\\text{Huber}}^{(2)}, \\Delta_{\\text{sq}}^{(3)}, \\Delta_{\\text{Huber}}^{(3)}],\n$$\nwhere the superscript denotes the test case number. Each entry must be a floating-point number. No external input is permitted; all values are specified above and must be embedded in the program.",
            "solution": "The problem requires us to compare the replace-one stability of two empirical risk minimization (ERM) predictors for a simple linear model. The model is given by the hypothesis class $f_w(x) = wx$, where $w \\in \\mathbb{R}$. We are given a training set $S = \\{(x_i, y_i)\\}_{i=1}^n$. The goal is to find the parameter $w$ that minimizes the empirical risk, $R_S(w) = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, w x_i)$, for two different loss functions: the squared loss and the Huber loss. We then quantify stability by measuring the change in prediction at a point $x_\\star$ when one training label $y_k$ is replaced by $y_k'$.\n\nFirst, we derive the ERM solution for each loss function.\n\n**Squared Loss ERM Solution**\nThe squared loss function is $\\ell_{\\text{sq}}(y, \\hat{y}) = (y - \\hat{y})^2$. The empirical risk for our model is:\n$$\nR_S(w) = \\frac{1}{n} \\sum_{i=1}^n (y_i - w x_i)^2\n$$\nThis is a quadratic function of $w$ and is convex. To find the minimum, we take the derivative with respect to $w$ and set it to zero:\n$$\n\\frac{dR_S(w)}{dw} = \\frac{1}{n} \\sum_{i=1}^n \\frac{d}{dw} (y_i - w x_i)^2 = \\frac{1}{n} \\sum_{i=1}^n 2(y_i - w x_i)(-x_i) = -\\frac{2}{n} \\sum_{i=1}^n (y_i x_i - w x_i^2)\n$$\nSetting the derivative to zero yields:\n$$\n\\sum_{i=1}^n (y_i x_i - w x_i^2) = 0\n$$\n$$\n\\sum_{i=1}^n y_i x_i = w \\sum_{i=1}^n x_i^2\n$$\nAssuming that not all $x_i$ are zero, i.e., $\\sum_{i=1}^n x_i^2 > 0$, we obtain a unique closed-form solution for the optimal parameter, which we denote $w_{\\text{sq}}$:\n$$\nw_{\\text{sq}} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}\n$$\nThis is the familiar ordinary least squares (OLS) estimator for a regression model passing through the origin.\n\n**Huber Loss ERM Solution**\nThe Huber loss is defined as:\n$$\n\\ell_{\\text{Huber}, \\delta}(y, \\hat{y}) =\n\\begin{cases}\n\\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\le \\delta, \\\\\n\\delta \\left(|y - \\hat{y}| - \\frac{1}{2}\\delta\\right) & \\text{if } |y - \\hat{y}| > \\delta.\n\\end{cases}\n$$\nThe empirical risk is $R_S(w) = \\frac{1}{n} \\sum_{i=1}^n \\ell_{\\text{Huber}, \\delta}(y_i, w x_i)$. The Huber loss is a convex function, and since the sum of convex functions is convex, $R_S(w)$ is also a convex function of $w$. Therefore, a global minimum exists and can be found using standard convex optimization techniques.\n\nUnlike the squared loss, there is no simple closed-form solution for $w$ that works for all datasets. The optimal $w$ is the solution to the stationarity condition, which requires the subgradient of the risk to contain zero. The subgradient of $\\ell_{\\text{Huber}, \\delta}$ with respect to its second argument (evaluated at a residual $r=y-\\hat{y}$) is a function often denoted $\\psi(r)$:\n$$\n\\psi(r) = \\begin{cases}\nr & \\text{if } |r| < \\delta \\\\\n\\delta \\cdot \\text{sign}(r) & \\text{if } |r| > \\delta \\\\\n\\left[-\\delta, \\delta\\right] & \\text{if } |r| = \\delta\n\\end{cases}\n$$\nThe stationarity condition for $R_S(w)$ is:\n$$\n0 \\in \\frac{\\partial R_S(w)}{\\partial w} \\implies 0 \\in \\sum_{i=1}^n x_i \\cdot \\psi(y_i - w x_i)\n$$\nThis equation is nonlinear in $w$ because the form of $\\psi$ depends on $w$ itself. While it can be solved by analyzing the piecewise linear function $\\sum_i x_i \\psi(y_i - w x_i)$, a more direct and robust approach is to perform a one-dimensional numerical minimization of the convex function $R_S(w)$. We can use a numerical solver, such as `scipy.optimize.minimize_scalar`, to find the value $w_{\\text{Huber}} = \\arg\\min_w R_S(w)$.\n\n**Algorithm and Calculation of Prediction Shift**\n\nFor each test case and for each loss function (squared and Huber), we perform the following steps:\n$1$. Let the original dataset be $S = \\{(\\mathbf{x}_i, \\mathbf{y}_i)\\}_{i=1}^n$.\n$2$. Let the modified dataset be $S^{(k \\to y_k')} = \\{(\\mathbf{x}_i, \\mathbf{y}'_i)\\}_{i=1}^n$, where $\\mathbf{y}'$ is identical to $\\mathbf{y}$ except that $\\mathbf{y}'_k = y_k'$.\n$3$. For the squared loss:\n    a. Calculate $w_{\\text{sq}, S} = (\\sum_{i=1}^n x_i y_i) / (\\sum_{i=1}^n x_i^2)$.\n    b. Calculate $w_{\\text{sq}, S'} = (\\sum_{i=1}^n x_i y'_i) / (\\sum_{i=1}^n x_i^2)$.\n    c. Compute the shift $\\Delta_{\\text{sq}} = |(w_{\\text{sq}, S} - w_{\\text{sq}, S'}) x_\\star|$.\n$4$. For the Huber loss with parameter $\\delta$:\n    a. Define the objective function $J(w) = \\sum_{i=1}^n \\ell_{\\text{Huber}, \\delta}(y_i, w x_i)$.\n    b. Use a numerical optimizer to find $w_{\\text{Huber}, S} = \\arg\\min_w J(w)$ for the original data $\\mathbf{y}$.\n    c. Use the same optimizer to find $w_{\\text{Huber}, S'} = \\arg\\min_w J(w)$ for the modified data $\\mathbf{y}'$.\n    d. Compute the shift $\\Delta_{\\text{Huber}} = |(w_{\\text{Huber}, S} - w_{\\text{Huber}, S'}) x_\\star|$.\n\nThis procedure will be applied to each of the three test cases specified in the problem statement. The resulting values of $\\Delta_{\\text{sq}}$ and $\\Delta_{\\text{Huber}}$ for each case are then collected and formatted as required. The final Python program implements these calculations. A helper function computes the total Huber risk for a given $w$, which is then passed to `scipy.optimize.minimize_scalar` to find the optimal parameter. The closed-form solution is used for the squared loss.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef solve():\n    \"\"\"\n    Computes the prediction shift for linear ERM with squared and Huber losses\n    across a suite of test cases, demonstrating algorithmic stability.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 5, \"x\": np.array([1.0, 2.0, 3.0, 4.0, 5.0]), \"y\": np.array([1.0, 2.0, 3.0, 4.0, 1000.0]),\n            \"k\": 5, \"y_k_prime\": 5.0, \"delta\": 1.0, \"x_star\": 2.0\n        },\n        {\n            \"n\": 4, \"x\": np.array([-2.0, -1.0, 0.5, 1.5]), \"y\": np.array([1.0, -200.0, 0.5, -1.0]),\n            \"k\": 2, \"y_k_prime\": -2.0, \"delta\": 0.5, \"x_star\": -1.0\n        },\n        {\n            \"n\": 3, \"x\": np.array([1.0, 2.0, 3.0]), \"y\": np.array([1.0, 2.0, 3.0]),\n            \"k\": 2, \"y_k_prime\": 2.0, \"delta\": 1.0, \"x_star\": 1.0\n        }\n    ]\n\n    results = []\n\n    def huber_risk(w, x, y, d):\n        \"\"\"Computes the total empirical risk for the Huber loss.\"\"\"\n        residuals = y - w * x\n        abs_residuals = np.abs(residuals)\n        \n        # Use boolean indexing for vectorized computation\n        small_err_mask = abs_residuals <= d\n        large_err_mask = ~small_err_mask\n\n        loss = np.zeros_like(residuals, dtype=float)\n        loss[small_err_mask] = 0.5 * residuals[small_err_mask]**2\n        loss[large_err_mask] = d * (abs_residuals[large_err_mask] - 0.5 * d)\n        \n        return np.sum(loss)\n\n    for case in test_cases:\n        x_vec = case[\"x\"]\n        y_vec = case[\"y\"]\n        k = case[\"k\"]\n        y_k_prime = case[\"y_k_prime\"]\n        delta = case[\"delta\"]\n        x_star = case[\"x_star\"]\n\n        y_vec_prime = np.copy(y_vec)\n        # Note: problem uses 1-based indexing for k\n        y_vec_prime[k - 1] = y_k_prime\n\n        # Squared Loss Calculation\n        sum_x_sq = np.dot(x_vec, x_vec)\n        \n        w_sq_S = np.dot(x_vec, y_vec) / sum_x_sq\n        w_sq_S_prime = np.dot(x_vec, y_vec_prime) / sum_x_sq\n        delta_sq = np.abs((w_sq_S - w_sq_S_prime) * x_star)\n        results.append(delta_sq)\n\n        # Huber Loss Calculation\n        # Minimize for the original dataset S\n        res_S = optimize.minimize_scalar(huber_risk, args=(x_vec, y_vec, delta))\n        w_huber_S = res_S.x\n\n        # Minimize for the modified dataset S'\n        res_S_prime = optimize.minimize_scalar(huber_risk, args=(x_vec, y_vec_prime, delta))\n        w_huber_S_prime = res_S_prime.x\n        \n        delta_huber = np.abs((w_huber_S - w_huber_S_prime) * x_star)\n        results.append(delta_huber)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice explores a different dimension of instability, one that arises from the geometry of the input data rather than the output values. We will investigate how \"high-leverage\" points—data points with unusual feature vectors—can disproportionately influence a linear model and how ridge regression can mitigate this effect . By deriving and computing the exact change in a ridge regression predictor when a point is removed, you will connect the abstract concept of leverage scores to the practical diagnosis of model instability.",
            "id": "3098822",
            "problem": "You will investigate algorithmic stability in ridge regression by quantifying how the learned predictor changes when a single high-leverage training point is removed. Use linear algebra from first principles. Begin from the Empirical Risk Minimization (ERM) formulation for ridge regression with squared loss, the normal equations, and the Sherman–Morrison identity for rank-one updates of a matrix inverse. You may also use the Cauchy–Schwarz inequality and the definition of the hat matrix and leverage scores. Do not use or assume any results that directly give the final expressions; instead, derive them from the fundamental definitions.\n\nLet a training set be given by a design matrix $X \\in \\mathbb{R}^{n \\times d}$ and a response vector $y \\in \\mathbb{R}^{n}$, and consider ridge regression with regularization parameter $\\lambda \\gt 0$, where the learned weight vector $w_S \\in \\mathbb{R}^{d}$ minimizes the objective\n$$\nJ(w) = \\tfrac{1}{2}\\|X w - y\\|_2^2 + \\tfrac{\\lambda}{2}\\|w\\|_2^2.\n$$\nDefine the learned predictor as $f_S(x) = x^\\top w_S$ for any $x \\in \\mathbb{R}^{d}$. Let $S \\setminus \\{i\\}$ denote the dataset with the $i$-th example $(x_i, y_i)$ removed, and let $f_{S \\setminus \\{i\\}}(x)$ be the corresponding predictor. Let $A = X^\\top X + \\lambda I_d$, where $I_d$ is the $d \\times d$ identity matrix.\n\nTasks:\n1. Starting from the normal equations for ridge regression and the Sherman–Morrison identity, derive an exact expression for the prediction change $f_S(x) - f_{S \\setminus \\{i\\}}(x)$ at an arbitrary query $x \\in \\mathbb{R}^{d}$ in terms of $A^{-1}$, $x$, $x_i$, the residual $y_i - f_S(x_i)$, and the leverage score $h_{ii}$ of the $i$-th training point under ridge regression. The leverage score is defined as the $i$-th diagonal element of the ridge hat matrix $H = X A^{-1} X^\\top$, that is, $h_{ii} = x_i^\\top A^{-1} x_i$.\n2. Using the Cauchy–Schwarz inequality on the quadratic form induced by $A^{-1}$, derive a computable upper bound on $\\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert$ that depends only on $\\lvert y_i - f_S(x_i) \\rvert$, the leverage score $h_{ii}$, and the query leverage $h_x = x^\\top A^{-1} x$.\n3. For each test case below, compute two quantities over a finite query set $Q$: \n   - The actual maximum prediction change $\\max_{x \\in Q} \\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert$.\n   - The leverage-only Cauchy–Schwarz upper bound $\\max_{x \\in Q} \\Big(\\sqrt{h_x \\, h_{ii}} \\cdot \\lvert y_i - f_S(x_i) \\rvert \\big/ (1 - h_{ii})\\Big)$.\n   Report also the leverage score $h_{ii}$ for the removed point. Interpret how a large leverage score indicates stability risk.\n\nImplementation requirements:\n- Your program must implement the derivation and compute the requested quantities exactly as specified.\n- All vectors and matrices are real-valued, and all calculations are dimensionally consistent.\n- There are no physical units or angles in this problem.\n\nTest suite:\nFor each case, you are given $(X, y, \\lambda, i, Q)$:\n- Case $1$ (high-leverage with large residual, weak regularization):\n  - $X = \\begin{bmatrix} 0.0 \\\\ 0.2 \\\\ 0.4 \\\\ 0.6 \\\\ 0.8 \\\\ 10.0 \\end{bmatrix}$ as an $n \\times d$ matrix with $n = 6$ and $d = 1$.\n  - $y = \\begin{bmatrix} 0.0,\\, 0.4,\\, 0.8,\\, 1.2,\\, 1.6,\\, 35.0 \\end{bmatrix}^\\top$.\n  - $\\lambda = 10^{-6}$.\n  - $i = 5$ (zero-based index; this is the sixth row, the leverage point).\n  - $Q = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 2.0 \\\\ 5.0 \\\\ 10.0 \\end{bmatrix}$ as an $m \\times d$ matrix with $d = 1$.\n- Case $2$ (same data, stronger regularization):\n  - $X, y$ as in Case $1$.\n  - $\\lambda = 5.0$.\n  - $i = 5$.\n  - $Q$ as in Case $1$.\n- Case $3$ (no leverage outlier, weak regularization):\n  - $X = \\begin{bmatrix} 0.0 \\\\ 0.25 \\\\ 0.5 \\\\ 0.75 \\\\ 1.0 \\\\ 1.25 \\end{bmatrix}$ as an $n \\times d$ matrix with $n = 6$ and $d = 1$.\n  - $y$ given by $y_j = 1.5 \\cdot x_j + 0.5 + \\varepsilon_j$ with $\\varepsilon = \\begin{bmatrix} 0.0,\\, 0.02,\\, -0.01,\\, 0.0,\\, 0.03,\\, -0.02 \\end{bmatrix}^\\top$, that is $y = \\begin{bmatrix} 0.5,\\, 0.875,\\, 1.24,\\, 1.625,\\, 2.03,\\, 2.355 \\end{bmatrix}^\\top$.\n  - $\\lambda = 10^{-6}$.\n  - $i = 2$.\n  - $Q = \\begin{bmatrix} 0.0 \\\\ 0.5 \\\\ 1.0 \\\\ 1.5 \\end{bmatrix}$.\n- Case $4$ (high-leverage aligned with the trend, weak regularization):\n  - $X$ as in Case $1$.\n  - $y$ given by the exact linear trend $y_j = 2.0 \\cdot x_j$, that is $y = \\begin{bmatrix} 0.0,\\, 0.4,\\, 0.8,\\, 1.2,\\, 1.6,\\, 20.0 \\end{bmatrix}^\\top$.\n  - $\\lambda = 10^{-6}$.\n  - $i = 5$.\n  - $Q$ as in Case $1$.\n\nRequired final output format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets.\n- For each case, output three floats in this order: \n  $[\\text{actual\\_max\\_change}, \\, h_{ii}, \\, \\text{cs\\_bound}]$.\n- Concatenate the results for the four cases into a single flat list. \n- Round every float to $6$ decimal places.\n- Example of the required structure with placeholders: $[a\\_1, h\\_1, b\\_1, a\\_2, h\\_2, b\\_2, a\\_3, h\\_3, b\\_3, a\\_4, h\\_4, b\\_4]$.",
            "solution": "The problem requires an analysis of the algorithmic stability of ridge regression by deriving and computing the change in the predictor when a single data point is removed. The derivation will proceed from first principles as requested.\n\n### Step 1: Derivation of the Prediction Change (Task 1)\n\nThe ridge regression objective function is given by:\n$$\nJ(w) = \\frac{1}{2}\\|Xw - y\\|_2^2 + \\frac{\\lambda}{2}\\|w\\|_2^2\n$$\nwhere $X \\in \\mathbb{R}^{n \\times d}$ is the design matrix, $y \\in \\mathbb{R}^{n}$ is the response vector, $w \\in \\mathbb{R}^{d}$ is the weight vector, and $\\lambda > 0$ is the regularization parameter.\n\nTo find the optimal weight vector $w_S$ that minimizes $J(w)$, we compute the gradient with respect to $w$ and set it to zero:\n$$\n\\nabla_w J(w) = X^\\top(Xw - y) + \\lambda w = 0\n$$\nThis leads to the normal equations for ridge regression:\n$$\n(X^\\top X + \\lambda I_d)w = X^\\top y\n$$\nDefining $A = X^\\top X + \\lambda I_d$, where $I_d$ is the $d \\times d$ identity matrix, the solution for the full dataset $S$ is:\n$$\nw_S = (X^\\top X + \\lambda I_d)^{-1} X^\\top y = A^{-1} X^\\top y\n$$\nSince $\\lambda > 0$ and $X^\\top X$ is positive semi-definite, $A$ is guaranteed to be positive definite and thus invertible.\n\nNow, consider the dataset $S \\setminus \\{i\\}$, where the $i$-th example $(x_i, y_i)$ is removed. Let $X_{\\setminus i} \\in \\mathbb{R}^{(n-1) \\times d}$ and $y_{\\setminus i} \\in \\mathbb{R}^{n-1}$ be the data matrices with the $i$-th row and element removed, respectively. The weight vector for this reduced dataset, $w_{S \\setminus \\{i\\}}$, is the solution to the corresponding ridge regression problem:\n$$\nw_{S \\setminus \\{i\\}} = (X_{\\setminus i}^\\top X_{\\setminus i} + \\lambda I_d)^{-1} X_{\\setminus i}^\\top y_{\\setminus i}\n$$\nWe can express the terms $X_{\\setminus i}^\\top X_{\\setminus i}$ and $X_{\\setminus i}^\\top y_{\\setminus i}$ in terms of the full dataset quantities. The matrix $X^\\top X$ is the sum of outer products $\\sum_{j=1}^n x_j x_j^\\top$. Removing the $i$-th point yields:\n$$\nX_{\\setminus i}^\\top X_{\\setminus i} = \\sum_{j \\neq i} x_j x_j^\\top = \\left(\\sum_{j=1}^n x_j x_j^\\top\\right) - x_i x_i^\\top = X^\\top X - x_i x_i^\\top\n$$\nSimilarly, for the cross-product term:\n$$\nX_{\\setminus i}^\\top y_{\\setminus i} = \\sum_{j \\neq i} x_j y_j = \\left(\\sum_{j=1}^n x_j y_j\\right) - x_i y_i = X^\\top y - x_i y_i\n$$\nLet $A_{\\setminus i} = X_{\\setminus i}^\\top X_{\\setminus i} + \\lambda I_d$. Substituting the expression for $X_{\\setminus i}^\\top X_{\\setminus i}$:\n$$\nA_{\\setminus i} = (X^\\top X - x_i x_i^\\top) + \\lambda I_d = (X^\\top X + \\lambda I_d) - x_i x_i^\\top = A - x_i x_i^\\top\n$$\nThis shows that $A_{\\setminus i}$ is a rank-one update of $A$. We can find its inverse using the Sherman–Morrison formula: for an invertible matrix $M$ and vectors $u, v$, $(M - uv^\\top)^{-1} = M^{-1} + \\frac{M^{-1}uv^\\top M^{-1}}{1 - v^\\top M^{-1}u}$.\nWith $M=A$ and $u=v=x_i$, we get:\n$$\nA_{\\setminus i}^{-1} = (A - x_i x_i^\\top)^{-1} = A^{-1} + \\frac{A^{-1}x_i x_i^\\top A^{-1}}{1 - x_i^\\top A^{-1}x_i}\n$$\nThe term $x_i^\\top A^{-1} x_i$ is the leverage score of the $i$-th point, denoted $h_{ii}$. So,\n$$\nA_{\\setminus i}^{-1} = A^{-1} + \\frac{A^{-1}x_i x_i^\\top A^{-1}}{1 - h_{ii}}\n$$\nNow we can write the expression for $w_{S \\setminus \\{i\\}}$:\n$$\nw_{S \\setminus \\{i\\}} = A_{\\setminus i}^{-1} (X^\\top y - x_i y_i) = \\left(A^{-1} + \\frac{A^{-1}x_i x_i^\\top A^{-1}}{1 - h_{ii}}\\right) (X^\\top y - x_i y_i)\n$$\nExpanding this, and substituting $w_S = A^{-1} X^\\top y$:\n$$\nw_{S \\setminus \\{i\\}} = A^{-1}X^\\top y - A^{-1}x_i y_i + \\frac{A^{-1}x_i x_i^\\top A^{-1}X^\\top y - A^{-1}x_i (x_i^\\top A^{-1}x_i) y_i}{1-h_{ii}} \\\\\n= w_S - A^{-1}x_i y_i + \\frac{A^{-1}x_i x_i^\\top w_S - A^{-1}x_i h_{ii} y_i}{1-h_{ii}}\n$$\nLet's find the change in the weight vector, $w_S - w_{S \\setminus \\{i\\}}$:\n$$\nw_S - w_{S \\setminus \\{i\\}} = A^{-1}x_i y_i - \\frac{A^{-1}x_i (x_i^\\top w_S - h_{ii} y_i)}{1 - h_{ii}} \\\\\n= A^{-1}x_i \\left( y_i - \\frac{x_i^\\top w_S - h_{ii} y_i}{1 - h_{ii}} \\right) \\\\\n= A^{-1}x_i \\left( \\frac{y_i(1 - h_{ii}) - (x_i^\\top w_S - h_{ii} y_i)}{1 - h_{ii}} \\right) \\\\\n= A^{-1}x_i \\left( \\frac{y_i - y_i h_{ii} - x_i^\\top w_S + h_{ii} y_i}{1 - h_{ii}} \\right) \\\\\n= A^{-1}x_i \\frac{y_i - x_i^\\top w_S}{1 - h_{ii}}\n$$\nThe term $y_i - x_i^\\top w_S$ is the residual of the $i$-th point, which is $y_i - f_S(x_i)$. The change in the predictor at a query point $x$ is $f_S(x) - f_{S \\setminus \\{i\\}}(x) = x^\\top(w_S - w_{S \\setminus \\{i\\}})$. Substituting the expression for the weight change:\n$$\nf_S(x) - f_{S \\setminus \\{i\\}}(x) = x^\\top \\left( A^{-1}x_i \\frac{y_i - x_i^\\top w_S}{1 - h_{ii}} \\right)\n$$\nRearranging the scalars, we arrive at the exact expression for the prediction change:\n$$\nf_S(x) - f_{S \\setminus \\{i\\}}(x) = \\frac{y_i - f_S(x_i)}{1 - h_{ii}} x^\\top A^{-1} x_i\n$$\n\n### Step 2: Derivation of the Cauchy-Schwarz Upper Bound (Task 2)\n\nTo bound the magnitude of the prediction change, we take the absolute value of the expression derived above:\n$$\n\\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert = \\left\\lvert \\frac{y_i - f_S(x_i)}{1 - h_{ii}} x^\\top A^{-1} x_i \\right\\rvert = \\frac{\\lvert y_i - f_S(x_i) \\rvert}{\\lvert 1 - h_{ii} \\rvert} \\lvert x^\\top A^{-1} x_i \\rvert\n$$\nSince $A = X^\\top X + \\lambda I_d$ with $\\lambda > 0$, $A$ is symmetric and positive definite (SPD). Its inverse $A^{-1}$ is also SPD. For any SPD matrix $M$, we can define an inner product $\\langle u, v \\rangle_M = u^\\top M v$. The Cauchy-Schwarz inequality for this inner product is $\\lvert \\langle u, v \\rangle_M \\rvert^2 \\le \\langle u, u \\rangle_M \\langle v, v \\rangle_M$.\nApplying this with $M = A^{-1}$, $u = x$, and $v = x_i$:\n$$\n\\lvert x^\\top A^{-1} x_i \\rvert \\le \\sqrt{(x^\\top A^{-1} x)(x_i^\\top A^{-1} x_i)}\n$$\nUsing the given definitions of the query leverage $h_x = x^\\top A^{-1} x$ and the point's leverage score $h_{ii} = x_i^\\top A^{-1} x_i$, the inequality becomes:\n$$\n\\lvert x^\\top A^{-1} x_i \\rvert \\le \\sqrt{h_x h_{ii}}\n$$\nThe leverage scores for ridge regression $h_{ii}$ are known to be in the range $(0, 1)$. Therefore, $1 - h_{ii} > 0$, and $\\lvert 1 - h_{ii} \\rvert = 1 - h_{ii}$. Substituting the bound for $\\lvert x^\\top A^{-1} x_i \\rvert$ into the expression for prediction change magnitude, we obtain the upper bound:\n$$\n\\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert \\le \\frac{\\lvert y_i - f_S(x_i) \\rvert}{1 - h_{ii}} \\sqrt{h_x h_{ii}}\n$$\nThis is the desired computable upper bound.\n\n### Step 3: Numerical Computation and Interpretation (Task 3)\n\nThe derived formulas will now be implemented to compute the required quantities for the given test cases. The quantities are:\n1.  $\\max_{x \\in Q} \\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert$: The maximum actual change in prediction over the query set $Q$.\n2.  $h_{ii}$: The leverage score of the removed point.\n3.  $\\max_{x \\in Q} \\Big(\\frac{\\lvert y_i - f_S(x_i) \\rvert}{1 - h_{ii}} \\sqrt{h_x h_{ii}}\\Big)$: The maximum value of the Cauchy–Schwarz upper bound over the query set $Q$.\n\nThe analysis of these quantities across the test cases reveals key insights into algorithmic stability. The derived bound shows that a point $(x_i, y_i)$ is influential (i.e., its removal causes a large change in the predictor) if two conditions are met simultaneously:\n-   The point has a high leverage score $h_{ii}$, which approaches $1$. This makes the denominator $1 - h_{ii}$ small, amplifying the effect.\n-   The point has a large residual $\\lvert y_i - f_S(x_i) \\rvert$, meaning it is poorly explained by the model trained on the full dataset.\n\nCase 1 (high leverage, large residual) and Case 4 (high leverage, small residual) will highlight this interaction. Case 2 demonstrates how increasing regularization $\\lambda$ affects stability, and Case 3 serves as a baseline with no high-leverage points. Note that for the $d=1$ case, the Cauchy-Schwarz inequality becomes an equality, so the actual change and the bound will be identical.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the algorithmic stability problem for ridge regression.\n    The function iterates through predefined test cases, calculates the required\n    quantities based on the derived formulas, and prints the results in the\n    specified format.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: High-leverage with large residual, weak regularization\n        {\n            \"X\": np.array([[0.0], [0.2], [0.4], [0.6], [0.8], [10.0]]),\n            \"y\": np.array([[0.0], [0.4], [0.8], [1.2], [1.6], [35.0]]),\n            \"lambda\": 1e-6,\n            \"i\": 5,\n            \"Q\": np.array([[0.0], [1.0], [2.0], [5.0], [10.0]])\n        },\n        # Case 2: Same data, stronger regularization\n        {\n            \"X\": np.array([[0.0], [0.2], [0.4], [0.6], [0.8], [10.0]]),\n            \"y\": np.array([[0.0], [0.4], [0.8], [1.2], [1.6], [35.0]]),\n            \"lambda\": 5.0,\n            \"i\": 5,\n            \"Q\": np.array([[0.0], [1.0], [2.0], [5.0], [10.0]])\n        },\n        # Case 3: No leverage outlier, weak regularization\n        {\n            \"X\": np.array([[0.0], [0.25], [0.5], [0.75], [1.0], [1.25]]),\n            \"y\": np.array([[0.5], [0.875], [1.24], [1.625], [2.03], [2.355]]),\n            \"lambda\": 1e-6,\n            \"i\": 2,\n            \"Q\": np.array([[0.0], [0.5], [1.0], [1.5]])\n        },\n        # Case 4: High-leverage aligned with the trend, weak regularization\n        {\n            \"X\": np.array([[0.0], [0.2], [0.4], [0.6], [0.8], [10.0]]),\n            \"y\": np.array([[0.0], [0.4], [0.8], [1.2], [1.6], [20.0]]),\n            \"lambda\": 1e-6,\n            \"i\": 5,\n            \"Q\": np.array([[0.0], [1.0], [2.0], [5.0], [10.0]])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X, y, lambda_reg, i, Q = case[\"X\"], case[\"y\"], case[\"lambda\"], case[\"i\"], case[\"Q\"]\n        n, d = X.shape\n\n        # Step 1: Compute full dataset solution w_S\n        A = X.T @ X + lambda_reg * np.eye(d)\n        A_inv = np.linalg.inv(A)\n        w_S = A_inv @ X.T @ y\n\n        # Step 2: Extract point i and compute its leverage and residual\n        x_i_row = X[i:i+1, :]  # Shape (1, d)\n        x_i_col = x_i_row.T    # Shape (d, 1)\n        y_i_val = y[i, 0]\n\n        # Leverage score h_ii = x_i^T A^{-1} x_i\n        h_ii = (x_i_row @ A_inv @ x_i_col)[0, 0]\n\n        # Residual r_i = y_i - f_S(x_i)\n        f_S_xi = (x_i_row @ w_S)[0, 0]\n        r_i = y_i_val - f_S_xi\n\n        # Step 3: Compute actual maximum prediction change over Q\n        # Change formula: (r_i / (1 - h_ii)) * x^T A^{-1} x_i\n        change_factor = r_i / (1 - h_ii)\n        \n        # Q @ A_inv @ x_i_col broadcasts the calculation over all x in Q\n        cross_terms = Q @ A_inv @ x_i_col # Shape (m, 1)\n        \n        actual_changes = np.abs(change_factor * cross_terms)\n        actual_max_change = np.max(actual_changes)\n\n        # Step 4: Compute the Cauchy-Schwarz upper bound over Q\n        # Bound formula: |r_i|/(1-h_ii) * sqrt(h_x * h_ii)\n        \n        # Compute query leverages h_x = x^T A^{-1} x for all x in Q\n        # np.sum((Q @ A_inv) * Q, axis=1) is an efficient way to get diagonals of Q A_inv Q.T\n        h_Q = np.sum((Q @ A_inv) * Q, axis=1) # Shape (m,)\n        \n        bounds_vec = (np.abs(r_i) / (1 - h_ii)) * np.sqrt(h_Q * h_ii)\n        cs_bound = np.max(bounds_vec)\n\n        # Append results for this case\n        results.extend([actual_max_change, h_ii, cs_bound])\n        \n    # Format the final output string\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}