{
    "hands_on_practices": [
        {
            "introduction": "为了在统计学习中建立稳健的模型，理解线性相关性的后果至关重要。本次练习将通过一个特意构造的设计矩阵，让你亲手验证列向量之间的线性依赖关系。通过从第一性原理出发计算矩阵的秩和零空间，你将深刻理解为什么多重共线性会导致模型参数无法唯一确定，并为后续学习正则化等解决方案奠定基础。",
            "id": "3140085",
            "problem": "考虑一个用于统计学习的线性回归设计矩阵 $X \\in \\mathbb{R}^{4 \\times 5}$，其列向量 $c_{1}, c_{2}, c_{3}, c_{4}, c_{5} \\in \\mathbb{R}^{4}$ 如下所示：\n$$\nc_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix},\\quad\nc_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 3 \\\\ 2 \\end{pmatrix},\\quad\nc_{4} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\end{pmatrix},\n$$\n并定义两个额外的特征为精确的和：\n$$\nc_{3} = c_{1} + c_{2},\\quad c_{5} = c_{2} + c_{4}.\n$$\n令 $X = [\\,c_{1}\\; c_{2}\\; c_{3}\\; c_{4}\\; c_{5}\\,]$。\n\n仅使用适用于统计学习的线性代数基本定义（线性无关、秩、零空间和单位矩阵的定义），完成以下任务：\n\n- 验证各列之间指定的线性依赖关系，并确定 $c_{1}, c_{2}, c_{4}$ 是否线性无关。\n- 根据第一性原理，确定 $X$ 的秩，并计算其零空间 $\\mathcal{N}(X) = \\{\\,w \\in \\mathbb{R}^{5} : Xw = 0\\,\\}$ 的一组基。\n- 在均值为零的噪声 $\\varepsilon$ 的线性模型 $y = X\\beta + \\varepsilon$ 的背景下，讨论任何非平凡零空间对参数可解释性的影响，并从概念上解释向格拉姆型矩阵添加单位矩阵 $I_{5}$ 的正倍数与可辨识性的关系。\n\n你的最终答案必须是给出 $X$ 零空间的一组基的单个解析表达式（你可以将基向量表示为单个矩阵的列）。请勿在最终答案框中包含解释性文本。",
            "solution": "该问题经评估有效，因为它在数学和科学上是合理的，内容完整，提法明确，并且与线性代数和统计学习的指定主题直接相关。我们可以进行完整解答。\n\n设计矩阵为 $X \\in \\mathbb{R}^{4 \\times 5}$，其列向量为 $c_1, c_2, c_3, c_4, c_5 \\in \\mathbb{R}^4$。\n列向量给定如下：\n$c_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix}$，$c_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 3 \\\\ 2 \\end{pmatrix}$，$c_{4} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 3 \\end{pmatrix}$。\n其余的列由线性组合定义：\n$c_{3} = c_{1} + c_{2}$\n$c_{5} = c_{2} + c_{4}$\n\n首先，我们验证向量 $c_1, c_2, c_4$ 是否线性无关。根据定义，一组向量是线性无关的，如果它们等于零向量的唯一线性组合是所有系数都为零的平凡组合。我们寻求标量 $a_1, a_2, a_4 \\in \\mathbb{R}$ 的解，满足方程：\n$$a_1 c_1 + a_2 c_2 + a_4 c_4 = 0$$\n这个向量方程对应于一个齐次线性方程组：\n$$\n\\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 2  3  0 \\\\ -1  2  3 \\end{pmatrix}\n\\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_4 \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n我们对系数矩阵进行高斯消元：\n$$\n\\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 2  3  0 \\\\ -1  2  3 \\end{pmatrix}\n\\xrightarrow[R_4 \\to R_4 + R_1]{R_3 \\to R_3 - 2R_1}\n\\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 0  3  -4 \\\\ 0  2  5 \\end{pmatrix}\n\\xrightarrow[R_4 \\to R_4 - 2R_2]{R_3 \\to R_3 - 3R_2}\n\\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 0  0  -1 \\\\ 0  0  7 \\end{pmatrix}\n\\xrightarrow{R_4 \\to R_4 + 7R_3}\n\\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 0  0  -1 \\\\ 0  0  0 \\end{pmatrix}\n$$\n行阶梯形矩阵有 $3$ 个主元。这表明该方程组的唯一解是平凡解 $a_1 = 0$, $a_2 = 0$, $a_4 = 0$。因此，向量 $c_1, c_2, c_4$ 是线性无关的。\n\n接下来，我们确定 $X$ 的秩。矩阵的秩是其列空间的维度，等价于线性无关列的最大数量。$X$ 的列空间是 $\\mathcal{C}(X) = \\text{span}\\{c_1, c_2, c_3, c_4, c_5\\}$。使用 $c_3$ 和 $c_5$ 的给定定义：\n$$\\mathcal{C}(X) = \\text{span}\\{c_1, c_2, c_1+c_2, c_4, c_2+c_4\\}$$\n由于 $c_3$ 和 $c_5$ 是集合中其他向量的线性组合，它们不增加生成空间的维度。因此，生成集可以简化为：\n$$\\mathcal{C}(X) = \\text{span}\\{c_1, c_2, c_4\\}$$\n正如我们刚刚证明的，$\\{c_1, c_2, c_4\\}$ 是一个线性无关集，它构成了 $X$ 的列空间的一组基。列空间的维度是其基中向量的数量，即 $3$。因此，$X$ 的秩为 $\\text{rank}(X) = 3$。\n\n现在，我们计算零空间 $\\mathcal{N}(X)$ 的一组基。零空间是所有满足 $Xw = 0$ 的向量 $w \\in \\mathbb{R}^5$ 的集合。令 $w = (w_1, w_2, w_3, w_4, w_5)^T$。条件 $Xw = 0$ 可以写成 $X$ 的列向量的线性组合：\n$$w_1 c_1 + w_2 c_2 + w_3 c_3 + w_4 c_4 + w_5 c_5 = 0$$\n代入 $c_3$ 和 $c_5$ 的定义：\n$$w_1 c_1 + w_2 c_2 + w_3 (c_1 + c_2) + w_4 c_4 + w_5 (c_2 + c_4) = 0$$\n我们按线性无关的向量 $c_1, c_2, c_4$ 对各项进行分组：\n$$(w_1 + w_3)c_1 + (w_2 + w_3 + w_5)c_2 + (w_4 + w_5)c_4 = 0$$\n由于 $c_1, c_2, c_4$ 是线性无关的，它们的系数必须全为零：\n\\begin{align*} w_1 + w_3 = 0 \\\\ w_2 + w_3 + w_5 = 0 \\\\ w_4 + w_5 = 0 \\end{align*}\n这是一个包含 $5$ 个未知数的 $3$ 个方程组。自由变量的数量是 $5 - 3 = 2$，这对应于 $X$ 的零度。这与秩-零度定理一致：$\\text{rank}(X) + \\text{nullity}(X) = 5$。我们选择 $w_3$ 和 $w_5$ 作为自由参数。令 $w_3 = s$ 和 $w_5 = t$，对于任意 $s, t \\in \\mathbb{R}$。\n由第一个方程得：$w_1 = -w_3 = -s$。\n由第三个方程得：$w_4 = -w_5 = -t$。\n由第二个方程得：$w_2 = -w_3 - w_5 = -s - t$。\n通解向量 $w$ 为：\n$$w = \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\\\ w_5 \\end{pmatrix} = \\begin{pmatrix} -s \\\\ -s-t \\\\ s \\\\ -t \\\\ t \\end{pmatrix} = s \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + t \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 1 \\end{pmatrix}$$\n零空间的一组基由将一个自由参数设为 $1$ 而另一个设为 $0$ 所对应的向量构成。\n令 $v_1 = (-1, -1, 1, 0, 0)^T$（对于 $s=1, t=0$）和 $v_2 = (0, -1, 0, -1, 1)^T$（对于 $s=0, t=1$）。\n集合 $\\{v_1, v_2\\}$ 是 $\\mathcal{N}(X)$ 的一组基。\n\n在线性模型 $y = X\\beta + \\varepsilon$ 的背景下，$X$ 的非平凡零空间（意即 $\\mathcal{N}(X) \\neq \\{0\\}$）表示预测变量之间存在完全多重共线性。这对参数的可解释性有严重影响。普通最小二乘法（OLS）旨在寻找一个系数向量 $\\beta$ 来最小化残差平方和，这需要求解正规方程 $(X^T X)\\beta = X^T y$。如果 $X$ 的列向量是线性相关的，那么格拉姆矩阵 $X^T X$ 是奇异的，不可逆。因此，$\\beta$ 没有唯一解。\n如果 $\\hat{\\beta}$ 是任意一个特解，那么对于任意向量 $w \\in \\mathcal{N}(X)$，向量 $\\beta^* = \\hat{\\beta} + w$ 也是一个解，因为 $X\\beta^* = X(\\hat{\\beta} + w) = X\\hat{\\beta} + Xw = X\\hat{\\beta} + 0 = X\\hat{\\beta}$。这意味着无数个不同的系数向量 $\\beta$ 会产生完全相同的预测。这使得辨识每个预测变量的独特“效应”变得不可能。例如，使用基向量 $v_1$，我们看到将 $\\beta_1$ 和 $\\beta_2$ 减少某个量 $k$ 的同时将 $\\beta_3$ 增加 $k$，会得到一个观测上完全相同的模型。各个系数 $\\beta_j$ 是不可辨识的。\n\n向格拉姆矩阵 $X^T X$ 添加单位矩阵的正倍数 $\\lambda I_5$（其中 $\\lambda  0$）是岭回归的核心机制。岭回归的解是 $\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I_5)^{-1} X^T y$。这个过程通过保证矩阵 $(X^T X + \\lambda I_5)$ 即使在 $X^T X$ 是奇异的情况下也是可逆的，从而确保了可辨识性。\n从概念上讲，这是因为 $X^T X$ 是一个半正定矩阵，意味着它的所有特征值 $\\mu_i$ 都是非负的（$\\mu_i \\ge 0$）。由于 $X$ 的列是线性相关的，$X^T X$ 是奇异的，这意味着它至少有一个特征值恰好为 $0$。设 $v$ 是 $X^T X$ 的一个特征向量，其特征值为 $\\mu$。那么 $(X^T X + \\lambda I_5)v = (X^T X)v + (\\lambda I_5)v = \\mu v + \\lambda v = (\\mu + \\lambda)v$。这表明正则化矩阵 $(X^T X + \\lambda I_5)$ 的特征值是 $(\\mu_i + \\lambda)$。因为 $\\mu_i \\ge 0$ 且 $\\lambda  0$，所有的特征值 $(\\mu_i + \\lambda)$ 都是严格为正的。一个方阵可逆当且仅当它的所有特征值都非零。通过将所有特征值移动到严格为正，矩阵 $(X^T X + \\lambda I_5)$ 变得可逆，从而为 $\\beta$ 提供了一个唯一解。这种正则化方法在数学上解决了多重共线性问题，允许对一组唯一的（尽管有偏的）参数进行稳定估计。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} -1  0 \\\\ -1  -1 \\\\ 1  0 \\\\ 0  -1 \\\\ 0  1 \\end{pmatrix} } $$"
        },
        {
            "introduction": "理论概念只有在实践中才能真正被掌握。本次动手编码练习将模拟统计建模中的一个常见场景：处理分类变量。你将通过对分类特征进行独热编码（one-hot encoding）来构建设计矩阵，并探索包含截距项、交互项和多项式项时，不同编码策略如何导致结构性的秩亏损。这个练习能让你直观地看到多重共线性问题是如何在特征工程阶段产生的。",
            "id": "3140137",
            "problem": "考虑一个统计学习中的回归设计矩阵，该矩阵包含分类编码、多项式项和交互作用项。请使用以下基础定义作为您推理和建构的基础：如果 $X\\alpha = 0$ 的唯一解是 $\\alpha = 0$，则矩阵 $X$ 的各列是线性独立的；列秩 $\\operatorname{rank}(X)$ 是 $X$ 的列空间的维度；单位矩阵 $I_p$ 是一个 $p \\times p$ 的矩阵，其对角线上的元素为 1，其余元素为 0；在分类变量的独热编码中，每个观测值恰好有一个指示符为 $1$，其余为 $0$；对于任何虚拟变量 $d \\in \\{0,1\\}$ 及任何整数 $m \\ge 1$，多项式等式 $d^m = d$ 恒成立。请仅使用这些基本事实来建构设计矩阵 $X$，不要假设任何额外的快捷公式。\n\n给定 $n = 12$ 个观测值，包含两个分类变量 $C$ 和 $D$。变量 $C$ 有 $3$ 个水平，标记为 $\\{0,1,2\\}$，变量 $D$ 有 $2$ 个水平，标记为 $\\{0,1\\}$。观测序列按以下顺序涵盖每个 $(C,D)$ 配对两次：\n$[(0,0),(0,1),(1,0),(1,1),(2,0),(2,1),(0,0),(0,1),(1,0),(1,1),(2,0),(2,1)]$。\n根据这些数据，为 $C$ 建构独热指示符列（表示为 $C0$, $C1$, $C2$），并为 $D$ 建构独热指示符列（表示为 $D0$, $D1$）。对于交互作用项，将配对 $(C=c,D=d)$ 的单元格指示符按字典顺序表示为 $J00$, $J01$, $J10$, $J11$, $J20$, $J21$。将截距项列表示为 $I$（全为 1 的列向量）。\n\n您必须编写一个完整的程序，完全按照指定的列顺序建构以下三个测试案例的设计矩阵 $X$，从第一性原理计算列秩 $\\operatorname{rank}(X)$，并识别由冗余编码引起的结构性秩亏。如果 $X$ 中的某一列是其他列的线性组合，则该列是结构性冗余的。请报告所有结构性冗余列的从零开始的索引，这些列被定义为不属于极大线性独立列集合的那些列。\n\n测试套件：\n- 测试案例 1（快乐路径，通过完整编码、交互作用项和多项式重复来刻意制造冗余）：\n  按此确切顺序建构 $X$ 的列：\n  $[I, C0, C1, C2, D0, D1, J00, J01, J10, J11, J20, J21, C0^2, C1^2, C2^2, J00^2, J01^2, J10^2, J11^2, J20^2, J21^2]$。\n  此处 $Ck^2$ 和 $Jcd^2$ 表示对应虚拟列的逐元素平方。\n- 测试案例 2（边界案例，使用参考水平编码以避免冗余）：\n  建构无截距项的 $X$，并对主效应使用参考水平编码。保留 $C0$ 和 $C1$（将 $C2$ 作为参考水平舍弃），并保留 $D0$（将 $D1$ 作为参考水平舍弃）。仅包含由保留的主效应虚拟变量的乘积形成的交互作用项。确切的列顺序为：\n  $[C0, C1, D0, C0 \\cdot D0, C1 \\cdot D0]$。\n- 测试案例 3（边缘案例，包含显式重复列和一个零列）：\n  按此确切顺序建构 $X$ 的列：\n  $[I, C0, C1, C2, D0, D1, I_{\\text{dup}}, C1_{\\text{dup}}, Z]$，\n  其中 $I_{\\text{dup}}$ 是 $I$ 的直接重复，$C1_{\\text{dup}}$ 是 $C1$ 的直接重复，而 $Z$ 是逐元素差值 $D1 - D1^2$。\n\n算法要求：\n- 使用基于基础定义的数值稳健线性代数程序（例如，选择极大独立列集合的列主元分解）计算 $\\operatorname{rank}(X)$。\n- 识别一个极大线性独立列集合，并将所有剩余列的从零开始的索引报告为结构性冗余。\n\n最终输出格式：\n您的程序应产生单行输出，其中包含三个测试案例的汇总结果，格式为列表的列表，每个内部列表的形式为 $[p, r, \\text{dep}]$，其中 $p$ 是 $X$ 的列数（一个整数），$r$ 是计算出的列秩（一个整数），$\\text{dep}$ 是结构性冗余列的从零开始的索引列表。例如，打印的输出必须看起来像 $[[p_1,r_1,[\\dots]],[p_2,r_2,[\\dots]],[p_3,r_3,[\\dots]]]$，不得包含额外文字。\n\n此任务不涉及任何物理单位或角度单位。所有数值都应以整数或整数列表的形式报告。请确保科学上的真实性，方法是完全按照所述，从指定的独热编码、交互作用项和逐元素多项式来建构 $X$，并严格通过线性代数计算秩和冗余性。",
            "solution": "该问题要求建构并分析三个由分类特征和多项式特征衍生的设计矩阵 $X$。核心任务是确定每个矩阵的列秩，并识别所有结构性冗余列。此分析必须基于线性代数的第一性原理。\n\n### 基础原理\n\n1.  **线性独立性与列秩**：矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列（表示为 $x_1, \\dots, x_p$）是线性独立的，如果向量方程式 $\\sum_{j=1}^{p} \\alpha_j x_j = 0$（或 $X\\alpha = 0$）的唯一解是平凡解 $\\alpha_1 = \\dots = \\alpha_p = 0$。$X$ 的列秩，表示为 $\\operatorname{rank}(X)$，是 $X$ 中线性独立的最大可能列子集的大小。这等同于 $X$ 的列空间的维度，列空间是由其列向量所张成的向量空间。\n\n2.  **设计矩阵的建构**：问题提供了 $n=12$ 个包含两个分类变量 $C \\in \\{0, 1, 2\\}$ 和 $D \\in \\{0, 1\\}$ 的观测值。设计矩阵是从这些变量建构的。\n    - **截距项**：截距项列 $I$ 是一个全为 1 的向量，代表回归模型中的常数偏移。\n    - **独热编码**：对于一个有 $k$ 个水平的分类变量，会创建 $k$ 个二元指示符（虚拟）列。对于每个观测值，对应于观测水平的列被设为 $1$，所有其他 $k-1$ 个指示符列被设为 $0$。对变量 $C$ 这样做，得到 $C0, C1, C2$ 列。对变量 $D$ 这样做，得到 $D0, D1$ 列。\n    - **交互作用项**：交互作用项捕捉特定于不同变量水平组合的效应。对于单元格 $(C=c, D=d)$ 的交互作用项列，表示为 $Jcd$，在观测值为 $C=c$ 和 $D=d$ 时为 $1$，否则为 $0$。这等同于对应主效应指示符的逐元素乘积：$Jcd = Cc \\cdot Dd$。\n    - **多项式项**：对于一个虚拟变量 $d \\in \\{0, 1\\}$，任何整数次方 $d^m$（其中 $m \\ge 1$）都等于 $d$ 本身，因为 $0^m = 0$ 且 $1^m = 1$。$d^m=d$ 这一性质是识别某些冗余性的关键洞见。\n\n### 秩与冗余侦测的算法\n\n要确定秩并识别冗余列，需要一种数值稳健的方法。列主元 QR 分解是一个绝佳的选择。对于一个矩阵 $X \\in \\mathbb{R}^{n \\times p}$，此分解计算：\n$$\nXP = QR\n$$\n其中 $P$ 是一个 $p \\times p$ 的置换矩阵，用于重新排列 $X$ 的列；$Q$ 是一个 $n \\times n$ 的正交矩阵；$R$ 是一个 $p \\times p$ 的上梯形矩阵，其对角线元素 $|R_{11}| \\ge |R_{22}| \\ge \\dots \\ge |R_{pp}|$ 的量值非递增。\n\n$X$ 的秩 $r$ 是 $R$ 中非零对角线元素的数量。在数值计算上，如果一个元素 $R_{kk}$ 的量值大于一个很小的容差 $\\tau$，则视其为非零。对于 $\\tau$ 的一个稳健选择是相对于最大的对角线元素和矩阵维度，例如 $\\tau = \\max(n, p) \\cdot |R_{11}| \\cdot \\epsilon$，其中 $\\epsilon$ 是机器精度。\n\n置换矩阵 $P$（表示为主元索引的向量）有效地从 $X$ 中选出-一个极大线性独立列集合。主元向量 $P$ 中的前 $r$ 个索引对应于构成列空间基底的 $X$ 的列。$P$ 中剩余的 $p-r$ 个索引对应于线性相依于所选基底列的列。这些就是结构性冗余列。\n\n### 测试案例分析\n\n首先，我们从给定的观测序列建构基础列向量。设 $n=12$。\n$(C, D)$ 配对的序列是 $[(0,0), (0,1), (1,0), (1,1), (2,0), (2,1), (0,0), (0,1), (1,0), (1,1), (2,0), (2,1)]$。\n这定义了以下列（作为 $12 \\times 1$ 的向量）：\n- $I = [1,1,1,1,1,1,1,1,1,1,1,1]^T$\n- $C0 = [1,1,0,0,0,0,1,1,0,0,0,0]^T$\n- $C1 = [0,0,1,1,0,0,0,0,1,1,0,0]^T$\n- $C2 = [0,0,0,0,1,1,0,0,0,0,1,1]^T$\n- $D0 = [1,0,1,0,1,0,1,0,1,0,1,0]^T$\n- $D1 = [0,1,0,1,0,1,0,1,0,1,0,1]^T$\n- 交互作用项列 $Jcd = Cc \\cdot Dd$ 是 $3 \\times 2 = 6$ 个唯一单元格的指示符。\n\n**测试案例 1**：\n$X_1$ 有 $p_1=21$ 列：$[I, C0, C1, C2, D0, D1, J00, \\dots, J21, C0^2, \\dots, C2^2, J00^2, \\dots, J21^2]$。\n此矩阵有多重形式的结构性冗余：\n1.  **完整虚拟变量集冗余**：一个变量的独热编码列之和等于截距项列。因此，$C0+C1+C2=I$ 且 $D0+D1=I$。这引入了线性相依性。例如，$C2 = I - C0 - C1$。\n2.  **交互作用项冗余**：主效应列是交互作用项列的和。例如，$C0 = J00 + J01$ 且 $D0 = J00 + J10 + J20$。\n3.  **多项式冗余**：由于所有指示符列只包含 $0$ 和 $1$，将它们平方会得到相同的列，例如 $C0^2 = C0$。第 12 到 20 列是前面列的精确副本。\n所有这些列所张成空间的基本维度是唯一的 $(C,D)$ 配对数量，即 $6$。交互作用项列的集合 $\\{J00, \\dots, J21\\}$ 为这个 $6$ 维空间构成一个正交基。因此秩必定为 $r_1=6$。一个从左到右进行地贪婪列选择程序（如带主元的 QR 分解）将会选取它遇到的前 $6$ 个线性独立的列。一个可能的基底是 $\\{I, C0, C1, D0, J00, J10\\}$。所有其他 $21-6=15$ 列都是冗余的。\n\n**测试案例 2**：\n$X_2$ 有 $p_2=5$ 列：$[C0, C1, D0, C0 \\cdot D0, C1 \\cdot D0]$。\n此矩阵对应于一个无截距项的参考水平编码方案（通常称为虚拟编码）。$C2$ 和 $D1$ 被隐式地作为参考水平。这种建构旨在避免共线性。这些列是 $\\{C0, C1, D0, J00, J10\\}$。我们可以通过将它们的线性组合设为零，$a_1 C0 + a_2 C1 + a_3 D0 + a_4 J00 + a_5 J10 = 0$，并证明所有系数必须为零，来验证它们的线性独立性。例如，在一个 $(C,D)=(0,1)$ 的观测值上，只有 $C0$ 非零，这迫使 $a_1=0$。以此类推，可以确认所有系数都必须为零。因此，这些列是线性独立的。秩将等于列数，所以 $r_2=5$，并且没有冗余列。\n\n**测试案例 3**：\n$X_3$ 有 $p_3=9$ 列：$[I, C0, C1, C2, D0, D1, I_{\\text{dup}}, C1_{\\text{dup}}, Z]$。\n这个案例引入了显式冗余和一个零列。\n1.  **完整虚拟变量集冗余**：如同案例 1，我们有 $C2 = I - C0 - C1$ 和 $D1 = I - D0$。\n2.  **显式重复**：$I_{\\text{dup}}$ 与 $I$（第 0 列）完全相同，$C1_{\\text{dup}}$ 与 $C1$（第 2 列）完全相同。\n3.  **零列**：列 $Z = D1 - D1^2$ 是一个零向量，因为对于虚拟变量 $D1$，$D1^2=D1$。一个零列总是线性相依的，除非它是矩阵中唯一的列。\n秩由 $\\{I, C0, C1, C2, D0, D1\\}$ 中线性独立的列数决定。基于这些相依性，此集合的一个基底是 $\\{I, C0, C1, D0\\}$。这个集合有 $4$ 个向量，所以秩为 $r_3=4$。所有其他列（$C2$、$D1$、$I_{\\text{dup}}$、$C1_{\\text{dup}}$、$Z$）都是冗余的。这导致有 $9-4=5$ 个冗余列。",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem by constructing three design matrices, computing their\n    ranks, and identifying redundant columns using column-pivoted QR factorization.\n    \"\"\"\n    \n    # Define the observation data for C and D variables for n=12 observations.\n    obs = np.array([\n        (0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1),\n        (0, 0), (0, 1), (1, 0), (1, 1), (2, 0), (2, 1)\n    ])\n    c_obs = obs[:, 0]\n    d_obs = obs[:, 1]\n    n_obs = len(c_obs)\n\n    # Construct all necessary base column vectors as (n, 1) numpy arrays.\n    I = np.ones((n_obs, 1))\n    C0 = (c_obs == 0).astype(int).reshape(-1, 1)\n    C1 = (c_obs == 1).astype(int).reshape(-1, 1)\n    C2 = (c_obs == 2).astype(int).reshape(-1, 1)\n    D0 = (d_obs == 0).astype(int).reshape(-1, 1)\n    D1 = (d_obs == 1).astype(int).reshape(-1, 1)\n    \n    # Interaction columns Jcd = Cc * Dd\n    J00 = C0 * D0\n    J01 = C0 * D1\n    J10 = C1 * D0\n    J11 = C1 * D1\n    J20 = C2 * D0\n    J21 = C2 * D1\n\n    # Dictionary of all base columns for easy access\n    columns = {\n        'I': I, 'C0': C0, 'C1': C1, 'C2': C2, 'D0': D0, 'D1': D1,\n        'J00': J00, 'J01': J01, 'J10': J10, 'J11': J11, 'J20': J20, 'J21': J21,\n        'C0^2': C0**2, 'C1^2': C1**2, 'C2^2': C2**2,\n        'J00^2': J00**2, 'J01^2': J01**2, 'J10^2': J10**2, 'J11^2': J11**2,\n        'J20^2': J20**2, 'J21^2': J21**2,\n        'C0*D0': C0 * D0,\n        'C1*D0': C1 * D0,\n        'I_dup': I,\n        'C1_dup': C1,\n        'Z': D1 - D1**2\n    }\n\n    # Define the column sets for each test case\n    test_case_defs = {\n        1: ['I', 'C0', 'C1', 'C2', 'D0', 'D1', 'J00', 'J01', 'J10', 'J11', \n            'J20', 'J21', 'C0^2', 'C1^2', 'C2^2', 'J00^2', 'J01^2', 'J10^2', \n            'J11^2', 'J20^2', 'J21^2'],\n        2: ['C0', 'C1', 'D0', 'C0*D0', 'C1*D0'],\n        3: ['I', 'C0', 'C1', 'C2', 'D0', 'D1', 'I_dup', 'C1_dup', 'Z']\n    }\n\n    test_cases = []\n    for i in sorted(test_case_defs.keys()):\n        col_names = test_case_defs[i]\n        X = np.hstack([columns[name] for name in col_names])\n        test_cases.append(X)\n        \n    def analyze_matrix(X):\n        \"\"\"\n        Computes rank and redundant columns for a matrix X.\n        \n        Uses column-pivoted QR factorization. The rank is the number of\n        diagonal elements of R with magnitude above a tolerance. Redundant\n        columns are those not selected as pivots for the basis.\n        \"\"\"\n        n, p = X.shape\n        if p == 0:\n            return 0, 0, []\n        \n        Q, R, P = linalg.qr(X, pivoting=True)\n        \n        # Determine rank using a relative tolerance\n        diag_R = np.abs(np.diag(R))\n        tol = diag_R[0] * max(n, p) * np.finfo(R.dtype).eps if p > 0 else 0\n        \n        rank = np.sum(diag_R > tol)\n        \n        # Pivots P[:rank] are independent, P[rank:] are redundant\n        redundant_indices = sorted(P[rank:])\n        \n        return p, rank, redundant_indices\n\n    results = []\n    for X in test_cases:\n        p, r, dep = analyze_matrix(X)\n        results.append([p, r, dep])\n\n    # The string representation of list of lists is required.\n    # The template's suggestion `f\"[{','.join(map(str, results))}]\"`\n    # creates the desired format.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "在实际数据中，预测变量之间很少是完全正交的，也未必是完全线性相关的。因此，我们需要一个工具来衡量“接近”线性相关的程度及其影响。本次练习将引导你推导并计算方差膨胀因子（VIF），这是一个关键的回归诊断指标。通过处理给定的格拉姆矩阵 $X^{\\top} X$，你将学习如何将设计矩阵的几何性质与模型系数估计的方差联系起来，从而量化多重共线性的严重性。",
            "id": "3140092",
            "problem": "在统计学习中使用的线性回归中，考虑模型 $y = X\\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，$I_{n}$ 是 $n \\times n$ 的单位矩阵，$X$ 是一个 $n \\times p$ 的设计矩阵，有 $p=3$ 个预测变量，并且 $X$ 的列已经过均值中心化和标准化处理，使其具有单位方差。假设样本量为 $n=120$，格拉姆矩阵为\n$$\nX^{\\top} X = \\begin{pmatrix}\n120  108  24 \\\\\n108  120  12 \\\\\n24  12  120\n\\end{pmatrix}.\n$$\n仅使用基本定义，从普通最小二乘 (OLS) 估计量及其抽样方差，以及方差膨胀因子 (VIF) 的定义（即在相关预测变量下 OLS 系数的抽样方差与在具有相同边际尺度的正交预测变量下其抽样方差的比率）出发。在标准化预测变量的设定下，推导一个将 VIF 与 $(X^{\\top} X)^{-1}$ 的对角线元素联系起来的关系，然后计算由给定的 $X^{\\top} X$ 所隐含的三个预测变量的最大 VIF。将最终答案四舍五入到四位有效数字。将最终答案表示为一个不带单位的数字。",
            "solution": "首先评估问题的有效性。按要求逐字提取所有给定条件。\n- 模型：$y = X\\beta + \\varepsilon$\n- 误差分布：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$\n- 单位矩阵：$I_{n}$ 是 $n \\times n$ 的单位矩阵。\n- 设计矩阵：$X$ 是一个 $n \\times p$ 的矩阵。\n- 预测变量数量：$p=3$。\n- 样本量：$n=120$。\n- 标准化：$X$ 的列经过均值中心化和标准化，使其具有单位方差。\n- 格拉姆矩阵：$X^{\\top} X = \\begin{pmatrix} 120  108  24 \\\\ 108  120  12 \\\\ 24  12  120 \\end{pmatrix}$。\n- VIF 定义：在相关预测变量下 OLS 系数的抽样方差与在具有相同边际尺度的正交预测变量下其抽样方差的比率。\n- 任务：推导 VIF 与 $(X^{\\top} X)^{-1}$ 的对角线元素之间的关系，然后计算最大的 VIF。\n- 四舍五入：将最终答案四舍五入到四位有效数字。\n\n该问题是有效的。这是回归分析中的一个标准的、适定的问题，回归分析是统计学习中的一个主题。所提供的数据是内部一致的。预测变量被标准化为单位方差的条件意味着格拉姆矩阵 $X^{\\top} X$ 的对角元素是 $X_j^{\\top}X_j$。问题提供的对角元素为 $120$，这等于样本量 $n=120$。这对应于在这种情况下标准化的一个常见定义，即 $\\frac{1}{n} \\sum_{i=1}^n X_{ij}^2 = 1$，因此 $X_j^{\\top}X_j = \\sum_{i=1}^n X_{ij}^2 = n$。给定的格拉姆矩阵也是对称正定的，符合要求。\n\n按要求，解答过程分两部分进行：首先是推导，然后是计算。\n\n**第一部分：VIF 关系的推导**\n\n系数向量 $\\beta$ 的普通最小二乘 (OLS) 估计量由下式给出：\n$$\n\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y\n$$\n$\\hat{\\beta}$ 的抽样方差-协方差矩阵推导如下，将设计矩阵 $X$ 视为固定的：\n$$\n\\text{Var}(\\hat{\\beta}) = \\text{Var}((X^{\\top} X)^{-1} X^{\\top} y) = (X^{\\top} X)^{-1} X^{\\top} \\text{Var}(y) (X^{\\top} X^{-1} X^{\\top})^{\\top}\n$$\n根据模型假设 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，响应向量 $y$ 的方差为 $\\text{Var}(y) = \\text{Var}(X\\beta + \\varepsilon) = \\text{Var}(\\varepsilon) = \\sigma^2 I_n$。将此代入 $\\text{Var}(\\hat{\\beta})$ 的表达式中：\n$$\n\\text{Var}(\\hat{\\beta}) = (X^{\\top} X)^{-1} X^{\\top} (\\sigma^2 I_n) X ((X^{\\top} X)^{-1})^{\\top}\n$$\n由于 $X^{\\top} X$ 是对称的，其逆矩阵也是对称的。因此，$((X^{\\top} X)^{-1})^{\\top} = (X^{\\top} X)^{-1}$。\n$$\n\\text{Var}(\\hat{\\beta}) = \\sigma^2 (X^{\\top} X)^{-1} X^{\\top} I_n X (X^{\\top} X)^{-1} = \\sigma^2 (X^{\\top} X)^{-1} (X^{\\top} X) (X^{\\top} X)^{-1} = \\sigma^2 (X^{\\top} X)^{-1}\n$$\n单个系数估计量 $\\hat{\\beta}_j$ 的抽样方差是该协方差矩阵的第 $j$ 个对角元素：\n$$\n\\text{Var}(\\hat{\\beta}_j) = \\sigma^2 \\left( (X^{\\top} X)^{-1} \\right)_{jj}\n$$\n这是 VIF 比率中的分子。\n\n对于分母，我们考虑一个假设情况，其中预测变量是正交的但具有相同的“边际尺度”。对于标准化的预测变量，其尺度由列范数定义。如前所述，标准化意味着 $X_j^{\\top}X_j = n$。一个具有相同边际尺度的正交设计矩阵，我们记为 $X_{ortho}$，将满足 $(X_{ortho, j})^{\\top} (X_{ortho, k}) = 0$（对于 $j \\neq k$）和 $(X_{ortho, j})^{\\top} (X_{ortho, j}) = n$。这导致一个对角格拉姆矩阵：\n$$\nX_{ortho}^{\\top} X_{ortho} = \\text{diag}(n, n, \\dots, n) = n I_p\n$$\n该格拉姆矩阵的逆矩阵是：\n$$\n(X_{ortho}^{\\top} X_{ortho})^{-1} = (n I_p)^{-1} = \\frac{1}{n} I_p\n$$\n在这种理想化的正交情况下，第 $j$ 个系数估计量（记为 $\\hat{\\beta}_{j, ortho}$）的抽样方差将是：\n$$\n\\text{Var}(\\hat{\\beta}_{j, ortho}) = \\sigma^2 \\left( (X_{ortho}^{\\top} X_{ortho})^{-1} \\right)_{jj} = \\sigma^2 \\left( \\frac{1}{n} I_p \\right)_{jj} = \\frac{\\sigma^2}{n}\n$$\n第 $j$ 个预测变量的方差膨胀因子 VIF$_j$ 定义为这两个方差的比率：\n$$\n\\text{VIF}_j = \\frac{\\text{Var}(\\hat{\\beta}_j)}{\\text{Var}(\\hat{\\beta}_{j, ortho})} = \\frac{\\sigma^2 \\left( (X^{\\top} X)^{-1} \\right)_{jj}}{\\sigma^2 / n}\n$$\n$\\sigma^2$ 项相互抵消，从而得出标准化预测变量所需的关系：\n$$\n\\text{VIF}_j = n \\left( (X^{\\top} X)^{-1} \\right)_{jj}\n$$\n\n**第二部分：最大 VIF 的计算**\n\n我们必须计算给定格拉姆矩阵 $A = X^{\\top} X$ 的逆矩阵的对角元素：\n$$\nA = \\begin{pmatrix}\n120  108  24 \\\\\n108  120  12 \\\\\n24  12  120\n\\end{pmatrix}\n$$\n我们使用矩阵的逆公式 $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$，其中 $\\text{adj}(A)$ 是伴随矩阵（代数余子式矩阵的转置）。$A^{-1}$ 的对角元素由 $(A^{-1})_{jj} = \\frac{C_{jj}}{\\det(A)}$ 给出，其中 $C_{jj}$ 是 $(j,j)$ 位置的代数余子式。\n\n首先，我们计算 $A$ 的行列式：\n$$\n\\det(A) = 120(120 \\cdot 120 - 12 \\cdot 12) - 108(108 \\cdot 120 - 12 \\cdot 24) + 24(108 \\cdot 12 - 120 \\cdot 24)\n$$\n$$\n\\det(A) = 120(14400 - 144) - 108(12960 - 288) + 24(1296 - 2880)\n$$\n$$\n\\det(A) = 120(14256) - 108(12672) + 24(-1584) = 1710720 - 1368576 - 38016 = 304128\n$$\n接下来，我们计算对角代数余子式，$C_{jj} = (-1)^{j+j}M_{jj} = M_{jj}$，其中 $M_{jj}$ 是通过移除第 $j$ 行和第 $j$ 列得到的子矩阵的行列式。\n$$\nC_{11} = \\det \\begin{pmatrix} 120  12 \\\\ 12  120 \\end{pmatrix} = 120^2 - 12^2 = 14400 - 144 = 14256\n$$\n$$\nC_{22} = \\det \\begin{pmatrix} 120  24 \\\\ 24  120 \\end{pmatrix} = 120^2 - 24^2 = 14400 - 576 = 13824\n$$\n$$\nC_{33} = \\det \\begin{pmatrix} 120  108 \\\\ 108  120 \\end{pmatrix} = 120^2 - 108^2 = 14400 - 11664 = 2736\n$$\n现在我们求逆矩阵 $A^{-1}$ 的对角元素：\n$$\n(A^{-1})_{11} = \\frac{C_{11}}{\\det(A)} = \\frac{14256}{304128}\n$$\n$$\n(A^{-1})_{22} = \\frac{C_{22}}{\\det(A)} = \\frac{13824}{304128}\n$$\n$$\n(A^{-1})_{33} = \\frac{C_{33}}{\\det(A)} = \\frac{2736}{304128}\n$$\n使用推导出的公式 $\\text{VIF}_j = n \\left( (X^{\\top} X)^{-1} \\right)_{jj}$，其中 $n=120$：\n$$\n\\text{VIF}_1 = 120 \\times \\frac{14256}{304128} = \\frac{1710720}{304128} = 5.625\n$$\n$$\n\\text{VIF}_2 = 120 \\times \\frac{13824}{304128} = \\frac{1658880}{304128} \\approx 5.4545...\n$$\n$$\n\\text{VIF}_3 = 120 \\times \\frac{2736}{304128} = \\frac{328320}{304128} \\approx 1.0795...\n$$\n比较这三个值：\n$\\text{VIF}_1 = 5.625$\n$\\text{VIF}_2 \\approx 5.4545$\n$\\text{VIF}_3 \\approx 1.0795$\n最大的 VIF 是 $\\text{VIF}_1 = 5.625$。问题要求将答案四舍五入到四位有效数字。数字 $5.625$ 已经有四位有效数字。",
            "answer": "$$\n\\boxed{5.625}\n$$"
        }
    ]
}