{
    "hands_on_practices": [
        {
            "introduction": "A powerful application of vectors is representing complex states, such as the gene expression profile of a cell. This exercise provides a tangible way to understand vectors as points in a multi-dimensional space. By calculating the Euclidean distance between a 'healthy' and 'diseased' state vector, you will practice a fundamental vector operation and build intuition for how we can quantitatively measure the magnitude of change in complex biological systems. ",
            "id": "1477133",
            "problem": "In systems biology, the state of a biological cell can be modeled as a vector in a high-dimensional \"state space,\" where each component of the vector represents the expression level of a particular gene. A change in the cell's condition, such as the onset of a disease, corresponds to a displacement of this state vector.\n\nA researcher is investigating a specific cellular pathway involving four key genes: Gene Alpha, Gene Beta, Gene Gamma, and Gene Delta. The expression levels of these genes are used to define a 4-dimensional state vector.\n\nFor a healthy cell, the normalized expression levels are measured, forming the \"healthy state vector,\" $V_H$. The components of this vector, corresponding to (Alpha, Beta, Gamma, Delta), are given by:\n$$ V_H = (7.5, 11.2, 5.1, 14.3) $$\n\nFor a cell in a diseased state, the expression levels for the same set of genes are measured, forming the \"diseased state vector,\" $V_D$:\n$$ V_D = (9.1, 6.8, 5.9, 10.4) $$\n\nThe expression levels are provided in arbitrary units (a.u.). To quantify the total change in the cellular state, calculate the magnitude of the displacement between the healthy and diseased states in this 4-dimensional gene expression space.\n\nExpress your final answer in arbitrary units, rounded to three significant figures.",
            "solution": "The displacement between the healthy and diseased cellular states in a 4-dimensional gene expression space is the Euclidean distance between the vectors $V_{H}$ and $V_{D}$. Denote the displacement vector by $\\Delta V$, defined as $\\Delta V = V_{D} - V_{H}$. The magnitude is given by\n$$\n\\|\\Delta V\\| = \\sqrt{\\sum_{i=1}^{4} (V_{D,i} - V_{H,i})^{2}}.\n$$\nCompute the component-wise differences:\n$$\n\\Delta V = (9.1 - 7.5,\\; 6.8 - 11.2,\\; 5.9 - 5.1,\\; 10.4 - 14.3) = (1.6,\\; -4.4,\\; 0.8,\\; -3.9).\n$$\nSquare each component and sum:\n$$\n1.6^{2} = 2.56,\\quad (-4.4)^{2} = 19.36,\\quad 0.8^{2} = 0.64,\\quad (-3.9)^{2} = 15.21,\n$$\n$$\n2.56 + 19.36 + 0.64 + 15.21 = 37.77.\n$$\nTake the square root to obtain the magnitude:\n$$\n\\|\\Delta V\\| = \\sqrt{37.77} \\approx 6.14573.\n$$\nRounded to three significant figures (in arbitrary units), this is $6.15$.",
            "answer": "$$\\boxed{6.15}$$"
        },
        {
            "introduction": "Beyond representing static states, vectors and matrices are essential for modeling dynamic systems. In this problem, a matrix acts as a transformation, converting a vector of reaction rates (fluxes) into a vector of concentration changes. This exercise demonstrates the power of matrix-vector multiplication to elegantly capture the interconnectedness of a complex network, providing a snapshot of the system's dynamics in a single calculation. ",
            "id": "1477176",
            "problem": "In systems biology, the dynamic changes in the concentrations of intracellular metabolites can be modeled using a linear system based on reaction stoichiometry. Consider a simplified metabolic network within a hypothetical microorganism that produces a valuable compound, C. The network involves three key internal metabolites: A, B, and C. Their interactions are described by five reactions, each with a specific rate or flux.\n\nThe stoichiometry of the system is captured by the stoichiometric matrix $S$. The rows of $S$ correspond to the metabolites (in the order A, B, C), and the columns correspond to the five reaction fluxes (in the order $v_1, v_2, v_3, v_4, v_5$). The stoichiometric matrix $S$ is given by:\n$$\nS = \\begin{pmatrix}\n1 & 0 & -1 & 0 & -1 \\\\\n0 & 1 & -2 & 0 & 0 \\\\\n0 & 0 & 1 & -1 & 0\n\\end{pmatrix}\n$$\nHere, a positive entry indicates production of a metabolite, a negative entry indicates consumption, and zero indicates no involvement in the reaction.\n\nAt a specific moment, the organism's metabolic state is characterized by a flux vector $\\mathbf{v}$, where each component represents the rate of the corresponding reaction. The measured fluxes are given by the vector:\n$$\n\\mathbf{v} = \\begin{pmatrix} 10 \\\\ 12 \\\\ 4 \\\\ 3 \\\\ 2 \\end{pmatrix}\n$$\nThe units for all fluxes are in millimoles per gram dry weight per hour (mmol gDW$^{-1}$ h$^{-1}$).\n\nCalculate the vector representing the net rates of change for the concentrations of metabolites A, B, and C, respectively. Express your answer as a single row matrix, with values given in units of mmol gDW$^{-1}$ h$^{-1}$.",
            "solution": "The fundamental equation that relates the rate of change of metabolite concentrations to the reaction fluxes is given by the matrix-vector product:\n$$\n\\frac{d\\mathbf{c}}{dt} = S\\mathbf{v}\n$$\nwhere $\\frac{d\\mathbf{c}}{dt}$ is the vector of the rates of change of metabolite concentrations, $S$ is the stoichiometric matrix, and $\\mathbf{v}$ is the flux vector.\n\nThe vector of concentration changes, which we can denote as $\\mathbf{r} = \\begin{pmatrix} r_A & r_B & r_C \\end{pmatrix}^T$, is calculated by multiplying the given matrix $S$ by the vector $\\mathbf{v}$.\n\nThe given matrices are:\n$$\nS = \\begin{pmatrix}\n1 & 0 & -1 & 0 & -1 \\\\\n0 & 1 & -2 & 0 & 0 \\\\\n0 & 0 & 1 & -1 & 0\n\\end{pmatrix}\n\\quad \\text{and} \\quad\n\\mathbf{v} = \\begin{pmatrix} 10 \\\\ 12 \\\\ 4 \\\\ 3 \\\\ 2 \\end{pmatrix}\n$$\n\nWe perform the matrix-vector multiplication:\n$$\n\\begin{pmatrix} r_A \\\\ r_B \\\\ r_C \\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & -1 & 0 & -1 \\\\\n0 & 1 & -2 & 0 & 0 \\\\\n0 & 0 & 1 & -1 & 0\n\\end{pmatrix}\n\\begin{pmatrix} 10 \\\\ 12 \\\\ 4 \\\\ 3 \\\\ 2 \\end{pmatrix}\n$$\n\nLet's calculate each component of the resulting vector.\n\nFor the first component, the rate of change of metabolite A ($r_A$):\n$$\nr_A = (1 \\times 10) + (0 \\times 12) + (-1 \\times 4) + (0 \\times 3) + (-1 \\times 2)\n$$\n$$\nr_A = 10 + 0 - 4 + 0 - 2 = 4\n$$\n\nFor the second component, the rate of change of metabolite B ($r_B$):\n$$\nr_B = (0 \\times 10) + (1 \\times 12) + (-2 \\times 4) + (0 \\times 3) + (0 \\times 2)\n$$\n$$\nr_B = 0 + 12 - 8 + 0 + 0 = 4\n$$\n\nFor the third component, the rate of change of metabolite C ($r_C$):\n$$\nr_C = (0 \\times 10) + (0 \\times 12) + (1 \\times 4) + (-1 \\times 3) + (0 \\times 2)\n$$\n$$\nr_C = 0 + 0 + 4 - 3 + 0 = 1\n$$\n\nSo, the vector of net rates of change for the concentrations of metabolites A, B, and C is $\\begin{pmatrix} 4 \\\\ 4 \\\\ 1 \\end{pmatrix}$. The units for these rates are the same as the flux units, which are mmol gDW$^{-1}$ h$^{-1}$.\n\nThe problem asks for the answer to be presented as a single row matrix.\n$$\n\\begin{pmatrix} 4 & 4 & 1 \\end{pmatrix}\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix} 4 & 4 & 1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "This advanced practice integrates vector and matrix concepts into a complete computational workflow, a common task in modern statistical learning. You will implement and compare two 'whitening' transformations, PCA and ZCA, which are critical data preprocessing steps that rely on the eigendecomposition of the covariance matrix. This hands-on coding exercise bridges theory and practice, revealing how abstract linear algebra operations directly impact the performance of a machine learning model. ",
            "id": "3192878",
            "problem": "You are given a dataset-based comparison task in the domain of statistical learning focused on vectors and matrices. The goal is to implement and contrast Principal Component Analysis (PCA) whitening and Zero-phase Component Analysis (ZCA) whitening, and then to quantify the differences in downstream linear classification margins produced by ridge regression. All algorithmic definitions must be derived from core linear algebraic facts and standard statistical learning formulations. Your program must be a complete, runnable Python program that performs the following steps and produces results that are objectively testable.\n\nLet the data matrix be $X \\in \\mathbb{R}^{n \\times d}$, with rows as samples and columns as features. Define the centered data matrix $X_c = X - \\mathbf{1}\\mu^\\top$, where $\\mu \\in \\mathbb{R}^d$ is the sample mean vector and $\\mathbf{1} \\in \\mathbb{R}^{n}$ is the vector of ones. Define the empirical covariance matrix by the well-tested fact\n$$\n\\Sigma = \\frac{1}{n} X_c^\\top X_c,\n$$\nwhich is symmetric and positive semidefinite. The eigendecomposition of the covariance is given by\n$$\n\\Sigma = U \\Lambda U^\\top,\n$$\nwhere $U \\in \\mathbb{R}^{d \\times d}$ is orthonormal and $\\Lambda \\in \\mathbb{R}^{d \\times d}$ is diagonal with nonnegative entries. To stabilize whitening in the presence of small or zero eigenvalues, introduce a small regularization parameter $\\delta > 0$ and define\n$$\n\\Lambda_\\delta = \\Lambda + \\delta I_d, \\quad \\Lambda_\\delta^{-1/2} = \\operatorname{diag}\\left(\\frac{1}{\\sqrt{\\lambda_i + \\delta}}\\right).\n$$\nDefine PCA whitening and ZCA whitening transformations on $X_c$ by\n$$\nX_{\\text{PCA}} = X_c U \\Lambda_\\delta^{-1/2}, \\quad X_{\\text{ZCA}} = X_c U \\Lambda_\\delta^{-1/2} U^\\top.\n$$\n\nFor linear classification, consider ridge regression (RR), a standard linear model that fits a weight vector $w \\in \\mathbb{R}^d$ and an intercept $b \\in \\mathbb{R}$ by minimizing the penalized squared loss. Let the labels be $y \\in \\{-1, +1\\}^n$ and let the design matrix be $Z = [X \\ \\mathbf{1}] \\in \\mathbb{R}^{n \\times (d+1)}$. The RR objective is\n$$\n\\min_{w, b} \\ \\|y - X w - b \\mathbf{1}\\|_2^2 + \\lambda \\|w\\|_2^2,\n$$\nwhere $\\lambda > 0$ is the regularization strength and the intercept $b$ is not penalized. Using well-known normal-equation derivations, the unique minimizer is obtained by setting the gradient to zero, yielding\n$$\n\\theta^\\star = \\begin{bmatrix} w^\\star \\\\ b^\\star \\end{bmatrix} = \\left(Z^\\top Z + R\\right)^{-1} Z^\\top y,\n$$\nwhere $R = \\operatorname{diag}(\\lambda, \\ldots, \\lambda, 0) \\in \\mathbb{R}^{(d+1) \\times (d+1)}$ places the penalty on the feature weights but not on the intercept. For each sample $i$, the geometric margin is defined by\n$$\nm_i = \\frac{y_i \\left(w^{\\star\\top} x_i + b^\\star \\right)}{\\|w^\\star\\|_2},\n$$\nand we summarize classifier performance by the average margin $\\bar{m} = \\frac{1}{n} \\sum_{i=1}^{n} m_i$ and the minimum margin $m_{\\min} = \\min_i m_i$.\n\nYour task is to implement the above definitions to:\n- Center the data,\n- Compute the covariance and its eigendecomposition,\n- Apply PCA whitening and ZCA whitening with eigenvalue regularization $\\delta$,\n- Train a ridge regression classifier on each whitened dataset and compute the geometric margins,\n- Quantify the differences between ZCA and PCA whitening by reporting\n$$\n\\Delta \\bar{m} = \\bar{m}_{\\text{ZCA}} - \\bar{m}_{\\text{PCA}}, \\quad \\Delta m_{\\min} = m_{\\min,\\text{ZCA}} - m_{\\min,\\text{PCA}}.\n$$\n\nTest Suite. Implement and evaluate the following three test cases. Angles, where specified, must be interpreted in degrees in the problem specification; ensure correct conversion to radians in your code.\n\n- Test case $1$ (general anisotropic, rotated covariance; happy path):\n  - Dimensionality $d = 2$; samples per class $n_+ = 80$, $n_- = 80$ so $n = 160$.\n  - Class means: $\\mu_+ = [2, 0]^\\top$, $\\mu_- = [-2, 0]^\\top$.\n  - Base covariance: $\\operatorname{diag}(3, 0.2)$.\n  - Rotation angle: $30^\\circ$. Let the rotation matrix be\n    $$\n    R(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}\n    $$\n    with $\\theta = 30^\\circ$ converted to radians, and set $\\Sigma_{\\text{rot}} = R(\\theta) \\operatorname{diag}(3, 0.2) R(\\theta)^\\top$ for both classes.\n  - Whitening regularization $\\delta = 10^{-5}$, ridge strength $\\lambda = 10^{-1}$.\n\n- Test case $2$ (near-singular covariance; boundary condition):\n  - Dimensionality $d = 2$; samples per class $n_+ = 100$, $n_- = 100$ so $n = 200$.\n  - Class means: $\\mu_+ = [0.5, -0.5]^\\top$, $\\mu_- = [-0.5, 0.5]^\\top$.\n  - Base covariance: $\\operatorname{diag}(10, 10^{-6})$.\n  - Rotation angle: $75^\\circ$. Use $R(\\theta)$ with $\\theta = 75^\\circ$ converted to radians; set $\\Sigma_{\\text{rot}} = R(\\theta) \\operatorname{diag}(10, 10^{-6}) R(\\theta)^\\top$.\n  - Whitening regularization $\\delta = 10^{-3}$, ridge strength $\\lambda = 10^{-1}$.\n\n- Test case $3$ (already whitened data; significant edge case where PCA and ZCA are expected to align in effect):\n  - Dimensionality $d = 2$; total samples $n = 400$.\n  - Data sampled from a standard normal with identity covariance: $x_i \\sim \\mathcal{N}(0, I_2)$, labels defined deterministically by $y_i = \\operatorname{sign}(x_{i,1})$ with tie-breaking $y_i = +1$ when $x_{i,1} = 0$.\n  - Whitening regularization $\\delta = 10^{-5}$, ridge strength $\\lambda = 10^{-1}$.\n\nImplementation specifics:\n- Use a fixed random number generator seed to ensure reproducibility across all test cases.\n- For each test case, form the combined dataset by stacking the two classes (where applicable), center as described, compute whitening transforms with the specified $\\delta$, solve for $(w^\\star, b^\\star)$ using the closed-form normal equations, and compute $\\bar{m}$ and $m_{\\min}$ for both PCA-whitened and ZCA-whitened data.\n- The final outputs must be expressed in a single line containing a comma-separated list enclosed in square brackets. Each element corresponds to one test case in order and must be the two-element list $[\\Delta \\bar{m}, \\Delta m_{\\min}]$ with both entries as floating-point numbers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[[a,b],[c,d],[e,f]]\"). No physical units, angles, or percentages need to be printed by the program; angles specified in the test suite are solely for constructing rotation matrices and must be converted to radians internally.",
            "solution": "The problem statement is critically validated and found to be valid. It is scientifically grounded in established principles of linear algebra and statistical learning, well-posed with all necessary parameters and definitions provided, and objective in its formulation. The task is a computational exercise to implement and compare two whitening techniques and their effect on a downstream classification task, based on standard and non-contradictory formulas.\n\nThe solution proceeds by first implementing a data generation scheme for the three specified test cases. Then, a common pipeline is applied to process the generated data, including centering, whitening, training a classifier, and calculating performance metrics.\n\n**1. Data Generation and Problem Setup**\n\nA fixed random seed is used to ensure the reproducibility of the results. For each test case, a data matrix $X \\in \\mathbb{R}^{n \\times d}$ and a corresponding label vector $y \\in \\{-1, +1\\}^n$ are generated according to the specifications.\n\nFor Test Cases $1$ and $2$, the data is drawn from a mixture of two Gaussian distributions.\n- Let the parameters be the dimensionality $d$, the number of samples per class $n_+$ and $n_-$, the class means $\\mu_+$ and $\\mu_-$, a base diagonal covariance matrix $\\Sigma_{\\text{base}}$, and a rotation angle $\\theta$. The total number of samples is $n = n_+ + n_-$.\n- The rotation matrix is constructed as $R(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}$, where $\\theta$ is converted from degrees to radians.\n- The shared covariance matrix for both classes is $\\Sigma_{\\text{rot}} = R(\\theta) \\Sigma_{\\text{base}} R(\\theta)^\\top$.\n- $n_+$ samples are drawn from the distribution $\\mathcal{N}(\\mu_+, \\Sigma_{\\text{rot}})$ and assigned the label $+1$.\n- $n_-$ samples are drawn from the distribution $\\mathcal{N}(\\mu_-, \\Sigma_{\\text{rot}})$ and assigned the label $-1$.\n- The final data matrix $X$ is formed by stacking these samples, and the label vector $y$ is formed by concatenating the labels.\n\nFor Test Case $3$, the data is drawn from a standard normal distribution.\n- $n$ samples $x_i$ are drawn from $\\mathcal{N}(0, I_d)$, where $d$ is the dimensionality and $I_d$ is the $d \\times d$ identity matrix.\n- The labels are determined by the sign of the first feature: $y_i = \\operatorname{sign}(x_{i,1})$. As specified, the case $x_{i,1} = 0$ is handled by setting $y_i = +1$.\n\n**2. Data Centering and Covariance Estimation**\n\nGiven a data matrix $X \\in \\mathbb{R}^{n \\times d}$, the first step in the processing pipeline is to center the data.\n- The sample mean vector $\\mu \\in \\mathbb{R}^d$ is computed as $\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i$, where $x_i$ are the row vectors of $X$.\n- The centered data matrix $X_c$ is then calculated as $X_c = X - \\mathbf{1}\\mu^\\top$, where $\\mathbf{1} \\in \\mathbb{R}^n$ is a column vector of ones.\n- The empirical covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$ is computed from the centered data using the formula $\\Sigma = \\frac{1}{n} X_c^\\top X_c$.\n\n**3. Eigendecomposition and Whitening Transformations**\n\nWhitening requires the eigendecomposition of the covariance matrix.\n- Since $\\Sigma$ is a symmetric positive semidefinite matrix, its eigendecomposition is given by $\\Sigma = U \\Lambda U^\\top$, where $U$ is an orthonormal matrix of eigenvectors and $\\Lambda$ is a diagonal matrix of corresponding real, non-negative eigenvalues $\\lambda_i$.\n- To ensure numerical stability, especially when some eigenvalues are close to zero, a regularization parameter $\\delta > 0$ is introduced. The regularized eigenvalue matrix is $\\Lambda_\\delta = \\Lambda + \\delta I_d$.\n- The inverse square root of this matrix is computed as $\\Lambda_\\delta^{-1/2} = \\operatorname{diag}\\left(\\frac{1}{\\sqrt{\\lambda_i + \\delta}}\\right)$.\n\nUsing these components, the two whitening transformations are applied to the centered data $X_c$:\n- **PCA Whitening**: The data is projected onto the principal axes (the columns of $U$) and scaled. The transformed data is $X_{\\text{PCA}} = X_c U \\Lambda_\\delta^{-1/2}$. The covariance of this new data is approximately the identity matrix.\n- **ZCA Whitening**: This transformation rotates the PCA-whitened data back to be as close as possible to the original data. The transformed data is $X_{\\text{ZCA}} = X_c U \\Lambda_\\delta^{-1/2} U^\\top$.\n\n**4. Ridge Regression and Margin Calculation**\n\nFor both the PCA-whitened data ($X_{\\text{PCA}}$) and the ZCA-whitened data ($X_{\\text{ZCA}}$), a ridge regression classifier is trained. Let the whitened data be denoted generically by $X_w \\in \\mathbb{R}^{n \\times d}$.\n- The ridge regression model finds a weight vector $w \\in \\mathbb{R}^d$ and an intercept $b \\in \\mathbb{R}$ that minimize the objective function $\\|y - X_w w - b \\mathbf{1}\\|_2^2 + \\lambda \\|w\\|_2^2$, where $\\lambda > 0$ is the regularization strength.\n- To solve this, we form an augmented design matrix $Z = [X_w \\ \\mathbf{1}] \\in \\mathbb{R}^{n \\times (d+1)}$ and a corresponding parameter vector $\\theta = [w^\\top \\ b]^\\top$.\n- The penalty is applied only to the weights $w$, not the intercept $b$. This is codified in the penalty matrix $R = \\operatorname{diag}(\\lambda, \\ldots, \\lambda, 0) \\in \\mathbb{R}^{(d+1) \\times (d+1)}$.\n- The optimal parameter vector $\\theta^\\star$ is found via the normal equations: $\\theta^\\star = (Z^\\top Z + R)^{-1} Z^\\top y$. This linear system is solved numerically for $\\theta^\\star = [w^{\\star\\top} \\ b^\\star]^\\top$.\n\nOnce the optimal classifier $(w^\\star, b^\\star)$ is found, the geometric margin for each sample $i$ is calculated. The margin measures the signed, normalized distance of a sample from the decision hyperplane $w^{\\star\\top}x + b^\\star = 0$.\n- For each sample $x_i$ from the whitened dataset $X_w$, the margin is $m_i = \\frac{y_i (w^{\\star\\top} x_i + b^\\star)}{\\|w^\\star\\|_2}$.\n- Two summary statistics are computed from the margins: the average margin $\\bar{m} = \\frac{1}{n} \\sum_{i=1}^n m_i$ and the minimum margin $m_{\\min} = \\min_i m_i$.\n\n**5. Final Comparison**\n\nThis entire process yields an average and minimum margin for both PCA-whitened data ($\\bar{m}_{\\text{PCA}}, m_{\\min,\\text{PCA}}$) and ZCA-whitened data ($\\bar{m}_{\\text{ZCA}}, m_{\\min,\\text{ZCA}}$). The final step is to compute the differences in these metrics, as requested:\n- $\\Delta \\bar{m} = \\bar{m}_{\\text{ZCA}} - \\bar{m}_{\\text{PCA}}$\n- $\\Delta m_{\\min} = m_{\\min,\\text{ZCA}} - m_{\\min,\\text{PCA}}$\n\nThese two values are reported for each test case, providing a quantitative comparison of the impact of the two whitening methods on the geometric margin of a subsequent linear classifier.",
            "answer": "```python\nimport numpy as np\n# No other libraries are permitted by the problem statement.\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    def run_case(params):\n        \"\"\"\n        Executes a single test case, from data generation to final metric calculation.\n        \"\"\"\n        # Step 1: Data Generation\n        case_type = params[\"type\"]\n        d = params[\"d\"]\n        lambda_val = params[\"lambda\"]\n        delta = params[\"delta\"]\n\n        if case_type == \"gaussian_mixture\":\n            n_pos = params[\"n_pos\"]\n            n_neg = params[\"n_neg\"]\n            n = n_pos + n_neg\n            mu_pos = np.array(params[\"mu_pos\"])\n            mu_neg = np.array(params[\"mu_neg\"])\n            sigma_base = np.diag(params[\"sigma_base_diag\"])\n            theta_deg = params[\"theta_deg\"]\n            \n            theta_rad = np.deg2rad(theta_deg)\n            c, s = np.cos(theta_rad), np.sin(theta_rad)\n            rot_matrix = np.array([[c, -s], [s, c]])\n            sigma_rot = rot_matrix @ sigma_base @ rot_matrix.T\n\n            X_pos = np.random.multivariate_normal(mu_pos, sigma_rot, n_pos)\n            X_neg = np.random.multivariate_normal(mu_neg, sigma_rot, n_neg)\n\n            X = np.vstack([X_pos, X_neg])\n            y = np.hstack([np.ones(n_pos), -np.ones(n_neg)])\n\n        elif case_type == \"std_normal\":\n            n = params[\"n\"]\n            X = np.random.multivariate_normal(np.zeros(d), np.identity(d), n)\n            y = np.sign(X[:, 0])\n            y[y == 0] = 1.0 # Handle the tie-breaking case\n        else:\n            raise ValueError(\"Unknown test case type\")\n\n        # Step 2: Centering and Covariance\n        mu = np.mean(X, axis=0)\n        X_c = X - mu\n        \n        # Ensure n is integer for division\n        n_samples = X_c.shape[0]\n        Sigma = (X_c.T @ X_c) / n_samples\n        \n        # Step 3: Eigendecomposition and Whitening Transforms\n        eigvals, U = np.linalg.eigh(Sigma)\n        Lambda_delta_inv_sqrt = np.diag(1.0 / np.sqrt(eigvals + delta))\n\n        # PCA whitening\n        X_pca = X_c @ U @ Lambda_delta_inv_sqrt\n        # ZCA whitening\n        X_zca = X_pca @ U.T\n\n        def train_and_get_margins(X_w, y_labels, reg_lambda):\n            \"\"\"\n            Trains a ridge regression classifier and computes geometric margins.\n            \"\"\"\n            n_w, d_w = X_w.shape\n            \n            # Form the design matrix Z = [X_w, 1]\n            Z = np.hstack([X_w, np.ones((n_w, 1))])\n            \n            # Form the regularization matrix R\n            R_diag = np.full(d_w + 1, reg_lambda)\n            R_diag[-1] = 0.0 # No penalty on the intercept\n            R = np.diag(R_diag)\n            \n            # Solve the normal equations: (Z^T Z + R) theta = Z^T y\n            A = Z.T @ Z + R\n            b_vec = Z.T @ y_labels\n            theta_star = np.linalg.solve(A, b_vec)\n            \n            w_star = theta_star[:-1]\n            b_star = theta_star[-1]\n            \n            w_norm = np.linalg.norm(w_star)\n\n            # Avoid division by zero if w_star is all zeros\n            if w_norm == 0:\n                margins = np.zeros(n_w)\n            else:\n                # Calculate geometric margins\n                margins = (y_labels * (X_w @ w_star + b_star)) / w_norm\n            \n            m_bar = np.mean(margins)\n            m_min = np.min(margins)\n            \n            return m_bar, m_min\n\n        # Step 4: Train on each whitened set and get margins\n        m_bar_pca, m_min_pca = train_and_get_margins(X_pca, y, lambda_val)\n        m_bar_zca, m_min_zca = train_and_get_margins(X_zca, y, lambda_val)\n\n        # Step 5: Compute differences\n        delta_m_bar = m_bar_zca - m_bar_pca\n        delta_m_min = m_min_zca - m_min_pca\n        \n        return [delta_m_bar, delta_m_min]\n\n    test_cases_params = [\n        # Test case 1\n        {\n            \"type\": \"gaussian_mixture\", \"d\": 2, \"n_pos\": 80, \"n_neg\": 80,\n            \"mu_pos\": [2, 0], \"mu_neg\": [-2, 0], \"sigma_base_diag\": [3, 0.2],\n            \"theta_deg\": 30, \"delta\": 1e-5, \"lambda\": 1e-1\n        },\n        # Test case 2\n        {\n            \"type\": \"gaussian_mixture\", \"d\": 2, \"n_pos\": 100, \"n_neg\": 100,\n            \"mu_pos\": [0.5, -0.5], \"mu_neg\": [-0.5, 0.5], \"sigma_base_diag\": [10, 1e-6],\n            \"theta_deg\": 75, \"delta\": 1e-3, \"lambda\": 1e-1\n        },\n        # Test case 3\n        {\n            \"type\": \"std_normal\", \"d\": 2, \"n\": 400, \"delta\": 1e-5, \"lambda\": 1e-1\n        }\n    ]\n\n    results = [run_case(params) for params in test_cases_params]\n\n    # Format output as specified: [[a,b],[c,d],[e,f]]\n    inner_strings = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}