{
    "hands_on_practices": [
        {
            "introduction": "This exercise will ground your understanding of logistic regression in its probabilistic foundation, the Bernoulli distribution. You will derive the gradient for L2-regularized logistic regression from first principles, revealing the direct link between the penalty term and the concept of weight decay. By implementing gradient descent, you will also gain practical insight into how the learning rate, $\\eta$, affects optimization and model training dynamics. ",
            "id": "3116203",
            "problem": "You are given a binary classification setting modeled with a Bernoulli likelihood and a logistic link. The fundamental base for this problem consists of the following well-tested facts and core definitions: the Bernoulli probability mass function $p(y \\mid \\pi) = \\pi^{y} (1 - \\pi)^{1 - y}$ for $y \\in \\{0,1\\}$, independence of observations implying the joint likelihood is the product of individual likelihoods, and the logistic function $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$ that maps real-valued scores $z$ to probabilities in $(0,1)$. The regularization uses a penalty on the squared Euclidean two-norm (L2) of the weights, expressed as $\\dfrac{\\lambda}{2} \\lVert w \\rVert^{2}$ for a nonnegative coefficient $\\lambda$.\n\nTask A (derivation from first principles): Consider $n$ independent observations $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ with $x_{i} \\in \\mathbb{R}^{d}$ and $y_{i} \\in \\{0,1\\}$. Assume the Bernoulli parameter for $y_{i}$ is $\\pi_{i} = \\sigma(z_{i})$, where $z_{i} = w^{\\top} x_{i}$ and $w \\in \\mathbb{R}^{d}$ is the weight vector. Starting from the Bernoulli probability mass function and the logistic function definition, derive the gradient with respect to $w$ of the regularized negative log-likelihood\n$$\nJ(w) = \\sum_{i=1}^{n} \\left[ - y_{i} \\log \\sigma(z_{i}) - (1 - y_{i}) \\log \\left(1 - \\sigma(z_{i})\\right) \\right] + \\frac{\\lambda}{2} \\lVert w \\rVert^{2}.\n$$\nThen, show how this gradient induces a multiplicative shrinkage on $w$ under gradient descent, clarifying the relation to weight decay.\n\nTask B (algorithmic analysis): Implement gradient descent using the derived gradient to study the training dynamics under different learning rates. Use the following fixed, scientifically sound test suite to ensure coverage of typical and edge cases:\n- Data dimension $d = 2$ and sample count $n = 5$.\n- Feature matrix $X \\in \\mathbb{R}^{n \\times d}$ and labels $y \\in \\{0,1\\}^{n}$ given explicitly by\n$$\nX = \\begin{bmatrix}\n1.0 & 0.0 \\\\\n1.0 & 1.0 \\\\\n2.0 & -1.0 \\\\\n-1.0 & 2.0 \\\\\n0.0 & -2.0\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n0 \\\\\n0\n\\end{bmatrix}.\n$$\n- Regularization coefficient $\\lambda = 0.1$.\n- Initialization $w^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ (i.e., all weights initialized to zero).\n- Number of gradient descent iterations $T = 50$.\n- Learning rates to test: $\\eta \\in \\{0.0, 0.05, 0.2, 1.5\\}$.\n\nFor numerical robustness, evaluate the negative log-likelihood using the identity with the softplus function to avoid overflow:\n$$\n\\text{softplus}(z) = \\log(1 + e^{z}) = \\max(0,z) + \\log\\!\\left(1 + e^{-\\lvert z \\rvert}\\right),\n$$\nand use a numerically stable implementation of the logistic function based on case analysis on the sign of $z$. For each learning rate $\\eta$ in the test suite, run the gradient descent for $T$ iterations and compute the final regularized objective value $J\\!\\left(w^{(T)}\\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the four final objective values $J\\!\\left(w^{(T)}\\right)$ corresponding to the four learning rates in the order $\\eta = 0.0$, $\\eta = 0.05$, $\\eta = 0.2$, $\\eta = 1.5$, each printed as a decimal rounded to six places, for example, $\\left[\\text{val}_{1},\\text{val}_{2},\\text{val}_{3},\\text{val}_{4}\\right]$. No physical units apply, and no percentages are used; all values are real numbers in decimal form.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, self-contained, well-posed, and objective. It presents a standard task in statistical learning—deriving and implementing gradient descent for L2-regularized logistic regression—with all necessary data and parameters clearly specified.\n\n### Task A: Derivation and Analysis\n\nThe objective is to derive the gradient of the regularized negative log-likelihood function and analyze its effect within a gradient descent update rule.\n\nThe regularized negative log-likelihood function is given as:\n$$\nJ(w) = \\sum_{i=1}^{n} \\left[ - y_{i} \\log \\sigma(z_{i}) - (1 - y_{i}) \\log \\left(1 - \\sigma(z_{i})\\right) \\right] + \\frac{\\lambda}{2} \\lVert w \\rVert^{2}\n$$\nwhere $z_i = w^\\top x_i$, $w \\in \\mathbb{R}^d$, $x_i \\in \\mathbb{R}^d$, and $y_i \\in \\{0, 1\\}$. The logistic function is $\\sigma(z) = (1 + e^{-z})^{-1}$.\n\nThe gradient $\\nabla_w J(w)$ can be computed by considering the loss term and the regularization term separately. Let $L(w) = \\sum_{i=1}^{n} L_i(w)$ be the loss (negative log-likelihood) and $R(w) = \\frac{\\lambda}{2} \\lVert w \\rVert^2$ be the regularization term. Then $J(w) = L(w) + R(w)$.\n\nFirst, we find the gradient of the regularization term. Since $\\lVert w \\rVert^2 = w^\\top w$, we have:\n$$\n\\nabla_w R(w) = \\nabla_w \\left(\\frac{\\lambda}{2} w^\\top w\\right) = \\frac{\\lambda}{2} (2 w) = \\lambda w\n$$\nThis is a standard result from vector calculus.\n\nNext, we find the gradient of the loss term for a single observation $i$, $L_i(w) = - y_{i} \\log \\sigma(z_{i}) - (1 - y_{i}) \\log(1 - \\sigma(z_{i}))$. We use the chain rule: $\\nabla_w L_i(w) = \\frac{\\partial L_i}{\\partial z_i} \\nabla_w z_i$.\n\nThe gradient of $z_i = w^\\top x_i$ with respect to $w$ is simply:\n$$\n\\nabla_w z_i = x_i\n$$\nTo find $\\frac{\\partial L_i}{\\partial z_i}$, we first need the derivative of the logistic function, $\\frac{d\\sigma(z)}{dz}$.\n$$\n\\frac{d\\sigma(z)}{dz} = \\frac{d}{dz} (1 + e^{-z})^{-1} = -1(1 + e^{-z})^{-2} (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2} = \\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}}\n$$\nRecognizing that $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ and $1 - \\sigma(z) = \\frac{e^{-z}}{1 + e^{-z}}$, we arrive at the identity:\n$$\n\\frac{d\\sigma(z)}{dz} = \\sigma(z) (1 - \\sigma(z))\n$$\nNow we can compute $\\frac{\\partial L_i}{\\partial z_i}$ using the chain rule again, $\\frac{\\partial L_i}{\\partial z_i} = \\frac{\\partial L_i}{\\partial \\sigma(z_i)} \\frac{d\\sigma(z_i)}{dz_i}$.\n$$\n\\frac{\\partial L_i}{\\partial \\sigma(z_i)} = -\\frac{y_i}{\\sigma(z_i)} + \\frac{1-y_i}{1-\\sigma(z_i)} = \\frac{-y_i(1-\\sigma(z_i)) + (1-y_i)\\sigma(z_i)}{\\sigma(z_i)(1-\\sigma(z_i))} = \\frac{\\sigma(z_i) - y_i}{\\sigma(z_i)(1-\\sigma(z_i))}\n$$\nCombining these results:\n$$\n\\frac{\\partial L_i}{\\partial z_i} = \\frac{\\partial L_i}{\\partial \\sigma(z_i)} \\frac{d\\sigma(z_i)}{dz_i} = \\left( \\frac{\\sigma(z_i) - y_i}{\\sigma(z_i)(1-\\sigma(z_i))} \\right) \\cdot \\left( \\sigma(z_i)(1-\\sigma(z_i)) \\right) = \\sigma(z_i) - y_i\n$$\nThis elegant result shows that the derivative of the cross-entropy loss with respect to the pre-activation score $z_i$ is the difference between the predicted probability and the true label.\n\nThe gradient of $L_i(w)$ is therefore:\n$$\n\\nabla_w L_i(w) = \\frac{\\partial L_i}{\\partial z_i} \\nabla_w z_i = (\\sigma(w^\\top x_i) - y_i) x_i\n$$\nThe gradient of the total loss $L(w)$ is the sum over all observations:\n$$\n\\nabla_w L(w) = \\sum_{i=1}^{n} (\\sigma(w^\\top x_i) - y_i) x_i\n$$\nCombining the gradients of the loss and regularization terms, we obtain the full gradient of the objective function $J(w)$:\n$$\n\\nabla_w J(w) = \\sum_{i=1}^{n} (\\sigma(w^\\top x_i) - y_i) x_i + \\lambda w\n$$\nIn matrix notation, where $X \\in \\mathbb{R}^{n \\times d}$ is the design matrix with rows $x_i^\\top$, $y \\in \\{0,1\\}^n$ is the vector of labels, and $\\sigma$ is applied element-wise, this is:\n$$\n\\nabla_w J(w) = X^\\top (\\sigma(Xw) - y) + \\lambda w\n$$\n\nNow, we analyze the effect of this gradient in a gradient descent update. The update rule for the weight vector $w$ at iteration $t$ with learning rate $\\eta > 0$ is:\n$$\nw^{(t+1)} \\leftarrow w^{(t)} - \\eta \\nabla_w J(w^{(t)})\n$$\nSubstituting the derived gradient:\n$$\nw^{(t+1)} \\leftarrow w^{(t)} - \\eta \\left( \\sum_{i=1}^{n} (\\sigma(w^{(t)\\top} x_i) - y_i) x_i + \\lambda w^{(t)} \\right)\n$$\nWe can rearrange the terms to isolate the effect of the regularization term on $w^{(t)}$:\n$$\nw^{(t+1)} \\leftarrow w^{(t)} - \\eta \\lambda w^{(t)} - \\eta \\left( \\sum_{i=1}^{n} (\\sigma(w^{(t)\\top} x_i) - y_i) x_i \\right)\n$$\nFactoring out $w^{(t)}$ from the first two terms gives:\n$$\nw^{(t+1)} \\leftarrow (1 - \\eta \\lambda) w^{(t)} - \\eta \\nabla_w L(w^{(t)})\n$$\nThis form explicitly reveals the relationship to weight decay. Before subtracting the gradient of the unregularized loss, the current weight vector $w^{(t)}$ is multiplied by a factor of $(1 - \\eta \\lambda)$. Since $\\eta > 0$ and $\\lambda > 0$, this factor is less than $1$, causing the weights to shrink towards zero at each iteration. This multiplicative shrinkage is precisely what is known as \"weight decay\". Therefore, for standard gradient descent, L2 regularization on the objective function is mathematically equivalent to applying weight decay to the update rule.\n\n### Task B: Algorithmic Implementation\n\nThe analysis now transitions to a concrete numerical implementation using the derived gradient. The task is to run gradient descent for the specified dataset and parameters and report the final objective value $J(w^{(T)})$ for different learning rates.\n\nFor numerical stability, the objective function $J(w)$ will be computed using the identity for binary cross-entropy loss:\n$$\nL_i(w) = \\log(1 + e^{z_i}) - y_i z_i = \\text{softplus}(z_i) - y_i z_i\n$$\nwhere $z_i = w^\\top x_i$. The `softplus` function is evaluated stably as $\\text{softplus}(z) = \\max(0,z) + \\log(1 + e^{-\\lvert z \\rvert})$.\nThe total objective function is thus:\n$$\nJ(w) = \\sum_{i=1}^{n} (\\text{softplus}(w^\\top x_i) - y_i (w^\\top x_i)) + \\frac{\\lambda}{2} w^\\top w\n$$\nThe gradient descent algorithm proceeds by initializing $w^{(0)}$ to the zero vector and iterating for $T=50$ steps using the derived gradient $\\nabla_w J(w) = X^\\top (\\sigma(Xw) - y) + \\lambda w$. The logistic function $\\sigma(z)$ is implemented with case analysis to prevent overflow for large negative inputs. The process is repeated for each learning rate $\\eta \\in \\{0.0, 0.05, 0.2, 1.5\\}$.\nFor $\\eta=0.0$, the weights do not update from their initial zero-vector state. The final objective is $J(w=\\mathbf{0}) = \\sum_{i=1}^5 \\text{softplus}(0) + 0 = 5 \\log(2) \\approx 3.465736$.\nFor other learning rates, the iterative updates will move $w$ towards the minimum of the convex objective function. A small, stable learning rate like $\\eta=0.05$ or $\\eta=0.2$ is expected to converge smoothly, resulting in a low objective value. A very large learning rate like $\\eta=1.5$ might cause the iterates to overshoot the minimum and diverge, leading to a large or non-finite objective value.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements gradient descent for L2-regularized logistic regression and\n    evaluates the final objective function value for a suite of learning rates.\n    \"\"\"\n    # Define the fixed, scientifically sound test suite from the problem.\n    X_data = np.array([\n        [1.0, 0.0],\n        [1.0, 1.0],\n        [2.0, -1.0],\n        [-1.0, 2.0],\n        [0.0, -2.0]\n    ])\n    y_data = np.array([1, 1, 1, 0, 0])\n    \n    test_params = {\n        'lambda_reg': 0.1,\n        'w_init': np.zeros(2),\n        'iterations': 50,\n        'learning_rates': [0.0, 0.05, 0.2, 1.5]\n    }\n\n    def stable_sigmoid(z):\n        \"\"\"Numerically stable implementation of the logistic function.\"\"\"\n        # Use np.where for vectorized case analysis\n        return np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))\n\n    def stable_softplus(z):\n        \"\"\"Numerically stable implementation of the softplus function.\"\"\"\n        # Using the identity: softplus(z) = max(0,z) + log(1 + exp(-|z|))\n        return np.maximum(0, z) + np.log(1 + np.exp(-np.abs(z)))\n\n    def objective_function(w, X, y, lambda_reg):\n        \"\"\"Computes the regularized negative log-likelihood (J(w)).\"\"\"\n        z = X @ w\n        # Loss using the stable softplus identity: log(1+exp(z)) - y*z\n        loss = np.sum(stable_softplus(z) - y * z)\n        # L2 regularization term\n        reg_term = (lambda_reg / 2) * np.dot(w, w)\n        return loss + reg_term\n\n    results = []\n    \n    lambda_reg = test_params['lambda_reg']\n    T = test_params['iterations']\n\n    for eta in test_params['learning_rates']:\n        w = test_params['w_init'].copy()\n        \n        for _ in range(T):\n            # Calculate scores z = Xw\n            z = X_data @ w\n            \n            # Predictions pi = sigma(z)\n            pi = stable_sigmoid(z)\n            \n            # Gradient: X^T * (pi - y) + lambda * w\n            gradient = X_data.T @ (pi - y_data) + lambda_reg * w\n            \n            # Gradient descent update\n            w = w - eta * gradient\n        \n        # Compute final objective value\n        final_objective = objective_function(w, X_data, y_data, lambda_reg)\n        results.append(final_objective)\n\n    # Final print statement in the exact required format.\n    # The formatted string f'{r:.6f}' rounds and ensures 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the concept of parameter estimation, this practice shifts our focus to the Binomial distribution, which models the number of successes in a fixed number of trials. You will explore the trade-offs inherent in statistical estimation by comparing the standard Maximum Likelihood Estimator (MLE) with a penalized version that shrinks the estimate towards a prior value, $p_0$. This hands-on analysis of the bias-variance tradeoff is crucial for understanding the core principles of regularization in statistical learning. ",
            "id": "3116263",
            "problem": "Consider a random variable $Y$ that follows a Binomial distribution with parameters $n$ and $p$, written as $Y \\sim \\text{Binomial}(n,p)$. The Binomial probability mass function is given by\n$$\n\\mathbb{P}(Y=y) = \\binom{n}{y} p^{y} (1-p)^{n-y} \\quad \\text{for } y \\in \\{0,1,\\ldots,n\\}.\n$$\nThe associated log-likelihood for $p \\in (0,1)$ given an observation $Y=y$ is\n$$\n\\ell(p; y) = y \\log p + (n-y) \\log (1-p).\n$$\nMaximum Likelihood Estimation (MLE) refers to the method of estimating the parameter $p$ by maximizing the log-likelihood $\\ell(p; y)$ over $p \\in [0,1]$. In statistical learning, it is common to consider penalized likelihoods to incorporate prior beliefs or regularization. Consider the penalized objective\n$$\n\\ell_{\\lambda}(p; y) = \\ell(p; y) - \\lambda \\lvert p - p_0 \\rvert,\n$$\nwhere $\\lambda \\ge 0$ is a penalty strength and $p_0 \\in [0,1]$ is a fixed reference value. The penalized MLE of $p$ is defined as the maximizer of $\\ell_{\\lambda}(p; y)$ over $p \\in [0,1]$.\n\nTasks:\n1. Starting from the fundamental definition of the Binomial likelihood, define the standard MLE $\\hat{p}_{\\text{std}}(y)$ and the penalized MLE $\\hat{p}_{\\lambda}(y)$ for a single observation $Y=y$. The definition must be consistent with maximizing the respective concave objectives over $p \\in [0,1]$.\n2. Using the law of the unconscious statistician, express the bias and variance of an estimator $\\hat{p}(Y)$ under the Binomial sampling model with true parameter $p$ as\n$$\n\\text{Bias}(\\hat{p}) = \\mathbb{E}[\\hat{p}(Y)] - p, \\quad \\text{Var}(\\hat{p}) = \\mathbb{E}[\\hat{p}(Y)^2] - \\left(\\mathbb{E}[\\hat{p}(Y)]\\right)^2,\n$$\nwhere expectations are taken with respect to the distribution of $Y \\sim \\text{Binomial}(n,p)$. Compute these quantities exactly (without simulation) by summing over $y \\in \\{0,\\ldots,n\\}$:\n$$\n\\mathbb{E}[\\hat{p}(Y)] = \\sum_{y=0}^{n} \\hat{p}(y) \\binom{n}{y} p^{y} (1-p)^{n-y}, \\quad \\mathbb{E}[\\hat{p}(Y)^2] = \\sum_{y=0}^{n} \\hat{p}(y)^2 \\binom{n}{y} p^{y} (1-p)^{n-y}.\n$$\n3. Analyze the bias-variance tradeoff induced by the penalty strength $\\lambda$ by computing the bias and variance for both the standard MLE $\\hat{p}_{\\text{std}}(Y)$ and the penalized MLE $\\hat{p}_{\\lambda}(Y)$ for the following test suite. No physical units are involved, and all quantities should be expressed as real numbers (floats). Do not use percentage signs; all probabilities must be treated as decimals in $[0,1]$.\n\nTest suite:\n- Case A: $n=20$, $p=0.4$, $p_0=0.5$, $\\Lambda = \\{0.0, 0.5, 2.0\\}$.\n- Case B: $n=20$, $p=0.05$, $p_0=0.2$, $\\Lambda = \\{0.0, 1.0, 4.0\\}$.\n- Case C: $n=20$, $p=0.95$, $p_0=0.8$, $\\Lambda = \\{0.0, 1.0, 4.0\\}$.\n\nComputational requirements:\n- For each case, for each $y \\in \\{0,\\ldots,n\\}$, compute $\\hat{p}_{\\text{std}}(y)$ and $\\hat{p}_{\\lambda}(y)$ by maximizing the appropriate objective over $p \\in [0,1]$. The standard MLE is obtained by maximizing $\\ell(p; y)$; the penalized MLE is obtained by maximizing $\\ell_{\\lambda}(p; y)$. The penalized objective is concave but nonsmooth at $p=p_0$, so use a numerically stable unimodal search on $p \\in [0,1]$.\n- Using the Binomial probability mass function, compute $\\text{Bias}$, $\\text{Var}$, and the mean squared error $\\text{MSE} = \\text{Bias}^2 + \\text{Var}$ for each estimator.\n\nFinal output specification:\n- Your program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets.\n- For each case (in the order A, B, C), output first three floats for the standard MLE: $[\\text{Bias}_{\\text{std}}, \\text{Var}_{\\text{std}}, \\text{MSE}_{\\text{std}}]$, followed by, for each $\\lambda$ in the specified set $\\Lambda$ (in the given order), the three corresponding floats for the penalized estimator: $[\\text{Bias}_{\\lambda}, \\text{Var}_{\\lambda}, \\text{MSE}_{\\lambda}]$.\n- Numbers should be formatted as decimals. For example, the output structure is\n$$\n[\\text{A-std-bias}, \\text{A-std-var}, \\text{A-std-mse}, \\text{A-}\\lambda_1\\text{-bias}, \\text{A-}\\lambda_1\\text{-var}, \\text{A-}\\lambda_1\\text{-mse}, \\ldots, \\text{C-}\\lambda_3\\text{-mse}].\n$$\nEnsure the list order exactly matches the case order (A, B, C) and, within each case, the $\\lambda$ order as given.",
            "solution": "The problem requires a statistical analysis of a penalized Maximum Likelihood Estimator (MLE) for the parameter $p$ of a Binomial distribution, $Y \\sim \\text{Binomial}(n, p)$. We are tasked with computing and comparing the bias, variance, and mean squared error (MSE) of the standard MLE and a penalized MLE. The analysis will be performed for a specific set of test cases.\n\n### Step 1: Definition of the Estimators\n\nFirst, we formally define the estimators for a given observation $Y=y$.\n\n**1.1. Standard Maximum Likelihood Estimator (MLE)**\n\nThe standard MLE, denoted $\\hat{p}_{\\text{std}}(y)$, is the value of $p$ that maximizes the log-likelihood function $\\ell(p; y)$ over the valid parameter space $p \\in [0,1]$. The log-likelihood is given by:\n$$\n\\ell(p; y) = y \\log p + (n-y) \\log (1-p)\n$$\nFor $p \\in (0,1)$, this function is strictly concave. Its unique maximizer is found by setting its derivative with respect to $p$ to zero:\n$$\n\\frac{\\partial \\ell(p; y)}{\\partial p} = \\frac{y}{p} - \\frac{n-y}{1-p} = 0\n$$\nSolving for $p$ yields $y(1-p) = (n-y)p$, which simplifies to $y = np$, giving $p = y/n$.\nFor the boundary cases $y=0$ and $y=n$, the log-likelihood function is $n \\log(1-p)$ and $n \\log p$, respectively. These are maximized at $p=0$ and $p=1$. Thus, the MLE is uniformly given by:\n$$\n\\hat{p}_{\\text{std}}(y) = \\frac{y}{n} \\quad \\text{ for } y \\in \\{0, 1, \\ldots, n\\}\n$$\nAs a random variable, the estimator is $\\hat{p}_{\\text{std}}(Y) = Y/n$. It is well-known that this estimator is unbiased, i.e., $\\mathbb{E}[\\hat{p}_{\\text{std}}(Y)] = \\mathbb{E}[Y/n] = (np)/n = p$.\n\n**1.2. Penalized Maximum Likelihood Estimator**\n\nThe penalized MLE, denoted $\\hat{p}_{\\lambda}(y)$, maximizes the penalized objective function $\\ell_{\\lambda}(p; y)$ over $p \\in [0,1]$:\n$$\n\\ell_{\\lambda}(p; y) = \\ell(p; y) - \\lambda \\lvert p - p_0 \\rvert\n$$\nwhere $\\lambda \\ge 0$ is a penalty strength and $p_0 \\in [0,1]$ is a reference value. The objective function $\\ell_{\\lambda}(p; y)$ is a sum of a strictly concave function, $\\ell(p; y)$, and a concave function, $-\\lambda|p-p_0|$. Therefore, $\\ell_{\\lambda}(p; y)$ is strictly concave, which guarantees the existence of a unique maximizer for any given $y, n, \\lambda, p_0$.\n\nThe absolute value term $|p - p_0|$ makes the objective non-differentiable at $p=p_0$. While an analytical solution can be derived by analyzing the subgradient of $\\ell_{\\lambda}(p; y)$, the problem suggests a numerical approach. As the objective function is concave and thus unimodal, we can employ a robust numerical optimization algorithm like ternary search or golden-section search to find the maximizer.\n\nThe effect of the penalty is to \"shrink\" the standard estimate $\\hat{p}_{\\text{std}}(y) = y/n$ towards the reference value $p_0$. Consequently, the maximizer $\\hat{p}_{\\lambda}(y)$ must lie in the closed interval between $\\hat{p}_{\\text{std}}(y)$ and $p_0$. This property allows us to constrain the search space for the numerical optimization to $[\\min(y/n, p_0), \\max(y/n, p_0)]$, which improves numerical stability and efficiency, particularly by avoiding the boundaries $p=0$ and $p=1$ where the logarithm is undefined, unless $y/n$ is one of these boundaries.\n\n### Step 2: Bias, Variance, and MSE Computation\n\nThe statistical properties of an estimator $\\hat{p}(Y)$ are evaluated with respect to the true data-generating distribution, $Y \\sim \\text{Binomial}(n, p)$. We compute the bias, variance, and MSE.\n\nThe expectation of any function $g(Y)$ is calculated by summing over all possible outcomes of $Y$, weighted by their probabilities:\n$$\n\\mathbb{E}[g(Y)] = \\sum_{y=0}^{n} g(y) \\mathbb{P}(Y=y) = \\sum_{y=0}^{n} g(y) \\binom{n}{y} p^{y} (1-p)^{n-y}\n$$\nUsing this, we can compute the first and second moments of an estimator $\\hat{p}(Y)$:\n- First moment: $\\mathbb{E}[\\hat{p}(Y)] = \\sum_{y=0}^{n} \\hat{p}(y) \\mathbb{P}(Y=y)$\n- Second moment: $\\mathbb{E}[\\hat{p}(Y)^2] = \\sum_{y=0}^{n} \\hat{p}(y)^2 \\mathbbP(Y=y)$\n\nThe bias, variance, and MSE are then defined as:\n- **Bias**: $\\text{Bias}(\\hat{p}) = \\mathbb{E}[\\hat{p}(Y)] - p$\n- **Variance**: $\\text{Var}(\\hat{p}) = \\mathbb{E}[\\hat{p}(Y)^2] - \\left(\\mathbb{E}[\\hat{p}(Y)]\\right)^2$\n- **Mean Squared Error (MSE)**: $\\text{MSE}(\\hat{p}) = \\text{Bias}(\\hat{p})^2 + \\text{Var}(\\hat{p})$\n\n### Step 3: Computational Procedure\n\nThe overall procedure to obtain the results for the test suite is as follows:\n1.  For each test case (A, B, C) defined by $(n, p, p_0, \\Lambda)$:\n    a.  Generate the probability mass function (PMF) values, $\\mathbb{P}(Y=y)$, for $y \\in \\{0, 1, \\ldots, n\\}$.\n    b.  First, compute the metrics for the standard MLE, $\\hat{p}_{\\text{std}}(Y)$. The estimates are $\\hat{p}_{\\text{std}}(y) = y/n$.\n    c.  Then, for each penalty strength $\\lambda$ in the set $\\Lambda$:\n        i.   Determine the penalized estimates $\\hat{p}_{\\lambda}(y)$ for all $y \\in \\{0, 1, \\ldots, n\\}$ by numerically maximizing $\\ell_{\\lambda}(p; y)$ using a ternary search over the constrained interval $[\\min(y/n, p_0), \\max(y/n, p_0)]$. The case $\\lambda=0.0$ serves to reproduce the standard MLE, providing a consistency check.\n        ii.  Using the computed estimates $\\{\\hat{p}(y)\\}_{y=0}^n$ and the PMF, calculate $\\mathbb{E}[\\hat{p}(Y)]$ and $\\mathbb{E}[\\hat{p}(Y)^2]$.\n        iii. From these moments, compute the bias, variance, and MSE.\n2.  Aggregate all computed metrics into a single list in the specified order.\n\nThis structured approach allows us to systematically evaluate the impact of the L1-style penalty on the estimator's performance, thus exploring the bias-variance tradeoff. We anticipate that as $\\lambda$ increases, the bias will generally increase (unless $p_0$ is coincidentally equal to $p$), while the variance will decrease. The MSE, which balances these two, may decrease for small $\\lambda$ before increasing again.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing bias, variance, and MSE for standard and \n    penalized MLEs of a Binomial parameter.\n    \"\"\"\n    \n    test_cases = [\n        {'n': 20, 'p': 0.4, 'p0': 0.5, 'lambdas': [0.0, 0.5, 2.0]},\n        {'n': 20, 'p': 0.05, 'p0': 0.2, 'lambdas': [0.0, 1.0, 4.0]},\n        {'n': 20, 'p': 0.95, 'p0': 0.8, 'lambdas': [0.0, 1.0, 4.0]},\n    ]\n\n    def get_penalized_mle(y, n, lam, p0, tol=1e-12):\n        \"\"\"\n        Computes the penalized MLE for p for a given observation y.\n        \"\"\"\n        p_std = y / n\n        \n        # Handle the case where lambda is zero, which is the standard MLE.\n        if lam == 0.0:\n            return p_std\n\n        # Objective function to be maximized.\n        def objective(p, y, n, lam, p0):\n            if p  tol and y > 0:\n                return -np.inf\n            if p > 1 - tol and y  n:\n                return -np.inf\n\n            # Use x*log(y) logic to avoid 0*log(0) = NaN\n            log_p_term = 0 if y == 0 else y * np.log(p)\n            log_1_p_term = 0 if n == y else (n - y) * np.log(1 - p)\n            \n            log_lik = log_p_term + log_1_p_term\n            penalty = lam * abs(p - p0)\n            return log_lik - penalty\n\n        # The maximizer is known to lie between the standard MLE and p0.\n        low = min(p_std, p0)\n        high = max(p_std, p0)\n\n        # If the interval is trivial.\n        if abs(high - low)  tol:\n            return low\n\n        # Ternary search for the maximum of a unimodal function.\n        for _ in range(100): # 100 iterations are sufficient for high precision.\n            if high - low  tol:\n                break\n            m1 = low + (high - low) / 3\n            m2 = high - (high - low) / 3\n            if objective(m1, y, n, lam, p0)  objective(m2, y, n, lam, p0):\n                low = m1\n            else:\n                high = m2\n        \n        return (low + high) / 2\n\n    results = []\n    \n    for case in test_cases:\n        n, p_true, p0, lambdas = case['n'], case['p'], case['p0'], case['lambdas']\n        \n        ys = np.arange(n + 1)\n        # Pre-compute binomial probabilities\n        pmf = np.array([comb(n, y) * (p_true**y) * ((1 - p_true)**(n - y)) for y in ys])\n\n        # --- Standard MLE ---\n        phat_std_values = ys / n\n        \n        E_phat_std = np.sum(phat_std_values * pmf)\n        E_phat_std_sq = np.sum(phat_std_values**2 * pmf)\n        \n        bias_std = E_phat_std - p_true\n        var_std = E_phat_std_sq - E_phat_std**2\n        mse_std = bias_std**2 + var_std\n        \n        results.extend([bias_std, var_std, mse_std])\n        \n        # --- Penalized MLE for each lambda ---\n        for lam in lambdas:\n            phat_pen_values = np.array([get_penalized_mle(y, n, lam, p0) for y in ys])\n            \n            E_phat_pen = np.sum(phat_pen_values * pmf)\n            E_phat_pen_sq = np.sum(phat_pen_values**2 * pmf)\n\n            bias_pen = E_phat_pen - p_true\n            var_pen = E_phat_pen_sq - E_phat_pen**2\n            mse_pen = bias_pen**2 + var_pen\n            \n            results.extend([bias_pen, var_pen, mse_pen])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In this final practice, we move to modeling count data using the powerful framework of Poisson regression. You will implement the Iteratively Reweighted Least Squares (IRLS) algorithm to fit a model and confront a common real-world challenge: model misspecification. By generating data with over-dispersion and comparing your model's performance to an ideal \"oracle,\" you will gain a deep appreciation for the importance of checking model assumptions and understanding their impact on predictive accuracy. ",
            "id": "3116193",
            "problem": "You are given a discrete-data modeling task grounded in the core definitions of the Poisson distribution and the Negative Binomial distribution, and the fundamental properties of conditional expectations for prediction. The goal is to construct synthetic datasets in which the conditional mean of counts is governed by a log-linear function of features, but the true data-generating process exhibits over-dispersion consistent with a Negative Binomial model. You must then fit a Poisson regression model under a misspecified variance assumption and quantify the out-of-sample prediction error relative to the oracle predictor.\n\nUse the following principles as the starting point of your derivation and implementation:\n- The Poisson distribution with rate parameter $\\lambda$ has probability mass function $P(N = k \\mid \\lambda) = \\exp(-\\lambda)\\lambda^{k}/k!$ for $k \\in \\{0,1,2,\\dots\\}$, mean $E[N \\mid \\lambda] = \\lambda$, and variance $\\operatorname{Var}(N \\mid \\lambda) = \\lambda$.\n- The Negative Binomial distribution can be represented as a Poisson-Gamma mixture: if $\\Lambda \\sim \\operatorname{Gamma}(\\text{shape}=\\theta,\\text{scale}=\\mu/\\theta)$ and $N \\mid \\Lambda \\sim \\operatorname{Poisson}(\\Lambda)$, then $N$ has a Negative Binomial distribution with mean $E[N] = \\mu$ and variance $\\operatorname{Var}(N) = \\mu + \\mu^{2}/\\theta$. This captures over-dispersion relative to the Poisson model when $\\theta$ is finite.\n- For prediction under squared error loss, the conditional expectation $E[N \\mid X]$ is the oracle predictor that minimizes mean squared prediction error.\n\nYour program must:\n1. For each specified test case $t$, generate a training dataset $\\{(x_{i}, n_{i})\\}_{i=1}^{n_{\\text{train}}}$ and an independent test dataset $\\{(x_{j}^{\\ast}, n_{j}^{\\ast})\\}_{j=1}^{n_{\\text{test}}}$ according to the following process.\n   - Features: Let $p$ denote the number of non-intercept features. Construct $x_{i} \\in \\mathbb{R}^{p+1}$ by concatenating an intercept term $x_{i,0} = 1$ with $p$ independent standard normal coordinates $x_{i,1},\\dots,x_{i,p} \\sim \\mathcal{N}(0,1)$. Do the same for $x_{j}^{\\ast}$ on the test set.\n   - Mean function: Let $\\beta \\in \\mathbb{R}^{p+1}$ be fixed. Define the conditional mean $\\mu_{i} = \\exp(\\beta^{\\top} x_{i})$ and $\\mu_{j}^{\\ast} = \\exp(\\beta^{\\top} x_{j}^{\\ast})$.\n   - True counts (Negative Binomial): For each $i$, draw a latent rate $\\lambda_{i} \\sim \\operatorname{Gamma}(\\text{shape}=\\theta, \\text{scale}=\\mu_{i}/\\theta)$ and then draw the count $n_{i} \\sim \\operatorname{Poisson}(\\lambda_{i})$. For each $j$, draw $\\lambda_{j}^{\\ast} \\sim \\operatorname{Gamma}(\\text{shape}=\\theta, \\text{scale}=\\mu_{j}^{\\ast}/\\theta)$ and then $n_{j}^{\\ast} \\sim \\operatorname{Poisson}(\\lambda_{j}^{\\ast})$.\n   - Randomness must be reproducible by using the provided seed $s$ for the training set and seed $s+1000$ for the test set in each case.\n2. Fit a Poisson regression model to the training data by maximum likelihood using the canonical log link. Specifically, treat $n_{i} \\sim \\operatorname{Poisson}(\\tilde{\\mu}_{i})$ with $\\tilde{\\mu}_{i} = \\exp(\\tilde{\\beta}^{\\top} x_{i})$ and compute the maximum likelihood estimate $\\hat{\\beta}$ by solving the first-order optimality condition implied by the Poisson log-likelihood. You must implement a numerical method based on iteratively reweighted least squares derived from the gradient and Hessian; the use of external Generalized Linear Model (GLM) libraries is not permitted. Linear algebra routines from the standard library or the specified numerical libraries are permitted.\n3. On the test set, compute the following quantities:\n   - The Poisson-fit predictor $\\hat{\\mu}_{j} = \\exp(\\hat{\\beta}^{\\top} x_{j}^{\\ast})$ and its mean squared error $\\operatorname{MSE}_{\\text{P}} = \\frac{1}{n_{\\text{test}}}\\sum_{j=1}^{n_{\\text{test}}} (n_{j}^{\\ast} - \\hat{\\mu}_{j})^{2}$.\n   - The oracle predictor $\\mu_{j}^{\\ast} = \\exp(\\beta^{\\top} x_{j}^{\\ast})$ and its mean squared error $\\operatorname{MSE}_{\\text{O}} = \\frac{1}{n_{\\text{test}}}\\sum_{j=1}^{n_{\\text{test}}} (n_{j}^{\\ast} - \\mu_{j}^{\\ast})^{2}$.\n   - The misspecification error ratio $R = \\operatorname{MSE}_{\\text{P}} / \\operatorname{MSE}_{\\text{O}}$.\n4. Produce the final output as a single line containing the list of ratios $[R_{1}, R_{2}, R_{3}]$ corresponding to the test cases below, rounded to six decimal places, with no other text.\n\nTest suite specification:\n- Case $1$ (moderate over-dispersion, happy path): $n_{\\text{train}} = 500$, $n_{\\text{test}} = 5000$, $p = 3$, $\\beta = [0.2, 0.5, -0.3, 0.1]$ (first element is the intercept), $\\theta = 2.0$, seed $s = 2025$.\n- Case $2$ (near-Poisson variance, boundary condition): $n_{\\text{train}} = 500$, $n_{\\text{test}} = 5000$, $p = 3$, $\\beta = [0.2, 0.5, -0.3, 0.1]$, $\\theta = 50.0$, seed $s = 2026$.\n- Case $3$ (high over-dispersion and smaller training set, edge case): $n_{\\text{train}} = 200$, $n_{\\text{test}} = 5000$, $p = 3$, $\\beta = [0.2, 0.5, -0.3, 0.1]$, $\\theta = 0.5$, seed $s = 2027$.\n\nFinal output format:\n- Your program should produce a single line of output containing the ratios as a comma-separated list enclosed in square brackets and rounded to six decimal places, for example, $[1.234567,1.000001,2.500000]$.",
            "solution": "The posed problem requires the construction of a simulation study to evaluate the predictive performance of a misspecified Poisson regression model. The true data-generating process follows a Negative Binomial distribution, which exhibits over-dispersion relative to the Poisson model. The evaluation is performed by comparing the Mean Squared Error (MSE) of the fitted model against the MSE of an oracle predictor, which has access to the true underlying data-generating parameters. The solution involves three main stages: data generation, model fitting via Iteratively Reweighted Least Squares (IRLS), and performance evaluation.\n\n### 1. Data Generation Process\n\nFor each test case, we generate a training set $\\{(x_{i}, n_{i})\\}_{i=1}^{n_{\\text{train}}}$ and a test set $\\{(x_{j}^{\\ast}, n_{j}^{\\ast})\\}_{j=1}^{n_{\\text{test}}}$.\n\n**Features**: The feature vectors $x_i \\in \\mathbb{R}^{p+1}$ are constructed by concatenating an intercept term $x_{i,0} = 1$ with $p$ feature coordinates drawn independently from a standard normal distribution, $x_{i,k} \\sim \\mathcal{N}(0, 1)$ for $k=1, \\dots, p$. The design matrices for the training and test sets are denoted by $X_{\\text{train}}$ and $X_{\\text{test}}$, respectively.\n\n**Counts**: The count data $n_i$ are generated from a Negative Binomial distribution using the specified Poisson-Gamma mixture formulation. This process correctly models a conditional mean $\\mu = E[N \\mid X]$ while introducing over-dispersion.\nFirst, the conditional mean $\\mu_i$ for a given feature vector $x_i$ is defined by a log-linear model:\n$$ \\mu_i = \\exp(\\beta^{\\top} x_i) $$\nwhere $\\beta$ is the true vector of regression coefficients.\n\nThe Negative Binomial counts are then generated in two steps:\n1.  A latent rate variable $\\lambda_i$ is drawn from a Gamma distribution:\n    $$ \\lambda_i \\sim \\operatorname{Gamma}(\\text{shape}=a, \\text{scale}=s) $$\n    where the shape parameter is $a = \\theta$ and the scale parameter is $s = \\mu_i / \\theta$. The mean of this Gamma distribution is $E[\\lambda_i] = a \\cdot s = \\theta (\\mu_i / \\theta) = \\mu_i$.\n2.  The observed count $n_i$ is drawn from a Poisson distribution with the latent rate $\\lambda_i$:\n    $$ n_i \\mid \\lambda_i \\sim \\operatorname{Poisson}(\\lambda_i) $$\nThe marginal distribution of $n_i$ is Negative Binomial with mean $E[n_i] = E[E[n_i \\mid \\lambda_i]] = E[\\lambda_i] = \\mu_i$ and variance $\\operatorname{Var}(n_i) = E[\\operatorname{Var}(n_i \\mid \\lambda_i)] + \\operatorname{Var}(E[n_i \\mid \\lambda_i]) = E[\\lambda_i] + \\operatorname{Var}(\\lambda_i) = \\mu_i + \\theta (\\mu_i / \\theta)^2 = \\mu_i + \\mu_i^2 / \\theta$. The term $\\mu_i^2 / \\theta$ represents the over-dispersion; as $\\theta \\to \\infty$, the variance approaches the mean $\\mu_i$, and the Negative Binomial distribution converges to the Poisson distribution.\n\nRandom seeds $s$ and $s+1000$ are used for the training and test sets respectively to ensure the reproducibility of the generated data.\n\n### 2. Poisson Regression via Iteratively Reweighted Least Squares (IRLS)\n\nA Poisson regression model is fitted to the training data $\\{(x_i, n_i)\\}$. This model is misspecified because it assumes $n_i \\sim \\operatorname{Poisson}(\\tilde{\\mu}_i)$ with $\\operatorname{Var}(n_i) = \\tilde{\\mu}_i$, whereas the data are over-dispersed. The model link function is logarithmic: $\\tilde{\\mu}_i = \\exp(x_i^\\top \\tilde{\\beta})$. The parameter vector $\\tilde{\\beta}$ is estimated by maximizing the Poisson log-likelihood function, which for the entire training set of size $n_{\\text{train}}$ is:\n$$ \\ell(\\tilde{\\beta}) = \\sum_{i=1}^{n_{\\text{train}}} \\left( n_i \\log(\\tilde{\\mu}_i) - \\tilde{\\mu}_i - \\log(n_i!) \\right) = \\sum_{i=1}^{n_{\\text{train}}} \\left( n_i (x_i^\\top \\tilde{\\beta}) - \\exp(x_i^\\top \\tilde{\\beta}) - \\log(n_i!) \\right) $$\nThe maximum likelihood estimate $\\hat{\\beta}$ is found by solving $\\nabla_{\\tilde{\\beta}} \\ell(\\tilde{\\beta}) = 0$. This is done using a Newton-Raphson-based algorithm known as a Fisher scoring or, in this context, Iteratively Reweighted Least Squares (IRLS).\n\nThe gradient of the log-likelihood (score vector) is:\n$$ g(\\tilde{\\beta}) = \\nabla_{\\tilde{\\beta}} \\ell(\\tilde{\\beta}) = \\sum_{i=1}^{n_{\\text{train}}} (n_i - \\tilde{\\mu}_i) x_i = X_{\\text{train}}^\\top (n - \\tilde{\\mu}) $$\nThe Hessian matrix is:\n$$ H(\\tilde{\\beta}) = \\nabla_{\\tilde{\\beta}}^2 \\ell(\\tilde{\\beta}) = -\\sum_{i=1}^{n_{\\text{train}}} \\tilde{\\mu}_i x_i x_i^\\top = -X_{\\text{train}}^\\top W X_{\\text{train}} $$\nwhere $W$ is a diagonal matrix of weights with $W_{ii} = \\tilde{\\mu}_i$. The Newton-Raphson update at iteration $k$ is:\n$$ \\tilde{\\beta}^{(k+1)} = \\tilde{\\beta}^{(k)} - [H(\\tilde{\\beta}^{(k)})]^{-1} g(\\tilde{\\beta}^{(k)}) = \\tilde{\\beta}^{(k)} + (X_{\\text{train}}^\\top W^{(k)} X_{\\text{train}})^{-1} X_{\\text{train}}^\\top (n - \\tilde{\\mu}^{(k)}) $$\nThis can be rearranged into the IRLS form. By defining a \"working response\" vector $z^{(k)}$ with elements:\n$$ z_i^{(k)} = (x_i^\\top \\tilde{\\beta}^{(k)}) + \\frac{n_i - \\tilde{\\mu}_i^{(k)}}{\\tilde{\\mu}_i^{(k)}} $$\nthe update becomes a weighted least squares solution:\n$$ \\tilde{\\beta}^{(k+1)} = (X_{\\text{train}}^\\top W^{(k)} X_{\\text{train}})^{-1} X_{\\text{train}}^\\top W^{(k)} z^{(k)} $$\nThe algorithm proceeds as follows:\n1. Initialize $\\tilde{\\beta}^{(0)}$, for instance as a zero vector.\n2. For $k = 0, 1, 2, \\dots$ until convergence:\n   a. Compute the linear predictor $\\eta^{(k)} = X_{\\text{train}} \\tilde{\\beta}^{(k)}$.\n   b. Compute the mean $\\tilde{\\mu}^{(k)} = \\exp(\\eta^{(k)})$.\n   c. Form the weight matrix $W^{(k)} = \\operatorname{diag}(\\tilde{\\mu}_i^{(k)})$.\n   d. Compute the working response $z^{(k)} = \\eta^{(k)} + (n - \\tilde{\\mu}^{(k)}) / \\tilde{\\mu}^{(k)}$.\n   e. Solve the weighted least squares system for $\\tilde{\\beta}^{(k+1)}$.\n3. The converged vector is the MLE, denoted $\\hat{\\beta}$.\n\n### 3. Performance Evaluation\n\nThe predictive performance is assessed on the independent test set. Under squared error loss, the optimal predictor (the oracle) is the conditional expectation of the counts given the features, $E[n_j^\\ast \\mid x_j^\\ast]$, which is the true mean function $\\mu_j^{\\ast} = \\exp(\\beta^\\top x_j^\\ast)$.\n\nThe following quantities are computed:\n- **Oracle Predictor**: $\\mu_j^{\\ast} = \\exp(\\beta^\\top x_j^\\ast)$.\n- **Oracle MSE**: $\\operatorname{MSE}_{\\text{O}} = \\frac{1}{n_{\\text{test}}}\\sum_{j=1}^{n_{\\text{test}}} (n_j^{\\ast} - \\mu_j^{\\ast})^2$. This measures the irreducible error due to the inherent randomness (variance) of the Negative Binomial process.\n- **Poisson-fit Predictor**: $\\hat{\\mu}_j = \\exp(\\hat{\\beta}^\\top x_j^{\\ast})$, using the estimated $\\hat{\\beta}$ from the training set.\n- **Poisson-fit MSE**: $\\operatorname{MSE}_{\\text{P}} = \\frac{1}{n_{\\text{test}}}\\sum_{j=1}^{n_{\\text{test}}} (n_j^{\\ast} - \\hat{\\mu}_j)^2$. This error includes both the irreducible error and the additional error from using an estimated, misspecified model.\n- **Misspecification Error Ratio**: $R = \\operatorname{MSE}_{\\text{P}} / \\operatorname{MSE}_{\\text{O}}$. This ratio quantifies the degradation in predictive accuracy. A value close to $1$ indicates that the misspecified model performs nearly as well as the oracle, while a value greater than $1$ indicates a performance loss.\n\nThis ratio is computed for each of the three test cases specified in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef fit_poisson_regression(X, n, max_iter=50, tol=1e-8):\n    \"\"\"\n    Fits a Poisson regression model using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        X (np.ndarray): The design matrix (n_samples, n_features).\n        n (np.ndarray): The vector of observed counts (n_samples,).\n        max_iter (int): Maximum number of iterations for the IRLS algorithm.\n        tol (float): Convergence tolerance for the change in the beta vector norm.\n\n    Returns:\n        np.ndarray: The estimated coefficient vector beta_hat.\n    \"\"\"\n    # Initialize beta coefficients to zero\n    beta = np.zeros(X.shape[1])\n    \n    for k in range(max_iter):\n        beta_old = beta.copy()\n        \n        # Calculate linear predictor and mean\n        eta = X @ beta\n        mu = np.exp(eta)\n        \n        # Calculate weights and working response\n        # W is a diagonal matrix, but we can use element-wise multiplication for efficiency\n        weights = mu\n        z = eta + (n - mu) / mu\n        \n        # Solve the weighted least squares system: beta = (X'WX)^-1 * X'Wz\n        # Using np.linalg.solve for numerical stability\n        A = X.T @ (weights[:, np.newaxis] * X)\n        b = X.T @ (weights * z)\n        \n        try:\n            beta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # If matrix is singular, return the last valid beta\n            # This might happen in edge cases with sparse data\n            return beta_old\n            \n        # Check for convergence\n        if np.linalg.norm(beta - beta_old)  tol:\n            break\n            \n    return beta\n\ndef generate_data(n_samples, p, beta, theta, seed):\n    \"\"\"\n    Generates synthetic data according to the Negative Binomial process.\n\n    Args:\n        n_samples (int): The number of data points to generate.\n        p (int): The number of non-intercept features.\n        beta (np.ndarray): The true coefficient vector.\n        theta (float): The dispersion parameter for the Gamma distribution.\n        seed (int): The random seed for reproducibility.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray, np.ndarray]: X, n, mu_true\n            - X: design matrix\n            - n: generated counts\n            - mu_true: true conditional means\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate features X: intercept + N(0,1) features\n    X0 = np.ones((n_samples, 1))\n    X_features = rng.standard_normal(size=(n_samples, p))\n    X = np.hstack((X0, X_features))\n    \n    # Calculate true conditional mean mu\n    mu_true = np.exp(X @ beta)\n    \n    # Generate counts from Negative Binomial (Poisson-Gamma mixture)\n    # 1. Draw lambda from Gamma(shape=theta, scale=mu/theta)\n    shape = theta\n    scale = mu_true / theta\n    lambdas = rng.gamma(shape, scale, size=n_samples)\n    \n    # 2. Draw n from Poisson(lambda)\n    n = rng.poisson(lambdas)\n    \n    return X, n, mu_true\n\ndef run_simulation_case(n_train, n_test, p, beta_list, theta, seed):\n    \"\"\"\n    Runs the full simulation for one test case.\n\n    Args:\n        n_train (int): Size of the training set.\n        n_test (int): Size of the test set.\n        p (int): Number of non-intercept features.\n        beta_list (list): The true coefficient vector as a list.\n        theta (float): The dispersion parameter.\n        seed (int): The base random seed.\n\n    Returns:\n        float: The misspecification error ratio R = MSE_P / MSE_O.\n    \"\"\"\n    beta_true = np.array(beta_list)\n    \n    # 1. Generate training and test data\n    X_train, n_train, _ = generate_data(n_train, p, beta_true, theta, seed)\n    X_test, n_test, mu_oracle = generate_data(n_test, p, beta_true, theta, seed + 1000)\n    \n    # 2. Fit Poisson regression model on training data\n    beta_hat = fit_poisson_regression(X_train, n_train)\n    \n    # 3. Evaluate on the test set\n    # Predictions from the fitted misspecified Poisson model\n    mu_hat_p = np.exp(X_test @ beta_hat)\n    \n    # Mean Squared Error for the Poisson-fit model\n    mse_p = np.mean((n_test - mu_hat_p)**2)\n    \n    # Mean Squared Error for the oracle predictor\n    mse_o = np.mean((n_test - mu_oracle)**2)\n    \n    # Calculate the misspecification error ratio\n    ratio = mse_p / mse_o\n    \n    return ratio\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n_train, n_test, p, beta, theta, seed)\n        (500, 5000, 3, [0.2, 0.5, -0.3, 0.1], 2.0, 2025), # Case 1\n        (500, 5000, 3, [0.2, 0.5, -0.3, 0.1], 50.0, 2026), # Case 2\n        (200, 5000, 3, [0.2, 0.5, -0.3, 0.1], 0.5, 2027), # Case 3\n    ]\n    \n    results = []\n    for case in test_cases:\n        ratio = run_simulation_case(*case)\n        results.append(ratio)\n    \n    # Format the results to exactly six decimal places\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Print the final output in the required format\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}