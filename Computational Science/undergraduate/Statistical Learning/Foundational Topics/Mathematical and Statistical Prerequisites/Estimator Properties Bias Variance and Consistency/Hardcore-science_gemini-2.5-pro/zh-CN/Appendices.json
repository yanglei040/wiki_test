{
    "hands_on_practices": [
        {
            "introduction": "在处理分组数据时，一个直观的方法是为每组单独计算均值。这种“无汇集”（no-pooling）估计量是无偏的，但当某些组的样本量很小时，其估计结果可能非常不稳定且不可靠。本练习  将引导你推导并分析一种“部分汇集”（partial pooling）估计量，它通过向全局均值“收缩”（shrinkage）来主动引入少量偏差，以换取方差的大幅降低，从而让你深刻理解偏差-方差权衡的精髓。",
            "id": "3118672",
            "problem": "一位研究人员正在使用正态-正态分层模型对分组数据进行建模。对于组 $g$，他们观测到 $n_{g}$ 个样本 $y_{g1}, y_{g2}, \\dots, y_{g n_{g}}$，这些样本遵循数据生成过程 $y_{gi} \\mid \\theta_{g} \\sim \\mathcal{N}(\\theta_{g}, \\sigma^{2})$，并且特定于组的参数遵循先验分布 $\\theta_{g} \\sim \\mathcal{N}(\\mu, \\tau^{2})$，其中 $\\mu$、$\\sigma^{2}$ 和 $\\tau^{2}$ 是已知常数，且 $\\sigma^{2}  0$ 和 $\\tau^{2}  0$。假设在以 $\\theta_{g}$ 为条件的每个组内，样本是独立同分布的 (i.i.d.)。\n\n考虑组均值 $\\theta_{g}$ 的两种估计量：\n- 无池化 (No pooling)：$\\hat{\\theta}_{g}^{\\mathrm{NP}} = \\bar{y}_{g}$，其中 $\\bar{y}_{g} = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi}$。\n- 部分池化 (Partial pooling)：在分层先验和高斯似然下，最小化后验期望平方损失的估计量。\n\n使用估计量的偏差、方差和一致性的核心定义，并从高斯似然和先验出发，执行以下操作：\n- 从贝叶斯法则推导出部分池化估计量的封闭形式，并计算其条件偏差 $E(\\hat{\\theta}_{g} \\mid \\theta_{g}) - \\theta_{g}$ 和条件方差 $\\mathrm{Var}(\\hat{\\theta}_{g} \\mid \\theta_{g})$。\n- 计算无池化估计量的条件偏差和方差。\n- 根据您推导出的估计量公式，解释部分池化相对于无池化如何减少方差，并分析当 $n_{g} \\to \\infty$ 时两种估计量的一致性（依概率收敛）。\n\n最后，将估计量 $\\hat{\\theta}_{g}$ 的条件均方误差 (MSE) 定义为 $\\mathrm{MSE}(\\hat{\\theta}_{g} \\mid \\theta_{g}) = E\\!\\left[(\\hat{\\theta}_{g} - \\theta_{g})^{2} \\mid \\theta_{g}\\right]$。推导、简化并以单一封闭形式解析表达式呈现以下差值\n$$\\Delta = \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) - \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g})$$\n完全用 $n_{g}$、$\\sigma^{2}$、$\\tau^{2}$、$\\mu$ 和 $\\theta_{g}$ 表示。您的最终答案必须是 $\\Delta$ 的这个精确符号表达式。",
            "solution": "问题陈述经过严格验证，确认有效。这是一个在贝叶斯统计学中定义明确、有科学依据的问题，没有不一致、模糊不清或事实错误之处。所有推导所需量值的必要信息均已提供。\n\n我们首先分析组均值 $\\theta_{g}$ 的两种估计量。除非另有说明，所有期望 $E[\\cdot]$ 和方差 $\\mathrm{Var}(\\cdot)$ 都是以 $\\theta_{g}$ 的真实值为条件的。\n\n**无池化估计量：$\\hat{\\theta}_{g}^{\\mathrm{NP}}$**\n\n无池化估计量定义为组 $g$ 的样本均值：$\\hat{\\theta}_{g}^{\\mathrm{NP}} = \\bar{y}_{g} = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi}$。数据 $y_{gi}$ 从 $y_{gi} \\mid \\theta_{g} \\sim \\mathcal{N}(\\theta_{g}, \\sigma^{2})$ 中抽取。由于样本在以 $\\theta_{g}$ 为条件下是独立同分布 (i.i.d.) 的，样本均值 $\\bar{y}_{g}$ 的抽样分布也是正态的。\n\n$\\bar{y}_{g}$ 的条件期望是：\n$$E[\\bar{y}_{g} \\mid \\theta_{g}] = E\\left[\\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi} \\mid \\theta_{g}\\right] = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} E[y_{gi} \\mid \\theta_{g}] = \\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} \\theta_{g} = \\theta_{g}$$\n$\\bar{y}_{g}$ 的条件方差是：\n$$\\mathrm{Var}(\\bar{y}_{g} \\mid \\theta_{g}) = \\mathrm{Var}\\left(\\frac{1}{n_{g}} \\sum_{i=1}^{n_{g}} y_{gi} \\mid \\theta_{g}\\right) = \\frac{1}{n_{g}^{2}} \\sum_{i=1}^{n_{g}} \\mathrm{Var}(y_{gi} \\mid \\theta_{g}) = \\frac{1}{n_{g}^{2}} (n_{g}\\sigma^{2}) = \\frac{\\sigma^{2}}{n_{g}}$$\n因此，抽样分布为 $\\bar{y}_{g} \\mid \\theta_{g} \\sim \\mathcal{N}(\\theta_{g}, \\frac{\\sigma^{2}}{n_{g}})$。\n\n$\\hat{\\theta}_{g}^{\\mathrm{NP}}$ 的条件偏差是：\n$$E[\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}] - \\theta_{g} = E[\\bar{y}_{g} \\mid \\theta_{g}] - \\theta_{g} = \\theta_{g} - \\theta_{g} = 0$$\n无池化估计量是条件无偏的。\n\n$\\hat{\\theta}_{g}^{\\mathrm{NP}}$ 的条件方差是：\n$$\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}) = \\mathrm{Var}(\\bar{y}_{g} \\mid \\theta_{g}) = \\frac{\\sigma^{2}}{n_{g}}$$\n\n**部分池化估计量：$\\hat{\\theta}_{g}^{\\mathrm{PP}}$**\n\n部分池化估计量是最小化后验期望平方损失的量，即后验均值 $E[\\theta_{g} \\mid y_{g1}, \\dots, y_{gn_{g}}]$。我们通过应用贝叶斯法则来找到它：$p(\\theta_{g} \\mid \\mathbf{y}_{g}) \\propto p(\\mathbf{y}_{g} \\mid \\theta_{g}) p(\\theta_{g})$。样本均值 $\\bar{y}_{g}$ 是 $\\theta_{g}$ 的一个充分统计量。\n$\\bar{y}_{g}$ 的似然为 $p(\\bar{y}_{g} \\mid \\theta_{g}) \\sim \\mathcal{N}(\\theta_{g}, \\frac{\\sigma^{2}}{n_{g}})$。\n$\\theta_{g}$ 的先验为 $p(\\theta_{g}) \\sim \\mathcal{N}(\\mu, \\tau^{2})$。\n\n后验分布与这两个高斯密度的乘积成正比：\n$$p(\\theta_{g} \\mid \\bar{y}_{g}) \\propto \\exp\\left(-\\frac{(\\bar{y}_{g} - \\theta_{g})^{2}}{2\\sigma^{2}/n_{g}}\\right) \\exp\\left(-\\frac{(\\theta_{g} - \\mu)^{2}}{2\\tau^{2}}\\right)$$\n这是一个共轭模型，因此 $\\theta_{g}$ 的后验分布也是一个正态分布，$p(\\theta_{g} \\mid \\bar{y}_{g}) \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\sigma_{\\text{post}}^{2})$。后验精度是先验精度和数据精度的和：\n$$\\frac{1}{\\sigma_{\\text{post}}^{2}} = \\frac{1}{\\tau^{2}} + \\frac{1}{\\sigma^{2}/n_{g}} = \\frac{1}{\\tau^{2}} + \\frac{n_{g}}{\\sigma^{2}}$$\n后验均值 $\\mu_{\\text{post}}$ 是先验均值和数据均值的精度加权平均值：\n$$\\mu_{\\text{post}} = \\sigma_{\\text{post}}^{2} \\left(\\frac{\\mu}{\\tau^{2}} + \\frac{\\bar{y}_{g}}{\\sigma^{2}/n_{g}}\\right) = \\left(\\frac{1}{\\tau^{2}} + \\frac{n_{g}}{\\sigma^{2}}\\right)^{-1} \\left(\\frac{\\mu}{\\tau^{2}} + \\frac{n_{g}\\bar{y}_{g}}{\\sigma^{2}}\\right)$$\n估计量是 $\\hat{\\theta}_{g}^{\\mathrm{PP}} = \\mu_{\\text{post}}$。简化表达式：\n$$\\hat{\\theta}_{g}^{\\mathrm{PP}} = \\frac{\\frac{\\mu}{\\tau^{2}} + \\frac{n_{g}\\bar{y}_{g}}{\\sigma^{2}}}{\\frac{1}{\\tau^{2}} + \\frac{n_{g}}{\\sigma^{2}}} = \\frac{\\sigma^{2}\\mu + n_{g}\\tau^{2}\\bar{y}_{g}}{\\sigma^{2} + n_{g}\\tau^{2}}$$\n这可以表示为加权平均：\n$$\\hat{\\theta}_{g}^{\\mathrm{PP}} = \\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\bar{y}_{g} + \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\mu = w\\bar{y}_{g} + (1-w)\\mu$$\n其中权重为 $w = \\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}$。\n\n$\\hat{\\theta}_{g}^{\\mathrm{PP}}$ 的条件偏差是通过在 $\\theta_{g}$ 条件下取期望来计算的：\n$$E[\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}] = E[w\\bar{y}_{g} + (1-w)\\mu \\mid \\theta_{g}] = w E[\\bar{y}_{g} \\mid \\theta_{g}] + (1-w)\\mu = w\\theta_{g} + (1-w)\\mu$$\n偏差 $= E[\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}] - \\theta_{g} = w\\theta_{g} + (1-w)\\mu - \\theta_{g} = (w-1)\\theta_{g} + (1-w)\\mu = (1-w)(\\mu - \\theta_{g})$。\n代入 $1-w = \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}$，偏差为：\n$$\\text{Bias}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}(\\mu - \\theta_{g})$$\n这个估计量偏向于先验均值 $\\mu$，这种效应被称为收缩 (shrinkage)。\n\n$\\hat{\\theta}_{g}^{\\mathrm{PP}}$ 的条件方差是：\n$$\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\mathrm{Var}(w\\bar{y}_{g} + (1-w)\\mu \\mid \\theta_{g}) = w^{2}\\mathrm{Var}(\\bar{y}_{g} \\mid \\theta_{g}) = w^{2} \\frac{\\sigma^{2}}{n_{g}}$$\n代入 $w$ 的表达式：\n$$\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\left(\\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}}$$\n\n**比较与一致性分析**\n\n方差减小：部分池化估计量的方差为 $\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = w^{2} \\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g})$。由于 $\\sigma^{2}  0$ 且 $\\tau^{2}  0$，权重 $w = \\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}$ 满足 $0  w  1$。因此，$w^{2}  1$，这证明了部分池化估计量的条件方差严格小于无池化估计量。这种方差的减小是通过将估计值“收缩”到先验均值 $\\mu$ 来实现的，使其不易受到数据 $\\bar{y}_{g}$ 中抽样噪声的影响。这是偏差-方差权衡的一个例子；我们引入偏差以减小方差。\n\n一致性：如果一个估计量随着样本量的增长依概率收敛于真实参数，则该估计量是一致的。一个充分条件是其偏差和方差都收敛于零。\n\n对于 $\\hat{\\theta}_{g}^{\\mathrm{NP}}$，当 $n_{g} \\to \\infty$ 时：\n- 偏差始终为 $0$。\n- $\\mathrm{Var}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}) = \\frac{\\sigma^{2}}{n_{g}} \\to 0$。\n因此，$\\hat{\\theta}_{g}^{\\mathrm{NP}}$ 是 $\\theta_{g}$ 的一致估计量。\n\n对于 $\\hat{\\theta}_{g}^{\\mathrm{PP}}$，当 $n_g \\to \\infty$ 时：\n- 偏差：$\\lim_{n_{g}\\to\\infty} \\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}(\\mu - \\theta_{g}) = 0$。\n- 方差：$\\lim_{n_{g}\\to\\infty} \\left(\\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}} = \\lim_{n_{g}\\to\\infty} \\left(\\frac{\\tau^{2}}{\\sigma^{2}/n_{g} + \\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}} = \\left(\\frac{\\tau^{2}}{\\tau^{2}}\\right)^{2} \\cdot 0 = 0$。\n由于偏差和方差都收敛到 $0$，$\\hat{\\theta}_{g}^{\\mathrm{PP}}$ 也是 $\\theta_{g}$ 的一致估计量。随着 $n_g$ 的增加，权重 $w$ 趋近于 $1$，这意味着 $\\hat{\\theta}_{g}^{\\mathrm{PP}}$ 收敛于 $\\hat{\\theta}_{g}^{\\mathrm{NP}}$。数据压倒了先验信息。\n\n**均方误差 (MSE) 差**\n\n条件均方误差定义为 $\\mathrm{MSE}(\\hat{\\theta}_{g} \\mid \\theta_{g}) = (E[\\hat{\\theta}_{g} \\mid \\theta_{g}] - \\theta_{g})^{2} + \\mathrm{Var}(\\hat{\\theta}_{g} \\mid \\theta_{g})$。\n\n对于无池化估计量：\n$$\\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g}) = (0)^{2} + \\frac{\\sigma^{2}}{n_{g}} = \\frac{\\sigma^{2}}{n_{g}}$$\n\n对于部分池化估计量：\n$$\\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) = \\left(\\frac{\\sigma^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}(\\mu - \\theta_{g})\\right)^{2} + \\left(\\frac{n_{g}\\tau^{2}}{\\sigma^{2} + n_{g}\\tau^{2}}\\right)^{2} \\frac{\\sigma^{2}}{n_{g}}$$\n$$= \\frac{\\sigma^{4}(\\mu - \\theta_{g})^{2}}{(n_{g}\\tau^{2} + \\sigma^{2})^{2}} + \\frac{n_{g}^{2}\\tau^{4}\\sigma^{2}}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}} = \\frac{\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}\\tau^{4}\\sigma^{2}}{(n_{g}\\tau^{2} + \\sigma^{2})^{2}}$$\n\n我们现在计算差值 $\\Delta = \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{PP}} \\mid \\theta_{g}) - \\mathrm{MSE}(\\hat{\\theta}_{g}^{\\mathrm{NP}} \\mid \\theta_{g})$：\n$$\\Delta = \\frac{\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}\\tau^{4}\\sigma^{2}}{(n_{g}\\tau^{2} + \\sigma^{2})^{2}} - \\frac{\\sigma^{2}}{n_{g}}$$\n使用公分母 $n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}$：\n$$ \\Delta = \\frac{n_{g}(\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}\\tau^{4}\\sigma^{2}) - \\sigma^{2}(n_{g}\\tau^{2} + \\sigma^{2})^{2}}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}} $$\n展开分子：\n$$ \\text{Numerator} = n_{g}\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}^{2}\\tau^{4}\\sigma^{2} - \\sigma^{2}(n_{g}^{2}\\tau^{4} + 2n_{g}\\tau^{2}\\sigma^{2} + \\sigma^{4}) $$\n$$ = n_{g}\\sigma^{4}(\\mu - \\theta_{g})^{2} + n_{g}^{2}\\tau^{4}\\sigma^{2} - n_{g}^{2}\\tau^{4}\\sigma^{2} - 2n_{g}\\tau^{2}\\sigma^{4} - \\sigma^{6} $$\n$n_{g}^{2}\\tau^{4}\\sigma^{2}$ 项相互抵消：\n$$ \\text{Numerator} = n_{g}\\sigma^{4}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2}\\sigma^{4} - \\sigma^{6} $$\n提取公因数 $\\sigma^{4}$：\n$$ \\text{Numerator} = \\sigma^{4}[n_{g}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2} - \\sigma^{2}] $$\n因此，MSE 差值的最终表达式为：\n$$ \\Delta = \\frac{\\sigma^{4}[n_{g}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2} - \\sigma^{2}]}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}} $$",
            "answer": "$$\\boxed{\\frac{\\sigma^{4}\\left[n_{g}(\\mu - \\theta_{g})^{2} - 2n_{g}\\tau^{2} - \\sigma^{2}\\right]}{n_{g}(n_{g}\\tau^{2} + \\sigma^{2})^{2}}}$$"
        },
        {
            "introduction": "在认识到有偏估计量有时可能更优之后，我们现在转向如何在无偏估计量中做出最佳选择。当线性模型的误差方差不恒定（即存在异方差性）时，普通最小二乘法（OLS）虽然仍是无偏的，但不再是方差最小的估计量。本练习  将通过分析加权最小二乘法（WLS），展示如何获得一个更有效的无偏估计量，从而具体阐释高斯-马尔可夫定理中“最佳”的含义。",
            "id": "3118728",
            "problem": "考虑一个无截距项的异方差简单线性回归模型，\n$$\ny_{i} = \\beta x_{i} + \\epsilon_{i}, \\quad i=1,\\dots,n,\n$$\n其中回归量 $x_{i}$ 与误差 $\\epsilon_{i}$ 独立，条件均值满足 $E[\\epsilon_{i}\\,|\\,x_{i}] = 0$，条件方差具有异方差性，其形式为\n$$\n\\operatorname{Var}(\\epsilon_{i}\\,|\\,x_{i}) = \\sigma^{2} x_{i}^{2},\n$$\n其中 $\\sigma^{2}  0$ 是一个固定的常数。假设 $x_{i}$ 是从标准正态分布 $\\mathcal{N}(0,1)$ 中抽取的独立同分布样本，并且与 $\\epsilon_{i}$ 独立。\n\n定义三个斜率估计量：\n- 普通最小二乘法 (OLS)：$\\hat{\\beta}_{\\text{OLS}}$ 是 $\\sum_{i=1}^{n} (y_{i} - \\beta x_{i})^{2}$ 的最小化子，等价于\n$$\n\\hat{\\beta}_{\\text{OLS}} = \\frac{\\sum_{i=1}^{n} x_{i} y_{i}}{\\sum_{i=1}^{n} x_{i}^{2}}.\n$$\n- 使用正确权重的加权最小二乘法 (WLS)：$\\hat{\\beta}_{\\star}$ 是 $\\sum_{i=1}^{n} w_{i}^{\\star} (y_{i} - \\beta x_{i})^{2}$ 的最小化子，其中权重为 $w_{i}^{\\star} = 1/(\\sigma^{2} x_{i}^{2})$。\n- 使用不正确权重的 WLS：$\\tilde{\\beta}$ 是 $\\sum_{i=1}^{n} \\tilde{w}_{i} (y_{i} - \\beta x_{i})^{2}$ 的最小化子，其中权重为 $\\tilde{w}_{i} = 1/|x_{i}|$。\n\n从无偏性、方差和一致性的定义出发，并且只使用条件期望、方差和独立和的基本性质，完成以下任务：\n1. 给定观测值 $x_{1},\\dots,x_{n}$，推导每个估计量的精确条件偏倚和条件方差。\n2. 在给定的数据生成过程和权重下，说明当 $n \\to \\infty$ 时，每个估计量对于 $\\beta$ 是否是一致的。\n3. 最后，计算当 $n \\to \\infty$ 时，各估计量的方差与使用正确权重的 WLS 估计量 $\\hat{\\beta}_{\\star}$ 的方差之比的大样本极限：\n$$\n\\frac{\\operatorname{Var}(\\hat{\\beta}_{\\text{OLS}}\\,|\\,x_{1},\\dots,x_{n})}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,x_{1},\\dots,x_{n})}, \\quad\n\\frac{\\operatorname{Var}(\\tilde{\\beta}\\,|\\,x_{1},\\dots,x_{n})}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,x_{1},\\dots,x_{n})}, \\quad\n\\frac{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,x_{1},\\dots,x_{n})}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,x_{1},\\dots,x_{n})}.\n$$\n\n将你的答案以一个单行向量的形式报告，其中包含三个极限的精确形式，不要四舍五入。不涉及物理单位。",
            "solution": "该问题要求在一个无截距项的异方差简单线性回归模型中，对斜率参数 $\\beta$ 的三个估计量进行分析。模型由 $y_{i} = \\beta x_{i} + \\epsilon_{i}$ 给出，其中 $i=1,\\dots,n$，且 $E[\\epsilon_{i}\\,|\\,x_{i}] = 0$ 和 $\\operatorname{Var}(\\epsilon_{i}\\,|\\,x_{i}) = \\sigma^{2} x_{i}^{2}$。回归量 $x_i$ 是从标准正态分布 $\\mathcal{N}(0,1)$ 中抽取的独立同分布样本。\n\n我们首先建立这些估计量的一般形式。这三个估计量，即普通最小二乘法 (OLS) 和两种加权最小二乘法 (WLS) 的变体，都是最小化加权残差平方和 $\\sum_{i=1}^{n} w_{i} (y_{i} - \\beta x_{i})^{2}$ 的解。关于 $\\beta$ 的一阶条件是：\n$$\n\\frac{\\partial}{\\partial \\beta} \\sum_{i=1}^{n} w_{i} (y_{i} - \\beta x_{i})^{2} = \\sum_{i=1}^{n} w_{i} (2) (y_{i} - \\beta x_{i}) (-x_{i}) = -2 \\sum_{i=1}^{n} w_{i} x_{i} (y_{i} - \\beta x_{i}) = 0\n$$\n对 $\\beta$ 求解，我们得到该模型中 WLS 估计量的一般公式：\n$$\n\\hat{\\beta} = \\frac{\\sum_{i=1}^{n} w_{i} x_{i} y_{i}}{\\sum_{i=1}^{n} w_{i} x_{i}^{2}}\n$$\n这个通用形式可以通过选择合适的权重 $w_i$ 来表示所有三个估计量：\n1.  对于 $\\hat{\\beta}_{\\text{OLS}}$，权重是均匀的，$w_{i}=1$ 对所有 $i$ 成立。\n2.  对于 $\\hat{\\beta}_{\\star}$，权重为 $w_{i}^{\\star} = 1/(\\sigma^{2} x_{i}^{2})$。\n3.  对于 $\\tilde{\\beta}$，权重为 $\\tilde{w}_{i} = 1/|x_{i}|$。\n\n令 $X$ 表示观测到的回归量集合 $\\{x_1, \\dots, x_n\\}$。我们在给定 $X$ 的条件下分析这些估计量的性质。\n\n**1. 条件偏倚和方差**\n\n为了求条件偏倚，我们将 $\\hat{\\beta}$ 用真实参数 $\\beta$ 和误差 $\\epsilon_i$ 表示。将 $y_i = \\beta x_i + \\epsilon_i$ 代入 $\\hat{\\beta}$ 的一般公式：\n$$\n\\hat{\\beta} = \\frac{\\sum w_{i} x_{i} (\\beta x_{i} + \\epsilon_{i})}{\\sum w_{i} x_{i}^{2}} = \\frac{\\beta \\sum w_{i} x_{i}^{2} + \\sum w_{i} x_{i} \\epsilon_{i}}{\\sum w_{i} x_{i}^{2}} = \\beta + \\frac{\\sum_{i=1}^{n} w_{i} x_{i} \\epsilon_{i}}{\\sum_{i=1}^{n} w_{i} x_{i}^{2}}\n$$\n给定 $X$，$\\hat{\\beta}$ 的条件期望为：\n$$\nE[\\hat{\\beta} \\,|\\, X] = E\\left[\\beta + \\frac{\\sum w_{i} x_{i} \\epsilon_{i}}{\\sum w_{i} x_{i}^{2}} \\bigg| X\\right] = \\beta + \\frac{\\sum w_{i} x_{i} E[\\epsilon_{i} \\,|\\, X]}{\\sum w_{i} x_{i}^{2}}\n$$\n根据问题陈述，在给定 $X$ 的条件下，误差 $\\epsilon_i$ 相互独立，且 $E[\\epsilon_{i}\\,|\\,x_{i}] = 0$。这意味着 $E[\\epsilon_{i}\\,|\\,X] = E[\\epsilon_{i}\\,|\\,x_i] = 0$。因此，\n$$\nE[\\hat{\\beta} \\,|\\, X] = \\beta\n$$\n条件偏倚 $E[\\hat{\\beta} \\,|\\, X] - \\beta$ 为 $0$。这对任何是 $x_i$ 函数的权重 $w_i$ 都成立。因此，所有三个估计量 $\\hat{\\beta}_{\\text{OLS}}$、$\\hat{\\beta}_{\\star}$ 和 $\\tilde{\\beta}$ 都是条件无偏的。\n\n为了求条件方差，我们计算：\n$$\n\\operatorname{Var}(\\hat{\\beta} \\,|\\, X) = \\operatorname{Var}\\left(\\beta + \\frac{\\sum w_{i} x_{i} \\epsilon_{i}}{\\sum w_{i} x_{i}^{2}} \\bigg| X\\right) = \\frac{1}{\\left(\\sum w_{i} x_{i}^{2}\\right)^2} \\operatorname{Var}\\left(\\sum_{i=1}^{n} w_{i} x_{i} \\epsilon_{i} \\bigg| X\\right)\n$$\n鉴于 $\\epsilon_i$ 是条件独立的，和的方差等于方差的和：\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} w_{i} x_{i} \\epsilon_{i} \\bigg| X\\right) = \\sum_{i=1}^{n} \\operatorname{Var}(w_{i} x_{i} \\epsilon_{i} \\,|\\, X) = \\sum_{i=1}^{n} (w_{i} x_{i})^2 \\operatorname{Var}(\\epsilon_{i} \\,|\\, X)\n$$\n使用给定的异方差结构 $\\operatorname{Var}(\\epsilon_{i}\\,|\\,x_{i}) = \\sigma^{2} x_{i}^{2}$，这等价于 $\\operatorname{Var}(\\epsilon_i \\,|\\, X) = \\sigma^2 x_i^2$：\n$$\n\\operatorname{Var}(\\hat{\\beta} \\,|\\, X) = \\sigma^{2} \\frac{\\sum_{i=1}^{n} w_{i}^{2} x_{i}^{4}}{\\left(\\sum_{i=1}^{n} w_{i} x_{i}^{2}\\right)^2}\n$$\n现在我们将此通用公式应用于每个估计量。\n\n- **OLS 估计量 ($\\hat{\\beta}_{\\text{OLS}}$)**：$w_i = 1$。\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\text{OLS}} \\,|\\, X) = \\sigma^{2} \\frac{\\sum_{i=1}^{n} (1)^2 x_{i}^{4}}{\\left(\\sum_{i=1}^{n} (1) x_{i}^{2}\\right)^2} = \\sigma^{2} \\frac{\\sum_{i=1}^{n} x_{i}^{4}}{\\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^2}\n$$\n\n- **正确加权的 WLS 估计量 ($\\hat{\\beta}_{\\star}$)**：$w_{i}^{\\star} = 1/(\\sigma^{2} x_{i}^{2})$。\n分子项是 $\\sum (w_{i}^{\\star})^2 x_{i}^{4} = \\sum \\left(\\frac{1}{\\sigma^2 x_i^2}\\right)^2 x_i^4 = \\sum \\frac{1}{\\sigma^4 x_i^4} x_i^4 = \\sum \\frac{1}{\\sigma^4} = \\frac{n}{\\sigma^4}$。\n分母项是 $\\sum w_{i}^{\\star} x_{i}^{2} = \\sum \\frac{1}{\\sigma^2 x_i^2} x_i^2 = \\sum \\frac{1}{\\sigma^2} = \\frac{n}{\\sigma^2}$。\n$$\n\\operatorname{Var}(\\hat{\\beta}_{\\star} \\,|\\, X) = \\sigma^{2} \\frac{n/\\sigma^4}{\\left(n/\\sigma^2\\right)^2} = \\sigma^{2} \\frac{n/\\sigma^4}{n^2/\\sigma^4} = \\frac{\\sigma^2}{n}\n$$\n\n- **不正确加权的 WLS 估计量 ($\\tilde{\\beta}$)**：$\\tilde{w}_i = 1/|x_i|$。\n分子项是 $\\sum (\\tilde{w}_{i})^2 x_{i}^{4} = \\sum \\left(\\frac{1}{|x_i|}\\right)^2 x_i^4 = \\sum \\frac{x_i^4}{x_i^2} = \\sum x_i^2$。\n分母项是 $\\sum \\tilde{w}_{i} x_{i}^{2} = \\sum \\frac{1}{|x_i|} x_i^2 = \\sum |x_i|$。\n$$\n\\operatorname{Var}(\\tilde{\\beta} \\,|\\, X) = \\sigma^{2} \\frac{\\sum_{i=1}^{n} x_{i}^{2}}{\\left(\\sum_{i=1}^{n} |x_{i}|\\right)^2}\n$$\n\n**2. 一致性**\n\n如果一个估计量 $\\hat{\\theta}_n$ 在 $n \\to \\infty$ 时依概率收敛于 $\\theta$，则称其对 $\\theta$ 是一致的。一系列无偏估计量具有一致性的一个充分条件是其方差收敛于零。由于所有三个估计量都是无偏的，我们检查它们方差的极限。这些方差是关于 $X$ 的条件方差，因此是随机变量。我们检查它们是否依概率收敛到 $0$。这需要将大数定律 (LLN) 应用于 $x_i \\sim \\mathcal{N}(0,1)$ 的函数的样本均值。我们需要以下期望值：\n- $E[x_i^2] = 1$ (因为 $x_i$ 的方差为 $1$，均值为 $0$）。\n- $E[x_i^4] = 3$ (标准正态分布的四阶矩)。\n- $E[|x_i|] = \\int_{-\\infty}^{\\infty} |z| \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) dz = 2 \\int_{0}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) dz = \\sqrt{\\frac{2}{\\pi}}$。\n\n根据大数定律，当 $n \\to \\infty$ 时：\n$\\frac{1}{n} \\sum x_i^2 \\xrightarrow{p} 1$, $\\frac{1}{n} \\sum x_i^4 \\xrightarrow{p} 3$, 以及 $\\frac{1}{n} \\sum |x_i| \\xrightarrow{p} \\sqrt{2/\\pi}$。\n\n对于 $\\hat{\\beta}_{\\text{OLS}}$，$\\operatorname{Var}(\\hat{\\beta}_{\\text{OLS}} \\,|\\, X) = \\frac{\\sigma^2}{n} \\frac{\\frac{1}{n}\\sum x_i^4}{(\\frac{1}{n}\\sum x_i^2)^2} \\xrightarrow{p} \\frac{\\sigma^2}{n} \\frac{3}{1^2} = \\frac{3\\sigma^2}{n} \\to 0$。\n对于 $\\hat{\\beta}_{\\star}$，$\\operatorname{Var}(\\hat{\\beta}_{\\star} \\,|\\, X) = \\frac{\\sigma^2}{n} \\to 0$。\n对于 $\\tilde{\\beta}$，$\\operatorname{Var}(\\tilde{\\beta} \\,|\\, X) = \\frac{\\sigma^2}{n} \\frac{\\frac{1}{n}\\sum x_i^2}{(\\frac{1}{n}\\sum |x_i|)^2} \\xrightarrow{p} \\frac{\\sigma^2}{n} \\frac{1}{(\\sqrt{2/\\pi})^2} = \\frac{\\pi\\sigma^2}{2n} \\to 0$。\n\n由于所有三个估计量的条件方差在 $n \\to \\infty$ 时都收敛于 $0$，所以这三个估计量（$\\hat{\\beta}_{\\text{OLS}}$、$\\hat{\\beta}_{\\star}$、$\\tilde{\\beta}$）对于 $\\beta$ 都是一致的。\n\n**3. 大样本方差比**\n\n我们计算当 $n \\to \\infty$ 时条件方差之比的极限。这些极限被解释为依概率收敛的极限。\n\n- **比率 1**：\n$$\n\\frac{\\operatorname{Var}(\\hat{\\beta}_{\\text{OLS}}\\,|\\,X)}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,X)} = \\frac{\\sigma^{2} \\frac{\\sum x_{i}^{4}}{(\\sum x_{i}^{2})^2}}{\\sigma^2/n} = \\frac{n \\sum x_{i}^{4}}{(\\sum x_{i}^{2})^2} = \\frac{n(n \\cdot \\frac{1}{n}\\sum x_i^4)}{(n \\cdot \\frac{1}{n}\\sum x_i^2)^2} = \\frac{\\frac{1}{n}\\sum x_i^4}{(\\frac{1}{n}\\sum x_i^2)^2}\n$$\n当 $n \\to \\infty$ 时取极限，并应用大数定律和连续映射定理：\n$$\n\\lim_{n \\to \\infty} \\frac{\\frac{1}{n}\\sum x_i^4}{(\\frac{1}{n}\\sum x_i^2)^2} \\xrightarrow{p} \\frac{E[x_i^4]}{(E[x_i^2])^2} = \\frac{3}{1^2} = 3\n$$\n\n- **比率 2**：\n$$\n\\frac{\\operatorname{Var}(\\tilde{\\beta}\\,|\\,X)}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,X)} = \\frac{\\sigma^{2} \\frac{\\sum x_{i}^{2}}{(\\sum |x_{i}|)^2}}{\\sigma^2/n} = \\frac{n \\sum x_{i}^{2}}{(\\sum |x_{i}|)^2} = \\frac{n(n \\cdot \\frac{1}{n}\\sum x_i^2)}{(n \\cdot \\frac{1}{n}\\sum |x_i|)^2} = \\frac{\\frac{1}{n}\\sum x_i^2}{(\\frac{1}{n}\\sum |x_i|)^2}\n$$\n当 $n \\to \\infty$ 时取极限：\n$$\n\\lim_{n \\to \\infty} \\frac{\\frac{1}{n}\\sum x_i^2}{(\\frac{1}{n}\\sum |x_i|)^2} \\xrightarrow{p} \\frac{E[x_i^2]}{(E[|x_i|])^2} = \\frac{1}{(\\sqrt{2/\\pi})^2} = \\frac{1}{2/\\pi} = \\frac{\\pi}{2}\n$$\n\n- **比率 3**：\n$$\n\\frac{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,X)}{\\operatorname{Var}(\\hat{\\beta}_{\\star}\\,|\\,X)} = 1\n$$\n一个常数的极限是其本身，所以极限是 $1$。\n\n按所要求的顺序，这三个极限是 $3$、$\\pi/2$ 和 $1$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 3  \\frac{\\pi}{2}  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "现在，我们将偏差-方差权衡应用到线性回归这一经典场景中，特别是在面对多重共线性问题时。当预测变量之间存在高度相关性时，普通最小二乘（OLS）估计的方差会急剧膨胀，使其变得极不稳定。本练习  将通过一个精心设计的数值实验，让你量化比较岭回归（Ridge Regression）与OLS的偏差和方差，直观地看到前者如何通过引入偏差来有效控制由共线性引起的方差问题。",
            "id": "3118692",
            "problem": "考虑一个具有两个预测变量的固定设计线性回归模型，其中响应向量根据 $y = X \\beta + \\varepsilon$ 生成。这里 $X \\in \\mathbb{R}^{n \\times 2}$ 是一个非随机设计矩阵，$\\beta \\in \\mathbb{R}^{2}$ 是真实系数向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是一个噪声向量，其分量独立且满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。定义普通最小二乘 (OLS) 估计量和岭估计量如下：\n$$\\hat{\\beta}_{\\mathrm{OLS}} = (X^{\\top} X)^{-1} X^{\\top} y,$$\n$$\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda) = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} y,$$\n其中 $\\lambda \\ge 0$ 是岭惩罚参数，$I_{2}$ 是 $2 \\times 2$ 单位矩阵。使用以下基本定义和事实：\n- 参数 $\\theta$ 的估计量 $\\hat{\\theta}$ 的偏差为 $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$。\n- 在 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 条件下，线性估计量 $A y$ 的方差为 $\\mathrm{Var}(A y) = \\sigma^{2} A A^{\\top}$。\n- 对于 OLS 估计量，$\\mathbb{E}[\\hat{\\beta}_{\\mathrm{OLS}}] = \\beta$ 且 $\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}) = \\sigma^{2} (X^{\\top} X)^{-1}$。\n- 对于岭估计量，$\\mathbb{E}[\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)] = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X \\beta$ 且 $\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)) = \\sigma^{2} (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{2})^{-1}$。\n\n您的任务是构建一个确定性的共线性设计族 $X$，以分离共线性对估计量方差的影响，然后量化 OLS 与岭估计的偏差和方差。具体而言：\n\n1. 构造 $X$，使得格拉姆矩阵 (Gram matrix) $G = X^{\\top} X$ 的特征分解为 $G = V \\mathrm{diag}(s_{1}, s_{2}) V^{\\top}$，其中标准正交特征向量为 $v_{1} = \\frac{1}{\\sqrt{2}}(1, 1)^{\\top}$ 和 $v_{2} = \\frac{1}{\\sqrt{2}}(1, -1)^{\\top}$，特征值为 $s_{1} = n(1+\\rho)$ 和 $s_{2} = n(1-\\rho)$，$\\rho \\in [0, 1)$ 是一个指定的相​​关水平。当 $\\rho$ 接近 1 时，这确保了共线性。使用偶数 $n$，并通过 $\\mathbb{R}^{n}$ 中的标准正交列来构造 $X$，使得 $X^{\\top} X$ 精确匹配指定的特征结构。取真实系数向量 $\\beta = (1, 1)^{\\top}$ 和噪声方差 $\\sigma^{2} = 1$。\n\n2. 对于给定的 $(n, \\rho, \\lambda)$ 三元组，计算：\n   - 岭偏差向量的欧几里得范数平方，$\\|\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\|_{2}^{2}$，其中\n     $$\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)) = \\left((X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X - I_{2}\\right)\\beta.$$\n   - 岭方差的迹与 OLS 方差的迹之比：\n     $$R_{\\mathrm{var}}(X, \\lambda) = \\frac{\\mathrm{tr}\\left(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\right)}{\\mathrm{tr}\\left(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}})\\right)} = \\frac{\\mathrm{tr}\\left(\\sigma^{2} (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{2})^{-1}\\right)}{\\mathrm{tr}\\left(\\sigma^{2} (X^{\\top} X)^{-1}\\right)}.$$\n\n3. 设计并评估以下 $(n, \\rho, \\lambda)$ 测试套件以探究不同情况：\n   - 情况 A (高共线性，方差减小且偏差接近于零): $n = 1000$, $\\rho = 0.999$, $\\lambda = 1$。\n   - 情况 B (边界情况，无正则化): $n = 1000$, $\\rho = 0.999$, $\\lambda = 0$。\n   - 情况 C (中度共线性): $n = 500$, $\\rho = 0.9$, $\\lambda = 1$。\n   - 情况 D (低共线性): $n = 500$, $\\rho = 0.0$, $\\lambda = 1$。\n\n4. 您的程序必须是一个单一、完整的脚本，该脚本：\n   - 为每个测试用例完全按照第 1 项中的规定构造 $X$。\n   - 为每个测试用例计算第 2 项中的量。\n   - 生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，按 $[\\|\\mathrm{Bias}\\|^{2}_{\\mathrm{A}}, R_{\\mathrm{var},\\mathrm{A}}, \\|\\mathrm{Bias}\\|^{2}_{\\mathrm{B}}, R_{\\mathrm{var},\\mathrm{B}}, \\|\\mathrm{Bias}\\|^{2}_{\\mathrm{C}}, R_{\\mathrm{var},\\mathrm{C}}, \\|\\mathrm{Bias}\\|^{2}_{\\mathrm{D}}, R_{\\mathrm{var},\\mathrm{D}}]$ 的顺序排列。\n\n所有量都是没有物理单位的纯数。不涉及角度。确保所有矩阵计算均使用指定的公式进行。输出必须是可复现的，并且除了上述 $X$ 的确定性构造外，不得依赖任何外部随机性。",
            "solution": "该问题是适定的、有科学依据且内部一致的。它提供了一套完整的定义、参数和约束，以推导出一个唯一的、确定性的解。所有待计算的量都可以表示为格拉姆矩阵 $G = X^{\\top} X$ 的函数，该矩阵的完整特征分解已给出。“构造 $X$”的指令是一个概念上的工具，以确保这样的矩阵存在，其存在性由奇异值分解定理保证。然而，由于最终的量仅依赖于 $G=X^\\top X$，我们可以直接使用其指定的属性进行计算，从而绕过对 $n \\times 2$ 矩阵 $X$ 的显式构造。\n\n我们的步骤如下：\n1.  使用给定的 $G$ 的特征分解，推导岭偏差向量的欧几里得范数平方 $\\|\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\|_{2}^{2}$ 的解析公式。\n2.  使用相同的特征分解，推导方差比 $R_{\\mathrm{var}}(X, \\lambda)$ 的解析公式。\n3.  将这些公式应用于四个指定的测试用例。\n\n格拉姆矩阵定义为 $G = X^{\\top} X = V \\Lambda V^{\\top}$，其中 $\\Lambda = \\mathrm{diag}(s_{1}, s_{2})$ 是特征值的对角矩阵，$V = [v_1, v_2]$ 是相应特征向量的正交矩阵。\n特征值由 $s_{1} = n(1+\\rho)$ 和 $s_{2} = n(1-\\rho)$ 给出。\n特征向量为 $v_{1} = \\frac{1}{\\sqrt{2}}(1, 1)^{\\top}$ 和 $v_{2} = \\frac{1}{\\sqrt{2}}(1, -1)^{\\top}$，构成矩阵 $V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}$。\n真实系数向量是 $\\beta = (1, 1)^{\\top}$，噪声方差是 $\\sigma^2=1$。\n\n**1. 偏差范数平方的推导**\n\n岭估计量的偏差由下式给出：\n$$ \\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)) = \\mathbb{E}[\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda)] - \\beta = \\left( (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X - I_{2} \\right) \\beta $$\n代入 $G = X^{\\top} X = V \\Lambda V^{\\top}$ 和 $I_{2} = V V^{\\top}$：\n$$ \\mathrm{Bias}(\\cdot) = \\left( (V \\Lambda V^{\\top} + \\lambda V V^{\\top})^{-1} V \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta $$\n$$ = \\left( (V (\\Lambda + \\lambda I_{2}) V^{\\top})^{-1} V \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta $$\n$$ = \\left( V (\\Lambda + \\lambda I_{2})^{-1} V^{\\top} V \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta $$\n使用 $V^{\\top}V = I_{2}$：\n$$ = \\left( V (\\Lambda + \\lambda I_{2})^{-1} \\Lambda V^{\\top} - V V^{\\top} \\right) \\beta = V \\left( (\\Lambda + \\lambda I_{2})^{-1} \\Lambda - I_{2} \\right) V^{\\top} \\beta $$\n中心矩阵项简化为：\n$$ (\\Lambda + \\lambda I_{2})^{-1} \\Lambda - I_{2} = \\begin{pmatrix} \\frac{s_1}{s_1+\\lambda}  0 \\\\ 0  \\frac{s_2}{s_2+\\lambda} \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{\\lambda}{s_1+\\lambda}  0 \\\\ 0  -\\frac{\\lambda}{s_2+\\lambda} \\end{pmatrix} = -\\lambda(\\Lambda+\\lambda I_{2})^{-1} $$\n所以，偏差向量为 $\\mathrm{Bias}(\\cdot) = -\\lambda V (\\Lambda+\\lambda I_{2})^{-1} V^{\\top} \\beta$。\n为了求其范数平方 $\\|\\mathrm{Bias}(\\cdot)\\|_{2}^{2}$，我们首先将 $\\beta$ 变换到特征向量基：\n$$ V^{\\top}\\beta = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix} $$\n这表明 $\\beta$ 与第一个特征向量 $v_1$ 对齐，因为 $\\beta = \\sqrt{2}v_1$。\n由于 $V$ 是一个正交矩阵，它保持欧几里得范数不变。因此，我们有：\n$$ \\|\\mathrm{Bias}(\\cdot)\\|_{2}^{2} = \\|-\\lambda V (\\Lambda+\\lambda I_{2})^{-1} V^{\\top} \\beta\\|_{2}^{2} = \\|-\\lambda (\\Lambda+\\lambda I_{2})^{-1} V^{\\top} \\beta\\|_{2}^{2} $$\n$$ = \\left\\| -\\lambda \\begin{pmatrix} \\frac{1}{s_1+\\lambda}  0 \\\\ 0  \\frac{1}{s_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} \\\\ 0 \\end{pmatrix} \\right\\|_{2}^{2} = \\left\\| \\begin{pmatrix} -\\frac{\\lambda\\sqrt{2}}{s_1+\\lambda} \\\\ 0 \\end{pmatrix} \\right\\|_{2}^{2} = \\left(-\\frac{\\lambda\\sqrt{2}}{s_1+\\lambda}\\right)^2 $$\n这得出了偏差范数平方的最终公式：\n$$ \\|\\mathrm{Bias}(\\hat{\\beta}_{\\mathrm{ridge}}(\\lambda))\\|_{2}^{2} = \\frac{2\\lambda^2}{(s_1+\\lambda)^2} $$\n\n**2. 方差比的推导**\n\n方差比为 $R_{\\mathrm{var}} = \\frac{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}}))}{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}))}$。给定 $\\sigma^2=1$，我们分别分析分子和分母。\n\n分母，OLS 方差的迹：\n$$ \\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}})) = \\mathrm{tr}(\\sigma^2 (X^{\\top}X)^{-1}) = \\mathrm{tr}(G^{-1}) = \\mathrm{tr}((V\\Lambda V^\\top)^{-1}) = \\mathrm{tr}(V\\Lambda^{-1}V^\\top) $$\n使用迹的循环特性 $\\mathrm{tr}(ABC) = \\mathrm{tr}(CAB)$：\n$$ \\mathrm{tr}(V\\Lambda^{-1}V^\\top) = \\mathrm{tr}(V^\\top V\\Lambda^{-1}) = \\mathrm{tr}(\\Lambda^{-1}) = \\frac{1}{s_1} + \\frac{1}{s_2} $$\n\n分子，岭方差的迹：\n$$ \\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{ridge}})) = \\mathrm{tr}(\\sigma^2 (G+\\lambda I)^{-1} G (G+\\lambda I)^{-1}) $$\n$$ = \\mathrm{tr}( (V(\\Lambda+\\lambda I)V^\\top)^{-1} (V\\Lambda V^\\top) (V(\\Lambda+\\lambda I)V^\\top)^{-1} ) $$\n$$ = \\mathrm{tr}( V(\\Lambda+\\lambda I)^{-1} V^{\\top} V\\Lambda V^{\\top} V(\\Lambda+\\lambda I)^{-1} V^{\\top} ) $$\n$$ = \\mathrm{tr}( V (\\Lambda+\\lambda I)^{-1} \\Lambda (\\Lambda+\\lambda I)^{-1} V^{\\top} ) $$\n再次使用迹的循环特性：\n$$ = \\mathrm{tr}( (\\Lambda+\\lambda I)^{-1} \\Lambda (\\Lambda+\\lambda I)^{-1} ) = \\mathrm{tr}\\left( \\begin{pmatrix} \\frac{s_1}{(s_1+\\lambda)^2}  0 \\\\ 0  \\frac{s_2}{(s_2+\\lambda)^2} \\end{pmatrix} \\right) = \\frac{s_1}{(s_1+\\lambda)^2} + \\frac{s_2}{(s_2+\\lambda)^2} $$\n\n结合分子和分母，得到方差比的表达式：\n$$ R_{\\mathrm{var}}(X, \\lambda) = \\frac{\\frac{s_1}{(s_1+\\lambda)^2} + \\frac{s_2}{(s_2+\\lambda)^2}}{\\frac{1}{s_1} + \\frac{1}{s_2}} $$\n\n**3. 测试用例的计算**\n\n我们现在将这些公式应用于给定的测试用例，代入 $s_1 = n(1+\\rho)$ 和 $s_2 = n(1-\\rho)$。\n\n**情况 A**: $(n, \\rho, \\lambda) = (1000, 0.999, 1)$。\n$s_1 = 1000(1.999) = 1999$, $s_2 = 1000(0.001) = 1$。\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(1^2)}{(1999+1)^2} = \\frac{2}{2000^2} = 5 \\times 10^{-7}$。\n$R_{\\mathrm{var}} = \\frac{1999/(1999+1)^2 + 1/(1+1)^2}{1/1999 + 1/1} = \\frac{1999/4000000 + 1/4}{2000/1999} \\approx 0.25037$。\n\n**情况 B**: $(n, \\rho, \\lambda) = (1000, 0.999, 0)$。\n这对应于 OLS 估计量。\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(0^2)}{(1999+0)^2} = 0$，因为 OLS 是无偏的。\n$R_{\\mathrm{var}} = \\frac{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}))}{\\mathrm{tr}(\\mathrm{Var}(\\hat{\\beta}_{\\mathrm{OLS}}))} = 1$。\n\n**情况 C**: $(n, \\rho, \\lambda) = (500, 0.9, 1)$。\n$s_1 = 500(1.9) = 950$, $s_2 = 500(0.1) = 50$。\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(1^2)}{(950+1)^2} = \\frac{2}{951^2} \\approx 2.211 \\times 10^{-6}$。\n$R_{\\mathrm{var}} = \\frac{950/(950+1)^2 + 50/(50+1)^2}{1/950 + 1/50} \\approx 0.9630$。\n\n**情况 D**: $(n, \\rho, \\lambda) = (500, 0.0, 1)$。\n$s_1 = 500(1) = 500$, $s_2 = 500(1) = 500$。\n$\\|\\mathrm{Bias}\\|^2 = \\frac{2(1^2)}{(500+1)^2} = \\frac{2}{501^2} \\approx 7.968 \\times 10^{-6}$。\n$R_{\\mathrm{var}} = \\frac{500/(500+1)^2 + 500/(500+1)^2}{1/500 + 1/500} = \\frac{2 \\cdot 500/501^2}{2/500} = \\frac{500^2}{501^2} \\approx 0.9960$。\n\n实现将编码这些推导出的解析公式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the squared bias norm and variance ratio for OLS vs. Ridge estimators\n    under different scenarios of collinearity.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, rho, lambda), description\n        (1000, 0.999, 1), # Case A: high collinearity, variance reduction\n        (1000, 0.999, 0), # Case B: boundary case, no regularization (OLS)\n        (500, 0.9, 1),    # Case C: moderate collinearity\n        (500, 0.0, 1),    # Case D: low collinearity (orthogonal design)\n    ]\n\n    results = []\n    for n, rho, lmbda in test_cases:\n        # The problem is structured such that all calculations depend only on the\n        # eigenvalues of the Gram matrix G = X'X, not on X itself.\n        # We calculate these eigenvalues directly.\n        s1 = float(n) * (1.0 + rho)\n        s2 = float(n) * (1.0 - rho)\n\n        # 1. Compute the squared Euclidean norm of the ridge bias vector.\n        # The analytical formula derived in the solution is:\n        # ||Bias||^2 = 2 * lambda^2 / (s1 + lambda)^2\n        # This simplification arises because the true beta vector is aligned with\n        # the first principal component of the design.\n        sq_bias_norm = 2.0 * lmbda**2 / (s1 + lmbda)**2\n\n        # 2. Compute the ratio of the trace of the ridge variance to the trace of OLS variance.\n        # The analytical formula derived in the solution is:\n        # R_var = (s1/(s1+l)^2 + s2/(s2+l)^2) / (1/s1 + 1/s2)\n        if lmbda == 0:\n            # For OLS (lambda=0), the ridge estimator is the OLS estimator,\n            # so the ratio of their variance traces is exactly 1.\n            r_var = 1.0\n        else:\n            # Denominator: Trace of OLS variance (scaled by sigma^2 = 1)\n            # The problem constraint rho in [0, 1) ensures s1 > 0 and s2 >= 0.\n            # If rho is close to 1, s2 is close to 0, which is the source of high variance.\n            # If rho  1, s2 > 0, so ols_var_trace is finite.\n            ols_var_trace = (1.0 / s1) + (1.0 / s2)\n\n            # Numerator: Trace of Ridge variance (scaled by sigma^2 = 1)\n            ridge_var_trace = s1 / (s1 + lmbda)**2 + s2 / (s2 + lmbda)**2\n\n            r_var = ridge_var_trace / ols_var_trace\n                \n        results.append(sq_bias_norm)\n        results.append(r_var)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}