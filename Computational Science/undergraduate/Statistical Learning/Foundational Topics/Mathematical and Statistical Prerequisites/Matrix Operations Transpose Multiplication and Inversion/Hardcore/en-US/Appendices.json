{
    "hands_on_practices": [
        {
            "introduction": "In statistical learning, the Gram matrix $X^\\top X$ is the cornerstone of linear regression, appearing at the heart of the normal equations. This exercise provides a foundational look into its structure, particularly how it changes when we add an intercept term to our model. By deconstructing the Gram matrix into block form, you will gain a deeper appreciation for the interplay between the intercept and feature coefficients, and discover the algebraic reason why centering your data can simplify calculations and interpretations .",
            "id": "3146970",
            "problem": "In a linear model used in statistical learning, it is common to include an intercept term by augmenting the feature matrix with a column of ones. Consider a dataset with $n=4$ observations and $p=2$ features. The feature matrix is\n$$\nX=\\begin{pmatrix}\n0  1\\\\\n1  1\\\\\n2  3\\\\\n3  4\n\\end{pmatrix},\n$$\nand the intercept column is $\\mathbf{1}=\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix}$. Define the augmented design matrix $\\tilde{X}=\\begin{pmatrix}\\mathbf{1}  X\\end{pmatrix}$ and the Gram matrix $\\tilde{G}=\\tilde{X}^{\\top}\\tilde{X}$.\n\nUsing only the definitions of matrix transpose, multiplication, and inverse, do the following:\n- Express $\\tilde{G}$ in block form by partitioning conformably with the intercept and the features. Identify each block in terms of $n$, the column sums of $X$, and $X^{\\top}X$, starting from the definition of matrix multiplication.\n- Explain, based on your block expression, why centering the columns of $X$ (i.e., making each feature have sample mean zero) affects the off-diagonal blocks and can decouple the intercept from the slopes.\n- Derive, by solving a partitioned linear system constructed from $\\tilde{G}$ and without quoting any pre-memorized inversion formulas, a closed-form expression for the $(1,1)$ entry of $\\tilde{G}^{-1}$ in terms of $n$, $X^{\\top}X$, and the column sums of $X$.\n- For the specific $X$ given above, compute the exact value of the $(1,1)$ entry of $\\tilde{G}^{-1}$.\n\nProvide the final answer as a single exact number. No rounding is required, and no units are involved. The final answer must be only the value of the $(1,1)$ entry of $\\tilde{G}^{-1}$.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We can proceed with the derivation and calculation.\n\nThe augmented design matrix $\\tilde{X}$ is formed by prepending a column of ones, $\\mathbf{1}$, to the feature matrix $X$. Given that $\\mathbf{1}$ is an $n \\times 1$ matrix and $X$ is an $n \\times p$ matrix, we can write $\\tilde{X}$ in block form as $\\tilde{X} = \\begin{pmatrix} \\mathbf{1}  X \\end{pmatrix}$. For this problem, $n=4$ and $p=2$.\n\n**Part 1: Block form of the Gram matrix $\\tilde{G}$**\n\nThe Gram matrix $\\tilde{G}$ is defined as $\\tilde{G} = \\tilde{X}^{\\top}\\tilde{X}$. To express this in block form, we first find the transpose of $\\tilde{X}$:\n$$\n\\tilde{X}^{\\top} = \\begin{pmatrix} \\mathbf{1}  X \\end{pmatrix}^{\\top} = \\begin{pmatrix} \\mathbf{1}^{\\top} \\\\ X^{\\top} \\end{pmatrix}\n$$\nHere, $\\mathbf{1}^{\\top}$ is a $1 \\times n$ row vector of ones, and $X^{\\top}$ is the $p \\times n$ transpose of the feature matrix.\n\nNow, we perform block matrix multiplication to find $\\tilde{G}$:\n$$\n\\tilde{G} = \\tilde{X}^{\\top}\\tilde{X} = \\begin{pmatrix} \\mathbf{1}^{\\top} \\\\ X^{\\top} \\end{pmatrix} \\begin{pmatrix} \\mathbf{1}  X \\end{pmatrix} = \\begin{pmatrix} \\mathbf{1}^{\\top}\\mathbf{1}  \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1}  X^{\\top}X \\end{pmatrix}\n$$\nWe identify each block:\n- The top-left block is $\\mathbf{1}^{\\top}\\mathbf{1} = \\sum_{i=1}^{n} 1 \\cdot 1 = n$. This is a $1 \\times 1$ block (a scalar).\n- The top-right block is $\\mathbf{1}^{\\top}X$. This is a $1 \\times p$ row vector where the $j$-th element is the dot product of $\\mathbf{1}$ with the $j$-th column of $X$, which is $\\sum_{i=1}^{n} x_{ij}$. Thus, $\\mathbf{1}^{\\top}X$ is the row vector of column sums of $X$.\n- The bottom-left block is $X^{\\top}\\mathbf{1}$. This is a $p \\times 1$ column vector. It is the transpose of $\\mathbf{1}^{\\top}X$: $( \\mathbf{1}^{\\top}X )^{\\top} = X^{\\top}(\\mathbf{1}^{\\top})^{\\top} = X^{\\top}\\mathbf{1}$. So, it is the column vector of column sums of $X$.\n- The bottom-right block is $X^{\\top}X$, which is the $p \\times p$ Gram matrix of the original features.\n\nTherefore, the block form of $\\tilde{G}$ is:\n$$\n\\tilde{G} = \\begin{pmatrix} n  \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1}  X^{\\top}X \\end{pmatrix}\n$$\n\n**Part 2: Effect of Centering Features**\n\nCentering the columns of $X$ means replacing each feature column $\\mathbf{x}_j$ with a new column $\\mathbf{x}'_j = \\mathbf{x}_j - \\bar{x}_j\\mathbf{1}$, where $\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^{n} x_{ij}$ is the sample mean of the $j$-th feature. Let the centered matrix be denoted $X_c$.\n\nThe crucial effect of this transformation is on the sum of the elements in each new column. The sum of the elements in the $j$-th centered column is:\n$$\n\\mathbf{1}^{\\top}\\mathbf{x}'_j = \\mathbf{1}^{\\top}(\\mathbf{x}_j - \\bar{x}_j\\mathbf{1}) = \\mathbf{1}^{\\top}\\mathbf{x}_j - \\bar{x}_j(\\mathbf{1}^{\\top}\\mathbf{1}) = \\left(\\sum_{i=1}^{n} x_{ij}\\right) - \\bar{x}_j \\cdot n\n$$\nBy definition of the mean $\\bar{x}_j$, we have $\\sum_{i=1}^{n} x_{ij} = n\\bar{x}_j$. Substituting this in, we get:\n$$\n\\mathbf{1}^{\\top}\\mathbf{x}'_j = n\\bar{x}_j - n\\bar{x}_j = 0\n$$\nThis means that for the centered matrix $X_c$, the off-diagonal blocks of the corresponding Gram matrix $\\tilde{G}_c$ become zero matrices. The top-right block $\\mathbf{1}^{\\top}X_c$ is a $1 \\times p$ zero vector, and the bottom-left block $X_c^{\\top}\\mathbf{1}$ is a $p \\times 1$ zero vector.\n\nThe Gram matrix for the centered data thus becomes block-diagonal:\n$$\n\\tilde{G}_c = \\begin{pmatrix} n  \\mathbf{0}_{1 \\times p} \\\\ \\mathbf{0}_{p \\times 1}  X_c^{\\top}X_c \\end{pmatrix}\n$$\nThe inverse of a block-diagonal matrix is the block-diagonal matrix of the inverses of the blocks:\n$$\n\\tilde{G}_c^{-1} = \\begin{pmatrix} n^{-1}  \\mathbf{0}_{1 \\times p} \\\\ \\mathbf{0}_{p \\times 1}  (X_c^{\\top}X_c)^{-1} \\end{pmatrix}\n$$\nIn linear regression, the vector of estimated coefficients (intercept and slopes) is given by $\\hat{\\boldsymbol{\\beta}} = (\\tilde{X}^{\\top}\\tilde{X})^{-1}\\tilde{X}^{\\top}\\mathbf{y}$. The block-diagonal structure of $\\tilde{G}_c^{-1}$ implies that the estimation of the intercept term becomes decoupled from the estimation of the slope coefficients. The intercept estimate depends only on the top-left block, while the slope estimates depend only on the bottom-right block. This simplifies both the computation and the interpretation of the model coefficients.\n\n**Part 3: Derivation of the $(1,1)$ entry of $\\tilde{G}^{-1}$**\n\nWe wish to find the entry in the first row and first column of $\\tilde{G}^{-1}$. This entry is the first component of the first column of $\\tilde{G}^{-1}$. Let the first column of $\\tilde{G}^{-1}$ be denoted by the vector $\\mathbf{m}_1$. By definition of the matrix inverse, this column is the solution to the linear system $\\tilde{G}\\mathbf{m}_1 = \\mathbf{e}_1$, where $\\mathbf{e}_1$ is the first standard basis vector, i.e., $\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}$.\n\nWe partition the vector $\\mathbf{m}_1$ conformably with the block structure of $\\tilde{G}$. Since $\\tilde{G}$ is a $(p+1) \\times (p+1)$ matrix, $\\mathbf{m}_1$ is a $(p+1) \\times 1$ vector. We partition it as $\\mathbf{m}_1 = \\begin{pmatrix} m_{11} \\\\ \\mathbf{g} \\end{pmatrix}$, where $m_{11}$ is a scalar (the $(1,1)$ entry we are looking for) and $\\mathbf{g}$ is a $p \\times 1$ vector. The vector $\\mathbf{e}_1$ is partitioned as $\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ \\mathbf{0}_{p \\times 1} \\end{pmatrix}$.\n\nThe linear system in block form is:\n$$\n\\begin{pmatrix} n  \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1}  X^{\\top}X \\end{pmatrix} \\begin{pmatrix} m_{11} \\\\ \\mathbf{g} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\mathbf{0}_{p \\times 1} \\end{pmatrix}\n$$\nThis expands into two equations:\n1. $n \\cdot m_{11} + (\\mathbf{1}^{\\top}X)\\mathbf{g} = 1$\n2. $(X^{\\top}\\mathbf{1})m_{11} + (X^{\\top}X)\\mathbf{g} = \\mathbf{0}_{p \\times 1}$\n\nFrom the second equation, we solve for $\\mathbf{g}$. Assuming $X^{\\top}X$ is invertible (which is true if $X$ has full column rank):\n$$\n(X^{\\top}X)\\mathbf{g} = -(X^{\\top}\\mathbf{1})m_{11} \\implies \\mathbf{g} = -(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})m_{11}\n$$\nNow, we substitute this expression for $\\mathbf{g}$ into the first equation:\n$$\nn \\cdot m_{11} + (\\mathbf{1}^{\\top}X) \\left( -(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})m_{11} \\right) = 1\n$$\nSince $m_{11}$ is a scalar, we can factor it out:\n$$\nm_{11} \\left( n - (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) \\right) = 1\n$$\nSolving for $m_{11}$, which is the $(1,1)$ entry of $\\tilde{G}^{-1}$, we obtain the closed-form expression:\n$$\n(\\tilde{G}^{-1})_{11} = m_{11} = \\left( n - (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) \\right)^{-1}\n$$\n\n**Part 4: Computation for the specific matrix $X$**\n\nWe are given $n=4$ and $X=\\begin{pmatrix} 0  1\\\\ 1  1\\\\ 2  3\\\\ 3  4 \\end{pmatrix}$. We compute the necessary components for the formula derived above.\n\n- The vector of column sums of $X$ is:\n  $\\mathbf{1}^{\\top}X = \\begin{pmatrix} 0+1+2+3  1+1+3+4 \\end{pmatrix} = \\begin{pmatrix} 6  9 \\end{pmatrix}$.\n- The transpose is $X^{\\top}\\mathbf{1} = \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix}$.\n\n- The matrix $X^{\\top}X$ is:\n  $X^{\\top}X = \\begin{pmatrix} 0  1  2  3 \\\\ 1  1  3  4 \\end{pmatrix} \\begin{pmatrix} 0  1\\\\ 1  1\\\\ 2  3\\\\ 3  4 \\end{pmatrix} = \\begin{pmatrix} 0+1+4+9  0+1+6+12 \\\\ 0+1+6+12  1+1+9+16 \\end{pmatrix} = \\begin{pmatrix} 14  19 \\\\ 19  27 \\end{pmatrix}$.\n\n- We need the inverse of $X^{\\top}X$. For a $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\n  The determinant of $X^{\\top}X$ is $\\det(X^{\\top}X) = (14)(27) - (19)(19) = 378 - 361 = 17$.\n  The inverse is $(X^{\\top}X)^{-1} = \\frac{1}{17}\\begin{pmatrix} 27  -19 \\\\ -19  14 \\end{pmatrix}$.\n\n- Now we compute the quadratic form $(\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})$:\n  $$\n  (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) = \\begin{pmatrix} 6  9 \\end{pmatrix} \\left( \\frac{1}{17}\\begin{pmatrix} 27  -19 \\\\ -19  14 \\end{pmatrix} \\right) \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix}\n  $$\n  First, multiply the row vector by the matrix:\n  $$\n  \\frac{1}{17} \\begin{pmatrix} 6  9 \\end{pmatrix} \\begin{pmatrix} 27  -19 \\\\ -19  14 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 6(27)+9(-19)  6(-19)+9(14) \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 162-171  -114+126 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} -9  12 \\end{pmatrix}\n  $$\n  Now, multiply by the column vector:\n  $$\n  \\frac{1}{17} \\begin{pmatrix} -9  12 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix} = \\frac{1}{17}(-9(6) + 12(9)) = \\frac{1}{17}(-54 + 108) = \\frac{54}{17}\n  $$\n\n- Finally, we compute the $(1,1)$ entry of $\\tilde{G}^{-1}$:\n  $$\n  (\\tilde{G}^{-1})_{11} = \\left( n - \\frac{54}{17} \\right)^{-1} = \\left( 4 - \\frac{54}{17} \\right)^{-1}\n  $$\n  $$\n  4 - \\frac{54}{17} = \\frac{4 \\times 17}{17} - \\frac{54}{17} = \\frac{68 - 54}{17} = \\frac{14}{17}\n  $$\n  Therefore,\n  $$\n  (\\tilde{G}^{-1})_{11} = \\left(\\frac{14}{17}\\right)^{-1} = \\frac{17}{14}\n  $$",
            "answer": "$$\n\\boxed{\\frac{17}{14}}\n$$"
        },
        {
            "introduction": "Beyond just fitting a model, it's crucial to understand how individual data points influence the outcome. The \"hat matrix,\" $H = X(X^\\top X)^{-1}X^\\top$, provides exactly this insight, with its diagonal elements, known as leverage scores, quantifying the influence of each observation. This practice will guide you through deriving the formula for these leverage scores and calculating one for a polynomial regression model, bridging the abstract concept of a projection matrix to the practical task of identifying high-leverage points in a dataset .",
            "id": "3146914",
            "problem": "Consider ordinary least squares (OLS) polynomial regression of degree $2$ in the statistical learning framework. You are given inputs $x_{1}, x_{2}, x_{3}, x_{4}$ and form the Vandermonde design matrix $V$ whose $i$-th row is $[1, x_{i}, x_{i}^{2}]$. The fitted values are linear in the observed responses, defining a linear operator known as the hat matrix, whose diagonal entries $h_{ii}$ quantify the leverage of observation $i$.\n\nStarting only from the normal equations $V^{\\top}V\\,\\hat{\\beta} = V^{\\top}y$ and the fact that OLS produces the orthogonal projection of $y$ onto the column space of $V$, derive an analytic expression for the diagonal hat value $h_{ii}$ in terms of the $i$-th row of $V$ and the matrix $(V^{\\top}V)^{-1}$. Then, for the specific input configuration $x_{1} = -1$, $x_{2} = 0$, $x_{3} = 1$, $x_{4} = 2$, compute the exact value of $h_{22}$ for the quadratic Vandermonde matrix described above. Express your final answer as an exact rational number.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of linear algebra and statistical regression, well-posed with sufficient information for a unique solution, and expressed in objective, formal language. There are no contradictions, ambiguities, or pseudo-scientific claims. We may proceed with the solution.\n\nThe problem consists of two parts: first, to derive a general expression for a diagonal element of the hat matrix, and second, to compute a specific value for a given configuration.\n\nPart 1: Derivation of the expression for $h_{ii}$.\n\nIn ordinary least squares (OLS) regression, the goal is to find the coefficient vector $\\hat{\\beta}$ that minimizes the sum of squared residuals. This leads to the normal equations:\n$$V^{\\top}V\\,\\hat{\\beta} = V^{\\top}y$$\nwhere $V$ is the design matrix and $y$ is the vector of observed responses.\n\nAssuming the columns of $V$ are linearly independent, the matrix $V^{\\top}V$ is invertible. We can solve for the estimated coefficient vector $\\hat{\\beta}$:\n$$\\hat{\\beta} = (V^{\\top}V)^{-1} V^{\\top}y$$\n\nThe fitted values of the model, denoted by the vector $\\hat{y}$, are given by:\n$$\\hat{y} = V\\hat{\\beta}$$\n\nSubstituting the expression for $\\hat{\\beta}$ into the equation for $\\hat{y}$, we obtain:\n$$\\hat{y} = V \\left( (V^{\\top}V)^{-1} V^{\\top}y \\right)$$\n\nBy the associativity of matrix multiplication, we can regroup the terms:\n$$\\hat{y} = \\left( V (V^{\\top}V)^{-1} V^{\\top} \\right) y$$\n\nThe matrix that transforms the observed responses $y$ into the fitted values $\\hat{y}$ is defined as the hat matrix, $H$. Therefore:\n$$H = V (V^{\\top}V)^{-1} V^{\\top}$$\nThis matrix $H$ represents the orthogonal projection onto the column space of $V$. The diagonal elements of this matrix, $h_{ii}$, are the leverage scores for each observation $i$.\n\nWe seek an expression for the element $h_{ii}$, which is the element in the $i$-th row and $i$-th column of $H$. Let $v_k^{\\top}$ denote the $k$-th row of the matrix $V$. Then $V$ can be expressed as a stack of its row vectors:\n$$V = \\begin{pmatrix} v_1^{\\top} \\\\ v_2^{\\top} \\\\ \\vdots \\\\ v_n^{\\top} \\end{pmatrix}$$\nThe transpose, $V^{\\top}$, is a concatenation of the column vectors $v_k$:\n$$V^{\\top} = \\begin{pmatrix} v_1  v_2  \\dots  v_n \\end{pmatrix}$$\nThe $(i,j)$-th element of a matrix product $A B C$ is given by $(\\text{row } i \\text{ of } A) \\times B \\times (\\text{col } j \\text{ of } C)$. To find the diagonal element $h_{ii}$ of $H = V (V^{\\top}V)^{-1} V^{\\top}$, we take the $i$-th row of $V$ and the $i$-th column of $V^{\\top}$.\n\nThe $i$-th row of $V$ is $v_i^{\\top}$.\nThe $i$-th column of $V^{\\top}$ is the transpose of the $i$-th row of $V$, which is $v_i$.\n\nTherefore, the expression for $h_{ii}$ is:\n$$h_{ii} = v_i^{\\top} (V^{\\top}V)^{-1} v_i$$\nThis is the required analytic expression for the diagonal hat value $h_{ii}$ in terms of the $i$-th row of $V$ and the matrix $(V^{\\top}V)^{-1}$.\n\nPart 2: Computation of $h_{22}$ for the specific input configuration.\n\nThe given inputs are $x_{1} = -1$, $x_{2} = 0$, $x_{3} = 1$, and $x_{4} = 2$. The regression is a polynomial of degree $2$. The rows of the Vandermonde design matrix $V$ are of the form $[1, x_{i}, x_{i}^{2}]$. The resulting $4 \\times 3$ matrix $V$ is:\n$$V = \\begin{pmatrix}\n1  x_1  x_1^2 \\\\\n1  x_2  x_2^2 \\\\\n1  x_3  x_3^2 \\\\\n1  x_4  x_4^2\n\\end{pmatrix} = \\begin{pmatrix}\n1  -1  1 \\\\\n1  0  0 \\\\\n1  1  1 \\\\\n1  2  4\n\\end{pmatrix}$$\nWe need to compute $h_{22}$. Using the formula derived above with $i=2$:\n$$h_{22} = v_2^{\\top} (V^{\\top}V)^{-1} v_2$$\nwhere $v_2^{\\top}$ is the second row of $V$, so $v_2^{\\top} = \\begin{pmatrix} 1  0  0 \\end{pmatrix}$.\n\nFirst, we compute the matrix $V^{\\top}V$:\n$$V^{\\top}V = \\begin{pmatrix} 1  1  1  1 \\\\ -1  0  1  2 \\\\ 1  0  1  4 \\end{pmatrix} \\begin{pmatrix} 1  -1  1 \\\\ 1  0  0 \\\\ 1  1  1 \\\\ 1  2  4 \\end{pmatrix}$$\nThe entries of $V^{\\top}V$ are sums of powers of the $x_i$ values:\n$$V^{\\top}V = \\begin{pmatrix}\n\\sum_{i=1}^{4} 1  \\sum_{i=1}^{4} x_i  \\sum_{i=1}^{4} x_i^2 \\\\\n\\sum_{i=1}^{4} x_i  \\sum_{i=1}^{4} x_i^2  \\sum_{i=1}^{4} x_i^3 \\\\\n\\sum_{i=1}^{4} x_i^2  \\sum_{i=1}^{4} x_i^3  \\sum_{i=1}^{4} x_i^4\n\\end{pmatrix}$$\nLet's compute these sums:\n$\\sum x_i^0 = 1+1+1+1 = 4$\n$\\sum x_i^1 = -1+0+1+2 = 2$\n$\\sum x_i^2 = (-1)^2+0^2+1^2+2^2 = 1+0+1+4 = 6$\n$\\sum x_i^3 = (-1)^3+0^3+1^3+2^3 = -1+0+1+8 = 8$\n$\\sum x_i^4 = (-1)^4+0^4+1^4+2^4 = 1+0+1+16 = 18$\nSo, the matrix is:\n$$V^{\\top}V = \\begin{pmatrix} 4  2  6 \\\\ 2  6  8 \\\\ 6  8  18 \\end{pmatrix}$$\nNext, we must find the inverse, $(V^{\\top}V)^{-1}$. We first calculate the determinant:\n$$\\det(V^{\\top}V) = 4(6 \\cdot 18 - 8 \\cdot 8) - 2(2 \\cdot 18 - 6 \\cdot 8) + 6(2 \\cdot 8 - 6 \\cdot 6)$$\n$$\\det(V^{\\top}V) = 4(108 - 64) - 2(36 - 48) + 6(16 - 36)$$\n$$\\det(V^{\\top}V) = 4(44) - 2(-12) + 6(-20) = 176 + 24 - 120 = 80$$\nThe expression for $h_{22}$ is $v_2^{\\top} (V^{\\top}V)^{-1} v_2$. Since $v_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$, the quadratic form $v_2^{\\top} M v_2$ simply extracts the top-left element, $M_{11}$, of any matrix $M$. Thus, $h_{22}$ is the $(1,1)$ element of $(V^{\\top}V)^{-1}$.\n\nThe $(1,1)$ element of the inverse matrix is given by $\\frac{C_{11}}{\\det(V^{\\top}V)}$, where $C_{11}$ is the $(1,1)$ cofactor of $V^{\\top}V$.\n$$C_{11} = (-1)^{1+1} \\det \\begin{pmatrix} 6  8 \\\\ 8  18 \\end{pmatrix} = 6 \\cdot 18 - 8 \\cdot 8 = 108 - 64 = 44$$\nTherefore, the $(1,1)$ element of $(V^{\\top}V)^{-1}$ is:\n$$((V^{\\top}V)^{-1})_{11} = \\frac{44}{80} = \\frac{11}{20}$$\nAnd so, we have:\n$$h_{22} = \\begin{pmatrix} 1  0  0 \\end{pmatrix} (V^{\\top}V)^{-1} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = ((V^{\\top}V)^{-1})_{11} = \\frac{11}{20}$$\n\nThe leverage of the second observation is $\\frac{11}{20}$.",
            "answer": "$$\\boxed{\\frac{11}{20}}$$"
        },
        {
            "introduction": "While ordinary least squares is a powerful tool, it assumes the model's parameters are free to take any value. In practice, we often have prior knowledge or specific requirements that can be formulated as linear constraints on the coefficients. This advanced exercise demonstrates how to solve such constrained optimization problems using the method of Lagrange multipliers, transforming the problem into a system of linear equations that can be elegantly solved using block matrix inversion, showcasing the versatility of matrix operations in handling complex, real-world modeling scenarios .",
            "id": "3146942",
            "problem": "Consider a linear regression model with design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$. The constrained least squares estimator $\\hat{\\beta}$ minimizes the squared error subject to linear equality constraints. Starting from the definition of the least squares objective and equality constraints, form the Lagrangian and derive the first-order optimality conditions known as the Karush-Kuhn-Tucker (KKT) conditions. Then, solve the resulting linear system using block matrix inversion techniques (specifically, a Schur complement argument) to obtain an analytic expression for $\\hat{\\beta}$. Finally, evaluate this expression for the specific data:\n$$\nX = \\begin{bmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}, \\quad\nC = \\begin{bmatrix}\n1  1\n\\end{bmatrix}, \\quad\nd = \\begin{bmatrix}\n1\n\\end{bmatrix}.\n$$\nCompute the second component of the constrained least squares estimator, denoted $ \\hat{\\beta}_{2} $. The final answer must be a single real number. Do not round your answer.",
            "solution": "The constrained least squares problem is formulated as:\n$$\n\\text{minimize} \\quad f(\\beta) = \\|y - X\\beta\\|_2^2 = (y - X\\beta)^T(y - X\\beta) \\\\\n\\text{subject to} \\quad C\\beta = d\n$$\nTo solve this, we form the Lagrangian function $\\mathcal{L}(\\beta, \\lambda)$, where $\\lambda$ is a vector of Lagrange multipliers.\n$$\n\\mathcal{L}(\\beta, \\lambda) = (y - X\\beta)^T(y - X\\beta) + \\lambda^T(C\\beta - d)\n$$\nExpanding the objective function term:\n$$\n(y - X\\beta)^T(y - X\\beta) = y^T y - y^T X \\beta - \\beta^T X^T y + \\beta^T X^T X \\beta\n$$\nSince $\\beta^T X^T y$ is a scalar, it is equal to its transpose, so $y^T X \\beta = \\beta^T X^T y$. The Lagrangian becomes:\n$$\n\\mathcal{L}(\\beta, \\lambda) = y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta + \\lambda^T C\\beta - \\lambda^T d\n$$\nThe first-order optimality conditions (KKT conditions) are found by setting the gradients of the Lagrangian with respect to $\\beta$ and $\\lambda$ to zero.\n\nThe gradient with respect to $\\beta$:\n$$\n\\nabla_{\\beta} \\mathcal{L}(\\beta, \\lambda) = \\frac{\\partial}{\\partial \\beta} (y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta + \\lambda^T C\\beta - \\lambda^T d) = 0\n$$\n$$\n-2X^T y + 2X^T X \\beta + C^T \\lambda = 0\n$$\n$$\n2X^T X \\beta + C^T \\lambda = 2X^T y \\quad (1)\n$$\nThe gradient with respect to $\\lambda$:\n$$\n\\nabla_{\\lambda} \\mathcal{L}(\\beta, \\lambda) = \\frac{\\partial}{\\partial \\lambda} (\\dots + \\lambda^T(C\\beta - d)) = 0\n$$\n$$\nC\\beta - d = 0 \\implies C\\beta = d \\quad (2)\n$$\nThese two equations form a system of linear equations that can be expressed in block matrix form:\n$$\n\\begin{bmatrix}\n2X^T X  C^T \\\\\nC  0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta \\\\\n\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2X^T y \\\\\nd\n\\end{bmatrix}\n$$\nTo solve for $\\beta$, we can use algebraic substitution, which is equivalent to applying the block matrix inversion formula derived from the Schur complement. From equation (1), assuming $X^T X$ is invertible:\n$$\n2X^T X \\beta = 2X^T y - C^T \\lambda\n$$\n$$\n\\beta = (2X^T X)^{-1}(2X^T y - C^T \\lambda) = (X^T X)^{-1}X^T y - \\frac{1}{2}(X^T X)^{-1}C^T \\lambda\n$$\nLet $\\hat{\\beta}_{OLS} = (X^T X)^{-1}X^T y$ be the ordinary least squares (unconstrained) solution.\n$$\n\\beta = \\hat{\\beta}_{OLS} - \\frac{1}{2}(X^T X)^{-1}C^T \\lambda\n$$\nSubstitute this expression for $\\beta$ into equation (2):\n$$\nC \\left( \\hat{\\beta}_{OLS} - \\frac{1}{2}(X^T X)^{-1}C^T \\lambda \\right) = d\n$$\n$$\nC\\hat{\\beta}_{OLS} - \\frac{1}{2} C(X^T X)^{-1}C^T \\lambda = d\n$$\nWe solve for $\\lambda$, assuming the matrix $C(X^T X)^{-1}C^T$ is invertible:\n$$\n\\frac{1}{2} C(X^T X)^{-1}C^T \\lambda = C\\hat{\\beta}_{OLS} - d\n$$\n$$\n\\lambda = 2 \\left( C(X^T X)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}_{OLS} - d)\n$$\nFinally, we substitute $\\lambda$ back into the expression for $\\beta$ to get the constrained estimator $\\hat{\\beta}$:\n$$\n\\hat{\\beta} = \\hat{\\beta}_{OLS} - (X^T X)^{-1}C^T \\left( C(X^T X)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}_{OLS} - d)\n$$\nThis is the analytic expression for the constrained least squares estimator $\\hat{\\beta}$.\n\nNow, we evaluate this expression for the given data:\n$$\nX = \\begin{bmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}, \\quad y = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1  1 \\end{bmatrix}, \\quad d = \\begin{bmatrix} 1 \\end{bmatrix}\n$$\nFirst, calculate $X^T X$:\n$$\nX^T X = \\begin{bmatrix} 1  1  1 \\\\ 0  1  2 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1+1 \\cdot 1+1 \\cdot 1  1 \\cdot 0+1 \\cdot 1+1 \\cdot 2 \\\\ 0 \\cdot 1+1 \\cdot 1+2 \\cdot 1  0 \\cdot 0+1 \\cdot 1+2 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 3  3 \\\\ 3  5 \\end{bmatrix}\n$$\nNext, calculate the inverse $(X^T X)^{-1}$:\n$$\n\\det(X^T X) = 3 \\cdot 5 - 3 \\cdot 3 = 15 - 9 = 6\n$$\n$$\n(X^T X)^{-1} = \\frac{1}{6} \\begin{bmatrix} 5  -3 \\\\ -3  3 \\end{bmatrix} = \\begin{bmatrix} 5/6  -1/2 \\\\ -1/2  1/2 \\end{bmatrix}\n$$\nNext, calculate $X^T y$:\n$$\nX^T y = \\begin{bmatrix} 1  1  1 \\\\ 0  1  2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1+1 \\cdot 2+1 \\cdot 3 \\\\ 0 \\cdot 1+1 \\cdot 2+2 \\cdot 3 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix}\n$$\nNow, compute the unconstrained estimator $\\hat{\\beta}_{OLS}$:\n$$\n\\hat{\\beta}_{OLS} = (X^T X)^{-1}X^T y = \\begin{bmatrix} 5/6  -1/2 \\\\ -1/2  1/2 \\end{bmatrix} \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix} = \\begin{bmatrix} (5/6) \\cdot 6 + (-1/2) \\cdot 8 \\\\ (-1/2) \\cdot 6 + (1/2) \\cdot 8 \\end{bmatrix} = \\begin{bmatrix} 5 - 4 \\\\ -3 + 4 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n$$\nNow we compute the terms needed for the correction part of the formula.\n$$\nC(X^T X)^{-1}C^T = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 5/6  -1/2 \\\\ -1/2  1/2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5/6-1/2  -1/2+1/2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2/6  0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{3}\n$$\n$$\n\\left( C(X^T X)^{-1}C^T \\right)^{-1} = \\left( \\frac{1}{3} \\right)^{-1} = 3\n$$\nAlso, we need $C\\hat{\\beta}_{OLS} - d$:\n$$\nC\\hat{\\beta}_{OLS} = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 1+1=2\n$$\n$$\nC\\hat{\\beta}_{OLS} - d = 2 - 1 = 1\n$$\nAnd the term $(X^T X)^{-1}C^T$:\n$$\n(X^T X)^{-1}C^T = \\begin{bmatrix} 5/6  -1/2 \\\\ -1/2  1/2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5/6 - 1/2 \\\\ -1/2 + 1/2 \\end{bmatrix} = \\begin{bmatrix} 2/6 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1/3 \\\\ 0 \\end{bmatrix}\n$$\nPutting it all together to find $\\hat{\\beta}$:\n$$\n\\hat{\\beta} = \\hat{\\beta}_{OLS} - (X^T X)^{-1}C^T \\left( C(X^T X)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}_{OLS} - d)\n$$\n$$\n\\hat{\\beta} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1/3 \\\\ 0 \\end{bmatrix} (3) (1) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n$$\nThe constrained least squares estimator is $\\hat{\\beta} = \\begin{bmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\nThe problem asks for the second component, $\\hat{\\beta}_2$.\n$$\n\\hat{\\beta}_2 = 1\n$$",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}