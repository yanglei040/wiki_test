{
    "hands_on_practices": [
        {
            "introduction": "在构建大多数线性模型时，加入截距项是一个标准步骤。本练习旨在探究在特征矩阵中添加一列全为 1 的向量后所产生的代数结构。你将使用分块矩阵乘法来观察截距项如何与特征斜率相互作用，并从第一性原理出发推导逆矩阵的一部分，这比单纯记忆公式能让你获得更深刻的理解。",
            "id": "3146970",
            "problem": "在统计学习中使用的线性模型中，通常会通过用一列1来增广特征矩阵，从而包含一个截距项。考虑一个包含 $n=4$ 个观测值和 $p=2$ 个特征的数据集。其特征矩阵为\n$$\nX=\\begin{pmatrix}\n0  1\\\\\n1  1\\\\\n2  3\\\\\n3  4\n\\end{pmatrix},\n$$\n截距列为 $\\mathbf{1}=\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix}$。定义增广设计矩阵 $\\tilde{X}=\\begin{pmatrix}\\mathbf{1}  X\\end{pmatrix}$ 和格拉姆矩阵 $\\tilde{G}=\\tilde{X}^{\\top}\\tilde{X}$。\n\n仅使用矩阵转置、乘法和逆的定义，完成以下任务：\n- 通过与截距和特征相容的分块，将 $\\tilde{G}$ 表示为分块形式。从矩阵乘法的定义出发，用 $n$、X的列和以及 $X^{\\top}X$ 来表示每个分块。\n- 基于你的分块表达式，解释为什么对X的列进行中心化（即，使每个特征的样本均值为零）会影响非对角块，并且可以将截距与斜率解耦。\n- 通过求解由 $\\tilde{G}$ 构造的分块线性系统，且不引用任何预先记忆的求逆公式，推导 $\\tilde{G}^{-1}$ 的 $(1,1)$ 元素的闭式表达式，该表达式用 $n$、 $X^{\\top}X$ 和X的列和表示。\n- 对于上面给定的特定 $X$，计算 $\\tilde{G}^{-1}$ 的 $(1,1)$ 元素的精确值。\n\n最终答案以单个精确数字的形式提供。不需要四舍五入，也不涉及单位。最终答案必须仅为 $\\tilde{G}^{-1}$ 的 $(1,1)$ 元素的值。",
            "solution": "该问题是适定的，有科学依据，并为获得唯一解提供了所有必要信息。我们可以进行推导和计算。\n\n增广设计矩阵 $\\tilde{X}$ 是通过在特征矩阵 $X$ 前面附加一列1（即 $\\mathbf{1}$）而形成的。已知 $\\mathbf{1}$ 是一个 $n \\times 1$ 矩阵，而 $X$ 是一个 $n \\times p$ 矩阵，我们可以将 $\\tilde{X}$ 写成分块形式 $\\tilde{X} = \\begin{pmatrix} \\mathbf{1} & X \\end{pmatrix}$。对于本问题，$n=4$ 且 $p=2$。\n\n**第1部分：格拉姆矩阵 $\\tilde{G}$ 的分块形式**\n\n格拉姆矩阵 $\\tilde{G}$ 定义为 $\\tilde{G} = \\tilde{X}^{\\top}\\tilde{X}$。为了将其表示为分块形式，我们首先求 $\\tilde{X}$ 的转置：\n$$\n\\tilde{X}^{\\top} = \\begin{pmatrix} \\mathbf{1} & X \\end{pmatrix}^{\\top} = \\begin{pmatrix} \\mathbf{1}^{\\top} \\\\ X^{\\top} \\end{pmatrix}\n$$\n这里，$\\mathbf{1}^{\\top}$ 是一个 $1 \\times n$ 的全1行向量，$X^{\\top}$ 是特征矩阵的 $p \\times n$ 转置。\n\n现在，我们执行分块矩阵乘法来求 $\\tilde{G}$：\n$$\n\\tilde{G} = \\tilde{X}^{\\top}\\tilde{X} = \\begin{pmatrix} \\mathbf{1}^{\\top} \\\\ X^{\\top} \\end{pmatrix} \\begin{pmatrix} \\mathbf{1} & X \\end{pmatrix} = \\begin{pmatrix} \\mathbf{1}^{\\top}\\mathbf{1} & \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1} & X^{\\top}X \\end{pmatrix}\n$$\n我们识别每个分块：\n- 左上角的分块是 $\\mathbf{1}^{\\top}\\mathbf{1} = \\sum_{i=1}^{n} 1 \\cdot 1 = n$。这是一个 $1 \\times 1$ 的分块（一个标量）。\n- 右上角的分块是 $\\mathbf{1}^{\\top}X$。这是一个 $1 \\times p$ 的行向量，其第 $j$ 个元素是 $\\mathbf{1}$ 与 $X$ 的第 $j$ 列的点积，即 $\\sum_{i=1}^{n} x_{ij}$。因此，$\\mathbf{1}^{\\top}X$ 是由 $X$ 的列和组成的行向量。\n- 左下角的分块是 $X^{\\top}\\mathbf{1}$。这是一个 $p \\times 1$ 的列向量。它是 $\\mathbf{1}^{\\top}X$ 的转置：$( \\mathbf{1}^{\\top}X )^{\\top} = X^{\\top}(\\mathbf{1}^{\\top})^{\\top} = X^{\\top}\\mathbf{1}$。所以，它是由 $X$ 的列和组成的列向量。\n- 右下角的分块是 $X^{\\top}X$，即原始特征的 $p \\times p$ 格拉姆矩阵。\n\n因此，$\\tilde{G}$ 的分块形式为：\n$$\n\\tilde{G} = \\begin{pmatrix} n & \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1} & X^{\\top}X \\end{pmatrix}\n$$\n\n**第2部分：特征中心化的影响**\n\n对 $X$ 的列进行中心化意味着将每个特征列 $\\mathbf{x}_j$ 替换为一个新列 $\\mathbf{x}'_j = \\mathbf{x}_j - \\bar{x}_j\\mathbf{1}$，其中 $\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^{n} x_{ij}$ 是第 $j$ 个特征的样本均值。设中心化后的矩阵为 $X_c$。\n\n这种变换的关键影响在于每个新列中元素的总和。第 $j$ 个中心化列的元素之和为：\n$$\n\\mathbf{1}^{\\top}\\mathbf{x}'_j = \\mathbf{1}^{\\top}(\\mathbf{x}_j - \\bar{x}_j\\mathbf{1}) = \\mathbf{1}^{\\top}\\mathbf{x}_j - \\bar{x}_j(\\mathbf{1}^{\\top}\\mathbf{1}) = \\left(\\sum_{i=1}^{n} x_{ij}\\right) - \\bar{x}_j \\cdot n\n$$\n根据均值 $\\bar{x}_j$ 的定义，我们有 $\\sum_{i=1}^{n} x_{ij} = n\\bar{x}_j$。将其代入可得：\n$$\n\\mathbf{1}^{\\top}\\mathbf{x}'_j = n\\bar{x}_j - n\\bar{x}_j = 0\n$$\n这意味着对于中心化矩阵 $X_c$，其对应的格拉姆矩阵 $\\tilde{G}_c$ 的非对角块成为零矩阵。右上角的分块 $\\mathbf{1}^{\\top}X_c$ 是一个 $1 \\times p$ 的零向量，左下角的分块 $X_c^{\\top}\\mathbf{1}$ 是一个 $p \\times 1$ 的零向量。\n\n因此，中心化后数据的格拉姆矩阵变为块对角矩阵：\n$$\n\\tilde{G}_c = \\begin{pmatrix} n & \\mathbf{0}_{1 \\times p} \\\\ \\mathbf{0}_{p \\times 1} & X_c^{\\top}X_c \\end{pmatrix}\n$$\n块对角矩阵的逆是各分块的逆组成的块对角矩阵：\n$$\n\\tilde{G}_c^{-1} = \\begin{pmatrix} n^{-1} & \\mathbf{0}_{1 \\times p} \\\\ \\mathbf{0}_{p \\times 1} & (X_c^{\\top}X_c)^{-1} \\end{pmatrix}\n$$\n在线性回归中，估计系数（截距和斜率）向量由 $\\hat{\\boldsymbol{\\beta}} = (\\tilde{X}^{\\top}\\tilde{X})^{-1}\\tilde{X}^{\\top}\\mathbf{y}$ 给出。$\\tilde{G}_c^{-1}$ 的块对角结构意味着截距项的估计与斜率系数的估计解耦。截距估计值仅取决于左上角的分块，而斜率估计值仅取决于右下角的分块。这简化了模型系数的计算和解释。\n\n**第3部分：$\\tilde{G}^{-1}$ 的 $(1,1)$ 元素的推导**\n\n我们希望找到 $\\tilde{G}^{-1}$ 第一行第一列的元素。该元素是 $\\tilde{G}^{-1}$ 第一列的第一个分量。设 $\\tilde{G}^{-1}$ 的第一列用向量 $\\mathbf{m}_1$ 表示。根据矩阵逆的定义，该列是线性系统 $\\tilde{G}\\mathbf{m}_1 = \\mathbf{e}_1$ 的解，其中 $\\mathbf{e}_1$ 是第一个标准基向量，即 $\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}$。\n\n我们根据 $\\tilde{G}$ 的分块结构对向量 $\\mathbf{m}_1$ 进行相容的分塊。由于 $\\tilde{G}$ 是一个 $(p+1) \\times (p+1)$ 矩阵，$\\mathbf{m}_1$ 是一个 $(p+1) \\times 1$ 向量。我们将其分块为 $\\mathbf{m}_1 = \\begin{pmatrix} m_{11} \\\\ \\mathbf{g} \\end{pmatrix}$，其中 $m_{11}$ 是一个标量（我们正在寻找的 $(1,1)$ 元素），$\\mathbf{g}$ 是一个 $p \\times 1$ 向量。向量 $\\mathbf{e}_1$ 分块为 $\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ \\mathbf{0}_{p \\times 1} \\end{pmatrix}$。\n\n该线性系统的分块形式为：\n$$\n\\begin{pmatrix} n & \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1} & X^{\\top}X \\end{pmatrix} \\begin{pmatrix} m_{11} \\\\ \\mathbf{g} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\mathbf{0}_{p \\times 1} \\end{pmatrix}\n$$\n这可以展开为两个方程：\n1. $n \\cdot m_{11} + (\\mathbf{1}^{\\top}X)\\mathbf{g} = 1$\n2. $(X^{\\top}\\mathbf{1})m_{11} + (X^{\\top}X)\\mathbf{g} = \\mathbf{0}_{p \\times 1}$\n\n从第二个方程中，我们解出 $\\mathbf{g}$。假设 $X^{\\top}X$ 是可逆的（如果 $X$ 是满列秩，则该假设成立）：\n$$\n(X^{\\top}X)\\mathbf{g} = -(X^{\\top}\\mathbf{1})m_{11} \\implies \\mathbf{g} = -(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})m_{11}\n$$\n现在，我们将 $\\mathbf{g}$ 的这个表达式代入第一个方程：\n$$\nn \\cdot m_{11} + (\\mathbf{1}^{\\top}X) \\left( -(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})m_{11} \\right) = 1\n$$\n由于 $m_{11}$ 是一个标量，我们可以将其提取出来：\n$$\nm_{11} \\left( n - (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) \\right) = 1\n$$\n解出 $m_{11}$（即 $\\tilde{G}^{-1}$ 的 $(1,1)$ 元素），我们得到闭式表达式：\n$$\n(\\tilde{G}^{-1})_{11} = m_{11} = \\left( n - (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) \\right)^{-1}\n$$\n\n**第4部分：对特定矩阵 $X$ 的计算**\n\n给定 $n=4$ 和 $X=\\begin{pmatrix} 0 & 1\\\\ 1 & 1\\\\ 2 & 3\\\\ 3 & 4 \\end{pmatrix}$。我们为上面推导出的公式计算必要的分量。\n\n- $X$ 的列和向量是：\n  $\\mathbf{1}^{\\top}X = \\begin{pmatrix} 0+1+2+3 & 1+1+3+4 \\end{pmatrix} = \\begin{pmatrix} 6 & 9 \\end{pmatrix}$。\n- 其转置为 $X^{\\top}\\mathbf{1} = \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix}$。\n\n- 矩阵 $X^{\\top}X$ 是：\n  $X^{\\top}X = \\begin{pmatrix} 0 & 1 & 2 & 3 \\\\ 1 & 1 & 3 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1\\\\ 1 & 1\\\\ 2 & 3\\\\ 3 & 4 \\end{pmatrix} = \\begin{pmatrix} 0+1+4+9 & 0+1+6+12 \\\\ 0+1+6+12 & 1+1+9+16 \\end{pmatrix} = \\begin{pmatrix} 14 & 19 \\\\ 19 & 27 \\end{pmatrix}$。\n\n- 我们需要 $X^{\\top}X$ 的逆。对于一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$，其逆为 $\\frac{1}{ad-bc}\\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$。\n  $X^{\\top}X$ 的行列式为 $\\det(X^{\\top}X) = (14)(27) - (19)(19) = 378 - 361 = 17$。\n  其逆为 $(X^{\\top}X)^{-1} = \\frac{1}{17}\\begin{pmatrix} 27 & -19 \\\\ -19 & 14 \\end{pmatrix}$。\n\n- 现在我们计算二次型 $(\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})$：\n  $$\n  (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) = \\begin{pmatrix} 6 & 9 \\end{pmatrix} \\left( \\frac{1}{17}\\begin{pmatrix} 27 & -19 \\\\ -19 & 14 \\end{pmatrix} \\right) \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix}\n  $$\n  首先，将行向量乘以矩阵：\n  $$\n  \\frac{1}{17} \\begin{pmatrix} 6 & 9 \\end{pmatrix} \\begin{pmatrix} 27 & -19 \\\\ -19 & 14 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 6(27)+9(-19) & 6(-19)+9(14) \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 162-171 & -114+126 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} -9 & 12 \\end{pmatrix}\n  $$\n  现在，乘以列向量：\n  $$\n  \\frac{1}{17} \\begin{pmatrix} -9 & 12 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix} = \\frac{1}{17}(-9(6) + 12(9)) = \\frac{1}{17}(-54 + 108) = \\frac{54}{17}\n  $$\n\n- 最后，我们计算 $\\tilde{G}^{-1}$ 的 $(1,1)$ 元素：\n  $$\n  (\\tilde{G}^{-1})_{11} = \\left( n - \\frac{54}{17} \\right)^{-1} = \\left( 4 - \\frac{54}{17} \\right)^{-1}\n  $$\n  $$\n  4 - \\frac{54}{17} = \\frac{4 \\times 17}{17} - \\frac{54}{17} = \\frac{68 - 54}{17} = \\frac{14}{17}\n  $$\n  因此，\n  $$\n  (\\tilde{G}^{-1})_{11} = \\left(\\frac{14}{17}\\right)^{-1} = \\frac{17}{14}\n  $$",
            "answer": "$$\n\\boxed{\\frac{17}{14}}\n$$"
        },
        {
            "introduction": "在回归分析中，我们常常希望了解哪些数据点对模型的影响最大。本练习展示了如何运用矩阵运算来计算“帽子矩阵”的对角线元素，即“杠杆值”，它量化了每个观测值对其自身拟合值的影响力。你将推导这些杠杆值的表达式，并在一个多项式回归的背景下进行计算，这是一个揭示矩阵求逆在回归诊断中核心作用的经典范例。",
            "id": "3146914",
            "problem": "在统计学习框架下，考虑$2$次多项式普通最小二乘（OLS）回归。给定输入 $x_{1}, x_{2}, x_{3}, x_{4}$，并构成范德蒙设计矩阵 $V$，其第 $i$ 行为 $[1, x_{i}, x_{i}^{2}]$。拟合值是观测响应的线性函数，定义了一个称为帽子矩阵的线性算子，其对角线元素 $h_{ii}$ 量化了观测点 $i$ 的杠杆率。\n\n仅从正规方程 $V^{\\top}V\\,\\hat{\\beta} = V^{\\top}y$ 以及 OLS 产生 $y$ 到 $V$ 的列空间上的正交投影这一事实出发，推导对角帽子值 $h_{ii}$ 关于 $V$ 的第 $i$ 行和矩阵 $(V^{\\top}V)^{-1}$ 的解析表达式。然后，对于特定的输入配置 $x_{1} = -1$、$x_{2} = 0$、$x_{3} = 1$、$x_{4} = 2$，计算上述二次范德蒙矩阵的 $h_{22}$ 的精确值。请将最终答案表示为一个精确的有理数。",
            "solution": "该问题陈述经评估为有效。它在科学上基于线性代数和统计回归的原理，问题提法良好，信息充分，可得出唯一解，并以客观、正式的语言表述。不存在矛盾、歧义或伪科学论断。我们可以开始求解。\n\n该问题包含两部分：首先，推导帽子矩阵对角线元素的一般表达式；其次，为给定的配置计算一个具体值。\n\n第1部分：推导 $h_{ii}$ 的表达式。\n\n在普通最小二乘（OLS）回归中，目标是找到使残差平方和最小化的系数向量 $\\hat{\\beta}$。这引出了正规方程：\n$$V^{\\top}V\\,\\hat{\\beta} = V^{\\top}y$$\n其中 $V$ 是设计矩阵，$y$ 是观测响应向量。\n\n假设 $V$ 的列是线性无关的，那么矩阵 $V^{\\top}V$ 是可逆的。我们可以解出估计的系数向量 $\\hat{\\beta}$：\n$$\\hat{\\beta} = (V^{\\top}V)^{-1} V^{\\top}y$$\n\n模型的拟合值，用向量 $\\hat{y}$ 表示，由以下公式给出：\n$$\\hat{y} = V\\hat{\\beta}$$\n\n将 $\\hat{\\beta}$ 的表达式代入 $\\hat{y}$ 的方程，我们得到：\n$$\\hat{y} = V \\left( (V^{\\top}V)^{-1} V^{\\top}y \\right)$$\n\n根据矩阵乘法的结合律，我们可以重新组合这些项：\n$$\\hat{y} = \\left( V (V^{\\top}V)^{-1} V^{\\top} \\right) y$$\n\n将观测响应 $y$ 转换为拟合值 $\\hat{y}$ 的矩阵定义为帽子矩阵 $H$。因此：\n$$H = V (V^{\\top}V)^{-1} V^{\\top}$$\n这个矩阵 $H$ 代表到 $V$ 的列空间上的正交投影。该矩阵的对角线元素 $h_{ii}$ 是每个观测点 $i$ 的杠杆分数。\n\n我们寻求元素 $h_{ii}$ 的表达式，它是 $H$ 的第 $i$ 行和第 $i$ 列的元素。设 $v_k^{\\top}$ 表示矩阵 $V$ 的第 $k$ 行。那么 $V$ 可以表示为其行向量的堆叠：\n$$V = \\begin{pmatrix} v_1^{\\top} \\\\ v_2^{\\top} \\\\ \\vdots \\\\ v_n^{\\top} \\end{pmatrix}$$\n$V$ 的转置 $V^{\\top}$ 是列向量 $v_k$ 的拼接：\n$$V^{\\top} = \\begin{pmatrix} v_1 & v_2 & \\dots & v_n \\end{pmatrix}$$\n矩阵乘积 $A B C$ 的 $(i,j)$ 元由 $(A \\text{ 的第 } i \\text{ 行}) \\times B \\times (C \\text{ 的第 } j \\text{ 列})$ 给出。为了找到 $H = V (V^{\\top}V)^{-1} V^{\\top}$ 的对角线元素 $h_{ii}$，我们取 $V$ 的第 $i$ 行和 $V^{\\top}$ 的第 $i$ 列。\n\n$V$ 的第 $i$ 行是 $v_i^{\\top}$。\n$V^{\\top}$ 的第 $i$ 列是 $V$ 的第 $i$ 行的转置，即 $v_i$。\n\n因此，$h_{ii}$ 的表达式为：\n$$h_{ii} = v_i^{\\top} (V^{\\top}V)^{-1} v_i$$\n这就是所求的对角帽子值 $h_{ii}$ 关于 $V$ 的第 $i$ 行和矩阵 $(V^{\\top}V)^{-1}$ 的解析表达式。\n\n第2部分：为特定输入配置计算 $h_{22}$。\n\n给定的输入为 $x_{1} = -1$、$x_{2} = 0$、$x_{3} = 1$ 和 $x_{4} = 2$。回归是 $2$ 次多项式。范德蒙设计矩阵 $V$ 的行形式为 $[1, x_{i}, x_{i}^{2}]$。得到的 $4 \\times 3$ 矩阵 $V$ 是：\n$$V = \\begin{pmatrix}\n1 & -1 & 1 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 1 \\\\\n1 & 2 & 4\n\\end{pmatrix}$$\n我们需要计算 $h_{22}$。使用上面推导的公式，令 $i=2$：\n$$h_{22} = v_2^{\\top} (V^{\\top}V)^{-1} v_2$$\n其中 $v_2^{\\top}$ 是 $V$ 的第二行，所以 $v_2^{\\top} = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}$。\n\n首先，我们计算矩阵 $V^{\\top}V$：\n$$V^{\\top}V = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & 0 & 1 & 2 \\\\ 1 & 0 & 1 & 4 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & 1 \\\\ 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 1 & 2 & 4 \\end{pmatrix}$$\n$V^{\\top}V$ 的元素是 $x_i$ 值的幂次和：\n$$V^{\\top}V = \\begin{pmatrix}\n\\sum_{i=1}^{4} 1 & \\sum_{i=1}^{4} x_i & \\sum_{i=1}^{4} x_i^2 \\\\\n\\sum_{i=1}^{4} x_i & \\sum_{i=1}^{4} x_i^2 & \\sum_{i=1}^{4} x_i^3 \\\\\n\\sum_{i=1}^{4} x_i^2 & \\sum_{i=1}^{4} x_i^3 & \\sum_{i=1}^{4} x_i^4\n\\end{pmatrix}$$\n我们来计算这些和：\n$\\sum x_i^0 = 1+1+1+1 = 4$\n$\\sum x_i^1 = -1+0+1+2 = 2$\n$\\sum x_i^2 = (-1)^2+0^2+1^2+2^2 = 1+0+1+4 = 6$\n$\\sum x_i^3 = (-1)^3+0^3+1^3+2^3 = -1+0+1+8 = 8$\n$\\sum x_i^4 = (-1)^4+0^4+1^4+2^4 = 1+0+1+16 = 18$\n所以，该矩阵为：\n$$V^{\\top}V = \\begin{pmatrix} 4 & 2 & 6 \\\\ 2 & 6 & 8 \\\\ 6 & 8 & 18 \\end{pmatrix}$$\n接下来，我们必须求逆矩阵 $(V^{\\top}V)^{-1}$。我们首先计算行列式：\n$$\\det(V^{\\top}V) = 4(6 \\cdot 18 - 8 \\cdot 8) - 2(2 \\cdot 18 - 6 \\cdot 8) + 6(2 \\cdot 8 - 6 \\cdot 6)$$\n$$\\det(V^{\\top}V) = 4(108 - 64) - 2(36 - 48) + 6(16 - 36)$$\n$$\\det(V^{\\top}V) = 4(44) - 2(-12) + 6(-20) = 176 + 24 - 120 = 80$$\n$h_{22}$ 的表达式是 $v_2^{\\top} (V^{\\top}V)^{-1} v_2$。由于 $v_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$，二次型 $v_2^{\\top} M v_2$ 只是提取任意矩阵 $M$ 的左上角元素 $M_{11}$。因此，$h_{22}$ 是 $(V^{\\top}V)^{-1}$ 的 $(1,1)$ 元素。\n\n逆矩阵的 $(1,1)$ 元素由 $\\frac{C_{11}}{\\det(V^{\\top}V)}$ 给出，其中 $C_{11}$ 是 $V^{\\top}V$ 的 $(1,1)$ 代数余子式。\n$$C_{11} = (-1)^{1+1} \\det \\begin{pmatrix} 6 & 8 \\\\ 8 & 18 \\end{pmatrix} = 6 \\cdot 18 - 8 \\cdot 8 = 108 - 64 = 44$$\n因此，$(V^{\\top}V)^{-1}$ 的 $(1,1)$ 元素是：\n$$((V^{\\top}V)^{-1})_{11} = \\frac{44}{80} = \\frac{11}{20}$$\n因此，我们有：\n$$h_{22} = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} (V^{\\top}V)^{-1} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = ((V^{\\top}V)^{-1})_{11} = \\frac{11}{20}$$\n\n第二个观测点的杠杆率为 $\\frac{11}{20}$。",
            "answer": "$$\\boxed{\\frac{11}{20}}$$"
        },
        {
            "introduction": "标准的普通最小二乘法（OLS）并非总是足够的；有时我们希望模型能遵循一些以约束形式存在的先验知识。本高级练习将引导你使用拉格朗日乘子法和 Karush-Kuhn-Tucker (KKT) 条件这一强大框架来求解一个带约束的最小二乘问题。通过将优化问题转化为一个分块矩阵线性系统并求解，你将掌握一项连接了优化理论与统计建模的通用技能。",
            "id": "3146942",
            "problem": "考虑一个线性回归模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$。约束最小二乘估计量 $\\hat{\\beta}$ 在满足线性等式约束的条件下最小化平方误差。从最小二乘目标和等式约束的定义出发，构建拉格朗日函数，并推导一阶最优性条件，即 Karush-Kuhn-Tucker (KKT) 条件，其中 Karush-Kuhn-Tucker (KKT) 指的是约束优化中的最优性必要条件。然后，使用分块矩阵求逆技术（具体来说，使用舒尔补（Schur complement）论证）求解所得到的线性系统，以获得 $\\hat{\\beta}$ 的解析表达式。最后，对该表达式使用以下特定数据进行求值：\n$$\nX = \\begin{bmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}, \\quad\nC = \\begin{bmatrix}\n1  1\n\\end{bmatrix}, \\quad\nd = \\begin{bmatrix}\n1\n\\end{bmatrix}.\n$$\n计算约束最小二乘估计量的第二个分量，记为 $ \\hat{\\beta}_{2} $。最终答案必须是一个实数。请勿对答案进行四舍五入。",
            "solution": "约束最小二乘问题表述为：\n$$\n\\text{minimize} \\quad f(\\beta) = \\|y - X\\beta\\|_2^2 = (y - X\\beta)^T(y - X\\beta) \\\\\n\\text{subject to} \\quad C\\beta = d\n$$\n为了解决这个问题，我们构建拉格朗日函数 $\\mathcal{L}(\\beta, \\lambda)$，其中 $\\lambda$ 是拉格朗日乘子向量。\n$$\n\\mathcal{L}(\\beta, \\lambda) = (y - X\\beta)^T(y - X\\beta) + \\lambda^T(C\\beta - d)\n$$\n展开目标函数项：\n$$\n(y - X\\beta)^T(y - X\\beta) = y^T y - y^T X \\beta - \\beta^T X^T y + \\beta^T X^T X \\beta\n$$\n由于 $\\beta^T X^T y$ 是一个标量，它等于其转置 $(y^T X \\beta)^T$。因此，$y^T X \\beta = \\beta^T X^T y$。拉格朗日函数变为：\n$$\n\\mathcal{L}(\\beta, \\lambda) = y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta + \\lambda^T C\\beta - \\lambda^T d\n$$\n通过将拉格朗日函数对 $\\beta$ 和 $\\lambda$ 的梯度设置为零，可以找到一阶最优性条件（KKT 条件）。\n\n关于 $\\beta$ 的梯度：\n$$\n\\nabla_{\\beta} \\mathcal{L}(\\beta, \\lambda) = \\frac{\\partial}{\\partial \\beta} (y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta + \\lambda^T C\\beta - \\lambda^T d) = 0\n$$\n$$\n-2X^T y + 2X^T X \\beta + C^T \\lambda = 0\n$$\n$$\n2X^T X \\beta + C^T \\lambda = 2X^T y \\quad (1)\n$$\n关于 $\\lambda$ 的梯度：\n$$\n\\nabla_{\\lambda} \\mathcal{L}(\\beta, \\lambda) = \\frac{\\partial}{\\partial \\lambda} (\\dots + \\lambda^T(C\\beta - d)) = 0\n$$\n$$\nC\\beta - d = 0 \\implies C\\beta = d \\quad (2)\n$$\n这两个方程构成一个线性方程组，可以用分块矩阵形式表示：\n$$\n\\begin{bmatrix}\n2X^T X & C^T \\\\\nC & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta \\\\\n\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2X^T y \\\\\nd\n\\end{bmatrix}\n$$\n为了求解 $\\beta$，我们可以使用代数代换，这等同于应用从舒尔补导出的分块矩阵求逆公式。从方程 (1) 出发，假设 $X^T X$ 是可逆的：\n$$\n2X^T X \\beta = 2X^T y - C^T \\lambda\n$$\n$$\n\\beta = (2X^T X)^{-1}(2X^T y - C^T \\lambda) = (X^T X)^{-1}X^T y - \\frac{1}{2}(X^T X)^{-1}C^T \\lambda\n$$\n令 $\\hat{\\beta}_{OLS} = (X^T X)^{-1}X^T y$ 为普通最小二乘（无约束）解。\n$$\n\\beta = \\hat{\\beta}_{OLS} - \\frac{1}{2}(X^T X)^{-1}C^T \\lambda\n$$\n将这个 $\\beta$ 的表达式代入方程 (2)：\n$$\nC \\left( \\hat{\\beta}_{OLS} - \\frac{1}{2}(X^T X)^{-1}C^T \\lambda \\right) = d\n$$\n$$\nC\\hat{\\beta}_{OLS} - \\frac{1}{2} C(X^T X)^{-1}C^T \\lambda = d\n$$\n我们求解 $\\lambda$，假设矩阵 $C(X^T X)^{-1}C^T$ 是可逆的：\n$$\n\\frac{1}{2} C(X^T X)^{-1}C^T \\lambda = C\\hat{\\beta}_{OLS} - d\n$$\n$$\n\\lambda = 2 \\left( C(X^T X)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}_{OLS} - d)\n$$\n最后，我们将 $\\lambda$ 代回到 $\\beta$ 的表达式中，以获得约束估计量 $\\hat{\\beta}$：\n$$\n\\hat{\\beta} = \\hat{\\beta}_{OLS} - \\frac{1}{2}(X^T X)^{-1}C^T \\left[ 2 \\left( C(X^T X)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}_{OLS} - d) \\right]\n$$\n$$\n\\hat{\\beta} = \\hat{\\beta}_{OLS} - (X^T X)^{-1}C^T \\left( C(X^T X)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}_{OLS} - d)\n$$\n这就是约束最小二乘估计量 $\\hat{\\beta}$ 的解析表达式。\n\n现在，我们使用给定数据对该表达式进行求值：\n$$\nX = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{bmatrix}, \\quad y = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1 & 1 \\end{bmatrix}, \\quad d = \\begin{bmatrix} 1 \\end{bmatrix}\n$$\n首先，计算 $X^T X$：\n$$\nX^T X = \\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1+1 \\cdot 1+1 \\cdot 1 & 1 \\cdot 0+1 \\cdot 1+1 \\cdot 2 \\\\ 0 \\cdot 1+1 \\cdot 1+2 \\cdot 1 & 0 \\cdot 0+1 \\cdot 1+2 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 3 & 3 \\\\ 3 & 5 \\end{bmatrix}\n$$\n接着，计算逆矩阵 $(X^T X)^{-1}$：\n$$\n\\det(X^T X) = 3 \\cdot 5 - 3 \\cdot 3 = 15 - 9 = 6\n$$\n$$\n(X^T X)^{-1} = \\frac{1}{6} \\begin{bmatrix} 5 & -3 \\\\ -3 & 3 \\end{bmatrix} = \\begin{bmatrix} 5/6 & -1/2 \\\\ -1/2 & 1/2 \\end{bmatrix}\n$$\n接着，计算 $X^T y$：\n$$\nX^T y = \\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1+1 \\cdot 2+1 \\cdot 3 \\\\ 0 \\cdot 1+1 \\cdot 2+2 \\cdot 3 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix}\n$$\n现在，计算无约束估计量 $\\hat{\\beta}_{OLS}$：\n$$\n\\hat{\\beta}_{OLS} = (X^T X)^{-1}X^T y = \\begin{bmatrix} 5/6 & -1/2 \\\\ -1/2 & 1/2 \\end{bmatrix} \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix} = \\begin{bmatrix} (5/6) \\cdot 6 + (-1/2) \\cdot 8 \\\\ (-1/2) \\cdot 6 + (1/2) \\cdot 8 \\end{bmatrix} = \\begin{bmatrix} 5 - 4 \\\\ -3 + 4 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n$$\n现在我们计算公式中修正部分所需的项。\n$$\nC(X^T X)^{-1}C^T = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 5/6 & -1/2 \\\\ -1/2 & 1/2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5/6-1/2 & -1/2+1/2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2/6 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{3}\n$$\n$$\n\\left( C(X^T X)^{-1}C^T \\right)^{-1} = \\left( \\frac{1}{3} \\right)^{-1} = 3\n$$\n此外，我们需要 $C\\hat{\\beta}_{OLS} - d$：\n$$\nC\\hat{\\beta}_{OLS} = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 1+1=2\n$$\n$$\nC\\hat{\\beta}_{OLS} - d = 2 - 1 = 1\n$$\n以及项 $(X^T X)^{-1}C^T$：\n$$\n(X^T X)^{-1}C^T = \\begin{bmatrix} 5/6 & -1/2 \\\\ -1/2 & 1/2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5/6 - 1/2 \\\\ -1/2 + 1/2 \\end{bmatrix} = \\begin{bmatrix} 2/6 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1/3 \\\\ 0 \\end{bmatrix}\n$$\n将所有部分组合起来以求得 $\\hat{\\beta}$：\n$$\n\\hat{\\beta} = \\hat{\\beta}_{OLS} - (X^T X)^{-1}C^T \\left( C(X^T X)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}_{OLS} - d)\n$$\n$$\n\\hat{\\beta} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1/3 \\\\ 0 \\end{bmatrix} (3) (1) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n$$\n约束最小二乘估计量为 $\\hat{\\beta} = \\begin{bmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n问题要求的是第二个分量 $\\hat{\\beta}_2$。\n$$\n\\hat{\\beta}_2 = 1\n$$",
            "answer": "$$\\boxed{1}$$"
        }
    ]
}