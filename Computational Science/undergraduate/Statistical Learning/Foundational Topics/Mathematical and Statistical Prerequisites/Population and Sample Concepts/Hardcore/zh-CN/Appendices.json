{
    "hands_on_practices": [
        {
            "introduction": "我们知道，总体中预测变量之间的相关性（共线性）会影响估计的稳定性。但是，即使我们知道了总体的相关性，我们抽取的任何有限样本都会有其自身的样本相关性。这个练习将探讨当样本的共线性偶然高于总体时，它如何显著地放大我们回归系数估计的方差，从而降低模型的可靠性 。",
            "id": "3159127",
            "problem": "考虑一个特征向量总体 $\\mathbf{X} = (X_{1}, X_{2})^{\\top}$，其联合分布为二元正态分布 $\\mathcal{N}(\\mathbf{0}, \\Sigma)$，其中总体协方差矩阵为\n$$\n\\Sigma = \\begin{pmatrix}\n4   5.4 \\\\\n5.4   9\n\\end{pmatrix}.\n$$\n这意味着总体相关系数为 $\\rho_{\\text{pop}} = \\frac{5.4}{\\sqrt{4 \\cdot 9}} = 0.90$，因此 $X_{1}$ 与 $X_{2}$ 强相关。一个大小为 $n = 200$ 的数据集是从该总体中独立同分布抽样得到的。在一次具体的实现样本中，$X_{1}$ 的观测样本方差为 $4$，$X_{2}$ 的观测样本方差为 $9$，观测样本协方差为 $5.94$，对应的样本相关系数为 $\\rho_{\\text{samp}} = \\frac{5.94}{\\sqrt{4 \\cdot 9}} = 0.99$。\n\n假设响应变量 $Y$ 服从线性模型\n$$\nY = \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon,\n$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$ 与 $\\mathbf{X}$ 独立，且 $\\sigma_{\\varepsilon}^{2} = 1$。将 $\\hat{\\beta} = (\\hat{\\beta}_{1}, \\hat{\\beta}_{2})^{\\top}$ 定义为基于观测样本的普通最小二乘 (OLS) 估计量。\n\n从协方差矩阵和同方差误差下线性回归估计量方差的基本定义出发（不使用任何给定的快捷公式），推导出一个用预测变量的 $2 \\times 2$ 协方差矩阵表示的 $\\operatorname{Var}(\\hat{\\beta}_{1})$ 的精确表达式。然后，计算在实现的样本设计下（相关系数为 $0.99$）的 $\\operatorname{Var}(\\hat{\\beta}_{1})$ 相对于使用总体协方差矩阵（相关系数为 $0.90$）计算出的方差的倍数，假设边际方差如上所述。将最终的因子以一个实数形式给出，并将答案四舍五入到四位有效数字。",
            "solution": "问题要求推导普通最小二乘 (OLS) 估计量 $\\hat{\\beta}_{1}$ 的方差，然后计算在两种不同的预测变量协方差结构情景下该方差的比率。\n\n首先，我们推导 $\\operatorname{Var}(\\hat{\\beta}_{1})$ 的表达式。线性模型的矩阵形式为 $\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{Y}$ 是响应变量观测值的 $n \\times 1$ 向量，$\\mathbf{X}$ 是预测变量观测值的 $n \\times p$ 设计矩阵（本例中 $p=2$），$\\boldsymbol{\\beta}$ 是真实系数的 $p \\times 1$ 向量，$\\boldsymbol{\\varepsilon}$ 是误差的 $n \\times 1$ 向量。假设误差是独立同分布的，均值为 $0$，方差为 $\\sigma_{\\varepsilon}^{2}$，即 $\\operatorname{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$ 且 $\\operatorname{Var}(\\boldsymbol{\\varepsilon}) = \\sigma_{\\varepsilon}^{2}\\mathbf{I}_{n}$，其中 $\\mathbf{I}_{n}$ 是 $n \\times n$ 单位矩阵。\n\nOLS 估计量 $\\hat{\\boldsymbol{\\beta}}$ 定义为：\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{Y}\n$$\n为了求 $\\hat{\\boldsymbol{\\beta}}$ 的方差，我们首先用真实系数 $\\boldsymbol{\\beta}$ 和误差项 $\\boldsymbol{\\varepsilon}$ 来表示它：\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon} = \\boldsymbol{\\beta} + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n$$\n该估计量在设计矩阵 $\\mathbf{X}$ 给定的条件下的方差为：\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\operatorname{Var}(\\boldsymbol{\\beta} + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon} | \\mathbf{X})\n$$\n由于 $\\boldsymbol{\\beta}$ 是一个常数向量，并且我们以 $\\mathbf{X}$ 为条件，上式可简化为：\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\operatorname{Var}((\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon} | \\mathbf{X})\n$$\n利用对于矩阵 $\\mathbf{A}$ 和随机向量 $\\mathbf{z}$ 有 $\\operatorname{Var}(\\mathbf{A}\\mathbf{z}) = \\mathbf{A}\\operatorname{Var}(\\mathbf{z})\\mathbf{A}^{\\top}$ 的性质，我们有：\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = ((\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}) \\operatorname{Var}(\\boldsymbol{\\varepsilon}) ((\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top})^{\\top}\n$$\n代入 $\\operatorname{Var}(\\boldsymbol{\\varepsilon}) = \\sigma_{\\varepsilon}^{2}\\mathbf{I}_{n}$：\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top} (\\mathbf{X}((\\mathbf{X}^{\\top}\\mathbf{X})^{-1})^{\\top}) = \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{X}((\\mathbf{X}^{\\top}\\mathbf{X})^{\\top})^{-1}\n$$\n由于 $\\mathbf{X}^{\\top}\\mathbf{X}$ 是一个对称矩阵，$(\\mathbf{X}^{\\top}\\mathbf{X})^{\\top} = \\mathbf{X}^{\\top}\\mathbf{X}$，可得：\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1} = \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\n$$\n题目说明预测变量的总体均值为 $\\mathbf{0}$。对于一个中心化的设计矩阵，预测变量的样本协方差矩阵 $\\mathbf{S}$ 由 $\\mathbf{S} = \\frac{1}{n}\\mathbf{X}^{\\top}\\mathbf{X}$ 给出。因此，$\\mathbf{X}^{\\top}\\mathbf{X} = n\\mathbf{S}$。\n于是 $\\hat{\\boldsymbol{\\beta}}$ 的方差-协方差矩阵为：\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma_{\\varepsilon}^{2} (n\\mathbf{S})^{-1} = \\frac{\\sigma_{\\varepsilon}^{2}}{n} \\mathbf{S}^{-1}\n$$\n对于本题，样本协方差矩阵是一个 $2 \\times 2$ 矩阵：\n$$\n\\mathbf{S} = \\begin{pmatrix} s_{1}^{2}   s_{12} \\\\ s_{12}   s_{2}^{2} \\end{pmatrix}\n$$\n其中 $s_{1}^{2}$ 和 $s_{2}^{2}$ 分别是 $X_{1}$ 和 $X_{2}$ 的样本方差，$s_{12}$ 是它们的样本协方差。\n$\\mathbf{S}$ 的逆矩阵是：\n$$\n\\mathbf{S}^{-1} = \\frac{1}{s_{1}^{2}s_{2}^{2} - s_{12}^{2}} \\begin{pmatrix} s_{2}^{2}   -s_{12} \\\\ -s_{12}   s_{1}^{2} \\end{pmatrix}\n$$\n$\\hat{\\beta}_{1}$ 的方差，记为 $\\operatorname{Var}(\\hat{\\beta}_{1})$，是矩阵 $\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X})$ 的 $(1,1)$ 元素：\n$$\n\\operatorname{Var}(\\hat{\\beta}_{1}) = \\frac{\\sigma_{\\varepsilon}^{2}}{n} (\\mathbf{S}^{-1})_{11} = \\frac{\\sigma_{\\varepsilon}^{2}}{n} \\left( \\frac{s_{2}^{2}}{s_{1}^{2}s_{2}^{2} - s_{12}^{2}} \\right)\n$$\n这就是用协方差矩阵的元素表示所要求的表达式。我们可以使用样本相关系数 $\\rho_{\\text{samp}} = \\frac{s_{12}}{\\sqrt{s_{1}^{2}s_{2}^{2}}}$ 来简化这个表达式。分母可以改写为：\n$$\ns_{1}^{2}s_{2}^{2} - s_{12}^{2} = s_{1}^{2}s_{2}^{2} - (\\rho_{\\text{samp}}\\sqrt{s_{1}^{2}s_{2}^{2}})^{2} = s_{1}^{2}s_{2}^{2}(1 - \\rho_{\\text{samp}}^{2})\n$$\n将此代回方差的表达式中：\n$$\n\\operatorname{Var}(\\hat{\\beta}_{1}) = \\frac{\\sigma_{\\varepsilon}^{2}}{n} \\frac{s_{2}^{2}}{s_{1}^{2}s_{2}^{2}(1 - \\rho_{\\text{samp}}^{2})} = \\frac{\\sigma_{\\varepsilon}^{2}}{n s_{1}^{2}(1 - \\rho_{\\text{samp}}^{2})}\n$$\n现在，我们计算问题所要求的两个方差。\n\n$1$. 在实现的样本设计下的方差，我们将其记为 $V_{\\text{samp}}$。\n给定的样本值为 $n=200$， $s_{1}^{2}=4$， $\\rho_{\\text{samp}}=0.99$ 以及 $\\sigma_{\\varepsilon}^{2}=1$。\n$$\nV_{\\text{samp}} = \\frac{1}{200 \\cdot 4 \\cdot (1 - 0.99^{2})}\n$$\n\n$2$. 使用总体协方差矩阵计算的方差，我们将其记为 $V_{\\text{pop}}$。这是一个假设性的计算，其中样本统计量被它们对应的总体参数所取代。题目说明边际方差是相同的，$\\sigma_{1}^{2} = 4$。总体相关系数为 $\\rho_{\\text{pop}} = 0.90$。\n$$\nV_{\\text{pop}} = \\frac{1}{n \\sigma_{1}^{2}(1 - \\rho_{\\text{pop}}^{2})} = \\frac{1}{200 \\cdot 4 \\cdot (1 - 0.90^{2})}\n$$\n\n$V_{\\text{samp}}$ 超过 $V_{\\text{pop}}$ 的乘法因子是比率 $\\frac{V_{\\text{samp}}}{V_{\\text{pop}}}$：\n$$\n\\text{因子} = \\frac{\\frac{\\sigma_{\\varepsilon}^{2}}{n s_{1}^{2}(1 - \\rho_{\\text{samp}}^{2})}}{\\frac{\\sigma_{\\varepsilon}^{2}}{n \\sigma_{1}^{2}(1 - \\rho_{\\text{pop}}^{2})}}\n$$\n由于 $\\sigma_{\\varepsilon}^{2}=1$，$n=200$，且 $s_{1}^{2}=\\sigma_{1}^{2}=4$，这些项被消掉了：\n$$\n\\text{因子} = \\frac{1 - \\rho_{\\text{pop}}^{2}}{1 - \\rho_{\\text{samp}}^{2}}\n$$\n代入给定的相关系数值：$\\rho_{\\text{pop}}=0.90$ 和 $\\rho_{\\text{samp}}=0.99$。\n$$\n\\text{因子} = \\frac{1 - 0.90^{2}}{1 - 0.99^{2}} = \\frac{1 - 0.81}{1 - 0.9801} = \\frac{0.19}{0.0199}\n$$\n计算数值：\n$$\n\\text{因子} = 9.54773869...\n$$\n四舍五入到四位有效数字，该因子为 $9.548$。",
            "answer": "$$\\boxed{9.548}$$"
        },
        {
            "introduction": "在理想情况下，我们的样本是总体的无偏微缩。然而在现实中，数据采集过程常常引入系统性偏差，例如某些类别的数据更难被观察到。本练习将介绍一种常见的“随机缺失”情景，并引导你使用“逆概率加权”(Inverse Probability Weighting, IPW) 这一强大技术来校正样本偏差，从而准确估计总体的真实风险 。",
            "id": "3159121",
            "problem": "考虑一个二元分类总体 $\\mathcal{P}$，其定义如下。设标签 $Y \\in \\{0,1\\}$ 满足 $\\mathbb{P}(Y=1)=0.4$ 和 $\\mathbb{P}(Y=0)=0.6$。在给定 $Y$ 的条件下，特征 $X \\in \\mathbb{R}$ 的分布为 $X \\mid Y=1 \\sim \\mathcal{N}(2,1)$ 和 $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$。对于一个固定的分类器 $f(x)=\\mathbb{I}\\{x \\ge 1\\}$ 和 $0$-$1$ 损失 $L(y,\\hat{y})=\\mathbb{I}\\{y \\ne \\hat{y}\\}$，总体风险为 $R(f)=\\mathbb{E}[L(Y,f(X))]$。\n\n标签的观测遵循一个与标签相关的随机缺失（MAR）机制：对每个个体，一个指示符 $S \\in \\{0,1\\}$ 记录其标签是否被观测到。该机制满足 $S \\perp X \\mid Y$ 且对于 $y \\in \\{0,1\\}$，$s(y)=\\mathbb{P}(S=1 \\mid Y=y)$，此概率对分析师是未知的。给定两个独立的样本：\n\n- 一个大小为 $N=1000$ 的主样本，该样本是在 MAR 机制下从 $\\mathcal{P}$ 中独立同分布（i.i.d.）抽取的。在这个样本中，所有个体的 $X$ 和 $S$ 都被观测到；只有当 $S=1$ 时，$Y$ 才被观测到。在 $S=1$ 的个体中，有 $278$ 个个体的 $Y=1$，其中 $43$ 个被 $f$ 错分；有 $312$ 个个体的 $Y=0$，其中 $50$ 个被 $f$ 错分。\n\n- 一个大小为 $M=200$ 的校准样本，该样本是从 $\\mathcal{P}$ 中独立同分布（i.i.d.）抽取的，其中所有个体的 $Y$ 都被观测到（无论 $S$ 为何值），$S$ 也被记录下来。在这个校准样本中，有 $78$ 个个体的 $Y=1$，其中 $56$ 个的 $S=1$；有 $122$ 个个体的 $Y=0$，其中 $63$ 个的 $S=1$。\n\n从概率论和统计学习的核心定义出发，推导一个估计量，该估计量使用从样本中学到的逆概率加权来重构总体风险 $R(f)$，其中逆概率从校准样本中估计得出。然后，使用所提供的数据计算该估计量的数值。将您的最终答案四舍五入到四位有效数字。",
            "solution": "问题要求使用逆概率加权（IPW）方法，推导并计算分类器 $f$ 的总体风险 $R(f)$ 的一个估计量。问题提供了来自两个样本的数据以实现此目标。\n\n首先，我们来形式化目标量。对于一个分类器 $f(x)$ 和 $0$-$1$ 损失函数 $L(y, \\hat{y}) = \\mathbb{I}\\{y \\ne \\hat{y}\\}$，其总体风险 $R(f)$ 是在整个总体 $\\mathcal{P}$ 上的期望损失。\n$$R(f) = \\mathbb{E}[L(Y, f(X))] = \\mathbb{E}[\\mathbb{I}\\{Y \\ne f(X)\\}]$$\n该期望是关于 $(X, Y)$ 的联合分布计算的。使用全期望定律，我们可以将风险表示为：\n$$R(f) = \\mathbb{P}(Y=1)\\mathbb{P}(f(X) \\ne 1 \\mid Y=1) + \\mathbb{P}(Y=0)\\mathbb{P}(f(X) \\ne 0 \\mid Y=0)$$\n设 $p_y = \\mathbb{P}(Y=y)$ 为类别先验概率，$R_y(f) = \\mathbb{P}(f(X) \\ne y \\mid Y=y)$ 为类别条件风险。那么，\n$$R(f) = p_1 R_1(f) + p_0 R_0(f)$$\n问题陈述提供了总体参数 $p_1=0.4$ 和 $p_0=0.6$，以及条件分布 $X \\mid Y=1 \\sim \\mathcal{N}(2,1)$ 和 $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$，但任务是使用提供的样本推导一个估计量，而不是从这些参数计算真实的风险。\n\n在大小为 $N=1000$ 的主样本中，个体 $i$ 的真实标签 $Y_i$ 并非总是能被观测到。其观测由一个随机指示符 $S_i \\in \\{0,1\\}$ 控制。我们已知标签是随机缺失（MAR）的，这在形式上意味着标签的观测在给定真实标签 $Y$ 的条件下与特征 $X$ 是条件独立的。\n$$S \\perp X \\mid Y$$\n观测到标签的概率，也称为倾向得分，仅取决于真实标签 $y$：\n$$s(y) = \\mathbb{P}(S=1 \\mid Y=y)$$\n由于我们只能观测到 $S_i=1$ 的数据，因此仅从这个数据子集得出的风险的朴素估计是有偏的。逆概率加权是纠正这种选择偏差的一种标准技术。IPW 的原理依赖于以下恒等式：对于任意函数 $g(X,Y)$，\n$$\\mathbb{E}[g(X,Y)] = \\mathbb{E}\\left[\\frac{S}{s(Y)}g(X,Y)\\right]$$\n这由 MAR 假设保证成立：\n$$\\mathbb{E}\\left[\\frac{S}{s(Y)}g(X,Y)\\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\frac{S}{s(Y)}g(X,Y) \\mid X, Y\\right]\\right] = \\mathbb{E}\\left[\\frac{g(X,Y)}{s(Y)}\\mathbb{E}[S \\mid X, Y]\\right]$$\n根据 MAR，$\\mathbb{E}[S \\mid X, Y] = \\mathbb{P}(S=1 \\mid X,Y) = \\mathbb{P}(S=1 \\mid Y) = s(Y)$。因此，\n$$\\mathbb{E}\\left[\\frac{g(X,Y)}{s(Y)} s(Y)\\right] = \\mathbb{E}[g(X,Y)]$$\n将此应用于我们的问题，我们设 $g(X,Y) = L(Y, f(X))$。总体风险可以表示为：\n$$R(f) = \\mathbb{E}\\left[\\frac{S}{s(Y)}L(Y, f(X))\\right]$$\n这个公式启发我们基于样本 $\\{(X_i, Y_i, S_i)\\}_{i=1}^N$ 构造一个经验估计量。$R(f)$ 的一个相合估计量是加权损失的样本均值：\n$$\\hat{R}(f) = \\frac{1}{N} \\sum_{i=1}^N \\frac{S_i}{s(Y_i)}L(Y_i, f(X_i))$$\n由于倾向得分 $s(y)$ 是未知的，我们必须对它们进行估计。为此，问题提供了一个大小为 $M=200$ 的校准样本，其中 $Y$ 和 $S$ 都被完全观测到。设 $M_y$ 是校准样本中标签为 $Y=y$ 的个体数量，$M_{y1}$ 是其中 $S=1$ 的个体数量。$s(y)$ 的最大似然估计量是样本比例：\n$$\\hat{s}(y) = \\frac{M_{y1}}{M_y}$$\n根据校准样本的数据：\n- 对于 $Y=1$：$M_1 = 78$ 且 $M_{11} = 56$。所以，$\\hat{s}(1) = \\frac{56}{78}$。\n- 对于 $Y=0$：$M_0 = 122$ 且 $M_{01} = 63$。所以，$\\hat{s}(0) = \\frac{63}{122}$。\n\n将这些估计值代入风险估计量公式，我们得到实际的 IPW 估计量：\n$$\\hat{R}(f) = \\frac{1}{N} \\sum_{i=1}^N \\frac{S_i}{\\hat{s}(Y_i)} L(Y_i, f(X_i))$$\n该求和仅对 $S_i=1$ 的个体非零。我们可以根据 $Y_i$ 的值将求和分开：\n$$\\hat{R}(f) = \\frac{1}{N} \\left( \\sum_{i: S_i=1, Y_i=1} \\frac{L(1, f(X_i))}{\\hat{s}(1)} + \\sum_{i: S_i=1, Y_i=0} \\frac{L(0, f(X_i))}{\\hat{s}(0)} \\right)$$\n问题提供了主样本中已观测部分里每个类别的错分总数。\n设 $N_{11,m}$ 为 $S=1$ 和 $Y=1$ 的个体中被错分的数量。我们有 $N_{11,m} = 43$。\n设 $N_{01,m}$ 为 $S=1$ 和 $Y=0$ 的个体中被错分的数量。我们有 $N_{01,m} = 50$。\n注意 $L(y, f(X_i)) = \\mathbb{I}\\{Y_i \\ne f(X_i)\\}$，所以求和就是错分的计数：\n$$\\sum_{i: S_i=1, Y_i=1} L(1, f(X_i)) = N_{11,m}$$\n$$\\sum_{i: S_i=1, Y_i=0} L(0, f(X_i)) = N_{01,m}$$\n估计量简化为：\n$$\\hat{R}(f) = \\frac{1}{N} \\left( \\frac{N_{11,m}}{\\hat{s}(1)} + \\frac{N_{01,m}}{\\hat{s}(0)} \\right)$$\n现在我们代入问题陈述中的数值：\n- 主样本大小：$N = 1000$。\n- 主样本的错分计数：$N_{11,m} = 43$, $N_{01,m} = 50$。\n- 从校准样本估计的倾向得分：$\\hat{s}(1) = \\frac{56}{78}$ 和 $\\hat{s}(0) = \\frac{63}{122}$。\n\n将这些值代入估计量：\n$$\\hat{R}(f) = \\frac{1}{1000} \\left( \\frac{43}{56/78} + \\frac{50}{63/122} \\right)$$\n$$\\hat{R}(f) = \\frac{1}{1000} \\left( 43 \\times \\frac{78}{56} + 50 \\times \\frac{122}{63} \\right)$$\n我们可以化简分数 $\\frac{78}{56} = \\frac{39}{28}$。\n$$\\hat{R}(f) = \\frac{1}{1000} \\left( \\frac{43 \\times 39}{28} + \\frac{50 \\times 122}{63} \\right)$$\n$$\\hat{R}(f) = \\frac{1}{1000} \\left( \\frac{1677}{28} + \\frac{6100}{63} \\right)$$\n现在我们计算括号内各项的数值：\n$$\\frac{1677}{28} \\approx 59.892857$$\n$$\\frac{6100}{63} \\approx 96.825397$$\n将这些值相加：\n$$59.892857 + 96.825397 = 156.718254$$\n最后，除以 $N=1000$：\n$$\\hat{R}(f) = \\frac{156.718254}{1000} = 0.156718254$$\n问题要求答案四舍五入到四位有效数字。\n$$0.156718254 \\approx 0.1567$$\n这个值是我们对总体风险 $R(f)$ 的最终估计。",
            "answer": "$$\\boxed{0.1567}$$"
        },
        {
            "introduction": "当总体本身就存在“病态”性质（如特征间高度相关）时，抽样过程可能会让问题变得更糟，导致估计极其不稳定。本练习将展示正则化方法（如岭回归）如何为我们提供一种稳健的估计器。你将通过推导发现，即使直接求解会因样本的随机性而失败，正则化解也能稳定地逼近真实但难以捉摸的总体最优解 。",
            "id": "3159194",
            "problem": "考虑一个线性预测问题，其中协变量是二维的，损失是二次的。设协变量向量为 $x \\in \\mathbb{R}^{2}$，其总体均值为 $\\mathbb{E}[x] = 0$，总体协方差矩阵为 $\\Sigma = \\begin{pmatrix} 1   0 \\\\ 0   \\delta \\end{pmatrix}$，其中 $\\delta \\in (0,1)$ 是一个很小的常数，使得 $\\Sigma$ 近奇异。设响应为 $y = x_{1} + \\gamma x_{2}$，其中 $\\gamma \\in \\mathbb{R}$ 是一个固定常数。预测是线性的，$f_{w}(x) = x^{\\top} w$，其中 $w \\in \\mathbb{R}^{2}$，总体风险是二次损失 $L(w) = \\mathbb{E}\\big[(y - x^{\\top} w)^{2}\\big]$。\n\n您观察到从总体模型生成的 $n$ 个独立同分布 (i.i.d.) 样本 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$。设经验（样本）协方差为 $S = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} x_{i}^{\\top}$，经验互协方差为 $\\hat{s} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} y_{i}$。为了在 $S$ 近奇异时稳定估计，您应用了带有惩罚参数 $\\lambda  0$ 的岭回归正则化的经验风险最小化，即最小化\n$$\nL_{n}(w,\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\big(y_{i} - x_{i}^{\\top} w\\big)^{2} + \\lambda \\|w\\|_{2}^{2}.\n$$\n\n从协方差、互协方差和二次损失的定义出发，并仅使用基本原理（微积分和线性代数），完成以下任务：\n\n1.  根据总体协方差 $\\Sigma$ 和总体互协方差向量 $s = \\mathbb{E}[x y]$，推导出 $L(w)$ 的总体最小化子 $w_{\\text{pop}}$。然后，根据给定的 $\\Sigma$ 和 $y$ 显式计算 $w_{\\text{pop}}$，展示其对 $\\gamma$ 和 $\\delta$ 的依赖关系。\n\n2.  通过将 $L_{n}(w,\\lambda)$ 的梯度设为零并求解得到的正规方程，推导出岭回归正则化的经验最小化子 $w_{n}(\\lambda)$（用 $S$ 和 $\\hat{s}$ 表示）。使用大数定律证明，当 $n \\to \\infty$ 时，$w_{n}(\\lambda)$ 依概率收敛到一个确定性总体目标 $w_{\\lambda}$，并确定 $w_{\\lambda}$（用 $\\Sigma$、$\\lambda$ 和 $s$ 表示）。\n\n3.  对于给定的 $\\Sigma = \\begin{pmatrix} 1   0 \\\\ 0   \\delta \\end{pmatrix}$ 和对应于 $y = x_{1} + \\gamma x_{2}$ 的 $s$，显式计算 $w_{\\lambda}$。然后取极限 $\\lambda \\downarrow 0^{+}$，以展示在近奇异情况下，岭回归正则化的经验风险最小化如何以总体伪逆解为目标。将您的最终答案表示为仅含 $\\gamma$ 的极限向量。\n\n您的最终答案必须是使用规定格式写成的单个 $2 \\times 1$ 行矩阵的闭式解析表达式。无需四舍五入，不涉及单位。",
            "solution": "我们按要求分三部分进行推导。\n\n### 第一部分：总体最小化子\n\n总体风险由二次损失 $L(w) = \\mathbb{E}\\big[(y - x^{\\top} w)^{2}\\big]$ 给出。为了找到最小化子 $w_{\\text{pop}}$，我们首先展开风险的表达式：\n$$\nL(w) = \\mathbb{E}\\big[y^{2} - 2yx^{\\top}w + (x^{\\top}w)^{2}\\big]\n$$\n利用期望算子的线性性质，我们可以写出：\n$$\nL(w) = \\mathbb{E}[y^{2}] - 2\\mathbb{E}[yx^{\\top}]w + \\mathbb{E}[(x^{\\top}w)^{\\top}(x^{\\top}w)]\n$$\n$$\nL(w) = \\mathbb{E}[y^{2}] - 2\\mathbb{E}[y x^{\\top}]w + \\mathbb{E}[w^{\\top}xx^{\\top}w]\n$$\n由于 $w$ 是一个相对于期望的常数向量，我们有：\n$$\nL(w) = \\mathbb{E}[y^{2}] - 2\\mathbb{E}[xy]^{\\top}w + w^{\\top}\\mathbb{E}[xx^{\\top}]w\n$$\n我们定义总体互协方差向量 $s = \\mathbb{E}[xy]$ 和总体协方差矩阵 $\\Sigma = \\mathbb{E}[xx^{\\top}]$，这里我们利用了 $\\mathbb{E}[x] = 0$ 这一事实。风险函数变为 $w$ 的二次型：\n$$\nL(w) = \\mathbb{E}[y^{2}] - 2s^{\\top}w + w^{\\top}\\Sigma w\n$$\n为了找到最小化子，我们计算 $L(w)$ 关于 $w$ 的梯度并将其设为零。使用标准矩阵微积分结果（对于对称矩阵 $A$，有 $\\nabla_z(a^{\\top}z) = a$ 和 $\\nabla_z(z^{\\top}Az) = 2Az$），我们得到：\n$$\n\\nabla_{w} L(w) = -2s + 2\\Sigma w\n$$\n将梯度设为零，得到总体正规方程：\n$$\n2\\Sigma w = 2s \\implies \\Sigma w = s\n$$\n由于 $\\det(\\Sigma) = 1 \\cdot \\delta = \\delta  0$，矩阵 $\\Sigma$ 是可逆的。因此，总体最小化子是唯一的，由下式给出：\n$$\nw_{\\text{pop}} = \\Sigma^{-1}s\n$$\n接下来，我们为给定的模型计算向量 $s$。给定 $y = x_{1} + \\gamma x_{2}$。\n$$\ns = \\mathbb{E}[xy] = \\mathbb{E}\\left[\\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} (x_{1} + \\gamma x_{2})\\right] = \\mathbb{E}\\left[\\begin{pmatrix} x_{1}^{2} + \\gamma x_{1}x_{2} \\\\ x_{1}x_{2} + \\gamma x_{2}^{2} \\end{pmatrix}\\right]\n$$\n再次利用期望的线性性质：\n$$\ns = \\begin{pmatrix} \\mathbb{E}[x_{1}^{2}] + \\gamma \\mathbb{E}[x_{1}x_{2}] \\\\ \\mathbb{E}[x_{1}x_{2}] + \\gamma \\mathbb{E}[x_{2}^{2}] \\end{pmatrix}\n$$\n协方差矩阵 $\\Sigma = \\mathbb{E}[xx^{\\top}]$ 的分量为 $\\Sigma_{ij} = \\mathbb{E}[x_i x_j]$。给定 $\\Sigma = \\begin{pmatrix} 1   0 \\\\ 0   \\delta \\end{pmatrix}$，我们有 $\\mathbb{E}[x_{1}^{2}] = 1$，$\\mathbb{E}[x_{2}^{2}] = \\delta$ 和 $\\mathbb{E}[x_{1}x_{2}] = 0$。将这些值代入 $s$ 的表达式：\n$$\ns = \\begin{pmatrix} 1 + \\gamma(0) \\\\ 0 + \\gamma(\\delta) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\gamma\\delta \\end{pmatrix}\n$$\n现在我们可以计算 $w_{\\text{pop}}$。对角矩阵 $\\Sigma$ 的逆矩阵是 $\\Sigma^{-1} = \\begin{pmatrix} 1   0 \\\\ 0   1/\\delta \\end{pmatrix}$。\n$$\nw_{\\text{pop}} = \\Sigma^{-1}s = \\begin{pmatrix} 1   0 \\\\ 0   \\frac{1}{\\delta} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\gamma\\delta \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(\\gamma\\delta) \\\\ (0)(1) + (\\frac{1}{\\delta})(\\gamma\\delta) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\gamma \\end{pmatrix}\n$$\n\n### 第二部分：岭回归正则化的经验最小化子及其总体目标\n\n岭回归正则化的经验风险是 $L_{n}(w,\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\big(y_{i} - x_{i}^{\\top} w\\big)^{2} + \\lambda \\|w\\|_{2}^{2}$。\n我们可以使用经验协方差矩阵 $S = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} x_{i}^{\\top}$ 和经验互协方差向量 $\\hat{s} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} y_{i}$ 的定义来重写第一项。\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} (y_i - x_i^\\top w)^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i^2 - 2y_i x_i^\\top w + w^\\top x_i x_i^\\top w)\n$$\n$$\n= \\left(\\frac{1}{n}\\sum_{i=1}^{n} y_i^2\\right) - 2\\left(\\frac{1}{n}\\sum_{i=1}^{n} y_i x_i^\\top\\right)w + w^\\top\\left(\\frac{1}{n}\\sum_{i=1}^{n} x_i x_i^\\top\\right)w\n$$\n$$\n= (\\text{常数}) - 2\\hat{s}^{\\top} w + w^{\\top} S w\n$$\n完整的目标函数是：\n$$\nL_{n}(w,\\lambda) = w^{\\top} S w - 2\\hat{s}^{\\top} w + \\lambda w^{\\top}Iw + (\\text{常数}) = w^{\\top}(S + \\lambda I)w - 2\\hat{s}^{\\top} w + (\\text{常数})\n$$\n其中 $I$ 是 $2 \\times 2$ 单位矩阵。这是一个关于 $w$ 的凸二次函数。其梯度为：\n$$\n\\nabla_{w} L_{n}(w,\\lambda) = 2(S + \\lambda I)w - 2\\hat{s}\n$$\n将梯度设为零，得到正则化的正规方程：\n$$\n(S + \\lambda I)w = \\hat{s}\n$$\n由于 $S$ 是半正定的且 $\\lambda  0$，矩阵 $(S + \\lambda I)$ 是正定的，因此可逆。唯一的经验最小化子是：\n$$\nw_{n}(\\lambda) = (S + \\lambda I)^{-1}\\hat{s}\n$$\n为了找到总体目标 $w_{\\lambda}$，我们使用大数定律 (LLN)。由于样本 $\\{(x_i, y_i)\\}_{i=1}^n$ 是独立同分布的，这些样本的函数的样本均值在 $n \\to \\infty$ 时依概率收敛到它们的期望。具体来说：\n$$\nS = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} x_{i}^{\\top} \\xrightarrow{p} \\mathbb{E}[xx^{\\top}] = \\Sigma\n$$\n$$\n\\hat{s} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} y_{i} \\xrightarrow{p} \\mathbb{E}[xy] = s\n$$\n求解岭回归解的过程涉及矩阵求逆和乘法，这些是其参数的连续函数（只要待求逆的矩阵保持可逆）。根据连续映射定理，岭估计量 $w_n(\\lambda)$ 依概率收敛到由总体量 $\\Sigma$ 和 $s$ 构成的相同函数：\n$$\nw_{n}(\\lambda) = (S + \\lambda I)^{-1}\\hat{s} \\xrightarrow{p} (\\Sigma + \\lambda I)^{-1}s\n$$\n因此，岭估计量的确定性总体目标是 $w_{\\lambda} = (\\Sigma + \\lambda I)^{-1}s$。\n\n### 第三部分：正则化目标的显式极限\n\n我们现在使用前面推导出的总体量来显式计算 $w_{\\lambda}$。\n我们有 $\\Sigma = \\begin{pmatrix} 1   0 \\\\ 0   \\delta \\end{pmatrix}$，$s = \\begin{pmatrix} 1 \\\\ \\gamma\\delta \\end{pmatrix}$，以及 $\\lambda  0$。\n$$\n\\Sigma + \\lambda I = \\begin{pmatrix} 1   0 \\\\ 0   \\delta \\end{pmatrix} + \\lambda\\begin{pmatrix} 1   0 \\\\ 0   1 \\end{pmatrix} = \\begin{pmatrix} 1+\\lambda   0 \\\\ 0   \\delta+\\lambda \\end{pmatrix}\n$$\n其逆矩阵是：\n$$\n(\\Sigma + \\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{1+\\lambda}   0 \\\\ 0   \\frac{1}{\\delta+\\lambda} \\end{pmatrix}\n$$\n现在我们计算 $w_{\\lambda}$：\n$$\nw_{\\lambda} = (\\Sigma + \\lambda I)^{-1}s = \\begin{pmatrix} \\frac{1}{1+\\lambda}   0 \\\\ 0   \\frac{1}{\\delta+\\lambda} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\gamma\\delta \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1+\\lambda} \\\\ \\frac{\\gamma\\delta}{\\delta+\\lambda} \\end{pmatrix}\n$$\n最后，我们取正则化参数 $\\lambda$ 从正侧趋于零的极限（$\\lambda \\downarrow 0^{+}$）：\n$$\n\\lim_{\\lambda \\downarrow 0^{+}} w_{\\lambda} = \\lim_{\\lambda \\downarrow 0^{+}} \\begin{pmatrix} \\frac{1}{1+\\lambda} \\\\ \\frac{\\gamma\\delta}{\\delta+\\lambda} \\end{pmatrix} = \\begin{pmatrix} \\lim_{\\lambda \\to 0^{+}} \\frac{1}{1+\\lambda} \\\\ \\lim_{\\lambda \\to 0^{+}} \\frac{\\gamma\\delta}{\\delta+\\lambda} \\end{pmatrix}\n$$\n各分量的极限是：\n$$\n\\lim_{\\lambda \\to 0^{+}} \\frac{1}{1+\\lambda} = \\frac{1}{1+0} = 1\n$$\n$$\n\\lim_{\\lambda \\to 0^{+}} \\frac{\\gamma\\delta}{\\delta+\\lambda} = \\frac{\\gamma\\delta}{\\delta+0} = \\gamma\n$$\n因此，极限向量是：\n$$\n\\lim_{\\lambda \\downarrow 0^{+}} w_{\\lambda} = \\begin{pmatrix} 1 \\\\ \\gamma \\end{pmatrix}\n$$\n这个结果与总体最小化子 $w_{\\text{pop}}$ 相同。对于任何固定的 $\\delta  0$，$\\Sigma$ 是可逆的，所以它的伪逆就是其逆矩阵，$\\Sigma^{\\dagger} = \\Sigma^{-1}$。极限 $\\lim_{\\lambda \\to 0^+} w_\\lambda = \\Sigma^\\dagger s = \\Sigma^{-1}s = w_{\\text{pop}}$ 是预期的理论结果。近奇异性给基于样本的估计带来了不稳定性，而岭回归修正了这一点，但当正则化趋于零时，其总体目标仍然是真实的总体最小化子。最终结果仅取决于 $\\gamma$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 \\\\\n\\gamma\n\\end{pmatrix}\n}\n$$"
        }
    ]
}