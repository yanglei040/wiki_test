{
    "hands_on_practices": [
        {
            "introduction": "In theory, a random sample provides a miniature portrait of the population. In practice, however, sampling variation can lead to a sample whose characteristics differ meaningfully from the population it was drawn from. This exercise explores a crucial example of this divergence: multicollinearity. You will investigate a scenario where a specific sample exhibits much higher correlation between predictors than the population, and quantify the resulting inflation in the variance of your regression estimates, demonstrating how a \"bad draw\" can compromise statistical inference .",
            "id": "3159127",
            "problem": "Consider a population of feature vectors $\\mathbf{X} = (X_{1}, X_{2})^{\\top}$ with a joint distribution that is bivariate normal $\\mathcal{N}(\\mathbf{0}, \\Sigma)$, where the population covariance matrix is\n$$\n\\Sigma = \\begin{pmatrix}\n4 & 5.4 \\\\\n5.4 & 9\n\\end{pmatrix}.\n$$\nThis implies a population correlation $\\rho_{\\text{pop}} = \\frac{5.4}{\\sqrt{4 \\cdot 9}} = 0.90$, so $X_{1}$ is strongly correlated with $X_{2}$. A dataset of size $n = 200$ is collected as independent and identically distributed draws from this population. In a particular realized sample, the observed sample variances are $4$ for $X_{1}$ and $9$ for $X_{2}$, and the observed sample covariance is $5.94$, corresponding to a sample correlation $\\rho_{\\text{samp}} = \\frac{5.94}{\\sqrt{4 \\cdot 9}} = 0.99$.\n\nSuppose the response $Y$ follows the linear model\n$$\nY = \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon,\n$$\nwith $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$ independent of $\\mathbf{X}$ and $\\sigma_{\\varepsilon}^{2} = 1$. Define $\\hat{\\beta} = (\\hat{\\beta}_{1}, \\hat{\\beta}_{2})^{\\top}$ as the ordinary least squares (OLS) estimator based on the observed sample.\n\nStarting from the fundamental definitions of covariance matrices and the variance of linear regression estimators under homoscedastic errors (without invoking any pre-given shortcut formulas), derive an exact expression for $\\operatorname{Var}(\\hat{\\beta}_{1})$ in terms of the $2 \\times 2$ covariance matrix of the predictors. Then, compute the multiplicative factor by which $\\operatorname{Var}(\\hat{\\beta}_{1})$ under the realized sample design (with correlation $0.99$) exceeds the variance computed using the population covariance matrix (with correlation $0.90$), assuming the marginal variances are as stated. Provide the final factor as a single real number, and round your answer to four significant figures.",
            "solution": "The problem asks for a derivation of the variance of an Ordinary Least Squares (OLS) estimator, $\\hat{\\beta}_{1}$, and then to compute a ratio of this variance under two different scenarios for the predictor covariance structure.\n\nFirst, we derive the expression for $\\operatorname{Var}(\\hat{\\beta}_{1})$. The linear model is given in matrix form as $\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{Y}$ is the $n \\times 1$ vector of observations of the response variable, $\\mathbf{X}$ is the $n \\times p$ design matrix of predictor observations (in this case, $p=2$), $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of true coefficients, and $\\boldsymbol{\\varepsilon}$ is the $n \\times 1$ vector of errors. The errors are assumed to be independently and identically distributed with mean $0$ and variance $\\sigma_{\\varepsilon}^{2}$, i.e., $\\operatorname{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$ and $\\operatorname{Var}(\\boldsymbol{\\varepsilon}) = \\sigma_{\\varepsilon}^{2}\\mathbf{I}_{n}$, where $\\mathbf{I}_{n}$ is the $n \\times n$ identity matrix.\n\nThe OLS estimator $\\hat{\\boldsymbol{\\beta}}$ is defined as:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{Y}\n$$\nThe variance of the estimator, conditional on the design matrix $\\mathbf{X}$, is derived from $\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}$:\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\operatorname{Var}((\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon} | \\mathbf{X}) = ((\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}) \\operatorname{Var}(\\boldsymbol{\\varepsilon}) ((\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top})^{\\top}\n$$\nSubstituting $\\operatorname{Var}(\\boldsymbol{\\varepsilon}) = \\sigma_{\\varepsilon}^{2}\\mathbf{I}_{n}$ and simplifying yields the well-known result:\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\n$$\nThe problem states the population mean of predictors is $\\mathbf{0}$. For a centered design matrix, the sample covariance matrix of the predictors, $\\mathbf{S}$, is given by $\\mathbf{S} = \\frac{1}{n}\\mathbf{X}^{\\top}\\mathbf{X}$. Thus, $\\mathbf{X}^{\\top}\\mathbf{X} = n\\mathbf{S}$.\nThe variance-covariance matrix of $\\hat{\\boldsymbol{\\beta}}$ is then:\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma_{\\varepsilon}^{2} (n\\mathbf{S})^{-1} = \\frac{\\sigma_{\\varepsilon}^{2}}{n} \\mathbf{S}^{-1}\n$$\nFor this problem, the sample covariance matrix is a $2 \\times 2$ matrix $\\mathbf{S} = \\begin{pmatrix} s_{1}^{2} & s_{12} \\\\ s_{12} & s_{2}^{2} \\end{pmatrix}$. Its inverse is $\\mathbf{S}^{-1} = \\frac{1}{s_{1}^{2}s_{2}^{2} - s_{12}^{2}} \\begin{pmatrix} s_{2}^{2} & -s_{12} \\\\ -s_{12} & s_{1}^{2} \\end{pmatrix}$.\n\nThe variance of $\\hat{\\beta}_{1}$, denoted $\\operatorname{Var}(\\hat{\\beta}_{1})$, is the $(1,1)$ entry of the matrix $\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X})$:\n$$\n\\operatorname{Var}(\\hat{\\beta}_{1}) = \\frac{\\sigma_{\\varepsilon}^{2}}{n} (\\mathbf{S}^{-1})_{11} = \\frac{\\sigma_{\\varepsilon}^{2}}{n} \\left( \\frac{s_{2}^{2}}{s_{1}^{2}s_{2}^{2} - s_{12}^{2}} \\right)\n$$\nThis can be simplified using the sample correlation coefficient, $\\rho_{\\text{samp}} = \\frac{s_{12}}{\\sqrt{s_{1}^{2}s_{2}^{2}}}$. The denominator becomes $s_{1}^{2}s_{2}^{2}(1 - \\rho_{\\text{samp}}^{2})$.\n$$\n\\operatorname{Var}(\\hat{\\beta}_{1}) = \\frac{\\sigma_{\\varepsilon}^{2}}{n} \\frac{s_{2}^{2}}{s_{1}^{2}s_{2}^{2}(1 - \\rho_{\\text{samp}}^{2})} = \\frac{\\sigma_{\\varepsilon}^{2}}{n s_{1}^{2}(1 - \\rho_{\\text{samp}}^{2})}\n$$\nNow, we compute the two variances required by the problem.\n\n1.  Variance under the realized sample design, $V_{\\text{samp}}$.\nThe given sample values are $n=200$, $s_{1}^{2}=4$, $\\rho_{\\text{samp}}=0.99$, and $\\sigma_{\\varepsilon}^{2}=1$.\n$$\nV_{\\text{samp}} = \\frac{1}{200 \\cdot 4 \\cdot (1 - 0.99^{2})}\n$$\n\n2.  Variance computed using the population covariance matrix, $V_{\\text{pop}}$. This is a hypothetical calculation where the sample statistics are replaced by their corresponding population parameters. The problem states that the marginal variances are the same, $\\sigma_{1}^{2} = 4$. The population correlation is $\\rho_{\\text{pop}} = 0.90$.\n$$\nV_{\\text{pop}} = \\frac{1}{n \\sigma_{1}^{2}(1 - \\rho_{\\text{pop}}^{2})} = \\frac{1}{200 \\cdot 4 \\cdot (1 - 0.90^{2})}\n$$\n\nThe multiplicative factor by which $V_{\\text{samp}}$ exceeds $V_{\\text{pop}}$ is the ratio $\\frac{V_{\\text{samp}}}{V_{\\text{pop}}}$:\n$$\n\\text{Factor} = \\frac{\\frac{\\sigma_{\\varepsilon}^{2}}{n s_{1}^{2}(1 - \\rho_{\\text{samp}}^{2})}}{\\frac{\\sigma_{\\varepsilon}^{2}}{n \\sigma_{1}^{2}(1 - \\rho_{\\text{pop}}^{2})}}\n$$\nSince $\\sigma_{\\varepsilon}^{2}=1$, $n=200$, and $s_{1}^{2}=\\sigma_{1}^{2}=4$, these terms cancel out:\n$$\n\\text{Factor} = \\frac{1 - \\rho_{\\text{pop}}^{2}}{1 - \\rho_{\\text{samp}}^{2}}\n$$\nSubstituting the given correlation values: $\\rho_{\\text{pop}}=0.90$ and $\\rho_{\\text{samp}}=0.99$.\n$$\n\\text{Factor} = \\frac{1 - 0.90^{2}}{1 - 0.99^{2}} = \\frac{1 - 0.81}{1 - 0.9801} = \\frac{0.19}{0.0199}\n$$\nCalculating the numerical value:\n$$\n\\text{Factor} = 9.54773869...\n$$\nRounding to four significant figures, the factor is $9.548$.",
            "answer": "$$\\boxed{9.548}$$"
        },
        {
            "introduction": "Beyond random sampling error, systematic biases can arise from the data collection process itself, creating a sample that is fundamentally unrepresentative of the population. This problem tackles a common form of this issue known as sample selection bias, or truncation. By analyzing a hypothetical study where data is only recorded for extreme outcomes, you will derive how this biased sampling mechanism distorts the estimated relationship between variables and develop a method to correct for it, recovering the true population slope .",
            "id": "3159162",
            "problem": "A researcher wishes to study a linear relationship between a predictor variable $X$ and a response variable $Y$ in a large population, but the sampling mechanism records individuals only when the observed response satisfies $|Y| \\ge \\tau$ (threshold sampling). The population joint distribution of $(X,Y)$ is assumed to be bivariate normal with mean vector $(0,0)$, variances $\\operatorname{Var}(X) = \\sigma_{X}^{2}$ and $\\operatorname{Var}(Y) = \\sigma_{Y}^{2}$, and correlation $\\rho$. The population slope in the linear regression of $Y$ on $X$ is defined as $\\beta_{\\text{pop}} = \\operatorname{Cov}(X,Y)/\\operatorname{Var}(X)$. The researcher fits the linear model $Y$ on $X$ by Ordinary Least Squares (OLS) in the thresholded sample (that is, conditional on $|Y| \\ge \\tau$) and obtains a slope estimate $\\hat{b}_{t}$. Assume the sample size is sufficiently large that $\\hat{b}_{t}$ equals its expectation under threshold sampling.\n\nStarting from core definitions of covariance, variance, and conditional expectation, and using well-tested facts about the normal distribution, derive the expected OLS slope in the thresholded sample as a function of the population correlation $\\rho$, the variances $\\sigma_{X}^{2}$ and $\\sigma_{Y}^{2}$, and the threshold $\\tau$. Then invert this relationship to express the population slope $\\beta_{\\text{pop}}$ in terms of the observed threshold-sample slope $\\hat{b}_{t}$ and the known population marginal variances and threshold. Finally, compute the corrected population slope $\\beta_{\\text{pop}}$ for the following scientifically plausible specifications:\n- $\\sigma_{X} = 10$,\n- $\\sigma_{Y} = 10$,\n- $\\tau = 10$,\n- $\\hat{b}_{t} = 0.6000$.\n\nRound your final numerical answer to four significant figures.",
            "solution": "The goal is to derive the relationship between the population OLS slope, $\\beta_{\\text{pop}}$, and the slope estimated from a sample truncated on the response variable, $b_t$. We assume the sample is large enough that the estimate $\\hat{b}_t$ equals its expectation $b_t = E[ \\text{OLS slope} | |Y| \\ge \\tau ]$.\n\nThe slope in the truncated sample is $b_t = \\frac{\\operatorname{Cov}(X,Y | |Y| \\ge \\tau)}{\\operatorname{Var}(X | |Y| \\ge \\tau)}$. Let $E_t[\\cdot]$ denote the expectation conditional on $|Y| \\ge \\tau$. Since the joint distribution of $(X,Y)$ is a centered bivariate normal and the truncation condition is symmetric, the conditional means $E_t[X]$ and $E_t[Y]$ are both zero. The slope simplifies to $b_t = \\frac{E_t[XY]}{E_t[X^2]}$.\n\nUsing the law of total expectation and properties of the bivariate normal distribution ($E[X|Y=y] = \\rho \\frac{\\sigma_X}{\\sigma_Y}y$ and $\\operatorname{Var}(X|Y=y) = \\sigma_X^2(1-\\rho^2)$), we can express the numerator and denominator in terms of the conditional second moment of $Y$:\n$$E_t[XY] = E[E[XY|Y] | |Y|\\ge\\tau] = E\\left[Y \\left(\\rho \\frac{\\sigma_X}{\\sigma_Y}Y\\right) \\middle| |Y| \\ge \\tau\\right] = \\rho \\frac{\\sigma_X}{\\sigma_Y} E_t[Y^2]$$\n$$E_t[X^2] = E[E[X^2|Y] | |Y|\\ge\\tau] = E[\\operatorname{Var}(X|Y) + (E[X|Y])^2 | |Y|\\ge\\tau] = \\sigma_X^2(1-\\rho^2) + \\rho^2 \\frac{\\sigma_X^2}{\\sigma_Y^2} E_t[Y^2]$$\n\nSubstituting these into the slope expression gives:\n$$b_t = \\frac{\\rho \\frac{\\sigma_X}{\\sigma_Y} E_t[Y^2]}{\\sigma_X^2(1-\\rho^2) + \\rho^2 \\frac{\\sigma_X^2}{\\sigma_Y^2} E_t[Y^2]}$$\nLet's define $C = E_t[Y^2]/\\sigma_Y^2$. The population slope is $\\beta_{\\text{pop}} = \\rho \\frac{\\sigma_Y}{\\sigma_X}$. Rearranging the expression for $b_t$ in terms of $\\beta_{\\text{pop}}$ and $C$:\n$$b_t = \\beta_{\\text{pop}} \\frac{C}{1-\\rho^2 + \\rho^2 C} = \\beta_{\\text{pop}} \\frac{C}{1 + \\rho^2(C-1)}$$\nThe term $C$ is the conditional variance of a standard normal variable $Z=Y/\\sigma_Y$ given $|Z| \\ge \\alpha = \\tau/\\sigma_Y$. A standard result for truncated normal distributions is that $C = E[Z^2 | |Z| \\ge \\alpha] = 1 + \\alpha\\lambda(\\alpha)$, where $\\lambda(\\alpha) = \\phi(\\alpha)/(1-\\Phi(\\alpha))$ is the inverse Mills ratio.\n\nTo find $\\beta_{\\text{pop}}$, we substitute $\\rho = \\beta_{\\text{pop}} \\frac{\\sigma_X}{\\sigma_Y}$ back into the equation for $b_t$ and solve for $\\beta_{\\text{pop}}$. This leads to a quadratic equation:\n$$b_t (C-1) \\left(\\frac{\\sigma_X}{\\sigma_Y}\\right)^2 \\beta_{\\text{pop}}^2 - C \\beta_{\\text{pop}} + b_t = 0$$\nThe physically meaningful solution, which recovers $\\beta_{\\text{pop}} = b_t$ in the limit of no truncation ($\\tau \\to 0, C \\to 1$), is given by the smaller root:\n$$\\beta_{\\text{pop}} = \\frac{C - \\sqrt{C^2 - 4 b_t^2 (C-1) (\\sigma_X/\\sigma_Y)^2}}{2 b_t (C-1) (\\sigma_X/\\sigma_Y)^2}$$\n\nWe are given $\\sigma_X = 10$, $\\sigma_Y = 10$, $\\tau = 10$, and $\\hat{b}_t = 0.6000$.\nFirst, compute the constants. The ratio $\\sigma_X/\\sigma_Y = 1$. The standardized threshold is $\\alpha = \\tau/\\sigma_Y = 10/10 = 1$.\nWe calculate $C = 1 + \\lambda(1)$, using standard values for the normal distribution:\n$\\phi(1) = \\frac{1}{\\sqrt{2\\pi}}e^{-0.5} \\approx 0.24197$\n$\\Phi(1) \\approx 0.84134$\n$1-\\Phi(1) \\approx 0.15866$\n$\\lambda(1) = \\frac{0.24197}{0.15866} \\approx 1.52515$\n$C = 1 + 1.52515 = 2.52515$.\nNow, we substitute these values into the formula for $\\beta_{\\text{pop}}$ with $\\hat{b}_t = 0.6$:\n$$\\beta_{\\text{pop}} = \\frac{C - \\sqrt{C^2 - 4 \\hat{b}_t^2 (C-1)}}{2 \\hat{b}_t (C-1)}$$\n$$\\beta_{\\text{pop}} = \\frac{2.52515 - \\sqrt{(2.52515)^2 - 4 (0.6)^2 (1.52515)}}{2 (0.6) (1.52515)}$$\n$$\\beta_{\\text{pop}} = \\frac{2.52515 - \\sqrt{6.37639 - 4(0.36)(1.52515)}}{1.83018}$$\n$$\\beta_{\\text{pop}} = \\frac{2.52515 - \\sqrt{6.37639 - 2.19622}}{1.83018} = \\frac{2.52515 - \\sqrt{4.18017}}{1.83018}$$\n$$\\beta_{\\text{pop}} = \\frac{2.52515 - 2.04455}{1.83018} = \\frac{0.4806}{1.83018} \\approx 0.26260$$\nRounding to four significant figures, we get $\\beta_{\\text{pop}} = 0.2626$. As expected, the true population slope is smaller in magnitude than the slope observed in the truncated sample.",
            "answer": "$$\\boxed{0.2626}$$"
        },
        {
            "introduction": "Real-world datasets are often incomplete, with missing values that can hinder our ability to estimate population-level quantities like the true risk of a classifier. When data is missing in a systematic, label-dependent way, simply analyzing the complete cases leads to biased results. This exercise introduces a powerful technique, inverse probability weighting (IPW), to address this challenge. You will use a separate, fully-observed calibration sample to learn the missingness mechanism and then use that knowledge to re-weight an incomplete main sample, allowing you to reconstruct an unbiased estimate of the population risk .",
            "id": "3159121",
            "problem": "Consider a binary classification population $\\mathcal{P}$ defined as follows. Let the label $Y \\in \\{0,1\\}$ satisfy $\\mathbb{P}(Y=1)=0.4$ and $\\mathbb{P}(Y=0)=0.6$. Conditional on $Y$, the feature $X \\in \\mathbb{R}$ is distributed as $X \\mid Y=1 \\sim \\mathcal{N}(2,1)$ and $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$. For a fixed classifier $f(x)=\\mathbb{I}\\{x \\ge 1\\}$ and the $0$-$1$ loss $L(y,\\hat{y})=\\mathbb{I}\\{y \\ne \\hat{y}\\}$, the population risk is $R(f)=\\mathbb{E}[L(Y,f(X))]$.\n\nLabels are observed subject to a label-dependent Missing At Random (MAR) mechanism: for each individual, an indicator $S \\in \\{0,1\\}$ records whether the label is observed. The mechanism satisfies $S \\perp X \\mid Y$ and $s(y)=\\mathbb{P}(S=1 \\mid Y=y)$ for $y \\in \\{0,1\\}$, unknown to the analyst. You are given two independent samples:\n\n- A main sample of size $N=1000$ drawn independently and identically distributed (i.i.d.) from $\\mathcal{P}$ under the MAR mechanism. In this sample, $X$ and $S$ are observed for all individuals; $Y$ is observed only when $S=1$. Among those with $S=1$, there are $278$ individuals with $Y=1$, of whom $43$ are misclassified by $f$, and $312$ individuals with $Y=0$, of whom $50$ are misclassified by $f$.\n\n- A calibration sample of size $M=200$ drawn i.i.d. from $\\mathcal{P}$ in which $Y$ is observed for all individuals regardless of $S$, and $S$ is also recorded. In this calibration sample, $78$ individuals have $Y=1$, of whom $56$ have $S=1$, and $122$ individuals have $Y=0$, of whom $63$ have $S=1$.\n\nStarting from core definitions in probability and statistical learning, derive an estimator that reconstructs the population risk $R(f)$ using inverse probability weighting learned from samples, where the inverse probabilities are estimated from the calibration sample. Then compute the numerical value of this estimator using the provided data. Round your final answer to four significant figures.",
            "solution": "The problem asks for the derivation and computation of an estimator for the population risk $R(f)$ of a classifier $f$ using inverse probability weighting (IPW). The problem provides data from two samples to achieve this.\n\nFirst, let us formalize the target quantity. The population risk $R(f)$ is the expected $0$-$1$ loss over the entire population $\\mathcal{P}$:\n$$R(f) = \\mathbb{E}[L(Y, f(X))] = \\mathbb{E}[\\mathbb{I}\\{Y \\ne f(X)\\}]$$\nIn the main sample, the true label $Y_i$ is observed only when an indicator $S_i=1$. The observation probability, or propensity score, depends on the true label: $s(y) = \\mathbb{P}(S=1 \\mid Y=y)$. This introduces selection bias.\n\nInverse probability weighting corrects for this bias. The principle relies on the identity $\\mathbb{E}[g(X,Y)] = \\mathbb{E}\\left[\\frac{S}{s(Y)}g(X,Y)\\right]$, which holds under the given Missing At Random (MAR) assumption ($S \\perp X \\mid Y$). Setting $g(X,Y) = L(Y, f(X))$, the population risk can be expressed as:\n$$R(f) = \\mathbb{E}\\left[\\frac{S}{s(Y)}L(Y, f(X))\\right]$$\nThis suggests an empirical estimator based on a sample of size $N$:\n$$\\hat{R}(f) = \\frac{1}{N} \\sum_{i=1}^N \\frac{S_i}{s(Y_i)}L(Y_i, f(X_i))$$\nThe propensity scores $s(y)$ are unknown and must be estimated. The calibration sample, where both $Y$ and $S$ are fully observed, is used for this. Let $M_y$ be the number of individuals in the calibration sample with label $Y=y$, and $M_{y1}$ be the number among them for whom $S=1$. The maximum likelihood estimator for $s(y)$ is the sample proportion:\n$$\\hat{s}(y) = \\frac{M_{y1}}{M_y}$$\nFrom the calibration sample data ($M=200$):\n- For $Y=1$: $M_1 = 78$ and $M_{11} = 56$. So, $\\hat{s}(1) = \\frac{56}{78}$.\n- For $Y=0$: $M_0 = 122$ and $M_{01} = 63$. So, $\\hat{s}(0) = \\frac{63}{122}$.\n\nWe substitute these estimates into the risk estimator. The sum is non-zero only for individuals in the main sample with $S_i=1$. We can group terms by the value of $Y_i$:\n$$\\hat{R}(f) = \\frac{1}{N} \\left( \\sum_{i: S_i=1, Y_i=1} \\frac{L(1, f(X_i))}{\\hat{s}(1)} + \\sum_{i: S_i=1, Y_i=0} \\frac{L(0, f(X_i))}{\\hat{s}(0)} \\right)$$\nThe problem provides the total number of misclassifications for each class in the observed portion of the main sample. Let $N_{11,m}$ be the number of misclassified individuals with $S=1, Y=1$, and $N_{01,m}$ be the number with $S=1, Y=0$.\nWe are given $N_{11,m} = 43$ and $N_{01,m} = 50$.\nThe sums are simply these counts: $\\sum_{i: S_i=1, Y_i=1} L(1, f(X_i)) = N_{11,m}$ and $\\sum_{i: S_i=1, Y_i=0} L(0, f(X_i)) = N_{01,m}$.\nThe estimator simplifies to:\n$$\\hat{R}(f) = \\frac{1}{N} \\left( \\frac{N_{11,m}}{\\hat{s}(1)} + \\frac{N_{01,m}}{\\hat{s}(0)} \\right)$$\nNow we substitute the numerical values from the problem statement:\n- Main sample size: $N = 1000$.\n- Misclassification counts from main sample: $N_{11,m} = 43$, $N_{01,m} = 50$.\n- Estimated propensity scores from calibration sample: $\\hat{s}(1) = \\frac{56}{78}$ and $\\hat{s}(0) = \\frac{63}{122}$.\n\nPlugging these into the estimator:\n$$\\hat{R}(f) = \\frac{1}{1000} \\left( \\frac{43}{56/78} + \\frac{50}{63/122} \\right)$$\n$$\\hat{R}(f) = \\frac{1}{1000} \\left( 43 \\times \\frac{78}{56} + 50 \\times \\frac{122}{63} \\right)$$\n$$\\hat{R}(f) = \\frac{1}{1000} \\left( \\frac{3354}{56} + \\frac{6100}{63} \\right)$$\nNow we compute the numerical values of the terms inside the parenthesis:\n$$\\frac{3354}{56} \\approx 59.892857$$\n$$\\frac{6100}{63} \\approx 96.825397$$\nSumming these values:\n$$59.892857 + 96.825397 = 156.718254$$\nFinally, dividing by $N=1000$:\n$$\\hat{R}(f) = \\frac{156.718254}{1000} = 0.156718254$$\nThe problem requires the answer to be rounded to four significant figures.\n$$0.156718254 \\approx 0.1567$$\nThis value is our final estimate for the population risk $R(f)$.",
            "answer": "$$\\boxed{0.1567}$$"
        }
    ]
}