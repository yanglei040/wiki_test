{
    "hands_on_practices": [
        {
            "introduction": "In statistical learning, we build models from samples, but our ultimate goal is to understand the entire population. This first exercise explores the crucial gap between sample statistics and population parameters. You will investigate a scenario where a random sample happens to exhibit much stronger collinearity than the population from which it was drawn, and quantify how this sampling fluctuation dramatically increases the uncertainty of your regression estimates .",
            "id": "3159127",
            "problem": "Consider a population of feature vectors $\\mathbf{X} = (X_{1}, X_{2})^{\\top}$ with a joint distribution that is bivariate normal $\\mathcal{N}(\\mathbf{0}, \\Sigma)$, where the population covariance matrix is\n$$\n\\Sigma = \\begin{pmatrix}\n4 & 5.4 \\\\\n5.4 & 9\n\\end{pmatrix}.\n$$\nThis implies a population correlation $\\rho_{\\text{pop}} = \\frac{5.4}{\\sqrt{4 \\cdot 9}} = 0.90$, so $X_{1}$ is strongly correlated with $X_{2}$. A dataset of size $n = 200$ is collected as independent and identically distributed draws from this population. In a particular realized sample, the observed sample variances are $4$ for $X_{1}$ and $9$ for $X_{2}$, and the observed sample covariance is $5.94$, corresponding to a sample correlation $\\rho_{\\text{samp}} = \\frac{5.94}{\\sqrt{4 \\cdot 9}} = 0.99$.\n\nSuppose the response $Y$ follows the linear model\n$$\nY = \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\varepsilon,\n$$\nwith $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$ independent of $\\mathbf{X}$ and $\\sigma_{\\varepsilon}^{2} = 1$. Define $\\hat{\\beta} = (\\hat{\\beta}_{1}, \\hat{\\beta}_{2})^{\\top}$ as the ordinary least squares (OLS) estimator based on the observed sample.\n\nStarting from the fundamental definitions of covariance matrices and the variance of linear regression estimators under homoscedastic errors (without invoking any pre-given shortcut formulas), derive an exact expression for $\\operatorname{Var}(\\hat{\\beta}_{1})$ in terms of the $2 \\times 2$ covariance matrix of the predictors. Then, compute the multiplicative factor by which $\\operatorname{Var}(\\hat{\\beta}_{1})$ under the realized sample design (with correlation $0.99$) exceeds the variance computed using the population covariance matrix (with correlation $0.90$), assuming the marginal variances are as stated. Provide the final factor as a single real number, and round your answer to four significant figures.",
            "solution": "The problem asks for a derivation of the variance of an Ordinary Least Squares (OLS) estimator, $\\hat{\\beta}_{1}$, and then to compute a ratio of this variance under two different scenarios for the predictor covariance structure.\n\nFirst, we derive the expression for $\\operatorname{Var}(\\hat{\\beta}_{1})$. The linear model is given in matrix form as $\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{Y}$ is the $n \\times 1$ vector of observations of the response variable, $\\mathbf{X}$ is the $n \\times p$ design matrix of predictor observations (in this case, $p=2$), $\\boldsymbol{\\beta}$ is the $p \\times 1$ vector of true coefficients, and $\\boldsymbol{\\varepsilon}$ is the $n \\times 1$ vector of errors. The errors are assumed to be independently and identically distributed with mean $0$ and variance $\\sigma_{\\varepsilon}^{2}$, i.e., $\\operatorname{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$ and $\\operatorname{Var}(\\boldsymbol{\\varepsilon}) = \\sigma_{\\varepsilon}^{2}\\mathbf{I}_{n}$, where $\\mathbf{I}_{n}$ is the $n \\times n$ identity matrix.\n\nThe OLS estimator $\\hat{\\boldsymbol{\\beta}}$ is defined as:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{Y}\n$$\nTo find the variance of $\\hat{\\boldsymbol{\\beta}}$, we first express it in terms of the true coefficients $\\boldsymbol{\\beta}$ and the error term $\\boldsymbol{\\varepsilon}$:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon} = \\boldsymbol{\\beta} + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon}\n$$\nThe variance of the estimator, conditional on the design matrix $\\mathbf{X}$, is:\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\operatorname{Var}(\\boldsymbol{\\beta} + (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon} | \\mathbf{X})\n$$\nSince $\\boldsymbol{\\beta}$ is a constant vector and we are conditioning on $\\mathbf{X}$, this simplifies to:\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\operatorname{Var}((\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\varepsilon} | \\mathbf{X})\n$$\nUsing the property that for a matrix $\\mathbf{A}$ and a random vector $\\mathbf{z}$, $\\operatorname{Var}(\\mathbf{A}\\mathbf{z}) = \\mathbf{A}\\operatorname{Var}(\\mathbf{z})\\mathbf{A}^{\\top}$, we have:\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = ((\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}) \\operatorname{Var}(\\boldsymbol{\\varepsilon}) ((\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top})^{\\top}\n$$\nSubstituting $\\operatorname{Var}(\\boldsymbol{\\varepsilon}) = \\sigma_{\\varepsilon}^{2}\\mathbf{I}_{n}$ and using properties of the transpose, we get:\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{I}_{n}\\mathbf{X}((\\mathbf{X}^{\\top}\\mathbf{X})^{-1})^{\\top} = \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{X}((\\mathbf{X}^{\\top}\\mathbf{X})^{\\top})^{-1}\n$$\nSince $\\mathbf{X}^{\\top}\\mathbf{X}$ is a symmetric matrix, $(\\mathbf{X}^{\\top}\\mathbf{X})^{\\top} = \\mathbf{X}^{\\top}\\mathbf{X}$, which gives:\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{X}(\\mathbf{X}^{\\top}\\mathbf{X})^{-1} = \\sigma_{\\varepsilon}^{2} (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\n$$\nThe problem states the population mean of predictors is $\\mathbf{0}$. For a centered design matrix, the sample covariance matrix of the predictors, $\\mathbf{S}$, is given by $\\mathbf{S} = \\frac{1}{n}\\mathbf{X}^{\\top}\\mathbf{X}$. Thus, $\\mathbf{X}^{\\top}\\mathbf{X} = n\\mathbf{S}$.\nThe variance-covariance matrix of $\\hat{\\boldsymbol{\\beta}}$ is then:\n$$\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma_{\\varepsilon}^{2} (n\\mathbf{S})^{-1} = \\frac{\\sigma_{\\varepsilon}^{2}}{n} \\mathbf{S}^{-1}\n$$\nFor this problem, the sample covariance matrix is a $2 \\times 2$ matrix:\n$$\n\\mathbf{S} = \\begin{pmatrix} s_{1}^{2} & s_{12} \\\\ s_{12} & s_{2}^{2} \\end{pmatrix}\n$$\nwhere $s_{1}^{2}$ and $s_{2}^{2}$ are the sample variances of $X_{1}$ and $X_{2}$, and $s_{12}$ is their sample covariance.\nThe inverse of $\\mathbf{S}$ is:\n$$\n\\mathbf{S}^{-1} = \\frac{1}{s_{1}^{2}s_{2}^{2} - s_{12}^{2}} \\begin{pmatrix} s_{2}^{2} & -s_{12} \\\\ -s_{12} & s_{1}^{2} \\end{pmatrix}\n$$\nThe variance of $\\hat{\\beta}_{1}$, denoted $\\operatorname{Var}(\\hat{\\beta}_{1})$, is the $(1,1)$ entry of the matrix $\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X})$:\n$$\n\\operatorname{Var}(\\hat{\\beta}_{1}) = \\frac{\\sigma_{\\varepsilon}^{2}}{n} (\\mathbf{S}^{-1})_{11} = \\frac{\\sigma_{\\varepsilon}^{2}}{n} \\left( \\frac{s_{2}^{2}}{s_{1}^{2}s_{2}^{2} - s_{12}^{2}} \\right)\n$$\nThis is the required expression. We can simplify this using the sample correlation coefficient, $\\rho_{\\text{samp}} = \\frac{s_{12}}{\\sqrt{s_{1}^{2}s_{2}^{2}}}$. The denominator can be rewritten as:\n$$\ns_{1}^{2}s_{2}^{2} - s_{12}^{2} = s_{1}^{2}s_{2}^{2} - (\\rho_{\\text{samp}}\\sqrt{s_{1}^{2}s_{2}^{2}})^{2} = s_{1}^{2}s_{2}^{2}(1 - \\rho_{\\text{samp}}^{2})\n$$\nSubstituting this back into the expression for the variance:\n$$\n\\operatorname{Var}(\\hat{\\beta}_{1}) = \\frac{\\sigma_{\\varepsilon}^{2}}{n} \\frac{s_{2}^{2}}{s_{1}^{2}s_{2}^{2}(1 - \\rho_{\\text{samp}}^{2})} = \\frac{\\sigma_{\\varepsilon}^{2}}{n s_{1}^{2}(1 - \\rho_{\\text{samp}}^{2})}\n$$\nNow, we compute the two variances required by the problem.\n\n$1$. Variance under the realized sample design, which we denote $V_{\\text{samp}}$.\nThe given sample values are $n=200$, $s_{1}^{2}=4$, $\\rho_{\\text{samp}}=0.99$, and $\\sigma_{\\varepsilon}^{2}=1$.\n$$\nV_{\\text{samp}} = \\frac{1}{200 \\cdot 4 \\cdot (1 - 0.99^{2})}\n$$\n\n$2$. Variance computed using the population covariance matrix, which we denote $V_{\\text{pop}}$. This is a hypothetical calculation where the sample statistics are replaced by their corresponding population parameters. The problem states that the marginal variances are the same, $\\sigma_{1}^{2} = 4$. The population correlation is $\\rho_{\\text{pop}} = 0.90$.\n$$\nV_{\\text{pop}} = \\frac{1}{n \\sigma_{1}^{2}(1 - \\rho_{\\text{pop}}^{2})} = \\frac{1}{200 \\cdot 4 \\cdot (1 - 0.90^{2})}\n$$\n\nThe multiplicative factor by which $V_{\\text{samp}}$ exceeds $V_{\\text{pop}}$ is the ratio $\\frac{V_{\\text{samp}}}{V_{\\text{pop}}}$:\n$$\n\\text{Factor} = \\frac{\\frac{\\sigma_{\\varepsilon}^{2}}{n s_{1}^{2}(1 - \\rho_{\\text{samp}}^{2})}}{\\frac{\\sigma_{\\varepsilon}^{2}}{n \\sigma_{1}^{2}(1 - \\rho_{\\text{pop}}^{2})}}\n$$\nSince $\\sigma_{\\varepsilon}^{2}=1$, $n=200$, and $s_{1}^{2}=\\sigma_{1}^{2}=4$, these terms cancel out:\n$$\n\\text{Factor} = \\frac{1 - \\rho_{\\text{pop}}^{2}}{1 - \\rho_{\\text{samp}}^{2}}\n$$\nSubstituting the given correlation values: $\\rho_{\\text{pop}}=0.90$ and $\\rho_{\\text{samp}}=0.99$.\n$$\n\\text{Factor} = \\frac{1 - 0.90^{2}}{1 - 0.99^{2}} = \\frac{1 - 0.81}{1 - 0.9801} = \\frac{0.19}{0.0199}\n$$\nCalculating the numerical value:\n$$\n\\text{Factor} = 9.54773869...\n$$\nRounding to four significant figures, the factor is $9.548$.",
            "answer": "$$\\boxed{9.548}$$"
        },
        {
            "introduction": "Beyond random fluctuations, our data can suffer from systematic biases introduced by the sampling process itself, leading to incorrect conclusions about the population. This practice confronts a classic example of sample selection bias, where data is only recorded for extreme outcomes. By deriving the relationship between the biased sample slope and the true population slope, you will learn a powerful technique for identifying and correcting such distortions .",
            "id": "3159162",
            "problem": "A researcher wishes to study a linear relationship between a predictor variable $X$ and a response variable $Y$ in a large population, but the sampling mechanism records individuals only when the observed response satisfies $|Y| \\ge \\tau$ (threshold sampling). The population joint distribution of $(X,Y)$ is assumed to be bivariate normal with mean vector $(0,0)$, variances $\\operatorname{Var}(X) = \\sigma_{X}^{2}$ and $\\operatorname{Var}(Y) = \\sigma_{Y}^{2}$, and correlation $\\rho$. The population slope in the linear regression of $Y$ on $X$ is defined as $\\beta_{\\text{pop}} = \\operatorname{Cov}(X,Y)/\\operatorname{Var}(X)$. The researcher fits the linear model $Y$ on $X$ by Ordinary Least Squares (OLS) in the thresholded sample (that is, conditional on $|Y| \\ge \\tau$) and obtains a slope estimate $\\hat{b}_{t}$. Assume the sample size is sufficiently large that $\\hat{b}_{t}$ equals its expectation under threshold sampling.\n\nStarting from core definitions of covariance, variance, and conditional expectation, and using well-tested facts about the normal distribution, derive the expected OLS slope in the thresholded sample as a function of the population correlation $\\rho$, the variances $\\sigma_{X}^{2}$ and $\\sigma_{Y}^{2}$, and the threshold $\\tau$. Then invert this relationship to express the population slope $\\beta_{\\text{pop}}$ in terms of the observed threshold-sample slope $\\hat{b}_{t}$ and the known population marginal variances and threshold. Finally, compute the corrected population slope $\\beta_{\\text{pop}}$ for the following scientifically plausible specifications:\n- $\\sigma_{X} = 10$,\n- $\\sigma_{Y} = 10$,\n- $\\tau = 10$,\n- $\\hat{b}_{t} = 0.6000$.\n\nRound your final numerical answer to four significant figures.",
            "solution": "The goal is to find the population slope $\\beta_{\\text{pop}}$ from the observed slope $\\hat{b}_t$ in a sample truncated by $|Y| \\ge \\tau$. Assuming a large sample, $\\hat{b}_t$ equals its expectation $b_t$.\n\nThe OLS slope in the truncated sample is given by the ratio of the conditional covariance to the conditional variance:\n$$b_t = \\frac{\\operatorname{Cov}(X,Y | |Y| \\ge \\tau)}{\\operatorname{Var}(X | |Y| \\ge \\tau)}$$\nLet the subscript $t$ denote expectation conditional on $|Y| \\ge \\tau$, i.e., $E_t[\\cdot] = E[\\cdot | |Y| \\ge \\tau]$. Then:\n$$b_t = \\frac{E_t[XY] - E_t[X]E_t[Y]}{E_t[X^2] - (E_t[X])^2}$$\nSince $(X,Y)$ is a centered bivariate normal, the distribution is symmetric about the origin. The truncation condition $|Y| \\ge \\tau$ is also symmetric. Due to this symmetry, the conditional expectations of $X$ and $Y$ are zero:\n$E_t[Y] = E[Y | |Y| \\ge \\tau] = 0$.\n$E_t[X] = E[E[X|Y] | |Y| \\ge \\tau] = E\\left[\\rho \\frac{\\sigma_X}{\\sigma_Y}Y \\middle| |Y| \\ge \\tau\\right] = \\rho \\frac{\\sigma_X}{\\sigma_Y} E_t[Y] = 0$.\n\nThe slope expression simplifies to:\n$$b_t = \\frac{E_t[XY]}{E_t[X^2]}$$\nWe compute the numerator and denominator using the law of total expectation.\nFor the numerator:\n$$E_t[XY] = E[XY | |Y| \\ge \\tau] = E[Y \\cdot E[X|Y] | |Y| \\ge \\tau] = E\\left[Y \\left(\\rho \\frac{\\sigma_X}{\\sigma_Y}Y\\right) \\middle| |Y| \\ge \\tau\\right] = \\rho \\frac{\\sigma_X}{\\sigma_Y} E_t[Y^2]$$\nFor the denominator, we use $E[X^2|Y] = \\operatorname{Var}(X|Y) + (E[X|Y])^2 = \\sigma_X^2(1-\\rho^2) + \\left(\\rho \\frac{\\sigma_X}{\\sigma_Y}Y\\right)^2$:\n$$E_t[X^2] = E\\left[\\sigma_X^2(1-\\rho^2) + \\rho^2 \\frac{\\sigma_X^2}{\\sigma_Y^2}Y^2 \\middle| |Y| \\ge \\tau\\right] = \\sigma_X^2(1-\\rho^2) + \\rho^2 \\frac{\\sigma_X^2}{\\sigma_Y^2} E_t[Y^2]$$\nSubstituting these back, and using $\\beta_{\\text{pop}} = \\rho \\frac{\\sigma_Y}{\\sigma_X}$:\n$$b_t = \\frac{\\rho \\frac{\\sigma_X}{\\sigma_Y} E_t[Y^2]}{\\sigma_X^2(1-\\rho^2) + \\rho^2 \\frac{\\sigma_X^2}{\\sigma_Y^2} E_t[Y^2]} = \\frac{\\beta_{\\text{pop}} \\frac{E_t[Y^2]}{\\sigma_Y^2}}{1-\\rho^2 + \\rho^2 \\frac{E_t[Y^2]}{\\sigma_Y^2}}$$\nLet $C = E_t[Y^2]/\\sigma_Y^2$. A standard result for truncated normal distributions is that $C = 1 + \\alpha\\lambda(\\alpha)$, where $\\alpha = \\tau/\\sigma_Y$ is the standardized threshold and $\\lambda(\\alpha) = \\phi(\\alpha)/(1-\\Phi(\\alpha))$ is the inverse Mills ratio. The relationship becomes:\n$$b_t = \\beta_{\\text{pop}} \\frac{C}{1-\\rho^2 + \\rho^2 C} = \\beta_{\\text{pop}} \\frac{C}{1 + \\rho^2(C-1)}$$\nWe solve this for $\\beta_{\\text{pop}}$ using $\\rho = \\beta_{\\text{pop}} \\frac{\\sigma_X}{\\sigma_Y}$, which yields a quadratic equation in $\\beta_{\\text{pop}}$:\n$$\\hat{b}_t (C-1) \\left(\\frac{\\sigma_X}{\\sigma_Y}\\right)^2 \\beta_{\\text{pop}}^2 - C \\beta_{\\text{pop}} + \\hat{b}_t = 0$$\nThe physically relevant solution (the smaller root) is:\n$$\\beta_{\\text{pop}} = \\frac{C - \\sqrt{C^2 - 4 \\hat{b}_t^2 (C-1) (\\sigma_X/\\sigma_Y)^2}}{2 \\hat{b}_t (C-1) (\\sigma_X/\\sigma_Y)^2}$$\nNow we plug in the numerical values: $\\sigma_X = 10, \\sigma_Y = 10, \\tau = 10$, and $\\hat{b}_t = 0.6000$.\n- The ratio $\\sigma_X/\\sigma_Y = 1$.\n- The standardized threshold is $\\alpha = \\tau/\\sigma_Y = 10/10 = 1$.\n- We calculate $C = 1 + \\alpha\\lambda(\\alpha) = 1 + \\lambda(1)$. Using standard values for the normal distribution: $\\phi(1) \\approx 0.24197$ and $\\Phi(1) \\approx 0.84134$.\n$$\\lambda(1) = \\frac{\\phi(1)}{1-\\Phi(1)} \\approx \\frac{0.24197}{1-0.84134} \\approx 1.5251$$\n$$C = 1 + 1.5251 = 2.5251$$\n- Substituting into the solution for $\\beta_{\\text{pop}}$:\n$$\\beta_{\\text{pop}} = \\frac{2.5251 - \\sqrt{(2.5251)^2 - 4 (0.6)^2 (1.5251)(1)^2}}{2 (0.6) (1.5251)(1)^2}$$\n$$\\beta_{\\text{pop}} = \\frac{2.5251 - \\sqrt{6.3761 - 2.1961}}{1.8301} = \\frac{2.5251 - \\sqrt{4.1800}}{1.8301} = \\frac{2.5251 - 2.0445}{1.8301} \\approx 0.26260$$\nRounding to four significant figures, the corrected population slope is $0.2626$.",
            "answer": "$$\\boxed{0.2626}$$"
        },
        {
            "introduction": "When sample data is ill-conditioned—for example, due to the high collinearity explored earlier—our estimates can become highly unstable. This final exercise introduces ridge regularization as a principled method to combat this problem. You will derive the ridge solution from first principles and see how adding a small penalty term stabilizes the estimation process, allowing it to reliably target the true underlying population relationship even when a direct approach would fail .",
            "id": "3159194",
            "problem": "Consider a linear prediction problem in which the covariates are two-dimensional and the loss is quadratic. Let the covariate vector be $x \\in \\mathbb{R}^{2}$, with population mean $\\mathbb{E}[x] = 0$ and population covariance matrix $\\Sigma = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\delta \\end{pmatrix}$, where $\\delta \\in (0,1)$ is a small constant so that $\\Sigma$ is nearly singular. Let the response be $y = x_{1} + \\gamma x_{2}$, where $\\gamma \\in \\mathbb{R}$ is a fixed constant. The prediction is linear, $f_{w}(x) = x^{\\top} w$ with $w \\in \\mathbb{R}^{2}$, and the population risk is the quadratic loss $L(w) = \\mathbb{E}\\big[(y - x^{\\top} w)^{2}\\big]$.\n\nYou observe $n$ independent and identically distributed (i.i.d.) samples $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ generated from the population model. Let the empirical (sample) covariance be $S = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} x_{i}^{\\top}$ and the empirical cross-covariance be $\\hat{s} = \\frac{1}{n} \\sum_{i=1}^{n} x_{i} y_{i}$. To stabilize the estimation when $S$ is nearly singular, you apply ridge-regularized empirical risk minimization with penalty parameter $\\lambda > 0$, minimizing\n$$\nL_{n}(w,\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\big(y_{i} - x_{i}^{\\top} w\\big)^{2} + \\lambda \\|w\\|_{2}^{2}.\n$$\n\nStarting from the definitions of covariance, cross-covariance, and quadratic loss, and using only first principles (calculus and linear algebra), perform the following:\n\n1. Derive the population minimizer $w_{\\text{pop}}$ of $L(w)$ in terms of the population covariance $\\Sigma$ and the population cross-covariance vector $s = \\mathbb{E}[x y]$. Then compute $w_{\\text{pop}}$ explicitly for the given $\\Sigma$ and $y$, showing its dependence on $\\gamma$ and $\\delta$.\n\n2. Derive the ridge-regularized empirical minimizer $w_{n}(\\lambda)$ by setting the gradient of $L_{n}(w,\\lambda)$ to zero and solving the resulting normal equations in terms of $S$ and $\\hat{s}$. Justify, using the law of large numbers, that $w_{n}(\\lambda)$ converges in probability to a deterministic population target $w_{\\lambda}$ as $n \\to \\infty$, and identify $w_{\\lambda}$ in terms of $\\Sigma$, $\\lambda$, and $s$.\n\n3. Compute $w_{\\lambda}$ explicitly for the given $\\Sigma = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\delta \\end{pmatrix}$ and $s$ corresponding to $y = x_{1} + \\gamma x_{2}$. Then take the limit $\\lambda \\downarrow 0^{+}$ to show how ridge-regularized empirical minimization targets the population pseudoinverse solution in the nearly singular regime. Express your final answer as the limiting vector, expressed as a $2 \\times 1$ column matrix.\n\nYour final answer must be a single closed-form analytic expression written as a $2 \\times 1$ column matrix. No rounding is required and no units are involved.",
            "solution": "The problem is valid as it is scientifically grounded in statistical learning theory, well-posed with all necessary information provided, and stated objectively using standard mathematical formalism. We proceed with the derivation in three parts as requested.\n\n### Part 1: Population Minimizer\n\nThe population risk is given by the quadratic loss $L(w) = \\mathbb{E}\\big[(y - x^{\\top} w)^{2}\\big]$. To find the minimizer $w_{\\text{pop}}$, we first expand the expression for the risk:\n$$\nL(w) = \\mathbb{E}\\big[y^{2} - 2yx^{\\top}w + (x^{\\top}w)^{2}\\big]\n$$\nUsing the linearity of the expectation operator and matrix properties, we can write:\n$$\nL(w) = \\mathbb{E}[y^{2}] - 2\\mathbb{E}[yx^{\\top}]w + \\mathbb{E}[w^{\\top}xx^{\\top}w] = \\mathbb{E}[y^{2}] - 2\\mathbb{E}[xy]^{\\top}w + w^{\\top}\\mathbb{E}[xx^{\\top}]w\n$$\nWe identify the population cross-covariance vector $s = \\mathbb{E}[xy]$ and the population covariance matrix $\\Sigma = \\mathbb{E}[xx^{\\top}]$, where we use the fact that $\\mathbb{E}[x] = 0$. The risk function becomes a quadratic form in $w$:\n$$\nL(w) = \\mathbb{E}[y^{2}] - 2s^{\\top}w + w^{\\top}\\Sigma w\n$$\nTo find the minimizer, we compute the gradient of $L(w)$ with respect to $w$ and set it to zero.\n$$\n\\nabla_{w} L(w) = -2s + 2\\Sigma w\n$$\nSetting the gradient to zero gives the normal equations for the population: $\\Sigma w = s$. Since $\\det(\\Sigma) = \\delta > 0$, $\\Sigma$ is invertible. The population minimizer is unique and given by:\n$$\nw_{\\text{pop}} = \\Sigma^{-1}s\n$$\nNext, we compute the vector $s = \\mathbb{E}[xy]$ for the given model $y = x_{1} + \\gamma x_{2}$.\n$$\ns = \\mathbb{E}\\left[\\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} (x_{1} + \\gamma x_{2})\\right] = \\mathbb{E}\\left[\\begin{pmatrix} x_{1}^{2} + \\gamma x_{1}x_{2} \\\\ x_{1}x_{2} + \\gamma x_{2}^{2} \\end{pmatrix}\\right] = \\begin{pmatrix} \\mathbb{E}[x_{1}^{2}] + \\gamma \\mathbb{E}[x_{1}x_{2}] \\\\ \\mathbb{E}[x_{1}x_{2}] + \\gamma \\mathbb{E}[x_{2}^{2}] \\end{pmatrix}\n$$\nFrom $\\Sigma = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\delta \\end{pmatrix}$, we have $\\mathbb{E}[x_{1}^{2}] = 1$, $\\mathbb{E}[x_{2}^{2}] = \\delta$, and $\\mathbb{E}[x_{1}x_{2}] = 0$. Substituting these values:\n$$\ns = \\begin{pmatrix} 1 + \\gamma(0) \\\\ 0 + \\gamma(\\delta) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\gamma\\delta \\end{pmatrix}\n$$\nNow we compute $w_{\\text{pop}}$ using $\\Sigma^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1/\\delta \\end{pmatrix}$.\n$$\nw_{\\text{pop}} = \\Sigma^{-1}s = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{\\delta} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\gamma\\delta \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\gamma \\end{pmatrix}\n$$\n\n### Part 2: Ridge-Regularized Empirical Minimizer and its Population Target\n\nThe ridge-regularized empirical risk is $L_{n}(w,\\lambda) = \\frac{1}{n} \\sum_{i=1}^{n} \\big(y_{i} - x_{i}^{\\top} w\\big)^{2} + \\lambda \\|w\\|_{2}^{2}$.\nWe can rewrite the objective function in matrix terms using the empirical covariance matrix $S = \\frac{1}{n} \\sum x_{i} x_{i}^{\\top}$ and cross-covariance vector $\\hat{s} = \\frac{1}{n} \\sum x_{i} y_{i}$:\n$$\nL_{n}(w,\\lambda) = w^{\\top} S w - 2\\hat{s}^{\\top} w + \\lambda w^{\\top}Iw + (\\text{constant}) = w^{\\top}(S + \\lambda I)w - 2\\hat{s}^{\\top} w + (\\text{constant})\n$$\nThe gradient is $\\nabla_{w} L_{n}(w,\\lambda) = 2(S + \\lambda I)w - 2\\hat{s}$. Setting the gradient to zero yields the regularized normal equations:\n$$\n(S + \\lambda I)w = \\hat{s}\n$$\nSince $S$ is positive semi-definite and $\\lambda > 0$, $(S + \\lambda I)$ is invertible. The unique empirical minimizer is:\n$$\nw_{n}(\\lambda) = (S + \\lambda I)^{-1}\\hat{s}\n$$\nBy the law of large numbers (LLN), as $n \\to \\infty$, the sample moments converge in probability to the population moments: $S \\xrightarrow{p} \\Sigma$ and $\\hat{s} \\xrightarrow{p} s$.\nBy the continuous mapping theorem, the ridge estimate $w_n(\\lambda)$ converges in probability to the same function of the population quantities:\n$$\nw_{n}(\\lambda) = (S + \\lambda I)^{-1}\\hat{s} \\xrightarrow{p} (\\Sigma + \\lambda I)^{-1}s\n$$\nThe deterministic population target of the ridge estimator is $w_{\\lambda} = (\\Sigma + \\lambda I)^{-1}s$.\n\n### Part 3: Explicit Limit of the Regularization Target\n\nWe compute $w_{\\lambda}$ explicitly using the population quantities:\n$$\n\\Sigma + \\lambda I = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\delta \\end{pmatrix} + \\lambda\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1+\\lambda & 0 \\\\ 0 & \\delta+\\lambda \\end{pmatrix}\n$$\nThe inverse is $(\\Sigma + \\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{1+\\lambda} & 0 \\\\ 0 & \\frac{1}{\\delta+\\lambda} \\end{pmatrix}$.\nNow we compute $w_{\\lambda}$:\n$$\nw_{\\lambda} = (\\Sigma + \\lambda I)^{-1}s = \\begin{pmatrix} \\frac{1}{1+\\lambda} & 0 \\\\ 0 & \\frac{1}{\\delta+\\lambda} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\gamma\\delta \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1+\\lambda} \\\\ \\frac{\\gamma\\delta}{\\delta+\\lambda} \\end{pmatrix}\n$$\nFinally, we take the limit as the regularization parameter $\\lambda$ goes to zero from the positive side ($\\lambda \\downarrow 0^{+}$):\n$$\n\\lim_{\\lambda \\downarrow 0^{+}} w_{\\lambda} = \\lim_{\\lambda \\downarrow 0^{+}} \\begin{pmatrix} \\frac{1}{1+\\lambda} \\\\ \\frac{\\gamma\\delta}{\\delta+\\lambda} \\end{pmatrix} = \\begin{pmatrix} \\lim_{\\lambda \\to 0^{+}} \\frac{1}{1+\\lambda} \\\\ \\lim_{\\lambda \\to 0^{+}} \\frac{\\gamma\\delta}{\\delta+\\lambda} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1} \\\\ \\frac{\\gamma\\delta}{\\delta} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\gamma \\end{pmatrix}\n$$\nThis limiting vector is the final answer. It is identical to the population minimizer $w_{\\text{pop}}$, which is the expected result since for any fixed $\\delta > 0$, $\\Sigma$ is invertible and its pseudoinverse is its regular inverse.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 \\\\\n\\gamma\n\\end{pmatrix}\n}\n$$"
        }
    ]
}