{
    "hands_on_practices": [
        {
            "introduction": "我们通过一个基础问题开始动手实践：估计正态分布的方差。这个练习  在单个参数估计的场景下，对最大似然估计、矩估计和最大后验估计进行了清晰而直接的比较。通过解决这个问题，你将巩固对每种方法推导过程的理解，并亲身体会贝叶斯先验如何影响最终估计值及其统计特性（如偏差）。",
            "id": "3157673",
            "problem": "考虑来自正态（高斯）分布的独立同分布观测值 $X_{1},\\dots,X_{n}$，其均值 $\\,\\mu\\,$ 已知，方差 $\\,\\sigma^{2}\\,$ 未知，即 $\\,X_{i}\\sim \\mathcal{N}(\\mu,\\sigma^{2})\\,$ 对 $\\,i=1,\\dots,n\\,$ 成立。定义中心化平方和 $\\,Q=\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}\\,$ 和样本二阶中心矩 $\\,S^{2}=\\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}=\\frac{Q}{n}\\,$。\n\n仅使用似然和矩的基本原理，完成以下任务：\n\n1) 使用正态模型的似然定义以及通过导数最大化可微函数的法则，推导 $\\,\\sigma^{2}\\,$ 的最大似然估计量。使用矩方法的定义，将样本二阶中心矩与其总体对应量相等，并基于 $\\,S^{2}\\,$ 推导 $\\,\\sigma^{2}\\,$ 的矩方法估计量。说明这两个估计量之间的关系。\n\n2) 为 $\\,\\sigma^{2}\\,$ 设置一个先验，该先验由形状参数为 $\\,\\alpha>0\\,$、尺度参数为 $\\,\\beta>0\\,$ 的逆伽马分布给出，其密度函数为 $\\,p(\\sigma^{2})=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}(\\sigma^{2})^{-(\\alpha+1)}\\exp\\!\\big(-\\beta/\\sigma^{2}\\big)\\,$ (对于 $\\,\\sigma^{2}>0\\,$)，其中 $\\,\\Gamma(\\cdot)\\,$ 是伽马函数。使用贝叶斯法则和对数后验的微积分，推导 $\\,\\sigma^{2}\\,$ 的最大后验估计量（后验众数），并将其表示为 $\\,Q\\,$, $\\,\\alpha\\,$, $\\,\\beta\\,$ 和 $\\,n\\,$ 的函数。\n\n3) 在真实方差 $\\,\\sigma^{2}\\,$ 下将数据视为随机变量，利用均值已知的正态数据中 $\\,Q\\,$ 的分布性质，计算最大后验估计量相对于真实参数的偏差，该偏差定义为 $\\,\\mathbb{E}_{\\sigma^{2}}[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}]-\\sigma^{2}\\,$。以 $\\,\\alpha\\,$, $\\,\\beta\\,$, $\\,n\\,$ 和 $\\,\\sigma^{2}\\,$ 的符号表达式给出其闭合形式。\n\n仅提供偏差的最终符号表达式作为你的最终答案。无需进行数值舍入。最终答案必须是单一的解析表达式。",
            "solution": "问题陈述在科学上是合理的、自洽的、适定的，提出了一个统计推断中的标准问题，可以用既定原理解决。我们着手进行推导。\n\n设观测值为 $X_{1}, \\dots, X_{n}$，它们独立同分布（i.i.d.）于正态分布 $\\mathcal{N}(\\mu, \\sigma^{2})$，其中均值 $\\mu$ 已知，方差 $\\sigma^{2}$ 未知。\n\n**1) 最大似然估计量和矩方法估计量**\n\n首先，我们推导 $\\sigma^{2}$ 的最大似然估计量（MLE）。单个观测值 $X_{i}$ 的概率密度函数为：\n$$f(x_{i} | \\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(x_{i}-\\mu)^{2}}{2\\sigma^{2}}\\right)$$\n由于观测值是独立同分布的，整个样本 $\\mathbf{x} = (x_{1}, \\dots, x_{n})$ 的似然函数是各独立密度的乘积：\n$$L(\\sigma^{2} | \\mathbf{x}) = \\prod_{i=1}^{n} f(x_{i} | \\sigma^{2}) = \\left(\\frac{1}{2\\pi\\sigma^{2}}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{n}(x_{i}-\\mu)^{2}\\right)$$\n使用定义 $Q = \\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$，似然函数可以写为：\n$$L(\\sigma^{2} | \\mathbf{x}) = (2\\pi\\sigma^{2})^{-n/2} \\exp\\left(-\\frac{Q}{2\\sigma^{2}}\\right)$$\n为了找到最大值，我们处理对数似然函数 $\\ell(\\sigma^{2} | \\mathbf{x}) = \\ln L(\\sigma^{2} | \\mathbf{x})$：\n$$\\ell(\\sigma^{2} | \\mathbf{x}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{Q}{2\\sigma^{2}}$$\n我们对 $\\ell$ 关于 $\\sigma^{2}$ 求导，并令结果为零。为方便起见，令 $\\theta = \\sigma^{2}$。\n$$\\frac{\\partial\\ell}{\\partial\\theta} = -\\frac{n}{2\\theta} + \\frac{Q}{2\\theta^{2}}$$\n令导数为零以找到临界点：\n$$-\\frac{n}{2\\hat{\\theta}} + \\frac{Q}{2\\hat{\\theta}^{2}} = 0 \\implies \\frac{Q}{2\\hat{\\theta}^{2}} = \\frac{n}{2\\hat{\\theta}}$$\n假设 $\\hat{\\theta} \\neq 0$，我们可以乘以 $2\\hat{\\theta}^{2}$ 得到 $Q = n\\hat{\\theta}$。解出 $\\hat{\\theta}$：\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MLE}} = \\hat{\\theta} = \\frac{Q}{n} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$$\n这就是样本二阶中心矩 $S^{2}$。\n\n接下来，我们推导矩方法（MoM）估计量。该方法将总体矩与样本矩相等。第一个相关的总体矩是二阶中心矩 $\\mathbb{E}[(X-\\mu)^{2}]$。根据定义，对于一个随机变量 $X$，这就是它的方差。\n$$\\mathbb{E}[(X-\\mu)^{2}] = \\mathrm{Var}(X) = \\sigma^{2}$$\n相应地，问题中给出的样本二阶中心矩为 $S^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$。\n将总体矩与样本矩相等，得到 $\\sigma^{2}$ 的 MoM 估计量：\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MoM}} = S^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$$\n比较这两个结果，我们发现对于这个模型，$\\sigma^{2}$ 的最大似然估计量和矩方法估计量是相同的：\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MLE}} = \\widehat{\\sigma^{2}}_{\\mathrm{MoM}} = S^{2}$$\n\n**2) 最大后验（MAP）估计量**\n\nMAP估计量最大化后验分布。根据贝叶斯法则，后验密度 $p(\\sigma^{2} | \\mathbf{x})$ 与似然和先验的乘积成正比：\n$$p(\\sigma^{2} | \\mathbf{x}) \\propto L(\\sigma^{2} | \\mathbf{x}) p(\\sigma^{2})$$\n$\\sigma^{2}$ 的先验是逆伽马$(\\alpha, \\beta)$分布：\n$$p(\\sigma^{2}) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}(\\sigma^{2})^{-(\\alpha+1)}\\exp\\left(-\\frac{\\beta}{\\sigma^{2}}\\right)$$\n我们处理对数后验，它与对数似然和对数先验的和成正比：\n$$\\ln p(\\sigma^{2} | \\mathbf{x}) = \\ln L(\\sigma^{2} | \\mathbf{x}) + \\ln p(\\sigma^{2}) + C$$\n其中 $C$ 是一个不依赖于 $\\sigma^{2}$ 的常数。\n$$\\ln p(\\sigma^{2} | \\mathbf{x}) = \\left(-\\frac{n}{2}\\ln(\\sigma^{2}) - \\frac{Q}{2\\sigma^{2}}\\right) + \\left(-(\\alpha+1)\\ln(\\sigma^{2}) - \\frac{\\beta}{\\sigma^{2}}\\right) + C'$$\n合并项：\n$$\\ln p(\\sigma^{2} | \\mathbf{x}) = -\\left(\\frac{n}{2} + \\alpha + 1\\right)\\ln(\\sigma^{2}) - \\frac{1}{\\sigma^{2}}\\left(\\frac{Q}{2} + \\beta\\right) + C'$$\n为了找到后验的众数（即 MAP 估计量），我们对 $\\sigma^{2}$ 求导并令导数为零。令 $\\theta = \\sigma^{2}$：\n$$\\frac{\\partial}{\\partial\\theta}\\ln p(\\theta | \\mathbf{x}) = -\\left(\\frac{n}{2} + \\alpha + 1\\right)\\frac{1}{\\theta} + \\left(\\frac{Q}{2} + \\beta\\right)\\frac{1}{\\theta^{2}}$$\n令其为零：\n$$\\left(\\frac{Q}{2} + \\beta\\right)\\frac{1}{\\hat{\\theta}^{2}} = \\left(\\frac{n}{2} + \\alpha + 1\\right)\\frac{1}{\\hat{\\theta}}$$\n$$\\frac{Q + 2\\beta}{2\\hat{\\theta}^{2}} = \\frac{n + 2\\alpha + 2}{2\\hat{\\theta}}$$\n解出 $\\hat{\\theta}$ 得到 MAP 估计量：\n$$\\widehat{\\sigma^{2}}_{\\mathrm{MAP}} = \\hat{\\theta} = \\frac{Q + 2\\beta}{n + 2\\alpha + 2}$$\n\n**3) MAP 估计量的偏差**\n\n估计量 $\\hat{\\theta}$ 对参数 $\\theta$ 的偏差定义为 $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$。这里，我们需要计算 $\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}$ 相对于真实方差 $\\sigma^{2}$ 的偏差。期望是针对数据的抽样分布计算的，其中 $Q$ 是一个随机变量。\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] - \\sigma^{2}$$\n首先，我们求该估计量的期望：\n$$\\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] = \\mathbb{E}_{\\sigma^{2}}\\left[\\frac{Q + 2\\beta}{n + 2\\alpha + 2}\\right]$$\n利用期望的线性性质，并注意到 $n, \\alpha, \\beta$ 是常数：\n$$\\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] = \\frac{\\mathbb{E}_{\\sigma^{2}}[Q] + 2\\beta}{n + 2\\alpha + 2}$$\n为了求 $\\mathbb{E}_{\\sigma^{2}}[Q]$，我们使用 $Q$ 的分布性质。对于 $X_{i} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$，标准化变量 $Z_{i} = \\frac{X_{i}-\\mu}{\\sigma}$ 服从标准正态分布 $\\mathcal{N}(0, 1)$。标准正态变量的平方 $Z_{i}^{2}$ 服从自由度为 1 的卡方分布 $\\chi^{2}_{1}$。\n量 $Q$ 定义为 $Q = \\sum_{i=1}^{n}(X_{i}-\\mu)^{2}$。我们可以将其写为：\n$$Q = \\sum_{i=1}^{n} \\left(\\sigma \\frac{X_{i}-\\mu}{\\sigma}\\right)^{2} = \\sigma^{2} \\sum_{i=1}^{n} Z_{i}^{2}$$\n由于 $X_{i}$ 是独立的，所以 $Z_{i}$ 也是独立的。$n$ 个独立的 $\\chi^{2}_{1}$ 随机变量之和是一个自由度为 $n$ 的 $\\chi^{2}$ 随机变量。因此：\n$$\\frac{Q}{\\sigma^{2}} = \\sum_{i=1}^{n} Z_{i}^{2} \\sim \\chi^{2}_{n}$$\n一个 $\\chi^{2}_{n}$ 随机变量的期望值是其自由度 $n$。\n$$\\mathbb{E}\\left[\\frac{Q}{\\sigma^{2}}\\right] = n$$\n根据期望的线性性质，$\\frac{1}{\\sigma^{2}}\\mathbb{E}_{\\sigma^{2}}[Q] = n$，这意味着：\n$$\\mathbb{E}_{\\sigma^{2}}[Q] = n\\sigma^{2}$$\n将此代回 MAP 估计量的期望表达式中：\n$$\\mathbb{E}_{\\sigma^{2}}\\left[\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}\\right] = \\frac{n\\sigma^{2} + 2\\beta}{n + 2\\alpha + 2}$$\n最后，我们计算偏差：\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{n\\sigma^{2} + 2\\beta}{n + 2\\alpha + 2} - \\sigma^{2}$$\n为了简化，我们将所有项通分：\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{n\\sigma^{2} + 2\\beta - \\sigma^{2}(n + 2\\alpha + 2)}{n + 2\\alpha + 2}$$\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{n\\sigma^{2} + 2\\beta - n\\sigma^{2} - (2\\alpha + 2)\\sigma^{2}}{n + 2\\alpha + 2}$$\n$$\\mathrm{Bias}(\\widehat{\\sigma^{2}}_{\\mathrm{MAP}}) = \\frac{2\\beta - (2\\alpha + 2)\\sigma^{2}}{n + 2\\alpha + 2} = \\frac{2\\beta - 2(\\alpha + 1)\\sigma^{2}}{n + 2(\\alpha + 1)}$$\n这就是 MAP 估计量偏差的最终符号表达式。",
            "answer": "$$\n\\boxed{\\frac{2\\beta - 2(\\alpha + 1)\\sigma^{2}}{n + 2\\alpha + 2}}\n$$"
        },
        {
            "introduction": "接下来，我们将处理一个在实践中经常遇到的场景：为过离散的计数数据估计参数。本练习  采用负二项分布模型，它比泊松分布更灵活，能够更好地捕捉现实世界数据中常见的额外变异。通过这个例子，你将对比矩估计的简洁性与基于似然的方法（最大似然估计和最大后验估计）的复杂性，并理解为何在某些模型中，求解最大似然估计需要借助数值优化方法。",
            "id": "3157630",
            "problem": "一项研究记录了表现出过度离散（方差超过均值）的计数数据。假设这些计数是独立同分布的，服从参数为 $r>0$ 和 $p\\in(0,1)$ 的负二项分布，记作 $X_i \\sim \\text{NegBin}(r,p)$，该分布解释为在一系列成功概率为 $p$ 的独立伯努利试验中，观测到第 $r$ 次成功之前失败的次数。其概率质量函数为\n$$\n\\mathbb{P}(X=x \\mid r,p) \\;=\\; \\binom{x+r-1}{x} (1-p)^{x} p^{r}, \\quad x \\in \\{0,1,2,\\dots\\},\n$$\n其中 $\\binom{x+r-1}{x} \\;=\\; \\frac{\\Gamma(x+r)}{\\Gamma(r)\\,\\Gamma(x+1)}$ 且 $\\Gamma(\\cdot)$ 是伽马函数。对于此参数化，\n$$\n\\mathbb{E}[X] \\;=\\; \\frac{r(1-p)}{p}, \\qquad \\mathrm{Var}(X) \\;=\\; \\frac{r(1-p)}{p^{2}}.\n$$\n收集了一个大小为 $n$ 的样本，得到的样本均值为 $m$，样本方差为 $v$，且 $v>m$。\n\n任务：\n1) 使用矩估计法（method of moments）的基本定义（将前两个样本矩与前两个模型矩相等），推导矩估计量 $\\hat{r}_{\\mathrm{MoM}}$ 和 $\\hat{p}_{\\mathrm{MoM}}$ 关于 $m$ 和 $v$ 的解析表达式。\n\n2) 从独立性假设下的似然函数定义出发，写出样本 $\\{x_1,\\dots,x_n\\}$ 的对数似然函数 $\\ell(r,p)$，并为最大似然估计量 $\\hat{r}_{\\mathrm{MLE}}$ 和 $\\hat{p}_{\\mathrm{MLE}}$ 推导一阶条件（得分方程）。将 $\\hat{p}_{\\mathrm{MLE}}$ 表示为关于 $r$、$n$ 和 $\\sum_{i=1}^{n} x_i$ 的闭式表达式，并将关于 $r$ 的剩余条件用 $\\psi(t)=\\frac{d}{dt}\\ln\\Gamma(t)$ 定义的 digamma 函数 $\\psi(\\cdot)$ 表示。\n\n3) 假设独立的先验分布为 $p \\sim \\mathrm{Beta}(a,b)$（其中 $a>1, b>1$）和 $r \\sim \\mathrm{Gamma}(\\alpha,\\beta)$（其中形状参数 $\\alpha>0$，率参数 $\\beta>0$）。使用最大后验 (MAP) 估计量的基本定义（最大化后验密度），推导对数后验函数以及 MAP 估计量 $(\\hat{r}_{\\mathrm{MAP}}, \\hat{p}_{\\mathrm{MAP}})$ 的平稳性条件。证明 $\\hat{p}_{\\mathrm{MAP}}$ 存在一个关于 $r$、$n$、$a$、$b$ 和 $\\sum_{i=1}^{n} x_i$ 的闭式表达式。\n\n4) 对于一个样本均值为 $m = 4$、样本方差为 $v = 8$ 的数据集，计算矩估计值 $(\\hat{r}_{\\mathrm{MoM}}, \\hat{p}_{\\mathrm{MoM}})$ 并报告结果，将每个值四舍五入到四位有效数字。\n\n答案格式要求：你的最终数值答案必须是一个包含两个值 $(\\hat{r}_{\\mathrm{MoM}}, \\hat{p}_{\\mathrm{MoM}})$ 的单行矩阵，四舍五入到四位有效数字，不带单位。",
            "solution": "该问题经验证是适定的、有科学依据且客观的。这是一个统计点估计中的标准问题。\n\n解题过程按照题目要求分为四个部分呈现。\n\n任务 1：矩估计法 (MoM) 估计量\n\n矩估计法将前两个样本矩与分布相应的前两个理论矩相等同。对于负二项分布 $\\text{NegBin}(r,p)$，前两个矩是均值和方差。\n理论矩由下式给出：\n$$\n\\mathbb{E}[X] = \\frac{r(1-p)}{p}\n$$\n$$\n\\mathrm{Var}(X) = \\frac{r(1-p)}{p^2}\n$$\n样本矩是样本均值 $m$ 和样本方差 $v$。矩估计法为估计量 $\\hat{r}_{\\mathrm{MoM}}$ 和 $\\hat{p}_{\\mathrm{MoM}}$ 建立以下方程组：\n$$\nm = \\frac{\\hat{r}_{\\mathrm{MoM}}(1-\\hat{p}_{\\mathrm{MoM}})}{\\hat{p}_{\\mathrm{MoM}}} \\quad (1)\n$$\n$$\nv = \\frac{\\hat{r}_{\\mathrm{MoM}}(1-\\hat{p}_{\\mathrm{MoM}})}{\\hat{p}_{\\mathrm{MoM}}^2} \\quad (2)\n$$\n为求解该方程组，我们将方程 $(2)$ 除以方程 $(1)$：\n$$\n\\frac{v}{m} = \\frac{\\left(\\frac{\\hat{r}_{\\mathrm{MoM}}(1-\\hat{p}_{\\mathrm{MoM}})}{\\hat{p}_{\\mathrm{MoM}}^2}\\right)}{\\left(\\frac{\\hat{r}_{\\mathrm{MoM}}(1-\\hat{p}_{\\mathrm{MoM}})}{\\hat{p}_{\\mathrm{MoM}}}\\right)} = \\frac{1}{\\hat{p}_{\\mathrm{MoM}}}\n$$\n求解 $\\hat{p}_{\\mathrm{MoM}}$ 得到 $p$ 的矩估计量：\n$$\n\\hat{p}_{\\mathrm{MoM}} = \\frac{m}{v}\n$$\n条件 $v > m > 0$ 确保了 $0 < \\hat{p}_{\\mathrm{MoM}} < 1$。\n接下来，我们将 $\\hat{p}_{\\mathrm{MoM}}$ 的这个表达式代回方程 $(1)$ 以求解 $\\hat{r}_{\\mathrm{MoM}}$：\n$$\nm = \\frac{\\hat{r}_{\\mathrm{MoM}}(1 - \\frac{m}{v})}{\\frac{m}{v}} = \\frac{\\hat{r}_{\\mathrm{MoM}}\\left(\\frac{v-m}{v}\\right)}{\\frac{m}{v}}\n$$\n$$\nm = \\hat{r}_{\\mathrm{MoM}} \\frac{v-m}{m}\n$$\n求解 $\\hat{r}_{\\mathrm{MoM}}$ 得到 $r$ 的矩估计量：\n$$\n\\hat{r}_{\\mathrm{MoM}} = \\frac{m^2}{v-m}\n$$\n条件 $v > m$ 确保了 $\\hat{r}_{\\mathrm{MoM}} > 0$。\n\n任务 2：最大似然估计 (MLE)\n\n给定一个来自 $\\text{NegBin}(r,p)$ 的独立同分布观测样本 $\\{x_1, \\dots, x_n\\}$，似然函数 $L(r,p)$ 是各个概率的乘积：\n$$\nL(r,p) = \\prod_{i=1}^{n} \\mathbb{P}(X_i=x_i \\mid r,p) = \\prod_{i=1}^{n} \\binom{x_i+r-1}{x_i} (1-p)^{x_i} p^{r}\n$$\n对数似然函数 $\\ell(r,p) = \\ln L(r,p)$ 为：\n$$\n\\ell(r,p) = \\sum_{i=1}^{n} \\left[ \\ln\\binom{x_i+r-1}{x_i} + x_i\\ln(1-p) + r\\ln p \\right]\n$$\n使用伽马函数表示法 $\\binom{k+n-1}{k} = \\frac{\\Gamma(k+n)}{\\Gamma(n)\\Gamma(k+1)}$，对数似然函数变为：\n$$\n\\ell(r,p) = \\sum_{i=1}^{n} \\left[ \\ln\\left(\\frac{\\Gamma(x_i+r)}{\\Gamma(r)\\Gamma(x_i+1)}\\right) \\right] + \\left(\\sum_{i=1}^{n}x_i\\right)\\ln(1-p) + nr\\ln p\n$$\n$$\n\\ell(r,p) = \\sum_{i=1}^{n} \\left[ \\ln\\Gamma(x_i+r) - \\ln\\Gamma(r) - \\ln\\Gamma(x_i+1) \\right] + \\left(\\sum_{i=1}^{n}x_i\\right)\\ln(1-p) + nr\\ln p\n$$\n通过将 $\\ell(r,p)$ 对 $p$ 和 $r$ 的偏导数设为零，可以找到得分方程。\n关于 $p$ 的偏导数为：\n$$\n\\frac{\\partial \\ell}{\\partial p} = -\\frac{\\sum_{i=1}^{n}x_i}{1-p} + \\frac{nr}{p}\n$$\n将其设为零以求得 MLE $\\hat{p}_{\\mathrm{MLE}}$：\n$$\n\\frac{n\\hat{r}_{\\mathrm{MLE}}}{\\hat{p}_{\\mathrm{MLE}}} = \\frac{\\sum_{i=1}^{n}x_i}{1-\\hat{p}_{\\mathrm{MLE}}} \\implies n\\hat{r}_{\\mathrm{MLE}}(1-\\hat{p}_{\\mathrm{MLE}}) = \\hat{p}_{\\mathrm{MLE}}\\sum_{i=1}^{n}x_i\n$$\n$$\nn\\hat{r}_{\\mathrm{MLE}} = \\hat{p}_{\\mathrm{MLE}}(n\\hat{r}_{\\mathrm{MLE}} + \\sum_{i=1}^{n}x_i)\n$$\n这给出了 $\\hat{p}_{\\mathrm{MLE}}$ 关于 $\\hat{r}_{\\mathrm{MLE}}$ 和数据的闭式表达式：\n$$\n\\hat{p}_{\\mathrm{MLE}} = \\frac{n\\hat{r}_{\\mathrm{MLE}}}{n\\hat{r}_{\\mathrm{MLE}} + \\sum_{i=1}^{n}x_i}\n$$\n关于 $r$ 的偏导数为，使用 $\\psi(t) = \\frac{d}{dt}\\ln\\Gamma(t)$：\n$$\n\\frac{\\partial \\ell}{\\partial r} = \\sum_{i=1}^{n} \\left[ \\psi(x_i+r) - \\psi(r) \\right] + n\\ln p\n$$\n将其设为零得到第二个得分方程：\n$$\n\\sum_{i=1}^{n} \\left[ \\psi(x_i+\\hat{r}_{\\mathrm{MLE}}) - \\psi(\\hat{r}_{\\mathrm{MLE}}) \\right] + n\\ln(\\hat{p}_{\\mathrm{MLE}}) = 0\n$$\n代入 $\\hat{p}_{\\mathrm{MLE}}$ 的表达式，关于 $\\hat{r}_{\\mathrm{MLE}}$ 的剩余条件是：\n$$\n\\sum_{i=1}^{n} \\psi(x_i+\\hat{r}_{\\mathrm{MLE}}) - n\\psi(\\hat{r}_{\\mathrm{MLE}}) + n\\ln\\left(\\frac{n\\hat{r}_{\\mathrm{MLE}}}{n\\hat{r}_{\\mathrm{MLE}} + \\sum_{i=1}^{n}x_i}\\right) = 0\n$$\n该方程没有关于 $\\hat{r}_{\\mathrm{MLE}}$ 的闭式解，必须通过数值方法求解。\n\n任务 3：最大后验 (MAP) 估计\n\n后验分布正比于似然函数与先验分布的乘积。先验分布给定为 $p \\sim \\mathrm{Beta}(a,b)$ 和 $r \\sim \\mathrm{Gamma}(\\alpha,\\beta)$，其密度函数为 $\\pi(p) \\propto p^{a-1}(1-p)^{b-1}$ 和 $\\pi(r) \\propto r^{\\alpha-1}\\exp(-\\beta r)$。对数后验函数是对数似然函数与对数先验函数之和（忽略常数项）：\n$$\n\\ell_{\\text{post}}(r,p) = \\ell(r,p) + \\ln\\pi(p) + \\ln\\pi(r)\n$$\n$$\n\\ell_{\\text{post}}(r,p) = \\ell(r,p) + (a-1)\\ln p + (b-1)\\ln(1-p) + (\\alpha-1)\\ln r - \\beta r + C\n$$\n其中 $C$ 是一个与 $r$ 和 $p$ 无关的常数。\n通过将对数后验函数的偏导数设为零，可以找到平稳性条件。\n关于 $p$ 的偏导数为：\n$$\n\\frac{\\partial \\ell_{\\text{post}}}{\\partial p} = \\frac{\\partial \\ell}{\\partial p} + \\frac{a-1}{p} - \\frac{b-1}{1-p} = \\left(-\\frac{\\sum_{i=1}^{n}x_i}{1-p} + \\frac{nr}{p}\\right) + \\frac{a-1}{p} - \\frac{b-1}{1-p}\n$$\n$$\n\\frac{\\partial \\ell_{\\text{post}}}{\\partial p} = \\frac{nr+a-1}{p} - \\frac{\\sum_{i=1}^{n}x_i + b-1}{1-p}\n$$\n将其设为零可得：\n$$\n\\frac{n\\hat{r}_{\\mathrm{MAP}}+a-1}{\\hat{p}_{\\mathrm{MAP}}} = \\frac{\\sum_{i=1}^{n}x_i + b-1}{1-\\hat{p}_{\\mathrm{MAP}}}\n$$\n求解 $\\hat{p}_{\\mathrm{MAP}}$ 得到关于 $\\hat{r}_{\\mathrm{MAP}}$ 和数据/超参数的闭式表达式：\n$$\n\\hat{p}_{\\mathrm{MAP}} = \\frac{n\\hat{r}_{\\mathrm{MAP}}+a-1}{n\\hat{r}_{\\mathrm{MAP}} + \\sum_{i=1}^{n}x_i + a+b-2}\n$$\n关于 $r$ 的偏导数为：\n$$\n\\frac{\\partial \\ell_{\\text{post}}}{\\partial r} = \\frac{\\partial \\ell}{\\partial r} + \\frac{\\alpha-1}{r} - \\beta = \\left(\\sum_{i=1}^{n}[\\psi(x_i+r)-\\psi(r)] + n\\ln p\\right) + \\frac{\\alpha-1}{r} - \\beta\n$$\n将其设为零得到第二个平稳性条件：\n$$\n\\sum_{i=1}^{n}\\left[\\psi(x_i+\\hat{r}_{\\mathrm{MAP}})-\\psi(\\hat{r}_{\\mathrm{MAP}})\\right] + n\\ln(\\hat{p}_{\\mathrm{MAP}}) + \\frac{\\alpha-1}{\\hat{r}_{\\mathrm{MAP}}} - \\beta = 0\n$$\n\n任务 4：矩估计法的数值计算\n\n对于一个样本均值 $m=4$、样本方差 $v=8$ 的数据集，我们使用任务 1 中推导出的矩估计量。\n$$\n\\hat{p}_{\\mathrm{MoM}} = \\frac{m}{v} = \\frac{4}{8} = 0.5\n$$\n$$\n\\hat{r}_{\\mathrm{MoM}} = \\frac{m^2}{v-m} = \\frac{4^2}{8-4} = \\frac{16}{4} = 4\n$$\n将这些值四舍五入到四位有效数字，我们得到：\n$$\n\\hat{p}_{\\mathrm{MoM}} = 0.5000\n$$\n$$\n\\hat{r}_{\\mathrm{MoM}} = 4.000\n$$\n最终答案以行矩阵 $(\\hat{r}_{\\mathrm{MoM}}, \\hat{p}_{\\mathrm{MoM}})$ 的形式呈现。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4.000 & 0.5000\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "最后的练习将我们的关注点从纯粹的理论推导转向计算应用。变点检测问题  要求你实现并评估基于最大似然、最大后验和矩估计思想的算法，用以识别数据序列中的结构性突变。这项练习弥合了理论与实际数据分析之间的鸿沟，展示了如何将点估计原理扩展到复杂的模型选择任务中，并通过模拟来评估不同估计器的性能。",
            "id": "3157658",
            "problem": "考虑一个用于一元独立观测序列的单变点模型。令 $X_1,\\dots,X_n$ 为独立随机变量，存在一个未知的变点索引 $\\tau^\\star \\in \\{1,\\dots,n-1\\}$，使得\n- 对所有 $i \\le \\tau^\\star$，$X_i \\sim \\mathcal{N}(\\mu_1,\\sigma_1^2)$，\n- 对所有 $i > \\tau^\\star$，$X_i \\sim \\mathcal{N}(\\mu_2,\\sigma_2^2)$，\n所有参数 $(\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2,\\tau^\\star)$ 对估计器未知，且样本在索引 $i$ 上是独立的。\n\n您的任务是实现变点位置 $\\tau^\\star$ 的三个点估计器，并在给定的测试套件上评估它们的检测能力：\n- 最大似然估计 (MLE)，\n- 具有指定变点先验的最大后验估计 (MAP)，\n- 使用经验矩累积偏差的矩方法 (MoM)。\n\n请基于以下基本定义和事实进行推导和算法设计：\n- 在高斯模型下，联合似然是分段高斯密度的乘积。\n- 对于固定分段，高斯分布的均值和方差的最大似然估计量是样本均值和样本方差（除数为分段长度）。\n- 对于 MAP 估计器，后验与似然乘以先验成正比，后验密度的最大化者即为对数后验的最大化者。\n\n需要实现的估计器规格：\n- MLE：对于搜索集 $\\mathcal{K}=\\{k_{\\min},k_{\\min}+1,\\dots,k_{\\max}\\}$（其中 $k_{\\min}=2$ 且 $k_{\\max}=n-2$）中的每个候选变点 $k$，计算在变点为 $k$ 的模型下最大化的高斯对数似然，该模型允许分段特定的均值和方差。MLE $\\widehat{\\tau}_{\\mathrm{MLE}}$ 是使该最大化对数似然最大的 $k \\in \\mathcal{K}$。若出现平局，则选择最小的 $k$。\n- MAP：对变点使用支持在 $\\mathcal{K}$ 上的离散拉普拉斯先验，由 $p(k) \\propto \\exp\\!\\left(-\\lambda \\lvert k-k_0\\rvert\\right)$ 给出，其中超参数为 $k_0$ 和 $\\lambda>0$。MAP 估计器 $\\widehat{\\tau}_{\\mathrm{MAP}}$ 是使后验最大化的 $k \\in \\mathcal{K}$，这等价于最大化对数似然加上对数先验。若出现平局，则选择最小的 $k$。\n- MoM：令 $\\overline{X}=\\frac{1}{n}\\sum_{i=1}^n X_i$ 为经验均值，$S^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\overline{X})^2$ 为经验方差。定义一阶矩中心化序列 $Y^{(1)}_i = X_i - \\overline{X}$ 和二阶矩中心化序列 $Y^{(2)}_i = (X_i-\\overline{X})^2 - S^2$。将每个序列标准化为单位经验标准差以平衡其尺度。对于每个 $k \\in \\mathcal{K}$，计算两个序列截至 $k$ 的绝对标准化累积偏差，并选择 $\\widehat{\\tau}_{\\mathrm{MoM}}$ 为使这些绝对标准化累积偏差之和最大的 $k$。若出现平局，则选择最小的 $k$。\n\n检测成功标准：\n- 对于每个数据集，定义容差窗口 $w=\\max\\{3,\\mathrm{round}(0.05\\,n)\\}$，它是一个整数。如果 $\\lvert \\widehat{\\tau}-\\tau^\\star\\rvert \\le w$，则认为估计器成功。\n\n数据生成协议（必须完全按照描述实现，以确保确定性和可测试的行为）：\n- 为伪随机数生成器使用固定种子，以便所有结果都是可复现的。将 NumPy 随机数生成器初始化为 $R=\\mathrm{default\\_rng}(s)$，其中 $s=20231123$。\n- 对于每个测试用例，通过连接两个独立的、具有指定参数的高斯分段来生成数据，其长度分别为 $\\tau^\\star$ 和 $n-\\tau^\\star$。\n\n测试套件：\n在以下参数集上实现并评估您的估计器。对于每个集合，生成一个数据集，并使用上述标准计算三个估计器及其检测成功的布尔值。在所有情况下，搜索范围为 $\\mathcal{K}=\\{2,3,\\dots,n-2\\}$。\n1. 情况 A：$n=120$, $\\tau^\\star=60$, $(\\mu_1,\\sigma_1)=(0.0,1.0)$, $(\\mu_2,\\sigma_2)=(1.2,1.0)$, MAP 超参数 $k_0=60$, $\\lambda=0.02$。\n2. 情况 B：$n=200$, $\\tau^\\star=30$, $(\\mu_1,\\sigma_1)=(0.0,1.0)$, $(\\mu_2,\\sigma_2)=(0.6,1.0)$, MAP 超参数 $k_0=100$, $\\lambda=0.05$。\n3. 情况 C：$n=150$, $\\tau^\\star=140$, $(\\mu_1,\\sigma_1)=(0.0,0.8)$, $(\\mu_2,\\sigma_2)=(0.0,1.8)$, MAP 超参数 $k_0=75$, $\\lambda=0.03$。\n4. 情况 D：$n=100$, $\\tau^\\star=50$, $(\\mu_1,\\sigma_1)=(0.5,1.0)$, $(\\mu_2,\\sigma_2)=(0.5,1.8)$, MAP 超参数 $k_0=80$, $\\lambda=0.04$。\n5. 情况 E：$n=80$, $\\tau^\\star=10$, $(\\mu_1,\\sigma_1)=(-0.5,0.8)$, $(\\mu_2,\\sigma_2)=(0.5,0.8)$, MAP 超参数 $k_0=20$, $\\lambda=0.08$。\n6. 情况 F：$n=180$, $\\tau^\\star=90$, $(\\mu_1,\\sigma_1)=(0.0,1.0)$, $(\\mu_2,\\sigma_2)=(0.3,1.0)$, MAP 超参数 $k_0=90$, $\\lambda=0.02$。\n\n输出规格：\n- 对于每个测试用例，您的程序必须按顺序输出一个包含以下条目的列表：$[\\widehat{\\tau}_{\\mathrm{MLE}},\\widehat{\\tau}_{\\mathrm{MAP}},\\widehat{\\tau}_{\\mathrm{MoM}},\\mathrm{success}_{\\mathrm{MLE}},\\mathrm{success}_{\\mathrm{MAP}},\\mathrm{success}_{\\mathrm{MoM}}]$，其中前三个条目是整数，后三个条目是根据成功标准的布尔值。\n- 您的程序应生成一行输出，其中包含所有用例的结果，形式为这些用例列表的逗号分隔列表，并用方括号括起来，例如 $[\\dots]$。\n\n不涉及物理单位或角度单位。任何百分比（如有）都必须表示为小数，而不是使用百分号。确保问题中的所有数值量都酌情视为实数或整数，并且所有随机抽样都遵循指定的高斯分布和独立性假设。解决方案必须是自包含的，并且不需要用户输入。代码必须遵守指定的执行环境和库。",
            "solution": "用户希望我解决一个统计估计问题。我将首先验证问题陈述，然后提供详细的解决方案及相应的代码。\n\n### 问题验证\n\n**步骤1：提取给定信息**\n\n- **模型**：一个独立随机变量序列 $X_1, \\dots, X_n$。\n- **变点**：一个未知的索引 $\\tau^\\star \\in \\{1, \\dots, n-1\\}$。\n- **变点前分布**：对于 $i \\le \\tau^\\star$，$X_i \\sim \\mathcal{N}(\\mu_1, \\sigma_1^2)$。\n- **变点后分布**：对于 $i > \\tau^\\star$，$X_i \\sim \\mathcal{N}(\\mu_2, \\sigma_2^2)$。\n- **未知参数**：$(\\mu_1, \\mu_2, \\sigma_1^2, \\sigma_2^2, \\tau^\\star)$。\n- **估计器**：最大似然估计 (MLE)，最大后验估计 (MAP)，矩方法 (MoM)。\n- **估计器的搜索集**：$\\mathcal{K} = \\{k_{\\min}, k_{\\min}+1, \\dots, k_{\\max}\\}$，其中 $k_{\\min}=2$，$k_{\\max}=n-2$。\n- **平局处理规则**：对于所有估计器，若出现平局，选择最小的索引 $k$。\n- **MLE 规格**：在 $k \\in \\mathcal{K}$ 上最大化高斯对数似然，其中对于每个 $k$，分段特定的均值和方差使用其各自的 MLE（样本均值和样本方差，除数为分段长度）进行估计。\n- **MAP 规格**：最大化对数后验，即对数似然加上对数先验。变点 $k \\in \\mathcal{K}$ 上的先验是离散拉普拉斯先验 $p(k) \\propto \\exp(-\\lambda |k-k_0|)$，给定超参数 $k_0$ 和 $\\lambda > 0$。\n- **MoM 规格**：\n    - 令 $\\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$ 且 $S^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\overline{X})^2$。\n    - 定义序列 $Y^{(1)}_i = X_i - \\overline{X}$ 和 $Y^{(2)}_i = (X_i - \\overline{X})^2 - S^2$。\n    - 将 $Y^{(1)}$ 和 $Y^{(2)}$ 标准化为单位经验标准差。\n    - 对于每个 $k \\in \\mathcal{K}$，统计量是两个序列的绝对标准化累积偏差之和。\n    - $\\widehat{\\tau}_{\\mathrm{MoM}}$ 是使此和最大化的 $k$。\n- **成功标准**：如果 $|\\widehat{\\tau} - \\tau^\\star| \\le w$，则估计 $\\widehat{\\tau}$ 成功，其中 $w = \\max\\{3, \\text{round}(0.05n)\\}$。\n- **数据生成**：使用固定随机种子 $s=20231123$ 和 `numpy.random.default_rng`。通过连接两个独立的高斯分段生成数据。\n- **测试套件**：提供了六个具体情况（A-F），包含 $n, \\tau^\\star, (\\mu_1, \\sigma_1), (\\mu_2, \\sigma_2)$ 和 MAP 超参数 $(k_0, \\lambda)$ 的参数。\n- **输出格式**：单行 JSON 风格的列表的列表，其中每个内部列表为 $[\\widehat{\\tau}_{\\mathrm{MLE}}, \\widehat{\\tau}_{\\mathrm{MAP}}, \\widehat{\\tau}_{\\mathrm{MoM}}, \\text{success}_{\\mathrm{MLE}}, \\text{success}_{\\mathrm{MAP}}, \\text{success}_{\\mathrm{MoM}}]$。\n\n**步骤2：使用提取的给定信息进行验证**\n\n- **科学基础**：该问题在变点分析的统计理论中有坚实的基础。高斯模型以及 MLE、MAP 和 MoM 估计器是该领域的标准工具。\n- **适定性**：该问题是适定的。每个估计器的定义都是精确的，包括搜索空间 $\\mathcal{K}$ 和明确的平局处理规则。约束 $k_{\\min}=2$ 和 $k_{\\max}=n-2$ 确保每个分段至少有两个数据点，使得均值和方差的估计都是明确且非退化的。固定的随机种子使整个过程具有确定性和可复现性。\n- **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n\n**结论**：该问题是有效的，因为它在科学上是合理的，在数学上是适定的，客观且自包含的。\n\n### 求解推导\n\n该问题要求为序列 $n$ 个独立观测值 $X_1, \\dots, X_n$ 中的单个变点 $\\tau^\\star$ 实现三种不同的估计器。我们假设一个模型，其中数据在点 $\\tau^\\star$ 之前服从参数为 $(\\mu_1, \\sigma_1^2)$ 的正态分布，在 $\\tau^\\star$ 之后服从参数为 $(\\mu_2, \\sigma_2^2)$ 的正态分布。所有估计器都在集合 $\\mathcal{K} = \\{2, 3, \\dots, n-2\\}$ 内搜索变点。\n\n对于一个候选变点 $k \\in \\mathcal{K}$，数据被划分为两个分段：长度为 $n_1=k$ 的 $S_1(k) = \\{X_1, \\dots, X_k\\}$ 和长度为 $n_2=n-k$ 的 $S_2(k) = \\{X_{k+1}, \\dots, X_n\\}$。\n\n#### 1. 最大似然估计器 (MLE)\n对于给定的变点 $k$ 和分段参数 $(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2)$，数据的对数似然是两个分段对数似然之和：\n$$ \\ell(k, \\mu_1, \\sigma_1^2, \\mu_2, \\sigma_2^2) = \\sum_{i=1}^{k} \\log p(X_i | \\mu_1, \\sigma_1^2) + \\sum_{i=k+1}^{n} \\log p(X_i | \\mu_2, \\sigma_2^2) $$\n其中 $p(x|\\mu, \\sigma^2)$ 是高斯概率密度函数。对于固定的 $k$，当参数 $(\\mu_1, \\sigma_1^2, \\mu_2, \\sigma_2^2)$ 被替换为它们各自在每个分段上的最大似然估计时，该对数似然达到最大值。这些估计是样本均值和样本方差（除数为 $n_j$）：\n$$ \\hat{\\mu}_1(k) = \\frac{1}{k} \\sum_{i=1}^{k} X_i \\quad \\text{和} \\quad \\hat{\\sigma}_1^2(k) = \\frac{1}{k} \\sum_{i=1}^{k} (X_i - \\hat{\\mu}_1(k))^2 $$\n$$ \\hat{\\mu}_2(k) = \\frac{1}{n-k} \\sum_{i=k+1}^{n} X_i \\quad \\text{和} \\quad \\hat{\\sigma}_2^2(k) = \\frac{1}{n-k} \\sum_{i=k+1}^{n} (X_i - \\hat{\\mu}_2(k))^2 $$\n将这些估计代入高斯对数似然公式，得到给定 $k$ 的最大化对数似然，或称轮廓对数似然：\n$$ \\hat{\\ell}(k) = -\\frac{k}{2}\\log(2\\pi\\hat{\\sigma}_1^2(k)) - \\frac{k}{2} - \\frac{n-k}{2}\\log(2\\pi\\hat{\\sigma}_2^2(k)) - \\frac{n-k}{2} $$\n为了找到变点的 MLE $\\widehat{\\tau}_{\\mathrm{MLE}}$，我们在 $k \\in \\mathcal{K}$ 上最大化 $\\hat{\\ell}(k)$。这等价于最小化依赖于 $k$ 的项的负数：\n$$ C_{\\mathrm{MLE}}(k) = k\\log(\\hat{\\sigma}_1^2(k)) + (n-k)\\log(\\hat{\\sigma}_2^2(k)) $$\n因此，MLE 估计器是 $\\widehat{\\tau}_{\\mathrm{MLE}} = \\arg\\min_{k \\in \\mathcal{K}} C_{\\mathrm{MLE}}(k)$。\n\n#### 2. 最大后验 (MAP) 估计器\n变点的 MAP 估计器在 $k \\in \\mathcal{K}$ 上最大化后验概率 $p(k|X)$。根据贝叶斯定理，这等价于最大化对数后验：\n$$ \\widehat{\\tau}_{\\mathrm{MAP}} = \\arg\\max_{k \\in \\mathcal{K}} \\log p(k|X) = \\arg\\max_{k \\in \\mathcal{K}} (\\log p(X|k) + \\log p(k)) $$\n项 $\\log p(X|k)$ 是上面推导出的轮廓对数似然 $\\hat{\\ell}(k)$。先验被给出为离散拉普拉斯分布，$p(k) \\propto \\exp(-\\lambda|k-k_0|)$，所以对数先验是 $\\log p(k) = -\\lambda|k-k_0| + \\text{constant}$。\n要最大化的目标函数是 $\\hat{\\ell}(k) - \\lambda|k-k_0|$。与 MLE 一样，通过去掉常数并加上缩放后的惩罚项，这等价于最小化一个更简单的成本函数：\n$$ C_{\\mathrm{MAP}}(k) = k\\log(\\hat{\\sigma}_1^2(k)) + (n-k)\\log(\\hat{\\sigma}_2^2(k)) + 2\\lambda|k-k_0| $$\n$2$ 这个因子来自于将负对数似然乘以 $-2$。因此，估计器是 $\\widehat{\\tau}_{\\mathrm{MAP}} = \\arg\\min_{k \\in \\mathcal{K}} C_{\\mathrm{MAP}}(k)$。\n\n#### 3. 矩方法 (MoM) 估计器\n该估计器基于 CUSUM 型统计量构建。\n1.  计算全局样本均值 $\\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$ 和全局样本方差 $S^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\overline{X})^2$。\n2.  形成两个中心化序列：\n    - 一阶矩序列：$Y^{(1)}_i = X_i - \\overline{X}$\n    - 二阶矩序列：$Y^{(2)}_i = (X_i - \\overline{X})^2 - S^2$\n3.  将每个序列标准化，使其具有单位经验标准差。\n    - 令 $S_1 = \\mathrm{std}(Y^{(1)}) = \\sqrt{\\frac{1}{n}\\sum (Y_i^{(1)})^2} = S$。标准化序列为 $Z^{(1)}_i = Y^{(1)}_i / S_1$。\n    - 令 $S_2 = \\mathrm{std}(Y^{(2)}) = \\sqrt{\\frac{1}{n}\\sum (Y_i^{(2)})^2}$。标准化序列为 $Z^{(2)}_i = Y^{(2)}_i / S_2$。\n    - 如果标准差为零，则相应的标准化序列被视为零向量。\n4.  对于每个候选变点 $k \\in \\mathcal{K}$，计算统计量 $D(k)$ 作为累积和的绝对值之和：\n    $$ D(k) = \\left| \\sum_{i=1}^{k} Z^{(1)}_i \\right| + \\left| \\sum_{i=1}^{k} Z^{(2)}_i \\right| $$\n5.  MoM 估计器是使该统计量最大化的 $k$ 值：\n    $$ \\widehat{\\tau}_{\\mathrm{MoM}} = \\arg\\max_{k \\in \\mathcal{K}} D(k) $$\n\n#### 评估\n对于每个生成的数据集和真实变点 $\\tau^\\star$，如果估计的变点 $\\widehat{\\tau}$ 落在真实值的容差窗口 $w$ 内，即 $|\\widehat{\\tau} - \\tau^\\star| \\le w$，则认为是一次成功，其中 $w = \\max\\{3, \\mathrm{round}(0.05n)\\}$。实现将遵循这些推导，为每个测试用例生成数据，计算三个估计值，并评估其成功与否。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the change-point estimation simulation.\n    It generates data, applies MLE, MAP, and MoM estimators,\n    and evaluates their success based on the problem's criteria.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'case': 'A', 'n': 120, 'tau_star': 60, 'mu1': 0.0, 'sigma1': 1.0, 'mu2': 1.2, 'sigma2': 1.0, 'k0': 60, 'lambda': 0.02},\n        {'case': 'B', 'n': 200, 'tau_star': 30, 'mu1': 0.0, 'sigma1': 1.0, 'mu2': 0.6, 'sigma2': 1.0, 'k0': 100, 'lambda': 0.05},\n        {'case': 'C', 'n': 150, 'tau_star': 140, 'mu1': 0.0, 'sigma1': 0.8, 'mu2': 0.0, 'sigma2': 1.8, 'k0': 75, 'lambda': 0.03},\n        {'case': 'D', 'n': 100, 'tau_star': 50, 'mu1': 0.5, 'sigma1': 1.0, 'mu2': 0.5, 'sigma2': 1.8, 'k0': 80, 'lambda': 0.04},\n        {'case': 'E', 'n': 80, 'tau_star': 10, 'mu1': -0.5, 'sigma1': 0.8, 'mu2': 0.5, 'sigma2': 0.8, 'k0': 20, 'lambda': 0.08},\n        {'case': 'F', 'n': 180, 'tau_star': 90, 'mu1': 0.0, 'sigma1': 1.0, 'mu2': 0.3, 'sigma2': 1.0, 'k0': 90, 'lambda': 0.02},\n    ]\n\n    # Initialize random number generator with fixed seed for reproducibility.\n    R = np.random.default_rng(20231123)\n\n    results = []\n\n    def estimate_mle_map(X, k_min, k_max, k0=None, lambda_=None):\n        \"\"\"\n        Computes MLE and MAP estimates. If k0 and lambda_ are None, computes MLE.\n        \"\"\"\n        n = len(X)\n        costs = np.full(k_max - k_min + 1, np.inf)\n\n        for i, k in enumerate(range(k_min, k_max + 1)):\n            seg1 = X[:k]\n            seg2 = X[k:]\n            \n            var1 = np.var(seg1)\n            var2 = np.var(seg2)\n\n            if var1 > 0 and var2 > 0:\n                log_lik_cost = k * np.log(var1) + (n - k) * np.log(var2)\n                if k0 is not None and lambda_ is not None: # MAP case\n                    penalty = 2 * lambda_ * abs(k - k0)\n                    costs[i] = log_lik_cost + penalty\n                else: # MLE case\n                    costs[i] = log_lik_cost\n            # If variance is zero, cost is -inf which we treat as np.inf for minimization\n            # but given the problem setup, this is extremely unlikely and indicates a perfect fit.\n            # We stick to minimizing `costs`, where a zero variance would yield a very low value.\n            # `np.log(0)` is `-inf`, so this case gets selected if it occurs.\n        \n        # argmin gets the first occurrence, fulfilling the tie-breaking rule.\n        best_idx = np.argmin(costs)\n        return k_min + best_idx\n\n    def estimate_mom(X, k_min, k_max):\n        \"\"\"\n        Computes the Method of Moments estimate.\n        \"\"\"\n        n = len(X)\n        mean_X = np.mean(X)\n        var_X = np.var(X)\n        \n        Y1 = X - mean_X\n        Y2 = (X - mean_X)**2 - var_X\n        \n        std_Y1 = np.std(Y1)\n        Z1 = np.divide(Y1, std_Y1, out=np.zeros_like(Y1), where=std_Y1!=0)\n        \n        std_Y2 = np.std(Y2)\n        Z2 = np.divide(Y2, std_Y2, out=np.zeros_like(Y2), where=std_Y2!=0)\n        \n        cusum1 = np.cumsum(Z1)\n        cusum2 = np.cumsum(Z2)\n        \n        search_range_indices = np.arange(k_min, k_max + 1)\n        # Python indexing: sum up to k is at index k-1\n        stats = np.abs(cusum1[search_range_indices - 1]) + np.abs(cusum2[search_range_indices - 1])\n        \n        # argmax gets the first occurrence, fulfilling the tie-breaking rule.\n        best_idx = np.argmax(stats)\n        return k_min + best_idx\n\n    for params in test_cases:\n        n = params['n']\n        tau_star = params['tau_star']\n        mu1, sigma1 = params['mu1'], params['sigma1']\n        mu2, sigma2 = params['mu2'], params['sigma2']\n        k0, lambda_ = params['k0'], params['lambda']\n\n        # Generate data\n        seg1 = R.normal(loc=mu1, scale=sigma1, size=tau_star)\n        seg2 = R.normal(loc=mu2, scale=sigma2, size=n - tau_star)\n        X = np.concatenate((seg1, seg2))\n        \n        k_min = 2\n        k_max = n - 2\n        \n        # Compute estimators\n        tau_hat_mle = estimate_mle_map(X, k_min, k_max)\n        tau_hat_map = estimate_mle_map(X, k_min, k_max, k0, lambda_)\n        tau_hat_mom = estimate_mom(X, k_min, k_max)\n        \n        # Evaluate success\n        w = int(max(3, round(0.05 * n)))\n        success_mle = abs(tau_hat_mle - tau_star) = w\n        success_map = abs(tau_hat_map - tau_star) = w\n        success_mom = abs(tau_hat_mom - tau_star) = w\n        \n        # Format result correctly for boolean values\n        case_result = [\n            int(tau_hat_mle), \n            int(tau_hat_map), \n            int(tau_hat_mom), \n            bool(success_mle), \n            bool(success_map), \n            bool(success_mom)\n        ]\n        results.append(str(case_result).lower().replace(\" \", \"\"))\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}