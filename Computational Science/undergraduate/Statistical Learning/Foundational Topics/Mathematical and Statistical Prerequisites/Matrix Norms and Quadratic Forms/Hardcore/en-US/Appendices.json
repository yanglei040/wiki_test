{
    "hands_on_practices": [
        {
            "introduction": "In many statistical learning applications, preprocessing features by 'whitening' them can significantly improve model performance. However, a common practical challenge arises when the empirical covariance matrix, $\\hat{\\Sigma}$, is rank-deficient, making the standard whitening transformation ill-defined. This exercise  guides you through a theoretical analysis of a robust solution, where the regularized covariance $\\hat{\\Sigma}_{\\epsilon} = \\hat{\\Sigma} + \\epsilon I$ is used instead. By deriving the exact distortion in the whitened norm, $x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x - x^{\\top} \\hat{\\Sigma}^{\\dagger} x$, you will gain a deeper understanding of how regularization works at the spectral level and its role in ensuring numerical stability.",
            "id": "3146475",
            "problem": "In a feature preprocessing pipeline for statistical learning, an estimated covariance matrix $\\hat{\\Sigma} \\in \\mathbb{R}^{4 \\times 4}$ is used for whitening. However, due to limited data, $\\hat{\\Sigma}$ is rank-deficient. To stabilize whitening, practitioners add a small ridge regularization and use $\\hat{\\Sigma}_{\\epsilon} = \\hat{\\Sigma} + \\epsilon I$ with $\\epsilon > 0$. The resulting approximate whitening transform is $\\hat{\\Sigma}_{\\epsilon}^{-1/2}$, and the corresponding whitened squared norm for a feature vector $x \\in \\mathbb{R}^{4}$ is $x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x$ by the identity $\\|\\hat{\\Sigma}_{\\epsilon}^{-1/2} x\\|_{2}^{2} = x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x$.\n\nAssume $\\hat{\\Sigma}$ is symmetric positive semidefinite with orthonormal eigenvectors $\\{u_{1}, u_{2}, u_{3}, u_{4}\\}$ and eigenvalues $\\lambda_{1} = 5$, $\\lambda_{2} = 2$, $\\lambda_{3} = 0$, $\\lambda_{4} = 0$. Consider a feature vector $x$ with spectral decomposition $x = 3 u_{1} + 2 u_{2} + 1 u_{3} + 2 u_{4}$.\n\nAn idealized notion of whitening that ignores directions in the nullspace of $\\hat{\\Sigma}$ uses the Moore–Penrose pseudoinverse (MPP) $\\hat{\\Sigma}^{\\dagger}$, yielding the ideal whitened squared norm $x^{\\top} \\hat{\\Sigma}^{\\dagger} x$.\n\nStarting only from the spectral theorem for real symmetric matrices and the definitions above, derive an exact analytic expression for the distortion\n$$\nD(\\epsilon) \\equiv x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x \\;-\\; x^{\\top} \\hat{\\Sigma}^{\\dagger} x\n$$\nas a function of $\\epsilon$. Express your final answer as a single closed-form analytic expression in terms of $\\epsilon$. No numerical approximation is required.",
            "solution": "The problem is validated as being scientifically grounded, well-posed, and objective. It is a standard problem in linear algebra applied to statistical learning concepts. All necessary information is provided, and the problem is free of contradictions or ambiguities. I will now proceed with the solution.\n\nThe solution rests on the spectral theorem for real symmetric matrices. The matrix $\\hat{\\Sigma} \\in \\mathbb{R}^{4 \\times 4}$ is symmetric and thus has a spectral decomposition. We are given its orthonormal eigenvectors $\\{u_{1}, u_{2}, u_{3}, u_{4}\\}$ and corresponding eigenvalues $\\lambda_{1} = 5$, $\\lambda_{2} = 2$, $\\lambda_{3} = 0$, and $\\lambda_{4} = 0$. The spectral decomposition of $\\hat{\\Sigma}$ can be written as:\n$$\n\\hat{\\Sigma} = \\sum_{i=1}^{4} \\lambda_i u_i u_i^{\\top}\n$$\n\nFirst, we analyze the regularized matrix $\\hat{\\Sigma}_{\\epsilon} = \\hat{\\Sigma} + \\epsilon I$, where $I$ is the $4 \\times 4$ identity matrix and $\\epsilon > 0$. The eigenvectors $u_i$ of $\\hat{\\Sigma}$ are also the eigenvectors of $\\hat{\\Sigma}_{\\epsilon}$:\n$$\n\\hat{\\Sigma}_{\\epsilon} u_i = (\\hat{\\Sigma} + \\epsilon I) u_i = \\hat{\\Sigma} u_i + \\epsilon I u_i = \\lambda_i u_i + \\epsilon u_i = (\\lambda_i + \\epsilon) u_i\n$$\nThus, the eigenvalues of $\\hat{\\Sigma}_{\\epsilon}$ are $\\lambda_i' = \\lambda_i + \\epsilon$. For the given eigenvalues of $\\hat{\\Sigma}$, the eigenvalues of $\\hat{\\Sigma}_{\\epsilon}$ are $5+\\epsilon$, $2+\\epsilon$, $\\epsilon$, and $\\epsilon$. Since $\\epsilon > 0$, all eigenvalues of $\\hat{\\Sigma}_{\\epsilon}$ are strictly positive, which means $\\hat{\\Sigma}_{\\epsilon}$ is invertible.\n\nThe inverse matrix $\\hat{\\Sigma}_{\\epsilon}^{-1}$ has the same eigenvectors $u_i$ with eigenvalues that are the reciprocal of the eigenvalues of $\\hat{\\Sigma}_{\\epsilon}$. The spectral decomposition of $\\hat{\\Sigma}_{\\epsilon}^{-1}$ is:\n$$\n\\hat{\\Sigma}_{\\epsilon}^{-1} = \\sum_{i=1}^{4} \\frac{1}{\\lambda_i + \\epsilon} u_i u_i^{\\top}\n$$\n\nNext, we compute the quadratic form $x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x$. The vector $x$ is given in its spectral decomposition with respect to the basis $\\{u_i\\}$:\n$$\nx = 3 u_{1} + 2 u_{2} + 1 u_{3} + 2 u_{4} = \\sum_{i=1}^{4} c_i u_i\n$$\nwhere the coefficients are $c_1=3$, $c_2=2$, $c_3=1$, and $c_4=2$. Substituting the expansions for $x$ and $\\hat{\\Sigma}_{\\epsilon}^{-1}$ into the quadratic form:\n$$\nx^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x = \\left( \\sum_{j=1}^{4} c_j u_j^{\\top} \\right) \\left( \\sum_{i=1}^{4} \\frac{1}{\\lambda_i + \\epsilon} u_i u_i^{\\top} \\right) \\left( \\sum_{k=1}^{4} c_k u_k \\right)\n$$\nUsing the orthonormality property of the eigenvectors, $u_j^{\\top} u_i = \\delta_{ji}$ (the Kronecker delta), the expression simplifies significantly:\n$$\nx^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x = \\sum_{i=1}^{4} \\frac{c_i^2}{\\lambda_i + \\epsilon}\n$$\nSubstituting the given values for $\\lambda_i$ and $c_i$:\n$$\nx^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x = \\frac{3^2}{5 + \\epsilon} + \\frac{2^2}{2 + \\epsilon} + \\frac{1^2}{0 + \\epsilon} + \\frac{2^2}{0 + \\epsilon} = \\frac{9}{5 + \\epsilon} + \\frac{4}{2 + \\epsilon} + \\frac{1}{\\epsilon} + \\frac{4}{\\epsilon} = \\frac{9}{5 + \\epsilon} + \\frac{4}{2 + \\epsilon} + \\frac{5}{\\epsilon}\n$$\n\nNow, we consider the Moore-Penrose pseudoinverse (MPP), $\\hat{\\Sigma}^{\\dagger}$. For a symmetric matrix, its MPP is found by taking the reciprocal of the non-zero eigenvalues and keeping the zero eigenvalues as zero. So, $\\hat{\\Sigma}^{\\dagger}$ has the same eigenvectors $u_i$ with eigenvalues $\\lambda_i^{\\dagger}$ defined as $\\lambda_i^{\\dagger} = 1/\\lambda_i$ if $\\lambda_i \\neq 0$, and $\\lambda_i^{\\dagger}=0$ if $\\lambda_i=0$.\nThe eigenvalues of $\\hat{\\Sigma}^{\\dagger}$ are:\n$\\lambda_1^{\\dagger} = 1/5$, $\\lambda_2^{\\dagger} = 1/2$, $\\lambda_3^{\\dagger} = 0$, $\\lambda_4^{\\dagger} = 0$.\nThe quadratic form $x^{\\top} \\hat{\\Sigma}^{\\dagger} x$ is computed similarly:\n$$\nx^{\\top} \\hat{\\Sigma}^{\\dagger} x = \\sum_{i=1}^{4} c_i^2 \\lambda_i^{\\dagger} = c_1^2 \\lambda_1^{\\dagger} + c_2^2 \\lambda_2^{\\dagger} + c_3^2 \\lambda_3^{\\dagger} + c_4^2 \\lambda_4^{\\dagger}\n$$\nSubstituting the values:\n$$\nx^{\\top} \\hat{\\Sigma}^{\\dagger} x = (3^2) \\left(\\frac{1}{5}\\right) + (2^2) \\left(\\frac{1}{2}\\right) + (1^2)(0) + (2^2)(0) = \\frac{9}{5} + \\frac{4}{2} = \\frac{9}{5} + 2\n$$\n\nFinally, we compute the distortion $D(\\epsilon) = x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x - x^{\\top} \\hat{\\Sigma}^{\\dagger} x$:\n$$\nD(\\epsilon) = \\left( \\frac{9}{5 + \\epsilon} + \\frac{4}{2 + \\epsilon} + \\frac{5}{\\epsilon} \\right) - \\left( \\frac{9}{5} + 2 \\right)\n$$\nTo obtain a single closed-form expression, we combine these terms into a single rational function. We can group the terms as follows:\n$$\nD(\\epsilon) = \\left( \\frac{9}{5 + \\epsilon} - \\frac{9}{5} \\right) + \\left( \\frac{4}{2 + \\epsilon} - 2 \\right) + \\frac{5}{\\epsilon}\n$$\n$$\nD(\\epsilon) = \\frac{9 \\cdot 5 - 9(5 + \\epsilon)}{5(5 + \\epsilon)} + \\frac{4 - 2(2 + \\epsilon)}{2 + \\epsilon} + \\frac{5}{\\epsilon}\n$$\n$$\nD(\\epsilon) = \\frac{45 - 45 - 9\\epsilon}{5(5 + \\epsilon)} + \\frac{4 - 4 - 2\\epsilon}{2 + \\epsilon} + \\frac{5}{\\epsilon}\n$$\n$$\nD(\\epsilon) = \\frac{-9\\epsilon}{5(\\epsilon + 5)} - \\frac{2\\epsilon}{\\epsilon + 2} + \\frac{5}{\\epsilon}\n$$\nNow, we find a common denominator, which is $5\\epsilon(\\epsilon+2)(\\epsilon+5)$:\n$$\nD(\\epsilon) = \\frac{-9\\epsilon \\cdot \\epsilon(\\epsilon+2) - 2\\epsilon \\cdot 5\\epsilon(\\epsilon+5) + 5 \\cdot 5(\\epsilon+2)(\\epsilon+5)}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n$$\nD(\\epsilon) = \\frac{-9\\epsilon^2(\\epsilon+2) - 10\\epsilon^2(\\epsilon+5) + 25(\\epsilon^2+7\\epsilon+10)}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n$$\nD(\\epsilon) = \\frac{(-9\\epsilon^3 - 18\\epsilon^2) - (10\\epsilon^3 + 50\\epsilon^2) + (25\\epsilon^2 + 175\\epsilon + 250)}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\nCombining terms in the numerator by powers of $\\epsilon$:\n$$\nD(\\epsilon) = \\frac{\\epsilon^3(-9-10) + \\epsilon^2(-18-50+25) + \\epsilon(175) + 250}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n$$\nD(\\epsilon) = \\frac{-19\\epsilon^3 - 43\\epsilon^2 + 175\\epsilon + 250}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\nExpanding the denominator gives the final expression:\n$$\nD(\\epsilon) = \\frac{-19\\epsilon^3 - 43\\epsilon^2 + 175\\epsilon + 250}{5\\epsilon^3 + 35\\epsilon^2 + 50\\epsilon}\n$$\nThis is the required single closed-form analytic expression for the distortion $D(\\epsilon)$.",
            "answer": "$$\n\\boxed{\\frac{-19\\epsilon^{3} - 43\\epsilon^{2} + 175\\epsilon + 250}{5\\epsilon^{3} + 35\\epsilon^{2} + 50\\epsilon}}\n$$"
        },
        {
            "introduction": "The performance of a machine learning model is fundamentally tied to the quality of its input data. This practice  explores this relationship through a controlled thought experiment where a clean feature matrix $X$ is systematically corrupted. You will use the spectral norm $\\|X\\|_{2}$ and the quadratic loss $L(w) = \\frac{1}{2}\\|Xw - y\\|_{2}^{2}$ to precisely measure the impact of this corruption, and more importantly, see how a ridge regularization parameter $\\lambda$ can be tuned to counteract these effects, providing a clear illustration of regularization as a tool for enhancing model robustness.",
            "id": "3146460",
            "problem": "Consider a supervised learning setting with a feature matrix $X \\in \\mathbb{R}^{2 \\times 2}$ and response vector $y \\in \\mathbb{R}^{2}$. You will analyze how a rank-one corruption of the features affects the spectral norm and a quadratic loss, and then derive a regularization choice that mitigates the corruption along its dominant direction.\n\nUse the following fundamental definitions as your starting point:\n- The spectral norm $\\|A\\|_{2}$ of a matrix $A$ is the largest singular value of $A$, equivalently $\\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^{\\top}A)}$ where $\\lambda_{\\max}(\\cdot)$ denotes the largest eigenvalue.\n- The empirical quadratic loss for weights $w \\in \\mathbb{R}^{2}$ is $L(w) = \\frac{1}{2}\\|Xw - y\\|_{2}^{2}$.\n- Ordinary least squares (OLS) solves $\\min_{w} \\frac{1}{2}\\|Xw - y\\|_{2}^{2}$.\n- Ridge regression solves $\\min_{w} \\frac{1}{2}\\|Xw - y\\|_{2}^{2} + \\frac{\\lambda}{2}\\|w\\|_{2}^{2}$ with $\\lambda > 0$.\n\nLet the clean data be\n$$\nX = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\nAssume the features are corrupted by a rank-one perturbation of the form $\\Delta X = u v^{\\top}$ where\n$$\nu = \\begin{pmatrix} a \\\\ 0 \\end{pmatrix}, \\quad v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad a \\geq 0,\n$$\nso that $\\Delta X = \\begin{pmatrix} a & 0 \\\\ 0 & 0 \\end{pmatrix}$ and the corrupted feature matrix is $X_{\\text{cor}} = X + \\Delta X$.\n\nTasks:\n1. Using only the definition of the spectral norm, compute the exact change in the spectral norm due to the corruption, i.e., find $\\|X_{\\text{cor}}\\|_{2} - \\|X\\|_{2}$, as an expression in $a$.\n2. Let $w_{\\text{OLS}}$ denote an OLS solution trained on the clean data $(X,y)$. Compute the exact change in the empirical quadratic loss when $w_{\\text{OLS}}$ is evaluated on the corrupted features, i.e., find $L_{\\text{cor}}(w_{\\text{OLS}}) - L(w_{\\text{OLS}})$, as an expression in $a$, where $L_{\\text{cor}}(w) = \\frac{1}{2}\\|X_{\\text{cor}}w - y\\|_{2}^{2}$.\n3. Train ridge regression on the clean data to obtain $w_{\\lambda}$ as a function of $\\lambda$. Derive a value of the regularization parameter $\\lambda$ (as a function of $a$) that completely cancels the corruption in the first coordinate of the residual vector when $w_{\\lambda}$ is evaluated on the corrupted features, i.e., choose $\\lambda$ so that the first entry of $(X_{\\text{cor}}w_{\\lambda} - y)$ is exactly zero.\n\nReport your final results for Tasks 1–3 as exact expressions in terms of $a$ in a single row matrix, in the order: change in spectral norm, change in quadratic loss, and the mitigating regularization parameter. No rounding is required. Express your final answer without units.",
            "solution": "The problem statement is well-posed, mathematically consistent, and grounded in the principles of linear algebra and statistical learning. We shall proceed with solving the three tasks in the specified order.\n\nLet the given matrices and vectors be:\n$$\nX = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\nThe corruption is given by $\\Delta X = \\begin{pmatrix} a & 0 \\\\ 0 & 0 \\end{pmatrix}$ for $a \\geq 0$. The corrupted matrix is:\n$$\nX_{\\text{cor}} = X + \\Delta X = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} a & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 3+a & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n\n**Task 1: Change in Spectral Norm**\n\nThe spectral norm of a matrix $A$ is defined as $\\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^{\\top}A)}$.\n\nFirst, we compute the spectral norm of the clean matrix $X$.\n$$\nX^{\\top}X = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 9 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe eigenvalues of this diagonal matrix are its diagonal entries, $\\lambda_1 = 9$ and $\\lambda_2 = 1$. The largest eigenvalue is $\\lambda_{\\max}(X^{\\top}X) = 9$.\nTherefore, the spectral norm of $X$ is:\n$$\n\\|X\\|_{2} = \\sqrt{9} = 3\n$$\n\nNext, we compute the spectral norm of the corrupted matrix $X_{\\text{cor}}$.\n$$\nX_{\\text{cor}}^{\\top}X_{\\text{cor}} = \\begin{pmatrix} 3+a & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3+a & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} (3+a)^2 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe eigenvalues are $(3+a)^2$ and $1$. Since $a \\geq 0$, we have $3+a \\geq 3$, and thus $(3+a)^2 \\geq 9$. The largest eigenvalue is $\\lambda_{\\max}(X_{\\text{cor}}^{\\top}X_{\\text{cor}}) = (3+a)^2$.\nTherefore, the spectral norm of $X_{\\text{cor}}$ is:\n$$\n\\|X_{\\text{cor}}\\|_{2} = \\sqrt{(3+a)^2} = |3+a| = 3+a\n$$\nThe change in the spectral norm is:\n$$\n\\|X_{\\text{cor}}\\|_{2} - \\|X\\|_{2} = (3+a) - 3 = a\n$$\n\n**Task 2: Change in Quadratic Loss**\n\nFirst, we find the ordinary least squares (OLS) solution $w_{\\text{OLS}}$ for the clean data $(X, y)$. The OLS solution minimizes $L(w) = \\frac{1}{2}\\|Xw - y\\|_{2}^{2}$ and is given by $w_{\\text{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y$. Since $X$ is invertible, this simplifies to $w_{\\text{OLS}} = X^{-1}y$.\n$$\nX^{-1} = \\begin{pmatrix} \\frac{1}{3} & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n$$\nw_{\\text{OLS}} = \\begin{pmatrix} \\frac{1}{3} & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nThe loss on the clean data for this solution is:\n$$\nL(w_{\\text{OLS}}) = \\frac{1}{2}\\|Xw_{\\text{OLS}} - y\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\|\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\|_{2}^{2} = 0\n$$\nNow, we evaluate the loss for $w_{\\text{OLS}}$ on the corrupted data, $L_{\\text{cor}}(w_{\\text{OLS}}) = \\frac{1}{2}\\|X_{\\text{cor}}w_{\\text{OLS}} - y\\|_{2}^{2}$.\n$$\nL_{\\text{cor}}(w_{\\text{OLS}}) = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3+a & 0 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3+a \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} a \\\\ 0 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}(a^2) = \\frac{a^2}{2}\n$$\nThe change in the empirical quadratic loss is:\n$$\nL_{\\text{cor}}(w_{\\text{OLS}}) - L(w_{\\text{OLS}}) = \\frac{a^2}{2} - 0 = \\frac{a^2}{2}\n$$\n\n**Task 3: Mitigating Regularization Parameter**\n\nWe first find the ridge regression solution $w_{\\lambda}$ trained on the clean data, which is given by $w_{\\lambda} = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y$.\nWe have already computed $X^{\\top}X = \\begin{pmatrix} 9 & 0 \\\\ 0 & 1 \\end{pmatrix}$. We also need $X^{\\top}y$:\n$$\nX^{\\top}y = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 1 \\end{pmatrix}\n$$\nNow we construct the term to be inverted:\n$$\nX^{\\top}X + \\lambda I = \\begin{pmatrix} 9 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{pmatrix} = \\begin{pmatrix} 9+\\lambda & 0 \\\\ 0 & 1+\\lambda \\end{pmatrix}\n$$\nIts inverse is:\n$$\n(X^{\\top}X + \\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{9+\\lambda} & 0 \\\\ 0 & \\frac{1}{1+\\lambda} \\end{pmatrix}\n$$\nThe ridge solution is then:\n$$\nw_{\\lambda} = \\begin{pmatrix} \\frac{1}{9+\\lambda} & 0 \\\\ 0 & \\frac{1}{1+\\lambda} \\end{pmatrix} \\begin{pmatrix} 9 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{9}{9+\\lambda} \\\\ \\frac{1}{1+\\lambda} \\end{pmatrix}\n$$\nWe need to find $\\lambda$ such that the first entry of the residual vector $(X_{\\text{cor}}w_{\\lambda} - y)$ is zero. Let this residual be $r_{\\text{cor}}$.\n$$\nr_{\\text{cor}} = X_{\\text{cor}}w_{\\lambda} - y = \\begin{pmatrix} 3+a & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{9}{9+\\lambda} \\\\ \\frac{1}{1+\\lambda} \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (3+a)\\frac{9}{9+\\lambda} \\\\ \\frac{1}{1+\\lambda} \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{9(3+a)}{9+\\lambda} - 3 \\\\ \\frac{1}{1+\\lambda} - 1 \\end{pmatrix}\n$$\nSetting the first entry to zero:\n$$\n\\frac{9(3+a)}{9+\\lambda} - 3 = 0\n$$\n$$\n9(3+a) = 3(9+\\lambda)\n$$\nDividing by $3$:\n$$\n3(3+a) = 9+\\lambda\n$$\n$$\n9 + 3a = 9 + \\lambda\n$$\nSolving for $\\lambda$ yields:\n$$\n\\lambda = 3a\n$$\nThis value of $\\lambda$ ensures the corruption's effect is cancelled in the first coordinate of the residual. The condition $\\lambda > 0$ for ridge regression is satisfied for any $a>0$. If $a=0$, $\\lambda=0$, corresponding to the OLS case where the residual is already zero.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} a & \\frac{a^2}{2} & 3a \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Moving from theoretical analysis to practical implementation is a key step in mastering statistical learning. This final practice  challenges you to build a complete algorithm that automates the control of a model's properties. You will implement a randomized algorithm to efficiently estimate the spectral norm $\\|\\hat{\\Sigma}\\|_2$ of a covariance matrix and use this estimate to dynamically tune a regularization parameter $\\lambda$, ensuring a critical variance constraint $w_{\\lambda}^{\\top} \\hat{\\Sigma} w_{\\lambda} \\le \\tau$ is met. This task bridges theory and code, demonstrating how abstract mathematical concepts are used to engineer robust, real-world machine learning systems.",
            "id": "3146495",
            "problem": "Consider a centered data matrix $X \\in \\mathbb{R}^{n \\times d}$, with empirical covariance estimator $\\hat{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ defined as\n$$\n\\hat{\\Sigma} = \\frac{1}{n} X^\\top X,\n$$\nwhere each column of $X$ has been mean-centered. Let $w \\in \\mathbb{R}^d$ be a given weight vector and let $\\tau \\in \\mathbb{R}_{\\geq 0}$ be a prescribed tolerance. The spectral norm of a matrix $A$, denoted by $\\|A\\|_2$, is the largest singular value of $A$. For a symmetric positive semidefinite matrix (such as a covariance), $\\|A\\|_2$ equals its largest eigenvalue. A quadratic form is given by $w^\\top \\hat{\\Sigma} w$. In statistical learning, controlling $w^\\top \\hat{\\Sigma} w$ can be interpreted as bounding the variance induced by model weights under the empirical covariance.\n\nYour task is to design a complete, runnable program that:\n- Constructs an estimator of the spectral norm $\\|\\hat{\\Sigma}\\|_2$ using a randomized range finder based on the Singular Value Decomposition (SVD), where the randomized range finder uses Gaussian sketching with an oversampling parameter and a fixed number of power iterations.\n- Uses the estimated spectral norm to propose and then safely tune a regularization parameter $\\lambda \\in \\mathbb{R}_{\\geq 0}$ that scales the weights via the rule\n$$\nw_\\lambda = \\frac{1}{1 + \\lambda} \\, w,\n$$\nso that the resulting quadratic form satisfies\n$$\nw_\\lambda^\\top \\hat{\\Sigma} \\, w_\\lambda \\le \\tau.\n$$\nYour derivation and algorithm must start from core definitions and facts: properties of covariance matrices, spectral norm, and quadratic forms; and the general idea of randomized SVD as a well-tested numerical method for estimating dominant singular values. You must not rely on any shortcut formulas not justified from these bases. The estimator and tuning rule must ensure the inequality $w_\\lambda^\\top \\hat{\\Sigma} \\, w_\\lambda \\le \\tau$ holds for the actual empirical covariance, even when the spectral norm estimator is imperfect; if the initial proposal based on the estimator does not satisfy the inequality, your algorithm must safely adjust $\\lambda$ to achieve it.\n\nImplement the following test suite of four cases. In each case, $X$ must be constructed deterministically using the specified random seed and a prescribed generative model. Then compute $\\hat{\\Sigma}$, estimate $\\|\\hat{\\Sigma}\\|_2$ via randomized SVD with target rank $k = 1$, oversampling $p$, and power iterations $q$, tune $\\lambda$ to satisfy the inequality, and report the results.\n\nTest case $1$ (full-rank diagonal covariance):\n- Dimensions: $n = 200$, $d = 5$.\n- Random seed: $s = 42$.\n- Construct $X$ by drawing $Z \\in \\mathbb{R}^{n \\times d}$ with independent standard normal entries and setting $X = Z \\, D^{1/2}$, where $D = \\operatorname{diag}(3.0, 2.0, 1.0, 0.5, 0.1)$ and $D^{1/2} = \\operatorname{diag}(\\sqrt{3.0}, \\sqrt{2.0}, \\sqrt{1.0}, \\sqrt{0.5}, \\sqrt{0.1})$; then mean-center each column of $X$.\n- Weight vector: $w = (1.0, -0.5, 0.75, 0.0, -1.25)$.\n- Tolerance: $\\tau = 3.5$.\n- Randomized SVD parameters: target rank $k = 1$, oversampling $p = 4$, power iterations $q = 2$.\n\nTest case $2$ (low-rank covariance):\n- Dimensions: $n = 100$, $d = 6$.\n- Random seed: $s = 7$.\n- Construct $X$ by drawing $Z \\in \\mathbb{R}^{n \\times 2}$ with independent standard normal entries and $B \\in \\mathbb{R}^{2 \\times d}$ with independent standard normal entries using the same seed, and set $X = Z B$; then mean-center each column of $X$.\n- Weight vector: $w = (0.5, -0.5, 0.5, -0.5, 0.5, -0.5)$.\n- Tolerance: $\\tau = 1.0$.\n- Randomized SVD parameters: target rank $k = 1$, oversampling $p = 4$, power iterations $q = 2$.\n\nTest case $3$ (stringent tolerance):\n- Dimensions: $n = 50$, $d = 4$.\n- Random seed: $s = 0$.\n- Construct $X$ by drawing $Z \\in \\mathbb{R}^{n \\times d}$ with independent standard normal entries and setting $X = Z \\, D^{1/2}$, where $D = \\operatorname{diag}(2.5, 1.5, 0.3, 0.1)$; then mean-center each column of $X$.\n- Weight vector: $w = (2.0, 1.0, -1.0, 0.5)$.\n- Tolerance: $\\tau = 0.05$.\n- Randomized SVD parameters: target rank $k = 1$, oversampling $p = 4$, power iterations $q = 2$.\n\nTest case $4$ (zero weights):\n- Dimensions: $n = 60$, $d = 5$.\n- Random seed: $s = 123$.\n- Construct $X$ by drawing $Z \\in \\mathbb{R}^{n \\times d}$ with independent standard normal entries and setting $X = Z \\, D^{1/2}$, where $D = \\operatorname{diag}(1.0, 0.8, 0.6, 0.4, 0.2)$; then mean-center each column of $X$.\n- Weight vector: $w = (0.0, 0.0, 0.0, 0.0, 0.0)$.\n- Tolerance: $\\tau = 1.0$.\n- Randomized SVD parameters: target rank $k = 1$, oversampling $p = 4$, power iterations $q = 2$.\n\nAlgorithmic requirements:\n- Center the columns of $X$ before forming $\\hat{\\Sigma}$.\n- Implement a randomized range finder to estimate $\\|\\hat{\\Sigma}\\|_2$: draw a Gaussian sketch $\\Omega \\in \\mathbb{R}^{d \\times (k+p)}$, form $Y = \\hat{\\Sigma} \\Omega$, apply $q$ power iterations $Y \\leftarrow \\hat{\\Sigma} Y$ repeatedly, compute an orthonormal basis $Q$ for the columns of $Y$ via a QR decomposition, and then estimate the top singular value of $Q^\\top \\hat{\\Sigma} Q$.\n- Propose $\\lambda$ using the spectral norm estimate and the scaling rule $w_\\lambda = \\frac{1}{1+\\lambda} w$. If the proposal does not satisfy $w_\\lambda^\\top \\hat{\\Sigma} w_\\lambda \\le \\tau$, safely increase $\\lambda$ so that the inequality holds for the actual quadratic form.\n\nFinal output format:\n- For each test case, output the list $[\\text{norm\\_est}, \\text{lambda}, \\text{qf\\_final}, \\text{satisfied}]$, where $\\text{norm\\_est}$ is a floating-point estimate of $\\|\\hat{\\Sigma}\\|_2$, $\\text{lambda}$ is the tuned regularization parameter (a floating-point number), $\\text{qf\\_final}$ is the resulting quadratic form $w_\\lambda^\\top \\hat{\\Sigma} w_\\lambda$ (a floating-point number), and $\\text{satisfied}$ is a boolean indicating whether $w_\\lambda^\\top \\hat{\\Sigma} w_\\lambda \\le \\tau$ holds.\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, with no spaces; for example, in the form $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$.",
            "solution": "The problem requires the design and implementation of an algorithm to tune a regularization parameter, $\\lambda \\in \\mathbb{R}_{\\geq 0}$, for a weight vector $w \\in \\mathbb{R}^d$. The tuning must ensure that a specific quadratic form, which represents a measure of variance, is bounded by a given tolerance $\\tau \\in \\mathbb{R}_{\\geq 0}$. The weight vector is scaled according to the rule $w_\\lambda = \\frac{1}{1 + \\lambda} w$, and the constraint to be satisfied is $w_\\lambda^\\top \\hat{\\Sigma} w_\\lambda \\le \\tau$, where $\\hat{\\Sigma} = \\frac{1}{n} X^\\top X$ is the empirical covariance matrix of centered data $X \\in \\mathbb{R}^{n \\times d}$. The tuning process must use an estimate of the spectral norm of the covariance matrix, $\\|\\hat{\\Sigma}\\|_2$, obtained via a randomized range-finding algorithm, and must include a safety mechanism to guarantee the constraint is met.\n\nOur derivation proceeds in three stages: first, we analyze the constraint to find the exact regularization required; second, we derive a proposal for the regularization parameter based on the spectral norm estimate; and third, we combine these into a complete, safe tuning algorithm.\n\n**1. Analysis of the Regularization Constraint**\n\nThe core of the problem is the inequality:\n$$\nw_\\lambda^\\top \\hat{\\Sigma} \\, w_\\lambda \\le \\tau\n$$\nSubstituting the definition of the scaled weight vector, $w_\\lambda = \\frac{1}{1 + \\lambda} w$, we get:\n$$\n\\left(\\frac{1}{1 + \\lambda} w\\right)^\\top \\hat{\\Sigma} \\left(\\frac{1}{1 + \\lambda} w\\right) \\le \\tau\n$$\nSince $\\lambda \\ge 0$, the term $(1 + \\lambda)$ is a positive scalar. We can factor it out:\n$$\n\\frac{1}{(1 + \\lambda)^2} w^\\top \\hat{\\Sigma} w \\le \\tau\n$$\nLet us define the unscaled quadratic form as $Q_0 = w^\\top \\hat{\\Sigma} w$. The inequality simplifies to:\n$$\n\\frac{Q_0}{(1 + \\lambda)^2} \\le \\tau\n$$\nTwo trivial cases can be immediately identified:\n- If the initial quadratic form already satisfies the constraint, i.e., $Q_0 \\le \\tau$, then no regularization is needed. We can set $\\lambda = 0$.\n- If the weight vector is the zero vector, $w=0$, then $Q_0 = 0$. Since the tolerance is non-negative, $\\tau \\ge 0$, the constraint $0 \\le \\tau$ is always met. Thus, $\\lambda=0$ is the appropriate choice.\n\nIf $Q_0 > \\tau$, we must find a $\\lambda > 0$. Rearranging the inequality gives:\n$$\n(1 + \\lambda)^2 \\ge \\frac{Q_0}{\\tau}\n$$\nSince $1 + \\lambda > 0$, we can take the square root of both sides:\n$$\n1 + \\lambda \\ge \\sqrt{\\frac{Q_0}{\\tau}} \\implies \\lambda \\ge \\sqrt{\\frac{Q_0}{\\tau}} - 1\n$$\nTo satisfy the constraint with the minimal amount of regularization, we should choose the smallest valid $\\lambda$. This gives us the exact, minimal value required:\n$$\n\\lambda_{exact} = \\sqrt{\\frac{Q_0}{\\tau}} - 1\n$$\nThis value serves as our \"safe\" fallback.\n\n**2. Proposal of $\\lambda$ via Spectral Norm Estimation**\n\nThe problem requires that we *propose* a value for $\\lambda$ using an estimate of the spectral norm of the covariance matrix, $\\|\\hat{\\Sigma}\\|_2$. The connection between the quadratic form $Q_0$ and the spectral norm $\\|\\hat{\\Sigma}\\|_2$ is established by the properties of Rayleigh quotients. For any vector $w$, we have the inequality:\n$$\nw^\\top \\hat{\\Sigma} w \\le \\lambda_{max}(\\hat{\\Sigma}) \\|w\\|_2^2\n$$\nwhere $\\lambda_{max}(\\hat{\\Sigma})$ is the largest eigenvalue of $\\hat{\\Sigma}$, which is equal to its spectral norm, $\\|\\hat{\\Sigma}\\|_2$, because $\\hat{\\Sigma}$ is symmetric and positive semidefinite.\n\nLet $\\tilde{\\lambda}_{max}$ be our estimate of $\\|\\hat{\\Sigma}\\|_2$. We can form an estimated upper bound for $Q_0$: $Q_0 \\le \\tilde{\\lambda}_{max} \\|w\\|_2^2$. A conservative approach is to choose a $\\lambda$ that satisfies the constraint for this upper bound:\n$$\n\\frac{1}{(1 + \\lambda)^2} \\left(\\tilde{\\lambda}_{max} \\|w\\|_2^2\\right) \\le \\tau\n$$\nSolving for $\\lambda$ in the same manner as before yields a proposed value:\n$$\n\\lambda_{prop} = \\max\\left(0, \\sqrt{\\frac{\\tilde{\\lambda}_{max} \\|w\\|_2^2}{\\tau}} - 1\\right)\n$$\nThis proposal depends on the quality of the estimate $\\tilde{\\lambda}_{max}$ and the slack in the Rayleigh quotient inequality.\n\n**3. The Complete, Safe Tuning Algorithm**\n\nWe now synthesize these components into a robust algorithm that proposes $\\lambda$ and then verifies its validity, correcting it if necessary.\n\n1.  **Initialization**: Compute the empirical covariance matrix $\\hat{\\Sigma} = \\frac{1}{n} X^\\top X$ from the centered data. Calculate the unscaled quadratic form $Q_0 = w^\\top \\hat{\\Sigma} w$.\n\n2.  **Trivial Check**: If $Q_0 \\le \\tau$, the constraint is already satisfied. Set the final parameter $\\lambda_{final} = 0$.\n\n3.  **Regularization Required**: If $Q_0 > \\tau$:\n    a. **Estimate Norm**: Compute the estimate $\\tilde{\\lambda}_{max} \\approx \\|\\hat{\\Sigma}\\|_2$ using the specified randomized algorithm.\n    b. **Propose $\\lambda$**: Calculate the proposed parameter $\\lambda_{prop} = \\max\\left(0, \\sqrt{\\frac{\\tilde{\\lambda}_{max} \\|w\\|_2^2}{\\tau}} - 1\\right)$.\n    c. **Safety Check**: Evaluate the quadratic form with the proposed parameter, $Q_{prop} = \\frac{Q_0}{(1 + \\lambda_{prop})^2}$.\n    d. **Finalize $\\lambda$**:\n        i. If $Q_{prop} \\le \\tau$, the proposal is safe and sufficient. Set $\\lambda_{final} = \\lambda_{prop}$.\n        ii. If $Q_{prop} > \\tau$, the proposal is insufficient. This may occur if $\\tilde{\\lambda}_{max}$ is an underestimate or the Rayleigh bound is loose. We revert to the exact minimal value: $\\lambda_{final} = \\sqrt{\\frac{Q_0}{\\tau}} - 1$.\n\n4.  **Final Result**: The final scaled weights are $w_{\\lambda_{final}} = \\frac{1}{1+\\lambda_{final}}w$, and the final quadratic form is $w_{\\lambda_{final}}^\\top \\hat{\\Sigma} w_{\\lambda_{final}}$, which is guaranteed to be less than or equal to $\\tau$.\n\n**Randomized Spectral Norm Estimation**\n\nThe estimation of $\\tilde{\\lambda}_{max} = \\|\\hat{\\Sigma}\\|_2$ is performed using a randomized range finder with power iterations. Given the matrix $\\hat{\\Sigma} \\in \\mathbb{R}^{d \\times d}$, a target rank $k$, an oversampling parameter $p$, and a number of power iterations $q$:\n\n1.  **Sketching**: Draw a random Gaussian matrix $\\Omega \\in \\mathbb{R}^{d \\times (k+p)}$.\n2.  **Range Approximation**: Form the sketched matrix $Y_0 = \\hat{\\Sigma} \\Omega$.\n3.  **Power Iterations**: Enhance the alignment of the sketched matrix with the dominant eigenspace of $\\hat{\\Sigma}$. For $i=1, \\dots, q$, update $Y_i = \\hat{\\Sigma} Y_{i-1}$. Let the final matrix be $Y = Y_q$.\n4.  **Orthonormalization**: Compute an orthonormal basis for the subspace spanned by the columns of $Y$ via a QR decomposition: $Y = QR$. The columns of $Q \\in \\mathbb{R}^{d \\times (k+p)}$ form this basis.\n5.  **Projection**: Form the small, projected matrix $B = Q^\\top \\hat{\\Sigma} Q \\in \\mathbb{R}^{(k+p) \\times (k+p)}$. The spectral properties of $B$ approximate those of $\\hat{\\Sigma}$.\n6.  **Estimation**: The spectral norm of $\\hat{\\Sigma}$ is estimated as the spectral norm of $B$: $\\tilde{\\lambda}_{max} = \\|B\\|_2$. Since $B$ is symmetric and positive semidefinite, its spectral norm is its largest eigenvalue, which can be found by computing the singular values of $B$ and taking the maximum.\n\nThis principled and robust procedure ensures that all problem requirements are met, including the critical safety guarantee.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    def process_case(n, d, seed, X_params, w, tau, rsvd_params):\n        \"\"\"\n        Executes a single test case from data generation to final result calculation.\n\n        Args:\n            n (int): Number of samples.\n            d (int): Number of features.\n            seed (int): Random seed for reproducibility.\n            X_params (dict): Parameters for generating the data matrix X.\n            w (list): The weight vector.\n            tau (float): The tolerance for the quadratic form.\n            rsvd_params (dict): Parameters for the randomized SVD.\n\n        Returns:\n            list: A list containing [norm_est, lambda_final, qf_final, satisfied].\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct data matrix X and center it\n        if X_params[\"type\"] == \"diag\":\n            Z = rng.standard_normal(size=(n, d))\n            D_sqrt = np.diag(np.sqrt(X_params[\"D_diag\"]))\n            X = Z @ D_sqrt\n        elif X_params[\"type\"] == \"low_rank\":\n            rank = X_params[\"rank\"]\n            Z = rng.standard_normal(size=(n, rank))\n            B = rng.standard_normal(size=(rank, d))\n            X = Z @ B\n        \n        X_centered = X - X.mean(axis=0)\n\n        # 2. Compute empirical covariance\n        Sigma_hat = (1/n) * (X_centered.T @ X_centered)\n\n        # 3. Estimate spectral norm using randomized range finder\n        k, p, q = rsvd_params['k'], rsvd_params['p'], rsvd_params['q']\n        l = k + p\n        \n        # Use the same RNG sequence for all random operations within a case\n        Omega = rng.standard_normal(size=(d, l))\n        \n        Y = Sigma_hat @ Omega\n        for _ in range(q):\n            Y = Sigma_hat @ Y\n        \n        Q, _ = np.linalg.qr(Y)\n        \n        B = Q.T @ Sigma_hat @ Q\n        \n        # Spectral norm of B is its largest singular value\n        s_values_B = np.linalg.svd(B, compute_uv=False)\n        norm_est = s_values_B[0] if s_values_B.size > 0 else 0.0\n\n        # 4. Propose and tune lambda\n        w_vec = np.array(w)\n        Q0 = w_vec.T @ Sigma_hat @ w_vec\n\n        lambda_final = 0.0\n        \n        if Q0 > tau:\n            w_norm_sq = np.sum(w_vec**2)\n            \n            # This check is mostly for logical completeness; Q0 > tau implies w is not a zero vector.\n            if np.isclose(w_norm_sq, 0):\n                 lambda_final = 0.0\n            else:\n                # Propose lambda based on the spectral norm estimate\n                Q0_est = norm_est * w_norm_sq\n                \n                # The proposal must be non-negative. tau must be > 0 here.\n                lambda_prop = max(0.0, np.sqrt(Q0_est / tau) - 1.0)\n                \n                # Safety check: see if the proposed lambda is sufficient\n                Q_prop = Q0 / (1.0 + lambda_prop)**2\n                \n                if Q_prop <= tau:\n                    lambda_final = lambda_prop\n                else:\n                    # Safety fallback: use the exact value derived from the true Q0\n                    lambda_final = np.sqrt(Q0 / tau) - 1.0\n\n        # 5. Calculate final quadratic form and check satisfaction\n        qf_final = Q0 / (1.0 + lambda_final)**2\n        satisfied = qf_final <= tau\n\n        return [norm_est, lambda_final, qf_final, bool(satisfied)]\n\n    test_cases = [\n        # Case 1\n        {\n            \"n\": 200, \"d\": 5, \"seed\": 42,\n            \"X_params\": {\"type\": \"diag\", \"D_diag\": [3.0, 2.0, 1.0, 0.5, 0.1]},\n            \"w\": [1.0, -0.5, 0.75, 0.0, -1.25],\n            \"tau\": 3.5,\n            \"rsvd_params\": {\"k\": 1, \"p\": 4, \"q\": 2}\n        },\n        # Case 2\n        {\n            \"n\": 100, \"d\": 6, \"seed\": 7,\n            \"X_params\": {\"type\": \"low_rank\", \"rank\": 2},\n            \"w\": [0.5, -0.5, 0.5, -0.5, 0.5, -0.5],\n            \"tau\": 1.0,\n            \"rsvd_params\": {\"k\": 1, \"p\": 4, \"q\": 2}\n        },\n        # Case 3\n        {\n            \"n\": 50, \"d\": 4, \"seed\": 0,\n            \"X_params\": {\"type\": \"diag\", \"D_diag\": [2.5, 1.5, 0.3, 0.1]},\n            \"w\": [2.0, 1.0, -1.0, 0.5],\n            \"tau\": 0.05,\n            \"rsvd_params\": {\"k\": 1, \"p\": 4, \"q\": 2}\n        },\n        # Case 4\n        {\n            \"n\": 60, \"d\": 5, \"seed\": 123,\n            \"X_params\": {\"type\": \"diag\", \"D_diag\": [1.0, 0.8, 0.6, 0.4, 0.2]},\n            \"w\": [0.0, 0.0, 0.0, 0.0, 0.0],\n            \"tau\": 1.0,\n            \"rsvd_params\": {\"k\": 1, \"p\": 4, \"q\": 2}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(process_case(**case))\n\n    # Format the final output string exactly as required:\n    # a list of lists, with no spaces and standard string representations.\n    outer_list = []\n    for res_list in results:\n        # Convert boolean to standard capitalized string representation, e.g., True\n        inner_list = [str(item) for item in res_list]\n        outer_list.append('[' + ','.join(inner_list) + ']')\n    final_output = '[' + ','.join(outer_list) + ']'\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}