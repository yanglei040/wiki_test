{
    "hands_on_practices": [
        {
            "introduction": "在理想世界中，我们总能精确地测量我们感兴趣的变量。然而，在现实世界中，从物理实验到社会科学调查，测量误差无处不在。这个练习将带你深入探讨一个基本但至关重要的问题：当我们的预测变量存在随机测量误差时，它如何系统性地影响我们对变量间关系的估计？通过运用协方差和方差的基本性质，你将亲自推导出著名的“衰减偏误”（attenuation bias），理解为什么测量误差会“削弱”我们观察到的相关性和回归系数 。",
            "id": "3119226",
            "problem": "考虑一个实值统计学习场景，其中单个预测变量存在经典测量误差。设未观测到的真实值为 $X^{\\text{true}}$，观测到的预测变量为 $X^{\\text{obs}} = X^{\\text{true}} + U$，其中 $U$ 是测量误差。响应变量由线性模型 $Y = \\alpha + \\beta X^{\\text{true}} + \\varepsilon$ 生成。假设 $E[U] = 0$，$E[\\varepsilon] = 0$，且 $U$ 与 $X^{\\text{true}}$ 和 $\\varepsilon$ 均独立，所有变量都具有有限二阶矩。定义 $\\sigma_{X}^{2} = \\operatorname{Var}(X^{\\text{true}})$，$\\sigma_{U}^{2} = \\operatorname{Var}(U)$ 以及 $\\sigma_{\\varepsilon}^{2} = \\operatorname{Var}(\\varepsilon)$。\n\n假设一位从业者使用普通最小二乘法 (OLS) 将 $Y$ 对 $X^{\\text{obs}}$ 进行回归，并忽略了测量误差的存在。令 $\\hat{\\beta}_{\\text{naive}}$ 表示将 $Y$ 对 $X^{\\text{obs}}$ 进行回归得到的总体 OLS 斜率。仅使用期望、方差、协方差和相关性的核心定义以及给定的独立性假设，推导：\n\n1. 衰减因子 $\\lambda = \\hat{\\beta}_{\\text{naive}} / \\beta$，用 $\\sigma_{X}^{2}$ 和 $\\sigma_{U}^{2}$ 表示。\n2. 相关性的乘性偏差比 $R = \\operatorname{Corr}(X^{\\text{obs}}, Y) / \\operatorname{Corr}(X^{\\text{true}}, Y)$，用 $\\sigma_{X}^{2}$ 和 $\\sigma_{U}^{2}$ 尽可能简化表示。\n\n将你的最终答案表示为一个包含两个闭式表达式 $(\\lambda, R)$ 的单行矩阵。无需四舍五入。",
            "solution": "该问题陈述是统计学习和计量经济学中一个定义明确的经典问题，具体涉及线性回归模型中预测变量存在测量误差的后果。所有必要的组成部分和假设都已明确给出，或通过标准模型定义隐含给出。模型 $Y = \\alpha + \\beta X^{\\text{true}} + \\varepsilon$ 是一个标准的线性模型，其基本假设是误差项 $\\varepsilon$ 与预测变量 $X^{\\text{true}}$ 不相关。因此，我们将在 $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$ 的标准假设下进行推导。所有推导都将基于给定的条件以及期望、方差、协方差和相关性的基本定义。\n\n令各量如问题陈述中定义：\n- 真实模型：$Y = \\alpha + \\beta X^{\\text{true}} + \\varepsilon$\n- 测量模型：$X^{\\text{obs}} = X^{\\text{true}} + U$\n- 方差：$\\sigma_{X}^{2} = \\operatorname{Var}(X^{\\text{true}})$，$\\sigma_{U}^{2} = \\operatorname{Var}(U)$，$\\sigma_{\\varepsilon}^{2} = \\operatorname{Var}(\\varepsilon)$\n- 假设：$E[U] = 0$，$E[\\varepsilon] = 0$。$U$ 与 $X^{\\text{true}}$ 和 $\\varepsilon$ 独立。由此引申，我们也假设 $\\varepsilon$ 与 $X^{\\text{true}}$ 独立。独立性意味着协方差为零。因此，我们有 $\\operatorname{Cov}(X^{\\text{true}}, U) = 0$，$\\operatorname{Cov}(U, \\varepsilon) = 0$ 以及 $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$。\n\n首先，我们推导衰减因子 $\\lambda = \\hat{\\beta}_{\\text{naive}} / \\beta$。\n响应变量 $Y$ 对预测变量 $Z$ 进行回归的总体普通最小二乘法 (OLS) 斜率系数由公式 $\\operatorname{Cov}(Z, Y) / \\operatorname{Var}(Z)$ 给出。在本问题中，从业者将 $Y$ 对观测到的预测变量 $X^{\\text{obs}}$ 进行回归，因此朴素 OLS 斜率为：\n$$\n\\hat{\\beta}_{\\text{naive}} = \\frac{\\operatorname{Cov}(X^{\\text{obs}}, Y)}{\\operatorname{Var}(X^{\\text{obs}})}\n$$\n我们必须计算分子和分母。\n\n分母是观测到的预测变量的方差，$\\operatorname{Var}(X^{\\text{obs}})$。\n使用定义 $X^{\\text{obs}} = X^{\\text{true}} + U$：\n$$\n\\operatorname{Var}(X^{\\text{obs}}) = \\operatorname{Var}(X^{\\text{true}} + U)\n$$\n使用属性 $\\operatorname{Var}(A+B) = \\operatorname{Var}(A) + \\operatorname{Var}(B) + 2\\operatorname{Cov}(A,B)$ 以及 $X^{\\text{true}}$ 和 $U$ 独立的假设（因此 $\\operatorname{Cov}(X^{\\text{true}}, U) = 0$）：\n$$\n\\operatorname{Var}(X^{\\text{obs}}) = \\operatorname{Var}(X^{\\text{true}}) + \\operatorname{Var}(U) = \\sigma_{X}^{2} + \\sigma_{U}^{2}\n$$\n\n分子是观测到的预测变量与响应变量之间的协方差，$\\operatorname{Cov}(X^{\\text{obs}}, Y)$。\n代入 $X^{\\text{obs}}$ 和 $Y$ 的表达式：\n$$\n\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\operatorname{Cov}(X^{\\text{true}} + U, \\alpha + \\beta X^{\\text{true}} + \\varepsilon)\n$$\n使用协方差的双线性属性，并注意到与常数 ($\\alpha$) 的协方差为零：\n$$\n\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\operatorname{Cov}(X^{\\text{true}} + U, \\beta X^{\\text{true}} + \\varepsilon) = \\operatorname{Cov}(X^{\\text{true}}, \\beta X^{\\text{true}}) + \\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) + \\operatorname{Cov}(U, \\beta X^{\\text{true}}) + \\operatorname{Cov}(U, \\varepsilon)\n$$\n我们计算每一项：\n- $\\operatorname{Cov}(X^{\\text{true}}, \\beta X^{\\text{true}}) = \\beta \\operatorname{Cov}(X^{\\text{true}}, X^{\\text{true}}) = \\beta \\operatorname{Var}(X^{\\text{true}}) = \\beta \\sigma_{X}^{2}$\n- $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$ (标准模型假设)\n- $\\operatorname{Cov}(U, \\beta X^{\\text{true}}) = \\beta \\operatorname{Cov}(U, X^{\\text{true}}) = 0$ (因为 $U$ 和 $X^{\\text{true}}$ 独立)\n- $\\operatorname{Cov}(U, \\varepsilon) = 0$ (因为 $U$ 和 $\\varepsilon$ 独立)\n\n将这些项相加得到分子：\n$$\n\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\beta \\sigma_{X}^{2}\n$$\n\n现在，我们可以组合出 $\\hat{\\beta}_{\\text{naive}}$ 的表达式：\n$$\n\\hat{\\beta}_{\\text{naive}} = \\frac{\\beta \\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\n$$\n衰减因子 $\\lambda$ 是这个朴素斜率与真实斜率 $\\beta$ 的比值：\n$$\n\\lambda = \\frac{\\hat{\\beta}_{\\text{naive}}}{\\beta} = \\frac{\\frac{\\beta \\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}{\\beta} = \\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}\n$$\n\n其次，我们推导相关性的乘性偏差比 $R = \\operatorname{Corr}(X^{\\text{obs}}, Y) / \\operatorname{Corr}(X^{\\text{true}}, Y)$。\n相关系数的定义是 $\\operatorname{Corr}(A, B) = \\frac{\\operatorname{Cov}(A, B)}{\\sqrt{\\operatorname{Var}(A)\\operatorname{Var}(B)}}$。\n\n我们首先计算 $\\operatorname{Corr}(X^{\\text{obs}}, Y)$。我们已经求得 $\\operatorname{Cov}(X^{\\text{obs}}, Y) = \\beta \\sigma_{X}^{2}$ 和 $\\operatorname{Var}(X^{\\text{obs}}) = \\sigma_{X}^{2} + \\sigma_{U}^{2}$。我们需要求出 $\\operatorname{Var}(Y)$：\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}(\\alpha + \\beta X^{\\text{true}} + \\varepsilon) = \\operatorname{Var}(\\beta X^{\\text{true}} + \\varepsilon)\n$$\n$$\n\\operatorname{Var(Y)} = \\operatorname{Var}(\\beta X^{\\text{true}}) + \\operatorname{Var}(\\varepsilon) + 2\\operatorname{Cov}(\\beta X^{\\text{true}}, \\varepsilon) = \\beta^2 \\operatorname{Var}(X^{\\text{true}}) + \\sigma_{\\varepsilon}^{2} + 2\\beta \\operatorname{Cov}(X^{\\text{true}}, \\varepsilon)\n$$\n使用假设 $\\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = 0$：\n$$\n\\operatorname{Var}(Y) = \\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2}\n$$\n所以，观测到的预测变量与响应变量之间的相关性是：\n$$\n\\operatorname{Corr}(X^{\\text{obs}}, Y) = \\frac{\\operatorname{Cov}(X^{\\text{obs}}, Y)}{\\sqrt{\\operatorname{Var}(X^{\\text{obs}})\\operatorname{Var}(Y)}} = \\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{(\\sigma_{X}^{2} + \\sigma_{U}^{2})(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}\n$$\n\n接下来，我们计算 $\\operatorname{Corr}(X^{\\text{true}}, Y)$。我们首先需要 $\\operatorname{Cov}(X^{\\text{true}}, Y)$：\n$$\n\\operatorname{Cov}(X^{\\text{true}}, Y) = \\operatorname{Cov}(X^{\\text{true}}, \\alpha + \\beta X^{\\text{true}} + \\varepsilon) = \\operatorname{Cov}(X^{\\text{true}}, \\beta X^{\\text{true}}) + \\operatorname{Cov}(X^{\\text{true}}, \\varepsilon) = \\beta \\sigma_{X}^{2}\n$$\n方差为 $\\operatorname{Var}(X^{\\text{true}}) = \\sigma_{X}^{2}$ 和 $\\operatorname{Var}(Y) = \\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2}$。\n所以，真实的相关性是：\n$$\n\\operatorname{Corr}(X^{\\text{true}}, Y) = \\frac{\\operatorname{Cov}(X^{\\text{true}}, Y)}{\\sqrt{\\operatorname{Var}(X^{\\text{true}})\\operatorname{Var}(Y)}} = \\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{\\sigma_{X}^{2}(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}\n$$\n\n最后，我们计算比率 $R$：\n$$\nR = \\frac{\\operatorname{Corr}(X^{\\text{obs}}, Y)}{\\operatorname{Corr}(X^{\\text{true}}, Y)} = \\frac{\\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{(\\sigma_{X}^{2} + \\sigma_{U}^{2})(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}}{\\frac{\\beta \\sigma_{X}^{2}}{\\sqrt{\\sigma_{X}^{2}(\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2})}}}\n$$\n分子和分母中的项 $\\beta \\sigma_{X}^{2}$ 和 $\\sqrt{\\beta^2 \\sigma_{X}^{2} + \\sigma_{\\varepsilon}^{2}}$相互抵消，剩下：\n$$\nR = \\frac{1/\\sqrt{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}{1/\\sqrt{\\sigma_{X}^{2}}} = \\frac{\\sqrt{\\sigma_{X}^{2}}}{\\sqrt{\\sigma_{X}^{2} + \\sigma_{U}^{2}}} = \\sqrt{\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}\n$$\n这就是相关性中偏差比的简化表达式。注意 $R = \\sqrt{\\lambda}$。\n所需的两个表达式是 $\\lambda = \\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}$ 和 $R = \\sqrt{\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}} & \\sqrt{\\frac{\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "统计数据有时会说谎，尤其是当我们忽视了其背后的潜在结构时。辛普森悖论（Simpson's Paradox）就是这样一个经典的例子：在分组观察时呈现出的一种趋势，在将数据合并后却呈现出完全相反的趋势。这个动手实践将指导你通过编程来构建一个展示辛普森悖论的数据集，让你亲眼见证协方差和相关性是如何在不同层面上给出矛盾信号的 。此外，你还将通过模型选择来学习如何识别并纠正这种由混淆变量引起的误导性结论。",
            "id": "3119190",
            "problem": "您必须编写一个完整的程序，构建在两个实值随机变量的子组特异性关联背景下展示辛普森悖论 (Simpson's paradox) 的合成数据集，然后评估在省略与包含混淆组指示变量两种情况下模型选择的结果。所有计算必须从期望、方差、协方差和相关性的基本定义，以及作为残差平方和最小化器的普通最小二乘估计出发。\n\n定义基础：\n- 对于具有有限二阶矩的实值随机变量 $X$ 和 $Y$，期望为 $E[X]$，方差为 $\\operatorname{Var}(X) = E\\left[(X - E[X])^2\\right]$，协方差为 $\\operatorname{Cov}(X,Y) = E\\left[(X - E[X])(Y - E[Y])\\right]$，皮尔逊相关性 (Pearson correlation) 为 $\\operatorname{Corr}(X,Y) = \\dfrac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$。\n- 对于一个数据集 $\\{(x_i,y_i)\\}_{i=1}^n$，其样本类比通过用样本均值替换期望并使用相应的中心化和来获得。使用除数为 $n-1$ 的无偏样本协方差和方差；相关性是样本协方差与样本标准差乘积的比值。\n- 普通最小二乘 (Ordinary Least Squares, OLS) 拟合通过最小化关于回归参数的 $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ 来实现。残差平方和 (RSS) 为 $\\mathrm{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$。对于模型选择，使用赤池信息准则 (Akaike Information Criterion, AIC)，对于高斯误差，其定义（忽略一个加法常数）为 $\\mathrm{AIC}^\\star = n \\log(\\mathrm{RSS}/n) + 2k$，其中 $k$ 是估计参数的数量，$\\mathrm{AIC}^\\star$ 值较小的模型更优。\n\n数据生成过程：\n- 存在 $2$ 个由 $g \\in \\{0,1\\}$ 索引的子组，样本量分别为 $n_0$ 和 $n_1$。对于每个组 $g$，生成 $X \\mid G=g \\sim \\mathcal{N}(\\mu_{x,g}, \\sigma_x^2)$ 和 $Y \\mid X,G=g$，后者遵循线性模型 $Y = \\alpha_g + \\beta X + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$ 且独立于 $X$。\n- 这种构造在 $X$ 和 $Y$ 之间产生了具有共同斜率 $\\beta$ 但可能具有不同截距 $\\alpha_0$ 和 $\\alpha_1$ 的组内线性关联，从而在 $(X,Y)$ 平面中造成了组间的混淆偏移。通过适当选择 $(\\mu_{x,0},\\mu_{x,1},\\alpha_0,\\alpha_1,\\beta,\\sigma_x,\\sigma_\\varepsilon)$， $X$ 和 $Y$ 之间的边际关联（在合并各组后）可以与组内关联的符号相反，从而展示辛普森悖论。\n\n需对每个数据集执行的计算：\n1. 对每个组 $g \\in \\{0,1\\}$，计算无偏样本协方差 $\\widehat{\\operatorname{Cov}}_g(X,Y)$，并使用 $\\operatorname{sign}(z)$ 函数确定其符号。$\\operatorname{sign}(z)$ 定义为：如果 $z > \\tau$，则为 $+1$；如果 $z  -\\tau$，则为 $-1$；否则为 $0$。容差 $\\tau = 10^{-10}$。\n2. 在合并数据上，计算无偏样本皮尔逊相关性 $\\widehat{r} = \\widehat{\\operatorname{Corr}}(X,Y)$。\n3. 定义一个反转指示器 $R$，如果两个子组的协方差具有相同的非零符号，且 $\\widehat{r}$ 的符号与之相反，则 $R$ 等于 $1$，否则等于 $0$。\n4. 拟合两个 OLS 模型：\n   - 模型 $\\mathcal{M}_0$：$Y$ 仅对截距项和 $X$ 进行回归。\n   - 模型 $\\mathcal{M}_1$：$Y$ 对截距项、$X$ 和二元组指示变量 $G$ 进行回归。\n   对于每个模型，在完整样本上计算 $\\mathrm{RSS}$，然后计算 $\\mathrm{AIC}^\\star = n \\log(\\mathrm{RSS}/n) + 2k$，其中 $\\mathcal{M}_0$ 的 $k=2$，$\\mathcal{M}_1$ 的 $k=3$，$n = n_0 + n_1$。选择 $\\mathrm{AIC}^\\star$ 值较小的模型，并将其索引报告为 $0$（对于 $\\mathcal{M}_0$）或 $1$（对于 $\\mathcal{M}_1$）。\n5. 为每个数据集报告三元组 $[\\widehat{r}_{\\text{rounded}}, R, M]$，其中 $\\widehat{r}_{\\text{rounded}}$ 是 $\\widehat{r}$ 四舍五入到 $4$ 位小数的结果，$R \\in \\{0,1\\}$ 是反转指示器，$M \\in \\{0,1\\}$ 是选定模型的索引。\n\n测试套件：\n使用以下 $5$ 组参数集。每个案例是一个元组 $(\\text{seed}, n_0, n_1, \\mu_{x,0}, \\mu_{x,1}, \\beta, \\alpha_0, \\alpha_1, \\sigma_x, \\sigma_\\varepsilon)$。\n\n- 案例 A（经典的 Simpson 反转，组内正相关，边际负相关）：$(12345, 200, 200, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6)$。\n- 案例 B（组内负相关，边际正相关）：$(54321, 200, 200, 4.0, 1.0, -1.0, 3.0, -3.0, 0.6, 0.6)$。\n- 案例 C（边界情况，设计上边际关联接近零，使用大样本 $n$ 以保证稳定性）：$(102938, 4000, 4000, 1.5, -1.5, 1.0, -1.666666667, 1.666666667, 0.5, 0.5)$。\n- 案例 D（小样本边缘情况，强混淆效应，结果可能存在噪声）：$(777, 5, 5, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6)$。\n- 案例 E（极小的组内方差以强调组间反转）：$(24680, 100, 100, 5.0, 1.0, 1.0, -4.0, 4.0, 0.05, 0.05)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由 5 个结果三元组组成的列表，按 A到E 的顺序排列，不含空格，形式为由方括号括起来的逗号分隔列表。例如，输出应如下所示：\n$[[r_A,R_A,M_A],[r_B,R_B,M_B],[r_C,R_C,M_C],[r_D,R_D,M_D],[r_E,R_E,M_E]]$\n其中每个 $r_\\cdot$ 是一个四舍五入到 4 位小数的浮点数，每个 $R_\\cdot$ 和 $M_\\cdot$ 是 $\\{0,1\\}$ 中的一个整数。",
            "solution": "用户希望解决一个统计问题，该问题涉及模拟辛普森悖论 (Simpson's paradox) 并执行模型选择。\n\n### 第一步：提取已知信息\n- **主题**：统计学习，具体涉及期望、方差、协方差、相关性等概念及其在回归和模型选择中的应用。\n- **任务**：生成展示辛普森悖论的合成数据集，并使用赤池信息准则 (Akaike Information Criterion, AIC) 比较两个回归模型。\n- **核心定义**：\n    - 期望 $E[X]$、方差 $\\operatorname{Var}(X) = E\\left[(X - E[X])^2\\right]$、协方差 $\\operatorname{Cov}(X,Y) = E\\left[(X - E[X])(Y - E[Y])\\right]$ 和皮尔逊相关性 $\\operatorname{Corr}(X,Y) = \\frac{\\operatorname{Cov}(X,Y)}{\\sqrt{\\operatorname{Var}(X)\\operatorname{Var}(Y)}}$。\n    - 样本统计量应使用除数为 $n-1$ 的无偏估计量。\n    - 普通最小二乘法 (OLS) 最小化残差平方和 (RSS)，即 $\\mathrm{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$。\n    - 模型选择准则是 AIC 的一个变体，$\\mathrm{AIC}^\\star = n \\log(\\mathrm{RSS}/n) + 2k$，其中 $n$ 是样本量，$k$ 是参数数量。\n- **数据生成过程**：\n    - 两个子组 $g \\in \\{0,1\\}$，样本量为 $n_0, n_1$。\n    - 预测变量 $X$ 以组为条件生成：$X \\mid G=g \\sim \\mathcal{N}(\\mu_{x,g}, \\sigma_x^2)$。\n    - 响应变量 $Y$ 从一个具有组依赖截距的线性模型生成：$Y = \\alpha_g + \\beta X + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$。\n- **所需计算**：\n    1.  对每个子组 $g \\in \\{0,1\\}$，计算无偏样本协方差 $\\widehat{\\operatorname{Cov}}_g(X,Y)$ 的符号，使用容差 $\\tau = 10^{-10}$。\n    2.  在完整的合并数据集上，计算样本皮尔逊相关性 $\\widehat{r}$。\n    3.  计算反转指示器 $R$。如果子组协方差共享相同的非零符号，且合并相关性的符号与之相反，则 $R=1$，否则为 $0$。\n    4.  对合并数据拟合两个 OLS 模型：$\\mathcal{M}_0: Y \\sim 1+X$（忽略分组）和 $\\mathcal{M}_1: Y \\sim 1+X+G$（包含组指示变量 $G$）。选择 $\\mathrm{AIC}^\\star$ 较低的模型，并报告其索引 $M \\in \\{0,1\\}$。\n    5.  对每个参数集，报告三元组 $[\\widehat{r}_{\\text{rounded}}, R, M]$。\n- **测试用例**：提供了五个用于模拟的参数集 $(\\text{seed}, n_0, n_1, \\mu_{x,0}, \\mu_{x,1}, \\beta, \\alpha_0, \\alpha_1, \\sigma_x, \\sigma_\\varepsilon)$。\n- **输出格式**：单行输出 `[[r_A,R_A,M_A],[r_B,R_B,M_B],...]`，包含五个案例的结果。\n\n### 第二步：使用提取的已知信息进行验证\n- **科学/事实合理性**：该问题在科学上是合理的。这是一个标准的统计模拟练习，基于线性回归、混淆和模型选择（辛普森悖论、OLS、AIC）的公认原理。数据生成过程是创建混淆数据的有效统计模型。\n- **良置性**：该问题是良置的。数据生成过程被完全指定，包括用于可复现性的随机种子。计算任务定义明确。每个测试用例都可以获得唯一的数值结果。\n- **客观性**：问题使用客观、形式化的数学和统计语言陈述。所有定义和程序都精确且不含主观性。\n- **完整性与一致性**：提供了所有必要信息（参数、公式、程序）。问题陈述中没有矛盾。无偏估计量的使用和指定的 AIC 公式与标准统计实践一致。\n- **其他缺陷**：在模拟情境中，该问题并非微不足道、隐喻性或不切实际。它直接针对指定主题，是理解统计混淆的一个有价值的练习。\n\n### 第三步：结论与行动\n该问题是**有效的**。将提供完整解决方案。\n\n### 解决方案\n\n该问题要求我们模拟展示辛普森悖论的数据集。辛普森悖论是一种统计现象，即在数据子组内观察到的趋势在数据聚合时发生反转。然后，我们将使用一个标准的模型选择准则来确定考虑子组结构是否能得到一个更好的统计模型。\n\n**1. 概念框架：辛普森悖论与混淆**\n\n辛普森悖论是由于存在混淆变量而产生的。在这个问题中，组指示变量 $G$ 就是混淆变量。它是一个同时与预测变量 $X$ 和结果变量 $Y$ 相关的变量。数据的生成方式如下：\n- 在每个组 $g$ *内部*，$X$ 和 $Y$ 之间的关系是线性的，具有共同的斜率 $\\beta$：$Y = \\alpha_g + \\beta X + \\varepsilon$。$\\beta$ 的符号决定了真实的、潜在关联的方向。\n- 各组在 $X$ 的分布（通过均值 $\\mu_{x,g}$）和 $Y$ 的基线水平（通过截距 $\\alpha_g$）上有所不同。\n通过仔细选择参数 $(\\mu_{x,0}, \\mu_{x,1})$ 和 $(\\alpha_0, \\alpha_1)$，我们可以创造出一种情况，即连接两个组级数据云中心的趋势与每个云内部的趋势方向相反。当数据被合并且忽略组结构时，这种虚假的组间趋势可能会压倒真实的组内趋势，导致聚合（或边际）相关性的符号与 $\\beta$ 相反。\n\n**2. 数据生成与分析步骤**\n\n对于每个测试用例，我们执行以下操作序列：\n\n**a. 数据模拟：**\n使用给定的参数 $(\\text{seed}, n_0, n_1, \\mu_{x,0}, \\mu_{x,1}, \\beta, \\alpha_0, \\alpha_1, \\sigma_x, \\sigma_\\varepsilon)$，我们为组 $g=0$ 和 $g=1$ 生成两个数据集。\n- 对于组 $g=0$，我们抽取 $n_0$ 个 $X_0 \\sim \\mathcal{N}(\\mu_{x,0}, \\sigma_x^2)$ 的样本。然后，我们计算相应的 $Y_0$ 值：$Y_0 = \\alpha_0 + \\beta X_0 + \\varepsilon_0$，其中 $\\varepsilon_0 \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$。\n- 类似地，对于组 $g=1$，我们抽取 $n_1$ 个 $X_1 \\sim \\mathcal{N}(\\mu_{x,1}, \\sigma_x^2)$ 的样本，并计算 $Y_1 = \\alpha_1 + \\beta X_1 + \\varepsilon_1$，其中 $\\varepsilon_1 \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$。\n\n**b. 子组与合并关联分析：**\n- **组内协方差：** 对于每个组 $g \\in \\{0,1\\}$，我们计算无偏样本协方差，它衡量该组内线性关联的方向和强度：\n  $$ \\widehat{\\operatorname{Cov}}_g(X,Y) = \\frac{1}{n_g-1} \\sum_{i=1}^{n_g} (x_{i,g} - \\bar{x}_g)(y_{i,g} - \\bar{y}_g) $$\n  然后，我们使用指定的数值容差 $\\tau=10^{-10}$ 来确定其符号 $\\operatorname{sign}(\\widehat{\\operatorname{Cov}}_g(X,Y))$。在成功的模拟中，两个符号都应与真实斜率参数 $\\beta$ 的符号匹配。\n\n- **合并相关性：** 我们将两个数据集组合成一个大小为 $n=n_0+n_1$ 的单一合并样本。然后，我们计算皮尔逊样本相关系数 $\\widehat{r}$：\n  $$ \\widehat{r} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}} $$\n  这衡量了忽略组结构时的边际线性关联。\n\n- **反转指示器 ($R$)：** 我们正式检查悖论是否存在。如果观察到悖论，指示器 $R$ 被设置为 $1$，即组内关联一致且非零，而合并关联方向相反。形式上：若 $\\operatorname{sign}(\\widehat{\\operatorname{Cov}}_0) = \\operatorname{sign}(\\widehat{\\operatorname{Cov}}_1) \\neq 0$ 且 $\\operatorname{sign}(\\widehat{r}) = -\\operatorname{sign}(\\widehat{\\operatorname{Cov}}_0)$，则 $R=1$。否则 $R=0$。\n\n**c. 通过 OLS 和 AIC 进行模型比较：**\n我们使用普通最小二乘法 (OLS) 对合并数据拟合两个线性模型。OLS 寻找使残差平方和 (RSS) 最小化的参数估计。\n\n- **模型 $\\mathcal{M}_0$（简单模型）：** $Y_i = b_0 + b_1 X_i + e_i$。此模型将 $Y$ 对 $X$ 和一个截距项进行回归，忽略了混淆变量组 $G$。估计的参数数量为 $k_0 = 2$。\n- **模型 $\\mathcal{M}_1$（混淆变量模型）：** $Y_i = b'_0 + b'_1 X_i + b'_2 G_i + e'_i$。此模型将二元组指示变量 $G$（例如，组0的 $G_i=0$，组1的 $G_i=1$）作为预测变量包含在内。这相当于拟合一个具有共同斜率但各组截距不同的模型，这与我们的数据生成过程相匹配。参数数量为 $k_1 = 3$。\n\n对于每个模型 $\\mathcal{M}_j$，我们计算其 $\\mathrm{RSS}_j$，然后计算其赤池信息准则分数：\n$$ \\mathrm{AIC}^\\star_j = n \\log(\\mathrm{RSS}_j/n) + 2k_j $$\n$\\mathrm{AIC}^\\star$ 在模型拟合度（较低的 $\\mathrm{RSS}$ 更好）和模型复杂度（较低的 $k$ 更好）之间进行权衡。$\\mathrm{AIC}^\\star$ 值较小的模型更优。我们预计，当辛普森悖论存在时，$\\mathcal{M}_1$ 将提供显著更好的拟合（$\\mathrm{RSS}_1$ 会低得多），其优势将超过增加一个额外参数带来的惩罚，因此将被选中。我们报告所选模型的索引 $M$。\n\n最后，对每个测试用例，我们组装所需的三元组：$[\\widehat{r}_{\\text{rounded}}, R, M]$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs synthetic datasets exhibiting Simpson’s paradox, evaluates model selection\n    with and without a confounding variable, and reports the results.\n    \"\"\"\n    test_cases = [\n        # (seed, n0, n1, mu_x0, mu_x1, beta, alpha0, alpha1, sigma_x, sigma_e)\n        (12345, 200, 200, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6),\n        (54321, 200, 200, 4.0, 1.0, -1.0, 3.0, -3.0, 0.6, 0.6),\n        (102938, 4000, 4000, 1.5, -1.5, 1.0, -1.666666667, 1.666666667, 0.5, 0.5),\n        (777, 5, 5, 4.0, 1.0, 1.0, -3.0, 3.0, 0.6, 0.6),\n        (24680, 100, 100, 5.0, 1.0, 1.0, -4.0, 4.0, 0.05, 0.05),\n    ]\n\n    results = []\n\n    def sign_func(z, tau=1e-10):\n        if z > tau:\n            return 1\n        elif z  -tau:\n            return -1\n        else:\n            return 0\n\n    for case in test_cases:\n        seed, n0, n1, mu_x0, mu_x1, beta, alpha0, alpha1, sigma_x, sigma_e = case\n\n        rng = np.random.default_rng(seed)\n\n        # Generate data for both groups\n        X0 = rng.normal(loc=mu_x0, scale=sigma_x, size=n0)\n        Y0 = alpha0 + beta * X0 + rng.normal(loc=0, scale=sigma_e, size=n0)\n\n        X1 = rng.normal(loc=mu_x1, scale=sigma_x, size=n1)\n        Y1 = alpha1 + beta * X1 + rng.normal(loc=0, scale=sigma_e, size=n1)\n\n        # 1. Compute subgroup unbiased sample covariance and its sign\n        cov0 = np.cov(X0, Y0, ddof=1)[0, 1]\n        cov1 = np.cov(X1, Y1, ddof=1)[0, 1]\n        sign_cov0 = sign_func(cov0)\n        sign_cov1 = sign_func(cov1)\n\n        # Pool the data\n        X_pooled = np.concatenate((X0, X1))\n        Y_pooled = np.concatenate((Y0, Y1))\n        n = n0 + n1\n\n        # 2. Compute pooled sample Pearson correlation\n        r_pooled = np.corrcoef(X_pooled, Y_pooled)[0, 1]\n        sign_r_pooled = sign_func(r_pooled)\n\n        # 3. Compute reversal indicator R\n        R = 0\n        if sign_cov0 != 0 and sign_cov0 == sign_cov1 and sign_r_pooled == -sign_cov0:\n            R = 1\n\n        # 4. Fit OLS models and compute AIC\n        # Model M0: Y ~ 1 + X\n        X_mat0 = np.vstack([np.ones(n), X_pooled]).T\n        # Solve (X'X)b = X'y for b, which is more stable than inv(X'X)\n        try:\n            b0 = np.linalg.solve(X_mat0.T @ X_mat0, X_mat0.T @ Y_pooled)\n            Y_hat0 = X_mat0 @ b0\n            RSS0 = np.sum((Y_pooled - Y_hat0)**2)\n            k0 = 2\n            AIC0 = n * np.log(RSS0 / n) + 2 * k0\n        except np.linalg.LinAlgError:\n            AIC0 = np.inf\n\n\n        # Model M1: Y ~ 1 + X + G\n        G_indicator = np.concatenate([np.zeros(n0), np.ones(n1)])\n        X_mat1 = np.vstack([np.ones(n), X_pooled, G_indicator]).T\n        try:\n            b1 = np.linalg.solve(X_mat1.T @ X_mat1, X_mat1.T @ Y_pooled)\n            Y_hat1 = X_mat1 @ b1\n            RSS1 = np.sum((Y_pooled - Y_hat1)**2)\n            k1 = 3\n            AIC1 = n * np.log(RSS1 / n) + 2 * k1\n        except np.linalg.LinAlgError:\n            AIC1 = np.inf\n        \n        # Select model with smaller AIC\n        M = 1 if AIC1  AIC0 else 0\n\n        # 5. Formulate the result triple\n        r_rounded = round(r_pooled, 4)\n        results.append([r_rounded, R, M])\n\n    # Format the final output string as specified\n    output_str = '[' + ','.join([f'[{r},{res_R},{res_M}]' for r, res_R, res_M in results]) + ']'\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在理解了协方差和相关性如何导致统计问题之后，我们现在转向一个更具建设性的主题：如何利用这些概念来构建更强大的预测模型。集成学习（Ensemble Learning）中的自助汇聚法（Bagging）就是一种巧妙利用统计特性的技术，它通过平均多个基学习器的预测来降低整体方差。这个练习将引导你从第一性原理出发，推导自助汇聚法（Bagging）的方差如何依赖于基学习器之间的相关性 。通过这个过程，你将深刻理解为什么“多样性”对集成模型的成功至关重要，以及相关性如何成为限制模型性能提升的瓶颈。",
            "id": "3119186",
            "problem": "考虑一个简化的自助聚合（bagging）分析模型，该模型分离了基学习器之间依赖性的作用。假设有 $B$ 个基估计量，它们的预测被平均以形成套袋估计量。每个基估计量由 $m$ 个噪声贡献构成。将第 $b$ 个基估计量建模为 $X_b = \\theta + \\varepsilon_b$，其中 $\\theta$ 是一个固定常数，且\n$$\n\\varepsilon_b = \\frac{1}{m} \\sum_{j=1}^{m} Z_{b,j},\n$$\n其中 $\\{Z_{b,j}\\}$ 是独立同分布的随机变量，满足 $\\mathbb{E}[Z_{b,j}] = 0$ 和 $\\operatorname{Var}(Z_{b,j}) = \\sigma_Z^2$。为捕捉自助样本间的重叠，假设对于任意一对不同的基估计量 $(b,c)$，有比例为 $s$ 的 $m$ 个噪声项是完全共享的（即，$s m$ 对 $(Z_{b,j}, Z_{c,j})$ 是相同的随机变量），而剩下的 $(1-s)m$ 个项在 $b$ 和 $c$ 之间是独立的。当 $s$ 接近 $1$ 时，这模拟了高重叠情况；当 $s$ 接近 $0$ 时，则模拟了低重叠情况。\n\n你的任务是：\n- 仅使用期望、方差、协方差和相关性的基本定义，推导套袋估计量\n$$\n\\overline{X} = \\frac{1}{B} \\sum_{b=1}^{B} X_b\n$$\n的方差，用 $B$、$m$、$\\sigma_Z^2$ 和 $s$ 表示。\n- 根据你的推导，将单一估计量方差、重叠度为 $s$ 时的套袋估计量方差以及方差比 $\\operatorname{Var}(\\overline{X}) / \\operatorname{Var}(X_b)$，都表示为 $B$、$m$、$\\sigma_Z^2$ 和 $s$ 的函数。\n- 此外，计算基估计量相互独立（即，如同 $s=0$）的特殊情况下的假设套袋估计量方差，以作为比较的朴素基线。\n\n实现一个完整的、可运行的程序，对于下面的每个测试用例，按以下顺序输出一个包含三个浮点数的列表：\n$[$bagged\\_variance, independent\\_bagged\\_variance, variance\\_ratio$]$，其中：\n- bagged\\_variance 是给定 $s$ 时的 $\\operatorname{Var}(\\overline{X})$，\n- independent\\_bagged\\_variance 是 $s=0$ 时的 $\\operatorname{Var}(\\overline{X})$，\n- variance\\_ratio 是给定 $s$ 时的 $\\operatorname{Var}(\\overline{X}) / \\operatorname{Var}(X_b)$。\n\n使用以下测试套件，其中每个元组为 $(B, m, \\sigma_Z^2, s)$：\n- 情况 1：$(B, m, \\sigma_Z^2, s) = (10, 100, 1.0, 0.8)$，一个典型的高重叠场景。\n- 情况 2：$(B, m, \\sigma_Z^2, s) = (10, 100, 1.0, 0.0)$，独立基线。\n- 情况 3：$(B, m, \\sigma_Z^2, s) = (100, 50, 2.0, 0.5)$，具有中等重叠的大型集成。\n- 情况 4：$(B, m, \\sigma_Z^2, s) = (1, 200, 1.0, 0.9)$，单个模型的边界情况。\n- 情况 5：$(B, m, \\sigma_Z^2, s) = (50, 100, 1.0, 0.95)$，极端重叠情况。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个元素对应一个测试用例，其本身也是一个用方括号括起来的逗号分隔列表。例如，整体格式必须是\n$[ [\\text{case}_1], [\\text{case}_2], \\dots ]$\n不含任何额外文本。所有输出必须是十进制形式的实数。此问题不涉及物理单位或角度单位。",
            "solution": "目标是推导套袋估计量 $\\overline{X} = \\frac{1}{B} \\sum_{b=1}^{B} X_b$ 的方差及相关量。推导过程将首先计算单个估计量的方差，然后计算任意两个不同估计量之间的协方差，最后将它们组合起来以求得其平均值的方差。\n\n随机变量之和的方差基本公式为：\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{n} Y_i\\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\operatorname{Cov}(Y_i, Y_j) = \\sum_{i=1}^{n} \\operatorname{Var}(Y_i) + \\sum_{i \\neq j} \\operatorname{Cov}(Y_i, Y_j)\n$$\n将此公式应用于套袋估计量 $\\overline{X}$：\n$$\n\\operatorname{Var}(\\overline{X}) = \\operatorname{Var}\\left(\\frac{1}{B} \\sum_{b=1}^{B} X_b\\right) = \\frac{1}{B^2} \\operatorname{Var}\\left(\\sum_{b=1}^{B} X_b\\right) = \\frac{1}{B^2} \\left( \\sum_{b=1}^{B} \\operatorname{Var}(X_b) + \\sum_{b \\neq c} \\operatorname{Cov}(X_b, X_c) \\right)\n$$\n为了计算该表达式，我们必须计算 $b \\neq c$ 时的 $\\operatorname{Var}(X_b)$ 和 $\\operatorname{Cov}(X_b, X_c)$。\n\n**第一步：单个基估计量的方差，$\\operatorname{Var}(X_b)$**\n\n索引为 $b$ 的基估计量 $X_b$ 由 $X_b = \\theta + \\varepsilon_b$ 给出，其中 $\\theta$ 是一个常数。因此其方差为：\n$$\n\\operatorname{Var}(X_b) = \\operatorname{Var}(\\theta + \\varepsilon_b) = \\operatorname{Var}(\\varepsilon_b)\n$$\n噪声项 $\\varepsilon_b$ 是 $m$ 个独立同分布随机变量 $Z_{b,j}$ 的平均值：\n$$\n\\varepsilon_b = \\frac{1}{m} \\sum_{j=1}^{m} Z_{b,j}\n$$\n已知对于固定的 $b$，变量 $\\{Z_{b,j}\\}_{j=1}^m$ 是独立的，且 $\\operatorname{Var}(Z_{b,j}) = \\sigma_Z^2$：\n$$\n\\operatorname{Var}(\\varepsilon_b) = \\operatorname{Var}\\left(\\frac{1}{m} \\sum_{j=1}^{m} Z_{b,j}\\right) = \\frac{1}{m^2} \\operatorname{Var}\\left(\\sum_{j=1}^{m} Z_{b,j}\\right) = \\frac{1}{m^2} \\sum_{j=1}^{m} \\operatorname{Var}(Z_{b,j})\n$$\n$$\n\\operatorname{Var}(\\varepsilon_b) = \\frac{1}{m^2} \\sum_{j=1}^{m} \\sigma_Z^2 = \\frac{1}{m^2} (m \\sigma_Z^2) = \\frac{\\sigma_Z^2}{m}\n$$\n因此，单个基估计量的方差为：\n$$\n\\operatorname{Var}(X_b) = \\frac{\\sigma_Z^2}{m}\n$$\n\n**第二步：两个不同基估计量之间的协方差，$\\operatorname{Cov}(X_b, X_c)$（$b \\neq c$）**\n\n对于两个不同的估计量 $X_b$ 和 $X_c$（其中 $b \\neq c$），协方差为：\n$$\n\\operatorname{Cov}(X_b, X_c) = \\operatorname{Cov}(\\theta + \\varepsilon_b, \\theta + \\varepsilon_c) = \\operatorname{Cov}(\\varepsilon_b, \\varepsilon_c)\n$$\n$$\n\\operatorname{Cov}(\\varepsilon_b, \\varepsilon_c) = \\operatorname{Cov}\\left(\\frac{1}{m} \\sum_{j=1}^{m} Z_{b,j}, \\frac{1}{m} \\sum_{k=1}^{m} Z_{c,k}\\right) = \\frac{1}{m^2} \\operatorname{Cov}\\left(\\sum_{j=1}^{m} Z_{b,j}, \\sum_{k=1}^{m} Z_{c,k}\\right)\n$$\n根据协方差的双线性性质：\n$$\n\\operatorname{Cov}(\\varepsilon_b, \\varepsilon_c) = \\frac{1}{m^2} \\sum_{j=1}^{m} \\sum_{k=1}^{m} \\operatorname{Cov}(Z_{b,j}, Z_{c,k})\n$$\n问题陈述，比例为 $s$ 的噪声项是共享的。不失一般性，我们假设前 $sm$ 个项是共享的，即 $Z_{b,j} = Z_{c,j}$ 对 $j \\in \\{1, \\dots, sm\\}$ 成立。剩下的 $(1-s)m$ 个项是独立的。仅当 $Z_{b,j}$ 和 $Z_{c,k}$ 是同一个随机变量时，协方差 $\\operatorname{Cov}(Z_{b,j}, Z_{c,k})$ 才非零。这发生在 $j=k$ 且 $j \\in \\{1, \\dots, sm\\}$ 时。在这种情况下：\n$$\n\\operatorname{Cov}(Z_{b,j}, Z_{c,j}) = \\operatorname{Var}(Z_{b,j}) = \\sigma_Z^2\n$$\n对于所有其他对 $(j,k)$，变量是独立的，因此 $\\operatorname{Cov}(Z_{b,j}, Z_{c,k}) = 0$。因此，双重求和简化为对共享索引的单次求和：\n$$\n\\sum_{j=1}^{m} \\sum_{k=1}^{m} \\operatorname{Cov}(Z_{b,j}, Z_{c,k}) = \\sum_{j=1}^{sm} \\operatorname{Cov}(Z_{b,j}, Z_{c,j}) = \\sum_{j=1}^{sm} \\sigma_Z^2 = sm\\sigma_Z^2\n$$\n将此结果代回 $\\operatorname{Cov}(\\varepsilon_b, \\varepsilon_c)$ 的表达式：\n$$\n\\operatorname{Cov}(X_b, X_c) = \\frac{1}{m^2} (sm\\sigma_Z^2) = s \\frac{\\sigma_Z^2}{m}\n$$\n这表明任意两个不同估计量之间的协方差等于重叠比例 $s$ 乘以单个估计量的方差。\n\n**第三步：套袋估计量的方差，$\\operatorname{Var}(\\overline{X})$**\n\n现在我们可以计算 $\\operatorname{Var}(\\overline{X})$ 的完整表达式。该和式有 $B$ 个方差项（$b=c$ 时）和 $B(B-1)$ 个协方差项（$b \\neq c$ 时）。\n$$\n\\operatorname{Var}(\\overline{X}) = \\frac{1}{B^2} \\left[ \\sum_{b=1}^{B} \\operatorname{Var}(X_b) + \\sum_{b \\neq c} \\operatorname{Cov}(X_b, X_c) \\right]\n$$\n$$\n\\operatorname{Var}(\\overline{X}) = \\frac{1}{B^2} \\left[ B \\cdot \\operatorname{Var}(X_b) + B(B-1) \\cdot \\operatorname{Cov}(X_b, X_c) \\right]\n$$\n代入方差和协方差的表达式：\n$$\n\\operatorname{Var}(\\overline{X}) = \\frac{1}{B^2} \\left[ B \\left(\\frac{\\sigma_Z^2}{m}\\right) + B(B-1) \\left(s \\frac{\\sigma_Z^2}{m}\\right) \\right]\n$$\n提取公因式：\n$$\n\\operatorname{Var}(\\overline{X}) = \\frac{B}{B^2} \\frac{\\sigma_Z^2}{m} \\left[ 1 + (B-1)s \\right] = \\frac{1}{B} \\frac{\\sigma_Z^2}{m} \\left[ 1 + Bs - s \\right]\n$$\n重新整理这些项可得到一个更易于解释的形式：\n$$\n\\operatorname{Var}(\\overline{X}) = \\frac{\\sigma_Z^2}{m} \\left[ \\frac{1 - s}{B} + s \\right]\n$$\n这就是套袋估计量方差的最终表达式。\n\n**第四步：所求量的总结**\n\n根据推导，我们现在可以列出所需量的表达式。\n\n1.  **单一估计量方差**：这在第一步中已计算得出。\n    $$\n    \\operatorname{Var}(X_b) = \\frac{\\sigma_Z^2}{m}\n    $$\n\n2.  **套袋估计量方差 (bagged\\_variance)**：这是第三步的主要结果。\n    $$\n    \\operatorname{Var}(\\overline{X}) = \\frac{\\sigma_Z^2}{m} \\left( s + \\frac{1-s}{B} \\right)\n    $$\n\n3.  **假设的独立套袋估计量方差 (independent\\_bagged\\_variance)**：这对应于在 $\\operatorname{Var}(\\overline{X})$ 的公式中设置 $s=0$。\n    $$\n    \\operatorname{Var}(\\overline{X})_{s=0} = \\frac{\\sigma_Z^2}{m} \\left( 0 + \\frac{1-0}{B} \\right) = \\frac{\\sigma_Z^2}{mB}\n    $$\n    这是 $B$ 个独立随机变量均值的方差的经典结果。\n\n4.  **方差比 (variance\\_ratio)**：这是套袋估计量方差与单一估计量方差之比。\n    $$\n    \\frac{\\operatorname{Var}(\\overline{X})}{\\operatorname{Var}(X_b)} = \\frac{\\frac{\\sigma_Z^2}{m} \\left(s + \\frac{1-s}{B}\\right)}{\\frac{\\sigma_Z^2}{m}} = s + \\frac{1-s}{B}\n    $$\n该比率量化了通过 bagging 实现的方差缩减。当 $B \\to \\infty$ 时，该比率接近 $s$。这表明方差缩减的潜力受限于基估计量之间的相关性 $s$。如果 $s > 0$，无论使用多少个估计量 $B$，方差都无法降至零。\n\n这些表达式为程序中的数值计算提供了基础。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# numpy and scipy are not required for this problem's calculations.\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (B, m, sigma_Z^2, s)\n    test_cases = [\n        (10, 100, 1.0, 0.8),    # Case 1\n        (10, 100, 1.0, 0.0),    # Case 2\n        (100, 50, 2.0, 0.5),    # Case 3\n        (1, 200, 1.0, 0.9),     # Case 4\n        (50, 100, 1.0, 0.95),   # Case 5\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Unpack parameters for the current test case.\n        B, m, sigma_Z_sq, s = case\n\n        # Derived formulas from the analytical solution:\n        # bagged_variance = (sigma_Z^2 / m) * (s + (1-s)/B)\n        # independent_bagged_variance = sigma_Z^2 / (m * B)\n        # variance_ratio = s + (1-s)/B\n        \n        # 1. Calculate the bagged variance with the given overlap s.\n        bagged_variance = (sigma_Z_sq / m) * (s + (1.0 - s) / B)\n        \n        # 2. Calculate the hypothetical bagged variance for independent estimators (s=0).\n        independent_bagged_variance = sigma_Z_sq / (m * B)\n        \n        # 3. Calculate the variance reduction ratio.\n        # This is equivalent to bagged_variance / (sigma_Z_sq / m).\n        variance_ratio = s + (1.0 - s) / B\n        \n        case_results = [bagged_variance, independent_bagged_variance, variance_ratio]\n        all_results.append(case_results)\n\n    # Format the final output string according to the specified format.\n    # The required format is a list of lists, e.g., [[r1,r2,r3],[...],...].\n    result_strings = []\n    for res_list in all_results:\n        # Format each list of results as a string \"[n1,n2,n3]\"\n        # Using map(str, ...) ensures floating point numbers are converted correctly.\n        inner_string = f\"[{','.join(map(str, res_list))}]\"\n        result_strings.append(inner_string)\n    \n    # Join the individual case strings with commas and enclose in brackets.\n    final_output_string = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}