{
    "hands_on_practices": [
        {
            "introduction": "In many real-world scenarios, from predicting inventory counts to customer ratings, our model's output must be an integer. This exercise  moves beyond simple regression by applying statistical decision theory to select the best integer prediction. You will implement and compare various strategies, from simple rounding to principled risk minimization, discovering how the choice of a loss function like the squared error ($L_2$) or absolute error ($L_1$) dictates the optimal decision.",
            "id": "3170698",
            "problem": "Consider a statistical learning setting where the response (target) variable $Y$ is restricted to take integer values in a finite support $S = \\{0,1,\\ldots,K\\}$. For each test case, you are given: (i) a true conditional distribution $q$ over $S$ that governs $Y$, (ii) a learned discrete predictive distribution $p$ over $S$ produced by a model, and (iii) a real-valued regression output $\\hat{\\mu}$ from another model that ignores the integer constraint. Your task is to compare decision rules that respect the integer output constraint and evaluate their true expected loss under $q$.\n\nYou must build your program starting from the risk minimization principle: for any loss function $L(a,y)$ and any distribution $r$ over $S$, the expected loss (risk) of predicting $a \\in S$ is\n$$\n\\mathcal{R}_r(a) = \\mathbb{E}_{Y \\sim r}[L(a,Y)] = \\sum_{y \\in S} r(y)\\,L(a,y).\n$$\nA Bayes-optimal decision with respect to $r$ is any $a^\\star \\in \\arg\\min_{a \\in S} \\mathcal{R}_r(a)$. Ties must be resolved by choosing the smallest feasible integer.\n\nYou must implement and evaluate the following five strategies that produce an integer-valued prediction constrained to $S$:\n- Strategy $0$ (nearest rounding): Predict $R_{\\text{near}}(\\hat{\\mu}) = \\left\\lfloor \\hat{\\mu} + 0.5 \\right\\rfloor$ and then clip to $S$ by replacing any value below $0$ by $0$ and any value above $K$ by $K$.\n- Strategy $1$ (floor): Predict $R_{\\lfloor \\cdot \\rfloor}(\\hat{\\mu}) = \\left\\lfloor \\hat{\\mu} \\right\\rfloor$ and clip to $S$.\n- Strategy $2$ (ceiling): Predict $R_{\\lceil \\cdot \\rceil}(\\hat{\\mu}) = \\left\\lceil \\hat{\\mu} \\right\\rceil$ and clip to $S$.\n- Strategy $3$ (stochastic rounding): If $\\hat{\\mu} \\in [0,K]$ and $\\hat{\\mu} \\notin \\mathbb{Z}$, write $\\hat{\\mu} = k + \\alpha$ with $k \\in \\{0,\\ldots,K-1\\}$ and $\\alpha \\in (0,1)$. Predict $k$ with probability $1-\\alpha$ and $k+1$ with probability $\\alpha$. If $\\hat{\\mu} \\in \\mathbb{Z}$, predict that integer. If $\\hat{\\mu} < 0$, predict $0$; if $\\hat{\\mu} > K$, predict $K$. For evaluation, compute the expected risk under $q$ by averaging the risk of each possible rounded value with its rounding probability.\n- Strategy $4$ (learned-distribution decision): Choose $a^\\star \\in \\arg\\min_{a \\in S} \\mathcal{R}_p(a)$, that is, minimize expected loss under the learned distribution $p$, break ties by the smallest $a$, then evaluate its true expected loss under $q$.\n\nEvaluate each strategy under three losses:\n- Squared loss $L_2(a,y) = (a - y)^2$.\n- Absolute loss $L_1(a,y) = |a - y|$.\n- Asymmetric absolute loss (pinball loss) with parameter $\\tau \\in (0,1)$:\n$$\nL_\\tau(a,y) = \\tau \\max(a - y, 0) + (1 - \\tau) \\max(y - a, 0).\n$$\nUse $\\tau = 0.3$ for all test cases.\n\nFor each test case and each loss, compute the true expected loss under $q$ for all strategies $0$ through $4$, and return the index of the strategy that achieves the minimal true expected loss (breaking ties by the smallest index). The final output for each test case must be a list of three integers $[i_{2}, i_{1}, i_{\\tau}]$, where $i_{2}$ is the index minimizing the squared loss, $i_{1}$ is the index minimizing the absolute loss, and $i_{\\tau}$ is the index minimizing the asymmetric loss with $\\tau = 0.3$.\n\nYour program must use the following fixed test suite. In all cases, the support is $S = \\{0,1,2,3,4\\}$, so $K = 4$.\n- Test case $1$: $q = [0.05, 0.15, 0.50, 0.20, 0.10]$, $p = [0.10, 0.20, 0.40, 0.20, 0.10]$, $\\hat{\\mu} = 2.2$.\n- Test case $2$: $q = [0.00, 0.50, 0.50, 0.00, 0.00]$, $p = [0.05, 0.45, 0.45, 0.05, 0.00]$, $\\hat{\\mu} = 1.5$.\n- Test case $3$: $q = [0.05, 0.05, 0.10, 0.20, 0.60]$, $p = [0.20, 0.20, 0.20, 0.20, 0.20]$, $\\hat{\\mu} = 3.8$.\n- Test case $4$: $q = [0.45, 0.05, 0.00, 0.05, 0.45]$, $p = [0.05, 0.10, 0.70, 0.10, 0.05]$, $\\hat{\\mu} = 2.0$.\n- Test case $5$: $q = [0.70, 0.20, 0.05, 0.03, 0.02]$, $p = [0.65, 0.20, 0.10, 0.03, 0.02]$, $\\hat{\\mu} = -0.3$.\n- Test case $6$: $q = [0.02, 0.03, 0.05, 0.20, 0.70]$, $p = [0.05, 0.10, 0.15, 0.20, 0.50]$, $\\hat{\\mu} = 5.1$.\n\nAngle or physical units are not involved. All probabilities are given as decimals and sum to $1$ for each distribution.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The line must be a Python-style list of six sublists, each sublist corresponding to one test case in the same order as above, and each sublist containing the three integers $[i_{2}, i_{1}, i_{\\tau}]$. For example, a valid output would look like $[[0,1,4],[\\ldots],\\ldots]$ but with the actual computed integers for this problem.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in statistical decision theory, self-contained, well-posed, objective, and its components are formally and unambiguously defined. All necessary data and constraints are provided.\n\nThe core of the problem is to evaluate five distinct strategies for producing an integer prediction for a response variable $Y$ with a finite integer support $S = \\{0, 1, \\ldots, K\\}$. The evaluation is based on the principle of risk minimization. The risk, or expected loss, of a prediction (action) $a$ with respect to a probability distribution $r$ over $S$ is given by:\n$$\n\\mathcal{R}_r(a) = \\mathbb{E}_{Y \\sim r}[L(a,Y)] = \\sum_{y=0}^{K} r(y)\\,L(a,y)\n$$\nwhere $L(a,y)$ is a specified loss function. We will compute the true expected loss, meaning the expectation is taken with respect to the true data-generating distribution $q$.\n\nThe support for all test cases is $S = \\{0, 1, 2, 3, 4\\}$, corresponding to $K=4$. We will evaluate the strategies under three loss functions:\n1.  **Squared Loss ($L_2$)**: $L_2(a,y) = (a - y)^2$. The Bayes-optimal predictor for this loss is the expected value of the distribution, $\\mathbb{E}[Y]$.\n2.  **Absolute Loss ($L_1$)**: $L_1(a,y) = |a - y|$. The Bayes-optimal predictor for this loss is the median of the distribution.\n3.  **Asymmetric Absolute Loss ($L_\\tau$)**: $L_\\tau(a,y) = \\tau \\max(a - y, 0) + (1 - \\tau) \\max(y - a, 0)$, with parameter $\\tau = 0.3$. The Bayes-optimal predictor for this loss is the $\\tau$-quantile of the distribution.\n\nFor each test case, we are given a true distribution $q$, a learned distribution $p$, and a real-valued regression output $\\hat{\\mu}$. We will calculate the prediction for each of the five strategies and then compute its true risk $\\mathcal{R}_q(\\cdot)$ under each of the three loss functions.\n\nThe five strategies are as follows:\n\n**Strategy $0$: Nearest Rounding**\nThe prediction $a_0$ is obtained by rounding $\\hat{\\mu}$ to the nearest integer, then clipping the result to the valid range $[0, K]$.\n$$\na_0 = \\text{clip}(\\lfloor \\hat{\\mu} + 0.5 \\rfloor, 0, K)\n$$\nThis is a deterministic strategy, yielding a single integer prediction.\n\n**Strategy $1$: Floor**\nThe prediction $a_1$ is obtained by taking the floor of $\\hat{\\mu}$, then clipping.\n$$\na_1 = \\text{clip}(\\lfloor \\hat{\\mu} \\rfloor, 0, K)\n$$\nThis is also a deterministic strategy.\n\n**Strategy $2$: Ceiling**\nThe prediction $a_2$ is obtained by taking the ceiling of $\\hat{\\mu}$, then clipping.\n$$\na_2 = \\text{clip}(\\lceil \\hat{\\mu} \\rceil, 0, K)\n$$\nThis is also a deterministic strategy.\n\n**Strategy $3$: Stochastic Rounding**\nThis strategy's prediction can be probabilistic.\n- If $\\hat{\\mu} < 0$, the prediction is $0$.\n- If $\\hat{\\mu} > K$, the prediction is $K$.\n- If $\\hat{\\mu} \\in [0, K]$ is an integer, the prediction is $\\hat{\\mu}$.\n- If $\\hat{\\mu} \\in (0, K)$ is not an integer, let $\\hat{\\mu} = k + \\alpha$ where $k = \\lfloor\\hat{\\mu}\\rfloor$ is an integer and $\\alpha \\in (0,1)$ is the fractional part. The strategy predicts $k$ with probability $1-\\alpha$ and $k+1$ with probability $\\alpha$.\n\nFor deterministic outcomes of this strategy, the true risk is $\\mathcal{R}_q(a_3)$. For the stochastic case, the evaluated quantity is the expected true risk, averaged over the rounding probabilities:\n$$\n\\mathbb{E}[\\mathcal{R}_q(a_3)] = (1-\\alpha) \\mathcal{R}_q(k) + \\alpha \\mathcal{R}_q(k+1)\n$$\n\n**Strategy $4$: Learned-Distribution Decision**\nThis strategy uses the learned distribution $p$ to find a Bayes-optimal prediction. The prediction $a_4$ is chosen to minimize the expected loss under $p$:\n$$\na_4 = \\arg\\min_{a \\in S} \\mathcal{R}_p(a) = \\arg\\min_{a \\in S} \\sum_{y=0}^{K} p(y) L(a,y)\n$$\nTies are broken by choosing the smallest integer $a$. It is crucial to note that the prediction $a_4$ depends on the loss function being considered. Thus, we will derive a different $a_4$ for each of $L_2$, $L_1$, and $L_\\tau$. Once $a_4$ is determined for a given loss, its performance is evaluated by computing the true risk $\\mathcal{R}_q(a_4)$.\n\n**Evaluation**\nFor each loss function ($L_2$, $L_1$, $L_\\tau$), we will produce a vector of five risk values, one for each strategy.\n$$\n[\\mathcal{R}_q(a_0), \\mathcal{R}_q(a_1), \\mathcal{R}_q(a_2), \\mathbb{E}[\\mathcal{R}_q(a_3)], \\mathcal{R}_q(a_4)]\n$$\nWe then find the index of the minimum value in this vector. In case of a tie, the smallest index wins. For each test case, this process yields three indices: $i_2$ for squared loss, $i_1$ for absolute loss, and $i_\\tau$ for asymmetric loss. The final output is a list of these three indices $[i_2, i_1, i_\\tau]$. This procedure is repeated for all provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the statistical learning problem by evaluating five decision strategies\n    under three different loss functions for a series of test cases.\n    \"\"\"\n    \n    # Problem Constants\n    K = 4\n    S_arr = np.arange(K + 1)\n    TAU = 0.3\n\n    # Test cases from the problem statement\n    test_cases = [\n        # (q, p, mu_hat)\n        (np.array([0.05, 0.15, 0.50, 0.20, 0.10]), np.array([0.10, 0.20, 0.40, 0.20, 0.10]), 2.2),\n        (np.array([0.00, 0.50, 0.50, 0.00, 0.00]), np.array([0.05, 0.45, 0.45, 0.05, 0.00]), 1.5),\n        (np.array([0.05, 0.05, 0.10, 0.20, 0.60]), np.array([0.20, 0.20, 0.20, 0.20, 0.20]), 3.8),\n        (np.array([0.45, 0.05, 0.00, 0.05, 0.45]), np.array([0.05, 0.10, 0.70, 0.10, 0.05]), 2.0),\n        (np.array([0.70, 0.20, 0.05, 0.03, 0.02]), np.array([0.65, 0.20, 0.10, 0.03, 0.02]), -0.3),\n        (np.array([0.02, 0.03, 0.05, 0.20, 0.70]), np.array([0.05, 0.10, 0.15, 0.20, 0.50]), 5.1),\n    ]\n\n    # --- Loss Functions ---\n    def loss_l2(a, y):\n        return (a - y) ** 2\n\n    def loss_l1(a, y):\n        return np.abs(a - y)\n\n    def loss_ltau(a, y, tau):\n        return tau * np.maximum(a - y, 0) + (1 - tau) * np.maximum(y - a, 0)\n\n    loss_functions = [loss_l2, loss_l1, loss_ltau]\n\n    def calculate_risk(a, dist, loss_func, S):\n        \"\"\"Calculates the expected loss (risk) for a given action and distribution.\"\"\"\n        if loss_func == loss_ltau:\n            losses = loss_func(a, S, TAU)\n        else:\n            losses = loss_func(a, S)\n        return np.sum(dist * losses)\n\n    all_results = []\n    for q, p, mu_hat in test_cases:\n        \n        # --- Generate Predictions for Strategies 0, 1, 2 ---\n        a0 = int(np.clip(np.floor(mu_hat + 0.5), 0, K))\n        a1 = int(np.clip(np.floor(mu_hat), 0, K))\n        a2 = int(np.clip(np.ceil(mu_hat), 0, K))\n\n        case_best_indices = []\n        for loss_func in loss_functions:\n            risks_for_loss = []\n\n            # --- Evaluate Strategies 0, 1, 2 ---\n            risks_for_loss.append(calculate_risk(a0, q, loss_func, S_arr))\n            risks_for_loss.append(calculate_risk(a1, q, loss_func, S_arr))\n            risks_for_loss.append(calculate_risk(a2, q, loss_func, S_arr))\n\n            # --- Evaluate Strategy 3 (Stochastic Rounding) ---\n            risk_s3 = 0.0\n            if mu_hat < 0:\n                risk_s3 = calculate_risk(0, q, loss_func, S_arr)\n            elif mu_hat > K:\n                risk_s3 = calculate_risk(K, q, loss_func, S_arr)\n            elif mu_hat == int(mu_hat):\n                risk_s3 = calculate_risk(int(mu_hat), q, loss_func, S_arr)\n            else: # Stochastic case\n                k = int(np.floor(mu_hat))\n                alpha = mu_hat - k\n                risk_k = calculate_risk(k, q, loss_func, S_arr)\n                risk_k1 = calculate_risk(k + 1, q, loss_func, S_arr)\n                risk_s3 = (1 - alpha) * risk_k + alpha * risk_k1\n            risks_for_loss.append(risk_s3)\n\n            # --- Evaluate Strategy 4 (Learned-Distribution Decision) ---\n            # 1. Find the Bayes-optimal action 'a4' under the learned distribution 'p'\n            risks_under_p = [calculate_risk(a_candidate, p, loss_func, S_arr) for a_candidate in S_arr]\n            a4 = np.argmin(risks_under_p)\n            \n            # 2. Evaluate the true risk of 'a4' under the true distribution 'q'\n            risk_s4 = calculate_risk(a4, q, loss_func, S_arr)\n            risks_for_loss.append(risk_s4)\n\n            # --- Determine the best strategy for this loss function ---\n            best_strategy_index = np.argmin(risks_for_loss)\n            case_best_indices.append(best_strategy_index)\n        \n        all_results.append(case_best_indices)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A powerful way to improve a model's generalization is to bake in prior knowledge about the true target function, such as its smoothness. This practice  explores how to enforce a Lipschitz constraint—a formal measure of smoothness—directly on a model's outputs. By solving a constrained optimization problem, you will see firsthand how this structural prior can lead to models that are less prone to overfitting and more faithful to the underlying process, especially when data is noisy or sparse.",
            "id": "3170644",
            "problem": "You are given a one-dimensional supervised learning setup in which a scalar response $y$ is generated from a deterministic target function $g : [0,1] \\to \\mathbb{R}$ plus independent noise. We study the effect of enforcing a Lipschitz constraint on the outputs of the learned model when the target has a bounded derivative on $[0,1]$. You must implement and compare two models: a Lipschitz-constrained regression and an unconstrained polynomial least-squares baseline. Your program must be fully deterministic and produce a single-line output as specified at the end.\n\nDefinitions and assumptions:\n- The response variable is $y \\in \\mathbb{R}$, observed at input $x \\in [0,1]$. The target function is $g(x)$, and the model output is $\\hat{f}(x)$.\n- A function $h$ is $L$-Lipschitz on $[0,1]$ if for all $x,x' \\in [0,1]$, $|h(x)-h(x')| \\le L |x-x'|$. By the Mean Value Theorem, if $g$ is differentiable on $[0,1]$ with $|g'(x)| \\le L_\\star$ for all $x$, then $g$ is $L_\\star$-Lipschitz.\n- Data are generated as $y_i = g(x_i) + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ independently, where all trigonometric function arguments are in radians.\n\nMethodology to implement:\n1) Lipschitz-constrained empirical risk minimization on the training points: sort the training inputs $x_{(1)} \\le \\cdots \\le x_{(n)}$. Let $\\mathbf{f} = (f_1,\\ldots,f_n)$ denote model outputs at the sorted training inputs. Solve the convex optimization\n$$\n\\min_{\\mathbf{f} \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2} \\sum_{i=1}^n (f_i - y_{(i)})^2\n\\quad \\text{subject to} \\quad\n|f_{i+1} - f_i| \\le L \\,(x_{(i+1)} - x_{(i)}) \\;\\; \\text{for all} \\;\\; i \\in \\{1,\\dots,n-1\\}.\n$$\nThe adjacent constraints suffice in one dimension because the triangle inequality yields all pairwise bounds. This problem has a unique solution because the objective is strictly convex and the feasible set is a nonempty closed convex polyhedron.\n\n2) Lipschitz extension to test inputs using the McShane construction: given the fitted values $\\mathbf{f}$ at the training inputs and the same Lipschitz constant $L$, define for any $x \\in [0,1]$\n$$\n\\hat{f}(x) \\;=\\; \\min_{1 \\le i \\le n} \\left\\{ f_i + L \\, |x - x_{(i)}| \\right\\}.\n$$\nThis provides an $L$-Lipschitz function on $[0,1]$ that agrees with $f_i$ at $x_{(i)}$.\n\n3) Baseline model: fit an unconstrained polynomial by ordinary least squares with degree $d$ using the features $\\{1,x,x^2,\\dots,x^d\\}$ on the training data, and evaluate on test inputs.\n\n4) Evaluation: on a uniform test grid of size $m$ over $[0,1]$, compute the mean squared error (MSE) of each model with respect to the noise-free target $g$,\n$$\n\\mathrm{MSE}(\\hat{f}) \\;=\\; \\frac{1}{m} \\sum_{j=1}^m \\left(\\hat{f}(x^{\\mathrm{test}}_j) - g(x^{\\mathrm{test}}_j)\\right)^2.\n$$\nFor each test case below, report the signed improvement\n$$\n\\Delta \\;=\\; \\mathrm{MSE}(\\text{poly}) \\;-\\; \\mathrm{MSE}(\\text{Lipschitz}),\n$$\nso that a positive $\\Delta$ indicates that the Lipschitz-constrained model generalizes better than the polynomial baseline.\n\nTarget functions and their derivative bounds:\n- $g_{\\sin}(x) = \\tfrac{1}{2} \\sin(2\\pi x)$, with $|g'_{\\sin}(x)| \\le \\pi$ for all $x$, so $L_\\star = \\pi$.\n- $g_{\\mathrm{quad}}(x) = x^2$, with $|g'_{\\mathrm{quad}}(x)| \\le 2$ for all $x \\in [0,1]$, so $L_\\star = 2$.\n- $g_{\\tanh}(x) = \\tfrac{1}{2} + 0.3 \\tanh(4(x - \\tfrac{1}{2}))$, whose derivative is $g'_{\\tanh}(x) = 1.2 \\,\\mathrm{sech}^2(4(x - \\tfrac{1}{2}))$ and is bounded by $1.2$, so $L_\\star = 1.2$.\n\nImplementation details required for determinism:\n- Use a uniform test grid with $m = 200$ points in $[0,1]$ (including endpoints).\n- For each test case, generate $n$ training inputs by sampling independently and uniformly from $[0,1]$ and then sorting them in nondecreasing order. Use an independent fixed seed for each case to initialize a pseudorandom number generator before sampling. Use the same generator instance to sample independent Gaussian noise with standard deviation $\\sigma$.\n- Use ordinary least squares for the polynomial baseline of degree $d$ (no regularization).\n- Use the above quadratic program with adjacent constraints for the Lipschitz fit, and the McShane extension for predictions.\n- All trigonometric arguments are in radians.\n\nTest suite:\nFor each case, you are given $(\\text{target id}, n, \\sigma, L, d, \\text{seed})$.\n- Case $1$: $(\\text{sin}, 12, 0.05, 3.2, 5, 1)$\n- Case $2$: $(\\text{quad}, 8, 0.15, 2.0, 5, 2)$\n- Case $3$: $(\\text{tanh}, 25, 0.05, 1.2, 5, 3)$\n- Case $4$: $(\\text{tanh}, 60, 0.01, 1.2, 5, 4)$\n- Case $5$: $(\\text{sin}, 30, 0.02, 2.5, 5, 5)$\n\nRequired final output format:\nYour program must output exactly one line containing a single Python-style list of $5$ floats corresponding to the cases in order, namely $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$, with each float rounded to exactly six decimal places (round half to even is acceptable).\n\nAngle unit specification:\nAll angles used in the trigonometric functions (e.g., the argument to $\\sin(\\cdot)$) are in radians.\n\nNo physical units are involved.\n\nThe program must be a complete, runnable script that takes no input and uses no external files or network access.",
            "solution": "The user requests the implementation and comparison of two regression models: a Lipschitz-constrained model and an unconstrained polynomial-basis model. The problem is a well-defined numerical experiment in statistical learning.\n\n### Step 1: Problem Validation\n\n**Extracted Givens:**\n- **Data Model:** $y_i = g(x_i) + \\epsilon_i$ where $x_i \\in [0,1]$, $g$ is a deterministic target function, and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are independent noise terms.\n- **Test Grid:** Uniform grid of $m=200$ points in $[0,1]$.\n- **Target Functions:**\n    1.  $g_{\\sin}(x) = \\tfrac{1}{2} \\sin(2\\pi x)$, with true Lipschitz constant $L_\\star = \\pi$.\n    2.  $g_{\\mathrm{quad}}(x) = x^2$, with $L_\\star = 2$ on $[0,1]$.\n    3.  $g_{\\tanh}(x) = \\tfrac{1}{2} + 0.3 \\tanh(4(x - \\tfrac{1}{2}))$, with $L_\\star = 1.2$.\n- **Lipschitz Model:**\n    - Solves the Quadratic Program (QP): $\\min_{\\mathbf{f}} \\frac{1}{2} \\sum_{i=1}^n (f_i - y_{(i)})^2$ subject to $|f_{i+1} - f_i| \\le L (x_{(i+1)} - x_{(i)})$ for $i=1,\\dots,n-1$. Here, $x_{(i)}$ are the sorted training inputs and $y_{(i)}$ are the corresponding observations.\n    - Prediction is done via McShane extension: $\\hat{f}(x) = \\min_{i} \\{ f_i + L |x - x_{(i)}| \\}$.\n- **Polynomial Model:**\n    - Ordinary Least Squares regression on features $\\{1, x, \\dots, x^d\\}$.\n- **Evaluation Metric:** $\\Delta = \\mathrm{MSE}(\\text{poly}) - \\mathrm{MSE}(\\text{Lipschitz})$, where $\\mathrm{MSE}(\\hat{f}) = \\frac{1}{m} \\sum_{j=1}^m (\\hat{f}(x^{\\mathrm{test}}_j) - g(x^{\\mathrm{test}}_j))^2$.\n- **Test Cases:**\n    - Case 1: $(\\text{target id}=\\text{'sin'}, n=12, \\sigma=0.05, L=3.2, d=5, \\text{seed}=1)$\n    - Case 2: $(\\text{target id}=\\text{'quad'}, n=8, \\sigma=0.15, L=2.0, d=5, \\text{seed}=2)$\n    - Case 3: $(\\text{target id}=\\text{'tanh'}, n=25, \\sigma=0.05, L=1.2, d=5, \\text{seed}=3)$\n    - Case 4: $(\\text{target id}=\\text{'tanh'}, n=60, \\sigma=0.01, L=1.2, d=5, \\text{seed}=4)$\n    - Case 5: $(\\text{target id}=\\text{'sin'}, n=30, \\sigma=0.02, L=2.5, d=5, \\text{seed}=5)$\n- **Determinism:** Random number generation is seeded for each case. All trigonometric arguments are in radians.\n\n**Validation Verdict:**\nThe problem is **valid**. It is scientifically grounded in standard statistical learning theory, well-posed, objective, and self-contained. The specified methodologies are standard (or clearly defined), and all parameters are provided for a deterministic, reproducible computation. The problem requires the implementation of a convex optimization problem (a QP), a standard linear regression, and a specific prediction and evaluation protocol, which is a non-trivial but feasible task.\n\n### Step 2: Solution Design\n\nThe solution will be a Python script that iterates through the five specified test cases. For each case, it will perform the following steps:\n\n1.  **Data Generation:** A pseudorandom number generator is seeded with the case-specific seed. $n$ training points $x_i$ are sampled from a uniform distribution on $[0,1]$ and sorted. The corresponding noisy observations $y_i$ are generated using the specified target function $g(x)$ and noise standard deviation $\\sigma$. A uniform test grid of $m=200$ points is also created.\n\n2.  **Lipschitz-Constrained Regression:**\n    a.  **Optimization:** A quadratic program is solved to find the optimal function values $\\mathbf{f} = (f_1, \\dots, f_n)$ at the training points $x_{(i)}$. The objective function is $\\frac{1}{2} \\|\\mathbf{f} - \\mathbf{y}\\|^2_2$, and the constraints are linear: $|f_{i+1} - f_i| \\le L(x_{(i+1)} - x_{(i)})$. This is implemented using `scipy.optimize.minimize` with the `SLSQP` method. Analytical gradients (Jacobians) are provided for both the objective function and the constraints for efficiency and accuracy.\n    b.  **Prediction:** Predictions on the test grid are made using the McShane extension formula, $\\hat{f}(x) = \\min_{i} \\{f_i + L|x - x_{(i)}|\\}$. This is implemented using vectorized NumPy operations.\n\n3.  **Polynomial Regression Baseline:**\n    a.  **Fitting:** An ordinary least squares (OLS) regression is performed to fit a polynomial of degree $d$. A Vandermonde matrix is constructed from the training inputs $x_{(i)}$, and the polynomial coefficients are found using `numpy.linalg.lstsq`.\n    b.  **Prediction:** The fitted polynomial is evaluated on the test grid points to get predictions.\n\n4.  **Evaluation:**\n    For both models, the Mean Squared Error (MSE) is calculated with respect to the true, noise-free target function values on the test grid. The final score for the case, $\\Delta$, is the difference between the polynomial model's MSE and the Lipschitz model's MSE.\n\n5.  **Output:** The calculated $\\Delta$ values for the five cases are collected, formatted to six decimal places, and printed as a single-line list.\n\nThis design strictly adheres to the problem specification, ensuring deterministic and correct results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef solve():\n    \"\"\"\n    Implements and compares Lipschitz-constrained regression against polynomial OLS regression\n    based on the problem specification.\n    \"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        # (target_id, n, sigma, L, d, seed)\n        ('sin', 12, 0.05, 3.2, 5, 1),\n        ('quad', 8, 0.15, 2.0, 5, 2),\n        ('tanh', 25, 0.05, 1.2, 5, 3),\n        ('tanh', 60, 0.01, 1.2, 5, 4),\n        ('sin', 30, 0.02, 2.5, 5, 5),\n    ]\n\n    # Target functions as defined in the problem. Arguments are in radians for trig functions.\n    targets = {\n        'sin': lambda x: 0.5 * np.sin(2 * np.pi * x),\n        'quad': lambda x: x**2,\n        'tanh': lambda x: 0.5 + 0.3 * np.tanh(4 * (x - 0.5))\n    }\n\n    m = 200\n    x_test = np.linspace(0, 1, m)\n    results = []\n\n    for target_id, n, sigma, L, d, seed in test_cases:\n        target_func = targets[target_id]\n\n        # 1. Generate Data\n        # Initialize RNG with the specified seed for determinism.\n        rng = np.random.default_rng(seed)\n        \n        # Sample training inputs from U[0,1] and sort them.\n        x_train = rng.uniform(0, 1, size=n)\n        x_train.sort()\n        \n        # Generate noisy observations corresponding to the sorted inputs.\n        g_train = target_func(x_train)\n        noise = rng.normal(0, sigma, size=n)\n        y_train = g_train + noise\n\n        # Compute noise-free target values on the test grid.\n        g_test = target_func(x_test)\n\n        # 2. Lipschitz-constrained Model\n        # 2.1. Solve the Quadratic Program\n        # The objective is to minimize 0.5 * ||f - y_train||^2.\n        f_obj = lambda f: 0.5 * np.sum((f - y_train)**2)\n        f_jac = lambda f: f - y_train\n\n        # The constraints are |f_{i+1} - f_i| <= L * (x_{i+1} - x_i), which are\n        # linear inequalities. We provide them to the solver in the form g(x) >= 0.\n        # This is equivalent to:\n        # L*(x_{i+1}-x_i) - (f_{i+1}-f_i) >= 0\n        # L*(x_{i+1}-x_i) + (f_{i+1}-f_i) >= 0\n        \n        # Construct the (n-1)x(n) finite difference matrix D for f[1:]-f[:-1].\n        D_matrix = np.zeros((n - 1, n))\n        rows = np.arange(n - 1)\n        D_matrix[rows, rows] = -1\n        D_matrix[rows, rows + 1] = 1\n\n        diff_x = x_train[1:] - x_train[:-1]\n        \n        # The constraint function returns a vector of all 2*(n-1) constraint values.\n        cons_fun = lambda f: np.concatenate([\n            L * diff_x - (D_matrix @ f),\n            L * diff_x + (D_matrix @ f)\n        ])\n        \n        # The Jacobian of the constraint function is a constant matrix.\n        cons_jac_mat = np.vstack([-D_matrix, D_matrix])\n        cons_jac = lambda f: cons_jac_mat\n        \n        constraints = [{'type': 'ineq', 'fun': cons_fun, 'jac': cons_jac}]\n\n        # Use the noisy observations as the initial guess for the optimizer.\n        f_initial = y_train.copy()\n        \n        opt_res = optimize.minimize(\n            f_obj, f_initial, method='SLSQP', jac=f_jac, constraints=constraints)\n        \n        f_hat_train = opt_res.x\n\n        # 2.2. Predict on test set using McShane extension\n        # f_hat(x) = min_i {f_i + L*|x - x_i|}\n        y_pred_lip = np.min(f_hat_train + L * np.abs(x_test[:, np.newaxis] - x_train), axis=1)\n\n        # 3. Polynomial Baseline Model (Ordinary Least Squares)\n        # Create Vandermonde matrix for features {1, x, ..., x^d}.\n        X_train = np.vander(x_train, d + 1, increasing=True)\n        # Solve for polynomial coefficients.\n        coeffs, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n        \n        # Predict on the test grid.\n        X_test = np.vander(x_test, d + 1, increasing=True)\n        y_pred_poly = X_test @ coeffs\n\n        # 4. Evaluation\n        # Compute MSE for both models with respect to the noise-free target.\n        mse_lip = np.mean((y_pred_lip - g_test)**2)\n        mse_poly = np.mean((y_pred_poly - g_test)**2)\n        \n        # Calculate the signed improvement.\n        delta = mse_poly - mse_lip\n        results.append(delta)\n\n    # 5. Format and print output\n    # Format each result to exactly six decimal places as a string.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    # Print the list in the specified format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A model's prediction is only useful if it's reliable, especially when faced with minor, potentially malicious, input changes. This exercise  delves into the critical field of adversarial robustness, distinguishing between a model's output and the ground truth. You will learn to compute a 'certified radius' for a linear classifier, providing a mathematical guarantee that the model's output remains unchanged for any input perturbation within a specific $\\ell_2$-norm ball.",
            "id": "3170624",
            "problem": "A binary linear classifier makes a prediction (model output) from an input vector, while a separate oracle defines the true target. An adversary perturbs the input within a bounded Euclidean radius without changing the true target. Your job is to formalize and compute whether the classifier’s output is certifiably robust to such perturbations using certified margins, and to detect when adversarial examples can shift the model output while the true target remains unchanged.\n\nFundamental base and definitions to use:\n- The input is a vector $\\mathbf{x} \\in \\mathbb{R}^d$.\n- The classifier is linear with parameters $\\mathbf{w} \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$, and outputs the sign $\\hat{y} = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\operatorname{sign}(t) = +1$ if $t > 0$ and $-1$ if $t < 0$.\n- The true target is given by a different oracle vector $\\mathbf{v} \\in \\mathbb{R}^d$ via $y^\\star = \\operatorname{sign}(\\mathbf{v}^\\top \\mathbf{x})$.\n- An adversary may add a perturbation $\\boldsymbol{\\delta}$ satisfying the Euclidean norm bound $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$, with $\\varepsilon \\ge 0$.\n- Use only the following well-tested facts as a starting point:\n  1. The Euclidean norm is $\\lVert \\mathbf{a} \\rVert_2 = \\sqrt{\\sum_i a_i^2}$.\n  2. The distance from a point $\\mathbf{x}$ to the hyperplane $\\{\\mathbf{z} : \\mathbf{a}^\\top \\mathbf{z} + c = 0\\}$ in Euclidean space equals $\\dfrac{\\lvert \\mathbf{a}^\\top \\mathbf{x} + c \\rvert}{\\lVert \\mathbf{a} \\rVert_2}$.\n  3. The Cauchy–Schwarz inequality: $\\lvert \\mathbf{u}^\\top \\mathbf{z} \\rvert \\le \\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{z} \\rVert_2$.\n\nTasks you must carry out and compute from first principles:\n1. Derive an expression for the certified radius (certified margin) of the classifier under $\\ell_2$ perturbations at $\\mathbf{x}$, defined as the minimum $\\ell_2$-norm of a perturbation needed to change the classifier’s sign. Express this purely in terms of $\\mathbf{w}$, $b$, and $\\mathbf{x}$.\n2. Derive a condition that ensures the true target $y^\\star$ does not change for any perturbation $\\boldsymbol{\\delta}$ with $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$. Express this in terms of the distance from $\\mathbf{x}$ to the oracle hyperplane $\\{\\mathbf{z} : \\mathbf{v}^\\top \\mathbf{z} = 0\\}$ and the bound $\\varepsilon$.\n3. Using your derivations, for each test case below, compute:\n   - A boolean $T$ indicating whether the true target is invariant for all allowed perturbations. Use strict inequality: declare invariant only if the distance to the oracle hyperplane is strictly greater than $\\varepsilon$.\n   - The certified radius $R$ of the classifier at $\\mathbf{x}$ under $\\ell_2$ perturbations.\n   - A boolean $C$ indicating whether the classifier is certifiably robust against all perturbations with $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$. Use strict inequality: declare certifiable robustness only if $R$ is strictly greater than $\\varepsilon$.\n   - A boolean $A$ indicating whether an adversarial example exists that changes the classifier’s output without changing the true target, i.e., $A$ is true if and only if $T$ is true and $C$ is false.\n\nTest suite (each case lists $(\\mathbf{w}, b, \\mathbf{v}, \\mathbf{x}, \\varepsilon)$ with dimension $d = 2$):\n- Case $1$: $\\mathbf{w} = (1, 0)$, $b = 0$, $\\mathbf{v} = (1, 0)$, $\\mathbf{x} = (2, 0)$, $\\varepsilon = 0.5$.\n- Case $2$: $\\mathbf{w} = (1, 0)$, $b = 0$, $\\mathbf{v} = (1, 0)$, $\\mathbf{x} = (0.6, 0)$, $\\varepsilon = 0.8$.\n- Case $3$: $\\mathbf{w} = (1, 0)$, $b = 0$, $\\mathbf{v} = (1, 0)$, $\\mathbf{x} = (1.0, 0)$, $\\varepsilon = 1.0$.\n- Case $4$: $\\mathbf{w} = (1, 0)$, $b = -2.5$, $\\mathbf{v} = (1, 0)$, $\\mathbf{x} = (3.0, 0)$, $\\varepsilon = 0.6$.\n- Case $5$: $\\mathbf{w} = (0, 1)$, $b = 0$, $\\mathbf{v} = (2, 0)$, $\\mathbf{x} = (1.5, 0.4)$, $\\varepsilon = 0.7$.\n- Case $6$: $\\mathbf{w} = (1, -1)$, $b = 0.1$, $\\mathbf{v} = (1, -1)$, $\\mathbf{x} = (10, 0)$, $\\varepsilon = 7.0$.\n\nNumerical and output requirements:\n- For each test case, output a list of the form $[T, R, C, A]$, where $T$, $C$, and $A$ are booleans, and $R$ is a floating-point number rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of these lists, enclosed in square brackets, for example: $[[\\dots],[\\dots],\\dots]$.\n- There are no physical units or angles in this problem.",
            "solution": "The problem requires the formal analysis of a binary linear classifier's robustness to adversarial perturbations. We must derive expressions for the certified radius and the stability of an oracle-defined true target, and then apply these to a series of test cases.\n\nLet us begin by establishing the mathematical framework from first principles.\n\n**1. Certified Radius of the Classifier**\n\nThe classifier's output for an input $\\mathbf{x} \\in \\mathbb{R}^d$ is given by $\\hat{y} = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\mathbf{w} \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are the model parameters. An adversarial perturbation $\\boldsymbol{\\delta}$ is applied, such that the new input is $\\mathbf{x}' = \\mathbf{x} + \\boldsymbol{\\delta}$ and the perturbation is bounded by $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$.\n\nThe classifier's output is flipped if the sign of its decision function, $f(\\mathbf{z}) = \\mathbf{w}^\\top \\mathbf{z} + b$, changes. That is, $\\operatorname{sign}(f(\\mathbf{x}')) \\neq \\operatorname{sign}(f(\\mathbf{x}))$. The smallest perturbation required to achieve this will move the point $\\mathbf{x}$ exactly onto the classifier's decision boundary hyperplane, which is the set of points $\\mathbf{z}$ where $f(\\mathbf{z}) = 0$.\n\nTherefore, we seek to find the perturbation $\\boldsymbol{\\delta}$ with the minimum Euclidean norm, $\\lVert \\boldsymbol{\\delta} \\rVert_2$, that satisfies the condition:\n$$ \\mathbf{w}^\\top (\\mathbf{x} + \\boldsymbol{\\delta}) + b = 0 $$\nRearranging this equation, we get:\n$$ \\mathbf{w}^\\top \\boldsymbol{\\delta} = -(\\mathbf{w}^\\top \\mathbf{x} + b) $$\nThe certified radius, denoted by $R$, is the minimum value of $\\lVert \\boldsymbol{\\delta} \\rVert_2$ that satisfies this constraint. This is a classic optimization problem: minimize $\\lVert \\boldsymbol{\\delta} \\rVert_2$ subject to $\\mathbf{w}^\\top \\boldsymbol{\\delta} = c$, where $c = -(\\mathbf{w}^\\top \\mathbf{x} + b)$.\n\nFrom the Cauchy–Schwarz inequality, we have $\\lvert \\mathbf{w}^\\top \\boldsymbol{\\delta} \\rvert \\le \\lVert \\mathbf{w} \\rVert_2 \\lVert \\boldsymbol{\\delta} \\rVert_2$. To satisfy the constraint with minimal $\\lVert \\boldsymbol{\\delta} \\rVert_2$, this inequality must be tight (an equality), which occurs when $\\boldsymbol{\\delta}$ is collinear with $\\mathbf{w}$. Thus, the optimal perturbation is in the direction of $-\\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b) \\cdot \\mathbf{w}$.\n\nSolving for $\\lVert \\boldsymbol{\\delta} \\rVert_2$:\n$$ \\lVert \\boldsymbol{\\delta} \\rVert_2 \\ge \\frac{\\lvert \\mathbf{w}^\\top \\boldsymbol{\\delta} \\rvert}{\\lVert \\mathbf{w} \\rVert_2} $$\nSubstituting the constraint $\\mathbf{w}^\\top \\boldsymbol{\\delta} = -(\\mathbf{w}^\\top \\mathbf{x} + b)$:\n$$ \\lVert \\boldsymbol{\\delta} \\rVert_2 \\ge \\frac{\\lvert -(\\mathbf{w}^\\top \\mathbf{x} + b) \\rvert}{\\lVert \\mathbf{w} \\rVert_2} = \\frac{\\lvert \\mathbf{w}^\\top \\mathbf{x} + b \\rvert}{\\lVert \\mathbf{w} \\rVert_2} $$\nThe minimum norm is the lower bound. Thus, the certified radius $R$ is:\n$$ R = \\frac{\\lvert \\mathbf{w}^\\top \\mathbf{x} + b \\rvert}{\\lVert \\mathbf{w} \\rVert_2} $$\nThis expression corresponds to the given definition of the Euclidean distance from a point $\\mathbf{x}$ to the hyperplane $\\{\\mathbf{z} : \\mathbf{w}^\\top \\mathbf{z} + b = 0\\}$.\n\n**2. Invariance of the True Target**\n\nThe true target is given by an oracle as $y^\\star = \\operatorname{sign}(\\mathbf{v}^\\top \\mathbf{x})$. For a perturbed input $\\mathbf{x} + \\boldsymbol{\\delta}$, the new target would be $\\operatorname{sign}(\\mathbf{v}^\\top (\\mathbf{x} + \\boldsymbol{\\delta}))$. The target is invariant if its sign does not change for any allowed perturbation $\\boldsymbol{\\delta}$ where $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$.\n\nThis requires that the quantity $\\mathbf{v}^\\top \\mathbf{x}$ and $\\mathbf{v}^\\top (\\mathbf{x} + \\boldsymbol{\\delta}) = \\mathbf{v}^\\top \\mathbf{x} + \\mathbf{v}^\\top \\boldsymbol{\\delta}$ have the same sign. This condition is guaranteed if the magnitude of the change, $\\lvert \\mathbf{v}^\\top \\boldsymbol{\\delta} \\rvert$, is strictly less than the magnitude of the original value, $\\lvert \\mathbf{v}^\\top \\mathbf{x} \\rvert$.\n$$ \\lvert \\mathbf{v}^\\top \\boldsymbol{\\delta} \\rvert < \\lvert \\mathbf{v}^\\top \\mathbf{x} \\rvert $$\nWe need this to hold for all $\\boldsymbol{\\delta}$ such that $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$. To find the most challenging perturbation, we must maximize the term $\\lvert \\mathbf{v}^\\top \\boldsymbol{\\delta} \\rvert$. According to the Cauchy–Schwarz inequality:\n$$ \\lvert \\mathbf{v}^\\top \\boldsymbol{\\delta} \\rvert \\le \\lVert \\mathbf{v} \\rVert_2 \\lVert \\boldsymbol{\\delta} \\rVert_2 $$\nSince $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$, the maximum value is $\\lVert \\mathbf{v} \\rVert_2 \\varepsilon$. This worst-case change occurs when $\\boldsymbol{\\delta}$ is aligned with $\\mathbf{v}$ and has magnitude $\\varepsilon$.\n\nThus, for the target to remain invariant under all possible perturbations, the worst-case change must not be sufficient to flip the sign:\n$$ \\lVert \\mathbf{v} \\rVert_2 \\varepsilon < \\lvert \\mathbf{v}^\\top \\mathbf{x} \\rvert $$\nAssuming $\\mathbf{v}$ is not the zero vector (which holds for all test cases, since $\\lVert \\mathbf{v} \\rVert_2 > 0$), we can rearrange this to:\n$$ \\varepsilon < \\frac{\\lvert \\mathbf{v}^\\top \\mathbf{x} \\rvert}{\\lVert \\mathbf{v} \\rVert_2} $$\nThe term on the right is the Euclidean distance from $\\mathbf{x}$ to the oracle's decision boundary, the hyperplane $\\{\\mathbf{z} : \\mathbf{v}^\\top \\mathbf{z} = 0\\}$. The condition for target invariance is that this distance must be strictly greater than the perturbation bound $\\varepsilon$.\n\n**3. Computation of Per-Case Metrics**\n\nBased on the derivations above, we can now define the computational procedure for each test case.\n- **$T$ (Target Invariance):** This boolean is true if the true target $y^\\star$ is invariant for all perturbations with $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$. From our derivation, this corresponds to the condition:\n$$ T := \\left( \\frac{\\lvert \\mathbf{v}^\\top \\mathbf{x} \\rvert}{\\lVert \\mathbf{v} \\rVert_2} > \\varepsilon \\right) $$\n- **$R$ (Certified Radius):** This is the value derived in the first part:\n$$ R := \\frac{\\lvert \\mathbf{w}^\\top \\mathbf{x} + b \\rvert}{\\lVert \\mathbf{w} \\rVert_2} $$\n- **$C$ (Certified Robustness):** This boolean is true if the classifier is certifiably robust for the given $\\varepsilon$. This means that the minimum perturbation needed to change the classification, $R$, is strictly greater than the maximum allowed perturbation, $\\varepsilon$.\n$$ C := (R > \\varepsilon) $$\n- **$A$ (Adversarial Example Existence):** An adversarial example exists if it is possible to change the classifier's prediction ($\\neg C$) while the true target remains unchanged ($T$). The problem defines this condition as $A$ being true if and only if $T$ is true and $C$ is false.\n$$ A := T \\land (\\neg C) $$\nThese formulas are implemented to process the provided test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes robustness metrics for a binary linear classifier under L2 perturbations.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (w, b, v, x, epsilon)\n    test_cases = [\n        # Case 1\n        (np.array([1.0, 0.0]), 0.0, np.array([1.0, 0.0]), np.array([2.0, 0.0]), 0.5),\n        # Case 2\n        (np.array([1.0, 0.0]), 0.0, np.array([1.0, 0.0]), np.array([0.6, 0.0]), 0.8),\n        # Case 3\n        (np.array([1.0, 0.0]), 0.0, np.array([1.0, 0.0]), np.array([1.0, 0.0]), 1.0),\n        # Case 4\n        (np.array([1.0, 0.0]), -2.5, np.array([1.0, 0.0]), np.array([3.0, 0.0]), 0.6),\n        # Case 5\n        (np.array([0.0, 1.0]), 0.0, np.array([2.0, 0.0]), np.array([1.5, 0.4]), 0.7),\n        # Case 6\n        (np.array([1.0, -1.0]), 0.1, np.array([1.0, -1.0]), np.array([10.0, 0.0]), 7.0),\n    ]\n\n    def compute_metrics(w, b, v, x, epsilon):\n        \"\"\"\n        Calculates the metrics T, R, C, A for a single test case.\n        \"\"\"\n        \n        # Calculate T: Target Invariance\n        # Condition: distance to oracle hyperplane > epsilon\n        # Distance = |v.T * x| / ||v||_2\n        norm_v = np.linalg.norm(v, 2)\n        if norm_v == 0:\n            # Oracle is undefined, but problem constraints ensure norm_v > 0\n            dist_to_oracle_hyperplane = np.inf\n        else:\n            dist_to_oracle_hyperplane = np.abs(np.dot(v, x)) / norm_v\n        \n        T = dist_to_oracle_hyperplane > epsilon\n\n        # Calculate R: Certified Radius\n        # R = |w.T * x + b| / ||w||_2\n        norm_w = np.linalg.norm(w, 2)\n        if norm_w == 0:\n            # Classifier is undefined, but constraints ensure norm_w > 0\n            R = np.inf\n        else:\n            R = np.abs(np.dot(w, x) + b) / norm_w\n\n        # Calculate C: Certified Robustness\n        # Condition: R > epsilon\n        C = R > epsilon\n        \n        # Calculate A: Adversarial Example Existence\n        # Condition: T is true and C is false\n        A = T and not C\n        \n        return [T, R, C, A]\n\n    results_as_lists = []\n    for case in test_cases:\n        w_vec, b_scalar, v_vec, x_vec, eps_scalar = case\n        result = compute_metrics(w_vec, b_scalar, v_vec, x_vec, eps_scalar)\n        results_as_lists.append(result)\n    \n    # Format the results into the required string format\n    # Example: [[True,2.000000,True,False],[...]]\n    formatted_sublists = []\n    for res in results_as_lists:\n        T, R, C, A = res\n        # Format R to 6 decimal places, and convert booleans to 'True'/'False' strings\n        sublist_str = f\"[{str(T)},{R:.6f},{str(C)},{str(A)}]\"\n        formatted_sublists.append(sublist_str)\n        \n    final_output = f\"[{','.join(formatted_sublists)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}