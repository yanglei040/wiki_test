{
    "hands_on_practices": [
        {
            "introduction": "现实世界中的许多学习任务，其响应变量本质上是整数，例如事件发生的次数或物品的评级。然而，标准的回归模型通常输出连续的实数值。这就引出了一个关键问题：如何以一种有原则的方式将连续预测转化为符合约束的整数输出？本练习  将引导你通过编程实践，基于统计决策理论来比较不同的整数化策略（如四舍五入和基于学习分布的决策），从而深刻理解在不同损失函数下，选择最优输出策略的权衡。",
            "id": "3170698",
            "problem": "考虑一个统计学习情境，其中响应（目标）变量 $Y$ 被限制在有限支撑集 $S = \\{0,1,\\ldots,K\\}$ 中取整数值。对于每个测试用例，您会获得：(i) 一个决定 $Y$ 的、在 $S$ 上的真实条件分布 $q$，(ii) 一个由模型生成的、在 $S$ 上的已学习离散预测分布 $p$，以及 (iii) 另一个忽略整数约束的模型提供的实值回归输出 $\\hat{\\mu}$。您的任务是比较遵守整数输出约束的决策规则，并评估它们在 $q$下的真实期望损失。\n\n您必须从风险最小化原则开始构建您的程序：对于任何损失函数 $L(a,y)$ 和 $S$ 上的任何分布 $r$，预测 $a \\in S$ 的期望损失（风险）为\n$$\n\\mathcal{R}_r(a) = \\mathbb{E}_{Y \\sim r}[L(a,Y)] = \\sum_{y \\in S} r(y)\\,L(a,y).\n$$\n关于 $r$ 的贝叶斯最优决策是任意 $a^\\star \\in \\arg\\min_{a \\in S} \\mathcal{R}_r(a)$。平局必须通过选择最小的可行整数来解决。\n\n您必须实现并评估以下五种生成限制在 $S$ 内的整数值预测的策略：\n- 策略 $0$ (最近取整)：预测 $R_{\\text{near}}(\\hat{\\mu}) = \\left\\lfloor \\hat{\\mu} + 0.5 \\right\\rfloor$，然后裁剪到 $S$ 范围内，方法是将任何小于 $0$ 的值替换为 $0$，将任何大于 $K$ 的值替换为 $K$。\n- 策略 $1$ (向下取整)：预测 $R_{\\lfloor \\cdot \\rfloor}(\\hat{\\mu}) = \\left\\lfloor \\hat{\\mu} \\right\\rfloor$ 并裁剪到 $S$ 范围内。\n- 策略 $2$ (向上取整)：预测 $R_{\\lceil \\cdot \\rceil}(\\hat{\\mu}) = \\left\\lceil \\hat{\\mu} \\right\\rceil$ 并裁剪到 $S$ 范围内。\n- 策略 $3$ (随机取整)：如果 $\\hat{\\mu} \\in [0,K]$ 且 $\\hat{\\mu} \\notin \\mathbb{Z}$，则写作 $\\hat{\\mu} = k + \\alpha$，其中 $k \\in \\{0,\\ldots,K-1\\}$ 且 $\\alpha \\in (0,1)$。以 $1-\\alpha$ 的概率预测 $k$，以 $\\alpha$ 的概率预测 $k+1$。如果 $\\hat{\\mu} \\in \\mathbb{Z}$，则预测该整数。如果 $\\hat{\\mu} < 0$，预测 $0$；如果 $\\hat{\\mu} > K$，预测 $K$。为进行评估，通过将每个可能取整值的风险与其取整概率进行加权平均，来计算在 $q$ 下的期望风险。\n- 策略 $4$ (基于已学习分布的决策)：选择 $a^\\star \\in \\arg\\min_{a \\in S} \\mathcal{R}_p(a)$，即在已学习分布 $p$ 下最小化期望损失，通过选择最小的 $a$ 来打破平局，然后评估其在 $q$ 下的真实期望损失。\n\n在三种损失函数下评估每种策略：\n- 平方损失 $L_2(a,y) = (a - y)^2$。\n- 绝对损失 $L_1(a,y) = |a - y|$。\n- 带参数 $\\tau \\in (0,1)$ 的非对称绝对损失（弹球损失）：\n$$\nL_\\tau(a,y) = \\tau \\max(a - y, 0) + (1 - \\tau) \\max(y - a, 0).\n$$\n对所有测试用例使用 $\\tau = 0.3$。\n\n对于每个测试用例和每种损失，计算所有策略 $0$ 到 $4$ 在 $q$ 下的真实期望损失，并返回达到最小真实期望损失的策略索引（通过最小索引打破平局）。每个测试用例的最终输出必须是一个包含三个整数的列表 $[i_{2}, i_{1}, i_{\\tau}]$，其中 $i_{2}$ 是最小化平方损失的索引，$i_{1}$ 是最小化绝对损失的索引，而 $i_{\\tau}$ 是最小化 $\\tau = 0.3$ 的非对称损失的索引。\n\n您的程序必须使用以下固定的测试套件。在所有情况下，支撑集为 $S = \\{0,1,2,3,4\\}$，因此 $K = 4$。\n- 测试用例 $1$：$q = [0.05, 0.15, 0.50, 0.20, 0.10]$，$p = [0.10, 0.20, 0.40, 0.20, 0.10]$，$\\hat{\\mu} = 2.2$。\n- 测试用例 $2$：$q = [0.00, 0.50, 0.50, 0.00, 0.00]$，$p = [0.05, 0.45, 0.45, 0.05, 0.00]$，$\\hat{\\mu} = 1.5$。\n- 测试用例 $3$：$q = [0.05, 0.05, 0.10, 0.20, 0.60]$，$p = [0.20, 0.20, 0.20, 0.20, 0.20]$，$\\hat{\\mu} = 3.8$。\n- 测试用例 $4$：$q = [0.45, 0.05, 0.00, 0.05, 0.45]$，$p = [0.05, 0.10, 0.70, 0.10, 0.05]$，$\\hat{\\mu} = 2.0$。\n- 测试用例 $5$：$q = [0.70, 0.20, 0.05, 0.03, 0.02]$，$p = [0.65, 0.20, 0.10, 0.03, 0.02]$，$\\hat{\\mu} = -0.3$。\n- 测试用例 $6$：$q = [0.02, 0.03, 0.05, 0.20, 0.70]$，$p = [0.05, 0.10, 0.15, 0.20, 0.50]$，$\\hat{\\mu} = 5.1$。\n\n不涉及角度或物理单位。所有概率均以小数形式给出，并且每个分布的概率之和为 $1$。\n\n最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。该行必须是 Python 风格的列表，包含六个子列表，每个子列表按上述顺序对应一个测试用例，并且每个子列表包含三个整数 $[i_{2}, i_{1}, i_{\\tau}]$。例如，一个有效的输出看起来会像 $[[0,1,4],[\\ldots],\\ldots]$，但使用的是此问题实际计算出的整数。",
            "solution": "此问题经评估有效。它在科学上基于统计决策理论，内容自洽，提法明确，客观，且其组成部分均有形式化且无歧义的定义。所有必要的数据和约束都已提供。\n\n问题的核心是评估五种不同的策略，为具有有限整数支撑集 $S = \\{0, 1, \\ldots, K\\}$ 的响应变量 $Y$ 生成整数预测。评估基于风险最小化原则。对于 $S$ 上的概率分布 $r$，预测（行动）$a$ 的风险（或期望损失）由下式给出：\n$$\n\\mathcal{R}_r(a) = \\mathbb{E}_{Y \\sim r}[L(a,Y)] = \\sum_{y=0}^{K} r(y)\\,L(a,y)\n$$\n其中 $L(a,y)$ 是一个指定的损失函数。我们将计算真实期望损失，这意味着期望是相对于真实数据生成分布 $q$ 计算的。\n\n所有测试用例的支撑集均为 $S = \\{0, 1, 2, 3, 4\\}$，对应 $K=4$。我们将在三种损失函数下评估这些策略：\n1.  **平方损失 ($L_2$)**：$L_2(a,y) = (a - y)^2$。此损失的贝叶斯最优预测器是分布的期望值 $\\mathbb{E}[Y]$。\n2.  **绝对损失 ($L_1$)**：$L_1(a,y) = |a - y|$。此损失的贝叶斯最优预测器是分布的中位数。\n3.  **非对称绝对损失 ($L_\\tau$)**：$L_\\tau(a,y) = \\tau \\max(a - y, 0) + (1 - \\tau) \\max(y - a, 0)$，参数 $\\tau = 0.3$。此损失的贝叶斯最优预测器是分布的 $\\tau$-分位数。\n\n对于每个测试用例，我们都给定一个真实分布 $q$、一个学习到的分布 $p$ 和一个实值回归输出 $\\hat{\\mu}$。我们将为五种策略中的每一种计算预测，然后在三种损失函数下分别计算其真实风险 $\\mathcal{R}_q(\\cdot)$。\n\n五种策略如下：\n\n**策略 $0$：最近取整**\n预测 $a_0$ 是通过将 $\\hat{\\mu}$ 四舍五入到最近的整数，然后将结果裁剪到有效范围 $[0, K]$ 内得到的。\n$$\na_0 = \\text{clip}(\\lfloor \\hat{\\mu} + 0.5 \\rfloor, 0, K)\n$$\n这是一个确定性策略，产生单个整数预测。\n\n**策略 $1$：向下取整**\n预测 $a_1$ 是通过对 $\\hat{\\mu}$ 向下取整，然后进行裁剪得到的。\n$$\na_1 = \\text{clip}(\\lfloor \\hat{\\mu} \\rfloor, 0, K)\n$$\n这也是一个确定性策略。\n\n**策略 $2$：向上取整**\n预测 $a_2$ 是通过对 $\\hat{\\mu}$ 向上取整，然后进行裁剪得到的。\n$$\na_2 = \\text{clip}(\\lceil \\hat{\\mu} \\rceil, 0, K)\n$$\n这也是一个确定性策略。\n\n**策略 $3$：随机取整**\n此策略的预测可以是概率性的。\n- 如果 $\\hat{\\mu} < 0$，预测为 $0$。\n- 如果 $\\hat{\\mu} > K$，预测为 $K$。\n- 如果 $\\hat{\\mu} \\in [0, K]$ 是一个整数，预测为 $\\hat{\\mu}$。\n- 如果 $\\hat{\\mu} \\in (0, K)$ 不是整数，设 $\\hat{\\mu} = k + \\alpha$，其中 $k = \\lfloor\\hat{\\mu}\\rfloor$ 是一个整数，$\\alpha \\in (0,1)$ 是小数部分。该策略以 $1-\\alpha$ 的概率预测 $k$，以 $\\alpha$ 的概率预测 $k+1$。\n\n对于此策略的确定性结果，真实风险为 $\\mathcal{R}_q(a_3)$。对于随机情况，评估的量是期望真实风险，即对取整概率进行平均：\n$$\n\\mathbb{E}[\\mathcal{R}_q(a_3)] = (1-\\alpha) \\mathcal{R}_q(k) + \\alpha \\mathcal{R}_q(k+1)\n$$\n\n**策略 $4$：基于已学习分布的决策**\n此策略使用学习到的分布 $p$ 来寻找贝叶斯最优预测。选择预测 $a_4$ 以最小化在 $p$ 下的期望损失：\n$$\na_4 = \\arg\\min_{a \\in S} \\mathcal{R}_p(a) = \\arg\\min_{a \\in S} \\sum_{y=0}^{K} p(y) L(a,y)\n$$\n平局通过选择最小的整数 $a$ 来解决。至关重要的是要注意，预测 $a_4$ 取决于所考虑的损失函数。因此，我们将为 $L_2$、$L_1$ 和 $L_\\tau$ 分别推导出一个不同的 $a_4$。一旦针对给定的损失确定了 $a_4$，其性能将通过计算真实风险 $\\mathcal{R}_q(a_4)$ 来评估。\n\n**评估**\n对于每种损失函数（$L_2$, $L_1$, $L_\\tau$），我们将生成一个包含五个风险值的向量，每个策略对应一个值。\n$$\n[\\mathcal{R}_q(a_0), \\mathcal{R}_q(a_1), \\mathcal{R}_q(a_2), \\mathbb{E}[\\mathcal{R}_q(a_3)], \\mathcal{R}_q(a_4)]\n$$\n然后我们找到该向量中最小值的索引。如果出现平局，则选择最小的索引。对于每个测试用例，此过程产生三个索引：$i_2$ 用于平方损失，$i_1$ 用于绝对损失，$i_\\tau$ 用于非对称损失。最终输出是这三个索引的列表 $[i_2, i_1, i_\\tau]$。对所有提供的测试用例重复此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the statistical learning problem by evaluating five decision strategies\n    under three different loss functions for a series of test cases.\n    \"\"\"\n    \n    # Problem Constants\n    K = 4\n    S_arr = np.arange(K + 1)\n    TAU = 0.3\n\n    # Test cases from the problem statement\n    test_cases = [\n        # (q, p, mu_hat)\n        (np.array([0.05, 0.15, 0.50, 0.20, 0.10]), np.array([0.10, 0.20, 0.40, 0.20, 0.10]), 2.2),\n        (np.array([0.00, 0.50, 0.50, 0.00, 0.00]), np.array([0.05, 0.45, 0.45, 0.05, 0.00]), 1.5),\n        (np.array([0.05, 0.05, 0.10, 0.20, 0.60]), np.array([0.20, 0.20, 0.20, 0.20, 0.20]), 3.8),\n        (np.array([0.45, 0.05, 0.00, 0.05, 0.45]), np.array([0.05, 0.10, 0.70, 0.10, 0.05]), 2.0),\n        (np.array([0.70, 0.20, 0.05, 0.03, 0.02]), np.array([0.65, 0.20, 0.10, 0.03, 0.02]), -0.3),\n        (np.array([0.02, 0.03, 0.05, 0.20, 0.70]), np.array([0.05, 0.10, 0.15, 0.20, 0.50]), 5.1),\n    ]\n\n    # --- Loss Functions ---\n    def loss_l2(a, y):\n        return (a - y) ** 2\n\n    def loss_l1(a, y):\n        return np.abs(a - y)\n\n    def loss_ltau(a, y, tau):\n        return tau * np.maximum(a - y, 0) + (1 - tau) * np.maximum(y - a, 0)\n\n    loss_functions = [loss_l2, loss_l1, loss_ltau]\n\n    def calculate_risk(a, dist, loss_func, S):\n        \"\"\"Calculates the expected loss (risk) for a given action and distribution.\"\"\"\n        if loss_func == loss_ltau:\n            losses = loss_func(a, S, TAU)\n        else:\n            losses = loss_func(a, S)\n        return np.sum(dist * losses)\n\n    all_results = []\n    for q, p, mu_hat in test_cases:\n        \n        # --- Generate Predictions for Strategies 0, 1, 2 ---\n        a0 = int(np.clip(np.floor(mu_hat + 0.5), 0, K))\n        a1 = int(np.clip(np.floor(mu_hat), 0, K))\n        a2 = int(np.clip(np.ceil(mu_hat), 0, K))\n\n        case_best_indices = []\n        for loss_func in loss_functions:\n            risks_for_loss = []\n\n            # --- Evaluate Strategies 0, 1, 2 ---\n            risks_for_loss.append(calculate_risk(a0, q, loss_func, S_arr))\n            risks_for_loss.append(calculate_risk(a1, q, loss_func, S_arr))\n            risks_for_loss.append(calculate_risk(a2, q, loss_func, S_arr))\n\n            # --- Evaluate Strategy 3 (Stochastic Rounding) ---\n            risk_s3 = 0.0\n            if mu_hat < 0:\n                risk_s3 = calculate_risk(0, q, loss_func, S_arr)\n            elif mu_hat > K:\n                risk_s3 = calculate_risk(K, q, loss_func, S_arr)\n            elif mu_hat == int(mu_hat):\n                risk_s3 = calculate_risk(int(mu_hat), q, loss_func, S_arr)\n            else: # Stochastic case\n                k = int(np.floor(mu_hat))\n                alpha = mu_hat - k\n                risk_k = calculate_risk(k, q, loss_func, S_arr)\n                risk_k1 = calculate_risk(k + 1, q, loss_func, S_arr)\n                risk_s3 = (1 - alpha) * risk_k + alpha * risk_k1\n            risks_for_loss.append(risk_s3)\n\n            # --- Evaluate Strategy 4 (Learned-Distribution Decision) ---\n            # 1. Find the Bayes-optimal action 'a4' under the learned distribution 'p'\n            risks_under_p = [calculate_risk(a_candidate, p, loss_func, S_arr) for a_candidate in S_arr]\n            a4 = np.argmin(risks_under_p)\n            \n            # 2. Evaluate the true risk of 'a4' under the true distribution 'q'\n            risk_s4 = calculate_risk(a4, q, loss_func, S_arr)\n            risks_for_loss.append(risk_s4)\n\n            # --- Determine the best strategy for this loss function ---\n            best_strategy_index = np.argmin(risks_for_loss)\n            case_best_indices.append(best_strategy_index)\n        \n        all_results.append(case_best_indices)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在分类问题中，一个优秀的模型不仅应能预测正确的类别，其输出的概率也应是可靠的。一个“良好校准”的模型，其预测概率能够真实反映事件发生的可能性。本练习  将带你亲手计算“期望校准误差”（Expected Calibration Error, ECE），这是衡量模型校准度的关键指标。通过这个过程，你将理解校准的含义，并看到一个简单的“向量缩放”技术如何能改善模型的概率输出质量。",
            "id": "3170639",
            "problem": "一个多类概率分类器通过对 logits 应用 softmax 函数来为 $K=3$ 个类别生成输出。对于每个输入 $i \\in \\{1,2,3,4\\}$（共有 $N=4$ 个输入），其 softmax 前的 logits 由 $z_{ik} = \\ln(n_{ik})$ 给出，其中 $n_{ik}$ 是下面列出的正整数。每个输入的响应变量 $Y$（目标类别）也已给出。\n\n- 输入 $1$：$(n_{1,1}, n_{1,2}, n_{1,3}) = (4, 1, 1)$ 且 $Y_1 = 1$。\n- 输入 $2$：$(n_{2,1}, n_{2,2}, n_{2,3}) = (1, 3, 1)$ 且 $Y_2 = 2$。\n- 输入 $3$：$(n_{3,1}, n_{3,2}, n_{3,3}) = (1, 1, 5)$ 且 $Y_3 = 3$。\n- 输入 $4$：$(n_{4,1}, n_{4,2}, n_{4,3}) = (2, 2, 1)$ 且 $Y_4 = 2$。\n\n对于输入 $i$，其 softmax 输出（预测的类别概率）为 $p_{ik} = \\exp(z_{ik}) \\big/ \\sum_{j=1}^{3} \\exp(z_{ij})$，其中 $k \\in \\{1,2,3\\}$。考虑使用一对多可靠性图进行按类校准，使用两个等宽的区间：$[0, 0.5]$ 和 $(0.5, 1]$。类别 $k$ 的按类期望校准误差 (ECE) 是对这些区间内，类别 $k$ 的平均预测概率与该区间内 $Y=k$ 的经验频率之间的加权绝对差进行聚合。宏平均 ECE 是这 3 个类别的按类 ECE 值的简单平均值。\n\n现在考虑一种向量缩放校准，它通过 $z'_{ik} = a_k z_{ik} + b_k$ 将 logits 映射到新的 logits，然后通过 softmax 生成校准后的概率 $p'_{ik} = \\exp(z'_{ik}) \\big/ \\sum_{j=1}^{3} \\exp(z'_{ij})$。假设这些参数固定为 $a_1 = 1$，$a_2 = 1$，$a_3 = 1$，以及 $b_1 = \\ln(2)$，$b_2 = 0$，$b_3 = -\\ln(2)$。\n\n仅使用上述基础定义（softmax 概率、一对多区间划分和按类期望校准误差），计算在指定的双区间方案下，校准后输出 $p'_{ik}$ 的按类宏平均 ECE。将最终答案表示为精确分数。无需四舍五入。",
            "solution": "问题陈述已经过验证，被认为是合理的。它是自洽的，科学上基于统计学习的原理，并且是良定的，为唯一解提供了所有必要的数据和定义。\n\n目标是为一组校准后的概率输出计算按类宏平均期望校准误差 (ECE)。计算过程分为四个主要步骤：\n1.  计算校准后的 logits $z'_{ik}$。\n2.  计算校准后的 softmax 概率 $p'_{ik}$。\n3.  为 $K=3$ 个类别中的每一个计算按类 ECE，$\\text{ECE}_k$。\n4.  计算宏平均 ECE，即按类 ECE 的算术平均值。\n\n输入总数为 $N=4$。\n\n**步骤 1：计算校准后的 Logits**\n\n原始 logits 由 $z_{ik} = \\ln(n_{ik})$ 给出。校准变换是由 $z'_{ik} = a_k z_{ik} + b_k$ 定义的向量缩放。\n给定参数为 $a_1=1$，$a_2=1$，$a_3=1$，以及 $b_1 = \\ln(2)$，$b_2 = 0$，$b_3 = -\\ln(2)$。\n\n对于每个类别 $k \\in \\{1, 2, 3\\}$ 和任意输入 $i$：\n$z'_{i1} = 1 \\cdot z_{i1} + \\ln(2) = \\ln(n_{i1}) + \\ln(2) = \\ln(2n_{i1})$\n$z'_{i2} = 1 \\cdot z_{i2} + 0 = \\ln(n_{i2})$\n$z'_{i3} = 1 \\cdot z_{i3} - \\ln(2) = \\ln(n_{i3}) + \\ln(\\frac{1}{2}) = \\ln(\\frac{n_{i3}}{2})$\n\n**步骤 2：计算校准后的概率**\n\n校准后的概率由应用于校准后 logits 的 softmax 函数给出：\n$p'_{ik} = \\frac{\\exp(z'_{ik})}{\\sum_{j=1}^{3} \\exp(z'_{ij})}$\n使用步骤 1 的结果，我们有 $\\exp(z'_{i1}) = 2n_{i1}$，$\\exp(z'_{i2}) = n_{i2}$，以及 $\\exp(z'_{i3}) = \\frac{n_{i3}}{2}$。\n分母是 $\\sum_{j=1}^{3} \\exp(z'_{ij}) = 2n_{i1} + n_{i2} + \\frac{n_{i3}}{2}$。\n\n现在我们为 $N=4$ 个输入中的每一个计算这些概率。\n\n- **输入 1**：$(n_{1,1}, n_{1,2}, n_{1,3}) = (4, 1, 1)$。\n分母 = $2(4) + 1 + \\frac{1}{2} = 8 + 1 + 0.5 = 9.5 = \\frac{19}{2}$。\n$p'_{1,1} = \\frac{2(4)}{19/2} = \\frac{8}{19/2} = \\frac{16}{19}$。\n$p'_{1,2} = \\frac{1}{19/2} = \\frac{2}{19}$。\n$p'_{1,3} = \\frac{1/2}{19/2} = \\frac{1}{19}$。\n\n- **输入 2**：$(n_{2,1}, n_{2,2}, n_{2,3}) = (1, 3, 1)$。\n分母 = $2(1) + 3 + \\frac{1}{2} = 2 + 3 + 0.5 = 5.5 = \\frac{11}{2}$。\n$p'_{2,1} = \\frac{2(1)}{11/2} = \\frac{4}{11}$。\n$p'_{2,2} = \\frac{3}{11/2} = \\frac{6}{11}$。\n$p'_{2,3} = \\frac{1/2}{11/2} = \\frac{1}{11}$。\n\n- **输入 3**：$(n_{3,1}, n_{3,2}, n_{3,3}) = (1, 1, 5)$。\n分母 = $2(1) + 1 + \\frac{5}{2} = 2 + 1 + 2.5 = 5.5 = \\frac{11}{2}$。\n$p'_{3,1} = \\frac{2(1)}{11/2} = \\frac{4}{11}$。\n$p'_{3,2} = \\frac{1}{11/2} = \\frac{2}{11}$。\n$p'_{3,3} = \\frac{5/2}{11/2} = \\frac{5}{11}$。\n\n- **输入 4**：$(n_{4,1}, n_{4,2}, n_{4,3}) = (2, 2, 1)$。\n分母 = $2(2) + 2 + \\frac{1}{2} = 4 + 2 + 0.5 = 6.5 = \\frac{13}{2}$。\n$p'_{4,1} = \\frac{2(2)}{13/2} = \\frac{8}{13}$。\n$p'_{4,2} = \\frac{2}{13/2} = \\frac{4}{13}$。\n$p'_{4,3} = \\frac{1/2}{13/2} = \\frac{1}{13}$。\n\n**步骤 3：计算按类 ECE**\n\n类别 $k$ 的按类 ECE 由 $\\text{ECE}_k = \\sum_{m=1}^{M} \\frac{|B_{m,k}|}{N} |\\text{acc}(B_{m,k}) - \\text{conf}(B_{m,k})|$ 给出，其中 $M=2$ 是区间的数量，$B_{1,k}$ 对应于 $[0, 0.5]$ 内的概率，$B_{2,k}$ 对应于 $(0.5, 1]$ 内的概率。\n这可以简化为 $\\text{ECE}_k = \\frac{1}{N} \\sum_{m=1}^{M} \\left| \\sum_{i \\in B_{m,k}} (\\mathbb{I}(Y_i=k) - p'_{ik}) \\right|$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数，真实标签为 $Y_1=1, Y_2=2, Y_3=3, Y_4=2$。\n\n- **类别 1 ($k=1$) 的 ECE**：\n类别 1 的概率是 $p'_{1,1}=\\frac{16}{19} \\approx 0.842$，$p'_{2,1}=\\frac{4}{11} \\approx 0.364$，$p'_{3,1}=\\frac{4}{11} \\approx 0.364$，$p'_{4,1}=\\frac{8}{13} \\approx 0.615$。\n类别 1 的真实标签是 $\\mathbb{I}(Y_1=1)=1, \\mathbb{I}(Y_2=1)=0, \\mathbb{I}(Y_3=1)=0, \\mathbb{I}(Y_4=1)=0$。\n区间划分：\n$B_{1,1}$ (区间 $[0, 0.5]$) 包含输入 2 和 3。\n$B_{2,1}$ (区间 $(0.5, 1]$) 包含输入 1 和 4。\n各区间的绝对差之和为：\n对于 $B_{1,1}$：$\\left| (\\mathbb{I}(Y_2=1) - p'_{2,1}) + (\\mathbb{I}(Y_3=1) - p'_{3,1}) \\right| = \\left| (0 - \\frac{4}{11}) + (0 - \\frac{4}{11}) \\right| = \\left| -\\frac{8}{11} \\right| = \\frac{8}{11}$。\n对于 $B_{2,1}$：$\\left| (\\mathbb{I}(Y_1=1) - p'_{1,1}) + (\\mathbb{I}(Y_4=1) - p'_{4,1}) \\right| = \\left| (1 - \\frac{16}{19}) + (0 - \\frac{8}{13}) \\right| = \\left| \\frac{3}{19} - \\frac{8}{13} \\right| = \\left| \\frac{39 - 152}{247} \\right| = \\frac{113}{247}$。\n$\\text{ECE}_1 = \\frac{1}{4} \\left( \\frac{8}{11} + \\frac{113}{247} \\right) = \\frac{1}{4} \\left( \\frac{8 \\cdot 247 + 113 \\cdot 11}{11 \\cdot 247} \\right) = \\frac{1}{4} \\left( \\frac{1976 + 1243}{2717} \\right) = \\frac{3219}{10868}$。\n\n- **类别 2 ($k=2$) 的 ECE**：\n概率为 $p'_{1,2}=\\frac{2}{19} \\approx 0.105$，$p'_{2,2}=\\frac{6}{11} \\approx 0.545$，$p'_{3,2}=\\frac{2}{11} \\approx 0.182$，$p'_{4,2}=\\frac{4}{13} \\approx 0.308$。\n真实标签为 $\\mathbb{I}(Y_1=2)=0, \\mathbb{I}(Y_2=2)=1, \\mathbb{I}(Y_3=2)=0, \\mathbb{I}(Y_4=2)=1$。\n区间划分：\n$B_{1,2}$ (区间 $[0, 0.5]$) 包含输入 1, 3, 4。\n$B_{2,2}$ (区间 $(0.5, 1]$) 包含输入 2。\n各区间的绝对差之和为：\n对于 $B_{1,2}$：$\\left| (0 - \\frac{2}{19}) + (0 - \\frac{2}{11}) + (1 - \\frac{4}{13}) \\right| = \\left| -\\frac{2}{19} - \\frac{2}{11} + \\frac{9}{13} \\right| = \\left| \\frac{-2(143) - 2(247) + 9(209)}{2717} \\right| = \\left| \\frac{-286 - 494 + 1881}{2717} \\right| = \\frac{1101}{2717}$。\n对于 $B_{2,2}$：$\\left| (1 - \\frac{6}{11}) \\right| = \\left| \\frac{5}{11} \\right| = \\frac{5}{11}$。\n$\\text{ECE}_2 = \\frac{1}{4} \\left( \\frac{1101}{2717} + \\frac{5}{11} \\right) = \\frac{1}{4} \\left( \\frac{1101 + 5 \\cdot 247}{2717} \\right) = \\frac{1}{4} \\left( \\frac{1101 + 1235}{2717} \\right) = \\frac{2336}{10868}$。\n\n- **类别 3 ($k=3$) 的 ECE**：\n概率为 $p'_{1,3}=\\frac{1}{19} \\approx 0.053$，$p'_{2,3}=\\frac{1}{11} \\approx 0.091$，$p'_{3,3}=\\frac{5}{11} \\approx 0.455$，$p'_{4,3}=\\frac{1}{13} \\approx 0.077$。\n真实标签为 $\\mathbb{I}(Y_1=3)=0, \\mathbb{I}(Y_2=3)=0, \\mathbb{I}(Y_3=3)=1, \\mathbb{I}(Y_4=3)=0$。\n区间划分：所有概率都小于 0.5。\n$B_{1,3}$ (区间 $[0, 0.5]$) 包含所有输入 1, 2, 3, 4。\n$B_{2,3}$ (区间 $(0.5, 1]$) 是空的。\n各区间的绝对差之和为：\n对于 $B_{1,3}$：$\\left| (0-\\frac{1}{19}) + (0-\\frac{1}{11}) + (1-\\frac{5}{11}) + (0-\\frac{1}{13}) \\right| = \\left| -\\frac{1}{19} + (1 - \\frac{1}{11} - \\frac{5}{11}) - \\frac{1}{13} \\right| = \\left| -\\frac{1}{19} + \\frac{5}{11} - \\frac{1}{13} \\right| = \\left| \\frac{-143 + 5(247) - 209}{2717} \\right| = \\left| \\frac{-143 + 1235 - 209}{2717} \\right| = \\frac{883}{2717}$。\n对于 $B_{2,3}$：和为 $0$。\n$\\text{ECE}_3 = \\frac{1}{4} \\left( \\frac{883}{2717} + 0 \\right) = \\frac{883}{10868}$。\n\n**步骤 4：计算宏平均 ECE**\n\n宏平均 ECE 是按类 ECE 的平均值。\n宏平均 ECE $= \\frac{1}{3} (\\text{ECE}_1 + \\text{ECE}_2 + \\text{ECE}_3)$\n$= \\frac{1}{3} \\left( \\frac{3219}{10868} + \\frac{2336}{10868} + \\frac{883}{10868} \\right)$\n$= \\frac{1}{3} \\left( \\frac{3219 + 2336 + 883}{10868} \\right)$\n$= \\frac{1}{3} \\left( \\frac{6438}{10868} \\right) = \\frac{6438}{32604}$。\n\n为了简化分数，我们寻找公因数。\n$6438 = 6 \\times 1073$\n$32604 = 6 \\times 5434$\n分数变为 $\\frac{1073}{5434}$。\n分子的质因数分解是 $1073 = 29 \\times 37$。\n分母的质因数分解是 $5434 = 2 \\times 2717 = 2 \\times 11 \\times 247 = 2 \\times 11 \\times 13 \\times 19$。\n没有共同的质因数，所以该分数已是最简形式。",
            "answer": "$$\\boxed{\\frac{1073}{5434}}$$"
        },
        {
            "introduction": "在许多情况下，我们对生成数据的真实函数具有先验知识，例如我们知道它是平滑的。将这类知识作为结构性约束融入模型，可以有效防止过拟合，并提升模型的泛化能力。本练习  让你通过代码实现，探索如何对回归模型强制施加“利普希茨连续性”（Lipschitz continuity）约束。你将看到，与无约束的多项式模型相比，这种方法如何在数据稀疏或含噪声的情况下，学习到更接近真实规律的函数。",
            "id": "3170644",
            "problem": "给定一个一维监督学习设置，其中标量响应 $y$ 由一个确定性目标函数 $g : [0,1] \\to \\mathbb{R}$ 加上独立噪声生成。我们研究当目标函数在 $[0,1]$ 上具有有界导数时，对学习模型的输出施加利普希茨约束的效果。您必须实现并比较两种模型：一种是利普希茨约束回归，另一种是无约束的多项式最小二乘基线模型。您的程序必须是完全确定性的，并按文末指定的方式产生单行输出。\n\n定义和假设：\n- 响应变量为 $y \\in \\mathbb{R}$，在输入 $x \\in [0,1]$ 处观测。目标函数为 $g(x)$，模型输出为 $\\hat{f}(x)$。\n- 如果对于所有 $x,x' \\in [0,1]$，都有 $|h(x)-h(x')| \\le L |x-x'|$，则称函数 $h$ 在 $[0,1]$ 上是 $L$-利普希茨的。根据中值定理，如果 $g$ 在 $[0,1]$ 上可微，且对所有 $x$ 都有 $|g'(x)| \\le L_\\star$，则 $g$ 是 $L_\\star$-利普希茨的。\n- 数据按 $y_i = g(x_i) + \\epsilon_i$ 生成，其中 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ 独立同分布，所有三角函数的参数均以弧度为单位。\n\n需实现的方法：\n1) 对训练点进行利普希茨约束的经验风险最小化：对训练输入进行排序 $x_{(1)} \\le \\cdots \\le x_{(n)}$。令 $\\mathbf{f} = (f_1,\\ldots,f_n)$ 表示在排序后的训练输入处的模型输出。求解以下凸优化问题：\n$$\n\\min_{\\mathbf{f} \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2} \\sum_{i=1}^n (f_i - y_{(i)})^2\n\\quad \\text{subject to} \\quad\n|f_{i+1} - f_i| \\le L \\,(x_{(i+1)} - x_{(i)}) \\;\\; \\text{for all} \\;\\; i \\in \\{1,\\dots,n-1\\}.\n$$\n在一维情况下，相邻约束已足够，因为三角不等式可以导出所有成对的边界。该问题有唯一解，因为目标函数是严格凸的，且可行集是一个非空的闭凸多面体。\n\n2) 使用 McShane 构造将利普希茨函数扩展到测试输入：给定在训练输入处的拟合值 $\\mathbf{f}$ 和相同的利普希茨常数 $L$，对任意 $x \\in [0,1]$ 定义：\n$$\n\\hat{f}(x) \\;=\\; \\min_{1 \\le i \\le n} \\left\\{ f_i + L \\, |x - x_{(i)}| \\right\\}.\n$$\n这提供了一个在 $[0,1]$ 上的 $L$-利普希茨函数，它在 $x_{(i)}$ 处与 $f_i$ 的值一致。\n\n3) 基线模型：在训练数据上，使用特征 $\\{1,x,x^2,\\dots,x^d\\}$，通过普通最小二乘法拟合一个 $d$ 次无约束多项式，并在测试输入上进行评估。\n\n4) 评估：在 $[0,1]$ 上的一个大小为 $m$ 的均匀测试网格上，计算每个模型相对于无噪声目标函数 $g$ 的均方误差 (MSE)：\n$$\n\\mathrm{MSE}(\\hat{f}) \\;=\\; \\frac{1}{m} \\sum_{j=1}^m \\left(\\hat{f}(x^{\\mathrm{test}}_j) - g(x^{\\mathrm{test}}_j)\\right)^2.\n$$\n对于下方的每个测试用例，报告带符号的改进量：\n$$\n\\Delta \\;=\\; \\mathrm{MSE}(\\text{poly}) \\;-\\; \\mathrm{MSE}(\\text{Lipschitz}),\n$$\n因此，一个正的 $\\Delta$ 值表明利普希茨约束模型的泛化能力优于多项式基线模型。\n\n目标函数及其导数界：\n- $g_{\\sin}(x) = \\tfrac{1}{2} \\sin(2\\pi x)$，对于所有 $x$ 都有 $|g'_{\\sin}(x)| \\le \\pi$，因此 $L_\\star = \\pi$。\n- $g_{\\mathrm{quad}}(x) = x^2$，对于所有 $x \\in [0,1]$ 都有 $|g'_{\\mathrm{quad}}(x)| \\le 2$，因此 $L_\\star = 2$。\n- $g_{\\tanh}(x) = \\tfrac{1}{2} + 0.3 \\tanh(4(x - \\tfrac{1}{2}))$，其导数为 $g'_{\\tanh}(x) = 1.2 \\,\\mathrm{sech}^2(4(x - \\tfrac{1}{2}))$ 且其界为 $1.2$，因此 $L_\\star = 1.2$。\n\n为保证确定性所需的实现细节：\n- 使用一个在 $[0,1]$ 区间内包含 $m = 200$ 个点（含端点）的均匀测试网格。\n- 对于每个测试用例，通过在 $[0,1]$ 上进行独立均匀采样生成 $n$ 个训练输入，然后将它们按非递减顺序排序。在采样前，为每个用例使用一个独立的固定种子来初始化伪随机数生成器。使用同一个生成器实例来采样标准差为 $\\sigma$ 的独立高斯噪声。\n- 对 $d$ 次多项式基线使用普通最小二乘法（无正则化）。\n- 对利普希茨拟合使用上述带相邻约束的二次规划，并使用 McShane 扩展进行预测。\n- 所有三角函数参数均以弧度为单位。\n\n测试套件：\n对每个用例，给定 $(\\text{目标函数ID}, n, \\sigma, L, d, \\text{种子})$。\n- 用例 1: $(\\text{sin}, 12, 0.05, 3.2, 5, 1)$\n- 用例 2: $(\\text{quad}, 8, 0.15, 2.0, 5, 2)$\n- 用例 3: $(\\text{tanh}, 25, 0.05, 1.2, 5, 3)$\n- 用例 4: $(\\text{tanh}, 60, 0.01, 1.2, 5, 4)$\n- 用例 5: $(\\text{sin}, 30, 0.02, 2.5, 5, 5)$\n\n最终输出格式要求：\n您的程序必须仅输出一行，其中包含一个 Python 风格的列表，内含 5 个浮点数，按顺序对应于 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$，每个浮点数需四舍五入到恰好六位小数（接受“银行家舍入法”）。\n\n角度单位说明：\n所有用于三角函数（例如 $\\sin(\\cdot)$ 的参数）的角度均以弧度为单位。\n\n不涉及物理单位。\n\n该程序必须是一个完整、可运行的脚本，不接受任何输入，也不使用任何外部文件或网络访问。",
            "solution": "用户要求实现并比较两种回归模型：一种是利普希茨约束模型，另一种是无约束多项式基模型。该问题是统计学习领域中一个定义明确的数值实验。\n\n### 第 1 步：问题验证\n\n**提取的已知条件：**\n- **数据模型：** $y_i = g(x_i) + \\epsilon_i$，其中 $x_i \\in [0,1]$，$g$ 是一个确定性目标函数，$\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立噪声项。\n- **测试网格：** 在 $[0,1]$ 区间内包含 $m=200$ 个点的均匀网格。\n- **目标函数：**\n    1.  $g_{\\sin}(x) = \\tfrac{1}{2} \\sin(2\\pi x)$，真实利普希茨常数 $L_\\star = \\pi$。\n    2.  $g_{\\mathrm{quad}}(x) = x^2$，在 $[0,1]$ 上 $L_\\star = 2$。\n    3.  $g_{\\tanh}(x) = \\tfrac{1}{2} + 0.3 \\tanh(4(x - \\tfrac{1}{2}))$，$L_\\star = 1.2$。\n- **利普希茨模型：**\n    - 求解二次规划 (QP)：$\\min_{\\mathbf{f}} \\frac{1}{2} \\sum_{i=1}^n (f_i - y_{(i)})^2$，约束条件为 $|f_{i+1} - f_i| \\le L (x_{(i+1)} - x_{(i)})$，其中 $i=1,\\dots,n-1$。这里，$x_{(i)}$ 是排序后的训练输入，$y_{(i)}$ 是对应的观测值。\n    - 预测通过 McShane 扩展完成：$\\hat{f}(x) = \\min_{i} \\{ f_i + L |x - x_{(i)}| \\}$。\n- **多项式模型：**\n    - 在特征 $\\{1, x, \\dots, x^d\\}$ 上进行普通最小二乘回归。\n- **评估指标：** $\\Delta = \\mathrm{MSE}(\\text{poly}) - \\mathrm{MSE}(\\text{Lipschitz})$，其中 $\\mathrm{MSE}(\\hat{f}) = \\frac{1}{m} \\sum_{j=1}^m (\\hat{f}(x^{\\mathrm{test}}_j) - g(x^{\\mathrm{test}}_j))^2$。\n- **测试用例：**\n    - 用例 1: $(\\text{target id}=\\text{'sin'}, n=12, \\sigma=0.05, L=3.2, d=5, \\text{seed}=1)$\n    - 用例 2: $(\\text{target id}=\\text{'quad'}, n=8, \\sigma=0.15, L=2.0, d=5, \\text{seed}=2)$\n    - 用例 3: $(\\text{target id}=\\text{'tanh'}, n=25, \\sigma=0.05, L=1.2, d=5, \\text{seed}=3)$\n    - 用例 4: $(\\text{target id}=\\text{'tanh'}, n=60, \\sigma=0.01, L=1.2, d=5, \\text{seed}=4)$\n    - 用例 5: $(\\text{target id}=\\text{'sin'}, n=30, \\sigma=0.02, L=2.5, d=5, \\text{seed}=5)$\n- **确定性：** 每个用例的随机数生成都使用种子。所有三角函数参数均以弧度为单位。\n\n**验证结论：**\n该问题是**有效的**。它在科学上基于标准的统计学习理论，是良构的、客观的且自洽的。所指定的方法是标准的（或有明确定义的），并且所有参数都已提供，以实现确定性的、可复现的计算。该问题要求实现一个凸优化问题（一个 QP）、一个标准的线性回归以及一个特定的预测和评估协议，这是一项不简单但可行的任务。\n\n### 第 2 步：解决方案设计\n\n解决方案将是一个 Python 脚本，它会遍历五个指定的测试用例。对于每个用例，它将执行以下步骤：\n\n1.  **数据生成：** 用特定于用例的种子初始化伪随机数生成器。从 $[0,1]$ 上的均匀分布中采样 $n$ 个训练点 $x_i$ 并进行排序。使用指定的目标函数 $g(x)$ 和噪声标准差 $\\sigma$ 生成相应的带噪声的观测值 $y_i$。同时创建一个包含 $m=200$ 个点的均匀测试网格。\n\n2.  **利普希茨约束回归：**\n    a.  **优化：** 求解一个二次规划问题，以找到在训练点 $x_{(i)}$ 处的最优函数值 $\\mathbf{f} = (f_1, \\dots, f_n)$。目标函数为 $\\frac{1}{2} \\|\\mathbf{f} - \\mathbf{y}\\|^2_2$，约束是线性的：$|f_{i+1} - f_i| \\le L(x_{(i+1)} - x_{(i)})$。这将使用 `scipy.optimize.minimize` 和 `SLSQP` 方法实现。为提高效率和准确性，将为目标函数和约束提供解析梯度（雅可比矩阵）。\n    b.  **预测：** 使用 McShane 扩展公式 $\\hat{f}(x) = \\min_{i} \\{f_i + L|x - x_{(i)}|\\}$ 在测试网格上进行预测。这将使用向量化的 NumPy 操作实现。\n\n3.  **多项式回归基线：**\n    a.  **拟合：** 执行普通最小二乘 (OLS) 回归来拟合一个 $d$ 次多项式。从训练输入 $x_{(i)}$ 构建一个范德蒙矩阵，并使用 `numpy.linalg.lstsq` 找到多项式系数。\n    b.  **预测：** 在测试网格点上对拟合后的多项式进行求值以获得预测结果。\n\n4.  **评估：**\n    对于两种模型，都计算其在测试网格上相对于真实的、无噪声的目标函数值的均方误差 (MSE)。该用例的最终得分 $\\Delta$ 是多项式模型的 MSE 与利普希茨模型的 MSE 之差。\n\n5.  **输出：** 将五个用例计算出的 $\\Delta$ 值收集起来，格式化为六位小数，并以单行列表的形式打印。\n\n此设计严格遵守问题规范，确保结果的确定性和正确性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef solve():\n    \"\"\"\n    Implements and compares Lipschitz-constrained regression against polynomial OLS regression\n    based on the problem specification.\n    \"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        # (target_id, n, sigma, L, d, seed)\n        ('sin', 12, 0.05, 3.2, 5, 1),\n        ('quad', 8, 0.15, 2.0, 5, 2),\n        ('tanh', 25, 0.05, 1.2, 5, 3),\n        ('tanh', 60, 0.01, 1.2, 5, 4),\n        ('sin', 30, 0.02, 2.5, 5, 5),\n    ]\n\n    # Target functions as defined in the problem. Arguments are in radians for trig functions.\n    targets = {\n        'sin': lambda x: 0.5 * np.sin(2 * np.pi * x),\n        'quad': lambda x: x**2,\n        'tanh': lambda x: 0.5 + 0.3 * np.tanh(4 * (x - 0.5))\n    }\n\n    m = 200\n    x_test = np.linspace(0, 1, m)\n    results = []\n\n    for target_id, n, sigma, L, d, seed in test_cases:\n        target_func = targets[target_id]\n\n        # 1. Generate Data\n        # Initialize RNG with the specified seed for determinism.\n        rng = np.random.default_rng(seed)\n        \n        # Sample training inputs from U[0,1] and sort them.\n        x_train = rng.uniform(0, 1, size=n)\n        x_train.sort()\n        \n        # Generate noisy observations corresponding to the sorted inputs.\n        g_train = target_func(x_train)\n        noise = rng.normal(0, sigma, size=n)\n        y_train = g_train + noise\n\n        # Compute noise-free target values on the test grid.\n        g_test = target_func(x_test)\n\n        # 2. Lipschitz-constrained Model\n        # 2.1. Solve the Quadratic Program\n        # The objective is to minimize 0.5 * ||f - y_train||^2.\n        f_obj = lambda f: 0.5 * np.sum((f - y_train)**2)\n        f_jac = lambda f: f - y_train\n\n        # The constraints are |f_{i+1} - f_i| = L * (x_{i+1} - x_i), which are\n        # linear inequalities. We provide them to the solver in the form g(x) >= 0.\n        # This is equivalent to:\n        # L*(x_{i+1}-x_i) - (f_{i+1}-f_i) >= 0\n        # L*(x_{i+1}-x_i) + (f_{i+1}-f_i) >= 0\n        \n        # Construct the (n-1)x(n) finite difference matrix D for f[1:]-f[:-1].\n        D_matrix = np.zeros((n - 1, n))\n        rows = np.arange(n - 1)\n        D_matrix[rows, rows] = -1\n        D_matrix[rows, rows + 1] = 1\n\n        diff_x = x_train[1:] - x_train[:-1]\n        \n        # The constraint function returns a vector of all 2*(n-1) constraint values.\n        cons_fun = lambda f: np.concatenate([\n            L * diff_x - (D_matrix @ f),\n            L * diff_x + (D_matrix @ f)\n        ])\n        \n        # The Jacobian of the constraint function is a constant matrix.\n        cons_jac_mat = np.vstack([-D_matrix, D_matrix])\n        cons_jac = lambda f: cons_jac_mat\n        \n        constraints = [{'type': 'ineq', 'fun': cons_fun, 'jac': cons_jac}]\n\n        # Use the noisy observations as the initial guess for the optimizer.\n        f_initial = y_train.copy()\n        \n        opt_res = optimize.minimize(\n            f_obj, f_initial, method='SLSQP', jac=f_jac, constraints=constraints)\n        \n        f_hat_train = opt_res.x\n\n        # 2.2. Predict on test set using McShane extension\n        # f_hat(x) = min_i {f_i + L*|x - x_i|}\n        y_pred_lip = np.min(f_hat_train + L * np.abs(x_test[:, np.newaxis] - x_train), axis=1)\n\n        # 3. Polynomial Baseline Model (Ordinary Least Squares)\n        # Create Vandermonde matrix for features {1, x, ..., x^d}.\n        X_train = np.vander(x_train, d + 1, increasing=True)\n        # Solve for polynomial coefficients.\n        coeffs, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n        \n        # Predict on the test grid.\n        X_test = np.vander(x_test, d + 1, increasing=True)\n        y_pred_poly = X_test @ coeffs\n\n        # 4. Evaluation\n        # Compute MSE for both models with respect to the noise-free target.\n        mse_lip = np.mean((y_pred_lip - g_test)**2)\n        mse_poly = np.mean((y_pred_poly - g_test)**2)\n        \n        # Calculate the signed improvement.\n        delta = mse_poly - mse_lip\n        results.append(delta)\n\n    # 5. Format and print output\n    # Format each result to exactly six decimal places as a string.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    # Print the list in the specified format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}