{
    "hands_on_practices": [
        {
            "introduction": "Many real-world problems require integer predictions, from counting objects in an image to predicting a star rating. This exercise challenges you to move beyond standard regression by exploring different strategies to enforce this integer constraint, such as rounding or using a learned probability distribution. By comparing these methods under various loss functions, you'll gain a deeper, practical understanding of how to make optimal decisions when model outputs must be discrete .",
            "id": "3170698",
            "problem": "Consider a statistical learning setting where the response (target) variable $Y$ is restricted to take integer values in a finite support $S = \\{0,1,\\ldots,K\\}$. For each test case, you are given: (i) a true conditional distribution $q$ over $S$ that governs $Y$, (ii) a learned discrete predictive distribution $p$ over $S$ produced by a model, and (iii) a real-valued regression output $\\hat{\\mu}$ from another model that ignores the integer constraint. Your task is to compare decision rules that respect the integer output constraint and evaluate their true expected loss under $q$.\n\nYou must build your program starting from the risk minimization principle: for any loss function $L(a,y)$ and any distribution $r$ over $S$, the expected loss (risk) of predicting $a \\in S$ is\n$$\n\\mathcal{R}_r(a) = \\mathbb{E}_{Y \\sim r}[L(a,Y)] = \\sum_{y \\in S} r(y)\\,L(a,y).\n$$\nA Bayes-optimal decision with respect to $r$ is any $a^\\star \\in \\arg\\min_{a \\in S} \\mathcal{R}_r(a)$. Ties must be resolved by choosing the smallest feasible integer.\n\nYou must implement and evaluate the following five strategies that produce an integer-valued prediction constrained to $S$:\n- Strategy $0$ (nearest rounding): Predict $R_{\\text{near}}(\\hat{\\mu}) = \\left\\lfloor \\hat{\\mu} + 0.5 \\right\\rfloor$ and then clip to $S$ by replacing any value below $0$ by $0$ and any value above $K$ by $K$.\n- Strategy $1$ (floor): Predict $R_{\\lfloor \\cdot \\rfloor}(\\hat{\\mu}) = \\left\\lfloor \\hat{\\mu} \\right\\rfloor$ and clip to $S$.\n- Strategy $2$ (ceiling): Predict $R_{\\lceil \\cdot \\rceil}(\\hat{\\mu}) = \\left\\lceil \\hat{\\mu} \\right\\rceil$ and clip to $S$.\n- Strategy $3$ (stochastic rounding): If $\\hat{\\mu} \\in [0,K]$ and $\\hat{\\mu} \\notin \\mathbb{Z}$, write $\\hat{\\mu} = k + \\alpha$ with $k \\in \\{0,\\ldots,K-1\\}$ and $\\alpha \\in (0,1)$. Predict $k$ with probability $1-\\alpha$ and $k+1$ with probability $\\alpha$. If $\\hat{\\mu} \\in \\mathbb{Z}$, predict that integer. If $\\hat{\\mu}  0$, predict $0$; if $\\hat{\\mu}  K$, predict $K$. For evaluation, compute the expected risk under $q$ by averaging the risk of each possible rounded value with its rounding probability.\n- Strategy $4$ (learned-distribution decision): Choose $a^\\star \\in \\arg\\min_{a \\in S} \\mathcal{R}_p(a)$, that is, minimize expected loss under the learned distribution $p$, break ties by the smallest $a$, then evaluate its true expected loss under $q$.\n\nEvaluate each strategy under three losses:\n- Squared loss $L_2(a,y) = (a - y)^2$.\n- Absolute loss $L_1(a,y) = |a - y|$.\n- Asymmetric absolute loss (pinball loss) with parameter $\\tau \\in (0,1)$:\n$$\nL_\\tau(a,y) = \\tau \\max(a - y, 0) + (1 - \\tau) \\max(y - a, 0).\n$$\nUse $\\tau = 0.3$ for all test cases.\n\nFor each test case and each loss, compute the true expected loss under $q$ for all strategies $0$ through $4$, and return the index of the strategy that achieves the minimal true expected loss (breaking ties by the smallest index). The final output for each test case must be a list of three integers $[i_{2}, i_{1}, i_{\\tau}]$, where $i_{2}$ is the index minimizing the squared loss, $i_{1}$ is the index minimizing the absolute loss, and $i_{\\tau}$ is the index minimizing the asymmetric loss with $\\tau = 0.3$.\n\nYour program must use the following fixed test suite. In all cases, the support is $S = \\{0,1,2,3,4\\}$, so $K = 4$.\n- Test case $1$: $q = [0.05, 0.15, 0.50, 0.20, 0.10]$, $p = [0.10, 0.20, 0.40, 0.20, 0.10]$, $\\hat{\\mu} = 2.2$.\n- Test case $2$: $q = [0.00, 0.50, 0.50, 0.00, 0.00]$, $p = [0.05, 0.45, 0.45, 0.05, 0.00]$, $\\hat{\\mu} = 1.5$.\n- Test case $3$: $q = [0.05, 0.05, 0.10, 0.20, 0.60]$, $p = [0.20, 0.20, 0.20, 0.20, 0.20]$, $\\hat{\\mu} = 3.8$.\n- Test case $4$: $q = [0.45, 0.05, 0.00, 0.05, 0.45]$, $p = [0.05, 0.10, 0.70, 0.10, 0.05]$, $\\hat{\\mu} = 2.0$.\n- Test case $5$: $q = [0.70, 0.20, 0.05, 0.03, 0.02]$, $p = [0.65, 0.20, 0.10, 0.03, 0.02]$, $\\hat{\\mu} = -0.3$.\n- Test case $6$: $q = [0.02, 0.03, 0.05, 0.20, 0.70]$, $p = [0.05, 0.10, 0.15, 0.20, 0.50]$, $\\hat{\\mu} = 5.1$.\n\nAngle or physical units are not involved. All probabilities are given as decimals and sum to $1$ for each distribution.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The line must be a Python-style list of six sublists, each sublist corresponding to one test case in the same order as above, and each sublist containing the three integers $[i_{2}, i_{1}, i_{\\tau}]$. For example, a valid output would look like $[[0,1,4],[\\ldots],\\ldots]$ but with the actual computed integers for this problem.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in statistical decision theory, self-contained, well-posed, objective, and its components are formally and unambiguously defined. All necessary data and constraints are provided.\n\nThe core of the problem is to evaluate five distinct strategies for producing an integer prediction for a response variable $Y$ with a finite integer support $S = \\{0, 1, \\ldots, K\\}$. The evaluation is based on the principle of risk minimization. The risk, or expected loss, of a prediction (action) $a$ with respect to a probability distribution $r$ over $S$ is given by:\n$$\n\\mathcal{R}_r(a) = \\mathbb{E}_{Y \\sim r}[L(a,Y)] = \\sum_{y=0}^{K} r(y)\\,L(a,y)\n$$\nwhere $L(a,y)$ is a specified loss function. We will compute the true expected loss, meaning the expectation is taken with respect to the true data-generating distribution $q$.\n\nThe support for all test cases is $S = \\{0, 1, 2, 3, 4\\}$, corresponding to $K=4$. We will evaluate the strategies under three loss functions:\n1.  **Squared Loss ($L_2$)**: $L_2(a,y) = (a - y)^2$. The Bayes-optimal predictor for this loss is the expected value of the distribution, $\\mathbb{E}[Y]$.\n2.  **Absolute Loss ($L_1$)**: $L_1(a,y) = |a - y|$. The Bayes-optimal predictor for this loss is the median of the distribution.\n3.  **Asymmetric Absolute Loss ($L_\\tau$)**: $L_\\tau(a,y) = \\tau \\max(a - y, 0) + (1 - \\tau) \\max(y - a, 0)$, with parameter $\\tau = 0.3$. The Bayes-optimal predictor for this loss is the $\\tau$-quantile of the distribution.\n\nFor each test case, we are given a true distribution $q$, a learned distribution $p$, and a real-valued regression output $\\hat{\\mu}$. We will calculate the prediction for each of the five strategies and then compute its true risk $\\mathcal{R}_q(\\cdot)$ under each of the three loss functions.\n\nThe five strategies are as follows:\n\n**Strategy $0$: Nearest Rounding**\nThe prediction $a_0$ is obtained by rounding $\\hat{\\mu}$ to the nearest integer, then clipping the result to the valid range $[0, K]$.\n$$\na_0 = \\text{clip}(\\lfloor \\hat{\\mu} + 0.5 \\rfloor, 0, K)\n$$\nThis is a deterministic strategy, yielding a single integer prediction.\n\n**Strategy $1$: Floor**\nThe prediction $a_1$ is obtained by taking the floor of $\\hat{\\mu}$, then clipping.\n$$\na_1 = \\text{clip}(\\lfloor \\hat{\\mu} \\rfloor, 0, K)\n$$\nThis is also a deterministic strategy.\n\n**Strategy $2$: Ceiling**\nThe prediction $a_2$ is obtained by taking the ceiling of $\\hat{\\mu}$, then clipping.\n$$\na_2 = \\text{clip}(\\lceil \\hat{\\mu} \\rceil, 0, K)\n$$\nThis is also a deterministic strategy.\n\n**Strategy $3$: Stochastic Rounding**\nThis strategy's prediction can be probabilistic.\n- If $\\hat{\\mu}  0$, the prediction is $0$.\n- If $\\hat{\\mu}  K$, the prediction is $K$.\n- If $\\hat{\\mu} \\in [0, K]$ is an integer, the prediction is $\\hat{\\mu}$.\n- If $\\hat{\\mu} \\in (0, K)$ is not an integer, let $\\hat{\\mu} = k + \\alpha$ where $k = \\lfloor\\hat{\\mu}\\rfloor$ is an integer and $\\alpha \\in (0,1)$ is the fractional part. The strategy predicts $k$ with probability $1-\\alpha$ and $k+1$ with probability $\\alpha$.\n\nFor deterministic outcomes of this strategy, the true risk is $\\mathcal{R}_q(a_3)$. For the stochastic case, the evaluated quantity is the expected true risk, averaged over the rounding probabilities:\n$$\n\\mathbb{E}[\\mathcal{R}_q(a_3)] = (1-\\alpha) \\mathcal{R}_q(k) + \\alpha \\mathcal{R}_q(k+1)\n$$\n\n**Strategy $4$: Learned-Distribution Decision**\nThis strategy uses the learned distribution $p$ to find a Bayes-optimal prediction. The prediction $a_4$ is chosen to minimize the expected loss under $p$:\n$$\na_4 = \\arg\\min_{a \\in S} \\mathcal{R}_p(a) = \\arg\\min_{a \\in S} \\sum_{y=0}^{K} p(y) L(a,y)\n$$\nTies are broken by choosing the smallest integer $a$. It is crucial to note that the prediction $a_4$ depends on the loss function being considered. Thus, we will derive a different $a_4$ for each of $L_2$, $L_1$, and $L_\\tau$. Once $a_4$ is determined for a given loss, its performance is evaluated by computing the true risk $\\mathcal{R}_q(a_4)$.\n\n**Evaluation**\nFor each loss function ($L_2$, $L_1$, $L_\\tau$), we will produce a vector of five risk values, one for each strategy.\n$$\n[\\mathcal{R}_q(a_0), \\mathcal{R}_q(a_1), \\mathcal{R}_q(a_2), \\mathbb{E}[\\mathcal{R}_q(a_3)], \\mathcal{R}_q(a_4)]\n$$\nWe then find the index of the minimum value in this vector. In case of a tie, the smallest index wins. For each test case, this process yields three indices: $i_2$ for squared loss, $i_1$ for absolute loss, and $i_\\tau$ for asymmetric loss. The final output is a list of these three indices $[i_2, i_1, i_\\tau]$. This procedure is repeated for all provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the statistical learning problem by evaluating five decision strategies\n    under three different loss functions for a series of test cases.\n    \"\"\"\n    \n    # Problem Constants\n    K = 4\n    S_arr = np.arange(K + 1)\n    TAU = 0.3\n\n    # Test cases from the problem statement\n    test_cases = [\n        # (q, p, mu_hat)\n        (np.array([0.05, 0.15, 0.50, 0.20, 0.10]), np.array([0.10, 0.20, 0.40, 0.20, 0.10]), 2.2),\n        (np.array([0.00, 0.50, 0.50, 0.00, 0.00]), np.array([0.05, 0.45, 0.45, 0.05, 0.00]), 1.5),\n        (np.array([0.05, 0.05, 0.10, 0.20, 0.60]), np.array([0.20, 0.20, 0.20, 0.20, 0.20]), 3.8),\n        (np.array([0.45, 0.05, 0.00, 0.05, 0.45]), np.array([0.05, 0.10, 0.70, 0.10, 0.05]), 2.0),\n        (np.array([0.70, 0.20, 0.05, 0.03, 0.02]), np.array([0.65, 0.20, 0.10, 0.03, 0.02]), -0.3),\n        (np.array([0.02, 0.03, 0.05, 0.20, 0.70]), np.array([0.05, 0.10, 0.15, 0.20, 0.50]), 5.1),\n    ]\n\n    # --- Loss Functions ---\n    def loss_l2(a, y):\n        return (a - y) ** 2\n\n    def loss_l1(a, y):\n        return np.abs(a - y)\n\n    def loss_ltau(a, y, tau):\n        return tau * np.maximum(a - y, 0) + (1 - tau) * np.maximum(y - a, 0)\n\n    loss_functions = [loss_l2, loss_l1, loss_ltau]\n\n    def calculate_risk(a, dist, loss_func, S):\n        \"\"\"Calculates the expected loss (risk) for a given action and distribution.\"\"\"\n        if loss_func == loss_ltau:\n            losses = loss_func(a, S, TAU)\n        else:\n            losses = loss_func(a, S)\n        return np.sum(dist * losses)\n\n    all_results = []\n    for q, p, mu_hat in test_cases:\n        \n        # --- Generate Predictions for Strategies 0, 1, 2 ---\n        a0 = int(np.clip(np.floor(mu_hat + 0.5), 0, K))\n        a1 = int(np.clip(np.floor(mu_hat), 0, K))\n        a2 = int(np.clip(np.ceil(mu_hat), 0, K))\n\n        case_best_indices = []\n        for loss_func in loss_functions:\n            risks_for_loss = []\n\n            # --- Evaluate Strategies 0, 1, 2 ---\n            risks_for_loss.append(calculate_risk(a0, q, loss_func, S_arr))\n            risks_for_loss.append(calculate_risk(a1, q, loss_func, S_arr))\n            risks_for_loss.append(calculate_risk(a2, q, loss_func, S_arr))\n\n            # --- Evaluate Strategy 3 (Stochastic Rounding) ---\n            risk_s3 = 0.0\n            if mu_hat  0:\n                risk_s3 = calculate_risk(0, q, loss_func, S_arr)\n            elif mu_hat  K:\n                risk_s3 = calculate_risk(K, q, loss_func, S_arr)\n            elif mu_hat == int(mu_hat):\n                risk_s3 = calculate_risk(int(mu_hat), q, loss_func, S_arr)\n            else: # Stochastic case\n                k = int(np.floor(mu_hat))\n                alpha = mu_hat - k\n                risk_k = calculate_risk(k, q, loss_func, S_arr)\n                risk_k1 = calculate_risk(k + 1, q, loss_func, S_arr)\n                risk_s3 = (1 - alpha) * risk_k + alpha * risk_k1\n            risks_for_loss.append(risk_s3)\n\n            # --- Evaluate Strategy 4 (Learned-Distribution Decision) ---\n            # 1. Find the Bayes-optimal action 'a4' under the learned distribution 'p'\n            risks_under_p = [calculate_risk(a_candidate, p, loss_func, S_arr) for a_candidate in S_arr]\n            a4 = np.argmin(risks_under_p)\n            \n            # 2. Evaluate the true risk of 'a4' under the true distribution 'q'\n            risk_s4 = calculate_risk(a4, q, loss_func, S_arr)\n            risks_for_loss.append(risk_s4)\n\n            # --- Determine the best strategy for this loss function ---\n            best_strategy_index = np.argmin(risks_for_loss)\n            case_best_indices.append(best_strategy_index)\n        \n        all_results.append(case_best_indices)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A good classifier doesn't just predict the right class; it also provides trustworthy probabilities that reflect its confidence. This practice delves into the crucial concept of model calibration, guiding you through the calculation of the Expected Calibration Error (ECE) for a multiclass model. You will implement a calibration technique called vector scaling and measure its impact, learning how to diagnose and improve the reliability of your model's probabilistic outputs .",
            "id": "3170639",
            "problem": "A multiclass probabilistic classifier produces outputs for $K=3$ classes by applying the softmax function to logits. For each input $i \\in \\{1,2,3,4\\}$ (there are $N=4$ inputs), the pre-softmax logits are given by $z_{ik} = \\ln(n_{ik})$, where the $n_{ik}$ are positive integers listed below. The response variable $Y$ (the target class) for each input is also given.\n\n- Input $1$: $(n_{1,1}, n_{1,2}, n_{1,3}) = (4, 1, 1)$ and $Y_1 = 1$.\n- Input $2$: $(n_{2,1}, n_{2,2}, n_{2,3}) = (1, 3, 1)$ and $Y_2 = 2$.\n- Input $3$: $(n_{3,1}, n_{3,2}, n_{3,3}) = (1, 1, 5)$ and $Y_3 = 3$.\n- Input $4$: $(n_{4,1}, n_{4,2}, n_{4,3}) = (2, 2, 1)$ and $Y_4 = 2$.\n\nThe softmax outputs (the predicted class probabilities) for input $i$ are $p_{ik} = \\exp(z_{ik}) \\big/ \\sum_{j=1}^{3} \\exp(z_{ij})$ for $k \\in \\{1,2,3\\}$. Consider class-wise calibration using one-versus-rest reliability diagrams with two equal-width bins: $[0, 0.5]$ and $(0.5, 1]$. The class-wise expected calibration error (ECE) for class $k$ aggregates, over these bins, the weighted absolute difference between average predicted probability for class $k$ in the bin and the empirical frequency of $Y=k$ in the bin. The macro-average ECE is the simple average of the class-wise ECE values across the $3$ classes.\n\nNow consider a vector scaling calibration that maps logits to new logits via $z'_{ik} = a_k z_{ik} + b_k$, followed by a softmax to produce calibrated probabilities $p'_{ik} = \\exp(z'_{ik}) \\big/ \\sum_{j=1}^{3} \\exp(z'_{ij})$. Suppose these parameters are fixed as $a_1 = 1$, $a_2 = 1$, $a_3 = 1$, and $b_1 = \\ln(2)$, $b_2 = 0$, $b_3 = -\\ln(2)$.\n\nUsing only the foundational definitions described above (softmax probabilities, one-versus-rest binning, and class-wise expected calibration error), compute the macro-average class-wise ECE of the calibrated outputs $p'_{ik}$ under the two-bin scheme specified. Express your final answer as an exact fraction. No rounding is required.",
            "solution": "The problem statement has been validated and is deemed sound. It is self-contained, scientifically grounded in the principles of statistical learning, and well-posed, providing all necessary data and definitions for a unique solution.\n\nThe objective is to compute the macro-average class-wise Expected Calibration Error (ECE) for a set of calibrated probabilistic outputs. The calculation proceeds in four main steps:\n1.  Compute the calibrated logits $z'_{ik}$.\n2.  Compute the calibrated softmax probabilities $p'_{ik}$.\n3.  Compute the class-wise ECE, $\\text{ECE}_k$, for each of the $K=3$ classes.\n4.  Compute the macro-average ECE as the arithmetic mean of the class-wise ECEs.\n\nThe total number of inputs is $N=4$.\n\n**Step 1: Compute Calibrated Logits**\n\nThe original logits are given by $z_{ik} = \\ln(n_{ik})$. The calibration transformation is a vector scaling defined by $z'_{ik} = a_k z_{ik} + b_k$.\nGiven parameters are $a_1=1$, $a_2=1$, $a_3=1$, and $b_1 = \\ln(2)$, $b_2 = 0$, $b_3 = -\\ln(2)$.\n\nFor each class $k \\in \\{1, 2, 3\\}$ and any input $i$:\n$z'_{i1} = 1 \\cdot z_{i1} + \\ln(2) = \\ln(n_{i1}) + \\ln(2) = \\ln(2n_{i1})$\n$z'_{i2} = 1 \\cdot z_{i2} + 0 = \\ln(n_{i2})$\n$z'_{i3} = 1 \\cdot z_{i3} - \\ln(2) = \\ln(n_{i3}) + \\ln(\\frac{1}{2}) = \\ln(\\frac{n_{i3}}{2})$\n\n**Step 2: Compute Calibrated Probabilities**\n\nThe calibrated probabilities are given by the softmax function applied to the calibrated logits:\n$p'_{ik} = \\frac{\\exp(z'_{ik})}{\\sum_{j=1}^{3} \\exp(z'_{ij})}$\nUsing the results from Step 1, we have $\\exp(z'_{i1}) = 2n_{i1}$, $\\exp(z'_{i2}) = n_{i2}$, and $\\exp(z'_{i3}) = \\frac{n_{i3}}{2}$.\nThe denominator is $\\sum_{j=1}^{3} \\exp(z'_{ij}) = 2n_{i1} + n_{i2} + \\frac{n_{i3}}{2}$.\n\nWe now compute these probabilities for each of the $N=4$ inputs.\n\n- **Input 1**: $(n_{1,1}, n_{1,2}, n_{1,3}) = (4, 1, 1)$.\nDenominator = $2(4) + 1 + \\frac{1}{2} = 8 + 1 + 0.5 = 9.5 = \\frac{19}{2}$.\n$p'_{1,1} = \\frac{2(4)}{19/2} = \\frac{8}{19/2} = \\frac{16}{19}$.\n$p'_{1,2} = \\frac{1}{19/2} = \\frac{2}{19}$.\n$p'_{1,3} = \\frac{1/2}{19/2} = \\frac{1}{19}$.\n\n- **Input 2**: $(n_{2,1}, n_{2,2}, n_{2,3}) = (1, 3, 1)$.\nDenominator = $2(1) + 3 + \\frac{1}{2} = 2 + 3 + 0.5 = 5.5 = \\frac{11}{2}$.\n$p'_{2,1} = \\frac{2(1)}{11/2} = \\frac{4}{11}$.\n$p'_{2,2} = \\frac{3}{11/2} = \\frac{6}{11}$.\n$p'_{2,3} = \\frac{1/2}{11/2} = \\frac{1}{11}$.\n\n- **Input 3**: $(n_{3,1}, n_{3,2}, n_{3,3}) = (1, 1, 5)$.\nDenominator = $2(1) + 1 + \\frac{5}{2} = 2 + 1 + 2.5 = 5.5 = \\frac{11}{2}$.\n$p'_{3,1} = \\frac{2(1)}{11/2} = \\frac{4}{11}$.\n$p'_{3,2} = \\frac{1}{11/2} = \\frac{2}{11}$.\n$p'_{3,3} = \\frac{5/2}{11/2} = \\frac{5}{11}$.\n\n- **Input 4**: $(n_{4,1}, n_{4,2}, n_{4,3}) = (2, 2, 1)$.\nDenominator = $2(2) + 2 + \\frac{1}{2} = 4 + 2 + 0.5 = 6.5 = \\frac{13}{2}$.\n$p'_{4,1} = \\frac{2(2)}{13/2} = \\frac{8}{13}$.\n$p'_{4,2} = \\frac{2}{13/2} = \\frac{4}{13}$.\n$p'_{4,3} = \\frac{1/2}{13/2} = \\frac{1}{13}$.\n\n**Step 3: Compute Class-wise ECE**\n\nThe class-wise ECE for class $k$ is given by $\\text{ECE}_k = \\sum_{m=1}^{M} \\frac{|B_{m,k}|}{N} |\\text{acc}(B_{m,k}) - \\text{conf}(B_{m,k})|$, where $M=2$ is the number of bins, $B_{1,k}$ for probabilities in $[0, 0.5]$ and $B_{2,k}$ for probabilities in $(0.5, 1]$.\nThis can be simplified to $\\text{ECE}_k = \\frac{1}{N} \\sum_{m=1}^{M} \\left| \\sum_{i \\in B_{m,k}} (\\mathbb{I}(Y_i=k) - p'_{ik}) \\right|$, where $\\mathbb{I}(\\cdot)$ is the indicator function and the true labels are $Y_1=1, Y_2=2, Y_3=3, Y_4=2$.\n\n- **ECE for Class 1 ($k=1$)**:\nThe probabilities for class $1$ are $p'_{1,1}=\\frac{16}{19} \\approx 0.842$, $p'_{2,1}=\\frac{4}{11} \\approx 0.364$, $p'_{3,1}=\\frac{4}{11} \\approx 0.364$, $p'_{4,1}=\\frac{8}{13} \\approx 0.615$.\nThe true labels for class $1$ are $\\mathbb{I}(Y_1=1)=1, \\mathbb{I}(Y_2=1)=0, \\mathbb{I}(Y_3=1)=0, \\mathbb{I}(Y_4=1)=0$.\nBinning:\n$B_{1,1}$ (bin $[0, 0.5]$) contains inputs $2$ and $3$.\n$B_{2,1}$ (bin $(0.5, 1]$) contains inputs $1$ and $4$.\nThe sum of absolute differences for the bins are:\nFor $B_{1,1}$: $\\left| (\\mathbb{I}(Y_2=1) - p'_{2,1}) + (\\mathbb{I}(Y_3=1) - p'_{3,1}) \\right| = \\left| (0 - \\frac{4}{11}) + (0 - \\frac{4}{11}) \\right| = \\left| -\\frac{8}{11} \\right| = \\frac{8}{11}$.\nFor $B_{2,1}$: $\\left| (\\mathbb{I}(Y_1=1) - p'_{1,1}) + (\\mathbb{I}(Y_4=1) - p'_{4,1}) \\right| = \\left| (1 - \\frac{16}{19}) + (0 - \\frac{8}{13}) \\right| = \\left| \\frac{3}{19} - \\frac{8}{13} \\right| = \\left| \\frac{39 - 152}{247} \\right| = \\frac{113}{247}$.\n$\\text{ECE}_1 = \\frac{1}{4} \\left( \\frac{8}{11} + \\frac{113}{247} \\right) = \\frac{1}{4} \\left( \\frac{8 \\cdot 247 + 113 \\cdot 11}{11 \\cdot 247} \\right) = \\frac{1}{4} \\left( \\frac{1976 + 1243}{2717} \\right) = \\frac{3219}{10868}$.\n\n- **ECE for Class 2 ($k=2$)**:\nThe probabilities are $p'_{1,2}=\\frac{2}{19} \\approx 0.105$, $p'_{2,2}=\\frac{6}{11} \\approx 0.545$, $p'_{3,2}=\\frac{2}{11} \\approx 0.182$, $p'_{4,2}=\\frac{4}{13} \\approx 0.308$.\nThe true labels are $\\mathbb{I}(Y_1=2)=0, \\mathbb{I}(Y_2=2)=1, \\mathbb{I}(Y_3=2)=0, \\mathbb{I}(Y_4=2)=1$.\nBinning:\n$B_{1,2}$ (bin $[0, 0.5]$) contains inputs $1, 3, 4$.\n$B_{2,2}$ (bin $(0.5, 1]$) contains input $2$.\nThe sum of absolute differences for the bins are:\nFor $B_{1,2}$: $\\left| (0 - \\frac{2}{19}) + (0 - \\frac{2}{11}) + (1 - \\frac{4}{13}) \\right| = \\left| -\\frac{2}{19} - \\frac{2}{11} + \\frac{9}{13} \\right| = \\left| \\frac{-2(143) - 2(247) + 9(209)}{2717} \\right| = \\left| \\frac{-286 - 494 + 1881}{2717} \\right| = \\frac{1101}{2717}$.\nFor $B_{2,2}$: $\\left| (1 - \\frac{6}{11}) \\right| = \\left| \\frac{5}{11} \\right| = \\frac{5}{11}$.\n$\\text{ECE}_2 = \\frac{1}{4} \\left( \\frac{1101}{2717} + \\frac{5}{11} \\right) = \\frac{1}{4} \\left( \\frac{1101 + 5 \\cdot 247}{2717} \\right) = \\frac{1}{4} \\left( \\frac{1101 + 1235}{2717} \\right) = \\frac{2336}{10868}$.\n\n- **ECE for Class 3 ($k=3$)**:\nThe probabilities are $p'_{1,3}=\\frac{1}{19} \\approx 0.053$, $p'_{2,3}=\\frac{1}{11} \\approx 0.091$, $p'_{3,3}=\\frac{5}{11} \\approx 0.455$, $p'_{4,3}=\\frac{1}{13} \\approx 0.077$.\nThe true labels are $\\mathbb{I}(Y_1=3)=0, \\mathbb{I}(Y_2=3)=0, \\mathbb{I}(Y_3=3)=1, \\mathbb{I}(Y_4=3)=0$.\nBinning: All probabilities are less than $0.5$.\n$B_{1,3}$ (bin $[0, 0.5]$) contains all inputs $1, 2, 3, 4$.\n$B_{2,3}$ (bin $(0.5, 1]$) is empty.\nThe sum of absolute differences for the bins are:\nFor $B_{1,3}$: $\\left| (0-\\frac{1}{19}) + (0-\\frac{1}{11}) + (1-\\frac{5}{11}) + (0-\\frac{1}{13}) \\right| = \\left| -\\frac{1}{19} + (1 - \\frac{1}{11} - \\frac{5}{11}) - \\frac{1}{13} \\right| = \\left| -\\frac{1}{19} + \\frac{5}{11} - \\frac{1}{13} \\right| = \\left| \\frac{-143 + 5(247) - 209}{2717} \\right| = \\left| \\frac{-143 + 1235 - 209}{2717} \\right| = \\frac{883}{2717}$.\nFor $B_{2,3}$: The sum is $0$.\n$\\text{ECE}_3 = \\frac{1}{4} \\left( \\frac{883}{2717} + 0 \\right) = \\frac{883}{10868}$.\n\n**Step 4: Compute Macro-Average ECE**\n\nThe macro-average ECE is the mean of the class-wise ECEs.\nMacro-ECE $= \\frac{1}{3} (\\text{ECE}_1 + \\text{ECE}_2 + \\text{ECE}_3)$\n$= \\frac{1}{3} \\left( \\frac{3219}{10868} + \\frac{2336}{10868} + \\frac{883}{10868} \\right)$\n$= \\frac{1}{3} \\left( \\frac{3219 + 2336 + 883}{10868} \\right)$\n$= \\frac{1}{3} \\left( \\frac{6438}{10868} \\right) = \\frac{6438}{32604}$.\n\nTo simplify the fraction, we find common factors.\n$6438 = 6 \\times 1073$\n$32604 = 6 \\times 5434$\nThe fraction becomes $\\frac{1073}{5434}$.\nThe prime factorization of the numerator is $1073 = 29 \\times 37$.\nThe prime factorization of the denominator is $5434 = 2 \\times 2717 = 2 \\times 11 \\times 247 = 2 \\times 11 \\times 13 \\times 19$.\nThere are no common prime factors, so the fraction is in its simplest form.",
            "answer": "$$\\boxed{\\frac{1073}{5434}}$$"
        },
        {
            "introduction": "In many applications, a model's prediction must be robust to small, often imperceptible, changes in its input. This hands-on problem introduces the concept of certified robustness against adversarial perturbations for a linear classifier. You will derive and compute the 'certified radius'—a guaranteed safe zone around an input—to determine if the model's output is stable, providing a foundational understanding of how to build more reliable and secure machine learning systems .",
            "id": "3170624",
            "problem": "A binary linear classifier makes a prediction (model output) from an input vector, while a separate oracle defines the true target. An adversary perturbs the input within a bounded Euclidean radius without changing the true target. Your job is to formalize and compute whether the classifier’s output is certifiably robust to such perturbations using certified margins, and to detect when adversarial examples can shift the model output while the true target remains unchanged.\n\nFundamental base and definitions to use:\n- The input is a vector $\\mathbf{x} \\in \\mathbb{R}^d$.\n- The classifier is linear with parameters $\\mathbf{w} \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$, and outputs the sign $\\hat{y} = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\operatorname{sign}(t) = +1$ if $t  0$ and $-1$ if $t  0$.\n- The true target is given by a different oracle vector $\\mathbf{v} \\in \\mathbb{R}^d$ via $y^\\star = \\operatorname{sign}(\\mathbf{v}^\\top \\mathbf{x})$.\n- An adversary may add a perturbation $\\boldsymbol{\\delta}$ satisfying the Euclidean norm bound $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$, with $\\varepsilon \\ge 0$.\n- Use only the following well-tested facts as a starting point:\n  1. The Euclidean norm is $\\lVert \\mathbf{a} \\rVert_2 = \\sqrt{\\sum_i a_i^2}$.\n  2. The distance from a point $\\mathbf{x}$ to the hyperplane $\\{\\mathbf{z} : \\mathbf{a}^\\top \\mathbf{z} + c = 0\\}$ in Euclidean space equals $\\dfrac{\\lvert \\mathbf{a}^\\top \\mathbf{x} + c \\rvert}{\\lVert \\mathbf{a} \\rVert_2}$.\n  3. The Cauchy–Schwarz inequality: $\\lvert \\mathbf{u}^\\top \\mathbf{z} \\rvert \\le \\lVert \\mathbf{u} \\rVert_2 \\lVert \\mathbf{z} \\rVert_2$.\n\nTasks you must carry out and compute from first principles:\n1. Derive an expression for the certified radius (certified margin) of the classifier under $\\ell_2$ perturbations at $\\mathbf{x}$, defined as the minimum $\\ell_2$-norm of a perturbation needed to change the classifier’s sign. Express this purely in terms of $\\mathbf{w}$, $b$, and $\\mathbf{x}$.\n2. Derive a condition that ensures the true target $y^\\star$ does not change for any perturbation $\\boldsymbol{\\delta}$ with $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$. Express this in terms of the distance from $\\mathbf{x}$ to the oracle hyperplane $\\{\\mathbf{z} : \\mathbf{v}^\\top \\mathbf{z} = 0\\}$ and the bound $\\varepsilon$.\n3. Using your derivations, for each test case below, compute:\n   - A boolean $T$ indicating whether the true target is invariant for all allowed perturbations. Use strict inequality: declare invariant only if the distance to the oracle hyperplane is strictly greater than $\\varepsilon$.\n   - The certified radius $R$ of the classifier at $\\mathbf{x}$ under $\\ell_2$ perturbations.\n   - A boolean $C$ indicating whether the classifier is certifiably robust against all perturbations with $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$. Use strict inequality: declare certifiable robustness only if $R$ is strictly greater than $\\varepsilon$.\n   - A boolean $A$ indicating whether an adversarial example exists that changes the classifier’s output without changing the true target, i.e., $A$ is true if and only if $T$ is true and $C$ is false.\n\nTest suite (each case lists $(\\mathbf{w}, b, \\mathbf{v}, \\mathbf{x}, \\varepsilon)$ with dimension $d = 2$):\n- Case $1$: $\\mathbf{w} = (1, 0)$, $b = 0$, $\\mathbf{v} = (1, 0)$, $\\mathbf{x} = (2, 0)$, $\\varepsilon = 0.5$.\n- Case $2$: $\\mathbf{w} = (1, 0)$, $b = 0$, $\\mathbf{v} = (1, 0)$, $\\mathbf{x} = (0.6, 0)$, $\\varepsilon = 0.8$.\n- Case $3$: $\\mathbf{w} = (1, 0)$, $b = 0$, $\\mathbf{v} = (1, 0)$, $\\mathbf{x} = (1.0, 0)$, $\\varepsilon = 1.0$.\n- Case $4$: $\\mathbf{w} = (1, 0)$, $b = -2.5$, $\\mathbf{v} = (1, 0)$, $\\mathbf{x} = (3.0, 0)$, $\\varepsilon = 0.6$.\n- Case $5$: $\\mathbf{w} = (0, 1)$, $b = 0$, $\\mathbf{v} = (2, 0)$, $\\mathbf{x} = (1.5, 0.4)$, $\\varepsilon = 0.7$.\n- Case $6$: $\\mathbf{w} = (1, -1)$, $b = 0.1$, $\\mathbf{v} = (1, -1)$, $\\mathbf{x} = (10, 0)$, $\\varepsilon = 7.0$.\n\nNumerical and output requirements:\n- For each test case, output a list of the form $[T, R, C, A]$, where $T$, $C$, and $A$ are booleans, and $R$ is a floating-point number rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of these lists, enclosed in square brackets, for example: $[[\\dots],[\\dots],\\dots]$.\n- There are no physical units or angles in this problem.",
            "solution": "The problem requires the formal analysis of a binary linear classifier's robustness to adversarial perturbations. We must derive expressions for the certified radius and the stability of an oracle-defined true target, and then apply these to a series of test cases.\n\nLet us begin by establishing the mathematical framework from first principles.\n\n**1. Certified Radius of the Classifier**\n\nThe classifier's output for an input $\\mathbf{x} \\in \\mathbb{R}^d$ is given by $\\hat{y} = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$, where $\\mathbf{w} \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are the model parameters. An adversarial perturbation $\\boldsymbol{\\delta}$ is applied, such that the new input is $\\mathbf{x}' = \\mathbf{x} + \\boldsymbol{\\delta}$ and the perturbation is bounded by $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$.\n\nThe classifier's output is flipped if the sign of its decision function, $f(\\mathbf{z}) = \\mathbf{w}^\\top \\mathbf{z} + b$, changes. That is, $\\operatorname{sign}(f(\\mathbf{x}')) \\neq \\operatorname{sign}(f(\\mathbf{x}))$. The smallest perturbation required to achieve this will move the point $\\mathbf{x}$ exactly onto the classifier's decision boundary hyperplane, which is the set of points $\\mathbf{z}$ where $f(\\mathbf{z}) = 0$.\n\nTherefore, we seek to find the perturbation $\\boldsymbol{\\delta}$ with the minimum Euclidean norm, $\\lVert \\boldsymbol{\\delta} \\rVert_2$, that satisfies the condition:\n$$ \\mathbf{w}^\\top (\\mathbf{x} + \\boldsymbol{\\delta}) + b = 0 $$\nRearranging this equation, we get:\n$$ \\mathbf{w}^\\top \\boldsymbol{\\delta} = -(\\mathbf{w}^\\top \\mathbf{x} + b) $$\nThe certified radius, denoted by $R$, is the minimum value of $\\lVert \\boldsymbol{\\delta} \\rVert_2$ that satisfies this constraint. This is a classic optimization problem: minimize $\\lVert \\boldsymbol{\\delta} \\rVert_2$ subject to $\\mathbf{w}^\\top \\boldsymbol{\\delta} = c$, where $c = -(\\mathbf{w}^\\top \\mathbf{x} + b)$.\n\nFrom the Cauchy–Schwarz inequality, we have $\\lvert \\mathbf{w}^\\top \\boldsymbol{\\delta} \\rvert \\le \\lVert \\mathbf{w} \\rVert_2 \\lVert \\boldsymbol{\\delta} \\rVert_2$. To satisfy the constraint with minimal $\\lVert \\boldsymbol{\\delta} \\rVert_2$, this inequality must be tight (an equality), which occurs when $\\boldsymbol{\\delta}$ is collinear with $\\mathbf{w}$. Thus, the optimal perturbation is in the direction of $-\\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b) \\cdot \\mathbf{w}$.\n\nSolving for $\\lVert \\boldsymbol{\\delta} \\rVert_2$:\n$$ \\lVert \\boldsymbol{\\delta} \\rVert_2 \\ge \\frac{\\lvert \\mathbf{w}^\\top \\boldsymbol{\\delta} \\rvert}{\\lVert \\mathbf{w} \\rVert_2} $$\nSubstituting the constraint $\\mathbf{w}^\\top \\boldsymbol{\\delta} = -(\\mathbf{w}^\\top \\mathbf{x} + b)$:\n$$ \\lVert \\boldsymbol{\\delta} \\rVert_2 \\ge \\frac{\\lvert -(\\mathbf{w}^\\top \\mathbf{x} + b) \\rvert}{\\lVert \\mathbf{w} \\rVert_2} = \\frac{\\lvert \\mathbf{w}^\\top \\mathbf{x} + b \\rvert}{\\lVert \\mathbf{w} \\rVert_2} $$\nThe minimum norm is the lower bound. Thus, the certified radius $R$ is:\n$$ R = \\frac{\\lvert \\mathbf{w}^\\top \\mathbf{x} + b \\rvert}{\\lVert \\mathbf{w} \\rVert_2} $$\nThis expression corresponds to the given definition of the Euclidean distance from a point $\\mathbf{x}$ to the hyperplane $\\{\\mathbf{z} : \\mathbf{w}^\\top \\mathbf{z} + b = 0\\}$.\n\n**2. Invariance of the True Target**\n\nThe true target is given by an oracle as $y^\\star = \\operatorname{sign}(\\mathbf{v}^\\top \\mathbf{x})$. For a perturbed input $\\mathbf{x} + \\boldsymbol{\\delta}$, the new target would be $\\operatorname{sign}(\\mathbf{v}^\\top (\\mathbf{x} + \\boldsymbol{\\delta}))$. The target is invariant if its sign does not change for any allowed perturbation $\\boldsymbol{\\delta}$ where $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$.\n\nThis requires that the quantity $\\mathbf{v}^\\top \\mathbf{x}$ and $\\mathbf{v}^\\top (\\mathbf{x} + \\boldsymbol{\\delta}) = \\mathbf{v}^\\top \\mathbf{x} + \\mathbf{v}^\\top \\boldsymbol{\\delta}$ have the same sign. This condition is guaranteed if the magnitude of the change, $\\lvert \\mathbf{v}^\\top \\boldsymbol{\\delta} \\rvert$, is strictly less than the magnitude of the original value, $\\lvert \\mathbf{v}^\\top \\mathbf{x} \\rvert$.\n$$ \\lvert \\mathbf{v}^\\top \\boldsymbol{\\delta} \\rvert  \\lvert \\mathbf{v}^\\top \\mathbf{x} \\rvert $$\nWe need this to hold for all $\\boldsymbol{\\delta}$ such that $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$. To find the most challenging perturbation, we must maximize the term $\\lvert \\mathbf{v}^\\top \\boldsymbol{\\delta} \\rvert$. According to the Cauchy–Schwarz inequality:\n$$ \\lvert \\mathbf{v}^\\top \\boldsymbol{\\delta} \\rvert \\le \\lVert \\mathbf{v} \\rVert_2 \\lVert \\boldsymbol{\\delta} \\rVert_2 $$\nSince $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$, the maximum value is $\\lVert \\mathbf{v} \\rVert_2 \\varepsilon$. This worst-case change occurs when $\\boldsymbol{\\delta}$ is aligned with $\\mathbf{v}$ and has magnitude $\\varepsilon$.\n\nThus, for the target to remain invariant under all possible perturbations, the worst-case change must not be sufficient to flip the sign:\n$$ \\lVert \\mathbf{v} \\rVert_2 \\varepsilon  \\lvert \\mathbf{v}^\\top \\mathbf{x} \\rvert $$\nAssuming $\\mathbf{v}$ is not the zero vector (which holds for all test cases, since $\\lVert \\mathbf{v} \\rVert_2  0$), we can rearrange this to:\n$$ \\varepsilon  \\frac{\\lvert \\mathbf{v}^\\top \\mathbf{x} \\rvert}{\\lVert \\mathbf{v} \\rVert_2} $$\nThe term on the right is the Euclidean distance from $\\mathbf{x}$ to the oracle's decision boundary, the hyperplane $\\{\\mathbf{z} : \\mathbf{v}^\\top \\mathbf{z} = 0\\}$. The condition for target invariance is that this distance must be strictly greater than the perturbation bound $\\varepsilon$.\n\n**3. Computation of Per-Case Metrics**\n\nBased on the derivations above, we can now define the computational procedure for each test case.\n- **$T$ (Target Invariance):** This boolean is true if the true target $y^\\star$ is invariant for all perturbations with $\\lVert \\boldsymbol{\\delta} \\rVert_2 \\le \\varepsilon$. From our derivation, this corresponds to the condition:\n$$ T := \\left( \\frac{\\lvert \\mathbf{v}^\\top \\mathbf{x} \\rvert}{\\lVert \\mathbf{v} \\rVert_2}  \\varepsilon \\right) $$\n- **$R$ (Certified Radius):** This is the value derived in the first part:\n$$ R := \\frac{\\lvert \\mathbf{w}^\\top \\mathbf{x} + b \\rvert}{\\lVert \\mathbf{w} \\rVert_2} $$\n- **$C$ (Certified Robustness):** This boolean is true if the classifier is certifiably robust for the given $\\varepsilon$. This means that the minimum perturbation needed to change the classification, $R$, is strictly greater than the maximum allowed perturbation, $\\varepsilon$.\n$$ C := (R  \\varepsilon) $$\n- **$A$ (Adversarial Example Existence):** An adversarial example exists if it is possible to change the classifier's prediction ($\\neg C$) while the true target remains unchanged ($T$). The problem defines this condition as $A$ being true if and only if $T$ is true and $C$ is false.\n$$ A := T \\land (\\neg C) $$\nThese formulas are implemented to process the provided test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes robustness metrics for a binary linear classifier under L2 perturbations.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (w, b, v, x, epsilon)\n    test_cases = [\n        # Case 1\n        (np.array([1.0, 0.0]), 0.0, np.array([1.0, 0.0]), np.array([2.0, 0.0]), 0.5),\n        # Case 2\n        (np.array([1.0, 0.0]), 0.0, np.array([1.0, 0.0]), np.array([0.6, 0.0]), 0.8),\n        # Case 3\n        (np.array([1.0, 0.0]), 0.0, np.array([1.0, 0.0]), np.array([1.0, 0.0]), 1.0),\n        # Case 4\n        (np.array([1.0, 0.0]), -2.5, np.array([1.0, 0.0]), np.array([3.0, 0.0]), 0.6),\n        # Case 5\n        (np.array([0.0, 1.0]), 0.0, np.array([2.0, 0.0]), np.array([1.5, 0.4]), 0.7),\n        # Case 6\n        (np.array([1.0, -1.0]), 0.1, np.array([1.0, -1.0]), np.array([10.0, 0.0]), 7.0),\n    ]\n\n    def compute_metrics(w, b, v, x, epsilon):\n        \"\"\"\n        Calculates the metrics T, R, C, A for a single test case.\n        \"\"\"\n        \n        # Calculate T: Target Invariance\n        # Condition: distance to oracle hyperplane  epsilon\n        # Distance = |v.T * x| / ||v||_2\n        norm_v = np.linalg.norm(v, 2)\n        if norm_v == 0:\n            # Oracle is undefined, but problem constraints ensure norm_v  0\n            dist_to_oracle_hyperplane = np.inf\n        else:\n            dist_to_oracle_hyperplane = np.abs(np.dot(v, x)) / norm_v\n        \n        T = dist_to_oracle_hyperplane  epsilon\n\n        # Calculate R: Certified Radius\n        # R = |w.T * x + b| / ||w||_2\n        norm_w = np.linalg.norm(w, 2)\n        if norm_w == 0:\n            # Classifier is undefined, but constraints ensure norm_w  0\n            R = np.inf\n        else:\n            R = np.abs(np.dot(w, x) + b) / norm_w\n\n        # Calculate C: Certified Robustness\n        # Condition: R  epsilon\n        C = R  epsilon\n        \n        # Calculate A: Adversarial Example Existence\n        # Condition: T is true and C is false\n        A = T and not C\n        \n        return [T, R, C, A]\n\n    results_as_lists = []\n    for case in test_cases:\n        w_vec, b_scalar, v_vec, x_vec, eps_scalar = case\n        result = compute_metrics(w_vec, b_scalar, v_vec, x_vec, eps_scalar)\n        results_as_lists.append(result)\n    \n    # Format the results into the required string format\n    # Example: [[True,2.000000,True,False],[...]]\n    formatted_sublists = []\n    for res in results_as_lists:\n        T, R, C, A = res\n        # Format R to 6 decimal places, and convert booleans to 'True'/'False' strings\n        sublist_str = f\"[{str(T)},{R:.6f},{str(C)},{str(A)}]\"\n        formatted_sublists.append(sublist_str)\n        \n    final_output = f\"[{','.join(formatted_sublists)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}