{
    "hands_on_practices": [
        {
            "introduction": "At the heart of the parametric versus non-parametric debate lie the assumptions we are willing to make about our data. This practice provides a foundational, theoretical comparison of these two philosophies through the lens of the bootstrap, a powerful method for estimating uncertainty. By comparing a parametric bootstrap that assumes a specific data-generating distribution with a non-parametric bootstrap that resamples directly from the data, you will derive how these fundamentally different starting points lead to distinct estimates of uncertainty .",
            "id": "852032",
            "problem": "Consider a random sample $X_1, X_2, \\ldots, X_n$ of size $n$ drawn independently and identically from a geometric distribution. The probability mass function (PMF) for this distribution is given by $P(X=k) = p(1-p)^{k-1}$ for $k \\in \\{1, 2, 3, \\ldots\\}$, where $p$ is the probability of success. The statistic of interest is the sample mean, $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$.\n\nWe wish to estimate the standard error of the sample mean, $\\text{SE}(\\bar{X})$, using two different bootstrap methods:\n\n1.  **Non-parametric Bootstrap:** In this method, the standard error is estimated by resampling from the original data. The variance of the sample mean in a non-parametric bootstrap resampling scheme is given by $\\frac{1}{n}\\hat{\\sigma}_{mle}^2$, where $\\hat{\\sigma}_{mle}^2 = \\frac{1}{n}\\sum_{i=1}^n(X_i - \\bar{X})^2$ is the maximum likelihood estimate of the population variance. The non-parametric bootstrap estimate of the standard error is thus $\\hat{\\text{se}}_{\\text{NP}} = \\sqrt{\\frac{1}{n}\\hat{\\sigma}_{mle}^2}$.\n\n2.  **Parametric Bootstrap:** This method assumes the data follows a specific parametric model, in this case, the geometric distribution. The procedure involves first estimating the parameter $p$ from the data using the maximum likelihood estimator (MLE), which for the geometric distribution is $\\hat{p} = 1/\\bar{X}$. Then, the standard error is calculated using the theoretical formula for the standard error of the mean for a geometric distribution, but with the estimated parameter $\\hat{p}$ in place of the true parameter $p$.\n\nYour task is to derive the ratio of the expected value of the squared parametric bootstrap standard error estimator to the expected value of the squared non-parametric bootstrap standard error estimator. Specifically, you need to calculate:\n$$\nR = \\frac{E\\left[\\hat{\\text{se}}_{\\text{P}}^2\\right]}{E\\left[\\hat{\\text{se}}_{\\text{NP}}^2\\right]}\n$$\nwhere the expectation is taken with respect to the true underlying geometric distribution with parameter $p$. The final result should be a function of the sample size $n$ only.",
            "solution": "1. The parametric bootstrap estimator squared is\n$$\n\\hat{\\mathrm{se}}_P^2\n=\\frac{1-\\hat p}{\\hat p^2\\,n},\\quad \\hat p=\\frac1{\\bar X},\n$$\nso\n$$\n\\hat{\\mathrm{se}}_P^2\n=\\frac{1-\\frac1{\\bar X}}{\\frac1{\\bar X^2}\\,n}\n=\\frac{\\bar X(\\bar X-1)}{n}.\n$$\n2. The nonparametric bootstrap estimator squared is\n$$\n\\hat{\\mathrm{se}}_{NP}^2\n=\\frac1n\\hat\\sigma_{mle}^2\n=\\frac1{n^2}\\sum_{i=1}^n(X_i-\\bar X)^2.\n$$\n3. Compute expectations under $X_i\\sim\\mathrm{Geom}(p)$, $\\mathbb E[X_i]=1/p$, $\\mathrm{Var}(X_i)=(1-p)/p^2$.\n   \n   a. \n$$\n\\mathbb E[\\hat{\\mathrm{se}}_P^2]\n=\\frac1n\\Bigl(\\mathbb E[\\bar X^2]-\\mathbb E[\\bar X]\\Bigr)\n=\\frac1n\\Bigl(\\tfrac{1-p}{p^2n}+\\tfrac1{p^2}-\\tfrac1p\\Bigr)\n=\\frac{(n+1)(1-p)}{n^2p^2}.\n$$\n   b.\n$$\n\\mathbb E[\\hat{\\mathrm{se}}_{NP}^2]\n=\\frac1{n^2}\\,\\mathbb E\\Bigl[\\sum_{i=1}^n(X_i-\\bar X)^2\\Bigr]\n=\\frac1{n^2}(n-1)\\frac{1-p}{p^2}.\n$$\n4. Ratio:\n$$\nR=\\frac{\\mathbb E[\\hat{\\mathrm{se}}_P^2]}{\\mathbb E[\\hat{\\mathrm{se}}_{NP}^2]}\n=\\frac{\\frac{(n+1)(1-p)}{n^2p^2}}{\\frac{(n-1)(1-p)}{n^2p^2}}\n=\\frac{n+1}{n-1}.\n$$",
            "answer": "$$\\boxed{\\frac{n+1}{n-1}}$$"
        },
        {
            "introduction": "Moving from theory to practice, one of the most important distinctions between model families is their sensitivity to the scale of input features. In this hands-on coding exercise, you will investigate how a parametric linear regression model and a non-parametric, distance-based kernel regression model respond to feature rescaling. This comparison is not just academic; it highlights a crucial data preprocessing step and builds intuition for why understanding a model's inner workings is essential for achieving reliable performance .",
            "id": "3155821",
            "problem": "You will implement a principled comparison between a parametric method and a non-parametric method to study the effect of feature rescaling on interpretability and predictive performance. The parametric method is ordinary least squares (OLS) linear regression applied to standardized predictors, and the non-parametric method is the Nadaraya–Watson kernel regression with a Gaussian kernel and a fixed bandwidth. You must derive a testable performance drift metric with respect to rescaling of input features and implement it as a complete program.\n\nData generating process:\n- Let the random vector of predictors be $X = (X_{1}, X_{2})$, where $X_{1} \\sim \\mathrm{Uniform}[-2,2]$ and $X_{2} \\sim \\mathrm{Uniform}[-1,1]$, independent.\n- Let the noise be $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ independent of $X$, with $\\sigma = 0.5$.\n- Define the response by\n$$\nY \\;=\\; 3 X_{1} \\;-\\; 2 X_{2} \\;+\\; 0.3 \\sin(3 X_{1}) \\;+\\; \\varepsilon.\n$$\n\nSampling:\n- Generate a training sample of size $n_{\\text{train}} = 200$ and a test sample of size $n_{\\text{test}} = 100$ using a pseudorandom number generator with fixed seed $7$ so that results are deterministic.\n\nRescaling protocol:\n- For a scaling factor $s > 0$, define the rescaled predictors as $X^{(s)} = s \\cdot X$, applied componentwise to both training and test predictors.\n\nParametric method (standardized OLS):\n- For each $s$, compute the standardized predictors $Z^{(s)} = \\mathrm{standardize}(X^{(s)})$ using training-set statistics, where standardization is defined by $Z^{(s)}_{j} = \\frac{X^{(s)}_{j} - \\mu^{(s)}_{j,\\text{train}}}{\\sigma^{(s)}_{j,\\text{train}}}$ for each predictor index $j \\in \\{1,2\\}$, with $\\mu^{(s)}_{j,\\text{train}}$ and $\\sigma^{(s)}_{j,\\text{train}}$ being the sample mean and sample standard deviation computed on the rescaled training set. Fit an OLS model with intercept to predict $Y$ from $Z^{(s)}$. Let $\\widehat{\\beta}^{(s)} \\in \\mathbb{R}^{2}$ denote the vector of slope coefficients on the standardized predictors (excluding the intercept). Define the parametric coefficient drift relative to the baseline $s=1$ as\n$$\nd_{\\text{param}}(s) \\;=\\; \\big\\| \\widehat{\\beta}^{(s)} - \\widehat{\\beta}^{(1)} \\big\\|_{2}.\n$$\n\nNon-parametric method (Gaussian kernel regression):\n- For each $s$, fit a Nadaraya–Watson estimator on the rescaled training set without any standardization. Use an isotropic Gaussian kernel with fixed bandwidth $h = 0.5$. For a test point $x \\in \\mathbb{R}^{2}$, the prediction is\n$$\n\\widehat{m}^{(s)}(x) \\;=\\; \\frac{\\sum_{i=1}^{n_{\\text{train}}} \\exp\\!\\left(-\\frac{\\|x - x^{(s)}_{i}\\|_{2}^{2}}{2 h^{2}}\\right) \\, y_{i}}{\\sum_{i=1}^{n_{\\text{train}}} \\exp\\!\\left(-\\frac{\\|x - x^{(s)}_{i}\\|_{2}^{2}}{2 h^{2}}\\right)},\n$$\nwhere $x^{(s)}_{i}$ are the rescaled training predictors and $y_{i}$ are the corresponding training responses. If the denominator is smaller than a numerical threshold $\\tau = 10^{-12}$ at any test point, define $\\widehat{m}^{(s)}(x)$ to be the training response mean to ensure numerical stability. On the rescaled test set, compute the mean squared error (MSE)\n$$\n\\mathrm{MSE}_{\\text{kernel}}(s) \\;=\\; \\frac{1}{n_{\\text{test}}} \\sum_{j=1}^{n_{\\text{test}}} \\Big( y^{\\text{test}}_{j} - \\widehat{m}^{(s)}(x^{(s),\\text{test}}_{j}) \\Big)^{2}.\n$$\nDefine the non-parametric performance drift relative to the baseline $s=1$ as\n$$\nd_{\\text{kernel}}(s) \\;=\\; \\big| \\mathrm{MSE}_{\\text{kernel}}(s) - \\mathrm{MSE}_{\\text{kernel}}(1) \\big|.\n$$\n\nTest suite:\n- Use the scaling set $S = \\{0.5, 1.0, 2.0, 10.0\\}$.\n- For each $s \\in S$, compute $d_{\\text{param}}(s)$ and $d_{\\text{kernel}}(s)$ as defined above.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order corresponding to the elements of $S$:\n$$\n\\big[ d_{\\text{param}}(0.5), \\; d_{\\text{kernel}}(0.5), \\; d_{\\text{param}}(1.0), \\; d_{\\text{kernel}}(1.0), \\; d_{\\text{param}}(2.0), \\; d_{\\text{kernel}}(2.0), \\; d_{\\text{param}}(10.0), \\; d_{\\text{kernel}}(10.0) \\big].\n$$\nEach entry must be a real number (a float). No additional text should be printed.",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions and parameters for a well-defined computational experiment in statistical learning. The task is to compare the effect of feature scaling on a parametric model (Ordinary Least Squares on standardized data) and a non-parametric model (Nadaraya–Watson kernel regression). This will be accomplished by implementing the specified models and calculating drift metrics for their parameters and performance, respectively.\n\n### Theoretical Foundation\n\nThe core of this problem rests on a fundamental distinction between how parametric and non-parametric models handle input features.\n\n1.  **Parametric Model: Invariance to Scaling via Standardization**\n\n    The parametric method specified is Ordinary Least Squares (OLS) regression applied to standardized predictors. For a given predictor feature $X_j$ and a scaling factor $s > 0$, the rescaled feature is $X_j^{(s)} = s X_j$. The standardization process involves centering and scaling the feature using its sample mean $\\mu[ \\cdot ]$ and sample standard deviation $\\sigma[ \\cdot ]$, computed on the training data.\n\n    Let $\\mu_{j,\\text{train}} = \\mu[X_{j,\\text{train}}]$ and $\\sigma_{j,\\text{train}} = \\sigma[X_{j,\\text{train}}]$. The mean and standard deviation of the rescaled training data are:\n    $$\n    \\mu^{(s)}_{j,\\text{train}} = \\mu[s X_{j,\\text{train}}] = s \\cdot \\mu[X_{j,\\text{train}}] = s \\mu_{j,\\text{train}}\n    $$\n    $$\n    \\sigma^{(s)}_{j,\\text{train}} = \\sigma[s X_{j,\\text{train}}] = s \\cdot \\sigma[X_{j,\\text{train}}] = s \\sigma_{j,\\text{train}}\n    $$\n    These scaling properties are fundamental to the mean and standard deviation operators. The standardized predictor for the rescaled data, $Z^{(s)}_j$, is then:\n    $$\n    Z^{(s)}_j = \\frac{X^{(s)}_j - \\mu^{(s)}_{j,\\text{train}}}{\\sigma^{(s)}_{j,\\text{train}}} = \\frac{s X_j - s \\mu_{j,\\text{train}}}{s \\sigma_{j,\\text{train}}} = \\frac{s (X_j - \\mu_{j,\\text{train}})}{s \\sigma_{j,\\text{train}}} = \\frac{X_j - \\mu_{j,\\text{train}}}{\\sigma_{j,\\text{train}}} = Z^{(1)}_j\n    $$\n    This demonstrates that the standardized predictor $Z^{(s)}_j$ is mathematically identical for any scaling factor $s > 0$. Since the OLS model is fitted to predict the fixed response vector $Y$ from the design matrix of these standardized predictors, and this design matrix is invariant to $s$, the resulting vector of OLS slope coefficients $\\widehat{\\beta}^{(s)}$ must also be invariant. Consequently, the parametric coefficient drift, $d_{\\text{param}}(s) = \\| \\widehat{\\beta}^{(s)} - \\widehat{\\beta}^{(1)} \\|_{2}$, is theoretically expected to be exactly $0$ for all $s$. Any non-zero result would be attributable to floating-point precision limitations.\n\n2.  **Non-Parametric Model: Sensitivity to Scaling via Distance Metric**\n\n    The non-parametric method is the Nadaraya–Watson kernel estimator. Its prediction for a point $x$ depends on the weighted average of training responses $y_i$, where the weights are determined by a kernel function of the distance between $x$ and the training points $x_i$. The specified isotropic Gaussian kernel uses the Euclidean distance.\n\n    When the features are rescaled by a factor $s$, a test point $x$ becomes $s x$ and the training points $x_i$ become $s x_i$. The squared Euclidean distance term in the kernel's exponent becomes:\n    $$\n    \\| s x - s x_i \\|_2^2 = \\| s(x - x_i) \\|_2^2 = s^2 \\| x - x_i \\|_2^2\n    $$\n    The kernel function, with a fixed bandwidth $h$, is therefore affected by the scaling:\n    $$\n    K_h(s x, s x_i) = \\exp\\left(-\\frac{\\| s x - s x_i \\|_{2}^{2}}{2 h^{2}}\\right) = \\exp\\left(-\\frac{s^2 \\| x - x_i \\|_{2}^{2}}{2 h^{2}}\\right) = \\exp\\left(-\\frac{\\| x - x_i \\|_{2}^{2}}{2 (h/s)^{2}}\\right)\n    $$\n    This shows that scaling the data by a factor $s$ is equivalent to using the original, unscaled data with an effective bandwidth of $h_{\\text{eff}} = h/s$. The bandwidth parameter $h$ critically controls the bias-variance trade-off of the estimator. A smaller bandwidth leads to a more flexible (low bias, high variance) fit, while a larger bandwidth results in a smoother (high bias, low variance) fit. Since the scaling factor $s$ directly modulates the effective bandwidth, it alters the model's complexity and its ability to fit the underlying function. This will, in general, change the model's predictive performance on the test set, as measured by the Mean Squared Error (MSE). Therefore, the non-parametric performance drift, $d_{\\text{kernel}}(s) = | \\mathrm{MSE}_{\\text{kernel}}(s) - \\mathrm{MSE}_{\\text{kernel}}(1) |$, is expected to be non-zero for $s \\neq 1$.\n\n### Computational Procedure\n\nThe analysis will proceed as follows:\n\n1.  **Data Generation**: Using a fixed random seed of $7$, generate a training set of size $n_{\\text{train}} = 200$ and a test set of size $n_{\\text{test}} = 100$. The predictors $X_1$ and $X_2$ are drawn from their respective uniform distributions, and the response $Y$ is computed from the specified data generating process including the sinusoidal non-linearity and Gaussian noise.\n\n2.  **Iterative Analysis over Scaling Factors**: For each scaling factor $s$ in the set $S = \\{0.5, 1.0, 2.0, 10.0\\}$:\n    *   The training and test predictors, $X_{\\text{train}}$ and $X_{\\text{test}}$, are rescaled by multiplying them component-wise by $s$ to produce $X^{(s)}_{\\text{train}}$ and $X^{(s)}_{\\text{test}}$.\n    *   **Parametric Analysis**:\n        *   The sample mean and sample standard deviation (with $n-1$ in the denominator, `ddof=1`) are computed for each feature of $X^{(s)}_{\\text{train}}$.\n        *   These statistics are used to standardize both $X^{(s)}_{\\text{train}}$ and $X^{(s)}_{\\text{test}}$.\n        *   An OLS model with an intercept is fitted to predict $Y_{\\text{train}}$ from the standardized training predictors. The two slope coefficients, forming the vector $\\widehat{\\beta}^{(s)}$, are stored.\n    *   **Non-Parametric Analysis**:\n        *   The Nadaraya–Watson estimator is fitted using the rescaled training data $(X^{(s)}_{\\text{train}}, Y_{\\text{train}})$ with a fixed bandwidth $h=0.5$.\n        *   Predictions are made for each point in the rescaled test set, $X^{(s)}_{\\text{test}}$. The numerical stability check (denominator against $\\tau=10^{-12}$) is implemented.\n        *   The Mean Squared Error, $\\mathrm{MSE}_{\\text{kernel}}(s)$, between the predictions and the true test responses $Y_{\\text{test}}$ is calculated and stored.\n\n3.  **Drift Calculation**:\n    *   After iterating through all scaling factors in $S$, the baseline results corresponding to $s=1.0$, namely $\\widehat{\\beta}^{(1)}$ and $\\mathrm{MSE}_{\\text{kernel}}(1)$, are retrieved.\n    *   For each $s \\in S$, the drift metrics are computed:\n        *   $d_{\\text{param}}(s) = \\| \\widehat{\\beta}^{(s)} - \\widehat{\\beta}^{(1)} \\|_{2}$.\n        *   $d_{\\text{kernel}}(s) = | \\mathrm{MSE}_{\\text{kernel}}(s) - \\mathrm{MSE}_{\\text{kernel}}(1) |$.\n\n4.  **Final Output**: The computed drift values are collated into a single list in the specified order and printed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a comparison between a parametric (standardized OLS) and a \n    non-parametric (Nadaraya-Watson) method to study the effect of \n    feature rescaling.\n    \"\"\"\n    \n    # Problem parameters\n    n_train = 200\n    n_test = 100\n    noise_sigma = 0.5\n    seed = 7\n    h = 0.5\n    tau = 1e-12\n    scaling_factors = [0.5, 1.0, 2.0, 10.0]\n\n    # Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # --- Data Generation ---\n    # Training data\n    X1_train = rng.uniform(-2, 2, size=n_train)\n    X2_train = rng.uniform(-1, 1, size=n_train)\n    X_train = np.stack([X1_train, X2_train], axis=1)\n    eps_train = rng.normal(0, noise_sigma, size=n_train)\n    Y_train = 3 * X1_train - 2 * X2_train + 0.3 * np.sin(3 * X1_train) + eps_train\n\n    # Test data\n    X1_test = rng.uniform(-2, 2, size=n_test)\n    X2_test = rng.uniform(-1, 1, size=n_test)\n    X_test = np.stack([X1_test, X2_test], axis=1)\n    eps_test = rng.normal(0, noise_sigma, size=n_test)\n    Y_test = 3 * X1_test - 2 * X2_test + 0.3 * np.sin(3 * X1_test) + eps_test\n    \n    Y_train_mean = np.mean(Y_train)\n\n    # Storage for results\n    beta_coeffs = {}\n    kernel_mses = {}\n\n    for s in scaling_factors:\n        # --- Rescaling ---\n        X_train_s = s * X_train\n        X_test_s = s * X_test\n\n        # --- Parametric Method (Standardized OLS) ---\n        mu_s = np.mean(X_train_s, axis=0)\n        # Use ddof=1 for sample standard deviation as per statistical convention\n        sigma_s = np.std(X_train_s, axis=0, ddof=1)\n        \n        # Handle case where a feature has zero standard deviation\n        sigma_s[sigma_s == 0] = 1.0\n\n        Z_train_s = (X_train_s - mu_s) / sigma_s\n        Z_train_s_intercept = np.hstack([np.ones((n_train, 1)), Z_train_s])\n        \n        # Solve OLS using least squares\n        coeffs_s, _, _, _ = np.linalg.lstsq(Z_train_s_intercept, Y_train, rcond=None)\n        beta_coeffs[s] = coeffs_s[1:]  # Store only slope coefficients\n\n        # --- Non-Parametric Method (Nadaraya-Watson) ---\n        Y_pred_kernel = np.zeros(n_test)\n        \n        for j in range(n_test):\n            x_test_point = X_test_s[j]\n            \n            # Calculate squared Euclidean distances from the test point to all training points\n            dists_sq = np.sum((X_train_s - x_test_point)**2, axis=1)\n            \n            # Compute Gaussian kernel weights\n            weights = np.exp(-dists_sq / (2 * h**2))\n            \n            # Calculate numerator and denominator for the prediction\n            numerator = np.sum(weights * Y_train)\n            denominator = np.sum(weights)\n\n            if denominator  tau:\n                Y_pred_kernel[j] = Y_train_mean\n            else:\n                Y_pred_kernel[j] = numerator / denominator\n        \n        # Calculate MSE for the kernel regression\n        mse_s = np.mean((Y_test - Y_pred_kernel)**2)\n        kernel_mses[s] = mse_s\n\n    # --- Drift Calculation ---\n    results = []\n    beta_baseline = beta_coeffs[1.0]\n    mse_baseline = kernel_mses[1.0]\n\n    for s in scaling_factors:\n        # Parametric coefficient drift\n        d_param = np.linalg.norm(beta_coeffs[s] - beta_baseline)\n        \n        # Non-parametric performance drift\n        d_kernel = np.abs(kernel_mses[s] - mse_baseline)\n        \n        results.append(d_param)\n        results.append(d_kernel)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A model's true test often comes when it is asked to predict outside the domain of its training data—a task known as extrapolation. This exercise contrasts the extrapolation behavior of a parametric linear model with that of a non-parametric Gaussian Process, using an intuitive geospatial context. By computing and comparing how each model quantifies its predictive uncertainty in unobserved regions, you will gain a deeper appreciation for how a model's underlying structure governs its confidence when venturing into the unknown .",
            "id": "3155828",
            "problem": "You are given a small geospatial dataset consisting of sensor locations and scalar readings. You will compare two models to quantify predictive uncertainty at specified locations where no sensor reading is available. The two models are: a parametric linear trend in latitude and longitude, and a non-parametric Gaussian Process (GP) kriging model. Your task is to compute, for each test location, the posterior variance of the latent signal (not including observation noise) under each model, and assemble the results in a single line of output in the format specified below.\n\nFundamental base:\n- Parametric model definition: a finite-dimensional linear model is defined by an assumed functional form and a finite parameter vector. Consider the ordinary least squares (OLS) linear model with Gaussian noise, where the response at a location with latitude $lat$ and longitude $lon$ is modeled as $y = \\beta_{0} + \\beta_{1} \\, lat + \\beta_{2} \\, lon + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$, and the parameters are $\\beta_{0}, \\beta_{1}, \\beta_{2}$.\n- Non-parametric model definition: a Gaussian Process (GP) places a distribution over functions, defined by a mean function and a covariance kernel, in an infinite-dimensional function space. For locations $x = (lat, lon)$, the prior is $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$, and observations follow $y = f(x) + \\eta$, with $\\eta \\sim \\mathcal{N}(0, \\tau^{2})$.\n\nWell-tested formulas and facts to be used as starting points:\n- In the OLS linear model with Gaussian noise, the parameter estimator has sampling distribution $ \\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}, \\sigma^{2} (X^{\\top} X)^{-1}) $, where $X$ is the design matrix with rows $x_{i}^{\\top} = [1, lat_{i}, lon_{i}]$, and the unbiased estimator of the noise variance is $\\hat{\\sigma}^{2} = \\mathrm{RSS} / (n - p)$, with $\\mathrm{RSS}$ the residual sum of squares, $n$ the number of observations, and $p$ the number of parameters.\n- In Gaussian Process regression with observation noise, the joint distribution of training function values and a test function value is multivariate Gaussian, and conditioning yields a Gaussian posterior whose variance depends only on the kernel and locations.\n\nYour program must use the following dataset and model hyperparameters. All angles are in degrees. The coordinates are latitudes and longitudes in degrees, and distances in the kernel are computed directly in degrees (no conversion to radians). There are $8$ sensors, with coordinates and scalar readings as follows:\n- Sensor $1$: $lat = -9$, $lon = -18$, $y = 9.7$.\n- Sensor $2$: $lat = -6$, $lon = 12$, $y = 4.8$.\n- Sensor $3$: $lat = 0$, $lon = 0$, $y = 9.7$.\n- Sensor $4$: $lat = 4$, $lon = -10$, $y = 14.5$.\n- Sensor $5$: $lat = 9$, $lon = 19$, $y = 10.3$.\n- Sensor $6$: $lat = -3$, $lon = 5$, $y = 7.3$.\n- Sensor $7$: $lat = 7$, $lon = -4$, $y = 14.4$.\n- Sensor $8$: $lat = -10$, $lon = 15$, $y = 2.4$.\n\nParametric model:\n- Use the linear trend model $y = \\beta_{0} + \\beta_{1} \\, lat + \\beta_{2} \\, lon + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$.\n- Fit the model by ordinary least squares (OLS), estimate $\\hat{\\sigma}^{2}$ by $\\hat{\\sigma}^{2} = \\mathrm{RSS}/(n-p)$ with $n = 8$ and $p = 3$.\n- For a test location with feature vector $x_{*}^{\\top} = [1, lat_{*}, lon_{*}]$, compute the posterior variance of the latent signal (the mean function) under the parametric model as $ \\mathrm{Var}_{\\text{param}}(x_{*}) = \\hat{\\sigma}^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}$, where $X$ is the $n \\times p$ design matrix of the training sensors.\n\nNon-parametric GP kriging model:\n- Use a zero-mean Gaussian Process (GP) prior with an anisotropic squared exponential (Gaussian) kernel:\n$$\nk\\big((lat, lon), (lat', lon')\\big) = s^{2} \\exp\\left( -\\tfrac{1}{2} \\left[ \\left( \\frac{lat - lat'}{\\ell_{\\mathrm{lat}}} \\right)^{2} + \\left( \\frac{lon - lon'}{\\ell_{\\mathrm{lon}}} \\right)^{2} \\right] \\right).\n$$\n- Hyperparameters are fixed as $s^{2} = 9$, $\\ell_{\\mathrm{lat}} = 6$, $\\ell_{\\mathrm{lon}} = 10$, and observation noise variance $\\tau^{2} = 0.25$.\n- For a test location $x_{*}$, the posterior variance of the latent signal (excluding observation noise) is\n$$\n\\mathrm{Var}_{\\text{GP}}(x_{*}) = k(x_{*}, x_{*}) - \\boldsymbol{k}_{*}^{\\top} (K + \\tau^{2} I)^{-1} \\boldsymbol{k}_{*},\n$$\nwhere $K$ is the $n \\times n$ kernel matrix computed over training sensor locations using $k$, and $\\boldsymbol{k}_{*}$ is the $n \\times 1$ kernel vector between $x_{*}$ and each training sensor location.\n\nTest suite:\nCompute both $\\mathrm{Var}_{\\text{param}}(x_{*})$ and $\\mathrm{Var}_{\\text{GP}}(x_{*})$ for the following $4$ test locations, all in degrees:\n- Test $1$: $lat_{*} = 20$, $lon_{*} = 35$ (far outside the training region).\n- Test $2$: $lat_{*} = 1$, $lon_{*} = 2$ (near the center of the training region).\n- Test $3$: $lat_{*} = -10$, $lon_{*} = 15$ (exactly at a training sensor location).\n- Test $4$: $lat_{*} = 12$, $lon_{*} = -18$ (outside but near the training region).\n\nFinal output format:\nYour program should produce a single line of output containing the $8$ floating-point results as a comma-separated list enclosed in square brackets, in the exact order\n$$\n[\\mathrm{Var}_{\\text{param}}(x_{1}), \\mathrm{Var}_{\\text{GP}}(x_{1}), \\mathrm{Var}_{\\text{param}}(x_{2}), \\mathrm{Var}_{\\text{GP}}(x_{2}), \\mathrm{Var}_{\\text{param}}(x_{3}), \\mathrm{Var}_{\\text{GP}}(x_{3}), \\mathrm{Var}_{\\text{param}}(x_{4}), \\mathrm{Var}_{\\text{GP}}(x_{4})].\n$$\n\nAngle unit specification:\nAll angles are in degrees. Use degrees consistently for the kernel distance computation. No conversion to radians is required.\n\nNote: The problem asks for the uncertainty of the latent signal under each model, not the variance of a noisy observation. Therefore, do not add the observation noise variance to the predictive variances.",
            "solution": "We compare parametric and non-parametric uncertainty quantification from first principles, starting from core definitions of linear regression and Gaussian Process (GP) regression. We will derive the posterior variance of the latent signal at a new location under both models and then implement the corresponding algorithms.\n\nParametric model (finite-dimensional linear model):\n- Definition: A parametric model specifies a finite-dimensional parameter vector and a fixed functional form. For geospatial coordinates with latitude $lat$ and longitude $lon$, consider the linear model\n$$\ny_{i} = \\beta_{0} + \\beta_{1} \\, lat_{i} + \\beta_{2} \\, lon_{i} + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2}),\n$$\nfor $i = 1, \\ldots, n$.\n- Design matrix: Stack the predictors into a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with $p = 3$, where each row is $x_{i}^{\\top} = [1, lat_{i}, lon_{i}]$.\n- Estimation: Ordinary Least Squares (OLS) estimators minimize the residual sum of squares $\\mathrm{RSS} = \\sum_{i=1}^{n} (y_{i} - x_{i}^{\\top} \\hat{\\boldsymbol{\\beta}})^{2}$. Under the Gaussian noise assumption, the sampling distribution of the estimator is $ \\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}, \\sigma^{2} (X^{\\top} X)^{-1})$ and the unbiased estimator of the noise variance is $ \\hat{\\sigma}^{2} = \\mathrm{RSS} / (n - p)$.\n- Predictive mean and uncertainty of the latent signal: The latent signal (mean function) at a new point with feature vector $x_{*}^{\\top} = [1, lat_{*}, lon_{*}]$ is $m_{*} = x_{*}^{\\top} \\boldsymbol{\\beta}$. The estimator $x_{*}^{\\top} \\hat{\\boldsymbol{\\beta}}$ has variance\n$$\n\\mathrm{Var}\\left( x_{*}^{\\top} \\hat{\\boldsymbol{\\beta}} \\right) = \\sigma^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}.\n$$\nReplacing $\\sigma^{2}$ by $\\hat{\\sigma}^{2}$ yields the practical posterior variance estimate of the latent signal under the parametric model:\n$$\n\\mathrm{Var}_{\\text{param}}(x_{*}) = \\hat{\\sigma}^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}.\n$$\nNote that this is the uncertainty in the mean function estimate, not including the irreducible noise $\\sigma^{2}$ of a new observation.\n\nAlgorithmic design for the parametric model:\n- Construct $X$ by concatenating ones, $lat_{i}$, and $lon_{i}$ for all sensors.\n- Solve the normal equations to obtain $\\hat{\\boldsymbol{\\beta}}$ using $ \\hat{\\boldsymbol{\\beta}} = (X^{\\top} X)^{-1} X^{\\top} \\boldsymbol{y}$.\n- Compute residuals $\\boldsymbol{r} = \\boldsymbol{y} - X \\hat{\\boldsymbol{\\beta}}$ and $\\hat{\\sigma}^{2} = \\boldsymbol{r}^{\\top} \\boldsymbol{r} / (n - p)$.\n- For each test location, form $x_{*}$ and compute $ \\mathrm{Var}_{\\text{param}}(x_{*}) = \\hat{\\sigma}^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}$. Numerically, avoid explicit matrix inversion by solving $(X^{\\top} X) \\boldsymbol{w} = x_{*}$ and computing $x_{*}^{\\top} \\boldsymbol{w}$.\n\nNon-parametric model (Gaussian Process kriging):\n- Definition: A Gaussian Process (GP) places a distribution over functions $f$ with $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$. Observations follow $y_{i} = f(x_{i}) + \\eta_{i}$ with $\\eta_{i} \\sim \\mathcal{N}(0, \\tau^{2})$.\n- Kernel: Use the anisotropic squared exponential kernel in degrees with amplitude $s^{2}$ and length scales $\\ell_{\\mathrm{lat}}$ and $\\ell_{\\mathrm{lon}}$:\n$$\nk(x, x') = s^{2} \\exp\\left( -\\tfrac{1}{2} \\left[ \\left( \\frac{lat - lat'}{\\ell_{\\mathrm{lat}}} \\right)^{2} + \\left( \\frac{lon - lon'}{\\ell_{\\mathrm{lon}}} \\right)^{2} \\right] \\right).\n$$\n- Conditioning rule for multivariate Gaussians: The joint prior of $\\boldsymbol{f}$ at training inputs and $f_{*}$ at a test input is Gaussian\n$$\n\\begin{bmatrix}\n\\boldsymbol{f} \\\\\nf_{*}\n\\end{bmatrix}\n\\sim\n\\mathcal{N}\\left(\n\\begin{bmatrix}\n\\boldsymbol{0} \\\\\n0\n\\end{bmatrix},\n\\begin{bmatrix}\nK  \\boldsymbol{k}_{*} \\\\\n\\boldsymbol{k}_{*}^{\\top}  k(x_{*}, x_{*})\n\\end{bmatrix}\n\\right),\n$$\nand with observations $\\boldsymbol{y} = \\boldsymbol{f} + \\boldsymbol{\\eta}$, the posterior over $f_{*}$ given $\\boldsymbol{y}$ has variance\n$$\n\\mathrm{Var}_{\\text{GP}}(x_{*}) = k(x_{*}, x_{*}) - \\boldsymbol{k}_{*}^{\\top} (K + \\tau^{2} I)^{-1} \\boldsymbol{k}_{*}.\n$$\nCritically, the posterior variance depends only on the kernel and the sensor locations (and $\\tau^{2}$), not on the observed values $\\boldsymbol{y}$.\n- Numerical stability: Compute $(K + \\tau^{2} I)^{-1} \\boldsymbol{k}_{*}$ via a Cholesky factorization $K_{\\tau} = K + \\tau^{2} I = L L^{\\top}$, solve $L \\boldsymbol{v} = \\boldsymbol{k}_{*}$, and compute $ \\mathrm{Var}_{\\text{GP}}(x_{*}) = k(x_{*}, x_{*}) - \\boldsymbol{v}^{\\top} \\boldsymbol{v}$. Add a small jitter $\\epsilon$ to the diagonal if needed to ensure positive definiteness.\n\nAlgorithmic design for the GP model:\n- Compute the $n \\times n$ kernel matrix $K$ from training coordinates using $k$.\n- Form $K_{\\tau} = K + \\tau^{2} I + \\epsilon I$ with small $\\epsilon$.\n- For each test location, compute $\\boldsymbol{k}_{*}$ and $k(x_{*}, x_{*}) = s^{2}$, perform the Cholesky-based solve to get $\\boldsymbol{v}$, and compute $ \\mathrm{Var}_{\\text{GP}}(x_{*}) = s^{2} - \\boldsymbol{v}^{\\top} \\boldsymbol{v}$.\n\nTest coverage rationale:\n- Test $1$ is far outside the training region to examine extrapolation behavior. Parametric uncertainty grows according to $x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}$, while GP variance approaches the prior variance $s^{2}$ as $x_{*}$ moves far from sensors.\n- Test $2$ is near the center, examining interpolation where both methods have more information.\n- Test $3$ coincides with a training sensor location, testing the GP’s variance reduction at training inputs in the presence of noise variance $\\tau^{2}$, and the parametric model’s nonzero mean-function uncertainty at that input.\n- Test $4$ is outside but near the training region, probing boundary behavior.\n\nImplementation details:\n- All angle quantities are kept in degrees; kernel distances use degree differences scaled by the length scales.\n- The final output is a single list of $8$ floating-point numbers in the specified order. No physical units are required for the variances, and the angle unit is fixed to degrees.\n\nWith these principles and algorithms, the program computes $\\mathrm{Var}_{\\text{param}}(x_{*})$ and $\\mathrm{Var}_{\\text{GP}}(x_{*})$ for each test location and prints them in the required single-line format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_design_matrix(lats, lons):\n    \"\"\"\n    Construct the design matrix X for the linear model y = b0 + b1*lat + b2*lon + noise.\n    \"\"\"\n    n = len(lats)\n    X = np.column_stack([np.ones(n), np.array(lats, dtype=float), np.array(lons, dtype=float)])\n    return X\n\ndef fit_ols_params(X, y):\n    \"\"\"\n    Fit OLS parameters and estimate sigma^2 using unbiased estimator RSS/(n-p).\n    Returns beta_hat and sigma2_hat.\n    \"\"\"\n    # Solve normal equations without explicit inversion for numerical stability\n    XtX = X.T @ X\n    Xty = X.T @ y\n    beta_hat = np.linalg.solve(XtX, Xty)\n    residuals = y - X @ beta_hat\n    n, p = X.shape\n    rss = float(residuals.T @ residuals)\n    sigma2_hat = rss / (n - p)\n    return beta_hat, sigma2_hat, XtX\n\ndef parametric_variance_at(XtX, sigma2_hat, lat_star, lon_star):\n    \"\"\"\n    Compute Var_param(x*) = sigma2_hat * x_*^T (XtX)^-1 x_*\n    without explicitly inverting XtX by solving (XtX) w = x_*.\n    \"\"\"\n    x_star = np.array([1.0, float(lat_star), float(lon_star)])\n    # Solve (XtX) w = x_star\n    w = np.linalg.solve(XtX, x_star)\n    var_param = sigma2_hat * float(x_star.T @ w)\n    return var_param\n\ndef se_kernel(lat1, lon1, lat2, lon2, s2, ell_lat, ell_lon):\n    \"\"\"\n    Anisotropic squared exponential kernel in degrees.\n    \"\"\"\n    dlat = (lat1 - lat2) / ell_lat\n    dlon = (lon1 - lon2) / ell_lon\n    return s2 * np.exp(-0.5 * (dlat**2 + dlon**2))\n\ndef build_kernel_matrix(coords, s2, ell_lat, ell_lon):\n    \"\"\"\n    Build the kernel matrix K for training coordinates.\n    coords: list of (lat, lon)\n    \"\"\"\n    n = len(coords)\n    K = np.empty((n, n), dtype=float)\n    for i in range(n):\n        lat_i, lon_i = coords[i]\n        for j in range(n):\n            lat_j, lon_j = coords[j]\n            K[i, j] = se_kernel(lat_i, lon_i, lat_j, lon_j, s2, ell_lat, ell_lon)\n    return K\n\ndef gp_posterior_variance(K, coords, lat_star, lon_star, s2, ell_lat, ell_lon, tau2, jitter=1e-9):\n    \"\"\"\n    Compute GP posterior variance of the latent signal at x*:\n    Var_GP(x*) = k(x*,x*) - k_*^T (K + tau2*I)^-1 k_*\n    Uses Cholesky factorization for numerical stability.\n    \"\"\"\n    n = len(coords)\n    # Compute k_* vector\n    k_star = np.empty(n, dtype=float)\n    for i in range(n):\n        lat_i, lon_i = coords[i]\n        k_star[i] = se_kernel(lat_star, lon_star, lat_i, lon_i, s2, ell_lat, ell_lon)\n    # k(x*, x*) for SE kernel is s2\n    k_xx = s2\n    # Add noise variance and jitter to K\n    K_tilde = K + (tau2 + jitter) * np.eye(n)\n    # Cholesky factorization\n    try:\n        L = np.linalg.cholesky(K_tilde)\n    except np.linalg.LinAlgError:\n        # Increase jitter if needed\n        K_tilde = K + (tau2 + 1e-6) * np.eye(n)\n        L = np.linalg.cholesky(K_tilde)\n    # Solve L v = k_star\n    v = np.linalg.solve(L, k_star)\n    var_gp = float(k_xx - v.T @ v)\n    # Numerical safety: variance should be non-negative\n    if var_gp  0 and var_gp > -1e-10:\n        var_gp = 0.0\n    return var_gp\n\ndef solve():\n    # Training sensors (lat, lon, y)\n    sensors = [\n        (-9.0, -18.0, 9.7),\n        (-6.0,  12.0, 4.8),\n        ( 0.0,   0.0, 9.7),\n        ( 4.0, -10.0,14.5),\n        ( 9.0,  19.0,10.3),\n        (-3.0,   5.0, 7.3),\n        ( 7.0,  -4.0,14.4),\n        (-10.0, 15.0, 2.4),\n    ]\n    lats = [s[0] for s in sensors]\n    lons = [s[1] for s in sensors]\n    ys   = [s[2] for s in sensors]\n    y = np.array(ys, dtype=float)\n\n    # Design matrix and OLS fit\n    X = build_design_matrix(lats, lons)\n    beta_hat, sigma2_hat, XtX = fit_ols_params(X, y)\n\n    # GP hyperparameters\n    s2 = 9.0\n    ell_lat = 6.0  # degrees\n    ell_lon = 10.0 # degrees\n    tau2 = 0.25\n\n    # Build kernel matrix for training coords\n    coords = [(lat, lon) for lat, lon, _ in sensors]\n    K = build_kernel_matrix(coords, s2, ell_lat, ell_lon)\n\n    # Test cases: list of (lat*, lon*)\n    test_cases = [\n        (20.0,  35.0),   # Test 1: far outside\n        ( 1.0,   2.0),   # Test 2: near center\n        (-10.0, 15.0),   # Test 3: exactly a training sensor\n        (12.0, -18.0),   # Test 4: outside near region\n    ]\n\n    results = []\n    for lat_star, lon_star in test_cases:\n        var_param = parametric_variance_at(XtX, sigma2_hat, lat_star, lon_star)\n        var_gp = gp_posterior_variance(K, coords, lat_star, lon_star, s2, ell_lat, ell_lon, tau2)\n        results.append(var_param)\n        results.append(var_gp)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}