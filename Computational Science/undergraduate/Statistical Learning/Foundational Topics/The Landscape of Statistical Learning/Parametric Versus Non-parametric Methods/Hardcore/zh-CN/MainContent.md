## 引言
在[统计学习](@entry_id:269475)的广阔领域中，模型选择是连接理论与实践的桥梁，也是所有数据分析任务的基石。我们如何从纷繁复杂的数据中提炼出有意义的模式？我们应赋予模型多大的灵活性去贴[合数](@entry_id:263553)据，又应施加多强的结构性假设以避免过拟合？这些问题的核心，最终指向了一个根本性的抉择：采用参数化方法还是非参数化方法。这一选择不仅影响模型的预测精度，更深刻地决定了我们从数据中推断结论的范围与可靠性。

本文旨在系统性地剖析参数化与非[参数化](@entry_id:272587)方法之间的本质区别、内在联系与实践权衡。许多初学者和从业者常常在这两种思想之间感到困惑，不清楚何时、为何选择其一。本文将填补这一知识鸿沟，带领读者深入理解这两种建模哲学的核心思想。

在接下来的内容中，我们将分三步展开：
- **原理与机制**：我们将首先奠定理论基础，精确定义[参数化](@entry_id:272587)、非参数化及半[参数化](@entry_id:272587)模型，并深入探讨它们在灵活性、效率、偏差-[方差](@entry_id:200758)、计算复杂性以及推断范围等方面的深刻差异。
- **应用与跨学科联系**：我们将穿越多个学科领域，从生物统计到[金融风险管理](@entry_id:138248)，从信号处理到机器学习，通过一系列生动的实例展示这些理论在解决真实世界问题中的强大威力与巧妙应用。
- **动手实践**：最后，我们将通过具体的编程练习，让您亲手体验和验证这两种方法在面对不同数据挑战（如[特征缩放](@entry_id:271716)、模型外推和[集成学习](@entry_id:637726)）时的行为差异，将理论知识转化为实践技能。

通过这一趟从理论到应用的旅程，您将能够更深刻地理解[统计建模](@entry_id:272466)的艺术，并在未来的数据分析工作中做出更明智、更有效的决策。

## 原理与机制

在[统计学习](@entry_id:269475)中，选择合适的模型是核心任务之一。模型是对数据生成过程的抽象和简化，而我们选择的模型家族（或假设类别）从根本上决定了我们能从数据中学到什么，以及学习的效率如何。模型家族的选择谱系中，最重要的一条[分界线](@entry_id:175112)在于[参数化](@entry_id:272587)方法与非[参数化](@entry_id:272587)方法之间。本章将深入探讨这两种方法的根本原理、核心机制以及它们在实践中引发的一系列深刻权衡。

### 定义分界：[参数化](@entry_id:272587)与非参数化模型

要精确理解两种方法的区别，我们必须从它们所依赖的[假设空间](@entry_id:635539)的维度入手。

**参数化模型 (Parametric Models)** 的核心特征在于，它事先假定目标函数或[概率分布](@entry_id:146404)的具体形式，而这种形式由一个维数固定且有限的参数向量 $\boldsymbol{\theta}$ 完全决定。换言之，其[假设空间](@entry_id:635539) $\mathcal{H}$ 可以被一个欧氏空间 $\mathbb{R}^p$ 的[子集](@entry_id:261956) $\Theta$ 所索引，即 $\mathcal{H} = \{ f_{\boldsymbol{\theta}} : \boldsymbol{\theta} \in \Theta \subseteq \mathbb{R}^p \}$。这里的关键在于，参数空间的维度 $p$ 是一个预先确定的、不随样本量 $n$ 变化的有限整数。

一个典型的例子是固定阶数的自回归外生输入（ARX）模型 。一个 $(n_a, n_b)$ 阶的[ARX模型](@entry_id:269528)由[差分方程](@entry_id:262177)描述，其行为完全由一个包含 $p = n_a + n_b$ 个系数的参数向量 $\boldsymbol{\theta}$ 决定。一旦我们选定阶数，无论我们收集100个还是100万个数据点，需要估计的参数数量始终是 $p$。这种事先对[模型复杂度](@entry_id:145563)的严格限定，是[参数化](@entry_id:272587)方法的标志。

**非参数化模型 (Non-parametric Models)** 则与之相反，它不对目标函数做严格的功能形式假定。其[假设空间](@entry_id:635539)通常是一个无穷维的[函数空间](@entry_id:143478)，例如所有[连续函数](@entry_id:137361)的集合、[索博列夫空间](@entry_id:141995)（Sobolev space）或[再生核希尔伯特空间](@entry_id:633928)（Reproducing Kernel Hilbert Space, RKHS）。

一个常见的误解是认为“非参数化”意味着“没有参数”。事实并非如此。非[参数化](@entry_id:272587)方法的“参数”数量通常不是预先固定的，而是会随着样本量 $n$ 的增加而增长。例如，在基于[核方法](@entry_id:276706)的[回归模型](@entry_id:163386)中，根据[表示定理](@entry_id:637872)（representer theorem），对于一个大小为 $n$ 的[训练集](@entry_id:636396)，其解可以表示为 $n$ 个[核函数](@entry_id:145324)的[线性组合](@entry_id:154743)，即 $f(\cdot) = \sum_{i=1}^n \alpha_i k(x_i, \cdot)$ 。这里有 $n$ 个系数 $\alpha_i$ 需要确定。然而，这并不意味着模型是[参数化](@entry_id:272587)的。根本区别在于，其底层的[假设空间](@entry_id:635539)（例如，某个RKHS）是无穷维的。最终估计器的复杂度（由系数数量体现）依赖于数据量 $n$，这正是非[参数化](@entry_id:272587)方法的本质特征。

另一类重要的非[参数化](@entry_id:272587)方法被称为“筛法”（method of sieves）。这类方法构建一个嵌套的、复杂度递增的[参数化](@entry_id:272587)模型序列，并在估计时根据数据量 $n$ 选择合适的复杂度。例如，考虑一个[有限脉冲响应](@entry_id:192542)（FIR）滤波器，我们可以允许其阶数 $p$ 随着样本量 $n$ 增长而增长，即 $p=p(n)$。由于没有任何一个预先固定的有限 $p$ 可以描述所有可能的模型，其总[假设空间](@entry_id:635539)是所有有限阶数模型的并集 $\bigcup_{p \in \mathbb{N}} \mathcal{F}_p$，这是一个[无穷维空间](@entry_id:141268)，因此该方法被归类为非[参数化](@entry_id:272587)方法 。

### 中间地带：半参数化模型

在参数化与非[参数化](@entry_id:272587)的两极之间，存在着广阔的中间地带，即**半[参数化](@entry_id:272587)模型 (Semi-parametric Models)**。这类模型巧妙地结合了两种方法的特点，其结构中既包含有限维的参数化部分，也包含无穷维的非[参数化](@entry_id:272587)部分。

[生存分析](@entry_id:163785)中的**[Cox比例风险模型](@entry_id:174252) (Cox Proportional Hazards Model)** 是半参数化模型最经典的范例 。该模型描述了在给定协变量向量 $\mathbf{X}$ 的情况下，个体在时间 $t$ 的瞬时风险率（hazard rate）$h(t | \mathbf{X})$：
$$h(t | \mathbf{X}) = h_0(t) \exp(\boldsymbol{\beta}^T \mathbf{X})$$
在这个模型中：
-   **参数化部分**是 $\exp(\boldsymbol{\beta}^T \mathbf{X})$。它明确规定了协变量 $\mathbf{X}$ 与风险率之间的函数关系（对数线性关系），这种关系由一个有限维的参数向量 $\boldsymbol{\beta}$ 控制。
-   **非[参数化](@entry_id:272587)部分**是**基准[风险函数](@entry_id:166593) (baseline hazard function)** $h_0(t)$。模型对 $h_0(t)$ 的具体形式不做任何假定，它仅仅是一个未知的、非负的函数。$h_0(t)$ 属于一个无穷维的[函数空间](@entry_id:143478)。

[Cox模型](@entry_id:164053)之所以强大且应用广泛，正是因为它在对协变量效应做出具体（参数化）假设的同时，又保持了对时间依赖的基准风险（非参数化）的极大灵活性。

### 灵活性与效率的权衡

参数化与非参数化方法之间的选择，本质上是在模型的**灵活性 (Flexibility)** 与**效率 (Efficiency)** 之间进行权衡。这种权衡通常表现为统计学中经典的**[偏差-方差权衡](@entry_id:138822) (Bias-Variance Tradeoff)**。

#### 灵活性与偏差

参数化模型建立在对数据生成过程的**强假设**之上（例如，[线性关系](@entry_id:267880)、[正态分布](@entry_id:154414)等）。如果这些假设与现实相符，模型就能以简洁的形式捕捉数据的主要规律。然而，如果假设错误，即**[模型设定错误](@entry_id:170325) (model misspecification)**，[参数化](@entry_id:272587)模型就会产生系统性的**偏差 (bias)**。无论我们拥有多少数据，一个设定错误的模型都无法收敛到真实的数据[生成函数](@entry_id:146702)，它只能找到其有限[假设空间](@entry_id:635539)内“最不坏”的近似。

相比之下，非[参数化](@entry_id:272587)模型只做非常**弱的假设**（例如，函数是连续的或光滑的），因此具有高度的灵活性。它们能够适应各种复杂的数据结构，如非[线性关系](@entry_id:267880)、多峰[分布](@entry_id:182848)等。理论上，只要有足够多的数据，非[参数化](@entry_id:272587)模型可以逼近任意函数，因此它们是**渐进无偏 (asymptotically unbiased)** 的。

一个很好的例子是[密度估计](@entry_id:634063) 。假设我们有一组数据，我们想估计其潜在的概率密度函数。
-   一个参数化的方法是假定数据来自[正态分布](@entry_id:154414) $\mathcal{N}(\mu, \sigma^2)$，然后用[最大似然](@entry_id:146147)法估计参数 $(\mu, \sigma^2)$。如果真实[分布](@entry_id:182848)确实是正态的，这是一个非常高效的做法。但如果真实[分布](@entry_id:182848)是双峰的，那么我们得到的最佳[正态近似](@entry_id:261668)，无论如何也无法捕捉到双峰的特征。
-   一个非[参数化](@entry_id:272587)的方法是**[核密度估计](@entry_id:167724) (Kernel Density Estimation, KDE)**。KDE通过在每个数据点上放置一个“核函数”（如一个小的[正态分布](@entry_id:154414)），然后将它们叠加起来，形成最终的[密度估计](@entry_id:634063)。KDE不对整体[分布](@entry_id:182848)形状做任何假设。当样本量 $n$ 趋于无穷时，只要[核函数](@entry_id:145324)和带宽参数选择得当，KDE可以一致地收敛到任何（足够正则的）真实密度函数，无论其形状多么复杂 。

#### 效率与[方差](@entry_id:200758)

模型的灵活性是有代价的，这个代价通常体现在估计的**[方差](@entry_id:200758) (variance)** 上。

由于参数化模型将学习任务限制在一个小的、结构化的空间内，它们能更有效地利用数据。对于给定的样本量，它们通常能产生[方差](@entry_id:200758)更低的估计。当模型假设正确时，参数化模型往往是完成该统计任务最**高效**的方法。

非参数化模型因为需要从数据中学习函数形式本身，所以“问题更难”，需要更多的数据来稳定其估计。它们的估计量通常有更高的[方差](@entry_id:200758)。

我们可以再次通过一个思想实验来理解这一点 。假设我们知道数据 $X_1, \dots, X_n$ 来自一个正态分布 $\mathcal{N}(\mu, \sigma^2)$，其中 $\sigma^2$ 已知，我们的任务是预测下一个观测值 $X_{n+1}$。
-   **[参数化](@entry_id:272587)预测器**: 利用模型假设，我们知道样本均值 $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$ 是对 $\mu$ 的一个良好估计。于是我们用 $\bar{X}$ 作为对 $X_{n+1}$ 的预测。其期望预测误差为 $\mathbb{E}[(X_{n+1} - \bar{X})^2] = \sigma^2(1 + \frac{1}{n})$。
-   **非参数化预测器**: 一个非常简单的非[参数化](@entry_id:272587)预测器是，从[训练集](@entry_id:636396)中随机抽取一个点 $X_J$ 作为对 $X_{n+1}$ 的预测（这本质上是一种“近邻”思想）。其期望预测误差为 $\mathbb{E}[(X_{n+1} - X_J)^2] = 2\sigma^2$。

对于任何 $n \ge 2$，我们都有 $\sigma^2(1 + \frac{1}{n})  2\sigma^2$。这表明，当模型假设正确时，利用了该假设的[参数化](@entry_id:272587)预测器比不利用该假设的非[参数化](@entry_id:272587)预测器更有效率（误差更小）。

### 机制与推论

[参数化](@entry_id:272587)与非参数化之间的哲学差异，在具体的建模机制和我们能够得出的结论范围上，产生了深远的影响。

#### [数据压缩](@entry_id:137700)与充分统计量

许多经典的[参数化](@entry_id:272587)模型（特别是[指数族](@entry_id:263444)[分布](@entry_id:182848)中的模型）拥有一个强大的特性：存在一个低维的**充分统计量 (sufficient statistic)**。充分统计量是数据的一个函数，它包含了数据中关于未知参数的所有信息。一旦计算出充分统计量，原始数据就可以被丢弃，而不会损失任何关于[参数推断](@entry_id:753157)的信息。

最著名的例子是[正态分布](@entry_id:154414) 。对于来自 $\mathcal{N}(\mu, \sigma^2)$ 的一串独立同分布样本，样本均值 $\bar{X}$ 和样本[方差](@entry_id:200758) $S^2$ 构成了 $(\mu, \sigma^2)$ 的充分统计量。根据内曼-费舍尔分解定理（Neyman–Fisher factorization theorem），似然函数可以被分解，所有与参数 $(\mu, \sigma^2)$ 相关的信息都只通过 $\bar{X}$ 和 $S^2$ 体现。这意味着，为了估计[正态分布](@entry_id:154414)的参数，我们只需要记录这两个数，而无需存储整个数据集。这是一种极致的**[数据压缩](@entry_id:137700)**。

这种压缩效应导致[参数化](@entry_id:272587)估计具有一种特殊的**不变性**。任何不改变充分统计量的数据变换，都不会改变最终的参数估计。例如，只要样本均值不变，无论内部数据点如何变化，对高斯均值的最大似然估计都保持不变。

非参数化方法则截然不同。像[核密度估计](@entry_id:167724)（KDE）或k近邻（k-NN）这样的方法，其估计结果依赖于每个数据点的具体位置。它们需要存储并利用整个数据集来进行预测或估计。不存在一个能“压缩”所有信息的低维统计量。因此，非参数化估计对数据中更细微的结构变化很敏感，这既是其灵活性的来源，也是其需要更多存储和计算资源的原因。

#### 计算复杂性与[核技巧](@entry_id:144768)

在处理高维数据时，两种方法面临着不同的计算挑战。

[参数化](@entry_id:272587)方法，如[多项式回归](@entry_id:176102)，如果想捕捉高阶[交互作用](@entry_id:176776)，就需要显式地创建特征。例如，在一个 $d$ 维输入空间中，创建一个直到 $m$ 阶的多项式特征集，特征的数量会以 $\binom{d+m}{m}$ 的速度[组合爆炸](@entry_id:272935) 。当 $d$ 或 $m$ 稍大时，显式构造这些特征在计算上和存储上都变得不可行。

非参数化方法，特别是[核方法](@entry_id:276706)，提供了一种优雅的解决方案：**[核技巧](@entry_id:144768) (the kernel trick)**。[核技巧](@entry_id:144768)允许我们在一个极高维甚至无穷维的[特征空间](@entry_id:638014)中进行计算，而无需显式地表示或计算该空间中的向量。其核心思想是，许多学习算法（如支持向量机）的计算过程仅依赖于数据点在特征空间中的[内积](@entry_id:158127)。[核函数](@entry_id:145324) $k(\mathbf{x}, \mathbf{z})$ 就是一个能高效计算[特征空间](@entry_id:638014)[内积](@entry_id:158127)的函数，即 $k(\mathbf{x}, \mathbf{z}) = \langle \phi(\mathbf{x}), \phi(\mathbf{z}) \rangle$，其中 $\phi$ 是从原始空间到高维[特征空间](@entry_id:638014)的映射。

例如，一个 $m$ 阶的多项式核SVM，能够学习一个与显式 $m$ 阶[多项式回归](@entry_id:176102)处在相同函数类中的决策边界，但它通过计算一个 $n \times n$ 的核矩阵（Gram matrix）来完成，避免了特征的组合爆炸 。更有甚者，像高斯核（[RBF核](@entry_id:166868)）这样的[核函数](@entry_id:145324)，其对应的特征空间是无穷维的，能够隐式地捕捉任意高阶的[交互作用](@entry_id:176776)，这是显式[特征工程](@entry_id:174925)完全无法企及的 。然而，这种计算上的优雅也有其代价：[核方法](@entry_id:276706)的训练时间通常至少与样本量 $n$ 的平方成正比，因为需要计算和处理 $n \times n$ 的[Gram矩阵](@entry_id:148915)。

#### 推断范围与基本假设

方法的选择不仅影响预测精度，还决定了我们能从[统计显著性](@entry_id:147554)结果中得出何种范围的结论。

许多经典的[参数化](@entry_id:272587)检验，如[双样本t检验](@entry_id:164898)，其理论基础是**[随机抽样](@entry_id:175193)模型 (random sampling model)** 。该模型假设我们观察到的数据是从某个更广泛的目标人群中随机抽取的样本。因此，当t检验得到一个显著的[p值](@entry_id:136498)时（例如，$p=0.03$），我们可以将结论推广到**总体 (population)**。我们可以说：“药物在目标人群中（平均而言）是有效的。”这种推广能力是其巨大优势，但其有效性建立在[随机抽样](@entry_id:175193)以及数据[分布](@entry_id:182848)（如正态性）等强假设之上。

与此相对，许多非[参数化](@entry_id:272587)检验，如**[置换检验](@entry_id:175392) (permutation test)**，其理论基础是**随机分配模型 (random assignment model)** 。该检验不关心样本如何从总体中抽取，而只关注处理（如药物或安慰剂）如何被随机分配给实验中的个体。其[零假设](@entry_id:265441)是“[尖锐零假设](@entry_id:177768)”（sharp null hypothesis），即处理对任何一个个体都没有任何影响。[p值](@entry_id:136498)（例如，$p=0.03$）的含义是：“如果处理完全无效，那么在所有可能的随机分配中，有多大比例会产生像我们观察到的这样大或更大的组间差异。”因此，拒绝[零假设](@entry_id:265441)允许我们得出一个关于**样本内因果效应**的强结论：“对于这20名受试者，药物的分配导致了[血压](@entry_id:177896)下降。”这个结论的假设更少，但其范围也更窄，它本身并不直接推广到总体。

#### [不确定性量化](@entry_id:138597)：自助法

参数化与非[参数化](@entry_id:272587)的思想也延伸到了评估[模型不确定性](@entry_id:265539)的方法中，例如**[自助法](@entry_id:139281) (Bootstrap)**。

**非[参数化](@entry_id:272587)自助法**是一种“模型无关”的方法。它直接从原始观测数据中有放回地[重复抽样](@entry_id:274194)，生成许多“自助样本集”。然后，在每个自助样本集上重新估计模型，通过观察估计结果在这些样本集上的变化来评估其不确定性（如置信区间）。这种方法假设[经验分布](@entry_id:274074)是真实[分布](@entry_id:182848)的一个良好近似，并直接利用它来模拟抽样变异 。在[系统发育学](@entry_id:147399)中，这意味着从原始[序列比对](@entry_id:172191)的列（位点）中有放回地抽样，以构建新的比对矩阵。

**[参数化](@entry_id:272587)[自助法](@entry_id:139281)**则是一种“模型依赖”的方法。其过程是：首先，用原始数据拟合一个参数化模型，得到参数的最佳估计值。然后，假定这个拟合好的模型就是真实的数据生成器，并用它来**模拟**生成许多新的数据集。后续步骤与非[参数化](@entry_id:272587)自助法相同。这种方法的有效性完全取决于初始参数化模型的设定是否正确 。在[系统发育学](@entry_id:147399)中，这意味着使用[最大似然](@entry_id:146147)法找到的最佳树和[替换模型](@entry_id:177799)参数，来模拟全新的DNA序列。

### 理论基础 (高级主题)

为了更深刻地理解这两种方法的根本差异，我们可以借助[统计学习理论](@entry_id:274291)中的一些高级概念。

#### 维度灾难与极小化极大[收敛率](@entry_id:146534)

我们可以用**[极小化极大风险](@entry_id:751993) (minimax risk)** 来精确刻画模型在最坏情况下的学习效率。对于回归问题 $Y = f(X) + \varepsilon$，均方[积分误差](@entry_id:171351)（MISE）的[极小化极大风险](@entry_id:751993)衡量了任何估计器 $\hat{f}$ 在某个函数类别 $\mathcal{F}$ 上可能遇到的最差表现。

理论研究表明，两种方法的[收敛率](@entry_id:146534)存在天壤之别 ：
-   在**[参数化](@entry_id:272587)**设定下，如果真实函数 $f$ 确实属于一个由 $k$ 个参数决定的[光滑模](@entry_id:752104)型族，那么最优估计的风险[收敛速度](@entry_id:636873)为：
    $$R_n^* \asymp n^{-1}$$
    这个[收敛率](@entry_id:146534)非常快，并且关键在于，它**与数据的维度 $d$ 无关**。这是对模型假设正确的巨大回报。

-   在**非[参数化](@entry_id:272587)**设定下，假设真实函数 $f$ 属于一个光滑度为 $s$ 的函数空间（例如Hölder或Sobolev空间），其风险[收敛速度](@entry_id:636873)为：
    $$R_n^* \asymp n^{-\frac{2s}{2s+d}}$$
    这个[收敛率](@entry_id:146534)揭示了几个关键点：
    1.  它比参数化速率 $n^{-1}$ 慢得多。
    2.  它依赖于真实函数的光滑度 $s$：函数越光滑（$s$ 越大），收敛越快。
    3.  它严重依赖于数据的维度 $d$。随着维度 $d$ 的增加，指数 $\frac{2s}{2s+d}$ 趋于0，[收敛速度](@entry_id:636873)急剧下降。这就是著名的**[维度灾难](@entry_id:143920) (curse of dimensionality)**。在高维空间中，数据点变得极其稀疏，要可靠地学习一个未知的函数形式需要天文数字的样本量。这是为[模型灵活性](@entry_id:637310)付出的代价。

#### 可识别性：从秩条件到完备性

**可识别性 (Identifiability)** 是一个根本性的问题：我们能否从一个理想的、无穷大的数据集中唯一地确定模型的参数或函数？如果一个模型是不可识别的，那么即使有无限数据，也存在多个不同的参数（或函数）能够完美解释数据，使得推断变得毫无意义。

参数化与非[参数化](@entry_id:272587)模型的可识别性条件，优美地展示了有限维线性代数到无穷维泛函分析的延伸 。以工具变量（Instrumental Variable, IV）回归为例：
-   在线性[参数化](@entry_id:272587)IV模型 $Y = X^{\top}\beta + U$ 中，参数 $\beta$ 的可识别性等价于一个**满秩条件**：矩阵 $\mathbb{E}[Z X^{\top}]$ 必须具有[满列秩](@entry_id:749628)。这本质上是要求一个从[参数空间](@entry_id:178581) $\mathbb{R}^p$ 到矩空间 $\mathbb{R}^k$ 的线性映射（由矩阵 $\mathbb{E}[Z X^{\top}]$ 代表）是单射的。

-   在非[参数化](@entry_id:272587)IV模型 $Y = g(X) + U$ 中，函数 $g$ 的可识别性问题转化为一个无穷维问题。其核心方程为 $\mathbb{E}[Y | Z] = \mathbb{E}[g(X) | Z]$。为了唯一地从 $\mathbb{E}[Y | Z]$ 中“解”出 $g$，我们需要保证算子 $T(g) = \mathbb{E}[g(X) | Z]$ 是[单射](@entry_id:183792)的。这个[单射性](@entry_id:147722)条件在[泛函分析](@entry_id:146220)中被称为**完备性条件 (completeness condition)**。它要求，如果 $\mathbb{E}[h(X) | Z] = 0$ 对几乎所有 $Z$ 成立，那么必然有 $h(X) = 0$。

因此，非[参数化](@entry_id:272587)模型中的“完备性”概念，正是参数化模型中“矩阵满秩”条件在[无穷维空间](@entry_id:141268)中的深刻类比。两者都保证了从未知目标（参数或函数）到可观测矩的映射是可逆的。

#### 相关性背景下的可解释性

最后，在追求模型**[可解释性](@entry_id:637759) (interpretability)** 时，两种方法都面临着共同的挑战，尤其是在预测变量高度相关（即存在多重共线性）的情况下 。
-   对于[参数化](@entry_id:272587)的线性模型，特征的重要性通常与[标准化](@entry_id:637219)后的[回归系数](@entry_id:634860) $|\hat{\beta}_j|$ 的大小相关联。然而，当特征 $X_1$ 和 $X_2$ 高度相关时，$\hat{\beta}_1$ 和 $\hat{\beta}_2$ 的估计会变得极不稳定，其[方差](@entry_id:200758)会急剧膨胀。它们的具体数值和相对大小可能因数据的微小扰动而剧烈变化，因此不能作为衡量各自独立重要性的可靠依据。

-   对于非[参数化](@entry_id:272587)模型（如[随机森林](@entry_id:146665)），一个常用的重要性度量是**[置换](@entry_id:136432)重要性 (permutation importance)**。该方法通过打乱单个特征的取值，观察模型预测误差的增加量来评估其重要性。然而，当特征相关时，这种“朴素”的[置换](@entry_id:136432)会破坏特征间的联合分布，创造出在现实世界中不可能出现的“[分布](@entry_id:182848)外”数据点。模型在这些怪异数据点上的糟糕表现可能导致其重要性被高估或低估，并且该方法无法区分冗余特征的共同贡献和单个特征的独特贡献。

这个例子警示我们，无论是看似简单的参数化模型，还是灵活的非参数化模型，当[数据结构](@entry_id:262134)变得复杂（如存在[多重共线性](@entry_id:141597)）时，简单的[可解释性方法](@entry_id:636310)都会失效。追求有意义的[特征重要性](@entry_id:171930)，需要更复杂的模型和更精巧的归因技术，例如分组重要性或者条件[置换](@entry_id:136432)重要性。这再次凸显了[统计学习](@entry_id:269475)中不存在“免费午餐”的永恒主题。