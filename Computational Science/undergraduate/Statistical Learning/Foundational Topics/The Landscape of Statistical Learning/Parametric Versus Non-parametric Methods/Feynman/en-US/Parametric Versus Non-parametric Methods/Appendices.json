{
    "hands_on_practices": [
        {
            "introduction": "One of the most fundamental distinctions between parametric and non-parametric methods is their sensitivity to the scale of input features. This exercise provides a hands-on comparison of a standardized linear regression model, a parametric approach, with a non-parametric kernel regression model. By implementing this comparison, you will directly observe how feature scaling can drastically alter the performance of a distance-based method while leaving a properly standardized parametric model unchanged, revealing a key practical difference in their application .",
            "id": "3155821",
            "problem": "You will implement a principled comparison between a parametric method and a non-parametric method to study the effect of feature rescaling on interpretability and predictive performance. The parametric method is ordinary least squares (OLS) linear regression applied to standardized predictors, and the non-parametric method is the Nadaraya–Watson kernel regression with a Gaussian kernel and a fixed bandwidth. You must derive a testable performance drift metric with respect to rescaling of input features and implement it as a complete program.\n\nData generating process:\n- Let the random vector of predictors be $X = (X_{1}, X_{2})$, where $X_{1} \\sim \\mathrm{Uniform}[-2,2]$ and $X_{2} \\sim \\mathrm{Uniform}[-1,1]$, independent.\n- Let the noise be $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ independent of $X$, with $\\sigma = 0.5$.\n- Define the response by\n$$\nY \\;=\\; 3 X_{1} \\;-\\; 2 X_{2} \\;+\\; 0.3 \\sin(3 X_{1}) \\;+\\; \\varepsilon.\n$$\n\nSampling:\n- Generate a training sample of size $n_{\\text{train}} = 200$ and a test sample of size $n_{\\text{test}} = 100$ using a pseudorandom number generator with fixed seed $7$ so that results are deterministic.\n\nRescaling protocol:\n- For a scaling factor $s > 0$, define the rescaled predictors as $X^{(s)} = s \\cdot X$, applied componentwise to both training and test predictors.\n\nParametric method (standardized OLS):\n- For each $s$, compute the standardized predictors $Z^{(s)} = \\mathrm{standardize}(X^{(s)})$ using training-set statistics, where standardization is defined by $Z^{(s)}_{j} = \\frac{X^{(s)}_{j} - \\mu^{(s)}_{j,\\text{train}}}{\\sigma^{(s)}_{j,\\text{train}}}$ for each predictor index $j \\in \\{1,2\\}$, with $\\mu^{(s)}_{j,\\text{train}}$ and $\\sigma^{(s)}_{j,\\text{train}}$ being the sample mean and sample standard deviation computed on the rescaled training set. Fit an OLS model with intercept to predict $Y$ from $Z^{(s)}$. Let $\\widehat{\\beta}^{(s)} \\in \\mathbb{R}^{2}$ denote the vector of slope coefficients on the standardized predictors (excluding the intercept). Define the parametric coefficient drift relative to the baseline $s=1$ as\n$$\nd_{\\text{param}}(s) \\;=\\; \\big\\| \\widehat{\\beta}^{(s)} - \\widehat{\\beta}^{(1)} \\big\\|_{2}.\n$$\n\nNon-parametric method (Gaussian kernel regression):\n- For each $s$, fit a Nadaraya–Watson estimator on the rescaled training set without any standardization. Use an isotropic Gaussian kernel with fixed bandwidth $h = 0.5$. For a test point $x \\in \\mathbb{R}^{2}$, the prediction is\n$$\n\\widehat{m}^{(s)}(x) \\;=\\; \\frac{\\sum_{i=1}^{n_{\\text{train}}} \\exp\\!\\left(-\\frac{\\|x - x^{(s)}_{i}\\|_{2}^{2}}{2 h^{2}}\\right) \\, y_{i}}{\\sum_{i=1}^{n_{\\text{train}}} \\exp\\!\\left(-\\frac{\\|x - x^{(s)}_{i}\\|_{2}^{2}}{2 h^{2}}\\right)},\n$$\nwhere $x^{(s)}_{i}$ are the rescaled training predictors and $y_{i}$ are the corresponding training responses. If the denominator is smaller than a numerical threshold $\\tau = 10^{-12}$ at any test point, define $\\widehat{m}^{(s)}(x)$ to be the training response mean to ensure numerical stability. On the rescaled test set, compute the mean squared error (MSE)\n$$\n\\mathrm{MSE}_{\\text{kernel}}(s) \\;=\\; \\frac{1}{n_{\\text{test}}} \\sum_{j=1}^{n_{\\text{test}}} \\Big( y^{\\text{test}}_{j} - \\widehat{m}^{(s)}(x^{(s),\\text{test}}_{j}) \\Big)^{2}.\n$$\nDefine the non-parametric performance drift relative to the baseline $s=1$ as\n$$\nd_{\\text{kernel}}(s) \\;=\\; \\big| \\mathrm{MSE}_{\\text{kernel}}(s) - \\mathrm{MSE}_{\\text{kernel}}(1) \\big|.\n$$\n\nTest suite:\n- Use the scaling set $S = \\{0.5, 1.0, 2.0, 10.0\\}$.\n- For each $s \\in S$, compute $d_{\\text{param}}(s)$ and $d_{\\text{kernel}}(s)$ as defined above.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order corresponding to the elements of $S$:\n$$\n\\big[ d_{\\text{param}}(0.5), \\; d_{\\text{kernel}}(0.5), \\; d_{\\text{param}}(1.0), \\; d_{\\text{kernel}}(1.0), \\; d_{\\text{param}}(2.0), \\; d_{\\text{kernel}}(2.0), \\; d_{\\text{param}}(10.0), \\; d_{\\text{kernel}}(10.0) \\big].\n$$\nEach entry must be a real number (a float). No additional text should be printed.",
            "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. It provides a complete and consistent set of definitions and parameters for a well-defined computational experiment in statistical learning. The task is to compare the effect of feature scaling on a parametric model (Ordinary Least Squares on standardized data) and a non-parametric model (Nadaraya–Watson kernel regression). This will be accomplished by implementing the specified models and calculating drift metrics for their parameters and performance, respectively.\n\n### Theoretical Foundation\n\nThe core of this problem rests on a fundamental distinction between how parametric and non-parametric models handle input features.\n\n1.  **Parametric Model: Invariance to Scaling via Standardization**\n\n    The parametric method specified is Ordinary Least Squares (OLS) regression applied to standardized predictors. For a given predictor feature $X_j$ and a scaling factor $s > 0$, the rescaled feature is $X_j^{(s)} = s X_j$. The standardization process involves centering and scaling the feature using its sample mean $\\mu[ \\cdot ]$ and sample standard deviation $\\sigma[ \\cdot ]$, computed on the training data.\n\n    Let $\\mu_{j,\\text{train}} = \\mu[X_{j,\\text{train}}]$ and $\\sigma_{j,\\text{train}} = \\sigma[X_{j,\\text{train}}]$. The mean and standard deviation of the rescaled training data are:\n    $$\n    \\mu^{(s)}_{j,\\text{train}} = \\mu[s X_{j,\\text{train}}] = s \\cdot \\mu[X_{j,\\text{train}}] = s \\mu_{j,\\text{train}}\n    $$\n    $$\n    \\sigma^{(s)}_{j,\\text{train}} = \\sigma[s X_{j,\\text{train}}] = s \\cdot \\sigma[X_{j,\\text{train}}] = s \\sigma_{j,\\text{train}}\n    $$\n    These scaling properties are fundamental to the mean and standard deviation operators. The standardized predictor for the rescaled data, $Z^{(s)}_j$, is then:\n    $$\n    Z^{(s)}_j = \\frac{X^{(s)}_j - \\mu^{(s)}_{j,\\text{train}}}{\\sigma^{(s)}_{j,\\text{train}}} = \\frac{s X_j - s \\mu_{j,\\text{train}}}{s \\sigma_{j,\\text{train}}} = \\frac{s (X_j - \\mu_{j,\\text{train}})}{s \\sigma_{j,\\text{train}}} = \\frac{X_j - \\mu_{j,\\text{train}}}{\\sigma_{j,\\text{train}}} = Z^{(1)}_j\n    $$\n    This demonstrates that the standardized predictor $Z^{(s)}_j$ is mathematically identical for any scaling factor $s > 0$. Since the OLS model is fitted to predict the fixed response vector $Y$ from the design matrix of these standardized predictors, and this design matrix is invariant to $s$, the resulting vector of OLS slope coefficients $\\widehat{\\beta}^{(s)}$ must also be invariant. Consequently, the parametric coefficient drift, $d_{\\text{param}}(s) = \\| \\widehat{\\beta}^{(s)} - \\widehat{\\beta}^{(1)} \\|_{2}$, is theoretically expected to be exactly $0$ for all $s$. Any non-zero result would be attributable to floating-point precision limitations.\n\n2.  **Non-Parametric Model: Sensitivity to Scaling via Distance Metric**\n\n    The non-parametric method is the Nadaraya–Watson kernel estimator. Its prediction for a point $x$ depends on the weighted average of training responses $y_i$, where the weights are determined by a kernel function of the distance between $x$ and the training points $x_i$. The specified isotropic Gaussian kernel uses the Euclidean distance.\n\n    When the features are rescaled by a factor $s$, a test point $x$ becomes $s x$ and the training points $x_i$ become $s x_i$. The squared Euclidean distance term in the kernel's exponent becomes:\n    $$\n    \\| s x - s x_i \\|_2^2 = \\| s(x - x_i) \\|_2^2 = s^2 \\| x - x_i \\|_2^2\n    $$\n    The kernel function, with a fixed bandwidth $h$, is therefore affected by the scaling:\n    $$\n    K_h(s x, s x_i) = \\exp\\left(-\\frac{\\| s x - s x_i \\|_{2}^{2}}{2 h^{2}}\\right) = \\exp\\left(-\\frac{s^2 \\| x - x_i \\|_{2}^{2}}{2 h^{2}}\\right) = \\exp\\left(-\\frac{\\| x - x_i \\|_{2}^{2}}{2 (h/s)^{2}}\\right)\n    $$\n    This shows that scaling the data by a factor $s$ is equivalent to using the original, unscaled data with an effective bandwidth of $h_{\\text{eff}} = h/s$. The bandwidth parameter $h$ critically controls the bias-variance trade-off of the estimator. A smaller bandwidth leads to a more flexible (low bias, high variance) fit, while a larger bandwidth results in a smoother (high bias, low variance) fit. Since the scaling factor $s$ directly modulates the effective bandwidth, it alters the model's complexity and its ability to fit the underlying function. This will, in general, change the model's predictive performance on the test set, as measured by the Mean Squared Error (MSE). Therefore, the non-parametric performance drift, $d_{\\text{kernel}}(s) = | \\mathrm{MSE}_{\\text{kernel}}(s) - \\mathrm{MSE}_{\\text{kernel}}(1) |$, is expected to be non-zero for $s \\neq 1$.\n\n### Computational Procedure\n\nThe analysis will proceed as follows:\n\n1.  **Data Generation**: Using a fixed random seed of $7$, generate a training set of size $n_{\\text{train}} = 200$ and a test set of size $n_{\\text{test}} = 100$. The predictors $X_1$ and $X_2$ are drawn from their respective uniform distributions, and the response $Y$ is computed from the specified data generating process including the sinusoidal non-linearity and Gaussian noise.\n\n2.  **Iterative Analysis over Scaling Factors**: For each scaling factor $s$ in the set $S = \\{0.5, 1.0, 2.0, 10.0\\}$:\n    *   The training and test predictors, $X_{\\text{train}}$ and $X_{\\text{test}}$, are rescaled by multiplying them component-wise by $s$ to produce $X^{(s)}_{\\text{train}}$ and $X^{(s)}_{\\text{test}}$.\n    *   **Parametric Analysis**:\n        *   The sample mean and sample standard deviation (with $n-1$ in the denominator, `ddof=1`) are computed for each feature of $X^{(s)}_{\\text{train}}$.\n        *   These statistics are used to standardize both $X^{(s)}_{\\text{train}}$ and $X^{(s)}_{\\text{test}}$.\n        *   An OLS model with an intercept is fitted to predict $Y_{\\text{train}}$ from the standardized training predictors. The two slope coefficients, forming the vector $\\widehat{\\beta}^{(s)}$, are stored.\n    *   **Non-Parametric Analysis**:\n        *   The Nadaraya–Watson estimator is fitted using the rescaled training data $(X^{(s)}_{\\text{train}}, Y_{\\text{train}})$ with a fixed bandwidth $h=0.5$.\n        *   Predictions are made for each point in the rescaled test set, $X^{(s)}_{\\text{test}}$. The numerical stability check (denominator against $\\tau=10^{-12}$) is implemented.\n        *   The Mean Squared Error, $\\mathrm{MSE}_{\\text{kernel}}(s)$, between the predictions and the true test responses $Y_{\\text{test}}$ is calculated and stored.\n\n3.  **Drift Calculation**:\n    *   After iterating through all scaling factors in $S$, the baseline results corresponding to $s=1.0$, namely $\\widehat{\\beta}^{(1)}$ and $\\mathrm{MSE}_{\\text{kernel}}(1)$, are retrieved.\n    *   For each $s \\in S$, the drift metrics are computed:\n        *   $d_{\\text{param}}(s) = \\| \\widehat{\\beta}^{(s)} - \\widehat{\\beta}^{(1)} \\|_{2}$.\n        *   $d_{\\text{kernel}}(s) = | \\mathrm{MSE}_{\\text{kernel}}(s) - \\mathrm{MSE}_{\\text{kernel}}(1) |$.\n\n4.  **Final Output**: The computed drift values are collated into a single list in the specified order and printed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a comparison between a parametric (standardized OLS) and a \n    non-parametric (Nadaraya-Watson) method to study the effect of \n    feature rescaling.\n    \"\"\"\n    \n    # Problem parameters\n    n_train = 200\n    n_test = 100\n    noise_sigma = 0.5\n    seed = 7\n    h = 0.5\n    tau = 1e-12\n    scaling_factors = [0.5, 1.0, 2.0, 10.0]\n\n    # Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # --- Data Generation ---\n    # Training data\n    X1_train = rng.uniform(-2, 2, size=n_train)\n    X2_train = rng.uniform(-1, 1, size=n_train)\n    X_train = np.stack([X1_train, X2_train], axis=1)\n    eps_train = rng.normal(0, noise_sigma, size=n_train)\n    Y_train = 3 * X1_train - 2 * X2_train + 0.3 * np.sin(3 * X1_train) + eps_train\n\n    # Test data\n    X1_test = rng.uniform(-2, 2, size=n_test)\n    X2_test = rng.uniform(-1, 1, size=n_test)\n    X_test = np.stack([X1_test, X2_test], axis=1)\n    eps_test = rng.normal(0, noise_sigma, size=n_test)\n    Y_test = 3 * X1_test - 2 * X2_test + 0.3 * np.sin(3 * X1_test) + eps_test\n    \n    Y_train_mean = np.mean(Y_train)\n\n    # Storage for results\n    beta_coeffs = {}\n    kernel_mses = {}\n\n    for s in scaling_factors:\n        # --- Rescaling ---\n        X_train_s = s * X_train\n        X_test_s = s * X_test\n\n        # --- Parametric Method (Standardized OLS) ---\n        mu_s = np.mean(X_train_s, axis=0)\n        # Use ddof=1 for sample standard deviation as per statistical convention\n        sigma_s = np.std(X_train_s, axis=0, ddof=1)\n        \n        # Handle case where a feature has zero standard deviation\n        sigma_s[sigma_s == 0] = 1.0\n\n        Z_train_s = (X_train_s - mu_s) / sigma_s\n        Z_train_s_intercept = np.hstack([np.ones((n_train, 1)), Z_train_s])\n        \n        # Solve OLS using least squares\n        coeffs_s, _, _, _ = np.linalg.lstsq(Z_train_s_intercept, Y_train, rcond=None)\n        beta_coeffs[s] = coeffs_s[1:]  # Store only slope coefficients\n\n        # --- Non-Parametric Method (Nadaraya-Watson) ---\n        Y_pred_kernel = np.zeros(n_test)\n        \n        for j in range(n_test):\n            x_test_point = X_test_s[j]\n            \n            # Calculate squared Euclidean distances from the test point to all training points\n            dists_sq = np.sum((X_train_s - x_test_point)**2, axis=1)\n            \n            # Compute Gaussian kernel weights\n            weights = np.exp(-dists_sq / (2 * h**2))\n            \n            # Calculate numerator and denominator for the prediction\n            numerator = np.sum(weights * Y_train)\n            denominator = np.sum(weights)\n\n            if denominator  tau:\n                Y_pred_kernel[j] = Y_train_mean\n            else:\n                Y_pred_kernel[j] = numerator / denominator\n        \n        # Calculate MSE for the kernel regression\n        mse_s = np.mean((Y_test - Y_pred_kernel)**2)\n        kernel_mses[s] = mse_s\n\n    # --- Drift Calculation ---\n    results = []\n    beta_baseline = beta_coeffs[1.0]\n    mse_baseline = kernel_mses[1.0]\n\n    for s in scaling_factors:\n        # Parametric coefficient drift\n        d_param = np.linalg.norm(beta_coeffs[s] - beta_baseline)\n        \n        # Non-parametric performance drift\n        d_kernel = np.abs(kernel_mses[s] - mse_baseline)\n        \n        results.append(d_param)\n        results.append(d_kernel)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A critical test for any predictive model is its behavior when asked to extrapolate far beyond the boundaries of the training data. This practice contrasts a parametric linear trend model with a non-parametric Gaussian Process (GP) to predict geospatial sensor readings. You will implement both models to see how the parametric model's rigid structure leads to confident but potentially misleading extrapolations, whereas the GP's uncertainty appropriately increases in regions without data, a hallmark of robust non-parametric inference .",
            "id": "3155828",
            "problem": "You are given a small geospatial dataset consisting of sensor locations and scalar readings. You will compare two models to quantify predictive uncertainty at specified locations where no sensor reading is available. The two models are: a parametric linear trend in latitude and longitude, and a non-parametric Gaussian Process (GP) kriging model. Your task is to compute, for each test location, the posterior variance of the latent signal (not including observation noise) under each model, and assemble the results in a single line of output in the format specified below.\n\nFundamental base:\n- Parametric model definition: a finite-dimensional linear model is defined by an assumed functional form and a finite parameter vector. Consider the ordinary least squares (OLS) linear model with Gaussian noise, where the response at a location with latitude $lat$ and longitude $lon$ is modeled as $y = \\beta_{0} + \\beta_{1} \\, lat + \\beta_{2} \\, lon + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$, and the parameters are $\\beta_{0}, \\beta_{1}, \\beta_{2}$.\n- Non-parametric model definition: a Gaussian Process (GP) places a distribution over functions, defined by a mean function and a covariance kernel, in an infinite-dimensional function space. For locations $x = (lat, lon)$, the prior is $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$, and observations follow $y = f(x) + \\eta$, with $\\eta \\sim \\mathcal{N}(0, \\tau^{2})$.\n\nWell-tested formulas and facts to be used as starting points:\n- In the OLS linear model with Gaussian noise, the parameter estimator has sampling distribution $ \\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}, \\sigma^{2} (X^{\\top} X)^{-1}) $, where $X$ is the design matrix with rows $x_{i}^{\\top} = [1, lat_{i}, lon_{i}]$, and the unbiased estimator of the noise variance is $\\hat{\\sigma}^{2} = \\mathrm{RSS} / (n - p)$, with $\\mathrm{RSS}$ the residual sum of squares, $n$ the number of observations, and $p$ the number of parameters.\n- In Gaussian Process regression with observation noise, the joint distribution of training function values and a test function value is multivariate Gaussian, and conditioning yields a Gaussian posterior whose variance depends only on the kernel and locations.\n\nYour program must use the following dataset and model hyperparameters. All angles are in degrees. The coordinates are latitudes and longitudes in degrees, and distances in the kernel are computed directly in degrees (no conversion to radians). There are $8$ sensors, with coordinates and scalar readings as follows:\n- Sensor $1$: $lat = -9$, $lon = -18$, $y = 9.7$.\n- Sensor $2$: $lat = -6$, $lon = 12$, $y = 4.8$.\n- Sensor $3$: $lat = 0$, $lon = 0$, $y = 9.7$.\n- Sensor $4$: $lat = 4$, $lon = -10$, $y = 14.5$.\n- Sensor $5$: $lat = 9$, $lon = 19$, $y = 10.3$.\n- Sensor $6$: $lat = -3$, $lon = 5$, $y = 7.3$.\n- Sensor $7$: $lat = 7$, $lon = -4$, $y = 14.4$.\n- Sensor $8$: $lat = -10$, $lon = 15$, $y = 2.4$.\n\nParametric model:\n- Use the linear trend model $y = \\beta_{0} + \\beta_{1} \\, lat + \\beta_{2} \\, lon + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$.\n- Fit the model by ordinary least squares (OLS), estimate $\\hat{\\sigma}^{2}$ by $\\hat{\\sigma}^{2} = \\mathrm{RSS}/(n-p)$ with $n = 8$ and $p = 3$.\n- For a test location with feature vector $x_{*}^{\\top} = [1, lat_{*}, lon_{*}]$, compute the posterior variance of the latent signal (the mean function) under the parametric model as $ \\mathrm{Var}_{\\text{param}}(x_{*}) = \\hat{\\sigma}^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}$, where $X$ is the $n \\times p$ design matrix of the training sensors.\n\nNon-parametric GP kriging model:\n- Use a zero-mean Gaussian Process (GP) prior with an anisotropic squared exponential (Gaussian) kernel:\n$$\nk\\big((lat, lon), (lat', lon')\\big) = s^{2} \\exp\\left( -\\tfrac{1}{2} \\left[ \\left( \\frac{lat - lat'}{\\ell_{\\mathrm{lat}}} \\right)^{2} + \\left( \\frac{lon - lon'}{\\ell_{\\mathrm{lon}}} \\right)^{2} \\right] \\right).\n$$\n- Hyperparameters are fixed as $s^{2} = 9$, $\\ell_{\\mathrm{lat}} = 6$, $\\ell_{\\mathrm{lon}} = 10$, and observation noise variance $\\tau^{2} = 0.25$.\n- For a test location $x_{*}$, the posterior variance of the latent signal (excluding observation noise) is\n$$\n\\mathrm{Var}_{\\text{GP}}(x_{*}) = k(x_{*}, x_{*}) - \\boldsymbol{k}_{*}^{\\top} (K + \\tau^{2} I)^{-1} \\boldsymbol{k}_{*},\n$$\nwhere $K$ is the $n \\times n$ kernel matrix computed over training sensor locations using $k$, and $\\boldsymbol{k}_{*}$ is the $n \\times 1$ kernel vector between $x_{*}$ and each training sensor location.\n\nTest suite:\nCompute both $\\mathrm{Var}_{\\text{param}}(x_{*})$ and $\\mathrm{Var}_{\\text{GP}}(x_{*})$ for the following $4$ test locations, all in degrees:\n- Test $1$: $lat_{*} = 20$, $lon_{*} = 35$ (far outside the training region).\n- Test $2$: $lat_{*} = 1$, $lon_{*} = 2$ (near the center of the training region).\n- Test $3$: $lat_{*} = -10$, $lon_{*} = 15$ (exactly at a training sensor location).\n- Test $4$: $lat_{*} = 12$, $lon_{*} = -18$ (outside but near the training region).\n\nFinal output format:\nYour program should produce a single line of output containing the $8$ floating-point results as a comma-separated list enclosed in square brackets, in the exact order\n$$\n[\\mathrm{Var}_{\\text{param}}(x_{1}), \\mathrm{Var}_{\\text{GP}}(x_{1}), \\mathrm{Var}_{\\text{param}}(x_{2}), \\mathrm{Var}_{\\text{GP}}(x_{2}), \\mathrm{Var}_{\\text{param}}(x_{3}), \\mathrm{Var}_{\\text{GP}}(x_{3}), \\mathrm{Var}_{\\text{param}}(x_{4}), \\mathrm{Var}_{\\text{GP}}(x_{4})].\n$$\n\nAngle unit specification:\nAll angles are in degrees. Use degrees consistently for the kernel distance computation. No conversion to radians is required.\n\nNote: The problem asks for the uncertainty of the latent signal under each model, not the variance of a noisy observation. Therefore, do not add the observation noise variance to the predictive variances.",
            "solution": "We compare parametric and non-parametric uncertainty quantification from first principles, starting from core definitions of linear regression and Gaussian Process (GP) regression. We will derive the posterior variance of the latent signal at a new location under both models and then implement the corresponding algorithms.\n\nParametric model (finite-dimensional linear model):\n- Definition: A parametric model specifies a finite-dimensional parameter vector and a fixed functional form. For geospatial coordinates with latitude $lat$ and longitude $lon$, consider the linear model\n$$\ny_{i} = \\beta_{0} + \\beta_{1} \\, lat_{i} + \\beta_{2} \\, lon_{i} + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2}),\n$$\nfor $i = 1, \\ldots, n$.\n- Design matrix: Stack the predictors into a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with $p = 3$, where each row is $x_{i}^{\\top} = [1, lat_{i}, lon_{i}]$.\n- Estimation: Ordinary Least Squares (OLS) estimators minimize the residual sum of squares $\\mathrm{RSS} = \\sum_{i=1}^{n} (y_{i} - x_{i}^{\\top} \\hat{\\boldsymbol{\\beta}})^{2}$. Under the Gaussian noise assumption, the sampling distribution of the estimator is $ \\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}, \\sigma^{2} (X^{\\top} X)^{-1})$ and the unbiased estimator of the noise variance is $ \\hat{\\sigma}^{2} = \\mathrm{RSS} / (n - p)$.\n- Predictive mean and uncertainty of the latent signal: The latent signal (mean function) at a new point with feature vector $x_{*}^{\\top} = [1, lat_{*}, lon_{*}]$ is $m_{*} = x_{*}^{\\top} \\boldsymbol{\\beta}$. The estimator $x_{*}^{\\top} \\hat{\\boldsymbol{\\beta}}$ has variance\n$$\n\\mathrm{Var}\\left( x_{*}^{\\top} \\hat{\\boldsymbol{\\beta}} \\right) = \\sigma^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}.\n$$\nReplacing $\\sigma^{2}$ by $\\hat{\\sigma}^{2}$ yields the practical posterior variance estimate of the latent signal under the parametric model:\n$$\n\\mathrm{Var}_{\\text{param}}(x_{*}) = \\hat{\\sigma}^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}.\n$$\nNote that this is the uncertainty in the mean function estimate, not including the irreducible noise $\\sigma^{2}$ of a new observation.\n\nAlgorithmic design for the parametric model:\n- Construct $X$ by concatenating ones, $lat_{i}$, and $lon_{i}$ for all sensors.\n- Solve the normal equations to obtain $\\hat{\\boldsymbol{\\beta}}$ using $ \\hat{\\boldsymbol{\\beta}} = (X^{\\top} X)^{-1} X^{\\top} \\boldsymbol{y}$.\n- Compute residuals $\\boldsymbol{r} = \\boldsymbol{y} - X \\hat{\\boldsymbol{\\beta}}$ and $\\hat{\\sigma}^{2} = \\boldsymbol{r}^{\\top} \\boldsymbol{r} / (n - p)$.\n- For each test location, form $x_{*}$ and compute $ \\mathrm{Var}_{\\text{param}}(x_{*}) = \\hat{\\sigma}^{2} \\, x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}$. Numerically, avoid explicit matrix inversion by solving $(X^{\\top} X) \\boldsymbol{w} = x_{*}$ and computing $x_{*}^{\\top} \\boldsymbol{w}$.\n\nNon-parametric model (Gaussian Process kriging):\n- Definition: A Gaussian Process (GP) places a distribution over functions $f$ with $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$. Observations follow $y_{i} = f(x_{i}) + \\eta_{i}$ with $\\eta_{i} \\sim \\mathcal{N}(0, \\tau^{2})$.\n- Kernel: Use the anisotropic squared exponential kernel in degrees with amplitude $s^{2}$ and length scales $\\ell_{\\mathrm{lat}}$ and $\\ell_{\\mathrm{lon}}$:\n$$\nk(x, x') = s^{2} \\exp\\left( -\\tfrac{1}{2} \\left[ \\left( \\frac{lat - lat'}{\\ell_{\\mathrm{lat}}} \\right)^{2} + \\left( \\frac{lon - lon'}{\\ell_{\\mathrm{lon}}} \\right)^{2} \\right] \\right).\n$$\n- Conditioning rule for multivariate Gaussians: The joint prior of $\\boldsymbol{f}$ at training inputs and $f_{*}$ at a test input is Gaussian\n$$\n\\begin{bmatrix}\n\\boldsymbol{f} \\\\\nf_{*}\n\\end{bmatrix}\n\\sim\n\\mathcal{N}\\left(\n\\begin{bmatrix}\n\\boldsymbol{0} \\\\\n0\n\\end{bmatrix},\n\\begin{bmatrix}\nK  \\boldsymbol{k}_{*} \\\\\n\\boldsymbol{k}_{*}^{\\top}  k(x_{*}, x_{*})\n\\end{bmatrix}\n\\right),\n$$\nand with observations $\\boldsymbol{y} = \\boldsymbol{f} + \\boldsymbol{\\eta}$, the posterior over $f_{*}$ given $\\boldsymbol{y}$ has variance\n$$\n\\mathrm{Var}_{\\text{GP}}(x_{*}) = k(x_{*}, x_{*}) - \\boldsymbol{k}_{*}^{\\top} (K + \\tau^{2} I)^{-1} \\boldsymbol{k}_{*}.\n$$\nCritically, the posterior variance depends only on the kernel and the sensor locations (and $\\tau^{2}$), not on the observed values $\\boldsymbol{y}$.\n- Numerical stability: Compute $(K + \\tau^{2} I)^{-1} \\boldsymbol{k}_{*}$ via a Cholesky factorization $K_{\\tau} = K + \\tau^{2} I = L L^{\\top}$, solve $L \\boldsymbol{v} = \\boldsymbol{k}_{*}$, and compute $ \\mathrm{Var}_{\\text{GP}}(x_{*}) = k(x_{*}, x_{*}) - \\boldsymbol{v}^{\\top} \\boldsymbol{v}$. Add a small jitter $\\epsilon$ to the diagonal if needed to ensure positive definiteness.\n\nAlgorithmic design for the GP model:\n- Compute the $n \\times n$ kernel matrix $K$ from training coordinates using $k$.\n- Form $K_{\\tau} = K + \\tau^{2} I + \\epsilon I$ with small $\\epsilon$.\n- For each test location, compute $\\boldsymbol{k}_{*}$ and $k(x_{*}, x_{*}) = s^{2}$, perform the Cholesky-based solve to get $\\boldsymbol{v}$, and compute $ \\mathrm{Var}_{\\text{GP}}(x_{*}) = s^{2} - \\boldsymbol{v}^{\\top} \\boldsymbol{v}$.\n\nTest coverage rationale:\n- Test $1$ is far outside the training region to examine extrapolation behavior. Parametric uncertainty grows according to $x_{*}^{\\top} (X^{\\top} X)^{-1} x_{*}$, while GP variance approaches the prior variance $s^{2}$ as $x_{*}$ moves far from sensors.\n- Test $2$ is near the center, examining interpolation where both methods have more information.\n- Test $3$ coincides with a training sensor location, testing the GP’s variance reduction at training inputs in the presence of noise variance $\\tau^{2}$, and the parametric model’s nonzero mean-function uncertainty at that input.\n- Test $4$ is outside but near the training region, probing boundary behavior.\n\nImplementation details:\n- All angle quantities are kept in degrees; kernel distances use degree differences scaled by the length scales.\n- The final output is a single list of $8$ floating-point numbers in the specified order. No physical units are required for the variances, and the angle unit is fixed to degrees.\n\nWith these principles and algorithms, the program computes $\\mathrm{Var}_{\\text{param}}(x_{*})$ and $\\mathrm{Var}_{\\text{GP}}(x_{*})$ for each test location and prints them in the required single-line format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_design_matrix(lats, lons):\n    \"\"\"\n    Construct the design matrix X for the linear model y = b0 + b1*lat + b2*lon + noise.\n    \"\"\"\n    n = len(lats)\n    X = np.column_stack([np.ones(n), np.array(lats, dtype=float), np.array(lons, dtype=float)])\n    return X\n\ndef fit_ols_params(X, y):\n    \"\"\"\n    Fit OLS parameters and estimate sigma^2 using unbiased estimator RSS/(n-p).\n    Returns beta_hat and sigma2_hat.\n    \"\"\"\n    # Solve normal equations without explicit inversion for numerical stability\n    XtX = X.T @ X\n    Xty = X.T @ y\n    beta_hat = np.linalg.solve(XtX, Xty)\n    residuals = y - X @ beta_hat\n    n, p = X.shape\n    rss = float(residuals.T @ residuals)\n    sigma2_hat = rss / (n - p)\n    return beta_hat, sigma2_hat, XtX\n\ndef parametric_variance_at(XtX, sigma2_hat, lat_star, lon_star):\n    \"\"\"\n    Compute Var_param(x*) = sigma2_hat * x_*^T (XtX)^-1 x_*\n    without explicitly inverting XtX by solving (XtX) w = x_*.\n    \"\"\"\n    x_star = np.array([1.0, float(lat_star), float(lon_star)])\n    # Solve (XtX) w = x_star\n    w = np.linalg.solve(XtX, x_star)\n    var_param = sigma2_hat * float(x_star.T @ w)\n    return var_param\n\ndef se_kernel(lat1, lon1, lat2, lon2, s2, ell_lat, ell_lon):\n    \"\"\"\n    Anisotropic squared exponential kernel in degrees.\n    \"\"\"\n    dlat = (lat1 - lat2) / ell_lat\n    dlon = (lon1 - lon2) / ell_lon\n    return s2 * np.exp(-0.5 * (dlat**2 + dlon**2))\n\ndef build_kernel_matrix(coords, s2, ell_lat, ell_lon):\n    \"\"\"\n    Build the kernel matrix K for training coordinates.\n    coords: list of (lat, lon)\n    \"\"\"\n    n = len(coords)\n    K = np.empty((n, n), dtype=float)\n    for i in range(n):\n        lat_i, lon_i = coords[i]\n        for j in range(n):\n            lat_j, lon_j = coords[j]\n            K[i, j] = se_kernel(lat_i, lon_i, lat_j, lon_j, s2, ell_lat, ell_lon)\n    return K\n\ndef gp_posterior_variance(K, coords, lat_star, lon_star, s2, ell_lat, ell_lon, tau2, jitter=1e-9):\n    \"\"\"\n    Compute GP posterior variance of the latent signal at x*:\n    Var_GP(x*) = k(x*,x*) - k_*^T (K + tau2*I)^-1 k_*\n    Uses Cholesky factorization for numerical stability.\n    \"\"\"\n    n = len(coords)\n    # Compute k_* vector\n    k_star = np.empty(n, dtype=float)\n    for i in range(n):\n        lat_i, lon_i = coords[i]\n        k_star[i] = se_kernel(lat_star, lon_star, lat_i, lon_i, s2, ell_lat, ell_lon)\n    # k(x*, x*) for SE kernel is s2\n    k_xx = s2\n    # Add noise variance and jitter to K\n    K_tilde = K + (tau2 + jitter) * np.eye(n)\n    # Cholesky factorization\n    try:\n        L = np.linalg.cholesky(K_tilde)\n    except np.linalg.LinAlgError:\n        # Increase jitter if needed\n        K_tilde = K + (tau2 + 1e-6) * np.eye(n)\n        L = np.linalg.cholesky(K_tilde)\n    # Solve L v = k_star\n    v = np.linalg.solve(L, k_star)\n    var_gp = float(k_xx - v.T @ v)\n    # Numerical safety: variance should be non-negative\n    if var_gp  0 and var_gp > -1e-10:\n        var_gp = 0.0\n    return var_gp\n\ndef solve():\n    # Training sensors (lat, lon, y)\n    sensors = [\n        (-9.0, -18.0, 9.7),\n        (-6.0,  12.0, 4.8),\n        ( 0.0,   0.0, 9.7),\n        ( 4.0, -10.0,14.5),\n        ( 9.0,  19.0,10.3),\n        (-3.0,   5.0, 7.3),\n        ( 7.0,  -4.0,14.4),\n        (-10.0, 15.0, 2.4),\n    ]\n    lats = [s[0] for s in sensors]\n    lons = [s[1] for s in sensors]\n    ys   = [s[2] for s in sensors]\n    y = np.array(ys, dtype=float)\n\n    # Design matrix and OLS fit\n    X = build_design_matrix(lats, lons)\n    beta_hat, sigma2_hat, XtX = fit_ols_params(X, y)\n\n    # GP hyperparameters\n    s2 = 9.0\n    ell_lat = 6.0  # degrees\n    ell_lon = 10.0 # degrees\n    tau2 = 0.25\n\n    # Build kernel matrix for training coords\n    coords = [(lat, lon) for lat, lon, _ in sensors]\n    K = build_kernel_matrix(coords, s2, ell_lat, ell_lon)\n\n    # Test cases: list of (lat*, lon*)\n    test_cases = [\n        (20.0,  35.0),   # Test 1: far outside\n        ( 1.0,   2.0),   # Test 2: near center\n        (-10.0, 15.0),   # Test 3: exactly a training sensor\n        (12.0, -18.0),   # Test 4: outside near region\n    ]\n\n    results = []\n    for lat_star, lon_star in test_cases:\n        var_param = parametric_variance_at(XtX, sigma2_hat, lat_star, lon_star)\n        var_gp = gp_posterior_variance(K, coords, lat_star, lon_star, s2, ell_lat, ell_lon, tau2)\n        results.append(var_param)\n        results.append(var_gp)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond single models, we can often achieve better predictive performance by combining multiple models into an ensemble. This exercise delves into two powerful but philosophically different ensemble strategies: parametric model averaging using Akaike Information Criterion (AIC) weights and non-parametric bagging of $k$-nearest neighbors. Through a Monte Carlo simulation, you will quantify how these methods affect bias and variance, offering insight into how parametric and non-parametric approaches can be used to manage model uncertainty and stability .",
            "id": "3155854",
            "problem": "Write a complete program that, using Monte Carlo simulation, compares a parametric model-averaged predictor based on Akaike Information Criterion (AIC) weights against a non-parametric bagged predictor based on bootstrap-aggregated $k$-nearest neighbors, and quantifies bias and variance at a fixed prediction point. The setting is univariate regression with additive Gaussian noise. The following core definitions are assumed as the fundamental base: (i) the Gaussian likelihood for linear regression, (ii) maximum likelihood estimation for regression parameters and noise variance, (iii) Akaike Information Criterion (AIC) for model comparison, (iv) bootstrap resampling as an empirical approximation to sampling from the data-generating distribution, (v) the $k$-nearest neighbors regression rule, and (vi) the definitions of bias and variance of an estimator. Use only these foundational elements to derive your program.\n\nYour program shall implement the following, grounded in first principles.\n\n1) Data-generating process and prediction target. For each independent replicate $r \\in \\{1,\\dots,R\\}$, draw $x_{i}^{(r)} \\sim \\mathrm{Uniform}[0,1]$ independently for $i \\in \\{1,\\dots,n\\}$, define a deterministic regression function $f(\\cdot)$ specified per test case, generate responses\n$$\ny_{i}^{(r)} = f\\!\\left(x_{i}^{(r)}\\right) + \\varepsilon_{i}^{(r)},\n$$\nwith $\\varepsilon_{i}^{(r)} \\sim \\mathcal{N}(0,\\sigma^{2})$ independent across $i$ and $r$, and then compute predictors at a fixed point $x_{0}$.\n\n2) Parametric model set and AIC-weighted averaging. Consider the candidate parametric models given by polynomial regression of degree $d \\in \\{1,2\\}$ with an intercept. For a fixed $d$, the mean function is\n$$\nm_{d}(x;\\boldsymbol{\\beta}) = \\sum_{j=0}^{d} \\beta_{j}\\, x^{j},\n$$\nwith parameters $\\boldsymbol{\\beta} \\in \\mathbb{R}^{d+1}$ and Gaussian errors of variance $\\sigma^{2}$. For each replicate, fit each model by maximum likelihood (equivalently, ordinary least squares under Gaussian noise) to obtain $\\widehat{\\boldsymbol{\\beta}}_{d}$ and the residual sum of squares. Let $\\widehat{\\sigma}^{2}_{d}$ denote the maximum likelihood estimate of $\\sigma^{2}$, and let $k_{d}$ be the number of estimated parameters in the model, which includes both regression coefficients and the noise variance, so $k_{d} = (d+1) + 1 = d + 2$. For Gaussian linear regression with maximum likelihood estimates, the log-likelihood at the fitted parameters is\n$$\n\\log L_{d} = -\\frac{n}{2}\\Big(\\log(2\\pi) + \\log(\\widehat{\\sigma}^{2}_{d}) + 1\\Big).\n$$\nThe Akaike Information Criterion is\n$$\n\\mathrm{AIC}_{d} = 2\\,k_{d} - 2\\,\\log L_{d}.\n$$\nCompute AIC differences $\\Delta_{d} = \\mathrm{AIC}_{d} - \\min_{d' \\in \\{1,2\\}} \\mathrm{AIC}_{d'}$ and the AIC weights\n$$\nw_{d} = \\frac{\\exp\\!\\left(-\\tfrac{1}{2}\\Delta_{d}\\right)}{\\sum_{d' \\in \\{1,2\\}} \\exp\\!\\left(-\\tfrac{1}{2}\\Delta_{d'}\\right)}.\n$$\nForm the parametric model-averaged predictor at $x_{0}$ for each replicate,\n$$\n\\widehat{f}_{\\mathrm{P}}^{(r)}(x_{0}) = \\sum_{d \\in \\{1,2\\}} w_{d}\\, m_{d}\\!\\left(x_{0};\\widehat{\\boldsymbol{\\beta}}_{d}\\right).\n$$\n\n3) Non-parametric $k$-nearest neighbors and bagging. For $k \\in \\mathbb{N}$, the non-parametric $k$-nearest neighbors predictor at $x_{0}$ uses the average of the $k$ response values whose covariates are closest to $x_{0}$ in Euclidean distance. For each replicate, compute:\n- The single-sample $k$-nearest neighbors predictor $\\widehat{f}_{\\mathrm{N,single}}^{(r)}(x_{0})$ using all $n$ training observations.\n- The bagged $k$-nearest neighbors predictor $\\widehat{f}_{\\mathrm{N,bag}}^{(r)}(x_{0})$ by drawing $B$ bootstrap resamples (each of size $n$ with replacement) from the training data, computing a $k$-nearest neighbors prediction $\\widehat{f}_{b}(x_{0})$ on each resample $b \\in \\{1,\\dots,B\\}$, and averaging,\n$$\n\\widehat{f}_{\\mathrm{N,bag}}^{(r)}(x_{0}) = \\frac{1}{B}\\sum_{b=1}^{B} \\widehat{f}_{b}(x_{0}).\n$$\n\n4) Bias and variance quantification at $x_{0}$. Let the true target be $f(x_{0})$. Across the $R$ replicates, for each method $M \\in \\{\\mathrm{P}, \\mathrm{N,single}, \\mathrm{N,bag}\\}$, estimate\n- the bias:\n$$\n\\widehat{\\mathrm{Bias}}_{M} = \\frac{1}{R}\\sum_{r=1}^{R}\\widehat{f}_{M}^{(r)}(x_{0}) - f(x_{0}),\n$$\n- and the variance:\n$$\n\\widehat{\\mathrm{Var}}_{M} = \\frac{1}{R-1}\\sum_{r=1}^{R}\\left(\\widehat{f}_{M}^{(r)}(x_{0}) - \\frac{1}{R}\\sum_{r'=1}^{R}\\widehat{f}_{M}^{(r')}(x_{0})\\right)^{2}.\n$$\nAdditionally, quantify variance reduction due to bagging for the non-parametric method via the ratio\n$$\n\\rho = \\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{N,bag}}}{\\widehat{\\mathrm{Var}}_{\\mathrm{N,single}}}.\n$$\n\n5) Test suite. Your program must run the following test cases, each defined by the tuple $(\\text{seed}, n, \\sigma, x_{0}, k, B, R, f)$, where $f$ specifies the deterministic regression function:\n- Case $1$: $(\\;12345,\\;50,\\;1.0,\\;0.5,\\;7,\\;200,\\;400,\\;f(x)=2+3x\\;)$.\n- Case $2$: $(\\;54321,\\;60,\\;0.5,\\;0.3,\\;9,\\;200,\\;400,\\;f(x)=\\sin(2\\pi x)\\;)$.\n- Case $3$: $(\\;20231105,\\;20,\\;2.0,\\;0.2,\\;5,\\;300,\\;300,\\;f(x)=x^{3}-x\\;)$.\n\n6) Output. For each case, produce a list of $7$ floating-point numbers in the following order:\n$[\\widehat{\\mathrm{Bias}}_{\\mathrm{P}},\\;\\widehat{\\mathrm{Var}}_{\\mathrm{P}},\\;\\widehat{\\mathrm{Bias}}_{\\mathrm{N,single}},\\;\\widehat{\\mathrm{Var}}_{\\mathrm{N,single}},\\;\\widehat{\\mathrm{Bias}}_{\\mathrm{N,bag}},\\;\\widehat{\\mathrm{Var}}_{\\mathrm{N,bag}},\\;\\rho]$.\nYour program should produce a single line of output containing the results as a comma-separated list of these per-case lists, enclosed in square brackets (for example, $[\\,[\\cdot],\\,[\\cdot],\\,[\\cdot]\\,]$). Report each floating-point number rounded to $6$ decimal places. No other text should be printed.\n\nAll quantities are purely mathematical; no physical units are involved. Angles, when appearing inside trigonometric functions, are in radians. Express all ratios and averages as decimals, not as percentages.",
            "solution": "The user-provided problem is a well-defined computational task in statistical learning and is therefore valid. It is scientifically grounded in established statistical theory, self-contained with all necessary parameters and definitions, and objective. The task is to write a program that performs a Monte Carlo simulation to compare a parametric model-averaged predictor against a non-parametric bagged predictor. The comparison is based on estimating the bias and variance of each predictor at a fixed point.\n\nThe solution is implemented by following the steps outlined in the problem statement. The overall structure is a main simulation loop that iterates over a number of replicates, $R$, to approximate the sampling distributions of the predictors.\n\n**1. Simulation Framework**\nThe core of the program is a loop that runs for $R$ replicates. For each replicate $r \\in \\{1,\\dots,R\\}$, a new dataset is generated according to the specified data-generating process. On this dataset, we train and evaluate three different predictors:\n1.  A parametric model-averaged predictor, $\\widehat{f}_{\\mathrm{P}}^{(r)}(x_0)$.\n2.  A standard non-parametric $k$-nearest neighbors predictor, $\\widehat{f}_{\\mathrm{N,single}}^{(r)}(x_0)$.\n3.  A bagged (bootstrap-aggregated) non-parametric $k$-nearest neighbors predictor, $\\widehat{f}_{\\mathrm{N,bag}}^{(r)}(x_0)$.\n\nThe predictions from each method at the target point $x_0$ are stored for each replicate. After all $R$ replicates are completed, these stored predictions are used to estimate the bias and variance of each predictor.\n\n**2. Data Generation**\nFor each replicate $r$, a training dataset $\\{ (x_i^{(r)}, y_i^{(r)}) \\}_{i=1}^n$ of size $n$ is created. The process is as follows:\n-   The covariates $x_i^{(r)}$ are drawn independently from a uniform distribution, $x_{i}^{(r)} \\sim \\mathrm{Uniform}[0,1]$.\n-   The responses $y_i^{(r)}$ are generated by the model $y_{i}^{(r)} = f(x_{i}^{(r)}) + \\varepsilon_{i}^{(r)}$, where $f(\\cdot)$ is the true deterministic regression function specified for the test case, and $\\varepsilon_{i}^{(r)}$ are independent noise terms drawn from a Gaussian distribution, $\\varepsilon_{i}^{(r)} \\sim \\mathcal{N}(0, \\sigma^2)$.\n\n**3. Parametric Model-Averaged Predictor ($\\widehat{f}_{\\mathrm{P}}$)**\nThis predictor is constructed by averaging the predictions of a set of candidate polynomial models, weighted by their Akaike Information Criterion (AIC) scores. The candidate models are polynomial regressions of degree $d \\in \\{1, 2\\}$, with a mean function $m_{d}(x;\\boldsymbol{\\beta}) = \\sum_{j=0}^{d} \\beta_{j}\\, x^{j}$.\n\nFor each replicate and for each degree $d$:\n-   Construct the design matrix $\\mathbf{X}_d$ of size $n \\times (d+1)$, where the $i$-th row is $[1, x_i, \\dots, x_i^d]$.\n-   Estimate the regression coefficients $\\widehat{\\boldsymbol{\\beta}}_d$ by solving the ordinary least squares problem, which is equivalent to maximum likelihood estimation under the Gaussian noise assumption. This is achieved by finding $\\widehat{\\boldsymbol{\\beta}}_d$ that minimizes the residual sum of squares (RSS), $\\|\\boldsymbol{y} - \\mathbf{X}_d \\boldsymbol{\\beta}\\|_2^2$.\n-   Calculate the maximum likelihood estimate of the error variance: $\\widehat{\\sigma}^2_d = \\frac{1}{n} \\mathrm{RSS}_d = \\frac{1}{n} \\sum_{i=1}^n (y_i - m_d(x_i; \\widehat{\\boldsymbol{\\beta}}_d))^2$.\n-   Determine the number of estimated parameters in the model, $k_d = (d+1) + 1 = d+2$, which includes the $d+1$ regression coefficients and the variance parameter $\\sigma^2$.\n-   Compute the maximized log-likelihood: $\\log L_d = -\\frac{n}{2}\\big(\\log(2\\pi) + \\log(\\widehat{\\sigma}^2_d) + 1\\big)$.\n-   Calculate the AIC for the model: $\\mathrm{AIC}_d = 2k_d - 2\\log L_d$.\n\nAfter computing $\\mathrm{AIC}_1$ and $\\mathrm{AIC}_2$:\n-   Determine the minimum AIC value, $\\mathrm{AIC}_{\\min} = \\min(\\mathrm{AIC}_1, \\mathrm{AIC}_2)$.\n-   Compute the AIC differences, $\\Delta_d = \\mathrm{AIC}_d - \\mathrm{AIC}_{\\min}$.\n-   Calculate the Akaike weights for each model: $w_d = \\frac{\\exp(-\\frac{1}{2}\\Delta_d)}{\\sum_{d' \\in \\{1,2\\}} \\exp(-\\frac{1}{2}\\Delta_{d'})}$. These weights represent the relative likelihood of each model being the best approximation to the true data-generating process.\n-   The final model-averaged prediction at the point $x_0$ for the replicate is the weighted average of the individual model predictions: $\\widehat{f}_{\\mathrm{P}}^{(r)}(x_0) = \\sum_{d \\in \\{1,2\\}} w_d \\, m_d(x_0; \\widehat{\\boldsymbol{\\beta}}_d)$.\n\n**4. Non-Parametric Predictors ($\\widehat{f}_{\\mathrm{N,single}}$ and $\\widehat{f}_{\\mathrm{N,bag}}$)**\nThese predictors are based on the $k$-nearest neighbors (kNN) algorithm.\n\n-   **Single-sample kNN predictor ($\\widehat{f}_{\\mathrm{N,single}}$)**: For a given replicate, this predictor is calculated directly on the training data $\\{(x_i^{(r)}, y_i^{(r)})\\}_{i=1}^n$. We identify the set $\\mathcal{N}_k(x_0)$ of $k$ training points whose covariates $x_i$ are closest to $x_0$. The prediction is the average of the corresponding response values: $\\widehat{f}_{\\mathrm{N,single}}^{(r)}(x_0) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x_0)} y_i^{(r)}$.\n\n-   **Bagged kNN predictor ($\\widehat{f}_{\\mathrm{N,bag}}$)**: Bagging (bootstrap aggregating) is a variance reduction technique. For each replicate, the following procedure is applied:\n    -   Generate $B$ bootstrap resamples from the original training data. Each resample is created by drawing $n$ data points with replacement.\n    -   For each bootstrap resample $b \\in \\{1, \\dots, B\\}$, train a kNN predictor and compute its prediction at $x_0$, denoted $\\widehat{f}_b(x_0)$.\n    -   The bagged prediction is the average of these individual predictions: $\\widehat{f}_{\\mathrm{N,bag}}^{(r)}(x_0) = \\frac{1}{B} \\sum_{b=1}^{B} \\widehat{f}_b(x_0)$. By averaging over many models trained on slightly perturbed versions of the data, bagging typically reduces the variance of unstable predictors like kNN.\n\n**5. Bias and Variance Estimation**\nAfter completing $R$ replicates, we have $R$ predictions for each of the three methods, forming empirical sampling distributions. Let $\\{\\widehat{f}_M^{(r)}(x_0)\\}_{r=1}^R$ be the set of predictions for a method $M \\in \\{\\mathrm{P}, \\mathrm{N,single}, \\mathrm{N,bag}\\}$.\n\n-   The average prediction for method $M$ is $\\overline{\\widehat{f}_M(x_0)} = \\frac{1}{R} \\sum_{r=1}^R \\widehat{f}_M^{(r)}(x_0)$.\n-   The **bias** is estimated as the difference between the average prediction and the true value $f(x_0)$:\n    $$ \\widehat{\\mathrm{Bias}}_{M} = \\overline{\\widehat{f}_M(x_0)} - f(x_0) $$\n-   The **variance** is estimated using the sample variance of the predictions, with Bessel's correction:\n    $$ \\widehat{\\mathrm{Var}}_{M} = \\frac{1}{R-1} \\sum_{r=1}^R \\left(\\widehat{f}_{M}^{(r)}(x_0) - \\overline{\\widehat{f}_M(x_0)}\\right)^2 $$\n-   Finally, the **variance reduction ratio** due to bagging is quantified by $\\rho = \\frac{\\widehat{\\mathrm{Var}}_{\\mathrm{N,bag}}}{\\widehat{\\mathrm{Var}}_{\\mathrm{N,single}}}$. A value of $\\rho  1$ indicates that bagging successfully reduced the variance of the kNN predictor.\n\nThe program implements this entire simulation, iterates through the specified test cases, and formats the resulting seven performance metrics for each case as required.",
            "answer": "```python\nimport numpy as np\n\ndef run_simulation(seed, n, sigma, x0, k, B, R, f_func):\n    \"\"\"\n    Runs a single Monte Carlo simulation for a given test case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Storage for predictions from each replicate\n    preds_p = np.zeros(R)\n    preds_ns = np.zeros(R)\n    preds_nb = np.zeros(R)\n\n    for r in range(R):\n        # 1. Data-generating process\n        x_train = rng.uniform(0, 1, n)\n        epsilon = rng.normal(0, sigma, n)\n        y_train = f_func(x_train) + epsilon\n\n        # 2. Parametric model-averaged predictor\n        aics = []\n        betas = []\n        degrees = [1, 2]\n        \n        for d in degrees:\n            # Fit polynomial regression of degree d\n            X_mat = np.vander(x_train, d + 1, increasing=True)\n            beta_hat, res, _, _ = np.linalg.lstsq(X_mat, y_train, rcond=None)\n            \n            # Note: np.linalg.lstsq returns the sum of squared residuals, not RSS.\n            # If n  d+1, res is RSS. If n = d+1, res is an empty array.\n            # We assume n  max(d)+1, which holds for test cases.\n            rss = res[0]\n            sigma2_hat_ml = rss / n\n            \n            # The MLE for sigma^2 can be 0 if RSS is 0 (perfect fit).\n            # This is unlikely with noise but must be handled for numerical stability.\n            if sigma2_hat_ml = 0:\n                # Assign a huge AIC to penalize this model\n                aic = 1e9\n            else:\n                logL = -n/2 * (np.log(2 * np.pi) + np.log(sigma2_hat_ml) + 1)\n                k_d = d + 2  # (d+1) coefficients + 1 variance parameter\n                aic = 2 * k_d - 2 * logL\n            \n            aics.append(aic)\n            betas.append(beta_hat)\n\n        aics = np.array(aics)\n        delta_aics = aics - np.min(aics)\n        exp_term = np.exp(-0.5 * delta_aics)\n        weights = exp_term / np.sum(exp_term)\n\n        # Calculate model-averaged prediction at x0\n        f_hat_p = 0\n        for i, d in enumerate(degrees):\n            x0_poly_vec = np.vander(np.array([x0]), d + 1, increasing=True)[0]\n            pred_d = x0_poly_vec @ betas[i]\n            f_hat_p += weights[i] * pred_d\n        \n        preds_p[r] = f_hat_p\n\n        # 3. Non-parametric k-nearest neighbors predictors\n        \n        # Helper function for kNN prediction\n        def knn_predict(x_target, x_data, y_data, k_neighbors):\n            distances = np.abs(x_data - x_target)\n            # Find indices of the k nearest neighbors\n            neighbor_indices = np.argsort(distances)[:k_neighbors]\n            return np.mean(y_data[neighbor_indices])\n\n        # Single-sample kNN predictor\n        preds_ns[r] = knn_predict(x0, x_train, y_train, k)\n\n        # Bagged kNN predictor\n        bootstrap_preds = np.zeros(B)\n        for b in range(B):\n            # Create a bootstrap resample\n            bootstrap_indices = rng.choice(n, size=n, replace=True)\n            x_boot = x_train[bootstrap_indices]\n            y_boot = y_train[bootstrap_indices]\n            bootstrap_preds[b] = knn_predict(x0, x_boot, y_boot, k)\n        \n        preds_nb[r] = np.mean(bootstrap_preds)\n\n    # 4. Bias and variance quantification\n    f_true_x0 = f_func(x0)\n    \n    # Parametric method\n    mean_p = np.mean(preds_p)\n    bias_p = mean_p - f_true_x0\n    var_p = np.var(preds_p, ddof=1)\n    \n    # Non-parametric single-sample method\n    mean_ns = np.mean(preds_ns)\n    bias_ns = mean_ns - f_true_x0\n    var_ns = np.var(preds_ns, ddof=1)\n\n    # Non-parametric bagged method\n    mean_nb = np.mean(preds_nb)\n    bias_nb = mean_nb - f_true_x0\n    var_nb = np.var(preds_nb, ddof=1)\n\n    # Variance reduction ratio\n    rho = var_nb / var_ns if var_ns > 0 else 0\n\n    return [bias_p, var_p, bias_ns, var_ns, bias_nb, var_nb, rho]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # 5. Test suite\n    test_cases = [\n        (12345, 50, 1.0, 0.5, 7, 200, 400, lambda x: 2 + 3 * x),\n        (54321, 60, 0.5, 0.3, 9, 200, 400, lambda x: np.sin(2 * np.pi * x)),\n        (20231105, 20, 2.0, 0.2, 5, 300, 300, lambda x: x**3 - x)\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        results = run_simulation(*case)\n        # Format results to 6 decimal places\n        results_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n        all_results_str.append(results_str)\n\n    # 6. Output\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}