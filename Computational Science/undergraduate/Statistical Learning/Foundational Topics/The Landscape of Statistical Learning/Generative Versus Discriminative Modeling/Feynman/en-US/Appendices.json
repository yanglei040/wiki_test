{
    "hands_on_practices": [
        {
            "introduction": "Before building complex models, it's crucial to understand a fundamental theoretical distinction between generative and discriminative approaches: what aspects of the data they try to learn. This practice explores the concept of parameter identifiability through carefully constructed thought experiments . You will discover that it's possible for different generative models, with distinct internal parameters, to produce the exact same discriminative behavior, highlighting that modeling the full data-generating process $p(x, y)$ can involve complexities that are irrelevant for the classification task of finding $p(y|x)$.",
            "id": "3124837",
            "problem": "Consider binary classification with class label $y \\in \\{0,1\\}$ and a real-valued feature $x \\in \\mathbb{R}$. Let $\\pi_y = \\mathbb{P}(y)$ denote the class prior, and let $p(x \\mid y)$ denote the class-conditional density. Recall the fundamental definitions:\n- Identifiability of a parametric statistical model means that the mapping from parameters to the induced distribution is one-to-one. If two distinct parameter values induce the same distribution over observables, the parameters are non-identifiable.\n- Bayes’ rule implies $p(y \\mid x) = \\dfrac{\\pi_y \\, p(x \\mid y)}{\\sum_{y' \\in \\{0,1\\}} \\pi_{y'} \\, p(x \\mid y')}$, and the Bayes decision rule for minimum $0$-$1$ loss assigns the class with larger posterior probability.\n\nWe will analyze two concrete settings.\n\nSetting $\\mathrm{S1}$ (within-class mixture with overlapping components): Suppose\n- $\\pi_0 = \\pi_1 = \\tfrac{1}{2}$,\n- For class $y=0$, the class-conditional density is a two-component Gaussian mixture with a weight parameter $w \\in (0,1)$,\n$$\np(x \\mid y=0; w) \\;=\\; w \\, \\mathcal{N}(x; 0, 1) \\;+\\; (1-w) \\, \\mathcal{N}(x; 0, 1),\n$$\n- For class $y=1$, $p(x \\mid y=1) \\;=\\; \\mathcal{N}(x; 1, 1)$,\nwhere $\\mathcal{N}(x;\\mu,\\sigma^2)$ denotes the Gaussian density with mean $\\mu$ and variance $\\sigma^2$.\n\nSetting $\\mathrm{S2}$ (two distinct generative models that yield the same discriminative behavior): Consider two models $\\mathrm{G1}$ and $\\mathrm{G2}$, each with class-conditional Gaussian densities of equal within-model variance:\n- Model $\\mathrm{G1}$: $\\pi_1^{(1)} = \\tfrac{1}{2}$, $\\pi_0^{(1)} = \\tfrac{1}{2}$, $p^{(1)}(x \\mid y=1) = \\mathcal{N}(x; 1, 1)$, $p^{(1)}(x \\mid y=0) = \\mathcal{N}(x; -1, 1)$.\n- Model $\\mathrm{G2}$: $\\pi_1^{(2)} = \\dfrac{e^{2}}{1+e^{2}}$, $\\pi_0^{(2)} = \\dfrac{1}{1+e^{2}}$, $p^{(2)}(x \\mid y=1) = \\mathcal{N}(x; 3, 2)$, $p^{(2)}(x \\mid y=0) = \\mathcal{N}(x; -1, 2)$.\n\nAnswer the following multiple-choice question by selecting all correct options.\n\nWhich statements are true?\n\nA. In Setting $\\mathrm{S1}$, the generative parameter $w$ is non-identifiable from $\\{(x_i,y_i)\\}_{i=1}^n$ because all $w \\in (0,1)$ induce the same $p(x \\mid y=0)$ and hence the same $p(y \\mid x)$. Therefore, $w$ cannot be recovered even with infinite data.\n\nB. In Setting $\\mathrm{S2}$, for every $x \\in \\mathbb{R}$, the posterior $p^{(1)}(y=1 \\mid x)$ under $\\mathrm{G1}$ equals the posterior $p^{(2)}(y=1 \\mid x)$ under $\\mathrm{G2}$, even though $\\{\\pi_y^{(1)}, p^{(1)}(x \\mid y)\\}$ and $\\{\\pi_y^{(2)}, p^{(2)}(x \\mid y)\\}$ differ.\n\nC. If two different generative parameterizations produce the same $p(y \\mid x)$ for all $x \\in \\mathbb{R}$, then they must also have the same class-conditional densities $p(x \\mid y)$ for each $y$ and the same priors $\\pi_y$. In other words, discriminative identifiability implies generative identifiability.\n\nD. Because the class-conditional densities overlap in both settings, the Bayes optimal decision boundary is not uniquely determined; the overlap alone makes the decision boundary ambiguous.",
            "solution": "We proceed from first principles: the definition of identifiability and Bayes’ rule $p(y \\mid x) = \\dfrac{\\pi_y \\, p(x \\mid y)}{\\sum_{y' \\in \\{0,1\\}} \\pi_{y'} \\, p(x \\mid y')}$.\n\nAnalysis of Setting $\\mathrm{S1}$:\n- For class $y=0$, the mixture is\n$$\np(x \\mid y=0; w) \\;=\\; w \\, \\mathcal{N}(x; 0, 1) \\;+\\; (1-w) \\, \\mathcal{N}(x; 0, 1).\n$$\nBy linearity, since the two components are identical densities, the mixture reduces to\n$$\np(x \\mid y=0; w) \\;=\\; \\mathcal{N}(x; 0, 1) \\quad \\text{for every } w \\in (0,1).\n$$\nTherefore, the observable distributions $p(x \\mid y=0; w)$ are identical for all $w$, and likewise the joint $p(x,y)$ and the posterior $p(y \\mid x)$ are independent of $w$. Consequently, $w$ is non-identifiable. This is structural non-identifiability: there is an uncountable family of parameter values that induce the same data distribution.\n- Since $\\pi_0 = \\pi_1 = \\tfrac{1}{2}$ and $p(x \\mid y=1)$ is fixed as $\\mathcal{N}(x; 1, 1)$, Bayes’ rule gives a posterior $p(y \\mid x)$ that does not depend on $w$ at all. Hence, even with infinite data, $w$ cannot be recovered.\n\nConclusion for $\\mathrm{S1}$: Statement A is Correct.\n\nAnalysis of Setting $\\mathrm{S2}$:\nWe compare $p^{(1)}(y=1 \\mid x)$ and $p^{(2)}(y=1 \\mid x)$ via the log-odds $\\ell(x) = \\log \\dfrac{p(y=1 \\mid x)}{p(y=0 \\mid x)}$. From Bayes’ rule,\n$$\n\\ell(x) \\;=\\; \\log \\frac{\\pi_1 \\, p(x \\mid y=1)}{\\pi_0 \\, p(x \\mid y=0)} \\;=\\; \\log \\frac{\\pi_1}{\\pi_0} \\;+\\; \\log \\frac{p(x \\mid y=1)}{p(x \\mid y=0)}.\n$$\nFor Gaussian class-conditionals with equal variance $\\sigma^2$, a direct expansion shows that $\\log \\dfrac{p(x \\mid y=1)}{p(x \\mid y=0)}$ is an affine function of $x$. We derive this explicitly for each model.\n\n- Model $\\mathrm{G1}$: $\\pi_1^{(1)} = \\tfrac{1}{2}$, $\\pi_0^{(1)} = \\tfrac{1}{2}$, $p^{(1)}(x \\mid y=1) = \\mathcal{N}(x;1,1)$, $p^{(1)}(x \\mid y=0) = \\mathcal{N}(x;-1,1)$. Then\n$$\n\\ell_1(x) \\;=\\; \\log \\frac{\\tfrac{1}{2}}{\\tfrac{1}{2}} \\;+\\; \\log \\frac{\\mathcal{N}(x;1,1)}{\\mathcal{N}(x;-1,1)}\n\\;=\\; 0 \\;-\\; \\frac{(x-1)^2 - (x+1)^2}{2 \\cdot 1}\n\\;=\\; -\\frac{(x^2 - 2x + 1) - (x^2 + 2x + 1)}{2}\n\\;=\\; -\\frac{-4x}{2}\n\\;=\\; 2x.\n$$\nTherefore $p^{(1)}(y=1 \\mid x) \\;=\\; \\dfrac{1}{1 + e^{-\\ell_1(x)}} \\;=\\; \\dfrac{1}{1 + e^{-2x}}$.\n\n- Model $\\mathrm{G2}$: $\\pi_1^{(2)} = \\dfrac{e^{2}}{1+e^{2}}$, $\\pi_0^{(2)} = \\dfrac{1}{1+e^{2}}$, $p^{(2)}(x \\mid y=1) = \\mathcal{N}(x;3,2)$, $p^{(2)}(x \\mid y=0) = \\mathcal{N}(x;-1,2)$. Then\n$$\n\\ell_2(x) \\;=\\; \\log \\frac{\\pi_1^{(2)}}{\\pi_0^{(2)}} \\;+\\; \\log \\frac{\\mathcal{N}(x;3,2)}{\\mathcal{N}(x;-1,2)}\n\\;=\\; \\log \\left( \\frac{\\tfrac{e^{2}}{1+e^{2}}}{\\tfrac{1}{1+e^{2}}} \\right)\n\\;-\\; \\frac{(x-3)^2 - (x+1)^2}{2 \\cdot 2}.\n$$\nCompute the terms:\n$$\n\\log \\left( \\frac{e^{2}}{1} \\right) \\;=\\; 2, \\quad\n(x-3)^2 - (x+1)^2 \\;=\\; (x^2 - 6x + 9) - (x^2 + 2x + 1) \\;=\\; -8x + 8.\n$$\nThus\n$$\n\\ell_2(x) \\;=\\; 2 \\;-\\; \\frac{-8x + 8}{4}\n\\;=\\; 2 \\;-\\; (-2x + 2)\n\\;=\\; 2x.\n$$\nTherefore $p^{(2)}(y=1 \\mid x) \\;=\\; \\dfrac{1}{1 + e^{-2x}}$, identical to the posterior under $\\mathrm{G1}$ for every $x \\in \\mathbb{R}$, even though the priors and the class-conditionals differ between $\\mathrm{G1}$ and $\\mathrm{G2}$.\n\nConclusion for $\\mathrm{S2}$: Statement B is Correct.\n\nImplications for Statements C and D:\n- Statement C claims that equality of $p(y \\mid x)$ for all $x$ forces equality of the generative components $p(x \\mid y)$ and $\\pi_y$. The explicit counterexample above (Models $\\mathrm{G1}$ and $\\mathrm{G2}$) shows this is false: we have the same $p(y \\mid x)$ for all $x$ but different $\\{\\pi_y, p(x \\mid y)\\}$. Hence C is Incorrect.\n- Statement D claims that overlap of $p(x \\mid y=0)$ and $p(x \\mid y=1)$ implies a non-unique Bayes decision boundary. This is not correct. The Bayes decision boundary in the binary case is defined by the set of $x$ satisfying $\\pi_1 p(x \\mid y=1) = \\pi_0 p(x \\mid y=0)$, equivalently $p(y=1 \\mid x) = \\tfrac{1}{2}$. In $\\mathrm{G1}$, we found $\\ell_1(x) = 2x$, so the boundary is the unique point where $\\ell_1(x) = 0$, namely $x = 0$. In $\\mathrm{G2}$, $\\ell_2(x) = 2x$ also yields the unique boundary $x = 0$. Overlap affects error rates but does not by itself cause ambiguity in the Bayes rule; the posterior is well-defined and so is the boundary, except for degenerate cases of exact ties over an interval, which do not occur here. Hence D is Incorrect.\n\nOption-by-option analysis summary:\n- A: Correct. In $\\mathrm{S1}$, $p(x \\mid y=0; w)$ is the same for all $w$, so $w$ is non-identifiable and $p(y \\mid x)$ is unchanged.\n- B: Correct. In $\\mathrm{S2}$, both models yield $\\ell(x) = 2x$ and thus the same $p(y=1 \\mid x)$ for all $x$.\n- C: Incorrect. $\\mathrm{G1}$ and $\\mathrm{G2}$ are a counterexample: same $p(y \\mid x)$ but different $\\{\\pi_y, p(x \\mid y)\\}$.\n- D: Incorrect. Overlap does not imply non-unique Bayes boundary; here the boundary is uniquely $x=0$ in both models.",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "After exploring theoretical distinctions, this exercise demonstrates how they translate into significant practical consequences, particularly when dealing with imperfect data . We will analyze a scenario where the pattern of missing features is itself informative about the class label. This practice will guide you to show how a generative model, by its nature of modeling the complete data-generating process, can leverage this subtle information, whereas a standard discriminative model that simply discards or ignores missingness can be systematically biased and less accurate.",
            "id": "3124923",
            "problem": "You are given a binary classification setting with a single real-valued feature and an explicit missingness mechanism. Let the class label be a Bernoulli random variable $Y \\in \\{0,1\\}$ with prior $P(Y=1)=\\pi$ and $P(Y=0)=1-\\pi$. Let $X \\in \\mathbb{R}$ be a feature that may be missing, and let $M \\in \\{0,1\\}$ be the missingness indicator, where $M=1$ denotes that $X$ is missing and $M=0$ denotes that $X$ is observed. Assume the following data-generating process:\n- The joint distribution factorizes as $p(y,x,m) = p(y)\\,p(x \\mid y)\\,p(m \\mid y)$, that is, missingness depends only on the class and is conditionally independent of the feature given the class (Missing At Random (MAR) with respect to $X$).\n- The missingness mechanism is defined class-wise as $P(M=1 \\mid Y=1)=\\alpha_1$ and $P(M=1 \\mid Y=0)=\\alpha_0$, with $0 \\lt \\alpha_0 \\lt 1$ and $0 \\lt \\alpha_1 \\lt 1$.\n\nA discriminative model that ignores $M$ attempts to learn $p(y \\mid x)$ directly from observed pairs. When $X$ is missing at prediction time and $M$ is ignored, such a model degenerates to predicting the prior $P(Y=1)=\\pi$, since there is no observed feature value to condition on. A generative model uses the full factorization $p(y)\\,p(x \\mid y)\\,p(m \\mid y)$, which allows updating beliefs about $Y$ from the missingness indicator via $p(m \\mid y)$, even when $X$ is unobserved.\n\nTask A (derivation). Starting only from Bayes’ rule and the law of total probability:\n- Derive $P(Y=1 \\mid M=1)$ and $P(Y=1 \\mid M=0)$ in terms of $\\pi$, $\\alpha_0$, and $\\alpha_1$.\n- Define the discriminative prediction that ignores $M$ (when $X$ is missing) as $\\widehat{P}_{\\text{disc}}(Y=1 \\mid M=1)=\\pi$. Define the bias for $M=1$ as $b_1 = P(Y=1 \\mid M=1) - \\pi$, and analogously define the bias for $M=0$ as $b_0 = P(Y=1 \\mid M=0) - \\pi$. Express $b_1$ and $b_0$ in terms of $\\pi$, $\\alpha_0$, and $\\alpha_1$.\n- Consider training that discards missing-feature cases (that is, restricts to $M=0$) and then fits a discriminative model for $p(y \\mid x)$ on this subset while ignoring $M$. Under the MAR assumption above, derive the relationship between the log-odds of $P(Y=1 \\mid X=x, M=0)$ and the log-odds of $P(Y=1 \\mid X=x)$. Show that they differ by an additive constant shift that depends only on $\\alpha_0$ and $\\alpha_1$. Denote this shift by $s$ and express it in terms of $\\alpha_0$ and $\\alpha_1$.\n\nTask B (computation). Implement a program that, given $(\\pi,\\alpha_0,\\alpha_1)$, computes the three quantities:\n- $b_1$, the bias for $M=1$ defined above.\n- $b_0$, the bias for $M=0$ defined above.\n- $s$, the constant log-odds shift defined above.\n\nUse the following test suite of parameter triples $(\\pi,\\alpha_0,\\alpha_1)$:\n- Test case $1$: $(\\pi,\\alpha_0,\\alpha_1) = (0.5, 0.2, 0.8)$.\n- Test case $2$: $(\\pi,\\alpha_0,\\alpha_1) = (0.4, 0.3, 0.3)$.\n- Test case $3$: $(\\pi,\\alpha_0,\\alpha_1) = (0.3, 0.01, 0.99)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[b_1^{(1)}, b_0^{(1)}, s^{(1)}, b_1^{(2)}, b_0^{(2)}, s^{(2)}, b_1^{(3)}, b_0^{(3)}, s^{(3)}]$, where the superscript indicates the test case index. Each number must be rounded to exactly $6$ decimal places in the printed output. No other text should be printed.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\nThe givens are:\n- A binary class label $Y \\in \\{0, 1\\}$ with prior $P(Y=1) = \\pi$.\n- A single real-valued feature $X \\in \\mathbb{R}$.\n- A missingness indicator $M \\in \\{0, 1\\}$, where $M=1$ indicates $X$ is missing.\n- The joint distribution factorization: $p(y,x,m) = p(y) \\, p(x \\mid y) \\, p(m \\mid y)$. This implies the Missing At Random (MAR) assumption that $X$ is conditionally independent of $M$ given $Y$.\n- The class-conditional missingness probabilities: $P(M=1 \\mid Y=1) = \\alpha_1$ and $P(M=1 \\mid Y=0) = \\alpha_0$.\n- Parameter constraints: $0 \\lt \\alpha_0 \\lt 1$ and $0 \\lt \\alpha_1 \\lt 1$.\n- Definition of discriminative prediction when $X$ is missing: $\\widehat{P}_{\\text{disc}}(Y=1 \\mid M=1) = \\pi$.\n- Definition of biases: $b_1 = P(Y=1 \\mid M=1) - \\pi$ and $b_0 = P(Y=1 \\mid M=0) - \\pi$.\n- Definition of log-odds shift $s$: the additive constant difference between the log-odds of $P(Y=1 \\mid X=x, M=0)$ and the log-odds of $P(Y=1 \\mid X=x)$.\n\nThe problem is evaluated for validity.\n- **Scientific Grounding**: The problem is well-grounded in the statistical theory of missing data and probabilistic modeling. The concepts of MAR, generative models, discriminative models, Bayes' rule, and log-odds are standard and rigorously defined.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary definitions and parameters to derive unique and meaningful quantities for the specified tasks.\n- **Objectivity**: The problem is stated in precise, formal, and objective mathematical language.\n\nThe problem statement is deemed valid as it is scientifically sound, well-posed, objective, and contains no discernible flaws. We may proceed with the solution.\n\nTask A: Derivations\n\nFirst, we derive the posterior probability of the class $Y=1$ given the missingness status $M$. We will use Bayes' rule, $P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}$, and the law of total probability, $P(B) = \\sum_i P(B \\mid A_i)P(A_i)$.\n\nDerivation of $P(Y=1 \\mid M=1)$:\nThe posterior probability $P(Y=1 \\mid M=1)$ is given by Bayes' rule:\n$$P(Y=1 \\mid M=1) = \\frac{P(M=1 \\mid Y=1) P(Y=1)}{P(M=1)}$$\nThe terms in the numerator are given as $P(M=1 \\mid Y=1) = \\alpha_1$ and $P(Y=1) = \\pi$. The denominator, $P(M=1)$, is the marginal probability of missingness, which we expand using the law of total probability over the classes $Y=0$ and $Y=1$:\n$$P(M=1) = P(M=1 \\mid Y=1)P(Y=1) + P(M=1 \\mid Y=0)P(Y=0)$$\nSubstituting the known quantities, $P(Y=0)=1-\\pi$ and $P(M=1 \\mid Y=0)=\\alpha_0$:\n$$P(M=1) = \\alpha_1 \\pi + \\alpha_0 (1-\\pi)$$\nCombining these results gives the expression for the posterior:\n$$P(Y=1 \\mid M=1) = \\frac{\\alpha_1 \\pi}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)}$$\n\nDerivation of $P(Y=1 \\mid M=0)$:\nThis derivation follows the same logic. We first establish the probabilities of observing the feature ($M=0$):\n$P(M=0 \\mid Y=1) = 1 - P(M=1 \\mid Y=1) = 1 - \\alpha_1$\n$P(M=0 \\mid Y=0) = 1 - P(M=1 \\mid Y=0) = 1 - \\alpha_0$\nUsing Bayes' rule:\n$$P(Y=1 \\mid M=0) = \\frac{P(M=0 \\mid Y=1) P(Y=1)}{P(M=0)}$$\nThe numerator is $(1 - \\alpha_1) \\pi$. The denominator is the marginal probability of the feature being observed:\n$$P(M=0) = P(M=0 \\mid Y=1)P(Y=1) + P(M=0 \\mid Y=0)P(Y=0) = (1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)$$\nTherefore, the posterior is:\n$$P(Y=1 \\mid M=0) = \\frac{(1 - \\alpha_1) \\pi}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)}$$\n\nDerivation of the biases $b_1$ and $b_0$:\nThe bias $b_1$ is defined as $b_1 = P(Y=1 \\mid M=1) - \\pi$. Substituting the derived expression for $P(Y=1 \\mid M=1)$:\n$$b_1 = \\frac{\\alpha_1 \\pi}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} - \\pi = \\pi \\left( \\frac{\\alpha_1}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} - 1 \\right)$$\n$$b_1 = \\pi \\left( \\frac{\\alpha_1 - (\\alpha_1 \\pi + \\alpha_0 (1-\\pi))}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} \\right) = \\pi \\left( \\frac{\\alpha_1(1-\\pi) - \\alpha_0(1-\\pi)}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} \\right)$$\n$$b_1 = \\frac{\\pi (1-\\pi) (\\alpha_1 - \\alpha_0)}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)}$$\nThis expression shows that the bias $b_1$ is zero if and only if $\\alpha_1 = \\alpha_0$, i.e., when missingness is completely independent of the class.\n\nThe bias $b_0$ is defined as $b_0 = P(Y=1 \\mid M=0) - \\pi$. Substituting the derived expression for $P(Y=1 \\mid M=0)$:\n$$b_0 = \\frac{(1 - \\alpha_1) \\pi}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} - \\pi = \\pi \\left( \\frac{1 - \\alpha_1}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} - 1 \\right)$$\n$$b_0 = \\pi \\left( \\frac{(1 - \\alpha_1) - ((1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi))}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} \\right)$$\n$$b_0 = \\pi \\left( \\frac{(1 - \\alpha_1)(1-\\pi) - (1 - \\alpha_0)(1-\\pi)}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} \\right)$$\n$$b_0 = \\frac{\\pi (1-\\pi) (\\alpha_0 - \\alpha_1)}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)}$$\n\nDerivation of the log-odds shift $s$:\nThe log-odds of an event with probability $p$ is $\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$. We need to compare the log-odds of $P(Y=1 \\mid X=x)$ with the log-odds of $P(Y=1 \\mid X=x, M=0)$.\nThe odds for $P(Y=1 \\mid X=x)$ are:\n$$\\frac{P(Y=1 \\mid X=x)}{P(Y=0 \\mid X=x)} = \\frac{p(x \\mid Y=1)P(Y=1)}{p(x \\mid Y=0)P(Y=0)} = \\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}$$\nTaking the natural logarithm gives the log-odds:\n$$\\text{log-odds}(P(Y=1 \\mid X=x)) = \\log\\left(\\frac{p(x \\mid Y=1)}{p(x \\mid Y=0)}\\right) + \\log\\left(\\frac{\\pi}{1-\\pi}\\right)$$\nThe odds for $P(Y=1 \\mid X=x, M=0)$ are:\n$$\\frac{P(Y=1 \\mid X=x, M=0)}{P(Y=0 \\mid X=x, M=0)} = \\frac{p(Y=1, x, M=0)}{p(Y=0, x, M=0)}$$\nUsing the given factorization $p(y,x,m) = p(y)p(x \\mid y)p(m \\mid y)$:\n$$ \\frac{p(Y=1)p(x \\mid Y=1)p(M=0 \\mid Y=1)}{p(Y=0)p(x \\mid Y=0)p(M=0 \\mid Y=0)} = \\frac{\\pi \\, p(x \\mid Y=1) \\, (1-\\alpha_1)}{(1-\\pi) \\, p(x \\mid Y=0) \\, (1-\\alpha_0)} $$\n$$ \\frac{P(Y=1 \\mid X=x, M=0)}{P(Y=0 \\mid X=x, M=0)} = \\left(\\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}\\right) \\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right)$$\nTaking the natural logarithm gives the log-odds for the subset of observed data:\n$$ \\text{log-odds}(P(Y=1 \\mid X=x, M=0)) = \\log\\left(\\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}\\right) + \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right) $$\n$$ \\text{log-odds}(P(Y=1 \\mid X=x, M=0)) = \\text{log-odds}(P(Y=1 \\mid X=x)) + \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right) $$\nThe problem defines the shift $s$ as this additive constant. Therefore:\n$$s = \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right)$$\nThis shift represents the bias introduced in the log-odds scale when a model is trained only on the subset of data with observed features, under the specified MAR mechanism. The bias depends solely on the differential rates of observation between the two classes.\n\nThese derivations complete Task A. The formulas will now be used for computation in Task B.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes biases and log-odds shift for given test cases\n    based on a model of missing data in binary classification.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (pi, alpha_0, alpha_1).\n    test_cases = [\n        (0.5, 0.2, 0.8),\n        (0.4, 0.3, 0.3),\n        (0.3, 0.01, 0.99),\n    ]\n\n    # List to store the results for all test cases.\n    results = []\n\n    for case in test_cases:\n        pi, alpha_0, alpha_1 = case\n\n        # --- Calculate b_1, the bias for M=1 ---\n        # Formula: b_1 = (pi * (1-pi) * (alpha_1 - alpha_0)) / (alpha_1 * pi + alpha_0 * (1-pi))\n        # This formula is well-defined as the denominator is P(M=1), which is > 0.\n        b1_numerator = pi * (1.0 - pi) * (alpha_1 - alpha_0)\n        b1_denominator = alpha_1 * pi + alpha_0 * (1.0 - pi)\n        b1 = b1_numerator / b1_denominator\n\n        # --- Calculate b_0, the bias for M=0 ---\n        # Formula: b_0 = (pi * (1-pi) * (alpha_0 - alpha_1)) / ((1-alpha_1) * pi + (1-alpha_0) * (1-pi))\n        # This formula is well-defined as the denominator is P(M=0), which is > 0.\n        b0_numerator = pi * (1.0 - pi) * (alpha_0 - alpha_1)\n        b0_denominator = (1.0 - alpha_1) * pi + (1.0 - alpha_0) * (1.0 - pi)\n        b0 = b0_numerator / b0_denominator\n\n        # --- Calculate s, the log-odds shift ---\n        # Formula: s = log((1-alpha_1) / (1-alpha_0))\n        # np.log is the natural logarithm.\n        # The argument is well-defined and positive because 0 < alpha_0, alpha_1 < 1.\n        s = np.log((1.0 - alpha_1) / (1.0 - alpha_0))\n\n        results.extend([b1, b0, s])\n\n    # Format the final output string exactly as required.\n    # Each number is rounded to 6 decimal places.\n    output_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n    print(output_str)\n\n# Execute the main function.\nsolve()\n```"
        },
        {
            "introduction": "Having examined the unique strengths and weaknesses of both approaches, we can now move beyond a strict dichotomy and explore a more modern, nuanced perspective. This practice treats the generative-discriminative division as a continuum, allowing us to build a hybrid model . You will construct and train a model whose objective function blends the discriminative task of learning $p(y|x)$ with a generative-style regularizer based on the marginal data density $p(x)$. By tuning the trade-off parameter $\\lambda$, you will empirically investigate how incorporating a generative component can improve a model's generalization to new, unseen data, effectively getting the best of both worlds.",
            "id": "3124935",
            "problem": "You are given a binary classification setting with inputs $x \\in \\mathbb{R}$ and labels $y \\in \\{0,1\\}$. Consider the hybrid empirical risk that combines a conditional negative log-loss with a marginal density regularizer. The model uses a shared parameter $ \\mu \\in \\mathbb{R} $ to couple the conditional and marginal components, and a slope parameter $ a \\in \\mathbb{R} $ for the conditional link. The conditional model is $p(y \\mid x) = \\mathrm{Bernoulli}(\\sigma(z))$ with $z = a(x - \\mu)$, where $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ is the logistic sigmoid. The marginal density model for $x$ is a Gaussian with known variance $s^2$ and unknown mean $\\mu$, that is $p(x) = \\mathcal{N}(\\mu, s^2)$. For a dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$, define the hybrid empirical risk\n$$\nR_n(\\mu, a; \\lambda) \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\left[ - y_i \\log \\sigma\\!\\left(a(x_i - \\mu)\\right) - (1-y_i) \\log \\left(1 - \\sigma\\!\\left(a(x_i - \\mu)\\right)\\right) \\right] \\;+\\; \\lambda \\cdot \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right],\n$$\nwhere $\\lambda \\ge 0$ is a trade-off parameter. The first summand is the empirical conditional negative log-loss, and the second is the empirical negative log marginal density with a penalty weight $\\lambda$. Your task is to analyze how $\\lambda$ affects generalization by deriving learning rules from first principles and then evaluating test conditional loss.\n\nFundamental base for derivation:\n- Use the definition of the logistic sigmoid $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ and the Bernoulli negative log-likelihood.\n- Use the negative log-density of a Gaussian $\\mathcal{N}(\\mu, s^2)$ in one dimension.\n- Use the concept of empirical risk minimization to connect training loss to generalization.\n\nRequired derivations and computations:\n1) Starting from the definitions above, derive the gradients of $R_n(\\mu, a; \\lambda)$ with respect to $\\mu$ and $a$. Express them in terms of $\\{x_i, y_i\\}_{i=1}^n$, $a$, $\\mu$, $s^2$, and $\\lambda$, using only basic calculus and the chain rule. Do not assume any pre-known closed-form for the gradients beyond the fundamental definitions stated here.\n2) Implement full-batch gradient descent to minimize $R_n(\\mu, a; \\lambda)$ over $(\\mu, a)$ for a fixed $\\lambda$ using the following fixed settings to ensure determinism:\n   - Initialization: $a_0 = 1.0$ and $\\mu_0 = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n   - Learning rate schedule: $\\eta_k = \\frac{\\eta_0}{\\sqrt{k+1}}$ with $\\eta_0 = 0.3$, where $k$ is the $k$-th iteration index starting at $k = 0$.\n   - Number of iterations: $T = 2000$.\n   - Known variance: $s^2 = 1.0$.\n   - Parameter clipping after each update: clip $a$ to the interval $[-10, 10]$ and $\\mu$ to $[-5, 5]$.\n3) For each trained $(\\widehat{\\mu}, \\widehat{a})$, evaluate the test conditional negative log-loss\n$$\n\\widehat{R}_{\\text{test}}(\\widehat{\\mu}, \\widehat{a}) \\;=\\; \\frac{1}{m} \\sum_{j=1}^m \\left[ - y^{\\text{test}}_j \\log \\sigma\\!\\left(\\widehat{a}(x^{\\text{test}}_j - \\widehat{\\mu})\\right) - \\left(1-y^{\\text{test}}_j\\right) \\log \\left(1 - \\sigma\\!\\left(\\widehat{a}(x^{\\text{test}}_j - \\widehat{\\mu})\\right)\\right) \\right].\n$$\n4) For each dataset below, and for each $\\lambda$ in the specified grid, train using the prescription in item $2)$, compute the test conditional loss as in item $3)$, and select the $\\lambda$ that minimizes the test conditional loss. In case of ties up to an absolute tolerance of $10^{-6}$, select the smallest $\\lambda$ among the minimizers.\n\nThe test suite consists of three datasets, each with a training and a test split, and the same $\\lambda$ grid and variance $s^2$:\n- Dataset A (balanced clusters):\n  - Training: $x^{\\text{train}} = [-1.3, -1.0, -0.7, 0.6, 0.9, 1.2]$, $y^{\\text{train}} = [0, 0, 0, 1, 1, 1]$.\n  - Test: $x^{\\text{test}} = [-1.1, -0.9, 0.8, 1.1]$, $y^{\\text{test}} = [0, 0, 1, 1]$.\n- Dataset B (no signal):\n  - Training: $x^{\\text{train}} = [-0.3, -0.1, 0.1, 0.3, -0.2, 0.2]$, $y^{\\text{train}} = [0, 1, 0, 1, 1, 0]$.\n  - Test: $x^{\\text{test}} = [-0.4, 0.0, 0.4]$, $y^{\\text{test}} = [0, 1, 0]$.\n- Dataset C (class imbalance):\n  - Training: $x^{\\text{train}} = [-1.2, -0.8, 0.4, 0.8, 1.0, 1.2, 1.4]$, $y^{\\text{train}} = [0, 0, 1, 1, 1, 1, 1]$.\n  - Test: $x^{\\text{test}} = [-0.9, -0.7, 0.9, 1.1]$, $y^{\\text{test}} = [0, 0, 1, 1]$.\n- Variance: $s^2 = 1.0$ for all datasets.\n- Trade-off grid: $\\Lambda = [0.0, 0.1, 1.0, 10.0]$.\n\nAngle units do not apply. There are no physical units in this problem. All outputs must be real-valued floats. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the $k$-th entry is the selected $\\lambda$ for the $k$-th dataset in the order A, B, C (for example, an output like $[0.0,0.1,1.0]$). Ensure that the implementation uses only the specifications provided and no randomness.\n\nYour final program must implement the derivations and computations and output the selected $\\lambda$ for each dataset as the single-line format described above. The goal is to connect the theory that hybrid objectives interpolate between discriminative and generative modeling to an empirical selection of $\\lambda$ that promotes better test conditional loss, thus analyzing $\\lambda$ in terms of generalization.",
            "solution": "The problem requires us to analyze a hybrid empirical risk function by deriving its gradients and implementing a gradient descent-based learning algorithm. The goal is to find the optimal trade-off parameter $\\lambda$ that minimizes the test conditional loss for three distinct datasets.\n\nThe hybrid empirical risk is given by:\n$$\nR_n(\\mu, a; \\lambda) \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\left[ - y_i \\log \\sigma\\!\\left(a(x_i - \\mu)\\right) - (1-y_i) \\log \\left(1 - \\sigma\\!\\left(a(x_i - \\mu)\\right)\\right) \\right] \\;+\\; \\lambda \\cdot \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right]\n$$\nThis can be written as the sum of a conditional loss component $L_C$ and a marginal loss component $L_M$, weighted by $\\lambda$:\n$$\nR_n(\\mu, a; \\lambda) = L_C(\\mu, a) + \\lambda L_M(\\mu)\n$$\nwhere\n$$\nL_C(\\mu, a) = \\frac{1}{n} \\sum_{i=1}^n \\left[ - y_i \\log \\sigma(z_i) - (1-y_i) \\log(1 - \\sigma(z_i)) \\right] \\text{, with } z_i = a(x_i - \\mu)\n$$\n$$\nL_M(\\mu) = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right]\n$$\n\nTo minimize this risk function using gradient descent, we must compute its partial derivatives with respect to the parameters $\\mu$ and $a$.\n\n**1. Gradient Derivations**\n\nFirst, we establish the derivative of the logistic sigmoid function $\\sigma(t) = \\frac{1}{1 + e^{-t}} = (1 + e^{-t})^{-1}$. Using the chain rule:\n$$\n\\frac{d\\sigma(t)}{dt} = -1 \\cdot (1 + e^{-t})^{-2} \\cdot (-e^{-t}) = \\frac{e^{-t}}{(1+e^{-t})^2}\n$$\nThis can be rewritten as:\n$$\n\\frac{d\\sigma(t)}{dt} = \\frac{1}{1+e^{-t}} \\cdot \\frac{e^{-t}}{1+e^{-t}} = \\sigma(t) \\cdot \\frac{(1+e^{-t})-1}{1+e^{-t}} = \\sigma(t) \\cdot \\left(1 - \\frac{1}{1+e^{-t}}\\right) = \\sigma(t)(1-\\sigma(t))\n$$\n\nNow, let's find the gradient of the per-sample conditional loss term, $l_i = - y_i \\log \\sigma(z_i) - (1-y_i) \\log(1 - \\sigma(z_i))$, with respect to $z_i$:\n$$\n\\frac{\\partial l_i}{\\partial z_i} = \\frac{\\partial l_i}{\\partial \\sigma(z_i)} \\frac{d\\sigma(z_i)}{dz_i}\n$$\nThe first part is:\n$$\n\\frac{\\partial l_i}{\\partial \\sigma(z_i)} = -\\frac{y_i}{\\sigma(z_i)} - \\frac{1-y_i}{1-\\sigma(z_i)}(-1) = \\frac{-y_i(1-\\sigma(z_i)) + (1-y_i)\\sigma(z_i)}{\\sigma(z_i)(1-\\sigma(z_i))} = \\frac{\\sigma(z_i) - y_i}{\\sigma(z_i)(1-\\sigma(z_i))}\n$$\nCombining this with the derivative of the sigmoid function:\n$$\n\\frac{\\partial l_i}{\\partial z_i} = \\frac{\\sigma(z_i) - y_i}{\\sigma(z_i)(1-\\sigma(z_i))} \\cdot \\sigma(z_i)(1-\\sigma(z_i)) = \\sigma(z_i) - y_i\n$$\n\nWith this result, we can find the gradients of the total empirical risk $R_n$.\n\n**Gradient with respect to $\\mu$:**\n$$\n\\frac{\\partial R_n}{\\partial \\mu} = \\frac{\\partial L_C}{\\partial \\mu} + \\lambda \\frac{\\partial L_M}{\\partial \\mu}\n$$\nThe gradient of the conditional part $L_C$ is found using the chain rule:\n$$\n\\frac{\\partial L_C}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial l_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\mu}\n$$\nSince $z_i = a(x_i - \\mu)$, we have $\\frac{\\partial z_i}{\\partial \\mu} = -a$.\n$$\n\\frac{\\partial L_C}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(a(x_i - \\mu)) - y_i) \\cdot (-a) = -\\frac{a}{n} \\sum_{i=1}^n (\\sigma(a(x_i - \\mu)) - y_i)\n$$\nThe gradient of the marginal part $L_M$ is:\n$$\n\\frac{\\partial L_M}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial \\mu} \\left( \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right) = \\frac{1}{n} \\sum_{i=1}^n \\left( 0 + \\frac{2(x_i - \\mu) \\cdot (-1)}{2s^2} \\right) = -\\frac{1}{ns^2} \\sum_{i=1}^n (x_i - \\mu)\n$$\nCombining both parts, the full gradient with respect to $\\mu$ is:\n$$\n\\frac{\\partial R_n}{\\partial \\mu} = -\\frac{a}{n} \\sum_{i=1}^n (\\sigma(a(x_i-\\mu)) - y_i) - \\frac{\\lambda}{ns^2} \\sum_{i=1}^n (x_i - \\mu)\n$$\nThis can be written as an average over the samples:\n$$\n\\frac{\\partial R_n}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n \\left[ -a(\\sigma(a(x_i-\\mu)) - y_i) - \\frac{\\lambda}{s^2}(x_i - \\mu) \\right]\n$$\n\n**Gradient with respect to $a$:**\n$$\n\\frac{\\partial R_n}{\\partial a} = \\frac{\\partial L_C}{\\partial a} + \\lambda \\frac{\\partial L_M}{\\partial a}\n$$\nThe gradient of the conditional part $L_C$ with respect to $a$ is:\n$$\n\\frac{\\partial L_C}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial l_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial a}\n$$\nSince $z_i = a(x_i - \\mu)$, we have $\\frac{\\partial z_i}{\\partial a} = x_i - \\mu$.\n$$\n\\frac{\\partial L_C}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(a(x_i - \\mu)) - y_i) (x_i - \\mu)\n$$\nThe marginal part $L_M$ does not depend on $a$, so $\\frac{\\partial L_M}{\\partial a} = 0$.\nTherefore, the full gradient with respect to $a$ is:\n$$\n\\frac{\\partial R_n}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)(\\sigma(a(x_i - \\mu)) - y_i)\n$$\n\nThese derived gradients are used in a full-batch gradient descent algorithm to find the parameters $(\\widehat{\\mu}, \\widehat{a})$ that minimize the hybrid empirical risk for each given value of $\\lambda$. The algorithm proceeds by initializing the parameters, then iteratively updating them in the opposite direction of the gradient. After training, the performance is measured by the conditional negative log-loss on a separate test set. This process is repeated for each dataset and each value of $\\lambda$ to find the configuration that yields the best generalization performance, as measured by the lowest test loss.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the best lambda for a hybrid generative-discriminative model.\n    \"\"\"\n\n    def sigmoid(t):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        # Clip the input to avoid overflow in np.exp\n        t = np.clip(t, -500, 500)\n        return 1 / (1 + np.exp(-t))\n\n    def softplus(x):\n        \"\"\"\n        Numerically stable computation of log(1 + exp(x)).\n        This is equivalent to: np.log(1.0 + np.exp(-np.abs(x))) + np.maximum(x, 0)\n        \"\"\"\n        return np.where(x > 30, x, np.log(1 + np.exp(x)))\n\n    test_cases = [\n        (\n            np.array([-1.3, -1.0, -0.7, 0.6, 0.9, 1.2]),\n            np.array([0, 0, 0, 1, 1, 1]),\n            np.array([-1.1, -0.9, 0.8, 1.1]),\n            np.array([0, 0, 1, 1])\n        ),\n        (\n            np.array([-0.3, -0.1, 0.1, 0.3, -0.2, 0.2]),\n            np.array([0, 1, 0, 1, 1, 0]),\n            np.array([-0.4, 0.0, 0.4]),\n            np.array([0, 1, 0])\n        ),\n        (\n            np.array([-1.2, -0.8, 0.4, 0.8, 1.0, 1.2, 1.4]),\n            np.array([0, 0, 1, 1, 1, 1, 1]),\n            np.array([-0.9, -0.7, 0.9, 1.1]),\n            np.array([0, 0, 1, 1])\n        )\n    ]\n    \n    s2 = 1.0\n    lambda_grid = [0.0, 0.1, 1.0, 10.0]\n    eta0 = 0.3\n    T = 2000\n    \n    final_lambdas = []\n\n    for train_x, train_y, test_x, test_y in test_cases:\n        min_test_loss = float('inf')\n        best_lambda_for_case = -1.0\n        \n        for lambda_val in lambda_grid:\n            # Initialization\n            mu = np.mean(train_x)\n            a = 1.0\n            \n            # Gradient Descent\n            for k in range(T):\n                eta_k = eta0 / np.sqrt(k + 1)\n                \n                # Calculate gradients\n                z = a * (train_x - mu)\n                sigma_z = sigmoid(z)\n                \n                # Gradient w.r.t. mu\n                grad_mu_term1 = -a * (sigma_z - train_y)\n                grad_mu_term2 = lambda_val * (-(1 / s2) * (train_x - mu))\n                grad_mu = np.mean(grad_mu_term1 + grad_mu_term2)\n\n                # Gradient w.r.t. a\n                grad_a = np.mean((train_x - mu) * (sigma_z - train_y))\n                \n                # Update parameters\n                mu -= eta_k * grad_mu\n                a -= eta_k * grad_a\n                \n                # Clipping\n                mu = np.clip(mu, -5.0, 5.0)\n                a = np.clip(a, -10.0, 10.0)\n            \n            mu_hat, a_hat = mu, a\n            \n            # Calculate test conditional negative log-loss\n            z_test = a_hat * (test_x - mu_hat)\n            \n            # Use numerically stable loss formulation: y*softplus(-z) + (1-y)*softplus(z)\n            loss_per_item = test_y * softplus(-z_test) + (1 - test_y) * softplus(z_test)\n            current_test_loss = np.mean(loss_per_item)\n\n            # Update best lambda based on minimizing test loss\n            # Tie-breaking rule: choose smallest lambda for ties within 1e-6 tolerance.\n            # Since lambda_grid is sorted, we only update if there is a strict improvement.\n            if current_test_loss < min_test_loss - 1e-6:\n                min_test_loss = current_test_loss\n                best_lambda_for_case = lambda_val\n            # If best_lambda_for_case is unassigned, assign it\n            if best_lambda_for_case == -1.0:\n                 best_lambda_for_case = lambda_val\n\n        final_lambdas.append(best_lambda_for_case)\n        \n    print(f\"[{','.join(map(str, final_lambdas))}]\")\n\nsolve()\n```"
        }
    ]
}