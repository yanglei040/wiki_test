{
    "hands_on_practices": [
        {
            "introduction": "A key theoretical difference between generative and discriminative models lies in what they model. Generative models learn the full joint distribution $p(x, y)$, allowing them to understand the underlying data-generating process, while discriminative models focus solely on the decision boundary by learning $p(y|x)$. This practice explores a powerful consequence of this difference through a hypothetical scenario involving missing data . You will derive and compute how a generative model can leverage the pattern of missingness itself as a valuable signal for classification, a source of information that a naive discriminative model would discard.",
            "id": "3124923",
            "problem": "You are given a binary classification setting with a single real-valued feature and an explicit missingness mechanism. Let the class label be a Bernoulli random variable $Y \\in \\{0,1\\}$ with prior $P(Y=1)=\\pi$ and $P(Y=0)=1-\\pi$. Let $X \\in \\mathbb{R}$ be a feature that may be missing, and let $M \\in \\{0,1\\}$ be the missingness indicator, where $M=1$ denotes that $X$ is missing and $M=0$ denotes that $X$ is observed. Assume the following data-generating process:\n- The joint distribution factorizes as $p(y,x,m) = p(y)\\,p(x \\mid y)\\,p(m \\mid y)$, that is, missingness depends only on the class and is conditionally independent of the feature given the class (Missing At Random (MAR) with respect to $X$).\n- The missingness mechanism is defined class-wise as $P(M=1 \\mid Y=1)=\\alpha_1$ and $P(M=1 \\mid Y=0)=\\alpha_0$, with $0 \\lt \\alpha_0 \\lt 1$ and $0 \\lt \\alpha_1 \\lt 1$.\n\nA discriminative model that ignores $M$ attempts to learn $p(y \\mid x)$ directly from observed pairs. When $X$ is missing at prediction time and $M$ is ignored, such a model degenerates to predicting the prior $P(Y=1)=\\pi$, since there is no observed feature value to condition on. A generative model uses the full factorization $p(y)\\,p(x \\mid y)\\,p(m \\mid y)$, which allows updating beliefs about $Y$ from the missingness indicator via $p(m \\mid y)$, even when $X$ is unobserved.\n\nTask A (derivation). Starting only from Bayesâ€™ rule and the law of total probability:\n- Derive $P(Y=1 \\mid M=1)$ and $P(Y=1 \\mid M=0)$ in terms of $\\pi$, $\\alpha_0$, and $\\alpha_1$.\n- Define the discriminative prediction that ignores $M$ (when $X$ is missing) as $\\widehat{P}_{\\text{disc}}(Y=1 \\mid M=1)=\\pi$. Define the bias for $M=1$ as $b_1 = P(Y=1 \\mid M=1) - \\pi$, and analogously define the bias for $M=0$ as $b_0 = P(Y=1 \\mid M=0) - \\pi$. Express $b_1$ and $b_0$ in terms of $\\pi$, $\\alpha_0$, and $\\alpha_1$.\n- Consider training that discards missing-feature cases (that is, restricts to $M=0$) and then fits a discriminative model for $p(y \\mid x)$ on this subset while ignoring $M$. Under the MAR assumption above, derive the relationship between the log-odds of $P(Y=1 \\mid X=x, M=0)$ and the log-odds of $P(Y=1 \\mid X=x)$. Show that they differ by an additive constant shift that depends only on $\\alpha_0$ and $\\alpha_1$. Denote this shift by $s$ and express it in terms of $\\alpha_0$ and $\\alpha_1$.\n\nTask B (computation). Implement a program that, given $(\\pi,\\alpha_0,\\alpha_1)$, computes the three quantities:\n- $b_1$, the bias for $M=1$ defined above.\n- $b_0$, the bias for $M=0$ defined above.\n- $s$, the constant log-odds shift defined above.\n\nUse the following test suite of parameter triples $(\\pi,\\alpha_0,\\alpha_1)$:\n- Test case $1$: $(\\pi,\\alpha_0,\\alpha_1) = (0.5, 0.2, 0.8)$.\n- Test case $2$: $(\\pi,\\alpha_0,\\alpha_1) = (0.4, 0.3, 0.3)$.\n- Test case $3$: $(\\pi,\\alpha_0,\\alpha_1) = (0.3, 0.01, 0.99)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[b_1^{(1)}, b_0^{(1)}, s^{(1)}, b_1^{(2)}, b_0^{(2)}, s^{(2)}, b_1^{(3)}, b_0^{(3)}, s^{(3)}]$, where the superscript indicates the test case index. Each number must be rounded to exactly $6$ decimal places in the printed output. No other text should be printed.",
            "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\nThe givens are:\n- A binary class label $Y \\in \\{0, 1\\}$ with prior $P(Y=1) = \\pi$.\n- A single real-valued feature $X \\in \\mathbb{R}$.\n- A missingness indicator $M \\in \\{0, 1\\}$, where $M=1$ indicates $X$ is missing.\n- The joint distribution factorization: $p(y,x,m) = p(y) \\, p(x \\mid y) \\, p(m \\mid y)$. This implies the Missing At Random (MAR) assumption that $X$ is conditionally independent of $M$ given $Y$.\n- The class-conditional missingness probabilities: $P(M=1 \\mid Y=1) = \\alpha_1$ and $P(M=1 \\mid Y=0) = \\alpha_0$.\n- Parameter constraints: $0 \\lt \\alpha_0 \\lt 1$ and $0 \\lt \\alpha_1 \\lt 1$.\n- Definition of discriminative prediction when $X$ is missing: $\\widehat{P}_{\\text{disc}}(Y=1 \\mid M=1) = \\pi$.\n- Definition of biases: $b_1 = P(Y=1 \\mid M=1) - \\pi$ and $b_0 = P(Y=1 \\mid M=0) - \\pi$.\n- Definition of log-odds shift $s$: the additive constant difference between the log-odds of $P(Y=1 \\mid X=x, M=0)$ and the log-odds of $P(Y=1 \\mid X=x)$.\n\nThe problem is evaluated for validity.\n- **Scientific Grounding**: The problem is well-grounded in the statistical theory of missing data and probabilistic modeling. The concepts of MAR, generative models, discriminative models, Bayes' rule, and log-odds are standard and rigorously defined.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary definitions and parameters to derive unique and meaningful quantities for the specified tasks.\n- **Objectivity**: The problem is stated in precise, formal, and objective mathematical language.\n\nThe problem statement is deemed valid as it is scientifically sound, well-posed, objective, and contains no discernible flaws. We may proceed with the solution.\n\nTask A: Derivations\n\nFirst, we derive the posterior probability of the class $Y=1$ given the missingness status $M$. We will use Bayes' rule, $P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}$, and the law of total probability, $P(B) = \\sum_i P(B \\mid A_i)P(A_i)$.\n\nDerivation of $P(Y=1 \\mid M=1)$:\nThe posterior probability $P(Y=1 \\mid M=1)$ is given by Bayes' rule:\n$$P(Y=1 \\mid M=1) = \\frac{P(M=1 \\mid Y=1) P(Y=1)}{P(M=1)}$$\nThe terms in the numerator are given as $P(M=1 \\mid Y=1) = \\alpha_1$ and $P(Y=1) = \\pi$. The denominator, $P(M=1)$, is the marginal probability of missingness, which we expand using the law of total probability over the classes $Y=0$ and $Y=1$:\n$$P(M=1) = P(M=1 \\mid Y=1)P(Y=1) + P(M=1 \\mid Y=0)P(Y=0)$$\nSubstituting the known quantities, $P(Y=0)=1-\\pi$ and $P(M=1 \\mid Y=0)=\\alpha_0$:\n$$P(M=1) = \\alpha_1 \\pi + \\alpha_0 (1-\\pi)$$\nCombining these results gives the expression for the posterior:\n$$P(Y=1 \\mid M=1) = \\frac{\\alpha_1 \\pi}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)}$$\n\nDerivation of $P(Y=1 \\mid M=0)$:\nThis derivation follows the same logic. We first establish the probabilities of observing the feature ($M=0$):\n$P(M=0 \\mid Y=1) = 1 - P(M=1 \\mid Y=1) = 1 - \\alpha_1$\n$P(M=0 \\mid Y=0) = 1 - P(M=1 \\mid Y=0) = 1 - \\alpha_0$\nUsing Bayes' rule:\n$$P(Y=1 \\mid M=0) = \\frac{P(M=0 \\mid Y=1) P(Y=1)}{P(M=0)}$$\nThe numerator is $(1 - \\alpha_1) \\pi$. The denominator is the marginal probability of the feature being observed:\n$$P(M=0) = P(M=0 \\mid Y=1)P(Y=1) + P(M=0 \\mid Y=0)P(Y=0) = (1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)$$\nTherefore, the posterior is:\n$$P(Y=1 \\mid M=0) = \\frac{(1 - \\alpha_1) \\pi}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)}$$\n\nDerivation of the biases $b_1$ and $b_0$:\nThe bias $b_1$ is defined as $b_1 = P(Y=1 \\mid M=1) - \\pi$. Substituting the derived expression for $P(Y=1 \\mid M=1)$:\n$$b_1 = \\frac{\\alpha_1 \\pi}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} - \\pi = \\pi \\left( \\frac{\\alpha_1}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} - 1 \\right)$$\n$$b_1 = \\pi \\left( \\frac{\\alpha_1 - (\\alpha_1 \\pi + \\alpha_0 (1-\\pi))}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} \\right) = \\pi \\left( \\frac{\\alpha_1(1-\\pi) - \\alpha_0(1-\\pi)}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)} \\right)$$\n$$b_1 = \\frac{\\pi (1-\\pi) (\\alpha_1 - \\alpha_0)}{\\alpha_1 \\pi + \\alpha_0 (1-\\pi)}$$\nThis expression shows that the bias $b_1$ is zero if and only if $\\alpha_1 = \\alpha_0$, i.e., when missingness is completely independent of the class.\n\nThe bias $b_0$ is defined as $b_0 = P(Y=1 \\mid M=0) - \\pi$. Substituting the derived expression for $P(Y=1 \\mid M=0)$:\n$$b_0 = \\frac{(1 - \\alpha_1) \\pi}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} - \\pi = \\pi \\left( \\frac{1 - \\alpha_1}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} - 1 \\right)$$\n$$b_0 = \\pi \\left( \\frac{(1 - \\alpha_1) - ((1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi))}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} \\right)$$\n$$b_0 = \\pi \\left( \\frac{(1 - \\alpha_1)(1-\\pi) - (1 - \\alpha_0)(1-\\pi)}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)} \\right)$$\n$$b_0 = \\frac{\\pi (1-\\pi) (\\alpha_0 - \\alpha_1)}{(1 - \\alpha_1) \\pi + (1 - \\alpha_0) (1-\\pi)}$$\n\nDerivation of the log-odds shift $s$:\nThe log-odds of an event with probability $p$ is $\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)$. We need to compare the log-odds of $P(Y=1 \\mid X=x)$ with the log-odds of $P(Y=1 \\mid X=x, M=0)$.\nThe odds for $P(Y=1 \\mid X=x)$ are:\n$$\\frac{P(Y=1 \\mid X=x)}{P(Y=0 \\mid X=x)} = \\frac{p(x \\mid Y=1)P(Y=1)}{p(x \\mid Y=0)P(Y=0)} = \\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}$$\nTaking the natural logarithm gives the log-odds:\n$$\\text{log-odds}(P(Y=1 \\mid X=x)) = \\log\\left(\\frac{p(x \\mid Y=1)}{p(x \\mid Y=0)}\\right) + \\log\\left(\\frac{\\pi}{1-\\pi}\\right)$$\nThe odds for $P(Y=1 \\mid X=x, M=0)$ are:\n$$\\frac{P(Y=1 \\mid X=x, M=0)}{P(Y=0 \\mid X=x, M=0)} = \\frac{p(Y=1, x, M=0)}{p(Y=0, x, M=0)}$$\nUsing the given factorization $p(y,x,m) = p(y)p(x \\mid y)p(m \\mid y)$:\n$$ \\frac{p(Y=1)p(x \\mid Y=1)p(M=0 \\mid Y=1)}{p(Y=0)p(x \\mid Y=0)p(M=0 \\mid Y=0)} = \\frac{\\pi \\, p(x \\mid Y=1) \\, (1-\\alpha_1)}{(1-\\pi) \\, p(x \\mid Y=0) \\, (1-\\alpha_0)} $$\n$$ \\frac{P(Y=1 \\mid X=x, M=0)}{P(Y=0 \\mid X=x, M=0)} = \\left(\\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}\\right) \\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right)$$\nTaking the natural logarithm gives the log-odds for the subset of observed data:\n$$ \\text{log-odds}(P(Y=1 \\mid X=x, M=0)) = \\log\\left(\\frac{p(x \\mid Y=1)\\pi}{p(x \\mid Y=0)(1-\\pi)}\\right) + \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right) $$\n$$ \\text{log-odds}(P(Y=1 \\mid X=x, M=0)) = \\text{log-odds}(P(Y=1 \\mid X=x)) + \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right) $$\nThe problem defines the shift $s$ as this additive constant. Therefore:\n$$s = \\log\\left(\\frac{1-\\alpha_1}{1-\\alpha_0}\\right)$$\nThis shift represents the bias introduced in the log-odds scale when a model is trained only on the subset of data with observed features, under the specified MAR mechanism. The bias depends solely on the differential rates of observation between the two classes.\n\nThese derivations complete Task A. The formulas will now be used for computation in Task B.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes biases and log-odds shift for given test cases\n    based on a model of missing data in binary classification.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (pi, alpha_0, alpha_1).\n    test_cases = [\n        (0.5, 0.2, 0.8),\n        (0.4, 0.3, 0.3),\n        (0.3, 0.01, 0.99),\n    ]\n\n    # List to store the results for all test cases.\n    results = []\n\n    for case in test_cases:\n        pi, alpha_0, alpha_1 = case\n\n        # --- Calculate b_1, the bias for M=1 ---\n        # Formula: b_1 = (pi * (1-pi) * (alpha_1 - alpha_0)) / (alpha_1 * pi + alpha_0 * (1-pi))\n        # This formula is well-defined as the denominator is P(M=1), which is > 0.\n        b1_numerator = pi * (1.0 - pi) * (alpha_1 - alpha_0)\n        b1_denominator = alpha_1 * pi + alpha_0 * (1.0 - pi)\n        b1 = b1_numerator / b1_denominator\n\n        # --- Calculate b_0, the bias for M=0 ---\n        # Formula: b_0 = (pi * (1-pi) * (alpha_0 - alpha_1)) / ((1-alpha_1) * pi + (1-alpha_0) * (1-pi))\n        # This formula is well-defined as the denominator is P(M=0), which is > 0.\n        b0_numerator = pi * (1.0 - pi) * (alpha_0 - alpha_1)\n        b0_denominator = (1.0 - alpha_1) * pi + (1.0 - alpha_0) * (1.0 - pi)\n        b0 = b0_numerator / b0_denominator\n\n        # --- Calculate s, the log-odds shift ---\n        # Formula: s = log((1-alpha_1) / (1-alpha_0))\n        # np.log is the natural logarithm.\n        # The argument is well-defined and positive because 0 < alpha_0, alpha_1 < 1.\n        s = np.log((1.0 - alpha_1) / (1.0 - alpha_0))\n\n        results.extend([b1, b0, s])\n\n    # Format the final output string exactly as required.\n    # Each number is rounded to 6 decimal places.\n    output_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n    print(output_str)\n\n# Execute the main function.\nsolve()\n```"
        },
        {
            "introduction": "The philosophical divide between generative and discriminative approaches leads to different criteria for what makes a feature \"important.\" A generative model values a feature that fits its class-conditional density well, whereas a discriminative model values a feature that helps separate the classes at the decision boundary. This exercise provides a hands-on comparison of these two perspectives by asking you to implement and contrast two feature selection algorithms . You will see how a generative, likelihood-based score and a discriminative, gradient-based score can sometimes lead to different conclusions about which features are most useful.",
            "id": "3124940",
            "problem": "You are given a binary classification setting with random variables $X \\in \\mathbb{R}^d$ and $Y \\in \\{0,1\\}$. Two families of models are considered: a generative model that specifies the class-conditional distribution $p(x \\mid y)$, and a discriminative model that specifies the conditional class probability $p(y \\mid x)$. Your task is to implement two feature selection procedures and compare them across several synthetic test cases.\n\nThe fundamental base for this task is the definition of conditional probability and Bayes rule, the maximum likelihood principle for parameter estimation, and the well-tested formulation of the logistic regression model for $p(y \\mid x)$. The generative procedure will score features by their contribution to the empirical expectation of the class-conditional log-likelihood, while the discriminative procedure will score features by the empirical average magnitude of the gradient of the log-conditional probability with respect to each feature.\n\nDefinitions to use:\n- The empirical expectation of a function $f(X,Y)$ over a dataset $\\{(x_i,y_i)\\}_{i=1}^n$ is $\\frac{1}{n}\\sum_{i=1}^n f(x_i,y_i)$.\n- Under the naive Bayes assumption of feature-wise independence, a class-conditional Gaussian model takes the form $p(x \\mid y=k) = \\prod_{j=1}^d \\mathcal{N}(x_j \\mid \\mu_{k,j}, \\sigma_{k,j}^2)$, where $\\mu_{k,j}$ and $\\sigma_{k,j}^2$ are the class-specific mean and variance of feature $j$ for class $k \\in \\{0,1\\}$, estimated by Maximum Likelihood Estimation (MLE).\n- Logistic regression models $p(y=1 \\mid x) = \\sigma(w^\\top x + b)$, where $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ is the logistic function, $w \\in \\mathbb{R}^d$ is the weight vector, and $b \\in \\mathbb{R}$ is the bias, estimated by Maximum Likelihood Estimation (MLE), optionally with $\\ell_2$-regularization (also known as Ridge penalty) to improve numerical stability.\n\nYour program must implement:\n- Generative feature score for feature $j$: $s^{\\text{gen}}_j = \\mathbb{E}\\left[\\log p(x_j \\mid y)\\right]$, where the expectation is taken over the empirical dataset and $p(x_j \\mid y)$ is the one-dimensional Gaussian $\\mathcal{N}(x_j \\mid \\mu_{y,j}, \\sigma_{y,j}^2)$ with parameters learned by MLE from the training data restricted to class $y$. For numeric stability, you may floor variances at a small positive value $10^{-6}$.\n- Discriminative feature score for feature $j$: $s^{\\text{disc}}_j = \\mathbb{E}\\left[\\left|\\frac{\\partial}{\\partial x_j}\\log p(y \\mid x)\\right|\\right]$ evaluated at the fitted logistic regression parameters $(w,b)$ found by MLE (with an $\\ell_2$-penalty $\\lambda$ on $w$ of magnitude $\\lambda = 0.1$ to ensure a well-posed optimization). The gradient is taken with respect to the input feature $x_j$.\n\nFor both procedures, select the top-$k$ features by descending score, breaking ties by preferring the smaller feature index. Report the selected feature index lists for each test case.\n\nData generation:\nAll test cases use a fixed pseudo-random seed $42$ to ensure reproducibility. Each case is a binary classification problem with $d=6$ features. Independent Gaussian features are used unless stated otherwise. For class $y=0$ and class $y=1$, per-feature means and variances are specified below. For each case, draw $n_0$ samples from the class $y=0$ distribution and $n_1$ samples from the class $y=1$ distribution, concatenate them to form the dataset $\\{(x_i,y_i)\\}_{i=1}^{n}$ with $n=n_0+n_1$, and set $k$ as specified. Angles do not appear, and there are no physical units.\n\n- Case $1$ (balanced, two informative features):\n  - $n_0 = 200$, $n_1 = 200$, $d = 6$, $k=2$.\n  - Class $0$: feature $0$ has mean $-2$ and variance $1$; feature $1$ has mean $2$ and variance $1$; features $2,3,4,5$ have mean $0$ and variance $3$.\n  - Class $1$: feature $0$ has mean $2$ and variance $1$; feature $1$ has mean $-2$ and variance $1$; features $2,3,4,5$ have mean $0$ and variance $3$.\n\n- Case $2$ (balanced, one low-variance highly informative feature):\n  - $n_0 = 300$, $n_1 = 300$, $d = 6$, $k=2$.\n  - Class $0$: feature $0$ has mean $-1$ and variance $1$; feature $2$ has mean $-1$ and variance $0.2$; features $1,3,4,5$ have mean $0$ and variance $2$.\n  - Class $1$: feature $0$ has mean $1$ and variance $1$; feature $2$ has mean $1$ and variance $0.2$; features $1,3,4,5$ have mean $0$ and variance $2$.\n\n- Case $3$ (class-imbalanced, two informative features):\n  - $n_0 = 60$, $n_1 = 240$, $d = 6$, $k=2$.\n  - Class $0$: feature $0$ has mean $-1$ and variance $1$; feature $3$ has mean $-1$ and variance $1$; features $1,2,4,5$ have mean $0$ and variance $5$.\n  - Class $1$: feature $0$ has mean $1$ and variance $1$; feature $3$ has mean $1$ and variance $1$; features $1,2,4,5$ have mean $0$ and variance $5$.\n\n- Case $4$ (balanced, two redundant correlated informative features):\n  - $n_0 = 250$, $n_1 = 250$, $d = 6$, $k=2$.\n  - Construct a latent scalar $z$ per sample with $z \\sim \\mathcal{N}(\\mu_y, 1)$, where $\\mu_0 = -2$ and $\\mu_1 = 2$.\n  - Set feature $0$ as $x_0 = z + \\epsilon_0$ and feature $1$ as $x_1 = z + \\epsilon_1$, with $\\epsilon_0 \\sim \\mathcal{N}(0, 0.1^2)$ and $\\epsilon_1 \\sim \\mathcal{N}(0, 0.1^2)$.\n  - Features $2,3,4,5$ are independent noise with mean $0$ and variance $9$ for both classes.\n\nImplementation constraints:\n- Use Maximum Likelihood Estimation (MLE) to fit $\\mu_{k,j}$ and $\\sigma_{k,j}^2$ for the generative model.\n- Fit logistic regression parameters $(w,b)$ by minimizing the regularized negative log-likelihood with an $\\ell_2$-penalty of magnitude $\\lambda = 0.1$ on $w$, using a numerically stable optimizer. Standardize features for the discriminative fitting by subtracting the global mean and dividing by the global standard deviation of each feature computed across all training samples (not class-specific).\n- For the generative score, compute $s^{\\text{gen}}_j$ exactly as the empirical mean of $\\log \\mathcal{N}(x_{i,j} \\mid \\mu_{y_i,j}, \\sigma_{y_i,j}^2)$.\n- For the discriminative score, compute $s^{\\text{disc}}_j$ as the empirical mean of $\\left|\\frac{\\partial}{\\partial x_j}\\log p(y_i \\mid x_i)\\right|$ at the fitted $(w,b)$.\n\nTest suite and answer specification:\n- Implement the above for the four cases specified, using the fixed seed $42$.\n- For each case, output the top-$k$ selected feature indices under the generative scoring and under the discriminative scoring.\n- The final output must be a single line containing a comma-separated list enclosed in square brackets, where each element corresponds to a case and is itself a two-element list $[G,D]$. Here $G$ is the list of $k$ integers for the generative selection and $D$ is the list of $k$ integers for the discriminative selection. For example, an output line with two cases would look like $[[[0,1],[0,1]],[[2,0],[2,0]]]$. There must be no whitespace characters in the printed output.\n\nYour program must be a complete, runnable program that requires no user input and uses only the Python standard library, NumPy, and SciPy as specified.",
            "solution": "This problem requires the implementation and comparison of two distinct feature selection methodologies: a generative approach and a discriminative approach. The solution involves generating synthetic data for several test cases, applying both selection algorithms, and reporting the top-ranked features.\n\n### 1. Generative Feature Selection\n\nThe generative approach models the class-conditional probability distribution of the features, $p(x \\mid y)$. From this, the posterior probability $p(y \\mid x)$ can be derived using Bayes' rule, which necessitates modeling the class prior $p(y)$ as well. The specified generative model is a Naive Bayes classifier with Gaussian class-conditional distributions. This model rests on the strong assumption that features are conditionally independent given the class label.\n\nFor a given class $k \\in \\{0, 1\\}$, the distribution is:\n$$p(x \\mid y=k) = \\prod_{j=1}^d p(x_j \\mid y=k) = \\prod_{j=1}^d \\mathcal{N}(x_j \\mid \\mu_{k,j}, \\sigma_{k,j}^2)$$\nwhere $x_j$ is the $j$-th feature, and $\\mathcal{N}(x \\mid \\mu, \\sigma^2)$ is the probability density function of a Gaussian distribution.\n\n**Parameter Estimation**: The parameters $(\\mu_{k,j}, \\sigma_{k,j}^2)$ for each class $k$ and feature $j$ are estimated using Maximum Likelihood Estimation (MLE) from the training data. For a set of samples $\\{x_{i,j}\\}_{i \\mid y_i=k}$ belonging to class $k$, the MLE estimates are the sample mean and sample variance:\n$$\\hat{\\mu}_{k,j} = \\frac{1}{n_k} \\sum_{i: y_i=k} x_{i,j}$$\n$$\\hat{\\sigma}_{k,j}^2 = \\frac{1}{n_k} \\sum_{i: y_i=k} (x_{i,j} - \\hat{\\mu}_{k,j})^2$$\nwhere $n_k$ is the number of samples in class $k$. For numerical stability, estimated variances are floored at a small positive value, $\\epsilon = 10^{-6}$.\n\n**Feature Scoring**: The score for feature $j$, denoted $s^{\\text{gen}}_j$, is defined as the empirical expectation of the class-conditional log-likelihood for that feature:\n$$s^{\\text{gen}}_j = \\mathbb{E}\\left[\\log p(x_j \\mid y)\\right] = \\frac{1}{n} \\sum_{i=1}^n \\log p(x_{i,j} \\mid y_i)$$\nThis score is calculated using the estimated parameters $(\\hat{\\mu}_{y_i,j}, \\hat{\\sigma}_{y_i,j}^2)$ for each sample $(x_i, y_i)$. The log-likelihood for a single observation $x_{i,j}$ given its class $y_i$ is:\n$$\\log \\mathcal{N}(x_{i,j} \\mid \\hat{\\mu}_{y_i,j}, \\hat{\\sigma}_{y_i,j}^2) = -\\frac{1}{2} \\log(2\\pi\\hat{\\sigma}_{y_i,j}^2) - \\frac{(x_{i,j} - \\hat{\\mu}_{y_i,j})^2}{2\\hat{\\sigma}_{y_i,j}^2}$$\nThis score measures how well, on average, the data for feature $j$ conforms to the learned class-conditional Gaussian models. Features that yield a higher average log-likelihood are considered more informative.\n\n### 2. Discriminative Feature Selection\n\nThe discriminative approach models the posterior probability $p(y \\mid x)$ directly, bypassing the need to model the feature distribution $p(x \\mid y)$. The specified model is logistic regression.\n\nThe logistic regression model defines the probability of class $1$ as:\n$$p(y=1 \\mid x) = \\sigma(w^\\top x' + b) = \\frac{1}{1 + \\exp(-(w^\\top x' + b))}$$\nwhere $w \\in \\mathbb{R}^d$ is a weight vector, $b \\in \\mathbb{R}$ is a bias term, and $\\sigma(\\cdot)$ is the logistic sigmoid function. The input vector $x'$ represents the features after standardization (subtracting the global mean and dividing by the global standard deviation of each feature across all data).\n\n**Parameter Estimation**: The parameters $(w, b)$ are found by minimizing the regularized negative log-likelihood (also known as cross-entropy loss with an $\\ell_2$ penalty). The objective function to minimize is:\n$$L(w,b) = -\\sum_{i=1}^n \\left[ y_i \\log p(y_i=1 \\mid x_i) + (1-y_i) \\log p(y_i=0 \\mid x_i) \\right] + \\frac{\\lambda}{2} \\|w\\|_2^2$$\nwhere $\\lambda=0.1$ is the regularization strength. This is a convex optimization problem, which can be solved efficiently using numerical methods like L-BFGS.\n\n**Feature Scoring**: The score for feature $j$, $s^{\\text{disc}}_j$, is the empirical average of the magnitude of the gradient of the log-conditional probability with respect to the original (unstandardized) feature $x_j$:\n$$s^{\\text{disc}}_j = \\mathbb{E}\\left[\\left|\\frac{\\partial}{\\partial x_j}\\log p(y \\mid x)\\right|\\right] = \\frac{1}{n} \\sum_{i=1}^n \\left|\\frac{\\partial}{\\partial x_j}\\log p(y_i \\mid x_i)\\right|$$\nWe derive the gradient using the chain rule. Let $z_i = w^\\top x'_i + b$. The log-likelihood for a single sample is $l_i = \\log p(y_i | x_i)$. The gradient with respect to the linear activation $z_i$ is $\\frac{\\partial l_i}{\\partial z_i} = y_i - \\sigma(z_i)$. Since $x'_{i,j} = (x_{i,j} - \\mu^{\\text{glob}}_j) / \\sigma^{\\text{glob}}_j$, we have $\\frac{\\partial z_i}{\\partial x_j} = \\frac{w_j}{\\sigma^{\\text{glob}}_j}$. Thus, the gradient with respect to the original feature is:\n$$\\frac{\\partial l_i}{\\partial x_j} = \\frac{\\partial l_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial x_j} = (y_i - \\sigma(z_i)) \\frac{w_j}{\\sigma^{\\text{glob}}_j}$$\nThe score for feature $j$ becomes:\n$$s^{\\text{disc}}_j = \\frac{1}{n} \\sum_{i=1}^n \\left| (y_i - \\sigma(z_i)) \\frac{w_j}{\\sigma^{\\text{glob}}_j} \\right| = \\left|\\frac{w_j}{\\sigma^{\\text{glob}}_j}\\right| \\left(\\frac{1}{n} \\sum_{i=1}^n |y_i - \\sigma(z_i)|\\right)$$\nSince the term $\\frac{1}{n} \\sum_i |y_i - \\sigma(z_i)|$ is constant across all features $j$, the ranking of features is determined solely by the magnitude of the learned weight $w_j$ for the standardized feature, divided by the standard deviation of the original feature, $\\sigma^{\\text{glob}}_j$. This metric quantifies the influence of each feature on the model's prediction.\n\n### 3. Feature Ranking and Selection\n\nFor both methods, after computing the scores $\\{s_j\\}$ for all features $j=1, \\dots, d$, the features are ranked in descending order of their scores. To ensure deterministic ranking, ties are broken by preferring the feature with the smaller index. The top-$k$ features from this sorted list are then selected. Computationally, this is achieved via a lexicographical sort on the keys $(-\\text{score}, \\text{index})$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit, xlogy\n\ndef generate_data(case_params, rng):\n    \"\"\"\n    Generates synthetic data for a given test case.\n    \"\"\"\n    d = case_params['d']\n    n0, n1 = case_params['n0'], case_params['n1']\n    n = n0 + n1\n    X = np.zeros((n, d))\n    y = np.concatenate([np.zeros(n0), np.ones(n1)])\n\n    if case_params['id'] in [1, 2, 3]:\n        means0, var0 = case_params['class0_params']\n        means1, var1 = case_params['class1_params']\n        std0, std1 = np.sqrt(var0), np.sqrt(var1)\n        \n        X[:n0, :] = rng.normal(loc=means0, scale=std0, size=(n0, d))\n        X[n0:, :] = rng.normal(loc=means1, scale=std1, size=(n1, d))\n    \n    elif case_params['id'] == 4:\n        mu0, mu1 = -2, 2\n        \n        # Generate latent variable z\n        z0 = rng.normal(loc=mu0, scale=1, size=n0)\n        z1 = rng.normal(loc=mu1, scale=1, size=n1)\n        z = np.concatenate([z0, z1])\n        \n        # Generate correlated features x0, x1 from z\n        epsilon0 = rng.normal(loc=0, scale=0.1, size=n)\n        epsilon1 = rng.normal(loc=0, scale=0.1, size=n)\n        X[:, 0] = z + epsilon0\n        X[:, 1] = z + epsilon1\n        \n        # Generate independent noise features\n        X[:, 2:] = rng.normal(loc=0, scale=3, size=(n, d - 2))\n        \n    return X, y\n\ndef get_top_k_indices(scores, k):\n    \"\"\"\n    Selects top-k feature indices based on scores, with tie-breaking.\n    \"\"\"\n    indices = np.arange(len(scores))\n    # Sort by score (descending) and then index (ascending) to break ties\n    sorted_indices = np.lexsort((indices, -scores))\n    return sorted_indices[:k]\n    \ndef generative_selector(X, y, k):\n    \"\"\"\n    Computes generative feature scores and selects top k.\n    \"\"\"\n    n, d = X.shape\n    \n    X0 = X[y == 0]\n    X1 = X[y == 1]\n    n0, n1 = len(X0), len(X1)\n\n    # MLE for parameters per class\n    mu0 = np.mean(X0, axis=0) if n0 > 0 else np.zeros(d)\n    var0 = np.var(X0, axis=0) if n0 > 1 else np.ones(d)\n    mu1 = np.mean(X1, axis=0) if n1 > 0 else np.zeros(d)\n    var1 = np.var(X1, axis=0) if n1 > 1 else np.ones(d)\n    \n    # Apply variance floor\n    var0 = np.maximum(var0, 1e-6)\n    var1 = np.maximum(var1, 1e-6)\n    \n    mus = np.array([mu0, mu1])\n    variances = np.array([var0, var1])\n    \n    # Compute empirical expectation of log-likelihood\n    log_likelihoods = np.zeros(d)\n    for i in range(n):\n        yi = int(y[i])\n        xi = X[i, :]\n        mu_yi = mus[yi, :]\n        var_yi = variances[yi, :]\n        \n        log_pdf_i = -0.5 * np.log(2 * np.pi * var_yi) - ((xi - mu_yi)**2) / (2 * var_yi)\n        log_likelihoods += log_pdf_i\n        \n    scores = log_likelihoods / n\n\n    return get_top_k_indices(scores, k)\n\ndef discriminative_selector(X, y, k, lambda_reg):\n    \"\"\"\n    Computes discriminative feature scores and selects top k.\n    \"\"\"\n    n, d = X.shape\n    \n    # Standardize features (globally)\n    mu_glob = np.mean(X, axis=0)\n    std_glob = np.std(X, axis=0)\n    std_glob[std_glob == 0] = 1.0  # Avoid division by zero\n    \n    X_std = (X - mu_glob) / std_glob\n    \n    # Objective function for logistic regression\n    def objective_function(params, X_s, y_l, lambda_r):\n        w = params[:-1]\n        b = params[-1]\n        z = X_s @ w + b\n        y_hat = expit(z)\n        \n        # Regularized negative log-likelihood (cost)\n        nll = -np.sum(xlogy(y_l, y_hat) + xlogy(1 - y_l, 1 - y_hat))\n        l2_penalty = 0.5 * lambda_r * np.sum(w**2)\n        cost = nll + l2_penalty\n        \n        # Gradient\n        error = y_hat - y_l\n        grad_w = X_s.T @ error + lambda_r * w\n        grad_b = np.sum(error)\n        grad = np.concatenate((grad_w, [grad_b]))\n        return cost, grad\n\n    initial_params = np.zeros(d + 1)\n    res = minimize(\n        objective_function,\n        initial_params,\n        args=(X_std, y, lambda_reg),\n        jac=True,\n        method='L-BFGS-B'\n    )\n    \n    w_opt = res.x[:-1]\n    \n    # Feature score is proportional to |w_j / std_j|\n    scores = np.abs(w_opt / std_glob)\n    \n    return get_top_k_indices(scores, k)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            'id': 1, 'n0': 200, 'n1': 200, 'd': 6, 'k': 2,\n            'class0_params': (np.array([-2, 2, 0, 0, 0, 0]), np.array([1, 1, 3, 3, 3, 3])),\n            'class1_params': (np.array([2, -2, 0, 0, 0, 0]), np.array([1, 1, 3, 3, 3, 3]))\n        },\n        {\n            'id': 2, 'n0': 300, 'n1': 300, 'd': 6, 'k': 2,\n            'class0_params': (np.array([-1, 0, -1, 0, 0, 0]), np.array([1, 2, 0.2, 2, 2, 2])),\n            'class1_params': (np.array([1, 0, 1, 0, 0, 0]), np.array([1, 2, 0.2, 2, 2, 2]))\n        },\n        {\n            'id': 3, 'n0': 60, 'n1': 240, 'd': 6, 'k': 2,\n            'class0_params': (np.array([-1, 0, 0, -1, 0, 0]), np.array([1, 5, 5, 1, 5, 5])),\n            'class1_params': (np.array([1, 0, 0, 1, 0, 0]), np.array([1, 5, 5, 1, 5, 5]))\n        },\n        {\n            'id': 4, 'n0': 250, 'n1': 250, 'd': 6, 'k': 2\n            # Special generation logic is handled inside generate_data\n        }\n    ]\n    \n    rng = np.random.default_rng(42)\n    lambda_reg = 0.1\n    \n    results_str_list = []\n    \n    for case in test_cases:\n        X, y = generate_data(case, rng)\n        k = case['k']\n        \n        gen_indices = generative_selector(X, y, k)\n        disc_indices = discriminative_selector(X, y, k, lambda_reg)\n        \n        gen_indices_str = f\"[{','.join(map(str, gen_indices))}]\"\n        disc_indices_str = f\"[{','.join(map(str, disc_indices))}]\"\n        \n        results_str_list.append(f\"[{gen_indices_str},{disc_indices_str}]\")\n        \n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While often presented as a strict dichotomy, the line between generative and discriminative modeling can be blurred. In practice, we can construct hybrid models that blend the strengths of both paradigms. This advanced exercise guides you through the process of building such a model, where a standard discriminative loss function is regularized by a term that encourages the model to also explain the marginal distribution of the data, $p(x)$ . By optimizing a trade-off parameter, $\\lambda$, you will investigate how this generative regularization can improve a model's ability to generalize to new, unseen data.",
            "id": "3124935",
            "problem": "You are given a binary classification setting with inputs $x \\in \\mathbb{R}$ and labels $y \\in \\{0,1\\}$. Consider the hybrid empirical risk that combines a conditional negative log-loss with a marginal density regularizer. The model uses a shared parameter $ \\mu \\in \\mathbb{R} $ to couple the conditional and marginal components, and a slope parameter $ a \\in \\mathbb{R} $ for the conditional link. The conditional model is $p(y \\mid x) = \\mathrm{Bernoulli}(\\sigma(z))$ with $z = a(x - \\mu)$, where $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ is the logistic sigmoid. The marginal density model for $x$ is a Gaussian with known variance $s^2$ and unknown mean $\\mu$, that is $p(x) = \\mathcal{N}(\\mu, s^2)$. For a dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$, define the hybrid empirical risk\n$$\nR_n(\\mu, a; \\lambda) \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\left[ - y_i \\log \\sigma\\!\\left(a(x_i - \\mu)\\right) - (1-y_i) \\log \\left(1 - \\sigma\\!\\left(a(x_i - \\mu)\\right)\\right) \\right] \\;+\\; \\lambda \\cdot \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right],\n$$\nwhere $\\lambda \\ge 0$ is a trade-off parameter. The first summand is the empirical conditional negative log-loss, and the second is the empirical negative log marginal density with a penalty weight $\\lambda$. Your task is to analyze how $\\lambda$ affects generalization by deriving learning rules from first principles and then evaluating test conditional loss.\n\nFundamental base for derivation:\n- Use the definition of the logistic sigmoid $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ and the Bernoulli negative log-likelihood.\n- Use the negative log-density of a Gaussian $\\mathcal{N}(\\mu, s^2)$ in one dimension.\n- Use the concept of empirical risk minimization to connect training loss to generalization.\n\nRequired derivations and computations:\n1) Starting from the definitions above, derive the gradients of $R_n(\\mu, a; \\lambda)$ with respect to $\\mu$ and $a$. Express them in terms of $\\{x_i, y_i\\}_{i=1}^n$, $a$, $\\mu$, $s^2$, and $\\lambda$, using only basic calculus and the chain rule. Do not assume any pre-known closed-form for the gradients beyond the fundamental definitions stated here.\n2) Implement full-batch gradient descent to minimize $R_n(\\mu, a; \\lambda)$ over $(\\mu, a)$ for a fixed $\\lambda$ using the following fixed settings to ensure determinism:\n   - Initialization: $a_0 = 1.0$ and $\\mu_0 = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n   - Learning rate schedule: $\\eta_k = \\frac{\\eta_0}{\\sqrt{k+1}}$ with $\\eta_0 = 0.3$, where $k$ is the $k$-th iteration index starting at $k = 0$.\n   - Number of iterations: $T = 2000$.\n   - Known variance: $s^2 = 1.0$.\n   - Parameter clipping after each update: clip $a$ to the interval $[-10, 10]$ and $\\mu$ to $[-5, 5]$.\n3) For each trained $(\\widehat{\\mu}, \\widehat{a})$, evaluate the test conditional negative log-loss\n$$\n\\widehat{R}_{\\text{test}}(\\widehat{\\mu}, \\widehat{a}) \\;=\\; \\frac{1}{m} \\sum_{j=1}^m \\left[ - y^{\\text{test}}_j \\log \\sigma\\!\\left(\\widehat{a}(x^{\\text{test}}_j - \\widehat{\\mu})\\right) - \\left(1-y^{\\text{test}}_j\\right) \\log \\left(1 - \\sigma\\!\\left(\\widehat{a}(x^{\\text{test}}_j - \\widehat{\\mu})\\right)\\right) \\right].\n$$\n4) For each dataset below, and for each $\\lambda$ in the specified grid, train using the prescription in item $2)$, compute the test conditional loss as in item $3)$, and select the $\\lambda$ that minimizes the test conditional loss. In case of ties up to an absolute tolerance of $10^{-6}$, select the smallest $\\lambda$ among the minimizers.\n\nThe test suite consists of three datasets, each with a training and a test split, and the same $\\lambda$ grid and variance $s^2$:\n- Dataset A (balanced clusters):\n  - Training: $x^{\\text{train}} = [-1.3, -1.0, -0.7, 0.6, 0.9, 1.2]$, $y^{\\text{train}} = [0, 0, 0, 1, 1, 1]$.\n  - Test: $x^{\\text{test}} = [-1.1, -0.9, 0.8, 1.1]$, $y^{\\text{test}} = [0, 0, 1, 1]$.\n- Dataset B (no signal):\n  - Training: $x^{\\text{train}} = [-0.3, -0.1, 0.1, 0.3, -0.2, 0.2]$, $y^{\\text{train}} = [0, 1, 0, 1, 1, 0]$.\n  - Test: $x^{\\text{test}} = [-0.4, 0.0, 0.4]$, $y^{\\text{test}} = [0, 1, 0]$.\n- Dataset C (class imbalance):\n  - Training: $x^{\\text{train}} = [-1.2, -0.8, 0.4, 0.8, 1.0, 1.2, 1.4]$, $y^{\\text{train}} = [0, 0, 1, 1, 1, 1, 1]$.\n  - Test: $x^{\\text{test}} = [-0.9, -0.7, 0.9, 1.1]$, $y^{\\text{test}} = [0, 0, 1, 1]$.\n- Variance: $s^2 = 1.0$ for all datasets.\n- Trade-off grid: $\\Lambda = [0.0, 0.1, 1.0, 10.0]$.\n\nAngle units do not apply. There are no physical units in this problem. All outputs must be real-valued floats. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the $k$-th entry is the selected $\\lambda$ for the $k$-th dataset in the order A, B, C (for example, an output like $[0.0,0.1,1.0]$). Ensure that the implementation uses only the specifications provided and no randomness.\n\nYour final program must implement the derivations and computations and output the selected $\\lambda$ for each dataset as the single-line format described above. The goal is to connect the theory that hybrid objectives interpolate between discriminative and generative modeling to an empirical selection of $\\lambda$ that promotes better test conditional loss, thus analyzing $\\lambda$ in terms of generalization.",
            "solution": "The problem requires us to analyze a hybrid empirical risk function by deriving its gradients and implementing a gradient descent-based learning algorithm. The goal is to find the optimal trade-off parameter $\\lambda$ that minimizes the test conditional loss for three distinct datasets.\n\nThe hybrid empirical risk is given by:\n$$\nR_n(\\mu, a; \\lambda) \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\left[ - y_i \\log \\sigma\\!\\left(a(x_i - \\mu)\\right) - (1-y_i) \\log \\left(1 - \\sigma\\!\\left(a(x_i - \\mu)\\right)\\right) \\right] \\;+\\; \\lambda \\cdot \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right]\n$$\nThis can be written as the sum of a conditional loss component $L_C$ and a marginal loss component $L_M$, weighted by $\\lambda$:\n$$\nR_n(\\mu, a; \\lambda) = L_C(\\mu, a) + \\lambda L_M(\\mu)\n$$\nwhere\n$$\nL_C(\\mu, a) = \\frac{1}{n} \\sum_{i=1}^n \\left[ - y_i \\log \\sigma(z_i) - (1-y_i) \\log(1 - \\sigma(z_i)) \\right] \\text{, with } z_i = a(x_i - \\mu)\n$$\n$$\nL_M(\\mu) = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right]\n$$\n\nTo minimize this risk function using gradient descent, we must compute its partial derivatives with respect to the parameters $\\mu$ and $a$.\n\n**1. Gradient Derivations**\n\nFirst, we establish the derivative of the logistic sigmoid function $\\sigma(t) = \\frac{1}{1 + e^{-t}} = (1 + e^{-t})^{-1}$. Using the chain rule:\n$$\n\\frac{d\\sigma(t)}{dt} = -1 \\cdot (1 + e^{-t})^{-2} \\cdot (-e^{-t}) = \\frac{e^{-t}}{(1+e^{-t})^2}\n$$\nThis can be rewritten as:\n$$\n\\frac{d\\sigma(t)}{dt} = \\frac{1}{1+e^{-t}} \\cdot \\frac{e^{-t}}{1+e^{-t}} = \\sigma(t) \\cdot \\frac{(1+e^{-t})-1}{1+e^{-t}} = \\sigma(t) \\cdot \\left(1 - \\frac{1}{1+e^{-t}}\\right) = \\sigma(t)(1-\\sigma(t))\n$$\n\nNow, let's find the gradient of the per-sample conditional loss term, $l_i = - y_i \\log \\sigma(z_i) - (1-y_i) \\log(1 - \\sigma(z_i))$, with respect to $z_i$:\n$$\n\\frac{\\partial l_i}{\\partial z_i} = \\frac{\\partial l_i}{\\partial \\sigma(z_i)} \\frac{d\\sigma(z_i)}{dz_i}\n$$\nThe first part is:\n$$\n\\frac{\\partial l_i}{\\partial \\sigma(z_i)} = -\\frac{y_i}{\\sigma(z_i)} - \\frac{1-y_i}{1-\\sigma(z_i)}(-1) = \\frac{-y_i(1-\\sigma(z_i)) + (1-y_i)\\sigma(z_i)}{\\sigma(z_i)(1-\\sigma(z_i))} = \\frac{\\sigma(z_i) - y_i}{\\sigma(z_i)(1-\\sigma(z_i))}\n$$\nCombining this with the derivative of the sigmoid function:\n$$\n\\frac{\\partial l_i}{\\partial z_i} = \\frac{\\sigma(z_i) - y_i}{\\sigma(z_i)(1-\\sigma(z_i))} \\cdot \\sigma(z_i)(1-\\sigma(z_i)) = \\sigma(z_i) - y_i\n$$\n\nWith this result, we can find the gradients of the total empirical risk $R_n$.\n\n**Gradient with respect to $\\mu$:**\n$$\n\\frac{\\partial R_n}{\\partial \\mu} = \\frac{\\partial L_C}{\\partial \\mu} + \\lambda \\frac{\\partial L_M}{\\partial \\mu}\n$$\nThe gradient of the conditional part $L_C$ is found using the chain rule:\n$$\n\\frac{\\partial L_C}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial l_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\mu}\n$$\nSince $z_i = a(x_i - \\mu)$, we have $\\frac{\\partial z_i}{\\partial \\mu} = -a$.\n$$\n\\frac{\\partial L_C}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(a(x_i - \\mu)) - y_i) \\cdot (-a) = -\\frac{a}{n} \\sum_{i=1}^n (\\sigma(a(x_i - \\mu)) - y_i)\n$$\nThe gradient of the marginal part $L_M$ is:\n$$\n\\frac{\\partial L_M}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial \\mu} \\left( \\frac{1}{2}\\log(2\\pi s^2) + \\frac{(x_i - \\mu)^2}{2 s^2} \\right) = \\frac{1}{n} \\sum_{i=1}^n \\left( 0 + \\frac{2(x_i - \\mu) \\cdot (-1)}{2s^2} \\right) = -\\frac{1}{ns^2} \\sum_{i=1}^n (x_i - \\mu)\n$$\nCombining both parts, the full gradient with respect to $\\mu$ is:\n$$\n\\frac{\\partial R_n}{\\partial \\mu} = -\\frac{a}{n} \\sum_{i=1}^n (\\sigma(a(x_i-\\mu)) - y_i) - \\frac{\\lambda}{ns^2} \\sum_{i=1}^n (x_i - \\mu)\n$$\nThis can be written as an average over the samples:\n$$\n\\frac{\\partial R_n}{\\partial \\mu} = \\frac{1}{n} \\sum_{i=1}^n \\left[ -a(\\sigma(a(x_i-\\mu)) - y_i) - \\frac{\\lambda}{s^2}(x_i - \\mu) \\right]\n$$\n\n**Gradient with respect to $a$:**\n$$\n\\frac{\\partial R_n}{\\partial a} = \\frac{\\partial L_C}{\\partial a} + \\lambda \\frac{\\partial L_M}{\\partial a}\n$$\nThe gradient of the conditional part $L_C$ with respect to $a$ is:\n$$\n\\frac{\\partial L_C}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial l_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial a}\n$$\nSince $z_i = a(x_i - \\mu)$, we have $\\frac{\\partial z_i}{\\partial a} = x_i - \\mu$.\n$$\n\\frac{\\partial L_C}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n (\\sigma(a(x_i - \\mu)) - y_i) (x_i - \\mu)\n$$\nThe marginal part $L_M$ does not depend on $a$, so $\\frac{\\partial L_M}{\\partial a} = 0$.\nTherefore, the full gradient with respect to $a$ is:\n$$\n\\frac{\\partial R_n}{\\partial a} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)(\\sigma(a(x_i - \\mu)) - y_i)\n$$\n\nThese derived gradients are used in a full-batch gradient descent algorithm to find the parameters $(\\widehat{\\mu}, \\widehat{a})$ that minimize the hybrid empirical risk for each given value of $\\lambda$. The algorithm proceeds by initializing the parameters, then iteratively updating them in the opposite direction of the gradient. After training, the performance is measured by the conditional negative log-loss on a separate test set. This process is repeated for each dataset and each value of $\\lambda$ to find the configuration that yields the best generalization performance, as measured by the lowest test loss.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the best lambda for a hybrid generative-discriminative model.\n    \"\"\"\n\n    def sigmoid(t):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        # Clip the input to avoid overflow in np.exp\n        t = np.clip(t, -500, 500)\n        return 1 / (1 + np.exp(-t))\n\n    def softplus(x):\n        \"\"\"\n        Numerically stable computation of log(1 + exp(x)).\n        This is equivalent to: np.log(1.0 + np.exp(-np.abs(x))) + np.maximum(x, 0)\n        \"\"\"\n        return np.where(x > 30, x, np.log(1 + np.exp(x)))\n\n    test_cases = [\n        (\n            np.array([-1.3, -1.0, -0.7, 0.6, 0.9, 1.2]),\n            np.array([0, 0, 0, 1, 1, 1]),\n            np.array([-1.1, -0.9, 0.8, 1.1]),\n            np.array([0, 0, 1, 1])\n        ),\n        (\n            np.array([-0.3, -0.1, 0.1, 0.3, -0.2, 0.2]),\n            np.array([0, 1, 0, 1, 1, 0]),\n            np.array([-0.4, 0.0, 0.4]),\n            np.array([0, 1, 0])\n        ),\n        (\n            np.array([-1.2, -0.8, 0.4, 0.8, 1.0, 1.2, 1.4]),\n            np.array([0, 0, 1, 1, 1, 1, 1]),\n            np.array([-0.9, -0.7, 0.9, 1.1]),\n            np.array([0, 0, 1, 1])\n        )\n    ]\n    \n    s2 = 1.0\n    lambda_grid = [0.0, 0.1, 1.0, 10.0]\n    eta0 = 0.3\n    T = 2000\n    \n    final_lambdas = []\n\n    for train_x, train_y, test_x, test_y in test_cases:\n        min_test_loss = float('inf')\n        best_lambda_for_case = -1.0\n        \n        for lambda_val in lambda_grid:\n            # Initialization\n            mu = np.mean(train_x)\n            a = 1.0\n            \n            # Gradient Descent\n            for k in range(T):\n                eta_k = eta0 / np.sqrt(k + 1)\n                \n                # Calculate gradients\n                z = a * (train_x - mu)\n                sigma_z = sigmoid(z)\n                \n                # Gradient w.r.t. mu\n                grad_mu_term1 = -a * (sigma_z - train_y)\n                grad_mu_term2 = lambda_val * (-(1 / s2) * (train_x - mu))\n                grad_mu = np.mean(grad_mu_term1 + grad_mu_term2)\n\n                # Gradient w.r.t. a\n                grad_a = np.mean((train_x - mu) * (sigma_z - train_y))\n                \n                # Update parameters\n                mu -= eta_k * grad_mu\n                a -= eta_k * grad_a\n                \n                # Clipping\n                mu = np.clip(mu, -5.0, 5.0)\n                a = np.clip(a, -10.0, 10.0)\n            \n            mu_hat, a_hat = mu, a\n            \n            # Calculate test conditional negative log-loss\n            z_test = a_hat * (test_x - mu_hat)\n            \n            # Use numerically stable loss formulation: y*softplus(-z) + (1-y)*softplus(z)\n            loss_per_item = test_y * softplus(-z_test) + (1 - test_y) * softplus(z_test)\n            current_test_loss = np.mean(loss_per_item)\n\n            # Update best lambda based on minimizing test loss\n            # Tie-breaking rule: choose smallest lambda for ties within 1e-6 tolerance.\n            # Since lambda_grid is sorted, we only update if there is a strict improvement.\n            if current_test_loss  min_test_loss - 1e-6:\n                min_test_loss = current_test_loss\n                best_lambda_for_case = lambda_val\n            # If best_lambda_for_case is unassigned, assign it\n            if best_lambda_for_case == -1.0:\n                 best_lambda_for_case = lambda_val\n\n        final_lambdas.append(best_lambda_for_case)\n        \n    print(f\"[{','.join(map(str, final_lambdas))}]\")\n\nsolve()\n```"
        }
    ]
}