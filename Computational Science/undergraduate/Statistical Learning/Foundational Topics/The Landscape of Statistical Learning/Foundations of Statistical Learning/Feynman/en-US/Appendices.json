{
    "hands_on_practices": [
        {
            "introduction": "Understanding a model's performance requires looking beyond a single error metric. The bias-variance decomposition is a foundational concept that dissects a model's expected prediction error into three distinct components: irreducible error, bias, and variance. This practice  provides a concrete, mathematical derivation of this trade-off in the context of polynomial regression, revealing how increasing model complexity (the polynomial degree $k$) affects each component of the error.",
            "id": "3121938",
            "problem": "Consider a one-dimensional regression problem on the interval $[-1,1]$. Let $x$ be drawn from the uniform distribution on $[-1,1]$, and let the response be generated by $y = f^{\\star}(x) + \\varepsilon$, where $\\varepsilon$ is independent noise with $\\mathbb{E}[\\varepsilon \\mid x] = 0$ and $\\operatorname{Var}(\\varepsilon \\mid x) = \\sigma^{2}$. Assume the unknown regression function $f^{\\star}$ admits an $L^{2}$ expansion with respect to an orthonormal polynomial basis $\\{\\phi_{j}\\}_{j \\ge 0}$ on $[-1,1]$ under the uniform measure, namely $f^{\\star}(x) = \\sum_{j=0}^{\\infty} \\theta_{j} \\phi_{j}(x)$ with $\\sum_{j=0}^{\\infty} \\theta_{j}^{2} < \\infty$, where $\\int_{-1}^{1} \\phi_{j}(x)\\phi_{\\ell}(x)\\,\\frac{dx}{2} = \\mathbf{1}\\{j=\\ell\\}$.\n\nYou collect a training sample $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ of size $n$, with the $x_{i}$ chosen deterministically so that the first $k+1$ basis functions are empirically orthonormal and orthogonal to all higher-degree basis functions with respect to the discrete uniform measure on the design points:\n\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\phi_{j}(x_{i})\\,\\phi_{\\ell}(x_{i}) = \n\\begin{cases}\n1, & j=\\ell \\le k,\\\\\n0, & j \\ne \\ell,\\; j,\\ell \\le k,\\\\\n0, & \\ell \\le k < j.\n\\end{cases}\n$$\n\nYou fit the degree-$k$ polynomial least-squares predictor $\\widehat{f}_{k}(x) = \\sum_{j=0}^{k} \\widehat{\\beta}_{j}\\,\\phi_{j}(x)$ by ordinary least squares on the features $\\{\\phi_{0},\\dots,\\phi_{k}\\}$.\n\nUsing only first principles (namely, the definitions of conditional expectation and variance, the orthonormality relations above, and the normal equations for least squares), derive the bias-variance decomposition of the expected integrated squared prediction error for a fresh test point $(x,y)$ drawn independently from the same data-generating process:\n\n$$\n\\mathcal{R}_{k,n} \\equiv \\mathbb{E}\\big[(y - \\widehat{f}_{k}(x))^{2}\\big],\n$$\n\nwhere the expectation is over the training noise, the training inputs, the test input $x$, and the test noise. Express your final result in closed form as a function of $k$, $n$, $\\sigma^{2}$, and the coefficients $\\{\\theta_{j}\\}_{j \\ge 0}$. Your final answer must be a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It represents a standard theoretical exercise in statistical learning. We may therefore proceed with the derivation.\n\nThe quantity to be analyzed is the expected integrated squared prediction error, given by\n$$ \\mathcal{R}_{k,n} \\equiv \\mathbb{E}\\big[(y - \\widehat{f}_{k}(x))^{2}\\big] $$\nThe total expectation $\\mathbb{E}$ is taken over all sources of randomness: the test point's input $x$ and its associated noise $\\varepsilon$, and the noise in the training data, which we denote as $\\{\\varepsilon_{i}\\}_{i=1}^{n}$. The training inputs $\\{x_i\\}_{i=1}^n$ are deterministic. The fitted model $\\widehat{f}_k$ depends on the training data $\\{(x_i, y_i)\\}_{i=1}^n$ and thus on the training noise $\\{\\varepsilon_i\\}_{i=1}^n$.\n\nWe begin by applying the law of total expectation, conditioning on the training data $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ and the test input $x$. The response $y$ is given by $y = f^{\\star}(x)+\\varepsilon$.\n$$ \\mathcal{R}_{k,n} = \\mathbb{E}_{\\mathcal{D}, x} \\left[ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} \\left[ (f^{\\star}(x) + \\varepsilon - \\widehat{f}_{k}(x))^2 \\mid \\mathcal{D}, x \\right] \\right] $$\nExpanding the square, we get:\n$$ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 + 2\\varepsilon(f^{\\star}(x) - \\widehat{f}_{k}(x)) + \\varepsilon^2 $$\nThe conditional expectation of the cross-term is zero, since the test noise $\\varepsilon$ is independent of $\\mathcal{D}$ and $x$, and $\\mathbb{E}[\\varepsilon | x] = 0$:\n$$ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} \\left[ 2\\varepsilon(f^{\\star}(x) - \\widehat{f}_{k}(x)) \\mid \\mathcal{D}, x \\right] = 2(f^{\\star}(x) - \\widehat{f}_{k}(x))\\mathbb{E}_{\\varepsilon | x}[\\varepsilon] = 0 $$\nThe conditional expectation of the $\\varepsilon^2$ term is the conditional variance of the noise, since its mean is zero:\n$$ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} [\\varepsilon^2 \\mid \\mathcal{D}, x] = \\mathbb{E}_{\\varepsilon|x}[\\varepsilon^2 | x] = \\operatorname{Var}(\\varepsilon|x) + (\\mathbb{E}[\\varepsilon|x])^2 = \\sigma^2 + 0^2 = \\sigma^2 $$\nSubstituting these back, the risk becomes:\n$$ \\mathcal{R}_{k,n} = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 + \\sigma^2 \\right] = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 \\right] + \\sigma^2 $$\nThe term $\\sigma^2$ is the irreducible error. The remaining term is the Mean Integrated Squared Error (MISE). We decompose it into bias and variance components by introducing the average predictor function $\\bar{f}_k(x) = \\mathbb{E}_{\\mathcal{D}}[\\widehat{f}_k(x)]$.\n$$ \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 \\right] = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\bar{f}_k(x) + \\bar{f}_k(x) - \\widehat{f}_{k}(x))^2 \\right] $$\nExpanding the square and noting that the cross-term $\\mathbb{E}_{\\mathcal{D}}[(f^{\\star}(x) - \\bar{f}_k(x))(\\bar{f}_k(x) - \\widehat{f}_{k}(x))]$ vanishes because $\\mathbb{E}_{\\mathcal{D}}[\\bar{f}_k(x) - \\widehat{f}_{k}(x)] = 0$, we get the standard decomposition:\n$$ \\mathcal{R}_{k,n} = \\underbrace{\\mathbb{E}_{x}\\left[(f^{\\star}(x) - \\bar{f}_k(x))^2\\right]}_{\\text{Integrated Squared Bias}} + \\underbrace{\\mathbb{E}_{\\mathcal{D},x}\\left[(\\widehat{f}_{k}(x) - \\bar{f}_k(x))^2\\right]}_{\\text{Integrated Variance}} + \\sigma^2 $$\nWe now derive each term. This requires finding the ordinary least squares (OLS) coefficients $\\widehat{\\beta}_j$. The OLS estimator minimizes $\\sum_{i=1}^n (y_i - \\sum_{j=0}^k \\beta_j \\phi_j(x_i))^2$. The normal equations are:\n$$ \\sum_{j=0}^{k} \\widehat{\\beta}_j \\left(\\frac{1}{n}\\sum_{i=1}^{n} \\phi_j(x_i) \\phi_{\\ell}(x_i)\\right) = \\frac{1}{n}\\sum_{i=1}^{n} y_i \\phi_{\\ell}(x_i), \\quad \\text{for } \\ell=0, \\dots, k $$\nUsing the given empirical orthonormality condition on the design points $\\{x_i\\}$, the left side simplifies to $\\widehat{\\beta}_{\\ell}$. Thus, for $j=0, \\dots, k$:\n$$ \\widehat{\\beta}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\phi_j(x_i) $$\nNext, we find the average predictor $\\bar{f}_k(x) = \\sum_{j=0}^k \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] \\phi_j(x)$. The expectation is over the training noise. Since $y_i = f^{\\star}(x_i) + \\varepsilon_i$ and $\\mathbb{E}[\\varepsilon_i|x_i]=0$, we have $\\mathbb{E}_{\\mathcal{D}}[y_i] = f^{\\star}(x_i)$.\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}_{\\mathcal{D}}[y_i] \\phi_j(x_i) = \\frac{1}{n} \\sum_{i=1}^{n} f^{\\star}(x_i) \\phi_j(x_i) $$\nSubstituting the expansion $f^{\\star}(x_i) = \\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\phi_{\\ell}(x_i)$:\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\phi_{\\ell}(x_i)\\right) \\phi_j(x_i) = \\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\left(\\frac{1}{n}\\sum_{i=1}^{n} \\phi_{\\ell}(x_i) \\phi_j(x_i)\\right) $$\nUsing the provided empirical orthonormality conditions for $j \\le k$, the term in parentheses is $\\mathbf{1}\\{j=\\ell\\}$ if $\\ell \\le k$ and $0$ if $\\ell > k$. Thus, for $j \\le k$:\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\theta_j $$\nThe average predictor is the projection of $f^{\\star}$ onto the chosen function space:\n$$ \\bar{f}_k(x) = \\sum_{j=0}^{k} \\theta_j \\phi_j(x) $$\nNow we can compute the bias term. The expectation $\\mathbb{E}_x$ corresponds to integration with respect to $\\frac{dx}{2}$ over $[-1,1]$.\n$$ \\text{Bias}^2 = \\mathbb{E}_{x}\\left[ \\left( f^{\\star}(x) - \\bar{f}_k(x) \\right)^2 \\right] = \\mathbb{E}_{x}\\left[ \\left( \\sum_{j=0}^{\\infty} \\theta_j \\phi_j(x) - \\sum_{j=0}^{k} \\theta_j \\phi_j(x) \\right)^2 \\right] $$\n$$ = \\mathbb{E}_{x}\\left[ \\left( \\sum_{j=k+1}^{\\infty} \\theta_j \\phi_j(x) \\right)^2 \\right] = \\sum_{j=k+1}^{\\infty} \\sum_{\\ell=k+1}^{\\infty} \\theta_j \\theta_{\\ell} \\mathbb{E}_{x}[\\phi_j(x)\\phi_{\\ell}(x)] $$\nUsing the population orthonormality, $\\mathbb{E}_{x}[\\phi_j(x)\\phi_{\\ell}(x)] = \\int_{-1}^1 \\phi_j(x)\\phi_{\\ell}(x)\\frac{dx}{2} = \\mathbf{1}\\{j=\\ell\\}$, the bias term is:\n$$ \\text{Integrated Squared Bias} = \\sum_{j=k+1}^{\\infty} \\theta_j^2 $$\nNext, we compute the variance term.\n$$ \\text{Variance} = \\mathbb{E}_{\\mathcal{D},x}\\left[ (\\widehat{f}_{k}(x) - \\bar{f}_k(x))^2 \\right] = \\mathbb{E}_{\\mathcal{D}} \\left[ \\mathbb{E}_{x} \\left[ \\left( \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)\\phi_j(x) \\right)^2 \\right] \\right] $$\nThe inner expectation over $x$ simplifies due to population orthonormality:\n$$ \\mathbb{E}_{x} \\left[ \\sum_{j=0}^{k} \\sum_{\\ell=0}^{k} (\\widehat{\\beta}_j - \\theta_j)(\\widehat{\\beta}_{\\ell} - \\theta_{\\ell}) \\phi_j(x)\\phi_{\\ell}(x) \\right] = \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)^2 $$\nThe integrated variance is then the expectation over the training data:\n$$ \\text{Integrated Variance} = \\mathbb{E}_{\\mathcal{D}}\\left[ \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)^2 \\right] = \\sum_{j=0}^{k} \\mathbb{E}_{\\mathcal{D}}\\left[ (\\widehat{\\beta}_j - \\theta_j)^2 \\right] = \\sum_{j=0}^{k} \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) $$\nsince $\\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\theta_j$. We compute the variance of the coefficients.\n$$ \\widehat{\\beta}_j - \\theta_j = \\left( \\frac{1}{n}\\sum_{i=1}^n y_i \\phi_j(x_i) \\right) - \\left( \\frac{1}{n}\\sum_{i=1}^n f^{\\star}(x_i) \\phi_j(x_i) \\right) = \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i \\phi_j(x_i) $$\nThe noises $\\{\\varepsilon_i\\}$ are independent with variance $\\sigma^2$, and the $\\phi_j(x_i)$ are deterministic constants.\n$$ \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) = \\operatorname{Var}\\left( \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i \\phi_j(x_i) \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}(\\varepsilon_i \\phi_j(x_i)) = \\frac{1}{n^2} \\sum_{i=1}^n (\\phi_j(x_i))^2 \\operatorname{Var}(\\varepsilon_i) $$\n$$ = \\frac{\\sigma^2}{n^2} \\sum_{i=1}^n (\\phi_j(x_i))^2 = \\frac{\\sigma^2}{n} \\left(\\frac{1}{n}\\sum_{i=1}^n \\phi_j(x_i)\\phi_j(x_i)\\right) $$\nUsing the empirical orthonormality for $j \\le k$, the term in parentheses is $1$.\n$$ \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) = \\frac{\\sigma^2}{n} $$\nThe integrated variance is the sum over $j=0, \\dots, k$:\n$$ \\text{Integrated Variance} = \\sum_{j=0}^{k} \\frac{\\sigma^2}{n} = \\frac{(k+1)\\sigma^2}{n} $$\nFinally, we assemble the three components to obtain the total expected error $\\mathcal{R}_{k,n}$.\n$$ \\mathcal{R}_{k,n} = \\text{Integrated Squared Bias} + \\text{Integrated Variance} + \\text{Irreducible Error} $$\n$$ \\mathcal{R}_{k,n} = \\sum_{j=k+1}^{\\infty} \\theta_j^2 + \\frac{(k+1)\\sigma^2}{n} + \\sigma^2 $$\nThis expression represents the complete bias-variance decomposition of the expected prediction error.",
            "answer": "$$\\boxed{\\sigma^2 + \\frac{\\sigma^2(k+1)}{n} + \\sum_{j=k+1}^{\\infty} \\theta_j^2}$$"
        },
        {
            "introduction": "After seeing the effects of model complexity, a natural question arises: how can we formally measure the 'capacity' or 'expressive power' of a model class? The Vapnik-Chervonenkis (VC) dimension provides a combinatorial answer to this question, which is independent of the data distribution. In this exercise , you will calculate the VC dimension for a simple yet illustrative class of threshold classifiers and see how this measure connects to the amount of data needed to guarantee generalization.",
            "id": "3122009",
            "problem": "Consider binary classification on the real line with the hypothesis class of thresholds. For each threshold $t \\in \\mathbb{R}$ define the classifier $h_t : \\mathbb{R} \\to \\{0,1\\}$ by $h_t(x) = \\mathbb{I}\\{x \\ge t\\}$, where $\\mathbb{I}\\{\\cdot\\}$ is the indicator function. Suppose data are drawn independently and identically distributed from an unknown distribution on $\\mathbb{R} \\times \\{0,1\\}$ that is realizable by this class: there exists an unknown $t_{\\star} \\in \\mathbb{R}$ such that $Y = \\mathbb{I}\\{X \\ge t_{\\star}\\}$ almost surely.\n\nStarting from the definitions of growth function and Vapnik–Chervonenkis (VC) dimension, and using only basic probabilistic tools and independence, do the following:\n\n1. Compute the growth function $\\Pi_{\\mathcal{H}}(n)$ of the threshold class on $\\mathbb{R}$, where $\\Pi_{\\mathcal{H}}(n)$ is defined as the maximum, over all sets of $n$ distinct points in $\\mathbb{R}$, of the number of distinct $\\{0,1\\}$-labelings that classifiers in $\\mathcal{H}$ can induce on those points.\n\n2. Determine the Vapnik–Chervonenkis dimension $d_{\\mathrm{VC}}$ of $\\mathcal{H}$.\n\n3. Derive a nonasymptotic realizable-case sample size condition that ensures the following generalization property for empirical risk minimization: with probability at least $1 - \\delta$, any classifier in $\\mathcal{H}$ that achieves zero empirical error on $m$ independently and identically distributed samples has true error at most $\\epsilon$. Express this condition explicitly in terms of $\\epsilon$, $\\delta$, and the growth function, and then specialize it using the $\\Pi_{\\mathcal{H}}(\\cdot)$ you computed.\n\nProvide your final answer as the two quantities $\\Pi_{\\mathcal{H}}(n)$ and $d_{\\mathrm{VC}}$, in that order, formatted as a single row. No numerical rounding is required, and no physical units are involved.",
            "solution": "The problem will be validated and, if valid, solved in three parts as requested: computation of the growth function, determination of the Vapnik-Chervonenkis (VC) dimension, and derivation of a sample size condition for the realizable case.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Hypothesis Class**: $\\mathcal{H} = \\{h_t : \\mathbb{R} \\to \\{0,1\\} \\mid h_t(x) = \\mathbb{I}\\{x \\ge t\\}, t \\in \\mathbb{R}\\}$. The classifiers are thresholds on the real line.\n- **Data Distribution**: Data points $(X, Y)$ are drawn independently and identically distributed (i.i.d.) from an unknown distribution on $\\mathbb{R} \\times \\{0,1\\}$.\n- **Realizability Assumption**: There exists a true threshold $t_{\\star} \\in \\mathbb{R}$ such that the labels are generated by the rule $Y = \\mathbb{I}\\{X \\ge t_{\\star}\\}$ almost surely.\n- **Task 1**: Compute the growth function $\\Pi_{\\mathcal{H}}(n) = \\max_{x_1, \\dots, x_n \\in \\mathbb{R}} |\\mathcal{H}|_{\\{x_1, \\dots, x_n\\}}|$, where $|\\mathcal{H}|_{\\{x_1, \\dots, x_n\\}}|$ is the number of distinct labelings on the set $\\{x_1, \\dots, x_n\\}$ induced by classifiers in $\\mathcal{H}$.\n- **Task 2**: Determine the VC dimension, $d_{\\mathrm{VC}} = \\max\\{n \\in \\mathbb{N} \\mid \\Pi_{\\mathcal{H}}(n) = 2^n\\}$.\n- **Task 3**: Derive a sample size condition on $m$ that guarantees, with probability at least $1 - \\delta$, any classifier $\\hat{h} \\in \\mathcal{H}$ with zero empirical error has true error at most $\\epsilon$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific or Factual Unsoundness**: The problem is a standard, fundamental exercise in statistical learning theory. The concepts of threshold classifiers, growth function, and VC dimension are mathematically and scientifically sound. No flaws are present.\n- **Non-Formalizable or Irrelevant**: The problem is formally stated and is directly relevant to the foundations of statistical learning. No flaws are present.\n- **Incomplete or Contradictory Setup**: The problem is self-contained. All necessary definitions (hypothesis class, growth function, VC dimension) and assumptions (realizability, i.i.d. data) are provided and are mutually consistent. No flaws are present.\n- **Unrealistic or Infeasible**: The setup is a simplified but canonical model used for pedagogical purposes in learning theory. It is not physically or scientifically implausible. No flaws are present.\n- **Ill-Posed or Poorly Structured**: The problem is well-posed. The quantities to be computed ($\\Pi_{\\mathcal{H}}(n)$ and $d_{\\mathrm{VC}}$) are uniquely determined by the problem definition. The derivation of the sample complexity bound is a standard procedure. No flaws are present.\n- **Pseudo-Profound, Trivial, or Tautological**: While a classic example, solving it requires a clear understanding and correct application of foundational definitions. It is not trivial or contrived. No flaws are present.\n- **Outside Scientific Verifiability**: The claims are mathematically derivable and verifiable. No flaws are present.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\n**Part 1: Computation of the Growth Function $\\Pi_{\\mathcal{H}}(n)$**\n\nThe growth function $\\Pi_{\\mathcal{H}}(n)$ is the maximum number of ways the hypothesis class $\\mathcal{H}$ can label a set of $n$ points. Let $S = \\{x_1, x_2, \\dots, x_n\\}$ be any set of $n$ distinct points in $\\mathbb{R}$. To analyze the dichotomies, we can order these points without loss of generality: $x_{(1)} < x_{(2)} < \\dots < x_{(n)}$.\n\nA classifier $h_t(x) = \\mathbb{I}\\{x \\ge t\\}$ assigns the label $1$ to points greater than or equal to the threshold $t$ and $0$ to points less than $t$. The vector of labels for the ordered points is $(h_t(x_{(1)}), h_t(x_{(2)}), \\dots, h_t(x_{(n)}))$. The specific labeling depends on the position of the threshold $t$ relative to the points $x_{(i)}$.\n\nThe $n$ distinct points partition the real line into $n+1$ open intervals. We analyze the labeling produced by placing $t$ in each of these regions.\n1.  If $t > x_{(n)}$, then for all $i \\in \\{1, \\dots, n\\}$, $x_{(i)} < t$. Thus, $h_t(x_{(i)}) = 0$ for all $i$. The labeling vector is $(0, 0, \\dots, 0)$.\n2.  If $x_{(n-1)} < t \\le x_{(n)}$, then $h_t(x_{(n)}) = 1$, and for all $i < n$, $x_{(i)} < t$, so $h_t(x_{(i)}) = 0$. The labeling vector is $(0, 0, \\dots, 0, 1)$.\n3.  If $x_{(n-2)} < t \\le x_{(n-1)}$, then $h_t(x_{(n-1)}) = 1$ and $h_t(x_{(n)}) = 1$. For all $i < n-1$, $x_{(i)} < t$, so $h_t(x_{(i)}) = 0$. The labeling vector is $(0, 0, \\dots, 1, 1)$.\n...\nk. In general, for $k \\in \\{1, \\dots, n\\}$, if we place the threshold such that $x_{(k-1)} < t \\le x_{(k)}$ (with the convention $x_{(0)} = -\\infty$), then $h_t(x_{(i)}) = 1$ for all $i \\ge k$, and $h_t(x_{(i)}) = 0$ for all $i < k$.\n...\nn+1. If $t \\le x_{(1)}$, then for all $i \\in \\{1, \\dots, n\\}$, $x_{(i)} \\ge t$. Thus, $h_t(x_{(i)}) = 1$ for all $i$. The labeling vector is $(1, 1, \\dots, 1)$.\n\nLet's list the distinct labelings generated for the ordered points $(x_{(1)}, \\dots, x_{(n)})$:\n- $(0, 0, \\dots, 0, 0)$\n- $(0, 0, \\dots, 0, 1)$\n- $(0, 0, \\dots, 1, 1)$\n- ...\n- $(0, 1, \\dots, 1, 1)$\n- $(1, 1, \\dots, 1, 1)$\n\nThere are $n$ labelings with at least one $0$ and one $1$, plus the all-zeros vector and the all-ones vector. This gives a total of $n+1$ distinct labelings. For instance, the labeling $(0, \\dots, 0, 1, \\dots, 1)$ with exactly $j$ ones is generated for any $t \\in (x_{(n-j)}, x_{(n-j+1)}]$. Since all dichotomies are of the form \"all points to the right of some point are labeled $1$\", we cannot, for example, generate the labeling $(1, 0, \\dots)$ on the ordered points.\n\nThe number of distinct dichotomies is $n+1$ regardless of the specific locations of the $n$ distinct points. Therefore, the maximum number over all sets of size $n$ is also $n+1$.\n$$ \\Pi_{\\mathcal{H}}(n) = n+1 $$\n\n**Part 2: Determination of the Vapnik-Chervonenkis Dimension $d_{\\mathrm{VC}}$**\n\nThe VC dimension, $d_{\\mathrm{VC}}$, is defined as the largest integer $n$ for which the hypothesis class $\\mathcal{H}$ can shatter a set of $n$ points, meaning it can generate all $2^n$ possible labelings for that set. This is equivalent to finding the largest $n$ such that $\\Pi_{\\mathcal{H}}(n) = 2^n$.\n\nUsing the growth function we just computed, we need to find the largest integer $n$ satisfying:\n$$ n+1 = 2^n $$\nLet's test small integer values for $n$:\n- For $n=1$: $1+1 = 2$ and $2^1 = 2$. The equality holds. Thus, $\\mathcal{H}$ can shatter $1$ point. Indeed, for any point $x_1$, the labeling $\\{1\\}$ is produced by $h_{x_1}$ and the labeling $\\{0\\}$ is produced by $h_{x_1+\\delta}$ for any $\\delta > 0$.\n- For $n=2$: $2+1 = 3$ and $2^2 = 4$. The equality does not hold ($3 < 4$). As shown in Part 1, for any two points $x_1 < x_2$, the labeling $(1,0)$ cannot be generated, because if $h_t(x_1)=1$, then $t \\le x_1$, which implies $t < x_2$, so $h_t(x_2)$ must also be $1$.\n- For $n > 2$: It can be shown by induction that for $n > 1$, $n+1 < 2^n$.\n\nThe largest integer $n$ for which $\\Pi_{\\mathcal{H}}(n) = 2^n$ is $n=1$.\n$$ d_{\\mathrm{VC}} = 1 $$\n\n**Part 3: Derivation of a Sample Size Condition**\n\nWe are in the realizable setting and seek a sample size $m$ such that with probability at least $1-\\delta$, any hypothesis $\\hat{h} \\in \\mathcal{H}$ with zero empirical risk ($R_S(\\hat{h}) = 0$) has true risk at most $\\epsilon$ ($R(\\hat{h}) \\le \\epsilon$). This is a standard Probably Approximately Correct (PAC) learning guarantee.\n\nThe condition can be stated as bounding the probability of the \"bad\" event, where there exists a hypothesis that is consistent with the sample but has a high true error:\n$$ P(\\exists h \\in \\mathcal{H} \\text{ such that } R_S(h)=0 \\text{ and } R(h) > \\epsilon) \\le \\delta $$\nLet $H_{bad} = \\{h \\in \\mathcal{H} \\mid R(h) > \\epsilon\\}$. The event of interest is that our sample $S$ of size $m$ is perfectly labeled by at least one hypothesis in $H_{bad}$.\n\nA standard method to bound this probability involves applying a union bound. A naive union bound over the (potentially infinite) set $H_{bad}$ is not viable. Instead, the bound is applied over the finite number of distinct ways the hypothesis class $\\mathcal{H}$ can label the specific sample points $x_1, \\dots, x_m$. The number of such labelings is at most $\\Pi_{\\mathcal{H}}(m)$. For any fixed 'bad' hypothesis $h \\in H_{bad}$, its probability of being consistent with a random sample of size $m$ is $(1-R(h))^m < (1-\\epsilon)^m$. A simplified argument, which combines these ideas, yields the following bound on the probability of the bad event:\n$$ P(\\text{bad event}) \\le \\Pi_{\\mathcal{H}}(m)(1-\\epsilon)^m $$\nWhile a fully rigorous proof requires more advanced techniques like symmetrization, this bound is a common starting point and captures the essential trade-off. We set this upper bound to be less than or equal to $\\delta$:\n$$ \\Pi_{\\mathcal{H}}(m)(1-\\epsilon)^m \\le \\delta $$\nTo find a condition on $m$, we can take the natural logarithm of both sides:\n$$ \\ln(\\Pi_{\\mathcal{H}}(m)) + m \\ln(1-\\epsilon) \\le \\ln(\\delta) $$\nUsing the well-known inequality $\\ln(1-x) \\le -x$ for $x \\in [0,1)$, we have $\\ln(1-\\epsilon) \\le -\\epsilon$. Substituting this provides a simpler (though slightly looser) condition:\n$$ \\ln(\\Pi_{\\mathcal{H}}(m)) - m\\epsilon \\le \\ln(\\delta) $$\nRearranging this inequality to express a condition on $m$ gives:\n$$ m\\epsilon \\ge \\ln(\\Pi_{\\mathcal{H}}(m)) + \\ln\\left(\\frac{1}{\\delta}\\right) $$\n$$ m \\ge \\frac{1}{\\epsilon} \\left( \\ln(\\Pi_{\\mathcal{H}}(m)) + \\ln\\left(\\frac{1}{\\delta}\\right) \\right) $$\nThis is a general sample size condition expressed in terms of the growth function $\\Pi_{\\mathcal{H}}(\\cdot)$, $\\epsilon$, and $\\delta$.\n\nNow, we specialize this result using the growth function for the threshold class, $\\Pi_{\\mathcal{H}}(m) = m+1$:\n$$ m \\ge \\frac{1}{\\epsilon} \\left( \\ln(m+1) + \\ln\\left(\\frac{1}{\\delta}\\right) \\right) $$\nThis implicit inequality is the required sample size condition. It shows that for fixed $\\epsilon$ and $\\delta$, the required sample size $m$ grows logarithmically with itself, leading to an overall sample complexity that is roughly proportional to $\\frac{1}{\\epsilon}\\ln(\\frac{1}{\\epsilon\\delta})$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} n+1 & 1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While the VC dimension provides a powerful, distribution-free measure of complexity, other tools like Rademacher complexity can offer a more nuanced, data-dependent view. This practice  bridges theory and practice by using Rademacher complexity to analyze one of the most common techniques for controlling model capacity: regularization. By comparing the generalization bounds for $\\ell_1$ (Lasso) and $\\ell_2$ (Ridge) regularized models, you will uncover the theoretical reasons why $\\ell_1$ regularization is so effective for learning sparse solutions in high-dimensional settings.",
            "id": "3121905",
            "problem": "Consider a binary classification problem in $\\mathbb{R}^{d}$ with linear predictors $f_{\\mathbf{w}}(\\mathbf{x}) = \\mathbf{w}^{\\top} \\mathbf{x}$, learned by Empirical Risk Minimization (ERM) under norm constraints. Let the loss function $\\ell(y, f_{\\mathbf{w}}(\\mathbf{x}))$ be $1$-Lipschitz in its second argument and bounded in $[0,1]$. Suppose the input $\\mathbf{x}$ satisfies, almost surely, both $\\|\\mathbf{x}\\|_{\\infty} \\leq r$ and $\\|\\mathbf{x}\\|_{2} \\leq R$, where $r > 0$, $R > 0$, and $d \\geq 2$.\n\nDefine the two hypothesis classes:\n$$\n\\mathcal{F}_{1} = \\left\\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{1} \\leq B_{1} \\right\\}\n\\quad\\text{and}\\quad\n\\mathcal{F}_{2} = \\left\\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{2} \\leq B_{2} \\right\\},\n$$\nwith $B_{1}, B_{2} > 0$. Assume the unknown target parameter $\\mathbf{w}^{\\star}$ is $s$-sparse, meaning it has at most $s$ nonzero coordinates, and satisfies $\\|\\mathbf{w}^{\\star}\\|_{2} \\leq B_{2}$. Under this sparsity, use the inequality $\\|\\mathbf{w}^{\\star}\\|_{1} \\leq \\sqrt{s}\\,\\|\\mathbf{w}^{\\star}\\|_{2}$ to set $B_{1} = \\sqrt{s}\\,B_{2}$ so that both $\\mathcal{F}_{1}$ and $\\mathcal{F}_{2}$ contain $\\mathbf{w}^{\\star}$.\n\nLet $S = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ be a sample of size $n \\geq 2$. Starting from the definition of empirical Rademacher complexity,\n$$\n\\mathfrak{R}_{n}(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}, S} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} f(\\mathbf{x}_{i}) \\right],\n$$\nwhere $\\sigma_{1},\\dots,\\sigma_{n}$ are independent Rademacher random variables taking values in $\\{-1,+1\\}$, derive upper bounds on $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ and $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$ in terms of $B_{1}$, $B_{2}$, $r$, $R$, $d$, and $n$, by appealing only to dual norm relationships and basic concentration for sums of Rademacher random variables. Then, using a standard generalization argument that smaller empirical Rademacher complexity yields a smaller ERM generalization gap for $1$-Lipschitz bounded losses, compute the largest sparsity level $s_{\\star}$ (as a closed-form symbolic expression depending on $d$, $r$, and $R$) such that the upper bound on $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ is less than or equal to the upper bound on $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$. Provide your final answer as the expression $s_{\\star} = \\cdots$.",
            "solution": "The problem requires the derivation of upper bounds on the empirical Rademacher complexities for two hypothesis classes, $\\mathcal{F}_{1}$ and $\\mathcal{F}_{2}$, and then finding the maximum sparsity level $s_{\\star}$ for which the bound for $\\mathcal{F}_{1}$ is tighter than or equal to the bound for $\\mathcal{F}_{2}$.\n\nLet $S = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ be a fixed sample. The empirical Rademacher complexity is defined as:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} f(\\mathbf{x}_{i}) \\right]\n$$\nThe total Rademacher complexity is $\\mathfrak{R}_{n}(\\mathcal{F}) = \\mathbb{E}_{S}[\\hat{\\mathfrak{R}}_{S}(\\mathcal{F})]$. We will derive upper bounds on $\\hat{\\mathfrak{R}}_{S}$ that are independent of the specific sample $S$, which will then directly serve as bounds on $\\mathfrak{R}_{n}(\\mathcal{F})$.\n\n**Step 1: Derivation of the upper bound for $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$**\n\nThe hypothesis class $\\mathcal{F}_{1}$ is defined as $\\mathcal{F}_{1} = \\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{1} \\leq B_{1} \\}$.\nThe empirical Rademacher complexity for $\\mathcal{F}_{1}$ is:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{1} \\leq B_{1}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{w}^{\\top}\\mathbf{x}_{i} \\right]\n$$\nBy linearity of the inner product, we can rearrange the summation:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{1} \\leq B_{1}} \\mathbf{w}^{\\top} \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) \\right]\n$$\nThe expression inside the expectation is of the form $\\sup_{\\|\\mathbf{w}\\|_{p} \\leq B} \\mathbf{w}^{\\top}\\mathbf{v}$. By the definition of a dual norm, this supremum is equal to $B \\|\\mathbf{v}\\|_{q}$, where $\\|\\cdot\\|_{q}$ is the dual norm to $\\|\\cdot\\|_{p}$. The dual norm of the $\\ell_1$-norm is the $\\ell_{\\infty}$-norm. Thus, we have:\n$$\n\\sup_{\\|\\mathbf{w}\\|_{1} \\leq B_{1}} \\mathbf{w}^{\\top} \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) = B_{1} \\left\\| \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{\\infty}\n$$\nSubstituting this back into the expression for $\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1})$:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) = \\frac{B_{1}}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{\\infty} \\right]\n$$\nTo bound the expectation, we use a standard maximal inequality for sums of independent random vectors. For a set of vectors $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}$ in $\\mathbb{R}^d$, a bound derived from Hoeffding's inequality and a union bound is:\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{v}_{i} \\right\\|_{\\infty} \\right] \\leq \\sqrt{2 \\ln(2d)} \\max_{j \\in \\{1,\\dots,d\\}} \\sqrt{\\sum_{i=1}^{n} v_{ij}^{2}}\n$$\nIn our case, $\\mathbf{v}_i = \\mathbf{x}_i$. We are given that $\\|\\mathbf{x}_{i}\\|_{\\infty} \\leq r$ for all $i$, which implies that $|x_{ij}| \\leq r$ for all $i,j$. We can bound the sum of squares:\n$$\n\\sum_{i=1}^{n} x_{ij}^{2} \\leq \\sum_{i=1}^{n} r^{2} = n r^{2}\n$$\nThis holds for any coordinate $j$. Therefore, $\\max_{j} \\sqrt{\\sum_{i=1}^{n} x_{ij}^{2}} \\leq \\sqrt{n r^{2}} = r\\sqrt{n}$.\nPlugging this into the maximal inequality:\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{\\infty} \\right] \\leq r \\sqrt{n} \\sqrt{2 \\ln(2d)}\n$$\nFinally, we substitute this back into the expression for $\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1})$:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) \\leq \\frac{B_{1}}{n} \\left( r \\sqrt{n} \\sqrt{2 \\ln(2d)} \\right) = \\frac{B_{1} r \\sqrt{2 \\ln(2d)}}{\\sqrt{n}}\n$$\nSince this upper bound is independent of the sample $S$, it is also an upper bound for the full Rademacher complexity $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$.\n\n**Step 2: Derivation of the upper bound for $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$**\n\nThe hypothesis class $\\mathcal{F}_{2}$ is defined as $\\mathcal{F}_{2} = \\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{2} \\leq B_{2} \\}$.\nThe empirical Rademacher complexity for $\\mathcal{F}_{2}$ is:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{2} \\leq B_{2}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{w}^{\\top}\\mathbf{x}_{i} \\right] = \\frac{1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{2} \\leq B_{2}} \\mathbf{w}^{\\top} \\left( \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) \\right]\n$$\nThe $\\ell_2$-norm is self-dual. Therefore:\n$$\n\\sup_{\\|\\mathbf{w}\\|_{2} \\leq B_{2}} \\mathbf{w}^{\\top} \\left( \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) = B_{2} \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}\n$$\nSubstituting this back, we get:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2}) = \\frac{B_{2}}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2} \\right]\n$$\nWe can bound the expectation using Jensen's inequality, since the square root function is concave: $\\mathbb{E}[\\sqrt{X}] \\leq \\sqrt{\\mathbb{E}[X]}$.\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2} \\right] = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sqrt{\\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}^{2}} \\right] \\leq \\sqrt{\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}^{2} \\right]}\n$$\nLet's compute the argument of the square root:\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}^{2} \\right] = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left(\\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i}\\right)^{\\top} \\left(\\sum_{j=1}^{n} \\sigma_{j} \\mathbf{x}_{j}\\right) \\right] = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\sigma_{i}\\sigma_{j} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{j}) \\right]\n$$\nBy linearity of expectation, we move the expectation inside the sums. Since $\\sigma_i$ are independent Rademacher variables, $\\mathbb{E}[\\sigma_i]=0$ and $\\mathbb{E}[\\sigma_i^2]=1$. Thus, $\\mathbb{E}[\\sigma_i \\sigma_j] = \\delta_{ij}$ (the Kronecker delta).\n$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{j}) \\mathbb{E}_{\\boldsymbol{\\sigma}}[\\sigma_{i}\\sigma_{j}] = \\sum_{i=1}^{n} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i}) \\mathbb{E}[\\sigma_{i}^{2}] = \\sum_{i=1}^{n} \\|\\mathbf{x}_{i}\\|_{2}^{2}\n$$\nWe are given that $\\|\\mathbf{x}_{i}\\|_{2} \\leq R$ for all $i$. So, $\\sum_{i=1}^{n} \\|\\mathbf{x}_{i}\\|_{2}^{2} \\leq \\sum_{i=1}^{n} R^{2} = nR^{2}$.\nPutting this together:\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2} \\right] \\leq \\sqrt{\\sum_{i=1}^{n} \\|\\mathbf{x}_{i}\\|_{2}^{2}} \\leq \\sqrt{nR^{2}} = R\\sqrt{n}\n$$\nSubstituting this into the expression for $\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2})$:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2}) \\leq \\frac{B_{2}}{n} (R\\sqrt{n}) = \\frac{B_{2}R}{\\sqrt{n}}\n$$\nThis upper bound is also independent of $S$, so it is an upper bound for $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$.\n\n**Step 3: Comparison of the bounds**\n\nWe want to find the largest sparsity level $s_{\\star}$ such that the upper bound for $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ is less than or equal to the upper bound for $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$:\n$$\n\\frac{B_{1} r \\sqrt{2 \\ln(2d)}}{\\sqrt{n}} \\leq \\frac{B_{2}R}{\\sqrt{n}}\n$$\nSince $n \\geq 2$, we can multiply by $\\sqrt{n}$:\n$$\nB_{1} r \\sqrt{2 \\ln(2d)} \\leq B_{2}R\n$$\nThe problem specifies that we should set $B_{1} = \\sqrt{s}B_{2}$ to ensure the $s$-sparse vector $\\mathbf{w}^{\\star}$ is in $\\mathcal{F}_{1}$. Substituting this into the inequality:\n$$\n(\\sqrt{s}B_{2}) r \\sqrt{2 \\ln(2d)} \\leq B_{2}R\n$$\nSince $B_2 > 0$, we can divide by $B_{2}$:\n$$\n\\sqrt{s} \\, r \\sqrt{2 \\ln(2d)} \\leq R\n$$\nNow, we solve for $s$. Since $r, R > 0$ and $d \\ge 2$, all terms are positive.\n$$\n\\sqrt{s} \\leq \\frac{R}{r \\sqrt{2 \\ln(2d)}}\n$$\nSquaring both sides gives the condition on $s$:\n$$\ns \\leq \\frac{R^2}{r^2 (2 \\ln(2d))} = \\frac{R^2}{2r^2 \\ln(2d)}\n$$\nThe largest value of $s$ that satisfies this inequality is the right-hand side. This is the desired sparsity level $s_{\\star}$.",
            "answer": "$$\\boxed{s_{\\star} = \\frac{R^2}{2 r^2 \\ln(2d)}}$$"
        }
    ]
}