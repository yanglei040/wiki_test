{
    "hands_on_practices": [
        {
            "introduction": "This first exercise explores the most fundamental difference between regression and classification: the nature of their irreducible error. By constructing a scenario where classification is perfect yet regression is inherently noisy, you will derive the Bayes-optimal risk for each task. This practice will solidify your understanding of why a seemingly \"simple\" classification problem does not necessarily imply a simple regression problem, even with the exact same input data .",
            "id": "3169383",
            "problem": "Consider a supervised learning setting with a single real-valued feature $X$ drawn from a continuous distribution and two learning tasks defined on the same input $X$ but with different targets: a binary classification task and a real-valued regression task. Let the data-generating mechanism be:\n- $X \\sim \\mathrm{Uniform}(-1,1)$,\n- $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ independent of $X$,\n- classification label $C \\in \\{0,1\\}$ defined by $C = \\mathbb{I}\\{X \\geq 0\\}$,\n- regression target $Y \\in \\mathbb{R}$ defined by $Y = X + \\epsilon$.\n\nYou are asked to start from the foundational definitions of expected risk for zero-one loss in classification and squared error loss in regression, together with the definitions of conditional probability and conditional expectation. Using only these bases, derive the Bayes-optimal risk for each task under the above generative model. In particular:\n\n1. For the classification task with zero-one loss, derive the minimal achievable expected misclassification rate across all measurable classifiers.\n2. For the regression task with squared error loss, derive the minimal achievable expected mean squared error (MSE) across all measurable predictors.\n\nExplain why this construction provides a counterexample in which perfect classification is possible (the classes are separable), yet the regression MSE is bounded below by a positive constant due to aleatoric noise. Express your final answer as a row matrix whose first entry is the minimal classification risk and whose second entry is the minimal regression risk, in terms of $\\sigma^{2}$. No rounding is required.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is a standard problem in statistical learning theory used to illustrate the concepts of Bayes-optimal predictors and irreducible error (aleatoric uncertainty). All provided definitions and distributions are standard. There are no contradictions or ambiguities. The problem is valid and can be solved.\n\nWe will address the two tasks sequentially, starting from the foundational definitions of risk and the Bayes-optimal predictor for each case.\n\n**1. Minimal Classification Risk (Zero-One Loss)**\n\nThe task is to classify an observation with feature $X$ into one of two classes, $C \\in \\{0, 1\\}$. The loss function is the zero-one loss, defined as $L(C, \\hat{C}) = \\mathbb{I}\\{C \\neq \\hat{C}\\}$, where $\\hat{C}$ is the predicted class from a classifier $\\hat{c}(X)$. The risk of a classifier $\\hat{c}$ is the expected loss:\n$$\nR(\\hat{c}) = E[L(C, \\hat{c}(X))] = E[\\mathbb{I}\\{C \\neq \\hat{c}(X)\\}] = P(C \\neq \\hat{c}(X))\n$$\nThe goal is to find the classifier $\\hat{c}^*(X)$ that minimizes this risk. This classifier is known as the Bayes classifier. The risk of the Bayes classifier is the Bayes risk.\n\nBy the law of total expectation, the risk can be written as:\n$$\nR(\\hat{c}) = E_X[E_{C|X}[ \\mathbb{I}\\{C \\neq \\hat{c}(X)\\} | X=x ]]\n$$\nTo minimize the overall risk, we must minimize the inner conditional expectation for each value of $x$. The Bayes decision rule for a given $x$ is:\n$$\n\\hat{c}^*(x) = \\arg\\min_{k \\in \\{0,1\\}} E[ \\mathbb{I}\\{C \\neq k\\} | X=x ] = \\arg\\min_{k \\in \\{0,1\\}} P(C \\neq k | X=x)\n$$\nMinimizing the probability of being wrong is equivalent to maximizing the probability of being right. Thus, the Bayes classifier chooses the class with the highest posterior probability:\n$$\n\\hat{c}^*(x) = \\arg\\max_{k \\in \\{0,1\\}} P(C = k | X=x)\n$$\nLet us define the conditional probability function $\\eta(x) = P(C=1|X=x)$. Then $P(C=0|X=x) = 1 - \\eta(x)$. The rule becomes: choose class $1$ if $\\eta(x)  1/2$, and class $0$ otherwise.\n\nIn this problem, the class label $C$ is a deterministic function of the feature $X$: $C = \\mathbb{I}\\{X \\geq 0\\}$. This means that given a value $x$ for the random variable $X$, the value of $C$ is known with certainty. There is no randomness in $C$ conditional on $X$.\nSpecifically:\n- If $x \\geq 0$, then $C = 1$ with probability $1$. Thus, $\\eta(x) = P(C=1|X=x) = 1$.\n- If $x  0$, then $C = 0$ with probability $1$. Thus, $\\eta(x) = P(C=1|X=x) = 0$.\n\nApplying the Bayes decision rule:\n- For $x \\geq 0$, $\\eta(x)=1  1/2$, so the Bayes classifier predicts $\\hat{c}^*(x) = 1$.\n- For $x  0$, $\\eta(x)=0  1/2$, so the Bayes classifier predicts $\\hat{c}^*(x) = 0$.\n\nThis can be summarized as $\\hat{c}^*(X) = \\mathbb{I}\\{X \\geq 0\\}$. We observe that the Bayes classifier is identical to the true data-generating function for the class label: $\\hat{c}^*(X) = C$.\n\nThe minimal achievable risk (the Bayes risk) is the risk of this optimal classifier:\n$$\nR^* = R(\\hat{c}^*) = E[\\mathbb{I}\\{C \\neq \\hat{c}^*(X)\\}]\n$$\nSince $\\hat{c}^*(X) = C$ for all possible outcomes of $X$, the event $\\{C \\neq \\hat{c}^*(X)\\}$ is an impossible event. Therefore, the indicator function is always $0$.\n$$\nR^* = E[0] = 0\n$$\nThe minimal achievable expected misclassification rate is $0$. This signifies that the classes are perfectly separable, and the optimal decision boundary can be learned without error.\n\n**2. Minimal Regression Risk (Squared Error Loss)**\n\nThe task is to predict a real-valued target $Y$ from the feature $X$. The loss function is the squared error loss, defined as $L(Y, \\hat{Y}) = (Y - \\hat{Y})^2$, where $\\hat{Y}$ is the predicted value from a regression function $f(X)$. The risk of a regression function $f$ is the expected loss, or Mean Squared Error (MSE):\n$$\nR(f) = E[L(Y, f(X))] = E[(Y - f(X))^2]\n$$\nThe goal is to find the function $f^*(X)$ that minimizes this risk. This is the Bayes predictor for regression.\n\nUsing the law of total expectation, we decompose the risk:\n$$\nR(f) = E_X[E_{Y|X}[(Y - f(X))^2|X=x]]\n$$\nTo minimize the total risk, we must select $f(x)$ to minimize the inner conditional expectation for each $x$. For a fixed $x$, $f(x)$ is a constant, let's call it $g$. We need to find $g$ that minimizes $E[(Y-g)^2|X=x]$. This is a classic result from probability theory: the value of $g$ that minimizes the expected squared difference to a random variable $Y$ is the expectation of $Y$. Therefore, the optimal predictor is the conditional expectation of the target given the feature:\n$$\nf^*(x) = E[Y|X=x]\n$$\nLet's compute this for the given data-generating process, $Y = X + \\epsilon$, where $X \\sim \\mathrm{Uniform}(-1,1)$ and $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ are independent.\n$$\nf^*(x) = E[X + \\epsilon | X=x]\n$$\nUsing the linearity of conditional expectation:\n$$\nf^*(x) = E[X | X=x] + E[\\epsilon | X=x]\n$$\nFor the first term, $E[X | X=x] = x$. For the second term, since $\\epsilon$ and $X$ are independent, conditioning on $X$ does not provide any information about $\\epsilon$. Thus, $E[\\epsilon | X=x] = E[\\epsilon]$. We are given that $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$, so its mean is $E[\\epsilon] = 0$.\nSubstituting these back, we get the Bayes predictor:\n$$\nf^*(x) = x + 0 = x\n$$\nSo, the optimal regression function is $f^*(X) = X$.\n\nNow, we compute the minimal achievable risk (the Bayes risk) by evaluating the MSE for this optimal predictor:\n$$\nR^* = R(f^*) = E[(Y - f^*(X))^2] = E[(Y - X)^2]\n$$\nSubstitute the definition of $Y = X + \\epsilon$:\n$$\nR^* = E[((X + \\epsilon) - X)^2] = E[\\epsilon^2]\n$$\nThe value $E[\\epsilon^2]$ can be found from the definition of variance: $\\mathrm{Var}(\\epsilon) = E[\\epsilon^2] - (E[\\epsilon])^2$.\nWe are given $\\mathrm{Var}(\\epsilon) = \\sigma^2$ and $E[\\epsilon] = 0$.\n$$\n\\sigma^2 = E[\\epsilon^2] - 0^2 \\implies E[\\epsilon^2] = \\sigma^2\n$$\nThus, the minimal achievable MSE is:\n$$\nR^* = \\sigma^2\n$$\n\n**Explanation of the Counterexample**\n\nThis construction serves as a clear counterexample to any naive assumption that \"easy\" classification problems imply \"easy\" regression problems on the same features.\n- In the classification task, the target $C = \\mathbb{I}\\{X \\geq 0\\}$ is a deterministic, noise-free function of the input feature $X$. The relationship is exact. Therefore, an ideal learner can find the decision boundary $X = 0$ and achieve perfect classification, resulting in a Bayes risk of $0$.\n- In the regression task, the target $Y = X + \\epsilon$ has two components: a deterministic part ($X$) and a stochastic part ($\\epsilon$). The Bayes predictor $f^*(X)=X$ can perfectly learn the deterministic relationship. However, the noise term $\\epsilon$ is random and, crucially, independent of $X$. No function of $X$ can possibly predict the value of $\\epsilon$. This unpredictable component gives rise to an irreducible error, often called aleatoric uncertainty. The Bayes risk, $R^* = \\sigma^2$, is precisely the variance of this noise. Unless $\\sigma^2=0$, the minimal MSE is strictly positive, meaning perfect regression is impossible.\n\nThe minimal classification risk is $0$, and the minimal regression risk is $\\sigma^2$. The final answer is the row matrix containing these two values.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  \\sigma^{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Building on the concept of risk, this practice delves into the subtle but crucial relationship between probabilistic classification and regression. You will analyze a setting with \"soft\" labels, where the goal is to predict a probability, and compare the consequences of using squared error loss versus cross-entropy loss. This exercise demonstrates that while both approaches may seek the same optimal predictor, the choice of loss function fundamentally changes how we measure the unavoidable, irreducible error associated with the task .",
            "id": "3169373",
            "problem": "Consider a binary response variable $Y \\in \\{0,1\\}$ and a feature vector $X$ taking values in a measurable space. Let the conditional distribution of $Y$ given $X=x$ be specified by the soft label $\\mathbb{P}(Y=1 \\mid X=x)=p(x)$, so that $\\mathbb{P}(Y=0 \\mid X=x)=1-p(x)$. Assume the induced random variable $p(X)$ is uniformly distributed on $[0,1]$. Two learning paradigms are considered, each producing a pointwise predictor $s(x)\\in(0,1)$: (i) regression with squared error loss $\\ell_{\\mathrm{SE}}(y,s)=(y-s)^{2}$, trained on the numeric encoding $y\\in\\{0,1\\}$; and (ii) probabilistic classification with binary cross-entropy loss $\\ell_{\\mathrm{CE}}(y,s)=-y\\ln(s)-(1-y)\\ln(1-s)$.\n\nUsing the foundational definition that the Bayes decision rule minimizes the conditional expected loss, and that the Bayes risk is the expectation, over $X$, of this minimized conditional loss, derive from first principles the Bayes risk $R_{\\mathrm{SE}}$ under squared error and $R_{\\mathrm{CE}}$ under cross-entropy, and then compute the exact value of the difference $R_{\\mathrm{CE}}-R_{\\mathrm{SE}}$ under the stated uniform distribution of $p(X)$ on $[0,1]$. Express your final answer exactly as a simplified fraction.",
            "solution": "The problem requires the derivation and calculation of the Bayes risks for two different loss functions, squared error and cross-entropy, under a specific distributional assumption, and then to compute the difference between these two risks.\n\nFirst, we establish the general framework for finding the Bayes decision rule and Bayes risk. Let $Y$ be the response variable, $X$ be the feature vector, and $s(x)$ be a predictor for the outcome of $Y$ given $X=x$. The conditional expected loss at $X=x$ for a given loss function $\\ell(y, s)$ is defined as:\n$$L(s(x) \\mid X=x) = \\mathbb{E}_{Y \\mid X=x}[\\ell(Y, s(x))]$$\nThe problem states that $Y$ is a binary random variable, $Y \\in \\{0,1\\}$, with conditional probability mass function given by $\\mathbb{P}(Y=1 \\mid X=x) = p(x)$ and $\\mathbb{P}(Y=0 \\mid X=x) = 1-p(x)$. Thus, the conditional expectation can be written as:\n$$L(s(x) \\mid x) = \\ell(1, s(x)) \\cdot p(x) + \\ell(0, s(x)) \\cdot (1-p(x))$$\nThe Bayes decision rule (or Bayes predictor) $s^*(x)$ is the function that minimizes this conditional expected loss for each $x$:\n$$s^*(x) = \\arg\\min_{s \\in (0,1)} L(s \\mid x)$$\nThe minimized value of this conditional loss is the conditional Bayes risk, $R(x) = L(s^*(x) \\mid x)$. The overall Bayes risk $R$ is the expectation of the conditional Bayes risk over the distribution of $X$:\n$$R = \\mathbb{E}_X[R(X)] = \\mathbb{E}_X[L(s^*(X) \\mid X)]$$\n\nWe will now apply this framework to the two specified loss functions.\n\n**Part 1: Squared Error Loss**\n\nThe squared error loss is given by $\\ell_{\\mathrm{SE}}(y,s) = (y-s)^2$. The conditional expected loss is:\n$$L_{\\mathrm{SE}}(s \\mid x) = (1-s)^2 \\cdot p(x) + (0-s)^2 \\cdot (1-p(x)) = (1-2s+s^2)p(x) + s^2(1-p(x))$$\n$$L_{\\mathrm{SE}}(s \\mid x) = p(x) - 2sp(x) + s^2p(x) + s^2 - s^2p(x) = s^2 - 2sp(x) + p(x)$$\nThis is a quadratic function of $s$, convex in $s$. To find the minimum, we take the derivative with respect to $s$ and set it to $0$:\n$$\\frac{\\partial L_{\\mathrm{SE}}(s \\mid x)}{\\partial s} = 2s - 2p(x) = 0$$\nThis yields the Bayes predictor for squared error loss:\n$$s_{\\mathrm{SE}}^*(x) = p(x)$$\nThe conditional Bayes risk for squared error, $R_{\\mathrm{SE}}(x)$, is obtained by substituting $s_{\\mathrm{SE}}^*(x)$ back into the conditional expected loss function:\n$$R_{\\mathrm{SE}}(x) = L_{\\mathrm{SE}}(p(x) \\mid x) = \\mathbb{E}_{Y \\mid X=x}[(Y - p(x))^2]$$\nThis is the definition of the variance of the Bernoulli random variable $Y \\mid X=x$, which has parameter $p(x)$. The variance of a Bernoulli($p$) random variable is $p(1-p)$. Therefore:\n$$R_{\\mathrm{SE}}(x) = p(x)(1-p(x))$$\n\n**Part 2: Cross-Entropy Loss**\n\nThe binary cross-entropy loss is given by $\\ell_{\\mathrm{CE}}(y,s) = -y\\ln(s) - (1-y)\\ln(1-s)$. The conditional expected loss is:\n$$L_{\\mathrm{CE}}(s \\mid x) = [-\\ln(s)] \\cdot p(x) + [-\\ln(1-s)] \\cdot (1-p(x))$$\n$$L_{\\mathrm{CE}}(s \\mid x) = -p(x)\\ln(s) - (1-p(x))\\ln(1-s)$$\nThis function is also convex in $s$. To find the minimum, we take the derivative with respect to $s$ and set it to $0$:\n$$\\frac{\\partial L_{\\mathrm{CE}}(s \\mid x)}{\\partial s} = -\\frac{p(x)}{s} - (1-p(x))\\left(\\frac{-1}{1-s}\\right) = -\\frac{p(x)}{s} + \\frac{1-p(x)}{1-s} = 0$$\n$$\\frac{1-p(x)}{1-s} = \\frac{p(x)}{s} \\implies s(1-p(x)) = p(x)(1-s) \\implies s - sp(x) = p(x) - sp(x)$$\nThis yields the Bayes predictor for cross-entropy loss:\n$$s_{\\mathrm{CE}}^*(x) = p(x)$$\nThe conditional Bayes risk for cross-entropy, $R_{\\mathrm{CE}}(x)$, is obtained by substituting $s_{\\mathrm{CE}}^*(x)$ back into the conditional expected loss function:\n$$R_{\\mathrm{CE}}(x) = L_{\\mathrm{CE}}(p(x) \\mid x) = -p(x)\\ln(p(x)) - (1-p(x))\\ln(1-p(x))$$\nThis is the Shannon entropy of a Bernoulli random variable with parameter $p(x)$.\n\n**Part 3: Calculation of Overall Bayes Risks**\n\nThe problem states that the induced random variable $P = p(X)$ follows a uniform distribution on the interval $[0,1]$. Its probability density function is $f_P(p)=1$ for $p \\in [0,1]$ and $0$ otherwise. The overall Bayes risks are the expectations of the conditional Bayes risks with respect to this distribution.\n\nFor squared error:\n$$R_{\\mathrm{SE}} = \\mathbb{E}_P[P(1-P)] = \\int_0^1 p(1-p) \\, dp = \\int_0^1 (p-p^2) \\, dp$$\n$$R_{\\mathrm{SE}} = \\left[ \\frac{p^2}{2} - \\frac{p^3}{3} \\right]_0^1 = \\left(\\frac{1^2}{2} - \\frac{1^3}{3}\\right) - (0) = \\frac{1}{2} - \\frac{1}{3} = \\frac{3-2}{6} = \\frac{1}{6}$$\n\nFor cross-entropy:\n$$R_{\\mathrm{CE}} = \\mathbb{E}_P[-P\\ln(P) - (1-P)\\ln(1-P)] = \\int_0^1 [-p\\ln(p) - (1-p)\\ln(1-p)] \\, dp$$\n$$R_{\\mathrm{CE}} = -\\int_0^1 p\\ln(p) \\, dp - \\int_0^1 (1-p)\\ln(1-p) \\, dp$$\nLet's evaluate the integral $\\int_0^1 p\\ln(p) \\, dp$ using integration by parts, with $u=\\ln(p)$ and $dv=p \\, dp$. Then $du = \\frac{1}{p}dp$ and $v = \\frac{p^2}{2}$.\n$$\\int p\\ln(p) \\, dp = \\frac{p^2}{2}\\ln(p) - \\int \\frac{p^2}{2} \\cdot \\frac{1}{p} \\, dp = \\frac{p^2}{2}\\ln(p) - \\int \\frac{p}{2} \\, dp = \\frac{p^2}{2}\\ln(p) - \\frac{p^2}{4}$$\nEvaluating the definite integral:\n$$\\int_0^1 p\\ln(p) \\, dp = \\left[ \\frac{p^2}{2}\\ln(p) - \\frac{p^2}{4} \\right]_0^1$$\nThe upper limit gives $\\frac{1^2}{2}\\ln(1) - \\frac{1^2}{4} = 0 - \\frac{1}{4} = -\\frac{1}{4}$. For the lower limit, we need $\\lim_{p \\to 0^+} \\left( \\frac{p^2}{2}\\ln(p) - \\frac{p^2}{4} \\right)$. The term $\\frac{p^2}{4}$ goes to $0$. The term $p^2\\ln(p)$ has an indeterminate form $0 \\cdot (-\\infty)$, which can be evaluated using L'Hôpital's rule:\n$$\\lim_{p \\to 0^+} p^2\\ln(p) = \\lim_{p \\to 0^+} \\frac{\\ln(p)}{p^{-2}} = \\lim_{p \\to 0^+} \\frac{1/p}{-2p^{-3}} = \\lim_{p \\to 0^+} \\frac{-p^2}{2} = 0$$\nSo the lower limit evaluates to $0$. The integral is $-\\frac{1}{4} - 0 = -\\frac{1}{4}$.\n\nThe second integral in the expression for $R_{\\mathrm{CE}}$ is $-\\int_0^1 (1-p)\\ln(1-p) \\, dp$. Let $q=1-p$, so $dq=-dp$. The limits of integration for $q$ are from $1$ to $0$.\n$$-\\int_1^0 q\\ln(q) (-dq) = \\int_1^0 q\\ln(q) \\, dq = -\\int_0^1 q\\ln(q) \\, dq$$\nThis is the negative of the same integral form we just computed. Thus, its value is $-(-\\frac{1}{4})=\\frac{1}{4}$.\nThe first integral in the expression for $R_{\\mathrm{CE}}$ is $-\\int_0^1 p\\ln(p) \\, dp = -(-\\frac{1}{4}) = \\frac{1}{4}$.\nTherefore, the total Bayes risk for cross-entropy is:\n$$R_{\\mathrm{CE}} = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$$\n\n**Part 4: Final Calculation**\n\nThe problem asks for the difference $R_{\\mathrm{CE}} - R_{\\mathrm{SE}}$:\n$$R_{\\mathrm{CE}} - R_{\\mathrm{SE}} = \\frac{1}{2} - \\frac{1}{6} = \\frac{3}{6} - \\frac{1}{6} = \\frac{2}{6} = \\frac{1}{3}$$\nThis result represents the average difference in irreducible error between the two loss functions, over a uniform distribution of possible true probabilities.",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        },
        {
            "introduction": "Our final exercise moves from theoretical risk to the practical behavior of a common non-parametric estimator. You will investigate a scenario with a sharp jump in the target function and analyze how a local averaging method performs when framed as a regression versus a classification problem. This practice provides a concrete illustration of how regression estimators can suffer from smoothing bias at discontinuities, while classification can remain remarkably robust in identifying the decision boundary .",
            "id": "3169366",
            "problem": "Consider a one-dimensional input variable $X$ distributed uniformly on $[0,1]$, that is, $X \\sim \\mathrm{Uniform}([0,1])$. The true target function $f(x)$ is piecewise-constant with a single jump (step) at $x=\\frac{1}{2}$:\n$$\nf(x) =\n\\begin{cases}\na,  x  \\frac{1}{2},\\\\\nb,  x \\ge \\frac{1}{2},\n\\end{cases}\n$$\nwhere $a \\ne b$ are fixed real constants. You are asked to contrast regression and classification in this setting and to quantify the error induced by smoothing across the jump for regression.\n\nRegression task: You observe a very large sample $\\{(X_i,Y_i)\\}_{i=1}^{n}$ with $Y_i = f(X_i) + \\varepsilon_i$, where the noise $\\varepsilon_i$ satisfies $\\mathbb{E}[\\varepsilon_i \\mid X_i] = 0$ and $\\mathrm{Var}(\\varepsilon_i \\mid X_i) = \\sigma^2$ with $\\sigma^2 \\in (0,\\infty)$ fixed. Consider the local-constant (Nadaraya–Watson) estimator with a box kernel of bandwidth $h \\in (0,\\frac{1}{2})$,\n$$\n\\widehat{f}_h(x) \\equiv \\frac{\\sum_{i=1}^{n} \\mathbf{1}\\{|X_i - x| \\le h\\} \\, Y_i}{\\sum_{i=1}^{n} \\mathbf{1}\\{|X_i - x| \\le h\\}}.\n$$\nClassification task: Define the class label $C = \\mathbf{1}\\{x \\ge \\frac{1}{2}\\}$. A majority-vote classifier uses local averaging of the indicator $\\mathbf{1}\\{X_i \\ge \\frac{1}{2}\\}$ inside the same window of width $2h$ to decide the class at $x$.\n\nWork in the infinite-sample limit (that is, let $n \\to \\infty$ so that variance effects vanish by the law of large numbers) and ignore boundary effects at $x=0$ and $x=1$ (assume $h$ is small enough so that windows entirely inside $[0,1]$ except possibly near $x=\\frac{1}{2}$). Using only fundamental definitions of risk under squared loss for regression and zero-one loss for classification, and the law of large numbers to replace empirical window averages by their population limits, do the following:\n\n1. Explain why the majority-vote classifier makes the Bayes-optimal decision almost everywhere on $[0,1]$ in this limit, including points arbitrarily close to the jump at $x=\\frac{1}{2}$, whereas the local-constant regression estimator suffers a nonzero smoothing bias in a neighborhood of the jump.\n\n2. Let $b_h(x) \\equiv \\mathbb{E}[\\widehat{f}_h(x)] - f(x)$ denote the pointwise bias of the regression estimator in the infinite-sample limit. Compute the leading-order contribution (in $h$) to the integrated squared bias,\n$$\n\\int_{0}^{1} \\big(b_h(x)\\big)^2 \\, dx,\n$$\narising solely from the jump at $x=\\frac{1}{2}$. Express your final answer in closed form as a function of $a$, $b$, and $h$.\n\nYour final answer must be a single analytic expression. No rounding is required. Do not include units.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, and objective. It represents a standard, formalizable problem in non-parametric statistical learning.\n\nThis problem contrasts the behavior of local averaging methods for regression and classification in the presence of a discontinuity. We operate in the infinite-sample limit ($n \\to \\infty$), which, by the Law of Large Numbers, allows us to replace empirical averages over a window with their theoretical expectations. The variance of the estimators vanishes, and their performance is determined solely by their bias. We ignore boundary effects at $x=0$ and $x=1$ as per the problem statement.\n\nFirst, we analyze the regression task. The Nadaraya-Watson estimator with a box kernel is given by $\\widehat{f}_h(x) = \\frac{\\sum_{i=1}^{n} \\mathbf{1}\\{|X_i - x| \\le h\\} \\, Y_i}{\\sum_{i=1}^{n} \\mathbf{1}\\{|X_i - x| \\le h\\}}$. In the infinite-sample limit, its expected value at a point $x$ is the conditional expectation of $Y$ given that $X$ falls into the window $[x-h, x+h]$.\n$$\n\\mathbb{E}[\\widehat{f}_h(x)] = \\mathbb{E}[Y \\mid X \\in [x-h, x+h]]\n$$\nGiven $Y = f(X) + \\varepsilon$ with $\\mathbb{E}[\\varepsilon \\mid X]=0$, this simplifies to the average of the true function $f$ over the window:\n$$\n\\mathbb{E}[\\widehat{f}_h(x)] = \\mathbb{E}[f(X) \\mid X \\in [x-h, x+h]]\n$$\nSince $X$ is uniformly distributed on $[0,1]$, the conditional distribution of $X$ over the window is also uniform. Thus, the expected value of the estimator is the integral average of $f$ over the window:\n$$\n\\mathbb{E}[\\widehat{f}_h(x)] = \\frac{1}{2h} \\int_{x-h}^{x+h} f(u) \\, du\n$$\nThe bias is $b_h(x) = \\mathbb{E}[\\widehat{f}_h(x)] - f(x)$. If the window $[x-h, x+h]$ does not contain the jump at $x=1/2$, then $f(u)$ is constant within the window, so $\\mathbb{E}[\\widehat{f}_h(x)] = f(x)$ and the bias is zero. However, for any $x$ in the interval $(\\frac{1}{2}-h, \\frac{1}{2}+h)$, the window $[x-h, x+h]$ contains the jump. The estimator will average the values $a$ and $b$, yielding an estimate $\\mathbb{E}[\\widehat{f}_h(x)]$ that is strictly between $a$ and $b$. Since the true function $f(x)$ is either $a$ or $b$, the bias $b_h(x)$ will be non-zero in this neighborhood of the jump. This smoothing effect is inherent to local averaging regression estimators at discontinuities.\n\nNext, we analyze the classification task. The goal is to predict the class label $C = \\mathbf{1}\\{x \\ge 1/2\\}$. The Bayes-optimal classifier for zero-one loss is $C^*(x) = \\mathbf{1}\\{P(C=1 \\mid X=x)  1/2\\}$. The true conditional probability is $P(C=1 \\mid X=x) = \\mathbf{1}\\{x \\ge 1/2\\}$, which is $0$ for $x1/2$ and $1$ for $x \\ge 1/2$. Thus, the Bayes-optimal rule is to predict class $0$ for $x1/2$ and class $1$ for $x \\ge 1/2$. This is $C^*(x) = \\mathbf{1}\\{x \\ge 1/2\\}$.\n\nThe majority-vote classifier first estimates the conditional probability locally: $\\widehat{p}(x) = \\mathbb{E}[\\mathbf{1}\\{X \\ge 1/2\\} \\mid X \\in [x-h, x+h]]$. As $X \\sim \\mathrm{Uniform}([0,1])$, this is the relative length of the portion of the window $[x-h, x+h]$ that lies at or to the right of $1/2$:\n$$\n\\widehat{p}(x) = \\frac{\\text{length}([1/2, \\infty) \\cap [x-h, x+h])}{2h}\n$$\nThe classifier then makes a decision $\\widehat{C}(x) = \\mathbf{1}\\{\\widehat{p}(x)  1/2\\}$.\nLet's analyze this decision rule for $x \\ne 1/2$:\n1. If $x  1/2$, the center of the window is to the right of the decision boundary. The interval $[x-h, x+h]$ will have more than half of its length in the region $[1/2, 1]$. Specifically, for $x \\in (1/2, 1/2+h]$, the intersection is $[1/2, x+h]$ with length $x+h-1/2$. So $\\widehat{p}(x) = \\frac{x+h-1/2}{2h}$. The condition $\\widehat{p}(x)  1/2$ becomes $\\frac{x+h-1/2}{2h}  1/2 \\implies x+h-1/2  h \\implies x  1/2$. This holds true. For $x  1/2+h$, $\\widehat{p}(x)=1$, so the condition holds. Thus, for any $x  1/2$, $\\widehat{C}(x)=1$, which matches the Bayes-optimal decision $C^*(x)$.\n2. If $x  1/2$, the center of the window is to the left of the decision boundary. The interval $[x-h, x+h]$ will have less than half of its length in the region $[1/2, 1]$. For $x \\in [1/2-h, 1/2)$, the intersection is $[1/2, x+h]$ with length $x+h-1/2$. The condition $\\widehat{p}(x)  1/2$ requires $x  1/2$, which is false. Thus $\\widehat{p}(x) \\le 1/2$ and $\\widehat{C}(x)=0$. For $x  1/2-h$, $\\widehat{p}(x)=0$, so $\\widehat{C}(x)=0$. Thus, for any $x  1/2$, $\\widehat{C}(x)=0$, which matches $C^*(x)$.\nAt the singular point $x=1/2$, we get $\\widehat{p}(1/2) = \\frac{h}{2h} = 1/2$, so $\\widehat{C}(1/2)=0$, while $C^*(1/2)=1$. The classifier is incorrect at this one point. However, since a single point has measure zero, the classifier makes the Bayes-optimal decision almost everywhere. The local averaging successfully recovers the decision boundary perfectly, even for points arbitrarily close to the jump, because the classification decision only depends on which side of $1/2$ the majority of the local data points lie, a property robustly determined by the location of the window's center $x$ relative to $1/2$.\n\nNow, we compute the integrated squared bias for the regression estimator. The bias $b_h(x)$ is non-zero only for $x \\in [\\frac{1}{2}-h, \\frac{1}{2}+h]$. The total integrated squared bias arising from the jump is $\\int_{1/2-h}^{1/2+h} (b_h(x))^2 \\, dx$.\nFor any $x \\in (\\frac{1}{2}-h, \\frac{1}{2}+h)$, the expected estimate is:\n$$\n\\mathbb{E}[\\widehat{f}_h(x)] = \\frac{1}{2h} \\left( \\int_{x-h}^{1/2} a \\, du + \\int_{1/2}^{x+h} b \\, du \\right) = \\frac{1}{2h} \\left( a(\\tfrac{1}{2} - (x-h)) + b(x+h - \\tfrac{1}{2}) \\right)\n$$\nWe split the calculation into two regions:\n1. For $x \\in [\\frac{1}{2}-h, \\frac{1}{2})$, $f(x)=a$. The bias is:\n$$\nb_h(x) = \\frac{a(\\frac{1}{2} - x + h) + b(x+h - \\frac{1}{2})}{2h} - a = \\frac{a(\\frac{1}{2} - x + h - 2h) + b(x+h - \\frac{1}{2})}{2h} = \\frac{(b-a)(x+h-\\frac{1}{2})}{2h}\n$$\n2. For $x \\in [\\frac{1}{2}, \\frac{1}{2}+h]$, $f(x)=b$. The bias is:\n$$\nb_h(x) = \\frac{a(\\frac{1}{2} - x + h) + b(x+h - \\frac{1}{2})}{2h} - b = \\frac{a(\\frac{1}{2} - x + h) + b(x+h - \\frac{1}{2} - 2h)}{2h} = \\frac{(a-b)(\\frac{1}{2}-x+h)}{2h}\n$$\nThe integrated squared bias is the sum of integrals over these two regions.\nFor the first region, let $u = x+h-\\frac{1}{2}$, so $du=dx$. The integration limits for $u$ are from $0$ to $h$.\n$$\n\\int_{1/2-h}^{1/2} \\left( \\frac{(b-a)(x+h-\\frac{1}{2})}{2h} \\right)^2 dx = \\int_{0}^{h} \\left( \\frac{b-a}{2h} u \\right)^2 du = \\frac{(b-a)^2}{4h^2} \\int_{0}^{h} u^2 du = \\frac{(b-a)^2}{4h^2} \\left[ \\frac{u^3}{3} \\right]_0^h = \\frac{(b-a)^2 h}{12}\n$$\nFor the second region, let $v = \\frac{1}{2}-x+h$, so $dv=-dx$. The integration limits for $v$ are from $h$ to $0$.\n$$\n\\int_{1/2}^{1/2+h} \\left( \\frac{(a-b)(\\frac{1}{2}-x+h)}{2h} \\right)^2 dx = \\int_{h}^{0} \\left( \\frac{a-b}{2h} v \\right)^2 (-dv) = \\int_{0}^{h} \\left( \\frac{a-b}{2h} v \\right)^2 dv\n$$\nSince $(a-b)^2 = (b-a)^2$, this integral is identical to the first one and its value is also $\\frac{(b-a)^2 h}{12}$.\nThe total integrated squared bias from the jump is the sum of these two contributions:\n$$\n\\text{ISB}_{\\text{jump}} = \\frac{(b-a)^2 h}{12} + \\frac{(b-a)^2 h}{12} = \\frac{2(b-a)^2 h}{12} = \\frac{(b-a)^2 h}{6}\n$$\nThis is the leading-order contribution in $h$ to the integrated squared bias.",
            "answer": "$$\n\\boxed{\\frac{(b-a)^{2}h}{6}}\n$$"
        }
    ]
}