{
    "hands_on_practices": [
        {
            "introduction": "我们首先探讨分类与回归之间最根本的区别：我们试图最小化的误差的性质。这个思想实验构建了一个场景，其中分类可以达到完美，而回归却因噪声的存在而具有内在的局限性。通过为每个任务推导最小可能误差（贝叶斯风险），你将对以下问题获得核心的直觉：为什么即使输入数据相同，一个任务可能“简单”，而另一个任务却“困难” 。",
            "id": "3169383",
            "problem": "考虑一个监督学习场景，其中有一个从连续分布中抽取的单一实值特征 $X$。在相同的输入 $X$ 上定义了两个学习任务，但目标不同：一个是二元分类任务，另一个是实值回归任务。设数据生成机制为：\n- $X \\sim \\mathrm{Uniform}(-1,1)$,\n- $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ 且独立于 $X$,\n- 分类标签 $C \\in \\{0,1\\}$ 由 $C = \\mathbb{I}\\{X \\geq 0\\}$ 定义，\n- 回归目标 $Y \\in \\mathbb{R}$ 由 $Y = X + \\epsilon$ 定义。\n\n请从分类的 0-1 损失和回归的平方误差损失的期望风险的基本定义，以及条件概率和条件期望的定义出发。仅使用这些基础知识，推导在上述生成模型下每个任务的贝叶斯最优风险。具体而言：\n\n1. 对于具有 0-1 损失的分类任务，推导在所有可测分类器中可实现的最小期望错分率。\n2. 对于具有平方误差损失的回归任务，推导在所有可测预测器中可实现的最小期望均方误差 (MSE)。\n\n解释为什么这个构造提供了一个反例，其中完美分类是可能的（即类别是可分的），但由于偶然噪声的存在，回归的均方误差被一个正常数限定了下界。将你的最终答案表示为一个行矩阵，其第一个条目是最小分类风险，第二个条目是最小回归风险，用 $\\sigma^{2}$ 表示。不需要四舍五入。",
            "solution": "所述问题在科学上是合理的、良定的、客观的且自洽的。这是统计学习理论中的一个标准问题，用来说明贝叶斯最优预测器和不可约误差（偶然不确定性）的概念。所有给出的定义和分布都是标准的。没有矛盾或含糊之处。该问题是有效的，并且可以解决。\n\n我们将按顺序处理这两个任务，从每种情况下风险和贝叶斯最优预测器的基本定义开始。\n\n**1. 最小分类风险 (0-1 损失)**\n\n任务是将具有特征 $X$ 的观测值分类到两个类别 $C \\in \\{0, 1\\}$ 中的一个。损失函数是 0-1 损失，定义为 $L(C, \\hat{C}) = \\mathbb{I}\\{C \\neq \\hat{C}\\}$，其中 $\\hat{C}$ 是分类器 $\\hat{c}(X)$ 预测的类别。分类器 $\\hat{c}$ 的风险是期望损失：\n$$\nR(\\hat{c}) = E[L(C, \\hat{c}(X))] = E[\\mathbb{I}\\{C \\neq \\hat{c}(X)\\}] = P(C \\neq \\hat{c}(X))\n$$\n目标是找到最小化此风险的分类器 $\\hat{c}^*(X)$。该分类器被称为贝叶斯分类器。贝叶斯分类器的风险即为贝叶斯风险。\n\n根据全期望定律，风险可以写为：\n$$\nR(\\hat{c}) = E_X[E_{C|X}[ \\mathbb{I}\\{C \\neq \\hat{c}(X)\\} | X=x ]]\n$$\n为了最小化总风险，我们必须对 $x$ 的每个值最小化内部的条件期望。对于给定的 $x$，贝叶斯决策规则是：\n$$\n\\hat{c}^*(x) = \\arg\\min_{k \\in \\{0,1\\}} E[ \\mathbb{I}\\{C \\neq k\\} | X=x ] = \\arg\\min_{k \\in \\{0,1\\}} P(C \\neq k | X=x)\n$$\n最小化犯错的概率等同于最大化正确的概率。因此，贝叶斯分类器选择具有最高后验概率的类别：\n$$\n\\hat{c}^*(x) = \\arg\\max_{k \\in \\{0,1\\}} P(C = k | X=x)\n$$\n让我们定义条件概率函数 $\\eta(x) = P(C=1|X=x)$。那么 $P(C=0|X=x) = 1 - \\eta(x)$。规则变为：如果 $\\eta(x) > 1 - \\eta(x)$，即如果 $\\eta(x) > 1/2$，则选择类别 1，否则选择类别 0。\n\n在这个问题中，类别标签 $C$ 是特征 $X$ 的一个确定性函数：$C = \\mathbb{I}\\{X \\geq 0\\}$。这意味着对于随机变量 $X$ 的一个给定值 $x$，$C$ 的值是确定已知的。在以 $X$ 为条件的 $C$ 中没有随机性。\n具体来说：\n- 如果 $x \\geq 0$，则 $C = 1$ 的概率为 1。因此，$\\eta(x) = P(C=1|X=x) = 1$。\n- 如果 $x  0$，则 $C = 0$ 的概率为 1。因此，$\\eta(x) = P(C=1|X=x) = 0$。\n\n应用贝叶斯决策规则：\n- 对于 $x \\geq 0$，$\\eta(x)=1  1/2$，因此贝叶斯分类器预测 $\\hat{c}^*(x) = 1$。\n- 对于 $x  0$，$\\eta(x)=0  1/2$，因此贝叶斯分类器预测 $\\hat{c}^*(x) = 0$。\n\n这可以概括为 $\\hat{c}^*(X) = \\mathbb{I}\\{X \\geq 0\\}$。我们观察到贝叶斯分类器与类别标签的真实数据生成函数完全相同：$\\hat{c}^*(X) = C$。\n\n可实现的最小风险（贝叶斯风险）是这个最优分类器的风险：\n$$\nR^* = R(\\hat{c}^*) = E[\\mathbb{I}\\{C \\neq \\hat{c}^*(X)\\}]\n$$\n由于对于 $X$ 的所有可能结果，都有 $\\hat{c}^*(X) = C$，事件 $\\{C \\neq \\hat{c}^*(X)\\}$ 是一个不可能事件。因此，指示函数始终为 $0$。\n$$\nR^* = E[0] = 0\n$$\n可实现的最小期望错分率为 $0$。这表明类别是完全可分的，最优决策边界可以无误差地学习到。\n\n**2. 最小回归风险 (平方误差损失)**\n\n任务是从特征 $X$ 预测一个实值目标 $Y$。损失函数是平方误差损失，定义为 $L(Y, \\hat{Y}) = (Y - \\hat{Y})^2$，其中 $\\hat{Y}$ 是回归函数 $f(X)$ 的预测值。回归函数 $f$ 的风险是期望损失，即均方误差 (MSE)：\n$$\nR(f) = E[L(Y, f(X))] = E[(Y - f(X))^2]\n$$\n目标是找到最小化此风险的函数 $f^*(X)$。这就是回归问题的贝叶斯预测器。\n\n使用全期望定律，我们分解风险：\n$$\nR(f) = E_X[E_{Y|X}[(Y - f(X))^2|X=x]]\n$$\n为了最小化总风险，我们必须选择 $f(x)$ 来最小化对每个 $x$ 的内部条件期望。对于一个固定的 $x$，$f(x)$ 是一个常数，我们称之为 $g$。我们需要找到最小化 $E[(Y-g)^2|X=x]$ 的 $g$。这是概率论中的一个经典结果：最小化与随机变量 $Y$ 的期望平方差的 $g$ 值是 $Y$ 的期望。因此，最优预测器是给定特征下目标的条件期望：\n$$\nf^*(x) = E[Y|X=x]\n$$\n让我们为给定的数据生成过程 $Y = X + \\epsilon$ 计算这个值，其中 $X \\sim \\mathrm{Uniform}(-1,1)$ 和 $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立的。\n$$\nf^*(x) = E[X + \\epsilon | X=x]\n$$\n利用条件期望的线性性：\n$$\nf^*(x) = E[X | X=x] + E[\\epsilon | X=x]\n$$\n对于第一项，$E[X | X=x] = x$。对于第二项，由于 $\\epsilon$ 和 $X$ 是独立的，以 $X$ 为条件并不提供任何关于 $\\epsilon$ 的信息。因此，$E[\\epsilon | X=x] = E[\\epsilon]$。我们已知 $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$，所以它的均值是 $E[\\epsilon] = 0$。\n将这些代回，我们得到贝叶斯预测器：\n$$\nf^*(x) = x + 0 = x\n$$\n所以，最优回归函数是 $f^*(X) = X$。\n\n现在，我们通过评估这个最优预测器的 MSE 来计算可实现的最小风险（贝叶斯风险）：\n$$\nR^* = R(f^*) = E[(Y - f^*(X))^2] = E[(Y - X)^2]\n$$\n代入 $Y = X + \\epsilon$ 的定义：\n$$\nR^* = E[((X + \\epsilon) - X)^2] = E[\\epsilon^2]\n$$\n值 $E[\\epsilon^2]$ 可以从方差的定义中找到：$\\mathrm{Var}(\\epsilon) = E[\\epsilon^2] - (E[\\epsilon])^2$。\n我们已知 $\\mathrm{Var}(\\epsilon) = \\sigma^2$ 和 $E[\\epsilon] = 0$。\n$$\n\\sigma^2 = E[\\epsilon^2] - 0^2 \\implies E[\\epsilon^2] = \\sigma^2\n$$\n因此，可实现的最小 MSE 是：\n$$\nR^* = \\sigma^2\n$$\n\n**反例解释**\n\n这个构造清楚地反驳了任何天真的假设，即在相同特征上的“简单”分类问题意味着“简单”的回归问题。\n- 在分类任务中，目标 $C = \\mathbb{I}\\{X \\geq 0\\}$ 是输入特征 $X$ 的一个确定性的、无噪声的函数。这种关系是精确的。因此，一个理想的学习器可以找到决策边界 $X = 0$ 并实现完美分类，从而得到 $0$ 的贝叶斯风险。\n- 在回归任务中，目标 $Y = X + \\epsilon$ 有两个组成部分：一个确定性部分 ($X$) 和一个随机性部分 ($\\epsilon$)。贝叶斯预测器 $f^*(X)=X$ 可以完美地学习确定性关系。然而，噪声项 $\\epsilon$ 是随机的，并且关键是，它独立于 $X$。任何关于 $X$ 的函数都无法预测 $\\epsilon$ 的值。这个不可预测的成分导致了不可约误差，通常称为偶然不确定性。贝叶斯风险 $R^* = \\sigma^2$ 正是这个噪声的方差。除非 $\\sigma^2=0$，否则最小 MSE 严格为正，这意味着完美的回归是不可能的。\n\n最小分类风险是 $0$，最小回归风险是 $\\sigma^2$。最终答案是包含这两个值的行矩阵。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  \\sigma^{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在建立了不可约误差的概念之后，我们现在来研究回归和分类的选择如何影响实际估计器的行为。本练习使用一个带有急剧跳变的函数来模拟价格突变或相变等情景。你将分析为什么局部平均法（一种常见的非参数技术）在回归设置中难以准确估计跳变值，却能成功地为分类任务识别出边界 。",
            "id": "3169366",
            "problem": "考虑一个在 $[0,1]$ 上均匀分布的一维输入变量 $X$，即 $X \\sim \\mathrm{Uniform}([0,1])$。真实目标函数 $f(x)$ 是分段常数函数，在 $x=\\frac{1}{2}$ 处有一个单独的跳跃（阶跃）：\n$$\nf(x) =\n\\begin{cases}\na,  x  \\frac{1}{2},\\\\\nb,  x \\ge \\frac{1}{2},\n\\end{cases}\n$$\n其中 $a \\ne b$ 是固定的实常数。要求您在此设置中对比回归和分类，并量化回归中因跨越跳跃点平滑而产生的误差。\n\n回归任务：您观察到一个非常大的样本 $\\{(X_i,Y_i)\\}_{i=1}^{n}$，其中 $Y_i = f(X_i) + \\varepsilon_i$，噪声 $\\varepsilon_i$ 满足 $\\mathbb{E}[\\varepsilon_i \\mid X_i] = 0$ 和 $\\mathrm{Var}(\\varepsilon_i \\mid X_i) = \\sigma^2$，其中 $\\sigma^2 \\in (0,\\infty)$ 是固定的。考虑使用带宽为 $h \\in (0,\\frac{1}{2})$ 的盒状核的局部常数（Nadaraya–Watson）估计量，\n$$\n\\widehat{f}_h(x) \\equiv \\frac{\\sum_{i=1}^{n} \\mathbf{1}\\{|X_i - x| \\le h\\} \\, Y_i}{\\sum_{i=1}^{n} \\mathbf{1}\\{|X_i - x| \\le h\\}}.\n$$\n分类任务：定义类别标签 $C = \\mathbf{1}\\{X \\ge \\frac{1}{2}\\}$。多数投票分类器在宽度为 $2h$ 的相同窗口内对指示函数 $\\mathbf{1}\\{X_i \\ge \\frac{1}{2}\\}$ 进行局部平均，以决定点 $x$ 处的类别。\n\n在无穷样本极限（即令 $n \\to \\infty$，从而根据大数定律方差效应消失）下进行分析，并忽略在 $x=0$ 和 $x=1$ 处的边界效应（假设 $h$ 足够小，以至于除了可能在 $x=\\frac{1}{2}$ 附近，窗口完全位于 $[0,1]$ 内）。仅使用平方损失下回归的风险和 0-1 损失下分类的风险的基本定义，以及用大数定律将经验窗口平均替换为其总体极限，完成以下任务：\n\n1. 解释为什么在此极限下，多数投票分类器在 $[0,1]$ 上几乎处处都能做出贝叶斯最优决策，包括任意靠近跳跃点 $x=\\frac{1}{2}$ 的点，而局部常数回归估计量在跳跃点的一个邻域内会存在非零的平滑偏差。\n\n2. 令 $b_h(x) \\equiv \\mathbb{E}[\\widehat{f}_h(x)] - f(x)$ 表示无穷样本极限下回归估计量的逐点偏差。计算积分平方偏差\n$$\n\\int_{0}^{1} \\big(b_h(x)\\big)^2 \\, dx,\n$$\n中仅由 $x=\\frac{1}{2}$ 处的跳跃引起的主阶贡献（关于 $h$）。将您的最终答案表示为 $a$、$b$ 和 $h$ 的闭式函数。\n\n您的最终答案必须是单个解析表达式。无需四舍五入。不要包含单位。",
            "solution": "经评估，此问题具有科学依据，提法明确且客观，是有效的问题。它代表了非参数统计学习中的一个标准的、可形式化的问题。\n\n这个问题对比了在存在不连续点的情况下，局部平均方法在回归和分类中的行为。我们在无穷样本极限（$n \\to \\infty$）下进行操作，根据大数定律，这使我们能够用窗口上的经验平均替换为其理论期望。估计量的方差消失，其性能完全由其偏差决定。根据问题陈述，我们忽略在 $x=0$ 和 $x=1$ 处的边界效应。\n\n首先，我们分析回归任务。使用盒状核的 Nadaraya-Watson 估计量由 $\\widehat{f}_h(x) = \\frac{\\sum_{i=1}^{n} \\mathbf{1}\\{|X_i - x| \\le h\\} \\, Y_i}{\\sum_{i=1}^{n} \\mathbf{1}\\{|X_i - x| \\le h\\}}$ 给出。在无穷样本极限下，其在点 $x$ 处的期望值是给定 $X$ 落入窗口 $[x-h, x+h]$ 时 $Y$ 的条件期望。\n$$\n\\mathbb{E}[\\widehat{f}_h(x)] = \\mathbb{E}[Y \\mid X \\in [x-h, x+h]]\n$$\n给定 $Y = f(X) + \\varepsilon$ 且 $\\mathbb{E}[\\varepsilon \\mid X]=0$，这简化为真实函数 $f$ 在该窗口上的平均值：\n$$\n\\mathbb{E}[\\widehat{f}_h(x)] = \\mathbb{E}[f(X) \\mid X \\in [x-h, x+h]]\n$$\n由于 $X$ 在 $[0,1]$ 上均匀分布，因此 $X$ 在该窗口上的条件分布也是均匀的。因此，估计量的期望值是 $f$ 在该窗口上的积分平均：\n$$\n\\mathbb{E}[\\widehat{f}_h(x)] = \\frac{1}{2h} \\int_{x-h}^{x+h} f(u) \\, du\n$$\n偏差为 $b_h(x) = \\mathbb{E}[\\widehat{f}_h(x)] - f(x)$。如果窗口 $[x-h, x+h]$ 不包含 $x=1/2$ 处的跳跃点，那么 $f(u)$ 在窗口内是常数，所以 $\\mathbb{E}[\\widehat{f}_h(x)] = f(x)$ 且偏差为零。然而，对于区间 $(\\frac{1}{2}-h, \\frac{1}{2}+h)$ 中的任何 $x$，窗口 $[x-h, x+h]$ 都包含该跳跃点。估计量将对值 $a$ 和 $b$ 进行平均，得到一个严格介于 $a$ 和 $b$ 之间的估计值 $\\mathbb{E}[\\widehat{f}_h(x)]$。由于真实函数 $f(x)$ 的值是 $a$ 或 $b$，因此在跳跃点的这个邻域内，偏差 $b_h(x)$ 将是非零的。这种平滑效应是局部平均回归估计量在不连续点处的固有特性。\n\n接下来，我们分析分类任务。目标是预测类别标签 $C = \\mathbf{1}\\{x \\ge 1/2\\}$。对于 0-1 损失，贝叶斯最优分类器是 $C^*(x) = \\mathbf{1}\\{P(C=1 \\mid X=x)  1/2\\}$。真实的条件概率是 $P(C=1 \\mid X=x) = \\mathbf{1}\\{x \\ge 1/2\\}$，当 $x1/2$ 时为 $0$，当 $x \\ge 1/2$ 时为 $1$。因此，贝叶斯最优法则是对 $x1/2$ 预测为类别 $0$，对 $x \\ge 1/2$ 预测为类别 $1$。即 $C^*(x) = \\mathbf{1}\\{x \\ge 1/2\\}$。\n\n多数投票分类器首先局部地估计条件概率：$\\widehat{p}(x) = \\mathbb{E}[\\mathbf{1}\\{X \\ge 1/2\\} \\mid X \\in [x-h, x+h]]$。由于 $X \\sim \\mathrm{Uniform}([0,1])$，这等于窗口 $[x-h, x+h]$ 中位于 $1/2$ 或其右侧部分的相对长度：\n$$\n\\widehat{p}(x) = \\frac{\\text{length}([1/2, \\infty) \\cap [x-h, x+h])}{2h}\n$$\n分类器随后做出决策 $\\widehat{C}(x) = \\mathbf{1}\\{\\widehat{p}(x)  1/2\\}$。\n让我们对 $x \\ne 1/2$ 分析这个决策规则：\n1. 如果 $x  1/2$，则窗口的中心位于决策边界的右侧。区间 $[x-h, x+h]$ 的一半以上长度将位于区域 $[1/2, 1]$ 中。具体来说，对于 $x \\in (1/2, 1/2+h]$，交集为 $[1/2, x+h]$，长度为 $x+h-1/2$。所以 $\\widehat{p}(x) = \\frac{x+h-1/2}{2h}$。条件 $\\widehat{p}(x)  1/2$ 变为 $\\frac{x+h-1/2}{2h}  1/2 \\implies x+h-1/2  h \\implies x  1/2$。这是成立的。对于 $x  1/2+h$，$\\widehat{p}(x)=1$，所以条件成立。因此，对于任何 $x  1/2$，$\\widehat{C}(x)=1$，这与贝叶斯最优决策 $C^*(x)$ 相匹配。\n2. 如果 $x  1/2$，则窗口的中心位于决策边界的左侧。区间 $[x-h, x+h]$ 将有不到一半的长度位于区域 $[1/2, 1]$ 中。对于 $x \\in [1/2-h, 1/2)$，交集为 $[1/2, x+h]$，长度为 $x+h-1/2$。条件 $\\widehat{p}(x)  1/2$ 要求 $x  1/2$，这是不成立的。因此 $\\widehat{p}(x) \\le 1/2$ 且 $\\widehat{C}(x)=0$。对于 $x  1/2-h$，$\\widehat{p}(x)=0$，所以 $\\widehat{C}(x)=0$。因此，对于任何 $x  1/2$，$\\widehat{C}(x)=0$，这与 $C^*(x)$ 相匹配。\n在奇异点 $x=1/2$ 处，我们得到 $\\widehat{p}(1/2) = \\frac{h}{2h} = 1/2$，所以 $\\widehat{C}(1/2)=0$，而 $C^*(1/2)=1$。分类器在这一点上是错误的。然而，由于单点的测度为零，分类器几乎处处都能做出贝叶斯最优决策。局部平均成功地完美恢复了决策边界，即使对于任意靠近跳跃点的点也是如此，因为分类决策仅取决于局部数据点的大多数位于 $1/2$ 的哪一侧，这是一个由窗口中心 $x$ 相对于 $1/2$ 的位置稳健决定的属性。\n\n现在，我们计算回归估计量的积分平方偏差。偏差 $b_h(x)$ 仅在 $x \\in [\\frac{1}{2}-h, \\frac{1}{2}+h]$ 时非零。由跳跃产生的总积分平方偏差为 $\\int_{1/2-h}^{1/2+h} (b_h(x))^2 \\, dx$。\n对于任何 $x \\in (\\frac{1}{2}-h, \\frac{1}{2}+h)$，期望的估计值为：\n$$\n\\mathbb{E}[\\widehat{f}_h(x)] = \\frac{1}{2h} \\left( \\int_{x-h}^{1/2} a \\, du + \\int_{1/2}^{x+h} b \\, du \\right) = \\frac{1}{2h} \\left( a(\\tfrac{1}{2} - (x-h)) + b(x+h - \\tfrac{1}{2}) \\right)\n$$\n我们将计算分为两个区域：\n1. 对于 $x \\in [\\frac{1}{2}-h, \\frac{1}{2})$，$f(x)=a$。偏差为：\n$$\nb_h(x) = \\frac{a(\\frac{1}{2} - x + h) + b(x+h - \\frac{1}{2})}{2h} - a = \\frac{a(\\frac{1}{2} - x + h - 2h) + b(x+h - \\frac{1}{2})}{2h} = \\frac{(b-a)(x+h-\\frac{1}{2})}{2h}\n$$\n2. 对于 $x \\in [\\frac{1}{2}, \\frac{1}{2}+h]$，$f(x)=b$。偏差为：\n$$\nb_h(x) = \\frac{a(\\frac{1}{2} - x + h) + b(x+h - \\frac{1}{2})}{2h} - b = \\frac{a(\\frac{1}{2} - x + h) + b(x+h - \\frac{1}{2} - 2h)}{2h} = \\frac{(a-b)(\\frac{1}{2}-x+h)}{2h}\n$$\n积分平方偏差是这两个区域上积分的总和。\n对于第一个区域，令 $u = x+h-\\frac{1}{2}$，则 $du=dx$。$u$ 的积分限从 $0$ 到 $h$。\n$$\n\\int_{1/2-h}^{1/2} \\left( \\frac{(b-a)(x+h-\\frac{1}{2})}{2h} \\right)^2 dx = \\int_{0}^{h} \\left( \\frac{b-a}{2h} u \\right)^2 du = \\frac{(b-a)^2}{4h^2} \\int_{0}^{h} u^2 du = \\frac{(b-a)^2}{4h^2} \\left[ \\frac{u^3}{3} \\right]_0^h = \\frac{(b-a)^2 h}{12}\n$$\n对于第二个区域，令 $v = \\frac{1}{2}-x+h$，则 $dv=-dx$。$v$ 的积分限从 $h$ 到 $0$。\n$$\n\\int_{1/2}^{1/2+h} \\left( \\frac{(a-b)(\\frac{1}{2}-x+h)}{2h} \\right)^2 dx = \\int_{h}^{0} \\left( \\frac{a-b}{2h} v \\right)^2 (-dv) = \\int_{0}^{h} \\left( \\frac{a-b}{2h} v \\right)^2 dv\n$$\n由于 $(a-b)^2 = (b-a)^2$，这个积分与第一个积分相同，其值也为 $\\frac{(b-a)^2 h}{12}$。\n由跳跃产生的总积分平方偏差是这两部分贡献之和：\n$$\n\\text{ISB}_{\\text{jump}} = \\frac{(b-a)^2 h}{12} + \\frac{(b-a)^2 h}{12} = \\frac{2(b-a)^2 h}{12} = \\frac{(b-a)^2 h}{6}\n$$\n这是积分平方偏差中关于 $h$ 的主阶贡献。",
            "answer": "$$\n\\boxed{\\frac{(b-a)^{2}h}{6}}\n$$"
        },
        {
            "introduction": "在现实世界中，不仅目标变量可能存在噪声，输入特征本身的测量也常常带有误差。这最后一个练习探讨了这种“变量误差”带来的后果。你将推导预测变量中的测量误差如何系统性地使回归结果产生偏差——这一现象被称为“衰减”——并观察到这种偏差如何扭曲后续分类任务中的决策边界，从而以一种实际的方式将这两个问题联系起来 。",
            "id": "3169412",
            "problem": "一个标量预测变量的测量含有噪声。设真实的预测变量为 $X$，其期望 $\\mathbb{E}[X] = 0$，方差 $\\operatorname{Var}(X) = \\sigma_{X}^{2}$。观测到的预测变量是 $W = X + U$，其中 $U$ 是测量误差，其期望 $\\mathbb{E}[U] = 0$，方差 $\\operatorname{Var}(U) = \\sigma_{U}^{2}$，且独立于 $X$。响应为 $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$，其中 $\\mathbb{E}[\\varepsilon] = 0$，方差 $\\operatorname{Var}(\\varepsilon) = \\sigma_{\\varepsilon}^{2}$，且 $\\varepsilon$ 独立于 $X$ 和 $U$。考虑 $Y$ 对 $W$ 的总体普通最小二乘 (OLS) 回归，其斜率和截距分别记为 $\\tilde{\\beta}_{1}$ 和 $\\tilde{\\beta}_{0}$。一位实践者使用基于此朴素拟合构建的回归阈值分类器：如果线性得分 $\\tilde{\\beta}_{0} + \\tilde{\\beta}_{1} w$ 大于等于给定阈值 $\\tau$，则预测为类别 1，否则为类别 0。\n\n从期望、方差和协方差的定义以及这些算子在独立性下的线性性质出发，推导由测量误差引起的回归衰减，并说明在观测预测变量空间中，基于阈值的分类边界相对于真实预测变量下的边界是如何扭曲的。然后，假设 $\\sigma_{X}^{2}$ 和 $\\sigma_{U}^{2}$ 已知，为回归斜率和分类决策边界提供闭式偏差校正，使得校正后的分类器使用 $W$ 但能匹配由真实预测变量所隐含的边界。\n\n用 $\\tilde{\\beta}_{1}$、$\\sigma_{X}^{2}$、$\\sigma_{U}^{2}$、$\\beta_{0}$ 和 $\\tau$ 来表示你的最终答案。你的最终答案必须是一个单一的闭式解析表达式，其中包含这两个量，并使用 $\\mathrm{pmatrix}$ 环境排列成一个两元素的行矩阵。无需四舍五入。",
            "solution": "我们从期望、方差和协方差的基本定义开始。对 $Y$ 关于 $W$ 进行回归的总体普通最小二乘 (OLS) 斜率由下式给出\n$$\n\\tilde{\\beta}_{1} \\;=\\; \\frac{\\operatorname{Cov}(Y, W)}{\\operatorname{Var}(W)}.\n$$\nOLS 截距是\n$$\n\\tilde{\\beta}_{0} \\;=\\; \\mathbb{E}[Y] \\;-\\; \\tilde{\\beta}_{1}\\,\\mathbb{E}[W].\n$$\n根据独立性和线性性质，我们计算 $\\operatorname{Cov}(Y,W)$。使用 $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$ 和 $W = X + U$，\n$$\n\\operatorname{Cov}(Y, W) \\;=\\; \\operatorname{Cov}(\\beta_{0} + \\beta_{1} X + \\varepsilon,\\, X + U)\n\\;=\\; \\beta_{1}\\,\\operatorname{Cov}(X, X) \\;+\\; \\beta_{1}\\,\\operatorname{Cov}(X, U) \\;+\\; \\operatorname{Cov}(\\varepsilon, X) \\;+\\; \\operatorname{Cov}(\\varepsilon, U).\n$$\n因为如题所设，$X$、$U$ 和 $\\varepsilon$ 相互独立且中心化，所以 $\\operatorname{Cov}(X,U) = 0$，$\\operatorname{Cov}(\\varepsilon,X) = 0$ 且 $\\operatorname{Cov}(\\varepsilon,U) = 0$。因此\n$$\n\\operatorname{Cov}(Y, W) \\;=\\; \\beta_{1}\\,\\operatorname{Var}(X) \\;=\\; \\beta_{1}\\,\\sigma_{X}^{2}.\n$$\n接下来，\n$$\n\\operatorname{Var}(W) \\;=\\; \\operatorname{Var}(X + U) \\;=\\; \\operatorname{Var}(X) + \\operatorname{Var}(U) \\;=\\; \\sigma_{X}^{2} + \\sigma_{U}^{2},\n$$\n同样是根据独立性。因此，$Y$ 对 $W$ 的朴素回归的总体 OLS 斜率为\n$$\n\\tilde{\\beta}_{1} \\;=\\; \\frac{\\beta_{1}\\,\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}},\n$$\n这展示了经典的衰减现象：测量得到的斜率被因子 $\\lambda = \\sigma_{X}^{2}/(\\sigma_{X}^{2} + \\sigma_{U}^{2})$ 向零收缩。对于截距，注意到 $\\mathbb{E}[W] = \\mathbb{E}[X] + \\mathbb{E}[U] = 0$ 并且 $\\mathbb{E}[Y] = \\beta_{0} + \\beta_{1}\\mathbb{E}[X] + \\mathbb{E}[\\varepsilon] = \\beta_{0}$，所以\n$$\n\\tilde{\\beta}_{0} \\;=\\; \\beta_{0} \\;-\\; \\tilde{\\beta}_{1}\\cdot 0 \\;=\\; \\beta_{0}.\n$$\n\n我们现在分析基于阈值的分类边界。在真实预测变量 $X$ 下，当 $Y$ 的确定性部分超过 $\\tau$ 时预测为类别 1 的回归阈值决策规则可以写成：如果 $\\beta_{0} + \\beta_{1} x \\ge \\tau$，则分类为 1。在 $X$ 空间中的边界通过求解下式得到\n$$\n\\beta_{0} + \\beta_{1} x^{\\ast} = \\tau \\quad\\Rightarrow\\quad x^{\\ast} = \\frac{\\tau - \\beta_{0}}{\\beta_{1}}.\n$$\n实践者使用对 $W$ 的朴素回归，构建决策规则：如果 $\\tilde{\\beta}_{0} + \\tilde{\\beta}_{1} w \\ge \\tau$，则预测为类别 1。由于 $\\tilde{\\beta}_{0} = \\beta_{0}$，在 $W$ 空间中的边界为\n$$\nw_{\\text{naive}} \\;=\\; \\frac{\\tau - \\beta_{0}}{\\tilde{\\beta}_{1}} \n\\;=\\; \\frac{\\tau - \\beta_{0}}{\\beta_{1}\\,\\sigma_{X}^{2}/(\\sigma_{X}^{2} + \\sigma_{U}^{2})}\n\\;=\\; \\frac{\\sigma_{X}^{2} + \\sigma_{U}^{2}}{\\sigma_{X}^{2}} \\cdot \\frac{\\tau - \\beta_{0}}{\\beta_{1}}\n\\;=\\; \\frac{1}{\\lambda}\\,x^{\\ast}.\n$$\n这显示了分类边界的扭曲：测量误差导致在观测预测变量空间中的决策边界值被因子 $(\\sigma_{X}^{2} + \\sigma_{U}^{2})/\\sigma_{X}^{2}$ 扩大。\n\n假设 $\\sigma_{X}^{2}$ 和 $\\sigma_{U}^{2}$ 已知，我们可以通过对衰减因子求逆来校正斜率中的衰减偏差。定义校正因子\n$$\nc \\;=\\; \\frac{\\sigma_{X}^{2} + \\sigma_{U}^{2}}{\\sigma_{X}^{2}} \\;=\\; \\frac{1}{\\lambda}.\n$$\n那么一个经过偏差校正的斜率是\n$$\n\\hat{\\beta}_{\\text{corr}} \\;=\\; \\tilde{\\beta}_{1}\\,c \\;=\\; \\tilde{\\beta}_{1}\\,\\frac{\\sigma_{X}^{2} + \\sigma_{U}^{2}}{\\sigma_{X}^{2}}.\n$$\n在回归阈值分类器中使用这个校正后的斜率，可以得到在 $W$ 空间中的校正边界：\n$$\nw_{\\text{corr}} \\;=\\; \\frac{\\tau - \\beta_{0}}{\\hat{\\beta}_{\\text{corr}}}\n\\;=\\; \\frac{\\tau - \\beta_{0}}{\\tilde{\\beta}_{1}\\,(\\sigma_{X}^{2} + \\sigma_{U}^{2})/\\sigma_{X}^{2}}\n\\;=\\; \\frac{\\sigma_{X}^{2}}{\\tilde{\\beta}_{1}\\,(\\sigma_{X}^{2} + \\sigma_{U}^{2})}\\,(\\tau - \\beta_{0}).\n$$\n等价地，由于 $\\tilde{\\beta}_{1} = \\lambda \\beta_{1}$，我们有 $w_{\\text{corr}} = (\\tau - \\beta_{0})/\\beta_{1} = x^{\\ast}$，这表明当在观测预测变量空间中表示时，校正后的分类器恢复了真实的边界。\n\n所要求的、纯粹用 $\\tilde{\\beta}_{1}$、$\\sigma_{X}^{2}$、$\\sigma_{U}^{2}$、$\\beta_{0}$ 和 $\\tau$ 表示的闭式偏差校정，即为上面的配对 $(\\hat{\\beta}_{\\text{corr}},\\, w_{\\text{corr}})$。",
            "answer": "$$\\boxed{\\begin{pmatrix}\n\\tilde{\\beta}_{1}\\,\\dfrac{\\sigma_{X}^{2} + \\sigma_{U}^{2}}{\\sigma_{X}^{2}}    \\dfrac{\\sigma_{X}^{2}}{\\tilde{\\beta}_{1}\\,(\\sigma_{X}^{2} + \\sigma_{U}^{2})}\\,(\\tau - \\beta_{0})\n\\end{pmatrix}}$$"
        }
    ]
}