{
    "hands_on_practices": [
        {
            "introduction": "我们如何评估一个模型的好坏？选择损失函数是建模的核心步骤。这个选择不仅决定了我们如何找到“最佳”的预测值，更重要的是，它隐含了我们对数据噪声结构的假设。本练习将通过一个简单的例子揭示一个深刻的道理：即使不同的损失函数（如平方损失、绝对损失和Huber损失）能够得到完全相同的预测结果，它们对预测结果可靠性的推断（即标准误）却可能大相径庭。",
            "id": "3148943",
            "problem": "给定一个学习任务，其模型是一个没有输入特征的仅含截距的预测器 $f(x;\\beta)=\\beta$，观测值 $\\{y_i\\}_{i=1}^n$ 是实数值。考虑使用三种不同的经验损失来估计 $\\beta$：\n- 平方损失：$\\ell_2(y,\\beta)=\\frac{1}{2}(y-\\beta)^2$。\n- 绝对损失：$\\ell_1(y,\\beta)=|y-\\beta|$。\n- 带阈值 $\\delta0$ 的Huber损失：\n$$\n\\rho_\\delta(r)=\n\\begin{cases}\n\\frac{1}{2}r^2  \\text{if } |r|\\le \\delta, \\\\\n\\delta|r|-\\frac{1}{2}\\delta^2  \\text{if } |r|\\delta,\n\\end{cases}\n$$\n应用于残差 $r=y-\\beta$。\n对于每个损失函数 $\\ell$，设经验风险为 $R_n^\\ell(\\beta)=\\frac{1}{n}\\sum_{i=1}^n \\ell(y_i,\\beta)$。\n\n使用的基本定义：\n- 经验风险最小化 (ERM) 选择 $\\hat{\\beta}$ 以最小化指定损失的 $R_n^\\ell(\\beta)$。\n- 在位置模型中，对于平方损失，ERM 是样本均值；对于绝对损失，它是样本中位数；对于 Huber 损失，ERM 求解 $\\sum_{i=1}^n \\psi_\\delta(y_i-\\beta)=0$，其中 Huber 得分函数为\n$$\n\\psi_\\delta(r)=\n\\begin{cases}\nr  \\text{if } |r|\\le \\delta, \\\\\n\\delta\\,\\mathrm{sign}(r)  \\text{if } |r|\\delta,\n\\end{cases}\n$$\n其导数为 $\\psi'_\\delta(r)=\\mathbf{1}\\{|r|\\le \\delta\\}$。\n- 关于 $\\beta$ 的推断取决于与损失函数相关联的假定噪声模型：\n  1. 在与平方损失对齐的高斯模型下，位置参数的最大似然估计量是样本均值，其渐近标准误为 $\\sqrt{\\sigma^2/n}$，其中 $\\sigma^2$ 是方差；使用最大似然估计 $\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{\\beta})^2$ 得到插件标准误 $\\mathrm{SE}_{\\mathrm{Gauss}}=\\sqrt{\\hat{\\sigma}^2/n}$。\n  2. 在与绝对损失对齐、尺度参数为 $b$ 的拉普拉斯模型下，位置的费雪信息为 $I(\\mu)=1/b^2$，得到 $\\mathrm{SE}_{\\mathrm{Laplace}}=b/\\sqrt{n}$；使用最大似然估计 $\\hat{b}=\\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{\\beta}|$ 得到插件标准误 $\\mathrm{SE}_{\\mathrm{Laplace}}=\\hat{b}/\\sqrt{n}$。\n  3. 对于 Huber $M$-估计量，在一般条件下，其渐近方差由三明治公式 $\\mathrm{Var}(\\hat{\\beta})=\\frac{B}{nA^2}$ 给出，其中 $A=\\mathbb{E}[\\psi'_\\delta(\\varepsilon)]$ 且 $B=\\mathbb{E}[\\psi_\\delta(\\varepsilon)^2]$。经验插件估计为 $A_{\\mathrm{hat}}=\\frac{1}{n}\\sum_{i=1}^n \\psi'_\\delta(r_i)$ 和 $B_{\\mathrm{hat}}=\\frac{1}{n}\\sum_{i=1}^n \\psi_\\delta(r_i)^2$，得到 $\\mathrm{SE}_{\\mathrm{Huber}}=\\sqrt{B_{\\mathrm{hat}}}/(A_{\\mathrm{hat}}\\sqrt{n})$。\n\n任务。构建并分析一个案例：相同的经验风险最小化器 $\\hat{\\beta}$ 在三种损失下同时产生良好的经验预测，但关于 $\\beta$ 的推断因损失特定的解释和曲率而异。使用以下有限样本，它们都围绕 $0$ 对称，因此均值、中位数和 Huber 的 ERM 都在 $\\hat{\\beta}=0$ 处重合，但它们的标准误不同。对于每个测试用例，计算：\n- 平方损失下的 ERM $\\hat{\\beta}_{2}$、绝对损失下的 ERM $\\hat{\\beta}_{1}$ 和 Huber 损失下的 ERM $\\hat{\\beta}_{H}$（通过数值求解 $\\sum \\psi_\\delta(y_i-\\beta)=0$）。\n- 一个共享的最小化器 $\\hat{\\beta}_{\\mathrm{shared}}=\\hat{\\beta}_2$ 和一个布尔值检查 $E$，该检查确认 $\\hat{\\beta}_2$、$\\hat{\\beta}_1$ 和 $\\hat{\\beta}_H$ 在数值容差 $10^{-9}$ 内相等。\n- 在每种损失下，$\\hat{\\beta}_{\\mathrm{shared}}$ 处的经验风险及其与由其自身的 ERM 实现的最小经验风险的比率：\n$$\n\\mathrm{ratio}_{\\ell}=\\frac{R_n^\\ell(\\hat{\\beta}_{\\mathrm{shared}})}{\\min_\\beta R_n^\\ell(\\beta)},\n$$\n对于 $\\ell\\in\\{\\ell_2,\\ell_1,\\rho_\\delta\\}$。\n- 在 $\\hat{\\beta}_{\\mathrm{shared}}$ 处评估的插件标准误 $\\mathrm{SE}_{\\mathrm{Gauss}}$、$\\mathrm{SE}_{\\mathrm{Laplace}}$ 和 $\\mathrm{SE}_{\\mathrm{Huber}}$，以及推断离散度\n$$\nS=\\frac{\\max\\{\\mathrm{SE}_{\\mathrm{Gauss}},\\mathrm{SE}_{\\mathrm{Laplace}},\\mathrm{SE}_{\\mathrm{Huber}}\\}}{\\min\\{\\mathrm{SE}_{\\mathrm{Gauss}},\\mathrm{SE}_{\\mathrm{Laplace}},\\mathrm{SE}_{\\mathrm{Huber}}\\}}.\n$$\n解释：即使预测同样好（比率等于 $1$），由于损失特定的曲率和隐含的噪声模型，标准误也可能存在显著差异。\n\n测试套件。使用以下参数集：\n- 测试用例 1（重尾对称样本，稳健阈值）：$y=[-50,-30,-20,-10,-5,-2,-1,0,1,2,5,10,20,30,50]$, $\\delta=2$。\n- 测试用例 2（相同样本，类平方阈值）：$y=[-50,-30,-20,-10,-5,-2,-1,0,1,2,5,10,20,30,50]$, $\\delta=100$。\n- 测试用例 3（轻尾对称样本）：$y=[-2,-1,-1,0,0,0,1,1,2]$, $\\delta=1$。\n\n输出规格。你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，按顺序输出以下八个值：\n1. $E$ (布尔值),\n2. $\\mathrm{ratio}_{\\ell_2}$ (浮点数),\n3. $\\mathrm{ratio}_{\\ell_1}$ (浮点数),\n4. $\\mathrm{ratio}_{\\rho_\\delta}$ (浮点数),\n5. $\\mathrm{SE}_{\\mathrm{Gauss}}$ (浮点数),\n6. $\\mathrm{SE}_{\\mathrm{Laplace}}$ (浮点数),\n7. $\\mathrm{SE}_{\\mathrm{Huber}}$ (浮点数),\n8. $S$ (浮点数)。\n按顺序汇总三个测试用例；例如，最终输出应类似于\n$[\\text{case1\\_v1},\\dots,\\text{case1\\_v8},\\text{case2\\_v1},\\dots,\\text{case3\\_v8}]$。",
            "solution": "所提供的问题是有效的。这是一个在计算统计学领域中定义明确的练习，它科学严谨且客观。它引导用户在经验风险最小化（ERM）的背景下，使用不同的损失函数来分析预测与推断之间的关系。\n\n该问题研究一个仅含截距的模型 $f(x;\\beta) = \\beta$，使用数据 $\\{y_i\\}_{i=1}^n$。这是一个估计位置参数的基本问题。我们被要求比较由三种不同损失函数派生出的三个 $\\beta$ 的估计量：平方误差 $\\ell_2(y,\\beta)=\\frac{1}{2}(y-\\beta)^2$、绝对误差 $\\ell_1(y,\\beta)=|y-\\beta|$ 和 Huber 损失 $\\rho_\\delta(y-\\beta)$。\n\n问题的构造方式使得数据样本围绕 $0$ 对称。这是一个刻意的设计选择。对于对称样本，样本均值（$\\ell_2$ 的 ERM）、样本中位数（$\\ell_1$ 的 ERM）和 Huber M-估计量（$\\rho_\\delta$ 的 ERM）都在对称点重合，本例中即为 $\\beta=0$。这可以被正式证明：\n- 一组围绕 $0$ 对称的数的样本均值为 $0$。\n- 一组围绕 $0$ 对称的数的样本中位数为 $0$。\n- Huber M-估计量 $\\hat{\\beta}_H$ 是方程 $\\sum_{i=1}^n \\psi_\\delta(y_i - \\beta) = 0$ 的解。得分函数 $\\psi_\\delta(r)$ 是奇函数，意味着 $\\psi_\\delta(-r) = -\\psi_\\delta(r)$。对于一个围绕 $0$ 对称的样本 $\\{y_i\\}$，总和 $\\sum_i \\psi_\\delta(y_i - 0) = \\sum_i \\psi_\\delta(y_i)$ 等于 $0$，因为对于样本中的每个 $y_i$，$-y_i$ 也存在，它们的贡献 $\\psi_\\delta(y_i)$ 和 $\\psi_\\delta(-y_i)$ 会相互抵消。因此，$\\hat{\\beta}_H=0$ 是唯一解。\n\n这种设置确保了三种方法的估计参数 $\\hat{\\beta}$ 都相同（为 $0$）。因此，在所有情况下，预测模型 $f(x) = \\hat{\\beta}$ 都是相同的。在这个共同的最小化器上评估的经验风险 $R_n^\\ell(\\hat{\\beta}_{\\mathrm{shared}})$ 因此是可实现的最小风险，使得比率 $\\mathrm{ratio}_{\\ell}$ 都等于 $1$。这表明，从纯粹的预测角度（最小化经验风险）来看，这三种损失函数在这些特定数据集上导致了相同的结果。\n\n然而，问题的核心在于将这种预测等价性与推断差异进行对比。关于 $\\beta$ 的推断，特别是其不确定性，由估计量 $\\hat{\\beta}$ 的标准误来捕捉。标准误与（对数似然或）风险函数在其最小值处的曲率成反比。更尖锐的谷底（更高的曲率）意味着更精确的估计和更小的标准误。问题为与每种损失函数相关的标准误提供了插件公式，反映了与每种损失相关的隐含分布假设：\n1.  **平方损失 ($\\ell_2$) 和高斯噪声**：标准误 $\\mathrm{SE}_{\\mathrm{Gauss}}=\\sqrt{\\hat{\\sigma}^2/n}$，其中 $\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{\\beta})^2$，是在假定高斯噪声的情况下从最大似然理论推导出来的。样本方差 $\\hat{\\sigma}^2$ 对异常值高度敏感，因此对于重尾数据，此标准误可能很大。\n\n2.  **绝对损失 ($\\ell_1$) 和拉普拉斯噪声**：标准误 $\\mathrm{SE}_{\\mathrm{Laplace}}=\\hat{b}/\\sqrt{n}$，其中 $\\hat{b}=\\frac{1}{n}\\sum_{i=1}^n |y_i-\\hat{\\beta}|$，对应于拉普拉斯噪声模型。平均绝对偏差 $\\hat{b}$ 比方差对异常值更不敏感，使得该估计量更具稳健性。\n\n3.  **Huber 损失和稳健 M-估计**：标准误由三明治公式 $\\mathrm{SE}_{\\mathrm{Huber}}=\\sqrt{B_{\\mathrm{hat}}}/(A_{\\mathrm{hat}}\\sqrt{n})$ 给出，其中 $A_{\\mathrm{hat}}=\\frac{1}{n}\\sum_{i=1}^n \\psi'_\\delta(r_i)$ 且 $B_{\\mathrm{hat}}=\\frac{1}{n}\\sum_{i=1}^n \\psi_\\delta(r_i)^2$。此公式更具通用性，不假设噪声模型与损失函数完美匹配。$A_{\\mathrm{hat}}$ 衡量平均曲率，而 $B_{\\mathrm{hat}}$ 衡量得分的方差。参数 $\\delta$ 控制权衡：对于大的 $\\delta$，Huber 损失近似于 $\\ell_2$ 损失；而对于小的 $\\delta$，对于大的残差，其行为更像 $\\ell_1$ 损失，从而赋予其稳健性。\n\n我们现在继续对每个测试用例进行计算。对于所有用例，$\\hat{\\beta}_2 = \\hat{\\beta}_1 = \\hat{\\beta}_H = 0$，因此 $\\hat{\\beta}_{\\mathrm{shared}}=0$，布尔检查 $E$ 为真，并且所有风险比率都为 $1$。分析将集中在标准误上。\n\n**测试用例 1**：$y=[-50,-30,-20,-10,-5,-2,-1,0,1,2,5,10,20,30,50]$, $\\delta=2$。\n该样本是对称且重尾的。当 $\\hat{\\beta}_{\\mathrm{shared}}=0$ 且 $n=15$ 时：\n- $\\mathrm{SE}_{\\mathrm{Gauss}}$: $\\hat{\\sigma}^2 = \\frac{1}{15}\\sum y_i^2 = 524$。 $\\mathrm{SE}_{\\mathrm{Gauss}} = \\sqrt{524/15} \\approx 5.910$。\n- $\\mathrm{SE}_{\\mathrm{Laplace}}$: $\\hat{b} = \\frac{1}{15}\\sum |y_i| = 236/15 \\approx 15.733$。 $\\mathrm{SE}_{\\mathrm{Laplace}} = \\hat{b}/\\sqrt{15} \\approx 4.062$。\n- $\\mathrm{SE}_{\\mathrm{Huber}}$: 当 $\\delta=2$ 时，只有绝对值 $\\le 2$ 的残差被二次处理。 $A_{\\mathrm{hat}} = \\frac{1}{15}\\sum \\mathbf{1}\\{|y_i|\\le 2\\} = 5/15 = 1/3$。 $B_{\\mathrm{hat}} = \\frac{1}{15}\\sum \\psi_2(y_i)^2 = 50/15 = 10/3$。 $\\mathrm{SE}_{\\mathrm{Huber}} = \\sqrt{10/3}/((1/3)\\sqrt{15}) = \\sqrt{2} \\approx 1.414$。\n推断离散度为 $S = 5.910 / 1.414 \\approx 4.179$。如此大的离散度凸显了假定的噪声模型对推断的深远影响。带有小 $\\delta$ 的 Huber 估计量的稳健性从其显著更小的标准误中可见一斑。\n\n**测试用例 2**：与用例 1 相同的 $y$，但 $\\delta=100$。\n由于所有数据点 $|y_i|$ 都小于 $\\delta=100$，对于此样本，Huber 损失的行为与平方损失完全相同。\n- $\\mathrm{SE}_{\\mathrm{Gauss}}$ 和 $\\mathrm{SE}_{\\mathrm{Laplace}}$ 不变：$\\approx 5.910$ 和 $\\approx 4.062$。\n- $\\mathrm{SE}_{\\mathrm{Huber}}$: $A_{\\mathrm{hat}} = \\frac{1}{15}\\sum \\mathbf{1}\\{|y_i|\\le 100\\} = 1$。 $B_{\\mathrm{hat}} = \\frac{1}{15}\\sum \\psi_{100}(y_i)^2 = \\frac{1}{15}\\sum y_i^2 = 524$。 $\\mathrm{SE}_{\\mathrm{Huber}} = \\sqrt{524}/(1 \\cdot \\sqrt{15}) = \\mathrm{SE}_{\\mathrm{Gauss}} \\approx 5.910$。\n离散度为 $S = 5.910/4.062 \\approx 1.455$。正如预期的那样，大的 $\\delta$ 使 Huber 估计量的行为类似于均值，其标准误估计收敛于基于高斯模型的估计。\n\n**测试用例 3**：$y=[-2,-1,-1,0,0,0,1,1,2]$, $\\delta=1$。\n此样本是轻尾的（平峰态）。这里 $n=9$。\n- $\\mathrm{SE}_{\\mathrm{Gauss}}$: $\\hat{\\sigma}^2 = \\frac{1}{9}\\sum y_i^2 = 12/9 = 4/3$。 $\\mathrm{SE}_{\\mathrm{Gauss}} = \\sqrt{(4/3)/9} = \\sqrt{4/27} \\approx 0.385$。\n- $\\mathrm{SE}_{\\mathrm{Laplace}}$: $\\hat{b} = \\frac{1}{9}\\sum |y_i| = 8/9$。 $\\mathrm{SE}_{\\mathrm{Laplace}} = (8/9)/\\sqrt{9} = 8/27 \\approx 0.296$。\n- $\\mathrm{SE}_{\\mathrm{Huber}}$: 当 $\\delta=1$ 时, $A_{\\mathrm{hat}} = \\frac{1}{9}\\sum \\mathbf{1}\\{|y_i|\\le 1\\} = 7/9$。 $B_{\\mathrm{hat}} = \\frac{1}{9}\\sum \\psi_1(y_i)^2 = 6/9 = 2/3$。 $\\mathrm{SE}_{\\mathrm{Huber}} = \\sqrt{2/3}/((7/9)\\sqrt{9}) = \\sqrt{6}/7 \\approx 0.350$。\n离散度为 $S = 0.385/0.296 \\approx 1.299$。对于这种表现良好、轻尾的数据，所有三种方法的标准误都非常接近，表明底层建模假设之间的冲突较小。\n\n总之，这些测试用例有效地证明了，虽然不同的损失函数在经过筛选的数据上可能产生相同的点估计，从而具有相同的预测性能，但推断结论（即估计的不确定性）可能存在显著差异。这凸显了损失函数的关键作用，它不仅在于找到一个“最佳”参数，还在于定义其精度的度量。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases, computes the required metrics,\n    and prints the formatted output.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def loss_huber_vec(r, delta):\n        \"\"\"Vectorized Huber loss function.\"\"\"\n        abs_r = np.abs(r)\n        mask = abs_r = delta\n        loss = np.zeros_like(r, dtype=float)\n        loss[mask] = 0.5 * r[mask]**2\n        loss[~mask] = delta * abs_r[~mask] - 0.5 * delta**2\n        return loss\n\n    def huber_score_vec(r, delta):\n        \"\"\"Vectorized Huber score function.\"\"\"\n        score = np.clip(r, -delta, delta)\n        return score\n\n    def huber_score_deriv_vec(r, delta):\n        \"\"\"Vectorized derivative of the Huber score function.\"\"\"\n        return (np.abs(r) = delta).astype(float)\n\n    def calculate_metrics(y, delta):\n        \"\"\"\n        Calculates all required metrics for a given data sample y and delta.\n        \"\"\"\n        y_np = np.array(y, dtype=float)\n        n = len(y_np)\n\n        # 1. Compute ERMs\n        # ERM for squared loss (mean)\n        beta2 = np.mean(y_np)\n        \n        # ERM for absolute loss (median)\n        beta1 = np.median(y_np)\n        \n        # ERM for Huber loss (numerical root finding)\n        huber_obj_func = lambda b: np.sum(huber_score_vec(y_np - b, delta))\n        # Bracket for the root finder. The minimum must be within the data range.\n        bracket = [np.min(y_np), np.max(y_np)]\n        if bracket[0] == bracket[1]: # All data points are the same\n            bracket[0] -= 1\n            bracket[1] += 1\n        sol = root_scalar(huber_obj_func, bracket=bracket, method='brentq')\n        betaH = sol.root\n\n        # 2. Shared minimizer and equality check\n        beta_shared = beta2\n        E = np.allclose([beta1, betaH], beta_shared, atol=1e-9, rtol=0)\n        \n        # 3. Empirical risks and ratios\n        # Since data is symmetric, all ERMs are 0. Ratios will be 1.0.\n        # Minimal risks\n        min_risk2 = np.mean(0.5 * (y_np - beta2)**2)\n        min_risk1 = np.mean(np.abs(y_np - beta1))\n        min_riskH = np.mean(loss_huber_vec(y_np - betaH, delta))\n        \n        # Risks at shared beta\n        shared_risk2 = np.mean(0.5 * (y_np - beta_shared)**2)\n        shared_risk1 = np.mean(np.abs(y_np - beta_shared))\n        shared_riskH = np.mean(loss_huber_vec(y_np - beta_shared, delta))\n        \n        ratio2 = shared_risk2 / min_risk2 if min_risk2  0 else 1.0\n        ratio1 = shared_risk1 / min_risk1 if min_risk1  0 else 1.0\n        ratioH = shared_riskH / min_riskH if min_riskH  0 else 1.0\n        \n        # 4. Standard Errors at beta_shared\n        r_shared = y_np - beta_shared\n        \n        # SE_Gauss\n        sigma2_hat = np.mean(r_shared**2)\n        se_gauss = np.sqrt(sigma2_hat / n)\n        \n        # SE_Laplace\n        b_hat = np.mean(np.abs(r_shared))\n        se_laplace = b_hat / np.sqrt(n)\n        \n        # SE_Huber\n        A_hat = np.mean(huber_score_deriv_vec(r_shared, delta))\n        B_hat = np.mean(huber_score_vec(r_shared, delta)**2)\n        \n        if A_hat  0:\n            se_huber = np.sqrt(B_hat) / (A_hat * np.sqrt(n))\n        else: # Should not happen with given data\n            se_huber = np.inf\n            \n        # 5. Inference Spread\n        ses = [se_gauss, se_laplace, se_huber]\n        S = np.max(ses) / np.min(ses) if np.min(ses)  0 else np.inf\n        \n        return [E, ratio2, ratio1, ratioH, se_gauss, se_laplace, se_huber, S]\n\n    # --- Test Cases ---\n    test_cases = [\n        ([-50, -30, -20, -10, -5, -2, -1, 0, 1, 2, 5, 10, 20, 30, 50], 2.0),\n        ([-50, -30, -20, -10, -5, -2, -1, 0, 1, 2, 5, 10, 20, 30, 50], 100.0),\n        ([-2, -1, -1, 0, 0, 0, 1, 1, 2], 1.0)\n    ]\n\n    # --- Main Logic ---\n    all_results = []\n    for y_data, delta_val in test_cases:\n        case_results = calculate_metrics(y_data, delta_val)\n        all_results.extend(case_results)\n\n    # --- Final Output ---\n    # Python's default str() for bools ('True', 'False') is acceptable.\n    formatted_results = [f\"{val}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在实际数据分析中，我们常常面临一个两难选择：是否要对数据进行变换？例如，Box-Cox变换等方法可以稳定方差，使数据更符合模型假设，从而提升预测的准确性。然而，这种为预测服务的操作，却可能让我们在原始、有意义的尺度上解释模型参数变得异常复杂。本练习将引导你亲手量化这种变换带来的影响，计算在预测和推断中都必须进行的偏差校正，从而让你深刻体会这一权衡。",
            "id": "3148926",
            "problem": "本题要求您在响应变量经过变换以稳定方差时，对模型预测和模型推断进行比较。考虑应用于严格为正的响应变量的 Box–Cox 变换族。设该变换对于任意实参数 $\\lambda$ 定义如下：\n$$\nT_{\\lambda}(y) =\n\\begin{cases}\n\\dfrac{y^{\\lambda}-1}{\\lambda},  \\lambda \\neq 0, \\\\\n\\log(y),  \\lambda = 0,\n\\end{cases}\n$$\n其反向映射 $g_{\\lambda}(z) = T_{\\lambda}^{-1}(z)$ 由下式给出：\n$$\ng_{\\lambda}(z) =\n\\begin{cases}\n(\\lambda z + 1)^{1/\\lambda},  \\lambda \\neq 0, \\\\\n\\exp(z),  \\lambda = 0.\n\\end{cases}\n$$\n假设在变换后的尺度上存在一个线性回归模型：\n$$\nZ \\equiv T_{\\lambda}(Y) \\mid X \\sim \\text{Normal}(\\mu, \\sigma^2),\n$$\n其中 $\\mu = X^{\\top}\\beta$，$\\sigma^2  0$ 是变换后尺度上的条件方差。您的任务是通过一个程序，实现以下几个量，用以对比在原始尺度 $Y$ 上的预测和推断：\n- 对于预测：计算朴素反变换预测 $g_{\\lambda}(\\mu)$ 和通过二阶展开考虑了反变换偏差的偏差校正预测。\n- 对于推断：计算单个回归量 $x_j$ 对原始尺度条件均值的边际效应，首先通过对 $g_{\\lambda}$ 使用链式法则进行朴素计算，然后使用二阶展开进行方差感知校正。\n\n使用以下基本依据：\n- 上述 Box–Cox 定义。\n- 一个二次可微函数 $g$ 在 $m$ 附近的二阶泰勒展开：\n$$\ng(Z) \\approx g(m) + g'(m)(Z-m) + \\tfrac{1}{2}g''(m)(Z-m)^2,\n$$\n并且，对于一个随机变量 $Z$，其期望 $\\mathbb{E}[Z]=m$ 且方差 $\\operatorname{Var}(Z)=s^2$，可推得\n$$\n\\mathbb{E}[g(Z)] \\approx g(m) + \\tfrac{1}{2} g''(m) s^2.\n$$\n- 微分链式法则。\n\n基于这些依据，不引入任何未经证明的额外公式，为每个带有参数 $(\\lambda,\\mu,\\sigma^2,\\beta_j)$ 的测试用例推导出以下程序输出：\n1. 原始尺度上的朴素反变换预测，即忽略方差时的 $\\mathbb{E}[Y \\mid X]$ 值：一个等于 $g_{\\lambda}(\\mu)$ 的浮点数。\n2. 使用二阶展开在原始尺度上的偏差校正预测：一个等于 $g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\,\\sigma^2$ 的浮点数。\n3. $x_j$ 对原始尺度条件均值的朴素边际效应，仅使用链式法则和系数 $\\beta_j$：一个等于 $\\beta_j\\, g_{\\lambda}'(\\mu)$ 的浮点数。\n4. 使用二阶展开的 $x_j$ 对原始尺度条件均值的方差感知校正边际效应：一个等于 $\\beta_j \\big(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\big)$ 的浮点数。\n\n实现细节和约束：\n- 根据定义 $g_{0}(z)=\\exp(z)$ 处理 $\\lambda = 0$ 的情况。对于 $\\lambda=0$ 时 $g_{\\lambda}$ 的导数，使用指数函数的相应导数。\n- 对于所有其他实数 $\\lambda$，反函数 $g_{\\lambda}(z)$ 仅在 $\\lambda z + 1  0$ 时有定义；所有提供的测试用例在求值点 $z=\\mu$ 处均满足此条件。\n- 不涉及物理单位。所有输出均为实值浮点数。\n- 不使用角度。\n- 分数和小数均可接受；不要使用百分号。\n\n测试套件：\n提供以下四组参数集的结果，每组参数写为 $(\\lambda,\\mu,\\sigma^2,\\beta_j)$：\n- 案例 1：$(0,\\, 0.0,\\, 0.04,\\, 0.2)$。\n- 案例 2：$(0.5,\\, 1.5,\\, 0.36,\\, 1.0)$。\n- 案例 3：$(1.0,\\, 3.0,\\, 2.0,\\, 2.0)$。\n- 案例 4：$(-0.5,\\, 1.0,\\, 0.25,\\, 0.5)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个案例的结果，形式为列表的列表，每个内层列表对应一个案例，并按 $[\\text{朴素预测}, \\text{偏差校正预测}, \\text{朴素边际效应}, \\text{偏差校正边际效应}]$ 的顺序排列。\n例如，程序必须打印一个形如\n$[[a_{11},a_{12},a_{13},a_{14}],[a_{21},a_{22},a_{23},a_{24}],[a_{31},a_{32},a_{33},a_{34}],[a_{41},a_{42},a_{43},a_{44}]]$\n的字符串，其中为上述测试套件填入数值，且不含任何额外文本。",
            "solution": "我们从指定的定义和原则出发。Box–Cox 反函数 $g_{\\lambda}(z)$ 为\n$$\ng_{\\lambda}(z) =\n\\begin{cases}\n(\\lambda z + 1)^{1/\\lambda},  \\lambda \\neq 0, \\\\\n\\exp(z),  \\lambda = 0.\n\\end{cases}\n$$\n我们假设变换后的响应 $Z = T_{\\lambda}(Y)$ 满足 $Z \\mid X \\sim \\text{Normal}(\\mu, \\sigma^2)$，其中 $\\mu = X^{\\top}\\beta$ 且 $\\sigma^2  0$。\n\n目标1（预测）：我们的目标是计算 $\\mathbb{E}[Y \\mid X] = \\mathbb{E}[g_{\\lambda}(Z) \\mid X]$。朴素反变换忽略了 $Z$ 的随机性，并设\n$$\n\\text{朴素预测} = g_{\\lambda}(\\mu).\n$$\n为了考虑非线性引起的反变换偏差，我们使用 $g_{\\lambda}(Z)$ 在 $m=\\mu$ 附近的二阶泰勒展开：\n$$\ng_{\\lambda}(Z) \\approx g_{\\lambda}(\\mu) + g_{\\lambda}'(\\mu)(Z-\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu) (Z-\\mu)^2.\n$$\n在给定 $X$ 的条件下取条件期望，并利用 $\\mathbb{E}[Z-\\mu \\mid X]=0$ 和 $\\mathbb{E}[(Z-\\mu)^2 \\mid X] = \\sigma^2$，得到二阶 delta 方法近似\n$$\n\\mathbb{E}[Y \\mid X] = \\mathbb{E}[g_{\\lambda}(Z) \\mid X] \\approx g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2.\n$$\n因此，偏差校正的预测为\n$$\n\\text{偏差校正预测} \\approx g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2.\n$$\n\n目标2（推断）：单个回归量 $x_j$ 对原始尺度条件均值的边际效应是偏导数 $\\dfrac{\\partial}{\\partial x_j} \\mathbb{E}[Y \\mid X]$。首先，朴素地将 $\\mathbb{E}[Y \\mid X]$ 近似为 $g_{\\lambda}(\\mu)$，根据链式法则和 $\\mu = X^{\\top}\\beta$，我们得到\n$$\n\\text{朴素边际效应} = \\dfrac{\\partial}{\\partial x_j} g_{\\lambda}(\\mu) = g_{\\lambda}'(\\mu) \\cdot \\dfrac{\\partial \\mu}{\\partial x_j} = g_{\\lambda}'(\\mu) \\cdot \\beta_j.\n$$\n接下来，通过对条件均值使用相同的二阶近似来纳入方差，\n$$\n\\mathbb{E}[Y \\mid X] \\approx g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2,\n$$\n我们对 $x_j$ 求导并再次使用链式法则。由于在此设置中 $\\sigma^2$ 被视为相对于 $x_j$ 是常数，我们得到\n$$\n\\dfrac{\\partial}{\\partial x_j} \\mathbb{E}[Y \\mid X] \\approx g_{\\lambda}'(\\mu)\\, \\beta_j + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\, \\beta_j = \\beta_j\\left(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\right).\n$$\n因此，\n$$\n\\text{偏差校正的边际效应} \\approx \\beta_j\\left(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\right).\n$$\n\n我们现在计算上面所需的 $g_{\\lambda}$ 的导数。对于 $\\lambda \\neq 0$，$g_{\\lambda}$ 可以写成 $g_{\\lambda}(z) = (\\lambda z + 1)^{K}$，其中 $K = 1/\\lambda$。对一个仿射函数的幂进行重复求导，对于任意实数 $k$：\n$$\n\\dfrac{d}{dz} (a z + b)^k = k a (a z + b)^{k-1}, \\quad\n\\dfrac{d^2}{dz^2} (a z + b)^k = k (k-1) a^2 (a z + b)^{k-2}, \\quad\n\\dfrac{d^3}{dz^3} (a z + b)^k = k (k-1) (k-2) a^3 (a z + b)^{k-3}.\n$$\n设 $a=\\lambda$, $b=1$, 且 $k=K=1/\\lambda$，我们得到对于 $\\lambda \\neq 0$ 的情况：\n$$\ng_{\\lambda}'(z) = (\\lambda z + 1)^{\\frac{1}{\\lambda}-1},\n$$\n$$\ng_{\\lambda}''(z) = \\left(\\frac{1}{\\lambda}-1\\right)\\lambda\\, (\\lambda z + 1)^{\\frac{1}{\\lambda}-2},\n$$\n$$\ng_{\\lambda}'''(z) = \\left(\\frac{1}{\\lambda}-1\\right)\\left(\\frac{1}{\\lambda}-2\\right)\\lambda^2\\, (\\lambda z + 1)^{\\frac{1}{\\lambda}-3}.\n$$\n对于 $\\lambda = 0$，我们有 $g_{0}(z) = \\exp(z)$，因此\n$$\ng_{0}'(z) = \\exp(z), \\quad g_{0}''(z) = \\exp(z), \\quad g_{0}'''(z) = \\exp(z).\n$$\n\n程序算法设计：\n- 对于每个案例 $(\\lambda,\\mu,\\sigma^2,\\beta_j)$，使用适当的分支（$\\lambda=0$ 或 $\\lambda \\neq 0$）计算 $g_{\\lambda}(\\mu)$, $g_{\\lambda}''(\\mu)$ 以及 $g_{\\lambda}'(\\mu)$, $g_{\\lambda}'''(\\mu)$。\n- 计算每个案例所需的四个输出：\n  - 朴素预测：$g_{\\lambda}(\\mu)$。\n  - 偏差校正预测：$g_{\\lambda}(\\mu) + \\tfrac{1}{2} g_{\\lambda}''(\\mu)\\, \\sigma^2$。\n  - 朴素边际效应：$\\beta_j\\, g_{\\lambda}'(\\mu)$。\n  - 偏差校正的边际效应：$\\beta_j\\left(g_{\\lambda}'(\\mu) + \\tfrac{1}{2} g_{\\lambda}'''(\\mu)\\, \\sigma^2\\right)$。\n- 将所有测试用例的输出按指定顺序汇集成一个列表的列表，并将其打印在单行上，不含额外文本。\n\n测试套件覆盖范围：\n- 案例 $1$ 使用 $\\lambda = 0$（对数变换），运用了特殊分支，并说明了反变换偏差如何通过指数函数依赖于 $\\sigma^2$。\n- 案例 $2$ 使用 $\\lambda = 0.5$ 以及中等的 $\\mu$ 和 $\\sigma^2$，在典型的方差稳定化设置中测试一般 Box–Cox 导数。\n- 案例 $3$ 使用 $\\lambda = 1.0$（在一个位移内是恒等变换），此时 $g_{\\lambda}''(\\mu)=0$ 且 $g_{\\lambda}'''(\\mu)=0$，产生零偏差校正，边际效应等于 $\\beta_j$，这是一个有用的边界检查。\n- 案例 $4$ 使用 $\\lambda = -0.5$ 且满足 $\\lambda \\mu + 1  0$，测试了负参数的行为，并确认了在求值点上对定义域的正确处理。\n\n相对于模型推断与模型预测的解释：\n- 原始尺度上的预测受益于变换后尺度上的方差稳定化，但在反变换时需要进行偏差校正以近似 $\\mathbb{E}[Y \\mid X]$；当 $g_{\\lambda}$ 在 $\\mu$ 处是凸函数时（例如指数函数情况），朴素的 $g_{\\lambda}(\\mu)$ 会低估原始均值。\n- 原始尺度上的推断变得复杂，因为边际效应通过 $g_{\\lambda}'(\\mu)$ 和 $g_{\\lambda}'''(\\mu)$ 依赖于 $\\mu$ 和 $\\sigma^2$；因此，变换后尺度上的单个系数 $\\beta_j$ 不会转化为原始尺度上的一个恒定效应，并且方差感知的推断会引入额外的曲率项。\n\n程序直接实现了这些推导，为指定的测试套件生成所需的数值输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef g_lambda(z: float, lam: float) - float:\n    \"\"\"Inverse Box-Cox transformation g_lambda(z).\"\"\"\n    if abs(lam)  1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    # Domain check is assumed valid at evaluation point by problem statement.\n    return math.pow(base, 1.0 / lam)\n\ndef g1_lambda(z: float, lam: float) - float:\n    \"\"\"First derivative g'(z) for the inverse Box-Cox.\"\"\"\n    if abs(lam)  1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    return math.pow(base, (1.0 / lam) - 1.0)\n\ndef g2_lambda(z: float, lam: float) - float:\n    \"\"\"Second derivative g''(z) for the inverse Box-Cox.\"\"\"\n    if abs(lam)  1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    return ((1.0 / lam) - 1.0) * lam * math.pow(base, (1.0 / lam) - 2.0)\n\ndef g3_lambda(z: float, lam: float) - float:\n    \"\"\"Third derivative g'''(z) for the inverse Box-Cox.\"\"\"\n    if abs(lam)  1e-12:\n        return math.exp(z)\n    base = lam * z + 1.0\n    return ((1.0 / lam) - 1.0) * ((1.0 / lam) - 2.0) * (lam ** 2) * math.pow(base, (1.0 / lam) - 3.0)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (lambda, mu, sigma2, beta_j)\n    test_cases = [\n        (0.0, 0.0, 0.04, 0.2),    # Case 1\n        (0.5, 1.5, 0.36, 1.0),    # Case 2\n        (1.0, 3.0, 2.0, 2.0),     # Case 3\n        (-0.5, 1.0, 0.25, 0.5),   # Case 4\n    ]\n\n    results = []\n    for lam, mu, sigma2, beta_j in test_cases:\n        g = g_lambda(mu, lam)\n        g1 = g1_lambda(mu, lam)\n        g2 = g2_lambda(mu, lam)\n        g3 = g3_lambda(mu, lam)\n\n        naive_pred = g\n        bias_corr_pred = g + 0.5 * g2 * sigma2\n\n        naive_effect = beta_j * g1\n        bias_corr_effect = beta_j * (g1 + 0.5 * g3 * sigma2)\n\n        results.append([naive_pred, bias_corr_pred, naive_effect, bias_corr_effect])\n\n    # Final print statement in the exact required format.\n    # Print a single line JSON-like list of lists with full-precision floats.\n    def fmt(x):\n        # Use repr-like formatting for better precision consistency\n        return format(x, '.15g')\n    line = \"[\" + \",\".join(\"[\" + \",\".join(fmt(v) for v in row) + \"]\" for row in results) + \"]\"\n    print(line)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在概率模型中，“不确定性”本身也分为不同类型。我们可以讨论“预测不确定性”（对一个新数据点的分类有多大把握？），也可以讨论“推断不确定性”（我们对模型本身的参数有多大把握？）。本练习将带你深入探索这两种不确定性的区别。通过在一个正则化的逻辑回归模型中亲手实现参数估计，你将发现这两种不确定性是截然不同的概念，而像正则化这样的建模决策会以不同甚至相反的方式影响它们。",
            "id": "3149003",
            "problem": "在统计学习的二元分类背景下，您需要仔细比较模型预测和模型推断。考虑一个带有逻辑斯蒂链接的伯努利条件模型：对于给定的特征向量 $x \\in \\mathbb{R}^d$ 和参数向量 $\\theta \\in \\mathbb{R}^d$，可观察的标签 $y \\in \\{0,1\\}$ 通过条件概率 $p(y=1 \\mid x,\\theta) = \\sigma(\\theta^\\top x)$ 进行建模，其中逻辑斯蒂S型函数 $\\sigma(z)$ 由基本关系 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ 定义。对于一个拟合参数 $\\hat{\\theta}$，在点 $x$ 处的预测熵由香农熵给出，以自然单位（奈特）计算，$H(x) = -\\hat{p}(y=1 \\mid x)\\ln \\hat{p}(y=1 \\mid x) - \\hat{p}(y=0 \\mid x)\\ln \\hat{p}(y=0 \\mid x)$。\n\n为了估计参数向量，使用最大后验（MAP）估计，该估计基于一个精度为 $\\lambda  0$ 的零均值各向同性高斯先验，这意味着先验密度与 $\\exp\\!\\left(-\\frac{\\lambda}{2}\\|\\theta\\|_2^2\\right)$ 成正比。训练数据为数据对 $(x_i,y_i)$，其中 $i=1,\\dots,n$，$y_i \\in \\{0,1\\}$ 且 $x_i \\in \\mathbb{R}^d$。参数 $\\theta$ 的后验分布与似然和先验的乘积成正比。仅从独立样本的伯努利似然、逻辑斯蒂链接和高斯先验的核心定义出发，推导对数后验的梯度和Hessian矩阵。然后，实现一个迭代二阶方法来寻找对数后验的一个驻点 $\\hat{\\theta}$（即MAP估计）。在 $\\hat{\\theta}$ 处使用对数后验的局部二次近似，将推断协方差矩阵 $\\widehat{\\Sigma}$ 定义为在 $\\hat{\\theta}$ 处的负Hessian矩阵的逆。您必须报告的推断方差度量是 $\\widehat{\\Sigma}$ 的迹，记为 $\\operatorname{tr}(\\widehat{\\Sigma})$，它汇总了各参数的不确定性。对于一个查询点 $x^\\star$ 的预测，计算 $\\hat{p}(y=1 \\mid x^\\star) = \\sigma(\\hat{\\theta}^\\top x^\\star)$ 及其使用自然对数的熵 $H(x^\\star)$。总而言之，对于每个测试用例，您必须输出由预测熵和推断方差迹组成的对，即 $[H(x^\\star), \\operatorname{tr}(\\widehat{\\Sigma})]$。\n\n您的程序必须在没有任何外部输入的情况下实现以下内容：\n\n- 使用固定的训练集，其中 $n = 10$ 且 $d = 3$（包括一个截距项）。特征向量的第一个分量为截距，其值等于 $1$。正类（$y_i = 1$）有五个点，其特征 $(x_{i,1}, x_{i,2}, x_{i,3})$ 分别为 $(1, 2.0, 2.0)$、$(1, 2.5, 1.5)$、$(1, 1.7, 2.3)$、$(1, 2.2, 1.8)$、$(1, 1.8, 2.1)$。负类（$y_i = 0$）有五个点，其特征 $(x_{i,1}, x_{i,2}, x_{i,3})$ 分别为 $(1, -2.0, -2.0)$、$(1, -2.5, -1.5)$、$(1, -1.7, -2.3)$、$(1, -2.2, -1.8)$、$(1, -1.8, -2.1)$。\n\n- 仅从定义出发，通过使用对数后验的梯度和Hessian矩阵的迭代二阶上升过程，正确推导并实现MAP估计。通过在每次迭代中检查参数更新的范数，确保收敛到一个驻点，并在该范数低于一个小的容差时停止。\n\n- 在MAP估计值 $\\hat{\\theta}$ 处，使用局部二次近似来形成推断协方差 $\\widehat{\\Sigma}$，其为在 $\\hat{\\theta}$ 处的负Hessian矩阵的逆，并计算 $\\operatorname{tr}(\\widehat{\\Sigma})$。\n\n- 对于每个查询，使用自然对数（奈特）计算预测概率 $\\hat{p}(y=1 \\mid x^\\star)$ 和预测熵 $H(x^\\star)$。\n\n通过适当选择先验精度 $\\lambda$ 和查询点 $x^\\star$，设计并分析预测熵增加而推断方差减少的条件。规定了以下四个测试用例来探究不同方面：\n\n- 测试用例A（理想路径：低正则化，置信预测）：先验精度 $\\lambda = 0.1$，查询向量 $x^\\star$ 的分量等于 $1, 2.5, 2.0$。\n\n- 测试用例B（增加正则化以收缩参数）：先验精度 $\\lambda = 20.0$，查询向量 $x^\\star$ 的分量等于 $1, 2.5, 2.0$。\n\n- 测试用例C（边界情况：非截距特征为零）：先验精度 $\\lambda = 0.1$，查询向量 $x^\\star$ 的分量等于 $1, 0.0, 0.0$。\n\n- 测试用例D（边界压力：强正则化，大查询幅度）：先验精度 $\\lambda = 50.0$，查询向量 $x^\\star$ 的分量等于 $1, 6.0, 6.0$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个结果本身都是一个双元素浮点数列表 $[H(x^\\star), \\operatorname{tr}(\\widehat{\\Sigma})]$，并按此顺序排列。例如，打印输出必须类似于 $[[h_1,v_1],[h_2,v_2],[h_3,v_3],[h_4,v_4]]$，其中的 $h_j$ 和 $v_j$ 是由您的程序计算出的实际数值。",
            "solution": "该问题要求在贝叶斯逻辑斯蒂回归框架内比较模型预测和模型推断。这包括推导和实现模型参数的最大后验（MAP）估计，然后计算两个不同的度量：一个预测不确定性度量（熵）和一个推断不确定性度量（参数协方差矩阵的迹）。\n\n解决方案分为四个阶段：\n1.  根据指定的似然和先验构建对数后验函数。\n2.  推导对数后验的梯度和Hessian矩阵，这对于优化是必需的。\n3.  指定用于寻找MAP估计 $\\hat{\\theta}$ 的二阶优化算法（Newton-Raphson）。\n4.  定义要计算的预测和推断不确定性度量。\n\n**1. 对数后验的构建**\n\n对于给定的特征向量 $x_i \\in \\mathbb{R}^d$，二元标签 $y_i \\in \\{0, 1\\}$ 的模型是伯努利分布，其概率为 $p_i = p(y_i=1 \\mid x_i, \\theta) = \\sigma(\\theta^\\top x_i)$，其中 $\\sigma(z) = (1+e^{-z})^{-1}$ 是逻辑斯蒂S型函数。\n\n单个观测值 $(x_i, y_i)$ 的似然为 $p(y_i \\mid x_i, \\theta) = p_i^{y_i} (1-p_i)^{1-y_i}$。对数似然为 $\\ln p(y_i \\mid x_i, \\theta) = y_i \\ln p_i + (1-y_i) \\ln(1-p_i)$。使用恒等式 $\\ln p_i = \\ln \\sigma(\\theta^\\top x_i) = -\\ln(1+e^{-\\theta^\\top x_i})$ 和 $\\ln(1-p_i) = \\ln(1-\\sigma(\\theta^\\top x_i)) = -\\theta^\\top x_i - \\ln(1+e^{-\\theta^\\top x_i})$，我们可以将对数似然重写为：\n$$\n\\ln p(y_i \\mid x_i, \\theta) = y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i})\n$$\n对于一个包含 $n$ 个独立样本的数据集 $D = \\{(x_i, y_i)\\}_{i=1}^n$，总对数似然是所有样本的总和：\n$$\nLL(\\theta) = \\sum_{i=1}^n \\left( y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i}) \\right)\n$$\n该问题指定参数向量 $\\theta$ 上的一个零均值各向同性高斯先验，其精度为 $\\lambda  0$。先验密度为 $p(\\theta) \\propto \\exp\\left(-\\frac{\\lambda}{2}\\|\\theta\\|_2^2\\right)$。对数先验（不计加法常数）为：\n$$\n\\ln p(\\theta) = -\\frac{\\lambda}{2} \\theta^\\top \\theta\n$$\n我们旨在为MAP估计最大化的对数后验函数 $\\mathcal{L}(\\theta)$，是对数似然和对数先验的和：\n$$\n\\mathcal{L}(\\theta) = \\ln p(\\theta \\mid D) \\propto LL(\\theta) + \\ln p(\\theta) = \\left( \\sum_{i=1}^n \\left( y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i}) \\right) \\right) - \\frac{\\lambda}{2} \\theta^\\top \\theta\n$$\n\n**2. 对数后验的梯度和Hessian矩阵**\n\n为了优化 $\\mathcal{L}(\\theta)$，我们计算它关于 $\\theta$ 的梯度和Hessian矩阵。\n\n对数后验的梯度为：\n$$\n\\nabla_{\\theta} \\mathcal{L}(\\theta) = \\sum_{i=1}^n \\nabla_{\\theta}\\left( y_i(\\theta^\\top x_i) - \\ln(1+e^{\\theta^\\top x_i}) \\right) - \\nabla_{\\theta}\\left(\\frac{\\lambda}{2} \\theta^\\top \\theta\\right)\n$$\n使用链式法则，$\\nabla_{\\theta}\\ln(1+e^{\\theta^\\top x_i}) = \\frac{e^{\\theta^\\top x_i}}{1+e^{\\theta^\\top x_i}} x_i = \\sigma(\\theta^\\top x_i)x_i = p_i x_i$。单个样本的对数似然的梯度为 $(y_i - p_i)x_i$。对数先验的梯度为 $-\\lambda\\theta$。\n对所有样本求和，完整的梯度为：\n$$\n\\nabla_{\\theta} \\mathcal{L}(\\theta) = \\sum_{i=1}^n (y_i - \\sigma(\\theta^\\top x_i)) x_i - \\lambda\\theta\n$$\n在矩阵表示法中，设 $X$ 为 $n \\times d$ 的设计矩阵，$y$ 为标签向量，$p$ 为概率向量，则梯度为 $g(\\theta) = X^\\top(y-p) - \\lambda\\theta$。\n\nHessian矩阵是二阶导数矩阵，$\\nabla^2_{\\theta} \\mathcal{L}(\\theta)$。单个样本的对数似然项的二阶导数为：\n$$\n\\nabla^2_{\\theta} \\left((y_i - p_i)x_i \\right) = -x_i (\\nabla_{\\theta} p_i)^\\top = -x_i \\left( \\sigma'(\\theta^\\top x_i) x_i \\right)^\\top = - \\sigma(\\theta^\\top x_i)(1-\\sigma(\\theta^\\top x_i)) x_i x_i^\\top\n$$\n对数先验项的Hessian矩阵为 $-\\lambda I$，其中 $I$ 是 $d \\times d$ 的单位矩阵。\n对数后验的完整Hessian矩阵是：\n$$\nH(\\theta) = \\nabla^2_{\\theta} \\mathcal{L}(\\theta) = - \\sum_{i=1}^n \\sigma(\\theta^\\top x_i)(1-\\sigma(\\theta^\\top x_i)) x_i x_i^\\top - \\lambda I\n$$\n在矩阵表示法中，这是 $H(\\theta) = -X^\\top W X - \\lambda I$，其中 $W$ 是一个 $n \\times n$ 的对角矩阵，其元素为 $W_{ii} = p_i(1-p_i)$。由于 $p_i(1-p_i) \\ge 0$，矩阵 $X^\\top W X$ 是半正定的。对于 $\\lambda  0$，Hessian矩阵 $H(\\theta)$ 是严格负定的，这意味着对数后验 $\\mathcal{L}(\\theta)$ 是一个严格凹函数。这保证了唯一的全局最大值的存在，并且可以被高效地找到。\n\n**3. 通过Newton-Raphson方法进行优化**\n\n我们使用一个迭代二阶方法，特别是Newton-Raphson算法，来找到最大化 $\\mathcal{L}(\\theta)$ 的MAP估计 $\\hat{\\theta}$。从一个初始猜测 $\\theta_0$（例如，零向量）开始，参数被迭代更新：\n$$\n\\theta_{k+1} = \\theta_k - [H(\\theta_k)]^{-1} g(\\theta_k)\n$$\n其中 $g(\\theta_k)$ 和 $H(\\theta_k)$ 是在当前估计 $\\theta_k$ 处计算的梯度和Hessian矩阵。更新步长 $\\Delta\\theta_k = -[H(\\theta_k)]^{-1} g(\\theta_k)$ 通过求解线性系统 $H(\\theta_k)\\Delta\\theta_k = -g(\\theta_k)$ 得到。迭代持续进行，直到更新步长的范数 $\\|\\Delta\\theta_k\\|_2$ 小于一个小的容差。得到的参数向量就是MAP估计 $\\hat{\\theta}$。\n\n**4. 推断和预测度量**\n\n收敛到 $\\hat{\\theta}$ 后，我们计算两个指定的量：\n\n*   **推断方差：** 参数估计中的不确定性由后验协方差矩阵捕获。使用Hessian矩阵在其众数 $\\hat{\\theta}$ 处形成后验分布的局部高斯近似。该近似的协方差矩阵是在众数处计算的负Hessian矩阵的逆：\n    $$\n    \\widehat{\\Sigma} = [-H(\\hat{\\theta})]^{-1} = [X^\\top \\widehat{W} X + \\lambda I]^{-1}\n    $$\n    其中 $\\widehat{W}$ 是在 $\\hat{\\theta}$ 处计算的权重矩阵。这个矩阵是由先验正则化的观测Fisher信息矩阵。总参数不确定性的度量是该协方差矩阵的迹，即 $\\operatorname{tr}(\\widehat{\\Sigma})$。\n\n*   **预测熵：** 对于一个新的查询点 $x^\\star$，模型的预测是概率 $\\hat{p}(y=1 \\mid x^\\star) = \\sigma(\\hat{\\theta}^\\top x^\\star)$。这个单一预测的不确定性由相应伯努利分布的香农熵量化，以奈特（nats）为单位：\n    $$\n    H(x^\\star) = - \\hat{p}^\\star \\ln(\\hat{p}^\\star) - (1-\\hat{p}^\\star)\\ln(1-\\hat{p}^\\star)\n    $$\n    其中 $\\hat{p}^\\star = \\hat{p}(y=1 \\mid x^\\star)$。当预测最不确定（$\\hat{p}^\\star = 0.5$）时，熵在 $H=\\ln(2) \\approx 0.693$ 处达到最大值；当预测完全置信（$\\hat{p}^\\star = 0$ 或 $\\hat{p}^\\star = 1$）时，熵为 $0$。\n\n以下程序为指定的数据集和测试用例实现了这整个过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_entropy(p):\n    \"\"\"Calculates the Shannon entropy for a Bernoulli probability p.\"\"\"\n    if p = 1e-12 or p = 1 - 1e-12:\n        return 0.0\n    return -(p * np.log(p) + (1 - p) * np.log(1 - p))\n\ndef compute_map_estimate(X, y, lam, tol=1e-9, max_iter=100):\n    \"\"\"\n    Computes the MAP estimate for logistic regression using Newton's method.\n    \n    Args:\n        X (np.ndarray): Design matrix of shape (n, d).\n        y (np.ndarray): Target vector of shape (n,).\n        lam (float): Precision of the Gaussian prior.\n        tol (float): Convergence tolerance.\n        max_iter (int): Maximum number of iterations.\n        \n    Returns:\n        tuple: A tuple containing:\n            - theta_hat (np.ndarray): The MAP parameter estimate of shape (d,).\n            - hessian_at_map (np.ndarray): The Hessian of the log-posterior at theta_hat.\n    \"\"\"\n    n, d = X.shape\n    theta = np.zeros(d)\n\n    for i in range(max_iter):\n        # Calculate linear predictors and probabilities\n        z = X @ theta\n        p = 1 / (1 + np.exp(-z))\n\n        # Calculate gradient of the log-posterior\n        grad = X.T @ (y - p) - lam * theta\n\n        # Calculate Hessian of the log-posterior\n        W_diag = p * (1 - p)\n        # Efficiently compute X.T @ W @ X without forming a dense W\n        Hess = -X.T @ (W_diag[:, np.newaxis] * X) - lam * np.identity(d)\n\n        # Solve the Newton step: H * delta = -g\n        delta_theta = np.linalg.solve(Hess, -grad)\n\n        # Update parameters\n        theta += delta_theta\n        \n        # Check for convergence\n        if np.linalg.norm(delta_theta)  tol:\n            break\n    \n    # After convergence, one final calculation of the Hessian at the final theta\n    z_final = X @ theta\n    p_final = 1 / (1 + np.exp(-z_final))\n    W_diag_final = p_final * (1 - p_final)\n    hessian_at_map = -X.T @ (W_diag_final[:, np.newaxis] * X) - lam * np.identity(d)\n    \n    return theta, hessian_at_map\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, running all test cases.\n    \"\"\"\n    # Define the fixed training set\n    X_train = np.array([\n        # Positive class (y=1)\n        [1.0, 2.0, 2.0],\n        [1.0, 2.5, 1.5],\n        [1.0, 1.7, 2.3],\n        [1.0, 2.2, 1.8],\n        [1.0, 1.8, 2.1],\n        # Negative class (y=0)\n        [1.0, -2.0, -2.0],\n        [1.0, -2.5, -1.5],\n        [1.0, -1.7, -2.3],\n        [1.0, -2.2, -1.8],\n        [1.0, -1.8, -2.1]\n    ])\n    y_train = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n\n    # Define the prescribed test cases\n    test_cases = [\n        # (lambda, x_star)\n        (0.1, np.array([1.0, 2.5, 2.0])), # Case A\n        (20.0, np.array([1.0, 2.5, 2.0])), # Case B\n        (0.1, np.array([1.0, 0.0, 0.0])), # Case C\n        (50.0, np.array([1.0, 6.0, 6.0]))  # Case D\n    ]\n\n    results = []\n    for lam, x_star in test_cases:\n        # 1. Compute MAP estimate and Hessian at MAP\n        theta_hat, hessian_at_map = compute_map_estimate(X_train, y_train, lam)\n        \n        # 2. Compute inferential variance trace\n        # Inferential covariance is the inverse of the negative Hessian\n        neg_hessian = -hessian_at_map\n        try:\n            sigma_hat = np.linalg.inv(neg_hessian)\n            tr_sigma = np.trace(sigma_hat)\n        except np.linalg.LinAlgError:\n            tr_sigma = float('inf') # Should not happen given problem structure\n            \n        # 3. Compute predictive entropy\n        z_star = x_star @ theta_hat\n        p_star = 1 / (1 + np.exp(-z_star))\n        entropy = calculate_entropy(p_star)\n\n        # Store the pair of results\n        results.append([entropy, tr_sigma])\n    \n    # Format the final output string exactly as specified\n    formatted_results = [f\"[{h:.6f},{v:.6f}]\" for h, v in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}