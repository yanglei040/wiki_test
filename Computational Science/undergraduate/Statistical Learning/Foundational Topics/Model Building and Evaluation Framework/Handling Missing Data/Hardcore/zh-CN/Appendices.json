{
    "hands_on_practices": [
        {
            "introduction": "我们从最基本的数据插补技术——均值插补——开始。这个练习将向你展示如何用一个基因在其他样本中的平均表达量来填补该基因的缺失值，这是一个快速但需要谨慎使用的基本方法。通过这个简单的计算练习，你可以掌握最直观的缺失数据处理思路，为后续学习更复杂的方法打下基础。",
            "id": "1437187",
            "problem": "一个研究小组正在研究人类细胞对一种新型治疗化合物的转录反应。他们测量了五个关键信号基因（标记为 G1、G2、G3、G4、G5）在四个重复细胞培养物（标记为 S1、S2、S3、S4）中的表达水平。数据以归一化表达单位表示，并组织成一个矩阵，其中行对应基因，列对应重复样本。\n\n由于微阵列玻片上的一个污点，基因 G3 在重复样本 S3 中的表达值无法读取。记录的数据集如下，其中“X”表示缺失值：\n\n- **G1：** [2.15, 2.30, 2.25, 2.18]\n- **G2：** [0.88, 0.95, 0.91, 1.02]\n- **G3：** [1.54, 1.71, X, 1.60]\n- **G4：** [3.40, 3.11, 3.25, 3.32]\n- **G5：** [0.45, 0.39, 0.41, 0.44]\n\n为了在进行进一步的统计分析之前补全数据集，该团队决定采用一种基本的数据插补技术。你的任务是使用基因层面的均值插补法计算“X”的值。该技术包括将特定基因的缺失值替换为该基因在所有其他重复样本中观测到的表达值的算术平均值。\n\n计算基因 G3 在重复样本 S3 中的插补表达值。将你的最终答案四舍五入到三位有效数字。",
            "solution": "我们使用基因层面的均值插补法：用同一基因的观测值的算术平均值替换缺失值。对于基因 G3，在可用的重复样本中，观测到的表达值为 $1.54$、$1.71$ 和 $1.60$。\n\n插补值 $X$ 计算如下\n$$\nX=\\frac{1}{3}\\left(1.54+1.71+1.60\\right).\n$$\n首先将观测值相加：\n$$\n1.54+1.71=3.25,\\quad 3.25+1.60=4.85.\n$$\n除以观测值的数量：\n$$\nX=\\frac{4.85}{3}=1.616\\overline{6}.\n$$\n四舍五入到三位有效数字，得到\n$$\nX\\approx 1.62.\n$$",
            "answer": "$$\\boxed{1.62}$$"
        },
        {
            "introduction": "虽然均值插补很简单，但它可能会严重扭曲数据的内在结构，尤其是在数据分布不均匀时。通过这个思想实验，你将从理论上分析均值插补如何影响数据的方差，从而深刻理解选择合适插补方法的重要性。这个练习强调了在应用任何统计方法之前，批判性地评估其对数据分布假设的必要性。",
            "id": "1437194",
            "problem": "在一项系统生物学实验中，研究人员分析了一个大细胞群体中某个基因 `Gene-Z` 的表达情况。该基因表现出双稳态行为：比例为 $p$ 的细胞处于“低表达”状态，而剩余比例为 $1-p$ 的细胞处于“高表达”状态。我们可以通过假设来为这种理想化情景建模：处于低表达状态的每个细胞的表达水平均恰好为 $E_{\\text{low}}$，而处于高表达状态的每个细胞的表达水平均恰好为 $E_{\\text{high}}$，其中 $E_{\\text{high}} > E_{\\text{low}}$。\n\n在数据采集过程中，一次技术故障导致总数据点中比例为 $f$ 的部分丢失了。后续分析表明，所有这些丢失的测量值都完全来自于处于“高表达”状态的细胞。假设 $f  1-p$，以确保仍有部分高表达数据被观测到。\n\n一位分析师没有意识到数据的双峰性质，决定使用简单均值插补法来处理缺失值。他们计算了剩余观测数据点的算术平均值，并用这个单一的计算均值替换了每个缺失值。\n\n目标是量化这种插补方法对数据变异性造成的扭曲。计算最终插补数据集的总体方差与原始完整数据集（在任何数据丢失前）的总体方差之比。请用 $p$ 和 $f$ 来表示你的答案的符号表达式。",
            "solution": "设两个表达水平为 $E_{\\text{low}}$ 和 $E_{\\text{high}}$，其中 $E_{\\text{high}} > E_{\\text{low}}$。定义 $D = E_{\\text{high}} - E_{\\text{low}}$ 和 $a = E_{\\text{low}}$，因此 $E_{\\text{high}} = a + D$。在原始完整群体中，均值为\n$$\\mu_{\\text{orig}} = p\\,a + (1-p)(a + D) = a + (1-p)D,$$\n一个两点混合分布的总体方差为\n$$\\operatorname{Var}_{\\text{orig}} = p(1-p)D^{2}.$$\n\n所有数据点中比例为 $f$ 的部分丢失了，且全部来自高表达状态。因此，观测到的数据在 $a$ 处的权重为 $p$，在 $a + D$ 处的权重为 $(1-p) - f$，总观测比例为 $1 - f$。用于插补的观测均值为\n$$m_{\\text{obs}} = \\frac{p\\,a + \\big((1-p)-f\\big)(a + D)}{1 - f} = a + \\frac{1 - p - f}{1 - f}\\,D.$$\n经过均值插补后，完整的数据集有三个点质量：在 $a$ 处的权重为 $p$，在 $a + D$ 处的权重为 $(1-p)-f$，在 $m_{\\text{obs}}$ 处的权重为 $f$。最终的均值为\n$$\\mu_{\\text{final}} = p\\,a + \\big((1-p)-f\\big)(a + D) + f\\,m_{\\text{obs}} = \\frac{p\\,a + \\big((1-p)-f\\big)(a + D)}{1 - f} = m_{\\text{obs}}.$$\n因此，在 $m_{\\text{obs}}$ 处的插补组对总方差的贡献为零。最终的方差为\n$$\\operatorname{Var}_{\\text{final}} = p\\big(a - \\mu_{\\text{final}}\\big)^{2} + \\big(1-p - f\\big)\\big((a + D) - \\mu_{\\text{final}}\\big)^{2}.$$\n使用 $\\mu_{\\text{final}} = a + \\frac{1 - p - f}{1 - f}\\,D$，偏差为\n$$a - \\mu_{\\text{final}} = -\\frac{1 - p - f}{1 - f}\\,D,\\qquad (a + D) - \\mu_{\\text{final}} = \\frac{p}{1 - f}\\,D.$$\n代入得，\n$$\\operatorname{Var}_{\\text{final}} = p\\left(\\frac{1 - p - f}{1 - f}\\right)^{2} D^{2} + \\big(1 - p - f\\big)\\left(\\frac{p}{1 - f}\\right)^{2} D^{2}.$$\n因式分解并化简，\n$$\\operatorname{Var}_{\\text{final}} = \\frac{D^{2}}{(1 - f)^{2}}\\left[p(1 - p - f)^{2} + p^{2}(1 - p - f)\\right] = \\frac{D^{2}}{(1 - f)^{2}}\\,p(1 - p - f)\\big((1 - p - f) + p\\big) = \\frac{D^{2}}{(1 - f)^{2}}\\,p(1 - p - f)(1 - f) = \\frac{D^{2}p(1 - p - f)}{1 - f}.$$\n因此，最终总体方差与原始总体方差之比为\n$$\\frac{\\operatorname{Var}_{\\text{final}}}{\\operatorname{Var}_{\\text{orig}}} = \\frac{\\frac{D^{2}p(1 - p - f)}{1 - f}}{p(1-p)D^{2}} = \\frac{1 - p - f}{(1 - f)(1 - p)}.$$\n该结果只取决于 $p$ 和 $f$，符合题目要求，并且满足检验条件：当 $f=0$ 时，比值为 $1$；当 $f \\to 1 - p$ 时，比值趋向于 $0$。",
            "answer": "$$\\boxed{\\frac{1-p-f}{(1-f)(1-p)}}$$"
        },
        {
            "introduction": "在掌握了简单方法的局限性后，我们转向一种更为强大和有原则的解决方法：期望最大化（EM）算法。在这个实践中，你将为一个多维高斯模型推导并实现EM算法来处理缺失数据，并进一步探究缺失数据对推断模型稀疏性的影响。这是一个结合了理论、编程和统计推断的综合性练习，能让你体验解决真实世界中复杂缺失数据问题的完整流程。",
            "id": "3127506",
            "problem": "给定一个多元正态模型，您必须使用期望最大化（EM）算法处理缺失数据，然后研究缺失情况如何影响推断出的精度矩阵的稀疏模式。考虑一个均值为 $\\mu \\in \\mathbb{R}^p$、协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{p \\times p}$ 的 $p$ 维高斯随机向量 $X \\in \\mathbb{R}^p$。回想一下，精度矩阵是协方差矩阵的逆，即 $\\Theta = \\Sigma^{-1}$。$\\Theta$ 的稀疏模式编码了高斯图模型中的条件独立性：如果对于 $i \\neq j$ 有 $\\Theta_{ij} = 0$，则变量 $i$ 和 $j$ 在给定其他变量的情况下是条件独立的。数据中的缺失项使得 $\\mu$ 和 $\\Sigma$（进而也包括 $\\Theta$）的估计变得复杂。\n\n从以下基本定义出发：多元正态族下 $n$ 个独立同分布样本的完整数据对数似然是关于充分统计量 $\\sum_{i=1}^n X_i$ 和 $\\sum_{i=1}^n X_i X_i^\\top$ 的一个指数族，以及分块多元正态向量的条件分布仍然是多元正态分布。请基于这些原理推导 EM 算法，以获得在有缺失项情况下 $\\mu$ 和 $\\Sigma$ 的最大似然估计。您的推导必须从这些原理开始，并得出明确的更新规则。然后，使用估计出的 $\\Sigma$ 计算 $\\Theta$，并通过对 $\\Theta$ 的非对角线元素的绝对值进行阈值化处理来推断稀疏模式。\n\n您的程序必须实现上述从第一性原理推导出的 EM 算法，然后量化相对于无缺失项的基线，数据缺失对推断出的稀疏模式的影响。\n\n请使用以下测试套件。在所有情况下，均使用 $p = 3$ 个变量，$n = 400$ 个样本，以及一个真实的精度矩阵\n$$\n\\Theta_{\\text{true}} = \\begin{bmatrix}\n1.0  -0.4  0.0 \\\\\n-0.4  1.0  -0.3 \\\\\n0.0  -0.3  1.0\n\\end{bmatrix},\n$$\n其中 $\\mu_{\\text{true}} = \\mathbf{0} \\in \\mathbb{R}^3$ 且 $\\Sigma_{\\text{true}} = \\Theta_{\\text{true}}^{-1}$。使用固定的伪随机种子 $s = 42$ 从 $\\mathcal{N}(\\mu_{\\text{true}}, \\Sigma_{\\text{true}})$ 中独立生成数据 $X_1, \\ldots, X_n$，以确保可重复性。\n\n定义三种缺失场景，每种场景都会生成一个二元观测掩码 $M \\in \\{0,1\\}^{n \\times p}$，其中 $M_{ij} = 1$ 表示条目 $(i,j)$ 是观测到的，$M_{ij} = 0$ 表示缺失：\n\n- 情况 A（基线，无缺失）：所有条目均被观测，即对所有 $i,j$，$M_{ij} = 1$。\n- 情况 B（随机缺失）：每个条目以概率 $r = 0.3$ 独立地设置为缺失，否则为观测值，并确保每个变量至少有一个观测条目。\n- 情况 C（结构化缺失）：对于前 $250$ 个样本，索引为 $2$ 的变量（从零开始索引）是缺失的，即对于 $i \\in \\{1,\\ldots,250\\}$，$M_{i,2} = 0$，所有其他条目最初都是观测到的；然后，在所有条目上以比率 $q = 0.1$ 叠加额外的独立缺失，同样要确保每个变量总体上至少保留一个观测条目。\n\n对于每种情况，使用以下超参数通过 EM 算法估计 $\\mu$ 和 $\\Sigma$：最大迭代次数 $200$，参数变化欧几里得范数的收敛容差 $\\varepsilon_{\\text{tol}} = 10^{-8}$，并且在每次迭代中，如果需要，通过向 $\\Sigma$ 添加一个脊项 $\\delta I$（其中 $\\delta = 10^{-6}$）来确保 $\\Sigma$ 的对称性和数值稳定性，以保持其正定性。收敛后，计算精度估计值 $\\widehat{\\Theta} = (\\widehat{\\Sigma} + \\delta I)^{-1}$。\n\n通过对绝对非对角线条目进行阈值化来定义推断出的无向边集 $E$：当且仅当 $|\\widehat{\\Theta}_{ij}|  \\tau$ 时，包含边 $\\{i,j\\}$（其中 $i  j$），阈值 $\\tau = 0.2$。令 $E^{(A)}$ 表示情况 A 的边集。对于每种情况 $X \\in \\{A,B,C\\}$，计算整数不匹配计数 $d^{(X)}$，其值为 $E^{(X)}$ 和 $E^{(A)}$ 之间对称差的大小，同时计算推断出的边的整数数量 $m^{(X)} = |E^{(X)}|$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按以下顺序排列结果：情况 A、情况 B、情况 C。每种情况必须输出为一个双元素列表 $[d^{(X)}, m^{(X)}]$。因此，最终输出必须类似于 $[ [d^{(A)},m^{(A)}],[d^{(B)},m^{(B)}],[d^{(C)},m^{(C)}] ]$。\n\n不涉及物理单位。所有实值参数均以小数形式给出。不出现角度。您必须严格遵守指定的输出格式。程序必须是完全自包含的，不需要用户输入，并且在给定指定种子和参数的情况下必须确定性地运行。",
            "solution": "已根据指定标准对用户提供的问题陈述进行了分析和验证。\n\n### 步骤 1：提取给定信息\n\n-   **模型**：$p$ 维高斯随机向量 $X \\in \\mathbb{R}^p$，均值为 $\\mu \\in \\mathbb{R}^p$，协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{p \\times p}$。精度矩阵为 $\\Theta = \\Sigma^{-1}$。\n-   **数据生成**：\n    -   变量数，$p = 3$。\n    -   样本数，$n = 400$。\n    -   真实均值，$\\mu_{\\text{true}} = \\mathbf{0} \\in \\mathbb{R}^3$。\n    -   真实精度矩阵，$\\Theta_{\\text{true}} = \\begin{bmatrix} 1.0  -0.4  0.0 \\\\ -0.4  1.0  -0.3 \\\\ 0.0  -0.3  1.0 \\end{bmatrix}$。\n    -   真实协方差矩阵，$\\Sigma_{\\text{true}} = \\Theta_{\\text{true}}^{-1}$。\n    -   数据 $X_1, \\ldots, X_n$ 从 $\\mathcal{N}(\\mu_{\\text{true}}, \\Sigma_{\\text{true}})$ 中独立同分布生成。\n    -   伪随机种子，$s = 42$。\n-   **缺失场景**：\n    -   观测掩码 $M \\in \\{0,1\\}^{n \\times p}$。\n    -   **情况 A（基线）**：对所有 $i,j$，$M_{ij} = 1$。\n    -   **情况 B（随机缺失）**：每个条目以概率 $r = 0.3$ 缺失。每个变量必须至少有一个观测条目。\n    -   **情况 C（结构化缺失）**：对于样本 $i \\in \\{1,\\ldots,250\\}$，索引为 2 的变量缺失（$M_{i,2} = 0$）。此外，在所有条目上以概率 $q = 0.1$ 叠加独立缺失。每个变量必须至少有一个观测条目。\n-   **EM 算法参数**：\n    -   最大迭代次数：$200$。\n    -   收敛容差：参数变化欧几里得范数的容差为 $\\varepsilon_{\\text{tol}} = 10^{-8}$。\n    -   正则化/稳定性：如果在迭代中需要，向 $\\Sigma$ 添加脊项 $\\delta I$（其中 $\\delta = 10^{-6}$）以保持正定性。\n-   **稀疏性推断**：\n    -   估计的精度矩阵：$\\widehat{\\Theta} = (\\widehat{\\Sigma} + \\delta I)^{-1}$，其中 $\\delta = 10^{-6}$。\n    -   推断的边集 $E$：如果 $|\\widehat{\\Theta}_{ij}|  \\tau$（其中 $\\tau = 0.2$），则包含边 $\\{i,j\\}$（$i  j$）。\n-   **输出指标**：对于每种情况 $X \\in \\{A, B, C\\}$：\n    -   $m^{(X)} = |E^{(X)}|$：推断出的边的数量。\n    -   $d^{(X)} = |E^{(X)} \\Delta E^{(A)}|$：$E^{(X)}$ 和 $E^{(A)}$ 之间对称差的大小。\n-   **输出格式**：单行 `[ [d^{(A)},m^{(A)}],[d^{(B)},m^{(B)}],[d^{(C)},m^{(C)}] ]`。\n\n### 步骤 2：使用提取的给定信息进行验证\n\n-   **科学基础**：该问题基于成熟的期望最大化（EM）算法，用于估计存在缺失数据的多元正态分布参数，这是统计学习和计算统计学中的一个标准课题。精度矩阵的稀疏性与高斯图模型中条件独立性之间的联系是一项基本原理。问题的前提在事实上和科学上都是合理的。\n-   **适定性**：问题以高精度指定。所有参数、常数、初始条件（通过固定种子）以及算法超参数都已明确定义。目标是定量的、明确的。这种结构确保可以计算出唯一的、确定性的、有意义的解。\n-   **客观性**：问题以正式、客观的语言陈述，不含任何主观或基于意见的主张。\n\n该问题没有表现出任何无效性缺陷：\n1.  **科学上不合理**：无。\n2.  **非形式化/不相关**：无。该问题是统计学习中的一个形式化练习。\n3.  **不完整/矛盾**：无。设置是全面的、自洽的。关于“如果需要”添加脊项的轻微模糊性，已通过后续计算 $\\widehat{\\Theta}$ 的指令得到澄清，一个稳健的实现可以很容易地将“如果需要”解释为处理潜在的数值不稳定性。\n4.  **不现实/不可行**：无。问题规模（$n=400, p=3$）对于现代硬件而言在计算上是微不足道的。\n5.  **不适定/结构不良**：无。问题结构良好，会导向一个单一的、可验证的答案。\n6.  **伪深刻/琐碎**：无。从指定原理推导并实现 EM 算法需要实质性的理解，并且不是一项琐碎的任务。\n7.  **超出科学可验证性**：无。结果是可以通过计算验证的。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将提供一个解决方案。\n\n### 基于原理的 EM 算法推导\n\n目标是为参数 $\\theta = (\\mu, \\Sigma)$ 找到最大似然估计（MLE），给定一个包含缺失条目的数据集 $X = (X_1, \\dots, X_n)$。期望最大化（EM）算法是完成此任务的理想迭代过程。\n\n设完整数据集为 $X$，并将其划分为观测部分 $X_{\\text{obs}}$ 和缺失部分 $X_{\\text{mis}}$。EM 的核心思想是迭代地最大化完整数据对数似然的期望值，该期望值以观测数据和当前参数估计为条件。\n\n假设有 $n$ 个来自 $\\mathcal{N}(\\mu, \\Sigma)$ 的独立同分布样本，完整数据 $X$ 的对数似然为：\n$$\n\\log L(\\theta; X) = \\sum_{i=1}^n \\log p(X_i | \\mu, \\Sigma)\n$$\n单个样本 $X_i$ 的概率密度函数为：\n$$\np(X_i | \\mu, \\Sigma) = (2\\pi)^{-p/2} |\\Sigma|^{-1/2} \\exp\\left(-\\frac{1}{2}(X_i - \\mu)^\\top \\Sigma^{-1} (X_i - \\mu)\\right)\n$$\n因此，完整数据的对数似然为：\n$$\n\\log L(\\theta; X) = -\\frac{np}{2}\\log(2\\pi) - \\frac{n}{2}\\log|\\Sigma| - \\frac{1}{2} \\sum_{i=1}^n (X_i - \\mu)^\\top \\Sigma^{-1} (X_i - \\mu)\n$$\n该表达式是指数族的一员。依赖于数据的项在求和号内。展开二次型，我们得到：\n$$\n\\sum_{i=1}^n (X_i - \\mu)^\\top \\Sigma^{-1} (X_i - \\mu) = \\text{tr}\\left( \\Sigma^{-1} \\sum_{i=1}^n (X_i - \\mu)(X_i - \\mu)^\\top \\right)\n$$\n$$\n= \\text{tr}\\left( \\Sigma^{-1} \\left( \\sum_{i=1}^n X_i X_i^\\top - \\mu \\left(\\sum_{i=1}^n X_i\\right)^\\top - \\left(\\sum_{i=1}^n X_i\\right) \\mu^\\top + n\\mu\\mu^\\top \\right) \\right)\n$$\n因此，完整数据的充分统计量为 $S_1 = \\sum_{i=1}^n X_i$ 和 $S_2 = \\sum_{i=1}^n X_i X_i^\\top$。\n\nEM 算法在每次迭代 $t$ 中分两步进行：\n1.  **E-步**：计算 $Q$ 函数，即完整数据对数似然关于缺失数据在给定观测数据和当前参数 $\\theta^{(t)} = (\\mu^{(t)}, \\Sigma^{(t)})$ 条件下的条件分布的期望。\n    $$\n    Q(\\theta | \\theta^{(t)}) = \\mathbb{E}_{X_{\\text{mis}} | X_{\\text{obs}}, \\theta^{(t)}}[\\log L(\\theta; X)]\n    $$\n    由于期望的线性性质，我们只需要计算充分统计量的条件期望：\n    $$\n    \\mathbb{E}[S_1 | X_{\\text{obs}}, \\theta^{(t)}] = \\sum_{i=1}^n \\mathbb{E}[X_i | X_{i, \\text{obs}}, \\theta^{(t)}]\n    $$\n    $$\n    \\mathbb{E}[S_2 | X_{\\text{obs}}, \\theta^{(t)}] = \\sum_{i=1}^n \\mathbb{E}[X_i X_i^\\top | X_{i, \\text{obs}}, \\theta^{(t)}]\n    $$\n\n2.  **M-步**：关于 $\\theta$ 最大化 $Q(\\theta | \\theta^{(t)})$ 以获得更新后的参数 $\\theta^{(t+1)}$。这等同于使用在 E-步中计算出的期望充分统计量来应用标准的 MLE 公式。\n\n**E-步更新的推导：**\n对于每个数据点 $X_i$，我们根据其观测（$o$）和缺失（$m$）分量对其以及参数 $\\mu^{(t)}$ 和 $\\Sigma^{(t)}$ 进行分块。设 $o_i$ 为样本 $i$ 的观测特征索引集，而 $m_i$ 为缺失特征索引集。\n$$\nX_i = \\begin{pmatrix} X_{i,o} \\\\ X_{i,m} \\end{pmatrix}, \\quad \\mu^{(t)} = \\begin{pmatrix} \\mu_o^{(t)} \\\\ \\mu_m^{(t)} \\end{pmatrix}, \\quad \\Sigma^{(t)} = \\begin{pmatrix} \\Sigma_{oo}^{(t)}  \\Sigma_{om}^{(t)} \\\\ \\Sigma_{mo}^{(t)}  \\Sigma_{mm}^{(t)} \\end{pmatrix}\n$$\n条件分布 $p(X_{i,m} | X_{i,o}, \\theta^{(t)})$ 是多元正态分布，其：\n-   条件均值：$\\mathbb{E}[X_{i,m} | X_{i,o}, \\theta^{(t)}] = \\mu_m^{(t)} + \\Sigma_{mo}^{(t)} (\\Sigma_{oo}^{(t)})^{-1} (X_{i,o} - \\mu_o^{(t)})$\n-   条件协方差：$\\text{Cov}(X_{i,m} | X_{i,o}, \\theta^{(t)}) = \\Sigma_{mm}^{(t)} - \\Sigma_{mo}^{(t)} (\\Sigma_{oo}^{(t)})^{-1} \\Sigma_{om}^{(t)}$\n\n使用这些，我们为每个样本 $i$ 计算所需的期望：\n1.  $X_i$ 的条件期望，记为 $\\hat{X}_i^{(t)}$：\n    $$\n    \\hat{X}_i^{(t)} = \\mathbb{E}[X_i | X_{i,o}, \\theta^{(t)}]\n    $$\n    $\\hat{X}_i^{(t)}$ 的分量是观测特征的观测值和缺失特征的条件均值。\n2.  外积 $X_i X_i^\\top$ 的条件期望，记为 $\\widehat{X_iX_i^\\top}^{(t)}$：\n    $$\n    \\widehat{X_iX_i^\\top}^{(t)} = \\mathbb{E}[X_i X_i^\\top | X_{i,o}, \\theta^{(t)}] = \\text{Cov}(X_i | X_{i,o}, \\theta^{(t)}) + \\hat{X}_i^{(t)} (\\hat{X}_i^{(t)})^\\top\n    $$\n    条件协方差矩阵 $\\text{Cov}(X_i | X_{i,o}, \\theta^{(t)})$ 是一个 $p \\times p$ 的零矩阵，除了对应于缺失特征 $(m_i, m_i)$ 的块，该块由 $\\text{Cov}(X_{i,m} | X_{i,o}, \\theta^{(t)})$ 填充。\n\n为所有 $i=1, \\dots, n$ 计算完这些量后，我们通过求和得到总的期望充分统计量。\n\n**M-步更新的推导：**\n更新后的参数 $(\\mu^{(t+1)}, \\Sigma^{(t+1)})$ 是使 $Q(\\theta|\\theta^{(t)})$ 最大化的值。此最大化过程产生标准的 MLE 更新规则，其中充分统计量被其期望所替代：\n$$\n\\mu^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n \\hat{X}_i^{(t)}\n$$\n$$\n\\Sigma^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}[(X_i - \\mu^{(t+1)})(X_i - \\mu^{(t+1)})^\\top | X_{i,o}, \\theta^{(t)}]\n$$\n展开协方差项：\n$$\n\\Sigma^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n \\widehat{X_iX_i^\\top}^{(t)} - \\mu^{(t+1)} (\\mu^{(t+1)})^\\top\n$$\n为确保数值稳定性和正定性，通过与自身转置求平均值来使更新后的 $\\Sigma^{(t+1)}$ 对称，即 $\\Sigma^{(t+1)} \\leftarrow \\frac{1}{2}(\\Sigma^{(t+1)} + (\\Sigma^{(t+1)})^\\top)$。\n\n重复 E-步和 M-步，直到参数 $(\\mu, \\Sigma)$ 的变化量低于容差 $\\varepsilon_{\\text{tol}}$。\n\n**稀疏性推断：**\nEM 算法收敛后，我们获得最终估计值 $\\widehat{\\mu}$ 和 $\\widehat{\\Sigma}$。估计的精度矩阵 $\\widehat{\\Theta}$ 通过正则化求逆计算得出：\n$$\n\\widehat{\\Theta} = (\\widehat{\\Sigma} + \\delta I)^{-1}\n$$\n其中 $\\delta  0$ 是一个确保矩阵良态且可逆的小常数。如果精度矩阵对应的非对角线元素显著非零，则推断在底层高斯图模型中存在一条无向边 $\\{i, j\\}$。这通过阈值化操作实现：对于给定的阈值 $\\tau$，如果 $|\\widehat{\\Theta}_{ij}|  \\tau$，则边 $\\{i,j\\}$（其中 $i  j$）被包含在估计的边集 $E$ 中。此过程允许研究不同的缺失数据模式如何影响真实条件独立性结构的恢复。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import inv\n\ndef solve():\n    \"\"\"\n    Implements the EM algorithm for multivariate normal data with missing values,\n    and analyzes the effect of missingness on the inferred precision matrix sparsity.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    p = 3\n    n = 400\n    s = 42\n    theta_true = np.array([\n        [1.0, -0.4, 0.0],\n        [-0.4, 1.0, -0.3],\n        [0.0, -0.3, 1.0]\n    ])\n    mu_true = np.zeros(p)\n    \n    # EM Hyperparameters\n    max_iter = 200\n    tol = 1e-8\n    delta = 1e-6\n    tau = 0.2\n\n    # --- Data Generation ---\n    sigma_true = inv(theta_true)\n    rng = np.random.default_rng(s)\n    x_full = rng.multivariate_normal(mu_true, sigma_true, n)\n\n    # --- Mask Generation ---\n    masks = {}\n\n    # Case A: No missingness\n    masks['A'] = np.ones((n, p), dtype=int)\n\n    # Case B: Missing at random\n    r = 0.3\n    while True:\n        mask_b = (rng.random((n, p)) > r).astype(int)\n        if np.all(np.sum(mask_b, axis=0) > 0):\n            masks['B'] = mask_b\n            break\n\n    # Case C: Structured missingness + random\n    q = 0.1\n    while True:\n        mask_c_base = np.ones((n, p), dtype=int)\n        mask_c_base[0:250, 2] = 0\n        additional_missing = rng.random((n, p))  q\n        mask_c = np.logical_and(mask_c_base, np.logical_not(additional_missing)).astype(int)\n        if np.all(np.sum(mask_c, axis=0) > 0):\n            masks['C'] = mask_c\n            break\n            \n    def em_algorithm(data, mask):\n        \"\"\"\n        Performs EM for multivariate normal parameter estimation with missing data.\n        \"\"\"\n        n_samples, n_features = data.shape\n        \n        # Initialization\n        nan_data = np.where(mask.astype(bool), data, np.nan)\n        mu = np.nanmean(nan_data, axis=0)\n        # Handle cases where a column is fully missing (though prevented by mask generation)\n        mu[np.isnan(mu)] = 0\n        sigma = np.eye(n_features)\n        \n        for _ in range(max_iter):\n            mu_old = mu.copy()\n            sigma_old = sigma.copy()\n            \n            sum_x_hat = np.zeros(n_features)\n            sum_xxt_hat = np.zeros((n_features, n_features))\n            \n            # E-Step: Iterate over each sample\n            for i in range(n_samples):\n                x_i = data[i, :]\n                mask_i = mask[i, :]\n                \n                o_idx = np.where(mask_i == 1)[0]\n                m_idx = np.where(mask_i == 0)[0]\n                \n                if len(m_idx) == 0:  # Fully observed\n                    x_i_hat = x_i\n                    cov_i_hat = np.zeros((n_features, n_features))\n                elif len(o_idx) == 0:  # Fully missing\n                    x_i_hat = mu\n                    cov_i_hat = sigma\n                else:  # Partially observed\n                    mu_o = mu[o_idx]\n                    mu_m = mu[m_idx]\n                    sigma_oo = sigma[np.ix_(o_idx, o_idx)]\n                    sigma_mo = sigma[np.ix_(m_idx, o_idx)]\n                    sigma_mm = sigma[np.ix_(m_idx, m_idx)]\n                    \n                    try:\n                        sigma_oo_inv = inv(sigma_oo)\n                    except np.linalg.LinAlgError:\n                        # Add ridge for stability if submatrix is singular\n                        sigma_oo_inv = inv(sigma_oo + delta * np.eye(len(o_idx)))\n\n                    cond_mean = mu_m + sigma_mo @ sigma_oo_inv @ (x_i[o_idx] - mu_o)\n                    cond_cov = sigma_mm - sigma_mo @ sigma_oo_inv @ sigma_mo.T\n                    \n                    x_i_hat = np.zeros(n_features)\n                    x_i_hat[o_idx] = x_i[o_idx]\n                    x_i_hat[m_idx] = cond_mean\n                    \n                    cov_i_hat = np.zeros((n_features, n_features))\n                    cov_i_hat[np.ix_(m_idx, m_idx)] = cond_cov\n\n                xxt_i_hat = cov_i_hat + np.outer(x_i_hat, x_i_hat)\n                sum_x_hat += x_i_hat\n                sum_xxt_hat += xxt_i_hat\n            \n            # M-Step\n            mu = sum_x_hat / n_samples\n            sigma = (sum_xxt_hat / n_samples) - np.outer(mu, mu)\n            sigma = (sigma + sigma.T) / 2  # Ensure symmetry\n            \n            # Convergence Check based on Euclidean norm of flattened parameters\n            param_vec_old = np.concatenate((mu_old, sigma_old.ravel()))\n            param_vec_new = np.concatenate((mu, sigma.ravel()))\n            diff = np.linalg.norm(param_vec_new - param_vec_old)\n\n            if diff  tol:\n                break\n                \n        return mu, sigma\n\n    def analyze_sparsity(sigma):\n        \"\"\"\n        Computes the precision matrix and infers the edge set by thresholding.\n        \"\"\"\n        theta_hat = inv(sigma + delta * np.eye(p))\n        edge_set = set()\n        for i in range(p):\n            for j in range(i + 1, p):\n                if abs(theta_hat[i, j])  tau:\n                    edge_set.add(tuple(sorted((i, j))))\n        return edge_set\n\n    # --- Main Analysis Loop ---\n    edge_sets = {}\n    for case in ['A', 'B', 'C']:\n        mask = masks[case]\n        _, sigma_hat = em_algorithm(x_full, mask)\n        edge_sets[case] = analyze_sparsity(sigma_hat)\n\n    # --- Compute Final Metrics ---\n    e_a = edge_sets['A']\n    e_b = edge_sets['B']\n    e_c = edge_sets['C']\n\n    m_a = len(e_a)\n    m_b = len(e_b)\n    m_c = len(e_c)\n\n    d_a = 0  # Symmetric difference with itself is 0\n    d_b = len(e_a.symmetric_difference(e_b))\n    d_c = len(e_a.symmetric_difference(e_c))\n\n    # --- Format and Print Output ---\n    final_results = [\n        [d_a, m_a],\n        [d_b, m_b],\n        [d_c, m_c]\n    ]\n\n    # Format string to remove spaces for exact match\n    formatted_strings = [str(res).replace(' ', '') for res in final_results]\n    print(f\"[{','.join(formatted_strings)}]\")\n\nsolve()\n```"
        }
    ]
}