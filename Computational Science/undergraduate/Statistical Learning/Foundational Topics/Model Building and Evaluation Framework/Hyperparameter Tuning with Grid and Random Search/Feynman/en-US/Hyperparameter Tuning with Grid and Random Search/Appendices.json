{
    "hands_on_practices": [
        {
            "introduction": "This practice builds on the geometric intuition behind the effectiveness of random search. In many machine learning problems, performance is highly sensitive to only a few \"important\" hyperparameters, while being relatively insensitive to others. This exercise  models this scenario by defining a \"near-optimal\" region as a narrow ellipse, allowing you to quantitatively compare how effectively grid search and random search can discover good configurations when the underlying structure is not aligned with the search grid.",
            "id": "3129463",
            "problem": "Consider a two-dimensional hyperparameter space $\\mathcal{H} = [-1,1]^2$. Suppose the \"near-optimal\" configurations of two hyperparameters are described by an elliptical set centered at the origin,\n$$\n\\mathcal{E}(\\theta,a,b) = \\left\\{ x \\in \\mathcal{H} \\,:\\, x^\\top Q(\\theta,a,b)\\, x \\le 1 \\right\\},\n$$\nwhere the quadratic form matrix $Q(\\theta,a,b)$ is defined from the rotation matrix\n$$\nR(\\theta) = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\[4pt] \\sin\\theta & \\cos\\theta \\end{pmatrix},\n$$\nand the diagonal scaling matrix\n$$\nD(a,b) = \\operatorname{diag}\\!\\left(\\frac{1}{a^2}, \\frac{1}{b^2}\\right),\n$$\nso that\n$$\nQ(\\theta,a,b) = R(\\theta)\\, D(a,b)\\, R(\\theta)^\\top.\n$$\nHere, $\\theta$ (in radians) encodes the correlation structure of the hyperparameters by rotating the principal axes of the ellipse, and $a>0$, $b>0$ are the semi-axis lengths. Assume $\\mathcal{E}(\\theta,a,b) \\subset \\mathcal{H}$ for the parameters considered.\n\nDefine two hyperparameter search strategies:\n- Grid search: sample $S$ points deterministically on an axis-aligned $N \\times N$ grid with $N = \\sqrt{S}$ (assume $S$ is a perfect square), using equispaced coordinates $\\{-1 + \\frac{2(i-1)}{N-1} \\,:\\, i=1,\\dots,N\\}$ along each axis.\n- Random search: sample $S$ points independently and uniformly from $\\mathcal{H}$, i.e., $S$ Independent and Identically Distributed (IID) draws from the uniform distribution on $\\mathcal{H}$, but evaluate its performance using analytical expectation rather than randomness.\n\nUse the \"coverage fraction\" (fraction of sampled points that fall inside $\\mathcal{E}(\\theta,a,b)$) as the performance metric. Let $\\hat{\\alpha}_{\\text{grid}}$ be the empirical coverage fraction of grid search, computed exactly by testing grid points for membership in $\\mathcal{E}(\\theta,a,b)$ via the inequality $x^\\top Q x \\le 1$. Let $\\alpha_{\\text{rand}}$ be the expected coverage fraction of random search, computed analytically from first principles under uniform sampling over $\\mathcal{H}$.\n\nFrom well-tested facts, the area of an ellipse with semi-axes $a$ and $b$ is $\\pi a b$, and the area of the square $\\mathcal{H}$ is $4$. Under uniform sampling, the expected coverage fraction is the ratio of areas, i.e., $\\alpha_{\\text{rand}} = \\frac{\\pi a b}{4}$, which is invariant to $\\theta$.\n\nYour task is to implement a program that, for each test case in the set below, computes:\n- $\\hat{\\alpha}_{\\text{grid}}$ exactly from the grid,\n- $\\alpha_{\\text{rand}}$ analytically as $\\frac{\\pi a b}{4}$,\n- the difference $\\Delta = \\hat{\\alpha}_{\\text{grid}} - \\alpha_{\\text{rand}}$.\n\nAngles are in radians. No physical units apply. The final outputs for each test case must be floats.\n\nTest suite (each tuple lists $(\\theta, a, b, S)$):\n- Case $1$: $(0, 0.6, 0.2, 100)$\n- Case $2$: $\\left(\\frac{\\pi}{4}, 0.6, 0.2, 100\\right)$\n- Case $3$: $\\left(\\frac{\\pi}{3}, 0.8, 0.05, 64\\right)$\n- Case $4$: $(0.3, 0.3, 0.3, 25)$\n- Case $5$: $\\left(\\frac{\\pi}{4}, 0.45, 0.25, 400\\right)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases above. For example, the output format must be of the form $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$.",
            "solution": "The problem requires a comparative analysis of two hyperparameter search strategies, grid search and random search, on a two-dimensional hyperparameter space $\\mathcal{H} = [-1,1]^2$. The performance metric is the \"coverage fraction,\" which measures the proportion of sampled points that lie within a specified elliptical region of \"near-optimal\" configurations, $\\mathcal{E}(\\theta,a,b)$.\n\nOur objective is to compute the difference $\\Delta = \\hat{\\alpha}_{\\text{grid}} - \\alpha_{\\text{rand}}$ for a given set of test cases. Here, $\\hat{\\alpha}_{\\text{grid}}$ is the empirical coverage fraction for a deterministic grid search, and $\\alpha_{\\text{rand}}$ is the expected coverage fraction for a uniform random search.\n\nFirst, let us formalize the components of the problem.\n\nThe elliptical region $\\mathcal{E}$ is defined by the set of points $x \\in \\mathcal{H}$ satisfying the inequality $x^\\top Q x \\le 1$. The matrix $Q$ is a positive-definite symmetric matrix encoding the shape, size, and orientation of the ellipse. It is constructed as $Q(\\theta,a,b) = R(\\theta)\\, D(a,b)\\, R(\\theta)^\\top$, where:\n-   $a>0$ and $b>0$ are the lengths of the semi-axes of the ellipse.\n-   $\\theta$ is the rotation angle of the ellipse's principal axes with respect to the coordinate axes, given in radians.\n-   $R(\\theta)$ is the $2 \\times 2$ rotation matrix:\n    $$\n    R(\\theta) = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix}\n    $$\n-   $D(a,b)$ is a diagonal matrix containing the inverse squared semi-axis lengths:\n    $$\n    D(a,b) = \\operatorname{diag}\\!\\left(\\frac{1}{a^2}, \\frac{1}{b^2}\\right) = \\begin{pmatrix} \\frac{1}{a^2} & 0 \\\\ 0 & \\frac{1}{b^2} \\end{pmatrix}\n    $$\nThe quadratic form for a point $x = (x_1, x_2)^\\top$ can be written as $x^\\top Q x = Q_{11}x_1^2 + (Q_{12}+Q_{21})x_1x_2 + Q_{22}x_2^2$. Since $Q$ is symmetric ($Q=Q^\\top$), this simplifies to $Q_{11}x_1^2 + 2Q_{12}x_1x_2 + Q_{22}x_2^2$.\n\nThe calculation proceeds in two parts for each test case $(\\theta, a, b, S)$.\n\n**1. Expected Coverage Fraction for Random Search ($\\alpha_{\\text{rand}}$)**\n\nRandom search involves drawing $S$ points independently from a uniform distribution over the hyperparameter space $\\mathcal{H}$. The probability of a single point falling within the subregion $\\mathcal{E}$ is the ratio of the area of $\\mathcal{E}$ to the area of $\\mathcal{H}$. By the law of large numbers, the empirical fraction of points converges to this probability. The problem specifies using this analytical expectation for performance.\n\nThe area of the hyperparameter space $\\mathcal{H} = [-1,1]^2$ is $\\text{Area}(\\mathcal{H}) = (1 - (-1)) \\times (1 - (-1)) = 2 \\times 2 = 4$.\nThe area of an ellipse with semi-axes $a$ and $b$ is given by $\\text{Area}(\\mathcal{E}) = \\pi a b$. The problem assumes $\\mathcal{E} \\subset \\mathcal{H}$, so we do not need to consider the more complex case of intersection.\n\nTherefore, the expected coverage fraction for random search is:\n$$\n\\alpha_{\\text{rand}} = \\frac{\\text{Area}(\\mathcal{E})}{\\text{Area}(\\mathcal{H})} = \\frac{\\pi a b}{4}\n$$\nThis quantity depends only on the semi-axis lengths $a$ and $b$, and is invariant to the rotation angle $\\theta$.\n\n**2. Empirical Coverage Fraction for Grid Search ($\\hat{\\alpha}_{\\text{grid}}$)**\n\nGrid search samples points deterministically from a predefined grid. We are given $S$ total sample points, arranged on an $N \\times N$ grid, where $N = \\sqrt{S}$. The grid points are formed by the Cartesian product of one-dimensional coordinate sets. For each axis, the $N$ coordinates are equispaced within the interval $[-1, 1]$:\n$$\nc_i = -1 + \\frac{2(i-1)}{N-1} \\quad \\text{for } i = 1, 2, \\dots, N\n$$\nThe grid consists of $S$ points $(c_i, c_j)$ for all pairs $(i, j)$ where $i,j \\in \\{1, \\dots, N\\}$.\n\nTo find the empirical coverage fraction $\\hat{\\alpha}_{\\text{grid}}$, we must explicitly test each of the $S$ grid points for membership in $\\mathcal{E}$. Let $N_{\\text{inside}}$ be the number of grid points $x$ satisfying the condition $x^\\top Q x \\le 1$. The fraction is then:\n$$\n\\hat{\\alpha}_{\\text{grid}} = \\frac{N_{\\text{inside}}}{S}\n$$\nThe computational algorithm for each test case $(\\theta, a, b, S)$ is as follows:\n1.  Calculate $\\alpha_{\\text{rand}} = (\\pi a b) / 4$.\n2.  Determine the grid dimension $N = \\sqrt{S}$.\n3.  Construct the matrix $Q$ from the given $\\theta$, $a$, and $b$.\n4.  Generate the set of $S$ grid points $(x_1, x_2)$ where $x_1, x_2 \\in \\{-1 + \\frac{2(i-1)}{N-1} \\mid i=1,\\dots,N\\}$.\n5.  Initialize a counter $N_{\\text{inside}} = 0$.\n6.  For each grid point $x = (x_1, x_2)^\\top$:\n    a.  Compute the quadratic form value $v = x^\\top Q x = Q_{11}x_1^2 + 2Q_{12}x_1x_2 + Q_{22}x_2^2$.\n    b.  If $v \\le 1$, increment $N_{\\text{inside}}$.\n7.  Calculate $\\hat{\\alpha}_{\\text{grid}} = N_{\\text{inside}} / S$.\n8.  Compute the final difference $\\Delta = \\hat{\\alpha}_{\\text{grid}} - \\alpha_{\\text{rand}}$.\n\nThis procedure will be implemented for each test case specified in the problem statement to generate the final list of results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the hyperparameter search comparison problem for a suite of test cases.\n    \"\"\"\n    \n    # Test suite: each tuple is (theta, a, b, S)\n    test_cases = [\n        (0, 0.6, 0.2, 100),\n        (np.pi / 4, 0.6, 0.2, 100),\n        (np.pi / 3, 0.8, 0.05, 64),\n        (0.3, 0.3, 0.3, 25),\n        (np.pi / 4, 0.45, 0.25, 400),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        theta, a, b, S = case\n        \n        # 1. Calculate the expected coverage fraction for random search (alpha_rand)\n        # This is the ratio of the area of the ellipse to the area of the search space.\n        # Area of ellipse = pi * a * b\n        # Area of search space [-1,1]^2 = 4\n        alpha_rand = (np.pi * a * b) / 4.0\n        \n        # 2. Calculate the empirical coverage fraction for grid search (alpha_grid)\n        \n        # Grid dimension N\n        N = int(np.sqrt(S))\n        \n        # Construct the quadratic form matrix Q = R * D * R.T\n        c, s = np.cos(theta), np.sin(theta)\n        R = np.array([[c, -s], \n                      [s, c]])\n        \n        D = np.diag([1.0 / a**2, 1.0 / b**2])\n        \n        Q = R @ D @ R.T\n        \n        # Generate grid points\n        # Using np.linspace is robust for N=1 case (though not in test suite)\n        if N > 1:\n            coords = np.linspace(-1.0, 1.0, N)\n        else:\n            # A 1x1 grid is at the center (0,0)\n            coords = np.array([0.0])\n\n        grid_x, grid_y = np.meshgrid(coords, coords)\n        \n        # Evaluate the quadratic form x.T * Q * x for all grid points\n        # x.T*Q*x = Q_11*x1^2 + 2*Q_12*x1*x2 + Q_22*x2^2\n        # Since Q is symmetric, Q_12 = Q_21\n        qf_values = (Q[0, 0] * grid_x**2 + \n                     (Q[0, 1] + Q[1, 0]) * grid_x * grid_y +\n                     Q[1, 1] * grid_y**2)\n                     \n        # Count points inside or on the ellipse (where x.T*Q*x <= 1)\n        points_inside = np.sum(qf_values <= 1.0)\n        \n        alpha_grid = points_inside / S\n        \n        # 3. Compute the difference Delta\n        delta = alpha_grid - alpha_rand\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After understanding the geometric advantage of random search, the next crucial step is choosing the right sampling distribution. Many hyperparameters, such as regularization strength or learning rates, often influence model performance on a multiplicative or logarithmic scale. This exercise  provides a rigorous, first-principles derivation to show why sampling a parameter uniformly on a log scale is more efficient than sampling on a linear scale in such cases, a concept quantified by minimizing expected regret.",
            "id": "3129504",
            "problem": "Consider the task of tuning a regularization parameter $\\lambda$ for a model whose validation performance depends primarily on the logarithm of the parameter. Assume the following simplified performance model in natural logarithm: the performance score for a given $\\lambda$ is $P(\\lambda)=1-c\\cdot\\left|\\log(\\lambda)-\\log(\\lambda^*)\\right|$, where $c>0$ is a small constant and $\\lambda^*$ is the unknown optimal parameter within the search interval. A random search draws $N$ independent samples $\\lambda_1,\\dots,\\lambda_N$ from a specified sampling distribution over a search interval $\\lambda\\in[\\lambda_{\\min},\\lambda_{\\max}]$. The random search selects the sample with the highest performance. Define the regret as $R=P(\\lambda^*)-\\max_{i=1,\\dots,N} P(\\lambda_i)$.\n\nStarting from the definitions of expected value and independent sampling, and without using any shortcut formulas, derive how to compute the expected regret $\\mathbb{E}[R]$ for two sampling distributions:\n\n1. Uniform-in-parameter sampling: each $\\lambda_i$ is drawn independently from a uniform distribution over $[\\lambda_{\\min},\\lambda_{\\max}]$.\n2. Uniform-in-log sampling: each $\\lambda_i$ is drawn independently so that $X_i=\\log(\\lambda_i)$ is uniform over $[a,b]$, where $a=\\log(\\lambda_{\\min})$ and $b=\\log(\\lambda_{\\max})$.\n\nUse the following fundamental base:\n- The definition of expected value $\\mathbb{E}[Y]=\\int y\\,\\mathrm{d}\\mathbb{P}(y)$ for a random variable $Y$.\n- The independence of samples implies that the probability of no sample falling in a measurable set $A$ is the product of the complements of the single-sample probabilities.\n- The identity for nonnegative random variables $Y$: $\\mathbb{E}[Y]=\\int_{0}^{\\infty}\\mathbb{P}(Y>t)\\,\\mathrm{d}t$.\n\nLet $X^*=\\log(\\lambda^*)$, $a=\\log(\\lambda_{\\min})$, and $b=\\log(\\lambda_{\\max})$, and define the minimum absolute log-distance $D_N=\\min_{i=1,\\dots,N}\\left|X_i-X^*\\right|$. For this performance model, the regret can be written as $R=c\\cdot D_N$, so $\\mathbb{E}[R]=c\\cdot\\mathbb{E}[D_N]$. Show that for each sampling scheme, $\\mathbb{E}[D_N]$ can be expressed as a one-dimensional integral over $t\\in[0,t_{\\max}]$, where $t_{\\max}=\\max\\{X^*-a,\\,b-X^*\\}$, involving the probability that none of the $N$ samples fall into the interval $(X^*-t,X^*+t)$ intersected with $[a,b]$. Precisely specify this probability for each scheme:\n- For uniform-in-log sampling, write the single-sample probability of landing in $(X^*-t,X^*+t)\\cap[a,b]$ as the ratio of lengths in log-space.\n- For uniform-in-parameter sampling, write the single-sample probability of landing in the corresponding interval as the ratio of lengths in parameter space, using the exponential mapping from log-space.\n\nYour program must compute $\\mathbb{E}[R]$ numerically for both sampling schemes using the integral representation and produce the following outputs for a test suite of four cases. For each case, output three floating-point numbers in order: the expected regret under uniform-in-parameter sampling, the expected regret under uniform-in-log sampling, and their difference (uniform minus log). Aggregate all results into a single line as a comma-separated list enclosed in square brackets.\n\nUse natural logarithms in all computations. The test suite is:\n- Case $1$: $\\lambda_{\\min}=10^{-4}$, $\\lambda_{\\max}=10^{2}$, $\\lambda^*=10^{-1}$, $c=0.05$, $N=20$.\n- Case $2$: $\\lambda_{\\min}=10^{-4}$, $\\lambda_{\\max}=10^{2}$, $\\lambda^*=10^{-4}$, $c=0.05$, $N=20$.\n- Case $3$: $\\lambda_{\\min}=10^{-6}$, $\\lambda_{\\max}=10^{0}$, $\\lambda^*=10^{-3}$, $c=0.10$, $N=1$.\n- Case $4$: $\\lambda_{\\min}=10^{-3}$, $\\lambda_{\\max}=10^{-2}$, $\\lambda^*=5\\cdot 10^{-3}$, $c=0.20$, $N=10$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order: $[\\mathbb{E}[R]_{\\text{uniform}},\\mathbb{E}[R]_{\\text{log}},\\Delta,\\dots]$ for the four cases, where $\\Delta=\\mathbb{E}[R]_{\\text{uniform}}-\\mathbb{E}[R]_{\\text{log}}$ for each case.",
            "solution": "The problem statement is first subjected to a rigorous validation process before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extracted Givens**\n\nThe verbatim data, variables, and conditions provided in the problem statement are as follows:\n- Performance model: $P(\\lambda)=1-c\\cdot\\left|\\log(\\lambda)-\\log(\\lambda^*)\\right|$, with $c>0$.\n- Optimal parameter: $\\lambda^*$.\n- Search interval: $\\lambda\\in[\\lambda_{\\min},\\lambda_{\\max}]$.\n- Search method: Random search with $N$ independent samples $\\lambda_1, \\dots, \\lambda_N$.\n- Regret definition: $R=P(\\lambda^*)-\\max_{i=1,\\dots,N} P(\\lambda_i)$.\n- Sampling distributions:\n  1.  Uniform-in-parameter: $\\lambda_i \\sim U[\\lambda_{\\min},\\lambda_{\\max}]$.\n  2.  Uniform-in-log: $X_i=\\log(\\lambda_i) \\sim U[a,b]$, where $a=\\log(\\lambda_{\\min})$ and $b=\\log(\\lambda_{\\max})$.\n- Fundamental principles to be used:\n  - Definition of expected value: $\\mathbb{E}[Y]=\\int y\\,\\mathrm{d}\\mathbb{P}(y)$.\n  - Independence of samples.\n  - Identity for nonnegative random variables: $\\mathbb{E}[Y]=\\int_{0}^{\\infty}\\mathbb{P}(Y>t)\\,\\mathrm{d}t$.\n- Notation: $X^*=\\log(\\lambda^*)$, $D_N=\\min_{i=1,\\dots,N}\\left|X_i-X^*\\right|$.\n- Relation: $R=c\\cdot D_N$, and $\\mathbb{E}[R]=c\\cdot\\mathbb{E}[D_N]$.\n- Task: Express $\\mathbb{E}[D_N]$ as a one-dimensional integral over $t\\in[0,t_{\\max}]$, where $t_{\\max}=\\max\\{X^*-a,\\,b-X^*\\}$.\n- Test cases:\n  - Case $1$: $\\lambda_{\\min}=10^{-4}$, $\\lambda_{\\max}=10^{2}$, $\\lambda^*=10^{-1}$, $c=0.05$, $N=20$.\n  - Case $2$: $\\lambda_{\\min}=10^{-4}$, $\\lambda_{\\max}=10^{2}$, $\\lambda^*=10^{-4}$, $c=0.05$, $N=20$.\n  - Case $3$: $\\lambda_{\\min}=10^{-6}$, $\\lambda_{\\max}=10^{0}$, $\\lambda^*=10^{-3}$, $c=0.10$, $N=1$.\n  - Case $4$: $\\lambda_{\\min}=10^{-3}$, $\\lambda_{\\max}=10^{-2}$, $\\lambda^*=5\\cdot 10^{-3}$, $c=0.20$, $N=10$.\n- Computation rule: Use natural logarithms in all computations.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the established validation criteria:\n- **Scientifically Grounded:** The problem is a well-established exercise in statistical learning theory, concerning the analysis of hyperparameter optimization strategies. The mathematical framework is standard probability theory. The performance model is a valid simplification used in theoretical analyses. This criterion is met.\n- **Well-Posed:** The problem is clearly specified. It provides all necessary data and definitions to derive the requested formulas and to perform the numerical computations. A unique, stable, and meaningful solution exists. This criterion is met.\n- **Objective:** The problem is stated in precise, mathematical language, free of any subjective or ambiguous terminology. This criterion is met.\n- **Completeness and Consistency:** The setup is self-contained and internally consistent. The relationship $R=c\\cdot D_N$ is a direct consequence of the definitions of $P(\\lambda)$ and $R$. All parameters for the test cases are provided. This criterion is met.\n- **Realism and Feasibility:** The problem is a theoretical model but is directly relevant to real-world challenges in machine learning. The parameter values are scientifically plausible. This criterion is met.\n- **Non-Triviality:** The derivation requires a rigorous application of probability theory, calculus, and transformation of random variables. It is a substantive problem that is not reducible to a trivial statement. This criterion is met.\n\n**Step 3: Verdict and Action**\n\nThe problem is determined to be **valid**. It is scientifically sound, well-posed, and complete. A full solution will be provided.\n\n### Derivation of the Expected Regret\n\nThe objective is to derive the expected regret, $\\mathbb{E}[R]$, for two different sampling schemes. As given, the regret $R$ is related to the minimum absolute log-distance $D_N$ by $R=c \\cdot D_N$. Due to the linearity of expectation, we have $\\mathbb{E}[R] = c \\cdot \\mathbb{E}[D_N]$.\n\nThe derivation of $\\mathbb{E}[D_N]$ will proceed from the fundamental identity for a non-negative random variable $Y$:\n$$ \\mathbb{E}[Y] = \\int_0^\\infty \\mathbb{P}(Y > t) \\, \\mathrm{d}t $$\nIn our case, the random variable is $D_N = \\min_{i=1, \\dots, N} |X_i - X^*|$, which is non-negative. Thus,\n$$ \\mathbb{E}[D_N] = \\int_0^\\infty \\mathbb{P}(D_N > t) \\, \\mathrm{d}t $$\nThe event $\\{D_N > t\\}$ is equivalent to the event that for all samples $i \\in \\{1, \\dots, N\\}$, the condition $|X_i - X^*| > t$ holds. The samples $X_i$ are drawn independently and are identically distributed (i.i.d.). Therefore, the probability of the joint event is the product of the individual probabilities:\n$$ \\mathbb{P}(D_N > t) = \\mathbb{P}\\left(\\bigcap_{i=1}^N \\{|X_i - X^*| > t\\}\\right) = \\prod_{i=1}^N \\mathbb{P}(|X_i - X^*| > t) = [\\mathbb{P}(|X_1 - X^*| > t)]^N $$\nLet $p(t)$ denote the probability that a single sample $X_1$ falls within a log-distance $t$ of the optimum $X^*$, i.e., $p(t) = \\mathbb{P}(|X_1 - X^*| \\le t)$. The complementary probability is $\\mathbb{P}(|X_1 - X^*| > t) = 1 - p(t)$. Substituting this into the expression for $\\mathbb{P}(D_N > t)$ gives:\n$$ \\mathbb{P}(D_N > t) = [1 - p(t)]^N $$\nThe search space for the log-parameter $X_1$ is the interval $[a, b] = [\\log(\\lambda_{\\min}), \\log(\\lambda_{\\max})]$. Since $X_1 \\in [a, b]$ and $X^* \\in [a, b]$, the maximum possible value for $|X_1 - X^*|$ is $t_{\\max} = \\max(X^* - a, b - X^*)$. For any $t > t_{\\max}$, the interval $[a, b]$ is fully contained within $[X^* - t, X^* + t]$. This implies that for $t > t_{\\max}$, $p(t) = \\mathbb{P}(X_1 \\in [a, b]) = 1$, and consequently $\\mathbb{P}(D_N > t) = [1 - 1]^N = 0$. The integral for the expected value can thus be restricted to the interval $[0, t_{\\max}]$:\n$$ \\mathbb{E}[D_N] = \\int_0^{t_{\\max}} [1 - p(t)]^N \\, \\mathrm{d}t $$\nWe now derive the specific form of $p(t)$ for each sampling scheme.\n\n**1. Uniform-in-Log Sampling**\n\nIn this scheme, $X_i = \\log(\\lambda_i)$ is drawn from a uniform distribution over $[a, b]$. The probability density function is $f_X(x) = 1/(b-a)$ for $x \\in [a, b]$.\nThe probability $p_{\\text{log}}(t) = \\mathbb{P}(|X_1 - X^*| \\le t)$ is the probability that $X_1$ falls in the interval $[X^* - t, X^* + t]$. Since $X_1$ is confined to $[a, b]$, we are interested in the probability of $X_1$ falling into the intersection of these two intervals: $I(t) = [X^* - t, X^* + t] \\cap [a, b]$.\nFor a uniform distribution, this probability is the ratio of the length of the intersection to the length of the total space:\n$$ p_{\\text{log}}(t) = \\frac{\\text{length}(I(t))}{b-a} = \\frac{\\text{length}([\\max(a, X^* - t), \\min(b, X^* + t)])}{b-a} = \\frac{\\min(b, X^* + t) - \\max(a, X^* - t)}{b-a} $$\nThe expected regret for uniform-in-log sampling is therefore:\n$$ \\mathbb{E}[R]_{\\text{log}} = c \\int_0^{t_{\\max}} \\left[1 - \\frac{\\min(b, X^* + t) - \\max(a, X^* - t)}{b-a}\\right]^N \\, \\mathrm{d}t $$\n\n**2. Uniform-in-Parameter Sampling**\n\nIn this scheme, $\\lambda_i$ is drawn from a uniform distribution over $[\\lambda_{\\min}, \\lambda_{\\max}]$. The probability density function is $f_\\lambda(\\ell) = 1/(\\lambda_{\\max} - \\lambda_{\\min})$ for $\\ell \\in [\\lambda_{\\min}, \\lambda_{\\max}]$.\nWe need to find $p_{\\text{uniform}}(t) = \\mathbb{P}(|X_1 - X^*| \\le t)$. Since $X_1 = \\log(\\lambda_1)$ and $X^* = \\log(\\lambda^*)$, this is equivalent to $\\mathbb{P}(\\log(\\lambda^*) - t \\le \\log(\\lambda_1) \\le \\log(\\lambda^*) + t)$, which simplifies to $\\mathbb{P}(\\lambda^* e^{-t} \\le \\lambda_1 \\le \\lambda^* e^t)$.\nThis probability is computed with respect to the uniform distribution of $\\lambda_1$ on $[\\lambda_{\\min}, \\lambda_{\\max}]$. The event corresponds to $\\lambda_1$ falling in the interval $J(t) = [\\lambda^* e^{-t}, \\lambda^* e^t] \\cap [\\lambda_{\\min}, \\lambda_{\\max}]$.\nThe probability is the ratio of the lengths of the intervals in the parameter space:\n$$ p_{\\text{uniform}}(t) = \\frac{\\text{length}(J(t))}{\\lambda_{\\max} - \\lambda_{\\min}} = \\frac{\\text{length}([\\max(\\lambda_{\\min}, \\lambda^* e^{-t}), \\min(\\lambda_{\\max}, \\lambda^* e^t)])}{\\lambda_{\\max} - \\lambda_{\\min}} $$\n$$ p_{\\text{uniform}}(t) = \\frac{\\min(\\lambda_{\\max}, \\lambda^* e^t) - \\max(\\lambda_{\\min}, \\lambda^* e^{-t})}{\\lambda_{\\max} - \\lambda_{\\min}} $$\nThe expected regret for uniform-in-parameter sampling is:\n$$ \\mathbb{E}[R]_{\\text{uniform}} = c \\int_0^{t_{\\max}} \\left[1 - \\frac{\\min(\\lambda_{\\max}, \\lambda^* e^t) - \\max(\\lambda_{\\min}, \\lambda^* e^{-t})}{\\lambda_{\\max} - \\lambda_{\\min}}\\right]^N \\, \\mathrm{d}t $$\nThese integral expressions allow for the numerical computation of the expected regret for both sampling strategies.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Computes the expected regret for hyperparameter tuning under two sampling schemes\n    for a suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda_min, lambda_max, lambda_star, c, N)\n        (10**-4, 10**2, 10**-1, 0.05, 20),\n        (10**-4, 10**2, 10**-4, 0.05, 20),\n        (10**-6, 10**0, 10**-3, 0.10, 1),\n        (10**-3, 10**-2, 5 * 10**-3, 0.20, 10),\n    ]\n\n    results = []\n    \n    for l_min, l_max, l_star, c, N in test_cases:\n        # Calculate log-space parameters. All logs are natural logarithms.\n        a = np.log(l_min)\n        b = np.log(l_max)\n        X_star = np.log(l_star)\n\n        # The upper limit of integration for t, the log-distance.\n        t_max = max(X_star - a, b - X_star)\n\n        # --- Integrand for Uniform-in-Log Sampling ---\n        def integrand_log(t, a, b, X_star, N):\n            \"\"\"\n            Computes the value of the integrand for the expected log-distance\n            under uniform-in-log sampling. The integrand is P(D_N > t).\n            \"\"\"\n            # Length of the intersection: [X*-t, X*+t] cap [a,b]\n            len_intersection = min(b, X_star + t) - max(a, X_star - t)\n            \n            # Probability p(t) of one sample falling within log-distance t\n            p_t = len_intersection / (b - a)\n            \n            # Probability that all N samples fall outside log-distance t\n            return (1.0 - p_t)**N\n\n        # --- Integrand for Uniform-in-Parameter Sampling ---\n        def integrand_uniform(t, l_min, l_max, l_star, N):\n            \"\"\"\n            Computes the value of the integrand for the expected log-distance\n            under uniform-in-parameter sampling.\n            \"\"\"\n            # Length of the intersection in the parameter space:\n            # [l_star*exp(-t), l_star*exp(t)] cap [l_min, l_max]\n            len_intersection = min(l_max, l_star * np.exp(t)) - max(l_min, l_star * np.exp(-t))\n            \n            # Probability p(t) that one sample's log falls within distance t\n            if l_max - l_min == 0:\n                p_t = 1.0 # Should not happen with valid inputs\n            else:\n                p_t = len_intersection / (l_max - l_min)\n\n            return (1.0 - p_t)**N\n\n        # --- Numerical Integration ---\n\n        # Expected log-distance for uniform-in-log sampling\n        E_DN_log, _ = quad(\n            integrand_log, 0, t_max, args=(a, b, X_star, N)\n        )\n        # Expected regret\n        E_R_log = c * E_DN_log\n\n        # Expected log-distance for uniform-in-parameter sampling\n        E_DN_uniform, _ = quad(\n            integrand_uniform, 0, t_max, args=(l_min, l_max, l_star, N)\n        )\n        # Expected regret\n        E_R_uniform = c * E_DN_uniform\n        \n        # Difference in expected regrets\n        diff = E_R_uniform - E_R_log\n\n        results.extend([E_R_uniform, E_R_log, diff])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.16g}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world hyperparameter spaces are rarely simple hypercubes; they often possess a conditional structure where the choice of one hyperparameter (e.g., a kernel type) activates or deactivates others. This practice  tackles this complexity head-on, challenging you to analyze a conditional search space inspired by Support Vector Machines. You will compare the combinatorial explosion faced by an exhaustive grid search with the flexible, budget-based approach of random search, exploring advanced concepts like the probability of covering all model sub-families within a given budget.",
            "id": "3129431",
            "problem": "You are given a finite conditional hyperparameter space motivated by Support Vector Machine (SVM) kernels. The global hyperparameters consist of a kernel choice $k \\in \\{\\text{linear}, \\text{radial basis function (RBF)}, \\text{polynomial}\\}$ and a regularization parameter $C \\in S_C$. Conditional hyperparameters are activated based on the chosen kernel: if $k=\\text{RBF}$ then the parameter $\\gamma \\in S_\\gamma$ is tuned, if $k=\\text{polynomial}$ then the parameters $\\text{degree} \\in S_{\\text{degree}}$ and $\\text{coef0} \\in S_{\\text{coef0}}$ are tuned, and if $k=\\text{linear}$ then there are no additional conditional parameters. All sets $S_C$, $S_\\gamma$, $S_{\\text{degree}}$, and $S_{\\text{coef0}}$ are finite and may be empty. A configuration is valid if and only if all hyperparameters required by the chosen kernel have values from their corresponding nonempty sets.\n\nStart from the following fundamental bases:\n- The cardinality of a Cartesian product, for finite sets $A$ and $B$, is $|A \\times B| = |A| \\cdot |B|$.\n- Grid search enumerates the entire Cartesian product of all applicable hyperparameter sets for each kernel.\n- Random search draws independent samples uniformly at random from the set of valid configurations (equivalently, draw $k$ according to the group-size weights, then draw each applicable hyperparameter uniformly from its finite set).\n\nUsing these bases, derive and implement the following in a single, complete program:\n1. Compute the total number of valid grid configurations $N_{\\text{tot}}$ and the per-kernel configuration counts\n   $$N_{\\text{linear}} = |S_C|,\\quad N_{\\text{RBF}} = |S_C| \\cdot |S_\\gamma|,\\quad N_{\\text{poly}} = |S_C| \\cdot |S_{\\text{degree}}| \\cdot |S_{\\text{coef0}}|.$$\n   Kernels with a zero count (because some required set is empty) contribute nothing to the valid configuration space.\n2. Under uniform random sampling over valid configurations, compute the kernel-selection probabilities $p_i = N_i / N_{\\text{tot}}$ over kernels $i$ with $N_i > 0$.\n3. For a given integer budget $B \\ge 0$, compute the probability $P_{\\text{cover}}(B)$ that after $B$ independent samples you have observed at least one configuration from each kernel group with $N_i > 0$. Use the inclusion–exclusion principle: if $A_i$ denotes the event “no draw from kernel $i$ in $B$ samples,” then\n   $$P\\big(A_i\\big) = (1 - p_i)^B,$$\n   $$P\\Big(\\bigcap_{i \\in S} A_i\\Big) = \\left(1 - \\sum_{i \\in S} p_i\\right)^B,$$\n   and\n   $$P_{\\text{miss}}(B) = \\sum_{\\emptyset \\ne S \\subseteq \\{i : N_i > 0\\}} (-1)^{|S|+1} \\left(1 - \\sum_{i \\in S} p_i\\right)^B,$$\n   $$P_{\\text{cover}}(B) = 1 - P_{\\text{miss}}(B).$$\n   All probabilities must be expressed as decimals (not with a percentage sign).\n4. Compute the expected number of distinct configurations visited after $B$ uniform samples:\n   $$\\mathbb{E}[D(B)] = N_{\\text{tot}} \\left(1 - \\left(1 - \\frac{1}{N_{\\text{tot}}}\\right)^B\\right).$$\n5. Compare the computational work of exhaustive grid enumeration to random search via the ratio\n   $$r(B) = \\frac{N_{\\text{tot}}}{B}.$$\n   If $B = 0$, define $r(0) = \\infty$.\n6. For a given coverage threshold $\\tau \\in (0,1)$, compute the minimal integer budget $B_{\\min}$ such that $P_{\\text{cover}}(B_{\\min}) \\ge \\tau$.\n\nYour program must implement the above computations for each of the test cases below and produce a single line of output containing a comma-separated list of per-test-case results, each result being a list of the form $[N_{\\text{tot}}, P_{\\text{cover}}(B), r(B), \\mathbb{E}[D(B)], B_{\\min}]$.\n\nTest Suite:\n- Case $1$ (happy path):\n  $S_C = \\{0.1, 1.0, 10.0\\}$,\n  $S_\\gamma = \\{0.001, 0.01, 0.1, 1.0\\}$,\n  $S_{\\text{degree}} = \\{2, 3, 4\\}$,\n  $S_{\\text{coef0}} = \\{0.0, 1.0\\}$,\n  $B = 50$,\n  $\\tau = 0.95$.\n- Case $2$ (edge: one kernel invalid, zero budget):\n  $S_C = \\{0.5, 2.0\\}$,\n  $S_\\gamma = \\varnothing$,\n  $S_{\\text{degree}} = \\{2, 3\\}$,\n  $S_{\\text{coef0}} = \\{0.0\\}$,\n  $B = 0$,\n  $\\tau = 0.5$.\n- Case $3$ (edge: only linear kernel valid):\n  $S_C = \\{1.0, 5.0, 25.0, 125.0\\}$,\n  $S_\\gamma = \\varnothing$,\n  $S_{\\text{degree}} = \\varnothing$,\n  $S_{\\text{coef0}} = \\varnothing$,\n  $B = 1$,\n  $\\tau = 0.9$.\n- Case $4$ (larger space):\n  $S_C = \\{0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0\\}$,\n  $S_\\gamma$ consists of $50$ positive values (e.g., logarithmically spaced over a plausible range),\n  $S_{\\text{degree}} = \\{2, 3, 4, 5, 6\\}$,\n  $S_{\\text{coef0}} = \\{0.0, 0.1, 0.5, 1.0\\}$,\n  $B = 1000$,\n  $\\tau = 0.9$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, with no spaces, in the form\n$$[[N_{\\text{tot}},P_{\\text{cover}}(B),r(B),\\mathbb{E}[D(B)],B_{\\min}],\\ldots].$$",
            "solution": "We begin with the finite hyperparameter sets and the kernel-conditioned activation of parameters. By definition, grid search enumerates all valid configurations by taking the Cartesian product of applicable sets for each kernel, then summing these products across kernels. Using the fundamental cardinality rule for Cartesian products $|A \\times B| = |A| \\cdot |B|$, we obtain\n$$N_{\\text{linear}} = |S_C|,$$\n$$N_{\\text{RBF}} = |S_C| \\cdot |S_\\gamma|,$$\n$$N_{\\text{poly}} = |S_C| \\cdot |S_{\\text{degree}}| \\cdot |S_{\\text{coef0}}|,$$\nand the total number of valid configurations\n$$N_{\\text{tot}} = N_{\\text{linear}} + N_{\\text{RBF}} + N_{\\text{poly}}.$$\nAny kernel whose required conditional set is empty contributes $0$ to its count, thereby removing it from the valid configuration space.\n\nRandom search uniformly sampling over valid configurations can be realized conditionally: first select a kernel according to the probability proportional to its configuration count, then uniformly select its active hyperparameters. The kernel-selection probabilities are\n$$p_i = \\frac{N_i}{N_{\\text{tot}}},\\quad \\text{for kernels } i \\text{ with } N_i > 0.$$\nGiven $B$ independent samples and defining the event $A_i$ as “no sample from kernel $i$,” the probability for $A_i$ is the complement of selecting kernel $i$ in each of $B$ draws:\n$$P(A_i) = (1 - p_i)^B.$$\nFor any nonempty index set $S$ of kernels, the event that none of the kernels in $S$ appear in the $B$ samples is the intersection $\\bigcap_{i \\in S} A_i$, whose probability equals the chance that each of the $B$ draws lands outside the entire union of kernels in $S$. Since draws are independent and the union has total probability $\\sum_{i \\in S} p_i$, we have\n$$P\\Big(\\bigcap_{i \\in S} A_i\\Big) = \\left(1 - \\sum_{i \\in S} p_i\\right)^B.$$\nBy the inclusion–exclusion principle, the probability of missing at least one kernel among the active groups is\n$$P_{\\text{miss}}(B) = \\sum_{\\emptyset \\ne S \\subseteq \\{i : N_i > 0\\}} (-1)^{|S|+1} \\left(1 - \\sum_{i \\in S} p_i\\right)^B,$$\nand the coverage probability is the complement\n$$P_{\\text{cover}}(B) = 1 - P_{\\text{miss}}(B).$$\nWhen there is only one active kernel group with $p_1 = 1$, we verify that $P_{\\text{cover}}(B) = 1 - (1 - 1)^B = 1$ for any $B \\ge 1$, and $P_{\\text{cover}}(0) = 0$, which aligns with intuition.\n\nTo quantify the expected number of distinct configurations visited after $B$ samples, we use indicator random variables and linearity of expectation. Let $X_j$ be $1$ if configuration $j$ is visited at least once in $B$ samples and $0$ otherwise. The sample distribution is uniform over $N_{\\text{tot}}$ configurations, so the probability that configuration $j$ is never selected in $B$ draws is $\\left(1 - \\frac{1}{N_{\\text{tot}}}\\right)^B$. Therefore,\n$$\\mathbb{E}[X_j] = 1 - \\left(1 - \\frac{1}{N_{\\text{tot}}}\\right)^B,$$\nand summing over all configurations gives\n$$\\mathbb{E}[D(B)] = \\sum_{j=1}^{N_{\\text{tot}}} \\mathbb{E}[X_j] = N_{\\text{tot}} \\left(1 - \\left(1 - \\frac{1}{N_{\\text{tot}}}\\right)^B\\right).$$\n\nTo compare complexities, exhaustive grid search evaluates all $N_{\\text{tot}}$ configurations, while random search evaluates $B$ samples. A simple ratio measuring relative work is\n$$r(B) = \\frac{N_{\\text{tot}}}{B},$$\nand we define $r(0) = \\infty$ when $B = 0$.\n\nFor a specified coverage threshold $\\tau \\in (0,1)$, we seek the minimal budget $B_{\\min}$ such that $P_{\\text{cover}}(B_{\\min}) \\ge \\tau$. Since $P_{\\text{cover}}(B)$ is nondecreasing in $B$, we can find $B_{\\min}$ by incrementing $B$ from $0$ upward and stopping at the first $B$ satisfying the inequality. This is computationally tractable given the small number of kernel groups, because the inclusion–exclusion computation scales with the number of groups rather than the number of configurations.\n\nAlgorithmic design:\n- Compute $N_{\\text{linear}}$, $N_{\\text{RBF}}$, $N_{\\text{poly}}$, and $N_{\\text{tot}}$.\n- Construct the list of active group probabilities $p_i$ for kernels with $N_i > 0$.\n- Implement inclusion–exclusion over all nonempty subsets of active kernel indices to compute $P_{\\text{miss}}(B)$ and hence $P_{\\text{cover}}(B)$.\n- Compute $\\mathbb{E}[D(B)]$ using the occupancy formula above.\n- Compute $r(B)$, with the convention $r(0) = \\infty$.\n- Increment $B$ to find $B_{\\min}$ satisfying $P_{\\text{cover}}(B_{\\min}) \\ge \\tau$.\n\nWe then evaluate these for the test suite, including a normal case, a zero-budget boundary case with one invalid kernel, a single-kernel edge case, and a large-space case, and output the per-case results in the specified single-line nested list format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef per_kernel_counts(S_C, S_gamma, S_degree, S_coef0):\n    \"\"\"Compute per-kernel and total configuration counts.\"\"\"\n    nC = len(S_C)\n    nGamma = len(S_gamma)\n    nDeg = len(S_degree)\n    nCoef0 = len(S_coef0)\n\n    n_linear = nC\n    n_rbf = nC * nGamma if nGamma > 0 else 0\n    n_poly = nC * nDeg * nCoef0 if (nDeg > 0 and nCoef0 > 0) else 0\n    n_tot = n_linear + n_rbf + n_poly\n    return n_linear, n_rbf, n_poly, n_tot\n\ndef coverage_probability(p_list, B):\n    \"\"\"\n    Compute P_cover(B) using inclusion-exclusion over nonempty subsets of active groups.\n    p_list: list of group probabilities (only for groups with >0 configs).\n    \"\"\"\n    K = len(p_list)\n    if K == 0:\n        # No valid groups: by convention, coverage of \"all active groups\" is 1 (vacuous truth),\n        # but there are no valid configurations. We'll treat coverage as 1 if B >= 0.\n        return 1.0\n    # If B == 0, coverage is 0 unless K == 0 (handled above).\n    # Inclusion-exclusion for P_miss:\n    p_array = np.array(p_list, dtype=float)\n    P_miss = 0.0\n    # iterate over all nonempty subsets via bit masks\n    for mask in range(1, 1 << K):\n        # count bits\n        m = bin(mask).count(\"1\")\n        # sum of p_i over subset\n        sum_p = float(np.sum(p_array[[i for i in range(K) if (mask >> i) & 1]]))\n        term = (1.0 - sum_p) ** B\n        # sign = (-1)^{|S|+1}\n        if (m % 2) == 1:\n            P_miss += term\n        else:\n            P_miss -= term\n    P_cover = 1.0 - P_miss\n    # Numerical guard: clamp to [0,1]\n    if P_cover < 0.0:\n        P_cover = 0.0\n    if P_cover > 1.0:\n        P_cover = 1.0\n    return P_cover\n\ndef expected_distinct(n_tot, B):\n    \"\"\"Expected number of distinct configurations visited after B uniform samples.\"\"\"\n    if n_tot == 0:\n        return 0.0\n    return float(n_tot * (1.0 - (1.0 - 1.0 / n_tot) ** B))\n\ndef ratio_grid_to_random(n_tot, B):\n    \"\"\"Compute r = n_tot / B, with r(0) = inf.\"\"\"\n    if B == 0:\n        return float('inf')\n    return float(n_tot) / float(B)\n\ndef minimal_budget_for_threshold(p_list, tau):\n    \"\"\"\n    Minimal integer B >= 0 s.t. coverage_probability(p_list, B) >= tau.\n    \"\"\"\n    # Handle trivial case: no active groups -> coverage is 1 for all B\n    if len(p_list) == 0:\n        return 0\n    B = 0\n    # Upper bound to prevent infinite loops in degenerate scenarios.\n    # Since coverage approaches 1 exponentially fast when any p_i > 0,\n    # we set a generous cap.\n    cap = 100000\n    while B <= cap:\n        pc = coverage_probability(p_list, B)\n        if pc >= tau:\n            return B\n        B += 1\n    # If not found within cap, return cap (worst case fallback).\n    return cap\n\ndef format_nested_list(list_of_lists):\n    \"\"\"\n    Format a list of lists without spaces as required: [[...],[...],...]\n    \"\"\"\n    inner_strs = []\n    for lst in list_of_lists:\n        # Convert each element to string as-is (floats, ints, inf)\n        elems = \",\".join(map(str, lst))\n        inner_strs.append(f\"[{elems}]\")\n    return f\"[{','.join(inner_strs)}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Case 1\n    S_C_1 = [0.1, 1.0, 10.0]\n    S_gamma_1 = [0.001, 0.01, 0.1, 1.0]\n    S_degree_1 = [2, 3, 4]\n    S_coef0_1 = [0.0, 1.0]\n    B_1 = 50\n    tau_1 = 0.95\n\n    # Case 2\n    S_C_2 = [0.5, 2.0]\n    S_gamma_2 = []  # empty -> RBF invalid\n    S_degree_2 = [2, 3]\n    S_coef0_2 = [0.0]\n    B_2 = 0\n    tau_2 = 0.5\n\n    # Case 3\n    S_C_3 = [1.0, 5.0, 25.0, 125.0]\n    S_gamma_3 = []\n    S_degree_3 = []\n    S_coef0_3 = []\n    B_3 = 1\n    tau_3 = 0.9\n\n    # Case 4\n    S_C_4 = [0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0, 100.0]\n    # 50 gamma values, logarithmically spaced\n    S_gamma_4 = list(np.logspace(-4, 2, 50))\n    S_degree_4 = [2, 3, 4, 5, 6]\n    S_coef0_4 = [0.0, 0.1, 0.5, 1.0]\n    B_4 = 1000\n    tau_4 = 0.9\n\n    test_cases = [\n        (S_C_1, S_gamma_1, S_degree_1, S_coef0_1, B_1, tau_1),\n        (S_C_2, S_gamma_2, S_degree_2, S_coef0_2, B_2, tau_2),\n        (S_C_3, S_gamma_3, S_degree_3, S_coef0_3, B_3, tau_3),\n        (S_C_4, S_gamma_4, S_degree_4, S_coef0_4, B_4, tau_4),\n    ]\n\n    results = []\n    for S_C, S_gamma, S_degree, S_coef0, B, tau in test_cases:\n        n_linear, n_rbf, n_poly, n_tot = per_kernel_counts(S_C, S_gamma, S_degree, S_coef0)\n        # Active group probabilities\n        groups = [n_linear, n_rbf, n_poly]\n        active_counts = [g for g in groups if g > 0]\n        if n_tot > 0:\n            p_list = [float(g) / float(n_tot) for g in active_counts]\n        else:\n            p_list = []\n\n        P_cover = coverage_probability(p_list, B)\n        r_ratio = ratio_grid_to_random(n_tot, B)\n        E_distinct = expected_distinct(n_tot, B)\n        B_min = minimal_budget_for_threshold(p_list, tau)\n\n        results.append([n_tot, P_cover, r_ratio, E_distinct, B_min])\n\n    # Final print statement in the exact required format.\n    print(format_nested_list(results))\n\nsolve()\n```"
        }
    ]
}