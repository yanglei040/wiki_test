{
    "hands_on_practices": [
        {
            "introduction": "To truly appreciate the necessity of stratified cross-validation, it is essential to understand what can go wrong without it. In datasets with significant class imbalance, a simple random split can easily result in folds that are not representative of the overall class distribution, leading to unreliable performance estimates. This exercise provides a rigorous, quantitative justification for stratification by tasking you with modeling the \"error inflation\"—the expected increase in misclassification risk—that arises from the variability of class priors in non-stratified folds .",
            "id": "3134712",
            "problem": "Consider a binary classification problem with severe class imbalance. Let the class label be $Y \\in \\{0,1\\}$ with population prior $\\mathbb{P}(Y=1)=\\pi$ and $\\mathbb{P}(Y=0)=1-\\pi$, where $\\pi \\ll 0.5$. Given a single real-valued feature $X$, suppose the class-conditional distributions are Gaussian with equal variance: $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$ and $X \\mid Y=1 \\sim \\mathcal{N}(\\mu,1)$, with $\\mu>0$ known. Consider $k$-fold Cross-Validation (CV) performed on a dataset of size $N$, with $k$ equally sized folds.\n\nUse the following fundamental basis:\n- The Bayes classifier under known class-conditional densities and prior uses the likelihood ratio test, which reduces to a threshold rule on $X$ when the variances are equal.\n- The misclassification risk is defined as the population error $\\mathbb{P}(\\hat{Y}\\neq Y)$, computed under the true data distribution.\n- A non-stratified $k$-fold split is a random partition of the $N$ samples into $k$ folds without regard to class labels.\n- A stratified $k$-fold split ensures each fold reflects the population class proportion as closely as possible. In this problem, assume exact stratification: $N\\pi$ is an integer and each fold contains exactly $N\\pi/k$ samples from the minority class and $N(1-\\pi)/k$ from the majority class.\n\nDefine the classifier trained within a fold as the likelihood-ratio threshold rule that uses the empirical class prior $\\hat{\\pi}$ estimated from the training portion of that fold, while using the true, known class-conditional densities. Specifically, the decision rule is \"predict $Y=1$ if $X \\ge t(\\hat{\\pi})$ and $Y=0$ otherwise\", where $t(\\hat{\\pi})$ is the threshold implied by the likelihood ratio test with prior $\\hat{\\pi}$. The fold’s test error is measured on data drawn from the same population.\n\nYou must:\n1. Derive from first principles the decision threshold $t(\\hat{\\pi})$ for the Gaussian equal-variance case starting from the likelihood ratio test, and the corresponding misclassification risk $R(t)$ as a function of $t$, $\\pi$, and $\\mu$.\n2. Under non-stratified $k$-fold, model the training prior $\\hat{\\pi}$ as a random variable induced by the random fold assignment. Let $M=N\\pi$ be the total number of minority samples (an integer by assumption), and $n_{\\text{train}}=N(k-1)/k$ be the training size per fold. For a given fold, the number of minority samples in the training set, $m$, follows a Hypergeometric distribution with parameters $(N, M, n_{\\text{train}})$. Therefore, $\\hat{\\pi}=m/n_{\\text{train}}$. Using this, compute the expected non-stratified CV risk as the expectation of $R\\big(t(\\hat{\\pi})\\big)$ with respect to the Hypergeometric distribution of $m$.\n3. Under stratified $k$-fold with exact per-fold class proportions, the training prior equals the population prior $\\pi$ exactly, hence the stratified CV risk equals $R\\big(t(\\pi)\\big)$.\n4. Quantify the error inflation $\\Delta$ due to non-stratification as\n$$\n\\Delta = \\mathbb{E}\\left[R\\big(t(\\hat{\\pi})\\big)\\right] - R\\big(t(\\pi)\\big).\n$$\nCompute $\\Delta$ for each test case specified below.\n\nYou must implement a complete, runnable program that, for each parameter tuple $(N,k,\\pi,\\mu)$, computes the single-fold expected non-stratified CV risk via the exact Hypergeometric distribution, the stratified CV risk, and outputs the inflation $\\Delta$ as a float. Handle the degenerate cases $\\hat{\\pi}=0$ and $\\hat{\\pi}=1$ correctly by interpreting the threshold rule limits: if $\\hat{\\pi}=0$, the classifier predicts $Y=0$ always, yielding risk $R=\\pi$; if $\\hat{\\pi}=1$, the classifier predicts $Y=1$ always, yielding risk $R=1-\\pi$.\n\nTest Suite:\n- Case $1$ (happy path): $(N,k,\\pi,\\mu) = (500,5,0.05,2.0)$.\n- Case $2$ (small data, still severely imbalanced): $(N,k,\\pi,\\mu) = (100,10,0.10,1.5)$.\n- Case $3$ (large data, extreme imbalance): $(N,k,\\pi,\\mu) = (5000,10,0.01,1.0)$.\n- Case $4$ (moderate data, strong separation): $(N,k,\\pi,\\mu) = (1200,4,0.03,3.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is the computed $\\Delta$ for Case $i$, rounded to $6$ decimal places. No other text should be printed.",
            "solution": "The problem requires us to quantify the error inflation, denoted by $\\Delta$, that arises from using non-stratified $k$-fold cross-validation compared to stratified $k$-fold cross-validation for a binary classifier with a severe class imbalance. The analysis is based on a specific theoretical setup involving Gaussian class-conditional distributions. We will proceed by first deriving the necessary theoretical components, then applying them to the specified cross-validation schemes.\n\n### 1. Decision Threshold and Misclassification Risk\n\nThe classifier's decision is based on the likelihood ratio test, which compares the posterior probabilities of the two classes. For a given feature value $X$, the classifier predicts class $Y=1$ if $\\mathbb{P}(Y=1|X) > \\mathbb{P}(Y=0|X)$. Using Bayes' theorem, this is equivalent to $\\frac{p(X|Y=1)\\mathbb{P}(Y=1)}{p(X)} > \\frac{p(X|Y=0)\\mathbb{P}(Y=0)}{p(X)}$, which simplifies to the likelihood ratio test:\n$$\n\\frac{p(X|Y=1)}{p(X|Y=0)} > \\frac{\\mathbb{P}(Y=0)}{\\mathbb{P}(Y=1)}\n$$\nThe problem specifies that the classifier uses the true class-conditional densities but estimates the prior probability $\\mathbb{P}(Y=1)$ with its empirical estimate $\\hat{\\pi}$ from the training data. The decision rule thus becomes:\n$$\n\\frac{p(X|Y=1)}{p(X|Y=0)} > \\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\n$$\nThe class-conditional densities are given as $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$ and $X \\mid Y=1 \\sim \\mathcal{N}(\\mu,1)$. Their probability density functions (PDFs) are:\n$$\np(X|Y=0) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{X^2}{2}\\right)\n$$\n$$\np(X|Y=1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(X-\\mu)^2}{2}\\right)\n$$\nThe likelihood ratio is:\n$$\n\\frac{p(X|Y=1)}{p(X|Y=0)} = \\frac{\\exp\\left(-\\frac{(X-\\mu)^2}{2}\\right)}{\\exp\\left(-\\frac{X^2}{2}\\right)} = \\exp\\left(\\frac{X^2 - (X-\\mu)^2}{2}\\right) = \\exp\\left(\\frac{X^2 - (X^2 - 2\\mu X + \\mu^2)}{2}\\right) = \\exp\\left(\\mu X - \\frac{\\mu^2}{2}\\right)\n$$\nSubstituting this into the decision rule inequality:\n$$\n\\exp\\left(\\mu X - \\frac{\\mu^2}{2}\\right) > \\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\n$$\nTaking the natural logarithm of both sides (a monotonic transformation that preserves the inequality):\n$$\n\\mu X - \\frac{\\mu^2}{2} > \\ln\\left(\\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\\right)\n$$\nSince $\\mu > 0$ is given, we can solve for $X$ to find the decision threshold $t(\\hat{\\pi})$:\n$$\nX > \\frac{1}{\\mu}\\left(\\frac{\\mu^2}{2} + \\ln\\left(\\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\\right)\\right)\n$$\nThus, the decision threshold as a function of the empirical prior $\\hat{\\pi}$ is:\n$$\nt(\\hat{\\pi}) = \\frac{\\mu}{2} + \\frac{1}{\\mu}\\ln\\left(\\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\\right)\n$$\nThe classifier's rule is to predict $Y=1$ if $X \\geq t(\\hat{\\pi})$ and $Y=0$ otherwise.\n\nNext, we derive the misclassification risk $R(t)$ for a given threshold $t$. The risk is the population error rate, evaluated using the true population prior $\\pi$. It is the sum of the probabilities of Type I and Type II errors, weighted by the true class priors:\n$$\nR(t) = \\mathbb{P}(\\text{predict } 1 \\mid Y=0)\\mathbb{P}(Y=0) + \\mathbb{P}(\\text{predict } 0 \\mid Y=1)\\mathbb{P}(Y=1)\n$$\n$$\nR(t) = \\mathbb{P}(X \\ge t \\mid Y=0)(1-\\pi) + \\mathbb{P}(X < t \\mid Y=1)\\pi\n$$\nLet $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$.\nFor the first term, since $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$, we have $\\mathbb{P}(X \\ge t \\mid Y=0) = 1 - \\Phi(t)$.\nFor the second term, since $X \\mid Y=1 \\sim \\mathcal{N}(\\mu,1)$, the variable $X-\\mu$ follows $\\mathcal{N}(0,1)$. Thus, $\\mathbb{P}(X < t \\mid Y=1) = \\mathbb{P}(X-\\mu < t-\\mu \\mid Y=1) = \\Phi(t-\\mu)$.\nThe misclassification risk is therefore:\n$$\nR(t) = (1-\\pi)(1 - \\Phi(t)) + \\pi \\Phi(t-\\mu)\n$$\n\n### 2. Stratified vs. Non-Stratified Cross-Validation Risk\n\n**Stratified CV Risk:**\nIn a stratified $k$-fold split, each fold is constructed to have the same class proportion as the overall dataset. The training set for any fold, which comprises $k-1$ folds, will therefore also have this exact proportion. Thus, the empirical prior $\\hat{\\pi}$ estimated from the training set is always equal to the population prior $\\pi$.\n$$\n\\hat{\\pi}_{\\text{strat}} = \\pi\n$$\nThe decision threshold is constant for all folds: $t_{\\text{strat}} = t(\\pi) = \\frac{\\mu}{2} + \\frac{1}{\\mu}\\ln\\left(\\frac{1-\\pi}{\\pi}\\right)$. This is the optimal Bayes threshold. The resulting risk, which we denote as $R_{\\text{strat}}$, is the Bayes error rate:\n$$\nR_{\\text{strat}} = R(t(\\pi)) = (1-\\pi)(1 - \\Phi(t(\\pi))) + \\pi \\Phi(t(\\pi)-\\mu)\n$$\n\n**Non-Stratified CV Risk:**\nIn a non-stratified split, the data is partitioned randomly into $k$ folds. The number of minority class samples in the training data for a fold is a random variable. Let $N$ be the total sample size, $M=N\\pi$ be the total number of minority samples, and $n_{\\text{train}} = N(k-1)/k$ be the training set size. The number of minority samples in the training set, $m$, follows a Hypergeometric distribution with parameters for population size ($N$), number of successes in population ($M$), and number of draws ($n_{\\text{train}}$). We write this as $m \\sim \\text{Hypergeometric}(N, M, n_{\\text{train}})$.\nThe probability mass function (PMF) is $\\mathbb{P}(m) = \\frac{\\binom{M}{m}\\binom{N-M}{n_{\\text{train}}-m}}{\\binom{N}{n_{\\text{train}}}}$.\nThe empirical prior for a given fold is $\\hat{\\pi} = m/n_{\\text{train}}$, which is a random variable. The decision threshold $t(\\hat{\\pi})$ and the corresponding risk $R(t(\\hat{\\pi}))$ are also random variables. The expected non-stratified CV risk is the expectation of this risk over the distribution of $m$:\n$$\n\\mathbb{E}\\left[R\\big(t(\\hat{\\pi})\\big)\\right] = \\sum_{m} \\mathbb{P}(m) \\cdot R\\left(t\\left(\\frac{m}{n_{\\text{train}}}\\right)\\right)\n$$\nThe summation is over the support of the Hypergeometric distribution, $m \\in [\\max(0, n_{\\text{train}} - (N-M)), \\min(n_{\\text{train}}, M)]$.\n\nThe problem specifies handling for the edge cases where $\\hat{\\pi}=0$ or $\\hat{\\pi}=1$:\n-   If $\\hat{\\pi}=0$ (i.e., $m=0$), $\\ln((1-\\hat{\\pi})/\\hat{\\pi}) \\to \\infty$, so $t(0) \\to \\infty$. The classifier always predicts $Y=0$. The risk is $R(\\infty) = (1-\\pi)(1-\\Phi(\\infty)) + \\pi \\Phi(\\infty-\\mu) = (1-\\pi) \\cdot 0 + \\pi \\cdot 1 = \\pi$.\n-   If $\\hat{\\pi}=1$ (i.e., $m=n_{\\text{train}}$), $\\ln((1-\\hat{\\pi})/\\hat{\\pi}) \\to -\\infty$, so $t(1) \\to -\\infty$. The classifier always predicts $Y=1$. The risk is $R(-\\infty) = (1-\\pi)(1-\\Phi(-\\infty)) + \\pi \\Phi(-\\infty-\\mu) = (1-\\pi) \\cdot 1 + \\pi \\cdot 0 = 1-\\pi$.\n\n### 3. Error Inflation $\\Delta$\n\nThe error inflation $\\Delta$ is defined as the difference between the expected risk under non-stratified CV and the risk under stratified CV:\n$$\n\\Delta = \\mathbb{E}\\left[R\\big(t(\\hat{\\pi})\\big)\\right] - R_{\\text{strat}}\n$$\nThis quantity captures the average increase in misclassification error due to the variability of the empirical prior estimate in non-stratified folds. This variability pushes the decision threshold away from the optimal Bayes threshold, and due to the convexity of the risk function around its minimum (the Bayes error), the expected risk is higher than the risk at the expected prior. By Jensen's inequality, since the risk function is convex, $\\mathbb{E}[R(t(\\hat{\\pi}))] \\ge R(t(\\mathbb{E}[\\hat{\\pi}]))$. Since $\\mathbb{E}[\\hat{\\pi}] = \\pi$, we expect $\\Delta \\ge 0$. Our task is to compute this value for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import hypergeom, norm\n\ndef solve():\n    \"\"\"\n    Computes the error inflation delta for a series of test cases.\n    \"\"\"\n    \n    # Test cases: tuples of (N, k, pi, mu)\n    test_cases = [\n        (500, 5, 0.05, 2.0),\n        (100, 10, 0.10, 1.5),\n        (5000, 10, 0.01, 1.0),\n        (1200, 4, 0.03, 3.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, k, pi, mu = case\n        \n        # Ensure N*pi is an integer as per problem statement\n        M = int(round(N * pi))\n        n_train = int(N * (k - 1) / k)\n        \n        # --- 1. Calculate Stratified CV Risk (R_strat) ---\n        # The empirical prior is the true prior\n        pi_strat = pi\n        \n        # Calculate the Bayes optimal threshold t(pi)\n        # logit(pi) = log(pi / (1-pi)), so -log((1-pi)/pi)\n        t_strat = mu / 2.0 - (1.0 / mu) * np.log(pi_strat / (1.0 - pi_strat))\n        \n        # Calculate the risk R(t(pi)), which is the Bayes error rate\n        risk_type1_strat = 1.0 - norm.cdf(t_strat)\n        risk_type2_strat = norm.cdf(t_strat - mu)\n        R_strat = (1.0 - pi) * risk_type1_strat + pi * risk_type2_strat\n        \n        # --- 2. Calculate Expected Non-Stratified CV Risk (E_non_strat) ---\n        \n        # The number of minority samples 'm' in the training set follows a\n        # Hypergeometric distribution.\n        # Scipy's hypergeom(M, n, N) takes:\n        # M: total number of objects (our N)\n        # n: total number of type I objects (our M)\n        # N: number of draws (our n_train)\n        dist = hypergeom(M=N, n=M, N=n_train)\n        \n        # Support of the hypergeometric distribution for m\n        m_min = max(0, n_train - (N - M))\n        m_max = min(n_train, M)\n        \n        E_non_strat = 0.0\n        \n        for m in range(m_min, m_max + 1):\n            prob_m = dist.pmf(m)\n            \n            # If there's no probability mass, skip to avoid unnecessary calculation\n            if prob_m == 0:\n                continue\n\n            pi_hat = m / n_train\n            \n            # Handle edge cases as per problem statement\n            if m == 0:  # pi_hat = 0\n                risk_m = pi\n            elif m == n_train:  # pi_hat = 1\n                risk_m = 1.0 - pi\n            else:\n                # Calculate threshold t(pi_hat) for this m\n                t_m = mu / 2.0 - (1.0 / mu) * np.log(pi_hat / (1.0 - pi_hat))\n                \n                # Calculate risk R(t(pi_hat))\n                risk_type1_m = 1.0 - norm.cdf(t_m)\n                risk_type2_m = norm.cdf(t_m - mu)\n                risk_m = (1.0 - pi) * risk_type1_m + pi * risk_type2_m\n            \n            E_non_strat += prob_m * risk_m\n            \n        # --- 3. Compute Error Inflation Delta ---\n        delta = E_non_strat - R_strat\n        results.append(round(delta, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While stratification is a powerful technique, it is not without its practical limitations, especially when dealing with extremely rare classes. If a class has too few samples, it may be impossible to guarantee its presence in both the training and validation set for every fold, which can render metrics like precision and recall undefined. This practice explores the operational boundaries of stratified $k$-fold cross-validation, guiding you to derive the minimum per-class sample size, $n_c$, required for a robust evaluation and to propose a smoothing technique to handle cases that fall below this threshold .",
            "id": "3177513",
            "problem": "A dataset for a multi-class classification task contains a particular class labeled $c$ with total count $n_c$. You plan to evaluate a classifier using stratified $k$-fold cross-validation, where each fold uses one stratified split as the validation set and the remaining $k-1$ splits as the training set. Stratification means each fold’s validation set receives either $\\lfloor n_c/k \\rfloor$ or $\\lceil n_c/k \\rceil$ instances of class $c$, and the corresponding training set receives the complementary count of class $c$.\n\nPer-class recall for class $c$ computed on a validation fold is defined from the confusion matrix entries as $\\mathrm{Recall}_c = \\frac{\\mathrm{TP}_c}{\\mathrm{TP}_c + \\mathrm{FN}_c}$, where $\\mathrm{TP}_c$ and $\\mathrm{FN}_c$ denote the number of true positives and false negatives for class $c$ in that fold’s validation set. Per-class precision is defined as $\\mathrm{Precision}_c = \\frac{\\mathrm{TP}_c}{\\mathrm{TP}_c + \\mathrm{FP}_c}$, where $\\mathrm{FP}_c$ denotes the number of false positives for class $c$ in the validation set. A metric is undefined in a fold if its denominator equals $0$.\n\nStarting only from the definitions above and the stratified $k$-fold allocation rule, derive a conservative minimal per-class support requirement on $n_c$ (as a function of $k$) that guarantees, in every fold’s validation set, strictly positive actual support for class $c$ (so that the recall denominator is strictly positive) while also ensuring the corresponding training set for that fold has at least one instance of class $c$. To eliminate rounding edge cases, impose the stronger requirement that every validation fold must contain at least $2$ instances of class $c$.\n\nThen, for the regime $n_c < 2k$, propose an additive-Laplace smoothing scheme that makes per-class precision and recall well-defined in every fold regardless of zero counts. Specifically, apply a symmetric additive constant $\\alpha > 0$ to each confusion-matrix cell of the one-versus-rest decomposition for class $c$ in the validation set, and derive closed-form expressions for the smoothed per-class precision and smoothed per-class recall in terms of $\\mathrm{TP}_c$, $\\mathrm{FP}_c$, $\\mathrm{FN}_c$, and $\\alpha$.\n\nYour final answer must list:\n- the minimal $n_c$ in terms of $k$ that suffices under the stated conservative requirement, and\n- the analytic expressions for the Laplace-smoothed per-class precision and recall.\n\nExpress all quantities symbolically. No numerical rounding is required.",
            "solution": "This problem consists of two parts. The first part is to determine the minimum total count $n_c$ for a class $c$ to satisfy certain conditions in a stratified $k$-fold cross-validation setup. The second part is to derive expressions for Laplace-smoothed per-class precision and recall.\n\nPart 1: Minimal Per-Class Support Requirement\n\nLet $n_c$ be the total number of instances of a class $c$ in the dataset, and let $k$ be the number of folds in the cross-validation, where $k$ is an integer greater than or equal to $2$.\n\nIn stratified $k$-fold cross-validation, the $n_c$ instances are partitioned into $k$ disjoint subsets (folds). For any given fold $i$ (where $i \\in \\{1, 2, \\dots, k\\}$), that fold's subset serves as the validation set, and the union of the other $k-1$ subsets serves as the training set.\n\nLet $n_{c, \\text{val}}^{(i)}$ be the number of instances of class $c$ in the validation set of fold $i$. According to the problem's stratification rule, this number is given by:\n$$n_{c, \\text{val}}^{(i)} \\in \\{\\lfloor n_c/k \\rfloor, \\lceil n_c/k \\rceil\\}$$\nLet $n_{c, \\text{train}}^{(i)}$ be the number of instances of class $c$ in the corresponding training set for fold $i$. Since the training and validation sets are complementary with respect to the total set of instances of class $c$:\n$$n_{c, \\text{train}}^{(i)} = n_c - n_{c, \\text{val}}^{(i)}$$\nThe problem imposes two requirements that must be met for *every* fold $i$:\n1. The validation set for each fold must contain at least $2$ instances of class $c$.\n2. The training set for each fold must contain at least $1$ instance of class $c$.\n\nLet us formalize these requirements as inequalities.\nRequirement 1: For all $i \\in \\{1, 2, \\dots, k\\}$,\n$$n_{c, \\text{val}}^{(i)} \\geq 2$$\nTo guarantee this for every fold, the condition must hold even for the fold that receives the minimum possible number of instances. The minimum number of instances allocated to any fold's validation set is $\\lfloor n_c/k \\rfloor$. Therefore, we must enforce:\n$$\\lfloor n_c/k \\rfloor \\geq 2$$\nBy the definition of the floor function, if $\\lfloor x \\rfloor \\geq m$ for an integer $m$, then $x \\geq m$. Applying this, we get:\n$$\\frac{n_c}{k} \\geq 2 \\implies n_c \\geq 2k$$\n\nRequirement 2: For all $i \\in \\{1, 2, \\dots, k\\}$,\n$$n_{c, \\text{train}}^{(i)} \\geq 1$$\nSubstituting the expression for the training set count, we have:\n$$n_c - n_{c, \\text{val}}^{(i)} \\geq 1$$\nTo guarantee this for every fold, the condition must hold even when the training set is at its smallest. The training set size $n_{c, \\text{train}}^{(i)}$ is minimized when the corresponding validation set size $n_{c, \\text{val}}^{(i)}$ is maximized. The maximum number of instances allocated to any fold's validation set is $\\lceil n_c/k \\rceil$. Therefore, we must enforce:\n$$n_c - \\lceil n_c/k \\rceil \\geq 1$$\n\nWe now have two conditions on $n_c$: $n_c \\geq 2k$ and $n_c - \\lceil n_c/k \\rceil \\geq 1$. The final minimal value of $n_c$ must satisfy both. Let's check if the first, more stringent-appearing condition ($n_c \\geq 2k$) is sufficient to satisfy the second one.\n\nAssume $n_c \\geq 2k$. We know that for any real number $x$, $\\lceil x \\rceil < x+1$. Let $x = n_c/k$.\n$$n_c - \\lceil n_c/k \\rceil > n_c - \\left(\\frac{n_c}{k} + 1\\right) = n_c \\left(1 - \\frac{1}{k}\\right) - 1 = \\frac{n_c(k-1)}{k} - 1$$\nSince we assumed $n_c \\geq 2k$, we can substitute this into the inequality:\n$$\\frac{n_c(k-1)}{k} - 1 \\geq \\frac{2k(k-1)}{k} - 1 = 2(k-1) - 1 = 2k - 2 - 1 = 2k - 3$$\nSo, we have established that $n_c - \\lceil n_c/k \\rceil > 2k-3$. For the condition $n_c - \\lceil n_c/k \\rceil \\geq 1$ to hold, we need $2k-3 \\geq 0$ (as the left side is an integer). This is true for $k \\geq 1.5$. Since $k$ represents the number of folds, $k \\geq 2$ is a standard assumption, so this is satisfied.\nThus, the condition $n_c \\geq 2k$ is the stricter of the two and is sufficient to guarantee both requirements.\n\nThe question asks for the minimal per-class support $n_c$. Since $n_c$ must be an integer, the minimal value that satisfies $n_c \\geq 2k$ is $n_c = 2k$.\n\nPart 2: Laplace-Smoothed Metrics\n\nThe problem asks for expressions for smoothed per-class precision and recall. The smoothing is performed by adding a constant $\\alpha > 0$ to each cell of the one-versus-rest confusion matrix for class $c$.\n\nThe one-versus-rest confusion matrix for class $c$ has four cells based on predictions made on a validation set:\n- $\\mathrm{TP}_c$: True Positives (correctly predicted as class $c$)\n- $\\mathrm{FN}_c$: False Negatives (incorrectly predicted as not class $c$)\n- $\\mathrm{FP}_c$: False Positives (incorrectly predicted as class $c$)\n- $\\mathrm{TN}_c$: True Negatives (correctly predicted as not class $c$)\n\nApplying the symmetric additive smoothing, the smoothed counts become:\n- $\\mathrm{TP}'_c = \\mathrm{TP}_c + \\alpha$\n- $\\mathrm{FN}'_c = \\mathrm{FN}_c + \\alpha$\n- $\\mathrm{FP}'_c = \\mathrm{FP}_c + \\alpha$\n- $\\mathrm{TN}'_c = \\mathrm{TN}_c + \\alpha$\n\nThe standard definition for per-class recall for class $c$ is:\n$$\\mathrm{Recall}_c = \\frac{\\mathrm{TP}_c}{\\mathrm{TP}_c + \\mathrm{FN}_c}$$\nThe denominator $\\mathrm{TP}_c + \\mathrm{FN}_c$ represents the total number of actual instances of class $c$ in the validation set.\n\nThe smoothed recall, $\\mathrm{Recall}'_c$, is derived by substituting the smoothed counts into this formula:\n$$\\mathrm{Recall}'_c = \\frac{\\mathrm{TP}'_c}{\\mathrm{TP}'_c + \\mathrm{FN}'_c} = \\frac{\\mathrm{TP}_c + \\alpha}{(\\mathrm{TP}_c + \\alpha) + (\\mathrm{FN}_c + \\alpha)} = \\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FN}_c + 2\\alpha}$$\n\nThe standard definition for per-class precision for class $c$ is:\n$$\\mathrm{Precision}_c = \\frac{\\mathrm{TP}_c}{\\mathrm{TP}_c + \\mathrm{FP}_c}$$\nThe denominator $\\mathrm{TP}_c + \\mathrm{FP}_c$ represents the total number of instances predicted as class $c$.\n\nThe smoothed precision, $\\mathrm{Precision}'_c$, is derived by substituting the smoothed counts into this formula:\n$$\\mathrm{Precision}'_c = \\frac{\\mathrm{TP}'_c}{\\mathrm{TP}'_c + \\mathrm{FP}'_c} = \\frac{\\mathrm{TP}_c + \\alpha}{(\\mathrm{TP}_c + \\alpha) + (\\mathrm{FP}_c + \\alpha)} = \\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FP}_c + 2\\alpha}$$\nSince $\\alpha > 0$ and the counts ($\\mathrm{TP}_c$, $\\mathrm{FP}_c$, $\\mathrm{FN}_c$) are non-negative integers, the denominators of these smoothed metrics are guaranteed to be strictly positive, ensuring the metrics are always well-defined.\n\nThe final answer requires three components: the minimal $n_c$ in terms of $k$, the expression for smoothed precision, and the expression for smoothed recall.\n1. Minimal $n_c$: $2k$\n2. Smoothed Precision: $\\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FP}_c + 2\\alpha}$\n3. Smoothed Recall: $\\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FN}_c + 2\\alpha}$",
            "answer": "$$\\boxed{\\begin{pmatrix} 2k & \\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FP}_c + 2\\alpha} & \\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FN}_c + 2\\alpha} \\end{pmatrix}}$$"
        },
        {
            "introduction": "A standard cross-validation estimate assumes that the class proportions in your dataset perfectly mirror the real-world environment where the model will be deployed, but this is rarely the case. This advanced practice introduces importance weighting as a method to correct for this \"prior shift,\" allowing you to adjust the cross-validation estimate to reflect a known target class distribution. By deriving and applying per-fold class weights, you will learn how to produce a more accurate and unbiased estimate of test-time performance, a crucial skill for building models that are robust in the face of changing data distributions .",
            "id": "3177452",
            "problem": "In a binary classification task evaluated with stratified $k$-fold cross-validation (CV), suppose the true test-time class prior is $q_{1} = 0.6$ and $q_{0} = 0.4$, while the empirical class proportions vary by fold. You will derive a per-fold importance reweighting scheme that yields an unbiased CV estimate of the expected $0$-$1$ loss under the test-time class prior and then compute the resulting adjusted CV estimate for a given set of folds.\n\nStart from the following fundamental bases:\n- The risk of a classifier under a target distribution is the expectation of the loss: $R = \\mathbb{E}[\\ell]$.\n- Under class prior shift, the test-time distribution differs from the training distribution only through $P(Y)$, while the class-conditional distributions remain stable.\n- Importance weighting is used to convert expectations under one distribution into expectations under another.\n\nA binary classifier is trained and evaluated by stratified $k$-fold CV with $k = 3$. For each fold $j \\in \\{1,2,3\\}$, you are given the confusion counts computed on that fold’s held-out data. Let $n^{(j)}$ be the fold size, $n_{1}^{(j)}$ and $n_{0}^{(j)}$ the numbers of positive and negative examples, respectively, and $\\widehat{p}_{1}^{(j)} = n_{1}^{(j)}/n^{(j)}$, $\\widehat{p}_{0}^{(j)} = n_{0}^{(j)}/n^{(j)}$ the empirical class proportions in fold $j$. The confusion counts per fold are:\n- Fold $1$: $n^{(1)} = 50$, $n_{1}^{(1)} = 30$, $n_{0}^{(1)} = 20$; $TP^{(1)} = 24$, $FN^{(1)} = 6$, $TN^{(1)} = 15$, $FP^{(1)} = 5$.\n- Fold $2$: $n^{(2)} = 50$, $n_{1}^{(2)} = 20$, $n_{0}^{(2)} = 30$; $TP^{(2)} = 14$, $FN^{(2)} = 6$, $TN^{(2)} = 24$, $FP^{(2)} = 6$.\n- Fold $3$: $n^{(3)} = 50$, $n_{1}^{(3)} = 25$, $n_{0}^{(3)} = 25$; $TP^{(3)} = 20$, $FN^{(3)} = 5$, $TN^{(3)} = 18$, $FP^{(3)} = 7$.\n\nTasks:\n- Using only the principles stated above, derive per-fold class weights $\\alpha_{c}^{(j)}$ (one weight per class $c \\in \\{0,1\\}$ per fold $j$) such that, when applied to the $0$-$1$ loss within each fold, the resulting CV estimator is unbiased for the expected test-time risk under the target prior $\\{q_{c}\\}$.\n- Using your derived $\\alpha_{c}^{(j)}$, compute the pooled importance-weighted CV estimate of the misclassification risk as\n$$\n\\widehat{R} \\;=\\; \\frac{1}{N}\\sum_{j=1}^{3}\\sum_{i \\in \\text{fold } j} \\alpha_{Y_{i}}^{(j)} \\,\\mathbf{1}\\{ \\widehat{Y}_{i} \\neq Y_{i} \\},\n$$\nwhere $N = \\sum_{j=1}^{3} n^{(j)}$, $Y_{i}$ is the true label, and $\\widehat{Y}_{i}$ is the predicted label. Express your final numeric answer as a decimal fraction, rounded to $4$ significant figures.",
            "solution": "The problem requires the derivation of an importance reweighting scheme to correct for class prior shift in a stratified $k$-fold cross-validation (CV) setting, and then to apply this scheme to compute an adjusted risk estimate.\n\n### Part 1: Derivation of Importance Weights\n\nLet the target distribution be denoted by $P_{test}$ and the empirical distribution of a given fold $j$ by $P_{j}$. The true test-time risk $R_{test}$ is the expected $0$-$1$ loss under $P_{test}$:\n$$\nR_{test} = \\mathbb{E}_{(X,Y) \\sim P_{test}}[\\mathbf{1}\\{\\widehat{Y}(X) \\neq Y\\}]\n$$\nwhere $\\widehat{Y}(X)$ is the prediction of the classifier for input $X$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The expectation can be decomposed over the classes $c \\in \\{0,1\\}$:\n$$\nR_{test} = \\sum_{c \\in \\{0,1\\}} P_{test}(Y=c) \\mathbb{E}_{X \\sim P_{test}(X|Y=c)}[\\mathbf{1}\\{\\widehat{Y}(X) \\neq c\\}]\n$$\nLet $q_c = P_{test}(Y=c)$ be the target class prior and $R_c = \\mathbb{E}_{X|Y=c}[\\mathbf{1}\\{\\widehat{Y}(X) \\neq c\\}]$ be the true class-conditional risk for class $c$. The target risk is then:\n$$\nR_{test} = q_1 R_1 + q_0 R_0\n$$\nThe problem states that the class-conditional distributions are stable, meaning $P_{test}(X|Y=c) = P_j(X|Y=c)$ for any fold $j$. This implies that the true class-conditional risks $R_c$ are independent of the distribution's priors. The classifier $\\widehat{Y}$ is trained on data excluding fold $j$, and we assume it is a reasonable proxy for the final classifier, so its class-conditional error rates $R_c^{(j)}$ on fold $j$ are good estimates of the true $R_c$.\n\nThe principle of importance sampling allows us to estimate an expectation under a target distribution $P_{test}$ using samples from a proposal distribution $P_j$:\n$$\n\\mathbb{E}_{P_{test}}[f(Z)] = \\mathbb{E}_{P_j}\\left[\\frac{P_{test}(Z)}{P_j(Z)} f(Z)\\right]\n$$\nIn our case, $Z=(X,Y)$ and $f(Z) = \\mathbf{1}\\{\\widehat{Y}(X) \\neq Y\\}$. The importance weight for a sample $(X,Y)$ is:\n$$\nw(X,Y) = \\frac{P_{test}(X,Y)}{P_j(X,Y)} = \\frac{P_{test}(X|Y)P_{test}(Y)}{P_j(X|Y)P_j(Y)}\n$$\nUsing the prior shift assumption, $P_{test}(X|Y) = P_j(X|Y)$, the weight simplifies to depend only on the class label $Y$:\n$$\nw(Y) = \\frac{P_{test}(Y)}{P_j(Y)}\n$$\nFor a sample of class $c$ in fold $j$, the weight is the ratio of the target prior $q_c$ to the empirical prior in that fold, $\\widehat{p}_c^{(j)} = n_c^{(j)}/n^{(j)}$. We are asked to find weights $\\alpha_c^{(j)}$ such that the weighted estimator is unbiased. Let us set the weight for each sample of class $c$ in fold $j$ to be this ratio:\n$$\n\\alpha_c^{(j)} = \\frac{q_c}{\\widehat{p}_c^{(j)}}\n$$\nTo verify that this choice of weights yields an unbiased estimator, consider the expectation of the weighted loss for a single observation $i$ from fold $j$:\n$$\n\\mathbb{E}_{(X_i,Y_i) \\sim P_j}[\\alpha_{Y_i}^{(j)}\\mathbf{1}\\{\\widehat{Y}_i \\neq Y_i\\}] = \\sum_{c \\in \\{0,1\\}} P_j(Y=c) \\mathbb{E}_{(X|Y=c)\\sim P_j}[\\alpha_c^{(j)}\\mathbf{1}\\{\\widehat{Y} \\neq c\\}]\n$$\nSince $\\alpha_c^{(j)}$ is constant for class $c$, we have:\n$$\n= \\sum_{c \\in \\{0,1\\}} \\widehat{p}_c^{(j)} \\alpha_c^{(j)} \\mathbb{E}_{(X|Y=c)\\sim P_j}[\\mathbf{1}\\{\\widehat{Y} \\neq c\\}] = \\sum_{c \\in \\{0,1\\}} \\widehat{p}_c^{(j)} \\alpha_c^{(j)} R_c^{(j)}\n$$\nSubstituting our derived weight $\\alpha_c^{(j)} = q_c / \\widehat{p}_c^{(j)}$:\n$$\n= \\sum_{c \\in \\{0,1\\}} \\widehat{p}_c^{(j)} \\left(\\frac{q_c}{\\widehat{p}_c^{(j)}}\\right) R_c^{(j)} = \\sum_{c \\in \\{0,1\\}} q_c R_c^{(j)}\n$$\nAssuming $R_c^{(j)} \\approx R_c$, this is equal to $R_{test}$. The expectation of the full CV estimator $\\widehat{R}$ is the average of these expectations, which will also be $R_{test}$, confirming it is an unbiased estimator.\n\n### Part 2: Calculation of the Adjusted CV Estimate\n\nThe pooled importance-weighted CV estimate is given by:\n$$\n\\widehat{R} \\;=\\; \\frac{1}{N}\\sum_{j=1}^{3}\\sum_{i \\in \\text{fold } j} \\alpha_{Y_{i}}^{(j)} \\,\\mathbf{1}\\{ \\widehat{Y}_{i} \\neq Y_{i} \\}\n$$\nwhere $N = \\sum_{j=1}^{3} n^{(j)}$ is the total number of samples. The inner sum can be computed by summing the weights for all misclassified samples. The misclassified samples of class $1$ are the false negatives ($FN$), and the misclassified samples of class $0$ are the false positives ($FP$). Thus, the formula can be rewritten using the confusion counts for each fold:\n$$\n\\widehat{R} = \\frac{1}{N} \\sum_{j=1}^{3} \\left( FN^{(j)} \\cdot \\alpha_1^{(j)} + FP^{(j)} \\cdot \\alpha_0^{(j)} \\right)\n$$\nThe total number of samples is $N = 50 + 50 + 50 = 150$. The target priors are $q_1 = 0.6$ and $q_0 = 0.4$.\n\n**Fold 1:**\n- Data: $n^{(1)} = 50$, $n_1^{(1)} = 30$, $n_0^{(1)} = 20$.\n- Empirical priors: $\\widehat{p}_1^{(1)} = \\frac{30}{50} = 0.6$, $\\widehat{p}_0^{(1)} = \\frac{20}{50} = 0.4$.\n- Weights: $\\alpha_1^{(1)} = \\frac{q_1}{\\widehat{p}_1^{(1)}} = \\frac{0.6}{0.6} = 1$, $\\alpha_0^{(1)} = \\frac{q_0}{\\widehat{p}_0^{(1)}} = \\frac{0.4}{0.4} = 1$.\n- Confusion counts: $FN^{(1)} = 6$, $FP^{(1)} = 5$.\n- Weighted error sum: $W_1 = (6 \\cdot 1) + (5 \\cdot 1) = 11$.\n\n**Fold 2:**\n- Data: $n^{(2)} = 50$, $n_1^{(2)} = 20$, $n_0^{(2)} = 30$.\n- Empirical priors: $\\widehat{p}_1^{(2)} = \\frac{20}{50} = 0.4$, $\\widehat{p}_0^{(2)} = \\frac{30}{50} = 0.6$.\n- Weights: $\\alpha_1^{(2)} = \\frac{q_1}{\\widehat{p}_1^{(2)}} = \\frac{0.6}{0.4} = 1.5$, $\\alpha_0^{(2)} = \\frac{q_0}{\\widehat{p}_0^{(2)}} = \\frac{0.4}{0.6} = \\frac{2}{3}$.\n- Confusion counts: $FN^{(2)} = 6$, $FP^{(2)} = 6$.\n- Weighted error sum: $W_2 = (6 \\cdot 1.5) + (6 \\cdot \\frac{2}{3}) = 9 + 4 = 13$.\n\n**Fold 3:**\n- Data: $n^{(3)} = 50$, $n_1^{(3)} = 25$, $n_0^{(3)} = 25$.\n- Empirical priors: $\\widehat{p}_1^{(3)} = \\frac{25}{50} = 0.5$, $\\widehat{p}_0^{(3)} = \\frac{25}{50} = 0.5$.\n- Weights: $\\alpha_1^{(3)} = \\frac{q_1}{\\widehat{p}_1^{(3)}} = \\frac{0.6}{0.5} = 1.2$, $\\alpha_0^{(3)} = \\frac{q_0}{\\widehat{p}_0^{(3)}} = \\frac{0.4}{0.5} = 0.8$.\n- Confusion counts: $FN^{(3)} = 5$, $FP^{(3)} = 7$.\n- Weighted error sum: $W_3 = (5 \\cdot 1.2) + (7 \\cdot 0.8) = 6 + 5.6 = 11.6$.\n\n**Final Calculation:**\nThe total weighted sum of errors across all folds is:\n$$\n\\sum_{j=1}^{3} W_j = 11 + 13 + 11.6 = 35.6\n$$\nThe pooled importance-weighted CV estimate of the risk is:\n$$\n\\widehat{R} = \\frac{35.6}{150} \\approx 0.237333...\n$$\nRounding to $4$ significant figures, we get $0.2373$.",
            "answer": "$$\n\\boxed{0.2373}\n$$"
        }
    ]
}