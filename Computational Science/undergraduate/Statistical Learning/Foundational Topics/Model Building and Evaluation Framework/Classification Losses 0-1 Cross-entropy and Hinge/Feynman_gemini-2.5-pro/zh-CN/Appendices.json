{
    "hands_on_practices": [
        {
            "introduction": "为了深入理解不同损失函数的内在属性，我们首先从一个最简单的情景入手：一个没有任何特征信息、仅依赖于偏置项 $b$ 的分类器。这个思想实验旨在剥离特征带来的复杂性，让我们能够清晰地看到交叉熵损失和合页损失在面对类别不平衡数据时，如何仅通过调整偏置项来做出最优决策。通过解析推导，我们将揭示这两种损失函数对先验概率的不同敏感性。",
            "id": "3108628",
            "problem": "考虑一个二元分类任务，其样本和类别标签是独立同分布的。假设特征向量不携带任何信息，因此模型的线性得分简化为一个恒定的偏置项 $b$，即对于所有输入 $x$，都有 $g(x) = b$。数据集包含 $n_{+} = 20$ 个正类样本和 $n_{-} = 80$ 个负类样本。令 $\\pi_{+}$ 和 $\\pi_{-}$ 表示类别先验概率，通过经验频率估计为 $\\pi_{+} = \\frac{n_{+}}{n_{+}+n_{-}}$ 和 $\\pi_{-} = \\frac{n_{-}}{n_{+}+n_{-}}$。\n\n从损失函数和概率决策规则的基本定义出发：\n\n- 对于二元标签 $y \\in \\{0,1\\}$ 的交叉熵（也称为逻辑斯谛）损失，正类的预测概率为 $p = \\sigma(b)$，其中 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ 是逻辑斯谛函数。经验交叉熵损失是独立伯努利试验的负对数似然，即 $L_{\\mathrm{CE}}(b) = -\\sum_{i=1}^{n} \\left( y_{i} \\ln(\\sigma(b)) + (1-y_{i}) \\ln(1-\\sigma(b)) \\right)$。\n- 对于标签 $y \\in \\{-1,+1\\}$ 的合页损失，线性得分 $g(x)=b$ 的经验合页损失为 $L_{\\mathrm{hinge}}(b) = \\sum_{i=1}^{n} \\max\\!\\left(0,\\,1 - y_{i} b\\right)$。\n- 在 $0$-$1$ 损失和相等的错分成本下，贝叶斯决策规则选择后验概率最大的类别。在没有信息特征的情况下（因此对于所有 $x$，类别条件似然都相等），该决策简化为对一个表示正类对数几率的校准得分进行阈值判断。一个使后验概率相等的阈值 $\\tau$ 满足 $\\sigma(\\tau) = \\pi_{+}$，从而使得 $\\tau$ 与类别先验的对数几率相匹配。\n\n仅使用这些基础知识，完成以下任务：\n1. 推导使 $L_{\\mathrm{CE}}(b)$ 最小化的值 $b_{\\mathrm{CE}}^{\\star}$。\n2. 推导使 $L_{\\mathrm{hinge}}(b)$ 最小化的值 $b_{\\mathrm{hinge}}^{\\star}$。\n3. 在 $0$-$1$ 损失和相等的错分成本下，推导作用于得分 $g(x)=b$ 的贝叶斯最优阈值 $\\tau^{\\star}$，并用 $\\pi_{+}$ 和 $\\pi_{-}$ 表示。\n\n以精确解析表达式的形式给出 $\\left(b_{\\mathrm{CE}}^{\\star},\\,b_{\\mathrm{hinge}}^{\\star},\\,\\tau^{\\star}\\right)$ 的最终结果。不需要四舍五入。你的最终答案必须是单个表达式。",
            "solution": "该问题要求在二元分类场景下，为三种不同的损失函数框架推导最优偏置参数，其中模型得分是一个恒定的偏置项 $g(x) = b$。给定 $n_{+} = 20$ 个正样本和 $n_{-} = 80$ 个负样本。相应的经验类别先验概率为 $\\pi_{+} = \\frac{n_{+}}{n_{+}+n_{-}} = \\frac{20}{100} = \\frac{1}{5}$ 和 $\\pi_{-} = \\frac{n_{-}}{n_{+}+n_{-}} = \\frac{80}{100} = \\frac{4}{5}$。\n\n### 1. 最小化交叉熵损失\n\n经验交叉熵损失是为标签 $y \\in \\{0, 1\\}$ 定义的，其中正类标记为 $y=1$，负类标记为 $y=0$。总损失是所有 $n = n_{+} + n_{-}$ 个样本的和：\n$$L_{\\mathrm{CE}}(b) = -\\sum_{i=1}^{n} \\left( y_{i} \\ln(\\sigma(b)) + (1-y_{i}) \\ln(1-\\sigma(b)) \\right)$$\n其中 $\\sigma(b) = \\frac{1}{1+\\exp(-b)}$ 是逻辑斯谛函数。\n\n我们可以将求和分解为来自 $n_{+}$ 个正样本（其中 $y_i=1$）和 $n_{-}$ 个负样本（其中 $y_i=0$）的贡献：\n$$L_{\\mathrm{CE}}(b) = - n_{+} \\left( 1 \\cdot \\ln(\\sigma(b)) + (1-1) \\ln(1-\\sigma(b)) \\right) - n_{-} \\left( 0 \\cdot \\ln(\\sigma(b)) + (1-0) \\ln(1-\\sigma(b)) \\right)$$\n$$L_{\\mathrm{CE}}(b) = - n_{+} \\ln(\\sigma(b)) - n_{-} \\ln(1-\\sigma(b))$$\n为了找到最小化该损失的 $b$ 值，即 $b_{\\mathrm{CE}}^{\\star}$，我们计算 $L_{\\mathrm{CE}}(b)$ 关于 $b$ 的导数，并令其为零。我们使用以下求导性质：$\\frac{d}{db}\\sigma(b) = \\sigma(b)(1-\\sigma(b))$，$\\frac{d}{db}\\ln(\\sigma(b)) = 1-\\sigma(b)$，以及 $\\frac{d}{db}\\ln(1-\\sigma(b)) = -\\sigma(b)$。\n\n损失函数的导数是：\n$$\\frac{dL_{\\mathrm{CE}}}{db} = - n_{+} (1-\\sigma(b)) - n_{-} (-\\sigma(b)) = -n_{+} + n_{+}\\sigma(b) + n_{-}\\sigma(b)$$\n$$\\frac{dL_{\\mathrm{CE}}}{db} = -n_{+} + (n_{+}+n_{-})\\sigma(b)$$\n将导数设为零以找到临界点 $b_{\\mathrm{CE}}^{\\star}$：\n$$-n_{+} + (n_{+}+n_{-})\\sigma(b_{\\mathrm{CE}}^{\\star}) = 0$$\n$$\\sigma(b_{\\mathrm{CE}}^{\\star}) = \\frac{n_{+}}{n_{+}+n_{-}} = \\pi_{+}$$\n这个结果表明，最优的预测概率与正类的经验频率相匹配。为了求得 $b_{\\mathrm{CE}}^{\\star}$，我们在方程 $\\sigma(b) = \\pi_{+}$ 中求解 $b$：\n$$\\frac{1}{1+\\exp(-b_{\\mathrm{CE}}^{\\star})} = \\pi_{+}$$\n$$1 = \\pi_{+}(1+\\exp(-b_{\\mathrm{CE}}^{\\star}))$$\n$$\\frac{1}{\\pi_{+}} - 1 = \\exp(-b_{\\mathrm{CE}}^{\\star})$$\n$$\\frac{1-\\pi_{+}}{\\pi_{+}} = \\exp(-b_{\\mathrm{CE}}^{\\star})$$\n因为 $1-\\pi_{+} = \\pi_{-}$，我们有：\n$$\\frac{\\pi_{-}}{\\pi_{+}} = \\exp(-b_{\\mathrm{CE}}^{\\star})$$\n对两边取自然对数：\n$$-b_{\\mathrm{CE}}^{\\star} = \\ln\\left(\\frac{\\pi_{-}}{\\pi_{+}}\\right)$$\n$$b_{\\mathrm{CE}}^{\\star} = -\\ln\\left(\\frac{\\pi_{-}}{\\pi_{+}}\\right) = \\ln\\left(\\left(\\frac{\\pi_{-}}{\\pi_{+}}\\right)^{-1}\\right) = \\ln\\left(\\frac{\\pi_{+}}{\\pi_{-}}\\right)$$\n二阶导数 $\\frac{d^2L_{\\mathrm{CE}}}{db^2} = (n_{+}+n_{-})\\sigma(b)(1-\\sigma(b))$ 对所有 $b$ 均为正，这证实了损失函数是凸函数，并且 $b_{\\mathrm{CE}}^{\\star}$ 是一个全局最小值。\n\n代入给定值 $\\pi_{+} = \\frac{1}{5}$ 和 $\\pi_{-} = \\frac{4}{5}$：\n$$b_{\\mathrm{CE}}^{\\star} = \\ln\\left(\\frac{1/5}{4/5}\\right) = \\ln\\left(\\frac{1}{4}\\right) = -\\ln(4)$$\n\n### 2. 最小化合页损失\n\n经验合页损失是为标签 $y \\in \\{-1, +1\\}$ 定义的，其中正类为 $y=+1$，负类为 $y=-1$。总损失为：\n$$L_{\\mathrm{hinge}}(b) = \\sum_{i=1}^{n} \\max(0, 1 - y_{i} b)$$\n我们将求和分为对 $n_{+}$ 个正样本（$y_i=+1$）和 $n_{-}$ 个负样本（$y_i=-1$）的部分：\n$$L_{\\mathrm{hinge}}(b) = n_{+} \\max(0, 1 - b) + n_{-} \\max(0, 1 + b)$$\n该函数是凸函数且分段线性，在 $b=-1$ 和 $b=1$ 处不可微。我们分析 $L_{\\mathrm{hinge}}(b)$ 的次梯度来找到最小值。最小值 $b_{\\mathrm{hinge}}^{\\star}$ 必须满足条件 $0 \\in \\partial L_{\\mathrm{hinge}}(b_{\\mathrm{hinge}}^{\\star})$。\n\n次梯度是各项次梯度之和：\n- 对于 $b \\in (-1, 1)$，$\\partial L_{\\mathrm{hinge}}(b) = \\{ -n_{+} + n_{-} \\}$。\n- 对于 $b = -1$，$\\partial L_{\\mathrm{hinge}}(b) = \\{-n_{+}\\} + n_{-}[0, 1] = [-n_{+}, -n_{+} + n_{-}]$。\n- 对于 $b = 1$，$\\partial L_{\\mathrm{hinge}}(b) = n_{+}[-1, 0] + \\{n_{-}\\} = [-n_{+} + n_{-}, n_{-}]$。\n\n我们已知 $n_{+} = 20$ 和 $n_{-} = 80$。因此，$n_{-} > n_{+}$。\n- 对于 $b \\in (-1, 1)$，导数为 $n_{-} - n_{+} = 80 - 20 = 60 \\neq 0$。最小值不在此区间内。\n- 对于 $b = -1$，次梯度区间为 $[-n_{+}, -n_{+} + n_{-}] = [-20, 60]$。因为 $0 \\in [-20, 60]$，所以 $b = -1$ 是一个最小值点。\n- 对于 $b = 1$，次梯度区间为 $[-n_{+} + n_{-}, n_{-}] = [60, 80]$。因为 $0 \\notin [60, 80]$，所以 $b=1$ 不是一个最小值点。\n因为函数是凸的，次梯度包含零的点 $b_{\\mathrm{hinge}}^{\\star} = -1$ 是全局最小值。\n\n### 3. 0-1 损失下的贝叶斯最优阈值\n\n问题指出，在 0-1 损失、相等的错分成本和无信息特征的条件下，决策可以基于对一个校准得分的阈值 $\\tau$。问题将贝叶斯最优阈值 $\\tau^{\\star}$ 定义为满足以下条件的值：\n$$\\sigma(\\tau^{\\star}) = \\pi_{+}$$\n这个条件意味着与阈值本身相关的概率（通过逻辑斯谛函数）等于正类的先验概率。问题提到，这意味着 $\\tau^{\\star}$ 是类别先验的对数几率。我们可以通过求解 $\\tau^{\\star}$ 来直接推导这一点：\n$$\\frac{1}{1 + \\exp(-\\tau^{\\star})} = \\pi_{+}$$\n这与我们为 $b_{\\mathrm{CE}}^{\\star}$ 求解的方程相同。因此，解是相同的：\n$$\\tau^{\\star} = \\ln\\left(\\frac{\\pi_{+}}{\\pi_{-}}\\right)$$\n其逻辑是，对于一个被校准为后验概率对数几率的得分 $g(x)=\\ln\\left(\\frac{P(C_+|x)}{P(C_-|x)}\\right)$，0-1 损失的最优决策边界在 $g(x)=0$ 处。在我们的无特征情况下，得分是恒定的，等于先验概率的对数几率，即 $g(x) = \\ln(\\pi_+/\\pi_-)$。在一个等于 $b=g(x)$ 的得分上设置阈值 $\\tau = \\ln(\\pi_+/\\pi_-)$ 相当于询问是否 $b > \\tau$，但这不会是一个有用的决策规则，因为两者都是常数。问题的框架将 $\\tau^*$ 定义为先验概率对数几率本身的值。\n\n使用给定值：\n$$\\tau^{\\star} = \\ln\\left(\\frac{1/5}{4/5}\\right) = \\ln\\left(\\frac{1}{4}\\right) = -\\ln(4)$$\n\n### 结果总结\n推导出的值为：\n1. $b_{\\mathrm{CE}}^{\\star} = -\\ln(4)$\n2. $b_{\\mathrm{hinge}}^{\\star} = -1$\n3. $\\tau^{\\star} = -\\ln(4)$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\ln(4)  -1  -\\ln(4)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "合页损失的一个关键特性是其“满足性”：一旦所有样本的函数间隔 $y f(x)$ 都达到或超过 $1$，损失即为零，优化便可能停止。相比之下，交叉熵损失会持续“鼓励”模型增大间隔，即使所有样本都已被正确分类。这个练习通过一个具体的计算实验，直观地展示了这一差异对模型泛化能力的影响，揭示了为何追求更大的间隔往往能带来在未知测试数据上更好的表现。",
            "id": "3108625",
            "problem": "考虑标签为 $y \\in \\{-1,+1\\}$ 的二元分类问题，以及一个线性分数函数 $f_{\\boldsymbol{w},b}(\\boldsymbol{x}) = \\boldsymbol{w}^{\\top}\\boldsymbol{x} + b$。在一个数据集 $\\{(\\boldsymbol{x}_i,y_i)\\}_{i=1}^n$ 上的经验 $0$-$1$ 分类误差定义为满足 $y_i f_{\\boldsymbol{w},b}(\\boldsymbol{x}_i) \\le 0$ 的索引 $i$ 所占的比例。Hinge 损失定义为 $\\ell_{\\text{hinge}}(y, f) = \\max\\{0, 1 - y f\\}$，而二元标签的交叉熵（逻辑）损失定义为 $\\ell_{\\text{ce}}(y, f) = \\log\\big(1 + \\exp(-y f)\\big)$。线性分类器在某个数据集上的（几何）间隔定义为\n$$\n\\gamma(\\boldsymbol{w},b;\\{\\boldsymbol{x}_i,y_i\\}) = \\min_{i} \\frac{y_i(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_i + b)}{\\|\\boldsymbol{w}\\|_2}.\n$$\n仅从这些核心定义出发，构建并分析一个明确的例子来说明以下现象：\n- 当单独使用 Hinge 损失进行训练时（无正则化），为所有训练点实现 $y_i f_{\\boldsymbol{w},b}(\\boldsymbol{x}_i) \\ge 1$ 会得到零 Hinge 损失，这可能在相对较小的几何间隔下发生。\n- 当使用交叉熵进行训练时，损失会持续激励将 $y_i f_{\\boldsymbol{w},b}(\\boldsymbol{x}_i)$ 增加到超过 $1$，从而有效地推动实现更大的几何间隔。\n\n您的程序必须实例化一个可复现的、线性可分的训练数据集和一个更广泛的测试分布，然后对于一个保持方向固定并通过平移决策边界以满足指定间隔目标的线性分类器族，量化其经验测试 $0$-$1$ 风险如何随已达到的几何间隔而变化。请按以下步骤进行。\n\n1. 生成一个包含 $n=16$ 个点的 $\\mathbb{R}^2$ 训练数据集，其中恰好有 $8$ 个正样本独立地从均值为 $\\boldsymbol{\\mu}_+ = (3,3)$、协方差矩阵为 $\\sigma_{\\text{train}}^2 \\boldsymbol{I}$ 的正态分布中抽取，以及 $8$ 个负样本独立地从均值为 $\\boldsymbol{\\mu}_- = (-3,-3)$、协方差相同的正态分布中抽取。使用 $\\sigma_{\\text{train}} = 0.3$ 和一个固定的伪随机种子以确保可复现性。\n\n2. 生成一个包含 $N=2000$ 个点的 $\\mathbb{R}^2$ 测试数据集，其中恰好有 $1000$ 个正样本独立地从均值为 $\\boldsymbol{\\mu}_+ = (3,3)$、协方差矩阵为 $\\sigma_{\\text{test}}^2 \\boldsymbol{I}$ 的正态分布中抽取，以及 $1000$ 个负样本独立地从均值为 $\\boldsymbol{\\mu}_- = (-3,-3)$、协方差相同的正态分布中抽取。使用 $\\sigma_{\\text{test}} = 1.2$ 和相同的固定伪随机种子。\n\n3. 固定方向向量 $\\boldsymbol{v} = \\frac{1}{\\sqrt{2}}(1,1)$ 并设置 $\\|\\boldsymbol{v}\\|_2 = 1$。对于训练集，计算所有点的投影 $p_i = \\boldsymbol{v}^{\\top}\\boldsymbol{x}_i$。令 $p_{\\min}^+ = \\min\\{\\boldsymbol{v}^{\\top}\\boldsymbol{x}_i : y_i = +1\\}$ 和 $p_{\\max}^- = \\max\\{\\boldsymbol{v}^{\\top}\\boldsymbol{x}_i : y_i = -1\\}$，并定义投影间隙 $g = p_{\\min}^+ - p_{\\max}^-$。对于沿 $\\boldsymbol{v}$ 方向线性可分的样本，该间隙严格为正。那么，在正确分类训练集的同时，沿方向 $\\boldsymbol{v}$ 的最大可行几何间隔为 $m_{\\max} = \\frac{g}{2}$。\n\n4. 对于任意满足 $0  m \\le m_{\\max}$ 的目标几何间隔 $m$，构建一个权重为 $\\boldsymbol{w} = \\boldsymbol{v}$、偏置为\n$$\nb(m) = m - p_{\\min}^+\n$$\n的线性分类器，使得最近的正训练点恰好达到间隔 $m$。验证对于所有训练点，都有 $y_i(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_i + b(m)) \\ge m$，因此只要 $m \\ge 1$，经验 Hinge 损失就为零。这明确地展示了一个例子，其中 Hinge 损失可以在相对较小的几何间隔（例如，$m$ 略大于 $1$）下达到零训练误差，而交叉熵则会继续鼓励增加 $m$。\n\n5. 对于以下测试套件中的每个目标间隔 $m$，计算经验测试 $0$-$1$ 风险，即满足 $y \\cdot (\\boldsymbol{v}^{\\top}\\boldsymbol{x} + b(m)) \\le 0$ 的测试点 $(\\boldsymbol{x},y)$ 的比例：\n- $m = 1.0$（对于单位范数 $\\boldsymbol{w}$，零 Hinge 损失的边界条件），\n- $m = 1.5$（小间隔情况），\n- $m = 2.5$（中等间隔情况），\n- $m = 3.5$（较大间隔情况），\n- $m = m_{\\max} - 0.1$（近最大可行间隔情况）。\n\n根据构造，只有不超过 $m_{\\max}$ 的间隔 $m$ 才是可行的；对于此数据集，$m_{\\max}$ 将足够大，使得所有列出的间隔都是可行的。\n\n6. 您的程序应生成单行输出，其中包含按指定顺序排列的各个间隔的经验测试 $0$-$1$ 风险，格式为一个逗号分隔的列表并用方括号括起来（例如，$[r_{1.0},r_{1.5},r_{2.5},r_{3.5},r_{m_{\\max}-0.1}]$），其中每个 $r_m$ 是一个代表错分测试样本比例的小数。\n\n其目标是，从基本原理出发，展示如何在较小的几何间隔下实现零 Hinge 损失，以及对于这个明确的分类器族，增加间隔（如交叉熵训练所鼓励的那样）如何影响经验测试 $0$-$1$ 风险——所有这些都是在保持方向固定并通过偏置平移决策边界以实现指定间隔的前提下进行的。",
            "solution": "该问题陈述是有效的。它提出了一个定义明确、科学上合理且可通过计算验证的统计学习理论练习。任务是构建并分析一个具体的数值例子，以说明在二元分类中，Hinge 损失和交叉熵损失在几何间隔方面的不同行为。其设置是完整的、一致的，并提出了一个清晰且不平凡的问题，可以通过规定的逐步过程来回答。\n\n需要证明的核心原理是，最小化 Hinge 损失可以在所有数据点的函数间隔达到 $1$ 后终止，这可能导致次优的几何间隔和较差的泛化能力。相比之下，最小化交叉熵损失会持续激励间隔的增加，这通常会带来更好的泛化能力。我们现在将进行形式化的构建和分析。\n\n首先，我们为实验建立环境。我们处理一个二元分类问题，其中标签为 $y \\in \\{-1, +1\\}$，我们的分类器使用一个线性分数函数 $f_{\\boldsymbol{w},b}(\\boldsymbol{x}) = \\boldsymbol{w}^{\\top}\\boldsymbol{x} + b$。\n\n**1. 数据生成**\n\n按照规定，我们生成一个训练集和一个测试集。将使用固定的伪随机种子 $42$ 来确保结果的可复现性。\n\n训练数据集 $\\{(\\boldsymbol{x}_i,y_i)\\}_{i=1}^n$ 包含 $n=16$ 个 $\\mathbb{R}^2$ 中的点。\n- $8$ 个正样本 ($y_i = +1$) 从一个二元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}_+, \\sigma_{\\text{train}}^2 \\boldsymbol{I})$ 中抽取，其均值为 $\\boldsymbol{\\mu}_+ = (3,3)$，协方差为 $\\sigma_{\\text{train}}^2 \\boldsymbol{I}$，其中 $\\sigma_{\\text{train}} = 0.3$。\n- $8$ 个负样本 ($y_i = -1$) 从 $\\mathcal{N}(\\boldsymbol{\\mu}_-, \\sigma_{\\text{train}}^2 \\boldsymbol{I})$ 中抽取，其均值为 $\\boldsymbol{\\mu}_- = (-3,-3)$，协方差相同。\n较小的方差 $\\sigma_{\\text{train}}^2$ 确保了训练数据将以高概率线性可分。\n\n测试数据集包含 $N=2000$ 个点，从一个更广泛的分布中生成，以评估泛化能力。\n- $1000$ 个正样本从 $\\mathcal{N}(\\boldsymbol{\\mu}_+, \\sigma_{\\text{test}}^2 \\boldsymbol{I})$ 中抽取，其中 $\\sigma_{\\text{test}} = 1.2$。\n- $1000$ 个负样本从 $\\mathcal{N}(\\boldsymbol{\\mu}_-, \\sigma_{\\text{test}}^2 \\boldsymbol{I})$ 中抽取，$\\sigma_{\\text{test}} = 1.2$ 相同。\n较大的方差 $\\sigma_{\\text{test}}^2$ 在类别分布之间造成了显著的重叠，使得在测试集上的分类任务变得不平凡。\n\n**2. 分类器族的构建**\n\n为了隔离间隔的影响，我们固定分离超平面的方向，只改变其位置。我们选择一个方向向量 $\\boldsymbol{v} = \\frac{1}{\\sqrt{2}}(1,1)$，这是连接两个类别分布中心的最优方向。我们将分类器的权重向量固定为 $\\boldsymbol{w} = \\boldsymbol{v}$。由于 $\\|\\boldsymbol{v}\\|_2 = 1$，一个点 $(\\boldsymbol{x}_i, y_i)$ 的几何间隔从其通用定义 $\\gamma_i = \\frac{y_i(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_i + b)}{\\|\\boldsymbol{w}\\|_2}$ 简化为 $\\gamma_i = y_i(\\boldsymbol{v}^{\\top}\\boldsymbol{x}_i + b)$。这也被称为函数间隔。\n\n通过将数据点 $\\boldsymbol{x}_i$ 投影到方向 $\\boldsymbol{v}$ 上，问题实际上变成了一维问题。令 $p_i = \\boldsymbol{v}^{\\top}\\boldsymbol{x}_i$。为了使训练集能被一个法向量为 $\\boldsymbol{v}$ 的超平面线性分离，投影后的正负点不得重叠。我们定义：\n- $p_{\\min}^+ = \\min_{i: y_i = +1} p_i$，正训练点的最小投影。\n- $p_{\\max}^- = \\max_{i: y_i = -1} p_i$，负训练点的最大投影。\n\n可分性条件是 $p_{\\min}^+ > p_{\\max}^-$。这些点之间的空间是投影间隙，$g = p_{\\min}^+ - p_{\\max}^-$。\n对于一个方向为 $\\boldsymbol{v}$ 的分类器，在该训练集上可能的最大几何间隔是通过将决策边界置于此间隙的中点来实现的。从中点到 $p_{\\min}^+$ 或 $p_{\\max}^-$ 的距离是 $\\frac{g}{2}$。因此，最大可行几何间隔是 $m_{\\max} = \\frac{g}{2}$。\n\n现在我们构建一个由目标间隔 $m$ 参数化的分类器族，其中 $0  m \\le m_{\\max}$。对于每个 $m$，分类器由 $(\\boldsymbol{w}, b(m))$ 定义，其中 $\\boldsymbol{w}=\\boldsymbol{v}$，偏置 $b(m)$ 的选择是为了确保训练集上的最小几何间隔恰好为 $m$。我们设置偏置，使得离边界最近的正训练点（即定义 $p_{\\min}^+$ 的那个点）的间隔恰好为 $m$。\n该点的分数为 $\\boldsymbol{v}^{\\top}\\boldsymbol{x} + b(m) = p_{\\min}^+ + b(m)$。由于其标签为 $y=+1$，我们要求其间隔为 $1 \\cdot (p_{\\min}^+ + b(m)) = m$。解出偏置可得：\n$$\nb(m) = m - p_{\\min}^+\n$$\n我们来验证这个选择。对于任何正训练点 $(\\boldsymbol{x}_i, y_i=+1)$，其间隔为 $y_i(\\boldsymbol{v}^{\\top}\\boldsymbol{x}_i + b(m)) = 1 \\cdot (p_i + m - p_{\\min}^+) \\ge p_{\\min}^+ + m - p_{\\min}^+ = m$。\n对于任何负训练点 $(\\boldsymbol{x_j}, y_j=-1)$，其间隔为 $y_j(\\boldsymbol{v}^{\\top}\\boldsymbol{x}_j + b(m)) = -1 \\cdot (p_j + m - p_{\\min}^+) = p_{\\min}^+ - p_j - m$。由于 $p_j \\le p_{\\max}^-$，该间隔至少为 $p_{\\min}^+ - p_{\\max}^- - m = g - m$。为了使该间隔至少为 $m$，我们需要 $g - m \\ge m$，这意味着 $g \\ge 2m$，或 $m \\le \\frac{g}{2} = m_{\\max}$。\n这证实了对于任何 $m \\le m_{\\max}$，我们的分类器构造都保证了所有训练点的几何间隔至少为 $m$。\n\n**3. 损失函数和间隔**\n\nHinge 损失定义为 $\\ell_{\\text{hinge}}(y, f) = \\max\\{0, 1 - yf\\}$。对于我们的分类器，$yf = y(\\boldsymbol{v}^{\\top}\\boldsymbol{x} + b(m))$，即几何间隔。训练集上的最小间隔是 $m$。因此，对于每个训练点，其间隔都 $\\ge m$。如果我们选择 $m \\ge 1$，Hinge 损失就变为 $\\max\\{0, 1 - (\\text{间隔})\\} \\le \\max\\{0, 1-1\\} = 0$。这意味着我们族中任何 $m \\ge 1$ 的分类器在训练集上都能实现零经验 Hinge 损失。一个仅基于最小化 Hinge 损失的训练算法将没有动力将间隔增加到超过 $m=1$。\n\n交叉熵损失为 $\\ell_{\\text{ce}}(y, f) = \\log(1 + \\exp(-yf))$。项 $-yf$ 是几何间隔的负值。随着间隔 $m$ 的增加，$-yf$ 变得更负，$\\exp(-yf)$ 趋近于 $0$，$\\log(1 + \\exp(-yf))$ 也趋近于 $0$。由于对于 $z>0$，$\\log(1+z)$ 是严格递增的，所以损失是间隔的严格递减函数。即使间隔已经大于 $1$，通过增加它仍然可以进一步减小损失。因此，使用交叉熵损失进行训练会鼓励将间隔推向尽可能大的方向，即朝向 $m_{\\max}$。\n\n**4. 在测试集上的经验评估**\n\n我们现在将通过计算测试集上的 $0$-$1$ 分类风险来量化这种差异的实际后果。$0$-$1$ 风险是错分的测试点 $(\\boldsymbol{x}, y)$ 的比例。当分数的符号与标签不匹配时，即 $y(\\boldsymbol{v}^{\\top}\\boldsymbol{x} + b(m)) \\le 0$ 时，发生错分。我们将为以下目标间隔集计算此风险：$m \\in \\{1.0, 1.5, 2.5, 3.5, m_{\\max}-0.1\\}$。我们预期，将间隔从 Hinge 损失所满足的 $m=1.0$ 值增加，将会改善泛化能力并降低测试误差，从而说明交叉熵损失所鼓励的行为带来的好处。提供的代码实现了这一计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs an example to illustrate the relationship between geometric margin,\n    loss functions (hinge vs. cross-entropy), and generalization error.\n    \"\"\"\n    # 0. Define problem parameters and set seed for reproducibility.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    mu_pos = np.array([3, 3])\n    mu_neg = np.array([-3, -3])\n\n    n_train_per_class = 8\n    n_train = 2 * n_train_per_class\n    sigma_train = 0.3\n\n    n_test_per_class = 1000\n    n_test = 2 * n_test_per_class\n    sigma_test = 1.2\n\n    # 1. Generate training dataset\n    train_cov = sigma_train**2 * np.identity(2)\n    train_x_pos = rng.multivariate_normal(mu_pos, train_cov, size=n_train_per_class)\n    train_x_neg = rng.multivariate_normal(mu_neg, train_cov, size=n_train_per_class)\n    \n    train_X = np.vstack((train_x_pos, train_x_neg))\n    train_y = np.hstack((np.ones(n_train_per_class), -np.ones(n_train_per_class)))\n\n    # 2. Generate test dataset\n    test_cov = sigma_test**2 * np.identity(2)\n    test_x_pos = rng.multivariate_normal(mu_pos, test_cov, size=n_test_per_class)\n    test_x_neg = rng.multivariate_normal(mu_neg, test_cov, size=n_test_per_class)\n\n    test_X = np.vstack((test_x_pos, test_x_neg))\n    test_y = np.hstack((np.ones(n_test_per_class), -np.ones(n_test_per_class)))\n\n    # 3. Fix direction and compute maximum feasible margin on the training set\n    v = np.array([1, 1]) / np.sqrt(2) # Unit-norm direction vector\n\n    # Project training points onto the direction v\n    projections = train_X @ v\n    p_pos = projections[train_y == 1]\n    p_neg = projections[train_y == -1]\n\n    p_min_pos = np.min(p_pos)\n    p_max_neg = np.max(p_neg)\n\n    # Calculate the projection gap and maximum margin\n    projection_gap = p_min_pos - p_max_neg\n    m_max = projection_gap / 2.0\n    \n    # 4. Define the set of target margins to test\n    margins_to_test = [\n        1.0,\n        1.5,\n        2.5,\n        3.5,\n        m_max - 0.1\n    ]\n\n    results = []\n\n    # 5. Compute test risk for each target margin\n    for m in margins_to_test:\n        if m > m_max:\n            # This case should not happen given the problem parameters,\n            # but it is a good sanity check.\n            raise ValueError(f\"Target margin m={m} exceeds maximum feasible margin m_max={m_max}\")\n\n        # Construct the classifier (w=v, b(m))\n        # Bias b(m) is set to achieve margin exactly m for the closest positive point\n        b_m = m - p_min_pos\n        w = v\n\n        # Calculate scores on the test set: f(x) = w^T x + b\n        scores = test_X @ w + b_m\n\n        # A point is misclassified if y * f(x) = 0\n        y_times_f = test_y * scores\n        misclassified_count = np.sum(y_times_f = 0)\n\n        # Compute empirical 0-1 risk\n        risk = misclassified_count / n_test\n        results.append(risk)\n\n    # 6. Print results in the specified format\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界的数据集常常包含噪声，例如标记错误的样本或极端离群点。一个健壮的分类器应该能够抵抗这些离群点的过度影响。这个练习构建了一个包含极端误标记离群点的数据集，用以检验交叉熵和合页损失的健壮性，并与两种“有界”的鲁棒损失函数进行对比，从而揭示了不同损失函数在面对异常数据时的敏感度差异。",
            "id": "3108631",
            "problem": "考虑一个二维空间中的二元分类问题，其标签在 $\\{-1,+1\\}$ 中。线性分类器为 $f(x)=w^\\top x + b$，其中参数向量为 $w \\in \\mathbb{R}^2$，偏置为 $b \\in \\mathbb{R}$。对于一个样本 $(x_i,y_i)$，定义带符号间隔 $u_i = y_i f(x_i)$。对 $w$ 进行 $\\ell_2$ 正则化的经验风险为\n$$\nR(w,b) = \\frac{1}{n}\\sum_{i=1}^n L(u_i) + \\lambda \\lVert w \\rVert_2^2,\n$$\n其中 $L$ 是一个逐点损失函数，$\\lambda > 0$ 是正则化强度。您将分析不同损失函数如何响应一个极端的错误标记离群点。\n\n使用以下损失函数的基本定义：\n- 交叉熵（逻辑）损失：$L_{\\text{log}}(u) = \\log\\left(1 + e^{-u}\\right)$。\n- Hinge 损失：$L_{\\text{hinge}}(u) = \\max\\left(0, 1 - u\\right)$。\n- Ramp 损失（一种截断的 Hinge 型鲁棒损失）：$L_{\\text{ramp}}(u) = \\max\\left(0, \\min\\left(1 - u, 1\\right)\\right)$。\n- 截断交叉熵（有上限的逻辑）损失，上限为 $\\tau$：$L_{\\text{tlog}}(u) = \\min\\left(\\log\\left(1 + e^{-u}\\right), \\tau\\right)$。\n\n通过在 $R(w,b)$ 上使用全批量梯度下降进行固定次数和步长的训练，使用以下由链式法则推导出的精确梯度规则：\n- 对于交叉熵，$\\frac{\\partial L_{\\text{log}}}{\\partial u} = -\\frac{1}{1+e^{u}}$。\n- 对于 Hinge 损失，$\\frac{\\partial L_{\\text{hinge}}}{\\partial u} = -\\mathbb{I}\\{u  1\\}$，其中 $\\mathbb{I}\\{\\cdot\\}$ 是指示函数。\n- 对于 Ramp 损失，$\\frac{\\partial L_{\\text{ramp}}}{\\partial u} = \\begin{cases} -1  \\text{若 } 0 \\le u  1, \\\\ 0  \\text{其他情况} \\end{cases}$\n- 对于截断交叉熵，$\\frac{\\partial L_{\\text{tlog}}}{\\partial u} = \\begin{cases} -\\frac{1}{1+e^{u}}  \\text{若 } \\log(1+e^{-u})  \\tau, \\\\ 0  \\text{其他情况。} \\end{cases}$\n\n请注意，$\\frac{\\partial L}{\\partial f} = \\frac{\\partial L}{\\partial u} \\cdot y$，每个样本对梯度的贡献为 $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial f} \\cdot x$ 和 $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial f}$，正则化项对 $\\frac{\\partial R}{\\partial w}$ 的贡献为 $2\\lambda w$，对 $\\frac{\\partial R}{\\partial b}$ 的贡献为 $0$。\n\n数据集构建：\n- 固定一个随机种子以保证可复现性。\n- 从均值为 $(2,0)$、协方差矩阵为 $\\operatorname{diag}(0.25,0.25)$ 的高斯分布中为类别 $+1$ 抽取 $n_+=20$ 个内点。\n- 从均值为 $(-2,0)$、协方差矩阵相同的高斯分布中为类别 $-1$ 抽取 $n_-=20$ 个内点。\n- 在位置 $(d,d)$ 处附加一个标签为 $y=-1$ 的极端错误标记离群点，其中 $d>0$ 是一个参数。这样，每个测试案例总共有 $n=41$ 个样本。\n\n训练协议：\n- 初始化 $w=(0,0)$ 和 $b=0$。\n- 使用学习率 $\\eta = 0.001$，迭代次数 $T = 1000$，正则化强度 $\\lambda = 0.001$。\n- 训练四个独立的模型，分别对应 $L_{\\text{log}}$、$L_{\\text{hinge}}$、$L_{\\text{ramp}}$ 和 $L_{\\text{tlog}}$，其中截断上限 $\\tau=2.0$。\n\n鲁棒性分析指标：\n- 训练完每个模型后，计算离群点损失分数\n$$\nF_L(d) = \\frac{L(u_{\\text{outlier}})}{\\sum_{i=1}^{n} L(u_i)},\n$$\n其中 $L$ 为对应的损失函数。较小的 $F_L(d)$ 表示离群点在总训练损失中占比较小，因此该损失函数对该离群点更具鲁棒性。\n\n测试套件：\n- 考虑四个测试案例，离群点距离 $d \\in \\{3, 10, 30, 100\\}$。\n- 对每个 $d$，计算以下布尔结果：\n    1. $B_1(d)$: Hinge 损失是否比交叉熵损失更鲁棒，定义为 $F_{\\text{hinge}}(d)  F_{\\text{log}}(d)$。\n    2. $B_2(d)$: 截断交叉熵损失是否比 Hinge 损失提高了鲁棒性，定义为 $F_{\\text{tlog}}(d)  F_{\\text{hinge}}(d)$。\n    3. $B_3(d)$: Ramp 损失是否比 Hinge 损失提高了鲁棒性，定义为 $F_{\\text{ramp}}(d)  F_{\\text{hinge}}(d)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个测试距离的布尔值三元组，以逗号分隔的列表形式并用方括号括起来。即，以 $[[B_1(d_1),B_2(d_1),B_3(d_1)], [B_1(d_2),B_2(d_2),B_3(d_2)], [B_1(d_3),B_2(d_3),B_3(d_3)], [B_1(d_4),B_2(d_4),B_3(d_4)]]$ 的形式打印一个列表的列表，其中 $d_1=3, d_2=10, d_3=30, d_4=100$。",
            "solution": "用户希望验证所提供的问题陈述。\n\n### 第一步：提取给定信息\n- **分类器**：在 $\\mathbb{R}^2$ 中的线性分类器定义为 $f(x) = w^\\top x + b$，其中 $w \\in \\mathbb{R}^2$ 是权重向量， $b \\in \\mathbb{R}$ 是偏置。\n- **标签**：二元类别标签为 $y \\in \\{-1, +1\\}$。\n- **带符号间隔**：对于样本 $(x_i, y_i)$，带符号间隔为 $u_i = y_i f(x_i)$。\n- **目标函数**：带 $\\ell_2$ 正则化的经验风险是 $R(w,b) = \\frac{1}{n}\\sum_{i=1}^n L(u_i) + \\lambda \\lVert w \\rVert_2^2$，其中 $L$ 是一个损失函数，$\\lambda > 0$。\n- **损失函数**：\n    - 交叉熵（逻辑）：$L_{\\text{log}}(u) = \\log\\left(1 + e^{-u}\\right)$。\n    - Hinge 损失：$L_{\\text{hinge}}(u) = \\max\\left(0, 1 - u\\right)$。\n    - Ramp 损失：$L_{\\text{ramp}}(u) = \\max\\left(0, \\min\\left(1 - u, 1\\right)\\right)$。\n    - 截断交叉熵：$L_{\\text{tlog}}(u) = \\min\\left(\\log\\left(1 + e^{-u}\\right), \\tau\\right)$。\n- **损失的梯度（相对于 $u$）**：\n    - $\\frac{\\partial L_{\\text{log}}}{\\partial u} = -\\frac{1}{1+e^{u}}$。\n    - $\\frac{\\partial L_{\\text{hinge}}}{\\partial u} = -\\mathbb{I}\\{u  1\\}$。\n    - $\\frac{\\partial L_{\\text{ramp}}}{\\partial u}$ 定义为：当 $u  0$ 或 $u \\ge 1$ 时为 $0$；当 $0 \\le u  1$ 时为 $-1$。\n    - $\\frac{\\partial L_{\\text{tlog}}}{\\partial u}$ 定义为：当 $\\log(1+e^{-u})  \\tau$ 时为 $-\\frac{1}{1+e^{u}}$；否则为 $0$。\n- **数据集生成**：\n    - 使用固定的随机种子以保证可复现性。\n    - 为类别 $+1$ 从均值为 $(2,0)$、协方差为 $\\operatorname{diag}(0.25,0.25)$ 的高斯分布中抽取 $n_+=20$ 个内点。\n    - 为类别 $-1$ 从均值为 $(-2,0)$、协方差相同的高斯分布中抽取 $n_-=20$ 个内点。\n    - 一个标签为 $y=-1$ 的错误标记离群点，位于位置 $(d,d)$。\n    - 总样本数 $n=41$。\n- **训练协议**：\n    - 优化器：全批量梯度下降。\n    - 初始化：$w=(0,0)$，$b=0$。\n    - 学习率：$\\eta = 0.001$。\n    - 迭代次数：$T = 1000$。\n    - 正则化强度：$\\lambda = 0.001$。\n    - 截断上限：$\\tau=2.0$。\n- **评估指标**：\n    - 离群点损失分数：$F_L(d) = L(u_{\\text{outlier}}) / \\sum_{i=1}^{n} L(u_i)$。\n- **测试套件**：\n    - 离群点距离：$d \\in \\{3, 10, 30, 100\\}$。\n    - 布尔比较：\n        1.  $B_1(d): F_{\\text{hinge}}(d)  F_{\\text{log}}(d)$。\n        2.  $B_2(d): F_{\\text{tlog}}(d)  F_{\\text{hinge}}(d)$。\n        3.  $B_3(d): F_{\\text{ramp}}(d)  F_{\\text{hinge}}(d)$。\n- **输出格式**：一个布尔值列表的列表的字符串表示，格式为 `[[B1(3),...], [B1(10),...], ...]`。\n\n### 第二步：使用提取的给定信息进行验证\n1.  **科学或事实上的不健全性**：该问题在科学上是合理的。它描述了一个统计学习中的标准数值实验，用于比较已确立的损失函数的鲁棒性属性。数学公式是正确的。\n2.  **非形式化或不相关**：该问题是形式化的、定量的，并且与机器学习中分类损失和鲁棒优化主题直接相关。\n3.  **不完整或矛盾的设置**：该问题是完全指定的。数据集生成、模型训练和评估所需的所有必要参数都已提供。没有矛盾之处。\n4.  **不切实际或不可行**：该设置是一个合成实验，这是机器学习研究中一种常见且有效的方法。参数和计算完全可行。\n5.  **不适定或结构不良**：该问题是适定的。在指定的目标函数上使用梯度下降，并有固定的初始条件、超参数和数据集，将产生一个确定性的结果（给定固定的随机种子）。评估指标清晰，并为每个比较得出唯一的布尔结果。\n6.  **伪深刻、琐碎或同义反复**：该问题并非琐碎。它需要完整实现一个机器学习训练和评估流程，并能提供对不同损失函数重要的、非显而易见的实践行为的洞见。\n7.  **超出科学可验证范围**：结果是计算上可验证的。任何使用给定种子实现指定程序的人都将获得相同的结果。\n\n### 第三步：结论与行动\n该问题是有效且适定的。\n\n此问题研究不同损失函数对一个错误标记离群点的鲁棒性。其核心原理在于，对于大的负间隔 $u$（对应于置信度很高的错误预测），损失函数的值和梯度如何表现。\n\n1.  **鲁棒性原理**：如果一个损失函数能限制离群点对最终模型的影响，则认为它对离群点是鲁棒的。\n    - **无界损失**：交叉熵（$L_{\\text{log}}$）和 Hinge（$L_{\\text{hinge}}$）损失是无界的。随着间隔 $u_i$ 变得非常负（即，点被严重误分类），$L(u_i)$ 会线性增长。这意味着单个极端离群点可以对总损失贡献巨大，其梯度将持续将决策边界拉向它，这可能会以牺牲正确分类内点为代价。\n    - **有界损失**：Ramp（$L_{\\text{ramp}}$）和截断交叉熵（$L_{\\text{tlog}}$）损失是有界的。对于 Ramp 损失，当 $u0$ 时损失上限为 $1$。对于截断交叉熵，损失上限为 $\\tau=2.0$。关键在于，对于严重误分类的点（$u$ 非常负），这些损失的梯度会变为零。优化器实际上“放弃”了这些点，它们不再影响模型的参数。这种行为是鲁棒损失函数的标志。\n\n2.  **方法论**：解决方案首先实现实验的各个组成部分。\n    - **数据生成**：对于每个距离 $d$，创建一个数据集，其中包含 40 个从两个独立高斯分布中抽取的内点，以及一个位于 $(d,d)$、标签为负的极端离群点。固定的随机数生成器状态确保了可复现性。\n    - **损失和梯度函数**：根据定义实现四种损失函数及其各自关于间隔 $u$ 的梯度（或次梯度）。\n    - **训练**：一个 `train` 函数实现全批量梯度下降。在 1000 次迭代中，它计算正则化风险 $R(w,b)$ 的梯度，并更新参数 $w$ 和 $b$。对四种损失函数中的每一种都重复此过程。\n    - **评估**：训练后，为每个模型计算离群点损失分数 $F_L(d)$。该指标量化了离群点对总损失的贡献。分数越小，表示鲁棒性越强。\n\n3.  **分析**：通过比较 $F_L(d)$ 的值来计算布尔结果 $B_1, B_2, B_3$。\n    - 我们预计 $B_2$ 和 $B_3$ 为 `True`，特别是对于大的 $d$。$L_{\\text{ramp}}$ 和 $L_{\\text{tlog}}$ 的有界性将限制离群点的损失，而无界的 Hinge 损失 $L_{\\text{hinge}}$ 在同一点上会非常大。这应该导致 $F_{\\text{tlog}} \\ll F_{\\text{hinge}}$ 和 $F_{\\text{ramp}} \\ll F_{\\text{hinge}}$。\n    - Hinge 损失和交叉熵损失之间的比较（$B_1$）先验上不那么明显。两者都是无界的，并且对于大的负间隔都呈线性增长（$L_{\\text{hinge}}(u) = 1-u$ 对比 $L_{\\text{log}}(u) \\approx -u$）。它们的相对表现取决于训练过程的复杂动态。仿真将揭示经验结果。\n\n最终输出是针对每个 $d$ 值的这些布尔三元组的列表，格式化为字符串。",
            "answer": "```python\nimport numpy as np\nfrom functools import partial\n\ndef log_loss(u):\n    \"\"\"Computes the logistic loss.\"\"\"\n    return np.log(1 + np.exp(-u))\n\ndef log_loss_grad(u):\n    \"\"\"Computes the gradient of the logistic loss.\"\"\"\n    return -1 / (1 + np.exp(u))\n\ndef hinge_loss(u):\n    \"\"\"Computes the hinge loss.\"\"\"\n    return np.maximum(0, 1 - u)\n\ndef hinge_loss_grad(u):\n    \"\"\"Computes the subgradient of the hinge loss.\"\"\"\n    return -1.0 * (u  1)\n\ndef ramp_loss(u):\n    \"\"\"Computes the ramp loss.\"\"\"\n    return np.maximum(0, np.minimum(1 - u, 1))\n    \ndef ramp_loss_grad(u):\n    \"\"\"Computes the subgradient of the ramp loss.\"\"\"\n    grad = np.zeros_like(u)\n    mask = (u >= 0)  (u  1)\n    grad[mask] = -1.0\n    return grad\n\ndef tlog_loss(u, tau):\n    \"\"\"Computes the truncated logistic loss.\"\"\"\n    return np.minimum(log_loss(u), tau)\n\ndef tlog_loss_grad(u, tau):\n    \"\"\"Computes the subgradient of the truncated logistic loss.\"\"\"\n    grad = log_loss_grad(u)\n    # The gradient is zero where the loss is capped.\n    # The condition for capping is log(1 + exp(-u)) >= tau.\n    grad[log_loss(u) >= tau] = 0.0\n    return grad\n\ndef generate_data(d, rng):\n    \"\"\"Generates the dataset with inliers and one outlier.\"\"\"\n    n_plus = 20\n    n_minus = 20\n    mean_plus = np.array([2, 0])\n    mean_minus = np.array([-2, 0])\n    cov = np.array([[0.25, 0], [0, 0.25]])\n    \n    # Inliers\n    X_plus = rng.multivariate_normal(mean_plus, cov, n_plus)\n    y_plus = np.ones(n_plus)\n    X_minus = rng.multivariate_normal(mean_minus, cov, n_minus)\n    y_minus = -np.ones(n_minus)\n    \n    # Outlier\n    X_outlier = np.array([[d, d]])\n    y_outlier = np.array([-1])\n    \n    X = np.vstack((X_plus, X_minus, X_outlier))\n    y = np.hstack((y_plus, y_minus, y_outlier))\n    \n    return X, y\n\ndef train(X, y, loss_grad_fn, T, eta, lambda_reg):\n    \"\"\"Trains a linear classifier using gradient descent.\"\"\"\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    for _ in range(T):\n        f = X @ w + b\n        u = y * f\n        \n        grad_L_u = loss_grad_fn(u)\n        \n        grad_w_loss = (1 / n_samples) * X.T @ (y * grad_L_u)\n        grad_b_loss = (1 / n_samples) * np.sum(y * grad_L_u)\n        \n        grad_w = grad_w_loss + 2 * lambda_reg * w\n        grad_b = grad_b_loss\n        \n        w -= eta * grad_w\n        b -= eta * grad_b\n        \n    return w, b\n\ndef solve():\n    \"\"\"Main function to run the experiment and produce the final output.\"\"\"\n    test_ds = [3, 10, 30, 100]\n    eta = 0.001\n    T = 1000\n    lambda_reg = 0.001\n    tau = 2.0\n    seed = 42\n    \n    rng = np.random.default_rng(seed)\n\n    loss_functions = {\n        'log': log_loss,\n        'hinge': hinge_loss,\n        'ramp': ramp_loss,\n        'tlog': partial(tlog_loss, tau=tau)\n    }\n    loss_grad_functions = {\n        'log': log_loss_grad,\n        'hinge': hinge_loss_grad,\n        'ramp': ramp_loss_grad,\n        'tlog': partial(tlog_loss_grad, tau=tau)\n    }\n\n    all_results = []\n\n    for d in test_ds:\n        X, y = generate_data(d, rng)\n        \n        trained_models = {}\n        for name, grad_fn in loss_grad_functions.items():\n            w, b = train(X, y, grad_fn, T, eta, lambda_reg)\n            trained_models[name] = (w, b)\n            \n        loss_fractions = {}\n        for name, model in trained_models.items():\n            w, b = model\n            loss_fn = loss_functions[name]\n            \n            f = X @ w + b\n            u = y * f\n            \n            all_losses = loss_fn(u)\n            outlier_loss = all_losses[-1]\n            total_loss = np.sum(all_losses)\n            \n            # Avoid division by zero, though unlikely here.\n            loss_fractions[name] = outlier_loss / (total_loss + 1e-12)\n\n        F_log = loss_fractions['log']\n        F_hinge = loss_fractions['hinge']\n        F_tlog = loss_fractions['tlog']\n        F_ramp = loss_fractions['ramp']\n\n        B1 = F_hinge  F_log\n        B2 = F_tlog  F_hinge\n        B3 = F_ramp  F_hinge\n        \n        all_results.append([B1, B2, B3])\n\n    # Format the output string as a list of lists of booleans\n    sublist_strs = []\n    for res_list in all_results:\n        # Convert each boolean in the sublist to its string representation\n        # and join with commas. Enclose in square brackets.\n        sublist_str = f\"[{','.join(str(b).lower() for b in res_list)}]\"\n        sublist_strs.append(sublist_str)\n    \n    # Join the sublist strings with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(sublist_strs)}]\"\n    print(final_output.replace(\"true\", \"True\").replace(\"false\", \"False\"))\n\nsolve()\n```"
        }
    ]
}