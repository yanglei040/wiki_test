{
    "hands_on_practices": [
        {
            "introduction": "在线性模型中，当特征数量过多或存在多重共线性时，模型很容易过拟合训练数据。岭回归通过在损失函数中加入一个对模型系数大小的惩罚项来对抗过拟合，从而有效降低模型复杂度。这个练习  将指导你沿着正则化参数 $\\lambda$ 的路径进行探索，亲手计算并观察训练误差和测试误差的变化，直观地理解增加模型偏差（更高的训练误差）如何能够换来方差的降低，从而提高模型的泛化能力。",
            "id": "3188165",
            "problem": "您将执行一个明确定义的比较任务，比较岭回归在正则化路径上训练误差与测试误差的变化。该任务完全基于使用平方损失的经验风险最小化来构建。对于每个测试用例，您将使用三角函数（角度以弧度为单位）构建确定性的设计矩阵和响应，计算在一组预设的正则化强度网格上的岭回归估计量，并报告最小化测试误差的正则化值，同时检查相应的训练误差并非网格上的最小训练误差。\n\n待使用的基本定义：\n- 设 $X \\in \\mathbb{R}^{n \\times d}$ 为设计矩阵，$y \\in \\mathbb{R}^{n}$ 为响应向量。使用平方损失的岭回归定义为以下优化问题的解 $\\hat{\\beta}_{\\lambda} \\in \\mathbb{R}^{d}$\n$$\n\\hat{\\beta}_{\\lambda} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{d}} \\left\\{\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}\\right\\}。\n$$\n- 当 $\\lambda = 0$ 时，该问题简化为普通最小二乘经验风险最小化。如果正规方程存在多个解（例如，当 $n  d$ 时），则使用 Moore–Penrose 伪逆来获得最小化经验风险的最小欧几里得范数解。\n- 对于任何估计量 $\\hat{\\beta}$，训练均方误差 (Mean Squared Error (MSE)) 为\n$$\nR_{\\text{train}}(\\hat{\\beta}) = \\frac{1}{n}\\|y - X\\hat{\\beta}\\|_{2}^{2}，\n$$\n对于一个包含 $m$ 个样本的测试集 $(X_{\\text{test}}, y_{\\text{test}})$，测试均方误差为\n$$\nR_{\\text{test}}(\\hat{\\beta}) = \\frac{1}{m}\\|y_{\\text{test}} - X_{\\text{test}}\\hat{\\beta}\\|_{2}^{2}。\n$$\n\n任务：\n- 对于每个给定的测试用例，计算正则化网格\n$$\n\\Lambda = [\\,0,\\,10^{-8},\\,10^{-6},\\,10^{-4},\\,10^{-3},\\,10^{-2},\\,10^{-1},\\,1,\\,10,\\,100\\,]\n$$\n中每个 $\\lambda$ 对应的 $\\hat{\\beta}_{\\lambda}$。\n- 对于每个 $\\lambda \\in \\Lambda$，计算 $R_{\\text{train}}(\\hat{\\beta}_{\\lambda})$ 和 $R_{\\text{test}}(\\hat{\\beta}_{\\lambda})$。\n- 找出最小化 $R_{\\text{test}}(\\hat{\\beta}_{\\lambda})$ 的 $\\lambda^{\\star} \\in \\Lambda$ 的索引 $k^{\\star}$。如果存在平局，选择最小的索引（即首次出现的位置）。\n- 判断在 $\\lambda^{\\star}$ 处的训练误差是否严格大于网格上的最小训练误差，即\n$$\nR_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}}) > \\min_{\\lambda \\in \\Lambda} R_{\\text{train}}(\\hat{\\beta}_{\\lambda}) + \\varepsilon,\n$$\n是否成立，数值容差为 $\\varepsilon = 10^{-12}$。\n\n角度单位：所有三角函数均使用弧度作为输入。\n\n最终输出格式：\n- 对每个测试用例，输出一个列表 $[k^{\\star}, \\lambda^{\\star}, R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}}), \\text{train\\_nonmin}]$，其中：\n  - $k^{\\star}$ 是 $\\Lambda$ 的整数索引，\n  - $\\lambda^{\\star}$ 是一个四舍五入到 $6$ 位小数的浮点数，\n  - $R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}})$ 是一个四舍五入到 $6$ 位小数的浮点数，\n  - $\\text{train\\_nonmin}$ 是一个布尔值，表示 $R_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}})$ 是否严格大于网格上的最小训练均方误差（MSE）至少 $\\varepsilon$。\n- 将所有测试用例的结果聚合到一行中，打印成一个由方括号括起来的逗号分隔列表（例如，$[\\text{result\\_1},\\text{result\\_2},\\text{result\\_3}]$），其中每个 $\\text{result\\_i}$ 都是如上所述的列表。\n\n测试套件：\n根据以下规则构建三个确定性测试用例。所有角度均以弧度为单位。\n\n- 测试用例 $1$（欠定，$n  d$，噪声较大）：\n  - 训练集大小：$n = 8$, $d = 12$。测试集大小：$m = 200$。\n  - 对于 $i \\in \\{0,1,\\dots,n-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    X_{ij} = \\sin\\big((i+1)(j+1)\\big) + 0.1 \\cos(i - 2j).\n    $$\n  - 对于 $i \\in \\{0,1,\\dots,m-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (X_{\\text{test}})_{ij} = \\cos\\left(\\frac{(i+1)(j+1)}{2}\\right) + 0.1 \\sin(i + j).\n    $$\n  - 此用例的真实参数：对于 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (\\beta^{\\star})_{j} = (-1)^{j} \\cdot \\frac{j+1}{d}.\n    $$\n  - 噪声：\n    $$\n    \\varepsilon_{i} = 0.3 \\cdot \\sin(3i + 1) \\quad \\text{对于 } i \\in \\{0,\\dots,n-1\\}, \\quad\n    \\varepsilon^{\\text{test}}_{i} = 0.3 \\cdot \\cos(5i + 2) \\quad \\text{对于 } i \\in \\{0,\\dots,m-1\\}.\n    $$\n  - 响应：$y = X \\beta^{\\star} + \\varepsilon$ 和 $y_{\\text{test}} = X_{\\text{test}} \\beta^{\\star} + \\varepsilon^{\\text{test}}$。\n\n- 测试用例 $2$（超定但具有强共线性，中等噪声）：\n  - 训练集大小：$n = 40$, $d = 10$。测试集大小：$m = 200$。\n  - 对于 $i \\in \\{0,1,\\dots,n-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    X_{ij} = \\sin\\big(0.2(i+1)\\big) + 0.01 \\cos\\big(0.3(i+1) + 0.1(j+1)\\big).\n    $$\n  - 对于 $i \\in \\{0,1,\\dots,m-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (X_{\\text{test}})_{ij} = \\sin\\big(0.21(i+1)\\big) + 0.01 \\cos\\big(0.31(i+1) + 0.11(j+1)\\big).\n    $$\n  - 此用例的真实参数：对于 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (\\beta^{\\star})_{j} = \\frac{1}{j+1}.\n    $$\n  - 噪声：\n    $$\n    \\varepsilon_{i} = 0.2 \\cdot \\cos\\big(0.5(i+1)\\big), \\quad\n    \\varepsilon^{\\text{test}}_{i} = 0.2 \\cdot \\sin\\big(0.4(i+1)\\big).\n    $$\n  - 响应：$y = X \\beta^{\\star} + \\varepsilon$ 和 $y_{\\text{test}} = X_{\\text{test}} \\beta^{\\star} + \\varepsilon^{\\text{test}}$。\n\n- 测试用例 $3$（适定，低噪声）：\n  - 训练集大小：$n = 200$, $d = 5$。测试集大小：$m = 200$。\n  - 对于 $i \\in \\{0,1,\\dots,n-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    X_{ij} = \\sin\\left(\\frac{(i+1)(j+2)}{7}\\right) + 0.2 \\cos\\big((i+1) + 0.3(j+1)\\big).\n    $$\n  - 对于 $i \\in \\{0,1,\\dots,m-1\\}$ 和 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (X_{\\text{test}})_{ij} = \\sin\\left(\\frac{(i+1)(j+2)}{7.3}\\right) + 0.2 \\cos\\big((i+1) + 0.33(j+1)\\big).\n    $$\n  - 此用例的真实参数：对于 $j \\in \\{0,1,\\dots,d-1\\}$，\n    $$\n    (\\beta^{\\star})_{j} = (-0.7)^{j}.\n    $$\n  - 噪声：\n    $$\n    \\varepsilon_{i} = 0.02 \\cdot \\sin\\big(0.1(i+1) + 0.3\\big), \\quad\n    \\varepsilon^{\\text{test}}_{i} = 0.02 \\cdot \\cos\\big(0.12(i+1) - 0.1\\big).\n    $$\n  - 响应：$y = X \\beta^{\\star} + \\varepsilon$ 和 $y_{\\text{test}} = X_{\\text{test}} \\beta^{\\star} + \\varepsilon^{\\text{test}}$。\n\n实现要求：\n- 对于 $\\lambda > 0$，通过求解岭回归目标函数的一阶最优性条件来获得 $\\hat{\\beta}_{\\lambda}$。\n- 对于 $\\lambda = 0$，使用 Moore–Penrose 伪逆获得最小化经验风险的最小欧几里得范数解。\n- 在检查训练误差是否非最小时，使用数值容差 $\\varepsilon = 10^{-12}$ 来比较实数的严格不等关系。\n- 将输出中的所有浮点数四舍五入到 $6$ 位小数。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个元素是对应测试用例（按 1、2、3 的顺序）的列表 $[k^{\\star}, \\lambda^{\\star}, R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}}), \\text{train\\_nonmin}]$。",
            "solution": "该问题要求对岭回归在指定正则化参数网格 $\\Lambda$ 上的训练误差和测试误差进行比较分析。这是统计学习中的一个经典练习，用以说明偏差-方差权衡。题目提供了一个确定性流程，用于为三种不同场景构建训练和测试数据。\n\n问题的核心在于求解岭回归优化问题，以获得估计量 $\\hat{\\beta}_{\\lambda}$：\n$$\n\\hat{\\beta}_{\\lambda} = \\arg\\min_{\\beta \\in \\mathbb{R}^{d}} \\left\\{\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}\\right\\}\n$$\n其中 $X \\in \\mathbb{R}^{n \\times d}$ 是设计矩阵，$y \\in \\mathbb{R}^{n}$ 是响应向量，$\\lambda \\ge 0$ 是正则化参数。\n\n为求得解 $\\hat{\\beta}_{\\lambda}$，我们通过对目标函数关于 $\\beta$ 求梯度并令其为零，来推导一阶最优性条件：\n$$\n\\nabla_{\\beta} \\left( \\frac{1}{n}(y - X\\beta)^T(y - X\\beta) + \\lambda \\beta^T\\beta \\right) = 0\n$$\n$$\n\\frac{1}{n} \\nabla_{\\beta} (y^T y - 2y^T X\\beta + \\beta^T X^T X \\beta) + \\lambda (2\\beta) = 0\n$$\n$$\n\\frac{1}{n} (-2X^T y + 2X^T X \\beta) + 2\\lambda\\beta = 0\n$$\n两边乘以 $n/2$，得到：\n$$\n-X^T y + X^T X \\beta + n\\lambda\\beta = 0\n$$\n$$\n(X^T X + n\\lambda I) \\beta = X^T y\n$$\n对于 $\\lambda > 0$，矩阵 $(X^T X + n\\lambda I)$ 保证是可逆的，因为 $X^T X$ 是半正定的，加上一个单位矩阵 $I$ 的正倍数使其变为正定矩阵。因此，唯一解为：\n$$\n\\hat{\\beta}_{\\lambda} = (X^T X + n\\lambda I)^{-1} X^T y\n$$\n对于 $\\lambda = 0$ 的特殊情况，问题简化为普通最小二乘法 (OLS)。如果矩阵 $X^T X$ 是奇异的（例如，在 $n  d$ 的欠定情况下），则存在无穷多个解可以最小化训练误差。题目指定使用最小欧几里得范数解，该解由 Moore-Penrose 伪逆 $X^{+}$ 唯一给出：\n$$\n\\hat{\\beta}_{0} = X^{+} y\n$$\n\n计算流程如下：\n1.  对于每个测试用例，根据指定的确定性公式生成训练数据 $(X, y)$ 和测试数据 $(X_{\\text{test}}, y_{\\text{test}})$。参数 $n, d, m$ 以及用于生成矩阵、真实参数向量 $\\beta^{\\star}$ 和噪声项的函数都已提供。为提高效率，使用向量化的 `numpy` 操作实现数据生成。\n2.  遍历给定网格 $\\Lambda = [\\,0,\\,10^{-8},\\,10^{-6},\\,10^{-4},\\,10^{-3},\\,10^{-2},\\,10^{-1},\\,1,\\,10,\\,100\\,]$ 中的每个正则化强度 $\\lambda$。\n3.  在每次迭代中，使用相应的公式计算系数向量 $\\hat{\\beta}_{\\lambda}$：对于 $\\lambda=0$，使用 `np.linalg.pinv(X) @ y`；对于 $\\lambda>0$，使用 `np.linalg.inv(XTX + n*lambda*I) @ XTy`。\n4.  利用计算出的 $\\hat{\\beta}_{\\lambda}$，计算训练和测试的均方误差 (MSE)：\n    $$\n    R_{\\text{train}}(\\hat{\\beta}_{\\lambda}) = \\frac{1}{n}\\|y - X\\hat{\\beta}_{\\lambda}\\|_{2}^{2}\n    $$\n    $$\n    R_{\\text{test}}(\\hat{\\beta}_{\\lambda}) = \\frac{1}{m}\\|y_{\\text{test}} - X_{\\text{test}}\\hat{\\beta}_{\\lambda}\\|_{2}^{2}\n    $$\n5.  在计算完所有 $\\lambda \\in \\Lambda$ 的误差后，找出使测试误差 $R_{\\text{test}}$ 最小的 $\\lambda^{\\star}$ 所对应的索引 $k^{\\star}$。如果出现平局，则选择最小的索引。\n6.  最后，检查在这个最优 $\\lambda^{\\star}$ 下的训练误差 $R_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}})$ 是否严格大于整个网格上观测到的最小训练误差。岭回归的训练误差是关于 $\\lambda$ 的单调非递减函数。因此，最小训练误差出现在 $\\lambda=0$ 处。检查 $R_{\\text{train}}(\\hat{\\beta}_{\\lambda^{\\star}}) > \\min_{\\lambda \\in \\Lambda} R_{\\text{train}}(\\hat{\\beta}_{\\lambda}) + \\varepsilon$（其中 $\\varepsilon=10^{-12}$）是为了验证从 OLS 解（具有最佳训练性能）转向正则化解是否能带来测试性能的提升，这是有效正则化的一个标志。\n7.  收集每个测试用例的结果—$[k^{\\star}, \\lambda^{\\star}, R_{\\text{test}}(\\hat{\\beta}_{\\lambda^{\\star}}), \\text{train\\_nonmin}]$—并将最终输出格式化为这些结果列表的字符串表示。",
            "answer": "```python\nimport numpy as np\n\ndef _solve_one_case(n, d, m, X_gen_rule, X_test_gen_rule, beta_star_gen_rule, eps_gen_rule, eps_test_gen_rule):\n    \"\"\"\n    Solves a single test case for the ridge regression problem.\n    \"\"\"\n    LAMBDA_GRID = [0.0, 1e-8, 1e-6, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]\n    EPSILON = 1e-12\n\n    # --- Data Generation ---\n    # Use 0-based indices for generation, consistent with problem statement's math notation.\n    \n    # Training set\n    i_train_idx = np.arange(n)[:, np.newaxis]\n    j_dim_idx = np.arange(d)[np.newaxis, :]\n    X = X_gen_rule(i_train_idx, j_dim_idx)\n\n    beta_star = beta_star_gen_rule(np.arange(d))\n    \n    eps_train = eps_gen_rule(np.arange(n))\n    \n    y = X @ beta_star + eps_train\n\n    # Test set\n    i_test_idx = np.arange(m)[:, np.newaxis]\n    X_test = X_test_gen_rule(i_test_idx, j_dim_idx)\n    \n    eps_test = eps_test_gen_rule(np.arange(m))\n    \n    y_test = X_test @ beta_star + eps_test\n    \n    # --- Ridge Path Calculation ---\n    train_errors = []\n    test_errors = []\n    \n    XTX = X.T @ X\n    XTy = X.T @ y\n    I_d = np.identity(d)\n    \n    for lambda_val in LAMBDA_GRID:\n        if lambda_val == 0.0:\n            # For lambda=0, use Moore-Penrose pseudoinverse for min-norm OLS solution\n            beta_hat = np.linalg.pinv(X) @ y\n        else:\n            # For lambda > 0, solve (X'X + n*lambda*I)beta = X'y\n            A = XTX + n * lambda_val * I_d\n            beta_hat = np.linalg.inv(A) @ XTy\n\n        # --- Error Calculation ---\n        # Training MSE: R_train = (1/n) * ||y - X*beta||^2\n        train_mse = np.mean(np.square(y - X @ beta_hat))\n        train_errors.append(train_mse)\n        \n        # Test MSE: R_test = (1/m) * ||y_test - X_test*beta||^2\n        test_mse = np.mean(np.square(y_test - X_test @ beta_hat))\n        test_errors.append(test_mse)\n        \n    # --- Analysis ---\n    # Find the index of lambda that minimizes test error\n    k_star = int(np.argmin(test_errors))\n    lambda_star = LAMBDA_GRID[k_star]\n    min_test_error = test_errors[k_star]\n\n    # Check if the training error at lambda_star is strictly greater than the minimum\n    min_train_error = np.min(train_errors)\n    train_error_at_best_lambda = train_errors[k_star]\n    train_nonmin = bool(train_error_at_best_lambda > min_train_error + EPSILON)\n\n    # --- Formatting ---\n    return [\n        k_star, \n        round(lambda_star, 6), \n        round(min_test_error, 6), \n        train_nonmin\n    ]\n\n\ndef solve():\n    \"\"\"\n    Main function to define and run all test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (underdetermined, n  d, noisier)\n        {\n            \"n\": 8, \"d\": 12, \"m\": 200,\n            \"X_gen_rule\": lambda i, j: np.sin((i+1)*(j+1)) + 0.1 * np.cos(i - 2*j),\n            \"X_test_gen_rule\": lambda i, j: np.cos((i+1)*(j+1)/2.0) + 0.1 * np.sin(i + j),\n            \"beta_star_gen_rule\": lambda j: ((-1)**j) * (j+1)/12.0,\n            \"eps_gen_rule\": lambda i: 0.3 * np.sin(3*i + 1),\n            \"eps_test_gen_rule\": lambda i: 0.3 * np.cos(5*i + 2)\n        },\n        # Test case 2 (overdetermined with strong collinearity, moderate noise)\n        {\n            \"n\": 40, \"d\": 10, \"m\": 200,\n            \"X_gen_rule\": lambda i, j: np.sin(0.2*(i+1)) + 0.01 * np.cos(0.3*(i+1) + 0.1*(j+1)),\n            \"X_test_gen_rule\": lambda i, j: np.sin(0.21*(i+1)) + 0.01 * np.cos(0.31*(i+1) + 0.11*(j+1)),\n            \"beta_star_gen_rule\": lambda j: 1.0/(j+1),\n            \"eps_gen_rule\": lambda i: 0.2 * np.cos(0.5*(i+1)),\n            \"eps_test_gen_rule\": lambda i: 0.2 * np.sin(0.4*(i+1))\n        },\n        # Test case 3 (well-determined, low noise)\n        {\n            \"n\": 200, \"d\": 5, \"m\": 200,\n            \"X_gen_rule\": lambda i, j: np.sin((i+1)*(j+2)/7.0) + 0.2 * np.cos((i+1) + 0.3*(j+1)),\n            \"X_test_gen_rule\": lambda i, j: np.sin((i+1)*(j+2)/7.3) + 0.2 * np.cos((i+1) + 0.33*(j+1)),\n            \"beta_star_gen_rule\": lambda j: (-0.7)**j,\n            \"eps_gen_rule\": lambda i: 0.02 * np.sin(0.1*(i+1) + 0.3),\n            \"eps_test_gen_rule\": lambda i: 0.02 * np.cos(0.12*(i+1) - 0.1)\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = _solve_one_case(**case_params)\n        results.append(result)\n\n    # The final output must be a string representation of a list of lists, with no spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "与参数化的线性模型不同，决策树等非参数模型通过递归地划分特征空间来进行预测。如果不加限制，决策树会生长到极致，完美地拟合训练集中的每一个样本（包括噪声），但这往往会导致模型在新的数据上表现不佳。在这个练习  中，你将构建一棵完全生长的决策树，见证其零训练误差下的过拟合现象，然后运用成本复杂度剪枝技术，寻找一个在训练误差和测试误差之间取得更佳平衡的简化模型。",
            "id": "3188147",
            "problem": "你必须编写一个完整且可运行的程序，在一个合成数据集上构建一个生长至纯的二元分类决策树，然后应用代价复杂度剪枝来展示经验训练误差与样本外测试误差之间的对比。你的代码必须实现所有逻辑，不得依赖外部机器学习库；只允许使用基础的数值计算库。本问题中的所有符号、变量和数字均使用 LaTeX 书写。\n\n你必须使用的基本基础包括以下定义和原则。\n\n- 使用0-1损失的经验风险：给定数据 $\\{(x_i,y_i)\\}_{i=1}^{n}$，其中 $x_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\{0,1\\}$，一个分类器 $h$ 的经验误分类计数为 $R(h) = \\sum_{i=1}^{n} \\mathbf{1}\\{h(x_i) \\neq y_i\\}$。经验误分类率为 $\\hat r(h) = R(h)/n$。\n- 样本外测试误分类率的定义类似，它是在一个与训练集来自相同数据生成机制的独立测试集上定义的；记为 $r_{\\text{test}}(h)$。\n- 决策树分类器通过轴对齐的阈值分割递归地划分 $\\mathbb{R}^d$，并为每个终端区域（叶节点）分配一个类别标签。叶节点的预测是落入该叶节点的训练样本中的多数类。\n- 完全生长至纯：持续分裂，直到每个叶节点只包含单一类别的样本，或者每个叶节点最多只包含一个训练样本。在特征向量各不相同的情况下，这将产生 $\\hat r_{\\text{train}} = 0$。\n- 代价复杂度剪枝：给定一棵树 $T$，定义带惩罚项的经验目标\n$$\nR_{\\alpha}(T) = R(T) + \\alpha \\cdot |T|,\n$$\n其中 $R(T)$ 是由 $T$ 在训练集上引起的经验误分类计数，而 $|T|$ 是 $T$ 中的叶节点数量。对于任何 $\\alpha \\ge 0$，剪枝后的子树 $T_{\\alpha}$ 在完全生长树的所有子树中使 $R_{\\alpha}(T)$ 最小化。\n\n你的程序必须实现：\n\n1. 使用轴对齐分割在训练集上训练一个二元决策树。在能够减少不纯度时，使用基尼不纯度来选择分割。如果没有分割能减少不纯度，但一个节点包含至少两个不同标签的样本，你仍必须进行分割以实现完全生长至纯（例如，沿着某个特征分离相邻的样本）。仅当每个叶节点都是纯的或只包含一个样本时才停止。\n2. 计算给定树的经验训练误分类率 $\\hat r_{\\text{train}}$ 和测试误分类率 $r_{\\text{test}}$。\n3. 计算完全生长树的标准最弱连接代价复杂度剪枝路径。在每个内部节点 $t$ 处，令 $R(t)$ 为如果 $t$ 被替换为一个预测该节点多数类的叶节点时的经验误分类计数，令 $R(T_t)$ 为在 $t$ 下的当前子树的误分类计数。令 $L_t$ 为节点 $t$ 下的叶节点数量。特定于节点的剪枝值为\n$$\ng(t) = \\frac{R(t) - R(T_t)}{L_t - 1}.\n$$\n同时剪去所有达到最小 $g(t)$ 值的内部节点以获得路径上的下一个子树，并重复此过程直到只剩下一个单叶节点的树。这将产生一个非递减序列 $\\alpha_0 \\le \\alpha_1 \\le \\dots$，其中每个 $\\alpha_k$ 是第 $k$ 步的最小 $g(t)$。\n4. 按如下方式为给定的 $\\alpha \\ge 0$ 选择子树：选择剪枝路径上关联阈值 $\\alpha_k \\le \\alpha$ 的最大子树。当 $\\alpha = 0$ 时，必须选择完全生长的树。\n\n数据生成必须是确定性的且科学上真实的：\n\n- 输入空间维度为 $d=2$。对于给定的样本量 $n$，生成特征 $X \\in \\mathbb{R}^{n \\times 2}$，其坐标在 $[0,1]$ 上独立均匀分布。使用由指定整数作为种子的确定性伪随机数生成器以确保可复现性。\n- 定义底层的无噪声标签为\n$$\ny^{\\star} = \\mathbf{1}\\{x_1 > \\tau(x_2)\\}, \\quad \\text{其中 } \\tau(u) = \\tfrac{1}{2} + \\tfrac{1}{4}\\sin(4\\pi u).\n$$\n- 独立地注入标签噪声：以概率 $\\eta \\in [0,1)$ 将 $y^{\\star}$ 翻转为 $1 - y^{\\star}$。为此翻转过程使用一个独立的、同样由确定性种子初始化的伪随机数生成器流。\n- 测试集使用其自己指定的种子从相同的机制中独立生成。将所有误分类率报告为 $[0,1]$ 区间内的实数。\n\n测试套件和要求的输出：\n\n实现以下三个测试用例。对于每个用例，你必须构建完全生长树 $T_{\\text{full}}$，计算其训练误分类率 $\\hat r_{\\text{train}}(T_{\\text{full}})$ 和测试误分类率 $r_{\\text{test}}(T_{\\text{full}})$，然后为指定的 $\\alpha$ 生成剪枝树 $T_{\\alpha}$ 并计算相应的比率。你的程序必须返回一个包含三个布尔值的单一列表，每个测试用例一个，并聚合成指定格式的单行字符串。\n\n- 测试用例 1（快乐路径：展示过拟合并通过剪枝改进）：\n  - 参数：$n_{\\text{train}} = 120$，$n_{\\text{test}} = 8000$，$\\eta = 0.25$，训练种子 $= 1337$，测试种子 $= 2027$。\n  - $\\alpha$ 的选择：扫描剪枝路径并选择使测试误分类率 $r_{\\text{test}}(T_{\\alpha})$ 最小化的子树 $T_{\\alpha}$。通过选择叶节点较少的子树来打破平局，如果仍然平局，则选择在剪枝路径上出现较晚的那个（对应于较大的 $\\alpha$）。你必须为此用例输出的布尔值是\n    $$\n    b_1 = \\big(\\hat r_{\\text{train}}(T_{\\text{full}}) = 0\\big) \\wedge \\big(r_{\\text{test}}(T_{\\alpha})  r_{\\text{test}}(T_{\\text{full}})\\big) \\wedge \\big(\\hat r_{\\text{train}}(T_{\\alpha}) > \\hat r_{\\text{train}}(T_{\\text{full}})\\big).\n    $$\n- 测试用例 2（边界情况：$\\alpha = 0$ 恢复为不剪枝）：\n  - 参数：$n_{\\text{train}} = 120$，$n_{\\text{test}} = 8000$，$\\eta = 0.25$，训练种子 $= 1337$，测试种子 $= 2027$，以及 $\\alpha = 0$。\n  - 你必须为此用例输出的布尔值是\n    $$\n    b_2 = \\big(\\hat r_{\\text{train}}(T_{\\alpha}) = \\hat r_{\\text{train}}(T_{\\text{full}})\\big) \\wedge \\big(r_{\\text{test}}(T_{\\alpha}) = r_{\\text{test}}(T_{\\text{full}})\\big).\n    $$\n- 测试用例 3（极端情况：非常大的 $\\alpha$ 会使树坍缩为树桩）：\n  - 参数：$n_{\\text{train}} = 120$，$n_{\\text{test}} = 8000$，$\\eta = 0.25$，训练种子 $= 1337$，测试种子 $= 2027$，以及 $\\alpha = 10.0$。\n  - 你必须为此用例输出的布尔值是\n    $$\n    b_3 = \\big(|T_{\\alpha}| = 1\\big) \\wedge \\big(\\hat r_{\\text{train}}(T_{\\alpha}) > \\hat r_{\\text{train}}(T_{\\text{full}})\\big).\n    $$\n\n最终输出格式：\n\n你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表的结果，顺序为 $[b_1,b_2,b_3]$。每个 $b_i$ 必须是一个布尔字面量。例如，一个有效的输出行格式为 `[`True`,`False`,`True`]`。\n\n此任务中不需要物理单位或角度单位。所有概率和比率必须作为 $[0,1]$ 区间内的实数返回，但只有布尔值 $b_1$、$b_2$ 和 $b_3$ 必须作为最终输出打印。",
            "solution": "用户的要求是开发一个 Python 程序，从基本原理出发实现一个二元分类决策树，包括代价复杂度剪枝算法。然后，该程序必须用于在一个指定的合成数据集上分析训练误差和测试误差之间的权衡。该问题定义明确，在统计学习理论方面有科学依据，并为确定性和可验证的解决方案提供了所有必要的参数和定义。因此，该问题是有效的。\n\n该解决方案被架构为一组交互组件，旨在忠实地实现指定的算法。\n\n**1. 数据生成**\n\n一个函数根据问题的数据生成过程来生成训练和测试数据集。此过程是确定性的，依赖于使用指定整数作为种子的伪随机数生成器来确保可复现性。\n- 特征空间为 $\\mathbb{R}^2$，坐标 $x_1$ 和 $x_2$ 从 $[0, 1]$ 上的均匀分布中独立抽取。\n- 一个真实的、无噪声的类别标签 $y^{\\star}$ 基于一个非线性决策边界被分配：\n$$\ny^{\\star} = \\mathbf{1}\\{x_1 > \\tau(x_2)\\}, \\quad \\text{其中 } \\tau(u) = \\frac{1}{2} + \\frac{1}{4}\\sin(4\\pi u)\n$$\n这个边界是一个正弦波，创建了一个复杂的分离任务。\n- 通过以给定概率 $\\eta$ 翻转真实标签 $y^{\\star}$ 来引入标签噪声。为此过程使用了一个独立的、带种子的随机数流，以确保噪声与特征生成无关。\n\n**2. 决策树生长**\n\n分类器的核心是一个递归生长的决策树。使用一个 `Node` 类来表示树结构。每个节点存储了划分给它的数据信息、分裂标准（如果是内部节点）或预测值（如果是叶节点）。\n\n生长算法按以下方式进行：\n- **递归划分**：从根节点的整个训练集开始，算法寻找最佳的轴对齐分裂来划分数据。\n- **分裂标准**：使用基尼不纯度来评估一次分裂的质量。对于一个样本集 $S$，基尼不纯度为 $G(S) = 1 - \\sum_{c \\in \\{0,1\\}} p_c^2$，其中 $p_c$ 是类别 $c$ 的样本比例。算法搜索所有特征和所有可能的分割阈值，以找到能使子节点的加权平均基尼不纯度最低的分裂。可能的分裂阈值取自连续唯一特征值的中点。\n- **停止与纯度规则**：如果一个节点变得“纯”（只包含一个类别的样本）或只包含单个样本，则该分支的递归终止。关键的是，为了达到指定的训练误差为零，如果一个节点不纯但没有分裂能减少基尼不纯度，仍然会强制进行分裂。这是通过找到第一个可用的特征并将第一个数据点与其余数据点分开来实现的，这保证了树将一直生长直到每个叶节点都为纯，从而有效地记忆训练数据，包括噪声。这将产生一个完全生长的树 $T_{\\text{full}}$，其经验训练误分类率 $\\hat r_{\\text{train}}(T_{\\text{full}}) = 0$。\n\n**3. 代价复杂度剪枝**\n\n完全生长的树 $T_{\\text{full}}$ 预计会对训练数据过拟合。实现代价复杂度剪枝是为了找到一系列更小、可能泛化能力更好的子树。\n- **最弱连接剪枝**：该算法通过迭代地修剪分支来生成一个子树序列。在每一步中，它识别出“最弱的连接”——即移除该内部节点会导致每叶节点的误分类代价增加最小。\n- **剪枝参数 $g(t)$**：对于当前树中的每个内部节点 $t$，计算一个参数 $g(t)$：\n$$\ng(t) = \\frac{R(t) - R(T_t)}{L_t - 1}\n$$\n这里，$R(t)$ 是如果将节点 $t$ 转换为叶节点时在训练数据上的误分类计数，$R(T_t)$ 是当前以 $t$ 为根的整个子树的总误分类计数，而 $L_t$ 是该子树中的叶节点数。\n- **路径生成**：算法从 $T_0 = T_{\\text{full}}$ 开始。在第 $k$ 步，它为当前树 $T_{k-1}$ 中的所有内部节点计算 $g(t)$。找到的最小值 $\\alpha_k = \\min_t g(t)$ 成为剪枝序列中的下一个阈值。通过取 $T_{k-1}$ 并将所有 $g(t) = \\alpha_k$ 的节点 $t$ 转换为叶节点，形成一棵新树 $T_k$。重复此过程直到只剩下根节点，从而产生一个树的序列 $[T_0, T_1, \\dots, T_m]$ 和相应的非递减的复杂度参数序列 $[\\alpha_0, \\alpha_1, \\dots, \\alpha_m]$，其中 $\\alpha_0=0$。使用树结构的深拷贝来维护序列中不同的树。\n\n**4. 评估与测试用例**\n\n程序随后使用生成的数据、完整的树和剪枝路径来评估指定的三个测试用例。\n- 对于给定的复杂度参数 $\\alpha$ 值，从生成的路径中选择最优子树 $T_\\alpha$。选择规则是从路径中选择树 $T_k$，使其对应的参数 $\\alpha_k$ 是小于或等于给定 $\\alpha$ 的最大值。\n- 为相关树（$T_{\\text{full}}$ 和 $T_\\alpha$）计算训练集和测试集的误分类率。\n- **测试用例 1**：此用例需要找到剪枝路径上使测试误差最小化的子树。搜索会遍历路径中的所有树，计算它们的测试误差，并根据指定的平局打破规则（偏好更小的树，然后是更大的 $\\alpha$）选择最佳树。布尔条件 $b_1$ 验证了完整树过拟合（$\\hat r_{\\text{train}}=0$），剪枝后的树泛化得更好（$r_{\\text{test}}(T_{\\alpha})  r_{\\text{test}}(T_{\\text{full}})$），并且这种改进是以牺牲训练性能为代价的（$\\hat r_{\\text{train}}(T_{\\alpha}) > \\hat r_{\\text{train}}(T_{\\text{full}})$）。\n- **测试用例 2**：当 $\\alpha=0$ 时，选择规则必须选择路径中的第一棵树，即 $T_{\\text{full}}$ 本身。布尔条件 $b_2$ 验证了所选树的误差与完整树的误差相同。\n- **测试用例 3**：当 $\\alpha$ 非常大，为 $10.0$ 时，选择规则预计将选择剪枝最严重的树，即单叶节点的根树桩。布尔条件 $b_3$ 验证了结果树只有一个叶节点（$|T_{\\alpha}| = 1$）并且其训练误差高于完整树。\n\n最终输出将这三个测试的布尔结果汇总成一个格式化的字符串。",
            "answer": "```python\nimport numpy as np\nimport copy\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main solver function to implement and test the decision tree and pruning algorithm.\n    \"\"\"\n\n    class Node:\n        \"\"\"\n        Represents a node in the decision tree.\n        \"\"\"\n        _id_counter = 0\n\n        def __init__(self, indices):\n            self.id = Node._id_counter\n            Node._id_counter += 1\n            \n            self.indices = np.array(indices, dtype=int)\n            \n            self.is_leaf = False\n            self.feature_idx = None\n            self.threshold = None\n            self.left = None\n            self.right = None\n            \n            self.value = None\n\n            self.n_samples = len(self.indices)\n            self.n0 = 0\n            self.n1 = 0\n            self.misclass_as_leaf = 0.0 # R(t)\n            self.gini = 0.0\n\n    class DecisionTree:\n        \"\"\"\n        Encapsulates the decision tree logic: growing, predicting, and helper methods.\n        \"\"\"\n        def __init__(self):\n            self.root = None\n\n        def _calculate_gini(self, y_node):\n            if len(y_node) == 0:\n                return 0.0\n            p1 = np.sum(y_node == 1) / len(y_node)\n            p0 = 1.0 - p1\n            return 1.0 - (p0**2 + p1**2)\n\n        def _find_split(self, X, y, indices):\n            if len(indices) = 1:\n                return None\n\n            parent_gini = self._calculate_gini(y[indices])\n            best_gini = parent_gini\n            best_split = None\n            n_features = X.shape[1]\n\n            for feature_idx in range(n_features):\n                unique_vals = np.unique(X[indices, feature_idx])\n                if len(unique_vals) = 1:\n                    continue\n\n                for i in range(len(unique_vals) - 1):\n                    threshold = (unique_vals[i] + unique_vals[i+1]) / 2.0\n                    \n                    left_indices = indices[X[indices, feature_idx] = threshold]\n                    right_indices = indices[X[indices, feature_idx] > threshold]\n                    \n                    if len(left_indices) == 0 or len(right_indices) == 0:\n                        continue\n\n                    gini_left = self._calculate_gini(y[left_indices])\n                    gini_right = self._calculate_gini(y[right_indices])\n                    \n                    w_left = len(left_indices) / len(indices)\n                    w_right = len(right_indices) / len(indices)\n                    \n                    weighted_gini = w_left * gini_left + w_right * gini_right\n                    \n                    if weighted_gini  best_gini:\n                        best_gini = weighted_gini\n                        best_split = {\n                            'feature_idx': feature_idx,\n                            'threshold': threshold,\n                            'left_indices': left_indices,\n                            'right_indices': right_indices\n                        }\n            \n            # Force-split rule for purity if no Gini improvement\n            if best_split is None and parent_gini > 0:\n                for feature_idx in range(n_features):\n                    sorted_indices = indices[np.argsort(X[indices, feature_idx])]\n                    if X[sorted_indices[0], feature_idx]  X[sorted_indices[-1], feature_idx]:\n                        threshold = (X[sorted_indices[0], feature_idx] + X[sorted_indices[1], feature_idx]) / 2.0\n                        left_indices = sorted_indices[X[sorted_indices, feature_idx] = threshold]\n                        right_indices = sorted_indices[X[sorted_indices, feature_idx] > threshold]\n                        best_split = {\n                            'feature_idx': feature_idx,\n                            'threshold': threshold,\n                            'left_indices': left_indices,\n                            'right_indices': right_indices\n                        }\n                        break\n            \n            return best_split\n\n        def _grow(self, X, y, indices):\n            node = Node(indices)\n            y_node = y[node.indices]\n            \n            node.n0 = np.sum(y_node == 0)\n            node.n1 = np.sum(y_node == 1)\n            node.value = 1 if node.n1 > node.n0 else 0\n            node.misclass_as_leaf = min(node.n0, node.n1)\n            node.gini = self._calculate_gini(y_node)\n\n            if node.gini == 0.0 or node.n_samples = 1:\n                node.is_leaf = True\n                return node\n            \n            split = self._find_split(X, y, node.indices)\n\n            if split is None:\n                node.is_leaf = True\n                return node\n\n            node.feature_idx = split['feature_idx']\n            node.threshold = split['threshold']\n            node.left = self._grow(X, y, split['left_indices'])\n            node.right = self._grow(X, y, split['right_indices'])\n            \n            return node\n\n        def fit(self, X, y):\n            Node._id_counter = 0\n            self.root = self._grow(X, y, np.arange(len(y)))\n\n        def _predict_single(self, x, node):\n            if node.is_leaf:\n                return node.value\n            if x[node.feature_idx] = node.threshold:\n                return self._predict_single(x, node.left)\n            return self._predict_single(x, node.right)\n\n        def predict(self, X):\n            return np.array([self._predict_single(x, self.root) for x in X])\n\n    def generate_data(n, feature_seed, noise_seed, eta):\n        rng_features = np.random.default_rng(feature_seed)\n        X = rng_features.uniform(0, 1, size=(n, 2))\n        \n        tau = 0.5 + 0.25 * np.sin(4 * np.pi * X[:, 1])\n        y_star = (X[:, 0] > tau).astype(int)\n        \n        rng_noise = np.random.default_rng(noise_seed)\n        flips = rng_noise.random(n)  eta\n        y = y_star.copy()\n        y[flips] = 1 - y[flips]\n        \n        return X, y\n\n    def get_misclassification_rate(y_true, y_pred):\n        return np.mean(y_true != y_pred)\n\n    def get_subtree_info(node):\n        if node.is_leaf:\n            return 1, node.misclass_as_leaf\n        \n        left_leaves, left_misclass = get_subtree_info(node.left)\n        right_leaves, right_misclass = get_subtree_info(node.right)\n        \n        return left_leaves + right_leaves, left_misclass + right_misclass\n\n    def get_leaf_count(node):\n        if node is None:\n            return 0\n        if node.is_leaf:\n            return 1\n        return get_leaf_count(node.left) + get_leaf_count(node.right)\n\n    def get_pruning_path(tree, X_train, y_train):\n        path = [(0.0, copy.deepcopy(tree))]\n        current_tree = tree\n\n        while True:\n            internal_nodes = []\n            q = deque([current_tree.root])\n            node_map = {current_tree.root.id: current_tree.root}\n            while q:\n                node = q.popleft()\n                if not node.is_leaf:\n                    internal_nodes.append(node)\n                    if node.left: q.append(node.left); node_map[node.left.id] = node.left\n                    if node.right: q.append(node.right); node_map[node.right.id] = node.right\n            \n            if not internal_nodes:\n                break\n            \n            g_values = []\n            for node in internal_nodes:\n                L_t, R_Tt = get_subtree_info(node)\n                R_t = node.misclass_as_leaf\n                if L_t > 1:\n                    g = (R_t - R_Tt) / (L_t - 1)\n                    g_values.append((g, node.id))\n            \n            if not g_values:\n                break\n            \n            min_g = min(g for g, _ in g_values)\n            \n            next_tree_obj = copy.deepcopy(current_tree)\n            \n            nodes_to_prune_ids = {nid for g, nid in g_values if np.isclose(g, min_g)}\n            \n            q = deque([next_tree_obj.root])\n            next_node_map = {next_tree_obj.root.id: next_tree_obj.root}\n            while q:\n                node = q.popleft()\n                if not node.is_leaf:\n                    if node.left: q.append(node.left); next_node_map[node.left.id] = node.left\n                    if node.right: q.append(node.right); next_node_map[node.right.id] = node.right\n\n            for node_id in nodes_to_prune_ids:\n                node_to_prune = next_node_map[node_id]\n                node_to_prune.is_leaf = True\n                node_to_prune.left = None\n                node_to_prune.right = None\n\n            current_tree = next_tree_obj\n            path.append((min_g, current_tree))\n        \n        return path\n\n    def select_tree_for_alpha(path, alpha):\n        best_tree = None\n        best_alpha_k = -1.0\n        for alpha_k, tree in path:\n            if alpha_k = alpha and alpha_k >= best_alpha_k:\n                best_alpha_k = alpha_k\n                best_tree = tree\n        return best_tree\n\n    # --- Main Execution Logic ---\n    \n    # Common parameters for all test cases\n    n_train = 120\n    n_test = 8000\n    eta = 0.25\n    train_seed = 1337\n    test_seed = 2027\n\n    X_train, y_train = generate_data(n_train, train_seed, train_seed + 1, eta)\n    X_test, y_test = generate_data(n_test, test_seed, test_seed + 1, eta)\n\n    full_tree = DecisionTree()\n    full_tree.fit(X_train, y_train)\n\n    y_pred_train_full = full_tree.predict(X_train)\n    r_train_full = get_misclassification_rate(y_train, y_pred_train_full)\n\n    y_pred_test_full = full_tree.predict(X_test)\n    r_test_full = get_misclassification_rate(y_test, y_pred_test_full)\n\n    pruning_path = get_pruning_path(full_tree, X_train, y_train)\n    \n    results = []\n\n    # Test Case 1: Optimal alpha by test error\n    best_test_err = float('inf')\n    best_tree_alpha1 = None\n    best_leaf_count = float('inf')\n    best_alpha_val = -1.0\n\n    for alpha_k, tree_k in pruning_path:\n        y_pred_test_k = tree_k.predict(X_test)\n        test_err_k = get_misclassification_rate(y_test, y_pred_test_k)\n        leaf_count_k = get_leaf_count(tree_k.root)\n\n        if test_err_k  best_test_err:\n            best_test_err = test_err_k\n            best_tree_alpha1 = tree_k\n            best_leaf_count = leaf_count_k\n            best_alpha_val = alpha_k\n        elif np.isclose(test_err_k, best_test_err):\n            if leaf_count_k  best_leaf_count:\n                best_tree_alpha1 = tree_k\n                best_leaf_count = leaf_count_k\n                best_alpha_val = alpha_k\n            elif leaf_count_k == best_leaf_count and alpha_k > best_alpha_val:\n                best_tree_alpha1 = tree_k\n                best_alpha_val = alpha_k\n    \n    T_alpha1 = best_tree_alpha1\n    r_test_alpha1 = best_test_err\n    r_train_alpha1 = get_misclassification_rate(y_train, T_alpha1.predict(X_train))\n    \n    b1 = (np.isclose(r_train_full, 0.0)) and (r_test_alpha1  r_test_full) and (r_train_alpha1 > r_train_full)\n    results.append(b1)\n    \n    # Test Case 2: alpha = 0\n    alpha2 = 0.0\n    T_alpha2 = select_tree_for_alpha(pruning_path, alpha2)\n    r_train_alpha2 = get_misclassification_rate(y_train, T_alpha2.predict(X_train))\n    r_test_alpha2 = get_misclassification_rate(y_test, T_alpha2.predict(X_test))\n    \n    b2 = np.isclose(r_train_alpha2, r_train_full) and np.isclose(r_test_alpha2, r_test_full)\n    results.append(b2)\n\n    # Test Case 3: alpha = 10.0\n    alpha3 = 10.0\n    T_alpha3 = select_tree_for_alpha(pruning_path, alpha3)\n    r_train_alpha3 = get_misclassification_rate(y_train, T_alpha3.predict(X_train))\n    num_leaves_alpha3 = get_leaf_count(T_alpha3.root)\n    \n    b3 = (num_leaves_alpha3 == 1) and (r_train_alpha3 > r_train_full)\n    results.append(b3)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们不仅需要知道如何控制模型复杂度，还必须掌握如何可靠地评估模型的泛化性能。使用训练误差（或称重代入误差）来选择模型是一个常见的陷阱，因为它会系统性地偏爱更复杂的模型。此练习  将通过对比 k-近邻 (k-NN) 分类器在 $k=1$ 时的零训练误差和其更真实的留一法交叉验证 (LOOCV) 误差，来凸显使用正确评估方法的重要性。它将让你深刻理解为什么必须使用独立于训练过程的数据或交叉验证等方法来评估模型的真实表现。",
            "id": "3135589",
            "problem": "给定一个分类问题，其训练数据集有限，每个观测值为一对 $(\\mathbf{x}_i, y_i)$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是特征向量，$y_i \\in \\{0,1\\}$ 是类别标签，$i = 1, \\dots, n$。基本要素包括：(i) 欧几里得距离的定义 $d(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{x} - \\mathbf{z}\\|_2$；(ii) k-近邻规则，即选择与查询点距离最小的 k 个训练观测值；(iii) 在 0-1 损失下定义的经验风险（重代入误差），即当使用指定分类器和完整训练集对每个训练点进行预测时，错分次数的平均值；以及 (iv) 留一法交叉验证，通过对每个训练点，使用移除了该点的训练集对其进行预测，并对产生的 0-1 损失求平均值来评估分类器。在所有计算中，距离的平局必须确定性地打破：当按距离对邻居进行排序时，首先按距离升序排列，然后如果查询点本身存在，则优先考虑该点，最后按索引升序打破任何剩余的平局。对于 $k \\ge 2$ 的分类中的多数投票，如果 k 个邻居中的类别计数出现平局，则选择最小的类别标签。\n\n任务 A：从上述定义出发，确认当每个训练点的邻居集合包含该点本身时，k=1 近邻分类器的经验风险（重代入误差）为 $0$。然后，计算 $k=1$ 的留一法误差，并将其与下面指定的数据集上 $k=2$ 的重代入误差进行比较。比较必须基于误差率的数值。\n\n任务 B：基于观察到的数值比较，分析使用重代入误差与留一法误差在 $k=1$ 和 $k=2$ 之间进行模型选择的影响，并根据所提供的定义进行讨论（你的分析应在解决方案中呈现，而不是由代码打印）。\n\n使用以下数据集测试套件，每个数据集由一个特征矩阵和一个标签向量给出。所有特征都是二维的，$d = 2$，所有类别标签都在 $\\{0,1\\}$ 中。距离为欧几里得距离。不涉及物理单位。\n\n- 数据集 1（类别分离良好）：\n  - 特征 $X_1$: $\\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  1.0 \\\\ 5.0  5.0 \\\\ 5.0  6.0 \\end{array}\\right]$\n  - 标签 $y_1$: $[0, 0, 1, 1]$\n- 数据集 2（边界附近有重叠）：\n  - 特征 $X_2$: $\\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  0.1 \\\\ 0.05  0.0 \\end{array}\\right]$\n  - 标签 $y_2$: $[0, 0, 1]$\n- 数据集 3（混合一个噪声点）：\n  - 特征 $X_3$: $\\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  2.0 \\\\ 2.0  0.0 \\\\ 2.0  2.0 \\\\ 1.1  1.0 \\end{array}\\right]$\n  - 标签 $y_3$: $[0, 0, 1, 1, 0]$\n\n对于每个数据集 $j \\in \\{1,2,3\\}$，计算：\n- $E_{j}^{\\text{train}, k=1}$：$k=1$ 时的经验风险（重代入误差），邻居中包括点本身，\n- $E_{j}^{\\text{LOO}, k=1}$：$k=1$ 时的留一法误差，不包括点本身，\n- $E_{j}^{\\text{train}, k=2}$：$k=2$ 时的经验风险（重代入误差），邻居中包括点本身，并使用多数投票和指定的平局打破规则。\n\n你的程序应产生单行输出，其中包含一个由三个列表组成的列表，每个数据集对应一个内部列表，格式为 $[E_{j}^{\\text{train}, k=1}, E_{j}^{\\text{LOO}, k=1}, E_{j}^{\\text{train}, k=2}]$。格式必须是方括号括起来的逗号分隔列表，例如：“[[e11,e12,e13],[e21,e22,e23],[e31,e32,e33]]”。所有值都必须是 $[0,1]$ 范围内的浮点数，表示在指定评估设置下训练点的平均 0-1 损失。",
            "solution": "对问题陈述的有效性进行评估。\n\n### 步骤 1：提取已知条件\n- **数据集**：一个包含 $n$ 个观测值的有限训练数据集，每个观测值为一对 $(\\mathbf{x}_i, y_i)$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是特征向量，$y_i \\in \\{0,1\\}$ 是类别标签，$i = 1, \\dots, n$。\n- **距离度量**：欧几里得距离，$d(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{x} - \\mathbf{z}\\|_2$。\n- **分类器规则**：$k$-近邻（$k$-NN）规则选择与查询点距离最小的 $k$ 个训练观测值。\n- **误差度量**：\n    - **经验风险（重代入误差）** 在 0-1 损失下是当使用在完整训练集上训练的分类器对每个训练点进行预测时，错分次数的平均值。\n    - **留一法交叉验证 (LOOCV) 误差** 是平均 0-1 损失，其中每个训练点都是使用在所有其他点组成的集合上训练的分类器进行预测的。\n- **平局打破规则**：\n    - **距离**：按距离升序排列邻居。如果距离相等，优先考虑点本身（如果存在）。任何剩余的平局按索引 $i$ 升序打破。\n    - **多数投票**：对于 $k \\ge 2$ 的分类，如果 $k$ 个邻居的类别计数出现平局，则选择最小的类别标签（即类别 $0$）。\n- **任务**：\n    - **任务 A**：确认 $k=1$ 的经验风险为 $0$。计算三个给定数据集上 $k=1$ 的 LOOCV 误差和 $k=2$ 的经验风险。\n    - **任务 B**：分析数值结果对模型选择的影响。\n- **数据集**：提供了三个数据集，每个数据集的 $d=2$ 且 $y_i \\in \\{0,1\\}$。\n    - 数据集 $1$：$X_1 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  1.0 \\\\ 5.0  5.0 \\\\ 5.0  6.0 \\end{array}\\right]$, $y_1 = [0, 0, 1, 1]$。\n    - 数据集 $2$：$X_2 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  0.1 \\\\ 0.05  0.0 \\end{array}\\right]$, $y_2 = [0, 0, 1]$。\n    - 数据集 $3$：$X_3 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  2.0 \\\\ 2.0  0.0 \\\\ 2.0  2.0 \\\\ 1.1  1.0 \\end{array}\\right]$, $y_3 = [0, 0, 1, 1, 0]$。\n- **所需计算**：对于每个数据集 $j \\in \\{1,2,3\\}$，计算 $E_{j}^{\\text{train}, k=1}$、$E_{j}^{\\text{LOO}, k=1}$ 和 $E_{j}^{\\text{train}, k=2}$。\n- **输出格式**：一个包含三个列表的列表：`[[e11,e12,e13],[e21,e22,e23],[e31,e32,e33]]`。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据**：该问题基于统计学习和模式识别中的基本和标准概念，即 $k$-NN 算法、欧几里得距离以及标准的误差估计方法（重代入和交叉验证）。所有定义都是正确和标准的。\n- **定义明确**：该问题定义明确。它提供了所有必要的数据、明确的算法和确定性的平局打破规则，确保了唯一解的存在且可计算。任务是具体且可量化的。\n- **客观性**：该问题以精确、客观和形式化的数学语言陈述，没有主观性或模糊性。\n\n该问题没有任何缺陷，如科学上的不严谨、不完整性、矛盾或模糊性。这是统计学习领域一个定义明确的计算和分析练习。\n\n### 步骤 3：结论与行动\n问题有效。将提供一个合理的解决方案。\n\n---\n\n### 任务 A：确认与计算\n\n**第 1 部分：确认 $k=1$ 时的经验风险为零**\n\n分类器的经验风险（或重代入误差）是通过将分类器应用于训练集中的每个点 $\\mathbf{x}_i$ 并根据已知标签 $y_i$ 衡量预测误差来计算的。对于 $k=1$ 近邻分类器，我们必须在完整训练集 $\\{(\\mathbf{x}_j, y_j)\\}_{j=1}^n$ 中找到距离 $\\mathbf{x}_i$ 最近的单个点。\n\n根据定义，一个点到其自身的欧几里得距离为 $d(\\mathbf{x}_i, \\mathbf{x}_i) = 0$。对于任何其他不同的点 $\\mathbf{x}_j$（其中 $j \\ne i$），距离 $d(\\mathbf{x}_i, \\mathbf{x}_j)$ 必定大于 $0$。因此，任何训练点 $\\mathbf{x}_i$ 的最近邻始终是点 $\\mathbf{x}_i$ 本身。问题的平局打破规则明确指出，如果该点存在，则优先考虑该点，这强化了这一结论。\n\n对 $\\mathbf{x}_i$ 标签的 $1$-NN 预测（表示为 $\\hat{y}_i$）因此是其最近邻的标签，即 $y_i$。该预测的 0-1 损失为 $L_{0-1}(\\hat{y}_i, y_i) = L_{0-1}(y_i, y_i) = 0$。由于对训练集中每个点的预测都是正确的，总错误数为 $0$。经验风险作为平均损失，因此为 $E^{\\text{train}, k=1} = \\frac{1}{n} \\sum_{i=1}^n 0 = 0$。这对所有训练点都唯一的所有数据集普遍适用。因此，对于所有数据集 $j \\in \\{1,2,3\\}$，$E_{j}^{\\text{train}, k=1} = 0$。\n\n**第 2 部分：数值计算**\n\n我们现在为每个数据集计算 $E_{j}^{\\text{LOO}, k=1}$ 和 $E_{j}^{\\text{train}, k=2}$。\n\n**数据集 1**：$X_1 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  1.0 \\\\ 5.0  5.0 \\\\ 5.0  6.0 \\end{array}\\right]$，$y_1 = [0, 0, 1, 1]$，$n=4$。\n- **$E_{1}^{\\text{LOO}, k=1}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$，$y_1=0$：在 $\\{\\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4\\}$ 中最近的点是 $\\mathbf{x}_2$（距离 $1.0$）。预测 $\\hat{y}_1=y_2=0$。正确。\n    - 对于 $\\mathbf{x}_2=(0.0, 1.0)$，$y_2=0$：在 $\\{\\mathbf{x}_1, \\mathbf{x}_3, \\mathbf{x}_4\\}$ 中最近的点是 $\\mathbf{x}_1$（距离 $1.0$）。预测 $\\hat{y}_2=y_1=0$。正确。\n    - 对于 $\\mathbf{x}_3=(5.0, 5.0)$，$y_3=1$：在 $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_4\\}$ 中最近的点是 $\\mathbf{x}_4$（距离 $1.0$）。预测 $\\hat{y}_3=y_4=1$。正确。\n    - 对于 $\\mathbf{x}_4=(5.0, 6.0)$，$y_4=1$：在 $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\}$ 中最近的点是 $\\mathbf{x}_3$（距离 $1.0$）。预测 $\\hat{y}_4=y_3=1$。正确。\n    总错误数：$0$。$E_{1}^{\\text{LOO}, k=1} = 0/4 = 0.0$。\n- **$E_{1}^{\\text{train}, k=2}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$，$y_1=0$：邻居是其自身和 $\\mathbf{x}_2$。标签为 $\\{0, 0\\}$。多数投票结果是 $0$。正确。\n    - 对于 $\\mathbf{x}_2=(0.0, 1.0)$，$y_2=0$：邻居是其自身和 $\\mathbf{x}_1$。标签为 $\\{0, 0\\}$。多数投票结果是 $0$。正确。\n    - 对于 $\\mathbf{x}_3=(5.0, 5.0)$，$y_3=1$：邻居是其自身和 $\\mathbf{x}_4$。标签为 $\\{1, 1\\}$。多数投票结果是 $1$。正确。\n    - 对于 $\\mathbf{x}_4=(5.0, 6.0)$，$y_4=1$：邻居是其自身和 $\\mathbf{x}_3$。标签为 $\\{1, 1\\}$。多数投票结果是 $1$。正确。\n    总错误数：$0$。$E_{1}^{\\text{train}, k=2} = 0/4 = 0.0$。\n\n数据集 1 的结果：$[0.0, 0.0, 0.0]$。\n\n**数据集 2**：$X_2 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  0.1 \\\\ 0.05  0.0 \\end{array}\\right]$，$y_2 = [0, 0, 1]$，$n=3$。\n- **$E_{2}^{\\text{LOO}, k=1}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$，$y_1=0$：到 $\\mathbf{x}_2, \\mathbf{x}_3$ 的距离分别为 $0.1, 0.05$。最近的是 $\\mathbf{x}_3$。预测 $\\hat{y}_1=y_3=1$。错误。\n    - 对于 $\\mathbf{x}_2=(0.0, 0.1)$，$y_2=0$：到 $\\mathbf{x}_1, \\mathbf{x}_3$ 的距离分别为 $0.1, \\sqrt{0.05^2+0.1^2} \\approx 0.112$。最近的是 $\\mathbf{x}_1$。预测 $\\hat{y}_2=y_1=0$。正确。\n    - 对于 $\\mathbf{x}_3=(0.05, 0.0)$，$y_3=1$：到 $\\mathbf{x}_1, \\mathbf{x}_2$ 的距离分别为 $0.05, \\sqrt{0.05^2+0.1^2} \\approx 0.112$。最近的是 $\\mathbf{x}_1$。预测 $\\hat{y}_3=y_1=0$。错误。\n    总错误数：$2$。$E_{2}^{\\text{LOO}, k=1} = 2/3$。\n- **$E_{2}^{\\text{train}, k=2}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$，$y_1=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_3$（标签 $1$）。平局。预测为 $0$。正确。\n    - 对于 $\\mathbf{x}_2=(0.0, 0.1)$，$y_2=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_1$（标签 $0$）。多数投票结果是 $0$。正确。\n    - 对于 $\\mathbf{x}_3=(0.05, 0.0)$，$y_3=1$：邻居是其自身（标签 $1$）和 $\\mathbf{x}_1$（标签 $0$）。平局。预测为 $0$。错误。\n    总错误数：$1$。$E_{2}^{\\text{train}, k=2} = 1/3$。\n\n数据集 2 的结果：$[0.0, 2/3, 1/3]$。\n\n**数据集 3**：$X_3 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  2.0 \\\\ 2.0  0.0 \\\\ 2.0  2.0 \\\\ 1.1  1.0 \\end{array}\\right]$，$y_3=[0, 0, 1, 1, 0]$，$n=5$。\n- **$E_{3}^{\\text{LOO}, k=1}$**：\n    - 对于 $\\mathbf{x}_1=(0,0), y_1=0$：最近的是 $\\mathbf{x}_5$（距离 $\\sqrt{1.1^2+1^2} \\approx 1.487$）。预测 $\\hat{y}_1=y_5=0$。正确。\n    - 对于 $\\mathbf{x}_2=(0,2), y_2=0$：最近的是 $\\mathbf{x}_5$（距离 $\\sqrt{1.1^2+(-1)^2} \\approx 1.487$）。预测 $\\hat{y}_2=y_5=0$。正确。\n    - 对于 $\\mathbf{x}_3=(2,0), y_3=1$：最近的是 $\\mathbf{x}_5$（距离 $\\sqrt{(-0.9)^2+1^2} \\approx 1.345$）。预测 $\\hat{y}_3=y_5=0$。错误。\n    - 对于 $\\mathbf{x}_4=(2,2), y_4=1$：最近的是 $\\mathbf{x}_5$（距离 $\\sqrt{(2-1.1)^2+(2-1)^2} = \\sqrt{1.81} \\approx 1.345$）。预测 $\\hat{y}_4=y_5=0$。错误。\n    - 对于 $\\mathbf{x}_5=(1.1,1), y_5=0$：最近的是 $\\mathbf{x}_3$（距离 $\\approx 1.345$）。预测 $\\hat{y}_5=y_3=1$。错误。\n    总错误数：$3$。$E_{3}^{\\text{LOO}, k=1} = 3/5 = 0.6$。\n- **$E_{3}^{\\text{train}, k=2}$**：\n    - 对于 $\\mathbf{x}_1=(0,0), y_1=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_5$（标签 $0$）。多数投票结果为 $0$。正确。\n    - 对于 $\\mathbf{x}_2=(0,2), y_2=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_5$（标签 $0$）。多数投票结果为 $0$。正确。\n    - 对于 $\\mathbf{x}_3=(2,0), y_3=1$：邻居是其自身（标签 $1$）和 $\\mathbf{x}_5$（标签 $0$）。平局。预测为 $0$。错误。\n    - 对于 $\\mathbf{x}_4=(2,2), y_4=1$：邻居是其自身（标签 $1$）和 $\\mathbf{x}_5$（标签 $0$）。平局。预测为 $0$。错误。\n    - 对于 $\\mathbf{x}_5=(1.1,1), y_5=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_3$（标签 $1$）。平局。预测为 $0$。正确。\n    总错误数：$2$。$E_{3}^{\\text{train}, k=2} = 2/5 = 0.4$。\n\n数据集 3 的结果：$[0.0, 0.6, 0.4]$。\n\n### 任务 B：对模型选择影响的分析\n\n计算出的误差率如下：\n- 数据集 1：$[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.0, 0.0]$\n- 数据集 2：$[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.667, 0.333]$\n- 数据集 3：$[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.6, 0.4]$\n\n这些结果揭示了误差估计方法的关键属性及其对模型选择（特别是选择超参数 $k$）的影响。\n\n1.  **重代入误差过于乐观，是一个糟糕的度量标准**：正如分析上确认和数值上显示的那样，$k=1$ 的重代入误差 $E^{\\text{train}, k=1}$ 始终为 $0$。如果实践者通过最小化重代入误差来选择 $k$，他们总是会选择 $k=1$，因为它能获得完美的分数。这一选择对应于一个因记住了训练数据而训练误差为零的模型。这种现象是过拟合的典型例子。该模型具有高方差，对包括噪声在内的每个数据点都非常敏感，但其训练误差对其在新未见数据上的表现给出了一个误导性的乐观评估。\n\n2.  **留一法交叉验证提供了更现实的评估**：留一法误差 $E^{\\text{LOO}, k=1}$ 提供了对泛化误差更诚实（偏差较小）的估计。对于类别完美分离的数据集 1，LOOCV 正确地识别出 $1$-NN 分类器表现完美（$E_{1}^{\\text{LOO}, k=1}=0.0$）。然而，对于分别具有类别重叠和噪声点的数据集 2 和 3，$k=1$ 的 LOOCV 误差相当大（$0.667$ 和 $0.6$）。这种高误差正确地表明 $k=1$ 模型不鲁棒且对训练数据过拟合。在噪声区域，一个点的单个最近邻很可能来自错误的类别，导致 LOOCV 报告高误差。\n\n3.  **比较模型复杂度**：选择 $k$ 涉及偏差-方差权衡。小的 $k$（如 $k=1$）导致一个低偏差、高方差的模型，其决策边界非常复杂。较大的 $k$ 会增加偏差但减少方差，从而产生一个更平滑、更不复杂的决策边界。在数据集 2 和 3 中，$k=1$ 在 LOOCV 下的严重性能下降表明其高方差导致了差的泛化能力。\n\n4.  **$E^{\\text{train}, k=2}$ 的解释**：对于有噪声的数据集，$k=2$ 的重代入误差是非零的。这是因为预测是两个点的平均值。例如，在数据集 3 中，当分类点 $\\mathbf{x}_3$（标签 $1$）时，它的邻居是其自身（标签 $1$）和噪声点 $\\mathbf{x}_5$（标签 $0$）。由此产生的平局通过选择标签 $0$ 来打破，即使在训练数据上也会导致错分。虽然 $E^{\\text{train}, k=2}$ 仍然是一个乐观的有偏估计，但它比空洞的 $E^{\\text{train}, k=1}=0$ 更有信息量。问题所要求的比较，$E^{\\text{LOO}, k=1}$ 与 $E^{\\text{train}, k=2}$ 的比较，在方法论上不是一个在 $k=1$ 和 $k=2$ 之间进行选择的合理方式，因为它比较的是一个模型的几乎无偏误差估计和另一个模型的有偏估计。一个恰当的比较应该是在 $E^{\\text{LOO}, k=1}$ 和 $E^{\\text{LOO}, k=2}$ 之间进行。尽管如此，结果（$0.667$ vs $0.333$ 和 $0.6$ vs $0.4$）说明了一个关键点：一个不稳定的模型（$k=1$）可能具有非常高的泛化误差，甚至可能高于一个更稳定模型（$k=2$）的（乐观的）训练误差。\n\n总之，对于 $k$-NN 中的模型选择，重代入误差是一个有缺陷的度量标准，特别是对于小的 $k$。它系统性地偏好过拟合的模型。像 LOOCV 这样的交叉验证方法提供了对模型泛化能力更可靠的估计，并且对于调整像 $k$ 这样的超参数至关重要。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes k-NN error rates for three datasets as specified in the problem.\n    \"\"\"\n    datasets = [\n        (\n            np.array([[0.0, 0.0], [0.0, 1.0], [5.0, 5.0], [5.0, 6.0]]),\n            np.array([0, 0, 1, 1])\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 0.1], [0.05, 0.0]]),\n            np.array([0, 0, 1])\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 2.0], [2.0, 0.0], [2.0, 2.0], [1.1, 1.0]]),\n            np.array([0, 0, 1, 1, 0])\n        )\n    ]\n\n    all_results = []\n\n    for X, y in datasets:\n        n, d = X.shape\n        \n        # Task 1: Empirical risk for k=1 (E_train, k=1)\n        # This is analytically guaranteed to be 0, as each point is its own nearest neighbor.\n        e_train_k1 = 0.0\n        \n        # Task 2: Leave-one-out error for k=1 (E_LOO, k=1)\n        loo_errors = 0\n        for i in range(n):\n            x_query = X[i]\n            y_true = y[i]\n            \n            min_dist = np.inf\n            best_idx = -1\n            \n            for j in range(n):\n                if i == j:\n                    continue\n                \n                dist = np.linalg.norm(x_query - X[j])\n                \n                if dist  min_dist:\n                    min_dist = dist\n                    best_idx = j\n                elif dist == min_dist:\n                    # Tie-breaking by ascending index\n                    if j  best_idx:\n                        best_idx = j\n            \n            y_pred = y[best_idx]\n            if y_pred != y_true:\n                loo_errors += 1\n        \n        e_loo_k1 = loo_errors / n\n\n        # Task 3: Empirical risk for k=2 (E_train, k=2)\n        train_k2_errors = 0\n        for i in range(n):\n            x_query = X[i]\n            y_true = y[i]\n            \n            # The first neighbor is the point itself.\n            neighbor1_label = y_true\n            \n            # Find the second neighbor (closest among other points).\n            min_dist = np.inf\n            best_idx = -1\n            for j in range(n):\n                if i == j:\n                    continue\n                \n                dist = np.linalg.norm(x_query - X[j])\n                if dist  min_dist:\n                    min_dist = dist\n                    best_idx = j\n                elif dist == min_dist:\n                    # Tie-breaking by ascending index\n                    if j  best_idx:\n                        best_idx = j\n            \n            neighbor2_label = y[best_idx]\n            \n            # Majority vote with tie-breaking\n            if neighbor1_label == neighbor2_label:\n                y_pred = neighbor1_label\n            else:  # Tie in votes (one for each class)\n                y_pred = 0  # Select smallest class label\n                \n            if y_pred != y_true:\n                train_k2_errors += 1\n        \n        e_train_k2 = train_k2_errors / n\n        \n        all_results.append([e_train_k1, e_loo_k1, e_train_k2])\n\n    # Format the final output string as specified.\n    outer_list_str = []\n    for res in all_results:\n        inner_list_str = f\"[{','.join(map(str, res))}]\"\n        outer_list_str.append(inner_list_str)\n    \n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n```"
        }
    ]
}