{
    "hands_on_practices": [
        {
            "introduction": "数据预处理的第一步常常是处理非数值类型的特征。对于分类变量，我们如何将其转化为模型可以理解的数值形式？这个选择并非无关紧要，它深刻地影响着模型的性能和可解释性。本练习将引导你深入辨析两种常用编码方式——序号编码（label encoding）和独热编码（one-hot encoding）——在不同类型模型（如线性模型与决策树）下的表现差异，从而掌握为特定场景选择最优编码策略的核心准则。",
            "id": "3112621",
            "problem": "一个数据集有一个单一的分类预测变量 $Z$，其取值于一个有限集合 $\\mathcal{C}=\\{c_1,\\dots,c_k\\}$（其中 $k \\ge 3$），以及一个标量响应 $Y \\in \\mathbb{R}$。考虑两类模型：(i) 一个线性模型 $f(\\mathbf{x})=\\mathbf{w}^\\top \\mathbf{x}+b$，通过最小二乘法或逻辑斯蒂损失进行估计；(ii) 一个标准的轴对齐决策树，它通过形如 $x_j \\le t$ 的阈值来分裂数值特征。对于 $Z$，考虑两种编码方案：一种是序数（标签）编码 $e_{\\mathrm{ord}}: \\mathcal{C}\\to \\{1,2,\\dots,k\\}$，为每个类别分配一个整数编码；另一种是独热编码 $e_{\\mathrm{oh}}:\\mathcal{C}\\to \\{0,1\\}^k$，将每个类别映射到一个标准基向量。除非另有明确说明，否则假定类别是名义的，并且决策树的实现没有原生的多路或分类子集分裂功能（只有数值阈值分裂）。选择所有关于这些编码对这两类模型的影响的正确陈述。\n\nA. 对于带截距项的线性模型，对名义变量 $Z$ 使用序数编码会隐式地施加一种从类别 $c_j$ 移动到 $c_{j+1}$ 时的单调且等间距的影响，这可能会引入虚假的线性趋势，除非这些类别是真正有序的。\n\nB. 对于带数值阈值分裂的决策树，使用序数编码允许树通过最多一次分裂来隔离任意的类别子集，因此与独热编码相比，它不会限制模型的表达能力。\n\nC. 对于带截距项的线性模型，使用包含所有 $k$ 个哑变量列的独热编码会因完全多重共线性而导致设计矩阵秩亏；将一个类别作为参考类别并丢弃其对应的列可以避免此问题，并使得系数可解释为与参考类别的偏差。\n\nD. 对于没有原生分类子集分裂功能的轴对齐决策树，独热编码通常允许使用更浅、更简单的分裂来隔离特定类别（例如，对一个指示变量进行单次分裂），而序数编码可能需要多次阈值分裂来分离不连续的类别组；因此，对于名义变量 $Z$，在这类树中通常首选独热编码。\n\nE. 对于不带截距项的线性模型，如果数据中每个类别至少出现一次，则包含所有 $k$ 个独热哑变量列是可识别的（没有完全多重共线性），这允许模型为每个类别拟合一个单独的均值。\n\n选择所有适用项。",
            "solution": "问题陈述已经过验证并且是合理的。这是一个在统计学习领域内定义明确的问题，使用了标准的术语，并提出了一组可验证的论断。我们现在将对每个陈述进行详细分析。\n\n问题设定涉及一个分类预测变量 $Z$，它有 $k \\ge 3$ 个名义类别，来自集合 $\\mathcal{C}=\\{c_1,\\dots,c_k\\}$，以及一个实值响应 $Y$。我们比较两种对 $Z$ 的编码方案——序数编码 ($e_{\\mathrm{ord}}$) 和独热编码 ($e_{\\mathrm{oh}}$)——对两类模型（线性模型和决策树）的影响。\n\n**陈述 A 的分析**\n\n该陈述声称，对于带截距项的线性模型，对名义预测变量 $Z$ 使用序数编码会施加一种单调、等间距的影响。\n\n设序数编码为 $e_{\\mathrm{ord}}(c_j) = j$，其中 $j \\in \\{1, 2, \\dots, k\\}$。分类预测变量 $Z$ 被转换为单个数值特征，我们称之为 $x_Z$。带截距项的线性模型由下式给出：\n$$ E[Y|Z=c_j] = f(x_Z=j) = w \\cdot j + b $$\n其中 $w$ 是特征 $x_Z$ 的权重， $b$ 是截距项。\n\n我们来考察从任意类别 $c_j$ 移动到所施加序列中的下一个类别 $c_{j+1}$ 时，预测响应的变化：\n$$ E[Y|Z=c_{j+1}] - E[Y|Z=c_j] = (w \\cdot (j+1) + b) - (w \\cdot j + b) = w $$\n这个差值是常数，等于权重 $w$，与 $j$ 的具体值无关。这表明了两个属性：\n$1$. **单调性**：如果 $w > 0$，预测响应随着分配给类别的整数编码严格增加。如果 $w  0$，它严格减少。如果 $w = 0$，它保持不变。在任何情况下，关系相对于人为设定的顺序是单调的。\n$2$. **等间距效应**：在任意排序中，任何两个相邻类别之间的响应变化是相同的 ($w$)。这意味着模型被迫假定从 $c_1$ 变为 $c_2$ 的影响与从 $c_2$ 变为 $c_3$ 的影响是相同的，依此类推。\n\n对于一个名义变量（例如，“巴黎”、“东京”、“伦敦”），没有内在的顺序，更不用说等间距的顺序了。强加一个顺序（例如，巴黎=1，东京=2，伦敦=3）会迫使模型学习到一个虚假的线性趋势，这是编码的人为结果，而不是数据的真实属性。仅当类别是真正有序的，并且它们之间的步长可以被认为是相等时，这种做法才适用。因此，该陈述是正确的。\n\n**结论：正确**\n\n**陈述 B 的分析**\n\n该陈述声称，对于带数值阈值分裂的决策树，序数编码可以通过最多一次分裂来隔离任意的类别子集。\n\n带数值阈值分裂的决策树使用形如 $x \\le t$ 的规则来划分特征空间。当使用序数编码时，我们只有一个数值特征 $x_Z$，取整数值 $\\{1, 2, \\dots, k\\}$。对此特征的分裂形式为 $x_Z \\le t$。对于任何阈值 $t$，此分裂都将类别集合 $\\mathcal{C}$ 根据分配的整数编码分成两个连续的子集。对于阈值 $t$，其中 $\\lfloor t \\rfloor = j$ 且 $j \\in \\{1, \\dots, k-1\\}$，该分裂将数据划分为两组：一组是 $Z \\in \\{c_1, \\dots, c_j\\}$，另一组是 $Z \\in \\{c_{j+1}, \\dots, c_k\\}$。\n\n该陈述声称*任何任意子集*都可以被隔离。我们用一个反例来检验这一点。设 $k = 4$，类别为 $\\{c_1, c_2, c_3, c_4\\}$，编码为 $\\{1, 2, 3, 4\\}$。考虑任意子集 $\\{c_1, c_3\\}$。要隔离这个子集，我们需要将 $Z \\in \\{c_1, c_3\\}$ 的数据点与 $Z \\in \\{c_2, c_4\\}$ 的数据点分开。没有单一的形如 $x_Z \\le t$ 的分裂可以实现这一点。例如，分裂 $x_Z \\le 2.5$ 将 $\\{c_1, c_2\\}$ 与 $\\{c_3, c_4\\}$ 分开。它未能将 $c_1$ 与 $c_3$ 分组在一起。\n\n因此，对于名义变量，序数编码严重限制了决策树的表达能力，因为它无法在单次分裂中创建基于不连续类别组的划分。该陈述是错误的。\n\n**结论：不正确**\n\n**陈述 C 的分析**\n\n该陈述声称，对于带截距项的线性模型，使用所有 $k$ 个独热编码列会导致设计矩阵秩亏，而丢弃一列可以解决此问题。\n\n设 $Z$ 的独热编码产生 $k$ 个二元特征 $x_1, \\dots, x_k$，其中如果 $Z=c_j$ 则 $x_j=1$，否则 $x_j=0$。对于任何观测值，这些特征中恰好有一个是 $1$，这意味着对于每个数据点，以下关系成立：\n$$ \\sum_{j=1}^k x_j = 1 $$\n带截距项的线性模型指定为：\n$$ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + \\epsilon $$\n设计矩阵 $\\mathbf{X}$ 包含对应于截距项（一个全为1的列，记作 $\\mathbf{1}$）和 $k$ 个哑变量的列 $\\mathbf{x}_1, \\dots, \\mathbf{x}_k$。对于所有观测值，关系 $\\sum_{j=1}^k x_j = 1$ 意味着哑变量列的总和等于截距项列：\n$$ \\sum_{j=1}^k \\mathbf{x}_j = \\mathbf{1} $$\n这构成了设计矩阵 $\\mathbf{X}$ 各列之间的完全线性相关性。列向量集合 $\\{\\mathbf{1}, \\mathbf{x}_1, \\dots, \\mathbf{x}_k\\}$ 是线性相关的，意味着矩阵 $\\mathbf{X}$ 不是满秩的（它是秩亏的）。这种情况被称为完全多重共线性，它使得普通最小二乘估计量 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{Y}$ 不是唯一的，因为矩阵 $\\mathbf{X}^\\top\\mathbf{X}$ 是奇异的（不可逆的）。\n\n为了解决这个问题，需要丢弃一个哑变量列。假设我们丢弃对应于类别 $c_k$ 的列 $\\mathbf{x}_k$。模型变为：\n$$ Y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_{k-1} x_{k-1} + \\epsilon $$\n在这种形式中，$c_k$ 是*参考类别*。\n- 对于参考类别 $c_k$ 中的一个观测值，所有的 $x_j$（$j=1, \\dots, k-1$）都为 $0$，所以期望响应为 $E[Y|Z=c_k] = \\beta_0$。截距项 $\\beta_0$ 是参考类别的平均响应。\n- 对于另一个类别 $c_j$（其中 $j  k$）中的一个观测值，$x_j=1$ 且所有其他的 $x_i=0$。期望响应为 $E[Y|Z=c_j] = \\beta_0 + \\beta_j$。\n因此，系数 $\\beta_j$ 表示类别 $c_j$ 和参考类别 $c_k$ 之间的平均响应差异：$\\beta_j = E[Y|Z=c_j] - E[Y|Z=c_k]$。将其解释为“与参考类别的偏差”是正确的。\n\n**结论：正确**\n\n**陈述 D 的分析**\n\n该陈述比较了用于带数值阈值分裂的决策树的独热编码和序数编码，并得出结论：对于名义变量，独热编码更可取。\n\n如陈述 B 的分析所确立的，序数编码强加了人为的顺序，并且只允许将类别划分为两个连续块的分裂。要分离一个不连续的组（例如，$\\{c_1, c_4\\}$ 与 $\\{c_2, c_3\\}$），需要多次嵌套分裂，这可能导致树更深、更复杂，并可能过拟合。例如，为了隔离类别 $c_j$（其中 $1  j  k$），需要两次分裂：一次是形如 $x_Z > j-1$ 的分裂，另一次是形如 $x_Z \\le j$ 的分裂。\n\n使用独热编码，我们有 $k$ 个独立的二元特征 $x_1, \\dots, x_k$。\n- 要隔离单个类别 $c_j$，树可以在其对应的指示变量上使用单次分裂：$x_j > 0.5$（或 $x_j \\le 0.5$）。这在一个步骤中就将类别 $c_j$ 的所有观测值与所有其他观测值完美地分开了。这是一个非常高效和“简单”的分裂。\n- 树可以学习逼近类别的任何划分。例如，如果类别 $c_1$ 和 $c_3$ 对响应 $Y$ 有相似的影响，树可以学习将它们分组。它不受任何预定义邻接关系的约束。当预测变量是名义变量时，这种灵活性至关重要。\n\n因为独热编码不给类别强加任何虚假结构，并允许树根据数据找到最优分组，所以与序数编码的限制性相比，它通常能为名义变量带来表达能力更强且通常更简单（更浅）的模型。该陈述的推理和结论是合理的。\n\n**结论：正确**\n\n**陈述 E 的分析**\n\n该陈述声称，对于*不带*截距项的线性模型，使用所有 $k$ 个独热编码列是可识别的。\n\n不带截距项的模型是：\n$$ Y = \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + \\epsilon $$\n设计矩阵 $\\mathbf{X}$ 现在只包含 $k$ 个哑变量列 $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_k\\}$。截距项列（全为1的向量）不属于设计矩阵的一部分。\n\n为了检查可识别性（即没有完全多重共线性），我们必须检查 $\\mathbf{X}$ 的列是否线性无关。我们检验方程是否存在非平凡解：\n$$ a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 + \\dots + a_k \\mathbf{x}_k = \\mathbf{0} $$\n让我们考虑这个向量方程的任意第 $i$ 行，它对应于一个数据点。假设这个数据点属于类别 $c_j$。根据独热编码的定义，对于这一行，第 $j$ 个特征的值是 $x_{ij} = 1$，而所有其他特征值 $x_{il} = 0$（对于 $l \\neq j$）。因此，该行的方程变为：\n$$ a_1 \\cdot 0 + \\dots + a_j \\cdot 1 + \\dots + a_k \\cdot 0 = 0 \\implies a_j = 0 $$\n问题陈述指出，每个类别在数据中至少出现一次。这保证了对于每个 $j \\in \\{1,\\dots, k\\}$，至少有一行的 $x_j=1$。因此，我们可以对每个类别 $c_1, \\dots, c_k$ 应用此逻辑，并得出结论 $a_1=0, a_2=0, \\dots, a_k=0$。由于唯一的解是平凡解，所以列 $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_k\\}$ 是线性无关的。模型是可识别的。\n\n在这个模型中，对于类别 $c_j$ 中的一个观测值，预测响应为 $E[Y|Z=c_j] = \\beta_j$，因为 $x_j=1$ 且所有其他的 $x_l=0$。这意味着每个系数 $\\beta_j$ 直接建模了类别 $c_j$ 的平均响应。这是一种有效且常见的建模选择，被称为“效应编码”或拟合“单元格均值模型”。\n\n**结论：正确**",
            "answer": "$$\\boxed{ACDE}$$"
        },
        {
            "introduction": "线性回归模型的一个关键假设是误差项的方差恒定，即同方差性。然而，在许多现实世界的数据中，我们常常遇到异方差问题，即误差的方差随预测变量的增大而变化，尤其是在存在乘性噪声（$Y = \\beta X \\epsilon$）的情况下。本练习将通过一个动手编码的例子，向你展示如何通过对数变换（$\\log(Y) = \\log(\\beta) + \\log(X) + \\log(\\epsilon)$）来“驯服”这种噪声，将非线性、异方差的关系转化为满足线性模型假设的理想形式，并用代码量化其带来的改进。",
            "id": "3112629",
            "problem": "你的任务是根据一个乘性噪声模型构建数据集，并评估对数变换如何影响线性建模的性能。背景设定为包含数据预处理概念的统计学习。你可以使用的基本原理包括对数的性质，即对于正实数 $a$ 和 $b$，$\\log(ab)=\\log a+\\log b$，以及概率论中的标准定义，包括独立性下的期望和对数正态分布的成熟公式。\n\n考虑一对独立的正常数随机变量 $(X,\\varepsilon)$ 和一个正常数 $\\beta$，其生成关系为 $Y=\\beta X \\varepsilon$。假设 $\\varepsilon$ 服从对数正态分布，其中 $\\log \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$，因此其均值 $\\mathbb{E}[\\varepsilon]$ 等于 $\\exp(\\sigma^2/2)$。目标是比较对数变换前后的普通最小二乘法拟合，并评估该变换是否通过线性化乘性噪声的影响来提高拟合质量。\n\n对于每个测试用例，你的程序必须执行以下步骤：\n\n- 使用为 $X$ 指定的分布以及为 $\\beta$ 和 $\\sigma$ 指定的参数，生成一个大小为 $n$ 的数据集。\n- 根据测试用例给定的参数 $\\mu=0$ 和 $\\sigma$，从对数正态分布中独立抽取 $\\varepsilon$ 来构造 $Y=\\beta X \\varepsilon$。\n- 对未变换的变量 $Y$ 对 $X$ 拟合一个带截距的普通最小二乘 (OLS) 线性模型，即通过最小化残差平方和来近似 $Y \\approx a+bX$，并计算估计的斜率 $b_{\\text{pre}}$ 和决定系数 $R^2_{\\text{pre}}$。\n- 对对数变换后的变量 $\\log Y$ 对 $\\log X$ 拟合一个带截距的普通最小二乘线性模型，即近似 $\\log Y \\approx a_{\\log}+b_{\\log}\\log X$，并计算估计的斜率 $b_{\\text{post}}$ 和决定系数 $R^2_{\\text{post}}$。\n- 计算原始尺度下相对于总体条件均值线性斜率的绝对斜率误差 $\\left|b_{\\text{pre}} - \\beta \\exp(\\sigma^2/2)\\right|$，以及对数变换尺度下相对于预期线性关系的绝对斜率误差 $\\left|b_{\\text{post}} - 1\\right|$。\n- 报告对数变换后 $R^2$ 是否改善（布尔值 $R^2_{\\text{post}} > R^2_{\\text{pre}}$），以及两个斜率误差。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果必须是 $[R2\\_improves,err\\_pre,err\\_post]$ 形式的列表，其中 $R2\\_improves$ 是一个布尔值，而 $err\\_pre$ 和 $err\\_post$ 是四舍五入到六位小数的浮点数。最终打印的输出应类似于 $[[\\text{bool},\\text{float},\\text{float}],\\ldots]$，每个测试用例占一项，并按下面指定的顺序排列。\n\n使用以下测试套件。为保证可复现性，请为每个案例使用指定的独立伪随机数生成器种子。\n\n- 测试用例 1：$\\beta=2.0$, $n=200$, $\\sigma=0.3$, $X \\sim \\text{Uniform}[1,10]$, 种子 $42$。\n- 测试用例 2：$\\beta=1.5$, $n=200$, $\\sigma=0.0$, $X \\sim \\text{Uniform}[1,10]$, 种子 $123$。\n- 测试用例 3：$\\beta=1.0$, $n=1000$, $\\sigma=1.0$, $X \\sim \\text{Exponential}(\\text{scale}=3)$, 种子 $7$。\n- 测试用例 4：$\\beta=0.5$, $n=50$, $\\sigma=0.8$, $X \\sim \\text{Lognormal}(\\mu=-0.2,\\sigma=0.5)$, 种子 $2025$。\n\n角度单位不适用，且不涉及任何物理单位。不得使用百分比；所有量都应以小数表示。\n\n你的程序必须遵守指定的输出格式，并且不得读取输入或写入文件。",
            "solution": "问题陈述已经过验证，被认为是合理的。它在科学上基于统计学习原理，问题设定良好，目标明确，数据充分，并且没有歧义或矛盾。\n\n这个问题的核心在于理解和缓解线性回归背景下的异方差性——即误差的方差不恒定。我们给定了一个乘性噪声模型，这是此类问题的常见来源。\n\n**理论框架**\n\n数据的生成模型由关系式 $Y = \\beta X \\varepsilon$ 给出，其中 $Y$ 和 $X$ 是可观测变量，$\\beta$ 是一个正常数缩放因子，$\\varepsilon$ 是一个乘性噪声项。变量 $X$ 和 $\\varepsilon$ 是独立且为正的。噪声项 $\\varepsilon$ 被指定为服从对数正态分布，其自然对数 $\\log \\varepsilon$ 服从均值为 $0$、方差为 $\\sigma^2$ 的正态分布，记为 $\\log \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n**未变换模型的分析：$Y$ 对 $X$**\n\n首先，我们考虑对未变换的变量进行直接线性回归。$Y$ 和 $X$ 之间的理论关系由条件期望 $\\mathbb{E}[Y | X=x]$ 描述。由于 $X$ 和 $\\varepsilon$ 的独立性：\n$$\n\\mathbb{E}[Y | X=x] = \\mathbb{E}[\\beta x \\varepsilon | X=x] = \\beta x \\mathbb{E}[\\varepsilon]\n$$\n对于一个对数正态随机变量 $\\varepsilon$，其中 $\\log \\varepsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)$，其期望为 $\\mathbb{E}[\\varepsilon] = \\exp(\\mu + \\sigma^2/2)$。在我们的情况下，$\\mu=0$，所以 $\\mathbb{E}[\\varepsilon] = \\exp(\\sigma^2/2)$。\n因此，条件期望为：\n$$\n\\mathbb{E}[Y | X=x] = (\\beta e^{\\sigma^2/2}) x\n$$\n这个关系在 $x$ 上是线性的，斜率为 $\\beta e^{\\sigma^2/2}$。对 $Y$ 关于 $X$ 的普通最小二乘 (OLS) 拟合试图估计这个潜在的线性趋势。因此，用于比较的目标斜率是 $\\beta e^{\\sigma^2/2}$。\n\n然而，OLS 估计量成为最佳线性无偏估计量 (BLUE) 的一个关键假设是同方差性，即误差项的方差是恒定的。我们来考察一下条件方差：\n$$\n\\text{Var}(Y | X=x) = \\text{Var}(\\beta x \\varepsilon) = (\\beta x)^2 \\text{Var}(\\varepsilon)\n$$\n由于 $\\text{Var}(\\varepsilon)$ 是一个正常数（对于 $\\sigma > 0$），$Y$ 的方差与 $x^2$ 成正比。这是一个经典的异方差案例：观测值的方差随着预测变量 $X$ 的量级增加而增加。违反这一假设可能导致 OLS 估计量效率低下和假设检验结果不可靠。\n\n**对数变换模型的分析：$\\log Y$ 对 $\\log X$**\n\n为了解决乘性噪声和异方差性问题，我们提出对数变换。对生成模型应用自然对数，得到：\n$$\n\\log Y = \\log(\\beta X \\varepsilon) = \\log \\beta + \\log X + \\log \\varepsilon\n$$\n让我们定义一组新变量：$Y' = \\log Y$，$X' = \\log X$，以及一个加性误差项 $\\epsilon' = \\log \\varepsilon$。模型变为：\n$$\nY' = (\\log \\beta) + 1 \\cdot X' + \\epsilon'\n$$\n根据定义，误差项 $\\epsilon'$ 服从正态分布，$\\epsilon' \\sim \\mathcal{N}(0, \\sigma^2)$。在这个变换后的空间中，$Y'$ 和 $X'$ 之间的关系是完全线性的，真实斜率为 $1$，截距为 $\\log \\beta$。误差项 $\\epsilon'$ 现在是加性的，其方差 $\\sigma^2$ 对于所有 $X'$ 的值都是恒定的。这个新模型满足了 OLS 的关键假设（线性、误差正态性和同方差性）。因此，我们期望在对数变换后的数据上进行 OLS 拟合会更加稳健，并能更好地表征潜在关系。在这个变换尺度下，用于比较的目标斜率为 $1$。\n\n**计算方法**\n\n程序通过模拟来经验性地验证这些理论见解。对于每个指定的测试用例：\n$1$。用特定的种子初始化一个伪随机数生成器，以确保可复现性。\n$2$。生成一个大小为 $n$ 的数据集。预测变量 $X$ 从其指定分布中抽取。乘性噪声项 $\\varepsilon$ 从参数为 $\\mu=0$ 和给定 $\\sigma$ 的对数正态分布中抽取。然后构造响应变量 $Y$ 为 $Y = \\beta X \\varepsilon$。\n$3$。**变换前拟合**：将一个形式为 $Y \\approx a_{\\text{pre}} + b_{\\text{pre}}X$ 的 OLS 模型拟合到数据 $(X, Y)$。计算斜率估计值 $b_{\\text{pre}}$ 和决定系数 $R^2_{\\text{pre}}$。绝对斜率误差计算为 $|b_{\\text{pre}} - \\beta e^{\\sigma^2/2}|$。\n$4$。**变换后拟合**：将变量变换为 $\\log X$ 和 $\\log Y$。将一个形式为 $\\log Y \\approx a_{\\text{post}} + b_{\\text{post}}\\log X$ 的 OLS 模型拟合到数据 $(\\log X, \\log Y)$。计算斜率估计值 $b_{\\text{post}}$ 和决定系数 $R^2_{\\text{post}}$。绝对斜率误差计算为 $|b_{\\text{post}} - 1|$。\n$5$。决定系数 $R^2$ 计算为预测变量和响应变量之间皮尔逊相关系数的平方，对于简单线性回归，这等同于 $1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}$。\n$6$。最终的指标被组合成一个列表：$[R^2_{\\text{post}} > R^2_{\\text{pre}}, |b_{\\text{pre}} - \\beta e^{\\sigma^2/2}|, |b_{\\text{post}} - 1|]$。布尔值表示对数变换是否在解释方差方面改善了模型拟合。两个误差项量化了在各自域中斜率估计的准确性。",
            "answer": "```python\nimport numpy as np\n\ndef perform_analysis(beta, n, sigma, x_dist_params, seed):\n    \"\"\"\n    Generates a dataset under a multiplicative noise model and compares\n    OLS fits before and after a logarithmic transformation.\n\n    Args:\n        beta (float): The scaling factor in the model Y = beta * X * epsilon.\n        n (int): The number of data points to generate.\n        sigma (float): The scale parameter (std dev) of the log-normal noise.\n                       log(epsilon) ~ N(0, sigma^2).\n        x_dist_params (tuple): A tuple describing the distribution of X.\n                               e.g., ('uniform', low, high)\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        list: A list containing [R2_improves, err_pre, err_post].\n    \"\"\"\n    # 1. Initialize RNG and generate data\n    rng = np.random.default_rng(seed)\n\n    dist_type = x_dist_params[0]\n    if dist_type == 'uniform':\n        # Generate X from Uniform(low, high)\n        X = rng.uniform(low=x_dist_params[1], high=x_dist_params[2], size=n)\n    elif dist_type == 'exponential':\n        # Generate X from Exponential(scale)\n        X = rng.exponential(scale=x_dist_params[1], size=n)\n    elif dist_type == 'lognormal':\n        # Generate X from Lognormal(mu, sigma)\n        X = rng.lognormal(mean=x_dist_params[1], sigma=x_dist_params[2], size=n)\n    else:\n        # This case should not be reached with the given problem\n        raise ValueError(\"Unknown distribution for X\")\n\n    # Generate multiplicative noise epsilon from Lognormal(0, sigma)\n    # This means log(epsilon) is N(0, sigma^2)\n    epsilon = rng.lognormal(mean=0.0, sigma=sigma, size=n)\n\n    # Construct Y\n    Y = beta * X * epsilon\n    \n    # 2. Pre-transformation OLS fit (Y vs X)\n    # Using np.polyfit to get slope (b_pre) and intercept\n    b_pre, _ = np.polyfit(X, Y, 1)\n\n    # R-squared is the square of correlation for simple linear regression\n    # Handle cases where variance is zero to avoid NaN\n    if np.var(X) > 1e-12 and np.var(Y) > 1e-12:\n        R2_pre = np.corrcoef(X, Y)[0, 1]**2\n    else:\n        R2_pre = 0.0\n\n    # 3. Log-transform data\n    log_X = np.log(X)\n    log_Y = np.log(Y)\n\n    # 4. Post-transformation OLS fit (log Y vs log X)\n    b_post, _ = np.polyfit(log_X, log_Y, 1)\n\n    if np.var(log_X) > 1e-12 and np.var(log_Y) > 1e-12:\n        R2_post = np.corrcoef(log_X, log_Y)[0, 1]**2\n    else:\n        R2_post = 0.0\n\n    # 5. Compute comparison metrics\n    R2_improves = R2_post > R2_pre\n\n    # Absolute slope error for the pre-transformation model\n    target_slope_pre = beta * np.exp(sigma**2 / 2.0)\n    err_pre = np.abs(b_pre - target_slope_pre)\n\n    # Absolute slope error for the post-transformation model\n    target_slope_post = 1.0\n    err_post = np.abs(b_post - target_slope_post)\n\n    return [R2_improves, round(err_pre, 6), round(err_post, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {'beta': 2.0, 'n': 200, 'sigma': 0.3, 'x_dist_params': ('uniform', 1.0, 10.0), 'seed': 42},\n        {'beta': 1.5, 'n': 200, 'sigma': 0.0, 'x_dist_params': ('uniform', 1.0, 10.0), 'seed': 123},\n        {'beta': 1.0, 'n': 1000, 'sigma': 1.0, 'x_dist_params': ('exponential', 3.0), 'seed': 7},\n        {'beta': 0.5, 'n': 50, 'sigma': 0.8, 'x_dist_params': ('lognormal', -0.2, 0.5), 'seed': 2025}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = perform_analysis(**case)\n        results.append(result)\n\n    # Format the output string exactly as required\n    formatted_results = []\n    for res in results:\n        # res[0] is boolean, res[1] and res[2] are floats\n        formatted_results.append(f\"[{str(res[0]).lower()},{res[1]:.6f},{res[2]:.6f}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在处理时间序列数据时，一个最隐蔽也最致命的陷阱是“数据泄漏”——在模型训练中无意中使用了来自未来的信息。这种错误会导致模型在评估中表现出虚假的优异性能，但在真实世界的预测中却一败涂地。本练习将通过编程实践，让你亲手构建一个数据泄漏的场景，直观地看到其危害，并学习如何设计和实施严格遵守时间顺序的交叉验证方案来根除这一问题。",
            "id": "3112690",
            "problem": "你的任务是构建一个确定性程序，用以演示和量化在时间序列预处理中错误地包含了一个源于未来的特征时所产生的数据泄露。然后，设计一个时间感知交叉验证过程，该过程通过在每个折内重新计算特征且不使用未来信息来消除这种泄露。目标是从第一性原理出发，推理在存在有序数据的情况下，时间依赖性、平稳性以及交叉验证的有效性。\n\n使用以下基础和定义：单变量时间序列是由离散时间 $t$ 索引的序列 $\\{y_t\\}_{t=0}^{T}$。一阶自回归过程 (AR(1)) 由 $y_{t+1} = a \\, y_t + \\varepsilon_{t+1}$ 给出，其中 $\\varepsilon_{t}$ 是均值为零、方差为 $\\sigma^2$ 的噪声，且 $|a|  1$ 确保了弱平稳性。普通最小二乘 (OLS) 回归通过最小化均方误差 (MSE) 来估计系数 $\\hat{\\beta}$，MSE 由 $\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$ 给出。随机 K 折交叉验证（cross-validation (CV)）假设样本是独立同分布的，并将索引随机分成 $K$ 个不相交的折；在时间序列中，由于时间依赖性和因果关系约束，这一假设被违反。用于时间序列的时间感知 CV 使用遵循时间顺序的前向链（扩展窗口）划分：训练仅使用索引 $\\leq t_{\\text{train}}$ 的数据，而验证使用区间 $(t_{\\text{train}}, t_{\\text{val}}]$ 内的索引。\n\n按如下方式构建监督学习目标 $y_{t+1}$ 和在时间 $t$ 的特征。无泄露的、仅使用过去信息的特征是 $f_{1,t} = y_t$ 和 $f_{2,t} = \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t-j}$（一个仅使用当前值和 $W-1$ 个过去值的后向移动平均）。一个有泄露的、源于未来的特征是 $g_{2,t} = \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t+1-j}$，它使用了 $y_{t+1}$；将其与 $g_{1,t} = y_t$ 配对，当 $W \\ge 1$ 时，会得到一个有泄露的特征集，该特征集将目标直接嵌入到输入中，从而违反了训练时的信息约束。\n\n你的程序必须：\n\n1.  完全按照以下参数模拟一个 AR(1) 过程用于主要实验：长度 $N = 1200$，自回归系数 $a = 0.8$，创新标准差 $\\sigma = 1.0$，以及初始条件 $y_0 = 0$。使用固定的随机数生成器种子以确保确定性。\n\n2.  为索引为 $t \\in \\{W-1, W, \\dots, N-2\\}$ 的行构建两个特征集，使得目标为 $y_{t+1}$：\n   - 有泄露的特征：$x_t^{\\text{leak}} = \\big(y_t, \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t+1-j}, 1\\big)$，其中最后一个分量是截距项。\n   - 无泄露的特征：$x_t^{\\text{past}} = \\big(y_t, \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t-j}, 1\\big)$，其中最后一个分量是截距项。\n\n3.  实现两种 CV 方案：\n   - 随机 $K$ 折 CV，其中 $K = 6$，在划分前使用固定种子和对行索引的均匀随机混洗。对于此 CV，使用全局预计算的有泄露特征 $x_t^{\\text{leak}}$。\n   - 时间感知的前向链 CV，包含 $K = 5$ 个折，初始训练集大小 $n_{\\text{train},0} = 500$，验证块大小 $n_{\\text{val}} = 100$（第 $k$ 折使用截至索引 $n_{\\text{train},0} + (k-1) n_{\\text{val}} - 1$ 的训练行，并在接下来的 $n_{\\text{val}}$ 行上进行验证）。你必须实现两个变体：\n     (i) 使用全局预计算的有泄露特征 $x_t^{\\text{leak}}$，以证明如果特征是使用未来信息预计算的，仅靠时间感知划分并不能解决特征泄露问题。\n     (ii) 在每个折内部，严格仅使用截至时间 $t$ 的可用过去数据来为训练和验证段重新计算特征，从而为每个折生成无泄露的特征 $x_t^{\\text{past}}$。\n\n4.  在每个折中对指定的特征拟合带截距的 OLS 线性模型，并在验证集上计算 MSE；将各折的验证 MSE 取平均，以获得交叉验证的 MSE。\n\n5.  设计一个边界测试，其中滚动窗口 $W = 1$，这使得有泄露的滚动平均值恰好等于 $y_{t+1}$，并展示其灾难性的泄露形式。\n\n你的程序必须实现以下测试套件，并生成单行输出，按顺序包含每个测试的布尔值结果：\n\n- 测试 A (理想路径下的泄露检测)：在 $N = 1200$, $a = 0.8$, $\\sigma = 1.0$, $W = 5$ 的条件下，使用有泄露特征进行随机 $K=6$ 折 CV，计算交叉验证的 MSE，如果其严格小于 $0.1$，则返回 true。\n- 测试 B (不重新计算特征的时间感知划分仍然存在泄露)：使用相同的 $N$, $a$, $\\sigma$, $W$ 和指定的时间感知 CV，但使用全局预计算的有泄露特征，计算交叉验证的 MSE，如果其严格小于 $0.1$，则返回 true。\n- 测试 C (带有每折重新计算的时间感知 CV 可防止泄露)：使用相同的 $N$, $a$, $\\sigma$, $W$ 和指定的时间感知 CV，但在每个折内重新计算无泄露特征，计算交叉验证的 MSE，如果其位于区间 $[0.6, 1.4]$ 内，则返回 true。\n- 测试 D (边界情况，$W = 1$ 时的灾难性泄露)：在 $N = 800$, $a = 0.8$, $\\sigma = 1.0$, $W = 1$ 的条件下，使用全局预计算的有泄露特征进行随机 $K=6$ 折 CV，计算交叉验证的 MSE，如果其严格小于 $10^{-12}$，则返回 true。\n\n最终输出格式：你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[resultA,resultB,resultC,resultD]\"）。每个结果必须是布尔字面量。不得打印任何额外文本。不涉及任何物理单位或角度，也不应输出任何百分比。所有常量和测试参数必须严格按照上述规定进行硬编码，以确保在不同环境中的可复现性和可测试性。随机数生成器种子也必须在程序中固定，以使结果具有确定性。解决方案必须能在任何支持指定库和语言的现代环境中运行。",
            "solution": "该问题要求通过计算来演示时间序列预测中的数据泄露、其后果以及如何通过正确的交叉验证和特征工程来解决它。问题的核心在于时间有序数据中的因果性原则：一个预测未来状态的模型不能获取来自该未来状态的信息。\n\n该问题被验证为是合理的、适定的和有科学依据的。它在统计学习领域提出了一个清晰、可形式化的实验。\n\n**1. 基本原理：时间序列、因果关系和数据泄露**\n\n时间序列 $\\{y_t\\}$ 是按时间排序的观测序列。预测中的一个基本原则是因果性：时间 $t+1$ 的状态受时间 $\\tau \\leq t$ 的状态影响，但不受时间 $\\tau' > t+1$ 的状态影响。预测模型必须遵守这一约束。当来自未来的信息（相对于预测时间而言）被无意中用于训练模型时，就会发生数据泄露。这会导致对模型性能的评估过于乐观且不正确。\n\n该问题定义了两种类型的特征，用以在时间 $t$ 使用可用信息来预测目标 $y_{t+1}$：\n\n- **无泄露（仅使用过去信息）的特征**：$x_t^{\\text{past}} = \\big(y_t, \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t-j}, 1\\big)$。两个特征，即值 $y_t$ 和在 $[t-W+1, t]$ 上的后向移动平均，都只使用在时间 $t$ 或之前可用的信息。这在因果上是有效的。\n\n- **有泄露（包含未来信息）的特征**：$x_t^{\\text{leak}} = \\big(y_t, \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t+1-j}, 1\\big)$。第二个特征 $g_{2,t} = \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t+1-j}$ 是在窗口 $[t-W+2, t+1]$ 上的移动平均。关键是，这个平均值包含了项 $y_{t+1}$（当 $j=0$ 时），而这正是我们试图预测的量。这是一种直接的数据泄露形式。一个旨在最小化均方误差 (MSE) 的普通最小二乘 (OLS) 模型将学会给这个特征赋予很大的权重，因为它与目标完全相关。\n\n**2. 交叉验证方案及其有效性**\n\n该问题对比了两种交叉验证 (CV) 方案：\n\n- **随机 K 折 CV**：这种标准方法假设样本是独立同分布的 (i.i.d.)。它随机地混洗所有数据点 $(x_i, y_i)$ 并将它们分成 $K$ 个折。对于时间序列，这是无效的，因为它破坏了时间依赖结构。模型可能在 $t=100$ 的数据上训练，并在 $t=50$ 的数据上验证。从时间角度看，这是毫无意义的，并构成了另一种形式的泄露，因为模型相对于其验证任务“看到”了未来。\n\n- **时间感知的前向链 CV**：此方法保留了时间顺序。数据被划分为一系列训练集和验证集。对于第 $k$ 次划分，训练集由截至时间 $t_k$ 的数据组成，验证集由随后的时间段 $(t_k, t_k+h]$ 内的数据组成。这模拟了一个真实世界的场景，即模型会定期在新数据上重新训练，并用于预测不久的将来。\n\n**3. 对所要求测试的分析**\n\n解决方案将围绕四个已定义的测试来构建，每个测试都旨在揭示数据泄露问题的不同方面。核心预测模型是 OLS，它找到系数 $\\hat{\\beta}$ 以最小化 MSE，其公式为 $\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - x_i^T \\hat{\\beta})^2$。OLS 解由 $\\hat{\\beta} = (X^T X)^{-1} X^T \\mathbf{y}$ 给出。\n\n**测试 A：随机 CV 中的泄露**\n此测试将有泄露的特征 ($x_t^{\\text{leak}}$) 与无效的交叉验证方法（随机 K 折）相结合。有泄露的特征 $g_{2,t}$ 包含了目标 $y_{t+1}$。OLS 模型会轻易发现这种关系，从而在验证集上做出近乎完美的预测。随机 CV 中的数据混洗并不能缓解此问题；它仅确保了泄露在每个折中都存在。因此，产生的 MSE 预计会极低，远低于过程固有的随机性，从而满足条件 $\\text{MSE}  0.1$。\n\n**测试 B：时间感知 CV 中的泄露**\n此测试使用正确的时间感知 CV 划分，但作用于预先计算好的带有泄露的特征 ($x_t^{\\text{leak}}$)。虽然 CV 划分尊重因果关系（在过去数据上训练，在未来数据上验证），但特征本身已经被污染。在 CV 过程开始之前，泄露就已经被“烘焙”到特征矩阵 $X^{\\text{leak}}$ 中了。因此，每个折中的模型仍然可以通过有泄露的特征访问到目标变量，并将产生人为的低 MSE。此测试表明，如果特征工程存在缺陷，仅靠正确的 CV 划分是不够的。MSE 预计同样会非常低，满足 $\\text{MSE}  0.1$。\n\n**测试 C：使用正确的 CV 和特征无泄露**\n此测试将正确的时序感知 CV 与正确工程化的、无泄露的特征 ($x_t^{\\text{past}}$) 相结合。特征在每个折的上下文中被重新评估（概念上如此，尽管在这种情况下它意味着简单地使用预先计算好的 `X_past` 矩阵，这对所有时间点都有效）。模型现在必须仅使用来自 $t$ 及更早时间的信息来预测 $y_{t+1}$。潜在的数据生成过程是 $y_{t+1} = a y_t + \\varepsilon_{t+1}$，其中 $\\varepsilon_{t+1}$ 是方差为 $\\sigma^2 = 1.0^2 = 1.0$ 的不可预测噪声。使用过去数据的最佳线性模型只能解释 $a y_t$ 部分。因此，不可约误差（贝叶斯错误率）为 $\\text{Var}(\\varepsilon_{t+1}) = \\sigma^2 = 1.0$。我们的 OLS 模型使用的信息比仅仅 $y_t$ 稍多（它还使用了一个过去的滚动平均值），因此它可能达到略低于 $1.0$ 的 MSE。由于有限样本效应，计算出的 MSE 会在该理论值附近波动。条件 $\\text{MSE} \\in [0.6, 1.4]$ 为一个正确指定的、无泄露的模型提供了一个合理的范围。\n\n**测试 D：灾难性泄露的边界情况**\n此测试检验了滚动窗口大小为 $W=1$ 的极端情况。有泄露的特征变为 $g_{2,t} = \\frac{1}{1}\\sum_{j=0}^{0} y_{t+1-j} = y_{t+1}$。用于预测的特征向量是 $x_t^{\\text{leak}} = (y_t, y_{t+1}, 1)$，目标是 $y_{t+1}$。线性回归问题是为模型 $\\hat{y}_{t+1} = \\beta_0 y_t + \\beta_1 y_{t+1} + \\beta_2$ 找到 $\\beta_0, \\beta_1, \\beta_2$。最小化平方误差的 OLS 解是平凡的 $\\hat{\\beta} = (0, 1, 0)^T$。模型的预测是 $\\hat{y}_{t+1} = y_{t+1}$，导致误差为零。在计算上，这将表现为一个数量级为机器精度的 MSE，因此有条件 $\\text{MSE}  10^{-12}$。\n\n**算法实现**\n程序将通过首先使用固定种子模拟 AR(1) 过程来实现这四个测试。然后，它将构建有泄露和无泄露的特征矩阵。对于每个测试，它将应用指定的 CV 方法，在每个折中使用 `numpy.linalg.lstsq` 拟合 OLS 模型，计算平均验证 MSE，并将其与指定的阈值进行比较以产生一个布尔结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a series of tests to demonstrate and quantify data leakage\n    in time series preprocessing and cross-validation.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def simulate_ar1(N, a, sigma, y0, seed):\n        \"\"\"Simulates an AR(1) process.\"\"\"\n        np.random.seed(seed)\n        y = np.zeros(N)\n        y[0] = y0\n        innovations = np.random.normal(0, sigma, N - 1)\n        for t in range(N - 1):\n            y[t + 1] = a * y[t] + innovations[t]\n        return y\n\n    def create_feature_matrices(y, W):\n        \"\"\"\n        Constructs supervised learning matrices from a time series.\n        y: The raw time series.\n        W: The rolling window size.\n        \"\"\"\n        N = len(y)\n        # Valid time indices for features are t in [W-1, N-2]\n        # This gives (N-2) - (W-1) + 1 = N - W samples.\n        num_samples = N - W\n        \n        # Initialize matrices\n        X_past = np.zeros((num_samples, 3))\n        X_leak = np.zeros((num_samples, 3))\n        Y = np.zeros(num_samples)\n\n        for i in range(num_samples):\n            t = i + W - 1  # Time index in the original series y\n\n            # Target\n            Y[i] = y[t + 1]\n\n            # Non-leaky features (past-only)\n            past_mean = np.mean(y[t - W + 1 : t + 1])\n            X_past[i, :] = [y[t], past_mean, 1]\n\n            # Leaky features (includes future)\n            leak_mean = np.mean(y[t + 1 - W + 1 : t + 1 + 1])\n            X_leak[i, :] = [y[t], leak_mean, 1]\n            \n        return X_past, X_leak, Y\n    \n    def fit_ols_and_get_mse(X_train, Y_train, X_val, Y_val):\n        \"\"\"Fits an OLS model and computes validation MSE.\"\"\"\n        if X_train.shape[0] == 0:\n            return np.nan\n        beta, _, _, _ = np.linalg.lstsq(X_train, Y_train, rcond=None)\n        Y_pred = X_val @ beta\n        mse = np.mean((Y_val - Y_pred)**2)\n        return mse\n\n    def run_random_kfold_cv(X, Y, K, seed):\n        \"\"\"Performs random K-fold cross-validation.\"\"\"\n        np.random.seed(seed)\n        num_samples = X.shape[0]\n        indices = np.arange(num_samples)\n        np.random.shuffle(indices)\n        \n        fold_size = num_samples // K\n        mses = []\n\n        for i in range(K):\n            start = i * fold_size\n            end = (i + 1) * fold_size if i  K - 1 else num_samples\n            \n            val_indices = indices[start:end]\n            train_indices = np.setdiff1d(indices, val_indices, assume_unique=True)\n            \n            X_train, Y_train = X[train_indices], Y[train_indices]\n            X_val, Y_val = X[val_indices], Y[val_indices]\n            \n            mses.append(fit_ols_and_get_mse(X_train, Y_train, X_val, Y_val))\n            \n        return np.mean(mses)\n\n    def run_time_aware_cv(X, Y, K, n_train0, n_val):\n        \"\"\"Performs time-aware (forward-chaining) cross-validation.\"\"\"\n        mses = []\n        for i in range(K):\n            train_end_idx = n_train0 + i * n_val\n            val_end_idx = train_end_idx + n_val\n            \n            train_indices = np.arange(train_end_idx)\n            val_indices = np.arange(train_end_idx, val_end_idx)\n\n            X_train, Y_train = X[train_indices], Y[train_indices]\n            X_val, Y_val = X[val_indices], Y[val_indices]\n\n            mses.append(fit_ols_and_get_mse(X_train, Y_train, X_val, Y_val))\n        \n        return np.mean(mses)\n\n    results = []\n    \n    # --- Shared Parameters and Seeds ---\n    AR1_SEED = 42\n    CV_RANDOM_SEED = 123\n    \n    # --- Test A: Leaky features + Random CV ---\n    N_A, a_A, sigma_A, W_A, K_A = 1200, 0.8, 1.0, 5, 6\n    y_A = simulate_ar1(N_A, a_A, sigma_A, 0, seed=AR1_SEED)\n    _, X_leak_A, Y_A = create_feature_matrices(y_A, W_A)\n    mse_A = run_random_kfold_cv(X_leak_A, Y_A, K_A, seed=CV_RANDOM_SEED)\n    results.append(mse_A  0.1)\n\n    # --- Test B: Leaky features + Time-aware CV ---\n    N_B, a_B, sigma_B, W_B = 1200, 0.8, 1.0, 5\n    K_B, n_train0_B, n_val_B = 5, 500, 100\n    y_B = simulate_ar1(N_B, a_B, sigma_B, 0, seed=AR1_SEED)\n    _, X_leak_B, Y_B = create_feature_matrices(y_B, W_B)\n    mse_B = run_time_aware_cv(X_leak_B, Y_B, K_B, n_train0_B, n_val_B)\n    results.append(mse_B  0.1)\n\n    # --- Test C: Non-leaky features + Time-aware CV ---\n    N_C, a_C, sigma_C, W_C = 1200, 0.8, 1.0, 5\n    K_C, n_train0_C, n_val_C = 5, 500, 100\n    y_C = simulate_ar1(N_C, a_C, sigma_C, 0, seed=AR1_SEED)\n    X_past_C, _, Y_C = create_feature_matrices(y_C, W_C)\n    mse_C = run_time_aware_cv(X_past_C, Y_C, K_C, n_train0_C, n_val_C)\n    results.append(0.6 = mse_C = 1.4)\n\n    # --- Test D: Catastrophic leakage (W=1) + Random CV ---\n    N_D, a_D, sigma_D, W_D, K_D = 800, 0.8, 1.0, 1, 6\n    y_D = simulate_ar1(N_D, a_D, sigma_D, 0, seed=AR1_SEED)\n    _, X_leak_D, Y_D = create_feature_matrices(y_D, W_D)\n    mse_D = run_random_kfold_cv(X_leak_D, Y_D, K_D, seed=CV_RANDOM_SEED)\n    results.append(mse_D  1e-12)\n\n    # --- Final Output ---\n    # Format: [resultA,resultB,resultC,resultD]\n    print(f\"[{','.join(map(str, results))}]\".lower())\n\nsolve()\n```"
        }
    ]
}