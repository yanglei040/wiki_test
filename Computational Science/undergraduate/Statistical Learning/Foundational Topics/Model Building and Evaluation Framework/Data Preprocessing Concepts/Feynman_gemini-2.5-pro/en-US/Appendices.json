{
    "hands_on_practices": [
        {
            "introduction": "Many phenomena in science and economics follow multiplicative, not additive, relationships. For instance, the error in a measurement might scale with the measurement's size. This exercise  provides hands-on practice in tackling such scenarios by using a logarithmic transformation. You will empirically verify how this powerful preprocessing step can convert a multiplicative model like $Y = \\beta X \\varepsilon$ into an additive linear one, $\\log(Y) = \\log(\\beta) + \\log(X) + \\log(\\varepsilon)$, thereby stabilizing variance and dramatically improving the fit of a linear model.",
            "id": "3112629",
            "problem": "You are tasked with constructing datasets under a multiplicative noise model and evaluating how a logarithmic transformation affects linear modeling performance. The setting is statistical learning with data preprocessing concepts. The fundamental base you may use includes the property of logarithms that for positive real numbers $a$ and $b$, $\\log(ab)=\\log a+\\log b$, and standard definitions from probability, including expectations under independence and well-tested formulas for the lognormal distribution.\n\nConsider an independent pair of positive random variables $(X,\\varepsilon)$ and a positive constant $\\beta$, with the generative relationship $Y=\\beta X \\varepsilon$. Assume $\\varepsilon$ is lognormally distributed with $\\log \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$, so that the mean $\\mathbb{E}[\\varepsilon]$ equals $\\exp(\\sigma^2/2)$. The goal is to compare ordinary least squares fits before and after a logarithmic transformation and assess whether the transformation linearizes the effect of multiplicative noise in a way that improves fit quality.\n\nYour program must, for each test case, perform the following steps:\n\n- Generate a dataset of size $n$, using the specified distribution for $X$ and the specified parameters for $\\beta$ and $\\sigma$.\n- Construct $Y=\\beta X \\varepsilon$ with $\\varepsilon$ drawn independently from a lognormal distribution with parameters $\\mu=0$ and $\\sigma$ as given for the test case.\n- Fit an ordinary least squares (OLS) linear model with intercept to the untransformed variables $Y$ versus $X$, i.e., approximate $Y \\approx a+bX$ by minimizing the sum of squared residuals, and compute the estimated slope $b_{\\text{pre}}$ and the coefficient of determination $R^2_{\\text{pre}}$.\n- Fit an ordinary least squares linear model with intercept to the log-transformed variables $\\log Y$ versus $\\log X$, i.e., approximate $\\log Y \\approx a_{\\log}+b_{\\log}\\log X$, and compute the estimated slope $b_{\\text{post}}$ and the coefficient of determination $R^2_{\\text{post}}$.\n- Compute the absolute slope error in the original scale relative to the population conditional mean linear slope, $\\left|b_{\\text{pre}} - \\beta \\exp(\\sigma^2/2)\\right|$, and the absolute slope error in the log-transformed scale relative to the expected linear relation, $\\left|b_{\\text{post}} - 1\\right|$.\n- Report whether the $R^2$ improves after the logarithmic transformation as a boolean $R^2_{\\text{post}} > R^2_{\\text{pre}}$, along with the two slope errors.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the form $[R2\\_improves,err\\_pre,err\\_post]$, where $R2\\_improves$ is a boolean, and $err\\_pre$ and $err\\_post$ are floats rounded to six decimal places. The final printed output should look like $[[\\text{bool},\\text{float},\\text{float}],\\ldots]$ with one entry per test case, in the order they are specified below.\n\nUse the following test suite. For reproducibility, use independent pseudorandom number generator seeds as specified for each case.\n\n- Test case $1$: $\\beta=2.0$, $n=200$, $\\sigma=0.3$, $X \\sim \\text{Uniform}[1,10]$, seed $42$.\n- Test case $2$: $\\beta=1.5$, $n=200$, $\\sigma=0.0$, $X \\sim \\text{Uniform}[1,10]$, seed $123$.\n- Test case $3$: $\\beta=1.0$, $n=1000$, $\\sigma=1.0$, $X \\sim \\text{Exponential}(\\text{scale}=3)$, seed $7$.\n- Test case $4$: $\\beta=0.5$, $n=50$, $\\sigma=0.8$, $X \\sim \\text{Lognormal}(\\mu=-0.2,\\sigma=0.5)$, seed $2025$.\n\nAngle units are not applicable, and there are no physical units involved. Percentages must not be used; all quantities should be represented as decimals.\n\nYour program must adhere to the specified output format and must not read input or write files.",
            "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in statistical learning principles, well-posed with a clear objective and sufficient data, and free from ambiguity or contradiction.\n\nThe core of this problem lies in understanding and mitigating heteroscedasticity—non-constant variance of errors—in a linear regression context. We are given a multiplicative noise model which is a common source of such issues.\n\n**Theoretical Framework**\n\nThe generative model for the data is given by the relationship $Y = \\beta X \\varepsilon$, where $Y$ and $X$ are the observable variables, $\\beta$ is a positive constant scaling factor, and $\\varepsilon$ is a multiplicative noise term. The variables $X$ and $\\varepsilon$ are independent and positive. The noise term $\\varepsilon$ is specified to follow a lognormal distribution such that its natural logarithm, $\\log \\varepsilon$, is normally distributed with mean $0$ and variance $\\sigma^2$, denoted as $\\log \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n\n**Analysis of the Untransformed Model: $Y$ versus $X$**\n\nFirst, we consider a direct linear regression on the untransformed variables. The theoretical relationship between $Y$ and $X$ is described by the conditional expectation $\\mathbb{E}[Y | X=x]$. Due to the independence of $X$ and $\\varepsilon$:\n$$\n\\mathbb{E}[Y | X=x] = \\mathbb{E}[\\beta x \\varepsilon | X=x] = \\beta x \\mathbb{E}[\\varepsilon]\n$$\nFor a lognormal random variable $\\varepsilon$ where $\\log \\varepsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)$, its expectation is $\\mathbb{E}[\\varepsilon] = \\exp(\\mu + \\sigma^2/2)$. In our case, $\\mu=0$, so $\\mathbb{E}[\\varepsilon] = \\exp(\\sigma^2/2)$.\nTherefore, the conditional expectation is:\n$$\n\\mathbb{E}[Y | X=x] = (\\beta e^{\\sigma^2/2}) x\n$$\nThis relationship is linear in $x$, with a slope of $\\beta e^{\\sigma^2/2}$. An ordinary least squares (OLS) fit of $Y$ on $X$ attempts to estimate this underlying linear trend. The target slope for comparison is thus $\\beta e^{\\sigma^2/2}$.\n\nHowever, a critical assumption for the OLS estimator to be the Best Linear Unbiased Estimator (BLUE) is homoscedasticity, meaning the variance of the error term is constant. Let's examine the conditional variance:\n$$\n\\text{Var}(Y | X=x) = \\text{Var}(\\beta x \\varepsilon) = (\\beta x)^2 \\text{Var}(\\varepsilon)\n$$\nSince $\\text{Var}(\\varepsilon)$ is a positive constant (for $\\sigma > 0$), the variance of $Y$ is proportional to $x^2$. This is a classic case of heteroscedasticity: the variance of the observations increases as the magnitude of the predictor $X$ increases. This violation can lead to an inefficient OLS estimator and unreliable hypothesis tests.\n\n**Analysis of the Log-Transformed Model: $\\log Y$ versus $\\log X$**\n\nTo address the issues of multiplicative noise and heteroscedasticity, a logarithmic transformation is proposed. Applying the natural logarithm to the generative model yields:\n$$\n\\log Y = \\log(\\beta X \\varepsilon) = \\log \\beta + \\log X + \\log \\varepsilon\n$$\nLet us define a new set of variables: $Y' = \\log Y$, $X' = \\log X$, and an additive error term $\\epsilon' = \\log \\varepsilon$. The model becomes:\n$$\nY' = (\\log \\beta) + 1 \\cdot X' + \\epsilon'\n$$\nBy definition, the error term $\\epsilon'$ follows a normal distribution, $\\epsilon' \\sim \\mathcal{N}(0, \\sigma^2)$. In this transformed space, the relationship between $Y'$ and $X'$ is perfectly linear with a true slope of $1$ and an intercept of $\\log \\beta$. The error term $\\epsilon'$ is now additive, and its variance, $\\sigma^2$, is constant for all values of $X'$. This new model satisfies the key assumptions of OLS (linearity, normality of errors, and homoscedasticity). Consequently, we expect the OLS fit on the log-transformed data to be more robust and provide a better characterization of the underlying relationship. The target slope for comparison in this transformed scale is $1$.\n\n**Computational Methodology**\n\nThe program implements a simulation to empirically verify these theoretical insights. For each specified test case:\n$1$. A pseudorandom number generator is initialized with a specific seed to ensure reproducibility.\n$2$. A dataset of size $n$ is generated. The predictor variable $X$ is drawn from its specified distribution. The multiplicative noise term $\\varepsilon$ is drawn from a lognormal distribution with parameters $\\mu=0$ and the given $\\sigma$. The response variable $Y$ is then constructed as $Y = \\beta X \\varepsilon$.\n$3$. **Pre-transformation fit**: An OLS model of the form $Y \\approx a_{\\text{pre}} + b_{\\text{pre}}X$ is fitted to the data $(X, Y)$. The slope estimate $b_{\\text{pre}}$ and the coefficient of determination $R^2_{\\text{pre}}$ are calculated. The absolute slope error is computed as $|b_{\\text{pre}} - \\beta e^{\\sigma^2/2}|$.\n$4$. **Post-transformation fit**: The variables are transformed to $\\log X$ and $\\log Y$. An OLS model of the form $\\log Y \\approx a_{\\text{post}} + b_{\\text{post}}\\log X$ is fitted to the data $(\\log X, \\log Y)$. The slope estimate $b_{\\text{post}}$ and the coefficient of determination $R^2_{\\text{post}}$ are calculated. The absolute slope error is computed as $|b_{\\text{post}} - 1|$.\n$5$. The coefficient of determination, $R^2$, is computed as the square of the Pearson correlation coefficient between the predictor and response variables, which is equivalent to $1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}$ for simple linear regression.\n$6$. The final metrics are assembled into a list: $[R^2_{\\text{post}} > R^2_{\\text{pre}}, |b_{\\text{pre}} - \\beta e^{\\sigma^2/2}|, |b_{\\text{post}} - 1|]$. The boolean indicates whether the log transformation improved the model fit in terms of explained variance. The two error terms quantify the accuracy of the slope estimation in each domain.",
            "answer": "```python\nimport numpy as np\n\ndef perform_analysis(beta, n, sigma, x_dist_params, seed):\n    \"\"\"\n    Generates a dataset under a multiplicative noise model and compares\n    OLS fits before and after a logarithmic transformation.\n\n    Args:\n        beta (float): The scaling factor in the model Y = beta * X * epsilon.\n        n (int): The number of data points to generate.\n        sigma (float): The scale parameter (std dev) of the log-normal noise.\n                       log(epsilon) ~ N(0, sigma^2).\n        x_dist_params (tuple): A tuple describing the distribution of X.\n                               e.g., ('uniform', low, high)\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        list: A list containing [R2_improves, err_pre, err_post].\n    \"\"\"\n    # 1. Initialize RNG and generate data\n    rng = np.random.default_rng(seed)\n\n    dist_type = x_dist_params[0]\n    if dist_type == 'uniform':\n        # Generate X from Uniform(low, high)\n        X = rng.uniform(low=x_dist_params[1], high=x_dist_params[2], size=n)\n    elif dist_type == 'exponential':\n        # Generate X from Exponential(scale)\n        X = rng.exponential(scale=x_dist_params[1], size=n)\n    elif dist_type == 'lognormal':\n        # Generate X from Lognormal(mu, sigma)\n        X = rng.lognormal(mean=x_dist_params[1], sigma=x_dist_params[2], size=n)\n    else:\n        # This case should not be reached with the given problem\n        raise ValueError(\"Unknown distribution for X\")\n\n    # Generate multiplicative noise epsilon from Lognormal(0, sigma)\n    # This means log(epsilon) is N(0, sigma^2)\n    epsilon = rng.lognormal(mean=0.0, sigma=sigma, size=n)\n\n    # Construct Y\n    Y = beta * X * epsilon\n    \n    # 2. Pre-transformation OLS fit (Y vs X)\n    # Using np.polyfit to get slope (b_pre) and intercept\n    b_pre, _ = np.polyfit(X, Y, 1)\n\n    # R-squared is the square of correlation for simple linear regression\n    # Handle cases where variance is zero to avoid NaN\n    if np.var(X) > 1e-12 and np.var(Y) > 1e-12:\n        R2_pre = np.corrcoef(X, Y)[0, 1]**2\n    else:\n        R2_pre = 0.0\n\n    # 3. Log-transform data\n    log_X = np.log(X)\n    log_Y = np.log(Y)\n\n    # 4. Post-transformation OLS fit (log Y vs log X)\n    b_post, _ = np.polyfit(log_X, log_Y, 1)\n\n    if np.var(log_X) > 1e-12 and np.var(log_Y) > 1e-12:\n        R2_post = np.corrcoef(log_X, log_Y)[0, 1]**2\n    else:\n        R2_post = 0.0\n\n    # 5. Compute comparison metrics\n    R2_improves = R2_post > R2_pre\n\n    # Absolute slope error for the pre-transformation model\n    target_slope_pre = beta * np.exp(sigma**2 / 2.0)\n    err_pre = np.abs(b_pre - target_slope_pre)\n\n    # Absolute slope error for the post-transformation model\n    target_slope_post = 1.0\n    err_post = np.abs(b_post - target_slope_post)\n\n    return [R2_improves, round(err_pre, 6), round(err_post, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {'beta': 2.0, 'n': 200, 'sigma': 0.3, 'x_dist_params': ('uniform', 1.0, 10.0), 'seed': 42},\n        {'beta': 1.5, 'n': 200, 'sigma': 0.0, 'x_dist_params': ('uniform', 1.0, 10.0), 'seed': 123},\n        {'beta': 1.0, 'n': 1000, 'sigma': 1.0, 'x_dist_params': ('exponential', 3.0), 'seed': 7},\n        {'beta': 0.5, 'n': 50, 'sigma': 0.8, 'x_dist_params': ('lognormal', -0.2, 0.5), 'seed': 2025}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = perform_analysis(**case)\n        results.append(result)\n\n    # Format the output string exactly as required\n    formatted_results = []\n    for res in results:\n        # res[0] is boolean, res[1] and res[2] are floats\n        formatted_results.append(f\"[{str(res[0])},{res[1]:.6f},{res[2]:.6f}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world data is rarely perfect and often contains outliers that can disproportionately influence standard preprocessing techniques like mean-centering and standard scaling. This practice  introduces the concept of robust statistics by having you implement and test a Huber M-estimator, a method designed to be insensitive to a certain fraction of anomalous data. By comparing its performance against a non-robust scaler in the presence of adversarial outliers, you will gain a practical understanding of breakdown points and the importance of robustness in building reliable machine learning pipelines.",
            "id": "3112649",
            "problem": "You are given the task of empirically comparing the robustness of two one-dimensional scaling schemes used in data preprocessing within statistical learning: a non-robust scheme that uses the sample mean and the Median Absolute Deviation (MAD), and a robust scheme based on Huberization. You will construct adversarial outliers and estimate the empirical finite-sample breakdown fraction of each scheme under worst-case contamination.\n\nDefinitions and fundamental base:\n- Let a dataset be a vector $x \\in \\mathbb{R}^n$ with entries $x_i$ for $i \\in \\{1,\\dots,n\\}$.\n- The standardized values are defined by $z_i = \\frac{x_i - \\hat{\\mu}}{\\hat{s}}$, where $\\hat{\\mu}$ is an estimator of location and $\\hat{s}$ is an estimator of scale.\n- The sample mean is defined by $\\hat{\\mu}_{\\mathrm{mean}} = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n- The Median Absolute Deviation (MAD) is defined by $\\mathrm{MAD}(x) = \\mathrm{median}_{i}\\left(\\left|x_i - \\mathrm{median}_{j}(x_j)\\right|\\right)$, and we will use $\\hat{s}_{\\mathrm{MAD}} = \\mathrm{MAD}(x)$ as the scale estimate (without any efficiency rescaling constant) in the non-robust scheme.\n- The Huber loss with parameter $c>0$ is defined by\n$$\n\\rho_c(u) = \n\\begin{cases}\n\\dfrac{1}{2}u^2, & \\text{if } |u| \\le c, \\\\\nc|u| - \\dfrac{1}{2}c^2, & \\text{if } |u| > c,\n\\end{cases}\n$$\nand its corresponding influence (score) function is\n$$\n\\psi_c(u) = \n\\begin{cases}\nu, & \\text{if } |u| \\le c, \\\\\nc\\cdot \\mathrm{sign}(u), & \\text{if } |u| > c.\n\\end{cases}\n$$\n- A Huberized location estimator $\\hat{\\mu}_{\\mathrm{Huber}}$ and Huberized scale estimator $\\hat{s}_{\\mathrm{Huber}}$ can be obtained by solving the coupled M-estimation equations\n$$\n\\sum_{i=1}^n \\psi_c\\!\\left(\\dfrac{x_i - \\hat{\\mu}}{\\hat{s}}\\right) = 0, \n\\quad\n\\hat{s}^2 = \\dfrac{1}{n}\\sum_{i=1}^n \\min\\!\\left((x_i - \\hat{\\mu})^2, (c\\hat{s})^2\\right),\n$$\nwhich we will approximate numerically via fixed-point iteration using an initial robust scale based on MAD and an initial robust location based on the sample median.\n\nEmpirical breakdown fraction:\n- The finite-sample breakdown point of an estimator is the smallest fraction of contamination that can drive the estimator to arbitrarily bad values under adversarial replacement. We will approximate it numerically as the smallest contamination fraction $p$ on a fixed grid at which the estimated location or scale violates pre-specified large bounds derived from the uncontaminated dataset.\n- Given a base dataset $x^{(0)} \\in \\mathbb{R}^n$, a contamination fraction $p \\in [0,1]$, and an outlier value $A \\gg 0$, construct an adversarially contaminated dataset $x^{(p)}$ by replacing the first $k = \\lfloor pn \\rfloor$ entries of $x^{(0)}$ with the single value $+A$.\n- Let $s_0 = \\mathrm{MAD}(x^{(0)})$ denote the baseline robust scale of the uncontaminated data. Define bounds\n$$\nL = \\alpha_L s_0,\\quad S_{\\max} = \\alpha_H s_0,\\quad S_{\\min} = \\alpha_L^{\\prime} s_0,\n$$\nwhere $\\alpha_L > 0$, $\\alpha_H > 0$, and $\\alpha_L^{\\prime} > 0$ are constants.\n- For each contamination level $p$ on a predetermined grid, compute $(\\hat{\\mu},\\hat{s})$ under each scheme and declare breakdown if either $|\\hat{\\mu}| > L$ or $\\hat{s} > S_{\\max}$ or $\\hat{s}  S_{\\min}$. The empirical breakdown fraction is the smallest $p$ on the grid at which breakdown occurs.\n\nSchemes to implement:\n1. Mean/MAD scaling: $\\hat{\\mu} = \\hat{\\mu}_{\\mathrm{mean}}$ and $\\hat{s} = \\mathrm{MAD}(x)$.\n2. Huberized scaling: initialize $\\hat{\\mu}^{(0)} = \\mathrm{median}(x)$ and $\\hat{s}^{(0)} = \\mathrm{MAD}(x)$, then iterate\n$$\nr_i^{(t)} = \\dfrac{x_i - \\hat{\\mu}^{(t)}}{\\hat{s}^{(t)}},\\quad\nw_i^{(t)} = \\min\\!\\left(1, \\dfrac{c}{|r_i^{(t)}|+\\varepsilon}\\right),\\quad\n\\hat{\\mu}^{(t+1)} = \\dfrac{\\sum_i w_i^{(t)} x_i}{\\sum_i w_i^{(t)}},\n$$\n$$\n\\hat{s}^{(t+1)} = \\sqrt{\\dfrac{1}{n}\\sum_{i=1}^n \\min\\!\\left((x_i - \\hat{\\mu}^{(t+1)})^2, (c\\hat{s}^{(t)})^2\\right)},\n$$\nfor a fixed number of iterations, with $\\varepsilon$ a small positive constant to avoid division by zero. The final $(\\hat{\\mu},\\hat{s}) = (\\hat{\\mu}^{(T)},\\hat{s}^{(T)})$.\n\nTest suite:\nUse $n = 200$, Huber parameter $c = 1.345$, outlier value $A = 10^9$, contamination grid\n$$\n\\mathcal{P} = [\\,0,\\;0.001,\\;0.005,\\;0.01,\\;0.02,\\;0.05,\\;0.1,\\;0.2,\\;0.3,\\;0.4,\\;0.49,\\;0.5,\\;0.51\\,],\n$$\nand bounds $\\alpha_L = 10$, $\\alpha_H = 10$, $\\alpha_L^{\\prime} = 0.05$. Construct three base datasets $x^{(0)}$:\n- Case $1$: Standard Normal, $x_i \\sim \\mathcal{N}(0,1)$, with seed $123$.\n- Case $2$: Student-$t$ with $\\nu=3$ degrees of freedom, $x_i \\sim t_3$, with seed $456$.\n- Case $3$: Shifted Exponential, $y_i \\sim \\mathrm{Exp}(\\lambda=1)$ and $x_i = y_i - \\frac{1}{n}\\sum_j y_j$, with seed $789$.\n\nTasks:\n- For each case, compute the empirical breakdown fraction for both schemes, following the contamination process and breakdown criterion above.\n- Report each empirical breakdown fraction rounded to three decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, where each inner list corresponds to a test case and contains two floats: $[\\text{breakdown\\_mean\\_MAD},\\text{breakdown\\_Huberized}]$. For example, an output with three test cases should look like\n$$\n[[a_1,b_1],[a_2,b_2],[a_3,b_3]]\n$$\nwhere each $a_i$ and $b_i$ is a decimal rounded to three places. No other text should be printed. No physical units or angles are involved, and contamination fractions must be expressed as decimals.",
            "solution": "We aim to assess robustness for two scaling schemes via the empirical finite-sample breakdown fraction under adversarial contamination. The conceptual starting point is the definition of breakdown point in robust statistics: the minimal fraction of contamination that can force an estimator to attain arbitrarily large or nonsensical values. Since exact infinity cannot be reached numerically, we define thresholds relative to the uncontaminated data’s robust scale to detect practical divergence.\n\nStep-by-step principle-based design:\n1. Baseline robust scale and bounds: For an uncontaminated dataset $x^{(0)}$, compute $s_0 = \\mathrm{MAD}(x^{(0)})$. Use constants $\\alpha_L$, $\\alpha_H$, $\\alpha_L^{\\prime}$ to construct bounds $L = \\alpha_L s_0$, $S_{\\max} = \\alpha_H s_0$, and $S_{\\min} = \\alpha_L^{\\prime} s_0$. This anchors breakdown detection to the dataset’s intrinsic scale.\n\n2. Adversarial contamination model: For any chosen fraction $p \\in [0,1]$, replace $k = \\lfloor pn \\rfloor$ entries in $x^{(0)}$ by a single extreme outlier value $+A$ (with $A \\gg 0$). This is adversarial because placing all contamination at a large constant magnitude maximizes the location shift and can inflate scale, particularly for non-robust estimators.\n\n3. Non-robust scheme (Mean/MAD):\n   - Location: $\\hat{\\mu} = \\dfrac{1}{n}\\sum_i x_i$. This estimator has a theoretical breakdown point of $0$ because any non-zero fraction of arbitrarily large contamination can force $\\hat{\\mu}$ arbitrarily far from its uncontaminated value.\n   - Scale: $\\hat{s} = \\mathrm{MAD}(x)$, which on its own has a high breakdown point (about $0.5$), but in the scaling scheme the location uses the mean, making the standardized output vulnerable. In our breakdown detection, we carefully track whether $|\\hat{\\mu}|$ exceeds $L$ and whether $\\hat{s}$ violates its bounds.\n\n4. Robust scheme (Huberized scaling): We use Huber’s M-estimation principles. The location equation $\\sum \\psi_c\\!\\left(\\dfrac{x_i - \\hat{\\mu}}{\\hat{s}}\\right) = 0$ clamps the influence of large residuals via $\\psi_c$, and the scale update\n   $$\n   \\hat{s}^2 = \\dfrac{1}{n}\\sum_{i=1}^n \\min\\!\\left((x_i - \\hat{\\mu})^2, (c\\hat{s})^2\\right)\n   $$\n   winsorizes squared residuals at the threshold $c\\hat{s}$.\n   - Algorithmic solution: Start with robust initial values $\\hat{\\mu}^{(0)} = \\mathrm{median}(x)$ and $\\hat{s}^{(0)} = \\mathrm{MAD}(x)$ and apply fixed-point iterations. At each iteration $t$, form standardized residuals $r_i^{(t)}$, compute weights $w_i^{(t)} = \\min\\!\\left(1, \\dfrac{c}{|r_i^{(t)}|+\\varepsilon}\\right)$ for a small $\\varepsilon0$, update location as the weighted mean $\\hat{\\mu}^{(t+1)}$, and update scale via the winsorized second moment formula $\\hat{s}^{(t+1)}$. This procedure follows from the first-order conditions of Huber’s M-estimation and corresponds to iteratively reweighted least squares and winsorization.\n   - Robustness intuition: Because $\\psi_c$ bounds each point’s influence, sub-majority contamination cannot push the location far; the winsorized scale also limits the effect of extreme points. However, once contamination exceeds about one-half, the adversary can steer both equations since the majority of points are contaminated. Then the fixed-point iteration can inflate $\\hat{s}$ (due to repeated multiplication by the winsorization threshold) and shift $\\hat{\\mu}$, producing breakdown. Thus, the empirical breakdown for Huberized scaling is expected near $0.5$.\n\n5. Empirical breakdown detection: For each contamination fraction $p$ on a fixed grid, compute $(\\hat{\\mu},\\hat{s})$ under both schemes and check whether $|\\hat{\\mu}| > L$ or $\\hat{s} > S_{\\max}$ or $\\hat{s}  S_{\\min}$. The minimal $p$ for which this occurs is reported as the empirical breakdown fraction.\n\n6. Test suite design:\n   - Case $1$ (happy path): Standard Normal $x_i \\sim \\mathcal{N}(0,1)$. The mean/MAD should break at the smallest non-zero contamination level due to the extreme $A$, whereas Huberized scaling should resist until near $0.5$.\n   - Case $2$ (heavy tails): Student-$t$ with $\\nu=3$. The base data have heavy tails, challenging scale estimation stability, but the relative difference in breakdown between the schemes should persist.\n   - Case $3$ (skewed): Shifted Exponential. The non-symmetry tests robustness of initializations and the iteration, particularly the median/MAD start.\n\n7. Output: For each case, compute the empirical breakdown fractions for the two schemes, round them to three decimal places, and present them as a single line in the format $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$.\n\nThis approach adheres to the foundational definitions of robust estimation, Huber’s influence function, and the breakdown point, translating them into a practical algorithmic test of robustness under adversarial contamination without relying on shortcut formulas. The thresholds $L$, $S_{\\max}$, and $S_{\\min}$ serve to detect numerical divergence in a principled way relative to the uncontaminated dataset’s robust scale $s_0$.",
            "answer": "```python\nimport numpy as np\n\n# Environment: Python 3.12, numpy 1.23.5, scipy not used.\n\ndef mad(x: np.ndarray) -> float:\n    \"\"\"Median Absolute Deviation (unscaled).\"\"\"\n    med = np.median(x)\n    return np.median(np.abs(x - med))\n\ndef mean_mad_estimates(x: np.ndarray) -> tuple[float, float]:\n    \"\"\"Non-robust location (mean) and robust scale (MAD).\"\"\"\n    mu = float(np.mean(x))\n    s = float(mad(x))\n    # Guard against zero MAD\n    if s = 0.0 or not np.isfinite(s):\n        s = 1e-12\n    return mu, s\n\ndef huberized_location_scale(x: np.ndarray, c: float = 1.345, max_iter: int = 100, tol: float = 1e-9) -> tuple[float, float]:\n    \"\"\"\n    Huberized location and scale via fixed-point iteration:\n    - Location: weighted mean with weights w_i = min(1, c / |r_i|)\n    - Scale: winsorized second moment s^2 = mean(min(diff^2, (c*s)^2))\n    Initialization uses median and MAD.\n    \"\"\"\n    n = x.size\n    # Robust initializers\n    mu = float(np.median(x))\n    s = float(mad(x))\n    if s = 0.0 or not np.isfinite(s):\n        s = 1e-6\n    eps = 1e-12\n\n    for _ in range(max_iter):\n        # Standardized residuals\n        r = (x - mu) / (s + eps)\n        # Huber weights for location\n        w = np.minimum(1.0, c / (np.abs(r) + eps))\n        # Update location: weighted mean\n        w_sum = np.sum(w)\n        if w_sum = 0.0 or not np.isfinite(w_sum):\n            w_sum = 1e-12\n        mu_new = float(np.sum(w * x) / w_sum)\n\n        # Update scale: winsorized second moment at threshold c*s\n        diff = x - mu_new\n        clip_sq = np.minimum(diff * diff, (c * s) ** 2)\n        s_new = float(np.sqrt(np.mean(clip_sq)))\n        if s_new = 0.0 or not np.isfinite(s_new):\n            s_new = s  # fallback to previous\n\n        # Convergence check\n        if abs(mu_new - mu)  tol and abs(s_new - s)  tol:\n            mu, s = mu_new, s_new\n            break\n\n        mu, s = mu_new, s_new\n\n    return mu, s\n\ndef contaminate(x0: np.ndarray, p: float, A: float) -> np.ndarray:\n    \"\"\"Replace floor(p*n) entries with the adversarial value +A.\"\"\"\n    n = x0.size\n    k = int(np.floor(p * n))\n    if k = 0:\n        return x0.copy()\n    x = x0.copy()\n    x[:k] = A\n    return x\n\ndef empirical_breakdown_fraction(\n    x0: np.ndarray,\n    A: float,\n    p_grid: list[float],\n    alpha_L: float,\n    alpha_H: float,\n    alpha_L_prime: float,\n    scheme: str,\n    c: float = 1.345\n) -> float:\n    \"\"\"\n    Compute the empirical breakdown fraction for a given scheme:\n    - scheme in {\"mean_mad\", \"huberized\"}.\n    Breakdown if |mu| > L or s > S_max or s  S_min, where bounds are based on s0 = MAD(x0).\n    \"\"\"\n    s0 = mad(x0)\n    # Guard s0 to avoid zero baseline scale\n    if s0 = 0.0 or not np.isfinite(s0):\n        s0 = 1e-6\n\n    L = alpha_L * s0\n    S_max = alpha_H * s0\n    S_min = alpha_L_prime * s0\n\n    for p in p_grid:\n        x = contaminate(x0, p, A)\n        if scheme == \"mean_mad\":\n            mu_hat, s_hat = mean_mad_estimates(x)\n        elif scheme == \"huberized\":\n            mu_hat, s_hat = huberized_location_scale(x, c=c)\n        else:\n            raise ValueError(\"Unknown scheme\")\n\n        # Breakdown conditions\n        if (abs(mu_hat) > L) or (s_hat > S_max) or (s_hat  S_min) or (not np.isfinite(mu_hat)) or (not np.isfinite(s_hat)):\n            return float(p)\n\n    # If no breakdown observed on the grid, return 1.0 as a sentinel\n    return 1.0\n\ndef generate_base_data(case: int, n: int, seed: int) -> np.ndarray:\n    rng = np.random.RandomState(seed)\n    if case == 1:\n        # Standard Normal\n        return rng.normal(loc=0.0, scale=1.0, size=n)\n    elif case == 2:\n        # Student-t with nu=3\n        return rng.standard_t(df=3, size=n)\n    elif case == 3:\n        # Shifted Exponential: centered to have mean ~0\n        y = rng.exponential(scale=1.0, size=n)\n        return y - np.mean(y)\n    else:\n        raise ValueError(\"Unknown case\")\n\ndef format_results(rows: list[tuple[float, float]]) -> str:\n    \"\"\"Format nested list of floats with three decimals and no spaces.\"\"\"\n    inner = []\n    for a, b in rows:\n        inner.append(f\"[{a:.3f},{b:.3f}]\")\n    return \"[\" + \",\".join(inner) + \"]\"\n\ndef solve():\n    # Test suite parameters\n    n = 200\n    c = 1.345\n    A = 1e9\n    p_grid = [0.0, 0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.49, 0.5, 0.51]\n    alpha_L = 10.0\n    alpha_H = 10.0\n    alpha_L_prime = 0.05\n\n    test_cases = [\n        # (case_id, seed)\n        (1, 123),\n        (2, 456),\n        (3, 789),\n    ]\n\n    results = []\n    for case_id, seed in test_cases:\n        x0 = generate_base_data(case_id, n, seed)\n\n        bp_mean_mad = empirical_breakdown_fraction(\n            x0=x0, A=A, p_grid=p_grid,\n            alpha_L=alpha_L, alpha_H=alpha_H, alpha_L_prime=alpha_L_prime,\n            scheme=\"mean_mad\", c=c\n        )\n        bp_huber = empirical_breakdown_fraction(\n            x0=x0, A=A, p_grid=p_grid,\n            alpha_L=alpha_L, alpha_H=alpha_H, alpha_L_prime=alpha_L_prime,\n            scheme=\"huberized\", c=c\n        )\n        results.append((bp_mean_mad, bp_huber))\n\n    # Print final output line in exact required format\n    print(format_results(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "When working with time series data, the arrow of time imposes a strict constraint: you cannot use information from the future to predict the past or present. Violating this principle leads to data leakage, a subtle but critical error that results in overly optimistic performance estimates. This exercise  provides a crucial hands-on demonstration of how data leakage can occur during feature engineering and how to design a correct, time-aware cross-validation strategy to obtain a realistic assessment of your forecasting model's performance.",
            "id": "3112690",
            "problem": "You are given the task of constructing a deterministic program that demonstrates and quantifies data leakage in time series preprocessing when a future-derived feature is mistakenly included, and then designing a time-aware cross-validation procedure that eliminates this leakage by recomputing features without using future information inside each fold. The goal is to reason from first principles about temporal dependence, stationarity, and cross-validation validity in the presence of ordered data.\n\nUse the following foundational base and definitions: A univariate time series is a sequence $\\{y_t\\}_{t=0}^{T}$ indexed by discrete time $t$. An autoregressive process of order one (AR(1)) is given by $y_{t+1} = a \\, y_t + \\varepsilon_{t+1}$, where $\\varepsilon_{t}$ is zero-mean noise with variance $\\sigma^2$ and $|a|  1$ ensures weak stationarity. Ordinary least squares (OLS) regression estimates coefficients $\\hat{\\beta}$ by minimizing mean squared error (MSE) given by $\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$. Random $K$-fold cross-validation (cross-validation (CV)) assumes independent and identically distributed samples and splits indices into $K$ disjoint folds at random; in time series, this assumption is violated because of temporal dependence and causality constraints. Time-aware CV for time series uses forward-chaining (expanding window) splits that respect temporal order: training uses only indices $\\leq t_{\\text{train}}$ and validation uses indices in $(t_{\\text{train}}, t_{\\text{val}}]$.\n\nConstruct the supervised learning target as $y_{t+1}$ and features at time $t$ as follows. The non-leaky, past-only features are $f_{1,t} = y_t$ and $f_{2,t} = \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t-j}$ (a trailing mean that uses only the present and $W-1$ past values). A leaky, future-derived feature is $g_{2,t} = \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t+1-j}$, which uses $y_{t+1}$; pairing it with $g_{1,t} = y_t$ yields a leaky feature set that directly embeds the target into the inputs when $W \\ge 1$, thereby violating the training-time information constraints.\n\nYour program must:\n\n1. Simulate an AR(1) process with parameters exactly as follows for the main experiments: length $N = 1200$, autoregressive coefficient $a = 0.8$, innovation standard deviation $\\sigma = 1.0$, and initial condition $y_0 = 0$. Use a fixed random number generator seed to ensure determinism.\n\n2. Construct two feature sets for rows indexed by $t \\in \\{W-1, W, \\dots, N-2\\}$ so that the target is $y_{t+1}$:\n   - Leaky features: $x_t^{\\text{leak}} = \\big(y_t, \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t+1-j}, 1\\big)$ where the last component is an intercept term.\n   - Non-leaky features: $x_t^{\\text{past}} = \\big(y_t, \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t-j}, 1\\big)$ where the last component is an intercept term.\n\n3. Implement two CV schemes:\n   - Random $K$-fold CV with $K = 6$, using a fixed seed and uniform random shuffling of row indices before splitting. For this CV, use the globally precomputed leaky features $x_t^{\\text{leak}}$.\n   - Time-aware forward-chaining CV with $K = 5$ folds, initial training size $n_{\\text{train},0} = 500$, and validation block size $n_{\\text{val}} = 100$ (the $k$-th fold uses training rows up to index $n_{\\text{train},0} + (k-1) n_{\\text{val}} - 1$ and validates on the next $n_{\\text{val}}$ rows). You must implement two variants:\n     (i) Use globally precomputed leaky features $x_t^{\\text{leak}}$ to demonstrate that time-aware splitting alone does not cure feature leakage if features were precomputed with future information.\n     (ii) Recompute features inside each fold strictly using only available past data up to time $t$ for both the training and validation segments, thereby producing non-leaky features $x_t^{\\text{past}}$ per fold.\n\n4. Fit OLS linear models with an intercept on the specified features in each fold and compute the MSE on the validation set; average the validation MSEs over folds to obtain a cross-validated MSE.\n\n5. Design a boundary test where the rolling window is $W = 1$, which makes the leaky rolling mean equal to $y_{t+1}$ exactly, and show the catastrophic form of leakage.\n\nYour program must implement the following test suite and produce a single line of output containing the boolean outcomes for each test in order:\n\n- Test A (happy-path leakage detection): With $N = 1200$, $a = 0.8$, $\\sigma = 1.0$, $W = 5$, and random $K = 6$-fold CV using leaky features, compute the cross-validated MSE and return true if it is strictly less than $0.1$.\n- Test B (time-aware splitting without feature recomputation still leaks): With the same $N$, $a$, $\\sigma$, $W$, and the specified time-aware CV, but using leaky features precomputed globally, compute the cross-validated MSE and return true if it is strictly less than $0.1$.\n- Test C (time-aware CV with per-fold recomputation prevents leakage): With the same $N$, $a$, $\\sigma$, $W$, and the specified time-aware CV, but recomputing non-leaky features inside each fold, compute the cross-validated MSE and return true if it lies within the interval $[0.6, 1.4]$.\n- Test D (boundary case, $W = 1$ catastrophic leakage): With $N = 800$, $a = 0.8$, $\\sigma = 1.0$, $W = 1$, using random $K = 6$-fold CV with globally precomputed leaky features, compute the cross-validated MSE and return true if it is strictly less than $10^{-12}$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD]\"). Each result must be a boolean literal. No additional text may be printed. No physical units or angles are involved, and no percentages should be output. All constants and test parameters must be hard-coded exactly as specified above to ensure reproducibility and testability across environments. The random number generator seeds must also be fixed within the program to make results deterministic. The solution must run in any modern environment supporting the specified libraries and language.",
            "solution": "The problem requires a computational demonstration of data leakage in time series forecasting, its consequences, and its resolution through proper cross-validation and feature engineering. The core of the problem rests on the principle of causality in time-ordered data: a model predicting a future state cannot have access to information from that future state.\n\nThe problem is validated as sound, well-posed, and scientifically grounded. It presents a clear, formalizable experiment in statistical learning.\n\n**1. Foundational Principles: Time Series, Causality, and Data Leakage**\n\nA time series $\\{y_t\\}$ is a sequence of observations ordered in time. A fundamental principle in forecasting is causality: the state at time $t+1$ is influenced by states at times $\\tau \\leq t$, but not by states at times $\\tau' > t+1$. A predictive model must respect this constraint. Data leakage occurs when information from the future (relative to the time of prediction) is inadvertently used to train the model. This leads to an overly optimistic, and incorrect, assessment of the model's performance.\n\nThe problem defines two types of features to predict the target $y_{t+1}$ using information available at time $t$:\n\n- **Non-leaky (Past-only) Features**: $x_t^{\\text{past}} = \\big(y_t, \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t-j}, 1\\big)$. Both features, the value $y_t$ and the trailing mean over $[t-W+1, t]$, use only information available at or before time $t$. This is causally valid.\n\n- **Leaky (Future-inclusive) Features**: $x_t^{\\text{leak}} = \\big(y_t, \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t+1-j}, 1\\big)$. The second feature, $g_{2,t} = \\frac{1}{W}\\sum_{j=0}^{W-1} y_{t+1-j}$, is a moving average over the window $[t-W+2, t+1]$. Critically, this average includes the term $y_{t+1}$ (when $j=0$), which is the exact quantity we are trying to predict. This is a direct form of data leakage. An ordinary least squares (OLS) model, which minimizes the mean squared error (MSE), will learn to place a large weight on this feature, as it is perfectly correlated with the target.\n\n**2. Cross-Validation Schemes and their Validity**\n\nThe problem contrasts two cross-validation (CV) schemes:\n\n- **Random K-Fold CV**: This standard method assumes samples are independent and identically distributed (i.i.d.). It randomly shuffles all data points $(x_i, y_i)$ and splits them into $K$ folds. For time series, this is invalid because it breaks the temporal dependence structure. A model might be trained on data from $t=100$ and validated on data from $t=50$. This is nonsensical from a temporal perspective and constitutes another form of leakage, as the model has \"seen the future\" relative to its validation task.\n\n- **Time-Aware Forward-Chaining CV**: This method preserves chronological order. The data is split into a sequence of training and validation sets. For the $k$-th split, the training set consists of data up to a time $t_k$, and the validation set consists of data in the subsequent period $(t_k, t_k+h]$. This mimics a real-world scenario where a model is periodically retrained on new data and used to predict the immediate future.\n\n**3. Analysis of the Required Tests**\n\nThe solution will be structured around the four defined tests, each designed to expose a different aspect of the data leakage problem. The core predictive model is OLS, which finds coefficients $\\hat{\\beta}$ to minimize the MSE, given by $\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - x_i^T \\hat{\\beta})^2$. The OLS solution is given by $\\hat{\\beta} = (X^T X)^{-1} X^T \\mathbf{y}$.\n\n**Test A: Leakage with Random CV**\nThis test combines leaky features ($x_t^{\\text{leak}}$) with an invalid cross-validation method (random K-fold). The leaky feature $g_{2,t}$ contains the target $y_{t+1}$. The OLS model will easily discover this relationship, leading to near-perfect predictions on the validation sets. The shuffling in random CV does not mitigate this; it only ensures that the leakage is present in every fold. The resulting MSE is expected to be extremely low, much lower than the inherent randomness of the process, satisfying the condition $\\text{MSE}  0.1$.\n\n**Test B: Leakage with Time-Aware CV**\nThis test uses the correct time-aware CV splitting but on features that were pre-computed with leakage ($x_t^{\\text{leak}}$). While the CV splitting respects causality (training on the past, validating on the future), the features themselves are already contaminated. The leakage is baked into the feature matrix $X^{\\text{leak}}$ before the CV procedure even begins. Therefore, the model in each fold will still have access to the target variable via the leaky feature and will produce an artificially low MSE. This test demonstrates that proper CV splitting alone is insufficient if the feature engineering is flawed. The MSE is again expected to be very low, satisfying $\\text{MSE}  0.1$.\n\n**Test C: No Leakage with Correct CV and Features**\nThis test combines the correct time-aware CV with correctly engineered, non-leaky features ($x_t^{\\text{past}}$). The features are re-evaluated within each fold's context (conceptually, though in this case it means simply using the pre-computed `X_past` matrix, which is valid for all time points). The model must now predict $y_{t+1}$ using only information from $t$ and earlier. The underlying data generating process is $y_{t+1} = a y_t + \\varepsilon_{t+1}$, where $\\varepsilon_{t+1}$ is unpredictable noise with variance $\\sigma^2 = 1.0^2 = 1.0$. The best possible linear model using past data can only explain the $a y_t$ part. Therefore, the irreducible error (Bayes error rate) is $\\text{Var}(\\varepsilon_{t+1}) = \\sigma^2 = 1.0$. Our OLS model uses slightly more information than just $y_t$ (it also uses a past rolling mean), so it might achieve an MSE slightly below $1.0$. Due to finite sample effects, the calculated MSE will fluctuate around this theoretical value. The condition $\\text{MSE} \\in [0.6, 1.4]$ provides a reasonable range for a correctly specified, non-leaky model.\n\n**Test D: Catastrophic Leakage Boundary Case**\nThis test examines the extreme case where the rolling window size is $W=1$. The leaky feature becomes $g_{2,t} = \\frac{1}{1}\\sum_{j=0}^{0} y_{t+1-j} = y_{t+1}$. The feature vector for prediction is $x_t^{\\text{leak}} = (y_t, y_{t+1}, 1)$, and the target is $y_{t+1}$. The linear regression problem is to find $\\beta_0, \\beta_1, \\beta_2$ for the model $\\hat{y}_{t+1} = \\beta_0 y_t + \\beta_1 y_{t+1} + \\beta_2$. The OLS solution that minimizes squared error is trivially $\\hat{\\beta} = (0, 1, 0)^T$. The model's prediction is $\\hat{y}_{t+1} = y_{t+1}$, resulting in an error of zero. Computationally, this will manifest as an MSE on the order of machine precision, hence the condition $\\text{MSE}  10^{-12}$.\n\n**Algorithmic Implementation**\nThe program will implement these four tests by first simulating the AR(1) process with a fixed seed. Then, it will construct the leaky and non-leaky feature matrices. For each test, it will apply the specified CV methodology, fit OLS models in each fold using `numpy.linalg.lstsq`, calculate the average validation MSE, and compare it against the specified threshold to produce a boolean result.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a series of tests to demonstrate and quantify data leakage\n    in time series preprocessing and cross-validation.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def simulate_ar1(N, a, sigma, y0, seed):\n        \"\"\"Simulates an AR(1) process.\"\"\"\n        np.random.seed(seed)\n        y = np.zeros(N)\n        y[0] = y0\n        innovations = np.random.normal(0, sigma, N - 1)\n        for t in range(N - 1):\n            y[t + 1] = a * y[t] + innovations[t]\n        return y\n\n    def create_feature_matrices(y, W):\n        \"\"\"\n        Constructs supervised learning matrices from a time series.\n        y: The raw time series.\n        W: The rolling window size.\n        \"\"\"\n        N = len(y)\n        # Valid time indices for features are t in [W-1, N-2]\n        # This gives (N-2) - (W-1) + 1 = N - W samples.\n        num_samples = N - W\n        \n        # Initialize matrices\n        X_past = np.zeros((num_samples, 3))\n        X_leak = np.zeros((num_samples, 3))\n        Y = np.zeros(num_samples)\n\n        for i in range(num_samples):\n            t = i + W - 1  # Time index in the original series y\n\n            # Target\n            Y[i] = y[t + 1]\n\n            # Non-leaky features (past-only)\n            past_mean = np.mean(y[t - W + 1 : t + 1])\n            X_past[i, :] = [y[t], past_mean, 1]\n\n            # Leaky features (includes future)\n            leak_mean = np.mean(y[t + 1 - W + 1 : t + 1 + 1])\n            X_leak[i, :] = [y[t], leak_mean, 1]\n            \n        return X_past, X_leak, Y\n    \n    def fit_ols_and_get_mse(X_train, Y_train, X_val, Y_val):\n        \"\"\"Fits an OLS model and computes validation MSE.\"\"\"\n        if X_train.shape[0] == 0:\n            return np.nan\n        beta, _, _, _ = np.linalg.lstsq(X_train, Y_train, rcond=None)\n        Y_pred = X_val @ beta\n        mse = np.mean((Y_val - Y_pred)**2)\n        return mse\n\n    def run_random_kfold_cv(X, Y, K, seed):\n        \"\"\"Performs random K-fold cross-validation.\"\"\"\n        np.random.seed(seed)\n        num_samples = X.shape[0]\n        indices = np.arange(num_samples)\n        np.random.shuffle(indices)\n        \n        fold_size = num_samples // K\n        mses = []\n\n        for i in range(K):\n            start = i * fold_size\n            end = (i + 1) * fold_size if i  K - 1 else num_samples\n            \n            val_indices = indices[start:end]\n            train_indices = np.setdiff1d(indices, val_indices, assume_unique=True)\n            \n            X_train, Y_train = X[train_indices], Y[train_indices]\n            X_val, Y_val = X[val_indices], Y[val_indices]\n            \n            mses.append(fit_ols_and_get_mse(X_train, Y_train, X_val, Y_val))\n            \n        return np.mean(mses)\n\n    def run_time_aware_cv(X, Y, K, n_train0, n_val):\n        \"\"\"Performs time-aware (forward-chaining) cross-validation.\"\"\"\n        mses = []\n        for i in range(K):\n            train_end_idx = n_train0 + i * n_val\n            val_end_idx = train_end_idx + n_val\n            \n            train_indices = np.arange(train_end_idx)\n            val_indices = np.arange(train_end_idx, val_end_idx)\n\n            X_train, Y_train = X[train_indices], Y[train_indices]\n            X_val, Y_val = X[val_indices], Y[val_indices]\n\n            mses.append(fit_ols_and_get_mse(X_train, Y_train, X_val, Y_val))\n        \n        return np.mean(mses)\n\n    results = []\n    \n    # --- Shared Parameters and Seeds ---\n    AR1_SEED = 42\n    CV_RANDOM_SEED = 123\n    \n    # --- Test A: Leaky features + Random CV ---\n    N_A, a_A, sigma_A, W_A, K_A = 1200, 0.8, 1.0, 5, 6\n    y_A = simulate_ar1(N_A, a_A, sigma_A, 0, seed=AR1_SEED)\n    _, X_leak_A, Y_A = create_feature_matrices(y_A, W_A)\n    mse_A = run_random_kfold_cv(X_leak_A, Y_A, K_A, seed=CV_RANDOM_SEED)\n    results.append(mse_A  0.1)\n\n    # --- Test B: Leaky features + Time-aware CV ---\n    N_B, a_B, sigma_B, W_B = 1200, 0.8, 1.0, 5\n    K_B, n_train0_B, n_val_B = 5, 500, 100\n    y_B = simulate_ar1(N_B, a_B, sigma_B, 0, seed=AR1_SEED)\n    _, X_leak_B, Y_B = create_feature_matrices(y_B, W_B)\n    mse_B = run_time_aware_cv(X_leak_B, Y_B, K_B, n_train0_B, n_val_B)\n    results.append(mse_B  0.1)\n\n    # --- Test C: Non-leaky features + Time-aware CV ---\n    N_C, a_C, sigma_C, W_C = 1200, 0.8, 1.0, 5\n    K_C, n_train0_C, n_val_C = 5, 500, 100\n    y_C = simulate_ar1(N_C, a_C, sigma_C, 0, seed=AR1_SEED)\n    X_past_C, _, Y_C = create_feature_matrices(y_C, W_C)\n    mse_C = run_time_aware_cv(X_past_C, Y_C, K_C, n_train0_C, n_val_C)\n    results.append(0.6 = mse_C = 1.4)\n\n    # --- Test D: Catastrophic leakage (W=1) + Random CV ---\n    N_D, a_D, sigma_D, W_D, K_D = 800, 0.8, 1.0, 1, 6\n    y_D = simulate_ar1(N_D, a_D, sigma_D, 0, seed=AR1_SEED)\n    _, X_leak_D, Y_D = create_feature_matrices(y_D, W_D)\n    mse_D = run_random_kfold_cv(X_leak_D, Y_D, K_D, seed=CV_RANDOM_SEED)\n    results.append(mse_D  1e-12)\n\n    # --- Final Output ---\n    # Format: [resultA,resultB,resultC,resultD]\n    print(f\"[{','.join(map(str, results))}]\".lower())\n\nsolve()\n```"
        }
    ]
}