{
    "hands_on_practices": [
        {
            "introduction": "要真正理解自助法（Bootstrap Method）的原理，最好的方法莫过于在一个小规模的数据集上亲手进行计算。这个练习正是为此设计的，它引导你通过一个仅包含三个数据点的极简场景，一步步手动执行自助法来估计线性回归斜率的标准误。通过这个过程，你将清晰地看到自助法如何通过重复抽样来量化统计估计的不确定性，从而牢固掌握其核心机制 ()。",
            "id": "851901",
            "problem": "在统计分析中，非参数配对自助法（non-parametric pairs bootstrap）是一种强大的重抽样技术，用于在统计量的解析分布未知或依赖于强假设时，估计其不确定性。本题探讨了应用自助法来估计回归系数标准误的方法。\n\n考虑一个简单线性回归模型 $Y = \\beta_0 + \\beta_1 X + \\epsilon$，其中斜率参数 $\\beta_1$ 是从一组包含 $n$ 个数据对 $\\{(x_i, y_i)\\}_{i=1}^n$ 的数据集中估计得到的。斜率的普通最小二乘（OLS）估计量由下式给出：\n$$\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n$$\n其中 $\\bar{x}$ 和 $\\bar{y}$ 分别是 $X$ 和 $Y$ 的样本均值。\n\n估计 $\\hat{\\beta}_1$ 标准误的自助法程序如下：\n1.  从原始数据集 $\\mathcal{D}$ 中进行有放回抽样，抽取 $B$ 个自助样本 $\\mathcal{D}^*_1, \\mathcal{D}^*_2, ..., \\mathcal{D}^*_B$，每个样本的大小为 $n$。\n2.  对于每个自助样本 $\\mathcal{D}^*_b$，计算其斜率估计值，记为 $\\hat{\\beta}_{1,b}^*$。\n3.  $\\hat{\\beta}_1$ 的标准误的自助估计值是这 $B$ 个自助斜率估计值的样本标准差：\n    $$\n    \\hat{\\text{se}}_{\\text{boot}}(\\hat{\\beta}_1) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{\\beta}_{1,b}^* - \\bar{\\beta}_1^*)^2}\n    $$\n    其中 $\\bar{\\beta}_1^* = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\beta}_{1,b}^*$ 是自助估计值的均值。\n\n**问题：**\n假设你有一个大小为 $n=3$ 的原始数据集：\n$$\n\\mathcal{D} = \\{ (0,0), (1, \\alpha), (2, \\beta) \\}\n$$\n其中 $\\alpha$ 和 $\\beta$ 是实值参数，并且给定条件 $\\beta \\neq 2\\alpha$，这确保了这三个点不共线。\n\n你进行了一次自助分析，自助样本数量非常小，为 $B=2$。得到的两个自助样本是：\n1.  $\\mathcal{D}^*_1 = \\{ (0,0), (0,0), (1, \\alpha) \\}$\n2.  $\\mathcal{D}^*_2 = \\{ (0,0), (1, \\alpha), (2, \\beta) \\}$ (与原始样本 $\\mathcal{D}$ 相同)\n\n请推导斜率标准误的自助估计值 $\\hat{\\text{se}}_{\\text{boot}}(\\hat{\\beta}_1)$ 的一个闭式表达式，用参数 $\\alpha$ 和 $\\beta$ 来表示。",
            "solution": "1. 相关公式：\n   $\\displaystyle \\hat\\beta_1=\\frac{S_{xy}}{S_{xx}},\\quad S_{xy}=\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y),\\quad S_{xx}=\\sum_{i=1}^n (x_i-\\bar x)^2.$  \n   对于 $B=2$，自助标准误为  \n   $$\n   \\hat{\\mathrm{se}}_{\\mathrm{boot}}(\\hat\\beta_1)\n   =\\sqrt{\\frac{1}{B-1}\\sum_{b=1}^2\\bigl(\\hat\\beta_{1,b}^*-\\bar\\beta^*_1\\bigr)^2}\n   =\\sqrt{\\sum_{b=1}^2\\bigl(\\hat\\beta_{1,b}^*-\\bar\\beta^*_1\\bigr)^2}\\,,\n   $$\n   因为 $B-1=1$。\n\n2. 计算 $\\mathcal D_1^*=\\{(0,0),(0,0),(1,\\alpha)\\}$ 的 $\\hat\\beta_{1,1}^*$。\n   $$\n   \\bar x_1^*=\\frac{0+0+1}{3}=\\frac13,\\quad \\bar y_1^*=\\frac{0+0+\\alpha}{3}=\\frac\\alpha3;\n   $$\n   $$\n   S_{xy}^{(1)}=2\\bigl(-\\tfrac13\\bigr)\\bigl(-\\tfrac\\alpha3\\bigr)+\\bigl(\\tfrac23\\bigr)\\bigl(\\tfrac{2\\alpha}3\\bigr)\n   =\\frac{2\\alpha}{9}+\\frac{4\\alpha}{9}=\\frac{2\\alpha}{3},\\quad\n   S_{xx}^{(1)}=2\\!\\bigl(\\tfrac{1}{3}\\bigr)^2+\\bigl(\\tfrac{2}{3}\\bigr)^2=\\frac{2}{3},\n   $$\n   $$\n   \\hat\\beta_{1,1}^*=\\frac{S_{xy}^{(1)}}{S_{xx}^{(1)}}=\\frac{\\tfrac{2\\alpha}{3}}{\\tfrac{2}{3}}=\\alpha.\n   $$\n\n3. 计算原始样本 $\\{(0,0),(1,\\alpha),(2,\\beta)\\}$ 的 $\\hat\\beta_{1,2}^*$。\n   $$\n   \\bar x=1,\\quad \\bar y=\\frac{\\alpha+\\beta}{3},\\quad\n   S_{xy}=\\bar y+(\\beta-\\bar y)=\\beta,\\quad S_{xx}=1+0+1=2,\n   $$\n   $$\n   \\hat\\beta_{1,2}^*=\\frac{\\beta}{2}.\n   $$\n\n4. 自助斜率的均值：\n   $$\n   \\bar\\beta^*_1=\\frac{\\alpha+\\tfrac{\\beta}{2}}{2}=\\frac{2\\alpha+\\beta}{4}.\n   $$\n   偏差：\n   $$\n   \\hat\\beta_{1,1}^*-\\bar\\beta^*_1\n   =\\alpha-\\frac{2\\alpha+\\beta}{4}\n   =\\frac{2\\alpha-\\beta}{4},\\quad\n   \\hat\\beta_{1,2}^*-\\bar\\beta^*_1\n   =\\frac{\\beta}{2}-\\frac{2\\alpha+\\beta}{4}\n   =-\\frac{2\\alpha-\\beta}{4}.\n   $$\n   偏差平方和：\n   $$\n   \\sum_{b=1}^2\\bigl(\\hat\\beta_{1,b}^*-\\bar\\beta^*_1\\bigr)^2\n   =2\\Bigl(\\frac{2\\alpha-\\beta}{4}\\Bigr)^2\n   =\\frac{(2\\alpha-\\beta)^2}{8}.\n   $$\n\n5. 自助标准误：\n   $$\n   \\hat{\\mathrm{se}}_{\\mathrm{boot}}(\\hat\\beta_1)\n   =\\sqrt{\\frac{(2\\alpha-\\beta)^2}{8}}\n   =\\frac{|2\\alpha-\\beta|}{2\\sqrt2}.\n   $$",
            "answer": "$$\\boxed{\\frac{\\lvert\\beta-2\\alpha\\rvert}{2\\sqrt2}}$$"
        },
        {
            "introduction": "掌握了如何构建一个自助法置信区间之后，一个自然而然的问题是：“这个区间有多可靠？” 该练习通过一个理想化的理论场景来探讨这个问题。它将引导你计算一个自助法置信区间的真实“覆盖概率”——即在多次重复实验中，该方法构建的置信区间能够成功包含真实总体参数的频率。通过解决这个问题，你将深入理解自助法作为一种推断工具的性能及其理论保证，这对于在实践中明智地使用它至关重要 ()。",
            "id": "851841",
            "problem": "自助法（bootstrap）是统计学中一种强大的重抽样方法，用于估计估计量的不确定性。本题在一个简化的情景下，探讨非参数自助置信区间的理论性质。\n\n考虑一个大小为 $n=3$ 的随机样本 $X_1, X_2, X_3$，它们独立同分布地从一个率参数 $\\lambda$ 未知的指数分布中抽取。其概率密度函数为 $f(x; \\lambda) = \\lambda e^{-\\lambda x}$，$x \\ge 0$。我们感兴趣的是估计该分布的中位数，记为 $m$。\n\n该中位数的非参数百分位数自助置信区间按如下方式构建：\n1.  从原始观测样本 $\\{x_1, x_2, x_3\\}$ 中，生成大量的（$B$ 个）“自助样本”。每个自助样本 $\\{x_1^*, x_2^*, x_3^*\\}$ 是一个大小为 $n=3$ 的新样本，通过从原始样本 $\\{x_1, x_2, x_3\\}$ 中有放回地抽样得到。\n2.  对 $B$ 个自助样本中的每一个，计算其样本中位数 $\\hat{m}^*$。三个数的样本中位数是它们排序后的中间值。\n3.  这 $B$ 个自助中位数的集合 $\\{\\hat{m}^*_1, \\dots, \\hat{m}^*_B\\}$ 构成一个经验分布，该分布近似于样本中位数的抽样分布。\n4.  真实中位数 $m$ 的一个近似 95% 置信区间由该经验分布的第 2.5 和第 97.5 百分位数给出。\n\n对于本题，我们考虑自助重抽样次数 $B \\to \\infty$ 的理想化情况。在此极限下，百分位数由自助中位数 $\\hat{m}^*$ 在给定原始样本条件下的精确理论概率质量函数确定。对于一个离散分布，第 $p$ 百分位数定义为满足累积概率 $P(\\hat{m}^* \\le v)$ 至少为 $p$ 的最小值 $v$。\n\n你的任务是推导这个 95% 自助置信区间的精确理论覆盖概率。覆盖概率定义为由样本 $\\{X_1, X_2, X_3\\}$ 构建的随机区间成功包含真实总体中位数 $m$ 的概率。",
            "solution": "1. 从 $\\{x_{(1)},x_{(2)},x_{(3)}\\}$ 中有放回地抽取三次，得到的自助中位数 $\\hat m^*$ 的取值为 $x_{(1)},x_{(2)},x_{(3)}$，其概率如下：\n$$P(\\hat m^*=x_{(1)})=P(N_1\\ge2)=\\sum_{k=2}^3\\binom{3}{k}\\Bigl(\\tfrac13\\Bigr)^k\\Bigl(\\tfrac23\\Bigr)^{3-k}=\\frac7{27},$$\n$$P(\\hat m^*=x_{(3)})=P(N_3\\ge2)=\\frac7{27},\\quad\nP(\\hat m^*=x_{(2)})=1-\\frac{7+7}{27}=\\frac{13}{27}。$$\n2. 第 2.5 百分位数是满足 $P(\\hat m^*\\le v)\\ge0.025$ 的最小 $v$。由于 $P(\\hat m^*\\le x_{(1)})=7/27>0.025$，下界是 $x_{(1)}$。第 97.5 百分位数是满足 $P(\\hat m^*\\le v)\\ge0.975$ 的最小 $v$。由于 $P(\\hat m^*\\le x_{(2)})=20/27 \\approx 0.741  0.975$ 而 $P(\\hat m^*\\le x_{(3)})=1\\ge0.975$，因此上界是 $x_{(3)}$。因此，该区间为 $[X_{(1)},X_{(3)}]$。\n3. 覆盖概率为\n$$P\\bigl(X_{(1)}\\le m\\le X_{(3)}\\bigr) = 1 - P(X_{(3)}  m) - P(m  X_{(1)})$$\n事件 $m  X_{(1)}$ 意味着所有三个观测值都大于总体中位数 $m$。对于连续分布，这种情况发生的概率是 $(1/2)^3 = 1/8$。同样，事件 $X_{(3)}  m$ 意味着所有三个观测值都小于总体中位数，其概率也是 $(1/2)^3 = 1/8$。因此，区间的覆盖概率是 $1 - 1/8 - 1/8 = 3/4$。",
            "answer": "$$\\boxed{\\frac{3}{4}}$$"
        },
        {
            "introduction": "在现代数据分析中，自助法不仅是理论工具，更是强大的计算技术。本练习将理论付诸实践，要求你通过编程来解决一个真实世界中的诊断问题：识别“强影响点”。你将实现一种名为“自助法后刀切法”（Jackknife-after-Bootstrap）的先进技术，用以评估每个数据点对模型估计稳定性的影响。这个动手编码挑战不仅能巩固你对自助法工作流程的理解，还能让你体验到重采样方法在复杂数据诊断任务中的实际应用威力 ()。",
            "id": "3180777",
            "problem": "您会获得三个独立的测试用例，每个测试用例包含一个固定的成对观测数据集 $\\{(x_i,y_i)\\}_{i=1}^n$。对于每个测试用例，您必须使用自助法（bootstrap method）结合自助法后刀切法（jackknife-after-bootstrap）程序来量化每个观测值对所选统计量的自助法分布的影响，然后识别高影响点。所选的统计量是通过普通最小二乘法（OLS）拟合的简单线性回归的斜率参数，该方法使残差平方和最小化。\n\n基本原理：\n- 简单线性回归的普通最小二乘法（OLS）估计旨在最小化 $S(b_0,b_1) = \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2$。一阶最优性条件意味着估计量 $\\hat{b}_1$ 满足 $\\hat{b}_1 = \\dfrac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$，其中 $\\bar{x} = \\dfrac{1}{n}\\sum_{i=1}^n x_i$ 且 $\\bar{y} = \\dfrac{1}{n}\\sum_{i=1}^n y_i$。\n- 自助法通过从 $\\{1,\\dots,n\\}$ 中重复有放回地抽取 $n$ 个索引来形成自助法重抽样样本，并对每个重抽样样本计算统计量，从而近似统计量 $\\theta$ 的抽样分布。设 $B$ 为自助法重复次数。这 $B$ 个自助法统计量的经验分布近似于 $\\theta$ 的真实抽样分布。\n- 自助法后刀切法通过移除每个观测值 $i$ 后重新计算自助法分布来评估其影响，从而为该统计量得出一个新的自助法均值。将此均值与原始自助法均值进行比较，可提供一个影响度量。\n\n具体任务：\n1. 对于每个测试用例，在大小为 $n$ 的每个自助法重抽样样本上使用 $B$ 次重复计算 OLS 斜率统计量，以从完整数据集中获得一组斜率 $\\{\\hat{b}_1^{*(b)}\\}_{b=1}^B$。设 $\\bar{b}_{\\text{full}}$ 为有效自助法斜率的均值，设 $s_{\\text{full}}$ 为它们的样本标准差（自由度为 $1$）。\n2. 对于每个观测索引 $i \\in \\{0,\\dots,n-1\\}$，移除该观测值以形成大小为 $n-1$ 的简化数据集，在该简化数据集上生成 $B$ 次自助法重复（有放回地重抽样 $n-1$ 个点），计算此简化数据集的自助法斜率，并设 $\\bar{b}_{(-i)}$ 为此简化数据集的有效斜率的均值。\n3. 将观测值 $i$ 的影响度量定义为\n$$\nI_i = \n\\begin{cases}\n\\dfrac{\\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|}{s_{\\text{full}}},  \\text{若 } s_{\\text{full}}  0 \\\\\n\\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|,  \\text{若 } s_{\\text{full}} = 0\n\\end{cases}\n$$\n如果 $I_i  \\tau$，则将观测值 $i$ 分类为高影响点，其中 $\\tau$ 是给定的阈值。\n4. 您必须遵守的实现细节：\n   - 如果一个自助法重抽样样本的 $x$ 方差为零（即，对于该重抽样样本，$\\sum_{j} (x_j - \\bar{x})^2 = 0$），则 OLS 斜率未定义；您必须丢弃该次重复，并且不将其包含在 $\\bar{b}$ 或 $s$ 的计算中。\n   - 如果对于给定的数据集，完全没有有效的自助法重复（对于所提供的测试用例，这种情况极不可能发生），则对所有 $i$ 定义 $I_i = 0$。\n   - 全程对观测值使用基于 $0$ 的索引。\n   - 输出中不要使用任何物理单位；所有结果均为无单位的实数或整数。\n\n您必须将上述方法应用于以下测试套件：\n\n- 测试用例 1：\n  - 数据：$x = (0, 1, 2, 3, 10)$，$y = (0.5, 2.1, 3.9, 6.2, 50.0)$\n  - 重复次数：$B = 1200$\n  - 阈值：$\\tau = 2.0$\n- 测试用例 2：\n  - 数据：$x = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)$，$y = (0.1, 3.2, 6.1, 9.0, 12.0, 15.1, 18.2, 21.3, 24.1, 27.0)$\n  - 重复次数：$B = 1200$\n  - 阈值：$\\tau = 2.0$\n- 测试用例 3：\n  - 数据：$x = (0, 1, 1, 2, 2, 100)$，$y = (0.1, 0.7, 0.6, 1.0, 0.9, 55.0)$\n  - 重复次数：$B = 1200$\n  - 阈值：$\\tau = 1.5$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。每个元素对应一个测试用例，并且必须是一个整数列表，包含该用例中高影响观测值的基于 $0$ 的索引，按升序排列。例如，最终输出格式必须类似于 $[[i\\_1,i\\_2],[j\\_1,j\\_2,j\\_3],[]]$，其中空方括号表示该用例没有高影响观测值。",
            "solution": "用户的请求是一个定义明确的计算统计学问题。问题陈述的验证如下。\n\n**问题验证**\n\n1.  **提取给定条件**：\n    -   **目标统计量**：简单线性回归的普通最小二乘法 (OLS) 斜率参数，$\\hat{b}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$。\n    -   **核心方法**：自助法（bootstrap method），涉及通过从原始数据集中有放回地抽样，生成 $B$ 个大小为 $n$ 的重抽样样本。\n    -   **影响评估程序（自助法后刀切法）**：\n        1.  在完整数据集上，通过 $B$ 次自助法斜率重复，计算均值 $\\bar{b}_{\\text{full}}$ 和样本标准差 $s_{\\text{full}}$（方差使用 $N-1$ 自由度）。\n        2.  对于每个观测值 $i \\in \\{0, \\dots, n-1\\}$，移除第 $i$ 个点以创建一个简化数据集。在此简化数据集上，通过 $B$ 次新的自助法重复计算均值 $\\bar{b}_{(-i)}$。\n    -   **影响度量 ($I_i$)**：如果 $s_{\\text{full}}  0$，则 $I_i = \\frac{|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}|}{s_{\\text{full}}}$；如果 $s_{\\text{full}} = 0$，则 $I_i = |\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}|$。\n    -   **高影响标准**：如果 $I_i  \\tau$，则观测值 $i$ 被分类为高影响点。\n    -   **约束条件**：\n        -   自变量 ($x$) 方差为零的自助法重复将被丢弃。\n        -   如果一个数据集（完整的或简化的）没有产生任何有效的自助法重复，则相应的影​​响 $I_i$ 被视为 $0$。\n        -   需要使用基于 $0$ 的索引。\n    -   **数据套件**：提供了三个测试用例，每个用例都包含一个数据集 $(x, y)$、重复次数 $B$ 和阈值 $\\tau$。\n\n2.  **使用提取的给定条件进行验证**：\n    -   **科学依据**：该问题牢固地植根于基础统计理论。OLS 估计量是回归分析的基石。自助法是用于估计抽样分布的经典非参数技术。所描述的自助法后刀切法是诊断影响数据点的一种有效（尽管计算密集）的方法。所有定义和公式都是正确的。该问题在科学上是合理的。\n    -   **良态性与客观性**：该问题是良态的，提供了一套完整且明确的指令、数据和边缘情况处理规则。这确保了可以通过算法推导出一个唯一的、确定性的解决方案。使用定量阈值 $\\tau$ 使分类变得客观。\n\n3.  **结论**：该问题是有效的，可以按陈述求解。\n\n**基于原理的解决方案设计**\n\n该任务要求为每个测试用例实现一个多阶段的统计分析。通过将程序分解为模块化的、可重用的函数来设计解决方案，每个函数都基于特定的统计原理。\n\n1.  **OLS 斜率计算**：\n    核心统计量是 OLS 斜率参数 $\\hat{b}_1$。对于一组给定的成对观测值 $\\{(x_j, y_j)\\}_{j=1}^m$，估计量 $\\hat{b}_1$ 使用既定公式计算：\n    $$\n    \\hat{b}_1 = \\frac{\\sum_{j=1}^m (x_j - \\bar{x})(y_j - \\bar{y})}{\\sum_{j=1}^m (x_j - \\bar{x})^2}\n    $$\n    其中 $\\bar{x} = \\frac{1}{m}\\sum_{j=1}^m x_j$ 且 $\\bar{y} = \\frac{1}{m}\\sum_{j=1}^m y_j$。一个关键的实现细节是处理分母为零的情况，这种情况发生在样本中所有 $x_j$ 都相同时。在这种情况下，斜率未定义，必须按照问题陈述丢弃该次重复。\n\n2.  **Bootstrap 斜率分布生成**：\n    自助法提供了统计量抽样分布的非参数估计。这是通过从经验分布（即原始样本本身）中抽取数据来模拟抽样过程实现的。对于一个大小为 $m$ 的给定数据集，这包括：\n    -   重复 $B$ 次：\n        1.  从 $\\{0, 1, \\dots, m-1\\}$ 中有放回地抽取 $m$ 个索引。\n        2.  使用这些索引处的数据构建一个自助法重抽样样本。\n        3.  在此重抽样样本上计算 OLS 斜率 $\\hat{b}_1^*$。\n    -   收集所有有效的斜率 $\\{\\hat{b}_1^{*(b)}\\}_{b=1}^{B'}$，其中 $B' \\le B$。\n    此程序将应用于完整数据集（大小为 $n$）和 $n$ 个简化数据集（大小为 $n-1$）中的每一个。\n\n3.  **主程序：自助法后刀切法分析**：\n    主逻辑为单个测试用例协调整个分析过程。\n    -   **步骤 A：完整数据分析**。首先，我们将自助法程序应用于大小为 $n$ 的完整数据集，以生成斜率分布。从此分布中，我们计算其均值 $\\bar{b}_{\\text{full}}$ 和样本标准差 $s_{\\text{full}}$。标准差在分母中使用 $N-1$（贝塞尔校正）进行计算，这在 NumPy 等数值库中对应于 `ddof=1`。这为我们的影响比较建立了基线。\n    -   **步骤 B：留一法分析**。然后，我们遍历从 $0$ 到 $n-1$ 的每个观测值 $i$。在每次迭代中，我们通过从数据集中移除观测值 $i$ 来以编程方式执行刀切法删除。\n    -   **步骤 C：在简化数据上进行自助法**。对于这 $n$ 个简化数据集（大小为 $n-1$）中的每一个，我们都执行一个包含 $B$ 次重复的完整自助法程序。然后，我们计算所得斜率分布的均值，记为 $\\bar{b}_{(-i)}$。该值表示在数据生成过程中缺少观测值 $i$ 时的典型斜率估计。\n    -   **步骤 D：影响计算**。观测值 $i$ 的影响 $I_i$ 量化了因其移除而引起的平均自助法斜率的变化，并用原始自助法斜率的变异性进行标准化。按规定应用公式：\n    $$\n    I_i = \n    \\begin{cases}\n    \\dfrac{\\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|}{s_{\\text{full}}}  \\text{若 } s_{\\text{full}}  0 \\\\\n    \\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|  \\text{若 } s_{\\text{full}} = 0\n    \\end{cases}\n    $$\n    -   **步骤 E：识别**。最后，我们将每个影响度量 $I_i$ 与提供的阈值 $\\tau$ 进行比较。如果 $I_i  \\tau$，则将基于 $0$ 的索引 $i$ 记录为高影响点。然后将为该测试用例收集的索引按升序排序。\n\n这种结构化方法确保精确满足问题的所有规范，同时在算法与基础统计原理之间保持清晰的联系。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It implements the jackknife-after-bootstrap procedure to identify high-influence points.\n    \"\"\"\n\n    def ols_slope(x, y):\n        \"\"\"\n        Calculates the OLS slope for a simple linear regression.\n\n        Args:\n            x (np.ndarray): The independent variable data.\n            y (np.ndarray): The dependent variable data.\n\n        Returns:\n            float: The OLS slope, or np.nan if the slope is undefined.\n        \"\"\"\n        n = len(x)\n        if n  2:\n            return np.nan\n\n        x_mean = np.mean(x)\n        \n        # Denominator of the slope formula\n        ss_xx = np.sum((x - x_mean)**2)\n\n        # As per problem, if variance in x is zero, the slope is undefined.\n        if ss_xx == 0:\n            return np.nan\n\n        y_mean = np.mean(y)\n        # Numerator of the slope formula\n        ss_xy = np.sum((x - x_mean) * (y - y_mean))\n        \n        return ss_xy / ss_xx\n\n    def get_bootstrap_slopes(x, y, B):\n        \"\"\"\n        Generates a distribution of OLS slopes using the bootstrap method.\n\n        Args:\n            x (np.ndarray): The independent variable data.\n            y (np.ndarray): The dependent variable data.\n            B (int): The number of bootstrap replicates.\n\n        Returns:\n            np.ndarray: An array of valid bootstrap slopes.\n        \"\"\"\n        n = len(x)\n        if n == 0:\n            return np.array([])\n        \n        slopes = []\n        # Pre-generate all random indices for performance\n        # Each row is a set of indices for one bootstrap replicate\n        all_indices = np.random.choice(n, size=(B, n), replace=True)\n\n        for resample_indices in all_indices:\n            x_resample = x[resample_indices]\n            y_resample = y[resample_indices]\n            \n            slope = ols_slope(x_resample, y_resample)\n            \n            # Discard replicates where the slope is undefined\n            if not np.isnan(slope):\n                slopes.append(slope)\n                \n        return np.array(slopes)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        (np.array([0, 1, 2, 3, 10]), np.array([0.5, 2.1, 3.9, 6.2, 50.0]), 1200, 2.0),\n        # Test Case 2\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), np.array([0.1, 3.2, 6.1, 9.0, 12.0, 15.1, 18.2, 21.3, 24.1, 27.0]), 1200, 2.0),\n        # Test Case 3\n        (np.array([0, 1, 1, 2, 2, 100]), np.array([0.1, 0.7, 0.6, 1.0, 0.9, 55.0]), 1200, 1.5)\n    ]\n\n    all_results = []\n    for x_full, y_full, B, tau in test_cases:\n        n = len(x_full)\n        \n        # 1. Compute bootstrap statistics for the full dataset.\n        full_slopes = get_bootstrap_slopes(x_full, y_full, B)\n        \n        # Per problem: if no valid replicates, I_i = 0 for all i.\n        # This means no high-influence points.\n        if len(full_slopes) == 0:\n            all_results.append([])\n            continue\n\n        b_full_mean = np.mean(full_slopes)\n        \n        # Per problem: sample standard deviation (ddof=1).\n        # If less than 2 valid slopes, standard deviation is 0.\n        b_full_std = 0.0\n        if len(full_slopes) >= 2:\n            b_full_std = np.std(full_slopes, ddof=1)\n            \n        high_influence_indices = []\n        # 2. Loop through each observation for jackknife-after-bootstrap.\n        for i in range(n):\n            # Create the reduced dataset by removing observation i.\n            x_reduced = np.delete(x_full, i)\n            y_reduced = np.delete(y_full, i)\n            \n            # Compute bootstrap mean for the reduced dataset.\n            reduced_slopes = get_bootstrap_slopes(x_reduced, y_reduced, B)\n            \n            # Per problem: if no valid replicates, I_i = 0.\n            if len(reduced_slopes) == 0:\n                influence_i = 0.0\n            else:\n                b_reduced_mean = np.mean(reduced_slopes)\n                \n                # 3. Define the influence measure for observation i.\n                diff = np.abs(b_reduced_mean - b_full_mean)\n                if b_full_std > 0:\n                    influence_i = diff / b_full_std\n                else:\n                    influence_i = diff\n            \n            # 4. Classify observation i as high-influence.\n            if influence_i > tau:\n                high_influence_indices.append(i)\n        \n        # Results must be in ascending order.\n        all_results.append(sorted(high_influence_indices))\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}