{
    "hands_on_practices": [
        {
            "introduction": "To effectively address class imbalance, we must first understand its fundamental impact on a statistical model's decision-making process. This exercise provides a foundational look at how a classifier's optimal decision rule is inherently affected by the prior probabilities of the classes. By working through the mathematics of Linear Discriminant Analysis (LDA), you will derive precisely how an imbalance in class priors shifts the decision boundary, offering a clear, quantitative explanation for why models tend to favor the majority class . This practice is crucial for building the intuition that underlies all corrective measures for imbalance.",
            "id": "3127149",
            "problem": "Consider a two-class classification task modeled by Linear Discriminant Analysis (LDA), where class-conditional feature distributions are one-dimensional Gaussian and share a common variance. Specifically, suppose that for class label $y \\in \\{0,1\\}$, the feature $x \\in \\mathbb{R}$ is distributed as\n$$\nx \\mid y=k \\sim \\mathcal{N}(\\mu_k,\\sigma^2), \\quad k \\in \\{0,1\\},\n$$\nwith parameters $\\mu_0 = 0.5$, $\\mu_1 = 2.3$, and $\\sigma^2 = 1.44$. The prior probabilities are imbalanced: $\\pi_0 = 0.7$ and $\\pi_1 = 0.3$.\n\nStarting only from (i) Bayes’ decision rule “assign to the class with larger posterior probability,” (ii) Bayes’ theorem relating posteriors to priors and likelihoods, and (iii) the Gaussian probability density function, derive the location $x^{\\star}_{\\text{unequal}}$ of the LDA decision boundary under these unequal priors by solving for the point where the posteriors are equal. Then, derive the location $x^{\\star}_{\\text{equal}}$ of the decision boundary in the equal-prior case with $\\pi_0=\\pi_1=\\tfrac{1}{2}$ using the same principles.\n\nCompute the shift in the decision boundary due to class imbalance,\n$$\n\\Delta x \\equiv x^{\\star}_{\\text{unequal}} - x^{\\star}_{\\text{equal}}.\n$$\nRound your numeric answer for $\\Delta x$ to four significant figures.",
            "solution": "We begin with Bayes’ decision rule: assign an observation $x$ to the class with the larger posterior probability. The decision boundary is the set of $x$ for which the posteriors are equal:\n$$\np(y=1 \\mid x) = p(y=0 \\mid x).\n$$\nUsing Bayes’ theorem and the fact that the class-conditional densities share the same support, this boundary satisfies\n$$\np(x \\mid y=1)\\,\\pi_1 = p(x \\mid y=0)\\,\\pi_0.\n$$\nThe Gaussian probability density function for class $k \\in \\{0,1\\}$ is\n$$\np(x \\mid y=k) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(x-\\mu_k)^2}{2\\sigma^2}\\right).\n$$\nSubstituting these into the equality and canceling the common factor $\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}$ yields\n$$\n\\exp\\!\\left(-\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right)\\pi_1\n=\n\\exp\\!\\left(-\\frac{(x-\\mu_0)^2}{2\\sigma^2}\\right)\\pi_0.\n$$\nTaking natural logarithms of both sides gives\n$$\n-\\frac{(x-\\mu_1)^2}{2\\sigma^2} + \\ln \\pi_1\n=\n-\\frac{(x-\\mu_0)^2}{2\\sigma^2} + \\ln \\pi_0.\n$$\nRearranging,\n$$\n\\frac{(x-\\mu_0)^2 - (x-\\mu_1)^2}{2\\sigma^2}\n=\n\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right).\n$$\nExpanding the squares,\n$$\n(x^2 - 2x\\mu_0 + \\mu_0^2) - (x^2 - 2x\\mu_1 + \\mu_1^2)\n=\n2x(\\mu_1 - \\mu_0) + (\\mu_0^2 - \\mu_1^2).\n$$\nThus,\n$$\n\\frac{2x(\\mu_1 - \\mu_0) + (\\mu_0^2 - \\mu_1^2)}{2\\sigma^2}\n=\n\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right),\n$$\nso\n$$\n2x(\\mu_1 - \\mu_0) + (\\mu_0^2 - \\mu_1^2) = 2\\sigma^2 \\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right).\n$$\nSolving for $x$,\n$$\nx\n=\n\\frac{2\\sigma^2 \\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right) - (\\mu_0^2 - \\mu_1^2)}{2(\\mu_1 - \\mu_0)}\n=\n\\frac{\\mu_0 + \\mu_1}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0}\\,\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right).\n$$\nThis is the LDA decision boundary for unequal priors:\n$$\nx^{\\star}_{\\text{unequal}} = \\frac{\\mu_0 + \\mu_1}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0}\\,\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right).\n$$\nFor equal priors $\\pi_0=\\pi_1=\\tfrac{1}{2}$, we have $\\ln(\\pi_0/\\pi_1)=\\ln 1 = 0$, so the decision boundary simplifies to the midpoint:\n$$\nx^{\\star}_{\\text{equal}} = \\frac{\\mu_0 + \\mu_1}{2}.\n$$\nTherefore, the shift induced by unequal priors is\n$$\n\\Delta x \\equiv x^{\\star}_{\\text{unequal}} - x^{\\star}_{\\text{equal}}\n=\n\\frac{\\sigma^2}{\\mu_1 - \\mu_0}\\,\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right).\n$$\nNow substitute the given numerical values $\\mu_0 = 0.5$, $\\mu_1 = 2.3$, $\\sigma^2 = 1.44$, $\\pi_0 = 0.7$, and $\\pi_1 = 0.3$:\n$$\n\\mu_1 - \\mu_0 = 2.3 - 0.5 = 1.8,\n\\quad\n\\frac{\\sigma^2}{\\mu_1 - \\mu_0} = \\frac{1.44}{1.8} = 0.8,\n\\quad\n\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right) = \\ln\\!\\left(\\frac{0.7}{0.3}\\right) = \\ln\\!\\left(\\frac{7}{3}\\right).\n$$\nCompute the logarithm:\n$$\n\\ln\\!\\left(\\frac{7}{3}\\right) \\approx 0.847297860.\n$$\nTherefore,\n$$\n\\Delta x \\approx 0.8 \\times 0.847297860 \\approx 0.677838288.\n$$\nRounding to four significant figures yields\n$$\n\\Delta x \\approx 0.6778.\n$$\nThis positive shift indicates that, because $\\pi_0 > \\pi_1$, the decision boundary moves to the right relative to the equal-prior midpoint, enlarging the region predicted as class $0$. This is the manifestation of Linear Discriminant Analysis (LDA)’s log prior term handling class imbalance by offsetting the boundary toward the minority class side.",
            "answer": "$$\\boxed{0.6778}$$"
        },
        {
            "introduction": "Once we understand that class imbalance biases the decision boundary, the next step is to explore strategies to counteract this effect. This problem presents a direct comparison between two powerful families of techniques: altering the data during training versus adjusting the model's predictions. Using the transparent structure of a decision tree, you will analyze the difference between applying class weights during training and simply shifting the prediction threshold after training . This thought-provoking exercise reveals that while both methods can improve performance, only training-time interventions can change the model's structure to uncover rare patterns, highlighting a critical trade-off in strategy selection.",
            "id": "3112943",
            "problem": "Consider a binary classification task with classes $y \\in \\{0,1\\}$, where the training data are highly imbalanced. A decision tree is trained at the root with a single candidate split on a binary feature $X$ that partitions the training set into two regions $A$ and $B$. The training set has $190$ instances of class $0$ and $10$ instances of class $1$. Region $A$ contains $160$ instances of class $0$ and $2$ instances of class $1$, while region $B$ contains $30$ instances of class $0$ and $8$ instances of class $1$. The tree uses a standard stopping rule at each node: do not split unless the impurity decrease exceeds a minimum threshold $\\Delta I_{\\min} = 0.02$.\n\nTwo handling strategies for imbalance are considered:\n\n- Strategy $S_{\\text{train}}$: train the tree with class weights $w_0 = 1$ for class $0$ and $w_1 = 10$ for class $1$, applied in the splitting criterion and node weighting during training.\n- Strategy $S_{\\text{pred}}$: train the tree without class weights, but at prediction time use a probability threshold $t \\in (0,1)$ so that the classifier predicts class $1$ whenever the estimated posterior probability $\\hat{p}(y=1 \\mid x)$ exceeds $t$, else predicts class $0$.\n\nBased on first principles of risk minimization in classification, and the mechanics of decision tree splitting and prediction, which of the following statements are true?\n\nA. When the unweighted tree would not split at the root because the impurity decrease is less than $\\Delta I_{\\min}$, but the weighted tree would split because the minority class is emphasized, $S_{\\text{train}}$ can isolate a minority region and improve recall, while $S_{\\text{pred}}$ cannot create new partitions and therefore cannot recover that structure; in this case, $S_{\\text{train}}$ is preferable.\n\nB. If the tree’s leaf posterior estimates $\\hat{p}(y=1 \\mid x)$ are well calibrated, then $S_{\\text{pred}}$ and $S_{\\text{train}}$ are always exactly equivalent in their final classifications; adjusting $t$ after training can reproduce any effect of class weighting during training.\n\nC. If the deployment scenario changes misclassification costs or class priors relative to training, and the tree’s leaf posterior estimates $\\hat{p}(y=1 \\mid x)$ remain well calibrated for the deployment distribution, then adjusting $t$ at prediction time implements the Bayes decision rule without retraining; in this case $S_{\\text{pred}}$ is sufficient, and retraining with class weights is unnecessary.\n\nD. For decision trees using the Gini impurity, class weighting during training is mathematically equivalent to shifting the probability threshold $t$ at prediction time, regardless of calibration, stopping rules, or tree depth.\n\nE. Threshold adjustment at prediction time is preferable when the minority class is concentrated in a small, well-defined region of feature space that requires deeper splits to isolate, because $S_{\\text{pred}}$ can classify those cases correctly even if the tree never splits to isolate that region.",
            "solution": "The problem asks for an evaluation of statements comparing two strategies for handling class imbalance in a binary classification task: $S_{\\text{train}}$, which uses class weights during training, and $S_{\\text{pred}}$, which adjusts the prediction threshold after training an unweighted tree.\n\nFirst, the validity of the problem statement is confirmed. All given numerical data is self-consistent and the scenario is a standard problem in statistical learning.\n- Training set: $N=200$ instances. Class $0$: $N_0=190$. Class $1$: $N_1=10$.\n- Root node (let's call it $R$): $N_R=200$, with $N_{R,0}=190$ and $N_{R,1}=10$.\n- Candidate split partitions $R$ into:\n  - Region $A$: $N_A=162$, with $N_{A,0}=160$ and $N_{A,1}=2$.\n  - Region $B$: $N_B=38$, with $N_{B,0}=30$ and $N_{B,1}=8$.\n- Stopping rule threshold: $\\Delta I_{\\min} = 0.02$.\n- Strategy $S_{\\text{train}}$ weights: $w_0=1$ for class $0$, $w_1=10$ for class $1$.\n\nTo evaluate the specific scenario, we must calculate the impurity decrease for both the unweighted and weighted cases. We will use the Gini impurity, a standard metric for decision trees. The Gini impurity for a node $m$ with class proportions $p_{m,k}$ is $I_G(m) = 1 - \\sum_k p_{m,k}^2$. The impurity decrease for a split is $\\Delta I = I(\\text{parent}) - \\sum_{j \\in \\text{children}} \\frac{N_j}{N_{\\text{parent}}} I(\\text{child}_j)$.\n\n**Analysis of the Unweighted Case ($S_{\\text{pred}}$)**\n\nThe prediction threshold adjustment of $S_{\\text{pred}}$ is applied to the tree trained without weights. We must first determine if this tree will split at the root.\n\n1.  **Gini Impurity of the Root Node ($R$)**:\n    The proportions are $p_{R,0} = \\frac{190}{200} = 0.95$ and $p_{R,1} = \\frac{10}{200} = 0.05$.\n    $$I(R) = 1 - (0.95^2 + 0.05^2) = 1 - (0.9025 + 0.0025) = 0.095$$\n\n2.  **Gini Impurity of Child Nodes ($A$ and $B$)**:\n    - For node $A$, proportions are $p_{A,0} = \\frac{160}{162}$ and $p_{A,1} = \\frac{2}{162}$.\n      $$I(A) = 1 - \\left( \\left(\\frac{160}{162}\\right)^2 + \\left(\\frac{2}{162}\\right)^2 \\right) = 1 - \\frac{160^2 + 2^2}{162^2} = 1 - \\frac{25604}{26244} \\approx 0.0244$$\n    - For node $B$, proportions are $p_{B,0} = \\frac{30}{38}$ and $p_{B,1} = \\frac{8}{38}$.\n      $$I(B) = 1 - \\left( \\left(\\frac{30}{38}\\right)^2 + \\left(\\frac{8}{38}\\right)^2 \\right) = 1 - \\frac{30^2 + 8^2}{38^2} = 1 - \\frac{964}{1444} \\approx 0.3324$$\n\n3.  **Impurity Decrease ($\\Delta I$)**:\n    The impurity decrease is the weighted average impurity of the children subtracted from the parent's impurity.\n    $$\\Delta I = I(R) - \\left( \\frac{N_A}{N_R} I(A) + \\frac{N_B}{N_R} I(B) \\right)$$\n    $$\\Delta I \\approx 0.095 - \\left( \\frac{162}{200} \\times 0.0244 + \\frac{38}{200} \\times 0.3324 \\right)$$\n    $$\\Delta I \\approx 0.095 - (0.81 \\times 0.0244 + 0.19 \\times 0.3324) \\approx 0.095 - (0.01976 + 0.06316) \\approx 0.095 - 0.08292 = 0.01208$$\n\n4.  **Conclusion on Splitting**:\n    Since $\\Delta I \\approx 0.0121 < \\Delta I_{\\min} = 0.02$, the tree trained without weights will **not** split at the root. It will remain a single leaf node.\n\n**Analysis of the Weighted Case ($S_{\\text{train}}$)**\n\nClass weights modify the impurity calculation. The contribution of each class $k$ is multiplied by its weight $w_k$. In the Gini impurity, this is equivalent to using weighted proportions. The weighted proportion of class $k$ in node $m$ is $p'_{m,k} = \\frac{w_k N_{m,k}}{\\sum_j w_j N_{m,j}}$. Let $W_m = \\sum_j w_j N_{m,j}$ be the total weight in node $m$.\n\n1.  **Weighted Gini Impurity of the Root Node ($R$)**:\n    The total weight is $W_R = (w_0 \\times 190) + (w_1 \\times 10) = (1 \\times 190) + (10 \\times 10) = 290$.\n    The weighted proportions are $p'_{R,0} = \\frac{190}{290}$ and $p'_{R,1} = \\frac{100}{290}$.\n    $$I'(R) = 1 - \\left( \\left(\\frac{190}{290}\\right)^2 + \\left(\\frac{100}{290}\\right)^2 \\right) = 1 - \\frac{19^2 + 10^2}{29^2} = 1 - \\frac{361+100}{841} = 1 - \\frac{461}{841} \\approx 0.4518$$\n\n2.  **Weighted Gini Impurity of Child Nodes ($A$ and $B$)**:\n    - For node $A$, $W_A = (1 \\times 160) + (10 \\times 2) = 180$.\n      $$I'(A) = 1 - \\left( \\left(\\frac{160}{180}\\right)^2 + \\left(\\frac{20}{180}\\right)^2 \\right) = 1 - \\left( \\left(\\frac{8}{9}\\right)^2 + \\left(\\frac{1}{9}\\right)^2 \\right) = 1 - \\frac{64+1}{81} = \\frac{16}{81} \\approx 0.1975$$\n    - For node $B$, $W_B = (1 \\times 30) + (10 \\times 8) = 110$.\n      $$I'(B) = 1 - \\left( \\left(\\frac{30}{110}\\right)^2 + \\left(\\frac{80}{110}\\right)^2 \\right) = 1 - \\left( \\left(\\frac{3}{11}\\right)^2 + \\left(\\frac{8}{11}\\right)^2 \\right) = 1 - \\frac{9+64}{121} = \\frac{48}{121} \\approx 0.3967$$\n\n3.  **Weighted Impurity Decrease ($\\Delta I'$)**:\n    The child impurities are weighted by their share of the total weight.\n    $$\\Delta I' = I'(R) - \\left( \\frac{W_A}{W_R} I'(A) + \\frac{W_B}{W_R} I'(B) \\right)$$\n    $$\\Delta I' \\approx 0.4518 - \\left( \\frac{180}{290} \\times 0.1975 + \\frac{110}{290} \\times 0.3967 \\right)$$\n    $$\\Delta I' \\approx 0.4518 - (0.6207 \\times 0.1975 + 0.3793 \\times 0.3967) \\approx 0.4518 - (0.1226 + 0.1505) \\approx 0.4518 - 0.2731 = 0.1787$$\n\n4.  **Conclusion on Splitting**:\n    Since $\\Delta I' \\approx 0.1787 > \\Delta I_{\\min} = 0.02$, the tree trained with weights will **split** at the root.\n\nNow we evaluate each statement.\n\n**A. When the unweighted tree would not split at the root because the impurity decrease is less than $\\Delta I_{\\min}$, but the weighted tree would split because the minority class is emphasized, $S_{\\text{train}}$ can isolate a minority region and improve recall, while $S_{\\text{pred}}$ cannot create new partitions and therefore cannot recover that structure; in this case, $S_{\\text{train}}$ is preferable.**\nOur calculations have confirmed the premise of this statement: the unweighted tree does not split, but the weighted tree does. The $S_{\\text{pred}}$ strategy uses the unweighted tree, which is just a single node. It cannot distinguish between instances that would fall in region $A$ versus region $B$. All instances receive the same posterior estimate, $\\hat{p}(y=1|x) = 10/200 = 0.05$. Varying the threshold $t$ can only classify all instances as $0$ or all as $1$. It cannot achieve any separation. In contrast, $S_{\\text{train}}$ creates a partition. Leaf node $B$ is now relatively pure in the minority class (from a weighted perspective) and contains $8$ of the $10$ total class $1$ instances. This allows the model to identify a sub-population with a high likelihood of being class $1$, dramatically improving recall for the minority class. Therefore, the statement is a correct description of the dynamic between the two strategies in this exact scenario.\n**Verdict: Correct**\n\n**B. If the tree’s leaf posterior estimates $\\hat{p}(y=1 \\mid x)$ are well calibrated, then $S_{\\text{pred}}$ and $S_{\\text{train}}$ are always exactly equivalent in their final classifications; adjusting $t$ after training can reproduce any effect of class weighting during training.**\nThis statement is false. The core difference between the two strategies is that $S_{\\text{train}}$ (weighting) can alter the *structure* of the tree itself—the sequence of splits and the final partitions of the feature space. $S_{\\text{pred}}$ (thresholding) operates on a fixed tree structure and only changes the decision rule applied to the posteriors of the existing leaves. As demonstrated in our analysis of statement A, weighting can cause a split to occur where it otherwise wouldn't, leading to a fundamentally different set of leaf nodes and posteriors. Since the two strategies can produce different tree structures, they cannot be \"always exactly equivalent\".\n**Verdict: Incorrect**\n\n**C. If the deployment scenario changes misclassification costs or class priors relative to training, and the tree’s leaf posterior estimates $\\hat{p}(y=1 \\mid x)$ remain well calibrated for the deployment distribution, then adjusting $t$ at prediction time implements the Bayes decision rule without retraining; in this case $S_{\\text{pred}}$ is sufficient, and retraining with class weights is unnecessary.**\nThis statement is a correct application of Bayesian decision theory. The Bayes optimal decision rule is to predict the class that minimizes the expected loss. This decision can be expressed as comparing the posterior probability $p(y=1|x)$ to a threshold $t^*$ that is a function of the misclassification costs. The rule is: predict $1$ if $p(y=1|x) > t^*$. If class priors change from training to deployment, the posterior probabilities can also be updated (a process sometimes called prior correction), which is also equivalent to shifting the decision threshold on the original posteriors. Therefore, if a model produces well-calibrated posteriors, it is not necessary to retrain it to adapt to new costs or priors; one can simply calculate the new optimal threshold $t^*$ and apply it at prediction time. This is precisely the mechanism of the $S_{\\text{pred}}$ strategy. The key assumption is that the trained tree is \"good enough\" at capturing the conditional likelihoods $p(x|y)$, which is implied by the premise that the posteriors \"remain well calibrated\".\n**Verdict: Correct**\n\n**D. For decision trees using the Gini impurity, class weighting during training is mathematically equivalent to shifting the probability threshold $t$ at prediction time, regardless of calibration, stopping rules, or tree depth.**\nThis is an over-generalized and false statement. As shown in the analysis for A, class weighting affects the impurity calculation for each potential split, which in turn influences which feature is chosen for splitting and whether the stopping criteria are met. This fundamentally changes the tree's construction. Shifting the prediction threshold has no impact on tree construction. Because they affect different parts of the modeling process (training vs. prediction) and can lead to different tree structures, they are not mathematically equivalent. The phrase \"regardless of... stopping rules\" is directly contradicted by our calculations.\n**Verdict: Incorrect**\n\n**E. Threshold adjustment at prediction time is preferable when the minority class is concentrated in a small, well-defined region of feature space that requires deeper splits to isolate, because $S_{\\text{pred}}$ can classify those cases correctly even if the tree never splits to isolate that region.**\nThis statement contains a logical contradiction. If a tree \"never splits to isolate that region,\" then all instances in that small region will be grouped into a larger, more heterogeneous leaf node. The posterior probability for that leaf, $\\hat{p}(y=1|x)$, will be diluted by the majority class instances in the leaf. The $S_{\\text{pred}}$ strategy can only apply a threshold to this diluted posterior. It has no access to the fine-grained information within the leaf that the tree failed to partition. Therefore, it *cannot* classify those cases correctly. This scenario is, in fact, an argument *for* using a method like $S_{\\text{train}}$, which might force the tree to perform the necessary deeper splits to find that small, concentrated region. The statement's reasoning is flawed.\n**Verdict: Incorrect**\n\nThe true statements are A and C.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "Building on the concept of data-level interventions, this hands-on coding practice guides you through implementing a sophisticated undersampling technique. Simple random undersampling can be risky, as it might discard vital information near the decision boundary. This exercise introduces a more intelligent approach: density-aware undersampling, which selectively removes majority class instances that are redundant—those located in dense regions far from any minority class samples . By implementing this algorithm, you will gain practical experience with an advanced resampling method and empirically measure how it helps a classifier focus on the most informative data, thereby improving its ability to identify the rare but important minority class.",
            "id": "3127121",
            "problem": "You are given a binary classification task in $\\mathbb{R}^2$ with severe class imbalance. Let the majority class be labeled $0$ and the minority class be labeled $1$. The empirical training distribution is represented by samples $\\{(x_i, y_i)\\}_{i=1}^n$ with $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\{0,1\\}$. You will implement density-aware undersampling that removes redundant majority points in dense interior regions while preserving points near the decision boundary. You will then measure the effect of this undersampling on minority recall, defined as the true positive rate.\n\nFundamental basis and definitions:\n- The Bayes decision rule classifies a point $x$ to class $1$ if $P(Y=1 \\mid X=x) > P(Y=0 \\mid X=x)$, otherwise to class $0$. In practice, this rule is approximated by learned classifiers using empirical data.\n- The $k$-Nearest Neighbors (k-NN) classifier approximates the Bayes decision rule by majority vote among the $k$ nearest training samples in feature space.\n- Class imbalance skews empirical estimates of $P(Y \\mid X)$, biasing classifiers toward the majority class, which commonly reduces minority recall.\n- Minority recall is defined as $R = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$, expressed as a decimal in $[0,1]$.\n\nYou must design and implement an algorithm that removes majority points in regions of high majority density that are far from minority points, so as to preserve the empirical decision boundary. The design must begin with well-tested formulas:\n- Local density estimation via $k$-nearest neighbor distances. For each majority point $x$, define the local density proxy\n$$d_M(x) = \\frac{1}{\\epsilon + \\frac{1}{k}\\sum_{j=1}^{k} \\|x - \\text{NN}_j^{(M)}(x)\\|_2},$$\nwhere $\\text{NN}_j^{(M)}(x)$ denotes the $j$-th nearest neighbor of $x$ among majority points, and $\\epsilon > 0$ is a small constant to avoid division by zero.\n- Boundary proximity via distance to the nearest minority point\n$$d_m(x) = \\min_{z \\in \\mathcal{X}_m} \\|x-z\\|_2,$$\nwhere $\\mathcal{X}_m$ is the set of minority points.\n\nNormalize $d_M(x)$ and $d_m(x)$ across majority points to $[0,1]$ by min-max scaling to obtain $\\tilde{d}_M(x)$ and $\\tilde{d}_m(x)$. Define the removal propensity\n$$r(x) = \\tilde{d}_M(x) \\cdot \\tilde{d}_m(x).$$\nYou must retain the fraction $f \\in (0,1]$ of majority points with the smallest $r(x)$, removing the rest. This removes redundant dense interior points while keeping those close to the minority, which helps preserve the empirical decision boundary.\n\nClassifier and evaluation:\n- Train a $k$-Nearest Neighbors classifier with $k=5$ on the original training set and evaluate minority recall on a held-out test set.\n- Train the same classifier on the undersampled training set and evaluate minority recall on the same test set.\n- Report the recall improvement as $R_{\\text{after}} - R_{\\text{before}}$, expressed as a decimal and rounded to four decimal places.\n\nData generation:\n- For training data, the majority class is a mixture of two Gaussian components with centers at $(-2,0)$ and $(2,0)$ and isotropic standard deviation $\\sigma_M$ in each component; samples are split equally across the two components. The minority class is a single Gaussian with center $(c_x,c_y)$ and isotropic standard deviation $\\sigma_m$.\n- For test data, generate a fresh dataset from the same distributions (with independent randomness) containing a fixed number of majority and minority points; evaluation uses only the minority portion to compute recall.\n\nAngle units are not applicable. There are no physical units. Percentages must be expressed as decimals.\n\nTest suite:\nImplement the program to run the following four test cases. For each case $i$, the training set sizes, Gaussian parameters, and undersampling parameters are provided. The random number generator must be seeded as specified to ensure determinism.\n\n- Case $1$ (happy path):\n  - Seed $= 7$.\n  - Training majority count $= 1200$, training minority count $= 60$.\n  - Majority standard deviation $\\sigma_M = 1.0$, minority standard deviation $\\sigma_m = 0.5$.\n  - Minority center $(c_x,c_y) = (0, 0.5)$.\n  - Density $k$ for $d_M$: $k=15$.\n  - Majority keep fraction $f = 0.35$.\n  - Test majority count $= 400$, test minority count $= 200$.\n\n- Case $2$ (boundary condition: no undersampling):\n  - Seed $= 17$.\n  - Training majority count $= 800$, training minority count $= 40$.\n  - Majority standard deviation $\\sigma_M = 1.0$, minority standard deviation $\\sigma_m = 0.6$.\n  - Minority center $(c_x,c_y) = (0, 0.3)$.\n  - Density $k$ for $d_M$: $k=10$.\n  - Majority keep fraction $f = 1.0$.\n  - Test majority count $= 400$, test minority count $= 200$.\n\n- Case $3$ (extreme imbalance):\n  - Seed $= 23$.\n  - Training majority count $= 5000$, training minority count $= 50$.\n  - Majority standard deviation $\\sigma_M = 1.0$, minority standard deviation $\\sigma_m = 0.6$.\n  - Minority center $(c_x,c_y) = (0, 0.5)$.\n  - Density $k$ for $d_M$: $k=25$.\n  - Majority keep fraction $f = 0.20$.\n  - Test majority count $= 400$, test minority count $= 200$.\n\n- Case $4$ (minority well separated):\n  - Seed $= 101$.\n  - Training majority count $= 1500$, training minority count $= 80$.\n  - Majority standard deviation $\\sigma_M = 1.0$, minority standard deviation $\\sigma_m = 0.5$.\n  - Minority center $(c_x,c_y) = (0, 3.0)$.\n  - Density $k$ for $d_M$: $k=15$.\n  - Majority keep fraction $f = 0.30$.\n  - Test majority count $= 400$, test minority count $= 200$.\n\nImplementation requirements:\n- Use the specified algorithm for density-aware undersampling.\n- Use a $5$-nearest neighbors classifier for evaluation.\n- Minority recall must be reported as decimals; output the improvement $R_{\\text{after}} - R_{\\text{before}}$ for each case, rounded to four decimal places.\n- Final output format: Your program should produce a single line containing the four results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4]$ where each $r_i$ is the rounded decimal improvement for case $i$.\n\nThe program must be complete and runnable without any user input or external files.",
            "solution": "The problem statement is evaluated as valid. It presents a well-posed, scientifically grounded task within the domain of statistical learning. All necessary parameters and definitions for data generation, algorithmic implementation, and evaluation are provided. The procedure is deterministic given the specified random seeds, leading to a unique and verifiable solution. A minor ambiguity regarding the constant $\\epsilon$ in the density proxy formula is resolved by interpreting it as a standard small positive value for numerical stability, for which `numpy.finfo(float).eps` is a canonical choice. This interpretation does not affect the problem's integrity.\n\nThe solution proceeds by implementing the specified density-aware undersampling algorithm and evaluating its impact on a $k$-Nearest Neighbors classifier's performance. The methodology is as follows:\n\n1.  **Data Generation**: For each test case, training and test datasets are synthesized. The majority class, labeled $0$, is drawn from a mixture of two two-dimensional Gaussian distributions with centers at $(−2, 0)$ and $(2, 0)$ and standard deviation $\\sigma_M$. The minority class, labeled $1$, is drawn from a single Gaussian distribution centered at $(c_x, c_y)$ with standard deviation $\\sigma_m$. The random number generator is seeded to ensure reproducibility.\n\n2.  **Density-Aware Undersampling**: This core step aims to rebalance the training data by selectively removing majority class samples. The principle is to remove points that are both in a dense region of other majority points and far from any minority points. Such points are considered redundant and their removal is less likely to perturb the decision boundary. This is formalized by calculating a removal propensity $r(x)$ for each majority point $x$.\n    *   **Local Majority Density, $d_M(x)$**: The density around a majority point $x$ is estimated using the average distance to its $k$ nearest neighbors within the majority class. The density proxy is given by:\n        $$d_M(x) = \\frac{1}{\\epsilon + \\frac{1}{k}\\sum_{j=1}^{k} \\|x - \\text{NN}_j^{(M)}(x)\\|_2}$$\n        A higher $d_M(x)$ indicates that $x$ is in a denser region of the majority class. $\\text{NN}_j^{(M)}(x)$ is the $j$-th nearest neighbor of $x$ among majority points.\n    *   **Minority Proximity, $d_m(x)$**: The proximity of a majority point $x$ to the decision boundary is approximated by its Euclidean distance to the nearest minority point:\n        $$d_m(x) = \\min_{z \\in \\mathcal{X}_m} \\|x-z\\|_2$$\n        where $\\mathcal{X}_m$ is the set of all minority points. A larger $d_m(x)$ indicates $x$ is farther from the minority class.\n    *   **Removal Propensity, $r(x)$**: The values of $d_M(x)$ and $d_m(x)$ are normalized to the range $[0, 1]$ across all majority points, yielding $\\tilde{d}_M(x)$ and $\\tilde{d}_m(x)$. The removal propensity is the product of these normalized scores:\n        $$r(x) = \\tilde{d}_M(x) \\cdot \\tilde{d}_m(x)$$\n        A high $r(x)$ value indicates that a point is in a dense majority region and far from the minority region, making it a prime candidate for removal.\n    *   **Selection**: The algorithm retains the fraction $f$ of majority points having the lowest $r(x)$ scores, effectively preserving majority points that are either in sparse regions or close to the minority class.\n\n3.  **Classifier Training and Evaluation**: A $k$-Nearest Neighbors ($k$-NN) classifier with $k=5$ is used to assess the effect of undersampling.\n    *   **Baseline Recall ($R_{\\text{before}}$)**: The classifier is first trained on the original, imbalanced training set. Its performance is measured by computing the minority recall on the minority samples of the held-out test set. Minority recall is defined as $R = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$, where $\\text{TP}$ (True Positives) are minority test points correctly classified as minority, and $\\text{FN}$ (False Negatives) are minority test points incorrectly classified as majority.\n    *   **Post-Undersampling Recall ($R_{\\text{after}}$)**: The classifier is then trained on the new, undersampled training set, which consists of the retained majority points and all original minority points. Minority recall is re-evaluated on the same test set.\n    *   **Improvement**: The final reported metric for each case is the improvement in recall, $R_{\\text{after}} - R_{\\text{before}}$, rounded to four decimal places.\n\nThis entire process is repeated for each of the four specified test cases, and the results are aggregated into a final list.",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial import KDTree\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for density-aware undersampling.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {'seed': 7, 'train_maj': 1200, 'train_min': 60, 'sigma_M': 1.0, 'sigma_m': 0.5, 'c_minority': (0, 0.5), 'density_k': 15, 'f': 0.35, 'test_maj': 400, 'test_min': 200},\n        # Case 2 (boundary condition: no undersampling)\n        {'seed': 17, 'train_maj': 800, 'train_min': 40, 'sigma_M': 1.0, 'sigma_m': 0.6, 'c_minority': (0, 0.3), 'density_k': 10, 'f': 1.0, 'test_maj': 400, 'test_min': 200},\n        # Case 3 (extreme imbalance)\n        {'seed': 23, 'train_maj': 5000, 'train_min': 50, 'sigma_M': 1.0, 'sigma_m': 0.6, 'c_minority': (0, 0.5), 'density_k': 25, 'f': 0.20, 'test_maj': 400, 'test_min': 200},\n        # Case 4 (minority well separated)\n        {'seed': 101, 'train_maj': 1500, 'train_min': 80, 'sigma_M': 1.0, 'sigma_m': 0.5, 'c_minority': (0, 3.0), 'density_k': 15, 'f': 0.30, 'test_maj': 400, 'test_min': 200},\n    ]\n\n    results = []\n    \n    epsilon = np.finfo(float).eps  # Small constant for numerical stability\n    k_classifier = 5  # k for the evaluation classifier\n\n    def min_max_scale(data):\n        \"\"\"Performs min-max normalization on a 1D numpy array.\"\"\"\n        min_val = np.min(data)\n        max_val = np.max(data)\n        if max_val > min_val:\n            return (data - min_val) / (max_val - min_val)\n        return np.zeros_like(data, dtype=float)\n\n    def density_aware_undersampling(X_maj, X_min, k_density, f):\n        \"\"\"\n        Performs density-aware undersampling on the majority class.\n        \"\"\"\n        n_maj = X_maj.shape[0]\n        if n_maj == 0 or X_min.shape[0] == 0:\n            return X_maj\n\n        # 1. Calculate local density proxy d_M(x)\n        maj_tree = KDTree(X_maj)\n        # Query for k+1 neighbors to exclude the point itself\n        distances, _ = maj_tree.query(X_maj, k=k_density + 1)\n        mean_dists = np.mean(distances[:, 1:], axis=1)\n        d_M_values = 1.0 / (epsilon + mean_dists)\n\n        # 2. Calculate boundary proximity d_m(x)\n        all_dists_to_min = cdist(X_maj, X_min, 'euclidean')\n        d_m_values = np.min(all_dists_to_min, axis=1)\n\n        # 3. Normalize and calculate removal propensity r(x)\n        tilde_d_M = min_max_scale(d_M_values)\n        tilde_d_m = min_max_scale(d_m_values)\n        r_values = tilde_d_M * tilde_d_m\n\n        # 4. Retain fraction f with smallest r(x)\n        n_keep = int(n_maj * f)\n        indices_to_keep = np.argsort(r_values)[:n_keep]\n        \n        return X_maj[indices_to_keep]\n\n    def knn_predict(X_train, y_train, X_test, k):\n        \"\"\"\n        Predicts labels for X_test using k-NN classifier trained on X_train, y_train.\n        \"\"\"\n        if X_train.shape[0] == 0:\n            return np.zeros(X_test.shape[0], dtype=int)\n        \n        actual_k = min(k, X_train.shape[0])\n\n        train_tree = KDTree(X_train)\n        _, indices = train_tree.query(X_test, k=actual_k)\n\n        if indices.ndim == 1:\n            indices = indices.reshape(1, -1)\n\n        neighbor_labels = y_train[indices].astype(int)\n        \n        # Majority vote using bincount\n        predictions = np.apply_along_axis(\n            lambda x: np.argmax(np.bincount(x, minlength=2)),\n            axis=1,\n            arr=neighbor_labels\n        )\n        return predictions\n\n    for case in test_cases:\n        np.random.seed(case['seed'])\n\n        # Generate training data\n        n_maj_1 = case['train_maj'] // 2\n        n_maj_2 = case['train_maj'] - n_maj_1\n        X_train_maj = np.vstack([\n            np.random.normal(loc=[-2, 0], scale=case['sigma_M'], size=(n_maj_1, 2)),\n            np.random.normal(loc=[2, 0], scale=case['sigma_M'], size=(n_maj_2, 2))\n        ])\n        y_train_maj = np.zeros(case['train_maj'])\n        X_train_min = np.random.normal(loc=case['c_minority'], scale=case['sigma_m'], size=(case['train_min'], 2))\n        y_train_min = np.ones(case['train_min'])\n        \n        X_train = np.vstack((X_train_maj, X_train_min))\n        y_train = np.hstack((y_train_maj, y_train_min))\n        \n        # Generate test data (only minority set is needed for recall calculation)\n        X_test_min = np.random.normal(loc=case['c_minority'], scale=case['sigma_m'], size=(case['test_min'], 2))\n        \n        # --- \"Before\" undersampling ---\n        y_pred_before = knn_predict(X_train, y_train, X_test_min, k=k_classifier)\n        R_before = np.mean(y_pred_before) # Recall = TP / (TP+FN) = sum(preds) / len(preds)\n\n        # --- Perform undersampling ---\n        if case['f'] < 1.0:\n            X_train_maj_resampled = density_aware_undersampling(\n                X_train_maj, X_train_min, case['density_k'], case['f'])\n        else:\n            X_train_maj_resampled = X_train_maj.copy()\n        \n        y_train_maj_resampled = np.zeros(X_train_maj_resampled.shape[0])\n        \n        X_train_resampled = np.vstack((X_train_maj_resampled, X_train_min))\n        y_train_resampled = np.hstack((y_train_maj_resampled, y_train_min))\n        \n        # --- \"After\" undersampling ---\n        y_pred_after = knn_predict(X_train_resampled, y_train_resampled, X_test_min, k=k_classifier)\n        R_after = np.mean(y_pred_after)\n\n        # --- Calculate improvement ---\n        improvement = R_after - R_before\n        results.append(f\"{improvement:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}