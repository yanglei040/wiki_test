{
    "hands_on_practices": [
        {
            "introduction": "在回归任务中，平方误差（$L_2$损失）是最常用的损失函数之一，但它并非总是最佳选择。本练习旨在探讨其两个关键局限性：对离群点的敏感性和对数据尺度的依赖性。通过将$L_2$损失与更具鲁棒性的Huber损失进行比较，并引入一种能够应对尺度变化的相对误差损失，你将更深入地理解如何根据数据的具体特性来选择和设计合适的回归损失函数。",
            "id": "3143114",
            "problem": "考虑一个单变量回归设定，其中预测变量是一个常数 $b \\in \\mathbb{R}$，选择该常数以最小化三个目标值 $y_{1} = 1$、$y_{2} = 2$ 和 $y_{3} = 100$ 上的经验损失。你观察到目标值经过缩放后的版本 $c y_{i}$，其中 $c > 0$ 是一个已知的缩放因子。你将比较在平方损失和Huber损失下最小化子的行为，然后基于相对误差构建一个尺度不变的替代方案。\n\n使用以下基本定义：\n- 平方损失为 $\\ell_{2}(r) = r^{2}$。\n- 参数为 $\\delta > 0$ 的Huber损失为\n$$\n\\ell_{\\mathrm{Huber}, \\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2},  &|r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2},  &|r| > \\delta.\n\\end{cases}\n$$\n\n令 $R_{\\ell}(b; c) = \\sum_{i=1}^{3} \\ell(c y_{i} - b)$ 表示在损失 $\\ell$ 下，对于缩放后的目标值 $c y_{i}$ 的经验风险。\n\n1. 从第一性原理推导 $R_{\\ell_{2}}(b; c)$ 的最小化子 $b_{\\ell_{2}}^{\\ast}(c)$，并解释当目标值按 $c$ 缩放时，它是如何变换的。\n2. 对于Huber损失，在最小化子所处的区域内进行分析，该区域中 $y_{3}$ 的残差位于线性区域，而 $y_{1}$ 和 $y_{2}$ 的残差位于平方区域。从Huber损失及其导数的定义出发，推导在此区域下Huber最小化子 $b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)$ 的表达式。然后，确定 $c$ 和 $\\delta$ 上相应的条件，以确保该区域有效（即 $|c y_{1} - b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)| \\leq \\delta$，$|c y_{2} - b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)| \\leq \\delta$ 且 $c y_{3} - b_{\\mathrm{Huber}}^{\\ast}(c, \\delta) > \\delta$）。\n3. 提出相对平方误差损失\n$$\n\\ell_{\\mathrm{rel}}(r; y) = \\left( \\frac{r}{y} \\right)^{2},\n$$\n并从第一性原理推导经验风险 $R_{\\mathrm{rel}}(b; c) = \\sum_{i=1}^{3} \\left( \\frac{c y_{i} - b}{c y_{i}} \\right)^{2}$ 的最小化子 $b_{\\mathrm{rel}}^{\\ast}(c)$。解释在缩放因子 $c$ 下，$b_{\\mathrm{rel}}^{\\ast}(c)$ 的尺度行为。\n\n最后，定义\n$$\nD(c, \\delta) = b_{\\mathrm{Huber}}^{\\ast}(c, \\delta) - b_{\\mathrm{rel}}^{\\ast}(c).\n$$\n在第2部分指定的区域下，将 $D(c, \\delta)$ 表示为关于 $c$ 和 $\\delta$ 的单一简化的闭式解析表达式。你不需要对答案进行四舍五入。",
            "solution": "该问题要求在一个简单的回归设定中推导和比较三种不同损失函数的最小化子。我们将按顺序解决问题的每个部分。\n\n第1部分：平方损失的最小化子\n平方损失 $\\ell_{2}(r) = r^{2}$ 的经验风险由 $R_{\\ell_{2}}(b; c) = \\sum_{i=1}^{3} (c y_{i} - b)^{2}$ 给出。为了找到最小化子 $b_{\\ell_{2}}^{\\ast}(c)$，我们计算经验风险关于参数 $b$ 的导数并将其设为零。由于 $R_{\\ell_{2}}(b; c)$ 是 $b$ 的凸函数，一阶条件可以确定唯一的全局最小值。\n$$\n\\frac{d}{db} R_{\\ell_{2}}(b; c) = \\frac{d}{db} \\sum_{i=1}^{3} (c y_{i} - b)^{2} = \\sum_{i=1}^{3} 2(c y_{i} - b)(-1) = -2 \\sum_{i=1}^{3} (c y_{i} - b)\n$$\n将导数设为零，可得：\n$$\n\\sum_{i=1}^{3} (c y_{i} - b) = 0 \\implies c \\sum_{i=1}^{3} y_{i} - 3b = 0\n$$\n对 $b$ 求解，我们发现最小化子是缩放后数据点的样本均值：\n$$\nb_{\\ell_{2}}^{\\ast}(c) = \\frac{c}{3} \\sum_{i=1}^{3} y_{i}\n$$\n代入给定的目标值 $y_{1} = 1$、$y_{2} = 2$ 和 $y_{3} = 100$：\n$$\nb_{\\ell_{2}}^{\\ast}(c) = \\frac{c(1 + 2 + 100)}{3} = \\frac{103c}{3}\n$$\n最小化子在按 $c$ 缩放下的行为是，它是 $c$ 的线性函数。具体来说，$b_{\\ell_{2}}^{\\ast}(c) = c \\cdot b_{\\ell_{2}}^{\\ast}(1)$。这个性质被称为尺度等变性。\n\n第2部分：Huber损失的最小化子\n问题为Huber损失最小化子指定了一个区域，其中 $y_{1}$ 和 $y_{2}$ 的残差在平方区域（$|r| \\leq \\delta$），而 $y_{3}$ 的残差在线性区域（$|r| > \\delta$）。鉴于 $y_3=100$ 远大于 $y_1=1$ 和 $y_2=2$，残差 $c y_{3} - b$ 将为正。因此，区域条件为： $|c y_{1} - b| \\leq \\delta$，$|c y_{2} - b| \\leq \\delta$ 且 $c y_{3} - b > \\delta$。\n在这些条件下，经验风险为：\n$$\nR_{\\mathrm{Huber}}(b; c, \\delta) = \\frac{1}{2}(c y_{1} - b)^{2} + \\frac{1}{2}(c y_{2} - b)^{2} + \\left( \\delta(c y_{3} - b) - \\frac{1}{2}\\delta^{2} \\right)\n$$\n我们通过将关于 $b$ 的导数设为零来找到最小化子 $b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)$：\n$$\n\\frac{d}{db} R_{\\mathrm{Huber}}(b; c, \\delta) = -(c y_{1} - b) - (c y_{2} - b) - \\delta = 0\n$$\n$$\n-c(y_{1} + y_{2}) + 2b - \\delta = 0 \\implies 2b = c(y_{1} + y_{2}) + \\delta\n$$\n代入 $y_{1} = 1$ 和 $y_{2} = 2$，最小化子是：\n$$\nb_{\\mathrm{Huber}}^{\\ast}(c, \\delta) = \\frac{c(1 + 2) + \\delta}{2} = \\frac{3c + \\delta}{2}\n$$\n接下来，我们通过将 $b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)$ 代入区域不等式，来推导确保此区域有效的关于 $c$ 和 $\\delta$ 的条件：\n1.  $|c y_{1} - b_{\\mathrm{Huber}}^{\\ast}| \\leq \\delta \\implies |c - \\frac{3c + \\delta}{2}| \\leq \\delta \\implies |\\frac{-c - \\delta}{2}| \\leq \\delta$。由于 $c > 0$ 且 $\\delta > 0$，这变成 $\\frac{c+\\delta}{2} \\leq \\delta$，简化为 $c \\leq \\delta$。\n2.  $|c y_{2} - b_{\\mathrm{Huber}}^{\\ast}| \\leq \\delta \\implies |2c - \\frac{3c + \\delta}{2}| \\leq \\delta \\implies |\\frac{c - \\delta}{2}| \\leq \\delta$。这等价于 $-2\\delta \\leq c - \\delta \\leq 2\\delta$，或者 $-\\delta \\leq c \\leq 3\\delta$。已知 $c>0$，条件是 $0  c \\leq 3\\delta$。\n3.  $c y_{3} - b_{\\mathrm{Huber}}^{\\ast}  \\delta \\implies 100c - \\frac{3c + \\delta}{2}  \\delta \\implies 200c - 3c - \\delta  2\\delta \\implies 197c  3\\delta$，得出 $c  \\frac{3\\delta}{197}$。\n结合所有条件（$c \\leq \\delta$、$c \\leq 3\\delta$ 和 $c  \\frac{3\\delta}{197}$），所需的关系是 $\\frac{3\\delta}{197}  c \\leq \\delta$。\n\n第3部分：相对平方误差损失的最小化子\n相对平方误差 $\\ell_{\\mathrm{rel}}(r; y) = (r/y)^2$ 的经验风险由下式给出：\n$$\nR_{\\mathrm{rel}}(b; c) = \\sum_{i=1}^{3} \\left( \\frac{c y_{i} - b}{c y_{i}} \\right)^{2} = \\sum_{i=1}^{3} \\left( 1 - \\frac{b}{c y_{i}} \\right)^{2}\n$$\n我们通过将关于 $b$ 的导数设为零来找到最小化子 $b_{\\mathrm{rel}}^{\\ast}(c)$：\n$$\n\\frac{d}{db} R_{\\mathrm{rel}}(b; c) = \\sum_{i=1}^{3} 2\\left( 1 - \\frac{b}{c y_{i}} \\right) \\left(-\\frac{1}{c y_{i}}\\right) = -\\frac{2}{c} \\sum_{i=1}^{3} \\left( \\frac{1}{y_{i}} - \\frac{b}{c y_{i}^{2}} \\right) = 0\n$$\n$$\n\\sum_{i=1}^{3} \\frac{1}{y_{i}} - \\frac{b}{c} \\sum_{i=1}^{3} \\frac{1}{y_{i}^{2}} = 0\n$$\n对 $b$ 求解可得：\n$$\nb_{\\mathrm{rel}}^{\\ast}(c) = c \\frac{\\sum_{i=1}^{3} 1/y_{i}}{\\sum_{i=1}^{3} 1/y_{i}^{2}}\n$$\n我们使用 $y_{1}=1, y_{2}=2, y_{3}=100$ 计算总和：\n$$\n\\sum_{i=1}^{3} \\frac{1}{y_{i}} = \\frac{1}{1} + \\frac{1}{2} + \\frac{1}{100} = \\frac{100+50+1}{100} = \\frac{151}{100}\n$$\n$$\n\\sum_{i=1}^{3} \\frac{1}{y_{i}^{2}} = \\frac{1}{1^{2}} + \\frac{1}{2^{2}} + \\frac{1}{100^{2}} = 1 + \\frac{1}{4} + \\frac{1}{10000} = \\frac{10000+2500+1}{10000} = \\frac{12501}{10000}\n$$\n代入这些总和：\n$$\nb_{\\mathrm{rel}}^{\\ast}(c) = c \\frac{151/100}{12501/10000} = c \\frac{151}{100} \\frac{10000}{12501} = \\frac{15100}{12501} c\n$$\n$b_{\\mathrm{rel}}^{\\ast}(c)$ 的尺度行为是尺度等变的。最小化子是 $c$ 的一个线性函数，因此将目标值缩放 $c$ 倍也会将最优预测变量缩放 $c$ 倍。归一化的预测变量 $b_{\\mathrm{rel}}^{\\ast}(c)/c$ 是尺度不变的。\n\n$D(c, \\delta)$ 的最终计算\n我们被要求在第2部分的区域中找到 $D(c, \\delta) = b_{\\mathrm{Huber}}^{\\ast}(c, \\delta) - b_{\\mathrm{rel}}^{\\ast}(c)$。\n使用上面推导出的表达式：\n$$\nD(c, \\delta) = \\left( \\frac{3c + \\delta}{2} \\right) - \\left( \\frac{15100}{12501} c \\right)\n$$\n我们重排表达式，按 $c$ 和 $\\delta$ 对各项进行分组：\n$$\nD(c, \\delta) = \\left( \\frac{3}{2} - \\frac{15100}{12501} \\right) c + \\frac{\\delta}{2}\n$$\n我们为 $c$ 的系数找到一个公分母：\n$$\n\\frac{3}{2} - \\frac{15100}{12501} = \\frac{3 \\cdot 12501 - 2 \\cdot 15100}{2 \\cdot 12501} = \\frac{37503 - 30200}{25002} = \\frac{7303}{25002}\n$$\n分数 $\\frac{7303}{25002}$ 是不可约的。将其代回得到最终表达式：\n$$\nD(c, \\delta) = \\frac{7303}{25002} c + \\frac{\\delta}{2}\n$$",
            "answer": "$$\n\\boxed{\\frac{7303}{25002} c + \\frac{\\delta}{2}}\n$$"
        },
        {
            "introduction": "与回归问题类似，在分类任务中，损失函数的选择对模型性能同样至关重要，尤其是在处理含噪声数据时。本练习聚焦于比较两种广泛使用的分类损失函数——逻辑斯谛损失和指数损失——以探究它们的鲁棒性差异。通过模拟梯度下降的训练过程，你将直接观察到这两种损失函数在面对被错误标记或“困难”样本时的不同反应，从而揭示为何逻辑斯谛损失在许多实际应用中更受青睐。",
            "id": "3143216",
            "problem": "给定一个二元分类问题，其标签为 $y \\in \\{-1,+1\\}$，以及一个实值分数函数 $s \\in \\mathbb{R}$。边距 (margin) 定义为 $m = y s$。考虑作用于边距的两种损失函数：指数损失 $\\ell_{\\exp}(m)$ 和逻辑损失 $\\ell_{\\log}(m)$，定义如下：\n- $\\ell_{\\exp}(m) = \\exp(-m)$，\n- $\\ell_{\\log}(m) = \\log(1+\\exp(-m))$。\n\n对于一个固定的特征值，将条件类别概率 $\\eta \\in [0,1]$ 定义为 $\\eta = \\mathbb{P}(Y=+1 \\mid X)$。对于一个分数 $s$，其条件风险为\n$$\nR(s;\\eta) = \\eta \\, \\ell(s) + (1-\\eta) \\, \\ell(-s),\n$$\n其中 $\\ell$ 表示应用于边距的 $\\ell_{\\exp}$ 或 $\\ell_{\\log}$。\n\n任务概述：\n1) 仅从给定的定义出发，对每种损失函数（$\\ell_{\\exp}$ 和 $\\ell_{\\log}$），推导关于 $s$ 的导数 $R'(s;\\eta)$。你的推导必须使用链式法则，并且除了上述定义外，不能假设任何已知的快捷恒等式。\n2) 使用你推导出的 $R'(s;\\eta)$，设计一个小型模拟，模拟在固定 $\\eta$ 的情况下对 $R(s;\\eta)$ 进行一维梯度下降训练。对于给定的初始分数 $s_0 \\in \\mathbb{R}$、学习率 $\\alpha  0$ 和步数 $T \\in \\mathbb{N}$，为每种损失函数独立执行以下更新：\n   - 初始化 $s \\leftarrow s_0$。\n   - 对于 $t = 1,2,\\dots,T$：计算梯度 $g_t = R'(s;\\eta)$，记录其大小 $|g_t|$，并更新 $s \\leftarrow s - \\alpha \\, g_t$。\n   - 在 $T$ 步之后，计算平均梯度大小 $\\overline{G} = \\frac{1}{T} \\sum_{t=1}^T |g_t|$。\n3) 通过比较两种损失的平均梯度大小，研究极端噪声和尾部行为的影响。对于每个测试用例 $(\\eta, s_0)$，计算比率\n$$\n\\rho = \\frac{\\overline{G}_{\\log}}{\\overline{G}_{\\exp}},\n$$\n其中 $\\overline{G}_{\\log}$ 和 $\\overline{G}_{\\exp}$ 分别是从逻辑损失和指数损失获得的平均梯度大小。\n\n要求的模拟设置：\n- 使用学习率 $\\alpha = 10^{-3}$。\n- 使用 $T = 5$ 步。\n- 使用以下 $(\\eta, s_0)$ 对的测试套件，这些测试用例旨在探测尾部和噪声极端情况：\n  - $(\\eta, s_0) = (0.99, 8)$,\n  - $(\\eta, s_0) = (0.01, -8)$,\n  - $(\\eta, s_0) = (0.50, 8)$,\n  - $(\\eta, s_0) = (0.99, 0)$,\n  - $(\\eta, s_0) = (0.01, 8)$。\n\n程序要求：\n- 实现函数，根据定义和你的推导表达式直接计算每种损失的 $R'(s;\\eta)$。\n- 对于每个测试用例，使用相同的 $(\\eta, s_0)$、$\\alpha$ 和 $T$ 为两种损失独立运行模拟，然后计算 $\\rho$。\n- 将每个 $\\rho$ 四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内容为按上述测试套件顺序排列的结果，例如 $[\\rho_1,\\rho_2,\\rho_3,\\rho_4,\\rho_5]$。\n- 所有输出必须是四舍五入到 $6$ 位小数的浮点数。",
            "solution": "该问题是有效的，因为它在科学上基于统计学习理论，问题定义明确、客观，并为得出唯一解提供了所有必要信息。\n\n此问题要求推导二元分类中两种常用损失函数——指数损失和逻辑损失——的条件风险梯度，并通过数值模拟比较它们在不同条件下的行为。\n\n### 第 1 部分：条件风险梯度 $R'(s;\\eta)$ 的推导\n\n对于给定的分数 $s$ 和条件类别概率 $\\eta = \\mathbb{P}(Y=+1 \\mid X)$，条件风险定义为：\n$$\nR(s;\\eta) = \\eta \\, \\ell(s) + (1-\\eta) \\, \\ell(-s)\n$$\n其中，当真实标签为 $y=+1$（边距 $m=s$）时，损失为 $\\ell(s)$；当真实标签为 $y=-1$（边距 $m=-s$）时，损失为 $\\ell(-s)$。我们旨在求导数 $R'(s;\\eta) = \\frac{d}{ds}R(s;\\eta)$。\n\n#### 1.1 指数损失\n\n指数损失函数在边距 $m$ 上的定义为 $\\ell_{\\exp}(m) = \\exp(-m)$。\n指数损失的条件风险为：\n$$\nR_{\\exp}(s;\\eta) = \\eta \\, \\ell_{\\exp}(s) + (1-\\eta) \\, \\ell_{\\exp}(-s)\n$$\n代入 $\\ell_{\\exp}$ 的定义：\n$$\nR_{\\exp}(s;\\eta) = \\eta \\exp(-s) + (1-\\eta) \\exp(-(-s)) = \\eta \\exp(-s) + (1-\\eta) \\exp(s)\n$$\n为了求梯度，我们将 $R_{\\exp}(s;\\eta)$ 对 $s$ 求导：\n$$\nR'_{\\exp}(s;\\eta) = \\frac{d}{ds} \\left( \\eta \\exp(-s) + (1-\\eta) \\exp(s) \\right)\n$$\n利用导数的线性和对 $\\exp(-s)$ 应用链式法则：\n$$\nR'_{\\exp}(s;\\eta) = \\eta \\frac{d}{ds}(\\exp(-s)) + (1-\\eta) \\frac{d}{ds}(\\exp(s))\n$$\n$$\nR'_{\\exp}(s;\\eta) = \\eta (\\exp(-s) \\cdot (-1)) + (1-\\eta) (\\exp(s))\n$$\n$$\nR'_{\\exp}(s;\\eta) = -\\eta \\exp(-s) + (1-\\eta) \\exp(s)\n$$\n这就是指数损失的条件风险的导数。\n\n#### 1.2 逻辑损失\n\n逻辑损失函数在边距 $m$ 上的定义为 $\\ell_{\\log}(m) = \\log(1+\\exp(-m))$。\n逻辑损失的条件风险为：\n$$\nR_{\\log}(s;\\eta) = \\eta \\, \\ell_{\\log}(s) + (1-\\eta) \\, \\ell_{\\log}(-s)\n$$\n我们分别计算每一项的导数。首先，我们求逻辑损失函数 $\\ell_{\\log}(m)$ 关于其参数 $m$ 的导数。使用链式法则，令 $u(m) = 1 + \\exp(-m)$，则 $\\ell_{\\log}(m) = \\log(u(m))$。\n$$\n\\frac{d\\ell_{\\log}}{dm} = \\frac{d}{dm}\\log(u(m)) = \\frac{1}{u(m)} \\cdot \\frac{du}{dm} = \\frac{1}{1 + \\exp(-m)} \\cdot (-\\exp(-m)) = -\\frac{\\exp(-m)}{1 + \\exp(-m)}\n$$\n现在，我们将此结果应用于条件风险导数 $R'_{\\log}(s;\\eta) = \\frac{d}{ds}R_{\\log}(s;\\eta)$ 中的两项：\n$$\nR'_{\\log}(s;\\eta) = \\eta \\frac{d}{ds}(\\ell_{\\log}(s)) + (1-\\eta) \\frac{d}{ds}(\\ell_{\\log}(-s))\n$$\n对于第一项，我们设 $m=s$ 并使用我们刚求得的导数：\n$$\n\\frac{d}{ds}(\\ell_{\\log}(s)) = -\\frac{\\exp(-s)}{1 + \\exp(-s)}\n$$\n对于第二项，我们应用链式法则。令 $v(s) = -s$。则 $\\frac{d}{ds}(\\ell_{\\log}(-s)) = \\frac{d\\ell_{\\log}}{dv} \\cdot \\frac{dv}{ds}$。\n$$\n\\frac{d}{ds}(\\ell_{\\log}(-s)) = \\left( -\\frac{\\exp(-v)}{1 + \\exp(-v)} \\right) \\cdot (-1) = \\frac{\\exp(-(-s))}{1 + \\exp(-(-s))} = \\frac{\\exp(s)}{1 + \\exp(s)}\n$$\n结合这些结果：\n$$\nR'_{\\log}(s;\\eta) = \\eta \\left( -\\frac{\\exp(-s)}{1 + \\exp(-s)} \\right) + (1-\\eta) \\left( \\frac{\\exp(s)}{1 + \\exp(s)} \\right)\n$$\n这个表达式可以被简化。令 $\\sigma(s) = \\frac{1}{1+\\exp(-s)} = \\frac{\\exp(s)}{1+\\exp(s)}$ 为 sigmoid 函数。\n注意到 $\\frac{d}{ds}(\\ell_{\\log}(s)) = \\sigma(s)-1$ 且 $\\frac{d}{ds}(\\ell_{\\log}(-s)) = \\sigma(s)$。\n将这些简化形式代回 $R'_{\\log}(s;\\eta)$ 的表达式中：\n$$\nR'_{\\log}(s;\\eta) = \\eta (\\sigma(s)-1) + (1-\\eta)\\sigma(s)\n$$\n$$\nR'_{\\log}(s;\\eta) = \\eta\\sigma(s) - \\eta + \\sigma(s) - \\eta\\sigma(s)\n$$\n$$\nR'_{\\log}(s;\\eta) = \\sigma(s) - \\eta\n$$\n这就是逻辑损失的条件风险的最终简化导数。\n\n### 第 2 部分：模拟设计\n\n该模拟实现了一个一维梯度下降算法，以在固定 $\\eta$ 的情况下最小化条件风险 $R(s;\\eta)$。\n\n1.  **初始化**：对于每个测试用例 $(\\eta, s_0)$，我们初始化分数 $s \\leftarrow s_0$。此过程对指数损失和逻辑损失独立进行。\n2.  **迭代**：总共进行 $T=5$ 步，在每一步 $t \\in \\{1, 2, \\dots, T\\}$ 中：\n    a.  使用相应的推导公式 $R'_{\\exp}(s;\\eta)$ 或 $R'_{\\log}(s;\\eta)$ 计算梯度 $g_t = R'(s;\\eta)$。\n    b.  记录梯度的绝对大小 $|g_t|$。\n    c.  使用梯度下降规则更新分数：$s \\leftarrow s - \\alpha g_t$，学习率为 $\\alpha = 10^{-3}$。\n3.  **聚合**：经过 $T$ 步后，计算平均梯度大小 $\\overline{G} = \\frac{1}{T} \\sum_{t=1}^T |g_t|$。此计算对 $\\overline{G}_{\\exp}$ 和 $\\overline{G}_{\\log}$ 均进行。\n4.  **比较**：对于每个测试用例，计算比率 $\\rho = \\frac{\\overline{G}_{\\log}}{\\overline{G}_{\\exp}}$。该比率量化了两种损失函数在给定条件下产生的梯度的相对大小。一个小的 $\\rho$ 值表明逻辑损失的梯度远小于指数损失的梯度，这意味着逻辑损失具有更好的稳定性，对远离决策边界或可能被错误标记的数据点不那么敏感。\n\n所提供的测试用例旨在探索这种行为，特别是在分数 $s$ 的量级很大的“尾部”区域，以及对于 $\\eta$ 接近 $0$ 或 $1$ 但分数 $s$ 暗示相反类别的“噪声”标签。",
            "answer": "```python\n# 完整且可运行的 Python 3 代码。\n# 导入必须遵守指定的执行环境。\nimport numpy as np\n\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\n\ndef grad_exp(s: float, eta: float) - float:\n    \"\"\"\n    计算指数损失的条件风险的导数。\n    R'_exp(s; eta) = (1 - eta) * exp(s) - eta * exp(-s)\n    \"\"\"\n    return (1.0 - eta) * np.exp(s) - eta * np.exp(-s)\n\ndef grad_log(s: float, eta: float) - float:\n    \"\"\"\n    计算逻辑损失的条件风险的导数。\n    R'_log(s; eta) = sigma(s) - eta, 其中 sigma(s) 是 sigmoid 函数。\n    \"\"\"\n    # sigmoid函数的数值稳定实现\n    if s = 0:\n        z = np.exp(-s)\n        sigma_s = 1.0 / (1.0 + z)\n    else:\n        z = np.exp(s)\n        sigma_s = z / (1.0 + z)\n    return sigma_s - eta\n\ndef run_simulation(\n    grad_func,\n    eta: float,\n    s0: float,\n    alpha: float,\n    T: int\n) - float:\n    \"\"\"\n    运行一维梯度下降模拟并返回平均梯度大小。\n    \n    参数:\n        grad_func: 用于计算梯度的函数 (例如, grad_exp 或 grad_log)。\n        eta: 条件类别概率 P(Y=+1|X)。\n        s0: 初始分数。\n        alpha: 学习率。\n        T: 模拟步数。\n        \n    返回:\n        T步内梯度的平均绝对大小。\n    \"\"\"\n    s = float(s0)\n    grad_magnitudes = []\n    for _ in range(T):\n        gradient = grad_func(s, eta)\n        grad_magnitudes.append(np.abs(gradient))\n        s = s - alpha * gradient\n        \n    return np.mean(grad_magnitudes)\n\ndef solve():\n    \"\"\"\n    主函数，用于为所有测试用例运行模拟并打印结果。\n    \"\"\"\n    # 要求的模拟设置\n    alpha = 1e-3\n    T = 5\n\n    # (eta, s0) 对的测试用例套件\n    test_cases = [\n        (0.99, 8.0),\n        (0.01, -8.0),\n        (0.50, 8.0),\n        (0.99, 0.0),\n        (0.01, 8.0),\n    ]\n\n    results = []\n    for eta, s0 in test_cases:\n        # 为逻辑损失运行模拟\n        avg_grad_log = run_simulation(grad_log, eta, s0, alpha, T)\n        \n        # 为指数损失运行模拟\n        avg_grad_exp = run_simulation(grad_exp, eta, s0, alpha, T)\n\n        # 计算比率rho。对avg_grad_exp进行零检查是一种安全措施，\n        # 尽管在这些特定测试用例中预计不会出现。\n        if avg_grad_exp == 0.0:\n            # 如果两者都为零，则比率是不确定的（我们定义为1）。\n            if avg_grad_log == 0.0:\n                rho = 1.0\n            # 如果只有exp为零，则log/exp是无穷大。\n            else:\n                rho = np.inf\n        else:\n            rho = avg_grad_log / avg_grad_exp\n        \n        # 按要求四舍五入到6位小数。\n        results.append(round(rho, 6))\n\n    # 将输出格式化为方括号中的逗号分隔列表。\n    output_str = \",\".join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "最后，我们来解决一个在分类中非常普遍的实际挑战：类别不平衡。当某个类别的样本数量远少于其他类别时，标准的损失函数可能会导致模型在少数类上表现不佳。本练习将向你展示如何通过设计加权损失函数来有效应对这一问题。你将推导类别权重如何影响最优决策边界，并量化分析其在精确率与召回率之间的权衡，以及对模型校准度的影响。",
            "id": "3143139",
            "problem": "考虑在经验风险最小化（ERM）框架下的统计学习中的二元分类问题。设输入是一个实例向量，其条件类别概率表示为 $p(x) = \\mathbb{P}(Y=+1 \\mid X=x)$，其中 $Y \\in \\{-1,+1\\}$。我们研究一种依赖于标签的、基于间隔的逻辑损失，其中通过与类别相关的权重，使得正样本比负样本产生更大的惩罚。加权逻辑损失为正类别分配权重 $w_{+}  0$，为负类别分配权重 $w_{-}  0$。假设评分函数是一个实值函数 $f(x) \\in \\mathbb{R}$，并将单个实例的损失定义为\n$$\n\\ell(y, f(x)) = \\begin{cases}\nw_{+} \\cdot \\log\\left(1 + e^{-f(x)}\\right)  \\text{if } y = +1, \\\\\nw_{-} \\cdot \\log\\left(1 + e^{f(x)}\\right)  \\text{if } y = -1.\n\\end{cases}\n$$\n在一个固定的 $x$ 处的条件期望风险是 $\\ell(Y,f(x))$ 在给定 $X=x$ 的条件下对 $Y$ 的期望。研究 $w_{+}$ 和 $w_{-}$ 的选择如何影响 (i) 当朴素预测概率被定义为学习到的分数的 sigmoid 函数时的校准情况，以及 (ii) 当基于分数的符号进行分类时的查准率-查全率权衡。\n\n从期望风险和逻辑损失的核心定义出发，从第一性原理推导最小化固定 $x$ 处条件期望风险的贝叶斯最优分数 $f^{\\star}(x)$ 的形式，表示通过将 sigmoid 函数应用于 $f^{\\star}(x)$ 得到的朴素预测概率 $q(x)$，并确定当使用规则 $f^{\\star}(x) \\ge 0$ 预测正类别时对 $p(x)$ 的隐含阈值。使用这些推导设计一个算法，用于计算加权损失下的校准误差和期望查准率-查全率的经验度量。\n\n实现要求：\n- 在一个纯数学设定下工作，其中数据集由一个固定的、确定性的条件概率向量表示，无需采样标签。设数据集为 $N=99$ 个实例，其概率为\n$$\np_i = 0.01 + \\frac{i-1}{98} \\cdot 0.98 \\quad \\text{for } i=1,2,\\dots,99,\n$$\n它们在 $(0,1)$ 区间内均匀分布。\n- 对于测试套件中的每一对 $(w_{+}, w_{-})$，通过您推导的表达式计算每个实例的贝叶斯最优分数 $f^{\\star}(x)$，通过将 sigmoid 函数应用于 $f^{\\star}(x)$ 来计算朴素预测概率 $q(x)$，并用它来评估校准和分类性能。\n- 校准评估：按如下方式计算期望校准误差（ECE）。将朴素预测概率的范围划分为 $[0,1]$ 上的 $10$ 个等宽的区间。对于每个区间，计算该区间内朴素预测概率 $q(x)$ 的平均值和落入该区间的实例的真实概率 $p(x)$ 的平均值，并累加按该区间中实例比例加权的绝对差。具体来说，\n$$\n\\mathrm{ECE} = \\sum_{b=1}^{10} \\frac{n_b}{N} \\cdot \\left| \\overline{q}_b - \\overline{p}_b \\right|,\n$$\n其中 $n_b$ 是区间 $b$ 中的实例数，$\\overline{q}_b$ 是区间 $b$ 中 $q(x)$ 的均值，$\\overline{p}_b$ 是区间 $b$ 中 $p(x)$ 的均值。区间的定义使得所有在 $[0,0.1)$ 内的朴素预测概率属于区间1，$[0.1,0.2)$ 的属于区间2，依此类推，最后一个区间包含右端点 $1$。\n- 查准率和查全率：设分类规则在 $f^{\\star}(x) \\ge 0$ 时预测为正，否则为负。计算期望真阳性 $\\mathrm{TP} = \\sum_{i \\in \\mathcal{P}} p_i$ 和期望假阳性 $\\mathrm{FP} = \\sum_{i \\in \\mathcal{P}} (1 - p_i)$，其中 $\\mathcal{P}$ 是被分类为正的实例的索引集。同时计算期望假阴性 $\\mathrm{FN} = \\sum_{i \\notin \\mathcal{P}} p_i$ 和期望总阳性 $\\mathrm{P} = \\sum_{i=1}^{N} p_i$。然后计算查准率 $\\mathrm{Prec} = \\mathrm{TP} / (\\mathrm{TP} + \\mathrm{FP})$（当分母非零时，否则定义为 $0$），以及查全率 $\\mathrm{Rec} = \\mathrm{TP} / \\mathrm{P}$（当分母非零时，否则定义为 $0$）。所有量都是无量纲的，并且必须表示为实数。\n\n测试套件：\n- 案例1：$(w_{+}, w_{-}) = (1, 1)$。\n- 案例2：$(w_{+}, w_{-}) = (3, 1)$。\n- 案例3：$(w_{+}, w_{-}) = (10, 1)$。\n- 案例4：$(w_{+}, w_{-}) = (1.5, 1)$。\n\n您的程序应生成单行输出，其中包含测试套件的结果，格式为逗号分隔的列表的列表，每个内部列表包含对应案例的三个实数，顺序为 $[\\mathrm{ECE}, \\mathrm{Prec}, \\mathrm{Rec}]$。最终输出行不得包含空格，每个实数必须四舍五入到六位小数。例如，格式应为\n$$\n[[r_{1,1},r_{1,2},r_{1,3}],[r_{2,1},r_{2,2},r_{2,3}],[r_{3,1},r_{3,2},r_{3,3}],[r_{4,1},r_{4,2},r_{4,3}]].\n$$",
            "solution": "这个问题是适定的，有科学依据，并为完整的解析解和计算解提供了所有必要的信息。我们首先推导所要求的理论量，然后概述其计算算法。\n\n首先，我们推导最小化条件期望风险的贝叶斯最优分数 $f^{\\star}(x)$。条件期望风险 $R(f(x) \\mid x)$ 是在给定特征 $x$ 的条件下，损失函数 $\\ell(Y, f(x))$ 对标签 $Y$ 的条件分布的期望。设 $p(x) = \\mathbb{P}(Y=+1 \\mid X=x)$，这意味着 $\\mathbb{P}(Y=-1 \\mid X=x) = 1 - p(x)$。条件期望风险是：\n$$\nR(f(x) \\mid x) = \\mathbb{E}_{Y \\mid X=x}[\\ell(Y, f(x))] = p(x) \\cdot \\ell(+1, f(x)) + (1 - p(x)) \\cdot \\ell(-1, f(x))\n$$\n代入加权逻辑损失的定义：\n$$\nR(f(x) \\mid x) = p(x) \\cdot w_{+} \\log\\left(1 + e^{-f(x)}\\right) + (1 - p(x)) \\cdot w_{-} \\log\\left(1 + e^{f(x)}\\right)\n$$\n为了找到最小化此风险的分数 $f(x)$，我们计算 $R$ 关于 $f(x)$ 的偏导数并将其设为零。为简化符号，我们令 $f = f(x)$ 和 $p = p(x)$。\n$$\n\\frac{\\partial R}{\\partial f} = p \\cdot w_{+} \\left(\\frac{-e^{-f}}{1 + e^{-f}}\\right) + (1 - p) \\cdot w_{-} \\left(\\frac{e^{f}}{1 + e^{f}}\\right)\n$$\n简化括号中的项：\n$$\n\\frac{-e^{-f}}{1 + e^{-f}} = \\frac{-1}{e^{f} + 1} \\quad \\text{and} \\quad \\frac{e^{f}}{1 + e^{f}}\n$$\n于是导数变为：\n$$\n\\frac{\\partial R}{\\partial f} = -p \\cdot w_{+} \\frac{1}{1 + e^{f}} + (1 - p) \\cdot w_{-} \\frac{e^{f}}{1 + e^{f}}\n$$\n将导数设为零以求最优分数 $f^{\\star}$：\n$$\np \\cdot w_{+} \\frac{1}{1 + e^{f^{\\star}}} = (1 - p) \\cdot w_{-} \\frac{e^{f^{\\star}}}{1 + e^{f^{\\star}}}\n$$\n假设 $1+e^{f^{\\star}} \\neq 0$，对于实数 $f^{\\star}$ 这总是成立的，我们可以消去这一项：\n$$\np \\cdot w_{+} = (1 - p) \\cdot w_{-} e^{f^{\\star}}\n$$\n求解 $e^{f^{\\star}}$：\n$$\ne^{f^{\\star}} = \\frac{p \\cdot w_{+}}{(1 - p) \\cdot w_{-}}\n$$\n因此，贝叶斯最优分数 $f^{\\star}(x)$ 是：\n$$\nf^{\\star}(x) = \\log\\left(\\frac{p(x)}{1 - p(x)} \\frac{w_{+}}{w_{-}}\\right) = \\log\\left(\\frac{p(x)}{1 - p(x)}\\right) + \\log\\left(\\frac{w_{+}}{w_{-}}\\right)\n$$\n这揭示了最优分数是真实概率的对数几率与一个由类别权重比率决定的偏置项之和。\n\n第二，我们通过将 sigmoid 函数 $\\sigma(z) = (1 + e^{-z})^{-1}$ 应用于 $f^{\\star}(x)$ 来求朴素预测概率 $q(x)$。\n$$\nq(x) = \\sigma(f^{\\star}(x)) = \\frac{1}{1 + e^{-f^{\\star}(x)}}\n$$\n从 $e^{f^{\\star}}$ 的表达式，我们有 $e^{-f^{\\star}(x)} = \\frac{(1 - p(x)) \\cdot w_{-}}{p(x) \\cdot w_{+}}$。将其代入 sigmoid 函数：\n$$\nq(x) = \\frac{1}{1 + \\frac{(1 - p(x))w_{-}}{p(x)w_{+}}} = \\frac{p(x)w_{+}}{p(x)w_{+} + (1 - p(x))w_{-}}\n$$\n如果权重是平衡的（$w_{+} = w_{-}$），则 $q(x) = \\frac{p(x)w_{+}}{p(x)w_{+} + (1 - p(x))w_{+}} = p(x)$，这意味着朴素预测器是完美校准的。对于不平衡的权重，$q(x) \\neq p(x)$，这会导致校准误差。\n\n第三，我们确定对 $p(x)$ 的分类阈值。分类规则在 $f^{\\star}(x) \\ge 0$ 时预测为正类别。\n$$\n\\log\\left(\\frac{p(x)}{1 - p(x)} \\frac{w_{+}}{w_{-}}\\right) \\ge 0\n$$\n由于对数函数是单调递增函数，这等价于其参数大于或等于 $1$：\n$$\n\\frac{p(x)}{1 - p(x)} \\frac{w_{+}}{w_{-}} \\ge 1\n$$\n$$\n\\frac{p(x)}{1 - p(x)} \\ge \\frac{w_{-}}{w_{+}}\n$$\n假设 $p(x) \\in (0,1)$，我们可以乘以 $1 - p(x)  0$：\n$$\np(x)w_{+} \\ge (1 - p(x))w_{-} \\implies p(x)w_{+} \\ge w_{-} - p(x)w_{-} \\implies p(x)(w_{+} + w_{-}) \\ge w_{-}\n$$\n因此，预测正类别的真实条件概率 $p(x)$ 的阈值是：\n$$\np(x) \\ge \\frac{w_{-}}{w_{+} + w_{-}}\n$$\n当 $w_{+}  w_{-}$ 时，此阈值低于 $0.5$，鼓励正向预测；当 $w_{+}  w_{-}$ 时，此阈值高于 $0.5$，抑制正向预测。\n\n最后，我们设计算法来为给定的数据集和测试案例计算所需的度量。数据集包含 $N=99$ 个实例，其概率为 $p_i = 0.01 + \\frac{i-1}{98} \\cdot 0.98$ (其中 $i=1, \\dots, 99$)。这可以简化为一个等差数列 $p_i = 0.01 \\cdot i$ (其中 $i=1, \\dots, 99$)。对于每个测试案例 $(w_{+}, w_{-})$：\n\n1.  **计算朴素概率**：对于每个 $p_i$，计算 $q_i = \\frac{p_i w_{+}}{p_i w_{+} + (1-p_i)w_{-}}$。\n2.  **计算期望校准误差 (ECE)**：\n    - 将 $q_i$ 值划分到 10 个区间中：$[0, 0.1), [0.1, 0.2), \\dots, [0.9, 1.0]$。\n    - 对于每个区间 $b$，找到其 $q_i$ 值落入该区间的实例的索引集 $I_b$。\n    - 令 $n_b = |I_b|$。如果 $n_b  0$，计算平均预测概率 $\\overline{q}_b = \\frac{1}{n_b}\\sum_{i \\in I_b} q_i$ 和平均真实概率 $\\overline{p}_b = \\frac{1}{n_b}\\sum_{i \\in I_b} p_i$。\n    - ECE 为 $\\sum_{b=1}^{10} \\frac{n_b}{N} \\cdot \\left| \\overline{q}_b - \\overline{p}_b \\right|$。\n3.  **计算查准率和查全率**：\n    - 确定预测为正的实例集：$\\mathcal{P} = \\{i \\mid p_i \\ge \\frac{w_{-}}{w_{+} + w_{-}}\\}$。\n    - 计算期望真阳性：$\\mathrm{TP} = \\sum_{i \\in \\mathcal{P}} p_i$。\n    - 期望真阳性与期望假阳性之和为 $\\mathrm{TP} + \\mathrm{FP} = \\sum_{i \\in \\mathcal{P}} p_i + \\sum_{i \\in \\mathcal{P}} (1 - p_i) = \\sum_{i \\in \\mathcal{P}} 1 = |\\mathcal{P}|$。\n    - 查准率为 $\\mathrm{Prec} = \\mathrm{TP} / |\\mathcal{P}|$（如果 $|\\mathcal{P}|0$，否则为 $0$）。\n    - 期望总阳性为 $\\mathrm{P} = \\sum_{i=1}^{N} p_i$。\n    - 查全率为 $\\mathrm{Rec} = \\mathrm{TP} / \\mathrm{P}$（如果 $\\mathrm{P}0$，否则为 $0$）。\n\n对每个测试案例执行此程序以产生最终结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    通过推导和计算加权逻辑损失的校准和分类指标来解决问题。\n    \"\"\"\n    \n    # 定义条件概率的数据集\n    N = 99\n    p = np.arange(1, N + 1) / 100.0  # p_i = i/100 for i=1,...,99\n    \n    # 定义测试套件\n    test_cases = [\n        (1.0, 1.0),  # 案例 1\n        (3.0, 1.0),  # 案例 2\n        (10.0, 1.0), # 案例 3\n        (1.5, 1.0)   # 案例 4\n    ]\n    \n    results = []\n    \n    for w_plus, w_minus in test_cases:\n        # 1. 计算朴素预测概率 q_i\n        q = (p * w_plus) / (p * w_plus + (1 - p) * w_minus)\n\n        # 2. 计算期望校准误差 (ECE)\n        num_bins = 10\n        bins_p = [[] for _ in range(num_bins)]\n        bins_q = [[] for _ in range(num_bins)]\n        \n        for i in range(N):\n            q_val = q[i]\n            # 确定区间索引。区间为 [0, 0.1), [0.1, 0.2), ..., [0.9, 1.0]\n            if q_val == 1.0:\n                bin_idx = num_bins - 1\n            else:\n                bin_idx = int(np.floor(q_val * num_bins))\n            \n            bins_p[bin_idx].append(p[i])\n            bins_q[bin_idx].append(q_val)\n            \n        ece = 0.0\n        for b in range(num_bins):\n            n_b = len(bins_p[b])\n            if n_b  0:\n                mean_p_b = np.mean(bins_p[b])\n                mean_q_b = np.mean(bins_q[b])\n                ece += (n_b / N) * np.abs(mean_q_b - mean_p_b)\n\n        # 3. 计算查准率和查全率\n        # 确定p的分类阈值\n        p_threshold = w_minus / (w_plus + w_minus)\n        \n        # 识别被预测为正的实例\n        positive_indices = np.where(p = p_threshold)[0]\n        \n        # 计算期望的 TP, FP, P\n        if len(positive_indices)  0:\n            p_pos = p[positive_indices]\n            tp = np.sum(p_pos)\n            num_pos_pred = len(positive_indices)\n        else:\n            tp = 0.0\n            num_pos_pred = 0\n            \n        total_pos = np.sum(p)\n        \n        # 计算查准率\n        if num_pos_pred  0:\n            precision = tp / num_pos_pred\n        else:\n            precision = 0.0\n            \n        # 计算查全率\n        if total_pos  0:\n            recall = tp / total_pos\n        else:\n            recall = 0.0\n\n        results.append([ece, precision, recall])\n        \n    # 按指定格式化最终输出字符串\n    output_str = f\"[{','.join([f'[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]' for r in results])}]\"\n    print(output_str)\n\nsolve()\n\n```"
        }
    ]
}