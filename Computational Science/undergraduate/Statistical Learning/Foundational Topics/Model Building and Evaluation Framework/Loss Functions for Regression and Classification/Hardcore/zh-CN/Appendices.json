{
    "hands_on_practices": [
        {
            "introduction": "在回归问题中，损失函数的选择决定了模型如何衡量和惩罚预测误差。本练习将引导你对比研究两种重要的回归损失函数：对异常值敏感的标准平方损失（$L_2$ 损失），以及更为稳健的Huber损失。通过在一个包含明显异常值的数据集上推导不同损失函数下的最优预测值，你将亲身体会到它们数学性质上的差异如何转化为对异常值的不同处理方式，并深入理解损失函数在模型稳健性中的核心作用 。",
            "id": "3143114",
            "problem": "考虑一个单变量回归问题，其中预测变量是一个常数 $b \\in \\mathbb{R}$，其选择旨在最小化三个目标值 $y_{1} = 1$、$y_{2} = 2$ 和 $y_{3} = 100$ 的经验损失。你观察到的是目标值的缩放版本 $c y_{i}$，其中缩放因子 $c  0$ 是已知的。你将比较在平方损失和 Huber 损失下最小化器的行为，然后基于相对误差构建一个尺度不变的替代方案。\n\n使用以下基本定义：\n- 平方损失为 $\\ell_{2}(r) = r^{2}$。\n- 参数为 $\\delta  0$ 的 Huber 损失为\n$$\n\\ell_{\\mathrm{Huber}, \\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2},  |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2},  |r|  \\delta.\n\\end{cases}\n$$\n\n令 $R_{\\ell}(b; c) = \\sum_{i=1}^{3} \\ell(c y_{i} - b)$ 表示在损失函数 $\\ell$ 下，针对缩放后的目标值 $c y_{i}$ 的经验风险。\n\n1. 从第一性原理出发，推导 $R_{\\ell_{2}}(b; c)$ 的最小化器 $b_{\\ell_{2}}^{\\ast}(c)$，并解释当目标值按 $c$ 缩放时，它是如何变换的。\n2. 对于 Huber 损失，考虑这样一种机制：在最小化器处，$y_{3}$ 的残差位于线性区域，而 $y_{1}$ 和 $y_{2}$ 的残差位于二次区域。从 Huber 损失及其导数的定义出发，推导在此机制下 Huber 最小化器 $b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)$ 的表达式。然后，确定相应的关于 $c$ 和 $\\delta$ 的条件，以确保此机制有效（即，满足 $|c y_{1} - b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)| \\leq \\delta$、 $|c y_{2} - b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)| \\leq \\delta$ 和 $c y_{3} - b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)  \\delta$）。\n3. 提出相对平方误差损失\n$$\n\\ell_{\\mathrm{rel}}(r; y) = \\left( \\frac{r}{y} \\right)^{2},\n$$\n并从第一性原理出发，推导经验风险 $R_{\\mathrm{rel}}(b; c) = \\sum_{i=1}^{3} \\left( \\frac{c y_{i} - b}{c y_{i}} \\right)^{2}$ 的最小化器 $b_{\\mathrm{rel}}^{\\ast}(c)$。解释在缩放因子 $c$ 下，$b_{\\mathrm{rel}}^{\\ast}(c)$ 的尺度行为。\n\n最后，定义\n$$\nD(c, \\delta) = b_{\\mathrm{Huber}}^{\\ast}(c, \\delta) - b_{\\mathrm{rel}}^{\\ast}(c).\n$$\n在第 2 部分指定的机制下，将 $D(c, \\delta)$ 表示为关于 $c$ 和 $\\delta$ 的单个简化的闭式解析表达式。你不需要对答案进行四舍五入。",
            "solution": "该问题要求在一个简单的回归设置中，推导并比较三种不同损失函数的最小化器。我们将按顺序解决问题的每个部分。\n\n第 1 部分：平方损失的最小化器\n平方损失 $\\ell_{2}(r) = r^{2}$ 的经验风险由 $R_{\\ell_{2}}(b; c) = \\sum_{i=1}^{3} (c y_{i} - b)^{2}$ 给出。为了找到最小化器 $b_{\\ell_{2}}^{\\ast}(c)$，我们计算经验风险相对于参数 $b$ 的导数，并将其设为零。由于 $R_{\\ell_{2}}(b; c)$ 是关于 $b$ 的凸函数，一阶条件可以确定唯一的全局最小值。\n$$\n\\frac{d}{db} R_{\\ell_{2}}(b; c) = \\frac{d}{db} \\sum_{i=1}^{3} (c y_{i} - b)^{2} = \\sum_{i=1}^{3} 2(c y_{i} - b)(-1) = -2 \\sum_{i=1}^{3} (c y_{i} - b)\n$$\n将导数设为零可得：\n$$\n\\sum_{i=1}^{3} (c y_{i} - b) = 0 \\implies c \\sum_{i=1}^{3} y_{i} - 3b = 0\n$$\n解出 $b$，我们发现最小化器是缩放后数据点的样本均值：\n$$\nb_{\\ell_{2}}^{\\ast}(c) = \\frac{c}{3} \\sum_{i=1}^{3} y_{i}\n$$\n代入给定的目标值 $y_{1} = 1$、$y_{2} = 2$ 和 $y_{3} = 100$：\n$$\nb_{\\ell_{2}}^{\\ast}(c) = \\frac{c(1 + 2 + 100)}{3} = \\frac{103c}{3}\n$$\n最小化器在按 $c$ 缩放下的行为是，它是 $c$ 的线性函数。具体来说，$b_{\\ell_{2}}^{\\ast}(c) = c \\cdot b_{\\ell_{2}}^{\\ast}(1)$。此性质称为尺度等变性。\n\n第 2 部分：Huber 损失的最小化器\n问题为 Huber 损失最小化器指定了一种机制，其中 $y_{1}$ 和 $y_{2}$ 的残差位于二次区域（$|r| \\leq \\delta$），而 $y_{3}$ 的残差位于线性区域（$|r|  \\delta$）。鉴于 $y_3=100$ 远大于 $y_1=1$ 和 $y_2=2$，残差 $c y_{3} - b$ 将为正。因此，该机制的条件为：$|c y_{1} - b| \\leq \\delta$、 $|c y_{2} - b| \\leq \\delta$ 和 $c y_{3} - b  \\delta$。\n在这些条件下，经验风险为：\n$$\nR_{\\mathrm{Huber}}(b; c, \\delta) = \\frac{1}{2}(c y_{1} - b)^{2} + \\frac{1}{2}(c y_{2} - b)^{2} + \\left( \\delta(c y_{3} - b) - \\frac{1}{2}\\delta^{2} \\right)\n$$\n我们通过将关于 $b$ 的导数设为零来找到最小化器 $b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)$：\n$$\n\\frac{d}{db} R_{\\mathrm{Huber}}(b; c, \\delta) = -(c y_{1} - b) - (c y_{2} - b) - \\delta = 0\n$$\n$$\n-c(y_{1} + y_{2}) + 2b - \\delta = 0 \\implies 2b = c(y_{1} + y_{2}) + \\delta\n$$\n代入 $y_{1} = 1$ 和 $y_{2} = 2$，最小化器为：\n$$\nb_{\\mathrm{Huber}}^{\\ast}(c, \\delta) = \\frac{c(1 + 2) + \\delta}{2} = \\frac{3c + \\delta}{2}\n$$\n接下来，我们通过将 $b_{\\mathrm{Huber}}^{\\ast}(c, \\delta)$ 代入机制的不等式中，来推导确保此机制有效的关于 $c$ 和 $\\delta$ 的条件：\n1.  $|c y_{1} - b_{\\mathrm{Huber}}^{\\ast}| \\leq \\delta \\implies |c - \\frac{3c + \\delta}{2}| \\leq \\delta \\implies |\\frac{-c - \\delta}{2}| \\leq \\delta$。由于 $c  0$ 且 $\\delta  0$，这变为 $\\frac{c+\\delta}{2} \\leq \\delta$，简化为 $c \\leq \\delta$。\n2.  $|c y_{2} - b_{\\mathrm{Huber}}^{\\ast}| \\leq \\delta \\implies |2c - \\frac{3c + \\delta}{2}| \\leq \\delta \\implies |\\frac{c - \\delta}{2}| \\leq \\delta$。这等价于 $-2\\delta \\leq c - \\delta \\leq 2\\delta$，或 $-\\delta \\leq c \\leq 3\\delta$。鉴于 $c0$，条件是 $0  c \\leq 3\\delta$。\n3.  $c y_{3} - b_{\\mathrm{Huber}}^{\\ast}  \\delta \\implies 100c - \\frac{3c + \\delta}{2}  \\delta \\implies 200c - 3c - \\delta  2\\delta \\implies 197c  3\\delta$，得出 $c  \\frac{3\\delta}{197}$。\n结合所有条件（$c \\leq \\delta$、$c \\leq 3\\delta$ 和 $c  \\frac{3\\delta}{197}$），所需的关系是 $\\frac{3\\delta}{197}  c \\leq \\delta$。\n\n第 3 部分：相对平方误差损失的最小化器\n相对平方误差损失 $\\ell_{\\mathrm{rel}}(r; y) = (r/y)^2$ 的经验风险由下式给出：\n$$\nR_{\\mathrm{rel}}(b; c) = \\sum_{i=1}^{3} \\left( \\frac{c y_{i} - b}{c y_{i}} \\right)^{2} = \\sum_{i=1}^{3} \\left( 1 - \\frac{b}{c y_{i}} \\right)^{2}\n$$\n我们通过将关于 $b$ 的导数设为零来找到最小化器 $b_{\\mathrm{rel}}^{\\ast}(c)$：\n$$\n\\frac{d}{db} R_{\\mathrm{rel}}(b; c) = \\sum_{i=1}^{3} 2\\left( 1 - \\frac{b}{c y_{i}} \\right) \\left(-\\frac{1}{c y_{i}}\\right) = -\\frac{2}{c} \\sum_{i=1}^{3} \\left( \\frac{1}{y_{i}} - \\frac{b}{c y_{i}^{2}} \\right) = 0\n$$\n$$\n\\sum_{i=1}^{3} \\frac{1}{y_{i}} - \\frac{b}{c} \\sum_{i=1}^{3} \\frac{1}{y_{i}^{2}} = 0\n$$\n解出 $b$ 可得：\n$$\nb_{\\mathrm{rel}}^{\\ast}(c) = c \\frac{\\sum_{i=1}^{3} 1/y_{i}}{\\sum_{i=1}^{3} 1/y_{i}^{2}}\n$$\n我们使用 $y_{1}=1, y_{2}=2, y_{3}=100$ 计算总和：\n$$\n\\sum_{i=1}^{3} \\frac{1}{y_{i}} = \\frac{1}{1} + \\frac{1}{2} + \\frac{1}{100} = \\frac{100+50+1}{100} = \\frac{151}{100}\n$$\n$$\n\\sum_{i=1}^{3} \\frac{1}{y_{i}^{2}} = \\frac{1}{1^{2}} + \\frac{1}{2^{2}} + \\frac{1}{100^{2}} = 1 + \\frac{1}{4} + \\frac{1}{10000} = \\frac{10000+2500+1}{10000} = \\frac{12501}{10000}\n$$\n代入这些总和：\n$$\nb_{\\mathrm{rel}}^{\\ast}(c) = c \\frac{151/100}{12501/10000} = c \\frac{151}{100} \\frac{10000}{12501} = \\frac{15100}{12501} c\n$$\n$b_{\\mathrm{rel}}^{\\ast}(c)$ 的尺度行为是尺度等变的。最小化器是 $c$ 的线性函数，因此将目标值按 $c$ 缩放也会将最优预测变量按 $c$ 缩放。归一化的预测变量 $b_{\\mathrm{rel}}^{\\ast}(c)/c$ 是尺度不变的。\n\n$D(c, \\delta)$ 的最终计算\n我们需要在第 2 部分的机制下求出 $D(c, \\delta) = b_{\\mathrm{Huber}}^{\\ast}(c, \\delta) - b_{\\mathrm{rel}}^{\\ast}(c)$。\n使用上面推导出的表达式：\n$$\nD(c, \\delta) = \\left( \\frac{3c + \\delta}{2} \\right) - \\left( \\frac{15100}{12501} c \\right)\n$$\n我们重排表达式，按 $c$ 和 $\\delta$ 分组各项：\n$$\nD(c, \\delta) = \\left( \\frac{3}{2} - \\frac{15100}{12501} \\right) c + \\frac{\\delta}{2}\n$$\n我们为 $c$ 的系数找到一个公分母：\n$$\n\\frac{3}{2} - \\frac{15100}{12501} = \\frac{3 \\cdot 12501 - 2 \\cdot 15100}{2 \\cdot 12501} = \\frac{37503 - 30200}{25002} = \\frac{7303}{25002}\n$$\n分数 $\\frac{7303}{25002}$ 是不可约的。将其代回，得到最终表达式：\n$$\nD(c, \\delta) = \\frac{7303}{25002} c + \\frac{\\delta}{2}\n$$",
            "answer": "$$\n\\boxed{\\frac{7303}{25002} c + \\frac{\\delta}{2}}\n$$"
        },
        {
            "introduction": "转向分类问题，我们常常使用基于“间隔”（margin）的损失函数，其中指数损失（用于AdaBoost）和逻辑损失（用于逻辑回归）是两个经典代表。它们之间的一个关键区别在于如何处理被错误分类的样本，尤其是那些远离决策边界的样本（即“噪声”或“离群点”）。本练习将通过分析和模拟梯度下降过程，揭示这两种损失函数在面对噪声数据时的行为差异，帮助你理解为什么逻辑损失在实际应用中通常表现出更强的鲁棒性 。",
            "id": "3143216",
            "problem": "给定一个二元分类问题，其标签 $y \\in \\{-1,+1\\}$，以及一个实值分数函数 $s \\in \\mathbb{R}$。边距定义为 $m = y s$。考虑作用于边距的两种损失函数：指数损失 $\\ell_{\\exp}(m)$ 和逻辑损失 $\\ell_{\\log}(m)$，定义如下\n- $\\ell_{\\exp}(m) = \\exp(-m)$,\n- $\\ell_{\\log}(m) = \\log(1+\\exp(-m))$.\n\n在固定的特征值下，将条件类别概率 $\\eta \\in [0,1]$ 定义为 $\\eta = \\mathbb{P}(Y=+1 \\mid X)$。对于一个分数 $s$，其条件风险为\n$$\nR(s;\\eta) = \\eta \\, \\ell(s) + (1-\\eta) \\, \\ell(-s),\n$$\n其中 $\\ell$ 表示应用于边距的 $\\ell_{\\exp}$ 或 $\\ell_{\\log}$。\n\n任务概述：\n1) 仅从给定的定义出发，推导关于 $s$ 的导数 $R'(s;\\eta)$，分别针对 $\\ell_{\\exp}$ 和 $\\ell_{\\log}$。你的推导必须使用链式法则，并且除了上述定义外，不得假定任何已知的快捷恒等式。\n2) 使用你推导出的 $R'(s;\\eta)$，设计一个小型模拟，该模拟在一个固定的 $\\eta$ 值下，通过对 $R(s;\\eta)$ 进行梯度下降来模仿一维训练过程。对于给定的初始分数 $s_0 \\in \\mathbb{R}$、学习率 $\\alpha  0$ 和步数 $T \\in \\mathbb{N}$，为每种损失函数独立执行以下更新：\n   - 初始化 $s \\leftarrow s_0$。\n   - 对于 $t = 1,2,\\dots,T$：计算梯度 $g_t = R'(s;\\eta)$，记录其大小 $|g_t|$，并更新 $s \\leftarrow s - \\alpha \\, g_t$。\n   - 在 $T$ 步之后，计算平均梯度大小 $\\overline{G} = \\frac{1}{T} \\sum_{t=1}^T |g_t|$。\n3) 通过比较两种损失的平均梯度大小，研究极端噪声和尾部行为的影响。对于每个测试用例 $(\\eta, s_0)$，计算比率\n$$\n\\rho = \\frac{\\overline{G}_{\\log}}{\\overline{G}_{\\exp}},\n$$\n其中 $\\overline{G}_{\\log}$ 和 $\\overline{G}_{\\exp}$ 分别是从逻辑损失和指数损失获得的平均梯度大小。\n\n要求的模拟设置：\n- 使用学习率 $\\alpha = 10^{-3}$。\n- 使用 $T = 5$ 步。\n- 使用以下 $(\\eta, s_0)$ 对的测试套件，这些测试用例旨在探究尾部和噪声极端情况：\n  - $(\\eta, s_0) = (0.99, 8)$,\n  - $(\\eta, s_0) = (0.01, -8)$,\n  - $(\\eta, s_0) = (0.50, 8)$,\n  - $(\\eta, s_0) = (0.99, 0)$,\n  - $(\\eta, s_0) = (0.01, 8)$.\n\n程序要求：\n- 实现函数，根据定义和您推导的表达式直接计算每种损失的 $R'(s;\\eta)$。\n- 对于每个测试用例，使用相同的 $(\\eta, s_0)$、$\\alpha$ 和 $T$ 对两种损失独立运行模拟，然后计算 $\\rho$。\n- 将每个 $\\rho$ 四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果顺序与上面给出的测试套件顺序一致，例如 $[\\rho_1,\\rho_2,\\rho_3,\\rho_4,\\rho_5]$。\n- 所有输出都必须是四舍五入到 $6$ 位小数的浮点数。",
            "solution": "该问题是有效的，因为它在科学上基于统计学习理论，问题设定良好、客观，并为得出唯一解提供了所有必要信息。\n\n此问题要求推导二元分类中两种常见损失函数（指数损失和逻辑损失）的条件风险梯度，并通过数值模拟比较它们在不同条件下的行为。\n\n### 第1部分：条件风险梯度 $R'(s;\\eta)$ 的推导\n\n对于给定的分数 $s$ 和条件类别概率 $\\eta = \\mathbb{P}(Y=+1 \\mid X)$，条件风险定义为：\n$$\nR(s;\\eta) = \\eta \\, \\ell(s) + (1-\\eta) \\, \\ell(-s)\n$$\n其中 $\\ell(s)$ 是真实标签为 $y=+1$ 时的损失（边距 $m = s$），$\\ell(-s)$ 是真实标签为 $y=-1$ 时的损失（边距 $m=-s$）。我们需要找到导数 $R'(s;\\eta) = \\frac{d}{ds}R(s;\\eta)$。\n\n#### 1.1 指数损失\n\n指数损失函数在边距 $m$ 上的定义为 $\\ell_{\\exp}(m) = \\exp(-m)$。\n指数损失的条件风险为：\n$$\nR_{\\exp}(s;\\eta) = \\eta \\, \\ell_{\\exp}(s) + (1-\\eta) \\, \\ell_{\\exp}(-s)\n$$\n代入 $\\ell_{\\exp}$ 的定义：\n$$\nR_{\\exp}(s;\\eta) = \\eta \\exp(-s) + (1-\\eta) \\exp(-(-s)) = \\eta \\exp(-s) + (1-\\eta) \\exp(s)\n$$\n为了求梯度，我们将 $R_{\\exp}(s;\\eta)$ 对 $s$ 求导：\n$$\nR'_{\\exp}(s;\\eta) = \\frac{d}{ds} \\left( \\eta \\exp(-s) + (1-\\eta) \\exp(s) \\right)\n$$\n利用导数的线性和对 $\\exp(-s)$ 应用链式法则：\n$$\nR'_{\\exp}(s;\\eta) = \\eta \\frac{d}{ds}(\\exp(-s)) + (1-\\eta) \\frac{d}{ds}(\\exp(s))\n$$\n$$\nR'_{\\exp}(s;\\eta) = \\eta (\\exp(-s) \\cdot (-1)) + (1-\\eta) (\\exp(s))\n$$\n$$\nR'_{\\exp}(s;\\eta) = -\\eta \\exp(-s) + (1-\\eta) \\exp(s)\n$$\n这就是指数损失的条件风险的导数。\n\n#### 1.2 逻辑损失\n\n逻辑损失函数在边距 $m$ 上的定义为 $\\ell_{\\log}(m) = \\log(1+\\exp(-m))$。\n逻辑损失的条件风险为：\n$$\nR_{\\log}(s;\\eta) = \\eta \\, \\ell_{\\log}(s) + (1-\\eta) \\, \\ell_{\\log}(-s)\n$$\n我们分别计算每一项的导数。首先，求逻辑损失函数 $\\ell_{\\log}(m)$ 关于其参数 $m$ 的导数。使用链式法则，令 $u(m) = 1 + \\exp(-m)$，则 $\\ell_{\\log}(m) = \\log(u(m))$。\n$$\n\\frac{d\\ell_{\\log}}{dm} = \\frac{d}{dm}\\log(u(m)) = \\frac{1}{u(m)} \\cdot \\frac{du}{dm} = \\frac{1}{1 + \\exp(-m)} \\cdot (-\\exp(-m)) = -\\frac{\\exp(-m)}{1 + \\exp(-m)}\n$$\n现在，我们将此结果应用于条件风险导数 $R'_{\\log}(s;\\eta) = \\frac{d}{ds}R_{\\log}(s;\\eta)$ 中的两项：\n$$\nR'_{\\log}(s;\\eta) = \\eta \\frac{d}{ds}(\\ell_{\\log}(s)) + (1-\\eta) \\frac{d}{ds}(\\ell_{\\log}(-s))\n$$\n对于第一项，我们令 $m=s$ 并使用我们刚求出的导数：\n$$\n\\frac{d}{ds}(\\ell_{\\log}(s)) = -\\frac{\\exp(-s)}{1 + \\exp(-s)}\n$$\n对于第二项，我们应用链式法则。令 $v(s) = -s$。则 $\\frac{d}{ds}(\\ell_{\\log}(-s)) = \\frac{d\\ell_{\\log}}{dv} \\cdot \\frac{dv}{ds}$。\n$$\n\\frac{d}{ds}(\\ell_{\\log}(-s)) = \\left( -\\frac{\\exp(-v)}{1 + \\exp(-v)} \\right) \\cdot (-1) = \\frac{\\exp(-(-s))}{1 + \\exp(-(-s))} = \\frac{\\exp(s)}{1 + \\exp(s)}\n$$\n结合这些结果：\n$$\nR'_{\\log}(s;\\eta) = \\eta \\left( -\\frac{\\exp(-s)}{1 + \\exp(-s)} \\right) + (1-\\eta) \\left( \\frac{\\exp(s)}{1 + \\exp(s)} \\right)\n$$\n这个表达式可以简化。令 $\\sigma(s) = \\frac{1}{1+\\exp(-s)} = \\frac{\\exp(s)}{1+\\exp(s)}$ 为 sigmoid 函数。\n注意 $\\frac{d}{ds}(\\ell_{\\log}(s)) = \\sigma(s)-1$ 且 $\\frac{d}{ds}(\\ell_{\\log}(-s)) = \\sigma(s)$。\n将这些简化形式代回 $R'_{\\log}(s;\\eta)$ 的表达式中：\n$$\nR'_{\\log}(s;\\eta) = \\eta (\\sigma(s)-1) + (1-\\eta)\\sigma(s)\n$$\n$$\nR'_{\\log}(s;\\eta) = \\eta\\sigma(s) - \\eta + \\sigma(s) - \\eta\\sigma(s)\n$$\n$$\nR'_{\\log}(s;\\eta) = \\sigma(s) - \\eta\n$$\n这就是逻辑损失条件风险的最终简化导数。\n\n### 第2部分：模拟设计\n\n该模拟实现了一个一维梯度下降算法，用于在固定的 $\\eta$ 值下最小化条件风险 $R(s;\\eta)$。\n\n1.  **初始化**：对于每个测试用例 $(\\eta, s_0)$，我们初始化分数 $s \\leftarrow s_0$。该过程对指数损失和逻辑损失独立执行。\n2.  **迭代**：总共进行 $T=5$ 步，对于每一步 $t \\in \\{1, 2, \\dots, T\\}$：\n    a.  使用相应的推导公式 $R'_{\\exp}(s;\\eta)$ 或 $R'_{\\log}(s;\\eta)$ 计算梯度 $g_t = R'(s;\\eta)$。\n    b.  记录梯度的绝对大小 $|g_t|$。\n    c.  使用梯度下降规则更新分数：$s \\leftarrow s - \\alpha g_t$，学习率为 $\\alpha = 10^{-3}$。\n3.  **聚合**：在 $T$ 步之后，计算平均梯度大小 $\\overline{G} = \\frac{1}{T} \\sum_{t=1}^T |g_t|$。对 $\\overline{G}_{\\exp}$ 和 $\\overline{G}_{\\log}$ 均执行此操作。\n4.  **比较**：对于每个测试用例，计算比率 $\\rho = \\frac{\\overline{G}_{\\log}}{\\overline{G}_{\\exp}}$。该比率量化了在给定条件下两种损失函数产生的梯度的相对大小。一个小的 $\\rho$ 值表示逻辑损失的梯度远小于指数损失的梯度，这表明逻辑损失具有更好的稳定性，并且对远离决策边界或可能被错误标记的数据点不那么敏感。\n\n所提供的测试用例旨在探索这种行为，特别是在分数 $s$ 的绝对值很大的“尾部”区域，以及对于 $\\eta$ 接近 $0$ 或 $1$ 但分数 $s$ 指向相反类别的“噪声”标签。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\n\ndef grad_exp(s: float, eta: float) - float:\n    \"\"\"\n    Computes the derivative of the conditional risk for exponential loss.\n    R'_exp(s; eta) = (1 - eta) * exp(s) - eta * exp(-s)\n    \"\"\"\n    return (1.0 - eta) * np.exp(s) - eta * np.exp(-s)\n\ndef grad_log(s: float, eta: float) - float:\n    \"\"\"\n    Computes the derivative of the conditional risk for logistic loss.\n    R'_log(s; eta) = sigma(s) - eta, where sigma(s) is the sigmoid function.\n    \"\"\"\n    # Numerically stable implementation of the sigmoid function\n    if s = 0:\n        z = np.exp(-s)\n        sigma_s = 1.0 / (1.0 + z)\n    else:\n        z = np.exp(s)\n        sigma_s = z / (1.0 + z)\n    return sigma_s - eta\n\ndef run_simulation(\n    grad_func,\n    eta: float,\n    s0: float,\n    alpha: float,\n    T: int\n) - float:\n    \"\"\"\n    Runs a 1D gradient descent simulation and returns the average gradient magnitude.\n    \n    Args:\n        grad_func: The function to compute the gradient (e.g., grad_exp or grad_log).\n        eta: The conditional class probability P(Y=+1|X).\n        s0: The initial score.\n        alpha: The learning rate.\n        T: The number of simulation steps.\n        \n    Returns:\n        The average absolute magnitude of the gradients over T steps.\n    \"\"\"\n    s = float(s0)\n    grad_magnitudes = []\n    for _ in range(T):\n        gradient = grad_func(s, eta)\n        grad_magnitudes.append(np.abs(gradient))\n        s = s - alpha * gradient\n        \n    return np.mean(grad_magnitudes)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    # Required simulation settings\n    alpha = 1e-3\n    T = 5\n\n    # Test suite of (eta, s0) pairs\n    test_cases = [\n        (0.99, 8.0),\n        (0.01, -8.0),\n        (0.50, 8.0),\n        (0.99, 0.0),\n        (0.01, 8.0),\n    ]\n\n    results = []\n    for eta, s0 in test_cases:\n        # Run simulation for logistic loss\n        avg_grad_log = run_simulation(grad_log, eta, s0, alpha, T)\n        \n        # Run simulation for exponential loss\n        avg_grad_exp = run_simulation(grad_exp, eta, s0, alpha, T)\n\n        # Compute the ratio rho. A zero check for avg_grad_exp is a safeguard,\n        # though not expected for these specific test cases.\n        if avg_grad_exp == 0.0:\n            # If both are zero, the ratio is indeterminate (let's define as 1).\n            # If only exp is zero, log/exp is infinite.\n            if avg_grad_log == 0.0:\n                rho = 1.0\n            else:\n                rho = np.inf\n        else:\n            rho = avg_grad_log / avg_grad_exp\n        \n        # Round to 6 decimal places as required.\n        results.append(round(rho, 6))\n\n    # Format the output as a comma-separated list in brackets.\n    output_str = \",\".join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "在掌握了基础损失函数后，下一步是学习如何调整它们以解决现实世界中的挑战，例如类别不平衡问题。本练习介绍了一种有效的技术——加权逻辑损失，它通过为不同类别的样本分配不同权重的惩罚，来调整模型对少数类的关注度。你将推导加权如何改变最优决策阈值，并进一步探索它对模型校准（calibration）以及精确率（precision）和召回率（recall）之间权衡的影响，从而将损失函数的设计与关键的模型性能指标直接联系起来 。",
            "id": "3143139",
            "problem": "考虑统计学习中经验风险最小化（ERM）框架下的二元分类问题。设输入为一组实例，其条件类别概率表示为 $p(x) = \\mathbb{P}(Y=+1 \\mid X=x)$，其中 $Y \\in \\{-1,+1\\}$。我们研究一种依赖于标签的、基于间隔的逻辑斯谛损失函数，其中通过类别相关的权重，使得正样本比负样本招致更大的惩罚。加权逻辑斯谛损失为正类别分配权重 $w_{+}  0$，为负类别分配权重 $w_{-}  0$。假设评分函数是一个实值函数 $f(x) \\in \\mathbb{R}$，并将每个实例的损失定义为\n$$\n\\ell(y, f(x)) = \\begin{cases}\nw_{+} \\cdot \\log\\left(1 + e^{-f(x)}\\right)  \\text{若 } y = +1, \\\\\nw_{-} \\cdot \\log\\left(1 + e^{f(x)}\\right)  \\text{若 } y = -1.\n\\end{cases}\n$$\n在给定 $x$ 时的条件期望风险是 $\\ell(Y,f(x))$ 在给定 $X=x$ 时对 $Y$ 的期望。研究 $w_{+}$ 和 $w_{-}$ 的选择如何影响：(i) 当朴素预测概率定义为学习到的分数的 sigmoid 函数值时的校准性（calibration），以及 (ii) 当基于分数的符号进行分类时的精确率-召回率（precision-recall）权衡。\n\n从期望风险和逻辑斯谛损失的核心定义出发，根据第一性原理推导最小化给定 $x$ 处条件期望风险的贝叶斯最优分数 $f^{\\star}(x)$ 的形式，表达将 sigmoid 函数应用于 $f^{\\star}(x)$ 后得到的朴素预测概率 $q(x)$，并确定当使用规则 $f^{\\star}(x) \\ge 0$ 预测正类别时对 $p(x)$ 的隐含阈值。利用这些推导设计一个算法，用于计算在加权损失下校准误差和期望精确率-召回率的经验度量。\n\n实现要求：\n- 在一个纯数学的环境中进行，其中数据集由一个固定的、确定性的条件概率向量表示，无需对标签进行采样。设数据集为 $N=99$ 个实例，其概率为\n$$\np_i = 0.01 + \\frac{i-1}{98} \\cdot 0.98 \\quad \\text{对于 } i=1,2,\\dots,99,\n$$\n这些概率在 $(0,1)$ 区间内均匀分布。\n- 对于测试组中的每一对 $(w_{+}, w_{-})$，根据您推导的表达式计算每个实例的贝叶斯最优分数 $f^{\\star}(x)$，通过将 sigmoid 函数应用于 $f^{\\star}(x)$ 来计算朴素预测概率 $q(x)$，并用它来评估校准性和分类性能。\n- 校准性评估：按如下方式计算期望校准误差（Expected Calibration Error, ECE）。将朴素预测概率的范围划分为 $[0,1]$ 上的 10 个等宽的区间（bin）。对于每个区间，计算该区间内朴素预测概率 $q(x)$ 的平均值和落入该区间的实例的真实概率 $p(x)$ 的平均值，并累加由该区间中实例所占比例加权的绝对差值。具体来说，\n$$\n\\mathrm{ECE} = \\sum_{b=1}^{10} \\frac{n_b}{N} \\cdot \\left| \\overline{q}_b - \\overline{p}_b \\right|,\n$$\n其中 $n_b$ 是区间 $b$ 中的实例数，$\\overline{q}_b$ 是区间 $b$ 中 $q(x)$ 的均值，$\\overline{p}_b$ 是区间 $b$ 中 $p(x)$ 的均值。区间的定义方式为：所有在 $[0,0.1)$ 内的朴素预测概率属于区间 1，$[0.1,0.2)$ 属于区间 2，依此类推，最后一个区间包含右端点 1。\n- 精确率和召回率：设分类规则为当 $f^{\\star}(x) \\ge 0$ 时预测为正，否则为负。计算期望真阳性（expected true positives）$\\mathrm{TP} = \\sum_{i \\in \\mathcal{P}} p_i$ 和期望假阳性（expected false positives）$\\mathrm{FP} = \\sum_{i \\in \\mathcal{P}} (1 - p_i)$，其中 $\\mathcal{P}$ 是被分类为正类的实例的索引集。同时计算期望假阴性（expected false negatives）$\\mathrm{FN} = \\sum_{i \\notin \\mathcal{P}} p_i$ 和总期望阳性（total expected positives）$\\mathrm{P} = \\sum_{i=1}^{N} p_i$。然后计算精确率 $\\mathrm{Prec} = \\mathrm{TP} / (\\mathrm{TP} + \\mathrm{FP})$（当分母非零时，否则定义为 0），以及召回率 $\\mathrm{Rec} = \\mathrm{TP} / \\mathrm{P}$（当分母非零时，否则定义为 0）。所有量均为无量纲量，且必须表示为实数。\n\n测试组：\n- 情况 1：$(w_{+}, w_{-}) = (1, 1)$。\n- 情况 2：$(w_{+}, w_{-}) = (3, 1)$。\n- 情况 3：$(w_{+}, w_{-}) = (10, 1)$。\n- 情况 4：$(w_{+}, w_{-}) = (1.5, 1)$。\n\n您的程序应生成单行输出，其中包含测试组的结果，格式为逗号分隔的列表的列表。每个内部列表包含对应案例的三个实数，顺序为 $[\\mathrm{ECE}, \\mathrm{Prec}, \\mathrm{Rec}]$。最终输出行不得包含空格，每个实数必须四舍五入到六位小数。例如，格式应为\n$$\n[[r_{1,1},r_{1,2},r_{1,3}],[r_{2,1},r_{2,2},r_{2,3}],[r_{3,1},r_{3,2},r_{3,3}],[r_{4,1},r_{4,2},r_{4,3}]].\n$$",
            "solution": "该问题提法清晰，具有科学依据，并为完整的解析和计算求解提供了所有必要信息。我们首先推导所要求的理论量，然后概述计算这些量的算法。\n\n首先，我们推导最小化条件期望风险的贝叶斯最优分数 $f^{\\star}(x)$。条件期望风险 $R(f(x) \\mid x)$ 是损失函数 $\\ell(Y, f(x))$ 在给定特征 $x$ 的条件下对标签 $Y$ 的条件分布所取的期望。令 $p(x) = \\mathbb{P}(Y=+1 \\mid X=x)$，这意味着 $\\mathbb{P}(Y=-1 \\mid X=x) = 1 - p(x)$。条件期望风险为：\n$$\nR(f(x) \\mid x) = \\mathbb{E}_{Y \\mid X=x}[\\ell(Y, f(x))] = p(x) \\cdot \\ell(+1, f(x)) + (1 - p(x)) \\cdot \\ell(-1, f(x))\n$$\n代入加权逻辑斯谛损失的定义：\n$$\nR(f(x) \\mid x) = p(x) \\cdot w_{+} \\log\\left(1 + e^{-f(x)}\\right) + (1 - p(x)) \\cdot w_{-} \\log\\left(1 + e^{f(x)}\\right)\n$$\n为找到最小化此风险的分数 $f(x)$，我们计算 $R$ 关于 $f(x)$ 的偏导数并令其为零。为简化符号，我们令 $f = f(x)$ 和 $p = p(x)$。\n$$\n\\frac{\\partial R}{\\partial f} = p \\cdot w_{+} \\left(\\frac{-e^{-f}}{1 + e^{-f}}\\right) + (1 - p) \\cdot w_{-} \\left(\\frac{e^{f}}{1 + e^{f}}\\right)\n$$\n简化括号中的项：\n$$\n\\frac{-e^{-f}}{1 + e^{-f}} = \\frac{-1}{e^{f} + 1} \\quad \\text{和} \\quad \\frac{e^{f}}{1 + e^{f}}\n$$\n因此，导数变为：\n$$\n\\frac{\\partial R}{\\partial f} = -p \\cdot w_{+} \\frac{1}{1 + e^{f}} + (1 - p) \\cdot w_{-} \\frac{e^{f}}{1 + e^{f}}\n$$\n将导数设为零以求最优分数 $f^{\\star}$：\n$$\np \\cdot w_{+} \\frac{1}{1 + e^{f^{\\star}}} = (1 - p) \\cdot w_{-} \\frac{e^{f^{\\star}}}{1 + e^{f^{\\star}}}\n$$\n假设 $1+e^{f^{\\star}} \\neq 0$，对于实数 $f^{\\star}$ 这总是成立的，因此我们可以消去这一项：\n$$\np \\cdot w_{+} = (1 - p) \\cdot w_{-} e^{f^{\\star}}\n$$\n求解 $e^{f^{\\star}}$：\n$$\ne^{f^{\\star}} = \\frac{p \\cdot w_{+}}{(1 - p) \\cdot w_{-}}\n$$\n因此，贝叶斯最优分数 $f^{\\star}(x)$ 为：\n$$\nf^{\\star}(x) = \\log\\left(\\frac{p(x)}{1 - p(x)} \\frac{w_{+}}{w_{-}}\\right) = \\log\\left(\\frac{p(x)}{1 - p(x)}\\right) + \\log\\left(\\frac{w_{+}}{w_{-}}\\right)\n$$\n这表明最优分数是真实概率的对数几率（log-odds）与一个由类别权重之比决定的偏置项之和。\n\n其次，我们通过将 sigmoid 函数 $\\sigma(z) = (1 + e^{-z})^{-1}$ 应用于 $f^{\\star}(x)$ 来求得朴素预测概率 $q(x)$。\n$$\nq(x) = \\sigma(f^{\\star}(x)) = \\frac{1}{1 + e^{-f^{\\star}(x)}}\n$$\n根据 $e^{f^{\\star}}$ 的表达式，我们有 $e^{-f^{\\star}(x)} = \\frac{(1 - p(x)) \\cdot w_{-}}{p(x) \\cdot w_{+}}$。将其代入 sigmoid 函数：\n$$\nq(x) = \\frac{1}{1 + \\frac{(1 - p(x))w_{-}}{p(x)w_{+}}} = \\frac{p(x)w_{+}}{p(x)w_{+} + (1 - p(x))w_{-}}\n$$\n如果权重是平衡的（$w_{+} = w_{-}$），那么 $q(x) = \\frac{p(x)w_{+}}{p(x)w_{+} + (1 - p(x))w_{+}} = p(x)$，这意味着朴素预测器是完美校准的。对于不平衡的权重，$q(x) \\neq p(x)$，这会导致校准误差。\n\n第三，我们确定关于 $p(x)$ 的分类阈值。分类规则在 $f^{\\star}(x) \\ge 0$ 时预测为正类别。\n$$\n\\log\\left(\\frac{p(x)}{1 - p(x)} \\frac{w_{+}}{w_{-}}\\right) \\ge 0\n$$\n由于对数函数是单调递增函数，这等价于其参数大于或等于 1：\n$$\n\\frac{p(x)}{1 - p(x)} \\frac{w_{+}}{w_{-}} \\ge 1\n$$\n$$\n\\frac{p(x)}{1 - p(x)} \\ge \\frac{w_{-}}{w_{+}}\n$$\n假设 $p(x) \\in (0,1)$，我们可以乘以 $1 - p(x)  0$：\n$$\np(x)w_{+} \\ge (1 - p(x))w_{-} \\implies p(x)w_{+} \\ge w_{-} - p(x)w_{-} \\implies p(x)(w_{+} + w_{-}) \\ge w_{-}\n$$\n因此，预测正类别的真实条件概率 $p(x)$ 的阈值为：\n$$\np(x) \\ge \\frac{w_{-}}{w_{+} + w_{-}}\n$$\n当 $w_{+}  w_{-}$ 时，该阈值低于 0.5，从而鼓励正向预测；当 $w_{+}  w_{-}$ 时，该阈值高于 0.5，从而抑制正向预测。\n\n最后，我们设计算法来计算给定数据集和测试用例所需的指标。数据集包含 $N=99$ 个实例，其概率为 $p_i = 0.01 + \\frac{i-1}{98} \\cdot 0.98$（$i=1, \\dots, 99$）。这可以简化为一个等差数列 $p_i = 0.01 \\cdot i$（$i=1, \\dots, 99$）。对于每个测试用例 $(w_{+}, w_{-})$：\n\n1.  **计算朴素概率**：对于每个 $p_i$，计算 $q_i = \\frac{p_i w_{+}}{p_i w_{+} + (1-p_i)w_{-}}$。\n2.  **计算期望校准误差 (ECE)**：\n    - 将 $q_i$ 值划分到 10 个区间中：$[0, 0.1), [0.1, 0.2), \\dots, [0.9, 1.0]$。\n    - 对于每个区间 $b$，找到其 $q_i$ 值落入该区间的实例索引集合 $I_b$。\n    - 令 $n_b = |I_b|$。如果 $n_b  0$，计算平均预测概率 $\\overline{q}_b = \\frac{1}{n_b}\\sum_{i \\in I_b} q_i$ 和平均真实概率 $\\overline{p}_b = \\frac{1}{n_b}\\sum_{i \\in I_b} p_i$。\n    - ECE 为 $\\sum_{b=1}^{10} \\frac{n_b}{N} \\cdot \\left| \\overline{q}_b - \\overline{p}_b \\right|$。\n3.  **计算精确率和召回率**：\n    - 确定被预测为正类的实例集合：$\\mathcal{P} = \\{i \\mid p_i \\ge \\frac{w_{-}}{w_{+} + w_{-}}\\}$。\n    - 计算期望真阳性：$\\mathrm{TP} = \\sum_{i \\in \\mathcal{P}} p_i$。\n    - 期望真阳性与期望假阳性之和为 $\\mathrm{TP} + \\mathrm{FP} = \\sum_{i \\in \\mathcal{P}} p_i + \\sum_{i \\in \\mathcal{P}} (1 - p_i) = \\sum_{i \\in \\mathcal{P}} 1 = |\\mathcal{P}|$。\n    - 精确率为 $\\mathrm{Prec} = \\mathrm{TP} / |\\mathcal{P}|$（如果 $|\\mathcal{P}|0$，否则为 0）。\n    - 总期望阳性为 $\\mathrm{P} = \\sum_{i=1}^{N} p_i$。\n    - 召回率为 $\\mathrm{Rec} = \\mathrm{TP} / \\mathrm{P}$（如果 $\\mathrm{P}0$，否则为 0）。\n\n对每个测试用例执行此过程以生成最终结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving and computing calibration and classification\n    metrics for a weighted logistic loss.\n    \"\"\"\n    \n    # Define the dataset of conditional probabilities\n    N = 99\n    p = np.arange(1, N + 1) / 100.0  # p_i = i/100 for i=1,...,99\n    \n    # Define the test suite\n    test_cases = [\n        (1.0, 1.0),  # Case 1\n        (3.0, 1.0),  # Case 2\n        (10.0, 1.0), # Case 3\n        (1.5, 1.0)   # Case 4\n    ]\n    \n    results = []\n    \n    for w_plus, w_minus in test_cases:\n        # 1. Compute naive predicted probabilities q_i\n        q = (p * w_plus) / (p * w_plus + (1 - p) * w_minus)\n\n        # 2. Compute Expected Calibration Error (ECE)\n        num_bins = 10\n        bins_p = [[] for _ in range(num_bins)]\n        bins_q = [[] for _ in range(num_bins)]\n        \n        for i in range(N):\n            q_val = q[i]\n            # Determine bin index. Bins are [0, 0.1), [0.1, 0.2), ..., [0.9, 1.0]\n            if q_val == 1.0:\n                bin_idx = num_bins - 1\n            else:\n                bin_idx = int(np.floor(q_val * num_bins))\n            \n            bins_p[bin_idx].append(p[i])\n            bins_q[bin_idx].append(q_val)\n            \n        ece = 0.0\n        for b in range(num_bins):\n            n_b = len(bins_p[b])\n            if n_b  0:\n                mean_p_b = np.mean(bins_p[b])\n                mean_q_b = np.mean(bins_q[b])\n                ece += (n_b / N) * np.abs(mean_q_b - mean_p_b)\n\n        # 3. Compute Precision and Recall\n        # Determine classification threshold on p\n        p_threshold = w_minus / (w_plus + w_minus)\n        \n        # Identify instances predicted as positive\n        positive_indices = np.where(p = p_threshold)[0]\n        \n        # Calculate expected TP, FP, P\n        if len(positive_indices)  0:\n            p_pos = p[positive_indices]\n            tp = np.sum(p_pos)\n            num_pos_pred = len(positive_indices)\n        else:\n            tp = 0.0\n            num_pos_pred = 0\n            \n        total_pos = np.sum(p)\n        \n        # Calculate precision\n        if num_pos_pred  0:\n            precision = tp / num_pos_pred\n        else:\n            precision = 0.0\n            \n        # Calculate recall\n        if total_pos  0:\n            recall = tp / total_pos\n        else:\n            recall = 0.0\n\n        results.append([ece, precision, recall])\n        \n    # Format the final output string as specified\n    output_str = f\"[[{','.join([f'[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]' for r in results])}]]\"\n    print(output_str)\n\nsolve()\n\n```"
        }
    ]
}