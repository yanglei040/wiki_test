{
    "hands_on_practices": [
        {
            "introduction": "核技巧的“魔力”在于它能让我们在无需进行显式高维计算的情况下，利用高维空间的几何特性。本练习将逆转这一过程：从一个给定的多项式核函数出发，我们将推导出其显式的特征映射 $\\phi(\\mathbf{x})$。这个过程揭开了核技巧的神秘面纱，并帮助你建立一个具体的直观理解——即原始空间中一个相对复杂的函数，如何能捕捉到高维空间中简单的点积运算。",
            "id": "90260",
            "problem": "在计算材料科学领域，基于核的机器学习方法，如支持向量机（SVM）和核岭回归（KRR），是从一组描述符预测材料属性的强大工具。这些方法依赖于核函数 $K(\\mathbf{x}, \\mathbf{z})$，该函数计算由描述符向量 $\\mathbf{x}$ 和 $\\mathbf{z}$ 表示的两种材料之间的广义相似性。\n\n一个关键概念是“核技巧”，它允许我们在高维特征空间中工作，而无需显式计算该空间中数据的坐标。核函数等价于特征空间 $\\mathcal{F}$ 中的内积：$K(\\mathbf{x}, \\mathbf{z}) = \\langle \\phi(\\mathbf{x}), \\phi(\\mathbf{z}) \\rangle_{\\mathcal{F}}$，其中 $\\phi$ 是从输入空间到特征空间 $\\mathcal{F}$ 的映射。\n\n考虑一个二维材料属性的模型，其中每种材料由一个双分量描述符向量 $\\mathbf{x} = [x_1, x_2]^T$ 描述。为了捕捉描述符之间的非线性关系和不同重要性，提出了一个各向异性、非齐次的多项式核：\n$$\nK(\\mathbf{x}, \\mathbf{z}) = (\\alpha x_1 z_1 + \\beta x_2 z_2 + \\gamma)^2\n$$\n这里，$\\alpha$、$\\beta$ 和 $\\gamma$ 是模型的正实值超参数。\n\n该核对应于某个维度 $N$ 的隐式映射 $\\phi: \\mathbb{R}^2 \\to \\mathbb{R}^N$。您的任务是使这个映射显式化，并用它来推导特征空间的一个属性。\n\n**推导与该核相关的特征向量的L2范数平方，$\\|\\phi(\\mathbf{x})\\|^2$。您的推导必须首先找到特征映射向量 $\\phi(\\mathbf{x})$ 的一个显式、有效的表示。**",
            "solution": "问题要求计算与核 $K(\\mathbf{x}, \\mathbf{z}) = (\\alpha x_1 z_1 + \\beta x_2 z_2 + \\gamma)^2$ 对应的特征向量 $\\phi(\\mathbf{x})$ 的L2范数平方。推导过程必须包括找到 $\\phi(\\mathbf{x})$ 的显式形式。\n\n特征映射 $\\phi$ 的定义性质是核函数是特征向量的内积：\n$$\nK(\\mathbf{x}, \\mathbf{z}) = \\phi(\\mathbf{x})^T \\phi(\\mathbf{z})\n$$\n特征向量的L2范数平方是核函数求值的一个特例，即两个参数相同时的情况：\n$$\n\\|\\phi(\\mathbf{x})\\|^2 = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}) = K(\\mathbf{x}, \\mathbf{x})\n$$\n虽然这个恒等式为最终答案提供了一条捷径，但题目要求通过特征映射 $\\phi(\\mathbf{x})$ 进行显式推导。\n\n首先，我们来寻找特征映射 $\\phi(\\mathbf{x})$。我们可以重写核函数，使其与标准非齐次多项式核的形式相匹配。我们引入一个变换后的坐标系：\n$$\nx'_1 = \\sqrt{\\alpha} x_1\n$$\n$$\nx'_2 = \\sqrt{\\beta} x_2\n$$\n令 $\\mathbf{x'} = [x'_1, x'_2]^T$。类似地，$z'_1 = \\sqrt{\\alpha} z_1$ 且 $z'_2 = \\sqrt{\\beta} z_2$，所以 $\\mathbf{z'} = [z'_1, z'_2]^T$。\n\n将这些代入核表达式，我们得到：\n$$\nK(\\mathbf{x}, \\mathbf{z}) = ((\\sqrt{\\alpha}x_1)(\\sqrt{\\alpha}z_1) + (\\sqrt{\\beta}x_2)(\\sqrt{\\beta}z_2) + \\gamma)^2 = (x'_1 z'_1 + x'_2 z'_2 + \\gamma)^2\n$$\n这是 $\\mathbf{x'}$ 的二维空间中次数为2的标准非齐次多项式核，偏移量为 $c=\\gamma$：\n$$\nK(\\mathbf{x'}, \\mathbf{z'}) = (\\mathbf{x'}^T\\mathbf{z'} + \\gamma)^2\n$$\n为了找到特征映射 $\\phi(\\mathbf{x'})$，我们展开核表达式：\n$$\nK(\\mathbf{x'}, \\mathbf{z'}) = (\\mathbf{x'}^T\\mathbf{z'} + \\gamma)^2 = (\\mathbf{x'}^T\\mathbf{z'})^2 + 2\\gamma(\\mathbf{x'}^T\\mathbf{z'}) + \\gamma^2\n$$\n$$\nK(\\mathbf{x'}, \\mathbf{z'}) = (x'_1 z'_1 + x'_2 z'_2)^2 + 2\\gamma(x'_1 z'_1 + x'_2 z'_2) + \\gamma^2\n$$\n$$\nK(\\mathbf{x'}, \\mathbf{z'}) = (x'_1)^2(z'_1)^2 + (x'_2)^2(z'_2)^2 + 2x'_1 x'_2 z'_1 z'_2 + 2\\gamma x'_1 z'_1 + 2\\gamma x'_2 z'_2 + \\gamma^2\n$$\n我们需要将其写成乘积之和 $\\sum_i \\phi_i(\\mathbf{x'}) \\phi_i(\\mathbf{z'})$ 的形式。通过观察，我们可以识别出各个分量：\n- 项 $(x'_1)^2(z'_1)^2$：表明存在一个特征分量 $\\phi_i(\\mathbf{x'}) = (x'_1)^2$。\n- 项 $(x'_2)^2(z'_2)^2$：表明存在一个特征分量 $\\phi_j(\\mathbf{x'}) = (x'_2)^2$。\n- 项 $2x'_1 x'_2 z'_1 z'_2$：表明存在 $\\phi_k(\\mathbf{x'}) = \\sqrt{2}x'_1 x'_2$。那么 $\\phi_k(\\mathbf{x'}) \\phi_k(\\mathbf{z'}) = 2x'_1 x'_2 z'_1 z'_2$。\n- 项 $2\\gamma x'_1 z'_1$：表明存在 $\\phi_l(\\mathbf{x'}) = \\sqrt{2\\gamma}x'_1$。那么 $\\phi_l(\\mathbf{x'}) \\phi_l(\\mathbf{z'}) = 2\\gamma x'_1 z'_1$。\n- 项 $2\\gamma x'_2 z'_2$：表明存在 $\\phi_m(\\mathbf{x'}) = \\sqrt{2\\gamma}x'_2$。那么 $\\phi_m(\\mathbf{x'}) \\phi_m(\\mathbf{z'}) = 2\\gamma x'_2 z'_2$。\n- 项 $\\gamma^2$：表明存在 $\\phi_p(\\mathbf{x'}) = \\gamma$。那么 $\\phi_p(\\mathbf{x'}) \\phi_p(\\mathbf{z'}) = \\gamma^2$。\n\n结合这些，$\\mathbf{x'}$ 的一个有效特征映射是一个6维向量：\n$$\n\\phi(\\mathbf{x'}) = \\begin{pmatrix} (x'_1)^2 \\\\ (x'_2)^2 \\\\ \\sqrt{2}x'_1 x'_2 \\\\ \\sqrt{2\\gamma}x'_1 \\\\ \\sqrt{2\\gamma}x'_2 \\\\ \\gamma \\end{pmatrix}\n$$\n现在，我们代回原始坐标 $x_1, x_2$：\n$$\nx'_1 = \\sqrt{\\alpha} x_1, \\quad x'_2 = \\sqrt{\\beta} x_2\n$$\n这就得到了显式的特征映射 $\\phi(\\mathbf{x})$：\n$$\n\\phi(\\mathbf{x}) = \\begin{pmatrix} (\\sqrt{\\alpha}x_1)^2 \\\\ (\\sqrt{\\beta}x_2)^2 \\\\ \\sqrt{2}(\\sqrt{\\alpha}x_1)(\\sqrt{\\beta}x_2) \\\\ \\sqrt{2\\gamma}(\\sqrt{\\alpha}x_1) \\\\ \\sqrt{2\\gamma}(\\sqrt{\\beta}x_2) \\\\ \\gamma \\end{pmatrix} = \\begin{pmatrix} \\alpha x_1^2 \\\\ \\beta x_2^2 \\\\ \\sqrt{2\\alpha\\beta} x_1 x_2 \\\\ \\sqrt{2\\alpha\\gamma} x_1 \\\\ \\sqrt{2\\beta\\gamma} x_2 \\\\ \\gamma \\end{pmatrix}\n$$\n问题要求计算该向量的L2范数平方，即 $\\|\\phi(\\mathbf{x})\\|^2 = \\phi(\\mathbf{x})^T\\phi(\\mathbf{x})$。我们通过对其分量的平方求和来计算：\n$$\n\\|\\phi(\\mathbf{x})\\|^2 = (\\alpha x_1^2)^2 + (\\beta x_2^2)^2 + (\\sqrt{2\\alpha\\beta} x_1 x_2)^2 + (\\sqrt{2\\alpha\\gamma} x_1)^2 + (\\sqrt{2\\beta\\gamma} x_2)^2 + \\gamma^2\n$$\n$$\n\\|\\phi(\\mathbf{x})\\|^2 = \\alpha^2 x_1^4 + \\beta^2 x_2^4 + 2\\alpha\\beta x_1^2 x_2^2 + 2\\alpha\\gamma x_1^2 + 2\\beta\\gamma x_2^2 + \\gamma^2\n$$\n这个表达式可以被看作是一个三项式平方的展开式。让我们对各项进行分组以使其更清晰：\n$$\n\\|\\phi(\\mathbf{x})\\|^2 = (\\alpha^2 x_1^4) + (\\beta^2 x_2^4) + \\gamma^2 + 2(\\alpha\\beta x_1^2 x_2^2) + 2(\\alpha\\gamma x_1^2) + 2(\\beta\\gamma x_2^2)\n$$\n这是 $(A+B+C)^2 = A^2+B^2+C^2+2AB+2AC+2BC$ 的展开式，其中 $A = \\alpha x_1^2$，$B = \\beta x_2^2$，以及 $C = \\gamma$。\n因此，该表达式可以简化为：\n$$\n\\|\\phi(\\mathbf{x})\\|^2 = (\\alpha x_1^2 + \\beta x_2^2 + \\gamma)^2\n$$\n作为检验，我们可以使用恒等式 $\\|\\phi(\\mathbf{x})\\|^2 = K(\\mathbf{x}, \\mathbf{x})$。\n$$\nK(\\mathbf{x}, \\mathbf{x}) = (\\alpha x_1 x_1 + \\beta x_2 x_2 + \\gamma)^2 = (\\alpha x_1^2 + \\beta x_2^2 + \\gamma)^2\n$$\n结果相符，验证了推导的正确性。",
            "answer": "$$\n\\boxed{(\\alpha x_1^2 + \\beta x_2^2 + \\gamma)^2}\n$$"
        },
        {
            "introduction": "理解了核技巧的内在机制后，我们现在来见证它的威力。本实践将挑战著名的异或（XOR）问题，这是一个简单的线性分类器无法解决的经典非线性问题。通过动手实现一个核化的感知机，你将亲眼看到，将数据映射到更高维度的空间后，问题如何变得线性可分，从而让算法在标准线性版本失败的地方成功收敛。",
            "id": "3183909",
            "problem": "您将实现并比较两种用于二元分类的学习算法：一种是在输入空间中的标准感知机，另一种是使用多项式核以隐式地在更高维特征空间中工作的核化感知机。目标是展示核技巧如何使感知机能够解决异或关系，并量化其收敛行为。\n\n从以下基本概念开始：\n- 一个数据集由输入向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 和标签 $y \\in \\{-1,+1\\}$ 组成。\n- 输入空间中的线性分类器使用决策函数 $f(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$。\n- 感知机学习算法在样本 $(\\mathbf{x},y)$ 被错误分类时，执行形式为 $\\mathbf{w} \\leftarrow \\mathbf{w} + y \\mathbf{x}$ 和 $b \\leftarrow b + y$ 的更新，并以确定性的方式循环遍历训练集，直到收敛或达到最大轮次数。\n- 一个核函数 $k(\\mathbf{x},\\mathbf{z})$ 通过一个特征映射 $\\Phi$ 对应于某个特征空间中的内积，即 $k(\\mathbf{x},\\mathbf{z}) = \\langle \\Phi(\\mathbf{x}), \\Phi(\\mathbf{z}) \\rangle$。\n- 带有偏移量 $c$ 的 $d$ 次多项式核是 $k(\\mathbf{x},\\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z} + c)^d$。在本问题中，您必须始终使用 $c = 1$。\n\n定义您的程序必须执行的以下任务：\n1. 实现一个在输入空间中运行的确定性感知机，它：\n   - 使用标签 $y \\in \\{-1,+1\\}$。\n   - 通过一个常数特征来增广每个输入，以便将偏置项 $b$ 合并到权重向量中，从而使更新变为单个向量更新。\n   - 将权重向量初始化为零向量。\n   - 在每一轮中，以固定顺序 $(i=0,1,\\dots,n-1)$ 扫描训练样本，并在 $y_i (\\mathbf{w}^\\top \\mathbf{x}_i) \\le 0$ 时应用更新。\n   - 使用决策规则 $\\operatorname{sign}(s)$，当 $s \\ge 0$ 时返回 $+1$，否则返回 $-1$。\n   - 返回首次达到零训练误差的那个轮次（含）之前执行的总更新次数。如果在给定的最大轮次数内未能收敛，则返回 $-1$。\n\n2. 推导并实现一个仅使用多项式核 $k(\\mathbf{x},\\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z} + 1)^d$ 进行核函数求值的核化感知机：\n   - 用分配给训练样本的系数 $\\alpha_i \\ge 0$ 来表示分类器，其预测函数为 $f(\\mathbf{x}) = \\operatorname{sign}\\left(\\sum_{j=1}^n \\alpha_j y_j k(\\mathbf{x}_j,\\mathbf{x})\\right)$。\n   - 将所有系数 $\\alpha_i$ 初始化为零。\n   - 当 $(\\mathbf{x}_i,y_i)$ 被错误分类时，更新 $\\alpha_i \\leftarrow \\alpha_i + 1$。\n   - 每一轮中以固定的确定性顺序 $(i=0,1,\\dots,n-1)$ 进行迭代，并使用与上述相同的决策规则和停止准则。\n   - 返回收敛前的总更新次数，如果在分配的轮次内没有收敛，则返回 $-1$。\n\n构建并使用以下数据集：\n- $\\mathbb{R}^2$ 中的 XOR 数据集：输入 $\\mathbf{x} \\in \\{(0,0),(0,1),(1,0),(1,1)\\}$，当只有一个坐标等于 $1$ 时标签为 $y=+1$，否则为 $y=-1$。按给定顺序，标签向量明确为 $[-1,+1,+1,-1]$。\n- $\\mathbb{R}^2$ 中的 AND 数据集：输入相同，但仅当输入为 $(1,1)$ 时标签为 $y=+1$，否则为 $y=-1$。按给定顺序，标签向量明确为 $[-1,-1,-1,+1]$。\n\n本问题不使用角度单位。不涉及物理单位。\n\n测试套件和要求的输出：\n- 运行以下四个测试用例，每个用例由数据集名称、多项式次数 $d$ 和最大轮次数定义：\n  1. XOR, $d=2$, $\\text{max\\_epochs}=50$。\n  2. XOR, $d=1$, $\\text{max\\_epochs}=50$。\n  3. AND, $d=2$, $\\text{max\\_epochs}=50$。\n  4. XOR, $d=3$, $\\text{max\\_epochs}=50$。\n- 对于每个测试用例，计算两个整数：输入空间感知机收敛所需的更新次数，以及使用指定次数的核化感知机收敛所需的更新次数。如果某个方法在最大轮次数内未能收敛，则该方法报告 $-1$。\n- 您的程序应生成单行输出，其中包含一个含四个元素的列表，每个元素本身是一个双元素列表，格式为 [linear_updates, kernel_updates]，对应相应的测试用例，无空格。例如：\"[[a,b],[c,d],[e,f],[g,h]]\"，其中每个字母代表如上指定的整数。\n\n科学真实性和推导期望：\n- 在您的解决方案中，证明为何 XOR 数据集在输入空间中不是线性可分的，以及为何它在适当的多项式特征空间中变得线性可分，从而解释为什么当次数 $d \\ge 2$ 时核化感知机能够收敛，而线性感知机不能。\n- 您的推理应严格基于线性可分性、感知机更新规则以及核作为特征空间内积的定义。",
            "solution": "该问题要求实现并比较两种二元分类算法：一种是在输入空间中运行的标准感知机，另一种是使用多项式核的核化感知机。分析将侧重于它们解决线性和非线性可分问题的能力，分别以 AND 和 XOR 数据集为例。\n\n### 1. 输入空间中的标准感知机\n\n标准感知机算法学习一个线性判别函数来分离两个类别。对于输入向量 $\\mathbf{x} \\in \\mathbb{R}^d$，决策函数为 $f(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$，其中 $\\mathbf{w} \\in \\mathbb{R}^d$ 是权重向量， $b \\in \\mathbb{R}$ 是偏置。问题规定，$\\operatorname{sign}(s)$ 在 $s \\ge 0$ 时返回 $+1$，在 $s  0$ 时返回 $-1$。\n\n为了简化学习算法，可以将偏置 $b$ 合并到权重向量中。这可以通过为每个输入向量 $\\mathbf{x}$ 增广一个常数特征（通常为 $1$）来实现，形成一个增广向量 $\\mathbf{x}' = [\\mathbf{x}^\\top, 1]^\\top \\in \\mathbb{R}^{d+1}$。类似地，增广权重向量为 $\\mathbf{w}' = [\\mathbf{w}^\\top, b]^\\top \\in \\mathbb{R}^{d+1}$。决策函数则变为 $f(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{w}'^\\top \\mathbf{x}')$。\n\n学习过程是迭代的。算法循环遍历训练样本 $(\\mathbf{x}_i, y_i)$，其中 $i \\in \\{0, \\dots, n-1\\}$ 且 $y_i \\in \\{-1, +1\\}$。权重向量 $\\mathbf{w}'$ 初始化为零向量。对于每个样本，如果模型输出的符号与真实标签不匹配，则发生错分。根据问题规定，错分的条件是 $y_i(\\mathbf{w}'^\\top \\mathbf{x}'_i) \\le 0$。一旦错分，权重向量将根据以下规则进行更新：\n$$\n\\mathbf{w}' \\leftarrow \\mathbf{w}' + y_i \\mathbf{x}'_i\n$$\n算法逐轮（epoch）进行，一轮代表对训练数据的一次完整遍历。根据感知机收敛定理，如果一个训练数据集是线性可分的，该算法保证能在有限次更新内找到一个分离超平面。如果数据不是线性可分的，算法将不会收敛，并将无限循环下去。\n\n### 2. 核化感知机\n\n核技巧使得像感知机这样的线性分类器能够学习非线性决策边界。其核心思想是通过一个映射 $\\Phi: \\mathbb{R}^d \\to \\mathcal{F}$ 将输入数据 $\\mathbf{x}$ 隐式地映射到一个更高维的特征空间 $\\mathcal{F}$，然后在这个特征空间 $\\mathcal{F}$ 中应用线性分类器。\n\n特征空间 $\\mathcal{F}$ 中的权重向量 $\\mathbf{W}$ 可以用其“对偶形式”表示为映射后训练样本的线性组合：\n$$\n\\mathbf{W} = \\sum_{j=1}^n \\alpha_j y_j \\Phi(\\mathbf{x}_j)\n$$\n其中 $\\alpha_j$ 是系数，初始化为 $0$。对于一个新数据点 $\\mathbf{x}$ 的决策函数是 $f(\\mathbf{x}) = \\operatorname{sign}(\\mathbf{W}^\\top \\Phi(\\mathbf{x}))$。代入 $\\mathbf{W}$ 的对偶形式可得：\n$$\nf(\\mathbf{x}) = \\operatorname{sign}\\left( \\left( \\sum_{j=1}^n \\alpha_j y_j \\Phi(\\mathbf{x}_j) \\right)^\\top \\Phi(\\mathbf{x}) \\right) = \\operatorname{sign}\\left( \\sum_{j=1}^n \\alpha_j y_j \\langle \\Phi(\\mathbf{x}_j), \\Phi(\\mathbf{x}) \\rangle \\right)\n$$\n核技巧的关键在于，特征空间中的内积 $\\langle \\Phi(\\mathbf{x}_j), \\Phi(\\mathbf{x}) \\rangle$ 可以在原始输入空间中通过一个核函数 $k(\\mathbf{x}_j, \\mathbf{x})$ 高效地计算。决策函数完全通过核函数求值来表示：\n$$\nf(\\mathbf{x}) = \\operatorname{sign}\\left(\\sum_{j=1}^n \\alpha_j y_j k(\\mathbf{x}_j, \\mathbf{x})\\right)\n$$\n更新规则也在此对偶表示中推导。在原始特征空间中的一次更新 $\\mathbf{W} \\leftarrow \\mathbf{W} + y_i \\Phi(\\mathbf{x}_i)$，对应于将被错误分类的点 $(\\mathbf{x}_i, y_i)$ 的系数 $\\alpha_i$ 增加。更新规则即为 $\\alpha_i \\leftarrow \\alpha_i + 1$。\n\n问题指定了次数为 $d$ 且偏移量 $c=1$ 的多项式核：\n$$\nk(\\mathbf{x}, \\mathbf{z}) = (\\mathbf{x}^\\top \\mathbf{z} + 1)^d\n$$\n偏移量 $c=1$ 至关重要，因为它隐式地处理了偏置项。例如，对于 $d=2$ 和 $\\mathbf{x} \\in \\mathbb{R}^2$，核函数 $k(\\mathbf{x},\\mathbf{z}) = (\\mathbf{x}^\\top\\mathbf{z}+1)^2$ 对应于一个6维特征空间中的内积，其基向量类似于 $(1, \\sqrt{2}x_1, \\sqrt{2}x_2, x_1^2, x_2^2, \\sqrt{2}x_1x_2)$。特征映射中的常数特征 $1$ 使得学习偏置项成为可能。\n\n### 3. 数据集与可分性分析\n\n**AND 数据集**：输入为 $\\{(0,0), (0,1), (1,0), (1,1)\\}$，标签为 $\\{-1, -1, -1, +1\\}$。此数据集在 $\\mathbb{R}^2$ 中是线性可分的。例如，直线 $x_1 + x_2 - 1.5 = 0$ 完美地将点 $(1,1)$ 与其他三个点分开。由于数据是线性可分的，根据感知机收敛定理，标准感知机和核化感知机（对于任何次数 $d \\ge 1$）都保证能够收敛。\n\n**XOR 数据集**：输入相同，但标签为 $\\{-1, +1, +1, -1\\}$。\n- **在输入空间 $\\mathbb{R}^2$ 中**：此数据集不是线性可分的。一个几何证明很有启发性：正类样本点（连接 $(0,1)$ 和 $(1,0)$ 的线段）的凸包与负类样本点（连接 $(0,0)$ 和 $(1,1)$ 的线段）的凸包相交。由于它们的凸包相交，因此没有任何一条直线可以将这两组点分开。因此，标准感知机不保证收敛，并将无法找到解决方案。\n- **在多项式特征空间中**：核技巧可以克服这一限制。考虑次数 $d=2$ 的多项式核。该核将数据隐式映射到一个更高维空间，该空间中包含一个与乘积 $x_1x_2$ 成比例的特征。让我们在一个简化的、坐标为 $(x_1, x_2, x_1x_2)$ 的特征空间中检验其可分性：\n  - $(0,0) \\to (0,0,0)$，标签 $-1$\n  - $(0,1) \\to (0,1,0)$，标签 $+1$\n  - $(1,0) \\to (1,0,0)$，标签 $+1$\n  - $(1,1) \\to (1,1,1)$，标签 $-1$\n在这个三维空间中，这些点是线性可分的。例如，平面 $z_1+z_2 - 2z_3 - 1 = 0$ 可以分离这些点：\n  - $(0,0,0) \\to -1$。标签为 $-1$。$y_i(\\text{score}) > 0$。正确。\n  - $(0,1,0) \\to 0$。标签为 $+1$。$y_i(\\text{score}) \\ge 0$。正确。\n  - $(1,0,0) \\to 0$。标签为 $+1$。$y_i(\\text{score}) \\ge 0$。正确。\n  - $(1,1,1) \\to 1+1-2-1 = -1$。标签为 $-1$。$y_i(\\text{score}) > 0$。正确。\n由于XOR数据集在由 $d \\ge 2$ 的多项式核所引出的特征空间中变得线性可分，因此核化感知机保证能够收敛。对于 $d=1$，核函数为 $k(\\mathbf{x},\\mathbf{z}) = \\mathbf{x}^\\top\\mathbf{z}+1$，这等价于输入空间中的线性分类器，因此对于XOR问题它会失败。对于 $d=3$，特征空间更加丰富，数据仍然可分，因此也保证收敛。\n\n### 4. 预期结果\n\n基于此分析，各测试用例的预期结果如下：\n1.  **XOR, $d=2$**：线性感知机失败（返回 $-1$），核感知机收敛。\n2.  **XOR, $d=1$**：两者都失败（返回 $-1$），因为 $d=1$ 的核对应于线性分类器。\n3.  **AND, $d=2$**：两者都收敛，因为AND数据集是线性可分的。\n4.  **XOR, $d=3$**：线性感知机失败（返回 $-1$），核感知机收敛。\n\n现在，实现将执行这些算法并报告收敛前的更新次数。",
            "answer": "```python\nimport numpy as np\n\ndef standard_perceptron(X, y, max_epochs):\n    \"\"\"\n    Implements the standard perceptron algorithm in input space.\n    \"\"\"\n    n_samples, n_features = X.shape\n    # Augment X to handle bias term b\n    X_aug = np.c_[X, np.ones(n_samples)]\n    w = np.zeros(n_features + 1)\n    total_updates = 0\n\n    for _ in range(max_epochs):\n        updates_in_epoch = 0\n        for i in range(n_samples):\n            score = X_aug[i] @ w\n            # Update on misclassification\n            if y[i] * score = 0:\n                w += y[i] * X_aug[i]\n                total_updates += 1\n                updates_in_epoch += 1\n        \n        # Check for convergence\n        if updates_in_epoch == 0:\n            return total_updates\n            \n    # Failed to converge\n    return -1\n\ndef kernel_perceptron(X, y, degree, max_epochs):\n    \"\"\"\n    Implements the kernelized perceptron algorithm.\n    \"\"\"\n    n_samples, _ = X.shape\n    alphas = np.zeros(n_samples)\n    \n    # Pre-compute the Gram matrix using the polynomial kernel\n    gram_matrix = (X @ X.T + 1) ** degree\n    total_updates = 0\n\n    for _ in range(max_epochs):\n        updates_in_epoch = 0\n        for i in range(n_samples):\n            # The decision function uses kernel evaluations\n            score = (alphas * y) @ gram_matrix[:, i]\n            # Update on misclassification\n            if y[i] * score = 0:\n                alphas[i] += 1\n                total_updates += 1\n                updates_in_epoch += 1\n        \n        # Check for convergence\n        if updates_in_epoch == 0:\n            return total_updates\n            \n    # Failed to converge\n    return -1\n\ndef solve():\n    \"\"\"\n    Runs the defined test suite and prints the formatted results.\n    \"\"\"\n    # Define datasets\n    X_xor = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n    y_xor = np.array([-1., 1., 1.,-1.])\n    \n    X_and = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n    y_and = np.array([-1., -1., -1., 1.])\n    \n    datasets = {\n        'XOR': (X_xor, y_xor),\n        'AND': (X_and, y_and)\n    }\n    \n    # Define test cases\n    test_cases = [\n        # (dataset_name, polynomial_degree, max_epochs)\n        ('XOR', 2, 50),\n        ('XOR', 1, 50),\n        ('AND', 2, 50),\n        ('XOR', 3, 50),\n    ]\n\n    results = []\n    for d_name, degree, max_e in test_cases:\n        X, y = datasets[d_name]\n        \n        # Run standard perceptron\n        linear_updates = standard_perceptron(X, y, max_e)\n        \n        # Run kernelized perceptron\n        kernel_updates = kernel_perceptron(X, y, degree, max_e)\n\n        results.append([linear_updates, kernel_updates])\n        \n    # Format the output string to match the problem specification \"[[a,b],[c,d],...]\"\n    output_str = f\"[{','.join([str(r).replace(' ', '') for r in results])}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "核技巧并非“一招鲜，吃遍天”的万能解法，核函数的选择至关重要。本练习旨在探讨不同类型核函数之间的权衡，特别是具有全局特性的多项式核与具有局部特性的径向基函数（RBF）核。通过构建并分析一方表现优异而另一方失效的特定场景，你将更深刻地理解核函数的性质必须与数据内在结构相匹配，才能取得良好的泛化性能。",
            "id": "3183944",
            "problem": "您必须编写一个完整的程序，比较核岭回归 (KRR) 使用两种不同核函数——高斯径向基函数 (RBF) 核和多项式核——在三个确定性分类任务上的表现。目的是展示一个核函数的选择严重影响性能的案例：具体来说，是构建具有全局结构的数据，使得多项式核能够成功，而 RBF 核因其局部性而失败，并分析覆盖情况。\n\n在您的推导和实现中使用以下基本原理：\n- KRR 优化 Tikhonov 正则化经验风险\n$$\n\\min_{f \\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^{n} (f(x_i) - y_i)^2 + \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2,\n$$\n其中 $\\mathcal{H}$ 是与一个正定核 $k(\\cdot,\\cdot)$ 相关联的再生核希尔伯特空间 (RKHS)，$\\lambda \\gt 0$，且 $\\{(x_i, y_i)\\}_{i=1}^{n}$ 是训练样本对。根据表示定理 (Representer Theorem)，最优解的形式为 $f(\\cdot) = \\sum_{i=1}^{n} \\alpha_i k(x_i, \\cdot)$，并且系数 $\\alpha \\in \\mathbb{R}^n$ 满足\n$$\n\\alpha = (K + \\lambda I)^{-1} y,\n$$\n其中 $K \\in \\mathbb{R}^{n \\times n}$，$K_{ij} = k(x_i, x_j)$，且 $y \\in \\mathbb{R}^n$。\n- 对于标签 $y \\in \\{-1, +1\\}$ 的分类问题，预测值为 $\\hat{y} = \\operatorname{sign}(f(x))$，并使用平局打破规则 $\\operatorname{sign}(0) = +1$。\n\n使用的核函数：\n- 高斯 RBF 核：$k_{\\mathrm{RBF}}(x, z) = \\exp\\left(-\\gamma \\lVert x - z \\rVert_2^2 \\right)$，其中带宽参数 $\\gamma \\gt 0$。\n- 多项式核：$k_{\\mathrm{poly}}(x, z) = (x^\\top z + c)^d$，其中次数 $d \\in \\mathbb{N}$，系数 $c \\ge 0$。\n\n您的程序必须使用上述方程以对偶形式实现 KRR，并将其应用于三个测试用例，同时使用两种核函数。对于每个测试用例，计算多项式核和 RBF 核的分类准确率，然后计算它们的差值。准确率是正确分类的测试标签在 $[0,1]$ 区间内的比例。\n\n需要通过代码输出展示和解释的直觉：\n- 在具有全局分离方向的高维设置中，低次多项式核可以捕捉全局结构，因为它编码了诸如内积和跨坐标求和等全局代数交互。相比之下，具有非常小长度尺度（即大 $\\gamma$ 值）的高斯 RBF 核是高度局部的，因此对于远离训练集的测试点，其预测值会塌陷至 $0$；结合平局打破规则 $\\operatorname{sign}(0) = +1$，如果训练集没有覆盖全局结构，这将在具有全局结构的任务上导致较差的准确率。\n\n实现以下确定性测试套件。不涉及任何物理单位。所有角度（如果出现）必须以弧度为单位。百分比必须表示为 $[0,1]$ 区间内的小数。\n\n测试用例 A（RBF 失败但多项式成功的全局结构）：\n- 维度：$d = 20$。\n- 训练输入：$n = 10$ 个点 $x_i = t_i \\cdot \\mathbf{1}_d$，其中 $t_i = -1 + \\frac{2i}{9}$，对于 $i \\in \\{0, 1, \\dots, 9\\}$，$\\mathbf{1}_d \\in \\mathbb{R}^d$ 是全 1 向量。训练标签：$y_i = \\operatorname{sign}(t_i)$，平局打破规则为 $\\operatorname{sign}(0) = +1$（此处实际未触发）。\n- 测试输入：$m = 21$ 个点 $x = s_j \\cdot \\mathbf{1}_d$，其中 $s_j = -2 + \\frac{4j}{20}$，对于 $j \\in \\{0, 1, \\dots, 20\\}$。测试标签：$y = \\operatorname{sign}(s_j)$，其中 $\\operatorname{sign}(0) = +1$。\n- 核函数与超参数：\n  - 多项式核：次数 $d_{\\mathrm{poly}} = 1$，系数 $c = 0$，岭参数 $\\lambda = 10^{-6}$。\n  - 高斯 RBF 核：$\\gamma = 50$，岭参数 $\\lambda = 10^{-6}$。\n- 预期：多项式核能捕捉所有坐标上的全局线性规则；具有大 $\\gamma$ 值的 RBF 核过于局部，无法在整个定义域宽度上泛化。\n\n测试用例 B（两者均成功的理想情况）：\n- 维度：$d = 2$。\n- 训练输入：六个点，形成两个分离良好的簇，\n  - -1 类：$(-1.0, -1.0)$、 $(-1.1, -0.9)$、 $(-0.9, -1.1)$，\n  - +1 类：$(1.0, 1.0)$、 $(1.1, 0.9)$、 $(0.9, 1.1)$。\n- 测试输入：四个点，\n  - -1 类：$(-1.2, -0.8)$、 $(-0.8, -1.2)$，\n  - +1 类：$(1.2, 0.8)$、 $(0.8, 1.2)$。\n- 核函数与超参数：\n  - 多项式核：次数 $d_{\\mathrm{poly}} = 1$，系数 $c = 0$，岭参数 $\\lambda = 10^{-6}$。\n  - 高斯 RBF 核：$\\gamma = 1$，岭参数 $\\lambda = 10^{-6}$。\n- 预期：两种核函数都达到近乎完美的准确率。\n\n测试用例 C（边缘情况：次数不匹配导致多项式欠拟合而 RBF 成功）：\n- 维度：$d = 1$。\n- 训练输入：四个点 $x \\in \\{-1.0, -0.2, 0.2, 1.0\\}$，标签为 $y = \\operatorname{sign}(x)$，且 $\\operatorname{sign}(0) = +1$。\n- 测试输入：五个点 $x \\in \\{-0.8, -0.1, 0.0, 0.1, 0.8\\}$，标签为 $y = \\operatorname{sign}(x)$，且 $\\operatorname{sign}(0) = +1$。\n- 核函数与超参数：\n  - 多项式核：次数 $d_{\\mathrm{poly}} = 2$，系数 $c = 0$，岭参数 $\\lambda = 10^{-6}$。\n  - 高斯 RBF 核：$\\gamma = 2$，岭参数 $\\lambda = 10^{-6}$。\n- 预期：偶次多项式核无法很好地表示奇函数决策边界，可能会失败；具有适度局部性的 RBF 核则会成功。\n\n程序要求：\n- 按照描述实现 KRR。对每个测试用例，训练两个独立的模型（多项式核和 RBF 核），计算测试输入上的预测标签，并计算每个核的准确率（作为 $[0,1]$ 区间内的浮点数）。同时计算差值 $\\text{accuracy}_{\\mathrm{poly}} - \\text{accuracy}_{\\mathrm{RBF}}$。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个由方括号括起来的、逗号分隔的列表，内含 9 个结果：对于测试用例 A，输出多项式核准确率、RBF 核准确率以及它们的差值；然后按顺序对测试用例 B 和测试用例 C 重复这三个数值。例如，输出必须类似于\n\"[a1,a2,a3,a4,a5,a6,a7,a8,a9]\"\n其中每个 $a_k$ 都是一个十进制数。将每个报告的数字四舍五入到恰好 4 位小数。",
            "solution": "该问题要求在三个不同的分类任务中，比较使用多项式核与高斯径向基函数 (RBF) 核的核岭回归 (KRR)。问题的核心在于展示核函数的内在属性——特别是其全局性与局部性——如何决定其对特定数据结构的适用性。\n\n首先，我们对核岭回归算法进行形式化。给定一个包含 $n$ 个数据对的训练集 $\\{(x_i, y_i)\\}_{i=1}^{n}$，其中 $x_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\{-1, +1\\}$，KRR 旨在从一个由核函数 $k(\\cdot, \\cdot)$ 定义的再生核希尔伯特空间 (RKHS) $\\mathcal{H}$ 中寻找一个函数 $f$，以最小化正则化平方损失：\n$$\n\\min_{f \\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^{n} (f(x_i) - y_i)^2 + \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2\n$$\n在此，$\\lambda  0$ 是正则化参数，它控制着拟合数据与在 $\\mathcal{H}$ 中保持小范数之间的权衡，这有助于防止过拟合。表示定理 (Representer Theorem) 指出，最优解 $f^*$ 具有一个关于训练数据的有限展开式：\n$$\nf^*(x) = \\sum_{i=1}^{n} \\alpha_i k(x_i, x)\n$$\n对偶系数 $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^\\top \\in \\mathbb{R}^n$ 可通过求解以下线性系统获得：\n$$\n(K + n\\lambda I) \\alpha = y\n$$\n其中 $K$ 是 $n \\times n$ 的格拉姆矩阵 (Gram matrix)，其元素为 $K_{ij} = k(x_i, x_j)$，$I$ 是 $n \\times n$ 的单位矩阵，$y = (y_1, \\dots, y_n)^\\top$。请注意：问题陈述中将损失除以 $1/n$ 进行归一化，并给出 $\\alpha = (K + \\lambda I)^{-1}y$。如果我们对 $\\lambda$ 乘以 $n$ 进行重新缩放，这与上面的公式是等价的。我们将遵循问题陈述中给出的公式：$\\alpha = (K + \\lambda I)^{-1} y$。\n\n一旦系数 $\\alpha$ 确定，就可以通过评估 $f^*(x_{\\text{test}})$ 来对新的测试点 $x_{\\text{test}}$ 进行预测。对于分类问题，预测标签 $\\hat{y}$ 由该评估值的符号给出：\n$$\n\\hat{y} = \\operatorname{sign}(f^*(x_{\\text{test}})) = \\operatorname{sign}\\left(\\sum_{i=1}^{n} \\alpha_i k(x_i, x_{\\text{test}})\\right)\n$$\n我们使用指定的平局打破规则 $\\operatorname{sign}(0) = +1$。\n\n待比较的两种核函数是：\n1.  **多项式核**：$k_{\\mathrm{poly}}(x, z) = (x^\\top z + c)^d$。该核函数基于原始特征的多项式组合计算相似度。当次数 $d=1$ 且 $c=0$ 时，它简化为线性核 $k(x,z) = x^\\top z$，该核能捕捉全局线性关系。其特征空间由坐标间的全局交互构成。\n2.  **高斯 RBF 核**：$k_{\\mathrm{RBF}}(x, z) = \\exp(-\\gamma \\lVert x - z \\rVert_2^2)$。该核函数的值取决于点之间的欧几里得距离。参数 $\\gamma$ 控制核的“宽度”；大的 $\\gamma$ 对应小的长度尺度，使核函数高度*局部*化。点 $x$ 处的预测主要受与其非常接近的训练点的影响。\n\n实现将包括计算核矩阵的函数、求解 KRR 系数 $\\alpha$ 的函数以及进行预测的函数。这些将被应用于三个测试用例。\n\n**测试用例 A 分析**：这个案例旨在突出局部核在具有全局结构的任务上的失败。数据位于 $\\mathbb{R}^{20}$ 空间中由全 1 向量 $\\mathbf{1}_{20}$ 定义的一条直线上。标签仅由标量倍数的符号决定。次数 $d=1$ 的多项式核（即线性核）完全适合学习这种全局线性规则，并有望实现完美的准确率。而具有非常大 $\\gamma=50$ 的 RBF 核是高度局部化的。远离训练数据区间 $[-1, 1]$（在标量投影上）的测试点，在欧几里得距离上将远离所有训练点。核函数评估值 $k(x_{\\text{test}}, x_i)$ 对于所有 $i$ 都会接近于零。因此，预测函数 $f(x_{\\text{test}})$ 将趋近于 $0$，预测结果将默认为 $\\operatorname{sign}(0) = +1$。这将导致所有真实标签为 $-1$ 且位于训练数据范围之外的测试点被错误分类。\n\n**测试用例 B 分析**：这个案例展示了一个在 $\\mathbb{R}^2$ 空间中简单的、线性可分的分类问题。两个不同的、分离良好的点簇对应于两个类别。线性（多项式次数为 1）核和 RBF 核都有望表现完美。线性核可以找到分离超平面，而 RBF 核可以轻松地分离两个数据密度的“凸起”。\n\n**测试用例 C 分析**：这个案例展示了多项式核因函数形式（奇偶性）不匹配而导致的失效模式。任务是学习一维函数 $\\operatorname{sign}(x)$，它是一个*奇*函数 ($f(-x) = -f(x)$)。所选的多项式核 $k(x, z) = (xz)^2 = x^2z^2$ 生成一个由*偶*函数组成的 RKHS，因为其张成的空间中的任何函数形式都为 $f(x) = w x^2$（其中 $w$ 是某个权重）。偶函数从根本上不适合逼近奇函数，从而导致性能不佳。在对称定义域上，对 $\\operatorname{sign}(x)$ 的最佳偶函数逼近是 $f(x)=0$，这将导致所有预测均为 $+1$。而 RBF 核作为一个通用逼近器，不受此类对称性约束，并且足够灵活，可以正确地在 $x=0$ 处建模决策边界。\n\n程序将实现这些步骤，计算每个核在每个测试用例上的准确率，并计算它们的差值，从而为理论分析提供定量证据。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Kernel Ridge Regression with Polynomial and RBF kernels\n    across three specified test cases.\n    \"\"\"\n\n    # --- Kernel Functions ---\n    def rbf_kernel(X1, X2, gamma):\n        \"\"\"Computes the Gaussian RBF kernel matrix.\"\"\"\n        # Using the identity ||x-z||^2 = ||x||^2 + ||z||^2 - 2x^Tz for vectorization\n        X1_norm_sq = np.sum(X1**2, axis=1, keepdims=True)\n        X2_norm_sq = np.sum(X2**2, axis=1, keepdims=True)\n        dot_product = X1 @ X2.T\n        distsq = X1_norm_sq + X2_norm_sq.T - 2 * dot_product\n        return np.exp(-gamma * distsq)\n\n    def polynomial_kernel(X1, X2, d, c):\n        \"\"\"Computes the Polynomial kernel matrix.\"\"\"\n        return (X1 @ X2.T + c)**d\n\n    # --- KRR Algorithm ---\n    def train_krr(X_train, y_train, kernel_func, lmbda, **kernel_params):\n        \"\"\"Trains the KRR model by solving for the dual coefficients alpha.\"\"\"\n        n = X_train.shape[0]\n        K = kernel_func(X_train, X_train, **kernel_params)\n        # alpha = (K + lambda*I)^-1 * y\n        alpha = np.linalg.solve(K + lmbda * np.identity(n), y_train)\n        return alpha\n\n    def predict_krr(X_test, X_train, alpha, kernel_func, **kernel_params):\n        \"\"\"Makes predictions using the trained KRR model.\"\"\"\n        K_test = kernel_func(X_test, X_train, **kernel_params)\n        f_test = K_test @ alpha\n        # Classification with sign(0) = +1 convention\n        return np.where(f_test >= 0, 1.0, -1.0)\n\n    def calculate_accuracy(y_pred, y_true):\n        \"\"\"Calculates classification accuracy.\"\"\"\n        return np.mean(y_pred == y_true)\n\n    results = []\n    \n    # === Test Case A ===\n    d_a = 20\n    n_a = 10\n    t_a = -1 + 2 * np.arange(n_a) / (n_a - 1)\n    X_train_a = t_a[:, np.newaxis] * np.ones((1, d_a))\n    y_train_a = np.sign(t_a)\n    y_train_a[y_train_a == 0] = 1 # Apply sign(0) = +1 rule\n    \n    m_a = 21\n    s_a = -2 + 4 * np.arange(m_a) / (m_a - 1)\n    X_test_a = s_a[:, np.newaxis] * np.ones((1, d_a))\n    y_test_a = np.sign(s_a)\n    y_test_a[y_test_a == 0] = 1  # Apply sign(0) = +1 rule\n\n    poly_params_a = {'d': 1, 'c': 0, 'lmbda': 1e-6}\n    rbf_params_a = {'gamma': 50, 'lmbda': 1e-6}\n\n    # Polynomial kernel on Test Case A\n    alpha_poly_a = train_krr(X_train_a, y_train_a, polynomial_kernel, poly_params_a['lmbda'], d=poly_params_a['d'], c=poly_params_a['c'])\n    y_pred_poly_a = predict_krr(X_test_a, X_train_a, alpha_poly_a, polynomial_kernel, d=poly_params_a['d'], c=poly_params_a['c'])\n    acc_poly_a = calculate_accuracy(y_pred_poly_a, y_test_a)\n\n    # RBF kernel on Test Case A\n    alpha_rbf_a = train_krr(X_train_a, y_train_a, rbf_kernel, rbf_params_a['lmbda'], gamma=rbf_params_a['gamma'])\n    y_pred_rbf_a = predict_krr(X_test_a, X_train_a, alpha_rbf_a, rbf_kernel, gamma=rbf_params_a['gamma'])\n    acc_rbf_a = calculate_accuracy(y_pred_rbf_a, y_test_a)\n    \n    results.extend([acc_poly_a, acc_rbf_a, acc_poly_a - acc_rbf_a])\n\n    # === Test Case B ===\n    X_train_b = np.array([[-1.0, -1.0], [-1.1, -0.9], [-0.9, -1.1], [1.0, 1.0], [1.1, 0.9], [0.9, 1.1]])\n    y_train_b = np.array([-1, -1, -1, 1, 1, 1])\n    X_test_b = np.array([[-1.2, -0.8], [-0.8, -1.2], [1.2, 0.8], [0.8, 1.2]])\n    y_test_b = np.array([-1, -1, 1, 1])\n    \n    poly_params_b = {'d': 1, 'c': 0, 'lmbda': 1e-6}\n    rbf_params_b = {'gamma': 1, 'lmbda': 1e-6}\n\n    # Polynomial kernel on Test Case B\n    alpha_poly_b = train_krr(X_train_b, y_train_b, polynomial_kernel, poly_params_b['lmbda'], d=poly_params_b['d'], c=poly_params_b['c'])\n    y_pred_poly_b = predict_krr(X_test_b, X_train_b, alpha_poly_b, polynomial_kernel, d=poly_params_b['d'], c=poly_params_b['c'])\n    acc_poly_b = calculate_accuracy(y_pred_poly_b, y_test_b)\n\n    # RBF kernel on Test Case B\n    alpha_rbf_b = train_krr(X_train_b, y_train_b, rbf_kernel, rbf_params_b['lmbda'], gamma=rbf_params_b['gamma'])\n    y_pred_rbf_b = predict_krr(X_test_b, X_train_b, alpha_rbf_b, rbf_kernel, gamma=rbf_params_b['gamma'])\n    acc_rbf_b = calculate_accuracy(y_pred_rbf_b, y_test_b)\n    \n    results.extend([acc_poly_b, acc_rbf_b, acc_poly_b - acc_rbf_b])\n\n    # === Test Case C ===\n    X_train_c = np.array([-1.0, -0.2, 0.2, 1.0])[:, np.newaxis]\n    y_train_c = np.sign(X_train_c.flatten())\n    y_train_c[y_train_c == 0] = 1 # Apply sign(0) = +1 rule\n    \n    X_test_c = np.array([-0.8, -0.1, 0.0, 0.1, 0.8])[:, np.newaxis]\n    y_test_c = np.sign(X_test_c.flatten())\n    y_test_c[y_test_c == 0] = 1 # Apply sign(0) = +1 rule\n    \n    poly_params_c = {'d': 2, 'c': 0, 'lmbda': 1e-6}\n    rbf_params_c = {'gamma': 2, 'lmbda': 1e-6}\n\n    # Polynomial kernel on Test Case C\n    alpha_poly_c = train_krr(X_train_c, y_train_c, polynomial_kernel, poly_params_c['lmbda'], d=poly_params_c['d'], c=poly_params_c['c'])\n    y_pred_poly_c = predict_krr(X_test_c, X_train_c, alpha_poly_c, polynomial_kernel, d=poly_params_c['d'], c=poly_params_c['c'])\n    acc_poly_c = calculate_accuracy(y_pred_poly_c, y_test_c)\n\n    # RBF kernel on Test Case C\n    alpha_rbf_c = train_krr(X_train_c, y_train_c, rbf_kernel, rbf_params_c['lmbda'], gamma=rbf_params_c['gamma'])\n    y_pred_rbf_c = predict_krr(X_test_c, X_train_c, alpha_rbf_c, rbf_kernel, gamma=rbf_params_c['gamma'])\n    acc_rbf_c = calculate_accuracy(y_pred_rbf_c, y_test_c)\n\n    results.extend([acc_poly_c, acc_rbf_c, acc_poly_c - acc_rbf_c])\n\n    # Final print statement in the exact required format\n    formatted_results = [f\"{r:.4f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}