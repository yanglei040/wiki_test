{
    "hands_on_practices": [
        {
            "introduction": "The core principle of the maximal margin classifier is to find a separating hyperplane that is as far as possible from the nearest points of any class. This practice solidifies this geometric intuition by analyzing a dataset confined to two parallel strips . By reasoning about this specific configuration, you will develop a clear understanding of how the margin width is determined by the points at the inner edges of the class distributions and identify exactly which points become the crucial support vectors.",
            "id": "3147125",
            "problem": "Consider a binary classification dataset in two dimensions constructed as follows. Fix real numbers $\\delta > 0$, $\\epsilon$ satisfying $0 < \\epsilon < \\delta$, a span half-width $L > 0$, and an integer $m \\ge 1$. For $i \\in \\{1,\\dots,m\\}$, draw $x_i \\sim \\mathrm{Unif}([-L,L])$ independently and set\n- positive-class points: $(x_i, \\delta + \\eta_i)$ with label $+1$, where each $\\eta_i$ satisfies $|\\eta_i| \\le \\epsilon$,\n- negative-class points: $(x_i, -\\delta + \\tilde\\eta_i)$ with label $-1$, where each $\\tilde\\eta_i$ satisfies $|\\tilde\\eta_i| \\le \\epsilon$.\n\nThus, the data lie in a thin horizontal strip of width approximately $2\\delta$ and long extent $2L$ along the $x$-axis, with the two classes concentrated near the parallel lines $y = +\\delta$ and $y = -\\delta$. Consider training a linear Support Vector Machine (SVM) on this dataset in the hard-margin case (feasible since $0<\\epsilon<\\delta$) and also in the soft-margin case with penalty parameter $C > 0$.\n\nWhich of the following statements are true? Select all that apply.\n\nA. In the hard-margin case with $|\\eta_i|, |\\tilde\\eta_i| \\le \\epsilon < \\delta$, the maximal-margin separating hyperplane is the horizontal line $y=0$, and the margin width measured orthogonally to the strip equals $2(\\delta - \\epsilon)$.\n\nB. If $\\epsilon = 0$, then every training point is a support vector; moreover, any arbitrarily small changes to the $x$-coordinates alone (keeping all $y$-coordinates at $\\pm \\delta$) do not change the support vector set.\n\nC. For $\\epsilon > 0$, the support vectors are exactly those points that attain the smallest absolute $y$ within each class (i.e., those with $y$-coordinates closest to $0$); changing $x$-coordinates alone cannot change which points are support vectors, but arbitrarily small changes in the $y$-coordinates within the strip can swap which points are support vectors.\n\nD. In the soft-margin case with very large $C$, a single positive-class point moved slightly below the line $y=0$ cannot become a support vector as long as the downward perturbation is sufficiently small.\n\nE. In the soft-margin case with very small $C$, the optimal separating hyperplane must be parallel to the strip (i.e., vertical) to minimize the hinge loss.",
            "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Dataset**: A binary classification dataset in $\\mathbb{R}^2$.\n- **Positive-class points**: A set of $m$ points of the form $(x_i, y_i)$, where $y_i = \\delta + \\eta_i$, with label $t_i = +1$.\n- **Negative-class points**: A set of $m$ points of the form $(x_j, y_j)$, where $y_j = -\\delta + \\tilde\\eta_j$, with label $t_j = -1$.\n- **Parameters and variables**:\n    - $\\delta > 0$ is a real number.\n    - $\\epsilon$ is a real number satisfying $0 < \\epsilon < \\delta$.\n    - $L > 0$ is a real number.\n    - $m \\ge 1$ is an integer.\n    - The $x$-coordinates for all points are drawn independently from a uniform distribution $\\mathrm{Unif}([-L, L])$.\n    - The perturbations $\\eta_i$ and $\\tilde\\eta_j$ are real numbers satisfying $|\\eta_i| \\le \\epsilon$ and $|\\tilde\\eta_j| \\le \\epsilon$.\n- **Models**:\n    - Hard-margin Support Vector Machine (SVM).\n    - Soft-margin SVM with penalty parameter $C > 0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, relying on the established mathematical theory of Support Vector Machines. It is well-posed; the data generation process is clearly defined, and the condition $0 < \\epsilon < \\delta$ ensures the two classes are linearly separable. The range of positive-class $y$-coordinates is $[\\delta - \\epsilon, \\delta + \\epsilon]$ and for negative-class points is $[-\\delta - \\epsilon, -\\delta + \\epsilon]$. Since $\\delta - \\epsilon > 0$ and $-\\delta + \\epsilon < 0$, a gap exists between the two classes. The questions posed are objective and can be rigorously answered using the principles of statistical learning theory. There are no contradictions, ambiguities, or factual errors that would invalidate the problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution will be derived.\n\n### Derivation and Option Analysis\n\nA linear SVM seeks a separating hyperplane of the form $\\mathbf{w}^T\\mathbf{z} + b = 0$, where $\\mathbf{z} = (x, y)^T$ and $\\mathbf{w} = (w_1, w_2)^T$. The hard-margin SVM aims to maximize the margin width, $2/\\|\\mathbf{w}\\|$, subject to the constraints $t_i(\\mathbf{w}^T\\mathbf{z}_i + b) \\ge 1$ for all data points $(\\mathbf{z}_i, t_i)$.\n\nGiven the horizontal separation of the data strips, the maximal-margin hyperplane must be horizontal. Any non-zero $w_1$ component would cause the term $w_1 x_i$ to vary over the range $[-L|w_1|, L|w_1|]$. To satisfy the constraints $t_i(w_1 x_i + w_2 y_i + b) \\ge 1$ for all possible values of $x_i \\in [-L, L]$, $\\|\\mathbf{w}\\|$ would need to be larger than if $w_1=0$. Therefore, to minimize $\\|\\mathbf{w}\\|$, the optimal solution must have $w_1=0$. The separating hyperplane is of the form $w_2 y + b = 0$, which is a horizontal line.\n\nFor such a hyperplane, the constraints are:\n1. For positive points ($t_i=+1$): $w_2 y_i + b \\ge 1$ for all $y_i \\in [\\delta - \\epsilon, \\delta + \\epsilon]$.\n2. For negative points ($t_i=-1$): $-(w_2 y_j + b) \\ge 1 \\implies w_2 y_j + b \\le -1$ for all $y_j \\in [-\\delta - \\epsilon, -\\delta + \\epsilon]$.\n\nTo satisfy these for all points, they must hold for the \"worst-case\" $y$-coordinates, which are those closest to the other class.\n1. $w_2 (\\min y_i) + b \\ge 1 \\implies w_2(\\delta - \\epsilon) + b \\ge 1$.\n2. $w_2 (\\max y_j) + b \\le -1 \\implies w_2(-\\delta + \\epsilon) + b \\le -1$.\n\nTo maximize the margin $2/|w_2|$, we must minimize $|w_2|$. Assuming $w_2 > 0$ (so positive points are on the \"positive\" side), this occurs when the inequalities become equalities:\n$w_2(\\delta - \\epsilon) + b = 1$\n$w_2(-\\delta + \\epsilon) + b = -1$\n\nSubtracting the second equation from the first yields $w_2((\\delta - \\epsilon) - (-\\delta + \\epsilon)) = 2$, which simplifies to $w_2(2\\delta - 2\\epsilon) = 2$, so $w_2 = \\frac{1}{\\delta - \\epsilon}$.\nAdding the two equations yields $2b = 0$, so $b=0$.\n\nThe maximal-margin separating hyperplane is $y/(\\delta-\\epsilon) = 0$, which is $y=0$. The margin hyperplanes are $y/(\\delta-\\epsilon) = \\pm 1$, i.e., $y = \\pm(\\delta - \\epsilon)$.\n\n**A. In the hard-margin case with $|\\eta_i|, |\\tilde\\eta_i| \\le \\epsilon < \\delta$, the maximal-margin separating hyperplane is the horizontal line $y=0$, and the margin width measured orthogonally to the strip equals $2(\\delta - \\epsilon)$.**\n\nAs derived above, the maximal-margin separating hyperplane is indeed $y=0$. The margin width is given by $2/\\|\\mathbf{w}\\| = 2/\\sqrt{w_1^2 + w_2^2} = 2/|w_2| = 2(\\delta - \\epsilon)$. The strip is horizontal, and the measurement is orthogonal to it (i.e., in the $y$-direction), which is consistent with the calculation. This statement is a direct consequence of the SVM optimization on this dataset.\n\nVerdict: **Correct**.\n\n**B. If $\\epsilon = 0$, then every training point is a support vector; moreover, any arbitrarily small changes to the $x$-coordinates alone (keeping all $y$-coordinates at $\\pm \\delta$) do not change the support vector set.**\n\nIf $\\epsilon = 0$, all positive-class points are on the line $y=\\delta$, and all negative-class points are on the line $y=-\\delta$.\nFollowing the same derivation with $\\epsilon=0$, we find $w_2 = 1/\\delta$ and $b=0$.\nThe margin hyperplanes are $y = \\pm \\delta$.\nA point $\\mathbf{z}_i$ with label $t_i$ is a support vector if $t_i(\\mathbf{w}^T\\mathbf{z}_i + b) = 1$.\n- For any positive point $\\mathbf{z}_i = (x_i, \\delta)$ with $t_i=+1$: $t_i(\\mathbf{w}^T\\mathbf{z}_i + b) = 1 \\cdot ((0, 1/\\delta)^T(x_i, \\delta) + 0) = (1/\\delta)\\delta = 1$. So, all positive points are support vectors.\n- For any negative point $\\mathbf{z}_j = (x_j, -\\delta)$ with $t_j=-1$: $t_j(\\mathbf{w}^T\\mathbf{z}_j + b) = -1 \\cdot ((0, 1/\\delta)^T(x_j, -\\delta) + 0) = -1 \\cdot ((1/\\delta)(-\\delta)) = 1$. So, all negative points are support vectors.\nThus, every training point is a support vector.\n\nThe support vector condition $t_i(y_i/\\delta) = 1$ depends only on the $y$-coordinate. Changing the $x$-coordinate of any point does not affect this condition. Since all points are initially support vectors, they all remain support vectors after their $x$-coordinates are changed.\n\nVerdict: **Correct**.\n\n**C. For $\\epsilon > 0$, the support vectors are exactly those points that attain the smallest absolute $y$ within each class (i.e., those with $y$-coordinates closest to $0$); changing $x$-coordinates alone cannot change which points are support vectors, but arbitrarily small changes in the $y$-coordinates within the strip can swap which points are support vectors.**\n\nAs established, the margin hyperplanes are $y = \\pm(\\delta-\\epsilon)$. Support vectors are the points that lie on these hyperplanes.\n- A positive-class point $(x_i, y_i)$ is a support vector if $y_i = \\delta-\\epsilon$. This corresponds to the minimum possible $y$-value (and thus minimum absolute $y$-value, since $y>0$) for a positive-class point.\n- A negative-class point $(x_j, y_j)$ is a support vector if $y_j = -(\\delta-\\epsilon) = -\\delta+\\epsilon$. This corresponds to the maximum possible $y$-value for a negative-class point, which is the minimum absolute $y$-value ($|y_j| = \\delta-\\epsilon$).\nSo the first part of the statement is correct.\n\nSince the condition for being a support vector depends only on the $y$-coordinate, changing $x$-coordinates alone has no effect on which points are support vectors. The second part is also correct.\n\nFinally, consider two positive-class points $P_1 = (x_1, y_1)$ and $P_2 = (x_2, y_2)$ where $y_1 = \\delta-\\epsilon$ (making $P_1$ a support vector) and $y_2 = \\delta-\\epsilon+\\Delta$ for some small $\\Delta > 0$ (making $P_2$ not a support vector). We can apply an arbitrarily small perturbation to their $y$-coordinates: change $y_1$ to $y_1' = y_1 + \\Delta/2$ and $y_2$ to $y_2' = y_2 - \\Delta = \\delta-\\epsilon$. Now, $P_2$ has become a support vector, while $P_1$ is no longer one. This demonstrates that arbitrarily small changes in $y$-coordinates can change the support vector set. The third part is correct.\n\nVerdict: **Correct**.\n\n**D. In the soft-margin case with very large $C$, a single positive-class point moved slightly below the line $y=0$ cannot become a support vector as long as the downward perturbation is sufficiently small.**\n\nA very large penalty parameter $C$ makes the soft-margin SVM behave like a hard-margin SVM if the data is linearly separable. Let's move a positive-class point $(x_k, y_k)$ to a new position $(x_k, y_k')$ where $y_k'=-\\mu$ for some small $\\mu > 0$. The data may or may not remain linearly separable, depending on the $y$-coordinates of the negative points.\nCase 1: The new dataset is linearly separable (e.g., if $-\\mu > \\max_j y_j^-$). With large $C$, the SVM finds the new hard-margin solution. The point $(x_k, -\\mu)$ is now the positive-class point with the smallest $y$-value. The separating boundary will be determined by this point and the highest negative-class point. Therefore, $(x_k, -\\mu)$ will lie on the new margin and be a support vector.\nCase 2: The new dataset is not linearly separable. The SVM must tolerate some error. Points for which the constraint $t_i(\\mathbf{w}^T\\mathbf{z}_i+b) \\ge 1$ is violated will have a slack penalty $\\xi_i > 0$. In the dual formulation, this corresponds to the Lagrange multiplier for that point being at its upper bound, $\\alpha_i = C$. Since $C>0$, we have $\\alpha_i > 0$, which by definition means the point is a support vector. The point $(x_k, -\\mu)$ is a positive point in the negative region, so it will almost certainly be misclassified by any reasonable hyperplane, leading to $\\xi_k > 0$ and thus making it a support vector.\nIn all plausible scenarios, the moved point becomes a support vector. The statement that it *cannot* become a support vector is false.\n\nVerdict: **Incorrect**.\n\n**E. In the soft-margin case with very small $C$, the optimal separating hyperplane must be parallel to the strip (i.e., vertical) to minimize the hinge loss.**\n\nIn the soft-margin SVM, the objective is to minimize $\\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_i \\xi_i$. As $C \\to 0$, the first term, $\\frac{1}{2}\\|\\mathbf{w}\\|^2$, dominates the objective function. To minimize this term, the SVM will choose $\\mathbf{w}$ to be as close to the zero vector as possible. The optimal solution converges to $\\mathbf{w} = \\mathbf{0}$. A vertical hyperplane, $x=k$, corresponds to $\\mathbf{w} = (w_1, 0)$ with $w_1 \\ne 0$, so $\\|\\mathbf{w}\\| > 0$. This solution is clearly suboptimal compared to $\\mathbf{w}=\\mathbf{0}$ for sufficiently small $C$.\nFurthermore, the claim is that a vertical hyperplane is chosen \"to minimize the hinge loss.\" A vertical hyperplane would be a very poor separator for these horizontally-arranged data strips and would result in a large hinge loss, as roughly half the points in each class would be on the wrong side. A horizontal hyperplane would yield a much smaller (or zero) hinge loss. The statement is flawed in multiple ways.\n\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "While the concept of maximizing geometric margin is elegant, its practical application depends critically on how we measure distance. This exercise explores a scenario where features have vastly different scales, revealing that the Support Vector Machine is not inherently invariant to such transformations . By comparing the classifier in a distorted 'observed' space to one in a 'whitened' space, you will gain a hands-on appreciation for why feature normalization is an essential prerequisite for training a meaningful SVM.",
            "id": "3147185",
            "problem": "A binary dataset in $2$ dimensions is constructed as follows. First, define four points in an intrinsic, standardized coordinate system $z \\in \\mathbb{R}^2$:\n- Class $+1$: $z^{(+)}_1 = (1,1)$, $z^{(+)}_2 = (1.5,1.5)$.\n- Class $-1$: $z^{(-)}_1 = (-1,-1)$, $z^{(-)}_2 = (-1.5,-1.5)$.\n\nThese points lie on the line $z_2 = z_1$ and are symmetric about the origin. The observed features are then obtained by an anisotropic linear scaling $x = S z$ with\n$$\nS = \\mathrm{diag}(M,\\,1),\n$$\nwhere $M \\gg 1$ quantifies an extreme variance difference between the first and second feature (the first feature has much larger scale). Thus, in the observed coordinates $x \\in \\mathbb{R}^2$ the four points are\n$$\nx^{(+)}_1 = (M,\\,1),\\quad x^{(+)}_2 = (1.5M,\\,1.5),\\quad x^{(-)}_1 = (-M,\\,-1),\\quad x^{(-)}_2 = (-1.5M,\\,-1.5).\n$$\nAssume the data are centered at the origin (which holds by symmetry), and consider the hard-margin maximal margin classifier, that is, the Support Vector Machine (SVM) that minimizes the squared Euclidean norm of the normal vector subject to perfect separation constraints.\n\nWhitening is defined here as the linear transform that rescales the observed features to unit variance and zero mean. In this construction, because the second feature already has unit scale and the first is scaled by $M$, whitening reduces to the linear map $W = S^{-1} = \\mathrm{diag}(1/M,\\,1)$ applied to $x$.\n\nUsing only the definition of the maximal margin classifier as the solution of the convex program that minimizes the squared norm of the normal vector subject to linear separation constraints, reason about the orientation of the separating hyperplane in the observed space versus after whitening. Which statement is most accurate?\n\nA. In observed coordinates with $M \\gg 1$, the maximal margin normal vector is proportional to $(M,\\,1)$, producing a nearly vertical separating line (hyperplane almost parallel to the $x_2$-axis). After whitening by $W = S^{-1}$, the maximal margin normal vector is proportional to $(1,\\,1)$, yielding the balanced diagonal separator orthogonal to $x_1 + x_2 = 0$.\n\nB. Whitening leaves the maximal margin normal vector unchanged, because Support Vector Machines (SVMs) are invariant to feature scaling.\n\nC. Even without whitening, switching to a soft-margin SVM with any finite penalty parameter $C$ will rotate the separator in observed coordinates to be proportional to $(1,\\,1)$.\n\nD. In observed coordinates the maximal margin normal vector is proportional to $(1,\\,1)$ for all $M$, but whitening rotates it toward $(M,\\,1)$ when $M \\gg 1$.",
            "solution": "The user wants me to analyze the behavior of a hard-margin Support Vector Machine (SVM) under anisotropic feature scaling.\n\n### Step 1: Problem Validation\n\n**1. Extract Givens:**\n-   **Dataset:** A binary dataset in $2$ dimensions.\n-   **Intrinsic Coordinates ($z \\in \\mathbb{R}^2$):**\n    -   Class $+1$: $z^{(+)}_1 = (1,1)$, $z^{(+)}_2 = (1.5,1.5)$.\n    -   Class $-1$: $z^{(-)}_1 = (-1,-1)$, $z^{(-)}_2 = (-1.5,-1.5)$.\n-   **Anisotropic Scaling:** The observed coordinates $x \\in \\mathbb{R}^2$ are obtained from the intrinsic coordinates $z$ by the linear transformation $x = Sz$, where the scaling matrix is $S = \\mathrm{diag}(M, 1)$ with $M \\gg 1$.\n-   **Observed Coordinates ($x \\in \\mathbb{R}^2$):**\n    -   Class $+1$: $x^{(+)}_1 = (M,1)$, $x^{(+)}_2 = (1.5M,1.5)$.\n    -   Class $-1$: $x^{(-)}_1 = (-M,-1)$, $x^{(-)}_2 = (-1.5M,-1.5)$.\n-   **Classifier:** A hard-margin maximal margin classifier (SVM), which minimizes the squared Euclidean norm of the normal vector, $\\|w\\|^2$, subject to perfect separation.\n-   **Whitening:** A linear transformation $W = S^{-1} = \\mathrm{diag}(1/M, 1)$ applied to the observed features $x$. This transformation maps $x$ back to $z$, i.e., $z = Wx$.\n-   **Assumption:** The data is centered at the origin. This is true by construction.\n\n**2. Validate Using Extracted Givens:**\n-   **Scientific Grounding:** The problem is based on the standard and well-established mathematical framework of Support Vector Machines and linear algebra. All concepts are fundamental to statistical learning. The setup is scientifically sound.\n-   **Well-Posedness:** The data is explicitly defined and is linearly separable. The optimization problem for a hard-margin SVM on linearly separable data is a convex problem with a unique solution for the separating hyperplane. The question is clear and answerable through mathematical derivation.\n-   **Objectivity:** The problem is stated using precise mathematical language, free from subjective or ambiguous terms. The condition $M \\gg 1$ is a standard convention for analyzing asymptotic behavior.\n\n**3. Verdict and Action:**\nThe problem statement is valid. It is a well-posed problem in the field of statistical learning that tests a core property of SVMs. I will proceed with the solution.\n\n### Step 2: Derivation of the Solution\n\nThe maximal margin classifier finds a separating hyperplane defined by a normal vector $w$ and an offset $b$ that solves the following optimization problem:\n$$\n\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n$$\nsubject to:\n$$\ny_i (w^T x_i + b) \\geq 1 \\quad \\text{for all data points } (x_i, y_i).\n$$\n\nBy the symmetry of the given data points (for every point $x_i$ with label $y_i$, the point $-x_i$ exists with label $-y_i$), the optimal separating hyperplane must pass through the origin. This implies the offset $b=0$. The problem simplifies to:\n$$\n\\min_{w} \\frac{1}{2} \\|w\\|^2 \\quad \\text{subject to} \\quad y_i (w^T x_i) \\geq 1.\n$$\n\n#### Analysis in the Observed Space ($x$)\n\nThe data points in the observed space are $x^{(+)}_1 = (M,1)$, $x^{(+)}_2 = (1.5M,1.5)$, $x^{(-)}_1 = (-M,-1)$, and $x^{(-)}_2 = (-1.5M,-1.5)$.\nAll these points are collinear and lie on the line passing through the origin with direction vector $v = (M,1)$. Each point $x_i$ can be written as $x_i = c_i v$ for some scalar $c_i$. The scalars are $c_1=1$, $c_2=1.5$ for class $+1$ and $c_3=-1$, $c_4=-1.5$ for class $-1$. The corresponding labels are $y_1=1, y_2=1, y_3=-1, y_4=-1$.\n\nLet the normal vector in this space be $w_x$. The constraints are:\n$y_i (w_x^T (c_i v)) \\geq 1 \\implies (y_i c_i) (w_x^T v) \\geq 1$.\n\nLet's compute the products $y_i c_i$:\n-   $y_1 c_1 = (+1)(1) = 1$\n-   $y_2 c_2 = (+1)(1.5) = 1.5$\n-   $y_3 c_3 = (-1)(-1) = 1$\n-   $y_4 c_4 = (-1)(-1.5) = 1.5$\n\nAll values of $y_i c_i$ are positive. Let $K_x = w_x^T v$. The constraints for all four points become $1 \\cdot K_x \\geq 1$ and $1.5 \\cdot K_x \\geq 1$. For both to hold, we only need to satisfy the stricter one derived from the points with minimum $y_i c_i$, which is $K_x \\geq 1$.\n\nThe optimization problem is to minimize $\\frac{1}{2} \\|w_x\\|^2$ subject to the single constraint $w_x^T v \\geq 1$, which is $M w_{x1} + w_{x2} \\geq 1$. The minimum of $\\|w_x\\|^2$ will occur when the constraint is active, i.e., at equality: $M w_{x1} + w_{x2} = 1$.\n\nWe use the method of Lagrange multipliers. Let $L(w_x, \\lambda) = \\frac{1}{2}(w_{x1}^2 + w_{x2}^2) - \\lambda(M w_{x1} + w_{x2} - 1)$.\nTaking partial derivatives with respect to $w_{x1}$ and $w_{x2}$:\n$$\n\\frac{\\partial L}{\\partial w_{x1}} = w_{x1} - \\lambda M = 0 \\implies w_{x1} = \\lambda M\n$$\n$$\n\\frac{\\partial L}{\\partial w_{x2}} = w_{x2} - \\lambda = 0 \\implies w_{x2} = \\lambda\n$$\nSubstituting these into the constraint equation:\n$$\nM(\\lambda M) + \\lambda = 1 \\implies \\lambda(M^2+1) = 1 \\implies \\lambda = \\frac{1}{M^2+1}\n$$\nThe optimal normal vector is therefore:\n$$\nw_x = \\left(\\frac{M}{M^2+1}, \\frac{1}{M^2+1}\\right)\n$$\nThis vector is proportional to $(M,1)$. For $M \\gg 1$, the direction of $w_x$ approaches $(1,0)$, which is a horizontal vector. The separating hyperplane is defined by $w_x^T x = 0$, which is $M x_1 + x_2 = 0$. This is the line $x_2 = -M x_1$. For large $M$, this is a very steep line, i.e., a nearly vertical separating line, which is almost parallel to the $x_2$-axis.\n\n#### Analysis in the Whitened Space ($z$)\n\nThe whitening transformation $z=Wx$ maps the observed points $x_i$ back to the intrinsic points $z_i$. The analysis is performed on the points $z^{(+)}_1 = (1,1)$, $z^{(+)}_2 = (1.5,1.5)$, $z^{(-)}_1 = (-1,-1)$, and $z^{(-)}_2 = (-1.5,-1.5)$.\nThese points lie on the line through the origin with direction vector $u = (1,1)$.\nThe same logic applies. Let the normal vector in this space be $w_z$. The optimization problem is to minimize $\\frac{1}{2} \\|w_z\\|^2$ subject to $y_i(w_z^T z_i) \\geq 1$. This simplifies to minimizing $\\frac{1}{2} \\|w_z\\|^2$ subject to $w_z^T u \\geq 1$.\n\nThe minimum occurs when $w_z^T u = 1$, which is $w_{z1} + w_{z2} = 1$.\nUsing Lagrange multipliers for $L(w_z, \\lambda) = \\frac{1}{2}(w_{z1}^2 + w_{z2}^2) - \\lambda(w_{z1} + w_{z2} - 1)$:\n$$\n\\frac{\\partial L}{\\partial w_{z1}} = w_{z1} - \\lambda = 0 \\implies w_{z1} = \\lambda\n$$\n$$\n\\frac{\\partial L}{\\partial w_{z2}} = w_{z2} - \\lambda = 0 \\implies w_{z2} = \\lambda\n$$\nSubstituting into the constraint:\n$$\n\\lambda + \\lambda = 1 \\implies 2\\lambda=1 \\implies \\lambda = \\frac{1}{2}\n$$\nThe optimal normal vector is:\n$$\nw_z = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)\n$$\nThis vector is proportional to $(1,1)$. The separating hyperplane is $w_z^T z = 0$, which is $z_1 + z_2 = 0$. This is the line $z_2 = -z_1$, a diagonal line with slope $-1$. It is orthogonal to the line $z_2 = z_1$ on which the data lies. This is a balanced separator.\n\n### Step 3: Option-by-Option Analysis\n\n**A. In observed coordinates with $M \\gg 1$, the maximal margin normal vector is proportional to $(M,\\,1)$, producing a nearly vertical separating line (hyperplane almost parallel to the $x_2$-axis). After whitening by $W = S^{-1}$, the maximal margin normal vector is proportional to $(1,\\,1)$, yielding the balanced diagonal separator orthogonal to $x_1 + x_2 = 0$.**\n-   The analysis of the observed coordinates is correct. The normal vector is proportional to $(M,1)$, and for large $M$, the separating line $x_2 = -M x_1$ is nearly vertical.\n-   The analysis of the whitened coordinates is also correct. The normal vector is proportional to $(1,1)$, and the separator is the balanced diagonal line $z_1+z_2=0$.\n-   The final phrase, \"orthogonal to $x_1 + x_2 = 0$\", is awkwardly phrased. In the whitened space, the separator is $z_1+z_2=0$, which is orthogonal to the line $z_1-z_2=0$ (where the data lies). The phrase might be a typo for \"orthogonal to the line $z_1-z_2=0$\" or simply a poor description. However, the core physical and mathematical claims about the normal vectors are accurate. Given that this is a multiple-choice question, this option is the most accurate description of the results.\n**Verdict: Correct**\n\n**B. Whitening leaves the maximal margin normal vector unchanged, because Support Vector Machines (SVMs) are invariant to feature scaling.**\nThis statement is incorrect on two counts. First, our derivation shows that the normal vector changes direction from being proportional to $(M,1)$ to being proportional to $(1,1)$. Second, the reason given is false. Standard SVMs are known *not* to be invariant to feature scaling, which is the very phenomenon this problem demonstrates.\n**Verdict: Incorrect**\n\n**C. Even without whitening, switching to a soft-margin SVM with any finite penalty parameter $C$ will rotate the separator in observed coordinates to be proportional to $(1,\\,1)$.**\nIn the soft-margin SVM, the normal vector is given by $w_x = \\sum_i \\alpha_i y_i x_i$. Since every data point $x_i$ is a scalar multiple of the vector $v=(M,1)$, i.e., $x_i = c_i v$, the resulting normal vector is $w_x = \\sum_i \\alpha_i y_i c_i v = (\\sum_i \\alpha_i y_i c_i) v$. This shows that $w_x$ must be proportional to $v=(M,1)$ for any set of non-zero Lagrange multipliers $\\alpha_i$. Therefore, the orientation of the separator does not change, and it will not become proportional to $(1,1)$.\n**Verdict: Incorrect**\n\n**D. In observed coordinates the maximal margin normal vector is proportional to $(1,\\,1)$ for all $M$, but whitening rotates it toward $(M,\\,1)$ when $M \\gg 1$.**\nThis statement is the exact opposite of our findings. In observed coordinates, the normal vector is proportional to $(M,1)$, not $(1,1)$. After whitening, it is proportional to $(1,1)$, not $(M,1)$.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The soft-margin classifier's ability to handle non-separable data hinges on how it penalizes errors. While the standard SVM uses a linear penalty on margin violations, this is not the only option. This practice invites you to explore the profound impact of using a quadratic penalty instead . By comparing two candidate hyperplanes on a carefully constructed dataset, you will discover how different penalty functions lead to distinct strategies for handling outliers and errors, highlighting the trade-off between robustness and error distribution.",
            "id": "3147193",
            "problem": "A binary classification dataset consists of labeled points $\\{(x_i, y_i)\\}_{i=1}^n$ with $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\{-1, +1\\}$. A linear classifier is specified by a weight vector $w \\in \\mathbb{R}^2$ and bias $b \\in \\mathbb{R}$, inducing the decision function $f(x) = \\operatorname{sign}(w^\\top x + b)$. In maximal margin classification, the geometric margin is defined as $\\gamma = 1 / \\|w\\|$, and soft margin classification introduces slack variables $\\xi_i \\ge 0$ to allow violations of the unit-margin constraints $y_i \\,(w^\\top x_i + b) \\ge 1 - \\xi_i$. Consider a bi-criteria approach that seeks to maximize the margin $\\gamma$ while simultaneously minimizing the aggregate violation measured by the sum of squared slacks $\\sum_{i=1}^n \\xi_i^2$. To obtain a single objective, introduce a positive regularization parameter $C > 0$ that trades off margin against violation magnitude.\n\nNow, to reveal how quadratic slack penalization concentrates errors differently than linear penalization, analyze the following concrete dataset and two candidate hyperplanes. The dataset has $7$ points:\n- Positive points: $x_1 = (2, 0)$ with $y_1 = +1$, $x_2 = (3, 0)$ with $y_2 = +1$, and an outlier $x_3 = (-4, 0)$ with $y_3 = +1$.\n- Negative points: $x_4 = (-3, 0)$ with $y_4 = -1$, $x_5 = (-2.5, 0)$ with $y_5 = -1$, $x_6 = (-2, 0)$ with $y_6 = -1$, $x_7 = (-1.5, 0)$ with $y_7 = -1$.\n\nConsider two fixed candidate hyperplanes, each with the same weight norm $\\|w\\| = 1$ (so they have the same margin $\\gamma = 1$):\n- Hyperplane $H_1$: $w = (1, 0)$ and $b = 0$.\n- Hyperplane $H_2$: $w = (1, 0)$ and $b = 2$.\n\nFor each hyperplane, the slacks $\\xi_i$ are determined by the constraints $y_i \\,(w^\\top x_i + b) \\ge 1 - \\xi_i$, $\\xi_i \\ge 0$. Because both $H_1$ and $H_2$ have the same margin, the choice under the bi-criteria objective with quadratic penalization reduces to comparing the sums $\\sum_{i=1}^7 \\xi_i^2$. Which statement is correct about which hyperplane is preferred under quadratic slack penalization, and how this choice differs from what would be preferred under linear slack penalization $\\sum_{i=1}^7 \\xi_i$?\n\nA. Under quadratic penalization, $H_1$ is preferred because one large violation is cheaper than several small ones once squared; under linear penalization, $H_2$ would be preferred.\n\nB. Under quadratic penalization, $H_2$ is preferred because distributing violation across several smaller slacks reduces $\\sum \\xi_i^2$ even if the linear sum $\\sum \\xi_i$ increases; under linear penalization, $H_1$ would be preferred.\n\nC. Under quadratic penalization, $H_1$ and $H_2$ are equivalent because they have the same margin and the same sum of squared slacks; the linear penalization also yields equivalence.\n\nD. The preference between $H_1$ and $H_2$ cannot be determined without kernels or dual variables; margin equality prevents any conclusion.",
            "solution": "### Step 1: Extract Givens\nThe problem provides the following information:\n- A binary classification dataset $\\{(x_i, y_i)\\}_{i=1}^7$ where $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\{-1, +1\\}$.\n- A linear classifier is defined by $f(x) = \\operatorname{sign}(w^\\top x + b)$.\n- The geometric margin is $\\gamma = 1 / \\|w\\|$.\n- Soft margin constraints are $y_i \\,(w^\\top x_i + b) \\ge 1 - \\xi_i$, with slack variables $\\xi_i \\ge 0$.\n- The objective is a bi-criteria optimization: maximize margin $\\gamma$ and minimize the sum of squared slacks $\\sum_{i=1}^n \\xi_i^2$. This is contrasted with minimizing the sum of linear slacks $\\sum_{i=1}^n \\xi_i$.\n- The dataset consists of $7$ points:\n    - Positive class ($y_i = +1$): $x_1 = (2, 0)$, $x_2 = (3, 0)$, $x_3 = (-4, 0)$.\n    - Negative class ($y_i = -1$): $x_4 = (-3, 0)$, $x_5 = (-2.5, 0)$, $x_6 = (-2, 0)$, $x_7 = (-1.5, 0)$.\n- Two candidate hyperplanes are given for evaluation:\n    - Hyperplane $H_1$: $w_1 = (1, 0)$, $b_1 = 0$.\n    - Hyperplane $H_2$: $w_2 = (1, 0)$, $b_2 = 2$.\n- Both hyperplanes have the same weight norm $\\|w_1\\| = \\|w_2\\| = \\sqrt{1^2 + 0^2} = 1$, and thus the same margin $\\gamma = 1$.\n- The comparison between $H_1$ and $H_2$ under the quadratic penalization objective reduces to comparing their respective sums of squared slacks, $\\sum_{i=1}^7 \\xi_i^2$. The problem also asks for a comparison under linear penalization, $\\sum_{i=1}^7 \\xi_i$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is validated as follows:\n- **Scientifically Grounded:** The problem is a standard exercise in statistical learning, specifically concerning the properties of Support Vector Machines (SVMs) with different slack penalty functions ($L_2$ vs. $L_1$). All definitions and concepts are standard in the field.\n- **Well-Posed:** The problem is well-posed. It provides a concrete dataset and two specific hyperplanes to evaluate. The task is to calculate the slack variables and the corresponding penalty sums, which is a deterministic and well-defined procedure.\n- **Objective:** The language is formal, precise, and free of ambiguity or subjectivity.\n\nThe problem statement is complete, consistent, and scientifically sound. It presents a valid, solvable problem.\n\n### Step 3: Derivation and Analysis\nSince the margins of both hyperplanes are identical ($\\gamma=1$), the preference between them is determined solely by the aggregate violation term. We need to calculate the sum of linear slacks, $\\sum \\xi_i$, and the sum of squared slacks, $\\sum \\xi_i^2$, for each hyperplane.\n\nThe slack variable $\\xi_i$ for each point $(x_i, y_i)$ is determined by the constraints $y_i(w^\\top x_i + b) \\ge 1 - \\xi_i$ and $\\xi_i \\ge 0$. To minimize the penalty, we must choose the smallest possible non-negative $\\xi_i$ that satisfies the first constraint. This gives:\n$$ \\xi_i = \\max(0, 1 - y_i(w^\\top x_i + b)) $$\n\n#### Analysis of Hyperplane $H_1: w_1 = (1, 0), b_1 = 0$\nFor $H_1$, the term inside the max function is $1 - y_i(w_1^\\top x_i + b_1) = 1 - y_i((1, 0)^\\top(x_{i,1}, x_{i,2}) + 0) = 1 - y_i x_{i,1}$.\nLet's calculate the slack $\\xi_{i,1}$ for each point:\n- $x_1=(2,0), y_1=+1$: $\\xi_{1,1} = \\max(0, 1 - (+1)(2)) = \\max(0, -1) = 0$.\n- $x_2=(3,0), y_2=+1$: $\\xi_{2,1} = \\max(0, 1 - (+1)(3)) = \\max(0, -2) = 0$.\n- $x_3=(-4,0), y_3=+1$: $\\xi_{3,1} = \\max(0, 1 - (+1)(-4)) = \\max(0, 1+4) = 5$.\n- $x_4=(-3,0), y_4=-1$: $\\xi_{4,1} = \\max(0, 1 - (-1)(-3)) = \\max(0, 1-3) = 0$.\n- $x_5=(-2.5,0), y_5=-1$: $\\xi_{5,1} = \\max(0, 1 - (-1)(-2.5)) = \\max(0, 1-2.5) = 0$.\n- $x_6=(-2,0), y_6=-1$: $\\xi_{6,1} = \\max(0, 1 - (-1)(-2)) = \\max(0, 1-2) = 0$.\n- $x_7=(-1.5,0), y_7=-1$: $\\xi_{7,1} = \\max(0, 1 - (-1)(-1.5)) = \\max(0, 1-1.5) = 0$.\n\nThe slack variables for $H_1$ are $\\{0, 0, 5, 0, 0, 0, 0\\}$.\n- **Linear Penalty:** $\\sum_{i=1}^7 \\xi_{i,1} = 5$.\n- **Quadratic Penalty:** $\\sum_{i=1}^7 \\xi_{i,1}^2 = 5^2 = 25$.\n\n#### Analysis of Hyperplane $H_2: w_2 = (1, 0), b_2 = 2$\nFor $H_2$, the term is $1 - y_i(w_2^\\top x_i + b_2) = 1 - y_i((1, 0)^\\top(x_{i,1}, x_{i,2}) + 2) = 1 - y_i (x_{i,1} + 2)$.\nLet's calculate the slack $\\xi_{i,2}$ for each point:\n- $x_1=(2,0), y_1=+1$: $\\xi_{1,2} = \\max(0, 1 - (+1)(2+2)) = \\max(0, -3) = 0$.\n- $x_2=(3,0), y_2=+1$: $\\xi_{2,2} = \\max(0, 1 - (+1)(3+2)) = \\max(0, -4) = 0$.\n- $x_3=(-4,0), y_3=+1$: $\\xi_{3,2} = \\max(0, 1 - (+1)(-4+2)) = \\max(0, 1 - (-2)) = 3$.\n- $x_4=(-3,0), y_4=-1$: $\\xi_{4,2} = \\max(0, 1 - (-1)(-3+2)) = \\max(0, 1 - (-1)(-1)) = \\max(0, 0) = 0$.\n- $x_5=(-2.5,0), y_5=-1$: $\\xi_{5,2} = \\max(0, 1 - (-1)(-2.5+2)) = \\max(0, 1 - (-1)(-0.5)) = \\max(0, 1-0.5) = 0.5$.\n- $x_6=(-2,0), y_6=-1$: $\\xi_{6,2} = \\max(0, 1 - (-1)(-2+2)) = \\max(0, 1 - 0) = 1$.\n- $x_7=(-1.5,0), y_7=-1$: $\\xi_{7,2} = \\max(0, 1 - (-1)(-1.5+2)) = \\max(0, 1 - (-1)(0.5)) = \\max(0, 1+0.5) = 1.5$.\n\nThe slack variables for $H_2$ are $\\{0, 0, 3, 0, 0.5, 1, 1.5\\}$.\n- **Linear Penalty:** $\\sum_{i=1}^7 \\xi_{i,2} = 3 + 0.5 + 1 + 1.5 = 6$.\n- **Quadratic Penalty:** $\\sum_{i=1}^7 \\xi_{i,2}^2 = 3^2 + 0.5^2 + 1^2 + 1.5^2 = 9 + 0.25 + 1 + 2.25 = 12.5$.\n\n#### Comparison\n- **Quadratic Penalization ($\\sum \\xi_i^2$):**\n  - Cost for $H_1$: $25$.\n  - Cost for $H_2$: $12.5$.\n  Since $12.5 < 25$, hyperplane $H_2$ is preferred under quadratic penalization.\n\n- **Linear Penalization ($\\sum \\xi_i$):**\n  - Cost for $H_1$: $5$.\n  - Cost for $H_2$: $6$.\n  Since $5 < 6$, hyperplane $H_1$ is preferred under linear penalization.\n\nThe core insight is that the quadratic ($L_2$) penalty heavily penalizes large errors. Hyperplane $H_1$ accommodates the outlier $x_3$ badly, incurring a single large slack variable $\\xi_3=5$, which results in a very large squared penalty of $25$. Hyperplane $H_2$ shifts to better accommodate the outlier (reducing its slack to $\\xi_3=3$), but at the cost of introducing several smaller violations for other points. The sum of squared slacks for $H_2$ ($12.5$) is much lower than for $H_1$, because squaring punishes the single large value $5$ more than the collection of smaller values $\\{3, 0.5, 1, 1.5\\}$.\n\nConversely, the linear ($L_1$) penalty simply sums the violations. For $H_1$, the total violation is $5$. For $H_2$, the total violation is $6$. Therefore, the linear penalty prefers $H_1$, which correctly classifies more points, even though it makes one very large error. This demonstrates that an $L_1$ penalty is more robust to outliers, as it tolerates large errors on a few points in exchange for better overall performance on the majority, leading to a \"sparser\" set of violations.\n\n### Option-by-Option Analysis\n\n**A. Under quadratic penalization, $H_1$ is preferred because one large violation is cheaper than several small ones once squared; under linear penalization, $H_2$ would be preferred.**\n- The statement that $H_1$ is preferred under quadratic penalization is **Incorrect**. Our calculation shows $H_2$ is preferred ($12.5 < 25$).\n- The reasoning \"one large violation is cheaper than several small ones once squared\" is fundamentally **Incorrect**. Squaring makes large values disproportionately more costly.\n- The statement that $H_2$ would be preferred under linear penalization is **Incorrect**. Our calculation shows $H_1$ is preferred ($5 < 6$).\n\n**B. Under quadratic penalization, $H_2$ is preferred because distributing violation across several smaller slacks reduces $\\sum \\xi_i^2$ even if the linear sum $\\sum \\xi_i$ increases; under linear penalization, $H_1$ would be preferred.**\n- The statement \"Under quadratic penalization, $H_2$ is preferred\" is **Correct** ($12.5 < 25$).\n- The reasoning \"distributing violation across several smaller slacks reduces $\\sum \\xi_i^2$\" is **Correct**. This is the key property of $L_2$ penalties. Our calculation confirms this: $\\sum \\xi_{i,2}^2 = 12.5$ is less than $\\xi_{3,1}^2=25$. The condition \"even if the linear sum $\\sum \\xi_i$ increases\" is also met ($6 > 5$).\n- The statement \"under linear penalization, $H_1$ would be preferred\" is **Correct** ($5 < 6$).\n- This option correctly identifies the preferred hyperplane under both penalty schemes and provides the correct conceptual reasoning.\n\n**C. Under quadratic penalization, $H_1$ and $H_2$ are equivalent because they have the same margin and the same sum of squared slacks; the linear penalization also yields equivalence.**\n- The claim that they have the \"same sum of squared slacks\" is **Incorrect** ($25 \\neq 12.5$).\n- The claim that \"linear penalization also yields equivalence\" is **Incorrect** ($5 \\neq 6$).\n\n**D. The preference between $H_1$ and $H_2$ cannot be determined without kernels or dual variables; margin equality prevents any conclusion.**\n- This statement is **Incorrect**. The problem is fully determined and can be solved by direct calculation in the primal formulation. The equality of margins simplifies the problem by allowing a direct comparison of the slack penalty terms. Kernels and dual variables are tools for solving the SVM optimization problem, not for evaluating pre-defined candidate solutions.\n\nBased on the detailed analysis, option B is the only statement that is entirely correct.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}