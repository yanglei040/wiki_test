{
    "hands_on_practices": [
        {
            "introduction": "预测区间的宽度并非一个固定的数值，而是由几个关键因素共同决定的。这个练习将引导你从第一性原理出发，推导预测区间宽度的精确表达式，从而清晰地揭示它如何依赖于置信水平、数据固有变异性以及预测点的位置。通过亲手实现一个模拟，你将能直观地量化样本量的增加如何系统性地收紧预测区间，加深对理论公式背后动态关系的理解。",
            "id": "3160052",
            "problem": "本题要求您将线性模型中双侧预测区间的构建，与学生$t$分位数和残差标准误如何共同控制其宽度联系起来。请从第一性原理出发，基于以下基础进行推导。\n\n假设线性模型为 $y = X\\beta + \\varepsilon$，其中有 $n$ 个观测值和 $p$ 个参数，且 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。使用普通最小二乘法（OLS）估计量和高斯误差模型，推导在新设计点 $x_{0} \\in \\mathbb{R}^{p}$ 处标准化预测误差的抽样分布。然后，推导在新设计点 $x_{0}$ 处对新响应 $y_{0}$ 的精确双侧 $(1 - \\alpha)$ 预测区间及其宽度。您的推导应清楚地说明自由度为 $n-p$ 的学生$t$分位数和残差标准误 $\\hat{\\sigma}$ 如何出现在宽度表达式中，以及设计依赖性如何通过 $x_{0}$ 处的杠杆值引入。\n\n完成推导后，请编写一个程序，实现所推导的公式，用于一个受控设计，该设计旨在分离并量化$t$分位数和$\\hat{\\sigma}$的作用。在所有测试用例中，请使用以下确定性设置：\n\n- 模型：含截距项和一个中心化预测变量的简单线性回归，因此 $p = 2$。\n- 设计矩阵 $X \\in \\mathbb{R}^{n \\times 2}$ 的第一列为全1向量，第二列 $x \\in \\mathbb{R}^{n}$ 按以下方式确定性地构建：在 $[-1, 1]$ 上取 $n$ 个等距点，将其中心化以使经验均值为 $0$，然后进行缩放以使 $\\sum_{i=1}^{n} x_{i}^{2} = n$。这确保了 $X^{\\top} X$ 在数值对称性下是对角矩阵，其对角线元素近似为 $(n, n)$。\n- 用于预测的新设计点：$x_{0} = (1, 0)^{\\top}$，即在中心化预测变量为0的水平上进行预测。\n- 残差平方和（RSS）在所有比较中固定为相同的值，$\\mathrm{RSS} = 100$（无单位）。残差标准误为 $\\hat{\\sigma} = \\sqrt{\\mathrm{RSS}/(n-p)}$。\n- 使用由 $\\alpha$ 指定的双侧名义水平。\n\n对于下述每个测试用例，令 $n$ 表示基准样本量，$2n$ 表示加倍后的样本量，两者均使用相同的 $\\alpha$ 和相同的 $\\mathrm{RSS} = 100$。对于每个测试用例，您的程序必须计算：\n\n1. 在上述设计下，样本量为 $2n$ 时的预测区间宽度与样本量为 $n$ 时的宽度之比 $r_{\\mathrm{full}}$，其中对每个 $n$ 使用由 $X$ 确定的 $x_{0}$ 处的精确杠杆值。\n2. 绝对偏差 $|r_{\\mathrm{full}} - 0.5|$，用于量化宽度比率与“一半”的差距。\n3. 从宽度中移除设计杠杆值影响后得到的比率 $r_{t,\\sigma}$，即从 $n$ 变为 $2n$ 时，仅依赖于$t$分位数和$\\hat{\\sigma}$的因子的比率。\n\n所有量均为无单位量；不涉及物理单位。也不使用角度。\n\n测试套件（每个项目为一对 $(n,\\alpha)$；始终使用 $p = 2$ 和 $\\mathrm{RSS} = 100$）：\n- 案例 A（理想路径，小自由度）：$(n, \\alpha) = (3, 0.05)$。\n- 案例 B（边界情况，仍为小样本）：$(n, \\alpha) = (6, 0.05)$。\n- 案例 C（中等样本）：$(n, \\alpha) = (20, 0.05)$。\n- 案例 D（更严格的水平对$t$值的影响）：$(n, \\alpha) = (50, 0.01)$。\n\n最终输出格式：您的程序应生成单行输出，其中包含按测试套件顺序排列的结果，格式为逗号分隔的列表之列表。每个内部列表为 $[r_{\\mathrm{full}}, |r_{\\mathrm{full}} - 0.5|, r_{t,\\sigma}]$。例如，一个有效的输出类似于\n[[a11,a12,a13],[a21,a22,a23],[a31,a32,a33],[a41,a42,a43]]\n，其中每个 $a_{ij}$ 均为浮点数。",
            "solution": "用户提供的问题经评估为有效。该问题在科学上基于线性 statistical 模型理论，问题陈述清晰、目标明确、信息充分，并使用了客观、正式的语言。任务是推导预测区间的宽度，然后实现一个程序，以研究其在特定条件下的行为。\n\n解决方案分两部分进行。首先，对预测区间及其宽度进行理论推导。其次，根据该推导实现一个程序，以计算特定量值。\n\n**第一部分：预测区间及其宽度的推导**\n\n我们从指定的线性模型开始：\n$$ y = X\\beta + \\varepsilon $$\n其中 $y \\in \\mathbb{R}^{n}$ 是观测响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是秩为 $p$ 的已知设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是未知参数向量，而 $\\varepsilon \\in \\mathbb{R}^{n}$ 是未观测到的随机误差向量。我们假设误差是独立同分布的，服从高斯分布，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，其中 $\\sigma^2$ 是未知的误差方差，$I_n$ 是 $n \\times n$ 的单位矩阵。\n\n$\\beta$ 的普通最小二乘（OLS）估计量由下式给出：\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y $$\n根据对 $\\varepsilon$ 的分布假设，OLS估计量 $\\hat{\\beta}$ 也是一个服从高斯分布的随机变量。其期望值为 $E[\\hat{\\beta}] = (X^{\\top}X)^{-1}X^{\\top}E[y] = (X^{\\top}X)^{-1}X^{\\top}(X\\beta) = \\beta$，因此它是一个无偏估计量。其协方差矩阵为 $\\mathrm{Var}(\\hat{\\beta}) = \\mathrm{Var}((X^{\\top}X)^{-1}X^{\\top}y) = (X^{\\top}X)^{-1}X^{\\top}(\\sigma^2 I_n)X(X^{\\top}X)^{-1} = \\sigma^2(X^{\\top}X)^{-1}$。因此，该估计量的抽样分布为：\n$$ \\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2(X^{\\top}X)^{-1}) $$\n\n我们感兴趣的是在新设计点 $x_0 \\in \\mathbb{R}^{p}$ 处预测一个新的响应 $y_0$。这个新观测值的模型是：\n$$ y_0 = x_0^{\\top}\\beta + \\varepsilon_0 $$\n其中假设 $\\varepsilon_0 \\sim \\mathcal{N}(0, \\sigma^2)$ 与训练误差 $\\varepsilon$ 独立。\n\n$y_0$ 的点预测是通过将估计参数 $\\hat{\\beta}$ 代入新点的模型方程得到的：\n$$ \\hat{y}_0 = x_0^{\\top}\\hat{\\beta} $$\n预测误差是未来真实观测值 $y_0$ 与我们的预测值 $\\hat{y}_0$ 之间的差值：\n$$ y_0 - \\hat{y}_0 = (x_0^{\\top}\\beta + \\varepsilon_0) - x_0^{\\top}\\hat{\\beta} = \\varepsilon_0 - x_0^{\\top}(\\hat{\\beta} - \\beta) $$\n预测误差是高斯随机变量（$\\varepsilon_0$ 和 $\\hat{\\beta}$ 的元素）的线性组合，因此它也服从正态分布。其期望值为 $E[y_0 - \\hat{y}_0] = E[\\varepsilon_0] - x_0^{\\top}E[\\hat{\\beta} - \\beta] = 0 - x_0^{\\top}(0) = 0$。\n\n预测误差的方差计算如下，利用了 $\\varepsilon_0$ 和 $\\hat{\\beta}$（它是训练数据的函数）的独立性：\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\mathrm{Var}(\\varepsilon_0) + \\mathrm{Var}(x_0^{\\top}(\\hat{\\beta} - \\beta)) $$\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\sigma^2 + x_0^{\\top}\\mathrm{Var}(\\hat{\\beta})x_0 = \\sigma^2 + x_0^{\\top}(\\sigma^2(X^{\\top}X)^{-1})x_0 $$\n提出公因子 $\\sigma^2$，我们得到：\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\sigma^2 \\left(1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0\\right) $$\n因此，预测误差的分布为：\n$$ y_0 - \\hat{y}_0 \\sim \\mathcal{N}\\left(0, \\sigma^2 \\left(1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0\\right)\\right) $$\n项 $h_0 = x_0^{\\top}(X^{\\top}X)^{-1}x_0$ 被称为新点 $x_0$ 的杠杆值。\n\n如果 $\\sigma^2$ 已知，我们可以构造一个标准的正态枢轴量：\n$$ Z = \\frac{y_0 - \\hat{y}_0}{\\sigma \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}} \\sim \\mathcal{N}(0, 1) $$\n然而，$\\sigma^2$ 通常是未知的，必须从数据中估计。$\\sigma^2$ 的无偏估计量是均方误差（MSE），它基于残差平方和（RSS）：\n$$ \\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{n-p} = \\frac{1}{n-p}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $$\n量 $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$ 是残差标准误。高斯误差下线性模型理论的一个基本结果指出：\n$$ \\frac{(n-p)\\hat{\\sigma}^2}{\\sigma^2} = \\frac{\\mathrm{RSS}}{\\sigma^2} \\sim \\chi^2_{n-p} $$\n其中 $\\chi^2_{n-p}$ 是自由度为 $n-p$ 的卡方分布。此外，$\\hat{\\beta}$ 和 $\\hat{\\sigma}^2$ 是独立的。\n\n为了构造一个不依赖于 $\\sigma$ 的枢轴量，我们构建一个学生$t$统计量。该统计量是一个标准正态变量与一个独立的卡方变量除以其自由度的平方根之比。\n$$ T = \\frac{\\frac{y_0 - \\hat{y}_0}{\\sigma \\sqrt{1 + h_0}}}{\\sqrt{\\frac{(n-p)\\hat{\\sigma}^2/\\sigma^2}{n-p}}} = \\frac{\\frac{y_0 - \\hat{y}_0}{\\sigma \\sqrt{1 + h_0}}}{\\frac{\\hat{\\sigma}}{\\sigma}} = \\frac{y_0 - \\hat{y}_0}{\\hat{\\sigma} \\sqrt{1 + h_0}} $$\n这个量 $T$ 服从自由度为 $n-p$ 的学生$t$分布：\n$$ \\frac{y_0 - \\hat{y}_0}{\\hat{\\sigma} \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}} \\sim t_{n-p} $$\n这就是标准化预测误差的抽样分布。\n\n为了构造 $y_0$ 的一个双侧 $(1 - \\alpha)$ 预测区间，我们使用 $t_{n-p}$ 分布的分位数。令 $t_{\\alpha/2, n-p}$ 为满足 $P(T > t_{\\alpha/2, n-p}) = \\alpha/2$ 的值。那么：\n$$ P(-t_{\\alpha/2, n-p} \\le \\frac{y_0 - \\hat{y}_0}{\\hat{\\sigma} \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}} \\le t_{\\alpha/2, n-p}) = 1 - \\alpha $$\n将 $y_0$ 分离到不等式中间，得到预测区间：\n$$ \\hat{y}_0 \\pm t_{\\alpha/2, n-p} \\hat{\\sigma} \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0} $$\n该区间的宽度 $W$ 是上界与下界之差：\n$$ W = 2 \\cdot t_{\\alpha/2, n-p} \\cdot \\hat{\\sigma} \\cdot \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0} $$\n这个表达式清楚地将宽度分解为所要求的三个关键部分：\n1. 学生$t$分位数，$t_{\\alpha/2, n-p}$，它取决于置信水平 $\\alpha$ 和自由度 $n-p$。\n2. 残差标准误，$\\hat{\\sigma} = \\sqrt{\\mathrm{RSS}/(n-p)}$，它估计了数据的内在变异性 $\\sigma$。\n3. 依赖于设计的因子，$\\sqrt{1+h_0} = \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}$，它解释了估计 $\\beta$ 的不确定性以及这种不确定性如何传播到在 $x_0$ 处的预测。它取决于训练设计 $X$ 和新点 $x_0$ 的位置。\n\n**第二部分：针对特定设计的实现**\n\n题目要求在受控设置下进行实现。对于简单线性回归（$p=2$），设计矩阵 $X$ 的第一列为全1向量，第二列为中心化的预测变量 $x_i$，满足 $\\sum_{i=1}^n x_i = 0$ 和 $\\sum_{i=1}^n x_i^2 = n$。对于此特定设计，矩阵 $X^{\\top}X$ 变为对角矩阵：\n$$ X^{\\top}X = \\begin{pmatrix} \\sum 1  \\sum x_i \\\\ \\sum x_i  \\sum x_i^2 \\end{pmatrix} = \\begin{pmatrix} n  0 \\\\ 0  n \\end{pmatrix} = nI_2 $$\n其逆矩阵为 $(X^{\\top}X)^{-1} = \\frac{1}{n}I_2$。\n新设计点为 $x_0 = (1, 0)^{\\top}$。该点的杠杆值为：\n$$ h_0 = x_0^{\\top}(X^{\\top}X)^{-1}x_0 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1/n  0 \\\\ 0  1/n \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{n} $$\n因此，在这种特定情况下，宽度公式简化为：\n$$ W(n, \\alpha) = 2 \\cdot t_{\\alpha/2, n-2} \\cdot \\sqrt{\\frac{\\mathrm{RSS}}{n-2}} \\cdot \\sqrt{1 + \\frac{1}{n}} $$\n程序将计算样本量为 $n$ 和 $2n$ 时的宽度（或其比例分量），以找到所需的比率。宽度比 $W_{2n} / W_n$ 为：\n$$ r_{\\mathrm{full}} = \\frac{2 \\cdot t_{\\alpha/2, 2n-2} \\cdot \\sqrt{\\frac{\\mathrm{RSS}}{2n-2}} \\cdot \\sqrt{1 + \\frac{1}{2n}}}{2 \\cdot t_{\\alpha/2, n-2} \\cdot \\sqrt{\\frac{\\mathrm{RSS}}{n-2}} \\cdot \\sqrt{1 + \\frac{1}{n}}} = \\left(\\frac{t_{\\alpha/2, 2n-2}}{t_{\\alpha/2, n-2}}\\right) \\left(\\sqrt{\\frac{n-2}{2n-2}}\\right) \\left(\\sqrt{\\frac{1+1/(2n)}{1+1/n}}\\right) $$\n比率 $r_{t,\\sigma}$ 是上述表达式中前两项的乘積，不包括依赖于杠杆值的项。\n$$ r_{t,\\sigma} = \\left(\\frac{t_{\\alpha/2, 2n-2}}{t_{\\alpha/2, n-2}}\\right) \\left(\\sqrt{\\frac{n-2}{2n-2}}\\right) $$\n以下代码为给定的测试用例实现了这些计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Calculates prediction interval width ratios for specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (baseline sample size n, significance level alpha)\n    test_cases = [\n        (3, 0.05),   # Case A\n        (6, 0.05),   # Case B\n        (20, 0.05),  # Case C\n        (50, 0.01)   # Case D\n    ]\n\n    # Fixed parameters from the problem statement\n    p = 2      # Number of parameters (intercept + 1 predictor)\n    RSS = 100.0  # Residual Sum of Squares\n\n    results = []\n\n    def calculate_width_components(n_sample, alpha_level):\n        \"\"\"\n        Calculates the components of the prediction interval width.\n        \n        Args:\n            n_sample (int): The sample size n.\n            alpha_level (float): The significance level alpha.\n            \n        Returns:\n            A tuple containing the t-quantile, residual standard error, and leverage factor.\n        \"\"\"\n        if n_sample = p:\n            # Degrees of freedom must be positive.\n            return np.nan, np.nan, np.nan\n        \n        # Degrees of freedom for the t-distribution\n        df = n_sample - p\n        \n        # 1. Student's t-quantile\n        t_quantile = t.ppf(1 - alpha_level / 2, df)\n        \n        # 2. Residual standard error\n        sigma_hat = np.sqrt(RSS / df)\n        \n        # 3. Design dependence (leverage factor)\n        # For the given design, h_0 = 1/n.\n        h0 = 1 / n_sample\n        leverage_factor = np.sqrt(1 + h0)\n        \n        return t_quantile, sigma_hat, leverage_factor\n\n    for n_base, alpha in test_cases:\n        # Calculate components for the baseline sample size n\n        t_n, sigma_n, lev_n = calculate_width_components(n_base, alpha)\n        \n        # Calculate components for the doubled sample size 2n\n        n_doubled = 2 * n_base\n        t_2n, sigma_2n, lev_2n = calculate_width_components(n_doubled, alpha)\n        \n        # --- Calculate r_full ---\n        # The width is proportional to t_quantile * sigma_hat * leverage_factor.\n        # The constant factor of 2 cancels in the ratio.\n        width_prop_n = t_n * sigma_n * lev_n\n        width_prop_2n = t_2n * sigma_2n * lev_2n\n        \n        r_full = width_prop_2n / width_prop_n\n        \n        # --- Calculate |r_full - 0.5| ---\n        abs_dev_from_half = abs(r_full - 0.5)\n        \n        # --- Calculate r_t,sigma ---\n        # Ratio of factors depending only on t-quantile and sigma_hat\n        factor_ts_n = t_n * sigma_n\n        factor_ts_2n = t_2n * sigma_2n\n        \n        r_t_sigma = factor_ts_2n / factor_ts_n\n        \n        # Append the triple of results for the current test case.\n        results.append([r_full, abs_dev_from_half, r_t_sigma])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个稳健的统计模型不应被少数几个数据点轻易“绑架”。然而，普通最小二乘法（OLS）对异常值或强影响点很敏感，这些点会不成比例地影响模型的拟合结果，尤其是夸大残差标准误（$\\hat{\\sigma}$）。本练习将让你通过计算库克距离（Cook's distance）来亲手识别这些“害群之马”，并量化移除它们后预测区间宽度的变化，从而深刻体会到模型诊断在构建可靠预测中的核心作用。",
            "id": "3160002",
            "problem": "给定三个独立的简单线性回归场景。在每个场景中，数据来自模型 $y = \\beta_0 + \\beta_1 x + \\varepsilon$，其中误差项 $\\varepsilon$ 假定为独立同分布，均值为 $0$，方差为常数 $\\sigma^2$。对于每个场景，您必须拟合带截距的普通最小二乘模型，计算残差标准误 $\\hat{\\sigma}$，使用 Cook 距离识别影响点，并研究逐个移除每个影响点（一次移除一个）对三个典型设计点上未来观测值的双侧预测区间的宽度的影响。您的程序必须是一个完整的、可运行的程序，并且只能使用指定的运行时环境。所有计算都必须以纯数学术语进行，不涉及物理单位。不涉及角度。将任何置信水平表示为小数，不要使用百分号。\n\n在问题陈述中应使用的基本原则（无需引用快捷公式）：普通最小二乘法最小化残差平方和；残差标准误由残差平方和及相应的自由度计算得出；杠杆值源于由设计矩阵定义的帽子矩阵；Cook 距离衡量每个观测值对拟合回归的影响；在误差正态分布的假设下，未来观测值的双侧预测区间是使用学生 t 分布构建的，并同时考虑了估计不确定性和不可约误差。\n\n为每个场景实施以下步骤：\n- 拟合带截距的普通最小二乘模型，以获得 $\\hat{\\beta}_0$、$\\hat{\\beta}_1$ 和残差。\n- 根据拟合的残差和自由度 $n - p$ 计算残差标准误 $\\hat{\\sigma}$，其中 $n$ 是样本量，$p$ 是包括截距在内的拟合参数数量。\n- 计算每个观测值的帽子矩阵对角线元素（杠杆值）和 Cook 距离。将 Cook 距离超过该场景指定阈值 $\\tau$ 的观测值识别为影响点。\n- 将三个典型设计点定义为 $x$ 的最小值、$x$ 的均值和 $x$ 的最大值。对于每个拟合模型，使用置信水平 $1 - \\alpha$（$\\alpha$ 在下面给出）计算这三个设计点上各自的双侧预测区间宽度。然后，通过取这三个宽度的平均值来汇总，为该模型生成单个宽度摘要。\n- 对于每个场景，报告一个浮点数列表，其中包含基线平均宽度（使用所有数据），其后是逐个移除每个影响点后的平均宽度（一次只移除一个影响点，重新拟合和重新计算）。如果没有影响点，则该列表只包含基线平均宽度。\n\n使用以下测试套件。在每种情况下，数组都应按元素方式解释，所有数字都是精确常数。\n\n场景 A：\n- 输入：\n  - $x = [0,1,2,3,4,5,6,7,8,9,10,11]$。\n  - 通过 $y_i = 2 + 1.3 x_i + r_i$ 定义 $y$，残差为 $r = [0.2,-0.1,0.0,0.1,-0.2,0.05,-0.15,0.1,-0.05,0.1,16.0,-0.1]$。\n  - 置信参数 $\\alpha = 0.05$。\n  - Cook 距离阈值规则 $\\tau = 4/n$，其中 $n$ 是此场景中的观测值数量。\n- 场景 A 的输出：一个浮点数列表 $[w_0, w_1, \\dots]$，其中 $w_0$ 是三个典型设计点上的基线平均宽度，$w_j$（对于 $j \\ge 1$）是移除第 $j$ 个影响点（影响点按其原始索引升序排列）后的平均宽度。\n\n场景 B：\n- 输入：\n  - $x = [0,1,2,3,4,5,6,7,8,9]$。\n  - 通过 $y_i = 1 + 2 x_i + r_i$ 定义 $y$，残差为 $r = [0.02,-0.03,0.01,0.00,-0.02,0.01,-0.01,0.02,0.00,-0.02]$。\n  - 置信参数 $\\alpha = 0.05$。\n  - Cook 距离阈值 $\\tau = 10$。\n- 场景 B 的输出：一个如上所述的浮点数列表。如果未检测到影响点，则仅返回基线平均宽度。\n\n场景 C：\n- 输入：\n  - $x = [-5,-3,-1,0,1,2,3,4,5,20,-10,6,7,8,9]$。\n  - 通过 $y_i = 0.5 + 0.8 x_i + r_i$ 定义 $y$，残差为 $r = [0.1,-0.1,0.05,0.0,0.02,-0.03,0.04,-0.02,0.01,-11.5,12.5,-0.05,0.03,-0.04,0.02]$。\n  - 置信参数 $\\alpha = 0.05$。\n  - Cook 距离阈值规则 $\\tau = 4/n$，其中 $n$ 是此场景中的观测值数量。\n- 场景 C 的输出：一个如上所述的浮点数列表。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个场景的结果，格式为由方括号括起来的列表的逗号分隔列表，例如 $[[a_1,a_2],[b_1],[c_1,c_2,c_3]]$，每个浮点数使用标准四舍五入到最近的方法保留 $6$ 位小数。这三个列表必须按场景 A、场景 B、场景 C 的顺序出现。不应打印其他任何文本。",
            "solution": "该问题要求对三个不同场景下的简单线性回归模型进行分析。核心任务包括拟合模型、识别影响数据点以及评估它们对预测区间宽度的影响。该方法基于普通最小二乘 (OLS) 回归和标准统计诊断的原理。\n\n**1. 简单线性回归模型**\n基础的统计模型是一个简单线性回归：\n$$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$$\n其中 $y_i$ 是第 $i$ 个观测值的响应变量，$x_i$ 是预测变量，$\\beta_0$ 是截距，$\\beta_1$ 是斜率，$\\varepsilon_i$ 是独立同分布的随机误差，其均值 $E[\\varepsilon_i] = 0$，方差为常数 $\\text{Var}(\\varepsilon_i) = \\sigma^2$。问题为每个场景提供了数据对 $(x_i, y_i)$。\n\n**2. 普通最小二乘 (OLS) 估计**\nOLS 方法通过最小化残差平方和 (RSS) 来估计参数 $\\beta_0$ 和 $\\beta_1$。估计出的参数，记为 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$，由以下公式给出：\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\n其中 $n$ 是观测值的数量，$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ 是预测变量的样本均值，$\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ 是响应变量的样本均值。\n\n拟合值为 $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$，残差为 $e_i = y_i - \\hat{y}_i$。\n\n**3. 残差标准误 (RSE)**\nRSE，记为 $\\hat{\\sigma}$，是误差项标准差 $\\sigma$ 的一个估计。它是根据 RSS 计算的：\n$$ \\text{RSS} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\n然后 RSE 计算如下：\n$$ \\hat{\\sigma} = \\sqrt{\\frac{\\text{RSS}}{n-p}} $$\n其中自由度为 $n-p$。对于带截距的简单线性回归，估计的参数数量为 $p=2$（对应 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}_1$），因此自由度为 $n-2$。\n\n**4. 影响诊断**\n为了识别影响观测值，我们使用 Cook 距离。该度量结合了关于一个点的杠杆值及其残差大小的信息。\n\n首先，计算每个观测值 $i$ 的杠杆值 $h_{ii}$。杠杆值衡量一个观测值的预测变量值 $x_i$ 距离所有预测变量值的均值有多远。高杠杆点有潜力对回归拟合产生强烈影响。杠杆值是帽子矩阵 $H = X(X^T X)^{-1} X^T$ 的第 $i$ 个对角元素，其中 $X$ 是设计矩阵。对于简单线性回归，这可以简化为：\n$$ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2} $$\n\n其次，为每个观测值计算 Cook 距离 $D_i$：\n$$ D_i = \\frac{e_i^2}{p \\cdot \\hat{\\sigma}^2} \\left[ \\frac{h_{ii}}{(1-h_{ii})^2} \\right] $$\n如果一个观测值的 Cook 距离超过给定的阈值，即 $D_i  \\tau$，则该观测值被识别为影响点。在这个问题中，阈值 $\\tau$ 要么是一个固定值，要么是样本量 $n$ 的函数，例如 $\\tau = 4/n$。\n\n**5. 预测区间 (PI)**\n预测区间提供了一个范围，在给定的预测变量值 $x_0$ 下，未来的观测值 $y_0$ 预计会以一定的置信水平 $1-\\alpha$ 落入该范围。假设误差 $\\varepsilon_i$ 服从正态分布，则 $100(1-\\alpha)\\%$ 的 PI 构建如下：\n$$ \\hat{y}_0 \\pm t_{n-p, \\alpha/2} \\cdot \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2}} $$\n其中 $\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$ 是在 $x_0$ 处的预测值，$t_{n-p, \\alpha/2}$ 是来自自由度为 $n-p$ 的学生 t 分布的临界值，其上尾面积为 $\\alpha/2$。预测的标准误既包含了估计回归线的不确定性，也包含了一个新观测值的内在变异性 ($\\sigma^2$)。\n\n该区间的宽度为：\n$$ W(x_0) = 2 \\cdot t_{n-p, \\alpha/2} \\cdot \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2}} $$\n\n**6. 算法实现**\n对于每个场景，执行以下步骤：\n1.  **数据生成**：按照问题陈述中的规定构造向量 $x$ 和 $y$。\n2.  **基线分析**：\n    a. 对完整数据集 $(x, y)$ 拟合一个简单线性回归模型，以获得 $\\hat{\\beta}_0$、$\\hat{\\beta}_1$、$\\hat{\\sigma}$ 及其他模型统计量。\n    b. 在三个特定的设计点上计算 PI 宽度 $W(x_0)$：$x_{0, \\text{min}} = \\min(x)$、 $x_{0, \\text{mean}} = \\bar{x}$ 和 $x_{0, \\text{max}} = \\max(x)$。\n    c. 通过平均这三个宽度来计算基线平均宽度 $w_0$。这是该场景结果列表中的第一个值。\n3.  **影响分析**：\n    a. 使用完整模型，计算所有观测值的杠杆值 $h_{ii}$ 和 Cook 距离 $D_i$。\n    b. 通过收集满足 $D_i  \\tau$ 的索引 $i$ 来确定影响点集。这些索引按升序排序。\n4.  **敏感性分析**：\n    a. 如果没有发现影响点，则该场景的处理终止，结果就是 $[w_0]$。\n    b. 如果发现了影响点，则遍历它们的排序索引。对于每个影响点，将其从数据集中移除，创建一个新的、更小的数据集。\n    c. 对于每个新数据集，重新拟合回归模型，并使用与步骤 2 中相同的程序（使用新数据集的属性：$n_{new}$、$\\bar{x}_{new}$ 等）重新计算平均 PI 宽度 ($w_j$)。\n    d. 将这些新的平均宽度 ($w_1, w_2, \\dots$) 中的每一个追加到结果列表中。\n5.  **最终输出**：整理所有场景的宽度列表，并将它们格式化为指定的最终输出字符串。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to solve the three regression scenarios and print the results.\n    \"\"\"\n\n    def fit_and_get_stats(x, y):\n        \"\"\"\n        Fits a simple linear regression model and computes relevant statistics.\n        Returns a dictionary of statistics or None if fitting is not possible.\n        \"\"\"\n        n = len(x)\n        if n = 2:  # Cannot fit or compute RSE with = 2 points\n            return None\n        \n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        \n        S_xx = np.sum((x - x_mean)**2)\n        if S_xx == 0:  # Avoid division by zero if all x are the same\n            return None\n            \n        S_xy = np.sum((x - x_mean) * (y - y_mean))\n        \n        beta1_hat = S_xy / S_xx\n        beta0_hat = y_mean - beta1_hat * x\n        \n        y_hat = beta0_hat + beta1_hat * x\n        residuals = y - y_hat\n        rss = np.sum(residuals**2)\n        p = 2  # Number of parameters (intercept and slope)\n        dof = n - p\n        rse = np.sqrt(rss / dof)\n        \n        leverages = 1/n + (x - x_mean)**2 / S_xx\n        cooks_d = (residuals**2 / (p * rse**2)) * (leverages / (1 - leverages)**2)\n        \n        return {\n            'n': n,\n            'dof': dof,\n            'rse': rse,\n            'cooks_d': cooks_d,\n            'x_mean': x_mean,\n            'S_xx': S_xx\n        }\n\n    def get_avg_pi_width(stats, x_coords, alpha_val):\n        \"\"\"\n        Calculates the average prediction interval width for a given fitted model.\n        \"\"\"\n        if stats is None:\n            return np.nan\n        \n        n = stats['n']\n        dof = stats['dof']\n        rse = stats['rse']\n        x_mean = stats['x_mean']\n        S_xx = stats['S_xx']\n        \n        if S_xx == 0:\n            return np.nan\n\n        t_crit = t.ppf(1 - alpha_val / 2, dof)\n        \n        x0_points = [np.min(x_coords), np.mean(x_coords), np.max(x_coords)]\n        \n        widths = []\n        for x0 in x0_points:\n            se_pred_factor = np.sqrt(1 + 1/n + (x0 - x_mean)**2 / S_xx)\n            width = 2 * t_crit * rse * se_pred_factor\n            widths.append(width)\n            \n        return np.mean(widths)\n\n    def process_scenario(x_data, y_data, alpha, tau_val):\n        \"\"\"\n        Runs the full analysis for a single scenario.\n        \"\"\"\n        results_for_scenario = []\n        \n        # 1. Baseline analysis on the full dataset\n        baseline_stats = fit_and_get_stats(x_data, y_data)\n        baseline_avg_width = get_avg_pi_width(baseline_stats, x_data, alpha)\n        results_for_scenario.append(baseline_avg_width)\n        \n        # 2. Identify influential points\n        cooks_distances = baseline_stats['cooks_d']\n        influential_indices = np.where(cooks_distances > tau_val)[0]\n        # np.where returns sorted indices, so no extra sorting is needed.\n        \n        # 3. Re-run analysis after removing each influential point one-by-one\n        for idx in influential_indices:\n            x_new = np.delete(x_data, idx)\n            y_new = np.delete(y_data, idx)\n            \n            new_stats = fit_and_get_stats(x_new, y_new)\n            new_avg_width = get_avg_pi_width(new_stats, x_new, alpha)\n            results_for_scenario.append(new_avg_width)\n            \n        return results_for_scenario\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"x_raw\": [0,1,2,3,4,5,6,7,8,9,10,11],\n            \"y_def\": lambda x, r: 2 + 1.3 * x + r,\n            \"r_raw\": [0.2,-0.1,0.0,0.1,-0.2,0.05,-0.15,0.1,-0.05,0.1,16.0,-0.1],\n            \"alpha\": 0.05,\n            \"tau_rule\": lambda n: 4/n\n        },\n        {\n            \"x_raw\": [0,1,2,3,4,5,6,7,8,9],\n            \"y_def\": lambda x, r: 1 + 2 * x + r,\n            \"r_raw\": [0.02,-0.03,0.01,0.00,-0.02,0.01,-0.01,0.02,0.00,-0.02],\n            \"alpha\": 0.05,\n            \"tau_rule\": lambda n: 10\n        },\n        {\n            \"x_raw\": [-5,-3,-1,0,1,2,3,4,5,20,-10,6,7,8,9],\n            \"y_def\": lambda x, r: 0.5 + 0.8 * x + r,\n            \"r_raw\": [0.1,-0.1,0.05,0.0,0.02,-0.03,0.04,-0.02,0.01,-11.5,12.5,-0.05,0.03,-0.04,0.02],\n            \"alpha\": 0.05,\n            \"tau_rule\": lambda n: 4/n\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        x = np.array(case['x_raw'], dtype=float)\n        r = np.array(case['r_raw'], dtype=float)\n        y = case['y_def'](x, r)\n        alpha = case['alpha']\n        n = len(x)\n        tau = case['tau_rule'](n)\n        \n        scenario_results = process_scenario(x, y, alpha, tau)\n        all_results.append(scenario_results)\n\n    # Format the final output string as specified\n    output_parts = []\n    for res_list in all_results:\n        formatted_list = [f\"{x:.6f}\" for x in res_list]\n        output_parts.append(f\"[{','.join(formatted_list)}]\")\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "预测的可靠性并非在所有地方都一样。在远离我们已有数据中心的位置进行预测（即外推），其不确定性会急剧增加，这一现象由“杠杆率”（leverage）这一概念量化。这个练习将挑战你从被动观察转向主动探索：在一个给定的设计中，找到那个能让预测区间宽度达到最大的“最危险”的预测点。通过解决这个优化问题，你将对杠杆率的几何意义及其在规避不可靠预测中的重要性有更深刻的认识。",
            "id": "3160029",
            "problem": "考虑一个固定设计线性回归模型，其设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 为非随机矩阵，响应向量为 $y \\in \\mathbb{R}^{n}$。假设数据由模型 $y = X\\beta + \\varepsilon$ 生成，其中 $\\beta \\in \\mathbb{R}^{p}$ 是未知的，噪声向量 $\\varepsilon \\in \\mathbb{R}^{n}$ 的各分量独立同分布，满足 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$，其中 $\\sigma^2 \\gt 0$。用于预测的新协变量向量记为 $x_0 \\in \\mathbb{R}^{p}$，相应的新响应为 $y_0 = x_0^\\top \\beta + \\varepsilon_0$，其中 $\\varepsilon_0 \\sim \\mathcal{N}(0,\\sigma^2)$ 且与 $\\varepsilon$ 独立。\n\n从这些假设和普通最小二乘（OLS）估计量的定义出发，推导在显著性水平 $\\alpha \\in (0,1)$ 下，对给定 $x_0$ 的 $y_0$ 的双侧预测区间。将其半宽表示为残差标准误、自由度以及一个依赖于 $x_0$ 和 $X$ 的标量的函数。说明对 $x_0$ 的依赖关系是如何引入的，并找出在给定约束集下使半宽最大化的 $x_0$。你的推导必须从 OLS 正规方程和模型所蕴含的分布性质开始，且不得假定任何快捷公式。\n\n接下来，你将实现一个程序，为下面的测试套件中的每个案例计算以下内容：\n- 在指定的 $x_0$ 候选集上，使双侧预测区间半宽最大化的标识值。\n- 依赖于 $x_0$ 和 $X$ 的相应最大化标量值。\n- 相应的最大预测区间半宽。\n\n对于所有案例，使用残差标准误（RSE），定义为 $s = \\sqrt{\\mathrm{RSS}/(n - p)}$，其中 $\\mathrm{RSS}$ 是从 OLS 计算出的残差平方和。使用自由度为 $n-p$、显著性水平为 $\\alpha$ 的学生t分布，置信水平表示为 $1 - \\alpha$（其中 $\\alpha$ 以小数形式提供）。程序必须根据每个案例指定的规则来处理最大化值出现的平局情况。\n\n测试套件：\n- 案例1（含截距的简单线性回归，样本内候选）：$X$ 是一个 $n \\times p$ 矩阵，其中 $n = 5, p = 2$，其第一列全为1（截距），第二列为 $[0,1,2,3,4]^\\top$。响应向量为 $y = [2.3, 2.9, 3.1, 3.8, 4.4]^\\top$。$x_0$ 的候选集由样本内设计行组成。使用 $\\alpha = 0.10$。平局处理规则：如果多行产生相同的最大值，则返回最小的标量 $x_0$（即 $X$ 第二列中的最小值）。\n- 案例2（含截距的简单线性回归，区间约束候选）：使用与案例1相同的 $X$ 和 $y$。对于标量预测变量，$x_0$ 的候选集被约束在闭区间 $[x_{\\min}, x_{\\max}] = [0,4]$ 内，设计向量为 $[1, x_0]^\\top$。使用 $\\alpha = 0.10$。规则：选择 $[0,4]$ 中使半宽最大化的端点；如果两个端点产生的值在 $10^{-12}$ 的数值公差内相同，则返回较小的端点。\n- 案例3（含截距的多元线性回归，样本内候选）：$X$ 是一个 $n \\times p$ 矩阵，其中 $n = 6, p = 3$，第一列全为1（截距），第二列为 $x_1 = [0,1,2,-1,-2,3]^\\top$，第三列为 $x_2 = [0,2,-1,1,-2,4]^\\top$。响应向量为 $y = [1.6, 1.65, 3.4, 0.6, 0.55, 2.6]^\\top$。$x_0$ 的候选集由样本内设计行组成。使用 $\\alpha = 0.05$。平局处理规则：如果多行产生相同的最大值，则返回候选行中最小的从零开始的行索引。\n\n程序要求：\n- 使用 $X$ 和 $y$ 计算 OLS 估计量 $\\hat{\\beta}$。\n- 计算残差标准误 $s$ 和自由度 $n - p$。\n- 对于每个候选 $x_0$，计算决定预测区间半宽对 $x_0$ 和 $X$ 依赖关系的标量值，然后计算在显著性水平 $\\alpha$ 下的半宽。\n- 应用特定案例的平局处理规则来确定最大化的 $x_0$。\n- 对于案例1，输出使半宽最大化的标量 $x_0$（$X$ 第二列中的值）、相应的标量值以及相应的最大半宽，形式为一个包含三个实数的列表。\n- 对于案例2，输出按规则选择的端点标量 $x_0 \\in \\{0,4\\}$、相应的标量值以及相应的最大半宽，形式为一个包含三个实数的列表。\n- 对于案例3，输出使半宽最大化的样本内设计行的从零开始的行索引、相应的标量值以及相应的最大半宽，形式为一个列表，其中第一个元素是整数，其余元素是实数。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表；每个案例的结果本身必须是指定格式的列表。例如，输出必须类似于 $[[r_{1,1},r_{1,2},r_{1,3}],[r_{2,1},r_{2,2},r_{2,3}],[r_{3,1},r_{3,2},r_{3,3}]]$，不含空格。",
            "solution": "该问题要求在一个固定设计线性回归模型中，为一个新观测值推导双侧预测区间，并随后将此结果应用于具体的数值案例。推导过程必须源于普通最小二乘法（OLS）的基本原理和模型的分布假设。\n\n设线性回归模型指定为 $y = X\\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是满列秩的非随机设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是未知系数向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是不可观测的误差向量。假设误差是独立同分布（i.i.d.）的，即 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$，这意味着向量 $\\varepsilon$ 服从多元正态分布 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\n一个新观测值由相同的过程生成，具有新的协变量向量 $x_0 \\in \\mathbb{R}^{p}$ 和相应的响应 $y_0 = x_0^\\top \\beta + \\varepsilon_0$，其中 $\\varepsilon_0 \\sim \\mathcal{N}(0,\\sigma^2)$ 且独立于原始误差向量 $\\varepsilon$。\n\n**第1步：普通最小二乘（OLS）估计量**\nOLS 估计量 $\\hat{\\beta}$ 是通过最小化残差平方和（RSS）得到的，RSS 定义为残差向量 $e = y - X\\beta$ 的欧几里得范数的平方：\n$$ \\mathrm{RSS}(\\beta) = e^\\top e = (y - X\\beta)^\\top(y - X\\beta) = y^\\top y - 2\\beta^\\top X^\\top y + \\beta^\\top X^\\top X \\beta $$\n为了找到最小值，我们将 $\\mathrm{RSS}(\\beta)$ 对 $\\beta$ 求导，并令梯度为零：\n$$ \\frac{\\partial \\mathrm{RSS}}{\\partial \\beta} = -2X^\\top y + 2X^\\top X \\beta = 0 $$\n这得到了正规方程组：\n$$ (X^\\top X)\\hat{\\beta} = X^\\top y $$\n假设 $X$ 是满列秩的，则矩阵 $X^\\top X$ 可逆。因此，$\\beta$ 的 OLS 估计量是唯一的，由下式给出：\n$$ \\hat{\\beta} = (X^\\top X)^{-1}X^\\top y $$\n\n**第2步：OLS 估计量的分布**\n由于 $\\hat{\\beta}$ 是正态分布向量 $y$ 的线性变换，因此 $\\hat{\\beta}$ 也服从正态分布。\n$\\hat{\\beta}$ 的期望值为：\n$$ E[\\hat{\\beta}] = E[(X^\\top X)^{-1}X^\\top y] = (X^\\top X)^{-1}X^\\top E[y] = (X^\\top X)^{-1}X^\\top(X\\beta) = \\beta $$\n这表明 $\\hat{\\beta}$ 是 $\\beta$ 的无偏估计量。\n$\\hat{\\beta}$ 的协方差矩阵为：\n$$ \\mathrm{Cov}(\\hat{\\beta}) = \\mathrm{Cov}((X^\\top X)^{-1}X^\\top y) = (X^\\top X)^{-1}X^\\top \\mathrm{Cov}(y) (X^\\top X)^{-1} = (X^\\top X)^{-1}X^\\top (\\sigma^2 I_n) X (X^\\top X)^{-1} = \\sigma^2 (X^\\top X)^{-1} $$\n因此，OLS 估计量的分布为 $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2(X^\\top X)^{-1})$。\n\n**第3步：预测误差**\n我们的目标是预测新响应 $y_0$。自然的预测量是 $\\hat{y}_0 = x_0^\\top \\hat{\\beta}$。预测误差是未来实际响应 $y_0$ 与我们的预测 $\\hat{y}_0$ 之间的差值：\n$$ e_0 = y_0 - \\hat{y}_0 = (x_0^\\top \\beta + \\varepsilon_0) - x_0^\\top \\hat{\\beta} = x_0^\\top(\\beta - \\hat{\\beta}) + \\varepsilon_0 $$\n该误差是正态分布随机变量的和，因此它也服从正态分布。\n预测误差的期望值为：\n$$ E[e_0] = E[x_0^\\top(\\beta - \\hat{\\beta})] + E[\\varepsilon_0] = x_0^\\top(E[\\beta] - E[\\hat{\\beta}]) + 0 = x_0^\\top(\\beta - \\beta) = 0 $$\n计算预测误差的方差时，注意到 $\\hat{\\beta}$（依赖于 $\\varepsilon$）和 $\\varepsilon_0$ 是独立的：\n$$ \\mathrm{Var}(e_0) = \\mathrm{Var}(x_0^\\top(\\beta - \\hat{\\beta}) + \\varepsilon_0) = \\mathrm{Var}(x_0^\\top(\\hat{\\beta} - \\beta)) + \\mathrm{Var}(\\varepsilon_0) $$\n第一项是预测量 $\\hat{y}_0$ 的方差：\n$$ \\mathrm{Var}(x_0^\\top \\hat{\\beta}) = x_0^\\top \\mathrm{Cov}(\\hat{\\beta}) x_0 = x_0^\\top (\\sigma^2 (X^\\top X)^{-1}) x_0 = \\sigma^2 x_0^\\top(X^\\top X)^{-1}x_0 $$\n第二项是新误差的方差：$\\mathrm{Var}(\\varepsilon_0) = \\sigma^2$。\n将它们结合起来，预测误差的总方差为：\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\sigma^2 x_0^\\top(X^\\top X)^{-1}x_0 + \\sigma^2 = \\sigma^2 \\left(1 + x_0^\\top(X^\\top X)^{-1}x_0\\right) $$\n因此，预测误差服从分布 $y_0 - \\hat{y}_0 \\sim \\mathcal{N}\\left(0, \\sigma^2 \\left(1 + x_0^\\top(X^\\top X)^{-1}x_0\\right)\\right)$。\n\n**第4步：构造枢轴量**\n预测误差的方差依赖于未知参数 $\\sigma^2$。我们必须对其进行估计。$\\sigma^2$ 的标准无偏估计量是 $s^2 = \\frac{\\mathrm{RSS}}{n-p}$，其中 $\\mathrm{RSS} = (y - X\\hat{\\beta})^\\top(y - X\\hat{\\beta})$ 是残差平方和，$n-p$ 是自由度。该量的平方根 $s$ 是残差标准误（RSE）。\n线性模型理论中的一个关键结果（来自 Cochran 定理）指出：\n$$ \\frac{\\mathrm{RSS}}{\\sigma^2} = \\frac{(n-p)s^2}{\\sigma^2} \\sim \\chi^2_{n-p} $$\n其中 $\\chi^2_{n-p}$ 是自由度为 $n-p$ 的卡方分布。此外，$\\hat{\\beta}$ 和 RSS 是独立的。\n\n我们现在可以构造一个枢轴量（一个其分布不依赖于参数的可观测值和参数的函数）。我们用真实标准差来标准化预测误差，得到一个标准正态变量，然后将其除以独立的、经过缩放的卡方变量的平方根：\n$$ T = \\frac{(y_0 - \\hat{y}_0) / \\left(\\sigma \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0}\\right)}{\\sqrt{ \\frac{(n-p)s^2}{\\sigma^2} / (n-p) }} = \\frac{(y_0 - \\hat{y}_0) / \\left(\\sigma \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0}\\right)}{s/\\sigma} $$\n未知的 $\\sigma$ 项相互抵消，得到：\n$$ T = \\frac{y_0 - \\hat{y}_0}{s \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0}} $$\n该量服从自由度为 $n-p$ 的学生t分布，即 $T \\sim t_{n-p}$。\n\n**第5步：预测区间的推导**\n一个 $100(1-\\alpha)\\%$ 的 $y_0$ 预测区间是基于枢轴量 $T$ 构建的。设 $t_{n-p, \\alpha/2}$ 是来自 $t_{n-p}$ 分布的临界值，使得 $P(T  t_{n-p, \\alpha/2}) = \\alpha/2$。根据对称性，有 $P(|T| \\le t_{n-p, \\alpha/2}) = 1-\\alpha$。\n$$ P\\left(-t_{n-p, \\alpha/2} \\le \\frac{y_0 - \\hat{y}_0}{s \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0}} \\le t_{n-p, \\alpha/2}\\right) = 1-\\alpha $$\n将 $y_0$ 在不等式中分离出来，得到：\n$$ \\hat{y}_0 - t_{n-p, \\alpha/2} \\cdot s \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0} \\le y_0 \\le \\hat{y}_0 + t_{n-p, \\alpha/2} \\cdot s \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0} $$\n$y_0$ 的双侧预测区间为：\n$$ \\hat{y}_0 \\pm t_{n-p, \\alpha/2} \\cdot s \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0} $$\n\n**第6步：预测区间半宽分析**\n该区间的半宽（$HW$）为：\n$$ HW(x_0) = t_{n-p, \\alpha/2} \\cdot s \\cdot \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0} $$\n对于给定的数据集和回归模型，$s$、$n$、$p$ 以及因此 $t_{n-p, \\alpha/2}$ 的值都是固定的。半宽对新协变量向量 $x_0$ 的依赖性完全包含在 $x_0^\\top(X^\\top X)^{-1}x_0$ 这一项中。我们将这个标量定义为 $h_0(x_0) = x_0^\\top(X^\\top X)^{-1}x_0$。\n\n为了最大化预测区间的半宽，我们必须在指定的 $x_0$ 候选集上最大化 $h_0(x_0)$。量 $h_0(x_0)$ 是杠杆率的一种度量。它可以被解释为点 $x_0$ 到 $X$ 中数据点中心的平方统计距离（具体来说是马氏距离），并由 $X$ 的协方差结构进行缩放。远离现有数据中心的点 $x_0$ 具有更高的杠杆率，导致更宽、更不确定的预测区间。\n\n**算法规划：**\n对于每个测试案例：\n1.  构建设计矩阵 $X$ 和响应向量 $y$。\n2.  计算观测数 $n$ 和参数个数 $p$。\n3.  计算 OLS 估计 $\\hat{\\beta} = (X^\\top X)^{-1}X^\\top y$。\n4.  计算残差 $e = y - X\\hat{\\beta}$ 和残差平方和 RSS, $\\mathrm{RSS} = e^\\top e$。\n5.  计算自由度 $df = n - p$ 和残差标准误 RSE, $s = \\sqrt{\\mathrm{RSS}/df}$。\n6.  根据给定的显著性水平 $\\alpha$ 确定临界t值 $t_{df, \\alpha/2}$。\n7.  预计算矩阵 $(X^\\top X)^{-1}$。\n8.  遍历指定的向量 $x_0$ 候选集：\n    a. 对每个候选 $x_0$，计算标量 $h_0 = x_0^\\top(X^\\top X)^{-1}x_0$。\n9.  应用指定的平局处理规则，确定使 $h_0$ 最大化的候选 $x_0^*$。\n10. 对于这个最大化候选 $x_0^*$，计算最大半宽 $HW_{max} = t_{df, \\alpha/2} \\cdot s \\cdot \\sqrt{1 + h_0(x_0^*)}$。\n11. 组合并报告 $x_0^*$ 的标识符、最大化值 $h_0(x_0^*)$ 以及最大半宽 $HW_{max}$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Solves the prediction interval maximization problem for three test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"case_id\": 1,\n            \"X_def\": {\n                \"cols\": [np.ones(5), np.array([0, 1, 2, 3, 4])]\n            },\n            \"y\": np.array([2.3, 2.9, 3.1, 3.8, 4.4]),\n            \"alpha\": 0.10,\n            \"candidates_type\": \"in-sample\",\n            \"tie_break_rule\": \"smallest_scalar_x0\"\n        },\n        {\n            \"case_id\": 2,\n            \"X_def\": {\n                \"cols\": [np.ones(5), np.array([0, 1, 2, 3, 4])]\n            },\n            \"y\": np.array([2.3, 2.9, 3.1, 3.8, 4.4]),\n            \"alpha\": 0.10,\n            \"candidates_type\": \"interval\",\n            \"candidate_interval\": [0, 4],\n            \"tie_break_rule\": \"smaller_endpoint\"\n        },\n        {\n            \"case_id\": 3,\n            \"X_def\": {\n                \"cols\": [np.ones(6), np.array([0, 1, 2, -1, -2, 3]), np.array([0, 2, -1, 1, -2, 4])]\n            },\n            \"y\": np.array([1.6, 1.65, 3.4, 0.6, 0.55, 2.6]),\n            \"alpha\": 0.05,\n            \"candidates_type\": \"in-sample\",\n            \"tie_break_rule\": \"smallest_row_index\"\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Step 1: Unpack case data and construct X matrix\n        X = np.stack(case[\"X_def\"][\"cols\"], axis=1)\n        y = case[\"y\"]\n        alpha = case[\"alpha\"]\n        n, p = X.shape\n\n        # Step 2-5: Perform OLS regression and compute key statistics\n        # (X^T X)^-1\n        try:\n            X_t_X_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            # Handle cases where X is not full rank, though not expected here.\n            results.append([\"error\", \"singular_matrix\", \"n/a\"])\n            continue\n        \n        # OLS estimator beta_hat\n        beta_hat = X_t_X_inv @ X.T @ y\n        \n        # Residuals and RSS\n        residuals = y - (X @ beta_hat)\n        rss = residuals.T @ residuals\n        \n        # Degrees of freedom and Residual Standard Error (RSE)\n        df = n - p\n        rse = np.sqrt(rss / df)\n        \n        # Step 6: Get critical t-value\n        t_crit = t.ppf(1 - alpha / 2, df)\n\n        # Step 7-11: Find the maximizing candidate x0 and compute results\n        if case[\"candidates_type\"] == \"in-sample\":\n            # For in-sample candidates, the scalar quantity h_0 is the diagonal of the hat matrix\n            hat_matrix_diag = np.einsum('ij,ji->i', X, (X_t_X_inv @ X.T))\n\n            max_h0 = -1.0\n            best_identifier = -1\n            \n            # Find all candidates that achieve the maximum h0\n            max_h0_val = np.max(hat_matrix_diag)\n            \n            candidate_indices = np.where(np.isclose(hat_matrix_diag, max_h0_val))[0]\n            \n            # Apply tie-breaking rule\n            if case[\"tie_break_rule\"] == \"smallest_scalar_x0\":\n                # Assumes the scalar is in the second column (index 1)\n                candidate_scalars = X[candidate_indices, 1]\n                best_identifier = np.min(candidate_scalars)\n            elif case[\"tie_break_rule\"] == \"smallest_row_index\":\n                best_identifier = np.min(candidate_indices)\n                \n            max_h0 = max_h0_val\n\n\n        elif case[\"candidates_type\"] == \"interval\":\n            # For a convex quadratic, max on an interval is at an endpoint.\n            xmin, xmax = case[\"candidate_interval\"]\n            x0_min = np.array([1, xmin])\n            x0_max = np.array([1, xmax])\n            \n            h0_min = x0_min.T @ X_t_X_inv @ x0_min\n            h0_max_val = x0_max.T @ X_t_X_inv @ x0_max\n\n            # Apply tie-breaking rule\n            if abs(h0_max_val - h0_min)  1e-12: # Tolerance for float comparison\n                best_identifier = float(xmin)\n                max_h0 = h0_min\n            elif h0_max_val > h0_min:\n                best_identifier = float(xmax)\n                max_h0 = h0_max_val\n            else:\n                best_identifier = float(xmin)\n                max_h0 = h0_min\n        \n        # Calculate maximum half-width\n        max_half_width = t_crit * rse * np.sqrt(1 + max_h0)\n\n        if case[\"case_id\"] == 3:\n            results.append([int(best_identifier), max_h0, max_half_width])\n        else:\n            results.append([float(best_identifier), max_h0, max_half_width])\n\n    # Format the final output string without spaces\n    outer_list_str = []\n    for res_list in results:\n        inner_list_str = f\"[{','.join(map(str, res_list))}]\"\n        outer_list_str.append(inner_list_str)\n    final_output = f\"[{','.join(outer_list_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}