{
    "hands_on_practices": [
        {
            "introduction": "A prediction interval's width is not a single, static number; it is a dynamic quantity governed by several factors. This first practice invites you to look \"under the hood\" of the prediction interval formula. By combining a first-principles derivation with a controlled simulation, you will isolate and quantify how sample size ($n$), the residual standard error ($\\hat{\\sigma}$), and the Student's $t$-quantile work together to determine predictive uncertainty . This exercise builds a foundational understanding of why larger, less noisy datasets lead to more precise predictions.",
            "id": "3160052",
            "problem": "You are asked to connect the construction of a two-sided prediction interval in a linear model to how the Student’s $t$-quantile and the residual standard error jointly control its width. Work from first principles, starting from the following base.\n\nAssume the linear model $y = X\\beta + \\varepsilon$ with $n$ observations and $p$ parameters, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Use the ordinary least squares (OLS) estimator and the Gaussian error model to derive the sampling distribution of the standardized prediction error at a new design point $x_{0} \\in \\mathbb{R}^{p}$. Then derive the exact two-sided $(1 - \\alpha)$ prediction interval for a new response $y_{0}$ at $x_{0}$ and its width. Your derivation should make clear how the Student’s $t$-quantile with $n-p$ degrees of freedom and the residual standard error $\\hat{\\sigma}$ appear in the width, and how the design dependence enters via the leverage at $x_{0}$.\n\nAfter your derivation, write a program that implements the derived formulas for a controlled design that isolates and quantifies the roles of the $t$-quantile and $\\hat{\\sigma}$. Use the following deterministic setup, common across all test cases:\n\n- Model: simple linear regression with intercept and one centered predictor, so $p = 2$.\n- Design matrix $X \\in \\mathbb{R}^{n \\times 2}$ is given by the first column of ones and the second column $x \\in \\mathbb{R}^{n}$ constructed deterministically as follows: take $n$ equally spaced points on $[-1, 1]$, center them to have empirical mean $0$, then scale them so that $\\sum_{i=1}^{n} x_{i}^{2} = n$. This ensures $X^{\\top} X$ is diagonal up to numerical symmetry with diagonal approximately $(n, n)$.\n- New design point for prediction: $x_{0} = (1, 0)^{\\top}$, i.e., prediction at the centered predictor level.\n- Residual sum of squares (RSS) is fixed at the same value across comparisons, $\\mathrm{RSS} = 100$ (unitless). The residual standard error is $\\hat{\\sigma} = \\sqrt{\\mathrm{RSS}/(n-p)}$.\n- Use a two-sided nominal level specified by $\\alpha$.\n\nFor each test case below, let $n$ denote the baseline sample size and $2n$ the doubled sample size, both with the same $\\alpha$ and the same $\\mathrm{RSS} = 100$. For each test case, your program must compute:\n\n1. The ratio $r_{\\mathrm{full}}$ of the prediction-interval width at $2n$ to that at $n$ under the design above, using the exact leverage at $x_{0}$ implied by $X$ for each $n$.\n2. The absolute deviation $|r_{\\mathrm{full}} - 0.5|$ to quantify how far the width ratio is from “half.”\n3. The ratio $r_{t,\\sigma}$ obtained by removing the design leverage from the width, i.e., the ratio of the factors that depend only on the $t$-quantile and $\\hat{\\sigma}$ when going from $n$ to $2n$.\n\nAll quantities are unitless; no physical units are involved. Angles are not used.\n\nTest suite (each item is a pair $(n,\\alpha)$; use $p = 2$ and $\\mathrm{RSS} = 100$ throughout):\n- Case A (happy path, small degrees of freedom): $(n, \\alpha) = (3, 0.05)$.\n- Case B (boundary leaning, still small sample): $(n, \\alpha) = (6, 0.05)$.\n- Case C (moderate sample): $(n, \\alpha) = (20, 0.05)$.\n- Case D (effect of a tighter level on $t$): $(n, \\alpha) = (50, 0.01)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of lists in the order of the test suite, where each inner list is $[r_{\\mathrm{full}}, |r_{\\mathrm{full}} - 0.5|, r_{t,\\sigma}]$. For example, a valid output looks like\n[[a11,a12,a13],[a21,a22,a23],[a31,a32,a33],[a41,a42,a43]]\nwith floating-point numbers in place of each $a_{ij}$.",
            "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the theory of linear statistical models, well-posed with a clear objective and sufficient information, and uses objective, formal language. The task is to derive the width of a prediction interval and then implement a program to study its behavior under specific conditions.\n\nThe solution proceeds in two parts. First, a theoretical derivation of the prediction interval and its width. Second, the implementation of a program to compute specific quantities based on this derivation.\n\n**Part 1: Derivation of the Prediction Interval and its Width**\n\nWe begin with the specified linear model:\n$$ y = X\\beta + \\varepsilon $$\nwhere $y \\in \\mathbb{R}^{n}$ is the vector of observed responses, $X \\in \\mathbb{R}^{n \\times p}$ is the known design matrix of rank $p$, $\\beta \\in \\mathbb{R}^{p}$ is the vector of unknown parameters, and $\\varepsilon \\in \\mathbb{R}^{n}$ is the vector of unobserved random errors. We assume the errors are independent and identically distributed according to a Gaussian distribution, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, where $\\sigma^2$ is the unknown error variance and $I_n$ is the $n \\times n$ identity matrix.\n\nThe ordinary least squares (OLS) estimator for $\\beta$ is given by:\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y $$\nGiven the distributional assumption on $\\varepsilon$, the OLS estimator $\\hat{\\beta}$ is also a random variable with a Gaussian distribution. Its expected value is $E[\\hat{\\beta}] = (X^{\\top}X)^{-1}X^{\\top}E[y] = (X^{\\top}X)^{-1}X^{\\top}(X\\beta) = \\beta$, making it an unbiased estimator. Its covariance matrix is $\\mathrm{Var}(\\hat{\\beta}) = \\mathrm{Var}((X^{\\top}X)^{-1}X^{\\top}y) = (X^{\\top}X)^{-1}X^{\\top}(\\sigma^2 I_n)X(X^{\\top}X)^{-1} = \\sigma^2(X^{\\top}X)^{-1}$. Thus, the sampling distribution of the estimator is:\n$$ \\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2(X^{\\top}X)^{-1}) $$\n\nWe are interested in predicting a new response, $y_0$, at a new design point, $x_0 \\in \\mathbb{R}^{p}$. The model for this new observation is:\n$$ y_0 = x_0^{\\top}\\beta + \\varepsilon_0 $$\nwhere $\\varepsilon_0 \\sim \\mathcal{N}(0, \\sigma^2)$ is assumed to be independent of the training errors $\\varepsilon$.\n\nThe point prediction for $y_0$ is obtained by substituting the estimated parameters $\\hat{\\beta}$ into the model equation for the new point:\n$$ \\hat{y}_0 = x_0^{\\top}\\hat{\\beta} $$\nThe prediction error is the difference between the true future observation $y_0$ and our prediction $\\hat{y}_0$:\n$$ y_0 - \\hat{y}_0 = (x_0^{\\top}\\beta + \\varepsilon_0) - x_0^{\\top}\\hat{\\beta} = \\varepsilon_0 - x_0^{\\top}(\\hat{\\beta} - \\beta) $$\nThe prediction error is a linear combination of Gaussian random variables ($\\varepsilon_0$ and the elements of $\\hat{\\beta}$), so it is also normally distributed. Its expected value is $E[y_0 - \\hat{y}_0] = E[\\varepsilon_0] - x_0^{\\top}E[\\hat{\\beta} - \\beta] = 0 - x_0^{\\top}(0) = 0$.\n\nThe variance of the prediction error is calculated as follows, using the independence of $\\varepsilon_0$ and $\\hat{\\beta}$ (which is a function of the training data):\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\mathrm{Var}(\\varepsilon_0) + \\mathrm{Var}(x_0^{\\top}(\\hat{\\beta} - \\beta)) $$\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\sigma^2 + x_0^{\\top}\\mathrm{Var}(\\hat{\\beta})x_0 = \\sigma^2 + x_0^{\\top}(\\sigma^2(X^{\\top}X)^{-1})x_0 $$\nFactoring out $\\sigma^2$, we get:\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\sigma^2 \\left(1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0\\right) $$\nThus, the distribution of the prediction error is:\n$$ y_0 - \\hat{y}_0 \\sim \\mathcal{N}\\left(0, \\sigma^2 \\left(1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0\\right)\\right) $$\nThe term $h_0 = x_0^{\\top}(X^{\\top}X)^{-1}x_0$ is known as the leverage of the new point $x_0$.\n\nIf $\\sigma^2$ were known, we could form a standard normal pivot:\n$$ Z = \\frac{y_0 - \\hat{y}_0}{\\sigma \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}} \\sim \\mathcal{N}(0, 1) $$\nHowever, $\\sigma^2$ is typically unknown and must be estimated from the data. The unbiased estimator for $\\sigma^2$ is the mean squared error (MSE), based on the residual sum of squares (RSS):\n$$ \\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{n-p} = \\frac{1}{n-p}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $$\nThe quantity $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$ is the residual standard error. A fundamental result of linear model theory under Gaussian errors states that:\n$$ \\frac{(n-p)\\hat{\\sigma}^2}{\\sigma^2} = \\frac{\\mathrm{RSS}}{\\sigma^2} \\sim \\chi^2_{n-p} $$\nwhere $\\chi^2_{n-p}$ is the chi-squared distribution with $n-p$ degrees of freedom. Furthermore, $\\hat{\\beta}$ and $\\hat{\\sigma}^2$ are independent.\n\nTo form a pivotal quantity that does not depend on $\\sigma$, we construct a Student's $t$-statistic. This statistic is the ratio of a standard normal variable to the square root of an independent chi-squared variable divided by its degrees of freedom.\n$$ T = \\frac{\\frac{y_0 - \\hat{y}_0}{\\sigma \\sqrt{1 + h_0}}}{\\sqrt{\\frac{(n-p)\\hat{\\sigma}^2/\\sigma^2}{n-p}}} = \\frac{\\frac{y_0 - \\hat{y}_0}{\\sigma \\sqrt{1 + h_0}}}{\\frac{\\hat{\\sigma}}{\\sigma}} = \\frac{y_0 - \\hat{y}_0}{\\hat{\\sigma} \\sqrt{1 + h_0}} $$\nThis quantity $T$ follows a Student's $t$-distribution with $n-p$ degrees of freedom:\n$$ \\frac{y_0 - \\hat{y}_0}{\\hat{\\sigma} \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}} \\sim t_{n-p} $$\nThis is the sampling distribution of the standardized prediction error.\n\nTo construct a two-sided $(1 - \\alpha)$ prediction interval for $y_0$, we use the quantiles of the $t_{n-p}$ distribution. Let $t_{\\alpha/2, n-p}$ be the value such that $P(T > t_{\\alpha/2, n-p}) = \\alpha/2$. Then:\n$$ P(-t_{\\alpha/2, n-p} \\le \\frac{y_0 - \\hat{y}_0}{\\hat{\\sigma} \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}} \\le t_{\\alpha/2, n-p}) = 1 - \\alpha $$\nIsolating $y_0$ in the center of the inequality yields the prediction interval:\n$$ \\hat{y}_0 \\pm t_{\\alpha/2, n-p} \\hat{\\sigma} \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0} $$\nThe width, $W$, of this interval is the difference between the upper and lower bounds:\n$$ W = 2 \\cdot t_{\\alpha/2, n-p} \\cdot \\hat{\\sigma} \\cdot \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0} $$\nThis expression clearly decomposes the width into three key components as requested:\n1.  The Student's $t$-quantile, $t_{\\alpha/2, n-p}$, which depends on the confidence level $\\alpha$ and the degrees of freedom $n-p$.\n2.  The residual standard error, $\\hat{\\sigma} = \\sqrt{\\mathrm{RSS}/(n-p)}$, which estimates the intrinsic variability $\\sigma$ of the data.\n3.  The design-dependent factor, $\\sqrt{1+h_0} = \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}$, which accounts for the uncertainty in estimating $\\beta$ and how it propagates to the prediction at $x_0$. It depends on the training design $X$ and the location of the new point $x_0$.\n\n**Part 2: Implementation for the Specified Design**\n\nThe problem asks for an implementation under a controlled setup. For simple linear regression ($p=2$), the design matrix $X$ has a first column of ones and a second column of centered predictors $x_i$ such that $\\sum_{i=1}^n x_i = 0$ and $\\sum_{i=1}^n x_i^2 = n$. For this specific design, the matrix $X^{\\top}X$ becomes diagonal:\n$$ X^{\\top}X = \\begin{pmatrix} \\sum 1 & \\sum x_i \\\\ \\sum x_i & \\sum x_i^2 \\end{pmatrix} = \\begin{pmatrix} n & 0 \\\\ 0 & n \\end{pmatrix} = nI_2 $$\nIts inverse is $(X^{\\top}X)^{-1} = \\frac{1}{n}I_2$.\nThe new design point is $x_0 = (1, 0)^{\\top}$. The leverage at this point is:\n$$ h_0 = x_0^{\\top}(X^{\\top}X)^{-1}x_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1/n & 0 \\\\ 0 & 1/n \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{n} $$\nTherefore, the width formula simplifies for this specific case to:\n$$ W(n, \\alpha) = 2 \\cdot t_{\\alpha/2, n-2} \\cdot \\sqrt{\\frac{\\mathrm{RSS}}{n-2}} \\cdot \\sqrt{1 + \\frac{1}{n}} $$\nThe program will compute this width (or its proportional components) for sample sizes $n$ and $2n$ to find the required ratios. The ratio of widths $W_{2n} / W_n$ is:\n$$ r_{\\mathrm{full}} = \\frac{2 \\cdot t_{\\alpha/2, 2n-2} \\cdot \\sqrt{\\frac{\\mathrm{RSS}}{2n-2}} \\cdot \\sqrt{1 + \\frac{1}{2n}}}{2 \\cdot t_{\\alpha/2, n-2} \\cdot \\sqrt{\\frac{\\mathrm{RSS}}{n-2}} \\cdot \\sqrt{1 + \\frac{1}{n}}} = \\left(\\frac{t_{\\alpha/2, 2n-2}}{t_{\\alpha/2, n-2}}\\right) \\left(\\sqrt{\\frac{n-2}{2n-2}}\\right) \\left(\\sqrt{\\frac{1+1/(2n)}{1+1/n}}\\right) $$\nThe ratio $r_{t,\\sigma}$ is the product of the first two terms in the expression above, excluding the leverage-dependent term.\n$$ r_{t,\\sigma} = \\left(\\frac{t_{\\alpha/2, 2n-2}}{t_{\\alpha/2, n-2}}\\right) \\left(\\sqrt{\\frac{n-2}{2n-2}}\\right) $$\nThe following code implements these calculations for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Calculates prediction interval width ratios for specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (baseline sample size n, significance level alpha)\n    test_cases = [\n        (3, 0.05),   # Case A\n        (6, 0.05),   # Case B\n        (20, 0.05),  # Case C\n        (50, 0.01)   # Case D\n    ]\n\n    # Fixed parameters from the problem statement\n    p = 2      # Number of parameters (intercept + 1 predictor)\n    RSS = 100.0  # Residual Sum of Squares\n\n    results = []\n\n    def calculate_width_components(n_sample, alpha_level):\n        \"\"\"\n        Calculates the components of the prediction interval width.\n        \n        Args:\n            n_sample (int): The sample size n.\n            alpha_level (float): The significance level alpha.\n            \n        Returns:\n            A tuple containing the t-quantile, residual standard error, and leverage factor.\n        \"\"\"\n        if n_sample = p:\n            # Degrees of freedom must be positive.\n            return np.nan, np.nan, np.nan\n        \n        # Degrees of freedom for the t-distribution\n        df = n_sample - p\n        \n        # 1. Student's t-quantile\n        t_quantile = t.ppf(1 - alpha_level / 2, df)\n        \n        # 2. Residual standard error\n        sigma_hat = np.sqrt(RSS / df)\n        \n        # 3. Design dependence (leverage factor)\n        # For the given design, h_0 = 1/n.\n        h0 = 1 / n_sample\n        leverage_factor = np.sqrt(1 + h0)\n        \n        return t_quantile, sigma_hat, leverage_factor\n\n    for n_base, alpha in test_cases:\n        # Calculate components for the baseline sample size n\n        t_n, sigma_n, lev_n = calculate_width_components(n_base, alpha)\n        \n        # Calculate components for the doubled sample size 2n\n        n_doubled = 2 * n_base\n        t_2n, sigma_2n, lev_2n = calculate_width_components(n_doubled, alpha)\n        \n        # --- Calculate r_full ---\n        # The width is proportional to t_quantile * sigma_hat * leverage_factor.\n        # The constant factor of 2 cancels in the ratio.\n        width_prop_n = t_n * sigma_n * lev_n\n        width_prop_2n = t_2n * sigma_2n * lev_2n\n        \n        r_full = width_prop_2n / width_prop_n\n        \n        # --- Calculate |r_full - 0.5| ---\n        abs_dev_from_half = abs(r_full - 0.5)\n        \n        # --- Calculate r_t,sigma ---\n        # Ratio of factors depending only on t-quantile and sigma_hat\n        factor_ts_n = t_n * sigma_n\n        factor_ts_2n = t_2n * sigma_2n\n        \n        r_t_sigma = factor_ts_2n / factor_ts_n\n        \n        # Append the triple of results for the current test case.\n        results.append([r_full, abs_dev_from_half, r_t_sigma])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While larger datasets improve precision, the location of your prediction point is just as critical. This practice demonstrates one of the most important rules of predictive modeling: be wary of extrapolation. You will programmatically explore how the leverage term causes prediction interval widths to explode when predicting outside the range of your training data and test practical remedies like data augmentation . This provides a critical lesson in understanding a model's \"domain of competence\" and the geometric nature of prediction uncertainty.",
            "id": "3160049",
            "problem": "Consider the standard simple linear regression model under the normal linear model assumptions: There are $n$ observations, each with a scalar predictor $x \\in \\mathbb{R}$ and response $y \\in \\mathbb{R}$ satisfying $y = \\beta_0 + \\beta_1 x + \\varepsilon$, where the noise term $\\varepsilon$ is independent and identically distributed with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ and is independent of $x$. The training design matrix $X$ is constructed with an intercept column and the predictor column, and the Ordinary Least Squares (OLS) estimator is used to fit the model. The prediction interval (PI) for a future response at a new predictor value $x_0$ should be constructed based on the sampling distribution implied by the normal linear model and an estimate of the residual variance from OLS residuals. The Residual Standard Error (RSE) is the square root of the estimated residual variance. The leverage at $x_0$, denoted $h_0$, is defined using the OLS geometry and depends on the inverse of the empirical second-moment matrix of the design.\n\nYour task is to programmatically create a concrete scenario that exhibits prediction at or near the boundary of the predictor space and to quantify how the leverage $h_0$ grows and how the prediction interval width responds. You must then implement and quantify two remedies: (i) data augmentation near the boundary and (ii) a constraint that forbids predictions outside the observed predictor range by clipping $x_0$ to the observed range.\n\nUse the following scientifically realistic and reproducible setup:\n- Fix the random seed at $42$.\n- True parameters: $\\beta_0 = 1.25$, $\\beta_1 = -0.8$, and $\\sigma = 0.4$.\n- Baseline training design: generate $n = 30$ training predictors $x_i$ independently from a uniform distribution on $[-1, 1]$, i.e., $x_i \\sim \\mathrm{Uniform}(-1, 1)$, and responses $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n- Fit the OLS model with an intercept to obtain the OLS estimate and the Residual Standard Error (RSE) $s$ from the training residuals.\n\nFor each test case, compute:\n- The leverage $h_0$ at the specified $x_0$ using the hat-matrix geometry induced by OLS.\n- The Residual Standard Error $s$ from the fitted OLS model for the training set used in that case.\n- The full width $W$ of the symmetric prediction interval at confidence level $0.95$ for a new response at $x_0$ constructed from the appropriate sampling distribution under the normal linear model with unknown variance.\n\nDesign the following test suite:\n- Case $1$ (interior prediction): Baseline training; predict at $x_0 = 0.0$.\n- Case $2$ (boundary prediction): Baseline training; predict at $x_0 = 1.0$.\n- Case $3$ (outside prediction): Baseline training; predict at $x_0 = 1.5$.\n- Case $4$ (data augmentation remedy): Augment the baseline training set by adding $m = 30$ additional predictors drawn independently from a uniform distribution on $[1.2, 1.8]$ and responses generated from the same true model; refit OLS on the augmented data; predict at $x_0 = 1.5$.\n- Case $5$ (constraint remedy by clipping): Under the baseline training fit, clip the outside prediction $x_0 = 1.5$ to the observed predictor range $[-1, 1]$ (which yields $x_0^\\text{clip} = 1.0$) and compute the prediction interval at the clipped value.\n\nYour program must:\n- Implement OLS with an intercept, compute the Residual Sum of Squares (RSS), the Residual Standard Error $s$, and the inverse matrix $(X^\\top X)^{-1}$.\n- Compute the leverage $h_0$ for each specified $x_0$ using the OLS geometry.\n- Construct the $0.95$ prediction interval for a future response at each $x_0$ based on the appropriate distribution and output the full interval width $W$.\n\nFinal output format:\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must itself be a list of three floats in the order [$h_0$, $s$, $W$].\n- For example, your program should print a line like [$a_1$, $a_2$, $a_3$] for a single case, and for multiple cases it should be [$\\text{case}_1$, $\\text{case}_2$, $\\text{case}_3$, $\\text{case}_4$, $\\text{case}_5$], where each $\\text{case}_k$ is itself a list [$h_0$, $s$, $W$].\n\nAll quantities are unitless in this problem. Ensure that your implementation adheres to the definitions and constructs from the normal linear model and Ordinary Least Squares (OLS), and do not use any shortcuts or precomputed formulas beyond those implied by these foundations.",
            "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the principles of statistical learning, well-posed with a complete and consistent set of givens, and expressed in objective, formal language. It represents a standard, verifiable exercise in linear regression analysis. We may therefore proceed with a full solution.\n\n### Theoretical Framework: Simple Linear Regression\n\nWe consider the simple linear regression model:\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i = 1, \\dots, n $$\nThe model assumptions are that the noise terms $\\varepsilon_i$ are independent and identically distributed (i.i.d.) following a normal distribution with mean $0$ and variance $\\sigma^2$, denoted as $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nIn matrix notation, the model for the entire training set is:\n$$ \\mathbf{y} = X\\beta + \\varepsilon $$\nwhere $\\mathbf{y} = [y_1, \\dots, y_n]^\\top$ is the response vector, $\\beta = [\\beta_0, \\beta_1]^\\top$ is the vector of true coefficients, and $\\varepsilon = [\\varepsilon_1, \\dots, \\varepsilon_n]^\\top$ is the noise vector. The design matrix $X$ is an $n \\times 2$ matrix with a column of ones for the intercept and a column for the predictor values:\n$$ X = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ \\vdots  \\vdots \\\\ 1  x_n \\end{pmatrix} $$\n\n**1. Ordinary Least Squares (OLS) Estimation**\n\nThe OLS estimator $\\hat{\\beta}$ is found by minimizing the Residual Sum of Squares (RSS), $RSS(\\beta) = (\\mathbf{y} - X\\beta)^\\top (\\mathbf{y} - X\\beta)$. This yields the normal equations:\n$$ (X^\\top X) \\hat{\\beta} = X^\\top \\mathbf{y} $$\nAssuming $X^\\top X$ is invertible (which holds if there is variation in the $x_i$ values), the OLS estimator is unique and given by:\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top \\mathbf{y} $$\n\n**2. Residual Standard Error (RSE)**\n\nThe residuals are the differences between the observed and fitted values: $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - X\\hat{\\beta}$. The RSS can be expressed as $\\mathbf{e}^\\top \\mathbf{e}$. Under the normal linear model assumptions, an unbiased estimator for the error variance $\\sigma^2$ is the mean squared error, $s^2$:\n$$ s^2 = \\frac{RSS}{n-p} = \\frac{\\mathbf{e}^\\top \\mathbf{e}}{n-2} $$\nwhere $p=2$ is the number of parameters being estimated ($\\beta_0$ and $\\beta_1$). The Residual Standard Error (RSE) is the square root of this value, $s = \\sqrt{s^2}$, which serves as an estimate for the standard deviation of the errors, $\\sigma$.\n\n**3. Prediction and Leverage**\n\nFor a new predictor value $x_0$, the predicted response is $\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$. This can be written in vector form as $\\hat{y}_0 = (\\mathbf{x}_0^*)^\\top \\hat{\\beta}$, where $\\mathbf{x}_0^* = [1, x_0]^\\top$.\n\nThe quality of this prediction depends on its uncertainty. The prediction interval for a future response $y_0$ at $x_0$ accounts for two sources of error: the uncertainty in the estimated regression line (i.e., the variance of $\\hat{y}_0$) and the inherent variability of a new observation around the true regression line (the variance of $\\varepsilon_0$, which is $\\sigma^2$). The prediction error is $y_0 - \\hat{y}_0$. Its variance is:\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\mathrm{Var}(y_0) + \\mathrm{Var}(\\hat{y}_0) = \\sigma^2 + \\mathrm{Var}((\\mathbf{x}_0^*)^\\top \\hat{\\beta}) $$\nUsing the property $\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2(X^\\top X)^{-1}$, we get:\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\sigma^2 + (\\mathbf{x}_0^*)^\\top (\\sigma^2(X^\\top X)^{-1}) \\mathbf{x}_0^* = \\sigma^2 \\left( 1 + (\\mathbf{x}_0^*)^\\top (X^\\top X)^{-1} \\mathbf{x}_0^* \\right) $$\nThe term $h_0 = (\\mathbf{x}_0^*)^\\top (X^\\top X)^{-1} \\mathbf{x}_0^*$ is defined as the **leverage** of the point $x_0$. It measures the influence of this point on its own fitted value. The variance of the prediction error is thus $\\sigma^2(1+h_0)$. An analytical expression for leverage in simple linear regression is $h_0 = \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$, which clearly shows that leverage increases quadratically as $x_0$ moves away from the mean of the training predictors, $\\bar{x}$.\n\n**4. Prediction Interval (PI)**\n\nSince $\\sigma^2$ is unknown, we use its estimate $s^2$. The standardized prediction error follows a Student's t-distribution with $n-2$ degrees of freedom:\n$$ \\frac{y_0 - \\hat{y}_0}{s \\sqrt{1 + h_0}} \\sim t_{n-2} $$\nA $100(1 - \\alpha)\\%$ prediction interval for $y_0$ is constructed as:\n$$ \\hat{y}_0 \\pm t_{n-2, 1-\\alpha/2} \\cdot s \\sqrt{1 + h_0} $$\nwhere $t_{n-2, 1-\\alpha/2}$ is the critical value from the t-distribution for a given confidence level. For this problem, the confidence level is $0.95$, so $\\alpha=0.05$ and we use the $t$-quantile for a tail probability of $0.025$. The full width of this interval, $W$, is:\n$$ W = 2 \\cdot t_{n-2, 1-\\alpha/2} \\cdot s \\sqrt{1 + h_0} $$\nThis formula shows that the prediction interval width is directly influenced by the RSE ($s$) and grows with leverage ($h_0$).\n\n### Computational Procedure for Test Cases\n\nThe problem specifies five test cases to demonstrate these concepts.\n\n- **Case 1 (Interior):** $x_0 = 0.0$. Here, $x_0$ is near the center of the training data distribution $\\mathrm{Uniform}(-1, 1)$. We expect low leverage and a relatively narrow prediction interval.\n- **Case 2 (Boundary):** $x_0 = 1.0$. This point is at the boundary of the training data's support. Leverage will be higher than in Case 1, resulting in a wider interval.\n- **Case 3 (Outside):** $x_0 = 1.5$. This is an extrapolation, as $x_0$ lies outside the training range. Leverage, and consequently the PI width, is expected to increase substantially.\n- **Case 4 (Data Augmentation):** By adding new training data points in the region $[1.2, 1.8]$, we extend the support of the training data. When we then predict at $x_0=1.5$, this point is no longer an extrapolation but an interpolation. This should significantly reduce the leverage at $x_0=1.5$ compared to Case 3 and narrow the prediction interval. The RSE $s$ will also be re-estimated on the larger dataset of size $n+m=60$.\n- **Case 5 (Constraint by Clipping):** This remedy avoids extrapolation by fiat. Any prediction query for $x_0$ outside the established range $[-1, 1]$ is clipped to the nearest boundary point. For $x_0=1.5$, the clipped value is $x_0^\\text{clip}=1.0$. The analysis is then performed at this clipped point using the original baseline model. Consequently, the calculations and results for this case will be identical to those of Case 2.\n\nFor each case, we will programmatically generate the specified data, fit the OLS model, and compute the three required quantities: leverage $h_0$, RSE $s$, and PI width $W$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Computes leverage, RSE, and prediction interval width for five test cases\n    in a simple linear regression scenario.\n    \"\"\"\n    # Define problem constants and setup\n    RANDOM_SEED = 42\n    BETA_0 = 1.25\n    BETA_1 = -0.8\n    SIGMA = 0.4\n    N_BASE = 30\n    CONFIDENCE_LEVEL = 0.95\n    ALPHA = 1.0 - CONFIDENCE_LEVEL\n\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Generate baseline training data\n    x_base = rng.uniform(-1, 1, size=N_BASE)\n    epsilon_base = rng.normal(0, SIGMA, size=N_BASE)\n    y_base = BETA_0 + BETA_1 * x_base + epsilon_base\n\n    # Define the five test cases\n    test_cases_config = [\n        {'id': 1, 'x0': 0.0, 'augment': False, 'clip': False},\n        {'id': 2, 'x0': 1.0, 'augment': False, 'clip': False},\n        {'id': 3, 'x0': 1.5, 'augment': False, 'clip': False},\n        {'id': 4, 'x0': 1.5, 'augment': True,  'clip': False},\n        {'id': 5, 'x0': 1.5, 'augment': False, 'clip': True},\n    ]\n\n    results = []\n\n    for case in test_cases_config:\n        x_train, y_train = x_base, y_base\n        \n        # Case 4: Augment the training data\n        if case['augment']:\n            M_AUG = 30\n            x_aug = rng.uniform(1.2, 1.8, size=M_AUG)\n            epsilon_aug = rng.normal(0, SIGMA, size=M_AUG)\n            y_aug = BETA_0 + BETA_1 * x_aug + epsilon_aug\n            x_train = np.concatenate((x_base, x_aug))\n            y_train = np.concatenate((y_base, y_aug))\n\n        x_pred = case['x0']\n        \n        # Case 5: Clip the prediction point\n        if case['clip']:\n            # As per problem statement, clip to the theoretical range [-1, 1]\n            x_pred = np.clip(x_pred, -1.0, 1.0)\n            \n        # OLS estimation\n        n = len(x_train)\n        p = 2  # Number of parameters (beta_0, beta_1)\n        df = n - p # Degrees of freedom\n\n        # Construct the design matrix X\n        X = np.c_[np.ones(n), x_train]\n\n        # Calculate inverse of (X^T * X)\n        try:\n            inv_XTX = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            # This should not happen with the generated data\n            results.append([float('nan'), float('nan'), float('nan')])\n            continue\n\n        # Estimate coefficients beta_hat\n        beta_hat = inv_XTX @ X.T @ y_train\n\n        # Calculate Residual Sum of Squares (RSS)\n        y_hat = X @ beta_hat\n        residuals = y_train - y_hat\n        rss = residuals.T @ residuals\n\n        # Calculate Residual Standard Error (RSE), s\n        s = np.sqrt(rss / df)\n\n        # Calculate leverage h_0 for the new point x_pred\n        x0_star = np.array([1.0, x_pred])\n        h0 = x0_star.T @ inv_XTX @ x0_star\n\n        # Calculate prediction interval width W\n        # Find the t-critical value for the 95% PI\n        t_crit = t.ppf(1 - ALPHA / 2, df)\n        \n        # Calculate full width W\n        W = 2 * t_crit * s * np.sqrt(1 + h0)\n        \n        results.append([h0, s, W])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{h:.10f},{s:.10f},{w:.10f}]\" for h, s, w in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A model's predictions are sensitive not only to where you predict, but also to the quality of the data used for training. In this exercise, you will play the role of a data analyst investigating the stability of a regression model's uncertainty estimates . Using Cook's distance to identify influential data points, you will see firsthand how a single observation can distort the residual standard error ($\\hat{\\sigma}$) and, consequently, the width of all your prediction intervals, reinforcing the importance of regression diagnostics.",
            "id": "3160002",
            "problem": "You are given three self-contained simple linear regression scenarios. In each scenario, the data arise from the model $y = \\beta_0 + \\beta_1 x + \\varepsilon$, where the error term $\\varepsilon$ is assumed to be independent and identically distributed with mean $0$ and constant variance $\\sigma^2$. For each scenario, you must fit the ordinary least squares model with an intercept, compute the residual standard error $\\hat{\\sigma}$, identify influential observations using Cook’s distance, and study how removing each influential observation (one at a time) affects the widths of two-sided prediction intervals for a future observation at three typical design points. Your program must be a complete, runnable program that uses only the specified runtime environment. All computations must be carried out in purely mathematical terms, with no physical units. Angles are not involved. Express any confidence level as a decimal, not using a percentage sign.\n\nBase principles to use without quoting shortcut formulas in the problem statement: ordinary least squares minimizes the residual sum of squares; the residual standard error is computed from the residual sum of squares and appropriate degrees of freedom; leverage values arise from the hat matrix defined by the design matrix; Cook’s distance measures the influence of each observation on the fitted regression; two-sided prediction intervals for a future observation under the assumption of normally distributed errors are constructed using the Student’s $t$ distribution and account for both estimation uncertainty and irreducible error.\n\nImplement the following steps for each scenario:\n- Fit the ordinary least squares model with an intercept to obtain $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, and residuals.\n- Compute the residual standard error $\\hat{\\sigma}$ from the fitted residuals and the degrees of freedom $n - p$, where $n$ is the sample size and $p$ is the number of fitted parameters including the intercept.\n- Compute the hat matrix diagonal entries (leverages) and Cook’s distance for each observation. Identify as influential those observations whose Cook’s distance exceeds the specified threshold $\\tau$ for that scenario.\n- Define three typical design points as the minimum of $x$, the mean of $x$, and the maximum of $x$. For each fitted model, compute the two-sided prediction interval width at each of these three design points using a confidence level $1 - \\alpha$ (with $\\alpha$ given below). Then aggregate these three widths by taking their average to produce a single width summary for that model.\n- For each scenario, report a list of floats consisting of the baseline average width (using all data) followed by the average widths after removing each influential point one at a time (remove exactly one influential observation at a time, refit, and recompute). If there are no influential points, the list contains only the baseline average width.\n\nUse the following test suite. In each case, the arrays are to be interpreted elementwise, and all numbers are exact constants.\n\nScenario A:\n- Inputs:\n  - $x = [0,1,2,3,4,5,6,7,8,9,10,11]$.\n  - Define $y$ by $y_i = 2 + 1.3 x_i + r_i$ with error terms $r = [0.2,-0.1,0.0,0.1,-0.2,0.05,-0.15,0.1,-0.05,0.1,16.0,-0.1]$.\n  - Confidence parameter $\\alpha = 0.05$.\n  - Cook’s distance threshold rule $\\tau = 4/n$, where $n$ is the number of observations in this scenario.\n- Output for Scenario A: a list of floats $[w_0, w_1, \\dots]$ where $w_0$ is the baseline average width over the three typical design points, and $w_j$ for $j \\ge 1$ is the average width after removing the $j$-th influential observation (with influential observation indices ordered increasingly by their original index).\n\nScenario B:\n- Inputs:\n  - $x = [0,1,2,3,4,5,6,7,8,9]$.\n  - Define $y$ by $y_i = 1 + 2 x_i + r_i$ with error terms $r = [0.02,-0.03,0.01,0.00,-0.02,0.01,-0.01,0.02,0.00,-0.02]$.\n  - Confidence parameter $\\alpha = 0.05$.\n  - Cook’s distance threshold $\\tau = 10$.\n- Output for Scenario B: a list of floats as above. If no influential points are detected, return only the baseline average width.\n\nScenario C:\n- Inputs:\n  - $x = [-5,-3,-1,0,1,2,3,4,5,20,-10,6,7,8,9]$.\n  - Define $y$ by $y_i = 0.5 + 0.8 x_i + r_i$ with error terms $r = [0.1,-0.1,0.05,0.0,0.02,-0.03,0.04,-0.02,0.01,-11.5,12.5,-0.05,0.03,-0.04,0.02]$.\n  - Confidence parameter $\\alpha = 0.05$.\n  - Cook’s distance threshold rule $\\tau = 4/n$, where $n$ is the number of observations in this scenario.\n- Output for Scenario C: a list of floats as above.\n\nFinal output format:\n- Your program should produce a single line of output containing the three scenario results as a comma-separated list of lists enclosed in square brackets, for example, $[[a_1,a_2],[b_1],[c_1,c_2,c_3]]$, with each float rounded to $6$ decimal places using standard rounding to nearest. The three lists must appear in the order Scenario A, Scenario B, Scenario C. No other text should be printed.",
            "solution": "The problem requires an analysis of simple linear regression models for three distinct scenarios. The core tasks involve fitting the model, identifying influential data points, and assessing their impact on the width of prediction intervals. The methodology is grounded in the principles of ordinary least squares (OLS) regression and standard statistical diagnostics.\n\n**1. Simple Linear Regression Model**\nThe underlying statistical model is a simple linear regression:\n$$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$$\nwhere $y_i$ is the response variable for the $i$-th observation, $x_i$ is the predictor variable, $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\varepsilon_i$ are independent and identically distributed random errors with mean $E[\\varepsilon_i] = 0$ and constant variance $\\text{Var}(\\varepsilon_i) = \\sigma^2$. The problem provides the data pairs $(x_i, y_i)$ for each scenario.\n\n**2. Ordinary Least Squares (OLS) Estimation**\nThe OLS method estimates the parameters $\\beta_0$ and $\\beta_1$ by minimizing the Residual Sum of Squares (RSS). The estimated parameters, denoted $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, are given by:\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\nwhere $n$ is the number of observations, $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ is the sample mean of the predictor variable, and $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ is the sample mean of the response variable.\n\nThe fitted values are $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$, and the residuals are $e_i = y_i - \\hat{y}_i$.\n\n**3. Residual Standard Error (RSE)**\nThe RSE, denoted $\\hat{\\sigma}$, is an estimate of the standard deviation of the error term, $\\sigma$. It is calculated from the RSS:\n$$ \\text{RSS} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\nThe RSE is then computed as:\n$$ \\hat{\\sigma} = \\sqrt{\\frac{\\text{RSS}}{n-p}} $$\nwhere the degrees of freedom are $n-p$. For simple linear regression with an intercept, the number of estimated parameters is $p=2$ (for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$), so the degrees of freedom are $n-2$.\n\n**4. Influence Diagnostics**\nTo identify influential observations, we use Cook's distance. This metric combines information about a point's leverage and its residual size.\n\nFirst, the leverage $h_{ii}$ for each observation $i$ is calculated. Leverage measures how far an observation's predictor value $x_i$ is from the mean of the predictor values. A high-leverage point has the potential to strongly influence the regression fit. The leverage is the $i$-th diagonal element of the hat matrix $H = X(X^T X)^{-1} X^T$, where $X$ is the design matrix. For simple linear regression, this simplifies to:\n$$ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2} $$\n\nSecond, Cook's distance $D_i$ is computed for each observation:\n$$ D_i = \\frac{e_i^2}{p \\cdot \\hat{\\sigma}^2} \\left[ \\frac{h_{ii}}{(1-h_{ii})^2} \\right] $$\nAn observation is identified as influential if its Cook's distance exceeds a given threshold, $D_i  \\tau$. In this problem, the threshold $\\tau$ is either a fixed value or a function of the sample size $n$, such as $\\tau = 4/n$.\n\n**5. Prediction Intervals (PI)**\nA prediction interval provides a range within which a future observation $y_0$ is expected to fall for a given predictor value $x_0$, with a certain confidence level $1-\\alpha$. Assuming the errors $\\varepsilon_i$ are normally distributed, the $100(1-\\alpha)\\%$ PI is constructed as:\n$$ \\hat{y}_0 \\pm t_{n-p, \\alpha/2} \\cdot \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2}} $$\nwhere $\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$ is the predicted value at $x_0$, and $t_{n-p, \\alpha/2}$ is the critical value from the Student's $t$ distribution with $n-p$ degrees of freedom that leaves an area of $\\alpha/2$ in the upper tail. The standard error of prediction incorporates both the uncertainty in estimating the regression line and the inherent variability of a new observation ($\\sigma^2$).\n\nThe width of this interval is:\n$$ W(x_0) = 2 \\cdot t_{n-p, \\alpha/2} \\cdot \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2}} $$\n\n**6. Algorithmic Implementation**\nFor each scenario, the following procedure is executed:\n1.  **Data Generation**: Construct the vectors $x$ and $y$ as specified in the problem statement.\n2.  **Baseline Analysis**:\n    a. Fit a simple linear regression model to the full dataset $(x, y)$ to obtain $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, $\\hat{\\sigma}$, and other model statistics.\n    b. Compute the PI widths $W(x_0)$ at three specific design points: $x_{0, \\text{min}} = \\min(x)$, $x_{0, \\text{mean}} = \\bar{x}$, and $x_{0, \\text{max}} = \\max(x)$.\n    c. Calculate the baseline average width, $w_0$, by averaging the three widths. This is the first value in the result list for the scenario.\n3.  **Influence Analysis**:\n    a. Using the full model, calculate the leverages $h_{ii}$ and Cook's distances $D_i$ for all observations.\n    b. Identify the set of influential points by collecting the indices $i$ for which $D_i  \\tau$. These indices are sorted in increasing order.\n4.  **Sensitivity Analysis**:\n    a. If no influential points are found, the process for the scenario terminates, and the result is simply $[w_0]$.\n    b. If influential points are found, iterate through their sorted indices. For each influential point, remove it from the dataset, creating a new, smaller dataset.\n    c. For each new dataset, refit the regression model and re-calculate the average PI width ($w_j$) using the same procedure as in step 2 (using the new dataset's properties: $n_{new}$, $\\bar{x}_{new}$, etc.).\n    d. Append each of these new average widths ($w_1, w_2, \\dots$) to the result list.\n5.  **Final Output**: Collate the lists of widths from all scenarios and format them into the specified final output string.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to solve the three regression scenarios and print the results.\n    \"\"\"\n\n    def fit_and_get_stats(x, y):\n        \"\"\"\n        Fits a simple linear regression model and computes relevant statistics.\n        Returns a dictionary of statistics or None if fitting is not possible.\n        \"\"\"\n        n = len(x)\n        if n = 2:  # Cannot fit or compute RSE with = 2 points\n            return None\n        \n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        \n        S_xx = np.sum((x - x_mean)**2)\n        if S_xx == 0:  # Avoid division by zero if all x are the same\n            return None\n            \n        S_xy = np.sum((x - x_mean) * (y - y_mean))\n        \n        beta1_hat = S_xy / S_xx\n        beta0_hat = y_mean - beta1_hat * x_mean\n        \n        y_hat = beta0_hat + beta1_hat * x\n        residuals = y - y_hat\n        rss = np.sum(residuals**2)\n        p = 2  # Number of parameters (intercept and slope)\n        dof = n - p\n        rse = np.sqrt(rss / dof)\n        \n        leverages = 1/n + (x - x_mean)**2 / S_xx\n        cooks_d = (residuals**2 / (p * rse**2)) * (leverages / (1 - leverages)**2)\n        \n        return {\n            'n': n,\n            'dof': dof,\n            'rse': rse,\n            'cooks_d': cooks_d,\n            'x_mean': x_mean,\n            'S_xx': S_xx\n        }\n\n    def get_avg_pi_width(stats, x_coords, alpha_val):\n        \"\"\"\n        Calculates the average prediction interval width for a given fitted model.\n        \"\"\"\n        if stats is None:\n            return np.nan\n        \n        n = stats['n']\n        dof = stats['dof']\n        rse = stats['rse']\n        x_mean = stats['x_mean']\n        S_xx = stats['S_xx']\n        \n        if S_xx == 0:\n            return np.nan\n\n        t_crit = t.ppf(1 - alpha_val / 2, dof)\n        \n        x0_points = [np.min(x_coords), np.mean(x_coords), np.max(x_coords)]\n        \n        widths = []\n        for x0 in x0_points:\n            se_pred_factor = np.sqrt(1 + 1/n + (x0 - x_mean)**2 / S_xx)\n            width = 2 * t_crit * rse * se_pred_factor\n            widths.append(width)\n            \n        return np.mean(widths)\n\n    def process_scenario(x_data, y_data, alpha, tau_val):\n        \"\"\"\n        Runs the full analysis for a single scenario.\n        \"\"\"\n        results_for_scenario = []\n        \n        # 1. Baseline analysis on the full dataset\n        baseline_stats = fit_and_get_stats(x_data, y_data)\n        baseline_avg_width = get_avg_pi_width(baseline_stats, x_data, alpha)\n        results_for_scenario.append(baseline_avg_width)\n        \n        # 2. Identify influential points\n        cooks_distances = baseline_stats['cooks_d']\n        influential_indices = np.where(cooks_distances > tau_val)[0]\n        # np.where returns sorted indices, so no extra sorting is needed.\n        \n        # 3. Re-run analysis after removing each influential point one-by-one\n        for idx in influential_indices:\n            x_new = np.delete(x_data, idx)\n            y_new = np.delete(y_data, idx)\n            \n            new_stats = fit_and_get_stats(x_new, y_new)\n            new_avg_width = get_avg_pi_width(new_stats, x_new, alpha)\n            results_for_scenario.append(new_avg_width)\n            \n        return results_for_scenario\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"x_raw\": [0,1,2,3,4,5,6,7,8,9,10,11],\n            \"y_def\": lambda x, r: 2 + 1.3 * x + r,\n            \"r_raw\": [0.2,-0.1,0.0,0.1,-0.2,0.05,-0.15,0.1,-0.05,0.1,16.0,-0.1],\n            \"alpha\": 0.05,\n            \"tau_rule\": lambda n: 4/n\n        },\n        {\n            \"x_raw\": [0,1,2,3,4,5,6,7,8,9],\n            \"y_def\": lambda x, r: 1 + 2 * x + r,\n            \"r_raw\": [0.02,-0.03,0.01,0.00,-0.02,0.01,-0.01,0.02,0.00,-0.02],\n            \"alpha\": 0.05,\n            \"tau_rule\": lambda n: 10\n        },\n        {\n            \"x_raw\": [-5,-3,-1,0,1,2,3,4,5,20,-10,6,7,8,9],\n            \"y_def\": lambda x, r: 0.5 + 0.8 * x + r,\n            \"r_raw\": [0.1,-0.1,0.05,0.0,0.02,-0.03,0.04,-0.02,0.01,-11.5,12.5,-0.05,0.03,-0.04,0.02],\n            \"alpha\": 0.05,\n            \"tau_rule\": lambda n: 4/n\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        x = np.array(case['x_raw'], dtype=float)\n        r = np.array(case['r_raw'], dtype=float)\n        y = case['y_def'](x, r)\n        alpha = case['alpha']\n        n = len(x)\n        tau = case['tau_rule'](n)\n        \n        scenario_results = process_scenario(x, y, alpha, tau)\n        all_results.append(scenario_results)\n\n    # Format the final output string as specified\n    output_parts = []\n    for res_list in all_results:\n        formatted_list = [f\"{x:.6f}\" for x in res_list]\n        output_parts.append(f\"[{','.join(formatted_list)}]\")\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}