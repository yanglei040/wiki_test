{
    "hands_on_practices": [
        {
            "introduction": "In a multiple regression model, what does a single coefficient truly represent? This practice delves into the core meaning of regression coefficients by exploring the Frisch-Waugh-Lovell theorem. You will derive and compute a coefficient for a specific predictor by first \"partialling out\" the effects of all other variables from both the predictor and the response, making the abstract concept of a conditional effect concrete . This exercise is fundamental to understanding how multiple regression isolates the unique contribution of each variable.",
            "id": "3146050",
            "problem": "Consider multiple linear regression in matrix form with response vector $\\mathbf{y} \\in \\mathbb{R}^{n}$ and design matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ partitioned as $\\mathbf{X} = [\\mathbf{X}_{1}\\;\\mathbf{X}_{2}]$, where $\\mathbf{X}_{1} \\in \\mathbb{R}^{n \\times 1}$ is a single predictor of interest and $\\mathbf{X}_{2} \\in \\mathbb{R}^{n \\times q}$ contains an intercept and additional predictors. The ordinary least squares estimate is defined as the minimizer of the squared residual norm $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_{2}^{2}$ over $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$. Let the projection matrix onto the column space of $\\mathbf{X}_{2}$ be $\\mathbf{P}_{\\mathbf{X}_{2}} = \\mathbf{X}_{2}(\\mathbf{X}_{2}^{\\top}\\mathbf{X}_{2})^{-1}\\mathbf{X}_{2}^{\\top}$ and the associated residual maker matrix be $\\mathbf{M}_{\\mathbf{X}_{2}} = \\mathbf{I} - \\mathbf{P}_{\\mathbf{X}_{2}}$. Starting from these definitions and the normal equations implied by the least squares criterion, derive an explicit expression for the ordinary least squares coefficient associated with $\\mathbf{X}_{1}$ that depends only on $\\mathbf{X}_{1}$, $\\mathbf{y}$, and $\\mathbf{M}_{\\mathbf{X}_{2}}$.\n\nTo stress-test the derivation under overlapping covariate structure, consider $n = 5$ observations with\n$\\mathbf{x}_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$, $\\mathbf{z} = \\begin{pmatrix}1 \\\\ 3 \\\\ 3 \\\\ 5 \\\\ 5\\end{pmatrix}$, $\\mathbf{1}_{n} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix}$, $\\mathbf{X}_{2} = [\\mathbf{1}_{n}\\;\\mathbf{z}]$, and $\\mathbf{y} = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$.\nUse your derived expression to compute the numerical value of the coefficient on $\\mathbf{x}_{1}$. No rounding is required. Finally, explain, in terms of conditional effects, what this coefficient represents when $\\mathbf{x}_{1}$ overlaps substantially with the columns of $\\mathbf{X}_{2}$.",
            "solution": "The problem is first validated by extracting the given information and assessing its scientific soundness, completeness, and objectivity.\n\n**Step 1: Extract Givens**\n- Response vector: $\\mathbf{y} \\in \\mathbb{R}^{n}$\n- Design matrix: $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, partitioned as $\\mathbf{X} = [\\mathbf{X}_{1}\\;\\mathbf{X}_{2}]$\n- Partitions: $\\mathbf{X}_{1} \\in \\mathbb{R}^{n \\times 1}$ (predictor of interest), $\\mathbf{X}_{2} \\in \\mathbb{R}^{n \\times q}$ (intercept and other predictors). The total number of predictors is $p=1+q$.\n- OLS criterion: Minimize $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_{2}^{2}$ over $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$.\n- Projection matrix for $\\mathbf{X}_2$: $\\mathbf{P}_{\\mathbf{X}_{2}} = \\mathbf{X}_{2}(\\mathbf{X}_{2}^{\\top}\\mathbf{X}_{2})^{-1}\\mathbf{X}_{2}^{\\top}$\n- Residual maker for $\\mathbf{X}_2$: $\\mathbf{M}_{\\mathbf{X}_{2}} = \\mathbf{I} - \\mathbf{P}_{\\mathbf{X}_{2}}$\n- Task 1: Derive an explicit expression for the OLS coefficient of $\\mathbf{X}_1$, denoted $\\hat{\\beta}_1$, in terms of $\\mathbf{X}_1$, $\\mathbf{y}$, and $\\mathbf{M}_{\\mathbf{X}_2}$.\n- Task 2: For $n=5$, compute $\\hat{\\beta}_1$ given the data:\n  - $\\mathbf{X}_{1} = \\mathbf{x}_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$\n  - $\\mathbf{z} = \\begin{pmatrix}1 \\\\ 3 \\\\ 3 \\\\ 5 \\\\ 5\\end{pmatrix}$\n  - $\\mathbf{1}_{n} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix}$\n  - $\\mathbf{X}_{2} = [\\mathbf{1}_{n}\\;\\mathbf{z}]$\n  - $\\mathbf{y} = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$\n- Task 3: Explain the interpretation of the coefficient in the context of conditional effects and covariate overlap.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined and scientifically grounded. It presents a standard theoretical derivation in linear models, known as the Frisch-Waugh-Lovell theorem, followed by a direct numerical application. The provided data and definitions are self-contained and consistent. The matrices in the numerical example are specified such that the necessary matrix inverses, $(\\mathbf{X}_{2}^{\\top}\\mathbf{X}_{2})^{-1}$ and $(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}$, exist, ensuring a well-posed problem. The language is objective and formal. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n**Derivation of the Expression for $\\hat{\\beta}_1$**\n\nThe multiple linear regression model is given by $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$. The ordinary least squares (OLS) estimate $\\hat{\\boldsymbol{\\beta}}$ is the vector that minimizes the sum of squared residuals, $S(\\boldsymbol{\\beta}) = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_{2}^{2}$. The solution to this minimization problem satisfies the normal equations:\n$$\n\\mathbf{X}^{\\top}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\nWe partition the design matrix $\\mathbf{X}$ and the coefficient vector $\\boldsymbol{\\beta}$ according to the problem statement:\n$$\n\\mathbf{X} = [\\mathbf{X}_{1}\\;\\mathbf{X}_{2}] \\quad \\text{and} \\quad \\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_1 \\\\ \\boldsymbol{\\beta}_2 \\end{pmatrix}\n$$\nwhere $\\beta_1$ is a scalar corresponding to the single predictor $\\mathbf{X}_1$, and $\\boldsymbol{\\beta}_2 \\in \\mathbb{R}^q$ corresponds to the predictors in $\\mathbf{X}_2$. The OLS estimates are denoted $\\hat{\\beta}_1$ and $\\hat{\\boldsymbol{\\beta}}_2$.\n\nSubstituting the partitioned forms into the normal equations yields a block matrix system:\n$$\n\\begin{pmatrix} \\mathbf{X}_1^{\\top} \\\\ \\mathbf{X}_2^{\\top} \\end{pmatrix} [\\mathbf{X}_1\\;\\mathbf{X}_2] \\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\boldsymbol{\\beta}}_2 \\end{pmatrix} = \\begin{pmatrix} \\mathbf{X}_1^{\\top}\\mathbf{y} \\\\ \\mathbf{X}_2^{\\top}\\mathbf{y} \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} \\mathbf{X}_1^{\\top}\\mathbf{X}_1 & \\mathbf{X}_1^{\\top}\\mathbf{X}_2 \\\\ \\mathbf{X}_2^{\\top}\\mathbf{X}_1 & \\mathbf{X}_2^{\\top}\\mathbf{X}_2 \\end{pmatrix} \\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\boldsymbol{\\beta}}_2 \\end{pmatrix} = \\begin{pmatrix} \\mathbf{X}_1^{\\top}\\mathbf{y} \\\\ \\mathbf{X}_2^{\\top}\\mathbf{y} \\end{pmatrix}\n$$\nThis block system expands into two equations:\n1. $\\mathbf{X}_1^{\\top}\\mathbf{X}_1 \\hat{\\beta}_1 + \\mathbf{X}_1^{\\top}\\mathbf{X}_2 \\hat{\\boldsymbol{\\beta}}_2 = \\mathbf{X}_1^{\\top}\\mathbf{y}$\n2. $\\mathbf{X}_2^{\\top}\\mathbf{X}_1 \\hat{\\beta}_1 + \\mathbf{X}_2^{\\top}\\mathbf{X}_2 \\hat{\\boldsymbol{\\beta}}_2 = \\mathbf{X}_2^{\\top}\\mathbf{y}$\n\nOur goal is to find an expression for $\\hat{\\beta}_1$. We can achieve this by solving the second equation for $\\hat{\\boldsymbol{\\beta}}_2$ and substituting it into the first. From equation (2), assuming $\\mathbf{X}_2$ has full column rank so that $(\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1}$ exists:\n$$\n\\mathbf{X}_2^{\\top}\\mathbf{X}_2 \\hat{\\boldsymbol{\\beta}}_2 = \\mathbf{X}_2^{\\top}\\mathbf{y} - \\mathbf{X}_2^{\\top}\\mathbf{X}_1 \\hat{\\beta}_1\n$$\n$$\n\\hat{\\boldsymbol{\\beta}}_2 = (\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1}(\\mathbf{X}_2^{\\top}\\mathbf{y} - \\mathbf{X}_2^{\\top}\\mathbf{X}_1 \\hat{\\beta}_1)\n$$\nNow, substitute this expression for $\\hat{\\boldsymbol{\\beta}}_2$ into equation (1):\n$$\n\\mathbf{X}_1^{\\top}\\mathbf{X}_1 \\hat{\\beta}_1 + \\mathbf{X}_1^{\\top}\\mathbf{X}_2 (\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1}(\\mathbf{X}_2^{\\top}\\mathbf{y} - \\mathbf{X}_2^{\\top}\\mathbf{X}_1 \\hat{\\beta}_1) = \\mathbf{X}_1^{\\top}\\mathbf{y}\n$$\nDistribute the terms:\n$$\n\\mathbf{X}_1^{\\top}\\mathbf{X}_1 \\hat{\\beta}_1 + \\mathbf{X}_1^{\\top}\\mathbf{X}_2(\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1}\\mathbf{X}_2^{\\top}\\mathbf{y} - \\mathbf{X}_1^{\\top}\\mathbf{X}_2(\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1}\\mathbf{X}_2^{\\top}\\mathbf{X}_1 \\hat{\\beta}_1 = \\mathbf{X}_1^{\\top}\\mathbf{y}\n$$\nUsing the definition of the projection matrix $\\mathbf{P}_{\\mathbf{X}_2} = \\mathbf{X}_2(\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1}\\mathbf{X}_2^{\\top}$, we can simplify the equation:\n$$\n\\mathbf{X}_1^{\\top}\\mathbf{X}_1 \\hat{\\beta}_1 + \\mathbf{X}_1^{\\top}\\mathbf{P}_{\\mathbf{X}_2}\\mathbf{y} - \\mathbf{X}_1^{\\top}\\mathbf{P}_{\\mathbf{X}_2}\\mathbf{X}_1 \\hat{\\beta}_1 = \\mathbf{X}_1^{\\top}\\mathbf{y}\n$$\nGroup the terms involving $\\hat{\\beta}_1$ on the left side and the other terms on the right side:\n$$\n(\\mathbf{X}_1^{\\top}\\mathbf{X}_1 - \\mathbf{X}_1^{\\top}\\mathbf{P}_{\\mathbf{X}_2}\\mathbf{X}_1) \\hat{\\beta}_1 = \\mathbf{X}_1^{\\top}\\mathbf{y} - \\mathbf{X}_1^{\\top}\\mathbf{P}_{\\mathbf{X}_2}\\mathbf{y}\n$$\nFactor out $\\mathbf{X}_1^{\\top}$ on both sides:\n$$\n\\mathbf{X}_1^{\\top}(\\mathbf{I} - \\mathbf{P}_{\\mathbf{X}_2})\\mathbf{X}_1 \\hat{\\beta}_1 = \\mathbf{X}_1^{\\top}(\\mathbf{I} - \\mathbf{P}_{\\mathbf{X}_2})\\mathbf{y}\n$$\nUsing the definition of the residual maker matrix $\\mathbf{M}_{\\mathbf{X}_2} = \\mathbf{I} - \\mathbf{P}_{\\mathbf{X}_2}$, we arrive at:\n$$\n(\\mathbf{X}_1^{\\top}\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1) \\hat{\\beta}_1 = \\mathbf{X}_1^{\\top}\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{y}\n$$\nSince $\\mathbf{X}_1$ is a single column vector, $\\mathbf{X}_1^{\\top}\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1$ is a scalar. As long as $\\mathbf{X}_1$ is not in the column space of $\\mathbf{X}_2$ (i.e., $\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1 \\neq 0$), this scalar is non-zero, and we can solve for $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 = (\\mathbf{X}_1^{\\top}\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1)^{-1} (\\mathbf{X}_1^{\\top}\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{y})\n$$\nThis is the desired expression.\n\n**Numerical Calculation**\n\nWe are given:\n$\\mathbf{X}_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$, $\\mathbf{X}_{2} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix}$, $\\mathbf{y} = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$.\n\nWe need to compute the quantities $\\tilde{\\mathbf{x}}_1 = \\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1$ and $\\tilde{\\mathbf{y}} = \\mathbf{M}_{\\mathbf{X}_2}\\mathbf{y}$. These are the residuals from regressing $\\mathbf{X}_1$ and $\\mathbf{y}$ on $\\mathbf{X}_2$, respectively.\nFirst, compute $\\mathbf{X}_2^{\\top}\\mathbf{X}_2$ and its inverse:\n$$\n\\mathbf{X}_2^{\\top}\\mathbf{X}_2 = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 3 & 3 & 5 & 5 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix} = \\begin{pmatrix} 5 & 17 \\\\ 17 & 69 \\end{pmatrix}\n$$\nThe determinant is $\\det(\\mathbf{X}_2^{\\top}\\mathbf{X}_2) = (5)(69) - (17)(17) = 345 - 289 = 56$.\nThe inverse is:\n$$\n(\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1} = \\frac{1}{56} \\begin{pmatrix} 69 & -17 \\\\ -17 & 5 \\end{pmatrix}\n$$\nThe vector $\\tilde{\\mathbf{x}}_1 = \\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1 = \\mathbf{X}_1 - \\mathbf{P}_{\\mathbf{X}_2}\\mathbf{X}_1 = \\mathbf{X}_1 - \\mathbf{X}_2(\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1}\\mathbf{X}_2^{\\top}\\mathbf{X}_1$.\nFirst, calculate $\\mathbf{X}_2^{\\top}\\mathbf{X}_1$:\n$$\n\\mathbf{X}_2^{\\top}\\mathbf{X}_1 = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 3 & 3 & 5 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 15 \\\\ 61 \\end{pmatrix}\n$$\nThe OLS coefficients for regressing $\\mathbf{X}_1$ on $\\mathbf{X}_2$ are $\\hat{\\boldsymbol{\\gamma}}_{x_1} = (\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1}\\mathbf{X}_2^{\\top}\\mathbf{X}_1$:\n$$\n\\hat{\\boldsymbol{\\gamma}}_{x_1} = \\frac{1}{56} \\begin{pmatrix} 69 & -17 \\\\ -17 & 5 \\end{pmatrix} \\begin{pmatrix} 15 \\\\ 61 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 1035 - 1037 \\\\ -255 + 305 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} -2 \\\\ 50 \\end{pmatrix} = \\begin{pmatrix} -1/28 \\\\ 25/28 \\end{pmatrix}\n$$\nThe fitted values are $\\hat{\\mathbf{X}}_1 = \\mathbf{X}_2\\hat{\\boldsymbol{\\gamma}}_{x_1}$, and the residuals are $\\tilde{\\mathbf{x}}_1 = \\mathbf{X}_1 - \\hat{\\mathbf{X}}_1$:\n$$\n\\tilde{\\mathbf{x}}_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} - \\frac{1}{28}\\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 25 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} - \\frac{1}{28}\\begin{pmatrix} 24 \\\\ 74 \\\\ 74 \\\\ 124 \\\\ 124 \\end{pmatrix} = \\frac{1}{28}\\begin{pmatrix} 28-24 \\\\ 56-74 \\\\ 84-74 \\\\ 112-124 \\\\ 140-124 \\end{pmatrix} = \\frac{1}{28}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}\n$$\nNext, we compute the residuals for $\\mathbf{y}$, $\\tilde{\\mathbf{y}} = \\mathbf{M}_{\\mathbf{X}_2}\\mathbf{y} = \\mathbf{y} - \\mathbf{X}_2(\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1}\\mathbf{X}_2^{\\top}\\mathbf{y}$. First, $\\mathbf{X}_2^{\\top}\\mathbf{y}$:\n$$\n\\mathbf{X}_2^{\\top}\\mathbf{y} = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 3 & 3 & 5 & 5 \\end{pmatrix} \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} = \\begin{pmatrix} 48 \\\\ 172 \\end{pmatrix}\n$$\nThe OLS coefficients for regressing $\\mathbf{y}$ on $\\mathbf{X}_2$ are $\\hat{\\boldsymbol{\\gamma}}_{y} = (\\mathbf{X}_2^{\\top}\\mathbf{X}_2)^{-1}\\mathbf{X}_2^{\\top}\\mathbf{y}$:\n$$\n\\hat{\\boldsymbol{\\gamma}}_{y} = \\frac{1}{56} \\begin{pmatrix} 69 & -17 \\\\ -17 & 5 \\end{pmatrix} \\begin{pmatrix} 48 \\\\ 172 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 3312 - 2924 \\\\ -816 + 860 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 388 \\\\ 44 \\end{pmatrix} = \\begin{pmatrix} 97/14 \\\\ 11/14 \\end{pmatrix}\n$$\nThe residuals are $\\tilde{\\mathbf{y}} = \\mathbf{y} - \\mathbf{X}_2\\hat{\\boldsymbol{\\gamma}}_y$:\n$$\n\\tilde{\\mathbf{y}} = \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} - \\frac{1}{14}\\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix} \\begin{pmatrix} 97 \\\\ 11 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} - \\frac{1}{14}\\begin{pmatrix} 108 \\\\ 130 \\\\ 130 \\\\ 152 \\\\ 152 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 112-108 \\\\ 112-130 \\\\ 140-130 \\\\ 140-152 \\\\ 168-152 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}\n$$\nNow we compute $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 = \\frac{\\tilde{\\mathbf{x}}_1^{\\top}\\tilde{\\mathbf{y}}}{\\tilde{\\mathbf{x}}_1^{\\top}\\tilde{\\mathbf{x}}_1}\n$$\nWe observe that $\\tilde{\\mathbf{y}} = \\frac{1}{14}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}$ and $\\tilde{\\mathbf{x}}_1 = \\frac{1}{28}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}$.\nThus, we have a direct relationship: $\\tilde{\\mathbf{y}} = 2 \\tilde{\\mathbf{x}}_1$.\nSubstituting this into the formula for $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 = \\frac{\\tilde{\\mathbf{x}}_1^{\\top}(2\\tilde{\\mathbf{x}}_1)}{\\tilde{\\mathbf{x}}_1^{\\top}\\tilde{\\mathbf{x}}_1} = \\frac{2(\\tilde{\\mathbf{x}}_1^{\\top}\\tilde{\\mathbf{x}}_1)}{\\tilde{\\mathbf{x}}_1^{\\top}\\tilde{\\mathbf{x}}_1} = 2\n$$\nThis is valid as long as $\\tilde{\\mathbf{x}}_1$ is not a zero vector, which is true in this case. The numerical value of the coefficient is $2$.\n\n**Interpretation of the Coefficient**\n\nThe coefficient $\\hat{\\beta}_1$ represents the estimated conditional effect of the predictor $\\mathbf{X}_1$ on the response $\\mathbf{y}$. As shown in the derivation, $\\hat{\\beta}_1$ is the coefficient from a simple regression of the residuals of $\\mathbf{y}$ (after regressing on $\\mathbf{X}_2$) on the residuals of $\\mathbf{X}_1$ (after regressing on $\\mathbf{X}_2$). This means $\\hat{\\beta}_1$ quantifies the association between the part of $\\mathbf{X}_1$ that is linearly independent of the predictors in $\\mathbf{X}_2$ and the part of $\\mathbf{y}$ that is linearly independent of the predictors in $\\mathbf{X}_2$. In practical terms, it is the estimated change in the expected value of $y$ for a one-unit increase in the value of the predictor $\\mathbf{X}_1$, while holding the values of all other predictors in $\\mathbf{X}_2$ constant.\n\nWhen $\\mathbf{X}_1$ \"overlaps substantially\" with the columns of $\\mathbf{X}_2$, this implies a high degree of multicollinearity. In this situation, most of the variation in $\\mathbf{X}_1$ is already explained by the predictors in $\\mathbf{X}_2$. Consequently, the residual vector $\\tilde{\\mathbf{x}}_1 = \\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1$ will have components with small magnitudes, and its squared norm, $\\|\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1\\|_2^2 = \\mathbf{X}_1^{\\top}\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1$, which is the denominator in the final expression for $\\hat{\\beta}_1$, will be close to zero.\n\nThe variance of the estimator $\\hat{\\beta}_1$ is given by $\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 (\\mathbf{X}_1^{\\top}\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1)^{-1}$, where $\\sigma^2$ is the error variance. If the denominator $\\mathbf{X}_1^{\\top}\\mathbf{M}_{\\mathbf{X}_2}\\mathbf{X}_1$ is small, the variance of $\\hat{\\beta}_1$ becomes very large. This means the estimate is unstable and unreliable. A large variance implies a large standard error and a wide confidence interval, indicating great uncertainty about the true value of $\\beta_1$. While the coefficient still formally represents the conditional effect of $\\mathbf{X}_1$, the substantial overlap with $\\mathbf{X}_2$ makes it difficult for the model to disentangle the unique contribution of $\\mathbf{X}_1$ to the variation in $\\mathbf{y}$. The data simply do not contain enough information to precisely estimate this effect.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "A regression model can be highly sensitive to a few specific data points. This hands-on exercise moves from model interpretation to model diagnostics, focusing on identifying these influential observations. You will use the matrix formulation of regression, particularly the hat matrix $\\mathbf{H}$, to compute Cook's distance—a standard measure that combines an observation's leverage with its residual size to quantify its overall influence on the fitted model . Mastering this technique is essential for building robust and reliable statistical models.",
            "id": "3146024",
            "problem": "You will write a complete program that uses the matrix formulation of multiple linear regression to compute an influence measure for individual observations based on the projection (hat) matrix. Work entirely in linear algebraic terms, starting from foundational definitions of Ordinary Least Squares (OLS) and orthogonal projections in Euclidean space. Do not assume any formula for influence a priori; derive what you need from the definitions. Specifically, assume the multiple linear regression model $\\mathbf{y}=\\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}$ with $\\mathbb{E}[\\boldsymbol{\\varepsilon}]=\\mathbf{0}$ and $\\operatorname{Var}(\\boldsymbol{\\varepsilon})=\\sigma^{2}\\mathbf{I}$, where $\\mathbf{X}\\in\\mathbb{R}^{n\\times p}$ has full column rank, and an intercept column is included. The OLS estimator is defined as the minimizer of the residual sum of squares $\\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\rVert_{2}^{2}$, the fitted vector is $\\hat{\\mathbf{y}}=\\mathbf{X}\\hat{\\boldsymbol{\\beta}}$, and the residual vector is $\\mathbf{e}=\\mathbf{y}-\\hat{\\mathbf{y}}$. The hat matrix is the orthogonal projector that maps $\\mathbf{y}$ to $\\hat{\\mathbf{y}}$ in the column space of $\\mathbf{X}$. The unbiased estimator of the error variance is $\\hat{\\sigma}^{2}=\\lVert\\mathbf{e}\\rVert_{2}^{2}/(n-p)$. Cook’s distance (CD) is the standard influence measure that quantifies the overall change in the fitted values induced by removing a single observation; use the standard matrix expression obtained from these definitions to compute it for each observation.\n\nYour program must, for each dataset below, perform the following steps in the stated order:\n- Construct the design matrix $\\mathbf{X}$ by prepending an intercept column $\\mathbf{1}_{n}$ to the given predictor columns.\n- Compute $\\hat{\\boldsymbol{\\beta}}$ by solving the normal equations, construct the hat matrix $\\mathbf{H}$, and compute the residuals $\\mathbf{e}$.\n- Compute $\\hat{\\sigma}^{2}$ using $\\hat{\\sigma}^{2}=\\lVert\\mathbf{e}\\rVert_{2}^{2}/(n-p)$, where $p$ is the number of columns of $\\mathbf{X}$ including the intercept.\n- Derive and compute Cook’s distance for each observation using only the matrix quantities defined above.\n- Use the common decision rule with threshold $t=4/n$ to flag influential observations, i.e., those with Cook’s distance strictly greater than $t$.\n- For each dataset, output the pair consisting of: the maximum Cook’s distance rounded to $6$ decimal places, and the sorted list of zero-based indices of all observations flagged as influential.\n\nDatasets (each dataset lists its predictor values without the intercept, followed by its response values):\n- Dataset A: $n=6$, $p=3$ after adding the intercept.\n  - Predictors $\\in\\mathbb{R}^{6\\times 2}$ (rows are observations): $(0,1)$, $(1,0)$, $(2,1)$, $(3,0)$, $(4,1)$, $(10,5)$.\n  - Responses $\\in\\mathbb{R}^{6}$: $(0,3,4,7,8,60)$.\n- Dataset B: $n=6$, $p=3$ after adding the intercept. Predictors identical to Dataset A.\n  - Responses $\\in\\mathbb{R}^{6}$: $(0.2,2.8,4.1,7.2,7.9,16.05)$.\n- Dataset C: $n=5$, $p=2$ after adding the intercept.\n  - Single predictor $\\in\\mathbb{R}^{5\\times 1}$: $(0)$, $(1)$, $(2)$, $(3)$, $(10)$.\n  - Responses $\\in\\mathbb{R}^{5}$: $(2,2.5,3,3.5,20)$.\n\nAngle units are not applicable. Physical units are not applicable. All numeric answers must be real numbers (floats) or integers. The decision rule uses a threshold $t=4/n$ expressed as a real number, and the list of influential indices must be zero-based integers. The final output must aggregate the results for all datasets into a single line in the following structure: a list of length $3$ whose $i$-th element (for dataset $i$ in the order A, B, C) is the list $[d_{\\max}^{(i)},L^{(i)}]$, where $d_{\\max}^{(i)}$ is the maximum Cook’s distance rounded to $6$ decimal places for dataset $i$, and $L^{(i)}$ is the sorted list of zero-based indices flagged as influential for dataset $i$. For example, the structural form is $[[d_{\\max}^{(A)},L^{(A)}],[d_{\\max}^{(B)},L^{(B)}],[d_{\\max}^{(C)},L^{(C)}]]$.\n\nYour program should produce a single line of output containing exactly this aggregate list, with no extra text.",
            "solution": "The user has provided a well-posed and scientifically grounded problem in the domain of statistical learning. The task is to compute an influence measure, Cook's distance, for several datasets based on the matrix formulation of multiple linear regression. The problem is valid as it is self-contained, consistent, and rooted in established statistical theory. I will proceed with a solution.\n\nThe solution involves deriving the computational formula for Cook's distance from first principles as defined in the problem, and then applying this formula to the provided datasets.\n\n### Principles of Ordinary Least Squares (OLS) and the Hat Matrix\n\nWe are given the multiple linear regression model:\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the vector of responses, $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix of full column rank, $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ is the vector of coefficients, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ is the vector of errors with $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$ and $\\operatorname{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2\\mathbf{I}$. The design matrix $\\mathbf{X}$ is constructed by prepending a column of ones (for the intercept) to the predictor columns.\n\nThe Ordinary Least Squares (OLS) estimator $\\hat{\\boldsymbol{\\beta}}$ is found by minimizing the Residual Sum of Squares (RSS), $S(\\boldsymbol{\\beta})$:\n$$\nS(\\boldsymbol{\\beta}) = \\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\rVert_{2}^{2} = (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})^\\top(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})\n$$\nTo find the minimum, we compute the gradient of $S(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$ and set it to zero:\n$$\n\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}\n$$\nThis leads to the normal equations:\n$$\n(\\mathbf{X}^\\top\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^\\top\\mathbf{y}\n$$\nSince $\\mathbf{X}$ has full column rank, the matrix $\\mathbf{X}^\\top\\mathbf{X}$ is invertible. The unique OLS estimator is therefore:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}\n$$\nThe vector of fitted values, $\\hat{\\mathbf{y}}$, is obtained by projecting the response vector $\\mathbf{y}$ onto the column space of $\\mathbf{X}$.\n$$\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}\n$$\nWe define the **hat matrix**, $\\mathbf{H}$, as the projection matrix:\n$$\n\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\n$$\nThus, $\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}$. The matrix $\\mathbf{H}$ is symmetric ($\\mathbf{H}^\\top = \\mathbf{H}$) and idempotent ($\\mathbf{H}\\mathbf{H} = \\mathbf{H}$). The diagonal elements of the hat matrix, $h_{ii} = \\mathbf{x}_i^\\top(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{x}_i$, where $\\mathbf{x}_i^\\top$ is the $i$-th row of $\\mathbf{X}$, are called the leverages of the observations. Leverage $h_{ii}$ measures the potential influence of observation $i$ on its own fitted value $\\hat{y}_i$, as $\\hat{y}_i = \\sum_{j=1}^{n} H_{ij} y_j$, and so $\\frac{\\partial \\hat{y}_i}{\\partial y_i} = h_{ii}$.\n\nThe vector of residuals is the difference between the observed and fitted values:\n$$\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - \\mathbf{H}\\mathbf{y} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}\n$$\nThe unbiased estimator of the error variance $\\sigma^2$ is given by:\n$$\n\\hat{\\sigma}^2 = \\frac{\\mathbf{e}^\\top\\mathbf{e}}{n-p} = \\frac{\\lVert\\mathbf{e}\\rVert_2^2}{n-p}\n$$\n\n### Derivation of Cook's Distance\n\nCook's distance, $D_i$, quantifies the effect of deleting the $i$-th observation on the vector of fitted values. It is defined as a scaled distance between the vector of fitted values calculated with all $n$ observations, $\\hat{\\mathbf{y}}$, and the vector of fitted values calculated without observation $i$, which we denote $\\hat{\\mathbf{y}}_{(i)} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{(i)}$.\n$$\nD_i = \\frac{(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)})^\\top(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)})}{p\\hat{\\sigma}^2}\n$$\nThe term $\\hat{\\boldsymbol{\\beta}}_{(i)}$ is the coefficient vector estimated using the data with the $i$-th row removed. The difference between the coefficient vectors can be shown (using matrix inversion lemmas) to be:\n$$\n\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)} = \\frac{(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{x}_i e_i}{1 - h_{ii}}\n$$\nwhere $e_i$ is the $i$-th residual from the full model and $h_{ii}$ is the $i$-th leverage.\n\nSubstituting this into the expression for the change in fitted values:\n$$\n\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)} = \\mathbf{X}(\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)}) = \\mathbf{X}\\frac{(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{x}_i e_i}{1 - h_{ii}}\n$$\nNow we can compute the numerator of Cook's distance:\n$$\n(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)})^\\top(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)}) = \\left( \\frac{e_i}{1 - h_{ii}} \\right)^2 \\left( \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{x}_i \\right)^\\top \\left( \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{x}_i \\right)\n$$\nThe term on the right becomes:\n$$\n\\mathbf{x}_i^\\top ((\\mathbf{X}^\\top\\mathbf{X})^{-1})^\\top \\mathbf{X}^\\top\\mathbf{X} (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\mathbf{x}_i = \\mathbf{x}_i^\\top (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\mathbf{x}_i = h_{ii}\n$$\nTherefore, the numerator simplifies to:\n$$\n(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)})^\\top(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)}) = \\frac{e_i^2 h_{ii}}{(1 - h_{ii})^2}\n$$\nFinally, substituting this into the definition of $D_i$, we arrive at the computational formula for Cook's distance:\n$$\nD_i = \\frac{1}{p\\hat{\\sigma}^2} \\frac{e_i^2 h_{ii}}{(1 - h_{ii})^2} = \\frac{e_i^2}{p\\hat{\\sigma}^2} \\frac{h_{ii}}{(1 - h_{ii})^2}\n$$\nThis formula shows that an observation's influence depends on both its residual $e_i$ (how poorly it is fit by the model) and its leverage $h_{ii}$ (how far it is from the center of the predictor space).\n\n### Algorithmic Procedure\n\nFor each dataset provided, the following steps are executed:\n$1$. Construct the design matrix $\\mathbf{X}$ by prepending a column of ones to the given predictor matrix. Let $n$ be the number of rows (observations) and $p$ be the number of columns (parameters).\n$2$. Compute the OLS coefficients $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}$.\n$3$. Compute the hat matrix $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top$ and extract its diagonal elements to get the leverages $h_{ii}$.\n$4$. Compute the residual vector $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$.\n$5$. Compute the unbiased estimate of error variance $\\hat{\\sigma}^2 = \\frac{\\mathbf{e}^\\top\\mathbf{e}}{n-p}$.\n$6$. For each observation $i$ from $0$ to $n-1$, compute Cook's distance $D_i$ using the derived formula.\n$7$. Determine the influence threshold $t = 4/n$.\n$8$. Identify the set of influential points as all observations for which $D_i > t$.\n$9$. Find the maximum Cook's distance among all observations.\n$10$. Report the maximum Cook's distance rounded to $6$ decimal places and the sorted list of zero-based indices of the influential points.\n\nThis procedure will be applied to datasets A, B, and C as specified in the problem statement.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multiple linear regression influence analysis problem for the given datasets.\n    \"\"\"\n\n    def compute_influence(X_pred: np.ndarray, y: np.ndarray) -> list:\n        \"\"\"\n        Computes influence measures for a single dataset.\n\n        Args:\n            X_pred: The predictor variables matrix (n_samples, n_predictors).\n            y: The response variable vector (n_samples,).\n\n        Returns:\n            A list containing the maximum Cook's distance (rounded) and a sorted list\n            of indices for influential points.\n        \"\"\"\n        # Ensure X_pred is 2D\n        if X_pred.ndim == 1:\n            X_pred = X_pred.reshape(-1, 1)\n\n        # Step 1: Construct the design matrix X by prepending an intercept column.\n        n = X_pred.shape[0]\n        X = np.hstack([np.ones((n, 1)), X_pred])\n        _n, p = X.shape\n\n        # Step 2: Compute beta_hat by solving the normal equations, construct H, and compute e.\n        # This combines several steps for efficiency.\n        try:\n            XtX_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            # This case should not happen based on problem validation\n            raise ValueError(\"Design matrix X is singular, cannot solve normal equations.\")\n\n        # Compute OLS coefficients\n        beta_hat = XtX_inv @ (X.T @ y)\n\n        # Compute hat matrix H and leverages h_ii\n        H = X @ XtX_inv @ X.T\n        h = np.diag(H)\n\n        # Compute residuals e\n        y_hat = X @ beta_hat\n        e = y - y_hat\n\n        # Step 3: Compute sigma_squared_hat\n        rss = e.T @ e\n        sigma_sq_hat = rss / (n - p)\n\n        # Step 4: Derive and compute Cook's distance for each observation.\n        # The formula is D_i = (e_i^2 / (p * sigma_sq_hat)) * (h_ii / (1 - h_ii)^2)\n        # Handle division by zero if sigma_sq_hat is zero (perfect fit)\n        if sigma_sq_hat == 0:\n            cooks_d = np.zeros(n)\n        else:\n            cooks_d = (e**2 / (p * sigma_sq_hat)) * (h / (1 - h)**2)\n\n        # Step 5: Use the common decision rule with threshold t = 4/n.\n        threshold = 4 / n\n        influential_indices = np.where(cooks_d > threshold)[0].tolist()\n        influential_indices.sort()\n\n        # Find the maximum Cook's distance\n        max_d = np.max(cooks_d)\n\n        return [round(max_d, 6), influential_indices]\n\n    # Define the datasets from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"X_pred\": np.array([\n                [0, 1], [1, 0], [2, 1], [3, 0], [4, 1], [10, 5]\n            ]),\n            \"y\": np.array([0, 3, 4, 7, 8, 60]),\n        },\n        {\n            \"name\": \"B\",\n            \"X_pred\": np.array([\n                [0, 1], [1, 0], [2, 1], [3, 0], [4, 1], [10, 5]\n            ]),\n            \"y\": np.array([0.2, 2.8, 4.1, 7.2, 7.9, 16.05]),\n        },\n        {\n            \"name\": \"C\",\n            \"X_pred\": np.array([0, 1, 2, 3, 10]),\n            \"y\": np.array([2, 2.5, 3, 3.5, 20]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_influence(case[\"X_pred\"], case[\"y\"])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, with no spaces between list elements\n    # e.g., [[val,[idx]],[val,[idx]]]\n    # Using str().replace(' ', '') achieves this compact representation.\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n\n```"
        },
        {
            "introduction": "Ordinary Least Squares (OLS) can produce unstable and unreliable estimates when predictors are highly correlated, a problem known as multicollinearity. This practice introduces ridge regression as a powerful method to stabilize the model in such scenarios. By analyzing the model through the lens of Singular Value Decomposition (SVD), you will see precisely how ridge regression applies a \"shrinkage\" factor to the regression coefficients, particularly along directions in the data that are poorly estimated by OLS . This provides a deep, geometric understanding of regularization and its role in combating ill-conditioned problems.",
            "id": "3146059",
            "problem": "Consider the matrix formulation of multiple linear regression with an emphasis on the role of singular value decomposition (SVD) in interpreting regularization. Work in purely mathematical terms with no physical units. Begin from the definitions of the least-squares estimator as the minimizer of a squared error and the ridge estimator as the minimizer of a squared error with a squared-parameter penalty. You must not assume any closed forms a priori; instead, derive them where needed from these definitions and linear algebra facts. The goal is to express predictions under ridge regression and interpret how the penalty shrinks contributions along small singular directions via SVD, then design a concrete data matrix and response vector where small singular directions dominate ordinary least squares and observe how ridge regression stabilizes them.\n\nTask:\n1) Construct the design matrix $\\mathbf{X} \\in \\mathbb{R}^{6 \\times 3}$ as follows, where each entry is a real scalar:\n$$\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 1 + 10^{-4}\\cdot 1 & 1 + 2\\cdot 10^{-4}\\cdot (-2) \\\\\n2 & 2 + 10^{-4}\\cdot (-2) & 2 + 2\\cdot 10^{-4}\\cdot 1 \\\\\n3 & 3 + 10^{-4}\\cdot 3 & 3 + 2\\cdot 10^{-4}\\cdot (-3) \\\\\n4 & 4 + 10^{-4}\\cdot (-4) & 4 + 2\\cdot 10^{-4}\\cdot 4 \\\\\n5 & 5 + 10^{-4}\\cdot 5 & 5 + 2\\cdot 10^{-4}\\cdot (-5) \\\\\n6 & 6 + 10^{-4}\\cdot (-6) & 6 + 2\\cdot 10^{-4}\\cdot 6\n\\end{bmatrix}.\n$$\nThis matrix is full column rank but highly ill-conditioned because its columns are nearly collinear.\n\n2) Compute the singular value decomposition $\\mathbf{X} = \\mathbf{U}\\boldsymbol{\\Sigma} \\mathbf{V}^\\top$ with $\\mathbf{U} \\in \\mathbb{R}^{6 \\times 3}$ having orthonormal columns, $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{3 \\times 3}$ diagonal with nonnegative entries in nonincreasing order, and $\\mathbf{V} \\in \\mathbb{R}^{3 \\times 3}$ orthonormal. Define the response vector $\\mathbf{y} \\in \\mathbb{R}^{6}$ to be the left singular vector corresponding to the smallest singular value, i.e., the last column of $\\mathbf{U}$.\n\n3) Define the ordinary least squares estimator $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} \\in \\mathbb{R}^{3}$ as any minimizer of the problem\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2.\n$$\nDefine the ridge estimator $\\hat{\\boldsymbol{\\beta}}_{\\lambda} \\in \\mathbb{R}^{3}$ for any penalty strength $\\lambda \\ge 0$ as any minimizer of\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\frac{\\lambda}{2}\\|\\boldsymbol{\\beta}\\|_2^2.\n$$\nCompute the fitted values $\\hat{\\mathbf{y}}_{\\text{OLS}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$ and $\\hat{\\mathbf{y}}_{\\lambda} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\lambda}$.\n\n4) Interpret ridge as filtering singular directions. Using the SVD of $\\mathbf{X}$, express the predictions $\\hat{\\mathbf{y}}_{\\lambda}$ in the basis of the left singular vectors and identify the shrinkage factor applied to each singular component. Then, with $\\mathbf{y}$ chosen as in step $2$, verify computationally that the singular direction associated with the smallest singular value is the one most strongly shrunk.\n\n5) Test suite. Use the single matrix $\\mathbf{X}$ and response $\\mathbf{y}$ defined above, and evaluate the following four cases that together serve as a test suite:\n- Case A (boundary, penalty zero): $\\lambda = 0$. Verify whether the ridge predictions coincide with ordinary least squares predictions (within a numerical tolerance of $10^{-8}$). Output a boolean computed as $ \\|\\hat{\\mathbf{y}}_{\\lambda} - \\hat{\\mathbf{y}}_{\\text{OLS}}\\|_2 \\le 10^{-8}$.\n- Case B (small penalty, gentle shrinkage): $\\lambda = 10^{-6}$. Output the ratio $ \\|\\hat{\\boldsymbol{\\beta}}_{\\lambda}\\|_2 / \\|\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}\\|_2 $ as a floating-point number.\n- Case C (moderate penalty, singular-direction filter check): $\\lambda = 10^{-4}$. Let $\\mathbf{v}_{\\min}$ be the right singular vector corresponding to the smallest singular value $\\sigma_{\\min}$. Compute the component shrinkage ratio\n$$\nr_{\\text{emp}} = \\frac{|\\langle \\mathbf{v}_{\\min}, \\hat{\\boldsymbol{\\beta}}_{\\lambda} \\rangle|}{|\\langle \\mathbf{v}_{\\min}, \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} \\rangle|},\n$$\nand the theoretical filter factor\n$$\nr_{\\text{theory}} = \\frac{\\sigma_{\\min}^2}{\\sigma_{\\min}^2 + \\lambda}.\n$$\nOutput the absolute deviation $|r_{\\text{emp}} - r_{\\text{theory}}|$ as a floating-point number.\n- Case D (large penalty, heavy shrinkage): $\\lambda = 10^{2}$. Output the Euclidean norm $\\|\\hat{\\boldsymbol{\\beta}}_{\\lambda}\\|_2$ as a floating-point number.\n\n6) Final output format. Your program should produce a single line of output containing the four results in the order of Cases A through D as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD]\"). The results must be, respectively, a boolean for Case A and three floating-point numbers for Cases B through D.",
            "solution": "The problem requires an analysis of ridge regression as a shrinkage estimator, interpreted through the singular value decomposition (SVD) of the design matrix $\\mathbf{X}$. We must first derive the estimators from their optimization definitions, then apply them to a specifically constructed ill-conditioned matrix $\\mathbf{X}$ and a response vector $\\mathbf{y}$ chosen to highlight the regularization effect.\n\nFirst, we derive the ordinary least squares (OLS) and ridge regression estimators from first principles.\n\nThe OLS estimator $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$ is defined as the minimizer of the residual sum of squares:\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2}\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 = \\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\nTo find the minimum, we compute the gradient of $L(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$ and set it to zero.\n$$\n\\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} \\left( \\frac{1}{2}(\\mathbf{y}^\\top \\mathbf{y} - 2\\mathbf{y}^\\top \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X}\\boldsymbol{\\beta}) \\right) = -\\mathbf{X}^\\top \\mathbf{y} + \\mathbf{X}^\\top \\mathbf{X}\\boldsymbol{\\beta}\n$$\nSetting the gradient to zero gives the normal equations:\n$$\n\\mathbf{X}^\\top \\mathbf{X} \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\mathbf{X}^\\top \\mathbf{y}\n$$\nSince the problem specifies that $\\mathbf{X}$ is a $6 \\times 3$ matrix of full column rank, the matrix $\\mathbf{X}^\\top \\mathbf{X} \\in \\mathbb{R}^{3 \\times 3}$ is invertible. Thus, the unique OLS solution is:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n$$\n\nThe ridge estimator $\\hat{\\boldsymbol{\\beta}}_{\\lambda}$ is defined as the minimizer of the penalized residual sum of squares:\n$$\nL_\\lambda(\\boldsymbol{\\beta}) = \\frac{1}{2}\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\frac{\\lambda}{2}\\|\\boldsymbol{\\beta}\\|_2^2\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter. The gradient is:\n$$\n\\nabla_{\\boldsymbol{\\beta}} L_\\lambda(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} \\left( \\frac{1}{2}\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 \\right) + \\nabla_{\\boldsymbol{\\beta}} \\left( \\frac{\\lambda}{2}\\boldsymbol{\\beta}^\\top\\boldsymbol{\\beta} \\right) = (-\\mathbf{X}^\\top \\mathbf{y} + \\mathbf{X}^\\top \\mathbf{X}\\boldsymbol{\\beta}) + \\lambda\\boldsymbol{\\beta}\n$$\nSetting the gradient to zero yields:\n$$\n(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})\\hat{\\boldsymbol{\\beta}}_{\\lambda} = \\mathbf{X}^\\top \\mathbf{y}\n$$\nFor any $\\lambda > 0$, the matrix $(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})$ is positive definite and thus invertible. For $\\lambda = 0$, it reduces to the OLS case. The unique ridge solution is:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\lambda} = (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n$$\n\nNext, we interpret these estimators using the singular value decomposition (SVD) of $\\mathbf{X}$. Let $\\mathbf{X} = \\mathbf{U}\\boldsymbol{\\Sigma} \\mathbf{V}^\\top$, where $\\mathbf{U} \\in \\mathbb{R}^{6 \\times 3}$ has orthonormal columns, $\\boldsymbol{\\Sigma} = \\text{diag}(\\sigma_1, \\sigma_2, \\sigma_3)$ is a $3 \\times 3$ diagonal matrix with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 > 0$, and $\\mathbf{V} \\in \\mathbb{R}^{3 \\times 3}$ is an orthonormal matrix.\nWe substitute the SVD into the expressions for the estimators. First, note that $\\mathbf{X}^\\top \\mathbf{X} = (\\mathbf{U}\\boldsymbol{\\Sigma} \\mathbf{V}^\\top)^\\top(\\mathbf{U}\\boldsymbol{\\Sigma} \\mathbf{V}^\\top) = \\mathbf{V}\\boldsymbol{\\Sigma}^\\top \\mathbf{U}^\\top \\mathbf{U}\\boldsymbol{\\Sigma} \\mathbf{V}^\\top = \\mathbf{V}\\boldsymbol{\\Sigma}^2 \\mathbf{V}^\\top$.\n\nFor the OLS estimator:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{V}\\boldsymbol{\\Sigma}^2 \\mathbf{V}^\\top)^{-1} (\\mathbf{U}\\boldsymbol{\\Sigma} \\mathbf{V}^\\top)^\\top \\mathbf{y} = (\\mathbf{V}(\\boldsymbol{\\Sigma}^2)^{-1}\\mathbf{V}^\\top)(\\mathbf{V}\\boldsymbol{\\Sigma} \\mathbf{U}^\\top \\mathbf{y}) = \\mathbf{V}\\boldsymbol{\\Sigma}^{-1}\\mathbf{U}^\\top \\mathbf{y}\n$$\nIf we denote the columns of $\\mathbf{U}$ and $\\mathbf{V}$ as $\\mathbf{u}_j$ and $\\mathbf{v}_j$ respectively, this can be written as a sum:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\sum_{j=1}^{3} \\mathbf{v}_j \\frac{\\mathbf{u}_j^\\top \\mathbf{y}}{\\sigma_j}\n$$\nThe OLS solution is a linear combination of the right singular vectors $\\mathbf{v}_j$, where the coefficients are the projections of $\\mathbf{y}$ onto the left singular vectors $\\mathbf{u}_j$, amplified by the inverse of the corresponding singular values $1/\\sigma_j$.\n\nFor the ridge estimator:\n$$\n\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I} = \\mathbf{V}\\boldsymbol{\\Sigma}^2 \\mathbf{V}^\\top + \\lambda \\mathbf{V} \\mathbf{V}^\\top = \\mathbf{V}(\\boldsymbol{\\Sigma}^2 + \\lambda \\mathbf{I})\\mathbf{V}^\\top\n$$\nThe inverse is $(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1} = \\mathbf{V}(\\boldsymbol{\\Sigma}^2 + \\lambda \\mathbf{I})^{-1}\\mathbf{V}^\\top$. Thus,\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\lambda} = (\\mathbf{V}(\\boldsymbol{\\Sigma}^2 + \\lambda \\mathbf{I})^{-1}\\mathbf{V}^\\top)(\\mathbf{V}\\boldsymbol{\\Sigma} \\mathbf{U}^\\top \\mathbf{y}) = \\mathbf{V}(\\boldsymbol{\\Sigma}^2 + \\lambda \\mathbf{I})^{-1}\\boldsymbol{\\Sigma} \\mathbf{U}^\\top \\mathbf{y}\n$$\nIn sum notation, this is:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\lambda} = \\sum_{j=1}^{3} \\mathbf{v}_j \\frac{\\sigma_j}{\\sigma_j^2 + \\lambda}(\\mathbf{u}_j^\\top \\mathbf{y})\n$$\nComparing the coefficients of $\\mathbf{v}_j$ in the expansions for $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$ and $\\hat{\\boldsymbol{\\beta}}_{\\lambda}$, we see a multiplicative factor:\n$$\n\\frac{\\sigma_j/(\\sigma_j^2+\\lambda)}{1/\\sigma_j} = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}\n$$\nThis factor, which is always in the interval $[0, 1]$, is the shrinkage factor. For large singular values $\\sigma_j \\gg \\sqrt{\\lambda}$, the factor is close to $1$, and there is little shrinkage. For small singular values $\\sigma_j \\ll \\sqrt{\\lambda}$, the factor is close to $0$, and the corresponding component of the solution is strongly shrunk towards zero.\n\nThe predicted values are $\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\beta}$. For OLS, $\\hat{\\mathbf{y}}_{\\text{OLS}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{U}\\boldsymbol{\\Sigma} \\mathbf{V}^\\top)(\\mathbf{V}\\boldsymbol{\\Sigma}^{-1}\\mathbf{U}^\\top \\mathbf{y}) = \\mathbf{U}\\mathbf{U}^\\top \\mathbf{y}$, which is the orthogonal projection of $\\mathbf{y}$ onto the column space of $\\mathbf{X}$. For ridge,\n$$\n\\hat{\\mathbf{y}}_{\\lambda} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\lambda} = (\\mathbf{U}\\boldsymbol{\\Sigma} \\mathbf{V}^\\top)(\\mathbf{V}(\\boldsymbol{\\Sigma}^2 + \\lambda \\mathbf{I})^{-1}\\boldsymbol{\\Sigma} \\mathbf{U}^\\top \\mathbf{y}) = \\mathbf{U} \\boldsymbol{\\Sigma} (\\boldsymbol{\\Sigma}^2 + \\lambda \\mathbf{I})^{-1} \\boldsymbol{\\Sigma} \\mathbf{U}^\\top \\mathbf{y} = \\sum_{j=1}^{3} \\mathbf{u}_j \\left( \\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda} \\right) (\\mathbf{u}_j^\\top \\mathbf{y})\n$$\nThis shows that the ridge predictions are formed by shrinking the components of $\\mathbf{y}$ along the left singular vectors $\\mathbf{u}_j$ by the same factors.\n\nThe problem defines $\\mathbf{y}$ to be the left singular vector corresponding to the smallest singular value, i.e., $\\mathbf{y} = \\mathbf{u}_3$ (assuming $\\sigma_3 = \\sigma_{\\min}$). By orthonormality of the columns of $\\mathbf{U}$, we have $\\mathbf{u}_j^\\top \\mathbf{y} = \\mathbf{u}_j^\\top \\mathbf{u}_3 = \\delta_{j3}$.\nThis simplifies the estimators dramatically:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\mathbf{v}_3 \\frac{1}{\\sigma_3}\n$$\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\lambda} = \\mathbf{v}_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\n$$\nThe OLS solution's magnitude blows up as $\\sigma_3 \\to 0$, whereas the ridge solution remains bounded.\n\nWe now proceed with the specific calculations for the test suite.\nLet $\\mathbf{X}$ be the provided $6 \\times 3$ matrix. We compute its SVD $\\mathbf{X}=\\mathbf{U}\\boldsymbol{\\Sigma} \\mathbf{V}^\\top$ and define $\\mathbf{y} = \\mathbf{u}_3$, where $\\mathbf{u}_3$ corresponds to $\\sigma_3 = \\sigma_{\\min}$. Let $\\mathbf{v}_3 = \\mathbf{v}_{\\min}$.\n\nCase A ($\\lambda = 0$): We compare $\\hat{\\mathbf{y}}_{0}$ with $\\hat{\\mathbf{y}}_{\\text{OLS}}$. From the ridge prediction formula, for $\\lambda=0$:\n$$\n\\hat{\\mathbf{y}}_{0} = \\sum_{j=1}^{3} \\mathbf{u}_j \\left( \\frac{\\sigma_j^2}{\\sigma_j^2+0} \\right) (\\mathbf{u}_j^\\top \\mathbf{y}) = \\sum_{j=1}^{3} \\mathbf{u}_j (\\mathbf{u}_j^\\top \\mathbf{y}) = \\mathbf{U}\\mathbf{U}^\\top \\mathbf{y} = \\hat{\\mathbf{y}}_{\\text{OLS}}\n$$\nThe predictions must be identical. Numerically, $\\|\\hat{\\mathbf{y}}_{0} - \\hat{\\mathbf{y}}_{\\text{OLS}}\\|_2$ should be close to $0$.\n\nCase B ($\\lambda = 10^{-6}$): We compute the ratio of norms.\n$$\n\\frac{\\|\\hat{\\boldsymbol{\\beta}}_{\\lambda}\\|_2}{\\|\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}\\|_2} = \\frac{\\|\\mathbf{v}_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\\|_2}{\\|\\mathbf{v}_3 \\frac{1}{\\sigma_3}\\|_2} = \\frac{|\\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}|}{|\\frac{1}{\\sigma_3}|} \\|\\mathbf{v}_3\\|_2 = \\frac{\\sigma_3^2}{\\sigma_3^2 + \\lambda}\n$$\nThis is the theoretical shrinkage factor for $\\sigma_{\\min}$.\n\nCase C ($\\lambda = 10^{-4}$): We test the component shrinkage ratio.\nThe empirical ratio is $r_{\\text{emp}} = \\frac{|\\langle \\mathbf{v}_{\\min}, \\hat{\\boldsymbol{\\beta}}_{\\lambda} \\rangle|}{|\\langle \\mathbf{v}_{\\min}, \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} \\rangle|}$.\nUsing $\\mathbf{v}_{\\min} = \\mathbf{v}_3$ and the derived expressions for the estimators:\n$$\n\\langle \\mathbf{v}_3, \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} \\rangle = \\langle \\mathbf{v}_3, \\mathbf{v}_3 \\frac{1}{\\sigma_3} \\rangle = \\frac{1}{\\sigma_3} \\|\\mathbf{v}_3\\|_2^2 = \\frac{1}{\\sigma_3}\n$$\n$$\n\\langle \\mathbf{v}_3, \\hat{\\boldsymbol{\\beta}}_{\\lambda} \\rangle = \\langle \\mathbf{v}_3, \\mathbf{v}_3 \\frac{\\sigma_3}{\\sigma_3^2+\\lambda} \\rangle = \\frac{\\sigma_3}{\\sigma_3^2+\\lambda}\n$$\nThus, $r_{\\text{emp}} = \\frac{|\\sigma_3/(\\sigma_3^2+\\lambda)|}{|1/\\sigma_3|} = \\frac{\\sigma_3^2}{\\sigma_3^2+\\lambda}$, which is identical to the theoretical filter factor $r_{\\text{theory}}$. Their absolute difference $|r_{\\text{emp}} - r_{\\text{theory}}|$ should be numerically zero.\n\nCase D ($\\lambda = 10^2$): A large penalty should yield a solution vector with a small norm.\n$$\n\\|\\hat{\\boldsymbol{\\beta}}_{\\lambda}\\|_2 = \\left\\| \\mathbf{v}_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda} \\right\\|_2 = \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda} \\|\\mathbf{v}_3\\|_2 = \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\n$$\nWith $\\lambda = 100$, this norm will be very small.\n\nThe numerical implementation will follow these derived formulas.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ridge regression problem as specified.\n    1. Constructs the design matrix X.\n    2. Performs SVD on X and defines the response vector y.\n    3. Computes OLS and ridge solutions for different lambda values.\n    4. Calculates the required metrics for the four test cases.\n    \"\"\"\n\n    # 1. Construct the design matrix X\n    i_vals = np.arange(1, 7, dtype=float).reshape(-1, 1)\n    \n    # Perturbations for columns 2 and 3\n    p2_coeffs = np.array([1, -2, 3, -4, 5, -6]).reshape(-1, 1)\n    p3_coeffs = np.array([-2, 1, -3, 4, -5, 6]).reshape(-1, 1)\n    \n    col1 = i_vals\n    col2 = i_vals + 1e-4 * p2_coeffs\n    col3 = i_vals + 2e-4 * p3_coeffs\n    \n    X = np.hstack([col1, col2, col3])\n    n, p = X.shape\n\n    # 2. Compute SVD of X and define y\n    # U has shape (n, p), s has shape (p,), Vt has shape (p, p)\n    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n    \n    # y is the left singular vector for the smallest singular value\n    # which is the last column of U\n    y = U[:, -1]\n\n    # Get smallest singular value and corresponding right singular vector\n    sigma_min = s[-1]\n    # Vt is V.T, so the last row of Vt is the last column of V (as a row)\n    # v_min should be a column vector\n    v_min = Vt[-1, :].reshape(p, 1)\n    \n    # Helper to compute ridge estimator\n    def get_beta_ridge(lam, X_mat, y_vec):\n        p_dim = X_mat.shape[1]\n        XTX = X_mat.T @ X_mat\n        XTy = X_mat.T @ y_vec\n        I = np.identity(p_dim)\n        # Using np.linalg.solve for better numerical stability than inv()\n        beta = np.linalg.solve(XTX + lam * I, XTy)\n        return beta\n\n    results = []\n\n    # Case A: lambda = 0. Verify ridge prediction equals OLS prediction.\n    lam_A = 0.0\n    beta_ols = np.linalg.lstsq(X, y, rcond=None)[0]\n    y_hat_ols = X @ beta_ols\n    beta_ridge_A = get_beta_ridge(lam_A, X, y)\n    y_hat_ridge_A = X @ beta_ridge_A\n    result_A = np.linalg.norm(y_hat_ridge_A - y_hat_ols) <= 1e-8\n    results.append(result_A)\n\n    # Case B: lambda = 1e-6. Compute norm ratio.\n    lam_B = 1e-6\n    # beta_ols is already computed\n    beta_ridge_B = get_beta_ridge(lam_B, X, y)\n    result_B = np.linalg.norm(beta_ridge_B) / np.linalg.norm(beta_ols)\n    results.append(result_B)\n\n    # Case C: lambda = 1e-4. Compare empirical and theoretical shrinkage.\n    lam_C = 1e-4\n    # beta_ols is already computed\n    beta_ridge_C = get_beta_ridge(lam_C, X, y)\n    \n    # Empirical ratio\n    # Inner product of a (p,) vector and a (p,1) vector results in a (1,) array\n    # We extract the scalar value with [0]\n    r_emp_num = np.abs((v_min.T @ beta_ridge_C))[0]\n    r_emp_den = np.abs((v_min.T @ beta_ols))[0]\n    r_emp = r_emp_num / r_emp_den\n\n    # Theoretical ratio\n    r_theory = (sigma_min**2) / (sigma_min**2 + lam_C)\n    \n    result_C = np.abs(r_emp - r_theory)\n    results.append(result_C)\n\n    # Case D: lambda = 1e2. Compute norm of the heavily shrunk estimator.\n    lam_D = 1e2\n    beta_ridge_D = get_beta_ridge(lam_D, X, y)\n    result_D = np.linalg.norm(beta_ridge_D)\n    results.append(result_D)\n    \n    # Final output formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}