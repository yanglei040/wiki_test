{
    "hands_on_practices": [
        {
            "introduction": "评估分类器的性能看似简单，但仅仅依赖准确率等单一指标可能会产生严重的误导，尤其是在类别不均衡的情况下。本练习将挑战你进行更深入的分析，通过计算和比较不同的评估指标。通过计算宏平均（macro-averaged）和微平均（micro-averaged）的 $F_1$ 分数，你将直接观察到不同的评估策略如何揭示模型截然不同的优缺点，特别是模型在处理稀有但重要的少数类别时的表现。",
            "id": "3147853",
            "problem": "一个单标签、多类别分类器在一个包含 $N=100$ 个观测值的数据集上进行训练，这些观测值分属于三个类别 $A$、$B$ 和 $C$。真实的类别计数为 $|A|=80$、 $|B|=15$ 和 $|C|=5$。该分类器为每个观测值生成一个预测的类别标签和一个关于各个类别的概率分布。\n\n其在测试集上的总体表现如下：\n- 对于真实类别 $A$：$70$ 个被预测为 $A$，$10$ 个被预测为 $B$。\n- 对于真实类别 $B$：$8$ 个被预测为 $B$，$7$ 个被预测为 $A$。\n- 对于真实类别 $C$：$1$ 个被预测为 $C$，$4$ 个被预测为 $A$。\n\n对于每组观测值，分配给真实类别的概率（即分配给正确标签的概率质量，无论预测标签是什么）如下：\n- 真实类别为 $A$，预测类别为 $A$：每个样本在 $A$ 上的概率为 $0.90$。\n- 真实类别为 $A$，预测类别为 $B$：每个样本在 $A$ 上的概率为 $0.05$。\n- 真实类别为 $B$，预测类别为 $B$：每个样本在 $B$ 上的概率为 $0.85$。\n- 真实类别为 $B$，预测类别为 $A$：每个样本在 $B$ 上的概率为 $0.10$。\n- 真实类别为 $C$，预测类别为 $C$：该样本在 $C$ 上的概率为 $0.80$。\n- 真实类别为 $C$，预测类别为 $A$：每个样本在 $C$ 上的概率为 $0.02$。\n\n仅使用每个类别的精确率、每个类别的召回率、$F_1$ 分数、宏平均、微平均和平均负对数似然（对数损失，使用自然对数）的基本定义，确定哪个选项正确地描述了宏 $F_1$、微 $F_1$ 和对数损失，以及在给定的类别不平衡情况下，它们揭示了关于拟合质量的什么信息。\n\nA. 宏 $F_1 \\approx 0.563$ 显著低于微 $F_1 \\approx 0.790$，且对数损失约为 $0.706$，这表明尽管微平均性能很高，但对少数类别的过度自信的错误抬高了对数损失。\n\nB. 宏 $F_1 \\approx 0.790$ 等于微 $F_1$，且对数损失约为 $0.300$，表明该分类器在所有类别上的表现都同样出色。\n\nC. 宏 $F_1 \\approx 0.563$，微 $F_1$ 也约为 $0.563$，且对数损失约为 $0.200$，因此两种 $F_1$ 变体和对数损失都说明了同样的问题。\n\nD. 宏 $F_1 \\approx 0.790$ 高于微 $F_1 \\approx 0.563$，且对数损失约为 $0.706$，这意味着宏平均降低了对不平衡错误的敏感性。",
            "solution": "问题陈述已经过评估并被认定为有效。它在科学上基于统计学习的原理，问题提法适定，数据充分且一致，并以客观的语言表述。因此，我们可以着手推导解决方案。\n\n首要步骤是构建混淆矩阵 $M$，其中条目 $M_{ij}$ 表示真实类别为 $i$ 但被预测为类别 $j$ 的观测值数量。问题提供了以下数据：\n总观测值 $N=100$。\n真实类别计数：$|A|=80$, $|B|=15$, $|C|=5$。\n\n根据分类器行为的描述：\n- 对于真实类别 $A$：$70$ 个预测为 $A$，$10$ 个预测为 $B$。\n- 对于真实类别 $B$：$8$ 个预测为 $B$，$7$ 个预测为 $A$。\n- 对于真实类别 $C$：$1$ 个预测为 $C$，$4$ 个预测为 $A$。\n\n这导出了以下混淆矩阵：\n$$\nM = \n\\begin{pmatrix}\n  \\text{预测 } A  \\text{预测 } B  \\text{预测 } C \\\\\n\\text{真实 } A  70  10  0 \\\\\n\\text{真实 } B  7  8  0 \\\\\n\\text{真实 } C  4  0  1 \n\\end{pmatrix}\n$$\n行和与真实类别计数正确匹配（$70+10=80$, $7+8=15$, $4+1=5$）。列和得出每个类别的总预测数：\n- 预测为 $A$：$70+7+4 = 81$\n- 预测为 $B$：$10+8+0 = 18$\n- 预测为 $C$：$0+0+1 = 1$\n总观测值为 $81+18+1=100$，确认了一致性。\n\n根据混淆矩阵，我们计算每个类别 $k \\in \\{A, B, C\\}$ 的真阳性（$TP_k$）、假阳性（$FP_k$）和假阴性（$FN_k$）。\n\n对于类别 $A$：\n- $TP_A = 70$\n- $FP_A = 7+4 = 11$\n- $FN_A = 10+0 = 10$\n\n对于类别 $B$：\n- $TP_B = 8$\n- $FP_B = 10+0 = 10$\n- $FN_B = 7+0 = 7$\n\n对于类别 $C$：\n- $TP_C = 1$\n- $FP_C = 0+0 = 0$\n- $FN_C = 4+0 = 4$\n\n接下来，我们计算每个类别的精确率（$P_k$）和召回率（$R_k$）：\n$P_k = \\frac{TP_k}{TP_k + FP_k}$ 和 $R_k = \\frac{TP_k}{TP_k + FN_k}$。\n\n- 类别 $A$：\n  $P_A = \\frac{70}{70+11} = \\frac{70}{81}$\n  $R_A = \\frac{70}{70+10} = \\frac{70}{80} = \\frac{7}{8} = 0.875$\n- 类别 $B$：\n  $P_B = \\frac{8}{8+10} = \\frac{8}{18} = \\frac{4}{9}$\n  $R_B = \\frac{8}{8+7} = \\frac{8}{15}$\n- 类别 $C$：\n  $P_C = \\frac{1}{1+0} = 1$\n  $R_C = \\frac{1}{1+4} = \\frac{1}{5} = 0.2$\n\n每个类别的 $F_1$ 分数是精确率和召回率的调和平均数：$F_{1,k} = 2 \\frac{P_k R_k}{P_k + R_k}$。\n- $F_{1,A} = 2 \\frac{(70/81)(7/8)}{(70/81)+(7/8)} = 2 \\frac{490/648}{560/648 + 567/648} = 2 \\frac{490}{1127} = \\frac{980}{1127} \\approx 0.8696$\n- $F_{1,B} = 2 \\frac{(4/9)(8/15)}{(4/9)+(8/15)} = 2 \\frac{32/135}{60/135 + 72/135} = 2 \\frac{32}{132} = \\frac{64}{132} = \\frac{16}{33} \\approx 0.4848$\n- $F_{1,C} = 2 \\frac{1 \\cdot (1/5)}{1 + (1/5)} = 2 \\frac{1/5}{6/5} = \\frac{2}{6} = \\frac{1}{3} \\approx 0.3333$\n\n现在我们可以计算宏平均和微平均的 $F_1$ 分数。\n\n**宏 $F_1$ 分数**是各类别 $F_1$ 分数的未加权平均值。\n$$\n\\text{Macro } F_1 = \\frac{F_{1,A} + F_{1,B} + F_{1,C}}{3} = \\frac{1}{3} \\left(\\frac{980}{1127} + \\frac{16}{33} + \\frac{1}{3}\\right) \\approx \\frac{0.8696 + 0.4848 + 0.3333}{3} = \\frac{1.6877}{3} \\approx 0.5626\n$$\n\n**微 $F_1$ 分数**需要微平均精确率和召回率，它们是通过对所有 $TP$、$FP$ 和 $FN$求和来计算的。\n- $\\sum TP_k = 70+8+1 = 79$\n- $\\sum FP_k = 11+10+0 = 21$\n- $\\sum FN_k = 10+7+4 = 21$\n$P_{micro} = \\frac{\\sum TP_k}{\\sum TP_k + \\sum FP_k} = \\frac{79}{79+21} = \\frac{79}{100} = 0.79$\n$R_{micro} = \\frac{\\sum TP_k}{\\sum TP_k + \\sum FN_k} = \\frac{79}{79+21} = \\frac{79}{100} = 0.79$\n注意，在多类别分类中，微平均精确率、微平均召回率和总体准确率是相同的，均为 $\\frac{\\text{正确分类数}}{\\text{总数}}$。\n$$\n\\text{Micro } F_1 = 2 \\frac{P_{micro} R_{micro}}{P_{micro} + R_{micro}} = \\frac{2 \\cdot 0.79 \\cdot 0.79}{0.79+0.79} = 0.79\n$$\n\n最后，我们使用自然对数计算平均负对数似然（对数损失）$L$。公式为 $L = -\\frac{1}{N} \\sum_{i=1}^N \\ln(p_{i, \\text{true}})$，其中 $p_{i, \\text{true}}$ 是模型分配给观测值 $i$ 的真实类别的概率。我们将指定组的对数概率相加：\n- 真实类别 $A$，预测类别 $A$：$70$ 个实例，$p=0.90$。贡献：$70 \\times \\ln(0.90)$。\n- 真实类别 $A$，预测类别 $B$：$10$ 个实例，$p=0.05$。贡献：$10 \\times \\ln(0.05)$。\n- 真实类别 $B$，预测类别 $B$：$8$ 个实例，$p=0.85$。贡献：$8 \\times \\ln(0.85)$。\n- 真实类别 $B$，预测类别 $A$：$7$ 个实例，$p=0.10$。贡献：$7 \\times \\ln(0.10)$。\n- 真实类别 $C$，预测类别 $C$：$1$ 个实例，$p=0.80$。贡献：$1 \\times \\ln(0.80)$。\n- 真实类别 $C$，预测类别 $A$：$4$ 个实例，$p=0.02$。贡献：$4 \\times \\ln(0.02)$。\n\n$$\n\\begin{align*} L = -\\frac{1}{100} [70\\ln(0.90) + 10\\ln(0.05) + 8\\ln(0.85) + 7\\ln(0.10) + 1\\ln(0.80) + 4\\ln(0.02)] \\\\\n\\approx -\\frac{1}{100} [70(-0.1054) + 10(-2.9957) + 8(-0.1625) + 7(-2.3026) + 1(-0.2231) + 4(-3.9120)] \\\\\n\\approx -\\frac{1}{100} [-7.378 - 29.957 - 1.300 - 16.118 - 0.223 - 15.648] \\\\\n\\approx -\\frac{1}{100} [-70.624] \\approx 0.706\n\\end{align*}\n$$\n计算指标摘要：\n- 宏 $F_1 \\approx 0.563$\n- 微 $F_1 = 0.790$\n- 对数损失 $\\approx 0.706$\n\n现在我们评估每个选项。\n\nA. 宏 $F_1 \\approx 0.563$ 显著低于微 $F_1 \\approx 0.790$，且对数损失约为 $0.706$，这表明尽管微平均性能很高，但对少数类别的过度自信的错误抬高了对数损失。\n宏 $F_1$、微 $F_1$ 和对数损失的计算值都正确。其解释也是合理的。宏 $F_1$ 之所以低，是因为它对少数类别 $B$（$F_1$ 分数 $\\approx 0.485$）和 $C$（$F_1$ 分数 $\\approx 0.333$）的极差表现给予了同等权重。微 $F_1$ 之所以高，是因为它主要由模型在多数类别 $A$（占数据总量的 $80\\%$）上的表现所决定，而模型在该类别上表现良好。对数损失被那些模型赋予真实类别极低概率的错误显著抬高（例如，对类别 $C$ 的 $4$ 个错误，其概率 $p=0.02$，对总损失 $0.706$ 的贡献为 $-\\frac{4}{100}\\ln(0.02) \\approx 0.156$，这仅占 $4\\%$ 的数据却贡献了不成比例的巨大损失）。这些确实是针对少数类别的过度自信的错误。这个陈述是对模型性能正确且富有洞察力的总结。**正确。**\n\nB. 宏 $F_1 \\approx 0.790$ 等于微 $F_1$，且对数损失约为 $0.300$，表明该分类器在所有类别上的表现都同样出色。\n宏 $F_1$ 和对数损失的值不正确。此外，宏 $F_1$ 等于微 $F_1$ 的说法是错误的，分类器在各类别上表现一致地好的说法也是错误的。各类别 $F_1$ 分数（$0.870, 0.485, 0.333$）显示出极不均衡的表现。**不正确。**\n\nC. 宏 $F_1 \\approx 0.563$，微 $F_1$ 也约为 $0.563$，且对数损失约为 $0.200$，因此两种 $F_1$ 变体和对数损失都说明了同样的问题。\n微 $F_1$ 的值不正确；它应该是 $0.790$，而不是 $0.563$。对数损失的值也不正确。两种 $F_1$ 变体说明了同样的问题这一前提是错误的；它们的差异才是此处的关键洞见。**不正确。**\n\nD. 宏 $F_1 \\approx 0.790$ 高于微 $F_1 \\approx 0.563$，且对数损失约为 $0.706$，这意味着宏平均降低了对不平衡错误的敏感性。\n宏 $F_1$ 和微 $F_1$ 的值被互换了。宏 $F_1 \\approx 0.563$，而微 $F_1$ 是 $0.790$。解释也是相反的；宏平均*增加*了对少数类别表现的敏感性，而不是降低它。**不正确。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "真正衡量拟合质量，需要超越汇总统计数据，深入探究模型做出预测的核心机制。这个编程实践将指导你实现并比较两种典型的分类器：生成模型高斯朴素贝叶斯和判别模型逻辑回归。通过在不同数据集上（模型的某些核心假设在这些数据集上分别得到满足或被违反）测试它们，你不仅将探索它们的分类准确性，还将使用对数损失（log-loss）评估其概率输出的质量，并通过期望校准误差（Expected Calibration Error, ECE）来衡量其预测概率的可靠性。",
            "id": "3147784",
            "problem": "您将实现并比较一个生成式分类器（高斯朴素贝叶斯）和一个判别式分类器（逻辑回归）在合成二元分类数据集上的表现，并使用平均对数损失和校准度量来评估拟合质量。该比较必须反映两种建模方法在正确指定和错误指定的类条件结构下，以及在类别不平衡情况下的行为。程序必须是自包含的，并遵循下面指定的精确数据生成过程和评估程序。\n\n设置为二元分类问题，特征 $x \\in \\mathbb{R}^2$，标签 $y \\in \\{0,1\\}$。使用以下基本原理和定义来推导您的算法：\n- 联合概率分解 $p(x,y) = p(y)\\,p(x \\mid y)$ 和贝叶斯法则 $p(y \\mid x) \\propto p(x \\mid y)\\,p(y)$。\n- 标签的伯努利似然，其条件概率为 $p(y=1 \\mid x) = \\sigma(w^\\top \\tilde{x})$，其中 $\\sigma(z)$ 是逻辑函数，$\\tilde{x}$ 是增加了截距项的特征向量。\n- 平均对数损失的定义，即测试集上每个样本的经验负对数似然。\n- 校准误差的定义，即期望校准误差（ECE），通过将预测概率分箱，并比较每个箱内的平均预测概率与经验频率来计算。\n\n您必须实现：\n- 高斯朴素贝叶斯：假设给定类别下特征条件独立。在每个类别 $y \\in \\{0,1\\}$ 中，将每个特征建模为具有类别特定均值和方差的单变量正态分布。通过训练数据的最大似然估计来估计类别先验、均值和方差。基于单变量高斯似然的乘积，使用贝叶斯法则计算测试点的 $p(y=1 \\mid x)$。\n- 逻辑回归：通过在逻辑模型下最大化给定特征的标签的条件似然来拟合参数。对权重（不包括截距）使用 $\\ell_2$ 惩罚，正则化强度为 $\\lambda = 10^{-3}$。使用任何基于梯度的数值优化器来寻找最大似然估计量。\n\n在测试集上为每个模型计算的评估指标：\n- 平均对数损失：在所有测试样本上计算 $-\\log p(y \\mid x)$ 的平均值，使用真实类别的预测概率。以实数形式表示（无单位）。\n- 期望校准误差（ECE）：将区间 $[0,1]$ 划分为 $B$ 个等宽的箱，其中 $B=10$。对于每个下边界为 $l_b$、上边界为 $u_b$ 的箱 $b$，包含预测概率 $p(y=1 \\mid x)$ 满足 $l_b \\le p  u_b$ 的测试样本，最后一个箱则为 $l_B \\le p \\le u_B$。对于每个非空箱，计算该箱内平均预测概率与 $y=1$ 的经验频率之间的绝对差。按箱中样本的比例加权，并对所有箱求和。以实数形式表示 ECE（无单位）。\n\n数据生成和测试套件：\n- 为保证可复现性，使用固定的随机种子：$\\text{seed} = 314159$。\n- 对于每个数据集，生成一个大小为 $n_{\\text{train}} = 400$ 的训练集和一个大小为 $n_{\\text{test}} = 10000$ 的测试集。对于每个集合，精确生成 $n_1$ 个来自类别 $y=1$ 的样本和 $n_0$ 个来自类别 $y=0$ 的样本，其中 $n_1 = \\max\\{1, \\min\\{n-1, \\lfloor n \\pi_1 \\rceil\\}\\}$，$n_0 = n - n_1$，而 $\\pi_1$ 是该案例的目标类别1先验。此处 $\\lfloor \\cdot \\rceil$ 表示四舍五入到最近的整数。将类别0和类别1的样本连接起来，然后使用该案例的固定种子偏移量（见下文）随机排列它们的顺序。\n- 根据类别从具有类别特定均值和协方差的二维高斯分布中条件生成特征。具体来说，对于类别 $y=k \\in \\{0,1\\}$，从 $x \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$ 中抽取样本。\n\n定义三个数据集作为测试套件，每个数据集在生成训练和测试数据时都有其自己的种子偏移量加到基础种子上 ($\\text{seed} + s$)：\n\n- 案例A（正确指定的独立性；平衡类别）：\n  - 先验：$\\pi_1 = 0.5$。\n  - 均值：$\\mu_0 = [0.0, 0.0]$, $\\mu_1 = [1.0, 1.0]$。\n  - 协方差：$\\Sigma_0 = \\mathrm{diag}([0.49, 0.49])$, $\\Sigma_1 = \\mathrm{diag}([0.49, 0.49])$。\n  - 种子偏移量：$s = 0$。\n\n- 案例B（通过相关性违反独立性；平衡类别）：\n  - 先验：$\\pi_1 = 0.5$。\n  - 均值：$\\mu_0 = [0.0, 0.0]$, $\\mu_1 = [0.8, 0.8]$。\n  - 协方差：$\\Sigma_0 = \\begin{bmatrix} 1.0  0.8 \\\\ 0.8  1.0 \\end{bmatrix}$, $\\Sigma_1 = \\begin{bmatrix} 1.0  0.8 \\\\ 0.8  1.0 \\end{bmatrix}$。\n  - 种子偏移量：$s = 1$。\n\n- 案例C（正确指定的独立性；类别不平衡）：\n  - 先验：$\\pi_1 = 0.1$。\n  - 均值：$\\mu_0 = [0.0, 0.0]$, $\\mu_1 = [1.2, 1.2]$。\n  - 协方差：$\\Sigma_0 = \\mathrm{diag}([1.0, 1.0])$, $\\Sigma_1 = \\mathrm{diag}([1.0, 1.0])$。\n  - 种子偏移量：$s = 2$。\n\n实现和数值要求：\n- 在高斯朴素贝叶斯中，通过最大似然法从训练数据中估计类别先验 $\\hat{\\pi}_1$、各类别均值 $\\hat{\\mu}_{k}$ 和各类别各特征方差 $\\hat{\\sigma}^2_{k,j}$。为了数值稳定性，向 $\\hat{\\sigma}^2_{k,j}$ 添加一个小的方差下限 $\\varepsilon = 10^{-9}$。使用贝叶斯法则和单变量高斯似然的乘积计算 $p(y=1 \\mid x)$。\n- 在逻辑回归中，包含一个截距项，并对权重（不包括截距）使用 $\\ell_2$ 正则化 $\\lambda = 10^{-3}$ 来优化正则化的负对数似然。使用数值稳定的逻辑损失和概率公式。\n- 对于平均对数损失，在应用对数之前，将预测概率裁剪到 $[\\delta, 1-\\delta]$ 范围内，其中 $\\delta = 10^{-15}$。\n- 对于ECE，使用 $B = 10$ 个箱，其边界为 $0, 0.1, 0.2, \\ldots, 1.0$，采用上述的箱成员规则，并忽略空箱（它们的贡献为零）。\n\n输出规格：\n- 对于A、B、C三个案例，按顺序计算四个数字：高斯朴素贝叶斯的平均对数损失，逻辑回归的平均对数损失，高斯朴素贝叶斯的ECE，逻辑回归的ECE。\n- 将三个案例的所有结果汇总到一个包含十二个实数的扁平列表中，顺序为：$[\\text{LL}_{\\text{NB,A}}, \\text{LL}_{\\text{LR,A}}, \\text{ECE}_{\\text{NB,A}}, \\text{ECE}_{\\text{LR,A}}, \\text{LL}_{\\text{NB,B}}, \\text{LL}_{\\text{LR,B}}, \\text{ECE}_{\\text{NB,B}}, \\text{ECE}_{\\text{LR,B}}, \\text{LL}_{\\text{NB,C}}, \\text{LL}_{\\text{LR,C}}, \\text{ECE}_{\\text{NB,C}}, \\text{ECE}_{\\text{LR,C}}]$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,\\ldots,r_{12}]$）。每个实数都以标准十进制形式表示。\n\n约束：\n- 程序必须是完全确定性的和自包含的，没有用户输入，也不访问外部文件或网络。\n- 仅使用允许的库，并确保如上所述的数值稳定实现。",
            "solution": "该问题要求在三个合成数据集上实现并比较评估两种二元分类模型：高斯朴素贝叶斯（GNB）和逻辑回归（LR）。评估重点是拟合质量，通过平均对数损失和期望校准误差（ECE）来衡量。该问题定义明确，科学上基于统计学习原理，并提供了详尽的算法、数据生成过程和评估指标规范，从而确保了一个唯一且可验证的解。\n\n### 数据生成\n对于指定的三个案例（A、B、C）中的每一个，都会生成一个大小为 $n_{\\text{train}} = 400$ 的训练集和一个大小为 $n_{\\text{test}} = 10000$ 的测试集。类别 $y=1$ 的样本数量为 $n_1 = \\max\\{1, \\min\\{n-1, \\lfloor n \\pi_1 \\rceil\\}\\}$，其中 $n$ 是总样本大小（$n_{\\text{train}}$ 或 $n_{\\text{test}}$），$\\pi_1$ 是给定案例的类别1先验。类别 $y=0$ 的样本数量为 $n_0 = n - n_1$。每个类别 $k \\in \\{0, 1\\}$ 的特征是从一个具有指定均值 $\\mu_k$ 和协方差 $\\Sigma_k$ 的二维高斯分布 $x \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$ 中抽取的。两个类别的样本被连接起来，然后使用一个带种子的随机数生成器进行随机排列，以确保可复现性。\n\n### 生成模型：高斯朴素贝叶斯（GNB）\nGNB模型是基于贝叶斯定理的生成式分类器。它通过将联合概率分布 $p(x, y)$ 分解为类别先验 $p(y)$ 和类条件密度 $p(x|y)$ 来建模。其核心的“朴素”假设是，给定类别标签 $y$ 的情况下，特征 $x_j$ 是条件独立的。\n$$p(x|y) = \\prod_{j=1}^{D} p(x_j|y)$$\n其中 $D=2$ 是特征数量。对于类别 $k \\in \\{0, 1\\}$，每个特征密度 $p(x_j|y=k)$ 被建模为一个单变量高斯分布 $\\mathcal{N}(\\mu_{k,j}, \\sigma^2_{k,j})$。\n\n模型参数使用最大似然估计（MLE）从训练数据 $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$ 中估计得出：\n1.  **类别先验**：$\\hat{\\pi}_k = p(y=k)$ 被估计为类别 $k$ 中训练样本的比例。设 $N_k$ 是 $y_i=k$ 的样本计数。则 $\\hat{\\pi}_k = N_k / n_{\\text{train}}$。\n2.  **类条件均值**：$\\hat{\\mu}_{k,j}$ 是类别 $k$ 中所有样本的特征 $j$ 的样本均值。\n3.  **类条件方差**：$\\hat{\\sigma}^2_{k,j}$ 是类别 $k$ 中所有样本的特征 $j$ 的样本方差。为保证数值稳定性，添加了一个 $\\varepsilon = 10^{-9}$ 的方差下限。\n\n对于一个新的数据点 $x$，后验概率 $p(y=1|x)$ 使用贝叶斯法则计算：\n$$p(y=1|x) = \\frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1) + p(x|y=0)p(y=0)}$$\n为保持数值稳定性，计算在对数空间中进行。类别 $k$ 的对数后验（忽略归一化常数）为：\n$$\\log p(y=k|x) \\propto \\log \\hat{\\pi}_k + \\sum_{j=1}^{D} \\log p(x_j|y=k; \\hat{\\mu}_{k,j}, \\hat{\\sigma}^2_{k,j})$$\n其中 $\\log p(x_j|y=k; \\ldots)$ 是单变量高斯的对数概率密度。最终概率通过指数化和归一化获得，使用 log-sum-exp 技巧以防止下溢/上溢。\n\n### 判别模型：逻辑回归（LR）\n逻辑回归是一种直接参数化后验概率 $p(y=1|x)$ 的判别模型。该模型假设特征与结果的对数几率之间存在线性关系：\n$$p(y=1 | x, w) = \\sigma(w_0 + w_1 x_1 + w_2 x_2) = \\sigma(w^T \\tilde{x})$$\n其中 $\\sigma(z) = (1+e^{-z})^{-1}$ 是逻辑S型函数，$\\tilde{x} = [1, x_1, x_2]^T$ 是增加了截距项的特征向量，而 $w = [w_0, w_1, w_2]^T$ 是模型权重向量。\n\n权重 $w$ 是通过在训练数据上最小化正则化的负对数似然（也称为交叉熵损失）来找到的。要最小化的目标函数 $J(w)$ 是：\n$$ J(w) = -\\sum_{i=1}^{n_{\\text{train}}} \\left[ y_i \\log \\sigma(w^T \\tilde{x}_i) + (1-y_i) \\log(1 - \\sigma(w^T \\tilde{x}_i)) \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^2 w_j^2 $$\n$\\ell_2$ 正则化项仅应用于特征权重（$w_1, w_2$），而不应用于截距（$w_0$），正则化强度为 $\\lambda = 10^{-3}$。对于单个样本 $i$，对数似然项的数值稳定形式是 $z_i - y_i z_i + \\log(1+e^{-z_i})$，其中 $z_i = w^T \\tilde{x}_i$。目标函数的梯度是：\n$$ \\nabla_w J(w) = \\sum_{i=1}^{n_{\\text{train}}} (\\sigma(w^T \\tilde{x}_i) - y_i) \\tilde{x}_i + \\lambda w^* $$\n其中 $w^*$ 是截距分量设为零的权重向量（$w^* = [0, w_1, w_2]^T$）。这个优化问题是凸的，并使用 L-BFGS-B 算法求解，这是一种在 `scipy.optimize` 中可用的基于梯度的拟牛顿法。\n\n### 评估指标\n每个模型的性能在测试集上使用两个指标进行评估：\n\n1.  **平均对数损失**：该指标评估预测概率的质量。它是测试数据的负对数似然，在所有样本上取平均值。对于一个大小为 $n_{\\text{test}}$ 的测试集，其定义为：\n    $$ \\text{Log-Loss} = -\\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} \\left[ y_i \\log \\hat{p}_i + (1-y_i) \\log(1 - \\hat{p}_i) \\right] $$\n    其中 $\\hat{p}_i = p(y=1|x_i)$ 是模型对样本 $i$ 的预测概率。为防止 $\\log(0)$ 错误，预测概率被裁剪到 $[\\delta, 1-\\delta]$ 范围内，其中 $\\delta = 10^{-15}$。\n\n2.  **期望校准误差（ECE）**：该指标衡量预测概率在多大程度上反映了事件的真实可能性。区间 $[0, 1]$ 上的预测概率被划分为 $B=10$ 个等宽的箱。对于每个箱 $b_m$，我们计算平均预测概率（$\\text{conf}(b_m)$）和正标签的经验频率（$\\text{acc}(b_m)$）。ECE是这两个量之间绝对差的加权平均值：\n    $$ \\text{ECE} = \\sum_{m=1}^{B} \\frac{|b_m|}{n_{\\text{test}}} | \\text{acc}(b_m) - \\text{conf}(b_m) | $$\n    其中 $|b_m|$ 是箱 $m$ 中的样本数。空箱对总和的贡献为零。\n\n这些模型、数据集和指标的组合允许进行细致的比较。GNB 预计在案例A中表现出色，因为其建模假设得到满足。LR 预计在案例B中具有优势，因为 GNB 的独立性假设被违反。案例C测试了模型在类别不平衡下的行为。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements and compares Gaussian Naive Bayes and Logistic Regression classifiers.\n    \"\"\"\n    # --- Problem Constants ---\n    BASE_SEED = 314159\n    N_TRAIN = 400\n    N_TEST = 10000\n    LAMBDA_REG = 1e-3\n    VAR_FLOOR = 1e-9\n    LOGLOSS_CLIP = 1e-15\n    ECE_BINS = 10\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"pi1\": 0.5,\n            \"mu0\": np.array([0.0, 0.0]),\n            \"mu1\": np.array([1.0, 1.0]),\n            \"Sigma0\": np.diag([0.49, 0.49]),\n            \"Sigma1\": np.diag([0.49, 0.49]),\n            \"seed_offset\": 0,\n        },\n        {\n            \"name\": \"B\",\n            \"pi1\": 0.5,\n            \"mu0\": np.array([0.0, 0.0]),\n            \"mu1\": np.array([0.8, 0.8]),\n            \"Sigma0\": np.array([[1.0, 0.8], [0.8, 1.0]]),\n            \"Sigma1\": np.array([[1.0, 0.8], [0.8, 1.0]]),\n            \"seed_offset\": 1,\n        },\n        {\n            \"name\": \"C\",\n            \"pi1\": 0.1,\n            \"mu0\": np.array([0.0, 0.0]),\n            \"mu1\": np.array([1.2, 1.2]),\n            \"Sigma0\": np.diag([1.0, 1.0]),\n            \"Sigma1\": np.diag([1.0, 1.0]),\n            \"seed_offset\": 2,\n        },\n    ]\n\n    def generate_data(n, pi1, mu0, mu1, Sigma0, Sigma1, rng):\n        \"\"\"Generates a dataset for a given case.\"\"\"\n        n1 = int(np.round(n * pi1))\n        n1 = max(1, min(n - 1, n1))\n        n0 = n - n1\n\n        X0 = rng.multivariate_normal(mu0, Sigma0, size=n0)\n        y0 = np.zeros(n0, dtype=int)\n        \n        X1 = rng.multivariate_normal(mu1, Sigma1, size=n1)\n        y1 = np.ones(n1, dtype=int)\n\n        X = np.vstack((X0, X1))\n        y = np.concatenate((y0, y1))\n\n        perm = rng.permutation(n)\n        return X[perm], y[perm]\n\n    class GaussianNaiveBayes:\n        def __init__(self, variance_floor=1e-9):\n            self.variance_floor = variance_floor\n\n        def fit(self, X, y):\n            X0 = X[y == 0]\n            X1 = X[y == 1]\n            \n            n0, n1 = len(X0), len(X1)\n            n_total = n0 + n1\n            \n            self.pi0_ = n0 / n_total\n            self.pi1_ = n1 / n_total\n            \n            self.mu0_ = np.mean(X0, axis=0)\n            self.mu1_ = np.mean(X1, axis=0)\n            \n            self.var0_ = np.var(X0, axis=0) + self.variance_floor\n            self.var1_ = np.var(X1, axis=0) + self.variance_floor\n\n        def predict_proba(self, X):\n            log_prior0 = np.log(self.pi0_)\n            log_prior1 = np.log(self.pi1_)\n            \n            log_lik0 = np.sum(norm.logpdf(X, self.mu0_, np.sqrt(self.var0_)), axis=1)\n            log_lik1 = np.sum(norm.logpdf(X, self.mu1_, np.sqrt(self.var1_)), axis=1)\n            \n            log_post0 = log_prior0 + log_lik0\n            log_post1 = log_prior1 + log_lik1\n            \n            log_norm = np.logaddexp(log_post0, log_post1)\n            log_prob1 = log_post1 - log_norm\n            \n            return np.exp(log_prob1)\n\n    class LogisticRegression:\n        def __init__(self, lambda_reg=1e-3):\n            self.lambda_reg = lambda_reg\n            self.w_ = None\n\n        def _cost_and_grad(self, w, X, y):\n            m = X.shape[0]\n            X_aug = np.hstack([np.ones((m, 1)), X])\n            z = X_aug @ w\n\n            # Numerically stable negative log-likelihood\n            cost = np.sum(z - y*z + np.logaddexp(0, -z))\n            \n            # Regularization term (excluding intercept)\n            reg_cost = 0.5 * self.lambda_reg * np.sum(w[1:]**2)\n            total_cost = cost + reg_cost\n            \n            # Gradient\n            p = 1 / (1 + np.exp(-z))\n            grad = X_aug.T @ (p - y)\n            \n            # Regularization gradient\n            grad[1:] += self.lambda_reg * w[1:]\n            \n            return total_cost, grad\n\n        def fit(self, X, y):\n            initial_w = np.zeros(X.shape[1] + 1)\n            res = minimize(\n                lambda w: self._cost_and_grad(w, X, y),\n                initial_w,\n                method='L-BFGS-B',\n                jac=True\n            )\n            self.w_ = res.x\n\n        def predict_proba(self, X):\n            m = X.shape[0]\n            X_aug = np.hstack([np.ones((m, 1)), X])\n            z = X_aug @ self.w_\n            return 1 / (1 + np.exp(-z))\n\n    def average_log_loss(y_true, y_pred_proba, clip):\n        p = np.clip(y_pred_proba, clip, 1 - clip)\n        loss = - (y_true * np.log(p) + (1 - y_true) * np.log(1 - p))\n        return np.mean(loss)\n\n    def expected_calibration_error(y_true, y_pred_proba, n_bins):\n        n_samples = len(y_true)\n        ece = 0.0\n        \n        for i in range(n_bins):\n            bin_lower = i / n_bins\n            bin_upper = (i + 1) / n_bins\n            \n            if i  n_bins - 1:\n                in_bin = (y_pred_proba = bin_lower)  (y_pred_proba  bin_upper)\n            else: # Last bin is inclusive on upper bound\n                in_bin = (y_pred_proba = bin_lower)  (y_pred_proba = bin_upper)\n                \n            n_in_bin = np.sum(in_bin)\n            \n            if n_in_bin  0:\n                acc_in_bin = np.mean(y_true[in_bin])\n                conf_in_bin = np.mean(y_pred_proba[in_bin])\n                ece += (n_in_bin / n_samples) * np.abs(acc_in_bin - conf_in_bin)\n                \n        return ece\n\n    all_results = []\n    \n    for case in test_cases:\n        rng = np.random.default_rng(BASE_SEED + case[\"seed_offset\"])\n        \n        X_train, y_train = generate_data(\n            N_TRAIN, case[\"pi1\"], case[\"mu0\"], case[\"mu1\"], case[\"Sigma0\"], case[\"Sigma1\"], rng\n        )\n        X_test, y_test = generate_data(\n            N_TEST, case[\"pi1\"], case[\"mu0\"], case[\"mu1\"], case[\"Sigma0\"], case[\"Sigma1\"], rng\n        )\n\n        # Gaussian Naive Bayes\n        gnb = GaussianNaiveBayes(variance_floor=VAR_FLOOR)\n        gnb.fit(X_train, y_train)\n        y_pred_gnb = gnb.predict_proba(X_test)\n        ll_gnb = average_log_loss(y_test, y_pred_gnb, LOGLOSS_CLIP)\n        ece_gnb = expected_calibration_error(y_test, y_pred_gnb, ECE_BINS)\n\n        # Logistic Regression\n        lr = LogisticRegression(lambda_reg=LAMBDA_REG)\n        lr.fit(X_train, y_train)\n        y_pred_lr = lr.predict_proba(X_test)\n        ll_lr = average_log_loss(y_test, y_pred_lr, LOGLOSS_CLIP)\n        ece_lr = expected_calibration_error(y_test, y_pred_lr, ECE_BINS)\n\n        all_results.extend([ll_gnb, ll_lr, ece_gnb, ece_lr])\n\n    print(f\"[{','.join(f'{r:.10f}' for r in all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个高质量的模型不仅应该准确，还应该具有鲁棒性；它的预测不应因输入数据的微小、不相关的变化而剧烈波动。本练习将介绍对抗性敏感度（adversarial sensitivity）的概念，这是衡量模型鲁棒性的一个关键指标。你将亲手实现快速梯度符号法（Fast Gradient Sign Method, FGSM），为输入特征制造“最坏情况”下的扰动，并衡量模型性能因此下降的程度，从而对模型的稳定性进行量化评估。",
            "id": "3147847",
            "problem": "考虑一个参数保持不变的固定预测模型以及一个由特征向量和标签组成的数据集。目标是在范数约束下，量化已拟合模型对直接应用于输入特征的微小、最坏情况扰动的敏感度。该敏感度通过比较未受扰动输入和对抗性扰动输入的损失变化来衡量，其中回归任务使用均方根误差（RMSE），二元分类任务使用逻辑对数损失。均方根误差（RMSE）定义为 $$\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}_{i} - y_{i}\\right)^{2}},$$ 其中 $\\hat{y}_{i}$ 是模型预测值，而 $y_{i}$ 是真实响应值。对于预测值为 $\\hat{y}_{i} = x_{i}^{\\top} w$ 的线性回归模型，其中 $x_{i} \\in \\mathbb{R}^{d}$ 是样本 $i$ 的特征向量，$w \\in \\mathbb{R}^{d}$ 是一个固定的参数向量，RMSE 是其拟合优度的度量。对于二元分类，其预测值为 $p_{i} = \\sigma(x_{i}^{\\top} w)$（其中 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$），逻辑对数损失（也称为负对数似然或交叉熵）为 $$\\mathcal{L}_{\\mathrm{log}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_{i} \\log p_{i} + (1 - y_{i}) \\log (1 - p_{i}) \\right],$$ 其中 $y_{i} \\in \\{0,1\\}$ 是标签，$p_{i}$ 是预测的概率。\n\n你将使用快速梯度符号法（Fast Gradient Sign Method, FGSM）来构建对抗性扰动。该方法定义为，所选损失函数相对于每个样本特征向量的梯度的逐元素符号，由一个非负标量界限 $\\epsilon$ 进行缩放，并应用于 $\\ell_{\\infty}$ 范数约束下：$$\\|\\Delta x_{i}\\|_{\\infty} \\le \\epsilon.$$ 模型参数 $w$ 是固定的（不进行重新拟合）。具体而言，对于使用 RMSE 的回归问题，令 $e_{i} = \\hat{y}_{i} - y_{i}$ 且 $$\\mathrm{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} e_{i}^{2}.$$ RMSE 相对于特征向量 $x_{i}$ 的梯度是 $$\\nabla_{x_{i}} \\mathrm{RMSE} = \\frac{e_{i}}{n \\sqrt{\\mathrm{MSE}}} w.$$ 由于乘法标量 $\\frac{e_{i}}{n \\sqrt{\\mathrm{MSE}}}$ 的正负取决于 $e_{i}$，因此 RMSE 的快速梯度符号法方向与 $e_{i} w$ 的逐元素符号一致。因此，回归问题的对抗性扰动为 $$\\Delta x_{i} = \\epsilon \\cdot \\mathrm{sign}\\!\\left(e_{i} w\\right).$$ 对于使用逻辑对数损失的二元分类问题，其相对于 $x_{i}$ 的梯度为 $$\\nabla_{x_{i}} \\mathcal{L}_{\\mathrm{log}} = \\frac{1}{n} (p_{i} - y_{i}) w,$$ 因此对抗性扰动为 $$\\Delta x_{i} = \\epsilon \\cdot \\mathrm{sign}\\!\\left((p_{i} - y_{i}) w\\right).$$ 对抗性扰动后的特征为每个样本的 $(x_{i} + \\Delta x_{i})$，敏感度为所选损失的非负变化量：对抗性损失减去基线损失。\n\n实现一个程序，对每个提供的测试用例执行以下步骤：\n1. 使用 $X \\in \\mathbb{R}^{n \\times d}$、$y$ 和固定的 $w$ 计算基线损失（回归任务为 RMSE，分类任务为逻辑对数损失）。\n2. 根据上述描述，使用给定的 $\\epsilon$ 对每个样本应用 FGSM 来构建 $\\Delta X$。\n3. 在 $(X + \\Delta X)$ 上计算对抗性损失。\n4. 输出对抗性损失与基线损失之间的标量差值。\n\n将所有计算都视作无单位的标量；不涉及任何物理单位。在应用对数函数之前，应通过将二元分类概率 $p_{i}$ 裁剪到区间 $[10^{-15}, 1 - 10^{-15}]$ 内来进行数值稳定。\n\n测试套件：\n- 用例 1（回归，一般情况）：$X = \\begin{bmatrix}1.0  2.0\\\\ 0.5  -1.0\\\\ 3.0  0.0\\end{bmatrix}$, $y = \\begin{bmatrix}4.0\\\\ -1.0\\\\ 5.0\\end{bmatrix}$, $w = \\begin{bmatrix}1.2\\\\ 0.8\\end{bmatrix}$, $\\epsilon = 0.1$。\n- 用例 2（回归，边界情况 $\\epsilon = 0$）：$X = \\begin{bmatrix}2.0  -0.5\\\\ -1.0  1.0\\end{bmatrix}$, $y = \\begin{bmatrix}1.0\\\\ 0.0\\end{bmatrix}$, $w = \\begin{bmatrix}0.5\\\\ -0.5\\end{bmatrix}$, $\\epsilon = 0.0$。\n- 用例 3（分类，一般情况）：$X = \\begin{bmatrix}0.2  -0.1\\\\ 1.5  0.3\\\\ -0.3  0.8\\end{bmatrix}$, $y = \\begin{bmatrix}0\\\\ 1\\\\ 1\\end{bmatrix}$, $w = \\begin{bmatrix}0.7\\\\ -0.5\\end{bmatrix}$, $\\epsilon = 0.2$。\n- 用例 4（分类，零权重边缘情况）：$X = \\begin{bmatrix}1.0  2.0\\\\ -0.5  0.5\\end{bmatrix}$, $y = \\begin{bmatrix}0\\\\ 1\\end{bmatrix}$, $w = \\begin{bmatrix}0.0\\\\ 0.0\\end{bmatrix}$, $\\epsilon = 0.5$。\n- 用例 5（分类，大 $\\epsilon$ 压力测试）：$X = \\begin{bmatrix}2.0  -1.0\\\\ -1.0  2.5\\\\ 0.3  -0.7\\\\ 1.0  1.0\\end{bmatrix}$, $y = \\begin{bmatrix}1\\\\ 0\\\\ 0\\\\ 1\\end{bmatrix}$, $w = \\begin{bmatrix}1.0\\\\ 1.5\\end{bmatrix}$, $\\epsilon = 2.0$。\n\n你的程序应该生成单行输出，其中包含所有五个测试用例的结果，结果为方括号括起来的逗号分隔列表，顺序与上述用例列表一致，例如 `[result1,result2,result3,result4,result5]`。每个结果必须是一个实数（浮点数），等于相应案例的对抗性损失减去基线损失。不应打印任何其他文本。",
            "solution": "该问题要求计算模型对其输入特征的对抗性扰动的敏感度。当使用快速梯度符号法（Fast Gradient Sign Method, FGSM）修改输入时，这种敏感度被量化为拟合优度度量（损失函数）的变化。模型参数 $w$ 保持不变。该过程涉及两种类型的模型：使用均方根误差（RMSE）损失的线性回归，以及使用逻辑对数损失进行二元分类的逻辑回归。\n\n该方法的核心是为输入数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 计算一个扰动 $\\Delta X$，其中 $n$ 是样本数量，$d$ 是特征数量。每个样本 $x_i$ 的扰动由损失函数 $\\mathcal{L}$ 相对于 $x_i$ 的梯度导出，并受 $\\ell_{\\infty}$ 范数约束：\n$$\n\\Delta x_{i} = \\epsilon \\cdot \\mathrm{sign}(\\nabla_{x_{i}} \\mathcal{L})\n$$\n其中 $\\epsilon \\geq 0$ 是扰动幅度。每个测试用例的最终输出是差值 $\\mathcal{L}_{\\mathrm{adv}} - \\mathcal{L}_{\\mathrm{base}}$，其中 $\\mathcal{L}_{\\mathrm{base}}$ 是在原始数据 $X$ 上的损失，而 $\\mathcal{L}_{\\mathrm{adv}}$ 是在扰动数据 $X + \\Delta X$ 上的损失。\n\n首先，我们分析回归情况。模型对样本 $x_i$ 的预测是 $\\hat{y}_i = x_i^\\top w$。损失函数是均方根误差（RMSE）：\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}_{i} - y_{i}\\right)^{2}} = \\sqrt{\\mathrm{MSE}}\n$$\n其中 $y_i$ 是真实的连续值响应。RMSE 相对于特征向量 $x_i$ 的梯度由以下公式给出：\n$$\n\\nabla_{x_{i}} \\mathrm{RMSE} = \\frac{e_{i}}{n \\sqrt{\\mathrm{MSE}}} w\n$$\n其中 $e_i = \\hat{y}_i - y_i$ 是样本 $i$ 的预测误差。由于标量因子 $\\frac{1}{n \\sqrt{\\mathrm{MSE}}}$ 是正的（对于 $\\mathrm{MSE}  0$），梯度的符号由 $e_i w$ 的符号决定。因此，第 $i$ 个样本特征向量 $x_i$ 的扰动为：\n$$\n\\Delta x_i = \\epsilon \\cdot \\mathrm{sign}(e_i w)\n$$\n为了同时为所有样本实现这一点，我们计算预测值 $\\hat{y} = Xw$ 和误差向量 $e = \\hat{y} - y$。扰动矩阵 $\\Delta X \\in \\mathbb{R}^{n \\times d}$ 可以通过将误差向量 $e$（一个 $n \\times 1$ 的列向量）与参数向量 $w^\\top$（一个 $1 \\times d$ 的行向量）进行广播，取其逐元素符号，然后乘以 $\\epsilon$ 来计算：\n$$\n\\Delta X = \\epsilon \\cdot \\mathrm{sign}(e w^\\top)\n$$\n基线损失 $\\mathrm{RMSE}_{\\mathrm{base}}$ 在 $X$ 上计算。对抗性特征是 $X_{\\mathrm{adv}} = X + \\Delta X$。对抗性损失 $\\mathrm{RMSE}_{\\mathrm{adv}}$ 使用 $X_{\\mathrm{adv}}$ 计算。结果是 $\\mathrm{RMSE}_{\\mathrm{adv}} - \\mathrm{RMSE}_{\\mathrm{base}}$。\n\n接下来，我们分析二元分类情况。模型使用 sigmoid 函数预测类别 1 的概率，$p_i = \\sigma(x_i^\\top w)$，其中 $\\sigma(z) = (1 + e^{-z})^{-1}$。损失函数是逻辑对数损失（或交叉熵）：\n$$\n\\mathcal{L}_{\\mathrm{log}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_{i} \\log p_{i} + (1 - y_{i}) \\log (1 - p_{i}) \\right]\n$$\n其中 $y_i \\in \\{0, 1\\}$ 是真实的二元标签。对数损失相对于 $x_i$ 的梯度由以下公式给出：\n$$\n\\nabla_{x_{i}} \\mathcal{L}_{\\mathrm{log}} = \\frac{1}{n} (p_{i} - y_{i}) w\n$$\n因子 $1/n$ 是一个正标量，因此梯度的符号由 $(p_i - y_i)w$ 的符号决定。样本 $i$ 的扰动为：\n$$\n\\Delta x_i = \\epsilon \\cdot \\mathrm{sign}((p_i - y_i)w)\n$$\n与回归情况类似，我们可以为所有样本计算扰动矩阵 $\\Delta X$。令 $p$ 为概率向量，$y$ 为真实标签向量。\n$$\n\\Delta X = \\epsilon \\cdot \\mathrm{sign}((p - y)w^\\top)\n$$\n基线损失 $\\mathcal{L}_{\\mathrm{base}}$ 使用原始数据 $X$ 计算。为确保数值稳定性，预测概率 $p_i$ 在传递给对数函数之前被裁剪到范围 $[10^{-15}, 1 - 10^{-15}]$ 内。对抗性特征是 $X_{\\mathrm{adv}} = X + \\Delta X$，对抗性损失 $\\mathcal{L}_{\\mathrm{adv}}$ 在这些特征上计算（对新的概率应用裁剪）。结果是 $\\mathcal{L}_{\\mathrm{adv}} - \\mathcal{L}_{\\mathrm{base}}$。\n\n在特殊情况下，例如 $\\epsilon = 0$ 或梯度是零向量（例如，如果 $w=0$），扰动 $\\Delta X$ 成为一个零矩阵。在这种情况下，$X_{\\mathrm{adv}} = X$，对抗性损失等于基线损失，敏感度为 $0$。这种情况由逐元素符号函数自然处理，因为 $\\mathrm{sign}(0) = 0$。\n\n整体算法遍历每个测试用例，识别问题类型（回归或分类），并应用相应的逻辑来计算基线损失、FGSM 扰动、对抗性损失，最后计算两种损失之间的差值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def compute_regression_sensitivity(X, y, w, epsilon):\n        \"\"\"\n        Computes the sensitivity for a linear regression model.\n        \"\"\"\n        n, d = X.shape\n        # Ensure y and w are column vectors for matrix operations\n        y = y.reshape(-1, 1)\n        w = w.reshape(-1, 1)\n\n        # 1. Compute baseline loss (RMSE)\n        y_hat = X @ w\n        errors = y_hat - y\n        mse_base = np.mean(np.square(errors))\n        # Handle case where MSE is zero\n        rmse_base = np.sqrt(mse_base) if mse_base  0 else 0.0\n\n        if epsilon == 0.0:\n            return 0.0\n\n        # 2. Construct adversarial perturbation delta_X\n        # Shape of errors: (n, 1), w.T: (1, d) - result: (n, d)\n        sign_matrix = np.sign(errors @ w.T)\n        delta_X = epsilon * sign_matrix\n\n        # 3. Compute adversarial loss\n        X_adv = X + delta_X\n        y_hat_adv = X_adv @ w\n        errors_adv = y_hat_adv - y\n        mse_adv = np.mean(np.square(errors_adv))\n        rmse_adv = np.sqrt(mse_adv) if mse_adv  0 else 0.0\n        \n        # 4. Return the difference\n        return rmse_adv - rmse_base\n\n    def compute_classification_sensitivity(X, y, w, epsilon):\n        \"\"\"\n        Computes the sensitivity for a logistic classification model.\n        \"\"\"\n        n, d = X.shape\n        # Ensure y and w are column vectors for matrix operations\n        y = y.reshape(-1, 1)\n        w = w.reshape(-1, 1)\n        \n        clip_val = 1e-15\n\n        def sigmoid(z):\n            return 1 / (1 + np.exp(-z))\n\n        def log_loss(p, y_true):\n            p_clipped = np.clip(p, clip_val, 1 - clip_val)\n            return -np.mean(y_true * np.log(p_clipped) + (1 - y_true) * np.log(1 - p_clipped))\n\n        # 1. Compute baseline loss (Log-Loss)\n        z_base = X @ w\n        p_base = sigmoid(z_base)\n        loss_base = log_loss(p_base, y)\n\n        if epsilon == 0.0 or np.all(w == 0):\n             return 0.0\n\n        # 2. Construct adversarial perturbation delta_X\n        # Use unclipped probabilities for gradient calculation\n        pred_errors = p_base - y\n        # Shape of pred_errors: (n, 1), w.T: (1, d) - result: (n, d)\n        sign_matrix = np.sign(pred_errors @ w.T)\n        delta_X = epsilon * sign_matrix\n        \n        # 3. Compute adversarial loss\n        X_adv = X + delta_X\n        z_adv = X_adv @ w\n        p_adv = sigmoid(z_adv)\n        loss_adv = log_loss(p_adv, y)\n\n        # 4. Return the difference\n        return loss_adv - loss_base\n\n    test_cases = [\n        # Case 1 (Regression, general case)\n        ('regression', \n         np.array([[1.0, 2.0], [0.5, -1.0], [3.0, 0.0]]), \n         np.array([4.0, -1.0, 5.0]), \n         np.array([1.2, 0.8]), 0.1),\n        # Case 2 (Regression, boundary epsilon = 0)\n        ('regression', \n         np.array([[2.0, -0.5], [-1.0, 1.0]]), \n         np.array([1.0, 0.0]), \n         np.array([0.5, -0.5]), 0.0),\n        # Case 3 (Classification, general case)\n        ('classification', \n         np.array([[0.2, -0.1], [1.5, 0.3], [-0.3, 0.8]]), \n         np.array([0, 1, 1]), \n         np.array([0.7, -0.5]), 0.2),\n        # Case 4 (Classification, zero-weights edge)\n        ('classification', \n         np.array([[1.0, 2.0], [-0.5, 0.5]]), \n         np.array([0, 1]), \n         np.array([0.0, 0.0]), 0.5),\n        # Case 5 (Classification, large epsilon stress)\n        ('classification', \n         np.array([[2.0, -1.0], [-1.0, 2.5], [0.3, -0.7], [1.0, 1.0]]), \n         np.array([1, 0, 0, 1]), \n         np.array([1.0, 1.5]), 2.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        case_type, X, y, w, epsilon = case\n        if case_type == 'regression':\n            result = compute_regression_sensitivity(X, y, w, epsilon)\n        else: # 'classification'\n            result = compute_classification_sensitivity(X, y, w, epsilon)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}