{
    "hands_on_practices": [
        {
            "introduction": "我们从一个基本问题开始：改变预测变量的单位会如何影响其回归系数？本练习将展示，虽然系数的数值会随新尺度的变化而调整，但模型的预测值保持不变。这个实践旨在强化一个核心观点：回归系数的解释始终与其对应预测变量的“单位变化”紧密相连。",
            "id": "3132949",
            "problem": "一名研究人员使用普通最小二乘法（OLS）拟合了一个多元线性回归模型，以模拟收缩压作为身高和年龄的函数。响应变量是收缩压 $y$（单位为 $\\mathrm{mmHg}$），预测变量是身高 $x_{1}$（单位为厘米）和年龄 $x_{2}$（单位为岁）。使用厘米的拟合模型为\n$$\ny \\;=\\; \\beta_{0} \\;+\\; \\beta_{1}\\,x_{1} \\;+\\; \\beta_{2}\\,x_{2} \\;+\\; \\varepsilon,\n$$\n估计的系数为\n$$\n\\hat{\\beta}_{0} \\;=\\; 50,\\quad \\hat{\\beta}_{1} \\;=\\; 0.12,\\quad \\hat{\\beta}_{2} \\;=\\; 0.7.\n$$\n该研究人员随后决定使用米作为身高单位来报告结果，定义 $z_{1} \\equiv x_{1}/100$，并将模型写为\n$$\ny \\;=\\; \\gamma_{0} \\;+\\; \\gamma_{1}\\,z_{1} \\;+\\; \\gamma_{2}\\,x_{2} \\;+\\; \\varepsilon.\n$$\n任务：\n- 从多元线性回归模型的定义出发，以及对于同一个体，拟合的预测值必须对测量单位的变化保持不变这一原则，推导当用 $z_{1} \\equiv x_{1}/100$ 替换 $x_{1}$ 时，系数如何变换。并用此计算出 $\\hat{\\gamma}_{1}$ 的数值。\n- 对于一个身高 $x_{1} = 180$ 厘米（即 $z_{1} = 1.80$ 米）且年龄 $x_{2} = 50$ 岁的人，使用厘米和米两种表示方法计算拟合值 $\\hat{y}$，并验证其相等性。\n\n最终答案仅报告重新标度的斜率系数 $\\hat{\\gamma}_{1}$ 的值，无单位。无需四舍五入。",
            "solution": "首先对问题陈述进行验证。\n\n### 第1步：提取已知条件\n问题提供了以下信息：\n- 一个关于收缩压 `y` (单位 mmHg) 作为身高 `x_1` (单位厘米) 和年龄 `x_2` (单位岁) 函数的多元线性回归模型：\n$$y \\;=\\; \\beta_{0} \\;+\\; \\beta_{1}\\,x_{1} \\;+\\; \\beta_{2}\\,x_{2} \\;+\\; \\varepsilon$$\n- 通过普通最小二乘法 (OLS) 得到的该模型的估计系数为：\n$$\n\\hat{\\beta}_{0} \\;=\\; 50,\\quad \\hat{\\beta}_{1} \\;=\\; 0.12,\\quad \\hat{\\beta}_{2} \\;=\\; 0.7\n$$\n- 提出了第二个模型，其中身高以米为单位测量，`z_1`，通过变换 `z_{1} \\equiv x_{1}/100` 定义。该模型的形式为：\n$$y \\;=\\; \\gamma_{0} \\;+\\; \\gamma_{1}\\,z_{1} \\;+\\; \\gamma_{2}\\,x_{2} \\;+\\; \\varepsilon$$\n- 问题要求基于拟合预测值 `\\hat{y}` 对身高单位的变化必须保持不变的原则，推导系数从 `(\\beta_0, \\beta_1, \\beta_2)` 到 `(\\gamma_0, \\gamma_1, \\gamma_2)` 的变换。\n- 必须计算 `\\hat{\\gamma}_{1}` 的数值。\n- 对于一个身高 `x_1 = 180` 厘米且年龄 `x_2 = 50` 岁的特定个体，必须验证拟合值的相等性。\n\n### 第2步：使用提取的已知条件进行验证\n该问题在科学上和数学上都是合理的。它涉及线性回归中一个标准的、基本的概念：预测变量的线性变换对回归系数的影响。前提是一致的，语言是精确的，设置是适定的，从而可以得出一个唯一的、有意义的解。所提供的值在生物统计学背景下是合理的。该问题不违反任何无效性标准。\n\n### 第3步：结论与行动\n问题有效。将提供完整解答。\n\n### 解答推导\n设第一个拟合模型（使用身高单位为厘米 `x_1`）表示为 `M_1`，第二个拟合模型（使用身高单位为米 `z_1`）表示为 `M_2`。\n\n模型 `M_1` 预测的收缩压 `\\hat{y}` 的方程为：\n$$\n\\hat{y}_{M_1} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2}\n$$\n模型 `M_2` 预测值的方程为：\n$$\n\\hat{y}_{M_2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}z_{1} + \\hat{\\gamma}_{2}x_{2}\n$$\n问题中陈述的核心原则是，对于任何给定的个体，预测值必须与预测变量所用的单位无关。这是一个物理不变性原则。因此，对于任何身高为 `x_1`（厘米）或 `z_1`（米）且年龄为 `x_2` 的个体，预测值必须相等：\n$$\n\\hat{y}_{M_1} = \\hat{y}_{M_2}\n$$\n这意味着：\n$$\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}z_{1} + \\hat{\\gamma}_{2}x_{2}\n$$\n我们已知 `x_1` 和 `z_1` 之间的关系：`z_{1} = x_{1}/100`，这等价于 `x_{1} = 100z_{1}`。为了关联这些系数，我们将 `z_1` 的定义代入等式的右侧：\n$$\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}\\left(\\frac{x_{1}}{100}\\right) + \\hat{\\gamma}_{2}x_{2}\n$$\n让我们重新排列右侧，按原始预测变量 `x_1` 和 `x_2` 对项进行分组：\n$$\n\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = \\hat{\\gamma}_{0} + \\left(\\frac{\\hat{\\gamma}_{1}}{100}\\right)x_{1} + \\hat{\\gamma}_{2}x_{2}\n$$\n为了使这个恒等式对预测变量 `x_1` 和 `x_2` 的所有可能值都成立，等式两边对应项的系数必须相等。这是多项式系数相等原则的直接应用。\n\n通过比较常数项（截距），我们发现：\n$$\n\\hat{\\beta}_{0} = \\hat{\\gamma}_{0}\n$$\n通过比较 `x_1` 项的系数，我们发现：\n$$\n\\hat{\\beta}_{1} = \\frac{\\hat{\\gamma}_{1}}{100}\n$$\n通过比较 `x_2` 项的系数，我们发现：\n$$\n\\hat{\\beta}_{2} = \\hat{\\gamma}_{2}\n$$\n从第二个等式中，我们可以解出 `\\hat{\\gamma}_{1}`：\n$$\n\\hat{\\gamma}_{1} = 100 \\cdot \\hat{\\beta}_{1}\n$$\n使用给定的数值 `\\hat{\\beta}_{1} = 0.12`，我们计算 `\\hat{\\gamma}_{1}`：\n$$\n\\hat{\\gamma}_{1} = 100 \\cdot 0.12 = 12\n$$\n以米为单位的模型的其他系数是 `\\hat{\\gamma}_{0} = \\hat{\\beta}_{0} = 50` 和 `\\hat{\\gamma}_{2} = \\hat{\\beta}_{2} = 0.7`。\n\n### 验证\n现在我们验证对于一个身高 `x_1 = 180` 厘米且年龄 `x_2 = 50` 岁的个体，拟合值 `\\hat{y}` 是相同的。相应的身高（米）是 `z_1 = 180 / 100 = 1.80` 米。\n\n使用第一个模型（厘米）：\n$$\n\\hat{y}_{M_1} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\hat{\\beta}_{2}x_{2} = 50 + (0.12)(180) + (0.7)(50)\n$$\n$$\n\\hat{y}_{M_1} = 50 + 21.6 + 35 = 106.6\n$$\n使用第二个模型（米）：\n$$\n\\hat{y}_{M_2} = \\hat{\\gamma}_{0} + \\hat{\\gamma}_{1}z_{1} + \\hat{\\gamma}_{2}x_{2} = 50 + (12)(1.80) + (0.7)(50)\n$$\n$$\n\\hat{y}_{M_2} = 50 + 21.6 + 35 = 106.6\n$$\n预测值是相同的，`\\hat{y}_{M_1} = \\hat{y}_{M_2} = 106.6`，这验证了我们推导的系数变换的正确性。问题要求 `\\hat{\\gamma}_{1}` 的数值。",
            "answer": "$$\n\\boxed{12}\n$$"
        },
        {
            "introduction": "多元回归的核心思想是在“控制”其他变量的同时，分离出单个预测变量的偏效应。本练习呈现了一个经典的“抑制效应”场景：在考虑第三个变量后，一个预测变量与结果变量之间的关系发生了符号反转。通过从基本定义出发推导系数，你将从几何和代数层面更深入地理解，为何一个边际上的正相关关系在调整其他变量后会变成一个偏负相关关系。",
            "id": "3132937",
            "problem": "考虑一个总体，其中预测变量对 $\\{x_1, x_2\\}$ 和响应变量 $y$ 联合分布且具有有限二阶矩。预测变量经过标准化，使得 $\\operatorname{E}[x_1] = \\operatorname{E}[x_2] = 0$ 且 $\\operatorname{Var}(x_1) = \\operatorname{Var}(x_2) = 1$。假设预测变量正相关，其协方差为 $\\operatorname{Cov}(x_1, x_2) = \\rho$，其中 $0  \\rho  1$。响应变量的均值为零，即 $\\operatorname{E}[y] = 0$。对该总体的经验研究得出以下协方差：$\\operatorname{Cov}(x_1, y) = 0.2$ 和 $\\operatorname{Cov}(x_2, y) = 0.8$。\n\n我们通过普通最小二乘法 (OLS) 拟合多元线性模型 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$。该方法可解释为将 $y$ 线性投影到由 $\\{x_1, x_2\\}$ 张成的空间上，以最小化期望残差平方和。从协方差、方差和线性投影的最小二乘最优性（正规方程）的基本定义出发，推导系数 $\\beta_1$ 作为 $\\rho$ 的函数，然后在 $\\rho = 0.9$ 处计算其值。在推导过程中，请阐明为什么 $x_1$ 和 $y$ 之间的边际关联为正，并不妨碍在控制了 $x_2$ 之后偏系数 $\\beta_1$ 为负（抑制效应），并使用正交投影从几何角度解释这种符号反转。\n\n计算 $\\beta_1$ 在 $\\rho = 0.9$ 时的数值，并将答案四舍五入到四位有效数字。该系数不需要单位。",
            "solution": "该问题要求推导一个多元回归系数，解释抑制效应，并进行数值计算。我们首先验证问题的陈述。\n\n### 问题验证\n该问题在理论统计学和线性模型的框架内是定义明确的。\n**已知条件：**\n- 预测变量 $\\{x_1, x_2\\}$ 和响应变量 $y$ 是具有有限二阶矩的随机变量。\n- $\\operatorname{E}[x_1] = 0$, $\\operatorname{E}[x_2] = 0$, $\\operatorname{E}[y] = 0$。\n- $\\operatorname{Var}(x_1) = 1$, $\\operatorname{Var}(x_2) = 1$。\n- $\\operatorname{Cov}(x_1, x_2) = \\rho$，其中 $0  \\rho  1$。\n- $\\operatorname{Cov}(x_1, y) = 0.2$。\n- $\\operatorname{Cov}(x_2, y) = 0.8$。\n- 模型为 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$，通过普通最小二乘法 (OLS) 拟合。\n\n**验证：**\n1.  **科学依据：** 该问题使用了期望、方差、协方差和普通最小二乘回归等标准且成熟的概念。所有前提都与统计理论一致。\n2.  **适定性：** 该问题提供了所有必要的信息（协方差和方差）来确定 OLS 系数 $\\beta_1$ 和 $\\beta_2$。条件 $0  \\rho  1$ 确保了预测变量之间不是完全共线性的，从而保证了唯一解的存在。\n3.  **客观性：** 该问题以精确的数学语言陈述，并要求进行形式推导和数值计算。\n\n该问题被认为是有效且自洽的。\n\n### 系数 $\\beta_1$ 的推导\nOLS 过程旨在最小化期望残差平方和 $S = \\operatorname{E}[\\varepsilon^2] = \\operatorname{E}[(y - \\beta_0 - \\beta_1 x_1 - \\beta_2 x_2)^2]$。\n\n首先，我们确定截距项 $\\beta_0$。对模型方程两边取期望：\n$$ \\operatorname{E}[y] = \\operatorname{E}[\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon] = \\beta_0 + \\beta_1 \\operatorname{E}[x_1] + \\beta_2 \\operatorname{E}[x_2] + \\operatorname{E}[\\varepsilon] $$\n根据 OLS 的构造，残差的期望值为零，即 $\\operatorname{E}[\\varepsilon] = 0$。利用给定的零均值条件，我们有：\n$$ 0 = \\beta_0 + \\beta_1(0) + \\beta_2(0) + 0 $$\n这意味着 $\\beta_0 = 0$。模型简化为 $y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$。\n\n$S = \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)^2]$ 的最小化导出了正规方程。正规方程表明残差向量必须与每个预测变量向量正交。用期望表示，即 $\\operatorname{E}[\\varepsilon x_1] = 0$ 和 $\\operatorname{E}[\\varepsilon x_2] = 0$。\n$$ \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)x_1] = 0 \\implies \\operatorname{E}[yx_1] - \\beta_1 \\operatorname{E}[x_1^2] - \\beta_2 \\operatorname{E}[x_1x_2] = 0 $$\n$$ \\operatorname{E}[(y - \\beta_1 x_1 - \\beta_2 x_2)x_2] = 0 \\implies \\operatorname{E}[yx_2] - \\beta_1 \\operatorname{E}[x_1x_2] - \\beta_2 \\operatorname{E}[x_2^2] = 0 $$\n鉴于所有变量的均值为零，乘积的期望等于协方差，平方的期望等于方差：\n- $\\operatorname{E}[yx_1] = \\operatorname{Cov}(x_1, y) = 0.2$\n- $\\operatorname{E}[yx_2] = \\operatorname{Cov}(x_2, y) = 0.8$\n- $\\operatorname{E}[x_1^2] = \\operatorname{Var}(x_1) = 1$\n- $\\operatorname{E}[x_2^2] = \\operatorname{Var}(x_2) = 1$\n- $\\operatorname{E}[x_1x_2] = \\operatorname{Cov}(x_1, x_2) = \\rho$\n\n将这些代入正规方程，得到关于 $\\beta_1$ 和 $\\beta_2$ 的以下线性方程组：\n$$ \\begin{cases} (1) \\quad 1 \\cdot \\beta_1 + \\rho \\cdot \\beta_2 = 0.2 \\\\ (2) \\quad \\rho \\cdot \\beta_1 + 1 \\cdot \\beta_2 = 0.8 \\end{cases} $$\n我们可以解这个方程组来求 $\\beta_1$。从方程 (1)，我们将 $\\beta_2$ 表示为 $\\beta_1$ 的函数：\n$$ \\beta_2 = \\frac{0.2 - \\beta_1}{\\rho} $$\n将此代入方程 (2)：\n$$ \\rho \\beta_1 + \\left( \\frac{0.2 - \\beta_1}{\\rho} \\right) = 0.8 $$\n两边乘以 $\\rho$（非零）以消去分母：\n$$ \\rho^2 \\beta_1 + 0.2 - \\beta_1 = 0.8 \\rho $$\n$$ \\beta_1(\\rho^2 - 1) = 0.8 \\rho - 0.2 $$\n$$ \\beta_1 = \\frac{0.8 \\rho - 0.2}{\\rho^2 - 1} = \\frac{-(0.2 - 0.8 \\rho)}{-(1 - \\rho^2)} $$\n这就得出了 $\\beta_1$ 作为 $\\rho$ 的函数的表达式：\n$$ \\beta_1 = \\frac{0.2 - 0.8 \\rho}{1 - \\rho^2} $$\n\n### 抑制效应与几何解释\n$x_1$ 和 $y$ 之间的边际关联由 $\\operatorname{Cov}(x_1, y) = 0.2$ 给出，是正的。然而，偏系数 $\\beta_1$ 可能为负。这种现象是一种抑制效应。$\\beta_1$ 为负的条件是：\n$$ \\frac{0.2 - 0.8 \\rho}{1 - \\rho^2}  0 $$\n因为 $0  \\rho  1$，分母 $1 - \\rho^2$ 是正的。因此，$\\beta_1$ 的符号由分子决定：\n$$ 0.2 - 0.8 \\rho  0 \\implies 0.2  0.8 \\rho \\implies \\rho > \\frac{0.2}{0.8} = 0.25 $$\n所以，如果 $\\rho > 0.25$，尽管 $x_1$ 和 $y$ 的边际协方差为正，但 $x_1$ 对 $y$ 的偏效应是负的。\n\n**概念解释：**\n系数 $\\beta_1$ 表示在保持 $x_2$ 不变的情况下，$x_1$ 每增加一个单位时 $y$ 的期望变化量。预测变量 $x_2$ 与 $y$ 有很强的正相关性（$\\operatorname{Cov}(x_2, y) = 0.8$），并且 $x_1$ 也与 $x_2$ 正相关（$\\rho > 0$）。$x_1$ 和 $y$ 之间观察到的正相关性有一部分是通过 $x_2$ 中介的：$x_1$ 的增加与 $x_2$ 的增加相关，而 $x_2$ 的增加又与 $y$ 的强劲增加相关。多元回归“剔除”了这种间接效应。如果通过 $x_2$ 的间接正相关性足够强（即 $\\rho$ 足够大），它就可能掩盖 $x_1$ 和 $y$ 之间潜在的负向直接关联。\n\n**几何解释：**\n在零均值随机变量的向量空间中，OLS 系数 $\\beta_1$ 可以通过正交投影来理解，正如 Frisch-Waugh-Lovell 定理所阐明的那样。系数 $\\beta_1$ 是 $y$ 对 $x_1$ 中与 $x_2$ 正交的部分进行回归的回归系数。令 $x_{1|2}$ 为 $x_1$ 对 $x_2$ 回归的残差：\n$$ x_{1|2} = x_1 - \\frac{\\operatorname{Cov}(x_1, x_2)}{\\operatorname{Var}(x_2)} x_2 = x_1 - \\rho x_2 $$\n那么 $\\beta_1$ 由 $y$ 对 $x_{1|2}$ 的简单回归给出：\n$$ \\beta_1 = \\frac{\\operatorname{Cov}(y, x_{1|2})}{\\operatorname{Var}(x_{1|2})} $$\n分子是：\n$$ \\operatorname{Cov}(y, x_1 - \\rho x_2) = \\operatorname{Cov}(y, x_1) - \\rho \\operatorname{Cov}(y, x_2) = 0.2 - \\rho(0.8) $$\n分母是：\n$$ \\operatorname{Var}(x_1 - \\rho x_2) = \\operatorname{Var}(x_1) - 2\\rho \\operatorname{Cov}(x_1, x_2) + \\rho^2 \\operatorname{Var}(x_2) = 1 - 2\\rho(\\rho) + \\rho^2(1) = 1 - \\rho^2 $$\n因此，$\\beta_1 = \\frac{0.2 - 0.8\\rho}{1 - \\rho^2}$，这验证了我们的推导。从几何上看，$\\beta_1$ 不是由向量 $y$ 在 $x_1$ 上的投影（该投影为正）决定的，而是由 $y$ 在向量 $x_{1|2}$（$x_1$ 中与 $x_2$ 正交的部分）上的投影决定的。如果 $\\rho$ 是一个较大的正数，那么向量 $x_1$ 和 $x_2$ 的方向很接近。向量 $x_{1|2}$ 代表了从 $\\rho x_2$ 变为 $x_1$ 所需的“修正量”。即使 $y$ 在 $x_1$ 上的投影为正，$y$ 在这个“修正”向量上的投影也可能为负。这种情况发生在 $y$ 被 $x_2$ 强烈吸引（因为 $\\operatorname{Cov}(y, x_2) = 0.8$）以至于 $x_1$ 的残差部分所需的向量分量变为负值时。\n\n### 数值计算\n我们在 $\\rho = 0.9$ 处计算 $\\beta_1$ 的值：\n$$ \\beta_1 = \\frac{0.2 - 0.8(0.9)}{1 - (0.9)^2} = \\frac{0.2 - 0.72}{1 - 0.81} = \\frac{-0.52}{0.19} $$\n$$ \\beta_1 \\approx -2.736842... $$\n四舍五入到四位有效数字，我们得到 $\\beta_1 = -2.737$。\n结果证实了抑制效应：当相关性很高，$\\rho = 0.9$（大于 0.25）时，偏系数 $\\beta_1$ 确实为负。",
            "answer": "$$\\boxed{-2.737}$$"
        },
        {
            "introduction": "前面的练习探讨了添加变量的理论效应，而本实践则关注一个常见的实际问题：多重共线性，即预测变量之间高度相关。通过一个计算实验，你将观察到加入一个看似多余的预测变量会如何剧烈改变——甚至反转——另一个系数的符号。这突显了诊断共线性的重要性，以避免从不稳定的系数估计中得出误导性结论。",
            "id": "3132946",
            "problem": "考虑一个线性模型，其中响应向量 $y \\in \\mathbb{R}^n$ 与预测变量 $x_1, x_2, x_3 \\in \\mathbb{R}^n$ 通过 $y \\approx X \\beta$ 相关联，设计矩阵 $X$ 由列向量 $x_1, x_2, x_3$ 堆叠而成。在普通最小二乘法 (OLS) 的设定中，估计量 $\\hat{\\beta}$ 解决一个最小二乘问题：它最小化残差的欧几里得范数平方，并且当格拉姆矩阵 $X^\\top X$ 可逆时，满足正规方程组 $X^\\top X \\hat{\\beta} = X^\\top y$。当 $X^\\top X$ 不可逆或 $X$ 秩亏时，摩尔-彭若斯伪逆 $X^+$ 会得到最小欧几里得范数解 $\\hat{\\beta} = X^+ y$。\n\n多元回归中的解释基于这样一个原则：系数 $\\hat{\\beta}_1$ 表示在调整了其他回归变量后，$x_1$ 对 $y$ 的条件效应。然而，回归变量之间存在的共线性（例如，$x_3$ 几乎是 $x_1$ 和 $x_2$ 的线性组合）会使这种解释变得不稳定。本问题要求你设计并实现一个计算实验，以证明 $\\hat{\\beta}_1$ 的符号对于包含一个看似不相关但位于现有回归变量生成空间内的协变量 $x_3$ 的敏感性。其基本依据是 OLS 估计量的特征和欧几里得空间中的投影几何。\n\n你的程序必须：\n- 对于每个测试用例，构建设计矩阵 $X_A$（包含列 $x_1, x_2$）和 $X_B$（包含列 $x_1, x_2, x_3$）。\n- 计算 OLS 估计 $\\hat{\\beta}^{(A)} = X_A^+ y$ 和 $\\hat{\\beta}^{(B)} = X_B^+ y$，其中 $X^+$ 表示摩尔-彭若斯伪逆。\n- 从每个模型中提取第一个系数 $\\hat{\\beta}_1^{(A)}$ 和 $\\hat{\\beta}_1^{(B)}$。\n- 判断从模型 A 到模型 B，$\\hat{\\beta}_1$ 是否发生符号改变，仅当 $\\hat{\\beta}_1^{(A)}$ 和 $\\hat{\\beta}_1^{(B)}$ 中一个严格为正而另一个严格为负时，才判定为发生改变。任一估计值为 $0$ 的情况应被视为没有符号改变。\n\n测试套件：\n使用以下 $5$ 个确定性测试用例，每个用例有 $n = 3$ 个观测值。每个用例由向量 $x_1, x_2, x_3, y \\in \\mathbb{R}^3$ 定义：\n\n- 用例 1 (理想情况，正交且不相关的 $x_3$):\n  $$\n  x_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n  x_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n  x_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\\quad\n  y   = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n  $$\n  此处 $x_3$ 与 $x_1$ 和 $x_2$ 都正交，对拟合 $y$ 没有任何贡献。\n\n- 用例 2 (边界条件，$x_1$ 的完全复制):\n  $$\n  x_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n  x_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n  x_3 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n  y   = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n  $$\n  这测试了由 $x_1$ 和 $x_3$ 完美共线性导致的秩亏情况。\n\n- 用例 3 (中度共线性，无符号翻转):\n  $$\n  x_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n  x_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n  x_3 = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ 0 \\end{bmatrix},\\quad\n  y   = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n  $$\n\n- 用例 4 (强共线性导致因重新参数化而引起的符号翻转):\n  $$\n  x_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n  x_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n  x_3 = \\begin{bmatrix} 1 \\\\ 4 \\\\ 0 \\end{bmatrix},\\quad\n  y   = \\begin{bmatrix} 0.2 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n  $$\n\n- 用例 5 (极端共线性；近奇异的重新参数化):\n  $$\n  x_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n  x_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n  x_3 = \\begin{bmatrix} 1 \\\\ 20 \\\\ 0 \\end{bmatrix},\\quad\n  y   = \\begin{bmatrix} 0.02 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n  $$\n\n答案规格：\n- 对每个用例，根据上述规则输出一个布尔值，指示是否发生了符号改变。\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表，例如 $[\\,\\text{true},\\text{false},\\ldots\\,]$，使用标准的 Python 布尔字面量 $\\text{True}$ 和 $\\text{False}$。\n\n实现约束：\n- 使用通过数值线性代数计算出的摩尔-彭若斯伪逆 $X^+$ 来稳健地处理秩亏设计。\n- 不包含截距项；所有向量都已明确给出。\n- 最终答案必须是一个完整的、可运行的 Python 程序，该程序为测试套件计算所需的布尔值，并以指定的精确格式单行打印出来。",
            "solution": "我们从线性模型中普通最小二乘法 (OLS) 的核心定义开始。给定设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$（其列为 $x_1, x_2, \\ldots, x_p$）和响应向量 $y \\in \\mathbb{R}^n$，OLS 估计量 $\\hat{\\beta} \\in \\mathbb{R}^p$ 最小化残差范数的平方\n$$\n\\hat{\\beta} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\|y - X \\beta\\|_2^2.\n$$\n当 $X^\\top X$ 可逆时，一阶最优性条件得出正规方程组\n$$\nX^\\top X \\hat{\\beta} = X^\\top y,\n$$\n因此\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y.\n$$\n当 $X$ 秩亏或 $p > n$ 时，摩尔-彭若斯伪逆 $X^+$ 给出最小欧几里得范数解\n$$\n\\hat{\\beta} = X^+ y,\n$$\n这是在所有能得到相同拟合值的 $X \\beta$ 解中，具有最小 $\\|\\hat{\\beta}\\|_2$ 的唯一向量。\n\n多元回归中系数的解释依赖于投影的几何视角。对于一个排除 $x_1$ 的满列秩设计，系数 $\\hat{\\beta}_1$ 可以通过部分回归来表示，即在将 $y$ 和 $x_1$ 都投影到其他回归变量生成空间的正交补上之后，将残差化的 $y$ 对残差化的 $x_1$ 进行回归得到的斜率。用符号表示，如果 $X_{-1}$ 表示除 $x_1$ 外所有回归变量组成的矩阵，$P_{-1}$ 是到 $\\mathrm{span}(X_{-1})$ 上的正交投影算子，那么残差为 $r_y = (I - P_{-1}) y$ 和 $r_{x_1} = (I - P_{-1}) x_1$，并且\n$$\n\\hat{\\beta}_1 = \\frac{\\langle r_{x_1}, r_y \\rangle}{\\|r_{x_1}\\|_2^2}.\n$$\n因此，$\\hat{\\beta}_1$ 的符号取决于残差化预测变量 $r_{x_1}$ 和残差化响应 $r_y$ 之间的夹角；如果它们大致指向同一方向，符号为正，如果指向相反方向，符号为负。\n\n共线性深刻地影响了这种解释。当 $x_3$ 几乎是 $x_1$ 和 $x_2$ 的线性组合时，包含 $x_3$ 可能几乎不改变回归变量的生成空间，但在该生成空间内的解的参数化被改变，可能导致系数值——甚至其符号——变得不稳定，特别是当设计是秩亏的，并且最小范数准则在共线预测变量之间选择了不同的权重分配时。\n\n我们用指定的测试套件来说明这种敏感性。程序为每个用例构建设计矩阵 $X_A = [x_1\\; x_2]$ 和 $X_B = [x_1\\; x_2\\; x_3]$，并计算 $\\hat{\\beta}^{(A)} = X_A^+ y$ 和 $\\hat{\\beta}^{(B)} = X_B^+ y$，然后检查在加入 $x_3$ 时第一个系数的符号是否改变。\n\n符号翻转用例的解析洞察：\n考虑一个二维子空间，其中\n$$\nx_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix},\\quad\nx_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix},\\quad\nx_3 = \\begin{bmatrix} 1 \\\\ c \\end{bmatrix},\\quad\ny   = \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}.\n$$\n设计矩阵为\n$$\nX_A = \\begin{bmatrix} 1   0 \\\\ 0   1 \\end{bmatrix},\\quad\nX_B = \\begin{bmatrix} 1   0  1 \\\\ 0  1  c \\end{bmatrix}.\n$$\n对于 $X_A$，其伪逆与逆矩阵相同，因此\n$$\n\\hat{\\beta}^{(A)} = \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix},\\quad \\text{因此 } \\hat{\\beta}_1^{(A)} = y_1.\n$$\n对于 $X_B$，使用宽矩阵的摩尔-彭若斯伪逆，可以计算出\n$$\n\\hat{\\beta}^{(B)} = X_B^\\top (X_B X_B^\\top)^{-1} y.\n$$\n计算\n$$\nX_B X_B^\\top = \\begin{bmatrix} 2   c \\\\ c  1 + c^2 \\end{bmatrix},\\quad\n\\left(X_B X_B^\\top\\right)^{-1} = \\frac{1}{2 + c^2} \\begin{bmatrix} 1 + c^2   -c \\\\ -c  2 \\end{bmatrix}.\n$$\n因此\n$$\n\\hat{\\beta}_1^{(B)} = \\frac{(1 + c^2) y_1 - c\\, y_2}{2 + c^2}.\n$$\n$\\hat{\\beta}_1^{(B)}$ 的符号为负当且仅当\n$$\n(1 + c^2) y_1 - c\\, y_2  0 \\quad \\Longleftrightarrow \\quad y_1  \\frac{c}{1 + c^2} y_2.\n$$\n因此，即使当 $y_1 > 0$ (所以 $\\hat{\\beta}_1^{(A)} > 0$) 时，足够大的 $c$ 以及与 $y_2$ 相比相对较小的 $y_1$ 会迫使 $\\hat{\\beta}_1^{(B)}  0$，这表明符号翻转纯粹是由于共线性结构以及通过一个作为现有预测变量线性组合的额外回归变量所引起的重新参数化。\n\n映射到三维测试用例，指定向量中的第三个分量为零，因此分析在前两个坐标所张成的二维平面上进行。在用例 4 中，取 $c = 4$, $y_1 = 0.2$, $y_2 = 1$。条件变为\n$$\n0.2  \\frac{4}{1 + 4^2} \\cdot 1 = \\frac{4}{17} \\approx 0.23529,\n$$\n该条件成立。因此\n$$\n\\hat{\\beta}_1^{(B)} = \\frac{(1 + 16)\\cdot 0.2 - 4 \\cdot 1}{2 + 16} = \\frac{3.4 - 4}{18} = \\frac{-0.6}{18} \\approx -0.033\\ldots,\n$$\n并且由于 $\\hat{\\beta}_1^{(A)} = y_1 = 0.2 > 0$，发生了符号翻转。用例 5 选择 $c = 20$, $y_1 = 0.02$, $y_2 = 1$，其阈值为\n$$\n\\frac{20}{1 + 20^2} = \\frac{20}{401} \\approx 0.04988,\n$$\n所以 $0.02  0.04988$，符号再次翻转。\n\n相比之下，用例 1 使 $x_3$ 正交且不相关，保留了 $\\hat{\\beta}_1$；用例 2 复制了 $x_1$，导致秩亏，但最小范数解在共线预测变量之间分配权重而没有翻转符号；用例 3 具有中度共线性，$c=0.5$，得出\n$$\n\\hat{\\beta}_1^{(B)} = \\frac{(1 + 0.25)\\cdot 1 - 0.5 \\cdot 1}{2 + 0.25} = \\frac{0.75}{2.25} = \\frac{1}{3} > 0,\n$$\n因此没有翻转。\n\n算法设计：\n- 根据指定，为每个测试用例构建向量 $x_1, x_2, x_3, y$。\n- 通过列堆叠形成 $X_A$ 和 $X_B$。\n- 使用摩尔-彭若斯伪逆计算 $\\hat{\\beta}^{(A)} = X_A^+ y$ 和 $\\hat{\\beta}^{(B)} = X_B^+ y$。\n- 提取 $\\hat{\\beta}_1^{(A)}$ 和 $\\hat{\\beta}_1^{(B)}$ 并评估布尔谓词\n$$\n\\left( \\hat{\\beta}_1^{(A)} > 0 \\land \\hat{\\beta}_1^{(B)}  0 \\right) \\lor \\left( \\hat{\\beta}_1^{(A)}  0 \\land \\hat{\\beta}_1^{(B)} > 0 \\right).\n$$\n- 汇总所有用例的布尔值，并以要求的单行格式打印它们。\n\n这表明，多元回归中 $\\hat{\\beta}_1$ 的符号并非仅由预测变量 $x_1$ 本身决定的内在属性；它取决于共线性结构和在设计矩阵列空间内所选择的参数化。那些看似不相关且不扩展现有回归变量生成空间的变量，仍然可以通过最小范数解中的重新分配来动摇系数的符号，这突显了在共线性条件下解释回归系数时需要保持谨慎。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef ols_pinv(X, y):\n    \"\"\"\n    Compute the minimum-norm OLS estimate using the Moore-Penrose pseudoinverse.\n    \"\"\"\n    return np.linalg.pinv(X) @ y\n\ndef sign_flip(b1_a, b1_b):\n    \"\"\"\n    Determine if there is a sign change between two coefficients.\n    Change is True only if one is strictly positive and the other strictly negative.\n    Zero values are treated as no change.\n    \"\"\"\n    return (b1_a > 0 and b1_b  0) or (b1_a  0 and b1_b > 0)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple (x1, x2, x3, y), where each is a numpy array of shape (3,).\n    test_cases = [\n        # Case 1: Orthogonal and irrelevant x3\n        (np.array([1.0, 0.0, 0.0]),\n         np.array([0.0, 1.0, 0.0]),\n         np.array([0.0, 0.0, 1.0]),\n         np.array([1.0, 1.0, 0.0])),\n        # Case 2: Exact duplication of x1\n        (np.array([1.0, 0.0, 0.0]),\n         np.array([0.0, 1.0, 0.0]),\n         np.array([1.0, 0.0, 0.0]),\n         np.array([1.0, 1.0, 0.0])),\n        # Case 3: Moderate collinearity without sign flip\n        (np.array([1.0, 0.0, 0.0]),\n         np.array([0.0, 1.0, 0.0]),\n         np.array([1.0, 0.5, 0.0]),\n         np.array([1.0, 1.0, 0.0])),\n        # Case 4: Strong collinearity causing sign flip\n        (np.array([1.0, 0.0, 0.0]),\n         np.array([0.0, 1.0, 0.0]),\n         np.array([1.0, 4.0, 0.0]),\n         np.array([0.2, 1.0, 0.0])),\n        # Case 5: Extreme collinearity; near-singular reparameterization\n        (np.array([1.0, 0.0, 0.0]),\n         np.array([0.0, 1.0, 0.0]),\n         np.array([1.0, 20.0, 0.0]),\n         np.array([0.02, 1.0, 0.0])),\n    ]\n\n    results = []\n    for x1, x2, x3, y in test_cases:\n        # Model A: using x1 and x2\n        X_A = np.column_stack([x1, x2])\n        beta_A = ols_pinv(X_A, y)\n        b1_A = float(beta_A[0])\n\n        # Model B: using x1, x2, and x3\n        X_B = np.column_stack([x1, x2, x3])\n        beta_B = ols_pinv(X_B, y)\n        b1_B = float(beta_B[0])\n\n        # Determine sign change\n        results.append(sign_flip(b1_A, b1_B))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}