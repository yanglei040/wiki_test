{
    "hands_on_practices": [
        {
            "introduction": "A model's performance on the data used to train it can often be misleadingly optimistic. This exercise highlights the critical distinction between interpolation and extrapolation, demonstrating how a simple linear model that appears accurate can fail dramatically when used to predict outcomes for inputs far outside the range of the original data. By working through the scenarios in this problem , you will develop a crucial intuition for the dangers of model misspecification and the importance of understanding the domain of your data.",
            "id": "3138831",
            "problem": "Consider a one-dimensional supervised learning setup where the input is a scalar $x$ and the response is a scalar $y$. You will estimate regression coefficients via Ordinary Least Squares (OLS), defined as the choice of coefficients that minimize the Residual Sum of Squares (RSS). The fundamental base for this problem is the definition of the squared-error loss and the concept of minimizing empirical risk: given training pairs $\\{(x_i, y_i)\\}_{i=1}^n$, define the linear predictor $f(x) = \\beta_0 + \\beta_1 x$ and choose coefficients that minimize the training RSS, which is the sum of squared residuals.\n\nYour program must implement the following tasks grounded in first principles:\n\n1. Generate training data $(x_i, y_i)$ according to specified data-generating processes, where $y_i = f_{\\text{true}}(x_i) + \\varepsilon_i$, with $\\varepsilon_i$ sampled independently from a normal distribution with zero mean and specified standard deviation. The training inputs must lie in a specified interval that does not overlap with the test input interval.\n\n2. Fit a linear model with intercept by minimizing the training RSS to obtain coefficients $(\\hat{\\beta}_0, \\hat{\\beta}_1)$.\n\n3. Compute the training Mean Squared Error (MSE), defined as the mean of squared residuals on the training data, and the test Mean Squared Error (MSE) computed on a noise-free test set drawn from the specified test interval using the true data-generating function $f_{\\text{true}}(x)$ without noise.\n\n4. For each test case, output the ratio $r = \\text{MSE}_{\\text{test}} / \\text{MSE}_{\\text{train}}$ as a float. This ratio quantifies how extrapolation performance compares to in-sample fit. Express each ratio as a decimal rounded to $6$ places.\n\nYou must not use any pre-supplied regression formulas in the problem statement; instead, base your implementation on the principle of minimizing squared error. If the design matrix is rank-deficient, use a principled least-squares solution that attains the minimum training RSS.\n\nDefine the following acronyms on first use:\n- Ordinary Least Squares (OLS): the method that chooses coefficients to minimize the Residual Sum of Squares (RSS).\n- Residual Sum of Squares (RSS): the sum of squared residuals on training data.\n- Mean Squared Error (MSE): the average of squared residuals over a dataset.\n\nTest Suite Specification:\n- Case $1$ (happy path, linear truth and extrapolation is reliable):\n  - Training inputs: $n_{\\text{train}} = 50$ points uniformly spaced on $[-1, 1]$.\n  - Test inputs: $n_{\\text{test}} = 50$ points uniformly spaced on $[2, 3]$.\n  - True function: $f_{\\text{true}}(x) = 2x + 1$.\n  - Noise standard deviation: $\\sigma = 0.05$.\n  - Random seed for noise: seed $0$.\n\n- Case $2$ (nonlinear truth within small training support, extrapolation is poor despite small training RSS):\n  - Training inputs: $n_{\\text{train}} = 50$ points uniformly spaced on $[-0.5, 0.5]$.\n  - Test inputs: $n_{\\text{test}} = 50$ points uniformly spaced on $[2, 3]$.\n  - True function: $f_{\\text{true}}(x) = x^3$.\n  - Noise standard deviation: $\\sigma = 0.02$.\n  - Random seed for noise: seed $1$.\n\n- Case $3$ (boundary case with rank-deficient design, extrapolation becomes effectively constant):\n  - Training inputs: $n_{\\text{train}} = 30$ points all at $x = 0$.\n  - Test inputs: $n_{\\text{test}} = 30$ points uniformly spaced on $[1, 2]$.\n  - True function: $f_{\\text{true}}(x) = 2x + 1$.\n  - Noise standard deviation: $\\sigma = 0.05$.\n  - Random seed for noise: seed $2$.\n\nAlgorithmic Requirements:\n- Construct the design matrix $X$ with a column of ones and a column of training inputs.\n- Minimize the training RSS to obtain $(\\hat{\\beta}_0, \\hat{\\beta}_1)$. If the matrix implied by the normal equations is not invertible, use a Moore-Penrose pseudoinverse solution that minimizes training RSS.\n- Compute $\\text{MSE}_{\\text{train}}$ and $\\text{MSE}_{\\text{test}}$ as specified, where $\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$ for a dataset of size $m$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, specifically the list of the three ratios $[r_1, r_2, r_3]$ rounded to $6$ decimal places, where $r_k$ corresponds to Case $k$ in the order above.",
            "solution": "The problem requires the implementation and evaluation of a linear regression model based on the principle of Ordinary Least Squares (OLS), which is a method that selects regression coefficients to minimize the Residual Sum of Squares (RSS). The RSS is defined as the sum of squared differences between the observed responses and the predictions of the linear model. We will analyze the model's performance by comparing its in-sample fit to its extrapolation capability.\n\nThe model to be fitted is a simple linear regression model of the form $f(x) = \\beta_0 + \\beta_1 x$, where $\\beta_0$ is the intercept and $\\beta_1$ is the slope. Given a set of training data points $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$, the goal is to find the coefficient vector $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1]^\\top$ that minimizes the training RSS:\n$$\n\\text{RSS}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n_{\\text{train}}} (y_i - f(x_i))^2 = \\sum_{i=1}^{n_{\\text{train}}} (y_i - (\\beta_0 + \\beta_1 x_i))^2\n$$\n\nThis problem is most elegantly formulated using linear algebra. We can represent the system of linear equations for all training points in matrix form. Let $\\mathbf{y}$ be the vector of observed responses, $X$ be the design matrix, $\\boldsymbol{\\beta}$ be the vector of coefficients, and $\\boldsymbol{\\varepsilon}$ be the vector of errors. The model is $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$.\n\nThe design matrix $X$ for a simple linear model is constructed by augmenting a column of ones (for the intercept $\\beta_0$) to the vector of input values $\\mathbf{x}_{\\text{train}}$:\n$$\nX = \\begin{pmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_{n_{\\text{train}}}\n\\end{pmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{pmatrix}, \\quad\n\\mathbf{y} = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_{n_{\\text{train}}}\n\\end{pmatrix}\n$$\n\nThe RSS can then be written as the squared Euclidean norm of the residual vector $\\mathbf{r} = \\mathbf{y} - X\\boldsymbol{\\beta}$:\n$$\n\\text{RSS}(\\boldsymbol{\\beta}) = \\|\\mathbf{y} - X\\boldsymbol{\\beta}\\|^2_2\n$$\nTo find the coefficients $\\hat{\\boldsymbol{\\beta}}$ that minimize this quantity, we take the gradient with respect to $\\boldsymbol{\\beta}$ and set it to zero, which yields the normal equations:\n$$\n(X^\\top X) \\hat{\\boldsymbol{\\beta}} = X^\\top \\mathbf{y}\n$$\nIf the matrix $X^\\top X$ is invertible, the unique OLS solution is:\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}\n$$\nHowever, if $X^\\top X$ is singular (i.e., not invertible), which occurs when the columns of $X$ are linearly dependent, there are infinitely many solutions for $\\hat{\\boldsymbol{\\beta}}$ that minimize the RSS. This happens in Case $3$ of the problem, where all training inputs $x_i$ are identical, making the second column of $X$ a multiple of the first. In such cases, a unique solution is chosen by finding the one with the minimum Euclidean norm $\\|\\boldsymbol{\\beta}\\|_2$. This solution is given by the Moore-Penrose pseudoinverse, denoted by the `+` superscript:\n$$\n\\hat{\\boldsymbol{\\beta}} = X^+ \\mathbf{y}\n$$\nwhere $X^+ = (X^\\top X)^+ X^\\top$. This formulation provides the least-squares solution for both invertible and singular cases, and is thus the general approach we will implement.\n\nOnce the coefficients $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1]^\\top$ are estimated, we can make predictions for any given input $x$ using $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$. We then evaluate the model's performance using the Mean Squared Error (MSE), defined as the average of the squared residuals over a dataset of size $m$: $\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$.\n\nWe will compute two MSE values:\n$1$. The training MSE, $\\text{MSE}_{\\text{train}}$, is calculated on the training data $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$.\n$2$. The test MSE, $\\text{MSE}_{\\text{test}}$, is calculated on a noise-free test set. This set consists of inputs $x_{\\text{test}, j}$ and true, noiseless responses $y_{\\text{test}, j} = f_{\\text{true}}(x_{\\text{test}, j})$.\n\nThe final step for each case is to compute the ratio $r = \\text{MSE}_{\\text{test}} / \\text{MSE}_{\\text{train}}$, which quantifies the degradation of performance when extrapolating outside the training data domain.\n\n**Case 1:** Training on $[-1, 1]$, testing on $[2, 3]$. The true function is linear: $f_{\\text{true}}(x) = 2x + 1$. The linear model is correctly specified. We expect the estimated coefficients to be close to $(\\beta_0, \\beta_1) = (1, 2)$. As the model is correct, it should extrapolate well, leading to a ratio $r_1$ close to $1$.\n\n**Case 2:** Training on $[-0.5, 0.5]$, testing on $[2, 3]$. The true function is cubic: $f_{\\text{true}}(x) = x^3$. The linear model is misspecified. While it might provide a reasonable fit within the narrow training interval, its linear projection will drastically diverge from the cubic curve in the extrapolation region. This will result in a very large $\\text{MSE}_{\\text{test}}$ and consequently a large ratio $r_2$.\n\n**Case 3:** Training on $x_i=0$ for all $i$. The design matrix $X$ has a column of ones and a column of zeros, making it rank-deficient. The pseudoinverse solution will correctly estimate $\\hat{\\beta}_0$ as the mean of the training responses and set $\\hat{\\beta}_1 = 0$ (the minimum-norm choice). The resulting model is a constant function, $\\hat{y} = \\hat{\\beta}_0$. When this constant model is used to predict values for the test set on $[1, 2]$ where the true function is $f_{\\text{true}}(x) = 2x + 1$, a large test error will occur. This will produce a large ratio $r_3$.\n\nThe implementation will follow these principles, using `numpy` for numerical computations, including the pseudoinverse for solving the least-squares problem robustly across all cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates an Ordinary Least Squares (OLS) regression model\n    for three distinct test cases, computing the ratio of test MSE to training MSE.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Happy path, linear truth and reliable extrapolation\n        {\n            \"n_train\": 50,\n            \"train_x_spec\": {\"type\": \"uniform\", \"range\": [-1, 1]},\n            \"n_test\": 50,\n            \"test_x_spec\": {\"type\": \"uniform\", \"range\": [2, 3]},\n            \"f_true\": lambda x: 2 * x + 1,\n            \"noise_sigma\": 0.05,\n            \"seed\": 0,\n        },\n        # Case 2: Nonlinear truth, poor extrapolation\n        {\n            \"n_train\": 50,\n            \"train_x_spec\": {\"type\": \"uniform\", \"range\": [-0.5, 0.5]},\n            \"n_test\": 50,\n            \"test_x_spec\": {\"type\": \"uniform\", \"range\": [2, 3]},\n            \"f_true\": lambda x: x**3,\n            \"noise_sigma\": 0.02,\n            \"seed\": 1,\n        },\n        # Case 3: Rank-deficient design matrix\n        {\n            \"n_train\": 30,\n            \"train_x_spec\": {\"type\": \"constant\", \"value\": 0},\n            \"n_test\": 30,\n            \"test_x_spec\": {\"type\": \"uniform\", \"range\": [1, 2]},\n            \"f_true\": lambda x: 2 * x + 1,\n            \"noise_sigma\": 0.05,\n            \"seed\": 2,\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        #\n        # 1. Generate Training and Test Data\n        #\n        rng = np.random.default_rng(case[\"seed\"])\n\n        # Training data\n        n_train = case[\"n_train\"]\n        train_spec = case[\"train_x_spec\"]\n        if train_spec[\"type\"] == \"uniform\":\n            x_train = np.linspace(train_spec[\"range\"][0], train_spec[\"range\"][1], n_train)\n        elif train_spec[\"type\"] == \"constant\":\n            x_train = np.full(n_train, train_spec[\"value\"])\n        \n        y_true_train = case[\"f_true\"](x_train)\n        noise = rng.normal(0, case[\"noise_sigma\"], n_train)\n        y_train = y_true_train + noise\n\n        # Test data (noise-free)\n        n_test = case[\"n_test\"]\n        test_spec = case[\"test_x_spec\"]\n        if test_spec[\"type\"] == \"uniform\":\n            x_test = np.linspace(test_spec[\"range\"][0], test_spec[\"range\"][1], n_test)\n        \n        y_test = case[\"f_true\"](x_test)\n\n        #\n        # 2. Fit Linear Model using OLS from first principles\n        #\n        # Construct the design matrix X for training data\n        X_train = np.vstack([np.ones(n_train), x_train]).T\n        \n        # Solve for beta_hat using the Moore-Penrose pseudoinverse.\n        # This is equivalent to beta_hat = (X.T @ X)^+ @ X.T @ y\n        # and correctly handles rank-deficient cases.\n        beta_hat = np.linalg.pinv(X_train) @ y_train\n\n        #\n        # 3. Compute Training and Test MSE\n        #\n        \n        # Training MSE\n        # Predictions: y_hat = X @ beta_hat\n        y_hat_train = X_train @ beta_hat\n        \n        # Residuals: y - y_hat\n        residuals_train = y_train - y_hat_train\n        \n        # MSE = mean of squared residuals\n        mse_train = np.mean(residuals_train**2)\n\n        # Test MSE\n        # Construct the design matrix for test data\n        X_test = np.vstack([np.ones(n_test), x_test]).T\n        \n        # Predictions on test data using the fitted model\n        y_hat_test = X_test @ beta_hat\n        \n        # Residuals against the true (noise-free) test values\n        residuals_test = y_test - y_hat_test\n        \n        # MSE\n        mse_test = np.mean(residuals_test**2)\n\n        #\n        # 4. Compute the Ratio\n        #\n        ratio = mse_test / mse_train\n        results.append(round(ratio, 6))\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many real-world predictors are categorical (e.g., brand, location) rather than numerical. Incorporating them into a linear model requires careful encoding to avoid a common pitfall known as perfect multicollinearity, where predictors are linearly dependent, making the coefficient estimates non-unique. This practice  guides you through this issue, showing how a naive one-hot encoding scheme leads to a non-identifiable model and how to resolve it using constraints, yielding interpretable coefficients.",
            "id": "3138898",
            "problem": "A single categorical predictor with three levels is used to explain a real-valued response. Consider the following data with categorical levels $\\text{A}$, $\\text{B}$, and $\\text{C}$:\n- Group $\\text{A}$: responses $y$ equal to $2$ and $4$.\n- Group $\\text{B}$: responses $y$ equal to $3$, $5$, and $7$.\n- Group $\\text{C}$: response $y$ equal to $10$.\n\nWe attempt a linear model with an intercept and one-hot encoding that includes all three level indicators:\n$$\ny_{i} = \\beta_{0} + \\beta_{\\text{A}} \\,\\mathbf{1}\\{g_{i}=\\text{A}\\} + \\beta_{\\text{B}} \\,\\mathbf{1}\\{g_{i}=\\text{B}\\} + \\beta_{\\text{C}} \\,\\mathbf{1}\\{g_{i}=\\text{C}\\} + \\varepsilon_{i},\n$$\nwhere $g_{i}$ denotes the group of observation $i$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nStarting from the definition of ordinary least squares (OLS), which chooses coefficients by minimizing the sum of squared residuals, explain why the coefficients in this fully one-hot encoded model are not uniquely identifiable. Then, impose the identifiability constraint\n$$\n\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}} = 0\n$$\nand, working from first principles, derive the constrained least squares estimates in closed form. Compute their numerical values for the given data.\n\nFinally, briefly explain how adopting reference coding by dropping level $\\text{C}$ leads to an equivalent fitted model and what the resulting coefficients represent, without using any shortcut formulas.\n\nReport your final answer as the vector of constrained estimates $(\\beta_{0}, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}})$, expressed exactly. No rounding is required.",
            "solution": "The problem requires an analysis of a linear model with a three-level categorical predictor, first by explaining the non-identifiability of a fully one-hot encoded model, and then by deriving and computing constrained least squares estimates. Finally, an equivalent reference-coded model must be explained.\n\nFirst, we perform the problem validation.\n\n### Step 1: Extract Givens\n- Data:\n    - Group $\\text{A}$: responses $y$ are $2$ and $4$. The number of observations is $n_{\\text{A}}=2$.\n    - Group $\\text{B}$: responses $y$ are $3$, $5$, and $7$. The number of observations is $n_{\\text{B}}=3$.\n    - Group $\\text{C}$: response $y$ is $10$. The number of observations is $n_{\\text{C}}=1$.\n- Model (Full One-Hot Encoding):\n$$\ny_{i} = \\beta_{0} + \\beta_{\\text{A}} \\,\\mathbf{1}\\{g_{i}=\\text{A}\\} + \\beta_{\\text{B}} \\,\\mathbf{1}\\{g_{i}=\\text{B}\\} + \\beta_{\\text{C}} \\,\\mathbf{1}\\{g_{i}=\\text{C}\\} + \\varepsilon_{i}\n$$\n- Identifiability Constraint:\n$$\n\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}} = 0\n$$\n- Tasks:\n    1. Explain why the coefficients in the fully one-hot encoded model are not uniquely identifiable.\n    2. Derive the constrained least squares estimates from first principles.\n    3. Compute the numerical values of the constrained estimates.\n    4. Explain the equivalent reference-coded model where level $\\text{C}$ is dropped.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the theory of linear models and ordinary least squares (OLS), which are core concepts in statistics. The issue of non-identifiability due to multicollinearity in models with categorical predictors is a standard topic. The use of a sum-to-zero constraint to achieve identifiability is a valid and common technique. The problem is well-posed, providing all necessary data and a clear set of objectives. The language is precise and objective. There are no scientific or factual unsoundness, no missing information, and the setup is not contradictory or unrealistic. Therefore, the problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\n#### Non-identifiability of the Full Model\n\nThe specified linear model can be expressed in matrix form as $Y = X\\beta + \\varepsilon$. The vector of parameters is $\\beta = [\\beta_0, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}}]^\\top$. The design matrix $X$ has columns corresponding to the intercept and the three indicator variables for groups $\\text{A}$, $\\text{B}$, and $\\text{C}$. Let these columns be $X_0, X_{\\text{A}}, X_{\\text{B}}, X_{\\text{C}}$.\n- $X_0$ is a column of all ones, representing the intercept $\\beta_0$.\n- $X_{\\text{A}}$ is a column with a $1$ for observations in group $\\text{A}$ and $0$ otherwise.\n- $X_{\\text{B}}$ is a column with a $1$ for observations in group $\\text{B}$ and $0$ otherwise.\n- $X_{\\text{C}}$ is a column with a $1$ for observations in group $\\text{C}$ and $0$ otherwise.\n\nFor any observation $i$, the corresponding indicator variables satisfy the property $\\mathbf{1}\\{g_{i}=\\text{A}\\} + \\mathbf{1}\\{g_{i}=\\text{B}\\} + \\mathbf{1}\\{g_{i}=\\text{C}\\} = 1$. This means that the sum of the columns corresponding to the categorical levels is equal to the intercept column: $X_{\\text{A}} + X_{\\text{B}} + X_{\\text{C}} = X_0$. This indicates perfect linear dependency, or multicollinearity, among the columns of the design matrix $X$.\n\nThe OLS estimate for $\\beta$ is given by $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y$. Because the columns of $X$ are linearly dependent, the matrix $X^\\top X$ is singular (not invertible). Consequently, its inverse $(X^\\top X)^{-1}$ does not exist, and there is no unique solution for $\\hat{\\beta}$.\n\nAlternatively, consider a set of coefficients $(\\beta_0, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}})$ that minimizes the sum of squared residuals. For any arbitrary constant $c$, consider a new set of coefficients $(\\beta_0 + c, \\beta_{\\text{A}} - c, \\beta_{\\text{B}} - c, \\beta_{\\text{C}} - c)$. The predicted value for an observation in group $\\text{A}$ is $(\\beta_0 + c) + (\\beta_{\\text{A}} - c) = \\beta_0 + \\beta_{\\text{A}}$, which is unchanged. Similarly, predictions for groups $\\text{B}$ and $\\text{C}$ are also unchanged. Since the predicted values are identical for any choice of $c$, the sum of squared residuals is also identical. This implies there are infinitely many solutions, and thus the coefficients are not uniquely identifiable.\n\n#### Constrained Least Squares Estimation\n\nTo find the unique estimates under the given constraint, we minimize the Residual Sum of Squares (RSS) subject to the constraint. The objective function is the RSS:\n$$\n\\text{RSS} = \\sum_{i \\in \\text{A}} (y_i - (\\beta_0 + \\beta_{\\text{A}}))^2 + \\sum_{i \\in \\text{B}} (y_i - (\\beta_0 + \\beta_{\\text{B}}))^2 + \\sum_{i \\in \\text{C}} (y_i - (\\beta_0 + \\beta_{\\text{C}}))^2\n$$\nThe constraint is $\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}} = 0$.\n\nWe use the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(\\beta_0, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}}, \\lambda) = \\text{RSS} - \\lambda(\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}})\n$$\nWe find the partial derivatives with respect to each parameter and set them to zero. Let $\\bar{y}_{\\text{A}}, \\bar{y}_{\\text{B}}, \\bar{y}_{\\text{C}}$ be the sample means of the responses in each group, and $n_{\\text{A}}, n_{\\text{B}}, n_{\\text{C}}$ be the sample sizes.\n\n1.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_0} = -2 \\sum_{i \\in \\text{A}}(y_i - \\beta_0 - \\beta_{\\text{A}}) - 2 \\sum_{i \\in \\text{B}}(y_i - \\beta_0 - \\beta_{\\text{B}}) - 2 \\sum_{i \\in \\text{C}}(y_i - \\beta_0 - \\beta_{\\text{C}}) = 0$\n    This simplifies to $n_{\\text{A}}(\\bar{y}_{\\text{A}} - \\beta_0 - \\beta_{\\text{A}}) + n_{\\text{B}}(\\bar{y}_{\\text{B}} - \\beta_0 - \\beta_{\\text{B}}) + n_{\\text{C}}(\\bar{y}_{\\text{C}} - \\beta_0 - \\beta_{\\text{C}}) = 0$.\n\n2.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_{\\text{A}}} = -2 \\sum_{i \\in \\text{A}}(y_i - \\beta_0 - \\beta_{\\text{A}}) - \\lambda = 0 \\implies n_{\\text{A}}(\\bar{y}_{\\text{A}} - \\beta_0 - \\beta_{\\text{A}}) = \\lambda/2$.\n\n3.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_{\\text{B}}} = -2 \\sum_{i \\in \\text{B}}(y_i - \\beta_0 - \\beta_{\\text{B}}) - \\lambda = 0 \\implies n_{\\text{B}}(\\bar{y}_{\\text{B}} - \\beta_0 - \\beta_{\\text{B}}) = \\lambda/2$.\n\n4.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_{\\text{C}}} = -2 \\sum_{i \\in \\text{C}}(y_i - \\beta_0 - \\beta_{\\text{C}}) - \\lambda = 0 \\implies n_{\\text{C}}(\\bar{y}_{\\text{C}} - \\beta_0 - \\beta_{\\text{C}}) = \\lambda/2$.\n\n5.  $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}}) = 0$.\n\nSubstituting the results from (2), (3), and (4) into equation (1) gives:\n$\\lambda/2 + \\lambda/2 + \\lambda/2 = 0 \\implies 3\\lambda/2 = 0 \\implies \\lambda = 0$.\n\nWith $\\lambda=0$, equations (2)-(4) become (since $n_k > 0$):\n- $\\bar{y}_{\\text{A}} - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{A}} = 0 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{A}} = \\bar{y}_{\\text{A}}$\n- $\\bar{y}_{\\text{B}} - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{B}} = 0 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{B}} = \\bar{y}_{\\text{B}}$\n- $\\bar{y}_{\\text{C}} - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{C}} = 0 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{C}} = \\bar{y}_{\\text{C}}$\n\nThese equations show that the fitted value for each group is simply its sample mean. We can express the $\\hat{\\beta}_k$ terms as a function of $\\hat{\\beta}_0$:\n- $\\hat{\\beta}_{\\text{A}} = \\bar{y}_{\\text{A}} - \\hat{\\beta}_0$\n- $\\hat{\\beta}_{\\text{B}} = \\bar{y}_{\\text{B}} - \\hat{\\beta}_0$\n- $\\hat{\\beta}_{\\text{C}} = \\bar{y}_{\\text{C}} - \\hat{\\beta}_0$\n\nSubstituting these into the constraint equation (5):\n$(\\bar{y}_{\\text{A}} - \\hat{\\beta}_0) + (\\bar{y}_{\\text{B}} - \\hat{\\beta}_0) + (\\bar{y}_{\\text{C}} - \\hat{\\beta}_0) = 0$\n$\\bar{y}_{\\text{A}} + \\bar{y}_{\\text{B}} + \\bar{y}_{\\text{C}} - 3\\hat{\\beta}_0 = 0$\nSolving for $\\hat{\\beta}_0$:\n$$\n\\hat{\\beta}_0 = \\frac{\\bar{y}_{\\text{A}} + \\bar{y}_{\\text{B}} + \\bar{y}_{\\text{C}}}{3}\n$$\nThe remaining coefficients are then found as $\\hat{\\beta}_k = \\bar{y}_k - \\hat{\\beta}_0$. $\\hat{\\beta}_0$ is the unweighted average of the group means, and $\\hat{\\beta}_k$ is the deviation of the mean of group $k$ from this average.\n\n#### Numerical Computation\nFirst, we compute the sample means from the data:\n- $\\bar{y}_{\\text{A}} = \\frac{2+4}{2} = 3$\n- $\\bar{y}_{\\text{B}} = \\frac{3+5+7}{3} = \\frac{15}{3} = 5$\n- $\\bar{y}_{\\text{C}} = \\frac{10}{1} = 10$\n\nNow, we compute the coefficients:\n- $\\hat{\\beta}_0 = \\frac{3 + 5 + 10}{3} = \\frac{18}{3} = 6$\n- $\\hat{\\beta}_{\\text{A}} = \\bar{y}_{\\text{A}} - \\hat{\\beta}_0 = 3 - 6 = -3$\n- $\\hat{\\beta}_{\\text{B}} = \\bar{y}_{\\text{B}} - \\hat{\\beta}_0 = 5 - 6 = -1$\n- $\\hat{\\beta}_{\\text{C}} = \\bar{y}_{\\text{C}} - \\hat{\\beta}_0 = 10 - 6 = 4$\n\nThe vector of constrained estimates is $(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{A}}, \\hat{\\beta}_{\\text{B}}, \\hat{\\beta}_{\\text{C}}) = (6, -3, -1, 4)$.\nWe can verify the constraint: $\\hat{\\beta}_{\\text{A}} + \\hat{\\beta}_{\\text{B}} + \\hat{\\beta}_{\\text{C}} = -3 + (-1) + 4 = 0$.\n\n#### Equivalent Reference-Coded Model\nIf we adopt reference coding by dropping level $\\text{C}$, the model becomes:\n$$\ny_i = \\gamma_0 + \\gamma_{\\text{A}} \\mathbf{1}\\{g_i=\\text{A}\\} + \\gamma_{\\text{B}} \\mathbf{1}\\{g_i=\\text{B}\\} + \\varepsilon_i\n$$\nHere, group $\\text{C}$ is the reference level. The design matrix for this model has linearly independent columns, so the OLS estimates are unique. The expected values for each group are:\n- $E[y|g=\\text{A}] = \\gamma_0 + \\gamma_{\\text{A}}$\n- $E[y|g=\\text{B}] = \\gamma_0 + \\gamma_{\\text{B}}$\n- $E[y|g=\\text{C}] = \\gamma_0$\n\nThe OLS estimates of these expected values are the respective group sample means. Thus, we have:\n- $\\hat{\\gamma}_0 = \\bar{y}_{\\text{C}} = 10$\n- $\\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{A}} = \\bar{y}_{\\text{A}} \\implies \\hat{\\gamma}_{\\text{A}} = \\bar{y}_{\\text{A}} - \\hat{\\gamma}_0 = \\bar{y}_{\\text{A}} - \\bar{y}_{\\text{C}} = 3 - 10 = -7$\n- $\\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{B}} = \\bar{y}_{\\text{B}} \\implies \\hat{\\gamma}_{\\text{B}} = \\bar{y}_{\\text{B}} - \\hat{\\gamma}_0 = \\bar{y}_{\\text{B}} - \\bar{y}_{\\text{C}} = 5 - 10 = -5$\n\nThe interpretation is that $\\hat{\\gamma}_0$ is the estimated mean of the reference group ($\\text{C}$), while $\\hat{\\gamma}_{\\text{A}}$ and $\\hat{\\gamma}_{\\text{B}}$ are the estimated differences in means between groups $\\text{A}$ and $\\text{B}$ and the reference group $\\text{C}$, respectively.\n\nThe fitted values for this model are:\n- For group $\\text{A}$: $\\hat{y}_{\\text{A}} = \\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{A}} = 10 + (-7) = 3 = \\bar{y}_{\\text{A}}$\n- For group $\\text{B}$: $\\hat{y}_{\\text{B}} = \\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{B}} = 10 + (-5) = 5 = \\bar{y}_{\\text{B}}$\n- For group $\\text{C}$: $\\hat{y}_{\\text{C}} = \\hat{\\gamma}_0 = 10 = \\bar{y}_{\\text{C}}$\n\nThe fitted values are identical to those from the constrained model (e.g., for Group A, $\\hat{\\beta}_0 + \\hat{\\beta}_{\\text{A}} = 6 - 3 = 3$). Since both models produce the same fitted values for all observations, they are equivalent in terms of predictive power and goodness of fit (e.g., they have the same RSS). The parameterizations are different, but they describe the same underlying fit to the data.",
            "answer": "$$\\boxed{\\begin{pmatrix} 6 & -3 & -1 & 4 \\end{pmatrix}}$$"
        },
        {
            "introduction": "In a dataset, not all observations have an equal say in determining the regression line. This exercise explores the fundamental concepts of leverage, which measures a point's potential to influence the fit, and influence, which measures its actual impact. You will derive and implement a classic and computationally efficient formula for calculating leave-one-out cross-validation (LOOCV) predictions from a single model fit . This powerful technique will deepen your understanding of how individual data points shape the least squares solution and provide you with a tool for robust model diagnostics.",
            "id": "3138888",
            "problem": "You are given linear regression problems in which the target is to estimate regression coefficients by the method of ordinary least squares (OLS). Work from first principles of OLS and projection matrices to derive an implementable expression for the leave-one-out prediction for observation $i$ that uses only quantities from the full-data fit, without explicitly refitting the model $n$ times. Specifically, proceed from the definition of OLS as minimizing the sum of squared residuals, the normal equations, and the orthogonal projection onto the column space of the design matrix, and use well-tested linear algebra identities as needed. Do not rely on any pre-given shortcut formula.\n\nDefinitions and notation to use:\n- Let $X \\in \\mathbb{R}^{n \\times p}$ denote the design matrix with full column rank, and $y \\in \\mathbb{R}^{n}$ the response vector.\n- The OLS estimator $\\hat{\\beta} \\in \\mathbb{R}^{p}$ satisfies the normal equations $X^{\\top} X \\hat{\\beta} = X^{\\top} y$.\n- The fitted values are $\\hat{y} = X \\hat{\\beta}$ and the residuals are $r = y - \\hat{y}$.\n- The hat (projection) matrix is $H = X (X^{\\top} X)^{-1} X^{\\top}$, and the leverage of observation $i$ is $h_{ii} = H_{ii}$.\n- For leave-one-out cross-validation (LOOCV), the leave-one-out coefficient estimate is $\\hat{\\beta}^{(-i)}$ obtained by fitting OLS on all observations except $i$, and the corresponding leave-one-out prediction at the removed point is $\\hat{y}_{i}^{(-i)} = x_{i}^{\\top} \\hat{\\beta}^{(-i)}$, where $x_{i}^{\\top}$ is the $i$th row of $X$.\n\nTasks:\n1. Derive, from the definitions above and appropriate matrix identities, a closed-form expression for $\\hat{y}_{i}^{(-i)}$ expressed in terms of only full-fit quantities $r_{i}$ and $h_{ii}$, together with either $\\hat{y}_{i}$ or $y_{i}$. Your derivation must not assume any shortcut result as a starting point; it must follow from the normal equations and properties of projection matrices.\n2. Implement two computational procedures:\n   - A direct procedure that, for each $i \\in \\{1,\\dots,n\\}$, removes observation $i$, refits OLS on the remaining $n-1$ observations, and computes $\\hat{y}_{i}^{(-i)}$.\n   - A fast procedure that computes all $\\hat{y}_{i}^{(-i)}$ for $i \\in \\{1,\\dots,n\\}$ using only a single full-data OLS fit and your derived expression from Task $1$.\n3. Using these two procedures, evaluate the following test suite. For each case, compute the specified scalar output:\n   - Case A (happy path, exactly linear data):\n     - $X^{(A)} \\in \\mathbb{R}^{6 \\times 2}$ with rows $[1, x]$ for $x \\in \\{-2,-1,0,1,2,3\\}$, that is\n       $$X^{(A)} = \\begin{bmatrix}\n       1 & -2 \\\\\n       1 & -1 \\\\\n       1 & 0 \\\\\n       1 & 1 \\\\\n       1 & 2 \\\\\n       1 & 3\n       \\end{bmatrix}.$$\n     - $y^{(A)} \\in \\mathbb{R}^{6}$ given by $y^{(A)} = 1 + 2x$, i.e.,\n       $$y^{(A)} = \\begin{bmatrix} -3 \\\\ -1 \\\\ 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{bmatrix}.$$\n     - Output for Case A: the maximum absolute difference between the vector of leave-one-out predictions computed by the direct procedure and the vector computed by the fast procedure, that is\n       $$d^{(A)} = \\max_{1 \\le i \\le 6} \\left| \\hat{y}_{i,\\mathrm{direct}}^{(-i)} - \\hat{y}_{i,\\mathrm{fast}}^{(-i)} \\right|.$$\n   - Case B (general position, two predictors):\n     - $X^{(B)} \\in \\mathbb{R}^{8 \\times 3}$ with rows $[1, x_{1}, x_{2}]$:\n       $$X^{(B)} = \\begin{bmatrix}\n       1 & -1.0 & 2.0 \\\\\n       1 & -0.5 & -1.0 \\\\\n       1 & 0.0 & 0.5 \\\\\n       1 & 0.5 & -0.2 \\\\\n       1 & 1.0 & 1.5 \\\\\n       1 & 1.5 & -1.5 \\\\\n       1 & -1.5 & 1.0 \\\\\n       1 & 0.8 & 0.8\n       \\end{bmatrix}.$$\n     - $y^{(B)} \\in \\mathbb{R}^{8}$:\n       $$y^{(B)} = \\begin{bmatrix}\n       2.95 \\\\\n       0.78 \\\\\n       1.05 \\\\\n       -0.03 \\\\\n       0.21 \\\\\n       -1.83 \\\\\n       3.00 \\\\\n       0.13\n       \\end{bmatrix}.$$\n     - Output for Case B: the maximum absolute difference\n       $$d^{(B)} = \\max_{1 \\le i \\le 8} \\left| \\hat{y}_{i,\\mathrm{direct}}^{(-i)} - \\hat{y}_{i,\\mathrm{fast}}^{(-i)} \\right|.$$\n   - Case C (influence and leverage, one dramatically influential point):\n     - $X^{(C)} \\in \\mathbb{R}^{6 \\times 2}$ with rows $[1, x]$ for $x \\in \\{0.0, 0.1, -0.2, 0.3, -0.1, 20.0\\}$, that is\n       $$X^{(C)} = \\begin{bmatrix}\n       1 & 0.0 \\\\\n       1 & 0.1 \\\\\n       1 & -0.2 \\\\\n       1 & 0.3 \\\\\n       1 & -0.1 \\\\\n       1 & 20.0\n       \\end{bmatrix}.$$\n     - $y^{(C)} \\in \\mathbb{R}^{6}$ given by\n       $$y^{(C)} = \\begin{bmatrix}\n       3.02 \\\\\n       3.04 \\\\\n       2.90 \\\\\n       3.18 \\\\\n       2.93 \\\\\n       -50.0\n       \\end{bmatrix}.$$\n     - Let $\\hat{\\beta}^{\\mathrm{all}}$ be the OLS coefficients using all $6$ observations, and let $\\hat{\\beta}^{\\mathrm{omit}}$ be the OLS coefficients after removing the $6$th observation (the high-leverage outlier). Output for Case C the Euclidean norm of the coefficient shift,\n       $$d^{(C)} = \\left\\| \\hat{\\beta}^{\\mathrm{all}} - \\hat{\\beta}^{\\mathrm{omit}} \\right\\|_{2}.$$\n4. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[d^{(A)}, d^{(B)}, d^{(C)}]$.\n\nAll computations are purely mathematical; there are no physical units, and no angle units are involved. The outputs $d^{(A)}, d^{(B)}, d^{(C)}$ must be real numbers (floating-point values).",
            "solution": "The problem requires the derivation of a computationally efficient formula for leave-one-out cross-validation (LOOCV) predictions in ordinary least squares (OLS) regression, followed by a numerical implementation and verification.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Notation:**\n    - Design matrix: $X \\in \\mathbb{R}^{n \\times p}$, with full column rank.\n    - Response vector: $y \\in \\mathbb{R}^{n}$.\n    - OLS estimator: $\\hat{\\beta} \\in \\mathbb{R}^{p}$, satisfying the normal equations $X^\\top X \\hat{\\beta} = X^\\top y$.\n    - Fitted values: $\\hat{y} = X \\hat{\\beta}$.\n    - Residuals: $r = y - \\hat{y}$.\n    - Hat (projection) matrix: $H = X (X^\\top X)^{-1} X^\\top$.\n    - Leverage of observation $i$: $h_{ii} = H_{ii}$.\n    - Leave-one-out coefficient estimate: $\\hat{\\beta}^{(-i)}$, from OLS on all observations except $i$.\n    - Leave-one-out prediction: $\\hat{y}_{i}^{(-i)} = x_{i}^\\top \\hat{\\beta}^{(-i)}$, where $x_{i}^\\top$ is the $i$-th row of $X$.\n\n- **Tasks:**\n    1. Derive a closed-form expression for $\\hat{y}_{i}^{(-i)}$ using only full-fit quantities ($r_i$, $h_{ii}$, and $\\hat{y}_i$ or $y_i$).\n    2. Implement a direct procedure (refitting $n$ times) and a fast procedure (using the derived formula).\n    3. Evaluate three test cases and compute specified scalar outputs.\n\n- **Test Cases:**\n    - **Case A:**\n        - $n=6$, $p=2$.\n        - $X^{(A)} = \\begin{bmatrix} 1 & -2 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{bmatrix}$, $y^{(A)} = \\begin{bmatrix} -3 \\\\ -1 \\\\ 1 \\\\ 3 \\\\ 5 \\\\ 7 \\end{bmatrix}$.\n        - Output $d^{(A)} = \\max_{i} |\\hat{y}_{i,\\mathrm{direct}}^{(-i)} - \\hat{y}_{i,\\mathrm{fast}}^{(-i)}|$.\n    - **Case B:**\n        - $n=8$, $p=3$.\n        - $X^{(B)} = \\begin{bmatrix} 1 & -1.0 & 2.0 \\\\ 1 & -0.5 & -1.0 \\\\ 1 & 0.0 & 0.5 \\\\ 1 & 0.5 & -0.2 \\\\ 1 & 1.0 & 1.5 \\\\ 1 & 1.5 & -1.5 \\\\ 1 & -1.5 & 1.0 \\\\ 1 & 0.8 & 0.8 \\end{bmatrix}$, $y^{(B)} = \\begin{bmatrix} 2.95 \\\\ 0.78 \\\\ 1.05 \\\\ -0.03 \\\\ 0.21 \\\\ -1.83 \\\\ 3.00 \\\\ 0.13 \\end{bmatrix}$.\n        - Output $d^{(B)} = \\max_{i} |\\hat{y}_{i,\\mathrm{direct}}^{(-i)} - \\hat{y}_{i,\\mathrm{fast}}^{(-i)}|$.\n    - **Case C:**\n        - $n=6$, $p=2$.\n        - $X^{(C)} = \\begin{bmatrix} 1 & 0.0 \\\\ 1 & 0.1 \\\\ 1 & -0.2 \\\\ 1 & 0.3 \\\\ 1 & -0.1 \\\\ 1 & 20.0 \\end{bmatrix}$, $y^{(C)} = \\begin{bmatrix} 3.02 \\\\ 3.04 \\\\ 2.90 \\\\ 3.18 \\\\ 2.93 \\\\ -50.0 \\end{bmatrix}$.\n        - Output $d^{(C)} = \\| \\hat{\\beta}^{\\mathrm{all}} - \\hat{\\beta}^{\\mathrm{omit}} \\|_{2}$, where $\\hat{\\beta}^{\\mathrm{omit}}$ is the estimate with observation $6$ removed.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is assessed against the validation criteria.\n- **Scientifically Grounded:** The problem is rooted in the mathematical theory of linear statistical models, specifically ordinary least squares and cross-validation. All concepts are standard and well-established in statistics and linear algebra.\n- **Well-Posed:** The problem specifies that the design matrix $X$ has full column rank, which ensures that $(X^\\top X)$ is invertible and a unique OLS solution $\\hat{\\beta}$ exists. The leave-one-out subproblems are similarly well-posed. The tasks are specific and lead to uniquely defined numerical results.\n- **Objective:** The language is formal and precise. The data are provided explicitly. The required outputs are defined by objective mathematical formulas.\n- **Completeness and Consistency:** All necessary definitions, data, and constraints are provided. There are no contradictions.\n- **Feasibility:** The derivations and computations are standard and feasible using basic linear algebra operations.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is a well-posed, self-contained, and scientifically sound exercise in statistical computation. The solution process may proceed.\n\n### Solution and Derivations\n\n**Task 1: Derivation of the Fast LOOCV Formula**\n\nThe objective is to find an expression for the leave-one-out prediction $\\hat{y}_{i}^{(-i)} = x_{i}^\\top \\hat{\\beta}^{(-i)}$ without explicitly re-computing the OLS fit for each omitted observation $i$.\n\nLet $X^{(-i)}$ and $y^{(-i)}$ denote the design matrix and response vector with the $i$-th observation $(x_i^\\top, y_i)$ removed. The OLS estimate $\\hat{\\beta}^{(-i)}$ is the solution to the normal equations for this reduced dataset:\n$$(X^{(-i)})^\\top X^{(-i)} \\hat{\\beta}^{(-i)} = (X^{(-i)})^\\top y^{(-i)}$$\n\nThe matrices and vectors for the reduced dataset can be related to the full dataset. The matrix $X^\\top X$ is a sum of outer products: $X^\\top X = \\sum_{j=1}^{n} x_j x_j^\\top$. Removing the $i$-th observation corresponds to removing the $i$-th term from this sum.\n$$(X^{(-i)})^\\top X^{(-i)} = \\sum_{j \\neq i} x_j x_j^\\top = (X^\\top X) - x_i x_i^\\top$$\nSimilarly, for the right-hand side of the normal equations:\n$$(X^{(-i)})^\\top y^{(-i)} = \\sum_{j \\neq i} x_j y_j = (X^\\top y) - x_i y_i$$\n\nSubstituting these into the normal equations for $\\hat{\\beta}^{(-i)}$ yields:\n$$((X^\\top X) - x_i x_i^\\top) \\hat{\\beta}^{(-i)} = (X^\\top y) - x_i y_i$$\n\nTo solve for $\\hat{\\beta}^{(-i)}$, we need the inverse of the matrix $((X^\\top X) - x_i x_i^\\top)$. This matrix is a rank-$1$ update of the matrix $X^\\top X$. We can apply the Sherman-Morrison formula, which states that for an invertible matrix $A$ and vectors $u, v$:\n$$(A - uv^\\top)^{-1} = A^{-1} + \\frac{A^{-1}uv^\\top A^{-1}}{1 - v^\\top A^{-1}u}$$\nIn our case, $A = X^\\top X$ and $u=v=x_i$. The formula becomes:\n$$((X^\\top X) - x_i x_i^\\top)^{-1} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1}x_i x_i^\\top(X^\\top X)^{-1}}{1 - x_i^\\top(X^\\top X)^{-1}x_i}$$\nThe term in the denominator is the leverage of observation $i$, $h_{ii} = x_i^\\top(X^\\top X)^{-1}x_i$. This is the $i$-th diagonal element of the hat matrix $H = X(X^\\top X)^{-1}X^\\top$. The formula simplifies to:\n$$((X^\\top X) - x_i x_i^\\top)^{-1} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1}x_i x_i^\\top(X^\\top X)^{-1}}{1 - h_{ii}}$$\n\nNow, we can express $\\hat{\\beta}^{(-i)}$:\n$$\\hat{\\beta}^{(-i)} = \\left((X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1}x_i x_i^\\top(X^\\top X)^{-1}}{1 - h_{ii}}\\right) ((X^\\top y) - x_i y_i)$$\nLet's expand this expression. Let $\\hat{\\beta} = (X^\\top X)^{-1}X^\\top y$ be the full-data OLS estimate.\n$$\\hat{\\beta}^{(-i)} = (X^\\top X)^{-1}(X^\\top y) - (X^\\top X)^{-1}x_i y_i + \\frac{(X^\\top X)^{-1}x_i x_i^\\top(X^\\top X)^{-1}(X^\\top y)}{1 - h_{ii}} - \\frac{(X^\\top X)^{-1}x_i x_i^\\top(X^\\top X)^{-1}x_i y_i}{1 - h_{ii}}$$\nRecognizing terms:\n- $(X^\\top X)^{-1}(X^\\top y) = \\hat{\\beta}$\n- $x_i^\\top(X^\\top X)^{-1}(X^\\top y) = x_i^\\top\\hat{\\beta} = \\hat{y}_i$\n- $x_i^\\top(X^\\top X)^{-1}x_i = h_{ii}$\nSubstituting these yields:\n$$\\hat{\\beta}^{(-i)} = \\hat{\\beta} - (X^\\top X)^{-1}x_i y_i + \\frac{(X^\\top X)^{-1}x_i \\hat{y}_i}{1 - h_{ii}} - \\frac{(X^\\top X)^{-1}x_i h_{ii} y_i}{1 - h_{ii}}$$\nCombining terms involving $(X^\\top X)^{-1}x_i$:\n$$\\hat{\\beta}^{(-i)} = \\hat{\\beta} + (X^\\top X)^{-1}x_i \\left( -y_i + \\frac{\\hat{y}_i - h_{ii}y_i}{1 - h_{ii}} \\right)$$\nSimplifying the expression in the parenthesis:\n$$\\frac{-y_i(1-h_{ii}) + \\hat{y}_i - h_{ii}y_i}{1-h_{ii}} = \\frac{-y_i + y_i h_{ii} + \\hat{y}_i - h_{ii}y_i}{1-h_{ii}} = \\frac{\\hat{y}_i - y_i}{1-h_{ii}}$$\nThe numerator $\\hat{y}_i - y_i$ is the negative of the $i$-th residual, $-r_i$. Therefore,\n$$\\hat{\\beta}^{(-i)} = \\hat{\\beta} - \\frac{(X^\\top X)^{-1}x_i r_i}{1 - h_{ii}}$$\nThis expression relates the leave-one-out coefficient vector $\\hat{\\beta}^{(-i)}$ to the full-data estimate $\\hat{\\beta}$ and other full-data quantities.\n\nFinally, we compute the leave-one-out prediction $\\hat{y}_{i}^{(-i)} = x_i^\\top\\hat{\\beta}^{(-i)}$:\n$$\\hat{y}_{i}^{(-i)} = x_i^\\top\\left(\\hat{\\beta} - \\frac{(X^\\top X)^{-1}x_i r_i}{1 - h_{ii}}\\right) = x_i^\\top\\hat{\\beta} - \\frac{x_i^\\top(X^\\top X)^{-1}x_i r_i}{1 - h_{ii}}$$\nSubstituting $\\hat{y}_i = x_i^\\top\\hat{\\beta}$ and $h_{ii} = x_i^\\top(X^\\top X)^{-1}x_i$:\n$$\\hat{y}_{i}^{(-i)} = \\hat{y}_i - \\frac{h_{ii}r_i}{1 - h_{ii}}$$\nThis is the desired closed-form expression. It allows computation of $\\hat{y}_{i}^{(-i)}$ using only the full-data fitted value $\\hat{y}_i$, residual $r_i$, and leverage $h_{ii}$. An alternative form, using $r_i = y_i - \\hat{y}_i$, is often used for the so-called PRESS (Predicted Residual Sum of Squares) statistic. The PRESS residual is $e_{i, \\text{PRESS}} = y_i - \\hat{y}_i^{(-i)} = y_i - (\\hat{y}_i - \\frac{h_{ii}r_i}{1 - h_{ii}}) = (y_i - \\hat{y}_i) + \\frac{h_{ii}r_i}{1 - h_{ii}} = r_i + \\frac{h_{ii}r_i}{1 - h_{ii}} = \\frac{r_i(1-h_{ii}) + h_{ii}r_i}{1 - h_{ii}} = \\frac{r_i}{1 - h_{ii}}$. From this, we can also write $\\hat{y}_i^{(-i)} = y_i - \\frac{r_i}{1-h_{ii}}$. Both forms are equivalent and suitable for implementation.\n\n**Task 2 & 3: Implementation and Evaluation**\nThe implementation will consist of two primary procedures.\n1.  **Direct Procedure:** This procedure will iterate from $i=1$ to $n$. In each iteration, it will construct the reduced dataset $(X^{(-i)}, y^{(-i)})$ by deleting the $i$-th row from $X$ and $y$. It will then solve the normal equations for this reduced set to find $\\hat{\\beta}^{(-i)}$ and compute the prediction $\\hat{y}_{i}^{(-i)} = x_i^\\top\\hat{\\beta}^{(-i)}$. This method is computationally intensive, requiring $n$ separate OLS fits.\n2.  **Fast Procedure:** This procedure will first compute a single OLS fit on the full dataset $(X,y)$ to obtain $\\hat{\\beta}$, $\\hat{y}$, and $r$. It will also compute the hat matrix $H = X(X^\\top X)^{-1}X^\\top$ and extract its diagonal elements, the leverages $h_{ii}$. Then, for each $i \\in \\{1, \\dots, n\\}$, it will apply the derived formula $\\hat{y}_{i}^{(-i)} = \\hat{y}_i - \\frac{h_{ii}r_i}{1 - h_{ii}}$ to compute all leave-one-out predictions simultaneously. This is highly efficient.\n\nThese procedures will be applied to the three specified test cases.\n- For Cases A and B, the two vectors of leave-one-out predictions are computed and the maximum absolute difference between them is found. This difference should be close to floating-point precision error, validating the derived formula. For Case A, the data are perfectly linear, so all residuals $r_i$ from the full fit will be zero, leading to $\\hat{y}_{i}^{(-i)} = \\hat{y}_i = y_i$. The direct method should yield the same result.\n- For Case C, the task is to compute the Euclidean norm of the shift in the coefficient vector when the highly influential $6$-th point is removed. This involves computing $\\hat{\\beta}^{\\mathrm{all}}$ using all $6$ points, computing $\\hat{\\beta}^{\\mathrm{omit}}$ using the first $5$ points, and then calculating $\\|\\hat{\\beta}^{\\mathrm{all}} - \\hat{\\beta}^{\\mathrm{omit}}\\|_2$.",
            "answer": "```python\nimport numpy as np\n\ndef direct_loocv(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes leave-one-out predictions by explicitly refitting the model n times.\n    \n    Args:\n        X: Design matrix of shape (n, p).\n        y: Response vector of shape (n,).\n        \n    Returns:\n        A vector of leave-one-out predictions of shape (n,).\n    \"\"\"\n    n, _ = X.shape\n    y_pred_loocv = np.zeros(n)\n    \n    for i in range(n):\n        # Create the dataset with the i-th observation removed\n        X_minus_i = np.delete(X, i, axis=0)\n        y_minus_i = np.delete(y, i, axis=0)\n        \n        # Solve the normal equations for the reduced dataset\n        XTX_minus_i = X_minus_i.T @ X_minus_i\n        XTy_minus_i = X_minus_i.T @ y_minus_i\n        \n        # Using np.linalg.solve for numerical stability and correctness\n        beta_minus_i = np.linalg.solve(XTX_minus_i, XTy_minus_i)\n        \n        # Predict the value for the removed observation\n        x_i = X[i, :]\n        y_pred_loocv[i] = x_i @ beta_minus_i\n        \n    return y_pred_loocv\n\ndef fast_loocv(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes leave-one-out predictions using the efficient closed-form formula.\n    \n    Args:\n        X: Design matrix of shape (n, p).\n        y: Response vector of shape (n,).\n        \n    Returns:\n        A vector of leave-one-out predictions of shape (n,).\n    \"\"\"\n    # 1. Perform a single full OLS fit\n    XTX = X.T @ X\n    XTy = X.T @ y\n    beta_hat = np.linalg.solve(XTX, XTy)\n    \n    # 2. Compute full-fit quantities\n    y_hat = X @ beta_hat\n    residuals = y - y_hat\n    \n    # 3. Compute leverages h_ii\n    # H = X @ inv(X'X) @ X'\n    XTX_inv = np.linalg.inv(XTX)\n    H = X @ XTX_inv @ X.T\n    h = np.diag(H)\n    \n    # Check for h_ii = 1 which makes the formula undefined.\n    # This happens for extreme leverage points. A robust implementation\n    # might handle this, but for this problem, we assume h_ii < 1.\n    if np.any(np.isclose(h, 1.0)):\n        # Such points perfectly determine their own fitted value.\n        # The LOOCV prediction for them is undefined or handled as a special case.\n        # For this problem's scope, we proceed assuming h_ii < 1.\n        pass\n\n    # 4. Apply the derived formula\n    # y_hat_loo_i = y_hat_i - (h_ii * r_i) / (1 - h_ii)\n    # This can be vectorized.\n    y_pred_loocv = y_hat - (h * residuals) / (1 - h)\n    \n    return y_pred_loocv\n\ndef solve():\n    \"\"\"\n    Main function to execute the tasks for all test cases.\n    \"\"\"\n    \n    # Case A: Happy path, exactly linear data\n    X_A = np.array([\n        [1., -2.], [1., -1.], [1., 0.], [1., 1.], [1., 2.], [1., 3.]\n    ])\n    y_A = np.array([-3., -1., 1., 3., 5., 7.])\n\n    # Case B: General position, two predictors\n    X_B = np.array([\n        [1., -1.0, 2.0], [1., -0.5, -1.0], [1., 0.0, 0.5], [1., 0.5, -0.2],\n        [1., 1.0, 1.5], [1., 1.5, -1.5], [1., -1.5, 1.0], [1., 0.8, 0.8]\n    ])\n    y_B = np.array([2.95, 0.78, 1.05, -0.03, 0.21, -1.83, 3.00, 0.13])\n\n    # Case C: Influence and leverage\n    X_C = np.array([\n        [1., 0.0], [1., 0.1], [1., -0.2], [1., 0.3], [1., -0.1], [1., 20.0]\n    ])\n    y_C = np.array([3.02, 3.04, 2.90, 3.18, 2.93, -50.0])\n\n    test_cases = [\n        {'id': 'A', 'X': X_A, 'y': y_A},\n        {'id': 'B', 'X': X_B, 'y': y_B},\n        {'id': 'C', 'X': X_C, 'y': y_C}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X, y = case['X'], case['y']\n        \n        if case['id'] in ['A', 'B']:\n            y_pred_direct = direct_loocv(X, y)\n            y_pred_fast = fast_loocv(X, y)\n            d = np.max(np.abs(y_pred_direct - y_pred_fast))\n            results.append(d)\n\n        elif case['id'] == 'C':\n            # Compute beta_all using all 6 observations\n            beta_all = np.linalg.solve(X.T @ X, X.T @ y)\n\n            # Compute beta_omit by removing the 6th observation (index 5)\n            X_omit = np.delete(X, 5, axis=0)\n            y_omit = np.delete(y, 5, axis=0)\n            beta_omit = np.linalg.solve(X_omit.T @ X_omit, X_omit.T @ y_omit)\n            \n            # Compute the Euclidean norm of the difference\n            d = np.linalg.norm(beta_all - beta_omit)\n            results.append(d)\n            \n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}