{
    "hands_on_practices": [
        {
            "introduction": "拟合回归模型是数据分析的第一步，但理解其适用范围同样至关重要。一个在训练数据上看起来表现完美的模型，在应用于训练数据范围之外的新数据时，其预测能力可能会急剧下降。这种现象被称为外推（extrapolation），当真实关系并非严格线性时，其风险尤其突出。本练习  将通过具体案例，让您亲手揭示模型内插（interpolation）与外推之间的巨大性能差异，从而深刻理解最小二乘法应用的边界和潜在陷阱。",
            "id": "3138831",
            "problem": "考虑一个一维监督学习问题，其中输入是标量 $x$，响应是标量 $y$。你将通过普通最小二乘法 (Ordinary Least Squares, OLS) 来估计回归系数，该方法选择的系数能够最小化残差平方和 (Residual Sum of Squares, RSS)。这个问题的基础是平方误差损失的定义和经验风险最小化的概念：给定训练样本对 $\\{(x_i, y_i)\\}_{i=1}^n$，定义线性预测器 $f(x) = \\beta_0 + \\beta_1 x$，并选择系数来最小化训练集上的 RSS，即残差的平方和。\n\n你的程序必须基于第一性原理实现以下任务：\n\n1. 根据指定的数据生成过程生成训练数据 $(x_i, y_i)$，其中 $y_i = f_{\\text{true}}(x_i) + \\varepsilon_i$，$\\varepsilon_i$ 是从均值为零、标准差指定的正态分布中独立抽取的样本。训练输入必须位于一个指定的区间内，该区间与测试输入区间不重叠。\n\n2. 通过最小化训练集上的 RSS 拟合一个带截距的线性模型，以获得系数 $(\\hat{\\beta}_0, \\hat{\\beta}_1)$。\n\n3. 计算训练均方误差 (Mean Squared Error, MSE)，其定义为训练数据上残差平方的均值；并计算测试均方误差 (MSE)，该误差是在从指定测试区间抽取的、使用无噪声的真实数据生成函数 $f_{\\text{true}}(x)$ 生成的无噪声测试集上计算的。\n\n4. 对每个测试用例，输出比率 $r = \\text{MSE}_{\\text{test}} / \\text{MSE}_{\\text{train}}$，形式为浮点数。该比率量化了外推性能与样本内拟合的对比情况。将每个比率表示为四舍五入到 $6$ 位小数的十进制数。\n\n你不能使用题目陈述中任何预先提供的回归公式；相反，你的实现必须基于最小化平方误差的原理。如果设计矩阵是秩亏的，使用一个能达到最小训练 RSS 的、有原则的最小二乘解。\n\n在首次使用时定义以下缩写词：\n- 普通最小二乘法 (OLS)：一种选择系数以最小化残差平方和 (RSS) 的方法。\n- 残差平方和 (RSS)：训练数据上残差的平方和。\n- 均方误差 (MSE)：一个数据集上残差平方的平均值。\n\n测试套件规范：\n- 用例 $1$（理想情况，真实关系为线性且外推可靠）：\n  - 训练输入：在 $[-1, 1]$ 上均匀分布的 $n_{\\text{train}} = 50$ 个点。\n  - 测试输入：在 $[2, 3]$ 上均匀分布的 $n_{\\text{test}} = 50$ 个点。\n  - 真实函数：$f_{\\text{true}}(x) = 2x + 1$。\n  - 噪声标准差：$\\sigma = 0.05$。\n  - 噪声的随机种子：种子 $0$。\n\n- 用例 $2$（在小范围训练支持集上的非线性真实关系，尽管训练 RSS 很小，但外推性能差）：\n  - 训练输入：在 $[-0.5, 0.5]$ 上均匀分布的 $n_{\\text{train}} = 50$ 个点。\n  - 测试输入：在 $[2, 3]$ 上均匀分布的 $n_{\\text{test}} = 50$ 个点。\n  - 真实函数：$f_{\\text{true}}(x) = x^3$。\n  - 噪声标准差：$\\sigma = 0.02$。\n  - 噪声的随机种子：种子 $1$。\n\n- 用例 $3$（秩亏设计的边界情况，外推实际上变为常数）：\n  - 训练输入：$n_{\\text{train}} = 30$ 个点，全部位于 $x = 0$。\n  - 测试输入：在 $[1, 2]$ 上均匀分布的 $n_{\\text{test}} = 30$ 个点。\n  - 真实函数：$f_{\\text{true}}(x) = 2x + 1$。\n  - 噪声标准差：$\\sigma = 0.05$。\n  - 噪声的随机种子：种子 $2$。\n\n算法要求：\n- 构建设计矩阵 $X$，其中包含一列全为 1 的列和一列训练输入。\n- 最小化训练 RSS 以获得 $(\\hat{\\beta}_0, \\hat{\\beta}_1)$。如果正规方程组所引出的矩阵不可逆，则使用能够最小化训练 RSS 的 Moore-Penrose 伪逆解。\n- 按照规定计算 $\\text{MSE}_{\\text{train}}$ 和 $\\text{MSE}_{\\text{test}}$，其中对于大小为 $m$ 的数据集，$\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，即三个比率的列表 $[r_1, r_2, r_3]$，四舍五入到 $6$ 位小数，其中 $r_k$ 对应于上述顺序中的用例 $k$。",
            "solution": "该问题要求基于普通最小二乘法 (Ordinary Least Squares, OLS) 的原理实现并评估一个线性回归模型。OLS 是一种通过选择回归系数来最小化残差平方和 (Residual Sum of Squares, RSS) 的方法。RSS 定义为观测响应与线性模型预测值之间差异的平方和。我们将通过比较模型的样本内拟合与其外推能力来分析其性能。\n\n待拟合的模型是一个简单线性回归模型，形式为 $f(x) = \\beta_0 + \\beta_1 x$，其中 $\\beta_0$ 是截距，$\\beta_1$ 是斜率。给定一组训练数据点 $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$，目标是找到系数向量 $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1]^T$，以最小化训练集上的 RSS：\n$$\n\\text{RSS}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n_{\\text{train}}} (y_i - f(x_i))^2 = \\sum_{i=1}^{n_{\\text{train}}} (y_i - (\\beta_0 + \\beta_1 x_i))^2\n$$\n\n使用线性代数可以最优雅地表述这个问题。我们可以将所有训练点的线性方程组表示为矩阵形式。令 $\\mathbf{y}$ 为观测响应向量，$X$ 为设计矩阵，$\\boldsymbol{\\beta}$ 为系数向量，$\\boldsymbol{\\varepsilon}$ 为误差向量。模型为 $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$。\n\n对于一个简单线性模型，设计矩阵 $X$ 是通过将一列全为 1 的列（用于截距 $\\beta_0$）附加到输入值向量 $\\mathbf{x}_{\\text{train}}$ 上来构建的：\n$$\nX = \\begin{pmatrix}\n1  x_1 \\\\\n1  x_2 \\\\\n\\vdots  \\vdots \\\\\n1  x_{n_{\\text{train}}}\n\\end{pmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{pmatrix}, \\quad\n\\mathbf{y} = \\begin{pmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_{n_{\\text{train}}}\n\\end{pmatrix}\n$$\n\nRSS 随后可以写为残差向量 $\\mathbf{r} = \\mathbf{y} - X\\boldsymbol{\\beta}$ 的欧几里得范数的平方：\n$$\n\\text{RSS}(\\boldsymbol{\\beta}) = \\|\\mathbf{y} - X\\boldsymbol{\\beta}\\|^2_2\n$$\n为了找到最小化该量的系数 $\\hat{\\boldsymbol{\\beta}}$，我们对 $\\boldsymbol{\\beta}$ 求梯度并将其设为零，从而得到正规方程组：\n$$\n(X^T X) \\hat{\\boldsymbol{\\beta}} = X^T \\mathbf{y}\n$$\n如果矩阵 $X^T X$ 可逆，唯一的 OLS 解为：\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^T X)^{-1} X^T \\mathbf{y}\n$$\n然而，如果 $X^T X$ 是奇异的（即不可逆），这种情况发生在 $X$ 的列线性相关时，则有无穷多个解 $\\hat{\\boldsymbol{\\beta}}$ 可以最小化 RSS。这在问题的用例 3 中发生，其中所有训练输入 $x_i$ 都相同，导致 $X$ 的第二列是第一列的倍数。在这种情况下，通过找到具有最小欧几里得范数 $\\|\\boldsymbol{\\beta}\\|_2$ 的解来选择一个唯一的解。这个解由 Moore-Penrose 伪逆给出，用上标 `+` 表示：\n$$\n\\hat{\\boldsymbol{\\beta}} = X^+ \\mathbf{y}\n$$\n其中 $X^+ = (X^T X)^+ X^T$。这个公式为可逆和奇异情况都提供了最小二乘解，因此是我们即将实现的一般方法。\n\n一旦估计出系数 $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1]^T$，我们就可以对任何给定的输入 $x$ 使用 $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$ 进行预测。然后我们使用均方误差 (Mean Squared Error, MSE) 来评估模型的性能，其定义为在大小为 $m$ 的数据集上残差平方的平均值：$\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$。\n\n我们将计算两个 MSE 值：\n$1$. 训练 MSE，$\\text{MSE}_{\\text{train}}$，是在训练数据 $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$ 上计算的。\n$2$. 测试 MSE，$\\text{MSE}_{\\text{test}}$，是在一个无噪声的测试集上计算的。该测试集由输入 $x_{\\text{test}, j}$ 和真实的、无噪声的响应 $y_{\\text{test}, j} = f_{\\text{true}}(x_{\\text{test}, j})$ 组成。\n\n每个用例的最后一步是计算比率 $r = \\text{MSE}_{\\text{test}} / \\text{MSE}_{\\text{train}}$，它量化了在训练数据域之外进行外推时性能下降的程度。\n\n**用例 1：** 在 $[-1, 1]$ 上训练，在 $[2, 3]$ 上测试。真实函数是线性的：$f_{\\text{true}}(x) = 2x + 1$。线性模型被正确设定。我们期望估计出的系数接近 $(\\beta_0, \\beta_1) = (1, 2)$。由于模型是正确的，它应该能够很好地外推，从而导致比率 $r_1$ 接近 $1$。\n\n**用例 2：** 在 $[-0.5, 0.5]$ 上训练，在 $[2, 3]$ 上测试。真实函数是三次函数：$f_{\\text{true}}(x) = x^3$。线性模型被错误设定。虽然它可能在狭窄的训练区间内提供合理的拟合，但其线性投影将在外推区域与三次曲线急剧偏离。这将导致一个非常大的 $\\text{MSE}_{\\text{test}}$，从而得到一个很大的比率 $r_2$。\n\n**用例 3：** 对所有 $i$ 都在 $x_i=0$ 上训练。设计矩阵 $X$ 有一列全为 1 和一列全为 0，使其成为秩亏矩阵。伪逆解将正确地估计 $\\hat{\\beta}_0$ 为训练响应的均值，并设置 $\\hat{\\beta}_1 = 0$（最小范数选择）。得到的模型是一个常数函数 $\\hat{y} = \\hat{\\beta}_0$。当使用这个常数模型来预测在 $[1, 2]$ 区间上的测试集的值时（真实函数为 $f_{\\text{true}}(x) = 2x + 1$），将会产生很大的测试误差。这将产生一个很大的比率 $r_3$。\n\n实现将遵循这些原则，使用 `numpy` 进行数值计算，包括使用伪逆来稳健地解决所有用例中的最小二乘问题。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates an Ordinary Least Squares (OLS) regression model\n    for three distinct test cases, computing the ratio of test MSE to training MSE.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Happy path, linear truth and reliable extrapolation\n        {\n            \"n_train\": 50,\n            \"train_x_spec\": {\"type\": \"uniform\", \"range\": [-1, 1]},\n            \"n_test\": 50,\n            \"test_x_spec\": {\"type\": \"uniform\", \"range\": [2, 3]},\n            \"f_true\": lambda x: 2 * x + 1,\n            \"noise_sigma\": 0.05,\n            \"seed\": 0,\n        },\n        # Case 2: Nonlinear truth, poor extrapolation\n        {\n            \"n_train\": 50,\n            \"train_x_spec\": {\"type\": \"uniform\", \"range\": [-0.5, 0.5]},\n            \"n_test\": 50,\n            \"test_x_spec\": {\"type\": \"uniform\", \"range\": [2, 3]},\n            \"f_true\": lambda x: x**3,\n            \"noise_sigma\": 0.02,\n            \"seed\": 1,\n        },\n        # Case 3: Rank-deficient design matrix\n        {\n            \"n_train\": 30,\n            \"train_x_spec\": {\"type\": \"constant\", \"value\": 0},\n            \"n_test\": 30,\n            \"test_x_spec\": {\"type\": \"uniform\", \"range\": [1, 2]},\n            \"f_true\": lambda x: 2 * x + 1,\n            \"noise_sigma\": 0.05,\n            \"seed\": 2,\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        #\n        # 1. Generate Training and Test Data\n        #\n        rng = np.random.default_rng(case[\"seed\"])\n\n        # Training data\n        n_train = case[\"n_train\"]\n        train_spec = case[\"train_x_spec\"]\n        if train_spec[\"type\"] == \"uniform\":\n            x_train = np.linspace(train_spec[\"range\"][0], train_spec[\"range\"][1], n_train)\n        elif train_spec[\"type\"] == \"constant\":\n            x_train = np.full(n_train, train_spec[\"value\"])\n        \n        y_true_train = case[\"f_true\"](x_train)\n        noise = rng.normal(0, case[\"noise_sigma\"], n_train)\n        y_train = y_true_train + noise\n\n        # Test data (noise-free)\n        n_test = case[\"n_test\"]\n        test_spec = case[\"test_x_spec\"]\n        if test_spec[\"type\"] == \"uniform\":\n            x_test = np.linspace(test_spec[\"range\"][0], test_spec[\"range\"][1], n_test)\n        \n        y_test = case[\"f_true\"](x_test)\n\n        #\n        # 2. Fit Linear Model using OLS from first principles\n        #\n        # Construct the design matrix X for training data\n        X_train = np.vstack([np.ones(n_train), x_train]).T\n        \n        # Solve for beta_hat using the Moore-Penrose pseudoinverse.\n        # This is equivalent to beta_hat = (X.T @ X)^+ @ X.T @ y\n        # and correctly handles rank-deficient cases.\n        beta_hat = np.linalg.pinv(X_train) @ y_train\n\n        #\n        # 3. Compute Training and Test MSE\n        #\n        \n        # Training MSE\n        # Predictions: y_hat = X @ beta_hat\n        y_hat_train = X_train @ beta_hat\n        \n        # Residuals: y - y_hat\n        residuals_train = y_train - y_hat_train\n        \n        # MSE = mean of squared residuals\n        mse_train = np.mean(residuals_train**2)\n\n        # Test MSE\n        # Construct the design matrix for test data\n        X_test = np.vstack([np.ones(n_test), x_test]).T\n        \n        # Predictions on test data using the fitted model\n        y_hat_test = X_test @ beta_hat\n        \n        # Residuals against the true (noise-free) test values\n        residuals_test = y_test - y_hat_test\n        \n        # MSE\n        mse_test = np.mean(residuals_test**2)\n\n        #\n        # 4. Compute the Ratio\n        #\n        ratio = mse_test / mse_train\n        results.append(round(ratio, 6))\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在真实世界的数据中，我们经常需要处理分类型预测变量，例如品牌、地区或物种。将这些非数值信息纳入线性模型需要一种被称为“编码”的特殊处理。然而，一种看似直观的编码方法——为每个类别创建一个指示变量（one-hot encoding）——会引入“多重共线性”问题，导致模型系数无法唯一确定。本练习  旨在阐明这一核心问题，并引导您通过施加约束或使用参考编码（reference coding）等原则性方法，来获得一个可解释且稳健的模型。",
            "id": "3138898",
            "problem": "一个具有三个水平的分类预测变量被用来解释一个实值响应。考虑以下具有分类水平 $\\text{A}$、$\\text{B}$ 和 $\\text{C}$ 的数据：\n- A 组：响应 $y$ 等于 $2$ 和 $4$。\n- B 组：响应 $y$ 等于 $3$、$5$ 和 $7$。\n- C 组：响应 $y$ 等于 $10$。\n\n我们尝试一个带有截距和包含所有三个水平指示符的独热编码（one-hot encoding）的线性模型：\n$$\ny_{i} = \\beta_{0} + \\beta_{\\text{A}} \\,\\mathbf{1}\\{g_{i}=\\text{A}\\} + \\beta_{\\text{B}} \\,\\mathbf{1}\\{g_{i}=\\text{B}\\} + \\beta_{\\text{C}} \\,\\mathbf{1}\\{g_{i}=\\text{C}\\} + \\varepsilon_{i},\n$$\n其中 $g_{i}$ 表示观测值 $i$ 的组别，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n从普通最小二乘法（OLS）的定义出发，该方法通过最小化残差平方和来选择系数，解释为什么在这个完全独热编码模型中系数不是唯一可识别的。然后，施加可识别性约束\n$$\n\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}} = 0\n$$\n并从第一性原理出发，推导出封闭形式的约束最小二乘估计。计算给定数据的这些数值。\n\n最后，简要解释采用参考编码（reference coding），即去掉水平 $\\text{C}$，如何得到一个等价的拟合模型，并说明所得系数代表什么，过程中不使用任何快捷公式。\n\n将您的最终答案报告为约束估计的向量 $(\\beta_{0}, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}})$，以精确形式表示。无需四舍五入。",
            "solution": "该问题要求分析一个具有三水平分类预测变量的线性模型，首先解释完全独热编码模型的不可识别性，然后推导并计算约束最小二乘估计。最后，必须解释一个等价的参考编码模型。\n\n首先，我们进行问题验证。\n\n### 步骤 1：提取已知条件\n- 数据：\n    - A 组：响应 $y$ 为 $2$ 和 $4$。观测数量为 $n_{\\text{A}}=2$。\n    - B 组：响应 $y$ 为 $3$、$5$ 和 $7$。观测数量为 $n_{\\text{B}}=3$。\n    - C 组：响应 $y$ 为 $10$。观测数量为 $n_{\\text{C}}=1$。\n- 模型（完全独热编码）：\n$$\ny_{i} = \\beta_{0} + \\beta_{\\text{A}} \\,\\mathbf{1}\\{g_{i}=\\text{A}\\} + \\beta_{\\text{B}} \\,\\mathbf{1}\\{g_{i}=\\text{B}\\} + \\beta_{\\text{C}} \\,\\mathbf{1}\\{g_{i}=\\text{C}\\} + \\varepsilon_{i}\n$$\n- 可识别性约束：\n$$\n\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}} = 0\n$$\n- 任务：\n    1. 解释为什么在完全独热编码模型中系数不是唯一可识别的。\n    2. 从第一性原理推导约束最小二乘估计。\n    3. 计算约束估计的数值。\n    4. 解释去掉水平 $\\text{C}$ 后的等价参考编码模型。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题在科学上基于线性模型和普通最小二乘法（OLS）的理论，这些都是统计学的核心概念。由分类预测变量模型中的多重共线性（multicollinearity）引起的不可识别性问题是一个标准课题。使用和为零的约束来实现可识别性是一种有效且常用的技术。该问题提法得当，提供了所有必要的数据和一组明确的目标。语言精确客观。没有科学或事实上的不健全之处，没有信息缺失，并且设置并非自相矛盾或不切实际。因此，该问题是有效的。\n\n### 步骤 3：结论与行动\n该问题有效。我们继续进行解答。\n\n#### 完全模型的不可识别性\n\n指定的线性模型可以表示为矩阵形式 $Y = X\\beta + \\varepsilon$。参数向量为 $\\beta = [\\beta_0, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}}]^T$。设计矩阵 $X$ 的列对应于截距以及 A、B、C 三组的指示变量。设这些列为 $X_0, X_{\\text{A}}, X_{\\text{B}}, X_{\\text{C}}$。\n- $X_0$ 是一个全为1的列，代表截距 $\\beta_0$。\n- $X_{\\text{A}}$ 是一个列，对于 A 组的观测值为 $1$，否则为 $0$。\n- $X_{\\text{B}}$ 是一个列，对于 B 组的观测值为 $1$，否则为 $0$。\n- $X_{\\text{C}}$ 是一个列，对于 C 组的观测值为 $1$，否则为 $0$。\n\n对于任何观测值 $i$，其对应的指示变量满足属性 $\\mathbf{1}\\{g_{i}=\\text{A}\\} + \\mathbf{1}\\{g_{i}=\\text{B}\\} + \\mathbf{1}\\{g_{i}=\\text{C}\\} = 1$。这意味着对应于分类水平的列之和等于截距列：$X_{\\text{A}} + X_{\\text{B}} + X_{\\text{C}} = X_0$。这表明设计矩阵 $X$ 的列之间存在完全的线性相关性，即多重共线性。\n\n$\\beta$ 的 OLS 估计由 $\\hat{\\beta} = (X^T X)^{-1} X^T Y$ 给出。因为 $X$ 的列是线性相关的，所以矩阵 $X^T X$ 是奇异的（不可逆）。因此，其逆矩阵 $(X^T X)^{-1}$ 不存在，$\\hat{\\beta}$ 没有唯一解。\n\n或者，考虑一组最小化残差平方和的系数 $(\\beta_0, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}})$。对于任意常数 $c$，考虑一组新的系数 $(\\beta_0 + c, \\beta_{\\text{A}} - c, \\beta_{\\text{B}} - c, \\beta_{\\text{C}} - c)$。A 组中一个观测值的预测值为 $(\\beta_0 + c) + (\\beta_{\\text{A}} - c) = \\beta_0 + \\beta_{\\text{A}}$，保持不变。同样，B 组和 C 组的预测值也保持不变。由于对于任何 $c$ 的选择，预测值都是相同的，所以残差平方和也相同。这意味着存在无限多组解，因此系数不是唯一可识别的。\n\n#### 约束最小二乘估计\n\n为了在给定约束下找到唯一估计，我们最小化受该约束限制的残差平方和（RSS）。目标函数是 RSS：\n$$\n\\text{RSS} = \\sum_{i \\in \\text{A}} (y_i - (\\beta_0 + \\beta_{\\text{A}}))^2 + \\sum_{i \\in \\text{B}} (y_i - (\\beta_0 + \\beta_{\\text{B}}))^2 + \\sum_{i \\in \\text{C}} (y_i - (\\beta_0 + \\beta_{\\text{C}}))^2\n$$\n约束条件是 $\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}} = 0$。\n\n我们使用拉格朗日乘数法。拉格朗日函数 $\\mathcal{L}$ 为：\n$$\n\\mathcal{L}(\\beta_0, \\beta_{\\text{A}}, \\beta_{\\text{B}}, \\beta_{\\text{C}}, \\lambda) = \\text{RSS} - \\lambda(\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}})\n$$\n我们对每个参数求偏导数，并令其为零。设 $\\bar{y}_{\\text{A}}, \\bar{y}_{\\text{B}}, \\bar{y}_{\\text{C}}$ 分别是各组响应的样本均值，$n_{\\text{A}}, n_{\\text{B}}, n_{\\text{C}}$ 为样本量。\n\n1.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_0} = -2 \\sum_{i \\in \\text{A}}(y_i - \\beta_0 - \\beta_{\\text{A}}) - 2 \\sum_{i \\in \\text{B}}(y_i - \\beta_0 - \\beta_{\\text{B}}) - 2 \\sum_{i \\in \\text{C}}(y_i - \\beta_0 - \\beta_{\\text{C}}) = 0$\n    这可以简化为 $n_{\\text{A}}(\\bar{y}_{\\text{A}} - \\beta_0 - \\beta_{\\text{A}}) + n_{\\text{B}}(\\bar{y}_{\\text{B}} - \\beta_0 - \\beta_{\\text{B}}) + n_{\\text{C}}(\\bar{y}_{\\text{C}} - \\beta_0 - \\beta_{\\text{C}}) = 0$。\n\n2.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_{\\text{A}}} = -2 \\sum_{i \\in \\text{A}}(y_i - \\beta_0 - \\beta_{\\text{A}}) - \\lambda = 0 \\implies n_{\\text{A}}(\\bar{y}_{\\text{A}} - \\beta_0 - \\beta_{\\text{A}}) = \\lambda/2$。\n\n3.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_{\\text{B}}} = -2 \\sum_{i \\in \\text{B}}(y_i - \\beta_0 - \\beta_{\\text{B}}) - \\lambda = 0 \\implies n_{\\text{B}}(\\bar{y}_{\\text{B}} - \\beta_0 - \\beta_{\\text{B}}) = \\lambda/2$。\n\n4.  $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_{\\text{C}}} = -2 \\sum_{i \\in \\text{C}}(y_i - \\beta_0 - \\beta_{\\text{C}}) - \\lambda = 0 \\implies n_{\\text{C}}(\\bar{y}_{\\text{C}} - \\beta_0 - \\beta_{\\text{C}}) = \\lambda/2$。\n\n5.  $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(\\beta_{\\text{A}} + \\beta_{\\text{B}} + \\beta_{\\text{C}}) = 0$。\n\n将 (2)、(3) 和 (4) 的结果代入方程 (1) 得：\n$\\lambda/2 + \\lambda/2 + \\lambda/2 = 0 \\implies 3\\lambda/2 = 0 \\implies \\lambda = 0$。\n\n当 $\\lambda=0$ 时，方程 (2)-(4) 变为（因为 $n_k > 0$）：\n- $\\bar{y}_{\\text{A}} - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{A}} = 0 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{A}} = \\bar{y}_{\\text{A}}$\n- $\\bar{y}_{\\text{B}} - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{B}} = 0 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{B}} = \\bar{y}_{\\text{B}}$\n- $\\bar{y}_{\\text{C}} - \\hat{\\beta}_0 - \\hat{\\beta}_{\\text{C}} = 0 \\implies \\hat{\\beta}_0 + \\hat{\\beta}_{\\text{C}} = \\bar{y}_{\\text{C}}$\n\n这些方程表明，每组的拟合值就是其样本均值。我们可以将 $\\hat{\\beta}_k$ 项表示为 $\\hat{\\beta}_0$ 的函数：\n- $\\hat{\\beta}_{\\text{A}} = \\bar{y}_{\\text{A}} - \\hat{\\beta}_0$\n- $\\hat{\\beta}_{\\text{B}} = \\bar{y}_{\\text{B}} - \\hat{\\beta}_0$\n- $\\hat{\\beta}_{\\text{C}} = \\bar{y}_{\\text{C}} - \\hat{\\beta}_0$\n\n将这些代入约束方程 (5)：\n$(\\bar{y}_{\\text{A}} - \\hat{\\beta}_0) + (\\bar{y}_{\\text{B}} - \\hat{\\beta}_0) + (\\bar{y}_{\\text{C}} - \\hat{\\beta}_0) = 0$\n$\\bar{y}_{\\text{A}} + \\bar{y}_{\\text{B}} + \\bar{y}_{\\text{C}} - 3\\hat{\\beta}_0 = 0$\n求解 $\\hat{\\beta}_0$：\n$$\n\\hat{\\beta}_0 = \\frac{\\bar{y}_{\\text{A}} + \\bar{y}_{\\text{B}} + \\bar{y}_{\\text{C}}}{3}\n$$\n然后剩下的系数可以求得为 $\\hat{\\beta}_k = \\bar{y}_k - \\hat{\\beta}_0$。$\\hat{\\beta}_0$ 是组均值的未加权平均值，而 $\\hat{\\beta}_k$ 是第 $k$ 组的均值与这个平均值的偏差。\n\n#### 数值计算\n首先，我们根据数据计算样本均值：\n- $\\bar{y}_{\\text{A}} = \\frac{2+4}{2} = 3$\n- $\\bar{y}_{\\text{B}} = \\frac{3+5+7}{3} = \\frac{15}{3} = 5$\n- $\\bar{y}_{\\text{C}} = \\frac{10}{1} = 10$\n\n现在，我们计算系数：\n- $\\hat{\\beta}_0 = \\frac{3 + 5 + 10}{3} = \\frac{18}{3} = 6$\n- $\\hat{\\beta}_{\\text{A}} = \\bar{y}_{\\text{A}} - \\hat{\\beta}_0 = 3 - 6 = -3$\n- $\\hat{\\beta}_{\\text{B}} = \\bar{y}_{\\text{B}} - \\hat{\\beta}_0 = 5 - 6 = -1$\n- $\\hat{\\beta}_{\\text{C}} = \\bar{y}_{\\text{C}} - \\hat{\\beta}_0 = 10 - 6 = 4$\n\n约束估计的向量是 $(\\hat{\\beta}_0, \\hat{\\beta}_{\\text{A}}, \\hat{\\beta}_{\\text{B}}, \\hat{\\beta}_{\\text{C}}) = (6, -3, -1, 4)$。\n我们可以验证约束条件：$\\hat{\\beta}_{\\text{A}} + \\hat{\\beta}_{\\text{B}} + \\hat{\\beta}_{\\text{C}} = -3 + (-1) + 4 = 0$。\n\n#### 等价的参考编码模型\n如果我们采用参考编码，去掉水平 $\\text{C}$，模型变为：\n$$\ny_i = \\gamma_0 + \\gamma_{\\text{A}} \\mathbf{1}\\{g_i=\\text{A}\\} + \\gamma_{\\text{B}} \\mathbf{1}\\{g_i=\\text{B}\\} + \\varepsilon_i\n$$\n这里，C 组是参考水平。该模型的设计矩阵具有线性无关的列，因此 OLS 估计是唯一的。每组的期望值为：\n- $E[y|g=\\text{A}] = \\gamma_0 + \\gamma_{\\text{A}}$\n- $E[y|g=\\text{B}] = \\gamma_0 + \\gamma_{\\text{B}}$\n- $E[y|g=\\text{C}] = \\gamma_0$\n\n这些期望值的 OLS 估计分别是各自组的样本均值。因此，我们有：\n- $\\hat{\\gamma}_0 = \\bar{y}_{\\text{C}} = 10$\n- $\\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{A}} = \\bar{y}_{\\text{A}} \\implies \\hat{\\gamma}_{\\text{A}} = \\bar{y}_{\\text{A}} - \\hat{\\gamma}_0 = \\bar{y}_{\\text{A}} - \\bar{y}_{\\text{C}} = 3 - 10 = -7$\n- $\\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{B}} = \\bar{y}_{\\text{B}} \\implies \\hat{\\gamma}_{\\text{B}} = \\bar{y}_{\\text{B}} - \\hat{\\gamma}_0 = \\bar{y}_{\\text{B}} - \\bar{y}_{\\text{C}} = 5 - 10 = -5$\n\n其解释是 $\\hat{\\gamma}_0$ 是参考组（C 组）的估计均值，而 $\\hat{\\gamma}_{\\text{A}}$ 和 $\\hat{\\gamma}_{\\text{B}}$ 分别是 A 组和 B 组与参考组 C 组之间的估计均值差异。\n\n该模型的拟合值为：\n- 对于 A 组：$\\hat{y}_{\\text{A}} = \\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{A}} = 10 + (-7) = 3 = \\bar{y}_{\\text{A}}$\n- 对于 B 组：$\\hat{y}_{\\text{B}} = \\hat{\\gamma}_0 + \\hat{\\gamma}_{\\text{B}} = 10 + (-5) = 5 = \\bar{y}_{\\text{B}}$\n- 对于 C 组：$\\hat{y}_{\\text{C}} = \\hat{\\gamma}_0 = 10 = \\bar{y}_{\\text{C}}$\n\n拟合值与约束模型得到的拟合值相同（例如，对于 A 组，$\\hat{\\beta}_0 + \\hat{\\beta}_{\\text{A}} = 6 - 3 = 3$）。由于两个模型对所有观测值都产生相同的拟合值，因此它们在预测能力和拟合优度方面是等价的（例如，它们具有相同的 RSS）。参数化方式不同，但它们描述了对数据的相同潜在拟合。",
            "answer": "$$\\boxed{\\begin{pmatrix} 6  -3  -1  4 \\end{pmatrix}}$$"
        },
        {
            "introduction": "在最小二乘回归中，并非所有数据点都具有相同的重要性。某些“离群点”可能偏离整体趋势，而另一些“高杠杆点”则可能对回归线的位置产生不成比例的巨大影响，这些点被称为“强影响点”。仅仅检查残差大小不足以识别所有问题数据点。本练习  将指导您计算并比较两种关键的诊断统计量——学生化残差（studentized residuals）和库克距离（Cook's distance），从而学会区分“离群”与“影响”，并评估单个观测值对整个模型的潜在影响。",
            "id": "3138894",
            "problem": "你需要编写一个完整的程序，对一组给定的线性回归数据集，拟合带截距的普通最小二乘模型，计算外学生化残差和库克距离，然后评估比较这两种影响度量的特定逻辑条件。程序必须将所有案例的结果汇总到如下指定的单行输出中。\n\n从以下基本依据开始：\n\n- 最小二乘估计量解决的是最小化残差平方和的优化问题：给定一个满列秩的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个响应向量 $y \\in \\mathbb{R}^{n}$，最小二乘估计 $\\hat{\\beta} \\in \\mathbb{R}^{p}$ 满足正规方程 $X^{\\top}X \\hat{\\beta} = X^{\\top} y$。\n- 拟合值为 $\\hat{y} = X \\hat{\\beta}$，残差为 $e = y - \\hat{y}$。\n- 帽子矩阵为 $H = X (X^{\\top} X)^{-1} X^{\\top}$，其对角线元素为 $h_{ii}$ (杠杆值)。\n- 残差方差估计为 $s^{2} = \\frac{1}{n - p} \\sum_{i=1}^{n} e_{i}^{2}$，前提是 $n  p$。\n\n需要实现的定义：\n\n- 观测值 $i$ 的外学生化残差为 $t_{i} = \\dfrac{e_{i}}{s_{(i)} \\sqrt{1 - h_{ii}}}$，其中 $s_{(i)}^{2}$ 是在删除观测值 $i$ 后重新拟合模型计算出的残差方差估计。一种避免显式重新拟合的等效计算方法是在 $n - p - 1  0$ 时使用恒等式 $s_{(i)}^{2} = \\dfrac{(n - p) s^{2} - \\dfrac{e_{i}^{2}}{1 - h_{ii}}}{n - p - 1}$。\n- 观测值 $i$ 的库克距离定义为 $D_{i} = \\dfrac{(\\hat{\\beta} - \\hat{\\beta}_{(i)})^{\\top} X^{\\top} X (\\hat{\\beta} - \\hat{\\beta}_{(i)})}{p\\, s^{2}}$，其中 $\\hat{\\beta}_{(i)}$ 是从移除了观测值 $i$ 的数据集中计算出的最小二乘估计。你必须使用此定义来计算 $D_{i}$。\n\n实现细节：\n\n- 对每个数据集，通过拼接一列全为1的向量（截距项）和给定的的一维特征向量 $x$ 来构造 $X$。\n- 通过求解 $X^{\\top}X \\hat{\\beta} = X^{\\top} y$ 来计算 $\\hat{\\beta}$。\n- 按照上述定义计算 $\\hat{y}$、$e$、$H$、$h_{ii}$、$s^{2}$、外学生化残差 $t_{i}$ 和库克距离 $D_{i}$。\n\n测试套件：\n\n你的程序必须严格使用以下三个数据集和条件。索引是基于零的。\n\n- 案例 $1$ (单个垂直方向的离群点；预期两种度量都会标记同一点):\n  - $x = [\\,0,1,2,3,4,5,6,7,8,9\\,]$,\n  - $y = [\\,1.2,2.9,5.1,7.0,9.2,80.0,13.1,15.0,18.2,20.1\\,]$,\n  - 指定索引 $i^{\\star} = 5$,\n  - 阈值: $T_{r} = 2.0$, $T_{D} = \\dfrac{4}{n}$ 其中 $n$ 等于数据集大小,\n  - 此案例要求的布尔值: $\\big(|t_{i^{\\star}}|  T_{r}\\big) \\wedge \\big(D_{i^{\\star}}  T_{D}\\big)$。\n\n- 案例 $2$ (高杠杆值但残差小；显示尽管基于残差的信号很小，但仍有影响):\n  - $x = [\\,0,1,2,3,4,5,6,50\\,]$,\n  - $y = [\\,2.5,1.6,2.9,2.7,3.6,3.4,4.0,17.1\\,]$,\n  - 指定索引 $i^{\\star} = 7$,\n  - 阈值: $T_{r} = 2.0$, $T_{D} = \\dfrac{4}{n}$ 其中 $n$ 等于数据集大小,\n  - 此案例要求的布尔值: $\\big(|t_{i^{\\star}}| \\le T_{r}\\big) \\wedge \\big(D_{i^{\\star}}  T_{D}\\big)$。\n\n- 案例 $3$ (无离群点；所有数据点表现正常):\n  - $x = [\\,0,1,2,3,4,5,6,7,8,9\\,]$,\n  - $y = [\\,3.1,2.3,2.2,1.4,1.1,0.3,0.2,-0.5,-0.9,-1.6\\,]$,\n  - 阈值: $T_{r} = 2.0$, $T_{D} = \\dfrac{4}{n}$ 其中 $n$ 等于数据集大小,\n  - 此案例要求的布尔值: 所有观测值同时满足 $|t_{i}| \\le T_{r}$ 和 $D_{i} \\le T_{D}$。\n\n最终输出格式：\n\n- 你的程序应生成单行输出，其中包含三个案例的结果，形式为方括号括起来的逗号分隔列表，例如 $[\\,\\text{True},\\text{False},\\text{True}\\,]$。\n- 不应打印任何其他文本。\n- 不涉及物理单位或角度，因此不需要单位转换。",
            "solution": "该问题要求针对三个指定的数据集，实现普通最小二乘 (OLS) 回归，并计算两个关键的影响力诊断指标——外学生化残差和库克距离。然后必须评估基于这些诊断指标的特定逻辑条件的有效性。整个过程严格遵守所提供的数学定义。\n\n首先，我们建立线性回归模型。对于一个由一维特征向量 $x \\in \\mathbb{R}^{n}$ 和响应向量 $y \\in \\mathbb{R}^{n}$ 组成的数据集，我们拟合模型 $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$。其矩阵形式为 $y = X \\beta + \\epsilon$，其中 $\\beta = [\\beta_0, \\beta_1]^{\\top}$ 是参数向量。设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 是通过在特征向量 $x$ 前面加上一列全为1的向量来构建的，以对应截距项 $\\beta_0$。因此，$X = [\\mathbf{1} \\mid x]$ 且参数数量为 $p=2$。\n\nOLS估计 $\\hat{\\beta}$ 是通过最小化残差平方和 $RSS(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta)$ 找到的。该最小化问题的解由正规方程给出：\n$$\n(X^{\\top}X) \\hat{\\beta} = X^{\\top} y\n$$\n假设 $X$ 具有满列秩（对于所有给定的数据集都成立），我们可以求解 $\\hat{\\beta}$ 为 $\\hat{\\beta} = (X^{\\top}X)^{-1} X^{\\top}y$。\n\n从 $\\hat{\\beta}$，我们计算几个基本量：\n- 拟合值向量：$\\hat{y} = X \\hat{\\beta}$。\n- 残差向量：$e = y - \\hat{y}$。\n- 帽子矩阵，它将响应值映射到拟合值 ($\\hat{y} = Hy$)：$H = X(X^{\\top}X)^{-1}X^{\\top}$。$H$ 的对角线元素，记作 $h_{ii}$，是每个观测值的杠杆值。它们衡量了一个观测值影响其自身拟合值的潜力。\n- 误差方差 $\\sigma^2$ 的一个无偏估计是残差均方或残差方差估计 $s^2$：\n$$\ns^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} e_i^2 = \\frac{e^{\\top}e}{n-p}\n$$\n这在 $n  p$ 的条件下有效。\n\n有了这些量，我们就可以计算所需的影响力诊断指标。\n\n观测值 $i$ 的外学生化残差 $t_i$ 定义为：\n$$\nt_{i} = \\frac{e_{i}}{s_{(i)} \\sqrt{1 - h_{ii}}}\n$$\n此处，$s_{(i)}$ 是从删除了观测值 $i$ 的数据集上计算出的回归标准误。该诊断指标通过将第 $i$ 个观测值的残差与不包含该观测值拟合的模型的变异性进行比较，来衡量该观测值是离群点的程度。显式地重新拟合模型 $n$ 次在计算上是昂贵的。因此，我们使用提供的恒等式，它在 $n-p-1  0$ 时有效：\n$$\ns_{(i)}^{2} = \\frac{(n-p)s^2 - \\frac{e_i^2}{1-h_{ii}}}{n-p-1}\n$$\n\n库克距离 $D_i$ 衡量了删除观测值 $i$ 对整个估计系数向量 $\\hat{\\beta}$ 的影响。问题强制要求使用其定义公式：\n$$\nD_{i} = \\frac{(\\hat{\\beta} - \\hat{\\beta}_{(i)})^{\\top} X^{\\top} X (\\hat{\\beta} - \\hat{\\beta}_{(i)})}{p s^{2}}\n$$\n此处，$\\hat{\\beta}_{(i)}$ 是从移除了第 $i$ 个观测值的数据集估计出的系数向量。为遵守此要求，计算过程涉及一个从 $i=1$ 到 $n$ 的循环。在每次迭代 $i$ 中，从 $X$ 和 $y$ 中移除第 $i$ 行以形成 $X_{(i)}$ 和 $y_{(i)}$，然后求解正规方程 $(X_{(i)}^{\\top}X_{(i)}) \\hat{\\beta}_{(i)} = X_{(i)}^{\\top}y_{(i)}$ 以找到 $\\hat{\\beta}_{(i)}$。然后将此值用于 $D_i$ 的公式中。\n\n每个测试案例的总体算法如下：\n1.  接收向量 $x$ 和 $y$。将 $n$ 设为观测值数量，$p=2$。\n2.  构造设计矩阵 $X$。\n3.  求解正规方程以获得全数据集的系数估计 $\\hat{\\beta}$。\n4.  计算残差 $e$、帽子矩阵对角线元素 $h_{ii}$ 和残差方差 $s^2$。\n5.  对所有 $i \\in \\{1, \\dots, n\\}$，使用 $s_{(i)}^2$ 的公式计算外学生化残差 $t_i$。\n6.  对所有 $i \\in \\{1, \\dots, n\\}$，通过在留一数据集上显式重新拟合模型以找到 $\\hat{\\beta}_{(i)}$，来计算库克距离 $D_i$。\n7.  使用计算出的 $t_i$ 和 $D_i$ 数组以及给定的阈值，评估为该案例指定的布尔条件。\n\n此过程应用于三个测试案例中的每一个，并将所得的布尔值汇总到一个最终列表中。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the linear regression influence diagnostics problem for three test cases.\n    \"\"\"\n\n    def compute_diagnostics(x: np.ndarray, y: np.ndarray):\n        \"\"\"\n        Computes regression diagnostics for a given dataset (x, y).\n\n        Args:\n            x: 1D numpy array of feature values.\n            y: 1D numpy array of response values.\n\n        Returns:\n            A tuple containing:\n            - t_residuals: Externally studentized residuals.\n            - cooks_distances: Cook's distances.\n        \"\"\"\n        n = len(x)\n        p = 2  # Number of parameters: intercept (beta_0) and slope (beta_1)\n\n        # Construct the design matrix X with an intercept column.\n        X = np.c_[np.ones(n), x]\n\n        # --- Full model fit ---\n        # Solve the normal equations: (X'X)beta_hat = X'y\n        XTX = X.T @ X\n        XTy = X.T @ y\n        beta_hat = np.linalg.solve(XTX, XTy)\n\n        # Compute residuals and residual variance\n        y_hat = X @ beta_hat\n        e = y - y_hat\n        \n        # Check for n > p\n        if not n > p:\n            raise ValueError(\"n must be greater than p for s^2 to be defined.\")\n        s2 = (e.T @ e) / (n - p)\n\n        # Compute hat matrix H and leverages h_ii\n        XTX_inv = np.linalg.inv(XTX)\n        H = X @ XTX_inv @ X.T\n        h_ii = np.diag(H)\n        \n        # --- Externally Studentized Residuals ---\n        # Check for n - p - 1 > 0\n        if not n - p - 1 > 0:\n            raise ValueError(\"n-p-1 must be positive for s_(i)^2 to be defined.\")\n        \n        # An identity for s_(i)^2 is used to avoid refitting n times.\n        # s_(i)^2 = [(n-p)s^2 - e_i^2/(1-h_ii)] / (n-p-1)\n        s2_i = ((n - p) * s2 - e**2 / (1 - h_ii)) / (n - p - 1)\n        t_residuals = e / (np.sqrt(s2_i) * np.sqrt(1 - h_ii))\n\n        # --- Cook's Distances ---\n        # Computed via the definition, requiring a loop and refitting.\n        cooks_distances = np.zeros(n)\n        for i in range(n):\n            # Create leave-one-out data\n            X_i = np.delete(X, i, axis=0)\n            y_i = np.delete(y, i)\n\n            # Solve for beta_hat_(i)\n            beta_hat_i = np.linalg.solve(X_i.T @ X_i, X_i.T @ y_i)\n\n            # Compute Cook's distance D_i\n            beta_diff = beta_hat - beta_hat_i\n            numerator = beta_diff.T @ XTX @ beta_diff\n            cooks_distances[i] = numerator / (p * s2)\n            \n        return t_residuals, cooks_distances\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"x\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=float),\n            \"y\": np.array([1.2, 2.9, 5.1, 7.0, 9.2, 80.0, 13.1, 15.0, 18.2, 20.1], dtype=float),\n            \"i_star\": 5,\n            \"T_r\": 2.0,\n        },\n        {\n            \"x\": np.array([0, 1, 2, 3, 4, 5, 6, 50], dtype=float),\n            \"y\": np.array([2.5, 1.6, 2.9, 2.7, 3.6, 3.4, 4.0, 17.1], dtype=float),\n            \"i_star\": 7,\n            \"T_r\": 2.0,\n        },\n        {\n            \"x\": np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=float),\n            \"y\": np.array([3.1, 2.3, 2.2, 1.4, 1.1, 0.3, 0.2, -0.5, -0.9, -1.6], dtype=float),\n            \"T_r\": 2.0,\n        }\n    ]\n\n    results = []\n\n    # Case 1\n    case1 = test_cases[0]\n    n1 = len(case1[\"x\"])\n    t1, D1 = compute_diagnostics(case1[\"x\"], case1[\"y\"])\n    T_r1 = case1[\"T_r\"]\n    T_D1 = 4.0 / n1\n    i_star1 = case1[\"i_star\"]\n    result1 = (np.abs(t1[i_star1]) > T_r1) and (D1[i_star1] > T_D1)\n    results.append(result1)\n\n    # Case 2\n    case2 = test_cases[1]\n    n2 = len(case2[\"x\"])\n    t2, D2 = compute_diagnostics(case2[\"x\"], case2[\"y\"])\n    T_r2 = case2[\"T_r\"]\n    T_D2 = 4.0 / n2\n    i_star2 = case2[\"i_star\"]\n    result2 = (np.abs(t2[i_star2]) = T_r2) and (D2[i_star2] > T_D2)\n    results.append(result2)\n\n    # Case 3\n    case3 = test_cases[2]\n    n3 = len(case3[\"x\"])\n    t3, D3 = compute_diagnostics(case3[\"x\"], case3[\"y\"])\n    T_r3 = case3[\"T_r\"]\n    T_D3 = 4.0 / n3\n    result3 = np.all((np.abs(t3) = T_r3)  (D3 = T_D3))\n    results.append(result3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}