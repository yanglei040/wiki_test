{
    "hands_on_practices": [
        {
            "introduction": "我们将从一个回归分析中的基本挑战开始：完全多重共线性。这个练习使用常见的“虚拟变量陷阱”场景，来演示为何在一个过度参数化的模型中，设计矩阵 $X$ 的列是线性相关的，导致 $X^{\\top}X$ 奇异，从而无法计算出唯一的标准误。它将引导你理解如何通过模型重新参数化来“拯救”模型，为真正可估计的量（例如组均值之差）构建有意义的置信区间。",
            "id": "3176621",
            "problem": "一位数据分析师使用带有截距和组成员虚拟变量的线性回归模型来建模时薪。存在两个组，$A$ 组和 $B$ 组，分析师使用虚拟变量 $D_A$ 和 $D_B$ 对成员关系进行编码，其中对于 $A$ 组成员，$D_A=1$，否则为 $0$；对于 $B$ 组成员，$D_B=1$，否则为 $0$。观测到的数据是：\n- $A$ 组工资：$y=(10,12,11)$，\n- $B$ 组工资：$y=(7,9,8)$，\n因此总样本量为 $n=6$。分析师首先拟合一个包含截距和两个虚拟变量的模型，然后考虑重新参数化以解决任何潜在的可识别性问题。\n\n选择所有关于 $(X^{\\top}X)^{-1}$ 的存在性、回归系数的可识别性，以及在这种情况下重新参数化如何恢复回归系数的可识别标准误和置信区间的正确陈述。你可以假设一个同方差误差模型，并使用基于 $t$ 分布的常规 $95\\%$ 置信区间，其残差自由度与拟合模型相对应。\n\nA. 当模型包含截距以及 $D_A$ 和 $D_B$ 两个虚拟变量时，设计矩阵的列是完全共线的，因此 $X^{\\top}X$ 是奇异的，$(X^{\\top}X)^{-1}$ 不存在。\n\nB. 重新参数化为使用一个截距和一个虚拟变量（例如，去掉 $D_B$ 并保留 $D_A$）会得到一个满列秩的设计矩阵，并且 $D_A$ 的系数（$A$ 组与 $B$ 组的均值差异）的 $95\\%$ 双侧置信区间大约为 $[0.73,\\,5.27]$。\n\nC. 另一种方法是，去掉截距并保留 $D_A$ 和 $D_B$ 两个虚拟变量，这也会得到一个满列秩的设计矩阵，并且 $\\beta_A-\\beta_B$ 的 $95\\%$ 双侧置信区间与选项 B 中的相同。\n\nD. 即使不进行重新参数化，在包含截距和两个虚拟变量的过参数化模型中，仍然可以为 $D_A$ 和 $D_B$ 的单个系数获得唯一的、与软件无关的标准误和 $95\\%$ 置信区间，因为普通最小二乘法会自动忽略冗余的列。",
            "solution": "问题陈述是科学合理的、定义明确且客观的。它提供了一个标准的、教科书式的场景，用于演示线性回归中的多重共线性、模型参数化和可估计性等概念。分析所需的所有数据和条件都已给出且一致。我将对每个选项进行完整的推导和评估。\n\n响应向量 $y$ 结合了两组的工资：\n$$y = (10, 12, 11, 7, 9, 8)^{\\top}$$\n虚拟变量定义为：对于 $A$ 组的观测值（前三个），$D_A=1$；对于 $B$ 组的观测值（后三个），$D_A=0$。并且对于 $B$ 组成员，$D_B=1$；对于 $A$ 组成员，$D_B=0$。\n\nA. 当模型包含截距以及 $D_A$ 和 $D_B$ 两个虚拟变量时，设计矩阵的列是完全共线的，因此 $X^{\\top}X$ 是奇异的，$(X^{\\top}X)^{-1}$ 不存在。\n\n模型为 $y_i = \\beta_0 + \\beta_A D_{Ai} + \\beta_B D_{Bi} + \\epsilon_i$。设计矩阵 $X$ 有 $n=6$ 行和 $p=3$ 列（一列对应截距 $\\beta_0$，一列对应 $\\beta_A$，一列对应 $\\beta_B$）。\n这些列分别是截距向量 $c_1$，$D_A$ 的向量 $c_2$，以及 $D_B$ 的向量 $c_3$：\n$$\nc_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nc_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nc_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n设计矩阵为 $X = [c_1, c_2, c_3]$。这些列之间存在线性相关性，因为对于每个观测值，个体要么属于 $A$ 组，要么属于 $B$ 组，这意味着 $D_A + D_B = 1$。因此，虚拟变量列的总和等于截距列：\n$$c_2 + c_3 = c_1 \\implies 1 \\cdot c_1 - 1 \\cdot c_2 - 1 \\cdot c_3 = \\mathbf{0}$$\n这种完全共线性意味着 $X$ 的列是线性相关的，矩阵 $X$ 不是满列秩的。$X$ 的秩是 $2$，而不是 $3$。\n因此，矩阵 $X^{\\top}X$ 是奇异的（不可逆）。我们可以计算它来验证：\n$$\nX^{\\top}X = \\begin{pmatrix}\n1  1  1  1  1  1 \\\\\n1  1  1  0  0  0 \\\\\n0  0  0  1  1  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1  0 \\\\\n1  1  0 \\\\\n1  1  0 \\\\\n1  0  1 \\\\\n1  0  1 \\\\\n1  0  1\n\\end{pmatrix}\n= \\begin{pmatrix}\n6  3  3 \\\\\n3  3  0 \\\\\n3  0  3\n\\end{pmatrix}\n$$\n该矩阵的行列式为 $\\det(X^{\\top}X) = 6(3 \\cdot 3 - 0) - 3(3 \\cdot 3 - 0) + 3(0 - 3 \\cdot 3) = 54 - 27 - 27 = 0$。由于行列式为零，$X^{\\top}X$ 是奇异的，其逆矩阵 $(X^{\\top}X)^{-1}$ 不存在。这种情况被称为虚拟变量陷阱。系数 $\\beta_0, \\beta_A, \\beta_B$ 不是唯一可识别的。\n\n该陈述是 **正确的**。\n\nB. 重新参数化为使用一个截距和一个虚拟变量（例如，去掉 $D_B$ 并保留 $D_A$）会得到一个满列秩的设计矩阵，并且 $D_A$ 的系数（$A$ 组与 $B$ 组的均值差异）的 $95\\%$ 双侧置信区间大约为 $[0.73,\\,5.27]$。\n\n重新参数化的模型是 $y_i = \\beta'_0 + \\beta'_A D_{Ai} + \\epsilon'_i$。这是解决共线性问题的标准方法。设计矩阵 $X_1$ 有两列：\n$$\nX_1 = \\begin{pmatrix}\n1  1 \\\\\n1  1 \\\\\n1  1 \\\\\n1  0 \\\\\n1  0 \\\\\n1  0\n\\end{pmatrix}\n$$\n这两列是线性无关的，所以 $X_1$ 是满列秩（秩为 $2$），并且 $(X_1^{\\top}X_1)^{-1}$ 存在。\nOLS 估计量是 $\\hat{\\beta}' = (X_1^{\\top}X_1)^{-1}X_1^{\\top}y$。\n$$\nX_1^{\\top}X_1 = \\begin{pmatrix} 6  3 \\\\ 3  3 \\end{pmatrix} \\quad \\implies \\quad (X_1^{\\top}X_1)^{-1} = \\frac{1}{18-9} \\begin{pmatrix} 3  -3 \\\\ -3  6 \\end{pmatrix} = \\begin{pmatrix} 1/3  -1/3 \\\\ -1/3  2/3 \\end{pmatrix}\n$$\n$$\nX_1^{\\top}y = \\begin{pmatrix} \\sum y_i \\\\ \\sum_{i \\in A} y_i \\end{pmatrix} = \\begin{pmatrix} 57 \\\\ 33 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}' = \\begin{pmatrix} \\hat{\\beta}'_0 \\\\ \\hat{\\beta}'_A \\end{pmatrix} = \\begin{pmatrix} 1/3  -1/3 \\\\ -1/3  2/3 \\end{pmatrix} \\begin{pmatrix} 57 \\\\ 33 \\end{pmatrix} = \\begin{pmatrix} (57-33)/3 \\\\ (-57+66)/3 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 3 \\end{pmatrix}\n$$\n在这个模型中，$\\beta'_0$ 代表参考组（$B$ 组，其中 $D_A=0$）的平均工资，而 $\\beta'_A$ 代表 $A$ 组和 $B$ 组之间的平均工资差异。$B$ 组的样本均值为 $(7+9+8)/3 = 8$，所以 $\\hat{\\beta}'_0=8$。$A$ 组的样本均值为 $(10+12+11)/3 = 11$。均值差异为 $11-8=3$，所以 $\\hat{\\beta}'_A=3$。陈述中对系数的解释是正确的。\n\n为了找到 $\\beta'_A$ 的置信区间，我们需要它的标准误。$SE(\\hat{\\beta}'_A) = \\sqrt{\\hat{\\sigma}^2 (X_1^{\\top}X_1)^{-1}_{22}}$。\n首先，我们计算残差平方和（$RSS$）来估计误差方差 $\\sigma^2$。拟合值为：$A$ 组 $\\hat{y}_i = 8+3(1)=11$，$B$ 组 $\\hat{y}_i = 8+3(0)=8$。\n$$RSS = \\sum(y_i-\\hat{y}_i)^2 = (10-11)^2+(12-11)^2+(11-11)^2 + (7-8)^2+(9-8)^2+(8-8)^2 = 1+1+0+1+1+0 = 4$$\n参数数量为 $p=2$。残差的自由度为 $df=n-p=6-2=4$。\n误差方差的无偏估计为 $\\hat{\\sigma}^2 = \\frac{RSS}{n-p} = \\frac{4}{4} = 1$。\n$\\hat{\\beta}'_A$ 的方差是 $\\text{Var}(\\hat{\\beta}'_A) = \\hat{\\sigma}^2 (X_1^{\\top}X_1)^{-1}_{22} = 1 \\cdot (2/3) = 2/3$。\n标准误是 $SE(\\hat{\\beta}'_A) = \\sqrt{2/3}$。\n对于自由度 $df=4$ 的 $95\\%$ 置信区间，临界 t 值为 $t_{0.025, 4} = 2.776$。\n置信区间为 $\\hat{\\beta}'_A \\pm t_{0.025, 4} \\cdot SE(\\hat{\\beta}'_A)$:\n$$3 \\pm 2.776 \\cdot \\sqrt{2/3} \\approx 3 \\pm 2.776 \\cdot 0.8165 \\approx 3 \\pm 2.266$$\n这给出的区间为 $[0.734, 5.266]$，四舍五入后为 $[0.73, 5.27]$。\n\n该陈述是 **正确的**。\n\nC. 另一种方法是，去掉截距并保留 $D_A$ 和 $D_B$ 两个虚拟变量，这也会得到一个满列秩的设计矩阵，并且 $\\beta_A-\\beta_B$ 的 $95\\%$ 双侧置信区间与选项 B 中的相同。\n\n另一种模型是 $y_i = \\beta_A D_{Ai} + \\beta_B D_{Bi} + \\epsilon''_{i}$。其设计矩阵 $X_2$ 是：\n$$\nX_2 = \\begin{pmatrix}\n1  0 \\\\\n1  0 \\\\\n1  0 \\\\\n0  1 \\\\\n0  1 \\\\\n0  1\n\\end{pmatrix}\n$$\n这些列是正交的，因此是线性无关的。$X_2$ 是满列秩的。\nOLS 估计量是 $\\hat{\\beta}'' = (X_2^{\\top}X_2)^{-1}X_2^{\\top}y$。\n$$\nX_2^{\\top}X_2 = \\begin{pmatrix} 3  0 \\\\ 0  3 \\end{pmatrix} \\quad \\implies \\quad (X_2^{\\top}X_2)^{-1} = \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix}\n$$\n$$\nX_2^{\\top}y = \\begin{pmatrix} \\sum_{i \\in A} y_i \\\\ \\sum_{i \\in B} y_i \\end{pmatrix} = \\begin{pmatrix} 33 \\\\ 24 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}'' = \\begin{pmatrix} \\hat{\\beta}_A \\\\ \\hat{\\beta}_B \\end{pmatrix} = \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix} \\begin{pmatrix} 33 \\\\ 24 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 8 \\end{pmatrix}\n$$\n在这个模型中，$\\beta_A$ 是 $A$ 组的平均工资，$\\beta_B$ 是 $B$ 组的平均工资。它们的估计值分别是各自的样本均值。\n我们关心的是差异 $\\beta_A - \\beta_B$ 的置信区间。点估计为 $\\hat{\\beta}_A - \\hat{\\beta}_B = 11 - 8 = 3$。这与模型 B 中的点估计 $\\hat{\\beta}'_A$ 相同。\n这个差异的方差是 $\\text{Var}(\\hat{\\beta}_A - \\hat{\\beta}_B) = \\text{Var}(\\hat{\\beta}_A) + \\text{Var}(\\hat{\\beta}_B) - 2\\text{Cov}(\\hat{\\beta}_A, \\hat{\\beta}_B)$。\n为了找到系数的方差-协方差矩阵，我们首先需要这个模型的 $\\hat{\\sigma}^2$。拟合值为：$A$ 组 $\\hat{y}_i = 11$，$B$ 组 $\\hat{y}_i = 8$。这些与模型 B 中的拟合值相同。因此，残差和 $RSS$ 是相同的：$RSS=4$。参数数量为 $p=2$，所以自由度 $df=n-p=4$，也与之前相同。因此，$\\hat{\\sigma}^2 = RSS/df = 4/4 = 1$。\n方差-协方差矩阵是 $\\text{Cov}(\\hat{\\beta}'') = \\hat{\\sigma}^2(X_2^{\\top}X_2)^{-1} = 1 \\cdot \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix}$。\n由此可得，$\\text{Var}(\\hat{\\beta}_A) = 1/3$，$\\text{Var}(\\hat{\\beta}_B) = 1/3$，且 $\\text{Cov}(\\hat{\\beta}_A, \\hat{\\beta}_B) = 0$。\n所以，$\\text{Var}(\\hat{\\beta}_A - \\hat{\\beta}_B) = 1/3 + 1/3 - 0 = 2/3$。这个差异的标准误是 $SE(\\hat{\\beta}_A - \\hat{\\beta}_B) = \\sqrt{2/3}$。\n点估计（$3$）、标准误（$\\sqrt{2/3}$）和自由度（$4$）都与选项 B 中 $\\hat{\\beta}'_A$ 的相应值相同。因此，$\\beta_A-\\beta_B$ 的 $95\\%$ 置信区间必然是相同的。模型 B 和 C 都是有效的重新参数化，它们张成了相同的拟合值向量空间，并且 $\\beta_A-\\beta_B$ 和 $\\beta'_A$ 都是代表相同物理量（均值差异）的可估计函数，因此它们的估计值和置信区间必须相同。\n\n该陈述是 **正确的**。\n\nD. 即使不进行重新参数化，在包含截距和两个虚拟变量的过参数化模型中，仍然可以为 $D_A$ 和 $D_B$ 的单个系数获得唯一的、与软件无关的标准误和 $95\\%$ 置信区间，因为普通最小二乘法会自动忽略冗余的列。\n\n如在 A 中所证，过参数化模型的矩阵 $X^{\\top}X$ 是奇异的。这意味着系数向量 $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_A, \\hat{\\beta}_B)^{\\top}$ 没有唯一解。正规方程 $X^{\\top}X\\hat{\\beta}=X^{\\top}y$ 有无穷多解。\n具体来说，如果 $\\hat{\\beta}^*$ 是一个解，那么对于任意标量 $c$，向量 $\\hat{\\beta}^* + c(1, -1, -1)^{\\top}$ 也是一个解，因为 $(1, -1, -1)^{\\top}$ 在 $X$ 的零空间中。\n单个系数 $\\beta_A$ 和 $\\beta_B$ 不是“可估计”函数，这意味着它们的估计值依赖于为找到一个特定解而施加的任意约束。不同的统计软件包会施加不同的约束（例如，将一个系数设为零，或使用和为零的约束），这会导致 $\\hat{\\beta}_A$ 和 $\\hat{\\beta}_B$ 的值不同。例如，将 $\\beta_B$ 设为 $0$ 会得到 $\\hat{\\beta}_A=3$，而将 $\\beta_A$ 设为 $0$ 会得到 $\\hat{\\beta}_B=-3$。\n由于 $\\beta_A$ 和 $\\beta_B$ 的点估计不是唯一的，并且依赖于所使用的软件，它们的标准误和置信区间也不是唯一确定或独立于软件的。$\\hat{\\beta}$ 的方差形式上由 $\\sigma^2(X^{\\top}X)^-$ 给出，其中 $(X^{\\top}X)^-$ 是 $X^{\\top}X$ 的一个广义逆。广义逆不是唯一的，不同的选择会导致对于像 $\\beta_A$ 和 $\\beta_B$ 这样的不可估计系数产生不同（且无意义）的方差。\n声称 OLS “自动忽略冗余列” 是对某些软件实现方式的粗略描述，但选择忽略哪一列是任意的，并且会影响单个系数值。正是因为这种模糊性，才需要重新参数化来为可解释的参数获得有意义的、唯一的估计和推断。\n\n该陈述是 **不正确的**。",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "在了解了完全共线性如何“破坏”模型后，让我们来探讨一种更微妙的情况：截距项和预测变量之间的相关性。这个练习将揭示一个简单的数据预处理步骤——中心化——如何通过消除预测变量均值 $\\bar{x}$ 对截距项方差的影响，从而显著减小其标准误。你将亲自计算这一效应，并理解为何斜率的估计及其置信区间保持不变，从而加深对回归系数不确定性的理解。",
            "id": "3176657",
            "problem": "一位数据分析师对一组固定设计数据拟合了一个普通最小二乘法 (OLS) 简单线性回归模型，并假设满足经典线性模型的条件：对于 $i=1,\\dots,n$，响应变量满足 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$，其中误差 $\\varepsilon_i$ 独立同分布，服从 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$，且预测变量值 $x_i$ 是非随机的。分析师报告了原始（未中心化）模型的以下摘要信息：\n- 样本量 $n = 100$。\n- 预测变量均值 $\\bar{x} = 75$。\n- 预测变量离均差平方和 $S_{xx} = \\sum_{i=1}^{n}(x_i - \\bar{x})^2 = 39600$。\n- 估计斜率 $\\hat{\\beta}_1 = 0.45$。\n- 残差均方误差 $\\hat{\\sigma}^2 = 9$。\n\n然后，该分析师通过定义 $x_i^{c} = x_i - \\bar{x}$ 对预测变量进行中心化，并在相同假设下使用 OLS 重新对 $y_i$ 进行关于截距和 $x_i^{c}$ 的回归分析。\n\n以下哪个陈述最能概括中心化对估计截距和斜率t统计量的定量和定性影响，并解释其原因？\n\nA. 中心化后，截距标准误不变，但斜率估计值及其t统计量会发生变化，因为中心化改变了 $x$ 的散布程度。\n\nB. 中心化后，截距标准误从约 $1.17$ 下降到约 $0.30$，因为预测变量的均值变为零，而斜率的t统计量保持不变；中心化不改变与斜率相关的 $S_{xx}$ 或残差。\n\nC. 中心化后，截距和斜率的标准误都除以 $\\sqrt{n}$，相应的两个t统计量都会增加。\n\nD. 中心化后，斜率标准误增加，因为当 $x$ 移动时 $S_{xx}$ 减小，导致t统计量变小，而截距标准误保持不变。\n\nE. 中心化后，斜率的置信区间变宽，因为自由度从 $n-2$ 变为 $n-1$，而截距的置信区间略微变窄。",
            "solution": "用户希望我分析对预测变量进行中心化处理对简单线性回归模型的参数和检验统计量的影响。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n-   模型（原始）：$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$，其中 $i=1,\\dots,n$。\n-   假设：误差 $\\varepsilon_i$ 独立同分布，服从 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。预测变量值 $x_i$ 是非随机的（固定设计）。\n-   样本量：$n = 100$。\n-   预测变量均值：$\\bar{x} = 75$。\n-   预测变量离均差平方和：$S_{xx} = \\sum_{i=1}^{n}(x_i - \\bar{x})^2 = 39600$。\n-   估计斜率：$\\hat{\\beta}_1 = 0.45$。\n-   残差均方误差：$\\hat{\\sigma}^2 = 9$。\n-   中心化预测变量：$x_i^{c} = x_i - \\bar{x}$。\n-   模型（中心化）：$y_i = \\beta'_0 + \\beta'_1 x_i^c + \\varepsilon'_i$。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学上合理：** 该问题基于普通最小二乘法 (OLS) 回归理论，这是经典统计学的基石。其假设是经典线性模型的标准假设。整个设定在科学和数学上都是合理的。\n-   **良态的：** 该问题提供了所有必要的数值和模型定义，以比较原始回归模型和中心化回归模型的统计特性。可以推导出一个唯一的、有意义的解。\n-   **客观的：** 该问题使用精确、标准的统计术语陈述，没有主观解释的余地。\n\n未发现任何缺陷。该问题并非不合理、不完整、不现实或非良态。\n\n**步骤 3：结论与行动**\n问题陈述有效。我将继续进行推导和求解。\n\n### 推导\n\n我们首先分析原始（未中心化）模型，然后分析中心化模型。我们将用一个撇号（例如 $\\hat{\\beta}'_0$）来表示中心化模型中的量。\n\n**1. 原始（未中心化）模型分析**\n\n参数的 OLS 估计量由以下公式给出：\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\n这些估计量的估计方差（在给定假设下）为：\n$$ \\widehat{\\text{Var}}(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}^2}{S_{xx}} $$\n$$ \\widehat{\\text{Var}}(\\hat{\\beta}_0) = \\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right) $$\n标准误 ($SE$) 是这些估计方差的平方根。\n\n使用所提供的数据：\n-   $n = 100$\n-   $\\bar{x} = 75$\n-   $S_{xx} = 39600$\n-   $\\hat{\\beta}_1 = 0.45$\n-   $\\hat{\\sigma}^2 = 9$\n\n我们可以计算截距的标准误 $SE(\\hat{\\beta}_0)$：\n$$ SE(\\hat{\\beta}_0) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\beta}_0)} = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right)} = \\sqrt{9 \\left( \\frac{1}{100} + \\frac{75^2}{39600} \\right)} $$\n$$ SE(\\hat{\\beta}_0) = \\sqrt{9 \\left( 0.01 + \\frac{5625}{39600} \\right)} \\approx \\sqrt{9 (0.01 + 0.142045)} = \\sqrt{9(0.152045)} = \\sqrt{1.368409} \\approx 1.16979 $$\n所以，原始截距标准误约为 $1.17$。\n\n斜率的标准误 $SE(\\hat{\\beta}_1)$ 为：\n$$ SE(\\hat{\\beta}_1) = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = \\sqrt{\\frac{9}{39600}} = \\sqrt{\\frac{1}{4400}} \\approx 0.015076 $$\n斜率的t统计量 $t_{\\hat{\\beta}_1}$ 为：\n$$ t_{\\hat{\\beta}_1} = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} = \\frac{0.45}{\\sqrt{9/39600}} = 0.45 \\times \\sqrt{4400} \\approx 29.85 $$\n\n**2. 中心化模型分析**\n\n新的预测变量是 $x_i^c = x_i - \\bar{x}$。\n中心化预测变量的均值为 $\\bar{x}^c = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x}) = \\bar{x} - \\bar{x} = 0$。\n中心化预测变量的离均差平方和为：\n$$ S_{x^c x^c} = \\sum_{i=1}^n (x_i^c - \\bar{x}^c)^2 = \\sum_{i=1}^n (x_i - \\bar{x} - 0)^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2 = S_{xx} = 39600 $$\n因此，中心化不改变平方和 $S_{xx}$。\n\n现在，我们来求新的 OLS 估计量 $\\hat{\\beta}'_0$ 和 $\\hat{\\beta}'_1$：\n$$ \\hat{\\beta}'_1 = \\frac{\\sum_{i=1}^n (x_i^c - \\bar{x}^c)(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i^c - \\bar{x}^c)^2} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{S_{xx}} = \\hat{\\beta}_1 $$\n斜率估计值对于预测变量的中心化是不变的。所以，$\\hat{\\beta}'_1 = 0.45$。\n\n$$ \\hat{\\beta}'_0 = \\bar{y} - \\hat{\\beta}'_1 \\bar{x}^c = \\bar{y} - \\hat{\\beta}_1 (0) = \\bar{y} $$\n新的截距估计值是响应变量的样本均值。\n\n中心化模型的拟合值为 $\\hat{y}'_i = \\hat{\\beta}'_0 + \\hat{\\beta}'_1 x_i^c = \\bar{y} + \\hat{\\beta}_1 (x_i - \\bar{x})$。\n原始模型的拟合值为 $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i = (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) + \\hat{\\beta}_1 x_i = \\bar{y} + \\hat{\\beta}_1 (x_i - \\bar{x})$。\n由于对所有 $i$ 都有 $\\hat{y}'_i = \\hat{y}_i$，因此残差 $e'_i = y_i - \\hat{y}'_i$ 和 $e_i = y_i - \\hat{y}_i$ 是相同的。\n这意味着残差平方和 (RSS) 不变，因此残差均方误差 $\\hat{\\sigma}'^2 = \\frac{\\text{RSS}}{n-2}$ 也不变。因此，$\\hat{\\sigma}'^2 = \\hat{\\sigma}^2 = 9$。\n\n现在我们计算中心化模型的标准误：\n新斜率的标准误 $SE(\\hat{\\beta}'_1)$ 为：\n$$ SE(\\hat{\\beta}'_1) = \\sqrt{\\frac{\\hat{\\sigma}'^2}{S_{x^c x^c}}} = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = SE(\\hat{\\beta}_1) $$\n斜率标准误不变。\n\n新截距的标准误 $SE(\\hat{\\beta}'_0)$ 为：\n$$ SE(\\hat{\\beta}'_0) = \\sqrt{\\hat{\\sigma}'^2 \\left( \\frac{1}{n} + \\frac{(\\bar{x}^c)^2}{S_{x^c x^c}} \\right)} = \\sqrt{9 \\left( \\frac{1}{100} + \\frac{0^2}{39600} \\right)} = \\sqrt{9 \\left( \\frac{1}{100} \\right)} = \\sqrt{0.09} = 0.30 $$\n截距标准误从约 $1.17$ 下降到恰好 $0.30$。这种减少是因为方差公式中包含 $\\bar{x}^2$ 的项消失了。从几何角度看，中心化模型中的截距代表在预测变量均值处的预测值，此处的估计最为精确。在原始模型中，它代表 $x=0$ 处的预测值，这远离数据的中心（$\\bar{x}=75$），需要进行长距离的外推。\n\n最后，中心化斜率的t统计量 $t_{\\hat{\\beta}'_1}$ 为：\n$$ t_{\\hat{\\beta}'_1} = \\frac{\\hat{\\beta}'_1}{SE(\\hat{\\beta}'_1)} = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} = t_{\\hat{\\beta}_1} $$\n斜率的t统计量不变，因为斜率估计值及其标准误都没有改变。\n\n**影响总结：**\n-   截距估计值：从 $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$ 变为 $\\hat{\\beta}'_0 = \\bar{y}$。\n-   截距标准误：从约 $1.17$ 下降到 $0.30$。\n-   斜率估计值：不变 ($\\hat{\\beta}'_1 = \\hat{\\beta}_1$)。\n-   斜率标准误：不变。\n-   斜率t统计量：不变。\n-   残差和 $\\hat{\\sigma}^2$：不变。\n-   自由度：保持为 $n-2$，因为两个模型都估计了两个参数。\n\n### 逐项分析选项\n\n**A. 中心化后，截距标准误不变，但斜率估计值及其t统计量会发生变化，因为中心化改变了 $x$ 的散布程度。**\n-   截距标准误显著改变（从约 $1.17$ 下降到 $0.30$）。此陈述不正确。\n-   斜率估计值及其t统计量不变。此陈述不正确。\n-   中心化是位置平移，不改变散布程度 ($S_{xx}$)。此推理不正确。\n结论：**错误**。\n\n**B. 中心化后，截距标准误从约 $1.17$ 下降到约 $0.30$，因为预测变量的均值变为零，而斜率的t统计量保持不变；中心化不改变与斜率相关的 $S_{xx}$ 或残差。**\n-   根据我们的计算，截距标准误从约 $1.17$ 下降到约 $0.30$ 的陈述在数量上是正确的。\n-   给出的原因，即预测变量均值变为零，是对截距方差公式变化的正确解释。\n-   斜率t统计量不变的陈述是正确的。\n-   中心化不改变 $S_{xx}$ 或残差的推理也是正确的，并且是斜率统计量保持不变的根本原因。\n结论：**正确**。\n\n**C. 中心化后，截距和斜率的标准误都除以 $\\sqrt{n}$，相应的两个t统计量都会增加。**\n-   斜率标准误不变，而不是除以 $\\sqrt{n}$。\n-   截距标准误从 $\\sqrt{\\hat{\\sigma}^2 (1/n + \\bar{x}^2/S_{xx})}$ 变为 $\\sqrt{\\hat{\\sigma}^2/n}$，这并非简单的除以 $\\sqrt{n}$。\n-   斜率t统计量不变，而不是增加。\n结论：**错误**。\n\n**D. 中心化后，斜率标准误增加，因为当 $x$ 移动时 $S_{xx}$ 减小，导致t统计量变小，而截距标准误保持不变。**\n-   斜率标准误不变，而不是增加。\n-   $S_{xx}$ 不变，而不是减小。\n-   斜率t统计量不变，而不是变小。\n-   截距标准误减小，而不是保持不变。\n-   这个陈述的每一部分都是错误的。\n结论：**错误**。\n\n**E. 中心化后，斜率的置信区间变宽，因为自由度从 $n-2$ 变为 $n-1$，而截距的置信区间略微变窄。**\n-   斜率的置信区间 $\\hat{\\beta}_1 \\pm t_{n-2, \\alpha/2} SE(\\hat{\\beta}_1)$ 不变。\n-   自由度保持为 $n-2$，因为模型仍然有一个截距和一个斜率，总共估计了两个参数。\n-   截距的置信区间是显著变窄，而不是略微变窄。标准误减少了将近4倍（$1.17/0.3 \\approx 3.9$）。\n结论：**错误**。",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "我们的置信区间依赖于关于数据的假设，但如果某个观测值 $(x_i, y_i)$ 对整个模型产生了不成比例的影响，情况会怎样呢？这个编程练习将介绍一个关键的诊断工具——库克距离 (Cook's distance)，用以识别此类影响点。通过编程移除影响最大的数据点并重新拟合模型，你将直接观察到单个观测值如何能极大地改变系数的置信区间，从而学习到模型诊断与稳健性方面至关重要的一课。",
            "id": "3176663",
            "problem": "给定一系列独立的数据集，每个数据集都包含成对的观测值 $(x_i, y_i)$，适用于带截距项的简单线性回归。您的任务是编写一个完整的程序，对每个数据集进行定量分析，研究移除单个最具影响力的观测值（通过 Cook 距离衡量）后，斜率系数 $ \\beta_1 $ 的双侧 $ 95 $ 置信区间会如何变化。分析必须基于标准的简单线性回归模型，并且必须从第一性原理出发，即从普通最小二乘法和投影（帽子）矩阵的定义开始，并使用 Gauss–Markov 框架的经典假设。您只能使用以下基本事实：线性模型设定、最小二乘法正规方程、投影矩阵的性质、无偏残差方差估计量以及用于有限样本推断的 Student's $t$ 分布。\n\n对于每个数据集，仅使用这些基本事实执行以下操作：\n- 拟合简单线性回归模型 $ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i $，其中 $ \\varepsilon_i $ 独立同分布，均值为 $ 0 $，方差为常数 $ \\sigma^2 $。\n- 从帽子矩阵的对角线计算杠杆值 $ h_i $，计算残差 $ e_i $，并使用自由度为 $ n - p $ 的无偏估计量计算均方误差（MSE），其中 $ n $ 是样本量，$ p $ 是包括截距项在内的回归参数数量。\n- 根据这些量，计算每个观测值的 Cook 距离 $ D_i $，将具有最大 $ D_i $ 值的观测值确定为最具影响力的观测值，并记录在完整拟合下该观测值的杠杆值 $ h_i $、绝对残差 $ |e_i| $ 和 Cook 距离 $ D_i $。\n- 在完整数据下构建 $ \\beta_1 $ 的双侧 $ 95 $ 置信区间并计算其总宽度。然后移除最具影响力的观测值，重新拟合模型，并计算新的 $ \\beta_1 $ 双侧 $ 95 $ 置信区间宽度。\n- 对于每个数据集，按此确切顺序返回一个包含 $ 6 $ 个值的列表：$[\\text{width\\_before}, \\text{width\\_after}, \\text{narrower}, h_i, |e_i|, D_i]$，其中如果移除后区间宽度减小，则 $ \\text{narrower} $ 为 $ 1 $，否则为 $ 0 $。所有浮点值必须四舍五入到 $ 6 $ 位小数。\n\n测试套件：\n- 案例 $ 1 $（高杠杆值和大残差异常值）：\n  - $ x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20] $\n  - $ y = [1.0, 3.0, 5.1, 7.0, 9.2, 11.1, 13.0, 15.1, 17.2, 19.0, 21.1, 10.0] $\n- 案例 $ 2 $（高杠杆值但小残差值）：\n  - $ x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 20] $\n  - $ y = [1.2, 3.1, 5.0, 7.1, 9.0, 11.2, 13.2, 15.1, 17.0, 19.1, 40.9] $\n- 案例 $ 3 $（大残差和中等杠杆值）：\n  - $ x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] $\n  - $ y = [1.0, 3.1, 5.0, 7.2, 9.0, 50.0, 13.1, 15.0, 17.2, 19.1, 21.0] $\n- 案例 $ 4 $（小样本含极端 $ x $ 值）：\n  - $ x = [0, 1, 2, 10, 11] $\n  - $ y = [1.0, 3.0, 5.2, 15.0, 23.1] $\n\n实现要求：\n- 您的实现必须显式地构建必要的线性代数对象，并且必须使用 Student's $ t $ 分布来确定 $ \\beta_1 $ 的双侧 $ 95 $ 置信区间。\n- 在所有拟合中使用 $ p = 2 $ 个参数（截距和斜率）。\n- 双侧区间的水平使用 $ \\alpha = 0.05 $。\n- 将所有浮点输出四舍五入到 $ 6 $ 位小数。\n- 最终程序必须生成单行输出，其中包含所有案例的结果，格式为一个由方括号括起来的列表的逗号分隔列表，例如 $ [r_1, r_2, r_3, r_4] $，其中每个 $ r_k $ 是上面按指定顺序描述的 $ 6 $ 元组列表。",
            "solution": "该问题要求对简单线性回归中的影响点进行分析。分析将基于普通最小二乘法 (OLS) 框架，从第一性原理出发。我们将首先建立理论基础，然后概述计算过程。\n\n简单线性回归模型设定如下：\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\quad \\text{for } i = 1, \\dots, n $$\n其中 $y_i$ 是响应变量，$x_i$ 是预测变量，$\\beta_0$ 是截距，$\\beta_1$ 是斜率，$\\varepsilon_i$ 是独立同分布的误差项，其均值 $E[\\varepsilon_i] = 0$，方差为常数 $\\text{Var}(\\varepsilon_i) = \\sigma^2$。\n\n用矩阵表示法，模型表示为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中：\n$$\n\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ \\vdots  \\vdots \\\\ 1  x_n \\end{pmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}, \\quad\n\\boldsymbol{\\varepsilon} = \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix}\n$$\n设计矩阵 $\\mathbf{X}$ 有 $n$ 行和 $p=2$ 列。\n\nOLS 估计量 $\\hat{\\boldsymbol{\\beta}}$ 是通过最小化残差平方和 (SSE) 求得的，即 $\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$。对 $\\boldsymbol{\\beta}$ 求导并令其为零，得到正规方程：\n$$ (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y} $$\n假设 $\\mathbf{X}^T\\mathbf{X}$ 可逆，则 $\\boldsymbol{\\beta}$ 的 OLS 估计量为：\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n\n由此，我们定义几个关键量。拟合值为 $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbfX^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$。矩阵 $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$ 是一个投影矩阵，被称为帽子矩阵，因为它将“帽子”戴在 $\\mathbf{y}$ 上。\n第 $i$ 个观测值的杠杆值 $h_i$ 是帽子矩阵的第 $i$ 个对角元素 $H_{ii}$。它衡量了观测响应 $y_i$ 对其自身拟合值 $\\hat{y}_i$ 的影响，因为 $\\frac{\\partial \\hat{y}_i}{\\partial y_i} = h_i$。残差向量为 $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}$。\n\n误差方差 $\\sigma^2$ 的一个无偏估计量是均方误差 (MSE)：\n$$ s^2 = \\text{MSE} = \\frac{\\mathbf{e}^T\\mathbf{e}}{n-p} $$\n其中分母 $n-p$ 代表残差的自由度。对于本问题，$p=2$。\n\n为了识别影响点，我们使用 Cook 距离 $D_i$。它衡量删除观测值 $i$ 对整个估计系数向量的影响。可以使用完整模型拟合的量来高效地计算它：\n$$ D_i = \\frac{e_i^2}{p \\cdot \\text{MSE}} \\left[ \\frac{h_i}{(1-h_i)^2} \\right] $$\n较大的 $D_i$ 值表示观测值 $i$ 的影响力更大。具有最大 Cook 距离的观测值被认为是“最具影响力的”。\n\n对于斜率系数 $\\beta_1$ 的统计推断，我们使用其抽样分布。$\\hat{\\boldsymbol{\\beta}}$ 的方差-协方差矩阵是 $\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}$。我们用 $s^2$ 替换 $\\sigma^2$ 来估计这个矩阵。$\\hat{\\beta}_1$ 的估计方差是 $s^2(\\mathbf{X}^T\\mathbf{X})^{-1}$ 中第二行第二列的元素（使用基于 1 的索引），我们使用基于 0 的索引将其表示为 $s^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{11}$。$\\hat{\\beta}_1$ 的标准误是该值的平方根：\n$$ \\text{SE}(\\hat{\\beta}_1) = \\sqrt{s^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{11}} = s \\sqrt{[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{11}} $$\n枢轴量 $\\frac{\\hat{\\beta}_1 - \\beta_1}{\\text{SE}(\\hat{\\beta}_1)}$ 服从自由度为 $n-p$ 的 Student's $t$ 分布。$\\beta_1$ 的一个双侧 $100(1-\\alpha)\\%$ 置信区间构造如下：\n$$ \\hat{\\beta}_1 \\pm t_{1-\\alpha/2, n-p} \\cdot \\text{SE}(\\hat{\\beta}_1) $$\n其中 $t_{1-\\alpha/2, n-p}$ 是来自 $t$ 分布的上侧临界值。对于 $95\\%$ 的置信区间，$\\alpha=0.05$。该区间的总宽度为 $W = 2 \\cdot t_{0.975, n-p} \\cdot \\text{SE}(\\hat{\\beta}_1)$。\n\n每个数据集的处理流程如下：\n1.  对于完整数据集 $(x_i, y_i)_{i=1}^n$，构造设计矩阵 $\\mathbf{X}$ 并为所有 $i$ 计算 $\\hat{\\boldsymbol{\\beta}}$、$\\mathbf{H}$、$\\mathbf{e}$、$s^2$ 和 $D_i$。\n2.  确定对应于最大 Cook 距离 $\\max(D_i)$ 的索引 $i^*$。记录 $h_{i^*}$、 $|e_{i^*}|$ 和 $D_{i^*}$。\n3.  计算标准误 $\\text{SE}(\\hat{\\beta}_1)$ 和 $t$-临界值 $t_{0.975, n-2}$，以求出 $\\beta_1$ 的 $95\\%$ 置信区间的宽度，记为 `width_before`。\n4.  移除影响点 $(x_{i^*}, y_{i^*})$，形成一个大小为 $n-1$ 的新数据集。\n5.  在缩减后的数据集上重新拟合回归模型。这包括重新计算设计矩阵 $\\mathbf{X}'$、系数估计值 $\\hat{\\boldsymbol{\\beta}}'$、残差方差 $(s')^2$、标准误 $\\text{SE}(\\hat{\\beta}_1')$ 和新的 $t$-临界值 $t_{0.975, n-1-2}$。\n6.  计算新的 $95\\%$ 置信区间的宽度 `width_after`。\n7.  比较宽度，如果 `width_after` 小于 `width_before`，则将标志 `narrower` 设置为 $1$，否则设置为 $0$。\n8.  返回六个所需的值，四舍五入到六位小数。\n\n对所提供的四个测试案例中的每一个都实施了这整个过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20], dtype=np.float64),\n         np.array([1.0, 3.0, 5.1, 7.0, 9.2, 11.1, 13.0, 15.1, 17.2, 19.0, 21.1, 10.0], dtype=np.float64)),\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 20], dtype=np.float64),\n         np.array([1.2, 3.1, 5.0, 7.1, 9.0, 11.2, 13.2, 15.1, 17.0, 19.1, 40.9], dtype=np.float64)),\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64),\n         np.array([1.0, 3.1, 5.0, 7.2, 9.0, 50.0, 13.1, 15.0, 17.2, 19.1, 21.0], dtype=np.float64)),\n        (np.array([0, 1, 2, 10, 11], dtype=np.float64),\n         np.array([1.0, 3.0, 5.2, 15.0, 23.1], dtype=np.float64)),\n    ]\n\n    results = []\n    for x, y in test_cases:\n        result = analyze_influence(x, y)\n        results.append(result)\n    \n    # Format the final output string\n    output_str = \"[\" + ','.join([f\"[{','.join(map(str, r))}]\" for r in results]) + \"]\"\n    print(output_str)\n\ndef get_regression_stats(x, y, alpha=0.05):\n    \"\"\"\n    Performs simple linear regression and returns key statistics.\n    \"\"\"\n    n = len(x)\n    p = 2  # Number of parameters: intercept and slope\n\n    # Construct the design matrix X\n    X = np.c_[np.ones(n), x]\n\n    # Calculate beta_hat using normal equations: (X'X)b = X'y\n    # This is more numerically stable than computing the inverse directly for beta.\n    try:\n        XTX = X.T @ X\n        XTy = X.T @ y\n        beta_hat = np.linalg.solve(XTX, XTy)\n    except np.linalg.LinAlgError:\n        # Handle cases of singular matrix, though not expected with given data\n        return None\n\n    # Calculate hat matrix, residuals, and MSE\n    XTX_inv = np.linalg.inv(XTX)\n    H = X @ XTX_inv @ X.T\n    leverages = np.diag(H)\n    residuals = y - X @ beta_hat\n    mse = (residuals.T @ residuals) / (n - p)\n    \n    # Calculate Cook's distance\n    cooks_d = (residuals**2 / (p * mse)) * (leverages / (1 - leverages)**2)\n    \n    # Calculate confidence interval width for the slope (beta_1)\n    var_beta_hat = mse * XTX_inv\n    se_beta1 = np.sqrt(var_beta_hat[1, 1])\n    t_critical = t.ppf(1 - alpha/2, n - p)\n    ci_width = 2 * t_critical * se_beta1\n    \n    return {\n        'beta_hat': beta_hat,\n        'leverages': leverages,\n        'residuals': residuals,\n        'mse': mse,\n        'cooks_d': cooks_d,\n        'ci_width_beta1': ci_width\n    }\n\ndef analyze_influence(x, y, alpha=0.05):\n    \"\"\"\n    Analyzes the influence of the most influential point on B1's CI.\n    \"\"\"\n    # 1. Fit model on full data\n    full_fit_stats = get_regression_stats(x, y, alpha)\n    if full_fit_stats is None:\n        return [0.0] * 6 # Should not happen\n\n    width_before = full_fit_stats['ci_width_beta1']\n    \n    # 2. Identify most influential observation\n    cooks_d = full_fit_stats['cooks_d']\n    influential_idx = np.argmax(cooks_d)\n    \n    # 3. Record its stats\n    h_i = full_fit_stats['leverages'][influential_idx]\n    abs_e_i = np.abs(full_fit_stats['residuals'][influential_idx])\n    d_i = cooks_d[influential_idx]\n\n    # 4. Remove the influential observation and refit\n    x_reduced = np.delete(x, influential_idx)\n    y_reduced = np.delete(y, influential_idx)\n    \n    reduced_fit_stats = get_regression_stats(x_reduced, y_reduced, alpha)\n    if reduced_fit_stats is None:\n        width_after = 0.0 # Should not happen\n    else:\n        width_after = reduced_fit_stats['ci_width_beta1']\n    \n    # 5. Compare CI widths\n    narrower = 1 if width_after  width_before else 0\n\n    # 6. Format and return results\n    return [\n        round(width_before, 6),\n        round(width_after, 6),\n        narrower,\n        round(h_i, 6),\n        round(abs_e_i, 6),\n        round(d_i, 6)\n    ]\n\n# Execute the main function\nsolve()\n\n```"
        }
    ]
}