{
    "hands_on_practices": [
        {
            "introduction": "This practice explores a common and simple data pre-processing step: centering a predictor variable. While centering may seem like a trivial shift, it has profound consequences for the interpretation and precision of the model's intercept term, $\\hat{\\beta}_0$. This exercise will guide you through a quantitative analysis to see exactly how and why centering dramatically improves the stability of the intercept's estimate, while leaving the inference for the slope, $\\hat{\\beta}_1$, unchanged. ",
            "id": "3176657",
            "problem": "A data analyst fits an Ordinary Least Squares (OLS) simple linear regression model to fixed-design data, assuming the classical linear model conditions: for $i=1,\\dots,n$, the responses satisfy $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ where the errors $\\varepsilon_i$ are independent and identically distributed with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ and the predictor values $x_i$ are nonrandom. The analyst reports the following summary for the original (uncentered) model:\n- Sample size $n = 100$.\n- Predictor mean $\\bar{x} = 75$.\n- Predictor sum of squares about the mean $S_{xx} = \\sum_{i=1}^{n}(x_i - \\bar{x})^2 = 39600$.\n- Estimated slope $\\hat{\\beta}_1 = 0.45$.\n- Residual mean squared error $\\hat{\\sigma}^2 = 9$.\n\nThe analyst then centers the predictor by defining $x_i^{c} = x_i - \\bar{x}$ and refits the regression $y_i$ on an intercept and $x_i^{c}$ using OLS under the same assumptions.\n\nWhich statement best captures the quantitative and qualitative effects of centering on the estimated intercept and the slope $t$-statistic, and explains why?\n\nA. After centering, the intercept standard error is unchanged, but the slope estimate and its $t$-statistic change because centering modifies the spread of $x$.\n\nB. After centering, the intercept standard error drops from approximately $1.17$ to approximately $0.30$ because the predictor mean becomes zero, while the slope $t$-statistic is unchanged; centering does not alter $S_{xx}$ or the residuals relevant to the slope.\n\nC. After centering, both the intercept and slope standard errors are divided by $\\sqrt{n}$, and both $t$-statistics increase accordingly.\n\nD. After centering, the slope standard error increases because $S_{xx}$ decreases when $x$ is shifted, leading to a smaller $t$-statistic, while the intercept standard error remains the same.\n\nE. After centering, the slope confidence interval widens because the degrees of freedom change from $n-2$ to $n-1$, whereas the intercept confidence interval narrows slightly.",
            "solution": "The user wants me to analyze the effects of centering a predictor variable on the parameters and test statistics of a simple linear regression model.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Model (Original): $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, where $i=1,\\dots,n$.\n-   Assumptions: Errors $\\varepsilon_i$ are independent and identically distributed, with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$. Predictor values $x_i$ are nonrandom (fixed design).\n-   Sample size: $n = 100$.\n-   Predictor mean: $\\bar{x} = 75$.\n-   Predictor sum of squares about the mean: $S_{xx} = \\sum_{i=1}^{n}(x_i - \\bar{x})^2 = 39600$.\n-   Estimated slope: $\\hat{\\beta}_1 = 0.45$.\n-   Residual mean squared error: $\\hat{\\sigma}^2 = 9$.\n-   Centered predictor: $x_i^{c} = x_i - \\bar{x}$.\n-   Model (Centered): $y_i = \\beta'_0 + \\beta'_1 x_i^c + \\varepsilon'_i$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is based on the theory of Ordinary Least Squares (OLS) regression, a cornerstone of classical statistics. The assumptions are standard for the classical linear model. The setup is scientifically and mathematically sound.\n-   **Well-Posed:** The problem provides all necessary numerical values and model definitions to compare the statistical properties of the original and centered regression models. A unique, meaningful solution can be derived.\n-   **Objective:** The problem is stated using precise, standard statistical terminology, leaving no room for subjective interpretation.\n\nNo flaws are identified. The problem is not unsound, incomplete, unrealistic, or ill-posed.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the derivation and solution.\n\n### Derivation\n\nLet us first analyze the original (uncentered) model and then the centered model. We will denote quantities from the centered model with a prime (e.g., $\\hat{\\beta}'_0$).\n\n**1. Analysis of the Original (Uncentered) Model**\n\nThe OLS estimators for the parameters are given by:\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\nThe estimated variances of these estimators (under the given assumptions) are:\n$$ \\widehat{\\text{Var}}(\\hat{\\beta}_1) = \\frac{\\hat{\\sigma}^2}{S_{xx}} $$\n$$ \\widehat{\\text{Var}}(\\hat{\\beta}_0) = \\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right) $$\nThe standard errors ($SE$) are the square roots of these estimated variances.\n\nUsing the provided data:\n-   $n = 100$\n-   $\\bar{x} = 75$\n-   $S_{xx} = 39600$\n-   $\\hat{\\beta}_1 = 0.45$\n-   $\\hat{\\sigma}^2 = 9$\n\nWe can calculate the standard error of the intercept, $SE(\\hat{\\beta}_0)$:\n$$ SE(\\hat{\\beta}_0) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\beta}_0)} = \\sqrt{\\hat{\\sigma}^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}} \\right)} = \\sqrt{9 \\left( \\frac{1}{100} + \\frac{75^2}{39600} \\right)} $$\n$$ SE(\\hat{\\beta}_0) = \\sqrt{9 \\left( 0.01 + \\frac{5625}{39600} \\right)} \\approx \\sqrt{9 (0.01 + 0.142045)} = \\sqrt{9(0.152045)} = \\sqrt{1.368409} \\approx 1.16979 $$\nSo, the original intercept standard error is approximately $1.17$.\n\nThe standard error of the slope, $SE(\\hat{\\beta}_1)$, is:\n$$ SE(\\hat{\\beta}_1) = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = \\sqrt{\\frac{9}{39600}} = \\sqrt{\\frac{1}{4400}} \\approx 0.015076 $$\nThe $t$-statistic for the slope, $t_{\\hat{\\beta}_1}$, is:\n$$ t_{\\hat{\\beta}_1} = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} = \\frac{0.45}{\\sqrt{9/39600}} = 0.45 \\times \\sqrt{4400} \\approx 29.85 $$\n\n**2. Analysis of the Centered Model**\n\nThe new predictor is $x_i^c = x_i - \\bar{x}$.\nThe mean of the centered predictor is $\\bar{x}^c = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x}) = \\bar{x} - \\bar{x} = 0$.\nThe sum of squares of the centered predictor about its mean is:\n$$ S_{x^c x^c} = \\sum_{i=1}^n (x_i^c - \\bar{x}^c)^2 = \\sum_{i=1}^n (x_i - \\bar{x} - 0)^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2 = S_{xx} = 39600 $$\nThus, centering does not change the sum of squares $S_{xx}$.\n\nNow, let's find the new OLS estimators, $\\hat{\\beta}'_0$ and $\\hat{\\beta}'_1$:\n$$ \\hat{\\beta}'_1 = \\frac{\\sum_{i=1}^n (x_i^c - \\bar{x}^c)(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i^c - \\bar{x}^c)^2} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{S_{xx}} = \\hat{\\beta}_1 $$\nThe slope estimate is invariant to centering the predictor. So, $\\hat{\\beta}'_1 = 0.45$.\n\n$$ \\hat{\\beta}'_0 = \\bar{y} - \\hat{\\beta}'_1 \\bar{x}^c = \\bar{y} - \\hat{\\beta}_1 (0) = \\bar{y} $$\nThe new intercept estimate is the sample mean of the response variable.\n\nThe fitted values for the centered model are $\\hat{y}'_i = \\hat{\\beta}'_0 + \\hat{\\beta}'_1 x_i^c = \\bar{y} + \\hat{\\beta}_1 (x_i - \\bar{x})$.\nThe fitted values for the original model are $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i = (\\bar{y} - \\hat{\\beta}_1 \\bar{x}) + \\hat{\\beta}_1 x_i = \\bar{y} + \\hat{\\beta}_1 (x_i - \\bar{x})$.\nSince $\\hat{y}'_i = \\hat{y}_i$ for all $i$, the residuals $e'_i = y_i - \\hat{y}'_i$ and $e_i = y_i - \\hat{y}_i$ are identical.\nThis implies that the Residual Sum of Squares (RSS) is unchanged, and therefore the residual mean squared error $\\hat{\\sigma}'^2 = \\frac{\\text{RSS}}{n-2}$ is also unchanged. Thus, $\\hat{\\sigma}'^2 = \\hat{\\sigma}^2 = 9$.\n\nNow we compute the standard errors for the centered model:\nThe standard error of the new slope, $SE(\\hat{\\beta}'_1)$, is:\n$$ SE(\\hat{\\beta}'_1) = \\sqrt{\\frac{\\hat{\\sigma}'^2}{S_{x^c x^c}}} = \\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}} = SE(\\hat{\\beta}_1) $$\nThe slope standard error is unchanged.\n\nThe standard error of the new intercept, $SE(\\hat{\\beta}'_0)$, is:\n$$ SE(\\hat{\\beta}'_0) = \\sqrt{\\hat{\\sigma}'^2 \\left( \\frac{1}{n} + \\frac{(\\bar{x}^c)^2}{S_{x^c x^c}} \\right)} = \\sqrt{9 \\left( \\frac{1}{100} + \\frac{0^2}{39600} \\right)} = \\sqrt{9 \\left( \\frac{1}{100} \\right)} = \\sqrt{0.09} = 0.30 $$\nThe intercept standard error drops from approximately $1.17$ to exactly $0.30$. This reduction occurs because the term involving $\\bar{x}^2$ in the variance formula vanishes. Geometrically, the intercept in the centered model represents the predicted value at the mean of the predictor, where estimation is most precise. In the original model, it represents the predicted value at $x=0$, which is far from the data's center ($\\bar{x}=75$), requiring a long extrapolation.\n\nFinally, the $t$-statistic for the centered slope, $t_{\\hat{\\beta}'_1}$, is:\n$$ t_{\\hat{\\beta}'_1} = \\frac{\\hat{\\beta}'_1}{SE(\\hat{\\beta}'_1)} = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} = t_{\\hat{\\beta}_1} $$\nThe slope $t$-statistic is unchanged because neither the slope estimate nor its standard error changes.\n\n**Summary of Effects:**\n-   Intercept Estimate: Changes from $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$ to $\\hat{\\beta}'_0 = \\bar{y}$.\n-   Intercept Standard Error: Drops from $\\approx 1.17$ to $0.30$.\n-   Slope Estimate: Unchanged ($\\hat{\\beta}'_1 = \\hat{\\beta}_1$).\n-   Slope Standard Error: Unchanged.\n-   Slope $t$-statistic: Unchanged.\n-   Residuals and $\\hat{\\sigma}^2$: Unchanged.\n-   Degrees of freedom: Remain $n-2$ as two parameters are estimated in both models.\n\n### Option-by-Option Analysis\n\n**A. After centering, the intercept standard error is unchanged, but the slope estimate and its $t$-statistic change because centering modifies the spread of $x$.**\n-   The intercept standard error changes significantly (drops from $\\approx 1.17$ to $0.30$). This statement is incorrect.\n-   The slope estimate and its $t$-statistic are unchanged. This statement is incorrect.\n-   Centering is a location shift, which does not modify the spread ($S_{xx}$). This reasoning is incorrect.\nVerdict: **Incorrect**.\n\n**B. After centering, the intercept standard error drops from approximately $1.17$ to approximately $0.30$ because the predictor mean becomes zero, while the slope $t$-statistic is unchanged; centering does not alter $S_{xx}$ or the residuals relevant to the slope.**\n-   The statement that the intercept standard error drops from $\\approx 1.17$ to $\\approx 0.30$ is quantitatively correct, based on our calculations.\n-   The reason given, that the predictor mean becomes zero, is the correct explanation for the change in the intercept's variance formula.\n-   The statement that the slope $t$-statistic is unchanged is correct.\n-   The reasoning that centering does not alter $S_{xx}$ or the residuals is also correct and fundamental to why the slope statistics are invariant.\nVerdict: **Correct**.\n\n**C. After centering, both the intercept and slope standard errors are divided by $\\sqrt{n}$, and both $t$-statistics increase accordingly.**\n-   The slope standard error is unchanged, not divided by $\\sqrt{n}$.\n-   The intercept standard error changes from $\\sqrt{\\hat{\\sigma}^2 (1/n + \\bar{x}^2/S_{xx})}$ to $\\sqrt{\\hat{\\sigma}^2/n}$, which is not a simple division by $\\sqrt{n}$.\n-   The slope $t$-statistic is unchanged, not increased.\nVerdict: **Incorrect**.\n\n**D. After centering, the slope standard error increases because $S_{xx}$ decreases when $x$ is shifted, leading to a smaller $t$-statistic, while the intercept standard error remains the same.**\n-   The slope standard error is unchanged, not increased.\n-   $S_{xx}$ is unchanged, not decreased.\n-   The slope $t$-statistic is unchanged, not smaller.\n-   The intercept standard error decreases, not remains the same.\nEvery part of this statement is incorrect.\nVerdict: **Incorrect**.\n\n**E. After centering, the slope confidence interval widens because the degrees of freedom change from $n-2$ to $n-1$, whereas the intercept confidence interval narrows slightly.**\n-   The slope confidence interval, $\\hat{\\beta}_1 \\pm t_{n-2, \\alpha/2} SE(\\hat{\\beta}_1)$, is unchanged.\n-   The degrees of freedom remain $n-2$ because the model still has an intercept and a slope, for a total of two estimated parameters.\n-   The intercept confidence interval narrows substantially, not slightly. The standard error reduces by a factor of nearly $4$ ($1.17/0.3 \\approx 3.9$).\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Building on the idea of model specification, we now investigate a critical error known as the \"dummy variable trap.\" This occurs when we include redundant categorical predictors, leading to perfect multicollinearity and making it impossible to uniquely estimate the model coefficients. This practice problem demonstrates why the standard errors become infinite in this scenario and how a simple reparameterization restores our ability to calculate valid confidence intervals for meaningful comparisons, such as the difference between group means. ",
            "id": "3176621",
            "problem": "A data analyst models hourly wages using a linear regression with an intercept and dummy variables for group membership. There are two groups, $A$ and $B$, and the analyst encodes membership with dummy variables $D_A$ and $D_B$, where $D_A=1$ for group $A$ and $0$ otherwise, and $D_B=1$ for group $B$ and $0$ otherwise. The observed data are:\n- Group $A$ wages: $y=(10,12,11)$,\n- Group $B$ wages: $y=(7,9,8)$,\nso the total sample size is $n=6$. The analyst first fits a model with an intercept and both dummies, and then considers reparameterizations to resolve any potential identifiability issues.\n\nSelect all statements that are correct about the existence of $(X^{\\top}X)^{-1}$, the identifiability of regression coefficients, and how reparameterization restores identifiable standard errors and confidence intervals for regression coefficients in this setting. You may assume a homoskedastic error model and use the usual $t$-based confidence intervals at the $95\\%$ level with residual degrees of freedom corresponding to the fitted model.\n\nA. With an intercept and both $D_A$ and $D_B$ included, the design matrix columns are perfectly collinear, so $X^{\\top}X$ is singular and $(X^{\\top}X)^{-1}$ does not exist.\n\nB. Reparameterizing to use an intercept and a single dummy (e.g., drop $D_B$ and keep $D_A$) yields a full column rank design, and the $95\\%$ two-sided confidence interval for the coefficient on $D_A$ (the mean difference $A$ minus $B$) is approximately $[0.73,\\,5.27]$.\n\nC. Alternatively, dropping the intercept and keeping both $D_A$ and $D_B$ also yields a full column rank design, and the $95\\%$ two-sided confidence interval for $\\beta_A-\\beta_B$ is identical to that in option B.\n\nD. Even without reparameterization, one can still obtain unique, software-independent standard errors and $95\\%$ confidence intervals for the individual coefficients on $D_A$ and $D_B$ in the overparameterized model with an intercept and both dummies, because ordinary least squares automatically ignores redundant columns.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective. It provides a standard, text-book scenario for demonstrating the concepts of multicollinearity, model parameterization, and estimability in linear regression. All data and conditions required for analysis are present and consistent. I will proceed with a full derivation and evaluation of each option.\n\nThe response vector $y$ combines the wages from both groups:\n$$y = (10, 12, 11, 7, 9, 8)^{\\top}$$\nThe dummy variables are defined as $D_A=1$ for group $A$ observations (the first three) and $D_A=0$ for group $B$ observations (the last three), and $D_B=1$ for group $B$ and $D_B=0$ for group $A$.\n\nA. With an intercept and both $D_A$ and $D_B$ included, the design matrix columns are perfectly collinear, so $X^{\\top}X$ is singular and $(X^{\\top}X)^{-1}$ does not exist.\n\nThe model is $y_i = \\beta_0 + \\beta_A D_{Ai} + \\beta_B D_{Bi} + \\epsilon_i$. The design matrix $X$ has $n=6$ rows and $p=3$ columns (one for the intercept $\\beta_0$, one for $\\beta_A$, and one for $\\beta_B$).\nThe columns are an intercept vector $c_1$, a vector for $D_A$, $c_2$, and a vector for $D_B$, $c_3$:\n$$\nc_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nc_2 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nc_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe design matrix is $X = [c_1, c_2, c_3]$. A linear dependency exists among these columns because for every observation, an individual is in either group $A$ or group $B$, meaning $D_A + D_B = 1$. Thus, the sum of the dummy variable columns equals the intercept column:\n$$c_2 + c_3 = c_1 \\implies 1 \\cdot c_1 - 1 \\cdot c_2 - 1 \\cdot c_3 = \\mathbf{0}$$\nThis perfect collinearity means the columns of $X$ are linearly dependent, and the matrix $X$ is not of full column rank. The rank of $X$ is $2$, not $3$.\nConsequently, the matrix $X^{\\top}X$ is singular (not invertible). We can compute it to verify:\n$$\nX^{\\top}X = \\begin{pmatrix}\n1  1  1  1  1  1 \\\\\n1  1  1  0  0  0 \\\\\n0  0  0  1  1  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1  0 \\\\\n1  1  0 \\\\\n1  1  0 \\\\\n1  0  1 \\\\\n1  0  1 \\\\\n1  0  1\n\\end{pmatrix}\n= \\begin{pmatrix}\n6  3  3 \\\\\n3  3  0 \\\\\n3  0  3\n\\end{pmatrix}\n$$\nThe determinant of this matrix is $\\det(X^{\\top}X) = 6(3 \\cdot 3 - 0) - 3(3 \\cdot 3 - 0) + 3(0 - 3 \\cdot 3) = 54 - 27 - 27 = 0$. Since the determinant is zero, $X^{\\top}X$ is singular, and its inverse $(X^{\\top}X)^{-1}$ does not exist. This situation is known as the dummy variable trap. The coefficients $\\beta_0, \\beta_A, \\beta_B$ are not uniquely identifiable.\n\nThe statement is **Correct**.\n\nB. Reparameterizing to use an intercept and a single dummy (e.g., drop $D_B$ and keep $D_A$) yields a full column rank design, and the $95\\%$ two-sided confidence interval for the coefficient on $D_A$ (the mean difference $A$ minus $B$) is approximately $[0.73,\\,5.27]$.\n\nThe reparameterized model is $y_i = \\beta'_0 + \\beta'_A D_{Ai} + \\epsilon'_i$. This is a standard approach to resolve the collinearity. The design matrix $X_1$ has two columns:\n$$\nX_1 = \\begin{pmatrix}\n1  1 \\\\\n1  1 \\\\\n1  1 \\\\\n1  0 \\\\\n1  0 \\\\\n1  0\n\\end{pmatrix}\n$$\nThese columns are linearly independent, so $X_1$ has full column rank (rank $2$), and $(X_1^{\\top}X_1)^{-1}$ exists.\nThe OLS estimator is $\\hat{\\beta}' = (X_1^{\\top}X_1)^{-1}X_1^{\\top}y$.\n$$\nX_1^{\\top}X_1 = \\begin{pmatrix} 6  3 \\\\ 3  3 \\end{pmatrix} \\quad \\implies \\quad (X_1^{\\top}X_1)^{-1} = \\frac{1}{18-9} \\begin{pmatrix} 3  -3 \\\\ -3  6 \\end{pmatrix} = \\begin{pmatrix} 1/3  -1/3 \\\\ -1/3  2/3 \\end{pmatrix}\n$$\n$$\nX_1^{\\top}y = \\begin{pmatrix} \\sum y_i \\\\ \\sum_{i \\in A} y_i \\end{pmatrix} = \\begin{pmatrix} 57 \\\\ 33 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}' = \\begin{pmatrix} \\hat{\\beta}'_0 \\\\ \\hat{\\beta}'_A \\end{pmatrix} = \\begin{pmatrix} 1/3  -1/3 \\\\ -1/3  2/3 \\end{pmatrix} \\begin{pmatrix} 57 \\\\ 33 \\end{pmatrix} = \\begin{pmatrix} (57-33)/3 \\\\ (-57+66)/3 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 3 \\end{pmatrix}\n$$\nIn this model, $\\beta'_0$ represents the mean wage for the reference group (group $B$, where $D_A=0$), and $\\beta'_A$ represents the difference in mean wages between group $A$ and group $B$. The sample mean for group $B$ is $(7+9+8)/3 = 8$, so $\\hat{\\beta}'_0=8$. The sample mean for group $A$ is $(10+12+11)/3 = 11$. The mean difference is $11-8=3$, so $\\hat{\\beta}'_A=3$. The coefficient interpretation in the statement is correct.\n\nTo find the confidence interval for $\\beta'_A$, we need its standard error. $SE(\\hat{\\beta}'_A) = \\sqrt{\\hat{\\sigma}^2 (X_1^{\\top}X_1)^{-1}_{22}}$.\nFirst, we find the residual sum of squares ($RSS$) to estimate the error variance $\\sigma^2$. The fitted values are $\\hat{y}_i = 8+3(1)=11$ for group $A$ and $\\hat{y}_i = 8+3(0)=8$ for group $B$.\n$$RSS = \\sum(y_i-\\hat{y}_i)^2 = (10-11)^2+(12-11)^2+(11-11)^2 + (7-8)^2+(9-8)^2+(8-8)^2 = 1+1+0+1+1+0 = 4$$\nThe number of parameters is $p=2$. The degrees of freedom for the residuals is $df=n-p=6-2=4$.\nThe unbiased estimate of the error variance is $\\hat{\\sigma}^2 = \\frac{RSS}{n-p} = \\frac{4}{4} = 1$.\nThe variance of $\\hat{\\beta}'_A$ is $\\text{Var}(\\hat{\\beta}'_A) = \\hat{\\sigma}^2 (X_1^{\\top}X_1)^{-1}_{22} = 1 \\cdot (2/3) = 2/3$.\nThe standard error is $SE(\\hat{\\beta}'_A) = \\sqrt{2/3}$.\nFor a $95\\%$ confidence interval with $df=4$, the critical t-value is $t_{0.025, 4} = 2.776$.\nThe confidence interval is $\\hat{\\beta}'_A \\pm t_{0.025, 4} \\cdot SE(\\hat{\\beta}'_A)$:\n$$3 \\pm 2.776 \\cdot \\sqrt{2/3} \\approx 3 \\pm 2.776 \\cdot 0.8165 \\approx 3 \\pm 2.266$$\nThis gives the interval $[0.734, 5.266]$, which rounds to $[0.73, 5.27]$.\n\nThe statement is **Correct**.\n\nC. Alternatively, dropping the intercept and keeping both $D_A$ and $D_B$ also yields a full column rank design, and the $95\\%$ two-sided confidence interval for $\\beta_A-\\beta_B$ is identical to that in option B.\n\nThe alternative model is $y_i = \\beta_A D_{Ai} + \\beta_B D_{Bi} + \\epsilon''_{i}$. The design matrix $X_2$ is:\n$$\nX_2 = \\begin{pmatrix}\n1  0 \\\\\n1  0 \\\\\n1  0 \\\\\n0  1 \\\\\n0  1 \\\\\n0  1\n\\end{pmatrix}\n$$\nThe columns are orthogonal and thus linearly independent. $X_2$ has full column rank.\nThe OLS estimator is $\\hat{\\beta}'' = (X_2^{\\top}X_2)^{-1}X_2^{\\top}y$.\n$$\nX_2^{\\top}X_2 = \\begin{pmatrix} 3  0 \\\\ 0  3 \\end{pmatrix} \\quad \\implies \\quad (X_2^{\\top}X_2)^{-1} = \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix}\n$$\n$$\nX_2^{\\top}y = \\begin{pmatrix} \\sum_{i \\in A} y_i \\\\ \\sum_{i \\in B} y_i \\end{pmatrix} = \\begin{pmatrix} 33 \\\\ 24 \\end{pmatrix}\n$$\n$$\n\\hat{\\beta}'' = \\begin{pmatrix} \\hat{\\beta}_A \\\\ \\hat{\\beta}_B \\end{pmatrix} = \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix} \\begin{pmatrix} 33 \\\\ 24 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 8 \\end{pmatrix}\n$$\nIn this model, $\\beta_A$ is the mean wage for group $A$ and $\\beta_B$ is the mean wage for group $B$. Their estimates are the respective sample means.\nWe are interested in the confidence interval for the difference $\\beta_A - \\beta_B$. The point estimate is $\\hat{\\beta}_A - \\hat{\\beta}_B = 11 - 8 = 3$. This is identical to the point estimate $\\hat{\\beta}'_A$ from model B.\nThe variance of this difference is $\\text{Var}(\\hat{\\beta}_A - \\hat{\\beta}_B) = \\text{Var}(\\hat{\\beta}_A) + \\text{Var}(\\hat{\\beta}_B) - 2\\text{Cov}(\\hat{\\beta}_A, \\hat{\\beta}_B)$.\nTo find the variance-covariance matrix of the coefficients, we first need $\\hat{\\sigma}^2$ for this model. The fitted values are $\\hat{y}_i = 11$ for group $A$ and $\\hat{y}_i = 8$ for group $B$. These are the same fitted values as in model B. Thus, the residuals and the $RSS$ are identical: $RSS=4$. The number of parameters is $p=2$, so $df=n-p=4$, which is also the same. Therefore, $\\hat{\\sigma}^2 = RSS/df = 4/4 = 1$.\nThe variance-covariance matrix is $\\text{Cov}(\\hat{\\beta}'') = \\hat{\\sigma}^2(X_2^{\\top}X_2)^{-1} = 1 \\cdot \\begin{pmatrix} 1/3  0 \\\\ 0  1/3 \\end{pmatrix}$.\nFrom this, $\\text{Var}(\\hat{\\beta}_A) = 1/3$, $\\text{Var}(\\hat{\\beta}_B) = 1/3$, and $\\text{Cov}(\\hat{\\beta}_A, \\hat{\\beta}_B) = 0$.\nSo, $\\text{Var}(\\hat{\\beta}_A - \\hat{\\beta}_B) = 1/3 + 1/3 - 0 = 2/3$.\nThe standard error of the difference is $SE(\\hat{\\beta}_A - \\hat{\\beta}_B) = \\sqrt{2/3}$.\nThe point estimate ($3$), standard error ($\\sqrt{2/3}$), and degrees of freedom ($4$) are all identical to those for $\\hat{\\beta}'_A$ in option B. Therefore, the $95\\%$ confidence interval for $\\beta_A-\\beta_B$ must be identical. Both models (in B and C) are valid reparameterizations spanning the same vector space of fitted values, and $\\beta_A-\\beta_B$ and $\\beta'_A$ are estimable functions representing the same physical quantity (mean difference), so their estimates and confidence intervals must be the same.\n\nThe statement is **Correct**.\n\nD. Even without reparameterization, one can still obtain unique, software-independent standard errors and $95\\%$ confidence intervals for the individual coefficients on $D_A$ and $D_B$ in the overparameterized model with an intercept and both dummies, because ordinary least squares automatically ignores redundant columns.\n\nAs established in A, the matrix $X^{\\top}X$ for the overparameterized model is singular. This means there is no unique solution for the coefficient vector $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_A, \\hat{\\beta}_B)^{\\top}$. The normal equations $X^{\\top}X\\hat{\\beta}=X^{\\top}y$ have infinitely many solutions.\nSpecifically, if $\\hat{\\beta}^*$ is one solution, then for any arbitrary scalar $c$, the vector $\\hat{\\beta}^* + c(1, -1, -1)^{\\top}$ is also a solution, since $(1, -1, -1)^{\\top}$ is in the null space of $X$.\nThe individual coefficients $\\beta_A$ and $\\beta_B$ are not \"estimable\" functions, meaning their estimates depend on an arbitrary constraint imposed to find a particular solution. Different statistical software packages impose different constraints (e.g., setting one coefficient to zero, using sum-to-zero contrasts), leading to different values for $\\hat{\\beta}_A$ and $\\hat{\\beta}_B$. For instance, setting $\\beta_B=0$ yields $\\hat{\\beta}_A=3$, while setting $\\beta_A=0$ yields $\\hat{\\beta}_B=-3$.\nSince the point estimates for $\\beta_A$ and $\\beta_B$ are not unique and are software-dependent, their standard errors and confidence intervals are also not uniquely defined or software-independent. The variance of $\\hat{\\beta}$ is formally given by $\\sigma^2(X^{\\top}X)^-$, where $(X^{\\top}X)^-$ is a generalized inverse of $X^{\\top}X$. The generalized inverse is not unique, and different choices lead to different (and meaningless) variances for non-estimable coefficients like $\\beta_A$ and $\\beta_B$.\nThe claim that OLS \"automatically ignores redundant columns\" is a crude description of what some software implementations do, but the choice of which column to ignore is arbitrary and affects the values of the individual coefficients. It is precisely because of this ambiguity that reparameterization is required to obtain meaningful, unique estimates and inference for interpretable parameters.\n\nThe statement is **Incorrect**.",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "Our final practice moves from model specification to data diagnostics, exploring how individual data points can exert a disproportionate influence on our regression results. You will implement a procedure to identify the most influential point using Cook's distance, a metric that combines a point's leverage and its residual size. By removing this single point and refitting the model, you will gain a hands-on understanding of how outliers can inflate standard errors and drastically alter the confidence interval for a slope coefficient, potentially changing the conclusions of your analysis. ",
            "id": "3176663",
            "problem": "You are given a sequence of independent data sets, each consisting of paired observations $(x_i, y_i)$ suitable for a simple linear regression with an intercept. Your task is to write a complete program that, for each data set, quantitatively analyzes how removing the single most influential observation (as measured by Cook’s distance) changes the two-sided $95$ confidence interval for the slope coefficient $ \\beta_1 $. The analysis must be grounded in the standard simple linear regression model and must proceed from first principles, starting from the definitions of ordinary least squares and projection (hat) matrix, and using the classical assumptions of the Gauss–Markov framework. You must use the following base facts only: the linear model specification, least squares normal equations, properties of the projection matrix, the unbiased residual variance estimator, and the Student’s $t$ distribution for finite-sample inference.\n\nFor each data set, do the following using only these base facts:\n- Fit the simple linear regression model $ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i $, with $ \\varepsilon_i $ independent and identically distributed with mean $ 0 $ and constant variance $ \\sigma^2 $.\n- Compute the leverages $ h_i $ from the diagonal of the hat matrix, the residuals $ e_i $, and the Mean Squared Error (MSE) using the unbiased estimator with degrees of freedom $ n - p $, where $ n $ is the sample size and $ p $ is the number of regression parameters including the intercept.\n- From these quantities, compute Cook’s distance $ D_i $ for each observation, identify the index of the most influential observation as the one with maximal $ D_i $, and record that observation’s leverage $ h_i $, absolute residual $ |e_i| $, and Cook’s distance $ D_i $ under the full fit.\n- Construct the two-sided $ 95 $ confidence interval for $ \\beta_1 $ under the full data and compute its total width. Then remove the most influential observation, refit the model, and compute the new two-sided $ 95 $ confidence interval width for $ \\beta_1 $.\n- For each data set, return a list of $ 6 $ values in this exact order: $[\\text{width\\_before}, \\text{width\\_after}, \\text{narrower}, h_i, |e_i|, D_i]$, where $ \\text{narrower} $ is $ 1 $ if the interval width decreased after removal and $ 0 $ otherwise. All floating-point values must be rounded to $ 6 $ decimal places.\n\nTest suite:\n- Case $ 1 $ (high leverage and large residual outlier):\n  - $ x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20] $\n  - $ y = [1.0, 3.0, 5.1, 7.0, 9.2, 11.1, 13.0, 15.1, 17.2, 19.0, 21.1, 10.0] $\n- Case $ 2 $ (high leverage but small residual point):\n  - $ x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 20] $\n  - $ y = [1.2, 3.1, 5.0, 7.1, 9.0, 11.2, 13.2, 15.1, 17.0, 19.1, 40.9] $\n- Case $ 3 $ (large residual with moderate leverage):\n  - $ x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] $\n  - $ y = [1.0, 3.1, 5.0, 7.2, 9.0, 50.0, 13.1, 15.0, 17.2, 19.1, 21.0] $\n- Case $ 4 $ (small sample with an extreme $ x $ point):\n  - $ x = [0, 1, 2, 10, 11] $\n  - $ y = [1.0, 3.0, 5.2, 15.0, 23.1] $\n\nImplementation requirements:\n- Your implementation must construct the necessary linear algebra objects explicitly and must use the Student’s $ t $ distribution for determining the two-sided $ 95 $ confidence interval for $ \\beta_1 $.\n- Use $ p = 2 $ parameters (intercept and slope) in all fits.\n- Use $ \\alpha = 0.05 $ for the two-sided interval level.\n- Round all floating-point outputs to $ 6 $ decimal places.\n- The final program must produce a single line of output containing the results for all cases as a comma-separated list of lists enclosed in square brackets, for example $ [r_1, r_2, r_3, r_4] $, where each $ r_k $ is the $ 6 $-tuple list described above in the specified order.",
            "solution": "The problem requires an analysis of influential data points in simple linear regression. The analysis is to be conducted from first principles, based on the Ordinary Least Squares (OLS) framework. We will first establish the theoretical foundations and then outline the computational procedure.\n\nThe simple linear regression model is specified as:\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\quad \\text{for } i = 1, \\dots, n $$\nwhere $y_i$ is the response variable, $x_i$ is the predictor variable, $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\varepsilon_i$ are independent and identically distributed error terms with mean $E[\\varepsilon_i] = 0$ and constant variance $\\text{Var}(\\varepsilon_i) = \\sigma^2$.\n\nIn matrix notation, the model is expressed as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where:\n$$\n\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}, \\quad\n\\mathbf{X} = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ \\vdots  \\vdots \\\\ 1  x_n \\end{pmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}, \\quad\n\\boldsymbol{\\varepsilon} = \\begin{pmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{pmatrix}\n$$\nThe design matrix $\\mathbf{X}$ has $n$ rows and $p=2$ columns.\n\nThe OLS estimator $\\hat{\\boldsymbol{\\beta}}$ is found by minimizing the sum of squared residuals (SSE), $\\text{SSE} = \\sum_{i=1}^n e_i^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$. Taking the derivative with respect to $\\boldsymbol{\\beta}$ and setting it to zero yields the normal equations:\n$$ (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y} $$\nAssuming $\\mathbf{X}^T\\mathbf{X}$ is invertible, the OLS estimator for $\\boldsymbol{\\beta}$ is:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n\nFrom this, we define several key quantities. The fitted values are $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbfX^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$. The matrix $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$ is a projection matrix known as the hat matrix, as it \"puts the hat\" on $\\mathbf{y}$.\nThe leverage $h_i$ of the $i$-th observation is the $i$-th diagonal element of the hat matrix, $H_{ii}$. It measures the influence of the observed response $y_i$ on its own fitted value $\\hat{y}_i$, as $\\frac{\\partial \\hat{y}_i}{\\partial y_i} = h_i$. The vector of residuals is $\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}$.\n\nAn unbiased estimator for the error variance $\\sigma^2$ is the Mean Squared Error (MSE):\n$$ s^2 = \\text{MSE} = \\frac{\\mathbf{e}^T\\mathbf{e}}{n-p} $$\nwhere the denominator $n-p$ represents the degrees of freedom of the residuals. For this problem, $p=2$.\n\nTo identify influential observations, we use Cook's distance, $D_i$. It measures the effect of deleting observation $i$ on the entire vector of estimated coefficients. It can be computed efficiently using quantities from the full model fit:\n$$ D_i = \\frac{e_i^2}{p \\cdot \\text{MSE}} \\left[ \\frac{h_i}{(1-h_i)^2} \\right] $$\nA larger value of $D_i$ indicates that observation $i$ is more influential. The observation with the maximum Cook's distance is deemed the \"most influential.\"\n\nFor statistical inference on the slope coefficient $\\beta_1$, we use its sampling distribution. The variance-covariance matrix of $\\hat{\\boldsymbol{\\beta}}$ is $\\text{Var}(\\hat{\\boldsymbol{\\beta}}) = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1}$. We estimate this matrix by substituting $s^2$ for $\\sigma^2$. The estimated variance of $\\hat{\\beta}_1$ is the element in the second row and second column (using 1-based indexing) of $s^2(\\mathbf{X}^T\\mathbf{X})^{-1}$, which we denote as $s^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{11}$ using 0-based indexing. The standard error of $\\hat{\\beta}_1$ is the square root of this value:\n$$ \\text{SE}(\\hat{\\beta}_1) = \\sqrt{s^2 [(\\mathbf{X}^T\\mathbf{X})^{-1}]_{11}} = s \\sqrt{[(\\mathbf{X}^T\\mathbf{X})^{-1}]_{11}} $$\nThe pivotal quantity $\\frac{\\hat{\\beta}_1 - \\beta_1}{\\text{SE}(\\hat{\\beta}_1)}$ follows a Student's $t$-distribution with $n-p$ degrees of freedom. A two-sided $100(1-\\alpha)\\%$ confidence interval for $\\beta_1$ is constructed as:\n$$ \\hat{\\beta}_1 \\pm t_{1-\\alpha/2, n-p} \\cdot \\text{SE}(\\hat{\\beta}_1) $$\nwhere $t_{1-\\alpha/2, n-p}$ is the upper-tailed critical value from the $t$-distribution. For a $95\\%$ confidence interval, $\\alpha=0.05$. The total width of this interval is $W = 2 \\cdot t_{0.975, n-p} \\cdot \\text{SE}(\\hat{\\beta}_1)$.\n\nThe procedure for each dataset is as follows:\n1.  For the full dataset $(x_i, y_i)_{i=1}^n$, construct the design matrix $\\mathbf{X}$ and compute $\\hat{\\boldsymbol{\\beta}}$, $\\mathbf{H}$, $\\mathbf{e}$, $s^2$, and $D_i$ for all $i$.\n2.  Identify the index $i^*$ corresponding to the maximum Cook's distance, $\\max(D_i)$. Record $h_{i^*}$, $|e_{i^*}|$, and $D_{i^*}$.\n3.  Calculate the standard error $\\text{SE}(\\hat{\\beta}_1)$ and the $t$-critical value $t_{0.975, n-2}$ to find the width of the $95\\%$ confidence interval for $\\beta_1$, denoted `width_before`.\n4.  Remove the influential observation $(x_{i^*}, y_{i^*})$ to form a new dataset of size $n-1$.\n5.  Refit the regression model on the reduced dataset. This involves re-computing the design matrix $\\mathbf{X}'$, the coefficient estimates $\\hat{\\boldsymbol{\\beta}}'$, the residual variance $(s')^2$, the standard error $\\text{SE}(\\hat{\\beta}_1')$, and the new $t$-critical value $t_{0.975, n-1-2}$.\n6.  Calculate the width of the new $95\\%$ confidence interval, `width_after`.\n7.  Compare the widths and set a flag `narrower` to $1 if `width_after` is less than `width_before`, and $0$ otherwise.\n8.  Return the six required values, rounded to six decimal places.\n\nThis entire process is implemented for each of the four test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20], dtype=np.float64),\n         np.array([1.0, 3.0, 5.1, 7.0, 9.2, 11.1, 13.0, 15.1, 17.2, 19.0, 21.1, 10.0], dtype=np.float64)),\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 20], dtype=np.float64),\n         np.array([1.2, 3.1, 5.0, 7.1, 9.0, 11.2, 13.2, 15.1, 17.0, 19.1, 40.9], dtype=np.float64)),\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float64),\n         np.array([1.0, 3.1, 5.0, 7.2, 9.0, 50.0, 13.1, 15.0, 17.2, 19.1, 21.0], dtype=np.float64)),\n        (np.array([0, 1, 2, 10, 11], dtype=np.float64),\n         np.array([1.0, 3.0, 5.2, 15.0, 23.1], dtype=np.float64)),\n    ]\n\n    results = []\n    for x, y in test_cases:\n        result = analyze_influence(x, y)\n        results.append(result)\n    \n    # Format the final output string\n    output_str = \"[\" + ','.join([f\"[{','.join(map(str, r))}]\" for r in results]) + \"]\"\n    print(output_str)\n\ndef get_regression_stats(x, y, alpha=0.05):\n    \"\"\"\n    Performs simple linear regression and returns key statistics.\n    \"\"\"\n    n = len(x)\n    p = 2  # Number of parameters: intercept and slope\n\n    # Construct the design matrix X\n    X = np.c_[np.ones(n), x]\n\n    # Calculate beta_hat using normal equations: (X'X)b = X'y\n    # This is more numerically stable than computing the inverse directly for beta.\n    try:\n        XTX = X.T @ X\n        XTy = X.T @ y\n        beta_hat = np.linalg.solve(XTX, XTy)\n    except np.linalg.LinAlgError:\n        # Handle cases of singular matrix, though not expected with given data\n        return None\n\n    # Calculate hat matrix, residuals, and MSE\n    XTX_inv = np.linalg.inv(XTX)\n    H = X @ XTX_inv @ X.T\n    leverages = np.diag(H)\n    residuals = y - X @ beta_hat\n    mse = (residuals.T @ residuals) / (n - p)\n    \n    # Calculate Cook's distance\n    cooks_d = (residuals**2 / (p * mse)) * (leverages / (1 - leverages)**2)\n    \n    # Calculate confidence interval width for the slope (beta_1)\n    var_beta_hat = mse * XTX_inv\n    se_beta1 = np.sqrt(var_beta_hat[1, 1])\n    t_critical = t.ppf(1 - alpha/2, n - p)\n    ci_width = 2 * t_critical * se_beta1\n    \n    return {\n        'beta_hat': beta_hat,\n        'leverages': leverages,\n        'residuals': residuals,\n        'mse': mse,\n        'cooks_d': cooks_d,\n        'ci_width_beta1': ci_width\n    }\n\ndef analyze_influence(x, y, alpha=0.05):\n    \"\"\"\n    Analyzes the influence of the most influential point on B1's CI.\n    \"\"\"\n    # 1. Fit model on full data\n    full_fit_stats = get_regression_stats(x, y, alpha)\n    if full_fit_stats is None:\n        return [0.0] * 6 # Should not happen\n\n    width_before = full_fit_stats['ci_width_beta1']\n    \n    # 2. Identify most influential observation\n    cooks_d = full_fit_stats['cooks_d']\n    influential_idx = np.argmax(cooks_d)\n    \n    # 3. Record its stats\n    h_i = full_fit_stats['leverages'][influential_idx]\n    abs_e_i = np.abs(full_fit_stats['residuals'][influential_idx])\n    d_i = cooks_d[influential_idx]\n\n    # 4. Remove the influential observation and refit\n    x_reduced = np.delete(x, influential_idx)\n    y_reduced = np.delete(y, influential_idx)\n    \n    reduced_fit_stats = get_regression_stats(x_reduced, y_reduced, alpha)\n    if reduced_fit_stats is None:\n        width_after = 0.0 # Should not happen\n    else:\n        width_after = reduced_fit_stats['ci_width_beta1']\n    \n    # 5. Compare CI widths\n    narrower = 1 if width_after  width_before else 0\n\n    # 6. Format and return results\n    return [\n        round(width_before, 6),\n        round(width_after, 6),\n        narrower,\n        round(h_i, 6),\n        round(abs_e_i, 6),\n        round(d_i, 6)\n    ]\n\n# Execute the main function\nsolve()\n\n```"
        }
    ]
}