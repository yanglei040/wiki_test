{
    "hands_on_practices": [
        {
            "introduction": "To truly master the $R^2$ statistic, we must first understand its constituent parts. The coefficient of determination represents the proportion of total variance in the response variable that is explained by the regression model. This foundational exercise  provides a direct way to practice this concept by working backward from a given $R^2$ and the total sum of squares ($SST$) to find the regression sum of squares ($SSR$), solidifying your grasp of this crucial relationship.",
            "id": "1904808",
            "problem": "An urban planning student is investigating the relationship between the population density of a city district and the average daily ridership on its public transit system. After collecting data from various districts, the student performs a simple linear regression analysis to model this relationship. The analysis reveals that the total sum of squares (SST), which represents the total variation in the daily ridership data around its mean, is 240. The model's coefficient of determination, $R^2$, is found to be 0.25. The coefficient of determination measures the proportion of the variance in the dependent variable (ridership) that is predictable from the independent variable (population density).\n\nCalculate the regression sum of squares (SSR), which quantifies the amount of variation in the ridership data explained by the regression model.",
            "solution": "Let $SST$ denote the total sum of squares, $SSR$ the regression sum of squares, and $SSE$ the sum of squared errors. The standard decomposition is\n$$\nSST = SSR + SSE.\n$$\nThe coefficient of determination is defined by\n$$\nR^{2} = \\frac{SSR}{SST}.\n$$\nSolving for $SSR$ gives\n$$\nSSR = R^{2} \\cdot SST.\n$$\nSubstituting the given values $R^{2} = 0.25$ and $SST = 240$,\n$$\nSSR = 0.25 \\cdot 240 = \\frac{1}{4} \\cdot 240 = 60.\n$$\nThus, the regression sum of squares is $60$.",
            "answer": "$$\\boxed{60}$$"
        },
        {
            "introduction": "After understanding the components of $R^2$, it is insightful to explore its behavior in extreme scenarios. This practice  presents a thought experiment: what happens when you fit a linear model to the minimum number of points required to define a unique line? By determining the $R^2$ for a regression on just two distinct data points, you will gain a deeper intuition for what a \"perfect fit\" implies and why a high $R^2$ value must always be interpreted in the context of the data and model complexity.",
            "id": "1904860",
            "problem": "In the field of statistics, a simple linear regression model is used to describe the relationship between two variables. Consider a minimalist dataset consisting of exactly two distinct data points, given by $(x_1, y_1)$ and $(x_2, y_2)$. For this dataset, assume that $x_1 \\neq x_2$ and $y_1 \\neq y_2$. A simple linear regression model of the form $\\hat{y} = \\beta_0 + \\beta_1 x$ is fitted to these two points.\n\nYour task is to determine the exact value of the coefficient of determination, denoted as $R^2$, for this regression model.",
            "solution": "We fit the simple linear regression model $\\hat{y} = \\beta_{0} + \\beta_{1} x$ to the two points $(x_{1}, y_{1})$ and $(x_{2}, y_{2})$ with $x_{1} \\neq x_{2}$ and $y_{1} \\neq y_{2}$. Ordinary least squares chooses $(\\beta_{0}, \\beta_{1})$ to minimize the sum of squared errors\n$$\nS(\\beta_{0}, \\beta_{1}) = \\sum_{i=1}^{2} \\left(y_{i} - (\\beta_{0} + \\beta_{1} x_{i})\\right)^{2}.\n$$\nBecause there are two parameters and two data points with distinct $x$-values, there exists a unique line passing through both points. Setting $\\hat{y}_{i} = y_{i}$ for $i=1,2$ yields the system\n$$\n\\beta_{0} + \\beta_{1} x_{1} = y_{1}, \\quad \\beta_{0} + \\beta_{1} x_{2} = y_{2}.\n$$\nSubtracting the equations gives\n$$\n\\beta_{1} (x_{2} - x_{1}) = y_{2} - y_{1} \\quad \\Rightarrow \\quad \\beta_{1} = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}},\n$$\nand substituting back gives\n$$\n\\beta_{0} = y_{1} - \\beta_{1} x_{1}.\n$$\nWith these coefficients, the fitted values satisfy $\\hat{y}_{i} = y_{i}$ for $i=1,2$, so each residual is zero and the sum of squared errors is\n$$\n\\mathrm{SSE} = \\sum_{i=1}^{2} (y_{i} - \\hat{y}_{i})^{2} = 0.\n$$\nThe total sum of squares about the sample mean $\\bar{y} = \\frac{y_{1} + y_{2}}{2}$ is\n$$\n\\mathrm{SST} = \\sum_{i=1}^{2} (y_{i} - \\bar{y})^{2} = \\left(\\frac{y_{1} - y_{2}}{2}\\right)^{2} + \\left(\\frac{y_{2} - y_{1}}{2}\\right)^{2} = \\frac{(y_{1} - y_{2})^{2}}{2} > 0,\n$$\nusing $y_{1} \\neq y_{2}$. For a model with an intercept, the coefficient of determination is defined by\n$$\nR^{2} = 1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}.\n$$\nSubstituting $\\mathrm{SSE} = 0$ and $\\mathrm{SST} > 0$ yields\n$$\nR^{2} = 1.\n$$",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Building on the idea that a perfect $R^2$ does not always signify a meaningful model, this final practice presents a critical and cautionary tale in modern statistics: overfitting. Through a hands-on coding simulation , you will demonstrate how a model with too many predictors relative to its data can achieve a near-perfect in-sample $R^2$, even when the predictors are pure noise and have zero true predictive power. This exercise provides an unforgettable lesson on the dangers of spurious regression and emphasizes the absolute necessity of evaluating models on unseen data.",
            "id": "2407193",
            "problem": "Construct a program that demonstrates, via controlled simulation, that a linear regression model in a time-series setting can achieve a near-perfect in-sample coefficient of determination (R-squared) but have essentially zero predictive power for future periods. The setting is as follows.\n\nThere is a univariate target time series $\\{y_t\\}_{t=1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}}$ and a panel of $p$ contemporaneous regressors $\\{x_{t, j}\\}_{t=1, \\dots, T_{\\mathrm{train}}+T_{\\mathrm{test}};\\; j=1,\\dots,p}$. The data-generating process is such that $y_t$ and all $x_{t,j}$ are mutually independent and identically distributed Gaussian random variables with zero mean and specified standard deviations. Specifically, for each $t$ and each $j$,\n$$\ny_t \\sim \\mathcal{N}(0,\\sigma_y^2), \\quad x_{t,j} \\sim \\mathcal{N}(0,\\sigma_x^2),\n$$\nwith independence across $t$ and $j$, and independence between $y_t$ and $\\{x_{t,j}\\}_{j=1}^p$.\n\nFor the training sample indices $t = 1,\\dots, T_{\\mathrm{train}}$, fit an affine linear model with intercept by ordinary least squares (OLS),\n$$\ny_t = \\beta_0 + \\sum_{j=1}^p \\beta_j x_{t,j} + \\varepsilon_t,\n$$\nand obtain the fitted values $\\{\\hat{y}_t\\}_{t=1}^{T_{\\mathrm{train}}}$. Using the estimated coefficients, produce out-of-sample predictions $\\{\\hat{y}_t\\}_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}}$ for the test indices $t = T_{\\mathrm{train}}+1,\\dots,T_{\\mathrm{train}}+T_{\\mathrm{test}}$.\n\nCompute the in-sample coefficient of determination\n$$\nR^2_{\\mathrm{in}} \\equiv 1 - \\frac{\\sum_{t=1}^{T_{\\mathrm{train}}} (y_t - \\hat{y}_t)^2}{\\sum_{t=1}^{T_{\\mathrm{train}}} (y_t - \\bar{y}_{\\mathrm{train}})^2},\n\\quad \\text{where} \\quad\n\\bar{y}_{\\mathrm{train}} \\equiv \\frac{1}{T_{\\mathrm{train}}}\\sum_{t=1}^{T_{\\mathrm{train}}} y_t,\n$$\nand the out-of-sample coefficient of determination\n$$\nR^2_{\\mathrm{out}} \\equiv 1 - \\frac{\\sum_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}} (y_t - \\hat{y}_t)^2}{\\sum_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}} (y_t - \\bar{y}_{\\mathrm{test}})^2},\n\\quad \\text{where} \\quad\n\\bar{y}_{\\mathrm{test}} \\equiv \\frac{1}{T_{\\mathrm{test}}}\\sum_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}} y_t.\n$$\n\nDefine a boolean indicator for the presence of the phenomenon of interest as\n$$\n\\text{spurious\\_success} \\equiv \\left( R^2_{\\mathrm{in}} \\ge 0.999999 \\right) \\wedge \\left( R^2_{\\mathrm{out}} \\le 0.05 \\right).\n$$\n\nYour program must, for each test case below, generate the data using the specified random seed, estimate the model on the training sample, compute $R^2_{\\mathrm{in}}$ and $R^2_{\\mathrm{out}}$, and return the boolean $\\text{spurious\\_success}$ as defined above.\n\nTest Suite (each tuple specifies $(\\text{seed}, T_{\\mathrm{train}}, T_{\\mathrm{test}}, p, \\sigma_y, \\sigma_x)$):\n- Case A: $(7, 60, 400, 59, 1.0, 1.0)$\n- Case B: $(123, 60, 400, 10, 1.0, 1.0)$\n- Case C: $(42, 30, 200, 29, 1.0, 1.0)$\n- Case D: $(8, 200, 400, 0, 1.0, 1.0)$\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and ordered as $[\\text{Case A}, \\text{Case B}, \\text{Case C}, \\text{Case D}]$, where each entry is the boolean value $\\text{spurious\\_success}$ for that case, for example, $[True,False,True,False]$. No other text should be printed. There are no physical units or angle units involved, and no percentages should be reported outside of the real-number thresholds embedded in the definition of $\\text{spurious\\_success}$.",
            "solution": "The problem statement has been evaluated and is determined to be valid. It is scientifically grounded, well-posed, and objective. It describes a controlled simulation to demonstrate the well-known statistical phenomenon of overfitting in a high-dimensional linear regression setting, where the number of predictors is close to the number of observations.\n\nThe core principle to be demonstrated is that a linear model with a large number of parameters relative to the sample size can achieve an arbitrarily high coefficient of determination ($R^2$) on the data it was trained on (in-sample), even if the predictors have no actual relationship with the target variable. This in-sample success, however, is illusory and fails to translate to predictive power on new, unseen data (out-of-sample).\n\nThe mathematical model to be estimated via ordinary least squares (OLS) is:\n$$\ny_t = \\beta_0 + \\sum_{j=1}^p \\beta_j x_{t,j} + \\varepsilon_t\n$$\nIn matrix form for the training data ($t=1, \\dots, T_{\\mathrm{train}}$), this is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is a $T_{\\mathrm{train}} \\times 1$ vector of observations, $\\mathbf{X}$ is a $T_{\\mathrm{train}} \\times (p+1)$ design matrix (including a column of ones for the intercept $\\beta_0$), and $\\boldsymbol{\\beta}$ is the $(p+1) \\times 1$ vector of coefficients.\n\nThe OLS estimator $\\hat{\\boldsymbol{\\beta}}$ is found by minimizing the sum of squared errors ($SSE$):\n$$\nSSE = ||\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}||^2\n$$\nThe solution is given by $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$. A crucial point arises when the number of parameters to estimate, $k = p+1$, is equal to the number of observations, $T_{\\mathrm{train}}$. In this scenario, the design matrix $\\mathbf{X}$ is a square $T_{\\mathrm{train}} \\times T_{\\mathrm{train}}$ matrix. For data drawn from a continuous random distribution, such as the specified Gaussian, this matrix will be invertible with probability $1$. Consequently, the estimated coefficients are $\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^{-1}\\mathbf{y}$. The fitted values $\\hat{\\mathbf{y}}$ become:\n$$\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^{-1}\\mathbf{y}) = \\mathbf{y}\n$$\nThis implies that the fitted values are identical to the actual observations, $\\hat{y}_t = y_t$ for all $t$ in the training sample. The sum of squared errors in-sample, $SSE_{\\mathrm{in}} = \\sum_{t=1}^{T_{\\mathrm{train}}} (y_t - \\hat{y}_t)^2$, is therefore exactly zero. The in-sample coefficient of determination, $R^2_{\\mathrm{in}}$, is then:\n$$\nR^2_{\\mathrm{in}} = 1 - \\frac{SSE_{\\mathrm{in}}}{\\sum_{t=1}^{T_{\\mathrm{train}}} (y_t - \\bar{y}_{\\mathrm{train}})^2} = 1 - 0 = 1\n$$\nThis explains why a near-perfect in-sample $R^2$ is expected in cases where $p+1 = T_{\\mathrm{train}}$, such as Case A ($p=59, T_{\\mathrm{train}}=60$) and Case C ($p=29, T_{\\mathrm{train}}=30$). The threshold $R^2_{\\mathrm{in}} \\ge 0.999999$ accommodates minor floating-point inaccuracies in numerical computation.\n\nConversely, the out-of-sample $R^2$, defined as:\n$$\nR^2_{\\mathrm{out}} = 1 - \\frac{\\sum_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}} (y_t - \\hat{y}_t)^2}{\\sum_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}} (y_t - \\bar{y}_{\\mathrm{test}})^2}\n$$\nevaluates the model on new data. Since the data-generating process specifies that $y_t$ and $x_{t,j}$ are mutually independent, there is no true relationship to be learned. The coefficients $\\hat{\\boldsymbol{\\beta}}$ are not estimates of any true parameters but are artifacts of fitting noise. When applied to the test set, the predictions $\\hat{y}_t$ are effectively random, uncorrelated with the actual $y_t$. Therefore, the model's predictive performance is not expected to be better than a naive model that always predicts the test set's mean, $\\bar{y}_{\\mathrm{test}}$. This results in an $R^2_{\\mathrm{out}}$ value close to $0$, and it can even be negative if the model's predictions are worse than using the mean. The condition $R^2_{\\mathrm{out}} \\le 0.05$ is thus expected to be met.\n\nThe specific test cases are analyzed as follows:\n- Case A ($(\\text{seed}=7, T_{\\mathrm{train}}=60, T_{\\mathrm{test}}=400, p=59, \\sigma_y=1.0, \\sigma_x=1.0)$) and Case C ($(\\text{seed}=42, T_{\\mathrm{train}}=30, T_{\\mathrm{test}}=200, p=29, \\sigma_y=1.0, \\sigma_x=1.0)$): In both cases, the number of parameters $p+1$ equals the training sample size $T_{\\mathrm{train}}$. As derived, this will lead to $R^2_{\\mathrm{in}} \\approx 1$. Since the underlying data are independent, $R^2_{\\mathrm{out}}$ will be close to $0$. The `spurious_success` condition is expected to be `True`.\n- Case B ($(\\text{seed}=123, T_{\\mathrm{train}}=60, T_{\\mathrm{test}}=400, p=10, \\sigma_y=1.0, \\sigma_x=1.0)$): Here, $p+1 = 11 \\ll T_{\\mathrm{train}}=60$. The model is not sufficiently over-parameterized to perfectly fit the noise. The expected value of $R^2_{\\mathrm{in}}$ for regressing on pure noise is approximately $(p+1)/(T_{\\mathrm{train}}-1) = 11/59 \\approx 0.187$, which is far below the threshold of $0.999999$. The `spurious_success` condition will be `False`.\n- Case D ($(\\text{seed}=8, T_{\\mathrm{train}}=200, T_{\\mathrm{test}}=400, p=0, \\sigma_y=1.0, \\sigma_x=1.0)$): The model has $p=0$ regressors, so it is just $y_t = \\beta_0 + \\varepsilon_t$. The OLS estimate for the intercept is the sample mean, $\\hat{\\beta}_0 = \\bar{y}_{\\mathrm{train}}$. The fitted values are $\\hat{y}_t = \\bar{y}_{\\mathrm{train}}$ for all $t$ in the training set. By the definition of $R^2$, this model has an $R^2_{\\mathrm{in}}$ of exactly $0$. The condition $R^2_{\\mathrm{in}} \\ge 0.999999$ fails, so `spurious_success` must be `False`.\n\nThe implementation will proceed by generating the data for each case, fitting the OLS model using a numerically stable least-squares solver (`scipy.linalg.lstsq`), computing the specified $R^2$ metrics, and checking the boolean condition for `spurious_success`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem by running a simulation for each test case to demonstrate\n    the phenomenon of spurious regression success in high-dimensional settings.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple: (seed, T_train, T_test, p, sigma_y, sigma_x)\n    test_cases = [\n        (7, 60, 400, 59, 1.0, 1.0),    # Case A\n        (123, 60, 400, 10, 1.0, 1.0),   # Case B\n        (42, 30, 200, 29, 1.0, 1.0),    # Case C\n        (8, 200, 400, 0, 1.0, 1.0),      # Case D\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        seed, T_train, T_test, p, sigma_y, sigma_x = case\n        \n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n        \n        # Total number of time periods\n        T_total = T_train + T_test\n        \n        # Generate the data according to the Data-Generating Process (DGP)\n        # y_t ~ N(0, sigma_y^2)\n        y = np.random.normal(loc=0.0, scale=sigma_y, size=T_total)\n        \n        # x_{t,j} ~ N(0, sigma_x^2)\n        X = np.random.normal(loc=0.0, scale=sigma_x, size=(T_total, p))\n        \n        # Split data into training and testing sets\n        y_train = y[:T_train]\n        X_train = X[:T_train]\n        \n        y_test = y[T_train:]\n        X_test = X[T_train:]\n        \n        # Construct the design matrix for the training set with an intercept column\n        X_train_aug = np.hstack([np.ones((T_train, 1)), X_train])\n        \n        # Fit the OLS model using a numerically stable least-squares solver\n        # This solves for beta_hat in y_train = X_train_aug @ beta_hat\n        beta_hat, _, _, _ = linalg.lstsq(X_train_aug, y_train)\n        \n        # --- Compute In-Sample R-squared (R^2_in) ---\n        \n        # Calculate in-sample predictions\n        y_hat_train = X_train_aug @ beta_hat\n        \n        # Calculate Sum of Squared Errors (SSE)\n        sse_in = np.sum((y_train - y_hat_train)**2)\n        \n        # Calculate Total Sum of Squares (SST)\n        y_train_mean = np.mean(y_train)\n        sst_in = np.sum((y_train - y_train_mean)**2)\n        \n        # Handle the case where SST is zero to avoid division by zero\n        # This occurs if all y_train values are identical.\n        if sst_in == 0:\n            # If SSE is also zero, the fit is perfect. R^2 is 1.\n            # If SSE is non-zero, the fit is undefined/poor. R^2 is typically set to 0.\n            r_sq_in = 1.0 if sse_in == 0 else 0.0\n        else:\n            r_sq_in = 1.0 - (sse_in / sst_in)\n\n        # --- Compute Out-of-Sample R-squared (R^2_out) ---\n        \n        # Construct the design matrix for the test set\n        X_test_aug = np.hstack([np.ones((T_test, 1)), X_test])\n        \n        # Calculate out-of-sample predictions\n        y_hat_test = X_test_aug @ beta_hat\n        \n        # Calculate Sum of Squared Prediction Errors (SSE)\n        sse_out = np.sum((y_test - y_hat_test)**2)\n        \n        # Calculate Total Sum of Squares for the test set\n        y_test_mean = np.mean(y_test)\n        sst_out = np.sum((y_test - y_test_mean)**2)\n        \n        # Handle the case where SST_out is zero\n        if sst_out == 0:\n            r_sq_out = 1.0 if sse_out == 0 else 0.0\n        else:\n            r_sq_out = 1.0 - (sse_out / sst_out)\n\n        # Evaluate the spurious success condition\n        spurious_success = (r_sq_in >= 0.999999) and (r_sq_out = 0.05)\n        \n        results.append(spurious_success)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}