{
    "hands_on_practices": [
        {
            "introduction": "我们通过一个简单的思想实验来探讨 $R^2$ 的基本含义。这个练习旨在帮助你理解线性模型的“最佳情况”，并阐明 $R^2=1$ 的真正含义，即模型解释了数据中全部的方差。通过分析一个仅包含两个数据点的极简数据集，我们可以直观地揭示线性回归的几何本质和 $R^2$ 的上限。",
            "id": "1904860",
            "problem": "在统计学领域，简单线性回归模型用于描述两个变量之间的关系。考虑一个仅由两个不同数据点 $(x_1, y_1)$ 和 $(x_2, y_2)$ 组成的极简数据集。对于这个数据集，假设 $x_1 \\neq x_2$ 且 $y_1 \\neq y_2$。一个形式为 $\\hat{y} = \\beta_0 + \\beta_1 x$ 的简单线性回归模型被拟合到这两个点上。\n\n你的任务是确定该回归模型的决定系数（记为 $R^2$）的精确值。",
            "solution": "我们将简单线性回归模型 $\\hat{y} = \\beta_{0} + \\beta_{1} x$ 拟合到两个点 $(x_{1}, y_{1})$ 和 $(x_{2}, y_{2})$，其中 $x_{1} \\neq x_{2}$ 且 $y_{1} \\neq y_{2}$。普通最小二乘法选择 $(\\beta_{0}, \\beta_{1})$ 来最小化残差平方和\n$$\nS(\\beta_{0}, \\beta_{1}) = \\sum_{i=1}^{2} \\left(y_{i} - (\\beta_{0} + \\beta_{1} x_{i})\\right)^{2}.\n$$\n因为有两个参数和两个具有不同 $x$ 值的数据点，所以存在一条唯一的直线穿过这两个点。令 $i=1,2$ 时 $\\hat{y}_{i} = y_{i}$，得到方程组\n$$\n\\beta_{0} + \\beta_{1} x_{1} = y_{1}, \\quad \\beta_{0} + \\beta_{1} x_{2} = y_{2}.\n$$\n两方程相减得到\n$$\n\\beta_{1} (x_{2} - x_{1}) = y_{2} - y_{1} \\quad \\Rightarrow \\quad \\beta_{1} = \\frac{y_{2} - y_{1}}{x_{2} - x_{1}},\n$$\n代回可得\n$$\n\\beta_{0} = y_{1} - \\beta_{1} x_{1}.\n$$\n使用这些系数，拟合值满足 $i=1,2$ 时 $\\hat{y}_{i} = y_{i}$，因此每个残差都为零，残差平方和为\n$$\n\\mathrm{RSS} = \\sum_{i=1}^{2} (y_{i} - \\hat{y}_{i})^{2} = 0.\n$$\n关于样本均值 $\\bar{y} = \\frac{y_{1} + y_{2}}{2}$ 的总平方和为\n$$\n\\mathrm{TSS} = \\sum_{i=1}^{2} (y_{i} - \\bar{y})^{2} = \\left(\\frac{y_{1} - y_{2}}{2}\\right)^{2} + \\left(\\frac{y_{2} - y_{1}}{2}\\right)^{2} = \\frac{(y_{1} - y_{2})^{2}}{2} > 0,\n$$\n用到 $y_{1} \\neq y_{2}$。对于一个带截距的模型，决定系数定义为\n$$\nR^{2} = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}}.\n$$\n代入 $\\mathrm{RSS} = 0$ 和 $\\mathrm{TSS} > 0$ 可得\n$$\nR^{2} = 1.\n$$",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "虽然 $R^2$ 衡量了线性拟合的强度，但它本身不提供关于关系方向（正或负）的信息。本练习将探讨 $R^2$ 与皮尔逊相关系数 $r$ 之间的联系，后者确实捕捉了方向性。通过这个练习，你将巩固简单线性回归中 $R^2 = r^2$ 这一关键的数学关系，并理解如何从一个非定向的拟合优度度量推断出潜在关联的方向可能性。",
            "id": "1904873",
            "problem": "一位运筹学分析师正在为一台工厂机器每周的运行小时数（用 $x$ 表示）与其生产的产品单位数（用 $y$ 表示）之间的关系建模。采用简单线性回归模型来分析收集到的数据。分析得出的决定系数 $R^2$ 为 0.64。决定系数表示可由运行小时数解释的产品单位数方差的比例。在没有关于该关系具体性质的任何额外信息（例如，散点图或回归斜率的符号）的情况下，确定每周运行小时数与生产单位数之间的皮尔逊相关系数 $r$ 的可能值。\n\n从以下选项中选择正确的选项。\n\nA. 仅 0.80\n\nB. 仅 -0.80\n\nC. 0.80 和 -0.80\n\nD. 0.64\n\nE. 0.4096",
            "solution": "我们使用带截距的简单线性回归的恒等式，即决定系数等于预测变量和响应变量之间皮尔逊相关的平方：\n$$\nR^{2} = r^{2}.\n$$\n这可以从\n$$\nR^{2} = \\frac{\\text{SS}_{\\text{reg}}}{\\text{SS}_{\\text{tot}}} = \\frac{\\hat{\\beta}_{1}^{2}\\sum_{i}(x_{i}-\\bar{x})^{2}}{\\sum_{i}(y_{i}-\\bar{y})^{2}},\n$$\n以及以下事实看出\n$$\n\\hat{\\beta}_{1} = r\\,\\frac{s_{y}}{s_{x}}, \\quad s_{x}^{2} \\propto \\sum_{i}(x_{i}-\\bar{x})^{2}, \\quad s_{y}^{2} \\propto \\sum_{i}(y_{i}-\\bar{y})^{2},\n$$\n代入后即可得到 $R^{2} = r^{2}$。\n\n给定 $R^{2} = 0.64$，我们有\n$$\n|r| = \\sqrt{R^{2}} = \\sqrt{0.64} = 0.80.\n$$\n在没有关于斜率符号或关联方向的信息的情况下，相关可以是正的也可以是负的。因此，可能的值是 $0.80$ 和 $-0.80$，对应于选项 C。",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "现在，让我们从理论转向一个至关重要的实践技能：识别虚假的相关性。这个动手编码练习将引导你通过模拟来理解“过拟合”现象，即模型学习到了训练数据中的噪声而非真实的潜在规律。你将亲手证明，即使在一个变量间完全独立的数据生成过程中，一个拥有接近完美样本内 $R^2$ 值的模型也可能对未来数据毫无预测能力，这是现代数据分析中一个极其重要的警示。",
            "id": "2407193",
            "problem": "构建一个程序，通过受控模拟来证明，在时间序列设定中，一个线性回归模型可以达到近乎完美的样本内决定系数（决定系数 (R-squared)），但对未来时期基本上没有预测能力。设定如下。\n\n存在一个单变量目标时间序列 $\\{y_t\\}_{t=1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}}$ 和一个包含 $p$ 个同期回归变量的面板 $\\{x_{t, j}\\}_{t=1, \\dots, T_{\\mathrm{train}}+T_{\\mathrm{test}};\\; j=1,\\dots,p}$。数据生成过程设定为：$y_t$ 和所有 $x_{t,j}$ 均为相互独立同分布的高斯随机变量，其均值为零，标准差为指定值。具体而言，对于每个 $t$ 和每个 $j$：\n$$\ny_t \\sim \\mathcal{N}(0,\\sigma_y^2), \\quad x_{t,j} \\sim \\mathcal{N}(0,\\sigma_x^2),\n$$\n在 $t$ 和 $j$ 上是独立的，并且 $y_t$ 与 $\\{x_{t,j}\\}_{j=1}^p$ 之间也是独立的。\n\n对于训练样本索引 $t = 1,\\dots, T_{\\mathrm{train}}$，使用普通最小二乘法（ordinary least squares (OLS)）拟合一个带截距的仿射线性模型：\n$$\ny_t = \\beta_0 + \\sum_{j=1}^p \\beta_j x_{t,j} + \\varepsilon_t,\n$$\n并得到拟合值 $\\{\\hat{y}_t\\}_{t=1}^{T_{\\mathrm{train}}}$。使用估计出的系数，为测试索引 $t = T_{\\mathrm{train}}+1,\\dots,T_{\\mathrm{train}}+T_{\\mathrm{test}}}$ 生成样本外预测 $\\{\\hat{y}_t\\}_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}}$。\n\n计算样本内决定系数\n$$\nR^2_{\\mathrm{in}} \\equiv 1 - \\frac{\\sum_{t=1}^{T_{\\mathrm{train}}} (y_t - \\hat{y}_t)^2}{\\sum_{t=1}^{T_{\\mathrm{train}}} (y_t - \\bar{y}_{\\mathrm{train}})^2},\n\\quad \\text{其中} \\quad\n\\bar{y}_{\\mathrm{train}} \\equiv \\frac{1}{T_{\\mathrm{train}}}\\sum_{t=1}^{T_{\\mathrm{train}}} y_t,\n$$\n和样本外决定系数\n$$\nR^2_{\\mathrm{out}} \\equiv 1 - \\frac{\\sum_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}} (y_t - \\hat{y}_t)^2}{\\sum_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}} (y_t - \\bar{y}_{\\mathrm{test}})^2},\n\\quad \\text{其中} \\quad\n\\bar{y}_{\\mathrm{test}} \\equiv \\frac{1}{T_{\\mathrm{test}}}\\sum_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}} y_t.\n$$\n\n为所关注现象的存在定义一个布尔指标：\n$$\n\\text{spurious\\_success} \\equiv \\left( R^2_{\\mathrm{in}} \\ge 0.999999 \\right) \\wedge \\left( R^2_{\\mathrm{out}} \\le 0.05 \\right).\n$$\n\n您的程序必须为下方的每个测试用例，使用指定的随机种子生成数据，在训练样本上估计模型，计算 $R^2_{\\mathrm{in}}$ 和 $R^2_{\\mathrm{out}}$，并返回如上定义的布尔值 $\\text{spurious\\_success}$。\n\n测试套件（每个元组指定 $(\\text{seed}, T_{\\mathrm{train}}, T_{\\mathrm{test}}, p, \\sigma_y, \\sigma_x)$）：\n- 案例 A: $(7, 60, 400, 59, 1.0, 1.0)$\n- 案例 B: $(123, 60, 400, 10, 1.0, 1.0)$\n- 案例 C: $(42, 30, 200, 29, 1.0, 1.0)$\n- 案例 D: $(8, 200, 400, 0, 1.0, 1.0)$\n\n最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序为 $[\\text{案例 A}, \\text{案例 B}, \\text{案例 C}, \\text{案例 D}]$，其中每个条目是该案例的布尔值 $\\text{spurious\\_success}$，例如 $[True,False,True,False]$。不应打印任何其他文本。不涉及物理单位或角度单位，并且在 $\\text{spurious\\_success}$ 的定义中嵌入的实数阈值之外，不应报告任何百分比。",
            "solution": "问题陈述已经过评估，并被确定为有效。它有科学依据、适定且客观。它描述了一个受控模拟，用以演示在高维线性回归设定中众所周知的过拟合统计现象，其中预测变量的数量接近于观测值的数量。\n\n需要演示的核心原理是，当一个线性模型的参数数量相对于样本量较大时，即使预测变量与目标变量之间没有实际关系，它也可以在其训练数据（样本内）上达到任意高的决定系数（$R^2$）。然而，这种样本内的成功是虚假的，并且无法转化为对新的、未见过的数据（样本外）的预测能力。\n\n通过普通最小二乘法（OLS）待估计的数学模型是：\n$$\ny_t = \\beta_0 + \\sum_{j=1}^p \\beta_j x_{t,j} + \\varepsilon_t\n$$\n对于训练数据（$t=1, \\dots, T_{\\mathrm{train}}$），其矩阵形式为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{y}$ 是一个 $T_{\\mathrm{train}} \\times 1$ 的观测向量，$\\mathbf{X}$ 是一个 $T_{\\mathrm{train}} \\times (p+1)$ 的设计矩阵（包括一列对应于截距 $\\beta_0$ 的1），$\\boldsymbol{\\beta}$ 是一个 $(p+1) \\times 1$ 的系数向量。\n\nOLS 估计量 $\\hat{\\boldsymbol{\\beta}}$ 是通过最小化残差平方和（$SSR$）得到的：\n$$\nSSR = ||\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}||^2\n$$\n其解为 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$。一个关键点出现在当待估计的参数数量 $k = p+1$ 等于观测数量 $T_{\\mathrm{train}}$ 时。在这种情况下，设计矩阵 $\\mathbf{X}$ 是一个 $T_{\\mathrm{train}} \\times T_{\\mathrm{train}}$ 的方阵。对于从连续随机分布（如指定的高斯分布）中抽取的数据，该矩阵将以概率1是可逆的。因此，估计系数为 $\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^{-1}\\mathbf{y}$。拟合值 $\\hat{\\mathbf{y}}$ 变为：\n$$\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^{-1}\\mathbf{y}) = \\mathbf{y}\n$$\n这意味着对于训练样本中的所有 $t$，拟合值与实际观测值完全相同，即 $\\hat{y}_t = y_t$。因此，样本内残差平方和 $SSR_{\\mathrm{in}} = \\sum_{t=1}^{T_{\\mathrm{train}}} (y_t - \\hat{y}_t)^2$ 恰好为零。于是，样本内决定系数 $R^2_{\\mathrm{in}}$ 为：\n$$\nR^2_{\\mathrm{in}} = 1 - \\frac{SSR_{\\mathrm{in}}}{\\sum_{t=1}^{T_{\\mathrm{train}}} (y_t - \\bar{y}_{\\mathrm{train}})^2} = 1 - 0 = 1\n$$\n这解释了为什么在 $p+1 = T_{\\mathrm{train}}$ 的情况下（如案例 A ($p=59, T_{\\mathrm{train}}=60$) 和案例 C ($p=29, T_{\\mathrm{train}}=30$)），预计会得到一个近乎完美的样本内 $R^2$。阈值 $R^2_{\\mathrm{in}} \\ge 0.999999$ 是为了容纳数值计算中的微小浮点误差。\n\n相反，样本外 $R^2$ 定义为：\n$$\nR^2_{\\mathrm{out}} = 1 - \\frac{\\sum_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}} (y_t - \\hat{y}_t)^2}{\\sum_{t=T_{\\mathrm{train}}+1}^{T_{\\mathrm{train}}+T_{\\mathrm{test}}} (y_t - \\bar{y}_{\\mathrm{test}})^2}\n$$\n它在新数据上评估模型。由于数据生成过程指定 $y_t$ 和 $x_{t,j}$ 相互独立，因此没有真正的关系可供学习。系数 $\\hat{\\boldsymbol{\\beta}}$ 并非任何真实参数的估计，而是拟合噪声所产生的假象。当应用于测试集时，预测值 $\\hat{y}_t$ 实际上是随机的，与实际的 $y_t$ 不相关。因此，该模型的预测性能预计不会优于一个总是预测测试集均值 $\\bar{y}_{\\mathrm{test}}$ 的朴素模型。这导致 $R^2_{\\mathrm{out}}$ 的值接近于0，如果模型的预测比使用均值更差，它甚至可能为负。因此，条件 $R^2_{\\mathrm{out}} \\le 0.05$ 预计会得到满足。\n\n各个具体测试用例分析如下：\n- 案例 A ($(\\text{seed}=7, T_{\\mathrm{train}}=60, T_{\\mathrm{test}}=400, p=59, \\sigma_y=1.0, \\sigma_x=1.0)$) 和 案例 C ($(\\text{seed}=42, T_{\\mathrm{train}}=30, T_{\\mathrm{test}}=200, p=29, \\sigma_y=1.0, \\sigma_x=1.0)$)：在这两种情况下，参数数量 $p+1$ 等于训练样本大小 $T_{\\mathrm{train}}$。正如所推导的，这将导致 $R^2_{\\mathrm{in}} \\approx 1$。由于底层数据是独立的，$R^2_{\\mathrm{out}}$ 将接近于0。`spurious_success` 条件预计为 `True`。\n- 案例 B ($(\\text{seed}=123, T_{\\mathrm{train}}=60, T_{\\mathrm{test}}=400, p=10, \\sigma_y=1.0, \\sigma_x=1.0)$)：在这里，$p+1 = 11 \\ll T_{\\mathrm{train}}=60$。模型没有被充分地过度参数化以至于完美地拟合噪声。对于对纯噪声进行回归，$R^2_{\\mathrm{in}}$ 的期望值约为 $(p+1)/(T_{\\mathrm{train}}-1) = 11/59 \\approx 0.187$，远低于 $0.999999$ 的阈值。`spurious_success` 条件将为 `False`。\n- 案例 D ($(\\text{seed}=8, T_{\\mathrm{train}}=200, T_{\\mathrm{test}}=400, p=0, \\sigma_y=1.0, \\sigma_x=1.0)$)：该模型有 $p=0$ 个回归变量，所以它只是 $y_t = \\beta_0 + \\varepsilon_t$。截距的OLS估计是样本均值，$\\hat{\\beta}_0 = \\bar{y}_{\\mathrm{train}}$。对于训练集中的所有 $t$，拟合值为 $\\hat{y}_t = \\bar{y}_{\\mathrm{train}}$。根据 $R^2$ 的定义，这个模型的 $R^2_{\\mathrm{in}}$ 恰好为0。条件 $R^2_{\\mathrm{in}} \\ge 0.999999$ 不成立，所以 `spurious_success` 必须为 `False`。\n\n实现将按以下步骤进行：为每个案例生成数据，使用一个数值稳定的最小二乘求解器（`scipy.linalg.lstsq`）拟合OLS模型，计算指定的 $R^2$ 指标，并检查 `spurious_success` 的布尔条件。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem by running a simulation for each test case to demonstrate\n    the phenomenon of spurious regression success in high-dimensional settings.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple: (seed, T_train, T_test, p, sigma_y, sigma_x)\n    test_cases = [\n        (7, 60, 400, 59, 1.0, 1.0),    # Case A\n        (123, 60, 400, 10, 1.0, 1.0),   # Case B\n        (42, 30, 200, 29, 1.0, 1.0),    # Case C\n        (8, 200, 400, 0, 1.0, 1.0),      # Case D\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        seed, T_train, T_test, p, sigma_y, sigma_x = case\n        \n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n        \n        # Total number of time periods\n        T_total = T_train + T_test\n        \n        # Generate the data according to the Data-Generating Process (DGP)\n        # y_t ~ N(0, sigma_y^2)\n        y = np.random.normal(loc=0.0, scale=sigma_y, size=T_total)\n        \n        # x_{t,j} ~ N(0, sigma_x^2)\n        X = np.random.normal(loc=0.0, scale=sigma_x, size=(T_total, p))\n        \n        # Split data into training and testing sets\n        y_train = y[:T_train]\n        X_train = X[:T_train]\n        \n        y_test = y[T_train:]\n        X_test = X[T_train:]\n        \n        # Construct the design matrix for the training set with an intercept column\n        X_train_aug = np.hstack([np.ones((T_train, 1)), X_train])\n        \n        # Fit the OLS model using a numerically stable least-squares solver\n        # This solves for beta_hat in y_train = X_train_aug @ beta_hat\n        beta_hat, _, _, _ = linalg.lstsq(X_train_aug, y_train)\n        \n        # --- Compute In-Sample R-squared (R^2_in) ---\n        \n        # Calculate in-sample predictions\n        y_hat_train = X_train_aug @ beta_hat\n        \n        # Calculate Sum of Squared Residuals (SSR)\n        ssr_in = np.sum((y_train - y_hat_train)**2)\n        \n        # Calculate Total Sum of Squares (TSS)\n        y_train_mean = np.mean(y_train)\n        tss_in = np.sum((y_train - y_train_mean)**2)\n        \n        # Handle the case where TSS is zero to avoid division by zero\n        # This occurs if all y_train values are identical.\n        if tss_in == 0:\n            # If SSR is also zero, the fit is perfect. R^2 is 1.\n            # If SSR is non-zero, the fit is undefined/poor. R^2 is typically set to 0.\n            r_sq_in = 1.0 if ssr_in == 0 else 0.0\n        else:\n            r_sq_in = 1.0 - (ssr_in / tss_in)\n\n        # --- Compute Out-of-Sample R-squared (R^2_out) ---\n        \n        # Construct the design matrix for the test set\n        X_test_aug = np.hstack([np.ones((T_test, 1)), X_test])\n        \n        # Calculate out-of-sample predictions\n        y_hat_test = X_test_aug @ beta_hat\n        \n        # Calculate Sum of Squared Prediction Errors (SSE)\n        sse_out = np.sum((y_test - y_hat_test)**2)\n        \n        # Calculate Total Sum of Squares for the test set\n        y_test_mean = np.mean(y_test)\n        tss_out = np.sum((y_test - y_test_mean)**2)\n        \n        # Handle the case where TSS_out is zero\n        if tss_out == 0:\n            r_sq_out = 1.0 if sse_out == 0 else 0.0\n        else:\n            r_sq_out = 1.0 - (sse_out / tss_out)\n\n        # Evaluate the spurious success condition\n        spurious_success = (r_sq_in >= 0.999999) and (r_sq_out = 0.05)\n        \n        results.append(spurious_success)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}