{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of hypothesis testing in regression, we begin with the fundamental calculation of the t-statistic. This first exercise provides a direct application of the core formula for assessing the statistical significance of a predictor variable . By working through a straightforward environmental science scenario, you will practice the essential mechanics of computing the t-statistic from a given coefficient estimate and its standard error.",
            "id": "1955459",
            "problem": "An environmental scientist is studying the effect of a specific pollutant on the population density of a certain species of algae in a lake. Water samples are collected from various locations, and for each sample, the concentration of the pollutant (in micrograms per liter) and the density of the algae (in cells per milliliter) are measured.\n\nThe scientist fits a simple linear regression model to the data, given by $Y = \\beta_0 + \\beta_1 X + \\epsilon$, where $Y$ represents the algae density and $X$ represents the pollutant concentration. The analysis of the collected data yields an estimated slope coefficient of $\\hat{\\beta}_1 = -18.4$. The standard error associated with this slope estimate is found to be $SE(\\hat{\\beta}_1) = 5.25$.\n\nTo evaluate whether there is a statistically significant linear relationship between the pollutant concentration and the algae density, a hypothesis test is conducted. Calculate the value of the t-statistic for testing the null hypothesis that the true slope coefficient $\\beta_1$ is zero. Provide your answer rounded to three significant figures.",
            "solution": "We test the null hypothesis $H_{0}:\\beta_{1}=0$ against the alternative that $\\beta_{1}\\neq 0$. The test statistic for the slope in simple linear regression is\n$$\nt=\\frac{\\hat{\\beta}_{1}-\\beta_{1,0}}{SE(\\hat{\\beta}_{1})},\n$$\nwhere $\\beta_{1,0}$ is the hypothesized value under $H_{0}$. With $\\beta_{1,0}=0$, $\\hat{\\beta}_{1}=-18.4$, and $SE(\\hat{\\beta}_{1})=5.25$, we have\n$$\nt=\\frac{-18.4-0}{5.25}=-\\frac{18.4}{5.25}\\approx -3.5047619\\ldots\n$$\nRounding to three significant figures gives\n$$\nt\\approx -3.50.\n$$",
            "answer": "$$\\boxed{-3.50}$$"
        },
        {
            "introduction": "Beyond simple calculation, it is vital to understand the underlying properties of the t-statistic to interpret it correctly. This next practice explores the important concept of invariance, demonstrating that the t-statistic for a coefficient is unaffected by scaling or shifting the corresponding predictor variable . This thought experiment reveals a deep truth: the t-test assesses the intrinsic strength of the linear relationship, independent of the units of measurement.",
            "id": "3131042",
            "problem": "Consider simple linear regression with intercept, where a dataset of size $n \\ge 3$ consists of pairs $\\{(x_i, y_i)\\}_{i=1}^n$, and the model is $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$. Assume the errors $\\varepsilon_i$ satisfy the usual Gauss–Markov conditions (independent, mean $0$, common variance $\\sigma^2$) and, for validity of Student’s $t$-tests, that $\\varepsilon_i$ are normally distributed. Let $\\bar x = \\frac{1}{n}\\sum_{i=1}^n x_i$ and define the sample standard deviation $s_x = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar x)^2}$. Consider the transformed predictor $z_i = \\frac{x_i - \\bar x}{s_x}$, and fit the model $y_i = \\tilde\\beta_0 + \\tilde\\beta_1 z_i + \\varepsilon_i$ by Ordinary Least Squares (OLS).\n\nLet $\\hat\\beta_1$ denote the OLS estimator of $\\beta_1$ in the original model with predictor $x$, and let $\\hat\\beta_1^{(z)}$ denote the OLS estimator of the slope in the standardized model with predictor $z$. Let $t_1$ be the OLS $t$-statistic for testing $H_0:\\beta_1=0$ based on the original model, and let $t_1^{(z)}$ be the corresponding $t$-statistic in the standardized model.\n\nWhich statement best characterizes the relationship between $\\hat\\beta_1$, $\\hat\\beta_1^{(z)}$, $t_1$, and $t_1^{(z)}$?\n\nA. $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ and $t_1^{(z)} = t_1$.\n\nB. $\\hat\\beta_1^{(z)} = \\hat\\beta_1/s_x$ and $t_1^{(z)} = t_1$.\n\nC. $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ and $t_1^{(z)} = s_x \\, t_1$.\n\nD. $\\hat\\beta_1^{(z)} = \\hat\\beta_1$ and $t_1^{(z)} = t_1$.",
            "solution": "The problem statement will be validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Model 1 (Original)**: $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, for $i=1, \\dots, n$.\n- **Dataset**: $\\{(x_i, y_i)\\}_{i=1}^n$, with sample size $n \\ge 3$.\n- **Error Assumptions**: $\\varepsilon_i$ are independent and identically distributed (i.i.d.) normal random variables with mean $E[\\varepsilon_i]=0$ and common variance $Var(\\varepsilon_i)=\\sigma^2$. This encompasses the Gauss-Markov conditions plus normality.\n- **Definitions**:\n    - Sample mean of $x$: $\\bar x = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n    - Sample standard deviation of $x$: $s_x = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar x)^2}$. It is implicitly assumed that not all $x_i$ are identical, so $s_x > 0$.\n- **Transformation**: $z_i = \\frac{x_i - \\bar x}{s_x}$.\n- **Model 2 (Standardized)**: $y_i = \\tilde\\beta_0 + \\tilde\\beta_1 z_i + \\varepsilon_i$.\n- **Quantities of Interest**:\n    - $\\hat\\beta_1$: The Ordinary Least Squares (OLS) estimator of $\\beta_1$ in Model 1.\n    - $\\hat\\beta_1^{(z)}$: The OLS estimator of $\\tilde\\beta_1$ in Model 2.\n    - $t_1$: The OLS $t$-statistic for testing the null hypothesis $H_0: \\beta_1 = 0$ in Model 1.\n    - $t_1^{(z)}$: The OLS $t$-statistic for testing the null hypothesis $H_0: \\tilde\\beta_1 = 0$ in Model 2.\n- **Question**: Characterize the relationship between $\\hat\\beta_1$, $\\hat\\beta_1^{(z)}$, $t_1$, and $t_1^{(z)}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is well-grounded in the theory of linear regression, a core topic in statistics and statistical learning. All concepts, including OLS estimation, standardization of variables, and hypothesis testing with $t$-statistics, are standard and rigorously defined.\n- **Well-Posedness**: The problem is well-posed. The transformation from $x$ to $z$ is explicit. The relationships between the estimators and test statistics can be uniquely derived from first principles of OLS regression. The condition $n \\ge 3$ ensures the degrees of freedom for the model's error variance estimate, $n-2$, is at least $1$, which is necessary for the $t$-statistic to be well-defined.\n- **Objectivity**: The problem is stated in objective, mathematical language, free from ambiguity or subjectivity.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a standard, well-posed question in statistical theory. A solution will be derived.\n\n### Derivation\nThe solution requires deriving the relationships between the slope estimators and the $t$-statistics for the two models.\n\n**Part 1: Relationship between $\\hat\\beta_1$ and $\\hat\\beta_1^{(z)}$**\n\nThe OLS estimator for the slope coefficient in a simple linear regression of a response $Y$ on a predictor $X$ is given by:\n$$ \\hat\\beta_{slope} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2} $$\nFor the original model (Model 1), the predictor is $x$, and the OLS estimator for $\\beta_1$ is:\n$$ \\hat\\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y)}{\\sum_{i=1}^n (x_i - \\bar x)^2} $$\nFor the standardized model (Model 2), the predictor is $z$. We first find the sample mean and sum of squared deviations for $z$.\nThe sample mean of $z$ is:\n$$ \\bar z = \\frac{1}{n}\\sum_{i=1}^n z_i = \\frac{1}{n}\\sum_{i=1}^n \\frac{x_i - \\bar x}{s_x} = \\frac{1}{n s_x} \\left( \\sum_{i=1}^n x_i - n\\bar x \\right) = \\frac{1}{n s_x} (n\\bar x - n\\bar x) = 0 $$\nThe sum of squared deviations for $z$ is:\n$$ \\sum_{i=1}^n (z_i - \\bar z)^2 = \\sum_{i=1}^n (z_i - 0)^2 = \\sum_{i=1}^n z_i^2 = \\sum_{i=1}^n \\left( \\frac{x_i - \\bar x}{s_x} \\right)^2 $$\nUsing the definition $s_x^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar x)^2$, we have $\\sum_{i=1}^n (x_i - \\bar x)^2 = (n-1)s_x^2$.\nSubstituting this, we get:\n$$ \\sum_{i=1}^n (z_i - \\bar z)^2 = \\frac{1}{s_x^2} \\sum_{i=1}^n (x_i - \\bar x)^2 = \\frac{(n-1)s_x^2}{s_x^2} = n-1 $$\nNow we can write the OLS estimator for the slope $\\tilde\\beta_1$ in Model 2, which is denoted $\\hat\\beta_1^{(z)}$:\n$$ \\hat\\beta_1^{(z)} = \\frac{\\sum_{i=1}^n (z_i - \\bar z)(y_i - \\bar y)}{\\sum_{i=1}^n (z_i - \\bar z)^2} = \\frac{\\sum_{i=1}^n z_i (y_i - \\bar y)}{n-1} $$\nSubstitute $z_i = \\frac{x_i - \\bar x}{s_x}$:\n$$ \\hat\\beta_1^{(z)} = \\frac{\\sum_{i=1}^n \\frac{x_i - \\bar x}{s_x} (y_i - \\bar y)}{n-1} = \\frac{1}{(n-1)s_x} \\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y) $$\nFrom the formula for $\\hat\\beta_1$, we can express the numerator sum of cross-products as $\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y) = \\hat\\beta_1 \\sum_{i=1}^n (x_i - \\bar x)^2 = \\hat\\beta_1 (n-1)s_x^2$.\nSubstituting this into the expression for $\\hat\\beta_1^{(z)}$:\n$$ \\hat\\beta_1^{(z)} = \\frac{1}{(n-1)s_x} \\left( \\hat\\beta_1 (n-1)s_x^2 \\right) = s_x \\hat\\beta_1 $$\nThis establishes the first relationship: $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$.\n\n**Part 2: Relationship between $t_1$ and $t_1^{(z)}$**\n\nThe $t$-statistic for a slope coefficient is the ratio of the estimated coefficient to its standard error.\n$$ t_1 = \\frac{\\hat\\beta_1}{\\text{SE}(\\hat\\beta_1)} \\quad \\text{and} \\quad t_1^{(z)} = \\frac{\\hat\\beta_1^{(z)}}{\\text{SE}(\\hat\\beta_1^{(z)})} $$\nThe standard error of a slope estimate is given by $\\text{SE}(\\hat\\beta_{slope}) = \\sqrt{\\frac{\\hat\\sigma^2}{\\sum (X_i - \\bar{X})^2}}$, where $\\hat\\sigma^2$ is the unbiased estimator of the error variance $\\sigma^2$.\n\nLet's first show that the estimated error variance is the same for both models. The estimate $\\hat\\sigma^2$ is computed from the Residual Sum of Squares (RSS): $\\hat\\sigma^2 = \\frac{\\text{RSS}}{n-2}$.\nFor Model 1, the fitted values are $\\hat y_i = \\hat\\beta_0 + \\hat\\beta_1 x_i$. The OLS intercept is $\\hat\\beta_0 = \\bar y - \\hat\\beta_1 \\bar x$. Thus,\n$$ \\hat y_i = (\\bar y - \\hat\\beta_1 \\bar x) + \\hat\\beta_1 x_i = \\bar y + \\hat\\beta_1(x_i - \\bar x) $$\nFor Model 2, the fitted values are $\\hat y_i^{(z)} = \\hat{\\tilde\\beta}_0 + \\hat\\beta_1^{(z)} z_i$. The OLS intercept is $\\hat{\\tilde\\beta}_0 = \\bar y - \\hat\\beta_1^{(z)} \\bar z$. Since $\\bar z = 0$, we have $\\hat{\\tilde\\beta}_0 = \\bar y$. Thus,\n$$ \\hat y_i^{(z)} = \\bar y + \\hat\\beta_1^{(z)} z_i $$\nNow, substitute the relationships $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ and $z_i = \\frac{x_i - \\bar x}{s_x}$:\n$$ \\hat y_i^{(z)} = \\bar y + (s_x \\hat\\beta_1) \\left( \\frac{x_i - \\bar x}{s_x} \\right) = \\bar y + \\hat\\beta_1(x_i - \\bar x) $$\nWe see that $\\hat y_i^{(z)} = \\hat y_i$. The fitted values are identical for both models. Consequently, the residuals $e_i = y_i - \\hat y_i$ are also identical, the RSS is identical, and the estimated error variance is identical. Let's call it $\\hat\\sigma^2$ for both models.\n\nNow, we can compute the standard errors.\nFor Model 1:\n$$ \\text{SE}(\\hat\\beta_1) = \\sqrt{\\frac{\\hat\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar x)^2}} = \\frac{\\hat\\sigma}{\\sqrt{(n-1)s_x^2}} = \\frac{\\hat\\sigma}{s_x \\sqrt{n-1}} $$\nThe $t$-statistic is:\n$$ t_1 = \\frac{\\hat\\beta_1}{\\text{SE}(\\hat\\beta_1)} = \\frac{\\hat\\beta_1 s_x \\sqrt{n-1}}{\\hat\\sigma} $$\nFor Model 2:\n$$ \\text{SE}(\\hat\\beta_1^{(z)}) = \\sqrt{\\frac{\\hat\\sigma^2}{\\sum_{i=1}^n (z_i - \\bar z)^2}} = \\sqrt{\\frac{\\hat\\sigma^2}{n-1}} = \\frac{\\hat\\sigma}{\\sqrt{n-1}} $$\nThe $t$-statistic is:\n$$ t_1^{(z)} = \\frac{\\hat\\beta_1^{(z)}}{\\text{SE}(\\hat\\beta_1^{(z)})} = \\frac{s_x \\hat\\beta_1}{\\hat\\sigma / \\sqrt{n-1}} = \\frac{\\hat\\beta_1 s_x \\sqrt{n-1}}{\\hat\\sigma} $$\nComparing the final expressions, we see that $t_1^{(z)} = t_1$.\n\n*Alternative Derivation for the t-statistic:*\nThe $t$-statistic for testing the significance of the slope in simple linear regression can also be expressed in terms of the Pearson correlation coefficient $r$ between the predictor and the response:\n$$ t = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}} $$\nThe Pearson correlation coefficient is invariant to separate linear transformations of the two variables. That is, if $X' = aX+b$ and $Y' = cY+d$, then $|\\text{Corr}(X', Y')| = |\\text{Corr}(X, Y)|$. If $a$ and $c$ have the same sign, $\\text{Corr}(X', Y') = \\text{Corr}(X, Y)$.\nIn our case, the response $y$ is unchanged. The predictor is transformed from $x$ to $z$ via $z_i = \\frac{1}{s_x} x_i - \\frac{\\bar x}{s_x}$. This is a linear transformation with a positive scaling factor $\\frac{1}{s_x}$ (since $s_x > 0$). Therefore, the correlation between $z$ and $y$ is the same as the correlation between $x$ and $y$:\n$$ r_{zy} = \\text{Corr}(z, y) = \\text{Corr}(x, y) = r_{xy} $$\nSince the $t$-statistic depends only on the sample size $n$ and the correlation coefficient $r$, and both are identical for the two models, the $t$-statistics themselves must be identical.\n$$ t_1 = \\frac{r_{xy}\\sqrt{n-2}}{\\sqrt{1-r_{xy}^2}} = \\frac{r_{zy}\\sqrt{n-2}}{\\sqrt{1-r_{zy}^2}} = t_1^{(z)} $$\nBoth derivations confirm that $t_1^{(z)} = t_1$.\n\nIn summary:\n- $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$\n- $t_1^{(z)} = t_1$\n\n### Option-by-Option Analysis\n\n**A. $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ and $t_1^{(z)} = t_1$.**\n- The first part, $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$, is consistent with our derivation.\n- The second part, $t_1^{(z)} = t_1$, is also consistent with our derivation.\n- This statement is **Correct**.\n\n**B. $\\hat\\beta_1^{(z)} = \\hat\\beta_1/s_x$ and $t_1^{(z)} = t_1$.**\n- The first part, $\\hat\\beta_1^{(z)} = \\hat\\beta_1/s_x$, contradicts our finding that $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$. This would only be true if $s_x^2=1$, which is not generally the case.\n- Therefore, the statement is **Incorrect**.\n\n**C. $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$ and $t_1^{(z)} = s_x \\, t_1$.**\n- The second part, $t_1^{(z)} = s_x \\, t_1$, contradicts our finding that $t_1^{(z)} = t_1$. This would only be true if $s_x=1$.\n- Therefore, the statement is **Incorrect**.\n\n**D. $\\hat\\beta_1^{(z)} = \\hat\\beta_1$ and $t_1^{(z)} = t_1$.**\n- The first part, $\\hat\\beta_1^{(z)} = \\hat\\beta_1$, contradicts our finding that $\\hat\\beta_1^{(z)} = s_x \\hat\\beta_1$. This would only be true if $s_x=1$.\n- Therefore, the statement is **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Interpreting regression results in the presence of multiple predictors requires an awareness of potential pitfalls, the most common of which is multicollinearity. This final practice delves into this issue, where high correlation between predictors can obscure their individual importance . You will analyze a hypothetical scenario to see how multicollinearity inflates the standard errors of coefficient estimates, often leading to the paradoxical situation of a model with high overall predictive power (high $R^2$) but statistically insignificant individual coefficients.",
            "id": "3131116",
            "problem": "Consider the multiple linear regression model $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$ with i.i.d. errors from a $\\mathcal{N}(0,\\sigma^2)$ distribution and predictors centered so that $\\sum_{i=1}^{n} x_{1i} = 0$ and $\\sum_{i=1}^{n} x_{2i} = 0$. Suppose the design exhibits near-collinearity in the sense that $x_2 = x_1 + \\delta$, where $\\delta$ is a mean-$0$ perturbation independent of $x_1$ with $\\operatorname{Var}(\\delta) = \\tau^2$ that is small relative to $\\operatorname{Var}(x_1) = v_1  0$. Work from first principles of ordinary least squares and the sampling distribution under the Gaussian linear model (in particular, the definitions of the ordinary least squares estimator $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y$, its covariance $\\operatorname{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}$, and the role of orthogonal projections) to analyze how near-collinearity affects the standard error of $\\hat{\\beta}_2$ and the $t$-test of $H_0:\\beta_2=0$.\n\nWhich of the following statements are correct in this setup? Select all that apply.\n\nA. In this construction with $x_2 = x_1 + \\delta$ and centered predictors, the sampling variance of $\\hat{\\beta}_2$ equals $\\sigma^2 \\big/ \\big(n\\, \\operatorname{Var}(\\delta)\\big)$, so as $\\tau^2 \\to 0$ it diverges.\n\nB. The variance inflation factor linking $\\operatorname{Var}(\\hat{\\beta}_2)$ to the univariate regression variance is $(1 - R_2^2)^{-1}$, where $R_2^2$ is from regressing $x_2$ on $x_1$. In this setup $R_2^2 = v_1/(v_1+\\tau^2)$, so the inflation factor equals $(v_1+\\tau^2)/\\tau^2$ and grows without bound as $\\tau^2 \\to 0$.\n\nC. A model can have $R^2$ arbitrarily close to $1$ because of $x_1$ while the test of $H_0:\\beta_2=0$ yields a small $|t_2|$, since only the component of $x_2$ orthogonal to $x_1$ (namely $\\delta$) informs $\\beta_2$, and its variance is small.\n\nD. Because the residual standard error $s$ decreases when more predictors are added, $|t_2|$ must increase when $x_2$ is added to a model that already contains $x_1$.\n\nE. Near-collinearity induces bias in $\\hat{\\beta}_2$ toward $0$, which is the primary reason $|t_2|$ is small even when $R^2$ is large.",
            "solution": "The user wants a detailed analysis of a multiple linear regression model under conditions of near-collinearity.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Model:** $Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$\n-   **Error Distribution:** $\\varepsilon$ are independent and identically distributed (i.i.d.) as $\\mathcal{N}(0,\\sigma^2)$.\n-   **Predictor Centering:** $\\sum_{i=1}^{n} x_{1i} = 0$ and $\\sum_{i=1}^{n} x_{2i} = 0$.\n-   **Collinearity Structure:** $x_2 = x_1 + \\delta$.\n-   **Perturbation Properties:** $\\delta$ is a mean-$0$ perturbation, independent of $x_1$, with $\\operatorname{Var}(\\delta) = \\tau^2$. $\\tau^2$ is small relative to $\\operatorname{Var}(x_1) = v_1 > 0$.\n-   **OLS Estimator:** $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y$.\n-   **Covariance of OLS Estimator:** $\\operatorname{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}$.\n-   **Task:** Analyze the effect of near-collinearity on the standard error of $\\hat{\\beta}_2$ and the $t$-test for $H_0: \\beta_2 = 0$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement describes a canonical scenario used in statistics and econometrics to illustrate the consequences of multicollinearity.\n-   **Scientifically Grounded:** The setup is based on the standard Gaussian linear model and ordinary least squares estimation theory. All concepts are fundamental to statistical learning.\n-   **Well-Posed:** The problem is well-defined. It asks for an analysis of the properties of an estimator within a specified model. The conditions provided are sufficient to perform this analysis.\n-   **Objective:** The language is formal and mathematical, free of subjective content.\n-   **Consistency Check:** The centering conditions $\\sum x_{1i} = 0$ and $\\sum x_{2i} = 0$, combined with $x_{2i} = x_{1i} + \\delta_i$, imply $\\sum (x_{1i} + \\delta_i) = 0$, which gives $\\sum \\delta_i = 0$. This is consistent with $\\delta$ being a mean-$0$ perturbation. The setup is internally consistent.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed to derive the solution.\n\n### Derivation from First Principles\n\nThe core of the problem lies in understanding the variance of the OLS estimator $\\hat{\\beta}_2$. We are invited to work from first principles, including the idea of orthogonal projections. The Frisch-Waugh-Lovell (FWL) theorem is the relevant principle here. It states that the coefficient $\\hat{\\beta}_2$ in the multiple regression is identical to the coefficient of a simple regression of $Y_{\\perp 1}$ on $x_{2, \\perp 1}$, where $Y_{\\perp 1}$ is the vector of residuals from regressing $Y$ on $x_1$, and $x_{2, \\perp 1}$ is the vector of residuals from regressing $x_2$ on $x_1$.\n\nThe variance of $\\hat{\\beta}_2$ is given by:\n$$ \\operatorname{Var}(\\hat{\\beta}_2) = \\frac{\\sigma^2}{\\|\\mathbf{x}_{2, \\perp 1}\\|^2} $$\nwhere $\\|\\mathbf{x}_{2, \\perp 1}\\|^2 = \\sum_{i=1}^n (x_{2i, \\perp 1})^2$ is the sum of squared residuals from regressing $\\mathbf{x}_2$ on $\\mathbf{x}_1$.\n\nLet's compute these residuals. The model is $\\mathbf{x}_2 = \\gamma_1 \\mathbf{x}_1 + \\text{error}$. Since the predictors are centered, the intercept is $0$. The OLS estimate for $\\gamma_1$ is $\\hat{\\gamma}_1 = (\\mathbf{x}_1^\\top \\mathbf{x}_1)^{-1} \\mathbf{x}_1^\\top \\mathbf{x}_2$. The residual vector is $\\mathbf{x}_{2, \\perp 1} = \\mathbf{x}_2 - \\hat{\\gamma}_1 \\mathbf{x}_1$.\nUsing the problem's definition $\\mathbf{x}_2 = \\mathbf{x}_1 + \\mathbf{\\delta}$:\n$$ \\mathbf{x}_{2, \\perp 1} = (\\mathbf{x}_1 + \\mathbf{\\delta}) - \\hat{\\gamma}_1 \\mathbf{x}_1 $$\nWait, it is simpler to use the projection matrix formulation. Let $P_1$ be the projection onto the space spanned by $\\mathbf{x}_1$. Then $\\mathbf{x}_{2, \\perp 1} = (I - P_1)\\mathbf{x}_2$.\nSubstituting $\\mathbf{x}_2 = \\mathbf{x}_1 + \\mathbf{\\delta}$:\n$$ \\mathbf{x}_{2, \\perp 1} = (I - P_1)(\\mathbf{x}_1 + \\mathbf{\\delta}) = (I - P_1)\\mathbf{x}_1 + (I - P_1)\\mathbf{\\delta} = \\mathbf{0} + (I - P_1)\\mathbf{\\delta} $$\nThe term $(I - P_1)\\mathbf{x}_1$ is zero because $\\mathbf{x}_1$ is in its own column space. Thus, the residuals of $\\mathbf{x}_2$ on $\\mathbf{x}_1$ are precisely the residuals of $\\mathbf{\\delta}$ on $\\mathbf{x}_1$.\nThe denominator for the variance of $\\hat{\\beta}_2$ is the squared norm of this residual vector:\n$$ \\|\\mathbf{x}_{2, \\perp 1}\\|^2 = \\|(I - P_1)\\mathbf{\\delta}\\|^2 = \\text{RSS}_{\\delta \\sim x_1} $$\nThis is the residual sum of squares from regressing $\\mathbf{\\delta}$ on $\\mathbf{x}_1$. This is the \"component of $\\mathbf{\\delta}$ orthogonal to $\\mathbf{x}_1$\".\n\nThe problem states that $x_1$ and $\\delta$ are independent. In a finite sample, this does not guarantee that the sample correlation is zero (i.e., $\\sum x_{1i}\\delta_i = 0$). However, for this type of theoretical analysis, it is standard to assume sample properties reflect population properties, which implies sample orthogonality. Let's assume $\\sum_{i=1}^n x_{1i}\\delta_i = 0$. This means that in our sample, $\\mathbf{x}_1$ and $\\mathbf{\\delta}$ are orthogonal.\nUnder this standard idealization, the regression of $\\mathbf{\\delta}$ on $\\mathbf{x}_1$ yields a zero coefficient, and the residuals are just $\\mathbf{\\delta}$ itself.\n$$ \\text{If } \\mathbf{x}_1^\\top \\mathbf{\\delta} = 0, \\text{ then } \\mathbf{x}_{2, \\perp 1} = (I-P_1)\\mathbf{\\delta} = \\mathbf{\\delta}. $$\nTherefore, $\\|\\mathbf{x}_{2, \\perp 1}\\|^2 = \\|\\mathbf{\\delta}\\|^2 = \\sum_{i=1}^n \\delta_i^2$.\nSince $\\delta$ is a mean-0 perturbation, its sample variance (using $n$ in the denominator for simplicity, as is common) is $\\frac{1}{n} \\sum \\delta_i^2$. The problem gives this quantity as $\\tau^2 = \\operatorname{Var}(\\delta)$. So, $\\sum \\delta_i^2 = n \\tau^2 = n \\operatorname{Var}(\\delta)$.\n\nThus, under this standard simplification, we have:\n$$ \\operatorname{Var}(\\hat{\\beta}_2) = \\frac{\\sigma^2}{\\sum_{i=1}^n \\delta_i^2} = \\frac{\\sigma^2}{n \\tau^2} = \\frac{\\sigma^2}{n \\operatorname{Var}(\\delta)} $$\nAs $\\tau^2 \\to 0$, this variance grows without bound.\n\n### Option-by-Option Analysis\n\n**A. In this construction with $x_2 = x_1 + \\delta$ and centered predictors, the sampling variance of $\\hat{\\beta}_2$ equals $\\sigma^2 \\big/ \\big(n\\, \\operatorname{Var}(\\delta)\\big)$, so as $\\tau^2 \\to 0$ it diverges.**\n\nAs derived above, under the standard simplifying assumption of sample orthogonality ($\\sum x_{1i}\\delta_i=0$) that is characteristic of such theoretical problems, the variance is precisely $\\operatorname{Var}(\\hat{\\beta}_2) = \\sigma^2 / (n \\operatorname{Var}(\\delta))$. The quantity $\\operatorname{Var}(\\delta)$ is denoted $\\tau^2$. As $\\tau^2 \\to 0$, the denominator approaches $0$, and thus the variance diverges to infinity. Even without the simplifying assumption, the denominator $\\text{RSS}_{\\delta \\sim x_1}$ is of order $n\\tau^2$, so the conclusion that the variance diverges remains correct. The statement is a correct representation of the effect of near-collinearity.\n\n**Verdict: Correct**\n\n**B. The variance inflation factor linking $\\operatorname{Var}(\\hat{\\beta}_2)$ to the univariate regression variance is $(1 - R_2^2)^{-1}$, where $R_2^2$ is from regressing $x_2$ on $x_1$. In this setup $R_2^2 = v_1/(v_1+\\tau^2)$, so the inflation factor equals $(v_1+\\tau^2)/\\tau^2$ and grows without bound as $\\tau^2 \\to 0$.**\n\nThe variance inflation factor for $\\hat{\\beta}_2$ is $\\text{VIF}_2 = (1 - R_2^2)^{-1}$, where $R_2^2$ is the coefficient of determination from regressing $x_2$ on $x_1$.\n$R_2^2$ is defined as the square of the correlation between $x_1$ and $x_2$. For centered variables, $R_2^2= (\\sum x_{1i}x_{2i})^2 / ((\\sum x_{1i}^2)(\\sum x_{2i}^2))$.\nLet's use our simplified quantities:\n- $\\sum x_{1i}^2 = n \\operatorname{Var}(x_1) = n v_1$.\n- $\\sum x_{2i}^2 = \\sum (x_{1i}+\\delta_i)^2 = \\sum x_{1i}^2 + \\sum \\delta_i^2 + 2\\sum x_{1i}\\delta_i = n v_1 + n \\tau^2 + 0 = n(v_1+\\tau^2)$.\n- $\\sum x_{1i}x_{2i} = \\sum x_{1i}(x_{1i}+\\delta_i) = \\sum x_{1i}^2 + \\sum x_{1i}\\delta_i = n v_1 + 0 = n v_1$.\n\nPlugging these in:\n$$ R_2^2 = \\frac{(n v_1)^2}{(n v_1)(n(v_1+\\tau^2))} = \\frac{v_1}{v_1 + \\tau^2} $$\nThis matches the formula in the option. As $\\tau^2 \\to 0$, $R_2^2 \\to v_1/v_1 = 1$, indicating perfect collinearity.\nThe inflation factor is:\n$$ \\text{VIF}_2 = \\frac{1}{1 - R_2^2} = \\frac{1}{1 - \\frac{v_1}{v_1+\\tau^2}} = \\frac{1}{\\frac{(v_1+\\tau^2)-v_1}{v_1+\\tau^2}} = \\frac{v_1+\\tau^2}{\\tau^2} $$\nThis also matches the formula in the option. As $\\tau^2 \\to 0$, the numerator approaches $v_1 > 0$ and the denominator approaches $0$, so the VIF grows without bound. This statement is entirely correct.\n\n**Verdict: Correct**\n\n**C. A model can have $R^2$ arbitrarily close to $1$ because of $x_1$ while the test of $H_0:\\beta_2=0$ yields a small $|t_2|$, since only the component of $x_2$ orthogonal to $x_1$ (namely $\\delta$) informs $\\beta_2$, and its variance is small.**\n\nThis statement provides the conceptual explanation for the phenomenon of multicollinearity.\n1.  **High $R^2$**: If $x_1$ is a strong predictor of $Y$, the overall model $R^2$ can be high regardless of the contribution of $x_2$.\n2.  **Small $|t_2|$**: The $t$-statistic for $\\beta_2$ is $t_2 = \\hat{\\beta}_2 / \\text{SE}(\\hat{\\beta}_2)$. As shown in A and B, near-collinearity causes the standard error $\\text{SE}(\\hat{\\beta}_2)$ to become very large. Even if the true $\\beta_2$ is non-zero, a large denominator will result in a small $|t_2|$, leading to a failure to reject $H_0: \\beta_2 = 0$.\n3.  **Reason**: The reasoning provided is precisely correct and follows from the FWL theorem. The unique information contributed by $x_2$ for estimating its coefficient $\\beta_2$ is contained in its component orthogonal to $x_1$. We showed this component is $(I-P_1)\\mathbf{\\delta}$. The \"variance\" of this component (its sum of squares) is $\\|\\mathbf{x}_{2, \\perp 1}\\|^2 \\approx n\\tau^2$, which is small when $\\tau^2$ is small. A small amount of information leads to high uncertainty in the estimate, i.e., a large standard error.\nThe statement correctly identifies the classic symptom (high overall $R^2$, low individual $|t|$-statistics) and cause of multicollinearity.\n\n**Verdict: Correct**\n\n**D. Because the residual standard error $s$ decreases when more predictors are added, $|t_2|$ must increase when $x_2$ is added to a model that already contains $x_1$.**\n\nThis statement is flawed for two reasons.\n1.  The premise, \"the residual standard error $s$ decreases when more predictors are added,\" is not strictly true. The residual standard error is $s = \\sqrt{\\text{RSS}/(n-p-1)}$. Adding a predictor never increases RSS, but it decreases the degrees of freedom in the denominator $(n-p-1)$. If the reduction in RSS is trivial, $s$ can actually increase.\n2.  More importantly, the conclusion is the opposite of the truth. The $t$-statistic is $t_2 = \\hat{\\beta}_2 / \\text{SE}(\\hat{\\beta}_2)$. The standard error is $\\text{SE}(\\hat{\\beta}_2) = s / \\sqrt{\\|\\mathbf{x}_{2, \\perp 1}\\|^2}$. In the context of near-collinearity, the denominator term $\\|\\mathbf{x}_{2, \\perp 1}\\|^2$ is very small (approaching $0$). This makes $\\text{SE}(\\hat{\\beta}_2)$ very large, which in turn makes $|t_2|$ *small*, not large. The statement makes a claim that directly contradicts the primary effect of multicollinearity.\n\n**Verdict: Incorrect**\n\n**E. Near-collinearity induces bias in $\\hat{\\beta}_2$ toward $0$, which is the primary reason $|t_2|$ is small even when $R^2$ is large.**\n\nThis statement misattributes the effect of multicollinearity. The Ordinary Least Squares (OLS) estimator is unbiased as long as the Gauss-Markov assumptions hold, which they do in this problem's setup (specifically, $E[\\varepsilon|X]=0$).\nThe expected value of the OLS estimator is $E[\\hat{\\beta}] = \\beta$. This holds regardless of the degree of collinearity, as long as it is not perfect (i.e., $X^\\top X$ is invertible). Near-collinearity does not introduce bias. Instead, it inflates the *variance* of the estimators. The primary reason $|t_2|$ is small is the large standard error (the denominator of the $t$-statistic), not a bias in the estimate $\\hat{\\beta}_2$ (the numerator).\n\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}