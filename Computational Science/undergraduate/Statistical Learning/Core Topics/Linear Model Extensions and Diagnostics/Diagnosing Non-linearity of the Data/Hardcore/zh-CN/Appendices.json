{
    "hands_on_practices": [
        {
            "introduction": "我们从一个最基本的问题开始：是什么让一个数据集具有非线性？在深入研究复杂的回归问题之前，我们将使用经典的“异或”（XOR）分类问题进行一个思想实验。这个练习  将通过严谨的推导，揭示线性模型为何会失效，并阐明需要什么样的解决方案，从而为后续所有实践奠定概念基础。",
            "id": "3114954",
            "problem": "考虑一个二元分类任务，其特征向量为 $x = (x_1, x_2)$，标签为 $y \\in \\{-1, +1\\}$。训练集由单位正方形的四个角点组成，具体为点 $(0,0)$、$(0,1)$、$(1,0)$、$(1,1)$，其中标签由异或（exclusive-or）规则给出：如果 $x_1$ 或 $x_2$ 中只有一个等于 $1$，则 $y = +1$，否则 $y = -1$。在原始特征空间中，线性分类器使用形如 $f(x) = \\operatorname{sign}(w_1 x_1 + w_2 x_2 + b)$ 的决策函数，其中 $(w_1, w_2) \\in \\mathbb{R}^2$ 且 $b \\in \\mathbb{R}$。线性可分性的概念要求存在参数 $(w_1, w_2, b)$，使得对于每个训练点，都有 $y \\cdot (w_1 x_1 + w_2 x_2 + b) > 0$。\n\n使用此设定并从上述定义出发，判断训练数据在原始空间中是否线性可分，并选择下面所有陈述中，既 (i) 构成基于第一性原理的正确诊断论证，又 (ii) 提出了能够实现零训练误差的有效补救措施的选项。\n\nA. 为实现对 $(0,0)$、$(1,0)$、$(0,1)$、$(1,1)$ 的完美分类而建立的线性不等式组会揭示一个矛盾，从而证明了非线性可分性。一个补救措施是用中心化的交互项 $z = (x_1 - \\tfrac{1}{2})(x_2 - \\tfrac{1}{2})$ 来增强特征，并在 $(x_1, x_2, z)$ 上拟合一个线性分类器，这样可以线性地分离这四个点。\n\nB. 将 $x_1$ 和 $x_2$ 标准化为零均值和单位方差将使数据在原始空间中变得线性可分；缩放会充分改变几何结构，使得一个超平面可以在不引入非线性特征的情况下分离这些类别。\n\nC. 仅仅在决策函数 $f(x) = \\operatorname{sign}(w_1 x_1 + w_2 x_2 + b)$ 中添加一个偏置项 $b$（不引入任何新特征）就足以创建一个能够完美分类单位正方形角点上异或标签的决策边界。\n\nD. 使用 2 次多项式核或径向基函数（RBF）核训练一个硬间隔或大惩罚项的支持向量机将实现零训练误差；线性支持向量机无法在该数据集上实现零训练误差本身就是对非线性性的一个有效诊断。\n\nE. 应用主成分分析（PCA）并保留单个主成分将揭示一个能够线性分离异或类别的一维投影，因此仅 PCA 本身就是对非线性性的一个充分补救措施。",
            "solution": "对问题陈述进行验证。\n\n**步骤 1：提取已知条件**\n-   **任务：** 二元分类。\n-   **特征向量：** $x = (x_1, x_2)$。\n-   **标签空间：** $y \\in \\{-1, +1\\}$。\n-   **训练集：** 四个点 $(0,0)$、$(0,1)$、$(1,0)$ 和 $(1,1)$。\n-   **标签规则：** 异或（XOR）规则，如果 $x_1$ 或 $x_2$ 中只有一个等于 $1$，则 $y = +1$，否则 $y = -1$。\n-   **分类器模型：** 线性分类器，决策函数为 $f(x) = \\operatorname{sign}(w_1 x_1 + w_2 x_2 + b)$，其中 $(w_1, w_2) \\in \\mathbb{R}^2$ 且 $b \\in \\mathbb{R}$。\n-   **线性可分性定义：** 存在参数 $(w_1, w_2, b)$，使得对于训练集中的每个点，不等式 $y \\cdot (w_1 x_1 + w_2 x_2 + b) > 0$ 都成立。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学性：** 该问题描述了经典的异或问题，这是机器学习中的一个基本概念，用来说明线性模型的局限性。所有概念，包括线性分类器、线性可分性、特征增强和核函数，在统计学习领域都是标准且明确定义的。该问题具有科学合理性。\n-   **适定性：** 问题是适定的，要求对一个性质（线性可分性）进行诊断，并评估所提出的补救措施。所提供的定义足以得出一个唯一的、严谨的结论。\n-   **客观性：** 问题使用精确、客观的数学语言陈述。数据、标签规则和可分性条件都得到了正式定义。\n-   **完整性和一致性：** 问题提供了所有必要信息，并且不包含内部矛盾。\n\n**步骤 3：结论和行动**\n问题陈述有效。可以继续求解过程。\n\n**线性可分性诊断的推导**\n首先，我们根据异或规则确定每个训练点的标签：\n-   对于 $x=(0,0)$：$x_1$ 和 $x_2$ 都不是 $1$，所以 $y = -1$。\n-   对于 $x=(0,1)$：$x_1, x_2$ 中只有一个是 $1$，所以 $y = +1$。\n-   对于 $x=(1,0)$：$x_1, x_2$ 中只有一个是 $1$，所以 $y = +1$。\n-   对于 $x=(1,1)$：$x_1$ 和 $x_2$ 都是 $1$，所以 $y = -1$。\n\n接下来，我们应用线性可分性的定义，这要求存在参数 $w_1, w_2, b$ 同时满足以下四个不等式：\n$1.$ 对于 $(0,0)$ 且 $y=-1$：$(-1) \\cdot (w_1 \\cdot 0 + w_2 \\cdot 0 + b) > 0 \\implies -b > 0 \\implies b  0$。\n$2.$ 对于 $(0,1)$ 且 $y=+1$：$(+1) \\cdot (w_1 \\cdot 0 + w_2 \\cdot 1 + b)  0 \\implies w_2 + b  0$。\n$3.$ 对于 $(1,0)$ 且 $y=+1$：$(+1) \\cdot (w_1 \\cdot 1 + w_2 \\cdot 0 + b)  0 \\implies w_1 + b  0$。\n$4.$ 对于 $(1,1)$ 且 $y=-1$：$(-1) \\cdot (w_1 \\cdot 1 + w_2 \\cdot 1 + b)  0 \\implies -(w_1 + w_2 + b)  0 \\implies w_1 + w_2 + b  0$。\n\n我们来分析这个不等式组。\n从不等式 ($1$)，我们知道 $b$ 是严格负数。这意味着 $-b$ 是严格正数。\n从不等式 ($2$) 和 ($3$)，我们可以得出：\n-   $w_2  -b$\n-   $w_1  -b$\n由于 $-b  0$，所以 $w_1$ 和 $w_2$ 都必须是正数。\n将这两个不等式相加得到 $w_1 + w_2  -2b$。\n\n从不等式 ($4$)，我们可以得出：\n-   $w_1 + w_2  -b$\n\n因此，我们得到了关于量 $w_1 + w_2$ 的两个相互矛盾的约束：\n-   $w_1 + w_2  -2b$\n-   $w_1 + w_2  -b$\n由于 $b  0$，我们有 $-b > 0$，这意味着 $-2b  -b$。条件 $w_1 + w_2  -2b$ 与 $w_1 + w_2  -b$ 并不构成直接矛盾。让我们完善这个论证。\n我最初的矛盾证明是有缺陷的。让我们找出正确的矛盾。\n\n我们再次审视：\n1. $-b  0$\n2. $w_2 + b  0$\n3. $w_1 + b  0$\n4. $-(w_1 + w_2 + b)  0$\n\n将不等式 ($1$)、($2$) 和 ($3$) 相加：$(-b) + (w_2+b) + (w_1+b)  0 \\implies w_1+w_2+b  0$。\n这个结果 $w_1+w_2+b  0$ 与不等式 ($4$)（即 $w_1+w_2+b  0$）直接矛盾。\n这构成了一个严格的矛盾证明。该不等式组无解。因此，数据不是线性可分的。\n\n**逐项分析**\n\n**A. 为实现对 $(0,0)$、$(1,0)$、$(0,1)$、$(1,1)$ 的完美分类而建立的线性不等式组会揭示一个矛盾，从而证明了非线性可分性。一个补救措施是用中心化的交互项 $z = (x_1 - \\tfrac{1}{2})(x_2 - \\tfrac{1}{2})$ 来增强特征，并在 $(x_1, x_2, z)$ 上拟合一个线性分类器，这样可以线性地分离这四个点。**\n- **诊断论证：** 该陈述声称建立线性不等式组会揭示一个矛盾。如上所述，这正是正确的方法和结论。陈述的这一部分是正确的。\n- **提出的补救措施：** 补救措施是用 $z = (x_1 - \\tfrac{1}{2})(x_2 - \\tfrac{1}{2})$ 增强特征空间。我们来计算每个点的 $z$ 值：\n  - 对于 $x=(0,0)$: $z = (0 - 0.5)(0 - 0.5) = 0.25$。新点：$(0, 0, 0.25)$，标签 $y=-1$。\n  - 对于 $x=(0,1)$: $z = (0 - 0.5)(1 - 0.5) = -0.25$。新点：$(0, 1, -0.25)$，标签 $y=+1$。\n  - 对于 $x=(1,0)$: $z = (1 - 0.5)(0 - 0.5) = -0.25$。新点：$(1, 0, -0.25)$，标签 $y=+1$。\n  - 对于 $x=(1,1)$: $z = (1 - 0.5)(1 - 0.5) = 0.25$。新点：$(1, 1, 0.25)$，标签 $y=-1$。\n在新的特征空间 $(x_1, x_2, z)$ 中，我们寻找一个分离超平面 $w_1 x_1 + w_2 x_2 + w_z z + b = 0$。\n注意到标签为 $y=+1$ 的点都有 $z = -0.25$，而标签为 $y=-1$ 的点都有 $z = 0.25$。\n一个简单的线性分类器可以利用这一点。我们选择 $w_1=0$, $w_2=0$, $b=0$ 和 $w_z = -4$。决策函数变为 $\\operatorname{sign}(-4z)$。\n  - 对于 $y=-1$，$z=0.25$，所以 $\\operatorname{sign}(-4 \\cdot 0.25) = \\operatorname{sign}(-1) = -1$。正确。\n  - 对于 $y=+1$，$z=-0.25$，所以 $\\operatorname{sign}(-4 \\cdot -0.25) = \\operatorname{sign}(+1) = +1$。正确。\n由超平面 $z=0$ 定义的分类器完美地分离了数据。这个补救措施是有效的，并且实现了零训练误差。\n- **结论：** 正确。\n\n**B. 将 $x_1$ 和 $x_2$ 标准化为零均值和单位方差将使数据在原始空间中变得线性可分；缩放会充分改变几何结构，使得一个超平面可以在不引入非线性特征的情况下分离这些类别。**\n- **分析：** 标准化是一种仿射变换，它是一种线性变换。对于一个特征向量 $x$，标准化后的向量是 $x' = D(x - \\mu)$，其中 $\\mu$ 是均值向量，$D$ 是逆标准差的对角矩阵。如果一个数据集是线性可分的，那么存在一个向量 $w$ 和标量 $b$ 使得 $y (w^T x + b)  0$。变换后，我们在寻找 $w'$ 和 $b'$ 使得 $y(w'^T x' + b')  0$。代入 $x' = D(x - \\mu)$ 得到 $y(w'^T D(x-\\mu) + b')  0$，可以简化为 $y((w'^T D)x - w'^T D \\mu + b')  0$。这具有 $y(\\tilde{w}^T x + \\tilde{b})  0$ 的形式，其中 $\\tilde{w}^T = w'^T D$ 且 $\\tilde{b} = -w'^T D \\mu + b'$。反之，如果一个数据集不是线性可分的，那么就不存在这样的 $w$ 和 $b$。应用线性变换不能在原来不可分的地方创造可分性。类别之间基本的几何排列关系被保留了下来。异或问题仍然是异或问题，只是坐标不同而已。\n- **结论：** 错误。\n\n**C. 仅仅在决策函数 $f(x) = \\operatorname{sign}(w_1 x_1 + w_2 x_2 + b)$ 中添加一个偏置项 $b$（不引入任何新特征）就足以创建一个能够完美分类单位正方形角点上异或标签的决策边界。**\n- **分析：** 这个陈述基于一个有缺陷的前提。原始问题陈述已经将线性分类器定义为包含偏置项 $b$。上面展示的非可分性证明正是针对这个模型 $f(x) = \\operatorname{sign}(w_1 x_1 + w_2 x_2 + b)$进行的。我们证明了该模型是不充分的。因此，添加偏置项的“补救措施”并不能解决问题，因为即使存在偏置项，问题依然存在。\n- **结论：** 错误。\n\n**D. 使用 2 次多项式核或径向基函数（RBF）核训练一个硬间隔或大惩罚项的支持向量机将实现零训练误差；线性支持向量机无法在该数据集上实现零训练误差本身就是对非线性性的一个有效诊断。**\n- **诊断论证：** 线性支持向量机（SVM）寻找分离数据的最大间隔超平面。如果数据不是线性可分的，硬间隔线性SVM将找不到解。软间隔线性SVM（使用大的误分类惩罚）会找到一个解，但会产生非零的训练误差。这种无法达到零误差的情况是数据非线性可分的直接后果，因此是一个有效的诊断。这部分是正确的。\n- **提出的补救措施：** “核技巧”是处理非线性数据的标准SVM方法。它的工作原理是将数据隐式地映射到一个更高维的特征空间，在这个空间中数据变得线性可分。\n  - 一个2次多项式核，$K(u,v) = (\\gamma u^T v + c_0)^2$，对应于一个包含像 $x_1^2$、$x_2^2$ 以及关键的交互项 $x_1 x_2$ 的特征空间。正如在选项 A 的分析中所示，添加一个交互项（或其仿射变换）足以使异或数据线性可分。因此，一个带有2次多项式核的SVM可以在特征空间中找到一个分离超平面，从而得到零训练误差。\n  - RBF核，$K(u,v) = \\exp(-\\gamma \\|u-v\\|^2)$，是一个通用核。它可以逼近任何连续的决策边界。对于像异或问题这样的有限数据集，带有RBF核的SVM总能找到一个分离函数，它有效地在数据点上放置类高斯函数来区分类别。它可以实现零训练误差。\n- **结论：** 正确。\n\n**E. 应用主成分分析（PCA）并保留单个主成分将揭示一个能够线性分离异或类别的一维投影，因此仅 PCA 本身就是对非线性性的一个充分补救措施。**\n- **分析：** PCA 是一种线性降维技术。它为数据找到一个正交基，使得数据在这些轴上的方差最大化。该变换涉及坐标系的旋转和投影。作为一种线性变换，它不能使非线性可分的数据集变得线性可分。具体来说，四个数据点 $(0,0), (0,1), (1,0), (1,1)$ 的均值为 $(0.5, 0.5)$。中心化后的点为 $(-0.5, -0.5), (-0.5, 0.5), (0.5, -0.5), (0.5, 0.5)$。该数据的协方差矩阵与单位矩阵成正比，$\\begin{pmatrix} 0.25  0 \\\\ 0  0.25 \\end{pmatrix}$。这意味着方差在所有方向上都是相等的；不存在唯一的主成分。如果我们将数据投影到任何一维直线上（例如，$x_1$ 轴），$+1$ 类和 $-1$ 类的投影点将混合在一起，使得线性分离变得不可能。例如，投影到 $x_1$ 轴上会将 $(0,1)$（$+1$ 类）和 $(0,0)$（$-1$ 类）映射到相同的值 $x_1=0$。在投影空间中不可能将它们分开。\n- **结论：** 错误。",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "在建立了非线性的概念之后，我们转向一个实际的回归场景。这个练习  要求你模拟一种在科学和工程中常见的非线性形式——“饱和效应”。你将通过对比线性和非线性模型的拟合优度，以及（至关重要地）检查不充分的线性模型残差中是否存留有规律的模式，来学习如何诊断这种非线性关系。",
            "id": "3114995",
            "problem": "给定一个表现出饱和特性的数据生成过程。响应变量根据模型 $$y = \\alpha + \\beta\\left(1 - e^{-\\gamma x}\\right) + \\varepsilon,$$ 生成，其中 $x$ 是预测变量，$\\alpha$ 是截距，$\\beta$ 是振幅参数，$\\gamma$ 是控制饱和度的速率参数，$\\varepsilon$ 是均值为零的噪声项。您的任务是实现一个程序，对每个指定的测试用例，根据此模型模拟数据，拟合线性和非线性模型，计算诊断指标以评估非线性，然后为每个测试用例输出一个布尔决策，指示诊断指标是否检测到非线性。\n\n任务的基本原理：\n- 普通最小二乘法 (Ordinary Least Squares, OLS) 通过最小化残差平方和来拟合线性模型。给定观测对 $(x_i, y_i)$，$i=1,\\dots,n$，线性模型 $$y_i = a + b x_i + \\text{residual}_i$$ 通过最小化 $$\\sum_{i=1}^n \\left(y_i - a - b x_i\\right)^2$$ 来拟合。令拟合值为 $\\hat{y}_i^{\\text{lin}}$，残差为 $r_i^{\\text{lin}} = y_i - \\hat{y}_i^{\\text{lin}}$。\n- 非线性最小二乘法 (Nonlinear Least Squares) 通过最小化残差平方和来拟合参数化非线性模型。对于饱和模型 $$y_i = \\alpha + \\beta\\left(1 - e^{-\\gamma x_i}\\right) + \\text{residual}_i,$$ 我们通过最小化 $$\\sum_{i=1}^n \\left(y_i - \\alpha - \\beta\\left(1 - e^{-\\gamma x_i}\\right)\\right)^2$$ 来获得拟合值 $\\hat{y}_i^{\\text{nl}}$ 和残差 $r_i^{\\text{nl}} = y_i - \\hat{y}_i^{\\text{nl}}$。\n- 一组残差 $\\{r_i\\}_{i=1}^n$ 的均方误差 (mean squared error, MSE) 为 $$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n r_i^2.$$\n- 决定系数 (coefficient of determination) 定义为 $$R^2 = 1 - \\frac{\\sum_{i=1}^n r_i^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2},$$ 其中 $\\bar{y}$ 是 $y_i$ 的样本均值。\n- 两个序列 $(u_i)$ 和 $(v_i)$ 之间的斯皮尔曼等级相关系数 (Spearman rank correlation coefficient) 衡量基于秩的单调关联。记作 $\\rho_s(u, v)$。\n\n检测非线性的诊断标准：\n- 为每个案例计算线性拟合和非线性饱和拟合。从每次拟合中，计算 $\\text{MSE}_{\\text{lin}}$、$\\text{MSE}_{\\text{nl}}$、$R^2_{\\text{lin}}$、$R^2_{\\text{nl}}$，以及线性残差 $r^{\\text{lin}}$ 与预测变量 $x$ 之间的斯皮尔曼等级相关系数 $\\rho_s(r^{\\text{lin}}, x)$。\n- 如果以下两个条件同时成立，则判定该测试用例检测到非线性：\n  1. $$\\frac{\\text{MSE}_{\\text{lin}}}{\\text{MSE}_{\\text{nl}}} \\ge \\tau,$$ 其中 $\\tau = 1.05$。\n  2. $$\\left|\\rho_s\\left(r^{\\text{lin}}, x\\right)\\right| \\ge t,$$ 其中 $t = 0.25$。\n这些标准将两个互补的诊断方法操作化：非线性拟合带来的误差显著改善，以及线性残差中存在的单调结构。\n\n数据模拟详情：\n- 对于每个测试用例，在区间 $[0, x_{\\max}]$（含端点）上生成 $n$ 个等间距的点 $x$。\n- 为每个观测值独立地生成 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。\n- 使用为每个测试用例指定的参数 $(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n)$。\n- 为保证可复现性，使用固定的随机数生成器种子 $12345$。\n\n测试套件：\n- 案例 1：$(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n) = (0.5, 1.5, 0.02, 0.1, 5.0, 200)$\n- 案例 2：$(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n) = (0.5, 1.5, 1.0, 0.1, 5.0, 200)$\n- 案例 3：$(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n) = (0.5, 0.0, 0.5, 0.1, 5.0, 200)$\n- 案例 4：$(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n) = (0.5, 1.5, 0.5, 1.5, 5.0, 200)$\n- 案例 5：$(\\alpha, \\beta, \\gamma, \\sigma, x_{\\max}, n) = (0.5, 1.0, 0.5, 0.2, 3.0, 150)$\n\n您的程序必须：\n- 为每个测试用例实现数据模拟和两种模型拟合。\n- 计算 $\\text{MSE}_{\\text{lin}}$、$\\text{MSE}_{\\text{nl}}$、$R^2_{\\text{lin}}$、$R^2_{\\text{nl}}$ 和 $\\rho_s\\left(r^{\\text{lin}}, x\\right)$。\n- 应用决策规则（$\\tau = 1.05$ 和 $t = 0.25$），为每个案例生成一个布尔值，以指示是否检测到非线性。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $$[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_5],$$ 其中每个 $\\text{result}_k$ 是按顺序对应于测试用例 $k$ 的布尔值。",
            "solution": "我们从基本框架开始：普通最小二乘法 (OLS) 旨在寻找最小化残差平方和的参数。对于线性模型 $$y_i = a + b x_i + \\text{residual}_i,$$ OLS 估计值是通过最小化 $$\\sum_{i=1}^n \\left(y_i - a - b x_i\\right)^2$$ 获得的。此优化问题可以通过正规方程或最小二乘求解器解决；其有效性依赖于以下性质：在经典假设下（如误差独立、均值为零且方差有限），OLS 提供线性模型参数的无偏估计量，并且根据 Gauss-Markov 定理，它是最佳线性无偏估计量 (Best Linear Unbiased Estimator, BLUE)。即使没有这些假设，OLS 也能得到残差平方和的最小化器。\n\n对于饱和非线性模型 $$y_i = \\alpha + \\beta\\left(1 - e^{-\\gamma x_i}\\right) + \\text{residual}_i,$$ 非线性最小二乘法旨在最小化 $$\\sum_{i=1}^n \\left(y_i - \\alpha - \\beta\\left(1 - e^{-\\gamma x_i}\\right)\\right)^2.$$ 尽管非线性模型通常没有封闭形式的解，但迭代数值优化方法（如信赖域方法）可以从有根据的初始猜测值开始，有效地估计 $(\\alpha, \\beta, \\gamma)$。良好的初始值可以通过简单的启发式方法构建：例如，$\\alpha$ 可初始化为接近 $y$ 的最小值，$\\beta$ 接近 $y$ 的范围，而 $\\gamma$ 接近 $1/x_{\\max}$ 以反映饱和的尺度。\n\n诊断指标：\n- 均方误差 (MSE) $$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n r_i^2$$ 量化了平均残差平方的大小。比较 $\\text{MSE}_{\\text{lin}}$ 和 $\\text{MSE}_{\\text{nl}}$ 可以表明非线性拟合相对于线性拟合减少了多少误差。\n- 决定系数 $$R^2 = 1 - \\frac{\\sum_{i=1}^n r_i^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$$ 衡量了模型解释的方差比例。虽然它不用于决策规则，但提供了补充性的背景信息。\n- 斯皮尔曼等级相关系数 $\\rho_s(r^{\\text{lin}}, x)$ 捕捉线性残差相对于 $x$ 的单调模式。如果真实关系是非线性的（例如，饱和的），线性拟合通常会留下结构化的残差，这些残差表现出与 $x$ 的单调趋势。斯皮尔曼系数对非高斯分布具有稳健性，并反映了基于秩的单调关联，因此适合于检测基于顺序的结构。\n\n决策规则：\n- 如果两个独立出发的诊断指标都表明存在非线性，我们就判定存在非线性：\n  1. 非线性拟合带来的 MSE 充分改善：$$\\frac{\\text{MSE}_{\\text{lin}}}{\\text{MSE}_{\\text{nl}}} \\ge \\tau,$$ 其中 $\\tau = 1.05$ 被设定为要求至少有适度的改善。\n  2. 线性残差中存在显著的单调结构：$$\\left|\\rho_s\\left(r^{\\text{lin}}, x\\right)\\right| \\ge t,$$ 其中 $t = 0.25$ 被选择用来标记在线性模型设定良好的情况下不太可能出现的残差排序。\n\n每个测试用例的算法步骤：\n1. 在 $[0, x_{\\max}]$ 上生成 $n$ 个等间距的 $x$。使用固定的种子以保证可复现性，抽取 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。构成 $$y_i = \\alpha + \\beta\\left(1 - e^{-\\gamma x_i}\\right) + \\varepsilon_i.$$\n2. 通过最小二乘法拟合线性模型以获得 $(a, b)$，计算拟合值 $\\hat{y}_i^{\\text{lin}}$ 和残差 $r_i^{\\text{lin}} = y_i - \\hat{y}_i^{\\text{lin}}$。\n3. 通过非线性最小二乘法（带边界的信赖域方法）拟合非线性饱和模型，按所述方式初始化 $(\\alpha, \\beta, \\gamma)$，并计算拟合值 $\\hat{y}_i^{\\text{nl}}$ 和残差 $r_i^{\\text{nl}}$。\n4. 计算 $\\text{MSE}_{\\text{lin}}$、$\\text{MSE}_{\\text{nl}}$、$R^2_{\\text{lin}}$、$R^2_{\\text{nl}}$ 和 $\\rho_s\\left(r^{\\text{lin}}, x\\right)$。\n5. 应用决策规则（$\\tau = 1.05$ 和 $t = 0.25$）为非线性生成一个布尔分类结果。\n\n测试套件的覆盖范围：\n- 案例 1 使用小的 $\\gamma$ 来近似线性行为（当 $\\gamma x$ 很小时，$e^{-\\gamma x} \\approx 1 - \\gamma x$），因此非线性模型的表现不应显著优于线性模型，且残差的单调结构应该很弱。\n- 案例 2 使用大的 $\\gamma$ 以实现快速饱和，这导致线性拟合在残差中留下明显的结构，而非线性拟合则能提供显著的误差减少。\n- 案例 3 设置 $\\beta = 0$，使得 $y$ 是常数加噪声，因此不应检测到与 $x$ 的关系，并且两种模型都不应有明显偏好。\n- 案例 4 引入了高噪声和中等大小的 $\\gamma$，以致于信号被掩盖；诊断结果应趋于保守。\n- 案例 5 在较短的 $x$ 区间上引入了中等程度的饱和和中等程度的噪声，根据标准可以检测到非线性。\n\n程序将所有案例的布尔结果聚合成一个指定的、用方括号括起来的逗号分隔列表，并通过固定的随机种子确保确定性的输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import curve_fit\nfrom scipy.stats import spearmanr\n\ndef saturating_model(x, alpha, beta, gamma):\n    return alpha + beta * (1.0 - np.exp(-gamma * x))\n\ndef fit_linear(x, y):\n    # Design matrix for linear model: [1, x]\n    X = np.column_stack((np.ones_like(x), x))\n    # Solve least squares\n    coef, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n    a, b = coef\n    y_hat = a + b * x\n    residuals = y - y_hat\n    return a, b, y_hat, residuals\n\ndef fit_nonlinear(x, y):\n    # Initial guesses: alpha near min(y), beta near (max(y)-min(y)),\n    # gamma near 1/x_max (positive).\n    y_min = np.min(y)\n    y_max = np.max(y)\n    alpha0 = y_min\n    beta0 = max(y_max - y_min, 1e-3)\n    x_max = np.max(x)\n    gamma0 = 1.0 / max(x_max, 1.0)\n    p0 = (alpha0, beta0, gamma0)\n\n    # Bounds: gamma > 0; alpha and beta unbounded.\n    bounds = ((-np.inf, -np.inf, 1e-9), (np.inf, np.inf, np.inf))\n    try:\n        popt, _ = curve_fit(saturating_model, x, y, p0=p0, bounds=bounds, maxfev=10000)\n        alpha, beta, gamma = popt\n        y_hat = saturating_model(x, alpha, beta, gamma)\n        residuals = y - y_hat\n        return alpha, beta, gamma, y_hat, residuals\n    except Exception:\n        # Fallback: if nonlinear fit fails, return linear-like results to avoid crash\n        # This will make diagnostics conservative (likely not detect nonlinearity).\n        y_hat = np.full_like(y, np.mean(y))\n        residuals = y - y_hat\n        return np.mean(y), 0.0, 1.0, y_hat, residuals\n\ndef mse(residuals):\n    return float(np.mean(residuals**2))\n\ndef r2(y, y_hat):\n    y_mean = np.mean(y)\n    sse = np.sum((y - y_hat)**2)\n    sst = np.sum((y - y_mean)**2)\n    if sst == 0.0:\n        return 0.0\n    return float(1.0 - sse / sst)\n\ndef detect_nonlinearity(mse_lin, mse_nl, spearman_r, tau=1.05, t=0.25):\n    improvement_ratio = mse_lin / mse_nl if mse_nl > 0 else np.inf\n    return (improvement_ratio >= tau) and (abs(spearman_r) >= t)\n\ndef simulate_case(alpha, beta, gamma, sigma, x_max, n, rng):\n    x = np.linspace(0.0, x_max, n)\n    eps = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = alpha + beta * (1.0 - np.exp(-gamma * x)) + eps\n    return x, y\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, beta, gamma, sigma, x_max, n)\n        (0.5, 1.5, 0.02, 0.1, 5.0, 200),  # Case 1: near-linear\n        (0.5, 1.5, 1.0, 0.1, 5.0, 200),   # Case 2: strong saturation\n        (0.5, 0.0, 0.5, 0.1, 5.0, 200),   # Case 3: no relationship\n        (0.5, 1.5, 0.5, 1.5, 5.0, 200),   # Case 4: high noise\n        (0.5, 1.0, 0.5, 0.2, 3.0, 150),   # Case 5: moderate saturation\n    ]\n\n    # Fixed seed for reproducibility\n    rng = np.random.default_rng(12345)\n\n    results = []\n    for case in test_cases:\n        alpha, beta, gamma, sigma, x_max, n = case\n        x, y = simulate_case(alpha, beta, gamma, sigma, x_max, n, rng)\n\n        # Fit linear model\n        a_lin, b_lin, yhat_lin, resid_lin = fit_linear(x, y)\n\n        # Fit nonlinear saturating model\n        a_nl, b_nl, g_nl, yhat_nl, resid_nl = fit_nonlinear(x, y)\n\n        # Compute diagnostics\n        mse_lin = mse(resid_lin)\n        mse_nl = mse(resid_nl)\n        r2_lin = r2(y, yhat_lin)\n        r2_nl = r2(y, yhat_nl)\n        # Spearman rank correlation between linear residuals and x\n        spearman_r, _ = spearmanr(resid_lin, x)\n\n        # Decision rule\n        flag = detect_nonlinearity(mse_lin, mse_nl, spearman_r, tau=1.05, t=0.25)\n        results.append(flag)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "检查残差图是一种强大的诊断方法，但结论可能带有主观性。为了得出严格的统计论断，我们需要形式化的假设检验。本实践  将介绍拉姆齐重置检验（Ramsey RESET test），这是一种用于检测模型是否遗漏了非线性项的经典且广泛使用的工具。通过亲手实现这个检验并通过模拟来研究其统计功效（power），你将深刻理解如何运用统计推断来验证模型设定的正确性。",
            "id": "3115010",
            "problem": "要求您实现并研究拉姆齐回归设定误差检验（RESET），这是一种用于检测线性模型中遗漏的非线性项的经典诊断方法，并通过模拟在二次备择假设下校准其功效。您的程序必须是一个完整、可运行的实现，能够生成合成数据、应用该检验并报告经验拒绝率。\n\n考虑一个具有独立同分布观测值的单变量预测变量设定。对于每次模拟重复，按如下方式生成数据。对于给定的样本量 $n$，独立地抽取预测变量 $x_i \\sim \\mathcal{N}(0,1)$，其中 $i=1,\\dots,n$。给定参数 $\\beta_0$、$\\beta_1$、$\\gamma$ 和噪声尺度 $\\sigma0$，根据以下数据生成过程生成响应变量：\n$$\ny_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_i \\;+\\; \\gamma x_i^2 \\;+\\; \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)\\ \\text{independently}.\n$$\n\n将省略了非线性项的受限线性模型定义为：\n$$\ny_i \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_i \\;+\\; u_i,\n$$\n并通过普通最小二乘法（OLS）进行估计。令 $\\widehat{y}_i$ 表示此受限模型的拟合值。拉姆齐回归设定误差检验（RESET）通过使用拟合值的幂次来增广受限模型，从而构成一个非受限模型：\n$$\ny_i \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_i \\;+\\; \\sum_{j=2}^{p} \\delta_j \\,\\widehat{y}_i^{\\,j} \\;+\\; v_i,\n$$\n其中 $p \\ge 2$ 是所选的 RESET 阶数。原假设断言所有增广系数均为零，即 $H_0: \\delta_2=\\dots=\\delta_p=0$。您必须使用 Fisher–Snedecor 分布实现经典的嵌套模型决策规则：计算受限模型和非受限模型的普通最小二乘残差平方和，通过适当的比率对它们进行比较，该比率产生一个 F 统计量，其分子自由度等于增加的回归变量数量，分母自由度等于非受限模型的残差自由度。如果该统计量落在由 F 分布的上 $\\alpha$ 分位数决定的拒绝域中，则在显著性水平 $\\alpha$ 下拒绝 $H_0$。显著性水平 $\\alpha$ 必须以小数形式表示（例如，$0.05$）。\n\n您的程序必须执行蒙特卡洛模拟，以在二次备择假设下估计拒绝概率（经验功效）。具体方法是，对每个参数集重复以下步骤 $R$ 次：在指定参数下模拟一个数据集，运行阶数为 $p$、水平为 $\\alpha$ 的拉姆齐 RESET 检验，并记录是否拒绝 $H_0$。一个参数集的最终结果是这 $R$ 次重复中拒绝指示符的平均值。在 $\\gamma=0$ 的特殊情况下，该数量估计的是经验大小。\n\n您必须依赖的基础知识：\n- 普通最小二乘估计量最小化残差平方和，并求解线性回归的正规方程。\n- 对于具有高斯误差的嵌套线性模型，在原假设下，残差平方和的按比例缩减量服从 F 分布，其自由度由增加的参数数量（分子）和非受限模型的残差自由度（分母）给出。\n- 拉姆齐 RESET 检验通过使用受限模型拟合值的幂次来增广受限模型，以代理回归变量的被遗漏的非线性函数；如果存在此类非线性，这些增广项会改善拟合，导致拒绝频率高于名义水平。\n\n使用以下参数集测试套件实现模拟。每个测试用例是一个元组 $(n,\\beta_0,\\beta_1,\\gamma,\\sigma,\\alpha,R,p,\\text{seed})$：\n- 案例A（原假设下的经验大小）：$(n,\\beta_0,\\beta_1,\\gamma,\\sigma,\\alpha,R,p,\\text{seed}) = (\\,200,\\,0,\\,1,\\,0,\\,1,\\,0.05,\\,500,\\,3,\\,12345\\,)$。\n- 案例B（中等非线性，较一样本）：$(\\,200,\\,0,\\,1,\\,0.2,\\,1,\\,0.05,\\,500,\\,3,\\,54321\\,)$。\n- 案例C（强非线性，较小样本，较低阶数）：$(\\,50,\\,1,\\,1,\\,0.8,\\,1,\\,0.05,\\,500,\\,2,\\,2024\\,)$。\n- 案例D（无线性效应，高噪声，宽松水平）：$(\\,50,\\,0,\\,0,\\,0.5,\\,3,\\,0.1,\\,400,\\,3,\\,777\\,)$。\n\n对于每个案例，您的程序必须以十进制浮点数形式输出估计的拒绝概率。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如，“[0.052,0.412,0.873,0.268]”）。将每个报告的拒绝概率四舍五入到三位小数。不需要读取输入；所有参数都嵌入在程序中。使用指定的随机种子以确保跨次运行的可复现性。",
            "solution": "该问题要求实现一个蒙特卡洛模拟，以研究拉姆齐回归设定误差检验（RESET）的统计功效。该模拟将在各种参数配置下估计检验的拒绝概率，包括一个用于评估其经验大小的原假设案例，以及多个包含二次非线性的备择假设案例以评估其功效。\n\n### 理论框架\n\n分析始于一个定义好的数据生成过程（DGP），其中响应变量 $y_i$ 和预测变量 $x_i$ 之间的真实关系包含一个二次项。该设定由以下公式给出：\n$$\ny_i \\;=\\; \\beta_0 \\;+\\; \\beta_1 x_i \\;+\\; \\gamma x_i^2 \\;+\\; \\varepsilon_i\n$$\n其中 $x_i \\sim \\mathcal{N}(0,1)$ 且误差 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立同分布的。参数 $\\gamma$ 控制非线性的程度。如果 $\\gamma=0$，真实模型是线性的。如果 $\\gamma \\neq 0$，则由于遗漏了变量 $x_i^2$，一个简单的线性模型就是设定不当的。\n\n拉姆齐 RESET 是针对此类设定不当的通用检验。该过程包括以下步骤：\n\n1.  **估计受限模型**：首先，我们假设一个简单的线性模型，该模型可能是设定不当的。这是在正确设定的原假设下的受限模型：\n    $$\n    y_i \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_i \\;+\\; u_i\n    $$\n    该模型使用普通最小二乘法（OLS）进行估计，得到估计系数 $\\widehat{\\theta}_0$ 和 $\\widehat{\\theta}_1$，以及相应的拟合值：\n    $$\n    \\widehat{y}_i \\;=\\; \\widehat{\\theta}_0 \\;+\\; \\widehat{\\theta}_1 x_i\n    $$\n\n2.  **构建非受限（增广）模型**：RESET 检验的核心思想是，如果真实模型是非线性的，那么从设定不当的线性模型中得到的拟合值 $\\widehat{y}_i$ 本身将是预测变量的函数，因此可能捕捉到一些被遗漏的非线性效应。这些拟合值的幂次 $\\widehat{y}_i^j$ 可以作为未知非线性项的代理。通过用这些代理增广受限模型，可以构建出非受限模型：\n    $$\n    y_i \\;=\\; \\theta_0 \\;+\\; \\theta_1 x_i \\;+\\; \\sum_{j=2}^{p} \\delta_j \\,\\widehat{y}_i^{\\,j} \\;+\\; v_i\n    $$\n    其中 $p \\ge 2$ 是一个定义检验阶数的整数。\n\n### 嵌套模型的F检验\n\n模型设定的假设检验被构造成一个对新增项系数的检验。原假设 $H_0$ 表明受限模型设定正确，这意味着增广项的系数全部为零：\n$$\nH_0: \\delta_2 = \\delta_3 = \\dots = \\delta_p = 0\n$$\n备择假设 $H_1$ 是至少存在一个 $\\delta_j \\neq 0$，其中 $j \\in \\{2, \\dots, p\\}$。\n\n由于受限模型嵌套在非受限模型之内（通过将 $\\delta_j$ 系数设置为零得到），我们可以使用 F 检验来比较它们的拟合优度。令 $\\text{RSS}_R$ 为受限模型 OLS 估计的残差平方和，令 $\\text{RSS}_U$ 为非受限模型的残差平方和。F 统计量定义为：\n$$\nF = \\frac{(\\text{RSS}_R - \\text{RSS}_U) / q}{\\text{RSS}_U / (n - k_U)}\n$$\n该统计量的参数为：\n-   $n$: 观测数量。\n-   $q$: 约束的数量，即增加的回归变量的数量。在这里，$q = p-1$。\n-   $k_U$: 非受限模型中的参数总数。该模型包括一个截距项（1个）、原始预测变量 $x_i$（1个），以及从 2 到 $p$ 次的 $\\widehat{y}_i$ 的幂（$p-1$ 项）。因此，$k_U = 1 + 1 + (p-1) = p+1$。\n\n因此，F 检验的自由度为：分子自由度 $df_1 = q = p-1$，分母自由度 $df_2 = n - k_U = n - (p+1)$。该检验统计量为：\n$$\nF = \\frac{(\\text{RSS}_R - \\text{RSS}_U) / (p-1)}{\\text{RSS}_U / (n - (p+1))}\n$$\n在原假设 $H_0$ 下，该统计量服从自由度为 $p-1$ 和 $n-(p+1)$ 的 F 分布，即 $F \\sim F_{p-1, n-(p+1)}$。\n\n对于给定的显著性水平 $\\alpha$，决策规则是：如果计算出的 F 统计量超过临界值 $F^{(\\alpha)}_{p-1, n-(p+1)}$，则拒绝 $H_0$。该临界值是相应 F 分布的上 $\\alpha$ 分位数。\n\n### 蒙特卡洛模拟算法\n\n为了估计每个参数集 $(n, \\beta_0, \\beta_1, \\gamma, \\sigma, \\alpha, R, p, \\text{seed})$ 的拒绝概率，我们执行以下模拟：\n\n1.  **初始化**：对于一个给定的测试案例，为保证可复现性，设置随机数生成器种子。将拒绝计数器初始化为零。\n2.  **重复循环**：重复 $R$ 次：\n    a. **数据生成**：生成一个大小为 $n$ 的数据集。抽取 $x_i \\sim \\mathcal{N}(0,1)$ 和 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。计算 $y_i = \\beta_0 + \\beta_1 x_i + \\gamma x_i^2 + \\varepsilon_i$。\n    b. **模型估计**：\n       i.  定义大小为 $n \\times 2$ 的受限设计矩阵 $X_R$（一列 1 和 $x$ 向量）。使用 OLS 估计受限模型 $y = X_R\\beta_R + u$，以获得拟合值 $\\widehat{y} = X_R\\widehat{\\beta}_R$ 和残差平方和 $\\text{RSS}_R$。\n       ii. 构建幂次 $\\widehat{y}^2, \\dots, \\widehat{y}^p$。通过用这些新的回归变量增广 $X_R$，构建大小为 $n \\times (p+1)$ 的非受限设计矩阵 $X_U$。使用 OLS 估计非受限模型 $y = X_U\\beta_U + v$，以获得 $\\text{RSS}_U$。\n    c. **假设检验**：\n       i.  计算自由度：$df_1 = p-1$ 和 $df_2 = n-(p+1)$。\n       ii. 计算 F 统计量。由于有限精度算术，如果 $\\text{RSS}_R  \\text{RSS}_U$ 或 $\\text{RSS}_U$ 接近于零，则该统计量是病态的（ill-defined）；在这种情况下，我们不拒绝 $H_0$。否则，计算 $F = ((\\text{RSS}_R - \\text{RSS}_U)/df_1) / (\\text{RSS}_U/df_2)$。\n       iii. 使用 F 分布的逆累积分布函数（百分点函数）找到临界值 $F_{\\text{crit}} = F^{(\\alpha)}_{df_1, df_2}$。\n    d. **决策**：如果 $F  F_{\\text{crit}}$，则将拒绝计数器加一。\n3.  **结果计算**：经过 $R$ 次重复后，估计的拒绝概率是总拒绝次数除以 $R$。该值四舍五入到三位小数。\n\n对所有指定的测试案例重复此过程，以评估拉姆齐 RESET 检验的经验大小（当 $\\gamma=0$ 时）和功效（当 $\\gamma \\neq 0$ 时）。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import f\n\ndef solve():\n    \"\"\"\n    Implements and studies the Ramsey RESET diagnostic test via Monte Carlo simulation.\n\n    This function iterates through a suite of test cases, each defining a specific\n    data generating process and test parameters. For each case, it simulates data,\n    applies the Ramsey RESET, and calculates the empirical rejection rate over\n    many replications.\n    \"\"\"\n    test_cases = [\n        # (n, beta0, beta1, gamma, sigma, alpha, R, p, seed)\n        (200, 0, 1, 0, 1, 0.05, 500, 3, 12345),      # Case A: Empirical size\n        (200, 0, 1, 0.2, 1, 0.05, 500, 3, 54321),     # Case B: Moderate nonlinearity\n        (50, 1, 1, 0.8, 1, 0.05, 500, 2, 2024),       # Case C: Strong nonlinearity\n        (50, 0, 0, 0.5, 3, 0.1, 400, 3, 777)          # Case D: No linear effect, noisy\n    ]\n\n    results = []\n    for n, beta0, beta1, gamma, sigma, alpha, R, p, seed in test_cases:\n        # Set seed for reproducibility for each test case\n        rng = np.random.default_rng(seed)\n        rejection_count = 0\n\n        for _ in range(R):\n            # Step 1: Generate synthetic data according to the DGP\n            x = rng.normal(loc=0, scale=1, size=n)\n            epsilon = rng.normal(loc=0, scale=sigma, size=n)\n            y = beta0 + beta1 * x + gamma * x**2 + epsilon\n\n            # Step 2: Estimate the restricted linear model (y ~ 1 + x)\n            X_r = np.c_[np.ones(n), x]\n            \n            # Use np.linalg.lstsq to perform OLS regression\n            # It returns coefficients, sum of squared residuals, rank, singular values\n            beta_r_hat, residuals_r, _, _ = np.linalg.lstsq(X_r, y, rcond=None)\n            \n            # If the model is not full rank or sample size is too small, lstsq returns empty residuals\n            if residuals_r.size == 0:\n                continue # Skip this replication if OLS fails\n\n            rss_r = residuals_r[0]\n            y_hat = X_r @ beta_r_hat\n\n            # Step 3: Estimate the unrestricted model (y ~ 1 + x + y_hat^2 + ... + y_hat^p)\n            # Create the augmented regressors\n            y_hat_powers = np.array([y_hat**j for j in range(2, p + 1)]).T\n            X_u = np.c_[X_r, y_hat_powers]\n\n            beta_u_hat, residuals_u, _, _ = np.linalg.lstsq(X_u, y, rcond=None)\n\n            if residuals_u.size == 0:\n                continue # Skip if OLS fails\n\n            rss_u = residuals_u[0]\n\n            # Step 4: Compute the F-statistic\n            k_r = X_r.shape[1]  # Number of parameters in restricted model (2)\n            k_u = X_u.shape[1]  # Number of parameters in unrestricted model (p+1)\n            \n            df1 = k_u - k_r   # Numerator degrees of freedom (p-1)\n            df2 = n - k_u     # Denominator degrees of freedom (n-(p+1))\n            \n            f_statistic = 0.0\n            # Ensure valid degrees of freedom and that RSS_R > RSS_U\n            # RSS_R should be >= RSS_U. A small negative difference can occur due to floating point error.\n            if df2 > 0 and rss_u > 1e-9 and (rss_r - rss_u) > 1e-9:\n                f_statistic = ((rss_r - rss_u) / df1) / (rss_u / df2)\n\n            # Step 5: Perform the hypothesis test\n            # Get the critical value from the F-distribution\n            critical_value = f.ppf(1 - alpha, df1, df2) if df1 > 0 and df2 > 0 else np.inf\n\n            if f_statistic > critical_value:\n                rejection_count += 1\n        \n        # Calculate the empirical rejection probability (power or size)\n        rejection_prob = rejection_count / R\n        results.append(round(rejection_prob, 3))\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}