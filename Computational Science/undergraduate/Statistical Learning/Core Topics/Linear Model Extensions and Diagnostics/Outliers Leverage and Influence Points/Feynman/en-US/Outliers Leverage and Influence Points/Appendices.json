{
    "hands_on_practices": [
        {
            "introduction": "Leverage is a crucial concept for identifying observations that are unusual in their predictor values and thus have the potential to strongly influence a regression model. A common misconception is that leverage is an inherent property of a data point alone; in reality, it is fundamentally model-dependent, as defined by the hat matrix $H = X(X^T X)^{-1}X^T$. By constructing a dataset with a specific interaction pattern, you will see firsthand how an observation that appears unremarkable in a simple main-effects model can be revealed as a high-leverage point once the correct interaction term is included in the model . This exercise will solidify your understanding of how leverage works and reinforce the principle that our diagnostic tools must always match the model we are fitting.",
            "id": "3154847",
            "problem": "Consider a linear regression setting in which the response is modeled as a linear combination of features, possibly including interaction terms. The Ordinary Least Squares (OLS) estimator minimizes the sum of squared residuals and leads to a fitted value operator that is linear in the response. Your task is to implement a complete program that builds datasets with and without a deliberately constructed extreme interaction pattern and then detects high-leverage points via the augmented design matrix that includes the interaction term.\n\nBase the derivation and algorithm on the following fundamental definitions and facts:\n- In a linear model with design matrix $X \\in \\mathbb{R}^{n \\times p}$, the OLS fitted values are given by applying a linear operator to the response vector. The fitted value operator can be written in terms of a matrix (the projection or \"hat\" matrix) that maps any response vector to its predicted values in the column space of $X$.\n- The diagonal entries of the hat matrix quantify leverage. The leverage of observation $i$ is the $i$-th diagonal element of the hat matrix and measures how far the predictor vector of that observation is from the bulk of the predictor cloud in the geometry induced by $X$.\n- The trace of the hat matrix equals the number of columns $p$ in the design matrix, which implies that the average leverage is $p/n$.\n\nYou must not assume any shortcut formulas beyond these core facts. You should derive any working expressions and algorithms you need from these principles.\n\nProgram requirements:\n1. Construct three synthetic datasets, each with a response generated from a model with an interaction term. For each dataset, fit two models: a base model without the interaction term and an augmented model with the interaction term. For both models, compute the leverage values for all observations and flag high-leverage points using the criterion $h_{ii} > 2 \\cdot \\bar{h}$, where $\\bar{h}$ is the average leverage under the model in question.\n2. The response should be generated from the model\n   $$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 x_2) + \\varepsilon, $$\n   with parameters fixed as $ \\beta_0 = 0.5$, $ \\beta_1 = 1.0$, $ \\beta_2 = -0.7$, $ \\beta_3 = 0.8$, and independent noise $ \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ with $ \\sigma = 0.3$. All random generation in each dataset must be deterministic via fixed seeds so that results are reproducible.\n\n3. Datasets and test suite. Build the following three datasets exactly as specified:\n   - Test case $1$ (happy path, extreme interaction with moderate marginals relative to a low-variance bulk):\n     - Sample size $n = 60$.\n     - Random seed $s = 12345$.\n     - Generate $x_1$ and $x_2$ independently with $x_j \\sim \\mathcal{N}(0, 0.6^2)$ for $j \\in \\{1,2\\}$.\n     - Force a single observation at index $i^\\star = 12$ to have $x_1 = 1.8$ and $x_2 = 2.2$. This yields the interaction $x_1 x_2 \\approx 3.96$, which is extreme relative to the typical $x_1 x_2$ scale given the low-variance bulk.\n   - Test case $2$ (boundary condition, outlying marginal without extreme interaction):\n     - Sample size $n = 60$.\n     - Random seed $s = 24680$.\n     - Generate $x_1$ and $x_2$ independently with $x_j \\sim \\mathcal{N}(0, 0.6^2)$.\n     - Force a single observation at index $i^\\star = 7$ to have $x_1 = 4.0$ and $x_2 = 0.2$. This yields the interaction $x_1 x_2 = 0.8$, which is not extreme relative to the bulk, but $x_1$ is a strong marginal outlier.\n   - Test case $3$ (edge case, near collinearity amplifies interaction leverage):\n     - Sample size $n = 25$.\n     - Random seed $s = 31415$.\n     - Generate $x_1 \\sim \\mathcal{N}(0, 0.8^2)$ and set $x_2 = x_1 + \\delta$ with $\\delta \\sim \\mathcal{N}(0, 0.1^2)$ independently, creating near-collinearity.\n     - Force a single observation at index $i^\\star = 5$ to have $x_1 = 2.0$ and $x_2 = 2.0$, causing a large interaction $x_1 x_2 = 4.0$ in a low-$n$, near-collinear context.\n\n4. For each dataset:\n   - Construct the base design matrix with columns $[1, x_1, x_2]$ (intercept plus main effects).\n   - Construct the augmented design matrix with columns $[1, x_1, x_2, x_1 x_2]$ (intercept plus main effects plus interaction).\n   - For each design matrix, compute the hat matrix and its diagonal to obtain leverage values. Flag indices where $h_{ii} > 2 \\cdot (p/n)$, with $p$ equal to the number of columns in the design matrix used.\n   - Return, for each dataset, two lists: the sorted indices flagged by the base model and the sorted indices flagged by the augmented model.\n\n5. Final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of two lists: the first list is the base-model flagged indices, and the second list is the augmented-model flagged indices.\n   - Concretely, the output should look like\n     $$ [ [ [i_{1,1}, i_{1,2}, \\dots], [j_{1,1}, j_{1,2}, \\dots] ], [ [i_{2,1}, \\dots], [j_{2,1}, \\dots] ], [ [i_{3,1}, \\dots], [j_{3,1}, \\dots] ] ] $$\n     where indices are integers in ascending order.\n\nNo physical units are involved, no angle units are involved, and no percentages are required anywhere in this task.",
            "solution": "We begin from first principles of linear regression. In a linear model with design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$, the Ordinary Least Squares (OLS) estimator seeks parameters $\\hat{\\beta} \\in \\mathbb{R}^{p}$ that minimize the sum of squared residuals\n$$ S(\\beta) = \\| y - X \\beta \\|_2^2. $$\nSetting the gradient of $S(\\beta)$ with respect to $\\beta$ to zero gives the normal equations\n$$ X^\\top X \\hat{\\beta} = X^\\top y. $$\nWhen $X^\\top X$ is invertible, the unique solution is\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y. $$\nIf $X^\\top X$ is not invertible, the Mooreâ€“Penrose pseudoinverse can be used to obtain the minimum-norm solution\n$$ \\hat{\\beta} = (X^\\top X)^{+} X^\\top y, $$\nwhere $(\\cdot)^{+}$ denotes the pseudoinverse.\n\nThe OLS fitted values are\n$$ \\hat{y} = X \\hat{\\beta} = X (X^\\top X)^{+} X^\\top y = H y, $$\nwhere\n$$ H = X (X^\\top X)^{+} X^\\top $$\nis the projection matrix (the \"hat\" matrix). The matrix $H$ is symmetric and idempotent, with $H^\\top = H$ and $H^2 = H$. An immediate and fundamental property is\n$$ \\mathrm{trace}(H) = p, $$\nthe number of columns in $X$. Consequently, the average leverage is\n$$ \\bar{h} = \\frac{1}{n} \\sum_{i=1}^{n} h_{ii} = \\frac{p}{n}, $$\nwhere $h_{ii}$ denotes the $i$-th diagonal element of $H$, i.e., the leverage of observation $i$.\n\nLeverage quantifies how far an observation's predictor vector is from the center of the predictor space in the metric induced by $X$. In particular, adding columns to $X$ (e.g., an interaction feature) expands the feature space, creating directions along which observations can become far from the bulk, thereby potentially increasing their leverage. An observation can exhibit moderate main effects $x_1$ and $x_2$ individually but have an extreme interaction $x_1 x_2$; in the augmented space, this observation may be far from the main cloud along the interaction dimension, which increases $h_{ii}$ even if the main effects alone would not produce high leverage.\n\nAlgorithmic steps derived from these principles:\n1. For each dataset, construct two design matrices:\n   - Base design matrix $X_{\\mathrm{base}}$ with columns $[1, x_1, x_2]$ (intercept and main effects).\n   - Augmented design matrix $X_{\\mathrm{aug}}$ with columns $[1, x_1, x_2, x_1 x_2]$ (intercept, main effects, and interaction).\n2. For each $X$, compute the hat matrix $H = X (X^\\top X)^{+} X^\\top$ using the pseudoinverse of $X^\\top X$ to ensure numerical stability.\n3. Extract leverage values $h_{ii}$ as the diagonal of $H$.\n4. Using the fundamental property $\\bar{h} = p/n$, flag indices satisfying $h_{ii} > 2 \\cdot \\bar{h} = 2p/n$. The multiplicative constant $2$ sets a conservative criterion that highlights observations whose leverage substantially exceeds the average, and it is expressed entirely in terms of $p$ and $n$ derived from $X$.\n\nConstruction of datasets:\n- Test case $1$ (happy path):\n  - Generate $x_1, x_2 \\sim \\mathcal{N}(0, 0.6^2)$ independently for $n = 60$ using seed $s = 12345$. Force index $i^\\star = 12$ to have $x_1 = 1.8$, $x_2 = 2.2$, yielding interaction $x_1 x_2 \\approx 3.96$, large relative to the bulk scale of $x_1 x_2$ given the low variance.\n  - Generate $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\varepsilon$, with $\\beta_0 = 0.5$, $\\beta_1 = 1.0$, $\\beta_2 = -0.7$, $\\beta_3 = 0.8$, and $\\varepsilon \\sim \\mathcal{N}(0, 0.3^2)$ (independent).\n- Test case $2$ (boundary condition, marginal outlier, non-extreme interaction):\n  - Generate $x_1, x_2 \\sim \\mathcal{N}(0, 0.6^2)$ independently for $n = 60$ using seed $s = 24680$. Force index $i^\\star = 7$ to have $x_1 = 4.0$, $x_2 = 0.2$, yielding interaction $x_1 x_2 = 0.8$, which is not extreme under the bulk scale, while $x_1$ is a large marginal outlier.\n  - Generate $y$ as above.\n- Test case $3$ (edge case, near collinearity):\n  - Generate $x_1 \\sim \\mathcal{N}(0, 0.8^2)$ and $\\delta \\sim \\mathcal{N}(0, 0.1^2)$ independently for $n = 25$ using seed $s = 31415$. Set $x_2 = x_1 + \\delta$, producing near collinearity. Force index $i^\\star = 5$ to have $x_1 = 2.0$, $x_2 = 2.0$, so the interaction is $x_1 x_2 = 4.0$, which is notable in a low-$n$, near-collinear context.\n  - Generate $y$ as above.\n\nWhy the detection works:\n- For the base design matrix $X_{\\mathrm{base}}$ with $p = 3$, the average leverage is $\\bar{h} = 3/n$. A marginal outlier (e.g., $x_1$ large) tends to increase its leverage under the base model. However, an observation with moderate marginal values but a large interaction does not appear extreme in the base model, because the interaction is not represented there; hence its leverage may remain near the average.\n- For the augmented design matrix $X_{\\mathrm{aug}}$ with $p = 4$, the average leverage is $\\bar{h} = 4/n$. The interaction feature $x_1 x_2$ provides a new direction. An observation with a large $x_1 x_2$ relative to the typical scale will be far from the bulk along this direction, increasing its leverage $h_{ii}$ in $X_{\\mathrm{aug}}$. Thus, such an observation can be flagged as high leverage by the augmented model even if its marginal components are only moderate.\n\nImplementation and output:\n- For each dataset, compute the flagged indices for both the base and augmented models as described. Sort the flagged indices in ascending order.\n- The program must print a single line containing a list of results for the three test cases. Each result is a pair of lists: the first for the base model and the second for the augmented model. The format is\n  $$ [ [ \\text{base}_1, \\text{aug}_1 ], [ \\text{base}_2, \\text{aug}_2 ], [ \\text{base}_3, \\text{aug}_3 ] ], $$\n  where each $\\text{base}_k$ and $\\text{aug}_k$ is a list of integers representing indices of flagged observations for test case $k$.\n\nThis procedure is grounded entirely in the derivation of the OLS normal equations, the projection (hat) matrix $H$, and its fundamental properties, without invoking shortcut formulas beyond these core definitions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hat_leverages(X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute leverage values (diagonal of hat matrix) for design matrix X.\n    Uses the pseudoinverse of X^T X for numerical stability.\n    \"\"\"\n    XTX = X.T @ X\n    XTX_pinv = np.linalg.pinv(XTX)\n    H = X @ XTX_pinv @ X.T\n    # Numerical safety: clip tiny negative due to rounding to zero\n    h = np.clip(np.diag(H), 0.0, 1.0)\n    return h\n\ndef flag_high_leverage(h: np.ndarray, p: int) -> np.ndarray:\n    \"\"\"\n    Flag indices where leverage exceeds 2 * (p/n).\n    Returns sorted indices as a numpy array of ints.\n    \"\"\"\n    n = h.shape[0]\n    avg_h = p / n\n    threshold = 2.0 * avg_h\n    idx = np.where(h > threshold)[0]\n    return np.sort(idx)\n\ndef build_case_1():\n    # Test case 1: extreme interaction with moderate marginals relative to low-variance bulk\n    n = 60\n    rng = np.random.default_rng(12345)\n    x1 = rng.normal(0.0, 0.6, size=n)\n    x2 = rng.normal(0.0, 0.6, size=n)\n    # Force special observation\n    i_star = 12\n    x1[i_star] = 1.8\n    x2[i_star] = 2.2\n\n    # True parameters\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef build_case_2():\n    # Test case 2: marginal outlier without extreme interaction\n    n = 60\n    rng = np.random.default_rng(24680)\n    x1 = rng.normal(0.0, 0.6, size=n)\n    x2 = rng.normal(0.0, 0.6, size=n)\n    i_star = 7\n    x1[i_star] = 4.0\n    x2[i_star] = 0.2\n\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef build_case_3():\n    # Test case 3: near collinearity amplifies interaction leverage\n    n = 25\n    rng = np.random.default_rng(31415)\n    x1 = rng.normal(0.0, 0.8, size=n)\n    delta = rng.normal(0.0, 0.1, size=n)\n    x2 = x1 + delta\n    i_star = 5\n    x1[i_star] = 2.0\n    x2[i_star] = 2.0\n\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef solve():\n    # Define the test cases from the problem statement.\n    cases = [build_case_1, build_case_2, build_case_3]\n\n    results = []\n    for build in cases:\n        X_base, X_aug, y = build()\n\n        # Compute leverages\n        h_base = hat_leverages(X_base)\n        h_aug = hat_leverages(X_aug)\n\n        # Flag high leverage indices using threshold 2 * (p/n)\n        base_flags = flag_high_leverage(h_base, p=X_base.shape[1]).tolist()\n        aug_flags = flag_high_leverage(h_aug, p=X_aug.shape[1]).tolist()\n\n        results.append([base_flags, aug_flags])\n\n    # Final print statement in the exact required format.\n    print(str(results))\n\nsolve()\n```"
        },
        {
            "introduction": "While leverage is a powerful diagnostic, its standard Ordinary Least Squares (OLS) definition can sometimes be misleading, especially with strong collinearity among predictors. This exercise explores a scenario where OLS leverage, derived from the hat matrix $H$, gives prominence to points that slightly deviate from a collinear trend while overlooking a more extreme point that conforms to it. You will then apply ridge regression and observe how its regularized hat matrix, $H_{\\lambda} = X(X^T X + \\lambda I)^{-1}X^T$, re-weights the leverage scores to provide a more intuitive measure of a point's remoteness . This practice reveals the geometric subtleties of leverage and highlights how regularization can stabilize not just parameter estimates, but also the associated diagnostics.",
            "id": "3154882",
            "problem": "You are asked to construct and analyze a synthetic linear regression setting that exposes the behavior of leverage-based selection in the presence of strong collinearity, and to assess how ridge regularization changes this behavior. Work purely in mathematical and algorithmic terms in the space of statistical learning. The task centers on the relationship between outliers, leverage, and influence points.\n\nStart from the following fundamental base:\n- Ordinary Least Squares (OLS) is defined by the minimization of the sum of squared residuals for the linear model $y = X \\beta + \\varepsilon$, where $X \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^n$, and $\\varepsilon \\in \\mathbb{R}^n$. The OLS prediction operator is the orthogonal projection onto the column space of $X$, given by the hat matrix $H = X (X^{\\top} X)^{-1} X^{\\top}$. The OLS leverage of observation $i$ is $h_{ii} = H_{ii}$.\n- Ridge regression introduces a penalty $\\lambda \\lVert \\beta \\rVert_2^2$ to stabilize the inverse in the presence of collinearity. The ridge hat matrix for regularization parameter $\\lambda > 0$ is $H_{\\lambda} = X \\left( X^{\\top} X + \\lambda I_p \\right)^{-1} X^{\\top}$, where $I_p$ is the $p \\times p$ identity matrix. The ridge leverage of observation $i$ is $h^{(\\lambda)}_{ii} = (H_{\\lambda})_{ii}$.\n- Influence may be assessed through quantities that combine residuals and leverage (e.g., Cook's distance), but for the selection mechanism in this problem, use leverage-only subsampling.\n\nConstruct a toy dataset $X \\in \\mathbb{R}^{n \\times p}$ with $p=2$ predictors under strong collinearity and a response $y \\in \\mathbb{R}^n$:\n1. Generate $n_{\\text{normal}} = 50$ \"normal\" points with predictor $x_1$ drawn independently from a normal distribution with mean $0$ and standard deviation $5$, and define $x_2 = x_1 + \\varepsilon$ with $\\varepsilon$ independently drawn from a normal distribution with mean $0$ and standard deviation $0.001$. This creates near-perfect collinearity along the line $x_2 \\approx x_1$.\n2. Generate $n_{\\text{jitter}} = 6$ \"jitter\" points where $x_1$ is drawn independently from the same normal distribution as the normal points, but define $x_2 = x_1 + \\delta$ with a fixed offset $\\delta = 0.005$ to induce non-negligible deviations orthogonal to the collinear direction.\n3. Add a single \"bad leverage\" candidate point with $x_1^{\\star} = 12$ and $x_2^{\\star} = 12$; this point lies far along the principal collinear direction but has no orthogonal deviation.\n\nForm $X$ by stacking the rows from steps $1$â€“$3$ in the stated order. Center each predictor column to have mean $0$ before computing any leverage quantities. Define the response by $y = \\beta_1 x_1 + \\beta_2 x_2 + \\eta$ with $\\beta_1 = 1$, $\\beta_2 = 1$, and $\\eta$ drawn independently from a normal distribution with mean $0$ and standard deviation $0.1$ for all points except the last point $(x_1^{\\star}, x_2^{\\star})$, where set $y^{\\star} = \\beta_1 x_1^{\\star} + \\beta_2 x_2^{\\star} + 100$ to create a large residual at the candidate \"bad leverage\" point.\n\nImplement leverage-based subsampling as follows:\n- Given a choice of regularization parameter $\\lambda \\ge 0$, compute leverage scores $h^{(\\lambda)}_{ii}$ from $H_{\\lambda}$, with the convention that $\\lambda = 0$ corresponds to OLS. Then select the $m$ rows with the largest leverage scores as the subsample. Declare that the subsample \"includes the bad point\" if the final row of $X$ (the constructed candidate point) is among the selected indices.\n\nYour program must construct the dataset exactly as described and then evaluate the following test suite of parameter pairs $(\\lambda, m)$:\n- Test case $1$: $(\\lambda, m) = (0, 3)$.\n- Test case $2$: $(\\lambda, m) = (0.1, 3)$.\n- Test case $3$: $(\\lambda, m) = (1.0, 2)$.\n- Test case $4$: $(\\lambda, m) = (5.0, 1)$.\n\nFor each test case, output a boolean indicating whether the subsample includes the bad point. Aggregate all four booleans in order into a single line as a comma-separated list enclosed in square brackets, for example, $[b_1,b_2,b_3,b_4]$, where each $b_k$ is either $\\text{True}$ or $\\text{False}$.\n\nNo physical units, angle units, or percentages are involved. All calculations must be done with real numbers. The dataset generation must be deterministic by fixing the random seed to a constant value internally so that the outputs are reproducible across runs. The final output format must be a single line exactly matching the described list representation of the four booleans.",
            "solution": "The user's request is a valid and well-posed problem in statistical learning. It requires the construction of a specific synthetic dataset to explore the interplay between collinearity, leverage, and ridge regularization. The solution involves generating this dataset according to the provided rules, calculating leverage scores for both Ordinary Least Squares (OLS) and Ridge Regression, and then performing a selection task based on these scores for a given set of parameters.\n\n### Principle and Methodology\n\nThe core of this problem lies in understanding the geometric interpretation of leverage and how it is affected by regularization.\n\n**1. Dataset Construction and Centering**\n\nFirst, we construct the design matrix $X \\in \\mathbb{R}^{n \\times p}$ with $n=57$ and $p=2$, and the response vector $y \\in \\mathbb{R}^n$. The matrix $X$ is composed of three distinct groups of points:\n-   $n_{\\text{normal}} = 50$ points exhibiting strong collinearity, where $x_2 \\approx x_1$.\n-   $n_{\\text{jitter}} = 6$ points that deviate slightly from the main collinear trend, creating orthogonal outliers relative to the primary data structure.\n-   $1$ \"bad leverage\" candidate point $(x_1^{\\star}, x_2^{\\star}) = (12, 12)$, which is far from the data's center but lies perfectly along the principal direction of collinearity ($x_1 = x_2$).\n\nAfter stacking these points into a single matrix, a critical preprocessing step is to center the data. Let the uncentered data matrix be $X_{\\text{uncentered}}$. The centered matrix $X$ is computed as:\n$$\nX_{ij} = (X_{\\text{uncentered}})_{ij} - \\bar{x}_j\n$$\nwhere $\\bar{x}_j$ is the mean of the $j$-th column of $X_{\\text{uncentered}}$. This centering is essential as leverage is defined with respect to the center of the data cloud. All subsequent leverage calculations will use this centered matrix $X$. The response vector $y$ is generated based on the uncentered predictors, as specified, but it does not influence the leverage calculations.\n\n**2. OLS Leverage ($\\lambda=0$)**\n\nThe leverage of the $i$-th observation in an OLS model is given by $h_{ii}$, the $i$-th diagonal element of the hat matrix $H$:\n$$\nh_{ii} = [H]_{ii} = [X (X^{\\top} X)^{-1} X^{\\top}]_{ii} = x_i^{\\top} (X^{\\top} X)^{-1} x_i\n$$\nHere, $x_i^{\\top}$ is the $i$-th row of the (centered) design matrix $X$. Geometrically, $h_{ii}$ measures the remoteness of the observation $x_i$ from the center of the data, scaled by the data's covariance structure. In the presence of strong collinearity as constructed, the matrix $X^{\\top}X$ is nearly singular, and its inverse gives large weight to directions orthogonal to the main axis of data variation. The \"jitter\" points are constructed to be outliers in this particular orthogonal direction. Consequently, for OLS ($\\lambda=0$), these jitter points are expected to have very high leverage, potentially higher than the \"bad leverage\" point which, despite its large magnitude, conforms to the detected collinear structure.\n\n**3. Ridge Leverage ($\\lambda > 0$)**\n\nRidge regression modifies the hat matrix to $H_{\\lambda}$:\n$$\nH_{\\lambda} = X \\left( X^{\\top} X + \\lambda I_p \\right)^{-1} X^{\\top}\n$$\nThe ridge leverage of observation $i$ is the corresponding diagonal element, $h_{ii}^{(\\lambda)} = [H_{\\lambda}]_{ii}$. The introduction of the regularization term $\\lambda I_p$ has a profound effect. It adds a constant to the diagonal of $X^{\\top}X$, making the matrix invertible and better conditioned. Geometrically, this is equivalent to adding spherical information to the data's covariance structure. As $\\lambda$ increases, the term $\\lambda I_p$ begins to dominate $X^{\\top}X$. In the limit of large $\\lambda$, we have:\n$$\n(X^{\\top}X + \\lambda I_p)^{-1} \\approx (\\lambda I_p)^{-1} = \\frac{1}{\\lambda} I_p\n$$\nTherefore, the ridge leverage $h_{ii}^{(\\lambda)}$ approximates:\n$$\nh_{ii}^{(\\lambda)} \\approx x_i^{\\top} \\left( \\frac{1}{\\lambda} I_p \\right) x_i = \\frac{1}{\\lambda} x_i^{\\top} x_i = \\frac{1}{\\lambda} \\|x_i\\|^2_2\n$$\nThis shows that for large $\\lambda$, ridge leverage becomes proportional to the squared Euclidean norm of the centered observation vector $x_i$. The measure of \"remoteness\" transitions from being defined by the data's specific covariance structure to being defined by simple distance from the center. The \"bad leverage\" point $(x_1^{\\star}, x_2^{\\star}) = (12, 12)$ is intentionally placed far from the origin, ensuring it has a very large Euclidean norm after centering.\n\n**4. Subsampling and Evaluation**\n\nThe algorithm executes the following steps for each test case $(\\lambda, m)$:\n1.  Compute the leverages $h_{ii}^{(\\lambda)}$ for all $n=57$ data points using the centered matrix $X$ and the given $\\lambda$.\n2.  Identify the indices of the $m$ points with the largest leverage scores.\n3.  Check if the index of the \"bad leverage\" point (which is the last point, index $56$) is among these top $m$ indices.\n\nBased on the principles above:\n-   For $\\lambda=0$, the high structural leverage of the \"jitter\" points should place them at the top, likely excluding the \"bad\" point from a small subsample.\n-   As $\\lambda$ increases, the leverage of the \"bad\" point will increase relative to the \"jitter\" points because its large norm becomes more influential. Eventually, it will enter and then dominate the top-ranking leverage scores.\n\nThe provided test cases are designed to probe this transition.\n-   Test 1 $(\\lambda=0, m=3)$: OLS leverage, where jitter points are expected to be favored.\n-   Test 2 $(\\lambda=0.1, m=3)$: Small regularization, a possible transition point.\n-   Test 3 $(\\lambda=1.0, m=2)$: Moderate regularization, where Euclidean norm becomes more important.\n-   Test 4 $(\\lambda=5.0, m=1)$: Strong regularization, where the point with the largest norm is expected to have the highest leverage.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs a synthetic dataset to analyze leverage-based selection\n    under collinearity and ridge regularization, then runs a suite of tests.\n    \"\"\"\n    # Fix the random seed for reproducibility as required.\n    np.random.seed(0)\n\n    # 1. Construct the dataset as per the problem description.\n    # Parameters\n    p = 2\n    n_normal = 50\n    n_jitter = 6\n    n_bad = 1\n    n = n_normal + n_jitter + n_bad\n    \n    # Generate \"normal\" points with near-perfect collinearity\n    x1_normal = np.random.normal(loc=0, scale=5, size=n_normal)\n    eps_normal = np.random.normal(loc=0, scale=0.001, size=n_normal)\n    x2_normal = x1_normal + eps_normal\n    X_normal = np.column_stack((x1_normal, x2_normal))\n\n    # Generate \"jitter\" points with orthogonal deviation\n    x1_jitter = np.random.normal(loc=0, scale=5, size=n_jitter)\n    delta_jitter = 0.005\n    x2_jitter = x1_jitter + delta_jitter\n    X_jitter = np.column_stack((x1_jitter, x2_jitter))\n\n    # Define the \"bad leverage\" candidate point\n    X_bad = np.array([[12.0, 12.0]])\n\n    # Stack the components to form the uncentered data matrix\n    X_uncentered = np.vstack((X_normal, X_jitter, X_bad))\n\n    # Center the predictor columns (essential for leverage calculations)\n    X_mean = X_uncentered.mean(axis=0)\n    X = X_uncentered - X_mean\n\n    # NB: The response vector y is not used for leverage calculation but is\n    # constructed for completeness as specified in the problem.\n    beta = np.array([1.0, 1.0])\n    eta = np.random.normal(loc=0, scale=0.1, size=n)\n    y = X_uncentered @ beta + eta\n    # Adjust the last point to have a large residual\n    y[-1] = X_uncentered[-1] @ beta + 100.0\n\n    # 2. Define a function to compute leverage scores\n    def get_leverages(X_centered, lambda_val):\n        \"\"\"\n        Computes leverage scores for OLS (lambda=0) or Ridge (lambda>0).\n        Leverage is the diagonal of the hat matrix.\n        \"\"\"\n        _n, _p = X_centered.shape\n        XTX = X_centered.T @ X_centered\n        \n        if lambda_val == 0:\n            # OLS case: H = X (X^T X)^-1 X^T\n            # np.linalg.inv is fine for p=2\n            hat_matrix = X_centered @ np.linalg.inv(XTX) @ X_centered.T\n        else:\n            # Ridge case: H_lambda = X (X^T X + lambda*I)^-1 X^T\n            identity = np.identity(_p)\n            hat_matrix = X_centered @ np.linalg.inv(XTX + lambda_val * identity) @ X_centered.T\n            \n        # The leverage scores are the diagonal elements of the hat matrix\n        return np.diag(hat_matrix)\n\n    # 3. Evaluate the test suite\n    test_cases = [\n        (0, 3),    # Case 1: lambda=0, m=3\n        (0.1, 3),  # Case 2: lambda=0.1, m=3\n        (1.0, 2),  # Case 3: lambda=1.0, m=2\n        (5.0, 1),  # Case 4: lambda=5.0, m=1\n    ]\n\n    results = []\n    bad_point_index = n - 1 # The last point is the \"bad leverage\" one\n\n    for lam, m in test_cases:\n        # Compute leverage scores for the current lambda\n        leverages = get_leverages(X, lam)\n        \n        # Get the indices of the top m leverage scores\n        # np.argsort returns indices from smallest to largest\n        top_m_indices = np.argsort(leverages)[-m:]\n        \n        # Check if the bad point's index is in the top m set\n        is_bad_point_included = bad_point_index in top_m_indices\n        results.append(is_bad_point_included)\n\n    # 4. Format and print the final output\n    # Convert booleans to strings (\"True\" or \"False\") and join\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Ultimately, we study leverage because high-leverage points are candidates for being highly influential, especially when they also have large residuals. An influential point is one whose removal would cause a significant change in the model. This hands-on practice demonstrates the dramatic consequences of such points: they can corrupt the model selection process itself. By adding an influential point to an otherwise simple linear relationship, you will investigate how model selection criteria like the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) can be misled into choosing an overly complex model . This exercise provides a powerful lesson in the importance of diagnostic checks before trusting the output of automated modeling procedures.",
            "id": "3154883",
            "problem": "You are given a synthetic regression scenario to assess how the presence of a single influential observation affects model selection using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). Work within the Gaussian linear modeling framework: the response is generated by a linear signal plus noise, and candidate models are polynomials of degrees in $\\{1,2,3\\}$. Your program must implement the following, using only derivations from foundational definitions.\n\nStart from the Gaussian linear model with Ordinary Least Squares (OLS): the data consist of $n$ pairs $(x_i,y_i)$ with $x_i \\in \\mathbb{R}$, where the model is $y = X\\beta + \\varepsilon$ for a design matrix $X$, parameter vector $\\beta$, and noise vector $\\varepsilon$. Assume $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix and $\\sigma^2$ is an unknown variance. Use Maximum Likelihood Estimation (MLE) for the Gaussian model to derive the maximized log-likelihood in terms of the residual sum of squares and the sample size, and then use the definition of Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to obtain implementable expressions in terms of the fitted residuals, the number of parameters, and the sample size. Do not assume any pre-provided shortcut formulas.\n\nDefine candidate models as polynomial regressions of degrees $d \\in \\{1,2,3\\}$ with an intercept, so each design matrix includes columns $1, x, x^2, \\dots, x^d$. For each test case below:\n- Generate $n$ baseline predictors $x_i$ independently and identically distributed uniformly on $[-2,2]$.\n- Generate responses from the linear signal $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ with $\\beta_0 = 1.0$, $\\beta_1 = 2.0$, and $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n- Compute AIC and BIC for each candidate model, and record the selected degree (the one with the smallest criterion).\n- Add one influential point at $x_{\\text{out}}$ with response $y_{\\text{out}} = \\beta_0 + \\beta_1 x_{\\text{out}} + \\Delta$, where $\\Delta$ is a specified deviation, then recompute AIC and BIC and record the newly selected degrees.\n\nYour program must run the following test suite deterministically by using the specified seeds for the random number generator:\n- Test case $1$: $n=50$, $\\sigma=1.0$, $x_{\\text{out}}=6.0$, $\\Delta=30.0$, seed $=0$.\n- Test case $2$: $n=50$, $\\sigma=0.2$, $x_{\\text{out}}=8.0$, $\\Delta=-60.0$, seed $=1$.\n- Test case $3$: $n=20$, $\\sigma=0.05$, $x_{\\text{out}}=3.0$, $\\Delta=2.0$, seed $=2$.\n\nFor each test case, report the selected degrees before and after adding the outlier for both AIC and BIC. Let $d_{\\text{AIC}}^{\\text{before}}$ and $d_{\\text{AIC}}^{\\text{after}}$ be the selected degrees according to AIC before and after adding the outlier, respectively; similarly define $d_{\\text{BIC}}^{\\text{before}}$ and $d_{\\text{BIC}}^{\\text{after}}$. The required output format is a single line containing a list of lists:\n$[[d_{\\text{AIC}}^{\\text{before}},d_{\\text{AIC}}^{\\text{after}},d_{\\text{BIC}}^{\\text{before}},d_{\\text{BIC}}^{\\text{after}}],\\dots]$,\nwhere each inner list corresponds to one test case and all entries are integers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[[1,2,1,1],[\\dots],\\dots]$). No physical units or angles are involved in this problem; all outputs are dimensionless integers.",
            "solution": "The problem requires an analysis of model selection criteria, specifically the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), in the presence of an influential data point. We are operating within the framework of a Gaussian linear model, and the candidate models are polynomials of varying degrees. A critical first step is to derive the explicit formulas for AIC and BIC from their foundational definitions in this context.\n\n### Derivation of AIC and BIC for the Gaussian Linear Model\n\n**1. Model and Likelihood Function**\nThe specified model is the Gaussian linear model, $\\mathbf{y} = X\\beta + \\varepsilon$, where $\\mathbf{y}$ is the $n \\times 1$ vector of response variables, $X$ is the $n \\times p$ design matrix of predictors, $\\beta$ is the $p \\times 1$ vector of model coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of error terms. The problem states that the errors are independent and identically distributed (i.i.d.) following a normal distribution with mean $0$ and an unknown variance $\\sigma^2$, denoted as $\\varepsilon \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I_n)$.\n\nFor a single data point $(x_i, y_i)$, the probability density function is:\n$$\nf(y_i | \\mathbf{x}_i, \\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i^T\\beta)^2}{2\\sigma^2} \\right)\n$$\nwhere $\\mathbf{x}_i^T$ is the $i$-th row of the design matrix $X$.\nAssuming independence, the likelihood function $L$ for the entire dataset of $n$ observations is the product of the individual densities:\n$$\nL(\\beta, \\sigma^2; \\mathbf{y}, X) = \\prod_{i=1}^n f(y_i | \\mathbf{x}_i, \\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T\\beta)^2 \\right)\n$$\nIn matrix notation, this becomes:\n$$\nL(\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta) \\right)\n$$\nThe log-likelihood function, $\\ell(\\beta, \\sigma^2) = \\ln L(\\beta, \\sigma^2)$, is therefore:\n$$\n\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)\n$$\n\n**2. Maximum Likelihood Estimation (MLE)**\nTo find the maximized log-likelihood, we must find the Maximum Likelihood Estimates (MLEs) of the parameters $\\beta$ and $\\sigma^2$.\n\nFirst, for a fixed $\\sigma^2$, we maximize $\\ell$ with respect to $\\beta$. This is equivalent to minimizing the term $(\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)$, which is the Residual Sum of Squares (RSS). The solution is the standard Ordinary Least Squares (OLS) estimator:\n$$\n\\hat{\\beta}_{\\text{MLE}} = (X^T X)^{-1} X^T \\mathbf{y}\n$$\nNext, we substitute $\\hat{\\beta}_{\\text{MLE}}$ into the log-likelihood and maximize with respect to $\\sigma^2$. Let $\\text{RSS} = (\\mathbf{y} - X\\hat{\\beta}_{\\text{MLE}})^T (\\mathbf{y} - X\\hat{\\beta}_{\\text{MLE}})$ be the minimized residual sum of squares. The log-likelihood becomes:\n$$\n\\ell(\\hat{\\beta}_{\\text{MLE}}, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{\\text{RSS}}{2\\sigma^2}\n$$\nDifferentiating with respect to $\\sigma^2$ and setting to zero yields:\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{\\text{RSS}}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{\\text{RSS}}{n}\n$$\n\n**3. Maximized Log-Likelihood**\nSubstituting the MLEs $\\hat{\\beta}_{\\text{MLE}}$ and $\\hat{\\sigma}^2_{\\text{MLE}}$ back into the log-likelihood function gives the maximized value, $\\hat{\\ell}$:\n$$\n\\hat{\\ell} = \\ell(\\hat{\\beta}_{\\text{MLE}}, \\hat{\\sigma}^2_{\\text{MLE}}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{\\text{RSS}}{n}\\right) - \\frac{\\text{RSS}}{2(\\text{RSS}/n)}\n$$\n$$\n\\hat{\\ell} = -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 1 \\right)\n$$\n\n**4. Information Criteria Formulation**\nThe AIC and BIC are defined using $\\hat{\\ell}$, the number of observations $n$, and the number of estimated parameters $k$. For a polynomial regression of degree $d$, the model has $d+1$ coefficients ($\\beta_0, \\dots, \\beta_d$) and one variance parameter ($\\sigma^2$), so $k = (d+1) + 1 = d+2$.\n\nThe Akaike Information Criterion (AIC) is:\n$$\n\\text{AIC} = 2k - 2\\hat{\\ell} = 2(d+2) + n\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 1 \\right)\n$$\n\nThe Bayesian Information Criterion (BIC) is:\n$$\n\\text{BIC} = k\\ln(n) - 2\\hat{\\ell} = (d+2)\\ln(n) + n\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 1 \\right)\n$$\nIn both cases, a lower value indicates a better model fit, balancing goodness-of-fit (via $\\hat{\\ell}$) and model complexity (via $k$). For a fixed dataset, the term $n(\\ln(2\\pi) + 1)$ is constant across all models and can be ignored for comparison, but the implementation will use the full expressions for fidelity to the derivation.\n\n### Algorithmic Procedure\n\nThe program will execute the following steps for each test case:\n1.  Set the random seed for deterministic data generation.\n2.  Generate a baseline dataset of $n$ points $(x_i, y_i)$ where $x_i \\sim U[-2, 2]$ and $y_i = 1 + 2x_i + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n3.  For each candidate polynomial degree $d \\in \\{1, 2, 3\\}$:\n    a. Construct the $n \\times (d+1)$ design matrix $X$.\n    b. Compute $\\hat{\\beta}$ using OLS via `numpy.linalg.lstsq`.\n    c. Calculate RSS.\n    d. Compute AIC and BIC using the derived formulas.\n4.  Determine the degree $d_{\\text{AIC}}^{\\text{before}}$ and $d_{\\text{BIC}}^{\\text{before}}$ that yield the minimum AIC and BIC values, respectively.\n5.  Augment the dataset with the specified influential point $(x_{\\text{out}}, y_{\\text{out}})$, increasing the sample size to $n+1$.\n6.  Repeat step $3$ for the augmented dataset.\n7.  Determine the new optimal degrees, $d_{\\text{AIC}}^{\\text{after}}$ and $d_{\\text{BIC}}^{\\text{after}}$.\n8.  Collect and format the four selected degrees as specified for the final output.\nThis procedure directly simulates the impact of a high-leverage, high-residual point on model selection.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the effect of an influential point on model selection using AIC and BIC.\n    \"\"\"\n\n    def _select_model(x_data, y_data):\n        \"\"\"\n        Fits polynomial models and selects the best degree using AIC and BIC.\n\n        Args:\n            x_data (np.ndarray): 1D array of predictor values.\n            y_data (np.ndarray): 1D array of response values.\n        \n        Returns:\n            tuple: A tuple containing the selected degree by AIC and BIC.\n        \"\"\"\n        n_samples = len(y_data)\n        d_candidates = [1, 2, 3]\n        \n        model_metrics = []\n        for d in d_candidates:\n            # Construct the design matrix for a polynomial of degree d.\n            # X has columns for x^0, x^1, ..., x^d.\n            X = np.vander(x_data, d + 1, increasing=True)\n            \n            # Solve for beta_hat using Ordinary Least Squares (OLS).\n            # np.linalg.lstsq solves the equation y = Xb.\n            beta_hat, rss_container, _, _ = np.linalg.lstsq(X, y_data, rcond=None)\n            \n            # The returned rss_container is the sum of squared residuals if n > p,\n            # otherwise it's an empty array.\n            if n_samples > (d + 1):\n                rss = rss_container[0]\n            else:\n                # If n <= p, the fit is exact, so RSS must be calculated manually.\n                residuals = y_data - X @ beta_hat\n                rss = np.sum(residuals**2)\n\n            # The number of estimated parameters k is (d+1) for beta coefficients\n            # plus 1 for the variance sigma^2.\n            k = float(d + 2)\n            \n            # Guard against log(0) in case of a perfect fit.\n            if rss < 1e-15:\n                # A perfect fit suggests gross overfitting. Assign a very poor score.\n                log_lik_max = -np.inf\n            else:\n                # Maximized log-likelihood for a Gaussian model.\n                log_lik_max = -n_samples / 2.0 * (np.log(2.0 * np.pi) + np.log(rss / n_samples) + 1.0)\n            \n            # Calculate AIC and BIC using their formal definitions.\n            aic = 2.0 * k - 2.0 * log_lik_max\n            bic = k * np.log(n_samples) - 2.0 * log_lik_max\n            \n            model_metrics.append({'d': d, 'aic': aic, 'bic': bic})\n        \n        # Select the degree that minimizes each criterion.\n        best_aic_model = min(model_metrics, key=lambda x: x['aic'])\n        best_bic_model = min(model_metrics, key=lambda x: x['bic'])\n        \n        return best_aic_model['d'], best_bic_model['d']\n\n    test_cases = [\n        # (n, sigma, x_out, delta, seed)\n        (50, 1.0, 6.0, 30.0, 0),\n        (50, 0.2, 8.0, -60.0, 1),\n        (20, 0.05, 3.0, 2.0, 2),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n, sigma, x_out, delta, seed = case\n        \n        # Set the seed for reproducible random number generation.\n        np.random.seed(seed)\n        \n        beta_0, beta_1 = 1.0, 2.0\n        \n        # --- Analysis Before Adding Outlier ---\n        # Generate baseline data from a true linear model with Gaussian noise.\n        x_base = np.random.uniform(-2.0, 2.0, size=n)\n        epsilon = np.random.normal(0.0, sigma, size=n)\n        y_base = beta_0 + beta_1 * x_base + epsilon\n        \n        # Select best model degree for the baseline data.\n        d_aic_before, d_bic_before = _select_model(x_base, y_base)\n\n        # --- Analysis After Adding Outlier ---\n        # Create the influential point and add it to the dataset.\n        y_out = beta_0 + beta_1 * x_out + delta\n        x_after = np.append(x_base, x_out)\n        y_after = np.append(y_base, y_out)\n        \n        # Re-run model selection on the augmented data.\n        d_aic_after, d_bic_after = _select_model(x_after, y_after)\n        \n        all_results.append([d_aic_before, d_aic_after, d_bic_before, d_bic_after])\n    \n    # Format the output to match the precise specification: [[1,2,1,1],[...],...]\n    # without spaces inside the inner lists.\n    inner_list_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_list_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}