{
    "hands_on_practices": [
        {
            "introduction": "Leverage is a key diagnostic that measures an observation's potential to influence a regression model, based solely on its predictor values. It is quantified by the diagonal elements, $h_{ii}$, of the \"hat\" matrix. This exercise  challenges the simple intuition that high leverage only arises from an extreme value in a single predictor. You will construct a scenario where an observation's high leverage is masked in a simple model but is revealed when an interaction term is included, demonstrating that unusual *combinations* of predictors can be a more subtle source of high leverage.",
            "id": "3154847",
            "problem": "Consider a linear regression setting in which the response is modeled as a linear combination of features, possibly including interaction terms. The Ordinary Least Squares (OLS) estimator minimizes the sum of squared residuals and leads to a fitted value operator that is linear in the response. Your task is to implement a complete program that builds datasets with and without a deliberately constructed extreme interaction pattern and then detects high-leverage points via the augmented design matrix that includes the interaction term.\n\nBase the derivation and algorithm on the following fundamental definitions and facts:\n- In a linear model with design matrix $X \\in \\mathbb{R}^{n \\times p}$, the OLS fitted values are given by applying a linear operator to the response vector. The fitted value operator can be written in terms of a matrix (the projection or \"hat\" matrix) that maps any response vector to its predicted values in the column space of $X$.\n- The diagonal entries of the hat matrix quantify leverage. The leverage of observation $i$ is the $i$-th diagonal element of the hat matrix and measures how far the predictor vector of that observation is from the bulk of the predictor cloud in the geometry induced by $X$.\n- The trace of the hat matrix equals the number of columns $p$ in the design matrix, which implies that the average leverage is $p/n$.\n\nYou must not assume any shortcut formulas beyond these core facts. You should derive any working expressions and algorithms you need from these principles.\n\nProgram requirements:\n1. Construct three synthetic datasets, each with a response generated from a model with an interaction term. For each dataset, fit two models: a base model without the interaction term and an augmented model with the interaction term. For both models, compute the leverage values for all observations and flag high-leverage points using the criterion $h_{ii}  2 \\cdot \\bar{h}$, where $\\bar{h}$ is the average leverage under the model in question.\n2. The response should be generated from the model\n   $$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 x_2) + \\varepsilon, $$\n   with parameters fixed as $ \\beta_0 = 0.5$, $ \\beta_1 = 1.0$, $ \\beta_2 = -0.7$, $ \\beta_3 = 0.8$, and independent noise $ \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ with $ \\sigma = 0.3$. All random generation in each dataset must be deterministic via fixed seeds so that results are reproducible.\n\n3. Datasets and test suite. Build the following three datasets exactly as specified:\n   - Test case $1$ (happy path, extreme interaction with moderate marginals relative to a low-variance bulk):\n     - Sample size $n = 60$.\n     - Random seed $s = 12345$.\n     - Generate $x_1$ and $x_2$ independently with $x_j \\sim \\mathcal{N}(0, 0.6^2)$ for $j \\in \\{1,2\\}$.\n     - Force a single observation at index $i^\\star = 12$ to have $x_1 = 1.8$ and $x_2 = 2.2$. This yields the interaction $x_1 x_2 \\approx 3.96$, which is extreme relative to the typical $x_1 x_2$ scale given the low-variance bulk.\n   - Test case $2$ (boundary condition, outlying marginal without extreme interaction):\n     - Sample size $n = 60$.\n     - Random seed $s = 24680$.\n     - Generate $x_1$ and $x_2$ independently with $x_j \\sim \\mathcal{N}(0, 0.6^2)$.\n     - Force a single observation at index $i^\\star = 7$ to have $x_1 = 4.0$ and $x_2 = 0.2$. This yields the interaction $x_1 x_2 = 0.8$, which is not extreme relative to the bulk, but $x_1$ is a strong marginal outlier.\n   - Test case $3$ (edge case, near collinearity amplifies interaction leverage):\n     - Sample size $n = 25$.\n     - Random seed $s = 31415$.\n     - Generate $x_1 \\sim \\mathcal{N}(0, 0.8^2)$ and set $x_2 = x_1 + \\delta$ with $\\delta \\sim \\mathcal{N}(0, 0.1^2)$ independently, creating near-collinearity.\n     - Force a single observation at index $i^\\star = 5$ to have $x_1 = 2.0$ and $x_2 = 2.0$, causing a large interaction $x_1 x_2 = 4.0$ in a low-$n$, near-collinear context.\n\n4. For each dataset:\n   - Construct the base design matrix with columns $[1, x_1, x_2]$ (intercept plus main effects).\n   - Construct the augmented design matrix with columns $[1, x_1, x_2, x_1 x_2]$ (intercept plus main effects plus interaction).\n   - For each design matrix, compute the hat matrix and its diagonal to obtain leverage values. Flag indices where $h_{ii}  2 \\cdot (p/n)$, with $p$ equal to the number of columns in the design matrix used.\n   - Return, for each dataset, two lists: the sorted indices flagged by the base model and the sorted indices flagged by the augmented model.\n\n5. Final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of two lists: the first list is the base-model flagged indices, and the second list is the augmented-model flagged indices.\n   - Concretely, the output should look like\n     $$ [ [ [i_{1,1}, i_{1,2}, \\dots], [j_{1,1}, j_{1,2}, \\dots] ], [ [i_{2,1}, \\dots], [j_{2,1}, \\dots] ], [ [i_{3,1}, \\dots], [j_{3,1}, \\dots] ] ] $$\n     where indices are integers in ascending order.\n\nNo physical units are involved, no angle units are involved, and no percentages are required anywhere in this task.",
            "solution": "We begin from first principles of linear regression. In a linear model with design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$, the Ordinary Least Squares (OLS) estimator seeks parameters $\\hat{\\beta} \\in \\mathbb{R}^{p}$ that minimize the sum of squared residuals\n$$ S(\\beta) = \\| y - X \\beta \\|_2^2. $$\nSetting the gradient of $S(\\beta)$ with respect to $\\beta$ to zero gives the normal equations\n$$ X^\\top X \\hat{\\beta} = X^\\top y. $$\nWhen $X^\\top X$ is invertible, the unique solution is\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y. $$\nIf $X^\\top X$ is not invertible, the Moore–Penrose pseudoinverse can be used to obtain the minimum-norm solution\n$$ \\hat{\\beta} = (X^\\top X)^{+} X^\\top y, $$\nwhere $(\\cdot)^{+}$ denotes the pseudoinverse.\n\nThe OLS fitted values are\n$$ \\hat{y} = X \\hat{\\beta} = X (X^\\top X)^{+} X^\\top y = H y, $$\nwhere\n$$ H = X (X^\\top X)^{+} X^\\top $$\nis the projection matrix (the \"hat\" matrix). The matrix $H$ is symmetric and idempotent, with $H^\\top = H$ and $H^2 = H$. An immediate and fundamental property is\n$$ \\mathrm{trace}(H) = p, $$\nthe number of columns in $X$. Consequently, the average leverage is\n$$ \\bar{h} = \\frac{1}{n} \\sum_{i=1}^{n} h_{ii} = \\frac{p}{n}, $$\nwhere $h_{ii}$ denotes the $i$-th diagonal element of $H$, i.e., the leverage of observation $i$.\n\nLeverage quantifies how far an observation's predictor vector is from the center of the predictor space in the metric induced by $X$. In particular, adding columns to $X$ (e.g., an interaction feature) expands the feature space, creating directions along which observations can become far from the bulk, thereby potentially increasing their leverage. An observation can exhibit moderate main effects $x_1$ and $x_2$ individually but have an extreme interaction $x_1 x_2$; in the augmented space, this observation may be far from the main cloud along the interaction dimension, which increases $h_{ii}$ even if the main effects alone would not produce high leverage.\n\nAlgorithmic steps derived from these principles:\n1. For each dataset, construct two design matrices:\n   - Base design matrix $X_{\\mathrm{base}}$ with columns $[1, x_1, x_2]$ (intercept and main effects).\n   - Augmented design matrix $X_{\\mathrm{aug}}$ with columns $[1, x_1, x_2, x_1 x_2]$ (intercept, main effects, and interaction).\n2. For each $X$, compute the hat matrix $H = X (X^\\top X)^{+} X^\\top$ using the pseudoinverse of $X^\\top X$ to ensure numerical stability.\n3. Extract leverage values $h_{ii}$ as the diagonal of $H$.\n4. Using the fundamental property $\\bar{h} = p/n$, flag indices satisfying $h_{ii}  2 \\cdot \\bar{h} = 2p/n$. The multiplicative constant $2$ sets a conservative criterion that highlights observations whose leverage substantially exceeds the average, and it is expressed entirely in terms of $p$ and $n$ derived from $X$.\n\nConstruction of datasets:\n- Test case $1$ (happy path):\n  - Generate $x_1, x_2 \\sim \\mathcal{N}(0, 0.6^2)$ independently for $n = 60$ using seed $s = 12345$. Force index $i^\\star = 12$ to have $x_1 = 1.8$, $x_2 = 2.2$, yielding interaction $x_1 x_2 \\approx 3.96$, large relative to the bulk scale of $x_1 x_2$ given the low variance.\n  - Generate $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 + \\varepsilon$, with $\\beta_0 = 0.5$, $\\beta_1 = 1.0$, $\\beta_2 = -0.7$, $\\beta_3 = 0.8$, and $\\varepsilon \\sim \\mathcal{N}(0, 0.3^2)$ (independent).\n- Test case $2$ (boundary condition, marginal outlier, non-extreme interaction):\n  - Generate $x_1, x_2 \\sim \\mathcal{N}(0, 0.6^2)$ independently for $n = 60$ using seed $s = 24680$. Force index $i^\\star = 7$ to have $x_1 = 4.0$, $x_2 = 0.2$, yielding interaction $x_1 x_2 = 0.8$, which is not extreme under the bulk scale, while $x_1$ is a large marginal outlier.\n  - Generate $y$ as above.\n- Test case $3$ (edge case, near collinearity):\n  - Generate $x_1 \\sim \\mathcal{N}(0, 0.8^2)$ and $\\delta \\sim \\mathcal{N}(0, 0.1^2)$ independently for $n = 25$ using seed $s = 31415$. Set $x_2 = x_1 + \\delta$, producing near collinearity. Force index $i^\\star = 5$ to have $x_1 = 2.0$, $x_2 = 2.0$, so the interaction is $x_1 x_2 = 4.0$, which is notable in a low-$n$, near-collinear context.\n  - Generate $y$ as above.\n\nWhy the detection works:\n- For the base design matrix $X_{\\mathrm{base}}$ with $p = 3$, the average leverage is $\\bar{h} = 3/n$. A marginal outlier (e.g., $x_1$ large) tends to increase its leverage under the base model. However, an observation with moderate marginal values but a large interaction does not appear extreme in the base model, because the interaction is not represented there; hence its leverage may remain near the average.\n- For the augmented design matrix $X_{\\mathrm{aug}}$ with $p = 4$, the average leverage is $\\bar{h} = 4/n$. The interaction feature $x_1 x_2$ provides a new direction. An observation with a large $x_1 x_2$ relative to the typical scale will be far from the bulk along this direction, increasing its leverage $h_{ii}$ in $X_{\\mathrm{aug}}$. Thus, such an observation can be flagged as high leverage by the augmented model even if its marginal components are only moderate.\n\nImplementation and output:\n- For each dataset, compute the flagged indices for both the base and augmented models as described. Sort the flagged indices in ascending order.\n- The program must print a single line containing a list of results for the three test cases. Each result is a pair of lists: the first for the base model and the second for the augmented model. The format is\n  $$ [ [ \\text{base}_1, \\text{aug}_1 ], [ \\text{base}_2, \\text{aug}_2 ], [ \\text{base}_3, \\text{aug}_3 ] ], $$\n  where each $\\text{base}_k$ and $\\text{aug}_k$ is a list of integers representing indices of flagged observations for test case $k$.\n\nThis procedure is grounded entirely in the derivation of the OLS normal equations, the projection (hat) matrix $H$, and its fundamental properties, without invoking shortcut formulas beyond these core definitions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hat_leverages(X: np.ndarray) - np.ndarray:\n    \"\"\"\n    Compute leverage values (diagonal of hat matrix) for design matrix X.\n    Uses the pseudoinverse of X^T X for numerical stability.\n    \"\"\"\n    XTX = X.T @ X\n    XTX_pinv = np.linalg.pinv(XTX)\n    H = X @ XTX_pinv @ X.T\n    # Numerical safety: clip tiny negative due to rounding to zero\n    h = np.clip(np.diag(H), 0.0, 1.0)\n    return h\n\ndef flag_high_leverage(h: np.ndarray, p: int) - np.ndarray:\n    \"\"\"\n    Flag indices where leverage exceeds 2 * (p/n).\n    Returns sorted indices as a numpy array of ints.\n    \"\"\"\n    n = h.shape[0]\n    avg_h = p / n\n    threshold = 2.0 * avg_h\n    idx = np.where(h  threshold)[0]\n    return np.sort(idx)\n\ndef build_case_1():\n    # Test case 1: extreme interaction with moderate marginals relative to low-variance bulk\n    n = 60\n    rng = np.random.default_rng(12345)\n    x1 = rng.normal(0.0, 0.6, size=n)\n    x2 = rng.normal(0.0, 0.6, size=n)\n    # Force special observation\n    i_star = 12\n    x1[i_star] = 1.8\n    x2[i_star] = 2.2\n\n    # True parameters\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef build_case_2():\n    # Test case 2: marginal outlier without extreme interaction\n    n = 60\n    rng = np.random.default_rng(24680)\n    x1 = rng.normal(0.0, 0.6, size=n)\n    x2 = rng.normal(0.0, 0.6, size=n)\n    i_star = 7\n    x1[i_star] = 4.0\n    x2[i_star] = 0.2\n\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef build_case_3():\n    # Test case 3: near collinearity amplifies interaction leverage\n    n = 25\n    rng = np.random.default_rng(31415)\n    x1 = rng.normal(0.0, 0.8, size=n)\n    delta = rng.normal(0.0, 0.1, size=n)\n    x2 = x1 + delta\n    i_star = 5\n    x1[i_star] = 2.0\n    x2[i_star] = 2.0\n\n    beta0, beta1, beta2, beta3 = 0.5, 1.0, -0.7, 0.8\n    eps = rng.normal(0.0, 0.3, size=n)\n    y = beta0 + beta1 * x1 + beta2 * x2 + beta3 * (x1 * x2) + eps\n\n    X_base = np.column_stack([np.ones(n), x1, x2])\n    X_aug = np.column_stack([np.ones(n), x1, x2, x1 * x2])\n    return X_base, X_aug, y\n\ndef solve():\n    # Define the test cases from the problem statement.\n    cases = [build_case_1, build_case_2, build_case_3]\n\n    results = []\n    for build in cases:\n        X_base, X_aug, y = build()\n\n        # Compute leverages\n        h_base = hat_leverages(X_base)\n        h_aug = hat_leverages(X_aug)\n\n        # Flag high leverage indices using threshold 2 * (p/n)\n        base_flags = flag_high_leverage(h_base, p=X_base.shape[1]).tolist()\n        aug_flags = flag_high_leverage(h_aug, p=X_aug.shape[1]).tolist()\n\n        results.append([base_flags, aug_flags])\n\n    # Final print statement in the exact required format.\n    print(str(results))\n\nsolve()\n```"
        },
        {
            "introduction": "Once we identify a high-leverage point, the next question is about its actual impact, or *influence*. An influential point is one that, if removed, would cause a significant change in the fitted model, combining the effects of high leverage and a large residual. This practice  offers a dramatic demonstration of this principle by showing how a single influential observation can distort automated model selection criteria like the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). You will see firsthand how an outlier can mislead these criteria into selecting a more complex, and incorrect, model in a misguided attempt to accommodate it.",
            "id": "3154883",
            "problem": "You are given a synthetic regression scenario to assess how the presence of a single influential observation affects model selection using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). Work within the Gaussian linear modeling framework: the response is generated by a linear signal plus noise, and candidate models are polynomials of degrees in $\\{1,2,3\\}$. Your program must implement the following, using only derivations from foundational definitions.\n\nStart from the Gaussian linear model with Ordinary Least Squares (OLS): the data consist of $n$ pairs $(x_i,y_i)$ with $x_i \\in \\mathbb{R}$, where the model is $y = X\\beta + \\varepsilon$ for a design matrix $X$, parameter vector $\\beta$, and noise vector $\\varepsilon$. Assume $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix and $\\sigma^2$ is an unknown variance. Use Maximum Likelihood Estimation (MLE) for the Gaussian model to derive the maximized log-likelihood in terms of the residual sum of squares and the sample size, and then use the definition of Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) to obtain implementable expressions in terms of the fitted residuals, the number of parameters, and the sample size. Do not assume any pre-provided shortcut formulas.\n\nDefine candidate models as polynomial regressions of degrees $d \\in \\{1,2,3\\}$ with an intercept, so each design matrix includes columns $1, x, x^2, \\dots, x^d$. For each test case below:\n- Generate $n$ baseline predictors $x_i$ independently and identically distributed uniformly on $[-2,2]$.\n- Generate responses from the linear signal $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ with $\\beta_0 = 1.0$, $\\beta_1 = 2.0$, and $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n- Compute AIC and BIC for each candidate model, and record the selected degree (the one with the smallest criterion).\n- Add one influential point at $x_{\\text{out}}$ with response $y_{\\text{out}} = \\beta_0 + \\beta_1 x_{\\text{out}} + \\Delta$, where $\\Delta$ is a specified deviation, then recompute AIC and BIC and record the newly selected degrees.\n\nYour program must run the following test suite deterministically by using the specified seeds for the random number generator:\n- Test case $1$: $n=50$, $\\sigma=1.0$, $x_{\\text{out}}=6.0$, $\\Delta=30.0$, seed $=0$.\n- Test case $2$: $n=50$, $\\sigma=0.2$, $x_{\\text{out}}=8.0$, $\\Delta=-60.0$, seed $=1$.\n- Test case $3$: $n=20$, $\\sigma=0.05$, $x_{\\text{out}}=3.0$, $\\Delta=2.0$, seed $=2$.\n\nFor each test case, report the selected degrees before and after adding the outlier for both AIC and BIC. Let $d_{\\text{AIC}}^{\\text{before}}$ and $d_{\\text{AIC}}^{\\text{after}}$ be the selected degrees according to AIC before and after adding the outlier, respectively; similarly define $d_{\\text{BIC}}^{\\text{before}}$ and $d_{\\text{BIC}}^{\\text{after}}$. The required output format is a single line containing a list of lists:\n$[[d_{\\text{AIC}}^{\\text{before}},d_{\\text{AIC}}^{\\text{after}},d_{\\text{BIC}}^{\\text{before}},d_{\\text{BIC}}^{\\text{after}}],\\dots]$,\nwhere each inner list corresponds to one test case and all entries are integers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[[1,2,1,1],[\\dots],\\dots]$). No physical units or angles are involved in this problem; all outputs are dimensionless integers.",
            "solution": "The problem requires an analysis of model selection criteria, specifically the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), in the presence of an influential data point. We are operating within the framework of a Gaussian linear model, and the candidate models are polynomials of varying degrees. A critical first step is to derive the explicit formulas for AIC and BIC from their foundational definitions in this context.\n\n### Derivation of AIC and BIC for the Gaussian Linear Model\n\n**1. Model and Likelihood Function**\nThe specified model is the Gaussian linear model, $\\mathbf{y} = X\\beta + \\varepsilon$, where $\\mathbf{y}$ is the $n \\times 1$ vector of response variables, $X$ is the $n \\times p$ design matrix of predictors, $\\beta$ is the $p \\times 1$ vector of model coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of error terms. The problem states that the errors are independent and identically distributed (i.i.d.) following a normal distribution with mean $0$ and an unknown variance $\\sigma^2$, denoted as $\\varepsilon \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I_n)$.\n\nFor a single data point $(x_i, y_i)$, the probability density function is:\n$$\nf(y_i | \\mathbf{x}_i, \\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y_i - \\mathbf{x}_i^T\\beta)^2}{2\\sigma^2} \\right)\n$$\nwhere $\\mathbf{x}_i^T$ is the $i$-th row of the design matrix $X$.\nAssuming independence, the likelihood function $L$ for the entire dataset of $n$ observations is the product of the individual densities:\n$$\nL(\\beta, \\sigma^2; \\mathbf{y}, X) = \\prod_{i=1}^n f(y_i | \\mathbf{x}_i, \\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T\\beta)^2 \\right)\n$$\nIn matrix notation, this becomes:\n$$\nL(\\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta) \\right)\n$$\nThe log-likelihood function, $\\ell(\\beta, \\sigma^2) = \\ln L(\\beta, \\sigma^2)$, is therefore:\n$$\n\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} (\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)\n$$\n\n**2. Maximum Likelihood Estimation (MLE)**\nTo find the maximized log-likelihood, we must find the Maximum Likelihood Estimates (MLEs) of the parameters $\\beta$ and $\\sigma^2$.\n\nFirst, for a fixed $\\sigma^2$, we maximize $\\ell$ with respect to $\\beta$. This is equivalent to minimizing the term $(\\mathbf{y} - X\\beta)^T (\\mathbf{y} - X\\beta)$, which is the Residual Sum of Squares (RSS). The solution is the standard Ordinary Least Squares (OLS) estimator:\n$$\n\\hat{\\beta}_{\\text{MLE}} = (X^T X)^{-1} X^T \\mathbf{y}\n$$\nNext, we substitute $\\hat{\\beta}_{\\text{MLE}}$ into the log-likelihood and maximize with respect to $\\sigma^2$. Let $\\text{RSS} = (\\mathbf{y} - X\\hat{\\beta}_{\\text{MLE}})^T (\\mathbf{y} - X\\hat{\\beta}_{\\text{MLE}})$ be the minimized residual sum of squares. The log-likelihood becomes:\n$$\n\\ell(\\hat{\\beta}_{\\text{MLE}}, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{\\text{RSS}}{2\\sigma^2}\n$$\nDifferentiating with respect to $\\sigma^2$ and setting to zero yields:\n$$\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{\\text{RSS}}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{\\text{RSS}}{n}\n$$\n\n**3. Maximized Log-Likelihood**\nSubstituting the MLEs $\\hat{\\beta}_{\\text{MLE}}$ and $\\hat{\\sigma}^2_{\\text{MLE}}$ back into the log-likelihood function gives the maximized value, $\\hat{\\ell}$:\n$$\n\\hat{\\ell} = \\ell(\\hat{\\beta}_{\\text{MLE}}, \\hat{\\sigma}^2_{\\text{MLE}}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{\\text{RSS}}{n}\\right) - \\frac{\\text{RSS}}{2(\\text{RSS}/n)}\n$$\n$$\n\\hat{\\ell} = -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 1 \\right)\n$$\n\n**4. Information Criteria Formulation**\nThe AIC and BIC are defined using $\\hat{\\ell}$, the number of observations $n$, and the number of estimated parameters $k$. For a polynomial regression of degree $d$, the model has $d+1$ coefficients ($\\beta_0, \\dots, \\beta_d$) and one variance parameter ($\\sigma^2$), so $k = (d+1) + 1 = d+2$.\n\nThe Akaike Information Criterion (AIC) is:\n$$\n\\text{AIC} = 2k - 2\\hat{\\ell} = 2(d+2) + n\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 1 \\right)\n$$\n\nThe Bayesian Information Criterion (BIC) is:\n$$\n\\text{BIC} = k\\ln(n) - 2\\hat{\\ell} = (d+2)\\ln(n) + n\\left( \\ln(2\\pi) + \\ln\\left(\\frac{\\text{RSS}}{n}\\right) + 1 \\right)\n$$\nIn both cases, a lower value indicates a better model fit, balancing goodness-of-fit (via $\\hat{\\ell}$) and model complexity (via $k$). For a fixed dataset, the term $n(\\ln(2\\pi) + 1)$ is constant across all models and can be ignored for comparison, but the implementation will use the full expressions for fidelity to the derivation.\n\n### Algorithmic Procedure\n\nThe program will execute the following steps for each test case:\n1.  Set the random seed for deterministic data generation.\n2.  Generate a baseline dataset of $n$ points $(x_i, y_i)$ where $x_i \\sim U[-2, 2]$ and $y_i = 1 + 2x_i + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n3.  For each candidate polynomial degree $d \\in \\{1, 2, 3\\}$:\n    a. Construct the $n \\times (d+1)$ design matrix $X$.\n    b. Compute $\\hat{\\beta}$ using OLS via `numpy.linalg.lstsq`.\n    c. Calculate RSS.\n    d. Compute AIC and BIC using the derived formulas.\n4.  Determine the degree $d_{\\text{AIC}}^{\\text{before}}$ and $d_{\\text{BIC}}^{\\text{before}}$ that yield the minimum AIC and BIC values, respectively.\n5.  Augment the dataset with the specified influential point $(x_{\\text{out}}, y_{\\text{out}})$, increasing the sample size to $n+1$.\n6.  Repeat step $3$ for the augmented dataset.\n7.  Determine the new optimal degrees, $d_{\\text{AIC}}^{\\text{after}}$ and $d_{\\text{BIC}}^{\\text{after}}$.\n8.  Collect and format the four selected degrees as specified for the final output.\nThis procedure directly simulates the impact of a high-leverage, high-residual point on model selection.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the effect of an influential point on model selection using AIC and BIC.\n    \"\"\"\n\n    def _select_model(x_data, y_data):\n        \"\"\"\n        Fits polynomial models and selects the best degree using AIC and BIC.\n\n        Args:\n            x_data (np.ndarray): 1D array of predictor values.\n            y_data (np.ndarray): 1D array of response values.\n        \n        Returns:\n            tuple: A tuple containing the selected degree by AIC and BIC.\n        \"\"\"\n        n_samples = len(y_data)\n        d_candidates = [1, 2, 3]\n        \n        model_metrics = []\n        for d in d_candidates:\n            # Construct the design matrix for a polynomial of degree d.\n            # X has columns for x^0, x^1, ..., x^d.\n            X = np.vander(x_data, d + 1, increasing=True)\n            \n            # Solve for beta_hat using Ordinary Least Squares (OLS).\n            # np.linalg.lstsq solves the equation y = Xb.\n            beta_hat, rss_container, _, _ = np.linalg.lstsq(X, y_data, rcond=None)\n            \n            # The returned rss_container is the sum of squared residuals if n > p,\n            # otherwise it's an empty array.\n            if n_samples > (d + 1):\n                rss = rss_container[0]\n            else:\n                # If n = p, the fit is exact, so RSS must be calculated manually.\n                residuals = y_data - X @ beta_hat\n                rss = np.sum(residuals**2)\n\n            # The number of estimated parameters k is (d+1) for beta coefficients\n            # plus 1 for the variance sigma^2.\n            k = float(d + 2)\n            \n            # Guard against log(0) in case of a perfect fit.\n            if rss  1e-15:\n                # A perfect fit suggests gross overfitting. Assign a very poor score.\n                log_lik_max = -np.inf\n            else:\n                # Maximized log-likelihood for a Gaussian model.\n                log_lik_max = -n_samples / 2.0 * (np.log(2.0 * np.pi) + np.log(rss / n_samples) + 1.0)\n            \n            # Calculate AIC and BIC using their formal definitions.\n            aic = 2.0 * k - 2.0 * log_lik_max\n            bic = k * np.log(n_samples) - 2.0 * log_lik_max\n            \n            model_metrics.append({'d': d, 'aic': aic, 'bic': bic})\n        \n        # Select the degree that minimizes each criterion.\n        best_aic_model = min(model_metrics, key=lambda x: x['aic'])\n        best_bic_model = min(model_metrics, key=lambda x: x['bic'])\n        \n        return best_aic_model['d'], best_bic_model['d']\n\n    test_cases = [\n        # (n, sigma, x_out, delta, seed)\n        (50, 1.0, 6.0, 30.0, 0),\n        (50, 0.2, 8.0, -60.0, 1),\n        (20, 0.05, 3.0, 2.0, 2),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n, sigma, x_out, delta, seed = case\n        \n        # Set the seed for reproducible random number generation.\n        np.random.seed(seed)\n        \n        beta_0, beta_1 = 1.0, 2.0\n        \n        # --- Analysis Before Adding Outlier ---\n        # Generate baseline data from a true linear model with Gaussian noise.\n        x_base = np.random.uniform(-2.0, 2.0, size=n)\n        epsilon = np.random.normal(0.0, sigma, size=n)\n        y_base = beta_0 + beta_1 * x_base + epsilon\n        \n        # Select best model degree for the baseline data.\n        d_aic_before, d_bic_before = _select_model(x_base, y_base)\n\n        # --- Analysis After Adding Outlier ---\n        # Create the influential point and add it to the dataset.\n        y_out = beta_0 + beta_1 * x_out + delta\n        x_after = np.append(x_base, x_out)\n        y_after = np.append(y_base, y_out)\n        \n        # Re-run model selection on the augmented data.\n        d_aic_after, d_bic_after = _select_model(x_after, y_after)\n        \n        all_results.append([d_aic_before, d_aic_after, d_bic_before, d_bic_after])\n    \n    # Format the output to match the precise specification: [[1,2,1,1],[...],...]\n    # without spaces inside the inner lists.\n    inner_list_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_list_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world data science problems often involve predicting multiple outcomes simultaneously, a setting known as multiresponse regression. This raises a critical question: is it sufficient to check for influential points in each response variable independently? This exercise  explores the limitations of such a one-dimensional approach by extending the concept of influence to the multivariate domain. By implementing a multivariate version of Cook's distance, you will compare its diagnostic power against separate univariate analyses and discover situations where a point's joint influence is far more revealing than what is visible in any single response.",
            "id": "3154904",
            "problem": "You are given a multiresponse linear regression setting where the response matrix has multiple columns and one observation is extreme in all responses for certain test cases. Your task is to implement a program that, for each test case, constructs a synthetic but scientifically plausible dataset, fits the model using Ordinary Least Squares (OLS), computes a multivariate Cook’s distance per observation, computes univariate Cook’s distances for each response dimension, and compares the diagnostics.\n\nFundamental base for the derivation must start from the standard multiresponse linear model, OLS normal equations, the projection (hat) matrix, residuals, and the residual covariance estimator, without relying on shortcut influence formulas. Let $n$ denote the number of observations, $p$ the number of predictors including an intercept, and $m$ the number of response variables. The model is $Y \\in \\mathbb{R}^{n \\times m}$ with rows $y_i^\\top$, predictor matrix $X \\in \\mathbb{R}^{n \\times p}$ with rows $x_i^\\top$, coefficient matrix $B \\in \\mathbb{R}^{p \\times m}$, and additive noise $E \\in \\mathbb{R}^{n \\times m}$. You must use the OLS estimator derived from the normal equations to fit the model, and use the residual covariance estimator to scale the multivariate influence measure. Do not assume any special distributional shortcuts beyond the linear model and the least squares framework.\n\nFor each test case, generate the data as follows:\n- Create $X$ with an intercept column of ones and $p - 1$ standard normal features.\n- Draw $B$ with entries from a standard normal distribution.\n- Draw noise $E$ with independent entries from a normal distribution with mean $0$ and specified standard deviation.\n- Form $Y = X B + E$.\n- Apply the specified extreme configuration to the designated observation index $i^\\*$:\n    - High leverage modification: multiply the non-intercept features of row $i^\\*$ in $X$ by the given factor.\n    - Extreme response modification: add a constant offset to all $m$ responses for row $i^\\*$ in $Y$.\n- After modifying $X$ for row $i^\\*$, ensure that $Y_{i^\\* \\cdot}$ is recomputed as $x_{i^\\*}^\\top B + e_{i^\\*}$ before any response offset is applied, so that $Y$ remains internally consistent with the linear model plus noise before the extreme response shift.\n\nYou must compute:\n1. The multivariate Cook’s distance per observation using a principled construction rooted in the change in the OLS solution under case deletion and scaled by the residual covariance, and summarize each test case by a boolean indicating whether the designated extreme observation $i^\\*$ achieves the largest multivariate Cook’s distance.\n2. The univariate Cook’s distance per observation for each response dimension computed using standard OLS quantities, and summarize each test case by an integer counting how many of the $m$ univariate responses identify $i^\\*$ as the observation with the largest Cook’s distance.\n\nYour program must implement the calculations from first principles and not rely on any built-in statistical learning routines. The final output must aggregate these summaries for all test cases in a single line, formatted as a comma-separated list enclosed in square brackets.\n\nTest suite:\n- Case $1$ (happy path: extreme residual without high leverage):\n    - $n = 40$, $p = 3$ (one intercept plus $2$ features), $m = 3$, noise standard deviation $1.0$, random seed $12345$, extreme index $i^\\* = 5$, high leverage multiplier $1.0$ (no leverage change), extreme response offset $12.0$.\n- Case $2$ (boundary: high leverage without extreme residual):\n    - $n = 40$, $p = 3$, $m = 3$, noise standard deviation $1.0$, random seed $12346$, extreme index $i^\\* = 10$, high leverage multiplier $12.0$, extreme response offset $0.0$.\n- Case $3$ (edge: both high leverage and extreme residual):\n    - $n = 40$, $p = 3$, $m = 3$, noise standard deviation $1.0$, random seed $12347$, extreme index $i^\\* = 20$, high leverage multiplier $12.0$, extreme response offset $12.0$.\n\nAnswer format:\n- For each case, produce two values:\n    - A boolean for whether the designated $i^\\*$ is the argmax of the multivariate Cook’s distance.\n    - An integer equal to the count of response dimensions for which $i^\\*$ is the argmax of the univariate Cook’s distance.\n- Your program should produce a single line of output containing the six results as a comma-separated list enclosed in square brackets, in the order of the test cases: $[\\text{case1\\_multivariate\\_bool},\\text{case1\\_univariate\\_count},\\text{case2\\_multivariate\\_bool},\\text{case2\\_univariate\\_count},\\text{case3\\_multivariate\\_bool},\\text{case3\\_univariate\\_count}]$.\n\nNo physical units, angle units, or percentages are required in this problem, and all outputs must be boolean or integer types. Ensure scientific realism by adhering strictly to the linear model assumptions and reproducible random generation using the specified seeds.",
            "solution": "We begin from the multiresponse linear model with $n$ observations, $p$ predictors including an intercept, and $m$ response variables. Let $Y \\in \\mathbb{R}^{n \\times m}$ be the response matrix, $X \\in \\mathbb{R}^{n \\times p}$ be the predictor matrix, and assume the model $Y = X B + E$, where $B \\in \\mathbb{R}^{p \\times m}$ is the coefficient matrix to be estimated, and $E$ is noise.\n\nThe fundamental base for estimation is Ordinary Least Squares (OLS). The OLS estimator is obtained by minimizing the Frobenius norm of residuals, which yields the normal equations. The well-tested formula for the OLS solution is\n$$\n\\hat{B} = (X^\\top X)^{-1} X^\\top Y,\n$$\nassuming $X^\\top X$ is invertible. In practice, the Moore–Penrose pseudoinverse can be used when necessary, but with randomly generated features and $n \\gg p$, invertibility is typical. The fitted value matrix is $\\hat{Y} = X \\hat{B}$, and the residual matrix is $\\hat{E} = Y - \\hat{Y}$. The residual covariance estimator is given by\n$$\n\\hat{S} = \\frac{\\hat{E}^\\top \\hat{E}}{n - p},\n$$\nwhich is a well-tested estimator of the noise covariance assuming homoscedasticity within the multivariate linear model.\n\nLeverage is a fundamental concept: the projection or hat matrix $H$ is\n$$\nH = X (X^\\top X)^{-1} X^\\top,\n$$\nand the leverage values are $h_{ii} = H_{ii}$. High leverage indicates an observation with unusual predictor values, which can strongly affect the fitted model.\n\nFor influence comparison, we consider the effect of deleting observation $i$. Define the leave-one-out estimator $\\hat{B}_{(i)}$ computed from $X$ and $Y$ with row $i$ removed. The change in the coefficient estimate is $\\Delta_i = \\hat{B} - \\hat{B}_{(i)}$. A principled multivariate influence measure aggregates the scaled quadratic form of this change across responses using the $X^\\top X$ geometry and the inverse residual covariance,\n$$\nM_i = \\Delta_i^\\top (X^\\top X) \\Delta_i \\in \\mathbb{R}^{m \\times m}, \\quad \\text{and} \\quad D_i^{\\text{multi}} = \\frac{1}{m p} \\operatorname{tr}\\!\\left(M_i \\hat{S}^{-1}\\right).\n$$\nThis construction generalizes the univariate case’s use of $X^\\top X$ weighting and mean squared error normalization by appropriately scaling with the inverse residual covariance across the $m$ response dimensions and dividing by $m p$ so that in the special case $m = 1$ it reduces to the familiar univariate scaling by $p$ times mean squared error.\n\nFor univariate diagnostics, when considering a single response $y \\in \\mathbb{R}^n$, the OLS residual vector is $\\hat{e} = y - X \\hat{\\beta}$, the Mean Squared Error (MSE) is\n$$\n\\text{MSE} = \\frac{\\hat{e}^\\top \\hat{e}}{n - p},\n$$\nand an established influence measure is Cook’s distance,\n$$\nD_i^{\\text{uni}} = \\frac{\\hat{e}_i^2}{p \\cdot \\text{MSE}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2}.\n$$\nThis expression follows from the exact relationship between the leave-one-out change in fitted values and the residual $ \\hat{e}_i $ together with the leverage $ h_{ii} $ derived via partitioned inverse or Sherman–Morrison identities.\n\nAlgorithmic design:\n- For each test case, construct $X$, draw $B$, draw noise $E$, form $Y = X B + E$, apply the specified extreme configurations to $X$ and/or $Y$, ensuring internal consistency by recomputing $Y_{i^\\* \\cdot}$ as $x_{i^\\*}^\\top B + e_{i^\\*}$ prior to applying the extreme response offset.\n- Fit the multivariate OLS to obtain $\\hat{B}$, residuals $\\hat{E}$, and $\\hat{S}$, along with $X^\\top X$.\n- For each observation $i$, compute the leave-one-out estimator $\\hat{B}_{(i)}$ by refitting with row $i$ removed, compute $\\Delta_i = \\hat{B} - \\hat{B}_{(i)}$, form $M_i = \\Delta_i^\\top (X^\\top X) \\Delta_i$, and compute the multivariate Cook’s distance $D_i^{\\text{multi}} = \\frac{1}{m p} \\operatorname{tr}(M_i \\hat{S}^{-1})$.\n- To compute univariate Cook’s distances, for each response column $y^{(j)}$, compute $\\hat{\\beta}^{(j)}$, residuals $\\hat{e}^{(j)}$, MSE for that response, and leverage values $h_{ii}$ from the common hat matrix $H$, then apply the univariate formula to obtain $D_i^{\\text{uni}, j}$ for all $i$.\n- For each case, determine whether the designated extreme index $i^\\*$ is the argmax of the multivariate Cook’s distance and count in how many response dimensions $i^\\*$ is the argmax of the univariate Cook’s distance.\n\nInterpretation expectations in the test suite:\n- Case $1$ presents an observation with extreme residuals in all responses but no high leverage, which should be strongly flagged by both multivariate and univariate Cook’s distances.\n- Case $2$ presents an observation with high leverage but no extreme residuals; leverage alone without large residuals typically does not yield high Cook’s distance, so influence may be modest and not maximal.\n- Case $3$ presents both high leverage and extreme residuals, which together should induce a large influence in both multivariate and univariate diagnostics.\n\nThe program outputs a single line in the exact specified format, aggregating for the three cases the boolean multivariate identification of $i^\\*$ and the integer count of univariate identifications of $i^\\*$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_multivariate_ols(X, Y):\n    \"\"\"\n    Fit multiresponse OLS: returns B_hat, residual covariance S_hat, and XTX.\n    \"\"\"\n    # Use explicit normal equations with pinv for numerical stability\n    X_pinv = np.linalg.pinv(X)\n    B_hat = X_pinv @ Y\n    Y_hat = X @ B_hat\n    E_hat = Y - Y_hat\n    n, p = X.shape\n    # Residual covariance estimator\n    S_hat = (E_hat.T @ E_hat) / (n - p)\n    XTX = X.T @ X\n    return B_hat, S_hat, XTX\n\ndef multivariate_cooks_distance(X, Y):\n    \"\"\"\n    Compute multivariate Cook's distance per observation using case deletion.\n    D_i = (1/(m*p)) * tr( (Delta_i^T X^T X Delta_i) S_hat^{-1} ).\n    \"\"\"\n    n, p = X.shape\n    m = Y.shape[1]\n    B_hat, S_hat, XTX = fit_multivariate_ols(X, Y)\n    S_inv = np.linalg.inv(S_hat)\n    D = np.zeros(n)\n    for i in range(n):\n        # Delete observation i\n        X_minus_i = np.delete(X, i, axis=0)\n        Y_minus_i = np.delete(Y, i, axis=0)\n        # Refit\n        B_minus_i = np.linalg.pinv(X_minus_i) @ Y_minus_i\n        Delta = B_hat - B_minus_i  # p x m\n        M = Delta.T @ XTX @ Delta  # m x m\n        D[i] = (np.trace(M @ S_inv)) / (m * p)\n    return D\n\ndef univariate_cooks_distance(X, y):\n    \"\"\"\n    Compute univariate Cook's distance for a single response vector y.\n    Uses D_i = (e_i^2/(p*MSE)) * (h_ii/(1 - h_ii)^2).\n    \"\"\"\n    n, p = X.shape\n    # OLS fit\n    beta_hat = np.linalg.pinv(X) @ y\n    y_hat = X @ beta_hat\n    e = y - y_hat\n    # Hat matrix and leverages\n    H = X @ np.linalg.pinv(X)\n    h = np.clip(np.diag(H), 0.0, 1.0)\n    # Mean Squared Error\n    MSE = float(e.T @ e) / (n - p)\n    # Cook's distance\n    denom = (1.0 - h) ** 2\n    # Avoid division by zero with safe handling\n    safe = denom  1e-12\n    D = np.zeros(n)\n    D[safe] = ( (e[safe] ** 2) / (p * MSE) ) * ( h[safe] / denom[safe] )\n    # If denom is extremely small, set influence to a large number to reflect instability\n    D[~safe] = np.inf\n    return D\n\ndef generate_case(n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std):\n    \"\"\"\n    Generate a synthetic multiresponse regression case.\n    X: intercept + (p-1) standard normal features.\n    B: standard normal coefficients.\n    E: normal noise with std noise_std.\n    Apply leverage and response modifications at extreme_index.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    d_features = p - 1\n    X_features = rng.normal(0.0, 1.0, size=(n, d_features))\n    X = np.hstack([np.ones((n, 1)), X_features])\n    B = rng.normal(0.0, 1.0, size=(p, m))\n    # Noise\n    E = rng.normal(0.0, noise_std, size=(n, m))\n    # Apply leverage modification on X at extreme_index\n    if leverage_multiplier != 1.0:\n        X[extreme_index, 1:] *= leverage_multiplier\n    # Build Y consistent with X and noise\n    Y = X @ B + E\n    # Now apply response offset (extreme in all responses) at extreme_index\n    if response_offset != 0.0:\n        Y[extreme_index, :] += response_offset\n    return X, Y, B\n\ndef analyze_case(n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std):\n    \"\"\"\n    For a given case, compute multivariate Cook's distances and univariate counts.\n    Returns:\n      bool indicating if extreme_index is argmax of multivariate Cook's distance,\n      int count of univariate responses where extreme_index is argmax of Cook's distance.\n    \"\"\"\n    X, Y, _ = generate_case(n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std)\n    # Multivariate Cook's distances\n    D_multi = multivariate_cooks_distance(X, Y)\n    extreme_is_max_multi = int(np.argmax(D_multi)) == extreme_index\n    # Univariate Cook's distances per response\n    count_univariate_max = 0\n    for j in range(m):\n        D_uni = univariate_cooks_distance(X, Y[:, j])\n        if int(np.argmax(D_uni)) == extreme_index:\n            count_univariate_max += 1\n    return bool(extreme_is_max_multi), int(count_univariate_max)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (n, p, m, seed, extreme_index, leverage_multiplier, response_offset, noise_std)\n    test_cases = [\n        (40, 3, 3, 12345, 5, 1.0, 12.0, 1.0),   # Case 1: extreme residual, no leverage\n        (40, 3, 3, 12346, 10, 12.0, 0.0, 1.0),  # Case 2: high leverage, no residual extreme\n        (40, 3, 3, 12347, 20, 12.0, 12.0, 1.0), # Case 3: high leverage and extreme residual\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, m, seed, extreme_index, lev_mult, resp_off, noise_std = case\n        is_max_multi, uni_count = analyze_case(n, p, m, seed, extreme_index, lev_mult, resp_off, noise_std)\n        results.append(is_max_multi)\n        results.append(uni_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}