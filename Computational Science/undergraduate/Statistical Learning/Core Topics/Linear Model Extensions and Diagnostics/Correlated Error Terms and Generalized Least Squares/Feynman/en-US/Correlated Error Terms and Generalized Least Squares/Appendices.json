{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of Generalized Least Squares (GLS), it is crucial to first establish when it is truly necessary. This practice provides a computational exploration of the conditions under which the Ordinary Least Squares (OLS) estimator is identical to the GLS estimator. By implementing both estimators and comparing their results across various error covariance structures, you will verify the theoretical condition for their equivalence and observe how their estimates diverge as error correlations become more pronounced .",
            "id": "3112106",
            "problem": "You are given a linear model with possibly correlated error terms: $y = X \\beta + \\varepsilon$, where $X \\in \\mathbb{R}^{n \\times p}$ has full column rank, $y \\in \\mathbb{R}^{n}$, and the error vector $\\varepsilon$ has mean $0$ and covariance matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite. The Ordinary Least Squares (OLS) estimator is defined as the minimizer of the unweighted squared loss, and the Generalized Least Squares (GLS) estimator is defined as the minimizer of the squared loss weighted by the inverse covariance. Your task is to implement a program that, for a fixed dataset and a suite of covariance specifications, computes the Euclidean norm of the difference between the OLS and GLS estimators to assess when they coincide exactly and how sensitive GLS is to small deviations from exact-equality conditions.\n\nFundamental base for derivation and implementation:\n- The OLS estimator is the unique minimizer of the function $\\beta \\mapsto \\|y - X \\beta\\|_{2}^{2}$.\n- The GLS estimator is the unique minimizer of the function $\\beta \\mapsto (y - X \\beta)^{\\top} \\Sigma^{-1} (y - X \\beta)$.\n\nDataset (fixed across all test cases):\n- Sample size and number of features: $n = 6$, $p = 2$.\n- Design matrix:\n$$\nX = \\begin{bmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2 \\\\\n1  3 \\\\\n1  4 \\\\\n1  5\n\\end{bmatrix}.\n$$\n- Response vector:\n$$\ny = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n2 \\\\\n4 \\\\\n3 \\\\\n5\n\\end{bmatrix}.\n$$\n\nTest suite of covariance matrices $\\Sigma$:\n1. Exact-equality, spherical errors: $\\Sigma_{1} = \\sigma^{2} I$ with $\\sigma^{2} = 2.0$ and $I$ the $n \\times n$ identity matrix.\n2. Exact-equality, general condition: $\\Sigma_{2} = \\sigma^{2} I + X \\Lambda X^{\\top}$ with $\\sigma^{2} = 1.5$ and\n$$\n\\Lambda = \\begin{bmatrix}\n0.7  0.2 \\\\\n0.2  0.5\n\\end{bmatrix}.\n$$\n3. Near-equality, small perturbation orthogonal to $\\operatorname{col}(X)$: $\\Sigma_{3} = \\Sigma_{2} + \\epsilon \\, v v^{\\top}$ with $\\epsilon = 10^{-6}$ and\n$$\nv = \\begin{bmatrix}\n1 \\\\\n-2 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix},\n$$\nwhere $v$ is orthogonal to each column of $X$.\n4. Weak autoregressive correlation: $\\Sigma_{4}$ is the autoregressive of order $1$ (AR(1)) covariance with parameter $\\rho = 0.1$ and variance $\\sigma^{2} = 1.0$, i.e., entries $(\\Sigma_{4})_{ij} = \\sigma^{2} \\rho^{|i - j|}$ for all indices $i,j$.\n5. Strong autoregressive correlation: $\\Sigma_{5}$ is AR(1) with parameter $\\rho = 0.8$ and variance $\\sigma^{2} = 1.0$, i.e., entries $(\\Sigma_{5})_{ij} = \\sigma^{2} \\rho^{|i - j|}$ for all indices $i,j$.\n\nYour program must:\n- Compute the OLS estimator $\\hat{\\beta}_{\\mathrm{OLS}}$ as the unique minimizer of the unweighted squared loss.\n- For each $\\Sigma$ in the test suite, compute the GLS estimator $\\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma)$ as the unique minimizer of the $\\Sigma^{-1}$-weighted squared loss.\n- For each test case, compute the Euclidean norm of the difference $\\|\\hat{\\beta}_{\\mathrm{OLS}} - \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma)\\|_{2}$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to $8$ decimal places, in the order $\\Sigma_{1}, \\Sigma_{2}, \\Sigma_{3}, \\Sigma_{4}, \\Sigma_{5}$.\n\nDesign for coverage:\n- The case $\\Sigma_{1}$ tests the baseline condition where OLS and GLS coincide exactly.\n- The case $\\Sigma_{2}$ tests a non-trivial exact-equality condition.\n- The case $\\Sigma_{3}$ tests sensitivity to a tiny perturbation violating exact-equality conditions in a direction orthogonal to the columns of $X$.\n- The cases $\\Sigma_{4}$ and $\\Sigma_{5}$ test increasing deviations from equality under structured correlation (weak and strong AR(1)).\n\nFinal output format:\n- Your program should produce a single line of output containing the five results as a comma-separated list enclosed in square brackets (e.g., $[r_{1},r_{2},r_{3},r_{4},r_{5}]$), where each $r_{k}$ is a float rounded to $8$ decimal places. No other text must be printed.",
            "solution": "The user wants to compare the Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) estimators for a linear model by computing the Euclidean norm of their difference under various specifications of the error covariance matrix.\n\n### Step 1: Theoretical Formulation\n\nThe linear model is given by $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix of full column rank, $\\beta \\in \\mathbb{R}^{p}$ is the vector of coefficients, and $\\varepsilon \\in \\mathbb{R}^{n}$ is the error vector with mean $\\mathbb{E}[\\varepsilon] = 0$ and a symmetric positive definite covariance matrix $\\operatorname{Cov}(\\varepsilon) = \\Sigma$.\n\n**Ordinary Least Squares (OLS) Estimator**\n\nThe OLS estimator, $\\hat{\\beta}_{\\mathrm{OLS}}$, is found by minimizing the residual sum of squares (RSS):\n$$\n\\text{RSS}(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) = \\| y - X\\beta \\|_{2}^{2}\n$$\nTo find the minimum, we differentiate with respect to $\\beta$ and set the gradient to zero:\n$$\n\\frac{\\partial \\text{RSS}}{\\partial \\beta} = -2X^{\\top}(y - X\\beta) = 0\n$$\n$$\nX^{\\top}y - X^{\\top}X\\beta = 0 \\implies X^{\\top}X\\beta = X^{\\top}y\n$$\nSince $X$ is assumed to have full column rank, the matrix $X^{\\top}X$ is invertible. The unique OLS estimator is therefore:\n$$\n\\hat{\\beta}_{\\mathrm{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y\n$$\n\n**Generalized Least Squares (GLS) Estimator**\n\nThe GLS estimator, $\\hat{\\beta}_{\\mathrm{GLS}}$, accounts for the error covariance structure by minimizing a weighted residual sum of squares:\n$$\n\\text{RSS}_{\\Sigma}(\\beta) = (y - X\\beta)^{\\top}\\Sigma^{-1}(y - X\\beta)\n$$\nThis can be viewed as performing OLS on a transformed model. Since $\\Sigma$ is symmetric positive definite, its inverse $\\Sigma^{-1}$ is as well. We can find a matrix $L$ such that $\\Sigma^{-1} = L^{\\top}L$ (e.g., via Cholesky decomposition). The objective function becomes:\n$$\n\\text{RSS}_{\\Sigma}(\\beta) = (y - X\\beta)^{\\top}L^{\\top}L(y - X\\beta) = \\|L(y - X\\beta)\\|_{2}^{2} = \\|y' - X'\\beta\\|_{2}^{2}\n$$\nwhere $y' = Ly$ and $X' = LX$. The OLS solution for this transformed problem is:\n$$\n\\hat{\\beta}_{\\mathrm{GLS}} = ((X')^{\\top}X')^{-1}(X')^{\\top}y' = (X^{\\top}L^{\\top}LX)^{-1}X^{\\top}L^{\\top}Ly\n$$\nSubstituting back $\\Sigma^{-1} = L^{\\top}L$, we get the GLS estimator:\n$$\n\\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma) = (X^{\\top}\\Sigma^{-1}X)^{-1}X^{\\top}\\Sigma^{-1}y\n$$\n\n### Step 2: Condition for Equivalence of OLS and GLS\n\nThe OLS and GLS estimators are identical, i.e., $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}$, if and only if the column space of the design matrix, $\\operatorname{col}(X)$, is an invariant subspace of the covariance matrix $\\Sigma$. This means that for any vector $z \\in \\operatorname{col}(X)$, we must have $\\Sigma z \\in \\operatorname{col}(X)$. A practical way to state this condition is that there must exist a $p \\times p$ matrix $M$ such that $\\Sigma X = X M$.\n\nLet's examine the first three test cases with respect to this condition.\n1.  $\\Sigma_1 = \\sigma^2 I$: Here, $\\Sigma_1 X = (\\sigma^2 I) X = X(\\sigma^2 I_p)$. The condition holds with $M = \\sigma^2 I_p$, so $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_1)$.\n2.  $\\Sigma_2 = \\sigma^2 I + X \\Lambda X^\\top$: Here, $\\Sigma_2 X = (\\sigma^2 I + X \\Lambda X^\\top)X = \\sigma^2 X + X\\Lambda(X^\\top X) = X(\\sigma^2 I_p + \\Lambda(X^\\top X))$. The condition holds with $M = \\sigma^2 I_p + \\Lambda(X^\\top X)$, so $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_2)$.\n3.  $\\Sigma_3 = \\Sigma_2 + \\epsilon v v^\\top$: The vector $v$ is given to be orthogonal to each column of $X$, which implies $X^\\top v = 0$, and thus $v^\\top X = (X^\\top v)^\\top = 0$.\n    Then, $\\Sigma_3 X = (\\Sigma_2 + \\epsilon v v^\\top)X = \\Sigma_2 X + \\epsilon v(v^\\top X) = \\Sigma_2 X + 0 = \\Sigma_2 X$.\n    Since the invariance condition holds for $\\Sigma_2$, it must also hold for $\\Sigma_3$. Thus, we expect $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_3)$.\n\nFor cases $4$ and $5$, the autoregressive covariance structure does not generally satisfy this invariance property, so we expect a non-zero difference between the estimators.\n\n### Step 3: Computational Procedure\n\nThe solution is implemented by following these steps:\n1.  Define the given dataset, the design matrix $X$ and response vector $y$, as numerical arrays. The sample size is $n=6$ and the number of features is $p=2$.\n2.  Compute the OLS estimator $\\hat{\\beta}_{\\mathrm{OLS}}$ once using its closed-form formula. For numerical stability, the linear system $X^{\\top}X \\beta = X^{\\top}y$ is solved directly rather than by computing $(X^{\\top}X)^{-1}$ explicitly.\n3.  Iterate through the five specified test cases for the covariance matrix $\\Sigma$:\n    a. For each case, construct the corresponding $6 \\times 6$ matrix $\\Sigma_k$.\n    b. For cases $\\Sigma_4$ and $\\Sigma_5$, which have an AR($1$) structure, the covariance matrix is a symmetric Toeplitz matrix where $(\\Sigma)_{ij} = \\sigma^2 \\rho^{|i-j|}$.\n    c. Compute the GLS estimator $\\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_k)$ using its formula. Again, for stability, the linear system $(X^{\\top}\\Sigma_k^{-1}X)\\beta = X^{\\top}\\Sigma_k^{-1}y$ is solved directly.\n4.  For each case, calculate the Euclidean norm of the difference vector, $\\|\\hat{\\beta}_{\\mathrm{OLS}} - \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_k)\\|_{2}$.\n5.  Collect the five norms and format them into a single string as specified.\n\nThe theoretical analysis predicts the first three norms will be zero (or close to it, subject to floating-point precision limitations), while the last two will be non-zero, with the norm for $\\Sigma_5$ ($\\rho=0.8$) expected to be larger than that for $\\Sigma_4$ ($\\rho=0.1$).",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Computes the Euclidean norm of the difference between OLS and GLS estimators\n    for a given dataset and five different error covariance structures.\n    \"\"\"\n    # Fixed dataset\n    n, p = 6, 2\n    X = np.array([\n        [1.0, 0.0],\n        [1.0, 1.0],\n        [1.0, 2.0],\n        [1.0, 3.0],\n        [1.0, 4.0],\n        [1.0, 5.0]\n    ])\n    y = np.array([1.0, 2.0, 2.0, 4.0, 3.0, 5.0])\n\n    # Compute OLS estimator once\n    # Solve (X^T X) beta = X^T y\n    beta_ols = np.linalg.solve(X.T @ X, X.T @ y)\n\n    # --- Define the five covariance matrices ---\n\n    # Case 1: Spherical errors\n    sigma2_1 = 2.0\n    Sigma1 = sigma2_1 * np.identity(n)\n\n    # Case 2: General exact-equality condition\n    sigma2_2 = 1.5\n    Lambda = np.array([[0.7, 0.2], [0.2, 0.5]])\n    Sigma2 = sigma2_2 * np.identity(n) + X @ Lambda @ X.T\n\n    # Case 3: Near-equality, perturbation orthogonal to col(X)\n    epsilon = 1e-6\n    v = np.array([1.0, -2.0, 1.0, 0.0, 0.0, 0.0])\n    Sigma3 = Sigma2 + epsilon * np.outer(v, v)\n\n    # Case 4: Weak autoregressive correlation (AR(1))\n    rho4 = 0.1\n    sigma2_4 = 1.0\n    c4 = sigma2_4 * (rho4 ** np.arange(n))\n    Sigma4 = toeplitz(c4)\n\n    # Case 5: Strong autoregressive correlation (AR(1))\n    rho5 = 0.8\n    sigma2_5 = 1.0\n    c5 = sigma2_5 * (rho5 ** np.arange(n))\n    Sigma5 = toeplitz(c5)\n\n    # List of covariance matrices for iteration\n    sigmas = [Sigma1, Sigma2, Sigma3, Sigma4, Sigma5]\n    \n    results = []\n\n    # Iterate through each covariance matrix to compute GLS and the difference norm\n    for Sigma in sigmas:\n        # Compute GLS estimator\n        # Solve (X^T Sigma^-1 X) beta = X^T Sigma^-1 y\n        Sigma_inv = np.linalg.inv(Sigma)\n        XT_S_inv = X.T @ Sigma_inv\n        \n        A_gls = XT_S_inv @ X\n        b_gls = XT_S_inv @ y\n        \n        beta_gls = np.linalg.solve(A_gls, b_gls)\n        \n        # Compute the Euclidean norm of the difference\n        diff_norm = np.linalg.norm(beta_ols - beta_gls)\n        results.append(diff_norm)\n\n    # Format the final output string as per requirements\n    output_str = f\"[{','.join([f'{r:.8f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n\n```"
        },
        {
            "introduction": "While the presence of correlated errors suggests that GLS is more efficient than OLS, the magnitude of this efficiency gain is not constant. This practice delves into a more nuanced aspect of GLS: the critical interplay between the error covariance matrix $\\Sigma$ and the design matrix $X$. By constructing scenarios with orthogonal and nearly collinear predictors, you will investigate how the structure of your experimental design can either minimize or maximize the performance benefits of applying GLS, offering valuable insight into study design under correlated errors .",
            "id": "3112084",
            "problem": "Consider the linear regression model with correlated errors, defined by $y = X\\beta + \\varepsilon$, where $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix of fixed regressors, $\\beta \\in \\mathbb{R}^p$ is the coefficient vector, and $\\varepsilon \\in \\mathbb{R}^n$ is a mean-zero error vector with covariance matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$. Starting solely from these definitions, and from the facts that linear transformations of random vectors obey $\\operatorname{Cov}(A z) = A\\,\\operatorname{Cov}(z)\\,A^\\top$ for any compatible matrix $A$, and that matrix inverses and transposes obey standard linear algebra properties, derive expressions for the covariance matrices of the Ordinary Least Squares (OLS) estimator and the Generalized Least Squares (GLS) estimator, and use them to quantify the relative efficiency of GLS versus OLS for several fixed designs $X$ under a fixed correlated error covariance $\\Sigma$. Generalized Least Squares (GLS) refers to estimation under the known error covariance $\\Sigma$, while Ordinary Least Squares (OLS) refers to estimation that ignores $\\Sigma$.\n\nDefine the scalar efficiency of GLS relative to OLS for a given design $X$ and error covariance $\\Sigma$ as the ratio of the traces of the two estimator covariance matrices, namely $\\operatorname{Eff}(X,\\Sigma) = \\dfrac{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}})\\big)}{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}})\\big)}$, where $\\operatorname{tr}(\\cdot)$ denotes the trace of a square matrix. This quantity captures the average variance per parameter: values close to $1$ indicate minimal gains, while larger values indicate greater efficiency gains of GLS over OLS.\n\nYour task is to implement a program that, for a fixed correlated error covariance $\\Sigma$, constructs several design matrices $X$ that exemplify minimal and maximal GLS gains, computes $\\operatorname{Eff}(X,\\Sigma)$ for each, and reports the results. You must design $X$ to make GLS gains minimal by using orthogonal columns, and maximal by using near collinearity, all under the same $\\Sigma$. Additionally, include a boundary case where errors are uncorrelated to validate your implementation.\n\nUse the following test suite:\n\n- Let $n = 8$ and $p = 2$. For all cases except the boundary case, fix $\\Sigma$ to be the first-order autoregressive covariance with parameter $\\rho = 0.8$ and innovation variance $1$, i.e., $\\Sigma \\in \\mathbb{R}^{8 \\times 8}$ with entries $\\Sigma_{ij} = \\rho^{|i-j|}$ for all $i,j \\in \\{1,\\dots,8\\}$ and $\\rho = 0.8$.\n- For each case, let the first column of $X$ be the intercept vector $\\mathbf{1}_8 = [1,1,1,1,1,1,1,1]^\\top$ (eight ones), and the second column be $x_1$ specified per case as follows:\n  1. Case A (orthogonal columns, minimal gain target): $x_1 = [-3,-2,-1,0,1,2,3,0]^\\top$, which satisfies $\\mathbf{1}_8^\\top x_1 = 0$ and thus is orthogonal to the intercept in the Euclidean inner product.\n  2. Case B (near collinearity, maximal gain target): $x_1 = \\mathbf{1}_8 + 10^{-3} \\cdot [-3,-2,-1,0,1,2,3,0]^\\top$, which makes the second column nearly proportional to the intercept.\n  3. Case C (moderate collinearity): $x_1 = \\mathbf{1}_8 + 10^{-1} \\cdot [-3,-2,-1,0,1,2,3,0]^\\top$, which provides moderate correlation with the intercept.\n  4. Case D (boundary check with uncorrelated errors): use the same $X$ as in Case B, but set $\\Sigma = I_8$, the $8 \\times 8$ identity matrix, corresponding to $\\rho = 0$.\n\nFor each case, compute the scalar efficiency $\\operatorname{Eff}(X,\\Sigma)$ as defined above. Round each result to $6$ decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the above cases, i.e., $[\\operatorname{Eff}_A,\\operatorname{Eff}_B,\\operatorname{Eff}_C,\\operatorname{Eff}_D]$, where each entry is a float rounded to $6$ decimal places. No additional text or spaces should be printed.",
            "solution": "The problem requires the derivation of the covariance matrices for the Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) estimators and their use to compute a relative efficiency metric for several specified scenarios.\n\nFirst, we establish the theoretical foundation by deriving the necessary covariance matrices from the provided definitions. The linear regression model is $y = X\\beta + \\varepsilon$, where $y \\in \\mathbb{R}^n$, $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^p$, and $\\varepsilon \\in \\mathbb{R}^n$ is the error term with $E[\\varepsilon] = 0$ and $\\operatorname{Cov}(\\varepsilon) = \\Sigma$.\n\n**Derivation of the OLS Estimator Covariance Matrix**\n\nThe OLS estimator is defined as $\\hat{\\beta}_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top y$. It is derived under the assumption that $\\operatorname{Cov}(\\varepsilon) = \\sigma^2 I$, but here we evaluate its performance under the true, more general covariance structure $\\Sigma$. Substituting the model equation for $y$:\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top (X\\beta + \\varepsilon) = (X^\\top X)^{-1} X^\\top X\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon = \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon\n$$\nThe estimator is unbiased, since $E[\\hat{\\beta}_{\\text{OLS}}] = E[\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon] = \\beta + (X^\\top X)^{-1} X^\\top E[\\varepsilon] = \\beta$. The covariance matrix of $\\hat{\\beta}_{\\text{OLS}}$ is:\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\operatorname{Cov}(\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon) = \\operatorname{Cov}((X^\\top X)^{-1} X^\\top \\varepsilon)\n$$\nUsing the given rule for linear transformations of random vectors, $\\operatorname{Cov}(Az) = A\\operatorname{Cov}(z)A^\\top$, we let $A = (X^\\top X)^{-1} X^\\top$ and $z = \\varepsilon$. This gives:\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\left( (X^\\top X)^{-1} X^\\top \\right) \\operatorname{Cov}(\\varepsilon) \\left( (X^\\top X)^{-1} X^\\top \\right)^\\top\n$$\nSubstituting $\\operatorname{Cov}(\\varepsilon) = \\Sigma$ and simplifying the transpose term:\n$$\n\\left( (X^\\top X)^{-1} X^\\top \\right)^\\top = (X^\\top)^\\top \\left((X^\\top X)^{-1}\\right)^\\top = X \\left((X^\\top X)^\\top\\right)^{-1} = X(X^\\top X)^{-1}\n$$\nThe last step uses the fact that $X^\\top X$ is a symmetric matrix. Thus, the covariance matrix for the OLS estimator is:\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}}) = (X^\\top X)^{-1} X^\\top \\Sigma X (X^\\top X)^{-1}\n$$\n\n**Derivation of the GLS Estimator Covariance Matrix**\n\nThe GLS estimator, which accounts for the known covariance $\\Sigma$, is defined as $\\hat{\\beta}_{\\text{GLS}} = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} y$. Following a similar procedure, we substitute the model for $y$:\n$$\n\\hat{\\beta}_{\\text{GLS}} = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} (X\\beta + \\varepsilon) = \\beta + (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} \\varepsilon\n$$\nThe GLS estimator is also unbiased, as $E[\\hat{\\beta}_{\\text{GLS}}] = \\beta$. Its covariance is:\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = \\operatorname{Cov}\\left( (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} \\varepsilon \\right)\n$$\nLetting $B = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}$ and using $\\operatorname{Cov}(Bz) = B\\operatorname{Cov}(z)B^\\top$:\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = B \\Sigma B^\\top = \\left((X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}\\right) \\Sigma \\left((X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}\\right)^\\top\n$$\nThe transpose term is $\\left(\\Sigma^{-1}\\right)^\\top (X^\\top)^\\top \\left((X^\\top \\Sigma^{-1} X)^{-1}\\right)^\\top = \\Sigma^{-1} X (X^\\top \\Sigma^{-1} X)^{-1}$, since $\\Sigma$ and $X^\\top\\Sigma^{-1}X$ are symmetric. Substituting back:\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} \\Sigma \\Sigma^{-1} X (X^\\top \\Sigma^{-1} X)^{-1}\n$$\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = (X^\\top \\Sigma^{-1} X)^{-1} (X^\\top \\Sigma^{-1} X) (X^\\top \\Sigma^{-1} X)^{-1} = (X^\\top \\Sigma^{-1} X)^{-1}\n$$\nThis is the standard result for the covariance of the GLS estimator.\n\n**Relative Efficiency and Implementation Plan**\n\nThe scalar relative efficiency is given by the ratio of the traces of these two covariance matrices:\n$$\n\\operatorname{Eff}(X,\\Sigma) = \\dfrac{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}})\\big)}{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}})\\big)} = \\dfrac{\\operatorname{tr}\\left( (X^\\top X)^{-1} X^\\top \\Sigma X (X^\\top X)^{-1} \\right)}{\\operatorname{tr}\\left( (X^\\top \\Sigma^{-1} X)^{-1} \\right)}\n$$\nThe implementation will proceed by first defining a Python function to compute this quantity for a given design matrix $X$ and covariance matrix $\\Sigma$. This function will use the `numpy` library for all linear algebra operations, including matrix multiplication, transpose, inversion, and trace computation.\n\nFor each of the four specified cases, we will construct the `numpy` arrays for the corresponding $X$ and $\\Sigma$. The autoregressive covariance matrix $\\Sigma_{ij} = \\rho^{|i-j|}$ will be built for $\\rho=0.8$ and $n=8$. The design matrices $X$ will be constructed with an intercept column and a second column as specified for each case (orthogonal, near-collinear, moderately collinear). The boundary case uses an identity matrix for $\\Sigma$.\n\nThe efficiency will be calculated for each of the four cases, with the boundary case (Case D) expected to yield an efficiency of $1.0$, as when errors are uncorrelated ($\\Sigma=\\sigma^2I$), OLS is the Best Linear Unbiased Estimator (BLUE) and is identical to GLS. This serves as a critical validation of the derived formulae and the implementation. The final results are then rounded and formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_efficiency(X, Sigma):\n    \"\"\"\n    Computes the relative efficiency of GLS vs. OLS.\n\n    Args:\n        X (np.ndarray): The design matrix of shape (n, p).\n        Sigma (np.ndarray): The error covariance matrix of shape (n, n).\n\n    Returns:\n        float: The scalar efficiency Eff(X, Sigma).\n    \"\"\"\n    # Numerator: Trace of the OLS estimator's covariance matrix\n    # Cov(b_OLS) = (X'X)^-1 X' Sigma X (X'X)^-1\n    try:\n        XT = X.T\n        XTX_inv = np.linalg.inv(XT @ X)\n        cov_ols = XTX_inv @ XT @ Sigma @ X @ XTX_inv\n        tr_cov_ols = np.trace(cov_ols)\n    except np.linalg.LinAlgError:\n        # This can happen if X is not full rank, though not expected\n        # for the given test cases.\n        return np.nan\n\n    # Denominator: Trace of the GLS estimator's covariance matrix\n    # Cov(b_GLS) = (X' Sigma^-1 X)^-1\n    try:\n        Sigma_inv = np.linalg.inv(Sigma)\n        cov_gls = np.linalg.inv(XT @ Sigma_inv @ X)\n        tr_cov_gls = np.trace(cov_gls)\n    except np.linalg.LinAlgError:\n        # This can happen if Sigma is singular or X' Sigma^-1 X is singular.\n        # Not expected for the given test cases.\n        return np.nan\n\n    # The efficiency ratio\n    if tr_cov_gls == 0:\n        return np.inf if tr_cov_ols  0 else np.nan\n    \n    return tr_cov_ols / tr_cov_gls\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the GLS vs. OLS efficiency for four cases.\n    \"\"\"\n    # Define problem parameters\n    n = 8\n    rho = 0.8\n    \n    # Construct AR(1) covariance matrix for Cases A, B, C\n    # Sigma_ij = rho^|i-j|\n    indices = np.arange(n)\n    Sigma_AR1 = rho**np.abs(indices[:, np.newaxis] - indices)\n\n    # Construct identity covariance matrix for Case D\n    Sigma_I = np.identity(n)\n\n    # Common vectors for constructing design matrices\n    intercept = np.ones(n)\n    v = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 0.0])\n\n    # --- Case A (orthogonal columns) ---\n    x1_A = v\n    X_A = np.column_stack((intercept, x1_A))\n    eff_A = compute_efficiency(X_A, Sigma_AR1)\n\n    # --- Case B (near collinearity) ---\n    x1_B = intercept + 1e-3 * v\n    X_B = np.column_stack((intercept, x1_B))\n    eff_B = compute_efficiency(X_B, Sigma_AR1)\n    \n    # --- Case C (moderate collinearity) ---\n    x1_C = intercept + 1e-1 * v\n    X_C = np.column_stack((intercept, x1_C))\n    eff_C = compute_efficiency(X_C, Sigma_AR1)\n\n    # --- Case D (boundary check with uncorrelated errors) ---\n    # Use the same X as in Case B but with Sigma = I\n    X_D = X_B\n    eff_D = compute_efficiency(X_D, Sigma_I)\n\n    results = [eff_A, eff_B, eff_C, eff_D]\n    \n    # Format the output as specified\n    # The requirement is rounding to 6 decimal places.\n    # The f-string format specifier '{:.6f}' correctly handles this rounding.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "In many large-scale applications, such as network analysis or time series modeling, the error correlation possesses a specific structure that can be exploited for computational efficiency. This final practice bridges the gap between GLS theory and practical, scalable implementation. You will learn to handle a case where the inverse covariance (precision) matrix $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$ is sparse and tridiagonal, and you will implement an efficient GLS estimator using a specialized sparse Cholesky factorization, a technique essential for applying GLS to high-dimensional problems .",
            "id": "3112081",
            "problem": "You are given a linear model with correlated errors defined on a path network of nodes. The outcome vector is denoted by $\\mathbf{y} \\in \\mathbb{R}^n$, the design matrix is denoted by $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, and the error vector is denoted by $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$. The model is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, with $E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$ and $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\boldsymbol{\\Sigma}$. Errors are correlated due to shared links on a path graph; this correlation is encoded via a sparse precision (inverse covariance) matrix $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$.\n\nStart from the following fundamental base:\n- The linear model definition $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ with zero-mean errors.\n- The assumption that $\\boldsymbol{\\varepsilon}$ follows a multivariate normal distribution with covariance $\\boldsymbol{\\Sigma}$, so that the log-likelihood (up to an additive constant) is proportional to the negative of the quadratic form $(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\boldsymbol{\\Sigma}^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})$.\n- Sparse linear algebra principles for banded matrices and Cholesky factorization for symmetric positive definite matrices.\n\nNetwork-induced error correlation model. Consider a path graph of $n$ nodes, where the precision matrix $\\mathbf{Q}$ is defined as\n$$\n\\mathbf{Q} = \\alpha \\mathbf{I}_n + w \\mathbf{L}_{\\text{path}},\n$$\nwhere $\\alpha  0$, $w \\ge 0$, $\\mathbf{I}_n$ is the $n \\times n$ identity matrix, and $\\mathbf{L}_{\\text{path}}$ is the graph Laplacian of a path: $\\mathbf{L}_{\\text{path}}$ has diagonal entries equal to $1$ at the two endpoints and $2$ at interior nodes, and sub- and super-diagonal entries equal to $-1$ for adjacent nodes, with all other entries equal to $0$. This makes $\\mathbf{Q}$ symmetric positive definite and tridiagonal. Your task is to use this structure to implement Generalized Least Squares (GLS) efficiently by exploiting sparsity via a sparse Cholesky routine tailored to tridiagonal matrices.\n\nRequirements:\n- Derive, from the stated base only (do not use any shortcut estimator expressions provided to you), the computational steps to obtain the estimator for $\\boldsymbol{\\beta}$ that minimizes the appropriate quadratic form induced by the precision matrix $\\mathbf{Q}$.\n- Implement a banded (tridiagonal) sparse Cholesky factorization for $\\mathbf{Q}$. That is, compute a lower bidiagonal $\\mathbf{L}$ such that $\\mathbf{Q} = \\mathbf{L}\\mathbf{L}^\\top$, using only the $O(n)$ recurrences valid for symmetric positive definite tridiagonal matrices.\n- Use the factorization to transform the problem into an ordinary least squares problem on appropriately whitened variables, and solve for $\\boldsymbol{\\beta}$.\n- You must not rely on dense Cholesky routines. Your implementation must explicitly exploit the tridiagonal sparsity of $\\mathbf{Q}$.\n\nTest suite. For each test case below, construct $\\mathbf{Q}$ via the given $\\alpha$ and $w$, perform the sparse Cholesky-based GLS, and return the estimated coefficient vector $\\widehat{\\boldsymbol{\\beta}}$, rounded to $6$ decimal places.\n\n- Test Case A (happy path):\n  - $n = 5$, $\\alpha = 1.0$, $w = 0.4$.\n  - $\\mathbf{X} \\in \\mathbb{R}^{5 \\times 2}$ with rows $\\big([1,0],[1,1],[1,2],[1,3],[1,4]\\big)$.\n  - $\\mathbf{y} = [1.0, 2.2, 2.0, 3.6, 5.1]^\\top$.\n- Test Case B (boundary, uncorrelated errors):\n  - $n = 5$, $\\alpha = 1.0$, $w = 0.0$.\n  - Same $\\mathbf{X}$ and $\\mathbf{y}$ as in Test Case A.\n- Test Case C (stronger correlation and higher dimension):\n  - $n = 6$, $\\alpha = 0.5$, $w = 1.0$.\n  - $\\mathbf{X} \\in \\mathbb{R}^{6 \\times 3}$ with rows $\\big([1,0,0],[1,1,1],[1,2,0],[1,3,1],[1,4,0],[1,5,1]\\big)$.\n  - $\\mathbf{y} = [0.5, 1.6, 2.1, 3.2, 3.7, 4.8]^\\top$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list of the estimated coefficients for one test case in the order A, B, C. Each coefficient must be rounded to $6$ decimal places. For example, a valid output shape is similar to $[[b_{A,1},b_{A,2}], [b_{B,1},b_{B,2}], [b_{C,1},b_{C,2},b_{C,3}]]$ with the actual numeric values substituted.\n- There are no physical units or angles in this problem.\n- All answers must be numbers (floating-point), and the single line must be exactly the aggregate list as described, with no extra text.",
            "solution": "The problem requires the calculation of the Generalized Least Squares (GLS) estimator for a linear model where the errors exhibit correlation defined by a specific tridiagonal precision matrix $\\mathbf{Q}$. The solution must be derived from fundamental principles and must exploit the sparse structure of $\\mathbf{Q}$ through a custom sparse Cholesky factorization.\n\nThe linear model is given by $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y} \\in \\mathbb{R}^n$ is the outcome vector, $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the coefficient vector to be estimated, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$ is the error vector. The errors are assumed to follow a multivariate normal distribution, $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$, with mean $\\mathbf{0}$ and covariance matrix $\\boldsymbol{\\Sigma}$. The precision matrix is defined as $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$.\n\nThe probability density function of $\\boldsymbol{\\varepsilon}$ is proportional to $\\exp\\left(-\\frac{1}{2}\\boldsymbol{\\varepsilon}^\\top \\mathbf{Q} \\boldsymbol{\\varepsilon}\\right)$. Substituting $\\boldsymbol{\\varepsilon} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}$, the log-likelihood of the data, up to an additive constant, is proportional to $-\\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\mathbf{Q} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$. The maximum likelihood estimator $\\widehat{\\boldsymbol{\\beta}}$ is the vector that minimizes the quadratic form, which is the GLS objective function:\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\mathbf{Q} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\nThe problem specifies that the precision matrix $\\mathbf{Q}$ is symmetric and positive definite, which allows for a unique Cholesky decomposition $\\mathbf{Q} = \\mathbf{L}\\mathbf{L}^\\top$, where $\\mathbf{L}$ is a real, lower triangular matrix. Substituting this into the objective function:\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{L}\\mathbf{L}^\\top) (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\left(\\mathbf{L}^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\right)^\\top \\left(\\mathbf{L}^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\right)\n$$\nThis can be rewritten as a standard sum-of-squares. Let us define the \"whitened\" variables by pre-multiplying by $\\mathbf{L}^\\top$:\n$$\n\\mathbf{y}' = \\mathbf{L}^\\top\\mathbf{y}\n$$\n$$\n\\mathbf{X}' = \\mathbf{L}^\\top\\mathbf{X}\n$$\nThe objective function then becomes the Ordinary Least Squares (OLS) objective for the transformed variables:\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta})^\\top (\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta}) = ||\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta}||_2^2\n$$\nThe solution to this OLS problem, $\\widehat{\\boldsymbol{\\beta}}$, is found by solving the normal equations:\n$$\n(\\mathbf{X}')^\\top \\mathbf{X}' \\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}')^\\top \\mathbf{y}'\n$$\nThe computational strategy is therefore to:\n$1$. Construct the precision matrix $\\mathbf{Q}$.\n$2$. Compute its Cholesky factor $\\mathbf{L}$.\n$3$. Use $\\mathbf{L}$ to whiten the data $\\mathbf{y}$ and $\\mathbf{X}$.\n$4$. Solve the resulting OLS problem.\n\nThe key to an efficient implementation lies in exploiting the specific structure of $\\mathbf{Q}$. It is given as $\\mathbf{Q} = \\alpha \\mathbf{I}_n + w \\mathbf{L}_{\\text{path}}$, where $\\alpha  0$, $w \\ge 0$, and $\\mathbf{L}_{\\text{path}}$ is the Laplacian of a path graph. This makes $\\mathbf{Q}$ a symmetric tridiagonal matrix.\nLet the diagonal of $\\mathbf{Q}$ be $d_0, d_1, \\dots, d_{n-1}$ and its sub-diagonal be $e_0, e_1, \\dots, e_{n-2}$. From the definition of $\\mathbf{L}_{\\text{path}}$:\n- Diagonal: $d_i = \\alpha + 2w$ for $i \\in \\{1, \\dots, n-2\\}$, and $d_0 = d_{n-1} = \\alpha + w$.\n- Sub-diagonal: $e_i = -w$ for $i \\in \\{0, \\dots, n-2\\}$.\n\nThe Cholesky factor $\\mathbf{L}$ of a symmetric tridiagonal matrix is a lower bidiagonal matrix. Let its diagonal be $l_0, \\dots, l_{n-1}$ and its sub-diagonal be $m_0, \\dots, m_{n-2}$. Equating the elements of $\\mathbf{Q}$ with those of $\\mathbf{L}\\mathbf{L}^\\top$:\n$$\n\\mathbf{Q}_{i,i} = d_i = l_i^2 + m_{i-1}^2 \\quad (\\text{with } m_{-1}=0)\n$$\n$$\n\\mathbf{Q}_{i,i-1} = e_{i-1} = l_{i-1}m_{i-1}\n$$\nThis leads to the following $O(n)$ recurrence relations for the elements of $\\mathbf{L}$:\n$1$. $l_0 = \\sqrt{d_0}$\n$2$. For $i = 1, \\dots, n-1$:\n   a. $m_{i-1} = e_{i-1} / l_{i-1}$\n   b. $l_i = \\sqrt{d_i - m_{i-1}^2}$\nThis constitutes a sparse Cholesky factorization.\n\nNext, the whitening step $\\mathbf{v}' = \\mathbf{L}^\\top \\mathbf{v}$ must be performed efficiently. The matrix $\\mathbf{L}^\\top$ is upper bidiagonal, with diagonal $l_i$ and super-diagonal $(\\mathbf{L}^\\top)_{i, i+1} = m_i$. The multiplication can be carried out in $O(n)$ time without forming dense matrices:\n$$\nv'_i = l_i v_i + m_i v_{i+1} \\quad \\text{for } i = 0, \\dots, n-2\n$$\n$$\nv'_{n-1} = l_{n-1} v_{n-1}\n$$\nThis sparse multiplication is applied to compute $\\mathbf{y}' = \\mathbf{L}^\\top \\mathbf{y}$ and each column of $\\mathbf{X}' = \\mathbf{L}^\\top \\mathbf{X}$. The total cost for whitening is $O(np)$.\n\nFinally, the $p \\times p$ system of normal equations $(\\mathbf{X}')^\\top \\mathbf{X}' \\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}')^\\top \\mathbf{y}'$ is constructed and solved for $\\widehat{\\boldsymbol{\\beta}}$, typically using a standard linear system solver. This entire procedure avoids forming dense $n \\times n$ matrices and is computationally efficient.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    def perform_gls(n, alpha, w, X, y):\n        \"\"\"\n        Performs Generalized Least Squares using sparse Cholesky factorization.\n\n        Args:\n            n (int): Number of observations.\n            alpha (float): Parameter for the precision matrix.\n            w (float): Parameter for the precision matrix.\n            X (np.ndarray): Design matrix of shape (n, p).\n            y (np.ndarray): Outcome vector of shape (n,).\n\n        Returns:\n            np.ndarray: The estimated coefficient vector beta_hat.\n        \"\"\"\n        # Step 1: Construct the diagonal and sub-diagonal of the tridiagonal precision matrix Q.\n        # Q = alpha * I + w * L_path\n        q_diag = np.full(n, alpha + 2 * w)\n        if n  0:\n            q_diag[0] = alpha + w\n            q_diag[-1] = alpha + w\n        q_subdiag = np.full(n - 1, -w)\n\n        # Step 2: Perform sparse Cholesky factorization Q = LL^T.\n        # L is a lower bidiagonal matrix with diagonal l_diag and sub-diagonal m_subdiag.\n        l_diag = np.zeros(n)\n        m_subdiag = np.zeros(n - 1)\n\n        if n  0:\n            l_diag[0] = np.sqrt(q_diag[0])\n            for i in range(1, n):\n                m_subdiag[i - 1] = q_subdiag[i - 1] / l_diag[i - 1]\n                # The argument to sqrt is guaranteed to be positive because Q is positive definite.\n                l_diag[i] = np.sqrt(q_diag[i] - m_subdiag[i - 1]**2)\n\n        # Step 3: Whiten the variables using L^T.\n        # L^T is an upper bidiagonal matrix.\n        # y_prime = L^T * y and X_prime = L^T * X.\n        y_prime = np.zeros(n)\n        X_prime = np.zeros_like(X, dtype=float)\n        p = X.shape[1]\n\n        # Whiten y\n        if n  0:\n            for i in range(n - 1):\n                y_prime[i] = l_diag[i] * y[i] + m_subdiag[i] * y[i + 1]\n            y_prime[n - 1] = l_diag[n - 1] * y[n - 1]\n\n        # Whiten X column by column\n        if n  0:\n            for j in range(p):\n                x_col = X[:, j]\n                x_prime_col = np.zeros(n)\n                for i in range(n - 1):\n                    x_prime_col[i] = l_diag[i] * x_col[i] + m_subdiag[i] * x_col[i + 1]\n                x_prime_col[n - 1] = l_diag[n - 1] * x_col[n - 1]\n                X_prime[:, j] = x_prime_col\n        \n        # Step 4: Solve the OLS problem for the whitened variables.\n        # (X_prime^T * X_prime) * beta_hat = X_prime^T * y_prime\n        A_ols = X_prime.T @ X_prime\n        b_ols = X_prime.T @ y_prime\n        beta_hat = np.linalg.solve(A_ols, b_ols)\n\n        return beta_hat\n\n    test_cases = [\n        # Test Case A\n        {\n            \"n\": 5, \"alpha\": 1.0, \"w\": 0.4,\n            \"X\": np.array([[1, 0], [1, 1], [1, 2], [1, 3], [1, 4]]),\n            \"y\": np.array([1.0, 2.2, 2.0, 3.6, 5.1]),\n        },\n        # Test Case B\n        {\n            \"n\": 5, \"alpha\": 1.0, \"w\": 0.0,\n            \"X\": np.array([[1, 0], [1, 1], [1, 2], [1, 3], [1, 4]]),\n            \"y\": np.array([1.0, 2.2, 2.0, 3.6, 5.1]),\n        },\n        # Test Case C\n        {\n            \"n\": 6, \"alpha\": 0.5, \"w\": 1.0,\n            \"X\": np.array([[1, 0, 0], [1, 1, 1], [1, 2, 0], [1, 3, 1], [1, 4, 0], [1, 5, 1]]),\n            \"y\": np.array([0.5, 1.6, 2.1, 3.2, 3.7, 4.8]),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        beta_hat = perform_gls(case[\"n\"], case[\"alpha\"], case[\"w\"], case[\"X\"], case[\"y\"])\n        # Round to 6 decimal places and convert to a list for formatting.\n        results.append(np.round(beta_hat, 6).tolist())\n\n    # Format the output as a list of lists.\n    # The f-string calls str() on each element, which correctly formats the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}