{
    "hands_on_practices": [
        {
            "introduction": "在构建包含交互项的模型时，一个常见的挑战是多重共线性。当预测变量 $x_1$ 和 $x_2$ 本身相关时，它们与交互项 $x_1x_2$ 之间的共线性会变得更加严重，这会增大交互项系数估计值的不确定性。本次实践将通过一个可控的模拟实验，让你亲手验证并量化这一效应，直观地感受预测变量间的相关性如何影响我们对交互作用的推断精度。",
            "id": "3132247",
            "problem": "要求您通过受控模拟来检验，在正态线性模型中，预测变量之间的相关性如何增大了交互项估计系数的不确定性。构建一个程序，该程序针对一组指定的测试用例，在不同的预测变量相关性下，估计交互项系数的双侧置信区间的经验覆盖率和平均长度。该问题以纯数学术语提出，需要使用正态线性模型、普通最小二乘法 (OLS) 以及基于学生t分布的小样本推断等基本原理。\n\n考虑正态线性模型\n$$\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} (x_1 x_2) + \\varepsilon,\n$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 且独立于预测变量。对于每次模拟重复，将预测变量生成为一个相关的二元正态向量\n$$\n\\begin{pmatrix}\nx_1 \\\\\nx_2\n\\end{pmatrix}\n\\sim \\mathcal{N}\\!\\left(\n\\begin{pmatrix}\n\\mu_1 \\\\\n\\mu_2\n\\end{pmatrix},\n\\begin{pmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{pmatrix}\n\\right)\n$$\n其均值固定为 $\\mu_1 = \\mu_2 = 1$，相关系数为 $\\rho \\in (-1,1)$。使用固定参数 $\\beta_0 = 0$、$\\beta_1 = 1$、$\\beta_2 = 1$、$\\beta_{12} = 0.5$ 和 $\\sigma = 1$ 的模型生成响应。\n\n您的程序必须为每个测试用例执行包含 $R$ 次独立重复的蒙特卡洛模拟。在每次重复中：从指定的分布中抽取一个大小为 $n$ 的样本用于 $(x_1,x_2)$ 和 $\\varepsilon$，拟合包含截距、主效应 $x_1$、$x_2$ 以及交互项 $x_1 x_2$ 的OLS模型，并使用自由度为 $n-p$（其中 $p=4$ 是包括截距在内的回归系数数量）的学生t分布，为 $\\beta_{12}$ 构建一个名义水平为 $1-\\alpha = 0.95$ 的双侧置信区间。记录真实的 $\\beta_{12}$ 是否包含在区间内（若是，则覆盖指示符为 $1$，否则为 $0$）以及区间长度（一个非负实数）。在 $R$ 次重复之后，为每个测试用例报告：\n- 经验覆盖概率，以小数表示（覆盖指示符的平均值），以及\n- 平均区间长度（重复实验中区间长度的平均值）。\n\n对所有测试用例使用以下固定值：\n- $R = 500$,\n- $\\alpha = 0.05$,\n- $\\beta_0 = 0$, $\\beta_1 = 1$, $\\beta_2 = 1$, $\\beta_{12} = 0.5$,\n- $\\sigma = 1$,\n- $\\mu_1 = 1$, $\\mu_2 = 1$,\n- 为保证可复现性，随机种子固定为 $123456$。\n\n测试套件：\n1. $(n,\\rho) = (200, 0.0)$,\n2. $(n,\\rho) = (200, 0.9)$,\n3. $(n,\\rho) = (200, -0.9)$,\n4. $(n,\\rho) = (60, 0.99)$.\n\n输出规格：\n- 对于列表中的每个测试用例，按顺序输出一个双元素列表 $[c,\\ell]$，其中 $c$ 是经验覆盖概率，$\\ell$ 是平均置信区间长度。$c$ 和 $\\ell$ 都必须四舍五入到 $3$ 位小数。\n- 您的程序应生成单行输出，其中包含一个以方括号括起来的、逗号分隔的结果列表，每个元素本身都是相同格式的双元素列表，例如：\n$[[c_1,\\ell_1],[c_2,\\ell_2],[c_3,\\ell_3],[c_4,\\ell_4]]$。\n- 此问题中没有物理单位或角度。覆盖率必须以小数表示，不得使用百分号。\n\n科学真实性和推导基础：\n- 仅使用正态线性模型、OLS估计量和小样本t-置信区间的标准性质作为基本依据。除这些基础知识外，不要假设或使用任何快捷公式。\n- 确保所有随机数生成都由固定的种子控制，以便输出是可复现的。\n\n您的程序必须是一个完整的、可运行的脚本，不需要用户输入，也不需要外部文件，并且必须严格遵守指定的输出格式。",
            "solution": "该问题是有效的。这是一个在计算统计学领域中定义明确且具有科学依据的练习，旨在研究在标准正态线性模型框架内，多重共线性对交互项推断的影响。所有必需的参数、常数和程序都已明确指定。\n\n目标是进行一次蒙特卡洛模拟，以量化两个预测变量 $x_1$ 和 $x_2$ 之间的相关性 $\\rho$ 如何影响交互项系数 $\\beta_{12}$ 的置信区间的经验覆盖率和平均长度。\n\n其统计基础是正态线性模型，其矩阵形式表示为：\n$$\ny = X\\beta + \\varepsilon\n$$\n其中 $y$ 是一个 $n \\times 1$ 的观测向量，$X$ 是 $n \\times p$ 的设计矩阵，$\\beta$ 是 $p \\times 1$ 的系数向量，$\\varepsilon$ 是 $n \\times 1$ 的未观测误差向量。对于这个问题，模型是 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} (x_1 x_2) + \\varepsilon$，所以参数数量为 $p=4$。系数向量为 $\\beta = [\\beta_0, \\beta_1, \\beta_2, \\beta_{12}]^T$。设计矩阵 $X$ 的第 $i$ 行是 $[1, x_{i1}, x_{i2}, x_{i1}x_{i2}]$。误差 $\\varepsilon_i$ 假定为独立同分布于 $\\mathcal{N}(0, \\sigma^2)$。\n\n系数向量 $\\beta$ 的普通最小二乘 (OLS) 估计量由下式给出：\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n在模型假设下，给定设计矩阵 $X$ 的OLS估计量的条件分布是正态的，其均值为 $\\beta$，方差-协方差矩阵为：\n$$\n\\text{Var}(\\hat{\\beta} | X) = \\sigma^2 (X^T X)^{-1}\n$$\n在实际应用中，误差方差 $\\sigma^2$ 是未知的，必须从数据中估计。$\\sigma^2$ 的无偏估计量是：\n$$\ns^2 = \\frac{e^T e}{n-p} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-p}\n$$\n其中 $e = y - X\\hat{\\beta}$ 是残差向量，$n-p$ 是自由度。系数向量的估计方差则为 $\\hat{\\text{Var}}(\\hat{\\beta}) = s^2(X^T X)^{-1}$。单个系数估计 $\\hat{\\beta}_j$ 的标准误是该矩阵第 $j$ 个对角元素的平方根：\n$$\nse(\\hat{\\beta}_j) = \\sqrt{s^2 \\left((X^T X)^{-1}\\right)_{jj}}\n$$\n其中索引 $j$ 的范围是从 $0$ 到 $p-1=3$。我们感兴趣的系数 $\\beta_{12}$ 对应于索引 $j=3$。\n\n对于小样本，推断基于学生t分布。交互项系数 $\\beta_{12}$ 的枢轴量为：\n$$\n\\frac{\\hat{\\beta}_{12} - \\beta_{12}}{se(\\hat{\\beta}_{12})} \\sim t_{n-p}\n$$\n其中 $t_{n-p}$ 是自由度为 $n-p$ 的学生t分布。在名义水平为 $1-\\alpha$ 时，$\\beta_{12}$ 的双侧置信区间构造如下：\n$$\n\\hat{\\beta}_{12} \\pm t_{1-\\alpha/2, n-p} \\cdot se(\\hat{\\beta}_{12})\n$$\n其中 $t_{1-\\alpha/2, n-p}$ 是 $t_{n-p}$ 分布的上 $(1-\\alpha/2)$ 临界值。该区间的长度为 $2 \\cdot t_{1-\\alpha/2, n-p} \\cdot se(\\hat{\\beta}_{12})$。设计矩阵 $X$ 中预测变量之间的高度相关性（多重共线性）会增大 $(X^T X)^{-1}$ 的对角元素，从而导致更大的标准误，进而使置信区间变宽。本模拟旨在定量地展示这一效应。\n\n对于每个测试用例 $(n, \\rho)$，模拟过程如下：\n1. 使用固定的种子 $123456$ 初始化一个随机数生成器。\n2.  执行一个包含 $R=500$ 次重复的循环。在每次重复中：\n    a. 从一个均值为 $\\mu = [1, 1]^T$、协方差矩阵为 $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ 的二元正态分布中生成一个大小为 $n$ 的预测变量对 $(x_1, x_2)$ 样本。\n    b. 构建一个 $n \\times 4$ 的设计矩阵 $X$，其列分别为截距、$x_1$、$x_2$ 和交互项 $x_1 x_2$。\n    c. 从 $\\mathcal{N}(0, \\sigma^2)$（其中 $\\sigma=1$）中生成一个大小为 $n$ 的误差样本 $\\varepsilon$。\n    d. 使用真实模型 $y = X\\beta_{true} + \\varepsilon$ 生成响应向量 $y$，其中 $\\beta_{true} = [0, 1, 1, 0.5]^T$。\n    e. 计算OLS估计值 $\\hat{\\beta} = (X^T X)^{-1} X^T y$。\n    f. 计算估计的误差方差 $s^2$。\n    g. 使用上述公式计算交互项系数估计的标准误 $se(\\hat{\\beta}_{12})$。\n    h. 确定临界值 $t_{crit} = t_{1-\\alpha/2, n-4}$，其中 $\\alpha=0.05$。\n    i. 构建 $\\beta_{12}$ 的置信区间并计算其长度。\n    j. 记录一个二元指示符（如果区间包含真实值 $\\beta_{12}=0.5$ 则为 $1$，否则为 $0$）和区间长度。\n3. 所有重复完成后，计算覆盖指示符的平均值以获得经验覆盖概率，并计算区间长度的平均值。\n4. 存储该测试用例的最终四舍五入结果。\n\n对所有四个测试用例重复此过程，并将结果汇编成最终指定的输出格式。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Simulates the effect of predictor correlation on the confidence interval \n    of an interaction term in a linear model.\n    \"\"\"\n    # Define fixed parameters from the problem statement\n    R = 500\n    ALPHA = 0.05\n    BETA_TRUE = np.array([0.0, 1.0, 1.0, 0.5])  # [beta0, beta1, beta2, beta12]\n    SIGMA = 1.0\n    MU = np.array([1.0, 1.0])\n    RANDOM_SEED = 123456\n    \n    # Define test cases (n, rho)\n    test_cases = [\n        (200, 0.0),\n        (200, 0.9),\n        (200, -0.9),\n        (60, 0.99),\n    ]\n\n    all_results = []\n    \n    # Initialize a single random number generator for reproducibility across all cases\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    for n, rho in test_cases:\n        coverage_indicators = []\n        interval_lengths = []\n\n        # Number of model parameters (intercept, x1, x2, x1*x2)\n        p = 4\n        # Degrees of freedom for the t-distribution\n        df = n - p\n        # Critical t-value for a two-sided (1-alpha)% confidence interval\n        t_crit = t.ppf(1 - ALPHA / 2, df)\n\n        for _ in range(R):\n            # Step 1: Generate the data for one replicate\n            \n            # Define the covariance matrix for the predictors (x1, x2)\n            cov_matrix = np.array([[1.0, rho], [rho, 1.0]])\n            \n            # Generate n samples of [x1, x2]\n            predictors = rng.multivariate_normal(MU, cov_matrix, size=n)\n            x1 = predictors[:, 0]\n            x2 = predictors[:, 1]\n            \n            # Construct the design matrix X of size n x p\n            X = np.ones((n, p))\n            X[:, 1] = x1\n            X[:, 2] = x2\n            X[:, 3] = x1 * x2\n            \n            # Generate n error terms\n            epsilon = rng.normal(loc=0, scale=SIGMA, size=n)\n            \n            # Generate the response variable y\n            y = X @ BETA_TRUE + epsilon\n\n            # Step 2: Fit the OLS model and get coefficient estimates\n            \n            # Calculate (X'X)^-1 for standard errors\n            inv_xtx = np.linalg.inv(X.T @ X)\n            \n            # Calculate OLS coefficient estimates: beta_hat = (X'X)^-1 * X'y\n            beta_hat = inv_xtx @ X.T @ y\n            \n            # Step 3: Construct the confidence interval for the interaction term (beta_12)\n            \n            # Calculate residuals\n            residuals = y - X @ beta_hat\n            \n            # Calculate residual sum of squares (RSS)\n            rss = residuals.T @ residuals\n            \n            # Estimate the error variance (s^2)\n            s2 = rss / df\n            \n            # Calculate the standard error of the interaction coefficient estimate\n            # This is the 4th coefficient (index 3)\n            se_beta12 = np.sqrt(s2 * inv_xtx[3, 3])\n            \n            # Calculate the margin of error\n            margin_of_error = t_crit * se_beta12\n            \n            # Calculate the length of the confidence interval\n            length = 2 * margin_of_error\n            interval_lengths.append(length)\n            \n            # Get the point estimate for the interaction coefficient\n            beta12_hat = beta_hat[3]\n            \n            # Define the confidence interval bounds\n            lower_bound = beta12_hat - margin_of_error\n            upper_bound = beta12_hat + margin_of_error\n            \n            # Check if the true parameter value is covered by the interval\n            true_beta12 = BETA_TRUE[3]\n            covered = 1 if (lower_bound = true_beta12 = upper_bound) else 0\n            coverage_indicators.append(covered)\n\n        # Calculate empirical coverage and average length for the current test case\n        empirical_coverage = np.mean(coverage_indicators)\n        avg_length = np.mean(interval_lengths)\n        \n        # Format results as specified (rounded to 3 decimal places)\n        result = [round(empirical_coverage, 3), round(avg_length, 3)]\n        all_results.append(result)\n\n    # Final print statement in the exact required format: [[c1,l1],[c2,l2],...]\n    formatted_results = [f\"[{c},{l}]\" for c, l in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界中的交互作用并非总是“一视同仁”地作用于整个特征空间，它们可能只在数据的特定区域内存在。本练习将模拟这样一种情景，并要求你比较两种不同的建模策略：一个试图用单一全局交互项捕捉效应的多项式模型，以及一个能够自适应地划分数据空间的回归树模型。通过比较它们的预测性能，你将深入理解全局模型与局部模型在处理复杂、非均匀交互作用时的优劣，并体会模型选择的重要性。",
            "id": "3132277",
            "problem": "要求您编写一个完整、可运行的程序，用于在某个输入空间子集上存在预测变量交互作用的数据上，比较全局交互多项式回归与局部逐段常数回归树。您的实现必须遵循使用平方损失的经验风险最小化、普通最小二乘法以及贪心平方和分裂的定义。\n\n考虑预测变量 $x_1$ 和 $x_2$，它们独立地从 $[0,1]$ 上的均匀分布中采样。潜回归函数为\n$$\nf(x_1,x_2) = a_1 x_1 + a_2 x_2 + b_1 x_1^2 + b_2 x_2^2 + \\mathbf{1}\\{x_1  \\tau_1, x_2  \\tau_2\\}\\, c\\, x_1 x_2,\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数，$a_1 = 1.0$，$a_2 = -1.0$，$b_1 = 0.5$，$b_2 = -0.5$，以及 $(\\tau_1,\\tau_2)$ 和 $c$ 是特定情境的参数。观测值的生成方式为\n$$\ny = f(x_1,x_2) + \\varepsilon,\n$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 是独立的高斯噪声。\n\n您的程序必须实现并比较以下两个估计器：\n\n- 全局交互多项式：对特征向量\n$$\n\\phi(x_1,x_2) = \\left[1,\\, x_1,\\, x_2,\\, x_1^2,\\, x_2^2,\\, x_1 x_2\\right]\n$$\n使用普通最小二乘法（OLS, Ordinary Least Squares）拟合一个线性模型。该模型在整个输入空间上施加了一个单一的全局交互项 $x_1 x_2$。\n\n- 局部回归树：拟合一个二元、轴对齐的回归树，该树在每个叶节点中预测一个常数。通过在每次分裂时贪心地最小化经验平方误差和来构建树。在一个包含样本响应 $\\{y_i\\}_{i=1}^n$ 的节点上，节点不纯度为\n$$\n\\mathrm{SSE} = \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2,\\quad \\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i.\n$$\n在特征 $j \\in \\{1,2\\}$ 和阈值 $t$ 上的一个候选分裂将节点划分为左右两个子节点；选择能够最小化子节点平方误差和（SSE）的分裂，同时满足最小叶节点大小的约束。当达到最大深度、没有有效分裂满足最小叶节点大小，或没有分裂能够减少 SSE 时，停止分裂。每个叶节点预测该叶节点内 $y$ 的样本均值。\n\n对于评估，针对以下每个情境，生成一个大小为 $n_{\\text{train}}$ 的训练集和一个大小为 $n_{\\text{test}} = 20000$ 的独立测试集。在训练数据上训练两个模型。在测试集上，计算相对于无噪声目标 $f(x_1,x_2)$ 的样本外均方误差（MSE）：\n$$\n\\mathrm{MSE} = \\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} \\left(\\hat{f}(x_{1,i},x_{2,i}) - f(x_{1,i},x_{2,i})\\right)^2.\n$$\n此 MSE 衡量的是函数逼近的质量，而非噪声预测。\n\n使用以下情境测试套件。对于每个情境 $k$，使用指定的随机种子生成训练和测试数据以确保可复现性。树使用指定的​​最大深度和最小叶节点大小。\n\n- 情境 1：$n_{\\text{train}} = 400$，$c = 3.0$，$(\\tau_1,\\tau_2) = (0.5, 0.5)$，$\\sigma = 0.1$，树最大深度 $= 2$，树最小叶节点大小 $= 20$，训练种子 $= 7$，测试种子 $= 97$。\n- 情境 2：$n_{\\text{train}} = 400$，$c = 0.5$，$(\\tau_1,\\tau_2) = (0.6, 0.6)$，$\\sigma = 0.1$，树最大深度 $= 2$，树最小叶节点大小 $= 20$，训练种子 $= 8$，测试种子 $= 98$。\n- 情境 3：$n_{\\text{train}} = 400$，$c = 3.0$，$(\\tau_1,\\tau_2) = (0.5, 0.5)$，$\\sigma = 1.0$，树最大深度 $= 2$，树最小叶节点大小 $= 40$，训练种子 $= 9$，测试种子 $= 99$。\n- 情境 4：$n_{\\text{train}} = 800$，$c = 5.0$，$(\\tau_1,\\tau_2) = (0.85, 0.85)$，$\\sigma = 0.1$，树最大深度 $= 3$，树最小叶节点大小 $= 10$，训练种子 $= 10$，测试种子 $= 100$。\n\n您的程序必须按顺序为每个情境输出两个浮点数：首先是全局交互多项式的 MSE，然后是局部回归树的 MSE。将所有情境的结果连接成一个列表。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔的十进制数列表，每个数字精确到 $6$ 位小数，顺序如下\n$$\n[\\mathrm{MSE}^{\\text{global}}_1,\\mathrm{MSE}^{\\text{tree}}_1,\\mathrm{MSE}^{\\text{global}}_2,\\mathrm{MSE}^{\\text{tree}}_2,\\mathrm{MSE}^{\\text{global}}_3,\\mathrm{MSE}^{\\text{tree}}_3,\\mathrm{MSE}^{\\text{global}}_4,\\mathrm{MSE}^{\\text{tree}}_4].\n$$\n不应打印任何其他文本。",
            "solution": "该问题要求在一个模拟数据集上比较两种回归模型——一个全局多项式模型和一个局部回归树。该数据集的特点是交互项仅在特征空间的特定子区域中存在。比较的依据是相对于真实的、无噪声的数据生成函数的样本外均方误差（MSE）。\n\n### 步骤 1：问题验证\n\n第一步是验证问题陈述。\n\n**提取的已知条件：**\n- **预测变量：** $x_1, x_2$ 独立地从 $[0,1]$ 上的均匀分布中采样。\n- **潜函数：** $f(x_1,x_2) = a_1 x_1 + a_2 x_2 + b_1 x_1^2 + b_2 x_2^2 + \\mathbf{1}\\{x_1  \\tau_1, x_2  \\tau_2\\}\\, c\\, x_1 x_2$。\n- **常数：** $a_1 = 1.0$, $a_2 = -1.0$, $b_1 = 0.5$, $b_2 = -0.5$。\n- **噪声模型：** $y = f(x_1,x_2) + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$。\n- **估计器 1（全局多项式）：** 对特征向量 $\\phi(x_1,x_2) = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2]$ 进行普通最小二乘法（OLS）拟合。\n- **估计器 2（局部回归树）：** 一个二元、轴对齐的树，通过在每次分裂时贪心地最小化平方误差和（SSE）来构建，受最大深度和最小叶节点大小的约束。叶节点预测响应的样本均值。\n- **树不纯度：** $\\mathrm{SSE} = \\sum_{i=1}^n (y_i - \\bar{y})^2$。\n- **评估指标：** 在大小为 $n_{\\text{test}} = 20000$ 的测试集上的样本外 MSE，计算公式为 $\\mathrm{MSE} = \\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}(x_{1,i},x_{2,i}) - f(x_{1,i},x_{2,i}))^2$。\n- **情境：** 定义了四个情境，每个情境都规定了 $n_{\\text{train}}$、$c$、$(\\tau_1,\\tau_2)$、$\\sigma$、树最大深度、树最小叶节点大小以及用于训练和测试数据生成的随机种子的具体参数。\n\n**对照标准进行验证：**\n- **科学依据：** 该问题牢固地植根于标准的统计学习理论，采用了诸如 OLS、回归树、使用平方误差损失的经验风险最小化以及基于仿真的模型比较等成熟概念。所有定义都是标准的且在数学上是合理的。\n- **良定性：** 问题提供了一套完整的指令。数据生成过程、模型结构、拟合程序和评估指标都得到了明确的定义。使用随机种子确保了数值结果是可复现的。存在唯一且有意义的解。\n- **客观性：** 问题以精确、正式的语言陈述，没有歧义或主观性断言。\n- **完整性和一致性：** 提供了四个情境中每一个所需的所有参数，并且没有内部矛盾。\n- **相关性：** 该问题直接探讨了交互作用和非线性建模这一主题，这是统计学习的核心议题。它提出了一个有意义的问题，即在一个可能设定错误的全局模型和一个可能过拟合但更善于捕捉局部现象的局部、更灵活的模型之间进行权衡。\n\n**结论：** 该问题是有效的、科学上合理的、良定的和完整的。我们可以继续进行求解。\n\n### 步骤 2：解法推导与算法设计\n\n任务的核心是为每个给定的情境实现数据生成过程、两种指定的模型以及评估程序。\n\n**数据生成**\n对于每个情境，我们生成一个大小为 $n_{\\text{train}}$ 的训练集 $(X_{\\text{train}}, y_{\\text{train}})$ 和一个大小为 $n_{\\text{test}} = 20000$ 的测试集 $(X_{\\text{test}}, y_{\\text{test}})$。预测变量 $x_1, x_2$ 从 $\\text{Uniform}(0,1)$ 中抽取。计算真实函数值 $f(x_1, x_2)$，并通过添加高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 来生成观测响应 $y$。随机数生成器按照指定进行播种以确保可复现性。\n\n**模型 1：全局交互多项式**\n该模型假设预测变量和响应之间的关系可以通过整个特征空间上的单个多项式函数来近似：\n$$\n\\hat{f}(x_1, x_2) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_1^2 + \\hat{\\beta}_4 x_2^2 + \\hat{\\beta}_5 x_1 x_2\n$$\n系数 $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\dots, \\hat{\\beta}_5]^T$ 使用 OLS 进行估计。给定训练数据 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$，我们构建设计矩阵 $\\mathbf{X}$ 和响应向量 $\\mathbf{y}$：\n$$\n\\mathbf{X} = \\begin{pmatrix}\n1  x_{1,1}  x_{2,1}  x_{1,1}^2  x_{2,1}^2  x_{1,1}x_{2,1} \\\\\n1  x_{1,2}  x_{2,2}  x_{1,2}^2  x_{2,2}^2  x_{1,2}x_{2,2} \\\\\n\\vdots  \\vdots  \\vdots  \\vdots  \\vdots  \\vdots \\\\\n1  x_{1,n_{\\text{train}}}  x_{2,n_{\\text{train}}}  x_{1,n_{\\text{train}}}^2  x_{2,n_{\\text{train}}}^2  x_{1,n_{\\text{train}}}x_{2,n_{\\text{train}}}\n\\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n_{\\text{train}}} \\end{pmatrix}\n$$\nOLS 解找到最小化残差平方和 $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|^2$ 的 $\\hat{\\boldsymbol{\\beta}}$。该解由 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$ 给出。为保证数值稳定性，最好使用 QR 分解或 SVD 等方法求解此线性系统，这可由 `scipy.linalg.lstsq` 等库函数处理。一旦找到 $\\hat{\\boldsymbol{\\beta}}$，便可通过 $\\hat{\\mathbf{y}}_{\\text{test}} = \\mathbf{X}_{\\text{test}}\\hat{\\boldsymbol{\\beta}}$ 对测试集中的新数据点进行预测。\n\n**模型 2：局部回归树**\n该模型将特征空间 $[0,1] \\times [0,1]$ 划分为不相交的矩形区域，并在每个区域中拟合一个简单的常数模型（响应的均值）。划分是贪心地、递归地进行的。\n\n- **节点表示：** 树由节点组成。内部节点指定一个分裂（一个特征索引 $j \\in \\{1,2\\}$ 和一个阈值 $t$），并有两个子节点（左和右）。叶节点没有子节点，并存储一个预测值（落入其区域的训练点的 $y$ 的样本均值）。\n\n- **分裂内部节点：** 在包含训练数据子集的给定节点上，我们搜索最佳分裂。分裂由一个特征 $j$ 和一个值 $t$ 定义。它将数据划分为一个左集合 $\\{ \\mathbf{x}_i \\mid x_{i,j} \\le t \\}$ 和一个右集合 $\\{ \\mathbf{x}_i \\mid x_{i,j}  t \\}$。分裂的质量由总平方误差和的减少量来衡量：\n$$\n\\text{Gain}(j, t) = \\text{SSE}_{\\text{parent}} - (\\text{SSE}_{\\text{left}} + \\text{SSE}_{\\text{right}})\n$$\n其中 $\\text{SSE}_{\\text{region}} = \\sum_{i \\in \\text{region}} (y_i - \\bar{y}_{\\text{region}})^2$。我们搜索所有特征 $j \\in \\{1,2\\}$ 和所有有效的分割点 $t$，以找到最大化此增益的组合。一个分裂只有在遵守最小叶节点大小约束时才有效：左、右子节点都必须包含至少最小数量的样本。一个特征的潜在分割点 $t$ 可以有效地选择为该特征连续唯一排序值之间的中点。\n\n- **递归树构建：** 算法过程如下：\n  1. 从包含所有训练数据的根节点开始。\n  2. 对于一个节点，检查停止条件：\n     a. 当前深度等于允许的最大深度。\n     b. 节点中的样本数小于最小叶节点大小的两倍（因为任何分裂都会违反该约束）。\n     c. 节点中所有的响应值 $y_i$ 都相同（SSE 为 0，无法进一步改进）。\n  3. 如果满足停止条件，创建一个叶节点，并将其响应的均值存储为其预测值。\n  4. 否则，通过最大化 SSE 增益找到最佳分裂 $(j^*, t^*)$。\n  5. 如果没有分裂提供正增益，或者不存在有效分裂，则创建一个叶节点。\n  6. 否则，创建一个具有分裂 $(j^*, t^*)$ 的内部节点。划分数据并递归地构建左、右子树。\n\n- **预测：** 为了对新点 $\\mathbf{x}_{\\text{new}}$ 进行预测，我们从根节点开始遍历树。在每个内部节点，我们将 $\\mathbf{x}_{\\text{new}}$ 的相关特征与节点的阈值进行比较，以决定向左走还是向右走，直到到达一个叶节点。预测值就是存储在该叶节点中的值。\n\n**评估**\n对于每个情境，在训练数据上训练完两个模型后，我们为大型测试集中的特征生成预测 $\\hat{f}_{\\text{poly}}$ 和 $\\hat{f}_{\\text{tree}}$。性能通过相对于真实函数值 $f_{\\text{test}}$（无噪声）的 MSE 来衡量：\n$$\n\\text{MSE}_{\\text{poly}} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}_{\\text{poly}}(x_{1,i}, x_{2,i}) - f(x_{1,i}, x_{2,i}))^2\n$$\n$$\n\\text{MSE}_{\\text{tree}} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} (\\hat{f}_{\\text{tree}}(x_{1,i}, x_{2,i}) - f(x_{1,i}, x_{2,i}))^2\n$$\n为四个情境中的每一个计算这两个值并报告。\n\n这种系统化的方法确保了问题的所有要求都得到满足，从而在全局参数模型和局部非参数模型之间提供了严谨的比较。实现将精确遵循这些原则。",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the data generation, model training,\n    evaluation, and printing of results for all specified scenarios.\n    \"\"\"\n\n    # --- Helper Classes and Functions ---\n\n    class Node:\n        \"\"\"Represents a node in the regression tree.\"\"\"\n        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n            self.feature_index = feature_index  # Feature to split on\n            self.threshold = threshold          # Threshold for the split\n            self.left = left                    # Left subtree (for values = threshold)\n            self.right = right                  # Right subtree (for values > threshold)\n            self.value = value                  # Prediction value if it's a leaf node\n\n    class DecisionTree:\n        \"\"\"\n        A regression tree that uses greedy sum-of-squares splitting.\n        \"\"\"\n        def __init__(self, max_depth=2, min_samples_leaf=1):\n            self.max_depth = max_depth\n            self.min_samples_leaf = min_samples_leaf\n            self.root = None\n\n        def fit(self, X, y):\n            \"\"\"Build the regression tree from training data.\"\"\"\n            self.root = self._grow_tree(X, y)\n\n        def _grow_tree(self, X, y, depth=0):\n            \"\"\"Recursively grow the tree.\"\"\"\n            n_samples, n_features = X.shape\n            \n            # Check stopping criteria\n            is_pure = len(np.unique(y)) == 1\n            if (depth >= self.max_depth or\n                    n_samples  2 * self.min_samples_leaf or\n                    is_pure):\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n\n            best_split = self._find_best_split(X, y, n_samples, n_features)\n\n            if best_split['gain'] = 0:\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n            \n            left_indices = X[:, best_split['feature_index']] = best_split['threshold']\n            right_indices = ~left_indices\n            \n            left_subtree = self._grow_tree(X[left_indices, :], y[left_indices], depth + 1)\n            right_subtree = self._grow_tree(X[right_indices, :], y[right_indices], depth + 1)\n            \n            return Node(best_split['feature_index'], best_split['threshold'], left_subtree, right_subtree)\n\n        def _find_best_split(self, X, y, n_samples, n_features):\n            \"\"\"Find the best feature and threshold to split on.\"\"\"\n            best_split = {'gain': -1}\n            current_sse = self._calculate_sse(y)\n\n            for feat_idx in range(n_features):\n                thresholds = np.unique(X[:, feat_idx])\n                \n                # Use midpoints between unique sorted values as potential splits\n                if len(thresholds) > 1:\n                    test_thresholds = (thresholds[:-1] + thresholds[1:]) / 2\n                else: \n                    continue\n\n                for threshold in test_thresholds:\n                    left_indices = X[:, feat_idx] = threshold\n                    right_indices = ~left_indices\n                    \n                    if np.sum(left_indices)  self.min_samples_leaf or np.sum(right_indices)  self.min_samples_leaf:\n                        continue\n                        \n                    y_left, y_right = y[left_indices], y[right_indices]\n                    \n                    sse_left = self._calculate_sse(y_left)\n                    sse_right = self._calculate_sse(y_right)\n                    \n                    total_child_sse = sse_left + sse_right\n                    gain = current_sse - total_child_sse\n\n                    if gain > best_split['gain']:\n                        best_split = {\n                            'feature_index': feat_idx,\n                            'threshold': threshold,\n                            'gain': gain\n                        }\n            return best_split\n\n        @staticmethod\n        def _calculate_sse(y):\n            \"\"\"Calculate Sum of Squared Errors.\"\"\"\n            if len(y) == 0:\n                return 0\n            mean = np.mean(y)\n            return np.sum((y - mean) ** 2)\n\n        def predict(self, X):\n            \"\"\"Make predictions for a set of samples.\"\"\"\n            return np.array([self._predict_one(x, self.root) for x in X])\n\n        def _predict_one(self, x, node):\n            \"\"\"Traverse the tree to predict for a single sample.\"\"\"\n            if node.value is not None:\n                return node.value\n            if x[node.feature_index] = node.threshold:\n                return self._predict_one(x, node.left)\n            else:\n                return self._predict_one(x, node.right)\n\n    def true_f(x1, x2, c, tau1, tau2):\n        \"\"\"Computes the latent noise-free function value.\"\"\"\n        a1, a2, b1, b2 = 1.0, -1.0, 0.5, -0.5\n        interaction_term = c * x1 * x2 * ((x1 > tau1)  (x2 > tau2))\n        return a1 * x1 + a2 * x2 + b1 * x1**2 + b2 * x2**2 + interaction_term\n\n    def generate_data(n, seed, c, tau1, tau2, sigma):\n        \"\"\"Generates training or test data.\"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.uniform(0, 1, size=(n, 2))\n        x1, x2 = X[:, 0], X[:, 1]\n        \n        f_vals = true_f(x1, x2, c, tau1, tau2)\n        noise = rng.normal(0, sigma, size=n)\n        y = f_vals + noise\n        \n        return X, y, f_vals\n\n    def fit_predict_polynomial(X_train, y_train, X_test):\n        \"\"\"Fits OLS polynomial model and predicts on test data.\"\"\"\n        # Construct design matrix for training\n        X_design_train = np.c_[\n            np.ones(X_train.shape[0]),\n            X_train[:, 0],\n            X_train[:, 1],\n            X_train[:, 0]**2,\n            X_train[:, 1]**2,\n            X_train[:, 0] * X_train[:, 1]\n        ]\n        \n        # Solve for coefficients using least squares\n        beta, _, _, _ = linalg.lstsq(X_design_train, y_train)\n        \n        # Construct design matrix for testing\n        X_design_test = np.c_[\n            np.ones(X_test.shape[0]),\n            X_test[:, 0],\n            X_test[:, 1],\n            X_test[:, 0]**2,\n            X_test[:, 1]**2,\n            X_test[:, 0] * X_test[:, 1]\n        ]\n        \n        # Make predictions\n        y_pred = X_design_test @ beta\n        return y_pred\n    \n    # --- Scenarios and Main Execution Logic ---\n\n    test_cases = [\n        {'n_train': 400, 'c': 3.0, 'tau1': 0.5, 'tau2': 0.5, 'sigma': 0.1, 'depth': 2, 'min_leaf': 20, 'train_seed': 7, 'test_seed': 97},\n        {'n_train': 400, 'c': 0.5, 'tau1': 0.6, 'tau2': 0.6, 'sigma': 0.1, 'depth': 2, 'min_leaf': 20, 'train_seed': 8, 'test_seed': 98},\n        {'n_train': 400, 'c': 3.0, 'tau1': 0.5, 'tau2': 0.5, 'sigma': 1.0, 'depth': 2, 'min_leaf': 40, 'train_seed': 9, 'test_seed': 99},\n        {'n_train': 800, 'c': 5.0, 'tau1': 0.85, 'tau2': 0.85, 'sigma': 0.1, 'depth': 3, 'min_leaf': 10, 'train_seed': 10, 'test_seed': 100},\n    ]\n\n    all_results = []\n    n_test = 20000\n\n    for case in test_cases:\n        # Generate data\n        X_train, y_train, _ = generate_data(case['n_train'], case['train_seed'], case['c'], case['tau1'], case['tau2'], case['sigma'])\n        X_test, _, f_test = generate_data(n_test, case['test_seed'], case['c'], case['tau1'], case['tau2'], case['sigma'])\n\n        # Model 1: Global Polynomial\n        poly_preds = fit_predict_polynomial(X_train, y_train, X_test)\n        mse_poly = np.mean((poly_preds - f_test)**2)\n        \n        # Model 2: Local Regression Tree\n        tree = DecisionTree(max_depth=case['depth'], min_samples_leaf=case['min_leaf'])\n        tree.fit(X_train, y_train)\n        tree_preds = tree.predict(X_test)\n        mse_tree = np.mean((tree_preds - f_test)**2)\n\n        all_results.extend([mse_poly, mse_tree])\n    \n    # Format and print final results\n    print(f\"[{','.join([f'{r:.6f}' for r in all_results])}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "仅仅依靠回归系数的p值或大小来判断交互作用的重要性有时并不可靠，尤其是在复杂的非线性模型中。弗里德曼（Friedman）的H-统计量提供了一种更通用、与模型无关的方法来量化交互效应的强度。本次实践将指导你从第一性原理出发，基于偏依赖图的思想，动手实现H-统计量的估计，从而深入理解如何分解函数的总变异，并量化其中由交互作用贡献的部分。",
            "id": "3132262",
            "problem": "您的任务是在一个受控模拟中，使用 Friedman 交互作用统计量来实现一个成对交互强度的估计量。请在统计学习的框架内进行，该框架以预测变量的交互作用和非线性变换为核心。考虑一个包含三个独立预测变量以及前两个预测变量之间存在单一成对交互作用的函数：对于任意整数样本大小 $n \\geq 2$，抽取 $n$ 个独立样本 $x_{1}, x_{2}, x_{3} \\sim \\text{Uniform}(-1,1)$，并定义确定性目标函数 $$f(x_{1}, x_{2}, x_{3}) = a_{1} x_{1} + a_{2} x_{2} + a_{3} x_{3} + \\beta \\, x_{1} x_{2}。$$ 请使用以下核心定义作为您推导和实现的起点：部分依赖的概念、通过对无关变量进行积分来分离交互作用分量的容斥逻辑，以及应用于随机变量函数的方差算子。具体而言，$x_{1}$ 和 $x_{2}$ 之间的交互作用分量是根据相对于预测变量联合分布的期望来定义的，而交互强度 $H_{12}$ 应推导为仅由交互作用分量引起的变异性相对于完整函数变异性的归一化度量。这必须从第一性原理出发进行推导，不得使用任何非直接从所提供定义中获得的快捷公式。\n\n您的程序必须：\n- 对于每个测试用例，使用提供的整数种子初始化的伪随机数生成器生成样本 $x_{1}, x_{2}, x_{3}$，其中每个预测变量均独立地从 $\\text{Uniform}(-1,1)$ 分布中抽取。\n- 根据从部分依赖中分离交互作用的容斥逻辑，仅使用对无关变量的期望（通过样本均值近似）来计算 $(x_{1}, x_{2})$ 的成对交互作用分量的经验估计。\n- 计算经验 Friedman 交互作用统计量 $H_{12}$，其值为估计的交互作用分量的经验方差与完整函数 $f(x_{1}, x_{2}, x_{3})$ 的经验方差之比的平方根。所有期望和方差都必须是针对每个测试用例的模拟样本计算的。不允许使用外部模型或拟合程序；预言函数 $f$ 是已知的，必须直接使用。\n- 每个测试用例产生一个实值输出，即估计的 $H_{12}$ 值，使用确定性种子以确保可复现性。不涉及角度，也不适用任何物理单位。将最终的数值输出表示为普通十进制浮点数。\n\n测试套件：\n- 用例 $1$：$n = 64$，$a_{1} = 1$，$a_{2} = 1$，$a_{3} = 1$，$\\beta = 1$，种子 $= 7$。\n- 用例 $2$：$n = 256$，$a_{1} = 1$，$a_{2} = 1$，$a_{3} = 1$，$\\beta = 1$，种子 $= 7$。\n- 用例 $3$：$n = 1024$，$a_{1} = 1$，$a_{2} = 1$，$a_{3} = 1$，$\\beta = 1$，种子 $= 7$。\n- 用例 $4$：$n = 256$，$a_{1} = 1$，$a_{2} = 1$，$a_{3} = 1$，$\\beta = 0$，种子 $= 11$。\n- 用例 $5$：$n = 256$，$a_{1} = 0.5$，$a_{2} = 0.5$，$a_{3} = 0.5$，$\\beta = 3$，种子 $= 13$。\n\n覆盖设计：\n- 用例 $1$–$3$ 保持函数固定并改变 $n$ 值，以评估样本大小对估计的影响。\n- 用例 $4$ 是一个无交互作用（$\\beta = 0$）的边界条件，应产生一个接近 $0$ 的估计值。\n- 用例 $5$ 通过一个相对于加性分量而言较强的交互作用来对估计量进行压力测试。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表的结果（例如，$\\texttt{[result1,result2,result3]}$）。该列表必须精确包含 $5$ 个浮点数，对应于上述 $5$ 个测试用例的估计 $H_{12}$ 值，并按指定顺序排列。",
            "solution": "估计 Friedman 交互作用统计量 $H_{jk}$ 的问题是统计学习中一个明确定义的任务。它要求严格应用统计学原理，即部分依赖和函数方差的分解。我们将从第一性原理出发，推导出所需的经验估计量。\n\n**步骤 1：基本原理**\n\n设 $f(\\mathbf{x}) = f(x_1, x_2, \\dots, x_p)$ 是一个关于 $p$ 个预测变量的函数。这些预测变量是具有联合分布 $p(\\mathbf{x})$ 的随机变量。\n\n**部分依赖函数 (PDP)** 对于变量子集 $S \\subset \\{1, 2, \\dots, p\\}$（其值为 $\\mathbf{x}_S$），定义为 $f(\\mathbf{x})$ 在补集变量 $C = \\{1, 2, \\dots, p\\} \\setminus S$ 的边缘分布上的期望：\n$$f_S(\\mathbf{x}_S) = E_{X_C}[f(\\mathbf{x}_S, X_C)] = \\int f(\\mathbf{x}_S, \\mathbf{x}_C) \\, p(\\mathbf{x}_C) \\, d\\mathbf{x}_C$$\n其中 $p(\\mathbf{x}_C)$ 是 $C$ 中变量的边缘概率密度。\n\n**交互作用分量**，例如 $x_j$ 和 $x_k$ 两个变量之间的交互作用分量，是使用容斥逻辑定义的。它在考虑了各自的独立主效应后，分离出函数中同时依赖于 $x_j$ 和 $x_k$ 的部分。双变量交互函数 $f_{jk}(x_j, x_k)$ 为：\n$$f_{jk}(x_j, x_k) = f_{\\{j,k\\}}(x_j, x_k) - f_{\\{j\\}}(x_j) - f_{\\{k\\}}(x_k) + E[f(X)]$$\n其中 $f_{\\{j,k\\}}$、$f_{\\{j\\}}$ 和 $f_{\\{k\\}}$ 是相应变量集的 PDP，而 $E[f(X)]$ 是函数的全局均值。\n\n**Friedman H 统计量**，用于衡量 $x_j$ 和 $x_k$ 之间的交互作用，记为 $H_{jk}$，它通过总函数 $f$ 的方差来归一化交互作用分量的方差：\n$$H_{jk}^2 = \\frac{\\text{Var}[f_{jk}(X_j, X_k)]}{\\text{Var}[f(X)]}$$\n统计量 $H_{jk}$ 是该比率的平方根，提供了一个在 $0$ 到 $1$ 范围内的交互强度度量。\n\n**步骤 2：经验估计**\n\n问题要求基于一个给定的大小为 $n$ 的样本 $\\{\\mathbf{x}_i = (x_{1i}, x_{2i}, x_{3i})\\}_{i=1}^n$ 进行经验估计。所有的期望都由样本均值替代。\n\n给定函数 $f(x_1, x_2, x_3) = a_1 x_1 + a_2 x_2 + a_3 x_3 + \\beta x_1 x_2$。我们需要计算 $H_{12}$ 的经验估计值。\n\n首先，我们为特定观测值 $i$（其预测变量值为 $(x_{1i}, x_{2i}, x_{3i})$）定义交互作用分量公式中每一项的经验估计量。\n\n在 $(x_{1i}, x_{2i})$ 处评估的 **$(x_1, x_2)$ 的经验 PDP**，通过对无关变量 $x_3$ 的样本值求平均来估计：\n$$\\hat{f}_{\\{1,2\\}}(x_{1i}, x_{2i}) = \\frac{1}{n} \\sum_{l=1}^n f(x_{1i}, x_{2i}, x_{3l})$$\n\n在 $x_{1i}$ 处评估的 **$x_1$ 的经验 PDP**，通过对样本对 $(x_{2l}, x_{3l})$ 求平均来估计：\n$$\\hat{f}_{\\{1\\}}(x_{1i}) = \\frac{1}{n} \\sum_{l=1}^n f(x_{1i}, x_{2l}, x_{3l})$$\n\n在 $x_{2i}$ 处评估的 **$x_2$ 的经验 PDP**，通过对样本对 $(x_{1l}, x_{3l})$ 求平均来估计：\n$$\\hat{f}_{\\{2\\}}(x_{2i}) = \\frac{1}{n} \\sum_{l=1}^n f(x_{1l}, x_{2i}, x_{3l})$$\n\n**经验全局均值** 就是函数在整个样本上的平均值：\n$$\\hat{E}[f] = \\bar{f} = \\frac{1}{n} \\sum_{l=1}^n f(x_{1l}, x_{2l}, x_{3l})$$\n\n结合这些，第 $i$ 个观测值的估计交互作用分量是：\n$$\\hat{f}_{12,i} = \\hat{f}_{12}(x_{1i}, x_{2i}) = \\hat{f}_{\\{1,2\\}}(x_{1i}, x_{2i}) - \\hat{f}_{\\{1\\}}(x_{1i}) - \\hat{f}_{\\{2\\}}(x_{2i}) + \\bar{f}$$\n\n**步骤 3：针对特定函数的推导**\n\n这个通用公式的计算量很大（计算每个 $\\hat{f}_{12,i}$ 的时间复杂度为 $O(n^2)$）。然而，问题提供了一个特定的、简单的函数形式。我们可以将 $f(x_1, x_2, x_3) = a_1 x_1 + a_2 x_2 + a_3 x_3 + \\beta x_1 x_2$ 代入经验 PDP 的定义中，以获得一个高效的计算方法。\n\n设 $\\bar{x}_k = \\frac{1}{n} \\sum_{l=1}^n x_{kl}$ 为预测变量 $k$ 的样本均值，并设 $C_{12} = \\frac{1}{n} \\sum_{l=1}^n x_{1l}x_{2l}$ 为乘积 $x_1 x_2$ 的样本均值。\n\n将 $f$ 代入 PDP 估计量中：\n$$ \\hat{f}_{\\{1,2\\}}(x_{1i}, x_{2i}) = \\frac{1}{n} \\sum_{l=1}^n (a_1 x_{1i} + a_2 x_{2i} + a_3 x_{3l} + \\beta x_{1i} x_{2i}) = a_1 x_{1i} + a_2 x_{2i} + a_3 \\bar{x}_3 + \\beta x_{1i} x_{2i} $$\n$$ \\hat{f}_{\\{1\\}}(x_{1i}) = \\frac{1}{n} \\sum_{l=1}^n (a_1 x_{1i} + a_2 x_{2l} + a_3 x_{3l} + \\beta x_{1i} x_{2l}) = a_1 x_{1i} + a_2 \\bar{x}_2 + a_3 \\bar{x}_3 + \\beta x_{1i} \\bar{x}_2 $$\n$$ \\hat{f}_{\\{2\\}}(x_{2i}) = \\frac{1}{n} \\sum_{l=1}^n (a_1 x_{1l} + a_2 x_{2i} + a_3 x_{3l} + \\beta x_{1l} x_{2i}) = a_1 \\bar{x}_1 + a_2 x_{2i} + a_3 \\bar{x}_3 + \\beta x_{2i} \\bar{x}_1 $$\n$$ \\bar{f} = \\frac{1}{n} \\sum_{l=1}^n (a_1 x_{1l} + a_2 x_{2l} + a_3 x_{3l} + \\beta x_{1l} x_{2l}) = a_1 \\bar{x}_1 + a_2 \\bar{x}_2 + a_3 \\bar{x}_3 + \\beta C_{12} $$\n\n现在，将这些代入 $\\hat{f}_{12,i}$ 的公式中并消去各项：\n\\begin{align*}\n\\hat{f}_{12,i} = (a_1 x_{1i} + a_2 x_{2i} + a_3 \\bar{x}_3 + \\beta x_{1i} x_{2i}) \\\\\n           \\quad - (a_1 x_{1i} + a_2 \\bar{x}_2 + a_3 \\bar{x}_3 + \\beta x_{1i} \\bar{x}_2) \\\\\n           \\quad - (a_1 \\bar{x}_1 + a_2 x_{2i} + a_3 \\bar{x}_3 + \\beta x_{2i} \\bar{x}_1) \\\\\n           \\quad + (a_1 \\bar{x}_1 + a_2 \\bar{x}_2 + a_3 \\bar{x}_3 + \\beta C_{12})\n\\end{align*}\n涉及 $a_1$、$a_2$ 和 $a_3$ 的加性项都相互抵消，只剩下乘以 $\\beta$ 的项：\n$$ \\hat{f}_{12,i} = \\beta x_{1i} x_{2i} - \\beta x_{1i} \\bar{x}_2 - \\beta x_{2i} \\bar{x}_1 + \\beta C_{12} $$\n$$ \\hat{f}_{12,i} = \\beta (x_{1i} x_{2i} - x_{1i} \\bar{x}_2 - x_{2i} \\bar{x}_1 + C_{12}) $$\n这个简化的公式允许在计算初始样本均值后，以 $O(n)$ 的时间计算所有 $n$ 个交互作用分量的值。\n\n**步骤 4：经验 $H_{12}$ 的最终算法**\n\n对于每个具有参数 $n$、$a_1$、$a_2$、$a_3$、$\\beta$ 和一个随机种子的测试用例：\n\n1.  生成一个 $n \\times 3$ 的数据矩阵 $X$，其中每一列都是从 $\\text{Uniform}(-1, 1)$ 分布中独立抽取的 $n$ 个样本。设各列为 $X_1$、$X_2$、$X_3$。\n2.  计算完整函数 $f_i = f(x_{1i}, x_{2i}, x_{3i})$ 的 $n$ 个值，其中 $i=1, \\dots, n$。\n3.  计算完整函数值的经验方差（使用 $n$ 作为分母）：\n    $$ \\widehat{\\text{Var}}[f] = \\frac{1}{n} \\sum_{i=1}^n (f_i - \\bar{f})^2 $$\n4.  计算样本均值 $\\bar{x}_1, \\bar{x}_2$ 和乘积的样本均值 $C_{12} = \\frac{1}{n}\\sum_i x_{1i}x_{2i}$。\n5.  使用推导出的公式，计算估计的交互作用分量 $\\hat{f}_{12,i}$ 的 $n$ 个值，其中 $i=1, \\dots, n$。\n6.  计算这些交互作用分量值的经验方差：\n    $$ \\widehat{\\text{Var}}[\\hat{f}_{12}] = \\frac{1}{n} \\sum_{i=1}^n (\\hat{f}_{12,i} - \\overline{\\hat{f}_{12}})^2 $$\n    其中 $\\overline{\\hat{f}_{12}}$ 是 $\\hat{f}_{12,i}$ 值的均值。\n7.  计算估计的 H 统计量：\n    $$ \\hat{H}_{12} = \\sqrt{\\frac{\\widehat{\\text{Var}}[\\hat{f}_{12}]}{\\widehat{\\text{Var}}[f]}} $$\n    如果 $\\widehat{\\text{Var}}[f]$ 为零，则结果取为 $0$。这种情况发生在 $f$ 在整个样本中为常数时。对于用例 $4$ 就会发生这种情况，其中 $\\beta=0$，使得 $\\hat{f}_{12,i}$ 恒为零，因此 $\\widehat{\\text{Var}}[\\hat{f}_{12}]=0$。\n\n此过程被实现用来计算每个测试用例所需的值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Friedman's H-statistic for pairwise interaction for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, a1, a2, a3, beta, seed)\n        (64, 1, 1, 1, 1, 7),\n        (256, 1, 1, 1, 1, 7),\n        (1024, 1, 1, 1, 1, 7),\n        (256, 1, 1, 1, 0, 11),\n        (256, 0.5, 0.5, 0.5, 3, 13),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, a1, a2, a3, beta, seed = case\n        \n        # 1. Generate samples for predictors x1, x2, x3.\n        # Initialize the pseudorandom number generator with the specified seed.\n        rng = np.random.default_rng(seed)\n        # Draw n samples for 3 predictors from Uniform(-1, 1).\n        X = rng.uniform(low=-1.0, high=1.0, size=(n, 3))\n        x1, x2, x3 = X[:, 0], X[:, 1], X[:, 2]\n\n        # 2. Compute the values of the full function f for each sample.\n        f_vals = a1 * x1 + a2 * x2 + a3 * x3 + beta * x1 * x2\n        \n        # 3. Calculate the empirical variance of the full function.\n        # np.var uses the population variance formula (denominator n), as required.\n        var_f = np.var(f_vals)\n\n        # Handle the edge case where the function variance is zero.\n        if var_f == 0.0:\n            # If the total variance is zero, the interaction strength must be zero.\n            results.append(0.0)\n            continue\n            \n        # 4. Compute necessary sample means for the interaction component formula.\n        x1_mean = np.mean(x1)\n        x2_mean = np.mean(x2)\n        # C12 is the mean of the product of x1 and x2.\n        c12 = np.mean(x1 * x2)\n\n        # 5. Compute the values of the estimated interaction component.\n        # This uses the efficient formula derived from first principles for the given f.\n        f_12_vals = beta * (x1 * x2 - x1 * x2_mean - x2 * x1_mean + c12)\n\n        # 6. Calculate the empirical variance of the interaction component.\n        var_int = np.var(f_12_vals)\n\n        # 7. Compute the estimated H-statistic.\n        h_stat = np.sqrt(var_int / var_f)\n        results.append(h_stat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.17f}'.rstrip('0').rstrip('.') for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}