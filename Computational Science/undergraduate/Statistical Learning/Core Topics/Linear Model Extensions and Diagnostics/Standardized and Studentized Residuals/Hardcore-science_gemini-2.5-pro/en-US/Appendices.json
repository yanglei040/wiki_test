{
    "hands_on_practices": [
        {
            "introduction": "The formula for standardized residuals, $r_{i} = e_{i} / (s \\sqrt{1 - h_{ii}})$, can appear abstract at first. This practice builds intuition by applying the concept to the simplest possible case: an intercept-only linear model. By working through the derivation , you will uncover a direct and elegant connection between standardized residuals and the familiar $z$-scores from introductory statistics, solidifying your understanding of how these diagnostics adjust for leverage.",
            "id": "3176927",
            "problem": "Consider a univariate linear regression with an intercept only: for observations indexed by $i \\in \\{1,2,\\ldots,n\\}$, the model is $y_{i} = \\beta_{0} + \\varepsilon_{i}$ with $\\varepsilon_{i}$ independent and identically distributed as $\\mathcal{N}(0,\\sigma^{2})$. Let the design matrix be $X = \\mathbf{1}$, the $n \\times 1$ vector of ones. Let $\\hat{y}_{i}$ denote the fitted values, $e_{i} = y_{i} - \\hat{y}_{i}$ the residuals, and $H$ the hat matrix with entries $h_{ij}$. Define the residual sum of squares $\\mathrm{RSS} = \\sum_{i=1}^{n} e_{i}^{2}$ and the residual standard error $s = \\sqrt{\\mathrm{RSS}/(n-p)}$ with $p=1$. The standardized residual is defined as $r_{i} = \\dfrac{e_{i}}{s \\sqrt{1 - h_{ii}}}$. Let $\\bar{y} = \\dfrac{1}{n}\\sum_{i=1}^{n} y_{i}$ and the sample standard deviation $s_{y} = \\sqrt{\\dfrac{1}{n-1}\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}}$. Define the $z$-score $z_{i} = \\dfrac{y_{i} - \\bar{y}}{s_{y}}$.\n\nStarting from the standard definitions of the hat matrix, residuals, and variance estimators in the linear model, and without assuming any result specific to the intercept-only case in advance, derive $h_{ii}$, show the relationship between $s$ and $s_{y}$, and obtain a closed-form expression for the standardized residual $r_{i}$ written purely in terms of $z_{i}$ and $n$. Provide your final answer as a single closed-form analytic expression for $r_{i}$ in terms of $z_{i}$ and $n$ only. No numerical rounding is required.",
            "solution": "The objective is to derive a closed-form expression for the standardized residual $r_{i}$ in terms of the $z$-score $z_{i}$ and the sample size $n$ for a univariate linear regression model with only an intercept. The derivation proceeds by first principles, as required.\n\nThe model is specified as $y_{i} = \\beta_{0} + \\varepsilon_{i}$ for $i \\in \\{1, 2, \\ldots, n\\}$, where the errors $\\varepsilon_{i}$ are independent and identically distributed as $\\mathcal{N}(0,\\sigma^{2})$. The design matrix for this model is $X = \\mathbf{1}$, which is an $n \\times 1$ column vector of ones.\n\nFirst, we determine the ordinary least squares (OLS) estimate of the parameter $\\beta_{0}$, denoted as $\\hat{\\beta}_{0}$. The general formula for the OLS estimate vector is $\\hat{\\beta} = (X^{T}X)^{-1}X^{T}y$. We compute the components for our specific model.\nThe term $X^{T}X$ is:\n$$X^{T}X = \\mathbf{1}^{T}\\mathbf{1} = \\sum_{i=1}^{n} 1^{2} = n$$\nThe inverse is $(X^{T}X)^{-1} = n^{-1} = \\frac{1}{n}$.\nThe term $X^{T}y$ is:\n$$X^{T}y = \\mathbf{1}^{T}y = \\sum_{i=1}^{n} y_{i}$$\nBy definition, the sample mean is $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$, which implies $\\sum_{i=1}^{n} y_{i} = n\\bar{y}$.\nSubstituting these into the OLS formula yields the scalar estimate $\\hat{\\beta}_{0}$:\n$$\\hat{\\beta}_{0} = \\left(\\frac{1}{n}\\right)(n\\bar{y}) = \\bar{y}$$\n\nNext, we calculate the fitted values, $\\hat{y}_{i}$. The vector of fitted values is $\\hat{y} = X\\hat{\\beta}_{0}$.\n$$\\hat{y} = \\mathbf{1}\\bar{y}$$\nThis implies that each individual fitted value is $\\hat{y}_{i} = \\bar{y}$ for all $i \\in \\{1, \\ldots, n\\}$.\n\nThe hat matrix, $H$, is defined as $H = X(X^{T}X)^{-1}X^{T}$. Using our previously computed terms:\n$$H = \\mathbf{1}\\left(\\frac{1}{n}\\right)\\mathbf{1}^{T} = \\frac{1}{n}\\mathbf{1}\\mathbf{1}^{T}$$\nThe outer product $\\mathbf{1}\\mathbf{1}^{T}$ results in an $n \\times n$ matrix where every entry is $1$. Therefore, the hat matrix $H$ is an $n \\times n$ matrix where every entry $h_{ij}$ is equal to $\\frac{1}{n}$. The diagonal elements of the hat matrix, which are required for the standardized residual calculation, are thus $h_{ii} = \\frac{1}{n}$ for all $i$.\n\nThe residuals are defined by $e_{i} = y_{i} - \\hat{y}_{i}$. Substituting the expression for the fitted values, we obtain:\n$$e_{i} = y_{i} - \\bar{y}$$\n\nThe Residual Sum of Squares (RSS) is the sum of the squared residuals:\n$$\\mathrm{RSS} = \\sum_{i=1}^{n} e_{i}^{2} = \\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}$$\n\nThe residual standard error, $s$, is defined as $s = \\sqrt{\\mathrm{RSS}/(n-p)}$, where $p$ is the number of estimated parameters in the model. For the intercept-only model, we estimate one parameter, $\\beta_0$, so $p=1$.\n$$s = \\sqrt{\\frac{\\mathrm{RSS}}{n-1}} = \\sqrt{\\frac{\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}}{n-1}}$$\n\nThe problem provides the definition of the sample standard deviation of $y$ as $s_{y} = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}}$. By direct comparison of the expressions for $s$ and $s_{y}$, we establish the crucial relationship $s = s_{y}$.\n\nWe are now prepared to derive the expression for the standardized residual, $r_{i}$. The definition is given as $r_{i} = \\dfrac{e_{i}}{s \\sqrt{1 - h_{ii}}}$.\nWe substitute the expressions derived above: $e_{i} = y_{i} - \\bar{y}$, $s = s_{y}$, and $h_{ii} = \\frac{1}{n}$.\n$$r_{i} = \\frac{y_{i} - \\bar{y}}{s_{y} \\sqrt{1 - \\frac{1}{n}}}$$\nSimplifying the term in the denominator:\n$$r_{i} = \\frac{y_{i} - \\bar{y}}{s_{y} \\sqrt{\\frac{n-1}{n}}}$$\n\nThe final step is to express this result in terms of the given $z$-score, which is defined as $z_{i} = \\dfrac{y_{i} - \\bar{y}}{s_{y}}$. We substitute $z_i$ into our expression for $r_i$:\n$$r_{i} = \\frac{z_{i}}{\\sqrt{\\frac{n-1}{n}}}$$\nRearranging this expression gives the final closed-form relationship between the standardized residual $r_i$ and the $z$-score $z_i$:\n$$r_{i} = z_{i} \\sqrt{\\frac{n}{n-1}}$$\nThis expression depends only on $z_{i}$ and $n$, as required.",
            "answer": "$$\\boxed{z_{i} \\sqrt{\\frac{n}{n-1}}}$$"
        },
        {
            "introduction": "Not all influential points are created equal; some manifest as large residuals, while others exert their influence through high leverage, potentially masking their own outlier status. This coding practice  challenges you to build a dataset that contains both types of points and compare their detection using standardized ($r_i$) versus studentized ($t_i$) residuals. By implementing these diagnostics from first principles, you will gain a practical appreciation for why the studentized residual is a more robust tool for identifying outliers.",
            "id": "3176898",
            "problem": "Consider the classical linear regression model $y = X\\beta + \\varepsilon$ where $X \\in \\mathbb{R}^{n \\times p}$ is a fixed design matrix (with a column of ones to model an intercept), $\\beta \\in \\mathbb{R}^p$ is the parameter vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ is a random error vector assumed to satisfy $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathrm{Var}(\\varepsilon) = \\sigma^2 I_n$. Use the Ordinary Least Squares (OLS) estimator $\\hat{\\beta}$ and the associated residual vector $e = y - X\\hat{\\beta}$, the hat matrix $H = X(X^\\top X)^{-1}X^\\top$ with diagonal entries $h_{ii}$, and degrees of freedom $n - p$ to construct and analyze standardized residuals and studentized residuals from first principles.\n\nYou must write a complete program that:\n- Constructs several datasets, each consisting of a base set of points lying close to a true line together with two special points:\n  1. One point with large leverage (large $h_{ii}$) but small residual magnitude $|e_i|$.\n  2. One point with small leverage (small $h_{ii}$) but large residual magnitude $|e_i|$.\n- Fits the OLS model with intercept and one predictor for each dataset, computes $e_i$ and $h_{ii}$ for all observations, and then computes the standardized residual $r_i$ and the studentized residual $t_i$ for each observation, starting from the model assumptions and core definitions in linear regression. Do not use any prepackaged regression function; derive and implement all needed quantities directly from the definitions of OLS, the hat matrix, and residual-based variance estimators.\n\nFor the base line, use the deterministic model $y = \\beta_0 + \\beta_1 x$ with $\\beta_0 = 2$ and $\\beta_1 = 1.5$. Build each dataset by:\n- Creating $n_{\\text{base}}$ equally spaced design points $x$ over the closed interval $[-x_{\\max}, x_{\\max}]$, symmetric about $0$, with zero noise.\n- Appending the high-leverage point at $x_{\\text{HL}}$ with response exactly on the true line plus a specified small offset $\\epsilon_{\\text{HL}}$.\n- Appending the low-leverage point at $x = 0$ but with response offset by a specified large amount $\\delta_{\\text{LL}}$ away from the true line.\n\nIndex observations starting at $0$ and append the two special points at the end, in the order: high-leverage point first, then low-leverage point. Thus, for each dataset, the index of the high-leverage point is $n_{\\text{base}}$ and the index of the low-leverage large-residual point is $n_{\\text{base}} + 1$.\n\nUsing only the model assumptions and definitions, compute:\n- The OLS estimate $\\hat{\\beta}$ and residuals $e_i$ for all $i$.\n- The leverage values $h_{ii}$ for all $i$ via the hat matrix $H$.\n- The standardized residuals $r_i$ and studentized residuals $t_i$ for all $i$.\n\nYour program must process the following test suite of parameter sets, each given as a tuple $(n_{\\text{base}}, x_{\\max}, x_{\\text{HL}}, \\delta_{\\text{LL}}, \\epsilon_{\\text{HL}})$:\n- Test case $1$: $(12, 3, 15, 15, 0)$.\n- Test case $2$: $(10, 2, 50, 10, 0)$.\n- Test case $3$: $(12, 3, 8, 6, 0.5)$.\n\nFor each dataset, produce four quantities:\n1. The index $i_r$ of the observation with the largest absolute standardized residual $|r_i|$.\n2. The index $i_t$ of the observation with the largest absolute studentized residual $|t_i|$.\n3. A boolean $b_1$ indicating whether, for the low-leverage large-residual point, the studentized residual magnitude exceeds the standardized residual magnitude, that is, whether $|t_i| > |r_i|$ at index $n_{\\text{base}}+1$.\n4. A boolean $b_2$ indicating whether, for the high-leverage small-residual point, the standardized residual magnitude is at least as large as the studentized residual magnitude, that is, whether $|r_i| \\ge |t_i|$ at index $n_{\\text{base}}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each result is itself a list $[i_r, i_t, b_1, b_2]$ corresponding to one test case, in the same order as the test suite (for example, $[[i_{r,1}, i_{t,1}, b_{1,1}, b_{2,1}], [i_{r,2}, i_{t,2}, b_{1,2}, b_{2,2}], [i_{r,3}, i_{t,3}, b_{1,3}, b_{2,3}]]$). No physical units or angle units are involved in this problem.",
            "solution": "The user-provided problem is valid. It is a well-defined computational task in the field of statistical learning, grounded in the established principles of linear regression analysis. The problem is self-contained, scientifically sound, and all parameters and objectives are clearly specified. The solution proceeds by implementing the required calculations from first principles as requested.\n\n### Theoretical Foundation and Method\n\nThe problem requires an analysis of residuals in a simple linear regression model, $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$. This can be expressed in matrix form as $y = X\\beta + \\varepsilon$, where $y$ is the $n \\times 1$ vector of responses, $X$ is the $n \\times p$ design matrix (with $p=2$ for simple linear regression), $\\beta$ is the $p \\times 1$ vector of parameters, and $\\varepsilon$ is the $n \\times 1$ vector of random errors. The errors are assumed to be independent and identically distributed with mean $0$ and variance $\\sigma^2$, i.e., $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathrm{Var}(\\varepsilon) = \\sigma^2 I_n$.\n\n**1. Ordinary Least Squares (OLS) Estimation**\nThe OLS estimator $\\hat{\\beta}$ for the parameter vector $\\beta$ is found by minimizing the sum of squared residuals, $S(\\beta) = (y - X\\beta)^\\top(y - X\\beta)$. This yields the normal equations, $(X^\\top X)\\hat{\\beta} = X^\\top y$. Assuming the matrix $X^\\top X$ is invertible (which holds true if $X$ has full column rank), the unique OLS estimator is:\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\nThe vector of fitted values is $\\hat{y} = X\\hat{\\beta}$, and the vector of residuals is $e = y - \\hat{y}$.\n\n**2. The Hat Matrix and Leverage**\nThe fitted values can be expressed as a linear transformation of the observed values $y$:\n$$ \\hat{y} = X((X^\\top X)^{-1} X^\\top y) = H y $$\nThe matrix $H = X(X^\\top X)^{-1}X^\\top$ is known as the \"hat matrix\" because it transforms $y$ into $\\hat{y}$. It is an $n \\times n$ symmetric and idempotent ($H^2 = H$) projection matrix. The diagonal elements of the hat matrix, $h_{ii}$, are called the leverages. Each $h_{ii}$ measures how much the $i$-th response value $y_i$ influences its own fitted value $\\hat{y}_i$, since $\\hat{y}_i = \\sum_{j=1}^n H_{ij} y_j = h_{ii}y_i + \\sum_{j \\ne i} h_{ij} y_j$. Leverage values are bounded by $0 \\le h_{ii} \\le 1$. A high leverage value indicates that the $i$-th observation is an outlier in the space of the predictors (the $x$-values).\n\n**3. Residual Analysis**\nThe residuals are given by $e = y - \\hat{y} = y - Hy = (I - H)y$. The variance-covariance matrix of the residuals is:\n$$ \\mathrm{Var}(e) = \\mathrm{Var}((I-H)y) = (I-H)\\mathrm{Var}(y)(I-H)^\\top = (I-H)(\\sigma^2 I)(I-H) = \\sigma^2(I-H) $$\nThe variance of a single residual $e_i$ is therefore $\\mathrm{Var}(e_i) = \\sigma^2(1 - h_{ii})$. An unbiased estimator for the error variance $\\sigma^2$ is the Mean Squared Error (MSE):\n$$ \\hat{\\sigma}^2 = \\frac{e^\\top e}{n-p} = \\frac{\\sum_{i=1}^n e_i^2}{n-p} $$\nwhere $n-p$ are the residual degrees of freedom.\n\n**4. Standardized Residuals**\nStandardized residuals account for the fact that raw residuals $e_i$ do not have the same variance. The standardized residual for observation $i$, denoted $r_i$, is the raw residual scaled by an estimate of its standard deviation:\n$$ r_i = \\frac{e_i}{\\widehat{\\mathrm{sd}}(e_i)} = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}} $$\nThis metric adjusts each residual for its leverage. A high-leverage point (large $h_{ii}$) will have its residual magnified.\n\n**5. Studentized Residuals**\nA key issue with standardized residuals is that the observation $i$ being evaluated contributes to the estimate $\\hat{\\sigma}$. If observation $i$ is a significant outlier, it can inflate $\\hat{\\sigma}$, thereby \"masking\" its own outlier status by resulting in a smaller standardized residual. The studentized residual (or externally studentized residual), $t_i$, addresses this by using a variance estimate $\\hat{\\sigma}_{(i)}^2$ computed from a regression fit with the $i$-th observation removed:\n$$ t_i = \\frac{e_i}{\\hat{\\sigma}_{(i)}\\sqrt{1 - h_{ii}}} $$\nA computationally efficient formula relates $t_i$ to the standardized residual $r_i$:\n$$ t_i = r_i \\sqrt{\\frac{n - p - 1}{n - p - r_i^2}} $$\nThis formula demonstrates that $|t_i| > |r_i|$ if and only if $r_i^2 > 1$, meaning the studentized residual further amplifies the magnitude of already large standardized residuals, making it a more sensitive diagnostic for detecting outliers.\n\n### Implementation Strategy\n\nThe program will process each test case as follows:\n1.  **Data Construction**: For each set of parameters $(n_{\\text{base}}, x_{\\max}, x_{\\text{HL}}, \\delta_{\\text{LL}}, \\epsilon_{\\text{HL}})$, generate the $x$ and $y$ vectors. This involves creating the base points on the line $y = 2 + 1.5x$, and appending the specified high-leverage and low-leverage/large-residual points.\n2.  **Model Fitting**: Construct the design matrix $X$ by prepending a column of ones to the $x$ vector. Then, compute the OLS estimate $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$ using matrix operations.\n3.  **Residual and Leverage Calculation**: Compute the residuals $e = y - X\\hat{\\beta}$ and the leverages $h_{ii}$ as the diagonal of the hat matrix $H = X(X^\\top X)^{-1}X^\\top$.\n4.  **Diagnostic Computation**: Calculate the variance estimate $\\hat{\\sigma}^2$, and then compute the vectors of standardized residuals $r$ and studentized residuals $t$ using their respective formulas.\n5.  **Result Extraction**: From the computed vectors, identify the indices of the maximum absolute standardized and studentized residuals, and evaluate the two specified boolean conditions concerning the special points.\n6.  **Output Formatting**: Collate the results for all test cases into a list of lists and format it as a string for the final output.\n\nThe implementation relies on the `numpy` library for numerical linear algebra operations, adhering strictly to the formulas derived from first principles.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_residuals(n_base: int, x_max: float, x_HL: float, delta_LL: float, epsilon_HL: float) -> list:\n    \"\"\"\n    Constructs a dataset, fits an OLS model from first principles, and computes\n    standardized and studentized residuals to identify influential points.\n\n    Args:\n        n_base: Number of base points for the regression line.\n        x_max: The maximum absolute value for the range of base x-points.\n        x_HL: The x-coordinate of the high-leverage point.\n        delta_LL: The y-offset for the low-leverage, large-residual point.\n        epsilon_HL: The y-offset for the high-leverage point.\n\n    Returns:\n        A list containing [i_r, i_t, b1, b2]:\n        i_r: Index of the max absolute standardized residual.\n        i_t: Index of the max absolute studentized residual.\n        b1: Boolean, |t_i| > |r_i| for the low-leverage point.\n        b2: Boolean, |r_i| >= |t_i| for the high-leverage point.\n    \"\"\"\n    # True model parameters\n    beta0_true = 2.0\n    beta1_true = 1.5\n\n    # 1. Construct the dataset\n    # Base points on the true line\n    x_base = np.linspace(-x_max, x_max, n_base)\n    y_base = beta0_true + beta1_true * x_base\n    \n    # High-leverage point\n    x_hl = float(x_HL)\n    y_hl = (beta0_true + beta1_true * x_hl) + epsilon_HL\n    \n    # Low-leverage, large-residual point\n    x_ll = 0.0\n    y_ll = (beta0_true + beta1_true * x_ll) + delta_LL\n    \n    # Combine into the full dataset\n    x = np.concatenate((x_base, [x_hl, x_ll]))\n    y = np.concatenate((y_base, [y_hl, y_ll]))\n    \n    n = len(x)\n    p = 2  # Number of parameters (intercept beta0, slope beta1)\n    \n    # Indices of the special points\n    idx_hl = n_base\n    idx_ll = n_base + 1\n    \n    # 2. Fit OLS model from first principles\n    # Construct the design matrix X\n    X = np.c_[np.ones(n), x]\n    \n    # OLS estimator: beta_hat = (X'X)^-1 X'y\n    XTX_inv = np.linalg.inv(X.T @ X)\n    beta_hat = XTX_inv @ X.T @ y\n    \n    # Predicted values and residuals\n    y_hat = X @ beta_hat\n    e = y - y_hat\n    \n    # 3. Compute leverage values (h_ii)\n    # Hat matrix: H = X(X'X)^-1 X'\n    H = X @ XTX_inv @ X.T\n    h = np.diag(H)\n    \n    # 4. Compute standardized and studentized residuals\n    \n    # Unbiased estimator for error variance sigma^2\n    rss = e.T @ e\n    sigma2_hat = rss / (n - p)\n    sigma_hat = np.sqrt(sigma2_hat)\n    \n    # Standardized residuals: r_i = e_i / (sigma_hat * sqrt(1 - h_ii))\n    # Handle potential division by zero if h_ii is close to 1\n    sqrt_1_minus_h = np.sqrt(1 - h)\n    r = np.zeros_like(e)\n    valid_indices_r = (1 - h) > 1e-12\n    r[valid_indices_r] = e[valid_indices_r] / (sigma_hat * sqrt_1_minus_h[valid_indices_r])\n    \n    # Studentized residuals: t_i = r_i * sqrt((n - p - 1) / (n - p - r_i^2))\n    df_full = n - p\n    df_del = n - p - 1\n    \n    denom_t_sq = df_full - r**2\n    t = np.zeros_like(r)\n    \n    # Handle r_i^2 < n-p for valid square root\n    valid_indices_t = denom_t_sq > 1e-12\n    t[valid_indices_t] = r[valid_indices_t] * np.sqrt(df_del / denom_t_sq[valid_indices_t])\n    \n    # If r_i^2 -> n-p, t -> inf\n    infinite_indices = np.abs(denom_t_sq) <= 1e-12\n    t[infinite_indices] = np.inf * np.sign(r[infinite_indices])\n    \n    # 5. Determine the required outputs\n    i_r = int(np.argmax(np.abs(r)))\n    i_t = int(np.argmax(np.abs(t)))\n\n    b1 = bool(np.abs(t[idx_ll]) > np.abs(r[idx_ll]))\n    b2 = bool(np.abs(r[idx_hl]) >= np.abs(t[idx_hl]))\n    \n    return [i_r, i_t, b1, b2]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n_base, x_max, x_HL, delta_LL, epsilon_HL)\n        (12, 3, 15, 15, 0),\n        (10, 2, 50, 10, 0),\n        (12, 3, 8, 6, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack tuple into named arguments for clarity\n        n_base, x_max, x_HL, delta_LL, epsilon_HL = case\n        result = calculate_residuals(n_base, x_max, x_HL, delta_LL, epsilon_HL)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The str() conversion of a list includes spaces, which is standard.\n    # The problem example is symbolic; this literal interpretation of the template is safest.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "It is a common misconception that two models with an identical fit metric, such as Root Mean Squared Error (RMSE), are diagnostically equivalent. This advanced practice  delves into this subtlety by having you construct two models that share the exact same residuals and RMSE but possess different leverage structures. By comparing the distributions of their studentized residuals, you will see how model complexity can alter outlier diagnostics and perceived robustness, even when the overall goodness-of-fit remains unchanged.",
            "id": "3176914",
            "problem": "Consider the Gaussian linear model $y = X\\beta + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I_n)$ and ordinary least squares (OLS) estimation. Let $n$ denote the number of observations, and let $X$ denote the design matrix that includes an intercept column. Residual analysis in statistical learning often relies on standardized and studentized residuals for diagnosing model adequacy, outliers, and robustness. For this task, you will simulate two OLS models that have identical Root Mean Squared Error (RMSE) but differ in their leverage structure, which will lead to different distributions of absolute externally studentized residuals $|t_i|$.\n\nStarting from the linear model assumptions and the projection properties of the OLS hat matrix, derive the appropriate residual diagnostics from first principles (that is, without using pre-stated shortcut formulas in the problem statement). Then implement the following steps in a program:\n\n1. Generate a synthetic dataset with $n$ observations, one predictor $x$, and an intercept, with the response $y$ generated by a linear signal plus independent Gaussian noise. Fit OLS to obtain Model A with design matrix $X_A = [\\mathbf{1}, x]$.\n\n2. Construct a third predictor $z$ that is linearly independent of the columns in $X_A$ and orthogonal to the residual vector produced by Model A. Fit OLS to obtain Model B with design matrix $X_B = [\\mathbf{1}, x, z]$. The construction must ensure that Model A and Model B have identical RMSE while the leverage values differ across models.\n\n3. For both models, compute the externally studentized residuals $t_i$ as dictated by the linear model assumptions and leave-one-out variance estimation logic.\n\n4. For a test suite, evaluate four cases, each specified by a threshold $\\tau$ and a scaling factor $s_z$ applied to the constructed predictor $z$. For each case, compute:\n   - The absolute difference in RMSE between Model A and Model B, denoted $|\\mathrm{RMSE}_A - \\mathrm{RMSE}_B|$.\n   - The difference in proportions of observations with $|t_i|$ exceeding the threshold $\\tau$, computed as $\\text{proportion}_B - \\text{proportion}_A$.\n\n5. Scientific realism and reproducibility requirements:\n   - Use a fixed random seed.\n   - Use a reasonable sample size $n$ and noise scale to ensure the scenario is realistic and self-consistent.\n\n6. Final output specification:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\dots]$).\n   - Each test case contributes two floats to the output in the following order: first $|\\mathrm{RMSE}_A - \\mathrm{RMSE}_B|$, then $(\\text{proportion}_B - \\text{proportion}_A)$.\n   - All floats must be rounded to six decimal places.\n\nUse the following test suite of parameters:\n- Case $1$: $\\tau = 2.0$, $s_z = 5.0$.\n- Case $2$: $\\tau = 2.5$, $s_z = 10.0$.\n- Case $3$: $\\tau = 3.0$, $s_z = 10.0$.\n- Case $4$: $\\tau = 4.0$, $s_z = 20.0$.\n\nYour implementation must adhere to numerical correctness and produce the specified output format without additional text.",
            "solution": "The solution proceeds in three stages. First, we establish the theoretical relationship between Model A and Model B, confirming the conditions under which their RMSE is identical. Second, we derive the formula for the externally studentized residual from first principles. Third, we combine these theoretical results to design an algorithm that is then implemented.\n\n**1. Equivalence of RMSE and Orthogonality Condition**\n\nLet $X_A$ be the $n \\times p_A$ design matrix for Model A, and let $y$ be the $n \\times 1$ response vector. The OLS fitted values are $\\hat{y}_A = P_A y$ and the residuals are $e_A = y - \\hat{y}_A = (I - P_A)y$, where $P_A = X_A(X_A^T X_A)^{-1}X_A^T$ is the projection matrix onto the column space of $X_A$, denoted $\\mathcal{C}(X_A)$. The Sum of Squared Errors is $\\mathrm{SSE}_A = e_A^T e_A = y^T(I - P_A)y$. The RMSE is $\\sqrt{\\mathrm{SSE}_A/n}$.\n\nModel B is formed by adding a new predictor $z$ to Model A, such that its design matrix is $X_B = [X_A, z]$, an $n \\times p_B$ matrix where $p_B = p_A + 1$. Its projection matrix is $P_B = X_B(X_B^T X_B)^{-1}X_B^T$. Since $\\mathcal{C}(X_A)$ is a subspace of $\\mathcal{C}(X_B)$, the projection matrices are related. Let $z_{\\perp}$ be the component of $z$ that is orthogonal to $\\mathcal{C}(X_A)$, given by $z_{\\perp} = (I - P_A)z$. The projection matrix $P_B$ can be expressed as $P_B = P_A + P_{z_{\\perp}}$, where $P_{z_{\\perp}} = \\frac{z_{\\perp}z_{\\perp}^T}{z_{\\perp}^T z_{\\perp}}$ is the projection matrix onto the space spanned by $z_{\\perp}$.\n\nThe SSE for Model B is $\\mathrm{SSE}_B = y^T(I - P_B)y$. Using the relation between the projection matrices:\n$$ \\mathrm{SSE}_B = y^T(I - P_A - P_{z_{\\perp}})y = y^T(I-P_A)y - y^T P_{z_{\\perp}} y = \\mathrm{SSE}_A - \\frac{y^T z_{\\perp} z_{\\perp}^T y}{z_{\\perp}^T z_{\\perp}} = \\mathrm{SSE}_A - \\frac{(z_{\\perp}^T y)^2}{z_{\\perp}^T z_{\\perp}} $$\nThe problem requires $\\mathrm{RMSE}_A = \\mathrm{RMSE}_B$, which is equivalent to $\\mathrm{SSE}_A = \\mathrm{SSE}_B$. This equality holds if and only if the second term is zero, which requires $z_{\\perp}^T y = 0$.\nWe can rewrite $z_{\\perp}^T y$ as:\n$$ z_{\\perp}^T y = ((I - P_A)z)^T y = z^T(I - P_A)^T y = z^T(I - P_A)y = z^T e_A $$\nThus, the condition $\\mathrm{RMSE}_A = \\mathrm{RMSE}_B$ is mathematically equivalent to the condition $z^T e_A = 0$, meaning the new predictor $z$ must be orthogonal to the residual vector of Model A. A direct consequence of $z^T e_A = 0$ is that the fitted values are identical: $\\hat{y}_B = P_B y = (P_A + P_{z_{\\perp}})y = P_A y + \\frac{z_{\\perp}(z_{\\perp}^T y)}{z_{\\perp}^T z_{\\perp}} = \\hat{y}_A$. If the fitted values are identical, the residuals must also be identical: $e_B = y - \\hat{y}_B = y - \\hat{y}_A = e_A$.\n\n**2. Derivation of the Externally Studentized Residual**\n\nThe externally studentized residual for the $i$-th observation, $t_i$, is defined as:\n$$ t_i = \\frac{e_i}{\\hat{\\sigma}_{(i)} \\sqrt{1 - h_{ii}}} $$\nHere, $e_i = y_i - \\hat{y}_i$ is the ordinary residual, $h_{ii}$ is the $i$-th diagonal element of the hat matrix $P = X(X^T X)^{-1}X^T$ (the leverage of observation $i$), and $\\hat{\\sigma}_{(i)}^2$ is the unbiased estimate of the error variance $\\sigma^2$ obtained by fitting the model without the $i$-th observation. The number of parameters is $p$.\n\nThe variance estimate using all $n$ observations is $\\hat{\\sigma}^2 = \\frac{\\mathrm{SSE}}{n-p}$. A key identity connects $\\hat{\\sigma}_{(i)}^2$ to the full-dataset quantities:\n$$ (n-p-1)\\hat{\\sigma}_{(i)}^2 = (n-p)\\hat{\\sigma}^2 - \\frac{e_i^2}{1-h_{ii}} $$\nSubstituting $\\hat{\\sigma}^2 = \\frac{\\mathrm{SSE}}{n-p}$, we get:\n$$ (n-p-1)\\hat{\\sigma}_{(i)}^2 = \\mathrm{SSE} - \\frac{e_i^2}{1-h_{ii}} \\implies \\hat{\\sigma}_{(i)}^2 = \\frac{\\mathrm{SSE} - \\frac{e_i^2}{1-h_{ii}}}{n-p-1} $$\nNow we substitute this expression back into the definition of $t_i$. Squaring $t_i$ for convenience:\n$$ t_i^2 = \\frac{e_i^2}{\\hat{\\sigma}_{(i)}^2 (1 - h_{ii})} = \\frac{e_i^2}{\\left( \\frac{\\mathrm{SSE} - \\frac{e_i^2}{1-h_{ii}}}{n-p-1} \\right) (1-h_{ii})} = \\frac{e_i^2 (n-p-1)}{\\left(\\mathrm{SSE} - \\frac{e_i^2}{1-h_{ii}}\\right)(1-h_{ii})} $$\n$$ t_i^2 = \\frac{e_i^2 (n-p-1)}{\\mathrm{SSE}(1-h_{ii}) - e_i^2} $$\nTaking the square root (and preserving the sign of $e_i$) gives the desired computational formula:\n$$ t_i = e_i \\sqrt{\\frac{n-p-1}{\\mathrm{SSE}(1-h_{ii}) - e_i^2}} $$\nThis formula allows for the efficient calculation of all $t_i$ without re-fitting the model $n$ times.\n\n**3. Algorithmic Design and Analysis**\n\nFor our specific problem, we have $p_A = 2$ and $p_B = 3$. The residuals $e_i$ and the total SSE are identical for both models. However, the leverages and degrees of freedom differ.\nThe leverage for Model B is $h_{ii,B} = (P_B)_{ii} = (P_A + P_{z_\\perp})_{ii} = h_{ii,A} + (P_{z_\\perp})_{ii} = h_{ii,A} + \\frac{z_{\\perp,i}^2}{z_{\\perp}^T z_{\\perp}}$. Since the second term is non-negative, we have $h_{ii,B} \\geq h_{ii,A}$.\n\nA critical point is the role of the scaling factor $s_z$. The predictor used in Model B is $s_z z_{base}$, where $z_{base}^T e_A = 0$. The projection matrix $P_B$ depends on the column space $\\mathcal{C}(X_B) = \\mathcal{C}([X_A, s_z z_{base}])$. Because scaling a column vector does not change the space it spans, $\\mathcal{C}([X_A, s_z z_{base}]) = \\mathcal{C}([X_A, z_{base}])$ for any $s_z \\neq 0$. Therefore, the projection matrix $P_B$, its diagonal elements $h_{ii,B}$, and consequently the studentized residuals $t_{i,B}$ are all invariant to the choice of $s_z$. The parameter $s_z$ is a red herring with respect to the values of $t_{i,B}$. The differences across test cases will arise only from the changing threshold $\\tau$.\n\nThe algorithm is as follows:\n1.  Initialize a random number generator with a fixed seed. Define model parameters $n$, true coefficients, and noise variance $\\sigma^2$.\n2.  Generate data: predictor $x$ and response $y = \\beta_0 + \\beta_1 x + \\varepsilon$. Construct $X_A = [\\mathbf{1}, x]$.\n3.  Fit Model A: Compute $e_A = (I - P_A)y$, $\\mathrm{SSE} = e_A^T e_A$, and leverages $h_{ii,A}$.\n4.  Construct a base predictor $z_{base}$ orthogonal to $e_A$ by generating a random vector $w$ and applying Gram-Schmidt: $z_{base} = w - \\frac{e_A^T w}{e_A^T e_A} e_A$.\n5.  Construct Model B's static components: Form $X_{B,base} = [X_A, z_{base}]$ and compute the corresponding hat matrix $P_B$ and leverages $h_{ii,B}$. These are independent of $s_z$.\n6.  Pre-calculate the studentized residuals for both models using the derived formula:\n    -   $t_{i,A} = e_i \\sqrt{\\frac{n-2-1}{\\mathrm{SSE}(1-h_{ii,A}) - e_i^2}}$ for all $i=1, \\dots, n$.\n    -   $t_{i,B} = e_i \\sqrt{\\frac{n-3-1}{\\mathrm{SSE}(1-h_{ii,B}) - e_i^2}}$ for all $i=1, \\dots, n$.\n7.  Iterate through each test case $(\\tau, s_z)$:\n    -   The absolute difference in RMSE between the models is $|\\mathrm{RMSE}_A - \\mathrm{RMSE}_B| = 0$ by construction. Numerically, this will be a very small number close to machine precision. We compute it for verification.\n    -   Calculate $\\text{proportion}_A = \\frac{1}{n} \\sum_{i=1}^n I(|t_{i,A}| > \\tau)$.\n    -   Calculate $\\text{proportion}_B = \\frac{1}{n} \\sum_{i=1}^n I(|t_{i,B}| > \\tau)$.\n    -   Store the difference $\\text{proportion}_B - \\text{proportion}_A$.\n8.  Format and print the collected results.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the specified procedure to compare OLS models with identical RMSE but different leverage.\n    \"\"\"\n    # 5. Scientific realism and reproducibility requirements\n    np.random.seed(42)\n    n = 200  # Reasonable sample size\n    true_beta = np.array([2.0, 3.5])\n    noise_sigma = 5.0\n\n    # 1. Generate a synthetic dataset for Model A\n    x = np.random.normal(loc=0.0, scale=2.0, size=n)\n    X_A = np.c_[np.ones(n), x]\n    noise = np.random.normal(loc=0.0, scale=noise_sigma, size=n)\n    y = X_A @ true_beta + noise\n\n    # Fit OLS to obtain Model A\n    # P_A = X_A @ inv(X_A.T @ X_A) @ X_A.T\n    P_A = X_A @ np.linalg.pinv(X_A)\n    # hat values are the diagonal of P_A\n    h_A = np.diag(P_A)\n    # residuals e_A = (I - P_A)y\n    e_A = y - P_A @ y\n    # Sum of Squared Errors\n    sse_A = e_A.T @ e_A\n    # Root Mean Squared Error\n    rmse_A = np.sqrt(sse_A / n)\n    p_A = X_A.shape[1] # Number of parameters for model A\n\n    # 2. Construct a third predictor z for Model B\n    # z must be orthogonal to e_A to ensure RMSE is identical.\n    # We construct a base vector z_base and scale it later with s_z.\n    # Generate a random vector w and make it orthogonal to e_A.\n    w = np.random.normal(size=n)\n    z_base = w - (e_A.T @ w) / (e_A.T @ e_A) * e_A\n\n    # Pre-calculate components for Model B that are invariant to scaling factor s_z\n    # The hat matrix P_B depends on the column space of X_B, which is invariant to scaling z.\n    X_B_base = np.c_[X_A, z_base]\n    # P_B = X_B_base @ inv(X_B_base.T @ X_B_base) @ X_B_base.T\n    P_B = X_B_base @ np.linalg.pinv(X_B_base)\n    h_B = np.diag(P_B)\n    e_B = y - P_B @ y\n    sse_B = e_B.T @ e_B\n    rmse_B = np.sqrt(sse_B / n)\n    p_B = X_B_base.shape[1] # Number of parameters for model B\n    \n    # 3. Compute externally studentized residuals for both models from first principles\n    # Formula: t_i = e_i * sqrt((n-p-1) / (SSE*(1-h_ii) - e_i^2))\n    \n    # SSE is identical for both models by construction\n    sse = sse_A \n    # Residuals are identical for both models by construction\n    residuals = e_A\n\n    # For Model A (p_A = 2)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        denom_A = sse * (1 - h_A) - residuals**2\n        t_A = residuals * np.sqrt((n - p_A - 1) / denom_A)\n        t_A = np.nan_to_num(t_A, nan=np.inf) # Handle potential division by zero\n    \n    # For Model B (p_B = 3)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        denom_B = sse * (1 - h_B) - residuals**2\n        t_B = residuals * np.sqrt((n - p_B - 1) / denom_B)\n        t_B = np.nan_to_num(t_B, nan=np.inf) # Handle potential division by zero\n\n    # 4. Evaluate the test suite\n    test_cases = [\n        (2.0, 5.0),   # Case 1: tau, s_z\n        (2.5, 10.0),  # Case 2\n        (3.0, 10.0),  # Case 3\n        (4.0, 20.0),  # Case 4\n    ]\n\n    results = []\n    for tau, s_z in test_cases:\n        # Note: s_z does not affect h_B or t_B, so it's a red herring for these calculations.\n        # The RMSE difference is constant because the construction of z_base ensures it.\n        rmse_diff = np.abs(rmse_A - rmse_B)\n        \n        prop_A = np.mean(np.abs(t_A) > tau)\n        prop_B = np.mean(np.abs(t_B) > tau)\n        prop_diff = prop_B - prop_A\n\n        results.append(round(rmse_diff, 6))\n        results.append(round(prop_diff, 6))\n\n    # 6. Final output specification\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}