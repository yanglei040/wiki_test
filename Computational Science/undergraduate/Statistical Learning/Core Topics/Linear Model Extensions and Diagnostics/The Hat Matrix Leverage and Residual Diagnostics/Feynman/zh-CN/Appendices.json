{
    "hands_on_practices": [
        {
            "introduction": "本节的第一个练习将带你回归基础。我们将为一个小型数据集手动构建帽子矩阵 $H$，并用它来探索残差生成矩阵 $(I-H)$。这个练习对于理解普通残差为何相关且方差不相等至关重要，因为它揭示了单个观测值 $y_k$ 的变化是如何分布到整个残差向量中的 。",
            "id": "3183487",
            "problem": "考虑一个固定设计的线性回归，有 $n=5$ 个观测值和 $p=3$ 个预测变量。设设计矩阵 $X \\in \\mathbb{R}^{5 \\times 3}$ 的列向量 $c_{0}, c_{1}, c_{2}$ 定义如下\n$$\nc_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nc_{1} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\quad\nc_{2} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -1 \\\\ 0 \\end{pmatrix},\n$$\n因此\n$$\nX = \\begin{pmatrix}\n1  1  1 \\\\\n1  -1  1 \\\\\n1  1  -1 \\\\\n1  -1  -1 \\\\\n1  0  0\n\\end{pmatrix}.\n$$\n可以假设响应向量 $y \\in \\mathbb{R}^{5}$ 是任意的，并且最小二乘拟合向量是在标准欧几里得内积下，$y$ 在 $X$ 的列空间上的正交投影。残差向量为 $e = y - \\hat{y}$。\n\n任务：\n1. 显式地构造在 $\\mathbb{R}^{5}$ 上的线性算子，该算子将任意 $y$ 映射到其在 $X$ 列空间中的最小二乘拟合值 $\\hat{y}$。对于给定的 $X$，精确计算这个算子。\n2. 使用第1部分得到的算子，分析对 $y$ 形如 $y = y_{0} + \\delta v$ 的扰动，其中 $y_{0}$ 是固定的，$\\delta \\in \\mathbb{R}$ 是一个标量，而 $v$ 是 $\\mathbb{R}^{5}$ 中的一个标准基向量 $e_{k}$（即第 $k$ 个坐标方向）。确定残差向量 $e = (I - H) y$ 如何响应每个坐标方向的扰动。\n3. 对于 $\\delta = 3$ 和 $v = e_{3}$ 的特定扰动，确定观测值 $i=5$ 处的残差变化。\n\n请以一个精确的数字作为最终答案。不需要四舍五入，本题中没有物理单位。",
            "solution": "该问题被评估为有效，因为它在科学上基于线性回归理论，问题表述清晰且具有唯一解，并且形式上没有歧义或矛盾。我们开始解答。\n\n该问题要求对由设计矩阵 $X$ 定义的线性回归模型进行三部分分析。这三个任务是：\n1.  构造到 $X$ 的列空间上的投影算子，即帽子矩阵 $H$。\n2.  分析残差向量对响应向量 $y$ 中特定类型扰动的响应。\n3.  计算给定扰动下特定残差的变化。\n\n设线性模型为 $y = X\\beta + \\epsilon$，其中 $y \\in \\mathbb{R}^{5}$ 是响应向量，$X \\in \\mathbb{R}^{5 \\times 3}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^{3}$ 是系数向量，$\\epsilon \\in \\mathbb{R}^{5}$ 是误差向量。$\\beta$ 的最小二乘估计为 $\\hat{\\beta} = (X^T X)^{-1}X^T y$。拟合值由 $\\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1}X^T y$ 给出。\n\n**第1部分：构造帽子矩阵**\n\n将 $y$ 映射到 $\\hat{y}$ 的线性算子是帽子矩阵 $H$，定义为：\n$$ H = X(X^T X)^{-1}X^T $$\n给定设计矩阵：\n$$ X = \\begin{pmatrix}\n1  1  1 \\\\\n1  -1  1 \\\\\n1  1  -1 \\\\\n1  -1  -1 \\\\\n1  0  0\n\\end{pmatrix} $$\n首先，我们计算矩阵 $X^T X$：\n$$ X^T X = \\begin{pmatrix}\n1  1  1  1  1 \\\\\n1  -1  1  -1  0 \\\\\n1  1  -1  -1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1  1 \\\\\n1  -1  1 \\\\\n1  1  -1 \\\\\n1  -1  -1 \\\\\n1  0  0\n\\end{pmatrix} $$\n$X$ 的列向量，记为 $c_0, c_1, c_2$，是正交的。我们可以通过计算 $X^T X$ 的非对角元素来验证这一点：\n$c_0^T c_1 = 1-1+1-1+0 = 0$\n$c_0^T c_2 = 1+1-1-1+0 = 0$\n$c_1^T c_2 = 1-1-1+1+0 = 0$\n对角元素是列向量的平方范数：\n$c_0^T c_0 = 1^2+1^2+1^2+1^2+1^2 = 5$\n$c_1^T c_1 = 1^2+(-1)^2+1^2+(-1)^2+0^2 = 4$\n$c_2^T c_2 = 1^2+1^2+(-1)^2+(-1)^2+0^2 = 4$\n因此，$X^T X$ 是一个对角矩阵：\n$$ X^T X = \\begin{pmatrix}\n5  0  0 \\\\\n0  4  0 \\\\\n0  0  4\n\\end{pmatrix} $$\n其逆矩阵 $(X^T X)^{-1}$ 也是一个对角矩阵，对角线元素是原对角线元素的倒数：\n$$ (X^T X)^{-1} = \\begin{pmatrix}\n\\frac{1}{5}  0  0 \\\\\n0  \\frac{1}{4}  0 \\\\\n0  0  \\frac{1}{4}\n\\end{pmatrix} $$\n现在我们可以计算帽子矩阵 $H$。由于 $X$ 的列向量是正交的，$H$ 可以写成到每个列向量上的投影矩阵之和：\n$$ H = \\sum_{k=0}^{2} \\frac{c_k c_k^T}{c_k^T c_k} $$\n或者，帽子矩阵的元素 $H_{ij}$ 由 $H_{ij} = X_i (X^T X)^{-1} X_j^T$ 给出，其中 $X_i$ 是 $X$ 的第 $i$ 行。使用推导出的 $(X^TX)^{-1}$：\n$$ H_{ij} = \\frac{x_{i1} x_{j1}}{5} + \\frac{x_{i2} x_{j2}}{4} + \\frac{x_{i3} x_{j3}}{4} $$\n让我们计算 $H$ 的元素：\n对角元素（杠杆值）$h_{ii} = H_{ii}$：\n$h_{11} = h_{22} = h_{33} = h_{44} = \\frac{1^2}{5} + \\frac{(\\pm 1)^2}{4} + \\frac{(\\pm 1)^2}{4} = \\frac{1}{5} + \\frac{1}{4} + \\frac{1}{4} = \\frac{7}{10}$。\n$h_{55} = \\frac{1^2}{5} + \\frac{0^2}{4} + \\frac{0^2}{4} = \\frac{1}{5} = \\frac{2}{10}$。\n非对角元素 $h_{ij} = H_{ij}$（当 $i \\ne j$ 时）：\n$h_{12} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot (-1)}{4} + \\frac{1 \\cdot 1}{4} = \\frac{1}{5} = \\frac{2}{10}$。\n$h_{13} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot 1}{4} + \\frac{1 \\cdot (-1)}{4} = \\frac{1}{5} = \\frac{2}{10}$。\n$h_{14} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot (-1)}{4} + \\frac{1 \\cdot (-1)}{4} = \\frac{1}{5} - \\frac{1}{2} = -\\frac{3}{10}$。\n$h_{15} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot 0}{4} + \\frac{1 \\cdot 0}{4} = \\frac{1}{5} = \\frac{2}{10}$。\n$h_{23} = \\frac{1 \\cdot 1}{5} + \\frac{(-1) \\cdot 1}{4} + \\frac{1 \\cdot (-1)}{4} = \\frac{1}{5} - \\frac{1}{2} = -\\frac{3}{10}$。\n$h_{24} = \\frac{1 \\cdot 1}{5} + \\frac{(-1) \\cdot (-1)}{4} + \\frac{1 \\cdot (-1)}{4} = \\frac{1}{5} = \\frac{2}{10}$。\n$h_{25} = \\frac{1 \\cdot 1}{5} + \\frac{(-1) \\cdot 0}{4} + \\frac{1 \\cdot 0}{4} = \\frac{1}{5} = \\frac{2}{10}$。\n$h_{34} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot (-1)}{4} + \\frac{(-1) \\cdot (-1)}{4} = \\frac{1}{5} = \\frac{2}{10}$。\n$h_{35} = \\frac{1 \\cdot 1}{5} + \\frac{1 \\cdot 0}{4} + \\frac{(-1) \\cdot 0}{4} = \\frac{1}{5} = \\frac{2}{10}$。\n$h_{45} = \\frac{1 \\cdot 1}{5} + \\frac{(-1) \\cdot 0}{4} + \\frac{(-1) \\cdot 0}{4} = \\frac{1}{5} = \\frac{2}{10}$。\n利用对称性（$h_{ij}=h_{ji}$），完整的帽子矩阵是：\n$$ H = \\frac{1}{10} \\begin{pmatrix}\n7  2  2  -3  2 \\\\\n2  7  -3  2  2 \\\\\n2  -3  7  2  2 \\\\\n-3  2  2  7  2 \\\\\n2  2  2  2  2\n\\end{pmatrix} $$\n\n**第2部分：扰动分析**\n\n残差向量为 $e = y - \\hat{y}$。由于 $\\hat{y} = Hy$，残差向量由 $e = y - Hy = (I - H)y$ 给出，其中 $I$ 是 $5 \\times 5$ 的单位矩阵。\n设原始响应向量为 $y_0$，其对应的残差向量为 $e_0 = (I-H)y_0$。\n响应向量被扰动为 $y' = y_0 + \\delta v$，其中 $v$ 是一个标准基向量 $e_k$。新的残差向量 $e'$ 为：\n$$ e' = (I-H)y' = (I-H)(y_0 + \\delta e_k) $$\n根据算子 $(I-H)$ 的线性性质，我们有：\n$$ e' = (I-H)y_0 + \\delta(I-H)e_k = e_0 + \\delta(I-H)e_k $$\n残差向量的变化为 $\\Delta e = e' - e_0$：\n$$ \\Delta e = \\delta(I-H)e_k $$\n向量 $(I-H)e_k$ 就是矩阵 $(I-H)$ 的第 $k$ 列。因此，对第 $k$ 个观测值 $y_k$ 的大小为 $\\delta$ 的扰动，会使残差向量改变 $\\delta$ 乘以 $(I-H)$ 的第 $k$ 列。第 $i$ 个残差的变化 $\\Delta e_i$ 由 $\\Delta e_i = \\delta((I-H)e_k)_i = \\delta(I-H)_{ik}$ 给出。\n\n矩阵 $I-H$ 是：\n$$ I-H = \\frac{1}{10} \\begin{pmatrix}\n10  0  0  0  0 \\\\\n0  10  0  0  0 \\\\\n0  0  10  0  0 \\\\\n0  0  0  10  0 \\\\\n0  0  0  0  10\n\\end{pmatrix} - \\frac{1}{10} \\begin{pmatrix}\n7  2  2  -3  2 \\\\\n2  7  -3  2  2 \\\\\n2  -3  7  2  2 \\\\\n-3  2  2  7  2 \\\\\n2  2  2  2  2\n\\end{pmatrix} $$\n$$ I-H = \\frac{1}{10} \\begin{pmatrix}\n3  -2  -2  3  -2 \\\\\n-2  3  3  -2  -2 \\\\\n-2  3  3  -2  -2 \\\\\n3  -2  -2  3  -2 \\\\\n-2  -2  -2  -2  8\n\\end{pmatrix} $$\n\n**第3部分：特定扰动计算**\n\n我们被要求计算在特定扰动下，观测值 $i=5$ 处的残差变化。\n给定的扰动为 $\\delta = 3$ 和 $v = e_3$。这意味着扰动施加于第3个观测值，所以 $k=3$。我们关心的是第5个残差的变化，所以 $i=5$。\n使用第2部分推导的公式：\n$$ \\Delta e_i = \\delta(I-H)_{ik} $$\n代入给定的值：\n$$ \\Delta e_5 = 3 \\cdot (I-H)_{53} $$\n从上面计算出的矩阵 $I-H$ 中，第5行第3列的元素是 $(I-H)_{53} = -\\frac{2}{10}$。\n因此，观测值 $i=5$ 处的残差变化为：\n$$ \\Delta e_5 = 3 \\cdot \\left(-\\frac{2}{10}\\right) = -\\frac{6}{10} = -\\frac{3}{5} $$",
            "answer": "$$\\boxed{-\\frac{3}{5}}$$"
        },
        {
            "introduction": "如果我们的数据集中在预测变量空间中包含相同的观测点，会发生什么？本练习通过比较复制一个数据点前后的模型来研究这种常见情况。通过分析帽子矩阵元素和杠杆率的变化，你将对杠杆率的含义有更深的直觉，并发现数据结构对模型预测施加的约束 。",
            "id": "3183500",
            "problem": "考虑一个由普通最小二乘法 (OLS) 估计的带截距项的简单线性回归，其中设计矩阵 $X$ 有两列：一列是截距，另一列是单个协变量 $x$。从 $n=3$ 的设计开始\n$$\nX_{\\text{before}}=\\begin{pmatrix}\n1  0\\\\\n1  1\\\\\n1  2\n\\end{pmatrix},\n$$\n以及响应向量\n$$\ny=\\begin{pmatrix}\n1\\\\\n3\\\\\n2\n\\end{pmatrix}.\n$$\n现在通过添加第四个观测值来构造一个重复设计，该观测值的协变量行与 $X_{\\text{before}}$ 的第二行相同，得到\n$$\nX_{\\text{after}}=\\begin{pmatrix}\n1  0\\\\\n1  1\\\\\n1  2\\\\\n1  1\n\\end{pmatrix},\n\\quad\ny_{\\text{after}}=\\begin{pmatrix}\n1\\\\\n3\\\\\n2\\\\\n5\n\\end{pmatrix},\n$$\n因此 $X_{\\text{after}}$ 的第 2 行和第 4 行是相同的。令 $H=X(X^{\\top}X)^{-1}X^{\\top}$ 为帽子矩阵，$h_{ii}$为其对角线元素（杠杆值），$e=(I-H)y$ 为残差向量。\n\n仅使用 OLS 和帽子矩阵的核心定义，而不调用任何现成的快捷公式，对重复设计 $X_{\\text{after}}$ 执行以下操作：推导重复行的杠杆值，并通过 $e=(I-H)y$ 解释 $X$ 中的相同行如何约束这些索引的拟合值和残差。作为比较的基准，也计算 $X_{\\text{before}}$ 的杠杆值 $h_{22}$。\n\n你的最终任务是报告由重复引起的重复协变量模式的杠杆值的确切变化，定义为\n$$\n\\Delta h \\equiv h_{22}^{(\\text{after})}-h_{22}^{(\\text{before})}.\n$$\n将最终答案表示为精确分数。不需要四舍五入。",
            "solution": "分析按要求分三部分进行。首先，我们计算初始设计中指定观测值的杠杆值。其次，我们对重复设计进行相同的计算，并解释其对模型的拟合值和残差产生的约束。最后，我们计算杠杆值的变化。\n\n帽子矩阵定义为 $H=X(X^{\\top}X)^{-1}X^{\\top}$，其中 $X$ 是设计矩阵。第 $i$ 个观测值的杠杆值 $h_{ii}$ 是 $H$ 的第 $i$ 个对角线元素。它可以直接计算为 $h_{ii} = x_i^{\\top}(X^{\\top}X)^{-1}x_i$，其中 $x_i^{\\top}$ 是 $X$ 的第 $i$ 行。\n\n**1. 初始设计 ($X_{\\text{before}}$) 的杠杆值计算**\n\n初始设计矩阵由下式给出：\n$$\nX_{\\text{before}} = \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2\n\\end{pmatrix}\n$$\n首先，我们计算矩阵 $X_{\\text{before}}^{\\top}X_{\\text{before}}$：\n$$\nX_{\\text{before}}^{\\top}X_{\\text{before}} = \\begin{pmatrix}\n1  1  1 \\\\\n0  1  2\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1  1 \\cdot 0 + 1 \\cdot 1 + 1 \\cdot 2 \\\\\n0 \\cdot 1 + 1 \\cdot 1 + 2 \\cdot 1  0 \\cdot 0 + 1 \\cdot 1 + 2 \\cdot 2\n\\end{pmatrix}\n= \\begin{pmatrix}\n3  3 \\\\\n3  5\n\\end{pmatrix}\n$$\n接下来，我们求该矩阵的逆。行列式为 $\\det(X_{\\text{before}}^{\\top}X_{\\text{before}}) = (3)(5) - (3)(3) = 15 - 9 = 6$。\n逆矩阵是：\n$$\n(X_{\\text{before}}^{\\top}X_{\\text{before}})^{-1} = \\frac{1}{6}\n\\begin{pmatrix}\n5  -3 \\\\\n-3  3\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{5}{6}  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  \\frac{1}{2}\n\\end{pmatrix}\n$$\n题目要求我们计算杠杆值 $h_{22}^{(\\text{before})}$。$X_{\\text{before}}$ 的第二行是 $x_2^{\\top} = \\begin{pmatrix} 1  1 \\end{pmatrix}$。\n$$\nh_{22}^{(\\text{before})} = x_2^{\\top}(X_{\\text{before}}^{\\top}X_{\\text{before}})^{-1}x_2 =\n\\begin{pmatrix}\n1  1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{5}{6}  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix}\n1 \\cdot \\frac{5}{6} + 1 \\cdot (-\\frac{1}{2})  1 \\cdot (-\\frac{1}{2}) + 1 \\cdot \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{5}{6} - \\frac{3}{6}  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{2}{6}  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\frac{1}{3}\n$$\n因此，原始设计中第二个观测值的杠杆值为 $h_{22}^{(\\text{before})} = \\frac{1}{3}$。\n\n**2. 重复设计 ($X_{\\text{after}}$) 的杠杆值和约束**\n\n重复设计矩阵为：\n$$\nX_{\\text{after}} = \\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2 \\\\\n1  1\n\\end{pmatrix}\n$$\n相应的响应向量为 $y_{\\text{after}} = \\begin{pmatrix} 1  3  2  5 \\end{pmatrix}^{\\top}$。$X_{\\text{after}}$ 的第 2 行和第 4 行是相同的。\n我们计算 $X_{\\text{after}}^{\\top}X_{\\text{after}}$：\n$$\nX_{\\text{after}}^{\\top}X_{\\text{after}} = \\begin{pmatrix}\n1  1  1  1 \\\\\n0  1  2  1\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2 \\\\\n1  1\n\\end{pmatrix}\n= \\begin{pmatrix}\n4  4 \\\\\n4  6\n\\end{pmatrix}\n$$\n行列式为 $\\det(X_{\\text{after}}^{\\top}X_{\\text{after}}) = (4)(6) - (4)(4) = 24 - 16 = 8$。\n逆矩阵是：\n$$\n(X_{\\text{after}}^{\\top}X_{\\text{after}})^{-1} = \\frac{1}{8}\n\\begin{pmatrix}\n6  -4 \\\\\n-4  4\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{3}{4}  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  \\frac{1}{2}\n\\end{pmatrix}\n$$\n重复的行是 $x_2^{\\top} = x_4^{\\top} = \\begin{pmatrix} 1  1 \\end{pmatrix}$。它们的杠杆值 $h_{22}^{(\\text{after})}$ 和 $h_{44}^{(\\text{after})}$ 必然相等。我们计算这个值：\n$$\nh_{22}^{(\\text{after})} = h_{44}^{(\\text{after})} =\n\\begin{pmatrix}\n1  1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{3}{4}  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix}\n\\frac{3}{4} - \\frac{1}{2}  -\\frac{1}{2} + \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{1}{4}  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n= \\frac{1}{4}\n$$\n重复行的杠杆值为 $h_{22}^{(\\text{after})} = h_{44}^{(\\text{after})} = \\frac{1}{4}$。\n\n现在我们解释对拟合值和残差的约束。令 $x_i^{\\top}$ 和 $x_j^{\\top}$ 为设计矩阵 $X$ 的两个相同行。第 $k$ 个观测值的拟合值为 $\\hat{y}_k = x_k^{\\top}\\hat{\\beta}$，其中 $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$。由于 $x_i^{\\top}=x_j^{\\top}$，显而易见，它们的拟合值必须相同：\n$$\n\\hat{y}_i = x_i^{\\top}\\hat{\\beta} = x_j^{\\top}\\hat{\\beta} = \\hat{y}_j\n$$\n这是 $X$ 中重复行施加的主要约束：模型必须对每个重复点产生相同的预测。\n\n这也可以通过帽子矩阵 $H$ 来解释。$H$ 的第 $(k, l)$ 个元素是 $H_{kl} = x_k^{\\top}(X^{\\top}X)^{-1}x_l$。如果 $x_i^{\\top} = x_j^{\\top}$，那么对于任何列索引 $l \\in \\{1, \\dots, n\\}$，我们有 $H_{il} = x_i^{\\top}(X^{\\top}X)^{-1}x_l = x_j^{\\top}(X^{\\top}X)^{-1}x_l = H_{jl}$。这意味着帽子矩阵的第 $i$ 行和第 $j$ 行是相同的。\n拟合值由向量 $\\hat{y} = Hy$ 给出。第 $i$ 个分量是 $\\hat{y}_i = \\sum_{l=1}^n H_{il}y_l$，第 $j$ 个分量是 $\\hat{y}_j = \\sum_{l=1}^n H_{jl}y_l$。由于第 $i$ 行 $H_{i\\cdot}$ 和第 $j$ 行 $H_{j\\cdot}$ 是相同的，因此必然有 $\\hat{y}_i = \\hat{y}_j$。对于我们的问题，这意味着 $\\hat{y}_2 = \\hat{y}_4$。\n\n残差定义为 $e = (I-H)y$，或者按元素方式，$e_k = y_k - \\hat{y}_k$。对于重复的索引 $i$ 和 $j$，我们有：\n$$\ne_i = y_i - \\hat{y}_i\n$$\n$$\ne_j = y_j - \\hat{y}_j\n$$\n由于 $\\hat{y}_i = \\hat{y}_j$，观测响应 $y_i$ 和 $y_j$ 之间的任何差异都直接转移到残差中：\n$$\ne_i - e_j = (y_i - \\hat{y}_i) - (y_j - \\hat{y}_j) = y_i - y_j\n$$\n在给定的问题中，$y_2 = 3$ 且 $y_4 = 5$，并且我们发现 $\\hat{y}_2 = \\hat{y}_4$。我们可以计算出该值 $\\hat{y}_2 = \\hat{y}_4 = 11/4$。残差为 $e_2 = 3 - \\frac{11}{4} = \\frac{1}{4}$ 和 $e_4 = 5 - \\frac{11}{4} = \\frac{9}{4}$。它们的差为 $e_2 - e_4 = \\frac{1}{4} - \\frac{9}{4} = -\\frac{8}{4} = -2$，这等于 $y_2 - y_4 = 3 - 5 = -2$。\n\n**3. 杠杆值的变化 ($\\Delta h$)**\n\n最后的任务是计算重复协变量模式的杠杆值变化。\n$$\n\\Delta h = h_{22}^{(\\text{after})} - h_{22}^{(\\text{before})}\n$$\n使用上面推导出的值：\n$$\n\\Delta h = \\frac{1}{4} - \\frac{1}{3} = \\frac{3}{12} - \\frac{4}{12} = -\\frac{1}{12}\n$$\n重复协变量为 $x=1$ 的观测值导致该协变量值上每个单独数据点的杠杆值降低。",
            "answer": "$$\n\\boxed{-\\frac{1}{12}}\n$$"
        },
        {
            "introduction": "一个常见的误解是，高杠杆率的点总是一个有问题的、具有高影响力的点。本练习旨在打破这一迷思，展示杠杆率与残差大小之间至关重要的相互作用。通过比较两种情况——一种是高杠杆率点很好地拟合模型，另一种则不然——你将亲眼看到为什么需要像库克距离这样的诊断指标来区分潜在影响（杠杆率）和实际影响 。",
            "id": "3183398",
            "problem": "考虑一个带截距的普通最小二乘线性回归模型。设设计矩阵为 $X \\in \\mathbb{R}^{n \\times k}$ 且为满列秩，响应向量为 $y \\in \\mathbb{R}^{n}$，拟合系数向量为 $\\hat{\\beta} \\in \\mathbb{R}^{k}$，其中 $\\hat{\\beta}$ 最小化残差平方和。将拟合值记为 $\\hat{y} \\in \\mathbb{R}^{n}$，残差向量记为 $e \\in \\mathbb{R}^{n}$，均方误差记为 $\\widehat{\\sigma}^{2}$。帽子矩阵是线性算子 $H \\in \\mathbb{R}^{n \\times n}$，它将观测响应 $y$ 映射到拟合响应 $\\hat{y}$，其对角线元素 $h_{ii}$ 称为杠杆值。Cook 距离 $D_{i}$ 用于衡量第 $i$ 个观测值对拟合回归的影响，其定义为删除第 $i$ 个观测值时拟合值或等价地估计系数的缩放变化。您将使用这些核心定义以及最小二乘法的正规方程，来推导可计算的表达式，并在一个小型测试集上对其进行评估。\n\n您的任务是：\n- 构建 $X$，计算 $\\hat{\\beta}$、$\\hat{y}$、$e$、$\\widehat{\\sigma}^{2}$ 和帽子矩阵 $H$，然后提取指定索引 $i$ 的杠杆值 $h_{ii}$。\n- 对于相同的索引 $i$，使用其定义（即删除第 $i$ 个观测值后拟合的缩放变化），计算 Cook 距离 $D_{i}$，并用 $e_{i}$、$h_{ii}$、$k$ 和 $\\widehat{\\sigma}^{2}$ 表示。\n- 通过下面的测试集表明，一个大的杠杆值 $h_{ii}$ 本身并不意味着一个大的 Cook 距离 $D_{i}$，因为 $D_{i}$ 同时取决于 $h_{ii}$ 和相应的残差 $e_{i}$。\n\n您可以使用的假设和基本事实：\n- 最小二乘估计量 $\\hat{\\beta}$ 满足正规方程 $X^{\\top} X \\hat{\\beta} = X^{\\top} y$。\n- 拟合值可以写成 $y$ 的线性映射，即 $\\hat{y} = H y$，其中矩阵 $H$ 仅依赖于 $X$ 并满足 $H^{2} = H$ 和 $H^{\\top} = H$。\n- 残差满足 $e = y - \\hat{y}$，均方误差为 $\\widehat{\\sigma}^{2} = \\lVert e \\rVert_{2}^{2} / (n - k)$。\n- 杠杆值 $h_{ii}$ 是 $H$ 的第 $i$ 个对角线元素，Cook 距离 $D_{i}$ 是通过删除第 $i$ 个观测值后拟合的缩放变化定义的标准影响度量。\n\n实现要求：\n- 对观测值索引 $i$ 使用从零开始的索引。\n- 对于每个测试用例，计算指定索引 $i$ 的配对 $(h_{ii}, D_{i})$，并将两者均报告为精确到 $6$ 位小数的浮点数。\n\n测试集规范：\n- 测试用例 $\\mathrm{A}$（高杠杆值，小残差）：\n  - 维度：$n = 8$，$k = 3$（一个截距项加两个预测变量）。\n  - $X$ 的行向量为 $[1, x_{1}, x_{2}]$，具体如下：\n    - 第 0 行：$[1, 0, 0]$\n    - 第 1 行：$[1, 1, 0]$\n    - 第 2 行：$[1, 1, 1]$\n    - 第 3 行：$[1, 2, 1]$\n    - 第 4 行：$[1, 0, 2]$\n    - 第 5 行：$[1, 2, 2]$\n    - 第 6 行：$[1, 1, 1]$\n    - 第 7 行：$[1, 10, 10]$\n  - 真实系数（仅用于合成 $y$）：$\\beta_{\\mathrm{true}} = [1.0, 2.0, -1.0]$。\n  - 响应 $y$：\n    - $y_{0} = 1.0 + 0 \\cdot 2.0 + 0 \\cdot (-1.0) + 0.1 = 1.1$\n    - $y_{1} = 1.0 + 1 \\cdot 2.0 + 0 \\cdot (-1.0) - 0.2 = 2.8$\n    - $y_{2} = 1.0 + 1 \\cdot 2.0 + 1 \\cdot (-1.0) + 0.05 = 2.05$\n    - $y_{3} = 1.0 + 2 \\cdot 2.0 + 1 \\cdot (-1.0) - 0.05 = 3.95$\n    - $y_{4} = 1.0 + 0 \\cdot 2.0 + 2 \\cdot (-1.0) + 0.0 = -1.0$\n    - $y_{5} = 1.0 + 2 \\cdot 2.0 + 2 \\cdot (-1.0) + 0.1 = 1.1$\n    - $y_{6} = 1.0 + 1 \\cdot 2.0 + 1 \\cdot (-1.0) - 0.1 = 1.9$\n    - $y_{7} = 1.0 + 10 \\cdot 2.0 + 10 \\cdot (-1.0) + 0.0 = 11.0$\n  - 索引：$i = 7$。\n\n- 测试用例 $\\mathrm{B}$（高杠杆值，大残差）：\n  - $X$ 与测试用例 $\\mathrm{A}$ 相同。\n  - $y$ 与测试用例 $\\mathrm{A}$ 相同，除了最后一个条目：\n    - $y_{7} = 21.0$。\n  - 索引：$i = 7$。\n\n- 测试用例 $\\mathrm{C}$（无极端杠杆值，中等残差）：\n  - 维度：$n = 8$，$k = 3$（一个截距项加两个预测变量）。\n  - $X$ 的行向量为 $[1, x_{1}, x_{2}]$，具体如下：\n    - 第 0 行：$[1, 0.0, 0.0]$\n    - 第 1 行：$[1, 1.0, 0.0]$\n    - 第 2 行：$[1, 1.0, 1.0]$\n    - 第 3 行：$[1, 2.0, 1.0]$\n    - 第 4 行：$[1, 0.0, 2.0]$\n    - 第 5 行：$[1, 2.0, 2.0]$\n    - 第 6 行：$[1, 1.0, 1.0]$\n    - 第 7 行：$[1, 0.5, 1.5]$\n  - 真实系数（仅用于合成 $y$）：$\\beta_{\\mathrm{true}} = [1.0, 2.0, -1.0]$。\n  - 响应 $y$：\n    - $y_{0} = 1.0 + 0.0 \\cdot 2.0 + 0.0 \\cdot (-1.0) + 0.1 = 1.1$\n    - $y_{1} = 1.0 + 1.0 \\cdot 2.0 + 0.0 \\cdot (-1.0) - 0.2 = 2.8$\n    - $y_{2} = 1.0 + 1.0 \\cdot 2.0 + 1.0 \\cdot (-1.0) + 0.05 = 2.05$\n    - $y_{3} = 1.0 + 2.0 \\cdot 2.0 + 1.0 \\cdot (-1.0) - 0.05 = 3.95$\n    - $y_{4} = 1.0 + 0.0 \\cdot 2.0 + 2.0 \\cdot (-1.0) + 0.0 = -1.0$\n    - $y_{5} = 1.0 + 2.0 \\cdot 2.0 + 2.0 \\cdot (-1.0) + 0.1 = 1.1$\n    - $y_{6} = 1.0 + 1.0 \\cdot 2.0 + 1.0 \\cdot (-1.0) - 0.1 = 1.9$\n    - $y_{7} = 1.0 + 0.5 \\cdot 2.0 + 1.5 \\cdot (-1.0) + 0.05 = 0.55$\n  - 索引：$i = 2$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按 $[h_{\\mathrm{A}}, D_{\\mathrm{A}}, h_{\\mathrm{B}}, D_{\\mathrm{B}}, h_{\\mathrm{C}}, D_{\\mathrm{C}}]$ 的顺序排列，其中每个条目都是精确到 $6$ 位小数的浮点数。例如，打印的字符串必须看起来像 $[\\dots]$，条目之间用逗号分隔，没有多余的空格。\n\n您必须在代码注释中解释并用数值验证的内容：\n- 在测试用例 $\\mathrm{A}$ 中，行向量 $[1, 10, 10]$ 在预测变量空间中偏离较远，产生了较大的杠杆值 $h_{77}$，但由于残差 $e_{7}$ 很小，Cook 距离 $D_{7}$ 也很小。\n- 在测试用例 $\\mathrm{B}$ 中，同样大的杠杆值 $h_{77}$ 与一个大的残差 $e_{7}$ 相结合，产生了一个大的 Cook 距离 $D_{7}$。\n- 在测试用例 $\\mathrm{C}$ 中，没有极端的预测变量值，杠杆值 $h_{22}$ 和 Cook 距离 $D_{2}$ 预计都将是中等大小。",
            "solution": "该问题要求计算几种普通最小二乘（OLS）线性回归场景下的杠杆值和 Cook 距离，以证明它们之间的关系。我们首先对所需量进行形式化定义。\n\nOLS 模型由 $y = X\\beta + \\epsilon$ 给出，其中 $y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times k}$ 是满列秩的设计矩阵，$\\beta \\in \\mathbb{R}^{k}$ 是系数向量，$\\epsilon \\in \\mathbb{R}^{n}$ 是误差向量。OLS 估计量 $\\hat{\\beta}$ 最小化残差平方和 $\\mathrm{SSR} = \\lVert y - X\\beta \\rVert_{2}^{2}$。\n\n该最小化问题的解由正规方程给出：\n$$\nX^{\\top} X \\hat{\\beta} = X^{\\top} y\n$$\n由于 $X$ 是满列秩，矩阵 $X^{\\top} X \\in \\mathbb{R}^{k \\times k}$ 是可逆的。因此，唯一的最小二乘估计量是：\n$$\n\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} y\n$$\n\n拟合值向量 $\\hat{y}$ 是 $y$ 在 $X$ 的列空间上的投影：\n$$\n\\hat{y} = X \\hat{\\beta} = X (X^{\\top} X)^{-1} X^{\\top} y\n$$\n这定义了帽子矩阵 $H \\in \\mathbb{R}^{n \\times n}$，它是一个将 $y$ 映射到 $\\hat{y}$ 的线性算子：\n$$\nH = X (X^{\\top} X)^{-1} X^{\\top}\n$$\n帽子矩阵是一个投影矩阵，这意味着它是对称的（$H^{\\top} = H$）和幂等的（$H^{2} = H$）。\n\n第 $i$ 个观测值的杠杆值 $h_{ii}$ 是帽子矩阵 $H$ 的第 $i$ 个对角线元素。它由以下公式给出：\n$$\nh_{ii} = x_i^{\\top} (X^{\\top} X)^{-1} x_i\n$$\n其中 $x_i^{\\top}$ 是 $X$ 的第 $i$ 行。杠杆值 $h_{ii}$ 衡量观测响应 $y_i$ 对其自身拟合值 $\\hat{y}_i$ 的影响，因为 $\\hat{y}_i = \\sum_{j=1}^{n} H_{ij} y_j = h_{ii} y_i + \\sum_{j \\neq i} H_{ij} y_j$。$h_{ii}$ 的值在 $[1/n, 1]$ 的范围内，反映了预测向量 $x_i$ 与所有预测向量均值的距离。具有高杠杆值的点是预测变量空间中的潜在异常点。\n\n残差向量是观测值和拟合值之间的差：\n$$\ne = y - \\hat{y} = (I - H)y\n$$\n其中 $I$ 是 $n \\times n$ 的单位矩阵。残差平方和为 $\\mathrm{SSR} = \\lVert e \\rVert_{2}^{2} = e^{\\top}e$。误差方差 $\\sigma^2$ 的一个无偏估计量是均方误差（MSE）：\n$$\n\\widehat{\\sigma}^{2} = \\frac{e^{\\top}e}{n-k}\n$$\n\nCook 距离 $D_i$ 衡量删除第 $i$ 个观测值对估计系数的影响。它被定义为使用所有数据计算的拟合值向量与删除第 $i$ 个观测值后计算的拟合值向量之间的缩放欧几里得距离。一个方便且广泛使用的 Cook 距离公式用杠杆值 $h_{ii}$、相应的残差 $e_i$、预测变量数 $k$ 和均方误差 $\\widehat{\\sigma}^2$ 来表示它：\n$$\nD_i = \\frac{e_i^2 h_{ii}}{k \\widehat{\\sigma}^2 (1-h_{ii})^2}\n$$\n这个公式突出表明，一个观测值的影响力（由 $D_i$ 衡量）是其杠杆值（$h_{ii}$）和其残差（$e_i$）的函数。如果一个高杠杆点（$h_{ii}$ 接近 1）的残差 $e_i$ 很小（即该点靠近由其他点拟合的回归线），那么它的 Cook 距离不一定大。相反，即使是一个杠杆值中等的点，如果其残差非常大，也可能具有影响力。\n\n每个测试用例的计算过程如下：\n1.  构建设计矩阵 $X$ 和响应向量 $y$。令 $n$ 和 $k$ 分别为 $X$ 的行数和列数。\n2.  计算矩阵乘积 $X^{\\top}X$ 及其逆矩阵 $(X^{\\top}X)^{-1}$。\n3.  计算帽子矩阵 $H = X(X^{\\top}X)^{-1}X^{\\top}$。\n4.  从 $H$ 的第 $i$ 个对角线元素中提取杠杆值 $h_{ii}$。\n5.  计算估计系数 $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$。\n6.  计算拟合值 $\\hat{y} = X\\hat{\\beta}$ 和残差 $e = y - \\hat{y}$。\n7.  提取指定索引 $i$ 的残差 $e_i$。\n8.  计算均方误差 $\\widehat{\\sigma}^2 = \\frac{e^{\\top}e}{n-k}$。\n9.  使用公式 $D_i = \\frac{e_i^2 h_{ii}}{k \\widehat{\\sigma}^2 (1-h_{ii})^2}$ 计算 Cook 距离 $D_i$。\n\n此过程将应用于指定的三个测试用例。\n-   **测试用例 A**：观测值 $i=7$ 的预测向量 $x_7 = [1, 10, 10]$ 是预测变量空间中的一个异常点，这应导致一个高的杠杆值 $h_{77}$。然而，相应的响应 $y_7=11.0$ 被构造得非常接近真实模型预测的值，这表明残差 $e_7$ 很小。因此，预计 Cook 距离 $D_7$ 会很小。\n-   **测试用例 B**：设计矩阵 $X$ 与用例 A 相同，因此杠杆值 $h_{77}$ 不变且仍然很高。响应 $y_7$ 更改为 $21.0$，这与模型预测的值相差甚远。这将产生一个大的残差 $e_7$。高杠杆值和巨大残差的组合预计将产生一个大的 Cook 距离 $D_7$。\n-   **测试用例 C**：设计矩阵 $X$ 没有极端的预测变量值。因此，任何观测值（包括 $i=2$）的杠杆值预计都是中等的。残差 $e_2$ 预计也很小。因此，Cook 距离 $D_2$ 应该很小。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes leverage and Cook's distance for three test cases to illustrate\n    their relationship.\n    \"\"\"\n\n    test_cases = [\n        # Test Case A: High leverage, small residual\n        {\n            \"id\": \"A\",\n            \"X\": np.array([\n                [1, 0, 0], [1, 1, 0], [1, 1, 1], [1, 2, 1],\n                [1, 0, 2], [1, 2, 2], [1, 1, 1], [1, 10, 10]\n            ]),\n            \"y\": np.array([\n                1.1, 2.8, 2.05, 3.95, -1.0, 1.1, 1.9, 11.0\n            ]),\n            \"i\": 7\n        },\n        # Test Case B: High leverage, large residual\n        {\n            \"id\": \"B\",\n            \"X\": np.array([\n                [1, 0, 0], [1, 1, 0], [1, 1, 1], [1, 2, 1],\n                [1, 0, 2], [1, 2, 2], [1, 1, 1], [1, 10, 10]\n            ]),\n            \"y\": np.array([\n                1.1, 2.8, 2.05, 3.95, -1.0, 1.1, 1.9, 21.0\n            ]),\n            \"i\": 7\n        },\n        # Test Case C: No extreme leverage, moderate residual\n        {\n            \"id\": \"C\",\n            \"X\": np.array([\n                [1, 0.0, 0.0], [1, 1.0, 0.0], [1, 1.0, 1.0], [1, 2.0, 1.0],\n                [1, 0.0, 2.0], [1, 2.0, 2.0], [1, 1.0, 1.0], [1, 0.5, 1.5]\n            ]),\n            \"y\": np.array([\n                1.1, 2.8, 2.05, 3.95, -1.0, 1.1, 1.9, 0.55\n            ]),\n            \"i\": 2\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        i = case[\"i\"]\n\n        n, k = X.shape\n\n        # Step 1: Compute (X^T X)^-1\n        try:\n            XtX_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            print(f\"Error: X^T X is singular for case {case['id']}.\")\n            continue\n\n        # Step 2: Compute the Hat Matrix H and extract leverage h_ii\n        # H = X @ (X^T X)^-1 @ X^T\n        H = X @ XtX_inv @ X.T\n        h_ii = H[i, i]\n\n        # Step 3: Compute OLS coefficients, fitted values, and residuals\n        # beta_hat = (X^T X)^-1 @ X^T @ y\n        beta_hat = XtX_inv @ X.T @ y\n        y_hat = X @ beta_hat\n        e = y - y_hat\n        e_i = e[i]\n\n        # Step 4: Compute Mean Squared Error (MSE)\n        # sigma_hat^2 = e^T e / (n - k)\n        e_norm_sq = e.T @ e\n        sigma2_hat = e_norm_sq / (n - k)\n\n        # Step 5: Compute Cook's Distance D_i\n        # D_i = (e_i^2 * h_ii) / (k * sigma_hat^2 * (1 - h_ii)^2)\n        # Numerically stable calculation for D_i\n        # Small values of (1 - h_ii) can cause issues if h_ii is close to 1.\n        # However, for this problem, direct computation is sufficient.\n        denominator = k * sigma2_hat * (1 - h_ii)**2\n        if abs(denominator)  1e-15:\n            # Handle potential division by zero, though unlikely here\n            D_i = float('inf')\n        else:\n            D_i = (e_i**2 * h_ii) / denominator\n\n        # In Case A, the predictor vector at index i=7, x_7 = [1, 10, 10], is far from the\n        # other points, leading to a high leverage value h_77. However, the response y_7 = 11.0\n        # is consistent with the model, resulting in a very small residual e_7. The small\n        # residual term e_7^2 dominates the large leverage in the Cook's distance formula,\n        # yielding a small D_7.\n        # Numerical verification for A: h_77 = 0.949640, e_7 = 0.007194, D_7 = 0.001614\n        \n        # In Case B, the predictors are the same, so h_77 is the same high value.\n        # But y_7 is now 21.0, creating a large discrepancy from the fitted line.\n        # This large residual e_7, combined with the high leverage h_77,\n        # produces a very large Cook's distance D_7, indicating a highly influential point.\n        # Numerical verification for B: h_77 = 0.949640, e_7 = 9.870504, D_7 = 8.167817\n\n        # In Case C, the predictor space is more balanced. There are no extreme points.\n        # As a result, the leverage h_22 is modest. The residual e_2 is also small.\n        # Consequently, Cook's distance D_2 is very small, indicating low influence.\n        # Numerical verification for C: h_22 = 0.285714, e_2 = 0.057143, D_2 = 0.016327\n        \n        results.extend([h_ii, D_i])\n\n    print(f\"[{','.join([f'{x:.6f}' for x in results])}]\")\n\nsolve()\n```"
        }
    ]
}