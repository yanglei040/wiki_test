{
    "hands_on_practices": [
        {
            "introduction": "Understanding linear regression diagnostics begins with the hat matrix, $H$. This exercise provides a foundational, hands-on calculation of $H$ for a small, well-behaved dataset. By constructing the matrix from first principles and observing how it transforms a change in a single response value, you will gain a concrete understanding of leverage and the interconnected nature of residuals in an OLS model.",
            "id": "3183487",
            "problem": "Consider a fixed-design linear regression with $n=5$ observations and $p=3$ predictors. Let the design matrix $X \\in \\mathbb{R}^{5 \\times 3}$ have columns $c_{0}, c_{1}, c_{2}$ defined by\n$$\nc_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nc_{1} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 0 \\end{pmatrix}, \\quad\nc_{2} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -1 \\\\ 0 \\end{pmatrix},\n$$\nso that\n$$\nX = \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n1 & -1 & -1 \\\\\n1 & 0 & 0\n\\end{pmatrix}.\n$$\nYou may assume the response vector $y \\in \\mathbb{R}^{5}$ is arbitrary, and the least-squares fitted vector is the orthogonal projection of $y$ onto the column space of $X$ under the standard Euclidean inner product. The residual vector is $e = y - \\hat{y}$.\n\nTasks:\n1. Construct explicitly the linear operator on $\\mathbb{R}^{5}$ that maps any $y$ to its least-squares fitted value $\\hat{y}$ in the column space of $X$. Compute this operator exactly for the given $X$.\n2. Using the operator from part 1, analyze perturbations to $y$ of the form $y = y_{0} + \\delta v$, where $y_{0}$ is fixed, $\\delta \\in \\mathbb{R}$ is a scalar, and $v$ is a standard basis vector $e_{k}$ in $\\mathbb{R}^{5}$ (i.e., the $k$-th coordinate direction). Determine how the residual vector $e = (I - H) y$ responds per coordinate direction.\n3. For the specific perturbation with $\\delta = 3$ and $v = e_{3}$, determine the resulting change in the residual at observation $i=5$.\n\nProvide your final answer as a single exact number. No rounding is required and there are no physical units in this problem.",
            "solution": "The problem is evaluated to be valid as it is scientifically grounded in linear regression theory, well-posed with a unique solution, and formally stated with no ambiguities or contradictions. We proceed with the solution.\n\nThe problem requires a three-part analysis of a linear regression model defined by the design matrix $X$. The three tasks are:\n1.  To construct the projection operator onto the column space of $X$, known as the hat matrix $H$.\n2.  To analyze the response of the residual vector to a specific type of perturbation in the response vector $y$.\n3.  To compute the change in a specific residual for a given perturbation.\n\nLet the linear model be $y = X\\beta + \\epsilon$, where $y \\in \\mathbb{R}^{5}$ is the response vector, $X \\in \\mathbb{R}^{5 \\times 3}$ is the design matrix, $\\beta \\in \\mathbb{R}^{3}$ is the vector of coefficients, and $\\epsilon \\in \\mathbb{R}^{5}$ is the error vector. The least-squares estimate of $\\beta$ is $\\hat{\\beta} = (X^T X)^{-1}X^T y$. The fitted values are given by $\\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1}X^T y$.\n\n**Part 1: Construction of the Hat Matrix**\n\nThe linear operator that maps $y$ to $\\hat{y}$ is the hat matrix, $H$, defined as:\n$$ H = X(X^T X)^{-1}X^T $$\nWe are given the design matrix:\n$$ X = \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n1 & 1 & -1 \\\\\n1 & -1 & -1 \\\\\n1 & 0 & 0\n\\end{pmatrix} $$\nFirst, we compute the matrix $X^T X$. The columns of $X$, denoted $c_0, c_1, c_2$, are orthogonal. We can see this by computing the off-diagonal elements of $X^T X$:\n$c_0^T c_1 = 1-1+1-1+0 = 0$\n$c_0^T c_2 = 1+1-1-1+0 = 0$\n$c_1^T c_2 = 1-1-1+1+0 = 0$\nThe diagonal elements are the squared norms of the columns:\n$c_0^T c_0 = 1^2+1^2+1^2+1^2+1^2 = 5$\n$c_1^T c_1 = 1^2+(-1)^2+1^2+(-1)^2+0^2 = 4$\n$c_2^T c_2 = 1^2+1^2+(-1)^2+(-1)^2+0^2 = 4$\nThus, $X^T X$ is a diagonal matrix:\n$$ X^T X = \\begin{pmatrix}\n5 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 4\n\\end{pmatrix} $$\nThe inverse, $(X^T X)^{-1}$, is also a diagonal matrix with the reciprocals of the diagonal entries:\n$$ (X^T X)^{-1} = \\begin{pmatrix}\n\\frac{1}{5} & 0 & 0 \\\\\n0 & \\frac{1}{4} & 0 \\\\\n0 & 0 & \\frac{1}{4}\n\\end{pmatrix} $$\nNow we can compute the hat matrix $H$. The element $H_{ij}$ of the hat matrix is given by $H_{ij} = X_i (X^T X)^{-1} X_j^T$, where $X_i$ is the $i$-th row of $X$. Using the derived $(X^TX)^{-1}$:\n$$ H_{ij} = \\frac{x_{i1} x_{j1}}{5} + \\frac{x_{i2} x_{j2}}{4} + \\frac{x_{i3} x_{j3}}{4} $$\nThe complete hat matrix is:\n$$ H = \\frac{1}{10} \\begin{pmatrix}\n7 & 2 & 2 & -3 & 2 \\\\\n2 & 7 & -3 & 2 & 2 \\\\\n2 & -3 & 7 & 2 & 2 \\\\\n-3 & 2 & 2 & 7 & 2 \\\\\n2 & 2 & 2 & 2 & 2\n\\end{pmatrix} $$\n\n**Part 2: Analysis of Perturbations**\n\nThe residual vector is $e = y - \\hat{y}$. Since $\\hat{y} = Hy$, the residual vector is given by $e = y - Hy = (I - H)y$, where $I$ is the $5 \\times 5$ identity matrix.\nLet the original response vector be $y_0$ and its corresponding residual vector be $e_0 = (I-H)y_0$.\nThe response vector is perturbed to $y' = y_0 + \\delta v$, where $v$ is a standard basis vector $e_k$. The new residual vector $e'$ is:\n$$ e' = (I-H)y' = (I-H)(y_0 + \\delta e_k) $$\nBy the linearity of the operator $(I-H)$, we have:\n$$ e' = (I-H)y_0 + \\delta(I-H)e_k = e_0 + \\delta(I-H)e_k $$\nThe change in the residual vector is $\\Delta e = e' - e_0$:\n$$ \\Delta e = \\delta(I-H)e_k $$\nThe vector $(I-H)e_k$ is simply the $k$-th column of the matrix $(I-H)$. Therefore, a perturbation of size $\\delta$ to the $k$-th observation $y_k$ causes the residual vector to change by $\\delta$ times the $k$-th column of $(I-H)$. The change in the $i$-th residual, $\\Delta e_i$, is given by $\\Delta e_i = \\delta((I-H)e_k)_i = \\delta(I-H)_{ik}$.\n\nThe matrix $I-H$ is:\n$$ I-H = \\frac{1}{10} \\begin{pmatrix}\n3 & -2 & -2 & 3 & -2 \\\\\n-2 & 3 & 3 & -2 & -2 \\\\\n-2 & 3 & 3 & -2 & -2 \\\\\n3 & -2 & -2 & 3 & -2 \\\\\n-2 & -2 & -2 & -2 & 8\n\\end{pmatrix} $$\n\n**Part 3: Specific Perturbation Calculation**\n\nWe are asked to find the change in the residual at observation $i=5$ for a specific perturbation.\nThe given perturbation has $\\delta = 3$ and $v = e_3$. This means the perturbation is applied to the $3$-rd observation, so $k=3$. We are interested in the change in the $5$-th residual, so $i=5$.\nUsing the formula derived in Part 2:\n$$ \\Delta e_i = \\delta(I-H)_{ik} $$\nSubstituting the given values:\n$$ \\Delta e_5 = 3 \\cdot (I-H)_{53} $$\nFrom the matrix $I-H$ calculated above, the element in the $5$-th row and $3$-rd column is $(I-H)_{53} = -\\frac{2}{10}$.\nTherefore, the change in the residual at observation $i=5$ is:\n$$ \\Delta e_5 = 3 \\cdot \\left(-\\frac{2}{10}\\right) = -\\frac{6}{10} = -\\frac{3}{5} $$",
            "answer": "$$\\boxed{-\\frac{3}{5}}$$"
        },
        {
            "introduction": "While raw residuals provide a first look at model error, they can be misleading because not all residuals have the same variance. This coding exercise explores why, demonstrating that a point's leverage affects its residual's variance. Through carefully constructed test cases, you will verify key properties of the hat matrix and see firsthand why studentized residuals are essential for making fair comparisons and correctly identifying surprising data points.",
            "id": "3183472",
            "problem": "You are to write a complete program that constructs specific linear regression design matrices and response vectors, computes the projection (hat) matrix, leverages, residuals, and internally studentized residuals, and then evaluates specified logical conditions that test properties of leverage and residual diagnostics. The program must rely on the fundamental definitions of ordinary least squares and basic linear algebra to derive the necessary computational forms. Do not assume shortcut formulas; instead, base your approach on the definition that ordinary least squares minimizes the squared Euclidean norm of the residual vector. From this base, deduce how to compute the projection of a response vector onto the column space of a design matrix, how to obtain the residual vector, and how to standardize residuals to make them comparable across observations.\n\nDefinitions to use as starting points:\n- Let $X \\in \\mathbb{R}^{n \\times p}$ denote a design matrix with full column rank $p \\le n$, and let $y \\in \\mathbb{R}^{n}$ be a response vector. Ordinary least squares chooses $\\hat{\\beta}$ that minimizes $\\lVert y - X \\beta \\rVert_{2}^{2}$.\n- The fitted values are the orthogonal projection of $y$ onto the column space of $X$.\n- The residual vector is $r = y - \\hat{y}$, where $\\hat{y}$ is the projection of $y$ onto the column space of $X$.\n- The leverage of observation $i$ is the $i$-th diagonal element of the projection operator that maps $y$ to $\\hat{y}$.\n- Internally studentized residuals scale each residual by an estimate of its standard deviation derived from the ordinary least squares fit using all $n$ observations.\n\nYour program must implement the following four test cases. Use $0$-based indexing for any index references. In all cases, when computing the number of parameters $p$, you must treat it as the rank of $X$ (that is, use the effective number of linearly independent columns). When estimating the residual variance, use the unbiased residual mean square with divisor $n - p$.\n\nTest Case A (two points with identical leverage and differing residual magnitudes):\n- Construct $X \\in \\mathbb{R}^{5 \\times 2}$ with an intercept and a single predictor $x$ such that $x = [0,1,2,3,4]^{\\top}$ and the first column is all ones. Construct $y \\in \\mathbb{R}^{5}$ as follows: start from the baseline $y_{\\text{base}} = 1 + 0.2 x$, then add perturbations to obtain $y = y_{\\text{base}} + [0, 3, 0, -0.5, 0]^{\\top}$. Using ordinary least squares, compute the fitted values, residuals, the projection operator, and the leverages. Focus on indices $i=1$ and $j=3$ (corresponding to $x=1$ and $x=3$). Compute internally studentized residuals using the unbiased residual variance from the full model fit. Your program must evaluate the following conjunction of conditions:\n  1. The leverages at $i$ and $j$ are equal within an absolute tolerance of $10^{-12}$.\n  2. The absolute residual magnitudes at $i$ and $j$ differ by at least $0.1$.\n  3. The ratio of absolute studentized residuals at $i$ and $j$ equals the ratio of absolute raw residuals at $i$ and $j$ within an absolute tolerance of $10^{-10}$.\nReturn a single boolean for this test case equal to true if and only if all three conditions hold.\n\nTest Case B (trace property of the projection operator):\n- Construct $X \\in \\mathbb{R}^{6 \\times 3}$ with an intercept and two predictors given by:\n  - First column: all ones.\n  - Second column: $x_{1} = [-2,-1,0,1,2,3]^{\\top}$.\n  - Third column: $x_{2} = [0,1,0,1,0,1]^{\\top}$.\nConstruct any $y \\in \\mathbb{R}^{6}$ as it is not needed for this property. Compute the projection operator and verify that the sum of its diagonal elements equals the rank $p$ of $X$ within an absolute tolerance of $10^{-10}$. Return a single boolean for this test case.\n\nTest Case C (studentization normalizes comparisons under differing leverage):\n- Construct $X \\in \\mathbb{R}^{11 \\times 2}$ with an intercept and a single predictor $x$ such that $x = [0,1,2,\\dots,10]^{\\top}$ and the first column is all ones. Construct $y \\in \\mathbb{R}^{11}$ from the baseline $y_{\\text{base}} = 2 + 0.3 x$, then add perturbations only at indices $i_{\\text{high}}=10$ and $i_{\\text{low}}=5$ by setting $y[10] \\leftarrow y_{\\text{base}}[10] + 2$ and $y[5] \\leftarrow y_{\\text{base}}[5] - 2$. Using ordinary least squares, compute residuals, leverages, and internally studentized residuals. Your program must evaluate the following conjunction:\n  1. The absolute raw residual magnitudes at $i_{\\text{high}}$ and $i_{\\text{low}}$ are within $0.5$ of each other.\n  2. The absolute studentized residual at $i_{\\text{high}}$ strictly exceeds the absolute studentized residual at $i_{\\text{low}}$ by a factor of at least $1.2$.\nReturn a single boolean for this test case.\n\nTest Case D (average leverage property):\n- Construct $X \\in \\mathbb{R}^{7 \\times 1}$ consisting of a single intercept column of ones. Compute the leverages from the projection operator and verify the conjunction:\n  1. The mean leverage equals $p/n$ within an absolute tolerance of $10^{-12}$, where $p$ is the rank of $X$ and $n$ is the number of rows.\n  2. Every leverage equals $1/n$ within an absolute tolerance of $10^{-12}$.\nReturn a single boolean for this test case.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the four boolean outcomes, enclosed in square brackets. For example, an output might look like \"[True,False,True,True]\". No other text should be printed.\n\nYour implementation must be self-contained, must not read any input, and must not access any external files or network resources. Express all computations in floating-point arithmetic. No physical units, angle units, or percentages are involved in this problem. All indices are $0$-based as stated. The final output must be exactly one line in the format specified above.",
            "solution": "The user-provided problem has been assessed and is valid. It is scientifically grounded in the principles of linear regression, specifically the theory of ordinary least squares (OLS), the properties of the projection (hat) matrix, and residual diagnostics. The problem is well-posed, providing all necessary data, definitions, and explicit conditions for four distinct computational test cases. The language is objective and the requirements are unambiguous, with specified numerical tolerances for comparisons.\n\nThe solution proceeds by first principles as requested. The OLS method finds the coefficient vector $\\hat{\\beta}$ that minimizes the squared Euclidean norm of the residual vector, $r = y - X\\beta$. This minimization problem is expressed as $\\min_{\\beta} \\lVert y - X\\beta \\rVert_2^2$. The objective function is $L(\\beta) = (y - X\\beta)^\\top (y - X\\beta)$. To find the minimum, we compute the gradient with respect to $\\beta$ and set it to zero:\n$$ \\frac{\\partial L}{\\partial \\beta} = -2X^\\top y + 2X^\\top X \\beta = 0 $$\nThis yields the normal equations:\n$$ (X^\\top X) \\hat{\\beta} = X^\\top y $$\nGiven that the design matrix $X \\in \\mathbb{R}^{n \\times p}$ has full column rank $p$, the matrix $X^\\top X$ is invertible. The OLS estimator for $\\beta$ is therefore:\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\nThe vector of fitted values, $\\hat{y}$, is the orthogonal projection of the response vector $y$ onto the column space of $X$. It is calculated as:\n$$ \\hat{y} = X\\hat{\\beta} = X(X^\\top X)^{-1} X^\\top y $$\nThe matrix $H = X(X^\\top X)^{-1} X^\\top$ is the projection operator, commonly known as the hat matrix, because it maps $y$ to $\\hat{y}$ (i.e., it \"puts the hat on $y$\"). The leverage of the $i$-th observation, $h_{ii}$, is the $i$-th diagonal element of $H$. It measures the influence of the observation $y_i$ on its own fitted value $\\hat{y}_i$, since $\\hat{y}_i = \\sum_{j=1}^{n} H_{ij} y_j = h_{ii} y_i + \\sum_{j \\neq i} H_{ij} y_j$.\n\nThe residual vector is the difference between the observed and fitted values:\n$$ r = y - \\hat{y} = y - Hy = (I - H)y $$\nwhere $I$ is the $n \\times n$ identity matrix.\n\nTo standardize the residuals, we need an estimate of their variance. Assuming the model errors are independent with constant variance $\\sigma^2$, the variance of the residual vector is $\\text{Var}(r) = \\sigma^2(I - H)$. The variance of a single residual $r_i$ is $\\text{Var}(r_i) = \\sigma^2(1 - h_{ii})$. An unbiased estimator for the error variance $\\sigma^2$ is the residual mean square:\n$$ \\hat{\\sigma}^2 = \\frac{r^\\top r}{n - p} $$\nwhere $p$ is the rank of $X$. The internally studentized residual for observation $i$, denoted $t_i$, is the raw residual scaled by an estimate of its standard deviation:\n$$ t_i = \\frac{r_i}{\\hat{\\sigma}\\sqrt{1 - h_{ii}}} $$\nThis formulation will be used to implement the computations for each test case.\n\n**Test Case A:**\nThis case tests the relationship between leverage, raw residuals, and studentized residuals.\n1. Construct the design matrix $X \\in \\mathbb{R}^{5 \\times 2}$ with a column of ones and a predictor $x = [0, 1, 2, 3, 4]^\\top$.\n2. Construct the response vector $y \\in \\mathbb{R}^{5}$ by adding specified perturbations to a linear baseline.\n3. Compute the rank $p$ of $X$, the hat matrix $H = X(X^\\top X)^{-1}X^\\top$, and the leverages $h_{ii} = \\text{diag}(H)$.\n4. Compute the residuals $r = y - Hy$ and the unbiased variance estimate $\\hat{\\sigma}^2 = r^\\top r / (n-p)$.\n5. Compute the internally studentized residuals $t_i = r_i / (\\hat{\\sigma} \\sqrt{1 - h_{ii}})$.\n6. Evaluate the three specified conditions for indices $i=1$ and $j=3$:\n   a. Check if leverages are equal: $|h_{11} - h_{33}| \\le 10^{-12}$.\n   b. Check if absolute residual magnitudes differ: $| |r_1| - |r_3| | \\ge 0.1$.\n   c. Check if the ratio of absolute studentized residuals equals the ratio of absolute raw residuals: $| |t_1|/|t_3| - |r_1|/|r_3| | \\le 10^{-10}$.\n7. The result is the logical conjunction of these three conditions.\n\n**Test Case B:**\nThis case verifies the fundamental property that the trace of the hat matrix equals the number of parameters $p$.\n1. Construct the design matrix $X \\in \\mathbb{R}^{6 \\times 3}$ as specified.\n2. Compute the rank $p$ of $X$.\n3. Compute the hat matrix $H = X(X^\\top X)^{-1}X^\\top$.\n4. Check if the trace of $H$ equals $p$: $|\\text{trace}(H) - p| \\le 10^{-10}$.\n5. The result is the boolean outcome of this check.\n\n**Test Case C:**\nThis case demonstrates how studentization accounts for differing leverages when comparing residuals.\n1. Construct the design matrix $X \\in \\mathbb{R}^{11 \\times 2}$ with an intercept and predictor $x = [0, 1, \\dots, 10]^\\top$.\n2. Construct the response vector $y \\in \\mathbb{R}^{11}$ with perturbations at a low-leverage point ($i_{\\text{low}}=5$) and a high-leverage point ($i_{\\text{high}}=10$).\n3. Perform the full OLS diagnostic computation as in Test Case A to find residuals $r$ and studentized residuals $t$.\n4. Evaluate the two specified conditions for indices $i_{\\text{low}}$ and $i_{\\text{high}}$:\n   a. Check if absolute raw residual magnitudes are close: $||r_{10}| - |r_5|| \\le 0.5$.\n   b. Check if the absolute studentized residual at the high-leverage point is significantly larger: $|t_{10}| \\ge 1.2 \\cdot |t_5|$.\n5. The result is the logical conjunction of these two conditions.\n\n**Test Case D:**\nThis case verifies properties of leverage for an intercept-only model.\n1. Construct the design matrix $X \\in \\mathbb{R}^{7 \\times 1}$ as a column of ones.\n2. Compute the rank $p$ of $X$ and number of observations $n$.\n3. Compute the hat matrix $H$ and its diagonal elements (leverages) $h_{ii}$.\n4. Evaluate the two specified conditions:\n   a. Check if the mean leverage equals $p/n$: $|\\text{mean}(h) - p/n| \\le 10^{-12}$.\n   b. Check if all leverages are equal to $1/n$: $\\forall i, |h_{ii} - 1/n| \\le 10^{-12}$.\n5. The result is the logical conjunction of these two conditions.\n\nThe final program will implement these four test cases, collect the boolean results, and print them in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case_a():\n    \"\"\"\n    Test Case A: two points with identical leverage and differing residual magnitudes.\n    \"\"\"\n    x = np.arange(5, dtype=float).reshape(-1, 1)\n    X = np.hstack([np.ones_like(x), x])\n    \n    y_base = 1.0 + 0.2 * x.flatten()\n    perturbations = np.array([0, 3, 0, -0.5, 0], dtype=float)\n    y = y_base + perturbations\n    \n    n, _ = X.shape\n    p = np.linalg.matrix_rank(X)\n    \n    # H = X @ inv(X.T @ X) @ X.T\n    H = X @ np.linalg.inv(X.T @ X) @ X.T\n    leverages = np.diag(H)\n    \n    # r = y - H @ y\n    y_hat = H @ y\n    residuals = y - y_hat\n    \n    # s^2 = r.T @ r / (n - p)\n    s_sq = (residuals.T @ residuals) / (n - p)\n    s = np.sqrt(s_sq)\n    \n    # t_i = r_i / (s * sqrt(1 - h_ii))\n    studentized_residuals = residuals / (s * np.sqrt(1 - leverages))\n    \n    i, j = 1, 3\n    \n    cond1 = np.isclose(leverages[i], leverages[j], atol=1e-12)\n    cond2 = np.abs(np.abs(residuals[i]) - np.abs(residuals[j])) >= 0.1\n    \n    # The ratio of absolute studentized residuals equals the ratio of absolute raw residuals\n    # |t_i|/|t_j| = (|r_i|/|r_j|) * sqrt(1-h_jj)/sqrt(1-h_ii)\n    # If h_ii = h_jj, the sqrt term is 1, so the ratios are equal.\n    ratio_t = np.abs(studentized_residuals[i]) / np.abs(studentized_residuals[j])\n    ratio_r = np.abs(residuals[i]) / np.abs(residuals[j])\n    cond3 = np.isclose(ratio_t, ratio_r, atol=1e-10)\n    \n    return cond1 and cond2 and cond3\n\ndef run_case_b():\n    \"\"\"\n    Test Case B: trace property of the projection operator.\n    \"\"\"\n    x1 = np.array([-2, -1, 0, 1, 2, 3], dtype=float).reshape(-1, 1)\n    x2 = np.array([0, 1, 0, 1, 0, 1], dtype=float).reshape(-1, 1)\n    X = np.hstack([np.ones_like(x1), x1, x2])\n    \n    p = np.linalg.matrix_rank(X)\n    \n    # H = X @ inv(X.T @ X) @ X.T\n    H = X @ np.linalg.inv(X.T @ X) @ X.T\n    trace_H = np.trace(H)\n    \n    return np.isclose(trace_H, p, atol=1e-10)\n\ndef run_case_c():\n    \"\"\"\n    Test Case C: studentization normalizes comparisons under differing leverage.\n    \"\"\"\n    x = np.arange(11, dtype=float).reshape(-1, 1)\n    X = np.hstack([np.ones_like(x), x])\n    \n    y_base = 2.0 + 0.3 * x.flatten()\n    y = y_base.copy()\n    \n    i_high, i_low = 10, 5\n    y[i_high] += 2.0\n    y[i_low] -= 2.0\n    \n    n, _ = X.shape\n    p = np.linalg.matrix_rank(X)\n    \n    H = X @ np.linalg.inv(X.T @ X) @ X.T\n    leverages = np.diag(H)\n    \n    y_hat = H @ y\n    residuals = y - y_hat\n    \n    s_sq = (residuals.T @ residuals) / (n - p)\n    s = np.sqrt(s_sq)\n    \n    studentized_residuals = residuals / (s * np.sqrt(1 - leverages))\n    \n    cond1 = np.abs(np.abs(residuals[i_high]) - np.abs(residuals[i_low])) <= 0.5\n    cond2 = np.abs(studentized_residuals[i_high]) >= 1.2 * np.abs(studentized_residuals[i_low])\n    \n    return cond1 and cond2\n\ndef run_case_d():\n    \"\"\"\n    Test Case D: average leverage property.\n    \"\"\"\n    n = 7\n    X = np.ones((n, 1), dtype=float)\n    \n    p = np.linalg.matrix_rank(X)\n    \n    H = X @ np.linalg.inv(X.T @ X) @ X.T\n    leverages = np.diag(H)\n    \n    mean_leverage = np.mean(leverages)\n    \n    cond1 = np.isclose(mean_leverage, p / n, atol=1e-12)\n    cond2 = np.all(np.isclose(leverages, 1 / n, atol=1e-12))\n    \n    return cond1 and cond2\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    results = [\n        run_case_a(),\n        run_case_b(),\n        run_case_c(),\n        run_case_d()\n    ]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A point with high leverage has the *potential* to be influential, but is it always? This crucial question is at the heart of modern regression diagnostics. This practice guides you in constructing a scenario with a high-leverage point and then systematically evaluating its actual influence using Cook's distance. By observing how influence changes with the point's residual, you will solidify your understanding that true influence is a combination of both high leverage and a large, surprising error.",
            "id": "3183419",
            "problem": "Consider a fixed linear regression setup with a design matrix $X \\in \\mathbb{R}^{n \\times p}$ of full column rank under the Ordinary Least Squares (OLS) framework, where the fitted values are obtained by applying the linear projection that maps observed responses to the column space of $X$. The diagonal elements of the associated projection (hat) matrix quantify leverage. A high-leverage observation has a large diagonal element $h_{ii}$, and regression diagnostics relate the magnitude of residuals and the leverage to detect influential points. Your task is to construct a small, explicit dataset in which a single observation has leverage approximately $h_{ii} \\approx 0.9$, and then compute diagnostics to illustrate how leverage interacts with residual magnitude.\n\nUse the following concrete setup to ensure scientific realism and reproducibility. Let $n = 6$ and $p = 2$ (intercept and a single predictor), and define the predictor values\n$$x = [-8,\\,-1,\\,0,\\,1,\\,2,\\,3],$$\nwith an intercept column appended to form $X = [\\mathbf{1},\\,x]$. This choice yields a high-leverage first observation. Let the true linear relation be $y = \\alpha + \\beta x$, with $\\alpha = 1.0$ and $\\beta = 0.5$. Introduce small, fixed perturbations for the non-high-leverage observations (indices $2$ through $6$) to avoid degenerate variance: set the noise vector for indices $i = 2,3,4,5,6$ as\n$$\\nu = [0.1,\\,-0.05,\\,0.0,\\,0.03,\\,-0.02],$$\nso that for $i \\in \\{2,3,4,5,6\\}$, $y_i = \\alpha + \\beta x_i + \\nu_{i-1}$ and for $i=1$, $y_1 = \\alpha + \\beta x_1 + \\epsilon$, where $\\epsilon$ is a controlled residual at the high-leverage point.\n\nCompute, from first principles and core definitions (projection interpretation of OLS, leverage as diagonal entries of the hat matrix, residuals as differences between observed and fitted values, and conventional studentization used in regression diagnostics), the following diagnostics for observation $i=1$:\n- the leverage $h_{11}$,\n- the studentized residual (use the internally studentized residual definition),\n- Cook’s distance.\n\nDo not use shortcut formulas that bypass derivation from the stated base. All computations must be consistent with the OLS framework and use $n$ and $p$ as specified above.\n\nTest Suite:\nEvaluate the diagnostics under the following five values of the controlled residual at the high-leverage point, $\\epsilon \\in \\{0.0,\\,1.0,\\,-1.0,\\,8.0,\\,30.0\\}$, holding all other components ($X$, $\\alpha$, $\\beta$, and $\\nu$) fixed as specified. For each choice of $\\epsilon$, construct $y$ accordingly and compute the required diagnostics for $i=1$.\n\nAnswer Specification:\nFor each test case (each $\\epsilon$), your program must output a list of three floats in the order $[h_{11},\\,\\text{studentized residual at } i=1,\\,\\text{Cook’s distance at } i=1]$. Aggregate the results for all five test cases into a single list, in the same order of $\\epsilon$ values as above.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The aggregated list must contain the five per-case lists, each itself a comma-separated list enclosed in square brackets. For example, the output should have the form\n$$[\\,[h_{11}^{(1)},\\,r^{(1)},\\,D^{(1)}],\\,[h_{11}^{(2)},\\,r^{(2)},\\,D^{(2)}],\\,[h_{11}^{(3)},\\,r^{(3)},\\,D^{(3)}],\\,[h_{11}^{(4)},\\,r^{(4)},\\,D^{(4)}],\\,[h_{11}^{(5)},\\,r^{(5)},\\,D^{(5)}]\\,],$$\nwith all numerical values expressed as plain decimal floats. No physical units or angle units apply, and any fraction must be provided as a decimal number.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of Ordinary Least Squares (OLS) regression, well-posed with a complete and consistent set of givens, and objective in its formulation. All provided data and definitions are standard in the field of statistical learning.\n\nThe solution proceeds by first principles as requested. We will construct the necessary matrices and vectors, then derive expressions for the required diagnostics—leverage, studentized residual, and Cook's distance—as functions of the controlled residual $\\epsilon$.\n\n**1. Setup and Givens**\n\nThe problem is a linear regression with $n=6$ observations and $p=2$ parameters (an intercept and one predictor). The predictor values are given by the vector $x$:\n$$ x = \\begin{pmatrix} -8 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 3 \\end{pmatrix} $$\nThe design matrix $X \\in \\mathbb{R}^{6 \\times 2}$ is formed by prepending a column of ones for the intercept:\n$$ X = \\begin{pmatrix} 1 & -8 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{pmatrix} $$\nThe response vector $y \\in \\mathbb{R}^6$ is generated from a true linear model $y = \\alpha + \\beta x$ with $\\alpha=1.0$ and $\\beta=0.5$, plus some noise. For observations $i = 2, \\dots, 6$, the noise is fixed by the vector $\\nu = [0.1, -0.05, 0.0, 0.03, -0.02]$. For observation $i=1$, the noise is a variable $\\epsilon$.\nThe response vector $y$ is thus:\n$$ y_i = \\begin{cases} (1.0 + 0.5x_1) + \\epsilon = -3.0 + \\epsilon & \\text{if } i=1 \\\\ (1.0 + 0.5x_i) + \\nu_{i-1} & \\text{if } i \\in \\{2, \\dots, 6\\} \\end{cases} $$\nThis yields the vector $y$ as a function of $\\epsilon$:\n$$ y(\\epsilon) = \\begin{pmatrix} -3.0 + \\epsilon \\\\ 0.6 \\\\ 0.95 \\\\ 1.5 \\\\ 2.03 \\\\ 2.48 \\end{pmatrix} $$\n\n**2. Leverage ($h_{11}$)**\n\nLeverage is quantified by the diagonal elements $h_{ii}$ of the hat matrix $H$. The hat matrix is the projection matrix that maps the observed response vector $y$ to the fitted values $\\hat{y}$:\n$$ \\hat{y} = H y $$\nwhere $H$ is defined as:\n$$ H = X(X^T X)^{-1} X^T $$\nThe leverage of the $i$-th observation is $h_{ii} = x_i^T (X^T X)^{-1} x_i$, where $x_i^T$ is the $i$-th row of $X$. Critically, $H$ and its diagonal elements depend only on the design matrix $X$, not on the response vector $y$. Thus, $h_{11}$ will be constant across all test cases.\n\nFirst, we compute $X^T X$:\n$$ X^T X = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 & 1 \\\\ -8 & -1 & 0 & 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 1 & -8 \\\\ 1 & -1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 6 & -3 \\\\ -3 & 79 \\end{pmatrix} $$\nNext, we find its inverse, $(X^T X)^{-1}$:\n$$ \\det(X^T X) = (6)(79) - (-3)(-3) = 474 - 9 = 465 $$\n$$ (X^T X)^{-1} = \\frac{1}{465} \\begin{pmatrix} 79 & 3 \\\\ 3 & 6 \\end{pmatrix} $$\nThe first observation corresponds to the first row of $X$, $x_1^T = \\begin{pmatrix} 1 & -8 \\end{pmatrix}$. We can now compute the leverage $h_{11}$:\n$$ h_{11} = x_1^T (X^T X)^{-1} x_1 = \\begin{pmatrix} 1 & -8 \\end{pmatrix} \\frac{1}{465} \\begin{pmatrix} 79 & 3 \\\\ 3 & 6 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -8 \\end{pmatrix} $$\n$$ h_{11} = \\frac{1}{465} \\begin{pmatrix} 1 & -8 \\end{pmatrix} \\begin{pmatrix} 79(1) + 3(-8) \\\\ 3(1) + 6(-8) \\end{pmatrix} = \\frac{1}{465} \\begin{pmatrix} 1 & -8 \\end{pmatrix} \\begin{pmatrix} 55 \\\\ -45 \\end{pmatrix} $$\n$$ h_{11} = \\frac{1(55) - 8(-45)}{465} = \\frac{55 + 360}{465} = \\frac{415}{465} \\approx 0.89247 $$\nThis high leverage value, close to $1$, confirms that the first observation is an outlier in the predictor space and will have a strong influence on its own fitted value.\n\n**3. Residuals and Studentization**\n\nThe vector of OLS residuals is $e = y - \\hat{y} = y - Hy = (I-H)y$. The $i$-th residual is $e_i$.\nLet $y_0$ be the response vector when $\\epsilon=0$. The response vector for a general $\\epsilon$ can be written as $y(\\epsilon) = y_0 + \\delta$, where $\\delta = [\\epsilon, 0, 0, 0, 0, 0]^T$.\nThe residual vector is then $e(\\epsilon) = (I-H)(y_0+\\delta) = (I-H)y_0 + (I-H)\\delta = e_0 + (I-H)\\delta$.\nThe vector $(I-H)\\delta$ has its $i$-th component equal to $(1-h_{ii})\\delta_i - \\sum_{j \\neq i} h_{ij}\\delta_j$. Since only $\\delta_1 = \\epsilon$ is non-zero, the first residual is:\n$$ e_1(\\epsilon) = e_{1,0} + (1-h_{11})\\epsilon $$\nwhere $e_{1,0}$ is the residual for observation $1$ when $\\epsilon=0$.\nThe total Residual Sum of Squares (RSS) is $e^T e$. The RSS can be shown to be a quadratic function of $\\epsilon$:\n$$ \\text{RSS}(\\epsilon) = \\text{RSS}_0 + 2e_{1,0}\\epsilon + (1-h_{11})\\epsilon^2 $$\nwhere $\\text{RSS}_0$ is the residual sum of squares for $\\epsilon=0$.\n\nThe (internally) studentized residual for observation $i$ is defined as:\n$$ r_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}} $$\nwhere $\\hat{\\sigma}^2 = \\text{MSE} = \\text{RSS}/(n-p)$ is the mean squared error. For our case, $n-p = 6-2=4$.\nThus, for observation $1$:\n$$ r_1(\\epsilon) = \\frac{e_1(\\epsilon)}{\\sqrt{\\text{RSS}(\\epsilon)/(n-p)} \\sqrt{1-h_{11}}} $$\n\n**4. Cook's Distance**\n\nCook's distance, $D_i$, measures the influence of deleting observation $i$. It can be computed from the leverage and the studentized residual:\n$$ D_i = \\frac{r_i^2}{p} \\frac{h_{ii}}{1-h_{ii}} $$\nFor observation $1$:\n$$ D_1(\\epsilon) = \\frac{r_1(\\epsilon)^2}{2} \\frac{h_{11}}{1-h_{11}} = \\frac{r_1(\\epsilon)^2}{2} \\frac{415/465}{50/465} = \\frac{r_1(\\epsilon)^2}{2} \\frac{415}{50} = 4.15 \\cdot r_1(\\epsilon)^2 $$\n\n**5. Computation**\n\nTo compute the diagnostics for each $\\epsilon$, we first calculate the baseline quantities for $\\epsilon=0$.\n-   $y_0 = [-3.0, 0.6, 0.95, 1.5, 2.03, 2.48]^T$.\n-   The OLS coefficients are $\\hat{\\beta}_{\\text{vec},0} = (X^T X)^{-1} X^T y_0$, yielding $\\hat{\\alpha}_0 \\approx 1.00955$ and $\\hat{\\beta}_0 \\approx 0.49910$.\n-   The residuals $e_0 = y_0 - X\\hat{\\beta}_{\\text{vec},0}$ are calculated, from which we find $e_{1,0} \\approx -0.01677$ and $\\text{RSS}_0 = \\sum_{i=1}^6 e_{i,0}^2 \\approx 0.013137$.\n\nFor each given $\\epsilon \\in \\{0.0, 1.0, -1.0, 8.0, 30.0\\}$, we compute:\n1.  $h_{11} = 415/465$.\n2.  $e_1(\\epsilon) = e_{1,0} + (1-h_{11})\\epsilon$.\n3.  $\\text{RSS}(\\epsilon) = \\text{RSS}_0 + 2e_{1,0}\\epsilon + (1-h_{11})\\epsilon^2$.\n4.  $r_1(\\epsilon) = \\frac{e_1(\\epsilon)}{\\sqrt{\\text{RSS}(\\epsilon)/4} \\sqrt{1-h_{11}}}$.\n5.  $D_1(\\epsilon) = 4.15 \\cdot r_1(\\epsilon)^2$.\n\nThe calculations are performed numerically in the provided program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes regression diagnostics for a high-leverage point under different\n    residual magnitudes.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases_epsilon = [0.0, 1.0, -1.0, 8.0, 30.0]\n\n    # --- Step 1: Setup and Givens ---\n    n = 6\n    p = 2\n    x_vec = np.array([-8.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n    \n    # Design Matrix X\n    X = np.ones((n, p))\n    X[:, 1] = x_vec\n\n    # True model parameters\n    alpha_true = 1.0\n    beta_true = 0.5\n\n    # Noise for observations 2 through 6\n    nu = np.array([0.1, -0.05, 0.0, 0.03, -0.02])\n\n    # --- Step 2: Leverage Calculation (independent of y) ---\n    # Compute h_11 from H = X(X^T X)^-1 X^T\n    XTX = X.T @ X\n    XTX_inv = np.linalg.inv(XTX)\n    x1_row = X[0, :].reshape(1, -1)\n    h_11 = (x1_row @ XTX_inv @ x1_row.T)[0, 0]\n\n    # --- Step 3 & 4: Baseline Calculations for epsilon = 0 ---\n    # Construct response vector y for epsilon = 0\n    y0 = np.zeros(n)\n    y_true = alpha_true + beta_true * x_vec\n    y0[0] = y_true[0]  # Epsilon is 0\n    y0[1:] = y_true[1:] + nu\n    \n    # OLS coefficients for epsilon = 0\n    beta_hat_0 = XTX_inv @ X.T @ y0\n    \n    # Residuals for epsilon = 0\n    e_0 = y0 - X @ beta_hat_0\n    e_1_0 = e_0[0]\n    \n    # RSS for epsilon = 0\n    rss_0 = e_0.T @ e_0\n\n    results = []\n    # --- Main Loop over test cases ---\n    for epsilon in test_cases_epsilon:\n        \n        # Calculate diagnostics based on derived formulas\n        # e_1(eps) = e_1_0 + (1-h_11)*eps\n        # RSS(eps) = RSS_0 + 2*e_1_0*eps + (1-h_11)*eps^2\n        \n        e_1 = e_1_0 + (1.0 - h_11) * epsilon\n        \n        rss = rss_0 + 2.0 * e_1_0 * epsilon + (1.0 - h_11) * epsilon**2\n        \n        # Mean Squared Error (MSE) and Residual Standard Error (sigma_hat)\n        mse = rss / (n - p)\n        if mse < 0: # Should not happen in this problem\n             mse = 0\n        sigma_hat = np.sqrt(mse)\n        \n        # Internally Studentized Residual r_1\n        # r_1 = e_1 / (sigma_hat * sqrt(1-h_11))\n        # Account for sigma_hat = 0 in degenerate cases\n        denom = sigma_hat * np.sqrt(1.0 - h_11)\n        if np.isclose(denom, 0.0):\n            r_1 = np.inf if e_1 > 0 else -np.inf if e_1  0 else 0.0\n        else:\n            r_1 = e_1 / denom\n            \n        # Cook's Distance D_1\n        # D_1 = (r_1^2 / p) * (h_11 / (1-h_11))\n        D_1 = (r_1**2 / p) * (h_11 / (1.0 - h_11))\n        \n        case_result = [h_11, r_1, D_1]\n        results.append(case_result)\n\n    # --- Final Output Formatting ---\n    # Format each inner list into a string \"[v1,v2,v3]\"\n    inner_lists_str = [f\"[{','.join(map(str, res))}]\" for res in results]\n    # Join the inner list strings with commas and wrap in brackets\n    final_output_str = f\"[{','.join(inner_lists_str)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}