{
    "hands_on_practices": [
        {
            "introduction": "在构建线性回归模型时，一个基本要求是设计矩阵 $X$ 必须具有满列秩，这样才能保证普通最小二乘（OLS）估计量存在且唯一。一个常见的错误，即“虚拟变量陷阱”，会导致违反此假定，即在模型中同时包含截距项和所有类别的指示变量。通过本练习 ，您将通过编程实践识别并解决这个由完美多重共线性引起的问题，从而为稳健的建模奠定基础。",
            "id": "3099895",
            "problem": "考虑线性回归模型 $y = X \\beta + \\varepsilon$，其中设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，系数向量为 $\\beta \\in \\mathbb{R}^{p}$，噪声为 $\\varepsilon \\in \\mathbb{R}^{n}$。普通最小二乘法 (OLS) 的一个核心假设是 $X$ 具有满列秩，这等价于 $X^\\top X$ 是可逆的。在包含分类预测变量的统计学习中，一种常见的构造方法是为 $k$ 个类别中的每一个类别使用一个截距列和虚拟（独热）指示列。所谓的“虚拟变量陷阱”发生在截距与所有 $k$ 个虚拟指示变量一同被包含时，这会导致 $X$ 的列之间产生线性相关性，从而使得 $X^\\top X$ 不可逆。\n\n仅使用定义和线性代数，编写一个程序，对于给定的分类预测变量，通过三种方式构造设计矩阵 $X$，并检查每种情况下 $X^\\top X$ 的可逆性：\n- 初始构造：包含一个截距列 $1_n$ 和所有 $k$ 个虚拟指示列。\n- 补救方法 A：去掉截距，并包含所有 $k$ 个虚拟指示列。\n- 补救方法 B：保留截距，并去掉恰好一个虚拟指示列，选择数据集中出现频率最低的类别（如果出现平局，则去掉频率最低的类别中索引最小的那个）。\n\n你的程序必须通过确定 $X^\\top X$ 的数值秩是否等于 $p$ 来计算可逆性，其中 $p$ 是所构造的 $X$ 的列数。秩应使用奇异值分解和标准容差以数值稳定的方式计算。\n\n为以下分类分配的测试套件实现上述操作（每个测试用例提供每行的类别索引和类别总数 $k$）：\n- 测试用例 1：$n = 8$, $k = 3$，类别索引为 $[0,1,2,0,1,2,0,1]$。\n- 测试用例 2：$n = 7$, $k = 3$，类别索引为 $[0,1,0,1,0,1,1]$。\n- 测试用例 3：$n = 6$, $k = 2$，类别索引为 $[0,1,0,1,0,1]$。\n\n对于每个测试用例，按固定顺序产生三个布尔值结果：\n$[$初始可逆性, 补救方法 A 的可逆性, 补救方法 B 的可逆性$]$。\n\n你的程序应该生成单行输出，其中包含所有结果，这些结果按测试用例 $1, 2, 3$ 的顺序平铺成一个列表，并以逗号分隔的序列形式包含在方括号内。具体来说，输出必须是\n$[$初始$_1$, 补救A$_1$, 补救B$_1$, 初始$_2$, 补救A$_2$, 补救B$_2$, 初始$_3$, 补救A$_3$, 补救B$_3]$，\n其中每个条目都是一个布尔值。此问题不涉及任何物理单位或角度单位，也不需要百分比。",
            "solution": "该问题要求分析线性回归中的“虚拟变量陷阱”，这是一种由于对分类预测变量进行特定编码而产生的完全多重共线性现象。我们的任务是为一个给定的分类变量构建三种不同的设计矩阵，并确定每种情况下得到的 $X^\\top X$ 矩阵的可逆性。可逆性是存在唯一普通最小二乘 (OLS) 估计量 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$ 的一个关键条件。一个矩阵 $X^\\top X$ 是可逆的，当且仅当设计矩阵 $X$ 具有满列秩。\n\n其核心原理是线性无关性。如果方程 $c_1 v_1 + c_2 v_2 + \\dots + c_p v_p = 0$ 的唯一解是平凡解 $c_1 = c_2 = \\dots = c_p = 0$，则向量集合 $\\{v_1, v_2, \\dots, v_p\\}$ 是线性无关的。如果存在非平凡解，则这些向量是线性相关的。一个矩阵具有满列秩，当且仅当它的列构成一个线性无关集。\n\n我们将分析针对一个具有 $k$ 个水平、在 $n$ 个样本上观测的分类预测变量的三种构造方法。分类数据以索引向量的形式提供，其中每个索引都在 $\\{0, 1, \\dots, k-1\\}$ 范围内。\n\n1.  **初始构造**：设计矩阵 $X_{initial}$ 是通过包含一个截距列（全为1的向量，$1_n$）以及所有 $k$ 个虚拟指示列 $d_0, d_1, \\dots, d_{k-1}$ 形成的。每个列 $d_j$ 是一个向量，其中第 $i$ 个元素为1（如果第 $i$ 个样本属于类别 $j$），否则为0。该矩阵有 $p = k+1$ 列。存在一个基本的线性相关性，因为任何样本都必须恰好属于一个类别。因此，虚拟指示列的总和总是等于截距列：\n    $$ \\sum_{j=0}^{k-1} d_j = 1_n $$\n    这可以重写为 $1 \\cdot 1_n - 1 \\cdot d_0 - \\dots - 1 \\cdot d_{k-1} = 0$，这是 $X_{initial}$ 的列的一个等于零向量的非平凡线性组合。因此，这些列是线性相关的，$rank(X_{initial})  p$，且 $X_{initial}^\\top X_{initial}$ 永远不可逆。\n\n2.  **补救方法 A (无截距)**：设计矩阵 $X_A$ 是通过包含所有 $k$ 个虚拟列但省略截距而形成的。该矩阵为 $X_A = [d_0 | d_1 | \\dots | d_{k-1}]$，有 $p=k$ 列。为了检查线性无关性，我们考虑方程 $\\sum_{j=0}^{k-1} c_j d_j = 0$。对于属于类别 $j'$ 的任何样本 $i$，该行的方程简化为 $c_{j'} \\cdot 1 = 0$，这意味着 $c_{j'} = 0$。如果从 $0$ 到 $k-1$ 的每个类别在数据中至少出现一次，这个逻辑会强制所有系数 $c_0, \\dots, c_{k-1}$ 都为零。那么这些列是线性无关的，并且 $X_A$ 具有满列秩。然而，如果任何类别 $j'$ 不存在于数据集中，其对应的列 $d_{j'}$ 将是一个零向量。零向量与任何其他向量集都是平凡线性相关的（例如，$1 \\cdot d_{j'} = 0$），所以 $X_A$ 将不具有满列秩。\n\n3.  **补救方法 B (去掉一个虚拟变量)**：设计矩阵 $X_B$ 是通过包含截距和 $k$ 个虚拟列中的 $k-1$ 个而形成的。设被去掉的列为 $d_{j^*}$。该矩阵为 $X_B = [1_n | d_0 | \\dots | d_{j^*-1} | d_{j^*+1} | \\dots | d_{k-1}]$，有 $p=k$ 列。如果 $1_n$ 可以表示为其余 $k-1$ 个虚拟列的线性组合，则会存在线性相关性。其余虚拟列的和是 $\\sum_{j \\neq j^*} d_j$。由于 $\\sum_{j=0}^{k-1} d_j = 1_n$，我们有 $\\sum_{j \\neq j^*} d_j = 1_n - d_{j^*}$。这个和等于 $1_n$ 当且仅当 $d_{j^*} = 0$。这种情况恰好在被去掉的类别 $j^*$ 的观测值为零时发生。在这种情况下，线性相关性没有被解决。否则，如果被去掉的类别存在于数据中，则其余的列将是线性无关的，并且 $X_B$ 将具有满列秩。问题指定去掉最不常见类别的虚拟变量，这是一个合理的启发式方法，以避免去掉对应于大群体的列，但如果最不常见类别的频率为零，则无法解决相关性问题。\n\n计算过程是为每个测试用例实现这三种构造。对于每个构造的矩阵 $X$，计算其对应的格拉姆矩阵 $M = X^\\top X$。然后通过比较其数值秩与其维度 $p$ 来确定 $M$ 的可逆性。根据要求，使用奇异值分解 (SVD) 以鲁棒的方式计算秩。为每个测试用例的三种方法中的每一种都记录一个布尔结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _check_invertibility(X: np.ndarray) -> bool:\n    \"\"\"\n    Checks if X.T @ X is invertible by determining if its rank equals its number of columns.\n    Rank is computed using SVD, which is numerically stable.\n    The problem asks to check the invertibility of X^T X by checking its rank.\n    \"\"\"\n    p = X.shape[1]\n    \n    # A matrix with 0 columns has a 0x0 Gram matrix.\n    # The 0x0 matrix is invertible (its determinant is 1).\n    # This case is not reached by the test suite but is handled for correctness.\n    if p == 0:\n        return True\n    \n    # Form the Gram matrix X-transpose-X\n    XTX = X.T @ X\n    \n    # Compute the rank of the Gram matrix.\n    # np.linalg.matrix_rank uses SVD and a tolerance based on machine precision.\n    rank_XTX = np.linalg.matrix_rank(XTX)\n    \n    # The matrix is invertible if and only if it has full rank.\n    return rank_XTX == p\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        # (category_indices, k)\n        (np.array([0, 1, 2, 0, 1, 2, 0, 1]), 3),\n        (np.array([0, 1, 0, 1, 0, 1, 1]), 3),\n        (np.array([0, 1, 0, 1, 0, 1]), 2),\n    ]\n\n    all_results = []\n    \n    for category_indices, k in test_cases:\n        n = len(category_indices)\n        \n        # --- Common setup for each case ---\n        # Create dummy indicator columns for k categories.\n        # The columns of `dummies` correspond to categories 0, 1, ..., k-1.\n        dummies = np.zeros((n, k), dtype=int)\n        dummies[np.arange(n), category_indices] = 1\n        \n        # Create the intercept column (a vector of ones).\n        intercept = np.ones((n, 1), dtype=int)\n\n        # --- Method 1: Initial Construction (with dummy variable trap) ---\n        # Include intercept and all k dummy variables.\n        X_initial = np.hstack((intercept, dummies))\n        all_results.append(_check_invertibility(X_initial))\n\n        # --- Method 2: Remedy A (drop intercept) ---\n        # Include all k dummy variables, no intercept.\n        X_A = dummies\n        all_results.append(_check_invertibility(X_A))\n\n        # --- Method 3: Remedy B (drop one dummy variable) ---\n        # Keep intercept, drop one dummy variable.\n        # Find the least frequent category. np.bincount counts occurrences.\n        # minlength=k ensures all categories get a count, even if it is 0.\n        frequencies = np.bincount(category_indices, minlength=k)\n        \n        # np.argmin() finds the index of the first occurrence of the minimum value,\n        # which satisfies the tie-breaking rule (drop lowest-index category).\n        category_to_drop = np.argmin(frequencies)\n        \n        # In X_initial, column 0 is the intercept. Columns 1 to k are the dummies\n        # for categories 0 to k-1. So, the column for `category_to_drop`\n        # is at index `1 + category_to_drop`.\n        col_index_to_drop = 1 + category_to_drop\n        X_B = np.delete(X_initial, col_index_to_drop, axis=1)\n        all_results.append(_check_invertibility(X_B))\n\n    # Format the final output as a comma-separated string of booleans.\n    # The default string representation of a boolean in Python is \"True\" or \"False\".\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "线性回归的一个核心假设是同方差性，即所有观测的误差项具有相同的方差。当此假设不成立时（即存在异方差性），OLS估计量虽然仍是无偏的，但不再是最高效的，且其标准误会产生误导。本练习  将指导您应用加权最小二乘法（WLS）来应对异方差问题，并探讨在权重设定不准时如何使用“三明治”协方差估计量进行稳健推断。",
            "id": "3099884",
            "problem": "考虑一个带有抽样权重或精度权重的线性回归模型，表示为 $Y = X\\beta + \\epsilon$，其中 $E[\\epsilon \\mid X] = 0$。目标是分析将特定于观测值的非负权重 $w_i$ 纳入估计如何影响独立性和同方差性的假设，以及如何在加权情境下计算估计量的基于模型的方差和稳健方差。\n\n从将最小二乘法定义为最小化误差平方和出发，将加权最小二乘法 (WLS) 定义为最小化加权残差平方和。此外，考虑一个不依赖于方差模型正确设定的异方差稳健协方差估计量（通常称为“三明治”估计量）。您的程序必须：\n- 通过最小化加权残差平方和并求解得到的正规方程，从第一性原理推导 $\\beta$ 的 WLS 估计量。\n- 在经典假设下，即加权后的误差是条件同方差的，其方差与 $W^{-1}$ 成正比（其中 $W$ 是权重对角矩阵），计算 WLS 估计量的基于模型的协方差。\n- 为 WLS 估计量计算一个“三明治”协方差估计量，该估计量在未知形式的一般异方差性下是一致的。\n- 通过计算标准化残差的离散度量，量化加权使得残差近似同方差的程度。令 $r_i = y_i - x_i^\\top \\hat{\\beta}$ 表示残差，$z_i = \\sqrt{w_i}\\, r_i$ 表示标准化残差。将离散度量定义为 $D = \\mathrm{std}(|z|)/\\mathrm{mean}(|z|)$，其中 $\\mathrm{std}$ 是样本标准差，$\\mathrm{mean}$ 是样本均值。较小的 $D$ 值表示加权后更接近同方差性。\n\n使用以下测试套件，每个案例提供 $(X, y, w)$，有 $n = 6$ 个观测值和 $p = 2$ 个预测变量（截距和斜率）。在所有案例中，设计矩阵 $X$ 的行均为 $[1, x_i]$，其中 $x_i \\in \\{\\,0, 1, 2, 3, 4, 5\\,\\}$，即 $x_0 = 0$，$x_1 = 1$，$x_2 = 2$，$x_3 = 3$，$x_4 = 4$，$x_5 = 5$。\n\n案例 A（基线，同方差独立误差，等权重）：\n- 真实系数：$\\beta = [\\,1,\\,2\\,]$。\n- 误差：$\\epsilon = [\\,0.2,\\,-0.1,\\,0.0,\\,0.1,\\,-0.2,\\,0.0\\,]$。\n- 响应：$y_i = 1 + 2\\,x_i + \\epsilon_i$，对于 $i \\in \\{0,1,2,3,4,5\\}$。\n- 权重：$w = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$。\n\n案例 B（异方差误差，权重等于方差的倒数，独立性成立）：\n- 真实系数：$\\beta = [\\,1,\\,2\\,]$。\n- 标准化误差模式：$z = [\\,0.5,\\,-0.5,\\,0.5,\\,-0.5,\\,0.5,\\,-0.5\\,]$。\n- 权重：$w = [\\,4.0,\\,4.0,\\,1.0,\\,1.0,\\,0.25,\\,0.25\\,]$。\n- 误差：$\\epsilon_i = z_i / \\sqrt{w_i}$，对于 $i \\in \\{0,1,2,3,4,5\\}$。\n- 响应：$y_i = 1 + 2\\,x_i + \\epsilon_i$。\n\n案例 C（与案例 B 数据相同，权重设定错误，独立性成立）：\n- 使用与案例 B 中相同的 $X$ 和 $y$。\n- 权重：$w = [\\,0.25,\\,0.25,\\,1.0,\\,1.0,\\,4.0,\\,4.0\\,]$。\n\n案例 D（极端权重和强杠杆作用的边缘案例）：\n- 真实系数：$\\beta = [\\,0,\\,-1\\,]$。\n- 标准化误差模式：$z = [\\,1,\\,-1,\\,1,\\,-1,\\,1,\\,-1\\,]$。\n- 权重：$w = [\\,1000.0,\\,0.001,\\,10.0,\\,0.1,\\,50.0,\\,0.5\\,]$。\n- 误差：$\\epsilon_i = z_i / \\sqrt{w_i}$，对于 $i \\in \\{0,1,2,3,4,5\\}$。\n- 响应：$y_i = 0 - 1\\,x_i + \\epsilon_i = -x_i + \\epsilon_i$。\n\n对每个案例，计算：\n1. WLS 估计量 $\\hat{\\beta}$。\n2. 在假设 $\\mathrm{Var}(\\epsilon \\mid X) = \\sigma^2 W^{-1}$ 下，由加权残差平方和估计出的 $\\hat{\\sigma}^2$ 计算 $\\hat{\\beta}$ 的基于模型的标准误。\n3. $\\hat{\\beta}$ 的三明治（异方差稳健）标准误。\n4. 标准化残差的离散度量 $D$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含每个案例的一个包含七个浮点数的列表，顺序为 $[\\,\\hat{\\beta}_0,\\,\\hat{\\beta}_1,\\,\\mathrm{se}_{\\mathrm{model},0},\\,\\mathrm{se}_{\\mathrm{model},1},\\,\\mathrm{se}_{\\mathrm{sand},0},\\,\\mathrm{se}_{\\mathrm{sand},1},\\,D\\,]$。\n- 将四个案例的结果汇总到一个列表中，以逗号分隔的列表形式打印在一行上，并用方括号括起来，例如 $[[\\cdot],[\\cdot],[\\cdot],[\\cdot]]$。\n\n您的程序必须是一个完整的、可运行的程序，无需外部输入。所有计算必须是确定性的，并直接遵循上述定义和假设。将所有数值结果在最终输出列表中表示为浮点数。",
            "solution": "该问题要求推导和应用加权最小二乘法 (WLS) 估计量，以及其系数的两种协方差估计量：一种是基于模型的估计量，另一种是异方差稳健（三明治）估计量。我们将首先从第一性原理推导必要的公式，然后概述所提供测试案例的计算步骤。\n\n标准线性模型由 $Y = X\\beta + \\epsilon$ 给出，其中 $Y$ 是一个 $n \\times 1$ 的观测向量，$X$ 是一个 $n \\times p$ 的秩为 $p$ 的设计矩阵，$\\beta$ 是一个 $p \\times 1$ 的未知系数向量，$\\epsilon$ 是一个 $n \\times 1$ 的不可观测误差向量，其条件均值为 $E[\\epsilon \\mid X] = 0$。在普通最小二乘法 (OLS) 中，我们假设误差是不相关的并且具有恒定方差，即 $\\mathrm{Var}(\\epsilon \\mid X) = \\sigma^2 I_n$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\nWLS 处理异方差性的情况，其中误差方差不相等：$\\mathrm{Var}(\\epsilon_i \\mid X) = \\sigma_i^2$。我们引入一组已知的正权重 $w_i$，通常选择与误差方差成反比，$w_i \\propto 1/\\sigma_i^2$。\n\nWLS 估计量通过最小化加权残差平方和 (WRSS) 找到：\n$$\nS(\\beta) = \\sum_{i=1}^{n} w_i (y_i - x_i^\\top \\beta)^2\n$$\n其中 $y_i$ 是第 $i$ 个观测值，$x_i^\\top$ 是矩阵 $X$ 的第 $i$ 行。令 $W$ 为对角线上为权重 $w_i$ 的 $n \\times n$ 对角矩阵。目标函数可以写成矩阵形式：\n$$\nS(\\beta) = (Y - X\\beta)^\\top W (Y - X\\beta)\n$$\n为了找到最小化 $S(\\beta)$ 的估计量 $\\hat{\\beta}$，我们将 $S(\\beta)$ 对 $\\beta$ 求导，并令结果为零。\n$$\n\\frac{\\partial S(\\beta)}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} (Y^\\top W Y - 2\\beta^\\top X^\\top W Y + \\beta^\\top X^\\top W X \\beta) = -2X^\\top W Y + 2X^\\top W X \\beta\n$$\n将导数设为零，得到 WLS 正规方程：\n$$\nX^\\top W X \\hat{\\beta} = X^\\top W Y\n$$\n假设矩阵 $X^\\top W X$ 是可逆的（如果 $X$ 具有满列秩且所有 $w_i > 0$，则此条件成立），我们可以解出 WLS 估计量 $\\hat{\\beta}$：\n$$\n\\hat{\\beta}_{WLS} = (X^\\top W X)^{-1} X^\\top W Y\n$$\n该估计量是 $Y$ 的线性函数，并且在条件 $E[\\epsilon \\mid X] = 0$ 下是无偏的：\n$E[\\hat{\\beta} \\mid X] = E[(X^\\top W X)^{-1} X^\\top W (X\\beta + \\epsilon) \\mid X] = \\beta + (X^\\top W X)^{-1} X^\\top W E[\\epsilon \\mid X] = \\beta$。\n\n接下来，我们推导 $\\hat{\\beta}$ 的协方差矩阵。估计量与真实值之间的偏差是：\n$$\n\\hat{\\beta} - \\beta = (X^\\top W X)^{-1} X^\\top W \\epsilon\n$$\n则 $\\hat{\\beta}$ 的协方差矩阵为：\n$$\n\\mathrm{Var}(\\hat{\\beta} \\mid X) = E[(\\hat{\\beta}-\\beta)(\\hat{\\beta}-\\beta)^\\top \\mid X] = (X^\\top W X)^{-1} X^\\top W E[\\epsilon\\epsilon^\\top \\mid X] W X (X^\\top W X)^{-1}\n$$\n令 $\\Omega = E[\\epsilon\\epsilon^\\top \\mid X]$ 为误差的真实协方差矩阵。$\\hat{\\beta}$ 的协方差矩阵的一般形式为：\n$$\n\\mathrm{Var}(\\hat{\\beta} \\mid X) = (X^\\top W X)^{-1} (X^\\top W \\Omega W X) (X^\\top W X)^{-1}\n$$\n\n标准误的计算取决于对 $\\Omega$ 所作的假设。\n\n**1. 基于模型的协方差估计量**\n这种方法假设权重被正确指定，即它们与真实的误差方差成反比。这意味着对于某个未知的常数 $\\sigma^2$，有 $\\mathrm{Var}(\\epsilon_i \\mid X) = \\sigma^2/w_i$。以矩阵形式表示，$\\Omega = \\sigma^2 W^{-1}$。将此代入通用协方差公式可显著简化：\n$$\n\\mathrm{Var}(\\hat{\\beta} \\mid X) = (X^\\top W X)^{-1} X^\\top W (\\sigma^2 W^{-1}) W X (X^\\top W X)^{-1} = \\sigma^2 (X^\\top W X)^{-1}\n$$\n为了使其可操作，我们需要估计 $\\sigma^2$。在 $\\Omega = \\sigma^2 W^{-1}$ 的假设下，变换后的误差 $\\epsilon^* = W^{1/2}\\epsilon$ 是同方差的，其方差为 $\\sigma^2 I_n$。$\\sigma^2$ 的一个无偏估计量可以从回归的加权残差平方和除以自由度 $n-p$ 得出：\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^{n} w_i(y_i - x_i^\\top \\hat{\\beta})^2 = \\frac{(Y-X\\hat{\\beta})^\\top W (Y-X\\hat{\\beta})}{n-p}\n$$\n因此，估计的基于模型的协方差矩阵为：\n$$\n\\widehat{\\mathrm{Cov}}_{\\mathrm{model}}(\\hat{\\beta}) = \\hat{\\sigma}^2 (X^\\top W X)^{-1}\n$$\n系数 $\\hat{\\beta}_j$ 的基于模型的标准误是该矩阵对角线元素的平方根。该估计量仅在假设 $\\Omega = \\sigma^2 W^{-1}$ 成立时才有效。\n\n**2. 三明治（异方差稳健）协方差估计量**\n该估计量对权重矩阵 $W$ 的错误指定具有稳健性。它不假设 $\\Omega = \\sigma^2 W^{-1}$。相反，它直接估计通用协方差公式的中间部分（“肉”）。我们通常假设误差不相关但可能存在异方差，因此 $\\Omega = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, ..., \\sigma_n^2)$。“肉”项为 $M = X^\\top W \\Omega W X = \\sum_{i=1}^n w_i^2 \\sigma_i^2 x_i x_i^\\top$。\n为了估计 $M$，我们用其经验估计，即残差平方 $r_i^2 = (y_i - x_i^\\top \\hat{\\beta})^2$，来替换未知的方差 $\\sigma_i^2$。估计的“肉”矩阵为：\n$$\n\\hat{M} = \\sum_{i=1}^{n} w_i^2 r_i^2 x_i x_i^\\top = X^\\top W \\hat{\\Omega} W X\n$$\n其中 $\\hat{\\Omega} = \\mathrm{diag}(r_1^2, r_2^2, ..., r_n^2)$。三明治协方差估计量则为：\n$$\n\\widehat{\\mathrm{Cov}}_{\\mathrm{sand}}(\\hat{\\beta}) = (X^\\top W X)^{-1} \\hat{M} (X^\\top W X)^{-1}\n$$\n三明治标准误是该矩阵对角线元素的平方根。它们之所以是“一致的”，是因为即使权重 $w_i$ 没有正确地为异方差性建模，它们也能提供对 $\\hat{\\beta}$ 真实抽样变异性的有效近似。当权重被正确指定时（如案例 B），基于模型的估计量和三明治估计量应得出相似的结果。当权重被错误指定时（如案例 C），预计它们会产生差异。\n\n**3. 离散度量 $D$**\n为了评估加权方案在多大程度上实现了同方差性，我们检查标准化残差。原始残差为 $r_i = y_i - x_i^\\top \\hat{\\beta}$。标准化残差，对应于变换后的 OLS 问题的残差，为 $z_i = \\sqrt{w_i} r_i$。如果加权成功，$z_i$ 应具有大致恒定的方差。问题定义了一个离散度量 $D$ 来量化这一点：\n$$\nD = \\frac{\\mathrm{std}(|z|)}{\\mathrm{mean}(|z|)}\n$$\n其中 $|z|$ 是标准化残差绝对值的向量，$\\mathrm{std}$ 是样本标准差（使用分母 $n-1$），$\\mathrm{mean}$ 是样本均值。较小的 $D$ 值表示标准化残差的绝对值相对于其平均大小具有较低的变异性，这表明加权后更接近同方呈性。例如，如果所有 $|z_i|$ 都相同，则 $\\mathrm{std}(|z|)$ 将为 $0$，因此 $D$ 也为 $0$，表示在量级上完全同方差。",
            "answer": "```python\nimport numpy as np\n\ndef solve_case(X, y, w):\n    \"\"\"\n    Performs WLS estimation and computes required statistics for a single case.\n    \n    Args:\n        X (np.ndarray): Design matrix (n x p).\n        y (np.ndarray): Response vector (n x 1).\n        w (np.ndarray): Weight vector (n x 1).\n\n    Returns:\n        list: A list of 7 floats containing beta_hat, model-based SEs, \n              sandwich SEs, and the dispersion metric D.\n    \"\"\"\n    n, p = X.shape\n    W = np.diag(w)\n\n    # 1. Compute WLS estimator beta_hat\n    # beta_hat = (X^T W X)^-1 X^T W y\n    XT_W = X.T @ W\n    XT_W_X = XT_W @ X\n    inv_XT_W_X = np.linalg.inv(XT_W_X)\n    XT_W_y = XT_W @ y\n    beta_hat = inv_XT_W_X @ XT_W_y\n\n    # Compute residuals\n    residuals = y - X @ beta_hat\n    \n    # 2. Compute model-based standard errors\n    # cov_model = sigma_hat^2 * (X^T W X)^-1\n    # sigma_hat^2 = sum(w_i * r_i^2) / (n - p)\n    wrss = np.sum(w * residuals**2)\n    sigma_sq_hat = wrss / (n - p)\n    cov_model = sigma_sq_hat * inv_XT_W_X\n    se_model = np.sqrt(np.diag(cov_model))\n    \n    # 3. Compute sandwich (heteroskedasticity-consistent) standard errors\n    # cov_sand = (X^T W X)^-1 * (X^T W Omega_hat W X) * (X^T W X)^-1\n    # where Omega_hat = diag(r_i^2)\n    # The \"meat\" is M_hat = sum(w_i^2 * r_i^2 * x_i * x_i^T)\n    meat = np.zeros((p, p))\n    for i in range(n):\n        xi = X[i, :].reshape(p, 1)\n        meat += (w[i]**2) * (residuals[i]**2) * (xi @ xi.T)\n    \n    cov_sand = inv_XT_W_X @ meat @ inv_XT_W_X\n    se_sand = np.sqrt(np.diag(cov_sand))\n    \n    # 4. Compute dispersion metric D\n    # z_i = sqrt(w_i) * r_i\n    # D = std(|z|) / mean(|z|)\n    z = np.sqrt(w) * residuals\n    abs_z = np.abs(z)\n    \n    # Check for mean(|z|) == 0 to avoid division by zero\n    mean_abs_z = np.mean(abs_z)\n    if mean_abs_z == 0:\n        D = 0.0 # If all residuals are zero, variance is zero, dispersion is zero.\n    else:\n        # Use ddof=1 for sample standard deviation\n        std_abs_z = np.std(abs_z, ddof=1)\n        D = std_abs_z / mean_abs_z\n\n    return [\n        beta_hat[0], beta_hat[1],\n        se_model[0], se_model[1],\n        se_sand[0], se_sand[1],\n        D\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run calculations, and print results.\n    \"\"\"\n    x_i = np.array([0, 1, 2, 3, 4, 5], dtype=float)\n    X = np.vstack([np.ones_like(x_i), x_i]).T\n\n    # Case A\n    beta_a = np.array([1.0, 2.0])\n    eps_a = np.array([0.2, -0.1, 0.0, 0.1, -0.2, 0.0])\n    y_a = X @ beta_a + eps_a\n    w_a = np.ones(6)\n\n    # Case B\n    beta_b = np.array([1.0, 2.0])\n    z_b = np.array([0.5, -0.5, 0.5, -0.5, 0.5, -0.5])\n    w_b = np.array([4.0, 4.0, 1.0, 1.0, 0.25, 0.25])\n    eps_b = z_b / np.sqrt(w_b)\n    y_b = X @ beta_b + eps_b\n\n    # Case C\n    y_c = y_b  # same data as Case B\n    w_c = np.array([0.25, 0.25, 1.0, 1.0, 4.0, 4.0])\n\n    # Case D\n    beta_d = np.array([0.0, -1.0])\n    z_d = np.array([1.0, -1.0, 1.0, -1.0, 1.0, -1.0])\n    w_d = np.array([1000.0, 0.001, 10.0, 0.1, 50.0, 0.5])\n    eps_d = z_d / np.sqrt(w_d)\n    y_d = X @ beta_d + eps_d\n\n    test_cases = [\n        (X, y_a, w_a),\n        (X, y_b, w_b),\n        (X, y_c, w_c),\n        (X, y_d, w_d),\n    ]\n\n    all_results = []\n    for X_case, y_case, w_case in test_cases:\n        result = solve_case(X_case, y_case, w_case)\n        all_results.append(result)\n\n    # Format output as a string representing a list of lists.\n    # e.g., \"[[r1, r2, ...], [r1, r2, ...]]\"\n    case_strings = []\n    for case_result in all_results:\n        case_strings.append(f\"[{','.join(f'{x:.7f}' for x in case_result)}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在回归分析中，并非所有数据点的影响力都相同。某些观测值由于其在预测变量空间中的位置很极端（高杠杆点），或者其残差特别大，可能对模型的拟合结果产生不成比例的影响。本练习  将带您亲手计算杠杆值和库克距离这两个关键的诊断指标，学会区分具有高“潜在”影响力的点和那些“实际”上对模型有巨大影响的强影响点。",
            "id": "3099870",
            "problem": "您正在使用线性回归的普通最小二乘法框架。其基础如下：给定一个满列秩的设计矩阵 $X$ 和一个响应向量 $y$，普通最小二乘估计量通过最小化残差平方和来确定，其拟合值是 $y$ 在 $X$ 列空间上的正交投影。投影算子的对角线元素量化了每个观测值的杠杆值，而影响则是通过评估当单个观测值被扰动或移除时，拟合模型会发生多大变化来衡量的。在此任务中，您将为每个测试用例在 $X$ 中植入一个高杠杆点，根据与 $X$ 关联的投影算子计算杠杆值 $h_{ii}$ 和拟合残差，然后使用基于残差大小、杠杆值和模型自由度的库克距离 $D_i$ 定义来评估影响。\n\n操作要求：\n- 对于每个测试用例，通过添加一列截距项来增广 $X$，使得参数总数等于增广后 $X$ 的列数。\n- 使用增广后的 $X$ 拟合普通最小二乘模型并计算残差。\n- 计算杠杆值 $h_{ii}$，即与 $X$ 相关的投影算子的对角线项。\n- 使用其基于残差、杠杆值、参数数量和残差方差的定义来计算库克距离 $D_i$。\n- 使用以下检测规则：\n  - 高杠杆值经验法则：如果 $h_{ii} > \\frac{2p}{n}$，则将观测值 $i$ 标记为高杠杆值，其中 $p$ 是增广后 $X$ 的列数，$n$ 是观测值的数量。\n  - 影响点经验法则：如果 $D_i > \\frac{4}{n}$，则将观测值 $i$ 标记为影响点。\n- 对于每个测试用例，按顺序报告以下六个量：\n  $[$\n  植入索引 $i^\\star$ 的杠杆值 $h_{i^\\star i^\\star}$，四舍五入到六位小数，\n  具有最大杠杆值的观测值的基于 $0$ 的索引，\n  植入索引 $i^\\star$ 的库克距离 $D_{i^\\star}$，四舍五入到六位小数，\n  具有最大库克距离的观测值的基于 $0$ 的索引，\n  一个布尔值，指示根据经验法则，植入的索引是否为高杠杆值（使用 $True$ 或 $False$），\n  以及一个布尔值，指示根据经验法则，植入的索引是否为影响点（使用 $True$ 或 $False$）\n  $]$。\n\n您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个由逗号分隔并包含在方括号中的列表，每个测试用例的六个输出都包含在各自的方括号中；例如，$[$ $[\\dots], [\\dots], [\\dots]$ $]$，不含任何额外的空格或文本。\n\n测试套件（每个用例在 $X$ 中植入一个高杠杆行；索引为基于 $0$ 的，所有数字均为实值）：\n- 用例 $1$（单个预测变量，植入点处有中等残差）：\n  - $n = 10$，植入索引 $i^\\star = 9$。\n  - $X$ 的单列预测变量值（添加截距前）：$[-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]$。\n  - 响应 $y$：$[-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 21.2]$。\n- 用例 $2$（单个预测变量，植入点处有非常大的残差）：\n  - $n = 10$，植入索引 $i^\\star = 9$。\n  - 预测变量值：$[-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]$。\n  - 响应 $y$：$[-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 36.0]$。\n- 用例 $3$（两个预测变量，植入点在两个坐标上均为极端值）：\n  - $n = 12$，植入索引 $i^\\star = 11$。\n  - $X$ 的两列预测变量值（添加截距前）：\n    - 第一个预测变量：$[-1.0, 0.2, -0.3, 0.5, -0.1, 0.4, -0.7, 0.0, 0.3, -0.2, 0.6, 8.0]$。\n    - 第二个预测变量：$[0.5, -0.1, 0.2, -0.4, 0.3, -0.2, 0.1, -0.5, 0.4, -0.3, 0.0, -9.0]$。\n  - 响应 $y$：$[-1.5, 0.95, -0.17, 1.68, 0.04, 1.34, -0.64, 0.97, 0.57, 0.48, 1.4, 21.55]$。\n- 用例 $4$（单个预测变量，植入点离其余点极远，以将杠杆值推向上界 $1$）：\n  - $n = 8$，植入索引 $i^\\star = 7$。\n  - 预测变量值：$[-0.5, 0.2, -0.1, 0.3, -0.2, 0.1, 0.0, 50.0]$。\n  - 响应 $y$：$[1.51, 2.18, 1.93, 2.29, 1.8, 2.12, 1.99, 52.01]$。\n- 用例 $5$（单个预测变量，中等高杠杆值，噪声变化）：\n  - $n = 15$，植入索引 $i^\\star = 14$。\n  - 预测变量值：$[-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 0.1, -0.3, 0.5, 6.0]$。\n  - 响应 $y$：$[-0.3, -0.5, 0.0, -0.6, -0.1, 0.5, -0.2, 0.6, 0.1, 0.5, 0.0, 0.25, -0.45, 0.85, 2.0]$。\n\n实现说明：\n- 在所有用例中，都用一列全为1的截距项来增广 $X$。\n- 所有索引都使用基于 $0$ 的索引。\n- 将报告的植入索引的杠杆值和库克距离四舍五入到六位小数。\n-布尔值使用 $True$ 或 $False$。\n- 最终输出必须是精确格式的单行文本：一个顶层方括号列表，包含五个方括号子列表，每个子列表包含其测试用例的六个请求值，整行中用逗号分隔，不含任何空格。",
            "solution": "用户提供的问题已经过严格验证，并被确定为统计回归诊断领域内一个定义良好且具有科学依据的任务。问题陈述完整、客观且内部一致，允许得出唯一且有意义的解。\n\n该问题的核心是计算和解释两个关键的回归诊断指标：杠杆值和库克距离。这些指标用于识别对普通最小二乘法（OLS）回归分析结果具有异常大影响的观测值。\n\n### 理论框架\n\n标准多元线性回归模型如下：\n$$\ny = X\\beta + \\epsilon\n$$\n其中 $y$ 是一个 $n \\times 1$ 的观测值向量，$X$ 是一个 $n \\times p$ 的满列秩设计矩阵（包含预测变量，通常还有一个截距列），$\\beta$ 是一个 $p \\times 1$ 的未知系数向量，$\\epsilon$ 是一个 $n \\times 1$ 的不相关误差项向量，其均值为 $0$，方差为常数 $\\sigma^2$。\n\n$\\beta$ 的 OLS 估计量是最小化残差平方和 $RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ 的向量 $\\hat{\\beta}$。该估计量由正规方程的解给出：\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n\n拟合值是 $y$ 在 $X$ 列空间上的正交投影：\n$$\n\\hat{y} = X\\hat{\\beta} = X(X^T X)^{-1} X^T y = H y\n$$\n矩阵 $H = X(X^T X)^{-1} X^T$ 被称为投影矩阵或“帽子矩阵”，因为它将 $y$ 映射到 $\\hat{y}$。\n\n**杠杆值**\n第 $i$ 个观测值的杠杆值，记为 $h_{ii}$，是帽子矩阵 $H$ 的第 $i$ 个对角元素。它量化了观测响应 $y_i$ 对其自身拟合值 $\\hat{y}_i$ 的影响，因为 $\\hat{y}_i = \\sum_{j=1}^n H_{ij} y_j = h_{ii}y_i + \\sum_{j \\neq i} H_{ij} y_j$。杠杆值 $h_{ii}$ 完全由设计矩阵 $X$ 决定，代表了由于其在预测变量空间中的位置而成为影响点的“潜力”。远离预测数据云中心的点具有高杠杆值。杠杆值的范围是 $0 \\le h_{ii} \\le 1$，它们的总和等于参数数量，即 $\\sum_{i=1}^n h_{ii} = p$。一个常用的经验法则是，如果一个观测值的杠杆值超过平均杠杆值的两倍，即 $h_{ii} > 2p/n$，则将其标记为高杠杆点。\n\n**库克距离**\n杠杆值衡量的是潜在影响，而库克距离 $D_i$ 则衡量当移除第 $i$ 个观测值时，模型拟合值的实际总体变化。它是观测值杠杆值及其残差大小的综合度量。一个高杠杆值的观测值可能具有影响力，也可能不具有；如果其对应的残差也很大，它就变得具有影响力。库克距离定义为：\n$$\nD_i = \\frac{ \\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2 }{p \\cdot \\hat{\\sigma}^2}\n$$\n其中 $\\hat{y}_{j(i)}$ 是在没有观测值 $i$ 的情况下拟合模型得到的观测值 $j$ 的拟合值，而 $\\hat{\\sigma}^2$ 是使用所有观测值拟合模型得到的均方误差。一个更方便的计算公式是：\n$$\nD_i = \\frac{e_i^2}{p \\cdot \\hat{\\sigma}^2} \\left[ \\frac{h_{ii}}{(1 - h_{ii})^2} \\right]\n$$\n其中 $e_i = y_i - \\hat{y}_i$ 是第 $i$ 个残差，$\\hat{\\sigma}^2 = \\frac{1}{n-p}\\sum_{j=1}^n e_j^2$ 是均方误差（或残差方差）。一个常用的经验法则是，如果 $D_i > 4/n$，则认为该观测值具有影响力。\n\n### 算法流程\n\n对于每个测试用例，执行以下步骤以计算所需的六个量：\n1.  **构建设计矩阵**：将提供的预测变量值构造成一个矩阵，然后在其前面加上一列全为1的截距项。这样就得到了最终的 $n \\times p$ 设计矩阵 $X$。\n2.  **拟合 OLS 模型**：通过求解线性系统 $X^T X \\hat{\\beta} = X^T y$ 来计算 OLS 系数估计值 $\\hat{\\beta}$。这在数值上比显式计算 $X^T X$ 的逆矩阵更稳定。\n3.  **计算杠杆值**：计算帽子矩阵 $H = X (X^T X)^{-1} X^T$。然后从 $H$ 的对角线上提取杠杆值 $h_{ii}$。\n4.  **计算残差和均方误差**：计算拟合值 $\\hat{y} = X\\hat{\\beta}$，然后计算残差 $e = y - \\hat{y}$。接着计算均方误差 $\\hat{\\sigma}^2 = \\frac{e^T e}{n-p}$。\n5.  **计算库克距离**：使用杠杆值 $h_{ii}$、残差 $e_i$、参数数量 $p$ 和均方误差 $\\hat{\\sigma}^2$，利用上述公式计算所有观测值的库克距离 $D_i$。\n6.  **提取并报告结果**：\n    -   检索指定植入点 $i^\\star$ 的杠杆值 $h_{i^\\star i^\\star}$ 和库克距离 $D_{i^\\star}$。\n    -   找到具有最大杠杆值和最大库克距离的观测值的索引。\n    -   将植入点的杠杆值 $h_{i^\\star i^\\star}$ 与阈值 $2p/n$ 进行比较，以确定其是否为高杠杆点。\n    -   将植入点的库克距离 $D_{i^\\star}$ 与阈值 $4/n$ 进行比较，以确定其是否为影响点。\n    -   将这六个值编译成该测试用例的列表。\n\n对所有测试用例重复此过程，并将最终结果汇总为指定格式的单个字符串。",
            "answer": "```python\nimport numpy as np\n\ndef analyze_case(X_raw: np.ndarray, y: np.ndarray, istar: int):\n    \"\"\"\n    Performs leverage and influence analysis for a single OLS regression case.\n\n    Args:\n        X_raw: The raw predictor matrix (without intercept).\n        y: The response vector.\n        istar: The 0-based index of the planted point.\n\n    Returns:\n        A list containing the six required diagnostic quantities.\n    \"\"\"\n    n = len(y)\n    \n    # Ensure X_raw is a 2D array\n    if X_raw.ndim == 1:\n        X_raw = X_raw.reshape(-1, 1)\n\n    # 1. Construct Design Matrix with an intercept\n    X = np.c_[np.ones(n), X_raw]\n    n, p = X.shape\n\n    # 2. Fit OLS Model\n    try:\n        XtX = X.T @ X\n        Xty = X.T @ y\n        beta_hat = np.linalg.solve(XtX, Xty)\n        XtX_inv = np.linalg.inv(XtX)\n    except np.linalg.LinAlgError:\n         # Problem specification guarantees full column rank, so this is a fallback.\n        pinv_X = np.linalg.pinv(X)\n        beta_hat = pinv_X @ y\n        XtX_inv = np.linalg.pinv(XtX)\n\n    # 3. Compute Leverage\n    H = X @ XtX_inv @ X.T\n    h = np.diag(H)\n\n    # 4. Compute Residuals and MSE\n    y_hat = X @ beta_hat\n    e = y - y_hat\n    rss = e.T @ e\n    mse = rss / (n - p)\n\n    # 5. Compute Cook's Distances\n    # Using np.errstate to handle potential division by zero if h_ii = 1.\n    # In such a case, the residual e_i must be 0, leading to a 0/0 form.\n    # The calculated value would be NaN, but this doesn't occur in the test data.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        D = (e**2 / (p * mse)) * (h / (1 - h)**2)\n    D = np.nan_to_num(D, nan=0.0) # Replace any NaN with 0 for argmax consistency\n\n    # 6. Extract and Report Results\n    h_istar = h[istar]\n    max_h_idx = int(np.argmax(h))\n    \n    D_istar = D[istar]\n    max_D_idx = int(np.argmax(D))\n    \n    is_high_leverage = h_istar > (2 * p / n)\n    is_influential = D_istar > (4 / n)\n    \n    return [\n        round(h_istar, 6),\n        max_h_idx,\n        round(D_istar, 6),\n        max_D_idx,\n        is_high_leverage,\n        is_influential,\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"X_raw\": np.array([-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]),\n            \"y\": np.array([-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 21.2]),\n            \"istar\": 9\n        },\n        # Case 2\n        {\n            \"X_raw\": np.array([-1.2, 0.3, -0.5, 0.8, -0.1, 0.4, -0.7, 0.2, -0.3, 10.0]),\n            \"y\": np.array([-1.3, 1.55, 0.2, 2.5, 0.85, 1.65, -0.28, 1.32, 0.43, 36.0]),\n            \"istar\": 9\n        },\n        # Case 3\n        {\n            \"X_raw\": np.c_[\n                [-1.0, 0.2, -0.3, 0.5, -0.1, 0.4, -0.7, 0.0, 0.3, -0.2, 0.6, 8.0],\n                [0.5, -0.1, 0.2, -0.4, 0.3, -0.2, 0.1, -0.5, 0.4, -0.3, 0.0, -9.0]\n            ],\n            \"y\": np.array([-1.5, 0.95, -0.17, 1.68, 0.04, 1.34, -0.64, 0.97, 0.57, 0.48, 1.4, 21.55]),\n            \"istar\": 11\n        },\n        # Case 4\n        {\n            \"X_raw\": np.array([-0.5, 0.2, -0.1, 0.3, -0.2, 0.1, 0.0, 50.0]),\n            \"y\": np.array([1.51, 2.18, 1.93, 2.29, 1.8, 2.12, 1.99, 52.01]),\n            \"istar\": 7\n        },\n        # Case 5\n        {\n            \"X_raw\": np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 0.1, -0.3, 0.5, 6.0]),\n            \"y\": np.array([-0.3, -0.5, 0.0, -0.6, -0.1, 0.5, -0.2, 0.6, 0.1, 0.5, 0.0, 0.25, -0.45, 0.85, 2.0]),\n            \"istar\": 14\n        }\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        result = analyze_case(case[\"X_raw\"], case[\"y\"], case[\"istar\"])\n        all_results.append(result)\n\n    # Format the final output string exactly as required (no spaces)\n    result_strings = []\n    for res in all_results:\n        # Format: [leverage,max_lev_idx,cook_dist,max_cook_idx,is_high_lev,is_influential]\n        # Example: [0.97278,9,0.000955,2,True,False]\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{str(res[4])},{str(res[5])}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}