{
    "hands_on_practices": [
        {
            "introduction": "To begin, let's focus on the fundamental calculation of the Variance Inflation Factor (VIF). The VIF for a specific predictor is not arbitrary; it is directly computed from the coefficient of determination, $R^2$, of an auxiliary regression where that predictor is modeled by all other predictors. This exercise  provides a straightforward scenario to practice this core definition, building a solid foundation for understanding where the VIF value originates.",
            "id": "1938245",
            "problem": "In the context of multiple linear regression, multicollinearity is a phenomenon where one predictor variable in a model can be linearly predicted from the others with a substantial degree of accuracy. A key metric used to quantify the severity of multicollinearity is the Variance Inflation Factor (VIF).\n\nAn analyst is studying the factors that influence the price of a certain commodity. They build a multiple linear regression model with several predictor variables. To assess multicollinearity, they focus on a specific predictor, $X_j$. The analyst performs an auxiliary regression where $X_j$ is treated as the response variable and all other predictor variables in the original model are used as its predictors. The coefficient of determination, $R_j^2$, from this auxiliary regression is found to be $0.96$.\n\nBased on this information, calculate the Variance Inflation Factor for the regression coefficient $\\hat{\\beta}_j$ associated with the predictor $X_j$. Provide your answer as a single number.",
            "solution": "In multiple linear regression, the Variance Inflation Factor for coefficient $\\hat{\\beta}_{j}$ associated with predictor $X_{j}$ is defined by\n$$\n\\text{VIF}_{j}=\\frac{1}{1-R_{j}^{2}},\n$$\nwhere $R_{j}^{2}$ is the coefficient of determination from the auxiliary regression of $X_{j}$ on all the other predictors. This follows from the variance formula\n$$\n\\operatorname{Var}(\\hat{\\beta}_{j})=\\frac{\\sigma^{2}}{S_{X_{j}}^{2}\\left(1-R_{j}^{2}\\right)},\n$$\nso the inflation relative to the case with no collinearity among predictors is the factor $\\left(1-R_{j}^{2}\\right)^{-1}$.\n\nGiven $R_{j}^{2}=0.96$, compute the denominator:\n$$\n1-R_{j}^{2}=1-0.96=0.04.\n$$\nTherefore,\n$$\n\\text{VIF}_{j}=\\frac{1}{0.04}=25.\n$$",
            "answer": "$$\\boxed{25}$$"
        },
        {
            "introduction": "Now that we know how to calculate the VIF, we must learn to interpret its meaning. A high VIF score signals that the variance of a regression coefficient is being inflated by multicollinearity, but by how much? This practice  clarifies this by connecting VIF to the inflation of the coefficient's standard error, $\\text{SE}(\\hat{\\beta}_j)$, which is a more direct and intuitive measure of statistical uncertainty. Remember that the standard error is the square root of the variance, so the inflation factor for the standard error is simply $\\sqrt{\\text{VIF}}$.",
            "id": "1938212",
            "problem": "An econometrician is developing a multiple linear regression model to predict a country's Gross Domestic Product (GDP). The model includes several predictor variables, such as capital investment, labor force size, and average education level. During the diagnostic phase, the econometrician becomes concerned about multicollinearity, which occurs when predictor variables in a regression model are highly correlated with each other.\n\nTo quantify the severity of this issue for a specific coefficient estimate, $\\hat{\\beta}_j$, the econometrician calculates the Variance Inflation Factor (VIF). The VIF for a predictor $j$ measures how much the variance of the estimated regression coefficient is \"inflated\" by the presence of multicollinearity. It is defined as the ratio of the variance of $\\hat{\\beta}_j$ in the full model to the variance of $\\hat{\\beta}_j$ if it were part of a model where it is uncorrelated with the other predictors.\n\nAfter running the analysis, the econometrician finds that the VIF for the 'capital investment' coefficient is exactly 49. By what factor has the standard error of the coefficient estimate for 'capital investment' been inflated due to multicollinearity, compared to the hypothetical case where capital investment is completely uncorrelated with all other predictors in the model?",
            "solution": "The Variance Inflation Factor for predictor $j$ is defined as the ratio of the variance of $\\hat{\\beta}_{j}$ in the full model (with correlated predictors) to the variance it would have if predictor $j$ were uncorrelated with the others:\n$$\n\\text{VIF}_{j}=\\frac{\\operatorname{Var}_{\\text{full}}(\\hat{\\beta}_{j})}{\\operatorname{Var}_{\\text{uncorr}}(\\hat{\\beta}_{j})}.\n$$\nStandard error is the square root of variance, so the inflation factor for the standard error is the ratio\n$$\n\\frac{\\operatorname{SE}_{\\text{full}}(\\hat{\\beta}_{j})}{\\operatorname{SE}_{\\text{uncorr}}(\\hat{\\beta}_{j})}\n=\\sqrt{\\frac{\\operatorname{Var}_{\\text{full}}(\\hat{\\beta}_{j})}{\\operatorname{Var}_{\\text{uncorr}}(\\hat{\\beta}_{j})}}\n=\\sqrt{\\text{VIF}_{j}}.\n$$\nGiven $\\text{VIF}_{j}=49$, the standard error inflation factor is\n$$\n\\sqrt{49}=7.\n$$\nTherefore, the standard error has been inflated by a factor of $7$.",
            "answer": "$$\\boxed{7}$$"
        },
        {
            "introduction": "This final practice takes us from calculation and interpretation to hands-on verification through code. Here, you will not just use a given VIF but will actively construct a dataset with a precisely controlled level of collinearity . This computational exercise is designed to mechanize the theoretical relationships, allowing you to observe exactly how removing a redundant predictor reduces coefficient variance and solidifies your understanding of VIF's practical implications.",
            "id": "3150312",
            "problem": "You are asked to mechanize the relationship between multicollinearity and the Variance Inflation Factor (VIF) by constructing and analyzing artificial design matrices for linear regression. The task must be solved by writing a complete program. The fundamental base you must use is the ordinary linear model with a fixed design and homoscedastic errors: the response is modeled as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ with $\\boldsymbol{\\varepsilon}$ having independent components of variance $\\sigma^2$. The well-tested formula you must rely on is that the Ordinary Least Squares (OLS) estimator has variance-covariance matrix given by $\\operatorname{Var}(\\widehat{\\boldsymbol{\\beta}}) = \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}$. Do not assume any other shortcut formulas beyond this core fact and the definition of the Variance Inflation Factor (VIF) in statistical learning. Your analysis must quantify how removing a redundant predictor changes the standard error of another coefficient via the change in the Variance Inflation Factor (VIF).\n\nConstruct the design matrices as follows for each test case parameter pair $(n, r)$:\n- Generate three centered predictor columns of length $n$, denoted $x_1$, $u$, and $w$, by drawing independent samples from a standard normal distribution and subtracting their sample means.\n- Orthogonalize $u$ against $x_1$ using Gramâ€“Schmidt projection so that the sample inner product of $u$ with $x_1$ equals $0$.\n- Define $x_2 = r\\,x_1 + \\sqrt{1 - r^2}\\,u$, which ensures that the sample correlation between $x_1$ and $x_2$ equals $r$ and that $x_2$ is centered.\n- Orthogonalize $w$ against both $x_1$ and $u$ to obtain $x_3$, so that $x_3$ is centered and has zero sample inner product with $x_1$ and $u$ (and consequently with $x_2$).\n- Scale each of $x_1$, $u$, $x_2$, and $x_3$ to have squared Euclidean norm equal to $n$.\n\nLet the full design matrix be $\\mathbf{X}_{\\text{full}} = [x_1, x_2, x_3]$ and the reduced design be $\\mathbf{X}_{\\text{reduced}} = [x_1, x_3]$ (i.e., $x_2$ is removed). Treat the noise variance as $\\sigma^2 = 1$ throughout.\n\nFor each test case, compute the following quantities for the coefficient associated with $x_1$:\n1. The standard error in the full model, computed from $\\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}$.\n2. The standard error in the reduced model, computed from $\\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}$.\n3. The ratio of the reduced-model standard error to the full-model standard error.\n4. The Variance Inflation Factor (VIF) for $x_1$ in the full model, obtained by regressing $x_1$ on the other predictors and using the coefficient of determination to quantify inflation according to the definition of the Variance Inflation Factor (VIF) in statistical learning.\n5. The VIF for $x_1$ in the reduced model (i.e., after removing $x_2$).\n6. The ratio of the reduced-model VIF to the full-model VIF.\n\nYour program must implement the following test suite of $(n, r)$ pairs:\n- $(200, \\sqrt{3/4})$ to engineer a setting where removing the redundant feature halves the standard error of the $x_1$ coefficient.\n- $(200, 0.999)$ to probe a near-boundary case of extreme but not perfect multicollinearity.\n- $(200, 0)$ to test the no-redundancy edge case.\n- $(50, 0.6)$ to test moderate multicollinearity at smaller sample size.\n- $(500, 0.8)$ to test strong multicollinearity at larger sample size.\n\nAll outputs must be expressed as dimensionless real numbers (no physical units and no angle units). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes one list of six floating-point values in the order specified above. For example, the output format must be:\n$[ [\\text{SE}_{\\text{full}}, \\text{SE}_{\\text{reduced}}, \\text{SE ratio}, \\text{VIF}_{\\text{full}}, \\text{VIF}_{\\text{reduced}}, \\text{VIF ratio}], \\ldots ]$.",
            "solution": "The problem statement is computationally and theoretically sound, presenting a well-defined exercise in statistical learning. It is scientifically grounded in the principles of Ordinary Least Squares (OLS) regression, self-contained, and free of contradictions or ambiguities once the vector construction process is interpreted logically. The task requires mechanizing the relationship between multicollinearity, coefficient variance, and the Variance Inflation Factor (VIF) by implementing calculations from first principles.\n\nThe core of the problem lies in the model $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ where the errors $\\boldsymbol{\\varepsilon}$ are i.i.d. with variance $\\sigma^2$. The variance of the OLS estimator $\\widehat{\\boldsymbol{\\beta}}$ is given by $\\operatorname{Var}(\\widehat{\\boldsymbol{\\beta}}) = \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}$. We are instructed to use $\\sigma^2 = 1$ and to construct specific design matrices $\\mathbf{X}$ to analyze the variance of $\\widehat{\\beta}_1$, the coefficient for predictor $x_1$.\n\nFirst, let us derive the theoretical results that the computational procedure is expected to verify. The construction process for the predictors is key.\n1.  We start with centered random vectors and use a Gram-Schmidt-like procedure to create an orthonormal basis of vectors, which we denote $\\{v_1, v_2, v_3\\}$, each of length $n$.\n2.  The predictors $x_1$, $x_2$, and $x_3$ are then defined and scaled as follows:\n    - $x_1 = \\sqrt{n}\\,v_1$\n    - $x_2 = \\sqrt{n}\\,(r\\,v_1 + \\sqrt{1 - r^2}\\,v_2)$\n    - $x_3 = \\sqrt{n}\\,v_3$\n\nThis construction ensures that each predictor is centered (as they are linear combinations of centered vectors) and has a squared Euclidean norm of $n$. That is, $x_j^\\top x_j = n$ for $j \\in \\{1, 2, 3\\}$. The inner products (and thus correlations) between these predictors are fixed by the parameter $r$:\n- $x_1^\\top x_1 = (\\sqrt{n}\\,v_1)^\\top(\\sqrt{n}\\,v_1) = n\\,(v_1^\\top v_1) = n$.\n- $x_1^\\top x_3 = (\\sqrt{n}\\,v_1)^\\top(\\sqrt{n}\\,v_3) = n\\,(v_1^\\top v_3) = 0$ due to orthogonality of $v_1, v_3$.\n- $x_2^\\top x_3 = (\\sqrt{n}\\,(r\\,v_1 + \\sqrt{1 - r^2}\\,v_2))^\\top(\\sqrt{n}\\,v_3) = n\\,(r\\,v_1^\\top v_3 + \\sqrt{1 - r^2}\\,v_2^\\top v_3) = 0$.\n- $x_1^\\top x_2 = (\\sqrt{n}\\,v_1)^\\top(\\sqrt{n}\\,(r\\,v_1 + \\sqrt{1 - r^2}\\,v_2)) = n\\,(r\\,v_1^\\top v_1 + \\sqrt{1 - r^2}\\,v_1^\\top v_2) = n\\,r$.\nThe sample correlation between $x_1$ and $x_2$ is $\\frac{x_1^\\top x_2}{\\sqrt{(x_1^\\top x_1)(x_2^\\top x_2)}} = \\frac{nr}{\\sqrt{n \\cdot n}} = r$.\n\n**Analysis of the Reduced Model**\nThe reduced design matrix is $\\mathbf{X}_{\\text{reduced}} = [x_1, x_3]$. The Gram matrix is:\n$$\n\\mathbf{X}_{\\text{reduced}}^\\top \\mathbf{X}_{\\text{reduced}} =\n\\begin{pmatrix}\nx_1^\\top x_1 & x_1^\\top x_3 \\\\\nx_3^\\top x_1 & x_3^\\top x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nn & 0 \\\\\n0 & n\n\\end{pmatrix}\n= n\\mathbf{I}_2\n$$\nThe inverse is $(\\mathbf{X}_{\\text{reduced}}^\\top \\mathbf{X}_{\\text{reduced}})^{-1} = \\frac{1}{n}\\mathbf{I}_2$. The variance of $\\widehat{\\beta}_1$ is the first diagonal element of $\\sigma^2 (\\mathbf{X}_{\\text{reduced}}^\\top \\mathbf{X}_{\\text{reduced}})^{-1}$. With $\\sigma^2=1$, we get $\\operatorname{Var}(\\widehat{\\beta}_1)_{\\text{reduced}} = \\frac{1}{n}$.\nThe standard error is $\\text{SE}_{\\text{reduced}} = \\sqrt{1/n} = 1/\\sqrt{n}$.\n\n**Analysis of the Full Model**\nThe full design matrix is $\\mathbf{X}_{\\text{full}} = [x_1, x_2, x_3]$. The Gram matrix is:\n$$\n\\mathbf{X}_{\\text{full}}^\\top \\mathbf{X}_{\\text{full}} =\n\\begin{pmatrix}\nx_1^\\top x_1 & x_1^\\top x_2 & x_1^\\top x_3 \\\\\nx_2^\\top x_1 & x_2^\\top x_2 & x_2^\\top x_3 \\\\\nx_3^\\top x_1 & x_3^\\top x_2 & x_3^\\top x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nn & nr & 0 \\\\\nnr & n & 0 \\\\\n0 & 0 & n\n\\end{pmatrix}\n$$\nThe inverse of this block-diagonal matrix is:\n$$\n(\\mathbf{X}_{\\text{full}}^\\top \\mathbf{X}_{\\text{full}})^{-1} =\n\\begin{pmatrix}\n\\left( \\begin{smallmatrix} n & nr \\\\ nr & n \\end{smallmatrix} \\right)^{-1} & \\mathbf{0} \\\\\n\\mathbf{0} & n^{-1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{n^2(1-r^2)} \\begin{smallmatrix} n & -nr \\\\ -nr & n \\end{smallmatrix} & \\mathbf{0} \\\\\n\\mathbf{0} & n^{-1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{n(1-r^2)} & \\frac{-r}{n(1-r^2)} & 0 \\\\\n\\frac{-r}{n(1-r^2)} & \\frac{1}{n(1-r^2)} & 0 \\\\\n0 & 0 & \\frac{1}{n}\n\\end{pmatrix}\n$$\nThe variance of $\\widehat{\\beta}_1$ is the $(1,1)$ element: $\\operatorname{Var}(\\widehat{\\beta}_1)_{\\text{full}} = \\frac{1}{n(1-r^2)}$.\nThe standard error is $\\text{SE}_{\\text{full}} = \\sqrt{\\frac{1}{n(1-r^2)}} = \\frac{1}{\\sqrt{n}\\sqrt{1-r^2}}$.\n\nThe ratio of the standard errors is $\\frac{\\text{SE}_{\\text{reduced}}}{\\text{SE}_{\\text{full}}} = \\frac{1/\\sqrt{n}}{1/(\\sqrt{n}\\sqrt{1-r^2})} = \\sqrt{1-r^2}$.\n\n**Variance Inflation Factor (VIF) Analysis**\nThe VIF for a predictor $x_j$ is defined as $\\text{VIF}_j = 1/(1 - R_j^2)$, where $R_j^2$ is the coefficient of determination from regressing $x_j$ on the other predictors in the model.\nFor the reduced model, we compute $\\text{VIF}_{1, \\text{reduced}}$ by regressing $x_1$ on $x_3$. Since $x_1^\\top x_3 = 0$, they are orthogonal. The regression coefficient is $0$, leading to $R^2=0$. Thus, $\\text{VIF}_{1, \\text{reduced}} = 1/(1-0) = 1$.\nFor the full model, we compute $\\text{VIF}_{1, \\text{full}}$ by regressing $x_1$ on $x_2$ and $x_3$. Because $x_3$ is orthogonal to both $x_1$ and $x_2$, the regression of $x_1$ on $\\{x_2, x_3\\}$ simplifies to the regression of $x_1$ on $x_2$. The $R^2$ for this simple regression is simply the square of the sample correlation coefficient, so $R_1^2 = (\\text{corr}(x_1, x_2))^2 = r^2$.\nTherefore, $\\text{VIF}_{1, \\text{full}} = \\frac{1}{1 - r^2}$.\n\nThe ratio of the VIFs is $\\frac{\\text{VIF}_{1, \\text{reduced}}}{\\text{VIF}_{1, \\text{full}}} = \\frac{1}{1/(1-r^2)} = 1-r^2$.\nNotice that $(\\text{SE ratio})^2 = (\\sqrt{1-r^2})^2 = 1-r^2 = \\text{VIF ratio}$.\n\nThe following program implements the computational steps to arrive at these quantities from first principles for each given test case $(n, r)$, confirming these analytical relationships numerically. The procedure is as follows:\n1.  Generate and center three random vectors.\n2.  Use Gram-Schmidt to find an orthonormal basis for the space they span.\n3.  Construct predictors $x_1$, $x_2$, $x_3$ with the specified correlation structure and norm.\n4.  Form the full and reduced design matrices.\n5.  Compute the standard errors using the formula $\\text{SE}_j = \\sqrt{(\\sigma^2(\\mathbf{X}^\\top\\mathbf{X})^{-1})_{jj}}$ with $\\sigma^2=1$.\n6.  Compute the VIFs by performing the necessary auxiliary OLS regressions to find the relevant $R^2$ values.\n7.  Calculate the required ratios.\nThe results for each test case are then aggregated and printed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_metrics(n, r):\n    \"\"\"\n    Constructs design matrices and computes multicollinearity metrics for a given n and r.\n\n    Args:\n        n (int): The number of samples (rows) in the design matrix.\n        r (float): The target correlation between predictors x1 and x2.\n\n    Returns:\n        list: A list containing six float values:\n              [se_full, se_reduced, se_ratio, vif_full, vif_reduced, vif_ratio]\n    \"\"\"\n    # Set a seed for reproducibility of the random vectors.\n    np.random.seed(0)\n    sigma2 = 1.0\n\n    # 1. Generate three centered predictor columns.\n    x1_raw = np.random.randn(n)\n    u_raw = np.random.randn(n)\n    w_raw = np.random.randn(n)\n\n    x1_c = x1_raw - np.mean(x1_raw)\n    u_c = u_raw - np.mean(u_raw)\n    w_c = w_raw - np.mean(w_raw)\n\n    # 2. Create an orthonormal basis {v1, v2, v3} using Gram-Schmidt.\n    v1 = x1_c / np.linalg.norm(x1_c)\n    \n    u_perp = u_c - np.dot(u_c, v1) * v1\n    v2 = u_perp / np.linalg.norm(u_perp)\n    \n    w_perp = w_c - np.dot(w_c, v1) * v1 - np.dot(w_c, v2) * v2\n    v3 = w_perp / np.linalg.norm(w_perp)\n\n    # 3. Define x1, x2, x3 and scale to have squared norm n.\n    x1 = np.sqrt(n) * v1\n    x2 = np.sqrt(n) * (r * v1 + np.sqrt(1 - r**2) * v2)\n    x3 = np.sqrt(n) * v3\n\n    # 4. Form full and reduced design matrices.\n    X_full = np.stack([x1, x2, x3], axis=1)\n    X_reduced = np.stack([x1, x3], axis=1)\n\n    # 5. Compute standard errors from the variance-covariance matrix.\n    # Full model\n    cov_matrix_full = sigma2 * np.linalg.inv(X_full.T @ X_full)\n    se_full = np.sqrt(cov_matrix_full[0, 0])\n\n    # Reduced model\n    cov_matrix_reduced = sigma2 * np.linalg.inv(X_reduced.T @ X_reduced)\n    se_reduced = np.sqrt(cov_matrix_reduced[0, 0])\n    \n    se_ratio = se_reduced / se_full\n\n    # 6. Compute Variance Inflation Factors (VIFs).\n    # VIF is 1 / (1 - R^2), where R^2 is from regressing a predictor on others.\n\n    # VIF for x1 in the full model\n    X_other_full = np.stack([x2, x3], axis=1)\n    # OLS: beta_hat = (X'X)^-1 X'y\n    beta_hat_full = np.linalg.solve(X_other_full.T @ X_other_full, X_other_full.T @ x1)\n    x1_pred_full = X_other_full @ beta_hat_full\n    \n    # R^2 = 1 - RSS/TSS. Since x1 is centered, TSS = sum(x1^2) = x1.T @ x1\n    tss_full = x1.T @ x1\n    rss_full = (x1 - x1_pred_full).T @ (x1 - x1_pred_full)\n    r_squared_full = 1.0 - rss_full / tss_full\n    vif_full = 1.0 / (1.0 - r_squared_full)\n\n    # VIF for x1 in the reduced model\n    X_other_reduced = x3[:, np.newaxis] # Treat as a column vector\n    beta_hat_reduced = np.linalg.solve(X_other_reduced.T @ X_other_reduced, X_other_reduced.T @ x1)\n    x1_pred_reduced = X_other_reduced @ beta_hat_reduced\n\n    tss_reduced = x1.T @ x1\n    rss_reduced = (x1 - x1_pred_reduced).T @ (x1 - x1_pred_reduced)\n    r_squared_reduced = 1.0 - rss_reduced / tss_reduced\n    vif_reduced = 1.0 / (1.0 - r_squared_reduced)\n    \n    vif_ratio = vif_reduced / vif_full\n\n    return [se_full, se_reduced, se_ratio, vif_full, vif_reduced, vif_ratio]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (200, np.sqrt(3/4)),\n        (200, 0.999),\n        (200, 0.0),\n        (50, 0.6),\n        (500, 0.8)\n    ]\n\n    all_results = []\n    for n, r in test_cases:\n        result_list = compute_metrics(n, r)\n        all_results.append(result_list)\n\n    # Format the final output as a string representing a list of lists.\n    # The str() of a list automatically includes brackets and commas.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}