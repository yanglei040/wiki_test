{
    "hands_on_practices": [
        {
            "introduction": "The journey to understanding the Gauss-Markov theorem begins with its core vocabulary. Before we can appreciate why an estimator is the \"best,\" we must first define the class of estimators we are working with: *Linear Unbiased Estimators*. This exercise  provides a foundational practice in identifying these properties, helping you distinguish which potential estimates for a parameter are constructed as a simple weighted average of the data (linear) and which will, on average, hit the true target (unbiased).",
            "id": "1919547",
            "problem": "In statistical modeling, an estimator is a rule for calculating an estimate of a given quantity based on observed data. Consider a simple model where a series of $n$ measurements, $Y_1, Y_2, \\dots, Y_n$, are taken of a constant but unknown physical quantity $\\mu$. The model is given by:\n$$Y_i = \\mu + \\epsilon_i, \\quad \\text{for } i=1, \\dots, n$$\nThe terms $\\epsilon_i$ represent random measurement errors. These errors are assumed to be uncorrelated, with an expected value of zero ($E[\\epsilon_i]=0$) and a constant variance of $\\text{Var}(\\epsilon_i)=\\sigma^2$ for all $i$.\n\nAn estimator $\\hat{\\mu}$ for the parameter $\\mu$ is defined as *linear* if it can be written as a linear combination of the observations: $\\hat{\\mu} = \\sum_{i=1}^n c_i Y_i$, where the coefficients $c_i$ are constants.\nAn estimator $\\hat{\\mu}$ is defined as *unbiased* if its expected value is equal to the true parameter it is estimating, i.e., $E[\\hat{\\mu}]=\\mu$.\n\nThree different estimators are proposed to estimate $\\mu$ based on the set of observations. Assume $n > 1$.\n1.  $\\hat{\\mu}_1 = \\frac{1}{n} \\sum_{i=1}^n Y_i$\n2.  $\\hat{\\mu}_2 = \\frac{1}{n-1} \\sum_{i=1}^n Y_i$\n3.  $\\hat{\\mu}_3 = \\frac{Y_1 + Y_n}{2}$\n\nWhich of these three estimators are both linear and unbiased?\n\nA. $\\hat{\\mu}_1$ only\n\nB. $\\hat{\\mu}_3$ only\n\nC. $\\hat{\\mu}_1$ and $\\hat{\\mu}_2$\n\nD. $\\hat{\\mu}_1$ and $\\hat{\\mu}_3$\n\nE. All three estimators are linear and unbiased.",
            "solution": "We are given the model $Y_{i}=\\mu+\\epsilon_{i}$ with $E[\\epsilon_{i}]=0$ and $\\text{Var}(\\epsilon_{i})=\\sigma^{2}$, and the $\\epsilon_{i}$ are uncorrelated. Therefore, for each $i$,\n$$\nE[Y_{i}]=E[\\mu+\\epsilon_{i}]=\\mu+E[\\epsilon_{i}]=\\mu.\n$$\nA linear estimator has the form $\\hat{\\mu}=\\sum_{i=1}^{n}c_{i}Y_{i}$. Its expectation is\n$$\nE[\\hat{\\mu}]=E\\Bigg[\\sum_{i=1}^{n}c_{i}Y_{i}\\Bigg]=\\sum_{i=1}^{n}c_{i}E[Y_{i}]=\\sum_{i=1}^{n}c_{i}\\mu=\\mu\\sum_{i=1}^{n}c_{i}.\n$$\nThus, the necessary and sufficient condition for unbiasedness is\n$$\n\\sum_{i=1}^{n}c_{i}=1.\n$$\n\nNow check each proposed estimator.\n\n1) For $\\hat{\\mu}_{1}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$, the coefficients are $c_{i}=\\frac{1}{n}$ for all $i$. Then\n$$\n\\sum_{i=1}^{n}c_{i}=\\sum_{i=1}^{n}\\frac{1}{n}=1,\n$$\nso $\\hat{\\mu}_{1}$ is linear and unbiased.\n\n2) For $\\hat{\\mu}_{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}Y_{i}$, the coefficients are $c_{i}=\\frac{1}{n-1}$ for all $i$. Then\n$$\n\\sum_{i=1}^{n}c_{i}=\\sum_{i=1}^{n}\\frac{1}{n-1}=\\frac{n}{n-1}\\neq 1 \\quad \\text{for } n>1,\n$$\nso $\\hat{\\mu}_{2}$ is linear but not unbiased.\n\n3) For $\\hat{\\mu}_{3}=\\frac{Y_{1}+Y_{n}}{2}$, the coefficients are $c_{1}=\\frac{1}{2}$, $c_{n}=\\frac{1}{2}$, and $c_{i}=0$ for $i\\notin\\{1,n\\}$. Then\n$$\n\\sum_{i=1}^{n}c_{i}=\\frac{1}{2}+\\frac{1}{2}=1,\n$$\nso $\\hat{\\mu}_{3}$ is linear and unbiased.\n\nTherefore, among the three, exactly $\\hat{\\mu}_{1}$ and $\\hat{\\mu}_{3}$ are both linear and unbiased.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "Once we have a set of linear and unbiased estimators, how do we choose among them? The Gauss-Markov theorem's promise of the \"best\" estimator refers to efficiencyâ€”the estimator with the minimum variance. This practice  moves from identifying estimators to actively constructing the optimal one, challenging you to find the specific weights for your data that produce the most precise estimate possible. This exercise provides a direct, hands-on intuition for the \"best\" in BLUE.",
            "id": "1919555",
            "problem": "An experimentalist makes two independent measurements, $y_1$ and $y_2$, of a physical quantity whose true, unknown value is $\\mu$. Each measurement is subject to a random error. The statistical model for these measurements is given by $y_i = \\mu + \\epsilon_i$ for $i=1, 2$.\n\nThe random errors $\\epsilon_1$ and $\\epsilon_2$ are assumed to satisfy the following standard properties:\n1.  The expected value of each error is zero, i.e., $E[\\epsilon_i] = 0$.\n2.  The errors have a common, finite variance, denoted by $\\sigma^2 > 0$.\n3.  The errors are uncorrelated, meaning $\\text{Cov}(\\epsilon_1, \\epsilon_2) = 0$.\n\nTo combine the two measurements into a single, improved estimate for $\\mu$, a linear estimator of the form $\\tilde{\\mu} = w y_1 + (1-w) y_2$ is proposed, where $w$ is a real-valued weight. Your task is to find the specific numerical value of $w$ that minimizes the variance of this estimator, $\\text{Var}(\\tilde{\\mu})$.",
            "solution": "The goal is to find the value of the weight $w$ that minimizes the variance of the estimator $\\tilde{\\mu} = w y_1 + (1-w) y_2$. The variance of the estimator is denoted by $\\text{Var}(\\tilde{\\mu})$.\n\nFirst, we express the variance of $\\tilde{\\mu}$ using the properties of variance. For a linear combination of random variables, the variance is given by:\n$$\n\\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y)\n$$\nIn our case, the estimator is $\\tilde{\\mu} = w y_1 + (1-w) y_2$. The variables are $y_1$ and $y_2$, with coefficients $a=w$ and $b=1-w$.\n$$\n\\text{Var}(\\tilde{\\mu}) = \\text{Var}(w y_1 + (1-w) y_2) = w^2 \\text{Var}(y_1) + (1-w)^2 \\text{Var}(y_2) + 2w(1-w) \\text{Cov}(y_1, y_2)\n$$\nNext, we need to find the variance of each measurement $y_i$ and the covariance between them.\nThe variance of $y_i$ is:\n$$\n\\text{Var}(y_i) = \\text{Var}(\\mu + \\epsilon_i)\n$$\nSince $\\mu$ is a constant, its variance is zero. Thus, the variance of $y_i$ is simply the variance of the error term $\\epsilon_i$.\n$$\n\\text{Var}(y_i) = \\text{Var}(\\epsilon_i) = \\sigma^2\n$$\nThis holds for both $i=1$ and $i=2$.\n\nThe covariance between $y_1$ and $y_2$ is:\n$$\n\\text{Cov}(y_1, y_2) = \\text{Cov}(\\mu + \\epsilon_1, \\mu + \\epsilon_2)\n$$\nSince $\\mu$ is a constant, it does not affect the covariance.\n$$\n\\text{Cov}(y_1, y_2) = \\text{Cov}(\\epsilon_1, \\epsilon_2)\n$$\nThe problem states that the errors are uncorrelated, which means $\\text{Cov}(\\epsilon_1, \\epsilon_2) = 0$. Therefore, $\\text{Cov}(y_1, y_2) = 0$.\n\nNow, we substitute these back into the expression for $\\text{Var}(\\tilde{\\mu})$:\n$$\n\\text{Var}(\\tilde{\\mu}) = w^2 (\\sigma^2) + (1-w)^2 (\\sigma^2) + 2w(1-w)(0)\n$$\n$$\n\\text{Var}(\\tilde{\\mu}) = w^2 \\sigma^2 + (1-w)^2 \\sigma^2\n$$\nWe can factor out the constant variance $\\sigma^2$:\n$$\n\\text{Var}(\\tilde{\\mu}) = \\sigma^2 [w^2 + (1-w)^2]\n$$\nTo find the value of $w$ that minimizes this variance, we can define a function $f(w) = w^2 + (1-w)^2$ and find its minimum. Since $\\sigma^2 > 0$, minimizing $f(w)$ is equivalent to minimizing $\\text{Var}(\\tilde{\\mu})$.\n\nLet's expand the function $f(w)$:\n$$\nf(w) = w^2 + (1 - 2w + w^2) = 2w^2 - 2w + 1\n$$\nThis is a quadratic function of $w$, representing a parabola opening upwards. The minimum can be found using calculus by taking the first derivative with respect to $w$ and setting it to zero.\n$$\n\\frac{df}{dw} = \\frac{d}{dw}(2w^2 - 2w + 1) = 4w - 2\n$$\nSet the derivative to zero to find the critical point:\n$$\n4w - 2 = 0\n$$\n$$\n4w = 2\n$$\n$$\nw = \\frac{2}{4} = \\frac{1}{2}\n$$\nTo confirm this is a minimum, we can use the second derivative test. The second derivative is:\n$$\n\\frac{d^2f}{dw^2} = \\frac{d}{dw}(4w - 2) = 4\n$$\nSince the second derivative is positive ($4 > 0$), the critical point $w = 1/2$ corresponds to a local minimum. Because $f(w)$ is a parabola, this is a global minimum.\n\nThus, the variance of the estimator $\\tilde{\\mu}$ is minimized when $w = 1/2$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "The final step is to connect our theoretical understanding of a \"Best Linear Unbiased Estimator\" to a practical, widely-used statistical method. This exercise  demonstrates the power of the Ordinary Least Squares (OLS) framework by asking you to derive the OLS estimator for a simple model. You will discover that the result of this mechanical procedure is the very same minimum-variance estimator, illustrating the central conclusion of the Gauss-Markov theorem in a concrete way.",
            "id": "1919592",
            "problem": "An engineer is calibrating a new type of high-precision gravimeter. In a controlled environment with presumably constant gravitational acceleration, the device's output is expected to fluctuate around a true, stable value due to thermal and electronic noise. To characterize this baseline, a series of $n$ measurements, denoted by $y_1, y_2, \\dots, y_n$, are collected. The engineer models these measurements using a simple statistical relationship:\n$$\ny_i = \\beta_0 + \\epsilon_i\n$$\nwhere $\\beta_0$ represents the true, unknown baseline measurement, and $\\epsilon_i$ represents the random error for the $i$-th observation. To find the best estimate for $\\beta_0$, the engineer employs the method of Ordinary Least Squares (OLS). The OLS procedure determines the value of a parameter that minimizes the sum of the squared differences between the observed values ($y_i$) and the values predicted by the model.\n\nDetermine the OLS estimator for the baseline parameter $\\beta_0$, denoted as $\\hat{\\beta}_0$. Express your answer as a symbolic expression in terms of the measurements $y_i$.",
            "solution": "We are given the intercept-only model $y_{i}=\\beta_{0}+\\epsilon_{i}$ for $i=1,\\dots,n$. The Ordinary Least Squares estimator minimizes the sum of squared residuals with respect to $\\beta_{0}$. Define the objective function\n$$\nS(\\beta_{0})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}\\right)^{2}.\n$$\nTo find the minimizing value, take the derivative with respect to $\\beta_{0}$ and set it equal to zero (first-order condition):\n$$\n\\frac{dS(\\beta_{0})}{d\\beta_{0}}=\\sum_{i=1}^{n}2\\left(y_{i}-\\beta_{0}\\right)\\left(-1\\right)=-2\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}\\right)=0.\n$$\nThis yields the normal equation\n$$\n\\sum_{i=1}^{n}y_{i}-n\\beta_{0}=0 \\quad \\Longrightarrow \\quad \\hat{\\beta}_{0}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}.\n$$\nTo verify it is a minimum, compute the second derivative:\n$$\n\\frac{d^{2}S(\\beta_{0})}{d\\beta_{0}^{2}}=2n,\n$$\nwhich is strictly positive for $n>0$, confirming a global minimum. Therefore, the OLS estimator is the sample mean of the observations.",
            "answer": "$$\\boxed{\\frac{1}{n}\\sum_{i=1}^{n}y_{i}}$$"
        }
    ]
}