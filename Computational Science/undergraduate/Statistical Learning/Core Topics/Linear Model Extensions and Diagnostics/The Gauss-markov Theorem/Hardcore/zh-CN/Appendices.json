{
    "hands_on_practices": [
        {
            "introduction": "高斯-马尔可夫定理关注的是“最佳线性无偏估计量”（BLUE）。在找到“最佳”估计量之前，我们必须首先理解其基本构成部分：“线性”和“无偏”。本练习将通过评估几个不同的估计量，帮助您建立对这两个核心概念的直观理解，并为我们接下来寻找方-差最小的估计量奠定基础。通过区分这些估计量 ，您将掌握识别线性无偏估计量 (LUE) 的关键技能。",
            "id": "1919547",
            "problem": "在统计建模中，估计量是根据观测数据计算给定数量的估计值的一种规则。考虑一个简单模型，其中对一个恒定但未知的物理量 $\\mu$ 进行一系列 $n$ 次测量，即 $Y_1, Y_2, \\dots, Y_n$。该模型由下式给出：\n$$Y_i = \\mu + \\epsilon_i, \\quad \\text{for } i=1, \\dots, n$$\n项 $\\epsilon_i$ 代表随机测量误差。假设这些误差是不相关的，其期望值为零 ($E[\\epsilon_i]=0$)，并且对于所有 $i$ 都有一个恒定的方差 $\\text{Var}(\\epsilon_i)=\\sigma^2$。\n\n如果参数 $\\mu$ 的一个估计量 $\\hat{\\mu}$ 可以写成观测值的线性组合：$\\hat{\\mu} = \\sum_{i=1}^n c_i Y_i$，其中系数 $c_i$ 是常数，则该估计量被定义为*线性*的。\n如果一个估计量 $\\hat{\\mu}$ 的期望值等于它所估计的真实参数，即 $E[\\hat{\\mu}]=\\mu$，则该估计量被定义为*无偏*的。\n\n提出了三种不同的估计量来根据观测数据集估计 $\\mu$。假设 $n > 1$。\n1.  $\\hat{\\mu}_1 = \\frac{1}{n} \\sum_{i=1}^n Y_i$\n\n2.  $\\hat{\\mu}_2 = \\frac{1}{n-1} \\sum_{i=1}^n Y_i$\n\n3.  $\\hat{\\mu}_3 = \\frac{Y_1 + Y_n}{2}$\n\n这三种估计量中，哪些既是线性的又是无偏的？\n\nA. 仅 $\\hat{\\mu}_1$\nB. 仅 $\\hat{\\mu}_3$\nC. $\\hat{\\mu}_1$ 和 $\\hat{\\mu}_2$\nD. $\\hat{\\mu}_1$ 和 $\\hat{\\mu}_3$\nE. 三种估计量都是线性的且无偏的。",
            "solution": "给定模型 $Y_{i}=\\mu+\\epsilon_{i}$，其中 $E[\\epsilon_{i}]=0$ 且 $\\text{Var}(\\epsilon_{i})=\\sigma^{2}$，并且 $\\epsilon_{i}$ 是不相关的。因此，对于每个 $i$，\n$$\nE[Y_{i}]=E[\\mu+\\epsilon_{i}]=\\mu+E[\\epsilon_{i}]=\\mu.\n$$\n线性估计量的形式为 $\\hat{\\mu}=\\sum_{i=1}^{n}c_{i}Y_{i}$。其期望值为\n$$\nE[\\hat{\\mu}]=E\\Bigg[\\sum_{i=1}^{n}c_{i}Y_{i}\\Bigg]=\\sum_{i=1}^{n}c_{i}E[Y_{i}]=\\sum_{i=1}^{n}c_{i}\\mu=\\mu\\sum_{i=1}^{n}c_{i}.\n$$\n因此，无偏性的充要条件是\n$$\n\\sum_{i=1}^{n}c_{i}=1.\n$$\n\n现在逐一检查所提出的每个估计量。\n\n1) 对于 $\\hat{\\mu}_{1}=\\frac{1}{n}\\sum_{i=1}^{n}Y_{i}$，系数对于所有 $i$ 都是 $c_{i}=\\frac{1}{n}$。那么\n$$\n\\sum_{i=1}^{n}c_{i}=\\sum_{i=1}^{n}\\frac{1}{n}=1,\n$$\n所以 $\\hat{\\mu}_{1}$ 是线性的且无偏的。\n\n2) 对于 $\\hat{\\mu}_{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}Y_{i}$，系数对于所有 $i$ 都是 $c_{i}=\\frac{1}{n-1}$。那么\n$$\n\\sum_{i=1}^{n}c_{i}=\\sum_{i=1}^{n}\\frac{1}{n-1}=\\frac{n}{n-1}\\neq 1 \\quad \\text{for } n>1,\n$$\n所以 $\\hat{\\mu}_{2}$ 是线性的但不是无偏的。\n\n3) 对于 $\\hat{\\mu}_{3}=\\frac{Y_{1}+Y_{n}}{2}$，系数为 $c_{1}=\\frac{1}{2}$，$c_{n}=\\frac{1}{2}$，以及对于 $i\\notin\\{1,n\\}$ 有 $c_{i}=0$。那么\n$$\n\\sum_{i=1}^{n}c_{i}=\\frac{1}{2}+\\frac{1}{2}=1,\n$$\n所以 $\\hat{\\mu}_{3}$ 是线性的且无偏的。\n\n因此，在这三个估计量中，恰好 $\\hat{\\mu}_{1}$ 和 $\\hat{\\mu}_{3}$ 既是线性的又是无偏的。",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "在所有线性无偏估计量中，高斯-马尔可夫定理指出存在一个“最佳”的，即方差最小的估计量。这个练习将引导您从识别估计量转向主动寻找最优估计量 。您将通过最小化方差的方法，为一个简单的双观测模型推导出最佳的线性组合权重，从而亲手构建出该情境下的最佳线性无偏估计量 (BLUE)。",
            "id": "1919555",
            "problem": "一位实验者对一个物理量进行了两次独立的测量，$y_1$ 和 $y_2$，该物理量的真实未知值为 $\\mu$。每次测量都存在随机误差。这些测量的统计模型由 $y_i = \\mu + \\epsilon_i$ 给出，其中 $i=1, 2$。\n\n假设随机误差 $\\epsilon_1$ 和 $\\epsilon_2$ 满足以下标准属性：\n1.  每个误差的期望值为零，即 $E[\\epsilon_i] = 0$。\n2.  误差具有一个共同的、有限的方差，记为 $\\sigma^2 > 0$。\n3.  误差是不相关的，即 $\\text{Cov}(\\epsilon_1, \\epsilon_2) = 0$。\n\n为了将两次测量合并为对 $\\mu$ 的单一改进估计，提出了一个形式为 $\\tilde{\\mu} = w y_1 + (1-w) y_2$ 的线性估计量，其中 $w$ 是一个实值权重。您的任务是找到使该估计量的方差 $\\text{Var}(\\tilde{\\mu})$ 最小化的 $w$ 的具体数值。",
            "solution": "目标是找到权重 $w$ 的值，以最小化估计量 $\\tilde{\\mu} = w y_1 + (1-w) y_2$ 的方差。估计量的方差记为 $\\text{Var}(\\tilde{\\mu})$。\n\n首先，我们使用方差的性质来表示 $\\tilde{\\mu}$ 的方差。对于随机变量的线性组合，其方差由以下公式给出：\n$$\n\\text{Var}(aX + bY) = a^2 \\text{Var}(X) + b^2 \\text{Var}(Y) + 2ab \\text{Cov}(X, Y)\n$$\n在我们的例子中，估计量为 $\\tilde{\\mu} = w y_1 + (1-w) y_2$。变量是 $y_1$ 和 $y_2$，系数为 $a=w$ 和 $b=1-w$。\n$$\n\\text{Var}(\\tilde{\\mu}) = \\text{Var}(w y_1 + (1-w) y_2) = w^2 \\text{Var}(y_1) + (1-w)^2 \\text{Var}(y_2) + 2w(1-w) \\text{Cov}(y_1, y_2)\n$$\n接下来，我们需要找到每次测量 $y_i$ 的方差以及它们之间的协方差。\n$y_i$ 的方差是：\n$$\n\\text{Var}(y_i) = \\text{Var}(\\mu + \\epsilon_i)\n$$\n由于 $\\mu$ 是一个常数，其方差为零。因此，$y_i$ 的方差就是误差项 $\\epsilon_i$ 的方差。\n$$\n\\text{Var}(y_i) = \\text{Var}(\\epsilon_i) = \\sigma^2\n$$\n这对 $i=1$ 和 $i=2$ 都成立。\n\n$y_1$ 和 $y_2$ 之间的协方差是：\n$$\n\\text{Cov}(y_1, y_2) = \\text{Cov}(\\mu + \\epsilon_1, \\mu + \\epsilon_2)\n$$\n由于 $\\mu$ 是一个常数，它不影响协方差。\n$$\n\\text{Cov}(y_1, y_2) = \\text{Cov}(\\epsilon_1, \\epsilon_2)\n$$\n题目说明误差是不相关的，这意味着 $\\text{Cov}(\\epsilon_1, \\epsilon_2) = 0$。因此，$\\text{Cov}(y_1, y_2) = 0$。\n\n现在，我们将这些结果代回到 $\\text{Var}(\\tilde{\\mu})$ 的表达式中：\n$$\n\\text{Var}(\\tilde{\\mu}) = w^2 (\\sigma^2) + (1-w)^2 (\\sigma^2) + 2w(1-w)(0)\n$$\n$$\n\\text{Var}(\\tilde{\\mu}) = w^2 \\sigma^2 + (1-w)^2 \\sigma^2\n$$\n我们可以提出常数方差 $\\sigma^2$：\n$$\n\\text{Var}(\\tilde{\\mu}) = \\sigma^2 [w^2 + (1-w)^2]\n$$\n为了找到使该方差最小化的 $w$ 值，我们可以定义一个函数 $f(w) = w^2 + (1-w)^2$ 并求其最小值。由于 $\\sigma^2 > 0$，最小化 $f(w)$ 等价于最小化 $\\text{Var}(\\tilde{\\mu})$。\n\n我们来展开函数 $f(w)$：\n$$\nf(w) = w^2 + (1 - 2w + w^2) = 2w^2 - 2w + 1\n$$\n这是一个关于 $w$ 的二次函数，表示一个开口向上的抛物线。可以通过微积分求其最小值，即求其关于 $w$ 的一阶导数并令其为零。\n$$\n\\frac{df}{dw} = \\frac{d}{dw}(2w^2 - 2w + 1) = 4w - 2\n$$\n将导数设为零以找到临界点：\n$$\n4w - 2 = 0\n$$\n$$\n4w = 2\n$$\n$$\nw = \\frac{2}{4} = \\frac{1}{2}\n$$\n为了确认这是一个最小值，我们可以使用二阶导数检验。二阶导数为：\n$$\n\\frac{d^2f}{dw^2} = \\frac{d}{dw}(4w - 2) = 4\n$$\n由于二阶导数为正 ($4 > 0$)，临界点 $w = 1/2$ 对应一个局部最小值。因为 $f(w)$ 是一个抛物线，所以这也是一个全局最小值。\n\n因此，当 $w = 1/2$ 时，估计量 $\\tilde{\\mu}$ 的方差最小。",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "前面的练习揭示了样本均值在特定情况下的最优性，但这是否只是一个巧合？是否存在一种通用方法可以直接导出 BLUE？本练习将向您介绍一种强大的参数估计方法——普通最小二乘法 (OLS) 。您将通过最小化残差平方和来推导一个常数模型的估计量，并发现其结果恰好就是样本均值，从而深刻理解 OLS 与高斯-马尔可夫定理保证的 BLUE 特性之间的内在联系。",
            "id": "1919592",
            "problem": "一位工程师正在校准一种新型高精度重力仪。在一个假定重力加速度恒定的受控环境中，由于热噪声和电子噪声，该设备的输出预计会围绕一个真实、稳定的值波动。为了表征这个基线，收集了一系列的 $n$ 次测量值，记为 $y_1, y_2, \\dots, y_n$。工程师使用一个简单的统计关系来对这些测量值进行建模：\n$$\ny_i = \\beta_0 + \\epsilon_i\n$$\n其中 $\\beta_0$ 代表真实但未知的基线测量值，而 $\\epsilon_i$ 代表第 $i$ 次观测的随机误差。为了找到 $\\beta_0$ 的最佳估计，工程师采用了普通最小二乘法 (OLS)。OLS 程序确定一个参数值，该值使得观测值 ($y_i$) 与模型预测值之间的平方差之和最小。\n\n确定基线参数 $\\beta_0$ 的 OLS 估计量，记为 $\\hat{\\beta}_0$。将您的答案表示为关于测量值 $y_i$ 的符号表达式。",
            "solution": "给定仅含截距的模型 $y_{i}=\\beta_{0}+\\epsilon_{i}$，其中 $i=1,\\dots,n$。普通最小二乘估计量是使残差平方和相对于 $\\beta_{0}$ 最小化的值。定义目标函数\n$$\nS(\\beta_{0})=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}\\right)^{2}.\n$$\n为了找到最小值，对 $\\beta_{0}$ 求导并令其等于零（一阶条件）：\n$$\n\\frac{dS(\\beta_{0})}{d\\beta_{0}}=\\sum_{i=1}^{n}2\\left(y_{i}-\\beta_{0}\\right)\\left(-1\\right)=-2\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}\\right)=0.\n$$\n这得出了正规方程\n$$\n\\sum_{i=1}^{n}y_{i}-n\\beta_{0}=0 \\quad \\Longrightarrow \\quad \\hat{\\beta}_{0}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}.\n$$\n为了验证它是一个最小值，计算二阶导数：\n$$\n\\frac{d^{2}S(\\beta_{0})}{d\\beta_{0}^{2}}=2n,\n$$\n对于 $n>0$，该值严格为正，从而确认这是一个全局最小值。因此，OLS 估计量是观测值的样本均值。",
            "answer": "$$\\boxed{\\frac{1}{n}\\sum_{i=1}^{n}y_{i}}$$"
        }
    ]
}