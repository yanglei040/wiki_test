## 引言
[主成分分析](@entry_id:145395)（PCA）是[统计学习](@entry_id:269475)和数据科学领域中一种基石性的[降维](@entry_id:142982)与数据探索技术。它通过将[高维数据](@entry_id:138874)投影到一组新的、不相关的变量（即主成分）上，帮助我们揭示隐藏在复杂数据集背后的内在结构。然而，许多使用者仅仅将其视为一个“黑箱”工具，对于其核心要素——主成分（components）与载荷（loadings）——的真正含义、它们如何计算以及如何被准确解释缺乏深入的理解。这种知识上的差距限制了我们从PCA分析中提取深刻见解的能力。

本文旨在填补这一空白。我们将通过三个循序渐进的章节，带领读者从理论走向实践。在“原理与机制”一章中，我们将深入剖析PCA的数学基础，揭示成分与载荷的推导过程和几何意义。接着，在“应用与跨学科联系”一章中，我们将展示这些概念如何在[基因组学](@entry_id:138123)、[图像处理](@entry_id:276975)和生态学等不同领域中被用来解决实际问题。最后，“动手实践”部分将提供一系列精心设计的练习，帮助读者巩固所学知识。要想真正驾驭PCA的强大功能，我们必须首先从它的第一性原理出发。让我们进入第一章，深入探索定义了这一方法的原理与机制。

## 原理与机制

在上一章中，我们介绍了主成分分析（PCA）作为一种强大的降维和数据探索工具的基本思想。本章将深入探讨其核心的数学原理和工作机制。我们将从其基本目标——寻找最大[方差](@entry_id:200758)方向——出发，推导出其代数解，并探索其几何直观意义。随后，我们将通过一个完整的计算示例来巩固这些概念，并讨论[数据缩放](@entry_id:636242)、结果解释、与[奇异值分解](@entry_id:138057)（SVD）的深刻联系，以及在实际应用中必须注意的一些重要细节和微妙之处。

### 核心目标：寻找最大[方差](@entry_id:200758)方向

[主成分分析](@entry_id:145395)的核心思想是将数据投影到一组新的[正交坐标](@entry_id:166074)轴上，并使得数据在这些新坐标轴上的[方差](@entry_id:200758)依次最大化。想象一个在三维空间中呈椭球状[分布](@entry_id:182848)的数据点云。如果我们想用一根直线来最好地“概括”这个点云，我们应该选择穿过[椭球体](@entry_id:165811)最长轴的那根直线，因为数据点沿着这个方向的离散程度（即[方差](@entry_id:200758)）最大。这第一个方向就是**第一主成分**。接着，在所有与第一主成分正交的方向中，我们寻找[方差](@entry_id:200758)次大的方向，此为**第二主成分**，以此类推。

为了将此直观想法形式化，我们考虑一个已经中心化的数据矩阵 $X \in \mathbb{R}^{n \times p}$，其中 $n$ 是观测数量，$p$ 是特征（变量）数量。中心化意味着每个特征列的均值为零。我们希望找到一个单位[方向向量](@entry_id:169562) $l \in \mathbb{R}^{p}$（$\|l\|^2 = 1$），使得数据投影到该方向上的[方差](@entry_id:200758)最大。

数据矩阵 $X$ 在方向 $l$ 上的投影形成了一个新的向量，称为**分数（scores）**向量 $t = Xl \in \mathbb{R}^n$。该向量的每个元素 $t_i$ 是第 $i$ 个数据点在 $l$ 方向上的坐标。由于数据已经中心化，分数向量 $t$ 的均值也为零。因此，其样本[方差](@entry_id:200758)（为简化，此处使用 $1/n$ 作为除数）为：

$v(l) = \frac{1}{n} \sum_{i=1}^{n} t_i^2 = \frac{1}{n} \|t\|^2 = \frac{1}{n} \|Xl\|^2$

我们可以进一步将上式与样本协方差矩阵 $\hat{\Sigma} = \frac{1}{n} X^{\top} X$ 联系起来：

$v(l) = \frac{1}{n} (Xl)^{\top}(Xl) = \frac{1}{n} l^{\top}X^{\top}Xl = l^{\top} \left(\frac{1}{n} X^{\top}X\right) l = l^{\top}\hat{\Sigma}l$

因此，PCA 的目标就转化为一个约束优化问题：在满足 $l^{\top}l = 1$ 的条件下，最大化 $l^{\top}\hat{\Sigma}l$。利用拉格朗日乘子法，我们可以证明，最优解 $l$ 必须满足以下[特征值方程](@entry_id:192306) ：

$\hat{\Sigma}l = \lambda l$

这个方程表明，能够最大化投影[方差](@entry_id:200758)的方向向量 $l$ 必然是样本协方差矩阵 $\hat{\Sigma}$ 的一个**[特征向量](@entry_id:151813)**。此时，投影[方差](@entry_id:200758) $v(l) = l^{\top}\hat{\Sigma}l = l^{\top}(\lambda l) = \lambda(l^{\top}l) = \lambda$。为了使[方差](@entry_id:200758)最大化，我们自然应选择对应于最大[特征值](@entry_id:154894) $\lambda_1$ 的[特征向量](@entry_id:151813)。

由此，我们得到PCA的核心定义：
- **第 $j$ 个[主成分载荷](@entry_id:636346)（loading）向量 $l_j$** 是样本协方差矩阵 $\hat{\Sigma}$ 的第 $j$ 大[特征值](@entry_id:154894) $\lambda_j$ 所对应的单位[特征向量](@entry_id:151813)。
- **第 $j$ 个主成分分数（score）向量 $t_j$** 是原始数据在第 $j$ 个[载荷向量](@entry_id:635284)上的投影，即 $t_j = Xl_j$。
- 第 $j$ 个主成分解释的[方差](@entry_id:200758)等于其对应的[特征值](@entry_id:154894) $\lambda_j$。

所有[载荷向量](@entry_id:635284) $l_1, l_2, \dots, l_p$ 构成了一个新的[正交基](@entry_id:264024)，它们是数据空间中的**主方向**。同样，分数向量 $t_1, t_2, \dots, t_p$ 是原始数据在新[坐标系](@entry_id:156346)下的表示，并且它们是相互（样本）不相关的。

### 主成分的几何解释

代数上的[特征向量](@entry_id:151813)推导背后，有着清晰的几何意义。主成分方向正是数据点云构成的椭球体的主轴方向。我们可以通过一个构造性的例子来深刻理解这一点 。

考虑一个二维数据集。我们首先在单位圆上均匀选取8个点 $z_k = (\cos(k\pi/4), \sin(k\pi/4))^{\top}$。这些点构成一个完美的圆形。然后，我们通过一个对角矩阵 $D = \operatorname{diag}(3,1)$ 对这些点进行伸缩变换。这个变换将[单位圆](@entry_id:267290)拉伸为一个沿 $x$ 轴[半长轴](@entry_id:164167)为3、沿 $y$ 轴半短轴为1的椭圆。此时，数据变化最大的方向显然是水平方向，由向量 $\begin{pmatrix} 1  0 \end{pmatrix}^{\top}$ 表示。

最后，我们使用一个[旋转矩阵](@entry_id:140302) $R$ (例如，旋转 $\pi/6$ 弧度) 来旋转这个椭圆，得到最终的数据点 $x_k = R D z_k$。这个最终的数据集构成了一个倾斜的椭圆。直观上，这个新椭圆的长轴方向就是数据的最大[方差](@entry_id:200758)方向，即第一主成分的方向。这个方向应该是原始长轴方向 $\begin{pmatrix} 1  0 \end{pmatrix}^{\top}$ 经过相同的旋转 $R$ 得到的。

现在，我们从代数上验证这个直觉。我们构建数据矩阵 $X$，计算其样本协方差矩阵 $\hat{\Sigma} = \frac{1}{n} X^{\top} X$。经过推导，可以证明 $\hat{\Sigma} = \frac{1}{2} R D^2 R^{\top}$。根据上一节的结论，第一[主成分载荷](@entry_id:636346) $l_1$ 是 $\hat{\Sigma}$ 的最大[特征值](@entry_id:154894)对应的[特征向量](@entry_id:151813)。

$\hat{\Sigma}$ 的特征值问题 $\frac{1}{2} R D^2 R^{\top} u = \lambda u$ 可以通过左乘 $R^{\top}$ 转化为 $\frac{1}{2} D^2 (R^{\top} u) = \lambda (R^{\top} u)$。这表明 $R^{\top}u$ 是对角矩阵 $\frac{1}{2}D^2 = \operatorname{diag}(9/2, 1/2)$ 的[特征向量](@entry_id:151813)。$\frac{1}{2}D^2$ 的最大[特征值](@entry_id:154894)是 $9/2$，其对应的[特征向量](@entry_id:151813)是 $\begin{pmatrix} 1  0 \end{pmatrix}^{\top}$。因此，我们有 $R^{\top}l_1 = \begin{pmatrix} 1  0 \end{pmatrix}^{\top}$，这意味着 $l_1 = R \begin{pmatrix} 1  0 \end{pmatrix}^{\top}$。

这个代数结果完美地印证了我们的几何直觉：[协方差矩阵](@entry_id:139155)的最大[特征向量](@entry_id:151813)，确实是数据椭圆长轴的方向，它由初始长轴方向经过数据[旋转变换](@entry_id:200017)得到。这个例子清晰地揭示了PCA的本质：通过对协方差矩阵进行[特征分解](@entry_id:181333)，来找到数据[分布](@entry_id:182848)的几何[主轴](@entry_id:172691)。

### 一个完整的计算示例

为了将上述原理具体化，我们来手动完成一个PCA的全过程 。考虑一个中心化的 $n=4, p=3$ 的数据矩阵 $X$：
$$
X = \begin{pmatrix}
\sqrt{3}  \sqrt{3}  \tfrac{1}{\sqrt{2}}\\
-\sqrt{3}  0  \tfrac{1}{\sqrt{2}}\\
0  -\sqrt{3}  \tfrac{1}{\sqrt{2}}\\
0  0  -\tfrac{3}{\sqrt{2}}
\end{pmatrix}
$$

**第一步：计算样本协方差矩阵**
我们使用 $S = \frac{1}{n-1} X^{\top} X$ 作为样本协方差矩阵。
$X^{\top}X = \begin{pmatrix} 6  3  0 \\ 3  6  0 \\ 0  0  6 \end{pmatrix}$
$S = \frac{1}{3} X^{\top} X = \begin{pmatrix} 2  1  0 \\ 1  2  0 \\ 0  0  2 \end{pmatrix}$

**第二步：计算[特征值](@entry_id:154894)和[特征向量](@entry_id:151813)（载荷）**
我们求解 $S$ 的[特征方程](@entry_id:265849) $\det(S - \lambda I) = 0$，得到[特征值](@entry_id:154894)为 $\lambda_1 = 3, \lambda_2 = 2, \lambda_3 = 1$。这些值代表了三个主成分各自解释的[方差](@entry_id:200758)。
接下来，我们求解每个[特征值](@entry_id:154894)对应的单位[特征向量](@entry_id:151813)，即[载荷向量](@entry_id:635284)：
- 对于 $\lambda_1 = 3$, 解 $(S-3I)v = 0$ 得到 $l_1 = \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \\ 0 \end{pmatrix}$。
- 对于 $\lambda_2 = 2$, 解 $(S-2I)v = 0$ 得到 $l_2 = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}$。
- 对于 $\lambda_3 = 1$, 解 $(S-1I)v = 0$ 得到 $l_3 = \begin{pmatrix} 1/\sqrt{2} \\ -1/\sqrt{2} \\ 0 \end{pmatrix}$。

将这些[载荷向量](@entry_id:635284)按列组合，得到载荷矩阵 $L = \begin{pmatrix} l_1  l_2  l_3 \end{pmatrix}$。

**第三步：计算主成分分数**
分数矩阵 $T$ 通过将原始数据投影到主成分方向上得到：$T = XL$。
$$
T = \begin{pmatrix}
\sqrt{3}  \sqrt{3}  \tfrac{1}{\sqrt{2}}\\
-\sqrt{3}  0  \tfrac{1}{\sqrt{2}}\\
0  -\sqrt{3}  \tfrac{1}{\sqrt{2}}\\
0  0  -\tfrac{3}{\sqrt{2}}
\end{pmatrix}
\begin{pmatrix} \frac{1}{\sqrt{2}}  0  \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}  0  -\frac{1}{\sqrt{2}} \\ 0  1  0 \end{pmatrix} = \begin{pmatrix}
\sqrt{6}  \frac{1}{\sqrt{2}}  0 \\
-\sqrt{\frac{3}{2}}  \frac{1}{\sqrt{2}}  -\sqrt{\frac{3}{2}} \\
-\sqrt{\frac{3}{2}}  \frac{1}{\sqrt{2}}  \sqrt{\frac{3}{2}} \\
0  -\frac{3}{\sqrt{2}}  0
\end{pmatrix}
$$
矩阵 $T$ 的每一列就是对应主成分的分数。例如，第一个观测值在第一主成分上的分数是 $\sqrt{6}$。

**第四步：验证性质**
我们可以验证PCA的重要性质。首先，载荷矩阵 $L$ 是正交的，$L^{\top}L = I$。其次，分数是（样本）不相关的。我们可以计算分数的样本[协方差矩阵](@entry_id:139155) $\operatorname{Cov}(T) = \frac{1}{n-1}T^{\top}T$：
$$
\operatorname{Cov}(T) = \frac{1}{3} \begin{pmatrix} 9  0  0 \\ 0  6  0 \\ 0  0  3 \end{pmatrix} = \begin{pmatrix} 3  0  0 \\ 0  2  0 \\ 0  0  1 \end{pmatrix}
$$
结果是一个对角矩阵，对角线上的元素恰好是 $S$ 的[特征值](@entry_id:154894) $\lambda_1, \lambda_2, \lambda_3$。这表明PCA成功地将原始的、可能相关的变量转换成了一组不相关的、[方差](@entry_id:200758)依次递减的新变量（主成分）。第一个主成分的[方差](@entry_id:200758)为3。

### [数据缩放](@entry_id:636242)的角色：协[方差](@entry_id:200758)PCA vs. [相关系数](@entry_id:147037)PCA

在前面的例子中，我们默认对协方差矩阵进行PCA。然而，当原始变量的单位或量纲差异巨大时，这种方法会带来严重问题。考虑一个简单的例子：一个变量是身高（单位：米），另一个是体重（单位：克）。体重的数值会比身高大得多，其[方差](@entry_id:200758)也会不成比例地巨大。

让我们通过一个理想化的数据集来精确地审视这个问题 。假设我们有两个变量 $x$（单位：米）和 $y$（单位：克），它们之间存在完美的线性关系 $y = 100x$。

- **基于协[方差](@entry_id:200758)的PCA**：在这种情况下，变量 $y$ 的样本[方差](@entry_id:200758)将是变量 $x$ 样本[方差](@entry_id:200758)的 $100^2 = 10000$ 倍。由于PCA旨在最大化[方差](@entry_id:200758)，第一主成分将几乎完全被 $y$ 变量所主导。计算表明，第一主成分的[载荷向量](@entry_id:635284)将与 $\begin{pmatrix} 1  100 \end{pmatrix}^{\top}$ 成比例。这意味着，在这个主成分中，$y$ 的权重是 $x$ 的100倍。这个结果是任意的，如果我们把 $y$ 的单位从克换成千克，结果就会截然不同。

- **基于相关系数的PCA**：为了解决这个问题，我们可以在PCA之前对数据进行**标准化（standardization）**，即对每个变量减去其均值，然后除以其[标准差](@entry_id:153618)。这样处理后，每个变量的均值为0，[方差](@entry_id:200758)为1。对标准化后的数据执行PCA，等价于对[原始变量](@entry_id:753733)的**[相关系数](@entry_id:147037)矩阵**进行[特征分解](@entry_id:181333)。在我们的例子中，由于 $x$ 和 $y$ 完全线性相关，它们的[相关系数](@entry_id:147037)为1。[相关系数](@entry_id:147037)矩阵为 $R = \begin{pmatrix} 1  1 \\ 1  1 \end{pmatrix}$。对 $R$ 进行[特征分解](@entry_id:181333)，得到的第一[主成分载荷](@entry_id:636346)向量与 $\begin{pmatrix} 1  1 \end{pmatrix}^{\top}$ 成比例。这表明，在去除了[尺度效应](@entry_id:153734)后，两个变量对第一主成分的贡献是均等的，这更真实地反映了它们之间的内在关系。

**基本准则**：除非所有变量都具有相同的单位且其原始[方差](@entry_id:200758)的大小具有重要的物理解释，否则应**始终优先选择基于[相关系数](@entry_id:147037)的PCA**。这能确保分析结果不受变量任意缩放的影响，从而揭示变量之间更本质的结构。

### 解释载荷与分数

计算出主成分后，理解它们的含义至关重要。特别是当进行[相关系数](@entry_id:147037)PCA时，载荷和分数具有非常直观的解释 。

当变量已经标准化（[方差](@entry_id:200758)为1）后：

- **载荷的含义**：第 $j$ 个变量在第 $\ell$ 个主成分上的载荷 $l_{j\ell}$，等于该变量与第 $\ell$ 个主成分分数向量之间的**相关系数**。即 $l_{j\ell} = \operatorname{corr}(x_j, t_\ell)$。因此，载荷的[绝对值](@entry_id:147688)大小反映了原始变量与主成分之间的关联强度。

- **载荷平方的含义**：载荷的平方 $l_{j\ell}^2$ 表示第 $\ell$ 个主成分能够解释第 $j$ 个变量[方差](@entry_id:200758)的**比例**。例如，如果 $l_{j\ell} = 0.8$，则 $l_{j\ell}^2 = 0.64$，意味着第 $j$ 个变量[方差](@entry_id:200758)的 $64\%$ 可由第 $\ell$ 个主成分来解释。

- **两个重要的恒等式**：
    1.  对于任何一个变量 $j$，其在所有主成分上的载荷平方和为1：$\sum_{\ell=1}^p l_{j\ell}^2 = 1$。这说明所有主成分共同解释了一个变量的全部[方差](@entry_id:200758)。
    2.  对于任何一个主成分 $\ell$，其在所有变量上的载荷平方和等于该主成分的[方差](@entry_id:200758)（即对应的[特征值](@entry_id:154894) $\lambda_\ell$）：$\sum_{j=1}^p l_{j\ell}^2 = \lambda_\ell$。这表明一个主成分的[方差](@entry_id:200758)是由所有[原始变量](@entry_id:753733)贡献而来的。每个变量的贡献大小可以用 $\frac{l_{j\ell}^2}{\lambda_\ell}$ 来衡量，这些贡献的总和为1。

- **几何解释**：载荷 $l_{j\ell}$ 也等于在 $n$ 维观测空间中，[原始变量](@entry_id:753733)向量 $x_j$ 与分数向量 $t_\ell$ 之间夹角的**余弦值**。这为载荷提供了直观的几何图像。

### PCA、SVD 与低秩近似

虽然PCA可以通过对[协方差矩阵](@entry_id:139155)进行[特征分解](@entry_id:181333)来定义，但一个更深刻、更数值稳定的视角是通过**奇异值分解（Singular Value Decomposition, SVD）**来理解。

对于一个中心化的数据矩阵 $X \in \mathbb{R}^{n \times p}$，其SVD可以写作 $X = U \Sigma V^{\top}$，其中：
- $V \in \mathbb{R}^{p \times p}$ 是一个[正交矩阵](@entry_id:169220)，其列向量 $v_j$ 称为[右奇异向量](@entry_id:754365)。
- $U \in \mathbb{R}^{n \times p}$ (在 $n \ge p$ 的情况下) 是一个列正交矩阵，其列向量 $u_j$ 称为[左奇异向量](@entry_id:751233)。
- $\Sigma \in \mathbb{R}^{p \times p}$ 是一个[对角矩阵](@entry_id:637782)，其对角元素 $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_p \ge 0$ 称为奇异值。

将SVD代入样本[协方差矩阵](@entry_id:139155) $S = \frac{1}{n-1}X^{\top}X$（这里使用 $n-1$ 作为除数，这在很多软件中是标准做法）的定义中，我们得到：
$S = \frac{1}{n-1} (U\Sigma V^{\top})^{\top}(U\Sigma V^{\top}) = \frac{1}{n-1} V \Sigma^{\top} U^{\top} U \Sigma V^{\top} = \frac{1}{n-1} V \Sigma^2 V^{\top}$

这个表达式 $S = V (\frac{1}{n-1}\Sigma^2) V^{\top}$ 正是 $S$ 的谱分解（[特征分解](@entry_id:181333)）。由此我们建立了PCA和SVD之间的直接联系 ：

1.  **载荷与[右奇异向量](@entry_id:754365)**：PCA的[载荷向量](@entry_id:635284) $l_j$ 就是SVD的[右奇异向量](@entry_id:754365) $v_j$。即 $L=V$。
2.  **[方差](@entry_id:200758)与[奇异值](@entry_id:152907)**：PCA的第 $j$ 个主成分的[方差](@entry_id:200758)（即[特征值](@entry_id:154894) $\lambda_j$）与第 $j$ 个奇异值 $\sigma_j$ 的关系为 $\lambda_j = \frac{\sigma_j^2}{n-1}$。
3.  **分数与[左奇异向量](@entry_id:751233)**：PCA的分数矩阵 $T = XL = X V = (U\Sigma V^{\top})V = U\Sigma$。这意味着第 $j$ 个分数向量 $t_j = \sigma_j u_j$。

这个联系不仅提供了一种计算PCA的替代方法（SVD在数值上通常更稳定），还揭示了PCA的另一个重要身份：**最优低秩近似**。著名的**[Eckart-Young-Mirsky定理](@entry_id:149772)**指出，由前 $k$ 个主成分构成的对 $X$ 的近似 $X_k = T_k L_k^{\top} = U_k \Sigma_k V_k^{\top}$，是在所有秩为 $k$ 的矩阵中，与原矩阵 $X$ 的[弗罗贝尼乌斯范数](@entry_id:143384)（Frobenius norm）误差 $\lVert X - X_k \rVert_F$ 最小的矩阵。换言之，PCA不仅找到了最大化[方差](@entry_id:200758)的方向，还提供了对原始数据集的最佳 $k$ 维线性压缩表示 。当我们使用所有主成分时，重构是精确的：$X = TL^{\top}$ 。

### 实践中的重要考量

**1. 中心化的必要性**
我们在所有推导中都假设数据是中心化的。这是一个至关重要的步骤。如果不进行中心化，我们计算的 $X^{\top}X$ 就不再反映数据围绕其均值的变异，而是反映其围绕原点的离散程度。如果数据的[均值向量](@entry_id:266544) $\hat{\mu}$ 很大，那么 $X^{\top}X$ 的第一主方向将倾向于指向从原点到数据云中心的向量方向，而不是数据内部[方差](@entry_id:200758)最大的方向 。在这种情况下，第一主成分捕获的将是数据的位置信息，而非其内在的变异结构，这通常不是我们进行PCA的目的。因此，在执行PCA前，**中心化是一个不可或缺的预处理步骤**。

**2. 载荷的符号不确定性**
[特征向量](@entry_id:151813)的定义只到方向，其符号（正或负）是不确定的。如果 $l_j$ 是一个[特征向量](@entry_id:151813)，那么 $-l_j$ 也是对应于相同[特征值](@entry_id:154894)的[特征向量](@entry_id:151813)。在PCA中，这意味着[载荷向量](@entry_id:635284)的符号是任意的。然而，这并不会影响PCA的实质结果。如果我们将一个[载荷向量](@entry_id:635284) $l_j$ 的符号翻转为 $-l_j$，那么对应的分数向量也会自动翻转为 $t_j' = X(-l_j) = -t_j$。因此，它们对数据重构的贡献项 $t_j' (l_j')^{\top} = (-t_j)(-l_j)^{\top} = t_j l_j^{\top}$ 保持不变。同样，解释的[方差](@entry_id:200758) $\lambda_j$ 也保持不变。为了保证结果的[可复现性](@entry_id:151299)，许多软件会采用一个固定的符号约定，例如，强制要求每个[载荷向量](@entry_id:635284)中[绝对值](@entry_id:147688)最大的元素的符号为正 。

**3. [高维数据](@entry_id:138874)中的PCA ($p \gg n$)**
在现代数据分析中，我们经常遇到特征数量远大于样本数量（$p \gg n$）的情况，例如基因组学数据。在这种场景下，PCA有一个非常重要但可能违反直觉的特性。一个 $n \times p$ 的中心化数据矩阵 $X$，其行向量（观测值）张成的空间的维度最多为 $n$，且由于中心化约束（所有列向量与全1向量正交），其秩实际上最多为 $n-1$。根据线性代数的基本定理，$\operatorname{rank}(X^{\top}X) = \operatorname{rank}(X)$，因此样本[协方差矩阵](@entry_id:139155) $S$ 的秩也最多为 $n-1$。
这意味着，即使我们有 $p$ 个特征，也最多只能有 $n-1$ 个具有非零[方差](@entry_id:200758)的主成分。其余 $p - (n-1)$ 个或更多的主成分对应的[特征值](@entry_id:154894)都将为零。这些零[方差](@entry_id:200758)主成分的[载荷向量](@entry_id:635284)位于协方差[矩阵的零空间](@entry_id:152429)中，其选择是任意的，不具有[可解释性](@entry_id:637759)。因此，在 $p \gg n$ 的设定下，有意义的主成分数量的上限是 $n-1$，而不是 $p$ 。