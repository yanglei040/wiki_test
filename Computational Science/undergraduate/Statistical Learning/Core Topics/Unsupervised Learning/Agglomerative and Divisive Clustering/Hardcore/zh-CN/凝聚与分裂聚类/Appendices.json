{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握凝聚式聚类，我们必须深入其内部，理解驱动合并决策的数学原理。Ward 方法因其旨在通过最小化簇内总方差来创建紧凑的球形簇而得到广泛应用。\n\n本练习将引导你推导当两个簇合并时，平方误差和（Within-Cluster Sum of Squares, WSS）的增量公式，这是 Ward 连接方法的核心 。通过完成这一推导，你将从第一性原理的层面深刻理解该方法如何量化一次合并的“成本”。",
            "id": "3097665",
            "problem": "考虑在欧几里得空间中，对数据集 $\\mathcal{X} \\subset \\mathbb{R}^{p}$ 使用 Ward 链方法进行凝聚层次聚类。该方法在每次合并时，选择能使总簇内平方和增量最小的簇对。设 $A$ 和 $B$ 是两个不相交的簇，其大小分别为 $|A|=m$ 和 $|B|=n$，质心（均值）$\\mu_{A}$ 和 $\\mu_{B}$ 的定义为 $\\mu_{C}=\\frac{1}{|C|}\\sum_{x \\in C} x$（对于任意簇 $C$）。簇 $C$ 的簇内平方和为 $W(C)=\\sum_{x \\in C}\\|x-\\mu_{C}\\|^{2}$，总簇内平方和为 $W_{\\mathrm{tot}}=\\sum_{C} W(C)$，其中求和遍历所有当前簇。仅使用这些定义和标准线性代数恒等式，首先，推导合并后簇 $A \\cup B$ 的质心的精确表达式，用 $m$、$n$、$\\mu_{A}$ 和 $\\mu_{B}$ 表示。其次，推导因合并 $A$ 和 $B$ 而导致的总簇内平方和的增量 $\\Delta W = W(A \\cup B) - W(A) - W(B)$ 的精确表达式，仅用 $m$、$n$ 和欧几里得距离 $\\|\\mu_{A}-\\mu_{B}\\|$ 表示。第三，使用您推导出的 $\\Delta W$ 表达式，计算在 $\\mathbb{R}^{2}$ 中，当 $m=7$、$n=3$、$\\mu_{A}=(1,2)$ 和 $\\mu_{B}=(5,8)$ 时，该增量的数值，使用标准欧几里得范数。最后，定性讨论当质心距离 $\\|\\mu_{A}-\\mu_{B}\\|$ 保持不变时，成本增量如何依赖于大小比率 $r=m/n$。最终答案只提供针对指定的 $m$、$n$、$\\mu_{A}$ 和 $\\mu_{B}$ 的增量数值，四舍五入到4位有效数字。",
            "solution": "问题陈述经过严格审查，被认为是有效的。它在科学上基于统计学习的原理，特别是凝聚层次聚类。所提供的关于簇质心、簇内平方和以及 Ward 链方法的定义是标准且正确的。该问题提法明确，为所要求的推导和计算提供了所有必要信息，并且没有歧义、矛盾或事实错误。\n\n按照问题陈述的要求，解答过程分为四个部分。\n\n首先，我们推导合并后簇的质心 $\\mu_{A \\cup B}$ 的表达式。设有两个不相交的簇 $A$ 和 $B$，其大小分别为 $|A|=m$ 和 $|B|=n$，质心分别为 $\\mu_A$ 和 $\\mu_B$。合并后的簇为 $C = A \\cup B$，其大小为 $|C| = |A| + |B| = m+n$。\n\n根据质心的定义，$\\mu_{C} = \\frac{1}{|C|} \\sum_{x \\in C} x$。将其应用于合并簇 $A \\cup B$：\n$$ \\mu_{A \\cup B} = \\frac{1}{|A \\cup B|} \\sum_{x \\in A \\cup B} x $$\n由于 $A$ 和 $B$ 不相交，求和可以拆分：\n$$ \\mu_{A \\cup B} = \\frac{1}{m+n} \\left( \\sum_{x \\in A} x + \\sum_{x \\in B} x \\right) $$\n根据各个簇质心的定义，我们知道 $\\sum_{x \\in A} x = |A|\\mu_A = m\\mu_A$ 且 $\\sum_{x \\in B} x = |B|\\mu_B = n\\mu_B$。将这些代入表达式，得到合并簇的质心：\n$$ \\mu_{A \\cup B} = \\frac{m\\mu_A + n\\mu_B}{m+n} $$\n\n第二，我们推导总簇内平方和的增量 $\\Delta W = W(A \\cup B) - W(A) - W(B)$ 的表达式。合并簇 $A \\cup B$ 的簇内平方和为：\n$$ W(A \\cup B) = \\sum_{x \\in A \\cup B} \\|x - \\mu_{A \\cup B}\\|^2 $$\n同样，我们将求和拆分到不相交的集合 $A$ 和 $B$ 上：\n$$ W(A \\cup B) = \\sum_{x \\in A} \\|x - \\mu_{A \\cup B}\\|^2 + \\sum_{x \\in B} \\|x - \\mu_{A \\cup B}\\|^2 $$\n我们来分析第一项 $\\sum_{x \\in A} \\|x - \\mu_{A \\cup B}\\|^2$。我们可以在范数内加减质心 $\\mu_A$ 来引入它：\n$$ \\sum_{x \\in A} \\|(x - \\mu_A) + (\\mu_A - \\mu_{A \\cup B})\\|^2 $$\n展开平方范数 $\\|u+v\\|^2 = \\|u\\|^2 + 2u \\cdot v + \\|v\\|^2$：\n$$ \\sum_{x \\in A} \\left( \\|x - \\mu_A\\|^2 + 2(x - \\mu_A) \\cdot (\\mu_A - \\mu_{A \\cup B}) + \\|\\mu_A - \\mu_{A \\cup B}\\|^2 \\right) $$\n我们可以分配求和：\n$$ \\sum_{x \\in A} \\|x - \\mu_A\\|^2 + 2 \\left( \\sum_{x \\in A} (x - \\mu_A) \\right) \\cdot (\\mu_A - \\mu_{A \\cup B}) + \\sum_{x \\in A} \\|\\mu_A - \\mu_{A \\cup B}\\|^2 $$\n根据定义，第一项是 $W(A)$。第二项的求和部分是 $\\sum_{x \\in A} (x - \\mu_A) = \\sum_{x \\in A} x - \\sum_{x \\in A} \\mu_A = m\\mu_A - m\\mu_A = 0$。因此，整个第二项为零。第三项是 $m$ 个相同项的和，所以它等于 $m\\|\\mu_A - \\mu_{A \\cup B}\\|^2$。\n因此，对簇 $A$ 的求和简化为：\n$$ \\sum_{x \\in A} \\|x - \\mu_{A \\cup B}\\|^2 = W(A) + m\\|\\mu_A - \\mu_{A \\cup B}\\|^2 $$\n通过对簇 $B$ 的相同论证可得：\n$$ \\sum_{x \\in B} \\|x - \\mu_{A \\cup B}\\|^2 = W(B) + n\\|\\mu_B - \\mu_{A \\cup B}\\|^2 $$\n将这些代回 $W(A \\cup B)$ 的表达式：\n$$ W(A \\cup B) = W(A) + m\\|\\mu_A - \\mu_{A \\cup B}\\|^2 + W(B) + n\\|\\mu_B - \\mu_{A \\cup B}\\|^2 $$\n那么增量 $\\Delta W$ 是：\n$$ \\Delta W = W(A \\cup B) - W(A) - W(B) = m\\|\\mu_A - \\mu_{A \\cup B}\\|^2 + n\\|\\mu_B - \\mu_{A \\cup B}\\|^2 $$\n现在我们必须用 $m$、$n$ 和 $\\|\\mu_A - \\mu_B\\|$ 来表示它。我们使用 $\\mu_{A \\cup B}$ 的表达式：\n$$ \\mu_A - \\mu_{A \\cup B} = \\mu_A - \\frac{m\\mu_A + n\\mu_B}{m+n} = \\frac{(m+n)\\mu_A - (m\\mu_A + n\\mu_B)}{m+n} = \\frac{n(\\mu_A - \\mu_B)}{m+n} $$\n$$ \\mu_B - \\mu_{A \\cup B} = \\mu_B - \\frac{m\\mu_A + n\\mu_B}{m+n} = \\frac{(m+n)\\mu_B - (m\\mu_A + n\\mu_B)}{m+n} = \\frac{m(\\mu_B - \\mu_A)}{m+n} = -\\frac{m(\\mu_A - \\mu_B)}{m+n} $$\n将这些代入 $\\Delta W$ 的表达式中：\n$$ \\Delta W = m\\left\\|\\frac{n(\\mu_A - \\mu_B)}{m+n}\\right\\|^2 + n\\left\\|-\\frac{m(\\mu_A - \\mu_B)}{m+n}\\right\\|^2 $$\n使用性质 $\\|\\lambda v\\|^2 = \\lambda^2\\|v\\|^2$：\n$$ \\Delta W = m\\left(\\frac{n}{m+n}\\right)^2\\|\\mu_A - \\mu_B\\|^2 + n\\left(\\frac{m}{m+n}\\right)^2\\|\\mu_A - \\mu_B\\|^2 $$\n$$ \\Delta W = \\left( \\frac{mn^2}{(m+n)^2} + \\frac{nm^2}{(m+n)^2} \\right) \\|\\mu_A - \\mu_B\\|^2 $$\n$$ \\Delta W = \\frac{mn^2 + nm^2}{(m+n)^2} \\|\\mu_A - \\mu_B\\|^2 = \\frac{mn(n+m)}{(m+n)^2} \\|\\mu_A - \\mu_B\\|^2 $$\n$$ \\Delta W = \\frac{mn}{m+n} \\|\\mu_A - \\mu_B\\|^2 $$\n这就是我们想要的簇内平方和增量的表达式。\n\n第三，我们计算当 $m=7$、$n=3$、$\\mu_A=(1,2)$ 和 $\\mu_B=(5,8)$ 时的数值。\n系数为：\n$$ \\frac{mn}{m+n} = \\frac{7 \\times 3}{7+3} = \\frac{21}{10} = 2.1 $$\n质心之间的欧几里得距离的平方是：\n$$ \\|\\mu_A - \\mu_B\\|^2 = \\|(1-5, 2-8)\\|^2 = \\|(-4, -6)\\|^2 = (-4)^2 + (-6)^2 = 16 + 36 = 52 $$\n平方和的增量是：\n$$ \\Delta W = (2.1) \\times (52) = 109.2 $$\n值 $109.2$ 具有所要求的四位有效数字。\n\n第四，我们定性地讨论当质心距离 $\\|\\mu_A - \\mu_B\\|$ 保持不变时，成本增量 $\\Delta W$ 如何依赖于大小比率 $r=m/n$。设 $D = \\|\\mu_A - \\mu_B\\|$ 为常数。成本为 $\\Delta W = \\frac{mn}{m+n} D^2$。依赖关系由系数 $C(m,n) = \\frac{mn}{m+n}$ 决定。让我们用比率 $r=m/n$ 和其中一个簇（比如 $n$）的点数来表示这个系数。通过代入 $m=rn$，我们得到：\n$$ C(m,n) = \\frac{(rn)n}{rn+n} = \\frac{rn^2}{n(r+1)} = \\frac{rn}{r+1} $$\n这表明成本不仅取决于比率 $r$，还取决于簇的绝对大小。分析该比率影响的更好方法是固定总大小 $N=m+n$。那么 $m = N-n$，且 $C(m,n) = \\frac{(N-n)n}{N}$。这个关于 $n$ 的函数（对于 $1 \\le n  N$）是一个开口向下的抛物线，在 $n=N/2$ 处取得最大值。这对应于 $m=n$，即大小比率为 $r=1$。\n当 $r=1$（大小相等的簇）时，对于固定的距离 $D$ 和总大小 $N$，合并成本最高。当大小比率变得非常大或非常小（即 $r \\to \\infty$ 或 $r \\to 0$）时，系数 $\\frac{mn}{m+n}$ 接近 $\\min(m,n)$。对于固定的总大小 $N$，当一个簇的大小接近 $N$ 而另一个簇的大小接近 $0$ 时，这个值接近 $0$。\n总之，与将一个小簇合并到一个大簇中相比，Ward 方法更重地惩罚合并两个大小相等的平衡簇。这个特性促进了大小大致相等的簇的形成，这是该链方法的一个显著特征。",
            "answer": "$$\\boxed{109.2}$$"
        },
        {
            "introduction": "在层次聚类中，连接方法的选择是最关键的决策之一，因为它从根本上决定了算法能够识别出的簇的形状。例如，单一连接方法以其产生长链状簇的倾向而闻名，这种现象被称为“连锁效应”。\n\n本练习提供了一个动手编码的挑战，让你直接观察并量化这一效应 。通过将单一连接和完全连接方法应用于一个专门设计的数据集，你将亲眼看到这些方法的不同行为，并为何时使用它们建立起直观的认识。",
            "id": "3097643",
            "problem": "您将需要在一个确定性的平面数据集上实现并比较两种连接方式的凝聚聚类。该数据集包含两个由稀疏链条连接的密集斑块。您的目标是通过计数有多少次合并操作将一个密集斑块与该斑块之外的任何部分（包括链条或相对的斑块）连接起来，以此来量化链式效应，并将其作为距离阈值 $h$ 的函数。\n\n数据集定义：\n- 构建一个 $\\mathbb{R}^2$ 中的点集，该点集由三部分组成：\n  1. 左侧斑块 $L$：一个大小为 $3 \\times 3$ 的网格，中心在 $(-4,0)$ 附近，其坐标为 $x \\in \\{-4.1,-4.0,-3.9\\}$ 和 $y \\in \\{-0.1,0.0,0.1\\}$。这会产生 $9$ 个点。\n  2. 右侧斑块 $R$：一个大小为 $3 \\times 3$ 的网格，中心在 $(4,0)$ 附近，其坐标为 $x \\in \\{3.9,4.0,4.1\\}$ 和 $y \\in \\{-0.1,0.0,0.1\\}$。这会产生 $9$ 个点。\n  3. 链条 $C$：沿 $x$ 轴在 $y=0$ 处的共线点，其 $x \\in \\{-2.5,-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0,2.5\\}$。这会产生 $11$ 个点。\n- 总点数为 $29$。\n\n方法的基本原理：\n- 使用点 $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^2$ 之间的欧几里得距离 $d(\\mathbf{x},\\mathbf{y})=\\|\\mathbf{x}-\\mathbf{y}\\|_{2}$。\n- 在凝聚聚类中，从单例簇开始，重复合并具有最小簇间相异度的簇对 $\\mathcal{A},\\mathcal{B}$。对于单连接，簇间相异度为 $\\min\\{d(\\mathbf{x},\\mathbf{y}) : \\mathbf{x}\\in\\mathcal{A}, \\mathbf{y}\\in\\mathcal{B}\\}$。对于全连接，它为 $\\max\\{d(\\mathbf{x},\\mathbf{y}) : \\mathbf{x}\\in\\mathcal{A}, \\mathbf{y}\\in\\mathcal{B}\\}$。\n- 谱系图的合并高度是指合并发生步骤时的簇间相异度。\n\n链式效应计数规则：\n- 将左侧斑块索引集定义为 $L$，右侧斑块索引集定义为 $R$，链条索引集定义为 $C$（即 $L\\cup R$ 的补集）。\n- 在每个合并步骤中，假设两个当前簇 $\\mathcal{A}$ 和 $\\mathcal{B}$ 以高度 $h_{\\text{merge}}$ 进行合并。定义指示符 $I_{L}(\\mathcal{A})=\\mathbf{1}[\\mathcal{A}\\cap L\\neq\\emptyset]$ 和 $I_{R}(\\mathcal{A})=\\mathbf{1}[\\mathcal{A}\\cap R\\neq\\emptyset]$（对 $\\mathcal{B}$ 也类似）。\n- 如果 $(I_{L}(\\mathcal{A}) \\oplus I_{L}(\\mathcal{B})) \\lor (I_{R}(\\mathcal{A}) \\oplus I_{R}(\\mathcal{B}))$ 为真，则该合并被计为“跨链合并”，其中 $\\oplus$ 是异或，$\\lor$ 是逻辑或。直观地说，当且仅当一次合并将斑块的任何部分附加到一个尚未包含该斑块的集合上时，这次合并才被计数。\n\n阈值计数：\n- 对于给定的阈值 $h$，将链式计数 $M_{\\text{linkage}}(h)$ 定义为谱系图中所有合并高度满足 $h_{\\text{merge}} \\le h$ 的合并操作中，按上述规则计数的总次数。\n- 您必须为指定的阈值计算 $M_{\\text{single}}(h)$ 和 $M_{\\text{complete}}(h)$。\n\n测试集：\n- 使用以下阈值 $h$（与数据使用相同的欧几里得单位）：$[0.0,\\,0.1,\\,0.5,\\,1.401,\\,6.61,\\,8.21]$。\n  - 这些值包括一个边界情况 $h=0.0$、斑块内部间距 $h=0.1$、链条间距 $h=0.5$、一个略高于左/右斑块与链条接触尺度的值 $h=1.401$、一个中间尺度 $h=6.61$（该值高于全连接下左斑块到链条的合并高度，但低于左链复合体到右斑块的全连接合并高度），以及一个大的 $h=8.21$（该值高于全连接下左链复合体到右斑块的合并高度）。\n\n所需输出：\n- 您的程序必须执行两次凝聚聚类（单连接和全连接），为测试集中的每个 $h$ 计算 $M_{\\text{single}}(h)$ 和 $M_{\\text{complete}}(h)$，并输出一行包含逗号分隔的列表。该列表按阈值顺序排列，其中每个元素是一个形式为 $[M_{\\text{single}}(h),M_{\\text{complete}}(h)]$ 的数对。\n- 最终输出格式必须是精确的一行：\n  - 精确格式的示例如 $[[a_1,b_1],[a_2,b_2],\\dots,[a_6,b_6]]$，其中每个 $a_i$ 和 $b_i$ 都是整数。\n\n约束与注意事项：\n- 严格按照所述定义，使用欧几里得距离来实现聚类。不要使用任何预构建的层次聚类例程。\n- 不涉及角度；没有单位转换。\n- 所有输出必须是整数。",
            "solution": "用户提供的问题是有效的。这是一个在统计学习领域内定义明确、具有科学依据的任务，具体涉及凝聚聚类算法的特性。所有定义、数据和约束都清晰、一致，并且足以推导出唯一的解决方案。我将继续提供完整的解决方案。\n\n该问题要求在一个特殊构造的平面数据集上，根据第一性原理实现两种连接方法（单连接和全连接）的凝聚层次聚类。目标是通过计数在数据集不同区域之间架起桥梁的合并次数，来量化“链式效应”，并将其表示为距离阈值 $h$ 的函数。\n\n**1. 数据集构建**\n\n首先，我们按规定在 $\\mathbb{R}^2$ 中构建数据集。它包含 $N=29$ 个点，分为三组：\n- 左侧斑块 $L$：一个 $3 \\times 3$ 的网格，包含 $9$ 个点。坐标由 $x \\in \\{-4.1, -4.0, -3.9\\}$ 和 $y \\in \\{-0.1, 0.0, 0.1\\}$ 生成。我们为这些点分配索引 $i \\in \\{0, 1, \\dots, 8\\}$。\n- 右侧斑块 $R$：一个 $3 \\times 3$ 的网格，包含 $9$ 个点。坐标由 $x \\in \\{3.9, 4.0, 4.1\\}$ 和 $y \\in \\{-0.1, 0.0, 0.1\\}$ 生成。我们为这些点分配索引 $i \\in \\{9, 10, \\dots, 17\\}$。\n- 链条 $C$：沿 $x$ 轴（$y=0$）的一组 $11$ 个共线点。坐标为 $x \\in \\{-2.5, -2.0, \\dots, 2.5\\}$，步长为 $0.5$。我们为这些点分配索引 $i \\in \\{18, 19, \\dots, 28\\}$。\n\n任意两点 $\\mathbf{p}_i = (x_i, y_i)$ 和 $\\mathbf{p}_j = (x_j, y_j)$ 之间的距离度量是欧几里得距离 $d(\\mathbf{p}_i, \\mathbf{p}_j) = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$。我们预先计算所有点对之间的距离，并将其存储在一个对称的 $N \\times N$ 矩阵 $\\mathbf{D}$ 中，其中 $\\mathbf{D}_{ij} = d(\\mathbf{p}_i, \\mathbf{p}_j)$。\n\n**2. 凝聚聚类算法**\n\n我们根据其基本定义来实现凝聚聚类算法，而不依赖于预打包的库函数来实现聚类逻辑本身。一般流程如下：\n\n1.  **初始化**：从 $N=29$ 个簇开始，每个簇都是一个包含一个点的单例集，例如 $\\mathcal{C}_i = \\{i\\}$，其中 $i=0, \\dots, 28$。我们维护一个活动簇的列表。簇之间的相异度矩阵用点对距离矩阵 $\\mathbf{D}$ 进行初始化。\n\n2.  **迭代合并**：重复 $N-1=28$ 步，直到只剩下一个簇为止：\n    a.  **寻找最近对**：识别具有最小簇间相异度的不同活动簇对 $\\mathcal{A}$ 和 $\\mathcal{B}$。这个最小相异度值就是合并高度 $h_{\\text{merge}}$。\n    b.  **应用链式计数规则**：在合并之前，我们应用指定的规则来确定这次合并是否构成一次“链式”事件。\n    c.  **合并**：形成一个新簇 $\\mathcal{A} \\cup \\mathcal{B}$。停用旧簇 $\\mathcal{A}$ 和 $\\mathcal{B}$。\n    d.  **更新相异度**：计算新簇与所有其他活动簇之间的相异度。\n\n这个过程对单连接和全连接分别执行。关键区别在于相异度的更新规则（步骤2d），该规则由 Lance-Williams 公式决定。如果簇 $\\mathcal{U}$ 是通过合并 $\\mathcal{A}$ 和 $\\mathcal{B}$ 形成的，它与任何其他簇 $\\mathcal{V}$ 的相异度为：\n\n-   **单连接**：$d_{\\text{single}}(\\mathcal{U}, \\mathcal{V}) = \\min(d(\\mathcal{A}, \\mathcal{V}), d(\\mathcal{B}, \\mathcal{V}))$。这对应于 $\\mathcal{U}$ 中的任意点与 $\\mathcal{V}$ 中的任意点之间的最小距离。\n-   **全连接**：$d_{\\text{complete}}(\\mathcal{U}, \\mathcal{V}) = \\max(d(\\mathcal{A}, \\mathcal{V}), d(\\mathcal{B}, \\mathcal{V}))$。这对应于 $\\mathcal{U}$ 中的任意点与 $\\mathcal{V}$ 中的任意点之间的最大距离。\n\n**3. 链式效应计数规则**\n\n在每个合并步骤中，对于正在合并的两个簇 $\\mathcal{A}$ 和 $\\mathcal{B}$，我们评估一个特定的逻辑条件。我们根据一个簇是否包含来自原始斑块 $L$ 和 $R$ 的点来定义指示函数：\n- $I_{L}(\\mathcal{A}) = \\mathbf{1}[\\mathcal{A} \\cap L \\neq \\emptyset]$，如果簇 $\\mathcal{A}$ 至少包含一个来自斑块 $L$ 的点，则为 $1$，否则为 $0$。\n- $I_{R}(\\mathcal{A}) = \\mathbf{1}[\\mathcal{A} \\cap R \\neq \\emptyset]$，对斑块 $R$ 也类似。\n\n如果条件 $(I_{L}(\\mathcal{A}) \\oplus I_{L}(\\mathcal{B})) \\lor (I_{R}(\\mathcal{A}) \\oplus I_{R}(\\mathcal{B}))$ 为真，则该次合并被计数。这里，$\\oplus$ 表示异或（XOR）运算，$\\lor$ 表示逻辑或运算。该规则有效地标记了任何将包含特定斑块中点的簇与不包含该斑块中点的簇连接起来的合并，从而捕捉到斑块“延伸”到链条或其他斑块的行为。\n\n对于 $28$ 次合并中的每一次，我们记录一个元组 $(h_{\\text{merge}}, \\text{is\\_counted})$，其中 `is_counted` 是来自上述规则的二进制值（$1$ 或 $0$）。\n\n**4. 阈值计数计算**\n\n在完成给定连接类型的聚类过程并获得 $28$ 条合并记录的列表之后，我们为每个指定的阈值 $h$ 计算最终计数。阈值计数 $M_{\\text{linkage}}(h)$ 是所有满足 $h_{\\text{merge}} \\le h$ 的合并操作的 `is_counted` 值的累积和。\n\n对于测试集中的每个阈值 $h \\in \\{0.0, 0.1, 0.5, 1.401, 6.61, 8.21\\}$，我们计算 $M_{\\text{single}}(h)$ 和 $M_{\\text{complete}}(h)$。\n\n- **单连接分析**：此方法对簇之间的最近点很敏感。斑块内部的小距离（$d \\ge 0.1$）和链条上的小距离（$d=0.5$）导致在合并高度略高于 $0.5$ 时形成三个大簇：$\\mathcal{L}$（所有 $L$）、$\\mathcal{R}$（所有 $R$）和 $\\mathcal{C}$（所有 $C$）。接下来的合并操作连接了这些超簇。$\\mathcal{L}$ 和 $\\mathcal{C}$ 之间的最小距离是 $1.4$，$\\mathcal{R}$ 和 $\\mathcal{C}$ 之间的最小距离也是 $1.4$。第一次被计数的合并发生在 $h=1.4$（例如，$\\mathcal{L}$ 与 $\\mathcal{C}$）。随后的合并，即结果簇 $(\\mathcal{L} \\cup \\mathcal{C})$ 与 $\\mathcal{R}$ 的合并，也发生在 $h=1.4$ 并且也被计数。这导致了 $2$ 次被计数的“链式”合并。\n\n- **全连接分析**：此方法对簇之间最远的点（即直径）很敏感。斑块 $\\mathcal{L}$ 和 $\\mathcal{R}$ 会迅速合并，因为它们的直径很小（约 $0.28$）。链条 $\\mathcal{C}$ 的直径很大，为 $5.0$。一个关键的观察是，形成单个簇 $\\mathcal{C}$ 的合并高度是 $5.0$，这小于将一个斑块连接到链条所需的合并高度。完全形成的簇 $\\mathcal{L}$ 和完全形成的簇 $\\mathcal{C}$ 之间的全连接相异度由它们最远的点决定，即 $d((-4.1, \\cdot), (2.5,0)) \\approx 6.60$。因此，第一次被计数的合并发生在这个大得多的高度上。最后的合并，即 $(\\mathcal{L} \\cup \\mathcal{C})$ 与 $\\mathcal{R}$ 之间的合并，发生在一个更高的高度，约为 $8.20$。这种方法能够稳健地将斑块分离开，直到达到高距离阈值。\n\n实现通过明确执行所述算法来生成这些结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Main function to perform agglomerative clustering and calculate chaining counts.\n    \"\"\"\n\n    # 1. Dataset Definition\n    # Left blob L\n    points_l = np.array([[x, y] for x in [-4.1, -4.0, -3.9] for y in [-0.1, 0.0, 0.1]])\n    # Right blob R\n    points_r = np.array([[x, y] for x in [3.9, 4.0, 4.1] for y in [-0.1, 0.0, 0.1]])\n    # Chain C\n    points_c = np.array([[x, 0.0] for x in np.arange(-2.5, 2.51, 0.5)])\n    \n    points = np.vstack([points_l, points_r, points_c])\n    n_points = len(points)\n    \n    # Indices for each group\n    n_l, n_r, n_c = len(points_l), len(points_r), len(points_c)\n    indices_l = set(range(n_l))\n    indices_r = set(range(n_l, n_l + n_r))\n\n    # --- Agglomerative Clustering Implementation ---\n    def run_clustering(points, linkage, indices_l, indices_r):\n        \"\"\"\n        Performs agglomerative clustering from first principles.\n        \n        Args:\n            points (np.ndarray): The N x 2 array of data points.\n            linkage (str): 'single' or 'complete'.\n            indices_l (set): Indices of points in the left blob.\n            indices_r (set): Indices of points in the right blob.\n            \n        Returns:\n            list: A list of tuples (merge_height, is_counted) for each merge.\n        \"\"\"\n        n = len(points)\n        \n        # Initial pairwise distance matrix for points\n        dist_matrix = squareform(pdist(points, 'euclidean'))\n        # Using a large number for infinity to handle self-distances\n        # and distances to merged clusters.\n        inf = np.finfo(dist_matrix.dtype).max\n        np.fill_diagonal(dist_matrix, inf)\n        \n        # Each point starts as its own cluster\n        clusters = [{i} for i in range(n)]\n        \n        # Keep track of which blobs are in each cluster\n        # blob_content[i] = (has_L, has_R)\n        blob_content = []\n        for i in range(n):\n            has_l = i in indices_l\n            has_r = i in indices_r\n            blob_content.append([has_l, has_r])\n            \n        active_clusters = list(range(n))\n        merge_log = []\n\n        for _ in range(n - 1):\n            if len(active_clusters)  2:\n                break\n                \n            min_dist = inf\n            merge_idx1, merge_idx2 = -1, -1\n            \n            # Find the closest pair of active clusters\n            # Iterating through upper triangle of distance matrix for active clusters\n            for i_idx, c1_idx in enumerate(active_clusters):\n                for c2_idx in active_clusters[i_idx + 1:]:\n                    d = dist_matrix[c1_idx, c2_idx]\n                    if d  min_dist:\n                        min_dist = d\n                        merge_idx1, merge_idx2 = c1_idx, c2_idx\n            \n            merge_height = min_dist\n            \n            # Apply the chaining-effect counting rule\n            content1 = blob_content[merge_idx1]\n            content2 = blob_content[merge_idx2]\n            \n            # (I_L(A) XOR I_L(B)) OR (I_R(A) XOR I_R(B))\n            is_counted = (content1[0] ^ content2[0]) or (content1[1] ^ content2[1])\n            merge_log.append((merge_height, 1 if is_counted else 0))\n            \n            # Merge clusters and update dissimilarities (Lance-Williams)\n            # We merge merge_idx2 into merge_idx1\n            \n            # Update blob content for the merged cluster\n            blob_content[merge_idx1][0] = content1[0] or content2[0]\n            blob_content[merge_idx1][1] = content1[1] or content2[1]\n\n            # Update distance matrix row/col for merge_idx1\n            for other_idx in active_clusters:\n                if other_idx != merge_idx1 and other_idx != merge_idx2:\n                    d1 = dist_matrix[merge_idx1, other_idx]\n                    d2 = dist_matrix[merge_idx2, other_idx]\n                    \n                    if linkage == 'single':\n                        new_dist = min(d1, d2)\n                    elif linkage == 'complete':\n                        new_dist = max(d1, d2)\n                    else:\n                        raise ValueError(\"Invalid linkage method\")\n\n                    dist_matrix[merge_idx1, other_idx] = new_dist\n                    dist_matrix[other_idx, merge_idx1] = new_dist\n            \n            # Deactivate merge_idx2\n            dist_matrix[merge_idx2, :] = inf\n            dist_matrix[:, merge_idx2] = inf\n            active_clusters.remove(merge_idx2)\n\n        return merge_log\n\n    # --- Calculate Thresholded Counts ---\n    def calculate_counts(merge_log, thresholds):\n        \"\"\"Calculates cumulative counts for given thresholds.\"\"\"\n        counts = []\n        sorted_merges = sorted(merge_log)\n        \n        for h in thresholds:\n            count = 0\n            for merge_height, is_counted in sorted_merges:\n                if merge_height = h:\n                    count += is_counted\n                else:\n                    break\n            counts.append(count)\n        return counts\n\n    # Test suite of thresholds\n    thresholds = [0.0, 0.1, 0.5, 1.401, 6.61, 8.21]\n\n    # Run for single linkage\n    merges_single = run_clustering(points, 'single', indices_l, indices_r)\n    counts_single = calculate_counts(merges_single, thresholds)\n\n    # Run for complete linkage\n    merges_complete = run_clustering(points, 'complete', indices_l, indices_r)\n    counts_complete = calculate_counts(merges_complete, thresholds)\n\n    # Format the final output\n    results = []\n    for s_count, c_count in zip(counts_single, counts_complete):\n        results.append(f\"[{s_count},{c_count}]\")\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界的数据集很少是完美的，常常包含离群点或噪声。一个稳健的聚类算法应该能产生稳定的结果，不过度受少数异常数据点的影响。\n\n这最后一个练习将探讨稳健性这一实际问题，通过考察不同的连接方法——完全连接、平均连接和 Ward 方法——如何应对一个遥远的离群点的存在 。通过度量由此产生的簇划分的变化，你将对每种方法的稳定性获得宝贵的见解，这有助于你在处理不完美数据时做出更明智的选择。",
            "id": "3097618",
            "problem": "给定一个确定性的二维数据集，要求您比较三种标准的凝聚式层次聚类链接方法在添加一个远离中心的离群点时的响应。这三种链接方法是：完全链接、平均链接和 Ward 链接。在整个过程中使用欧几里得度量。您的任务是，对于每种链接方法和每个指定的离群点位置，计算当附加该离群点时，原始点上 $k=3$ 个簇的切割层级划分发生了多大变化。\n\n基本原理：\n- 凝聚式层次聚类从将每个观测值视为其自身的簇开始，然后根据链接规则和相异性度量迭代地合并簇。这里的度量是欧几里得距离，表示为 $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2$。\n- 完全链接将两个簇 $A$ 和 $B$ 之间的距离定义为 $\\max\\{d(\\mathbf{x}, \\mathbf{y}) : \\mathbf{x} \\in A, \\mathbf{y} \\in B\\}$。\n- 平均链接将两个簇 $A$ 和 $B$ 之间的距离定义为 $\\frac{1}{|A|\\,|B|} \\sum_{\\mathbf{x} \\in A} \\sum_{\\mathbf{y} \\in B} d(\\mathbf{x}, \\mathbf{y})$。\n- Ward 方法在每一步选择能使簇内总平方误差和 (SSE) 增量最小的合并方式，其中一个簇 $C$ 的 SSE（其质心为 $\\boldsymbol{\\mu}_C$）为 $\\sum_{\\mathbf{x} \\in C} \\|\\mathbf{x} - \\boldsymbol{\\mu}_C\\|_2^2$。\n\n数据集：\n- 基础集 $X = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{12}\\} \\subset \\mathbb{R}^2$ 包含 12 个点：\n  - 在 $(0,0)$ 附近的簇：$\\;(-0.2,\\,0.1),\\;(0.0,\\,0.2),\\;(0.2,\\,-0.1),\\;(0.1,\\,0.0)$。\n  - 在 $(4,0)$ 附近的簇：$\\;(3.8,\\,0.1),\\;(4.1,\\,-0.2),\\;(4.2,\\,0.2),\\;(3.9,\\,-0.1)$。\n  - 在 $(2,3)$ 附近的簇：$\\;(2.0,\\,3.1),\\;(2.1,\\,2.9),\\;(1.9,\\,3.2),\\;(2.2,\\,3.0)$。\n\n单个离群点的扰动：\n- 设 $n=12$，$\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_i$ 为基础集的质心。\n- 对于给定的标量 $d \\ge 0$，定义一个离群点 $\\mathbf{z}(d) = \\bar{\\mathbf{x}} + (0,\\,d)$，即放置在质心正上方，垂直偏移量为 $d$。\n- 构建扰动后的数据集 $X(d) = X \\cup \\{\\mathbf{z}(d)\\}$。\n\n切割层级划分与敏感性度量：\n- 对于每种链接方法 $\\ell \\in \\{\\text{complete}, \\text{average}, \\text{Ward}\\}$ 和每个数据集（基础集和扰动集），使用欧几里得距离形成凝聚式树状图，并将其切割以获得恰好 $k=3$ 个扁平簇。\n- 令 $\\pi_\\ell$ 表示在链接方法 $\\ell$ 下，通过在 $k=3$ 处切割基础集的树状图所获得的 $X$ 的划分。\n- 令 $\\pi_\\ell'(d)$ 表示通过在 $k=3$ 处切割所获得的 $X(d)$ 的划分，然后将 $\\pi_\\ell'(d)$ 限制在原始的 $n$ 个索引上，以获得 $X$ 的一个划分。\n- 将敏感性得分 $S_\\ell(d)$ 定义为，在 $1 \\le i  j \\le n$ 范围内的无序点对 $\\{i,j\\}$ 中，其点 $\\mathbf{x}_i$ 和 $\\mathbf{x}_j$ 的共同隶属关系在 $\\pi_\\ell$ 和 $\\pi_\\ell'(d)$ 的限制版本之间不同的对数。等价地，\n  $$S_\\ell(d) = \\sum_{1 \\le i  j \\le n} \\left| \\mathbb{I}\\{\\mathbf{x}_i \\sim_{\\pi_\\ell} \\mathbf{x}_j\\} - \\mathbb{I}\\{\\mathbf{x}_i \\sim_{\\pi_\\ell'(d)} \\mathbf{x}_j\\} \\right|,$$\n  其中 $\\mathbb{I}\\{\\cdot\\}$ 是指示函数，$\\sim_{\\pi}$ 表示“在划分 $\\pi$ 下属于同一个簇”。\n\n测试套件：\n- 评估 $d \\in \\{0,\\,8,\\,20,\\,60\\}$ 这四种情况。\n\n要求输出：\n- 对于 $d=0, d=8, d=20, d=60$ 中的每一个 $d$（按此顺序），计算并报告三元组 $[S_{\\text{complete}}(d),\\, S_{\\text{average}}(d),\\, S_{\\text{Ward}}(d)]$，结果为整数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素本身是一个包含三个整数的列表，对应于固定顺序 $\\big[$complete, average, Ward$\\big]$ 的三种链接方法。例如，包含两种情况的输出应类似于 $[[a,b,c],[d,e,f]]$，不含空格，其中 $a,\\dots,f$ 是整数。\n\n角度单位：不适用。物理单位：不适用。\n\n实现约束：\n- 使用欧几里得距离和指定的链接方法。\n- 在所有情况下，切割树状图以获得恰好 $k=3$ 个簇。\n- 仅使用执行环境允许的 Python 标准库、NumPy 和 SciPy。",
            "solution": "问题陈述已经过验证，被认为是科学上合理的、定义明确且完整的。我们可以继续进行正式的求解。\n\n目标是量化三种凝聚式层次聚类链接方法——完全链接、平均链接和 Ward 方法——对添加单个离群点的敏感性。该敏感性通过在引入一个离群点时，基础数据集上 $k=3$ 簇划分的变化来衡量。\n\n**1. 准备工作：数据集与划分**\n\n首先，我们定义基础数据集 $X = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{12}\\} \\subset \\mathbb{R}^2$，它由 $n=12$ 个点组成，分为三个视觉上截然不同的组：\n- $C_1^{\\text{true}} = \\{(-0.2, 0.1), (0.0, 0.2), (0.2, -0.1), (0.1, 0.0)\\}$\n- $C_2^{\\text{true}} = \\{(3.8, 0.1), (4.1, -0.2), (4.2, 0.2), (3.9, -0.1)\\}$\n- $C_3^{\\text{true}} = \\{(2.0, 3.1), (2.1, 2.9), (1.9, 3.2), (2.2, 3.0)\\}$\n\n对于每种链接方法 $\\ell \\in \\{\\text{complete}, \\text{average}, \\text{Ward}\\}$，我们首先建立一个基准划分 $\\pi_\\ell$。这是通过对基础集 $X$ 使用欧几里得度量进行凝聚式聚类，并切割生成的树状图以产生恰好 $k=3$ 个簇来实现的。鉴于数据的清晰分离，预计对于所有三种链接方法，得到的划分 $\\pi_\\ell$ 都将正确识别出这三个组，即 $\\pi_\\ell = \\{C_1^{\\text{true}}, C_2^{\\text{true}}, C_3^{\\text{true}}\\}$。\n\n**2. 扰动与敏感性度量**\n\n接下来，我们引入一个扰动。基础集的质心计算为 $\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_i$。一个离群点 $\\mathbf{z}(d)$ 被定义为放置在该质心垂直距离为 $d$ 的点：$\\mathbf{z}(d) = \\bar{\\mathbf{x}} + (0, d)$。扰动后的数据集则为 $X(d) = X \\cup \\{\\mathbf{z}(d)\\}$，包含 $n+1 = 13$ 个点。\n\n对于每个链接方法 $\\ell$ 和每个给定的 $d$ 值，我们对扰动后的数据集 $X(d)$ 进行凝聚式聚类，并再次切割树状图以形成 $k=3$ 个簇。这会得到一个 $X(d)$ 的划分，记为 $\\pi_\\ell'(d)$。然后我们考虑由 $\\pi_\\ell'(d)$ 导出的原始点集 $X$ 的划分。这是通过只观察前 $n=12$ 个点的簇分配来获得的。\n\n链接方法的敏感性由得分 $S_\\ell(d)$ 量化，该得分计算了来自原始集 $X$ 的无序点对 $\\{\\mathbf{x}_i, \\mathbf{x}_j\\}$ 中，其共同隶属状态发生变化的对数。在数学上，它被定义为：\n$$\nS_\\ell(d) = \\sum_{1 \\le i  j \\le n} \\left| \\mathbb{I}\\{\\mathbf{x}_i \\sim_{\\pi_\\ell} \\mathbf{x}_j\\} - \\mathbb{I}\\{\\mathbf{x}_i \\sim_{\\pi_\\ell'(d)} \\mathbf{x}_j\\} \\right|\n$$\n其中 $\\mathbb{I}\\{\\cdot\\}$ 是指示函数，$\\mathbf{x}_i \\sim_{\\pi} \\mathbf{x}_j$ 表示点 $\\mathbf{x}_i$ 和 $\\mathbf{x}_j$ 在划分 $\\pi$ 中属于同一个簇。\n\n得分 $S_\\ell(d)=0$ 表示原始 12 个点的划分完全不受离群点的影响。非零得分表示聚类结构发生了变化。例如，如果离群点的存在导致两个原始的 4 点簇合并，那么从“非同簇”变为“同簇”的点对数量为 $4 \\times 4 = 16$。因此，敏感性得分将为 $S_\\ell(d)=16$。\n\n**3. 算法流程**\n\n对每个要求的 $d \\in \\{0, 8, 20, 60\\}$ 值的计算遵循以下步骤：\n1.  **预计算**：对于每种链接方法 $\\ell \\in \\{\\text{complete}, \\text{average}, \\text{Ward}\\}$，计算 $X$ 中 12 个点的基准划分 $\\pi_\\ell$ 并存储相应的簇标签。这通过使用 `scipy.spatial.distance.pdist` 找到成对欧几里得距离，使用 `scipy.cluster.hierarchy.linkage` 构建层次结构，以及使用带 `criterion='maxclust'` 和阈值 3 的 `scipy.cluster.hierarchy.fcluster` 来获得标签。\n2.  **遍历 $d$**：对于每个 $d$ 值：\n    a. 构建扰动后的数据集 $X(d)$。\n    b. 对于每种链接方法 $\\ell$：\n        i.  对 $X(d)$ 进行凝聚式聚类，以获得 $k=3$ 的划分 $\\pi_\\ell'(d)$。\n        ii. 从 $\\pi_\\ell'(d)$ 中提取前 12 个点的簇标签。\n        iii. 比较基准划分和扰动后划分之间所有 $\\binom{12}{2} = 66$ 个点对的共同隶属关系。将不一致的数量相加，以得出得分 $S_\\ell(d)$。\n3.  **结果聚合**：将每个 $d$ 的得分收集成一个三元组 $[S_{\\text{complete}}(d), S_{\\text{average}}(d), S_{\\text{Ward}}(d)]$。\n\n此过程被系统地应用于测试套件，并且生成的整数列表按照问题规范进行格式化。此分析将揭示链接准则的选择如何决定聚类解决方案在存在离群点时的稳健性。众所周知，Ward 方法和平均链接通常比完全链接对离群点更敏感，我们预计这将在计算出的得分中得到反映。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\ndef solve():\n    \"\"\"\n    Computes the sensitivity of hierarchical clustering linkages to an outlier.\n    \"\"\"\n    # Define the base dataset X of n=12 points in 2D.\n    X_base = np.array([\n        # Cluster near (0,0)\n        [-0.2, 0.1], [0.0, 0.2], [0.2, -0.1], [0.1, 0.0],\n        # Cluster near (4,0)\n        [3.8, 0.1], [4.1, -0.2], [4.2, 0.2], [3.9, -0.1],\n        # Cluster near (2,3)\n        [2.0, 3.1], [2.1, 2.9], [1.9, 3.2], [2.2, 3.0]\n    ])\n    n = X_base.shape[0]\n\n    # Calculate the centroid of the base set.\n    x_bar = np.mean(X_base, axis=0)\n\n    # Define the test parameters.\n    test_ds = [0.0, 8.0, 20.0, 60.0]\n    linkage_methods = ['complete', 'average', 'ward']\n    num_clusters = 3\n\n    # Pre-compute the baseline partitions for the original dataset X.\n    # This is constant across all test cases for d.\n    base_partitions = {}\n    dist_base = pdist(X_base, 'euclidean')\n    for method in linkage_methods:\n        Z_base = linkage(dist_base, method=method)\n        labels_base = fcluster(Z_base, num_clusters, criterion='maxclust')\n        base_partitions[method] = labels_base\n\n    all_results = []\n    # Iterate through each outlier distance d.\n    for d in test_ds:\n        # Construct the outlier point and the perturbed dataset X(d).\n        z_d = x_bar + np.array([0, d])\n        X_perturbed = np.vstack([X_base, z_d])\n\n        scores_for_d = []\n        # Iterate through each linkage method.\n        for method in linkage_methods:\n            # Get the pre-computed baseline partition labels.\n            labels_base = base_partitions[method]\n\n            # Perform clustering on the perturbed dataset X(d).\n            dist_perturbed = pdist(X_perturbed, 'euclidean')\n            Z_perturbed = linkage(dist_perturbed, method=method)\n            labels_perturbed_full = fcluster(Z_perturbed, num_clusters, criterion='maxclust')\n            \n            # Restrict the partition to the original n points.\n            labels_perturbed_restricted = labels_perturbed_full[:n]\n            \n            # Calculate the sensitivity score S_l(d).\n            # This is the number of pairs whose co-membership changes.\n            score = 0\n            for i in range(n):\n                for j in range(i + 1, n):\n                    co_membership_base = (labels_base[i] == labels_base[j])\n                    co_membership_perturbed = (labels_perturbed_restricted[i] == labels_perturbed_restricted[j])\n                    if co_membership_base != co_membership_perturbed:\n                        score += 1\n            scores_for_d.append(score)\n        \n        all_results.append(scores_for_d)\n\n    # Format the final output string as specified.\n    # Example: [[c0,a0,w0],[c8,a8,w8],...]\n    formatted_results = ','.join([f\"[{','.join(map(str, r))}]\" for r in all_results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n\n```"
        }
    ]
}