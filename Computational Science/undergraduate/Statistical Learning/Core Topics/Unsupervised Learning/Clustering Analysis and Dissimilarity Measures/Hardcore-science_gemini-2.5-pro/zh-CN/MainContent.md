## 引言
在数据驱动的探索时代，[聚类分析](@entry_id:637205)是揭示数据内在结构、发现隐藏模式的核心技术。然而，任何[聚类算法](@entry_id:146720)的成功都取决于一个看似简单却至关重要的问题：我们如何量化两个数据点之间的“不相似”程度？这个问题的答案——即相异性度量的选择——是整个分析过程的基石，其影响深远，直接塑造了最终的[聚类](@entry_id:266727)结果。错误或草率的选择可能导致对[数据结构](@entry_id:262134)的误解，而明智的选择则能揭示出有意义的科学或商业洞见。

尽管这是一个基础性决策，但其背后的复杂性和微妙之处常常被忽视，构成了理论与实践之间的知识鸿沟。本文旨在系统性地填补这一鸿沟，为您提供一个关于相异性度量的全面指南。我们将超越对单一默认度量（如欧几里得距离）的依赖，深入探索各种度量方法的内在逻辑与适用场景。

在接下来的内容中，您将学到：
- **原理与机制**：我们将首先深入探讨相异性度量的核心理论。您将了解为定量数据（如[欧几里得距离](@entry_id:143990)、[马氏距离](@entry_id:269828)）、[分类数据](@entry_id:202244)（如[Jaccard相异性](@entry_id:637821)）和混合类型数据（如Gower相异性）设计的不同度量方法，以及[特征缩放](@entry_id:271716)等[预处理](@entry_id:141204)步骤为何至关重要。
- **应用与跨学科连接**：理论的价值在于应用。我们将穿越[生物信息学](@entry_id:146759)、文本分析、生态学等多个领域，展示在真实世界问题中，如何根据具体的科学假设和数据特性（如关心“幅度”还是“模式”）选择最恰当的相异性度量。
- **动手实践**：最后，您将通过一系列精心设计的编程练习，亲手实践和比较不同度量在具体数据集上的表现，从而将理论知识转化为解决实际问题的能力。

通过这趟旅程，您将不仅掌握“如何”计算相异性，更将深刻理解“为何”以及“何时”选择特定的度量，为您的数据分析工作奠定坚实的基础。

## 原理与机制

在[聚类分析](@entry_id:637205)的核心，存在一个基本概念：**相异性（dissimilarity）**。任何[聚类算法](@entry_id:146720)，无论是划分式的、层次式的还是基于密度的，其根本目的都是将“相似”的观测点分到同一组，并将“不相似”的观测点分到不同组。因此，我们如何量化“不相似”，即选择一个恰当的相异性度量，是整个聚类过程的基石。这个选择并非无关紧要，它深刻地定义了我们对数据结构和关系的理解，并直接决定了最终的聚类结果。本章将系统地阐述用于量化不同类型数据间相异性的核心原理与机制。

### 面向定量数据的相异性度量

对于以实数[向量表示](@entry_id:166424)的定量数据，[距离度量](@entry_id:636073)是最自然的相[异性选择](@entry_id:174974)。其中，一个庞大且重要的家族是闵可夫斯基距离（Minkowski distance）。

#### 闵可夫斯基距离（$L_p$范数）

对于两个在 $d$ 维空间中的向量 $x = (x_1, \dots, x_d)$ 和 $y = (y_1, \dots, y_d)$，它们之间的 $L_p$ 距离定义为：

$$
d_p(x, y) = \|x - y\|_p = \left(\sum_{i=1}^d |x_i - y_i|^p\right)^{1/p}
$$

其中 $p \ge 1$。不同的 $p$ 值定义了不同的几何空间和距离概念。在实践中，三种 $L_p$ 距离尤为常用：

1.  **$L_2$ 距离（[欧几里得距离](@entry_id:143990)，Euclidean Distance）**：当 $p=2$ 时，我们得到欧几里得距离，这是最直观、最常用的[距离度量](@entry_id:636073)，代表了两点之间的“直线”距离。
    $$
    d_2(x, y) = \sqrt{\sum_{i=1}^d (x_i - y_i)^2}
    $$
    [欧几里得距离](@entry_id:143990)的[等距点](@entry_id:637779)形成一个超球面。它隐含了一个核心假设：空间是**各向同性（isotropic）**的，即所有维度上的差异具有同等的重要性。在聚类中，这意味着两个[聚类](@entry_id:266727)中心之间的决策边界是一条[垂直平分线](@entry_id:163148)。

2.  **$L_1$ 距离（[曼哈顿距离](@entry_id:141126)，Manhattan Distance）**：当 $p=1$ 时，我们得到[曼哈顿距离](@entry_id:141126)，也称“城市街区”距离。它计算的是沿着坐标轴移动的总距离。
    $$
    d_1(x, y) = \sum_{i=1}^d |x_i - y_i|
    $$
    $L_1$ 距离的[等距点](@entry_id:637779)形成一个超菱形（在二维空间中是菱形）。相比于 $L_2$ 距离，它对异常值不那么敏感。

3.  **$L_\infty$ 距离（[切比雪夫距离](@entry_id:174938)，Chebyshev Distance）**：当 $p \to \infty$ 时，距离由各坐标上差异的最大值决定。
    $$
    d_\infty(x, y) = \max_{1 \le i \le d} |x_i - y_i|
    $$
    $L_\infty$ 距离的[等距点](@entry_id:637779)形成一个[超立方体](@entry_id:273913)（在二维空间中是正方形）。

[距离度量](@entry_id:636073)的选择会直接影响[聚类](@entry_id:266727)结果。考虑一个二维空间中的点，它与两个聚类中心的距离关系会随着 $p$ 值的变化而改变。例如，对于[中心点](@entry_id:636820) $c_0=(0,0)$ 和 $c_1=(3,0)$，一个数据点 $x_0=(2.9, 5)$ 的分配结果就依赖于所选的范数。在 $L_1$ 和 $L_2$ 范数下，它离 $c_1$ 更近，但在 $L_\infty$ 范数下，由于其在 $y$ 轴上的巨大分量，它与两个中心的距离变得相等，此时根据预设的平局规则（tie-breaking rule）可能会被分到 $c_0$ 。这揭示了[距离度量](@entry_id:636073)选择背后蕴含的几何直觉：我们认为什么样的“形状”构成了簇的自然边界。

#### [特征缩放](@entry_id:271716)的影响

[欧几里得距离](@entry_id:143990)的一个重要特性是它对特征的**尺度（scale）**非常敏感。在其平方形式 $\sum (x_i - y_i)^2$ 中，取值范围大的特征所产生的差异项会主导整个距离的计算，从而掩盖其他特征中可能存在的模式。

假设一个数据集有两个特征，其中一个特征的取值范围是 $[-100, 100]$，而另一个特征的范围仅为 $[0, 3]$。在使用[欧几里得距离](@entry_id:143990)进行聚类时，算法几乎会完全忽略第二个特征，聚类结果将主要由第一个特征的取值决定。然而，如果我们将数据进行**[标准化](@entry_id:637219)（standardization）**，即对每个特征减去其均值并除以其[标准差](@entry_id:153618)，使得所有特征都具有零均值和单位[方差](@entry_id:200758)，那么所有特征将在同等尺度上对距离做出贡献。此时，先前被掩盖的结构可能会显现出来，导致与原始数据截然不同的聚类结果 。因此，在使用欧几里得距离或任何其他依赖于特征尺度的度量之前，进行特征[标准化](@entry_id:637219)通常是一个至关重要的预处理步骤。

这种尺度效应的本质是**隐式特征加权（implicit feature weighting）**。我们可以通过一个思想实验来更深刻地理解这一点：如果在数据集中简单地复制一个特征，会发生什么？假设一个点在原始二维空间中的坐标为 $(x_1, x_2)$，我们通过复制第一个坐标将其映射到三维空间，得到 $(x_1, x_2, x_1)$。此时，任意两点间的平方[欧几里得距离](@entry_id:143990)从 $(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2$ 变为 $2(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2$。这表明，仅仅通过复制，第一个特征在 $k$-means 这类以最小化平方欧氏距离为目标的算法中的权重被隐式地加倍了。

更一般地，如果我们将一个特征复制 $m$ 次（总共有 $m+1$ 个副本），为了保持原始的距离结构，我们需要对这 $m+1$ 个副本的每一个都乘以一个缩放因子 $c$。为了使新的平方欧氏距离等于原始距离，即 $c^2(m+1)(\Delta x_1)^2 + (\Delta x_2)^2 = (\Delta x_1)^2 + (\Delta x_2)^2$，我们必须选择 $c(m) = \frac{1}{\sqrt{m+1}}$ 。这个结论清晰地揭示了特征冗余和缩放如何改变距离的几何性质，并强调了在应用[距离度量](@entry_id:636073)前理解和处理这些问题的必要性。

#### 超越各向同性：[马氏距离](@entry_id:269828)

欧几里得距离的各向同性假设（即簇是球形的）在许多现实场景中并不成立。数据簇可能是椭圆形的，并且沿着不同方向有不同的[方差](@entry_id:200758)。在这种情况下，一个点在物理上可能离一个椭球簇的中心更近，但从统计意义上讲，它可能属于另一个[方差](@entry_id:200758)更大的簇。

为了解决这个问题，我们可以使用**[马氏距离](@entry_id:269828)（Mahalanobis Distance）**。对于一个均值为 $\boldsymbol{\mu}$、协方差矩阵为 $\mathbf{\Sigma}$ 的数据[分布](@entry_id:182848)，一个点 $\mathbf{x}$ 到中心 $\boldsymbol{\mu}$ 的[马氏距离](@entry_id:269828)定义为：

$$
d_M(\mathbf{x}, \boldsymbol{\mu}; \mathbf{\Sigma}^{-1}) = \sqrt{(\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})}
$$

[马氏距离](@entry_id:269828)通过协方差矩阵的逆 $\mathbf{\Sigma}^{-1}$ 对数据进行了**白化（whitening）**变换。直观上，它将数据空间进行[旋转和缩放](@entry_id:154036)，使得原本椭球形的簇在新的[坐标系](@entry_id:156346)下变成球形。它度量的是一个点到[分布](@entry_id:182848)中心的距离，但这个距离是以标准差为单位的。

考虑一个场景，其中有两个高斯簇，它们的均值不同，且一个簇是球形的（例如 $\mathbf{\Sigma}_A = \mathrm{diag}(1,1)$），而另一个是扁平的椭球形（例如 $\mathbf{\Sigma}_B = \mathrm{diag}(9,1)$）。[欧几里得距离](@entry_id:143990)的决策边界将是两个均值连线的[垂直平分线](@entry_id:163148)，这会导致许多本应属于椭球簇的点被错误地划分到球形簇中。相比之下，基于[马氏距离](@entry_id:269828)的分类器会考虑每个簇自身的协[方差](@entry_id:200758)结构。它为每个簇定义了椭圆形的等距线，其形状和方向与簇的协[方差](@entry_id:200758)一致。这使得它能够更准确地捕捉数据的真实[分布](@entry_id:182848)，从而得到更合理的[聚类](@entry_id:266727)结果 。当两个簇的[协方差矩阵](@entry_id:139155)相同时（尤其是当它们都是[单位矩阵](@entry_id:156724)时），[马氏距离](@entry_id:269828)会退化为欧几里得距离。

#### 基于形状的相异性：相关性距离

在某些应用中，我们更关心两个数据系列（如时间序列）的“形状”或“模式”是否相似，而不是它们的绝对数值大小。例如，在分析基因表达谱时，两个基因的表达水平可能基线不同，但它们的表达模式（何时上升、何时下降）可能高度一致。

在这种情况下，**相关性距离（Correlation-based Dissimilarity）**是一个非常有效的度量。它通常定义为：

$$
d(x, y) = 1 - \rho_{xy}
$$

其中 $\rho_{xy}$ 是两个向量 $x$ 和 $y$ 之间的[皮尔逊相关系数](@entry_id:270276)（Pearson correlation coefficient）。相关系数本身衡量了两个变量[线性相关](@entry_id:185830)的强度和方向，其取值范围在 $[-1, 1]$ 之间。值为 $1$ 表示完全正相关，$-1$ 表示完全负相关，$0$ 表示没有线性关系。通过 $1 - \rho_{xy}$ 的转换，我们将相关性度量转化为了一个相异性度量，其取值范围在 $[0, 2]$ 之间。

[皮尔逊相关系数](@entry_id:270276)有一个关键的数学性质：它可以表示为中心化向量之间的夹角余弦。令 $\tilde{x} = x - \bar{x}\mathbf{1}$ 和 $\tilde{y} = y - \bar{y}\mathbf{1}$ 分别为 $x$ 和 $y$ 的中心化向量（其中 $\bar{x}, \bar{y}$ 是均值，$\mathbf{1}$ 是全1向量），则：

$$
\rho_{xy} = \frac{\tilde{x}^\top \tilde{y}}{\|\tilde{x}\| \|\tilde{y}\|}
$$

这个公式揭示了相关性距离的内在不变性。由于计算前先进行了中心化，任何对原始数据向量的**均值平移（mean shift）**都不会影响其中心化向量。例如，将向量 $y$ 变为 $y' = y + c\mathbf{1}$（$c$ 为常数），其新的均值变为 $\bar{y} + c$，但其中心化向量 $\tilde{y'}$ 仍然等于 $\tilde{y}$。因此，相关性距离对数据的基线水平不敏感，只关注其变化的模式 。这一特性使其成为分析时间序列、基因表达谱和其他类型剖面数据的理想选择。

### 面向[分类数据](@entry_id:202244)和混合数据的相异性度量

现实世界的数据集很少只包含纯粹的定量变量。当数据包含分类（categorical）或[序数](@entry_id:150084)（ordinal）变量时，闵可夫斯基距离不再适用，我们需要专门的度量方法。

#### 二[元数据](@entry_id:275500)的度量

对于只包含0和1的二[元数据](@entry_id:275500)，有几种标准的相异性度量：

1.  **汉明距离（Hamming Distance）**：这是最简单的度量，它计算两个二元向量之间不同位置的数量。对于向量 $x, y \in \{0,1\}^p$，汉明距离 $d_H(x,y) = \sum_{j=1}^p \mathbb{1}[x_j \neq y_j]$。有趣的是，对于二元数据，汉明距离在数值上等于[曼哈顿距离](@entry_id:141126) $d_1(x,y)$，也等于平方[欧几里得距离](@entry_id:143990) $\|x-y\|_2^2$ 。

2.  **简单匹配相异性（Simple Matching Dissimilarity）**：这是[汉明距离](@entry_id:157657)的归一化版本，即不匹配位置的比例：$d_S(x,y) = \frac{1}{p} d_H(x,y)$。由于它与汉明距离成正比，使用这两种度量进行聚类通常会得到相同的分配结果。

3.  **Jaccard 相异性（Jaccard Dissimilarity）**：与前两者不同，Jaccard 距离专门用于处理**非对称二元属性**。在这种属性中，正匹配（1-1）比负匹配（0-0）包含更多信息。例如，在购物篮分析中，两个人同时购买了面包（1-1）是一个有意义的相似性信号，而两个人同时没有购买牛奶（0-0）则几乎不提供任何信息。Jaccard 距离的定义恰恰反映了这一点：
    $$
    d_J(x,y) = 1 - \frac{\sum_{j=1}^p \mathbb{1}[x_j = 1 \land y_j = 1]}{\sum_{j=1}^p \mathbb{1}[x_j = 1 \lor y_j = 1]}
    $$
    它只关注两个向量中至少有一个为1的位置，并计算这些位置中同时为1的比例（Jaccard系数），然后用1减去该比例。由于忽略了0-0匹配，[Jaccard距离](@entry_id:637821)在处理稀疏二[元数据](@entry_id:275500)时尤为有效。在实际应用中，使用[汉明距离](@entry_id:157657)还是[Jaccard距离](@entry_id:637821)可能会导致不同的[聚类](@entry_id:266727)结果，这取决于0-0匹配是否被认为是有意义的相似性证据 。

#### 混合类型数据的Gower相异性

当数据集包含多种类型的变量（如数值、[序数](@entry_id:150084)、分类）时，我们需要一个统一的框架来计算相异性。**Gower相异性（Gower's Dissimilarity）**为此提供了一个优雅的解决方案。其核心思想是为每一对观测点 $(O_i, O_j)$ 计算一个总的相异性分数 $D(O_i, O_j)$，该分数是所有变量上逐一计算的相异性分数的加权平均值：

$$
D(O_i, O_j) = \frac{\sum_{k=1}^{p} w_k \delta_k d_{ij}^{(k)}}{\sum_{k=1}^{p} w_k \delta_k}
$$

-   $d_{ij}^{(k)}$ 是观测点 $i$ 和 $j$ 在第 $k$ 个变量上的相异性，其值被归一化到 $[0, 1]$ 区间。计算方式取决于变量类型：
    -   **数值变量**：使用范围归一化的绝对差，$d_{ij}^{(k)} = \frac{|x_{ik} - x_{jk}|}{R_k}$，其中 $R_k$ 是该变量的取值范围。
    -   **[序数](@entry_id:150084)变量**：首先将[序数](@entry_id:150084)水平（如“小”、“中”、“大”）映射为数值等级，然后按数值变量处理。
    -   **[分类变量](@entry_id:637195)**：使用简单匹配，如果值相同则 $d_{ij}^{(k)}=0$，不同则为 $1$。
-   $w_k$ 是赋予第 $k$ 个变量的权重，允许用户根据领域知识调整不同变量的重要性。
-   $\delta_k$ 是一个指示器，用于处理缺失值或特殊情况。例如，如果一个数值变量的范围为零，它对区分观测点没有贡献，可以将其从计算中排除。

Gower相异性的强大之处在于其灵活性。通过调整权重向量 $\mathbf{w}$，我们可以探索不同特征对[聚类](@entry_id:266727)结构的贡献。例如，通过增加某个变量的权重，可以使聚类结果更倾向于在该维度上形成分组。反之，将权重设为0可以完全忽略一个变量。通过比较不同权重方案下的[聚类](@entry_id:266727)结果（例如，使用兰德指数等指标衡量其稳定性），我们可以评估[聚类](@entry_id:266727)结构对特定变量的敏感性，从而更深入地理解数据内在的模式 。

### 相异性度量与[聚类算法](@entry_id:146720)的内在联系

选择相异性度量不仅是[数据预处理](@entry_id:197920)的一步，它还与[聚类算法](@entry_id:146720)的内部机制紧密相连，尤其是在像 $k$-means 这样的原型[聚类方法](@entry_id:747401)中。

#### 相异性与中心更新规则

标准 $k$-means 算法包含两个交替执行的步骤：分配步骤和更新步骤。在更新步骤中，每个簇的中心被重新计算为其所有成员点的**质心（centroid）**，即[算术平均值](@entry_id:165355)。这一看似自然的选择背后有着深刻的数学原理。算术平均值是唯一能最小化一个点集到该中心点的**平方[欧几里得距离](@entry_id:143990)之和**的点。

$$
\boldsymbol{\mu} = \arg\min_c \sum_{i=1}^n \|x_i - c\|_2^2
$$

因此，$k$-means 的[质心](@entry_id:265015)更新规则与平方欧几里得距离这个目标函数是“匹配”的。如果我们试图将 $k$-means 的[距离度量](@entry_id:636073)简单地替换为其他类型，例如 $L_1$（曼哈顿）距离，但仍然保留算术平均值作为中心更新规则，算法的收敛性保证就会失效。

这是因为，最小化 $L_1$ 距离之和的点不是算术平均值，而是**分量中位数（component-wise median）**。考虑一维空间中的点集 $\{0, 0, 10\}$。其均值为 $10/3$，到这三点的 $L_1$ 距离总和为 $|0 - 10/3| + |0 - 10/3| + |10 - 10/3| = 40/3 \approx 13.33$。而该点集的中位数为 $0$，到这三点的 $L_1$ 距离总和为 $|0-0| + |0-0| + |10-0| = 10$。显然，均值并非 $L_1$ 距离下的最优中心。在这种“不匹配”的情况下，算法的更新步骤可能不会降低（甚至可能增加）目标函数的值，从而破坏了算法的理论基础 。

#### 更通用的方法：$k$-中心点（$k$-medoids）

为了摆脱这种对特定[距离度量](@entry_id:636073)的依赖，我们可以采用一种更通用、更稳健的算法：**$k$-[中心点](@entry_id:636820)（$k$-medoids）**，其典型实现是**围绕[中心点](@entry_id:636820)划分（PAM）**算法。

$k$-medoids 与 $k$-means 的核心区别在于对“中心”的定义。$k$-medoids 算法不计算可能不存在于数据集中的“虚拟”质心，而是约束簇的代表（称为**中心点，medoid**）必须是数据集中的一个实际观测点。

这个约束带来了巨大的好处。更新步骤不再是求解一个可能很复杂的[优化问题](@entry_id:266749)（如找到多维中位数），而是变成一个离散的[搜索问题](@entry_id:270436)：对于一个给定的簇，遍历其所有成员点（或者所有数据点），找到那个能使簇内总相异性最小的点，并将其指定为新的[中心点](@entry_id:636820)。

由于这个过程只依赖于计算数据点之间的成对相异性，它不关心这些相异性值是如何计算出来的。这意味着 $k$-medoids 算法可以与**任何**合理的相异性度量配合使用，无论是[欧几里得距离](@entry_id:143990)、[曼哈顿距离](@entry_id:141126)，还是为[分类数据](@entry_id:202244)定义的汉明距离和[Jaccard距离](@entry_id:637821)，甚至是基于非结构化数据（如文本、图像）计算出的任意相异性矩阵。这种灵活性使得 $k$-medoids 成为一个功能强大且适用性广泛的[聚类](@entry_id:266727)工具 。

### 高级主题：[流形](@entry_id:153038)上的相异性

传统的[距离度量](@entry_id:636073)（如[欧几里得距离](@entry_id:143990)）是在一个平坦的[欧氏空间](@entry_id:138052)中测量的。然而，许多高维数据集的内在结构可能并非平坦的，而是位于一个嵌入在高维空间中的低维**[流形](@entry_id:153038)（manifold）**上。在这种情况下，欧氏距离（**外在距离，extrinsic distance**）可能无法准确反映数据点之间沿[流形](@entry_id:153038)表面的真实距离（**内在距离，intrinsic distance** 或 **[测地线](@entry_id:269969)距离，geodesic distance**）。

一个经典的例子是“瑞士卷”数据集。想象一张被卷起来的纸。纸上的两个点，如果沿着纸面测量，可能非常近；但如果直接在三维空间中测量它们的直线距离，可能会因为纸的卷曲而变得非常远。

直接在[流形](@entry_id:153038)上计算精确的[测地线](@entry_id:269969)距离通常很困难。一个有效的近似方法是构建一个**$k$-近邻图（$k$-nearest neighbor graph）**。图中的每个节点代表一个数据点，每个节点与其在[欧氏空间](@entry_id:138052)中最接近的 $k$ 个邻居之间有边相连，边的权重等于它们之间的欧氏距离。然后，任意两点之间的[测地线](@entry_id:269969)距离就可以近似为它们在图上的**[最短路径](@entry_id:157568)长度**。

使用这种近似的[测地线](@entry_id:269969)距离进行聚类，可以发现仅凭欧氏距离无法识别的结构。在瑞士卷的例子中，基于欧氏距离的聚类可能会错误地将卷的相邻层上的点分在一组，而基于[测地线](@entry_id:269969)距离的聚类则能够正确地“展开”这个卷，并根据点在原始二维纸面上的位置进行[聚类](@entry_id:266727) 。

这种方法的成功与否对参数 $k$ 的选择很敏感。如果 $k$ 太小，图可能是不连通的，导致无法计算所有点对之间的距离。如果 $k$ 太大（例如，接近数据点总数），图就趋于完全连通，最短路径就退化为直接的欧氏距离，从而丧失了捕捉[流形](@entry_id:153038)结构的能力。因此，找到一个合适的邻域大小是[流形学习](@entry_id:156668)和相关[聚类方法](@entry_id:747401)的关键挑战。