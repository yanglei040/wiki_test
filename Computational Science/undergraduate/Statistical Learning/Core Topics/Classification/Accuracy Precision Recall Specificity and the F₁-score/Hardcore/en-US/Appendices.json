{
    "hands_on_practices": [
        {
            "introduction": "To truly understand a classifier's performance, it's often helpful to compare it against a simple baseline. This exercise explores one such baseline: a model that naively predicts the positive class for every single instance. By deriving the core metrics for this model , you will gain intuition for how class prevalence directly impacts accuracy and precision, and see how the $F_1$-score behaves in this extreme, high-recall scenario.",
            "id": "3094187",
            "problem": "Consider a binary classification task with $N$ labeled instances, where each true label $y \\in \\{0,1\\}$ and the positive-class prevalence is $\\pi \\in (0,1)$, defined by $\\pi = N_{+}/N$, with $N_{+}$ the number of positives and $N_{-} = N - N_{+}$ the number of negatives. A deterministic classifier produces the prediction $\\hat{y} = 1$ for every instance.\n\nStarting from the standard confusion matrix counts—true positives, false positives, true negatives, and false negatives—and the fundamental definitions of accuracy, precision, recall (true positive rate), specificity (true negative rate), and the F1-score (F1), derive the explicit conditions on $N_{+}$ and $N_{-}$ under which the recall $R$ equals $1$ and the specificity $S$ equals $0$. Under these conditions, show that the precision $P$ equals $\\pi$ and the accuracy $\\text{Acc}$ equals $\\pi$. Then, express the F1-score as a closed-form function of the prevalence $\\pi$. Round nothing. Your final answer must be the simplified analytic expression for the F1-score in terms of $\\pi$ only.",
            "solution": "The problem requires the derivation of an expression for the F1-score for a specific type of binary classifier, starting from the fundamental definitions of common classification metrics. The validation of the problem statement is the first step.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Task**: Binary classification.\n- **Instances**: $N$ labeled instances.\n- **True Labels**: $y \\in \\{0,1\\}$, where $y=1$ is the positive class and $y=0$ is the negative class.\n- **Positive-Class Prevalence**: $\\pi \\in (0,1)$, defined as $\\pi = N_{+}/N$.\n- **Number of Positives**: $N_{+}$.\n- **Number of Negatives**: $N_{-} = N - N_{+}$.\n- **Classifier Behavior**: A deterministic classifier that produces the prediction $\\hat{y} = 1$ for every instance.\n- **Metrics to Use**: Accuracy (Acc), Precision ($P$), Recall ($R$), Specificity ($S$), F1-score ($F1$).\n- **Required Derivations**:\n    1.  Find the conditions on $N_{+}$ and $N_{-}$ for which $R=1$ and $S=0$.\n    2.  Show that under these conditions, $P = \\pi$ and $\\text{Acc} = \\pi$.\n    3.  Express the F1-score as a function of $\\pi$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard exercise in statistical learning, testing the understanding of classification metrics under a specific, well-defined baseline model. The model, while simple (always predicting the positive class), is a valid and commonly used point of reference. All terms ($N, N_{+}, N_{-}, \\pi$, etc.) are clearly defined, and the relationships between them are standard. The problem contains no scientific contradictions, ambiguities, or missing information. The condition $\\pi \\in (0,1)$ ensures that both positive and negative classes are present in the dataset (i.e., $N_{+} > 0$ and $N_{-} > 0$), which prevents division by zero in the definitions of the metrics.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe solution begins by determining the counts for the confusion matrix based on the classifier's specified behavior. The classifier predicts the positive class for all instances, i.e., $\\hat{y} = 1$ for all $N$ instances. The dataset consists of $N_{+}$ true positive instances ($y=1$) and $N_{-}$ true negative instances ($y=0$).\n\n1.  **Confusion Matrix Elements**:\n    -   **True Positives ($TP$)**: The number of instances where the true label is positive ($y=1$) and the prediction is also positive ($\\hat{y}=1$). Since the classifier predicts $\\hat{y}=1$ for all instances, it correctly classifies all $N_{+}$ positive instances. Thus, $TP = N_{+}$.\n    -   **False Positives ($FP$)**: The number of instances where the true label is negative ($y=0$) but the prediction is positive ($\\hat{y}=1$). The classifier incorrectly classifies all $N_{-}$ negative instances as positive. Thus, $FP = N_{-}$.\n    -   **True Negatives ($TN$)**: The number of instances where the true label is negative ($y=0$) and the prediction is also negative ($\\hat{y}=0$). Since the classifier never predicts $\\hat{y}=0$, this count is zero. Thus, $TN = 0$.\n    -   **False Negatives ($FN$)**: The number of instances where the true label is positive ($y=1$) but the prediction is negative ($\\hat{y}=0$). Since the classifier never predicts $\\hat{y}=0$, this count is also zero. Thus, $FN = 0$.\n\n2.  **Fundamental Metric Definitions**:\n    -   Recall ($R$) or True Positive Rate: $R = \\frac{TP}{TP + FN}$\n    -   Specificity ($S$) or True Negative Rate: $S = \\frac{TN}{TN + FP}$\n    -   Precision ($P$): $P = \\frac{TP}{TP + FP}$\n    -   Accuracy ($\\text{Acc}$): $\\text{Acc} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{TP+TN}{N}$\n    -   F1-score ($F1$): $F1 = \\frac{2 \\cdot P \\cdot R}{P + R}$\n\n3.  **Derivation of Conditions for $R=1$ and $S=0$**:\n    -   Substituting the confusion matrix counts into the definition of Recall:\n        $$R = \\frac{N_{+}}{N_{+} + 0} = \\frac{N_{+}}{N_{+}}$$\n        For $R$ to be equal to $1$, the denominator $N_{+}$ must be non-zero. The problem states that the prevalence $\\pi = N_{+}/N$ is in the open interval $(0,1)$. This implies $N_{+} > 0$ and $N_{-} > 0$. Therefore, under the given problem conditions, $R$ is always equal to $1$.\n    -   Substituting the confusion matrix counts into the definition of Specificity:\n        $$S = \\frac{0}{0 + N_{-}} = \\frac{0}{N_{-}}$$\n        For $S$ to be equal to $0$, the denominator $N_{-}$ must be non-zero. As established from $\\pi \\in (0,1)$, it follows that $N_{-} = N(1-\\pi) > 0$. Therefore, under the given problem conditions, $S$ is always equal to $0$.\n    -   The conditions on $N_{+}$ and $N_{-}$ under which $R=1$ and $S=0$ are $N_{+} > 0$ and $N_{-} > 0$. These are automatically satisfied by the given constraint $\\pi \\in (0,1)$.\n\n4.  **Verification of $P = \\pi$ and $\\text{Acc} = \\pi$**:\n    -   Using the definition of Precision:\n        $$P = \\frac{TP}{TP + FP} = \\frac{N_{+}}{N_{+} + N_{-}}$$\n        Since the total number of instances is $N = N_{+} + N_{-}$, we can write:\n        $$P = \\frac{N_{+}}{N}$$\n        By the problem's definition of prevalence, $\\pi = N_{+}/N$. Therefore, we have shown that $P=\\pi$.\n    -   Using the definition of Accuracy:\n        $$\\text{Acc} = \\frac{TP + TN}{N} = \\frac{N_{+} + 0}{N} = \\frac{N_{+}}{N}$$\n        Again, using the definition of prevalence, we find that $\\text{Acc}=\\pi$.\n\n5.  **Derivation of the F1-score in terms of $\\pi$**:\n    The F1-score is the harmonic mean of Precision and Recall:\n    $$F1 = \\frac{2 \\cdot P \\cdot R}{P + R}$$\n    From our previous steps, we have established that for this classifier, $P=\\pi$ and $R=1$. Substituting these expressions into the F1-score formula yields:\n    $$F1 = \\frac{2 \\cdot \\pi \\cdot 1}{\\pi + 1}$$\n    Simplifying this expression gives the final form for the F1-score as a function of the prevalence $\\pi$:\n    $$F1 = \\frac{2\\pi}{1+\\pi}$$\nThis expression represents the F1-score for a classifier that always predicts the positive class, expressed solely in terms of the positive-class prevalence $\\pi$.",
            "answer": "$$\\boxed{\\frac{2\\pi}{1+\\pi}}$$"
        },
        {
            "introduction": "In many real-world applications, such as medical diagnosis or fraud detection, the cost of different types of errors is unequal, leading to a trade-off between precision and recall. This problem presents a practical scenario involving two different automated assistants designed to detect rare events, one optimized for high recall and the other for high precision. By constructing confusion matrices and calculating a full suite of metrics for both systems , you will learn to quantitatively evaluate this trade-off and understand why a balanced measure like the $F_1$-score is essential for model selection in imbalanced datasets.",
            "id": "3094207",
            "problem": "A professional sports league is piloting two automated assistants to help referees detect a rare foul during games. The league has a labeled evaluation set of $N=10000$ plays, of which $F=100$ are actual fouls (ground truth), reflecting the rarity of fouls. For each automated assistant, a \"flag\" indicates the assistant predicts a foul on a play.\n\nAssistant $\\mathsf{A}$ (designed for high recall and low precision) flags $990$ plays; among these flagged plays, $90$ are actual fouls. Assistant $\\mathsf{B}$ (designed for high precision and low recall) flags $40$ plays; among these flagged plays, $38$ are actual fouls. Plays that are not flagged are predicted as \"no foul.\"\n\nConstruct, for each assistant, the $2\\times 2$ confusion matrix with counts of True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN). Then, using the standard definitions based on these counts, compute:\n- Accuracy,\n- Precision ($P$),\n- Recall ($R$),\n- Specificity,\n- The $F_1$-score.\n\nFinally, determine which assistant yields the larger $F_1$-score on this evaluation set.\n\nChoose one option:\n\nA. Assistant $\\mathsf{A}$ (high-$R$/low-$P$) has the larger $F_1$-score.\n\nB. Assistant $\\mathsf{B}$ (high-$P$/low-$R$) has the larger $F_1$-score.\n\nC. Both assistants have the same $F_1$-score.\n\nD. It cannot be determined without additional information about class prevalence beyond the given counts.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Total number of plays in the evaluation set: $N = 10000$.\n- Total number of actual fouls (positive class): $F = 100$.\n- The number of actual non-fouls (negative class) is $N - F = 10000 - 100 = 9900$.\n- For Assistant $\\mathsf{A}$:\n  - It flags (predicts as positive) $990$ plays.\n  - Among the flagged plays, $90$ are actual fouls.\n- For Assistant $\\mathsf{B}$:\n  - It flags (predicts as positive) $40$ plays.\n  - Among the flagged plays, $38$ are actual fouls.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is based on standard, well-defined metrics in statistical learning and machine learning (confusion matrix, accuracy, precision, recall, $F_1$-score). These concepts are mathematically rigorous and widely used for evaluating classification models.\n- **Well-Posedness**: The problem provides all necessary information to construct the confusion matrices for both assistants and subsequently calculate the required performance metrics. The final question is unambiguous.\n- **Objectivity**: The problem is stated in objective terms, using standard terminology from the field of statistics.\n- **Consistency and Completeness**: The provided numbers are internally consistent and sufficient. For each assistant, the number of true positives and the total number of predicted positives are given. Combined with the ground truth counts of actual positives and actual negatives for the entire dataset, all components of the confusion matrix can be uniquely determined. For instance, for Assistant $\\mathsf{A}$, we have $TP_A=90$ and $TP_A+FP_A=990$, which implies $FP_A=900$. Given that actual positives are $100$, $FN_A = 100 - TP_A = 10$. Given actual negatives are $9900$, $TN_A = 9900 - FP_A = 9000$. The sum $TP_A+FP_A+FN_A+TN_A=90+900+10+9000=10000=N$, confirming consistency. A similar check confirms consistency for Assistant $\\mathsf{B}$.\n\n**Step 3: Verdict and Action**\n- The problem is valid as it is scientifically grounded, well-posed, objective, complete, and consistent. The solution process may proceed.\n\n---\n\nThe task is to compare the $F_1$-score of two automated assistants, $\\mathsf{A}$ and $\\mathsf{B}$. The analysis requires constructing a $2 \\times 2$ confusion matrix for each assistant and calculating several performance metrics.\n\nThe confusion matrix for a binary classification task is structured as follows:\n- **True Positives ($TP$)**: Number of positive instances correctly classified as positive.\n- **False Positives ($FP$)**: Number of negative instances incorrectly classified as positive (Type I error).\n- **True Negatives ($TN$)**: Number of negative instances correctly classified as negative.\n- **False Negatives ($FN$)**: Number of positive instances incorrectly classified as negative (Type II error).\n\nFrom these counts, we define the following metrics:\n- **Precision ($P$)**: The proportion of predicted positive instances that are actually positive.\n  $$P = \\frac{TP}{TP + FP}$$\n- **Recall ($R$)**: The proportion of actual positive instances that are correctly identified. Also known as Sensitivity or True Positive Rate.\n  $$R = \\frac{TP}{TP + FN}$$\n- **$F_1$-score**: The harmonic mean of Precision and Recall.\n  $$F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$\n- **Accuracy**: The proportion of all instances that are correctly classified.\n  $$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n- **Specificity**: The proportion of actual negative instances that are correctly identified. Also known as True Negative Rate.\n  $$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n\nThe total number of actual positives (fouls) is $100$, and the total number of actual negatives (no fouls) is $10000 - 100 = 9900$.\n\n**Analysis of Assistant $\\mathsf{A}$**\n- Predicted positives (flagged plays) = $TP_A + FP_A = 990$.\n- True Positives ($TP_A$): Among the flagged plays, $90$ are actual fouls. So, $TP_A = 90$.\n- False Positives ($FP_A$): $FP_A = 990 - TP_A = 990 - 90 = 900$.\n- False Negatives ($FN_A$): The total number of actual fouls is $100$. $FN_A = (\\text{Actual Positives}) - TP_A = 100 - 90 = 10$.\n- True Negatives ($TN_A$): The total number of non-fouls is $9900$. $TN_A = (\\text{Actual Negatives}) - FP_A = 9900 - 900 = 9000$.\n\nThe confusion matrix for Assistant $\\mathsf{A}$ is:\n$$\n\\begin{array}{|c|cc|c|}\n\\hline\n & \\text{Predicted Foul} & \\text{Predicted No Foul} & \\text{Total} \\\\\n\\hline\n\\text{Actual Foul} & TP_A = 90 & FN_A = 10 & 100 \\\\\n\\text{Actual No Foul} & FP_A = 900 & TN_A = 9000 & 9900 \\\\\n\\hline\n\\text{Total} & 990 & 9010 & 10000 \\\\\n\\hline\n\\end{array}\n$$\nMetrics for Assistant $\\mathsf{A}$:\n- Accuracy$_A = \\frac{90 + 9000}{10000} = \\frac{9090}{10000} = 0.909$.\n- Specificity$_A = \\frac{9000}{9000 + 900} = \\frac{9000}{9900} = \\frac{10}{11} \\approx 0.9091$.\n- Precision $P_A = \\frac{TP_A}{TP_A + FP_A} = \\frac{90}{990} = \\frac{1}{11}$.\n- Recall $R_A = \\frac{TP_A}{TP_A + FN_A} = \\frac{90}{100} = 0.90 = \\frac{9}{10}$.\n- $F_{1,A}$-score $= 2 \\cdot \\frac{P_A \\cdot R_A}{P_A + R_A} = 2 \\cdot \\frac{(\\frac{1}{11}) \\cdot (\\frac{9}{10})}{(\\frac{1}{11}) + (\\frac{9}{10})} = 2 \\cdot \\frac{\\frac{9}{110}}{\\frac{10+99}{110}} = 2 \\cdot \\frac{9}{109} = \\frac{18}{109}$.\nNumerically, $F_{1,A} \\approx 0.1651$.\n\n**Analysis of Assistant $\\mathsf{B}$**\n- Predicted positives (flagged plays) = $TP_B + FP_B = 40$.\n- True Positives ($TP_B$): Among the flagged plays, $38$ are actual fouls. So, $TP_B = 38$.\n- False Positives ($FP_B$): $FP_B = 40 - TP_B = 40 - 38 = 2$.\n- False Negatives ($FN_B$): $FN_B = (\\text{Actual Positives}) - TP_B = 100 - 38 = 62$.\n- True Negatives ($TN_B$): $TN_B = (\\text{Actual Negatives}) - FP_B = 9900 - 2 = 9898$.\n\nThe confusion matrix for Assistant $\\mathsf{B}$ is:\n$$\n\\begin{array}{|c|cc|c|}\n\\hline\n & \\text{Predicted Foul} & \\text{Predicted No Foul} & \\text{Total} \\\\\n\\hline\n\\text{Actual Foul} & TP_B = 38 & FN_B = 62 & 100 \\\\\n\\text{Actual No Foul} & FP_B = 2 & TN_B = 9898 & 9900 \\\\\n\\hline\n\\text{Total} & 40 & 9960 & 10000 \\\\\n\\hline\n\\end{array}\n$$\nMetrics for Assistant $\\mathsf{B}$:\n- Accuracy$_B = \\frac{38 + 9898}{10000} = \\frac{9936}{10000} = 0.9936$.\n- Specificity$_B = \\frac{9898}{9898 + 2} = \\frac{9898}{9900} \\approx 0.9998$.\n- Precision $P_B = \\frac{TP_B}{TP_B + FP_B} = \\frac{38}{40} = 0.95 = \\frac{19}{20}$.\n- Recall $R_B = \\frac{TP_B}{TP_B + FN_B} = \\frac{38}{100} = 0.38 = \\frac{19}{50}$.\n- $F_{1,B}$-score $= 2 \\cdot \\frac{P_B \\cdot R_B}{P_B + R_B} = 2 \\cdot \\frac{(0.95) \\cdot (0.38)}{0.95 + 0.38} = 2 \\cdot \\frac{0.361}{1.33} = \\frac{0.722}{1.33} = \\frac{722}{1330} = \\frac{361}{665}$.\nNumerically, $F_{1,B} \\approx 0.5429$.\n\n**Comparison and Conclusion**\nWe must compare the $F_1$-scores:\n- $F_{1,A} = \\frac{18}{109} \\approx 0.1651$\n- $F_{1,B} = \\frac{361}{665} \\approx 0.5429$\n\nClearly, $F_{1,B} > F_{1,A}$. Assistant $\\mathsf{B}$ has the larger $F_1$-score. The problem notes that Assistant $\\mathsf{A}$ is high-recall/low-precision ($R_A=0.9$, $P_A\\approx 0.09$) and Assistant $\\mathsf{B}$ is high-precision/low-recall ($P_B=0.95$, $R_B=0.38$), which our calculations confirm. Our conclusion is that the high-precision/low-recall assistant has the higher $F_1$-score in this scenario.\n\n**Evaluation of Options**\n- **A. Assistant $\\mathsf{A}$ (high-$R$/low-$P$) has the larger $F_1$-score.** This is **Incorrect**. Our calculation shows $F_{1,A} < F_{1,B}$.\n- **B. Assistant $\\mathsf{B}$ (high-$P$/low-$R$) has the larger $F_1$-score.** This is **Correct**. Our calculation shows $F_{1,B} > F_{1,A}$.\n- **C. Both assistants have the same $F_1$-score.** This is **Incorrect**. $\\frac{18}{109} \\neq \\frac{361}{665}$.\n- **D. It cannot be determined without additional information about class prevalence beyond the given counts.** This is **Incorrect**. The class prevalence (number of actual fouls and non-fouls) for the evaluation set is given ($100$ fouls in $10000$ plays), and all necessary counts to compute the $F_1$-scores are provided or can be derived. The calculation is definitive.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "When tuning a classifier, we often need to know how adjusting one performance metric, like recall, will affect others and the overall $F_1$-score. This exercise challenges you to derive the analytical relationship between precision and recall, given a fixed specificity and class prevalence, a common situation when adjusting a model's decision threshold. By solving for the minimum recall needed to achieve a target $F_1$-score , you will practice the algebraic manipulation of metric definitions, a powerful skill for understanding model behavior and setting performance goals in imbalanced domains.",
            "id": "3094185",
            "problem": "A binary classifier is deployed in a highly imbalanced setting where the prevalence of the positive class is $\\pi = 0.01$. The classifier’s specificity is fixed at $S = 0.99$, and its decision threshold is varied to adjust the recall $R \\in [0,1]$. Using only the core definitions of precision, recall, specificity, and the $F_{1}$-score as the harmonic mean of precision and recall, derive how the precision $P$ depends on $R$ when $\\pi$ and $S$ are held fixed. Then, using this dependence, determine the smallest recall value $R_{\\min}$ such that the $F_{1}$-score is at least $0.5$. Report only $R_{\\min}$ as your final numeric answer, rounded to four significant figures.",
            "solution": "The problem requires us to first establish a functional relationship between precision, $P$, and recall, $R$, for a binary classifier with fixed prevalence, $\\pi$, and specificity, $S$. Subsequently, we must determine the minimum recall, $R_{\\min}$, for which the $F_1$-score is at least $0.5$.\n\nLet us begin by defining the core classification metrics in terms of the components of a confusion matrix: True Positives ($TP$), False Positives ($FP$), True Negatives ($TN$), and False Negatives ($FN$). Let $N_{pos} = TP + FN$ be the total count of actual positive instances and $N_{neg} = TN + FP$ be the total count of actual negative instances.\n\nThe definitions of recall ($R$), specificity ($S$), and precision ($P$) are:\n$$R = \\frac{TP}{N_{pos}}$$\n$$S = \\frac{TN}{N_{neg}}$$\n$$P = \\frac{TP}{TP + FP}$$\n\nThe prevalence of the positive class, $\\pi$, is the fraction of positive instances in the total population:\n$$\\pi = \\frac{N_{pos}}{N_{pos} + N_{neg}}$$\nFrom this, it follows that the fraction of negative instances is $1-\\pi$:\n$$1-\\pi = \\frac{N_{neg}}{N_{pos} + N_{neg}}$$\nThe ratio of negative to positive instances can be expressed in terms of prevalence:\n$$\\frac{N_{neg}}{N_{pos}} = \\frac{1-\\pi}{\\pi}$$\n\nOur first goal is to express $P$ as a function of $R$, using the fixed parameters $\\pi$ and $S$. We can express $TP$ and $FP$ in terms of $R$, $S$, $N_{pos}$, and $N_{neg}$.\nFrom the definition of recall, $TP = R \\cdot N_{pos}$.\nFrom the definition of specificity, $TN = S \\cdot N_{neg}$. The number of false positives is the remainder of the actual negatives, so $FP = N_{neg} - TN = N_{neg} - S \\cdot N_{neg} = (1-S) N_{neg}$.\n\nNow, we substitute these expressions for $TP$ and $FP$ into the formula for precision:\n$$P = \\frac{R \\cdot N_{pos}}{R \\cdot N_{pos} + (1-S) N_{neg}}$$\nTo eliminate the dependency on the absolute counts $N_{pos}$ and $N_{neg}$, we can divide the numerator and the denominator by $N_{pos}$:\n$$P(R) = \\frac{R}{R + (1-S) \\frac{N_{neg}}{N_{pos}}}$$\nSubstituting the ratio $\\frac{N_{neg}}{N_{pos}} = \\frac{1-\\pi}{\\pi}$, we arrive at the desired relationship between $P$ and $R$:\n$$P(R) = \\frac{R}{R + (1-S) \\frac{1-\\pi}{\\pi}}$$\nThis can be written in a more convenient form:\n$$P(R) = \\frac{R \\pi}{R \\pi + (1-S)(1-\\pi)}$$\n\nThe second part of the problem is to find the smallest recall $R_{\\min}$ such that the $F_1$-score is at least $0.5$. The $F_1$-score is the harmonic mean of precision and recall:\n$$F_1 = \\frac{2PR}{P+R}$$\nThe condition is $F_1 \\ge 0.5$, which is $\\frac{2PR}{P+R} \\ge \\frac{1}{2}$. Since $P$ and $R$ are non-negative, we can manipulate this inequality:\n$$4PR \\ge P+R$$\n$$P(4R-1) \\ge R$$\nGiven that $P$ and $F_1$ are increasing functions of $R$ for $R \\in [0,1]$ (as $P(R)$ is an increasing function of $R$), the minimum value of $R$ that satisfies this inequality will occur when equality holds. We therefore solve for $R$ in the equation $F_1 = 0.5$, which corresponds to $P(4R-1)=R$.\n$$P(4R-1) = R$$\nWe substitute the derived expression for $P(R)$:\n$$\\left( \\frac{R \\pi}{R \\pi + (1-S)(1-\\pi)} \\right) (4R-1) = R$$\nSince we are looking for a non-trivial solution where $R > 0$, we can divide both sides by $R$:\n$$\\frac{\\pi (4R-1)}{R \\pi + (1-S)(1-\\pi)} = 1$$\nThe denominator is strictly positive for $R \\ge 0$, so we can multiply both sides by it:\n$$\\pi(4R-1) = R\\pi + (1-S)(1-\\pi)$$\n$$4R\\pi - \\pi = R\\pi + (1-S)(1-\\pi)$$\nNow, we solve for $R$:\n$$3R\\pi = \\pi + (1-S)(1-\\pi)$$\n$$R = \\frac{\\pi + (1-S)(1-\\pi)}{3\\pi}$$\nThis value of $R$ is our required $R_{\\min}$.\n\nThe problem provides the numerical values $\\pi = 0.01$ and $S = 0.99$. We substitute these into the expression for $R_{\\min}$:\n$$1-\\pi = 1 - 0.01 = 0.99$$\n$$1-S = 1 - 0.99 = 0.01$$\n$$R_{\\min} = \\frac{0.01 + (0.01)(0.99)}{3(0.01)}$$\n$$R_{\\min} = \\frac{0.01(1 + 0.99)}{0.03}$$\n$$R_{\\min} = \\frac{0.01 \\times 1.99}{0.03} = \\frac{0.0199}{0.03}$$\n$$R_{\\min} = \\frac{199}{300}$$\nAs a decimal, this is $R_{\\min} = 0.663333...$. Rounding to four significant figures gives $R_{\\min} = 0.6633$.",
            "answer": "$$\\boxed{0.6633}$$"
        }
    ]
}