{
    "hands_on_practices": [
        {
            "introduction": "Understanding the fundamental mathematical difference between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) is the first step toward mastering their use. This practice guides you through deriving the one-dimensional decision boundary for QDA directly from Bayes' rule, starting with Gaussian class-conditional densities. By working through the algebra, you will see precisely how unequal variances naturally give rise to a quadratic decision rule, providing a sharp contrast to the linear boundary produced by LDA.",
            "id": "3164341",
            "problem": "Two classes $Y \\in \\{1,2\\}$ have one-dimensional features $X \\in \\mathbb{R}$ with class-conditional densities modeled as Gaussian: $X \\mid Y=k \\sim \\mathcal{N}(\\mu_{k}, \\sigma_{k}^{2})$ for $k \\in \\{1,2\\}$. The prior probabilities are $\\pi_{1} = 0.7$ and $\\pi_{2} = 0.3$. The parameters are $\\mu_{1} = 0$, $\\sigma_{1}^{2} = 1$, $\\mu_{2} = 3$, and $\\sigma_{2}^{2} = 4$. Using only Bayes’ rule and the definition of the Gaussian density, do the following:\n- Derive the closed-form posterior $p(Y=1 \\mid X=x)$ for Quadratic Discriminant Analysis (QDA) in this one-dimensional setting and show that the log-odds is a quadratic function of $x$.\n- Obtain and simplify the explicit decision thresholds for QDA by solving the equation $p(Y=1 \\mid X=x) = p(Y=2 \\mid X=x)$; provide the two thresholds in closed form.\n- Explain briefly how unequal priors and unequal variances can yield asymmetric decision regions on the real line for QDA in this setting.\n- For contrast, derive the Linear Discriminant Analysis (LDA) threshold under the assumption of a common variance and equal priors, and provide the closed-form threshold for the given means.\n\nExpress the final thresholds as exact analytic expressions with no numerical rounding. Your final answer must be a single row matrix containing the two QDA thresholds (ordered from smaller to larger) followed by the single LDA threshold.",
            "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Classes: $Y \\in \\{1,2\\}$\n- Feature space: $X \\in \\mathbb{R}$ (one-dimensional)\n- Class-conditional densities: $X \\mid Y=k \\sim \\mathcal{N}(\\mu_{k}, \\sigma_{k}^{2})$ for $k \\in \\{1,2\\}$\n- Priors for QDA: $\\pi_{1} = 0.7$, $\\pi_{2} = 0.3$\n- Parameters for QDA: $\\mu_{1} = 0$, $\\sigma_{1}^{2} = 1$, $\\mu_{2} = 3$, $\\sigma_{2}^{2} = 4$\n- Task 1 (QDA): Derive the posterior $p(Y=1 \\mid X=x)$ and show the log-odds is a quadratic function of $x$.\n- Task 2 (QDA): Find the decision thresholds by solving $p(Y=1 \\mid X=x) = p(Y=2 \\mid X=x)$.\n- Task 3 (QDA): Explain how unequal priors and variances cause asymmetric decision regions.\n- Task 4 (LDA): Derive the LDA threshold assuming a common variance and equal priors, using the given means $\\mu_1 = 0$ and $\\mu_2 = 3$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it deals with standard Bayesian classification models (QDA and LDA) based on Gaussian distributions, a core topic in statistical learning. The problem is well-posed; all necessary parameters for the QDA part are provided. For the LDA part, the assumption of a common variance is made, and as will be shown, its specific value is not needed as it cancels from the derivation of the threshold. The problem is objective, using clear and unambiguous mathematical definitions. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be provided.\n\n### Solution Derivation\n\n**Part 1: QDA Posterior and Log-Odds**\nThe posterior probability of class $Y=1$ given an observation $X=x$ is given by Bayes' rule:\n$$p(Y=1 \\mid X=x) = \\frac{p(X=x \\mid Y=1) p(Y=1)}{p(X=x \\mid Y=1) p(Y=1) + p(X=x \\mid Y=2) p(Y=2)}$$\nLet $\\pi_k = p(Y=k)$ be the prior probability for class $k$, and let $f_k(x) = p(X=x \\mid Y=k)$ be the class-conditional probability density. For a Gaussian distribution, this is:\n$$f_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} \\exp\\left(-\\frac{(x-\\mu_k)^2}{2\\sigma_k^2}\\right)$$\nThe posterior for class $1$ is:\n$$p(Y=1 \\mid X=x) = \\frac{\\pi_1 f_1(x)}{\\pi_1 f_1(x) + \\pi_2 f_2(x)}$$\nThe log-odds, or logit, is the natural logarithm of the ratio of the posterior probabilities:\n$$\\text{log-odds} = \\ln\\left(\\frac{p(Y=1 \\mid X=x)}{p(Y=2 \\mid X=x)}\\right) = \\ln\\left(\\frac{\\frac{\\pi_1 f_1(x)}{\\pi_1 f_1(x) + \\pi_2 f_2(x)}}{\\frac{\\pi_2 f_2(x)}{\\pi_1 f_1(x) + \\pi_2 f_2(x)}}\\right) = \\ln\\left(\\frac{\\pi_1 f_1(x)}{\\pi_2 f_2(x)}\\right)$$\nThis can be expanded as:\n$$\\text{log-odds} = \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) + \\ln(f_1(x)) - \\ln(f_2(x))$$\nSubstituting the expression for the Gaussian log-density, where $\\ln(f_k(x)) = -\\frac{1}{2}\\ln(2\\pi\\sigma_k^2) - \\frac{(x-\\mu_k)^2}{2\\sigma_k^2}$:\n$$\\text{log-odds} = \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) - \\frac{1}{2}\\ln(2\\pi\\sigma_1^2) - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} - \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_2^2) - \\frac{(x-\\mu_2)^2}{2\\sigma_2^2}\\right)$$\n$$\\text{log-odds} = \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\frac{(x-\\mu_2)^2}{2\\sigma_2^2}$$\nTo show this is a quadratic function of $x$, we expand the terms involving $x$:\n$$- \\frac{x^2 - 2x\\mu_1 + \\mu_1^2}{2\\sigma_1^2} + \\frac{x^2 - 2x\\mu_2 + \\mu_2^2}{2\\sigma_2^2}$$\nGrouping by powers of $x$, we have:\n$$x^2 \\left(\\frac{1}{2\\sigma_2^2} - \\frac{1}{2\\sigma_1^2}\\right) + x \\left(\\frac{\\mu_1}{\\sigma_1^2} - \\frac{\\mu_2}{\\sigma_2^2}\\right) + C$$\nwhere $C$ contains all constant terms. Since the variances are unequal ($\\sigma_1^2 \\neq \\sigma_2^2$), the coefficient of the $x^2$ term, $\\frac{1}{2\\sigma_2^2} - \\frac{1}{2\\sigma_1^2}$, is non-zero. Therefore, the log-odds is a quadratic function of $x$.\n\n**Part 2: QDA Decision Thresholds**\nThe decision thresholds are the values of $x$ where the posterior probabilities are equal, which is equivalent to the log-odds being zero.\n$$\\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\frac{(x-\\mu_2)^2}{2\\sigma_2^2} = 0$$\nWe substitute the given parameters: $\\pi_{1} = 0.7$, $\\pi_{2} = 0.3$, $\\mu_{1} = 0$, $\\sigma_{1}^{2} = 1$, $\\mu_{2} = 3$, $\\sigma_{2}^{2} = 4$.\n$$\\ln\\left(\\frac{0.7}{0.3}\\right) + \\frac{1}{2}\\ln\\left(\\frac{4}{1}\\right) - \\frac{(x-0)^2}{2(1)} + \\frac{(x-3)^2}{2(4)} = 0$$\n$$\\ln\\left(\\frac{7}{3}\\right) + \\frac{1}{2}\\ln(4) - \\frac{x^2}{2} + \\frac{x^2 - 6x + 9}{8} = 0$$\nSince $\\frac{1}{2}\\ln(4) = \\ln(\\sqrt{4}) = \\ln(2)$, the equation becomes:\n$$\\ln\\left(\\frac{7}{3}\\right) + \\ln(2) - \\frac{x^2}{2} + \\frac{x^2 - 6x + 9}{8} = 0$$\n$$\\ln\\left(\\frac{14}{3}\\right) - \\frac{4x^2}{8} + \\frac{x^2 - 6x + 9}{8} = 0$$\nMultiply by $8$:\n$$8\\ln\\left(\\frac{14}{3}\\right) - 4x^2 + x^2 - 6x + 9 = 0$$\n$$-3x^2 - 6x + \\left(9 + 8\\ln\\left(\\frac{14}{3}\\right)\\right) = 0$$\n$$3x^2 + 6x - \\left(9 + 8\\ln\\left(\\frac{14}{3}\\right)\\right) = 0$$\nWe solve this quadratic equation $ax^2 + bx + c = 0$ with $a=3$, $b=6$, and $c = -\\left(9 + 8\\ln\\left(\\frac{14}{3}\\right)\\right)$ using the quadratic formula $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$x = \\frac{-6 \\pm \\sqrt{6^2 - 4(3)\\left(-\\left(9 + 8\\ln\\left(\\frac{14}{3}\\right)\\right)\\right)}}{2(3)}$$\n$$x = \\frac{-6 \\pm \\sqrt{36 + 12\\left(9 + 8\\ln\\left(\\frac{14}{3}\\right)\\right)}}{6}$$\n$$x = \\frac{-6 \\pm \\sqrt{36 + 108 + 96\\ln\\left(\\frac{14}{3}\\right)}}{6}$$\n$$x = \\frac{-6 \\pm \\sqrt{144 + 96\\ln\\left(\\frac{14}{3}\\right)}}{6}$$\nWe can factor out $16$ from the term under the square root:\n$$x = \\frac{-6 \\pm \\sqrt{16\\left(9 + 6\\ln\\left(\\frac{14}{3}\\right)\\right)}}{6} = \\frac{-6 \\pm 4\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}}{6}$$\n$$x = -1 \\pm \\frac{2}{3}\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}$$\nThe two thresholds, ordered from smaller to larger, are:\n$$x_1 = -1 - \\frac{2}{3}\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}$$\n$$x_2 = -1 + \\frac{2}{3}\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}$$\n\n**Part 3: Asymmetric Decision Regions**\nThe decision regions are asymmetric for two main reasons in this QDA setting:\n1.  **Unequal Variances ($\\sigma_1^2 \\neq \\sigma_2^2$):** This is the fundamental reason for a quadratic boundary. The log-ratio of the Gaussian densities includes an $x^2$ term, resulting in the log-odds being a parabola. A quadratic equation $ax^2+bx+c=0$ generally has two distinct roots ($x_1, x_2$). In this problem, the coefficient of $x^2$ in the log-odds is negative ($\\frac{1}{2\\sigma_2^2} - \\frac{1}{2\\sigma_1^2} = \\frac{1}{8} - \\frac{1}{2} = -\\frac{3}{8}  0$), so the parabola opens downwards. This means class $1$ is predicted for $x \\in [x_1, x_2]$, a finite interval. Class $2$ is predicted for $x \\in (-\\infty, x_1) \\cup (x_2, \\infty)$, a disconnected region composed of two infinite intervals. This inherent difference in the geometric nature of the decision regions (one is a single finite segment, the other is not) constitutes a form of asymmetry.\n2.  **Unequal Priors ($\\pi_1 \\neq \\pi_2$):** The priors influence the constant term of the log-odds through the term $\\ln(\\pi_1 / \\pi_2)$. If the priors are unequal, this term is non-zero, causing a vertical shift of the log-odds parabola. This shift moves the decision boundaries $x_1$ and $x_2$, altering the size of the decision regions. For example, since $\\pi_1=0.7 > \\pi_2=0.3$, the log-prior term is positive, shifting the parabola upwards and thus widening the interval $[x_1, x_2]$ where class $1$ is favored compared to a scenario with equal priors.\n\n**Part 4: LDA Threshold**\nLinear Discriminant Analysis (LDA) assumes a common variance, $\\sigma_1^2 = \\sigma_2^2 = \\sigma^2$. For this part, we are also told to assume equal priors, $\\pi_1 = \\pi_2$.\nThe log-odds equation simplifies significantly. The term $\\ln(\\pi_1/\\pi_2) = \\ln(1) = 0$. The terms involving variance in the log-density ratio also cancel: $\\frac{1}{2}\\ln(\\sigma_2^2/\\sigma_1^2) = \\frac{1}{2}\\ln(1) = 0$. The log-odds equation becomes:\n$$\\text{log-odds} = - \\frac{(x-\\mu_1)^2}{2\\sigma^2} + \\frac{(x-\\mu_2)^2}{2\\sigma^2} = 0$$\nMultiplying by $2\\sigma^2$ (assuming $\\sigma^2 > 0$):\n$$-(x-\\mu_1)^2 + (x-\\mu_2)^2 = 0$$\n$$(x-\\mu_2)^2 = (x-\\mu_1)^2$$\nThis is a linear equation in $x$:\n$$x^2 - 2x\\mu_2 + \\mu_2^2 = x^2 - 2x\\mu_1 + \\mu_1^2$$\n$$-2x\\mu_2 + \\mu_2^2 = -2x\\mu_1 + \\mu_1^2$$\n$$2x\\mu_1 - 2x\\mu_2 = \\mu_1^2 - \\mu_2^2$$\n$$2x(\\mu_1 - \\mu_2) = (\\mu_1 - \\mu_2)(\\mu_1 + \\mu_2)$$\nAssuming $\\mu_1 \\neq \\mu_2$, we divide by $2(\\mu_1 - \\mu_2)$:\n$$x = \\frac{\\mu_1 + \\mu_2}{2}$$\nThe common variance $\\sigma^2$ indeed cancels. The threshold is simply the midpoint of the two means. Using the given means $\\mu_1 = 0$ and $\\mu_2 = 3$:\n$$x = \\frac{0 + 3}{2} = \\frac{3}{2}$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -1 - \\frac{2}{3}\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}  -1 + \\frac{2}{3}\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}  \\frac{3}{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While QDA's flexibility is powerful, when is it truly necessary? This exercise  explores a clever hypothetical scenario in two dimensions where two classes share the same mean and the same marginal variances, making them appear identical to simpler models. The only distinguishing feature is their internal correlation structure, providing a perfect illustration of a situation where QDA can identify a non-linear separating boundary while LDA, by its core assumption of a shared covariance, cannot.",
            "id": "3164346",
            "problem": "Consider a binary classification problem in $\\mathbb{R}^2$ with feature vector $\\mathbf{x} = (x_1,x_2)^\\top$ and class label $Y \\in \\{1,2\\}$. Suppose the class-conditional distributions are multivariate normal with equal priors, equal means, and equal marginal variances but different correlations:\n- $X \\mid Y=1 \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma}_1)$ with $\\boldsymbol{\\mu} = (0,0)^\\top$ and $\\mathbf{\\Sigma}_1 = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$,\n- $X \\mid Y=2 \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{\\Sigma}_2)$ with the same $\\boldsymbol{\\mu} = (0,0)^\\top$ and $\\mathbf{\\Sigma}_2 = \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}$,\nwhere $\\rho \\in (0,1)$ (for concreteness, you may think of $\\rho = 0.8$), and class priors $\\pi_1 = \\pi_2 = 1/2$. Thus, both classes have the same mean and the same marginal variances, and they differ only in correlation structure (the off-diagonal elements of the covariance matrices).\n\nUsing only first principles about Bayes-optimal classification under multivariate normal models, and the modeling assumptions underlying Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), determine which of the following statements are true.\n\nA. Under Quadratic Discriminant Analysis (QDA), the Bayes decision boundary between the two classes is the set of points satisfying $x_1 x_2 = 0$, which partitions the plane into the four quadrants; points with $x_1 x_2 > 0$ are assigned to class $1$, and those with $x_1 x_2  0$ to class $2$ (up to a null set on the axes).\n\nB. Under Linear Discriminant Analysis (LDA) with equal priors, the pooled covariance retains the off-diagonal information and yields a nontrivial linear separator along $x_1 = x_2$.\n\nC. Because the class means and marginal variances are equal, any LDA rule with equal priors assigns identical discriminant scores to all $\\mathbf{x}$ (up to ties), producing an expected error rate of $50\\%$.\n\nD. In this setup, the pooled covariance matrix used by LDA has off-diagonal entry equal to $0$, so LDA cannot exploit the difference in correlation between classes.\n\nE. The Bayes-optimal classifier (and thus QDA under correct model specification) achieves strictly less than $50\\%$ error, because each class concentrates probability mass in the quadrants consistent with the sign of its correlation.",
            "solution": "We start from the definition of the Bayes classifier: for two classes with class-conditional densities $p_k(\\mathbf{x})$ and priors $\\pi_k$, the Bayes rule assigns $\\mathbf{x}$ to the class with larger posterior probability, equivalently larger $\\pi_k p_k(\\mathbf{x})$. When $p_k$ are multivariate normal distributions and priors are equal, comparing $\\pi_k p_k(\\mathbf{x})$ reduces to comparing $p_k(\\mathbf{x})$ and hence to comparing their log-densities. Quadratic Discriminant Analysis (QDA) is the plug-in rule that uses class-specific covariance matrices, while Linear Discriminant Analysis (LDA) assumes a common covariance and uses the pooled estimate.\n\nBayes/QDA boundary for the given Gaussians. For $k \\in \\{1,2\\}$, with $\\boldsymbol{\\mu}_k = \\boldsymbol{\\mu} = \\mathbf{0}$ and $\\mathbf{\\Sigma}_k$ as given, the log-density is\n$\\log p_k(\\mathbf{x}) = -\\tfrac{1}{2}\\log |\\mathbf{\\Sigma}_k| - \\tfrac{1}{2} \\mathbf{x}^\\top \\mathbf{\\Sigma}_k^{-1} \\mathbf{x} + \\text{const}$.\nWith equal priors, the decision boundary solves $\\log p_1(\\mathbf{x}) = \\log p_2(\\mathbf{x})$, i.e.\n$-\\tfrac{1}{2}\\log |\\mathbf{\\Sigma}_1| - \\tfrac{1}{2} \\mathbf{x}^\\top \\mathbf{\\Sigma}_1^{-1} \\mathbf{x} = -\\tfrac{1}{2}\\log |\\mathbf{\\Sigma}_2| - \\tfrac{1}{2} \\mathbf{x}^\\top \\mathbf{\\Sigma}_2^{-1} \\mathbf{x}$,\nor equivalently\n$\\mathbf{x}^\\top (\\mathbf{\\Sigma}_2^{-1} - \\mathbf{\\Sigma}_1^{-1}) \\mathbf{x} = \\log |\\mathbf{\\Sigma}_2| - \\log |\\mathbf{\\Sigma}_1|$.\nHere $|\\mathbf{\\Sigma}_1| = 1 - \\rho^2 = |\\mathbf{\\Sigma}_2|$, hence $\\log |\\mathbf{\\Sigma}_2| - \\log |\\mathbf{\\Sigma}_1| = 0$, so the boundary simplifies to\n$\\mathbf{x}^\\top (\\mathbf{\\Sigma}_2^{-1} - \\mathbf{\\Sigma}_1^{-1}) \\mathbf{x} = 0$.\n\nWe compute $\\mathbf{\\Sigma}_k^{-1}$. For $\\mathbf{\\Sigma}(\\rho) = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$ with $|\\rho|1$, the inverse is\n$\\mathbf{\\Sigma}(\\rho)^{-1} = \\dfrac{1}{1 - \\rho^2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}$.\nThus\n$\\mathbf{\\Sigma}_1^{-1} = \\dfrac{1}{1 - \\rho^2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}, \\quad \\mathbf{\\Sigma}_2^{-1} = \\dfrac{1}{1 - \\rho^2} \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$,\nand hence\n$\\mathbf{\\Sigma}_2^{-1} - \\mathbf{\\Sigma}_1^{-1} = \\dfrac{1}{1 - \\rho^2} \\begin{pmatrix} 0  2\\rho \\\\ 2\\rho  0 \\end{pmatrix} = \\dfrac{2\\rho}{1 - \\rho^2} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$.\nTherefore,\n$\\mathbf{x}^\\top (\\mathbf{\\Sigma}_2^{-1} - \\mathbf{\\Sigma}_1^{-1}) \\mathbf{x} = \\dfrac{2\\rho}{1 - \\rho^2} \\, \\mathbf{x}^\\top \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} \\mathbf{x} = \\dfrac{4\\rho}{1 - \\rho^2} x_1 x_2$.\nThe constant factor $\\dfrac{4\\rho}{1 - \\rho^2} > 0$ for $\\rho \\in (0,1)$, so the boundary $\\mathbf{x}^\\top (\\mathbf{\\Sigma}_2^{-1} - \\mathbf{\\Sigma}_1^{-1}) \\mathbf{x} = 0$ is equivalent to $x_1 x_2 = 0$.\n\nThe decision rule for equal priors is to assign $\\mathbf{x}$ to class 1 if $\\log p_1(\\mathbf{x}) > \\log p_2(\\mathbf{x})$. Since the determinants $|\\mathbf{\\Sigma}_1|$ and $|\\mathbf{\\Sigma}_2|$ are equal, this inequality becomes $-\\frac{1}{2}\\mathbf{x}^\\top \\mathbf{\\Sigma}_1^{-1} \\mathbf{x} > -\\frac{1}{2}\\mathbf{x}^\\top \\mathbf{\\Sigma}_2^{-1} \\mathbf{x}$, which simplifies to $\\mathbf{x}^\\top (\\mathbf{\\Sigma}_2^{-1} - \\mathbf{\\Sigma}_1^{-1}) \\mathbf{x} > 0$. We computed that $\\mathbf{x}^\\top (\\mathbf{\\Sigma}_2^{-1} - \\mathbf{\\Sigma}_1^{-1}) \\mathbf{x}$ has the same sign as $x_1 x_2$. Therefore the Bayes/QDA rule is:\n- assign to class 1 (positive correlation $\\rho$) if $x_1 x_2 > 0$.\n- assign to class 2 (negative correlation $-\\rho$) if $x_1 x_2  0$.\nThis aligns with the intuition that for class 1, the components are likely to have the same sign, while for class 2, they are likely to have opposite signs.\n\nLDA under a common covariance. LDA assumes a common covariance $\\mathbf{\\Sigma}_p$, typically pooled across classes. With the two class covariances given, the pooled covariance is\n$\\mathbf{\\Sigma}_p = \\dfrac{1}{2}(\\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2) = \\dfrac{1}{2} \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} + \\dfrac{1}{2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\mathbf{I}$.\nThus, the off-diagonal entries cancel to $0$. The LDA discriminant score for class $k$ has the form (up to an additive constant common to both classes) $\\delta_k(\\mathbf{x}) = \\mathbf{x}^\\top \\mathbf{\\Sigma}_p^{-1} \\boldsymbol{\\mu}_k - \\tfrac{1}{2} \\boldsymbol{\\mu}_k^\\top \\mathbf{\\Sigma}_p^{-1} \\boldsymbol{\\mu}_k + \\log \\pi_k$. With $\\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 = \\mathbf{0}$ and equal priors, we obtain $\\delta_1(\\mathbf{x}) = \\delta_2(\\mathbf{x})$ for all $\\mathbf{x}$. Hence, LDA produces a tie everywhere and cannot discriminate; under any tie-breaking that does not use $\\mathbf{x}$, the expected error is $50\\%$ given equal priors.\n\nError comparison and intuition. The Bayes/QDA classifier separates by the sign of $x_1 x_2$, which aligns with the dependence structure: for a zero-mean bivariate normal with correlation $\\rho > 0$, the probability that $x_1$ and $x_2$ have the same sign is strictly greater than $1/2$; for $\\rho  0$, it is strictly less than $1/2$. A well-known identity for zero-mean bivariate normal gives $\\mathbb{P}(x_1 x_2 > 0) = \\tfrac{1}{2} + \\tfrac{1}{\\pi} \\arcsin(\\rho)$, which is strictly larger than $\\tfrac{1}{2}$ for $\\rho > 0$ and strictly smaller than $\\tfrac{1}{2}$ for $\\rho  0$. Therefore, the Bayes/QDA rule that assigns classes according to the sign of $x_1 x_2$ achieves strictly less than $50\\%$ error, while LDA is stuck at $50\\%$ for this construction.\n\nOption-by-option analysis:\n- Option A: The derivation shows the Bayes/QDA boundary is $x_1 x_2 = 0$. The regions are quadrants determined by the sign of $x_1 x_2$. The classification rule derived matches the statement. Verdict: Correct.\n- Option B: The pooled covariance is $\\mathbf{\\Sigma}_p = \\mathbf{I}$, which discards off-diagonal information. Therefore, LDA does not produce a nontrivial linear boundary such as $x_1 = x_2$; in fact, with equal means and priors, it produces a tie everywhere and no separation. Verdict: Incorrect.\n- Option C: With $\\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2 = \\mathbf{0}$, $\\mathbf{\\Sigma}_p = \\mathbf{I}$, and equal priors, $\\delta_1(\\mathbf{x}) = \\delta_2(\\mathbf{x})$ for all $\\mathbf{x}$, so LDA ties everywhere and yields an expected error of $50\\%$. Verdict: Correct.\n- Option D: We computed $\\mathbf{\\Sigma}_p = \\tfrac{1}{2}(\\mathbf{\\Sigma}_1 + \\mathbf{\\Sigma}_2) = \\mathbf{I}$, whose off-diagonal entry is $0$. Hence LDA cannot use the correlation difference. Verdict: Correct.\n- Option E: As argued, the Bayes/QDA rule aligns with the sign structure induced by correlation and achieves strictly less than $50\\%$ error for any $\\rho \\in (0,1)$. Verdict: Correct.",
            "answer": "$$\\boxed{ACDE}$$"
        },
        {
            "introduction": "The term \"quadratic decision boundary\" can sound abstract, but it corresponds to a rich family of geometric surfaces, including ellipsoids, paraboloids, and hyperboloids. This advanced practice  challenges you to move from two to three dimensions and actively engineer the covariance parameters to shape the QDA boundary into a specific form: a parabolic cylinder. This exercise provides tangible insight into the geometric complexity and expressive power of QDA in higher-dimensional spaces.",
            "id": "3164325",
            "problem": "Consider two classes in $\\mathbb{R}^{3}$ modeled by multivariate Gaussian distributions with equal priors $\\pi_{1}=\\pi_{2}=\\frac{1}{2}$. The class-conditional distributions are $\\mathcal{N}(\\boldsymbol{\\mu}_{k}, \\mathbf{\\Sigma}_{k})$ for $k \\in \\{1,2\\}$ with means $\\boldsymbol{\\mu}_{1}=\\begin{pmatrix}0 \\\\ 1 \\\\ 2\\end{pmatrix}$ and $\\boldsymbol{\\mu}_{2}=\\begin{pmatrix}1 \\\\ 0 \\\\ 2\\end{pmatrix}$, and covariance matrices\n$$\n\\mathbf{\\Sigma}_{1}=\\mathrm{diag}\\!\\big(2,\\,t,\\,3\\big), \\quad \\mathbf{\\Sigma}_{2}=\\mathrm{diag}\\!\\big(1,\\,4,\\,3\\big),\n$$\nwhere $t0$ is a scalar parameter. Quadratic Discriminant Analysis (QDA) and Linear Discriminant Analysis (LDA) are methods for classification based on Gaussian generative models. In QDA, the decision boundary between the two classes is the set of $\\mathbf{x}\\in\\mathbb{R}^{3}$ for which the posterior log-odds vanish, i.e., the equality of the two class log-posterior discriminants holds. Starting from the definition of the Gaussian class-conditional densities and equal priors, one can expand the equality of the two discriminants into a quadratic polynomial in $\\mathbf{x}$ of the form\n$$\n\\mathbf{x}^{\\top}\\mathbf{A}\\,\\mathbf{x}+\\mathbf{b}^{\\top}\\mathbf{x}+c=0,\n$$\nwith a symmetric matrix $\\mathbf{A}\\in\\mathbb{R}^{3\\times 3}$, a vector $\\mathbf{b}\\in\\mathbb{R}^{3}$, and a scalar $c\\in\\mathbb{R}$ that depend on $\\boldsymbol{\\mu}_{1},\\boldsymbol{\\mu}_{2},\\mathbf{\\Sigma}_{1},\\mathbf{\\Sigma}_{2}$ and $\\pi_{1},\\pi_{2}$.\n\nTasks:\n- Derive the quadratic decision equation by equating the two Gaussian discriminants (based on the densities and priors) and expand it into the form $\\mathbf{x}^{\\top}\\mathbf{A}\\,\\mathbf{x}+\\mathbf{b}^{\\top}\\mathbf{x}+c=0$, explicitly identifying $\\mathbf{A}$, $\\mathbf{b}$, and $c$ in terms of $\\boldsymbol{\\mu}_{1},\\boldsymbol{\\mu}_{2},\\mathbf{\\Sigma}_{1},\\mathbf{\\Sigma}_{2}$, and $\\pi_{1},\\pi_{2}$.\n- Determine the value of $t$ for which the QDA decision surface in $\\mathbb{R}^{3}$ is a parabolic cylinder, in the sense that it has only one nonzero principal quadratic coefficient, no quadratic dependence on two of the coordinates, and no dependence at all on one coordinate (so that the surface is extruded along that coordinate). Identify which entries of $\\mathbf{A}$ vanish to certify the surface type, and interpret geometrically which axis is the cylinder’s direction.\n- For that value of $t$, compute the coefficient multiplying $x_{1}^{2}$ in the unscaled polynomial obtained by expanding the difference of the two Gaussian discriminants into $\\mathbf{x}^{\\top}\\mathbf{A}\\,\\mathbf{x}+\\mathbf{b}^{\\top}\\mathbf{x}+c$.\n\nProvide as your final answer the single real number equal to that $x_{1}^{2}$ coefficient. No rounding is required. As a conceptual comparison, briefly state what happens to $\\mathbf{A}$ and to the decision surface in the LDA case (i.e., when $\\mathbf{\\Sigma}_{1}=\\mathbf{\\Sigma}_{2}$).",
            "solution": "We proceed from the generative model with Gaussian class-conditional densities. For $k\\in\\{1,2\\}$, the discriminant function (log-posterior up to an additive constant independent of $k$) for class $k$ is\n$$\ng_{k}(\\mathbf{x}) = -\\frac{1}{2}\\ln|\\mathbf{\\Sigma}_{k}| - \\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_{k})^{\\top}\\mathbf{\\Sigma}_{k}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_{k}) + \\ln \\pi_{k}.\n$$\nThe QDA decision boundary is the set of $\\mathbf{x}$ such that $g_{1}(\\mathbf{x})=g_{2}(\\mathbf{x})$. Consider the difference\n$$\nG(\\mathbf{x}) \\equiv g_{1}(\\mathbf{x}) - g_{2}(\\mathbf{x})=0.\n$$\nExpanding, we obtain\n\\begin{align*}\nG(\\mathbf{x})\n= -\\frac{1}{2}\\ln|\\mathbf{\\Sigma}_{1}| + \\frac{1}{2}\\ln|\\mathbf{\\Sigma}_{2}| \\\\\n\\quad -\\frac{1}{2}\\left[\\mathbf{x}^{\\top}(\\mathbf{\\Sigma}_{1}^{-1}-\\mathbf{\\Sigma}_{2}^{-1})\\mathbf{x} - 2(\\boldsymbol{\\mu}_{1}^{\\top}\\mathbf{\\Sigma}_{1}^{-1}-\\boldsymbol{\\mu}_{2}^{\\top}\\mathbf{\\Sigma}_{2}^{-1})\\mathbf{x} + \\left(\\boldsymbol{\\mu}_{1}^{\\top}\\mathbf{\\Sigma}_{1}^{-1}\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{2}^{\\top}\\mathbf{\\Sigma}_{2}^{-1}\\boldsymbol{\\mu}_{2}\\right)\\right] \\\\\n\\quad + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{2}}\\right).\n\\end{align*}\nCollecting terms into the canonical quadratic form $\\mathbf{x}^{\\top}\\mathbf{A}\\,\\mathbf{x}+\\mathbf{b}^{\\top}\\mathbf{x}+c$, we identify\n$$\n\\mathbf{A} = -\\frac{1}{2}\\left(\\mathbf{\\Sigma}_{1}^{-1}-\\mathbf{\\Sigma}_{2}^{-1}\\right) = \\frac{1}{2}\\left(\\mathbf{\\Sigma}_{2}^{-1}-\\mathbf{\\Sigma}_{1}^{-1}\\right),\n$$\n$$\n\\mathbf{b} = \\mathbf{\\Sigma}_{1}^{-1}\\boldsymbol{\\mu}_{1}-\\mathbf{\\Sigma}_{2}^{-1}\\boldsymbol{\\mu}_{2},\n$$\n$$\nc = -\\frac{1}{2}\\left(\\boldsymbol{\\mu}_{1}^{\\top}\\mathbf{\\Sigma}_{1}^{-1}\\boldsymbol{\\mu}_{1}-\\boldsymbol{\\mu}_{2}^{\\top}\\mathbf{\\Sigma}_{2}^{-1}\\boldsymbol{\\mu}_{2}\\right) - \\frac{1}{2}\\ln|\\mathbf{\\Sigma}_{1}| + \\frac{1}{2}\\ln|\\mathbf{\\Sigma}_{2}| + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{2}}\\right).\n$$\nBecause $\\pi_{1}=\\pi_{2}=\\frac{1}{2}$, the term $\\ln(\\pi_{1}/\\pi_{2})=0$.\n\nWe now specialize to the provided parameters:\n$$\n\\mathbf{\\Sigma}_{1}=\\mathrm{diag}(2,\\,t,\\,3), \\quad \\mathbf{\\Sigma}_{2}=\\mathrm{diag}(1,\\,4,\\,3).\n$$\nHence\n$$\n\\mathbf{\\Sigma}_{1}^{-1}=\\mathrm{diag}\\!\\left(\\frac{1}{2},\\,\\frac{1}{t},\\,\\frac{1}{3}\\right), \\quad \\mathbf{\\Sigma}_{2}^{-1}=\\mathrm{diag}\\!\\left(1,\\,\\frac{1}{4},\\,\\frac{1}{3}\\right).\n$$\nTherefore,\n$$\n\\mathbf{A}=\\frac{1}{2}\\left(\\mathbf{\\Sigma}_{2}^{-1}-\\mathbf{\\Sigma}_{1}^{-1}\\right)\n= \\frac{1}{2}\\,\\mathrm{diag}\\!\\left(1-\\frac{1}{2},\\,\\frac{1}{4}-\\frac{1}{t},\\,\\frac{1}{3}-\\frac{1}{3}\\right)\n= \\mathrm{diag}\\!\\left(\\frac{1}{4},\\,\\frac{1}{8}-\\frac{1}{2t},\\,0\\right).\n$$\nWe seek a parabolic cylinder in $\\mathbb{R}^{3}$. For a parabolic cylinder, the quadratic form must have exactly one nonzero principal quadratic coefficient and no quadratic dependence on two coordinates; moreover, the surface must be extruded along one coordinate, meaning the polynomial has no dependence on that coordinate at all (neither quadratic nor linear terms).\n\nFrom $\\mathbf{A}$ we see that the quadratic terms vanish in coordinates $x_{2}$ and $x_{3}$ if and only if\n$$\nA_{22}=\\frac{1}{8}-\\frac{1}{2t}=0 \\quad\\text{and}\\quad A_{33}=0.\n$$\nWe already have $A_{33}=0$ for any $t0$ because the third diagonal entries of $\\mathbf{\\Sigma}_{1}$ and $\\mathbf{\\Sigma}_{2}$ match. Solving $A_{22}=0$ yields\n$$\n\\frac{1}{8}=\\frac{1}{2t}\\quad\\Longrightarrow\\quad t=4.\n$$\nThus, at $t=4$, $\\mathbf{A}=\\mathrm{diag}\\!\\left(\\frac{1}{4},\\,0,\\,0\\right)$ has exactly one nonzero quadratic coefficient, namely in the $x_{1}$ direction.\n\nNext, we confirm the cylinder axis by checking linear dependence on $x_{3}$. The linear coefficient vector is\n$$\n\\mathbf{b}=\\mathbf{\\Sigma}_{1}^{-1}\\boldsymbol{\\mu}_{1}-\\mathbf{\\Sigma}_{2}^{-1}\\boldsymbol{\\mu}_{2}.\n$$\nFor $t=4$, we have\n$$\n\\mathbf{\\Sigma}_{1}^{-1}=\\mathrm{diag}\\!\\left(\\frac{1}{2},\\,\\frac{1}{4},\\,\\frac{1}{3}\\right), \\quad \\mathbf{\\Sigma}_{2}^{-1}=\\mathrm{diag}\\!\\left(1,\\,\\frac{1}{4},\\,\\frac{1}{3}\\right),\n$$\nand with $\\boldsymbol{\\mu}_{1}=\\begin{pmatrix}0 \\\\ 1 \\\\ 2\\end{pmatrix}$ and $\\boldsymbol{\\mu}_{2}=\\begin{pmatrix}1 \\\\ 0 \\\\ 2\\end{pmatrix}$ we obtain\n$$\n\\mathbf{\\Sigma}_{1}^{-1}\\boldsymbol{\\mu}_{1}=\\begin{pmatrix}0 \\\\ \\frac{1}{4} \\\\ \\frac{2}{3}\\end{pmatrix}, \\quad\n\\mathbf{\\Sigma}_{2}^{-1}\\boldsymbol{\\mu}_{2}=\\begin{pmatrix}1 \\\\ 0 \\\\ \\frac{2}{3}\\end{pmatrix},\n$$\nhence\n$$\n\\mathbf{b}=\\begin{pmatrix}-1 \\\\ \\frac{1}{4} \\\\ 0\\end{pmatrix}.\n$$\nTherefore, the polynomial $\\mathbf{x}^{\\top}\\mathbf{A}\\,\\mathbf{x}+\\mathbf{b}^{\\top}\\mathbf{x}+c$ has no dependence on $x_{3}$ at all: it has neither an $x_{3}^{2}$ term nor an $x_{3}$ term. Consequently, the decision surface is extruded along the $x_{3}$-axis, i.e., a parabolic cylinder with axis parallel to the $x_{3}$ direction. The nonvanishing entries of $\\mathbf{A}$ and $\\mathbf{b}$ certify the surface type: $A_{11}=\\frac{1}{4}\\neq 0$, $A_{22}=A_{33}=0$, and $b_{3}=0$.\n\nFinally, the requested coefficient multiplying $x_{1}^{2}$ in the unscaled polynomial $G(\\mathbf{x})=\\mathbf{x}^{\\top}\\mathbf{A}\\,\\mathbf{x}+\\mathbf{b}^{\\top}\\mathbf{x}+c$ (i.e., the specific expansion of $g_{1}(\\mathbf{x})-g_{2}(\\mathbf{x})$ without any further scaling) is exactly $A_{11}$, which at $t=4$ equals\n$$\n\\frac{1}{4}.\n$$\n\nConceptual comparison to Linear Discriminant Analysis (LDA): in LDA, one assumes $\\mathbf{\\Sigma}_{1}=\\mathbf{\\Sigma}_{2}$, which makes $\\mathbf{\\Sigma}_{2}^{-1}-\\mathbf{\\Sigma}_{1}^{-1}=\\mathbf{0}$ and therefore $\\mathbf{A}=\\mathbf{0}$. The decision surface then has no quadratic terms at all, reducing to an affine hyperplane $\\mathbf{b}^{\\top}\\mathbf{x}+c=0$.",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        }
    ]
}