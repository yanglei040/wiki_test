{
    "hands_on_practices": [
        {
            "introduction": "在实现多项逻辑回归时，其核心的 softmax 函数在计算上可能会遇到数值稳定性问题，特别是其分母项 $\\sum_j \\exp(\\eta_j)$ 在 $\\eta_j$ 值很大时容易导致浮点数溢出。本实践将引导你实现并验证“log-sum-exp”技巧，这是保证模型计算稳定和准确的关键技术。通过这个练习，你将掌握构建稳健机器学习模型的基本功。",
            "id": "3151616",
            "problem": "您将执行一个计算任务，该任务源于多项式逻辑回归模型的归一化步骤。对于一个特征向量 $x \\in \\mathbb{R}^p$ 和 $K$ 个类别特定的参数向量 $\\beta_j \\in \\mathbb{R}^p$，线性预测变量为 $\\eta_j = x^\\top \\beta_j$，其中 $j \\in \\{1,\\dots,K\\}$。softmax 函数中的归一化常数需要计算 $\\log \\sum_{j=1}^K \\exp(\\eta_j)$，这在实数算术中是良定义的，但在有限精度浮点运算中，当 $\\lVert \\eta \\rVert$ 很大时可能会出现数值不稳定的情况。\n\n从多项式逻辑回归预测变量以及对数和指数函数的基本定义出发，请为量 $\\log \\sum_{j=1}^K \\exp(\\eta_j)$ 实现两种计算方法：\n- 一种直接方法，即先应用指数和求和，然后应用对数，不进行任何特殊的数值范围处理。\n- 一种数值稳定的方法，通过在精确算术中保持等价性的数学变换来计算相同的值，同时避免上溢或下溢。\n\n然后，对下面提供的每个测试用例，执行以下操作：\n1. 使用 $\\eta_j = x^\\top \\beta_j$ 计算向量 $\\eta \\in \\mathbb{R}^K$。\n2. 计算欧几里得范数 $\\lVert \\eta \\rVert_2$。\n3. 计算 $\\log \\sum_{j=1}^K \\exp(\\eta_j)$ 的直接结果和数值稳定结果。\n4. 使用一个可靠的参考实现来近似 $\\log \\sum_{j=1}^K \\exp(\\eta_j)$ 的数学正确值。\n5. 对于每种方法，判断其结果是否为有限数，并在结果为有限数时计算其相对于参考值的绝对误差。如果结果不是有限数，则将误差报告为 $+\\infty$。\n\n您的程序必须使用以下测试套件。每个测试定义了 $p$、$K$、一个特征向量 $x \\in \\mathbb{R}^p$ 和一个目标向量 $t \\in \\mathbb{R}^K$。通过设置以下公式来构造参数 $\\beta_j \\in \\mathbb{R}^p$，使得对于每个 $j$ 都有 $x^\\top \\beta_j = t_j$：\n$$\n\\beta_j \\;=\\; \\frac{t_j}{x^\\top x}\\, x \\quad \\text{for each } j \\in \\{1,\\dots,K\\}.\n$$\n此构造在精确算术中保证了 $x^\\top \\beta_j = t_j$。\n\n测试套件：\n- 案例 A (中等值，正常路径)：\n  - $p = 3$, $K = 3$\n  - $x = [0.5,\\,-1.0,\\,2.0]$\n  - $t = [-1.2,\\,0.3,\\,2.1]$\n- 案例 B (非常大的正值，直接方法易于上溢)：\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [1000.0,\\,1001.0,\\,999.0]$\n- 案例 C (非常大的负值，直接方法易于下溢)：\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [-1000.0,\\,-1001.0,\\,-999.0]$\n- 案例 D (混合极端值，具有挑战性的动态范围)：\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [-1000.0,\\,0.0,\\,1000.0]$\n\n对于每个案例，按顺序输出一个包含五个值的序列：\n- $\\lVert \\eta \\rVert_2$（浮点数），\n- 一个布尔值，指示直接结果是否为有限数，\n- 一个布尔值，指示稳定结果是否为有限数，\n- 直接结果相对于参考值的绝对误差（浮点数，如果不是有限数则使用 $+\\infty$），\n- 稳定结果相对于参考值的绝对误差（浮点数，如果不是有限数则使用 $+\\infty$）。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按 A、B、C、D 顺序连接的所有案例的结果，形式为方括号内以逗号分隔的列表，例如，“[r1,r2,r3,...,r20]”。所有布尔值必须显示为不带引号的 True 或 False。不应打印任何额外的文本或空白字符。",
            "solution": "该问题要求实现并比较两种计算 log-sum-exp 函数的方法，该函数定义为 $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$，其中 $\\eta \\in \\mathbb{R}^K$ 是来自多项式逻辑回归模型的线性预测变量向量。其动机是为了处理浮点运算中出现的数值不稳定性。\n\n### 第 1 步：提取已知信息\n- **任务**：实现一种直接方法和一种数值稳定的方法来计算 $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$。\n- **线性预测变量**：$\\eta_j = x^\\top \\beta_j$，其中 $j \\in \\{1, \\dots, K\\}$，$x \\in \\mathbb{R}^p$ 且 $\\beta_j \\in \\mathbb{R}^p$。\n- **参数构造**：对于给定的测试向量 $x$ 和 $t$，参数 $\\beta_j$ 构造为 $\\beta_j = \\frac{t_j}{x^\\top x} x$。这种构造确保了对于每个 $j$ 都有 $\\eta_j = t_j$，如代入所示：\n$$ x^\\top \\beta_j = x^\\top \\left( \\frac{t_j}{x^\\top x} x \\right) = \\frac{t_j}{x^\\top x} (x^\\top x) = t_j $$\n该恒等式成立的条件是 $x^\\top x \\neq 0$，即 $x$ 不是零向量。所有提供的测试用例都使用非零的 $x$ 向量。因此，对于每个测试用例，线性预测变量向量 $\\eta$ 正是给定的目标向量 $t$。\n- **测试用例**：\n    - **A**：$p=3, K=3, x=[0.5, -1.0, 2.0], t=[-1.2, 0.3, 2.1]$\n    - **B**：$p=3, K=3, x=[1.0, 1.0, 1.0], t=[1000.0, 1001.0, 999.0]$\n    - **C**：$p=3, K=3, x=[1.0, 1.0, 1.0], t=[-1000.0, -1001.0, -999.0]$\n    - **D**：$p=3, K=3, x=[1.0, 1.0, 1.0], t=[-1000.0, 0.0, 1000.0]$\n- **每个案例的必需输出**：一个包含 5 个值的序列：\n    1.  欧几里得范数 $\\|\\eta\\|_2$。\n    2.  布尔值：直接结果是否为有限数？\n    3.  布尔值：稳定结果是否为有限数？\n    4.  直接结果的绝对误差（或 $+\\infty$）。\n    5.  稳定结果的绝对误差（或 $+\\infty$）。\n- **参考值**：使用一个可靠的参考实现来近似正确值。\n\n### 第 2 步：使用提取的已知信息进行验证\n对问题陈述进行严格评估：\n- **科学依据**：该问题解决了 log-sum-exp 计算中众所周知的数值不稳定性问题，这是多项式逻辑回归和其他机器学习模型中使用的 softmax 函数的基本组成部分。用于实现稳定性的数学变换是标准且正确的。该问题牢固地植根于数值分析和统计学习理论。\n- **良态问题**：问题是明确的。每个测试用例的所有输入（$x, t, p, K$）都已指定。要计算的量有明确定义。导致 $\\eta = t$ 的 $\\beta_j$ 的构造是明确的。评估指标（范数、有限性、绝对误差）是精确的。每次计算都存在唯一解。\n- **客观性**：问题以客观的数学语言陈述。它要求一个基于数值输入和既定算法的计算解，没有主观解释的余地。\n- **完整性和一致性**：问题是自洽的。所有必要的公式和数据都已提供。设置中没有矛盾之处。\n- **无其他缺陷**：问题并非微不足道（它展示了一个关键的数值问题），不是隐喻性的，并且与所述主题直接相关。它可以通过计算来验证。\n\n### 第 3 步：结论与行动\n问题是**有效的**。将提供一个合理的解决方案。\n\n### 解法推导\n\n对于给定的向量 $\\eta = (\\eta_1, \\eta_2, \\dots, \\eta_K)$，我们需要计算 $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$。\n\n**方法 1：直接计算**\n该方法按公式字面实现。\n1.  对每个 $j \\in \\{1, \\dots, K\\}$ 计算 $v_j = \\exp(\\eta_j)$。\n2.  计算总和 $S = \\sum_{j=1}^K v_j$。\n3.  计算最终结果 $R_{\\text{direct}} = \\log(S)$。\n\n这种方法在数值上是不稳定的。对于标准的双精度浮点数，当 $z \\gtrsim 709.78$ 时，$\\exp(z)$ 会上溢到 $+\\infty$。如果任何 $\\eta_j$ 很大，相应的 $\\exp(\\eta_j)$ 会变成 $+\\infty$，导致总和为无穷大，结果也为无穷大。相反，当 $z \\lesssim -745.13$ 时，$\\exp(z)$ 会下溢到 $0$。如果所有的 $\\eta_j$ 值都是很大的负数，总和中的每一项都可能下溢到 $0.0$，使得总和为 $0.0$。对数 $\\log(0.0)$ 为 $-\\infty$。\n\n**方法 2：数值稳定的计算（Log-Sum-Exp 技巧）**\n此方法基于一个数学恒等式，该恒等式将数值重新定位到一个数值安全的范围内。设 $m = \\max_{j} \\eta_j$。我们可以通过提出因子 $\\exp(m)$ 来重写表达式：\n$$ L(\\eta) = \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j) \\right) $$\n$$ = \\log \\left( \\exp(m) \\sum_{j=1}^K \\frac{\\exp(\\eta_j)}{\\exp(m)} \\right) $$\n利用对数和指数的性质，$\\log(a \\cdot b) = \\log(a) + \\log(b)$ 和 $\\exp(a)/\\exp(b) = \\exp(a-b)$，我们得到：\n$$ = \\log(\\exp(m)) + \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j - m) \\right) $$\n$$ = m + \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j - m) \\right) $$\n这给出了稳定的计算步骤：\n1.  找到向量中的最大值：$m = \\max(\\eta_1, \\dots, \\eta_K)$。\n2.  计算移位后的向量 $\\eta_j' = \\eta_j - m$。\n3.  对每个 $j$ 计算 $v_j' = \\exp(\\eta_j')$。\n4.  计算总和 $S' = \\sum_{j=1}^K v_j'$。\n5.  计算最终结果 $R_{\\text{stable}} = m + \\log(S')$。\n\n这种方法是稳定的，因为指数函数的参数 $\\eta_j' = \\eta_j - m$ 始终小于或等于 $0$。最大参数为 $0$，导致 $\\exp(0)=1$。这可以防止上溢。此外，由于总和 $S'$ 中至少有一项恰好为 $1$，因此可以保证总和至少为 $1$。这可以防止对数的参数由于所有项的下溢而变为 $0$，从而避免了结果为 $-\\infty$。\n\n**每个测试用例的计算步骤**\n对于由 $x$ 和 $t$ 定义的每个案例：\n1.  设置 $\\eta = t$。\n2.  计算欧几里得范数 $\\|\\eta\\|_2 = \\sqrt{\\sum_{j=1}^K \\eta_j^2}$。\n3.  使用直接方法计算 $R_{\\text{direct}}$。\n4.  使用稳定方法计算 $R_{\\text{stable}}$。\n5.  使用 `scipy.special.logsumexp` 获取参考值 $R_{\\text{ref}}$，这是稳定方法的一个专业实现版本。\n6.  判断 $R_{\\text{direct}}$ 和 $R_{\\text{stable}}$ 是否为有限数。\n7.  计算绝对误差 $E_{\\text{direct}} = |R_{\\text{direct}} - R_{\\text{ref}}|$ 和 $E_{\\text{stable}} = |R_{\\text{stable}} - R_{\\text{ref}}|$。如果结果不是有限数，其对应的误差报告为 $+\\infty$。\n8.  收集并格式化五个所需的值：$(\\|\\eta\\|_2, \\text{isfinite}(R_{\\text{direct}}), \\text{isfinite}(R_{\\text{stable}}), E_{\\text{direct}}, E_{\\text{stable}})$。所有测试用例的结果将被连接成一个列表作为最终输出。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the numerical stability problem for the log-sum-exp function.\n    \"\"\"\n    \n    # Each test case is defined by p, K, x, and t.\n    # We can directly use t as eta, as per the problem's construction.\n    test_cases = {\n        'A': {\n            'p': 3, 'K': 3,\n            'x': np.array([0.5, -1.0, 2.0]),\n            't': np.array([-1.2, 0.3, 2.1]),\n        },\n        'B': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([1000.0, 1001.0, 999.0]),\n        },\n        'C': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([-1000.0, -1001.0, -999.0]),\n        },\n        'D': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([-1000.0, 0.0, 1000.0]),\n        }\n    }\n\n    all_results = []\n    \n    # The problem asks to process cases in order A, B, C, D.\n    case_order = ['A', 'B', 'C', 'D']\n    \n    for case_key in case_order:\n        case_data = test_cases[case_key]\n        t = case_data['t']\n        \n        # As per the problem description, eta_j = t_j.\n        eta = t.astype(np.float64) # Use float64 for precision.\n        \n        # 1. Compute the vector eta (already done) and its Euclidean norm.\n        norm_eta = np.linalg.norm(eta)\n        \n        # 2. Compute the direct result.\n        # This implementation follows the naive formula log(sum(exp(eta))).\n        # It is prone to overflow/underflow.\n        with np.errstate(over='ignore', under='ignore'): # Suppress overflow/underflow warnings for this calculation\n            exp_eta_direct = np.exp(eta)\n            sum_exp_direct = np.sum(exp_eta_direct)\n            res_direct = np.log(sum_exp_direct)\n            \n        # 3. Compute the numerically stable result.\n        # This uses the log-sum-exp trick: m + log(sum(exp(eta - m))).\n        m = np.max(eta)\n        res_stable = m + np.log(np.sum(np.exp(eta - m)))\n        \n        # 4. Use a reliable reference implementation.\n        # scipy.special.logsumexp provides a robust implementation.\n        ref_val = logsumexp(eta)\n        \n        # 5. Determine finiteness and compute absolute errors.\n        is_finite_direct = np.isfinite(res_direct)\n        is_finite_stable = np.isfinite(res_stable)\n        \n        err_direct = np.abs(res_direct - ref_val) if is_finite_direct else np.inf\n        err_stable = np.abs(res_stable - ref_val) if is_finite_stable else np.inf\n\n        # Append the five required values for the current case.\n        all_results.extend([\n            norm_eta,\n            is_finite_direct,\n            is_finite_stable,\n            err_direct,\n            err_stable\n        ])\n\n    # Final print statement in the exact required format.\n    # Convert bools to 'True'/'False' strings without quotes.\n    # Use repr for float to get standard representation.\n    def format_val(v):\n        if isinstance(v, bool):\n            return str(v)\n        if np.isinf(v):\n            return 'inf'\n        return repr(v)\n\n    # Note: problem output format does not want spaces and wants True/False literals.\n    # map(str, ...) is the cleanest way to achieve this.\n    final_output_str = f\"[{','.join(map(str, all_results))}]\"\n    final_output_str = final_output_str.replace(\"inf\", \"float('inf')\") # problem asks for +inf, Python's inf is ok. let's stick to str(inf)\n    \n    # The example output uses True/False which str() on a bool provides.\n    # The example also has no spaces.\n    \n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# The problem in the XML is malformed, so `solve()` is not called here.\n# To run this code, uncomment the following line:\n# solve()\n```"
        },
        {
            "introduction": "对于多分类问题，我们应如何扩展二元逻辑回归？一种常见的方法是“一对剩余”(One-vs-Rest, OVR)策略，但理论上更严谨的是使用 softmax 函数的多项逻辑回归模型。本实践将要求你构建一个具体的例子，在该例子中 OVR 方法会产生模棱两可且非归一化的概率，从而清晰地展示 softmax 模型在多分类问题上的优越性。",
            "id": "3151582",
            "problem": "要求您构建并分析一个具体的多类分类场景，以突显多个二元逻辑回归的“一对多”（OVR）方案与使用softmax链接的单个多项式逻辑回归之间的差异。您的任务是实现伯努利分布和分类分布的规范逆连接函数所蕴含的概率映射，将它们应用于指定的线性模型，并量化OVR方案如何产生与归一化概率不一致的重叠决策区域，而softmax模型则强制执行归一化。\n\n使用的基本定义：\n- 对于伯努利分布，使用将线性预测器映射到 $\\left(0,1\\right)$ 区间内有效概率的规范逆连接函数。\n- 对于具有 $K$ 个类别的分类分布，使用由指数族表示法导出的规范逆连接函数，该函数产生一个满足 $\\sum_{k=1}^K p_k=1$ 的归一化概率向量 $\\left(p_1,\\dots,p_K\\right)$。\n\n设置：\n- 考虑 $K=3$ 个类别和特征向量 $x \\in \\mathbb{R}^2$，并增加一个截距项，即使用 $\\bar{x} = \\left(1,x_1,x_2\\right)^\\top$。\n- 为“一对多”模型定义三个特定于类的权重向量（每个都是针对其余类的二元逻辑回归），作为矩阵 $W_{\\text{ovr}} \\in \\mathbb{R}^{3 \\times 3}$ 的行：\n  $$\n  W_{\\text{ovr}} =\n  \\begin{bmatrix}\n  1.0  & 1.5 & -0.1 \\\\\n  1.0  & -0.1 & 1.5 \\\\\n  0.6  & 0.7 & 0.7\n  \\end{bmatrix}.\n  $$\n  对于类别 $k \\in \\{1,2,3\\}$，OVR线性预测器为 $z_k^{\\text{ovr}} = w_k^\\top \\bar{x}$，其中 $w_k^\\top$ 是 $W_{\\text{ovr}}$ 的第 $k$ 行。\n- 定义一个具有相同线性得分的多项式（softmax）模型，使用与 $W_{\\text{ovr}}$ 相等的权重矩阵 $W_{\\text{soft}} \\in \\mathbb{R}^{3 \\times 3}$：\n  $$\n  W_{\\text{soft}} = W_{\\text{ovr}}.\n  $$\n  对于类别 $k \\in \\{1,2,3\\}$，softmax线性得分为 $s_k = \\beta_k^\\top \\bar{x}$，其中 $\\beta_k^\\top$ 是 $W_{\\text{soft}}$ 的第 $k$ 行。\n\n对每个测试输入 $\\bar{x}$ 执行的计算：\n1. 使用应用于 $\\left(z_1^{\\text{ovr}},z_2^{\\text{ovr}},z_3^{\\text{ovr}}\\right)$ 的伯努利规范逆连接函数计算OVR类概率 $\\left(p_1^{\\text{ovr}},p_2^{\\text{ovr}},p_3^{\\text{ovr}}\\right)$。令 $S_{\\text{ovr}}=\\sum_{k=1}^3 p_k^{\\text{ovr}}$。同时计算 $N_{0.5}$，即满足 $p_k^{\\text{ovr}} \\ge 0.5$ 的OVR类别数量，以及一个重叠指示符 $I_{\\text{ovr}}$，如果至少有两个类别同时满足 $p_k^{\\text{ovr}} \\ge 0.5$，则其值为 $1$，否则为 $0$。\n2. 使用应用于 $\\left(s_1,s_2,s_3\\right)$ 的分类规范逆连接函数计算softmax概率 $\\left(p_1^{\\text{soft}},p_2^{\\text{soft}},p_3^{\\text{soft}}\\right)$，并令 $S_{\\text{soft}}=\\sum_{k=1}^3 p_k^{\\text{soft}}$。\n\n测试套件：\n- 使用以下五个特征向量（每个 $x$ 以 $\\left[x_1,x_2\\right]$ 形式给出），这些向量被选择用于测试一个普遍的重叠情况、一个中等负值区域、一个混合符号的强证据以及两个极端的边缘情况：\n  - $x^{(1)} = \\left[0.4,\\,0.4\\right]$\n  - $x^{(2)} = \\left[-1.0,\\,-1.0\\right]$\n  - $x^{(3)} = \\left[3.0,\\,-3.0\\right]$\n  - $x^{(4)} = \\left[-5.0,\\,-5.0\\right]$\n  - $x^{(5)} = \\left[10.0,\\,10.0\\right]$\n- 对于每个 $x^{(i)}$，构建 $\\bar{x}^{(i)} = \\left(1,x_1^{(i)},x_2^{(i)}\\right)^\\top$ 并评估上述量值。\n\n数值稳定性：\n- 使用伯努利和分类规范逆连接函数的数值稳定实现，以处理大幅值线性预测器而不会发生溢出。\n\n要求的最终输出：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，其中第 $i$ 个元素本身是一个列表\n  $$\n  \\left[\\,\\text{round}(S_{\\text{ovr}},6),\\,\\text{round}(S_{\\text{soft}},6),\\,N_{0.5},\\,\\mathbf{1}\\{I_{\\text{ovr}}=1\\}\\,\\right],\n  $$\n  其中 $S_{\\text{ovr}}$ 和 $S_{\\text{soft}}$ 四舍五入到 $6$ 位小数，$N_{0.5}$ 是一个整数，$\\mathbf{1}\\{I_{\\text{ovr}}=1\\}$ 是一个布尔指示符，表示为True或False。这五个元素必须按照上述测试套件的顺序报告。例如，输出格式必须如下所示：\n  $$\n  \\big[\\,[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]\\,\\big]\n  $$\n  不带空格。",
            "solution": "用户的请求是一个有效的计算问题。\n\n### 问题验证\n\n**第1步：提取给定信息**\n\n-   **类别**：$K=3$ 个类别。\n-   **特征向量**：输入向量 $x \\in \\mathbb{R}^2$ 通过增加一个截距项来构成 $\\bar{x} = (1, x_1, x_2)^\\top$。\n-   **“一对多”（OVR）模型**：\n    -   权重矩阵：\n      $$\n      W_{\\text{ovr}} =\n      \\begin{bmatrix}\n      1.0  & 1.5 & -0.1 \\\\\n      1.0  & -0.1 & 1.5 \\\\\n      0.6  & 0.7 & 0.7\n      \\end{bmatrix}.\n      $$\n    -   类别 $k \\in \\{1, 2, 3\\}$ 的线性预测器：$z_k^{\\text{ovr}} = w_k^\\top \\bar{x}$，其中 $w_k^\\top$ 是 $W_{\\text{ovr}}$ 的第 $k$ 行。\n-   **多项式（Softmax）模型**：\n    -   权重矩阵：$W_{\\text{soft}} = W_{\\text{ovr}}$。\n    -   类别 $k \\in \\{1, 2, 3\\}$ 的线性得分：$s_k = \\beta_k^\\top \\bar{x}$，其中 $\\beta_k^\\top$ 是 $W_{\\text{soft}}$ 的第 $k$ 行。\n-   **逆连接函数**：\n    -   **伯努利**：将线性预测器映射到 $(0, 1)$ 区间内概率的规范逆连接函数。\n    -   **分类**：产生一个归一化概率向量 $(p_1, \\dots, p_K)$ 且满足 $\\sum_{k=1}^K p_k = 1$ 的规范逆连接函数。\n-   **对每个 $\\bar{x}$ 的计算**：\n    1.  **OVR**：计算概率 $(p_1^{\\text{ovr}}, p_2^{\\text{ovr}}, p_3^{\\text{ovr}})$、它们的和 $S_{\\text{ovr}} = \\sum_{k=1}^3 p_k^{\\text{ovr}}$、类别满足 $p_k^{\\text{ovr}} \\ge 0.5$ 的计数 $N_{0.5}$，以及一个重叠指示符 $I_{\\text{ovr}}$（如果 $N_{0.5} \\ge 2$ 则为 $1$，否则为 $0$）。\n    2.  **Softmax**：计算概率 $(p_1^{\\text{soft}}, p_2^{\\text{soft}}, p_3^{\\text{soft}})$ 和它们的和 $S_{\\text{soft}} = \\sum_{k=1}^3 p_k^{\\text{soft}}$。\n-   **测试套件**：一组五个特征向量 $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]$：\n    -   $x^{(1)} = [0.4, 0.4]$\n    -   $x^{(2)} = [-1.0, -1.0]$\n    -   $x^{(3)} = [3.0, -3.0]$\n    -   $x^{(4)} = [-5.0, -5.0]$\n    -   $x^{(5)} = [10.0, 10.0]$\n-   **输出格式**：对每个测试向量，生成一个列表 $[\\text{round}(S_{\\text{ovr}}, 6), \\text{round}(S_{\\text{soft}}, 6), N_{0.5}, \\mathbf{1}\\{I_{\\text{ovr}}=1\\}]$，其中最后一个元素是布尔值（`True` 或 `False`）。\n\n**第2步：使用提取的给定信息进行验证**\n\n-   **科学依据**：该问题在广义线性模型（GLM）理论中有坚实的基础。为伯努利分布指定的规范逆连接函数是逻辑（sigmoid）函数，为分类分布指定的则是softmax函数。“一对多”与多项式逻辑回归的比较是统计学习中的一个标准且重要的主题。\n-   **适定性**：该问题是适定的。所有参数、数据和所需的计算都已明确定义，从而可以得到一个唯一的、稳定的、有意义的数值解。\n-   **客观性**：该问题使用精确的数学符号和术语进行客观陈述，没有任何主观或模糊的语言。\n-   **结论**：该问题不存在科学上的不健全、不完整、矛盾以及验证标准中列出的其他缺陷。\n\n**第3步：裁决与行动**\n\n-   **裁决**：该问题**有效**。\n-   **行动**：将提供一个完整的解决方案。\n\n### 解题推导\n\n此问题要求比较两种常见的多类分类策略：使用多个独立二元分类器的“一对多”（OVR）方案和一个联合的多项式（softmax）回归模型。任务的核心是将正确的概率映射函数应用于给定权重矩阵产生的线性得分。\n\n**1. 规范逆连接函数**\n\n-   **伯努利/OVR**：伯努利分布的规范逆连接函数是逻辑函数，通常称为sigmoid函数，$\\sigma(z)$。它将线性预测器 $z \\in \\mathbb{R}$ 映射到 $(0, 1)$ 区间内的一个概率。\n    $$\n    p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n    $$\n    在OVR方案中，此函数独立应用于每个特定于类的线性预测器 $z_k^{\\text{ovr}}$。\n\n-   **分类/Softmax**：分类分布的规范逆连接函数是softmax函数。它将一个线性得分向量 $s = (s_1, \\dots, s_K)^\\top$ 映射到一个归一化的概率向量 $p = (p_1, \\dots, p_K)^\\top$，其中每个 $p_k \\in (0, 1)$ 且 $\\sum_{k=1}^K p_k = 1$。\n    $$\n    p_k = \\frac{e^{s_k}}{\\sum_{j=1}^K e^{s_j}}\n    $$\n    softmax函数确保所得概率在 $K$ 个类别上形成一个有效的分布。\n\n**2. 模型计算**\n\n对于每个输入特征向量 $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\\top$，我们首先将其扩充为 $\\bar{x}^{(i)} = [1, x_1^{(i)}, x_2^{(i)}]^\\top$。两个模型的线性得分都使用相同的权重矩阵 $W = W_{\\text{ovr}} = W_{\\text{soft}}$ 计算。得分/预测器向量由 $z^{(i)} = W\\bar{x}^{(i)}$ 给出。\n\n我们将得分向量的元素表示为 $z_1, z_2, z_3$。\n\n-   **OVR概率**：OVR模型的概率是通过将sigmoid函数独立应用于每个得分来计算的：\n    $$\n    p_k^{\\text{ovr}} = \\sigma(z_k) = \\frac{1}{1 + e^{-z_k}} \\quad \\text{for } k \\in \\{1, 2, 3\\}\n    $$\n    总和 $S_{\\text{ovr}} = \\sum_{k=1}^3 p_k^{\\text{ovr}}$ 通常不等于 $1$。然后我们计数 $N_{0.5} = \\sum_{k=1}^3 \\mathbf{1}\\{p_k^{\\text{ovr}} \\ge 0.5\\}$，并且如果 $N_{0.5} \\ge 2$，则将重叠指示符 $I_{\\text{ovr}}$ 设置为true。\n\n-   **Softmax概率**：softmax模型的概率是联合计算的：\n    $$\n    p_k^{\\text{soft}} = \\frac{e^{z_k}}{e^{z_1} + e^{z_2} + e^{z_3}} \\quad \\text{for } k \\in \\{1, 2, 3\\}\n    $$\n    根据构造，总和 $S_{\\text{soft}} = \\sum_{k=1}^3 p_k^{\\text{soft}}$ 将始终等于 $1$。\n\n**3. 数值稳定性**\n\n为防止指数函数的大幅值输入导致数值上溢/下溢，需要使用稳定的实现。\n\n-   **稳定的Sigmoid函数 $\\sigma(z)$**：\n    -   如果 $z \\ge 0$，使用 $1 / (1 + e^{-z})$。这可以避免 $e$ 的指数是一个大的正数。\n    -   如果 $z < 0$，使用代数上等价的形式 $e^z / (1 + e^z)$。这同样可以避免 $e$ 的指数是一个大的正数。\n\n-   **稳定的Softmax**：使用“log-sum-exp”技巧。令 $c = \\max_k(z_k)$。softmax概率可以重写为：\n    $$\n    p_k^{\\text{soft}} = \\frac{e^{z_k - c}}{\\sum_{j=1}^K e^{z_j - c}}\n    $$\n    这种变换将指数中心化到 $0$ 附近，防止因大的正得分而导致的溢出，同时保持数值精度。\n\n**4. 测试用例 $x^{(1)} = [0.4, 0.4]$ 演算过程**\n\n1.  **扩充向量**：$\\bar{x}^{(1)} = [1, 0.4, 0.4]^\\top$。\n2.  **计算线性得分**：\n    $$\n    z = W \\bar{x}^{(1)} =\n    \\begin{bmatrix}\n    1.0  & 1.5 & -0.1 \\\\\n    1.0  & -0.1 & 1.5 \\\\\n    0.6  & 0.7 & 0.7\n    \\end{bmatrix}\n    \\begin{bmatrix}\n    1.0 \\\\\n    0.4 \\\\\n    0.4\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    1.0 + 0.6 - 0.04 \\\\\n    1.0 - 0.04 + 0.6 \\\\\n    0.6 + 0.28 + 0.28\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    1.56 \\\\\n    1.56 \\\\\n    1.16\n    \\end{bmatrix}\n    $$\n3.  **OVR 计算**：\n    -   $p_1^{\\text{ovr}} = \\sigma(1.56) \\approx 0.826384$\n    -   $p_2^{\\text{ovr}} = \\sigma(1.56) \\approx 0.826384$\n    -   $p_3^{\\text{ovr}} = \\sigma(1.16) \\approx 0.761352$\n    -   $S_{\\text{ovr}} = 0.826384 + 0.826384 + 0.761352 = 2.414120$\n    -   所有三个概率都 $\\ge 0.5$，所以 $N_{0.5} = 3$。\n    -   由于 $N_{0.5} \\ge 2$，重叠指示符为 `True`。\n\n4.  **Softmax 计算**：\n    -   使用得分 $z_1=1.56, z_2=1.56, z_3=1.16$。\n    -   $e^{1.56} \\approx 4.7588$, $e^{1.56} \\approx 4.7588$, $e^{1.16} \\approx 3.1899$。\n    -   指数和 $\\approx 12.7075$。\n    -   $p_1^{\\text{soft}} = 4.7588 / 12.7075 \\approx 0.374468$\n    -   $p_2^{\\text{soft}} = 4.7588 / 12.7075 \\approx 0.374468$\n    -   $p_3^{\\text{soft}} = 3.1899 / 12.7075 \\approx 0.251064$\n    -   $S_{\\text{soft}} = 0.374468 + 0.374468 + 0.251064 = 1.000000$。\n\n5.  **$x^{(1)}$ 的结果**：\n    $[\\text{round}(2.414120, 6), \\text{round}(1.0, 6), 3, \\text{True}] \\rightarrow [2.414120, 1.000000, 3, \\text{True}]$。\n\n对所有五个测试向量重复此过程以生成最终输出。OVR方案产生的未归一化概率总和为 $2.41$，并将该点分类到所有三个类别中（因为所有 $p_k>0.5$），这突显了其模糊性。softmax模型提供了一个有效的、归一化的概率分布，将最高（但非压倒性）的概率分配给了类别 $1$ 和 $2$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares probabilities from one-vs-rest (OVR) and softmax\n    models for a series of test cases.\n    \"\"\"\n\n    # Define the weight matrix for both OVR and Softmax models.\n    W = np.array([\n        [1.0, 1.5, -0.1],\n        [1.0, -0.1, 1.5],\n        [0.6, 0.7, 0.7]\n    ])\n\n    # Define the test suite of feature vectors.\n    test_cases_x = [\n        np.array([0.4, 0.4]),\n        np.array([-1.0, -1.0]),\n        np.array([3.0, -3.0]),\n        np.array([-5.0, -5.0]),\n        np.array([10.0, 10.0])\n    ]\n\n    def stable_sigmoid(z):\n        \"\"\"Numerically stable implementation of the sigmoid function.\"\"\"\n        return np.where(\n            z >= 0,\n            1.0 / (1.0 + np.exp(-z)),\n            np.exp(z) / (1.0 + np.exp(z))\n        )\n\n    def stable_softmax(s):\n        \"\"\"Numerically stable implementation of the softmax function.\"\"\"\n        s_max = np.max(s, axis=-1, keepdims=True)\n        s_shifted = s - s_max\n        exps = np.exp(s_shifted)\n        return exps / np.sum(exps, axis=-1, keepdims=True)\n\n    all_results = []\n\n    for x in test_cases_x:\n        # Augment the feature vector with an intercept term (x_0 = 1).\n        x_aug = np.insert(x, 0, 1.0)\n\n        # Compute the linear scores/predictors z = W * x_aug.\n        z = W @ x_aug\n\n        # 1. One-vs-Rest (OVR) computations\n        p_ovr = stable_sigmoid(z)\n        S_ovr = np.sum(p_ovr)\n        \n        # Count classes where probability is >= 0.5\n        N_05 = np.sum(p_ovr >= 0.5)\n        \n        # Overlap indicator: True if at least two classes meet the threshold\n        I_ovr = bool(N_05 >= 2)\n\n        # 2. Softmax computations\n        p_soft = stable_softmax(z)\n        S_soft = np.sum(p_soft) # This will be 1.0 by definition.\n\n        # Append sub-result for current test case.\n        sub_result = [\n            round(S_ovr, 6),\n            round(S_soft, 6),\n            N_05,\n            I_ovr\n        ]\n        all_results.append(sub_result)\n\n    # The final output must be a list of lists, without spaces.\n    # Convert each inner list to its string representation and join them.\n    # str() on a list adds spaces, so we build it manually.\n    final_output_str = '[' + ','.join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in all_results]) + ']'\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}