{
    "hands_on_practices": [
        {
            "introduction": "When faced with a multiclass classification problem, a common first thought is to train several independent binary classifiers in a one-vs-rest (OVR) scheme. This exercise  challenges that intuition by constructing a scenario where the OVR approach yields uncalibrated and inconsistent probabilities that do not sum to one. By comparing these results to those from a true multinomial logistic regression using the softmax function, you will gain a concrete understanding of why the softmax model provides a more coherent and principled framework for multiclass prediction.",
            "id": "3151582",
            "problem": "You are asked to construct and analyze a concrete multiclass classification scenario that highlights the difference between a one-vs-rest (OVR) scheme of multiple binary logistic regressions and a single multinomial logistic regression with the softmax link. Your task is to implement the probability mappings implied by the canonical inverse link functions of the Bernoulli and categorical distributions, apply them to specified linear models, and quantify how the OVR scheme can yield overlapping decision regions that are inconsistent with normalized probabilities, in contrast to the softmax model that enforces normalization.\n\nBase definitions to use:\n- For a Bernoulli distribution, use the canonical inverse link that maps a linear predictor to a valid probability in $\\left(0,1\\right)$.\n- For a categorical distribution with $K$ classes, use the canonical inverse link induced by the exponential family representation that yields a normalized probability vector $\\left(p_1,\\dots,p_K\\right)$ satisfying $\\sum_{k=1}^K p_k=1$.\n\nSetup:\n- Consider $K=3$ classes and feature vectors $x \\in \\mathbb{R}^2$ augmented with an intercept, i.e., use $\\bar{x} = \\left(1,x_1,x_2\\right)^\\top$.\n- Define three class-specific weight vectors for the one-vs-rest models (each a binary logistic regression against the rest) as rows of a matrix $W_{\\text{ovr}} \\in \\mathbb{R}^{3 \\times 3}$:\n  $$\n  W_{\\text{ovr}} =\n  \\begin{bmatrix}\n  1.0 & 1.5 & -0.1 \\\\\n  1.0 & -0.1 & 1.5 \\\\\n  0.6 & 0.7 & 0.7\n  \\end{bmatrix}.\n  $$\n  For class $k \\in \\{1,2,3\\}$, the OVR linear predictor is $z_k^{\\text{ovr}} = w_k^\\top \\bar{x}$, where $w_k^\\top$ is the $k$-th row of $W_{\\text{ovr}}$.\n- Define a multinomial (softmax) model with the same linear scores, using a weight matrix $W_{\\text{soft}} \\in \\mathbb{R}^{3 \\times 3}$ equal to $W_{\\text{ovr}}$:\n  $$\n  W_{\\text{soft}} = W_{\\text{ovr}}.\n  $$\n  For class $k \\in \\{1,2,3\\}$, the softmax linear score is $s_k = \\beta_k^\\top \\bar{x}$, where $\\beta_k^\\top$ is the $k$-th row of $W_{\\text{soft}}$.\n\nComputations to perform for each test input $\\bar{x}$:\n1. Compute the OVR class probabilities $\\left(p_1^{\\text{ovr}},p_2^{\\text{ovr}},p_3^{\\text{ovr}}\\right)$ using the Bernoulli canonical inverse link applied to $\\left(z_1^{\\text{ovr}},z_2^{\\text{ovr}},z_3^{\\text{ovr}}\\right)$. Let $S_{\\text{ovr}}=\\sum_{k=1}^3 p_k^{\\text{ovr}}$. Also compute $N_{0.5}$, the number of OVR classes with $p_k^{\\text{ovr}} \\ge 0.5$, and an overlap indicator $I_{\\text{ovr}}$ that is $1$ if at least two classes simultaneously satisfy $p_k^{\\text{ovr}} \\ge 0.5$ and $0$ otherwise.\n2. Compute the softmax probabilities $\\left(p_1^{\\text{soft}},p_2^{\\text{soft}},p_3^{\\text{soft}}\\right)$ using the categorical canonical inverse link applied to $\\left(s_1,s_2,s_3\\right)$, and let $S_{\\text{soft}}=\\sum_{k=1}^3 p_k^{\\text{soft}}$.\n\nTest suite:\n- Use the following five feature vectors (each $x$ given as $\\left[x_1,x_2\\right]$), which are chosen to test a general overlap case, moderate negative region, mixed-sign strong evidence, and two extreme edge cases:\n  - $x^{(1)} = \\left[0.4,\\,0.4\\right]$\n  - $x^{(2)} = \\left[-1.0,\\,-1.0\\right]$\n  - $x^{(3)} = \\left[3.0,\\,-3.0\\right]$\n  - $x^{(4)} = \\left[-5.0,\\,-5.0\\right]$\n  - $x^{(5)} = \\left[10.0,\\,10.0\\right]$\n- For each $x^{(i)}$, construct $\\bar{x}^{(i)} = \\left(1,x_1^{(i)},x_2^{(i)}\\right)^\\top$ and evaluate the quantities described above.\n\nNumerical stability:\n- Use numerically stable implementations of the Bernoulli and categorical canonical inverse links to handle large-magnitude linear predictors without overflow.\n\nRequired final output:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the $i$-th element is itself a list\n  $$\n  \\left[\\,\\text{round}(S_{\\text{ovr}},6),\\,\\text{round}(S_{\\text{soft}},6),\\,N_{0.5},\\,\\mathbf{1}\\{I_{\\text{ovr}}=1\\}\\,\\right],\n  $$\n  with $S_{\\text{ovr}}$ and $S_{\\text{soft}}$ rounded to $6$ decimal places, $N_{0.5}$ an integer, and $\\mathbf{1}\\{I_{\\text{ovr}}=1\\}$ a boolean indicator represented as either True or False. The five elements must be reported in the order of the test suite above. For example, the output format must look like:\n  $$\n  \\big[\\,[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]\\,\\big]\n  $$\n  with no spaces.",
            "solution": "The user's request is a valid computational problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Classes**: $K=3$ classes.\n-   **Feature Vectors**: Input vectors $x \\in \\mathbb{R}^2$ are augmented with an intercept term to form $\\bar{x} = (1, x_1, x_2)^\\top$.\n-   **One-vs-Rest (OVR) Model**:\n    -   Weight matrix:\n      $$\n      W_{\\text{ovr}} =\n      \\begin{bmatrix}\n      1.0 & 1.5 & -0.1 \\\\\n      1.0 & -0.1 & 1.5 \\\\\n      0.6 & 0.7 & 0.7\n      \\end{bmatrix}.\n      $$\n    -   Linear predictor for class $k \\in \\{1, 2, 3\\}$: $z_k^{\\text{ovr}} = w_k^\\top \\bar{x}$, where $w_k^\\top$ is the $k$-th row of $W_{\\text{ovr}}$.\n-   **Multinomial (Softmax) Model**:\n    -   Weight matrix: $W_{\\text{soft}} = W_{\\text{ovr}}$.\n    -   Linear score for class $k \\in \\{1, 2, 3\\}$: $s_k = \\beta_k^\\top \\bar{x}$, where $\\beta_k^\\top$ is the $k$-th row of $W_{\\text{soft}}$.\n-   **Inverse Link Functions**:\n    -   **Bernoulli**: The canonical inverse link mapping a linear predictor to a probability in $(0, 1)$.\n    -   **Categorical**: The canonical inverse link producing a normalized probability vector $(p_1, \\dots, p_K)$ such that $\\sum_{k=1}^K p_k = 1$.\n-   **Computations for each $\\bar{x}$**:\n    1.  **OVR**: Compute probabilities $(p_1^{\\text{ovr}}, p_2^{\\text{ovr}}, p_3^{\\text{ovr}})$, their sum $S_{\\text{ovr}} = \\sum_{k=1}^3 p_k^{\\text{ovr}}$, the count $N_{0.5}$ of classes where $p_k^{\\text{ovr}} \\ge 0.5$, and an overlap indicator $I_{\\text{ovr}}$ (equal to $1$ if $N_{0.5} \\ge 2$, else $0$).\n    2.  **Softmax**: Compute probabilities $(p_1^{\\text{soft}}, p_2^{\\text{soft}}, p_3^{\\text{soft}})$ and their sum $S_{\\text{soft}} = \\sum_{k=1}^3 p_k^{\\text{soft}}$.\n-   **Test Suite**: A set of five feature vectors $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]$:\n    -   $x^{(1)} = [0.4, 0.4]$\n    -   $x^{(2)} = [-1.0, -1.0]$\n    -   $x^{(3)} = [3.0, -3.0]$\n    -   $x^{(4)} = [-5.0, -5.0]$\n    -   $x^{(5)} = [10.0, 10.0]$\n-   **Output Format**: For each test vector, produce a list $[\\text{round}(S_{\\text{ovr}}, 6), \\text{round}(S_{\\text{soft}}, 6), N_{0.5}, \\mathbf{1}\\{I_{\\text{ovr}}=1\\}]$, where the last element is a boolean (`True` or `False`).\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is well-grounded in the theory of generalized linear models (GLMs). The specified canonical inverse link for a Bernoulli distribution is the logistic (sigmoid) function, and for a categorical distribution, it is the softmax function. The comparison of one-vs-rest and multinomial logistic regression is a standard and important topic in statistical learning.\n-   **Well-Posed**: The problem is well-posed. All parameters, data, and required computations are explicitly defined, leading to a unique, stable, and meaningful numerical solution.\n-   **Objective**: The problem is stated objectively using precise mathematical notation and terminology, without any subjective or ambiguous language.\n-   **Conclusion**: The problem is free of scientific unsoundness, incompleteness, contradictions, and other flaws listed in the validation criteria.\n\n**Step 3: Verdict and Action**\n\n-   **Verdict**: The problem is **valid**.\n-   **Action**: A complete solution will be provided.\n\n### Solution Derivation\n\nThis problem requires a comparison of two common multiclass classification strategies: a one-vs-rest (OVR) scheme using multiple independent binary classifiers and a joint multinomial (softmax) regression model. The core of the task is to apply the correct probability mapping functions to the linear scores produced by the given weight matrices.\n\n**1. Canonical Inverse Link Functions**\n\n-   **Bernoulli/OVR**: The canonical inverse link for the Bernoulli distribution is the logistic function, often called the sigmoid function, $\\sigma(z)$. It maps a linear predictor $z \\in \\mathbb{R}$ to a probability in $(0, 1)$.\n    $$\n    p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n    $$\n    In the OVR scheme, this function is applied independently to each class-specific linear predictor $z_k^{\\text{ovr}}$.\n\n-   **Categorical/Softmax**: The canonical inverse link for the categorical distribution is the softmax function. It maps a vector of linear scores $s = (s_1, \\dots, s_K)^\\top$ to a normalized probability vector $p = (p_1, \\dots, p_K)^\\top$ where each $p_k \\in (0, 1)$ and $\\sum_{k=1}^K p_k = 1$.\n    $$\n    p_k = \\frac{e^{s_k}}{\\sum_{j=1}^K e^{s_j}}\n    $$\n    The softmax function ensures that the resulting probabilities form a valid distribution over the $K$ classes.\n\n**2. Model Computations**\n\nFor each input feature vector $x^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\\top$, we first augment it to $\\bar{x}^{(i)} = [1, x_1^{(i)}, x_2^{(i)}]^\\top$. The linear scores for both models are computed using the same weight matrix $W = W_{\\text{ovr}} = W_{\\text{soft}}$. The vector of scores/predictors is given by $z^{(i)} = W\\bar{x}^{(i)}$.\n\nLet's denote the score vector elements as $z_1, z_2, z_3$.\n\n-   **OVR Probabilities**: The probabilities for the OVR model are calculated by applying the sigmoid function independently to each score:\n    $$\n    p_k^{\\text{ovr}} = \\sigma(z_k) = \\frac{1}{1 + e^{-z_k}} \\quad \\text{for } k \\in \\{1, 2, 3\\}\n    $$\n    The sum $S_{\\text{ovr}} = \\sum_{k=1}^3 p_k^{\\text{ovr}}$ is generally not equal to $1$. We then count $N_{0.5} = \\sum_{k=1}^3 \\mathbf{1}\\{p_k^{\\text{ovr}} \\ge 0.5\\}$ and set the overlap indicator $I_{\\text{ovr}}$ to be true if $N_{0.5} \\ge 2$.\n\n-   **Softmax Probabilities**: The probabilities for the softmax model are calculated jointly:\n    $$\n    p_k^{\\text{soft}} = \\frac{e^{z_k}}{e^{z_1} + e^{z_2} + e^{z_3}} \\quad \\text{for } k \\in \\{1, 2, 3\\}\n    $$\n    By construction, the sum $S_{\\text{soft}} = \\sum_{k=1}^3 p_k^{\\text{soft}}$ will always be equal to $1$.\n\n**3. Numerical Stability**\n\nTo prevent numerical overflow/underflow with large magnitude inputs to the exponential function, stable implementations are required.\n\n-   **Stable Sigmoid $\\sigma(z)$**:\n    -   If $z \\ge 0$, use $1 / (1 + e^{-z})$. This avoids $e$ raised to a large positive power.\n    -   If $z < 0$, use an algebraically equivalent form $e^z / (1 + e^z)$. This again avoids $e$ raised to a large positive power.\n\n-   **Stable Softmax**: The \"log-sum-exp\" trick is used. Let $c = \\max_k(z_k)$. The softmax probabilities can be rewritten as:\n    $$\n    p_k^{\\text{soft}} = \\frac{e^{z_k - c}}{\\sum_{j=1}^K e^{z_j - c}}\n    $$\n    This transformation centers the exponents around $0$, preventing overflow from large positive scores while maintaining numerical precision.\n\n**4. Walkthrough for Test Case $x^{(1)} = [0.4, 0.4]$**\n\n1.  **Augment vector**: $\\bar{x}^{(1)} = [1, 0.4, 0.4]^\\top$.\n2.  **Compute linear scores**:\n    $$\n    z = W \\bar{x}^{(1)} =\n    \\begin{bmatrix}\n    1.0 & 1.5 & -0.1 \\\\\n    1.0 & -0.1 & 1.5 \\\\\n    0.6 & 0.7 & 0.7\n    \\end{bmatrix}\n    \\begin{bmatrix}\n    1.0 \\\\\n    0.4 \\\\\n    0.4\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    1.0 + 0.6 - 0.04 \\\\\n    1.0 - 0.04 + 0.6 \\\\\n    0.6 + 0.28 + 0.28\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    1.56 \\\\\n    1.56 \\\\\n    1.16\n    \\end{bmatrix}\n    $$\n3.  **OVR Calculation**:\n    -   $p_1^{\\text{ovr}} = \\sigma(1.56) \\approx 0.826384$\n    -   $p_2^{\\text{ovr}} = \\sigma(1.56) \\approx 0.826384$\n    -   $p_3^{\\text{ovr}} = \\sigma(1.16) \\approx 0.761352$\n    -   $S_{\\text{ovr}} = 0.826384 + 0.826384 + 0.761352 = 2.414120$\n    -   All three probabilities are $\\ge 0.5$, so $N_{0.5} = 3$.\n    -   Since $N_{0.5} \\ge 2$, the overlap indicator is `True`.\n\n4.  **Softmax Calculation**:\n    -   Using scores $z_1=1.56, z_2=1.56, z_3=1.16$.\n    -   $e^{1.56} \\approx 4.7588$, $e^{1.56} \\approx 4.7588$, $e^{1.16} \\approx 3.1899$.\n    -   Sum of exponentials $\\approx 12.7075$.\n    -   $p_1^{\\text{soft}} = 4.7588 / 12.7075 \\approx 0.374468$\n    -   $p_2^{\\text{soft}} = 4.7588 / 12.7075 \\approx 0.374468$\n    -   $p_3^{\\text{soft}} = 3.1899 / 12.7075 \\approx 0.251064$\n    -   $S_{\\text{soft}} = 0.374468 + 0.374468 + 0.251064 = 1.000000$.\n\n5.  **Result for $x^{(1)}$**:\n    $[\\text{round}(2.414120, 6), \\text{round}(1.0, 6), 3, \\text{True}] \\rightarrow [2.41412, 1.0, 3, \\text{True}]$.\n\nThis process is repeated for all five test vectors to generate the final output. The OVR scheme yields unnormalized probabilities that sum to $2.41$ and classifies the point into all three classes (since all $p_k>0.5$), highlighting its ambiguity. The softmax model provides a valid, normalized probability distribution, assigning the highest (but not overwhelming) probability to classes $1$ and $2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares probabilities from one-vs-rest (OVR) and softmax\n    models for a series of test cases.\n    \"\"\"\n\n    # Define the weight matrix for both OVR and Softmax models.\n    W = np.array([\n        [1.0, 1.5, -0.1],\n        [1.0, -0.1, 1.5],\n        [0.6, 0.7, 0.7]\n    ])\n\n    # Define the test suite of feature vectors.\n    test_cases_x = [\n        np.array([0.4, 0.4]),\n        np.array([-1.0, -1.0]),\n        np.array([3.0, -3.0]),\n        np.array([-5.0, -5.0]),\n        np.array([10.0, 10.0])\n    ]\n\n    def stable_sigmoid(z):\n        \"\"\"Numerically stable implementation of the sigmoid function.\"\"\"\n        # Condition to avoid overflow in exp for negative z\n        # and to maintain precision for positive z.\n        # np.where(condition, x, y) is used for element-wise conditional evaluation.\n        return np.where(\n            z >= 0,\n            1.0 / (1.0 + np.exp(-z)),\n            np.exp(z) / (1.0 + np.exp(z))\n        )\n\n    def stable_softmax(s):\n        \"\"\"Numerically stable implementation of the softmax function.\"\"\"\n        # Subtract the max for numerical stability (log-sum-exp trick).\n        s_max = np.max(s)\n        s_shifted = s - s_max\n        exps = np.exp(s_shifted)\n        return exps / np.sum(exps)\n\n    results_for_print = []\n\n    for x in test_cases_x:\n        # Augment the feature vector with an intercept term (x_0 = 1).\n        x_aug = np.insert(x, 0, 1.0)\n\n        # Compute the linear scores/predictors z = W * x_aug.\n        # Since W is (3, 3) and x_aug is (3,), the result is a (3,) vector.\n        z = W @ x_aug\n\n        # 1. One-vs-Rest (OVR) computations\n        p_ovr = stable_sigmoid(z)\n        S_ovr = np.sum(p_ovr)\n        \n        # Count classes where probability is >= 0.5\n        N_05 = np.sum(p_ovr >= 0.5)\n        \n        # Overlap indicator: True if at least two classes meet the threshold\n        I_ovr = (N_05 >= 2)\n\n        # 2. Softmax computations\n        p_soft = stable_softmax(z)\n        S_soft = np.sum(p_soft) # This will be 1.0 by definition.\n\n        # Format the sub-result as a string to avoid spaces when printing\n        # the final list of lists.\n        # Rounding S_ovr and S_soft to 6 decimal places as required.\n        s_ovr_rounded = f\"{S_ovr:.6f}\"\n        s_soft_rounded = f\"{S_soft:.6f}\"\n        \n        sub_result_str = f\"[{s_ovr_rounded},{s_soft_rounded},{N_05},{I_ovr}]\"\n        results_for_print.append(sub_result_str)\n\n    # Final print statement in the exact required format.\n    # Joining the string-formatted sub-results with a comma.\n    print(f\"[{','.join(results_for_print)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "The softmax function is central to multinomial logistic regression, but its direct implementation can be a trap for the unwary, leading to significant numerical errors. This is because computing the term $\\log \\sum_j \\exp(\\eta_j)$ can easily result in floating-point overflow or underflow when the linear predictors $\\eta_j$ have large magnitudes. This hands-on practice  guides you through implementing and testing the \"log-sum-exp trick,\" a crucial technique that ensures your calculations are numerically stable and accurate, regardless of the scale of the inputs.",
            "id": "3151616",
            "problem": "You are given a computational task arising from the normalization step of the multinomial logistic regression model. For a feature vector $x \\in \\mathbb{R}^p$ and $K$ class-specific parameter vectors $\\beta_j \\in \\mathbb{R}^p$, the linear predictors are $\\eta_j = x^\\top \\beta_j$ for $j \\in \\{1,\\dots,K\\}$. The normalization constant in the softmax function requires computing $\\log \\sum_{j=1}^K \\exp(\\eta_j)$, which is well defined in real arithmetic but can be numerically unstable in finite-precision floating-point arithmetic when $\\lVert \\eta \\rVert$ is large.\n\nStarting from the fundamental definitions of the multinomial logistic regression predictors and the logarithm and exponential functions, implement two computational methods for the quantity $\\log \\sum_{j=1}^K \\exp(\\eta_j)$:\n- A direct method that applies the exponential and summation followed by the logarithm without any special handling of numeric range.\n- A numerically stable method that computes the same value while avoiding overflow or underflow through a mathematically justified transformation that preserves equality in exact arithmetic.\n\nThen, for each test case provided below, do the following:\n1. Compute the vector $\\eta \\in \\mathbb{R}^K$ using $\\eta_j = x^\\top \\beta_j$.\n2. Compute the Euclidean norm $\\lVert \\eta \\rVert_2$.\n3. Compute the direct result and the numerically stable result for $\\log \\sum_{j=1}^K \\exp(\\eta_j)$.\n4. Use a reliable reference implementation to approximate the mathematically correct value of $\\log \\sum_{j=1}^K \\exp(\\eta_j)$.\n5. For each method, determine whether its result is a finite number, and compute the absolute error relative to the reference when the result is finite. If the result is not finite, report the error as $+\\infty$.\n\nYour program must use the following test suite. Each test defines $p$, $K$, a feature vector $x \\in \\mathbb{R}^p$, and a target vector $t \\in \\mathbb{R}^K$. Construct parameters $\\beta_j \\in \\mathbb{R}^p$ so that $x^\\top \\beta_j = t_j$ for each $j$ by setting\n$$\n\\beta_j = \\frac{t_j}{x^\\top x} x \\quad \\text{for each } j \\in \\{1,\\dots,K\\}.\n$$\nThis construction guarantees that $x^\\top \\beta_j = t_j$ in exact arithmetic.\n\nTest suite:\n- Case A (moderate values, happy path):\n  - $p = 3$, $K = 3$\n  - $x = [0.5,\\,-1.0,\\,2.0]$\n  - $t = [-1.2,\\,0.3,\\,2.1]$\n- Case B (very large positive values, overflow-prone for the direct method):\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [1000.0,\\,1001.0,\\,999.0]$\n- Case C (very large negative values, underflow-prone for the direct method):\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [-1000.0,\\,-1001.0,\\,-999.0]$\n- Case D (mixed extreme values, challenging dynamic range):\n  - $p = 3$, $K = 3$\n  - $x = [1.0,\\,1.0,\\,1.0]$\n  - $t = [-1000.0,\\,0.0,\\,1000.0]$\n\nFor each case, output a sequence of five values in the order:\n- $\\lVert \\eta \\rVert_2$ as a floating-point number,\n- a boolean indicating whether the direct result is finite,\n- a boolean indicating whether the stable result is finite,\n- the absolute error of the direct result relative to the reference as a floating-point number (use $+\\infty$ if not finite),\n- the absolute error of the stable result relative to the reference as a floating-point number (use $+\\infty$ if not finite).\n\nFinal output format:\nYour program should produce a single line of output containing the results of all cases concatenated in the order A, B, C, D, as a comma-separated list enclosed in square brackets, for example, \"[r1,r2,r3,...,r20]\". All booleans must appear as unquoted True or False. No additional text or whitespace should be printed.",
            "solution": "The problem asks for the implementation and comparison of two methods for computing the log-sum-exp function, defined as $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$, where $\\eta \\in \\mathbb{R}^K$ is a vector of linear predictors from a multinomial logistic regression model. The motivation is to handle numerical instability that arises in floating-point arithmetic.\n\n### Step 1: Extract Givens\n- **Task**: Implement a direct method and a numerically stable method for computing $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$.\n- **Linear Predictors**: $\\eta_j = x^\\top \\beta_j$ for $j \\in \\{1, \\dots, K\\}$, with $x \\in \\mathbb{R}^p$ and $\\beta_j \\in \\mathbb{R}^p$.\n- **Parameter Construction**: For given test vectors $x$ and $t$, the parameters $\\beta_j$ are constructed as $\\beta_j = \\frac{t_j}{x^\\top x} x$. This construction ensures $\\eta_j = t_j$ for each $j$, as shown by the substitution:\n$$ x^\\top \\beta_j = x^\\top \\left( \\frac{t_j}{x^\\top x} x \\right) = \\frac{t_j}{x^\\top x} (x^\\top x) = t_j $$\nThis identity holds provided $x^\\top x \\neq 0$, i.e., $x$ is not the zero vector. All provided test cases use non-zero $x$ vectors. Therefore, for each test case, the vector of linear predictors $\\eta$ is exactly the given target vector $t$.\n- **Test Cases**:\n    - **A**: $p=3, K=3, x=[0.5, -1.0, 2.0], t=[-1.2, 0.3, 2.1]$\n    - **B**: $p=3, K=3, x=[1.0, 1.0, 1.0], t=[1000.0, 1001.0, 999.0]$\n    - **C**: $p=3, K=3, x=[1.0, 1.0, 1.0], t=[-1000.0, -1001.0, -999.0]$\n    - **D**: $p=3, K=3, x=[1.0, 1.0, 1.0], t=[-1000.0, 0.0, 1000.0]$\n- **Required Outputs per Case**: A sequence of $5$ values:\n    1.  Euclidean norm $\\|\\eta\\|_2$.\n    2.  Boolean: Is the direct result finite?\n    3.  Boolean: Is the stable result finite?\n    4.  Absolute error of the direct result (or $+\\infty$).\n    5.  Absolute error of the stable result (or $+\\infty$).\n- **Reference Value**: Use a reliable reference implementation to approximate the correct value.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is critically evaluated:\n- **Scientifically Grounded**: The problem addresses the well-known numerical instability of the log-sum-exp calculation, which is a fundamental component of the softmax function used in multinomial logistic regression and other machine learning models. The mathematical transformation used to achieve stability is standard and correct. The problem is firmly rooted in numerical analysis and statistical learning theory.\n- **Well-Posed**: The problem is unambiguous. All inputs ($x, t, p, K$) are specified for each test case. The quantity to be computed is clearly defined. The construction of $\\beta_j$ leading to $\\eta = t$ is explicit. The evaluation metrics (norm, finiteness, absolute error) are precise. A unique solution exists for each computation.\n- **Objective**: The problem is stated in objective, mathematical language. It requires a computational solution based on numerical inputs and established algorithms, free of subjective interpretation.\n- **Completeness and Consistency**: The problem is self-contained. All necessary formulas and data are provided. There are no contradictions in the setup.\n- **No other flaws**: The problem is not trivial (it demonstrates a critical numerical issue), not metaphorical, and is directly relevant to the stated topic. It is verifiable through computation.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A reasoned solution will be provided.\n\n### Solution Derivation\n\nFor a given vector $\\eta = (\\eta_1, \\eta_2, \\dots, \\eta_K)$, we need to compute $L(\\eta) = \\log \\sum_{j=1}^K \\exp(\\eta_j)$.\n\n**Method 1: Direct Computation**\nThis method implements the formula as written.\n1.  Compute $v_j = \\exp(\\eta_j)$ for each $j \\in \\{1, \\dots, K\\}$.\n2.  Compute the sum $S = \\sum_{j=1}^K v_j$.\n3.  Compute the final result $R_{\\text{direct}} = \\log(S)$.\n\nThis approach is numerically unstable. For standard double-precision floating-point numbers, $\\exp(z)$ overflows to $+\\infty$ for $z \\gtrsim 709.78$. If any $\\eta_j$ is large, the corresponding $\\exp(\\eta_j)$ becomes $+\\infty$, leading to an infinite sum and an infinite result. Conversely, $\\exp(z)$ underflows to $0$ for $z \\lesssim -745.13$. If all $\\eta_j$ values are large and negative, every term in the sum may underflow to $0.0$, making the sum $0.0$. The logarithm $\\log(0.0)$ is $-\\infty$.\n\n**Method 2: Numerically Stable Computation (Log-Sum-Exp Trick)**\nThis method is based on a mathematical identity that repositions the values to a numerically safe range. Let $m = \\max_{j} \\eta_j$. We can rewrite the expression by factoring out $\\exp(m)$:\n$$ L(\\eta) = \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j) \\right) $$\n$$ = \\log \\left( \\exp(m) \\sum_{j=1}^K \\frac{\\exp(\\eta_j)}{\\exp(m)} \\right) $$\nUsing the properties of logarithms and exponentials, $\\log(a \\cdot b) = \\log(a) + \\log(b)$ and $\\exp(a)/\\exp(b) = \\exp(a-b)$, we get:\n$$ = \\log(\\exp(m)) + \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j - m) \\right) $$\n$$ = m + \\log \\left( \\sum_{j=1}^K \\exp(\\eta_j - m) \\right) $$\nThis gives the stable computation steps:\n1.  Find the maximum value in the vector: $m = \\max(\\eta_1, \\dots, \\eta_K)$.\n2.  Compute the shifted vector $\\eta_j' = \\eta_j - m$.\n3.  Compute $v_j' = \\exp(\\eta_j')$ for each $j$.\n4.  Compute the sum $S' = \\sum_{j=1}^K v_j'$.\n5.  Compute the final result $R_{\\text{stable}} = m + \\log(S')$.\n\nThis method is stable because the arguments to the exponential function, $\\eta_j' = \\eta_j - m$, are always less than or equal to $0$. The maximum argument is $0$, resulting in $\\exp(0)=1$. This prevents overflow. Furthermore, since at least one term in the sum $S'$ is exactly $1$, the sum is guaranteed to be at least $1$. This prevents the argument of the logarithm from being $0$ due to the underflow of all terms, thereby avoiding a result of $-\\infty$.\n\n**Computational Steps for Each Test Case**\nFor each case defined by $x$ and $t$:\n1.  Set $\\eta = t$.\n2.  Compute the Euclidean norm $\\|\\eta\\|_2 = \\sqrt{\\sum_{j=1}^K \\eta_j^2}$.\n3.  Compute $R_{\\text{direct}}$ using the direct method.\n4.  Compute $R_{\\text{stable}}$ using the stable method.\n5.  Obtain the reference value $R_{\\text{ref}}$ using `scipy.special.logsumexp`, which is a professionally implemented version of the stable method.\n6.  Determine if $R_{\\text{direct}}$ and $R_{\\text{stable}}$ are finite numbers.\n7.  Calculate the absolute errors $E_{\\text{direct}} = |R_{\\text{direct}} - R_{\\text{ref}}|$ and $E_{\\text{stable}} = |R_{\\text{stable}} - R_{\\text{ref}}|$. If a result is not finite, its corresponding error is reported as $+\\infty$.\n8.  Collect and format the five required values: $(\\|\\eta\\|_2, \\text{isfinite}(R_{\\text{direct}}), \\text{isfinite}(R_{\\text{stable}}), E_{\\text{direct}}, E_{\\text{stable}})$. The results from all test cases will be concatenated into a single list for the final output.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the numerical stability problem for the log-sum-exp function.\n    \"\"\"\n    \n    # Each test case is defined by p, K, x, and t.\n    # We can directly use t as eta, as per the problem's construction.\n    test_cases = {\n        'A': {\n            'p': 3, 'K': 3,\n            'x': np.array([0.5, -1.0, 2.0]),\n            't': np.array([-1.2, 0.3, 2.1]),\n        },\n        'B': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([1000.0, 1001.0, 999.0]),\n        },\n        'C': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([-1000.0, -1001.0, -999.0]),\n        },\n        'D': {\n            'p': 3, 'K': 3,\n            'x': np.array([1.0, 1.0, 1.0]),\n            't': np.array([-1000.0, 0.0, 1000.0]),\n        }\n    }\n\n    all_results = []\n    \n    # The problem asks to process cases in order A, B, C, D.\n    case_order = ['A', 'B', 'C', 'D']\n    \n    for case_key in case_order:\n        case_data = test_cases[case_key]\n        t = case_data['t']\n        \n        # As per the problem description, eta_j = t_j.\n        eta = t.astype(np.float64) # Use float64 for precision.\n        \n        # 1. Compute the vector eta (already done) and its Euclidean norm.\n        norm_eta = np.linalg.norm(eta)\n        \n        # 2. Compute the direct result.\n        # This implementation follows the naive formula log(sum(exp(eta))).\n        # It is prone to overflow/underflow.\n        with np.errstate(over='ignore'): # Suppress overflow warnings for this calculation\n            exp_eta_direct = np.exp(eta)\n            sum_exp_direct = np.sum(exp_eta_direct)\n            res_direct = np.log(sum_exp_direct)\n            \n        # 3. Compute the numerically stable result.\n        # This uses the log-sum-exp trick: m + log(sum(exp(eta - m))).\n        m = np.max(eta)\n        res_stable = m + np.log(np.sum(np.exp(eta - m)))\n        \n        # 4. Use a reliable reference implementation.\n        # scipy.special.logsumexp provides a robust implementation.\n        ref_val = logsumexp(eta)\n        \n        # 5. Determine finiteness and compute absolute errors.\n        is_finite_direct = np.isfinite(res_direct)\n        is_finite_stable = np.isfinite(res_stable)\n        \n        err_direct = np.abs(res_direct - ref_val) if is_finite_direct else np.inf\n        err_stable = np.abs(res_stable - ref_val) if is_finite_stable else np.inf\n\n        # Append the five required values for the current case.\n        all_results.extend([\n            norm_eta,\n            is_finite_direct,\n            is_finite_stable,\n            err_direct,\n            err_stable\n        ])\n\n    # Final print statement in the exact required format.\n    # Convert bools to 'True'/'False' strings without quotes.\n    # Format floats to a consistent representation.\n    formatted_results = []\n    for r in all_results:\n        if isinstance(r, bool):\n            formatted_results.append(str(r))\n        else:\n            # Using repr() to get a standard floating point representation\n            formatted_results.append(repr(r))\n            \n    # The requested format is a comma-separated list inside square brackets.\n    # map(str,...) is sufficient and clean.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Standard regression models are agnostic to any prior knowledge we might have about the system being modeled. This practice  demonstrates how to enrich a multiple logistic regression model by incorporating domain expertise, specifically by enforcing a monotonicity constraint on a feature's effect. You will explore how a simple constraint like $\\beta_j \\ge 0$ can guarantee that the predicted probability is non-decreasing with feature $x_j$, and you will implement a constrained optimization to see how this prior knowledge shapes the final model.",
            "id": "3151573",
            "problem": "You are given binary classification problems modeled by multiple logistic regression. The conditional probability of a positive label given a feature vector is modeled as the logistic function with a linear predictor. You are asked to derive a constrained maximum likelihood estimator that enforces monotonicity in a specified feature, and to implement and evaluate the estimator numerically for a provided test suite.\n\nAssume the following base definitions and facts:\n- Binary labels are $y_i \\in \\{0,1\\}$ for $i \\in \\{1,\\dots,n\\}$, and feature vectors are $x_i \\in \\mathbb{R}^p$.\n- The logistic model specifies $p(y_i=1 \\mid x_i) = \\sigma(\\eta_i)$ with linear predictor $\\eta_i = \\beta_0 + x_i^\\top \\beta$, where $\\beta_0 \\in \\mathbb{R}$ is an intercept and $\\beta \\in \\mathbb{R}^p$ are coefficients, and $\\sigma(t) = \\frac{1}{1+\\exp(-t)}$ is the logistic function.\n- The negative log-likelihood for independent observations is $L(\\beta_0,\\beta) = \\sum_{i=1}^n \\left( \\log(1+\\exp(\\eta_i)) - y_i \\eta_i \\right)$, which is a convex function in $(\\beta_0,\\beta)$.\n\nMonotonicity requirement: For a chosen feature index $j \\in \\{1,\\dots,p\\}$, you are given prior knowledge that the probability $p(y=1 \\mid x)$ should be nondecreasing as $x_j$ increases, holding all other coordinates fixed. Under the above model, this monotonicity is enforced by the sign constraint $\\beta_j \\ge 0$.\n\nYour tasks:\n\n- From the base definitions, formulate the constrained maximum likelihood estimation problem as the minimization of the negative log-likelihood subject to bound constraints on $\\beta_j$. Clearly state the optimization problem.\n- Using first principles, derive the stationarity condition, the complementary slackness condition, and the feasibility conditions that characterize optimality under such a bound constraint, as embodied by the Karush–Kuhn–Tucker (KKT) conditions for the inequality $\\beta_j \\ge 0$. Do not assume any specific solver; reason from the Lagrangian and the convexity of $L$.\n- Explain why $\\beta_j \\ge 0$ implies the model’s conditional probability is nondecreasing in $x_j$ for all $x \\in \\mathbb{R}^p$ by computing $\\frac{\\partial}{\\partial x_j} \\sigma(\\beta_0 + x^\\top \\beta)$ and using properties of the logistic function.\n- Implement a numerical solver that minimizes the negative log-likelihood with and without the monotonicity constraint, using only bound constraints on the coefficient(s) that must be nonnegative. Treat the intercept $\\beta_0$ as always unconstrained.\n- For each test case defined below, fit both the unconstrained and constrained models, and compute the following quantities:\n  1. The unconstrained coefficient $\\hat{\\beta}_j$ for the specified monotone feature.\n  2. The constrained coefficient $\\hat{\\beta}_j^{\\text{mono}}$ for the specified monotone feature.\n  3. An activity indicator $a \\in \\{0,1\\}$ that equals $1$ if the constraint is active at the solution (i.e., $\\hat{\\beta}_j^{\\text{mono}} \\le \\varepsilon$) and equals $0$ otherwise, where $\\varepsilon = 10^{-8}$.\n  4. The empirical negative log-likelihood value $L_{\\text{uncon}}$ at the unconstrained optimum and $L_{\\text{con}}$ at the constrained optimum.\n  5. The difference $\\Delta L = L_{\\text{con}} - L_{\\text{uncon}}$.\n  6. A monotonicity check for the constrained model: for every data point $x_i$, construct $x_i^{(+)} = x_i + h e_j$ where $e_j$ is the $j$-th standard basis vector in $\\mathbb{R}^p$, and verify that $\\sigma(\\hat{\\beta}_0^{\\text{mono}} + (x_i^{(+)})^\\top \\hat{\\beta}^{\\text{mono}}) \\ge \\sigma(\\hat{\\beta}_0^{\\text{mono}} + x_i^\\top \\hat{\\beta}^{\\text{mono}}) - \\delta$ holds for all $i$, with step size $h = 10^{-1}$ and tolerance $\\delta = 10^{-12}$. Report a boolean indicating whether this condition holds for all $i$.\n\nTest suite:\n\nFor all cases below, the design matrix provided is $X \\in \\mathbb{R}^{n \\times p}$ without an intercept column. Your implementation must internally augment the model with an intercept $\\beta_0$ that is not part of $X$. The monotonicity constraint is applied only to the specified coordinate $j$ of the non-intercept coefficients.\n\n- Case $1$ (inactive constraint expected): $n = 10$, $p = 2$, monotone feature index $j = 1$, with\n  - $X = \\begin{bmatrix}\n  0.0 & -0.2\\\\\n  0.5 & -0.5\\\\\n  1.0 & 0.2\\\\\n  1.5 & -0.2\\\\\n  2.0 & 0.1\\\\\n  2.5 & -1.0\\\\\n  3.0 & 0.5\\\\\n  3.5 & -0.7\\\\\n  4.0 & 0.3\\\\\n  4.5 & -0.6\n  \\end{bmatrix}$,\n  - $y = [0,0,0,0,1,0,1,1,1,1]$.\n- Case $2$ (constraint likely active due to weak association): $n = 12$, $p = 2$, monotone feature index $j = 1$, with\n  - $X = \\begin{bmatrix}\n  0.0 & -2.0\\\\\n  0.5 & -1.5\\\\\n  1.0 & -0.5\\\\\n  1.5 & 0.0\\\\\n  0.2 & 1.0\\\\\n  0.3 & 1.5\\\\\n  0.1 & 2.0\\\\\n  1.2 & -2.5\\\\\n  2.3 & -1.0\\\\\n  1.7 & 0.7\\\\\n  0.8 & 1.2\\\\\n  0.6 & 0.4\n  \\end{bmatrix}$,\n  - $y = [1,1,1,1,0,0,0,1,1,0,0,1]$.\n- Case $3$ (constraint contradicts data trend): $n = 10$, $p = 2$, monotone feature index $j = 1$, with\n  - $X = \\begin{bmatrix}\n  0.0 & 0.0\\\\\n  0.5 & 0.2\\\\\n  1.0 & -0.1\\\\\n  1.5 & 0.1\\\\\n  2.0 & -0.2\\\\\n  2.5 & 0.0\\\\\n  3.0 & 0.2\\\\\n  3.5 & -0.3\\\\\n  4.0 & 0.1\\\\\n  4.5 & -0.2\n  \\end{bmatrix}$,\n  - $y = [1,1,1,0,0,0,0,1,0,0]$.\n\nNumerical details:\n- Use a smooth, twice-differentiable negative log-likelihood as defined above.\n- Use a gradient-based numerical optimizer that supports bound constraints and supply the exact gradient of the objective with respect to all coefficients, including the intercept.\n- Treat the intercept $\\beta_0$ as an additional decision variable with no constraints, and apply the bound $\\beta_j \\ge 0$ only to the specified coordinate $j$ of $\\beta$.\n- For the monotonicity check, use $h = 10^{-1}$ and $\\delta = 10^{-12}$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case contributes a sublist of the form $[\\hat{\\beta}_j, \\hat{\\beta}_j^{\\text{mono}}, a, L_{\\text{uncon}}, L_{\\text{con}}, \\Delta L, \\text{mono\\_ok}]$ in that order. The values $\\hat{\\beta}_j$, $\\hat{\\beta}_j^{\\text{mono}}$, $L_{\\text{uncon}}$, $L_{\\text{con}}$, and $\\Delta L$ must be printed as floating-point numbers; $a$ must be an integer in $\\{0,1\\}$; and $\\text{mono\\_ok}$ must be a boolean.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in statistical learning theory, well-posed as a convex optimization problem, and objectively formulated with all necessary information provided.\n\nThe tasks are to formulate a constrained logistic regression problem, derive its optimality conditions, explain the effect of the constraint, and implement a numerical solution.\n\n### 1. Formulation of the Constrained Optimization Problem\n\nThe model for binary classification proposes that the probability of a positive outcome $y_i=1$ given a feature vector $x_i \\in \\mathbb{R}^p$ is $p(y_i=1 \\mid x_i) = \\sigma(\\eta_i)$, where $\\sigma(t) = (1+\\exp(-t))^{-1}$ is the logistic function and $\\eta_i = \\beta_0 + x_i^\\top \\beta$ is the linear predictor. The parameters to be estimated are the intercept $\\beta_0 \\in \\mathbb{R}$ and the coefficient vector $\\beta \\in \\mathbb{R}^p$.\n\nMaximum likelihood estimation is equivalent to minimizing the negative log-likelihood function, $L(\\beta_0, \\beta)$. For $n$ independent observations $(x_i, y_i)$, this function is:\n$$\nL(\\beta_0, \\beta) = \\sum_{i=1}^n \\left( \\log(1+\\exp(\\eta_i)) - y_i \\eta_i \\right)\n$$\nThis function is convex in its parameters $(\\beta_0, \\beta)$.\n\nThe problem requires enforcing that the conditional probability $p(y=1 \\mid x)$ is nondecreasing in a specific feature $x_j$, where $j \\in \\{1, \\dots, p\\}$. As will be shown, this translates to the sign constraint $\\beta_j \\ge 0$.\n\nCombining these, the constrained maximum likelihood estimation problem is formulated as the following convex optimization problem:\n$$\n\\begin{aligned}\n& \\underset{\\beta_0, \\beta}{\\text{minimize}}\n& & L(\\beta_0, \\beta) = \\sum_{i=1}^n \\left( \\log(1+\\exp(\\beta_0 + x_i^\\top \\beta)) - y_i (\\beta_0 + x_i^\\top \\beta) \\right) \\\\\n& \\text{subject to}\n& & \\beta_j \\ge 0\n\\end{aligned}\n$$\nThis is a convex optimization problem with a single linear inequality constraint.\n\n### 2. Derivation of Karush–Kuhn–Tucker (KKT) Conditions\n\nTo derive the optimality conditions, we use the Karush–Kuhn–Tucker (KKT) framework. We rewrite the constraint as a less-than-or-equal-to zero inequality: $g(\\beta) = -\\beta_j \\le 0$.\n\nThe Lagrangian function $\\mathcal{L}$ is formed by adding the constraint to the objective function, weighted by a Lagrange multiplier $\\mu_j$:\n$$\n\\mathcal{L}(\\beta_0, \\beta, \\mu_j) = L(\\beta_0, \\beta) + \\mu_j(-\\beta_j) = L(\\beta_0, \\beta) - \\mu_j \\beta_j\n$$\nThe KKT conditions for an optimal solution $(\\beta_0^*, \\beta^*)$ and multiplier $\\mu_j^*$ are:\n\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the primal variables $(\\beta_0, \\beta)$ must vanish at the optimal point.\n    $$\n    \\nabla_{\\beta_0, \\beta} \\mathcal{L}(\\beta_0^*, \\beta^*, \\mu_j^*) = 0\n    $$\n    This gives us the following partial derivative conditions:\n    *   For the intercept: $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_0} = \\frac{\\partial L}{\\partial \\beta_0} = 0$.\n    *   For coefficients $k \\neq j$: $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_k} = \\frac{\\partial L}{\\partial \\beta_k} = 0$.\n    *   For the constrained coefficient $\\beta_j$: $\\frac{\\partial \\mathcal{L}}{\\partial \\beta_j} = \\frac{\\partial L}{\\partial \\beta_j} - \\mu_j = 0$, which implies $\\mu_j^* = \\frac{\\partial L}{\\partial \\beta_j}\\bigg|_{(\\beta_0^*, \\beta^*)}$.\n\n2.  **Primal Feasibility**: The solution must satisfy the original constraints.\n    $$\n    -\\beta_j^* \\le 0 \\implies \\beta_j^* \\ge 0\n    $$\n\n3.  **Dual Feasibility**: For an inequality constraint of the form $g(\\cdot) \\le 0$, the corresponding Lagrange multiplier must be non-negative.\n    $$\n    \\mu_j^* \\ge 0\n    $$\n\n4.  **Complementary Slackness**: The product of the Lagrange multiplier and the constraint function must be zero. This means that either the constraint is active (holds with equality) or the multiplier is zero.\n    $$\n    \\mu_j^* (-\\beta_j^*) = 0 \\implies \\mu_j^* \\beta_j^* = 0\n    $$\n\nCombining these conditions, we can characterize the solution. From stationarity, we have $\\mu_j^* = \\frac{\\partial L}{\\partial \\beta_j}$. Substituting this into dual feasibility and complementary slackness gives the following conditions on the optimal $\\beta_j^*$:\n*   **Primal Feasibility**: $\\beta_j^* \\ge 0$.\n*   **Combined Stationarity & Dual Feasibility**: $\\frac{\\partial L}{\\partial \\beta_j} \\ge 0$ (since $\\mu_j^* \\ge 0$).\n*   **Combined Stationarity & Complementary Slackness**: $\\beta_j^* \\left(\\frac{\\partial L}{\\partial \\beta_j}\\right) = 0$.\n\nThese conditions imply two cases for the optimal solution:\n*   If the constraint is inactive ($\\beta_j^* > 0$), then complementary slackness requires $\\frac{\\partial L}{\\partial \\beta_j} = 0$. The gradient component is zero, just as in the unconstrained case.\n*   If the constraint is active ($\\beta_j^* = 0$), then primal feasibility is met, and the only remaining condition is $\\frac{\\partial L}{\\partial \\beta_j} \\ge 0$. This means the negative log-likelihood would increase if $\\beta_j$ were to become positive, and it would also increase (or is not allowed to change) if $\\beta_j$ were to become negative.\n\n### 3. Monotonicity Implication of $\\beta_j \\ge 0$\n\nWe need to show that the constraint $\\beta_j \\ge 0$ ensures that the conditional probability $p(y=1 \\mid x) = \\sigma(\\beta_0 + x^\\top \\beta)$ is nondecreasing in the feature $x_j$, holding all other features $x_k$ (for $k \\neq j$) constant.\n\nLet's compute the partial derivative of the probability with respect to $x_j$. Let $p(x) = \\sigma(\\eta(x))$ where $\\eta(x) = \\beta_0 + x^\\top \\beta = \\beta_0 + \\sum_{k=1}^p \\beta_k x_k$. Using the chain rule for differentiation:\n$$\n\\frac{\\partial p(x)}{\\partial x_j} = \\frac{d\\sigma(\\eta)}{d\\eta} \\cdot \\frac{\\partial \\eta(x)}{\\partial x_j}\n$$\nFirst, we find the derivative of the logistic function $\\sigma(\\eta) = (1 + e^{-\\eta})^{-1}$:\n$$\n\\frac{d\\sigma(\\eta)}{d\\eta} = -(1+e^{-\\eta})^{-2} \\cdot (-e^{-\\eta}) = \\frac{e^{-\\eta}}{(1+e^{-\\eta})^2} = \\frac{1}{1+e^{-\\eta}} \\cdot \\frac{e^{-\\eta}}{1+e^{-\\eta}} = \\sigma(\\eta)(1-\\sigma(\\eta))\n$$\nSince the range of $\\sigma(\\eta)$ is $(0, 1)$, it follows that $\\sigma(\\eta)(1-\\sigma(\\eta)) > 0$ for all $\\eta \\in \\mathbb{R}$. The derivative of the logistic function is strictly positive.\n\nNext, we find the partial derivative of the linear predictor $\\eta(x)$ with respect to $x_j$:\n$$\n\\frac{\\partial \\eta(x)}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\beta_0 + \\sum_{k=1}^p \\beta_k x_k \\right) = \\beta_j\n$$\nCombining these results, we get:\n$$\n\\frac{\\partial p(x)}{\\partial x_j} = \\beta_j \\cdot \\sigma(\\eta(x))(1-\\sigma(\\eta(x)))\n$$\nSince $\\sigma(\\eta)(1-\\sigma(\\eta)) > 0$, the sign of the partial derivative $\\frac{\\partial p(x)}{\\partial x_j}$ is the same as the sign of the coefficient $\\beta_j$. Therefore, the condition $\\beta_j \\ge 0$ implies that $\\frac{\\partial p(x)}{\\partial x_j} \\ge 0$ for all $x \\in \\mathbb{R}^p$. This signifies that the function $p(x)$ is nondecreasing with respect to the variable $x_j$ when all other variables are held constant.\n\n### 4. Numerical Implementation and Evaluation\n\nThe tasks in parts 4 and 5 are addressed by the Python code provided in the `<final_answer>` block. The code implements a function that:\n1.  Defines the negative log-likelihood objective function and its analytical gradient.\n2.  Uses `scipy.optimize.minimize` with the `L-BFGS-B` method to find the optimal coefficients for both the unconstrained and constrained problems.\n3.  For the constrained problem, it sets a lower bound of $0$ on the specified coefficient $\\beta_j$.\n4.  It then computes the required quantities for each test case: $\\hat{\\beta}_j$, $\\hat{\\beta}_j^{\\text{mono}}$, the activity indicator $a$, the likelihood values $L_{\\text{uncon}}$ and $L_{\\text{con}}$, the difference $\\Delta L$, and a boolean check for monotonicity `mono_ok`.\n5.  Finally, it formats and prints the results as specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves for unconstrained and constrained logistic regression coefficients\n    for a series of test cases and computes specified metrics.\n    \"\"\"\n\n    # --- Test Case Definitions ---\n    # Case 1: Inactive constraint expected\n    X1 = np.array([\n        [0.0, -0.2], [0.5, -0.5], [1.0, 0.2], [1.5, -0.2], [2.0, 0.1],\n        [2.5, -1.0], [3.0, 0.5], [3.5, -0.7], [4.0, 0.3], [4.5, -0.6]\n    ])\n    y1 = np.array([0, 0, 0, 0, 1, 0, 1, 1, 1, 1])\n    j1 = 1\n\n    # Case 2: Constraint likely active\n    X2 = np.array([\n        [0.0, -2.0], [0.5, -1.5], [1.0, -0.5], [1.5, 0.0], [0.2, 1.0],\n        [0.3, 1.5], [0.1, 2.0], [1.2, -2.5], [2.3, -1.0], [1.7, 0.7],\n        [0.8, 1.2], [0.6, 0.4]\n    ])\n    y2 = np.array([1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1])\n    j2 = 1\n\n    # Case 3: Constraint contradicts data trend\n    X3 = np.array([\n        [0.0, 0.0], [0.5, 0.2], [1.0, -0.1], [1.5, 0.1], [2.0, -0.2],\n        [2.5, 0.0], [3.0, 0.2], [3.5, -0.3], [4.0, 0.1], [4.5, -0.2]\n    ])\n    y3 = np.array([1, 1, 1, 0, 0, 0, 0, 1, 0, 0])\n    j3 = 1\n\n    test_cases = [\n        (X1, y1, j1),\n        (X2, y2, j2),\n        (X3, y3, j3),\n    ]\n\n    results = []\n    for X, y, j in test_cases:\n        case_results = solve_case(X, y, j)\n        results.append(case_results)\n\n    # Format the final output string as a list of lists.\n    output_str = \"[\" + \",\".join([str(res) for res in results]) + \"]\"\n    print(output_str)\n\ndef objective_and_grad(theta, X_aug, y):\n    \"\"\"\n    Computes the negative log-likelihood and its gradient for logistic regression.\n    \n    Args:\n        theta (np.ndarray): Parameter vector [beta_0, beta_1, ..., beta_p].\n        X_aug (np.ndarray): Augmented design matrix with an intercept column.\n        y (np.ndarray): Binary labels {0, 1}.\n\n    Returns:\n        tuple[float, np.ndarray]: a tuple containing the negative log-likelihood\n                                  and the gradient vector.\n    \"\"\"\n    eta = X_aug @ theta\n    \n    # Numerically stable calculation of log(1 + exp(eta))\n    log_1_plus_exp_eta = np.logaddexp(0, eta)\n    \n    # Negative log-likelihood\n    L = np.sum(log_1_plus_exp_eta - y * eta)\n\n    # Probabilities sigma(eta)\n    p = 1 / (1 + np.exp(-eta))\n    \n    # Gradient\n    grad = X_aug.T @ (p - y)\n    \n    return L, grad\n\ndef solve_case(X, y, j):\n    \"\"\"\n    Performs unconstrained and constrained fitting for a single test case.\n    j is a 1-based index for the feature to constrain.\n\n    Returns:\n        list: A list containing the computed metrics for the case.\n    \"\"\"\n    n, p = X.shape\n    X_aug = np.c_[np.ones(n), X]\n\n    # Initial guess for parameters [beta_0, beta_1, ..., beta_p]\n    theta0 = np.zeros(p + 1)\n\n    # 1. Unconstrained optimization\n    res_uncon = minimize(objective_and_grad, theta0, args=(X_aug, y), jac=True, method='L-BFGS-B')\n    theta_uncon = res_uncon.x\n    # The coefficient beta_j corresponds to theta[j] because theta is [b0, b1, ...]\n    beta_j_uncon = theta_uncon[j]\n    L_uncon = res_uncon.fun\n\n    # 2. Constrained optimization\n    # Bounds for theta = [beta_0, beta_1, ..., beta_p]\n    # All are unconstrained except beta_j >= 0.\n    bounds = [(None, None)] * (p + 1)\n    bounds[j] = (0, None)  # theta[j] is beta_j\n    \n    res_con = minimize(objective_and_grad, theta0, args=(X_aug, y), jac=True, method='L-BFGS-B', bounds=bounds)\n    theta_mono = res_con.x\n    beta_j_mono = theta_mono[j]\n    L_con = res_con.fun\n\n    # 3. Activity indicator\n    epsilon = 1e-8\n    a = 1 if beta_j_mono = epsilon else 0\n\n    # 4. Difference in negative log-likelihood\n    delta_L = L_con - L_uncon\n\n    # 5. Monotonicity check for the constrained model\n    h = 1e-1\n    delta_tol = 1e-12\n    mono_ok = True\n    \n    beta0_mono = theta_mono[0]\n    beta_vec_mono = theta_mono[1:]\n    \n    def sigma(t):\n        # Clip to prevent overflow in exp(-t) for large negative t\n        t = np.clip(t, -500, 500)\n        return 1 / (1 + np.exp(-t))\n\n    for i in range(n):\n        x_i = X[i, :]\n        x_i_plus = x_i.copy()\n        # j is 1-based, array index is 0-based\n        x_i_plus[j - 1] += h\n\n        eta_i = beta0_mono + x_i @ beta_vec_mono\n        eta_i_plus = beta0_mono + x_i_plus @ beta_vec_mono\n        \n        p_i = sigma(eta_i)\n        p_i_plus = sigma(eta_i_plus)\n        \n        if not (p_i_plus >= p_i - delta_tol):\n            mono_ok = False\n            break\n            \n    return [beta_j_uncon, beta_j_mono, a, L_uncon, L_con, delta_L, mono_ok]\n\n# Execute the solver\nsolve()\n```"
        }
    ]
}