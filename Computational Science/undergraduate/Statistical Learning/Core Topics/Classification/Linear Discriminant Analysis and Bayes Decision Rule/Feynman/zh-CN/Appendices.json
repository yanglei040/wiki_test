{
    "hands_on_practices": [
        {
            "introduction": "本章的第一个练习旨在巩固线性判别分析 (LDA) 的理论基础。通过从高斯类条件密度的基本假设出发，您将亲手推导出线性决策边界，并计算理论上的最小错误率，即贝叶斯错误率。这个练习不仅能加深您对模型背后数学原理的理解，也为您评估任何实际分类器提供了一个黄金标准 。",
            "id": "3139739",
            "problem": "考虑一个二元分类问题，其类别标签为 $Y \\in \\{0,1\\}$，类别先验概率相等，即 $\\pi_{0}=\\pi_{1}=\\tfrac{1}{2}$，特征向量为 $X \\in \\mathbb{R}^{2}$。假设类别条件分布是具有公共协方差矩阵的多元高斯分布，\n$$\nX \\mid Y=k \\sim \\mathcal{N}(\\mu_{k},\\Sigma), \\quad k \\in \\{0,1\\},\n$$\n其中\n$$\n\\mu_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\quad \\mu_{1}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}, \\quad \\Sigma=\\begin{pmatrix}1  0.5 \\\\ 0.5  2\\end{pmatrix}.\n$$\n从等错分代价的贝叶斯决策规则的定义出发，推导此设定下的贝叶斯分类器，并求出该贝叶斯分类器的精确错分概率。使用标准正态累积分布函数 $\\Phi(\\cdot)$ 和给定的参数，以精确的闭式形式表示您的最终答案。您的最终答案必须是单一的闭式解析表达式。\n\n此外，假设您通过对每个类别的 $n$ 个带标签样本进行最大似然估计来估计 $\\mu_{0}$、$\\mu_{1}$ 和 $\\Sigma$，然后使用得到的线性规则进行分类，从而构建一个插件式线性判别分析 (LDA) 分类器。在您的解答中，请从基本原理出发解释，当 $n \\to \\infty$ 时，该插件式分类器的错分概率是否收敛，如果收敛，其收敛值是多少，并说明原因。\n\n不需要进行数值近似或四舍五入；请提供贝叶斯错分概率的精确表达式。",
            "solution": "该问题要求我们推导一个具有高斯类别条件密度的二元分类问题的贝叶斯分类器及其错分概率，并分析插件式线性判别分析 (LDA) 分类器的渐近行为。\n\n首先，我们推导贝叶斯分类器。贝叶斯决策规则旨在最小化错分概率。对于类别标签为 $Y \\in \\{0,1\\}$ 且错分代价相等的二元分类问题，该规则将特征向量 $x$ 分配给具有最高后验概率的类别。也就是说，如果 $P(Y=1|X=x)  P(Y=0|X=x)$，我们预测为类别 1，否则预测为类别 0。\n\n根据贝叶斯定理，后验概率为 $P(Y=k|X=x) = \\frac{p(x|Y=k)\\pi_k}{p(x)}$，其中 $p(x|Y=k) = f_k(x)$ 是类别条件密度，$\\pi_k = P(Y=k)$ 是类别先验概率。决策规则变为：如果 $f_1(x)\\pi_1  f_0(x)\\pi_0$，则预测为类别 1。\n\n题目说明先验概率相等，$\\pi_0 = \\pi_1 = \\frac{1}{2}$。因此，规则简化为比较类别条件密度：如果 $f_1(x)  f_0(x)$，则预测为类别 1。这等价于比较它们的对数，即 $\\ln f_1(x)  \\ln f_0(x)$，因为对数函数是严格单调递增函数。\n\n类别条件密度为多元高斯分布：$X | Y=k \\sim \\mathcal{N}(\\mu_k, \\Sigma)$。类别 $k$ 的对数密度为：\n$$\n\\ln f_k(x) = -\\frac{p}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\Sigma| - \\frac{1}{2}(x-\\mu_k)^T \\Sigma^{-1} (x-\\mu_k)\n$$\n其中 $p=2$ 是 $x$ 的维度。决策规则 $\\ln f_1(x)  \\ln f_0(x)$ 变为：\n$$\n-\\frac{1}{2}(x-\\mu_1)^T \\Sigma^{-1} (x-\\mu_1)  -\\frac{1}{2}(x-\\mu_0)^T \\Sigma^{-1} (x-\\mu_0)\n$$\n两边乘以 $-2$ 会使不等式反向：\n$$\n(x-\\mu_1)^T \\Sigma^{-1} (x-\\mu_1)  (x-\\mu_0)^T \\Sigma^{-1} (x-\\mu_0)\n$$\n展开二次型得到：\n$$\nx^T\\Sigma^{-1}x - 2x^T\\Sigma^{-1}\\mu_1 + \\mu_1^T\\Sigma^{-1}\\mu_1  x^T\\Sigma^{-1}x - 2x^T\\Sigma^{-1}\\mu_0 + \\mu_0^T\\Sigma^{-1}\\mu_0\n$$\n项 $x^T\\Sigma^{-1}x$ 被消去。重新整理剩余项以分离出 $x$ 相关项，得到线性决策边界：\n$$\n2x^T\\Sigma^{-1}(\\mu_1 - \\mu_0)  \\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_0^T\\Sigma^{-1}\\mu_0\n$$\n贝叶斯分类器由该规则定义。如果不等式成立，则预测为类别 1。这就是 LDA 的决策规则。不等式左侧是 $x$ 的一个线性函数。\n\n接下来，我们推导该贝叶斯分类器的错分概率。我们定义一个线性得分 $S(x) = (\\mu_1-\\mu_0)^T \\Sigma^{-1} x$。决策规则可以写成：如果 $S(x)  \\frac{1}{2}(\\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_0^T\\Sigma^{-1}\\mu_0)$，则预测为类别 1。\n随机变量 $S(X)$ 是高斯随机向量 $X$ 的线性投影。因此，其分布也是高斯分布。我们求出它在每个类别下的条件分布。\n\n如果 $X \\sim \\mathcal{N}(\\mu_k, \\Sigma)$，那么线性变换 $A X$ 的分布为 $\\mathcal{N}(A\\mu_k, A\\Sigma A^T)$。在这里，变换为 $A = (\\mu_1-\\mu_0)^T \\Sigma^{-1}$。\n在给定 $Y=k$ 的条件下，$S(X)$ 的条件均值为：\n$$\nm_k = E[S(X)|Y=k] = (\\mu_1-\\mu_0)^T \\Sigma^{-1} E[X|Y=k] = (\\mu_1-\\mu_0)^T \\Sigma^{-1} \\mu_k\n$$\n因为 $\\Sigma$ 是公共的，$S(X)$ 的条件方差对两个类别是相同的：\n$$\n\\sigma_S^2 = \\text{Var}(S(X)|Y=k) = ((\\mu_1-\\mu_0)^T \\Sigma^{-1}) \\Sigma ((\\mu_1-\\mu_0)^T \\Sigma^{-1})^T = (\\mu_1-\\mu_0)^T \\Sigma^{-1} (\\mu_1-\\mu_0)\n$$\n这个量是类别均值之间马氏距离 (Mahalanobis distance) 的平方，记为 $\\Delta^2$。因此，$\\sigma_S^2 = \\Delta^2$。\n得分的均值为 $m_0 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} \\mu_0$ 和 $m_1 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} \\mu_1$。注意 $m_1 - m_0 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} (\\mu_1-\\mu_0) = \\Delta^2$。\n\n$S(X)$ 的决策阈值为 $T = \\frac{1}{2}(\\mu_1^T\\Sigma^{-1}\\mu_1 - \\mu_0^T\\Sigma^{-1}\\mu_0) = \\frac{1}{2}((\\mu_1-\\mu_0)+\\mu_0)^T\\Sigma^{-1}((\\mu_1-\\mu_0)+\\mu_0) - \\frac{1}{2}\\mu_0^T\\Sigma^{-1}\\mu_0 = \\frac{1}{2}(m_1+m_0)$。该阈值正好位于两个得分条件均值的正中间。\n\n总错分概率，即贝叶斯错误率，为：\n$P(\\text{error}) = \\pi_0 P(\\text{predict } 1 | Y=0) + \\pi_1 P(\\text{predict } 0 | Y=1)$。\n当 $\\pi_0=\\pi_1=\\frac{1}{2}$ 时：\n$P(\\text{error}) = \\frac{1}{2} P(S(X)  T | Y=0) + \\frac{1}{2} P(S(X) \\le T | Y=1)$。\n给定 $Y=0$，$S(X) \\sim \\mathcal{N}(m_0, \\Delta^2)$。$P(S(X)  T | Y=0) = P\\left(\\frac{S(X)-m_0}{\\Delta}  \\frac{T-m_0}{\\Delta}\\right) = P\\left(Z  \\frac{(m_0+m_1)/2 - m_0}{\\Delta}\\right) = P\\left(Z  \\frac{m_1-m_0}{2\\Delta}\\right) = P\\left(Z  \\frac{\\Delta^2}{2\\Delta}\\right) = P(Z  \\frac{\\Delta}{2}) = 1 - \\Phi(\\frac{\\Delta}{2}) = \\Phi(-\\frac{\\Delta}{2})$，其中 $Z \\sim \\mathcal{N}(0,1)$。\n\n给定 $Y=1$，$S(X) \\sim \\mathcal{N}(m_1, \\Delta^2)$。$P(S(X) \\le T | Y=1) = P\\left(\\frac{S(X)-m_1}{\\Delta} \\le \\frac{T-m_1}{\\Delta}\\right) = P\\left(Z \\le \\frac{(m_0+m_1)/2 - m_1}{\\Delta}\\right) = P\\left(Z \\le \\frac{m_0-m_1}{2\\Delta}\\right) = P\\left(Z \\le -\\frac{\\Delta^2}{2\\Delta}\\right) = \\Phi(-\\frac{\\Delta}{2})$。\n\n两个条件错误概率相等。总错分概率为：\n$P(\\text{error}) = \\frac{1}{2}\\Phi(-\\frac{\\Delta}{2}) + \\frac{1}{2}\\Phi(-\\frac{\\Delta}{2}) = \\Phi(-\\frac{\\Delta}{2})$。\n\n现在，我们使用给定的参数计算 $\\Delta^2$：\n$$\n\\mu_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\quad \\mu_{1}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}, \\quad \\Sigma=\\begin{pmatrix}1  0.5 \\\\ 0.5  2\\end{pmatrix}.\n$$\n$\\Sigma$ 的行列式是 $\\det(\\Sigma) = (1)(2) - (0.5)(0.5) = 2 - 0.25 = 1.75 = \\frac{7}{4}$。\n$\\Sigma$ 的逆矩阵是：\n$$\n\\Sigma^{-1} = \\frac{1}{7/4} \\begin{pmatrix} 2  -0.5 \\\\ -0.5  1 \\end{pmatrix} = \\frac{4}{7} \\begin{pmatrix} 2  -0.5 \\\\ -0.5  1 \\end{pmatrix} = \\begin{pmatrix} 8/7  -2/7 \\\\ -2/7  4/7 \\end{pmatrix}.\n$$\n均值之差为 $\\mu_1 - \\mu_0 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n马氏距离的平方为：\n$$\n\\Delta^2 = (\\mu_1-\\mu_0)^T \\Sigma^{-1} (\\mu_1-\\mu_0) = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 8/7  -2/7 \\\\ -2/7  4/7 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\n\\Delta^2 = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} (8/7)(1) + (-2/7)(2) \\\\ (-2/7)(1) + (4/7)(2) \\end{pmatrix} = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 4/7 \\\\ 6/7 \\end{pmatrix} = (1)(\\frac{4}{7}) + (2)(\\frac{6}{7}) = \\frac{4+12}{7} = \\frac{16}{7}.\n$$\n马氏距离为 $\\Delta = \\sqrt{\\frac{16}{7}} = \\frac{4}{\\sqrt{7}}$。\n因此，贝叶斯错分概率为：\n$$\nP(\\text{error}) = \\Phi\\left(-\\frac{\\Delta}{2}\\right) = \\Phi\\left(-\\frac{1}{2} \\cdot \\frac{4}{\\sqrt{7}}\\right) = \\Phi\\left(-\\frac{2}{\\sqrt{7}}\\right).\n$$\n\n最后，关于插件式 LDA 分类器：该分类器是通过首先从每个类别 $n$ 个样本的训练集中估计未知参数 $\\mu_0$、$\\mu_1$ 和 $\\Sigma$，然后将这些估计值“代入”上面推导出的 LDA 决策规则来构建的。这些参数的标准估计量是样本均值和合并样本协方差矩阵。\n$$\n\\hat{\\mu}_k = \\frac{1}{n} \\sum_{i: y_i=k} x_i, \\quad \\hat{\\Sigma} = \\frac{1}{2n-2} \\sum_{k=0}^{1} \\sum_{i: y_i=k} (x_i-\\hat{\\mu}_k)(x_i-\\hat{\\mu}_k)^T\n$$\n根据大数定律，这些估计量是相合的。当样本数量 $n$ 趋于无穷大时，估计的参数依概率收敛到它们的真实值：\n$$\n\\hat{\\mu}_k \\xrightarrow{p} \\mu_k \\quad \\text{and} \\quad \\hat{\\Sigma} \\xrightarrow{p} \\Sigma \\quad \\text{as } n \\to \\infty.\n$$\nLDA 决策边界是这些参数的连续函数。根据连续映射定理，插件式分类器的决策函数 $\\hat{\\delta}(x)$ 依概率收敛到贝叶斯最优决策函数 $\\delta(x)$。\n$$\n\\hat{\\delta}(x) = x^T\\hat{\\Sigma}^{-1}(\\hat{\\mu}_1 - \\hat{\\mu}_0) - \\frac{1}{2}(\\hat{\\mu}_1^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_1 - \\hat{\\mu}_0^T\\hat{\\Sigma}^{-1}\\hat{\\mu}_0) \\xrightarrow{p} \\delta(x) \\quad \\text{as } n \\to \\infty\n$$\n因此，插件式 LDA 分类器的错分概率（即在真实数据分布下的期望）收敛到最优贝叶斯分类器的错分概率。这个最小可能错误率就是贝叶斯错误率。\n因此，当 $n \\to \\infty$ 时，插件式 LDA 分类器的错分概率收敛到我们计算出的精确贝叶斯错分概率，即 $\\Phi\\left(-\\frac{2}{\\sqrt{7}}\\right)$。",
            "answer": "$$\\boxed{\\Phi\\left(-\\frac{2}{\\sqrt{7}}\\right)}$$"
        },
        {
            "introduction": "理论模型通常依赖于在实践中可能不成立的理想化假设。这个编程练习将向您展示，单个异常值如何能极大地破坏标准LDA的参数估计（均值和协方差），从而导致错误的决策边界。通过引导您实现一个稳健的替代方案，本练习强调了稳健统计在解决现实世界问题中的重要性 。",
            "id": "3139742",
            "problem": "考虑一个二维二元分类问题，其类别由 $0$ 和 $1$ 索引。对于零一损失函数，贝叶斯决策规则选择能使后验概率 $P(Y=y \\mid \\mathbf{x})$ 最大化的类别标签 $y \\in \\{0,1\\}$。假设类条件密度是具有相同协方差矩阵和相等类先验概率的多元高斯分布。在此基础上，推导比较两个判别函数的线性决策规则，并为标准估计器和稳健估计器实现相应的线性判别分析（LDA）分类器。\n\n您的程序必须仅使用此处提供的信息完成以下任务。\n\n1) 建模假设和估计器：\n- 假设对于每个类别 $k \\in \\{0,1\\}$，训练数据 $\\mathbf{X}_k \\in \\mathbb{R}^{n_k \\times 2}$ 独立地从一个高斯分布中抽取，该分布的类别均值为 $\\boldsymbol{\\mu}_k \\in \\mathbb{R}^2$，共享协方差矩阵为 $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2 \\times 2}$，并且类别先验概率相等，$P(Y=0) = P(Y=1) = 1/2$。\n- 标准 LDA 估计器必须对每个类别 $k$ 使用样本均值 $\\hat{\\boldsymbol{\\mu}}_k$，以及通过对由 $(n_k - 1)$ 加权的类内无偏样本协方差进行平均得到的合并协方差估计器 $\\hat{\\boldsymbol{\\Sigma}}$。\n- 稳健 LDA 估计器必须使用 $\\alpha$-截尾策略以保证稳健性：对于每个类别 $k$，计算坐标中位数 $\\tilde{\\boldsymbol{m}}_k$，计算每个类别样本到 $\\tilde{\\boldsymbol{m}}_k$ 的欧几里得距离，舍弃最远的 $\\lceil \\alpha n_k \\rceil$ 个点，然后从截尾后的数据集中计算截尾类别均值 $\\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_k$ 和合并协方差 $\\hat{\\boldsymbol{\\Sigma}}^{(\\mathrm{rob})}$。使用 $\\alpha = 0.2$。如果由于有限样本效应导致协方差矩阵不可逆，则添加一个小的正岭项 $\\lambda \\mathbf{I}$，其中 $\\lambda = 10^{-6}$。\n\n2) 训练数据集：\n- 干净的训练集（无离群值）：\n  - 类别 $0$：$\\mathbf{X}_0^{(\\mathrm{clean})} = \\{(0,0), (0.2,0.1), (-0.1,-0.2), (0.1,0)\\}$，因此 $n_0 = 4$。\n  - 类别 $1$：$\\mathbf{X}_1^{(\\mathrm{clean})} = \\{(2,0), (2.1,0.2), (1.9,-0.1), (2.2,0.05)\\}$，因此 $n_1 = 4$。\n- 类别 $0$ 的含离群值数据集：\n  - 类别 $0$ 含一个离群值：$\\mathbf{X}_0^{(\\mathrm{out})} = \\mathbf{X}_0^{(\\mathrm{clean})} \\cup \\{(10,0)\\}$，因此 $n_0 = 5$。\n  - 类别 $1$ 保持为 $\\mathbf{X}_1^{(\\mathrm{clean})}$。\n\n3) 分类器和预测规则：\n- 使用从指定训练集估计的 $(\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}})$ 实现标准 LDA 分类器。\n- 使用按规定通过 $\\alpha=0.2$ 的 $\\alpha$-截尾法估计的 $(\\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_0, \\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_1, \\hat{\\boldsymbol{\\Sigma}}^{(\\mathrm{rob})})$ 实现稳健 LDA 分类器。\n- 使用从高斯共享协方差模型和相等先验概率推导出的线性判别规则，为任何输入 $\\mathbf{x} \\in \\mathbb{R}^2$ 生成 $\\{0,1\\}$ 中的预测标签。\n\n4) 测试套件和要求输出：\n- 定义查询点 $\\mathbf{q} = (1.1, 0.0)$。\n- 定义查询集 $\\mathcal{S} = \\{(0.96,0.0), (1.03,0.0), (1.10,0.0), (1.17,0.0), (1.24,0.0)\\}$。\n- 训练三个分类器：\n  - 在干净数据 $(\\mathbf{X}_0^{(\\mathrm{clean})}, \\mathbf{X}_1^{(\\mathrm{clean})})$ 上训练的标准 LDA。\n  - 在含离群值的数据 $(\\mathbf{X}_0^{(\\mathrm{out})}, \\mathbf{X}_1^{(\\mathrm{clean})})$ 上训练的标准 LDA。\n  - 在含离群值的数据上训练的稳健 LDA，在估计均值和合并协方差之前，对每个类别分别应用 $\\alpha = 0.2$ 的截尾处理。\n- 计算以下六个布尔值输出：\n  - $b_1$：在干净数据上训练的标准 LDA 对 $\\mathbf{q}$ 预测的标签等于 $1$。\n  - $b_2$：在含离群值数据上训练的标准 LDA 对 $\\mathbf{q}$ 预测的标签等于 $0$。\n  - $b_3$：在含离群值数据上训练的稳健 LDA 对 $\\mathbf{q}$ 预测的标签等于在干净数据上训练的标准 LDA 所预测的标签。\n  - $b_4$：在含离群值数据上训练的标准 LDA 和稳健 LDA 对 $\\mathbf{q}$ 预测的标签不同。\n  - $b_5$：在 $\\mathcal{S}$ 中至少存在一个点，其在干净数据上训练的标准 LDA 下的预测标签与在含离群值数据上训练的标准 LDA 下的预测标签不同。\n  - $b_6$：对于 $\\mathcal{S}$ 中的每一个点，其在干净数据上训练的标准 LDA 下的预测标签等于在含离群值数据上训练的稳健 LDA 下的预测标签。\n\n5) 最终输出格式：\n- 您的程序应生成一行输出，其中包含六个布尔值的结果，格式为方括号内的逗号分隔列表，顺序为 $[b_1, b_2, b_3, b_4, b_5, b_6]$。例如，一个语法上有效的例子是 $[True,False,True,True,False,True]$。\n\n不涉及物理单位。不使用角度。所有计算均按规定在 $\\mathbb{R}^2$ 中纯数值进行。程序必须是自包含的，并且不得读取任何输入。",
            "solution": "该问题被评估为有效。这是一个在统计学习领域内定义明确、有科学依据且客观的问题，特别关注线性判别分析（LDA）以及离群值对参数估计的影响。它要求实现标准估计器和稳健估计器，并将其应用于指定的数据集。\n\n### 原理与推导\n\n此问题的核心在于分类的贝叶斯决策理论。对于类别为 $k \\in \\{0, 1\\}$ 且使用零一损失函数的二元分类任务，贝叶斯决策规则将特征向量 $\\mathbf{x}$ 分配给具有最高后验概率的类别。\n$$ \\hat{y} = \\arg\\max_{k \\in \\{0,1\\}} P(Y=k \\mid \\mathbf{x}) $$\n使用贝叶斯定理，$P(Y=k \\mid \\mathbf{x}) \\propto P(\\mathbf{x} \\mid Y=k) P(Y=k) = f_k(\\mathbf{x}) \\pi_k$，其中 $f_k(\\mathbf{x})$ 是类条件概率密度，$\\pi_k$ 是类先验概率。\n\n问题陈述，类条件密度是多元高斯分布，具有特定于类的均值 $\\boldsymbol{\\mu}_k$ 和一个共同的协方差矩阵 $\\boldsymbol{\\Sigma}$：\n$$ f_k(\\mathbf{x}) = \\frac{1}{(2\\pi)^{d/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\\right) $$\n其中 $d=2$。先验概率给定为相等，$\\pi_0 = \\pi_1 = 1/2$。\n\n该决策规则等价于最大化后验概率的对数，由此我们推导出判别函数 $\\delta_k(\\mathbf{x})$：\n$$ \\delta_k(\\mathbf{x}) = \\ln(f_k(\\mathbf{x}) \\pi_k) = \\ln f_k(\\mathbf{x}) + \\ln \\pi_k $$\n代入高斯密度并舍去对所有类别 $k$ 均为常数的项（即 $-\\frac{d}{2}\\ln(2\\pi)$、$-\\frac{1}{2}\\ln|\\boldsymbol{\\Sigma}|$ 以及由于先验概率相等而为常数的 $\\ln \\pi_k$），最大化问题简化为：\n$$ \\hat{y} = \\arg\\max_{k \\in \\{0,1\\}} \\left[ -\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right] $$\n这等价于最小化从 $\\mathbf{x}$ 到类别均值 $\\boldsymbol{\\mu}_k$ 的马氏距离（Mahalanobis distance）的平方：\n$$ \\hat{y} = \\arg\\min_{k \\in \\{0,1\\}} \\left[ (\\mathbf{x} - \\boldsymbol{\\mu}_k)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k) \\right] $$\n决策边界是到两个类别均值的马氏距离相等的点的集合。如果输入 $\\mathbf{x}$ 到 $\\boldsymbol{\\mu}_1$ 的马氏距离小于到 $\\boldsymbol{\\mu}_0$ 的马氏距离，则将其分类为类别 $1$，否则分类为类别 $0$。\n\n### 参数估计\n\n在实践中，真实参数 $\\boldsymbol{\\mu}_k$ 和 $\\boldsymbol{\\Sigma}$ 是未知的，必须从训练数据中估计。设类别 $k$ 的训练数据为 $\\mathbf{X}_k \\in \\mathbb{R}^{n_k \\times d}$。\n\n1.  **标准估计器**：\n    -   类别均值 $\\boldsymbol{\\mu}_k$ 通过样本均值估计：$\\hat{\\boldsymbol{\\mu}}_k = \\frac{1}{n_k} \\sum_{i=1}^{n_k} \\mathbf{x}_{i}$。\n    -   共享协方差矩阵 $\\boldsymbol{\\Sigma}$ 通过合并协方差矩阵 $\\hat{\\boldsymbol{\\Sigma}}$ 估计，该矩阵是对类内散布矩阵进行平均：\n        $$ \\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{n_0 + n_1 - 2} \\sum_{k=0}^{1} \\sum_{i=1}^{n_k} (\\mathbf{x}_{i,k} - \\hat{\\boldsymbol{\\mu}}_k)(\\mathbf{x}_{i,k} - \\hat{\\boldsymbol{\\mu}}_k)^T $$\n        这对应于问题指定的对由 $(n_k-1)$ 加权的无偏样本协方差进行平均的程序。\n\n2.  **稳健估计器**：\n    为减轻离群值的影响，使用 $\\alpha=0.2$ 的 $\\alpha$-截尾估计策略。\n    -   对每个类别 $k$，计算其坐标中位数 $\\tilde{\\boldsymbol{m}}_k$。\n    -   对类别中的每个样本 $\\mathbf{x}_i$，计算其欧几里得距离 $\\|\\mathbf{x}_i - \\tilde{\\boldsymbol{m}}_k\\|_2$。\n    -   将具有最大距离的 $\\lceil \\alpha n_k \\rceil$ 个样本识别为离群值并从训练集中移除。\n    -   然后使用这些截尾后的数据集计算均值 $\\hat{\\boldsymbol{\\mu}}^{(\\mathrm{rob})}_k$ 和合并协方差 $\\hat{\\boldsymbol{\\Sigma}}^{(\\mathrm{rob})}$ 的标准估计器。\n\n### 实现与逻辑\n\n解决方案涉及按规定训练三个不同的 LDA 分类器：\n1.  **分类器 1 (C1)**：在干净数据集 $(\\mathbf{X}_0^{(\\mathrm{clean})}, \\mathbf{X}_1^{(\\mathrm{clean})})$ 上训练的标准 LDA。\n2.  **分类器 2 (C2)**：在含离群值的数据集 $(\\mathbf{X}_0^{(\\mathrm{out})}, \\mathbf{X}_1^{(\\mathrm{clean})})$ 上训练的标准 LDA。\n3.  **分类器 3 (C3)**：在含离群值的数据集 $(\\mathbf{X}_0^{(\\mathrm{out})}, \\mathbf{X}_1^{(\\mathrm{clean})})$ 上训练的稳健 LDA（使用 $\\alpha=0.2$ 截尾）。\n\n对于每个分类器，计算估计参数 $(\\hat{\\boldsymbol{\\mu}}_0, \\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\Sigma}})$。在预测之前，检查 $\\hat{\\boldsymbol{\\Sigma}}$ 的可逆性。如果它是奇异的（行列式接近于零），则通过加上 $\\lambda\\mathbf{I}$ 进行正则化，其中 $\\lambda = 10^{-6}$。\n\n使用推导出的马氏距离规则，为三个分类器中的每一个对查询点 $\\mathbf{q}$ 和查询集 $\\mathcal{S}$ 进行预测。最后，基于这些预测评估六个布尔条件（$b_1$ 到 $b_6$）。\n-   $b_1$：$C1(\\mathbf{q}) = 1$\n-   $b_2$：$C2(\\mathbf{q}) = 0$\n-   $b_3$：$C3(\\mathbf{q}) = C1(\\mathbf{q})$\n-   $b_4$：$C2(\\mathbf{q}) \\neq C3(\\mathbf{q})$\n-   $b_5$：$\\exists \\mathbf{p} \\in \\mathcal{S} \\text{ s.t. } C1(\\mathbf{p}) \\neq C2(\\mathbf{p})$\n-   $b_6$：$\\forall \\mathbf{p} \\in \\mathcal{S}, C1(\\mathbf{p}) = C3(\\mathbf{p})$\n\n最终输出是这些布尔值的列表。",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and evaluates standard and robust LDA classifiers based on the problem specification.\n    \"\"\"\n\n    # 1. Data Definitions\n    X0_clean = np.array([[0.0, 0.0], [0.2, 0.1], [-0.1, -0.2], [0.1, 0.0]])\n    X1_clean = np.array([[2.0, 0.0], [2.1, 0.2], [1.9, -0.1], [2.2, 0.05]])\n    X0_out = np.vstack([X0_clean, [10.0, 0.0]])\n\n    q = np.array([1.1, 0.0])\n    S_set = np.array([\n        [0.96, 0.0], [1.03, 0.0], [1.10, 0.0], [1.17, 0.0], [1.24, 0.0]\n    ])\n    \n    alpha = 0.2\n    regularization_lambda = 1e-6\n\n    # 2. Estimator and Classifier Implementation\n    def train_standard_lda(X0, X1):\n        \"\"\"\n        Estimates means and pooled covariance for standard LDA.\n        \"\"\"\n        n0, d = X0.shape\n        n1, _ = X1.shape\n        \n        mu0 = np.mean(X0, axis=0)\n        mu1 = np.mean(X1, axis=0)\n\n        # Sum of squares (scatter matrices)\n        S0 = (X0 - mu0).T @ (X0 - mu0)\n        S1 = (X1 - mu1).T @ (X1 - mu1)\n        \n        # Pooled covariance matrix\n        Sigma_pooled = (S0 + S1) / (n0 + n1 - 2)\n        \n        return mu0, mu1, Sigma_pooled\n\n    def train_robust_lda(X0, X1, alpha_val):\n        \"\"\"\n        Implements the alpha-trimmed robust estimation procedure.\n        \"\"\"\n        trimmed_datasets = []\n        for X_class in [X0, X1]:\n            n_class, _ = X_class.shape\n            m_class = np.median(X_class, axis=0)\n            dists = np.linalg.norm(X_class - m_class, axis=1)\n            n_trim = math.ceil(alpha_val * n_class)\n            \n            if n_trim > 0 and n_trim  n_class:\n                trim_indices = np.argsort(dists)[-n_trim:]\n                X_trimmed = np.delete(X_class, trim_indices, axis=0)\n                trimmed_datasets.append(X_trimmed)\n            else:\n                trimmed_datasets.append(X_class)\n        \n        X0_trimmed, X1_trimmed = trimmed_datasets\n        \n        return train_standard_lda(X0_trimmed, X1_trimmed)\n\n    def classify(x, params):\n        \"\"\"\n        Predicts class label for a point x using estimated LDA parameters.\n        \"\"\"\n        mu0, mu1, Sigma = params\n        \n        # Regularize if singular\n        if np.isclose(np.linalg.det(Sigma), 0):\n            Sigma = Sigma + regularization_lambda * np.identity(Sigma.shape[0])\n            \n        Sigma_inv = np.linalg.inv(Sigma)\n        \n        # Compare Mahalanobis distances\n        dist0 = (x - mu0).T @ Sigma_inv @ (x - mu0)\n        dist1 = (x - mu1).T @ Sigma_inv @ (x - mu1)\n        \n        return 0 if dist0  dist1 else 1\n\n    # 3. Training the Three Classifiers\n    # C1: Standard LDA on clean data\n    params_c1 = train_standard_lda(X0_clean, X1_clean)\n    \n    # C2: Standard LDA on outlier-contaminated data\n    params_c2 = train_standard_lda(X0_out, X1_clean)\n    \n    # C3: Robust LDA on outlier-contaminated data\n    params_c3 = train_robust_lda(X0_out, X1_clean, alpha)\n\n    # 4. Evaluating the Six Boolean Conditions\n    # Predictions for the query point q\n    pred_q_c1 = classify(q, params_c1)\n    pred_q_c2 = classify(q, params_c2)\n    pred_q_c3 = classify(q, params_c3)\n\n    # b1: label(q) by C1 is 1\n    b1 = (pred_q_c1 == 1)\n    \n    # b2: label(q) by C2 is 0\n    b2 = (pred_q_c2 == 0)\n\n    # b3: label(q) by C3 equals label by C1\n    b3 = (pred_q_c3 == pred_q_c1)\n    \n    # b4: labels for q by C2 and C3 are different\n    b4 = (pred_q_c2 != pred_q_c3)\n\n    # Predictions for the query set S\n    preds_S_c1 = np.array([classify(p, params_c1) for p in S_set])\n    preds_S_c2 = np.array([classify(p, params_c2) for p in S_set])\n    preds_S_c3 = np.array([classify(p, params_c3) for p in S_set])\n\n    # b5: Exists p in S with different labels from C1 and C2\n    b5 = np.any(preds_S_c1 != preds_S_c2)\n    \n    # b6: For every p in S, label from C1 equals label from C3\n    b6 = np.all(preds_S_c1 == preds_S_c3)\n\n    # 5. Final Output\n    results = [b1, b2, b3, b4, b5, b6]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "当特征维度 $p$ 远大于样本量 $n$ 时，经典LDA方法会失效，这便是“维度灾难”带来的挑战。这项高级练习将介绍稀疏LDA，这是一种现代的改进方法，它利用 $\\ell_1$ 正则化来同时进行特征选择和构建可解释的模型。您将通过近端梯度法实现一个解决方案，从而获得对现代统计机器学习至关重要的优化技术的实践经验 。",
            "id": "3139725",
            "problem": "考虑一个双类别分类问题，其中特征由实数向量 $x \\in \\mathbb{R}^p$ 表示。假设类别1和类别2服从具有相同协方差矩阵的多元正态分布，并定义均值差向量 $m = \\mu_1 - \\mu_2$。在协方差相等的假设下，贝叶斯决策规则（Bayes Decision Rule, BDR）产生一个线性决策边界。线性判别分析（Linear Discriminant Analysis, LDA）旨在寻找一个投影向量 $w \\in \\mathbb{R}^p$ 来优化Fisher准则。对于双类别问题，Fisher准则可以用类内协方差矩阵 $S_w$ 和由均值差 $m$ 捕获的类间信息来表示。\n\n基本原理如下：\n- 双类别的Fisher准则由以下比率定义\n$$\nJ(w) = \\frac{(w^\\top m)^2}{w^\\top S_w w}.\n$$\n- 在等协方差高斯模型下，贝叶斯最优线性判别式与 $S_w^{-1} m$ 成正比，而Fisher优化对应于最大化 $J(w)$。\n\n为了通过引入稀疏性来提高可解释性和进行变量选择，考虑以下稀疏公式。该公式通过将分子 $(w^\\top m)^2$ 固定为常数，并最小化带有旨在促进稀疏性的$\\ell_1$惩罚项的分母而得到：\n$$\n\\min_{w \\in \\mathbb{R}^p} \\quad w^\\top S_w w + \\lambda \\lVert w \\rVert_1 \\quad \\text{subject to} \\quad w^\\top m = 1,\n$$\n其中 $\\lambda \\ge 0$ 是一个控制稀疏性的正则化参数。该公式在保留Fisher结构的同时实现了变量选择。为了进行计算求解，将等式约束转换为二次惩罚项，并应用近端梯度法：\n- 定义带惩罚的目标函数\n$$\nF(w) = w^\\top S_w w + \\lambda \\lVert w \\rVert_1 + \\frac{\\rho}{2} (w^\\top m - 1)^2,\n$$\n其中 $\\rho  0$ 是一个惩罚参数。\n- $F(w)$ 的光滑部分是 $G(w) = w^\\top S_w w + \\frac{\\rho}{2} (w^\\top m - 1)^2$，其梯度为\n$$\n\\nabla G(w) = 2 S_w w + \\rho (w^\\top m - 1) m.\n$$\n- 使用近端梯度迭代\n$$\nw^{(k+1)} \\leftarrow \\text{soft}(w^{(k)} - \\eta \\nabla G(w^{(k)}), \\eta \\lambda),\n$$\n其中 $\\eta$ 是根据 $\\nabla G(w)$ 的Lipschitz常数选择的步长，而 $\\text{soft}(z, \\tau)$ 是分量软阈值算子 $\\text{soft}(z_i,\\tau) = \\text{sign}(z_i)\\max(|z_i| - \\tau, 0)$。\n\n收敛后，将选定的变量定义为满足 $|w_i|  \\tau_{\\text{sel}}$ 的索引 $i$，其中 $\\tau_{\\text{sel}}  0$ 是给定的选择阈值。为了解释在相关特征下变量选择的稳定性，需要衡量在 $m$ 的微小扰动下所选集合的变化情况；使用集合 $A$ 和 $B$ 之间的Jaccard相似度，\n$$\nJ(A,B) = \\frac{|A \\cap B|}{|A \\cup B|},\n$$\n并约定 $J(\\emptyset,\\emptyset) = 1$。\n\n实现一个程序，该程序：\n1. 通过带有约束二次惩罚项的近端梯度法求解上述稀疏优化问题。\n2. 针对指定的测试用例返回所选变量的索引。\n3. 通过计算 $m$ 的微小扰动下的平均Jaccard相似度，来计算在相关特征下的稳定性得分。\n\n使用以下测试套件，其中 $p = 6$，选择阈值 $\\tau_{\\text{sel}} = 10^{-6}$，惩罚参数 $\\rho = 50$：\n- 测试用例1（正常路径）：\n    - $S_w =$ \n    $$\n    \\begin{bmatrix}\n    1.0  0.8  0.0  0.0  0.0  0.0 \\\\\n    0.8  1.0  0.0  0.0  0.0  0.0 \\\\\n    0.0  0.0  1.0  0.3  0.0  0.0 \\\\\n    0.0  0.0  0.3  1.0  0.0  0.0 \\\\\n    0.0  0.0  0.0  0.0  1.0  0.0 \\\\\n    0.0  0.0  0.0  0.0  0.0  1.0\n    \\end{bmatrix}\n    $$\n    - $m = [1.0, 1.0, 0.5, 0.0, 0.0, -0.2]$\n    - $\\lambda = 0.15$\n    - 输出：按升序排列的所选索引列表。\n- 测试用例2（边界稀疏性）：\n    - 与测试用例1相同的 $S_w$ 和 $m$\n    - $\\lambda = 0.8$\n    - 输出：所选变量的整数数量。\n- 测试用例3（相关特征下的稳定性）：\n    - 与测试用例1相同的 $S_w$\n    - $m = [0.8, 0.8, 0.0, 0.0, 0.0, 0.0]$\n    - $\\lambda = 0.25$\n    - 执行 $B = 10$ 次扰动：对于每个 $b \\in \\{1,\\dots,10\\}$，从标准差为 $0.02$ 的零均值正态分布中抽取分量独立的 $\\varepsilon^{(b)} \\in \\mathbb{R}^6$，并定义 $m^{(b)} = m + \\varepsilon^{(b)}$。使用 $m$ 计算基准选择集，并使用 $m^{(b)}$ 计算选择集，然后报告基准选择与扰动后选择之间的平均Jaccard相似度。\n    - 输出：一个表示平均Jaccard相似度的浮点数。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[result1,result2,result3]”）。结果必须按顺序为：\n1. 测试用例1的所选索引列表。\n2. 测试用例2的所选变量的整数数量。\n3. 测试用例3的浮点数平均Jaccard相似度。",
            "solution": "问题陈述经评估有效。它提出了一个在统计学习领域，特别是稀疏线性判别分析（LDA）领域中定义明确的计算问题。该问题具有科学依据，数学上一致，并提供了推导唯一解所需的所有必要信息。所采用的方法基于通过近端梯度算法求解的惩罚优化公式，是现代统计学和机器学习中一种标准且可靠的方法。\n\n问题的核心是解决以下优化问题：\n$$\n\\min_{w \\in \\mathbb{R}^p} \\quad F(w) = w^\\top S_w w + \\lambda \\lVert w \\rVert_1 + \\frac{\\rho}{2} (w^\\top m - 1)^2\n$$\n其中 $w \\in \\mathbb{R}^p$ 是我们寻求的判别向量，$S_w$ 是类内协方差矩阵，$m$ 是类间均值差向量，$\\lambda \\ge 0$ 是一个诱导稀疏性的正则化参数，$\\rho  0$ 是对约束 $w^\\top m = 1$ 的惩罚参数。特征总数为 $p=6$。\n\n该解决方案利用近端梯度法，该方法非常适用于可分解为光滑部分和非光滑（但可计算近端算子）部分的目标函数。在此，光滑部分是 $G(w) = w^\\top S_w w + \\frac{\\rho}{2} (w^\\top m - 1)^2$，非光滑部分是 $\\lambda \\lVert w \\rVert_1$。\n\n近端梯度算法的迭代更新规则是：\n$$\nw^{(k+1)} \\leftarrow \\text{soft}(w^{(k)} - \\eta \\nabla G(w^{(k)}), \\eta \\lambda)\n$$\n其中 $k$ 是迭代索引，$\\text{soft}(\\cdot, \\cdot)$ 是分量软阈值算子：\n$$\n\\text{soft}(z_i, \\tau) = \\text{sign}(z_i)\\max(|z_i| - \\tau, 0)\n$$\n光滑部分 $\\nabla G(w)$ 的梯度由下式给出：\n$$\n\\nabla G(w) = 2 S_w w + \\rho (w^\\top m - 1) m\n$$\n必须选择步长 $\\eta$ 以确保收敛。一个标准的选择是 $\\eta \\le 1/L$，其中 $L$ 是 $\\nabla G(w)$ 的Lipschitz常数。对于二次函数 $G(w)$，$L$ 是其Hessian矩阵 $\\nabla^2 G(w)$ 谱范数的一个上界。该Hessian矩阵是常数：\n$$\nH = \\nabla^2 G(w) = 2 S_w + \\rho m m^\\top\n$$\n由于 $H$ 是对称半正定的（因为 $S_w$ 是协方差矩阵且 $m m^\\top$ 是半正定的），其谱范数是其最大特征值 $\\lambda_{\\max}(H)$。因此，我们设置步长 $\\eta = 1/\\lambda_{\\max}(H)$。\n\n算法收敛到解 $w$ 后，通过识别权重绝对值 $|w_i|$ 超过阈值 $\\tau_{\\text{sel}} = 10^{-6}$ 的索引 $i$ 来执行变量选择。\n\n实现过程首先定义一个函数来解决这个优化问题。然后将此函数应用于三个指定的测试用例。\n\n**测试用例1：所选索引**\n参数为：\n- $S_w$：给定的 $6 \\times 6$ 矩阵。\n- $m = [1.0, 1.0, 0.5, 0.0, 0.0, -0.2]^T$\n- $\\lambda = 0.15$\n- $\\rho = 50$\n使用这些参数执行近端梯度算法直至收敛。然后使用 $\\tau_{\\text{sel}} = 10^{-6}$ 对所得向量 $w$ 进行阈值处理，以识别所选变量索引的集合。这些索引以升序列表形式返回。\n\n**测试用例2：稀疏度**\n参数与测试用例1相同，但正则化参数增加到 $\\lambda = 0.8$。更大的 $\\lambda$ 值对非零权重施加更强的惩罚，从而导致更稀疏的解。使用这个新的 $\\lambda$ 再次运行算法，并计算所选变量的数量（即，大小超过 $\\tau_{\\text{sel}}$ 的权重的计数）。这个整数计数是此用例的结果。\n\n**测试用例3：稳定性分析**\n此用例评估在特征相关性下变量选择的稳定性。参数为：\n- $S_w$：与之前相同的矩阵。\n- $m = [0.8, 0.8, 0.0, 0.0, 0.0, 0.0]^T$。信号集中在前两个特征上，这两个特征高度相关（相关系数为0.8）。\n- $\\lambda = 0.25$\n- $\\rho = 50$\n首先，通过求解给定 $m$ 的优化问题来获得基准选择集。然后，进行 $B=10$ 次重复的模拟。在每次重复中，通过添加一个小的随机噪声向量 $\\varepsilon^{(b)}$ 来扰动均值向量 $m$，其中每个分量都独立地从均值为 $0$、标准差为 $0.02$ 的正态分布中抽取。对于每个扰动向量 $m^{(b)} = m + \\varepsilon^{(b)}$，计算一个新的解 $w^{(b)}$，并确定一个新的选择集。计算基准选择集与10个扰动后的选择集之间的Jaccard相似度 $J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$。问题指定了约定 $J(\\emptyset,\\emptyset)=1$。此用例的最终结果是这10个Jaccard相似度得分的平均值。为确保可复现性，对随机数生成器使用固定的种子。",
            "answer": "```python\nimport numpy as np\n\ndef jaccard_similarity(set1, set2):\n    \"\"\"\n    Calculates the Jaccard similarity between two sets.\n    Follows the convention J(emptyset, emptyset) = 1.\n    \"\"\"\n    if not set1 and not set2:\n        return 1.0\n    \n    intersection_card = len(set1.intersection(set2))\n    union_card = len(set1.union(set2))\n\n    if union_card == 0:\n        # This case is covered by the initial check, but included for robustness.\n        return 1.0\n    \n    return intersection_card / union_card\n\ndef solve_sparse_lda(Sw, m, lambda_reg, rho, p, tol=1e-9, max_iter=2000):\n    \"\"\"\n    Solves the sparse LDA optimization problem using a proximal gradient method.\n    \"\"\"\n    w = np.zeros(p)\n    \n    # Calculate step size eta = 1/L, where L is the Lipschitz constant of the gradient.\n    # L is the largest eigenvalue of the Hessian of the smooth part of the objective.\n    H = 2 * Sw + rho * np.outer(m, m)\n    L = np.max(np.linalg.eigvalsh(H))\n    eta = 1.0 / L\n    \n    # Proximal gradient descent iterations\n    for _ in range(max_iter):\n        w_old = w.copy()\n        \n        # Gradient of the smooth part: grad(G(w))\n        grad_G = 2 * Sw @ w + rho * (w @ m - 1) * m\n        \n        # Gradient descent step\n        z = w - eta * grad_G\n        \n        # Proximal operator (soft-thresholding)\n        tau = eta * lambda_reg\n        w = np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n        \n        # Check for convergence\n        if np.linalg.norm(w - w_old)  tol:\n            break\n            \n    return w\n\ndef solve():\n    \"\"\"\n    Main function to run the three test cases and print the results.\n    \"\"\"\n    # Common parameters across test cases\n    p = 6\n    tau_sel = 1e-6\n    rho = 50.0\n    Sw = np.array([\n        [1.0, 0.8, 0.0, 0.0, 0.0, 0.0],\n        [0.8, 1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.3, 0.0, 0.0],\n        [0.0, 0.0, 0.3, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0, 0.0],\n        [0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n    ])\n\n    results = []\n\n    # Test Case 1: Selected indices\n    m1 = np.array([1.0, 1.0, 0.5, 0.0, 0.0, -0.2])\n    lambda1 = 0.15\n    w1 = solve_sparse_lda(Sw, m1, lambda1, rho, p)\n    indices1 = np.where(np.abs(w1) > tau_sel)[0].tolist()\n    results.append(indices1)\n\n    # Test Case 2: Number of selected variables\n    m2 = m1\n    lambda2 = 0.8\n    w2 = solve_sparse_lda(Sw, m2, lambda2, rho, p)\n    count2 = int(np.sum(np.abs(w2) > tau_sel))\n    results.append(count2)\n    \n    # Test Case 3: Stability analysis\n    m3 = np.array([0.8, 0.8, 0.0, 0.0, 0.0, 0.0])\n    lambda3 = 0.25\n    B = 10\n    noise_std = 0.02\n    \n    # Use a fixed seed for reproducibility\n    rng = np.random.default_rng(seed=42)\n    \n    # Baseline solution and selected set\n    w_base = solve_sparse_lda(Sw, m3, lambda3, rho, p)\n    set_base = set(np.where(np.abs(w_base) > tau_sel)[0])\n    \n    # Perturbation analysis\n    jaccard_scores = []\n    for _ in range(B):\n        epsilon = rng.normal(loc=0.0, scale=noise_std, size=p)\n        m_pert = m3 + epsilon\n        w_pert = solve_sparse_lda(Sw, m_pert, lambda3, rho, p)\n        set_pert = set(np.where(np.abs(w_pert) > tau_sel)[0])\n        score = jaccard_similarity(set_base, set_pert)\n        jaccard_scores.append(score)\n        \n    avg_jaccard = np.mean(jaccard_scores)\n    results.append(avg_jaccard)\n\n    # Format and print the final output\n    print(f\"[{str(results[0])},{results[1]},{results[2]}]\")\n\nsolve()\n```"
        }
    ]
}