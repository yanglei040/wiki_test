{
    "hands_on_practices": [
        {
            "introduction": "本练习将揭示朴素贝叶斯分类器最关键的特性：其“朴素”的条件独立性假设。通过分析一个具有完全相关特征的场景，我们将量化模型如何“重复计算”证据，从而导致过度自信的预测。这项实践对于理解该模型的潜在缺陷和内在偏见至关重要。",
            "id": "3152503",
            "problem": "考虑一个二元分类问题，其类别变量为 $Y \\in \\{1,0\\}$，单个二元潜在信号为 $Z \\in \\{1,0\\}$。该潜在信号通过对抗性复制生成两个观测特征：$X_{1} = Z$ 和 $X_{2} = Z$。类别先验概率为 $P(Y=1) = \\pi$，其中 $\\pi = 0.3$，因此 $P(Y=0) = 1 - \\pi = 0.7$。潜在信号的条件分布是伯努利分布，参数为 $P(Z=1 \\mid Y=1) = \\theta_{1}$ 和 $P(Z=1 \\mid Y=0) = \\theta_{0}$，其中 $\\theta_{1} = 0.8$ 且 $\\theta_{0} = 0.4$。你观察到一个新实例 $x = (x_{1}, x_{2})$，其中 $x_{1} = 1$ 且 $x_{2} = 1$。\n\n仅从Bayes定理和朴素贝叶斯分类器（NBC）所使用的朴素条件独立性假设的定义出发，推导在 $(X_{1}, X_{2})$ 的真实生成模型下的精确后验对数几率 $\\ln\\!\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)$，以及在NBC下（错误地将 $X_{1}$ 和 $X_{2}$ 视为在给定 $Y$ 时条件独立）的后验对数几率。然后计算膨胀量\n$$\n\\Delta \\;=\\; \\ln\\!\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{NBC}} \\;-\\; \\ln\\!\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{true}}.\n$$\n给出 $\\Delta$ 的数值，四舍五入到四位有效数字。",
            "solution": "该问题要求计算膨胀量 $\\Delta$，即朴素贝叶斯分类器（NBC）计算的后验对数几率与给定观测值的真实后验对数几率之间的差值。观测值为 $x = (x_1, x_2)$，其中 $x_1 = 1$ 且 $x_2 = 1$。\n\n给定观测值 $x$，类别 $Y=1$ 相对于 $Y=0$ 的后验对数几率由Bayes定理推导得出：\n$$\n\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)} = \\frac{P(x \\mid Y=1) P(Y=1)}{P(x \\mid Y=0) P(Y=0)}\n$$\n对两边取自然对数，得到对数几率：\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right) = \\ln\\left(\\frac{P(x \\mid Y=1)}{P(x \\mid Y=0)}\\right) + \\ln\\left(\\frac{P(Y=1)}{P(Y=0)}\\right)\n$$\n右边的第一项是对数似然比，第二项是对数先验几率。类别先验概率给定为 $P(Y=1) = \\pi = 0.3$，所以 $P(Y=0) = 1-\\pi = 0.7$。对数先验几率项对于真实模型和NBC模型都是常数：\n$$\n\\ln\\left(\\frac{P(Y=1)}{P(Y=0)}\\right) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) = \\ln\\left(\\frac{0.3}{0.7}\\right)\n$$\n\n首先，我们计算真实生成模型下的后验对数几率。\n在真实模型中，特征是由一个潜在信号 $Z$ 生成的，使得 $X_1 = Z$ 和 $X_2 = Z$。对于给定的观测值 $x_1=1$ 和 $x_2=1$，事件 $(X_1=1, X_2=1)$ 等同于事件 $(Z=1)$。\n因此，在给定类别 $Y=y$ 的情况下，观测到 $x=(1,1)$ 的真实条件似然为：\n$$\nP(x=(1,1) \\mid Y=y)_{\\text{true}} = P(X_1=1, X_2=1 \\mid Y=y) = P(Z=1 \\mid Y=y)\n$$\n问题给出了 $Z$ 的条件分布：\n对于 $y=1$：$P(Z=1 \\mid Y=1) = \\theta_1 = 0.8$。\n对于 $y=0$：$P(Z=1 \\mid Y=0) = \\theta_0 = 0.4$。\n观测值 $x=(1,1)$ 的真实对数似然比为：\n$$\n\\ln\\left(\\frac{P(x=(1,1) \\mid Y=1)_{\\text{true}}}{P(x=(1,1) \\mid Y=0)_{\\text{true}}}\\right) = \\ln\\left(\\frac{P(Z=1 \\mid Y=1)}{P(Z=1 \\mid Y=0)}\\right) = \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right)\n$$\n真实的后验对数几率为：\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{true}} = \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\n$$\n\n接下来，我们计算朴素贝叶斯分类器（NBC）模型下的后验对数几率。\nNBC错误地假设特征 $X_1$ 和 $X_2$ 在给定类别 $Y$ 的条件下是条件独立的。因此，似然被分解为：\n$$\nP(x=(1,1) \\mid Y=y)_{\\text{NBC}} = P(X_1=1 \\mid Y=y) P(X_2=1 \\mid Y=y)\n$$\n要使用这个公式，我们必须首先找到边际条件概率 $P(X_j=1 \\mid Y=y)$。这些概率由真实的生成过程决定，因为它们代表了NBC可以从无限数据集中学习到的参数。我们对潜在变量 $Z$进行边缘化：\n$$\nP(X_j=1 \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_j=1, Z=z \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_j=1 \\mid Z=z, Y=y) P(Z=z \\mid Y=y)\n$$\n生成结构是 $Y \\to Z \\to X_j$，这意味着在给定 $Z$ 的情况下，$X_j$ 与 $Y$ 是条件独立的。因此，$P(X_j=1 \\mid Z=z, Y=y) = P(X_j=1 \\mid Z=z)$。\n由于 $X_j=Z$，我们有 $P(X_j=1 \\mid Z=1) = 1$ 和 $P(X_j=1 \\mid Z=0) = 0$。\n将这些代入，我们得到：\n$$\nP(X_j=1 \\mid Y=y) = 1 \\cdot P(Z=1 \\mid Y=y) + 0 \\cdot P(Z=0 \\mid Y=y) = P(Z=1 \\mid Y=y)\n$$\n所以，所需的边际概率是：\n$P(X_j=1 \\mid Y=1) = P(Z=1 \\mid Y=1) = \\theta_1 = 0.8$。\n$P(X_j=1 \\mid Y=0) = P(Z=1 \\mid Y=0) = \\theta_0 = 0.4$。\n\n在NBC假设下，$x=(1,1)$ 的条件似然为：\n$P(x=(1,1) \\mid Y=1)_{\\text{NBC}} = P(X_1=1 \\mid Y=1) P(X_2=1 \\mid Y=1) = \\theta_1 \\cdot \\theta_1 = \\theta_1^2$。\n$P(x=(1,1) \\mid Y=0)_{\\text{NBC}} = P(X_1=1 \\mid Y=0) P(X_2=1 \\mid Y=0) = \\theta_0 \\cdot \\theta_0 = \\theta_0^2$。\nNBC的对数似然比是：\n$$\n\\ln\\left(\\frac{P(x=(1,1) \\mid Y=1)_{\\text{NBC}}}{P(x=(1,1) \\mid Y=0)_{\\text{NBC}}}\\right) = \\ln\\left(\\frac{\\theta_1^2}{\\theta_0^2}\\right) = 2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right)\n$$\nNBC的后验对数几率是：\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{NBC}} = 2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\n$$\n膨胀量 $\\Delta$ 是NBC对数几率与真实对数几率之间的差值：\n$$\n\\Delta = \\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{NBC}} - \\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{true}}\n$$\n$$\n\\Delta = \\left(2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\right) - \\left(\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\right)\n$$\n对数先验几率项被抵消，剩下：\n$$\n\\Delta = 2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) - \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) = \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right)\n$$\n膨胀量表示NBC对来自单一潜在来源 $Z$ 的证据进行重复计算的程度，该证据被复制成了两个完全相关的特征。\n代入给定的数值 $\\theta_1=0.8$ 和 $\\theta_0=0.4$：\n$$\n\\Delta = \\ln\\left(\\frac{0.8}{0.4}\\right) = \\ln(2)\n$$\n$\\ln(2)$ 的数值约为 $0.693147...$。四舍五入到四位有效数字，我们得到：\n$$\n\\Delta \\approx 0.6931\n$$",
            "answer": "$$\\boxed{0.6931}$$"
        },
        {
            "introduction": "虽然通常与线性边界联系在一起，但高斯朴素贝叶斯分类器实际上可以产生非线性的决策边界。本练习将引导您完成一个推导，证明当不同类别的方差不相等时，决策边界会变成二次曲线。这个练习旨在挑战常见的误解，并揭示模型意想不到的灵活性。",
            "id": "3152506",
            "problem": "一个二元分类任务涉及单个实值特征 $x \\in \\mathbb{R}$ 和类别标签 $y \\in \\{0,1\\}$。考虑以下包含八个观测值的标记数据集：\n- 类别 $y=1$：$x \\in \\{2, 3, 4, 5\\}$。\n- 类别 $y=0$：$x \\in \\{0, 1, 1, 2\\}$。\n假设使用朴素贝叶斯分类器 (NBC)，并采用以下建模和估计选择：\n1. 类别先验 $P(y=c)$ 通过类别 $c \\in \\{0,1\\}$ 的经验频率进行估计。\n2. 给定 $y=c$ 时 $x$ 的类别条件分布被建模为均值为 $\\mu_{c}$、方差为 $\\sigma_{c}^{2}$ 的高斯（正态）分布，这些参数通过每个类别的数据进行最大似然估计，即 $\\mu_{c}$ 是样本均值，$\\sigma_{c}^{2}$ 是类别 $c$ 中样本与 $\\mu_{c}$ 的平方偏差的平均值。\n从贝叶斯法则和高斯概率密度函数出发，推导对数后验几率 $\\ln\\!\\left(\\frac{P(y=1 \\mid x)}{P(y=0 \\mid x)}\\right)$ 作为 $x$ 的二次函数的精确闭式表达式。请以精确的解析形式表示你的最终答案，所有对数保留为 $\\ln(\\cdot)$ 形式，不要四舍五入；不需要进行数值近似。你的最终表达式必须是关于 $x$ 的单个解析表达式。",
            "solution": "问题要求推导朴素贝叶斯分类器 (NBC) 的对数后验几率。任务是找到 $\\ln\\left(\\frac{P(y=1 \\mid x)}{P(y=0 \\mid x)}\\right)$ 作为特征 $x$ 的二次函数的闭式表达式。\n\n首先，我们从给定特征 $x$ 时类别 $c \\in \\{0,1\\}$ 的后验概率的贝叶斯法则开始：\n$$P(y=c \\mid x) = \\frac{P(x \\mid y=c) P(y=c)}{P(x)}$$\n后验几率是类别 $y=1$ 和类别 $y=0$ 的后验概率之比：\n$$\\frac{P(y=1 \\mid x)}{P(y=0 \\mid x)} = \\frac{P(x \\mid y=1) P(y=1) / P(x)}{P(x \\mid y=0) P(y=0) / P(x)} = \\frac{P(x \\mid y=1) P(y=1)}{P(x \\mid y=0) P(y=0)}$$\n项 $P(x)$ 被消掉了。取该比率的自然对数，得到对数后验几率：\n$$\\ln\\left(\\frac{P(y=1 \\mid x)}{P(y=0 \\mid x)}\\right) = \\ln\\left(\\frac{P(x \\mid y=1)}{P(x \\mid y=0)}\\right) + \\ln\\left(\\frac{P(y=1)}{P(y=0)}\\right)$$\n该表达式由对数似然比和对数先验几率组成。我们现在必须根据所提供的数据和模型假设来计算每个部分。\n\n1.  **计算先验概率和对数先验几率**\n    问题指明类别先验 $P(y=c)$ 是经验类别频率。数据集总共有 $N_{total} = 8$ 个观测值。\n    -   对于类别 $y=1$，数据为 $x \\in \\{2, 3, 4, 5\\}$，因此样本数量为 $N_1 = 4$。\n    -   对于类别 $y=0$，数据为 $x \\in \\{0, 1, 1, 2\\}$，因此样本数量为 $N_0 = 4$。\n\n    先验概率为：\n    $$P(y=1) = \\frac{N_1}{N_{total}} = \\frac{4}{8} = \\frac{1}{2}$$\n    $$P(y=0) = \\frac{N_0}{N_{total}} = \\frac{4}{8} = \\frac{1}{2}$$\n    因此，对数先验几率项为：\n    $$\\ln\\left(\\frac{P(y=1)}{P(y=0)}\\right) = \\ln\\left(\\frac{1/2}{1/2}\\right) = \\ln(1) = 0$$\n\n2.  **计算类别条件似然参数**\n    类别条件分布 $P(x \\mid y=c)$ 建模为高斯分布 $N(\\mu_c, \\sigma_c^2)$。参数 $\\mu_c$ 和 $\\sigma_c^2$ 使用最大似然估计 (MLE) 进行估计。对于高斯分布，均值的最大似然估计是样本均值，方差的最大似然估计是样本方差，其分母为 $N_c$（类别 $c$ 中的样本数）。\n\n    -   **类别 $y=1$ ($c=1$) 的参数：**\n        数据：$\\{2, 3, 4, 5\\}$。\n        样本均值 $\\mu_1$：\n        $$\\mu_1 = \\frac{2 + 3 + 4 + 5}{4} = \\frac{14}{4} = \\frac{7}{2}$$\n        样本方差 $\\sigma_1^2$：\n        $$\\sigma_1^2 = \\frac{1}{N_1} \\sum_{i=1}^{N_1} (x_i - \\mu_1)^2 = \\frac{1}{4} \\left[ \\left(2 - \\frac{7}{2}\\right)^2 + \\left(3 - \\frac{7}{2}\\right)^2 + \\left(4 - \\frac{7}{2}\\right)^2 + \\left(5 - \\frac{7}{2}\\right)^2 \\right]$$\n        $$\\sigma_1^2 = \\frac{1}{4} \\left[ \\left(-\\frac{3}{2}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 \\right] = \\frac{1}{4} \\left[ \\frac{9}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{9}{4} \\right] = \\frac{1}{4} \\left( \\frac{20}{4} \\right) = \\frac{5}{4}$$\n\n    -   **类别 $y=0$ ($c=0$) 的参数：**\n        数据：$\\{0, 1, 1, 2\\}$。\n        样本均值 $\\mu_0$：\n        $$\\mu_0 = \\frac{0 + 1 + 1 + 2}{4} = \\frac{4}{4} = 1$$\n        样本方差 $\\sigma_0^2$：\n        $$\\sigma_0^2 = \\frac{1}{N_0} \\sum_{i=1}^{N_0} (x_i - \\mu_0)^2 = \\frac{1}{4} \\left[ (0 - 1)^2 + (1 - 1)^2 + (1 - 1)^2 + (2 - 1)^2 \\right]$$\n        $$\\sigma_0^2 = \\frac{1}{4} \\left[ (-1)^2 + 0^2 + 0^2 + 1^2 \\right] = \\frac{1}{4} [1 + 0 + 0 + 1] = \\frac{2}{4} = \\frac{1}{2}$$\n\n3.  **推导对数似然比**\n    高斯分布的概率密度函数为：\n    $$P(x \\mid y=c) = \\frac{1}{\\sqrt{2\\pi\\sigma_c^2}} \\exp\\left(-\\frac{(x - \\mu_c)^2}{2\\sigma_c^2}\\right)$$\n    该密度的对数为：\n    $$\\ln(P(x \\mid y=c)) = -\\frac{1}{2}\\ln(2\\pi\\sigma_c^2) - \\frac{(x - \\mu_c)^2}{2\\sigma_c^2}$$\n    对数似然比为 $\\ln(P(x \\mid y=1)) - \\ln(P(x \\mid y=0))$：\n    $$\\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_1^2) - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2}\\right) - \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_0^2) - \\frac{(x - \\mu_0)^2}{2\\sigma_0^2}\\right)$$\n    $$= \\frac{1}{2}\\ln\\left(\\frac{2\\pi\\sigma_0^2}{2\\pi\\sigma_1^2}\\right) + \\frac{(x - \\mu_0)^2}{2\\sigma_0^2} - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2}$$\n    $$= \\frac{1}{2}\\ln\\left(\\frac{\\sigma_0^2}{\\sigma_1^2}\\right) + \\frac{(x - \\mu_0)^2}{2\\sigma_0^2} - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2}$$\n\n4.  **组合最终表达式**\n    由于对数先验几率项为 $0$，因此对数后验几率等于对数似然比。我们代入计算出的参数值：\n    $\\mu_0 = 1$, $\\sigma_0^2 = \\frac{1}{2}$\n    $\\mu_1 = \\frac{7}{2}$, $\\sigma_1^2 = \\frac{5}{4}$\n\n    对数后验几率：\n    $$= \\frac{1}{2}\\ln\\left(\\frac{1/2}{5/4}\\right) + \\frac{(x - 1)^2}{2(1/2)} - \\frac{(x - 7/2)^2}{2(5/4)}$$\n    $$= \\frac{1}{2}\\ln\\left(\\frac{1}{2} \\cdot \\frac{4}{5}\\right) + \\frac{(x - 1)^2}{1} - \\frac{(x - 7/2)^2}{5/2}$$\n    $$= \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right) + (x^2 - 2x + 1) - \\frac{2}{5}\\left(x^2 - 7x + \\frac{49}{4}\\right)$$\n    现在，展开并合并同类项：\n    $$= \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right) + x^2 - 2x + 1 - \\frac{2}{5}x^2 + \\frac{14}{5}x - \\frac{49}{10}$$\n    合并 $x$ 的各次幂的系数：\n    -   $x^2$ 的系数：$1 - \\frac{2}{5} = \\frac{3}{5}$\n    -   $x$ 的系数：$-2 + \\frac{14}{5} = -\\frac{10}{5} + \\frac{14}{5} = \\frac{4}{5}$\n    -   常数项：$1 - \\frac{49}{10} + \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right) = \\frac{10}{10} - \\frac{49}{10} + \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right) = -\\frac{39}{10} + \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right)$\n\n    将所有项放在一起，作为 $x$ 的二次函数的对数后验几率是：\n    $$\\frac{3}{5}x^2 + \\frac{4}{5}x - \\frac{39}{10} + \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right)$$",
            "answer": "$$\\boxed{\\frac{3}{5}x^2 + \\frac{4}{5}x - \\frac{39}{10} + \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right)}$$"
        },
        {
            "introduction": "让我们将所学知识应用于一个经典的现实世界问题：垃圾邮件过滤。这项练习将让您扮演一个试图通过策略性地注入“好”词（ham）来规避多项式朴素贝叶斯分类器的对抗者角色。通过计算改变分类器决策所需的最少词语数量，您将对模型如何权衡证据获得更深刻、更实际的理解。",
            "id": "3152525",
            "problem": "一家公司部署了一个基于内容的电子邮件过滤器，该过滤器使用多项式朴素贝叶斯（MNB）模型，将邮件视为词元（word tokens）的词袋，具有类条件词元概率，并假设在给定类别的情况下词元是条件独立的。类别为 $\\text{spam}$ 和 $\\text{ham}$，其先验概率分别为 $P(\\text{spam}) = 0.6$ 和 $P(\\text{ham}) = 0.4$。考虑一个足够大的词汇表，以至于下述邮件中不包含大多数单词；以下三个词元每次出现的类条件概率如下：对于词元“promo”，$P(\\text{promo}\\mid \\text{spam}) = 0.012$ 且 $P(\\text{promo}\\mid \\text{ham}) = 0.003$；对于词元“limited”，$P(\\text{limited}\\mid \\text{spam}) = 0.010$ 且 $P(\\text{limited}\\mid \\text{ham}) = 0.004$；对于词元“meeting”，$P(\\text{meeting}\\mid \\text{spam}) = 0.001$ 且 $P(\\text{meeting}\\mid \\text{ham}) = 0.012$。一封原始邮件包含 1 个“promo”实例，1 个“limited”实例和 0 个“meeting”实例，所有其他词元的计数均为 0。一个攻击者试图通过向邮件中注入 $n$ 个额外的词元“meeting”实例，而不改变任何其他内容，来使分类器的决策从 $\\text{spam}$ 翻转到 $\\text{ham}$。\n\n从 Bayes 定理和多项式朴素贝叶斯模型假设（包括给定类别下词元出现的独立性以及使用类条件词元概率）出发，推导出后验概率偏向 $\\text{ham}$ 而非 $\\text{spam}$ 的决策条件，并确定导致决策翻转到 $\\text{ham}$ 的最小正整数 $n$。使用自然对数表达你的推理过程。最终答案必须是最小整数 $n$。由于要求的是整数，因此无需舍入说明。",
            "solution": "该问题要求找到注入的“meeting”词元的最小正整数 $n$，该数量能使多项式朴素贝叶斯（MNB）分类器对给定邮件的决策从 $\\text{spam}$ 变为 $\\text{ham}$。\n\n分类决策基于给定文档 $D$ 的类别 $c$ 的后验概率，记为 $P(c \\mid D)$。分类器选择使该后验概率最大化的类别 $c$。根据 Bayes 定理，后验概率由下式给出：\n$$P(c \\mid D) = \\frac{P(D \\mid c) P(c)}{P(D)}$$\n由于对于给定的文档 $D$，证据 $P(D)$ 对所有考虑的类别都是一个常数，因此决策规则等同于最大化似然 $P(D \\mid c)$ 和先验概率 $P(c)$ 的乘积：\n$$\\hat{c} = \\arg\\max_{c \\in \\{\\text{spam}, \\text{ham}\\}} P(D \\mid c) P(c)$$\n最初，该邮件被分类为 $\\text{spam}$。如果新邮件 $D_n$ 被分类为 $\\text{ham}$，则对抗性修改成功。当 $\\text{ham}$ 的后验概率超过 $\\text{spam}$ 的后验概率时，决策发生翻转：\n$$P(\\text{ham} \\mid D_n) > P(\\text{spam} \\mid D_n)$$\n这等价于条件：\n$$P(D_n \\mid \\text{ham}) P(\\text{ham}) > P(D_n \\mid \\text{spam}) P(\\text{spam})$$\n设词汇表中的词元“promo”、“limited”和“meeting”分别表示为 $w_p$、$w_l$ 和 $w_m$。修改后的文档 $D_n$ 包含 1 个 $w_p$ 实例，1 个 $w_l$ 实例和 $n$ 个 $w_m$ 实例。$D_n$ 中的词元总数为 $L_n = 1 + 1 + n = n+2$。\n\n多项式朴素贝叶斯模型假设文档是一个词袋，似然 $P(D_n \\mid c)$ 由多项式概率质量函数给出。设各词元的计数分别为 $x_p=1$，$x_l=1$ 和 $x_m=n$。\n$$P(D_n \\mid c) = \\frac{L_n!}{x_p! x_l! x_m!} \\left( P(w_p \\mid c) \\right)^{x_p} \\left( P(w_l \\mid c) \\right)^{x_l} \\left( P(w_m \\mid c) \\right)^{x_m}$$\n代入计数，我们得到：\n$$P(D_n \\mid c) = \\frac{(n+2)!}{1! \\, 1! \\, n!} \\left( P(w_p \\mid c) \\right)^1 \\left( P(w_l \\mid c) \\right)^1 \\left( P(w_m \\mid c) \\right)^n$$\n现在，我们可以写出决策不等式：\n$$ \\frac{(n+2)!}{n!} P(w_p \\mid \\text{ham}) P(w_l \\mid \\text{ham}) (P(w_m \\mid \\text{ham}))^n P(\\text{ham}) > \\frac{(n+2)!}{n!} P(w_p \\mid \\text{spam}) P(w_l \\mid \\text{spam}) (P(w_m \\mid \\text{spam}))^n P(\\text{spam}) $$\n多项式系数 $\\frac{(n+2)!}{n!}$ 是两边的公因子，可以消去。这是在朴素贝叶斯中比较类别后验概率时的一个普遍属性，因为该项仅取决于文档的结构，而与类别无关。简化后的不等式为：\n$$ P(w_p \\mid \\text{ham}) P(w_l \\mid \\text{ham}) (P(w_m \\mid \\text{ham}))^n P(\\text{ham}) > P(w_p \\mid \\text{spam}) P(w_l \\mid \\text{spam}) (P(w_m \\mid \\text{spam}))^n P(\\text{spam}) $$\n按要求，我们对两边取自然对数。由于对数函数是单调递增函数，不等式的方向保持不变：\n$$ \\ln\\left(P(w_p \\mid \\text{ham})\\right) + \\ln\\left(P(w_l \\mid \\text{ham})\\right) + n \\ln\\left(P(w_m \\mid \\text{ham})\\right) + \\ln\\left(P(\\text{ham})\\right) > \\ln\\left(P(w_p \\mid \\text{spam})\\right) + \\ln\\left(P(w_l \\mid \\text{spam})\\right) + n \\ln\\left(P(w_m \\mid \\text{spam})\\right) + \\ln\\left(P(\\text{spam})\\right) $$\n为了解出 $n$，我们重排不等式，将含有 $n$ 的项组合在一起：\n$$ n \\ln\\left(P(w_m \\mid \\text{ham})\\right) - n \\ln\\left(P(w_m \\mid \\text{spam})\\right) > \\left[ \\ln\\left(P(\\text{spam})\\right) + \\ln\\left(P(w_p \\mid \\text{spam})\\right) + \\ln\\left(P(w_l \\mid \\text{spam})\\right) \\right] - \\left[ \\ln\\left(P(\\text{ham})\\right) + \\ln\\left(P(w_p \\mid \\text{ham})\\right) + \\ln\\left(P(w_l \\mid \\text{ham})\\right) \\right] $$\n使用属性 $\\ln(a) - \\ln(b) = \\ln(a/b)$，我们简化两边：\n$$ n \\ln\\left(\\frac{P(w_m \\mid \\text{ham})}{P(w_m \\mid \\text{spam})}\\right) > \\ln\\left(\\frac{P(\\text{spam})P(w_p \\mid \\text{spam})P(w_l \\mid \\text{spam})}{P(\\text{ham})P(w_p \\mid \\text{ham})P(w_l \\mid \\text{ham})}\\right) $$\n现在我们代入给定的数值：\n$P(\\text{spam}) = 0.6$\n$P(\\text{ham}) = 0.4$\n$P(w_p \\mid \\text{spam}) = P(\\text{promo}\\mid \\text{spam}) = 0.012$\n$P(w_p \\mid \\text{ham}) = P(\\text{promo}\\mid \\text{ham}) = 0.003$\n$P(w_l \\mid \\text{spam}) = P(\\text{limited}\\mid \\text{spam}) = 0.010$\n$P(w_l \\mid \\text{ham}) = P(\\text{limited}\\mid \\text{ham}) = 0.004$\n$P(w_m \\mid \\text{spam}) = P(\\text{meeting}\\mid \\text{spam}) = 0.001$\n$P(w_m \\mid \\text{ham}) = P(\\text{meeting}\\mid \\text{ham}) = 0.012$\n\n将这些值代入不等式。对于左边：\n$$ n \\ln\\left(\\frac{0.012}{0.001}\\right) = n \\ln(12) $$\n对于右边：\n$$ \\ln\\left(\\frac{0.6 \\times 0.012 \\times 0.010}{0.4 \\times 0.003 \\times 0.004}\\right) = \\ln\\left(\\frac{0.000072}{0.0000048}\\right) = \\ln\\left(\\frac{720}{48}\\right) = \\ln(15) $$\n计算可以简化为：\n$$ \\ln\\left( \\frac{0.6}{0.4} \\times \\frac{0.012}{0.003} \\times \\frac{0.010}{0.004} \\right) = \\ln\\left( 1.5 \\times 4 \\times 2.5 \\right) = \\ln(6 \\times 2.5) = \\ln(15) $$\n不等式变为：\n$$ n \\ln(12) > \\ln(15) $$\n因为 $12 > 1$，$\\ln(12)$ 是一个正数。我们可以用 $\\ln(12)$ 除两边而不改变不等式的方向：\n$$ n > \\frac{\\ln(15)}{\\ln(12)} $$\n为了求出数值：\n$$ n > \\frac{2.70805...}{2.48491...} \\approx 1.0898... $$\n问题要求满足此条件的最小正整数 $n$。由于 $n$ 必须是整数且大于约 $1.0898$，因此 $n$ 的最小整数值为 $2$。\n当 $n=1$ 时，不等式 $1 > 1.0898...$ 为假。当 $n=2$ 时，不等式 $2 > 1.0898...$ 为真。因此，最小正整数是 $2$。",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}