{
    "hands_on_practices": [
        {
            "introduction": "The power of the Naive Bayes classifier stems from its \"naive\" assumption that all features are conditionally independent given the class. While this simplification makes computation highly efficient, it is often violated by real-world data. This practice problem  provides a clear, quantitative look at the consequences of this violation by constructing an adversarial scenario where two features are perfectly correlated, demonstrating how the classifier \"double counts\" the evidence and inflates its confidence.",
            "id": "3152503",
            "problem": "Consider a binary classification problem with class variable $Y \\in \\{1,0\\}$ and a single binary latent signal $Z \\in \\{1,0\\}$ that generates two observed features by adversarial duplication: $X_{1} = Z$ and $X_{2} = Z$. The class prior is $P(Y=1) = \\pi$ with $\\pi = 0.3$, hence $P(Y=0) = 1 - \\pi = 0.7$. The conditional distribution of the latent signal is Bernoulli with parameters $P(Z=1 \\mid Y=1) = \\theta_{1}$ and $P(Z=1 \\mid Y=0) = \\theta_{0}$, where $\\theta_{1} = 0.8$ and $\\theta_{0} = 0.4$. You observe a new instance $x = (x_{1}, x_{2})$ with $x_{1} = 1$ and $x_{2} = 1$.\n\nStarting only from Bayes’ rule and the definition of the naive conditional independence assumption used by the naive Bayes classifier (NBC), derive the exact posterior log-odds $\\ln\\!\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)$ under the true generative model for $(X_{1}, X_{2})$ and the posterior log-odds under NBC that incorrectly treats $X_{1}$ and $X_{2}$ as conditionally independent given $Y$. Then compute the inflation amount\n$$\n\\Delta \\;=\\; \\ln\\!\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{NBC}} \\;-\\; \\ln\\!\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{true}}.\n$$\nProvide the numerical value of $\\Delta$ rounded to four significant figures.",
            "solution": "The problem requires the calculation of the inflation amount $\\Delta$, which is the difference between the posterior log-odds computed by a naive Bayes classifier (NBC) and the true posterior log-odds for a given observation. The observation is $x = (x_1, x_2)$ with $x_1 = 1$ and $x_2 = 1$.\n\nThe posterior log-odds of class $Y=1$ versus $Y=0$ given an observation $x$ is derived from Bayes' rule:\n$$\n\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)} = \\frac{P(x \\mid Y=1) P(Y=1)}{P(x \\mid Y=0) P(Y=0)}\n$$\nTaking the natural logarithm of both sides gives the log-odds:\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right) = \\ln\\left(\\frac{P(x \\mid Y=1)}{P(x \\mid Y=0)}\\right) + \\ln\\left(\\frac{P(Y=1)}{P(Y=0)}\\right)\n$$\nThe first term on the right is the log-likelihood ratio, and the second term is the log-prior odds. The class prior is given as $P(Y=1) = \\pi = 0.3$, so $P(Y=0) = 1-\\pi = 0.7$. The log-prior odds term is constant for both the true model and the NBC model:\n$$\n\\ln\\left(\\frac{P(Y=1)}{P(Y=0)}\\right) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) = \\ln\\left(\\frac{0.3}{0.7}\\right)\n$$\n\nFirst, we calculate the posterior log-odds under the true generative model.\nIn the true model, the features are generated by a latent signal $Z$ such that $X_1 = Z$ and $X_2 = Z$. For the given observation $x_1=1$ and $x_2=1$, the event $(X_1=1, X_2=1)$ is identical to the event $(Z=1)$.\nTherefore, the true conditional likelihood of observing $x=(1,1)$ given the class $Y=y$ is:\n$$\nP(x=(1,1) \\mid Y=y)_{\\text{true}} = P(X_1=1, X_2=1 \\mid Y=y) = P(Z=1 \\mid Y=y)\n$$\nThe problem provides the conditional distributions for $Z$:\nFor $y=1$: $P(Z=1 \\mid Y=1) = \\theta_1 = 0.8$.\nFor $y=0$: $P(Z=1 \\mid Y=0) = \\theta_0 = 0.4$.\nThe true log-likelihood ratio for the observation $x=(1,1)$ is:\n$$\n\\ln\\left(\\frac{P(x=(1,1) \\mid Y=1)_{\\text{true}}}{P(x=(1,1) \\mid Y=0)_{\\text{true}}}\\right) = \\ln\\left(\\frac{P(Z=1 \\mid Y=1)}{P(Z=1 \\mid Y=0)}\\right) = \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right)\n$$\nThe true posterior log-odds are:\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{true}} = \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\n$$\n\nNext, we calculate the posterior log-odds under the naive Bayes classifier (NBC) model.\nThe NBC incorrectly assumes that the features $X_1$ and $X_2$ are conditionally independent given the class $Y$. The likelihood is thus factored as:\n$$\nP(x=(1,1) \\mid Y=y)_{\\text{NBC}} = P(X_1=1 \\mid Y=y) P(X_2=1 \\mid Y=y)\n$$\nTo use this formula, we must first find the marginal conditional probabilities $P(X_j=1 \\mid Y=y)$. These are determined by the true generative process, as they represent the parameters the NBC would learn from an infinite dataset. We marginalize over the latent variable $Z$:\n$$\nP(X_j=1 \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_j=1, Z=z \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_j=1 \\mid Z=z, Y=y) P(Z=z \\mid Y=y)\n$$\nThe generative structure is $Y \\to Z \\to X_j$, which implies that $X_j$ is conditionally independent of $Y$ given $Z$. Thus, $P(X_j=1 \\mid Z=z, Y=y) = P(X_j=1 \\mid Z=z)$.\nSince $X_j=Z$, we have $P(X_j=1 \\mid Z=1) = 1$ and $P(X_j=1 \\mid Z=0) = 0$.\nSubstituting these in, we get:\n$$\nP(X_j=1 \\mid Y=y) = 1 \\cdot P(Z=1 \\mid Y=y) + 0 \\cdot P(Z=0 \\mid Y=y) = P(Z=1 \\mid Y=y)\n$$\nSo, the required marginals are:\n$P(X_j=1 \\mid Y=1) = P(Z=1 \\mid Y=1) = \\theta_1 = 0.8$.\n$P(X_j=1 \\mid Y=0) = P(Z=1 \\mid Y=0) = \\theta_0 = 0.4$.\n\nUnder the NBC assumption, the conditional likelihoods for $x=(1,1)$ are:\n$P(x=(1,1) \\mid Y=1)_{\\text{NBC}} = P(X_1=1 \\mid Y=1) P(X_2=1 \\mid Y=1) = \\theta_1 \\cdot \\theta_1 = \\theta_1^2$.\n$P(x=(1,1) \\mid Y=0)_{\\text{NBC}} = P(X_1=1 \\mid Y=0) P(X_2=1 \\mid Y=0) = \\theta_0 \\cdot \\theta_0 = \\theta_0^2$.\nThe NBC log-likelihood ratio is:\n$$\n\\ln\\left(\\frac{P(x=(1,1) \\mid Y=1)_{\\text{NBC}}}{P(x=(1,1) \\mid Y=0)_{\\text{NBC}}}\\right) = \\ln\\left(\\frac{\\theta_1^2}{\\theta_0^2}\\right) = 2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right)\n$$\nThe NBC posterior log-odds are:\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{NBC}} = 2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\n$$\nThe inflation amount $\\Delta$ is the difference between the NBC log-odds and the true log-odds:\n$$\n\\Delta = \\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{NBC}} - \\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{true}}\n$$\n$$\n\\Delta = \\left(2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\right) - \\left(\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\right)\n$$\nThe log-prior odds term cancels out, leaving:\n$$\n\\Delta = 2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) - \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) = \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right)\n$$\nThe inflation amount represents the degree to which the NBC double-counts the evidence from the single latent source $Z$, which was duplicated into two perfectly correlated features.\nSubstituting the given numerical values $\\theta_1=0.8$ and $\\theta_0=0.4$:\n$$\n\\Delta = \\ln\\left(\\frac{0.8}{0.4}\\right) = \\ln(2)\n$$\nThe numerical value of $\\ln(2)$ is approximately $0.693147...$. Rounding to four significant figures, we get:\n$$\n\\Delta \\approx 0.6931\n$$",
            "answer": "$$\\boxed{0.6931}$$"
        },
        {
            "introduction": "One of the most classic applications of Naive Bayes is in text classification, such as spam filtering. This exercise  places you in the role of an analyst examining an adversarial attack, where a user attempts to fool a spam filter by injecting specific words. By working through this problem, you will gain practical experience using the log-odds formulation to analyze the classifier's decision boundary and determine the precise threshold for flipping its prediction.",
            "id": "3152525",
            "problem": "A company deploys a content-based email filter modeled as Multinomial Naive Bayes (MNB), treating messages as bags of word tokens with class-conditional token probabilities and assuming conditional independence of tokens given the class. The classes are $\\text{spam}$ and $\\text{ham}$, with prior probabilities $P(\\text{spam}) = 0.6$ and $P(\\text{ham}) = 0.4$. Consider a vocabulary large enough that most words are not present in the message below; the following three tokens have class-conditional probabilities per token occurrence: for the token “promo,” $P(\\text{promo}\\mid \\text{spam}) = 0.012$ and $P(\\text{promo}\\mid \\text{ham}) = 0.003$; for the token “limited,” $P(\\text{limited}\\mid \\text{spam}) = 0.010$ and $P(\\text{limited}\\mid \\text{ham}) = 0.004$; for the token “meeting,” $P(\\text{meeting}\\mid \\text{spam}) = 0.001$ and $P(\\text{meeting}\\mid \\text{ham}) = 0.012$. An original message contains $1$ instance of “promo,” $1$ instance of “limited,” and $0$ instances of “meeting,” with all other token counts equal to $0$. An adversary attempts to cause the classifier’s decision to flip from $\\text{spam}$ to $\\text{ham}$ by injecting $n$ additional instances of the token “meeting” into the message, without changing any other content.\n\nStarting from Bayes’ theorem and the Multinomial Naive Bayes modeling assumptions (including the independence of token occurrences given the class and the use of class-conditional token probabilities), derive the decision condition under which the posterior favors $\\text{ham}$ over $\\text{spam}$, and determine the minimal positive integer $n$ that causes the decision to flip to $\\text{ham}$. Express your reasoning using natural logarithms. The final answer must be the minimal integer $n$. No rounding instruction is necessary because an integer is required.",
            "solution": "The problem requires finding the minimal positive integer $n$ of injected \"meeting\" tokens that causes a Multinomial Naive Bayes (MNB) classifier to change its decision for a given message from $\\text{spam}$ to $\\text{ham}$.\n\nThe classification decision is based on the posterior probability of a class $c$ given a document $D$, denoted $P(c \\mid D)$. The classifier chooses the class $c$ that maximizes this posterior. According to Bayes' theorem, the posterior is given by:\n$$P(c \\mid D) = \\frac{P(D \\mid c) P(c)}{P(D)}$$\nSince the evidence $P(D)$ is a constant for all classes under consideration for a given document $D$, the decision rule is equivalent to maximizing the product of the likelihood $P(D \\mid c)$ and the prior probability $P(c)$:\n$$\\hat{c} = \\arg\\max_{c \\in \\{\\text{spam}, \\text{ham}\\}} P(D \\mid c) P(c)$$\nInitially, the message is classified as $\\text{spam}$. An adversarial modification is successful if the new message, $D_n$, is classified as $\\text{ham}$. This decision flip occurs when the posterior for $\\text{ham}$ exceeds the posterior for $\\text{spam}$:\n$$P(\\text{ham} \\mid D_n) > P(\\text{spam} \\mid D_n)$$\nThis is equivalent to the condition:\n$$P(D_n \\mid \\text{ham}) P(\\text{ham}) > P(D_n \\mid \\text{spam}) P(\\text{spam})$$\nLet the vocabulary tokens be denoted as $w_p$ for \"promo,\" $w_l$ for \"limited,\" and $w_m$ for \"meeting.\" The modified document $D_n$ contains $1$ instance of $w_p$, $1$ instance of $w_l$, and $n$ instances of $w_m$. The total number of tokens in $D_n$ is $L_n = 1 + 1 + n = n+2$.\n\nThe Multinomial Naive Bayes model assumes that the document is a bag of words, and the likelihood $P(D_n \\mid c)$ is given by the multinomial probability mass function. Let $x_p=1$, $x_l=1$, and $x_m=n$ be the counts of the respective tokens.\n$$P(D_n \\mid c) = \\frac{L_n!}{x_p! x_l! x_m!} \\left( P(w_p \\mid c) \\right)^{x_p} \\left( P(w_l \\mid c) \\right)^{x_l} \\left( P(w_m \\mid c) \\right)^{x_m}$$\nSubstituting the counts, we get:\n$$P(D_n \\mid c) = \\frac{(n+2)!}{1! \\, 1! \\, n!} \\left( P(w_p \\mid c) \\right)^1 \\left( P(w_l \\mid c) \\right)^1 \\left( P(w_m \\mid c) \\right)^n$$\nNow, we can write the decision inequality:\n$$ \\frac{(n+2)!}{n!} P(w_p \\mid \\text{ham}) P(w_l \\mid \\text{ham}) (P(w_m \\mid \\text{ham}))^n P(\\text{ham}) > \\frac{(n+2)!}{n!} P(w_p \\mid \\text{spam}) P(w_l \\mid \\text{spam}) (P(w_m \\mid \\text{spam}))^n P(\\text{spam}) $$\nThe multinomial coefficient $\\frac{(n+2)!}{n!}$ is a common factor on both sides and can be canceled. This is a general property when comparing class posteriors in Naive Bayes, as the term only depends on the document's structure, not the class. The simplified inequality is:\n$$ P(w_p \\mid \\text{ham}) P(w_l \\mid \\text{ham}) (P(w_m \\mid \\text{ham}))^n P(\\text{ham}) > P(w_p \\mid \\text{spam}) P(w_l \\mid \\text{spam}) (P(w_m \\mid \\text{spam}))^n P(\\text{spam}) $$\nAs required, we take the natural logarithm of both sides. Since the logarithm is a monotonically increasing function, the direction of the inequality is preserved:\n$$ \\ln\\left(P(w_p \\mid \\text{ham})\\right) + \\ln\\left(P(w_l \\mid \\text{ham})\\right) + n \\ln\\left(P(w_m \\mid \\text{ham})\\right) + \\ln\\left(P(\\text{ham})\\right) > \\ln\\left(P(w_p \\mid \\text{spam})\\right) + \\ln\\left(P(w_l \\mid \\text{spam})\\right) + n \\ln\\left(P(w_m \\mid \\text{spam})\\right) + \\ln\\left(P(\\textspam)\\right) $$\nTo solve for $n$, we rearrange the inequality to group terms containing $n$:\n$$ n \\ln\\left(P(w_m \\mid \\text{ham})\\right) - n \\ln\\left(P(w_m \\mid \\text{spam})\\right) > \\left[ \\ln\\left(P(\\text{spam})\\right) + \\ln\\left(P(w_p \\mid \\text{spam})\\right) + \\ln\\left(P(w_l \\mid \\text{spam})\\right) \\right] - \\left[ \\ln\\left(P(\\text{ham})\\right) + \\ln\\left(P(w_p \\mid \\text{ham})\\right) + \\ln\\left(P(w_l \\mid \\text{ham})\\right) \\right] $$\nUsing the property $\\ln(a) - \\ln(b) = \\ln(a/b)$, we simplify both sides:\n$$ n \\ln\\left(\\frac{P(w_m \\mid \\text{ham})}{P(w_m \\mid \\text{spam})}\\right) > \\ln\\left(\\frac{P(\\text{spam})P(w_p \\mid \\text{spam})P(w_l \\mid \\text{spam})}{P(\\text{ham})P(w_p \\mid \\text{ham})P(w_l \\mid \\text{ham})}\\right) $$\nNow we substitute the given numerical values:\n$P(\\text{spam}) = 0.6$\n$P(\\text{ham}) = 0.4$\n$P(w_p \\mid \\text{spam}) = P(\\text{promo}\\mid \\text{spam}) = 0.012$\n$P(w_p \\mid \\text{ham}) = P(\\text{promo}\\mid \\text{ham}) = 0.003$\n$P(w_l \\mid \\text{spam}) = P(\\text{limited}\\mid \\text{spam}) = 0.010$\n$P(w_l \\mid \\text{ham}) = P(\\text{limited}\\mid \\text{ham}) = 0.004$\n$P(w_m \\mid \\text{spam}) = P(\\text{meeting}\\mid \\text{spam}) = 0.001$\n$P(w_m \\mid \\text{ham}) = P(\\text{meeting}\\mid \\text{ham}) = 0.012$\n\nSubstitute these into the inequality. For the left-hand side:\n$$ n \\ln\\left(\\frac{0.012}{0.001}\\right) = n \\ln(12) $$\nFor the right-hand side:\n$$ \\ln\\left(\\frac{0.6 \\times 0.012 \\times 0.010}{0.4 \\times 0.003 \\times 0.004}\\right) = \\ln\\left(\\frac{0.000072}{0.0000048}\\right) = \\ln\\left(\\frac{720}{48}\\right) = \\ln(15) $$\nThe calculation can be simplified as:\n$$ \\ln\\left( \\frac{0.6}{0.4} \\times \\frac{0.012}{0.003} \\times \\frac{0.010}{0.004} \\right) = \\ln\\left( 1.5 \\times 4 \\times 2.5 \\right) = \\ln(6 \\times 2.5) = \\ln(15) $$\nThe inequality becomes:\n$$ n \\ln(12) > \\ln(15) $$\nSince $12 > 1$, $\\ln(12)$ is a positive number. We can divide by $\\ln(12)$ without changing the inequality's direction:\n$$ n > \\frac{\\ln(15)}{\\ln(12)} $$\nTo find the numerical value:\n$$ n > \\frac{2.70805...}{2.48491...} \\approx 1.0898... $$\nThe problem asks for the minimal positive integer $n$ that satisfies this condition. Since $n$ must be an integer and greater than approximately $1.0898$, the smallest integer value for $n$ is $2$.\nFor $n=1$, the inequality $1 > 1.0898...$ is false. For $n=2$, the inequality $2 > 1.0898...$ is true. Therefore, the minimal positive integer is $2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "It is a common misconception that the \"naive\" assumption leads to simple, linear decision boundaries. While this is true in some cases, the reality is more nuanced. This exercise  delves into the mathematics of the Gaussian Naive Bayes classifier, guiding you to derive a key result: when the class-conditional variances are unequal, the decision boundary is not a line but a quadratic curve. This reveals a surprising degree of flexibility in the model and deepens your understanding of its geometric properties.",
            "id": "3152506",
            "problem": "A binary classification task involves a single real-valued feature $x \\in \\mathbb{R}$ and class label $y \\in \\{0,1\\}$. Consider the following labeled dataset of eight observations:\n- Class $y=1$: $x \\in \\{2, 3, 4, 5\\}$.\n- Class $y=0$: $x \\in \\{0, 1, 1, 2\\}$.\nAssume the Naive Bayes classifier (NBC) with the following modeling and estimation choices:\n1. The class prior $P(y=c)$ is estimated by the empirical class frequency for $c \\in \\{0,1\\}$.\n2. The class-conditional distribution of $x$ given $y=c$ is modeled as a Gaussian (normal) distribution with mean $\\mu_{c}$ and variance $\\sigma_{c}^{2}$, and these parameters are estimated by maximum likelihood from the data for each class, that is, $\\mu_{c}$ is the sample mean and $\\sigma_{c}^{2}$ is the average of squared deviations from $\\mu_{c}$ over the samples in class $c$.\nStarting from Bayes' rule and the Gaussian probability density function, derive the exact closed-form expression for the log posterior odds $\\ln\\!\\left(\\frac{P(y=1 \\mid x)}{P(y=0 \\mid x)}\\right)$ as a quadratic function of $x$. Express your final answer in exact analytic form with all logarithms left as $\\ln(\\cdot)$ and do not round; no numerical approximation is required. Your final expression must be a single analytic expression in $x$.",
            "solution": "The problem asks for the derivation of the log posterior odds for a Naive Bayes classifier (NBC). The task is to find a closed-form expression for $\\ln\\left(\\frac{P(y=1 \\mid x)}{P(y=0 \\mid x)}\\right)$ as a quadratic function of the feature $x$.\n\nFirst, we begin with Bayes' rule for the posterior probability of a class $c \\in \\{0,1\\}$ given the feature $x$:\n$$P(y=c \\mid x) = \\frac{P(x \\mid y=c) P(y=c)}{P(x)}$$\nThe posterior odds are the ratio of the posterior probabilities for class $y=1$ and class $y=0$:\n$$\\frac{P(y=1 \\mid x)}{P(y=0 \\mid x)} = \\frac{P(x \\mid y=1) P(y=1) / P(x)}{P(x \\mid y=0) P(y=0) / P(x)} = \\frac{P(x \\mid y=1) P(y=1)}{P(x \\mid y=0) P(y=0)}$$\nThe term $P(x)$ cancels out. Taking the natural logarithm of this ratio gives the log posterior odds:\n$$\\ln\\left(\\frac{P(y=1 \\mid x)}{P(y=0 \\mid x)}\\right) = \\ln\\left(\\frac{P(x \\mid y=1)}{P(x \\mid y=0)}\\right) + \\ln\\left(\\frac{P(y=1)}{P(y=0)}\\right)$$\nThis expression consists of the log-likelihood ratio and the log-prior odds. We must now calculate each component based on the provided data and model assumptions.\n\n1.  **Calculate the Priors and Log-Prior Odds**\n    The problem specifies that the class prior $P(y=c)$ is the empirical class frequency. The dataset has a total of $N_{total} = 8$ observations.\n    -   For class $y=1$, the data are $x \\in \\{2, 3, 4, 5\\}$, so the number of samples is $N_1 = 4$.\n    -   For class $y=0$, the data are $x \\in \\{0, 1, 1, 2\\}$, so the number of samples is $N_0 = 4$.\n\n    The prior probabilities are:\n    $$P(y=1) = \\frac{N_1}{N_{total}} = \\frac{4}{8} = \\frac{1}{2}$$\n    $$P(y=0) = \\frac{N_0}{N_{total}} = \\frac{4}{8} = \\frac{1}{2}$$\n    The log-prior odds term is therefore:\n    $$\\ln\\left(\\frac{P(y=1)}{P(y=0)}\\right) = \\ln\\left(\\frac{1/2}{1/2}\\right) = \\ln(1) = 0$$\n\n2.  **Calculate the Class-Conditional Likelihood Parameters**\n    The class-conditional distribution $P(x \\mid y=c)$ is modeled as a Gaussian $N(\\mu_c, \\sigma_c^2)$. The parameters $\\mu_c$ and $\\sigma_c^2$ are estimated using maximum likelihood estimation (MLE). For a Gaussian, the MLE of the mean is the sample mean, and the MLE of the variance is the sample variance with a denominator of $N_c$ (the number of samples in class $c$).\n\n    -   **Parameters for class $y=1$ ($c=1$):**\n        Data: $\\{2, 3, 4, 5\\}$.\n        Sample mean $\\mu_1$:\n        $$\\mu_1 = \\frac{2 + 3 + 4 + 5}{4} = \\frac{14}{4} = \\frac{7}{2}$$\n        Sample variance $\\sigma_1^2$:\n        $$\\sigma_1^2 = \\frac{1}{N_1} \\sum_{i=1}^{N_1} (x_i - \\mu_1)^2 = \\frac{1}{4} \\left[ \\left(2 - \\frac{7}{2}\\right)^2 + \\left(3 - \\frac{7}{2}\\right)^2 + \\left(4 - \\frac{7}{2}\\right)^2 + \\left(5 - \\frac{7}{2}\\right)^2 \\right]$$\n        $$\\sigma_1^2 = \\frac{1}{4} \\left[ \\left(-\\frac{3}{2}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 \\right] = \\frac{1}{4} \\left[ \\frac{9}{4} + \\frac{1}{4} + \\frac{1}{4} + \\frac{9}{4} \\right] = \\frac{1}{4} \\left( \\frac{20}{4} \\right) = \\frac{5}{4}$$\n\n    -   **Parameters for class $y=0$ ($c=0$):**\n        Data: $\\{0, 1, 1, 2\\}$.\n        Sample mean $\\mu_0$:\n        $$\\mu_0 = \\frac{0 + 1 + 1 + 2}{4} = \\frac{4}{4} = 1$$\n        Sample variance $\\sigma_0^2$:\n        $$\\sigma_0^2 = \\frac{1}{N_0} \\sum_{i=1}^{N_0} (x_i - \\mu_0)^2 = \\frac{1}{4} \\left[ (0 - 1)^2 + (1 - 1)^2 + (1 - 1)^2 + (2 - 1)^2 \\right]$$\n        $$\\sigma_0^2 = \\frac{1}{4} \\left[ (-1)^2 + 0^2 + 0^2 + 1^2 \\right] = \\frac{1}{4} [1 + 0 + 0 + 1] = \\frac{2}{4} = \\frac{1}{2}$$\n\n3.  **Derive the Log-Likelihood Ratio**\n    The probability density function for a Gaussian distribution is:\n    $$P(x \\mid y=c) = \\frac{1}{\\sqrt{2\\pi\\sigma_c^2}} \\exp\\left(-\\frac{(x - \\mu_c)^2}{2\\sigma_c^2}\\right)$$\n    The logarithm of this density is:\n    $$\\ln(P(x \\mid y=c)) = -\\frac{1}{2}\\ln(2\\pi\\sigma_c^2) - \\frac{(x - \\mu_c)^2}{2\\sigma_c^2}$$\n    The log-likelihood ratio is $\\ln(P(x \\mid y=1)) - \\ln(P(x \\mid y=0))$:\n    $$\\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_1^2) - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2}\\right) - \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_0^2) - \\frac{(x - \\mu_0)^2}{2\\sigma_0^2}\\right)$$\n    $$= \\frac{1}{2}\\ln\\left(\\frac{2\\pi\\sigma_0^2}{2\\pi\\sigma_1^2}\\right) + \\frac{(x - \\mu_0)^2}{2\\sigma_0^2} - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2}$$\n    $$= \\frac{1}{2}\\ln\\left(\\frac{\\sigma_0^2}{\\sigma_1^2}\\right) + \\frac{(x - \\mu_0)^2}{2\\sigma_0^2} - \\frac{(x - \\mu_1)^2}{2\\sigma_1^2}$$\n\n4.  **Assemble the Final Expression**\n    Since the log-prior odds term is $0$, the log posterior odds is equal to the log-likelihood ratio. We substitute the calculated parameter values:\n    $\\mu_0 = 1$, $\\sigma_0^2 = \\frac{1}{2}$\n    $\\mu_1 = \\frac{7}{2}$, $\\sigma_1^2 = \\frac{5}{4}$\n\n    Log posterior odds:\n    $$= \\frac{1}{2}\\ln\\left(\\frac{1/2}{5/4}\\right) + \\frac{(x - 1)^2}{2(1/2)} - \\frac{(x - 7/2)^2}{2(5/4)}$$\n    $$= \\frac{1}{2}\\ln\\left(\\frac{1}{2} \\cdot \\frac{4}{5}\\right) + \\frac{(x - 1)^2}{1} - \\frac{(x - 7/2)^2}{5/2}$$\n    $$= \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right) + (x^2 - 2x + 1) - \\frac{2}{5}\\left(x^2 - 7x + \\frac{49}{4}\\right)$$\n    Now, expand and collect terms:\n    $$= \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right) + x^2 - 2x + 1 - \\frac{2}{5}x^2 + \\frac{14}{5}x - \\frac{49}{10}$$\n    Combine coefficients for powers of $x$:\n    -   Coefficient of $x^2$: $1 - \\frac{2}{5} = \\frac{3}{5}$\n    -   Coefficient of $x$: $-2 + \\frac{14}{5} = -\\frac{10}{5} + \\frac{14}{5} = \\frac{4}{5}$\n    -   Constant term: $1 - \\frac{49}{10} + \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right) = \\frac{10}{10} - \\frac{49}{10} + \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right) = -\\frac{39}{10} + \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right)$\n\n    Putting all terms together, the log posterior odds as a quadratic function of $x$ is:\n    $$\\frac{3}{5}x^2 + \\frac{4}{5}x - \\frac{39}{10} + \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right)$$",
            "answer": "$$\\boxed{\\frac{3}{5}x^2 + \\frac{4}{5}x - \\frac{39}{10} + \\frac{1}{2}\\ln\\left(\\frac{2}{5}\\right)}$$"
        }
    ]
}