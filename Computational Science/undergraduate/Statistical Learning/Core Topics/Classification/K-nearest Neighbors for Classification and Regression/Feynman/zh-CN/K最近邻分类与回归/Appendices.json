{
    "hands_on_practices": [
        {
            "introduction": "评估模型性能是机器学习中的关键步骤。一个常见的误区是使用模型在训练数据上的表现（即训练误差）来衡量其好坏。这种方法可能极具误导性，因为它无法反映模型在面对新数据时的泛化能力。本练习  将通过对比 1-NN 分类器近乎完美的训练误差与其更为真实的留一法交叉验证误差，直观地揭示过拟合问题，并强调可靠评估方法的重要性。",
            "id": "3135589",
            "problem": "给定一个分类场景，其包含一个有限的训练数据集，其中每个观测是一个对 $(\\mathbf{x}_i, y_i)$，包含特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 和类别标签 $y_i \\in \\{0,1\\}$，对于 $i = 1, \\dots, n$。基本基础包括：(i) 欧几里得距离的定义 $d(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{x} - \\mathbf{z}\\|_2$，(ii) $k$-近邻规则，该规则选择与查询点距离最小的 $k$ 个训练观测，(iii) 在 $0$-$1$ 损失下定义的经验风险（重代入误差），即当使用指定的分类器和完整的训练集对每个训练点进行预测时，所产生的平均误分类数，以及 (iv) 留一法交叉验证，它通过对每个训练点，使用移除了该点的训练集对其进行预测，并对由此产生的 $0$-$1$ 损失求平均来评估分类器。在所有计算中，距离的平局必须确定性地打破：当按距离对邻居进行排序时，首先按距离升序排序，如果点本身存在，则优先考虑它，最后通过索引升序来打破任何剩余的平局。对于 $k \\ge 2$ 的分类中的多数投票，如果 $k$ 个邻居的类别计数出现平局，则选择最小的类别标签。\n\n任务 A：从上述定义出发，确认当每个训练点的邻居集包含该点本身时，$k=1$ 近邻分类器的经验风险（重代入误差）为 $0$。然后，计算 $k=1$ 的留一法误差，并将其与下面指定的数据集上 $k=2$ 的重代入误差进行比较。该比较必须基于错误率的数值。\n\n任务 B：基于观察到的数值比较，分析使用重代入误差与留一法误差在 $k=1$ 和 $k=2$ 之间进行模型选择的影响，将您的讨论建立在所提供的定义之上（您的分析将在您的解决方案中呈现，而不是由代码打印）。\n\n使用以下数据集测试套件，每个数据集由一个特征矩阵和一个标签向量给出。所有特征都是二维的，$d = 2$，所有类别标签都在 $\\{0,1\\}$ 中。距离为欧几里得距离。不涉及物理单位。\n\n- 数据集 $1$（类别分离良好）：\n  - 特征 $X_1$：$$\\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 5.0 & 5.0 \\\\ 5.0 & 6.0 \\end{array}\\right]$$\n  - 标签 $y_1$：$[0, 0, 1, 1]$\n- 数据集 $2$（边界附近有重叠）：\n  - 特征 $X_2$：$$\\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 0.1 \\\\ 0.05 & 0.0 \\end{array}\\right]$$\n  - 标签 $y_2$：$[0, 0, 1]$\n- 数据集 $3$（带有一个噪声点的混合）：\n  - 特征 $X_3$：$$\\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 2.0 \\\\ 2.0 & 0.0 \\\\ 2.0 & 2.0 \\\\ 1.1 & 1.0 \\end{array}\\right]$$\n  - 标签 $y_3$：$[0, 0, 1, 1, 0]$\n\n对于每个数据集 $j \\in \\{1,2,3\\}$，计算：\n- $E_{j}^{\\text{train}, k=1}$：$k=1$ 的经验风险（重代入误差），邻居中包括点本身，\n- $E_{j}^{\\text{LOO}, k=1}$：$k=1$ 的留一法误差，邻居中不包括点本身，\n- $E_{j}^{\\text{train}, k=2}$：$k=2$ 的经验风险（重代入误差），邻居中包括点本身，采用多数投票和指定的平局打破规则。\n\n您的程序应生成一行输出，其中包含一个由三个列表组成的列表，每个数据集一个，其中每个内部列表为 $[E_{j}^{\\text{train}, k=1}, E_{j}^{\\text{LOO}, k=1}, E_{j}^{\\text{train}, k=2}]$。格式必须是用方括号括起来的逗号分隔列表，例如：“[[e11,e12,e13],[e21,e22,e23],[e31,e32,e33]]”。所有值都必须是 $[0,1]$ 范围内的浮点数，表示在指定评估设置下训练点的平均 $0$-$1$ 损失。",
            "solution": "对问题陈述的有效性进行评估。\n\n### 步骤 1：提取已知条件\n- **数据集**：一个包含 $n$ 个观测的有限训练数据集，每个观测都是一个对 $(\\mathbf{x}_i, y_i)$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是一个特征向量，而 $y_i \\in \\{0,1\\}$ 是一个类别标签，对于 $i = 1, \\dots, n$。\n- **距离度量**：欧几里得距离，$d(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{x} - \\mathbf{z}\\|_2$。\n- **分类器规则**：$k$-近邻（$k$-NN）规则选择与查询点距离最小的 $k$ 个训练观测。\n- **误差度量**：\n    - 在 $0$-$1$ 损失下的**经验风险（重代入误差）**是指当使用在完整训练集上训练的分类器对每个训练点进行预测时，所产生的平均误分类数。\n    - **留一法交叉验证（LOOCV）误差**是指平均 $0$-$1$ 损失，其中每个训练点的预测是使用在所有其他点组成的集合上训练的分类器进行的。\n- **平局打破规则**：\n    - **距离**：按距离升序对邻居进行排序。如果距离相同，若点本身存在则优先考虑。任何剩余的平局通过索引 $i$ 的升序来打破。\n    - **多数投票**：对于 $k \\ge 2$ 的分类，如果 $k$ 个邻居的类别计数出现平局，则选择最小的类别标签（即类别 $0$）。\n- **任务**：\n    - **任务 A**：确认 $k=1$ 的经验风险为 $0$。计算 $k=1$ 的 LOOCV 误差和 $k=2$ 的经验风险，用于三个给定的数据集。\n    - **任务 B**：根据数值结果分析模型选择的影响。\n- **数据集**：提供了三个数据集，每个数据集的 $d=2$ 且 $y_i \\in \\{0,1\\}$。\n    - 数据集 $1$：$$X_1 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 5.0 & 5.0 \\\\ 5.0 & 6.0 \\end{array}\\right], \\quad y_1 = [0, 0, 1, 1]$$\n    - 数据集 $2$：$$X_2 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 0.1 \\\\ 0.05 & 0.0 \\end{array}\\right], \\quad y_2 = [0, 0, 1]$$\n    - 数据集 $3$：$$X_3 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 2.0 \\\\ 2.0 & 0.0 \\\\ 2.0 & 2.0 \\\\ 1.1 & 1.0 \\end{array}\\right], \\quad y_3 = [0, 0, 1, 1, 0]$$\n- **要求计算**：对于每个数据集 $j \\in \\{1,2,3\\}$，计算 $E_{j}^{\\text{train}, k=1}$、$E_{j}^{\\text{LOO}, k=1}$ 和 $E_{j}^{\\text{train}, k=2}$。\n- **输出格式**：一个包含三个列表的列表：`[[e11,e12,e13],[e21,e22,e23],[e31,e32,e33]]`。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学上成立**：该问题基于统计学习和模式识别中的基本和标准概念，即 $k$-NN 算法、欧几里得距离以及标准的误差估计方法（重代入和交叉验证）。所有定义都是正确和标准的。\n- **定义良好**：问题定义良好。它提供了所有必要的数据、明确的算法和确定性的平局打破规则，确保了唯一解的存在且可计算。任务是具体且可量化的。\n- **客观性**：问题以精确、客观和形式化的数学语言陈述，没有主观性或歧义。\n\n该问题没有表现出任何诸如科学上不健全、不完整、矛盾或歧义等缺陷。它是统计学习领域一个定义明确的计算和分析练习。\n\n### 步骤 3：结论与行动\n问题有效。将提供一个合理的解决方案。\n\n---\n\n### 任务 A：确认与计算\n\n**第一部分：确认 k=1 时的零经验风险**\n\n分类器的经验风险，或重代入误差，是通过将分类器应用于训练集中的每个点 $\\mathbf{x}_i$，并根据已知标签 $y_i$ 衡量预测误差来计算的。对于 $k=1$ 近邻分类器，我们必须在完整的训练集 $\\{(\\mathbf{x}_j, y_j)\\}_{j=1}^n$ 中找到距离 $\\mathbf{x}_i$ 最近的单个点。\n\n根据定义，一个点到其自身的欧几里得距离为 $d(\\mathbf{x}_i, \\mathbf{x}_i) = 0$。对于任何其他不同的点 $\\mathbf{x}_j$（其中 $j \\ne i$），距离 $d(\\mathbf{x}_i, \\mathbf{x}_j)$ 必须大于 $0$。因此，任何训练点 $\\mathbf{x}_i$ 的最近邻始终是点 $\\mathbf{x}_i$ 本身。问题的平局打破规则明确规定，如果点本身存在，则优先考虑它，从而强化了这一结论。\n\n因此，对 $\\mathbf{x}_i$ 标签的 $1$-NN 预测（表示为 $\\hat{y}_i$）是其最近邻的标签，即 $y_i$。该预测的 $0$-$1$ 损失为 $L_{0-1}(\\hat{y}_i, y_i) = L_{0-1}(y_i, y_i) = 0$。由于对训练集中的每个点预测都是正确的，所以总误差数为 $0$。经验风险作为平均损失，因此为 $E^{\\text{train}, k=1} = \\frac{1}{n} \\sum_{i=1}^n 0 = 0$。这对于所有训练点都唯一的数据集普遍成立。因此，对于所有数据集 $j \\in \\{1,2,3\\}$，$E_{j}^{\\text{train}, k=1} = 0$。\n\n**第二部分：数值计算**\n\n我们现在为每个数据集计算 $E_{j}^{\\text{LOO}, k=1}$ 和 $E_{j}^{\\text{train}, k=2}$。\n\n**数据集 1**：$X_1 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 1.0 \\\\ 5.0 & 5.0 \\\\ 5.0 & 6.0 \\end{array}\\right]$, $y_1 = [0, 0, 1, 1]$, $n=4$.\n- **$E_{1}^{\\text{LOO}, k=1}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$：在 $\\{\\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4\\}$ 中的最近点是 $\\mathbf{x}_2$ (距离 $1.0$)。预测 $\\hat{y}_1=y_2=0$。正确。\n    - 对于 $\\mathbf{x}_2=(0.0, 1.0)$, $y_2=0$：在 $\\{\\mathbf{x}_1, \\mathbf{x}_3, \\mathbf{x}_4\\}$ 中的最近点是 $\\mathbf{x}_1$ (距离 $1.0$)。预测 $\\hat{y}_2=y_1=0$。正确。\n    - 对于 $\\mathbf{x}_3=(5.0, 5.0)$, $y_3=1$：在 $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_4\\}$ 中的最近点是 $\\mathbf{x}_4$ (距离 $1.0$)。预测 $\\hat{y}_3=y_4=1$。正确。\n    - 对于 $\\mathbf{x}_4=(5.0, 6.0)$, $y_4=1$：在 $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\}$ 中的最近点是 $\\mathbf{x}_3$ (距离 $1.0$)。预测 $\\hat{y}_4=y_3=1$。正确。\n    总误差：$0$。$E_{1}^{\\text{LOO}, k=1} = 0/4 = 0.0$。\n- **$E_{1}^{\\text{train}, k=2}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$：邻居是其自身和 $\\mathbf{x}_2$。标签为 $\\{0, 0\\}$。多数投票为 $0$。正确。\n    - 对于 $\\mathbf{x}_2=(0.0, 1.0)$, $y_2=0$：邻居是其自身和 $\\mathbf{x}_1$。标签为 $\\{0, 0\\}$。多数投票为 $0$。正确。\n    - 对于 $\\mathbf{x}_3=(5.0, 5.0)$, $y_3=1$：邻居是其自身和 $\\mathbf{x}_4$。标签为 $\\{1, 1\\}$。多数投票为 $1$。正确。\n    - 对于 $\\mathbf{x}_4=(5.0, 6.0), y_4=1$：邻居是其自身和 $\\mathbf{x}_3$。标签为 $\\{1, 1\\}$。多数投票为 $1$。正确。\n    总误差：$0$。$E_{1}^{\\text{train}, k=2} = 0/4 = 0.0$。\n\n数据集 1 的结果：$[0.0, 0.0, 0.0]$。\n\n**数据集 2**：$X_2 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 0.1 \\\\ 0.05 & 0.0 \\end{array}\\right]$, $y_2 = [0, 0, 1]$, $n=3$.\n- **$E_{2}^{\\text{LOO}, k=1}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$：到 $\\mathbf{x}_2, \\mathbf{x}_3$ 的距离分别为 $0.1, 0.05$。最近的是 $\\mathbf{x}_3$。预测 $\\hat{y}_1=y_3=1$。错误。\n    - 对于 $\\mathbf{x}_2=(0.0, 0.1)$, $y_2=0$：到 $\\mathbf{x}_1, \\mathbf{x}_3$ 的距离分别为 $0.1, \\sqrt{0.05^2+0.1^2} \\approx 0.112$。最近的是 $\\mathbf{x}_1$。预测 $\\hat{y}_2=y_1=0$。正确。\n    - 对于 $\\mathbf{x}_3=(0.05, 0.0)$, $y_3=1$：到 $\\mathbf{x}_1, \\mathbf{x}_2$ 的距离分别为 $0.05, \\sqrt{0.05^2+0.1^2} \\approx 0.112$。最近的是 $\\mathbf{x}_1$。预测 $\\hat{y}_3=y_1=0$。错误。\n    总误差：$2$。$E_{2}^{\\text{LOO}, k=1} = 2/3$。\n- **$E_{2}^{\\text{train}, k=2}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$, $y_1=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_3$（标签 $1$）。平局。预测为 $0$。正确。\n    - 对于 $\\mathbf{x}_2=(0.0, 0.1)$, $y_2=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_1$（标签 $0$）。多数投票为 $0$。正确。\n    - 对于 $\\mathbf{x}_3=(0.05, 0.0)$, $y_3=1$：邻居是其自身（标签 $1$）和 $\\mathbf{x}_1$（标签 $0$）。平局。预测为 $0$。错误。\n    总误差：$1$。$E_{2}^{\\text{train}, k=2} = 1/3$。\n\n数据集 2 的结果：$[0.0, 2/3, 1/3]$。\n\n**数据集 3**：$X_3 = \\left[\\begin{array}{cc} 0.0 & 0.0 \\\\ 0.0 & 2.0 \\\\ 2.0 & 0.0 \\\\ 2.0 & 2.0 \\\\ 1.1 & 1.0 \\end{array}\\right]$, $y_3=[0, 0, 1, 1, 0]$, $n=5$.\n- **$E_{3}^{\\text{LOO}, k=1}$**：\n    - 对于 $\\mathbf{x}_1=(0,0), y_1=0$：最近的是 $\\mathbf{x}_5$ (距离 $\\sqrt{1.1^2+1^2} \\approx 1.487$)。预测 $\\hat{y}_1=y_5=0$。正确。\n    - 对于 $\\mathbf{x}_2=(0,2), y_2=0$：最近的是 $\\mathbf{x}_5$ (距离 $\\sqrt{1.1^2+(-1)^2} \\approx 1.487$)。预测 $\\hat{y}_2=y_5=0$。正确。\n    - 对于 $\\mathbf{x}_3=(2,0), y_3=1$：最近的是 $\\mathbf{x}_5$ (距离 $\\sqrt{(-0.9)^2+1^2} \\approx 1.345$)。预测 $\\hat{y}_3=y_5=0$。错误。\n    - 对于 $\\mathbf{x}_4=(2,2), y_4=1$：最近的是 $\\mathbf{x}_5$ (距离 $\\sqrt{(-0.9)^2+(-1)^2} \\approx 1.345$)。预测 $\\hat{y}_4=y_5=0$。错误。\n    - 对于 $\\mathbf{x}_5=(1.1,1), y_5=0$：最近的是 $\\mathbf{x}_3$ (距离 $\\approx 1.345$)。预测 $\\hat{y}_5=y_3=1$。错误。\n    总误差：$3$。$E_{3}^{\\text{LOO}, k=1} = 3/5 = 0.6$。\n- **$E_{3}^{\\text{train}, k=2}$**：\n    - 对于 $\\mathbf{x}_1=(0,0), y_1=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_5$（标签 $0$）。多数投票 $0$。正确。\n    - 对于 $\\mathbf{x}_2=(0,2), y_2=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_5$（标签 $0$）。多数投票 $0$。正确。\n    - 对于 $\\mathbf{x}_3=(2,0), y_3=1$：邻居是其自身（标签 $1$）和 $\\mathbf{x}_5$（标签 $0$）。平局。预测 $0$。错误。\n    - 对于 $\\mathbf{x}_4=(2,2), y_4=1$：邻居是其自身（标签 $1$）和 $\\mathbf{x}_5$（标签 $0$）。平局。预测 $0$。错误。\n    - 对于 $\\mathbf{x}_5=(1.1,1), y_5=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_3$（标签 $1$）。平局。预测 $0$。正确。\n    总误差：$2$。$E_{3}^{\\text{train}, k=2} = 2/5 = 0.4$。\n\n数据集 3 的结果：$[0.0, 0.6, 0.4]$。\n\n### 任务 B：对模型选择影响的分析\n\n计算出的错误率如下：\n- 数据集 $1$：$[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.0, 0.0]$\n- 数据集 $2$：$[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.667, 0.333]$\n- 数据集 $3$：$[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.6, 0.4]$\n\n这些结果揭示了误差估计方法的关键特性及其对模型选择的影响，特别是对选择超参数 $k$ 的影响。\n\n1.  **重代入误差过于乐观，是一个糟糕的度量标准**：正如分析上确认和数值上显示的那样，$k=1$ 的重代入误差 $E^{\\text{train}, k=1}$ 始终为 $0$。如果实践者通过最小化重代入误差来选择 $k$，他们将总是选择 $k=1$，因为它达到了完美的分数。这个选择对应于一个因为记忆了训练数据而具有零训练误差的模型。这种现象是过拟合的一个典型例子。该模型具有高方差，对每一个数据点（包括噪声）都非常敏感，但其训练误差对其在新未见数据上的性能给出了误导性的乐观评估。\n\n2.  **留一法交叉验证提供了更现实的评估**：留一法误差 $E^{\\text{LOO}, k=1}$ 提供了对泛化误差的更真实（偏差更小）的估计。对于数据集 $1$，其中类别完全分离，LOOCV 正确地识别出 $1$-NN 分类器表现完美（$E_{1}^{\\text{LOO}, k=1}=0.0$）。然而，对于分别具有类别重叠和噪声点的数据集 $2$ 和 $3$，$k=1$ 的 LOOCV 误差相当大（$0.667$ 和 $0.6$）。这个高误差正确地表明 $k=1$ 模型不鲁棒且过拟合了训练数据。在噪声区域，一个点的单个最近邻很可能来自错误的类别，导致 LOOCV 报告高误差。\n\n3.  **比较模型复杂度**：选择 $k$ 涉及偏差-方差权衡。一个小的 $k$（如 $k=1$）会导致一个低偏差、高方差的模型，其决策边界非常复杂。一个更大的 $k$ 会增加偏差但减少方差，从而产生一个更平滑、复杂度更低的决策边界。在数据集 $2$ 和 $3$ 中，$k=1$ 在 LOOCV 下的严重性能下降表明其高方差导致了差的泛化能力。\n\n4.  **$E^{\\text{train}, k=2}$ 的解释**：对于有噪声的数据集，$k=2$ 的重代入误差非零。这是因为预测是两个点的平均。例如，在数据集 $3$ 中，当分类点 $\\mathbf{x}_3$（标签 $1$）时，其邻居是它本身（标签 $1$）和噪声点 $\\mathbf{x}_5$（标签 $0$）。由此产生的平局通过选择标签 $0$ 来打破，即使在训练数据上也导致了误分类。虽然 $E^{\\text{train}, k=2}$ 仍然是一个乐观的有偏估计，但它比空洞的 $E^{\\text{train}, k=1}=0$ 更有信息量。问题所要求的比较，$E^{\\text{LOO}, k=1}$ 与 $E^{\\text{train}, k=2}$ 的比较，在方法论上并不是在 $k=1$ 和 $k=2$ 之间进行选择的合理方式，因为它比较了一个模型的近无偏误差估计和另一个模型的有偏估计。一个恰当的比较应该是在 $E^{\\text{LOO}, k=1}$ 和 $E^{\\text{LOO}, k=2}$ 之间。尽管如此，结果（$0.667$ vs $0.333$ 和 $0.6$ vs $0.4$）说明了一个关键点：一个不稳定的模型（$k=1$）可能具有非常高的泛化误差，甚至可能高于一个更稳定模型（$k=2$）的（乐观的）训练误差。\n\n总之，在 $k$-NN 的模型选择中，重代入误差是一个有缺陷的度量标准，特别是对于小的 $k$。它系统地偏爱过拟合的模型。像 LOOCV 这样的交叉验证方法为模型泛化能力提供了远为可靠的估计，并且对于调整像 $k$ 这样的超参数至关重要。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes k-NN error rates for three datasets as specified in the problem.\n    \"\"\"\n    datasets = [\n        (\n            np.array([[0.0, 0.0], [0.0, 1.0], [5.0, 5.0], [5.0, 6.0]]),\n            np.array([0, 0, 1, 1])\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 0.1], [0.05, 0.0]]),\n            np.array([0, 0, 1])\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 2.0], [2.0, 0.0], [2.0, 2.0], [1.1, 1.0]]),\n            np.array([0, 0, 1, 1, 0])\n        )\n    ]\n\n    all_results = []\n\n    for X, y in datasets:\n        n, d = X.shape\n        \n        # Task 1: Empirical risk for k=1 (E_train, k=1)\n        # This is analytically guaranteed to be 0, as each point is its own nearest neighbor.\n        e_train_k1 = 0.0\n        \n        # Task 2: Leave-one-out error for k=1 (E_LOO, k=1)\n        loo_errors = 0\n        for i in range(n):\n            x_query = X[i]\n            y_true = y[i]\n            \n            min_dist = np.inf\n            best_idx = -1\n            \n            for j in range(n):\n                if i == j:\n                    continue\n                \n                dist = np.linalg.norm(x_query - X[j])\n                \n                if dist  min_dist:\n                    min_dist = dist\n                    best_idx = j\n                elif dist == min_dist:\n                    # Tie-breaking by ascending index\n                    if j  best_idx:\n                        best_idx = j\n            \n            y_pred = y[best_idx]\n            if y_pred != y_true:\n                loo_errors += 1\n        \n        e_loo_k1 = loo_errors / n\n\n        # Task 3: Empirical risk for k=2 (E_train, k=2)\n        train_k2_errors = 0\n        for i in range(n):\n            x_query = X[i]\n            y_true = y[i]\n            \n            # The first neighbor is the point itself.\n            neighbor1_label = y_true\n            \n            # Find the second neighbor (closest among other points).\n            min_dist = np.inf\n            best_idx = -1\n            for j in range(n):\n                if i == j:\n                    continue\n                \n                dist = np.linalg.norm(x_query - X[j])\n                if dist  min_dist:\n                    min_dist = dist\n                    best_idx = j\n                elif dist == min_dist:\n                    # Tie-breaking by ascending index\n                    if j  best_idx:\n                        best_idx = j\n            \n            neighbor2_label = y[best_idx]\n            \n            # Majority vote with tie-breaking\n            if neighbor1_label == neighbor2_label:\n                y_pred = neighbor1_label\n            else:  # Tie in votes (one for each class)\n                y_pred = 0  # Select smallest class label\n                \n            if y_pred != y_true:\n                train_k2_errors += 1\n        \n        e_train_k2 = train_k2_errors / n\n        \n        all_results.append([e_train_k1, round(e_loo_k1, 3) if e_loo_k1 % 1 != 0 else e_loo_k1, round(e_train_k2, 3) if e_train_k2 % 1 != 0 else e_train_k2])\n\n    # Format the final output string as specified.\n    outer_list_str = []\n    for i, res in enumerate(all_results):\n        # Custom formatting to match the sample calculation's precision\n        if i == 0:\n            inner_list_str = f\"[{res[0]},{res[1]},{res[2]}]\"\n        elif i == 1:\n            inner_list_str = f\"[{res[0]},{res[1]:.3f},{res[2]:.3f}]\" # 2/3 and 1/3\n        else: # i == 2\n            inner_list_str = f\"[{res[0]},{res[1]},{res[2]}]\"\n            \n    # The problem asks for float values, but doesn't specify precision.\n    # The solution text uses fractions and decimals, so we generate a clean float representation.\n    final_results = []\n    for res in all_results:\n        final_results.append([float(v) for v in res])\n    \n    print(str(final_results).replace(\" \", \"\"))\n\n\nsolve()\n```"
        },
        {
            "introduction": "在许多现实世界的分类问题中，将一个类别错判为另一个类别的代价并非均等。例如，在医疗诊断中，将癌症患者误诊为健康（假阴性）的代价远高于将健康人误诊为患者（假阳性）。标准的 k-NN 算法采用简单的多数表决，完全忽略了这种成本差异。本实践  将引导你从风险最小化的基本原则出发，推导并实现一个成本敏感的 k-NN 分类器，让你掌握如何根据实际需求调整算法，做出更明智的决策。",
            "id": "3135604",
            "problem": "给定一个二元和多类分类场景，其中错分一个样本会产生一定的成本。考虑一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 和一个类别标签 $y \\in \\mathcal{Y} = \\{0,1,\\dots, K-1\\}$。设错分损失是依赖于类别的：预测 $\\hat{y} \\neq y$ 会产生一个成本 $C(y)$，而正确分类的成本为零。假设对于所有 $y \\in \\mathcal{Y}$ 都有 $C(y) \\ge 0$，并且 $C(y)$ 是已知的。目标是设计一个成本敏感的$k$-最近邻（k-NN）分类器，该分类器与最小化预期条件风险的目标一致，并在指定场景下进行测试。\n\n任务：\n1) 从分类风险和损失的基本定义出发，推导在上述依赖于类别的错分损失下，能使点 $\\mathbf{x}$ 处的预期条件风险最小化的决策条件。您的推导应从预期条件风险的定义 $R(\\hat{y} \\mid \\mathbf{x}) = \\mathbb{E}[L(\\hat{y}, Y) \\mid \\mathbf{x}]$ 和给定的损失结构开始，不使用任何预先推导的快捷公式。\n2) 实现一个$k$-最近邻分类器，该分类器是成本敏感的，因为它与您在第（1）部分中推导出的决策条件一致。在 $\\mathbb{R}^d$ 中使用欧几里得距离。假设类别标签是从0开始的整数，并且成本向量 $C$ 以实值数组的形式提供，其中 $C(j)$ 位于位置 $j$。如果最终决策分数下有多个类别打平，则通过选择最小的类别标签（即最小的整数）来打破平局。\n3) 将您的实现应用于以下独立的测试用例。对于每个用例，计算单个查询点的预测类别。所有点都在 $\\mathbb{R}^2$ 中（无单位），成本也无单位：\n- 用例1（基线，等成本）：\n  - 训练输入：(($0.0$, $0.0$), ($0.0$, $1.0$), ($1.0$, $0.0$), ($1.0$, $1.0$))\n  - 训练标签：($0$, $0$, $1$, $1$)\n  - 查询点：($0.1$, $0.1$)\n  - $k = 3$\n  - 成本 $C$：($1.0$, $1.0$)\n- 用例2（倾斜的成本反转决策）：\n  - 训练输入：(($0.0$, $0.0$), ($0.0$, $1.0$), ($1.0$, $0.0$), ($1.0$, $1.0$))\n  - 训练标签：($0$, $0$, $1$, $1$)\n  - 查询点：($0.1$, $0.1$)\n  - $k = 3$\n  - 成本 $C$：($1.0$, $4.0$)\n- 用例3（多类，三个类别具有不同成本）：\n  - 训练输入：(($0.0$, $0.0$), ($2.0$, $0.0$), ($0.0$, $2.0$))\n  - 训练标签：($0$, $1$, $2$)\n  - 查询点：($0.9$, $0.9$)\n  - $k = 3$\n  - 成本 $C$：($1.0$, $2.0$, $3.0$)\n- 用例4（边界情况 $k=1$）：\n  - 训练输入：(($0.0$, $0.0$), ($0.0$, $1.0$), ($1.0$, $0.0$), ($1.0$, $1.0$))\n  - 训练标签：($0$, $0$, $1$, $1$)\n  - 查询点：($0.05$, $0.05$)\n  - $k = 1$\n  - 成本 $C$：($1.0$, $100.0$)\n- 用例5（通过加权平局，用 $k=3$ 再现类似偶数 $k$ 的平局行为）：\n  - 训练输入：(($0.0$, $0.0$), ($1.0$, $0.0$), ($2.0$, $0.0$))\n  - 训练标签：($0$, $1$, $1$)\n  - 查询点：($0.9$, $0.0$)\n  - $k = 3$\n  - 成本 $C$：($2.0$, $1.0$)\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。具体来说，按顺序打印用例1到5的预测类别列表，例如，“[$a_1$, $a_2$, $a_3$, $a_4$, $a_5$]”，其中每个 $a_i$ 是用例 $i$ 的整数预测值。",
            "solution": "该问题是有效的，因为它在科学上基于统计决策理论，问题设定完整且一致，并且计算上可行。我们将提供完整的解决方案。\n\n解决方案分两部分呈现。首先，我们推导在指定的依赖于类别的错分成本下的最优决策规则，该规则最小化预期条件风险。其次，我们构建一个实现此决策规则的k-最近邻（k-NN）分类器，并将其应用于给定的测试用例。\n\n### 第1部分：最优决策规则的推导\n\n目标是找到一个决策函数 $\\hat{y}(\\mathbf{x})$，对于任何给定的特征向量 $\\mathbf{x}$，它从集合 $\\mathcal{Y} = \\{0, 1, \\dots, K-1\\}$ 中选择一个类别标签 $\\hat{y}$ 以最小化预期条件风险。\n\n当真实特征向量为 $\\mathbf{x}$ 时，预测类别 $\\hat{y}$ 的预期条件风险 $R(\\hat{y} \\mid \\mathbf{x})$ 定义为损失函数 $L(\\hat{y}, Y)$ 在给定 $\\mathbf{X} = \\mathbf{x}$ 时真实类别 $Y$ 的条件概率分布下的期望。\n令 $p(j \\mid \\mathbf{x}) = P(Y=j \\mid \\mathbf{X}=\\mathbf{x})$ 为类别 $j$ 的真实条件概率。风险为：\n$$\nR(\\hat{y} \\mid \\mathbf{x}) = \\mathbb{E}[L(\\hat{y}, Y) \\mid \\mathbf{X}=\\mathbf{x}] = \\sum_{j=0}^{K-1} L(\\hat{y}, j) p(j \\mid \\mathbf{x})\n$$\n问题指定了依赖于类别的错分损失：\n$$\nL(\\hat{y}, j) =\n\\begin{cases}\n0  \\text{if } \\hat{y} = j \\\\\nC(j)  \\text{if } \\hat{y} \\neq j\n\\end{cases}\n$$\n其中 $C(j) \\ge 0$ 是错分一个真正属于类别 $j$ 的实例的成本。\n\n将损失函数代入风险方程，我们可以将预测 $\\hat{y}$ 正确（$j = \\hat{y}$）的项与不正确（$j \\neq \\hat{y}$）的项分开：\n$$\nR(\\hat{y} \\mid \\mathbf{x}) = L(\\hat{y}, \\hat{y}) p(\\hat{y} \\mid \\mathbf{x}) + \\sum_{j \\neq \\hat{y}} L(\\hat{y}, j) p(j \\mid \\mathbf{x})\n$$\n由于当 $j \\neq \\hat{y}$ 时，$L(\\hat{y}, \\hat{y}) = 0$ 且 $L(\\hat{y}, j) = C(j)$，这可以简化为：\n$$\nR(\\hat{y} \\mid \\mathbf{x}) = 0 \\cdot p(\\hat{y} \\mid \\mathbf{x}) + \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x}) = \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x})\n$$\n贝叶斯最优分类器是选择最小化此风险的类别 $\\hat{y}^*$ 的分类器：\n$$\n\\hat{y}^* = \\arg\\min_{\\hat{y} \\in \\mathcal{Y}} R(\\hat{y} \\mid \\mathbf{x}) = \\arg\\min_{\\hat{y} \\in \\mathcal{Y}} \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x})\n$$\n为了使这个最小化问题更易于处理，我们可以重写这个和。所有可能真实类别的总预期错分成本是 $\\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x})$。我们可以将这个和表示为：\n$$\n\\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x}) = C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x}) + \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x})\n$$\n重新排列以分离出我们想要最小化的项：\n$$\n\\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x}) = \\left( \\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x}) \\right) - C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x})\n$$\n括号中的项 $\\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x})$ 是预期的总成本，不依赖于我们对预测 $\\hat{y}$ 的选择。因此，最小化左侧等价于最大化被减去的项 $C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x})$。\n\n因此，最优决策规则是选择使其错分成本和其条件概率的乘积最大化的类别：\n$$\n\\hat{y}^* = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} \\left\\{ C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x}) \\right\\}\n$$\n请注意，如果成本是统一的，即对于某个常数 $c  0$，所有 $y$ 的 $C(y) = c$，此规则简化为 $\\hat{y}^* = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} p(\\hat{y} \\mid \\mathbf{x})$，这是标准的贝叶斯分类器规则，它最小化错误概率（等同于0-1损失）。\n\n### 第2部分：成本敏感的k-最近邻分类器\n\nk-NN算法是一种非参数方法，它从训练数据中估计条件概率 $p(j \\mid \\mathbf{x})$。对于给定的查询点 $\\mathbf{x}$，我们首先确定其邻域 $N_k(\\mathbf{x})$，该邻域由根据某种距离度量（这里是欧几里得距离）最接近 $\\mathbf{x}$ 的 $k$ 个训练样本组成。\n\n条件概率 $p(j \\mid \\mathbf{x})$ 的 k-NN 估计是 $N_k(\\mathbf{x})$ 中属于类别 $j$ 的邻居的比例。设 $n_j(\\mathbf{x})$ 是 $N_k(\\mathbf{x})$ 中类别标签为 $j$ 的数据点的数量。那么，估计值 $\\hat{p}(j \\mid \\mathbf{x})$ 是：\n$$\n\\hat{p}(j \\mid \\mathbf{x}) = \\frac{n_j(\\mathbf{x})}{k}\n$$\n其中 $\\sum_{j=0}^{K-1} n_j(\\mathbf{x}) = k$。\n\n将此估计代入我们推导出的最优决策规则：\n$$\n\\hat{y}^* = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} \\left\\{ C(\\hat{y}) \\hat{p}(\\hat{y} \\mid \\mathbf{x}) \\right\\} = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} \\left\\{ C(\\hat{y}) \\frac{n_{\\hat{y}}(\\mathbf{x})}{k} \\right\\}\n$$\n由于 $k$ 是相对于在 $\\hat{y}$ 上最大化的正定常数，我们可以从表达式中移除它。我们的成本敏感k-NN分类器的最终决策规则是为每个类别 $j$ 计算一个分数 $S(j)$，并选择分数最高的类别：\n$$\n\\hat{y}^* = \\arg\\max_{j \\in \\mathcal{Y}} S(j) \\quad \\text{其中} \\quad S(j) = C(j) n_j(\\mathbf{x})\n$$\n如果多个类别具有相同的最高分数，则通过选择具有最小整数标签的类别来打破平局。\n\n算法流程如下：\n1. 对于一个查询点 $\\mathbf{x}_{query}$，计算其到所有训练点 $\\mathbf{x}_i$ 的欧几里得距离。\n2. 找出距离最小的 $k$ 个训练点。这些是最近邻。\n3. 对于每个类别 $j \\in \\{0, 1, \\dots, K-1\\}$，计算属于该类别的邻居数量 $n_j$。\n4. 对于每个类别 $j$，计算分数 $S(j) = C(j) \\cdot n_j$。\n5. 预测的类别是得分最高的那个。通过选择最小的类别索引来打破平局。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef cost_sensitive_knn_predict(X_train, y_train, x_query, k, costs):\n    \"\"\"\n    Predicts the class for a single query point using a cost-sensitive k-NN classifier.\n\n    Args:\n        X_train (np.ndarray): Training data feature vectors, shape (n_samples, n_features).\n        y_train (np.ndarray): Training data labels, shape (n_samples,).\n        x_query (np.ndarray): The query point feature vector, shape (n_features,).\n        k (int): The number of neighbors to consider.\n        costs (np.ndarray): The cost vector C, where costs[j] is C(j).\n\n    Returns:\n        int: The predicted class label.\n    \"\"\"\n    # 1. Compute Euclidean distances from the query point to all training points.\n    distances = np.linalg.norm(X_train - x_query, axis=1)\n\n    # 2. Find the indices of the k-nearest neighbors.\n    # np.argsort provides a stable sort, which is sufficient for breaking\n    # distance ties implicitly.\n    neighbor_indices = np.argsort(distances)[:k]\n\n    # Get the labels of the k-nearest neighbors.\n    neighbor_labels = y_train[neighbor_indices]\n\n    # 3. For each class, count the number of neighbors belonging to it.\n    num_classes = len(costs)\n    # np.bincount is highly efficient for this counting task.\n    # It requires non-negative integer labels. We ensure there's a bin for every class\n    # even if a class is not present in the neighbors, by setting minlength.\n    n_j = np.bincount(neighbor_labels, minlength=num_classes)\n\n    # 4. Calculate the score S(j) = C(j) * n_j for each class.\n    scores = costs * n_j\n    \n    # 5. The prediction is the class with the maximum score.\n    # np.argmax breaks ties by returning the index of the first occurrence\n    # of the maximum value, which corresponds to the smallest class label\n    # as required by the tie-breaking rule.\n    prediction = np.argmax(scores)\n    \n    return int(prediction)\n\ndef solve():\n    \"\"\"\n    Sets up and solves the 5 test cases provided in the problem statement.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (baseline, equal costs)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            \"y_train\": np.array([0, 0, 1, 1]),\n            \"query\": np.array([0.1, 0.1]),\n            \"k\": 3,\n            \"costs\": np.array([1.0, 1.0]),\n        },\n        # Case 2 (skewed costs flip the decision)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            \"y_train\": np.array([0, 0, 1, 1]),\n            \"query\": np.array([0.1, 0.1]),\n            \"k\": 3,\n            \"costs\": np.array([1.0, 4.0]),\n        },\n        # Case 3 (multi-class, three classes with distinct costs)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [2.0, 0.0], [0.0, 2.0]]),\n            \"y_train\": np.array([0, 1, 2]),\n            \"query\": np.array([0.9, 0.9]),\n            \"k\": 3,\n            \"costs\": np.array([1.0, 2.0, 3.0]),\n        },\n        # Case 4 (boundary case k = 1)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            \"y_train\": np.array([0, 0, 1, 1]),\n            \"query\": np.array([0.05, 0.05]),\n            \"k\": 1,\n            \"costs\": np.array([1.0, 100.0]),\n        },\n        # Case 5 (even-k-like tie behavior reproduced with k = 3 via weighted tie)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [1.0, 0.0], [2.0, 0.0]]),\n            \"y_train\": np.array([0, 1, 1]),\n            \"query\": np.array([0.9, 0.0]),\n            \"k\": 3,\n            \"costs\": np.array([2.0, 1.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        prediction = cost_sensitive_knn_predict(\n            X_train=case[\"X_train\"],\n            y_train=case[\"y_train\"],\n            x_query=case[\"query\"],\n            k=case[\"k\"],\n            costs=case[\"costs\"],\n        )\n        results.append(prediction)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在 k-NN 算法中，超参数 $k$ 的选择至关重要，它直接控制着模型的偏倚-方差权衡。虽然交叉验证是一种常用的经验性方法，但统计学习理论为我们提供了更具原则性的指导。本练习  将带你深入探索 $k$ 值的选择，通过实现一个基于经验 Bernstein 不等式的理论方法来确定 $k$，并将其结果与传统的留一法交叉验证所选出的值进行比较，让你体会理论与实践如何相互结合与验证。",
            "id": "3135635",
            "problem": "给定一个二元分类任务，要求您使用一种理论驱动的方法为 $k$-近邻分类器选择近邻数 $k$，并将其与交叉验证的选择进行比较。该理论方法必须依赖于近邻标签的经验方差估计和一个为经验均值误差提供高概率界的集中不等式。假设采用以下标准设置。\n\n基本设定：\n- 令 $(X,Y)$ 为一个随机对，其中 $X \\in \\mathbb{R}^d$ 且 $Y \\in \\{0,1\\}$。\n- 对于一个固定输入 $x$，条件类别概率为 $p(x) = \\mathbb{E}[Y \\mid X = x]$。\n- $k$-近邻规则通过计算 $x$ 的 $k$ 个最近训练点的标签的平均值来估计 $p(x)$，并通过将此估计值与 $0.5$ 进行比较来进行分类。\n- 使用针对有界随机变量的经验 Bernstein 不等式，这是一个经过充分检验的集中界。对于独立样本 $Z_1, \\dots, Z_k \\in [0,1]$，其经验均值为 $\\hat{\\mu}$，无偏样本方差为 $s^2$，该不等式表明，以下公式以至少 $1 - \\delta$ 的概率成立：\n$$\n\\left|\\hat{\\mu} - \\mu\\right| \\leq \\sqrt{\\frac{2 s^2 \\ln\\left(\\frac{3}{\\delta}\\right)}{k}} + \\frac{3 \\ln\\left(\\frac{3}{\\delta}\\right)}{k - 1},\n$$\n其中 $\\mu = \\mathbb{E}[Z_i]$。在我们的设定中，对于每个训练点 $x_i$，令 $Z_j$ 为近邻标签，并设置 $\\delta_i = \\delta/n$ 以便对所有 $n$ 个训练点应用并集界。\n\n您的程序必须实现以下精确步骤。\n\n1. 数据生成：\n   - 对于每个测试用例，按如下方式在 $d$ 维空间中生成 $n$ 个训练点。令类别大小为 $n_0 = \\lfloor n/2 \\rfloor$ 和 $n_1 = n - n_0$。类别 $0$ 的点独立地从多元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}_0, \\sigma^2 \\mathbf{I}_d)$ 中抽取，类别 $1$ 的点独立地从 $\\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma^2 \\mathbf{I}_d)$ 中抽取。相应地分配标签 $0$ 和 $1$。使用欧几里得距离进行最近邻计算。每个用例的随机数生成器必须使用指定的种子进行初始化，然后用于确定性地生成训练数据。\n   - 不需要单独的测试集；所有评估都在训练集上通过留一法进行。\n\n2. 候选 $k$ 值集合：\n   - 对于每个测试用例，评估用例中给出的所有候选 $k$ 值。对于以下所有逻辑，当 $k = 1$ 时，将经验 Bernstein 界中的第二项 $\\frac{3 \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k - 1}$ 视为 $+\\infty$，因此当 $k=1$ 时该界是无意义的。\n\n3. 基于经验 Bernstein 界的选择：\n   - 对于每个训练点 $x_i$，计算其在其他训练点中的 $k$ 个最近邻（排除 $x_i$ 本身），并令 $\\hat{p}_i(k)$ 为这 $k$ 个近邻标签的经验均值。令 $s_i^2(k)$ 为这些标签的无偏样本方差，\n     $$\n     s_i^2(k) = \\frac{1}{k - 1} \\sum_{j=1}^k \\left(y_{i,j} - \\hat{p}_i(k)\\right)^2,\n     $$\n     其中 $y_{i,j}$ 是近邻标签。定义单点界半宽\n     $$\n     B_i(k) = \\sqrt{\\frac{2 s_i^2(k) \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k}} + \\frac{3 \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k - 1}, \\quad \\delta_i = \\frac{\\delta}{n}.\n     $$\n   - 定义模糊性指标\n     $$\n     A_i(k) = \\begin{cases}\n     1  \\text{if } \\left|\\hat{p}_i(k) - \\frac{1}{2}\\right| \\leq B_i(k), \\\\\n     0  \\text{otherwise}.\n     \\end{cases}\n     $$\n     直观上，$A_i(k) = 1$ 表示在界限宽度内，经验估计 $\\hat{p}_i(k)$ 无法可靠地确定在 $\\frac{1}{2}$ 的某一侧。定义基于界的误分类率上界\n     $$\n     U(k) = \\frac{1}{n} \\sum_{i=1}^n A_i(k).\n     $$\n   - 选择使 $U(k)$ 最小化的 $k$ 作为 $k_{\\text{bound}}$。若存在平局，则选择最小的 $k$。\n\n4. 交叉验证比较：\n   - 计算留一法误差\n     $$\n     E_{\\text{LOO}}(k) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\left\\{ \\hat{y}_i^{(-i)}(k) \\neq y_i \\right\\},\n     $$\n     其中 $\\hat{y}_i^{(-i)}(k)$ 是根据排除 $x_i$ 的 $k$ 个最近邻对 $x_i$ 做出的预测标签，决策规则如下\n     $$\n     \\hat{y}_i^{(-i)}(k) = \\begin{cases}\n     1  \\text{if } \\hat{p}_i(k) \\geq \\frac{1}{2}, \\\\\n     0  \\text{otherwise}.\n     \\end{cases}\n     $$\n     选择使 $E_{\\text{LOO}}(k)$ 最小化的 $k$ 作为 $k_{\\text{CV}}$。若存在平局，则选择最小的 $k$。\n\n5. 输出：\n   - 对于每个测试用例，输出四元组 $[k_{\\text{bound}}, U(k_{\\text{bound}}), k_{\\text{CV}}, E_{\\text{LOO}}(k_{\\text{CV}})]$。\n   - 将 $U(\\cdot)$ 和 $E_{\\text{LOO}}(\\cdot)$ 表示为 $[0,1]$ 范围内的小数，并四舍五入到小数点后四位。\n   - 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个测试用例的结果应精确地格式化为一个无空格的方括号四元组。例如，两个用例的输出必须如下所示：$[[1,0.0000,3,0.0500],[2,0.1000,2,0.1000]]$。\n\n测试套件：\n- 用例 1：$n = 60$, $d = 2$, $\\boldsymbol{\\mu}_0 = (-1,-1)$, $\\boldsymbol{\\mu}_1 = (1,1)$, $\\sigma = 0.3$, 候选 $k$ 值 $[1,3,5,7,9,11]$, 总置信度参数 $\\delta = 0.1$, 随机种子 $0$。\n- 用例 2：$n = 100$, $d = 2$, $\\boldsymbol{\\mu}_0 = (-0.5,0)$, $\\boldsymbol{\\mu}_1 = (0.5,0)$, $\\sigma = 0.6$, 候选 $k$ 值 $[2,4,6,8,10,12,14]$, $\\delta = 0.05$, 随机种子 $1$。\n- 用例 3：$n = 80$, $d = 1$, $\\boldsymbol{\\mu}_0 = (-0.3)$, $\\boldsymbol{\\mu}_1 = (0.3)$, $\\sigma = 0.8$, 候选 $k$ 值 $[1,5,15,25,35]$, $\\delta = 0.2$, 随机种子 $2$。\n\n所有最近邻计算都必须使用欧几里得距离，并排除点本身（在训练集内部进行留一法）。请确保所有计算都严格遵守上述定义。",
            "solution": "用户提供的问题已经过分析和验证。该问题在科学上是合理的、适定的且客观的。所有必要信息均已提供，程序步骤定义清晰明确。该问题是计算统计学中的一个实质性任务，要求在 $k$-近邻算法的背景下，实现数据生成过程和两种不同的模型选择方法。未检测到任何缺陷。\n\n解决方案首先从一个双组分高斯混合模型中为一个二元分类问题生成合成数据集。随后，实现两种程序，从给定的候选集合中选择最佳近邻数 $k$。整个评估过程使用留一法在生成的训练数据上进行。\n\n### 1. 数据生成与留一法设置\n\n对于每个测试用例，我们在一个 $d$ 维空间 $\\mathbb{R}^d$ 中生成一个包含 $n$ 个点的训练集。数据包含两个类别，标记为 $0$ 和 $1$。每个类别中的点数分别为 $n_0 = \\lfloor n/2 \\rfloor$ 和 $n_1 = n - n_0$。类别 $0$ 的点从多元正态分布 $\\mathcal{N}(\\boldsymbol{\\mu}_0, \\sigma^2 \\mathbf{I}_d)$ 中抽取，类别 $1$ 的点从 $\\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma^2 \\mathbf{I}_d)$ 中抽取。每个用例都指定了参数 $n, d, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\sigma$ 和一个随机种子，以确保确定性可复现性。\n\n所有后续计算都基于留一法 (LOO) 方案。对于每个带标签 $y_i$ 的训练点 $x_i$，所有计算都使用其在其他训练点集合 $\\{x_j\\}_{j \\neq i}$ 中的 $k$ 个最近邻来执行。使用的距离度量是欧几里得距离。\n\n### 2. 方法1：基于界的 $k_{\\text{bound}}$ 选择\n\n该方法通过最小化分类错误率的一个上界来选择 $k$，该上界由一个集中不等式推导得出。其核心思想是识别并计数“模糊”分类。如果一个点 $x_i$ 的估计条件类别概率 $\\hat{p}_i(k)$ 在统计上与决策边界 $0.5$ 不可区分，则该点的分类被认为是模糊的。\n\n令 $y_{i,1}, \\dots, y_{i,k}$ 为 $x_i$ 的 $k$ 个最近邻的标签。局部类别概率通过它们的经验均值来估计：\n$$\n\\hat{p}_i(k) = \\frac{1}{k} \\sum_{j=1}^k y_{i,j}\n$$\n为了量化该估计中的统计不确定性，我们使用经验 Bernstein 不等式。这为真实局部概率 $p(x_i) = \\mathbb{E}[Y \\mid X=x_i]$ 提供了一个高概率置信区间。真实均值 $\\mu$（在此即 $p(x_i)$）以至少 $1-\\delta_i$ 的概率被界定在以下范围内：\n$$\n|\\hat{p}_i(k) - p(x_i)| \\leq B_i(k)\n$$\n其中 $B_i(k)$ 是界半宽。为确保该界对所有 $n$ 个训练点以总置信度 $1-\\delta$ 同时成立，我们通过设置 $\\delta_i = \\delta/n$ 来应用并集界。半宽 $B_i(k)$ 定义为：\n$$\nB_i(k) = \\sqrt{\\frac{2 s_i^2(k) \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k}} + \\frac{3 \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k - 1}\n$$\n此处，$s_i^2(k)$ 是近邻标签的无偏样本方差：\n$$\ns_i^2(k) = \\frac{1}{k - 1} \\sum_{j=1}^k \\left(y_{i,j} - \\hat{p}_i(k)\\right)^2\n$$\n对于给定的 $k$，如果点 $x_i$ 的 $p(x_i)$ 置信区间包含决策边界 $0.5$，则该点被宣告为模糊。这由模糊性指标 $A_i(k)$ 捕获：\n$$\nA_i(k) = \\begin{cases}\n1  \\text{if } |\\hat{p}_i(k) - 0.5| \\leq B_i(k) \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\n选择的 $k$ 值是为了最小化模糊点的总比例 $U(k)$：\n$$\nU(k) = \\frac{1}{n} \\sum_{i=1}^n A_i(k)\n$$\n在此准则下，最优 $k$ 为 $k_{\\text{bound}} = \\arg\\min_k U(k)$，平局则通过选择最小的 $k$ 来打破。对于 $k=1$，方差 $s_i^2(1)$ 未定义。问题规定在这种情况下界是无意义的，意味着 $B_i(1) = \\infty$，这蕴含着对所有 $i$ 都有 $A_i(1) = 1$，因此 $U(1)=1$。\n\n### 3. 方法2：用于 $k_{\\text{CV}}$ 的留一法交叉验证\n\n这是一种标准的模型选择经验方法。它直接估计分类器在不同 $k$ 值下的泛化误差。\n\n对于每个训练点 $x_i$，使用其余数据中的 $k$ 个最近邻来做出预测 $\\hat{y}_i^{(-i)}(k)$。该预测基于近邻中的多数投票，等价于对估计概率 $\\hat{p}_i(k)$ 进行阈值处理：\n$$\n\\hat{y}_i^{(-i)}(k) = \\begin{cases}\n1  \\text{if } \\hat{p}_i(k) \\geq 0.5 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\n给定 $k$ 的留一法误差是被错误分类的点的比例：\n$$\nE_{\\text{LOO}}(k) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\left\\{ \\hat{y}_i^{(-i)}(k) \\neq y_i \\right\\}\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。在此准则下，最优 $k$ 为 $k_{\\text{CV}} = \\arg\\min_k E_{\\text{LOO}}(k)$，平局同样通过选择最小的 $k$ 来打破。此方法选择在训练数据本身上以留一法方式评估时表现最佳的 $k$。\n\n### 4. 算法流程\n\n每个测试用例的实现遵循以下步骤：\n1.  使用指定的种子初始化伪随机数生成器，并生成训练数据 $(X, y)$。\n2.  为提高效率，预先计算 $X$ 中所有点之间的 $n \\times n$ 成对欧几里得距离矩阵。\n3.  对于每个候选 $k$ 值：\n    a. 初始化总模糊性得分和总留一法误差的累加器。\n    b. 对于每个点 $x_i$, $i=1, \\dots, n$：\n        i. 使用预计算的距离矩阵，找到其 $k$ 个最近邻的索引，排除 $x_i$ 本身。\n        ii. 检索这些近邻的标签，并计算 $\\hat{p}_i(k)$ 和 $s_i^2(k)$。\n        iii. 计算留一法预测 $\\hat{y}_i^{(-i)}(k)$，如果 $\\hat{y}_i^{(-i)}(k) \\neq y_i$，则更新留一法误差计数。\n        iv. 计算 Bernstein 界半宽 $B_i(k)$（单独处理 $k=1$ 的情况），并确定模糊性指标 $A_i(k)$，更新模糊性得分。\n    c. 计算当前 $k$ 的最终得分 $U(k)$ 和 $E_{\\text{LOO}}(k)$。\n4.  在评估完所有候选 $k$ 值后，通过找到最小化 $U(k)$ 的 $k$ 来确定 $k_{\\text{bound}}$，通过找到最小化 $E_{\\text{LOO}}(k)$ 的 $k$ 来确定 $k_{\\text{CV}}$。按规定应用平局打破规则。\n5.  将最终结果格式化为四元组 $[k_{\\text{bound}}, U(k_{\\text{bound}}), k_{\\text{CV}}, E_{\\text{LOO}}(k_{\\text{CV}})]$，其中比率值四舍五入到小数点后四位。",
            "answer": "```python\nimport numpy as np\n\ndef run_one_case(n, d, mu_0, mu_1, sigma, k_values, delta, seed):\n    \"\"\"\n    Executes a single test case according to the problem description.\n\n    Args:\n        n (int): Total number of training points.\n        d (int): Dimension of the feature space.\n        mu_0 (list or float): Mean vector for class 0.\n        mu_1 (list or float): Mean vector for class 1.\n        sigma (float): Standard deviation for the isotropic covariance.\n        k_values (list): List of candidate k values to evaluate.\n        delta (float): Total confidence parameter for Bernstein bound.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple: A tuple containing (k_bound, U(k_bound), k_CV, E_LOO(k_CV)).\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    n_0 = n // 2\n    n_1 = n - n_0\n\n    if d == 1:\n        # np.multivariate_normal expects mean to be 1-D array-like\n        mu_0_arr = [mu_0]\n        mu_1_arr = [mu_1]\n    else:\n        mu_0_arr = mu_0\n        mu_1_arr = mu_1\n\n    cov = (sigma**2) * np.identity(d)\n    X_0 = rng.multivariate_normal(mu_0_arr, cov, size=n_0)\n    X_1 = rng.multivariate_normal(mu_1_arr, cov, size=n_1)\n\n    X = np.vstack((X_0, X_1))\n    if d == 1 and X.ndim > 1 and X.shape[1] != d:\n        X = X.reshape(n, d)\n    y = np.array([0] * n_0 + [1] * n_1)\n\n    # 2. Pre-compute pairwise Euclidean distances\n    sum_sq = np.sum(X**2, axis=1, keepdims=True)\n    dist_sq = sum_sq + sum_sq.T - 2 * np.dot(X, X.T)\n    dist_sq[dist_sq  0] = 0  # Correct for numerical floating point inaccuracies\n    distances = np.sqrt(dist_sq)\n\n    results_per_k = []\n    delta_i = delta / n\n    log_term = np.log(3 / delta_i)\n\n    # 3. Iterate over candidate k values\n    for k in k_values:\n        total_A_i = 0\n        total_loo_errors = 0\n\n        # 4. Leave-one-out evaluation for each point\n        for i in range(n):\n            dists_i = distances[i, :]\n            sorted_indices = np.argsort(dists_i)\n            neighbor_indices = sorted_indices[1:k + 1]\n            neighbor_labels = y[neighbor_indices]\n\n            p_hat = np.mean(neighbor_labels)\n            \n            # --- Cross-Validation Error Calculation ---\n            y_hat = 1 if p_hat >= 0.5 else 0\n            if y_hat != y[i]:\n                total_loo_errors += 1\n\n            # --- Bernstein Bound-based Ambiguity Calculation ---\n            if k == 1:\n                A_i = 1 # Per problem spec, bound is vacuous, ambiguity is 1.\n            else:\n                s_squared = np.var(neighbor_labels, ddof=1) if k > 1 else 0\n                \n                term1 = np.sqrt(2 * s_squared * log_term / k)\n                term2 = 3 * log_term / (k - 1)\n                B_i = term1 + term2\n                \n                if np.abs(p_hat - 0.5) = B_i:\n                    A_i = 1\n                else:\n                    A_i = 0\n            \n            total_A_i += A_i\n\n        U_k = total_A_i / n\n        E_LOO_k = total_loo_errors / n\n        results_per_k.append({'k': k, 'U': U_k, 'E_LOO': E_LOO_k})\n\n    # 5. Select best k based on U(k) and E_LOO(k)\n    best_bound_res = sorted(results_per_k, key=lambda x: (x['U'], x['k']))[0]\n    k_bound = best_bound_res['k']\n    U_at_k_bound = best_bound_res['U']\n\n    best_cv_res = sorted(results_per_k, key=lambda x: (x['E_LOO'], x['k']))[0]\n    k_cv = best_cv_res['k']\n    E_LOO_at_k_cv = best_cv_res['E_LOO']\n\n    return k_bound, U_at_k_bound, k_cv, E_LOO_at_k_cv\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {'n': 60, 'd': 2, 'mu_0': [-1,-1], 'mu_1': [1,1], 'sigma': 0.3,\n         'k_values': [1,3,5,7,9,11], 'delta': 0.1, 'seed': 0},\n        {'n': 100, 'd': 2, 'mu_0': [-0.5,0], 'mu_1': [0.5,0], 'sigma': 0.6,\n         'k_values': [2,4,6,8,10,12,14], 'delta': 0.05, 'seed': 1},\n        {'n': 80, 'd': 1, 'mu_0': -0.3, 'mu_1': 0.3, 'sigma': 0.8,\n         'k_values': [1,5,15,25,35], 'delta': 0.2, 'seed': 2},\n    ]\n\n    results_str = []\n    for case in test_cases:\n        k_bound, u_val, k_cv, e_val = run_one_case(\n            case['n'], case['d'], case['mu_0'], case['mu_1'], case['sigma'],\n            case['k_values'], case['delta'], case['seed']\n        )\n        formatted_result = f\"[{k_bound},{u_val:.4f},{k_cv},{e_val:.4f}]\"\n        results_str.append(formatted_result)\n\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}