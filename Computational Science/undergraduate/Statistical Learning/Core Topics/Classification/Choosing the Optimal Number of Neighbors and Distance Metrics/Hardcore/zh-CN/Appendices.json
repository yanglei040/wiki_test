{
    "hands_on_practices": [
        {
            "introduction": "在选择距离度量时，一个基本的问题是：我们是否应该平等地对待所有特征？标准的欧几里得距离正是这样做的，但当特征具有不同尺度或相互关联时，这可能不是最佳选择。这个练习  将指导你实现一种“白化”距离度量，它能够适应数据的协方差结构，从而让你亲身体验为什么这种马氏距离（Mahalanobis distance）风格的度量在特定数据上能够显著优于原始欧几里得距离。",
            "id": "3108168",
            "problem": "您的任务是，以编程方式研究距离度量的选择如何影响$k$-最近邻 (kNN) 分类器中最近邻数量 ($k$) 的最优选择。您的研究必须基于以下核心定义和事实：\n\n- $k$-最近邻分类器通过在指定距离下找到一个查询点的$k$个最近的训练点，并返回这些邻居中的多数类别，来预测该查询点的类别标签。\n- 向量 $x \\in \\mathbb{R}^d$ 和 $y \\in \\mathbb{R}^d$ 之间的欧几里得距离是 $d_{\\text{raw}}(x,y) = \\|x - y\\|_2$。\n- 对于从训练数据中估计出的样本协方差矩阵 $\\hat{\\Sigma} \\in \\mathbb{R}^{d \\times d}$ 的数据，白化变换使用一个满足 $W^\\top W \\approx \\hat{\\Sigma}^{-1}$ 的矩阵 $W \\in \\mathbb{R}^{d \\times d}$ 来对特征进行重新缩放和去相关。相关联的距离是 $d_W(x,y) = \\|W(x-y)\\|_2$，当 $W$ 被选为对称矩阵 $W = U \\operatorname{diag}(\\lambda^{-1/2}) U^\\top$（其中 $\\hat{\\Sigma} = U \\operatorname{diag}(\\lambda) U^\\top$ 是谱分解）时，该距离等于由 $\\hat{\\Sigma}^{-1}$ 导出的马氏距离。\n- 留一法交叉验证 (LOOCV) 通过对每个样本 $i$ 在余下的 $n-1$ 个样本上进行训练，预测样本 $i$ 的标签，并将所有 $n$ 个样本的 $0$-$1$ 损失进行平均，来估计分类误差。\n\n您的程序必须：\n1. 对于下面描述的每个测试用例，从两个具有相同协方差但均值不同的多元高斯类中，生成一个平衡的二元分类数据集。使用两个独立的类别均值 $m_0 \\in \\mathbb{R}^d$ 和 $m_1 \\in \\mathbb{R}^d$，一个共享的协方差矩阵 $\\Sigma \\in \\mathbb{R}^{d \\times d}$，以及总样本大小为 $n$ 且各类别的样本量相等。所有随机抽样必须可通过给定的种子复现。\n2. 对于从 1 到 $\\min(31, n-1)$（含）的奇数集合中的每个候选邻居数 $k$，执行 LOOCV 以估计两种度量的误分类率：\n   - 原始欧几里得度量 $d_{\\text{raw}}(x,y) = \\|x-y\\|_2$。\n   - 白化度量 $d_W(x,y) = \\|W(x-y)\\|_2$，其中 $W$ 仅根据每个 LOOCV 折中的训练数据计算。具体来说，对于训练数据 $X_{\\text{train}} \\in \\mathbb{R}^{(n-1) \\times d}$，计算样本协方差 $\\hat{\\Sigma}$，然后通过 $\\hat{\\Sigma}$ 的对称谱分解来构造 $W$，即 $W = U \\operatorname{diag}(\\lambda^{-1/2}) U^\\top$。为确保数值稳定性，在对平方根求逆之前，对 $\\hat{\\Sigma}$ 应用岭正则化 $\\alpha I_d$，其中 $\\alpha$ 是一个基于 $\\hat{\\Sigma}$ 尺度的小的正标量。声明并使用一个一致的规则来处理多数投票中的平局情况。\n3. 对于每种度量，选择达到最小 LOOCV 误分类率的最小 $k$ 作为最优 $k$。\n\n您必须为以下测试套件实现上述过程。在每个案例中，需精确使用指定的参数。符号 $n$、$d$、$m_0$、$m_1$ 和 $\\Sigma$ 的定义如上。\n\n- 测试用例 1 (各向异性尺度)：seed $= 0$，$n = 120$，$d = 2$，$m_0 = (-1, 0)$，$m_1 = (1, 0)$，以及\n  $$\\Sigma = \\begin{bmatrix}1  0 \\\\ 0  9\\end{bmatrix}.$$\n- 测试用例 2 (各向同性)：seed $= 1$，$n = 120$，$d = 2$，$m_0 = (0, 0)$，$m_1 = (1, 0)$，以及\n  $$\\Sigma = \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix}.$$\n- 测试用例 3 (相关特征)：seed $= 2$，$n = 120$，$d = 3$，$m_0 = (0, 0, 0)$，$m_1 = (1, 0, 0)$，以及\n  $$\\Sigma = \\begin{bmatrix}1  0.8  0 \\\\ 0.8  1  0 \\\\ 0  0  0.2\\end{bmatrix}.$$\n- 测试用例 4 (小样本量 $n$，强各向异性)：seed $= 3$，$n = 40$，$d = 2$，$m_0 = (0, 0)$，$m_1 = (0.8, 0)$，以及\n  $$\\Sigma = \\begin{bmatrix}1  0 \\\\ 0  16\\end{bmatrix}.$$\n\n实现要求与假设：\n- 在每个测试用例中，使用由给定种子初始化的确定性伪随机数生成器。\n- 在 LOOCV 中，对于白化度量，白化矩阵 $W$ 必须严格地从每个折的训练子集中计算。使用形式为 $\\hat{\\Sigma}_{\\text{reg}} = \\hat{\\Sigma} + \\alpha I_d$ 的岭正则化，其中 $\\alpha = 10^{-6} \\cdot \\operatorname{trace}(\\hat{\\Sigma})/d$。\n- 在 kNN 的多数投票中，如果出现平局，预测为类别 $0$。\n- $k$ 值的候选集是 $\\{1, 3, 5, \\dots, \\min(31, n-1)\\}$ 中的所有奇数。\n\n您的程序应产生单行输出，其中包含一个用方括号括起来的、逗号分隔的结果列表。列表中的每个元素本身应为一个双元素列表 $[k_{\\text{white}}, k_{\\text{raw}}]$，其中 $k_{\\text{white}}$ 是白化度量下的最优 $k$，$k_{\\text{raw}}$ 是原始欧几里得度量下的最优 $k$，顺序按照上面列出的测试用例的顺序。例如，一个包含四个测试用例的输出应如下所示：$[[k_{1,\\text{white}},k_{1,\\text{raw}}],[k_{2,\\text{white}},k_{2,\\text{raw}}],[k_{3,\\text{white}},k_{3,\\text{raw}}],[k_{4,\\text{white}},k_{4,\\text{raw}}]]$。",
            "solution": "用户提供了一个问题，要求通过编程方式研究在两种不同的距离度量下，$k$-近邻 (kNN) 算法的最近邻数量 $k$ 的最优选择：标准的欧几里得距离和与马氏距离相关的白化距离。该问题定义明确、科学合理且计算上可行，是统计学习中的一个有效练习。\n\n问题的核心是比较距离度量的几何特性如何与数据分布的几何特性相互作用。我们将通过实现指定的流程来解决这个问题：对于四个测试用例中的每一个，我们将生成一个合成数据集，然后使用留一法交叉验证 (LOOCV) 为每种度量找到最优的 $k$。\n\n### 1. 理论框架\n\n**$k$-近邻 (kNN):**\nkNN 算法是一种非参数分类方法。为了对查询点 $x_q$ 进行分类，它根据选定的距离度量识别出与 $x_q$ 最近的 $k$ 个训练样本。$x_q$ 的预测类别是这 $k$ 个邻居中的多数类别。在投票出现平局的情况下，问题指定了一个确定性规则：预测为类别 $0$。由于 $k$ 的候选值都是奇数，且我们处理的是一个二元分类问题，平局是不可能发生的。尽管如此，该规则仍按规定实现。\n\n**距离度量:**\n1.  **原始欧几里得距离：** 对于两个向量 $x, y \\in \\mathbb{R}^d$，欧几里得距离由下式给出：\n    $$\n    d_{\\text{raw}}(x,y) = \\|x - y\\|_2 = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}\n    $$\n    这种度量是各向同性的，意味着它同等对待所有特征维度，并且不考虑它们之间的尺度差异或相关性。\n\n2.  **白化距离：** 在计算欧几里得距离之前，数据通过一个矩阵 $W$ 进行变换。该距离为：\n    $$\n    d_W(x,y) = \\|W(x-y)\\|_2\n    $$\n    问题指定了从训练数据的样本协方差矩阵 $\\hat{\\Sigma}$ 构造 $W$ 的方法。对于一个给定的训练集，$\\hat{\\Sigma}$ 被估计出来。为了处理潜在的奇异性并提高数值稳定性，一个正则化的协方差矩阵 $\\hat{\\Sigma}_{\\text{reg}}$ 被构作：\n    $$\n    \\hat{\\Sigma}_{\\text{reg}} = \\hat{\\Sigma} + \\alpha I_d\n    $$\n    其中 $\\alpha = 10^{-6} \\cdot \\operatorname{trace}(\\hat{\\Sigma})/d$。设这个对称正定矩阵的谱分解为 $\\hat{\\Sigma}_{\\text{reg}} = U \\Lambda U^\\top$，其中 $U$ 是特征向量的正交矩阵，$\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_d)$ 是正特征值的对角矩阵。白化矩阵 $W$ 被定义为 $\\hat{\\Sigma}_{\\text{reg}}$ 的逆的对称平方根：\n    $$\n    W = (\\hat{\\Sigma}_{\\text{reg}})^{-1/2} = U \\Lambda^{-1/2} U^\\top\n    $$\n    其中 $\\Lambda^{-1/2} = \\operatorname{diag}(\\lambda_1^{-1/2}, \\dots, \\lambda_d^{-1/2})$。通过选择这样的 $W$，距离的平方变为：\n    $$\n    d_W(x,y)^2 = \\|W(x-y)\\|_2^2 = (W(x-y))^\\top (W(x-y)) = (x-y)^\\top W^\\top W (x-y)\n    $$\n    由于 $W$ 是对称的 ($W=W^\\top$) 且 $W^2 = \\hat{\\Sigma}_{\\text{reg}}^{-1}$，上式简化为：\n    $$\n    d_W(x,y)^2 = (x-y)^\\top \\hat{\\Sigma}_{\\text{reg}}^{-1} (x-y)\n    $$\n    这是相对于正则化样本协方差的马氏距离。这种度量是自适应的；它根据特征的方差重新缩放特征，并考虑了它们之间的成对相关性，从而有效地将数据转换到一个特征不相关且单位方差的空间中。\n\n**留一法交叉验证 (LOOCV):**\nLOOCV 是一种用于估计模型性能的穷举法。对于一个大小为 $n$ 的数据集，它执行 $n$ 次迭代。在每次迭代 $i$ 中，第 $i$ 个数据点 $(x_i, y_i)$ 被留作验证集，模型在余下的 $n-1$ 个数据点上进行训练。然后，训练好的模型预测 $x_i$ 的标签。LOOCV 误差是错误预测的总数除以 $n$。\n\n一个关键的细节是，对于白化度量，白化矩阵 $W$ 必须在 $n$ 个折的每一个折内重新计算，且仅使用该折中可用的 $n-1$ 个训练样本。这可以防止数据从验证点泄露到模型的训练过程中（具体来说，是泄露到距离度量本身的构建过程中）。\n\n### 2. 算法流程\n\n对于四个测试用例中的每一个：\n1.  **数据生成：** 生成一个包含 $n$ 个样本和 $d$ 个维度的数据集。使用提供的随机 `seed` 初始化一个伪随机数生成器。对于类别 $0$，从多元正态分布 $\\mathcal{N}(m_0, \\Sigma)$ 中抽取 $n/2$ 个样本；对于类别 $1$，从 $\\mathcal{N}(m_1, \\Sigma)$ 中抽取 $n/2$ 个样本。\n\n2.  **交叉验证循环：** 从 $0$ 到 $n-1$ 遍历每个样本 $i$。\n    a. **数据分割：** 将样本 $i$ 指定为测试点 $(x_{\\text{test}}, y_{\\text{test}})$，其余 $n-1$ 个样本作为训练集 $(X_{\\text{train}}, y_{\\text{train}})$。\n    b. **特定于度量的计算：**\n        *   **原始度量：** 计算从 $x_{\\text{test}}$ 到 $X_{\\text{train}}$ 中所有点的欧几里得距离。根据这些距离对训练标签 $y_{\\text{train}}$ 进行排序，以获得邻居标签的有序列表。\n        *   **白化度量：** 首先，从 $X_{\\text{train}}$ 计算样本协方差矩阵 $\\hat{\\Sigma}$。应用指定的正则化得到 $\\hat{\\Sigma}_{\\text{reg}}$。计算其谱分解并构造白化矩阵 $W$。然后，计算从 $x_{\\text{test}}$ 到 $X_{\\text{train}}$ 中所有点的白化距离。根据这些距离对训练标签 $y_{\\text{train}}$ 进行排序。\n    c. **误差累积：** 对于每个候选值 $k$（从 $1$ 到 $\\min(31, n-1)$ 的奇数），通过取有序列表中的前 $k$ 个邻居的多数类别来确定预测标签。如果预测不正确，则为该特定度量和 $k$ 值增加一个误差计数器。一种同时为所有 $k$ 执行此操作的高效方法是计算排序后标签的累积和，并在每个所需索引处检查多数条件。\n\n3.  **最优 $k$ 的选择：** 在 LOOCV 循环完成后，我们将得到每种度量和每个候选 $k$ 的总误差计数。\n    a.  对于原始度量，找到在所有候选 $k$ 中实现的最小错误率。最优 $k_{\\text{raw}}$ 是产生此最小误差的最小 $k$。\n    b.  同样地，为白化度量找到最优的 $k_{\\text{white}}$。\n\n4.  **结果聚合：** 存储当前测试用例的配对 $[k_{\\text{white}}, k_{\\text{raw}}]$，并对所有案例重复此过程。最终输出是这些配对的列表。\n\n这个流程通过在给定数据上，使用一种可靠的误差估计技术，为两种度量找到各自的最佳可能性能（在 $k$ 的搜索空间内），从而对它们进行严格比较。我们预期，在特征具有各向异性尺度或相关的数集上，白化度量的表现会更好（并可能偏好更简单的模型，即更小的 $k$），因为它校正了这些数据结构。对于各向同性数据，其优势应该很小。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Investigates how the choice of distance metric affects the optimal number\n    of neighbors in the k-Nearest Neighbors (kNN) classifier.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"seed\": 0, \"n\": 120, \"d\": 2, \"m0\": np.array([-1, 0]),\n            \"m1\": np.array([1, 0]), \"sigma\": np.array([[1, 0], [0, 9]])\n        },\n        {\n            \"seed\": 1, \"n\": 120, \"d\": 2, \"m0\": np.array([0, 0]),\n            \"m1\": np.array([1, 0]), \"sigma\": np.array([[1, 0], [0, 1]])\n        },\n        {\n            \"seed\": 2, \"n\": 120, \"d\": 3, \"m0\": np.array([0, 0, 0]),\n            \"m1\": np.array([1, 0, 0]),\n            \"sigma\": np.array([[1, 0.8, 0], [0.8, 1, 0], [0, 0, 0.2]])\n        },\n        {\n            \"seed\": 3, \"n\": 40, \"d\": 2, \"m0\": np.array([0, 0]),\n            \"m1\": np.array([0.8, 0]), \"sigma\": np.array([[1, 0], [0, 16]])\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        seed, n, d, m0, m1, sigma = case[\"seed\"], case[\"n\"], case[\"d\"], \\\n                                    case[\"m0\"], case[\"m1\"], case[\"sigma\"]\n\n        # 1. Generate dataset\n        rng = np.random.default_rng(seed)\n        n_per_class = n // 2\n        X0 = rng.multivariate_normal(m0, sigma, n_per_class)\n        X1 = rng.multivariate_normal(m1, sigma, n_per_class)\n        X = np.vstack((X0, X1))\n        y = np.array([0] * n_per_class + [1] * n_per_class)\n\n        # 2. Perform LOOCV to find optimal k for each metric\n        k_candidates = list(range(1, min(31, n - 1) + 2, 2))\n        if not k_candidates or k_candidates[-1] > n-1:\n             k_candidates = list(range(1, n, 2))\n\n\n        errors_raw = {k: 0 for k in k_candidates}\n        errors_white = {k: 0 for k in k_candidates}\n\n        for i in range(n):\n            X_train = np.delete(X, i, axis=0)\n            y_train = np.delete(y, i)\n            x_test = X[i]\n            y_test = y[i]\n\n            # --- Raw Euclidean Metric ---\n            dists_raw = np.linalg.norm(X_train - x_test, axis=1)\n            sorted_neighbor_labels_raw = y_train[np.argsort(dists_raw)]\n\n            # --- Whitened Metric ---\n            # Compute whitening matrix W from this fold's training data\n            n_train, d_train = X_train.shape\n            cov = np.cov(X_train, rowvar=False, ddof=1)\n            \n            alpha = 1e-6 * np.trace(cov) / d_train\n            cov_reg = cov + alpha * np.identity(d_train)\n            \n            eigenvals, eigenvecs = np.linalg.eigh(cov_reg)\n            \n            # Clamp small/negative eigenvalues for stability\n            eigenvals[eigenvals = 0] = 1e-12\n\n            lambda_inv_sqrt = np.diag(1.0 / np.sqrt(eigenvals))\n            W = eigenvecs @ lambda_inv_sqrt @ eigenvecs.T\n            \n            diffs = X_train - x_test\n            # Since W is symmetric, W.T = W. dist = ||W * diff||\n            dists_white = np.linalg.norm(diffs @ W, axis=1)\n            sorted_neighbor_labels_white = y_train[np.argsort(dists_white)]\n\n            # --- Efficiently check all k values ---\n            label_sum_raw = np.cumsum(sorted_neighbor_labels_raw)\n            label_sum_white = np.cumsum(sorted_neighbor_labels_white)\n\n            for k in k_candidates:\n                # Prediction for raw metric\n                # current_sum is count of label 1s. Predict 1 if count_1 > k/2\n                current_sum_raw = label_sum_raw[k - 1]\n                pred_raw = 1 if current_sum_raw > k / 2 else 0\n                if pred_raw != y_test:\n                    errors_raw[k] += 1\n\n                # Prediction for whitened metric\n                current_sum_white = label_sum_white[k - 1]\n                pred_white = 1 if current_sum_white > k / 2 else 0\n                if pred_white != y_test:\n                    errors_white[k] += 1\n\n        # 3. Select optimal k for each metric\n        min_error_raw = min(errors_raw.values())\n        k_opt_raw = min(k for k, err in errors_raw.items() if err == min_error_raw)\n\n        min_error_white = min(errors_white.values())\n        k_opt_white = min(k for k, err in errors_white.items() if err == min_error_white)\n\n        all_results.append([k_opt_white, k_opt_raw])\n\n    # Format output as a string representation of a list of lists\n    result_str = \"[\" + \",\".join(f\"[{w},{r}]\" for w, r in all_results) + \"]\"\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "确定了距离度量之后，下一个关键问题是如何选择最佳的邻居数 $k$。这个选择会如何与不同的标准度量（如曼哈顿距离或切比雪夫距离）相互作用？在此练习  中，你将实现一个完整的交叉验证流程来选择最优的 $k$。通过针对 F1 分数在三种不同距离度量下进行优化，你将掌握一种稳健的模型选择方法，并直观地看到最优 $k$ 值对距离度量的几何特性和特定性能目标的敏感性。",
            "id": "3108117",
            "problem": "您需要实现一个完整的、确定性的方案，通过优化 F1 分数而非准确率来选择 $k$-近邻（k-NN）分类器中的邻居数 $k$，并分析距离度量的选择如何改变使 F1 分数最大化的 $k$ 值。请完全从第一性原理和定义出发。从统计学习的以下基础开始：(i) $k$-近邻分类器的定义，即在指定距离下，根据 $k$ 个最近训练点的多数投票进行分类，(ii) $L_{p}$ 范数作为 $\\mathbb{R}^{d}$ 中有效距离的定义，以及 (iii) 二元分类的 F1 分数的定义，即根据混淆矩阵计算出的精确率和召回率的调和平均数。\n\n您的程序必须：\n- 使用 $k$-近邻规则实现二元分类，采用无加权投票和以下确定性平局打破规则：对邻居排序时，按训练索引升序打破距离相等的情况；当投票数相等时，预测较小的标签，即 $0$。\n- 仅针对正类标签 $1$ 使用 F1 分数评估性能。对于一个具有正类计数 $\\mathrm{TP}$、$\\mathrm{FP}$ 和 $\\mathrm{FN}$ 的混淆矩阵，定义精确率为 $\\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FP})$（如果 $\\mathrm{TP}+\\mathrm{FP}0$），否则为 $0$；定义召回率为 $\\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FN})$（如果 $\\mathrm{TP}+\\mathrm{FN}0$），否则为 $0$；定义 F1 分数为 $2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall} / (\\mathrm{precision} + \\mathrm{recall})$（如果 $\\mathrm{precision} + \\mathrm{recall}  0$），否则为 $0$。\n- 在 $\\mathbb{R}^{2}$ 上使用三种距离度量：曼哈顿距离 $L_{1}$、欧几里得距离 $L_{2}$ 和切比雪夫距离 $L_{\\infty}$。对于一个差分向量 $\\Delta x \\in \\mathbb{R}^{2}$，它们的定义如下：$L_{1}(\\Delta x) = \\lvert \\Delta x_{1} \\rvert + \\lvert \\Delta x_{2} \\rvert$，$L_{2}(\\Delta x) = \\sqrt{(\\Delta x_{1})^{2} + (\\Delta x_{2})^{2}}$，以及 $L_{\\infty}(\\Delta x) = \\max\\{\\lvert \\Delta x_{1} \\rvert, \\lvert \\Delta x_{2} \\rvert\\}$。\n- 使用明确提供的折进行交叉验证来选择 $k$。对于集合 $\\{1,3,5,7\\}$ 中的每个候选 $k$ 和每种度量，将所有折中所有验证实例的预测汇集起来，形成一个单一的混淆矩阵并计算一个单一的 F1 分数（不要对每折的 F1 分数求平均）。选择使这个汇总的 F1 分数最大化的 $k$。如果不同 $k$ 值的 F1 分数出现平局，选择最小的 $k$。\n- 对每个测试用例，报告为 $L_{1}$、$L_{2}$ 和 $L_{\\infty}$ 选择的 $k$、相应的 F1 分数（四舍五入到三位小数），以及整数偏移量 $\\Delta k_{2-1} = k^{\\star}_{L_{2}} - k^{\\star}_{L_{1}}$ 和 $\\Delta k_{\\infty-2} = k^{\\star}_{L_{\\infty}} - k^{\\star}_{L_{2}}$。\n\n需要使用的基本定义：\n- $k$-近邻分类：给定一个训练集 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$，其中 $x_{i} \\in \\mathbb{R}^{d}$ 且 $y_{i} \\in \\{0,1\\}$，对于一个查询点 $x$，根据选定的度量计算其到所有训练点的距离，选择 $k$ 个最小距离（按训练索引打破平局），并通过这些 $k$ 个标签的无加权多数投票来预测标签（投票平局时倾向于标签 $0$）。\n- 距离度量：如上定义的 $L_{1}$、$L_{2}$ 和 $L_{\\infty}$。\n- F1 分数：如上根据正类的精确率和召回率定义。\n\n测试套件：\n实现您的程序，使其能在以下三个数据集上运行，每个数据集都有预设的 $3$ 折分区。每个数据集都是二维的，标签是二元的，其中 $0$ 为负类，$1$ 为正类。索引从 $0$ 开始。所有坐标都是 $\\mathbb{R}$ 中的实数。\n\n测试用例 1 (平衡、分离良好的簇)：\n- 点 $x_{i} \\in \\mathbb{R}^{2}$，其中 $i \\in \\{0,1,\\dots,19\\}$：\n  - 类别 $0$ 在索引 $0$ 到 $9$ 处：$(0.0,0.0)$, $(0.0,1.0)$, $(1.0,0.0)$, $(1.0,1.0)$, $(0.5,0.2)$, $(1.2,-0.1)$, $(0.3,1.4)$, $(1.5,0.7)$, $(-0.2,0.5)$, $(0.8,1.2)$。\n  - 类别 $1$ 在索引 $10$ 到 $19$ 处：$(5.0,5.0)$, $(5.0,6.0)$, $(6.0,5.0)$, $(6.0,6.0)$, $(5.2,5.1)$, $(5.8,5.2)$, $(6.3,5.7)$, $(5.1,6.2)$, $(5.5,5.6)$, $(6.1,6.2)$。\n- 标签 $y_{i}$：$y_{i} = 0$ 对于 $i \\in \\{0,1,\\dots,9\\}$，$y_{i} = 1$ 对于 $i \\in \\{10,11,\\dots,19\\}$。\n- 折：\n  - 折 A (验证索引)：$\\{0,1,2,10,11,12,13\\}$。\n  - 折 B (验证索引)：$\\{3,4,5,14,15,16\\}$。\n  - 折 C (验证索引)：$\\{6,7,8,9,17,18,19\\}$。\n\n测试用例 2 (不平衡、重叠)：\n- 点 $x_{i} \\in \\mathbb{R}^{2}$，其中 $i \\in \\{0,1,\\dots,19\\}$：\n  - 类别 $0$ 在索引 $0$ 到 $13$ 处：$(0.0,0.0)$, $(0.2,-0.1)$, $(-0.1,0.3)$, $(0.4,0.1)$, $(0.6,0.2)$, $(0.7,0.4)$, $(0.8,0.2)$, $(1.0,0.0)$, $(1.1,0.3)$, $(1.2,0.1)$, $(1.3,0.2)$, $(0.9,0.6)$, $(0.5,0.8)$, $(0.2,0.9)$。\n  - 类别 $1$ 在索引 $14$ 到 $19$ 处：$(1.1,1.1)$, $(1.2,1.0)$, $(1.3,1.1)$, $(1.4,1.2)$, $(1.5,1.3)$, $(1.6,1.2)$。\n- 标签 $y_{i}$：$y_{i} = 0$ 对于 $i \\in \\{0,1,\\dots,13\\}$，$y_{i} = 1$ 对于 $i \\in \\{14,15,\\dots,19\\}$。\n- 折：\n  - 折 A (验证索引)：$\\{0,1,2,14,15,16,17\\}$。\n  - 折 B (验证索引)：$\\{3,4,5,6,18,19\\}$。\n  - 折 C (验证索引)：$\\{7,8,9,10,11,12,13\\}$。\n\n测试用例 3 (各向异性且有平局情况)：\n- 点 $x_{i} \\in \\mathbb{R}^{2}$，其中 $i \\in \\{0,1,\\dots,17\\}$：\n  - 类别 $0$ 在索引 $0$ 到 $9$ 处：$(0.0,0.0)$, $(0.0,1.0)$, $(0.0,1.0)$, $(10.0,0.0)$, $(10.0,1.0)$, $(10.0,2.0)$, $(0.0,2.0)$, $(10.0,-1.0)$, $(0.0,-1.0)$, $(10.0,3.0)$。\n  - 类别 $1$ 在索引 $10$ 到 $17$ 处：$(5.0,0.0)$, $(5.0,1.0)$, $(5.0,2.0)$, $(5.0,3.0)$, $(5.0,-1.0)$, $(5.0,1.0)$, $(5.0,2.0)$, $(5.0,0.0)$。\n- 标签 $y_{i}$：$y_{i} = 0$ 对于 $i \\in \\{0,1,\\dots,9\\}$，$y_{i} = 1$ 对于 $i \\in \\{10,11,\\dots,17\\}$。\n- 折：\n  - 折 A (验证索引)：$\\{0,1,10,11,12,13\\}$。\n  - 折 B (验证索引)：$\\{2,3,4,14,15,16\\}$。\n  - 折 C (验证索引)：$\\{5,6,7,8,9,17\\}$。\n\n候选邻居数：\n- $k \\in \\{1,3,5,7\\}$。\n\n距离度量：\n- 如上定义的 $L_{1}$、$L_{2}$、$L_{\\infty}$。\n\n要求的最终输出格式：\n- 对于每个测试用例，按以下顺序输出一个包含 $8$ 个值的列表：$k^{\\star}_{L_{1}}$、$k^{\\star}_{L_{2}}$、$k^{\\star}_{L_{\\infty}}$、$\\mathrm{F1}_{L_{1}}$、$\\mathrm{F1}_{L_{2}}$、$\\mathrm{F1}_{L_{\\infty}}$、$\\Delta k_{2-1}$、$\\Delta k_{\\infty-2}$，其中 $\\mathrm{F1}$ 值四舍五入到三位小数。将三个用例的列表聚合成一个单一列表。\n- 您的程序应生成单行输出，其中包含此聚合结果，格式为逗号分隔的列表，无空格，例如：$[[1,3,5,0.750,0.800,0.820,2,2],[\\dots],[\\dots]]$。\n\n角度单位和物理单位不适用。\n\n您的实现必须是自包含的，且不得读取输入。必须按给定方式使用提供的折，其中训练索引定义为数据集中每折验证索引的补集。",
            "solution": "该问题要求实现和分析一个 $k$-近邻 (k-NN) 分类器。核心任务是从候选集 $\\{1, 3, 5, 7\\}$ 中选择最优的邻居数 $k$，并在三种不同的距离度量下比较结果：曼哈顿 ($L_1$)、欧几里得 ($L_2$) 和切比雪夫 ($L_\\infty$)。优化标准是正类（标签 $1$）的 F1 分数，使用指定的交叉验证程序进行评估。\n\n该解决方案从第一性原理出发，从各组件的定义到完整的算法。\n\n### 1. 基本原理\n\n#### 1.1. $k$-近邻 (k-NN) 分类器\nk-NN 算法是一种用于分类和回归的非参数方法。对于分类任务，其操作如下：\n给定一个训练数据集 $D = \\{(x_i, y_i)\\}_{i=1}^n$，其中 $x_i \\in \\mathbb{R}^d$ 是特征向量，$y_i \\in \\{0, 1\\}$ 是二元标签，对一个新的查询点 $x_q$ 的标签预测通过以下步骤进行：\n1.  **距离计算**：使用选定的距离度量，计算 $x_q$ 到每个训练点 $x_i$ 的距离。\n2.  **邻居识别**：找出与 $x_q$ 最接近的 $k$ 个训练点 $(x_i, y_i)$。这 $k$ 个点构成了 $x_q$ 的“邻域”。\n3.  **多数投票**：$x_q$ 的预测标签 $\\hat{y}_q$ 是这 $k$ 个邻居中最常见的标签。\n\n问题指定了确定性的平局打破规则，这对可复现性至关重要：\n-   如果多个训练点到 $x_q$ 的距离相同，它们的相对顺序由它们在训练数据中的原始索引决定，索引较小的被认为“更近”。\n-   如果多数投票导致平局（例如，对于偶数 $k$，或有多个类别时），预测默认为较小的类别标签，在此二元情况下为 $0$。\n\n#### 1.2. $\\mathbb{R}^2$ 中的距离度量\n距离度量的选择决定了邻域的几何形状，从而决定了决策边界的形状。问题为差分向量 $\\Delta x = (\\Delta x_1, \\Delta x_2) \\in \\mathbb{R}^2$ 指定了三种 $L_p$ 范数：\n-   **曼哈顿距离 ($L_1$)**: $d_1(\\Delta x) = \\|\\Delta x\\|_1 = |\\Delta x_1| + |\\Delta x_2|$。该度量如同在网格上移动一样测量距离，其等距“圆”是菱形。\n-   **欧几里得距离 ($L_2$)**: $d_2(\\Delta x) = \\|\\Delta x\\|_2 = \\sqrt{(\\Delta x_1)^2 + (\\Delta x_2)^2}$。这是标准的直线距离，其等距圆是传统圆形。\n-   **切比雪夫距离 ($L_\\infty$)**: $d_\\infty(\\Delta x) = \\|\\Delta x\\|_\\infty = \\max(|\\Delta x_1|, |\\Delta x_2|)$。该度量测量沿任何单个坐标轴的最大距离，其等距圆是正方形。\n\n不同的几何形状意味着对于给定的查询点，最近的 $k$ 个邻居集合可能会因使用的度量不同而改变。这反过来可能导致每种度量的最优 $k$ 值也不同。\n\n#### 1.3. 性能评估：F1 分数\n对于二元分类，混淆矩阵基于正类（标签 $1$）的真阳性 ($\\mathrm{TP}$)、假阳性 ($\\mathrm{FP}$)、真阴性 ($\\mathrm{TN}$) 和假阴性 ($\\mathrm{FN}$) 的计数来总结性能。\n-   $\\mathrm{TP}$：正确预测为正类的实例（真实标签 $1$，预测标签 $1$）。\n-   $\\mathrm{FP}$：错误预测为正类的实例（真实标签 $0$，预测标签 $1$）。\n-   $\\mathrm{FN}$：错误预测为负类的实例（真实标签 $1$，预测标签 $0$）。\n-   $\\mathrm{TN}$：正确预测为负类的实例（真实标签 $0$，预测标签 $0$）。\n\n由此，我们定义两个关键指标：\n-   **精确率 (Precision)**：正类预测中正确的比例。它衡量分类器的准确性。\n    $$ \\text{精确率} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} $$\n-   **召回率 (Recall / 灵敏度 Sensitivity)**：实际正类实例中被正确识别的比例。它衡量分类器的完备性。\n    $$ \\text{召回率} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} $$\n\nF1 分数是精确率和召回率的调和平均数，提供了一个平衡两者的单一指标：\n$$ \\mathrm{F1} = 2 \\cdot \\frac{\\text{精确率} \\cdot \\text{召回率}}{\\text{精确率} + \\text{召回率}} $$\n问题正确地指明了如何处理分母为零的情况，以避免除零错误。F1 分数在不平衡数据集中特别有用，因为在这些数据集中，准确率可能具有误导性。\n\n### 2. 算法流程\n\n整体流程涉及一个嵌套循环结构，为每个测试用例通过交叉验证进行模型选择。\n\n**对于每个提供的测试用例：**\n1.  初始化一个列表，用于存储该用例的最终结果。\n2.  **对于每个距离度量 $d \\in \\{L_1, L_2, L_\\infty\\}$：**\n    a. 初始化一个列表，用于存储每个候选 $k$ 的 F1 分数。\n    b. **对于每个候选邻居数 $k \\in \\{1, 3, 5, 7\\}$：**\n        i.   初始化汇总的混淆矩阵计数器：$\\mathrm{TP}_{\\text{pool}}=0$, $\\mathrm{FP}_{\\text{pool}}=0$, $\\mathrm{FN}_{\\text{pool}}=0$。\n        ii.  **对于提供的每个交叉验证折：**\n             A.  识别训练集（当前验证折之外的所有数据点）和验证集（由该折的索引指定的点）。\n             B.  **对于每个验证点 $(x_v, y_v)$：**\n                 1.  使用当前度量 $d$ 计算 $x_v$到训练集中每个点 $(x_t, y_t)$ 的距离。将这些存储为 `(距离, 训练索引, 训练标签)` 的元组。\n                 2.  首先按距离（升序）然后按训练索引（升序）对这些元组进行排序，以解决平局问题。\n                 3.  从排序后的列表中选择前 $k$ 个元组。这些是 $k$ 个最近的邻居。\n                 4.  统计这 $k$ 个邻居的标签投票。设 `count_1` 是标签为 $1$ 的邻居数量，`count_0` 是标签为 $0$ 的邻居数量。\n                 5.  确定预测标签 $\\hat{y}_v$。如果 `count_1  count_0`，则 $\\hat{y}_v = 1$。如果 `count_0  count_1`，则 $\\hat{y}_v = 0$。如果 `count_0 == count_1`（投票平局），则预测默认为 $0$。\n                 6.  根据真实标签 $y_v$ 和预测标签 $\\hat{y}_v$ 更新汇总的混淆矩阵计数器：\n                     - 如果 $y_v=1$ 且 $\\hat{y}_v=1$，则增加 $\\mathrm{TP}_{\\text{pool}}$。\n                     - 如果 $y_v=0$ 且 $\\hat{y}_v=1$，则增加 $\\mathrm{FP}_{\\text{pool}}$。\n                     - 如果 $y_v=1$ 且 $\\hat{y}_v=0$，则增加 $\\mathrm{FN}_{\\text{pool}}$。\n        iii. 遍历所有折后，使用汇总的计数器 $\\mathrm{TP}_{\\text{pool}}$、$\\mathrm{FP}_{\\text{pool}}$ 和 $\\mathrm{FN}_{\\text{pool}}$ 以及指定的公式计算当前 $k$ 的最终 F1 分数。\n        iv. 存储当前 $k$ 计算出的 F1 分数。\n    c. 在评估完所有候选 $k$ 值后，找到当前度量的最优 $k^\\star$。这是产生最大 F1 分数的 $k$。如果 F1 分数出现平局，则选择获胜者中最小的 $k$。\n    d. 存储最优的 $k^\\star$ 及其对应的 F1 分数。\n\n3.  遍历完所有三种度量后，确定最优值 $(k^\\star_{L_1}, \\mathrm{F1}_{L_1})$、$(k^\\star_{L_2}, \\mathrm{F1}_{L_2})$ 和 $(k^\\star_{L_\\infty}, \\mathrm{F1}_{L_\\infty})$。\n4.  计算所需的偏移量：$\\Delta k_{2-1} = k^\\star_{L_2} - k^\\star_{L_1}$ 和 $\\Delta k_{\\infty-2} = k^\\star_{L_\\infty} - k^\\star_{L_2}$。\n5.  为该测试用例组装最终列表：$[k^\\star_{L_1}, k^\\star_{L_2}, k^\\star_{L_\\infty}, \\mathrm{F1}_{L_1}, \\mathrm{F1}_{L_2}, \\mathrm{F1}_{L_\\infty}, \\Delta k_{2-1}, \\Delta k_{\\infty-2}]$，其中 F1 分数格式化为三位小数。\n\n对提供的三个测试用例中的每一个都重复此完整过程。最终输出是所有测试用例结果的聚合，形成一个单一的结构化列表。该实现需要仔细处理数据结构、循环，并精确应用所有指定规则。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the k-NN optimization problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.0, 1.0), (1.0, 0.0), (1.0, 1.0), (0.5, 0.2), (1.2, -0.1), (0.3, 1.4), (1.5, 0.7), (-0.2, 0.5), (0.8, 1.2),\n                (5.0, 5.0), (5.0, 6.0), (6.0, 5.0), (6.0, 6.0), (5.2, 5.1), (5.8, 5.2), (6.3, 5.7), (5.1, 6.2), (5.5, 5.6), (6.1, 6.2)\n            ]),\n            \"labels\": np.array([0]*10 + [1]*10),\n            \"folds\": [\n                {0, 1, 2, 10, 11, 12, 13},\n                {3, 4, 5, 14, 15, 16},\n                {6, 7, 8, 9, 17, 18, 19}\n            ]\n        },\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.2, -0.1), (-0.1, 0.3), (0.4, 0.1), (0.6, 0.2), (0.7, 0.4), (0.8, 0.2), (1.0, 0.0), (1.1, 0.3), (1.2, 0.1), (1.3, 0.2), (0.9, 0.6), (0.5, 0.8), (0.2, 0.9),\n                (1.1, 1.1), (1.2, 1.0), (1.3, 1.1), (1.4, 1.2), (1.5, 1.3), (1.6, 1.2)\n            ]),\n            \"labels\": np.array([0]*14 + [1]*6),\n            \"folds\": [\n                {0, 1, 2, 14, 15, 16, 17},\n                {3, 4, 5, 6, 18, 19},\n                {7, 8, 9, 10, 11, 12, 13}\n            ]\n        },\n        {\n            \"points\": np.array([\n                (0.0, 0.0), (0.0, 1.0), (0.0, 1.0), (10.0, 0.0), (10.0, 1.0), (10.0, 2.0), (0.0, 2.0), (10.0, -1.0), (0.0, -1.0), (10.0, 3.0),\n                (5.0, 0.0), (5.0, 1.0), (5.0, 2.0), (5.0, 3.0), (5.0, -1.0), (5.0, 1.0), (5.0, 2.0), (5.0, 0.0)\n            ]),\n            \"labels\": np.array([0]*10 + [1]*8),\n            \"folds\": [\n                {0, 1, 10, 11, 12, 13},\n                {2, 3, 4, 14, 15, 16},\n                {5, 6, 7, 8, 9, 17}\n            ]\n        }\n    ]\n    \n    candidate_k = [1, 3, 5, 7]\n    metrics = {\n        'L1': lambda v: np.abs(v[0]) + np.abs(v[1]),\n        'L2': lambda v: np.sqrt(v[0]**2 + v[1]**2),\n        'L_inf': lambda v: np.max(np.abs(v))\n    }\n\n    all_test_results = []\n\n    for case_data in test_cases:\n        points = case_data[\"points\"]\n        labels = case_data[\"labels\"]\n        folds = case_data[\"folds\"]\n        num_points = len(points)\n        all_indices = set(range(num_points))\n\n        case_results = {}\n\n        for metric_name, dist_func in metrics.items():\n            k_f1_scores = {}\n\n            for k in candidate_k:\n                tp, fp, fn = 0, 0, 0\n                \n                for val_indices in folds:\n                    train_indices = sorted(list(all_indices - val_indices))\n                    \n                    train_points = points[train_indices]\n                    train_labels = labels[train_indices]\n\n                    for val_idx in val_indices:\n                        query_point = points[val_idx]\n                        true_label = labels[val_idx]\n\n                        distances = []\n                        for i, train_idx in enumerate(train_indices):\n                            dist = dist_func(query_point - train_points[i])\n                            distances.append((dist, train_idx, train_labels[i]))\n                        \n                        distances.sort() # Sorts by first element (dist), then second (train_idx)\n\n                        neighbors_labels = [label for _, _, label in distances[:k]]\n                        \n                        count_1 = sum(1 for label in neighbors_labels if label == 1)\n                        count_0 = k - count_1\n\n                        if count_1 > count_0:\n                            pred_label = 1\n                        else: # includes count_0 > count_1 and count_0 == count_1\n                            pred_label = 0\n\n                        if pred_label == 1 and true_label == 1:\n                            tp += 1\n                        elif pred_label == 1 and true_label == 0:\n                            fp += 1\n                        elif pred_label == 0 and true_label == 1:\n                            fn += 1\n                \n                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n                f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n                \n                k_f1_scores[k] = f1_score\n\n            best_k = -1\n            max_f1 = -1.0\n            for k_val in sorted(k_f1_scores.keys()):\n                if k_f1_scores[k_val] > max_f1:\n                    max_f1 = k_f1_scores[k_val]\n                    best_k = k_val\n            \n            case_results[metric_name] = (best_k, max_f1)\n\n        k_l1, f1_l1 = case_results['L1']\n        k_l2, f1_l2 = case_results['L2']\n        k_linf, f1_linf = case_results['L_inf']\n        \n        delta_k_21 = k_l2 - k_l1\n        delta_k_inf2 = k_linf - k_l2\n\n        result_list = [\n            k_l1, k_l2, k_linf,\n            round(f1_l1, 3), round(f1_l2, 3), round(f1_linf, 3),\n            delta_k_21, delta_k_inf2\n        ]\n        all_test_results.append(result_list)\n\n    # Construct the final output string manually to avoid spaces.\n    output_str = \"[\"\n    for i, res in enumerate(all_test_results):\n        # Format F1 scores correctly\n        res[3] = f\"{res[3]:.3f}\"\n        res[4] = f\"{res[4]:.3f}\"\n        res[5] = f\"{res[5]:.3f}\"\n        output_str += f\"[{','.join(map(str, res))}]\"\n        if i  len(all_test_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "我们关于“距离”和“邻域”的直觉是在二维或三维世界中形成的。当数据存在于数百甚至数千维的空间中时，这些概念会如何变化？这个练习  通过一个受控的模拟实验，带你探索“维度灾难”对 k-NN 算法的影响。通过观察最优 $k$ 值如何随着数据维度的增加而变化，你将深入理解为何 k-NN 在高维空间中的行为可能与直觉相悖，以及为何需要采用像余弦距离这样的不同策略来处理文本分析等现代应用。",
            "id": "3108143",
            "problem": "给定一项任务，要求在一个受控的合成环境中，实证研究单位归一化嵌入向量的维度 $p$ 如何影响在余弦距离下 k 近邻分类的最优邻居数 $k$ 的选择。你的程序必须在多个维度上模拟带标签的数据，为一组固定的奇数邻居数量评估验证准确率，并报告每个维度下使验证准确率最大化的 $k$。该实验应可复现且完全指定如下。\n\n基本设定：\n- 两个非零向量 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^p$ 之间的余弦距离定义为 $d_{\\cos}(\\mathbf{x},\\mathbf{y}) = 1 - \\dfrac{\\mathbf{x}^\\top \\mathbf{y}}{\\lVert \\mathbf{x} \\rVert_2 \\lVert \\mathbf{y} \\rVert_2}$。如果所有向量都单位归一化，则简化为 $d_{\\cos}(\\mathbf{x},\\mathbf{y}) = 1 - \\mathbf{x}^\\top \\mathbf{y}$。\n- k 近邻分类器根据指定的距离，将查询点分配给其 $k$ 个最近的训练样本中的多数标签；使用奇数 $k$ 以避免投票平局。\n\n数据模型：\n- 恰好有 $2$ 个均衡的类别，其类别原型为单位球面上的 $\\mathbf{u}_1, \\mathbf{u}_2 \\in \\mathbb{R}^p$，且内积 $\\mathbf{u}_1^\\top \\mathbf{u}_2 = c$ 为一指定值，其中 $c \\in (-1,1)$ 是一个控制类别分离度的固定标量。将 $\\mathbf{u}_1$ 构建为第一个基向量，即 $\\mathbf{u}_1 = (1,0,\\dots,0)^\\top \\in \\mathbb{R}^p$，并将 $\\mathbf{u}_2$ 构建为\n$$\n\\mathbf{u}_2 = c \\, \\mathbf{e}_1 + \\sqrt{1-c^2} \\, \\mathbf{e}_2,\n$$\n其中 $\\mathbf{e}_1, \\mathbf{e}_2$ 是 $\\mathbb{R}^p$ 中的前两个标准基向量。\n- 对于每个类别 $y \\in \\{0,1\\}$ 及其原型 $\\mathbf{u}_{y+1}$，通过以下方式生成样本：\n$$\n\\tilde{\\mathbf{x}} = \\mathbf{u}_{y+1} + \\frac{\\sigma}{\\sqrt{p}} \\mathbf{z}, \\quad \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_p), \\quad \\mathbf{x} = \\frac{\\tilde{\\mathbf{x}}}{\\lVert \\tilde{\\mathbf{x}} \\rVert_2}.\n$$\n这会产生具有受控扰动幅度（由 $\\sigma  0$ 控制）的单位归一化嵌入。\n- 使用恰好 $2$ 个类别，每个数据划分中样本数量相等。在所有测试用例中使用相同的数据生成参数 $\\sigma$ 和 $c$，但改变维度 $p$。\n\n评估协议：\n- 对每个测试用例，使用上述模型的独立抽取，创建一个大小为 $n_{\\text{train}}$ 的训练集和一个大小为 $n_{\\text{val}}$ 的验证集，两者均为类别均衡的。\n- 考虑候选邻居数量 $k \\in \\{1,3,5,7,9,11,13,15\\}$。对于每个 $k$，使用训练集构建一个基于余弦距离的 k 近邻分类器，并计算其验证准确率，该准确率定义为正确分类的验证样本所占的比例，以 $[0,1]$ 区间内的小数表示。\n- 选择获得最高验证准确率的 $k$。如果多个 $k$ 值并列获得最高验证准确率，则选择其中最小的 $k$。\n\n确定性：\n- 对每个测试用例，使用一个固定的伪随机数生成器种子 $s = 424242 + p$ 来生成该维度 $p$ 的训练集和验证集，以保证结果可复现。不要使用任何其他随机性来源。\n\n测试套件：\n- 使用以下三个仅维度 $p$ 不同的测试用例：\n  1. 用例 A: $p = 10$, $n_{\\text{train}} = 400$, $n_{\\text{val}} = 200$, $\\sigma = 0.7$, $c = 0.25$。\n  2. 用例 B: $p = 100$, $n_{\\text{train}} = 400$, $n_{\\text{val}} = 200$, $\\sigma = 0.7$, $c = 0.25$。\n  3. 用例 C: $p = 500$, $n_{\\text{train}} = 400$, $n_{\\text{val}} = 200$, $\\sigma = 0.7$, $c = 0.25$。\n\n输出规范：\n- 你的程序必须产生单行输出，其中包含按上述测试用例顺序排列的最优邻居数，格式为方括号内的逗号分隔列表，例如 `[1,3,5]`。\n- 每个测试用例的输出必须是所选 $k$ 值的整数。\n\n约束：\n- 严格按照规定实现完整的模拟、分类和模型选择过程。\n- 在计算余弦相似度之前，将所有生成的向量归一化为单位范数，以确保数值稳定性。\n- 仅使用允许的库，并遵守确定性种子规则。\n\n你的任务：编写一个完整、可运行的程序，该程序能够生成数据，为每个候选 $k$ 评估验证准确率，根据平局规则为每个测试用例选择最优 $k$，并以所需格式在单行上打印这三个选定的 $k$ 值。",
            "solution": "用户的问题是进行一个数值实验，以确定 k 近邻（KNN）分类器中的最优邻居数 $k$。该研究在特定的、受控的条件下进行：数据是合成的，从一个单位超球面上的两个类别的模型生成，并且分类基于余弦距离。实验将在三种不同的数据维度 $p$ 下重复进行，以观察 $p$ 如何影响 $k$ 的选择。\n\n该问题已经过验证，被认为是有效的。它在科学上是合理的、定义明确的，并且所有参数和过程都得到了明确的规定，确保了实验的可复现性。我们现在将进行详细的解答。\n\n### 1. 模拟原理\n\n该问题的核心是为 KNN 分类器的超参数 $k$ 进行模型选择。模型选择的一般过程包括：\n1.  定义一组候选超参数值。\n2.  将可用数据划分为训练集和验证集。\n3.  对于每个候选超参数值，在训练集上训练一个模型。\n4.  使用选定的度量标准，在验证集上评估每个训练好的模型的性能。\n5.  选择在验证集上产生最佳性能的超参数值。\n\n本问题精确地规定了所有这些组成部分。\n\n### 2. 数据生成模型\n\n数据生成的方式是在一个 $p$ 维单位超球面的表面上创建两个不同但可能重叠的点集类别。\n\n**类别原型**：两个类别的中心由 $\\mathbb{R}^p$ 中的两个原型向量 $\\mathbf{u}_1$ 和 $\\mathbf{u}_2$ 定义。它们被构造成单位向量，并具有指定的余弦相似度（内积）$\\mathbf{u}_1^\\top \\mathbf{u}_2 = c$。\n-   $\\mathbf{u}_1$ 被设置为第一个标准基向量：\n    $$ \\mathbf{u}_1 = \\mathbf{e}_1 = (1, 0, \\dots, 0)^\\top $$\n-   $\\mathbf{u}_2$ 在由 $\\mathbf{e}_1$ 和 $\\mathbf{e}_2$ 张成的平面中构造：\n    $$ \\mathbf{u}_2 = c \\, \\mathbf{e}_1 + \\sqrt{1-c^2} \\, \\mathbf{e}_2 $$\n    这种构造确保了 $\\lVert \\mathbf{u}_1 \\rVert_2 = 1$, $\\lVert \\mathbf{u}_2 \\rVert_2 = 1$, 以及 $\\mathbf{u}_1^\\top \\mathbf{u}_2 = c$。参数 $c \\in (-1,1)$ 控制类别之间的分离度：$c \\to 1$ 使类别非常接近，而 $c \\to -1$ 使它们呈对跖状态。\n\n**样本生成**：对于每个类别 $y \\in \\{0, 1\\}$，数据点通过取相应的原型 $\\mathbf{u}_{y+1}$ 并添加高斯噪声来生成。\n1.  创建一个预归一化向量 $\\tilde{\\mathbf{x}}$：\n    $$ \\tilde{\\mathbf{x}} = \\mathbf{u}_{y+1} + \\frac{\\sigma}{\\sqrt{p}} \\mathbf{z}, \\quad \\text{其中} \\quad \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_p) $$\n    在这里，$\\sigma  0$ 控制扰动的幅度。噪声项 $\\mathbf{z}$ 按 $\\frac{1}{\\sqrt{p}}$ 缩放，以确保噪声的期望平方范数 $\\mathbb{E}\\left[\\left\\lVert \\frac{\\sigma}{\\sqrt{p}} \\mathbf{z} \\right\\rVert_2^2\\right]$ 等于 $\\sigma^2$，从而与维度 $p$ 无关。这可以防止随着维度增加，噪声压倒信号（原型向量）。\n2.  然后将向量投影到单位超球面上：\n    $$ \\mathbf{x} = \\frac{\\tilde{\\mathbf{x}}}{\\lVert \\tilde{\\mathbf{x}} \\rVert_2} $$\n    此步骤确保所有数据点都具有单位范数，这是简化余弦距离计算的前提条件。\n\n对于每个实验用例（由维度 $p$ 定义），我们必须生成一个大小为 $n_{\\text{train}}$ 的训练集和一个大小为 $n_{\\text{val}}$ 的验证集，两者都包含来自两个类别的相等数量的样本。\n\n**可复现性**：为确保结果的确定性，对每个维度 $p$ 使用一个特定的伪随机数生成器种子 $s = 424242 + p$。\n\n### 3. k 近邻分类\n\nKNN 分类器是一种非参数算法。要对一个查询点 $\\mathbf{x}_q$ 进行分类，它执行以下步骤：\n1.  计算 $\\mathbf{x}_q$ 与训练集中每个点 $\\mathbf{x}_i$ 之间的距离。\n2.  识别出与 $\\mathbf{x}_q$ 最接近的 $k$ 个训练点。这些是它的 k 个最近邻。\n3.  将 $\\mathbf{x}_q$ 归类为其 k 个最近邻中最频繁出现的类别标签（多数投票）。问题规定使用奇数 $k$ 以防止投票平局。\n\n**距离度量**：问题要求使用余弦距离：\n$$ d_{\\cos}(\\mathbf{x}_i, \\mathbf{x}_j) = 1 - \\frac{\\mathbf{x}_i^\\top \\mathbf{x}_j}{\\lVert \\mathbf{x}_i \\rVert_2 \\lVert \\mathbf{x}_j \\rVert_2} $$\n由于所有数据点都经过归一化，具有单位范数，即 $\\lVert \\mathbf{x} \\rVert_2 = 1$，这简化为：\n$$ d_{\\cos}(\\mathbf{x}_i, \\mathbf{x}_j) = 1 - \\mathbf{x}_i^\\top \\mathbf{x}_j $$\n最小化此距离等价于最大化余弦相似度 $\\text{sim}_{\\cos}(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^\\top \\mathbf{x}_j$。在计算上，通过一次矩阵乘法 $\\mathbf{S} = \\mathbf{X}_{\\text{val}} \\mathbf{X}_{\\text{train}}^\\top$ 来计算所有验证点和所有训练点之间的余弦相似度矩阵是高效的。$\\mathbf{S}$ 的每一行随后包含了单个验证点到所有训练点的相似度。\n\n### 4. 评估和最优 $k$ 的选择\n\n目标是从候选集 $K = \\{1, 3, 5, 7, 9, 11, 13, 15\\}$ 中找到最优的 $k$。对于给定的测试用例，其过程如下：\n1.  生成训练数据 $(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$ 和验证数据 $(\\mathbf{X}_{\\text{val}}, \\mathbf{y}_{\\text{val}})$。\n2.  对于每个验证点，通过对余弦相似度进行排序，找到其在训练集中的最近邻。\n3.  对于每个候选值 $k \\in K$：\n    a. 对于每个验证点，通过在其 $k$ 个最近邻中进行多数投票来确定预测标签。\n    b. 计算验证准确率：\n       $$ \\text{Accuracy}(k) = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} \\mathbb{I}(\\hat{y}_i = y_i) $$\n       其中 $\\hat{y}_i$ 是第 $i$ 个验证样本的预测标签， $y_i$ 是其真实标签，$\\mathbb{I}(\\cdot)$ 是指示函数。\n4.  在计算完所有候选 $k$ 值的准确率后，选择最优的 $k^*$。选择规则是选择使准确率最大化的 $k$。如果出现平局，则选择那些达到最大准确率的 $k$ 中最小的一个。\n\n对于三个测试用例中的每一个，都重复整个过程，仅改变维度 $p$。\n\n### 5. 算法实现\n\n对于每个测试用例 $(p, n_{\\text{train}}, n_{\\text{val}}, \\sigma, c)$，实现将遵循以下步骤：\n\n1.  **初始化**：将随机数生成器的种子设置为 $s = 424242 + p$。\n2.  **生成数据**：\n    -   构造原型 $\\mathbf{u}_1$ 和 $\\mathbf{u}_2$。\n    -   为类别 0 生成 $n_{\\text{train}}/2$ 个样本，为类别 1 生成 $n_{\\text{train}}/2$ 个样本，以形成 $(\\mathbf{X}_{\\text{train}}, \\mathbf{y}_{\\text{train}})$。对 $\\mathbf{X}_{\\text{train}}$ 中的所有向量进行归一化。\n    -   对验证集重复此过程以获得 $(\\mathbf{X}_{\\text{val}}, \\mathbf{y}_{\\text{val}})$。\n3.  **计算相似度**：计算相似度矩阵 $\\mathbf{S} = \\mathbf{X}_{\\text{val}} \\mathbf{X}_{\\text{train}}^\\top$。其形状将为 $(n_{\\text{val}}, n_{\\text{train}})$。\n4.  **找到邻居索引**：对于每个验证样本（$\\mathbf{S}$ 的每一行），按相似度降序对训练样本索引进行排序。这可以通过对 $-\\mathbf{S}$ 使用 `numpy.argsort` 来高效完成。\n5.  **评估每个 $k$**：\n    -   遍历排序后的候选列表 $k \\in \\{1, 3, \\dots, 15\\}$。\n    -   对于每个 $k$，选择前 $k$ 个邻居的索引。\n    -   使用这些索引从 $\\mathbf{y}_{\\text{train}}$ 中检索邻居的标签。\n    -   使用 `scipy.stats.mode` 为每个验证样本找到多数标签。\n    -   计算并存储准确率。\n6.  **选择最优 $k$**：找到达到的最大准确率以及产生该准确率的最小 $k$。\n7.  **存储结果**：将当前测试用例的最优 $k$ 附加到结果列表中。\n8.  **输出**：处理完所有测试用例后，格式化并打印最优 $k$ 值的列表。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import mode\n\ndef solve():\n    \"\"\"\n    Main function to run the KNN simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (p, n_train, n_val, sigma, c)\n        (10, 400, 200, 0.7, 0.25),   # Case A\n        (100, 400, 200, 0.7, 0.25),  # Case B\n        (500, 400, 200, 0.7, 0.25),  # Case C\n    ]\n\n    candidate_ks = [1, 3, 5, 7, 9, 11, 13, 15]\n    \n    optimal_k_results = []\n\n    for p, n_train, n_val, sigma, c in test_cases:\n        seed = 424242 + p\n        optimal_k = find_optimal_k(p, n_train, n_val, sigma, c, seed, candidate_ks)\n        optimal_k_results.append(optimal_k)\n    \n    print(f\"[{','.join(map(str, optimal_k_results))}]\")\n\ndef generate_data(p, n_samples, sigma, c, u1, u2, rng):\n    \"\"\"\n    Generates balanced, unit-normalized data for two classes.\n    \"\"\"\n    n_per_class = n_samples // 2\n    \n    # Class 0 data generation\n    z0 = rng.normal(loc=0.0, scale=1.0, size=(n_per_class, p))\n    x_tilde0 = u1 + (sigma / np.sqrt(p)) * z0\n    \n    # Class 1 data generation\n    z1 = rng.normal(loc=0.0, scale=1.0, size=(n_per_class, p))\n    x_tilde1 = u2 + (sigma / np.sqrt(p)) * z1\n    \n    # Combine and normalize\n    X = np.vstack([x_tilde0, x_tilde1])\n    norms = np.linalg.norm(X, axis=1, keepdims=True)\n    # Avoid division by zero, although very unlikely with this data model\n    norms[norms == 0] = 1.0\n    X_normalized = X / norms\n    \n    # Create labels\n    y = np.array([0] * n_per_class + [1] * n_per_class, dtype=int)\n    \n    return X_normalized, y\n\ndef find_optimal_k(p, n_train, n_val, sigma, c, seed, candidate_ks):\n    \"\"\"\n    Finds the optimal k for a given set of simulation parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct class prototypes\n    u1 = np.zeros(p)\n    u1[0] = 1.0\n    \n    u2 = np.zeros(p)\n    u2[0] = c\n    if p > 1:\n      u2[1] = np.sqrt(1 - c**2)\n\n    # 2. Generate training and validation sets\n    X_train, y_train = generate_data(p, n_train, sigma, c, u1, u2, rng)\n    X_val, y_val = generate_data(p, n_val, sigma, c, u1, u2, rng)\n    \n    # 3. Compute cosine similarities\n    # For unit vectors, cosine similarity is the dot product.\n    similarities = X_val @ X_train.T  # Shape: (n_val, n_train)\n    \n    # 4. Get indices of neighbors, sorted by similarity (descending)\n    # We sort by negative similarity to get descending order of similarity.\n    sorted_neighbor_indices = np.argsort(-similarities, axis=1)\n\n    accuracies = []\n    \n    # 5. Evaluate each k\n    for k in candidate_ks:\n        # Get labels of the k-nearest neighbors for each validation point\n        top_k_indices = sorted_neighbor_indices[:, :k]\n        top_k_labels = y_train[top_k_indices] # Shape: (n_val, k)\n        \n        # Predict labels by majority vote\n        # scipy.stats.mode is used for efficient majority voting along axis 1.\n        predicted_labels, _ = mode(top_k_labels, axis=1)\n        predicted_labels = predicted_labels.flatten()\n        \n        # Calculate accuracy\n        accuracy = np.mean(predicted_labels == y_val)\n        accuracies.append(accuracy)\n\n    # 6. Select optimal k based on the specified rule\n    accuracies = np.array(accuracies)\n    max_accuracy = np.max(accuracies)\n    \n    # Find all k values that achieve the max accuracy\n    best_k_indices = np.where(accuracies == max_accuracy)[0]\n    \n    # Tie-breaking: choose the smallest k\n    optimal_k_index = best_k_indices[0]\n    optimal_k = candidate_ks[optimal_k_index]\n    \n    return optimal_k\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}