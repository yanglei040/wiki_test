{
    "hands_on_practices": [
        {
            "introduction": "We begin by exploring the direct relationship between the geometric properties of a linear decision boundary and the algebraic constraints on its parameters. This exercise challenges you to define a linear classifier that must not only separate given data points but also pass through specific \"anchor\" locations in the feature space. By solving for the classifier's parameters under these combined constraints , you will practice the essential skill of translating geometric intuition into a solvable mathematical system, a cornerstone of designing custom machine learning models.",
            "id": "3116612",
            "problem": "In statistical learning, a linear discriminant function is defined as $g(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$ for a feature vector $\\mathbf{x} \\in \\mathbb{R}^{2}$, a weight vector $\\mathbf{w} \\in \\mathbb{R}^{2}$, and an offset $b \\in \\mathbb{R}$. The decision boundary is the set of points satisfying $g(\\mathbf{x}) = 0$. Consider the following constrained optimization problem for a binary linear classifier that must satisfy two kinds of constraints: \n- Anchor constraints requiring the decision boundary to pass through two specified points $\\mathbf{p}_{1} = (0,0)$ and $\\mathbf{p}_{2} = (2,2)$, which impose $\\mathbf{w}^{\\top}\\mathbf{p}_{i} + b = 0$ for $i \\in \\{1,2\\}$.\n- Unit-margin classification constraints on two labeled samples $(\\mathbf{x}_{+}, y_{+})$ and $(\\mathbf{x}_{-}, y_{-})$ with $y_{+} = +1$, $y_{-} = -1$, where $\\mathbf{x}_{+} = (4,3)$ and $\\mathbf{x}_{-} = (2,5)$, which impose $y_{j}(\\mathbf{w}^{\\top}\\mathbf{x}_{j} + b) \\geq 1$ for $j \\in \\{+, -\\}$.\n\nStarting from the definitions above and using only first principles of discriminant functions and feasibility in linear inequality and equality constraints, determine whether the constraint set is feasible (i.e., whether there exist $\\mathbf{w}$ and $b$ satisfying all constraints). If feasible, consider the optimization problem that minimizes the squared Euclidean norm of the weight vector, $\\|\\mathbf{w}\\|^{2}$, subject to the given constraints. Derive the minimal value of $\\|\\mathbf{w}\\|^{2}$ under these constraints. Express your final answer as a single real number. No rounding is required.",
            "solution": "The user has provided a constrained optimization problem in the context of linear classifiers. The task is to first determine the feasibility of the set of constraints and, if feasible, find the minimal value of the squared Euclidean norm of the weight vector, $\\|\\mathbf{w}\\|^{2}$.\n\nThe problem is defined by the following elements:\n- A linear discriminant function $g(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$, where $\\mathbf{x} \\in \\mathbb{R}^{2}$, $\\mathbf{w} \\in \\mathbb{R}^{2}$, and $b \\in \\mathbb{R}$.\n- A decision boundary defined by the set of points where $g(\\mathbf{x}) = 0$.\n- A set of constraints on the parameters $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ and $b$.\n\nFirst, we must validate the problem statement. The problem is a well-defined mathematical task in the field of statistical learning theory. It is scientifically grounded, objective, and provides all necessary information to proceed. It asks to determine feasibility, which is a standard part of analyzing a constrained system. Thus, the problem is valid, and we may proceed with the solution.\n\nThe solution process involves two main parts: analyzing the feasibility of the constraints and then solving the optimization problem.\n\n**Part 1: Analysis of Constraints and Feasibility**\n\nThe constraints are of two types: anchor constraints (equalities) and classification constraints (inequalities).\n\n1.  **Anchor Constraints:** These require the decision boundary to pass through two specified points, $\\mathbf{p}_{1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $\\mathbf{p}_{2} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$. This implies that these points must satisfy the equation of the decision boundary, $\\mathbf{w}^{\\top}\\mathbf{x} + b = 0$.\n\n    For $\\mathbf{p}_{1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n    $$ \\mathbf{w}^{\\top}\\mathbf{p}_{1} + b = w_1(0) + w_2(0) + b = 0 $$\n    This immediately yields the condition $b = 0$.\n\n    For $\\mathbf{p}_{2} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$, and using the result $b=0$:\n    $$ \\mathbf{w}^{\\top}\\mathbf{p}_{2} + 0 = w_1(2) + w_2(2) = 0 $$\n    $$ 2w_1 + 2w_2 = 0 \\implies w_1 + w_2 = 0 \\implies w_2 = -w_1 $$\n    The anchor constraints thus reduce the three free parameters ($w_1$, $w_2$, $b$) to a single degree of freedom. The offset $b$ must be $0$, and the weight vector must be of the form $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ -w_1 \\end{pmatrix}$ for some scalar $w_1 \\in \\mathbb{R}$. The decision boundary is defined by $w_1 x_1 - w_1 x_2 = 0$. If $w_1 \\neq 0$, this simplifies to $x_1-x_2=0$, which is the line passing through the two anchor points, as expected.\n\n2.  **Unit-Margin Classification Constraints:** These are given by the inequality $y_{j}(\\mathbf{w}^{\\top}\\mathbf{x}_{j} + b) \\geq 1$ for two labeled samples. We substitute the results from the anchor constraints ($b=0$ and $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ -w_1 \\end{pmatrix}$) into these inequalities.\n\n    For the positive sample $(\\mathbf{x}_{+}, y_{+})$, where $\\mathbf{x}_{+} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}$ and $y_{+} = +1$:\n    $$ (+1) \\left( \\mathbf{w}^{\\top}\\mathbf{x}_{+} + 0 \\right) \\geq 1 $$\n    $$ \\begin{pmatrix} w_1 & -w_1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix} \\geq 1 $$\n    $$ 4w_1 - 3w_1 \\geq 1 $$\n    $$ w_1 \\geq 1 $$\n\n    For the negative sample $(\\mathbf{x}_{-}, y_{-})$, where $\\mathbf{x}_{-} = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}$ and $y_{-} = -1$:\n    $$ (-1) \\left( \\mathbf{w}^{\\top}\\mathbf{x}_{-} + 0 \\right) \\geq 1 $$\n    $$ (-1) \\left( \\begin{pmatrix} w_1 & -w_1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} \\right) \\geq 1 $$\n    $$ (-1) (2w_1 - 5w_1) \\geq 1 $$\n    $$ (-1) (-3w_1) \\geq 1 $$\n    $$ 3w_1 \\geq 1 \\implies w_1 \\geq \\frac{1}{3} $$\n\n3.  **Feasibility Verdict:**\n    The full set of constraints on the single remaining parameter $w_1$ is:\n    - $w_1 \\geq 1$\n    - $w_1 \\geq \\frac{1}{3}$\n\n    The intersection of these two conditions is the interval $[1, \\infty)$. Since this interval is non-empty (e.g., $w_1 = 2$ is a valid choice), the constraint set is feasible. There exist parameters $\\mathbf{w}$ and $b$ that satisfy all given conditions.\n\n**Part 2: Minimization of $\\|\\mathbf{w}\\|^{2}$**\n\nHaving established feasibility, we now proceed to solve the optimization problem:\nMinimize $\\|\\mathbf{w}\\|^{2}$ subject to the derived constraints.\n\nThe objective function is the squared Euclidean norm of $\\mathbf{w}$:\n$$ \\|\\mathbf{w}\\|^{2} = w_1^2 + w_2^2 $$\nUsing the relation $w_2 = -w_1$ from the anchor constraints, we can express the objective function in terms of $w_1$ alone:\n$$ \\|\\mathbf{w}\\|^{2} = w_1^2 + (-w_1)^2 = 2w_1^2 $$\n\nThe optimization problem is thus reduced to a one-dimensional problem:\nMinimize $f(w_1) = 2w_1^2$\nSubject to $w_1 \\geq 1$.\n\nThe function $f(w_1) = 2w_1^2$ is a parabola with its vertex (global minimum) at $w_1=0$. On the domain $w_1 > 0$, the function is strictly increasing. The feasible region for $w_1$ is the interval $[1, \\infty)$. Since the function is increasing on this entire interval, its minimum value over this domain will occur at the leftmost boundary point, which is $w_1 = 1$.\n\nThe minimal value of the objective function is therefore obtained by evaluating $f(w_1)$ at $w_1=1$:\n$$ \\min(\\|\\mathbf{w}\\|^{2}) = f(1) = 2(1)^2 = 2 $$\n\nThis minimum is achieved for the parameters $w_1=1$, which implies $w_2 = -1$ and $b=0$. The optimal weight vector is $\\mathbf{w}^{*} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. The minimal value of its squared norm is $\\|\\mathbf{w}^{*}\\|^{2} = 1^2 + (-1)^2 = 2$.\nThis solution satisfies all constraints:\n- $w_1 = 1 \\geq 1$ (active constraint).\n- $w_1 = 1 \\geq \\frac{1}{3}$.\nThe minimal value is therefore $2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Linear boundaries are powerful, but many real-world datasets are not so cleanly separable. This practice tackles the challenge of non-linearity head-on by using polynomial discriminant functions to classify data from two intertwined spirals—a classic non-linear problem. You will implement a complete model selection workflow, systematically increasing the polynomial degree to find the simplest model that can successfully learn the complex boundary . This exercise provides a tangible demonstration of managing model complexity and the fundamental trade-off between bias and variance.",
            "id": "3116684",
            "problem": "Consider binary classification in the plane where the decision rule is defined by a discriminant function. A discriminant function $g(\\mathbf{x})$ maps $\\mathbf{x}\\in\\mathbb{R}^2$ to $\\mathbb{R}$ and induces a decision boundary given by $g(\\mathbf{x})=0$. Points with $g(\\mathbf{x})\\ge 0$ are assigned to class $+1$, and points with $g(\\mathbf{x})<0$ are assigned to class $-1$. You will study polynomial discriminants of bounded degree and determine the minimal degree needed to recover a smooth boundary separating two intertwined spirals.\n\nFundamental base for the derivation:\n- A discriminant function $g(\\mathbf{x})$ defines a decision boundary via the zero level set $g(\\mathbf{x})=0$.\n- Empirical risk minimization with squared loss fits the discriminant by minimizing $\\sum_{i=1}^{n} \\left(g(\\mathbf{x}_i)-y_i\\right)^2$, where $y_i\\in\\{-1,+1\\}$ are labels.\n- Polynomial basis functions yield a finite-dimensional linear model. Let $\\alpha=(\\alpha_1,\\alpha_2)$ be a multi-index with nonnegative integers and $|\\alpha|=\\alpha_1+\\alpha_2$. Define the monomial $\\mathbf{x}^\\alpha=x_1^{\\alpha_1}x_2^{\\alpha_2}$. A polynomial discriminant of degree at most $m$ is $g_m(\\mathbf{x})=\\sum_{|\\alpha|\\le m} c_\\alpha \\mathbf{x}^\\alpha$, where $c_\\alpha\\in\\mathbb{R}$ are coefficients.\n- Regularized least squares with a small ridge parameter stabilizes the fit: minimize $\\sum_{i=1}^{n} \\left(g_m(\\mathbf{x}_i)-y_i\\right)^2 + \\lambda \\sum_{|\\alpha|\\le m} c_\\alpha^2$ for a small $\\lambda>0$.\n\nData generation:\n- Two intertwined Archimedean spirals are parameterized in polar coordinates by $r=a\\,\\theta$ with angle $\\theta$ in radians. For class $+1$, sample $\\theta$ from a uniform distribution on $[0, 2\\pi T]$ and set $(x,y)=(r\\cos\\theta,r\\sin\\theta)$ with $r=a\\,\\theta$. For class $-1$, use angles shifted by $\\pi$, that is, $\\theta'=\\theta+\\pi$ and $(x',y')=(r'\\cos\\theta',r'\\sin\\theta')$ with $r'=a\\,\\theta$. Add independent Gaussian noise with standard deviation $\\sigma$ to both coordinates. Normalize features by dividing both $x$ and $y$ by the maximum radius $r_{\\max}=a\\cdot 2\\pi T$ so that inputs lie in a comparable range. Angles must be in radians.\n\nStudy design:\n- For each dataset, split the samples into a training set and a validation set using a fixed fraction $f_{\\text{train}}=0.7$ for training and $f_{\\text{val}}=0.3$ for validation. Use a fixed random seed per dataset to ensure reproducibility.\n- For each candidate polynomial degree $m\\in\\{1,2,\\dots,M_{\\max}\\}$, fit $g_m(\\mathbf{x})$ by regularized least squares using the training set with ridge parameter $\\lambda=10^{-6}$, then compute validation accuracy as the fraction of validation points whose predicted label $\\operatorname{sign}(g_m(\\mathbf{x}))$ matches the true label. When $g_m(\\mathbf{x})=0$ for a validation point, treat the prediction as $+1$.\n- Define the minimal degree for a dataset as the smallest $m$ in the candidate set for which the validation accuracy is at least the specified threshold $\\tau$. If no degree achieves the threshold, return $-1$.\n\nTest suite:\n- Use the following parameter sets, where each tuple is $(N,T,a,\\sigma,\\tau,M_{\\max},\\text{seed})$:\n    - Case $1$ (happy path): $(600,3,0.5,0.05,0.85,10,0)$.\n    - Case $2$ (low noise, fewer turns): $(600,2,0.7,0.0,0.90,10,1)$.\n    - Case $3$ (more turns, moderate noise): $(600,5,0.3,0.12,0.80,12,2)$.\n    - Case $4$ (edge case, high noise, fewer samples): $(300,4,0.6,0.25,0.75,12,3)$.\n\nRequired output:\n- Your program should produce a single line of output containing the minimal degrees for the four test cases in order, as a comma-separated list enclosed in square brackets (for example, $[m_1,m_2,m_3,m_4]$). Each $m_i$ must be an integer, where $-1$ indicates that no polynomial degree in the candidate set achieved the threshold for that case.",
            "solution": "The user wants to determine the minimal polynomial degree required for a discriminant function to accurately classify data points belonging to two intertwined Archimedean spirals. This is a model selection problem within the context of statistical learning. The solution involves a systematic computational experiment for a given set of parameters.\n\n### 1. Problem Formalization and Data Generation\n\nThe task is a binary classification problem in $\\mathbb{R}^2$. The two classes are labeled as $y=+1$ and $y=-1$. The data points are generated based on two intertwined Archimedean spirals.\n\nAn Archimedean spiral is described in polar coordinates $(r, \\phi)$ by the equation $r = a\\phi$. For a given set of parameters $(N, T, a, \\sigma)$, we generate $N/2$ points for each class.\n\n- **Class $+1$**: We sample $N/2$ angles $\\theta_i$ from a uniform distribution on the interval $[0, 2\\pi T]$. Each point in Cartesian coordinates is given by:\n$$\n\\mathbf{x}_{i,+} = \\begin{pmatrix} r_i\\cos\\theta_i \\\\ r_i\\sin\\theta_i \\end{pmatrix} + \\boldsymbol{\\epsilon}_i, \\quad \\text{where } r_i = a\\theta_i\n$$\nThe term $\\boldsymbol{\\epsilon}_i$ represents independent Gaussian noise added to each coordinate, drawn from a normal distribution with mean $0$ and standard deviation $\\sigma$, i.e., $\\boldsymbol{\\epsilon}_i \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$, where $I$ is the $2 \\times 2$ identity matrix.\n\n- **Class $-1$**: For each angle $\\theta_i$ used for class $+1$, the corresponding point for class $-1$ is generated using a shifted angle $\\theta'_i = \\theta_i + \\pi$ and a radius $r'_i = a\\theta_i$. Note that the radius is a function of the original angle $\\theta_i$. The Cartesian coordinates are:\n$$\n\\mathbf{x}_{i,-} = \\begin{pmatrix} r'_i\\cos\\theta'_i \\\\ r'_i\\sin\\theta'_i \\end{pmatrix} + \\boldsymbol{\\epsilon}'_i = \\begin{pmatrix} a\\theta_i\\cos(\\theta_i + \\pi) \\\\ a\\theta_i\\sin(\\theta_i + \\pi) \\end{pmatrix} + \\boldsymbol{\\epsilon}'_i = -\\begin{pmatrix} a\\theta_i\\cos\\theta_i \\\\ a\\theta_i\\sin\\theta_i \\end{pmatrix} + \\boldsymbol{\\epsilon}'_i\n$$\nThis construction implies that, before adding noise, the points for class $-1$ are perfectly point-symmetric to the points for class $+1$ with respect to the origin.\n\n- **Normalization**: After generating all $N$ points, their coordinates are normalized by dividing by the maximum possible radius, $r_{\\max} = a \\cdot 2\\pi T$, to ensure the features lie within a consistent range.\n\n### 2. Polynomial Discriminant Function\n\nThe classification is performed using a polynomial discriminant function $g_m(\\mathbf{x})$ of degree at most $m$. For a point $\\mathbf{x} = (x_1, x_2)^T \\in \\mathbb{R}^2$, the discriminant function is a linear combination of monomials:\n$$\ng_m(\\mathbf{x}) = \\sum_{|\\alpha| \\le m} c_\\alpha \\mathbf{x}^\\alpha = \\sum_{\\alpha_1+\\alpha_2 \\le m} c_{(\\alpha_1,\\alpha_2)} x_1^{\\alpha_1} x_2^{\\alpha_2}\n$$\nwhere $c_\\alpha$ are the real-valued coefficients. This can be expressed in vector form as a linear model, $g_m(\\mathbf{x}) = \\boldsymbol{\\phi}_m(\\mathbf{x})^T \\mathbf{c}$, where $\\boldsymbol{\\phi}_m(\\mathbf{x})$ is a vector of monomial basis functions (e.g., for $m=2$, $\\boldsymbol{\\phi}_2(\\mathbf{x}) = (1, x_1, x_2, x_1^2, x_1x_2, x_2^2)^T$) and $\\mathbf{c}$ is the vector of corresponding coefficients $c_\\alpha$.\n\nThe decision boundary is the set of points where $g_m(\\mathbf{x})=0$. The decision rule assigns a class label based on the sign of the discriminant:\n$$\n\\text{class}(\\mathbf{x}) = \\begin{cases} +1 & \\text{if } g_m(\\mathbf{x}) \\ge 0 \\\\ -1 & \\text{if } g_m(\\mathbf{x}) < 0 \\end{cases}\n$$\n\n### 3. Model Training via Regularized Least Squares\n\nThe coefficient vector $\\mathbf{c}$ is determined by fitting the model to a training set $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$ using regularized least squares (ridge regression). This method minimizes the sum of squared errors between the discriminant output and the true labels, plus a penalty term on the magnitude of the coefficients:\n$$\n\\min_{\\mathbf{c}} \\sum_{i=1}^{n_{\\text{train}}} (g_m(\\mathbf{x}_i) - y_i)^2 + \\lambda \\sum_{|\\alpha| \\le m} c_\\alpha^2\n$$\nwhere $\\lambda = 10^{-6}$ is the regularization parameter. In matrix form, this is:\n$$\n\\min_{\\mathbf{c}} \\|\\mathbf{\\Phi}\\mathbf{c} - \\mathbf{y}\\|^2_2 + \\lambda \\|\\mathbf{c}\\|^2_2\n$$\nwhere $\\mathbf{\\Phi}$ is the design matrix with rows $\\boldsymbol{\\phi}_m(\\mathbf{x}_i)^T$, and $\\mathbf{y}$ is the vector of labels. The analytical solution for the optimal coefficients $\\hat{\\mathbf{c}}$ is given by the normal equations:\n$$\n\\hat{\\mathbf{c}} = (\\mathbf{\\Phi}^T\\mathbf{\\Phi} + \\lambda I)^{-1}\\mathbf{\\Phi}^T\\mathbf{y}\n$$\nwhere $I$ is the identity matrix of appropriate dimension.\n\n### 4. Model Selection and Evaluation\n\nThe core task is to find the minimal polynomial degree $m^*$ that achieves a desired classification performance. The procedure is as follows:\n1.  For each test case, generate the data and split it into a training set ($f_{\\text{train}}=0.7$ of samples) and a validation set ($f_{\\text{val}}=0.3$). The split is performed after shuffling the dataset with a fixed random seed to ensure reproducibility.\n2.  For each candidate degree $m$ from $1$ to $M_{\\max}$:\n    a. Construct the polynomial feature matrix $\\mathbf{\\Phi}_{\\text{train}}$ for the training data.\n    b. Train the model by computing the coefficient vector $\\hat{\\mathbf{c}}$ using the ridge regression formula.\n    c. Evaluate the trained model on the validation set. For each validation point $\\mathbf{x}_j$, calculate the predicted label $\\hat{y}_j = \\operatorname{sign}(g_m(\\mathbf{x}_j))$.\n    d. Compute the validation accuracy as the fraction of correctly classified points: $\\text{Accuracy} = \\frac{1}{n_{\\text{val}}} \\sum_{j=1}^{n_{\\text{val}}} \\mathbb{I}(\\hat{y}_j = y_j)$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n3.  The minimal required degree $m^*$ is the smallest $m$ in the search range for which the validation accuracy is greater than or equal to a specified threshold $\\tau$.\n4.  If no degree $m \\in \\{1, \\dots, M_{\\max}\\}$ meets the accuracy threshold $\\tau$, the result is reported as $-1$.\n\nThis procedure is executed for each parameter set provided in the test suite to determine the corresponding minimal degree.",
            "answer": "```python\nimport numpy as np\n\ndef generate_polynomial_features(X, degree):\n    \"\"\"\n    Generates a design matrix for polynomial features up to a given degree.\n\n    Args:\n        X (np.ndarray): Input data of shape (n_samples, 2).\n        degree (int): The maximum degree of the polynomial.\n\n    Returns:\n        np.ndarray: The design matrix of shape (n_samples, n_features).\n    \"\"\"\n    n_samples = X.shape[0]\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n\n    num_features = (degree + 1) * (degree + 2) // 2\n    phi = np.empty((n_samples, num_features))\n\n    feature_idx = 0\n    for m in range(degree + 1):\n        for p1 in range(m + 1):\n            p2 = m - p1\n            phi[:, feature_idx] = (x1**p1) * (x2**p2)\n            feature_idx += 1\n    return phi\n\ndef run_case(N, T, a, sigma, tau, M_max, seed):\n    \"\"\"\n    Runs a single test case to find the minimal polynomial degree.\n\n    Args:\n        N (int): Total number of samples.\n        T (float): Number of turns for the spiral.\n        a (float): Spiral constant (r = a*theta).\n        sigma (float): Standard deviation of Gaussian noise.\n        tau (float): Accuracy threshold.\n        M_max (int): Maximum polynomial degree to test.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        int: The minimal polynomial degree, or -1 if threshold is not met.\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    n_per_class = N // 2\n    \n    # Generate angles for class +1\n    theta = rng.uniform(0, 2 * np.pi * T, size=n_per_class)\n    r = a * theta\n    \n    # Class +1 points\n    x_plus = r * np.cos(theta)\n    y_plus = r * np.sin(theta)\n    \n    # Class -1 points (point-symmetric to class +1)\n    x_minus = -x_plus\n    y_minus = -y_plus\n    \n    X_plus = np.stack((x_plus, y_plus), axis=1)\n    X_minus = np.stack((x_minus, y_minus), axis=1)\n    \n    # Add noise\n    X_plus += rng.normal(0, sigma, size=X_plus.shape)\n    X_minus += rng.normal(0, sigma, size=X_minus.shape)\n    \n    X = np.vstack((X_plus, X_minus))\n    y = np.hstack((np.ones(n_per_class), -np.ones(n_per_class)))\n    \n    # Normalization\n    r_max = a * 2 * np.pi * T\n    if r_max > 0:\n        X /= r_max\n        \n    # 2. Data Splitting (70% train, 30% validation)\n    indices = np.arange(N)\n    rng.shuffle(indices)\n    \n    train_size = int(0.7 * N)\n    train_idx = indices[:train_size]\n    val_idx = indices[train_size:]\n    \n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n\n    lambda_reg = 1e-6\n    \n    # 3. Model Selection Loop\n    for m in range(1, M_max + 1):\n        # Feature Engineering\n        phi_train = generate_polynomial_features(X_train, m)\n        phi_val = generate_polynomial_features(X_val, m)\n\n        # Training (Ridge Regression)\n        d = phi_train.shape[1]\n        I = np.eye(d)\n        \n        try:\n            # Solve normal equations: (Phi^T * Phi + lambda * I) * c = Phi^T * y\n            A = phi_train.T @ phi_train + lambda_reg * I\n            b = phi_train.T @ y_train\n            coeffs = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Failsafe for singular matrix, though unlikely with ridge\n            continue \n\n        # Validation\n        g_val = phi_val @ coeffs\n        \n        # Predictions: sign(g), with sign(0) -> +1\n        # (g_val >= 0) -> bool, * 2 - 1 maps True to 1 and False to -1\n        y_pred = (g_val >= 0) * 2 - 1\n        \n        accuracy = np.mean(y_pred == y_val)\n        \n        if accuracy >= tau:\n            return m\n            \n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to execute all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (600, 3, 0.5, 0.05, 0.85, 10, 0),\n        (600, 2, 0.7, 0.0, 0.90, 10, 1),\n        (600, 5, 0.3, 0.12, 0.80, 12, 2),\n        (300, 4, 0.6, 0.25, 0.75, 12, 3),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_case(*params)\n        results.append(result)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Preprocessing data is a critical step, but the wrong choice can be detrimental to classifier performance. This final practice explores a crucial, and often misunderstood, distinction between supervised and unsupervised dimensionality reduction through a carefully designed thought experiment. By analyzing a hypothetical dataset where the class-separating information lies in a direction of low variance, you will see why a method like Principal Component Analysis (PCA) can fail, while a supervised approach like Fisher's Linear Discriminant Analysis (LDA) succeeds . This deep dive will sharpen your critical thinking about the alignment between a tool's objective and your ultimate modeling goal.",
            "id": "3116599",
            "problem": "You are asked to formalize, analyze, and programmatically evaluate how unsupervised variance-based preprocessing can obscure class separation when the discriminant lies in low-variance directions. Work strictly from fundamental definitions in statistical learning.\n\nWe consider a binary classification problem in $d$-dimensional Euclidean space with two classes. Conditioned on class label $y \\in \\{1,2\\}$, the feature vector $x \\in \\mathbb{R}^d$ is distributed as a multivariate normal with equal prior probabilities and shared covariance. Specifically, assume $x \\mid y=i \\sim \\mathcal{N}(\\mu_i, \\Sigma)$ with equal class priors, where $\\mu_1, \\mu_2 \\in \\mathbb{R}^d$ and $\\Sigma \\in \\mathbb{R}^{d \\times d}$ is positive definite. The principal component analysis (PCA) preprocessing is an unsupervised linear projection that selects directions that maximize variance. Fisher’s Linear Discriminant Analysis (LDA) is a supervised linear projection that maximizes the ratio of between-class variance to within-class variance. Whitening (also called sphering) is a linear transformation that maps the pooled covariance to the identity matrix.\n\nTasks:\n1) Starting only from the definitions of the Bayes decision rule, the properties of affine transformations of Gaussian random variables, and the definition of the principal component analysis and the Fisher linear discriminant, derive a closed-form expression for the Bayes-optimal misclassification probability for this two-class, equal-covariance setting after an arbitrary linear projection. Your derivation must express the error in terms of the squared Mahalanobis distance between the projected class means with respect to the projected within-class covariance.\n2) Use the result of Task 1 to define four pipelines:\n   a) No dimensionality reduction: compute the Bayes-optimal misclassification probability in the original space.\n   b) Principal Component Analysis (PCA): compute the Bayes-optimal misclassification probability after projection onto the top $k$ principal axes of the pooled covariance.\n   c) Fisher’s Linear Discriminant Analysis (LDA): compute the Bayes-optimal misclassification probability after projection onto the one-dimensional Fisher direction.\n   d) Whitening followed by one-dimensional PCA: whiten the data using the pooled covariance, then project onto the top principal axis of the whitened, unlabeled mixture; compute the Bayes-optimal misclassification probability in this one-dimensional space.\n3) Construct and analyze a dataset that exhibits class separation only in low-variance directions. Concretely, use two-dimensional cases with diagonal covariance matrices and mean differences aligned with either the low-variance or the high-variance axis. Implement the four pipelines above and compute the corresponding misclassification probabilities.\n\nUse the following test suite. Each test case is a tuple $(\\mu_1, \\mu_2, \\Sigma, k)$, where $k$ is the number of retained principal components for PCA:\n- Test $1$: $\\mu_1 = [0, 2]^T$, $\\mu_2 = [0, -2]^T$, $\\Sigma = \\mathrm{diag}([100, 1])$, $k = 1$. This encodes separation only along the low-variance axis.\n- Test $2$: $\\mu_1 = [0, 0]^T$, $\\mu_2 = [0, 0]^T$, $\\Sigma = \\mathrm{diag}([100, 1])$, $k = 1$. This encodes no separation.\n- Test $3$: $\\mu_1 = [0, 2]^T$, $\\mu_2 = [0, -2]^T$, $\\Sigma = \\mathrm{diag}([100, 1])$, $k = 2$. This retains all principal components.\n- Test $4$: $\\mu_1 = [20, 0]^T$, $\\mu_2 = [-20, 0]^T$, $\\Sigma = \\mathrm{diag}([100, 1])$, $k = 1$. This encodes separation along the high-variance axis with suitably large mean difference.\n\nYour program must:\n- Implement the derivation from Task $1$ to a function that, given $(\\mu_1, \\mu_2, \\Sigma)$ and a linear projection $P \\in \\mathbb{R}^{d \\times k}$ (with $k \\ge 1$), returns the Bayes-optimal misclassification probability in the projected space.\n- For each test case, compute four floating-point results in this order: \n  $[$no-reduction error, PCA($k$)-reduction error, LDA(1D)-reduction error, whitening+PCA(1D)-reduction error$]$.\n- Aggregate the results across the four test cases into a single flat list of $16$ floating-point numbers.\n- Print a single line containing a comma-separated list enclosed in square brackets, with each number formatted to exactly $10$ digits after the decimal point, and no spaces. For example, output like $[a_1,a_2,\\dots,a_{16}]$ where each $a_i$ is a float with exactly $10$ digits after the decimal point.\n\nThere are no physical units or angles involved in this problem. All outputs must be floating-point numbers as specified. Your implementation must be self-contained and must not require any user input or external files.",
            "solution": "This problem requires us to derive the theoretical misclassification error for a projected Gaussian dataset and then apply this formula to compare four different linear transformation pipelines. The core of the analysis lies in understanding how the separability of two classes is affected by different choices of projection, particularly the contrast between an unsupervised method (PCA) and a supervised one (LDA).\n\nFirst, we derive a general expression for the Bayes-optimal misclassification probability. We are given a binary classification problem where the class-conditional densities for $x \\in \\mathbb{R}^d$ are multivariate normal with a shared covariance matrix:\n$$\nx \\mid y=i \\sim \\mathcal{N}(\\mu_i, \\Sigma) \\quad \\text{for } i \\in \\{1, 2\\}\n$$\nThe class prior probabilities are equal, $P(y=1) = P(y=2) = 1/2$. A linear projection is applied to the data, mapping $x \\in \\mathbb{R}^d$ to $z \\in \\mathbb{R}^k$ (where $k \\le d$) via a projection matrix $P \\in \\mathbb{R}^{d \\times k}$:\n$$\nz = P^T x\n$$\nUsing the property of affine transformations of Gaussian random variables, the distribution of the projected vector $z$ conditioned on the class is also Gaussian:\n$$\nz \\mid y=i \\sim \\mathcal{N}(P^T\\mu_i, P^T\\Sigma P)\n$$\nLet us denote the mean and covariance in the projected space as $\\mu_{z,i} = P^T\\mu_i$ and $\\Sigma_z = P^T\\Sigma P$. The problem now is to find the Bayes-optimal error for classifying $z$ between two Gaussian distributions, $\\mathcal{N}(\\mu_{z,1}, \\Sigma_z)$ and $\\mathcal{N}(\\mu_{z,2}, \\Sigma_z)$, with equal priors.\n\nThe Bayes decision rule assigns a point $z$ to the class with the maximum posterior probability, $P(y=i \\mid z)$. Since priors are equal, this is equivalent to assigning $z$ to the class with the maximum likelihood, $p(z \\mid y=i)$. For Gaussian distributions with a shared covariance, this defines a linear discriminant. The optimal decision boundary is a hyperplane.\n\nTo calculate the error, we can project the data onto the most discriminative one-dimensional line in the $k$-dimensional projected space. This direction is given by Fisher's linear discriminant in this space, $w_z = \\Sigma_z^{-1}(\\mu_{z,1} - \\mu_{z,2})$. Let's define a scalar decision variable $v = w_z^T z$. The distribution of $v$ conditioned on class $i$ is univariate normal:\n$$\nv \\mid y=i \\sim \\mathcal{N}(w_z^T\\mu_{z,i}, w_z^T\\Sigma_z w_z)\n$$\nLet's analyze the mean and variance of this scalar projection.\nThe means are $m_1 = w_z^T\\mu_{z,1}$ and $m_2 = w_z^T\\mu_{z,2}$.\nThe variance (which is common for both classes) is $\\sigma_v^2 = w_z^T\\Sigma_z w_z$. Substituting the expression for $w_z$:\n$$\n\\sigma_v^2 = (\\mu_{z,1} - \\mu_{z,2})^T \\Sigma_z^{-1} \\Sigma_z \\Sigma_z^{-1} (\\mu_{z,1} - \\mu_{z,2}) = (\\mu_{z,1} - \\mu_{z,2})^T \\Sigma_z^{-1} (\\mu_{z,1} - \\mu_{z,2})\n$$\nThis quantity is the squared Mahalanobis distance between the means in the projected space, which we denote as $\\Delta_z^2$.\nThe difference between the means of $v$ is:\n$$\nm_1 - m_2 = w_z^T(\\mu_{z,1} - \\mu_{z,2}) = (\\mu_{z,1} - \\mu_{z,2})^T \\Sigma_z^{-1} (\\mu_{z,1} - \\mu_{z,2}) = \\Delta_z^2\n$$\nSo, we have two univariate normal distributions, $\\mathcal{N}(m_1, \\Delta_z^2)$ and $\\mathcal{N}(m_2, \\Delta_z^2)$, where $m_1-m_2 = \\Delta_z^2$. The optimal decision threshold is halfway between the means, at $t = (m_1+m_2)/2$.\n\nThe probability of misclassification, assuming the true class is $2$, is the probability that $v > t$:\n$$\nP(\\text{error} \\mid y=2) = P(v > \\frac{m_1+m_2}{2} \\mid y=2)\n$$\nStandardizing the variable $v \\sim \\mathcal{N}(m_2, \\Delta_z^2)$, we get $Z = (v-m_2)/\\Delta_z \\sim \\mathcal{N}(0, 1)$.\n$$\nP(\\text{error} \\mid y=2) = P(Z > \\frac{(m_1+m_2)/2 - m_2}{\\Delta_z}) = P(Z > \\frac{m_1-m_2}{2\\Delta_z}) = P(Z > \\frac{\\Delta_z^2}{2\\Delta_z}) = P(Z > \\frac{\\Delta_z}{2})\n$$\nUsing the symmetry of the standard normal distribution, this is $\\Phi(-\\Delta_z/2)$, where $\\Phi$ is the standard normal cumulative distribution function (CDF). By symmetry of the problem, $P(\\text{error} \\mid y=1)$ is also $\\Phi(-\\Delta_z/2)$.\nThe total Bayes error rate is the average of the conditional errors, weighted by the priors:\n$$\nP(\\text{error}) = P(\\text{error} \\mid y=1)P(y=1) + P(\\text{error} \\mid y=2)P(y=2) = \\frac{1}{2}\\Phi(-\\frac{\\Delta_z}{2}) + \\frac{1}{2}\\Phi(-\\frac{\\Delta_z}{2}) = \\Phi(-\\frac{\\Delta_z}{2})\n$$\nThe final expression for the misclassification probability is $P_e = \\Phi(-\\frac{1}{2}\\sqrt{\\Delta_z^2})$, where $\\Delta_z^2 = (\\mu_{z,1}-\\mu_{z,2})^T\\Sigma_z^{-1}(\\mu_{z,1}-\\mu_{z,2})$.\n\nNext, we define the projection matrix $P$ for the four pipelines.\n\na) No dimensionality reduction: This corresponds to an identity projection in $\\mathbb{R}^d$. The projection matrix is $P = I_d$, the $d \\times d$ identity matrix. The projected-space parameters are simply the original parameters, so the squared Mahalanobis distance is $\\Delta^2 = (\\mu_1-\\mu_2)^T\\Sigma^{-1}(\\mu_1-\\mu_2)$, and the error is $\\Phi(-\\Delta/2)$.\n\nb) Principal Component Analysis (PCA): PCA is an unsupervised technique that finds directions of maximum variance in the data. For this, we must consider the mixture distribution $p(x) = \\frac{1}{2}\\mathcal{N}(\\mu_1, \\Sigma) + \\frac{1}{2}\\mathcal{N}(\\mu_2, \\Sigma)$. The covariance matrix of this mixture, $\\Sigma_{PCA}$, is the sum of the within-class covariance and the between-class covariance: $\\Sigma_{PCA} = \\Sigma + \\frac{1}{4}(\\mu_1-\\mu_2)(\\mu_1-\\mu_2)^T$. The PCA projection matrix $P_{PCA} \\in \\mathbb{R}^{d \\times k}$ is constructed from the $k$ eigenvectors of $\\Sigma_{PCA}$ corresponding to the $k$ largest eigenvalues.\n\nc) Fisher’s Linear Discriminant Analysis (LDA): LDA is a supervised method that explicitly seeks a projection to maximize class separability. For two classes with common covariance $\\Sigma$, the optimal $1$-dimensional projection direction is $w_{LDA} \\propto \\Sigma^{-1}(\\mu_1-\\mu_2)$. The projection matrix is $P_{LDA} \\in \\mathbb{R}^{d \\times 1}$, with its single column being this vector $w_{LDA}$. For this projection, the Mahalanobis distance in the projected space is equal to that in the original space, $\\Delta_z^2 = \\Delta^2$. Thus, LDA preserves all the class separability and achieves the minimum possible error for a $1$D projection, which is the same as the Bayes error in the full space.\n\nd) Whitening followed by one-dimensional PCA: This is a two-step process. First, the data is whitened (sphered) using the within-class covariance $\\Sigma$. The transformation is $\\tilde{x} = \\Sigma^{-1/2}x$, which results in whitened class-conditional distributions $\\tilde{x}|y=i \\sim \\mathcal{N}(\\Sigma^{-1/2}\\mu_i, I)$. Second, PCA is performed on the mixture of this whitened data. The covariance of the whitened mixture is $I + \\frac{1}{4}(\\tilde{\\mu}_1-\\tilde{\\mu}_2)(\\tilde{\\mu}_1-\\tilde{\\mu}_2)^T$, where $\\tilde{\\mu}_i=\\Sigma^{-1/2}\\mu_i$. The principal eigenvector of this matrix is proportional to $\\tilde{\\mu}_1-\\tilde{\\mu}_2 = \\Sigma^{-1/2}(\\mu_1-\\mu_2)$. The combined transformation is a projection onto the direction given by $\\Sigma^{-1/2}(\\Sigma^{-1/2}(\\mu_1-\\mu_2)) = \\Sigma^{-1}(\\mu_1-\\mu_2)$. This is precisely the same direction as LDA. Therefore, this pipeline is theoretically equivalent to LDA for this problem and will yield the same error rate.\n\nThe provided test cases are designed to illustrate the difference between these methods. Test $1$ is critical: class separation exists only along the axis of low variance. PCA, being unsupervised, will select the high-variance direction, which contains no class information, leading to an error rate of $0.5$. In contrast, LDA, being supervised, will find the low-variance, high-separation direction and achieve a low error rate. Test $4$ shows the opposite scenario where the direction of high variance aligns with class separation, and PCA performs optimally.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing misclassification probabilities for four pipelines\n    across four test cases as specified.\n    \"\"\"\n\n    def compute_bayes_error(mu1, mu2, Sigma, P):\n        \"\"\"\n        Computes the Bayes-optimal misclassification probability for projected data.\n\n        Args:\n            mu1 (np.ndarray): Mean of class 1.\n            mu2 (np.ndarray): Mean of class 2.\n            Sigma (np.ndarray): Shared covariance matrix.\n            P (np.ndarray): Projection matrix.\n\n        Returns:\n            float: The Bayes-optimal misclassification probability.\n        \"\"\"\n        # Project means\n        mu_z1 = P.T @ mu1\n        mu_z2 = P.T @ mu2\n\n        mu_z_diff = mu_z1 - mu_z2\n        \n        # If projected means are identical, classes are inseparable, error is 0.5\n        if np.allclose(mu_z_diff, 0):\n            return 0.5\n\n        # Project covariance\n        Sigma_z = P.T @ Sigma @ P\n\n        # If projected space is 1D, calculation is on scalars\n        if Sigma_z.ndim == 0:\n            sq_mahalanobis = mu_z_diff**2 / Sigma_z\n        else:\n            # For >1D, solve linear system for stability instead of inverting\n            try:\n                # v = inv(Sigma_z) * mu_z_diff\n                v = np.linalg.solve(Sigma_z, mu_z_diff)\n                sq_mahalanobis = mu_z_diff.T @ v\n            except np.linalg.LinAlgError:\n                # Should not happen with PD Sigma and valid P, but as a safeguard\n                return 0.5\n\n        # Mahalanobis distance must be non-negative\n        if sq_mahalanobis  0:\n            sq_mahalanobis = 0\n\n        delta = np.sqrt(sq_mahalanobis)\n        \n        # Error formula is Phi(-delta/2)\n        error = norm.cdf(-delta / 2.0)\n        return error\n\n    test_cases = [\n        (np.array([0, 2]), np.array([0, -2]), np.diag([100, 1]), 1),\n        (np.array([0, 0]), np.array([0, 0]), np.diag([100, 1]), 1),\n        (np.array([0, 2]), np.array([0, -2]), np.diag([100, 1]), 2),\n        (np.array([20, 0]), np.array([-20, 0]), np.diag([100, 1]), 1)\n    ]\n\n    results = []\n    for mu1, mu2, Sigma, k in test_cases:\n        d = mu1.shape[0]\n        mu_diff = mu1 - mu2\n\n        # 1. No dimensionality reduction\n        P_no_red = np.identity(d)\n        error_no_red = compute_bayes_error(mu1, mu2, Sigma, P_no_red)\n        results.append(error_no_red)\n\n        # 2. Principal Component Analysis (PCA)\n        # Covariance of the mixture distribution\n        Sigma_pca = Sigma + 0.25 * np.outer(mu_diff, mu_diff)\n        eigvals, eigvecs = np.linalg.eigh(Sigma_pca)\n        # Sort eigenvectors by descending eigenvalues\n        sorted_indices = np.argsort(eigvals)[::-1]\n        P_pca = eigvecs[:, sorted_indices[:k]]\n        error_pca = compute_bayes_error(mu1, mu2, Sigma, P_pca)\n        results.append(error_pca)\n\n        # 3. Fisher's Linear Discriminant Analysis (LDA)\n        # Handle case with no class separation\n        if np.allclose(mu_diff, 0):\n            # Projection direction is undefined (zero vector), leading to no separation\n            P_lda = np.zeros((d, 1))\n        else:\n            w_lda = np.linalg.solve(Sigma, mu_diff)\n            P_lda = w_lda.reshape(-1, 1)\n        error_lda = compute_bayes_error(mu1, mu2, Sigma, P_lda)\n        results.append(error_lda)\n\n        # 4. Whitening + PCA\n        # As derived, this is equivalent to LDA for this problem setting.\n        error_white_pca = error_lda\n        results.append(error_white_pca)\n\n    # Format the final output as specified\n    formatted_results = [f\"{res:.10f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}