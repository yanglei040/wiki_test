{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering any statistical tool is to understand its fundamental definition. This exercise challenges you to apply the Bayesian Information Criterion (BIC) formula directly to a negative binomial regression model, a common choice for count data. By working through this problem, you will practice the crucial skill of correctly identifying the total number of estimated parameters ($k$) in a given model, which is a key component of the BIC calculation .",
            "id": "806248",
            "problem": "**Background**\n\nA random variable $Y$ follows a Negative Binomial distribution with mean $\\mu > 0$ and dispersion parameter $\\theta > 0$, denoted by $Y \\sim \\text{NB}(\\mu, \\theta)$, if its probability mass function is given by:\n$$ P(Y=y) = \\frac{\\Gamma(y+\\theta)}{\\Gamma(y+1)\\Gamma(\\theta)} \\left(\\frac{\\theta}{\\theta+\\mu}\\right)^\\theta \\left(\\frac{\\mu}{\\theta+\\mu}\\right)^y, \\quad \\text{for } y \\in \\{0, 1, 2, \\dots\\}. $$\nThe mean and variance are $E[Y] = \\mu$ and $\\text{Var}(Y) = \\mu + \\mu^2/\\theta$.\n\nIn a Negative Binomial regression model, the conditional mean $\\mu_i$ of a response variable $Y_i$ is modeled as a function of a vector of predictor variables $\\mathbf{x}_i$. A common specification uses a log-link function:\n$$ \\log(\\mu_i) = \\mathbf{x}_i^T \\boldsymbol{\\beta}, $$\nwhere $\\boldsymbol{\\beta}$ is a vector of regression coefficients. The parameters of this model that are estimated from the data are the regression coefficients $\\boldsymbol{\\beta}$ and the common dispersion parameter $\\theta$.\n\nThe Bayesian Information Criterion (BIC) is a widely used criterion for model selection, defined as:\n$$ \\text{BIC} = k \\ln(n) - 2 \\hat{\\ell}, $$\nwhere $n$ is the sample size, $k$ is the total number of estimated parameters in the model, and $\\hat{\\ell}$ is the maximized value of the model's log-likelihood function.\n\n**Problem Statement**\n\nA biostatistician is modeling the number of lesions on plant leaves using a Negative Binomial regression model. The analysis is based on a sample of $n$ leaves. The regression model includes $p$ distinct predictor variables (such as plant age, soil pH, and sun exposure) along with an intercept term.\n\nAfter performing the maximum likelihood estimation, the maximized value of the log-likelihood function for the model is determined to be:\n$$ \\hat{\\ell} = Cnp - D n \\log(n) $$\nwhere $C$ and $D$ are given positive constants.\n\nYour task is to derive an expression for the Bayesian Information Criterion (BIC) for this Negative Binomial regression model in terms of $n$, $p$, $C$, and $D$.",
            "solution": "1. Number of estimated parameters:\n$$\nk = p + 1\\ (\\text{coefficients including intercept}) + 1\\ (\\theta)\n= p + 2.\n$$\n2. BIC definition:\n$$\n\\mathrm{BIC} = k\\ln(n) \\;-\\; 2\\,\\hat \\ell.\n$$\n3. Substitute $k$ and $\\hat \\ell$:\n$$\n\\mathrm{BIC} \n= (p+2)\\ln(n) \\;-\\; 2\\bigl(Cnp - Dn\\ln(n)\\bigr).\n$$\n4. Expand and simplify:\n$$\n\\mathrm{BIC}\n= (p+2)\\ln(n) - 2Cnp + 2D\\,n\\ln(n).\n$$",
            "answer": "$$\\boxed{(p + 2)\\ln(n) + 2D\\,n\\ln(n) - 2C\\,n\\,p}$$"
        },
        {
            "introduction": "The BIC is one of several criteria for model selection, and understanding its unique properties is key to using it effectively. This practice contrasts BIC with the closely related Akaike Information Criterion (AIC) to highlight how BIC's penalty for model complexity scales with sample size ($n$). By deriving the critical sample size at which BIC begins to favor a simpler model while AIC does not, you will gain a concrete understanding of why BIC is known for its tendency to select more parsimonious models in large datasets .",
            "id": "2734851",
            "problem": "Consider two nested continuous-time Markov chain substitution models for nucleotide evolution on a fixed phylogenetic tree, a simpler model $\\mathcal{M}_{s}$ with $k_{s}$ free parameters and a more complex model $\\mathcal{M}_{c}$ with $k_{c}$ free parameters, where $\\Delta k = k_{c} - k_{s}  0$. Let the alignment length be $n$ independent and identically distributed sites. Suppose that, for data generated under $\\mathcal{M}_{s}$, the maximum log-likelihood gain of $\\mathcal{M}_{c}$ over $\\mathcal{M}_{s}$ is a fixed constant $D = \\ln \\hat{L}_{c} - \\ln \\hat{L}_{s}  0$ that does not increase with $n$ (reflecting an overfitting gain that does not scale with additional independent sites). Using the standard definitions of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC), and treating $n$ as the sample size, do the following:\n\n- Explain, from first principles, how the penalty term in BIC depends on $\\ln n$, whereas the penalty term in AIC does not depend on $n$.\n- Derive an exact, closed-form expression for the smallest alignment length $n_{\\star}$ at which BIC will select $\\mathcal{M}_{s}$ while AIC still selects $\\mathcal{M}_{c}$, in terms of $D$ and $\\Delta k$.\n\nAssume $D  \\Delta k$ so that the Akaike Information Criterion initially prefers $\\mathcal{M}_{c}$ for small $n$. Your final answer must be a single analytic expression for $n_{\\star}$. No numerical evaluation is required, and no units are needed.",
            "solution": "We begin from the standard definitions of two information criteria used in model selection. The Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) for a model with maximized likelihood $\\hat{L}$, parameter count $k$, and sample size $n$ are\n$$\n\\mathrm{AIC} = -2 \\ln \\hat{L} + 2 k, \\quad \\mathrm{BIC} = -2 \\ln \\hat{L} + k \\ln n.\n$$\nThese are well-tested formulas derived, respectively, from an asymptotic estimate of Kullback–Leibler risk minimization and from a large-sample approximation to the marginal likelihood under a specific prior regularity.\n\nFirst, we show how the penalization behaves with $n$. The AIC penalty term is $2k$, which is constant with respect to $n$. The BIC penalty term is $k \\ln n$, which increases logarithmically with $n$ because $\\ln n$ is a monotonically increasing function of $n$ for $n  1$. Hence, as $n$ grows, the BIC penalty increases without bound proportionally to $\\ln n$, while the AIC penalty remains fixed.\n\nNext, compare the two nested models $\\mathcal{M}_{c}$ (complex) and $\\mathcal{M}_{s}$ (simple). Define $\\Delta k = k_{c} - k_{s}  0$ and $D = \\ln \\hat{L}_{c} - \\ln \\hat{L}_{s}  0$. Consider the differences in criteria:\n$$\n\\Delta \\mathrm{AIC} \\equiv \\mathrm{AIC}_{c} - \\mathrm{AIC}_{s} = \\left[-2 \\ln \\hat{L}_{c} + 2 k_{c}\\right] - \\left[-2 \\ln \\hat{L}_{s} + 2 k_{s}\\right] = -2(\\ln \\hat{L}_{c} - \\ln \\hat{L}_{s}) + 2 (k_{c} - k_{s}) = -2D + 2 \\Delta k.\n$$\nSimilarly,\n$$\n\\Delta \\mathrm{BIC} \\equiv \\mathrm{BIC}_{c} - \\mathrm{BIC}_{s} = \\left[-2 \\ln \\hat{L}_{c} + k_{c} \\ln n\\right] - \\left[-2 \\ln \\hat{L}_{s} + k_{s} \\ln n\\right] = -2(\\ln \\hat{L}_{c} - \\ln \\hat{L}_{s}) + (k_{c} - k_{s}) \\ln n = -2D + \\Delta k \\, \\ln n.\n$$\nA model is preferred by a given criterion when it yields the smaller value, so $\\mathcal{M}_{c}$ is preferred by AIC when $\\Delta \\mathrm{AIC}  0$ and $\\mathcal{M}_{s}$ is preferred when $\\Delta \\mathrm{AIC}  0$; likewise for BIC with $\\Delta \\mathrm{BIC}$. By assumption, $D  \\Delta k$, which implies\n$$\n\\Delta \\mathrm{AIC} = -2D + 2 \\Delta k  0,\n$$\nso the Akaike Information Criterion prefers $\\mathcal{M}_{c}$ regardless of $n$ because $\\Delta \\mathrm{AIC}$ is independent of $n$.\n\nFor the Bayesian Information Criterion to select the simpler model, we require $\\Delta \\mathrm{BIC}  0$, namely\n$$\n-2D + \\Delta k \\, \\ln n  0 \\quad \\Longleftrightarrow \\quad \\ln n  \\frac{2D}{\\Delta k} \\quad \\Longleftrightarrow \\quad n  \\exp\\!\\left(\\frac{2D}{\\Delta k}\\right).\n$$\nTherefore, the smallest alignment length at which the Bayesian Information Criterion will overtake the Akaike Information Criterion in favoring the simpler model (while AIC still favors the complex model) occurs at the threshold\n$$\nn_{\\star} = \\exp\\!\\left(\\frac{2D}{\\Delta k}\\right).\n$$\nThis threshold is well defined under the stated assumption $D  \\Delta k$, which ensures that AIC continues to prefer $\\mathcal{M}_{c}$ independently of $n$, while BIC eventually prefers $\\mathcal{M}_{s}$ once the $\\ln n$ penalty dominates the fixed overfitting gain $D$.",
            "answer": "$$\\boxed{\\exp\\!\\left(\\frac{2D}{\\Delta k}\\right)}$$"
        },
        {
            "introduction": "Theoretical concepts come to life through practical application, and this coding exercise places BIC in the context of a classic regression problem: multicollinearity. You will simulate data where predictor variables are highly correlated and use BIC to decide whether to include a redundant variable. This hands-on simulation will demonstrate how BIC's penalty term naturally guards against overfitting by penalizing complexity that adds little explanatory power, guiding you toward a more stable and interpretable model .",
            "id": "3102696",
            "problem": "Consider a Gaussian linear regression model where the response vector $y \\in \\mathbb{R}^n$ is generated from two predictors $x_1 \\in \\mathbb{R}^n$ and $x_2 \\in \\mathbb{R}^n$ with an intercept. You will study the effect of high collinearity between $x_1$ and $x_2$ on model selection via the Bayesian Information Criterion (BIC) and relate it to the Variance Inflation Factor (VIF) and changes in maximized log-likelihood. The task is to implement everything from first principles, starting from the likelihood of the Gaussian linear model.\n\nFundamental base:\n- Assume the data are generated from the Gaussian linear model $y \\mid X, \\beta, \\sigma^2 \\sim \\mathcal{N}(X \\beta, \\sigma^2 I_n)$, where $X$ is the design matrix with an intercept and predictors, $\\beta$ is the vector of regression coefficients, and $\\sigma^2$ is the noise variance. The log-likelihood must be derived from this model.\n- Generate collinear predictors by constructing $x_2$ with a target correlation to $x_1$ using independent standard normal noise.\n\nDefinitions required for implementation:\n- Bayesian Information Criterion (BIC) must be implemented using its definition based on the maximized log-likelihood for the Gaussian linear model and a complexity term proportional to the number of free parameters and the sample size. Count all regression coefficients including the intercept plus the variance parameter as free parameters.\n- Variance Inflation Factor (VIF) for a predictor $x_j$ is defined using the coefficient of determination when regressing $x_j$ on the remaining predictors.\n\nCandidate models to compare for each simulation:\n- Full model: intercept, $x_1$, and $x_2$.\n- Reduced model: intercept and $x_1$ only.\n\nFor each parameter set in the test suite below, perform the following steps:\n1. Construct correlated predictors $x_1$ and $x_2$ of length $n$ by first generating $x_1$ from a standard normal distribution and then setting $x_2 = \\rho \\, x_1 + \\sqrt{1 - \\rho^2} \\, \\eta$, where $\\eta$ is an independent standard normal vector. Use the provided random seed to ensure reproducibility.\n2. Generate $y$ according to $y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, using the specified parameters. Set the intercept $\\alpha = 0$.\n3. For each of the two candidate models, compute the maximum likelihood estimates for the regression coefficients and the noise variance, then compute the maximized log-likelihood. Use these to compute the Bayesian Information Criterion (BIC), with the number of free parameters counted as all regression coefficients (including the intercept) plus the variance parameter.\n4. Compute the Variance Inflation Factor (VIF) for $x_2$ with respect to $x_1$ in the full model by regressing $x_2$ on an intercept and $x_1$ and using the coefficient of determination in that auxiliary regression.\n5. Determine whether the BIC discards the redundant variable by checking if the reduced model has a strictly lower BIC than the full model. Report this as an integer: $1$ if the reduced model is preferred (discard $x_2$) and $0$ otherwise.\n6. Compute the absolute difference in maximized log-likelihood between the full and reduced models.\n7. For each test case, output a list of three items: the integer selection indicator, the VIF for $x_2$ rounded to four decimal places, and the absolute difference in maximized log-likelihood rounded to six decimal places.\n\nTest suite parameters:\n- Case $1$: $n = 200$, $\\rho = 0.95$, $\\beta_1 = 1.0$, $\\beta_2 = 0.0$, $\\sigma = 1.0$, seed $= 42$.\n- Case $2$: $n = 200$, $\\rho = 0.95$, $\\beta_1 = 1.0$, $\\beta_2 = 0.1$, $\\sigma = 1.0$, seed $= 123$.\n- Case $3$: $n = 200$, $\\rho = 0.0$, $\\beta_1 = 1.0$, $\\beta_2 = 0.5$, $\\sigma = 1.0$, seed $= 7$.\n- Case $4$: $n = 40$, $\\rho = 0.9$, $\\beta_1 = 1.0$, $\\beta_2 = 0.0$, $\\sigma = 1.0$, seed $= 777$.\n- Case $5$: $n = 200$, $\\rho = 0.99$, $\\beta_1 = 1.0$, $\\beta_2 = 0.5$, $\\sigma = 1.0$, seed $= 2024$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. Each element corresponds to a test case in the given order and is itself a list formatted as described in step $7$. For example, the output should look like $[[s_1,v_1,d_1],[s_2,v_2,d_2],\\dots]$, where $s_i$ is the selection indicator, $v_i$ is the rounded VIF, and $d_i$ is the rounded absolute log-likelihood difference for case $i$.",
            "solution": "The problem requires a thorough analysis of the Bayesian Information Criterion (BIC) in the context of a Gaussian linear regression model with collinear predictors. The analysis involves implementing the necessary statistical tools—Maximum Likelihood Estimation (MLE), BIC, and Variance Inflation Factor (VIF)—from first principles.\n\n### Theoretical Framework\n\n#### 1. Gaussian Linear Model and Maximum Likelihood Estimation\n\nThe foundation of this analysis is the Gaussian linear model. We assume the response vector $y \\in \\mathbb{R}^n$ is generated as:\n$$\ny = X\\beta + \\varepsilon\n$$\nwhere $X$ is the $n \\times (p+1)$ design matrix (including an intercept column), $\\beta \\in \\mathbb{R}^{p+1}$ is the vector of regression coefficients, and $\\varepsilon$ is a vector of i.i.d. Gaussian noise, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$.\n\nThe probability density function for a single observation $y_i$ is:\n$$\nf(y_i \\mid x_i, \\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right)\n$$\nGiven that the observations are independent, the likelihood function for the entire dataset is the product of the individual densities:\n$$\nL(\\beta, \\sigma^2 \\mid y, X) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - X\\beta\\|^2\\right)\n$$\nThe log-likelihood function, $\\ell(\\beta, \\sigma^2) = \\log L(\\beta, \\sigma^2)$, is:\n$$\n\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\|y - X\\beta\\|^2\n$$\nTo find the Maximum Likelihood Estimates (MLEs), we maximize $\\ell$ with respect to $\\beta$ and $\\sigma^2$.\nMaximizing $\\ell$ with respect to $\\beta$ is equivalent to minimizing the Residual Sum of Squares (RSS), $\\|y - X\\beta\\|^2$. This yields the standard Ordinary Least Squares (OLS) estimator:\n$$\n\\hat{\\beta}_{MLE} = (X^TX)^{-1}X^Ty\n$$\nSubstituting $\\hat{\\beta}_{MLE}$ into the log-likelihood and differentiating with respect to $\\sigma^2$ gives the MLE for the variance:\n$$\n\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\|y - X\\hat{\\beta}_{MLE}\\|^2 = \\frac{\\text{RSS}}{n}\n$$\nThe maximized log-likelihood, denoted $\\ell(\\hat{\\theta})$ where $\\hat{\\theta} = (\\hat{\\beta}_{MLE}, \\hat{\\sigma}^2_{MLE})$, is found by substituting these MLEs back into the log-likelihood function:\n$$\n\\ell(\\hat{\\theta}) = -\\frac{n}{2}\\left( \\log(2\\pi) + \\log(\\hat{\\sigma}^2_{MLE}) + 1 \\right)\n$$\nThis formula is central to computing the BIC.\n\n#### 2. Bayesian Information Criterion (BIC)\n\nBIC is a criterion for model selection that balances model fit (measured by the maximized log-likelihood) against model complexity. Its formula is:\n$$\n\\text{BIC} = k \\log(n) - 2\\ell(\\hat{\\theta})\n$$\nwhere $n$ is the number of samples and $k$ is the number of free parameters in the model. A lower BIC value indicates a preferred model.\n\nAs specified, $k$ is the count of all estimated parameters. For a linear model with $p$ predictors, we have $p+1$ regression coefficients (including the intercept) and one variance parameter, $\\sigma^2$. Thus, $k = p+2$.\nFor the two models under consideration:\n- **Full model ($M_f$)**: $y$ is regressed on an intercept, $x_1$, and $x_2$. Here, $p=2$, so the number of free parameters is $k_f = 2 + 2 = 4$.\n- **Reduced model ($M_r$)**: $y$ is regressed on an intercept and $x_1$. Here, $p=1$, so the number of free parameters is $k_r = 1 + 2 = 3$.\n\nThe BIC for each model is:\n$$\n\\text{BIC}_f = 4 \\log(n) - 2\\ell(\\hat{\\theta}_f)\n$$\n$$\n\\text{BIC}_r = 3 \\log(n) - 2\\ell(\\hat{\\theta}_r)\n$$\nThe reduced model is preferred if $\\text{BIC}_r  \\text{BIC}_f$.\n\n#### 3. Variance Inflation Factor (VIF)\n\nVIF quantifies the severity of multicollinearity in an OLS regression. The VIF for a predictor $x_j$ is given by:\n$$\n\\text{VIF}_j = \\frac{1}{1 - R_j^2}\n$$\nwhere $R_j^2$ is the coefficient of determination from an auxiliary regression of $x_j$ onto the other predictors in the model (including the intercept). A high VIF (typically $5$ or $10$) indicates that the predictor is highly correlated with other predictors, which can inflate the variance of its coefficient estimate.\n\nFor this problem, we compute the VIF for $x_2$ in the full model. This involves regressing $x_2$ on an intercept and $x_1$. The $R^2$ for this regression, $R^2_{x_2|x_1}$, is calculated as:\n$$\nR^2_{x_2|x_1} = 1 - \\frac{\\text{RSS}_{aux}}{\\text{TSS}_{aux}}\n$$\nwhere $\\text{RSS}_{aux}$ is the residual sum of squares from the auxiliary regression, and $\\text{TSS}_{aux}$ is the total sum of squares for $x_2$.\n\n### Computational Procedure\n\nFor each test case, the following sequence of calculations is performed:\n\n1.  **Data Generation**:\n    - A random number generator is initialized with the specified seed for reproducibility.\n    - An $n$-dimensional vector $x_1$ is drawn from a standard normal distribution, $\\mathcal{N}(0, 1)$.\n    - An independent $n$-dimensional noise vector $\\eta$ is also drawn from $\\mathcal{N}(0, 1)$.\n    - The correlated predictor $x_2$ is constructed as $x_2 = \\rho x_1 + \\sqrt{1 - \\rho^2} \\eta$, where $\\rho$ is the target correlation.\n    - A noise vector $\\varepsilon$ is drawn from $\\mathcal{N}(0, \\sigma^2 I_n)$.\n    - The response vector $y$ is generated via the true model: $y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$ (with intercept $\\alpha = 0$).\n\n2.  **Model Fitting and BIC Calculation**:\n    - A unified function is designed to compute the maximized log-likelihood and parameter count for any given design matrix $X$ and response $y$.\n    - **Full Model ($M_f$)**: The design matrix $X_f$ is formed by concatenating a column of ones, $x_1$, and $x_2$. Using $X_f$ and $y$, the maximized log-likelihood $\\ell(\\hat{\\theta}_f)$ and parameter count $k_f=4$ are computed, which are then used to calculate $\\text{BIC}_f$.\n    - **Reduced Model ($M_r$)**: The design matrix $X_r$ is formed with a column of ones and $x_1$. Similarly, $\\ell(\\hat{\\theta}_r)$, $k_r=3$, and $\\text{BIC}_r$ are computed.\n\n3.  **VIF Calculation**:\n    - The VIF for $x_2$ is computed. The auxiliary design matrix $X_{aux}$ is formed from a column of ones and $x_1$.\n    - An OLS regression of $x_2$ on $X_{aux}$ is performed to find the residual sum of squares $\\text{RSS}_{aux}$.\n    - The total sum of squares $\\text{TSS}_{aux}$ for $x_2$ is computed.\n    - The coefficient of determination $R^2$ and subsequently the VIF are calculated using their definitions.\n\n4.  **Result Aggregation**:\n    - The selection indicator is determined: $1$ if $\\text{BIC}_r  \\text{BIC}_f$, and $0$ otherwise.\n    - The absolute difference in maximized log-likelihoods, $|\\ell(\\hat{\\theta}_f) - \\ell(\\hat{\\theta}_r)|$, is computed.\n    - The final output for the case is a list containing the selection indicator, the VIF for $x_2$ (rounded to 4 decimal places), and the log-likelihood difference (rounded to 6 decimal places). This procedure is repeated for all test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs the full analysis for each test case as specified in the problem statement.\n    This involves data generation, model fitting (full and reduced), BIC comparison,\n    VIF calculation, and log-likelihood difference computation.\n    \"\"\"\n    test_cases = [\n        # (n, rho, beta1, beta2, sigma, seed)\n        (200, 0.95, 1.0, 0.0, 1.0, 42),\n        (200, 0.95, 1.0, 0.1, 1.0, 123),\n        (200, 0.0, 1.0, 0.5, 1.0, 7),\n        (40, 0.9, 1.0, 0.0, 1.0, 777),\n        (200, 0.99, 1.0, 0.5, 1.0, 2024),\n    ]\n\n    results = []\n\n    def compute_mle_metrics(X, y):\n        \"\"\"\n        Computes the maximized log-likelihood and parameter count for a linear model.\n\n        Args:\n            X (np.ndarray): The design matrix (n_samples, n_features_with_intercept).\n            y (np.ndarray): The response vector (n_samples,).\n\n        Returns:\n            tuple: A tuple containing:\n                - float: The maximized log-likelihood.\n                - int: The number of free parameters (coefficients + variance).\n        \"\"\"\n        n_samples = X.shape[0]\n        p_coeffs = X.shape[1]\n\n        # Solve for coefficients and RSS using numerically stable least squares\n        try:\n            _, rss_array, _, _ = np.linalg.lstsq(X, y, rcond=None)\n            # lstsq returns rss as an array, even if it's a single value\n            rss = rss_array[0]\n        except np.linalg.LinAlgError:\n            return -np.inf, p_coeffs + 1\n        \n        # MLE for variance sigma^2\n        mle_var = rss / n_samples\n\n        if mle_var = 1e-9:  # Avoid log(0) for perfect fits\n            log_likelihood = np.inf if np.allclose(y, X @ np.linalg.lstsq(X, y, rcond=None)[0]) else -np.inf\n        else:\n            # Maximized log-likelihood for Gaussian model\n            log_likelihood = -n_samples / 2.0 * (np.log(2.0 * np.pi) + np.log(mle_var) + 1.0)\n            \n        # Number of free parameters: p coefficients + 1 variance parameter\n        k = p_coeffs + 1\n        return log_likelihood, k\n\n    for n, rho, beta1, beta2, sigma, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Construct correlated predictors\n        x1 = rng.normal(size=n)\n        eta = rng.normal(size=n)\n        x2 = rho * x1 + np.sqrt(1 - rho**2) * eta\n\n        # Step 2: Generate response variable y\n        epsilon = rng.normal(loc=0, scale=sigma, size=n)\n        y = beta1 * x1 + beta2 * x2 + epsilon  # Intercept alpha is 0\n\n        # Step 3: Analyze full and reduced models\n        # Full model: intercept, x1, x2\n        X_f = np.c_[np.ones(n), x1, x2]\n        logL_f, k_f = compute_mle_metrics(X_f, y)\n        bic_f = k_f * np.log(n) - 2 * logL_f\n\n        # Reduced model: intercept, x1\n        X_r = np.c_[np.ones(n), x1]\n        logL_r, k_r = compute_mle_metrics(X_r, y)\n        bic_r = k_r * np.log(n) - 2 * logL_r\n\n        # Step 4: Compute VIF for x2\n        # Auxiliary regression: x2 on intercept and x1\n        X_aux = np.c_[np.ones(n), x1]\n        try:\n            _, rss_aux_array, _, _ = np.linalg.lstsq(X_aux, x2, rcond=None)\n            rss_aux = rss_aux_array.item()\n            # Total sum of squares for x2\n            tss_aux = np.sum((x2 - np.mean(x2))**2)\n            if tss_aux  1e-9: # x2 has no variance, R^2 is undefined\n                vif_x2 = 1.0\n            else:\n                r_squared_aux = 1 - rss_aux / tss_aux\n                if r_squared_aux >= 1.0: # Perfect collinearity\n                    vif_x2 = np.inf\n                else:    \n                    vif_x2 = 1 / (1 - r_squared_aux)\n        except np.linalg.LinAlgError:\n            vif_x2 = np.inf\n\n        # Step 5: Determine selection based on BIC\n        selection_indicator = 1 if bic_r  bic_f else 0\n\n        # Step 6: Compute absolute difference in maximized log-likelihood\n        log_likelihood_diff = np.abs(logL_f - logL_r)\n\n        # Step 7: Store results for this case\n        results.append([\n            selection_indicator,\n            round(vif_x2, 4),\n            round(log_likelihood_diff, 6)\n        ])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}