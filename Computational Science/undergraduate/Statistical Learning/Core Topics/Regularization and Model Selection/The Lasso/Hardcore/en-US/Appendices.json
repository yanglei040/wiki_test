{
    "hands_on_practices": [
        {
            "introduction": "The heart of the Lasso method is its unique penalty function, which is what enables it to perform feature selection. To truly grasp how Lasso works, we must first start with its most fundamental component: the $L_1$ norm. This first exercise  provides a straightforward calculation to solidify your understanding of how the Lasso penalty is computed from a given set of model coefficients.",
            "id": "1928640",
            "problem": "In statistical learning, the Least Absolute Shrinkage and Selection Operator (Lasso) is a regression analysis method that performs both variable selection and regularization to enhance the prediction accuracy and interpretability of the statistical model. The penalty applied in the Lasso is based on the $L_1$ norm of the vector of model coefficients.\n\nSuppose a data analyst has fit a Lasso regression model with four predictor variables. The final estimated coefficient vector for these predictors is given by $\\beta = (3, -1, 0, -4)^T$.\n\nCalculate the $L_1$ norm of this coefficient vector, denoted as $\\|\\beta\\|_1$.",
            "solution": "The $L_{1}$ norm of a vector is defined as the sum of the absolute values of its components. For $\\beta = (3, -1, 0, -4)^{T}$, this is\n$$\n\\|\\beta\\|_{1}=\\sum_{i=1}^{4}|\\beta_{i}|=|3|+|-1|+|0|+|-4|.\n$$\nEvaluating each absolute value,\n$$\n|3|=3,\\quad |-1|=1,\\quad |0|=0,\\quad |-4|=4,\n$$\nso\n$$\n\\|\\beta\\|_{1}=3+1+0+4=8.\n$$",
            "answer": "$$\\boxed{8}$$"
        },
        {
            "introduction": "While the Lasso penalty's form is simple, its effect is controlled by the crucial tuning parameter, $\\lambda$, which determines the strength of the regularization. A value of $\\lambda$ that is too small will not perform enough shrinkage, while a value that is too large will erase all features. This hands-on exercise  guides you through the standard process of using k-fold cross-validation to find the optimal $\\lambda$, demonstrating how to use your data to balance the trade-off between model fit and complexity.",
            "id": "1928609",
            "problem": "A materials scientist is developing a predictive model for the thermal conductivity, $y$, of a new alloy based on the concentrations of two key additives, $x_1$ and $x_2$. The proposed model is a linear relationship of the form $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$. To prevent overfitting and select a parsimonious model, the scientist decides to use the Least Absolute Shrinkage and Selection Operator (Lasso) regression technique.\n\nThe core of the Lasso is the tuning parameter, $\\lambda$, which controls the amount of regularization. The optimal value of $\\lambda$ is to be chosen from the set $\\{0.1, 1.0, 10.0\\}$ using 3-fold cross-validation. The scientist has collected the following six data points:\n\n| Data Point | $x_1$ | $x_2$ | $y$ |\n| :--- | :--- | :--- | :--- |\n| 1 | 1.0 | 0.0 | 1.5 |\n| 2 | 0.0 | 1.0 | 2.5 |\n| 3 | 2.0 | 1.0 | 4.0 |\n| 4 | 1.0 | 2.0 | 5.0 |\n| 5 | 3.0 | 2.0 | 6.5 |\n| 6 | 2.0 | 3.0 | 7.5 |\n\nThe 3-fold cross-validation is set up as follows:\n- **Fold 1**: Data points 1, 2\n- **Fold 2**: Data points 3, 4\n- **Fold 3**: Data points 5, 6\n\nFor each of the three cross-validation iterations, a Lasso model was trained on two of the folds (the training set) and then used to predict on the remaining fold (the validation set). The resulting model coefficients $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$ for each training run and each value of $\\lambda$ are provided below:\n\n**Coefficients from training on Folds 2 and 3 (for validating on Fold 1):**\n- For $\\lambda=0.1$: $\\hat{\\beta}_0=1.35, \\hat{\\beta}_1=0.84, \\hat{\\beta}_2=1.26$\n- For $\\lambda=1.0$: $\\hat{\\beta}_0=2.80, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.90$\n- For $\\lambda=10.0$: $\\hat{\\beta}_0=5.75, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.00$\n\n**Coefficients from training on Folds 1 and 3 (for validating on Fold 2):**\n- For $\\lambda=0.1$: $\\hat{\\beta}_0=0.40, \\hat{\\beta}_1=1.33, \\hat{\\beta}_2=1.49$\n- For $\\lambda=1.0$: $\\hat{\\beta}_0=1.13, \\hat{\\beta}_1=0.65, \\hat{\\beta}_2=1.55$\n- For $\\lambda=10.0$: $\\hat{\\beta}_0=4.50, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.00$\n\n**Coefficients from training on Folds 1 and 2 (for validating on Fold 3):**\n- For $\\lambda=0.1$: $\\hat{\\beta}_0=0.60, \\hat{\\beta}_1=1.14, \\hat{\\beta}_2=1.56$\n- For $\\lambda=1.0$: $\\hat{\\beta}_0=1.55, \\hat{\\beta}_1=0.40, \\hat{\\beta}_2=1.30$\n- For $\\lambda=10.0$: $\\hat{\\beta}_0=3.25, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.00$\n\nYour task is to calculate the cross-validation error for each candidate value of $\\lambda$. The error for each fold should be measured using the Mean Squared Error (MSE). The overall cross-validation error for a given $\\lambda$ is the average of the MSEs from the three folds. Based on your calculations, which of the following is the optimal value for the tuning parameter $\\lambda$?\n\nA. $0.1$\n\nB. $1.0$\n\nC. $10.0$\n\nD. All values of $\\lambda$ result in the same cross-validation error.",
            "solution": "We model the response as $\\hat{y}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{1}+\\hat{\\beta}_{2}x_{2}$. For each fold and $\\lambda$, we compute residuals $r_{i}=y_{i}-\\hat{y}_{i}$ on the validation points and the fold MSE as $\\text{MSE}=\\frac{1}{n}\\sum r_{i}^{2}$ with $n=2$. The cross-validation error for a given $\\lambda$ is the average of the three fold MSEs.\n\nFold 1 (validate on points 1 and 2; use coefficients from training on Folds 2 and 3):\n\nFor $\\lambda=0.1$: $(\\hat{\\beta}_{0},\\hat{\\beta}_{1},\\hat{\\beta}_{2})=(1.35,0.84,1.26)$.\n- Point 1: $(x_{1},x_{2},y)=(1,0,1.5)$, $\\hat{y}=1.35+0.84\\cdot 1+1.26\\cdot 0=2.19$, $r=1.5-2.19=-0.69$, $r^{2}=0.4761$.\n- Point 2: $(0,1,2.5)$, $\\hat{y}=1.35+0.84\\cdot 0+1.26\\cdot 1=2.61$, $r=2.5-2.61=-0.11$, $r^{2}=0.0121$.\n$$\\text{MSE}_{1}(0.1)=\\frac{0.4761+0.0121}{2}=0.2441.$$\n\nFor $\\lambda=1.0$: $(2.80,0.00,0.90)$.\n- Point 1: $\\hat{y}=2.80$, $r=-1.30$, $r^{2}=1.69$.\n- Point 2: $\\hat{y}=2.80+0.90=3.70$, $r=-1.20$, $r^{2}=1.44$.\n$$\\text{MSE}_{1}(1.0)=\\frac{1.69+1.44}{2}=1.565.$$\n\nFor $\\lambda=10.0$: $(5.75,0.00,0.00)$.\n- Point 1: $\\hat{y}=5.75$, $r=-4.25$, $r^{2}=18.0625$.\n- Point 2: $\\hat{y}=5.75$, $r=-3.25$, $r^{2}=10.5625$.\n$$\\text{MSE}_{1}(10.0)=\\frac{18.0625+10.5625}{2}=14.3125.$$\n\nFold 2 (validate on points 3 and 4; use coefficients from training on Folds 1 and 3):\n\nFor $\\lambda=0.1$: $(0.40,1.33,1.49)$.\n- Point 3: $(2,1,4.0)$, $\\hat{y}=0.40+1.33\\cdot 2+1.49\\cdot 1=4.55$, $r=-0.55$, $r^{2}=0.3025$.\n- Point 4: $(1,2,5.0)$, $\\hat{y}=0.40+1.33\\cdot 1+1.49\\cdot 2=4.71$, $r=0.29$, $r^{2}=0.0841$.\n$$\\text{MSE}_{2}(0.1)=\\frac{0.3025+0.0841}{2}=0.1933.$$\n\nFor $\\lambda=1.0$: $(1.13,0.65,1.55)$.\n- Point 3: $\\hat{y}=1.13+0.65\\cdot 2+1.55\\cdot 1=3.98$, $r=0.02$, $r^{2}=0.0004$.\n- Point 4: $\\hat{y}=1.13+0.65\\cdot 1+1.55\\cdot 2=4.88$, $r=0.12$, $r^{2}=0.0144$.\n$$\\text{MSE}_{2}(1.0)=\\frac{0.0004+0.0144}{2}=0.0074.$$\n\nFor $\\lambda=10.0$: $(4.50,0.00,0.00)$.\n- Point 3: $\\hat{y}=4.50$, $r=-0.50$, $r^{2}=0.25$.\n- Point 4: $\\hat{y}=4.50$, $r=0.50$, $r^{2}=0.25$.\n$$\\text{MSE}_{2}(10.0)=\\frac{0.25+0.25}{2}=0.25.$$\n\nFold 3 (validate on points 5 and 6; use coefficients from training on Folds 1 and 2):\n\nFor $\\lambda=0.1$: $(0.60,1.14,1.56)$.\n- Point 5: $(3,2,6.5)$, $\\hat{y}=0.60+1.14\\cdot 3+1.56\\cdot 2=7.14$, $r=-0.64$, $r^{2}=0.4096$.\n- Point 6: $(2,3,7.5)$, $\\hat{y}=0.60+1.14\\cdot 2+1.56\\cdot 3=7.56$, $r=-0.06$, $r^{2}=0.0036$.\n$$\\text{MSE}_{3}(0.1)=\\frac{0.4096+0.0036}{2}=0.2066.$$\n\nFor $\\lambda=1.0$: $(1.55,0.40,1.30)$.\n- Point 5: $\\hat{y}=1.55+0.40\\cdot 3+1.30\\cdot 2=5.35$, $r=1.15$, $r^{2}=1.3225$.\n- Point 6: $\\hat{y}=1.55+0.40\\cdot 2+1.30\\cdot 3=6.25$, $r=1.25$, $r^{2}=1.5625$.\n$$\\text{MSE}_{3}(1.0)=\\frac{1.3225+1.5625}{2}=1.4425.$$\n\nFor $\\lambda=10.0$: $(3.25,0.00,0.00)$.\n- Point 5: $\\hat{y}=3.25$, $r=3.25$, $r^{2}=10.5625$.\n- Point 6: $\\hat{y}=3.25$, $r=4.25$, $r^{2}=18.0625$.\n$$\\text{MSE}_{3}(10.0)=\\frac{10.5625+18.0625}{2}=14.3125.$$\n\nCompute average cross-validation error for each $\\lambda$:\n$$\\text{CV}(0.1)=\\frac{0.2441+0.1933+0.2066}{3}=0.214666\\ldots,$$\n$$\\text{CV}(1.0)=\\frac{1.565+0.0074+1.4425}{3}=1.004966\\ldots,$$\n$$\\text{CV}(10.0)=\\frac{14.3125+0.25+14.3125}{3}=9.625.$$\n\nThe smallest cross-validation error occurs at $\\lambda=0.1$, hence the optimal choice is option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Having learned what the Lasso penalty is and how to tune it, we can now look under the hood to see how the optimization process actually works. In the idealized setting of orthonormal features, the seemingly complex Lasso problem has an elegant and intuitive closed-form solution. This advanced exercise  will guide you through deriving the Karush-Kuhn-Tucker (KKT) optimality conditions to reveal this solution—known as soft-thresholding—and apply it to a concrete example, connecting the abstract penalty to the concrete acts of shrinking and selecting coefficients.",
            "id": "3184336",
            "problem": "Consider the least absolute shrinkage and selection operator (Lasso) problem with no intercept: minimize the objective\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is a fixed design matrix, $y \\in \\mathbb{R}^{n}$ is a response vector, $\\beta \\in \\mathbb{R}^{p}$ is the coefficient vector, and $\\lambda \\ge 0$ is a tuning parameter. Assume that the columns of $X$ are standardized to be orthonormal, that is, $X^{\\top}X = I_{p}$, and that there is no intercept term in the model.\n\nTasks:\n1) Starting only from the convex optimality principle that a point $\\beta^{\\star}$ minimizes a convex function $f$ if and only if $\\mathbf{0} \\in \\partial f(\\beta^{\\star})$, and from the definition of the subgradient of the $\\ell_{1}$-norm, derive explicit Karush-Kuhn-Tucker (KKT) conditions (stationarity with subgradients) that characterize optimal solutions $\\beta^{\\star}$ to the Lasso problem in this setting. Your derivation must make clear how these conditions specialize for coordinates $j$ with $\\beta^{\\star}_{j} \\ne 0$ and for coordinates with $\\beta^{\\star}_{j} = 0$.\n\n2) Using your derived conditions, hand-solve the following three-feature instance with $p = 3$. Suppose $X^{\\top}X = I_{3}$ and\n$$\nX^{\\top}y = \\begin{pmatrix} 1.8 \\\\ -0.5 \\\\ 0.4 \\end{pmatrix}, \\qquad \\lambda = 0.6.\n$$\nCompute the optimal coefficient vector $\\beta^{\\star} \\in \\mathbb{R}^{3}$.\n\nProvide your final numerical answer for $\\beta^{\\star}$ as a single row vector.",
            "solution": "**Task 1: Derivation of the Karush-Kuhn-Tucker (KKT) conditions**\n\nThe Lasso objective function to be minimized is:\n$$\nf(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^{n}$, $\\beta \\in \\mathbb{R}^{p}$, and $\\lambda \\ge 0$. We are given that the design matrix $X$ has orthonormal columns, i.e., $X^{\\top}X = I_{p}$.\n\nThe objective function $f(\\beta)$ is a sum of two convex functions:\n$1$. $g(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$, which is differentiable and convex.\n$2$. $h(\\beta) = \\lambda \\|\\beta\\|_{1}$, which is convex but not differentiable at points where any component of $\\beta$ is zero.\n\nThe function $f(\\beta)$ is convex. According to the fundamental principle of convex optimization, a point $\\beta^{\\star}$ is a global minimizer of $f(\\beta)$ if and only if the zero vector is an element of the subdifferential of $f$ at $\\beta^{\\star}$:\n$$\n\\mathbf{0} \\in \\partial f(\\beta^{\\star})\n$$\nSince $g(\\beta)$ is differentiable, the subdifferential of the sum $f(\\beta) = g(\\beta) + h(\\beta)$ is the sum of the gradient of $g(\\beta)$ and the subdifferential of $h(\\beta)$:\n$$\n\\partial f(\\beta^{\\star}) = \\nabla g(\\beta^{\\star}) + \\partial h(\\beta^{\\star})\n$$\nFirst, we compute the gradient of $g(\\beta)$. We expand the squared norm:\n$$\ng(\\beta) = \\frac{1}{2}(y - X\\beta)^{\\top}(y - X\\beta) = \\frac{1}{2}(y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta)\n$$\nTaking the gradient with respect to $\\beta$:\n$$\n\\nabla g(\\beta) = \\frac{1}{2}(-2X^{\\top}y + 2X^{\\top}X\\beta) = X^{\\top}X\\beta - X^{\\top}y\n$$\nUsing the given condition that $X^{\\top}X = I_{p}$, the gradient simplifies to:\n$$\n\\nabla g(\\beta) = \\beta - X^{\\top}y\n$$\nAt the optimum $\\beta^{\\star}$, the gradient is $\\nabla g(\\beta^{\\star}) = \\beta^{\\star} - X^{\\top}y$.\n\nNext, we characterize the subdifferential of $h(\\beta) = \\lambda \\|\\beta\\|_{1} = \\lambda \\sum_{j=1}^{p} |\\beta_j|$. The subdifferential $\\partial h(\\beta)$ is the Cartesian product of the subdifferentials of its components $\\lambda |\\beta_j|$. For a single component $\\beta_j$, the subdifferential of $\\lambda|\\beta_j|$ is:\n$$\n\\partial (\\lambda |\\beta_j|) = \\begin{cases} \\{\\lambda \\cdot \\text{sgn}(\\beta_j)\\} & \\text{if } \\beta_j \\ne 0 \\\\ [-\\lambda, \\lambda] & \\text{if } \\beta_j = 0 \\end{cases}\n$$\nwhere $\\text{sgn}(\\cdot)$ is the sign function. So, $\\partial h(\\beta)$ consists of all vectors $v \\in \\mathbb{R}^{p}$ with components $v_j \\in \\partial (\\lambda |\\beta_j|)$.\n\nThe optimality condition $\\mathbf{0} \\in \\nabla g(\\beta^{\\star}) + \\partial h(\\beta^{\\star})$ can now be written as:\n$$\n\\mathbf{0} \\in (\\beta^{\\star} - X^{\\top}y) + \\partial(\\lambda \\|\\beta^{\\star}\\|_{1})\n$$\nThis is equivalent to stating that there exists a subgradient vector $s \\in \\partial(\\|\\beta^{\\star}\\|_1)$ such that:\n$$\n\\mathbf{0} = (\\beta^{\\star} - X^{\\top}y) + \\lambda s\n$$\nRearranging gives the stationarity condition:\n$$\nX^{\\top}y - \\beta^{\\star} = \\lambda s\n$$\nWe now analyze this condition component-wise for $j \\in \\{1, 2, \\dots, p\\}$, letting $c_j = (X^{\\top}y)_j$:\n$$\nc_j - \\beta^{\\star}_j = \\lambda s_j, \\quad \\text{where } s_j \\in \\partial|\\beta^{\\star}_j|\n$$\n\nWe specialize this condition for the two cases as required.\n\n**Case 1: The optimal coefficient is non-zero, $\\beta^{\\star}_j \\ne 0$.**\nIn this case, the subgradient of $|\\beta^{\\star}_j|$ is single-valued: $s_j = \\text{sgn}(\\beta^{\\star}_j)$. The stationarity condition for this component becomes:\n$$\nc_j - \\beta^{\\star}_j = \\lambda \\cdot \\text{sgn}(\\beta^{\\star}_j)\n$$\nSolving for $\\beta^{\\star}_j$:\n$$\n\\beta^{\\star}_j = c_j - \\lambda \\cdot \\text{sgn}(\\beta^{\\star}_j)\n$$\nIf $\\beta^{\\star}_j > 0$, then $\\text{sgn}(\\beta^{\\star}_j) = 1$, which gives $\\beta^{\\star}_j = c_j - \\lambda$. Consistency requires $c_j - \\lambda > 0$, so $c_j > \\lambda$.\nIf $\\beta^{\\star}_j < 0$, then $\\text{sgn}(\\beta^{\\star}_j) = -1$, which gives $\\beta^{\\star}_j = c_j + \\lambda$. Consistency requires $c_j + \\lambda < 0$, so $c_j < -\\lambda$.\nIn both sub-cases, we have $|c_j| > \\lambda$, and the sign of $\\beta^{\\star}_j$ matches the sign of $c_j$. Thus, we can write $\\text{sgn}(\\beta^{\\star}_j) = \\text{sgn}(c_j)$. The explicit KKT condition for a non-zero coefficient is:\n$$\nc_j - \\beta^{\\star}_j = \\lambda \\cdot \\text{sgn}(c_j) \\quad \\text{or} \\quad \\beta^{\\star}_j = c_j - \\lambda \\cdot \\text{sgn}(c_j)\n$$\n\n**Case 2: The optimal coefficient is zero, $\\beta^{\\star}_j = 0$.**\nIn this case, the subgradient $s_j$ can be any value in the interval $[-1, 1]$. The stationarity condition for this component is:\n$$\nc_j - 0 = \\lambda s_j \\quad \\text{for some } s_j \\in [-1, 1]\n$$\nThis implies $c_j = \\lambda s_j$. Since $s_j$ can range over $[-1, 1]$, $c_j$ must lie in the range $[-\\lambda, \\lambda]$. The explicit KKT condition for a zero coefficient is:\n$$\n|c_j| \\le \\lambda\n$$\n\nIn summary, the KKT conditions derived from the subgradient optimality criterion are:\n- If $|(X^{\\top}y)_j| > \\lambda$, then $\\beta^{\\star}_j = (X^{\\top}y)_j - \\lambda \\cdot \\text{sgn}((X^{\\top}y)_j)$.\n- If $|(X^{\\top}y)_j| \\le \\lambda$, then $\\beta^{\\star}_j = 0$.\n\nThis solution is known as soft-thresholding and can be written compactly as $\\beta^{\\star}_j = S_{\\lambda}((X^{\\top}y)_j)$, where $S_{\\lambda}(z) = \\text{sgn}(z)\\max(0, |z|-\\lambda)$.\n\n**Task 2: Solving a specific instance**\n\nWe are given a three-feature instance ($p=3$) with $X^{\\top}X = I_3$ and:\n$$\nX^{\\top}y = \\begin{pmatrix} 1.8 \\\\ -0.5 \\\\ 0.4 \\end{pmatrix}, \\qquad \\lambda = 0.6\n$$\nWe apply the KKT conditions derived above to compute each component of the optimal coefficient vector $\\beta^{\\star}$. Let $c = X^{\\top}y$.\n\n**For component $j=1$:**\n$c_1 = 1.8$. We compare its absolute value to $\\lambda$:\n$|c_1| = 1.8$. Since $1.8 > 0.6$, we are in the case $|c_1| > \\lambda$.\nThe optimal coefficient is non-zero and is calculated as:\n$$\n\\beta^{\\star}_1 = c_1 - \\lambda \\cdot \\text{sgn}(c_1) = 1.8 - 0.6 \\cdot \\text{sgn}(1.8) = 1.8 - 0.6(1) = 1.2\n$$\n\n**For component $j=2$:**\n$c_2 = -0.5$. We compare its absolute value to $\\lambda$:\n$|c_2| = 0.5$. Since $0.5 \\le 0.6$, we are in the case $|c_2| \\le \\lambda$.\nThe optimal coefficient is zero:\n$$\n\\beta^{\\star}_2 = 0\n$$\n\n**For component $j=3$:**\n$c_3 = 0.4$. We compare its absolute value to $\\lambda$:\n$|c_3| = 0.4$. Since $0.4 \\le 0.6$, we are in the case $|c_3| \\le \\lambda$.\nThe optimal coefficient is zero:\n$$\n\\beta^{\\star}_3 = 0\n$$\n\nCombining these results, the optimal coefficient vector is:\n$$\n\\beta^{\\star} = \\begin{pmatrix} 1.2 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nAs a row vector, this is $\\begin{pmatrix} 1.2 & 0 & 0 \\end{pmatrix}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.2 & 0 & 0 \\end{pmatrix}}\n$$"
        }
    ]
}