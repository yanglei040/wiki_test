{
    "hands_on_practices": [
        {
            "introduction": "要真正理解Lasso回归，我们必须先揭开其数学原理的神秘面纱。本练习将带您深入Lasso的核心，探索其最优解必须满足的Karush-Kuhn-Tucker (KKT) 条件。通过在一个理想化的正交设定下推导并应用这些条件，您将亲手实践Lasso著名的“软阈值”操作，直观地看到系数是如何被压缩乃至归零的，从而实现变量筛选的功能 ()。",
            "id": "3184336",
            "problem": "考虑无截距项的最小绝对收缩和选择算子 (LASSO) 问题：最小化目标函数\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个固定的设计矩阵，$y \\in \\mathbb{R}^{n}$ 是一个响应向量，$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda \\ge 0$ 是一个调整参数。假设 $X$ 的列是标准化为标准正交的，即 $X^{\\top}X = I_{p}$，并且模型中没有截距项。\n\n任务：\n1) 仅从凸优化最优性原理（一个点 $\\beta^{\\star}$ 最小化凸函数 $f$ 当且仅当 $\\mathbf{0} \\in \\partial f(\\beta^{\\star})$）和 $\\ell_{1}$-范数次梯度的定义出发，推导在此设定下刻画 LASSO 问题最优解 $\\beta^{\\star}$ 的显式 Karush-Kuhn-Tucker (KKT) 条件（带有次梯度的平稳性条件）。您的推导必须清楚地说明这些条件如何针对坐标 $j$ 在 $\\beta^{\\star}_{j} \\ne 0$ 和 $\\beta^{\\star}_{j} = 0$ 的情况下进行特化。\n\n2) 使用您推导出的条件，手算以下 $p=3$ 的三特征实例。假设 $X^{\\top}X = I_{3}$ 且\n$$\nX^{\\top}y = \\begin{pmatrix} 1.8 \\\\ -0.5 \\\\ 0.4 \\end{pmatrix}, \\qquad \\lambda = 0.6.\n$$\n计算最优系数向量 $\\beta^{\\star} \\in \\mathbb{R}^{3}$。\n\n以单行向量的形式提供您对 $\\beta^{\\star}$ 的最终数值答案。",
            "solution": "用户提供的问题经评估有效。这是一个来自统计学习领域的适定、有科学依据的问题，其陈述清晰完整。\n\n该问题要求完成与标准正交设计矩阵假设下的 LASSO 目标函数相关的两个任务。\n\n**任务 1：推导 Karush-Kuhn-Tucker (KKT) 条件**\n\n需要最小化的 LASSO 目标函数是：\n$$\nf(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$，$y \\in \\mathbb{R}^{n}$，$\\beta \\in \\mathbb{R}^{p}$，且 $\\lambda \\ge 0$。给定设计矩阵 $X$ 具有标准正交的列，即 $X^{\\top}X = I_{p}$。\n\n目标函数 $f(\\beta)$ 是两个凸函数的和：\n1. $g(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$，它是可微且凸的。\n2. $h(\\beta) = \\lambda \\|\\beta\\|_{1}$，它是凸的，但在 $\\beta$ 的任何分量为零的点处不可微。\n\n函数 $f(\\beta)$ 是凸函数。根据凸优化的基本原理，一个点 $\\beta^{\\star}$ 是 $f(\\beta)$ 的全局最小化点当且仅当零向量是 $f$ 在 $\\beta^{\\star}$ 处的次微分的元素：\n$$\n\\mathbf{0} \\in \\partial f(\\beta^{\\star})\n$$\n由于 $g(\\beta)$ 是可微的，和函数 $f(\\beta) = g(\\beta) + h(\\beta)$ 的次微分是 $g(\\beta)$ 的梯度与 $h(\\beta)$ 的次微分之和：\n$$\n\\partial f(\\beta^{\\star}) = \\nabla g(\\beta^{\\star}) + \\partial h(\\beta^{\\star})\n$$\n首先，我们计算 $g(\\beta)$ 的梯度。我们展开平方范数：\n$$\ng(\\beta) = \\frac{1}{2}(y - X\\beta)^{\\top}(y - X\\beta) = \\frac{1}{2}(y^{\\top}y - 2\\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta)\n$$\n对 $\\beta$ 求梯度：\n$$\n\\nabla g(\\beta) = \\frac{1}{2}(-2X^{\\top}y + 2X^{\\top}X\\beta) = X^{\\top}X\\beta - X^{\\top}y\n$$\n使用给定条件 $X^{\\top}X = I_{p}$，梯度简化为：\n$$\n\\nabla g(\\beta) = \\beta - X^{\\top}y\n$$\n在最优点 $\\beta^{\\star}$ 处，梯度为 $\\nabla g(\\beta^{\\star}) = \\beta^{\\star} - X^{\\top}y$。\n\n接下来，我们刻画 $h(\\beta) = \\lambda \\|\\beta\\|_{1} = \\lambda \\sum_{j=1}^{p} |\\beta_j|$ 的次微分。次微分 $\\partial h(\\beta)$ 是其分量 $\\lambda |\\beta_j|$ 的次微分的笛卡尔积。对于单个分量 $\\beta_j$，$\\lambda|\\beta_j|$ 的次微分是：\n$$\n\\partial (\\lambda |\\beta_j|) = \\begin{cases} \\{\\lambda \\cdot \\text{sgn}(\\beta_j)\\}  \\text{if } \\beta_j \\ne 0 \\\\ [-\\lambda, \\lambda]  \\text{if } \\beta_j = 0 \\end{cases}\n$$\n其中 $\\text{sgn}(\\cdot)$ 是符号函数。所以，$\\partial h(\\beta)$ 由所有分量 $v_j \\in \\partial (\\lambda |\\beta_j|)$ 的向量 $v \\in \\mathbb{R}^{p}$ 组成。\n\n最优性条件 $\\mathbf{0} \\in \\nabla g(\\beta^{\\star}) + \\partial h(\\beta^{\\star})$ 现在可以写成：\n$$\n\\mathbf{0} \\in (\\beta^{\\star} - X^{\\top}y) + \\partial(\\lambda \\|\\beta^{\\star}\\|_{1})\n$$\n这等价于陈述存在一个次梯度向量 $s \\in \\partial(\\|\\beta^{\\star}\\|_1)$ 使得：\n$$\n\\mathbf{0} = (\\beta^{\\star} - X^{\\top}y) + \\lambda s\n$$\n整理后得到平稳性条件：\n$$\nX^{\\top}y - \\beta^{\\star} = \\lambda s\n$$\n我们现在对 $j \\in \\{1, 2, \\dots, p\\}$ 逐分量分析这个条件，令 $c_j = (X^{\\top}y)_j$：\n$$\nc_j - \\beta^{\\star}_j = \\lambda s_j, \\quad \\text{where } s_j \\in \\partial|\\beta^{\\star}_j|\n$$\n\n我们按要求将此条件特化为两种情况。\n\n**情况 1：最优系数非零，$\\beta^{\\star}_j \\ne 0$。**\n在这种情况下，$|\\beta^{\\star}_j|$ 的次梯度是单值的：$s_j = \\text{sgn}(\\beta^{\\star}_j)$。该分量的平稳性条件变为：\n$$\nc_j - \\beta^{\\star}_j = \\lambda \\cdot \\text{sgn}(\\beta^{\\star}_j)\n$$\n求解 $\\beta^{\\star}_j$：\n$$\n\\beta^{\\star}_j = c_j - \\lambda \\cdot \\text{sgn}(\\beta^{\\star}_j)\n$$\n如果 $\\beta^{\\star}_j > 0$，则 $\\text{sgn}(\\beta^{\\star}_j) = 1$，这给出 $\\beta^{\\star}_j = c_j - \\lambda$。一致性要求 $c_j - \\lambda > 0$，因此 $c_j > \\lambda$。\n如果 $\\beta^{\\star}_j  0$，则 $\\text{sgn}(\\beta^{\\star}_j) = -1$，这给出 $\\beta^{\\star}_j = c_j + \\lambda$。一致性要求 $c_j + \\lambda  0$，因此 $c_j  -\\lambda$。\n在这两种子情况下，我们都有 $|c_j| > \\lambda$，并且 $\\beta^{\\star}_j$ 的符号与 $c_j$ 的符号相匹配。因此，我们可以写成 $\\text{sgn}(\\beta^{\\star}_j) = \\text{sgn}(c_j)$。对于非零系数的显式 KKT 条件是：\n$$\nc_j - \\beta^{\\star}_j = \\lambda \\cdot \\text{sgn}(c_j) \\quad \\text{or} \\quad \\beta^{\\star}_j = c_j - \\lambda \\cdot \\text{sgn}(c_j)\n$$\n\n**情况 2：最优系数为零，$\\beta^{\\star}_j = 0$。**\n在这种情况下，次梯度 $s_j$ 可以是区间 $[-1, 1]$ 中的任何值。该分量的平稳性条件是：\n$$\nc_j - 0 = \\lambda s_j \\quad \\text{for some } s_j \\in [-1, 1]\n$$\n这意味着 $c_j = \\lambda s_j$。由于 $s_j$ 的取值范围是 $[-1, 1]$，$c_j$ 必须位于范围 $[-\\lambda, \\lambda]$ 内。对于零系数的显式 KKT 条件是：\n$$\n|c_j| \\le \\lambda\n$$\n\n总而言之，从次梯度最优性准则推导出的 KKT 条件是：\n- 如果 $|(X^{\\top}y)_j| > \\lambda$，则 $\\beta^{\\star}_j = (X^{\\top}y)_j - \\lambda \\cdot \\text{sgn}((X^{\\top}y)_j)$。\n- 如果 $|(X^{\\top}y)_j| \\le \\lambda$，则 $\\beta^{\\star}_j = 0$。\n\n这个解被称为软阈值法，可以紧凑地写为 $\\beta^{\\star}_j = S_{\\lambda}((X^{\\top}y)_j)$，其中 $S_{\\lambda}(z) = \\text{sgn}(z)\\max(0, |z|-\\lambda)$。\n\n**任务 2：求解一个具体实例**\n\n给定一个三特征实例（$p=3$），其中 $X^{\\top}X = I_3$ 且：\n$$\nX^{\\top}y = \\begin{pmatrix} 1.8 \\\\ -0.5 \\\\ 0.4 \\end{pmatrix}, \\qquad \\lambda = 0.6\n$$\n我们应用上面推导出的 KKT 条件来计算最优系数向量 $\\beta^{\\star}$ 的每个分量。令 $c = X^{\\top}y$。\n\n**对于分量 $j=1$：**\n$c_1 = 1.8$。我们将其绝对值与 $\\lambda$ 进行比较：\n$|c_1| = 1.8$。由于 $1.8 > 0.6$，我们处于 $|c_1| > \\lambda$ 的情况。\n最优系数非零，计算如下：\n$$\n\\beta^{\\star}_1 = c_1 - \\lambda \\cdot \\text{sgn}(c_1) = 1.8 - 0.6 \\cdot \\text{sgn}(1.8) = 1.8 - 0.6(1) = 1.2\n$$\n\n**对于分量 $j=2$：**\n$c_2 = -0.5$。我们将其绝对值与 $\\lambda$ 进行比较：\n$|c_2| = 0.5$。由于 $0.5 \\le 0.6$，我们处于 $|c_2| \\le \\lambda$ 的情况。\n最优系数为零：\n$$\n\\beta^{\\star}_2 = 0\n$$\n\n**对于分量 $j=3$：**\n$c_3 = 0.4$。我们将其绝对值与 $\\lambda$ 进行比较：\n$|c_3| = 0.4$。由于 $0.4 \\le 0.6$，我们处于 $|c_3| \\le \\lambda$ 的情况。\n最优系数为零：\n$$\n\\beta^{\\star}_3 = 0\n$$\n\n综合这些结果，最优系数向量是：\n$$\n\\beta^{\\star} = \\begin{pmatrix} 1.2 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n作为一个行向量，它是 $\\begin{pmatrix} 1.2  0  0 \\end{pmatrix}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1.2  0  0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "掌握了Lasso的求解机制后，下一步便是将其应用于真实场景。在实践中，我们面临的关键挑战是如何选择最佳的正则化参数 $\\lambda$，它直接控制着模型的稀疏程度和预测性能。本练习将指导您完成为Lasso模型选择 $\\lambda$ 的黄金标准流程——k折交叉验证。通过模拟一个材料科学的预测问题，您将手动计算不同 $\\lambda$ 值下的交叉验证误差，从而学会如何在模型拟合与模型复杂度之间做出权衡，以获得最佳的泛化能力 ()。",
            "id": "1928609",
            "problem": "一位材料科学家正在开发一个预测模型，用于根据两种关键添加剂 $x_1$ 和 $x_2$ 的浓度来预测一种新型合金的热导率 $y$。所提出的模型是一个形如 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$ 的线性关系。为防止过拟合并选择一个简约模型，该科学家决定使用最小绝对值收缩和选择算子 (LASSO) 回归技术。\n\nLASSO 的核心是调节参数 $\\lambda$，它控制正则化的程度。$\\lambda$ 的最优值将使用3折交叉验证从集合 $\\{0.1, 1.0, 10.0\\}$ 中选择。该科学家收集了以下六个数据点：\n\n| 数据点 | $x_1$ | $x_2$ | $y$ |\n| :--- | :--- | :--- | :--- |\n| 1 | 1.0 | 0.0 | 1.5 |\n| 2 | 0.0 | 1.0 | 2.5 |\n| 3 | 2.0 | 1.0 | 4.0 |\n| 4 | 1.0 | 2.0 | 5.0 |\n| 5 | 3.0 | 2.0 | 6.5 |\n| 6 | 2.0 | 3.0 | 7.5 |\n\n3折交叉验证的设置如下：\n- **第1折**：数据点 1, 2\n- **第2折**：数据点 3, 4\n- **第3折**：数据点 5, 6\n\n在三次交叉验证的每次迭代中，一个 LASSO 模型在其中两折（训练集）上进行训练，然后用于对剩下的一折（验证集）进行预测。每次训练和每个 $\\lambda$ 值得到的模型系数 $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$ 如下所示：\n\n**在第2折和第3折上训练得到的系数（用于在第1折上验证）：**\n- 对于 $\\lambda=0.1$：$\\hat{\\beta}_0=1.35, \\hat{\\beta}_1=0.84, \\hat{\\beta}_2=1.26$\n- 对于 $\\lambda=1.0$：$\\hat{\\beta}_0=2.80, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.90$\n- 对于 $\\lambda=10.0$：$\\hat{\\beta}_0=5.75, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.00$\n\n**在第1折和第3折上训练得到的系数（用于在第2折上验证）：**\n- 对于 $\\lambda=0.1$：$\\hat{\\beta}_0=0.40, \\hat{\\beta}_1=1.33, \\hat{\\beta}_2=1.49$\n- 对于 $\\lambda=1.0$：$\\hat{\\beta}_0=1.13, \\hat{\\beta}_1=0.65, \\hat{\\beta}_2=1.55$\n- 对于 $\\lambda=10.0$：$\\hat{\\beta}_0=4.50, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.00$\n\n**在第1折和第2折上训练得到的系数（用于在第3折上验证）：**\n- 对于 $\\lambda=0.1$：$\\hat{\\beta}_0=0.60, \\hat{\\beta}_1=1.14, \\hat{\\beta}_2=1.56$\n- 对于 $\\lambda=1.0$：$\\hat{\\beta}_0=1.55, \\hat{\\beta}_1=0.40, \\hat{\\beta}_2=1.30$\n- 对于 $\\lambda=10.0$：$\\hat{\\beta}_0=3.25, \\hat{\\beta}_1=0.00, \\hat{\\beta}_2=0.00$\n\n你的任务是计算每个候选 $\\lambda$ 值的交叉验证误差。每一折的误差应使用均方误差 (MSE) 来衡量。给定 $\\lambda$ 的总交叉验证误差是三折的均方误差的平均值。根据你的计算，以下哪个是调节参数 $\\lambda$ 的最优值？\n\nA. $0.1$\n\nB. $1.0$\n\nC. $10.0$\n\nD. 所有 $\\lambda$ 值导致的交叉验证误差都相同。",
            "solution": "我们将响应建模为 $\\hat{y}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{1}+\\hat{\\beta}_{2}x_{2}$。对于每一折和每个 $\\lambda$，我们在验证点上计算残差 $r_{i}=y_{i}-\\hat{y}_{i}$，并将该折的均方误差计算为 $\\text{MSE}=\\frac{1}{n}\\sum r_{i}^{2}$，其中 $n=2$。给定 $\\lambda$ 的交叉验证误差是三折均方误差的平均值。\n\n第1折（在数据点1和2上验证；使用在第2折和第3折上训练得到的系数）：\n\n对于 $\\lambda=0.1$：$(\\hat{\\beta}_{0},\\hat{\\beta}_{1},\\hat{\\beta}_{2})=(1.35,0.84,1.26)$。\n- 数据点 1：$(x_{1},x_{2},y)=(1,0,1.5)$，$\\hat{y}=1.35+0.84\\cdot 1+1.26\\cdot 0=2.19$，$r=1.5-2.19=-0.69$，$r^{2}=0.4761$。\n- 数据点 2：$(0,1,2.5)$，$\\hat{y}=1.35+0.84\\cdot 0+1.26\\cdot 1=2.61$，$r=2.5-2.61=-0.11$，$r^{2}=0.0121$。\n$$\\text{MSE}_{1}(0.1)=\\frac{0.4761+0.0121}{2}=0.2441。$$\n\n对于 $\\lambda=1.0$：$(2.80,0.00,0.90)$。\n- 数据点 1：$\\hat{y}=2.80$，$r=-1.30$，$r^{2}=1.69$。\n- 数据点 2：$\\hat{y}=2.80+0.90=3.70$，$r=-1.20$，$r^{2}=1.44$。\n$$\\text{MSE}_{1}(1.0)=\\frac{1.69+1.44}{2}=1.565。$$\n\n对于 $\\lambda=10.0$：$(5.75,0.00,0.00)$。\n- 数据点 1：$\\hat{y}=5.75$，$r=-4.25$，$r^{2}=18.0625$。\n- 数据点 2：$\\hat{y}=5.75$，$r=-3.25$，$r^{2}=10.5625$。\n$$\\text{MSE}_{1}(10.0)=\\frac{18.0625+10.5625}{2}=14.3125。$$\n\n第2折（在数据点3和4上验证；使用在第1折和第3折上训练得到的系数）：\n\n对于 $\\lambda=0.1$：$(0.40,1.33,1.49)$。\n- 数据点 3：$(2,1,4.0)$，$\\hat{y}=0.40+1.33\\cdot 2+1.49\\cdot 1=4.55$，$r=-0.55$，$r^{2}=0.3025$。\n- 数据点 4：$(1,2,5.0)$，$\\hat{y}=0.40+1.33\\cdot 1+1.49\\cdot 2=4.71$，$r=0.29$，$r^{2}=0.0841$。\n$$\\text{MSE}_{2}(0.1)=\\frac{0.3025+0.0841}{2}=0.1933。$$\n\n对于 $\\lambda=1.0$：$(1.13,0.65,1.55)$。\n- 数据点 3：$\\hat{y}=1.13+0.65\\cdot 2+1.55\\cdot 1=3.98$，$r=0.02$，$r^{2}=0.0004$。\n- 数据点 4：$\\hat{y}=1.13+0.65\\cdot 1+1.55\\cdot 2=4.88$，$r=0.12$，$r^{2}=0.0144$。\n$$\\text{MSE}_{2}(1.0)=\\frac{0.0004+0.0144}{2}=0.0074。$$\n\n对于 $\\lambda=10.0$：$(4.50,0.00,0.00)$。\n- 数据点 3：$\\hat{y}=4.50$，$r=-0.50$，$r^{2}=0.25$。\n- 数据点 4：$\\hat{y}=4.50$，$r=0.50$，$r^{2}=0.25$。\n$$\\text{MSE}_{2}(10.0)=\\frac{0.25+0.25}{2}=0.25。$$\n\n第3折（在数据点5和6上验证；使用在第1折和第2折上训练得到的系数）：\n\n对于 $\\lambda=0.1$：$(0.60,1.14,1.56)$。\n- 数据点 5：$(3,2,6.5)$，$\\hat{y}=0.60+1.14\\cdot 3+1.56\\cdot 2=7.14$，$r=-0.64$，$r^{2}=0.4096$。\n- 数据点 6：$(2,3,7.5)$，$\\hat{y}=0.60+1.14\\cdot 2+1.56\\cdot 3=7.56$，$r=-0.06$，$r^{2}=0.0036$。\n$$\\text{MSE}_{3}(0.1)=\\frac{0.4096+0.0036}{2}=0.2066。$$\n\n对于 $\\lambda=1.0$：$(1.55,0.40,1.30)$。\n- 数据点 5：$\\hat{y}=1.55+0.40\\cdot 3+1.30\\cdot 2=5.35$，$r=1.15$，$r^{2}=1.3225$。\n- 数据点 6：$\\hat{y}=1.55+0.40\\cdot 2+1.30\\cdot 3=6.25$，$r=1.25$，$r^{2}=1.5625$。\n$$\\text{MSE}_{3}(1.0)=\\frac{1.3225+1.5625}{2}=1.4425。$$\n\n对于 $\\lambda=10.0$：$(3.25,0.00,0.00)$。\n- 数据点 5：$\\hat{y}=3.25$，$r=3.25$，$r^{2}=10.5625$。\n- 数据点 6：$\\hat{y}=3.25$，$r=4.25$，$r^{2}=18.0625$。\n$$\\text{MSE}_{3}(10.0)=\\frac{10.5625+18.0625}{2}=14.3125。$$\n\n计算每个 $\\lambda$ 的平均交叉验证误差：\n$$\\text{CV}(0.1)=\\frac{0.2441+0.1933+0.2066}{3}=0.214666\\ldots,$$\n$$\\text{CV}(1.0)=\\frac{1.565+0.0074+1.4425}{3}=1.004966\\ldots,$$\n$$\\text{CV}(10.0)=\\frac{14.3125+0.25+14.3125}{3}=9.625.$$\n\n最小的交叉验证误差出现在 $\\lambda=0.1$ 时，因此最优选择是选项A。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "构建Lasso模型后，理解其在特定数据条件下的行为至关重要，尤其是在处理高度相关的特征时——这在现实数据中屡见不鲜。本练习通过一个思想实验，将Lasso ($L_1$ 惩罚) 与其“兄弟”模型Ridge回归 ($L_2$ 惩罚) 在完全共线性的极端情况下进行对比。通过分析这两种方法如何分配相关特征的系数，您将深刻理解Lasso进行变量选择的一个关键特性：它倾向于从一组强相关的变量中任意选择一个，而这一行为对模型解释的稳定性有着深远的影响 ()。",
            "id": "3184381",
            "problem": "给定一个线性回归问题，有 $n$ 个观测值和 $p=2$ 个预测变量，收集在设计矩阵 $X=[x_1\\; x_2]$ 中。假设预测变量是完全共线的，具体来说 $x_2=x_1$，并且两列都经过标准化，使其样本均值为 $0$ 且其欧几里得范数的平方为 $\\|x_j\\|_2^2=n$（对于 $j\\in\\{1,2\\}$）。响应向量 $y$ 经过中心化，使其样本均值为 $0$。考虑两种正则化最小二乘估计量，它们通过在 $(\\beta_1,\\beta_2)\\in\\mathbb{R}^2$ 上最小化以下目标函数来定义：\n- 岭回归 (Ridge regression)：平方误差加上一个 $L_2$ 惩罚项，调整参数为 $\\lambda0$。\n- 最小绝对收缩和选择算子 (LASSO)：平方误差加上一个 $L_1$ 惩罚项，调整参数为 $\\lambda0$。\n\n仅根据这些定义以及凸优化和几何学的基本原理，在 $x_2=x_1$ 的情况下，推断解的结构、唯一性及其影响。下列哪些陈述是正确的？选择所有适用项。\n\nA. 当 $x_2=x_1$ 时，岭回归产生一个唯一解，其中 $\\beta_1=\\beta_2$。相比之下，LASSO 允许存在一组非唯一的最优解；在这些解中，存在形如 $(\\beta_1,\\beta_2)=(\\hat{\\theta},0)$ 或 $(0,\\hat{\\theta})$ 的最优解，其中 $\\hat{\\theta}$ 是简化问题中 $\\theta=\\beta_1+\\beta_2$ 的最优值。\n\nB. 因为预测变量是相同的，LASSO 在最优点必然设置 $\\beta_1=\\beta_2$ 以最小化 $L_1$ 惩罚项。\n\nC. 当 $x_2=x_1$ 时，岭回归和 LASSO 的拟合值都仅依赖于 $\\theta=\\beta_1+\\beta_2$。因此，任何保持 $\\theta$ 不变的 $(\\beta_1,\\beta_2)$ 的重新分配都不会改变预测结果；岭回归唯一地选择相等的分配，而 LASSO 可能会选择一个稀疏的分配。\n\nD. 在完全共线性的情况下，岭估计量是唯一的，因为对于任何 $\\lambda0$，$X^\\top X+\\lambda I$ 都是可逆的，而 LASSO 估计量可能不是唯一的。因此，使用 LASSO 进行的变量选择在重复的预测变量之间可能不稳定，即使预测结果是稳定的。",
            "solution": "用户提供了一个关于岭回归和 LASSO 回归估计量在完全共线性下的行为的问题陈述。\n\n### 第 1 步：提取已知条件\n- 一个线性回归问题，有 $n$ 个观测值和 $p=2$ 个预测变量。\n- 设计矩阵为 $X=[x_1\\; x_2]$。\n- 预测变量完全共线：$x_2=x_1$。\n- 两个预测变量列都经过标准化，使其样本均值为 $0$，且欧几里得范数的平方为 $\\|x_j\\|_2^2=n$（对于 $j\\in\\{1,2\\}$）。\n- 响应向量 $y$ 经过中心化，使其样本均值为 $0$。\n- 岭回归估计量通过在 $(\\beta_1,\\beta_2)\\in\\mathbb{R}^2$ 上最小化目标函数 $\\sum_{i=1}^n (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2 + \\lambda (\\beta_1^2 + \\beta_2^2)$ 来定义，调整参数为 $\\lambda0$。\n- LASSO 估计量通过在 $(\\beta_1,\\beta_2)\\in\\mathbb{R}^2$ 上最小化目标函数 $\\sum_{i=1}^n (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2 + \\lambda (|\\beta_1| + |\\beta_2|)$ 来定义，调整参数为 $\\lambda0$。\n\n### 第 2 步：使用提取的已知条件进行验证\n问题陈述具有科学依据、提法恰当且客观。它提出了高维统计学和统计学习中的一个标准理论问题，用于阐明 $L_1$ 和 $L_2$ 正则化之间的根本区别。问题设定是自洽的，提供了所有必要的条件。条件 $x_2=x_1$ 和对 $j=1,2$ 均有 $\\|x_j\\|_2^2=n$ 是相互一致的。该问题是分析正则化方法的一个经典的、非平凡的场景。它没有违反任何科学原理，不是不完整的或矛盾的，也不是病态的（ill-posed）。\n\n### 第 3 步：结论和行动\n问题陈述是 **有效的**。我们可以继续进行解答。\n\n### 解结构的推导\n\n设系数向量为 $\\beta = (\\beta_1, \\beta_2)^\\top$。目标函数的残差平方和 (RSS) 部分是：\n$$RSS(\\beta) = \\|y - X\\beta\\|_2^2 = \\sum_{i=1}^n (y_i - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2$$\n考虑到完全共线性 $x_2 = x_1$，项 $X\\beta$ 变为：\n$$X\\beta = x_1\\beta_1 + x_2\\beta_2 = x_1\\beta_1 + x_1\\beta_2 = x_1(\\beta_1 + \\beta_2)$$\n我们定义一个新参数 $\\theta = \\beta_1 + \\beta_2$。RSS 可以纯粹表示为 $\\theta$ 的函数：\n$$RSS(\\theta) = \\|y - x_1\\theta\\|_2^2 = \\sum_{i=1}^n (y_i - x_{i1}\\theta)^2$$\n这表明，对于任何给定的 $\\theta$ 值，所有和为 $\\theta$ 的 $(\\beta_1, \\beta_2)$ 对都会产生完全相同的平方误差和。在 $(\\beta_1, \\beta_2)$ 参数空间中，RSS 函数的等值线是形如 $\\beta_1 + \\beta_2 = c$ 的平行线（其中 $c$ 为某个常数）。\n\n岭回归和 LASSO 这两种估计量是通过最小化这个 RSS 项和一个惩罚项的和来找到的。\n- 岭回归目标函数：$J_{ridge}(\\beta_1, \\beta_2) = RSS(\\beta_1+\\beta_2) + \\lambda(\\beta_1^2 + \\beta_2^2)$\n- LASSO 目标函数：$J_{LASSO}(\\beta_1, \\beta_2) = RSS(\\beta_1+\\beta_2) + \\lambda(|\\beta_1| + |\\beta_2|)$\n\n为了找到最优系数，我们可以把这看作一个两阶段过程。首先，对于一个固定的和 $\\theta = \\beta_1+\\beta_2$，我们找到最小化惩罚项的分配 $(\\beta_1, \\beta_2)$。然后，我们找到全局最优的 $\\hat\\theta$。\n\n**岭回归分析：**\n对于一个固定的和 $\\theta = \\beta_1+\\beta_2$，我们需要最小化惩罚项 $P_2(\\beta_1,\\beta_2) = \\beta_1^2 + \\beta_2^2$。这等价于在线 $\\beta_1+\\beta_2=\\theta$ 上找到离原点最近的点。这个点可以通过将原点投影到该直线上找到，得到 $\\beta_1 = \\beta_2 = \\theta/2$。\n因此，岭回归的目标函数可以简化为仅关于 $\\theta$ 的函数：\n$$J_{ridge}(\\theta) = \\|y - x_1\\theta\\|_2^2 + \\lambda \\left( (\\theta/2)^2 + (\\theta/2)^2 \\right) = \\|y - x_1\\theta\\|_2^2 + \\frac{\\lambda}{2}\\theta^2$$\n这个目标函数是两个凸函数的和，其中之一 ($\\frac{\\lambda}{2}\\theta^2$) 在 $\\lambda  0$ 时是严格凸的。因此，$J_{ridge}(\\theta)$ 在 $\\theta$ 上是严格凸的，并且有唯一的最小化子 $\\hat\\theta$。唯一的岭回归解就是 $(\\hat\\beta_1, \\hat\\beta_2) = (\\hat\\theta/2, \\hat\\theta/2)$。所以，岭回归产生一个唯一的解，其中相同预测变量的系数相等。\n\n**LASSO 分析：**\n对于一个固定的和 $\\theta = \\beta_1+\\beta_2$，我们需要最小化惩罚项 $P_1(\\beta_1,\\beta_2) = |\\beta_1| + |\\beta_2|$。\n根据三角不等式，有 $|\\theta| = |\\beta_1 + \\beta_2| \\leq |\\beta_1| + |\\beta_2|$。等号成立的充要条件是 $\\beta_1$ 和 $\\beta_2$ 同号（或者其中一个或两个为零），即 $\\beta_1\\beta_2 \\ge 0$。\n所以，对于一个固定的 $\\theta$，惩罚项的最小值是 $|\\theta|$。这个最小值可以通过任何满足 $\\beta_1 + \\beta_2 = \\theta$ 和 $\\beta_1\\beta_2 \\ge 0$ 的 $(\\beta_1, \\beta_2)$ 对来达到。这组点在 $(\\beta_1, \\beta_2)$ 平面中构成了连接点 $(\\theta, 0)$ 和点 $(0, \\theta)$ 的线段。\nLASSO 目标函数可以简化为关于 $\\theta$ 的函数：\n$$J_{LASSO}(\\theta) = \\|y - x_1\\theta\\|_2^2 + \\lambda|\\theta|$$\n这是只有一个预测变量 $x_1$ 的 LASSO 问题的目标函数。这个函数是严格凸的（因为它是一个二次函数和一个凸函数 $|\\cdot|$ 的和），所以它有一个唯一的最小化子，我们记为 $\\hat\\theta$。\n因此，LASSO 的最优解集 $(\\hat\\beta_1, \\hat\\beta_2)$ 不是一个单点（除非 $\\hat\\theta=0$），而是所有满足以下条件的点对的集合：\n1. $\\hat\\beta_1 + \\hat\\beta_2 = \\hat\\theta$\n2. $\\hat\\beta_1\\hat\\beta_2 \\ge 0$\n这对应于连接 $(\\hat\\theta, 0)$ 和 $(0, \\hat\\theta)$ 的线段。因此，LASSO 解通常不是唯一的。该线段上的任何点都是一个最优解，包括稀疏解 $(\\hat\\theta, 0)$ 和 $(0, \\hat\\theta)$，以及非稀疏解 $(\\hat\\theta/2, \\hat\\theta/2)$。\n\n### 选项评估\n\n**A. 当 $x_2=x_1$ 时，岭回归产生一个唯一解，其中 $\\beta_1=\\beta_2$。相比之下，LASSO 允许存在一组非唯一的最优解；在这些解中，存在形如 $(\\beta_1,\\beta_2)=(\\hat{\\theta},0)$ 或 $(0,\\hat{\\theta})$ 的最优解，其中 $\\hat{\\theta}$ 是简化问题中 $\\theta=\\beta_1+\\beta_2$ 的最优值。**\n这个陈述完美地总结了我们的发现。我们的分析表明，岭回归有一个唯一解，满足 $\\hat\\beta_1=\\hat\\beta_2=\\hat\\theta/2$。分析也表明，LASSO 的解是连接 $(\\hat\\theta, 0)$ 和 $(0, \\hat\\theta)$ 的线段上的点集，这是一个非唯一的集合（除非 $\\hat\\theta = 0$），并且明确包含所提到的稀疏解。\n**结论：正确。**\n\n**B. 因为预测变量是相同的，LASSO 在最优点必然设置 $\\beta_1=\\beta_2$ 以最小化 $L_1$ 惩罚项。**\n这是不正确的。虽然 $(\\hat\\theta/2, \\hat\\theta/2)$ 是一个可能的 LASSO 解，但它不是唯一的。对于一个固定的和 $\\theta$，任何和为 $\\theta$ 的非负分配 $\\beta_1, \\beta_2$（假设 $\\theta0$）都能最小化 $L_1$ 惩罚项 $|\\beta_1|+|\\beta_2|$。例如，如果 $\\hat\\theta = 2$，对于 $(\\beta_1, \\beta_2)=(1,1)$ 和 $(\\beta_1, \\beta_2)=(2,0)$，惩罚项都是 $|\\beta_1|+|\\beta_2|=2$。与岭回归惩罚项不同，LASSO 惩罚项并不唯一地偏好相等的分配。\n**结论：不正确。**\n\n**C. 当 $x_2=x_1$ 时，岭回归和 LASSO 的拟合值都仅依赖于 $\\theta=\\beta_1+\\beta_2$。因此，任何保持 $\\theta$ 不变的 $(\\beta_1,\\beta_2)$ 的重新分配都不会改变预测结果；岭回归唯一地选择相等的分配，而 LASSO 可能会选择一个稀疏的分配。**\n拟合值为 $\\hat{y} = X\\hat\\beta = x_1\\hat\\beta_1 + x_2\\hat\\beta_2 = x_1(\\hat\\beta_1+\\hat\\beta_2) = x_1\\hat\\theta$。因此，预测只依赖于和 $\\hat\\theta$。这部分是正确的。\n因此，任何和为最优值 $\\hat\\theta$ 的 $(\\hat\\beta_1, \\hat\\beta_2)$ 对都会产生相同的预测。这部分是正确的。\n我们的分析表明，岭回归强制要求唯一的分配 $\\hat\\beta_1 = \\hat\\beta_2 = \\hat\\theta/2$。这部分是正确的。\n我们的分析也表明，LASSO 的解集包括像 $(\\hat\\theta, 0)$ 这样的稀疏解。“可能会选择”这个措辞是准确的，因为这是一个有效的最优解。这部分是正确的。\n因此，整个陈述是正确的。\n**结论：正确。**\n\n**D. 在完全共线性的情况下，岭估计量是唯一的，因为对于任何 $\\lambda0$，$X^\\top X+\\lambda I$ 都是可逆的，而 LASSO 估计量可能不是唯一的。因此，使用 LASSO 进行的变量选择在重复的预测变量之间可能不稳定，即使预测结果是稳定的。**\n矩阵 $X^\\top X$ 是 $\\begin{pmatrix} x_1^\\top x_1  x_1^\\top x_1 \\\\ x_1^\\top x_1  x_1^\\top x_1 \\end{pmatrix} = \\begin{pmatrix} n  n \\\\ n  n \\end{pmatrix}$，这是奇异的。\n岭回归解的矩阵是 $X^\\top X + \\lambda I = \\begin{pmatrix} n+\\lambda  n \\\\ n  n+\\lambda \\end{pmatrix}$。它的行列式是 $(n+\\lambda)^2 - n^2 = \\lambda(2n+\\lambda)$。因为 $\\lambda0$ 且 $n \\ge 1$，这个行列式是严格为正的，所以该矩阵是可逆的。这保证了唯一的解 $\\hat\\beta_{ridge} = (X^\\top X + \\lambda I)^{-1} X^\\top y$。这部分是正确的。\n正如我们已经确定的，LASSO 解通常不是唯一的。这部分是正确的。\n所描述的后果是一个关键的见解。因为解线段上的任何点都是有效的，求解器可能会返回 $(\\hat\\theta, 0)$ 或 $(0, \\hat\\theta)$，这取决于实现细节或当共线性不完全是完美时数据的微小变化。这使得变量选择（“选择” $x_1$ 还是 $x_2$）不稳定。然而，由于所有解 $(\\hat\\beta_1, \\hat\\beta_2)$ 的和都是唯一的 $\\hat\\theta$，预测 $\\hat{y} = x_1\\hat\\theta$ 是稳定且唯一的。这部分也是正确的。\n**结论：正确。**",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}