{
    "hands_on_practices": [
        {
            "introduction": "选择“最佳”模型并非一个绝对的概念，它完全取决于我们用来衡量性能的标尺。本练习将通过三种不同的指标——Top-$k$ 准确率、F1 分数以及 AUROC——来评估三个分类器，从而阐明这一基本原则。你将亲身体会到，根据我们是更关心整体的预测正确性、某一特定类别的精确率与召回率的平衡，还是模型区分正负样本的排序能力，最终选出的“最佳”模型可能会截然不同。",
            "id": "3107729",
            "problem": "考虑一个多类别分类问题，其中有 $3$ 个类别，标记为 $0$、$1$ 和 $2$。现有 $N=8$ 个样本，其真实类别标签由向量 $y=(0,1,2,1,0,2,1,0)$ 给出。我们评估三个候选模型 $\\mathcal{M}_0$、$\\mathcal{M}_1$ 和 $\\mathcal{M}_2$，每个模型对每个样本输出一个跨类别的非负分数向量，用于排序。对于每个模型 $\\mathcal{M}_m$，令 $S^{(m)} \\in \\mathbb{R}^{8 \\times 3}$ 表示其分数矩阵，其中 $S^{(m)}_{i,c}$ 是模型 $m$ 为样本 $i \\in \\{0,1,\\dots,7\\}$ 的类别 $c \\in \\{0,1,2\\}$ 所分配的分数。分数矩阵如下：\n\n- 对于 $\\mathcal{M}_0$：各行为 $(0.45,0.40,0.15)$、$(0.10,0.80,0.10)$、$(0.30,0.20,0.50)$、$(0.25,0.60,0.15)$、$(0.30,0.65,0.05)$、$(0.25,0.25,0.50)$、$(0.20,0.75,0.05)$、$(0.55,0.25,0.20)$。\n- 对于 $\\mathcal{M}_1$：各行为 $(0.35,0.55,0.10)$、$(0.30,0.50,0.20)$、$(0.20,0.15,0.65)$、$(0.15,0.70,0.15)$、$(0.60,0.25,0.15)$、$(0.10,0.20,0.70)$、$(0.05,0.85,0.10)$、$(0.65,0.20,0.15)$。\n- 对于 $\\mathcal{M}_2$：各行为 $(0.30,0.35,0.35)$、$(0.05,0.92,0.03)$、$(0.25,0.40,0.35)$、$(0.10,0.85,0.05)$、$(0.20,0.55,0.25)$、$(0.15,0.35,0.50)$、$(0.02,0.95,0.03)$、$(0.40,0.45,0.15)$。\n\n我们定义以下基于核心定义的评估指标：\n\n1. Top-$k$ 准确率：对于给定的 $k \\in \\{1,2,3\\}$，将样本 $i$ 的 top-$k$ 集合定义为根据下述平局打破规则得分最高的 $k$ 个类别的集合。模型的 top-$k$ 准确率是真实类别 $y_i$ 位于此 top-$k$ 集合内的样本比例。类别排序的平局打破规则是：首先按分数降序对类别排序，如果分数出现平局，则按类别索引升序对平局的类别排序。\n\n2. F1 分数 (F1)：对于类别 $1$ 与其余类别的二元评估，我们将预测的正样本集合定义为那些类别 $1$ 分数高于某个阈值的样本。对于任意阈值，在类别 $1$ 的一对多（one-versus-rest）意义下定义真正例（$\\mathrm{TP}$）、假正例（$\\mathrm{FP}$）和假反例（$\\mathrm{FN}$）。精确率为 $\\mathrm{Precision} = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FP})$，召回率为 $\\mathrm{Recall} = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FN})$，F1 分数为 $F1 = 2 \\cdot \\mathrm{Precision} \\cdot \\mathrm{Recall} / (\\mathrm{Precision}+\\mathrm{Recall})$，等价于 $F1 = \\dfrac{2 \\mathrm{TP}}{2 \\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN}}$。一个用例约束对预测的正样本施加了资源预算：令 $b \\in (0,1]$ 是允许被标记为正样本的样本比例，因此预测的正样本数最多为 $\\lfloor b \\cdot N \\rfloor$。在此约束下，为在所有允许的阈值上最大化 $F1$ 而设定的最优阈值将根据类别 $1$ 的分数选择前 $m$ 个样本，其中 $m \\in \\{0,1,\\dots,\\lfloor bN \\rfloor\\}$，当分数相等时，遵循相同的索引平局打破规则。\n\n3. 受试者工作特征曲线下面积 (AUROC)：对于类别 $1$ 与其余类别的二元评估，受试者工作特征（ROC）曲线绘制了在类别 $1$ 分数的所有阈值下，真正例率（$\\mathrm{TPR} = \\mathrm{TP}/(\\mathrm{TP}+\\mathrm{FN})$）与假正例率（$\\mathrm{FPR} = \\mathrm{FP}/(\\mathrm{FP}+\\mathrm{TN})$）的关系图。受试者工作特征曲线下面积（AUROC）是该曲线下的面积，它可以通过 Mann–Whitney 统计量等价计算，即一个随机选择的正样本比一个随机选择的负样本具有更高类别 $1$ 分数的概率，平局计为二分之一。\n\n模型选择规则：对于给定的指标，所选模型的索引是 $\\{\\mathcal{M}_0,\\mathcal{M}_1,\\mathcal{M}_2\\}$ 中实现该指标最大值的最小索引 $m \\in \\{0,1,2\\}$，平局则通过选择最小索引来打破。\n\n您的程序必须为以下套件中的每个测试用例计算：\n- 最大化 top-$k$ 准确率的模型的索引。\n- 在预算约束 $b$ 下，最大化类别 $1$ 的 $F1$ 分数的模型的索引。\n- 最大化类别 $1$ 的 AUROC 的模型的索引。\n\n测试套件：\n- 测试用例 1：$k=1$，$b=0.5$。\n- 测试用例 2：$k=2$，$b=0.25$。\n- 测试用例 3：$k=2$，$b=0.125$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果本身也是一个方括号括起来的逗号分隔的三个整数列表，表示 $[$top-$k$ 模型索引, $b$ 约束下的 F1 模型索引, AUROC 模型索引$]$。例如，输出应类似于 $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$，行内任何地方都没有空格。",
            "solution": "该问题是有效的，因为它在科学上基于统计学习的原理，问题提出得很好，包含了所有必要的数据和明确的定义，并且其表述是客观的。我们将进行完整的解答。\n\n该问题要求我们评估三个模型 $\\mathcal{M}_0$、$\\mathcal{M}_1$ 和 $\\mathcal{M}_2$ 在一个有 $N=8$ 个样本和 $3$ 个类别的多类别分类任务上的表现。真实标签由向量 $y=(0,1,2,1,0,2,1,0)$ 给出。对于每个模型，我们都得到了一个 $8 \\times 3$ 的分数矩阵 $S^{(m)}$。我们将为每个模型计算三个指标：top-$k$ 准确率、类别 $1$ 的预算内 F1 分数，以及类别 $1$ 的受试者工作特征曲线下面积 (AUROC)。对于每个指标，我们选择得分最高的模型，并通过选择最小的模型索引 $m \\in \\{0,1,2\\}$ 来打破平局。\n\n让我们将模型集合表示为 $\\{\\mathcal{M}_0, \\mathcal{M}_1, \\mathcal{M}_2\\}$。分数矩阵 $S^{(0)}$、$S^{(1)}$、$S^{(2)}$ 和标签向量 $y$ 是主要输入。\n\n**1. Top-$k$ 准确率计算**\n\nTop-$k$ 准确率是指真实类别标签 $y_i$ 位于得分最高的 $k$ 个类别中的样本所占的比例。样本 $i$ 的类别排序是通过按分数降序排序来确定的，平局则按类别索引升序打破。\n\n令 $S^{(m)}_{i, \\cdot}$ 为模型 $\\mathcal{M}_m$ 对样本 $i$ 的分数向量。我们为 $c \\in \\{0,1,2\\}$ 构建配对 $(S^{(m)}_{i,c}, c)$。这些配对首先按分数（降序）排序，然后按类别索引 $c$（升序）排序。预测的 top-$k$ 类别集合 $\\hat{Y}_{i,k}^{(m)}$ 是此排序列表中的前 $k$ 个类别索引的集合。\n\n模型 $\\mathcal{M}_m$ 的 top-$k$ 准确率由下式给出：\n$$ \\text{Accuracy}_k(\\mathcal{M}_m) = \\frac{1}{N} \\sum_{i=0}^{N-1} I(y_i \\in \\hat{Y}_{i,k}^{(m)}) $$\n其中 $I(\\cdot)$ 是指示函数。\n\n**2. 类别 1 的预算内 F1 分数**\n\n该指标评估模型识别类别 $1$ 样本（正样本）与类别 $0$ 和 $2$ 样本（负样本）的能力。真正样本集为 $P = \\{i | y_i = 1\\} = \\{1, 3, 6\\}$，因此正样本总数为 $N_P = 3$。真负样本集为 $N_{set} = \\{i | y_i \\neq 1\\} = \\{0, 2, 4, 5, 7\\}$，其中 $N_N = 5$。\n\n预算约束将可预测为正样本的样本数量限制为最多 $m_{max} = \\lfloor b \\cdot N \\rfloor$。为了最大化 F1 分数，我们必须找到预测正样本的最优数量 $m^*$，其中 $0 \\le m^* \\le m_{max}$。对于每个可能的预测数量 $m_{pred} \\in \\{0, 1, \\dots, m_{max}\\}$，我们根据它们的类别 $1$ 分数 $S^{(m)}_{\\cdot, 1}$ 选择前 $m_{pred}$ 个样本。样本的排序是通过按其类别 $1$ 分数降序排列确定的，平局则按样本索引 $i$ 升序打破。\n\n对于每个 $m_{pred}$，我们计算：\n- $\\mathrm{TP}(m_{pred})$：前 $m_{pred}$ 个样本中的真正例数。\n- $\\mathrm{FP}(m_{pred})$：前 $m_{pred}$ 个样本中的假正例数。\n- $\\mathrm{FN}(m_{pred}) = N_P - \\mathrm{TP}(m_{pred})$。\n\n给定 $m_{pred}$ 的 F1 分数为：\n$$ F1(m_{pred}) = \\frac{2 \\cdot \\mathrm{TP}(m_{pred})}{2 \\cdot \\mathrm{TP}(m_{pred}) + \\mathrm{FP}(m_{pred}) + \\mathrm{FN}(m_{pred})} $$\n模型 $\\mathcal{M}_m$ 在预算 $b$ 下的 F1 分数是这些值的最大值：\n$$ F1_b(\\mathcal{M}_m) = \\max_{m_{pred}=0, \\dots, m_{max}} F1(m_{pred}) $$\n\n**3. 类别 1 的 AUROC**\n\n类别 $1$ 与其余类别二元任务的 AUROC 可以使用 Mann-Whitney U 统计量计算。它衡量的是一个随机选择的正样本比一个随机选择的负样本具有更高类别 $1$ 分数的概率。\n\n令 $S_P^{(m)} = \\{S_{i,1}^{(m)} | i \\in P\\}$ 为正样本的类别 $1$ 分数集合，$S_N^{(m)} = \\{S_{j,1}^{(m)} | j \\in N_{set}\\}$ 为负样本的分数集合。模型 $\\mathcal{M}_m$ 的 AUROC 为：\n$$ \\text{AUROC}(\\mathcal{M}_m) = \\frac{1}{N_P \\cdot N_N} \\sum_{s_p \\in S_P^{(m)}} \\sum_{s_n \\in S_N^{(m)}} \\left( I(s_p  s_n) + 0.5 \\cdot I(s_p = s_n) \\right) $$\n\n**模型选择**\n对于每个测试用例和每个指标，我们计算所有三个模型 $\\{\\mathcal{M}_0, \\mathcal{M}_1, \\mathcal{M}_2\\}$ 的指标值。令计算出的值为 $\\{v_0, v_1, v_2\\}$。最大值为 $v_{max} = \\max(v_0, v_1, v_2)$。所选模型的索引是 $\\arg \\min \\{m | v_m = v_{max}\\}$。\n\n**测试用例计算**\n\n让我们计算每个测试用例的结果。\n\n**测试用例 1：$k=1, b=0.5$**\n- $N=8$，因此对于 F1 分数，$m_{max} = \\lfloor 0.5 \\times 8 \\rfloor = 4$。\n- **Top-1 准确率：**\n  - $\\text{Acc}_1(\\mathcal{M}_0) = 7/8 = 0.875$\n  - $\\text{Acc}_1(\\mathcal{M}_1) = 7/8 = 0.875$\n  - $\\text{Acc}_1(\\mathcal{M}_2) = 4/8 = 0.5$\n  - 最大值为 $0.875$，在 $\\mathcal{M}_0$ 和 $\\mathcal{M}_1$ 之间平局。我们选择索引 $0$。\n- **F1 分数 ($b=0.5$)：**\n  - 对于 $\\mathcal{M}_0$，最大 F1 分数为 $6/7 \\approx 0.857$ (在 $m_{pred}=4$ 时)。\n  - 对于 $\\mathcal{M}_1$，最大 F1 分数为 $6/7 \\approx 0.857$ (在 $m_{pred}=4$ 时)。\n  - 对于 $\\mathcal{M}_2$，最大 F1 分数为 $1.0$ (在 $m_{pred}=3$ 时，其中 $\\mathrm{TP}=3, \\mathrm{FP}=0$)。\n  - 最大值为 $1.0$，由 $\\mathcal{M}_2$ 达到。我们选择索引 $2$。\n- **AUROC：**\n  - $\\text{AUROC}(\\mathcal{M}_0) = 14/15 \\approx 0.9333$\n  - $\\text{AUROC}(\\mathcal{M}_1) = 14/15 \\approx 0.9333$\n  - $\\text{AUROC}(\\mathcal{M}_2) = 15/15 = 1.0$\n  - 最大值为 $1.0$，由 $\\mathcal{M}_2$ 达到。我们选择索引 $2$。\n- **测试用例 1 的结果：** $[0, 2, 2]$\n\n**测试用例 2：$k=2, b=0.25$**\n- 对于 F1 分数，$m_{max} = \\lfloor 0.25 \\times 8 \\rfloor = 2$。\n- **Top-2 准确率：**\n  - $\\text{Acc}_2(\\mathcal{M}_0) = 8/8 = 1.0$\n  - $\\text{Acc}_2(\\mathcal{M}_1) = 8/8 = 1.0$\n  - $\\text{Acc}_2(\\mathcal{M}_2) = 6/8 = 0.75$\n  - 最大值为 $1.0$，在 $\\mathcal{M}_0$ 和 $\\mathcal{M}_1$ 之间平局。我们选择索引 $0$。\n- **F1 分数 ($b=0.25$)：**\n  - 对于 $\\mathcal{M}_0$，最大 F1 分数为 $0.8$ (在 $m_{pred}=2$ 时，$\\mathrm{TP}=2, \\mathrm{FP}=0$)。\n  - 对于 $\\mathcal{M}_1$，最大 F1 分数为 $0.8$ (在 $m_{pred}=2$ 时，$\\mathrm{TP}=2, \\mathrm{FP}=0$)。\n  - 对于 $\\mathcal{M}_2$，最大 F1 分数为 $0.8$ (在 $m_{pred}=2$ 时，$\\mathrm{TP}=2, \\mathrm{FP}=0$)。\n  - 所有模型得分均为 $0.8$。我们选择最小的索引 $0$。\n- **AUROC：** (此项与 $k$ 和 $b$ 无关)\n  - 数值与测试用例 1 相同。\n  - 最大值为 $1.0$，由 $\\mathcal{M}_2$ 达到。我们选择索引 $2$。\n- **测试用例 2 的结果：** $[0, 0, 2]$\n\n**测试用例 3：$k=2, b=0.125$**\n- 对于 F1 分数，$m_{max} = \\lfloor 0.125 \\times 8 \\rfloor = 1$。\n- **Top-2 准确率：** (此项与 $b$ 无关)\n  - 数值与测试用例 2 相同。\n  - 最大值为 $1.0$，在 $\\mathcal{M}_0$ 和 $\\mathcal{M}_1$ 之间平局。我们选择索引 $0$。\n- **F1 分数 ($b=0.125$)：**\n  - 对于 $\\mathcal{M}_0$，最大 F1 分数为 $0.5$ (在 $m_{pred}=1$ 时，$\\mathrm{TP}=1, \\mathrm{FP}=0$)。\n  - 对于 $\\mathcal{M}_1$，最大 F1 分数为 $0.5$ (在 $m_{pred}=1$ 时，$\\mathrm{TP}=1, \\mathrm{FP}=0$)。\n  - 对于 $\\mathcal{M}_2$，最大 F1 分数为 $0.5$ (在 $m_{pred}=1$ 时，$\\mathrm{TP}=1, \\mathrm{FP}=0$)。\n  - 所有模型得分均为 $0.5$。我们选择最小的索引 $0$。\n- **AUROC：** (此项与 $k$ 和 $b$ 无关)\n  - 数值与测试用例 1 相同。\n  - 最大值为 $1.0$，由 $\\mathcal{M}_2$ 达到。我们选择索引 $2$。\n- **测试用例 3 的结果：** $[0, 0, 2]$\n\n程序将系统地实现这些计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model evaluation problem by calculating three metrics for three models\n    across three test cases and selecting the best model for each.\n    \"\"\"\n    # Define problem givens\n    y_true = np.array([0, 1, 2, 1, 0, 2, 1, 0])\n    num_samples = len(y_true)\n    num_classes = 3\n\n    scores = [\n        np.array([\n            [0.45, 0.40, 0.15], [0.10, 0.80, 0.10], [0.30, 0.20, 0.50],\n            [0.25, 0.60, 0.15], [0.30, 0.65, 0.05], [0.25, 0.25, 0.50],\n            [0.20, 0.75, 0.05], [0.55, 0.25, 0.20]\n        ]),\n        np.array([\n            [0.35, 0.55, 0.10], [0.30, 0.50, 0.20], [0.20, 0.15, 0.65],\n            [0.15, 0.70, 0.15], [0.60, 0.25, 0.15], [0.10, 0.20, 0.70],\n            [0.05, 0.85, 0.10], [0.65, 0.20, 0.15]\n        ]),\n        np.array([\n            [0.30, 0.35, 0.35], [0.05, 0.92, 0.03], [0.25, 0.40, 0.35],\n            [0.10, 0.85, 0.05], [0.20, 0.55, 0.25], [0.15, 0.35, 0.50],\n            [0.02, 0.95, 0.03], [0.40, 0.45, 0.15]\n        ])\n    ]\n\n    test_cases = [\n        {'k': 1, 'b': 0.5},\n        {'k': 2, 'b': 0.25},\n        {'k': 2, 'b': 0.125}\n    ]\n\n    def select_best_model(metric_scores):\n        \"\"\"Selects model index with max score, breaking ties by smallest index.\"\"\"\n        max_score = np.max(metric_scores)\n        best_model_idx = np.where(metric_scores == max_score)[0][0]\n        return best_model_idx\n\n    # --- Metric Calculation Functions ---\n\n    def calculate_top_k_accuracy(model_scores, y_true, k, num_classes):\n        correct_predictions = 0\n        class_indices = np.arange(num_classes)\n        for i in range(len(y_true)):\n            sample_scores = model_scores[i]\n            # Tie-breaking: sort by score desc, then class index asc\n            sorted_indices = np.lexsort((class_indices, -sample_scores))\n            top_k_classes = sorted_indices[:k]\n            if y_true[i] in top_k_classes:\n                correct_predictions += 1\n        return correct_predictions / len(y_true)\n\n    def calculate_budgeted_f1(model_scores, y_true, b, num_samples):\n        class1_scores = model_scores[:, 1]\n        is_positive = (y_true == 1)\n        \n        sample_indices = np.arange(num_samples)\n        # Tie-breaking: sort samples by class 1 score desc, then sample index asc\n        ranked_sample_indices = np.lexsort((sample_indices, -class1_scores))\n        \n        num_positives_total = np.sum(is_positive)\n        max_predictions = int(np.floor(b * num_samples))\n        \n        max_f1 = 0.0\n        \n        for m_pred in range(max_predictions + 1):\n            if m_pred == 0:\n                tp = 0\n                fp = 0\n            else:\n                predicted_pos_indices = ranked_sample_indices[:m_pred]\n                tp = np.sum(is_positive[predicted_pos_indices])\n                fp = m_pred - tp\n            \n            fn = num_positives_total - tp\n            \n            denominator = 2 * tp + fp + fn\n            if denominator > 0:\n                f1 = (2 * tp) / denominator\n                if f1 > max_f1:\n                    max_f1 = f1\n        return max_f1\n\n    def calculate_auroc(model_scores, y_true):\n        class1_scores = model_scores[:, 1]\n        is_positive = (y_true == 1)\n        \n        pos_scores = class1_scores[is_positive]\n        neg_scores = class1_scores[~is_positive]\n        \n        if len(pos_scores) == 0 or len(neg_scores) == 0:\n            return 0.5 \n\n        numerator = 0.0\n        for p_score in pos_scores:\n            for n_score in neg_scores:\n                if p_score > n_score:\n                    numerator += 1.0\n                elif p_score == n_score:\n                    numerator += 0.5\n        \n        denominator = len(pos_scores) * len(neg_scores)\n        return numerator / denominator\n\n    # --- Main Loop ---\n    \n    all_results = []\n    \n    # Pre-calculate AUROC as it's independent of test cases\n    auroc_scores = np.array([calculate_auroc(s, y_true) for s in scores])\n    best_auroc_model = select_best_model(auroc_scores)\n\n    for case in test_cases:\n        k = case['k']\n        b = case['b']\n        \n        # Top-k accuracy\n        top_k_scores = np.array([\n            calculate_top_k_accuracy(s, y_true, k, num_classes) for s in scores\n        ])\n        best_top_k_model = select_best_model(top_k_scores)\n        \n        # Budgeted F1\n        f1_scores = np.array([\n            calculate_budgeted_f1(s, y_true, b, num_samples) for s in scores\n        ])\n        best_f1_model = select_best_model(f1_scores)\n        \n        case_results = [best_top_k_model, best_f1_model, best_auroc_model]\n        all_results.append(case_results)\n\n    # Format and print the final output\n    output_str = f\"[{','.join([f'[{r[0]},{r[1]},{r[2]}]' for r in all_results])}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "尽管准确率等标准指标很方便，但现实世界的应用中，不同类型的错误往往会带来不成比例的代价。本练习将引导你从通用指标转向一个更切合实际的决策框架：效用最大化。你将学习如何将一个明确的成本矩阵融入模型选择过程，并通过交叉验证来同时确定最优的分类器参数和决策阈值，以期在真实场景中获得最大的期望效用。",
            "id": "3107638",
            "problem": "您会获得三个独立的二元分类任务。对于每个任务，您的目标是实现一个有原则的程序，通过最大化由指定成本矩阵引出的预期效用估计值，来联合选择模型和决策阈值。您生成的程序必须是一个完整、可运行的程序，该程序执行此过程，并在单行中输出所有任务的最终测试集效用。\n\n基本原理。使用以下基础定义：\n- 二元分类器将特征向量 $x \\in \\mathbb{R}^d$ 映射到一个实值分数 $s(x) \\in \\mathbb{R}$，然后使用 logistic 函数将其解释为概率估计 $p(y=1 \\mid x) \\in [0,1]$。假设使用称为带 $\\ell_2$ 正则化的逻辑回归的概率线性模型来拟合 $p(y=1 \\mid x)$。\n- 带有阈值 $t \\in [0,1]$ 的决策规则预测 $\\hat{y}(x;t)=\\mathbf{1}\\{p(y=1 \\mid x) \\ge t\\}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- 成本矩阵 $C \\in \\mathbb{R}^{2 \\times 2}$ 为当真实类别为 $i \\in \\{0,1\\}$ 时预测为类别 $j \\in \\{0,1\\}$ 的情况分配一个非负成本 $C_{ij}$。通过 $U_{ij}=-C_{ij}$ 定义效用矩阵 $U$。分类器-阈值对在数据集 $\\{(x_n,y_n)\\}_{n=1}^N$ 上的预期效用是经验平均值 $\\frac{1}{N}\\sum_{n=1}^N U_{y_n,\\hat{y}(x_n;t)}$。\n- $K$ 折交叉验证（Cross-Validation (CV)）将训练集划分为 $K$ 个大小近似相等的互斥折，在 $K-1$ 个折上进行训练，在留出的折上进行验证，并对各折的验证标准进行平均，以估计泛化性能。\n\n任务。对于以下每种情况：\n1. 使用 $K=3$ 的 $K$ 折交叉验证，通过最大化平均验证预期效用，联合选择逻辑回归的正则化强度 $\\lambda$ 和决策阈值 $t$。搜索集为：\n   - 正则化候选项 $\\Lambda=\\{\\lambda_1,\\lambda_2,\\lambda_3\\}=\\{\\,0.0,\\,0.1,\\,1.0\\,\\}$。\n   - 阈值网格 $\\mathcal{T}=\\{\\,0.00,\\,0.05,\\,0.10,\\,\\dots,\\,0.95,\\,1.00\\,\\}$。\n   使用确定性的折：将样本索引 $n$（从零开始）分配到第 $n \\bmod K$ 折。\n   通过在最大化项中选择最小的 $\\lambda$ 来打破平局，然后在剩余的最大化项中选择最小的 $t$。\n2. 使用选定的 $\\lambda$ 在完整训练集上重新训练逻辑回归，然后使用选定的阈值 $t$ 在提供的测试集上评估预期效用。\n3. 对于每种情况，报告测试集预期效用，形式为四舍五入到6位小数的浮点数。不要报告中间值或参数。\n\n数据集和成本矩阵。\n\n案例 A:\n- 训练特征 $X_{\\text{train}} \\in \\mathbb{R}^{6 \\times 2}$ 和标签 $y_{\\text{train}} \\in \\{0,1\\}^6$:\n  $$\n  X_{\\text{train}}=\\begin{bmatrix}\n  0.0  0.0\\\\\n  0.2  0.1\\\\\n  0.4  0.2\\\\\n  0.6  0.8\\\\\n  0.8  0.7\\\\\n  1.0  0.9\n  \\end{bmatrix},\\quad\n  y_{\\text{train}}=\\begin{bmatrix}\n  0\\\\\n  0\\\\\n  0\\\\\n  1\\\\\n  1\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- 测试特征 $X_{\\text{test}} \\in \\mathbb{R}^{2 \\times 2}$ 和标签 $y_{\\text{test}} \\in \\{0,1\\}^2$:\n  $$\n  X_{\\text{test}}=\\begin{bmatrix}\n  0.3  0.2\\\\\n  0.7  0.75\n  \\end{bmatrix},\\quad\n  y_{\\text{test}}=\\begin{bmatrix}\n  0\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- 成本矩阵 $C=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}$。\n\n案例 B:\n- 训练特征和标签：\n  $$\n  X_{\\text{train}}=\\begin{bmatrix}\n  -1.0  -1.0\\\\\n  -0.8  -0.6\\\\\n  -0.6  -0.8\\\\\n  0.5  0.4\\\\\n  0.6  0.3\\\\\n  0.2  0.1\\\\\n  -0.2  0.0\n  \\end{bmatrix},\\quad\n  y_{\\text{train}}=\\begin{bmatrix}\n  0\\\\\n  0\\\\\n  0\\\\\n  1\\\\\n  1\\\\\n  1\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- 测试特征和标签：\n  $$\n  X_{\\text{test}}=\\begin{bmatrix}\n  -0.7  -0.7\\\\\n  0.55  0.35\\\\\n  0.0  0.1\n  \\end{bmatrix},\\quad\n  y_{\\text{test}}=\\begin{bmatrix}\n  0\\\\\n  1\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- 成本矩阵 $C=\\begin{bmatrix}0  1\\\\ 4  0\\end{bmatrix}$。\n\n案例 C:\n- 训练特征和标签：\n  $$\n  X_{\\text{train}}=\\begin{bmatrix}\n  0.0  1.0\\\\\n  0.1  0.9\\\\\n  0.9  0.1\\\\\n  1.0  0.0\\\\\n  0.45  0.55\\\\\n  0.55  0.45\n  \\end{bmatrix},\\quad\n  y_{\\text{train}}=\\begin{bmatrix}\n  0\\\\\n  0\\\\\n  1\\\\\n  1\\\\\n  0\\\\\n  1\n  \\end{bmatrix}.\n  $$\n- 测试特征和标签：\n  $$\n  X_{\\text{test}}=\\begin{bmatrix}\n  0.52  0.48\\\\\n  0.48  0.52\\\\\n  0.6  0.4\\\\\n  0.4  0.6\n  \\end{bmatrix},\\quad\n  y_{\\text{test}}=\\begin{bmatrix}\n  1\\\\\n  0\\\\\n  1\\\\\n  0\n  \\end{bmatrix}.\n  $$\n- 成本矩阵 $C=\\begin{bmatrix}0  3\\\\ 1  0\\end{bmatrix}$。\n\n实现要求。\n- 使用带有 $\\ell_2$ 正则化和偏置项的逻辑回归，通过在正则化的负对数似然上进行基于梯度的优化来训练。您可以假设标签在 $\\{0,1\\}$ 中，并使用 logistic 链接函数。\n- 对于每个阈值 $t \\in \\mathcal{T}$ 和 $\\lambda \\in \\Lambda$，估计 $K$ 折的平均验证预期效用，然后根据上述打破平局规则选择使该量最大化的对 $(\\lambda^\\star,t^\\star)$。\n- 选择后，使用 $\\lambda^\\star$ 在完整训练集上重新训练，然后使用 $t^\\star$ 评估平均测试集预期效用。\n- 角度单位不适用。没有物理单位。所有报告的预期效用必须是小数，四舍五入到小数点后6位。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序为案例 A、案例 B、案例 C，例如 $[u_A,u_B,u_C]$，其中每个 $u_\\cdot$ 都是按上述规定四舍五入的测试集预期效用。",
            "solution": "我们必须设计一个有原则的联合模型和阈值选择方案，该方案基于从成本矩阵派生的预期效用。其基本原理包括二元分类器的概率解释、通过阈值化构建决策规则、成本到效用的转换，以及将交叉验证 (CV) 作为样本外性能的估计器。\n\n首先，考虑一个输入为 $x \\in \\mathbb{R}^d$ 且标签为 $y \\in \\{0,1\\}$ 的二元分类设定。伯努利模型下的概率线性分类器使用 logistic 链接函数来生成 $p_\\theta(y=1\\mid x)=\\sigma(w^\\top x + b)$，其中 $\\sigma(z)=\\frac{1}{1+e^{-z}}$，参数为 $\\theta=(w,b)$，其中 $w \\in \\mathbb{R}^d$ 和 $b \\in \\mathbb{R}$，正则化强度 $\\lambda \\ge 0$ 控制对 $w$ 的 $\\ell_2$ 惩罚。训练过程通过在训练数据 $\\{(x_n,y_n)\\}_{n=1}^N$ 上最小化正则化的负对数似然来进行，这是一个关于 $(w,b)$ 的凸目标函数，对于固定的 $\\lambda$ 有唯一的最小化器。\n\n其次，决策过程需要一个阈值 $t \\in [0,1]$ 将概率转换为类别预测：$\\hat{y}(x;t)=\\mathbf{1}\\{\\sigma(w^\\top x+b)\\ge t\\}$。决策的后果由成本矩阵 $C \\in \\mathbb{R}^{2\\times 2}$ 捕获，其中 $C_{ij}$ 是当真实类别为 $i$ 且预测为 $j$ 时产生的成本。我们定义效用矩阵 $U$ 为 $U=-C$，因此最大化预期效用等同于最小化预期成本。给定一个数据集，$(\\theta,t)$ 的经验预期效用为\n$$\n\\bar{u}(\\theta,t) \\;=\\; \\frac{1}{N}\\sum_{n=1}^N U_{y_n,\\,\\hat{y}(x_n;t)}.\n$$\n该量根据效用矩阵奖励正确的决策，并根据负成本惩罚错误。\n\n第三，为了选择最优的模型和阈值，我们需要一个对样本外预期效用的无偏估计。$K$ 折交叉验证 (CV) 通过将训练集索引划分为 $K$ 个折 $\\{\\mathcal{I}_k\\}_{k=0}^{K-1}$，在 $\\bigcup_{j\\ne k}\\mathcal{I}_j$ 上训练并在 $\\mathcal{I}_k$ 上验证来提供此估计。对于任何候选的 $\\lambda$ 和 $t$，定义\n$$\n\\widehat{u}_{\\text{CV}}(\\lambda,t) \\;=\\; \\frac{1}{K}\\sum_{k=0}^{K-1} \\frac{1}{|\\mathcal{I}_k|}\\sum_{n\\in \\mathcal{I}_k} U_{y_n,\\,\\hat{y}_{\\lambda,k}(x_n;t)},\n$$\n其中 $\\hat{y}_{\\lambda,k}$ 是通过在训练折 $\\bigcup_{j\\ne k}\\mathcal{I}_j$ 上用正则化 $\\lambda$ 训练逻辑回归，并将阈值 $t$ 应用于其在验证折 $\\mathcal{I}_k$ 上的概率输出而获得的决策规则。然后选择对 $(\\lambda^\\star,t^\\star)$ 来在搜索网格上最大化 $\\widehat{u}_{\\text{CV}}(\\lambda,t)$。平局通过选择最小的 $\\lambda$，然后在这些当中选择最小的 $t$ 来打破。在 CV 内部选择 $t$ 至关重要，因为验证概率依赖于训练好的模型，并且在用于评估效用的相同验证数据上选择 $t$ 可以确保阈值的选择被调整以实现泛化，从而减少如果在最终测试集上调整 $t$ 可能发生的乐观偏差。\n\n算法步骤：\n1. 对于每种情况，通过将索引 $n$ 放入第 $n \\bmod K$ 折来构建一个确定性的 $K$ 折划分，确保可复现性和对边缘索引的覆盖。\n2. 对于每个 $\\lambda \\in \\{0.0,0.1,1.0\\}$:\n   - 对于每个折 $k \\in \\{0,1,2\\}$:\n     - 在其他折的并集上，用 $\\ell_2$ 惩罚 $\\lambda$ 训练逻辑回归，使用基于梯度的优化最小化正则化的负对数似然。$w$ 的梯度结合了平均残差项和正则化项；$b$ 的梯度是平均残差。迭代更新 $w \\leftarrow w - \\eta \\nabla_w$，$b \\leftarrow b - \\eta \\nabla_b$ 会减小凸目标函数值，其中 $\\eta0$ 是一个学习率。\n     - 计算验证折上的概率预测。\n   - 对于每个阈值 $t \\in \\{0.00,0.05,\\dots,1.00\\}$，将验证概率转换为预测，并计算每个折的效用，然后跨折求平均以获得 $\\widehat{u}_{\\text{CV}}(\\lambda,t)$。\n3. 使用指定的打破平局规则，选择最大化交叉验证估计值的 $(\\lambda^\\star,t^\\star)$。\n4. 使用 $\\lambda^\\star$ 在完整训练集上重新训练逻辑回归，计算测试集上的概率预测，应用 $t^\\star$ 获得决策，并计算平均测试集预期效用 $\\bar{u}_{\\text{test}}$。\n5. 将每个 $\\bar{u}_{\\text{test}}$ 四舍五入到6位小数。\n\n设计覆盖范围的基本原理：\n- 阈值网格包括边界值 $t=0$ 和 $t=1$，捕获了所有样本分别被预测为正类或负类的边缘情况，这在极端成本不对称性下很重要。\n- 正则化网格涵盖了无正则化 ($\\lambda=0.0$)、轻度正则化 ($\\lambda=0.1$) 和强正则化 ($\\lambda=1.0$)，以测试偏差-方差权衡。\n- 数据集涵盖：平衡成本（案例 A），更多地惩罚假阴性的不对称成本（案例 B），以及更多地惩罚假阳性的不对称成本（案例 C）。这确保了在相反方向上的阈值移动得到实践。\n\n该程序精确地实现了上述算法，确保了确定性的折划分、模型复杂度和阈值的联合选择、选择后在完整训练集上重新训练，并以所需格式计算最终输出 $[u_A,u_B,u_C]$。每个效用都是一个小数（而不是百分比），按规定四舍五入到6位小数。",
            "answer": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef train_logistic_regression(X, y, lambda_reg=0.0, lr=0.1, iters=3000):\n    \"\"\"\n    Train logistic regression with L2 regularization on weights (not bias).\n    X: (n_samples, n_features)\n    y: (n_samples,) in {0,1}\n    Returns weights w (n_features,) and bias b.\n    \"\"\"\n    n, d = X.shape\n    w = np.zeros(d, dtype=float)\n    b = 0.0\n    for _ in range(iters):\n        z = X @ w + b\n        p = sigmoid(z)\n        # gradient\n        residual = (p - y)  # shape (n,)\n        grad_w = (X.T @ residual) / n + lambda_reg * w\n        grad_b = np.sum(residual) / n\n        # update\n        w -= lr * grad_w\n        b -= lr * grad_b\n    return w, b\n\ndef predict_proba(X, w, b):\n    return sigmoid(X @ w + b)\n\ndef expected_utility_from_probs(y_true, y_prob, threshold, cost_matrix):\n    \"\"\"\n    y_true: (n,) in {0,1}\n    y_prob: (n,) in [0,1]\n    threshold: float in [0,1]\n    cost_matrix: 2x2 numpy array\n    Returns mean utility (negative cost).\n    \"\"\"\n    y_pred = (y_prob >= threshold).astype(int)\n    # Utility matrix is negative of cost matrix\n    U = -cost_matrix\n    # Map each pair (y_true[i], y_pred[i]) to utility\n    utilities = U[y_true, y_pred]\n    return float(np.mean(utilities))\n\ndef kfold_indices(n, K):\n    \"\"\"\n    Deterministic K-fold split by index modulo K.\n    Returns list of folds, each is a numpy array of indices.\n    \"\"\"\n    folds = [[] for _ in range(K)]\n    for idx in range(n):\n        folds[idx % K].append(idx)\n    return [np.array(f, dtype=int) for f in folds]\n\ndef joint_cv_select_threshold_and_lambda(X, y, cost_matrix, lambdas, thresholds, K=3, lr=0.1, iters=3000):\n    \"\"\"\n    Perform K-fold CV to jointly select lambda and threshold maximizing expected utility.\n    Returns selected (lambda_star, threshold_star).\n    Tie-breaking: smallest lambda, then smallest threshold.\n    \"\"\"\n    n = X.shape[0]\n    folds = kfold_indices(n, K)\n    # Precompute folds' train/val splits\n    fold_train_val = []\n    all_indices = np.arange(n, dtype=int)\n    for val_idx in folds:\n        train_idx = np.setdiff1d(all_indices, val_idx, assume_unique=True)\n        fold_train_val.append((train_idx, val_idx))\n\n    best_u = -np.inf\n    best_lambda = None\n    best_t = None\n\n    # Iterate lambdas in ascending order for tie-breaking\n    for lam in sorted(lambdas):\n        # For each fold, train model and store validation probabilities\n        val_probs_per_fold = []\n        y_val_per_fold = []\n        for (tr_idx, va_idx) in fold_train_val:\n            w, b = train_logistic_regression(X[tr_idx], y[tr_idx], lambda_reg=lam, lr=lr, iters=iters)\n            probs = predict_proba(X[va_idx], w, b)\n            val_probs_per_fold.append(probs)\n            y_val_per_fold.append(y[va_idx])\n        # Concatenate for averaging across folds per threshold\n        # However, to respect equal weighting per example, we compute fold-wise means and average,\n        # as specified in the problem statement.\n        # We'll compute per-fold utilities for each threshold and average across folds.\n        for t in sorted(thresholds):\n            u_folds = []\n            for probs, yv in zip(val_probs_per_fold, y_val_per_fold):\n                u = expected_utility_from_probs(yv, probs, t, cost_matrix)\n                u_folds.append(u)\n            u_cv = float(np.mean(u_folds))\n            if (u_cv > best_u) or (np.isclose(u_cv, best_u) and (best_lambda is not None) and (lam  best_lambda)) or (np.isclose(u_cv, best_u) and lam == best_lambda and (best_t is not None) and (t  best_t)):\n                best_u = u_cv\n                best_lambda = lam\n                best_t = t\n\n    return best_lambda, best_t\n\ndef solve():\n    # Define threshold grid and lambda grid\n    thresholds = np.round(np.linspace(0.0, 1.0, 21), 2)  # 0.00, 0.05, ..., 1.00\n    lambdas = [0.0, 0.1, 1.0]\n    K = 3\n\n    # Case A\n    X_train_A = np.array([\n        [0.0, 0.0],\n        [0.2, 0.1],\n        [0.4, 0.2],\n        [0.6, 0.8],\n        [0.8, 0.7],\n        [1.0, 0.9],\n    ], dtype=float)\n    y_train_A = np.array([0, 0, 0, 1, 1, 1], dtype=int)\n    X_test_A = np.array([\n        [0.3, 0.2],\n        [0.7, 0.75],\n    ], dtype=float)\n    y_test_A = np.array([0, 1], dtype=int)\n    C_A = np.array([[0.0, 1.0],\n                    [1.0, 0.0]], dtype=float)\n\n    # Case B\n    X_train_B = np.array([\n        [-1.0, -1.0],\n        [-0.8, -0.6],\n        [-0.6, -0.8],\n        [0.5, 0.4],\n        [0.6, 0.3],\n        [0.2, 0.1],\n        [-0.2, 0.0],\n    ], dtype=float)\n    y_train_B = np.array([0, 0, 0, 1, 1, 1, 1], dtype=int)\n    X_test_B = np.array([\n        [-0.7, -0.7],\n        [0.55, 0.35],\n        [0.0, 0.1],\n    ], dtype=float)\n    y_test_B = np.array([0, 1, 1], dtype=int)\n    C_B = np.array([[0.0, 1.0],\n                    [4.0, 0.0]], dtype=float)\n\n    # Case C\n    X_train_C = np.array([\n        [0.0, 1.0],\n        [0.1, 0.9],\n        [0.9, 0.1],\n        [1.0, 0.0],\n        [0.45, 0.55],\n        [0.55, 0.45],\n    ], dtype=float)\n    y_train_C = np.array([0, 0, 1, 1, 0, 1], dtype=int)\n    X_test_C = np.array([\n        [0.52, 0.48],\n        [0.48, 0.52],\n        [0.6, 0.4],\n        [0.4, 0.6],\n    ], dtype=float)\n    y_test_C = np.array([1, 0, 1, 0], dtype=int)\n    C_C = np.array([[0.0, 3.0],\n                    [1.0, 0.0]], dtype=float)\n\n    cases = [\n        (X_train_A, y_train_A, X_test_A, y_test_A, C_A),\n        (X_train_B, y_train_B, X_test_B, y_test_B, C_B),\n        (X_train_C, y_train_C, X_test_C, y_test_C, C_C),\n    ]\n\n    results = []\n    # Training parameters (chosen to ensure convergence on small datasets)\n    lr = 0.2\n    iters = 4000\n\n    for X_tr, y_tr, X_te, y_te, C in cases:\n        lam_star, t_star = joint_cv_select_threshold_and_lambda(\n            X_tr, y_tr, C, lambdas, thresholds, K=K, lr=lr, iters=iters\n        )\n        w, b = train_logistic_regression(X_tr, y_tr, lambda_reg=lam_star, lr=lr, iters=iters)\n        probs_test = predict_proba(X_te, w, b)\n        u_test = expected_utility_from_probs(y_te, probs_test, t_star, C)\n        # Round to 6 decimal places\n        results.append(f\"{u_test:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "模型选择的艺术也体现在对模型复杂度的精妙把握上，这一挑战的核心便是经典的“偏差-方差权衡”。本回归练习通过比较一个施加了强结构性假设的模型（保序回归）和一个更为灵活的模型（平滑样条），来深入探索这一权衡。通过在不同信噪比和信号复杂度的生成数据上运用交叉验证，你将领悟到，为何有时一个更简单、受更多约束的模型，其泛化能力反而会超越更复杂的模型。",
            "id": "3107704",
            "problem": "您将处理一个基于统计学习中经验风险最小化的回归选择任务。底层信号具有全局单调趋势，并带有一个局部的非单调区域。您的目标是实现两个模型族，并通过交叉验证选择最优模型，具体方法是比较一个单调约束模型和一个灵活的样条模型。目标是为每个测试用例输出性能更优模型的索引，并以单个列表的形式呈现。\n\n基本原理：\n- 数据为独立同分布样本 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $x_i \\in [0,1]$, $y_i = f(x_i) + \\varepsilon_i$，且噪声 $\\varepsilon_i$ 是均值为 0、方差为 $\\sigma^2$ 的高斯噪声。\n- 学习目标是通过最小化期望平方预测风险 $R(f) = \\mathbb{E}[(Y - f(X))^2]$ 来近似回归函数 $f(x)$。\n- 由于分布未知，请使用 $K$ 折交叉验证来估计泛化误差，并通过经验风险最小化进行模型选择。\n\n需要实现和比较的模型：\n- 单调约束模型：保序回归，它使用邻近合并算法 (pool-adjacent-violators algorithm) 对按 $x$ 排序的训练数据估计一个非递减函数 $\\hat{f}_{\\mathrm{iso}}$。对于新 $x$ 值的预测，将学习到的阶梯函数在观测到的训练点之间作为左常数进行评估。\n- 灵活模型：立方平滑样条，它估计一个平滑函数 $\\hat{f}_{\\mathrm{spline}}$，其中平滑参数 $s$ 控制保真度与平滑度之间的权衡。\n\n每个测试用例的数据生成：\n- 将 $x_i$ 生成为 $[0,1]$ 上的均匀间隔点。\n- 将底层函数定义为 $f(x) = x + b(x)$，其中凸起函数 $b(x)$ 引入了局部的非单调性：\n$$\nb(x) = \\begin{cases}\nA \\sin\\!\\left(\\dfrac{\\pi (x - x_0)}{w}\\right),  \\text{if } |x - x_0| \\le w,\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\n- 生成 $y_i = f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。\n\n交叉验证和模型选择协议：\n- 使用 $K=5$ 的 $K$ 折交叉验证，在按 $x$ 排序的顺序中，将索引集划分为 5 个连续且大小近似相等的折。\n- 对于保序回归，直接计算各折的均方误差 (MSE)。\n- 对于立方平滑样条，评估一个平滑参数网格，并选择产生最低交叉验证 MSE 的参数。通过尺度 $S = \\operatorname{Var}(y) \\cdot n$ 定义网格，并考虑 $s \\in \\{0, 0.05 S, 0.2 S, 1 S, 5 S, 20 S\\}$。\n- 在保序回归和网格中最佳的样条模型之间，选择交叉验证 MSE 最小的模型。\n\n答案表示：\n- 对于每个测试用例，以整数形式输出所选模型：如果选择保序回归，则输出 $0$；如果选择样条模型，则输出 $1$。\n\n测试套件：\n- 每个测试用例是一个元组 $(n, \\sigma, A, w, x_0, \\text{seed})$。\n- 使用以下用例：\n    1. $(200, 0.05, 0.0, 0.12, 0.60, 42)$：纯单调基线 ($A = 0$)。\n    2. $(200, 0.05, 0.20, 0.12, 0.60, 43)$：主要单调趋势上的小凸起。\n    3. $(200, 0.05, 0.80, 0.12, 0.60, 44)$：显著的局部非单调性。\n    4. $(200, 0.50, 0.80, 0.12, 0.60, 45)$：高噪声下的显著凸起。\n    5. $(30, 0.10, 0.50, 0.15, 0.55, 46)$：带有局部凸起的小样本。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_i \\in \\{0,1\\}$ 对应第 $i$ 个测试用例所选择的模型。",
            "solution": "该问题要求在一组指定的回归任务上，对两种回归模型——保序回归和立方平滑样条——进行比较分析。问题的核心在于通过交叉验证进行模型选择，这是统计学习中用于估计模型泛化误差并选择能最佳平衡偏差与方差模型的基本技术。\n\n该问题被认为是有效的，因为它以既定的统计学原理为科学基础，问题是适定的（well-posed），为可复现的计算实验提供了所有必要信息，并且其表述和评估标准是客观的。\n\n分步方法如下：\n\n**1. 数据生成**\n\n对于每个由参数元组 $(n, \\sigma, A, w, x_0, \\text{seed})$ 定义的测试用例，我们生成一个大小为 $n$ 的数据集。\n- 使用给定的 `seed` 初始化一个 `Numpy` 随机数生成器，以确保可复现性。\n- 预测变量 $x$ 由区间 $[0, 1]$ 上的 $n$ 个等间距点组成。\n- 真实的底层回归函数 $f(x)$ 被定义为一个线性趋势和一个局部正弦“凸起”之和：$f(x) = x + b(x)$，其中\n$$\nb(x) = \\begin{cases}\nA \\sin\\!\\left(\\dfrac{\\pi (x - x_0)}{w}\\right),  \\text{if } |x - x_0| \\le w,\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\n参数 $A$ 控制凸起的振幅，从而控制局部非单调性的程度。当 $A=0$ 时，函数 $f(x)=x$ 是纯单调的。\n- 观测到的响应变量 $y_i$ 是通过向真实函数值添加高斯噪声生成的：$y_i = f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n**2. 交叉验证框架**\n\n为了估计每个模型的泛化均方误差 (MSE)，我们采用 $K=5$ 的 $K$ 折交叉验证。\n- 数据集按 $x_i$ 的值自然排序，并被划分为 $K=5$ 个连续、不重叠的折。\n- 对于每一个折 $k \\in \\{1, \\dots, 5\\}$，第 $k$ 个折作为验证集，其余的 $K-1$ 个折构成训练集。\n- 模型在训练集上进行训练，并通过计算其在验证集上的均方误差来评估其性能。\n- 模型的最终交叉验证 MSE 是所有 $K$ 个折上计算出的 MSE 的平均值。\n\n**3. 模型实现与评估**\n\n我们实现并评估两种不同的模型。\n\n**模型 0：保序回归**\n保序回归是一种非参数方法，它将一个非递减的分段常数函数 $\\hat{f}_{\\mathrm{iso}}$ 拟合到数据上。该模型施加了一个很强的结构性约束（单调性），如果底层信号确实是单调的，这种方法会非常有效。\n- **训练**：对于每个训练集，使用邻近合并算法 (PAVA) 计算保序拟合。PAVA 迭代地合并违反非递减约束的相邻点块，用它们的加权平均值替换它们，直到整个拟合值序列是单调的。\n- **预测**：对于验证集中的一个新点 $x_{\\text{new}}$，预测值 $\\hat{f}_{\\mathrm{iso}}(x_{\\text{new}})$ 由拟合的阶梯函数确定。按照规定，我们使用左常数评估规则：预测值是对应于满足 $x_{\\text{train}} \\le x_{\\text{new}}$ 的最大训练点 $x_{\\text{train}}$ 的拟合值。\n- **交叉验证误差**：均方误差 $MSE_{\\text{iso}}$ 是通过对所有 $K$ 个折的预测误差求平均值来计算的。\n\n**模型 1：立方平滑样条**\n立方平滑样条 $\\hat{f}_{\\mathrm{spline}}$ 是一种更灵活的非参数模型，它将一条平滑曲线拟合到数据。它是最小化惩罚残差平方和问题的解：\n$$\n\\sum_{i=1}^{n_{\\text{train}}} (y_i - \\hat{f}(x_i))^2 + s \\int (\\hat{f}''(t))^2 dt\n$$\n平滑参数 $s \\ge 0$ 控制着拟合优度（第一项）与函数平滑度（第二项，对二阶导数平方积分的惩罚）之间的权衡。\n- **超参数调优**：样条的性能关键取决于 $s$ 的选择。我们执行网格搜索来找到最优的平滑参数。该网格是相对于数据尺度定义的：$S = n \\cdot \\operatorname{Var}(y)$，其中 $y$ 是整个数据集的响应向量。候选值网格为 $s \\in \\{0, 0.05 S, 0.2 S, 1 S, 5 S, 20 S\\}$。$s=0$ 的值会产生一个插值样条。\n- **交叉验证误差**：对于每个候选的 $s$ 值，我们计算其 $K$ 折交叉验证 MSE。最优样条对应于产生最小 CV-MSE 的参数 $s^*$。这个最小值 $MSE_{\\text{spline}} = \\min_{s} MSE_{\\text{spline}}(s)$，作为样条模型族的性能度量。这个过程嵌套在主模型比较中；对于 $K$ 个外部折中的每一个，都会为每个 $s$ 值拟合一个样條，并计算其在验证集上的误差。给定 $s$ 的总误差是 $K$ 个折上的平均值。\n\n**4. 模型选择**\n\n最后一步是为给定数据集选择性能更优的模型。决策基于对估计的泛化误差的直接比较：\n- 如果 $MSE_{\\text{iso}} \\le MSE_{\\text{spline}}$，则选择保序回归模型（模型 0）。在相等的情况下，优先选择更简单的模型。\n- 否则，如果 $MSE_{\\text{spline}}  MSE_{\\text{iso}}$，则选择立方平滑样条模型（模型 1）。\n\n**预期结果与偏差-方差权衡**\n\n每个测试用例的结果取决于信号结构、样本大小和噪声水平之间的相互作用，这阐释了偏差-方差权衡。\n- **情况 1 ($A=0.0$)：** 真实信号是单调的。保序回归被正确定型（低偏差）且受约束（低方差）。样条模型则过于灵活，可能会过拟合（更高方差）。预期会选择保序回归（模型 0）。\n- **情况 2 ($A=0.20$)：** 引入了一个小的非单调凸起。此时保序回归模型设定错误，在凸起区域会因为将其抹平而产生高偏差。然而，其方差仍然很低。样条模型因为能捕捉到凸起而具有较低偏差，但其灵活性也导致了更高的方差。选择是不明确的，取决于样条模型偏差的减少是否能超过其方差的增加。\n- **情况 3 ($A=0.80$)：** 凸起很显著。保序模型的偏差变得非常大，可能主导其误差。样条模型拟合非单调特征的能力（低偏差）应使其成为更优的模型，因此预期选择模型 1。\n- **情况 4 ($A=0.80$, $\\sigma=0.50$)：** 存在显著凸起但噪声很高。高噪声水平显著增加了灵活的样条模型过拟合的风险，导致非常高的方差。更简单、更稳定（尽管有偏）的保序模型可能会产生更低的总 MSE。模型 0 可能会被选中。\n- **情况 5 ($n=30$)：** 样本量小。在数据有限的情况下，方差是预测误差的主要组成部分。像样条这样的灵活模型非常容易过拟合。保序模型的强结构性假设起到了强大的正则化作用，导致较低的方差和可能更好的泛化能力。预期选择模型 0。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import UnivariateSpline\nimport sys\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        (200, 0.05, 0.0, 0.12, 0.60, 42),\n        (200, 0.05, 0.20, 0.12, 0.60, 43),\n        (200, 0.05, 0.80, 0.12, 0.60, 44),\n        (200, 0.50, 0.80, 0.12, 0.60, 45),\n        (30, 0.10, 0.50, 0.15, 0.55, 46),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, sigma, A, w_param, x0, seed = case\n        \n        # 1. Data Generation\n        rng = np.random.default_rng(seed)\n        x = np.linspace(0, 1, n)\n        \n        def bump_fn(x, A, w_param, x0):\n            b = np.zeros_like(x)\n            mask = np.abs(x - x0) = w_param\n            b[mask] = A * np.sin(np.pi * (x[mask] - x0) / w_param)\n            return b\n\n        f_x = x + bump_fn(x, A, w_param, x0)\n        y = f_x + rng.normal(0, sigma, size=n)\n\n        # 2. Cross-Validation Setup\n        K = 5\n        indices = np.arange(n)\n        folds = np.array_split(indices, K)\n\n        # 3. Model 0: Isotonic Regression Evaluation\n        def pava(y_in):\n            \"\"\"Pool-Adjacent-Violators Algorithm.\"\"\"\n            n_pava = len(y_in)\n            if n_pava == 0:\n                return np.array([])\n            \n            pools = [[val, 1] for val in y_in]\n            \n            i = 0\n            while i  len(pools) - 1:\n                if pools[i][0] > pools[i+1][0]:\n                    val1, w1 = pools[i]\n                    val2, w2 = pools[i+1]\n                    new_w = w1 + w2\n                    new_val = (val1 * w1 + val2 * w2) / new_w\n                    pools[i+1] = [new_val, new_w]\n                    pools.pop(i)\n                    i = max(0, i - 1)\n                else:\n                    i += 1\n            \n            y_iso = np.zeros(n_pava)\n            current_pos = 0\n            for val, weight in pools:\n                y_iso[current_pos : current_pos + weight] = val\n                current_pos += weight\n            return y_iso\n\n        iso_errors = []\n        for k in range(K):\n            val_indices = folds[k]\n            train_indices = np.concatenate([folds[j] for j in range(K) if j != k])\n            \n            x_train, y_train = x[train_indices], y[train_indices]\n            x_val, y_val = x[val_indices], y[val_indices]\n\n            y_iso_train = pava(y_train)\n            \n            # Predict on validation set (left-constant interpolation/extrapolation)\n            pred_indices = np.searchsorted(x_train, x_val, side='right') - 1\n            pred_indices = np.maximum(0, pred_indices)\n            y_iso_pred = y_iso_train[pred_indices]\n            \n            iso_errors.append(np.mean((y_val - y_iso_pred)**2))\n        \n        mse_iso = np.mean(iso_errors)\n\n        # 4. Model 1: Cubic Smoothing Spline Evaluation\n        S = np.var(y) * n if n > 1 else 1.0\n        s_grid = [0, 0.05 * S, 0.2 * S, 1 * S, 5 * S, 20 * S]\n        \n        spline_cv_mses = []\n        for s_val in s_grid:\n            spline_s_errors = []\n            for k in range(K):\n                val_indices = folds[k]\n                train_indices = np.concatenate([folds[j] for j in range(K) if j != k])\n            \n                x_train, y_train = x[train_indices], y[train_indices]\n                x_val, y_val = x[val_indices], y[val_indices]\n\n                # UnivariateSpline requires at least k+1 points for degree k\n                if len(x_train)  4: # k=3 is the default\n                    spline_s_errors.append(np.inf)\n                    continue\n\n                try:\n                    spline = UnivariateSpline(x_train, y_train, s=s_val, k=3)\n                    y_spline_pred = spline(x_val)\n                    spline_s_errors.append(np.mean((y_val - y_spline_pred)**2))\n                except:\n                     spline_s_errors.append(np.inf) # Handle potential fitting errors\n\n            spline_cv_mses.append(np.mean(spline_s_errors))\n            \n        mse_spline = np.min(spline_cv_mses)\n\n        # 5. Model Selection\n        # Select Model 0 (Isotonic) if its MSE is less than or equal to the spline's MSE\n        selected_model = 0 if mse_iso = mse_spline else 1\n        results.append(selected_model)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}