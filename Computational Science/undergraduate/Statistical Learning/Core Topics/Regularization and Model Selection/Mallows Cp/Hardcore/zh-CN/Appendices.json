{
    "hands_on_practices": [
        {
            "introduction": "要理解一个复杂的工具，最好的方法往往是从一个简化的理想情景开始。本练习采用正交设计，这是一种常见的教学工具，可以剥离掉不必要的复杂性，让我们直击核心。通过这个练习，你将推导出一个令人惊讶而优美的结论：在正交设定下，最小化马洛斯$C_p$等价于对回归系数进行简单的“硬阈值”筛选。这个练习将为你深入理解$C_p$的本质提供一个直观的视角——它本质上是一种判断哪些系数“足够大”而值得保留在模型中的方法。",
            "id": "3143696",
            "problem": "考虑固定设计线性模型 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^{n}$，$X \\in \\mathbb{R}^{n \\times p}$，且 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。为消除截距项带来的复杂性并简化几何结构，假设以下设定：\n- 响应变量 $y$ 和 $X$ 的每一列 $x_{j}$ 都已中心化，因此模型中不包含截距项。\n- $X$ 的列是正交的，并且经过缩放以满足 $X^{\\top} X = n I_{p}$，即对于 $j \\neq k$，有 $\\langle x_{j}, x_{k} \\rangle = 0$，且对于所有 $j \\in \\{1, \\dots, p\\}$，有 $\\|x_{j}\\|^{2} = n$。\n\n对于任意基数为 $|S| = k$ 的子集 $S \\subseteq \\{1, \\dots, p\\}$，用 $X_{S}$ 表示由 $S$ 索引的列组成的 $X$ 的子矩阵，并令 $\\hat{\\beta}_{S}$ 为将 $y$ 对 $X_{S}$ 进行回归得到的普通最小二乘 (OLS) 估计。令 $\\text{RSS}(S)$ 表示此子集拟合的残差平方和。令 $\\hat{\\sigma}^{2}$ 是从包含全部 $p$ 个变量的模型中获得的 $\\sigma^{2}$ 的一个固定估计值，并在所有子集中视为常数。\n\n模型 $S$ 的 Mallows' $C_{p}$ 定义为\n$$\nC_{p}(S) = \\frac{\\text{RSS}(S)}{\\hat{\\sigma}^{2}} - \\left(n - 2k\\right).\n$$\n\n从第一性原理出发，仅使用上述定义以及正交投影的线性代数和 OLS 正规方程，推导当 $X$ 如指定般正交时，通过在所有子集 $S$ 上最小化 $C_{p}(S)$ 所得到的精确决策规则。特别地，证明最小化 $C_{p}$ 的子集是通过对单个 OLS 系数 $\\{ \\hat{\\beta}_{j} \\}_{j=1}^{p}$ 进行硬阈值化处理得到的，具体方法是将 $|\\hat{\\beta}_{j}|$ 与一个不依赖于 $j$ 的、基于方差的单一阈值进行比较。\n\n这个通用阈值 $T$ 的精确闭式表达式（用 $n$ 和 $\\hat{\\sigma}^{2}$ 表示）是什么？使得最小化 $C_{p}$ 的规则当且仅当 $|\\hat{\\beta}_{j}| > T$ 时才包含预测变量 $j$。请以单个解析表达式的形式给出你的最终答案。不需要进行数值四舍五入。",
            "solution": "本问题的目标是，在设计矩阵 $X$ 正交的特定条件下，推导最小化 Mallows' $C_p$ 统计量的子集选择决策规则。我们必须证明该规则等价于对普通最小二乘 (OLS) 系数进行硬阈值化处理，并求出该阈值的显式表达式。\n\n对于一个包含大小为 $|S|=k$ 的预测变量子集 $S \\subseteq \\{1, \\dots, p\\}$ 的模型，其 Mallows' $C_p$ 统计量由下式给出：\n$$\nC_{p}(S) = \\frac{\\text{RSS}(S)}{\\hat{\\sigma}^{2}} - (n - 2k)\n$$\n其中 $\\text{RSS}(S)$ 是该模型使用预测变量子集 $S$ 时的残差平方和，$n$ 是观测数量，$\\hat{\\sigma}^{2}$ 是误差方差 $\\sigma^2$ 的一个固定估计值，通常来自全模型。为了最小化 $C_p(S)$，我们必须首先找到一个关于 $\\text{RSS}(S)$ 的更方便的表达式。\n\n问题陈述，设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列是正交的，并且经过缩放以满足 $X^{\\top} X = n I_{p}$。这一性质显著简化了 OLS 的计算。\n\n首先，考虑包含所有 $p$ 个预测变量的全模型的 OLS 估计。系数估计值 $\\hat{\\beta} \\in \\mathbb{R}^p$ 由正规方程给出：\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1} X^{\\top}y\n$$\n代入正交性条件 $X^{\\top}X = n I_{p}$，我们得到：\n$$\n\\hat{\\beta} = (n I_{p})^{-1} X^{\\top}y = \\frac{1}{n} X^{\\top}y\n$$\n该向量的第 $j$ 个分量是 $\\hat{\\beta}_j = \\frac{1}{n} x_j^{\\top}y$，其中 $x_j$ 是 $X$ 的第 $j$ 列。\n\n接下来，考虑一个使用由集合 $S$ 索引的预测变量的子集模型，其中 $|S|=k$。该模型的设计矩阵是 $X_S$。此子集模型的 OLS 估计 $\\hat{\\beta}_S$ 由下式给出：\n$$\n\\hat{\\beta}_S = (X_S^{\\top}X_S)^{-1} X_S^{\\top}y\n$$\n由于 $X$ 的列正交性，子矩阵 $X_S$ 的列也是正交的。因此，$X_S^{\\top}X_S = n I_{k}$。这导致：\n$$\n\\hat{\\beta}_S = (n I_{k})^{-1} X_S^{\\top}y = \\frac{1}{n} X_S^{\\top}y\n$$\n一个至关重要的结论是，对于任何预测变量 $j \\in S$，其在子集模型中的系数估计值为 $\\frac{1}{n} x_j^{\\top}y$，这与它在全模型中的系数估计值 $\\hat{\\beta}_j$ 完全相同。这意味着在正交条件下，子集选择不会改变所选变量的系数。\n\n现在，我们推导 $\\text{RSS}(S)$ 的表达式。子集模型的拟合值为 $\\hat{y}_S = X_S \\hat{\\beta}_S$。残差平方和为 $\\text{RSS}(S) = \\|y - \\hat{y}_S\\|^2$。由于 $\\hat{y}_S$ 是 $y$ 在 $X_S$ 列空间上的正交投影，根据勾股定理，我们有 $\\|y\\|^2 = \\|\\hat{y}_S\\|^2 + \\|y - \\hat{y}_S\\|^2$。因此，$\\text{RSS}(S) = \\|y\\|^2 - \\|\\hat{y}_S\\|^2$。\n我们来计算 $\\|\\hat{y}_S\\|^2$：\n$$\n\\|\\hat{y}_S\\|^2 = \\hat{y}_S^{\\top}\\hat{y}_S = (X_S \\hat{\\beta}_S)^{\\top}(X_S \\hat{\\beta}_S) = \\hat{\\beta}_S^{\\top} X_S^{\\top}X_S \\hat{\\beta}_S\n$$\n代入 $X_S^{\\top}X_S = n I_{k}$：\n$$\n\\|\\hat{y}_S\\|^2 = \\hat{\\beta}_S^{\\top} (n I_{k}) \\hat{\\beta}_S = n \\|\\hat{\\beta}_S\\|^2 = n \\sum_{j \\in S} (\\hat{\\beta}_j)^2\n$$\n注意，我们用 $\\hat{\\beta}_j$ 表示来自全模型的系数，我们已经证明对于 $j \\in S$，该系数是相同的。\n因此，子集模型的残差平方和为：\n$$\n\\text{RSS}(S) = \\|y\\|^2 - n \\sum_{j \\in S} \\hat{\\beta}_j^2\n$$\n现在我们将其代入 $C_p(S)$ 的表达式中：\n$$\nC_{p}(S) = \\frac{\\|y\\|^2 - n \\sum_{j \\in S} \\hat{\\beta}_j^2}{\\hat{\\sigma}^{2}} - (n - 2|S|)\n$$\n为了最小化 $C_p(S)$，我们需要选择集合 $S$。我们可以通过为每个预测变量 $j \\in \\{1, \\dots, p\\}$ 做出独立决策来重新表述这个最小化问题。我们为每个预测变量引入一个指示变量 $\\delta_j \\in \\{0, 1\\}$，如果预测变量 $j$ 被包含在模型中（即 $j \\in S$），则 $\\delta_j=1$，否则 $\\delta_j=0$。\n使用这个记号，我们有 $|S| = k = \\sum_{j=1}^{p} \\delta_j$ 和 $\\sum_{j \\in S} \\hat{\\beta}_j^2 = \\sum_{j=1}^{p} \\delta_j \\hat{\\beta}_j^2$。\n$C_p$ 的表达式变为：\n$$\nC_{p}(S) = \\frac{\\|y\\|^2 - n \\sum_{j=1}^{p} \\delta_j \\hat{\\beta}_j^2}{\\hat{\\sigma}^{2}} - n + 2 \\sum_{j=1}^{p} \\delta_j\n$$\n我们重新整理各项，以分离出依赖于 $\\delta_j$ 选择的部分：\n$$\nC_{p}(S) = \\left( \\frac{\\|y\\|^2}{\\hat{\\sigma}^2} - n \\right) + \\sum_{j=1}^{p} \\left( 2 \\delta_j - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\delta_j \\right)\n$$\n$$\nC_{p}(S) = \\left( \\frac{\\|y\\|^2}{\\hat{\\sigma}^2} - n \\right) + \\sum_{j=1}^{p} \\delta_j \\left( 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\right)\n$$\n项 $\\left( \\frac{\\|y\\|^2}{\\hat{\\sigma}^2} - n \\right)$ 相对于子集 $S$ 的选择是常数。因此，要最小化 $C_p(S)$，我们必须最小化求和项。由于对每个 $\\delta_j$ 的决策与其他决策是独立的，我们可以通过逐个最小化每一项来最小化总和。\n\n对于每个预测变量 $j$，我们对 $\\delta_j$ 有两种选择：\n1.  排除预测变量 $j$：设置 $\\delta_j = 0$。此预测变量对总和的贡献为 $0 \\times \\left( 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\right) = 0$。\n2.  包含预测变量 $j$：设置 $\\delta_j = 1$。此预测变量对总和的贡献为 $1 \\times \\left( 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2} \\right) = 2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2}$。\n\n当且仅当包含预测变量 $j$（即设置 $\\delta_j=1$）能减小总和时，我们才选择包含它。这种情况发生在包含它的贡献小于排除它的贡献时：\n$$\n2 - \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2}  0\n$$\n重新整理这个不等式，我们得到包含预测变量 $j$ 的条件：\n$$\n2  \\frac{n \\hat{\\beta}_j^2}{\\hat{\\sigma}^2}\n$$\n$$\n\\hat{\\beta}_j^2 > \\frac{2 \\hat{\\sigma}^2}{n}\n$$\n对两边取平方根（两边都非负），我们得到最终的决策规则：\n$$\n|\\hat{\\beta}_j| > \\sqrt{\\frac{2 \\hat{\\sigma}^2}{n}}\n$$\n这是一个硬阈值规则。最小化 $C_p$ 的子集 $S$ 由所有满足其全模型 OLS 系数估计 $\\hat{\\beta}_j$ 的绝对值大于一个通用阈值 $T = \\sqrt{\\frac{2 \\hat{\\sigma}^2}{n}}$ 的预测变量 $j$ 组成。这个阈值仅依赖于样本大小 $n$ 和方差估计 $\\hat{\\sigma}^2$，而与具体的预测变量索引 $j$ 无关。\n\n因此，这个阈值的精确闭式表达式是 $T = \\sqrt{\\frac{2 \\hat{\\sigma}^2}{n}}$。",
            "answer": "$$\\boxed{\\sqrt{\\frac{2 \\hat{\\sigma}^{2}}{n}}}$$"
        },
        {
            "introduction": "真实世界的数据很少具有完美的正交预测变量。在掌握了理想情况下的$C_p$行为后，本练习将我们带入一个更现实的场景，处理普遍存在的多重共线性问题。你将探索马洛斯$C_p$如何通过“有效自由度”（通过帽子矩阵的迹$tr(H)$来衡量）的概念来优雅地处理这种情况，而不是简单地计算预测变量的数量。这个练习将巩固你的理解：$C_p$中的惩罚项关乎模型的真实复杂度或秩，而非数据矩阵中的列数。",
            "id": "3143701",
            "problem": "考虑一个具有同方差高斯误差的线性回归模型，由 $y = X\\beta + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^{n}$，$X \\in \\mathbb{R}^{n \\times p}$ 包含一个截距项和 $p-1$ 个预测变量，$\\beta \\in \\mathbb{R}^{p}$ 是一个未知的参数向量，并且 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。假设您通过普通最小二乘法（OLS）拟合该模型，将估计量解释为通过 Moore–Penrose 伪逆矩阵到 $X$ 列空间的正交投影，因此拟合值为 $\\hat{y} = H y$，其中 $H = X X^{+}$，$X^{+}$ 表示 Moore–Penrose 伪逆矩阵。矩阵 $H$ 通常被称为帽子矩阵。Mallows' $C_{p}$ (Cp) 是一种模型选择准则，旨在在所述假设下，为观测设计下的样本内预测误差提供一个无偏估计。\n\n给定一个设计，其中有 $n = 25$ 个观测值和 $X$ 中的 $p = 5$ 列：一个截距项和四个预测变量列 $(x_{1}, x_{2}, x_{3}, x_{4})$。预测变量满足一个完全线性冗余关系 $x_{4} = 2 x_{1} - x_{3}$，并且您可以假设截距项列与其他预测变量列线性无关。您使用伪逆矩阵通过 OLS 拟合这个冗余模型，并得到残差平方和 $\\mathrm{RSS} = 180$。已知噪声方差为 $\\sigma^{2} = 9$。\n\n任务：\n1. 从线性最小二乘投影的基本性质和线性平滑器的无偏风险估计出发，解释为什么即使在完全冗余的情况下，$\\mathrm{tr}(H)$ 的值也等于 $X$ 的秩，而不是原始的列数。特别地，论证 Mallows' $C_{p}$ 中的惩罚项取决于 $\\mathrm{tr}(H)$，因此能反映秩亏。\n2. 确定给定 $X$ 的 $\\mathrm{tr}(H)$ 的值。\n3. 使用您的结果，计算此拟合模型的 Mallows' $C_{p}$ 值。\n\n以单个数字的形式提供 $C_{p}$ 的最终数值。无需四舍五入；给出精确值。",
            "solution": "问题要求解释在多重共线性情况下，帽子矩阵的迹在 Mallows' $C_p$ 中的作用，然后为一个给定的秩亏线性模型计算 $C_p$ 的值。\n\n### 第1部分：关于 $\\mathrm{tr}(H)$ 及其在 Mallows' $C_p$ 中作用的解释\n\n在普通最小二乘（OLS）回归中，拟合值由 $\\hat{y} = Hy$ 给出，其中 $H$ 是帽子矩阵。矩阵 $H$ 是到设计矩阵 $X$ 的列空间（记为 $\\mathrm{col}(X)$）上的正交投影矩阵。当 $X$ 是满列秩时，$H = X(X^T X)^{-1}X^T$。在一般情况下，包括秩亏的 $X$，帽子矩阵使用 Moore-Penrose 伪逆矩阵 $X^{+}$ 定义为 $H = XX^{+}$。在这两种情况下，$H$ 都是一个对称且幂等（$H^2 = H$）的投影矩阵。\n\n任何投影矩阵 $P$ 的一个基本性质是其迹等于其秩：$\\mathrm{tr}(P) = \\mathrm{rank}(P)$。由于 $H$ 是到 $\\mathrm{col}(X)$ 上的投影矩阵，它的秩是这个子空间的维度。根据定义，一个矩阵的列空间的维度就是该矩阵的秩。因此，我们有以下等式链：\n$$\n\\mathrm{tr}(H) = \\mathrm{rank}(H) = \\mathrm{dim}(\\mathrm{col}(X)) = \\mathrm{rank}(X)\n$$\n无论 $X$ 是满秩还是秩亏，这个关系都成立。如果 $X \\in \\mathbb{R}^{n \\times p}$ 且 $\\mathrm{rank}(X) = r \\le p$，那么 $\\mathrm{tr}(H) = r$。量 $d = \\mathrm{tr}(H)$ 通常被称为线性拟合的有效自由度。\n\nMallows' $C_p$ 旨在成为由噪声方差 $\\sigma^2$ 缩放的样本内均方预测误差的无偏估计。真实的样本内均方误差是拟合值 $\\hat{y}$ 与真实均值向量 $\\mu = E[y] = X\\beta$ 之间距离平方的期望值。让我们将此误差表示为 $J$：\n$$\nJ = \\frac{1}{\\sigma^2} E\\left[ \\| \\hat{y} - \\mu \\|_2^2 \\right]\n$$\n我们可以将 $\\hat{y} - \\mu$ 表示为 $Hy - \\mu = H(X\\beta + \\varepsilon) - X\\beta = H(X\\beta) + H\\varepsilon - X\\beta$。由于 $\\mu = X\\beta$ 在 $X$ 的列空间中，投影 $H$ 不会改变它，即 $H(X\\beta) = X\\beta$。因此，$\\hat{y} - \\mu = H\\varepsilon$。\n期望值变为：\n$$\nE\\left[ \\| H\\varepsilon \\|_2^2 \\right] = E\\left[ \\varepsilon^T H^T H \\varepsilon \\right] = E\\left[ \\varepsilon^T H \\varepsilon \\right]\n$$\n因为 $H$ 是幂等的（$H^2 = H$）和对称的（$H^T=H$）。使用随机向量二次型的迹恒等式，$E[\\varepsilon^T A \\varepsilon] = \\mathrm{tr}(A \\cdot \\mathrm{Cov}(\\varepsilon)) + E[\\varepsilon]^T A E[\\varepsilon]$，并且给定 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，我们有 $E[\\varepsilon]=0$ 和 $\\mathrm{Cov}(\\varepsilon) = \\sigma^2 I_n$。\n这得到：\n$$\nE\\left[ \\| H\\varepsilon \\|_2^2 \\right] = \\mathrm{tr}(H \\cdot \\sigma^2 I_n) = \\sigma^2 \\mathrm{tr}(H)\n$$\n所以，真实的缩放后样本内预测误差是 $J = \\frac{1}{\\sigma^2} (\\sigma^2 \\mathrm{tr}(H)) = \\mathrm{tr}(H) = d$。\n\nMallows' $C_p$ 统计量的标准定义是：\n$$\nC_p = \\frac{\\mathrm{RSS}}{\\sigma^2} - n + 2d\n$$\n其中 $\\mathrm{RSS} = \\|y - \\hat{y}\\|_2^2$ 是残差平方和，$d = \\mathrm{tr}(H)$。该统计量是 $J=d$ 的一个无偏估计量。为了证明这一点，我们计算它的期望值。残差平方和 RSS 可以写成 $\\mathrm{RSS} = \\|(I-H)y\\|^2 = \\|(I-H)(X\\beta+\\varepsilon)\\|^2 = \\|(I-H)\\varepsilon\\|^2$，因为 $(I-H)X\\beta = 0$。其期望值为：\n$$\nE[\\mathrm{RSS}] = E[\\|(I-H)\\varepsilon\\|^2] = \\sigma^2 \\mathrm{tr}(I-H) = \\sigma^2(n - \\mathrm{tr}(H)) = \\sigma^2(n-d)\n$$\n现在，计算 $C_p$ 统计量的期望值：\n$$\nE[C_p] = E\\left[\\frac{\\mathrm{RSS}}{\\sigma^2} - n + 2d\\right] = \\frac{E[\\mathrm{RSS}]}{\\sigma^2} - n + 2d = \\frac{\\sigma^2(n-d)}{\\sigma^2} - n + 2d = (n-d) - n + 2d = d\n$$\n由于 $E[C_p] = d = J$，所以 $C_p$ 统计量确实是缩放后样本内预测误差的无偏估计量。此表达式中的惩罚项 $2d$ 是有效自由度的两倍，即 $2 \\cdot \\mathrm{tr}(H)$。这明确地表明，惩罚项通过依赖于 $\\mathrm{rank}(X)$ 而不是总列数 $p$ 来正确地调整秩亏。\n\n### 第2部分：确定 $\\mathrm{tr}(H)$ 的值\n\n我们需要求出设计矩阵 $X$ 的秩。矩阵 $X$ 有 $n=25$ 行和 $p=5$ 列。这些列对应于一个截距项和四个预测变量 $(x_1, x_2, x_3, x_4)$。\n设 $X$ 的列为 $c_0, c_1, c_2, c_3, c_4$。\n我们已知预测变量列之间存在一个完全线性相关关系：\n$$\nx_4 = 2x_1 - x_3\n$$\n这意味着列向量 $c_4$ 是列 $c_1$ 和 $c_3$ 的线性组合：$c_4 = 2c_1 - c_3$。这个关系意味着这五列的集合是线性相关的。因此，$X$ 的秩必须小于5。这个相关性使秩至少减少一。\n\n问题暗示这是这些列中唯一的线性相关关系。具体来说，我们假设张成 $X$ 的列空间的列集合 $\\{c_0, c_1, c_2, c_3\\}$ 是线性无关的。\n$X$ 的列空间是 $\\mathrm{col}(X) = \\mathrm{span}\\{c_0, c_1, c_2, c_3, c_4\\}$。由于 $c_4$ 在 $\\{c_1, c_3\\}$ 的张成空间中，我们可以写成 $\\mathrm{col}(X) = \\mathrm{span}\\{c_0, c_1, c_2, c_3\\}$。\n这个空间的维度是基中向量的数量。如果 $\\{c_0, c_1, c_2, c_3\\}$ 是线性无关的，它们就构成了 $\\mathrm{col}(X)$ 的一组基。在这种情况下，列空间的维度是 $4$。\n因此，矩阵 $X$ 的秩为 $4$。\n\n正如在第1部分中确立的，$\\mathrm{tr}(H) = \\mathrm{rank}(X)$。\n因此，$\\mathrm{tr}(H)$ 的值为 $4$。\n\n### 第3部分：计算 Mallows' $C_p$ 的值\n\n当噪声方差 $\\sigma^2$ 已知时，Mallows' $C_p$ 的公式为：\n$$\nC_p = \\frac{\\mathrm{RSS}}{\\sigma^2} - n + 2d\n$$\n其中 $d = \\mathrm{tr}(H)$。\n\n根据问题陈述和我们的分析，我们有以下数值：\n-   残差平方和，$\\mathrm{RSS} = 180$。\n-   噪声方差，$\\sigma^2 = 9$。\n-   观测数量， $n = 25$。\n-   有效自由度， $d = \\mathrm{tr}(H) = 4$。\n\n将这些值代入公式：\n$$\nC_p = \\frac{180}{9} - 25 + 2(4)\n$$\n$$\nC_p = 20 - 25 + 8\n$$\n$$\nC_p = -5 + 8\n$$\n$$\nC_p = 3\n$$\n此拟合模型的 Mallows' $C_p$ 值为 $3$。",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "在建立了坚实的理论基础之后，现在是时候将马洛斯$C_p$应用于实际的数据分析任务了。这个编程练习模拟了一个常见的场景：决定是否向线性模型中添加非线性（二次）项。你将为两个竞争模型实现$C_p$的计算，并利用$C_p$的变化量（$\\Delta C_p$）来做出数据驱动的决策，从而将前面练习中的抽象原则转化为具体的模型选择算法。",
            "id": "3143724",
            "problem": "考虑一个带有线性回归模型的监督学习场景。设响应向量为 $y \\in \\mathbb{R}^n$，预测变量矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，模型中包含一个作为独立参数的截距项。假设采用经典的同方差独立噪声模型，其中误差是独立同分布的 (i.i.d.)，均值为 $0$，方差为 $\\sigma^2$。给定一个误差方差的外部估计值，记为 $\\hat{\\sigma}^2$，您将用它来评估向模型中添加非线性项的效果。具体来说，您将探究当用预测变量的平方项来增广线性模型时，被称为 Mallows $C_p$ 统计量的选择准则的敏感性，并且您将计算从线性模型转换到二次增广模型（即为每个原始预测变量 $x_j$ 添加 $x_j^2$）时 $C_p$ 的变化。\n\n您的任务是编写一个程序，该程序：\n- 使用普通最小二乘法 (OLS) 拟合线性模型，以计算训练残差平方和，称为残差平方和 (RSS)。\n- 使用普通最小二乘法 (OLS) 拟合二次增广模型，其中每个原始预测变量 $x_j$ 都伴随其平方项 $x_j^2$，并再次计算训练 RSS。\n- 将模型参数计数为包括截距在内的估计系数总数。对于一个有 $p$ 个预测变量的线性模型，参数数量为 $1 + p$。对于为每个预测变量添加了平方项（无交互项）的二次增广模型，参数数量为 $1 + p + p = 1 + 2p$。\n- 根据线性回归和误差方差的基本定义和性质，推导出每个模型的 Mallows $C_p$，然后计算 $C_p$ 的变化量，\n$$\\Delta C_p = C_p^{\\text{quad}} - C_p^{\\text{lin}}.$$\n- 根据计算出的 $\\Delta C_p$ 和给定的 $\\hat{\\sigma}^2$，使用一个从第一性原理推导出的基于原则的决策规则，来决定是否应该添加二次项。\n\n您必须根据以下明确定义的测试套件生成合成数据。在每种情况下，生成一个具有独立标准正态分布条目（即每个 $x_{ij} \\sim \\mathcal{N}(0,1)$）的矩阵 $X$，并从一个可能包含线性和二次贡献以及 i.i.d. 高斯噪声的模型中生成 $y$。为保证可复现性，请使用指定的随机种子。在以下所有情况中，截距都包含在数据生成过程中。\n\n测试套件（每种情况提供 $(\\text{seed}, n, p, \\text{intercept}, \\beta, \\gamma, \\hat{\\sigma}^2)$）：\n- 情况 1：seed $= 123$, $n = 50$, $p = 2$, intercept $= 0.5$, 线性系数 $\\beta = [1.0, -2.0]$, 二次系数 $\\gamma = [0.0, 0.0]$, 以及 $\\hat{\\sigma}^2 = 1.0$。数据生成过程为\n$$y = 0.5 + 1.0 \\cdot x_1 - 2.0 \\cdot x_2 + \\varepsilon,$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$。\n- 情况 2：seed $= 456$, $n = 50$, $p = 2$, intercept $= -0.2$, 线性系数 $\\beta = [1.0, -2.0]$, 二次系数 $\\gamma = [0.8, 0.0]$, 以及 $\\hat{\\sigma}^2 = 1.0$。数据生成过程为\n$$y = -0.2 + 1.0 \\cdot x_1 - 2.0 \\cdot x_2 + 0.8 \\cdot x_1^2 + \\varepsilon,$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$。\n- 情况 3：seed $= 789$, $n = 200$, $p = 3$, intercept $= 0.0$, 线性系数 $\\beta = [0.5, -1.0, 0.0]$, 二次系数 $\\gamma = [0.2, 0.2, 0.0]$, 以及 $\\hat{\\sigma}^2 = 1.0$。数据生成过程为\n$$y = 0.0 + 0.5 \\cdot x_1 - 1.0 \\cdot x_2 + 0.2 \\cdot x_1^2 + 0.2 \\cdot x_2^2 + \\varepsilon,$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$。\n- 情况 4：seed $= 42$, $n = 80$, $p = 1$, intercept $= 0.0$, 线性系数 $\\beta = [0.0]$, 二次系数 $\\gamma = [3.0]$, 以及 $\\hat{\\sigma}^2 = 1.0$。数据生成过程为\n$$y = 0.0 + 3.0 \\cdot x_1^2 + \\varepsilon,$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, 1.0)$。\n\n实现细节：\n- 使用普通最小二乘法 (OLS)，其定义为最小化训练残差平方和 (RSS)，并为每个拟合模型计算 $RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$。\n- 在二次增广模型中仅使用平方项（无交互项）。\n- 精确使用每个测试用例中给定的 $\\hat{\\sigma}^2$。\n- 仅当 $\\Delta C_p  0$ 时，判定添加二次项是合理的。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的决策，格式为一个以逗号分隔的布尔值列表，并用方括号括起来（例如，$[\\text{True},\\text{False},\\text{True},\\text{False}]$），其中 $\\text{True}$ 表示添加二次项是合理的，$\\text{False}$ 表示不合理。\n\n您的程序必须完全自包含，并且不得读取任何输入。它必须按上述规定生成合成数据，并遵循概述的规则来产生最终输出。唯一允许使用的库是 Numerical Python (NumPy) 和 Python 标准库。",
            "solution": "手头的问题要求对一个简单的线性回归模型和一个包含二次项的增广模型进行定量比较。目标是根据给定的数据集，决定是否应该包含这些高阶项。决策将基于 Mallows $C_p$ 统计量，这是一个用于模型选择的成熟准则，它在拟合优度和模型复杂度之间进行权衡。\n\n首先，我们形式化数学设置。我们有一个包含 $n$ 个观测值的数据集。对于每个观测值 $i \\in \\{1, 2, \\dots, n\\}$，我们有一个响应 $y_i$ 和 $p$ 个预测变量 $x_{i1}, x_{i2}, \\dots, x_{ip}$。我们可以用响应向量 $y \\in \\mathbb{R}^n$ 和预测变量矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 来表示数据。\n\n我们考虑两个嵌套模型来描述预测变量与响应之间的关系。\n\n1.  线性模型 ($M_{\\text{lin}}$)：该模型假设存在线性关系，并包含一个截距项 $\\beta_0$。模型方程为：\n    $$ y_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij} + \\varepsilon_i $$\n    需要估计的参数是系数 $\\beta_0, \\beta_1, \\dots, \\beta_p$。该模型中的总参数数量为 $d_{\\text{lin}} = p + 1$。\n\n2.  二次增广模型 ($M_{\\text{quad}}$)：该模型通过将每个预测变量的平方作为一个新特征来扩展线性模型。诸如 $x_{ij}x_{ik}$（其中 $j \\neq k$）的交互项被明确排除。模型方程为：\n    $$ y_i = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij} + \\sum_{j=1}^{p} \\gamma_j x_{ij}^2 + \\varepsilon_i $$\n    需要估计的参数是 $\\beta_0$、线性系数 $\\beta_1, \\dots, \\beta_p$ 和二次系数 $\\gamma_1, \\dots, \\gamma_p$。总参数数量为 $d_{\\text{quad}} = 1 + p + p = 2p + 1$。\n\n对于这两个模型，项 $\\varepsilon_i$ 代表误差，它被假设为一个独立同分布 (i.i.d.) 的随机变量，其均值 $E[\\varepsilon_i] = 0$，方差 $Var(\\varepsilon_i) = \\sigma^2$。\n\n这些模型的参数使用普通最小二乘法 (OLS) 进行估计。OLS 找到使残差平方和 (RSS) 最小化的参数值，RSS 是观测响应 $y_i$ 和预测响应 $\\hat{y}_i$ 之间差的平方和。\n$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n在矩阵表示法中，设 $Z$ 为给定模型的设计矩阵（其中包括一个用于截距的前导 1 列）。对于 $M_{\\text{lin}}$，$Z_{\\text{lin}} \\in \\mathbb{R}^{n \\times (p+1)}$。对于 $M_{\\text{quad}}$，$Z_{\\text{quad}} \\in \\mathbb{R}^{n \\times (2p+1)}$。OLS 系数向量 $\\hat{\\theta}$ 是正规方程的解：\n$$ (Z^T Z) \\hat{\\theta} = Z^T y $$\n预测值则为 $\\hat{y} = Z \\hat{\\theta}$，RSS 由 $\\|y - Z\\hat{\\theta}\\|_2^2$ 给出。\n\n由于 $M_{\\text{quad}}$ 具有更多参数并且包含了 $M_{\\text{lin}}$ 的所有项，因此它在训练数据上的拟合效果总是至少与 $M_{\\text{lin}}$ 一样好。这意味着 $\\text{RSS}_{\\text{quad}} \\le \\text{RSS}_{\\text{lin}}$。仅仅一个较小的 RSS 不足以让我们偏好一个更复杂的模型，因为这可能导致过拟合。\n\nMallows $C_p$ 通过惩罚模型复杂度来解决这个问题。它是对新数据上缩放后的期望预测误差的估计。对于一个有 $d$ 个参数的模型，$C_p$ 统计量定义为：\n$$ C_p = \\frac{\\text{RSS}}{\\hat{\\sigma}^2} + 2d - n $$\n这里，$\\hat{\\sigma}^2$ 是真实误差方差 $\\sigma^2$ 的一个估计。一个好的模型其 $C_p$ 值会接近其参数数量 $d$。在比较模型时，我们偏好 $C_p$ 值较低的模型。\n\n问题要求我们计算从线性模型转换到二次模型时 $C_p$ 的变化：\n$$ \\Delta C_p = C_p^{\\text{quad}} - C_p^{\\text{lin}} $$\n代入每个模型的 $C_p$ 定义：\n$$ \\Delta C_p = \\left( \\frac{\\text{RSS}_{\\text{quad}}}{\\hat{\\sigma}^2} + 2d_{\\text{quad}} - n \\right) - \\left( \\frac{\\text{RSS}_{\\text{lin}}}{\\hat{\\sigma}^2} + 2d_{\\text{lin}} - n \\right) $$\n$$ \\Delta C_p = \\frac{\\text{RSS}_{\\text{quad}} - \\text{RSS}_{\\text{lin}}}{\\hat{\\sigma}^2} + 2(d_{\\text{quad}} - d_{\\text{lin}}) $$\n我们有 $d_{\\text{lin}} = p+1$ 和 $d_{\\text{quad}} = 2p+1$，所以参数数量的变化是 $d_{\\text{quad}} - d_{\\text{lin}} = (2p+1) - (p+1) = p$。\n$C_p$ 变化的最终表达式是：\n$$ \\Delta C_p = \\frac{\\text{RSS}_{\\text{quad}} - \\text{RSS}_{\\text{lin}}}{\\hat{\\sigma}^2} + 2p $$\n决策规则是，如果二次增广模型产生更低的 $C_p$ 值，则选择它，这对应于条件 $\\Delta C_p  0$。这个条件可以改写为：\n$$ \\frac{\\text{RSS}_{\\text{lin}} - \\text{RSS}_{\\text{quad}}}{\\hat{\\sigma}^2} > 2p $$\n这个不等式提供了一个清晰的原则：残差平方和的减少量，经噪声方差估计值缩放后，必须大于所增加参数数量的两倍。这反映了偏差与方差之间的权衡。RSS 的大幅减少表明增加的二次项正在捕捉数据中的真实结构（减少偏差），如果这个减少量足够大以克服复杂度惩罚，则新模型是更优的选择。\n\n每个测试用例的计算过程如下：\n1.  设置随机种子以保证可复现性。\n2.  生成 $n \\times p$ 的预测变量矩阵 $X$，其条目从 $\\mathcal{N}(0,1)$ 中抽取。\n3.  使用指定的数据生成过程生成响应向量 $y$，该过程包括截距、线性项、二次项（如果有）和高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$，其中所有测试用例的 $\\sigma^2=1.0$。\n4.  通过在 $X$ 前面加上一列 1 来构建线性模型的设计矩阵 $Z_{\\text{lin}}$。\n5.  对 $y$ 和 $Z_{\\text{lin}}$ 进行 OLS 回归，以获得 $\\text{RSS}_{\\text{lin}}$。\n6.  通过将 $X$ 和 $X^2$（逐元素平方）连接起来，并在前面加上一列 1 来构建二次模型的设计矩阵 $Z_{\\text{quad}}$。\n7.  对 $y$ 和 $Z_{\\text{quad}}$ 进行 OLS 回归，以获得 $\\text{RSS}_{\\text{quad}}$。\n8.  使用给定的 $p$ 和 $\\hat{\\sigma}^2$ 值，计算 $\\Delta C_p = (\\text{RSS}_{\\text{quad}} - \\text{RSS}_{\\text{lin}}) / \\hat{\\sigma}^2 + 2p$。\n9.  通过检查 $\\Delta C_p  0$ 来确定是否应该添加二次项。记录布尔结果。\n对所有四个测试用例重复此过程，以生成最终输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of deciding whether to augment a linear model with quadratic terms\n    based on the change in Mallows' Cp statistic for four specified test cases.\n    \"\"\"\n    \n    # Each tuple contains: (seed, n, p, intercept, beta, gamma, sigma_hat_sq)\n    test_cases = [\n        (123, 50, 2, 0.5, np.array([1.0, -2.0]), np.array([0.0, 0.0]), 1.0),\n        (456, 50, 2, -0.2, np.array([1.0, -2.0]), np.array([0.8, 0.0]), 1.0),\n        (789, 200, 3, 0.0, np.array([0.5, -1.0, 0.0]), np.array([0.2, 0.2, 0.0]), 1.0),\n        (42, 80, 1, 0.0, np.array([0.0]), np.array([3.0]), 1.0)\n    ]\n\n    results = []\n\n    for case in test_cases:\n        seed, n, p, intercept, beta, gamma, sigma_hat_sq = case\n\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Generate synthetic data\n        # Generate predictor matrix X with standard normal entries\n        X = np.random.randn(n, p)\n        \n        # Generate response vector y based on the true model\n        # The true error variance is 1.0 for all cases\n        true_sigma = 1.0\n        epsilon = np.random.randn(n) * true_sigma\n        \n        # Calculate the true mean response\n        mu = intercept + X @ beta + (X**2) @ gamma\n        \n        y = mu + epsilon\n\n        # 2. Fit the linear model\n        # Construct the design matrix with an intercept term\n        Z_lin = np.c_[np.ones(n), X]\n        \n        # Perform OLS using np.linalg.lstsq\n        # The function returns coefficients, RSS, rank, and singular values.\n        # The second return value is an array containing the RSS.\n        _, rss_lin_array, _, _ = np.linalg.lstsq(Z_lin, y, rcond=None)\n        rss_lin = rss_lin_array[0]\n\n        # 3. Fit the quadratic-augmented model\n        # Construct the design matrix with intercept, linear, and squared terms\n        X_quad_features = np.c_[X, X**2]\n        Z_quad = np.c_[np.ones(n), X_quad_features]\n        \n        # Perform OLS for the quadratic model\n        _, rss_quad_array, _, _ = np.linalg.lstsq(Z_quad, y, rcond=None)\n        rss_quad = rss_quad_array[0]\n\n        # 4. Compute Delta Cp and make a decision\n        # The change in the number of parameters is p\n        # delta_d = (2*p + 1) - (p + 1) = p\n        \n        # Mallows' Cp formula leads to: Delta_Cp = (RSS_quad - RSS_lin)/sigma_hat^2 + 2*p\n        delta_cp = (rss_quad - rss_lin) / sigma_hat_sq + 2 * p\n        \n        # Decision: quadratic terms are warranted if Delta_Cp  0\n        is_warranted = delta_cp  0\n        results.append(is_warranted)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}