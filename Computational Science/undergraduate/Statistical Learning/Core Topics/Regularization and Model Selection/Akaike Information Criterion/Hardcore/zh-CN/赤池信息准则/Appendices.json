{
    "hands_on_practices": [
        {
            "introduction": "在分类问题中，一个基本问题是：不同类别的数据是否共享相同的结构（即协方差）？此练习将让你使用赤池信息准则（$AIC$）在简单模型（线性判别分析）和更复杂模型（二次判别分析）之间做出由数据驱动的决策。通过这个实践，你将更深刻地理解$AIC$如何在拟合多维高斯模型时，权衡模型拟合度和参数复杂性。",
            "id": "3097943",
            "problem": "一个二分类高斯判别分析 (GDA) 模型假设，每个带标签的观测值 $(x_i, y_i)$ 的生成过程如下：首先从具有类别概率 $(\\pi_0, \\pi_1)$ 的分类分布中抽取一个类别标签 $y_i \\in \\{0,1\\}$，然后根据 $y_i$ 从一个多元正态分布中抽取特征向量 $x_i \\in \\mathbb{R}^p$。存在两种常见的模型族：一种是各类别间协方差相等的模型（通常称为线性判别分析 (LDA)），另一种是各类别具有特定协方差的模型（通常称为二次判别分析 (QDA)）。给定带标签的数据和两个候选模型族，请选择使赤池信息准则 (Akaike Information Criterion, AIC) 最小化的那个模型族。赤池信息准则 (AIC) 是一个经典的信息论模型选择分数，由似然函数的最大值和自由参数的数量构建而成。\n\n您的程序必须使用最大似然估计法为每个候选模型族中的所有参数实现以下功能：\n\n- 对于协方差相等族：估计一个跨类别共享的协方差矩阵，以及特定于类别的均值向量和类别先验概率。\n- 对于协方差不相等族：为每个类别估计一个单独的协方差矩阵，以及特定于类别的均值向量和类别先验概率。\n- 对于每个模型族，在估计出的参数下计算带标签数据集的联合数据对数似然，然后计算该族的 AIC。选择 AIC 值较小的模型族。\n\n所有计算都必须根据上述生成性描述以纯数学术语执行。数据集以合成高斯数据的形式提供，使用固定种子生成以保证可复现性。对于每个测试用例，数据是通过连接来自两个多元正态分布的样本而构建的，这些分布具有指定的均值、协方差和已知的类别标签。协方差矩阵通过其类 Cholesky 因子给出以保证正定性：对于每个类别 $k \\in \\{0,1\\}$，协方差定义为 $\\Sigma_k = L_k L_k^{\\top}$。\n\n您的程序应生成单行输出，包含一个布尔值列表，每个测试用例对应一个布尔值。如果选择了协方差不相等族，则布尔值为 $\\texttt{True}$，否则为 $\\texttt{False}$。该行必须严格格式化为逗号分隔的 Python 列表，例如 $[\\texttt{True},\\texttt{False},\\texttt{True}]$。\n\n实现并评估以下测试套件。在所有情况下，使用指定的随机种子生成数据，并为类别 0 生成 $n_0$ 个样本，为类别 1 生成 $n_1$ 个样本。\n\n- 测试用例 1（理想情况，类别不平衡，协方差差异显著）：\n  - 维度 $p = 2$。\n  - 样本数量：$n_0 = 140$，$n_1 = 35$。\n  - 随机种子：$202311$。\n  - 均值：$\\mu_0 = [0, 0]$，$\\mu_1 = [2, -2]$。\n  - 协方差因子：\n    - $L_0 = \\begin{bmatrix} 1.0  0.0 \\\\ 0.3  0.7 \\end{bmatrix}$，因此 $\\Sigma_0 = L_0 L_0^{\\top}$。\n    - $L_1 = \\begin{bmatrix} 1.6  0.0 \\\\ -0.4  1.0 \\end{bmatrix}$，因此 $\\Sigma_1 = L_1 L_1^{\\top}$。\n\n- 测试用例 2（协方差几乎相等的临界情况）：\n  - 维度 $p = 3$。\n  - 样本数量：$n_0 = 100$，$n_1 = 60$。\n  - 随机种子：$202312$。\n  - 均值：$\\mu_0 = [0, 0, 0]$，$\\mu_1 = [0.2, -0.1, 0.1]$。\n  - 协方差因子：\n    - $L_0 = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.05  1.0  0.0 \\\\ 0.02  0.03  1.0 \\end{bmatrix}$，因此 $\\Sigma_0 = L_0 L_0^{\\top}$。\n    - $L_1 = \\begin{bmatrix} 1.02  0.0  0.0 \\\\ 0.05  1.05  0.0 \\\\ 0.02  0.03  0.95 \\end{bmatrix}$，因此 $\\Sigma_1 = L_1 L_1^{\\top}$。\n\n- 测试用例 3（高维度的边缘情况，协方差差异强烈，类别不平衡）：\n  - 维度 $p = 4$。\n  - 样本数量：$n_0 = 120$，$n_1 = 30$。\n  - 随机种子：$202313$。\n  - 均值：$\\mu_0 = [0, 0, 0, 0]$，$\\mu_1 = [1.5, -1.5, 1.0, 0.5]$。\n  - 协方差因子：\n    - $L_0 = \\begin{bmatrix} 1.0  0.0  0.0  0.0 \\\\ 0.1  1.0  0.0  0.0 \\\\ 0.0  0.05  1.0  0.0 \\\\ 0.0  0.0  0.02  1.0 \\end{bmatrix}$，因此 $\\Sigma_0 = L_0 L_0^{\\top}$。\n    - $L_1 = \\begin{bmatrix} 1.7  0.0  0.0  0.0 \\\\ 0.5  1.3  0.0  0.0 \\\\ 0.1  0.4  1.1  0.0 \\\\ 0.0  0.1  -0.2  1.0 \\end{bmatrix}$，因此 $\\Sigma_1 = L_1 L_1^{\\top}$。\n\n要求与说明：\n\n- 使用带标签的数据，在每个模型族下通过最大似然估计法来估计参数。\n- 将类别先验概率 $(\\pi_0,\\pi_1)$ 视为待从标签中估计的参数。\n- 在构建模型选择分数时，恰当地计算参数数量：根据模型族包括类别先验、类别均值和协方差矩阵的条目。\n- 不涉及角度；不涉及物理单位。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如，$[\\texttt{True},\\texttt{False},\\texttt{True}]$），其中每个布尔值表示是否为相应的测试用例选择了协方差不相等族。",
            "solution": "用户希望在两种高斯判别分析 (GDA) 模型族之间进行选择——一种是所有类别共享一个协方差矩阵（类似于线性判别分析，LDA），另一种是每个类别拥有特定的协方差矩阵（类似于二次判别分析，QDA）。选择标准是必须最小化的赤池信息准則 (AIC)。解决方案涉及推导和实现每个模型族参数的最大似然估计量 (MLEs)，计算最大化后的对数似然，然后计算 AIC 以进行比较。\n\n设数据集为 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$，其中 $x_i \\in \\mathbb{R}^p$ 是特征向量，$y_i \\in \\{0, 1\\}$ 是类别标签。设 $n_k$ 是类别 $k$ 中的样本数量，因此 $N = n_0 + n_1$。生成模型假设 $y \\sim \\text{Categorical}(\\pi_0, \\pi_1)$ 且 $x|y=k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$。\n\n数据的总对数似然由下式给出：\n$$ \\ln \\mathcal{L}(\\theta | \\mathcal{D}) = \\sum_{i=1}^N \\ln p(x_i, y_i | \\theta) = \\sum_{i=1}^N \\left[ \\ln p(y_i | \\theta) + \\ln p(x_i | y_i, \\theta) \\right] $$\n此表达式可分为类别标签项和特征项。\n\n_1. 公共参数的最大似然估计_\n类别先验概率 $\\pi_k$ 和类别均值 $\\mu_k$ 的最大似然估计量 (MLEs) 对于两种模型族是相同的。\n标签的对数似然为 $\\sum_{i=1}^N \\ln p(y_i) = n_0 \\ln \\pi_0 + n_1 \\ln \\pi_1$。在约束条件 $\\pi_0 + \\pi_1 = 1$ 下最大化此式可得：\n$$ \\hat{\\pi}_k = \\frac{n_k}{N} $$\n特征的对数似然按类别分开。对于每个类别 $k$，均值向量 $\\mu_k$ 的最大似然估计量是该类别中数据的样本均值：\n$$ \\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i: y_i=k} x_i $$\n\n_2. 协方差不相等模型 (QDA 族)_\n在此模型中，每个类别都有自己的协方差矩阵，$\\Sigma_0$ 和 $\\Sigma_1$。\n\n_参数估计_：每个协方差矩阵 $\\Sigma_k$ 的最大似然估计量是相应类别数据的样本协方差矩阵：\n$$ \\hat{\\Sigma}_k = \\frac{1}{n_k} \\sum_{i: y_i=k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^\\top $$\n\n_参数数量 ($k_{QDA}$)_：\n- 类别先验：$1$ 个参数（因为 $\\pi_0+\\pi_1=1$）。\n- 类别均值：$2$ 个大小为 $p$ 的向量，因此是 $2p$ 个参数。\n- 协方差矩阵：$2$ 个对称的 $p \\times p$ 矩阵。每个矩阵有 $p(p+1)/2$ 个独立参数。总计为 $2 \\times \\frac{p(p+1)}{2} = p(p+1)$ 个参数。\n总参数数量：$k_{QDA} = 1 + 2p + p(p+1) = p^2 + 3p + 1$。\n\n_最大化对数似然_：将最大似然估计量代入多元正态分布的对数似然函数中，类别 $k$ 的表达式简化为：\n$$ \\ln \\hat{\\mathcal{L}}_k = -\\frac{n_k}{2} \\left( p \\ln(2\\pi) + \\ln|\\hat{\\Sigma}_k| + p \\right) $$\nQDA 模型的总最大化对数似然是标签对数似然与每个类别特征对数似然的总和：\n$$ \\ln \\hat{\\mathcal{L}}_{QDA} = \\left( \\sum_{k=0}^1 n_k \\ln \\hat{\\pi}_k \\right) + \\left( \\sum_{k=0}^1 -\\frac{n_k}{2} (p \\ln(2\\pi) + \\ln|\\hat{\\Sigma}_k| + p) \\right) $$\n\n_3. 协方差相等模型 (LDA 族)_\n在此模型中，所有类别共享一个单一的协方差矩阵 $\\Sigma$。\n\n_参数估计_：共享协方差矩阵 $\\Sigma$ 的最大似然估计量是按类别大小加权的合并协方差矩阵：\n$$ \\hat{\\Sigma}_{pool} = \\frac{1}{N} \\sum_{k=0}^1 \\sum_{i: y_i=k} (x_i - \\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^\\top = \\frac{n_0 \\hat{\\Sigma}_0 + n_1 \\hat{\\Sigma}_1}{N} $$\n\n_参数数量 ($k_{LDA}$)_：\n- 类别先验：$1$ 个参数。\n- 类别均值：$2p$ 个参数。\n- 协方差矩阵：$1$ 个对称的 $p \\times p$ 矩阵，有 $p(p+1)/2$ 个独立参数。\n总参数数量：$k_{LDA} = 1 + 2p + \\frac{p(p+1)}{2}$。\n\n_最大化对数似然_：其逻辑与 QDA 情况类似，但对所有 $N$ 个数据点使用单一的合并协方差矩阵 $\\hat{\\Sigma}_{pool}$：\n$$ \\ln \\hat{\\mathcal{L}}_{LDA} = \\left( \\sum_{k=0}^1 n_k \\ln \\hat{\\pi}_k \\right) - \\frac{N}{2} \\left( p \\ln(2\\pi) + \\ln|\\hat{\\Sigma}_{pool}| + p \\right) $$\n\n_4. 使用 AIC进行模型选择_\n赤池信息准则定义为 $AIC = 2k - 2 \\ln \\hat{\\mathcal{L}}$，其中 $k$ 是估计参数的数量，$\\hat{\\mathcal{L}}$ 是似然函数的最大值。我们为每个模型计算 AIC：\n$$ AIC_{QDA} = 2k_{QDA} - 2 \\ln \\hat{\\mathcal{L}}_{QDA} $$\n$$ AIC_{LDA} = 2k_{LDA} - 2 \\ln \\hat{\\mathcal{L}}_{LDA} $$\nAIC 值较低的模型更优。如果 $AIC_{QDA}  AIC_{LDA}$，程序将返回 `True`，否则返回 `False`。此过程应用于每个测试用例，以生成最终的布尔结果列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_aic_selection(p, n0, n1, seed, mu0, mu1, L0, L1):\n    \"\"\"\n    Performs GDA model selection for a single test case.\n\n    Selects between an equal-covariance (LDA-like) and an unequal-covariance\n    (QDA-like) model using the Akaike Information Criterion (AIC).\n    \"\"\"\n\n    # 1. Generate synthetic data based on the problem specification\n    rng = np.random.default_rng(seed)\n    N = n0 + n1\n    \n    mu0_true = np.array(mu0)\n    mu1_true = np.array(mu1)\n    L0_true = np.array(L0)\n    L1_true = np.array(L1)\n    \n    Sigma0_true = L0_true @ L0_true.T\n    Sigma1_true = L1_true @ L1_true.T\n    \n    # Generate data for each class\n    X0 = rng.multivariate_normal(mu0_true, Sigma0_true, size=n0)\n    X1 = rng.multivariate_normal(mu1_true, Sigma1_true, size=n1)\n\n    # 2. Compute parameters and AIC for each model\n    # The term for the log-likelihood of labels is common to both models,\n    # so it's calculated once.\n    pi0_hat = n0 / N\n    pi1_hat = n1 / N\n    logL_labels = n0 * np.log(pi0_hat) + n1 * np.log(pi1_hat)\n\n    # --- Unequal-Covariance Model (QDA) ---\n    \n    # Number of parameters: 1 (priors) + 2*p (means) + p*(p+1) (two cov matrices)\n    k_qda = 1 + 2 * p + p * (p + 1)\n    \n    # MLE for class-specific covariance matrices\n    Sigma0_hat = np.cov(X0, rowvar=False, bias=True)\n    Sigma1_hat = np.cov(X1, rowvar=False, bias=True)\n    \n    # Log-determinants of the covariance matrices\n    _, log_det_S0 = np.linalg.slogdet(Sigma0_hat)\n    _, log_det_S1 = np.linalg.slogdet(Sigma1_hat)\n    \n    # Maximized log-likelihood for features under the QDA model\n    logL_feat_qda = -0.5 * n0 * (p * np.log(2 * np.pi) + log_det_S0 + p)\n    logL_feat_qda += -0.5 * n1 * (p * np.log(2 * np.pi) + log_det_S1 + p)\n    \n    # Total maximized log-likelihood and AIC for QDA\n    logL_qda = logL_labels + logL_feat_qda\n    aic_qda = 2 * k_qda - 2 * logL_qda\n\n    # --- Equal-Covariance Model (LDA) ---\n    \n    # Number of parameters: 1 (priors) + 2*p (means) + p*(p+1)/2 (one cov matrix)\n    k_lda = 1 + 2 * p + p * (p + 1) / 2\n    \n    # MLE for the pooled covariance matrix\n    Sigma_pool_hat = (n0 * Sigma0_hat + n1 * Sigma1_hat) / N\n    \n    # Log-determinant of the pooled covariance matrix\n    _, log_det_Spool = np.linalg.slogdet(Sigma_pool_hat)\n    \n    # Maximized log-likelihood for features under the LDA model\n    logL_feat_lda = -0.5 * N * (p * np.log(2 * np.pi) + log_det_Spool + p)\n\n    # Total maximized log-likelihood and AIC for LDA\n    logL_lda = logL_labels + logL_feat_lda\n    aic_lda = 2 * k_lda - 2 * logL_lda\n\n    # 3. Model Selection\n    # Return True if the unequal-covariance model (QDA) has a lower AIC\n    return aic_qda  aic_lda\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"p\": 2, \"n0\": 140, \"n1\": 35, \"seed\": 202311,\n            \"mu0\": [0, 0], \"mu1\": [2, -2],\n            \"L0\": [[1.0, 0.0], [0.3, 0.7]],\n            \"L1\": [[1.6, 0.0], [-0.4, 1.0]],\n        },\n        {\n            \"p\": 3, \"n0\": 100, \"n1\": 60, \"seed\": 202312,\n            \"mu0\": [0, 0, 0], \"mu1\": [0.2, -0.1, 0.1],\n            \"L0\": [[1.0, 0.0, 0.0], [0.05, 1.0, 0.0], [0.02, 0.03, 1.0]],\n            \"L1\": [[1.02, 0.0, 0.0], [0.05, 1.05, 0.0], [0.02, 0.03, 0.95]],\n        },\n        {\n            \"p\": 4, \"n0\": 120, \"n1\": 30, \"seed\": 202313,\n            \"mu0\": [0, 0, 0, 0], \"mu1\": [1.5, -1.5, 1.0, 0.5],\n            \"L0\": [[1.0, 0.0, 0.0, 0.0], [0.1, 1.0, 0.0, 0.0], [0.0, 0.05, 1.0, 0.0], [0.0, 0.0, 0.02, 1.0]],\n            \"L1\": [[1.7, 0.0, 0.0, 0.0], [0.5, 1.3, 0.0, 0.0], [0.1, 0.4, 1.1, 0.0], [0.0, 0.1, -0.2, 1.0]],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        decision = _calculate_aic_selection(\n            p=case[\"p\"],\n            n0=case[\"n0\"],\n            n1=case[\"n1\"],\n            seed=case[\"seed\"],\n            mu0=case[\"mu0\"],\n            mu1=case[\"mu1\"],\n            L0=case[\"L0\"],\n            L1=case[\"L1\"]\n        )\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "$AIC$的应用不局限于传统的统计模型，它是一个能跨越不同科学领域的通用工具。本练习将带你进入可靠性工程领域，使用$AIC$来判断一个简单的指数模型是否足以描述产品失效时间，还是有必要使用更灵活的威布尔模型。这个实践突显了$AIC$在嵌套模型选择中的作用，即通过评估额外参数是否显著提升模型的解释能力，来决定是否选择更复杂的模型。",
            "id": "3097983",
            "problem": "在可靠性工程中，给定独立的故障时间观测值，您需要为每个数据集判断，通过威布尔模型引入形状参数是否比指数模型具有更强的解释力。您的决策必须基于一个有原则的、基于推导的模型选择准则，该准则以信息论风险和最大似然为基础，而非临时的拟合优度度量。\n\n假设非负故障时间服从以下参数族：\n\n- 指数模型，其率参数为 $\\beta  0$，概率密度函数为 $f_{\\text{exp}}(t \\mid \\beta)$，定义于 $t  0$。\n- 威布尔模型，其形状参数为 $k  0$，尺度参数为 $\\lambda  0$，概率密度函数为 $f_{\\text{weib}}(t \\mid k,\\lambda)$，定义于 $t  0$。\n\n假设每个数据集中的所有观测值在模型参数给定的条件下都是独立同分布的。您的模型选择应基于连续分布以及最大似然估计 (MLE) 和 Kullback–Leibler 散度的框架，使用一个渐近地偏好最小化预期 Kullback–Leibler 风险的模型的准则。该准则必须依赖于最大化对数似然，并包含一个与模型中自由参数数量成正比的惩罚项。不要使用任何启发式或图形化方法。\n\n对于每个数据集，您的程序必须：\n- 在指数模型和威布尔模型下，通过最大化对数似然来估计参数。\n- 计算从最大化对数似然的渐近偏差推导出的信息论模型选择准则，该准则包含对自由参数数量的惩罚项。\n- 当且仅当威布尔模型的准则值比指数模型的准则值严格小至少一个微小的数值容差 $\\epsilon$（其中 $\\epsilon = 10^{-8}$）时，判定威布尔形状参数提升了解释力。\n\n所有故障时间均以小时为单位。无需进行单位转换；只需将时间视为实值正数。最终输出为布尔值；因此，输出中无需报告物理单位。\n\n测试套件：\n使用以下五个观测到的故障时间（单位：小时）数据集：\n- 测试用例 $1$：$[0.7, 1.1, 1.6, 2.2, 3.0, 3.8, 4.7, 5.7, 6.8, 8.0, 9.3, 10.7, 12.2, 13.8, 15.5]$。\n- 测试用例 $2$：$[0.05, 0.16, 0.20, 0.35, 0.42, 0.59, 0.60, 0.83, 0.90, 1.31, 1.39, 1.60, 2.50]$。\n- 测试用例 $3$：$[0.2, 0.3, 0.5, 0.9, 1.5, 2.5, 4.0, 6.8, 11.0, 18.0]$。\n- 测试用例 $4$：$[1.5, 0.7, 2.1, 1.0, 0.4, 3.3, 2.8, 1.2, 0.9, 2.0, 1.8, 2.4, 0.6, 3.0]$。\n- 测试用例 $5$：$[1.0, 2.0, 3.0]$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表。每个元素都应是一个布尔值，指示对于相应的测试用例，在所用准则下，添加威布尔形状参数是否提升了解释力。例如，输出应类似于 $[b_1,b_2,b_3,b_4,b_5]$，其中每个 $b_i$ 为 $\\text{True}$ 或 $\\text{False}$。",
            "solution": "用户提供了一个适定的统计模型选择问题，该问题基于可靠性工程和信息论的原理。任务是比较两个用于故障时间数据的嵌套参数模型——指数分布和威布尔分布——并为几个数据集决定哪个模型提供了更好的拟合。决策准则被指定为基于最大化似然函数，并根据模型参数的数量进行惩罚，这正是赤池信息准则 (Akaike Information Criterion, AIC) 的直接描述。\n\n验证过程确认了该问题在科学上是合理的、适定的和客观的。它包含了所有必要的信息，并且没有矛盾或含糊之处。我现在将着手进行解法的形式化推导。\n\n问题的核心在于应用赤池信息准则 ($AIC$)。对于一个给定的模型，$AIC$ 定义为：\n$$AIC = 2p - 2\\mathcal{L}_{\\text{max}}$$\n其中 $p$ 是模型中估计参数的数量，$\\mathcal{L}_{\\text{max}}$ 是模型对数似然函数的最大化值。具有较低 $AIC$ 值的模型被认为在拟合优度和模型复杂度之间有更好的权衡。\n\n假设一个故障时间数据集表示为 $T = \\{t_1, t_2, \\ldots, t_n\\}$，其中 $n$ 是观测值的数量。我们假设观测值是独立同分布的。\n\n**模型1：指数模型**\n指数分布有一个单一的率参数 $\\beta  0$。其概率密度函数 ($PDF$) 为：\n$$f_{\\text{exp}}(t \\mid \\beta) = \\beta e^{-\\beta t} \\quad \\text{for } t  0$$\n参数数量为 $p_{\\text{exp}} = 1$。数据集 $T$ 的似然函数为：\n$$L(\\beta \\mid T) = \\prod_{i=1}^{n} f_{\\text{exp}}(t_i \\mid \\beta) = \\prod_{i=1}^{n} \\beta e^{-\\beta t_i} = \\beta^n e^{-\\beta \\sum_{i=1}^{n} t_i}$$\n对数似然函数为：\n$$\\ell_{\\text{exp}}(\\beta \\mid T) = \\ln(L(\\beta \\mid T)) = n \\ln(\\beta) - \\beta \\sum_{i=1}^{n} t_i$$\n为了找到 $\\beta$ 的最大似然估计 (MLE)，我们对 $\\beta$ 求导并将结果设为零：\n$$\\frac{\\partial \\ell_{\\text{exp}}}{\\partial \\beta} = \\frac{n}{\\beta} - \\sum_{i=1}^{n} t_i = 0$$\n解出 $\\beta$ 得到其 MLE $\\hat{\\beta}$：\n$$\\hat{\\beta} = \\frac{n}{\\sum_{i=1}^{n} t_i} = \\frac{1}{\\bar{t}}$$\n其中 $\\bar{t}$ 是故障时间的样本均值。\n将 $\\hat{\\beta}$ 代回对数似然函数，得到最大化对数似然 $\\mathcal{L}_{\\text{exp}}$：\n$$\\mathcal{L}_{\\text{exp}} = \\ell_{\\text{exp}}(\\hat{\\beta} \\mid T) = n \\ln(\\hat{\\beta}) - \\hat{\\beta} \\sum_{i=1}^{n} t_i = n \\ln\\left(\\frac{1}{\\bar{t}}\\right) - \\frac{1}{\\bar{t}}(n\\bar{t}) = -n \\ln(\\bar{t}) - n$$\n因此，指数模型的 $AIC$ 为：\n$$AIC_{\\text{exp}} = 2p_{\\text{exp}} - 2\\mathcal{L}_{\\text{exp}} = 2(1) - 2(-n \\ln(\\bar{t}) - n) = 2 + 2n \\ln(\\bar{t}) + 2n$$\n\n**模型2：威布尔模型**\n威布尔分布有一个形状参数 $k  0$ 和一个尺度参数 $\\lambda  0$。其 $PDF$ 为：\n$$f_{\\text{weib}}(t \\mid k, \\lambda) = \\frac{k}{\\lambda} \\left(\\frac{t}{\\lambda}\\right)^{k-1} e^{-(t/\\lambda)^k} \\quad \\text{for } t  0$$\n参数数量为 $p_{\\text{weib}} = 2$。数据集 $T$ 的对数似然函数为：\n$$\\ell_{\\text{weib}}(k, \\lambda \\mid T) = \\sum_{i=1}^{n} \\ln\\left[\\frac{k}{\\lambda} \\left(\\frac{t_i}{\\lambda}\\right)^{k-1} e^{-(t_i/\\lambda)^k}\\right]$$\n$$ = \\sum_{i=1}^{n} \\left[ \\ln(k) - k\\ln(\\lambda) + (k-1)\\ln(t_i) - \\left(\\frac{t_i}{\\lambda}\\right)^k \\right]$$\n$$ = n\\ln(k) - nk\\ln(\\lambda) + (k-1)\\sum_{i=1}^{n}\\ln(t_i) - \\lambda^{-k}\\sum_{i=1}^{n}t_i^k$$\n为了找到 MLE $(\\hat{k}, \\hat{\\lambda})$，我们对 $k$ 和 $\\lambda$ 求偏导数并将其设为零。对 $\\lambda$ 的导数产生了一个方便的关系：\n$$\\frac{\\partial \\ell_{\\text{weib}}}{\\partial \\lambda} = -\\frac{nk}{\\lambda} + \\frac{k}{\\lambda^{k+1}}\\sum_{i=1}^{n}t_i^k = 0 \\implies \\lambda^k = \\frac{1}{n}\\sum_{i=1}^{n}t_i^k$$\n这个方程提供了 $\\lambda$ 作为 $k$ 的函数的 MLE：$\\hat{\\lambda}(k) = \\left(\\frac{1}{n}\\sum_{i=1}^{n}t_i^k\\right)^{1/k}$。\n将其代回对数似然函数，得到一个仅依赖于 $k$ 的剖面对数似然。对该剖面对数似然函数关于 $k$ 求导并设为零，得到以下关于 MLE $\\hat{k}$ 的隐式方程：\n$$\\frac{1}{k} - \\frac{\\sum_{i=1}^{n} t_i^k \\ln(t_i)}{\\sum_{i=1}^{n} t_i^k} + \\frac{1}{n}\\sum_{i=1}^{n}\\ln(t_i) = 0$$\n该方程无法以闭合形式求解 $k$。因此，必须使用求根算法数值求解 $\\hat{k}$。\n一旦确定了 $\\hat{k}$，就可以使用推导出的公式计算 $\\hat{\\lambda}$：\n$$\\hat{\\lambda} = \\left(\\frac{1}{n}\\sum_{i=1}^{n}t_i^{\\hat{k}}\\right)^{1/\\hat{k}}$$\n然后，通过将 $(\\hat{k}, \\hat{\\lambda})$ 代入对数似然函数来计算最大化对数似然 $\\mathcal{L}_{\\text{weib}}$：\n$$\\mathcal{L}_{\\text{weib}} = n\\ln(\\hat{k}) - n\\hat{k}\\ln(\\hat{\\lambda}) + (\\hat{k}-1)\\sum_{i=1}^{n}\\ln(t_i) - \\hat{\\lambda}^{-\\hat{k}}\\sum_{i=1}^{n}t_i^{\\hat{k}}$$\n使用 $\\hat{\\lambda}$ 的表达式，最后一项简化为 $\\hat{\\lambda}^{-\\hat{k}}\\sum_{i=1}^{n}t_i^{\\hat{k}} = (\\frac{1}{n}\\sum t_i^{\\hat{k}})^{-1} \\sum t_i^{\\hat{k}} = n$。所以，\n$$\\mathcal{L}_{\\text{weib}} = n\\ln(\\hat{k}) - n\\hat{k}\\ln(\\hat{\\lambda}) + (\\hat{k}-1)\\sum_{i=1}^{n}\\ln(t_i) - n$$\n威布尔模型的 $AIC$ 为：\n$$AIC_{\\text{weib}} = 2p_{\\text{weib}} - 2\\mathcal{L}_{\\text{weib}} = 2(2) - 2\\mathcal{L}_{\\text{weib}} = 4 - 2\\mathcal{L}_{\\text{weib}}$$\n\n**决策准则**\n问题陈述指出，当且仅当威布尔模型的准则值比指数模型的准则值严格小至少 $\\epsilon = 10^{-8}$ 时，引入威布尔形状参数才算提升了解释力。这可转换为以下条件：\n$$AIC_{\\text{weib}}  AIC_{\\text{exp}} - \\epsilon$$\n对于每个测试用例，我们将计算 $AIC_{\\text{exp}}$ 和 $AIC_{\\text{weib}}$，并应用此规则来确定是否偏好更复杂的威布尔模型。\n\n**实现算法**\n对于每个数据集：\n1.  将输入的故障时间列表转换为 `numpy` 数组。\n2.  使用解析公式计算 $AIC_{\\text{exp}}$。\n3.  定义威布尔形状参数 $\\hat{k}$ 的隐式函数。\n4.  使用数值求根器（如 `scipy.optimize.root_scalar`）求解 $\\hat{k}$。\n5.  计算相应的尺度参数 $\\hat{\\lambda}$。\n6.  计算最大化对数似然 $\\mathcal{L}_{\\text{weib}}$。\n7.  计算 $AIC_{\\text{weib}}$。\n8.  使用指定的决策规则比较 $AIC_{\\text{weib}}$ 和 $AIC_{\\text{exp}}$，并将得到的布尔值附加到结果列表中。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for the provided test cases.\n    It compares the an exponential model and a Weibull model\n    for failure time data using the Akaike Information Criterion (AIC).\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        [0.7, 1.1, 1.6, 2.2, 3.0, 3.8, 4.7, 5.7, 6.8, 8.0, 9.3, 10.7, 12.2, 13.8, 15.5],\n        # Test Case 2\n        [0.05, 0.16, 0.20, 0.35, 0.42, 0.59, 0.60, 0.83, 0.90, 1.31, 1.39, 1.60, 2.50],\n        # Test Case 3\n        [0.2, 0.3, 0.5, 0.9, 1.5, 2.5, 4.0, 6.8, 11.0, 18.0],\n        # Test Case 4\n        [1.5, 0.7, 2.1, 1.0, 0.4, 3.3, 2.8, 1.2, 0.9, 2.0, 1.8, 2.4, 0.6, 3.0],\n        # Test Case 5\n        [1.0, 2.0, 3.0]\n    ]\n\n    epsilon = 1e-8\n    results = []\n\n    for data_list in test_cases:\n        data = np.array(data_list)\n        n = len(data)\n\n        # 1. Exponential Model AIC Calculation\n        t_bar = np.mean(data)\n        # Maximized log-likelihood for Exponential model\n        max_log_lik_exp = -n * np.log(t_bar) - n\n        # Number of parameters for Exponential model\n        p_exp = 1\n        aic_exp = 2 * p_exp - 2 * max_log_lik_exp\n\n        # 2. Weibull Model AIC Calculation\n        log_data = np.log(data)\n        sum_log_data = np.sum(log_data)\n\n        # We need to solve for k_hat from the implicit equation derived from MLE.\n        # This function represents the equation set to zero.\n        def k_equation(k, data, n, sum_log_data):\n            if k = 0:\n                return -np.inf  # Ensure k is positive\n            try:\n                t_k = np.power(data, k)\n                sum_t_k = np.sum(t_k)\n                sum_t_k_log_t = np.sum(t_k * log_data)\n                \n                # Check for numerical instability\n                if sum_t_k == 0 or not np.isfinite(sum_t_k):\n                    return np.nan\n\n                # The equation to be solved for k:\n                # 1/k - (sum(t_i^k * log(t_i)) / sum(t_i^k)) + (sum(log(t_i)) / n) = 0\n                return 1.0 / k - sum_t_k_log_t / sum_t_k + sum_log_data / n\n            except (OverflowError, ValueError):\n                return np.nan\n\n        try:\n            # Search for the root k_hat in a reasonable interval.\n            # brentq is a robust root-finding method for a bracketed root.\n            sol = root_scalar(k_equation, args=(data, n, sum_log_data),\n                              bracket=[0.1, 25.0], method='brentq')\n            k_hat = sol.root\n            \n            # MLE for lambda (scale parameter)\n            lambda_hat = (np.sum(np.power(data, k_hat)) / n)**(1.0 / k_hat)\n\n            # Maximized log-likelihood for Weibull model\n            # loglik = n*log(k) - n*k*log(lambda) + (k-1)*sum(log(t_i)) - n\n            max_log_lik_weib = (n * np.log(k_hat) - n * k_hat * np.log(lambda_hat) +\n                                (k_hat - 1) * sum_log_data - n)\n\n            # Number of parameters for Weibull model\n            p_weib = 2\n            aic_weib = 2 * p_weib - 2 * max_log_lik_weib\n\n        except ValueError:\n            # If root finding fails (e.g., no root in bracket, equation is ill-behaved),\n            # the model fit is problematic. We treat this as not an improvement.\n            # A huge AIC indicates a very poor fit.\n            aic_weib = np.inf\n        \n        # 3. Decision\n        # Weibull is preferred if its AIC is smaller by at least epsilon\n        is_weibull_better = aic_weib  aic_exp - epsilon\n        results.append(is_weibull_better)\n        \n    # Format and print the final output as a single list of booleans\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "当模型的参数数量不明确时（例如在非参数模型中），我们应如何应用$AIC$？这个高级练习通过将$AIC$应用于局部多项式回归的带宽选择，解答了这一问题。你将学习使用“有效自由度”作为模型复杂度的度量，从而将$AIC$的应用范围扩展到纯参数模型之外，此练习为理解现代统计学习提供了关键的洞察。",
            "id": "3098018",
            "problem": "考虑在独立高斯噪声假设下的局部多项式回归中的带宽选择任务。您需要从第一性原理出发，推导并实现一个基于Akaike信息准则 (AIC) 的标准，以选择合适的带宽。最终程序必须为多个候选带宽计算AIC，并使用通过平滑矩阵的迹定义的有效自由度，选择使该准则最小化的带宽。\n\n您将处理一维输入上次数为 $1$ 的局部多项式回归（局部估计散点平滑，LOESS）。在每个评估点 $x_i$ 处，使用高斯核权重 $K(u) = \\exp\\!\\left(-\\frac{u^2}{2}\\right)$ 和带宽参数 $h  0$ 拟合一个加权最小二乘多项式模型。在点 $x_i$ 处拟合时，数据点 $x_j$ 的权重为 $w_{ij} = \\exp\\!\\left(-\\frac{(x_j - x_i)^2}{2 h^2}\\right)$。在 $x_i$ 处的局部设计矩阵是 $X_i \\in \\mathbb{R}^{n \\times 2}$，其列为 $[1, (x_j - x_i)]$，对角权重矩阵是 $W_i \\in \\mathbb{R}^{n \\times n}$，其对角线元素为 $w_{ij}$。在 $x_i$ 处的局部拟合值是局部加权最小二乘解的截距。遍历所有 $x_i$ 得到的估计器可以写成一个线性平滑算子 $\\hat{\\mathbf{y}} = S(h)\\,\\mathbf{y}$，其中 $S(h) \\in \\mathbb{R}^{n \\times n}$ 是依赖于 $h$ 的平滑矩阵。\n\n假设数据由 $y_i = f(x_i) + \\varepsilon_i$ 生成，其中 $\\varepsilon_i$ 是独立的，且 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$，$\\sigma^2  0$ 未知。令残差平方和为 $\\mathrm{RSS}(h) = \\sum_{i=1}^n (y_i - \\hat{y}_i(h))^2$。线性平滑算子的有效自由度定义为 $\\mathrm{df}(h) = \\mathrm{tr}\\!\\left(S(h)\\right)$。\n\n从模型下数据的高斯对数似然出发，并使用噪声方差的最大似然估计，推导一个形式为 $AIC(h)$ 的基于AIC的选择准则表达式。该表达式应聚合一个基于 $\\mathrm{RSS}(h)$ 的拟合优度项和一个基于 $\\mathrm{df}(h)$ 的复杂度惩罚项。将AIC中的参数数量视为平滑算子的有效自由度 $\\mathrm{df}(h)$。您的推导必须从高斯似然开始，并避免使用预先给出的AIC简化公式。\n\n然后，实现所得到的 $AIC(h)$，并为以下每个测试用例选择使其最小化的带宽。在所有情况下，使用次数为 $1$ 的局部多项式回归、上述指定的高斯核以及候选带宽列表 $[0.03, 0.06, 0.10, 0.20, 0.35, 0.50]$。\n\n测试套件：\n- 用例 $1$ (振荡信号)：$n = 80$，种子 $= 123$，输入 $x_i$ 从 $\\mathrm{Uniform}(0,1)$ 中独立采样，然后升序排序。均值函数为 $f(x) = \\sin(6\\pi x)$，噪声标准差为 $0.2$，因此 $y_i = f(x_i) + 0.2\\,\\eta_i$，其中 $\\eta_i \\sim \\mathcal{N}(0,1)$。\n- 用例 $2$ (近似线性)：$n = 70$，种子 $= 456$，输入 $x_i$ 从 $\\mathrm{Uniform}(0,1)$ 中独立采样，然后升序排序。均值函数为 $f(x) = 0.5 + 2 x$，噪声标准差为 $0.15$，因此 $y_i = f(x_i) + 0.15\\,\\eta_i$，其中 $\\eta_i \\sim \\mathcal{N}(0,1)$。\n- 用例 $3$ (近似常数)：$n = 60$，种子 $= 789$，输入 $x_i$ 从 $\\mathrm{Uniform}(0,1)$ 中独立采样，然后升序排序。均值函数为 $f(x) = 1.0$，噪声标准差为 $0.25$，因此 $y_i = f(x_i) + 0.25\\,\\eta_i$，其中 $\\eta_i \\sim \\mathcal{N}(0,1)$。\n\n要求：\n- 通过为每个 $x_i$ 显式地构建局部设计矩阵 $X_i$ 和权重 $W_i$，并使用加权最小二乘法得到从 $\\mathbf{y}$ 到 $\\hat{y}_i$ 的线性映射，来构造平滑矩阵 $S(h)$。具体来说，如果 $A_i = X_i^\\top W_i X_i$ 且 $B_i = X_i^\\top W_i$，那么 $S(h)$ 的第 $i$ 行必须由 $A_i^{-1} B_i$ 的第一行给出。在计算中使用数值稳定的伪逆来处理边界附近潜在的病态问题。\n- 对于每个候选带宽 $h$，计算 $\\hat{\\mathbf{y}}(h)$、$\\mathrm{RSS}(h)$、$\\mathrm{df}(h)$，然后计算推导出的 $AIC(h)$，并选择使 $AIC(h)$ 最小的 $h$。\n- 最终输出格式：您的程序应生成单行输出，其中包含三个用例所选的带宽，以逗号分隔的列表形式，并用方括号括起来。每个带宽必须打印到三位小数（例如，$[0.200,0.100,0.500]$）。不应打印任何其他文本。\n\n所有答案都必须是无单位的数字。最终输出是浮点数。确保您的实现和推导对于给定的假设是一致且科学上合理的。",
            "solution": "该问题被评估为有效。它在统计学习理论中有科学依据，问题良构，具有明确的目标和充足的数据，并且没有任何列出的无效性缺陷。\n\n### 基于AIC准则的推导\n\n我们首先建立统计框架。数据模型由 $y_i = f(x_i) + \\varepsilon_i$（$i=1, \\dots, n$）给出，其中误差 $\\varepsilon_i$ 根据高斯分布独立同分布，即 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。用向量形式表示，即为 $\\mathbf{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\sigma^2 I_n)$，其中 $\\boldsymbol{\\mu}$ 是真实均值 $f(x_i)$ 的向量。\n\n局部多项式回归产生一个拟合向量 $\\hat{\\mathbf{y}}(h) = S(h)\\mathbf{y}$，它作为我们对均值向量 $\\boldsymbol{\\mu}$ 的估计。在给定拟合值 $\\hat{\\mathbf{y}}(h)$ 和噪声方差 $\\sigma^2$ 的条件下，观测到数据 $\\mathbf{y}$ 的似然是各个高斯概率密度的乘积：\n$$ L(\\hat{\\mathbf{y}}(h), \\sigma^2 | \\mathbf{y}) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\hat{y}_i(h))^2}{2\\sigma^2}\\right) $$\n因此，对数似然函数（记为 $\\ell$）为：\n$$ \\ell(\\hat{\\mathbf{y}}(h), \\sigma^2 | \\mathbf{y}) = \\sum_{i=1}^n \\left( -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\sigma^2) - \\frac{(y_i - \\hat{y}_i(h))^2}{2\\sigma^2} \\right) $$\n使用残差平方和 $\\mathrm{RSS}(h) = \\sum_{i=1}^n (y_i - \\hat{y}_i(h))^2$，可以更紧凑地写为：\n$$ \\ell(\\hat{\\mathbf{y}}(h), \\sigma^2 | \\mathbf{y}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{\\mathrm{RSS}(h)}{2\\sigma^2} $$\n为了找到固定带宽 $h$ 的最大化对数似然，我们首先找到未知方差 $\\sigma^2$ 的最大似然估计 (MLE)。我们将 $\\ell$ 对 $\\sigma^2$ 求导，并令结果为零：\n$$ \\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{\\mathrm{RSS}(h)}{2(\\sigma^2)^2} = 0 $$\n解出 $\\sigma^2$ 得到其MLE：\n$$ \\hat{\\sigma}^2_{\\mathrm{ML}} = \\frac{\\mathrm{RSS}(h)}{n} $$\n将此估计值代回对数似然函数，得到给定 $h$ 的最大化对数似然：\n$$ \\ell_{\\max}(h) = \\ell(\\hat{\\mathbf{y}}(h), \\hat{\\sigma}^2_{\\mathrm{ML}} | \\mathbf{y}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{\\mathrm{RSS}(h)}{n}\\right) - \\frac{\\mathrm{RSS}(h)}{2\\left(\\frac{\\mathrm{RSS}(h)}{n}\\right)} $$\n$$ \\ell_{\\max}(h) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log\\left(\\frac{\\mathrm{RSS}(h)}{n}\\right) - \\frac{n}{2} $$\n$$ \\ell_{\\max}(h) = -\\frac{n}{2}\\left(\\log(2\\pi) + 1\\right) - \\frac{n}{2}\\log(\\mathrm{RSS}(h)) + \\frac{n}{2}\\log(n) $$\n\nAkaike信息准则 (AIC) 定义为：\n$$ \\mathrm{AIC} = -2 \\times (\\text{最大化对数似然}) + 2 \\times (\\text{参数数量}) $$\n问题指定平滑算子的有效参数数量是其有效自由度，$k = \\mathrm{df}(h) = \\mathrm{tr}(S(h))$。应用该定义，我们得到：\n$$ \\mathrm{AIC}(h) = -2 \\left( -\\frac{n}{2}\\left(\\log(2\\pi) + 1\\right) - \\frac{n}{2}\\log(\\mathrm{RSS}(h)) + \\frac{n}{2}\\log(n) \\right) + 2 \\, \\mathrm{df}(h) $$\n$$ \\mathrm{AIC}(h) = n\\left(\\log(2\\pi) + 1\\right) + n\\log(\\mathrm{RSS}(h)) - n\\log(n) + 2 \\, \\mathrm{df}(h) $$\n为了从一组候选中选择最佳带宽 $h$，我们只需考虑依赖于 $h$ 的项。项 $n(\\log(2\\pi) + 1)$ 和 $-n\\log(n)$ 对所有候选模型都是常数。去掉这些常数项，我们得到一个与完整AIC成正比并且能够产生相同模型排序的准则：\n$$ \\mathrm{AIC_{sel}}(h) = n\\log(\\mathrm{RSS}(h)) + 2 \\, \\mathrm{df}(h) $$\n这就是我们将要实现用来选择最优带宽 $h$ 的准则。我们将选择使该值最小化的带宽。\n\n### 实现策略\n\n对于每个测试用例和每个候选带宽 $h$，我们将执行以下步骤：\n\n1.  **数据生成**：根据指定的数据生成过程生成大小为 $n$ 的输入向量 $\\mathbf{x}$ 和响应向量 $\\mathbf{y}$，包括用于可复现性的随机种子。\n\n2.  **平滑矩阵构造**：构造 $n \\times n$ 的平滑矩阵 $S(h)$。这是逐行完成的。对于每个目标点 $x_i$（其中 $i \\in \\{1, \\dots, n\\}$）：\n    a.  计算所有 $j \\in \\{1, \\dots, n\\}$ 的权重 $w_{ij} = \\exp\\left(-\\frac{(x_j - x_i)^2}{2 h^2}\\right)$。\n    b.  构造局部设计矩阵 $X_i \\in \\mathbb{R}^{n \\times 2}$，其行为 $[1, (x_j - x_i)]$。\n    c.  根据问题要求，计算 $A_i = X_i^\\top W_i X_i$ 和 $B_i = X_i^\\top W_i$，其中 $W_i$ 是由权重 $w_{ij}$ 构成的对角矩阵。在数值上，这可以通过广播权重向量来高效完成。\n    d.  $S(h)$ 的第 $i$ 行（记为 $s_i^\\top$）是 $A_i^{-1} B_i$ 的第一行。为确保数值稳定性，使用伪逆 `np.linalg.pinv` 计算 $A_i^{-1}$。\n\n3.  **准则计算**：\n    a.  计算有效自由度 $\\mathrm{df}(h) = \\mathrm{tr}(S(h))$。\n    b.  计算拟合值 $\\hat{\\mathbf{y}}(h) = S(h) \\mathbf{y}$。\n    c.  计算残差平方和 $\\mathrm{RSS}(h) = \\sum_{j=1}^n (y_j - \\hat{y}_j(h))^2$。\n    d.  最后，计算选择准则值 $\\mathrm{AIC_{sel}}(h) = n\\log(\\mathrm{RSS}(h)) + 2 \\, \\mathrm{df}(h)$。\n\n4.  **带宽选择**：为所有候选带宽计算 $\\mathrm{AIC_{sel}}(h)$ 后，选择产生最小 $\\mathrm{AIC_{sel}}$ 值的带宽 $h$ 作为该测试用例的最优带宽。\n\n对所有三个测试用例重复此过程，并将所选带宽收集起来用于最终输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements an AIC-based criterion for bandwidth selection\n    in local polynomial regression.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (oscillatory signal)\n        {\"n\": 80, \"seed\": 123, \"f\": lambda x: np.sin(6 * np.pi * x), \"sigma\": 0.2},\n        # Case 2 (approximately linear)\n        {\"n\": 70, \"seed\": 456, \"f\": lambda x: 0.5 + 2 * x, \"sigma\": 0.15},\n        # Case 3 (approximately constant)\n        {\"n\": 60, \"seed\": 789, \"f\": lambda x: np.ones_like(x), \"sigma\": 0.25},\n    ]\n\n    h_candidates = [0.03, 0.06, 0.10, 0.20, 0.35, 0.50]\n    \n    selected_bandwidths = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        seed = case[\"seed\"]\n        f = case[\"f\"]\n        sigma = case[\"sigma\"]\n\n        # Generate data\n        rng = np.random.default_rng(seed)\n        x = rng.uniform(0, 1, n)\n        x = np.sort(x)\n        eta = rng.normal(0, 1, n)\n        y = f(x) + sigma * eta\n\n        aic_scores = []\n        \n        for h in h_candidates:\n            # Construct the smoother matrix S\n            S = np.zeros((n, n))\n            for i in range(n):\n                # Target point for the local fit\n                x_i = x[i]\n                \n                # Distances from the target point\n                d = x - x_i\n                \n                # Gaussian kernel weights\n                weights = np.exp(-d**2 / (2 * h**2))\n                \n                # Local design matrix X_i\n                X_i = np.vstack([np.ones(n), d]).T\n                \n                # Calculate A_i = X_i^T W_i X_i and B_i = X_i^T W_i efficiently\n                # (X_i.T * weights) performs X_i.T @ diag(weights)\n                A_i = (X_i.T * weights) @ X_i\n                B_i = X_i.T * weights\n                \n                # Use pseudoinverse for numerical stability\n                A_i_inv = np.linalg.pinv(A_i)\n                \n                # The i-th row of S is the first row of (A_i^-1 B_i)\n                s_i_row = (A_i_inv @ B_i)[0, :]\n                S[i, :] = s_i_row\n                \n            # Calculate effective degrees of freedom\n            df_h = np.trace(S)\n            \n            # Calculate fitted values and RSS\n            y_hat = S @ y\n            rss_h = np.sum((y - y_hat)**2)\n            \n            # Calculate AIC-based criterion\n            # AIC_sel(h) = n*log(RSS(h)) + 2*df(h)\n            if rss_h = 0: # Avoid log(0) or log(-)\n                aic_h = np.inf\n            else:\n                aic_h = n * np.log(rss_h) + 2 * df_h\n            \n            aic_scores.append(aic_h)\n            \n        # Select the bandwidth that minimizes AIC\n        best_h_index = np.argmin(aic_scores)\n        best_h = h_candidates[best_h_index]\n        selected_bandwidths.append(best_h)\n\n    # Format the final output\n    formatted_results = [f\"{bw:.3f}\" for bw in selected_bandwidths]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}