{
    "hands_on_practices": [
        {
            "introduction": "本练习旨在帮助您从实践层面建立对前向逐步选择的基本理解。您将亲手实现该算法，并观察统计学习中的一个核心原理：随着模型变得更加复杂，训练误差会持续下降，但其在未见数据上的表现（验证误差）通常会呈现U形曲线。通过这项实践 ，您将能够通过计算来探索偏差-方差权衡，并理解如何利用验证数据来选择最佳的模型复杂度。",
            "id": "3104976",
            "problem": "考虑使用前向逐步选择的线性回归。设训练观测值为 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $x_i \\in \\mathbb{R}^p$ 且 $y_i \\in \\mathbb{R}$。对于由 $S \\subseteq \\{1,\\dots,p\\}$ 索引的任何预测变量子集，定义一个包含截距项和 $S$ 中预测变量的模型，并将其训练残差平方和（RSS）表示为 $\\text{RSS}(S) = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i(S)\\right)^2$，其中 $\\hat{y}_i(S)$ 是使用 $S$ 中预测变量的最小二乘拟合。前向逐步选择构建一个嵌套的已选集合序列 $(S_k)_{k=0}^p$，使得 $S_0 = \\varnothing$，并且对于 $k \\geq 0$，有 $S_{k+1} = S_k \\cup \\{j^*\\}$，其中 $j^*$是从 $\\{1,\\dots,p\\} \\setminus S_k$ 中选择的，以最小化大小为 $k+1$ 时的训练 RSS。\n\n从以下基本概念开始：\n- 线性最小二乘拟合在所选预测变量和截距项张成的空间上最小化误差平方和。\n- 如果回归变量集合扩大，设计矩阵（包括截距项）的列空间会扩大或保持不变，因此最小化的 RSS 不会增加。\n\n你的任务：\n1. 在训练集上实现前向逐步选择，以生成大小为 $k = 0,1,\\dots,p$ 的模型 $S_k$。对于每个 $k$，在一个大小为 $m$ 的独立验证集上计算训练 RSS 和验证均方误差 $\\text{MSE}_{\\text{val}}(k) = \\frac{1}{m}\\sum_{i=1}^m \\left(y^{\\text{val}}_i - \\hat{y}^{\\text{val}}_i(S_k)\\right)^2$，其中 $\\hat{y}^{\\text{val}}_i(S_k)$ 使用在训练集上拟合的系数。\n2. 通过计算表明，对于一般情况下的数据，在前向选择中，随着 $k$ 的增加，训练 RSS 是严格递减的（使用一个小的数值容差来防止浮点效应）。\n3. 证明验证误差 $\\text{MSE}_{\\text{val}}(k)$ 作为 $k$ 的函数可以呈现 U 形行为：由于偏差减小而初始下降，然后由于过拟合引起的方差膨胀而增加。设计至少一个数据集，其中最小化误差的 $k$ 严格介于 $0$ 和 $p$ 之间。\n4. 使用以下数据集生成器参数的测试套件。每个数据集通过如下方式采样预测变量和响应来生成：抽取 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times p}$ 和 $X_{\\text{val}} \\in \\mathbb{R}^{m \\times p}$，其元素为独立的标准正态分布；选择一个系数向量 $\\beta \\in \\mathbb{R}^p$；生成 $y_{\\text{train}} = X_{\\text{train}} \\beta + \\varepsilon_{\\text{train}}$ 和 $y_{\\text{val}} = X_{\\text{val}} \\beta + \\varepsilon_{\\text{val}}$，其中噪声项的元素为独立的均值为 $0$、标准差为 $\\sigma$ 的正态分布。在所有拟合模型中使用截距项。测试套件的参数为：\n   - 情况 A（中间最优）：$n = 50$，$m = 400$，$p = 20$，$\\beta = (3.0, -2.5, 1.5, 0, \\dots, 0)$，$\\sigma = 2.0$，种子 $= 2024$。\n   - 情况 B（零模型最佳）：$n = 60$，$m = 400$，$p = 12$，$\\beta = (0, \\dots, 0)$，$\\sigma = 1.0$，种子 $= 2025$。\n   - 情况 C（全模型最佳）：$n = 80$，$m = 400$，$p = 6$，$\\beta = (1.0, -1.5, 0.8, 2.0, -1.0, 0.5)$，$\\sigma = 0.2$，种子 $= 7$。\n5. 对于每种情况，返回三个量：\n   - 一个布尔值，指示对于 $k = 0,1,\\dots,p$，序列 $\\text{RSS}(S_k)$ 是否随 $k$ 严格递减。\n   - 最小化 $\\text{MSE}_{\\text{val}}(k)$ 的整数 $k^\\star$，取值范围为 $\\{0,1,\\dots,p\\}$。\n   - 一个布尔值，指示验证误差是否呈现 U 形曲线，即 $0  k^\\star  p$ 且 $\\max\\{\\text{MSE}_{\\text{val}}(0), \\dots, \\text{MSE}_{\\text{val}}(k^\\star - 1)\\} > \\text{MSE}_{\\text{val}}(k^\\star)$ 和 $\\max\\{\\text{MSE}_{\\text{val}}(k^\\star + 1), \\dots, \\text{MSE}_{\\text{val}}(p)\\} > \\text{MSE}_{\\text{val}}(k^\\star)$ 同时成立。\n6. 你的程序应该生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，每个情况的结果是按上述顺序排列的三元素列表。例如：\"[[True,3,True],[True,0,False],[True,6,False]]\"。不涉及物理单位或角度，任何百分比（如有）都必须表示为小数。确保所有数学计算都遵循标准的线性代数运算。\n\n程序必须按规定实现数据生成和前向逐步选择，运行三种情况，并产生与上述格式完全匹配的单行输出。",
            "solution": "用户提供的问题被评估为**有效**的。\n\n该问题是一个定义明确的统计学习计算练习，专注于实现和分析线性回归前向逐步选择的行为。它在科学上基于回归、模型选择和偏差-方差权衡的既定原则。问题陈述是自包含的，提供了所有必要的参数、数据生成过程、用于可复现性的随机种子以及所需输出的精确定义。它没有歧义、主观声明或事实错误。这些任务在计算上是可行的，并测试了基本概念，例如训练误差的单调递减和验证误差的特征性U形曲线，这些是理解模型复杂性和过拟合的核心。\n\n### 基于原则的设计\n\n该解决方案围绕线性回归和前向逐步模型选择的核心原则构建。\n\n#### 1. 前向逐步选择算法\n\n前向逐步选择是一种用于为模型选择预测变量子集的贪心迭代算法。该过程从一个只包含截距项的零模型开始，并迭代地一次添加一个预测变量。\n\n- **初始化 ($k=0$)**：过程从最简单的模型 $S_0 = \\varnothing$ 开始，该模型不包含任何预测变量，只有一个截距项。对每个观测值的预测就是训练响应变量的均值，$\\hat{y}_i = \\bar{y}_{\\text{train}}$。为这个基础模型计算初始的训练残差平方和 $\\text{RSS}(S_0)$ 和验证均方误差 $\\text{MSE}_{\\text{val}}(0)$。\n\n- **迭代步骤 ($k \\to k+1$)**：对于从 $0$ 到 $p-1$ 的每一步 $k$，算法考虑从尚未在模型中的预测变量集合 $\\{1, \\dots, p\\} \\setminus S_k$ 中添加一个预测变量。对于每个候选预测变量 $j$，使用预测变量 $S_k \\cup \\{j\\}$ 拟合一个临时模型到训练数据，并计算其训练 RSS。算法贪心地选择能最大程度减少 RSS 的预测变量 $j^*$：\n$$\nj^* = \\arg\\min_{j \\in \\{1,\\dots,p\\} \\setminus S_k} \\text{RSS}(S_k \\cup \\{j\\})\n$$\n然后新模型被定义为 $S_{k+1} = S_k \\cup \\{j^*\\}$。此过程生成一个嵌套的模型序列 $S_0 \\subset S_1 \\subset \\dots \\subset S_p$，其大小从 $0$ 递增到 $p$。\n\n每个线性模型的拟合都是使用普通最小二乘法 (OLS) 进行的。对于具有预测变量集 $S$ 的模型，设计矩阵 $\\tilde{X}_S$ 是通过在所选预测变量矩阵 $X_S$ 前面增加一列全为1的列来构造的，以代表截距项。OLS 系数向量 $\\hat{\\beta}_S$ 是通过求解正规方程得到的，可以表示为 $\\hat{\\beta}_S = (\\tilde{X}_S^T \\tilde{X}_S)^{-1} \\tilde{X}_S^T y_{\\text{train}}$。像 `numpy.linalg.lstsq` 这样的数值稳定方法被用于此计算。\n\n#### 2. 训练 RSS 的单调性\n\n训练 RSS 是模型大小 $k$ 的一个非增函数。这是最小二乘法的一个基本属性。当添加一个预测变量时，线性模型的基向量集合（设计矩阵的列）被扩展。OLS 拟合找到了响应向量 $y_{\\text{train}}$ 在设计矩阵列空间上的投影。向基中添加一个向量只能扩展此空间或使其保持不变。因此，到新的、更大空间上的投影不能比到原始、更小空间上的投影离 $y_{\\text{train}}$ 更远。这意味着残差向量的长度，从而 RSS，不会增加。\n\n对于“通用”数据，例如从连续分布（如标准正态分布）生成的数据，新预测变量列与现有列完全线性相关的概率为零。因此，列空间在每一步都严格扩展，保证了训练 RSS 是**严格递减**的。我们通过检查是否对所有 $k$ 都有 $\\text{RSS}(S_k) > \\text{RSS}(S_{k+1})$ 来在计算上验证这一点，使用一个小的数值容差来考虑浮点运算。\n\n#### 3. 验证误差和偏差-方差权衡\n\n虽然训练误差随模型复杂度的增加而单调减少，但由于偏差-方差权衡，验证误差表现出更复杂的行为。\n\n- **偏差**：一个简单的模型（小 $k$）可能无法捕捉到预测变量和响应之间真实的潜在关系。这种系统误差被称为偏差。最初，当我们添加相关的预测变量时，模型的灵活性增加，它能更好地符合真实信号，导致偏差急剧减少，从而验证误差也随之减少。\n\n- **方差**：一个复杂的模型（大 $k$）具有高灵活性，可以捕捉到复杂的模式。然而，它可能开始拟合训练数据中的随机噪声，这种现象称为过拟合。这导致高方差，因为拟合的模型会随着不同的训练集而发生巨大变化。这种过拟合降低了模型泛化到未见数据的能力，导致验证误差增加。\n\n偏差和方差之间的相互作用通常导致验证误差作为模型复杂度 $k$ 的函数呈现出一条 U 形曲线。误差随着偏差的减少而下降，然后随着方差的主导而增加。最优模型对应于这个“U”形底部的 $k^\\star$ 值，它平衡了偏差和方差以在未见数据上实现最佳的预测性能。该问题要求识别这个最优的 $k^\\star$ 并验证验证误差曲线是否是“U形”的，即其最小值严格在内部（即 $0  k^\\star  p$）并且在最小值之前和之后都存在比最小值更高的误差值。\n\n#### 4. 实现策略\n\n该解决方案以遵循指定环境的 Python 程序实现。\n\n1.  **数据生成**：对于每个测试用例，为了可复现性，会播种一个随机数生成器。根据指定的维度（$n, m, p$）、真实系数向量（$\\beta$）和噪声水平（$\\sigma$），对训练和验证数据集（$X, y$）进行采样。\n2.  **前向选择循环**：算法如上所述进行。对于每个模型大小 $k \\in \\{0, \\dots, p\\}$，存储训练 `RSS`。在确定最佳模型 $S_k$ 后，其系数在训练数据上最终确定，然后使用这些系数在验证集上预测响应以计算 `MSE_val(k)`。\n3.  **结果计算**：循环完成后，分析存储的序列 `rss_k` 和 `mse_val_k`：\n    - 通过计算差值 `rss_k[k] - rss_k[k+1]` 并确保它们都为正（大于一个小容差），来检查 `rss_k` 的严格递减性。\n    - 使用 `numpy.argmin` 在 `mse_val_k` 数组上找到最优模型大小 `k_star`。\n    - 通过检查 `k_star` 是否介于 $0$ 和 $p$ 之间，以及在 `k_star` 的两侧是否存在严格大于最小值的验证误差值，来评估 U 形条件。\n4.  **输出格式化**：每个测试用例的结果（一个包含三个值的列表）被格式化为精确的字符串表示 `[Boolean,Integer,Boolean]`，然后聚合成最终所需的输出字符串 `[[...],[...],[...]]`。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case(n, m, p, beta_coeffs, sigma, seed):\n    \"\"\"\n    Generates data, runs forward stepwise selection, and computes required metrics for one case.\n\n    Args:\n        n (int): Number of training observations.\n        m (int): Number of validation observations.\n        p (int): Number of predictors.\n        beta_coeffs (tuple): Non-zero coefficients for the true model.\n        sigma (float): Standard deviation of the noise.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        list: A list containing [is_rss_decreasing, k_star, is_u_shaped].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate data\n    X_train = rng.standard_normal(size=(n, p))\n    X_val = rng.standard_normal(size=(m, p))\n    \n    beta = np.zeros(p)\n    beta[:len(beta_coeffs)] = beta_coeffs\n    \n    eps_train = rng.normal(0, sigma, size=n)\n    eps_val = rng.normal(0, sigma, size=m)\n    \n    y_train = X_train @ beta + eps_train\n    y_val = X_val @ beta + eps_val\n\n    # Arrays to store results for k = 0, 1, ..., p\n    rss_k = np.zeros(p + 1)\n    mse_val_k = np.zeros(p + 1)\n    \n    selected_indices = []\n    available_indices = list(range(p))\n\n    # --- k = 0: Intercept-only model ---\n    y_train_mean = np.mean(y_train)\n    rss_k[0] = np.sum((y_train - y_train_mean)**2)\n    mse_val_k[0] = np.mean((y_val - y_train_mean)**2)\n\n    # --- k = 1 to p: Forward stepwise selection ---\n    for k in range(1, p + 1):\n        best_rss_at_step = np.inf\n        best_new_predictor = -1\n        \n        # Find the best predictor to add from the available set\n        for j in available_indices:\n            current_predictors = selected_indices + [j]\n            \n            # Create design matrix with intercept\n            X_train_subset = X_train[:, current_predictors]\n            design_matrix = np.c_[np.ones(n), X_train_subset]\n            \n            # Solve least squares and get RSS\n            _, residuals, _, _ = np.linalg.lstsq(design_matrix, y_train, rcond=None)\n            \n            rss_candidate = residuals[0] if residuals.size > 0 else np.sum((y_train - design_matrix @ np.linalg.lstsq(design_matrix, y_train, rcond=None)[0])**2)\n\n            if rss_candidate  best_rss_at_step:\n                best_rss_at_step = rss_candidate\n                best_new_predictor = j\n        \n        # Add the best predictor to the model for this step\n        selected_indices.append(best_new_predictor)\n        available_indices.remove(best_new_predictor)\n        \n        # Store RSS for the model of size k\n        rss_k[k] = best_rss_at_step\n        \n        # --- Calculate validation MSE for the new model S_k ---\n        # Fit model on training data to get coefficients\n        X_train_final_subset = X_train[:, selected_indices]\n        design_matrix_train = np.c_[np.ones(n), X_train_final_subset]\n        coeffs, _, _, _ = np.linalg.lstsq(design_matrix_train, y_train, rcond=None)\n        \n        # Predict on validation data using the fitted coefficients\n        X_val_subset = X_val[:, selected_indices]\n        design_matrix_val = np.c_[np.ones(m), X_val_subset]\n        y_val_pred = design_matrix_val @ coeffs\n        \n        # Calculate validation MSE\n        mse_val_k[k] = np.mean((y_val - y_val_pred)**2)\n\n    # 2. Check if RSS sequence is strictly decreasing\n    is_decreasing = bool(np.all(np.diff(rss_k)  -1e-9))\n    \n    # 3. Find k that minimizes validation MSE\n    k_star = int(np.argmin(mse_val_k))\n    \n    # 4. Check for U-shaped profile as defined in the problem\n    is_u_shaped = False\n    if 0  k_star  p:\n        min_mse = mse_val_k[k_star]\n        # Check that there is at least one point > min_mse before and after k_star\n        has_larger_before = np.any(mse_val_k[:k_star] > min_mse)\n        has_larger_after = np.any(mse_val_k[k_star+1:] > min_mse)\n        if has_larger_before and has_larger_after:\n            is_u_shaped = True\n\n    return [is_decreasing, k_star, is_u_shaped]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Intermediate optimum\n        {'n': 50, 'm': 400, 'p': 20, 'beta_coeffs': (3.0, -2.5, 1.5), 'sigma': 2.0, 'seed': 2024},\n        # Case B: Null model best\n        {'n': 60, 'm': 400, 'p': 12, 'beta_coeffs': (), 'sigma': 1.0, 'seed': 2025},\n        # Case C: Full model best\n        {'n': 80, 'm': 400, 'p': 6, 'beta_coeffs': (1.0, -1.5, 0.8, 2.0, -1.0, 0.5), 'sigma': 0.2, 'seed': 7},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(**case)\n        results.append(result)\n\n    # Format the output string exactly as specified\n    formatted_results = []\n    for res in results:\n        # Manually format to get \"[True,3,True]\" without spaces\n        s = '[' + ','.join(map(str, res)) + ']'\n        formatted_results.append(s)\n    \n    final_output_string = '[' + ','.join(formatted_results) + ']'\n    \n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "虽然前向选择算法效率很高，但其贪心本质并非没有缺陷。本练习将探讨一个该贪心策略可能失效的经典场景。您将构建一个特殊的数据集，其中两个预测变量单独来看作用微弱，但结合在一起时却具有很强的预测能力——这种现象被称为协同效应或交互作用 。通过观察前向选择在这种“对抗性”设置下的行为，您将对这种只做局部最优选择的方法的局限性有更深刻的认识。",
            "id": "3105020",
            "problem": "给定一个统计学习中的场景，涉及使用前向逐步选择和沿前向路径的最优子集评估进行模型选择。目标是构建一个对于第一步的贪心选择具有对抗性的设计矩阵，即单个变量在验证时看起来没有帮助，但它们共同作用却能解释响应变量。您的程序必须实现指定的流程，在测试套件上进行评估，并按规定输出单行聚合结果。\n\n使用的基本和核心定义：\n- 普通最小二乘法 (OLS) 回归通过最小化训练残差平方和来估计系数。给定一个训练设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{n}$，包含截距项的增广设计为 $\\tilde{X} = [\\mathbf{1}, X] \\in \\mathbb{R}^{n \\times (p+1)}$。OLS 估计量是以下问题的解 $\\hat{\\beta}$：\n$$\n\\hat{\\beta} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p+1}} \\sum_{i=1}^{n} \\left( y_i - \\tilde{X}_{i\\cdot} \\beta \\right)^2,\n$$\n该解可通过 Moore-Penrose 伪逆或最小二乘程序计算。训练残差平方和 (RSS) 为：\n$$\n\\mathrm{RSS}_{\\text{train}} = \\sum_{i=1}^{n} \\left( y_i - \\tilde{X}_{i\\cdot} \\hat{\\beta} \\right)^2.\n$$\n- 对于一个已拟合的模型，在大小为 $m$ 的验证集 $(X^{\\text{val}}, y^{\\text{val}})$ 上评估时，验证均方误差 (MSE) 为：\n$$\n\\mathrm{MSE}_{\\text{val}} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( y^{\\text{val}}_i - \\tilde{X}^{\\text{val}}_{i\\cdot} \\hat{\\beta} \\right)^2,\n$$\n其中 $\\hat{\\beta}$ 仅在训练数据上拟合。本问题不涉及物理单位。\n\n对抗性设计的构建：\n- 考虑 $X \\in \\mathbb{R}^{n \\times 2}$ 中的两个预测变量，记为 $x_1$ 和 $x_2$，它们的生成方式使其呈负相关：\n$$\nx_{1,i} \\sim \\mathcal{N}(0,1), \\quad r_i \\sim \\mathcal{N}(0,s^2), \\quad x_{2,i} = -x_{1,i} + r_i,\n$$\n在 $i \\in \\{1,\\dots,n\\}$ 上独立。\n- 响应变量的生成方式如下：\n$$\ny_i = x_{1,i} + x_{2,i} + \\varepsilon_i = r_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n$$\n在 $i$ 上独立。通过这种构造，$x_1$ 和 $x_2$ 呈负相关，而它们的和 $x_1 + x_2$ 分离出了直接驱动 $y$ 的残差分量 $r$。在边际上，$y$ 和 $x_1$ 之间的协方差为 $0$，而 $y$ 和 $x_2$ 之间的协方差等于 $s^2$，通过选择一个小的 $s$ 可以使其变得很小。结果是，在有限样本中，每个变量单独在验证时可能显得无用，而它们两者合在一起则能解释 $y$。\n\n需要实现的流程：\n1. 在训练数据上进行前向逐步路径选择：从仅含截距项的模型开始，在步骤 $k \\in \\{1,2\\}$，在剩余的变量中，加入那个在与截距项和所有先前已选择的变量一起重新拟合模型时产生最小 $\\mathrm{RSS}_{\\text{train}}$ 的变量。\n2. 沿前向路径的最优子集选择：对于每个模型大小 $k \\in \\{0,1,2\\}$（其中 $k=0$ 表示仅含截距项），对由前向路径定义的大小至多为 $k$ 的子集，在训练集上拟合 OLS，然后在验证集上计算 $\\mathrm{MSE}_{\\text{val}}$。选择具有最小 $\\mathrm{MSE}_{\\text{val}}$ 的模型大小。\n3. 基于验证的贪心前向选择与提前停止：从仅含截距项的模型开始，在步骤 1，试探性地加入每个候选变量，在训练集上拟合，并计算 $\\mathrm{MSE}_{\\text{val}}$；仅当新的 $\\mathrm{MSE}_{\\text{val}}$ 严格小于当前值时才加入该变量。如果步骤 1 有所改进，则对步骤 2 重复此过程。如果在某一步骤没有改进，则停止并选择当前模型。\n\n每个测试用例需要报告的量：\n- 以列表 $[b_1, b_2]$ 的形式报告两个布尔值，其中：\n    - 当且仅当以下条件全部满足时，$b_1$ 为真：前向路径在第 2 步包含了两个变量；在大小为 $k \\in \\{0,1,2\\}$ 的模型中，最优子集（按最小 $\\mathrm{MSE}_{\\text{val}}$）是大小为 2 的模型；并且大小为 1 的模型的 $\\mathrm{MSE}_{\\text{val}}$ 严格大于大小为 0（仅含截距项）的模型的 $\\mathrm{MSE}_{\\text{val}}$。这测试了该变量对是否仅在第 2 步被恢复，以及每个单独的变量是否会损害在验证集上的拟合效果。\n    - 当且仅当基于验证的贪心前向选择从未添加任何变量（即，两个变量都未选择）时，$b_2$ 为真，这表明未能恢复联合起来提供信息的变量对。\n  \n测试套件参数：\n- 每个测试用例由 $(n_{\\text{train}}, n_{\\text{val}}, s, \\sigma, \\text{seed})$ 定义，包含以下三种情况：\n    1. 情况 A（强负相关，小残差）：$(n_{\\text{train}} = 60, n_{\\text{val}} = 2000, s = 0.05, \\sigma = 0.02, \\text{seed} = 0)$。\n    2. 情况 B（中度负相关）：$(n_{\\text{train}} = 60, n_{\\text{val}} = 2000, s = 0.5, \\sigma = 0.02, \\text{seed} = 1)$。\n    3. 情况 C（极端负相关和极小残差噪声）：$(n_{\\text{train}} = 30, n_{\\text{val}} = 5000, s = 0.005, \\sigma = 0.005, \\text{seed} = 2)$。\n\n您的程序必须：\n- 根据上述构造，为每个测试用例生成训练集和验证集。\n- 实现相关流程，并为每种情况计算两个布尔值 $b_1$ 和 $b_2$。\n- 生成单行输出，包含结果，格式为逗号分隔的列表的列表，并用方括号括起来，例如 $\\texttt{[[true,false],[false,false],[true,true]]}$，但使用 Python 布尔值和精确格式。\n\n最终输出格式要求：\n- 您的程序应生成单行输出，包含三个测试用例的结果，格式为逗号分隔的列表的列表，并用方括号括起来，其确切形式为 $\\texttt{[[b11,b12],[b21,b22],[b31,b32]]}$，其中每个 $b_{ij}$ 是一个 Python 布尔值（$\\texttt{True}$ 或 $\\texttt{False}$）。",
            "solution": "该问题被评估为有效。这是一个在计算统计学领域中表述清晰、科学合理且客观的问题，要求在特定的数据生成方案下实现和评估模型选择算法。所有必要的参数和流程都已明确定义，从而能够得到唯一且可验证的解。\n\n该问题要求构建一个特定的场景，使得贪心模型选择方法会失败。这通过生成两个强负相关的预测变量 $x_1$ 和 $x_2$ 来实现，而响应变量 $y$ 由它们的和决定。这创造了一种情况：每个预测变量单独与响应变量的关联度很低，但它们共同作用时能解释其大部分方差。我们将评估两种模型选择策略——沿前向路径的最优子集选择和基于验证的贪心前向搜索——以展示这种行为。解决方案首先生成数据，然后实现指定的算法，最后评估所需布尔输出的条件。\n\n**1. 数据生成**\n\n对于每个测试用例，我们都给定了参数 $(n_{\\text{train}}, n_{\\text{val}}, s, \\sigma, \\text{seed})$。我们生成一个大小为 $n_{\\text{train}}$ 的训练集 $(X, y)$ 和一个大小为 $n_{\\text{val}}$ 的验证集 $(X^{\\text{val}}, y^{\\text{val}})$。设计矩阵 $X$ 包含两个预测变量 $x_1$ 和 $x_2$。\n\n对于大小为 $N$（$n_{\\text{train}}$ 或 $n_{\\text{val}}$）的数据集，每个观测值 $i \\in \\{1, \\dots, N\\}$ 的生成过程如下：\n- 抽取一个标准正态变量：$x_{1,i} \\sim \\mathcal{N}(0, 1)$。\n- 抽取第二个预测变量的残差分量：$r_i \\sim \\mathcal{N}(0, s^2)$。\n- 构建第二个预测变量，使其与第一个预测变量负相关：$x_{2,i} = -x_{1,i} + r_i$。\n- 抽取响应变量的误差项：$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n- 响应变量由预测变量之和加噪声生成：$y_i = x_{1,i} + x_{2,i} + \\varepsilon_i$。代入 $x_1$ 和 $x_2$ 的表达式后，简化为 $y_i = r_i + \\varepsilon_i$。\n\n使用特定的 `seed` 来确保随机数生成器的可复现性。\n\n**2. 普通最小二乘法 (OLS) 回归**\n\n所有模型均使用 OLS 进行拟合。对于一个包含从 $X$ 中选出的一组预测变量的模型，我们通过在前面添加一列全为 1 的列来形成增广设计矩阵 $\\tilde{X}$，以代表截距项。系数向量 $\\hat{\\beta}$ 通过最小化训练数据上的残差平方和 (RSS) 来估计：\n$$\n\\hat{\\beta} = \\arg\\min_{\\beta} \\| y_{\\text{train}} - \\tilde{X}_{\\text{train}} \\beta \\|_2^2\n$$\n这通过标准的数值最小二乘程序求解，等效于使用 Moore-Penrose 伪逆。一旦找到 $\\hat{\\beta}$，对于任何设计矩阵 $\\tilde{Z}$ 的预测值由 $\\hat{y} = \\tilde{Z}\\hat{\\beta}$ 给出。训练 RSS 为 $\\mathrm{RSS}_{\\text{train}} = \\| y_{\\text{train}} - \\tilde{X}_{\\text{train}}\\hat{\\beta} \\|_2^2$。验证均方误差为 $\\mathrm{MSE}_{\\text{val}} = \\frac{1}{m} \\| y_{\\text{val}} - \\tilde{X}_{\\text{val}}\\hat{\\beta} \\|_2^2$，其中 $m = n_{\\text{val}}$。\n\n**3. 布尔值 $b_1$ 的计算**\n\n$b_1$ 的值通过分析前向逐步路径上模型的性能来确定。这包括两个阶段。\n\n**3.1. 前向逐步路径的构建**\n从仅含截距项的模型 $\\mathcal{M}_0$ 开始，我们通过逐步添加预测变量来构建一系列模型 $\\mathcal{M}_1, \\mathcal{M}_2$。\n- **步骤 1:** 我们比较两个模型：$\\mathcal{M}_{\\{1\\}}$（截距项和 $x_1$）和 $\\mathcal{M}_{\\{2\\}}$（截距项和 $x_2$）。每个模型都在训练数据上拟合。将导致较低 $\\mathrm{RSS}_{\\text{train}}$ 的预测变量添加到模型中。设此预测变量为 $j_1$。步骤 1 的模型为 $\\mathcal{M}_1 = \\mathcal{M}_{\\{j_1\\}}$。\n- **步骤 2:** 将剩余的预测变量 $j_2$ 加入，形成模型 $\\mathcal{M}_2 = \\mathcal{M}_{\\{j_1, j_2\\}}$，该模型包含截距项以及 $x_1$ 和 $x_2$。\n\n**3.2. $b_1$ 条件的评估**\n我们在验证集上评估来自前向路径的模型 $\\mathcal{M}_0, \\mathcal{M}_1, \\mathcal{M}_2$。\n- 设 $\\mathrm{MSE}_{\\text{val}}(0)$ 为仅含截距项模型 $\\mathcal{M}_0$ 的验证 MSE。\n- 设 $\\mathrm{MSE}_{\\text{val}}(1)$ 为大小为 1 的模型 $\\mathcal{M}_1$ 的验证 MSE。\n- 设 $\\mathrm{MSE}_{\\text{val}}(2)$ 为大小为 2 的模型 $\\mathcal{M}_2$ 的验证 MSE。\n\n当且仅当以下所有三个条件都成立时，布尔值 $b_1$ 为真：\n1. 第 2 步的前向路径包含两个变量。对于 $p=2$ 个预测变量，这总是成立的。\n2. 以最小 $\\mathrm{MSE}_{\\text{val}}$ 判断，最优模型是大小为 2 的模型：$\\mathrm{MSE}_{\\text{val}}(2)  \\mathrm{MSE}_{\\text{val}}(1)$ 且 $\\mathrm{MSE}_{\\text{val}}(2)  \\mathrm{MSE}_{\\text{val}}(0)$。\n3. 大小为 1 的模型在验证集上的表现比仅含截距项的模型差：$\\mathrm{MSE}_{\\text{val}}(1) > \\mathrm{MSE}_{\\text{val}}(0)$。\n\n这些条件形式化了对抗性场景：添加“最佳”的单个预测变量会增加验证误差，但同时添加两个预测变量才能得到最佳模型。\n\n**4. 布尔值 $b_2$ 的计算**\n\n$b_2$ 的值通过应用一个贪心前向选择程序来确定，该程序使用验证 MSE 进行决策并包含提前停止机制。\n- **步骤 0:** 当前模型是仅含截距项的模型 $\\mathcal{M}_0$。我们计算其验证误差 $\\mathrm{MSE}_{\\text{val}}(0)$。\n- **步骤 1:** 我们评估两个候选模型：$\\mathcal{M}_{\\{1\\}}$（含 $x_1$）和 $\\mathcal{M}_{\\{2\\}}$（含 $x_2$）。我们计算它们的验证误差 $\\mathrm{MSE}_{\\text{val}}(\\{1\\})$ 和 $\\mathrm{MSE}_{\\text{val}}(\\{2\\})$。\n- **决策:** 算法检查是否有任何候选模型提供了严格的改进。如果 $\\min(\\mathrm{MSE}_{\\text{val}}(\\{1\\}), \\mathrm{MSE}_{\\text{val}}(\\{2\\}))  \\mathrm{MSE}_{\\text{val}}(0)$，则将最优的那个加入，并继续进行到步骤 2。否则，算法停止，不添加任何预测变量。\n\n当且仅当贪心算法在步骤 1 停止，不添加任何变量时，布尔值 $b_2$ 为真。这种情况发生在两个单预测变量模型都不能提供比仅含截距项模型更低的验证 MSE 时：\n$$\n\\mathrm{MSE}_{\\text{val}}(\\{1\\}) \\ge \\mathrm{MSE}_{\\text{val}}(0) \\quad \\text{并且} \\quad \\mathrm{MSE}_{\\text{val}}(\\{2\\}) \\ge \\mathrm{MSE}_{\\text{val}}(0)\n$$\n这个条件证明了贪心的、基于验证的搜索方法未能识别出有用的预测变量对。\n\n对于每个测试用例，我们执行这些流程并计算 $[b_1, b_2]$。最终输出汇总这些结果。",
            "answer": "```python\nimport numpy as np\n\ndef fit_ols(X_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Fits an Ordinary Least Squares model with an intercept.\n    \n    Args:\n        X_train: Predictor matrix (n_samples, n_features).\n        y_train: Response vector (n_samples,).\n        \n    Returns:\n        Coefficient vector beta_hat, including the intercept.\n    \"\"\"\n    X_aug = np.c_[np.ones(X_train.shape[0]), X_train]\n    beta, _, _, _ = np.linalg.lstsq(X_aug, y_train, rcond=None)\n    return beta\n\ndef predict(X: np.ndarray, beta: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Makes predictions using a fitted OLS model.\n    \n    Args:\n        X: Predictor matrix (n_samples, n_features).\n        beta: Coefficient vector, including the intercept.\n        \n    Returns:\n        Predicted response vector (n_samples,).\n    \"\"\"\n    X_aug = np.c_[np.ones(X.shape[0]), X]\n    return X_aug @ beta\n\ndef calculate_mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculates Mean Squared Error.\"\"\"\n    return np.mean((y_true - y_pred)**2)\n\ndef calculate_rss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    \"\"\"Calculates Residual Sum of Squares.\"\"\"\n    return np.sum((y_true - y_pred)**2)\n\ndef solve_case(n_train, n_val, s, sigma, seed):\n    \"\"\"\n    Solves a single test case for the model selection problem.\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n\n    # Training data\n    x1_train = rng.normal(loc=0.0, scale=1.0, size=n_train)\n    r_train = rng.normal(loc=0.0, scale=s, size=n_train)\n    x2_train = -x1_train + r_train\n    eps_train = rng.normal(loc=0.0, scale=sigma, size=n_train)\n    y_train = r_train + eps_train\n    X_train = np.stack([x1_train, x2_train], axis=1)\n\n    # Validation data\n    x1_val = rng.normal(loc=0.0, scale=1.0, size=n_val)\n    r_val = rng.normal(loc=0.0, scale=s, size=n_val)\n    x2_val = -x1_val + r_val\n    eps_val = rng.normal(loc=0.0, scale=sigma, size=n_val)\n    y_val = r_val + eps_val\n    X_val = np.stack([x1_val, x2_val], axis=1)\n\n    # Empty matrix for intercept-only model\n    X_empty_train = np.empty((n_train, 0))\n    X_empty_val = np.empty((n_val, 0))\n\n    # --- Computation for b1 ---\n    \n    # 2.1. Forward stepwise path on training data (minimizing RSS)\n    # Step 1: Find which variable to add first\n    beta1 = fit_ols(X_train[:, [0]], y_train)\n    y_pred_train1 = predict(X_train[:, [0]], beta1)\n    rss1 = calculate_rss(y_train, y_pred_train1)\n\n    beta2 = fit_ols(X_train[:, [1]], y_train)\n    y_pred_train2 = predict(X_train[:, [1]], beta2)\n    rss2 = calculate_rss(y_train, y_pred_train2)\n    \n    forward_path = [0, 1] if rss1 = rss2 else [1, 0]\n\n    # 2.2. Best subset along the forward path (evaluating with validation MSE)\n    # Model k=0 (intercept-only)\n    beta_k0 = fit_ols(X_empty_train, y_train)\n    y_pred_val_k0 = predict(X_empty_val, beta_k0)\n    mse_k0 = calculate_mse(y_val, y_pred_val_k0)\n    \n    # Model k=1 (from forward path)\n    var_k1 = forward_path[0]\n    beta_k1 = fit_ols(X_train[:, [var_k1]], y_train)\n    y_pred_val_k1 = predict(X_val[:, [var_k1]], beta_k1)\n    mse_k1 = calculate_mse(y_val, y_pred_val_k1)\n\n    # Model k=2 (both variables)\n    beta_k2 = fit_ols(X_train, y_train)\n    y_pred_val_k2 = predict(X_val, beta_k2)\n    mse_k2 = calculate_mse(y_val, y_pred_val_k2)\n    \n    # 2.3. Evaluate b1 conditions\n    # Cond 1: Path has both vars (always true for p=2)\n    # Cond 2: Size-2 model is best\n    cond2_best_is_k2 = mse_k2  mse_k1 and mse_k2  mse_k0\n    # Cond 3: Size-1 model is worse than size-0\n    cond3_k1_worse_than_k0 = mse_k1 > mse_k0\n    b1 = cond2_best_is_k2 and cond3_k1_worse_than_k0\n\n    # --- Computation for b2 ---\n\n    # 3.1. Greedy validation-based forward selection with early stopping\n    # Step 1: Check if adding any single variable improves on k=0 model\n    # We need validation MSE for both size-1 models\n    beta_v1 = fit_ols(X_train[:, [0]], y_train)\n    y_pred_val_v1 = predict(X_val[:, [0]], beta_v1)\n    mse_v1 = calculate_mse(y_val, y_pred_val_v1)\n    \n    beta_v2 = fit_ols(X_train[:, [1]], y_train)\n    y_pred_val_v2 = predict(X_val[:, [1]], beta_v2)\n    mse_v2 = calculate_mse(y_val, y_pred_val_v2)\n    \n    # 3.2. Evaluate b2 condition\n    # Greedy search stops if neither variable offers a strict improvement\n    b2 = mse_v1 >= mse_k0 and mse_v2 >= mse_k0\n\n    return [b1, b2]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (60, 2000, 0.05, 0.02, 0),    # Case A\n        (60, 2000, 0.5, 0.02, 1),     # Case B\n        (30, 5000, 0.005, 0.005, 2), # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        n_train, n_val, s, sigma, seed = case\n        result = solve_case(n_train, n_val, s, sigma, seed)\n        results.append(result)\n\n    # Format results into the required string representation\n    formatted_results = [f\"[{str(b1)},{str(b2)}]\" for b1, b2 in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "模型选择涉及到一个根本性的权衡：计算成本与所选模型的最优性。本练习将前向选择置于一系列方法中进行比较，其对比对象包括一个允许修正性“交换”的改进版本，以及计算量巨大但能保证全局最优的最佳子集选择 。通过在精心设计的数据集上实现这三种程序，您将直接体验到增加算法复杂度有时如何能够带来更好的模型，并学会理解在统计建模中所做的各种实际权衡。",
            "id": "3104971",
            "problem": "请实现并比较三种用于带截距项的普通最小二乘法线性回归的模型选择过程：纯粹向前逐步选择、在达到固定模型大小后允许进行一次交换的向前选择，以及穷举最佳子集选择。请从最小二乘回归和残差平方和的核心定义出发，并利用这些原理来推导算法。\n\n给定一个响应向量 $y \\in \\mathbb{R}^n$ 和一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其列 $x_j$ 对应 $p$ 个候选预测变量，模型总是包含一个截距项。对于任何索引集 $S \\subseteq \\{1,\\dots,p\\}$，普通最小二乘法下的拟合值是通过最小化残差平方和得到的：\n$$\n\\mathrm{RSS}(S) \\;=\\; \\min_{\\beta_0,\\{\\beta_j\\}_{j \\in S}} \\; \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\sum_{j \\in S} \\beta_j x_{ij}\\right)^2,\n$$\n其中 $\\beta_0 \\in \\mathbb{R}$ 是截距项，$\\beta_j \\in \\mathbb{R}$ 是所选预测变量的系数。最小残差平方和可通过求解最小二乘法正规方程或使用伪逆得到；您可以假设标准的线性代数解是可用的。\n\n对于目标模型大小 $k$（其中 $1 \\le k \\le p$），请定义以下过程：\n\n- 纯粹向前逐步选择：从空集 $S=\\varnothing$ 开始，在每次迭代中，加入能使 $\\mathrm{RSS}(S \\cup \\{j\\})$ 减少量最大的单个变量 $j \\notin S$。重复此过程直到 $|S| = k$。\n\n- 带一次交换的向前选择（一次偶然的向后步骤）：首先运行纯粹向前选择，得到一个大小为 $k$ 的集合 $S$。然后，考虑所有可能的单次替换 $S' = (S \\setminus \\{u\\}) \\cup \\{v\\}$，其中 $u \\in S$ 且 $v \\notin S$，计算 $\\mathrm{RSS}(S')$。如果且仅当存在一个替换能产生比当前 $\\mathrm{RSS}(S)$ 严格更小的 RSS 值时，执行那个能产生最小 RSS 值的单次交换。最多执行一次此类交换。\n\n- 最佳子集选择：评估所有 $\\binom{p}{k}$ 个大小为 $k$ 的子集 $S$，并选择使 $\\mathrm{RSS}(S)$ 最小的那个集合。\n\n您的程序必须构建三个合成测试数据集，旨在探究存在协同预测变量对和局部最小值的情况：\n\n- 所有测试用例均使用以下基向量（这些是长度为8的向量）：\n  - $u = [\\,4,\\,-3,\\,5,\\,-4,\\,3,\\,-2,\\,1,\\,0\\,]$，\n  - $v = [\\,1,\\,2,\\,-1,\\,-2,\\,0,\\,1,\\,-1,\\,0\\,]$。\n  - 对于任意辅助向量 $a \\in \\mathbb{R}^8$ 和一组基向量 $B = \\{b_1,\\dots,b_m\\}$（其中 $m \\in \\{0,1,2,\\dots\\}$），定义正交化向量：\n    $$\n    \\mathrm{orth}(a;B) \\;=\\; a \\;-\\; \\sum_{\\ell=1}^{m} \\hat{\\gamma}_{\\ell} \\, b_{\\ell},\n    $$\n    其中系数 $\\hat{\\gamma}_{\\ell}$ 是通过求解最小二乘问题 $\\min_{\\gamma \\in \\mathbb{R}^m} \\lVert a - \\sum_{\\ell=1}^m \\gamma_{\\ell} b_{\\ell} \\rVert_2^2$ 得到的。这相当于从 $a$ 中减去其在 $B$ 的张成空间上的投影。\n\n- 测试用例 1（一个诱饵变量可通过单次交换被纠正的协同对）：\n  - 设 $n = 8, p = 4, k = 2$。\n  - 定义 $y = u + v$。\n  - 预测变量：\n    - $x_1 = u$,\n    - $x_2 = v$,\n    - 设 $s_0 = [\\,1,\\,-1,\\,2,\\,-2,\\,1,\\,-1,\\,2,\\,-2\\,]$ 且 $s = \\mathrm{orth}(s_0;\\{u,v\\})$,\n    - $x_3 = u + 0.5\\,y + 0.05\\,s$，这是一个与 $y$ 强相关、且被扰动到 $\\{u,v\\}$ 的张成空间之外以防止被 $x_2$ 精确表示的强诱饵变量,\n    - $x_4 = \\mathrm{orth}([\\,0.2,\\,-0.1,\\,0.05,\\,0.1,\\,-0.4,\\,0.3,\\,0.2,\\,-0.1\\,];\\{u,v,s\\})$，一个无关噪声方向。\n  - 直觉：大小为 $k$ 的最佳子集预期为 $\\{x_1,x_2\\}$，而纯粹向前选择可能因 $x_3$ 与 $y$ 的边际对齐度被放大而首先选择它。单次交换应能纠正这个早期错误。\n\n- 测试用例 2（纯粹向前选择已是最佳的理想情况）：\n  - 设 $n = 8, p = 3, k = 2$。\n  - 定义 $y = u + v$。\n  - 预测变量：\n    - $x_1 = u$,\n    - $x_2 = v$,\n    - $x_3 = \\mathrm{orth}([\\,2,\\,-2,\\,1,\\,-1,\\,2,\\,-2,\\,1,\\,-1\\,];\\{u,v\\})$，一个与信号张成空间正交的纯噪声方向。\n  - 直觉：向前选择和单次交换选择都应与最佳子集选择相匹配，因为信号是干净的，且诱饵变量是正交的。\n\n- 测试用例 3（单次交换带来局部改进但非全局最优）：\n  - 设 $n = 8, p = 4, k = 2$。\n  - 定义 $y = u + v$。\n  - 预测变量：\n    - $x_1 = u$,\n    - $x_2 = v$,\n    - 设 $t_1 = \\mathrm{orth}([\\,1,\\,2,\\,3,\\,4,\\,5,\\,6,\\,7,\\,8\\,];\\{u,v\\})$,\n    - 设 $t_2 = \\mathrm{orth}([\\,2,\\,-1,\\,0,\\,1,\\,-2,\\,3,\\,-3,\\,1\\,];\\{u,v,t_1\\})$,\n    - $x_3 = x_1 + 0.6\\,y + 0.2\\,t_1$,\n    - $x_4 = x_2 + 0.3\\,y + 0.4\\,t_2$。\n  - 直觉：向前选择可能因 $x_3$ 和 $x_4$ 的高边际对齐度而选择 $\\{x_3,x_4\\}$；单次交换（例如，用其干净的对应物 $x_1$ 替换一个诱饵变量）可以减少残差平方和，但可能仍无法匹配真正的最优对 $\\{x_1,x_2\\}$。\n\n对于每个测试用例，计算：\n- 纯粹向前选择法选出的大小为 $k$ 的集合及其残差平方和。\n- 经过单次交换精炼后的集合及其残差平方和。\n- 大小为 $k$ 的最佳子集及其残差平方和。\n\n在进行数值比较时，使用容差 $\\varepsilon = 10^{-8}$ 来判断残差平方和是否相等。\n\n您的程序必须为每个测试用例输出一个整数代码来总结结果：\n- 输出 $0$：如果单次交换方法未改变向前选择集，并且向前选择集在容差范围内与最佳子集匹配。\n- 输出 $1$：如果单次交换方法未改变向前选择集，并且向前选择集是严格次优的。\n- 输出 $2$：如果单次交换方法改变了向前选择集，并严格降低了向前选择的残差平方和，但仍然未在容差范围内与最佳子集匹配。\n- 输出 $3$：如果单次交换方法改变了向前选择集，并且在容差范围内与最佳子集匹配。\n\n最终输出格式：您的程序应生成一行包含测试用例1、2和3的整数代码的逗号分隔列表，并用方括号括起来（例如，\"[3,0,2]\"）。不应打印任何额外的文本。该问题不涉及物理单位、角度或百分比；所有输出都是无单位的整数。实现必须是完全确定性的，并且只能使用指定的环境。",
            "solution": "用户要求实现并比较三种用于线性回归的模型选择算法：纯粹向前逐步选择、带单次交换的向前选择和最佳子集选择。该任务涉及创建旨在揭示这些算法不同行为的合成数据集，并根据预定义的方案对结果进行分类。整个过程必须基于普通最小二乘法（OLS）和残差平方和（RSS）的原理。\n\n### 1. 基本原理：普通最小二乘法与残差平方和\n\n这个问题的基石是普通最小二乘法（OLS）线性回归模型。给定一个响应向量 $y \\in \\mathbb{R}^n$ 和一组由矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列表示的 $p$ 个候选预测变量，我们旨在找到一个能最好地解释 $y$ 的线性模型。对于由索引集 $S \\subseteq \\{1, \\dots, p\\}$ 选定的一组预测变量，模型为：\n$$\ny_i = \\beta_0 + \\sum_{j \\in S} \\beta_j x_{ij} + \\epsilon_i\n$$\n其中 $\\beta_0$ 是截距项，$\\{\\beta_j\\}_{j \\in S}$ 是预测变量的系数，$\\epsilon_i$ 是误差项。OLS方法通过最小化残差平方和（RSS）来找到系数，RSS定义为观测值与预测值之间差异的平方和：\n$$\n\\mathrm{RSS}(S) = \\min_{\\beta_0, \\{\\beta_j\\}_{j \\in S}} \\sum_{i=1}^{n} \\left(y_i - \\left(\\beta_0 + \\sum_{j \\in S} \\beta_j x_{ij}\\right)\\right)^2\n$$\n这是一个标准的矩阵问题。令 $X_S$ 为一个 $n \\times |S|$ 矩阵，包含 $X$ 中与 $S$ 中索引对应的列。我们构建一个增广设计矩阵 $A_S = [\\mathbf{1} | X_S]$，其中 $\\mathbf{1}$ 是一个代表截距项的 $n \\times 1$ 全1列向量。OLS问题是找到系数向量 $\\boldsymbol{\\beta}$，以最小化 $\\|y - A_S \\boldsymbol{\\beta}\\|_2^2$。解可以通过正规方程求得，或者更稳健地，通过QR分解或奇异值分解（SVD）求得。在我们的实现中，我们使用 `numpy.linalg.lstsq`，它提供了一个高效且数值稳定的解，并直接返回最小化的RSS。我们实现了一个辅助函数 `calculate_rss`，用于对任何给定的子集 $S$ 执行此核心计算。\n\n一个辅助计算是将向量 $a$ 相对于一组基向量 $B = \\{b_1, \\dots, b_m\\}$ 进行正交化。这定义为 $a$ 减去其在 $B$ 的张成空间上的投影：\n$$\n\\mathrm{orth}(a;B) = a - \\mathrm{proj}_{\\mathrm{span}(B)}(a)\n$$\n该投影通过求解最小二乘问题 $\\min_{\\gamma} \\|a - B\\gamma\\|_2^2$ 来找到系数 $\\gamma$，其中 $B$ 是以 $\\{b_1, \\dots, b_m\\}$ 为列的矩阵。正交化后的向量即为 $a - B\\hat{\\gamma}$。这也使用 `numpy.linalg.lstsq` 实现。\n\n### 2. 算法实现\n\n基于核心的RSS计算，我们为固定的模型大小 $k$ 实现了三种指定的模型选择算法。\n\n**纯粹向前逐步选择：** 这是一种贪心迭代算法。\n1. 从空模型 $S_0 = \\varnothing$ 开始。\n2. 对于 $i=1, \\dots, k$：找到单个预测变量 $j \\notin S_{i-1}$，当其加入当前模型时，能得到最低的RSS。即，找到 $j^* = \\arg\\min_{j \\notin S_{i-1}} \\mathrm{RSS}(S_{i-1} \\cup \\{j\\})$。\n3. 更新模型：$S_i = S_{i-1} \\cup \\{j^*\\}$。\n4. 最终模型为 $S_k$。\n这种方法计算效率高，但如果预测变量具有协同作用，可能会找不到全局最优模型，因为早期的贪心选择从长远来看可能是次优的。\n\n**带一次交换的向前选择：** 此过程是对纯粹向前选择的一种启发式改进，旨在减轻其短视性。\n1. 首先，执行纯粹向前选择，得到一个大小为 $k$ 的模型 $S_{fwd}$。\n2. 接下来，探索 $S_{fwd}$ 的邻域，该邻域由所有可通过单次交换达到的模型组成。一次交换涉及移除一个预测变量 $u \\in S_{fwd}$ 并添加一个预测变量 $v \\notin S_{fwd}$。\n3. 对每个可能的交换后模型 $S' = (S_{fwd} \\setminus \\{u\\}) \\cup \\{v\\}$，计算其RSS。\n4. 找出能最大程度降低RSS的单次交换。如果这次最佳交换产生的RSS严格低于 $S_{fwd}$ 的RSS，则更新模型。否则，模型保持不变。最多执行一次交换。\n\n**最佳子集选择：** 这是穷举的“黄金标准”方法。\n1. 检查所有大小为 $k$ 的可能预测变量子集。这样的子集数量为 $\\binom{p}{k}$。\n2. 对每个子集 $S$，计算 $\\mathrm{RSS}(S)$。\n3. 最终模型是所有候选中RSS最小的子集 $S^*$。\n虽然该方法保证能找到给定大小 $k$ 下RSS最低的模型，但对于中等大小的 $p$ 值，其计算上是不可行的。对于本问题中的小 $p$ 值（$p \\le 4$），它是完全可行的。\n\n### 3. 测试用例与分析\n\n问题指定了三个测试用例来探究这些算法的行为。对于所有情况，$n=8, k=2$，真实的响应是 $y=u+v$，其中 $u,v$ 是给定的基向量。大小为 $k=2$ 的最佳子集总是 $\\{x_1, x_2\\}$（其中 $x_1=u, x_2=v$），它完美地解释了 $y$，并产生（或数值上接近于）0的RSS。预测变量的构建旨在制造特定的挑战。\n\n- **案例1：** 一个“诱饵”预测变量 $x_3$ 与 $y$ 高度相关，诱使向前选择首先选择它。这导致了一个次优模型。然而，单次交换过程能够通过将诱饵变量换成正确的预测变量来纠正这个错误，从而达到全局最优。预计将得到代码 `3`。\n- **案例2：** 预测变量的结构使得向前选择的贪心选择与全局最优路径一致。一个噪声预测变量被构造成与信号空间正交，因此它对算法永远没有吸引力。所有三种方法都应在最优模型上达成一致。预计将得到代码 `0`。\n- **案例3：** 预测变量 $x_3$ 和 $x_4$ 被构建为真实预测变量 $x_1$ 和 $x_2$ 的“混淆”版本。向前选择预计会选择 $\\{x_3, x_4\\}$ 对。单次交换可以改进这个模型（例如，用 $x_1$ 交换 $x_3$），但需要第二次交换才能达到全局最优 $\\{x_1, x_2\\}$，而这是不允许的。因此，单次交换过程会找到一个局部最小值，它优于向前选择的结果，但仍然是次优的。预计将得到代码 `2`。\n\n该实现将这些算法和测试用例生成逻辑整合到一个程序中。每个测试用例的最终输出是一个整数代码，通过将三种算法得到的结果集及其RSS值与数值容差 $\\varepsilon = 10^{-8}$ 进行比较来确定。",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Implements and compares pure forward selection, forward selection with one swap,\n    and best subset selection for linear regression.\n    \"\"\"\n\n    # Epsilon for floating point comparisons\n    EPS = 1e-8\n\n    def orth(a, B_vectors):\n        \"\"\"Orthogonalizes vector a with respect to the span of vectors in B.\"\"\"\n        if not B_vectors:\n            return a\n        B = np.stack(B_vectors, axis=1)\n        # Solve least squares for a = B*gamma\n        gamma_hat = np.linalg.lstsq(B, a, rcond=None)[0]\n        projection = B @ gamma_hat\n        return a - projection\n\n    def calculate_rss(X, y, S_indices):\n        \"\"\"Calculates Residual Sum of Squares for a given subset of predictors.\"\"\"\n        n = X.shape[0]\n        if not S_indices:\n            # Intercept-only model\n            A = np.ones((n, 1))\n        else:\n            S_list = sorted(list(S_indices))\n            X_S = X[:, S_list]\n            A = np.c_[np.ones(n), X_S]\n\n        res = np.linalg.lstsq(A, y, rcond=None)[1]\n        \n        # Fallback if lstsq doesn't return RSS \n        if res.size == 0:\n            coeffs = np.linalg.lstsq(A, y, rcond=None)[0]\n            y_hat = A @ coeffs\n            return np.sum((y - y_hat)**2)\n            \n        return res[0]\n\n    def forward_selection(X, y, k):\n        \"\"\"Performs pure forward stepwise selection.\"\"\"\n        p = X.shape[1]\n        current_S = set()\n        \n        for _ in range(k):\n            best_new_predictor = -1\n            min_rss = float('inf')\n            \n            available_predictors = set(range(p)) - current_S\n            for j in available_predictors:\n                candidate_S = current_S | {j}\n                rss = calculate_rss(X, y, candidate_S)\n                if rss  min_rss:\n                    min_rss = rss\n                    best_new_predictor = j\n            \n            if best_new_predictor != -1:\n                current_S.add(best_new_predictor)\n            \n        final_rss = calculate_rss(X, y, current_S)\n        return frozenset(current_S), final_rss\n\n    def forward_one_swap(X, y, S_fwd, rss_fwd):\n        \"\"\"Performs forward selection refinement with one swap.\"\"\"\n        p = X.shape[1]\n        all_predictors = set(range(p))\n        \n        best_swap_S = S_fwd\n        best_swap_rss = rss_fwd\n        \n        predictors_in = S_fwd\n        predictors_out = all_predictors - S_fwd\n        \n        for u in predictors_in: # predictor to remove\n            for v in predictors_out: # predictor to add\n                candidate_S = (S_fwd - {u}) | {v}\n                candidate_rss = calculate_rss(X, y, candidate_S)\n                if candidate_rss  best_swap_rss:\n                    best_swap_rss = candidate_rss\n                    best_swap_S = candidate_S\n                    \n        return frozenset(best_swap_S), best_swap_rss\n\n    def best_subset_selection(X, y, k):\n        \"\"\"Performs exhaustive best subset selection.\"\"\"\n        p = X.shape[1]\n        best_S = None\n        min_rss = float('inf')\n        \n        for S_tuple in combinations(range(p), k):\n            S = frozenset(S_tuple)\n            rss = calculate_rss(X, y, S)\n            if rss  min_rss:\n                min_rss = rss\n                best_S = S\n                \n        return best_S, min_rss\n\n    def generate_data(case_id):\n        \"\"\"Generates synthetic data for the specified test case.\"\"\"\n        u = np.array([4., -3., 5., -4., 3., -2., 1., 0.])\n        v = np.array([1., 2., -1., -2., 0., 1., -1., 0.])\n        y = u + v\n        n = 8\n        \n        if case_id == 1:\n            p, k = 4, 2\n            x1 = u\n            x2 = v\n            s0 = np.array([1., -1., 2., -2., 1., -1., 2., -2.])\n            s = orth(s0, [u, v])\n            x3 = u + 0.5 * y + 0.05 * s\n            x4_raw = np.array([0.2, -0.1, 0.05, 0.1, -0.4, 0.3, 0.2, -0.1])\n            x4 = orth(x4_raw, [u, v, s])\n            X = np.stack([x1, x2, x3, x4], axis=1)\n            return X, y, p, k\n        \n        elif case_id == 2:\n            p, k = 3, 2\n            x1 = u\n            x2 = v\n            x3_raw = np.array([2., -2., 1., -1., 2., -2., 1., -1.])\n            x3 = orth(x3_raw, [u, v])\n            X = np.stack([x1, x2, x3], axis=1)\n            return X, y, p, k\n            \n        elif case_id == 3:\n            p, k = 4, 2\n            x1 = u\n            x2 = v\n            t1_raw = np.array([1., 2., 3., 4., 5., 6., 7., 8.])\n            t1 = orth(t1_raw, [u, v])\n            t2_raw = np.array([2., -1., 0., 1., -2., 3., -3., 1.])\n            t2 = orth(t2_raw, [u, v, t1])\n            x3 = x1 + 0.6 * y + 0.2 * t1\n            x4 = x2 + 0.3 * y + 0.4 * t2\n            X = np.stack([x1, x2, x3, x4], axis=1)\n            return X, y, p, k\n        \n        else:\n            raise ValueError(\"Invalid case_id\")\n\n    results = []\n    for case_id in [1, 2, 3]:\n        X, y, p, k = generate_data(case_id)\n        \n        S_fwd, rss_fwd = forward_selection(X, y, k)\n        S_swap, rss_swap = forward_one_swap(X, y, S_fwd, rss_fwd)\n        S_best, rss_best = best_subset_selection(X, y, k)\n        \n        set_changed = (S_fwd != S_swap)\n        fwd_is_optimal = (abs(rss_fwd - rss_best) = EPS)\n        swap_is_optimal = (abs(rss_swap - rss_best) = EPS)\n\n        code = -1\n        if not set_changed:\n            if fwd_is_optimal:\n                code = 0\n            else:\n                code = 1\n        else: # set_changed is True\n            if swap_is_optimal:\n                code = 3\n            else:\n                code = 2\n        \n        results.append(code)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}