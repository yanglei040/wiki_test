{
    "hands_on_practices": [
        {
            "introduction": "To truly understand backward elimination, it is invaluable to walk through the decision-making process step by step. This exercise simulates a manual selection procedure by providing the necessary summary statistics, the coefficients of determination ($R^2$), for all possible models.\n\nUsing this information, you will employ the partial $F$-test to assess the significance of each predictor at every stage of the elimination process. This practice solidifies the core mechanics of the algorithm and demonstrates how greedy, sequential decisions based on a statistical criterion lead to a final model, which may differ from models produced by other selection strategies .",
            "id": "1938945",
            "problem": "A materials scientist is developing a predictive model for the tensile strength ($Y$) of a new polymer composite. Three candidate predictor variables are being considered: the concentration of a reinforcing fiber ($X_1$), the concentration of a plasticizer ($X_2$), and the curing temperature ($X_3$). A study with a sample size of $n=104$ was conducted to explore the relationships between these variables.\n\nInstead of the raw data, the following summary of the coefficients of determination ($R^2$) for all possible multiple linear regression models is available:\n- $R^2$ for model with only $X_1$: $0.500$\n- $R^2$ for model with only $X_2$: $0.480$\n- $R^2$ for model with only $X_3$: $0.480$\n- $R^2$ for model with $X_1$ and $X_2$: $0.510$\n- $R^2$ for model with $X_1$ and $X_3$: $0.510$\n- $R^2$ for model with $X_2$ and $X_3$: $0.690$\n- $R^2$ for model with $X_1$, $X_2$, and $X_3$: $0.700$\n\nYour task is to determine the final set of predictors that would be chosen by two common automated model selection procedures: forward selection and backward elimination. For both procedures, a variable is entered into the model or removed from the model if the p-value associated with its corresponding F-statistic is less than a significance level of $\\alpha = 0.05$.\n\nFor the sample size $n=104$, the relevant critical F-value for a single parameter test at the $\\alpha = 0.05$ significance level can be taken as $F_{crit} = 3.94$. A calculated F-statistic greater than this value corresponds to a p-value less than $0.05$.\n\nWhich of the following options correctly identifies the final models for both procedures?\n\nA. Forward Selection: $\\{X_1\\}$; Backward Elimination: $\\{X_2, X_3\\}$\n\nB. Forward Selection: $\\{X_2, X_3\\}$; Backward Elimination: $\\{X_1\\}$\n\nC. Forward Selection: $\\{X_1, X_2, X_3\\}$; Backward Elimination: $\\{X_1\\}$\n\nD. Forward Selection: $\\{X_1\\}$; Backward Elimination: $\\{X_1\\}$\n\nE. Forward Selection: $\\{X_2, X_3\\}$; Backward Elimination: $\\{X_2, X_3\\}$\n\nF. Forward Selection: $\\{X_1\\}$; Backward Elimination: $\\{X_1, X_2, X_3\\}$",
            "solution": "We will perform the forward selection and backward elimination procedures step-by-step using the provided $R^2$ values and the critical F-value. The partial F-statistic is the key tool for these procedures.\n\nThe F-statistic for testing the inclusion or exclusion of a set of $q$ variables is given by the change in $R^2$:\n$$F = \\frac{(R^2_{full} - R^2_{reduced}) / q}{(1 - R^2_{full}) / (n - p_{full} - 1)}$$\nwhere $R^2_{full}$ and $R^2_{reduced}$ are the coefficients of determination for the models with and without the $q$ variables, respectively, $n$ is the sample size, and $p_{full}$ is the number of predictors in the full model. In our case, we are adding or removing one variable at a time, so $q=1$.\n\nWe are given $n=104$ and a critical F-value of $F_{crit}=3.94$ for $\\alpha = 0.05$.\n\n**Forward Selection Procedure**\n\nForward selection starts with an empty model (only the intercept) and adds predictors one at a time.\n\n**Step 1: Identify the first variable to enter.**\nWe test adding each variable individually to the null model (which has $R^2=0$).\n- For adding $X_1$: $p_{full}=1$.\n  $F_{X_1} = \\frac{(0.500 - 0) / 1}{(1 - 0.500) / (104 - 1 - 1)} = \\frac{0.500}{0.500 / 102} = 102.0$\n- For adding $X_2$: $p_{full}=1$.\n  $F_{X_2} = \\frac{(0.480 - 0) / 1}{(1 - 0.480) / (104 - 1 - 1)} = \\frac{0.480}{0.520 / 102} \\approx 94.15$\n- For adding $X_3$: $p_{full}=1$.\n  $F_{X_3} = \\frac{(0.480 - 0) / 1}{(1 - 0.480) / (104 - 1 - 1)} = \\frac{0.480}{0.520 / 102} \\approx 94.15$\n\nAll three F-statistics are greater than $F_{crit}=3.94$. We select the variable with the largest F-statistic, which is $X_1$. The model is now $\\{X_1\\}$.\n\n**Step 2: Identify the second variable to enter.**\nThe current model is $\\{X_1\\}$ with $R^2_{old} = R^2(X_1) = 0.500$. We test adding $X_2$ or $X_3$.\n- For adding $X_2$ to $\\{X_1\\}$: $p_{full}=2$. The full model is $\\{X_1, X_2\\}$ with $R^2_{full}=0.510$.\n  $F_{X_2 | X_1} = \\frac{(R^2(X_1, X_2) - R^2(X_1)) / 1}{(1 - R^2(X_1, X_2)) / (104 - 2 - 1)} = \\frac{(0.510 - 0.500) / 1}{(1 - 0.510) / 101} = \\frac{0.010}{0.490 / 101} \\approx 2.06$\n- For adding $X_3$ to $\\{X_1\\}$: $p_{full}=2$. The full model is $\\{X_1, X_3\\}$ with $R^2_{full}=0.510$.\n  $F_{X_3 | X_1} = \\frac{(R^2(X_1, X_3) - R^2(X_1)) / 1}{(1 - R^2(X_1, X_3)) / (104 - 2 - 1)} = \\frac{(0.510 - 0.500) / 1}{(1 - 0.510) / 101} = \\frac{0.010}{0.490 / 101} \\approx 2.06$\n\nBoth of these F-statistics are less than $F_{crit}=3.94$. Therefore, neither $X_2$ nor $X_3$ can be added to the model. The procedure terminates.\nThe final model from forward selection is $\\{X_1\\}$.\n\n**Backward Elimination Procedure**\n\nBackward elimination starts with the full model containing all predictors and removes them one at a time.\n\n**Step 1: Identify the first variable to remove from the full model.**\nThe starting model is $\\{X_1, X_2, X_3\\}$ with $R^2_{full}=0.700$ and $p_{full}=3$. We test removing each variable. The denominator for all F-tests in this step is $(1 - R^2_{full}) / (n - p_{full} - 1) = (1 - 0.700) / (104 - 3 - 1) = 0.300 / 100 = 0.003$.\n- For removing $X_1$: The reduced model is $\\{X_2, X_3\\}$ with $R^2_{reduced}=0.690$.\n  $F_{rem, X_1} = \\frac{(0.700 - 0.690) / 1}{0.003} = \\frac{0.010}{0.003} \\approx 3.33$\n- For removing $X_2$: The reduced model is $\\{X_1, X_3\\}$ with $R^2_{reduced}=0.510$.\n  $F_{rem, X_2} = \\frac{(0.700 - 0.510) / 1}{0.003} = \\frac{0.190}{0.003} \\approx 63.33$\n- For removing $X_3$: The reduced model is $\\{X_1, X_2\\}$ with $R^2_{reduced}=0.510$.\n  $F_{rem, X_3} = \\frac{(0.700 - 0.510) / 1}{0.003} = \\frac{0.190}{0.003} \\approx 63.33$\n\nWe look for any F-statistics that are less than $F_{crit}=3.94$. Only the test for removing $X_1$ results in an F-statistic ($3.33$) below the critical value. If multiple variables were candidates for removal, we would remove the one with the smallest F-statistic. Here, only $X_1$ is a candidate. So we remove $X_1$.\nThe model is now $\\{X_2, X_3\\}$.\n\n**Step 2: Identify the next variable to remove.**\nThe current model is $\\{X_2, X_3\\}$ with $R^2_{full}=0.690$ and $p_{full}=2$. We test removing the remaining variables. The denominator for the F-tests is $(1 - R^2_{full}) / (n - p_{full} - 1) = (1 - 0.690) / (104 - 2 - 1) = 0.310 / 101 \\approx 0.00307$.\n- For removing $X_2$: The reduced model is $\\{X_3\\}$ with $R^2_{reduced}=0.480$.\n  $F_{rem, X_2} = \\frac{(0.690 - 0.480) / 1}{0.310 / 101} = \\frac{0.210}{0.310 / 101} \\approx 68.42$\n- For removing $X_3$: The reduced model is $\\{X_2\\}$ with $R^2_{reduced}=0.480$.\n  $F_{rem, X_3} = \\frac{(0.690 - 0.480) / 1}{0.310 / 101} = \\frac{0.210}{0.310 / 101} \\approx 68.42$\n\nBoth of these F-statistics are much greater than $F_{crit}=3.94$. Therefore, no more variables can be removed from the model. The procedure terminates.\nThe final model from backward elimination is $\\{X_2, X_3\\}$.\n\n**Conclusion**\nThe forward selection procedure resulted in the model $\\{X_1\\}$. The backward elimination procedure resulted in the model $\\{X_2, X_3\\}$. This corresponds to option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Automated model selection tools like backward elimination are powerful, but they operate without context and can be easily misled by features that are spuriously predictive. This exercise demonstrates a classic pitfall known as data leakage, where a feature appears highly informative only because it improperly contains information about the outcome.\n\nYou will contrast a naive application of backward selection using the Bayesian Information Criterion ($BIC$) with a more robust workflow that includes a pre-processing step to detect and remove leaking variables. This practice imparts a critical lesson in applied modeling: algorithmic rigor must be paired with careful data inspection to build reliable and generalizable models .",
            "id": "3101321",
            "problem": "You must write a complete program that constructs synthetic regression datasets and demonstrates how a naive backward stepwise selection can retain a variable that is only predictive due to data leakage, while a leakage-aware preprocessing step can detect and remove such variables before selection. The program must implement both the leakage detection and the backward stepwise selection, and must output results for a specified test suite. All computations must be expressed in purely mathematical and logical terms without external input.\n\nStart from the following foundational base:\n- Ordinary Least Squares (OLS) linear regression assumes a response vector $y \\in \\mathbb{R}^n$ is generated from predictors $X \\in \\mathbb{R}^{n \\times p}$ and an intercept via $y = \\beta_0 \\mathbf{1} + X \\beta + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$.\n- The OLS estimator minimizes the residual sum of squares (RSS), where $\\mathrm{RSS} = \\lVert y - \\hat{y} \\rVert_2^2$.\n- Under the Gaussian noise model, the maximized log-likelihood can be expressed in terms of $\\mathrm{RSS}$, and the Bayesian Information Criterion (BIC) for a model with $k$ parameters (including the intercept) and $n$ observations is $ \\mathrm{BIC} = n \\log\\left(\\frac{\\mathrm{RSS}}{n}\\right) + k \\log(n)$. Lower $\\mathrm{BIC}$ indicates a preferred model.\n- Backward stepwise selection begins from a full set of candidate features and iteratively removes a single feature at a time, selecting the removal that most reduces the chosen information criterion, and stops when no single-feature removal improves it. If all features are removed, the model reduces to the intercept-only model.\n- Sample correlation between two vectors $a, b \\in \\mathbb{R}^n$ is the Pearson correlation, which is the covariance divided by the product of standard deviations.\n\nYour program must:\n1) Implement backward stepwise selection driven by BIC:\n   - Given $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^n$, with an intercept always included (not counted among the $p$ candidate features for removal), start from all $p$ features and at each step evaluate removing each single remaining feature. Remove the feature whose removal yields the smallest BIC if and only if that BIC is strictly smaller than the current BIC. Repeat until no removal reduces BIC. If all features are removed, the model reduces to the intercept-only model.\n\n2) Implement a leakage detection routine that flags suspicious variables before selection using only $X$, $y$, and the observation index vector $t = [0, 1, \\dots, n-1]^\\top$:\n   - Rule A (direct label-copy detection): If a single feature $x_j$ has absolute correlation with $y$ greater than a high threshold $\\tau_{\\text{direct}} = 0.995$, flag $x_j$ as leakage.\n   - Rule B (time-proxy detection): If a single feature $x_j$ has absolute correlation with the time index vector $t$ greater than $\\tau_{\\text{time}} = 0.98$ and absolute correlation with $y$ greater than $\\tau_{\\text{time},y} = 0.7$, flag $x_j$ as a likely time-based leakage proxy.\n   - All thresholds $\\tau_{\\text{direct}}$, $\\tau_{\\text{time}}$, and $\\tau_{\\text{time},y}$ are fixed and must be used exactly as specified.\n\n3) For each dataset, run backward selection twice:\n   - Naive selection: on all features, without leakage removal.\n   - Leakage-aware selection: after removing all features flagged by the leakage detection routine, then applying backward selection on the remaining features.\n\n4) Return, for each dataset, a triplet containing:\n   - The list of selected feature indices after naive selection.\n   - The list of selected feature indices after leakage-aware selection.\n   - A boolean indicating whether the designated candidate leakage feature (always the last column of $X$) was flagged by the leakage detection routine.\n\nBoth index lists must refer to the original column indices of $X$ (from $0$ to $p-1$), and must be sorted in increasing order.\n\nDataset generation details (test suite):\n- For all datasets, construct $X$ by horizontally concatenating three blocks: $X = [X_{\\text{true}} \\;|\\; X_{\\text{noise}} \\;|\\; x_{\\text{leak}}]$, where $X_{\\text{true}} \\in \\mathbb{R}^{n \\times q}$ are true predictors with nonzero coefficients, $X_{\\text{noise}} \\in \\mathbb{R}^{n \\times r}$ are pure noise predictors, and $x_{\\text{leak}} \\in \\mathbb{R}^n$ is the designated candidate leakage feature. The true response is generated as $y = X_{\\text{true}} \\beta + \\varepsilon$ or, in the specified time-trend case, $y = X_{\\text{true}} \\beta + \\gamma \\tilde{t} + \\varepsilon$, where $\\tilde{t}$ is the standardized time index vector with zero mean and unit variance. The noise $\\varepsilon$ is independent Gaussian with standard deviation $\\sigma$. Use an intercept in all regression fits.\n\nProvide the following four datasets with fixed parameters and random seeds for reproducibility:\n- Case $1$ (happy path with direct leakage):\n  - $n = 120$, $q = 2$, $r = 3$, $\\beta = [3.0, -2.0]$, $\\sigma = 1.5$.\n  - Random seed $42$.\n  - Leakage type: direct label copy; construct $x_{\\text{leak}} = y + \\eta$ with $\\eta \\sim \\mathcal{N}(0, \\sigma_{\\text{leak}}^2)$ and $\\sigma_{\\text{leak}} = 0.02$.\n- Case $2$ (no leakage, independent candidate):\n  - $n = 100$, $q = 3$, $r = 2$, $\\beta = [1.2, -0.8, 1.5]$, $\\sigma = 2.0$.\n  - Random seed $7$.\n  - Leakage type: random; construct $x_{\\text{leak}} \\sim \\mathcal{N}(0, 1)$ independent of everything else.\n- Case $3$ (small sample, many noise features):\n  - $n = 25$, $q = 2$, $r = 6$, $\\beta = [1.0, 0.5]$, $\\sigma = 1.0$.\n  - Random seed $123$.\n  - Leakage type: random; construct $x_{\\text{leak}} \\sim \\mathcal{N}(0, 1)$ independent of everything else.\n- Case $4$ (time-proxy leakage via trend):\n  - $n = 150$, $q = 1$, $r = 2$, $\\beta = [1.0]$, $\\sigma = 0.5$.\n  - Random seed $99$.\n  - Let $\\tilde{t}$ be the standardized time index vector. Response $y = X_{\\text{true}} \\beta + \\gamma \\tilde{t} + \\varepsilon$ with $\\gamma = 2.0$.\n  - Leakage type: time proxy; set $x_{\\text{leak}} = \\tilde{t}$.\n\nFor all cases, the entries of $X_{\\text{true}}$ and $X_{\\text{noise}}$ are independent and identically distributed as standard normal random variables, and the designated leakage variable is placed as the last column of $X$ so that its index is $p-1$ where $p = q + r + 1$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. Each dataset contributes one triplet of the form $[\\text{list\\_before},\\text{list\\_after},\\text{flagged}]$, where the lists contain integers and the flagged value is a boolean. The overall output is therefore a list of these four triplets, for the four datasets, for example, $[[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]]$.\n\nNo physical units, angles, or percentages are involved in this problem. All numeric thresholds and parameters must be used exactly as specified. The program must be self-contained, use the specified random seeds, and require no input.",
            "solution": "The problem posed is valid. It is a well-defined, self-contained, and scientifically grounded exercise in statistical learning, requiring the implementation and comparison of feature selection procedures under different data conditions. All necessary parameters, algorithms, and conditions are explicitly provided, allowing for a unique and verifiable solution.\n\nThe task is to construct a program that demonstrates the fallibility of a naive backward stepwise feature selection algorithm in the presence of data leakage, and how a pre-processing step designed to detect such leakage can rectify this issue. This will be achieved by implementing the necessary statistical components and applying them to four distinct, synthetically generated datasets.\n\nThe solution will be structured as follows: First, we define the Ordinary Least Squares (OLS) regression model and the Bayesian Information Criterion (BIC) used for model selection. Second, we formalize the backward stepwise selection algorithm. Third, we specify the heuristics for detecting data leakage. Finally, we outline the procedure for generating the datasets and running the comparative analysis.\n\n**1. Linear Regression and Model Selection**\n\nWe assume the standard linear model where a response vector $y \\in \\mathbb{R}^n$ is generated from a design matrix of predictors $X \\in \\mathbb{R}^{n \\times p}$ and a random error term $\\varepsilon \\in \\mathbb{R}^n$. The model, including an intercept term, is given by:\n$$ y = \\beta_0 \\mathbf{1} + X \\beta + \\varepsilon $$\nwhere $\\beta_0 \\in \\mathbb{R}$ is the intercept, $\\mathbf{1}$ is an $n$-dimensional vector of ones, $\\beta \\in \\mathbb{R}^p$ is the vector of feature coefficients, and the errors are assumed to be independent and identically distributed, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$.\n\nThe Ordinary Least Squares (OLS) method finds the estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}$ that minimize the Residual Sum of Squares (RSS):\n$$ \\mathrm{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\lVert y - \\hat{y} \\rVert_2^2 $$\nwhere $\\hat{y} = \\hat{\\beta}_0 \\mathbf{1} + X \\hat{\\beta}$ are the fitted values.\n\nFor model selection among a set of candidate models with varying subsets of predictors, we employ the Bayesian Information Criterion (BIC). The BIC for a model with $k$ estimated parameters (including the intercept) based on $n$ observations is defined as:\n$$ \\mathrm{BIC} = n \\log\\left(\\frac{\\mathrm{RSS}}{n}\\right) + k \\log(n) $$\nThe term $n \\log(\\mathrm{RSS}/n)$ is related to the model's goodness-of-fit (maximized log-likelihood), while $k \\log(n)$ is a penalty term for model complexity. A lower BIC value indicates a more parsimonious model that better balances fit and complexity.\n\n**2. Backward Stepwise Selection Algorithm**\n\nBackward stepwise selection is a greedy heuristic algorithm for feature selection. It starts with a model containing all candidate predictors and iteratively removes the least useful predictor until no further removal improves the model quality. The algorithm, driven by BIC, is as follows:\n\nLet $\\mathcal{F}$ be the set of all $p$ candidate feature indices, $\\mathcal{F} = \\{0, 1, \\dots, p-1\\}$.\n1.  **Initialization**: Start with the full model, where the set of active features is $\\mathcal{S}_{\\text{current}} = \\mathcal{F}$. Calculate the BIC for this model, $\\mathrm{BIC}_{\\text{current}}$.\n2.  **Iteration**:\n    a. For each feature $j \\in \\mathcal{S}_{\\text{current}}$, consider a trial model with the feature set $\\mathcal{S}_{\\text{trial}, j} = \\mathcal{S}_{\\text{current}} \\setminus \\{j\\}$. Calculate its corresponding BIC, $\\mathrm{BIC}_{\\text{trial}, j}$.\n    b. Find the feature $j^*$ whose removal results in the largest decrease (i.e., minimum value) in BIC: $j^* = \\arg\\min_{j \\in \\mathcal{S}_{\\text{current}}} \\mathrm{BIC}_{\\text{trial}, j}$. Let the best resulting BIC be $\\mathrm{BIC}_{\\text{best}} = \\mathrm{BIC}_{\\text{trial}, j^*}$.\n    c. **Decision**: If $\\mathrm{BIC}_{\\text{best}}  \\mathrm{BIC}_{\\text{current}}$, the removal is beneficial. Update the active set $\\mathcal{S}_{\\text{current}} \\leftarrow \\mathcal{S}_{\\text{current}} \\setminus \\{j^*\\}$ and update $\\mathrm{BIC}_{\\text{current}} \\leftarrow \\mathrm{BIC}_{\\text{best}}$. Repeat the iteration.\n    d. If $\\mathrm{BIC}_{\\text{best}} \\ge \\mathrm{BIC}_{\\text{current}}$, no single feature removal improves the model. The algorithm terminates.\n3.  **Output**: The final set of selected features is $\\mathcal{S}_{\\text{current}}$. If this set is empty, the final model is the intercept-only model.\n\n**3. Data Leakage Detection**\n\nData leakage occurs when information from outside the modeling process is improperly included in the feature set, often leading to unrealistically high predictive performance. We implement a pre-processing routine to flag and remove suspicious features based on two simple, powerful heuristics. Let $x_j$ be the vector for the $j$-th feature, $y$ be the response vector, and $t = [0, 1, \\dots, n-1]^\\top$ be the observation time index vector. The correlation is the Pearson correlation coefficient.\n\n-   **Rule A (Direct Label-Copy Detection)**: A feature that is a near-perfect copy of the target variable is a classic form of leakage. It is flagged if its absolute correlation with the response is above a high threshold $\\tau_{\\text{direct}}$.\n    $$ |\\text{corr}(x_j, y)|  \\tau_{\\text{direct}} = 0.995 $$\n-   **Rule B (Time-Proxy Detection)**: In time-series or ordered data, a feature might not directly copy the label but could be a proxy for time, which itself might have a strong trend that also drives the response. Such a feature is flagged if it is highly correlated with both the time index and the response.\n    $$ |\\text{corr}(x_j, t)|  \\tau_{\\text{time}} = 0.98 \\quad \\text{and} \\quad |\\text{corr}(x_j, y)|  \\tau_{\\text{time},y} = 0.7 $$\n\n**4. Experimental Procedure**\n\nFor each of the four specified datasets, we perform the following steps:\n1.  **Data Generation**: Synthesize the dataset $(X, y)$ according to its specification, including the number of true predictors ($q$), noise predictors ($r$), regression coefficients ($\\beta$), and the type of leakage for the designated leakage feature $x_{p-1}$. The random number generator is seeded for reproducibility.\n2.  **Naive Selection**: Apply the backward stepwise selection algorithm to the full feature matrix $X \\in \\mathbb{R}^{n \\times p}$ and the response vector $y$. The resulting set of selected feature indices is recorded.\n3.  **Leakage-Aware Selection**:\n    a. Apply the leakage detection routine to $(X, y)$ to identify a set of flagged feature indices, $\\mathcal{F}_{\\text{flagged}}$. We record whether the designated leakage feature (index $p-1$) is in this set.\n    b. Create a cleaned feature matrix $X_{\\text{clean}}$ by removing the columns corresponding to $\\mathcal{F}_{\\text{flagged}}$ from $X$.\n    c. Apply backward stepwise selection to $(X_{\\text{clean}}, y)$.\n    d. The indices selected from $X_{\\text{clean}}$ are mapped back to their original indices in $X$. This final set is recorded.\n4.  **Output**: For each dataset, a triplet is assembled, containing the sorted list of indices from naive selection, the sorted list of indices from leakage-aware selection, and a boolean indicating if the designated leakage feature was flagged.\n\nThis comparative analysis is designed to highlight scenarios where the naive approach erroneously retains a leakage variable due to its strong (but spurious) predictive power, while the leakage-aware pre-processing step correctly identifies and removes it, leading to a more robust and generalizable model.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the entire simulation as specified.\n    \"\"\"\n\n    def calculate_bic(X_subset, y):\n        \"\"\"\n        Calculates the Bayesian Information Criterion (BIC) for a linear model.\n        \n        Args:\n            X_subset: Design matrix for the model. Can be None for intercept-only.\n            y: Response vector.\n            \n        Returns:\n            BIC value.\n        \"\"\"\n        n = len(y)\n        if X_subset is None or X_subset.shape[1] == 0:\n            k = 1  # Intercept only\n            rss = np.sum((y - np.mean(y))**2)\n        else:\n            num_features = X_subset.shape[1]\n            k = num_features + 1  # a coefficient for each feature + intercept\n            X_aug = np.c_[np.ones(n), X_subset]\n            try:\n                coeffs, _, _, _ = np.linalg.lstsq(X_aug, y, rcond=None)\n                rss = np.sum((y - (X_aug @ coeffs))**2)\n            except np.linalg.LinAlgError:\n                return np.inf  # Should not happen with this problem's data generation\n\n        # To prevent log(0) for perfect fits\n        if rss  1e-9:\n            rss = 1e-9\n            \n        bic = n * np.log(rss / n) + k * np.log(n)\n        return bic\n\n    def backward_selection(X, y):\n        \"\"\"\n        Performs backward stepwise selection using BIC.\n        \n        Args:\n            X: Full design matrix of candidate predictors.\n            y: Response vector.\n            \n        Returns:\n            A sorted list of indices of the selected features.\n        \"\"\"\n        p = X.shape[1]\n        current_indices = list(range(p))\n        if not current_indices:\n            return []\n        \n        current_bic = calculate_bic(X, y)\n\n        while len(current_indices) > 0:\n            bics_on_removal = []\n            for idx_to_remove in current_indices:\n                trial_indices = [i for i in current_indices if i != idx_to_remove]\n                if not trial_indices:\n                    X_trial = None\n                else:\n                    X_trial = X[:, trial_indices]\n                \n                bic = calculate_bic(X_trial, y)\n                bics_on_removal.append((bic, idx_to_remove))\n            \n            if not bics_on_removal:\n                break\n\n            best_bic, removed_idx = min(bics_on_removal)\n\n            if best_bic  current_bic:\n                current_bic = best_bic\n                current_indices.remove(removed_idx)\n            else:\n                break # No further improvement\n        \n        return sorted(current_indices)\n\n    def detect_leakage(X, y):\n        \"\"\"\n        Detects suspicious features based on correlation rules.\n        \n        Args:\n            X: Design matrix.\n            y: Response vector.\n            \n        Returns:\n            A set of indices of flagged features.\n        \"\"\"\n        n, p = X.shape\n        flagged_indices = set()\n        \n        t = np.arange(n)\n        \n        tau_direct = 0.995\n        tau_time = 0.98\n        tau_time_y = 0.7\n\n        for j in range(p):\n            x_j = X[:, j]\n            if np.std(x_j) == 0: continue\n            \n            # Rule A: Direct label-copy detection\n            corr_xy = np.corrcoef(x_j, y)[0, 1]\n            if abs(corr_xy) > tau_direct:\n                flagged_indices.add(j)\n\n            # Rule B: Time-proxy detection\n            if np.std(t) == 0: continue\n            corr_xt = np.corrcoef(x_j, t)[0, 1]\n            if abs(corr_xt) > tau_time and abs(corr_xy) > tau_time_y:\n                 flagged_indices.add(j)\n        \n        return flagged_indices\n\n    def generate_dataset(n, q, r, beta, sigma, seed, leak_type, gamma=None, sigma_leak=None):\n        \"\"\"\n        Generates a synthetic dataset based on specified parameters.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        X_true = rng.standard_normal((n, q))\n        X_noise = rng.standard_normal((n, r))\n        \n        epsilon = rng.normal(0, sigma, n)\n        \n        if leak_type == 'time_proxy':\n            t = np.arange(n)\n            t_tilde = (t - np.mean(t)) / np.std(t)\n            y = X_true @ beta + gamma * t_tilde + epsilon\n            x_leak = t_tilde.reshape(-1, 1)\n        else:\n            y = X_true @ beta + epsilon\n            if leak_type == 'direct_leakage':\n                eta = rng.normal(0, sigma_leak, n)\n                x_leak = (y + eta).reshape(-1, 1)\n            else: # 'random'\n                x_leak = rng.standard_normal((n, 1))\n        \n        X_parts = []\n        if q > 0: X_parts.append(X_true)\n        if r > 0: X_parts.append(X_noise)\n        X_parts.append(x_leak)\n\n        X = np.hstack(X_parts)\n        return X, y\n\n    test_cases = [\n        {'n': 120, 'q': 2, 'r': 3, 'beta': [3.0, -2.0], 'sigma': 1.5, 'seed': 42, 'leak_type': 'direct_leakage', 'sigma_leak': 0.02},\n        {'n': 100, 'q': 3, 'r': 2, 'beta': [1.2, -0.8, 1.5], 'sigma': 2.0, 'seed': 7, 'leak_type': 'random'},\n        {'n': 25, 'q': 2, 'r': 6, 'beta': [1.0, 0.5], 'sigma': 1.0, 'seed': 123, 'leak_type': 'random'},\n        {'n': 150, 'q': 1, 'r': 2, 'beta': [1.0], 'sigma': 0.5, 'seed': 99, 'leak_type': 'time_proxy', 'gamma': 2.0},\n    ]\n\n    results = []\n    for params in test_cases:\n        X, y = generate_dataset(**params)\n        p = X.shape[1]\n        leakage_feature_idx = p - 1\n\n        # 1. Naive selection\n        naive_selected = backward_selection(X, y)\n        \n        # 2. Leakage-aware selection\n        flagged_indices = detect_leakage(X, y)\n        leak_feature_is_flagged = leakage_feature_idx in flagged_indices\n\n        clean_indices_map = [i for i in range(p) if i not in flagged_indices]\n        if not clean_indices_map:\n            aware_selected = []\n        else:\n            X_clean = X[:, clean_indices_map]\n            selected_clean_indices = backward_selection(X_clean, y)\n            aware_selected = [clean_indices_map[i] for i in selected_clean_indices]\n\n        results.append((naive_selected, aware_selected, leak_feature_is_flagged))\n\n    # Format the final output string without spaces\n    result_strings = []\n    for naive_list, aware_list, flag in results:\n        naive_str = f\"[{','.join(map(str, naive_list))}]\"\n        aware_str = f\"[{','.join(map(str, aware_list))}]\"\n        flag_str = 'true' if flag else 'false'\n        result_strings.append(f\"[{naive_str},{aware_str},{flag_str}]\")\n    \n    print(f\"[[{','.join(result_strings)}]]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building effective regression models often involves more than just selecting from a simple list of predictors. When interaction terms are included, sound statistical practice calls for respecting the hierarchy principle. This advanced exercise challenges you to adapt the backward elimination algorithm to enforce this crucial structural constraint.\n\nYou will implement a version of backward selection where the removal of main effects is conditional on the absence of their corresponding interactions. By using the partial $F$-test to guide these constrained decisions, you will learn to integrate logical principles into the selection algorithm, a key skill for developing interpretable and statistically robust models .",
            "id": "3101387",
            "problem": "You are tasked with implementing a statistically principled backward stepwise selection procedure for a linear regression model with interaction terms while enforcing the strong hierarchical principle. The model includes an intercept, three main effects, and two interactions. The procedure must use a valid statistical test derived from ordinary least squares (OLS) assumptions to decide which term to remove at each step.\n\nBase assumptions and definitions:\n- The data follow the linear model $y = X \\beta + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I_n)$, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^p$ is the coefficient vector, and $I_n$ is the $n \\times n$ identity matrix.\n- The ordinary least squares (OLS) estimator is the minimizer of the residual sum of squares and, under the Gaussian error model, yields test statistics with known distributions.\n- The strong hierarchical principle: if an interaction term is included, then its corresponding main effects must also be included. Equivalently, a main effect cannot be removed if any included interaction involves it, and an interaction can be considered for removal only when both of its main effects are currently included in the model.\n\nModel structure:\n- Predictors are $X_1, X_2, X_3$ and interactions are $X_1 X_2$ and $X_2 X_3$.\n- The design matrix columns are ordered as follows: index $0$ is the intercept, index $1$ is $X_1$, index $2$ is $X_2$, index $3$ is $X_3$, index $4$ is $X_1 X_2$, and index $5$ is $X_2 X_3$.\n- The intercept (index $0$) must always remain in the model and is never a candidate for removal.\n\nBackward stepwise rule to implement:\n- Start from the full model containing the intercept, all three main effects, and both interactions.\n- At each iteration, form the set of candidates that are eligible for removal under strong hierarchy:\n  - An interaction term (indices $4$ or $5$) is eligible only if its two corresponding main effects are included.\n  - A main effect (indices $1$, $2$, $3$) is eligible only if no interaction currently included involves it.\n- For every eligible single-term removal, compare the current model (full) to the reduced model that removes just that single term, via a valid nested-model F-test derived from first principles (using residual sums of squares).\n- Compute a two-sided decision via the F-statistic and its $p$-value. At each iteration remove the single eligible term with the largest $p$-value provided that this $p$-value is strictly greater than the threshold $\\alpha = 0.01$. If no eligible term has a $p$-value exceeding $\\alpha$, stop.\n\nTest statistic requirement:\n- Your derivation, implementation, and decision rule must start only from the linear model and Gaussian error assumptions. You must derive and use the correct nested-model F-test that compares the change in residual sum of squares to the estimated error variance, taking into account the correct degrees of freedom.\n\nSimulation and test suite:\n- For each test case, generate predictors independently as $X_1, X_2, X_3 \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,1)$ and construct the interactions $X_1 X_2$ and $X_2 X_3$ deterministically from the simulated main effects.\n- Generate responses as $y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_{12} (X_1 X_2) + \\beta_{23} (X_2 X_3) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma$ specified below.\n- Use independent random seeds for each test case to ensure deterministic results.\n- Use $n = 400$ and $\\sigma = 0.5$ for all test cases.\n\nProvide and use the following test suite with seeds and true coefficients:\n- Test case T1 (happy path): seed $= 123$, $(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_{12},\\beta_{23}) = (0, 1.0, 1.0, 0.0, 1.2, 0.0)$.\n- Test case T2 (boundary on interaction removal): seed $= 456$, $(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_{12},\\beta_{23}) = (0, 1.0, 0.8, 0.5, 0.0, 0.0)$.\n- Test case T3 (edge case with one interaction): seed $= 789$, $(\\beta_0,\\beta_1,\\beta_2,\\beta_3,\\beta_{12},\\beta_{23}) = (0, 0.0, 0.8, 0.8, 0.0, 1.2)$.\n\nRequired program behavior:\n- Implement the backward stepwise elimination with the strong hierarchical constraints described above and the nested-model F-test at significance level $\\alpha = 0.01$.\n- For each test case, output the final set of included predictors excluding the intercept as a list of integer column indices from $\\{1,2,3,4,5\\}$ in ascending order. For example, if the final model includes $X_1$, $X_2$, and $X_1 X_2$, print $[1,2,4]$ for that test case.\n- The final program output must aggregate the three test results into a single line that is a list of the three lists, in the same order as T1, T2, T3. For example, a valid overall output string would look like: [[1,2,4],[1,2,3],[2,3,5]].\n\nAngle units and physical units do not apply to this problem. All numeric outputs are unitless. Ensure your program produces exactly one line containing the specified aggregate list string and nothing else.",
            "solution": "The problem requires the implementation of a backward stepwise selection algorithm for a linear regression model, governed by the strong hierarchical principle and using a nested-model F-test for term removal.\n\n**1. Theoretical Framework: The Nested-Model F-Test**\n\nThe foundation of the selection procedure is the F-test for comparing two nested linear models. A reduced model (Model $0$) is said to be nested within a full model (Model $1$) if the predictors of Model $0$ are a subset of the predictors of Model $1$.\n\nThe general linear model is given by:\n$$y = X\\beta + \\varepsilon$$\nwhere $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix of $p$ predictors (including an intercept), $\\beta \\in \\mathbb{R}^p$ is the vector of coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is the vector of errors. We assume the errors are independent and identically distributed, following a normal distribution, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$.\n\nThe ordinary least squares (OLS) estimate of $\\beta$ is found by minimizing the Residual Sum of Squares (RSS):\n$$\\hat{\\beta} = \\arg\\min_{\\beta} ||y - X\\beta||^2 = (X^T X)^{-1} X^T y$$\nThe minimized RSS for a given model with design matrix $X$ is $RSS = ||y - X\\hat{\\beta}||^2$.\n\nTo compare a reduced model (Model $0$) with $p_0$ parameters and a full model (Model $1$) with $p_1$ parameters ($p_1  p_0$), we formulate a null hypothesis $H_0$ that the additional $p_1 - p_0$ coefficients in the full model are all zero. The test statistic is:\n$$F = \\frac{(RSS_0 - RSS_1) / (p_1 - p_0)}{RSS_1 / (n - p_1)}$$\nwhere $RSS_0$ and $RSS_1$ are the residual sums of squares for the reduced and full models, respectively. The term $MSE_1 = RSS_1 / (n - p_1)$ is the mean squared error of the full model, which is an unbiased estimator of the error variance $\\sigma^2$ if the full model is correct.\n\nUnder the null hypothesis $H_0$, this $F$ statistic follows an $F$-distribution with $(p_1 - p_0)$ and $(n - p_1)$ degrees of freedom. A large value of $F$ suggests that the additional predictors in the full model significantly reduce the RSS, leading to a rejection of $H_0$. The decision is made by comparing the test's $p$-value to a pre-defined significance level $\\alpha$.\n\nIn our specific backward selection context, we test the removal of a single term at each step. Thus, $p_1 - p_0 = 1$. The \"full model\" is the current model in the selection process, and the \"reduced model\" is the model with one candidate term removed. The F-statistic for testing the removal of term $j$ is:\n$$F_j = \\frac{RSS_{\\text{reduced}, j} - RSS_{\\text{current}}}{MSE_{\\text{current}}}$$\nwhere $p_{\\text{current}}$ is the number of parameters in the current model. The corresponding $p$-value is calculated from the $F_{1, n - p_{\\text{current}}}$ distribution.\n\n**2. Algorithm: Backward Stepwise Selection with Strong Hierarchy**\n\nThe algorithm starts with the full model and iteratively removes the least significant predictor that is eligible for removal, until no more predictors can be justifiably removed.\n\n**Model Structure:**\nThe full model includes an intercept, three main effects ($X_1, X_2, X_3$), and two interaction terms ($X_1 X_2, X_2 X_3$). The columns of the design matrix are indexed as follows: $0$: intercept, $1$: $X_1$, $2$: $X_2$, $3$: $X_3$, $4$: $X_1 X_2$, $5$: $X_2 X_3$. The intercept term (index $0$) is never removed.\n\n**Strong Hierarchical Principle:**\nThis principle imposes constraints on which terms are eligible for removal:\n1.  A main effect can be removed only if no interaction term currently in the model involves it.\n    -   $X_1$ (index $1$) is eligible for removal only if $X_1 X_2$ (index $4$) is not in the model.\n    -   $X_2$ (index $2$) is eligible for removal only if both $X_1 X_2$ (index $4$) and $X_2 X_3$ (index $5$) are not in the model.\n    -   $X_3$ (index $3$) is eligible for removal only if $X_2 X_3$ (index $5$) is not in the model.\n2.  An interaction term is always eligible for removal. (The stated precondition that its main effects must be included is always satisfied in a backward selection process starting from a hierarchical model).\n\n**Step-by-Step Procedure:**\n1.  **Initialization**: Start with the full set of predictor indices, $S = \\{1, 2, 3, 4, 5\\}$.\n2.  **Iteration**: Repeat the following until a stopping condition is met:\n    a.  **Identify Candidates**: Determine the subset of predictors $C \\subseteq S$ that are eligible for removal according to the strong hierarchical principle.\n    b.  **Test Candidates**: For each predictor index $j \\in C$:\n        i.   Define the current model with predictor indices $S$ (and an intercept). Let its parameter count be $p_{\\text{current}} = |S| + 1$.\n        ii.  Fit this model using OLS and compute its residual sum of squares, $RSS_{\\text{current}}$, and mean squared error, $MSE_{\\text{current}} = RSS_{\\text{current}} / (n - p_{\\text{current}})$.\n        iii. Define the reduced model with predictor indices $S \\setminus \\{j\\}$.\n        iv.  Fit the reduced model and compute its RSS, $RSS_{\\text{reduced}, j}$.\n        v.   Calculate the F-statistic: $F_j = (RSS_{\\text{reduced}, j} - RSS_{\\text{current}}) / MSE_{\\text{current}}$.\n        vi.  Calculate the corresponding $p$-value, $p_j$, from the $F_{1, n-p_{\\text{current}}}$ distribution.\n    c.  **Select for Removal**: Find the predictor $j^*$ with the maximum $p$-value: $j^* = \\arg\\max_{j \\in C} p_j$.\n    d.  **Decision**:\n        i.   If $p_{j^*}  \\alpha$ (where $\\alpha=0.01$), remove the predictor from the model: $S \\leftarrow S \\setminus \\{j^*\\}$. Continue to the next iteration.\n        ii.  If $p_{j^*} \\le \\alpha$, no predictor can be removed. The algorithm terminates.\n3.  **Final Model**: The final set of predictor indices is the set $S$ upon termination. The procedure is applied to each test case using the specified simulation parameters ($n=400, \\sigma=0.5$), seeds, and true coefficients.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import f\n\ndef fit_ols_get_rss(X_data, y_data, indices):\n    \"\"\"\n    Fits an OLS model for the given data using the specified predictor indices.\n    \n    Args:\n        X_data (np.ndarray): The full design matrix (n, 6).\n        y_data (np.ndarray): The response vector (n,).\n        indices (list): List of predictor indices (from {1, ..., 5}) to include.\n                        The intercept (index 0) is always included.\n    \n    Returns:\n        tuple: A tuple containing:\n            - rss (float): The residual sum of squares.\n            - p (int): The number of parameters in the model (including intercept).\n    \"\"\"\n    # Always include the intercept (column 0)\n    cols_to_use = [0] + indices\n    X_model = X_data[:, cols_to_use]\n    \n    # number of parameters in the model\n    p = X_model.shape[1]\n    \n    # Use np.linalg.lstsq for robust OLS fitting. It returns RSS as the second element.\n    _, rss, _, _ = np.linalg.lstsq(X_model, y_data, rcond=None)\n    \n    # np.linalg.lstsq returns a 0-dim array for rss if solution is unique.\n    # It might return an empty array if underdetermined, handle this.\n    if isinstance(rss, np.ndarray) and rss.size == 0:\n        # This case implies an underdetermined system where residuals are zero.\n        # Should not happen with n > p.\n        rss = 0.0\n    else:\n        # Ensure rss is a float\n        rss = float(rss)\n\n    return rss, p\n\n\ndef perform_backward_selection(seed, beta_true, n, sigma, alpha):\n    \"\"\"\n    Performs backward stepwise selection with strong hierarchy constraint.\n\n    Args:\n        seed (int): Random seed for data generation.\n        beta_true (tuple): True coefficients (b0, b1, b2, b3, b12, b23).\n        n (int): Number of samples.\n        sigma (float): Standard deviation of the noise.\n        alpha (float): Significance level for removal.\n\n    Returns:\n        list: Sorted list of indices of predictors in the final model.\n    \"\"\"\n    np.random.seed(seed)\n    \n    # 1. Generate Data\n    X1 = np.random.normal(0, 1, n)\n    X2 = np.random.normal(0, 1, n)\n    X3 = np.random.normal(0, 1, n)\n    \n    # Create interactions\n    X12 = X1 * X2\n    X23 = X2 * X3\n    \n    # Full design matrix X with intercept\n    X_full = np.column_stack([np.ones(n), X1, X2, X3, X12, X23])\n    \n    # Generate response y\n    epsilon = np.random.normal(0, sigma, n)\n    y = X_full @ np.array(beta_true) + epsilon\n\n    # 2. Backward Selection Algorithm\n    # Predictor indices {1:X1, 2:X2, 3:X3, 4:X1X2, 5:X2X3}\n    active_indices = list(range(1, 6))\n\n    while True:\n        # Fit current model to get RSS_current and MSE_current\n        rss_current, p_current = fit_ols_get_rss(X_full, y, active_indices)\n        df_error = n - p_current\n        if df_error = 0: # Avoid division by zero\n            break\n        mse_current = rss_current / df_error\n\n        # Determine eligible predictors for removal based on strong hierarchy\n        eligible_to_remove = []\n        for j in active_indices:\n            if j == 1:  # X1\n                if 4 not in active_indices:\n                    eligible_to_remove.append(j)\n            elif j == 2:  # X2\n                if 4 not in active_indices and 5 not in active_indices:\n                    eligible_to_remove.append(j)\n            elif j == 3:  # X3\n                if 5 not in active_indices:\n                    eligible_to_remove.append(j)\n            elif j in [4, 5]: # Interactions X1X2, X2X3\n                eligible_to_remove.append(j)\n\n        if not eligible_to_remove:\n            break\n\n        # Calculate p-values for all eligible predictors\n        p_values = {}\n        for j in eligible_to_remove:\n            reduced_indices = [idx for idx in active_indices if idx != j]\n            rss_reduced, _ = fit_ols_get_rss(X_full, y, reduced_indices)\n            \n            F_stat = (rss_reduced - rss_current) / mse_current\n            # p_1 - p_0 = 1 degree of freedom for numerator\n            p_val = f.sf(F_stat, 1, df_error)\n            p_values[j] = p_val\n        \n        # Find term with largest p-value\n        if not p_values:\n            break\n            \n        term_to_remove = max(p_values, key=p_values.get)\n        max_p_value = p_values[term_to_remove]\n        \n        # Decide whether to remove the term\n        if max_p_value > alpha:\n            active_indices.remove(term_to_remove)\n        else:\n            # Stop if no term can be removed\n            break\n            \n    return sorted(active_indices)\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the final output.\n    \"\"\"\n    # Parameters for all test cases\n    n = 400\n    sigma = 0.5\n    alpha = 0.01\n\n    # Test cases: (seed, beta_true_vector)\n    test_cases = [\n        (123, (0.0, 1.0, 1.0, 0.0, 1.2, 0.0)),   # T1\n        (456, (0.0, 1.0, 0.8, 0.5, 0.0, 0.0)),   # T2\n        (789, (0.0, 0.0, 0.8, 0.8, 0.0, 1.2)),   # T3\n    ]\n\n    results = []\n    for seed, beta_true in test_cases:\n        final_indices = perform_backward_selection(seed, beta_true, n, sigma, alpha)\n        results.append(final_indices)\n\n    # Format the output as a string representing a list of lists.\n    # e.g., \"[[1, 2, 4],[1, 2, 3],[2, 3, 5]]\"\n    output_str = \"[\" + \",\".join(str(res).replace(\" \", \"\") for res in results) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}