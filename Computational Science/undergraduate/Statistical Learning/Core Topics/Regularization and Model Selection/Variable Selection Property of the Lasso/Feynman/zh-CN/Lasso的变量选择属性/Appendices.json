{
    "hands_on_practices": [
        {
            "introduction": "为了深入理解 LASSO 的变量选择特性，最好从一个理想化的场景入手。本练习将引导您推导在正交设计下 LASSO 的数学原理，揭示其核心机制本质上是对普通最小二乘系数进行软阈值操作。通过这个基础练习，您将清晰地看到正则化参数 $\\lambda$ 是如何直接控制变量的取舍。",
            "id": "3191263",
            "problem": "考虑一个线性模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$。假设 $X$ 的列是标准正交的，即 $X^\\top X = I_p$。最小绝对收缩和选择算子 (LASSO) 估计量 $\\hat{\\beta}(\\lambda) \\in \\mathbb{R}^{p}$ 定义为以下优化问题的解\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1} \\right\\},\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数，$\\| \\cdot \\|_{1}$ 和 $\\| \\cdot \\|_{2}$ 分别表示 $\\ell_{1}$范数和 $\\ell_{2}$范数。\n\n仅从上述优化定义、标准正交条件 $X^\\top X = I_p$ 以及标准的一阶最优性条件（包括针对 $\\ell_{1}$范数的次梯度微积分）出发，推导 $\\hat{\\beta}(\\lambda)$ 关于 $z := X^\\top y$ 和 $\\lambda$ 的逐坐标刻画。利用此刻画，直接用 $z_{j}$ 和 $\\lambda$ 表示坐标分量 $\\hat{\\beta}_{j}(\\lambda)$ 非零的充分必要条件。\n\n然后，考虑一个 $p=6$ 的特定标准正交设计，其 $z = X^\\top y$ 由下式给出\n$$\nz = \\begin{pmatrix} 2.7  -1.3  0.9  3.4  -0.2  -2.1 \\end{pmatrix}.\n$$\n对于四个正则化值 $\\lambda \\in \\{0.5, 1.0, 2.0, 3.0\\}$，确定相应的选择集 $\\{ j : \\hat{\\beta}_{j}(\\lambda) \\neq 0 \\}$。\n\n最后，作为你报告的计算结果，请提供一个行向量，其元素是按 $\\lambda = 0.5$, $\\lambda = 1.0$, $\\lambda = 2.0$, $\\lambda = 3.0$ 的顺序排列的这些选择集的基数。无需四舍五入。",
            "solution": "该问题是有效的，因为它具有科学依据、问题明确且客观。这是一个统计学习理论中的标准问题，涉及在标准正交设计矩阵这一简化假设下LASSO估计量的性质。我现在开始解答。\n\nLASSO估计量 $\\hat{\\beta}(\\lambda)$ 是以下优化问题的解：\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1} \\right\\}\n$$\n目标函数，我们称之为 $L(\\beta)$，由一个可微的二次项和一个不可微但凸的 $\\ell_1$范数惩罚项组成。我们可以利用给定的标准正交条件 $X^\\top X = I_p$ 来简化二次项，其中 $I_p$ 是 $p \\times p$ 的单位矩阵。\n我们来展开平方 $\\ell_2$范数：\n$$\n\\| y - X \\beta \\|_{2}^{2} = (y - X \\beta)^\\top (y - X \\beta) = y^\\top y - 2 y^\\top X \\beta + \\beta^\\top X^\\top X \\beta\n$$\n代入 $X^\\top X = I_p$，表达式变为：\n$$\n\\| y - X \\beta \\|_{2}^{2} = y^\\top y - 2 (X^\\top y)^\\top \\beta + \\beta^\\top I_p \\beta = \\|y\\|_2^2 - 2 z^\\top \\beta + \\|\\beta\\|_2^2\n$$\n其中我们使用了定义 $z := X^\\top y$。\n\n现在目标函数 $L(\\beta)$ 可以写成：\n$$\nL(\\beta) = \\frac{1}{2} \\left( \\|y\\|_2^2 - 2 z^\\top \\beta + \\|\\beta\\|_2^2 \\right) + \\lambda \\| \\beta \\|_{1}\n$$\n为了找到最小值，我们可以忽略常数项 $\\frac{1}{2}\\|y\\|_2^2$，这使得优化问题等价于最小化：\n$$\n\\tilde{L}(\\beta) = \\frac{1}{2} \\|\\beta\\|_2^2 - z^\\top \\beta + \\lambda \\| \\beta \\|_{1}\n$$\n这个目标函数是按坐标可分的。我们可以将其写成关于分量 $j=1, \\dots, p$ 的和：\n$$\n\\tilde{L}(\\beta) = \\sum_{j=1}^{p} \\left( \\frac{1}{2} \\beta_j^2 - z_j \\beta_j + \\lambda |\\beta_j| \\right)\n$$\n为了最小化 $\\tilde{L}(\\beta)$，我们可以独立地对求和中的每一项关于其对应的 $\\beta_j$ 进行最小化。令 $f_j(\\beta_j) = \\frac{1}{2} \\beta_j^2 - z_j \\beta_j + \\lambda |\\beta_j|$。\n由于 $|\\beta_j|$ 在 $\\beta_j=0$ 处不可微，我们使用次梯度微积分。一阶最优性条件表明，在最小值点 $\\hat{\\beta}_j$ 处，$0$ 必须属于 $f_j(\\beta_j)$ 的次梯度。\n$f_j(\\beta_j)$ 的次梯度由下式给出：\n$$\n\\partial f_j(\\beta_j) = \\beta_j - z_j + \\lambda \\cdot \\partial(|\\beta_j|)\n$$\n绝对值函数 $|\\beta_j|$ 的次梯度为：\n$$\n\\partial(|\\beta_j|) = \\begin{cases} \\{1\\} & \\text{if } \\beta_j > 0 \\\\ \\{-1\\} & \\text{if } \\beta_j < 0 \\\\ [-1, 1] & \\text{if } \\beta_j = 0 \\end{cases}\n$$\n最优性条件 $0 \\in \\partial f_j(\\hat{\\beta}_j)$ 转化为 $z_j - \\hat{\\beta}_j \\in \\lambda \\cdot \\partial(|\\hat{\\beta}_j|)$。我们针对最优值 $\\hat{\\beta}_j$ 分三种情况来分析这个条件：\n\n情况1：$\\hat{\\beta}_j > 0$。\n次梯度 $\\partial(|\\hat{\\beta}_j|)$ 是 $\\{1\\}$。条件变为 $z_j - \\hat{\\beta}_j = \\lambda$，这得到 $\\hat{\\beta}_j = z_j - \\lambda$。为使该解与假设 $\\hat{\\beta}_j > 0$ 一致，我们必须有 $z_j - \\lambda > 0$，即 $z_j > \\lambda$。\n\n情况2：$\\hat{\\beta}_j < 0$。\n次梯度 $\\partial(|\\hat{\\beta}_j|)$ 是 $\\{-1\\}$。条件变为 $z_j - \\hat{\\beta}_j = -\\lambda$，这得到 $\\hat{\\beta}_j = z_j + \\lambda$。为使该解与假设 $\\hat{\\beta}_j < 0$ 一致，我们必须有 $z_j + \\lambda < 0$，即 $z_j < -\\lambda$。\n\n情况3：$\\hat{\\beta}_j = 0$。\n次梯度 $\\partial(|\\hat{\\beta}_j|)$ 是区间 $[-1, 1]$。条件变为 $z_j - 0 \\in \\lambda \\cdot [-1, 1]$，即 $z_j \\in [-\\lambda, \\lambda]$，或 $|z_j| \\le \\lambda$。\n\n综合这三种情况，我们得到 LASSO 解 $\\hat{\\beta}(\\lambda)$ 的逐坐标刻画：\n$$\n\\hat{\\beta}_j(\\lambda) = \\begin{cases} z_j - \\lambda & \\text{if } z_j > \\lambda \\\\ 0 & \\text{if } |z_j| \\le \\lambda \\\\ z_j + \\lambda & \\text{if } z_j < -\\lambda \\end{cases}\n$$\n这就是软阈值算子，可以紧凑地写为 $\\hat{\\beta}_j(\\lambda) = \\text{sign}(z_j) (|z_j| - \\lambda)_+$, 其中 $(x)_+ = \\max(x, 0)$。\n\n根据这个刻画，我们可以陈述坐标分量 $\\hat{\\beta}_j(\\lambda)$ 非零的充分必要条件。系数 $\\hat{\\beta}_j(\\lambda)$ 非零当且仅当我们处于情况1 ($z_j > \\lambda$) 或情况2 ($z_j < -\\lambda$)。这两个条件可以合并为一个单一条件：\n$$\n|z_j| > \\lambda\n$$\n这就是 $\\hat{\\beta}_j(\\lambda) \\neq 0$ 的充分必要条件。\n\n现在，我们将此应用于具体问题。向量 $z = X^\\top y$ 的分量为：\n$z_1 = 2.7$, $z_2 = -1.3$, $z_3 = 0.9$, $z_4 = 3.4$, $z_5 = -0.2$, $z_6 = -2.1$。\n我们来计算这些分量的绝对值：\n$|z_1| = 2.7$, $|z_2| = 1.3$, $|z_3| = 0.9$, $|z_4| = 3.4$, $|z_5| = 0.2$, $|z_6| = 2.1$。\n\n我们需要为 $\\lambda \\in \\{0.5, 1.0, 2.0, 3.0\\}$ 确定选择集 $S_\\lambda = \\{ j : \\hat{\\beta}_{j}(\\lambda) \\neq 0 \\} = \\{ j : |z_j| > \\lambda \\}$。\n\n对于 $\\lambda = 0.5$：\n我们检查哪些 $|z_j| > 0.5$。\n$|z_1| = 2.7 > 0.5$\n$|z_2| = 1.3 > 0.5$\n$|z_3| = 0.9 > 0.5$\n$|z_4| = 3.4 > 0.5$\n$|z_5| = 0.2 \\ngtr 0.5$\n$|z_6| = 2.1 > 0.5$\n选择集是 $S_{0.5} = \\{1, 2, 3, 4, 6\\}$。其基数为 $|S_{0.5}| = 5$。\n\n对于 $\\lambda = 1.0$：\n我们检查哪些 $|z_j| > 1.0$。\n$|z_1| = 2.7 > 1.0$\n$|z_2| = 1.3 > 1.0$\n$|z_3| = 0.9 \\ngtr 1.0$\n$|z_4| = 3.4 > 1.0$\n$|z_5| = 0.2 \\ngtr 1.0$\n$|z_6| = 2.1 > 1.0$\n选择集是 $S_{1.0} = \\{1, 2, 4, 6\\}$。其基数为 $|S_{1.0}| = 4$。\n\n对于 $\\lambda = 2.0$：\n我们检查哪些 $|z_j| > 2.0$。\n$|z_1| = 2.7 > 2.0$\n$|z_2| = 1.3 \\ngtr 2.0$\n$|z_3| = 0.9 \\ngtr 2.0$\n$|z_4| = 3.4 > 2.0$\n$|z_5| = 0.2 \\ngtr 2.0$\n$|z_6| = 2.1 > 2.0$\n选择集是 $S_{2.0} = \\{1, 4, 6\\}$。其基数为 $|S_{2.0}| = 3$。\n\n对于 $\\lambda = 3.0$：\n我们检查哪些 $|z_j| > 3.0$。\n$|z_1| = 2.7 \\ngtr 3.0$\n$|z_2| = 1.3 \\ngtr 3.0$\n$|z_3| = 0.9 \\ngtr 3.0$\n$|z_4| = 3.4 > 3.0$\n$|z_5| = 0.2 \\ngtr 3.0$\n$|z_6| = 2.1 \\ngtr 3.0$\n选择集是 $S_{3.0} = \\{4\\}$。其基数为 $|S_{3.0}| = 1$。\n\n问题要求提供这些基数组成的行向量，并按给定的 $\\lambda$ 值顺序排列。基数序列为 $5$, $4$, $3$, $1$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 5 & 4 & 3 & 1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "理论虽然简洁，但 LASSO 的实际应用却伴随着重要的注意事项，其中最关键的是它对预测变量尺度的敏感性。通过这个动手编码练习，您将探究未经标准化的特征如何导致误导性的变量选择，并理解为何数据预处理是保证模型公平选择变量的关键一步。",
            "id": "3191255",
            "problem": "考虑线性模型 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$，$X \\in \\mathbb{R}^{n \\times p}$，以及 $\\beta \\in \\mathbb{R}^p$。最小绝对收缩和选择算子 (lasso) 估计量 $\\hat{\\beta}$ 定义为以下目标函数的最小化器：\n$$\n\\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1,\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数，$\\lVert \\cdot \\rVert_1$ 表示绝对值之和。lasso 的“变量选择”属性指的是其解 $\\hat{\\beta}$ 的某些分量可以恰好为零的现象，从而选择预测变量的一个子集。\n\n任务：研究预测变量的缩放对 lasso 变量选择的影响。具体来说，比较当预测变量未标准化时与预测变量标准化（中心化至均值为零并缩放至单位经验标准差）时，lasso 所选择的变量。\n\n从上述定义出发，实现一个通用算法，通过优化 lasso 目标函数来计算给定设计矩阵 $X$、响应向量 $y$ 和正则化参数 $\\lambda$ 的 $\\hat{\\beta}$。使用坐标级迭代方法，不依赖于任何闭式解的“捷径”。\n\n对于下面的所有测试用例，使用 $p=3$ 个预测变量和 $n=90$ 个样本。按如下方式构建 $X$：将 $n$ 个样本划分为三个大小相等（均为 30）的不相交块。对于预测变量 $j \\in \\{1,2,3\\}$，通过从均值为 $0$、标准差为 $s_j$ 的正态分布中独立抽样来填充其在块 $j$ 中的值，并将其在块 $j$ 之外的值设为 $0$。这将产生三个相互正交且由 $\\{s_j\\}$ 决定其不同经验方差的预测变量。为了保证可复现性，使用固定种子为 $0$ 的伪随机数生成器。对于每个测试用例，设置 $\\varepsilon = 0$ 和 $y = X \\beta$。\n\n对于每个测试用例：\n- 使用原始的 $X$（未标准化）计算 $\\hat{\\beta}_{\\text{unstd}}$。\n- 使用按列标准化（均值为 $0$，单位经验标准差）的 $X$ 计算 $\\hat{\\beta}_{\\text{std}}$。\n- 将选定变量集合定义为满足 $|\\hat{\\beta}_j| > \\tau$（其中容差 $\\tau = 10^{-8}$）的索引 $j \\in \\{1,2,3\\}$ 的列表。\n- 使用基于 1 的索引（即预测变量标记为 $1,2,3$）报告两个选定变量集合。\n- 同时报告一个布尔值，指示这两个集合是否不同。\n\n使用以下四个测试用例，每个用例 $n=90$：\n1. 方差均衡（理想情况）：$s = [1.0, 1.0, 1.0]$, $\\beta = [0.4, 0.2, 0.06]$, $\\lambda = 0.05$。\n2. 大尺度噪声效应（未标准化情况可能选择小系数、大方差的预测变量）：$s = [0.5, 3.0, 1.0]$, $\\beta = [0.2, 0.05, 0.15]$, $\\lambda = 0.10$。\n3. 重要的小尺度预测变量（未标准化情况可能丢弃一个重要的小尺度预测变量，而标准化则保留它）：$s = [0.2, 1.0, 1.0]$, $\\beta = [0.6, 0.0, 0.0]$, $\\lambda = 0.05$。\n4. 边界条件（非常大的正则化使所有系数变为零）：$s = [2.0, 0.5, 1.5]$, $\\beta = [1.0, 0.3, 0.7]$, $\\lambda = 10.0$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果必须是一个列表，包含：未标准化设计的选定变量列表、标准化设计的选定变量列表，以及一个指示集合是否不同的布尔值。例如，一个有效的输出格式为\n$$\n\\text{[ [ [1,2], [1,2], False ], [ [2], [], True ], \\dots ]}\n$$\n其中内部列表包含整数，布尔值为 $\\text{True}$ 或 $\\text{False}$。",
            "solution": "我们从 lasso 的目标函数开始\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\}.\n$$\n这是一个凸优化问题，计算 $\\hat{\\beta}$ 的一种有原则的方法是使用由次梯度微积分指导的坐标级优化。核心观察是，平方误差项是可微的，而绝对值惩罚项在 $0$ 处不可微，但具有明确定义的次微分。Karush–Kuhn–Tucker (KKT) 条件提供了必要的最优性准则。对于坐标级更新，我们在更新坐标 $j$ 时考虑部分残差：\n$$\nr_{(j)} = y - \\sum_{k \\ne j} X_k \\beta_k,\n$$\n其中 $X_k$ 是 $X$ 的第 $k$ 列。在保持其他坐标固定的情况下，对 $\\beta_j$ 优化目标函数，问题简化为最小化一个关于 $\\beta_j$ 的一维凸函数。第 $j$ 个坐标的次梯度条件是\n$$\n\\frac{1}{n} X_j^\\top \\left( y - \\sum_{k=1}^p X_k \\beta_k \\right) + \\lambda \\cdot g_j = 0,\n$$\n其中 $g_j \\in \\partial |\\beta_j|$，当 $\\beta_j \\ne 0$ 时，$g_j = \\operatorname{sign}(\\beta_j)$；当 $\\beta_j = 0$ 时，$g_j \\in [-1,1]$。使用部分残差 $r_{(j)}$ 重新整理可得：\n$$\n\\frac{1}{n} X_j^\\top r_{(j)} - \\frac{1}{n} X_j^\\top X_j \\beta_j + \\lambda \\cdot g_j = 0.\n$$\n定义标量\n$$\np_j := \\frac{1}{n} X_j^\\top r_{(j)}, \\quad z_j := \\frac{1}{n} X_j^\\top X_j.\n$$\n坐标级最优性条件简化为求解\n$$\n\\min_{\\beta_j \\in \\mathbb{R}} \\left\\{ \\frac{1}{2} z_j \\beta_j^2 - p_j \\beta_j + \\lambda |\\beta_j| \\right\\}.\n$$\n通过配方法并应用次梯度微积分，解由软阈值算子给出：\n$$\n\\beta_j \\leftarrow \\frac{1}{z_j} \\cdot \\operatorname{Soft}(p_j, \\lambda),\n$$\n其中 $\\operatorname{Soft}(a,t) = \\operatorname{sign}(a)\\cdot \\max(|a| - t, 0)$。这个更新在 $j=1,\\dots,p$ 上重复迭代，直到收敛（例如，直到最大坐标变化量小于一个小的容差）。该算法是循环坐标下降的一种形式，对于这个凸目标函数保证收敛。\n\n我们现在分析缩放的影响。假设我们将预测变量 $j$ 缩放一个因子 $\\alpha_j > 0$，即 $\\tilde{X}_j = \\alpha_j X_j$。那么\n$$\n\\tilde{p}_j = \\frac{1}{n} \\tilde{X}_j^\\top r_{(j)} = \\alpha_j \\cdot \\frac{1}{n} X_j^\\top r_{(j)} = \\alpha_j p_j,\n$$\n并且\n$$\n\\tilde{z}_j = \\frac{1}{n} \\tilde{X}_j^\\top \\tilde{X}_j = \\alpha_j^2 \\cdot \\frac{1}{n} X_j^\\top X_j = \\alpha_j^2 z_j.\n$$\n坐标更新变为\n$$\n\\tilde{\\beta}_j \\leftarrow \\frac{1}{\\alpha_j^2 z_j} \\cdot \\operatorname{Soft}(\\alpha_j p_j, \\lambda).\n$$\n变量 $j$ 的选择（即 $\\tilde{\\beta}_j$ 是否为 $0$）取决于 $|\\alpha_j p_j|$ 是否超过 $\\lambda$；显然，缩放改变了有效阈值，因为 $p_j$ 随 $\\alpha_j$ 线性缩放。当预测变量被标准化为单位经验标准差时，缩放因子被吸收到标准化设计中，使得 $p_j$ 的跨变量比较更加均衡。\n\n在我们的构造中，预测变量在三个块上具有不相交的支撑集，使它们相互正交，即当 $j \\ne k$ 时 $X_j^\\top X_k = 0$。考虑无噪声情况，$y = X \\beta$。那么\n$$\np_j = \\frac{1}{n} X_j^\\top r_{(j)} = \\frac{1}{n} X_j^\\top y = \\frac{1}{n} X_j^\\top X_j \\beta_j = z_j \\beta_j.\n$$\n因此，选择准则 $|\\operatorname{Soft}(p_j,\\lambda)| > 0$ 简化为 $|p_j| > \\lambda$，即\n$$\n|z_j \\beta_j| > \\lambda.\n$$\n对于未标准化的预测变量，$z_j$ 反映了它们的经验尺度；具有较大 $z_j$ 的变量在相同的 $\\lambda$ 下更有可能被选中，即使 $|\\beta_j|$ 很小。在标准化为单位经验方差（使得 $z_j \\approx 1$）之后，选择条件近似变为 $|\\beta_j| > \\lambda$，从而使不同预测变量的阈值均等化，并可能改变所选的集合。\n\n算法设计：\n- 使用指定的块正态构造和固定种子生成 $X$，以确保可复现性。\n- 构造 $y = X \\beta$，其中 $\\varepsilon = 0$，以分离缩放效应。\n- 使用从次梯度/Karush–Kuhn–Tucker 分析中导出的软阈值更新来实现循环坐标下降。\n- 通过减去列均值并除以列经验标准差（对零标准差进行保护）来标准化 $X$。\n- 分别在 $X$ 和标准化后的 $X$ 上运行 lasso 求解器，然后将 $|\\hat{\\beta}_j| > \\tau$（其中 $\\tau = 10^{-8}$）的变量确定为选定变量。\n- 报告两种选择的基于 1 的索引，以及一个指示集合是否不同的布尔值。\n\n测试套件的覆盖范围：\n- 方差均衡：两种选择一致，因为所有 $j$ 的 $z_j$ 都相等。\n- 大尺度噪声效应：一个系数小但 $z_j$ 大的预测变量在未标准化的情况下可能被选中，但在标准化后不会。\n- 重要的小尺度预测变量：当 $z_j$ 相对于 $\\lambda$ 较小时，未标准化的方法可能会丢弃一个真正重要的预测变量，而标准化则会保留它。\n- 边界条件：一个非常大的 $\\lambda$ 在两种情况下都会产生空模型。\n\n最终程序遵循这些原则，并打印单行输出，其中包含每个测试用例的未标准化选定索引、标准化选定索引以及表示差异的布尔值。",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(a, t):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    if a > t:\n        return a - t\n    elif a  -t:\n        return a + t\n    else:\n        return 0.0\n\ndef lasso_coordinate_descent(X, y, lam, max_iter=10000, tol=1e-12):\n    \"\"\"\n    Solve the lasso problem:\n      (1/(2n)) ||y - X b||_2^2 + lam ||b||_1\n    using cyclic coordinate descent with soft-thresholding.\n    \"\"\"\n    n, p = X.shape\n    b = np.zeros(p, dtype=float)\n    # Residual r = y - X b; with b=0 initially, r=y\n    r = y.copy()\n    for _ in range(max_iter):\n        b_old = b.copy()\n        for j in range(p):\n            xj = X[:, j]\n            # Add back the old contribution of coordinate j to residual\n            r += xj * b[j]\n            # Compute p_j and z_j\n            p_j = np.dot(xj, r) / n\n            z_j = np.dot(xj, xj) / n\n            # Update via soft-threshold\n            b[j] = soft_threshold(p_j, lam) / (z_j if z_j > 0 else 1.0)\n            # Subtract the new contribution\n            r -= xj * b[j]\n        # Convergence check\n        if np.max(np.abs(b - b_old))  tol:\n            break\n    return b\n\ndef standardize_columns(X):\n    \"\"\"\n    Center columns to mean zero and scale to unit empirical std (ddof=0).\n    Returns standardized X, along with means and stds.\n    \"\"\"\n    means = X.mean(axis=0)\n    Xc = X - means\n    stds = Xc.std(axis=0, ddof=0)\n    stds_safe = np.where(stds == 0, 1.0, stds)\n    Xstd = Xc / stds_safe\n    return Xstd, means, stds_safe\n\ndef make_block_X(n, s):\n    \"\"\"\n    Construct X with 3 predictors, each active on a disjoint block of length n//3,\n    with values ~ N(0, s_j) in its block and zeros elsewhere.\n    \"\"\"\n    assert n % 3 == 0\n    m = n // 3\n    rng = np.random.default_rng(0)\n    X = np.zeros((n, 3), dtype=float)\n    for j in range(3):\n        start = j * m\n        end = (j + 1) * m\n        X[start:end, j] = rng.normal(0.0, s[j], size=m)\n    return X\n\ndef selected_indices(b, tol=1e-8):\n    # 1-based indexing for output\n    return [i + 1 for i, bi in enumerate(b) if abs(bi) > tol]\n\ndef run_case(n, s, beta_true, lam):\n    X = make_block_X(n, s)\n    y = X @ np.array(beta_true, dtype=float)\n    # Unstandardized lasso\n    b_unstd = lasso_coordinate_descent(X, y, lam)\n    sel_unstd = selected_indices(b_unstd)\n    # Standardized lasso\n    X_std, _, _ = standardize_columns(X)\n    b_std = lasso_coordinate_descent(X_std, y, lam)\n    sel_std = selected_indices(b_std)\n    differ = (sel_unstd != sel_std)\n    return [sel_unstd, sel_std, differ]\n\ndef solve():\n    test_cases = [\n        # 1. Balanced variances\n        {\"n\": 90, \"s\": [1.0, 1.0, 1.0], \"beta\": [0.4, 0.2, 0.06], \"lam\": 0.05},\n        # 2. Large scale noise effect: unstandardized selects a large-scale small-coefficient predictor\n        {\"n\": 90, \"s\": [0.5, 3.0, 1.0], \"beta\": [0.2, 0.05, 0.15], \"lam\": 0.10},\n        # 3. Important small-scale predictor: unstandardized drops it, standardized retains it\n        {\"n\": 90, \"s\": [0.2, 1.0, 1.0], \"beta\": [0.6, 0.0, 0.0], \"lam\": 0.05},\n        # 4. Boundary: very large lambda\n        {\"n\": 90, \"s\": [2.0, 0.5, 1.5], \"beta\": [1.0, 0.3, 0.7], \"lam\": 10.0},\n    ]\n    results = []\n    for case in test_cases:\n        res = run_case(case[\"n\"], case[\"s\"], case[\"beta\"], case[\"lam\"])\n        results.append(res)\n    \n    # Format the output string without spaces as requested\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "真实世界的数据常常包含高度相关的预测变量组，在这种情况下，LASSO 的选择可能不稳定，往往会任意地从一组中只选择一个变量。本练习将探讨这一现象，并将其与弹性网络（Elastic Net）进行对比，展示了增加 $\\ell_2$ 惩罚项如何通过鼓励“分组效应”来提供更稳健的解决方案。",
            "id": "3191214",
            "problem": "考虑一个具有分组且高度相关的预测变量的线性回归模型。令 $n$ 表示观测值的数量，$p$ 表示预测变量的数量。假设数据由模型 $y = X \\beta + \\varepsilon$ 生成，其中 $X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^{p}$，且噪声 $\\varepsilon \\in \\mathbb{R}^{n}$ 是独立同分布 (IID) 的，满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。预测变量被组织成两个大小相等的不相交组，每组大小为 3，使得每组内的预测变量具有共同的两两相关系数 $\\rho \\in [0,1)$。\n\n您的任务是编写一个完整的程序，该程序：\n- 通过求解优化问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\|\\beta\\|_1,\n$$\n实现由 $\\ell_1$ 范数惩罚的最小二乘回归，即最小绝对值收缩和选择算子 (LASSO)，并通过求解\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\|\\beta\\|_1 + \\frac{\\lambda_2}{2} \\|\\beta\\|_2^2.\n$$\n实现弹性网络 (EN)，它增加了一个二次惩罚项。\n- 使用一种对凸目标有保证收敛的有原则的迭代算法，例如带软阈值法的坐标下降法，并对标准化的特征进行操作。该算法不应依赖任何外部机器学习库。\n- 使用科学上合理的相​​关结构构造合成预测变量矩阵 $X$，该矩阵包含两组各 3 个预测变量。具体来说，对于每组，令 $z \\in \\mathbb{R}^n$ 是一个潜因子，满足 $z \\sim \\mathcal{N}(0, I_n)$，对于组内的每个预测变量 $j$，令 $\\epsilon_j \\sim \\mathcal{N}(0, I_n)$ 独立于 $z$。通过以下方式定义每个预测变量：\n$$\nx_j = \\sqrt{\\rho} \\, z + \\sqrt{1-\\rho} \\, \\epsilon_j,\n$$\n然后通过减去样本均值并进行缩放来对每一列进行标准化，以使 $(1/n) \\sum_{i=1}^n x_{ij}^2 = 1$ 精确成立。拼接这两个组以形成 $X \\in \\mathbb{R}^{n \\times 6}$。\n- 通过 $y = X \\beta + \\varepsilon$ 生成响应，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n- 对于每种拟合方法，将被选择的变量定义为满足 $|\\hat{\\beta}_j|  \\tau$ 的索引 $j$，其中 $\\tau = 10^{-3}$ 是一个固定阈值。\n- 按组报告被选择变量的数量（组内满足 $|\\hat{\\beta}_j|  \\tau$ 的 $j$ 的计数），并使用这些计数来评估以下性质：\n    1. LASSO 在每个高度相关的组中只选择一个代表变量的情况。\n    2. LASSO 完全忽略整个弱信号组的情况。\n    3. 与弹性网络进行对比，由于二次惩罚项，弹性网络倾向于在组内选择多个相关的预测变量。\n\n使用以下具有固定种子的测试套件以确保可复现性。对于每个测试用例，使用指定的参数构建数据并拟合 LASSO 和弹性网络。在所有情况下，以给定的数值表示 $n$、$\\rho$、$\\sigma$ 和惩罚水平。\n\n测试套件：\n- 测试用例 $1$ (理想情况：一个强信号组和一个弱信号组，两者都高度相关):\n    - $n = 800$\n    - 组 $1$ 相关性 $\\rho_1 = 0.9$，组 $2$ 相关性 $\\rho_2 = 0.9$\n    - 真实系数：组 $1$ 的 $\\beta = (1, 1, 1)$，组 $2$ 的 $\\beta = (0.2, 0.2, 0.2)$\n    - 噪声标准差 $\\sigma = 0.1$\n    - LASSO 惩罚 $\\lambda = 0.25$\n    - 弹性网络惩罚 $\\lambda_1 = 0.15$，$\\lambda_2 = 0.1$\n    - 随机种子 $123$\n    - 此案例需输出的布尔值评估：\n        - $b_{1,1}$: LASSO 在组 1 中恰好选择一个变量。\n        - $b_{1,2}$: LASSO 在组 2 中选择零个变量。\n        - $b_{1,3}$: 弹性网络在组 1 中至少选择两个变量。\n        - $b_{1,4}$: 弹性网络在组 2 中至少选择一个变量。\n- 测试用例 $2$ (边缘情况：一个弱信号相关组和一个空信号组):\n    - $n = 800$\n    - 组 $1$ 相关性 $\\rho_1 = 0.95$，组 $2$ 相关性 $\\rho_2 = 0.95$\n    - 真实系数：组 $1$ 的 $\\beta = (0, 0, 0)$，组 $2$ 的 $\\beta = (0.3, 0.3, 0.3)$\n    - 噪声标准差 $\\sigma = 0.1$\n    - LASSO 惩罚 $\\lambda = 0.30$\n    - 弹性网络惩罚 $\\lambda_1 = 0.18$, $\\lambda_2 = 0.12$\n    - 随机种子 $456$\n    - 此案例需输出的布尔值评估：\n        - $b_{2,1}$: LASSO 在组 2 中选择零个变量。\n        - $b_{2,2}$: 弹性网络在组 2 中至少选择一个变量。\n- 测试用例 $3$ (边界情况：不相关的预测变量):\n    - $n = 800$\n    - 组 $1$ 相关性 $\\rho_1 = 0.0$，组 $2$ 相关性 $\\rho_2 = 0.0$\n    - 真实系数：组 $1$ 的 $\\beta = (1, 1, 1)$，组 $2$ 的 $\\beta = (0.2, 0.2, 0.2)$\n    - 噪声标准差 $\\sigma = 0.1$\n    - LASSO 惩罚 $\\lambda = 0.25$\n    - 弹性网络惩罚 $\\lambda_1 = 0.15$, $\\lambda_2 = 0.1$\n    - 随机种子 $789$\n    - 此案例需输出的布尔值评估：\n        - $b_{3,1}$: LASSO 和弹性网络对两个组的选择计数相同（两种方法每组的计数相等）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按顺序排列的所有布尔结果\n$$\n[b_{1,1}, b_{1,2}, b_{1,3}, b_{1,4}, b_{2,1}, b_{2,2}, b_{3,1}],\n$$\n并用方括号括起来，值之间用逗号分隔（例如，“[True,False,True,...]”）。",
            "solution": "该问题要求实现和比较两种正则化的线性回归模型——LASSO 和弹性网络，以展示它们在存在高度相关预测变量时的变量选择特性。验证过程确认了该问题在科学上是合理的、适定的，并为可复现的数值实验提供了所有必要的规范。\n\n### 1. 问题构建与数据生成\n\n我们考虑一个线性模型 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是预测变量矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是独立同分布的高斯噪声，满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。预测变量的数量为 $p=6$，分为两组，每组 3 个。\n\n预测变量矩阵 $X$ 的构造旨在使其在每个组内具有特定的相关性结构。对于一个具有指定两两相关性 $\\rho$ 的组，每个预测变量列 $x_j$ 通过潜因子模型生成：\n$$\nx_j = \\sqrt{\\rho} \\, z + \\sqrt{1-\\rho} \\, \\epsilon_j\n$$\n其中 $z \\in \\mathbb{R}^n$ 是一个共享的潜因子，满足 $z \\sim \\mathcal{N}(0, I_n)$，而 $\\epsilon_j \\in \\mathbb{R}^n$ 是独立的特异性噪声项，也服从 $\\mathcal{N}(0, I_n)$ 分布。这种构造确保了同一组内任意两个预测变量 $x_j$ 和 $x_k$ 之间的理论相关性为 $\\text{Cov}(x_j, x_k) = \\rho$。两个组是独立生成的。构造完成后，将拼接后的矩阵 $X \\in \\mathbb{R}^{n \\times 6}$ 进行标准化，使每列的样本均值为 0，样本二阶矩为 1，即 $(1/n) \\sum_{i=1}^n x_{ij}^2 = 1$。\n\n### 2. 正则化回归模型与算法\n\n该任务要求实现两个相关的模型：\n\n**LASSO (最小绝对值收缩和选择算子)** 解决以下优化问题：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\|\\beta\\|_1\n$$\n$\\ell_1$ 范数惩罚项 $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ 能诱导稀疏性，迫使一些系数恰好为零。\n\n**弹性网络 (EN)** 在 LASSO 目标函数的基础上增加了一个 $\\ell_2$ 范数惩罚项：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\|\\beta\\|_1 + \\frac{\\lambda_2}{2} \\|\\beta\\|_2^2\n$$\n二次惩罚项 $\\|\\beta\\|_2^2 = \\sum_{j=1}^p \\beta_j^2$ 鼓励一种“分组效应”，即高度相关的预测变量会倾向于被一同选择或一同丢弃。\n\n我们使用**坐标下降法**来求解这些凸优化问题。该算法在固定其他所有系数的同时，迭代地更新每个系数 $\\beta_j$。对于弹性网络的目标函数，第 $j$ 个系数的更新规则是通过最小化仅关于 $\\beta_j$ 的目标函数来导出的。这产生了一个涉及软阈值算子 $S_\\alpha(z) = \\operatorname{sgn}(z) \\max(|z|-\\alpha, 0)$ 的闭式解。$\\beta_j$ 的更新公式为：\n$$\n\\beta_j^{\\text{new}} \\leftarrow \\frac{S_{\\lambda_1}(z_j)}{1 + \\lambda_2}\n$$\n其中 $z_j = \\frac{1}{n} X_j^T (y - \\sum_{k \\neq j} X_k \\beta_k)$ 是第 $j$ 个预测变量与部分残差之间的简单协方差。LASSO 的更新是 $\\lambda_2=0$ 的特例。算法会遍历所有系数进行迭代，直到 $\\beta$ 向量的变化小于指定的容差。对于本问题，我们使用标准化的预测变量和中心化的响应向量，这是拟合无截距项正则化模型的标准做法。\n\n### 3. 选择特性的评估\n\n将实现的模型应用于三个具有固定参数和随机种子的测试用例，以确保可复现性。对于每个模型拟合，如果其估计系数 $\\hat{\\beta}_j$ 的绝对值大于一个小的阈值，即 $|\\hat{\\beta}_j|  \\tau = 10^{-3}$，则认为变量 $j$ 被“选择”。然后，我们计算两个预测变量组中各自被选择变量的数量。\n\n- **测试用例 1** 使用高度相关的预测变量（$\\rho=0.9$），其中一个为强信号组（$\\beta_j=1$），另一个为弱信号组（$\\beta_j=0.2$）。此设置旨在展示 LASSO 从强信号组中选择单个代表变量并完全忽略弱信号组，而弹性网络由于其分组效应，会从强信号组中选择多个预测变量，并从弱信号组中选择至少一个。\n- **测试用例 2** 对比了一个空信号组（$\\beta_j=0$）和一个具有极高相关性（$\\rho=0.95$）的弱信号组（$\\beta_j=0.3$）。在较高的惩罚下，预计 LASSO 会忽略这个弱信号组，而弹性网络的分组效应可能有助于其识别该组的信号。\n- **测试用例 3** 检验了预测变量不相关（$\\rho=0.0$）的情况。在这种情景下，弹性网络的分组优势不复存在。该测试评估了 LASSO 和弹性网络的选择行为是否会变得相同，这取决于具体的惩罚参数。\n\n程序会为每个用例计算指定的布尔值评估，这些评估测试了上述预期行为并提供了量化结果。最终输出将这些布尔结果汇总到一个列表中。",
            "answer": "```python\nimport numpy as np\n\ndef generate_correlated_group(n, p_group, rho, rng):\n    \"\"\"Generates a group of correlated predictors using a given RNG.\"\"\"\n    # Latent common factor\n    z = rng.standard_normal(size=(n, 1))\n    # Independent idiosyncratic noise for each predictor\n    epsilon = rng.standard_normal(size=(n, p_group))\n    \n    # Construct predictors\n    X_group = np.sqrt(rho) * z + np.sqrt(1 - rho) * epsilon\n    return X_group\n\ndef generate_data(n, p_group, rho1, rho2, beta_true, sigma, seed):\n    \"\"\"Generates the full design matrix X and response y.\"\"\"\n    # Seed sequence for reproducible and independent random number generation\n    ss = np.random.SeedSequence(seed)\n    child_seeds = ss.spawn(3)\n    rng1, rng2, rng_noise = [np.random.default_rng(s) for s in child_seeds]\n\n    X1 = generate_correlated_group(n, p_group, rho1, rng1)\n    X2 = generate_correlated_group(n, p_group, rho2, rng2)\n    \n    # Concatenate to form the full design matrix\n    X = np.hstack((X1, X2))\n    \n    # Standardize X: mean 0 and sample variance 1.\n    # This satisfies (1/n) * sum(x_ij^2) = 1 because mean is 0.\n    X_mean = X.mean(axis=0)\n    X_std = X.std(axis=0, ddof=0)\n    # Avoid division by zero if a column is constant\n    X_std[X_std == 0] = 1.0\n    X_standardized = (X - X_mean) / X_std\n    \n    # Generate noise and response y\n    epsilon = rng_noise.normal(0, sigma, size=n)\n    y = X_standardized @ beta_true + epsilon\n    \n    return X_standardized, y\n\ndef fit_elastic_net(X, y, lambda1, lambda2, max_iter=2000, tol=1e-6):\n    \"\"\"\n    Fits an Elastic Net model using coordinate descent.\n    LASSO is a special case where lambda2 = 0.\n    \"\"\"\n    n, p = X.shape\n    \n    # Center y for intercept-less model\n    y_centered = y - y.mean()\n    \n    beta = np.zeros(p)\n    \n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        \n        for j in range(p):\n            # Calculate partial residual r_j = y - sum_{k!=j} X_k beta_k\n            r_j = y_centered - (X @ beta - X[:, j] * beta[j])\n            \n            # Argument to the soft-thresholding function\n            z_j = (X[:, j] @ r_j) / n\n            \n            # Apply the soft-thresholding update for Elastic Net\n            if z_j > lambda1:\n                beta[j] = (z_j - lambda1) / (1 + lambda2)\n            elif z_j  -lambda1:\n                beta[j] = (z_j + lambda1) / (1 + lambda2)\n            else:\n                beta[j] = 0.0\n        \n        # Check for convergence\n        if np.max(np.abs(beta - beta_old))  tol:\n            break\n            \n    return beta\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 800,\n            \"p_group\": 3,\n            \"rho1\": 0.9, \"rho2\": 0.9,\n            \"beta_true\": np.array([1, 1, 1, 0.2, 0.2, 0.2]),\n            \"sigma\": 0.1,\n            \"lasso_lambda\": 0.25,\n            \"en_lambda1\": 0.15, \"en_lambda2\": 0.1,\n            \"seed\": 123\n        },\n        {\n            \"n\": 800,\n            \"p_group\": 3,\n            \"rho1\": 0.95, \"rho2\": 0.95,\n            \"beta_true\": np.array([0, 0, 0, 0.3, 0.3, 0.3]),\n            \"sigma\": 0.1,\n            \"lasso_lambda\": 0.30,\n            \"en_lambda1\": 0.18, \"en_lambda2\": 0.12,\n            \"seed\": 456\n        },\n        {\n            \"n\": 800,\n            \"p_group\": 3,\n            \"rho1\": 0.0, \"rho2\": 0.0,\n            \"beta_true\": np.array([1, 1, 1, 0.2, 0.2, 0.2]),\n            \"sigma\": 0.1,\n            \"lasso_lambda\": 0.25,\n            \"en_lambda1\": 0.15, \"en_lambda2\": 0.1,\n            \"seed\": 789\n        }\n    ]\n\n    all_results = []\n    tau = 1e-3\n    p_group = 3\n\n    # Test Case 1\n    case = test_cases[0]\n    X, y = generate_data(case[\"n\"], case[\"p_group\"], case[\"rho1\"], case[\"rho2\"],\n                         case[\"beta_true\"], case[\"sigma\"], case[\"seed\"])\n    \n    beta_lasso = fit_elastic_net(X, y, case[\"lasso_lambda\"], 0)\n    beta_en = fit_elastic_net(X, y, case[\"en_lambda1\"], case[\"en_lambda2\"])\n\n    selected_lasso = np.abs(beta_lasso) > tau\n    selected_en = np.abs(beta_en) > tau\n\n    count_lasso_g1 = np.sum(selected_lasso[:p_group])\n    count_lasso_g2 = np.sum(selected_lasso[p_group:])\n    count_en_g1 = np.sum(selected_en[:p_group])\n    count_en_g2 = np.sum(selected_en[p_group:])\n\n    b_1_1 = (count_lasso_g1 == 1)\n    b_1_2 = (count_lasso_g2 == 0)\n    b_1_3 = (count_en_g1 >= 2)\n    b_1_4 = (count_en_g2 >= 1)\n    all_results.extend([b_1_1, b_1_2, b_1_3, b_1_4])\n\n    # Test Case 2\n    case = test_cases[1]\n    X, y = generate_data(case[\"n\"], case[\"p_group\"], case[\"rho1\"], case[\"rho2\"],\n                         case[\"beta_true\"], case[\"sigma\"], case[\"seed\"])\n    \n    beta_lasso = fit_elastic_net(X, y, case[\"lasso_lambda\"], 0)\n    beta_en = fit_elastic_net(X, y, case[\"en_lambda1\"], case[\"en_lambda2\"])\n\n    selected_lasso = np.abs(beta_lasso) > tau\n    selected_en = np.abs(beta_en) > tau\n\n    count_lasso_g2 = np.sum(selected_lasso[p_group:])\n    count_en_g2 = np.sum(selected_en[p_group:])\n    \n    b_2_1 = (count_lasso_g2 == 0)\n    b_2_2 = (count_en_g2 >= 1)\n    all_results.extend([b_2_1, b_2_2])\n\n    # Test Case 3\n    case = test_cases[2]\n    X, y = generate_data(case[\"n\"], case[\"p_group\"], case[\"rho1\"], case[\"rho2\"],\n                         case[\"beta_true\"], case[\"sigma\"], case[\"seed\"])\n    \n    beta_lasso = fit_elastic_net(X, y, case[\"lasso_lambda\"], 0)\n    beta_en = fit_elastic_net(X, y, case[\"en_lambda1\"], case[\"en_lambda2\"])\n\n    selected_lasso = np.abs(beta_lasso) > tau\n    selected_en = np.abs(beta_en) > tau\n\n    count_lasso_g1 = np.sum(selected_lasso[:p_group])\n    count_lasso_g2 = np.sum(selected_lasso[p_group:])\n    count_en_g1 = np.sum(selected_en[:p_group])\n    count_en_g2 = np.sum(selected_en[p_group:])\n\n    b_3_1 = (count_lasso_g1 == count_en_g1) and (count_lasso_g2 == count_en_g2)\n    all_results.append(b_3_1)\n\n    print(f\"[{','.join(map(str, all_results))}]\".replace(\"True\",\"True\").replace(\"False\",\"False\"))\n\nsolve()\n```"
        }
    ]
}