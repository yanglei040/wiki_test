{
    "hands_on_practices": [
        {
            "introduction": "Understanding a new statistical method often begins with seeing it in its simplest form. This first exercise strips ridge regression down to its core: fitting a single coefficient. By starting with the penalized sum of squares objective function and using basic calculus, you will derive the ridge estimator from first principles, building a solid foundation for tackling more complex, multi-variable problems. ",
            "id": "1951876",
            "problem": "In a machine learning context, we are tasked with fitting a simple linear model without an intercept, $y = \\beta x$, to a set of $n$ data points $(x_i, y_i)$. To prevent overfitting on a small dataset, we employ ridge regression. The ridge estimate for the coefficient $\\beta$ is the value that minimizes the penalized sum of squared errors, also known as the objective function $L(\\beta)$:\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\nwhere $\\lambda > 0$ is the regularization parameter that controls the amount of shrinkage.\n\nYour task is two-fold. First, by minimizing the objective function $L(\\beta)$, derive the general closed-form expression for the ridge estimate $\\hat{\\beta}_{\\text{ridge}}$ in terms of the data points $(x_i, y_i)$ and the parameter $\\lambda$.\n\nSecond, apply this derived expression to a specific dataset consisting of two points: $(x_1, y_1) = (1, 3)$ and $(x_2, y_2) = (2, 5)$. Calculate the numerical value of the ridge estimate $\\hat{\\beta}_{\\text{ridge}}$ using a regularization parameter of $\\lambda = 1$.\n\nProvide the final numerical value as an exact fraction.",
            "solution": "We minimize the penalized sum of squared errors for the no-intercept linear model $y=\\beta x$ with objective\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}, \\quad \\lambda>0.\n$$\nExpand the squared term and collect like powers of $\\beta$:\n$$\nL(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}^{2}-2\\beta x_{i}y_{i}+\\beta^{2}x_{i}^{2}\\right)+\\lambda \\beta^{2}\n= \\sum_{i=1}^{n}y_{i}^{2}-2\\beta \\sum_{i=1}^{n}x_{i}y_{i}+\\beta^{2}\\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta^{2}.\n$$\nDifferentiate with respect to $\\beta$ and set the derivative to zero (first-order optimality condition):\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}y_{i}+2\\beta \\sum_{i=1}^{n}x_{i}^{2}+2\\lambda \\beta=0.\n$$\nSolve for $\\beta$:\n$$\n2\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=2\\sum_{i=1}^{n}x_{i}y_{i}\n\\quad \\Longrightarrow \\quad\n\\hat{\\beta}_{\\text{ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\nThe second derivative is\n$$\n\\frac{d^{2}L}{d\\beta^{2}}=2\\sum_{i=1}^{n}x_{i}^{2}+2\\lambda>0,\n$$\nso the solution is the unique minimizer.\n\nApply this to $(x_{1},y_{1})=(1,3)$, $(x_{2},y_{2})=(2,5)$ with $\\lambda=1$:\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 3+2\\cdot 5=13,\\qquad\n\\sum_{i=1}^{2}x_{i}^{2}=1^{2}+2^{2}=5.\n$$\nTherefore,\n$$\n\\hat{\\beta}_{\\text{ridge}}=\\frac{13}{5+1}=\\frac{13}{6}.\n$$",
            "answer": "$$\\boxed{\\frac{13}{6}}$$"
        },
        {
            "introduction": "While deriving the estimator for a single parameter is insightful, real-world datasets almost always involve multiple predictors, which are best handled using linear algebra. This practice problem transitions you from summation notation to the standard matrix equation for ridge regression, a crucial step toward practical implementation. You will work with the pre-computed summary statistics that are often used as inputs for statistical software, focusing your attention on the core ridge calculation. ",
            "id": "1951893",
            "problem": "In the field of machine learning, ridge regression is a common technique used to regularize linear regression models. This is particularly useful for preventing overfitting and handling multicollinearity among predictor variables. The ridge regression estimator for the coefficient vector, $\\hat{\\beta}_{\\lambda}$, is given by the formula:\n$$ \\hat{\\beta}_{\\lambda} = (X^T X + \\lambda I)^{-1} X^T y $$\nHere, $X$ is the design matrix, $y$ is the vector of observed outcomes, $I$ is the identity matrix of appropriate dimensions, and $\\lambda$ is a non-negative regularization parameter.\n\nSuppose that for a particular dataset with two predictor variables, the following quantities have been pre-computed:\n$$ X^T X = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} \\quad \\text{and} \\quad X^T y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} $$\nUsing a regularization parameter of $\\lambda = 5$, determine the ridge regression coefficient vector $\\hat{\\beta}_5$.",
            "solution": "The ridge regression estimator is defined by\n$$\n\\hat{\\beta}_{\\lambda} = (X^{T}X + \\lambda I)^{-1} X^{T} y.\n$$\nWith the given data,\n$$\nX^{T}X = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix}, \\quad X^{T} y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 5.\n$$\nCompute the regularized matrix:\n$$\nX^{T}X + \\lambda I = \\begin{pmatrix} 10 & 5 \\\\ 5 & 10 \\end{pmatrix} + 5 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 15 & 5 \\\\ 5 & 15 \\end{pmatrix}.\n$$\nFor a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is given by\n$$\n\\left(\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\\right)^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}.\n$$\nApplying this,\n$$\n\\det(X^{T}X + \\lambda I) = 15 \\cdot 15 - 5 \\cdot 5 = 225 - 25 = 200,\n$$\nso\n$$\n(X^{T}X + \\lambda I)^{-1} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix}.\n$$\nThen\n$$\n\\hat{\\beta}_{5} = \\frac{1}{200} \\begin{pmatrix} 15 & -5 \\\\ -5 & 15 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 15 \\cdot 3 - 5 \\cdot 1 \\\\ -5 \\cdot 3 + 15 \\cdot 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 40 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ 0 \\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{5}\\\\0\\end{pmatrix}}$$"
        },
        {
            "introduction": "The power of ridge regression lies in its tuning parameter, $\\lambda$, which controls the trade-off between bias and variance. But how do we choose the best value for $\\lambda$? This exercise walks you through the logical steps of $K$-fold cross-validation, a standard and robust method for hyperparameter tuning. Mastering this procedure is essential for applying ridge regression effectively to real data and building models that generalize well to new observations. ",
            "id": "1951879",
            "problem": "A data scientist is tasked with building a predictive model using ridge regression. Ridge regression is a type of linear regression that includes a penalty term to shrink the coefficient estimates, which is particularly useful for mitigating multicollinearity and preventing overfitting. The strength of this penalty is controlled by a non-negative tuning parameter, $\\lambda$. A crucial step in the modeling process is to select the optimal value of $\\lambda$ from a set of candidate values. A common method for this is K-fold cross-validation.\n\nThe data scientist has identified the following key actions to perform K-fold cross-validation to find the optimal $\\lambda$ and build the final model. The prediction error is measured using Mean Squared Error (MSE).\n\n(i) Choose the value of $\\lambda$ from the candidate set that yields the smallest average MSE across the folds.\n(ii) For each candidate value of $\\lambda$, calculate the average MSE by iterating through the K folds, each time training the model on K-1 folds and validating on the held-out fold.\n(iii) Randomly partition the entire dataset into K subsets, or \"folds,\" of approximately equal size.\n(iv) Train a final ridge regression model on the *entire* dataset using the optimal $\\lambda$ value selected in the previous step.\n\nWhat is the correct logical sequence of these actions?\n\nA. (iii) -> (i) -> (ii) -> (iv)\n\nB. (ii) -> (iii) -> (i) -> (iv)\n\nC. (iii) -> (ii) -> (i) -> (iv)\n\nD. (iii) -> (ii) -> (iv) -> (i)\n\nE. (ii) -> (i) -> (iv) -> (iii)",
            "solution": "Ridge regression fits coefficients $\\beta$ by minimizing the penalized least squares objective\n$$\n\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2},\n$$\nwhere $\\lambda \\geq 0$ is a tuning parameter chosen by cross-validation. To select $\\lambda$, define a candidate set $\\Lambda$ and perform K-fold cross-validation as follows.\n\nFirst, randomly partition the dataset indices $\\{1,\\dots,n\\}$ into $K$ disjoint folds $I_{1},\\dots,I_{K}$ of approximately equal size, which corresponds to action (iii). For each $\\lambda \\in \\Lambda$, iterate over folds $k=1,\\dots,K$: fit the ridge model using $\\lambda$ on the training set indexed by $I_{-k} = \\{1,\\dots,n\\} \\setminus I_{k}$ to obtain $\\hat{\\beta}^{(-k,\\lambda)}$, compute validation predictions $\\hat{y}_{i}^{(-k,\\lambda)}$ for $i \\in I_{k}$, and compute the fold MSE\n$$\n\\mathrm{MSE}_{k}(\\lambda) = \\frac{1}{|I_{k}|} \\sum_{i \\in I_{k}} \\left(y_{i} - \\hat{y}_{i}^{(-k,\\lambda)}\\right)^{2}.\n$$\nThen compute the average cross-validated error for $\\lambda$,\n$$\n\\overline{\\mathrm{MSE}}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^{K} \\mathrm{MSE}_{k}(\\lambda),\n$$\nwhich corresponds to action (ii). Select the optimal tuning parameter by\n$$\n\\lambda^{*} = \\arg\\min_{\\lambda \\in \\Lambda} \\overline{\\mathrm{MSE}}(\\lambda),\n$$\nwhich corresponds to action (i). Finally, refit the ridge regression model on the entire dataset using $\\lambda^{*}$, which corresponds to action (iv).\n\nThus, the correct logical sequence is (iii) → (ii) → (i) → (iv), i.e., option C.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}