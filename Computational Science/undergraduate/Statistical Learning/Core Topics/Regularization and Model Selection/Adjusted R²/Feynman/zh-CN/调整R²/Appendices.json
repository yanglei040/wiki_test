{
    "hands_on_practices": [
        {
            "introduction": "在深入探讨调整$R^2$的概念细微之处前，首先必须熟练掌握其计算方法。第一个练习将直接应用调整$R^2$的公式，其中使用了回归分析输出的核心要素：残差平方和（RSS）、总平方和（TSS）、观测数量（$n$）以及预测变量数量（$p$）。通过完成这个计算练习 ，你将巩固对这些元素如何组合成最终指标的理解。",
            "id": "1031765",
            "problem": "在一项有 $n=25$ 个观测值和 $p=4$ 个预测变量的多元线性回归分析中，残差平方和 (RSS) 为 15.8，总平方和 (TSS) 为 120.5。计算调整后的决定系数，记为 $\\bar{R}^2$。  \n\n调整后的 $R^2$ 考虑了预测变量的数量，其定义为：  \n$$  \n\\bar{R}^2 = 1 - \\frac{\\mathrm{RSS}/(n-p-1)}{\\mathrm{TSS}/(n-1)}  \n$$  \n请给出 $\\bar{R}^2$ 的精确值。",
            "solution": "1. 从定义开始  \n$$\n\\bar R^2 \\;=\\; 1 \\;-\\; \\frac{\\mathrm{RSS}/(n-p-1)}{\\mathrm{TSS}/(n-1)}.\n$$  \n2. 代入 $n=25$，$p=4$，$\\mathrm{RSS}=15.8$，$\\mathrm{TSS}=120.5$。则  \n$$\nn-p-1 = 25-4-1 = 20,\\qquad n-1 = 24.\n$$  \n3. 计算各项均方：  \n$$\n\\frac{\\mathrm{RSS}}{n-p-1}\n= \\frac{15.8}{20}\n= \\frac{79/5}{20}\n= \\frac{79}{100},\n\\quad\n\\frac{\\mathrm{TSS}}{n-1}\n= \\frac{120.5}{24}\n= \\frac{241/2}{24}\n= \\frac{241}{48}.\n$$  \n4. 求出它们的比值：  \n$$\n\\frac{\\mathrm{RSS}/(n-p-1)}{\\mathrm{TSS}/(n-1)}\n= \\frac{79/100}{241/48}\n= \\frac{79\\cdot48}{100\\cdot241}\n= \\frac{3792}{24100}\n= \\frac{948}{6025}.\n$$  \n5. 因此  \n$$\n\\bar R^2\n=1 - \\frac{948}{6025}\n= \\frac{6025 - 948}{6025}\n= \\frac{5077}{6025}.\n$$",
            "answer": "$$\\boxed{5077/6025}$$"
        },
        {
            "introduction": "标准的$R^2$存在一个致命缺陷：当你在模型中添加更多预测变量时，即使这些变量纯属噪声，它的值也永远不会下降。这个模拟练习  旨在揭示这一问题，并展示为何调整$R^2$是更优的模型比较指标。你将创建一个包含有意义的“信号”变量和不相关的“噪声”变量的数据集，亲眼见证调整$R^2$如何惩罚那些并未真正提升模型预测能力的复杂性。",
            "id": "3152035",
            "problem": "考虑一个带截距的多元线性回归模型，其中响应向量 $y \\in \\mathbb{R}^n$ 由 $y = X_{\\text{signal}}\\beta_{\\text{signal}} + \\varepsilon$ 生成，噪声向量 $\\varepsilon \\in \\mathbb{R}^n$ 是独立同分布的高斯噪声，满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。预测变量矩阵分解为 $X = [X_{\\text{signal}}, X_{\\text{noise}}]$，其中 $X_{\\text{signal}} \\in \\mathbb{R}^{n \\times p_{\\text{signal}}}$ 包含真实的信号预测变量，而 $X_{\\text{noise}} \\in \\mathbb{R}^{n \\times q_{\\text{noise}}}$ 包含与 $X_{\\text{signal}}$ 和 $y$ 都无关的额外噪声预测变量。\n\n从多元线性回归模型和普通最小二乘法（OLS）的核心定义出发，实现以下程序，以比较在预测变量数量 $p$ 不断增加的情况下，决定系数（R-squared）和调整后 R-squared 的变化，并通过交叉验证误差来验证这种比较：\n\n- 对于下述测试套件中的每个测试用例，根据指定的参数生成数据：$X_{\\text{signal}}$ 和 $X_{\\text{noise}}$ 的每一列独立地从标准正态分布中抽取，$\\varepsilon$ 独立地从方差为 $\\sigma^2$ 的高斯分布中抽取。使用指定的 $\\beta_{\\text{signal}}$ 构建 $y$，并在所有拟合的模型中包含一个截距项。\n- 拟合两个 OLS 模型：一个仅使用 $X_{\\text{signal}}$ 的“仅信号”模型，以及一个同时使用 $X_{\\text{signal}}$ 和 $X_{\\text{noise}}$ 的“全预测变量”模型。两个模型都必须包含截距。\n- 对于每个拟合的模型，使用它们源于总方差分解和针对截距模型的自由度调整的标准定义，计算训练集决定系数 $R^2$ 和调整后 $R^2$。\n- 使用 $K$ 折交叉验证（CV）估计样本外性能。$K$ 折交叉验证的定义是将索引 $\\{1,\\dots,n\\}$ 划分为 $K$ 个大小近似相等的子集（折），在 $K-1$ 个折上进行拟合，在留出的那个折上进行预测，并计算所有折的均方误差（MSE）的平均值。报告每个模型的平均 CV MSE。\n\n您的程序必须对每个测试用例评估以下三个布尔语句，以比较“全预测变量”模型和“仅信号”模型：\n- “全预测变量”模型的 $R^2$ 大于或等于“仅信号”模型的 $R^2$。\n- “全预测变量”模型的调整后 $R^2$ 严格小于“仅信号”模型的调整后 $R^2$。\n- “全预测变量”模型的 $K$ 折 CV MSE 严格大于“仅信号”模型的 $K$ 折 CV MSE。\n\n测试套件参数（请使用所提供的确切值和种子）：\n- 测试用例 1：$n=400$, $p_{\\text{signal}}=5$, $q_{\\text{noise}}=40$, $\\beta_{\\text{signal}}=[2.0,-1.0,1.5,0.5,-0.8]$, $\\sigma=1.0$, $K=5$, 随机种子 $42$。\n- 测试用例 2：$n=120$, $p_{\\text{signal}}=3$, $q_{\\text{noise}}=80$, $\\beta_{\\text{signal}}=[1.8,-0.7,0.9]$, $\\sigma=1.2$, $K=5$, 随机种子 $7$。\n- 测试用例 3：$n=200$, $p_{\\text{signal}}=10$, $q_{\\text{noise}}=20$, $\\beta_{\\text{signal}}=[1.3,-0.6,0.7,0.5,-1.1,0.9,-0.4,0.3,-0.2,0.8]$, $\\sigma=2.0$, $K=5$, 随机种子 $99$。\n\n在这个纯统计问题中，角度单位和物理单位不适用。您的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表，其中每个元素对应一个测试用例，并且本身是一个包含上述三个布尔值的列表。例如，输出格式必须严格符合 $[[b_{11},b_{12},b_{13}],[b_{21},b_{22},b_{23}],[b_{31},b_{32},b_{33}]]$ 的形式，其中 $b_{ij} \\in \\{\\text{True},\\text{False}\\}$。",
            "solution": "用户希望解决一个统计学问题。\n计划如下：\n1.  **验证问题陈述**：该问题是一个标准的统计模拟研究，旨在展示多元线性回归中过拟合现象以及不同模型选择指标的特性。问题在科学上是合理的，表述清晰，客观且完整。所有参数和步骤都已明确定义。结论是**有效**。\n2.  **解释原理**：解决方案将围绕普通最小二乘法（OLS）、决定系数（$R^2$）、调整后 $R^2$ 和 K 折交叉验证的核心原理展开。\n3.  **分步逻辑**：按照问题中的规定，详细说明数据生成、模型拟合、指标计算和比较的算法。\n4.  **代码实现**：将使用 `numpy` 开发一个 Python 脚本，以对给定的测试用例执行该过程。\n\n### 原理与方法论\n\n本问题旨在探究在多元线性回归模型中包含不相关预测变量的后果。我们比较了一个只包含信号预测变量的“真实”模型与一个额外包含了噪声预测变量的“过拟合”模型。该比较通过三个关键指标进行：决定系数（$R^2$）、调整后 $R^2$ 和交叉验证均方误差（CV MSE）。\n\n#### 1. 普通最小二乘法（OLS）模型\n\n多元线性回归模型旨在将响应变量 $y \\in \\mathbb{R}^n$ 描述为预测变量（包含在设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 中）的线性组合。对于一个带截距的模型，方程为：\n$$ y = \\beta_0 + X\\beta + \\varepsilon $$\n其中 $\\beta_0$ 是截距，$\\beta \\in \\mathbb{R}^p$ 是预测变量系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是误差向量。通过使用一个全为1的列来增广设计矩阵，$X_{\\text{aug}} = [\\mathbf{1}_n, X]$，以及系数向量 $\\beta_{\\text{aug}} = [\\beta_0, \\beta^T]^T$，该模型可以更紧凑地写为 $y = X_{\\text{aug}}\\beta_{\\text{aug}} + \\varepsilon$。\n\nOLS 方法通过最小化残差平方和（RSS）来找到系数的估计值 $\\hat{\\beta}_{\\text{aug}}$，RSS 定义为：\n$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = (y - X_{\\text{aug}}\\hat{\\beta}_{\\text{aug}})^T (y - X_{\\text{aug}}\\hat{\\beta}_{\\text{aug}}) $$\n这个最小化问题的解由正规方程组给出：\n$$ \\hat{\\beta}_{\\text{aug}} = (X_{\\text{aug}}^T X_{\\text{aug}})^{-1} X_{\\text{aug}}^T y $$\n此过程将用于拟合两个模型：\n1.  **仅信号模型**：使用设计矩阵 $X_{\\text{signal}} \\in \\mathbb{R}^{n \\times p_{\\text{signal}}}$。\n2.  **全预测变量模型**：使用设计矩阵 $X = [X_{\\text{signal}}, X_{\\text{noise}}] \\in \\mathbb{R}^{n \\times (p_{\\text{signal}} + q_{\\text{noise}})}$。\n\n#### 2. 决定系数（$R^2$）\n\n$R^2$ 衡量响应变量 $y$ 的方差中可由预测变量预测的部分所占的比例。它基于总平方和（TSS）的分解来定义：\n$$ \\text{TSS} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 $$\n其中 $\\bar{y}$ 是 $y$ 的均值。分解式为 $\\text{TSS} = \\text{ESS} + \\text{RSS}$，其中 ESS 是解释平方和。$R^2$ 的计算公式为：\n$$ R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} $$\n$R^2$ 的一个关键特性是，当向模型中添加新的预测变量时，它是非递减的。这是因为增加一个预测变量只会减少（或保持不变）RSS，即最小化的平方误差和。因此，我们预期“全预测变量”模型的 $R^2$ 将大于或等于“仅信号”模型的 $R^2$。这直接对应于第一个布尔检验：$R^2_{\\text{all}} \\ge R^2_{\\text{signal}}$。\n\n#### 3. 调整后 $R^2$\n\n$R^2$ 的非递减特性使其不适合用于比较具有不同数量预测变量的模型。一个模型仅通过包含更多变量（即使它们是纯噪声）就可以获得更高的 $R^2$。调整后 $R^2$ 通过对每个增加的预测变量进行惩罚来修正这一点。其定义为：\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\text{RSS} / (n - p - 1)}{\\text{TSS} / (n - 1)} $$\n这里，$p$ 是预测变量的数量（不包括截距）。$\\text{RSS} / (n - p - 1)$ 项是误差方差 $\\sigma^2$ 的一个无偏估计。当增加一个无用的预测变量时，$p$ 增加 1，这会增大惩罚项。如果 RSS 的减少不足以抵消这种惩罚，调整后 $R^2$ 就会下降。由于“全预测变量”模型增加了 $q_{\\text{noise}}$ 个纯粹的随机变量，包含它们是不合理的，因此我们预期其调整后 $R^2$ 会低于更简单的“仅信号”模型。这对应于第二个布尔检验：$R^2_{\\text{adj, all}}  R^2_{\\text{adj, signal}}$。\n\n#### 4. K 折交叉验证（CV）\n\n虽然调整后 $R^2$ 提供了更好的样本内比较，但评估模型预测能力更稳健的方法是估计其在未见数据上的性能。K 折 CV 是一种实现这一目标的标准技术。数据集被划分为 $K$ 个大小相等的折。对于每一个折 $k \\in \\{1, \\dots, K\\}$，模型在其他 $K-1$ 个折上进行训练，然后用于预测被留出的第 $k$ 折的响应。对每个折计算均方误差（MSE）：\n$$ \\text{MSE}_k = \\frac{1}{n_k} \\sum_{i \\in \\text{fold } k} (y_i - \\hat{y}_i)^2 $$\n其中 $n_k$ 是第 $k$ 折中的观测数量。总体的 CV 分数是这些 MSE 的平均值：\n$$ \\text{MSE}_{\\text{CV}} = \\frac{1}{K} \\sum_{k=1}^{K} \\text{MSE}_k $$\n通过包含噪声预测变量，“全预测变量”模型倾向于对训练数据过拟合。它学习了特定训练样本中存在的伪相关性。当这个过拟合的模型应用于未见的测试折时，其性能会很差，导致更高的 MSE。因此，我们预期“全预测变量”模型的 CV MSE 会大于“仅信号”模型的 CV MSE，这对应于第三个布尔检验：$\\text{MSE}_{\\text{CV, all}} > \\text{MSE}_{\\text{CV, signal}}$。\n\n### 实现策略\n\n对于每个测试用例，将实现以下算法：\n1.  **初始化**：设置随机种子以保证可复现性。\n2.  **数据生成**：使用指定的参数（$n$, $p_{\\text{signal}}$, $q_{\\text{noise}}$, $\\sigma$）从各自的正态分布生成 $X_{\\text{signal}}$、$X_{\\text{noise}}$ 和 $\\varepsilon$。构建响应向量 $y = X_{\\text{signal}}\\beta_{\\text{signal}} + \\varepsilon$。\n3.  **创建折**：将数据索引 $\\{0, \\dots, n-1\\}$ 划分为 $K$ 个折。此划分将用于两个模型，以确保公平比较。\n4.  **模型分析**：\n    *   对于**“仅信号”模型**（使用 $X_1 = X_{\\text{signal}}$）：\n        *   在完整数据集上拟合 OLS 模型以获得 $\\hat{\\beta}_1$。\n        *   计算 $R^2_1$ 和 $R^2_{\\text{adj}, 1}$。\n        *   执行 $K$ 折 CV 以计算 $\\text{MSE}_{\\text{CV},1}$。\n    *   对于**“全预测变量”模型**（使用 $X_2 = [X_{\\text{signal}}, X_{\\text{noise}}]$）：\n        *   在完整数据集上拟合 OLS 模型以获得 $\\hat{\\beta}_2$。\n        *   计算 $R^2_2$ 和 $R^2_{\\text{adj}, 2}$。\n        *   执行 $K$ 折 CV 以计算 $\\text{MSE}_{\\text{CV},2}$。\n5.  **评估布尔条件**：\n    *   检查 $R^2_2 \\ge R^2_1$ 是否成立。\n    *   检查 $R^2_{\\text{adj}, 2}  R^2_{\\text{adj}, 1}$ 是否成立。\n    *   检查 $\\text{MSE}_{\\text{CV}, 2} > \\text{MSE}_{\\text{CV}, 1}$ 是否成立。\n6.  **存储和报告**：存储这三个布尔结果，并按照最终输出的要求进行格式化。",
            "answer": "```python\nimport numpy as np\n\ndef ols_fit(X, y):\n    \"\"\"\n    Fits an Ordinary Least Squares model with an intercept.\n    \n    Args:\n        X (np.ndarray): Predictor matrix (n_samples, n_features).\n        y (np.ndarray): Response vector (n_samples,).\n        \n    Returns:\n        tuple: (beta_hat, X_aug) where beta_hat are the fitted coefficients\n               (including intercept) and X_aug is the augmented design matrix.\n    \"\"\"\n    n_samples = X.shape[0]\n    X_aug = np.hstack([np.ones((n_samples, 1)), X])\n    \n    try:\n        # More stable than inv(X.T @ X) @ X.T @ y\n        beta_hat = np.linalg.solve(X_aug.T @ X_aug, X_aug.T @ y)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if matrix is singular\n        beta_hat = np.linalg.pinv(X_aug.T @ X_aug) @ X_aug.T @ y\n        \n    return beta_hat, X_aug\n\ndef calculate_metrics(X_aug, y, beta_hat):\n    \"\"\"\n    Calculates R-squared and Adjusted R-squared.\n    \n    Args:\n        X_aug (np.ndarray): Augmented design matrix (with intercept).\n        y (np.ndarray): Response vector.\n        beta_hat (np.ndarray): Fitted coefficients.\n        \n    Returns:\n        tuple: (r_squared, adj_r_squared).\n    \"\"\"\n    n_samples, n_params = X_aug.shape\n    p_predictors = n_params - 1\n    \n    y_hat = X_aug @ beta_hat\n    rss = np.sum((y - y_hat)**2)\n    tss = np.sum((y - np.mean(y))**2)\n    \n    if tss == 0:\n        # Edge case: y is constant. R-squared is not well-defined.\n        # Fits will be perfect if model is just an intercept, otherwise RSS > 0.\n        return (1.0 if rss == 0 else 0.0), (1.0 if rss == 0 else 0.0)\n\n    r_squared = 1 - rss / tss\n    \n    # Degrees of freedom for error and total\n    df_err = n_samples - p_predictors - 1\n    df_tot = n_samples - 1\n    \n    if df_err = 0:\n        # If p >= n-1, adjusted R^2 can be negative or undefined.\n        # This problem's test cases avoid this.\n        adj_r_squared = r_squared\n    else:\n        adj_r_squared = 1 - (rss / df_err) / (tss / df_tot)\n        \n    return r_squared, adj_r_squared\n\ndef k_fold_cv_mse(X, y, K, folds):\n    \"\"\"\n    Calculates K-fold cross-validation Mean Squared Error.\n    \n    Args:\n        X (np.ndarray): Predictor matrix.\n        y (np.ndarray): Response vector.\n        K (int): Number of folds.\n        folds (list of np.ndarray): List of arrays, each containing indices for a fold.\n\n    Returns:\n        float: The average MSE across all K folds.\n    \"\"\"\n    mse_scores = []\n    n_samples = len(y)\n    \n    for k in range(K):\n        test_indices = folds[k]\n        all_indices = np.arange(n_samples)\n        train_indices = np.setdiff1d(all_indices, test_indices)\n        \n        X_train, y_train = X[train_indices], y[train_indices]\n        X_test, y_test = X[test_indices], y[test_indices]\n        \n        # Fit model on training data\n        beta_hat_train, _ = ols_fit(X_train, y_train)\n        \n        # Predict on test data\n        X_test_aug = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n        y_pred = X_test_aug @ beta_hat_train\n        \n        # Calculate MSE for the fold\n        mse = np.mean((y_test - y_pred)**2)\n        mse_scores.append(mse)\n        \n    return np.mean(mse_scores)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases.\n    \"\"\"\n    test_cases = [\n        {'n': 400, 'p_signal': 5, 'q_noise': 40, 'beta_signal': [2.0, -1.0, 1.5, 0.5, -0.8], 'sigma': 1.0, 'K': 5, 'seed': 42},\n        {'n': 120, 'p_signal': 3, 'q_noise': 80, 'beta_signal': [1.8, -0.7, 0.9], 'sigma': 1.2, 'K': 5, 'seed': 7},\n        {'n': 200, 'p_signal': 10, 'q_noise': 20, 'beta_signal': [1.3, -0.6, 0.7, 0.5, -1.1, 0.9, -0.4, 0.3, -0.2, 0.8], 'sigma': 2.0, 'K': 5, 'seed': 99},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n = case['n']\n        p_signal = case['p_signal']\n        q_noise = case['q_noise']\n        beta_signal = np.array(case['beta_signal'])\n        sigma = case['sigma']\n        K = case['K']\n        seed = case['seed']\n        \n        np.random.seed(seed)\n        \n        # Generate data\n        X_signal = np.random.randn(n, p_signal)\n        X_noise = np.random.randn(n, q_noise)\n        epsilon = np.random.randn(n) * sigma\n        y = X_signal @ beta_signal + epsilon\n        \n        # Define predictor sets\n        X1 = X_signal  # Signal-only\n        X2 = np.hstack([X_signal, X_noise]) # All predictors\n        \n        # --- Model 1: Signal-only ---\n        beta_hat1, X1_aug = ols_fit(X1, y)\n        r2_1, adj_r2_1 = calculate_metrics(X1_aug, y, beta_hat1)\n\n        # --- Model 2: All predictors ---\n        beta_hat2, X2_aug = ols_fit(X2, y)\n        r2_2, adj_r2_2 = calculate_metrics(X2_aug, y, beta_hat2)\n        \n        # --- K-Fold CV ---\n        # Create folds once for fair comparison\n        indices = np.arange(n)\n        np.random.shuffle(indices)\n        folds = np.array_split(indices, K)\n        \n        cv_mse_1 = k_fold_cv_mse(X1, y, K, folds)\n        cv_mse_2 = k_fold_cv_mse(X2, y, K, folds)\n\n        # --- Evaluate boolean statements ---\n        bool1 = r2_2 >= r2_1\n        bool2 = adj_r2_2  adj_r2_1\n        bool3 = cv_mse_2 > cv_mse_1\n        \n        all_results.append([bool1, bool2, bool3])\n\n    # Format the final output string\n    inner_parts = [f\"[{','.join(map(lambda b: str(b), res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "模型构建并非总是直截了当；有时，某些单独看似无用的变量在组合使用时却能变得极具预测能力。这个有趣的练习  将通过一个精心构建的数据集，探索这种被称为“抑制效应”的场景。通过分析这个难题，你将看到调整$R^2$如何可能急剧增加，从而揭示出一种强大的联合关系，而这种关系在单独考察每个预测变量时会被完全忽略。",
            "id": "3096404",
            "problem": "一位数据分析师在一次多元线性回归中怀疑存在抑制效应：两个预测变量共同作用时，调整后的决定系数得到显著提升，而单独使用任何一个预测变量似乎都没有作用。请考虑以下包含六个观测值的数据集，其中预测变量为 $x_1$ 和 $x_2$，响应变量为 $y$：\n- $x_1 = \\{-5,\\,-3,\\,-1,\\,1,\\,3,\\,5\\}$\n- $x_2 = \\{-4,\\,-2,\\,-2,\\,0,\\,4,\\,6\\}$\n- $y = \\{-1,\\,-1,\\,1,\\,1,\\,-1,\\,-1\\}$\n\n您将为三个模型拟合带截距项的普通最小二乘 (OLS) 回归：仅使用 $x_1$ 对 $y$ 进行回归，仅使用 $x_2$ 对 $y$ 进行回归，以及同时使用 $x_1$ 和 $x_2$ 对 $y$ 进行回归。使用带截距项的线性回归中平方和与自由度的标准定义，计算每个模型的调整后决定系数。然后，解释为什么两个单独作用较弱的变量可以通过抑制效应和偏相关共同提升调整后的决定系数，并参考该数据集的结构进行说明。\n\n最后，报告包含两个预测变量的联合模型的调整后决定系数。将您的答案四舍五-入到四位有效数字，并将其表示为一个不带单位的纯数字。",
            "solution": "用户希望分析一个数据集，以理解多元线性回归中的抑制效应概念。这涉及到拟合三个普通最小二乘 (OLS) 模型，并比较它们的调整后决定系数 ($R^2_{adj}$)。\n\n首先，我们为分析建立必要的定义。设 $n$ 为观测值的数量，$p$ 为模型中预测变量的数量。设 $y_i$ 为观测到的响应值，$\\bar{y}$ 为响应值的均值，$\\hat{y}_i$ 为 OLS 模型预测的响应值。\n\n总平方和 (SST) 是响应变量的总变异：\n$$SST = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n\n误差平方和 (SSE)，也称为残差平方和，是模型未能解释的变异：\n$$SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n\n标准决定系数 ($R^2$) 是因变量方差中可由自变量预测的比例：\n$$R^2 = 1 - \\frac{SSE}{SST}$$\n\n调整后决定系数 ($R^2_{adj}$) 对 $R^2$ 进行了修正，以考虑模型中预测变量的数量。它会对添加未能改善模型的预测变量进行惩罚。\n$$R^2_{adj} = 1 - \\frac{SSE / (n - p - 1)}{SST / (n - 1)} = 1 - (1 - R^2) \\frac{n-1}{n-p-1}$$\n注意，所有模型都包含一个截距项，因此模型中的参数数量为 $p+1$。\n\n给定的数据是：\n$n=6$\n$x_1 = \\{-5,\\,-3,\\,-1,\\,1,\\,3,\\,5\\}$\n$x_2 = \\{-4,\\,-2,\\,-2,\\,0,\\,4,\\,6\\}$\n$y = \\{-1,\\,-1,\\,1,\\,1,\\,-1,\\,-1\\}$\n\n首先，我们计算响应变量的均值 $\\bar{y}$ 和总平方和 $SST$。\n$$ \\bar{y} = \\frac{1}{6} \\sum_{i=1}^{6} y_i = \\frac{-1 - 1 + 1 + 1 - 1 - 1}{6} = \\frac{-2}{6} = -\\frac{1}{3} $$\n$$ SST = \\sum_{i=1}^{6} (y_i - \\bar{y})^2 = 4 \\times \\left(-1 - \\left(-\\frac{1}{3}\\right)\\right)^2 + 2 \\times \\left(1 - \\left(-\\frac{1}{3}\\right)\\right)^2 $$\n$$ SST = 4 \\times \\left(-\\frac{2}{3}\\right)^2 + 2 \\times \\left(\\frac{4}{3}\\right)^2 = 4 \\times \\frac{4}{9} + 2 \\times \\frac{16}{9} = \\frac{16}{9} + \\frac{32}{9} = \\frac{48}{9} = \\frac{16}{3} $$\n\n**模型 1: $y$ 对 $x_1$ 的简单线性回归**\n该模型有 $p=1$ 个预测变量。模型为 $y = \\beta_0 + \\beta_1 x_1 + \\epsilon$。\n$x_1$ 的均值为 $\\bar{x}_1 = \\frac{1}{6}(-5 - 3 - 1 + 1 + 3 + 5) = 0$。\n斜率 $\\hat{\\beta}_1$ 的 OLS 估计由 $\\frac{\\sum(x_{1i}-\\bar{x}_1)(y_i-\\bar{y})}{\\sum(x_{1i}-\\bar{x}_1)^2}$ 给出。\n分子：$\\sum x_{1i} y_i$ 因为 $\\bar{x}_1=0$。\n$\\sum x_{1i} y_i = (-5)(-1) + (-3)(-1) + (-1)(1) + (1)(1) + (3)(-1) + (5)(-1) = 5 + 3 - 1 + 1 - 3 - 5 = 0$。\n由于分子为 $0$，斜率 $\\hat{\\beta}_1 = 0$。$x_1$ 和 $y$ 之间的相关性为 $0$。\n截距为 $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}_1 = \\bar{y} = -1/3$。\n所有 $i$ 的预测值都为 $\\hat{y}_i = -1/3$。\n因此，$SSE_1 = \\sum(y_i - \\hat{y}_i)^2 = \\sum(y_i - \\bar{y})^2 = SST = 16/3$。\n该模型的 $R^2$ 为 $R^2_1 = 1 - \\frac{SSE_1}{SST} = 1 - 1 = 0$。\n调整后的 $R^2$ 为：\n$$ R^2_{adj,1} = 1 - (1 - R^2_1) \\frac{n-1}{n-p-1} = 1 - (1 - 0) \\frac{6-1}{6-1-1} = 1 - \\frac{5}{4} = -0.25 $$\n负的 $R^2_{adj}$ 表明该模型比在均值处的简单水平线还要差。\n\n**模型 2: $y$ 对 $x_2$ 的简单线性回归**\n该模型同样有 $p=1$ 个预测变量。模型为 $y = \\beta_0 + \\beta_1 x_2 + \\epsilon$。\n$x_2$ 的均值为 $\\bar{x}_2 = \\frac{1}{6}(-4-2-2+0+4+6) = \\frac{2}{6} = 1/3$。\n计算 $R^2_2$: $R^2_2 = (\\text{corr}(x_2, y))^2$。\n$\\sum(x_{2i}-\\bar{x}_2)(y_i-\\bar{y}) = \\sum(x_{2i}y_i) - n\\bar{x}_2\\bar{y} = (-6) - 6(1/3)(-1/3) = -6 + 2/3 = -16/3$。\n$\\sum(x_{2i}-\\bar{x}_2)^2 = \\sum x_{2i}^2 - n\\bar{x}_2^2 = (16+4+4+0+16+36) - 6(1/3)^2 = 76 - 6/9 = 76 - 2/3 = 226/3$。\n$\\sum(y_i-\\bar{y})^2 = SST = 16/3$。\n$r_{x_2,y} = \\frac{-16/3}{\\sqrt{(226/3)(16/3)}} = \\frac{-16/3}{(4/3)\\sqrt{226}} = \\frac{-4}{\\sqrt{226}}$。\n$R^2_2 = r_{x_2,y}^2 = \\frac{16}{226} = \\frac{8}{113} \\approx 0.0708$。\n这是一个非常低的 $R^2$，表明线性关系很弱。\n调整后的 $R^2$ 为：\n$$ R^2_{adj,2} = 1 - (1 - R^2_2) \\frac{n-1}{n-p-1} = 1 - \\left(1 - \\frac{8}{113}\\right) \\frac{5}{4} = 1 - \\frac{105}{113} \\frac{5}{4} = 1 - \\frac{525}{452} = -\\frac{73}{452} \\approx -0.1615 $$\n同样，这个模型没有提供有意义的预测能力。\n\n**模型 3: $y$ 对 $x_1$ 和 $x_2$ 的多元线性回归**\n该模型有 $p=2$ 个预测变量。模型为 $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$。\n我们使用矩阵代数求解系数，$\\mathbf{\\hat{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$。\n设计矩阵 $\\mathbf{X}$ 和响应向量 $\\mathbf{y}$ 为：\n$$ \\mathbf{X} = \\begin{pmatrix} 1  -5  -4 \\\\ 1  -3  -2 \\\\ 1  -1  -2 \\\\ 1  1  0 \\\\ 1  3  4 \\\\ 1  5  6 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{pmatrix} $$\n我们计算 $\\mathbf{X}^T\\mathbf{X}$ 和 $\\mathbf{X}^T\\mathbf{y}$：\n$\\sum 1 = 6$, $\\sum x_1 = 0$, $\\sum x_2 = 2$, $\\sum x_1^2 = 70$, $\\sum x_2^2 = 76$, $\\sum x_1x_2 = 70$。\n$\\sum y = -2$, $\\sum x_1y = 0$, $\\sum x_2y = -6$。\n$$ \\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 6  0  2 \\\\ 0  70  70 \\\\ 2  70  76 \\end{pmatrix}, \\quad \\mathbf{X}^T\\mathbf{y} = \\begin{pmatrix} -2 \\\\ 0 \\\\ -6 \\end{pmatrix} $$\n估计系数的向量 $\\mathbf{\\hat{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2]^T$ 可以通过求解正规方程组 $(\\mathbf{X}^T\\mathbf{X})\\mathbf{\\hat{\\beta}} = \\mathbf{X}^T\\mathbf{y}$ 得到。\n从数据中注意到，对于所有的 $i$ 都有 $y_i = x_{1i} - x_{2i}$：\n$i=1: -1 = -5 - (-4)$\n$i=2: -1 = -3 - (-2)$\n$i=3: 1 = -1 - (-2)$\n$i=4: 1 = 1 - 0$\n$i=5: -1 = 3 - 4$\n$i=6: -1 = 5 - 6$\n该关系是精确的。这意味着使用模型 $\\hat{y} = \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2$ 可以实现完美拟合，其中 $\\hat{\\beta}_0=0$，$\\hat{\\beta}_1=1$，$\\hat{\\beta}_2=-1$。我们来验证一下这是否是 OLS 解：\n$$ (\\mathbf{X}^T\\mathbf{X})\\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 6  0  2 \\\\ 0  70  70 \\\\ 2  70  76 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0(6)+1(0)-1(2) \\\\ 0(0)+1(70)-1(70) \\\\ 0(2)+1(70)-1(76) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 0 \\\\ -6 \\end{pmatrix} = \\mathbf{X}^T\\mathbf{y} $$\n解被证实：$\\hat{\\beta}_0 = 0$, $\\hat{\\beta}_1 = 1$, $\\hat{\\beta}_2 = -1$。\n拟合后的模型是 $\\hat{y} = x_1 - x_2$。由于这个关系是精确的，预测值 $\\hat{y}_i$ 与观测值 $y_i$ 完全相同。\n因此，误差平方和为 $SSE_3 = \\sum (y_i - \\hat{y}_i)^2 = 0$。\n该模型的 $R^2$ 为 $R^2_3 = 1 - \\frac{SSE_3}{SST} = 1 - \\frac{0}{16/3} = 1$。\n调整后的 $R^2$ 为：\n$$ R^2_{adj,3} = 1 - (1 - R^2_3) \\frac{n-1}{n-p-1} = 1 - (1 - 1) \\frac{6-1}{6-2-1} = 1 - 0 \\times \\frac{5}{3} = 1 $$\n\n**对抑制效应的解释**\n结果显示，当同时使用两个预测变量时，解释能力急剧增加：\n- $R^2_{adj,1} = -0.25$\n- $R^2_{adj,2} \\approx -0.16$\n- $R^2_{adj,3} = 1.00$\n\n这种现象是抑制效应的一个典型例子。抑制变量是指一个与响应变量几乎没有或完全没有相关性，但与另一个预测变量相关的预测变量。将其包含在模型中会“抑制”另一个预测变量中的不相关方差，从而增强后者的预测能力。\n\n我们来检查一下相关性：\n1.  预测变量与响应变量的相关性：我们发现 $r_{x_1,y} = 0$ 且 $r_{x_2,y} \\approx -0.266$。单独来看，$x_1$ 与 $y$ 没有线性关系，而 $x_2$ 与 $y$ 的线性关系很弱。\n2.  预测变量之间的相关性：\n    $\\sum(x_{1i}-\\bar{x}_1)(x_{2i}-\\bar{x}_2) = \\sum x_{1i}(x_{2i}-1/3) = \\sum x_1x_2 - (1/3)\\sum x_1 = 70 - 0 = 70$。\n    $\\sum(x_{1i}-\\bar{x}_1)^2 = 70$。\n    $\\sum(x_{2i}-\\bar{x}_2)^2 = 226/3$。\n    $r_{x_1,x_2} = \\frac{70}{\\sqrt{70 \\times (226/3)}} = \\sqrt{\\frac{70 \\times 3}{226}} = \\sqrt{\\frac{210}{226}} \\approx 0.964$。\n    预测变量 $x_1$ 和 $x_2$ 之间存在非常强的正相关。\n\n在这里，$x_1$ 充当了抑制变量。它与 $y$ 不相关，但与 $x_2$ 高度相关。变量 $x_2$ 是两个部分的混合体：一个对预测 $y$ 有用，另一个（噪声）则没用。因为 $x_1$ 与 $x_2$ 高度相关但与 $y$ 不相关，所以 $x_1$ 必定是 $x_2$ 内部“噪声”成分的代理变量。\n\n当两个变量都包含在回归中时，模型可以使用 $x_1$ 来剔除或“抑制”来自 $x_2$ 的噪声。$x_1$ 的回归系数（为 $\\hat{\\beta}_1=1$）调整了 $x_2$ 的贡献（系数为 $\\hat{\\beta}_2=-1$），以移除共享的不相关方差。最终的线性组合 $x_1-x_2$ 分离出了能完美预测 $y$ 的纯信号。\n\n偏相关证实了这一点。在控制了 $x_2$ 之后，$y$ 和 $x_1$ 之间的偏相关为 $r_{y,x_1|x_2} = +1$。在控制了 $x_1$ 之后，$y$ 和 $x_2$ 之间的偏相关为 $r_{y,x_2|x_1} = -1$。一旦考虑了一个预测变量，另一个就能完美地解释 $y$ 中剩余的变异性。这是在这个特意构建的数据集中观察到的抑制效应的数学特征。\n\n题目要求的最终答案是联合模型的调整后决定系数。根据计算，这个值恰好是 $1$。四舍五入到四位有效数字，即为 $1.000$。",
            "answer": "$$\n\\boxed{1.000}\n$$"
        }
    ]
}