{
    "hands_on_practices": [
        {
            "introduction": "At the heart of a Gradient Boosting Machine is the iterative process of adding weak learners to correct the errors of the previous model. This process can be viewed as a form of gradient descent in a function space. This exercise compares two different ways to determine the \"step\" to take at each iteration: a first-order method using only the gradient of the loss function, and a more sophisticated second-order method using both the gradient and the Hessian (the curvature of the loss).\n\nBy implementing and comparing these two approaches for the logistic loss function , you will gain a deep understanding of the core mechanics of boosting. This practice illuminates the fundamental trade-off between computational complexity and convergence speed, a key consideration that motivates the design of modern, highly efficient GBM libraries.",
            "id": "3125494",
            "problem": "You will implement and compare two update rules in a Gradient Boosting Machine (GBM) for binary classification under the logistic loss: a first-order gradient-only update and a second-order Newton update. Your task is to compute, for each boosting round, the relative decrease in empirical risk and to assess the computational overhead versus the gains for both update rules under a fixed, shallow base learner.\n\nFundamental setting. Consider independent and identically distributed data points $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}$ and $y_i \\in \\{0, 1\\}$. Let the model’s current score (logit) be $F_i \\in \\mathbb{R}$ and the predicted probability be $\\sigma(F_i) = \\frac{1}{1 + e^{-F_i}}$, where $\\sigma$ is the logistic sigmoid function. The empirical risk under the binary logistic loss is\n$$\nR(F) \\;=\\; \\sum_{i=1}^N \\left( - y_i \\log \\sigma(F_i) - (1-y_i)\\log(1-\\sigma(F_i)) \\right).\n$$\n\nAt each boosting round, a tree stump base learner is fit and added to the model score $F$. The stump is defined by a fixed threshold $\\tau$ on the single feature $x$:\n- Left leaf index set: $\\mathcal{L} = \\{ i : x_i \\le \\tau \\}$,\n- Right leaf index set: $\\mathcal{R} = \\{ i : x_i > \\tau \\}$,\nwith constant outputs $v_{\\mathcal{L}}$ and $v_{\\mathcal{R}}$ added to $F_i$ for $i \\in \\mathcal{L}$ and $i \\in \\mathcal{R}$, respectively. The new model scores after the round are $F_i \\leftarrow F_i + \\nu\\, v_{\\text{leaf}(i)}$, where $\\nu \\in (0,1]$ is a given shrinkage (learning rate).\n\nFixed stump threshold rule. The threshold $\\tau$ is fixed once per dataset as the median of $\\{x_i\\}_{i=1}^N$, computed as follows: if $N$ is odd, $\\tau$ is the middle element of the sorted $\\{x_i\\}$; if $N$ is even, $\\tau$ is the average of the two middle elements of the sorted $\\{x_i\\}$. The same $\\tau$ is reused for all boosting rounds.\n\nUpdate rules. Let $p_i = \\sigma(F_i)$, the gradient (first derivative) of $R$ with respect to $F_i$ is\n$$\ng_i \\;=\\; \\frac{\\partial R}{\\partial F_i} \\;=\\; p_i - y_i,\n$$\nand the Hessian diagonal (second derivative) is\n$$\nh_i \\;=\\; \\frac{\\partial^2 R}{\\partial F_i^2} \\;=\\; p_i (1 - p_i).\n$$\nYou must implement two variants for the leaf values $v_{\\mathcal{L}}$ and $v_{\\mathcal{R}}$:\n- Gradient-only GBM: fit the negative gradient by least squares on each leaf, so that\n$$\nv_{\\mathcal{S}} \\;=\\; - \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} g_i, \\quad \\mathcal{S} \\in \\{\\mathcal{L}, \\mathcal{R}\\}.\n$$\n- Newton GBM: use a second-order (diagonal) Newton step per leaf, so that\n$$\nv_{\\mathcal{S}} \\;=\\; - \\frac{\\sum_{i \\in \\mathcal{S}} g_i}{\\sum_{i \\in \\mathcal{S}} h_i}, \\quad \\mathcal{S} \\in \\{\\mathcal{L}, \\mathcal{R}\\},\n$$\nwith the convention that if the denominator is $0$, then $v_{\\mathcal{S}} = 0$.\n\nRelative decrease per round. Let $R_{\\text{prev}}$ and $R_{\\text{new}}$ denote the empirical risk before and after a boosting round, respectively. Define the relative decrease as\n$$\n\\Delta R / R \\;=\\; \\frac{R_{\\text{prev}} - R_{\\text{new}}}{R_{\\text{prev}}}.\n$$\nRecord this value for each round.\n\nComputational overhead metric. To assess overhead in a reproducible manner, count only floating-point additions/subtractions and multiplications used to produce the update in each round, according to these rules:\n- Excluded operations: evaluations of $\\exp(\\cdot)$, divisions, comparisons, indexing, and any computations used purely for metrics (such as computing $R(F)$) are not counted.\n- Per-sample derivatives:\n  - Computing $g_i = p_i - y_i$ costs $1$ addition.\n  - For Newton GBM only, computing $h_i = p_i (1 - p_i)$ costs $1$ addition (for $1 - p_i$) plus $1$ multiplication.\n- Per-leaf reductions:\n  - Summing $\\sum_{i \\in \\mathcal{S}} g_i$ over a leaf of size $|\\mathcal{S}|$ costs $|\\mathcal{S}|$ additions.\n  - For Newton GBM only, summing $\\sum_{i \\in \\mathcal{S}} h_i$ over the leaf costs $|\\mathcal{S}|$ additions.\n- Model score updates:\n  - For each updated sample $i$, updating $F_i \\leftarrow F_i + \\nu\\, v_{\\text{leaf}(i)}$ costs $1$ multiplication and $1$ addition.\n\nFor a dataset of size $N$, with fixed partition sizes $|\\mathcal{L}|$ and $|\\mathcal{R}|$, this yields per-round operation counts:\n- Gradient-only GBM: $N$ additions for $g_i$; $|\\mathcal{L}| + |\\mathcal{R}| = N$ additions to sum $g_i$; and $2N$ operations for updates; total $4N$ counted operations.\n- Newton GBM: $N$ additions for $g_i$; $2N$ operations for $h_i$ (one addition and one multiplication per sample); $N$ additions to sum $g_i$; $N$ additions to sum $h_i$; and $2N$ operations for updates; total $7N$ counted operations.\nIf a leaf is empty, its contribution to reductions and updates is zero, and its leaf value is defined as $0$.\n\nPerformance summary. After $M$ rounds, compute:\n- The list of per-round relative decreases $\\Delta R / R$ for gradient-only GBM.\n- The list of per-round relative decreases $\\Delta R / R$ for Newton GBM.\n- A scalar “gain per overhead” for each method, defined as\n$$\n\\text{gain\\_per\\_kops} \\;=\\; \\frac{R(F^{(0)}) - R(F^{(M)})}{\\text{ops\\_total}/1000},\n$$\nwhere $\\text{ops\\_total}$ is the sum of counted operations across all $M$ rounds for the method.\n\nInitial conditions. Use $F_i^{(0)} = 0$ for all $i$, so that $p_i^{(0)} = \\sigma(0) = 0.5$.\n\nTest suite. Your program must implement the above for the following parameter sets. For each case, compute the median threshold $\\tau$ as specified and reuse it for all boosting rounds.\n\n- Case $1$: $X = [-2.0, -1.0, 0.0, 1.0, 2.0]$, $y = [0, 0, 0, 1, 1]$, $M = 5$, $\\nu = 0.5$.\n- Case $2$: $X = [0.0, 0.0, 0.0, 0.0]$, $y = [0, 1, 0, 1]$, $M = 8$, $\\nu = 0.3$.\n- Case $3$: $X = [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5]$, $y = [0, 0, 0, 1, 1, 1, 1]$, $M = 12$, $\\nu = 0.1$.\n- Case $4$: $X = [-10.0, 0.0, 10.0]$, $y = [0, 1, 1]$, $M = 1$, $\\nu = 0.8$.\n\nFinal output format. Your program must produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets. For each case, output a list of four elements:\n- The list of per-round $\\Delta R/R$ for gradient-only GBM, rounded to $6$ decimals.\n- The list of per-round $\\Delta R/R$ for Newton GBM, rounded to $6$ decimals.\n- The gradient-only gain per $1000$ operations, rounded to $6$ decimals.\n- The Newton gain per $1000$ operations, rounded to $6$ decimals.\n\nThus the overall output must be a single line string of the form\n$[\\text{case1},\\text{case2},\\text{case3},\\text{case4}]$,\nwhere each $\\text{casek}$ is a list formatted as\n$[[\\delta_{1}^{\\text{grad}}, \\ldots, \\delta_{M}^{\\text{grad}}], [\\delta_{1}^{\\text{newt}}, \\ldots, \\delta_{M}^{\\text{newt}}], G_{\\text{grad}}, G_{\\text{newt}}]$,\nwith all floating-point values rounded to $6$ decimals. No other text should be printed. Angles are not used; no physical units are involved. Percentages must be expressed as decimals (not with the percent sign).",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Data**: A set of $N$ data points $\\{(x_i, y_i)\\}_{i=1}^N$ with features $x_i \\in \\mathbb{R}$ and binary labels $y_i \\in \\{0, 1\\}$.\n- **Model Score**: $F_i$, the logit for sample $i$.\n- **Prediction**: The predicted probability is given by the logistic sigmoid function, $p_i = \\sigma(F_i) = \\frac{1}{1 + e^{-F_i}}$.\n- **Loss Function**: The empirical risk is the total binary logistic loss: $R(F) = \\sum_{i=1}^N \\left( - y_i \\log \\sigma(F_i) - (1-y_i)\\log(1-\\sigma(F_i)) \\right)$.\n- **Base Learner**: A tree stump with a fixed threshold $\\tau$.\n- **Leaf Partition**: The data is partitioned into a left leaf $\\mathcal{L} = \\{ i : x_i \\le \\tau \\}$ and a right leaf $\\mathcal{R} = \\{ i : x_i > \\tau \\}$.\n- **Threshold Rule**: $\\tau$ is the median of $\\{x_i\\}_{i=1}^N$, calculated as the middle element for odd $N$ and the average of the two middle elements for even $N$. This threshold is computed once and reused for all boosting rounds.\n- **Model Update Rule**: $F_i \\leftarrow F_i + \\nu \\cdot v_{\\text{leaf}(i)}$, where $\\nu$ is the shrinkage (learning rate).\n- **Derivatives**: The gradient is $g_i = p_i - y_i$ and the Hessian diagonal is $h_i = p_i (1 - p_i)$.\n- **Leaf Value Rules**:\n    - **Gradient-only**: $v_{\\mathcal{S}} = - \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} g_i$ for a leaf $\\mathcal{S}$. If a leaf is empty, $v_{\\mathcal{S}} = 0$.\n    - **Newton**: $v_{\\mathcal{S}} = - \\frac{\\sum_{i \\in \\mathcal{S}} g_i}{\\sum_{i \\in \\mathcal{S}} h_i}$. If the denominator is $0$, $v_{\\mathcal{S}} = 0$.\n- **Performance Metrics**:\n    - **Relative Decrease per Round**: $\\Delta R / R = \\frac{R_{\\text{prev}} - R_{\\text{new}}}{R_{\\text{prev}}}$.\n    - **Computational Overhead**: An abstract count of specific floating-point operations per round: $4N$ for the gradient-only method and $7N$ for the Newton method.\n    - **Gain per Overhead**: $\\text{gain\\_per\\_kops} = \\frac{R(F^{(0)}) - R(F^{(M)})}{\\text{ops\\_total}/1000}$.\n- **Initial Condition**: The initial model scores are $F_i^{(0)} = 0$ for all $i$.\n- **Test Suite**: Four specific test cases are provided, each with data $(X, y)$, number of rounds $M$, and shrinkage $\\nu$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-founded in the principles of statistical learning and numerical optimization. Gradient boosting, the logistic loss, and Newton's method are standard, well-established concepts. The provided formulas are correct.\n- **Well-Posed**: The problem is fully specified. It provides all necessary data, initial conditions, explicit mathematical formulas for all steps, and clear definitions for the required output metrics. The evolution of the system is deterministic, leading to a unique and meaningful solution.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined computational task based on sound scientific principles. A solution will be developed.\n\n### Solution Derivations and Algorithm\nThe task requires implementing and comparing two variants of a Gradient Boosting Machine (GBM) for binary classification. The comparison is based on the rate of convergence, measured by the relative decrease in empirical risk per round, and a defined `gain per overhead` metric.\n\nFirst, we establish the components of the GBM algorithm as specified. For each test case defined by data $(X, y)$, number of rounds $M$, and shrinkage $\\nu$:\n\n1.  **Initialization**:\n    - The number of data points is $N = |X|$.\n    - The fixed stump threshold $\\tau$ is computed from $X$ according to the specified median rule. This $\\tau$ defines the left and right leaf index sets, $\\mathcal{L}$ and $\\mathcal{R}$, which remain constant throughout all boosting rounds.\n    - Two models, one for the gradient-only method (`grad`) and one for the Newton method (`newt`), are initialized with scores $F_{\\text{grad}}^{(0)} = F_{\\text{newt}}^{(0)} = \\vec{0}$.\n    - The initial predicted probabilities are $p_i^{(0)} = \\sigma(0) = 0.5$ for all $i \\in \\{1, \\dots, N\\}$.\n    - The initial empirical risk is $R(F^{(0)}) = \\sum_{i=1}^N (-\\log(0.5)) = N \\log(2)$.\n\n2.  **Boosting Iterations**: For each round $m$ from $1$ to $M$, we perform an update for both the `grad` and `newt` models. For a generic model with scores $F$:\n    a.  The current empirical risk, $R_{\\text{prev}} = R(F)$, is computed.\n    b.  The current probabilities $p_i = \\sigma(F_i)$ are calculated for all samples $i$.\n    c.  The negative gradient vector (pseudo-residuals) is computed: $g_i = p_i - y_i$.\n    d.  **Leaf Value Calculation**: The core difference between the two methods lies here.\n        - **Gradient-only**: The leaf value $v_{\\mathcal{S}}$ is the average of the negative pseudo-residuals in that leaf, effectively fitting the negative gradient by least squares (mean):\n          $$ v_{\\mathcal{S}} = - \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} g_i $$\n          If a leaf $\\mathcal{S}$ is empty ($|\\mathcal{S}|=0$), its value is $v_{\\mathcal{S}} = 0$.\n        - **Newton**: The leaf value uses second-order information. The diagonal elements of the Hessian, $h_i = p_i(1-p_i)$, are computed. The leaf value is then a second-order (Newton) step:\n          $$ v_{\\mathcal{S}} = - \\frac{\\sum_{i \\in \\mathcal{S}} g_i}{\\sum_{i \\in \\mathcal{S}} h_i} $$\n          If $\\sum_{i \\in \\mathcal{S}} h_i = 0$, then $v_{\\mathcal{S}} = 0$.\n    e.  **Model Score Update**: The scores for all samples are updated by adding the corresponding leaf value, scaled by the shrinkage parameter $\\nu$:\n        $$ F_i \\leftarrow F_i + \\nu \\cdot v_{\\text{leaf}(i)} $$\n    f.  The new empirical risk, $R_{\\text{new}} = R(F_{\\text{new}})$, is computed.\n    g.  The relative decrease in risk for the round is calculated and stored: $\\Delta R / R = (R_{\\text{prev}} - R_{\\text{new}}) / R_{\\text{prev}}$.\n\n3.  **Performance Summary**: After $M$ rounds, the final metrics are calculated for both methods.\n    - The total risk reduction is $R(F^{(0)}) - R(F^{(M)})$.\n    - The total operation count, $\\text{ops\\_total}$, is $M \\times 4N$ for the `grad` method and $M \\times 7N$ for the `newt` method.\n    - The `gain_per_kops` is computed as $\\frac{R(F^{(0)}) - R(F^{(M)})}{\\text{ops\\_total}/1000}$.\n\n4.  **Implementation**: The algorithm is implemented in Python using the `numpy` library for efficient vector and matrix operations. Separate functions are created for computing the median threshold, the sigmoid function, the logistic risk, and for running the main GBM simulation loop for a given method. A main loop iterates through the four test cases, calls the simulation logic for each, computes the final metrics, and formats the output string as specified. Numerical stability is considered by clipping large score values before exponential computation to prevent overflow, and by clipping probabilities away from exactly $0$ or $1$ to prevent `log(0)` errors.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_median_threshold(X: np.ndarray) -> float:\n    \"\"\"Computes the median of X as per the problem's rules.\"\"\"\n    if len(X) == 0:\n        return 0.0\n    X_sorted = np.sort(X)\n    N = len(X_sorted)\n    if N % 2 == 1:\n        tau = X_sorted[N // 2]\n    else:\n        tau = (X_sorted[N // 2 - 1] + X_sorted[N // 2]) / 2.0\n    return float(tau)\n\ndef sigmoid(F: np.ndarray) -> np.ndarray:\n    \"\"\"Computes the logistic sigmoid function with overflow protection.\"\"\"\n    F_clipped = np.clip(F, -500, 500)\n    return 1.0 / (1.0 + np.exp(-F_clipped))\n\ndef logistic_risk(F: np.ndarray, y: np.ndarray) -> float:\n    \"\"\"Computes the empirical risk (total logistic loss) with numerical stability.\"\"\"\n    p = sigmoid(F)\n    epsilon = 1e-12\n    p_clipped = np.clip(p, epsilon, 1.0 - epsilon)\n    risk = -np.sum(y * np.log(p_clipped) + (1 - y) * np.log(1.0 - p_clipped))\n    return float(risk)\n\ndef run_gbm_variant(\n    X: np.ndarray,\n    y: np.ndarray,\n    M: int,\n    nu: float,\n    leaf_indices_L: np.ndarray,\n    leaf_indices_R: np.ndarray,\n    method: str\n) -> tuple[list[float], np.ndarray]:\n    \"\"\"Runs the GBM simulation for one variant (grad or newton).\"\"\"\n    N = len(y)\n    F = np.zeros(N, dtype=np.float64)\n    rel_decreases = []\n\n    for _ in range(M):\n        R_prev = logistic_risk(F, y)\n\n        p = sigmoid(F)\n        g = p - y\n\n        g_L = g[leaf_indices_L]\n        g_R = g[leaf_indices_R]\n\n        if method == 'grad':\n            v_L = -np.mean(g_L) if len(g_L) > 0 else 0.0\n            v_R = -np.mean(g_R) if len(g_R) > 0 else 0.0\n        else: # newton\n            h = p * (1 - p)\n            h_L = h[leaf_indices_L]\n            h_R = h[leaf_indices_R]\n            \n            sum_g_L = np.sum(g_L)\n            sum_h_L = np.sum(h_L)\n            sum_g_R = np.sum(g_R)\n            sum_h_R = np.sum(h_R)\n\n            v_L = -sum_g_L / sum_h_L if sum_h_L > 1e-9 else 0.0\n            v_R = -sum_g_R / sum_h_R if sum_h_R > 1e-9 else 0.0\n        \n        F[leaf_indices_L] += nu * v_L\n        F[leaf_indices_R] += nu * v_R\n\n        R_new = logistic_risk(F, y)\n        \n        rel_decr = (R_prev - R_new) / R_prev if R_prev > 1e-9 else 0.0\n        rel_decreases.append(rel_decr)\n\n    return rel_decreases, F\n\ndef process_case(case_data: dict) -> str:\n    \"\"\"Processes a single test case and returns the formatted string result.\"\"\"\n    X = case_data['X']\n    y = case_data['y']\n    M = case_data['M']\n    nu = case_data['nu']\n    \n    N = len(X)\n    \n    tau = get_median_threshold(X)\n    indices = np.arange(N)\n    leaf_L_indices = indices[X <= tau]\n    leaf_R_indices = indices[X > tau]\n\n    rel_decr_grad, F_final_grad = run_gbm_variant(X, y, M, nu, leaf_L_indices, leaf_R_indices, 'grad')\n    rel_decr_newt, F_final_newt = run_gbm_variant(X, y, M, nu, leaf_L_indices, leaf_R_indices, 'newton')\n\n    R0 = logistic_risk(np.zeros(N), y)\n    R_final_grad = logistic_risk(F_final_grad, y)\n    R_final_newt = logistic_risk(F_final_newt, y)\n\n    ops_total_grad = M * 4 * N\n    ops_total_newt = M * 7 * N\n\n    gain_grad = (R0 - R_final_grad) / (ops_total_grad / 1000.0) if ops_total_grad > 0 else 0.0\n    gain_newt = (R0 - R_final_newt) / (ops_total_newt / 1000.0) if ops_total_newt > 0 else 0.0\n\n    grad_list_str = f\"[{','.join([f'{v:.6f}' for v in rel_decr_grad])}]\"\n    newt_list_str = f\"[{','.join([f'{v:.6f}' for v in rel_decr_newt])}]\"\n    \n    case_result_str = (\n        f\"[{grad_list_str},\"\n        f\"{newt_list_str},\"\n        f\"{gain_grad:.6f},\"\n        f\"{gain_newt:.6f}]\"\n    )\n    \n    return case_result_str\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {'X': np.array([-2.0, -1.0, 0.0, 1.0, 2.0]), 'y': np.array([0, 0, 0, 1, 1]), 'M': 5, 'nu': 0.5},\n        {'X': np.array([0.0, 0.0, 0.0, 0.0]), 'y': np.array([0, 1, 0, 1]), 'M': 8, 'nu': 0.3},\n        {'X': np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5]), 'y': np.array([0, 0, 0, 1, 1, 1, 1]), 'M': 12, 'nu': 0.1},\n        {'X': np.array([-10.0, 0.0, 10.0]), 'y': np.array([0, 1, 1]), 'M': 1, 'nu': 0.8}\n    ]\n\n    all_results_str = [process_case(case) for case in test_cases]\n    \n    final_output = f\"[{','.join(all_results_str)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A powerful model is not just one that fits the training data well, but one that generalizes to new, unseen data. A key challenge in building such models is preventing them from learning spurious patterns from noise, a phenomenon known as overfitting. This practice investigates the behavior of GBMs when the dataset contains irrelevant \"noise\" features alongside genuinely predictive ones.\n\nThrough this simulation , you will see firsthand how a greedy, iterative algorithm can mistakenly select these noise features during the tree-building process. You will also discover the crucial role of the learning rate, $\\nu$, as a regularization parameter. By slowing down the learning process, you enable the model to prioritize stronger, more consistent patterns from relevant features, thereby building a more robust and reliable predictor.",
            "id": "3125513",
            "problem": "Consider a binary split regression tree used inside a Gradient Boosting Machine (GBM), where the impurity is measured by the sum of squared errors (SSE). The foundational base for this problem is the least-squares risk minimization principle in statistical learning, which defines the empirical risk for predictions as $$\\mathcal{R}(F) = \\sum_{i=1}^{n} \\left(y_i - F(x_i)\\right)^2,$$ where $F$ is a predictor, $x_i \\in \\mathbb{R}^d$ are feature vectors, and $y_i \\in \\mathbb{R}$ are target values. Gradient boosting for least squares proceeds by iteratively fitting a weak learner $h_m(x)$ to the negative gradient (the residuals), which for least squares equals $$r_i^{(m)} = y_i - F_{m-1}(x_i),$$ and updating the predictor by $$F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x),$$ where $\\nu \\in (0,1]$ is the learning rate (regularization parameter) controlling the step size.\n\nIn a binary split regression tree (a decision stump), given residuals $\\{r_i\\}_{i=1}^n$ at iteration $m$, a split on feature $j$ at threshold $\\tau$ divides the data into left and right nodes. For any set $S$ with $|S| = N$, the node prediction minimizing SSE is the mean residual $\\bar{r}_S = \\frac{1}{N} \\sum_{i \\in S} r_i$, and the node's SSE is $$\\mathrm{SSE}(S) = \\sum_{i \\in S} \\left(r_i - \\bar{r}_S\\right)^2.$$ The impurity decrease for a candidate split $(j,\\tau)$ is $$\\Delta(j,\\tau) = \\mathrm{SSE}(\\text{parent}) - \\left(\\mathrm{SSE}(\\text{left}) + \\mathrm{SSE}(\\text{right})\\right),$$ and the chosen split maximizes $\\Delta(j,\\tau)$ over all features and valid thresholds.\n\nSuppose the data-generating process has $r$ relevant features $\\{x_1,\\dots,x_r\\}$ and $p$ irrelevant features $\\{z_1,\\dots,z_p\\}$ injected into the feature set. The target is generated from relevant features plus noise. Under least-squares boosting, irrelevant features are statistically independent of the residuals in expectation, so their expected impurity decrease is lower than that of relevant features. Regularization via $\\nu$ modulates the magnitude of updates, affecting how quickly structured residual components are removed and how often subsequent splits may chase noise. Empirically, smaller $\\nu$ tends to reduce the frequency and impact of splits on irrelevant features over a fixed number of iterations $m$.\n\nYour task is to write a complete, runnable program that:\n- Simulates a dataset with $n$ samples, $r$ relevant features, and $p$ irrelevant features. Generate relevant features $\\{x_1,x_2\\}$ independently from a standard normal distribution, and generate $\\{z_k\\}_{k=1}^p$ independently from a standard normal distribution, all mutually independent. Define targets by a piecewise-constant rule aligned with tree splits:\n  $$y = 2 \\cdot \\mathbb{I}[x_1 > 0] + 1.5 \\cdot \\mathrm{sign}(x_2) + \\epsilon,$$\n  where $\\epsilon \\sim \\mathcal{N}(0, 0.5^2)$, and $\\mathrm{sign}(u) \\in \\{-1,0,1\\}$ with the convention that $\\mathrm{sign}(0)=0$. Use only the first $r=2$ relevant features as defined; all additional $p$ features are irrelevant.\n- Implements least-squares GBM with decision stumps for $m$ iterations and learning rate $\\nu$. At each iteration:\n  1. Compute residuals $r_i^{(m)} = y_i - F_{m-1}(x_i)$.\n  2. For each feature $j \\in \\{1,\\dots,r+p\\}$, consider all thresholds at midpoints between consecutive sorted unique feature values. For each threshold, compute $\\Delta(j,\\tau)$ using the SSE definition and select the best threshold per feature.\n  3. Choose the feature-threshold pair $(j^\\star, \\tau^\\star)$ with maximum impurity decrease $\\Delta^\\star$ across all features. Define the left and right leaf predictions as the mean residuals in the respective nodes, and update $F_m(x)$ with $\\nu$ times the stump's leaf predictions.\n  4. Track whether $j^\\star$ is one of the injected irrelevant features $\\{z_k\\}$ and accumulate the total impurity decrease $\\Delta^\\star$, as well as the portion attributable to irrelevant-feature splits (when $j^\\star$ is irrelevant).\n- For each test case, return two quantities:\n  1. The fraction of iterations in which an irrelevant feature is selected for the split, i.e., $$\\text{usage\\_fraction} = \\frac{\\#\\{\\text{iterations with } j^\\star \\in \\{z_k\\}\\}}{m}.$$\n  2. The ratio of cumulative impurity decrease attributed to irrelevant-feature splits to the total cumulative impurity decrease across all splits, i.e., $$\\text{impurity\\_share\\_irrelevant} = \\frac{\\sum_{\\text{irrelevant splits}} \\Delta^\\star}{\\sum_{\\text{all splits}} \\Delta^\\star}.$$\nIf the denominator in the fraction is zero, return $0$ for the ratio.\n\nUse the following test suite to evaluate different facets of the behavior:\n- Case A (baseline without irrelevant features): $n=300$, $r=2$, $p=0$, $m=20$, $\\nu=0.1$, with a fixed random seed $1$.\n- Case B (moderate irrelevant features, moderate learning rate): $n=300$, $r=2$, $p=10$, $m=20$, $\\nu=0.1$, with a fixed random seed $2$.\n- Case C (moderate irrelevant features, smaller learning rate): $n=300$, $r=2$, $p=10$, $m=20$, $\\nu=0.01$, with a fixed random seed $3$.\n- Case D (edge case, single iteration): $n=300$, $r=2$, $p=10$, $m=1$, $\\nu=0.1$, with a fixed random seed $4$.\n- Case E (many irrelevant features, more iterations): $n=300$, $r=2$, $p=50$, $m=30$, $\\nu=0.1$, with a fixed random seed $5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result should be a two-element list of floats in the order $[\\text{usage\\_fraction}, \\text{impurity\\_share\\_irrelevant}]$. For example, the final output format must be of the form `[[u_1, s_1], [u_2, s_2], ...]]`, printed exactly as a single line like `[[u1,s1],[u2,s2],...]`. No physical units or angles are involved; express all ratios as decimals.",
            "solution": "The user-provided problem has been analyzed and validated against the specified criteria.\n\n### Step 1: Extract Givens\n- **Model Framework**: Gradient Boosting Machine (GBM) using binary split regression trees (decision stumps).\n- **Optimization Principle**: Least-squares risk minimization, with empirical risk $\\mathcal{R}(F) = \\sum_{i=1}^{n} (y_i - F(x_i))^2$.\n- **Boosting Algorithm**: Iteratively fit a weak learner $h_m(x)$ to the negative gradient (residuals) $r_i^{(m)} = y_i - F_{m-1}(x_i)$.\n- **Model Update Rule**: $F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)$, where $\\nu$ is the learning rate.\n- **Weak Learner Details**:\n    - **Split Criterion**: Maximize impurity decrease $\\Delta(j,\\tau)$, measured by Sum of Squared Errors (SSE).\n    - **Node Impurity**: For a node with data index set $S$, $\\mathrm{SSE}(S) = \\sum_{i \\in S} (r_i - \\bar{r}_S)^2$, where $\\bar{r}_S$ is the mean residual in the node.\n    - **Impurity Decrease**: $\\Delta(j,\\tau) = \\mathrm{SSE}(\\text{parent}) - (\\mathrm{SSE}(\\text{left}) + \\mathrm{SSE}(\\text{right}))$.\n    - **Split Thresholds**: Midpoints between consecutive sorted unique feature values.\n- **Data Generation**:\n    - $n$ samples, $r$ relevant features $\\{x_j\\}_{j=1}^r$, $p$ irrelevant features $\\{z_k\\}_{k=1}^p$.\n    - $x_j \\sim \\mathcal{N}(0, 1)$ and $z_k \\sim \\mathcal{N}(0, 1)$, all mutually independent.\n    - Target: $y = 2 \\cdot \\mathbb{I}[x_1 > 0] + 1.5 \\cdot \\mathrm{sign}(x_2) + \\epsilon$, with $\\epsilon \\sim \\mathcal{N}(0, 0.5^2)$, $r=2$, and $\\mathrm{sign}(0)=0$.\n- **Task**: For a set of test cases, implement the GBM and compute two metrics:\n    1. `usage_fraction`: The proportion of boosting iterations where the split is on an irrelevant feature.\n    2. `impurity_share_irrelevant`: The ratio of cumulative impurity decrease from irrelevant-feature splits to the total cumulative impurity decrease.\n- **Test Cases**:\n    - A: $n=300, r=2, p=0, m=20, \\nu=0.1$, seed=1.\n    - B: $n=300, r=2, p=10, m=20, \\nu=0.1$, seed=2.\n    - C: $n=300, r=2, p=10, m=20, \\nu=0.01$, seed=3.\n    - D: $n=300, r=2, p=10, m=1, \\nu=0.1$, seed=4.\n    - E: $n=300, r=2, p=50, m=30, \\nu=0.1$, seed=5.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes the standard least-squares gradient boosting algorithm (LS-Boost). The concepts of residuals as negative gradients, weak learners, SSE impurity, and regularization via learning rate are fundamental to statistical learning theory. The setup is scientifically sound.\n- **Well-Posed**: The problem is well-posed. The data generation process, algorithmic steps, parameters, and desired outputs are all specified unambiguously. A single, deterministic solution exists for each test case given the fixed random seed. A minor omission is the initial model $F_0(x)$, but the standard convention for least-squares regression is to initialize with the constant model that minimizes SSE, which is the mean of the target variable, $F_0(x) = \\bar{y}$. This is a standard and non-controversial assumption.\n- **Objective**: The problem is stated in precise, objective mathematical and algorithmic language, free from subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined computational task in machine learning that requires a faithful implementation of a standard algorithm. The solution process will now proceed.\n\n### Solution\nThe problem requires the implementation of a Gradient Boosting Machine (GBM) with decision stumps as weak learners to analyze the behavior of the algorithm in the presence of irrelevant features. We will construct a simulation that adheres to the specified data generation process and GBM algorithm.\n\n**1. Data Generation**\nFor each test case with parameters $n$, $r$, $p$, and a random seed, we generate the data as follows:\n- A total of $r+p$ features are created. The first $r=2$ are relevant, and the subsequent $p$ are irrelevant.\n- The feature matrix $\\mathbf{X}$ of size $n \\times (r+p)$ is populated with values drawn from a standard normal distribution, $\\mathcal{N}(0, 1)$.\n- The target vector $\\mathbf{y}$ of size $n$ is generated according to the rule:\n$$y_i = 2 \\cdot \\mathbb{I}[x_{i,1} > 0] + 1.5 \\cdot \\mathrm{sign}(x_{i,2}) + \\epsilon_i$$\nwhere $x_{i,1}$ and $x_{i,2}$ are the two relevant features for the $i$-th sample, $\\mathbb{I}[\\cdot]$ is the indicator function, and $\\epsilon_i \\sim \\mathcal{N}(0, 0.5^2)$ is a noise term.\n\n**2. Gradient Boosting Algorithm**\nThe GBM is built iteratively.\n\n**Initialization**: The initial model $F_0(\\mathbf{x})$ must be chosen. For a least-squares loss function, the optimal constant prediction is the mean of the observed target values. Thus, we initialize the model for all samples $i=1, \\dots, n$ as:\n$$F_0(\\mathbf{x}_i) = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$$\n\n**Iteration**: The model is updated for $m$ iterations. For each iteration $k=1, \\dots, m$:\n\n**a. Compute Residuals**: The negative gradient of the SSE loss function with respect to the model predictions $F_{k-1}(\\mathbf{x}_i)$ are the residuals:\n$$r_i^{(k)} = y_i - F_{k-1}(\\mathbf{x}_i)$$\n\n**b. Fit a Decision Stump**: A decision stump $h_k(\\mathbf{x})$ is trained to predict the residuals $r_i^{(k)}$. This involves finding the best binary split across all features and all possible thresholds. A split is defined by a feature index $j$ and a threshold $\\tau$. The quality of a split is measured by the impurity decrease, which for SSE is the variance reduction.\n\nThe impurity of a node containing a set of residual indices $S$ is $\\mathrm{SSE}(S) = \\sum_{i \\in S} (r_i - \\bar{r}_S)^2$. The impurity decrease for a split of a parent node $S_P$ into left $S_L$ and right $S_R$ nodes is $\\Delta = \\mathrm{SSE}(S_P) - (\\mathrm{SSE}(S_L) + \\mathrm{SSE}(S_R))$. This can be computed more efficiently. Let $T_S = \\sum_{i \\in S} r_i$ and $N_S = |S|$. The SSE can be written as $\\mathrm{SSE}(S) = \\sum_{i \\in S} r_i^2 - T_S^2/N_S$. The impurity decrease simplifies to:\n$$\\Delta = \\left( \\frac{T_L^2}{N_L} + \\frac{T_R^2}{N_R} \\right) - \\frac{T_P^2}{N_P}$$\nIn our case, the parent node for each split contains all $n$ samples. We search for the feature $j^\\star$ and threshold $\\tau^\\star$ that maximize this quantity $\\Delta^\\star$.\n\nThe search for the optimal split $(j^\\star, \\tau^\\star)$ is performed by iterating through each feature $j \\in \\{1, \\dots, r+p\\}$. For each feature, we consider thresholds at the midpoints of its consecutive unique sorted values. An efficient method to find the best threshold for a feature involves sorting the feature values along with the corresponding residuals and using running sums to calculate the quantities $T_L$, $T_R$, $N_L$, and $N_R$ for each potential split point.\n\n**c. Update the Model**: Once the best split $(j^\\star, \\tau^\\star)$ is found, the stump $h_k(\\mathbf{x})$ is defined. Its predictions are constant values in the two leaves, equal to the mean of the residuals in each leaf:\n$$h_k(\\mathbf{x}) = \\begin{cases} \\bar{r}_{L} & \\text{if } x_j \\le \\tau^\\star \\\\ \\bar{r}_{R} & \\text{if } x_j > \\tau^\\star \\end{cases}$$\nThe ensemble model is then updated with a step of size $\\nu$:\n$$F_k(\\mathbf{x}) = F_{k-1}(\\mathbf{x}) + \\nu \\cdot h_k(\\mathbf{x})$$\n\n**3. Performance Metrics**\nThroughout the $m$ iterations, we track the chosen split feature $j^\\star$ and the corresponding impurity decrease $\\Delta^\\star$.\n- We count the number of times an irrelevant feature is chosen, $N_{\\text{irrelevant}} = \\sum_{k=1}^m \\mathbb{I}[j_k^\\star > r]$.\n- We accumulate the total impurity decrease, $\\Delta_{\\text{total}} = \\sum_{k=1}^m \\Delta_k^\\star$.\n- We accumulate the impurity decrease from irrelevant splits, $\\Delta_{\\text{irrelevant}} = \\sum_{k=1}^m \\Delta_k^\\star \\cdot \\mathbb{I}[j_k^\\star > r]$.\n\nFinally, we compute the required metrics:\n- $\\text{usage\\_fraction} = N_{\\text{irrelevant}} / m$\n- $\\text{impurity\\_share\\_irrelevant} = \\Delta_{\\text{irrelevant}} / \\Delta_{\\text{total}}$ (with the convention that this is $0$ if $\\Delta_{\\text{total}} = 0$).\n\nThe implementation will process each test case with its specific parameters and random seed, yielding the two specified floating-point values.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Gradient Boosting Machine simulation problem.\n    \"\"\"\n    test_cases = [\n        # (n, r, p, m, nu, seed)\n        (300, 2, 0, 20, 0.1, 1),   # Case A: baseline\n        (300, 2, 10, 20, 0.1, 2),  # Case B: moderate irrelevant, moderate nu\n        (300, 2, 10, 20, 0.01, 3), # Case C: moderate irrelevant, small nu\n        (300, 2, 10, 1, 0.1, 4),   # Case D: edge case, single iteration\n        (300, 2, 50, 30, 0.1, 5),  # Case E: many irrelevant, more iterations\n    ]\n\n    results = []\n    for case in test_cases:\n        n, r, p, m, nu, seed = case\n        \n        # Set random seed for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Data Generation\n        num_features = r + p\n        X = rng.normal(size=(n, num_features))\n\n        # Target variable generation based on the first r=2 features\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        epsilon = rng.normal(loc=0, scale=0.5, size=n)\n        y = 2 * (x1 > 0) + 1.5 * np.sign(x2) + epsilon\n\n        # 2. GBM Implementation\n        # Initialization\n        F = np.full(n, np.mean(y))\n        \n        irrelevant_split_count = 0\n        total_impurity_decrease = 0.0\n        irrelevant_impurity_decrease = 0.0\n\n        for _ in range(m):\n            # Compute residuals\n            residuals = y - F\n            \n            # Find the best split\n            best_gain = -1.0\n            best_split_info = None\n\n            sum_residuals_parent = np.sum(residuals)\n            # This term is constant for the current iteration across all splits\n            parent_impurity_term = (sum_residuals_parent**2) / n\n\n            for j in range(num_features):\n                feature_values = X[:, j]\n                \n                sorted_indices = np.argsort(feature_values)\n                sorted_residuals = residuals[sorted_indices]\n                sorted_feature_values = feature_values[sorted_indices]\n                \n                running_sum_left = 0.0\n                \n                for i in range(n - 1):\n                    n_left = i + 1\n                    running_sum_left += sorted_residuals[i]\n                    \n                    if sorted_feature_values[i] < sorted_feature_values[i+1]:\n                        n_right = n - n_left\n                        \n                        sum_residuals_left = running_sum_left\n                        sum_residuals_right = sum_residuals_parent - sum_residuals_left\n                        \n                        gain = (sum_residuals_left**2) / n_left + \\\n                               (sum_residuals_right**2) / n_right - \\\n                               parent_impurity_term\n                        \n                        if gain > best_gain:\n                            best_gain = gain\n                            threshold = (sorted_feature_values[i] + sorted_feature_values[i+1]) / 2.0\n                            best_split_info = {\n                                \"feature_idx\": j,\n                                \"threshold\": threshold,\n                                \"gain\": gain,\n                                \"mean_res_left\": sum_residuals_left / n_left,\n                                \"mean_res_right\": sum_residuals_right / n_right,\n                            }\n            \n            if best_split_info is None:\n                break # No valid split found\n\n            # Update tracking variables\n            total_impurity_decrease += best_split_info[\"gain\"]\n            if best_split_info[\"feature_idx\"] >= r:\n                irrelevant_split_count += 1\n                irrelevant_impurity_decrease += best_split_info[\"gain\"]\n                \n            # Update the model F\n            feature_idx = best_split_info[\"feature_idx\"]\n            threshold = best_split_info[\"threshold\"]\n            \n            left_mask = X[:, feature_idx] <= threshold\n            \n            F[left_mask] += nu * best_split_info[\"mean_res_left\"]\n            F[~left_mask] += nu * best_split_info[\"mean_res_right\"]\n\n        # 3. Calculate final metrics\n        usage_fraction = irrelevant_split_count / m if m > 0 else 0.0\n        \n        if total_impurity_decrease > 1e-9: # Use a small tolerance for floating point\n            impurity_share_irrelevant = irrelevant_impurity_decrease / total_impurity_decrease\n        else:\n            impurity_share_irrelevant = 0.0\n            \n        results.append([usage_fraction, impurity_share_irrelevant])\n\n    # Format the final output string exactly as required\n    result_strings = [f\"[{u},{s}]\" for u, s in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Tuning a Gradient Boosting Machine often involves finding a delicate balance between the learning rate $\\nu$ and the number of trees $M$. While it might be tempting to treat them as independent hyperparameters, they are deeply intertwined. This hands-on experiment explores the hypothesis that for a fixed total \"learning budget,\" represented by the product $M\\nu$, the model's training path becomes similar for different combinations of $\\nu$ and $M$, particularly as $\\nu$ gets smaller.\n\nBy analyzing the training loss trajectories under this constraint , you will develop a more sophisticated intuition for hyperparameter tuning. This exercise connects the discrete steps of the boosting algorithm to the continuous-time concept of a \"gradient flow,\" demonstrating that a smaller learning rate typically requires a larger number of trees to achieve a comparable level of performance. This reinforces the key trade-off between model complexity and the learning schedule in practice.",
            "id": "3125556",
            "problem": "You are asked to design and implement a program that empirically studies the interplay between the learning rate $\\nu$ and the number of base learners (trees) $M$ in a Gradient Boosting Machine (GBM), using the principle of functional gradient descent and empirical risk minimization under the squared error loss. The objective is to hold the product $M \\nu$ constant and compare the training loss trajectories $R(f_m)$ across different $\\nu$ values to test the hypothesis that, when $M \\nu$ is fixed, the training loss as a function of the cumulative stage $t = m \\nu$ follows similar paths.\n\nStart from the following fundamental base:\n- Empirical risk minimization for regression under squared error loss: for a model $f$ and data $\\{(x_i, y_i)\\}_{i=1}^n$, the empirical risk is $R(f) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, f(x_i))$ with $L(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2$.\n- Functional gradient descent: iterative methods that update a function $f_m$ to reduce $R(f_m)$ by following the negative functional gradient of $R$, approximated in a restricted function class (here, decision stumps).\n\nYou will implement GBM for regression with the following constraints:\n- Base learner is a decision stump (a regression tree of depth $1$): pick a single feature index $j$ and threshold $\\tau$, and predict a constant $c_\\ell$ if $x_j \\le \\tau$ and $c_r$ otherwise. Each stump must be fitted to minimize the squared error on the current pseudo-residuals via least squares. For any fixed split, the optimal region predictions must be the regional means of the residuals, and the split selection must minimize the sum of squared errors across regions. All thresholds must be chosen as midpoints of adjacent sorted feature values.\n- The initial model must be $f_0(x) = 0$ for all $x$.\n\nThe hypothesis to test is: for fixed $T = M \\nu$, and when $m$ is large enough and $\\nu$ is small enough to emulate a gradient flow in continuous time, the training loss trajectory $R(f_m)$ as a function of $t = m \\nu$ is similar across different $\\nu$ schedules. You must operationalize similarity by interpolating each trajectory onto a common grid of $t$ values in $[0, T]$ and computing the maximum absolute deviation across trajectories on that grid.\n\nImplement the following experimental setup.\n\nData generation:\n- All features must be independent and identically distributed uniform random variables on $[0, 1]$. Set the random seed explicitly for each test case to ensure reproducibility.\n- The label $y$ must be generated from a smooth or piecewise function of the features, plus independent Gaussian noise with mean $0$ and specified standard deviation $\\sigma$.\n\nModel and loss:\n- Use squared error loss $L(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2$ in the conceptual derivation, and report $R(f_m) = \\frac{1}{n} \\sum_{i=1}^n (y_i - f_m(x_i))^2$ at each stage for the empirical trajectory.\n\nExperimental parameters:\n- Fix the cumulative stage horizon $T = M \\nu = 2.0$.\n- Use learning rates $\\nu \\in \\{0.2, 0.1, 0.05, 0.01\\}$, and set $M = T / \\nu$ for each choice to keep $M \\nu$ constant.\n- Interpolate each loss trajectory onto a uniform grid of $t$ values with $201$ points in $[0, T]$.\n\nSimilarity metric:\n- For a given dataset and the four $(\\nu, M)$ settings, compute the interpolated loss curves on the common grid. Then compute the maximum absolute deviation across all pairs of interpolated curves, i.e., $\\max_{a < b} \\max_{t \\in \\text{grid}} |R_a(t) - R_b(t)|$.\n\nTest suite:\n- Provide three independent test cases that exercise different regimes:\n    1. Happy path: $n = 200$, $d = 2$, seed $= 0$, $\\sigma = 0.1$, $y = \\sin(2 \\pi x_1) + x_2^2 + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$, and $x_1, x_2 \\sim \\text{Uniform}(0, 1)$.\n    2. Noisier regime: $n = 200$, $d = 2$, seed $= 1$, $\\sigma = 0.5$, $y = \\sin(2 \\pi x_1) + x_2^2 + \\varepsilon$, same feature distribution as above.\n    3. Piecewise target: $n = 200$, $d = 1$, seed $= 2$, $\\sigma = 0.05$, $y = \\mathbb{I}\\{x_1 > 0.5\\} + \\varepsilon$, where $\\mathbb{I}\\{\\cdot\\}$ is the indicator function.\n\nOutput specification:\n- For each test case, compute the single float equal to the maximum absolute deviation across trajectories as defined above.\n- Your program should produce a single line of output containing these three results, rounded to six decimal places, as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3]\").\n\nNo user input should be read. All random seeds, dataset sizes, and parameters must be hard-coded exactly as specified. The program must be fully self-contained and deterministic given the stated seeds. Angles are not involved; no physical units are involved.",
            "solution": "This solution provides a detailed explanation and implementation of a Gradient Boosting Machine (GBM) for regression, designed to empirically study the relationship between the learning rate $\\nu$ and the number of estimators $M$ while their product $M\\nu$ is held constant. The framework is rooted in the principle of functional gradient descent.\n\n### 1. Theoretical Framework: Functional Gradient Descent\n\nGradient Boosting Machines are a class of ensemble learning methods that build a strong predictive model by sequentially adding weak learners. The core idea can be interpreted as performing gradient descent in a function space.\n\nThe objective is to find a function $f(x)$ that minimizes the empirical risk, defined as the average loss over the training data $\\{(x_i, y_i)\\}_{i=1}^n$:\n$$\nR(f) = \\frac{1}{n} \\sum_{i=1}^n L(y_i, f(x_i))\n$$\nFor this problem, the specified loss function is the squared error loss, scaled by a factor of $\\frac{1}{2}$:\n$$\nL(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2\n$$\nThe GBM algorithm is iterative. It starts with an initial model $f_0(x)$ and sequentially updates it:\n$$\nf_m(x) = f_{m-1}(x) + v_m(x)\n$$\nwhere $v_m(x)$ is a function chosen to \"steepest-descend\" the risk $R(f)$. In functional gradient descent, the ideal update $v_m(x_i)$ for each data point $i$ is proportional to the negative gradient of the loss function with respect to the function value $f(x_i)$, evaluated at the current model $f_{m-1}(x_i)$.\n\nFor a single data point $(x_i, y_i)$, the negative gradient is:\n$$\nr_{im} = - \\left[ \\frac{\\partial L(y_i, F)}{\\partial F} \\right]_{F=f_{m-1}(x_i)} = - \\frac{1}{2} \\cdot 2 \\cdot (f_{m-1}(x_i) - y_i) \\cdot (-1) = y_i - f_{m-1}(x_i)\n$$\nThese quantities, $r_{im}$, are called the pseudo-residuals for iteration $m$. The ideal update function $v_m(x)$ would be one that satisfies $v_m(x_i) = r_{im}$ for all $i$. However, we constrain our update functions to come from a restricted class of \"weak learners,\" in this case, decision stumps.\n\nTherefore, at each stage $m$, we fit a base learner, $h_m(x)$, to the current pseudo-residuals. That is, we solve the following optimization problem:\n$$\nh_m = \\arg\\min_{h \\in \\mathcal{H}} \\sum_{i=1}^n (r_{im} - h(x_i))^2\n$$\nwhere $\\mathcal{H}$ is the class of all possible decision stumps.\n\nOnce the best base learner $h_m$ is found, the model is updated using a shrunken step:\n$$\nf_m(x) = f_{m-1}(x) + \\nu h_m(x)\n$$\nHere, $\\nu \\in (0, 1]$ is the learning rate, which controls the contribution of each weak learner to the final model.\n\n### 2. Algorithm Implementation\n\nThe specific algorithm implemented here follows these steps:\n\n1.  **Initialization**: The initial model is set to $f_0(x) = 0$, as specified. The number of data points is $n$ and the number of features is $d$.\n\n2.  **Iteration**: For $m = 1, 2, \\dots, M$:\n    a.  **Compute Pseudo-Residuals**: For each sample $i=1, \\dots, n$, calculate the pseudo-residual $r_{im} = y_i - f_{m-1}(x_i)$.\n    b.  **Fit a Base Learner**: Fit a decision stump $h_m(x)$ to the training set $\\{(x_i, r_{im})\\}_{i=1}^n$ by minimizing the sum of squared errors.\n    c.  **Update the Model**: Update the ensemble model: $f_m(x_i) = f_{m-1}(x_i) + \\nu h_m(x_i)$ for all $i$.\n    d.  **Record Loss**: Calculate and store the training risk $R(f_m) = \\frac{1}{n} \\sum_{i=1}^n (y_i - f_m(x_i))^2$.\n\n### 3. Base Learner: Decision Stump\n\nA decision stump is a regression tree of depth $1$. It partitions the feature space with a single cut along one feature axis.\n-   A stump is defined by a feature index $j \\in \\{1, \\dots, d\\}$, a threshold $\\tau$, and two constant values for the resulting regions: $c_\\ell$ for the left leaf ($x_j \\le \\tau$) and $c_r$ for the right leaf ($x_j > \\tau$).\n-   To find the best stump for a given set of pseudo-residuals $\\{r_i\\}$, we iterate through all possible features $j$ and all valid thresholds $\\tau$.\n-   For each feature $j$, the potential thresholds $\\tau$ are the midpoints between adjacent, sorted values of that feature.\n-   For a given split $(j, \\tau)$, the data is partitioned into a left set $I_\\ell = \\{i \\mid x_{ij} \\le \\tau\\}$ and a right set $I_r = \\{i \\mid x_{ij} > \\tau\\}$.\n-   The optimal predictions $c_\\ell$ and $c_r$ that minimize the squared error for this split are the means of the residuals in each region:\n    $$\n    c_\\ell = \\frac{1}{|I_\\ell|} \\sum_{i \\in I_\\ell} r_i \\quad \\text{and} \\quad c_r = \\frac{1}{|I_r|} \\sum_{i \\in I_r} r_i\n    $$\n-   The quality of the split is measured by the total sum of squared errors (SSE):\n    $$\n    \\text{SSE}(j, \\tau) = \\sum_{i \\in I_\\ell} (r_i - c_\\ell)^2 + \\sum_{i \\in I_r} (r_i - c_r)^2\n    $$\n-   The algorithm selects the feature $j^*$ and threshold $\\tau^*$ that yield the minimum SSE. The resulting stump $h(x)$ is defined by $(j^*, \\tau^*, c_\\ell^*, c_r^*)$. An efficient implementation sorts the data by the feature value and uses running sums to calculate the SSE for all possible splits in $O(n \\log n)$ time per feature.\n\n### 4. Experimental Design and Analysis\n\nThe experiment aims to test the hypothesis that for a fixed cumulative \"time\" $T = M\\nu$, the training loss trajectory becomes independent of the specific choice of $\\nu$ and $M$, provided $\\nu$ is small and $M$ is large. This reflects the convergence of the discrete boosting updates to a continuous gradient flow.\n\n-   **Data Generation**: For each test case, a dataset of size $n=200$ is generated based on specified underlying functions, feature distributions, and noise levels, using a fixed random seed for reproducibility.\n-   **Parameter Schedules**: The total \"boosting time\" is fixed at $T = 2.0$. We test four learning rates: $\\nu \\in \\{0.2, 0.1, 0.05, 0.01\\}$. The corresponding number of boosting iterations are $M = T/\\nu$, which are $\\{10, 20, 40, 200\\}$.\n-   **Loss Trajectories**: For each $(\\nu, M)$ pair, the GBM is trained, and the training risk $R(f_m)$ is recorded at each step $m=0, \\dots, M$. This gives a sequence of loss values. We map these to the time-like variable $t_m = m \\nu$, yielding a set of points $(t_m, R(f_m))$.\n-   **Trajectory Comparison**: To compare the four loss trajectories, they must be evaluated on a common grid. We define a uniform grid of $201$ points for $t \\in [0, T]$. Each empirical loss trajectory is interpolated onto this grid using linear interpolation.\n-   **Similarity Metric**: The dissimilarity between the trajectories is quantified by the maximum absolute deviation. We compute this deviation for all pairs of interpolated curves and report the overall maximum:\n    $$\n    \\text{MaxDev} = \\max_{a, b \\in \\{\\nu_1, \\dots, \\nu_4\\}, a < b} \\left( \\max_{t \\in \\text{grid}} |R_{\\text{interp}, a}(t) - R_{\\text{interp}, b}(t)| \\right)\n    $$\nThis procedure is repeated for three distinct test cases to evaluate the hypothesis under different data-generating processes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GBM experiment and produce the final output.\n    \"\"\"\n\n    class DecisionStump:\n        \"\"\"\n        A regression tree of depth 1 (a decision stump).\n        \"\"\"\n        def __init__(self):\n            self.feature_index = None\n            self.threshold = None\n            self.left_value = None\n            self.right_value = None\n            self.min_sse = float('inf')\n\n        def fit(self, X, y):\n            \"\"\"\n            Finds the best split (feature and threshold) to minimize SSE.\n            \"\"\"\n            n_samples, n_features = X.shape\n            if n_samples <= 1:\n                return\n\n            for j in range(n_features):\n                feature_values = X[:, j]\n                sorted_indices = np.argsort(feature_values)\n                y_sorted = y[sorted_indices]\n\n                # Efficiently calculate SSE using running sums\n                sum_y_right = np.sum(y_sorted)\n                sum_sq_y_right = np.sum(y_sorted**2)\n                n_right = n_samples\n                \n                sum_y_left = 0.0\n                sum_sq_y_left = 0.0\n                n_left = 0\n\n                for i in range(n_samples - 1):\n                    val = y_sorted[i]\n                    n_left += 1\n                    n_right -= 1\n                    sum_y_left += val\n                    sum_y_right -= val\n                    sum_sq_y_left += val**2\n                    sum_sq_y_right -= val**2\n\n                    # Only consider splits between unique feature values\n                    if feature_values[sorted_indices[i]] == feature_values[sorted_indices[i+1]]:\n                        continue\n\n                    # SSE = sum(y^2) - (sum(y))^2 / n\n                    sse_left = sum_sq_y_left - (sum_y_left**2) / n_left\n                    sse_right = sum_sq_y_right - (sum_y_right**2) / n_right\n                    total_sse = sse_left + sse_right\n\n                    if total_sse < self.min_sse:\n                        self.min_sse = total_sse\n                        self.feature_index = j\n                        self.threshold = (feature_values[sorted_indices[i]] + feature_values[sorted_indices[i+1]]) / 2.0\n                        self.left_value = sum_y_left / n_left\n                        self.right_value = sum_y_right / n_right\n            \n            # If no split was found (e.g., all X values are identical), predict the mean\n            if self.feature_index is None:\n                self.left_value = np.mean(y)\n                self.right_value = np.mean(y)\n\n\n        def predict(self, X):\n            \"\"\"\n            Makes predictions using the fitted stump.\n            \"\"\"\n            if self.feature_index is None:\n                return np.full(X.shape[0], self.left_value)\n\n            predictions = np.zeros(X.shape[0])\n            left_indices = X[:, self.feature_index] <= self.threshold\n            right_indices = ~left_indices\n            predictions[left_indices] = self.left_value\n            predictions[right_indices] = self.right_value\n            return predictions\n\n    def generate_data(n, d, seed, sigma, target_func_type):\n        \"\"\"\n        Generates data for a specific test case.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.uniform(0, 1, size=(n, d))\n        noise = rng.normal(0, sigma, size=n)\n        \n        if target_func_type == 'smooth':\n            y = np.sin(2 * np.pi * X[:, 0]) + X[:, 1]**2 + noise\n        elif target_func_type == 'piecewise':\n            y = (X[:, 0] > 0.5).astype(float) + noise\n        else:\n            raise ValueError(\"Unknown target function type.\")\n            \n        return X, y\n\n    test_cases = [\n        {'n': 200, 'd': 2, 'seed': 0, 'sigma': 0.1, 'target_func_type': 'smooth'},\n        {'n': 200, 'd': 2, 'seed': 1, 'sigma': 0.5, 'target_func_type': 'smooth'},\n        {'n': 200, 'd': 1, 'seed': 2, 'sigma': 0.05, 'target_func_type': 'piecewise'},\n    ]\n\n    results = []\n    \n    for case_params in test_cases:\n        X, y = generate_data(**case_params)\n        \n        T = 2.0\n        nu_values = [0.2, 0.1, 0.05, 0.01]\n        t_grid = np.linspace(0, T, 201)\n        \n        interpolated_curves = []\n\n        for nu in nu_values:\n            M = int(np.round(T / nu))\n            \n            # This array will store the model's predictions f_m(x_i)\n            F = np.zeros_like(y)\n            losses = []\n            \n            # Initial model f_0 = 0, so initial loss is mean of y^2\n            # Per problem: report R(f_m) = 1/n sum(y-f_m)^2\n            initial_loss = np.mean((y - F)**2)\n            losses.append(initial_loss)\n            \n            for _ in range(M):\n                residuals = y - F\n                stump = DecisionStump()\n                stump.fit(X, residuals)\n                h_m_predictions = stump.predict(X)\n                F += nu * h_m_predictions\n                current_loss = np.mean((y - F)**2)\n                losses.append(current_loss)\n            \n            # Create time points for the loss trajectory\n            t_m = np.arange(M + 1) * nu\n            \n            # Interpolate the loss curve onto the common grid\n            interp_loss = np.interp(t_grid, t_m, losses)\n            interpolated_curves.append(interp_loss)\n        \n        # Calculate the maximum absolute deviation across all pairs of curves\n        max_dev = 0.0\n        num_curves = len(interpolated_curves)\n        for i in range(num_curves):\n            for j in range(i + 1, num_curves):\n                dev = np.max(np.abs(interpolated_curves[i] - interpolated_curves[j]))\n                if dev > max_dev:\n                    max_dev = dev\n        \n        results.append(max_dev)\n\n    # Format the final output string\n    output_str = f\"[{','.join([f'{r:.6f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}