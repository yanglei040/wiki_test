{
    "hands_on_practices": [
        {
            "introduction": "梯度提升的核心是函数空间中的梯度下降。此练习将带您深入探讨GBM的核心更新机制，比较一阶（仅梯度）更新与二阶（牛顿）更新在逻辑斯蒂损失下的表现 。通过亲手实现这两种方法，您将具体理解为何二阶方法能够带来更快的收敛速度，以及这与损失函数的曲率信息有何关联。",
            "id": "3125494",
            "problem": "您将在一台用于二分类的梯度提升机（GBM）中，在逻辑斯谛损失下，实现并比较两种更新规则：一阶纯梯度更新和二阶 Newton 更新。您的任务是为每个提升轮次计算经验风险的相对下降，并评估在固定的浅层基学习器下，两种更新规则的计算开销与收益。\n\n基本设置。考虑独立同分布的数据点 $\\{(x_i, y_i)\\}_{i=1}^N$，其中 $x_i \\in \\mathbb{R}$ 且 $y_i \\in \\{0, 1\\}$。设模型的当前得分（logit）为 $F_i \\in \\mathbb{R}$，预测概率为 $\\sigma(F_i) = \\frac{1}{1 + e^{-F_i}}$，其中 $\\sigma$ 是 logistic sigmoid 函数。在二元逻辑斯谛损失下的经验风险是\n$$\nR(F) \\;=\\; \\sum_{i=1}^N \\left( - y_i \\log \\sigma(F_i) - (1-y_i)\\log(1-\\sigma(F_i)) \\right).\n$$\n\n在每个提升轮次中，都会拟合一个决策树桩基学习器，并将其加到模型得分 $F$ 上。该决策树桩由单一特征 $x$ 上的一个固定阈值 $\\tau$ 定义：\n- 左叶节点索引集：$\\mathcal{L} = \\{ i : x_i \\le \\tau \\}$，\n- 右叶节点索引集：$\\mathcal{R} = \\{ i : x_i > \\tau \\}$，\n其常数输出 $v_{\\mathcal{L}}$ 和 $v_{\\mathcal{R}}$ 分别加到 $i \\in \\mathcal{L}$ 和 $i \\in \\mathcal{R}$ 的 $F_i$ 上。这一轮之后的新模型得分为 $F_i \\leftarrow F_i + \\nu\\, v_{\\text{leaf}(i)}$，其中 $\\nu \\in (0,1]$ 是给定的缩减率（学习率）。\n\n固定树桩阈值规则。对每个数据集，阈值 $\\tau$ 被一次性固定为 $\\{x_i\\}_{i=1}^N$ 的中位数，计算方法如下：如果 $N$ 是奇数，$\\tau$ 是排序后 $\\{x_i\\}$ 的中间元素；如果 $N$ 是偶数，$\\tau$ 是排序后 $\\{x_i\\}$ 中间两个元素的平均值。在所有提升轮次中重复使用相同的 $\\tau$。\n\n更新规则。设 $p_i = \\sigma(F_i)$，$R$ 相对于 $F_i$ 的梯度（一阶导数）是\n$$\ng_i \\;=\\; \\frac{\\partial R}{\\partial F_i} \\;=\\; p_i - y_i,\n$$\nHessian 矩阵对角线元素（二阶导数）是\n$$\nh_i \\;=\\; \\frac{\\partial^2 R}{\\partial F_i^2} \\;=\\; p_i (1 - p_i).\n$$\n您必须为叶节点值 $v_{\\mathcal{L}}$ 和 $v_{\\mathcal{R}}$ 实现两种变体：\n- 纯梯度 GBM：在每个叶节点上通过最小二乘法拟合负梯度，因此\n$$\nv_{\\mathcal{S}} \\;=\\; - \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} g_i, \\quad \\mathcal{S} \\in \\{\\mathcal{L}, \\mathcal{R}\\}.\n$$\n- Newton GBM：在每个叶节点上使用一个二阶（对角）Newton 步，因此\n$$\nv_{\\mathcal{S}} \\;=\\; - \\frac{\\sum_{i \\in \\mathcal{S}} g_i}{\\sum_{i \\in \\mathcal{S}} h_i}, \\quad \\mathcal{S} \\in \\{\\mathcal{L}, \\mathcal{R}\\},\n$$\n约定如果分母为 $0$，则 $v_{\\mathcal{S}} = 0$。\n\n每轮的相对下降。令 $R_{\\text{prev}}$ 和 $R_{\\text{new}}$ 分别表示一轮提升前后的经验风险。将相对下降定义为\n$$\n\\Delta R / R \\;=\\; \\frac{R_{\\text{prev}} - R_{\\text{new}}}{R_{\\text{prev}}}.\n$$\n记录每一轮的这个值。\n\n计算开销度量。为了以可复现的方式评估开销，根据以下规则，只计算每轮更新中使用的浮点加/减法和乘法运算：\n- 不计入的操作：$\\exp(\\cdot)$ 的求值、除法、比较、索引，以及任何纯粹用于度量（例如计算 $R(F)$）的计算。\n- 每个样本的导数计算：\n  - 计算 $g_i = p_i - y_i$ 消耗 1 次加法。\n  - 仅对于 Newton GBM，计算 $h_i = p_i (1 - p_i)$ 消耗 1 次加法（用于 $1 - p_i$）和 1 次乘法。\n- 每个叶节点的归约计算：\n  - 在一个大小为 $|\\mathcal{S}|$ 的叶节点上对 $\\sum_{i \\in \\mathcal{S}} g_i$ 求和消耗 $|\\mathcal{S}|$ 次加法。\n  - 仅对于 Newton GBM，对叶节点上的 $\\sum_{i \\in \\mathcal{S}} h_i$ 求和消耗 $|\\mathcal{S}|$ 次加法。\n- 模型得分更新：\n  - 对于每个更新的样本 $i$，更新 $F_i \\leftarrow F_i + \\nu\\, v_{\\text{leaf}(i)}$ 消耗 1 次乘法和 1 次加法。\n\n对于一个大小为 $N$ 的数据集，具有固定的分区大小 $|\\mathcal{L}|$ 和 $|\\mathcal{R}|$，这得出了每轮的操作计数：\n- 纯梯度 GBM：计算 $g_i$ 需要 $N$ 次加法；对 $g_i$ 求和需要 $|\\mathcal{L}| + |\\mathcal{R}| = N$ 次加法；更新需要 $2N$ 次操作；总计 $4N$ 次被计入的操作。\n- Newton GBM：计算 $g_i$ 需要 $N$ 次加法；计算 $h_i$ 需要 $2N$ 次操作（每个样本一次加法和一次乘法）；对 $g_i$ 求和需要 $N$ 次加法；对 $h_i$ 求和需要 $N$ 次加法；更新需要 $2N$ 次操作；总计 $7N$ 次被计入的操作。\n如果一个叶节点为空，其对归约和更新的贡献为零，其叶节点值定义为 $0$。\n\n性能总结。经过 $M$ 轮后，计算：\n- 纯梯度 GBM 每轮的相对下降 $\\Delta R / R$ 列表。\n- Newton GBM 每轮的相对下降 $\\Delta R / R$ 列表。\n- 每种方法的一个标量“单位开销增益”，定义为\n$$\n\\text{gain\\_per\\_kops} \\;=\\; \\frac{R(F^{(0)}) - R(F^{(M)})}{\\text{ops\\_total}/1000},\n$$\n其中 $\\text{ops\\_total}$ 是该方法在所有 $M$ 轮中被计入操作的总和。\n\n初始条件。对所有 $i$ 使用 $F_i^{(0)} = 0$，因此 $p_i^{(0)} = \\sigma(0) = 0.5$。\n\n测试套件。你的程序必须为以下参数集实现上述内容。对于每种情况，按照规定计算中位数阈值 $\\tau$，并在所有提升轮次中重复使用它。\n\n- 情况 1：$X = [-2.0, -1.0, 0.0, 1.0, 2.0]$，$y = [0, 0, 0, 1, 1]$，$M = 5$，$\\nu = 0.5$。\n- 情况 2：$X = [0.0, 0.0, 0.0, 0.0]$，$y = [0, 1, 0, 1]$，$M = 8$，$\\nu = 0.3$。\n- 情况 3：$X = [-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5]$，$y = [0, 0, 0, 1, 1, 1, 1]$，$M = 12$，$\\nu = 0.1$。\n- 情况 4：$X = [-10.0, 0.0, 10.0]$，$y = [0, 1, 1]$，$M = 1$，$\\nu = 0.8$。\n\n最终输出格式。你的程序必须生成单行输出，其中包含所有情况的结果，以逗号分隔的列表形式，并用方括号括起来。对于每种情况，输出一个包含四个元素的列表：\n- 纯梯度 GBM 每轮的 $\\Delta R/R$ 列表，四舍五入到 $6$ 位小数。\n- Newton GBM 每轮的 $\\Delta R/R$ 列表，四舍五入到 $6$ 位小数。\n- 纯梯度方法每 1000 次操作的增益，四舍五入到 $6$ 位小数。\n- Newton 方法每 1000 次操作的增益，四舍五入到 $6$ 位小数。\n\n因此，总输出必须是形如\n$[ \\text{case1}, \\text{case2}, \\text{case3}, \\text{case4} ]$ 的单行字符串，\n其中每个 $\\text{casek}$ 是一个格式为\n$[ [\\delta_{1}^{\\text{grad}}, \\ldots, \\delta_{M}^{\\text{grad}}], [\\delta_{1}^{\\text{newt}}, \\ldots, \\delta_{M}^{\\text{newt}}], G_{\\text{grad}}, G_{\\text{newt}} ]$ 的列表，\n所有浮点值都四舍五入到 $6$ 位小数。不应打印任何其他文本。不使用角度；不涉及物理单位。百分比必须表示为小数（不带百分号）。",
            "solution": "问题陈述首先经过严格的验证过程。\n\n### 步骤1：提取已知条件\n- **数据**：一组 $N$ 个数据点 $\\{(x_i, y_i)\\}_{i=1}^N$，包含特征 $x_i \\in \\mathbb{R}$ 和二元标签 $y_i \\in \\{0, 1\\}$。\n- **模型得分**：$F_i$，样本 $i$ 的 logit。\n- **预测**：预测概率由 logistic sigmoid 函数给出，$p_i = \\sigma(F_i) = \\frac{1}{1 + e^{-F_i}}$。\n- **损失函数**：经验风险是总的二元逻辑斯谛损失：$R(F) = \\sum_{i=1}^N \\left( - y_i \\log \\sigma(F_i) - (1-y_i)\\log(1-\\sigma(F_i)) \\right)$。\n- **基学习器**：一个具有固定阈值 $\\tau$ 的决策树桩。\n- **叶节点划分**：数据被划分为一个左叶节点 $\\mathcal{L} = \\{ i : x_i \\le \\tau \\}$ 和一个右叶节点 $\\mathcal{R} = \\{ i : x_i > \\tau \\}$。\n- **阈值规则**：$\\tau$ 是 $\\{x_i\\}_{i=1}^N$ 的中位数，当 $N$ 为奇数时取中间元素，当 $N$ 为偶数时取中间两个元素的平均值。该阈值计算一次后在所有提升轮次中重复使用。\n- **模型更新规则**：$F_i \\leftarrow F_i + \\nu \\cdot v_{\\text{leaf}(i)}$，其中 $\\nu$ 是缩减率（学习率）。\n- **导数**：梯度为 $g_i = p_i - y_i$，Hessian 矩阵对角线元素为 $h_i = p_i (1 - p_i)$。\n- **叶节点值规则**：\n    - **纯梯度法**：$v_{\\mathcal{S}} = - \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} g_i$ 对于一个叶节点 $\\mathcal{S}$。如果一个叶节点为空，则 $v_{\\mathcal{S}} = 0$。\n    - **Newton 法**：$v_{\\mathcal{S}} = - \\frac{\\sum_{i \\in \\mathcal{S}} g_i}{\\sum_{i \\in \\mathcal{S}} h_i}$。如果分母为 $0$，则 $v_{\\mathcal{S}} = 0$。\n- **性能度量**：\n    - **每轮相对下降**：$\\Delta R / R = \\frac{R_{\\text{prev}} - R_{\\text{new}}}{R_{\\text{prev}}}$。\n    - **计算开销**：每轮特定浮点运算的抽象计数：纯梯度法为 $4N$，Newton 法为 $7N$。\n    - **单位开销增益**：$\\text{gain\\_per\\_kops} = \\frac{R(F^{(0)}) - R(F^{(M)})}{\\text{ops\\_total}/1000}$。\n- **初始条件**：初始模型得分为 $F_i^{(0)} = 0$ 对于所有 $i$。\n- **测试套件**：提供了四个具体的测试用例，每个用例都包含数据 $(X, y)$、轮数 $M$ 和缩减率 $\\nu$。\n\n### 步骤2：使用提取的已知条件进行验证\n- **科学基础**：该问题在统计学习和数值优化的原理上有坚实的基础。梯度提升、逻辑斯谛损失和 Newton 法都是标准且成熟的概念。所提供的公式是正确的。\n- **适定性**：问题被完全指定。它提供了所有必要的数据、初始条件、所有步骤的明确数学公式，以及对所需输出度量的清晰定义。系统的演化是确定性的，会产生唯一且有意义的解。\n- **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观性陈述。\n\n### 步骤3：结论与行动\n问题是有效的。它是一个基于可靠科学原理的、定义明确的计算任务。将开发一个解决方案。\n\n### 求解推导与算法\n该任务要求实现并比较用于二分类的梯度提升机（GBM）的两种变体。比较基于收敛速度（通过每轮经验风险的相对下降来衡量）和一个定义的 `gain per overhead` 度量。\n\n首先，我们按规定建立 GBM 算法的组件。对于由数据 $(X, y)$、轮数 $M$ 和缩减率 $\\nu$ 定义的每个测试用例：\n\n1.  **初始化**：\n    - 数据点的数量为 $N = |X|$。\n    - 根据指定的中位数规则从 $X$ 计算固定的树桩阈值 $\\tau$。这个 $\\tau$ 定义了左叶节点和右叶节点的索引集 $\\mathcal{L}$ 和 $\\mathcal{R}$，它们在所有提升轮次中保持不变。\n    - 初始化两个模型，一个用于纯梯度方法 (`grad`)，另一个用于 Newton 方法 (`newt`)，其得分为 $F_{\\text{grad}}^{(0)} = F_{\\text{newt}}^{(0)} = \\vec{0}$。\n    - 初始预测概率为 $p_i^{(0)} = \\sigma(0) = 0.5$（对于所有 $i \\in \\{1, \\dots, N\\}$）。\n    - 初始经验风险为 $R(F^{(0)}) = \\sum_{i=1}^N (-\\log(0.5)) = N \\log(2)$。\n\n2.  **提升迭代**：对于从 $1$ 到 $M$ 的每一轮 $m$，我们对 `grad` 和 `newt` 模型都执行一次更新。对于一个得分为 $F$ 的通用模型：\n    a.  计算当前经验风险 $R_{\\text{prev}} = R(F)$。\n    b.  为所有样本 $i$ 计算当前概率 $p_i = \\sigma(F_i)$。\n    c.  计算负梯度向量（伪残差）：$g_i = p_i - y_i$。\n    d.  **叶节点值计算**：两种方法的核心区别在于此。\n        - **纯梯度法**：叶节点值 $v_{\\mathcal{S}}$ 是该叶节点中负伪残差的平均值，实际上是通过最小二乘法（均值）来拟合负梯度：\n          $$ v_{\\mathcal{S}} = - \\frac{1}{|\\mathcal{S}|} \\sum_{i \\in \\mathcal{S}} g_i $$\n          如果叶节点 $\\mathcal{S}$ 为空 ($|\\mathcal{S}|=0$)，其值为 $v_{\\mathcal{S}} = 0$。\n        - **Newton 法**：叶节点值使用二阶信息。计算 Hessian 矩阵的对角元素 $h_i = p_i(1-p_i)$。然后叶节点值是一个二阶 (Newton) 步：\n          $$ v_{\\mathcal{S}} = - \\frac{\\sum_{i \\in \\mathcal{S}} g_i}{\\sum_{i \\in \\mathcal{S}} h_i} $$\n          如果 $\\sum_{i \\in \\mathcal{S}} h_i = 0$，则 $v_{\\mathcal{S}} = 0$。\n    e.  **模型得分更新**：通过加上由缩减率参数 $\\nu$ 缩放的相应叶节点值来更新所有样本的得分：\n        $$ F_i \\leftarrow F_i + \\nu \\cdot v_{\\text{leaf}(i)} $$\n    f.  计算新的经验风险 $R_{\\text{new}} = R(F_{\\text{new}})$。\n    g.  计算并存储该轮的风险相对下降：$\\Delta R / R = (R_{\\text{prev}} - R_{\\text{new}}) / R_{\\text{prev}}$。\n\n3.  **性能总结**：经过 $M$ 轮后，为两种方法计算最终度量。\n    - 总风险减少量为 $R(F^{(0)}) - R(F^{(M)})$。\n    - `grad` 方法的总操作计数 $\\text{ops\\_total}$ 为 $M \\times 4N$，`newt` 方法为 $M \\times 7N$。\n    - `gain_per_kops` 计算公式为 $\\frac{R(F^{(0)}) - R(F^{(M)})}{\\text{ops\\_total}/1000}$。\n\n4.  **实现**：该算法使用 Python 的 `numpy` 库实现，以进行高效的向量和矩阵运算。创建了单独的函数来计算中位数阈值、sigmoid 函数、逻辑斯谛风险，以及为给定方法运行主 GBM 模拟循环。一个主循环遍历四个测试用例，为每个用例调用模拟逻辑，计算最终度量，并按规定格式化输出字符串。通过在指数计算前裁剪大的得分值以防止溢出，以及将概率裁剪到远离精确的 $0$ 或 $1$ 以防止 `log(0)` 错误，来考虑数值稳定性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_median_threshold(X: np.ndarray) -> float:\n    \"\"\"Computes the median of X as per the problem's rules.\"\"\"\n    if len(X) == 0:\n        return 0.0\n    X_sorted = np.sort(X)\n    N = len(X_sorted)\n    if N % 2 == 1:\n        tau = X_sorted[N // 2]\n    else:\n        tau = (X_sorted[N // 2 - 1] + X_sorted[N // 2]) / 2.0\n    return float(tau)\n\ndef sigmoid(F: np.ndarray) -> np.ndarray:\n    \"\"\"Computes the logistic sigmoid function with overflow protection.\"\"\"\n    F_clipped = np.clip(F, -500, 500)\n    return 1.0 / (1.0 + np.exp(-F_clipped))\n\ndef logistic_risk(F: np.ndarray, y: np.ndarray) -> float:\n    \"\"\"Computes the empirical risk (total logistic loss) with numerical stability.\"\"\"\n    p = sigmoid(F)\n    epsilon = 1e-12\n    p_clipped = np.clip(p, epsilon, 1.0 - epsilon)\n    risk = -np.sum(y * np.log(p_clipped) + (1 - y) * np.log(1.0 - p_clipped))\n    return float(risk)\n\ndef run_gbm_variant(\n    X: np.ndarray,\n    y: np.ndarray,\n    M: int,\n    nu: float,\n    leaf_indices_L: np.ndarray,\n    leaf_indices_R: np.ndarray,\n    method: str\n) -> tuple[list[float], np.ndarray]:\n    \"\"\"Runs the GBM simulation for one variant (grad or newton).\"\"\"\n    N = len(y)\n    F = np.zeros(N, dtype=np.float64)\n    rel_decreases = []\n\n    for _ in range(M):\n        R_prev = logistic_risk(F, y)\n\n        p = sigmoid(F)\n        g = p - y\n\n        g_L = g[leaf_indices_L]\n        g_R = g[leaf_indices_R]\n\n        if method == 'grad':\n            v_L = -np.mean(g_L) if len(g_L) > 0 else 0.0\n            v_R = -np.mean(g_R) if len(g_R) > 0 else 0.0\n        else: # newton\n            h = p * (1 - p)\n            h_L = h[leaf_indices_L]\n            h_R = h[leaf_indices_R]\n            \n            sum_g_L = np.sum(g_L)\n            sum_h_L = np.sum(h_L)\n            sum_g_R = np.sum(g_R)\n            sum_h_R = np.sum(h_R)\n\n            v_L = -sum_g_L / sum_h_L if sum_h_L > 1e-9 else 0.0\n            v_R = -sum_g_R / sum_h_R if sum_h_R > 1e-9 else 0.0\n        \n        F[leaf_indices_L] += nu * v_L\n        F[leaf_indices_R] += nu * v_R\n\n        R_new = logistic_risk(F, y)\n        \n        rel_decr = (R_prev - R_new) / R_prev if R_prev > 1e-9 else 0.0\n        rel_decreases.append(rel_decr)\n\n    return rel_decreases, F\n\ndef process_case(case_data: dict) -> str:\n    \"\"\"Processes a single test case and returns the formatted string result.\"\"\"\n    X = case_data['X']\n    y = case_data['y']\n    M = case_data['M']\n    nu = case_data['nu']\n    \n    N = len(X)\n    \n    tau = get_median_threshold(X)\n    indices = np.arange(N)\n    leaf_L_indices = indices[X = tau]\n    leaf_R_indices = indices[X > tau]\n\n    rel_decr_grad, F_final_grad = run_gbm_variant(X, y, M, nu, leaf_L_indices, leaf_R_indices, 'grad')\n    rel_decr_newt, F_final_newt = run_gbm_variant(X, y, M, nu, leaf_L_indices, leaf_R_indices, 'newton')\n\n    R0 = logistic_risk(np.zeros(N), y)\n    R_final_grad = logistic_risk(F_final_grad, y)\n    R_final_newt = logistic_risk(F_final_newt, y)\n\n    ops_total_grad = M * 4 * N\n    ops_total_newt = M * 7 * N\n\n    gain_grad = (R0 - R_final_grad) / (ops_total_grad / 1000.0) if ops_total_grad > 0 else 0.0\n    gain_newt = (R0 - R_final_newt) / (ops_total_newt / 1000.0) if ops_total_newt > 0 else 0.0\n\n    grad_list_str = f\"[{','.join([f'{v:.6f}' for v in rel_decr_grad])}]\"\n    newt_list_str = f\"[{','.join([f'{v:.6f}' for v in rel_decr_newt])}]\"\n    \n    case_result_str = (\n        f\"[{grad_list_str},\"\n        f\"{newt_list_str},\"\n        f\"{gain_grad:.6f},\"\n        f\"{gain_newt:.6f}]\"\n    )\n    \n    return case_result_str\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {'X': np.array([-2.0, -1.0, 0.0, 1.0, 2.0]), 'y': np.array([0, 0, 0, 1, 1]), 'M': 5, 'nu': 0.5},\n        {'X': np.array([0.0, 0.0, 0.0, 0.0]), 'y': np.array([0, 1, 0, 1]), 'M': 8, 'nu': 0.3},\n        {'X': np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5]), 'y': np.array([0, 0, 0, 1, 1, 1, 1]), 'M': 12, 'nu': 0.1},\n        {'X': np.array([-10.0, 0.0, 10.0]), 'y': np.array([0, 1, 1]), 'M': 1, 'nu': 0.8}\n    ]\n\n    all_results_str = [process_case(case) for case in test_cases]\n    \n    final_output = f\"[{','.join(all_results_str)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "梯度提升的一大优势是其内在的特征选择能力，即使在充满噪声和不相关特征的数据中也能识别出重要信号。本练习通过一个模拟实验，让您亲眼见证GBM是如何在众多特征中自动筛选出有效特征的 。您将通过编程实现并观察学习率 $\\nu$ 是如何作为正则化工具，帮助模型抵御无关特征的干扰，从而加深对模型鲁棒性的理解。",
            "id": "3125513",
            "problem": "考虑一个用于梯度提升机 (GBM) 内部的二元分裂回归树，其中不纯度由平方误差和 (SSE) 度量。此问题的基础是统计学习中的最小二乘风险最小化原则，该原则将预测的经验风险定义为 $$\\mathcal{R}(F) = \\sum_{i=1}^{n} \\left(y_i - F(x_i)\\right)^2,$$ 其中 $F$ 是一个预测器，$x_i \\in \\mathbb{R}^d$ 是特征向量，$y_i \\in \\mathbb{R}$ 是目标值。最小二乘的梯度提升通过迭代地将一个弱学习器 $h_m(x)$ 拟合到负梯度（即残差）来进行，对于最小二乘法，残差等于 $$r_i^{(m)} = y_i - F_{m-1}(x_i),$$ 并通过以下方式更新预测器 $$F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x),$$ 其中 $\\nu \\in (0,1]$ 是控制步长的学习率（正则化参数）。\n\n在一个二元分裂回归树（一个决策树桩）中，给定在迭代 $m$ 时的残差 $\\{r_i\\}_{i=1}^n$，在特征 $j$ 和阈值 $\\tau$ 上的一个分裂将数据划分为左节点和右节点。对于任何集合 $S$，其中 $|S| = N$，最小化 SSE 的节点预测是平均残差 $\\bar{r}_S = \\frac{1}{N} \\sum_{i \\in S} r_i$，且该节点的 SSE 为 $$\\mathrm{SSE}(S) = \\sum_{i \\in S} \\left(r_i - \\bar{r}_S\\right)^2.$$ 一个候选分裂 $(j,\\tau)$ 的不纯度下降为 $$\\Delta(j,\\tau) = \\mathrm{SSE}(\\text{parent}) - \\left(\\mathrm{SSE}(\\text{left}) + \\mathrm{SSE}(\\text{right})\\right),$$ 并且所选择的分裂会在所有特征和有效阈值上最大化 $\\Delta(j,\\tau)$。\n\n假设数据生成过程有 $r$ 个相关特征 $\\{x_1,\\dots,x_r\\}$ 和 $p$ 个注入到特征集中的无关特征 $\\{z_1,\\dots,z_p\\}$。目标值由相关特征加上噪声生成。在最小二乘提升下，无关特征在期望上与残差是统计独立的，因此它们的期望不纯度下降低于相关特征。通过 $\\nu$ 进行的正则化可以调节更新的幅度，影响结构化残差分量被移除的速度，以及后续分裂追逐噪声的频率。根据经验，在固定的迭代次数 $m$ 内，较小的 $\\nu$ 倾向于减少对无关特征进行分裂的频率和影响。\n\n您的任务是编写一个完整的、可运行的程序，该程序：\n- 模拟一个含有 $n$ 个样本、 $r$ 个相关特征和 $p$ 个无关特征的数据集。从标准正态分布中独立生成相关特征 $\\{x_1,x_2\\}$，并从标准正态分布中独立生成 $\\{z_k\\}_{k=1}^p$，所有特征相互独立。通过一个与树分裂对齐的分段常数规则来定义目标值：\n  $$y = 2 \\cdot \\mathbb{I}[x_1  0] + 1.5 \\cdot \\mathrm{sign}(x_2) + \\epsilon,$$\n  其中 $\\epsilon \\sim \\mathcal{N}(0, 0.5^2)$，$\\mathrm{sign}(u) \\in \\{-1,0,1\\}$ 并约定 $\\mathrm{sign}(0)=0$。仅使用定义的头 $r=2$ 个相关特征；所有额外的 $p$ 个特征都是无关的。\n- 实现使用决策树桩的最小二乘GBM，进行 $m$ 次迭代，学习率为 $\\nu$。在每次迭代中：\n  1. 计算残差 $r_i^{(m)} = y_i - F_{m-1}(x_i)$。\n  2. 对于每个特征 $j \\in \\{1,\\dots,r+p\\}$，考虑所有在连续排序的唯一特征值之间的中点作为阈值。对于每个阈值，使用 SSE 定义计算 $\\Delta(j,\\tau)$，并为每个特征选择最佳阈值。\n  3. 选择在所有特征中具有最大不纯度下降 $\\Delta^\\star$ 的特征-阈值对 $(j^\\star, \\tau^\\star)$。将左、右叶节点的预测定义为各自节点中的平均残差，并用 $\\nu$ 乘以该树桩的叶节点预测来更新 $F_m(x)$。\n  4. 跟踪 $j^\\star$ 是否为注入的无关特征 $\\{z_k\\}$ 之一，并累加总不纯度下降 $\\Delta^\\star$ 以及归因于无关特征分裂的部分（当 $j^\\star$ 是无关特征时）。\n- 对于每个测试用例，返回两个量：\n  1. 无关特征被选为分裂特征的迭代次数比例，即 $$\\text{usage\\_fraction} = \\frac{\\#\\{\\text{使用 } j^\\star \\in \\{z_k\\} \\text{ 的迭代次数}\\}}{m}。$$\n  2. 归因于无关特征分裂的累积不纯度下降与所有分裂的总累积不纯度下降之比，即 $$\\text{impurity\\_share\\_irrelevant} = \\frac{\\sum_{\\text{无关分裂}} \\Delta^\\star}{\\sum_{\\text{所有分裂}} \\Delta^\\star}。$$\n如果分数中的分母为零，则该比率返回 $0$。\n\n使用以下测试套件来评估行为的不同方面：\n- 情况A（基线情况，无无关特征）：$n=300$，$r=2$，$p=0$，$m=20$，$\\nu=0.1$，固定随机种子为 $1$。\n- 情况B（中等数量无关特征，中等学习率）：$n=300$，$r=2$，$p=10$，$m=20$，$\\nu=0.1$，固定随机种子为 $2$。\n- 情况C（中等数量无关特征，较小学习率）：$n=300$，$r=2$，$p=10$，$m=20$，$\\nu=0.01$，固定随机种子为 $3$。\n- 情况D（边缘情况，单次迭代）：$n=300$，$r=2$，$p=10$，$m=1$，$\\nu=0.1$，固定随机种子为 $4$。\n- 情况E（大量无关特征，更多迭代次数）：$n=300$，$r=2$，$p=50$，$m=30$，$\\nu=0.1$，固定随机种子为 $5$。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果应为一个包含两个浮点数的列表，顺序为 $[\\text{usage\\_fraction}, \\text{impurity\\_share\\_irrelevant}]$。例如，最终输出格式必须为 $$\\big[ [u_1, s_1], [u_2, s_2], \\dots \\big],$$ 精确地打印为单行，如 \"[[u1,s1],[u2,s2],...]\"。不涉及物理单位或角度；所有比率均以小数表示。",
            "solution": "已根据指定标准对用户提供的问题进行了分析和验证。\n\n### 步骤1：提取给定信息\n- **模型框架**：梯度提升机 (GBM)，使用二元分裂回归树（决策树桩）。\n- **优化原则**：最小二乘风险最小化，经验风险为 $\\mathcal{R}(F) = \\sum_{i=1}^{n} (y_i - F(x_i))^2$。\n- **提升算法**：迭代地将弱学习器 $h_m(x)$ 拟合到负梯度（残差） $r_i^{(m)} = y_i - F_{m-1}(x_i)$。\n- **模型更新规则**：$F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)$，其中 $\\nu$ 是学习率。\n- **弱学习器详情**：\n    - **分裂标准**：最大化由平方误差和 (SSE) 度量的不纯度下降 $\\Delta(j,\\tau)$。\n    - **节点不纯度**：对于包含数据索引集 $S$ 的节点，$\\mathrm{SSE}(S) = \\sum_{i \\in S} (r_i - \\bar{r}_S)^2$，其中 $\\bar{r}_S$ 是该节点中的平均残差。\n    - **不纯度下降**：$\\Delta(j,\\tau) = \\mathrm{SSE}(\\text{parent}) - (\\mathrm{SSE}(\\text{left}) + \\mathrm{SSE}(\\text{right}))$。\n    - **分裂阈值**：连续排序的唯一特征值之间的中点。\n- **数据生成**：\n    - $n$ 个样本，$r$ 个相关特征 $\\{x_j\\}_{j=1}^r$，$p$ 个无关特征 $\\{z_k\\}_{k=1}^p$。\n    - $x_j \\sim \\mathcal{N}(0, 1)$ 和 $z_k \\sim \\mathcal{N}(0, 1)$，所有特征相互独立。\n    - 目标值：$y = 2 \\cdot \\mathbb{I}[x_1  0] + 1.5 \\cdot \\mathrm{sign}(x_2) + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, 0.5^2)$，$r=2$，且 $\\mathrm{sign}(0)=0$。\n- **任务**：对于一组测试用例，实现 GBM 并计算两个指标：\n    1. `usage_fraction`：分裂发生在无关特征上的迭代比例。\n    2. `impurity_share_irrelevant`：由无关特征分裂产生的累积不纯度下降与总累积不纯度下降之比。\n- **测试用例**：\n    - A: $n=300, r=2, p=0, m=20, \\nu=0.1$, seed=1。\n    - B: $n=300, r=2, p=10, m=20, \\nu=0.1$, seed=2。\n    - C: $n=300, r=2, p=10, m=20, \\nu=0.01$, seed=3。\n    - D: $n=300, r=2, p=10, m=1, \\nu=0.1$, seed=4。\n    - E: $n=300, r=2, p=50, m=30, \\nu=0.1$, seed=5。\n\n### 步骤2：使用提取的给定信息进行验证\n- **科学依据**：该问题描述了标准的最小二乘梯度提升算法 (LS-Boost)。残差作为负梯度、弱学习器、SSE不纯度以及通过学习率进行正则化等概念是统计学习理论的基础。该设置在科学上是合理的。\n- **问题定义明确**：该问题定义明确。数据生成过程、算法步骤、参数和期望输出都被明确指定。对于每个给定的随机种子，测试用例存在唯一的确定性解。一个小小的疏漏是初始模型 $F_0(x)$，但对于最小二乘回归的标准惯例是以最小化 SSE 的常数模型进行初始化，即目标变量的均值 $F_0(x) = \\bar{y}$。这是一个标准且无争议的假设。\n- **客观性**：问题以精确、客观的数学和算法语言陈述，没有主观论断。\n\n### 步骤3：结论与行动\n问题是**有效的**。它是一个在机器学习领域中定义明确的计算任务，需要忠实地实现一个标准算法。现在将继续进行求解过程。\n\n### 解决方案\n该问题要求实现一个带有决策树桩作为弱学习器的梯度提升机 (GBM)，以分析该算法在存在无关特征时的行为。我们将构建一个遵循指定数据生成过程和 GBM 算法的模拟。\n\n**1. 数据生成**\n对于每个具有参数 $n, r, p$ 和随机种子的测试用例，我们按如下方式生成数据：\n- 共创建 $r+p$ 个特征。前 $r=2$ 个是相关的，随后的 $p$ 个是无关的。\n- 大小为 $n \\times (r+p)$ 的特征矩阵 $\\mathbf{X}$ 的值从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取。\n- 大小为 $n$ 的目标向量 $\\mathbf{y}$ 根据以下规则生成：\n$$y_i = 2 \\cdot \\mathbb{I}[x_{i,1}  0] + 1.5 \\cdot \\mathrm{sign}(x_{i,2}) + \\epsilon_i$$\n其中 $x_{i,1}$ 和 $x_{i,2}$ 是第 $i$ 个样本的两个相关特征，$\\mathbb{I}[\\cdot]$ 是指示函数，$\\epsilon_i \\sim \\mathcal{N}(0, 0.5^2)$ 是一个噪声项。\n\n**2. 梯度提升算法**\nGBM 是迭代构建的。\n\n**初始化**：必须选择初始模型 $F_0(\\mathbf{x})$。对于最小二乘损失函数，最优常数预测是观测目标值的均值。因此，我们为所有样本 $i=1, \\dots, n$ 初始化模型为：\n$$F_0(\\mathbf{x}_i) = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$$\n\n**迭代**：模型更新 $m$ 次迭代。对于每次迭代 $k=1, \\dots, m$：\n\n**a. 计算残差**：SSE 损失函数关于模型预测 $F_{k-1}(\\mathbf{x}_i)$ 的负梯度是残差：\n$$r_i^{(k)} = y_i - F_{k-1}(\\mathbf{x}_i)$$\n\n**b. 拟合一个决策树桩**：训练一个决策树桩 $h_k(\\mathbf{x})$ 来预测残差 $r_i^{(k)}$。这涉及在所有特征和所有可能阈值中找到最佳二元分裂。分裂由一个特征索引 $j$ 和一个阈值 $\\tau$ 定义。分裂的质量由不纯度下降度量，对于SSE来说即为方差减少。\n\n包含残差索引集 $S$ 的节点的不纯度是 $\\mathrm{SSE}(S) = \\sum_{i \\in S} (r_i - \\bar{r}_S)^2$。将父节点 $S_P$ 分裂为左节点 $S_L$ 和右节点 $S_R$ 的不纯度下降是 $\\Delta = \\mathrm{SSE}(S_P) - (\\mathrm{SSE}(S_L) + \\mathrm{SSE}(S_R))$。这可以更高效地计算。设 $T_S = \\sum_{i \\in S} r_i$ 且 $N_S = |S|$。SSE可以写作 $\\mathrm{SSE}(S) = \\sum_{i \\in S} r_i^2 - T_S^2/N_S$。不纯度下降简化为：\n$$\\Delta = \\left( \\frac{T_L^2}{N_L} + \\frac{T_R^2}{N_R} \\right) - \\frac{T_P^2}{N_P}$$\n在我们的情况中，每次分裂的父节点都包含所有 $n$ 个样本。我们搜索使这个量 $\\Delta^\\star$ 最大化的特征 $j^\\star$ 和阈值 $\\tau^\\star$。\n\n寻找最优分裂 $(j^\\star, \\tau^\\star)$ 的过程是通过遍历每个特征 $j \\in \\{1, \\dots, r+p\\}$ 来执行的。对于每个特征，我们考虑其连续唯一排序值的中点作为阈值。一种寻找特征最佳阈值的高效方法包括将特征值与相应的残差一起排序，并使用累加和来计算每个潜在分裂点的 $T_L, T_R, N_L, N_R$。\n\n**c. 更新模型**：一旦找到最佳分裂 $(j^\\star, \\tau^\\star)$，决策树桩 $h_k(\\mathbf{x})$ 就被定义了。它的预测是两个叶节点中的常数值，等于每个叶节点中残差的均值：\n$$h_k(\\mathbf{x}) = \\begin{cases} \\bar{r}_{L}  \\text{if } x_j \\le \\tau^\\star \\\\ \\bar{r}_{R}  \\text{if } x_j  \\tau^\\star \\end{cases}$$\n然后，集成模型以步长 $\\nu$ 进行更新：\n$$F_k(\\mathbf{x}) = F_{k-1}(\\mathbf{x}) + \\nu \\cdot h_k(\\mathbf{x})$$\n\n**3. 性能指标**\n在 $m$ 次迭代中，我们跟踪所选的分裂特征 $j^\\star$ 和相应的不纯度下降 $\\Delta^\\star$。\n- 我们计算无关特征被选择的次数，$N_{\\text{irrelevant}} = \\sum_{k=1}^m \\mathbb{I}[j_k^\\star  r]$。\n- 我们累加总不纯度下降，$\\Delta_{\\text{total}} = \\sum_{k=1}^m \\Delta_k^\\star$。\n- 我们累加来自无关分裂的不纯度下降，$\\Delta_{\\text{irrelevant}} = \\sum_{k=1}^m \\Delta_k^\\star \\cdot \\mathbb{I}[j_k^\\star  r]$。\n\n最后，我们计算所需的指标：\n- $\\text{usage\\_fraction} = N_{\\text{irrelevant}} / m$\n- $\\text{impurity\\_share\\_irrelevant} = \\Delta_{\\text{irrelevant}} / \\Delta_{\\text{total}}$（约定如果 $\\Delta_{\\text{total}} = 0$，则此值为 $0$）。\n\n该实现将处理每个测试用例及其特定参数和随机种子，得出两个指定的浮点值。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Gradient Boosting Machine simulation problem.\n    \"\"\"\n    test_cases = [\n        # (n, r, p, m, nu, seed)\n        (300, 2, 0, 20, 0.1, 1),   # Case A: baseline\n        (300, 2, 10, 20, 0.1, 2),  # Case B: moderate irrelevant, moderate nu\n        (300, 2, 10, 20, 0.01, 3), # Case C: moderate irrelevant, small nu\n        (300, 2, 10, 1, 0.1, 4),   # Case D: edge case, single iteration\n        (300, 2, 50, 30, 0.1, 5),  # Case E: many irrelevant, more iterations\n    ]\n\n    results = []\n    for case in test_cases:\n        n, r, p, m, nu, seed = case\n        \n        # Set random seed for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Data Generation\n        num_features = r + p\n        X = rng.normal(size=(n, num_features))\n\n        # Target variable generation based on the first r=2 features\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        epsilon = rng.normal(loc=0, scale=0.5, size=n)\n        y = 2 * (x1 > 0) + 1.5 * np.sign(x2) + epsilon\n\n        # 2. GBM Implementation\n        # Initialization\n        F = np.full(n, np.mean(y))\n        \n        irrelevant_split_count = 0\n        total_impurity_decrease = 0.0\n        irrelevant_impurity_decrease = 0.0\n\n        for _ in range(m):\n            # Compute residuals\n            residuals = y - F\n            \n            # Find the best split\n            best_gain = -1.0\n            best_split_info = None\n\n            sum_residuals_parent = np.sum(residuals)\n            # This term is constant for the current iteration across all splits\n            parent_impurity_term = (sum_residuals_parent**2) / n\n\n            for j in range(num_features):\n                feature_values = X[:, j]\n                \n                sorted_indices = np.argsort(feature_values)\n                sorted_residuals = residuals[sorted_indices]\n                sorted_feature_values = feature_values[sorted_indices]\n                \n                running_sum_left = 0.0\n                \n                for i in range(n - 1):\n                    n_left = i + 1\n                    running_sum_left += sorted_residuals[i]\n                    \n                    if sorted_feature_values[i]  sorted_feature_values[i+1]:\n                        n_right = n - n_left\n                        \n                        sum_residuals_left = running_sum_left\n                        sum_residuals_right = sum_residuals_parent - sum_residuals_left\n                        \n                        gain = (sum_residuals_left**2) / n_left + \\\n                               (sum_residuals_right**2) / n_right - \\\n                               parent_impurity_term\n                        \n                        if gain > best_gain:\n                            best_gain = gain\n                            threshold = (sorted_feature_values[i] + sorted_feature_values[i+1]) / 2.0\n                            best_split_info = {\n                                \"feature_idx\": j,\n                                \"threshold\": threshold,\n                                \"gain\": gain,\n                                \"mean_res_left\": sum_residuals_left / n_left,\n                                \"mean_res_right\": sum_residuals_right / n_right,\n                            }\n            \n            if best_split_info is None:\n                break # No valid split found\n\n            # Update tracking variables\n            total_impurity_decrease += best_split_info[\"gain\"]\n            if best_split_info[\"feature_idx\"] >= r:\n                irrelevant_split_count += 1\n                irrelevant_impurity_decrease += best_split_info[\"gain\"]\n                \n            # Update the model F\n            feature_idx = best_split_info[\"feature_idx\"]\n            threshold = best_split_info[\"threshold\"]\n            \n            left_mask = X[:, feature_idx] = threshold\n            \n            F[left_mask] += nu * best_split_info[\"mean_res_left\"]\n            F[~left_mask] += nu * best_split_info[\"mean_res_right\"]\n\n        # 3. Calculate final metrics\n        usage_fraction = irrelevant_split_count / m if m > 0 else 0.0\n        \n        if total_impurity_decrease > 1e-9: # Use a small tolerance for floating point\n            impurity_share_irrelevant = irrelevant_impurity_decrease / total_impurity_decrease\n        else:\n            impurity_share_irrelevant = 0.0\n            \n        results.append([usage_fraction, impurity_share_irrelevant])\n\n    # Format the final output string exactly as required\n    result_strings = [f\"[{u},{s}]\" for u, s in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        }
    ]
}