{
    "hands_on_practices": [
        {
            "introduction": "To truly master AdaBoost, it is essential to move from theoretical formulas to practical implementation and observe its behavior. This exercise challenges you to build the algorithm from its foundational principles and apply it to a dataset specifically designed to highlight its dynamic nature. By implementing the weight updates derived from exponential loss minimization, you will gain firsthand insight into how AdaBoost can sometimes exhibit oscillatory behavior, where the learning focus shifts back and forth between different subsets of the data, and how a simple technique like shrinkage can effectively dampen these oscillations for a more stable training process .",
            "id": "3095513",
            "problem": "You are asked to implement and analyze a version of Adaptive Boosting (AdaBoost) on a deliberately constructed small dataset that induces oscillations in the weighted error sequence. Your program must be a complete, runnable implementation that follows the specifications below and outputs a single line in the requested format.\n\nContext and foundational base:\n- Work within binary classification with labels in $\\{-1,+1\\}$.\n- At boosting round $t$, there is a weight distribution $D_t$ over $N$ training examples.\n- A weak learner $h_t$ maps inputs to $\\{-1,+1\\}$ and has weighted classification error $\\varepsilon_t = \\sum_{i=1}^N D_t(i)\\,\\mathbb{1}\\{y_i \\neq h_t(x_i)\\}$.\n- The round selects a weak learner $h_t$ from a fixed dictionary that minimizes the weighted error. The per-round weight update must be derived by minimizing the round-wise exponential loss $\\sum_{i=1}^N D_t(i)\\,\\exp(-\\alpha_t y_i h_t(x_i))$ with respect to the coefficient $\\alpha_t$; this minimization defines the normalization factor $Z_t$ and the update to $D_{t+1}$ after normalization. You must also consider a shrinkage parameter $\\nu \\in (0,1]$ that scales the chosen $\\alpha_t$ by a factor $\\nu$ to damp oscillations. No other heuristics or modifications are allowed.\n\nDataset and weak learner dictionary:\n- Use exactly the following dataset of $N=12$ points in $\\mathbb{R}^2$ with labels $y \\in \\{-1,+1\\}$:\n  - Four copies of $(x_1,x_2)=(+1,-1)$ with $y=-1$.\n  - Four copies of $(x_1,x_2)=(-1,+1)$ with $y=-1$.\n  - Two copies of $(x_1,x_2)=(+1,+1)$ with $y=+1$.\n  - Two copies of $(x_1,x_2)=(-1,-1)$ with $y=-1$.\n- The weak learner dictionary consists of exactly two fixed stumps:\n  - $h^{(1)}(x) = \\mathrm{sign}(x_1)$.\n  - $h^{(2)}(x) = \\mathrm{sign}(x_2)$.\n- At each boosting round $t$, choose the weak learner with smaller weighted error $\\varepsilon_t$. In case of a tie, choose the one with smaller index (i.e., choose $h^{(1)}$).\n\nProgram requirements:\n- Implement AdaBoost for $T$ rounds with shrinkage $\\nu \\in (0,1]$, starting from the uniform distribution $D_1(i)=1/N$.\n- At each round $t$:\n  - Compute the weighted errors for both weak learners and select $h_t$.\n  - Determine the coefficient $\\alpha_t$ by minimizing the round-wise exponential loss given the selected $h_t$. Then apply shrinkage to obtain $\\nu\\alpha_t$ as the actual update magnitude.\n  - Update the distribution $D_{t+1}$ via the exponential update with the shrunk coefficient and normalize it by the factor $Z_t$ implied by that update.\n  - Accumulate the scoring function $F_t(x) = \\sum_{s=1}^t \\nu\\alpha_s h_s(x)$ to track margins.\n- After $T$ rounds, compute the following quantities:\n  - The product $\\prod_{t=1}^T Z_t$.\n  - The minimum margin $\\min_{i \\in \\{1,\\dots,N\\}} y_i F_T(x_i)$.\n  - The number of sign changes in the sequence of first differences of $(\\varepsilon_t)_{t=1}^T$, i.e., in $\\Delta_t = \\varepsilon_{t+1} - \\varepsilon_t$ for $t=1,\\dots,T-1$, ignoring any $\\Delta_t=0$ when counting sign changes.\n  - The maximum oscillation amplitude $\\max_{t=1,\\dots,T-1} |\\varepsilon_{t+1} - \\varepsilon_t|$.\n\nTest suite:\n- Your program must run the algorithm for the following five parameter settings, each treated as an independent test case:\n  1. $T=12$, $\\nu=1.0$.\n  2. $T=12$, $\\nu=0.5$.\n  3. $T=12$, $\\nu=0.2$.\n  4. $T=1$, $\\nu=1.0$.\n  5. $T=30$, $\\nu=0.05$.\n- For each test case, compute a result list with four entries in the following order: $[\\prod_{t=1}^T Z_t,\\ \\min_i y_i F_T(x_i),\\ \\text{sign-changes},\\ \\text{max-oscillation-amplitude}]$.\n\nNumerical and formatting requirements:\n- All floating-point outputs must be rounded to exactly $6$ decimal places using standard rounding to the nearest representable value at that precision.\n- All integers must be exact integers.\n- Your program should produce a single line of output containing the results as a comma-separated list of the five per-test-case lists enclosed in square brackets, for example:\n  - Output format: $[[r_{11},r_{12},r_{13},r_{14}],[r_{21},r_{22},r_{23},r_{24}],\\dots,[r_{51},r_{52},r_{53},r_{54}]]$\nwhere $r_{jk}$ denotes the $k$-th result for test case $j$.\n\nScientific realism and derivation expectations:\n- You must start from the fundamental definitions of weighted error, exponential loss, and normalization $Z_t$, and from the rule to choose the coefficient that minimizes the round-wise exponential loss. Do not introduce extraneous formulas or shortcuts in the problem statement.\n- The dataset and weak-learner dictionary are deliberately chosen so that the weighted error $\\varepsilon_t$ oscillates across rounds as different portions of the data receive increased emphasis. Your implementation and outputs will quantify how these oscillations influence $Z_t$ and margin growth, and how shrinkage (controlled by $\\nu$) damps the oscillations and changes the outcomes.\n\nNo user input:\n- The code must be fully self-contained and must not read from input or files.",
            "solution": "The problem presented is valid. It is scientifically grounded in the principles of statistical learning, specifically the AdaBoost algorithm, and is well-posed, with a complete and consistent set of definitions, data, and objectives. The task is to implement a variant of AdaBoost with a shrinkage parameter, apply it to a specified dataset, and analyze its behavior, particularly the oscillation of the weighted error over boosting rounds.\n\n### Theoretical Foundation of the AdaBoost Update\n\nThe core of the AdaBoost algorithm is the iterative re-weighting of training examples. At each round $t$, we seek a weak learner $h_t$ and a corresponding coefficient $\\alpha_t$ that minimize the exponential loss function on the current weighted distribution of data, $D_t$.\n\nThe round-wise exponential loss is given by:\n$$L(\\alpha_t) = \\sum_{i=1}^N D_t(i)\\,\\exp(-\\alpha_t y_i h_t(x_i))$$\nwhere $y_i \\in \\{-1, +1\\}$ are the true labels and $h_t(x_i) \\in \\{-1, +1\\}$ are the predictions of the selected weak learner. The term $y_i h_t(x_i)$ is $+1$ for a correct classification and $-1$ for an incorrect one.\n\nTo find the optimal $\\alpha_t$, we differentiate $L(\\alpha_t)$ with respect to $\\alpha_t$ and set the result to zero:\n$$\\frac{\\partial L}{\\partial \\alpha_t} = \\sum_{i=1}^N D_t(i)\\,\\exp(-\\alpha_t y_i h_t(x_i))(-y_i h_t(x_i)) = 0$$\nWe can partition the sum over correctly and incorrectly classified examples. Let $\\varepsilon_t = \\sum_{i=1}^N D_t(i)\\,\\mathbb{1}\\{y_i \\neq h_t(x_i)\\}$ be the weighted error of $h_t$. The sum of weights for correctly classified examples is $1-\\varepsilon_t$. The equation becomes:\n$$-\\sum_{i: y_i=h_t(x_i)} D_t(i)\\,e^{-\\alpha_t} + \\sum_{i: y_i\\neq h_t(x_i)} D_t(i)\\,e^{\\alpha_t} = 0$$\n$$-(1-\\varepsilon_t)e^{-\\alpha_t} + \\varepsilon_t e^{\\alpha_t} = 0$$\n$$\\varepsilon_t e^{\\alpha_t} = (1-\\varepsilon_t) e^{-\\alpha_t}$$\n$$e^{2\\alpha_t} = \\frac{1-\\varepsilon_t}{\\varepsilon_t}$$\nThis yields the well-known formula for the coefficient $\\alpha_t$, provided $\\varepsilon_t \\in (0, 1)$:\n$$\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right)$$\nThe problem introduces a shrinkage parameter $\\nu \\in (0, 1]$, which damps the update. The effective coefficient for the update is thus $\\nu\\alpha_t$.\n\nThe weights for the next round, $D_{t+1}$, are updated by decreasing the weights of correctly classified examples and increasing the weights of incorrectly classified ones. The unnormalized weights are:\n$$D'_{t+1}(i) = D_t(i) \\exp(-\\nu\\alpha_t y_i h_t(x_i))$$\nTo ensure $D_{t+1}$ is a valid probability distribution, we normalize by a factor $Z_t$, which is the sum of the unnormalized weights:\n$$Z_t = \\sum_{i=1}^N D'_{t+1}(i) = \\sum_{i=1}^N D_t(i) \\exp(-\\nu\\alpha_t y_i h_t(x_i))$$\nTherefore, the final weight update rule is:\n$$D_{t+1}(i) = \\frac{D_t(i) \\exp(-\\nu\\alpha_t y_i h_t(x_i))}{Z_t}$$\n\n### Algorithmic Procedure\n\nThe implementation proceeds as follows:\n\n1.  **Data Representation**: The dataset consists of $N=12$ points, but only $4$ unique $(x, y)$ pairs. To a-chieve efficiency, we operate on these unique points, associating a count with each:\n    -   Type A: $(+1, -1), y=-1$, count: $4$\n    -   Type B: $(-1, +1), y=-1$, count: $4$\n    -   Type C: $(+1, +1), y=+1$, count: $2$\n    -   Type D: $(-1, -1), y=-1$, count: $2$\n    The total initial weight for each unique point type is its count divided by $N=12$.\n2.  **Weak Learners**: The two weak learners are $h^{(1)}(x) = \\mathrm{sign}(x_1)$ and $h^{(2)}(x) = \\mathrm{sign}(x_2)$. Their predictions and correctness are pre-calculated for each unique data point.\n    -   $h^{(1)}$ misclassifies Type A points.\n    -   $h^{(2)}$ misclassifies Type B points.\n3.  **Initialization**: Start with a uniform weight distribution over the $N=12$ points, which translates to weights of $4/12, 4/12, 2/12, 2/12$ for the unique point types A, B, C, D, respectively.\n4.  **Boosting Iterations**: For each round $t$ from $1$ to $T$:\n    a.  Calculate the weighted error for each weak learner: $\\varepsilon_t^{(j)} = \\sum_{k \\in \\{A,B,C,D\\}} w_t(k) \\cdot \\mathbb{1}\\{y_k \\neq h^{(j)}(x_k)\\}$. Here, $w_t(k)$ is the total weight for point type $k$.\n    b.  Select the learner $h_t$ with the minimum weighted error, $\\varepsilon_t$. The tie-breaking rule dictates choosing $h^{(1)}$ if $\\varepsilon_t^{(1)} = \\varepsilon_t^{(2)}$.\n    c.  Calculate $\\alpha_t = \\frac{1}{2} \\ln((1-\\varepsilon_t)/\\varepsilon_t)$. For numerical stability, $\\varepsilon_t$ is clipped to be strictly between $0$ and $1$.\n    d.  Calculate the normalization factor $Z_t$ using the shrunk coefficient $\\nu\\alpha_t$.\n    e.  Update the weights $w_{t+1}$ for each unique point type according to the derived update rule.\n    f.  Store $\\varepsilon_t$, $Z_t$, and the chosen shrunk coefficient $\\nu\\alpha_t$ and learner $h_t$ for post-processing.\n5.  **Final Computations**: After $T$ rounds:\n    a.  **Product of $Z_t$**: Compute $\\Pi_{t=1}^T Z_t$.\n    b.  **Minimum Margin**: The final classifier is $H(x) = \\mathrm{sign}(F_T(x))$, where $F_T(x) = \\sum_{t=1}^T \\nu\\alpha_t h_t(x)$. The margin for a point $x_i$ is $y_i F_T(x_i)$. We find the minimum margin over all unique data points.\n    c.  **Error Oscillation Analysis**: The sequence of differences $\\Delta_t = \\varepsilon_{t+1} - \\varepsilon_t$ for $t=1, \\dots, T-1$ is computed.\n        i.  The number of sign changes is found by counting switches between positive and negative values in the sequence of $\\Delta_t$, ignoring any $\\Delta_t=0$.\n        ii. The maximum oscillation amplitude is $\\max_{t=1, \\dots, T-1} |\\Delta_t|$.\n    For $T \\le 1$, the oscillation metrics are defined as $0$.\n\nThis structured procedure is applied to each of the five test cases defined in the problem statement. All floating-point results are rounded to six decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the AdaBoost simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (T, nu)\n        (12, 1.0),\n        (12, 0.5),\n        (12, 0.2),\n        (1, 1.0),\n        (30, 0.05),\n    ]\n\n    results = []\n    for T, nu in test_cases:\n        result = run_adaboost(T, nu)\n        results.append(result)\n\n    # Format the final output string as a list of lists.\n    # The str() of a list is its string representation, e.g., '[1, 2, 3]'.\n    # We join these string representations with commas and enclose in brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\ndef run_adaboost(T, nu):\n    \"\"\"\n    Implements the specified AdaBoost algorithm for T rounds with shrinkage nu.\n    \n    Args:\n        T (int): The number of boosting rounds.\n        nu (float): The shrinkage parameter.\n\n    Returns:\n        list: A list containing the four required metrics:\n              [product of Z_t, min margin, sign changes, max oscillation amplitude].\n    \"\"\"\n    # Dataset definition (compact form for unique points)\n    # Types: A, B, C, D\n    # x_unique = np.array([[+1, -1], [-1, +1], [+1, +1], [-1, -1]])\n    y_unique = np.array([-1, -1, 1, -1])\n    counts = np.array([4, 4, 2, 2])\n    N = counts.sum()\n\n    # Weak learners' predictions on unique points\n    # h1(x) = sign(x1), h2(x) = sign(x2)\n    h1_preds = np.array([1, -1, 1, -1])\n    h2_preds = np.array([-1, 1, 1, -1])\n\n    # Pre-calculate y_i * h(x_i) for each learner\n    y_h1 = y_unique * h1_preds  # Result: [-1, 1, 1, 1] - h1 misclassifies type A\n    y_h2 = y_unique * h2_preds  # Result: [1, -1, 1, 1] - h2 misclassifies type B\n\n    # Initialize weights based on counts\n    weights = counts / N\n\n    # Storage for per-round quantities\n    all_Z_t = []\n    all_err_t = []\n    \n    # Store coefficients and chosen learners to compute final score function F_T\n    alphas_nu = []\n    hs_preds = []\n\n    # Epsilon for numerical stability to avoid log(0) or division by zero\n    epsilon = 1e-15\n\n    for _ in range(T):\n        # Calculate weighted errors for each learner\n        err1 = np.sum(weights[y_h1 == -1])\n        err2 = np.sum(weights[y_h2 == -1])\n\n        # Select learner based on minimum error (with tie-breaking)\n        if err1 = err2:\n            err_t = err1\n            y_h_t = y_h1\n            hs_preds.append(h1_preds)\n        else:\n            err_t = err2\n            y_h_t = y_h2\n            hs_preds.append(h2_preds)\n\n        # Clip error to avoid numerical issues\n        err_t = np.clip(err_t, epsilon, 1.0 - epsilon)\n        \n        # Calculate alpha\n        alpha_t = 0.5 * np.log((1 - err_t) / err_t)\n        \n        # Store shrunk alpha\n        alphas_nu.append(nu * alpha_t)\n\n        # Update weights\n        w_update_exp = np.exp(-nu * alpha_t * y_h_t)\n        Z_t = np.sum(weights * w_update_exp)\n        weights = weights * w_update_exp / Z_t\n\n        # Store round results\n        all_Z_t.append(Z_t)\n        all_err_t.append(err_t)\n\n    # 1. Product of Z_t\n    prod_Z = np.prod(all_Z_t)\n\n    # 2. Minimum margin\n    F_T = np.zeros(len(y_unique))\n    for i in range(T):\n        F_T += alphas_nu[i] * hs_preds[i]\n    margins = y_unique * F_T\n    min_margin = np.min(margins)\n\n    # For T = 1, oscillation metrics are 0\n    if T = 1:\n        sign_changes = 0\n        max_osc_amp = 0.0\n    else:\n        # 3. Number of sign changes in error differences\n        err_diffs = np.diff(all_err_t)\n        # Filter out differences that are effectively zero\n        nonzero_diffs = err_diffs[np.abs(err_diffs)  epsilon]\n        \n        if len(nonzero_diffs)  2:\n            sign_changes = 0\n        else:\n            signs = np.sign(nonzero_diffs)\n            sign_changes = np.sum(np.diff(signs) != 0)\n        \n        # 4. Maximum oscillation amplitude\n        max_osc_amp = np.max(np.abs(err_diffs))\n\n    # Round final results to 6 decimal places, keeping integers as ints\n    return [\n        round(prod_Z, 6),\n        round(min_margin, 6),\n        int(sign_changes),\n        round(max_osc_amp, 6)\n    ]\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A key feature of AdaBoost is its relentless focus on misclassified examples, which it achieves by exponentially increasing their weights. While this is the source of its power, it can also be a vulnerability. This hands-on simulation explores how a single, persistent outlier or an adversarial example can cause its corresponding weight to grow uncontrollably, potentially dominating the entire model-fitting process. You will quantify this weight amplification and test a direct defense mechanism known as weight clipping, providing a concrete understanding of how to build more robust boosting models .",
            "id": "3095556",
            "problem": "Adaptive Boosting (AdaBoost) for binary classification maintains a distribution of example weights over rounds to focus subsequent weak learners on hard examples. Consider examples with labels $y_i \\in \\{-1, +1\\}$ and weak hypotheses $h_t(x_i) \\in \\{-1, +1\\}$ at round $t$. The algorithm starts with uniform weights $w_1(i) = 1/N$ for $N$ examples. At each round, the weighted error is defined as the sum of weights on misclassified examples, namely $\\varepsilon_t = \\sum_{i=1}^N w_t(i) \\,\\mathbf{1}\\{y_i \\neq h_t(x_i)\\}$. A weak learner’s contribution is scaled by a coefficient $\\alpha_t$, and the distribution is updated by the exponential rule\n$$\nw_{t+1}(i) \\propto w_t(i)\\,\\exp\\!\\big(-\\alpha_t\\,y_i\\,h_t(x_i)\\big),\n$$\nfollowed by normalization so that $\\sum_{i=1}^N w_{t+1}(i) = 1$. When an example is adversarially perturbed such that $h_t(x)$ flips sign across many rounds, the multiplicative factors amplify its weight repeatedly. This problem asks you to quantify that amplification and evaluate a defense that trims extreme loss contributions by clipping weights before normalization.\n\nStarting from the core definitions above, implement a simulator that, given a fixed dataset and predetermined weak hypothesis outputs per round, computes the growth of a targeted example’s weight under two regimes:\n- the standard AdaBoost update; and\n- a trimmed-loss defense where the pre-normalization candidate weights $\\tilde{w}_{t+1}(i) = w_t(i)\\,\\exp\\!\\big(-\\alpha_t\\,y_i\\,h_t(x_i)\\big)$ are clipped by $\\tilde{w}_{t+1}^{\\mathrm{clip}}(i) = \\min\\{\\tilde{w}_{t+1}(i), \\Lambda\\}$ for a fixed cap $\\Lambda \\in (0,1)$, then renormalized to sum to $1$.\n\nYour program must:\n- Use the dataset of $N=4$ examples with labels $y = [+1, +1, -1, -1]$.\n- For each test case below, simulate $T$ rounds, compute $\\varepsilon_t$ from the current weights, set $\\alpha_t = \\tfrac{1}{2}\\ln\\!\\big(\\tfrac{1-\\varepsilon_t}{\\varepsilon_t}\\big)$, and update weights as described.\n- Produce, for a targeted index $s$, the growth factor $g_{\\mathrm{std}} = \\dfrac{w_{T+1}(s)}{w_1(s)}$ under the standard update and $g_{\\mathrm{clip}} = \\dfrac{w^{\\mathrm{clip}}_{T+1}(s)}{w_1(s)}$ under the clipped update (where $w_1(s) = 1/N$). If $\\varepsilon_t$ ever equals $0$ or $1$ numerically, treat it as $\\varepsilon_t = 10^{-12}$ or $\\varepsilon_t = 1 - 10^{-12}$ respectively to maintain numerical stability.\n\nTest suite specifications (each case provides $T$, the per-round hypothesis outputs $h_t(x_i)$ for all $i$, the clip cap $\\Lambda$, and the targeted index $s$):\n\n- Case $1$ (happy path, occasional adversarial flip): $T=5$, $s=0$, $\\Lambda=0.2$, rounds\n  - $t=1$: $[+1,+1,-1,+1]$\n  - $t=2$: $[+1,-1,-1,-1]$\n  - $t=3$: $[-1,+1,-1,-1]$\n  - $t=4$: $[+1,+1,+1,-1]$\n  - $t=5$: $[+1,+1,-1,+1]$\n- Case $2$ (heavy adversarial flips on the target in early rounds): $T=6$, $s=0$, $\\Lambda=0.2$, rounds\n  - $t=1$: $[-1,+1,-1,-1]$\n  - $t=2$: $[-1,+1,-1,-1]$\n  - $t=3$: $[-1,+1,-1,-1]$\n  - $t=4$: $[-1,+1,-1,-1]$\n  - $t=5$: $[+1,+1,-1,+1]$\n  - $t=6$: $[+1,-1,-1,-1]$\n- Case $3$ (target remains correct; others occasionally wrong): $T=4$, $s=0$, $\\Lambda=0.2$, rounds\n  - $t=1$: $[+1,+1,-1,+1]$\n  - $t=2$: $[+1,-1,-1,-1]$\n  - $t=3$: $[+1,+1,+1,-1]$\n  - $t=4$: $[+1,+1,-1,+1]$\n- Case $4$ (near-boundary weighted error with two mistakes each round): $T=6$, $s=0$, $\\Lambda=0.2$, rounds\n  - $t=1$: $[-1,+1,+1,-1]$\n  - $t=2$: $[+1,-1,+1,-1]$\n  - $t=3$: $[-1,+1,-1,+1]$\n  - $t=4$: $[+1,-1,-1,+1]$\n  - $t=5$: $[-1,+1,+1,-1]$\n  - $t=6$: $[+1,-1,+1,-1]$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[g_{\\mathrm{std},1},g_{\\mathrm{clip},1},g_{\\mathrm{std},2},g_{\\mathrm{clip},2},g_{\\mathrm{std},3},g_{\\mathrm{clip},3},g_{\\mathrm{std},4},g_{\\mathrm{clip},4}]$, with each value rounded to six decimal places. No physical units are involved. Angles are not applicable. Percentages should not be used; all quantities are to be expressed as real numbers.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of statistical learning, specifically the AdaBoost algorithm. It is well-posed, providing a complete and consistent set of definitions, data, and parameters to execute a deterministic simulation. The objective is clear and requires a non-trivial but feasible implementation.\n\nThe solution will simulate the AdaBoost algorithm over a sequence of rounds for a fixed dataset and predetermined weak learner outputs. We will compute the evolution of example weights under two distinct update rules: the standard AdaBoost update and a modified version that incorporates weight clipping as a defense mechanism against adversarial weight amplification.\n\nThe core principle of AdaBoost is to sequentially build a strong classifier by combining weak learners. It does so by maintaining a distribution of weights over the training examples. In each round, a new weak learner is trained to minimize the weighted error, effectively focusing on the examples that previous learners misclassified. The weight of each example is then updated based on whether the new weak learner classifies it correctly.\n\nFirst, we establish the initial state. The dataset has $N=4$ examples with labels $y = [+1, +1, -1, -1]$. The process starts at round $t=1$ with a uniform weight distribution, where each example $i$ has a weight $w_1(i) = 1/N = 1/4 = 0.25$. We will run two parallel simulations, one for the standard algorithm and one for the clipped variant. Both start with these same initial weights.\n\nThe simulation proceeds iteratively for $T$ rounds, from $t=1$ to $t=T$. In each round $t$, we perform the following steps for both the standard and clipped simulations, noting that the weights for each simulation, $w_t^{\\mathrm{std}}(i)$ and $w_t^{\\mathrm{clip}}(i)$, will evolve independently after the first round.\n\n1.  **Calculate Weighted Error ($\\varepsilon_t$)**: The weighted error of the current weak hypothesis $h_t$ is the sum of the weights of the misclassified examples. An example $i$ is misclassified if $y_i \\neq h_t(x_i)$, or equivalently, if the product $y_i h_t(x_i) = -1$.\n    $$\n    \\varepsilon_t = \\sum_{i=1}^N w_t(i) \\,\\mathbf{1}\\{y_i \\neq h_t(x_i)\\}\n    $$\n    This calculation is performed separately for the standard and clipped simulations using their respective current weights, yielding $\\varepsilon_t^{\\mathrm{std}}$ and $\\varepsilon_t^{\\mathrm{clip}}$. To prevent division by zero or logarithmic errors, if $\\varepsilon_t$ is numerically $0$ or $1$, it is capped at $10^{-12}$ or $1 - 10^{-12}$, respectively.\n\n2.  **Calculate Hypothesis Weight ($\\alpha_t$)**: The contribution of the weak learner $h_t$ to the final classifier is determined by its weight, $\\alpha_t$. This is calculated based on the weighted error $\\varepsilon_t$.\n    $$\n    \\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right)\n    $$\n    A learner better than random guessing will have $\\varepsilon_t  0.5$, which results in $\\alpha_t  0$. This calculation is also done separately for each simulation, producing $\\alpha_t^{\\mathrm{std}}$ and $\\alpha_t^{\\mathrm{clip}}$.\n\n3.  **Update Example Weights**: The core of the algorithm lies in updating the example weights. The update rule multiplicatively increases the weights of misclassified examples and decreases the weights of correctly classified ones. The unnormalized weight for the next round, $\\tilde{w}_{t+1}(i)$, is:\n    $$\n    \\tilde{w}_{t+1}(i) = w_t(i)\\,\\exp\\!\\big(-\\alpha_t\\,y_i\\,h_t(x_i)\\big)\n    $$\n    Since $y_i h_t(x_i)$ is $+1$ for a correct classification and $-1$ for an incorrect one, the rule simplifies to multiplying the weight by $e^{\\alpha_t}$ for a mistake and by $e^{-\\alpha_t}$ for a correct prediction.\n\n4.  **Apply Clipping and Normalize**:\n    *   **Standard Simulation**: The unnormalized weights $\\tilde{w}_{t+1}^{\\mathrm{std}}(i)$ are normalized by dividing by their sum, $Z_t^{\\mathrm{std}} = \\sum_{j=1}^N \\tilde{w}_{t+1}^{\\mathrm{std}}(j)$, to ensure they form a valid probability distribution for the next round:\n        $$\n        w_{t+1}^{\\mathrm{std}}(i) = \\frac{\\tilde{w}_{t+1}^{\\mathrm{std}}(i)}{Z_t^{\\mathrm{std}}}\n        $$\n    *   **Clipped Simulation**: The unnormalized weights $\\tilde{w}_{t+1}^{\\mathrm{clip}}(i)$ are first clipped at a maximum value $\\Lambda$. This step limits the influence of any single example.\n        $$\n        \\tilde{w}_{t+1}^{\\mathrm{post\\_clip}}(i) = \\min\\{\\tilde{w}_{t+1}^{\\mathrm{clip}}(i), \\Lambda\\}\n        $$\n        These post-clipping weights are then normalized by their sum $Z_t^{\\mathrm{clip}} = \\sum_{j=1}^N \\tilde{w}_{t+1}^{\\mathrm{post\\_clip}}(j)$:\n        $$\n        w_{t+1}^{\\mathrm{clip}}(i) = \\frac{\\tilde{w}_{t+1}^{\\mathrm{post\\_clip}}(i)}{Z_t^{\\mathrm{clip}}}\n        $$\n\nThis process is repeated for $T$ rounds. After the final round, we obtain the weight distributions $w_{T+1}^{\\mathrm{std}}$ and $w_{T+1}^{\\mathrm{clip}}$.\n\nFinally, for the targeted example with index $s$, we calculate the growth factors. The growth factor measures the amplification of the example's weight from its initial uniform value $w_1(s) = 1/N$.\n$$\ng_{\\mathrm{std}} = \\frac{w_{T+1}^{\\mathrm{std}}(s)}{w_1(s)} \\quad \\text{and} \\quad g_{\\mathrm{clip}} = \\frac{w_{T+1}^{\\mathrm{clip}}(s)}{w_1(s)}\n$$\nThese values quantify the effectiveness of the clipping defense in mitigating weight explosion for the targeted example. The implementation will carry out these steps for each test case specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates AdaBoost and a clipped-weight variant to compute weight growth factors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (5,  # T\n         [[1, 1, -1, 1], [1, -1, -1, -1], [-1, 1, -1, -1], [1, 1, 1, -1], [1, 1, -1, 1]],  # h_t\n         0.2,  # Lambda\n         0),  # s\n        # Case 2\n        (6,  # T\n         [[-1, 1, -1, -1], [-1, 1, -1, -1], [-1, 1, -1, -1], [-1, 1, -1, -1], [1, 1, -1, 1], [1, -1, -1, -1]],  # h_t\n         0.2,  # Lambda\n         0),  # s\n        # Case 3\n        (4,  # T\n         [[1, 1, -1, 1], [1, -1, -1, -1], [1, 1, 1, -1], [1, 1, -1, 1]],  # h_t\n         0.2,  # Lambda\n         0),  # s\n        # Case 4\n        (6,  # T\n         [[-1, 1, 1, -1], [1, -1, 1, -1], [-1, 1, -1, 1], [1, -1, -1, 1], [-1, 1, 1, -1], [1, -1, 1, -1]],  # h_t\n         0.2,  # Lambda\n         0)  # s\n    ]\n\n    # Dataset specification\n    y = np.array([1, 1, -1, -1])\n    N = 4\n    \n    results = []\n    \n    for case in test_cases:\n        T, h_rounds_list, Lambda, s = case\n        h_rounds = np.array(h_rounds_list)\n\n        # Initialize weights for both standard and clipped simulations\n        w_std = np.full(N, 1.0 / N)\n        w_clip = np.full(N, 1.0 / N)\n\n        for t in range(T):\n            h_t = h_rounds[t]\n\n            # --- Standard AdaBoost update ---\n            # 1. Calculate weighted error\n            is_misclassified_std = (y != h_t)\n            eps_std = np.sum(w_std[is_misclassified_std])\n            eps_std = np.clip(eps_std, 1e-12, 1.0 - 1e-12)\n            \n            # 2. Calculate hypothesis weight\n            alpha_std = 0.5 * np.log((1.0 - eps_std) / eps_std)\n            \n            # 3. Update example weights\n            exponents_std = -alpha_std * y * h_t\n            w_tilde_std = w_std * np.exp(exponents_std)\n            \n            # 4. Normalize\n            w_std = w_tilde_std / np.sum(w_tilde_std)\n\n            # --- Clipped AdaBoost update ---\n            # 1. Calculate weighted error\n            is_misclassified_clip = (y != h_t)\n            eps_clip = np.sum(w_clip[is_misclassified_clip])\n            eps_clip = np.clip(eps_clip, 1e-12, 1.0 - 1e-12)\n\n            # 2. Calculate hypothesis weight\n            alpha_clip = 0.5 * np.log((1.0 - eps_clip) / eps_clip)\n            \n            # 3. Update example weights (unnormalized)\n            exponents_clip = -alpha_clip * y * h_t\n            w_tilde_clip = w_clip * np.exp(exponents_clip)\n\n            # 4. Apply clipping and then normalize\n            w_tilde_clip_clipped = np.minimum(w_tilde_clip, Lambda)\n            sum_clipped = np.sum(w_tilde_clip_clipped)\n            if sum_clipped > 0:\n                w_clip = w_tilde_clip_clipped / sum_clipped\n            else: # Handle case where all weights are clipped to 0\n                w_clip = np.full(N, 1.0 / N)\n\n        # Calculate final growth factors\n        w1_s = 1.0 / N\n        g_std = w_std[s] / w1_s\n        g_clip = w_clip[s] / w1_s\n        \n        results.append(f\"{g_std:.6f}\")\n        results.append(f\"{g_clip:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond understanding its statistical properties, a crucial aspect of deploying any algorithm is ensuring it can operate efficiently at scale. This practice shifts our focus from the \"what\" of AdaBoost to the \"how,\" challenging you to think like a machine learning engineer facing a massive dataset. By analyzing a scalable strategy that uses feature binning and weighted histograms, you will derive the computational cost of a single boosting round and estimate the real-world runtime, gaining an appreciation for the algorithmic optimizations necessary to make powerful models like AdaBoost practical for large-scale applications .",
            "id": "3095535",
            "problem": "Consider a binary classification task with training data $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ where $y_{i} \\in \\{-1,+1\\}$ and $x_{i} \\in \\mathbb{R}^{d}$. In the Adaptive Boosting (AdaBoost) procedure, the weighted error of a weak learner $h_{t}$ at round $t$ is defined by the fundamental base\n$$\\epsilon_{t} \\triangleq \\sum_{i=1}^{n} w_{i}^{(t)} \\,\\mathbf{1}\\!\\left(y_{i} \\neq h_{t}(x_{i})\\right),$$\nwhere $w_{i}^{(t)} \\ge 0$ are sample weights that sum to $1$. A decision stump weak learner chooses a single feature and a threshold to minimize $\\epsilon_{t}$.\n\nYou are asked to analyze computational limits and propose scalable strategies to compute $\\epsilon_{t}$ for decision stumps when $n$ is large. Take $n = 10^{6}$ samples and $d = 50$ features. Assume each feature is pre-binned into $B = 256$ uniformly spaced bins over its empirical range. For each feature, you will construct weighted histograms for the positive and negative classes, scan thresholds to find the minimum weighted error, and then perform the AdaBoost weight update.\n\nUse the following realistic, architecture-agnostic primitive operation model. A primitive operation is one of: addition, subtraction, multiplication, division, comparison, branch, or array index, and counts as $1$ unit. The Central Processing Unit (CPU) executes $r = 1.0 \\times 10^{9}$ primitive operations per second.\n\nPer-feature, per-sample histogram accumulation cost: mapping a raw value to its bin index and accumulating its weight is modeled as $c_{\\mathrm{hist}} = 10$ primitive operations. Per-feature threshold scan cost: computing cumulative class-weighted sums across bins and the corresponding weighted error at each candidate threshold is modeled as $c_{\\mathrm{scan}} = 6$ primitive operations per bin. After selecting the best stump, computing predictions for all samples using the chosen feature and threshold is modeled as $c_{\\mathrm{pred}} = 8$ primitive operations per sample. The weight update uses the standard AdaBoost update rule, but assume you precompute $\\exp(\\alpha_{t})$ and $\\exp(-\\alpha_{t})$ once and reuse them, so per-sample update (selecting which constant to multiply and updating the weight) is modeled as $c_{\\mathrm{upd}} = 4$ primitive operations. Finally, renormalizing the weights to sum to $1$ requires a pass to sum weights, costing $n$ additions, and a pass to divide each weight by the sum, costing $n$ divisions.\n\nStarting from the provided definition of $\\epsilon_{t}$ and the described histogram-based decision stump training strategy, derive a closed-form expression for the total primitive operation count per AdaBoost round, and then evaluate the total runtime in seconds for the given $n$, $d$, $B$, and $r$. Clearly state any additional justifiable assumptions you make, and ensure scientific realism and self-consistency in your reasoning.\n\nExpress the final runtime in seconds and round your numeric answer to four significant figures.",
            "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n-   **Data:** A training set $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$ is given, where $y_{i} \\in \\{-1,+1\\}$ and $x_{i} \\in \\mathbb{R}^{d}$.\n-   **Algorithm:** Adaptive Boosting (AdaBoost).\n-   **Weak Learner:** Decision Stump, which minimizes the weighted error $\\epsilon_{t} \\triangleq \\sum_{i=1}^{n} w_{i}^{(t)} \\,\\mathbf{1}\\!\\left(y_{i} \\neq h_{t}(x_{i})\\right)$, where $\\sum_{i=1}^{n} w_{i}^{(t)} = 1$.\n-   **Training Strategy:** For each feature, data is pre-binned. Weighted histograms for positive ($y_i=+1$) and negative ($y_i=-1$) classes are constructed. Thresholds are scanned to find the minimum weighted error. AdaBoost weight update follows.\n-   **Parameters:**\n    -   Number of samples, $n = 10^{6}$.\n    -   Number of features, $d = 50$.\n    -   Number of bins per feature, $B = 256$.\n-   **Computational Model:**\n    -   A primitive operation is one of: addition, subtraction, multiplication, division, comparison, branch, or array index, counting as $1$ unit.\n    -   CPU execution rate, $r = 1.0 \\times 10^{9}$ primitive operations per second.\n-   **Cost Model (Primitive Operations):**\n    -   Per-feature, per-sample histogram accumulation cost: $c_{\\mathrm{hist}} = 10$.\n    -   Per-feature, per-bin threshold scan cost: $c_{\\mathrm{scan}} = 6$.\n    -   Per-sample prediction cost after stump selection: $c_{\\mathrm{pred}} = 8$.\n    -   Per-sample weight update cost (reusing precomputed exponents): $c_{\\mathrm{upd}} = 4$.\n    -   Weight renormalization cost: $n$ additions plus $n$ divisions, for a total of $2n$ operations.\n-   **Objective:**\n    1.  Derive a closed-form expression for the total primitive operation count per AdaBoost round.\n    2.  Evaluate the total runtime in seconds for the given parameters, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n-   **Scientifically Grounded:** The problem describes a standard and computationally realistic implementation of the AdaBoost algorithm using histogram-based decision stumps. This is a well-established technique in machine learning for handling large datasets. The use of a primitive operation model to analyze computational complexity is a standard method in algorithm analysis. All specified concepts are rooted in computer science and statistics.\n-   **Well-Posed:** All necessary parameters ($n, d, B, r$) and cost constants ($c_{\\mathrm{hist}}, c_{\\mathrm{scan}}, c_{\\mathrm{pred}}, c_{\\mathrm{upd}}$) are explicitly defined. The algorithmic steps to be analyzed are described sequentially and unambiguously. The task is to derive an expression and compute a value, which is a well-defined objective.\n-   **Objective:** The problem is stated using precise, quantitative language and is free of subjectivity or opinion.\n-   **Completeness and Consistency:** The problem provides a self-contained description of a computational model and an algorithm. There are no missing definitions or contradictory constraints for the stated task.\n-   **Realism and Feasibility:** The given numerical values for sample size, features, bins, and processor speed are representative of modern large-scale data analysis scenarios. The abstracted cost-per-operation values are plausible estimates.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is a well-defined problem in algorithmic analysis applied to statistical learning.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete, reasoned solution will be provided.\n\n### Solution Derivation\nThe objective is to derive a closed-form expression for the total number of primitive operations, $C_{\\mathrm{total}}$, required for a single round of the AdaBoost algorithm as described, and then to calculate the corresponding runtime, $T$.\n\nThe analysis proceeds by decomposing one round of AdaBoost into its major computational stages and summing their costs. The key assumption is that the sequence of operations described in the problem statement is followed literally.\n\n**Stage 1: Finding the Optimal Decision Stump**\nThis stage involves evaluating all $d$ features to find the single feature and threshold that constitute the best weak learner $h_t$.\n\n-   **1a. Histogram Construction:** For each of the $d$ features, weighted histograms are constructed. This requires processing all $n$ samples. The cost is given as $c_{\\mathrm{hist}}$ operations per sample, per feature.\n    $$C_{\\mathrm{hist\\_build}} = d \\cdot n \\cdot c_{\\mathrm{hist}}$$\n-   **1b. Threshold Scan:** For each of the $d$ features, an optimal threshold is found by scanning its $B$ bins. The cost is given as $c_{\\mathrm{scan}}$ operations per bin.\n    $$C_{\\mathrm{scan\\_feat}} = d \\cdot B \\cdot c_{\\mathrm{scan}}$$\n-   **1c. Best Feature Selection:** After the minimum error is found for each of the $d$ features, these $d$ error values must be compared to find the overall minimum. This requires $d-1$ comparison operations.\n    $$C_{\\mathrm{select}} = d - 1$$\n\nThe total cost for finding the best stump is the sum of these components:\n$$C_{\\mathrm{stump}} = C_{\\mathrm{hist\\_build}} + C_{\\mathrm{scan\\_feat}} + C_{\\mathrm{select}} = d \\cdot n \\cdot c_{\\mathrm{hist}} + d \\cdot B \\cdot c_{\\mathrm{scan}} + d - 1$$\n\n**Stage 2: Computing Predictions**\nOnce the best stump is identified, its predictions, $h_t(x_i)$, are computed for all $n$ samples. This is required for the subsequent weight update step. The cost is specified as $c_{\\mathrm{pred}}$ per sample.\n$$C_{\\mathrm{pred}} = n \\cdot c_{\\mathrm{pred}}$$\n\n**Stage 3: Updating and Renormalizing Weights**\nThe sample weights $w_i^{(t)}$ are updated to $w_i^{(t+1)}$.\n\n-   **3a. Per-Sample Update:** Using the pre-computed predictions from Stage $2$, each of the $n$ weights is updated. The cost is given as $c_{\\mathrm{upd}}$ per sample.\n    $$C_{\\mathrm{upd\\_sample}} = n \\cdot c_{\\mathrm{upd}}$$\n-   **3b. Renormalization:** The new weights must be normalized to sum to $1$. This is described as a pass to sum the $n$ weights ($n$ additions) and a second pass to divide each weight by the total sum ($n$ divisions). Each operation is a primitive operation.\n    $$C_{\\mathrm{renorm}} = n + n = 2n$$\n\nThe total cost for updating weights is:\n$$C_{\\mathrm{update}} = C_{\\mathrm{upd\\_sample}} + C_{\\mathrm{renorm}} = n \\cdot c_{\\mathrm{upd}} + 2n = n(c_{\\mathrm{upd}} + 2)$$\n\n**Total Operation Count per Round**\nThe total primitive operation count per round, $C_{\\mathrm{total}}$, is the sum of the costs of all stages.\n$$C_{\\mathrm{total}} = C_{\\mathrm{stump}} + C_{\\mathrm{pred}} + C_{\\mathrm{update}}$$\n$$C_{\\mathrm{total}} = (d \\cdot n \\cdot c_{\\mathrm{hist}} + d \\cdot B \\cdot c_{\\mathrm{scan}} + d - 1) + (n \\cdot c_{\\mathrm{pred}}) + (n(c_{\\mathrm{upd}} + 2))$$\nCombining and rearranging terms gives the final closed-form expression:\n$$C_{\\mathrm{total}} = n(d \\cdot c_{\\mathrm{hist}} + c_{\\mathrm{pred}} + c_{\\mathrm{upd}} + 2) + d \\cdot B \\cdot c_{\\mathrm{scan}} + d - 1$$\n\n**Numerical Evaluation**\nWe now substitute the given values into the derived expression:\n-   $n = 10^{6}$\n-   $d = 50$\n-   $B = 256$\n-   $c_{\\mathrm{hist}} = 10$\n-   $c_{\\mathrm{scan}} = 6$\n-   $c_{\\mathrm{pred}} = 8$\n-   $c_{\\mathrm{upd}} = 4$\n\nFirst, calculate the total number of primitive operations, $C_{\\mathrm{total}}$:\n$$C_{\\mathrm{total}} = 10^{6} (50 \\cdot 10 + 8 + 4 + 2) + (50 \\cdot 256 \\cdot 6) + (50 - 1)$$\n$$C_{\\mathrm{total}} = 10^{6} (500 + 14) + (12800 \\cdot 6) + 49$$\n$$C_{\\mathrm{total}} = 10^{6} (514) + 76800 + 49$$\n$$C_{\\mathrm{total}} = 514,000,000 + 76,800 + 49$$\n$$C_{\\mathrm{total}} = 514,076,849$$\n\n**Runtime Calculation**\nThe total runtime $T$ is the total operation count divided by the CPU execution rate $r = 1.0 \\times 10^{9}$ operations/second.\n$$T = \\frac{C_{\\mathrm{total}}}{r} = \\frac{514,076,849}{1.0 \\times 10^{9}} \\text{ s}$$\n$$T = 0.514076849 \\text{ s}$$\n\nRounding the result to four significant figures, as requested:\n$$T \\approx 0.5141 \\text{ s}$$",
            "answer": "$$\\boxed{0.5141}$$"
        }
    ]
}