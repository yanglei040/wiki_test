{
    "hands_on_practices": [
        {
            "introduction": "截断幂基是构建样条模型的一种直观方法，但其基函数之间通常存在高度相关性，即多重共线性。本练习将引导你通过格兰-施密特正交化来解决这一问题，并量化比较原始基与正交基下系数估计的方差，从而深刻理解基函数正交化对模型稳定性的重要性 。",
            "id": "3157128",
            "problem": "您的任务是分析在采用三次样条基的普通最小二乘（OLS）线性回归模型中的系数方差。基本设定为线性模型 $y = X \\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，$X$ 是一个设计矩阵，其列是在 $n$ 个输入点上评估的样条基函数。在满列秩和同方差性的条件下，OLS 估计量的抽样方差的公认公式为 $\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}$。您将构建一个原始三次样条截断幂基，通过 Gram–Schmidt 过程对其进行正交化以减少共线性，并比较原始基与正交规范化基的理论系数方差。\n\n定义与构造：\n- 在 $[0,1]$ 上具有内部节点 $\\{k_1,\\dots,k_m\\}$ 的三次截断幂样条基由函数 $\\phi_0(x)=1$、$\\phi_1(x)=x$、$\\phi_2(x)=x^2$、$\\phi_3(x)=x^3$ 以及对每个内部节点 $k_j$ 的一个函数 $\\phi_{3+j}(x) = \\left( x - k_j \\right)_+^3$ 组成，其中 $(u)_+ = \\max(u, 0)$。\n- 对于输入 $\\{x_i\\}_{i=1}^n$，设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的元素为 $X_{i,j} = \\phi_j(x_i)$，其中 $j=0,\\dots,3+m$，因此 $p = 4 + m$。\n- Gram–Schmidt 正交化构造一个矩阵 $Q \\in \\mathbb{R}^{n \\times p}$，其列是正交规范向量，张成与 $X$ 相同的列空间，即 $Q^\\top Q \\approx I_p$ 且 $\\operatorname{span}\\{Q_{\\cdot,1},\\dots,Q_{\\cdot,p}\\} = \\operatorname{span}\\{X_{\\cdot,1},\\dots,X_{\\cdot,p}\\}$。\n\n需要实现的任务：\n1. 对每个测试用例，在 $[0,1]$ 上创建一个由 $n$ 个输入组成的网格 $x_i = \\frac{i - 0.5}{n}$，其中 $i = 1,\\dots,n$。\n2. 使用为该用例指定的内部节点，构建原始三次截断幂基设计矩阵 $X$。\n3. 计算原始基的理论 OLS 系数方差向量，定义为 $\\sigma^2 \\cdot \\operatorname{diag}\\!\\left( (X^\\top X)^{-1} \\right)$，其中 $\\operatorname{diag}(A)$ 表示矩阵 $A$ 的对角线元素组成的向量。\n4. 使用经典的修正 Gram–Schmidt 过程对 $X$ 的列进行正交化，以获得满足 $Q^\\top Q \\approx I_p$ 的矩阵 $Q$。\n5. 计算正交规范基的理论 OLS 系数方差向量，定义为 $\\sigma^2 \\cdot \\operatorname{diag}\\!\\left( (Q^\\top Q)^{-1} \\right)$。\n6. 将每个方差值四舍五入到六位小数。\n\n测试套件：\n- 用例 A（正常路径）：$n = 200$，内部节点 $\\{0.3, 0.6\\}$，噪声方差 $\\sigma^2 = 0.5$。\n- 用例 B（边缘用例，节点间距过近导致共线性增加）：$n = 50$，内部节点 $\\{0.49, 0.5, 0.51\\}$，噪声方差 $\\sigma^2 = 1.0$。\n- 用例 C（边界用例，无内部节点；纯三次多项式基）：$n = 150$，内部节点 $\\{\\}$，噪声方差 $\\sigma^2 = 0.2$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果必须是一对列表 $[v_{\\text{raw}}, v_{\\text{orth}}]$，其中 $v_{\\text{raw}}$ 是原始基的系数方差列表，$v_{\\text{orth}}$ 是正交规范基的系数方差列表，两者均四舍五入到六位小数。例如，一个包含两个用例的输出可能看起来像 $[[[0.1,0.2],[0.1,0.1]],[[0.3,0.4],[0.3,0.3]]]$。不应打印任何额外文本。",
            "solution": "用户提供的问题被评估为有效。它在科学上基于统计线性模型和数值线性代数的原理，问题陈述清晰，提供了所有必要信息，并且完全客观。该任务是比较使用两种不同三次样条基表示的模型中普通最小二乘（OLS）系数估计量的理论方差：一种是原始截断幂基，另一种是通过修正 Gram-Schmidt 过程派生的正交规范对应基。\n\n### 理论框架\n\n该问题设定在 OLS 线性回归的背景下。模型由下式给出：\n$$\ny = X \\beta + \\varepsilon\n$$\n其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是误差向量。我们假设误差是独立同分布的，均值为 0，方差为 $\\sigma^2$，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\n$\\beta$ 的 OLS 估计量通过最小化残差平方和得到，由下式给出：\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n$$\n此估计量的抽样方差-协方差矩阵是统计推断的基石，由下式给出：\n$$\n\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}\n$$\n第 $j$ 个系数估计量 $\\hat{\\beta}_j$ 的方差是该矩阵的第 $j$ 个对角元素：\n$$\n\\mathrm{Var}(\\hat{\\beta}_j) = \\sigma^2 \\left[ (X^\\top X)^{-1} \\right]_{jj}\n$$\n问题要求计算这个方差向量 $\\sigma^2 \\cdot \\operatorname{diag}((X^\\top X)^{-1})$，针对设计矩阵 $X$ 的两种不同选择。\n\n### 分步解决流程\n\n1.  **输入数据生成**：对于每个具有 $n$ 个数据点的测试用例，我们首先使用公式 $x_i = \\frac{i - 0.5}{n}$（其中 $i=1, \\dots, n$）在区间 $[0,1]$ 上生成一个均匀的输入网格 $\\{x_i\\}_{i=1}^n$。\n\n2.  **原始基设计矩阵 ($X$) 的构造**：\n    具有 $m$ 个内部节点 $\\{k_1, \\dots, k_m\\}$ 的三次截断幂样条基由 $p = 4 + m$ 个基函数组成：\n    -   $\\phi_0(x) = 1$\n    -   $\\phi_1(x) = x$\n    -   $\\phi_2(x) = x^2$\n    -   $\\phi_3(x) = x^3$\n    -   $\\phi_{3+j}(x) = (x - k_j)_+^3 = \\max(0, x - k_j)^3$ for $j=1, \\dots, m$.\n\n    设计矩阵 $X$ 是一个 $n \\times p$ 矩阵，其中每个元素为 $X_{ij} = \\phi_{j-1}(x_i)$（列使用从 0 开始的索引，$j=0, \\dots, p-1$）。$X$ 的列通常高度相关，这种情况被称为多重共线性。这使得矩阵 $X^\\top X$ 成为病态矩阵，意味着其逆矩阵对微小扰动非常敏感，这通常表现为系数估计量具有较大的方差。\n\n3.  **原始基的方差计算**：\n    根据理论公式，系数方差向量 $v_{\\text{raw}}$ 计算如下：\n    $$\n    v_{\\text{raw}} = \\sigma^2 \\cdot \\operatorname{diag}\\left((X^\\top X)^{-1}\\right)\n    $$\n    这包括计算格拉姆矩阵 $X^\\top X$、求其逆矩阵、提取对角线元素，并乘以给定的噪声方差 $\\sigma^2$。\n\n4.  **正交规范基设计矩阵 ($Q$) 的构造**：\n    为减轻多重共线性的影响，可以对基向量（$X$ 的列）进行正交化。问题指定使用修正 Gram-Schmidt（MGS）算法。与经典对应算法相比，MGS 是从输入矩阵 $X$ 生成正交规范矩阵 $Q$ 的一种数值上更稳定的过程。$Q$ 的列构成了 $X$ 的列空间的一个正交规范基。\n    MGS 算法的流程如下，从矩阵 $V^{(0)} = X$ 开始：\n    对于 $j=1, \\dots, p$：\n    1.  标准化第 $j$ 列向量：$q_j = V^{(j-1)}_{\\cdot, j} / \\|V^{(j-1)}_{\\cdot, j}\\|_2$。\n    2.  对于所有后续列 $k = j+1, \\dots, p$，移除与 $q_j$ 平行的分量：$V^{(j)}_{\\cdot, k} = V^{(j-1)}_{\\cdot, k} - (q_j^\\top V^{(j-1)}_{\\cdot, k}) q_j$。\n    最终得到的矩阵 $Q$ 的列 $q_1, \\dots, q_p$ 在构造上是正交规范的，因此 $Q^\\top Q = I_p$。\n\n5.  **正交规范基的方差计算**：\n    当我们使用正交规范基 $Q$ 时，线性模型被重新参数化为 $y = Q \\gamma + \\varepsilon$。新系数 $\\gamma$ 的 OLS 估计量是 $\\hat{\\gamma} = (Q^\\top Q)^{-1} Q^\\top y$。相应的方差-协方差矩阵是：\n    $$\n    \\mathrm{Var}(\\hat{\\gamma}) = \\sigma^2 (Q^\\top Q)^{-1}\n    $$\n    由于 $Q$ 是正交规范的，受浮点精度限制，$Q^\\top Q \\approx I_p$（单位矩阵）。因此，其逆矩阵也约等于单位矩阵，$(Q^\\top Q)^{-1} \\approx I_p$。系数方差向量 $v_{\\text{orth}}$ 变为：\n    $$\n    v_{\\text{orth}} = \\sigma^2 \\cdot \\operatorname{diag}(I_p) = \\sigma^2 \\cdot [1, 1, \\dots, 1]^\\top = [\\sigma^2, \\sigma^2, \\dots, \\sigma^2]^\\top\n    $$\n    这展示了使用正交基的一个关键优势：系数估计量是不相关的，并且它们的方差都等于噪声方差 $\\sigma^2$，而与原始基的结构无关。这一理论结果为数值实现提供了一个有价值的检验。\n\n最后一步是按要求将所有计算出的方差值四舍五入到六位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing OLS coefficient variances for raw and\n    orthonormalized cubic spline bases across several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path\n        {'n': 200, 'knots': [0.3, 0.6], 'sigma_sq': 0.5},\n        # Case B: edge case with closely spaced knots a.k.a. high collinearity\n        {'n': 50, 'knots': [0.49, 0.5, 0.51], 'sigma_sq': 1.0},\n        # Case C: boundary case with no internal knots (polynomial basis)\n        {'n': 150, 'knots': [], 'sigma_sq': 0.2}\n    ]\n\n    results = []\n    \n    # Helper functions\n    def format_list(lst):\n        # Format a list of numbers into a comma-separated string without spaces\n        return f\"[{','.join(f'{x:.6f}' for x in lst)}]\"\n\n    def format_case_result(case_result):\n        # case_result is [v_raw_list, v_orth_list]\n        return f\"[{format_list(case_result[0])},{format_list(case_result[1])}]\"\n\n    def modified_gram_schmidt(A):\n        \"\"\"\n        Performs Modified Gram-Schmidt orthogonalization on the columns of matrix A.\n        Returns an orthonormal matrix Q.\n        \"\"\"\n        # Work on a copy with float64 for numerical precision\n        V = A.copy().astype(np.float64)\n        num_cols = V.shape[1]\n        Q = np.zeros_like(V, dtype=np.float64)\n        \n        for j in range(num_cols):\n            # Normalize the j-th column of V to get the j-th column of Q\n            norm_vj = np.linalg.norm(V[:, j])\n            \n            # Handle potential linear dependency (column is ~zero)\n            if norm_vj  1e-12:\n                Q[:, j] = 0.0\n            else:\n                Q[:, j] = V[:, j] / norm_vj\n            \n            # Orthogonalize all subsequent columns of V against the new Q column\n            for k in range(j + 1, num_cols):\n                V[:, k] -= np.dot(Q[:, j], V[:, k]) * Q[:, j]\n                \n        return Q\n\n    for case in test_cases:\n        n = case['n']\n        knots = case['knots']\n        sigma_sq = case['sigma_sq']\n\n        # 1. Create a grid of n inputs\n        x = (np.arange(1, n + 1) - 0.5) / n\n\n        # 2. Construct the raw cubic truncated power basis design matrix X\n        p = 4 + len(knots)\n        X = np.zeros((n, p))\n        \n        # Polynomial part\n        X[:, 0] = 1\n        X[:, 1] = x\n        X[:, 2] = x**2\n        X[:, 3] = x**3\n        \n        # Truncated power part\n        if knots:\n            for j, knot in enumerate(knots):\n                X[:, 4 + j] = np.maximum(x - knot, 0)**3\n        \n        # 3. Compute the theoretical OLS coefficient variance vector for the raw basis\n        # Use float64 for higher precision, especially for inversion\n        X_64 = X.astype(np.float64)\n        gram_X = X_64.T @ X_64\n        try:\n            inv_gram_X = np.linalg.inv(gram_X)\n            v_raw_diag = np.diag(inv_gram_X)\n            v_raw = sigma_sq * v_raw_diag\n        except np.linalg.LinAlgError:\n            # A singular matrix would imply perfect collinearity\n            v_raw = np.full(p, np.inf)\n\n        # 4. Orthogonalize the columns of X to obtain Q\n        Q = modified_gram_schmidt(X)\n\n        # 5. Compute the theoretical OLS coefficient variance vector for the orthonormal basis\n        gram_Q = Q.T @ Q\n        try:\n            inv_gram_Q = np.linalg.inv(gram_Q)\n            v_orth_diag = np.diag(inv_gram_Q)\n            v_orth = sigma_sq * v_orth_diag\n        except np.linalg.LinAlgError:\n            v_orth = np.full(p, np.inf)\n            \n        # 6. Round and store results\n        # The problem statement implies rounding is the final output step,\n        # but for clean storage and formatting, we do it here.\n        v_raw_rounded = np.round(v_raw, 6)\n        v_orth_rounded = np.round(v_orth, 6)\n        \n        results.append([v_raw_rounded.tolist(), v_orth_rounded.tolist()])\n\n    # Final print statement in the exact required format.\n    formatted_results = [format_case_result(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "回归分析不仅限于预测条件均值，我们常常更关心数据的分位数，例如中位数或第90百分位数。本练习将样条的灵活性与分位数回归相结合，通过最小化弹球损失函数（pinball loss）来拟合非线性分位数曲线。你将亲手实现这一过程，并探究模型对节点位置的敏感性，这是在实践中选择模型时必须考虑的关键问题 。",
            "id": "3157118",
            "problem": "您的任务是在统计学习的分段多项式、节点和样条框架内，构建和分析分位数回归样条。您应该从通过 pinball 损失定义分位数以及通过截断幂基表示回归样条这两个基本定义出发。仅依赖于这些基础定义和经过充分检验的事实；不要假设任何预先推导出的简化公式。\n\n使用截断幂表示法构建一个三次回归样条基。对于一组输入 $x \\in \\mathbb{R}$、一个内部节点集 $\\mathcal{K} = \\{k_{1},\\dots,k_{m}\\}$（其中 $k_{j} \\in \\mathbb{R}$）以及次数 $p = 3$，将基函数定义为最高三次的多项式项与截断三次项的并集：\n$$\n\\{1, x, x^{2}, x^{3}\\} \\cup \\{(x - k_{j})_{+}^{3} \\,:\\, j = 1,\\dots,m\\},\n$$\n其中 $(t)_{+} = \\max\\{t, 0\\}$。\n\n设分位数水平为 $\\tau \\in (0,1)$ 时，对于残差 $u \\in \\mathbb{R}$ 的 pinball 损失定义为\n$$\n\\rho_{\\tau}(u) = u \\cdot \\left(\\tau - \\mathbb{I}\\{u  0\\}\\right).\n$$\n在水平 $\\tau$ 上的分位数回归通过最小化经验风险 $\\sum_{i=1}^{n} \\rho_{\\tau}(y_{i} - f_{\\beta}(x_{i}))$ 来估计系数 $\\beta \\in \\mathbb{R}^{d}$，其中 $f_{\\beta}(x)$ 是由上述基生成的样条模型。您必须从上述定义出发，以一种适用于凸分段线性目标的数值精确方法来表述和解决此问题。\n\n数据。使用以下固定数据集。设 $n = 41$ 并定义 $x_{i} = i/40$ 对于 $i \\in \\{0,1,\\dots,40\\}$，因此 $x_{i} \\in [0,1]$。定义确定性响应\n$$\ny_{i} = \\sin(2\\pi x_{i}) + 0.3 \\cdot \\sin(5\\pi x_{i}) + 0.1 \\cdot x_{i}.\n$$\n不允许存在随机性。\n\n任务。您将通过比较不同节点配置下的拟合函数，研究分位数样条估计对节点位置的敏感性。对于下面的每个任务，拟合两个样条模型，它们具有相同的次数 $p = 3$ 和指定的分位数水平 $\\tau$，但内部节点集不同。然后计算每种情况下定义的敏感性度量。\n\n- 测试一（中位数，网格最大差值）。分位数水平 $\\tau = 0.5$。使用内部节点 $\\mathcal{K}_{A} = \\{0.25, 0.5, 0.75\\}$ 和 $\\mathcal{K}_{B} = \\{0.2, 0.5, 0.8\\}$。在训练数据上拟合这两个模型。在 $[0,1]$ 上定义一个包含 $201$ 个点的均匀评估网格，并计算\n$$\nS_{1} = \\max_{x \\in \\text{grid}} \\left| \\hat{f}_{A,\\tau}(x) - \\hat{f}_{B,\\tau}(x) \\right|.\n$$\n\n- 测试二（上分位数，网格最大差值）。分位数水平 $\\tau = 0.9$。使用与测试一相同的节点，即 $\\mathcal{K}_{A} = \\{0.25, 0.5, 0.75\\}$ 和 $\\mathcal{K}_{B} = \\{0.2, 0.5, 0.8\\}$。在 $[0,1]$ 上定义相同的包含 $201$ 个点的均匀评估网格，并计算\n$$\nS_{2} = \\max_{x \\in \\text{grid}} \\left| \\hat{f}_{A,\\tau}(x) - \\hat{f}_{B,\\tau}(x) \\right|.\n$$\n\n- 测试三（边界条件，无节点与单节点对比）。分位数水平 $\\tau = 0.5$。比较一个没有内部节点 $\\mathcal{K}_{A} = \\varnothing$ 的全局三次多项式与一个带有一个内部节点 $\\mathcal{K}_{B} = \\{0.5\\}$ 的样条。在 $[0,1]$ 上使用相同的 $201$ 点网格，计算\n$$\nS_{3} = \\max_{x \\in \\text{grid}} \\left| \\hat{f}_{A,\\tau}(x) - \\hat{f}_{B,\\tau}(x) \\right|.\n$$\n\n- 测试四（训练点上的局部敏感性，上分位数）。分位数水平 $\\tau = 0.9$。使用 $\\mathcal{K}_{A} = \\{0.33, 0.66\\}$ 和 $\\mathcal{K}_{B} = \\{0.34, 0.66\\}$。计算在训练输入 $\\{x_{i}\\}_{i=1}^{n}$ 上评估的两个拟合的均方根偏差：\n$$\nS_{4} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{f}_{A,\\tau}(x_{i}) - \\hat{f}_{B,\\tau}(x_{i}) \\right)^{2}}.\n$$\n\n程序要求。\n\n- 实现一个函数，使用截断幂表示法为任何给定的内部节点集构建三次回归样条基。\n\n- 为给定的 $\\tau$ 实现一个分位数回归求解器，该求解器对于由 pinball 损失引出的凸分段线性目标应是数值精确的。您可以引入与定义一致的辅助变量，将问题转化为可通过标准凸优化工具求解的形式。\n\n- 严格按照上述规定使用数据集和测试套件。不允许有随机性。\n\n- 数值评估网格：在 $[0,1]$ 上精确使用 $201$ 个等距点，包括两个端点。\n\n- 输出规格：您的程序应生成单行输出，其中包含结果 $[S_{1}, S_{2}, S_{3}, S_{4}]$，形式为方括号内以逗号分隔的列表，每个值四舍五入到 $6$ 位小数，并严格遵循此顺序和格式，例如 $[0.123456,0.234567,0.345678,0.456789]$。\n\n注意。\n\n- 此问题中没有物理单位。\n\n- 所有角度均以弧度为单位，所有三角函数均为标准函数。\n\n- $S_{1}, S_{2}, S_{3}, S_{4}$ 中的每一个都必须是单个实数（浮点数）。",
            "solution": "该问题定义明确、科学合理且内部一致。所有必要的数据、函数和评估标准均已提供。任务是使用三次样条进行分位数回归，这是统计学习中的一种标准技术。解决分位数回归优化问题的方法已由目标函数的描述（凸分段线性）所暗示，对此，线性规划（$LP$）是精确且合适的数值方法。\n\n### 基于原则的设计\n\n1.  **模型设定**：模型是一个三次样条函数 $f_{\\beta}(x)$，使用截断幂基定义。对于给定的输入点集 $x$、次数 $p=3$ 以及一个包含 $m$ 个内部节点的集合 $\\mathcal{K} = \\{k_1, \\dots, k_m\\}$，该函数是基函数的线性组合：\n    $$\n    f_{\\beta}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{j=1}^{m} \\beta_{3+j} (x - k_j)_+^3\n    $$\n    这可以紧凑地写为 $f_{\\beta}(x) = H_x \\beta$，其中 $H_x$ 是在 $x$ 处评估的基函数的行向量，$\\beta$ 是系数向量。系数总数为 $d = p+1+m = 4+m$。\n\n2.  **分位数回归目标**：目标是找到系数向量 $\\beta$，以最小化经验风险，该风险定义为 $n$ 个数据点上 pinball 损失的总和：\n    $$\n    \\min_{\\beta \\in \\mathbb{R}^d} \\sum_{i=1}^{n} \\rho_{\\tau}(y_i - f_{\\beta}(x_i))\n    $$\n    其中，对于残差 $u_i = y_i - f_{\\beta}(x_i)$，在分位数水平 $\\tau \\in (0,1)$ 下的 pinball 损失由 $\\rho_{\\tau}(u_i) = u_i(\\tau - \\mathbb{I}\\{u_i  0\\})$ 给出。\n\n3.  **线性规划表述**：pinball 损失是一个凸分段线性函数。目标函数作为此类函数的总和，也是凸的。这使得优化问题可以转化为一个标准的线性规划（$LP$）问题。我们为每个数据点 $i \\in \\{1, \\dots, n\\}$ 引入非负辅助变量 $u_i^+$ 和 $u_i^-$。我们将残差 $u_i$ 分解为 $u_i = u_i^+ - u_i^-$，其中 $u_i^+ = \\max(u_i, 0)$ 且 $u_i^- = \\max(-u_i, 0)$。pinball 损失可以重新表示为这些新变量的线性函数：$\\rho_{\\tau}(u_i) = \\tau u_i^+ + (1-\\tau)u_i^-$。\n\n    优化问题现在变为：\n    $$\n    \\min_{\\beta, u^+, u^-} \\sum_{i=1}^{n} \\left( \\tau u_i^+ + (1-\\tau)u_i^- \\right)\n    $$\n    受制于 $i=1, \\dots, n$ 的约束条件：\n    $$\n    f_{\\beta}(x_i) + u_i^+ - u_i^- = y_i\n    $$\n    $$\n    u_i^+ \\ge 0, \\quad u_i^- \\ge 0\n    $$\n    系数 $\\beta_j$ 的符号不受约束。标准的 LP 求解器要求所有变量均为非负。因此，我们将每个系数 $\\beta_j$ 分解为其正部和负部：$\\beta_j = \\beta_j^+ - \\beta_j^-$，其中 $\\beta_j^+ \\ge 0$ 且 $\\beta_j^- \\ge 0$。\n\n    设 $H$ 为 $n \\times d$ 的设计矩阵，其中 $H_{ij}$ 是在 $x_i$ 处评估的第 $j$ 个基函数。最终的 LP 表述为：\n    $$\n    \\min_{\\beta^+, \\beta^-, u^+, u^-} \\left( \\mathbf{0}^T\\beta^+ + \\mathbf{0}^T\\beta^- + \\tau \\mathbf{1}^T u^+ + (1-\\tau)\\mathbf{1}^T u^- \\right)\n    $$\n    受制于：\n    $$\n    H\\beta^+ - H\\beta^- + I u^+ - I u^- = \\mathbf{y}\n    $$\n    $$\n    \\beta^+ \\ge \\mathbf{0}, \\quad \\beta^- \\ge \\mathbf{0}, \\quad u^+ \\ge \\mathbf{0}, \\quad u^- \\ge \\mathbf{0}\n    $$\n    这符合典范形式 $\\min \\mathbf{c}^T\\mathbf{z}$，受制于 $\\mathbf{A}_{eq}\\mathbf{z} = \\mathbf{b}_{eq}$ 和 $\\mathbf{z} \\ge \\mathbf{0}$，可以被高效求解。\n\n4.  **实现与评估**：\n    *   实现一个函数，用于根据三次截断幂基为任何给定的点集 $x$ 和节点集 $\\mathcal{K}$ 构建设计矩阵 $H$。\n    *   一个求解器函数接收训练数据 $(x_i, y_i)$、节点 $\\mathcal{K}$ 和分位数水平 $\\tau$，组装上述 LP 的各部分（$\\mathbf{c}, \\mathbf{A}_{eq}, \\mathbf{b}_{eq}$），并使用 `scipy.optimize.linprog` 找到最优解向量 $\\mathbf{z}$。从 $\\mathbf{z}$ 中提取系数向量 $\\beta^+$ 和 $\\beta^-$，并恢复最终的样条系数向量为 $\\beta = \\beta^+ - \\beta^-$。\n    *   对于四个指定的测试中的每一个，使用各自的节点集 $\\mathcal{K}_A$ 和 $\\mathcal{K}_B$ 拟合两个样条模型（$\\hat{f}_{A,\\tau}$ 和 $\\hat{f}_{B,\\tau}$）。\n    *   然后使用拟合的模型在指定的评估点（对于测试 1-3 是一个包含 201 个点的精细网格，对于测试 4 是训练点）上预测值。\n    *   根据这些预测计算所需的敏感性度量（$S_1, S_2, S_3, S_4$），以衡量每个测试中两个拟合函数之间的差异。收集结果并按指定格式进行格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes quantile regression splines by solving a series of\n    linear programming problems.\n    \"\"\"\n\n    def create_design_matrix(x, knots, p=3):\n        \"\"\"\n        Constructs the design matrix for a polynomial spline with a truncated power basis.\n        \n        Args:\n            x (np.ndarray): Input data points.\n            knots (list or np.ndarray): Internal knot locations.\n            p (int): Degree of the polynomial spline.\n\n        Returns:\n            np.ndarray: The design matrix H.\n        \"\"\"\n        x_flat = np.asarray(x).flatten()\n        \n        # Polynomial basis terms: 1, x, x^2, x^3\n        H_poly = np.vander(x_flat, N=p + 1, increasing=True)\n\n        if not knots:\n            return H_poly\n\n        knots_arr = np.asarray(knots)\n        # Truncated power basis terms: (x - k)_+^p\n        # The shape is (len(x), len(knots))\n        H_trunc = np.maximum(0, x_flat[:, np.newaxis] - knots_arr[np.newaxis, :])**p\n        \n        return np.hstack([H_poly, H_trunc])\n\n    def fit_quantile_spline(x_train, y_train, knots, tau):\n        \"\"\"\n        Fits a quantile regression spline by formulating and solving a linear program.\n\n        Args:\n            x_train (np.ndarray): Training input data.\n            y_train (np.ndarray): Training response data.\n            knots (list): Internal knot locations.\n            tau (float): The quantile level, in (0, 1).\n\n        Returns:\n            np.ndarray: The estimated spline coefficients beta.\n        \"\"\"\n        n = len(x_train)\n        H = create_design_matrix(x_train, knots, p=3)\n        d = H.shape[1] # Number of basis functions\n\n        # Formulate the Linear Program for quantile regression\n        # Variables z = [beta+, beta-, u+, u-]\n        # size = d + d + n + n = 2d + 2n\n        \n        # Objective vector c\n        c = np.concatenate([np.zeros(2 * d),\n                            tau * np.ones(n),\n                            (1 - tau) * np.ones(n)])\n        \n        # Equality constraints matrix A_eq\n        # H*beta+ - H*beta- + I*u+ - I*u- = y\n        A_eq = np.hstack([H, -H, np.eye(n), -np.eye(n)])\n        \n        # Equality constraints vector b_eq\n        b_eq = y_train\n\n        # All variables are non-negative, so bounds are (0, None). This is the default.\n        res = linprog(c, A_eq=A_eq, b_eq=b_eq, method='highs')\n\n        if not res.success:\n            raise RuntimeError(f\"LP solver failed for knots={knots}, tau={tau}: {res.message}\")\n\n        solution = res.x\n        beta_plus = solution[:d]\n        beta_minus = solution[d:2*d]\n        beta = beta_plus - beta_minus\n        \n        return beta\n\n    def predict(x_eval, knots, beta):\n        \"\"\"\n        Predicts response values using a fitted spline model.\n\n        Args:\n            x_eval (np.ndarray): Points to evaluate the model at.\n            knots (list): The knots used to fit the model.\n            beta (np.ndarray): The fitted spline coefficients.\n\n        Returns:\n            np.ndarray: The predicted values.\n        \"\"\"\n        H_eval = create_design_matrix(x_eval, knots, p=3)\n        return H_eval @ beta\n\n    # Define the fixed dataset\n    n = 41\n    x_train = np.linspace(0, 1, n)\n    y_train = np.sin(2 * np.pi * x_train) + 0.3 * np.sin(5 * np.pi * x_train) + 0.1 * x_train\n\n    # Define the evaluation grid\n    x_grid = np.linspace(0, 1, 201)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Test One\n        {'tau': 0.5, 'knots_A': [0.25, 0.5, 0.75], 'knots_B': [0.2, 0.5, 0.8], 'metric': 'max_abs_diff_grid'},\n        # Test Two\n        {'tau': 0.9, 'knots_A': [0.25, 0.5, 0.75], 'knots_B': [0.2, 0.5, 0.8], 'metric': 'max_abs_diff_grid'},\n        # Test Three\n        {'tau': 0.5, 'knots_A': [], 'knots_B': [0.5], 'metric': 'max_abs_diff_grid'},\n        # Test Four\n        {'tau': 0.9, 'knots_A': [0.33, 0.66], 'knots_B': [0.34, 0.66], 'metric': 'rmsd_train'}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        tau = case['tau']\n        knots_A = case['knots_A']\n        knots_B = case['knots_B']\n        \n        # Fit model A\n        beta_A = fit_quantile_spline(x_train, y_train, knots_A, tau)\n        \n        # Fit model B\n        beta_B = fit_quantile_spline(x_train, y_train, knots_B, tau)\n\n        if case['metric'] == 'max_abs_diff_grid':\n            pred_A = predict(x_grid, knots_A, beta_A)\n            pred_B = predict(x_grid, knots_B, beta_B)\n            metric_val = np.max(np.abs(pred_A - pred_B))\n        elif case['metric'] == 'rmsd_train':\n            pred_A = predict(x_train, knots_A, beta_A)\n            pred_B = predict(x_train, knots_B, beta_B)\n            metric_val = np.sqrt(np.mean((pred_A - pred_B)**2))\n        else:\n            raise ValueError(f\"Unknown metric: {case['metric']}\")\n            \n        results.append(metric_val)\n\n    # Format and print the final output\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "样条模型不仅是强大的预测工具，也可用于严谨的统计推断，例如检验是否存在阈值效应——一个在生物学和经济学中常见的问题。本练习将指导你使用分段线性样条来构建假设检验，并计算其统计功效，以判断模型检测斜率突变的能力。通过这个实践，你将学会如何利用样条模型来回答具体的科学问题 。",
            "id": "3157150",
            "problem": "考虑以下的目标节点分段线性样条回归模型，该模型旨在检测一个疑似生物学阈值处的斜率变化。对于 $[0,1]$ 区间内的一组固定设计点 $x_1, x_2, \\dots, x_n$，定义铰链函数 $(u)_+ = \\max(u, 0)$，以及在一个选定的节点位置 $\\tau_{\\text{knot}}$ 处的工作样条基，其由列向量 $1$、$x$ 和 $(x - \\tau_{\\text{knot}})_+$ 构成。回归模型为\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - \\tau_{\\text{knot}})_+ + \\varepsilon_i,\\quad i=1,\\dots,n,\n$$\n其中 $\\varepsilon_i$ 是独立同分布的高斯（正态）随机变量，其均值为 $0$，方差为 $\\sigma^2$，记作 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。\n\n假设真实均值函数在生物学阈值 $\\tau_{\\text{true}}$ 处表现出一个幅度为 $\\gamma$ 的斜率变化，由下式给出\n$$\nf_{\\text{true}}(x) = \\gamma \\,(x - \\tau_{\\text{true}})_+.\n$$\n我们希望量化在显著性水平 $\\alpha$ 下，对于以下假设的双边假设检验的统计功效\n$$\nH_0:\\ \\beta_2 = 0 \\quad \\text{versus} \\quad H_1:\\ \\beta_2 \\neq 0,\n$$\n该检验通过在疑似阈值 $\\tau_{\\text{true}}$ 附近放置目标节点 $\\tau_{\\text{knot}}$，来操作化地检测阈值处的急剧斜率变化。\n\n仅从普通最小二乘法（Ordinary Least Squares, OLS）的基本定义、高斯误差的性质以及线性回归中经过充分检验的分布事实出发，推导如何计算关于 $\\beta_2$ 的双边 $t$ 检验的统计功效 $P(\\text{reject } H_0 \\mid \\gamma,\\sigma,\\tau_{\\text{true}},\\tau_{\\text{knot}},n)$。您的推导必须从以下几点开始：\n- OLS 估计量的定义 $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}$，其中 $X$ 是由列向量 $1$、$x$ 和 $(x - \\tau_{\\text{knot}})_+$ 构成的 $n \\times p$ 设计矩阵（因此 $p=3$），$\\boldsymbol{y}$ 是响应向量。\n- 正态误差模型 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 I_n)$。\n- 一个经过充分检验的事实：当未知方差从残差中估计时，对于单个线性回归系数的 $t$ 统计量，在备择假设下服从非中心 $t$ 分布（Noncentral $t$ distribution, NCT），其自由度（Degrees of Freedom, DoF）为 $n-p$，非中心参数则依赖于真实均值在工作设计上的投影。\n\n然后，在一个完整的、可运行的程序中实现所推导的计算，该程序需要：\n- 构建固定设计点 $x_i$ 为 $[0,1]$ 上的 $n$ 个等间距点。\n- 在选定的 $\\tau_{\\text{knot}}$ 处形成工作设计矩阵 $X$。\n- 将 $f_{\\text{true}}(x)$ 投影到 $X$ 的列空间上，以找到伪真实系数向量 $\\boldsymbol{\\beta}^\\star = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}}$，并使用其第三个分量 $\\beta_2^\\star$ 来参数化关于铰链系数的双边 $t$ 检验。\n- 使用非中心 $t$ 分布计算在显著性水平 $\\alpha$ 下检验 $H_0:\\beta_2=0$ 的精确双边功效，其自由度为 $df=n-p$，非中心参数为 $\\delta = \\beta_2^\\star / (\\sigma \\sqrt{(X^\\top X)^{-1}_{3,3}})$。\n\n您的程序必须对以下测试套件评估功效，该套件涵盖了一般情况、节点位置设定错误、无信号边缘情况、边界阈值放置、高噪声与陡峭效应，以及偏移节点且阈值后数据点很少的情况：\n- 情况1：$n=60$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=2.0$, $\\alpha=0.05$。\n- 情况2：$n=60$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.55$, $\\gamma=2.0$, $\\alpha=0.05$。\n- 情况3：$n=30$, $\\sigma=2.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=0.0$, $\\alpha=0.05$。\n- 情况4：$n=120$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.10$, $\\tau_{\\text{knot}}=0.10$, $\\gamma=1.5$, $\\alpha=0.05$。\n- 情况5：$n=120$, $\\sigma=3.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=5.0$, $\\alpha=0.05$。\n- 情况6：$n=60$, $\\sigma=0.5$, $\\tau_{\\text{true}}=0.80$, $\\tau_{\\text{knot}}=0.75$, $\\gamma=1.0$, $\\alpha=0.05$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序与上述情况一致，例如 $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$。每个 $\\text{result}_k$ 必须是 $[0,1]$ 内的一个浮点数，等于为情况 $k$ 计算出的统计功效。",
            "solution": "任务是推导和计算一个分段线性样条回归模型中系数的假设检验的统计功效。功效是在特定备择假设为真时，正确拒绝原假设的概率。我们将遵循一个严谨的推导过程，从普通最小二乘法（OLS）回归的基本原理及其估计量的分布理论出发。\n\n**1. 模型设定与问题设置**\n\n所考虑的回归模型是：\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - \\tau_{\\text{knot}})_+ + \\varepsilon_i, \\quad i=1,\\dots,n\n$$\n其中 $(u)_+ = \\max(u, 0)$ 是铰链函数，$\\tau_{\\text{knot}}$ 是一个固定的节点，误差 $\\varepsilon_i$ 是独立同分布（i.i.d.）的正态随机变量，$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n用矩阵表示法，模型为 $\\boldsymbol{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中：\n- $\\boldsymbol{y} = [y_1, \\dots, y_n]^\\top$ 是响应向量。\n- $X$ 是 $n \\times 3$ 的设计矩阵，其列对应于基函数：\n$$\nX = \\begin{bmatrix}\n1  x_1  (x_1 - \\tau_{\\text{knot}})_+ \\\\\n1  x_2  (x_2 - \\tau_{\\text{knot}})_+ \\\\\n\\vdots  \\vdots  \\vdots \\\\\n1  x_n  (x_n - \\tau_{\\text{knot}})_+\n\\end{bmatrix}\n$$\n- $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^\\top$ 是系数向量。\n- $\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\dots, \\varepsilon_n]^\\top$ 是误差向量，其分布为 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 I_n)$，其中 $I_n$ 是 $n \\times n$ 单位矩阵。\n\n假设真实数据生成过程的均值函数由下式给出：\n$$\nf_{\\text{true}}(x) = \\gamma (x - \\tau_{\\text{true}})_+\n$$\n这意味着观测数据 $\\boldsymbol{y}$ 是模型 $\\boldsymbol{y} = \\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}$ 的实现，其中 $\\boldsymbol{f_{\\text{true}}}$ 是元素为 $f_{\\text{true}}(x_i)$ 的向量。\n\n我们在显著性水平 $\\alpha$ 下，检验原假设 $H_0: \\beta_2 = 0$ 与备择假设 $H_1: \\beta_2 \\neq 0$。\n\n**2. OLS 估计量的分布**\n\n$\\boldsymbol{\\beta}$ 的 OLS 估计量定义为 $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}$。为了在真实数据生成过程下分析其性质，我们代入 $\\boldsymbol{y} = \\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}$：\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top (\\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}) = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} + (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon}\n$$\n$\\hat{\\boldsymbol{\\beta}}$ 的期望值为：\n$$\nE[\\hat{\\boldsymbol{\\beta}}] = E[(X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}}] + E[(X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon}] = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} + (X^\\top X)^{-1} X^\\top E[\\boldsymbol{\\varepsilon}]\n$$\n由于 $E[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}$，期望简化为：\n$$\nE[\\hat{\\boldsymbol{\\beta}}] = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} \\equiv \\boldsymbol{\\beta}^\\star\n$$\n这个向量 $\\boldsymbol{\\beta}^\\star$ 是“伪真实”参数向量。它代表了在由设计矩阵 $X$ 的列所张成的向量空间内，对真实均值函数 $\\boldsymbol{f_{\\text{true}}}$ 的最佳逼近。一般而言，$\\boldsymbol{\\beta}^\\star$ 不等于 $(0, 0, \\gamma)^\\top$，除非基函数和节点与真实函数完美对齐，并且没有截距项或线性项。\n\n$\\hat{\\boldsymbol{\\beta}}$ 的协方差矩阵是：\n$$\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = E\\left[ (\\hat{\\boldsymbol{\\beta}} - E[\\hat{\\boldsymbol{\\beta}}]) (\\hat{\\boldsymbol{\\beta}} - E[\\hat{\\boldsymbol{\\beta}}])^\\top \\right] = E\\left[ \\left( (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon} \\right) \\left( (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon} \\right)^\\top \\right]\n$$\n$$\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = (X^\\top X)^{-1} X^\\top E[\\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon}^\\top] X (X^\\top X)^{-1} = (X^\\top X)^{-1} X^\\top (\\sigma^2 I_n) X (X^\\top X)^{-1} = \\sigma^2 (X^\\top X)^{-1}\n$$\n由于 $\\hat{\\boldsymbol{\\beta}}$ 是高斯向量 $\\boldsymbol{\\varepsilon}$ 的线性变换，它也服从正态分布：\n$$\n\\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}^\\star, \\sigma^2 (X^\\top X)^{-1})\n$$\n\n**3. t 统计量及其分布**\n\n我们感兴趣的是系数 $\\beta_2$。估计量 $\\hat{\\beta}_2$ 是 $\\hat{\\boldsymbol{\\beta}}$ 的第三个元素。其分布为单变量正态分布：\n$$\n\\hat{\\beta}_2 \\sim \\mathcal{N}(\\beta_2^\\star, \\sigma^2 C_{33})\n$$\n其中 $\\beta_2^\\star$ 是 $\\boldsymbol{\\beta}^\\star$ 的第三个元素，而 $C_{33} = [(X^\\top X)^{-1}]_{3,3}$ 是 Gram 矩阵逆矩阵的第三个对角元素。$\\hat{\\beta}_2$ 的标准误是 $\\text{se}(\\hat{\\beta}_2) = \\sigma \\sqrt{C_{33}}$。\n\n在实践中，$\\sigma^2$ 是未知的，必须从数据中估计。无偏估计量是 $\\hat{\\sigma}^2 = \\frac{1}{n-p} \\text{RSS}$，其中 $p=3$ 是参数个数，$\\text{RSS} = \\|\\boldsymbol{y} - X\\hat{\\boldsymbol{\\beta}}\\|^2$ 是残差平方和。检验统计量用估计的标准误 $\\hat{\\text{se}}(\\hat{\\beta}_2) = \\hat{\\sigma}\\sqrt{C_{33}}$ 代替真实的标准误：\n$$\nT = \\frac{\\hat{\\beta}_2 - 0}{\\hat{\\text{se}}(\\hat{\\beta}_2)} = \\frac{\\hat{\\beta}_2}{\\hat{\\sigma}\\sqrt{C_{33}}}\n$$\n在 $E[\\hat{\\beta}_2] = \\beta_2^\\star \\neq 0$ 的一般备择假设下，该统计量服从非中心 $t$ 分布。自由度（$df$）与方差估计相关，为 $df = n-p = n-3$。非中心参数（NCP）$\\delta$ 是分子项的标准化均值：\n$$\n\\delta = \\frac{E[\\hat{\\beta}_2]}{\\text{se}(\\hat{\\beta}_2)} = \\frac{\\beta_2^\\star}{\\sigma\\sqrt{C_{33}}}\n$$\n因此，在由 $\\boldsymbol{f_{\\text{true}}}$ 指定的备择假设下，检验统计量服从分布 $T \\sim t(df, \\delta)$，这是一个自由度为 $df=n-3$、非中心参数为 $\\delta$ 的非中心 $t$ 分布。\n\n**4. 功效计算**\n\n对于一个显著性水平为 $\\alpha$ 的双边检验，如果检验统计量的观测值 $|T|$ 超过一个临界值，我们就拒绝原假设 $H_0: \\beta_2=0$。这个我们记为 $t_{\\text{crit}}$ 的临界值是由原假设下的分布决定的。在 $H_0$ 下，$\\gamma=0$，这意味着 $\\boldsymbol{f_{\\text{true}}} = \\boldsymbol{0}$，$\\boldsymbol{\\beta}^\\star = \\boldsymbol{0}$ 且 $\\delta = 0$。因此，原假设下的分布是一个中心 $t$ 分布，$T \\sim t(df)$。临界值 $t_{\\text{crit}}$ 是这个中心 $t$ 分布的上 $(1-\\alpha/2)$ 分位数：\n$$\nt_{\\text{crit}} = t_{df, 1-\\alpha/2}\n$$\n统计功效是在备择假设为真时拒绝 $H_0$ 的概率，即当 $T \\sim t(df, \\delta)$ 时。\n$$\n\\text{Power} = P(|T| > t_{\\text{crit}} \\mid T \\sim t(df, \\delta)) = P(T > t_{\\text{crit}}) + P(T  -t_{\\text{crit}})\n$$\n设 $F_{t(df, \\delta)}$ 为非中心 $t$ 分布的累积分布函数（CDF）。则功效计算如下：\n$$\n\\text{Power} = \\left( 1 - F_{t(df, \\delta)}(t_{\\text{crit}}) \\right) + F_{t(df, \\delta)}(-t_{\\text{crit}})\n$$\n\n**5. 计算步骤**\n\n计算每种情况功效的步骤如下：\n1.  给定 $n, \\sigma, \\tau_{\\text{true}}, \\tau_{\\text{knot}}, \\gamma, \\alpha$。\n2.  将设计点 $x_i$ 定义为从 $0$ 到 $1$（含两端）的 $n$ 个等间距点。\n3.  构建 $n \\times 3$ 的设计矩阵 $X$，其列分别为 $1$、$x_i$ 和 $(x_i - \\tau_{\\text{knot}})_+$。\n4.  构建 $n \\times 1$ 的真实均值向量 $\\boldsymbol{f}_{\\text{true}}$，其元素为 $\\gamma (x_i - \\tau_{\\text{true}})_+$。\n5.  计算矩阵乘积 $X^\\top X$ 及其逆矩阵 $(X^\\top X)^{-1}$。\n6.  计算伪真实参数向量 $\\boldsymbol{\\beta}^\\star = (X^\\top X)^{-1} X^\\top \\boldsymbol{f}_{\\text{true}}$。\n7.  提取第三个分量 $\\beta_2^\\star = \\boldsymbol{\\beta}^\\star_2$。\n8.  提取逆矩阵的第三个对角元素 $C_{33} = [(X^\\top X)^{-1}]_{3,3}$。\n9.  计算非中心参数 $\\delta = \\beta_2^\\star / (\\sigma \\sqrt{C_{33}})$。\n10. 设置自由度 $df = n - 3$。\n11. 从中心 $t$ 分布的逆累积分布函数（CDF）（或称百分点函数）中找到临界值：$t_{\\text{crit}} = F^{-1}_{t(df)}(1 - \\alpha/2)$。\n12. 使用自由度为 $df$、非中心参数为 $\\delta$ 的非中心 $t$ 分布的CDF计算功效：$\\text{Power} = 1 - F_{t(df, \\delta)}(t_{\\text{crit}}) + F_{t(df, \\delta)}(-t_{\\text{crit}})$。\n\n至此，推导和算法说明完成。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, nct\n\ndef solve():\n    \"\"\"\n    Computes the statistical power for a hypothesis test on a coefficient in a\n    piecewise-linear spline regression model for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: n=60, sigma=1.0, tau_true=0.50, tau_knot=0.50, gamma=2.0, alpha=0.05\n        {'n': 60, 'sigma': 1.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 2.0, 'alpha': 0.05},\n        # Case 2: n=60, sigma=1.0, tau_true=0.50, tau_knot=0.55, gamma=2.0, alpha=0.05\n        {'n': 60, 'sigma': 1.0, 'tau_true': 0.50, 'tau_knot': 0.55, 'gamma': 2.0, 'alpha': 0.05},\n        # Case 3: n=30, sigma=2.0, tau_true=0.50, tau_knot=0.50, gamma=0.0, alpha=0.05\n        {'n': 30, 'sigma': 2.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 0.0, 'alpha': 0.05},\n        # Case 4: n=120, sigma=1.0, tau_true=0.10, tau_knot=0.10, gamma=1.5, alpha=0.05\n        {'n': 120, 'sigma': 1.0, 'tau_true': 0.10, 'tau_knot': 0.10, 'gamma': 1.5, 'alpha': 0.05},\n        # Case 5: n=120, sigma=3.0, tau_true=0.50, tau_knot=0.50, gamma=5.0, alpha=0.05\n        {'n': 120, 'sigma': 3.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 5.0, 'alpha': 0.05},\n        # Case 6: n=60, sigma=0.5, tau_true=0.80, tau_knot=0.75, gamma=1.0, alpha=0.05\n        {'n': 60, 'sigma': 0.5, 'tau_true': 0.80, 'tau_knot': 0.75, 'gamma': 1.0, 'alpha': 0.05},\n    ]\n\n    results = []\n    for params in test_cases:\n        # Extract parameters for the current case\n        n = params['n']\n        sigma = params['sigma']\n        tau_true = params['tau_true']\n        tau_knot = params['tau_knot']\n        gamma = params['gamma']\n        alpha = params['alpha']\n        \n        # 1. Construct the fixed design points x_i as n evenly spaced points on [0,1]\n        x = np.linspace(0.0, 1.0, n)\n        \n        # 2. Form the working design matrix X\n        # Columns are 1, x, and (x - tau_knot)_+\n        p = 3\n        X = np.zeros((n, p))\n        X[:, 0] = 1.0\n        X[:, 1] = x\n        X[:, 2] = np.maximum(0, x - tau_knot)\n\n        # 3. Construct the true mean function vector f_true\n        f_true = gamma * np.maximum(0, x - tau_true)\n        \n        # 4. Compute the pseudo-true coefficient vector beta_star\n        # beta_star = (X^T X)^-1 X^T f_true\n        try:\n            XTX_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            # This case occurs if X has linearly dependent columns.\n            # For the given knots, X should have full rank.\n            # E.g., if tau_knot >= 1, the third column is all zeros.\n            results.append(np.nan) # Mark as not-a-number if problem occurs\n            continue\n\n        beta_star = XTX_inv @ X.T @ f_true\n        \n        # 5. Extract the third component, beta_2_star\n        beta_2_star = beta_star[2]\n        \n        # 6. Extract the variance term for beta_2_hat from (X^T X)^-1\n        C_33 = XTX_inv[2, 2]\n        \n        # 7. Calculate the noncentrality parameter (NCP), delta\n        delta = beta_2_star / (sigma * np.sqrt(C_33))\n        \n        # 8. Define degrees of freedom for the t-test\n        df = n - p\n        \n        # 9. Find the critical value for the two-sided test\n        t_crit = t.ppf(1 - alpha / 2, df)\n        \n        # 10. Compute the power using the Noncentral t (NCT) distribution\n        # Power = P(T > t_crit) + P(T  -t_crit) where T ~ NCT(df, delta)\n        power = 1.0 - nct.cdf(t_crit, df, delta) + nct.cdf(-t_crit, df, delta)\n        \n        results.append(power)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}