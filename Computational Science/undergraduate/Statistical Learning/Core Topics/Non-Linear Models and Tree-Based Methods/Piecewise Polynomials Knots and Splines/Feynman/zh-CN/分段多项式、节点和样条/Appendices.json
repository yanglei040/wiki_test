{
    "hands_on_practices": [
        {
            "introduction": "样条基函数虽然灵活，但其选择对模型性能至关重要。例如，截断幂基函数虽然易于构建，但其基向量之间通常存在高度相关性（即多重共线性），这会导致模型系数的方差膨胀，使其估计不稳定。本实践将通过构建一个正交化的样条基，并与原始基下的系数方差进行比较，帮助你亲手探索和解决这一数值稳定性问题。",
            "id": "3157128",
            "problem": "你的任务是分析使用三次样条基的普通最小二乘（OLS）线性回归模型中的系数方差。基本设定是线性模型 $y = X \\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，$X$ 是一个设计矩阵，其列是在 $n$ 个输入点上求值得到的样条基函数。OLS 估计量的抽样方差的公认公式为 $\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}$，此公式在满列秩和同方差性的假设下成立。你将构建一个原始三次样条截断幂基，通过 Gram-Schmidt 过程对其进行正交化以减少共线性，并比较原始基与正交规范基的理论系数方差。\n\n定义与构造：\n- 在 $[0,1]$ 上具有内部节点 $\\{k_1,\\dots,k_m\\}$ 的三次截断幂样条基由以下函数组成：$\\phi_0(x)=1$, $\\phi_1(x)=x$, $\\phi_2(x)=x^2$, $\\phi_3(x)=x^3$，以及对于每个内部节点 $k_j$，一个函数 $\\phi_{3+j}(x) = \\left( x - k_j \\right)_+^3$，其中 $(u)_+ = \\max(u, 0)$。\n- 对于输入 $\\{x_i\\}_{i=1}^n$，设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的元素为 $X_{i,j} = \\phi_j(x_i)$，其中 $j=0,\\dots,3+m$，因此 $p = 4 + m$。\n- Gram-Schmidt 正交化构建一个矩阵 $Q \\in \\mathbb{R}^{n \\times p}$，其列是张成与 $X$ 相同列空间的正交规范向量，即 $Q^\\top Q = I_p$ 且 $\\operatorname{span}\\{Q_{\\cdot,1},\\dots,Q_{\\cdot,p}\\} = \\operatorname{span}\\{X_{\\cdot,1},\\dots,X_{\\cdot,p}\\}$。\n\n实现任务：\n1. 对于每个测试用例，在 $[0,1]$上创建 $n$ 个输入的网格 $x_i = \\frac{i - 0.5}{n}$，其中 $i = 1,\\dots,n$。\n2. 使用该用例指定的内部节点，构建原始三次截断幂基的设计矩阵 $X$。\n3. 计算原始基的理论 OLS 系数方差向量，定义为 $\\sigma^2 \\cdot \\operatorname{diag}\\!\\left( (X^\\top X)^{-1} \\right)$，其中 $\\operatorname{diag}(A)$ 表示矩阵 $A$ 的对角线元素构成的向量。\n4. 使用经典的修正 Gram-Schmidt 过程对 $X$ 的列进行正交化，得到满足 $Q^\\top Q \\approx I_p$ 的矩阵 $Q$。\n5. 计算正交规范基的理论 OLS 系数方差向量，定义为 $\\sigma^2 \\cdot \\operatorname{diag}\\!\\left( (Q^\\top Q)^{-1} \\right)$。\n6. 将每个方差值四舍五入到六位小数。\n\n测试套件：\n- 用例 A（理想路径）：$n = 200$，内部节点 $\\{0.3, 0.6\\}$，噪声方差 $\\sigma^2 = 0.5$。\n- 用例 B（边界情况，节点密集分布增加了共线性）：$n = 50$，内部节点 $\\{0.49, 0.5, 0.51\\}$，噪声方差 $\\sigma^2 = 1.0$。\n- 用例 C（边界情况，无内部节点；纯三次多项式基）：$n = 150$，内部节点 $\\{\\}$，噪声方差 $\\sigma^2 = 0.2$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个测试用例的结果必须是一对列表 $[v_{\\text{raw}}, v_{\\text{orth}}]$，其中 $v_{\\text{raw}}$ 是原始基系数方差的列表，$v_{\\text{orth}}$ 是正交规范基系数方差的列表，两者都四舍五入到六位小数。例如，包含两个用例的输出可能如下所示：$[[[0.1,0.2],[0.1,0.1]],[[0.3,0.4],[0.3,0.3]]]$。不应打印任何额外文本。",
            "solution": "用户提供的问题被评估为有效。它在科学上基于统计线性模型和数值线性代数的原理，是适定的（提供了所有必要信息），并且完全客观。任务是比较使用两种不同三次样条基表示的模型中，普通最小二乘（OLS）系数估计量的理论方差：一种是原始截断幂基，另一种是由修正 Gram-Schmidt 过程导出的其正交规范对应物。\n\n### 理论框架\n\n该问题置于 OLS 线性回归的背景下。模型由下式给出：\n$$\ny = X \\beta + \\varepsilon\n$$\n其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是误差向量。我们假设误差是独立同分布的，均值为0，方差为 $\\sigma^2$，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\n$\\beta$ 的 OLS 估计量是通过最小化残差平方和找到的，由下式给出：\n$$\n\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n$$\n该估计量的抽样方差-协方差矩阵是统计推断的基石，由下式给出：\n$$\n\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}\n$$\n第 $j$ 个系数估计量 $\\hat{\\beta}_j$ 的方差是该矩阵的第 $j$ 个对角元素：\n$$\n\\mathrm{Var}(\\hat{\\beta}_j) = \\sigma^2 \\left[ (X^\\top X)^{-1} \\right]_{jj}\n$$\n问题要求为设计矩阵 $X$ 的两种不同选择计算这个方差向量 $\\sigma^2 \\cdot \\operatorname{diag}((X^\\top X)^{-1})$。\n\n### 分步解决流程\n\n1.  **输入数据生成**：对于每个包含 $n$ 个数据点的测试用例，我们首先在区间 $[0,1]$ 上使用公式 $x_i = \\frac{i - 0.5}{n}$（其中 $i=1, \\dots, n$）生成一个均匀的输入网格 $\\{x_i\\}_{i=1}^n$。\n\n2.  **原始基设计矩阵 ($X$) 构建**：\n    具有 $m$ 个内部节点 $\\{k_1, \\dots, k_m\\}$ 的三次截断幂样条基由 $p = 4 + m$ 个基函数组成：\n    -   $\\phi_0(x) = 1$\n    -   $\\phi_1(x) = x$\n    -   $\\phi_2(x) = x^2$\n    -   $\\phi_3(x) = x^3$\n    -   $\\phi_{3+j}(x) = (x - k_j)_+^3 = \\max(0, x - k_j)^3$ for $j=1, \\dots, m$.\n\n    设计矩阵 $X$ 是一个 $n \\times p$ 矩阵，其中每个元素为 $X_{ij} = \\phi_{j-1}(x_i)$（对列使用0-based索引，$j=0, \\dots, p-1$）。$X$ 的列通常高度相关，这种情况称为多重共线性。这使得矩阵 $X^\\top X$ 成为病态矩阵，意味着其逆矩阵对微小扰动敏感，通常表现为系数估计量的方差很大。\n\n3.  **原始基的方差计算**：\n    根据理论公式，系数方差向量 $v_{\\text{raw}}$ 计算如下：\n    $$\n    v_{\\text{raw}} = \\sigma^2 \\cdot \\operatorname{diag}\\left((X^\\top X)^{-1}\\right)\n    $$\n    这涉及计算 Gram 矩阵 $X^\\top X$、求其逆矩阵、提取对角元素，并按给定的噪声方差 $\\sigma^2$进行缩放。\n\n4.  **正交规范基设计矩阵 ($Q$) 构建**：\n    为减轻多重共线性的影响，可以对基向量（$X$ 的列）进行正交化。问题指定使用修正 Gram-Schmidt (MGS) 算法。MGS 是一个比其经典对应方法在数值上更稳定的过程，用于从输入矩阵 $X$ 生成正交规范矩阵 $Q$。$Q$ 的列构成了 $X$ 的列空间的一个正交规范基。\n    MGS 算法如下进行，从矩阵 $V^{(0)} = X$ 开始：\n    对于 $j=1, \\dots, p$：\n    1.  规范化第 $j$ 个列向量：$q_j = V^{(j-1)}_{\\cdot, j} / \\|V^{(j-1)}_{\\cdot, j}\\|_2$。\n    2.  对于所有后续列 $k = j+1, \\dots, p$，移除与 $q_j$平行的分量：$V^{(j)}_{\\cdot, k} = V^{(j-1)}_{\\cdot, k} - (q_j^\\top V^{(j-1)}_{\\cdot, k}) q_j$。\n    得到的矩阵 $Q$ 的列 $q_1, \\dots, q_p$ 根据构造是正交规范的，因此 $Q^\\top Q = I_p$。\n\n5.  **正交规范基的方差计算**：\n    当我们使用正交规范基 $Q$ 时，线性模型被重新参数化为 $y = Q \\gamma + \\varepsilon$。新系数 $\\gamma$ 的 OLS 估计量是 $\\hat{\\gamma} = (Q^\\top Q)^{-1} Q^\\top y$。相应的方差-协方差矩阵是：\n    $$\n    \\mathrm{Var}(\\hat{\\gamma}) = \\sigma^2 (Q^\\top Q)^{-1}\n    $$\n    由于 $Q$ 是正交规范的，受浮点精度限制，$Q^\\top Q \\approx I_p$（单位矩阵）。因此，其逆矩阵也近似为单位矩阵，$(Q^\\top Q)^{-1} \\approx I_p$。系数方差向量 $v_{\\text{orth}}$ 变为：\n    $$\n    v_{\\text{orth}} = \\sigma^2 \\cdot \\operatorname{diag}(I_p) = \\sigma^2 \\cdot [1, 1, \\dots, 1]^\\top = [\\sigma^2, \\sigma^2, \\dots, \\sigma^2]^\\top\n    $$\n    这展示了使用正交基的一个关键优势：系数估计量是不相关的，并且它们的方差都等于噪声方差 $\\sigma^2$，而与原始基的结构无关。这一理论结果为数值实现提供了一个有价值的检验。\n\n最后一步是按要求将所有计算出的方差值四舍五入到六位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing OLS coefficient variances for raw and\n    orthonormalized cubic spline bases across several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path\n        {'n': 200, 'knots': [0.3, 0.6], 'sigma_sq': 0.5},\n        # Case B: edge case with closely spaced knots a.k.a. high collinearity\n        {'n': 50, 'knots': [0.49, 0.5, 0.51], 'sigma_sq': 1.0},\n        # Case C: boundary case with no internal knots (polynomial basis)\n        {'n': 150, 'knots': [], 'sigma_sq': 0.2}\n    ]\n\n    results = []\n    \n    # Helper functions\n    def format_list(lst):\n        # Format a list of numbers into a comma-separated string without spaces\n        return f\"[{','.join(f'{x:.6f}' for x in lst)}]\"\n\n    def format_case_result(case_result):\n        # case_result is [v_raw_list, v_orth_list]\n        return f\"[{format_list(case_result[0])},{format_list(case_result[1])}]\"\n\n    def modified_gram_schmidt(A):\n        \"\"\"\n        Performs Modified Gram-Schmidt orthogonalization on the columns of matrix A.\n        Returns an orthonormal matrix Q.\n        \"\"\"\n        # Work on a copy with float64 for numerical precision\n        V = A.copy().astype(np.float64)\n        num_cols = V.shape[1]\n        Q = np.zeros_like(V, dtype=np.float64)\n        \n        for j in range(num_cols):\n            # Normalize the j-th column of V to get the j-th column of Q\n            norm_vj = np.linalg.norm(V[:, j])\n            \n            # Handle potential linear dependency (column is ~zero)\n            if norm_vj > 1e-12:\n                Q[:, j] = V[:, j] / norm_vj\n            else:\n                Q[:, j] = 0.0\n            \n            # Orthogonalize all subsequent columns of V against the new Q column\n            for k in range(j + 1, num_cols):\n                V[:, k] -= np.dot(Q[:, j], V[:, k]) * Q[:, j]\n                \n        return Q\n\n    for case in test_cases:\n        n = case['n']\n        knots = case['knots']\n        sigma_sq = case['sigma_sq']\n\n        # 1. Create a grid of n inputs\n        x = (np.arange(1, n + 1) - 0.5) / n\n\n        # 2. Construct the raw cubic truncated power basis design matrix X\n        p = 4 + len(knots)\n        X = np.zeros((n, p))\n        \n        # Polynomial part\n        X[:, 0] = 1\n        X[:, 1] = x\n        X[:, 2] = x**2\n        X[:, 3] = x**3\n        \n        # Truncated power part\n        if knots:\n            for j, knot in enumerate(knots):\n                X[:, 4 + j] = np.maximum(x - knot, 0)**3\n        \n        # 3. Compute the theoretical OLS coefficient variance vector for the raw basis\n        # Use float64 for higher precision, especially for inversion\n        X_64 = X.astype(np.float64)\n        gram_X = X_64.T @ X_64\n        try:\n            inv_gram_X = np.linalg.inv(gram_X)\n            v_raw_diag = np.diag(inv_gram_X)\n            v_raw = sigma_sq * v_raw_diag\n        except np.linalg.LinAlgError:\n            # A singular matrix would imply perfect collinearity\n            v_raw = np.full(p, np.inf)\n\n        # 4. Orthogonalize the columns of X to obtain Q\n        Q = modified_gram_schmidt(X)\n\n        # 5. Compute the theoretical OLS coefficient variance vector for the orthonormal basis\n        gram_Q = Q.T @ Q\n        try:\n            inv_gram_Q = np.linalg.inv(gram_Q)\n            v_orth_diag = np.diag(inv_gram_Q)\n            v_orth = sigma_sq * v_orth_diag\n        except np.linalg.LinAlgError:\n            v_orth = np.full(p, np.inf)\n            \n        # 6. Round and store results\n        # The problem statement implies rounding is the final output step,\n        # but for clean storage and formatting, we do it here.\n        v_raw_rounded = np.round(v_raw, 6)\n        v_orth_rounded = np.round(v_orth, 6)\n        \n        results.append([v_raw_rounded.tolist(), v_orth_rounded.tolist()])\n\n    # Final print statement in the exact required format.\n    formatted_results = [format_case_result(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "样条模型不仅能拟合复杂的曲线，更是进行科学假说检验的有力工具。本练习将展示如何使用分段线性样条来检验一个具体的科学问题：数据中是否存在一个“阈值效应”或“转折点”。通过在一个假定的阈值位置放置一个节点，并对相应的斜率变化系数进行$t$检验，你将学会如何量化支持该假说的证据，并进一步通过功效分析来评估模型检验的可靠性。",
            "id": "3157150",
            "problem": "考虑以下指定节点的分段线性样条回归模型，该模型旨在检测在一个可疑的生物阈值处的斜率变化。对于 $[0,1]$ 内的一组固定设计点 $x_1, x_2, \\dots, x_n$，定义铰链函数 $(u)_+ = \\max(u, 0)$，并在选定的节点位置 $\\tau_{\\text{knot}}$ 处，由列向量 $1$、$x$ 和 $(x - \\tau_{\\text{knot}})_+$ 定义工作样条基。回归模型为\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - \\tau_{\\text{knot}})_+ + \\varepsilon_i,\\quad i=1,\\dots,n,\n$$\n其中 $\\varepsilon_i$ 是独立同分布的高斯（正态）随机变量，其均值为 $0$，方差为 $\\sigma^2$，记作 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。\n\n假设真实均值函数在生物阈值 $\\tau_{\\text{true}}$ 处表现出一个幅度为 $\\gamma$ 的斜率变化，由下式给出\n$$\nf_{\\text{true}}(x) = \\gamma \\,(x - \\tau_{\\text{true}})_+.\n$$\n我们希望量化对于以下假设检验的统计功效，该检验的显著性水平为 $\\alpha$：\n$$\nH_0:\\ \\beta_2 = 0 \\quad \\text{versus} \\quad H_1:\\ \\beta_2 \\neq 0,\n$$\n该检验通过在可疑阈值 $\\tau_{\\text{true}}$ 附近指定节点位置 $\\tau_{\\text{knot}}$，来操作性地检测阈值处的陡峭斜率变化。\n\n请仅从普通最小二乘法 (OLS) 的基本定义、高斯误差的性质以及线性回归中经过充分检验的分布事实出发，推导如何计算关于 $\\beta_2$ 的双边 t 检验的统计功效 $P(\\text{reject } H_0 \\mid \\gamma,\\sigma,\\tau_{\\text{true}},\\tau_{\\text{knot}},n)$。您的推导必须从以下几点开始：\n- OLS 估计量的定义 $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}$，其中 $X$ 是由列向量 $1$、$x$ 和 $(x - \\tau_{\\text{knot}})_+$ 构成的 $n \\times p$ 设计矩阵（因此 $p=3$），$\\boldsymbol{y}$ 是响应向量。\n- 正态误差模型 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 I_n)$。\n- 一个经过充分检验的事实：当方差未知并从残差中估计时，单个线性回归系数的 t 统计量在备择假设下服从一个非中心 t 分布 (NCT)，其自由度 (DoF) 为 $n-p$，非中心参数则取决于投影到工作设计上的真实均值。\n\n然后，在一个完整、可运行的程序中实现推导出的计算，该程序需要：\n- 将固定设计点 $x_i$ 构建为 $[0,1]$ 上的 $n$ 个均匀间隔点。\n- 在选定的 $\\tau_{\\text{knot}}$ 处构成工作设计矩阵 $X$。\n- 将 $f_{\\text{true}}(x)$ 投影到 $X$ 的列上，以找到伪真实系数向量 $\\boldsymbol{\\beta}^\\star = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}}$，并使用其第三个分量 $\\beta_2^\\star$ 来参数化铰链系数的双边 t 检验。\n- 使用非中心 t 分布计算在水平 $\\alpha$ 下检验 $H_0:\\beta_2=0$ 的精确双边功效，其自由度 $df=n-p$，非中心参数为 $\\delta = \\beta_2^\\star / (\\sigma \\sqrt{(X^\\top X)^{-1}_{3,3}})$。\n\n您的程序必须为以下测试套件评估功效，该套件涵盖了一般情况、节点位置错误指定、无信号边缘情况、边界阈值放置、伴有陡峭效应的高噪声，以及节点偏移且阈值后点数很少的情况：\n- 情况 1：$n=60$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=2.0$, $\\alpha=0.05$。\n- 情况 2：$n=60$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.55$, $\\gamma=2.0$, $\\alpha=0.05$。\n- 情况 3：$n=30$, $\\sigma=2.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=0.0$, $\\alpha=0.05$。\n- 情况 4：$n=120$, $\\sigma=1.0$, $\\tau_{\\text{true}}=0.10$, $\\tau_{\\text{knot}}=0.10$, $\\gamma=1.5$, $\\alpha=0.05$。\n- 情况 5：$n=120$, $\\sigma=3.0$, $\\tau_{\\text{true}}=0.50$, $\\tau_{\\text{knot}}=0.50$, $\\gamma=5.0$, $\\alpha=0.05$。\n- 情况 6：$n=60$, $\\sigma=0.5$, $\\tau_{\\text{true}}=0.80$, $\\tau_{\\text{knot}}=0.75$, $\\gamma=1.0$, $\\alpha=0.05$。\n\n您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内容为按上述情况顺序排列的结果，例如 $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$。每个 $\\text{result}_k$ 都必须是 $[0,1]$ 内的一个浮点数，等于为情况 k 计算出的统计功效。",
            "solution": "任务是推导和计算分段线性样条回归模型中一个系数的假设检验的统计功效。功效是在特定备择假设为真时正确拒绝零假设的概率。我们将遵循一个系统性的推导，从普通最小二乘法 (OLS) 回归的基础和其估计量的分布理论开始。\n\n**1. 模型设定与问题设置**\n\n所考虑的回归模型是：\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 (x_i - \\tau_{\\text{knot}})_+ + \\varepsilon_i, \\quad i=1,\\dots,n\n$$\n其中 $(u)_+ = \\max(u, 0)$ 是铰链函数，$\\tau_{\\text{knot}}$ 是一个固定的节点，误差项 $\\varepsilon_i$ 是独立同分布 (i.i.d.) 的正态随机变量，$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n用矩阵表示法，模型为 $\\boldsymbol{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中：\n- $\\boldsymbol{y} = [y_1, \\dots, y_n]^\\top$ 是响应向量。\n- $X$ 是 $n \\times 3$ 的设计矩阵，其列对应于基函数：\n$$\nX = \\begin{bmatrix}\n1  x_1  (x_1 - \\tau_{\\text{knot}})_+ \\\\\n1  x_2  (x_2 - \\tau_{\\text{knot}})_+ \\\\\n\\vdots  \\vdots  \\vdots \\\\\n1  x_n  (x_n - \\tau_{\\text{knot}})_+\n\\end{bmatrix}\n$$\n- $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^\\top$ 是系数向量。\n- $\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\dots, \\varepsilon_n]^\\top$ 是误差向量，其分布为 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 I_n)$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\n假设真实数据生成过程的均值函数由下式给出：\n$$\nf_{\\text{true}}(x) = \\gamma (x - \\tau_{\\text{true}})_+\n$$\n这意味着观测数据 $\\boldsymbol{y}$ 是来自模型 $\\boldsymbol{y} = \\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}$ 的实现，其中 $\\boldsymbol{f_{\\text{true}}}$ 是元素为 $f_{\\text{true}}(x_i)$ 的向量。\n\n我们正在以显著性水平 $\\alpha$ 检验零假设 $H_0: \\beta_2 = 0$ 与备择假设 $H_1: \\beta_2 \\neq 0$。\n\n**2. OLS 估计量的分布**\n\n$\\boldsymbol{\\beta}$ 的 OLS 估计量定义为 $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\boldsymbol{y}$。为了在真实数据生成过程下分析其性质，我们代入 $\\boldsymbol{y} = \\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}$：\n$$\n\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top (\\boldsymbol{f_{\\text{true}}} + \\boldsymbol{\\varepsilon}) = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} + (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon}\n$$\n$\\hat{\\boldsymbol{\\beta}}$ 的期望值为：\n$$\nE[\\hat{\\boldsymbol{\\beta}}] = E[(X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}}] + E[(X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon}] = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} + (X^\\top X)^{-1} X^\\top E[\\boldsymbol{\\varepsilon}]\n$$\n由于 $E[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}$，期望简化为：\n$$\nE[\\hat{\\boldsymbol{\\beta}}] = (X^\\top X)^{-1} X^\\top \\boldsymbol{f_{\\text{true}}} \\equiv \\boldsymbol{\\beta}^\\star\n$$\n这个向量 $\\boldsymbol{\\beta}^\\star$ 是“伪真实”参数向量。它表示在由设计矩阵 $X$ 的列向量张成的向量空间内，对真实均值函数 $\\boldsymbol{f_{\\text{true}}}$ 的最佳近似。一般情况下，$\\boldsymbol{\\beta}^\\star$ 不等于 $(0, 0, \\gamma)^\\top$，除非基函数和节点与真实函数完全对齐，并且没有截距项或线性项。\n\n$\\hat{\\boldsymbol{\\beta}}$ 的协方差矩阵是：\n$$\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = E\\left[ (\\hat{\\boldsymbol{\\beta}} - E[\\hat{\\boldsymbol{\\beta}}]) (\\hat{\\boldsymbol{\\beta}} - E[\\hat{\\boldsymbol{\\beta}}])^\\top \\right] = E\\left[ \\left( (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon} \\right) \\left( (X^\\top X)^{-1} X^\\top \\boldsymbol{\\varepsilon} \\right)^\\top \\right]\n$$\n$$\n\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = (X^\\top X)^{-1} X^\\top E[\\boldsymbol{\\varepsilon} \\boldsymbol{\\varepsilon}^\\top] X (X^\\top X)^{-1} = (X^\\top X)^{-1} X^\\top (\\sigma^2 I_n) X (X^\\top X)^{-1} = \\sigma^2 (X^\\top X)^{-1}\n$$\n由于 $\\hat{\\boldsymbol{\\beta}}$ 是高斯向量 $\\boldsymbol{\\varepsilon}$ 的线性变换，它也服从正态分布：\n$$\n\\hat{\\boldsymbol{\\beta}} \\sim \\mathcal{N}(\\boldsymbol{\\beta}^\\star, \\sigma^2 (X^\\top X)^{-1})\n$$\n\n**3. t 统计量及其分布**\n\n我们感兴趣的是系数 $\\beta_2$。估计量 $\\hat{\\beta}_2$ 是 $\\hat{\\boldsymbol{\\beta}}$ 的第三个元素。其分布是单变量正态分布：\n$$\n\\hat{\\beta}_2 \\sim \\mathcal{N}(\\beta_2^\\star, \\sigma^2 C_{33})\n$$\n其中 $\\beta_2^\\star$ 是 $\\boldsymbol{\\beta}^\\star$ 的第三个元素，$C_{33} = [(X^\\top X)^{-1}]_{3,3}$ 是 Gram 矩阵逆的第三个对角元素。$\\hat{\\beta}_2$ 的标准误是 $\\text{se}(\\hat{\\beta}_2) = \\sigma \\sqrt{C_{33}}$。\n\n实践中，$\\sigma^2$ 是未知的，必须从数据中估计。其无偏估计量是 $\\hat{\\sigma}^2 = \\frac{1}{n-p} \\text{RSS}$，其中 $p=3$ 是参数的数量，$\\text{RSS} = \\|\\boldsymbol{y} - X\\hat{\\boldsymbol{\\beta}}\\|^2$ 是残差平方和。检验统计量用估计的标准误 $\\hat{\\text{se}}(\\hat{\\beta}_2) = \\hat{\\sigma}\\sqrt{C_{33}}$ 替代真实的标准误：\n$$\nT = \\frac{\\hat{\\beta}_2 - 0}{\\hat{\\text{se}}(\\hat{\\beta}_2)} = \\frac{\\hat{\\beta}_2}{\\hat{\\sigma}\\sqrt{C_{33}}}\n$$\n在 $E[\\hat{\\beta}_2] = \\beta_2^\\star \\neq 0$ 的一般备择假设下，该统计量服从一个非中心 t 分布。自由度 ($df$) 是与方差估计相关联的自由度，即 $df = n-p = n-3$。非中心参数 (NCP) $\\delta$ 是分子的标准化均值：\n$$\n\\delta = \\frac{E[\\hat{\\beta}_2]}{\\text{se}(\\hat{\\beta}_2)} = \\frac{\\beta_2^\\star}{\\sigma\\sqrt{C_{33}}}\n$$\n因此，在由 $\\boldsymbol{f_{\\text{true}}}$ 指定的备择假设下，检验统计量的分布为 $T \\sim t(df, \\delta)$，这是一个自由度为 $df=n-3$、非中心参数为 $\\delta$ 的非中心 t 分布。\n\n**4. 功效计算**\n\n对于一个显著性水平为 $\\alpha$ 的双边检验，如果检验统计量的观测值 $|T|$ 超过一个临界值，我们就拒绝零假设 $H_0: \\beta_2=0$。这个我们记为 $t_{\\text{crit}}$ 的临界值是由零分布决定的。在 $H_0$ 下，$\\gamma=0$，这意味着 $\\boldsymbol{f_{\\text{true}}} = \\boldsymbol{0}$，$\\boldsymbol{\\beta}^\\star = \\boldsymbol{0}$，且 $\\delta = 0$。因此零分布是一个中心 t 分布，$T \\sim t(df)$。临界值 $t_{\\text{crit}}$ 是这个中心 t 分布的上 $(1-\\alpha/2)$ 分位数：\n$$\nt_{\\text{crit}} = t_{df, 1-\\alpha/2}\n$$\n统计功效是在备择假设为真时（即当 $T \\sim t(df, \\delta)$ 时）拒绝 $H_0$ 的概率。\n$$\n\\text{Power} = P(|T| > t_{\\text{crit}} \\mid T \\sim t(df, \\delta)) = P(T > t_{\\text{crit}}) + P(T  -t_{\\text{crit}})\n$$\n设 $F_{t(df, \\delta)}$ 为非中心 t 分布的累积分布函数 (CDF)。功效则计算如下：\n$$\n\\text{Power} = \\left( 1 - F_{t(df, \\delta)}(t_{\\text{crit}}) \\right) + F_{t(df, \\delta)}(-t_{\\text{crit}})\n$$\n\n**5. 计算步骤**\n\n为每种情况计算功效的步骤如下：\n1.  给定 $n, \\sigma, \\tau_{\\text{true}}, \\tau_{\\text{knot}}, \\gamma, \\alpha$。\n2.  将设计点 $x_i$ 定义为从 $0$ 到 $1$（含）的 $n$ 个等距点。\n3.  构建 $n \\times 3$ 的设计矩阵 $X$，其列为 $1$、$x_i$ 和 $(x_i - \\tau_{\\text{knot}})_+$。\n4.  构建 $n \\times 1$ 的真实均值向量 $\\boldsymbol{f}_{\\text{true}}$，其元素为 $\\gamma (x_i - \\tau_{\\text{true}})_+$。\n5.  计算矩阵乘积 $X^\\top X$ 及其逆 $(X^\\top X)^{-1}$。\n6.  计算伪真实参数向量 $\\boldsymbol{\\beta}^\\star = (X^\\top X)^{-1} X^\\top \\boldsymbol{f}_{\\text{true}}$。\n7.  提取第三个分量 $\\beta_2^\\star = \\boldsymbol{\\beta}^\\star_2$。\n8.  提取逆矩阵的第三个对角元素 $C_{33} = [(X^\\top X)^{-1}]_{2,2}$（对第三行/列使用 0-基索引）。\n9.  计算非中心参数 $\\delta = \\beta_2^\\star / (\\sigma \\sqrt{C_{33}})$。\n10. 设置自由度 $df = n - 3$。\n11. 从中心 t 分布的逆 CDF（或百分点函数）中找到临界值 $t_{\\text{crit}}$：$t_{\\text{crit}} = F^{-1}_{t(df)}(1 - \\alpha/2)$。\n12. 使用具有 $df$ 和 $\\delta$ 的非中心 t 分布的 CDF 计算功效：$\\text{Power} = 1 - F_{t(df, \\delta)}(t_{\\text{crit}}) + F_{t(df, \\delta)}(-t_{\\text{crit}})$。\n\n至此完成了算法的推导和说明。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, nct\n\ndef solve():\n    \"\"\"\n    Computes the statistical power for a hypothesis test on a coefficient in a\n    piecewise-linear spline regression model for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: n=60, sigma=1.0, tau_true=0.50, tau_knot=0.50, gamma=2.0, alpha=0.05\n        {'n': 60, 'sigma': 1.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 2.0, 'alpha': 0.05},\n        # Case 2: n=60, sigma=1.0, tau_true=0.50, tau_knot=0.55, gamma=2.0, alpha=0.05\n        {'n': 60, 'sigma': 1.0, 'tau_true': 0.50, 'tau_knot': 0.55, 'gamma': 2.0, 'alpha': 0.05},\n        # Case 3: n=30, sigma=2.0, tau_true=0.50, tau_knot=0.50, gamma=0.0, alpha=0.05\n        {'n': 30, 'sigma': 2.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 0.0, 'alpha': 0.05},\n        # Case 4: n=120, sigma=1.0, tau_true=0.10, tau_knot=0.10, gamma=1.5, alpha=0.05\n        {'n': 120, 'sigma': 1.0, 'tau_true': 0.10, 'tau_knot': 0.10, 'gamma': 1.5, 'alpha': 0.05},\n        # Case 5: n=120, sigma=3.0, tau_true=0.50, tau_knot=0.50, gamma=5.0, alpha=0.05\n        {'n': 120, 'sigma': 3.0, 'tau_true': 0.50, 'tau_knot': 0.50, 'gamma': 5.0, 'alpha': 0.05},\n        # Case 6: n=60, sigma=0.5, tau_true=0.80, tau_knot=0.75, gamma=1.0, alpha=0.05\n        {'n': 60, 'sigma': 0.5, 'tau_true': 0.80, 'tau_knot': 0.75, 'gamma': 1.0, 'alpha': 0.05},\n    ]\n\n    results = []\n    for params in test_cases:\n        # Extract parameters for the current case\n        n = params['n']\n        sigma = params['sigma']\n        tau_true = params['tau_true']\n        tau_knot = params['tau_knot']\n        gamma = params['gamma']\n        alpha = params['alpha']\n        \n        # 1. Construct the fixed design points x_i as n evenly spaced points on [0,1]\n        x = np.linspace(0.0, 1.0, n)\n        \n        # 2. Form the working design matrix X\n        # Columns are 1, x, and (x - tau_knot)_+\n        p = 3\n        X = np.zeros((n, p))\n        X[:, 0] = 1.0\n        X[:, 1] = x\n        X[:, 2] = np.maximum(0, x - tau_knot)\n\n        # 3. Construct the true mean function vector f_true\n        f_true = gamma * np.maximum(0, x - tau_true)\n        \n        # 4. Compute the pseudo-true coefficient vector beta_star\n        # beta_star = (X^T X)^-1 X^T f_true\n        try:\n            XTX_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            # This case occurs if X has linearly dependent columns.\n            # For the given knots, X should have full rank.\n            # E.g., if tau_knot >= 1, the third column is all zeros.\n            results.append(np.nan) # Mark as not-a-number if problem occurs\n            continue\n\n        beta_star = XTX_inv @ X.T @ f_true\n        \n        # 5. Extract the third component, beta_2_star\n        beta_2_star = beta_star[2]\n        \n        # 6. Extract the variance term for beta_2_hat from (X^T X)^-1\n        C_33 = XTX_inv[2, 2]\n        \n        # 7. Calculate the noncentrality parameter (NCP), delta\n        delta = beta_2_star / (sigma * np.sqrt(C_33))\n        \n        # 8. Define degrees of freedom for the t-test\n        df = n - p\n        \n        # 9. Find the critical value for the two-sided test\n        t_crit = t.ppf(1 - alpha / 2, df)\n        \n        # 10. Compute the power using the Noncentral t (NCT) distribution\n        # Power = P(T > t_crit) + P(T  -t_crit) where T ~ NCT(df, delta)\n        power = 1.0 - nct.cdf(t_crit, df, delta) + nct.cdf(-t_crit, df, delta)\n        \n        results.append(power)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "标准的最小二乘回归关注的是条件均值，但在许多应用中，我们对条件分布的其他特征（如中位数或不同分位数）更感兴趣。分位数回归为我们提供了超越均值建模的强大能力，而将其与样条结合，则可以灵活地拟合非线性的分位数函数。这项高级实践将指导你使用三次样条拟合分位数回归模型，并研究模型对节点位置选择的敏感性，从而加深对稳健非线性建模的理解。",
            "id": "3157118",
            "problem": "给定您构建和分析分位数回归样条的任务，该任务在统计学习中属于分段多项式、节点和样条的框架内。您应当从通过 pinball 损失定义分位数以及通过截断幂基表示回归样条这两个基本定义出发。仅依赖这些基础定义和经过充分检验的事实；不要假设任何预先推导出的快捷公式。\n\n使用截断幂表示法构建一个三次回归样条基。对于一组输入 $x \\in \\mathbb{R}$，一个内部节点集 $\\mathcal{K} = \\{k_{1},\\dots,k_{m}\\}$（其中 $k_{j} \\in \\mathbb{R}$）以及次数 $p = 3$，将基函数定义为最高三次的多项式项与截断三次项的并集：\n$$\n\\{1, x, x^{2}, x^{3}\\} \\cup \\{(x - k_{j})_{+}^{3} \\,:\\, j = 1,\\dots,m\\},\n$$\n其中 $(t)_{+} = \\max\\{t, 0\\}$。\n\n设分位数水平为 $\\tau \\in (0,1)$ 时，对于一个残差 $u \\in \\mathbb{R}$ 的 pinball 损失定义为\n$$\n\\rho_{\\tau}(u) = u \\cdot \\left(\\tau - \\mathbb{I}\\{u  0\\}\\right).\n$$\n在水平 $\\tau$ 上的分位数回归通过最小化经验风险 $\\sum_{i=1}^{n} \\rho_{\\tau}(y_{i} - f_{\\beta}(x_{i}))$ 来估计系数 $\\beta \\in \\mathbb{R}^{d}$，其中 $f_{\\beta}(x)$ 是由上述基函数导出的样条模型。您必须从上述定义出发，以一种对于凸分段线性目标而言数值精确的方式，来构建并解决此问题。\n\n数据。使用以下固定的数据集。设 $n = 41$ 并定义 $x_{i} = i/40$ 对于 $i \\in \\{0,1,\\dots,40\\}$，因此 $x_{i} \\in [0,1]$。定义确定性响应\n$$\ny_{i} = \\sin(2\\pi x_{i}) + 0.3 \\cdot \\sin(5\\pi x_{i}) + 0.1 \\cdot x_{i}.\n$$\n不允许有随机性。\n\n任务。您将通过比较不同节点配置下的拟合函数，研究分位数样条估计对节点位置的敏感性。对于下面的每项任务，使用相同的次数 $p = 3$ 和指定的分位数水平 $\\tau$ 来拟合两个样条模型，但使用不同的内部节点集。然后计算每种情况下定义的敏感性度量。\n\n- 测试一（中位数，网格最大差异）。分位数水平 $\\tau = 0.5$。使用内部节点 $\\mathcal{K}_{A} = \\{0.25, 0.5, 0.75\\}$ 和 $\\mathcal{K}_{B} = \\{0.2, 0.5, 0.8\\}$。在训练数据上拟合两个模型。在 $[0,1]$ 上定义一个包含 $201$ 个点的均匀评估网格，并计算\n$$\nS_{1} = \\max_{x \\in \\text{grid}} \\left| \\hat{f}_{A,\\tau}(x) - \\hat{f}_{B,\\tau}(x) \\right|.\n$$\n\n- 测试二（上分位数，网格最大差异）。分位数水平 $\\tau = 0.9$。使用与测试一中相同的节点，即 $\\mathcal{K}_{A} = \\{0.25, 0.5, 0.75\\}$ 和 $\\mathcal{K}_{B} = \\{0.2, 0.5, 0.8\\}$。在 $[0,1]$ 上定义相同的包含 $201$ 个点的均匀评估网格，并计算\n$$\nS_{2} = \\max_{x \\in \\text{grid}} \\left| \\hat{f}_{A,\\tau}(x) - \\hat{f}_{B,\\tau}(x) \\right|.\n$$\n\n- 测试三（边界情况，无节点与单节点）。分位数水平 $\\tau = 0.5$。比较一个没有内部节点 $\\mathcal{K}_{A} = \\varnothing$ 的全局三次多项式与一个有一个内部节点 $\\mathcal{K}_{B} = \\{0.5\\}$ 的样条。使用 $[0,1]$ 上相同的 $201$ 点网格，计算\n$$\nS_{3} = \\max_{x \\in \\text{grid}} \\left| \\hat{f}_{A,\\tau}(x) - \\hat{f}_{B,\\tau}(x) \\right|.\n$$\n\n- 测试四（在训练点上的局部敏感性，上分位数）。分位数水平 $\\tau = 0.9$。使用 $\\mathcal{K}_{A} = \\{0.33, 0.66\\}$ 和 $\\mathcal{K}_{B} = \\{0.34, 0.66\\}$。计算在训练输入 $\\{x_{i}\\}_{i=1}^{n}$ 上评估的两个拟合的均方根偏差：\n$$\nS_{4} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{f}_{A,\\tau}(x_{i}) - \\hat{f}_{B,\\tau}(x_{i}) \\right)^{2}}.\n$$\n\n程序要求。\n\n- 实现一个函数，使用截断幂表示法为任何给定的内部节点集构建三次回归样条基。\n\n- 实现一个在给定 $\\tau$ 下的分位数回归求解器，该求解器对于由 pinball 损失导出的凸分段线性目标是数值精确的。您可以引入与定义一致的辅助变量，将问题转换为可由标准凸优化工具求解的形式。\n\n- 严格按照上述规定使用数据集和测试套件。不允许有随机性。\n\n- 数值评估网格：在 $[0,1]$ 上精确使用 $201$ 个等距点，包括两个端点。\n\n- 输出规范：您的程序应生成单行输出，包含结果 $[S_{1}, S_{2}, S_{3}, S_{4}]$，形式为逗号分隔的列表，并用方括号括起来，每个值四舍五入到 $6$ 位小数，顺序和格式必须完全一致，例如 $[0.123456,0.234567,0.345678,0.456789]$。\n\n注意。\n\n- 此问题中没有物理单位。\n\n- 所有角度均以弧度为单位，所有三角函数均为标准函数。\n\n- $S_{1}, S_{2}, S_{3}, S_{4}$ 中的每一个都必须是单个实数（一个浮点数）。",
            "solution": "该问题定义明确、科学合理且内部一致。所有必需的数据、函数和评估标准都已提供。任务是使用三次样条进行分位数回归，这是统计学习中的一种标准技术。通过将目标函数描述为凸且分段线性的，暗示了解决分位数回归优化问题的方法，对此，线性规划（$LP$）是精确且合适的数值方法。\n\n### 基于原则的设计\n\n1.  **模型设定**：模型是一个使用截断幂基定义的三次样条函数 $f_{\\beta}(x)$。对于给定的输入点集 $x$，次数 $p=3$，以及 $m$ 个内部节点集 $\\mathcal{K} = \\{k_1, \\dots, k_m\\}$，该函数是基函数的线性组合：\n    $$\n    f_{\\beta}(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\sum_{j=1}^{m} \\beta_{3+j} (x - k_j)_+^3\n    $$\n    这可以紧凑地写为 $f_{\\beta}(x) = H_x \\beta$，其中 $H_x$ 是在 $x$ 处评估的基函数的行向量，$\\beta$ 是系数向量。系数总数为 $d = p+1+m = 4+m$。\n\n2.  **分位数回归目标**：目标是找到系数向量 $\\beta$，以最小化经验风险，该风险定义为 $n$ 个数据点上 pinball 损失的总和：\n    $$\n    \\min_{\\beta \\in \\mathbb{R}^d} \\sum_{i=1}^{n} \\rho_{\\tau}(y_i - f_{\\beta}(x_i))\n    $$\n    其中，对于残差 $u_i = y_i - f_{\\beta}(x_i)$，在分位数水平 $\\tau \\in (0,1)$ 上的 pinball 损失由 $\\rho_{\\tau}(u_i) = u_i(\\tau - \\mathbb{I}\\{u_i  0\\})$ 给出。\n\n3.  **线性规划公式化**：pinball 损失是一个凸的分段线性函数。目标函数作为此类函数的总和，也是凸的。这使得该优化问题可以转化为一个标准的线性规划（$LP$）问题。我们为每个数据点 $i \\in \\{1, \\dots, n\\}$ 引入非负辅助变量 $u_i^+$ 和 $u_i^-$。我们将残差 $u_i$ 分解为 $u_i = u_i^+ - u_i^-$，其中 $u_i^+ = \\max(u_i, 0)$ 和 $u_i^- = \\max(-u_i, 0)$。pinball 损失可以重新表示为这些新变量的线性函数：$\\rho_{\\tau}(u_i) = \\tau u_i^+ + (1-\\tau)u_i^-$。\n\n    优化问题现在变为：\n    $$\n    \\min_{\\beta, u^+, u^-} \\sum_{i=1}^{n} \\left( \\tau u_i^+ + (1-\\tau)u_i^- \\right)\n    $$\n    受制于 $i=1, \\dots, n$ 的约束条件：\n    $$\n    f_{\\beta}(x_i) + u_i^+ - u_i^- = y_i\n    $$\n    $$\n    u_i^+ \\ge 0, \\quad u_i^- \\ge 0\n    $$\n    系数 $\\beta_j$ 的符号无约束。标准 LP 求解器要求所有变量都为非负。因此，我们将每个系数 $\\beta_j$ 分解为其正部和负部：$\\beta_j = \\beta_j^+ - \\beta_j^-$，其中 $\\beta_j^+ \\ge 0$ 且 $\\beta_j^- \\ge 0$。\n\n    设 $H$ 为 $n \\times d$ 的设计矩阵，其中 $H_{ij}$ 是在 $x_i$ 处评估的第 $j$ 个基函数。最终的 LP 公式为：\n    $$\n    \\min_{\\beta^+, \\beta^-, u^+, u^-} \\left( \\mathbf{0}^T\\beta^+ + \\mathbf{0}^T\\beta^- + \\tau \\mathbf{1}^T u^+ + (1-\\tau)\\mathbf{1}^T u^- \\right)\n    $$\n    受制于：\n    $$\n    H\\beta^+ - H\\beta^- + I u^+ - I u^- = \\mathbf{y}\n    $$\n    $$\n    \\beta^+ \\ge \\mathbf{0}, \\quad \\beta^- \\ge \\mathbf{0}, \\quad u^+ \\ge \\mathbf{0}, \\quad u^- \\ge \\mathbf{0}\n    $$\n    这符合典范形式 $\\min \\mathbf{c}^T\\mathbf{z}$，受制于 $\\mathbf{A}_{eq}\\mathbf{z} = \\mathbf{b}_{eq}$ 和 $\\mathbf{z} \\ge \\mathbf{0}$，可以被高效求解。\n\n4.  **实现与评估**：\n    *   实现一个函数，用于为任何给定的点集 $x$ 和节点集 $\\mathcal{K}$，基于三次截断幂基构建设计矩阵 $H$。\n    *   一个求解器函数接收训练数据 $(x_i, y_i)$、节点集 $\\mathcal{K}$ 和分位数水平 $\\tau$，组装上述 LP 的各部分（$\\mathbf{c}, \\mathbf{A}_{eq}, \\mathbf{b}_{eq}$），并使用 `scipy.optimize.linprog` 找到最优解向量 $\\mathbf{z}$。从 $\\mathbf{z}$ 中提取系数向量 $\\beta^+$ 和 $\\beta^-$，并恢复最终的样条系数向量为 $\\beta = \\beta^+ - \\beta^-$。\n    *   对于四个指定的测试中的每一个，使用各自的节点集 $\\mathcal{K}_A$ 和 $\\mathcal{K}_B$ 拟合两个样条模型（$\\hat{f}_{A,\\tau}$ 和 $\\hat{f}_{B,\\tau}$）。\n    *   然后使用拟合的模型在指定的评估点上（对于测试 1-3 是一个包含 201 个点的精细网格，对于测试 4 是训练点）预测值。\n    *   根据这些预测计算所需的敏感性度量（$S_1, S_2, S_3, S_4$），以衡量每个测试中两个拟合函数之间的差异。收集结果并按规定格式化。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes quantile regression splines by solving a series of\n    linear programming problems.\n    \"\"\"\n\n    def create_design_matrix(x, knots, p=3):\n        \"\"\"\n        Constructs the design matrix for a polynomial spline with a truncated power basis.\n        \n        Args:\n            x (np.ndarray): Input data points.\n            knots (list or np.ndarray): Internal knot locations.\n            p (int): Degree of the polynomial spline.\n\n        Returns:\n            np.ndarray: The design matrix H.\n        \"\"\"\n        x_flat = np.asarray(x).flatten()\n        \n        # Polynomial basis terms: 1, x, x^2, x^3\n        H_poly = np.vander(x_flat, N=p + 1, increasing=True)\n\n        if not knots:\n            return H_poly\n\n        knots_arr = np.asarray(knots)\n        # Truncated power basis terms: (x - k)_+^p\n        # The shape is (len(x), len(knots))\n        H_trunc = np.maximum(0, x_flat[:, np.newaxis] - knots_arr[np.newaxis, :])**p\n        \n        return np.hstack([H_poly, H_trunc])\n\n    def fit_quantile_spline(x_train, y_train, knots, tau):\n        \"\"\"\n        Fits a quantile regression spline by formulating and solving a linear program.\n\n        Args:\n            x_train (np.ndarray): Training input data.\n            y_train (np.ndarray): Training response data.\n            knots (list): Internal knot locations.\n            tau (float): The quantile level, in (0, 1).\n\n        Returns:\n            np.ndarray: The estimated spline coefficients beta.\n        \"\"\"\n        n = len(x_train)\n        H = create_design_matrix(x_train, knots, p=3)\n        d = H.shape[1] # Number of basis functions\n\n        # Formulate the Linear Program for quantile regression\n        # Variables z = [beta+, beta-, u+, u-]\n        # size = d + d + n + n = 2d + 2n\n        \n        # Objective vector c\n        c = np.concatenate([np.zeros(2 * d),\n                            tau * np.ones(n),\n                            (1 - tau) * np.ones(n)])\n        \n        # Equality constraints matrix A_eq\n        # H*beta+ - H*beta- + I*u+ - I*u- = y\n        A_eq = np.hstack([H, -H, np.eye(n), -np.eye(n)])\n        \n        # Equality constraints vector b_eq\n        b_eq = y_train\n\n        # All variables are non-negative, so bounds are (0, None). This is the default.\n        res = linprog(c, A_eq=A_eq, b_eq=b_eq, method='highs')\n\n        if not res.success:\n            raise RuntimeError(f\"LP solver failed for knots={knots}, tau={tau}: {res.message}\")\n\n        solution = res.x\n        beta_plus = solution[:d]\n        beta_minus = solution[d:2*d]\n        beta = beta_plus - beta_minus\n        \n        return beta\n\n    def predict(x_eval, knots, beta):\n        \"\"\"\n        Predicts response values using a fitted spline model.\n\n        Args:\n            x_eval (np.ndarray): Points to evaluate the model at.\n            knots (list): The knots used to fit the model.\n            beta (np.ndarray): The fitted spline coefficients.\n\n        Returns:\n            np.ndarray: The predicted values.\n        \"\"\"\n        H_eval = create_design_matrix(x_eval, knots, p=3)\n        return H_eval @ beta\n\n    # Define the fixed dataset\n    n = 41\n    x_train = np.linspace(0, 1, n)\n    y_train = np.sin(2 * np.pi * x_train) + 0.3 * np.sin(5 * np.pi * x_train) + 0.1 * x_train\n\n    # Define the evaluation grid\n    x_grid = np.linspace(0, 1, 201)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Test One\n        {'tau': 0.5, 'knots_A': [0.25, 0.5, 0.75], 'knots_B': [0.2, 0.5, 0.8], 'metric': 'max_abs_diff_grid'},\n        # Test Two\n        {'tau': 0.9, 'knots_A': [0.25, 0.5, 0.75], 'knots_B': [0.2, 0.5, 0.8], 'metric': 'max_abs_diff_grid'},\n        # Test Three\n        {'tau': 0.5, 'knots_A': [], 'knots_B': [0.5], 'metric': 'max_abs_diff_grid'},\n        # Test Four\n        {'tau': 0.9, 'knots_A': [0.33, 0.66], 'knots_B': [0.34, 0.66], 'metric': 'rmsd_train'}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        tau = case['tau']\n        knots_A = case['knots_A']\n        knots_B = case['knots_B']\n        \n        # Fit model A\n        beta_A = fit_quantile_spline(x_train, y_train, knots_A, tau)\n        \n        # Fit model B\n        beta_B = fit_quantile_spline(x_train, y_train, knots_B, tau)\n\n        if case['metric'] == 'max_abs_diff_grid':\n            pred_A = predict(x_grid, knots_A, beta_A)\n            pred_B = predict(x_grid, knots_B, beta_B)\n            metric_val = np.max(np.abs(pred_A - pred_B))\n        elif case['metric'] == 'rmsd_train':\n            pred_A = predict(x_train, knots_A, beta_A)\n            pred_B = predict(x_train, knots_B, beta_B)\n            metric_val = np.sqrt(np.mean((pred_A - pred_B)**2))\n        else:\n            raise ValueError(f\"Unknown metric: {case['metric']}\")\n            \n        results.append(metric_val)\n\n    # Format and print the final output\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        }
    ]
}