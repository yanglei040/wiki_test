{
    "hands_on_practices": [
        {
            "introduction": "我们从一个基础但至关重要的思想实验开始。在实践中，我们可能会遇到多个模型在训练数据上表现完全相同的情况。成本复杂性剪枝如何在这种情况下做出选择？本练习  将通过一个精心设计的场景，让您亲手计算并理解正则化参数 $\\alpha$ 是如何通过惩罚模型复杂度来打破这种僵局，并选择出更简洁的模型。",
            "id": "3189470",
            "problem": "考虑一个二元分类任务和一棵由分类与回归树 (CART) 构建的大树。假设这棵大树的一个固定的内部节点 $t$ 是一棵子树的根，并假设在节点 $t$ 处对该子树进行剪枝的两个候选方案是：一棵较深的子树 $T_{A}$ 和一棵较浅的子树 $T_{B}$。对于每棵子树 $T$，其在 $0$–$1$ 损失下的经验训练误差定义为\n$$\nR(T) \\equiv \\sum_{l \\in \\text{Leaves}(T)} \\sum_{i \\in l} \\mathbf{1}\\!\\left(y_{i} \\neq \\hat{y}_{l}\\right),\n$$\n其中 $y_{i}$ 是训练点 $i$ 的类别标签，$\\hat{y}_{l}$ 是叶节点的预测值（即叶节点 $l$ 中的多数类）。令 $|\\!T\\!|$ 表示子树 $T$ 的终端节点（叶节点）数量。\n\n设计以下场景。对于较深的子树 $T_{A}$，其叶节点级别的错分类计数由多重集 $\\{2,1,0,3,1,0,2,3\\}$ 给出，因此 $|\\!T_{A}\\!| = 8$ 且 $R(T_{A}) = 12$。对于较浅的子树 $T_{B}$，其叶节点级别的错分类计数由多重集 $\\{4,0,2,3,3\\}$ 给出，因此 $|\\!T_{B}\\!| = 5$ 且 $R(T_{B}) = 12$。从经验风险最小化原理以及应对模型复杂度进行惩罚的结构风险最小化思想出发，考虑以下成本复杂度准则\n$$\nR_{\\alpha}(T) \\equiv R(T) + \\alpha \\, |\\!T\\!|,\n$$\n其中 $\\alpha \\geq 0$ 是一个正则化参数。\n\n仅使用这些基本定义，解释成本复杂度剪枝如何通过改变 $\\alpha$ 来解决 $T_{A}$ 和 $T_{B}$ 在 $R(T)$ 上的平局情况，并推导出确切的阈值 $\\alpha^{\\star}$，在该阈值下，一棵子树在 $R_{\\alpha}(\\cdot)$ 准则下严格优于另一棵子树（即，使得偏好的子树的 $R_{\\alpha}$ 值严格小于另一棵子树的最小 $\\alpha$ 值）。请将 $\\alpha^{\\star}$ 的最终答案以单个实数形式给出。无需四舍五入。",
            "solution": "首先验证问题陈述，以确保其科学基础扎实、问题定义明确、客观且完整。\n\n### 步骤 1：提取已知条件\n- 一个使用 CART 构建的树的二元分类任务。\n- 两个以同一内部节点 $t$ 为根的备选子树剪枝方案，$T_{A}$ 和 $T_{B}$。\n- 经验训练误差定义为 $R(T) \\equiv \\sum_{l \\in \\text{Leaves}(T)} \\sum_{i \\in l} \\mathbf{1}\\!\\left(y_{i} \\neq \\hat{y}_{l}\\right)$。\n- 树 $T$ 的复杂度是其叶节点数量，记为 $|\\!T\\!|$。\n- 对于较深的子树 $T_{A}$：\n  - 叶节点数量：$|\\!T_{A}\\!| = 8$。\n  - 总经验误差：$R(T_{A}) = 12$。\n- 对于较浅的子树 $T_{B}$：\n  - 叶节点数量：$|\\!T_{B}\\!| = 5$。\n  - 总经验误差：$R(T_{B}) = 12$。\n- 成本复杂度准则定义为 $R_{\\alpha}(T) \\equiv R(T) + \\alpha \\, |\\!T\\!|$，其中正则化参数 $\\alpha \\geq 0$。\n- 目标是解释成本复杂度剪枝如何解决 $R(T)$ 的平局情况，并找到阈值 $\\alpha^{\\star}$，该值定义为使得一棵子树严格占优的最小 $\\alpha$。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学基础扎实**：该问题基于分类与回归树 (CART) 的标准且成熟的理论以及成本复杂度剪枝，这是统计学习中的一个核心概念。定义和设置都是正确的。\n- **问题定义明确**：问题陈述清晰，包含了所有必要的量化信息。它要求一个特定的值 $\\alpha^{\\star}$，这个值可以从给定的数据和定义中唯一确定。\n- **客观性**：问题以精确的数学语言表述，没有主观性或歧义。\n- 问题内部一致，因为所述的总误差 $R(T_A)$ 和 $R(T_B)$ 与提供的叶节点级别错分类计数的总和相符。没有矛盾或信息缺失。\n\n### 步骤 3：结论与行动\n问题有效。可以构建一个合理的解决方案。\n\n### 求解推导\n\n问题要求我们比较两棵候选子树 $T_{A}$ 和 $T_{B}$，它们具有相同的经验训练误差但复杂度不同。这种情况突显了结构风险最小化的核心原则，通常与奥卡姆剃刀相关联：当两个模型在训练数据上表现同样出色时，应优先选择更简单的那个。成本复杂度剪枝提供了一个实现这一原则的正式机制。\n\n成本复杂度准则由 $R_{\\alpha}(T) = R(T) + \\alpha |\\!T\\!|$ 给出。其中，$R(T)$ 是经验风险（衡量训练数据拟合程度的指标），$\\alpha |\\!T\\!|$ 是惩罚模型复杂度的惩罚项，其中 $|\\!T\\!|$ 是复杂度的度量，$\\alpha \\geq 0$ 是控制权衡的参数。\n\n我们已知两棵子树的以下值：\n对于子树 $T_{A}$：\n- 经验误差：$R(T_{A}) = 12$\n- 复杂度（叶节点数量）：$|\\!T_{A}\\!| = 8$\n\n对于子树 $T_{B}$：\n- 经验误差：$R(T_{B}) = 12$\n- 复杂度（叶节点数量）：$|\\!T_{B}\\!| = 5$\n\n让我们将每棵子树的成本复杂度写成 $\\alpha$ 的函数：\n$$\nR_{\\alpha}(T_{A}) = R(T_{A}) + \\alpha |\\!T_{A}\\!| = 12 + 8\\alpha\n$$\n$$\nR_{\\alpha}(T_{B}) = R(T_{B}) + \\alpha |\\!T_{B}\\!| = 12 + 5\\alpha\n$$\n\n剪枝的目标是对于给定的 $\\alpha$，选择具有最小成本复杂度 $R_{\\alpha}(T)$ 的子树。\n\n首先，我们分析 $\\alpha = 0$ 的情况。这对应于纯粹的经验风险最小化，即没有对复杂度的惩罚。\n$$\nR_{0}(T_{A}) = 12 + 8(0) = 12\n$$\n$$\nR_{0}(T_{B}) = 12 + 5(0) = 12\n$$\n在这种情况下，$R_{0}(T_{A}) = R_{0}(T_{B})$。仅基于经验风险，没有理由偏好其中一棵子树；它们是平局。\n\n接下来，我们考虑 $\\alpha > 0$ 的情况。这引入了对复杂度的惩罚。结构风险最小化原则表明，应优先选择更简单的树 $T_{B}$。让我们通过比较 $R_{\\alpha}(T_{A})$ 和 $R_{\\alpha}(T_{B})$ 来验证这一点。我们寻找其中一个严格小于另一个的条件。\n我们来确定对于哪些 $\\alpha$ 值，子树 $T_{B}$ 是更优选择，即 $R_{\\alpha}(T_{B})  R_{\\alpha}(T_{A})$。\n$$\n12 + 5\\alpha  12 + 8\\alpha\n$$\n两边同时减去 $12$ 得到：\n$$\n5\\alpha  8\\alpha\n$$\n两边同时减去 $5\\alpha$：\n$$\n0  3\\alpha\n$$\n由于 $\\alpha$ 是一个非负参数，这个不等式对所有 $\\alpha > 0$ 都成立。\n\n这个结果解释了成本复杂度剪枝如何解决平局。只要对复杂度施加任何正向惩罚（即，对于任何 $\\alpha > 0$），平局就会被打破。因为 $T_{B}$ 更简单（$|\\!T_{B}\\!|  |\\!T_{A}\\!|$），它的惩罚项 $\\alpha |\\!T_{B}\\!|$ 小于 $T_{A}$ 的惩罚项，使其总成本复杂度 $R_{\\alpha}(T_{B})$ 严格更小。因此，对于任何 $\\alpha > 0$，$T_{B}$ 都是严格占优的子树。\n\n问题要求的是“一棵子树严格优于另一棵子树的阈值 $\\alpha^{\\star}$”。这个阈值是标记了无差异区域和严格偏好区域之间边界的 $\\alpha$ 值。我们可以通过将成本复杂度设为相等并求解 $\\alpha$ 来找到这个边界点：\n$$\nR_{\\alpha}(T_{A}) = R_{\\alpha}(T_{B})\n$$\n$$\n12 + 8\\alpha = 12 + 5\\alpha\n$$\n$$\n3\\alpha = 0\n$$\n$$\n\\alpha = 0\n$$\n这个值 $\\alpha = 0$ 是转换点。在 $\\alpha = 0$ 时，成本相等。对于任何大于 0 的 $\\alpha$ 值，都会出现对 $T_{B}$ 的严格偏好。问题将 $\\alpha^{\\star}$ 定义为存在严格偏好的最小 $\\alpha$。存在严格偏好的 $\\alpha$ 值集合是 $(0, \\infty)$。满足此条件的最小值是任意小的 $\\epsilon > 0$。问题要求的是阈值，也就是这个集合的下确界。集合 $(0, \\infty)$ 的下确界是 $0$。\n\n因此，出现严格偏好的阈值 $\\alpha^{\\star}$ 是 $0$。",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "理解了为何要进行剪枝后，下一步是掌握其具体机制。本练习  深入探讨了“最弱环节剪枝”（weakest-link pruning）算法的计算过程，这是生成一系列最优子树序列的核心方法。您将从一棵给定的最大树开始，通过计算每个内部节点的剪枝“成本效益”，逐步确定剪枝路径上的关键 $\\alpha$ 值，从而揭示剪枝的动态过程。",
            "id": "3189468",
            "problem": "一个回归树在训练集上生长为一棵已知的最大树 $T_{\\max}$，并将使用代价复杂度准则进行剪枝。令 $R(T)$ 表示树 $T$ 在训练集上的经验残差平方和，令 $|T|$ 表示 $T$ 的终端节点（叶节点）数量。对于任意非负惩罚参数 $\\alpha \\ge 0$，树 $T$ 的代价复杂度定义为 $R_{\\alpha}(T) = R(T) + \\alpha |T|$。最弱链接剪枝通过重复折叠内部节点来构造一个嵌套的子树序列，折叠的节点是在移除时，每移除一个叶节点所导致的 $R(T)$ 增加量最小的节点。\n\n给定 $T_{\\max}$ 的结构如下：\n- 根节点 $r$ 有两个内部子节点 $L$ 和 $R$。\n- $L$ 和 $R$ 各自分裂一次，形成两个终端节点；没有更深的分裂。因此， $|T_{\\max}| = 4$，内部节点为 $\\{r,L,R\\}$。\n- 如果一个节点是叶节点，其经验残差平方和由 $R(r)=400$，$R(L)=120$ 和 $R(R)=200$ 给出。\n- 每个双叶子树的经验残差平方和为 $R(T_{L})=80$ 和 $R(T_{R})=150$。因此，$R(T_{\\max}) = R(T_{L}) + R(T_{R}) = 230$。\n\n任务 A。从 $T_{\\max}$ 开始，执行最弱链接剪枝，并计算惩罚断点 $\\{\\alpha_{j}\\}$ 的非递减序列，在这些断点处剪枝后的子树会发生变化，从第一次剪枝直到最后剪枝成单个叶节点为止。按顺序报告断点 $\\alpha_{0}, \\alpha_{1}, \\alpha_{2}$，要求每个都是精确值。\n\n任务 B。考虑一个具有相同拓扑结构的参数化树族，其量\n$$\\Delta_{L} := R(L) - R(T_{L}), \\quad \\Delta_{R} := R(R) - R(T_{R}), \\quad \\Delta_{r} := R(r) - \\big(R(L) + R(R)\\big),$$\n假定满足不等式 $\\Delta_{r} + \\Delta_{L} \\ge 2 \\Delta_{R} \\ge 2 \\Delta_{L} > 0$。在这些约束条件下，用 $\\Delta_{L}, \\Delta_{R}, \\Delta_{r}$ 符号化地推导出剪枝断点序列，然后给出该序列中连续断点之间最大间隙的单个闭式表达式。在 $(\\alpha, R_{\\alpha}(T))$ 平面中，简要解释这些约束为何会产生异常大的间隙的几何原因。\n\n答案格式：以单行矩阵的形式提供最终答案，包含四个条目，顺序为\n$$(\\alpha_{0}, \\alpha_{1}, \\alpha_{2}, \\text{largest-gap-expression}).$$\n无需四舍五入。",
            "solution": "用户提供的问题已经过分析和验证，是统计学习领域中一个适定且有科学依据的问题。\n\n该问题要求对给定的回归树应用代价复杂度剪枝（也称为最弱链接剪枝）。该方法的核心是找到最大树 $T_{\\max}$ 的一个子树序列，该序列对于连续增加的惩罚参数 $\\alpha$ 是最优的。树 $T$ 的代价复杂度定义为 $R_{\\alpha}(T) = R(T) + \\alpha |T|$，其中 $R(T)$ 是残差平方和，|T| 是终端节点的数量。\n\n剪枝过程生成一个嵌套的子树序列 $T_0 \\supset T_1 \\supset \\dots \\supset T_k$，其中 $T_0 = T_{\\max}$。在每个阶段，算法会剪掉对应于“最弱链接”的内部节点 $t$，“最弱链接”定义为使函数 $g(t, T) = \\frac{R(t) - R(T_t)}{|T_t| - 1}$ 最小化的节点。这里，$T_t$ 是以 $t$ 为根的子树，$R(t)$ 表示如果节点 $t$ 是一个终端节点时的残差平方和。值 $\\alpha_{k+1} = \\min_{t} g(t, T_k)$ 成为序列中的下一个断点，即最优子树从 $T_k$ 过渡到 $T_{k+1}$ 时的惩罚参数值。\n\n我们来定义剪枝序列中涉及的树：\n- $T_0 = T_{\\max}$：最大树，有 $|T_0| = 4$ 个叶节点。其 RSS 给出为 $R(T_0) = R(T_{\\max}) = 230$。\n- $T_1$：通过剪枝节点 $L$ 得到的树。它有 $|T_1| = 3$ 个叶节点（$L$ 和 $R$ 的两个子节点）。其 RSS 为 $R(T_1) = R(L) + R(T_R) = 120 + 150 = 270$。\n- $T_2$：通过同时剪枝 $L$ 和 $R$ 得到的树。它有 $|T_2| = 2$ 个叶节点（$L$ 和 $R$）。其 RSS 为 $R(T_2) = R(L) + R(R) = 120 + 200 = 320$。\n- $T_3$：根树，由单个叶节点 $\\{r\\}$ 组成。它有 $|T_3| = 1$ 个叶节点。其 RSS 为 $R(T_3) = R(r) = 400$。\n\n还存在另一种可能的3叶树，即先剪枝节点 $R$，我们称之为 $T'_1$。其 RSS 将是 $R(T'_1) = R(T_L) + R(R) = 80 + 200 = 280$。由于 $R(T_1)=270  R(T'_1)=280$，对于任何 $\\alpha>0$，都有 $R_\\alpha(T_1)  R_\\alpha(T'_1)$。因此，$T'_1$ 永远不会是最优子树，也不在 $(|T|,R(T))$ 点集的下凸包上。随着 $\\alpha$ 的增加，序列 $T_0, T_1, T_2, T_3$ 是正确的最优子树序列。\n\n### 任务 A：断点的数值计算\n\n我们从树 $T_0 = T_{\\max}$ 开始，确定第一步剪枝。内部节点是 $L$ 和 $R$。最弱链接参数为：\n- 对于节点 $L$：$g(L, T_0) = \\frac{R(L) - R(T_L)}{|T_L| - 1} = \\frac{120 - 80}{2 - 1} = \\frac{40}{1} = 40$。\n- 对于节点 $R$：$g(R, T_0) = \\frac{R(R) - R(T_R)}{|T_R| - 1} = \\frac{200 - 150}{2 - 1} = \\frac{50}{1} = 50$。\n\n最小值为 $40$，所以第一个断点是 $\\alpha_0 = 40$。此时，节点 $L$ 被剪枝，最优树变为 $T_1$。\n\n接下来，我们从树 $T_1$ 开始。其内部节点是 $R$ 和根节点 $r$。\n- 对于节点 $R$：要剪枝的分支是 $T_R$。$g(R, T_1) = \\frac{R(R) - R(T_R)}{|T_R| - 1} = \\frac{50}{1} = 50$。\n- 对于根节点 $r$：在 $r$ 处剪枝意味着将整个树 $T_1$ 折叠成单个节点 $\\{r\\}$。RSS 的增加量为 $R(r) - R(T_1) = 400 - 270 = 130$。叶节点数量减少了 $|T_1| - 1 = 3 - 1 = 2$。\n$g(r, T_1) = \\frac{R(r) - R(T_1)}{|T_1| - 1} = \\frac{130}{2} = 65$。\n\n最小值为 $50$，所以第二个断点是 $\\alpha_1 = 50$。此时，节点 $R$ 被剪枝，最优树变为 $T_2$。\n\n最后，我们从树 $T_2$ 开始。其唯一的内部节点是根节点 $r$。\n- 对于根节点 $r$：剪枝意味着将 $T_2$ 折叠成 $\\{r\\}$。RSS 的增加量为 $R(r) - R(T_2) = 400 - 320 = 80$。叶节点数量减少了 $|T_2| - 1 = 2 - 1 = 1$。\n$g(r, T_2) = \\frac{R(r) - R(T_2)}{|T_2| - 1} = \\frac{80}{1} = 80$。\n\n这一定是下一个断点，所以 $\\alpha_2 = 80$。对于 $\\alpha \\ge 80$，最优树是根树 $T_3$。\n\n断点序列为 $\\{\\alpha_0, \\alpha_1, \\alpha_2\\} = \\{40, 50, 80\\}$。\n\n### 任务 B：符号推导与分析\n\n我们有如下定义：\n$\\Delta_{L} = R(L) - R(T_{L})$\n$\\Delta_{R} = R(R) - R(T_{R})$\n$\\Delta_{r} = R(r) - (R(L) + R(R))$\n以及约束条件 $\\Delta_{r} + \\Delta_{L} \\ge 2 \\Delta_{R} \\ge 2 \\Delta_{L} > 0$。\n\n剪枝断点可以通过找到最优树发生变化时的 $\\alpha$ 值来找到。这发生在 $R_\\alpha(T_k) = R_\\alpha(T_{k+1})$ 时。\n1.  **断点 $\\alpha_0$（从 $T_0$ 过渡到 $T_1$）**：\n    $R_{\\alpha}(T_0) = R_{\\alpha}(T_1) \\implies R(T_0) + 4\\alpha = R(T_1) + 3\\alpha$\n    $\\alpha = R(T_1) - R(T_0) = (R(L) + R(T_R)) - (R(T_L) + R(T_R)) = R(L) - R(T_L) = \\Delta_L$。\n    所以, $\\alpha_0 = \\Delta_L$。这与最弱链接的计算结果一致，因为从 $2\\Delta_R \\ge 2\\Delta_L$ 可得 $\\Delta_R \\ge \\Delta_L$，这使得 $L$ 成为最弱链接。\n\n2.  **断点 $\\alpha_1$（从 $T_1$ 过渡到 $T_2$）**：\n    $R_{\\alpha}(T_1) = R_{\\alpha}(T_2) \\implies R(T_1) + 3\\alpha = R(T_2) + 2\\alpha$\n    $\\alpha = R(T_2) - R(T_1) = (R(L) + R(R)) - (R(L) + R(T_R)) = R(R) - R(T_R) = \\Delta_R$。\n    所以, $\\alpha_1 = \\Delta_R$。\n\n3.  **断点 $\\alpha_2$（从 $T_2$ 过渡到 $T_3$）**：\n    $R_{\\alpha}(T_2) = R_{\\alpha}(T_3) \\implies R(T_2) + 2\\alpha = R(T_3) + \\alpha$\n    $\\alpha = R(T_3) - R(T_2) = R(r) - (R(L) + R(R)) = \\Delta_r$。\n    所以, $\\alpha_2 = \\Delta_r$。\n\n断点序列为 $\\{\\Delta_L, \\Delta_R, \\Delta_r\\}$。给定的不等式 $2\\Delta_R \\ge 2\\Delta_L$ 意味着 $\\Delta_R \\ge \\Delta_L$。不等式 $\\Delta_r + \\Delta_L \\ge 2\\Delta_R$ 可以改写为 $\\Delta_r \\ge 2\\Delta_R - \\Delta_L$。由于 $\\Delta_R \\ge \\Delta_L$，我们有 $-\\Delta_L \\ge -\\Delta_R$，这意味着 $\\Delta_r \\ge 2\\Delta_R - \\Delta_L \\ge 2\\Delta_R - \\Delta_R = \\Delta_R$。因此，我们得到了排序 $\\Delta_r \\ge \\Delta_R \\ge \\Delta_L$，这证实了 $\\{\\alpha_0, \\alpha_1, \\alpha_2\\}$ 是一个非递减序列。\n\n连续断点之间的间隙为：\n- 间隙 1：$\\alpha_1 - \\alpha_0 = \\Delta_R - \\Delta_L$。\n- 间隙 2：$\\alpha_2 - \\alpha_1 = \\Delta_r - \\Delta_R$。\n\n为了找到最大的间隙，我们比较这两个表达式。约束条件 $\\Delta_r + \\Delta_L \\ge 2\\Delta_R$ 可以重新排列为 $\\Delta_r - \\Delta_R \\ge \\Delta_R - \\Delta_L$。这个不等式表明第二个间隙大于或等于第一个间隙。因此，连续断点之间的最大间隙是 $\\Delta_r - \\Delta_R$。\n\n**大间隙的几何原因：**\n在 $(\\alpha, R_{\\alpha}(T))$ 平面中，每个候选子树 $T_k$ 的代价复杂度是一条斜率为 $|T_k|$ 的直线 $R_{\\alpha}(T_k) = R(T_k) + \\alpha |T_k|$。最优代价 $R_{\\alpha}^* = \\min_k R_{\\alpha}(T_k)$ 是这些线的下包络线，它是 $\\alpha$ 的一个分段线性凹函数。断点 $\\{\\alpha_j\\}$ 是这个下包络线顶点的 $\\alpha$ 坐标。\n我们发现断点为 $\\alpha_0 = \\Delta_L, \\alpha_1 = \\Delta_R, \\alpha_2 = \\Delta_r$。量 $\\Delta_L, \\Delta_R, \\Delta_r$ 代表三个连续剪枝步骤中每一步 RSS 的增加量。约束条件 $\\Delta_r - \\Delta_R \\ge \\Delta_R - \\Delta_L$ 意味着 RSS 增量序列 $(\\Delta_L, \\Delta_R, \\Delta_r)$ 是凸的。随着剪枝的进行，RSS 增量的这种“加速”导致断点（它们等于这些增量）随着 $\\alpha$ 的增加而间隔得更远。当 $\\Delta_r+\\Delta_L$ 显著大于 $2\\Delta_R$ 时，就会出现“异常大的间隙”，这使得 RSS 增量序列呈尖锐的凸形，从而拉长了树 $T_2$ 为最优的区间 $(\\alpha_1, \\alpha_2)$。\n\n最终答案组装：\n$\\alpha_0 = 40$\n$\\alpha_1 = 50$\n$\\alpha_2 = 80$\n最大间隙表达式: $\\Delta_r - \\Delta_R$\n\n这四个值将按要求放入一个行矩阵中。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n40  50  80  \\Delta_{r} - \\Delta_{R}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "理论和计算练习最终要应用于解决实际问题。这个综合性编程练习  将引导您从零开始，实现一个完整的决策树剪枝和评估流程。您将亲手构建决策树，应用交叉验证来选择最优的复杂度参数 $\\alpha$，并使用标准的“1-SE规则”进行模型选择，同时还将探索一种诊断过度剪枝的高级方法，从而将所有关键概念融会贯通。",
            "id": "3189411",
            "problem": "要求你设计并实现一个完全可复现的实验，以检测使用代价复杂度剪枝的二元决策树中潜在的过度剪枝问题。该实验必须是程序化的和自包含的，为几个预先指定的案例分别生成一个布尔值结果。你的程序必须从基本原理出发，实现以下原则和步骤。\n\n基本原理：\n- 使用经验风险最小化和结构风险控制的定义：当一个子树在保持较少叶节点数量的同时减少经验错分误差时，它更受青睐。考虑一个表示为 $\\alpha \\ge 0$ 的非负惩罚权重，该权重用于权衡经验错分与由叶节点数量衡量的结构复杂度。\n- 通过最弱连接剪枝构建代价复杂度剪枝序列：在每一步中，识别移除后导致每个叶节点的平均训练错分增量最小的内部节点，并将其剪枝；这将产生一个随 $\\alpha$ 递增的关键惩罚权重序列，在这些权重值上最优子树会发生变化。该序列是有限的，并且按 $\\alpha$ 递增全序排列。\n- 为了量化所选惩罚权重附近的验证误差的稳定性，将验证误差拟合为一个关于惩罚权重对数的局部二次模型，并测量其局部曲率（二阶导数）。在所选惩罚权重附近出现的剧烈正曲率表明，过于简单的子树会导致验证误差迅速增加。\n\n你需要实现的具体任务：\n- 使用贪婪递归划分，基于单个实值特征构建一个单变量（1维）二元分类决策树：\n  - 分裂准则：在每个非叶节点上，考虑排序后的唯一特征值之间的阈值，并选择最小化加权基尼不纯度的分裂点。对于节点中类别比例为 $p$ 的情况，使用基尼不纯度 $2p(1-p)$。\n  - 停止规则：如果满足以下任一条件，则停止分裂并将节点变为叶节点：节点是纯的，任一子节点将包含少于最少3个样本（最小叶节点大小），或者节点处可用的唯一特征值少于2个。\n  - 叶节点预测：预测多数类；如果票数相等，则预测类别0。\n- 通过最弱连接剪枝实现代价复杂度剪枝：\n  - 对于每个内部节点 $t$，将其训练错分数定义为 $t$ 中的样本数减去 $t$ 中多数类的样本数，并将其子树的训练错分数定义为其叶节点上错分数的总和。设 $|T_t|$ 为以 $t$ 为根的子树中的叶节点数量。该连接的强度是 $t$ 下的子树所实现的每个叶节点的平均训练错分减少量。首先剪掉最弱的连接，然后迭代进行，以获得最优子树发生变化时递增的关键惩罚值序列。\n  - 对于任意给定的 $\\alpha \\ge 0$，最优子树是通过按强度的非递减顺序剪枝所有连接，直到下一个连接的强度超过 $\\alpha$ 为止得到的。\n- 使用 $K=5$ 的 $K$折交叉验证，在一个公共惩罚网格上为每个子树估计平均验证误差及其标准误差（该网格由所有折的关键惩罚的并集构成，并补充了 $0$ 和一个足够大的值以确保包含仅有根节点的树）。\n- 应用1-标准误（1-SE）规则：设 $\\bar{e}(\\alpha)$ 为在惩罚 $\\alpha$ 下各折的平均验证误差，设 $s(\\alpha)$ 为标准误差（各折的标准差除以 $\\sqrt{5}$）。设 $\\alpha_{\\min}$ 为 $\\bar{e}(\\alpha)$ 的任一最小化者。选择最大的惩罚 $\\alpha_{\\text{1SE}}$，使得 $\\bar{e}(\\alpha_{\\text{1SE}}) \\le \\bar{e}(\\alpha_{\\min}) + s(\\alpha_{\\min})$。\n- 量化选择点附近的曲率：令 $x=\\log(\\alpha+\\delta)$，其中小的正数 $\\delta$ 定义如下。对 $\\bar{e}$ 作为 $x$ 的函数，在 $\\alpha_{\\text{1SE}}$ 周围的三个最近网格点上进行二次拟合，并从该二次拟合中估计曲率（即二阶导数）$d^2 \\bar{e}/dx^2$。同时，通过在所有内部网格点上拟合此类局部二次模型并取绝对曲率的中位数，计算一个稳健基线。如果网格上存在正的 $\\alpha$，则使用 $\\delta = 0.1 \\times \\min\\{\\alpha: \\alpha>0\\}$，否则 $\\delta = 10^{-6}$。\n- 如果以下所有条件同时成立，则将布尔标志“over-pruning detected”（检测到过度剪枝）定义为真：\n  - 所选惩罚严格大于最小化者，即 $\\alpha_{\\text{1SE}} > \\alpha_{\\min}$。\n  - 选择点处的超出平均验证误差（以小数表示）超过 $0.05$，即 $\\bar{e}(\\alpha_{\\text{1SE}}) - \\bar{e}(\\alpha_{\\min}) > 0.05$。\n  - 选择点处的局部曲率超过 $0.3$ 和稳健基线曲率三倍中的较大者，即 $\\frac{d^2 \\bar{e}}{dx^2}\\big|_{\\alpha_{\\text{1SE}}} > \\max\\{0.3, 3\\,\\text{median}_{i} |d^2 \\bar{e}/dx^2|_i\\}$，其中中位数取自所有内部网格点。\n\n数据生成和测试套件：\n- 所有案例均为单变量（1维）二元分类问题，在指定情况下标签是平衡的。使用提供的种子进行数据生成和折分配，以确保可复现性。\n- 案例 A（分离良好的高斯分布）：总共 $N=120$ 个样本，平衡。类别 0：$X \\sim \\mathcal{N}(-1.0, 0.3^2)$。类别 1：$X \\sim \\mathcal{N}(1.0, 0.3^2)$。使用种子 $13$。\n- 案例 B（带轻微噪声的交替区间）：总共 $N=180$ 个样本。在 $[-3, 3]$ 上均匀抽取 $X$。如果 $X \\in [-3,-2] \\cup [-1,0] \\cup [1,2]$，则设 $Y=1$，否则 $Y=0$。以 $0.05$ 的概率独立翻转每个标签。使用种子 $7$。\n- 案例 C（小样本阈值）：总共 $N=40$ 个样本。抽取 $X \\sim \\mathcal{N}(0,1)$。设 $Y=\\mathbf{1}\\{X>0.2\\}$。使用种子 $29$。\n\n实现细节：\n- 使用分层5折交叉验证，在每折中尽可能保持类别平衡。当类别计数不能被5整除时，将余数分配给索引较小的折。\n- 最小叶节点大小为3。在排序后的唯一值之间的阈值上使用严格的二元分裂。\n- 对于任何带折的评估，始终将各折的关键惩罚的并集计算成一个单一的排序网格，包含 $\\alpha=0$，并包含一个足够大的值以确保仅有根节点的树在网格上有所体现。\n\n程序输出要求：\n- 你的程序必须按 A、B、C 的顺序为测试套件中的每个案例计算“over-pruning detected”布尔值，并输出一行包含这三个布尔值的列表，使用Python列表语法，例如：[True,False,True]。\n- 不涉及物理单位。如果在辅助计算中出现任何角度，它们在这里都是无关的。\n- 所有数值输出都必须是布尔值。最终输出必须将所有提供的测试案例的结果聚合成单行，并采用上述确切格式。\n\n你的最终交付物必须是一个完整的、可运行的程序，该程序执行上述所有步骤并以所需格式准确打印一行输出。",
            "solution": "用户的要求是设计并实现一个完全指定的、可复现的实验，以检测单变量二元决策树中潜在的过度剪枝。这需要从基本原理出发实现几个统计学习的核心算法：决策树归纳、代价复杂度剪枝和交叉验证，最终形成一个用于检测过度剪枝的特定启发式测试。该问题是良定的、有科学依据的，并且是可程序化形式化的。\n\n该方法论分为四个主要阶段：\n1.  **决策树生长**：使用贪婪递归划分构建一个单变量二元决策树。\n2.  **代价复杂度剪枝**：使用最弱连接剪枝算法生成一个最优剪枝子树序列。\n3.  **交叉验证**：使用 $K$ 折交叉验证来估计子树的泛化误差。\n4.  **过度剪枝检测**：使用一套特定标准，包括验证误差曲线的局部曲率度量，来标记潜在的过度剪枝。\n\n整个过程被封装在一个Python程序中，该程序在三个预定义的测试案例上运行实验，并为每个案例输出一个布尔标志。\n\n**1. 通过递归划分进行决策树生长**\n\n二元决策树是通过重复地将数据分裂成两个分区来构建的。在每个节点，算法寻找形式为 $X  t$ 的最佳分裂，其中 $X$ 是单个特征，$t$ 是一个阈值。\n\n-   **分裂准则**：最优分裂是与父节点相比，能最大程度减少子节点不纯度的分裂。使用的不纯度度量是基尼不纯度，对于一个类别比例为 $p$ 和 $1-p$ 的节点，定义为 $G = 2p(1-p)$。所选的分裂会最小化两个子节点的加权平均基尼不纯度。阈值 $t$ 在排序后的唯一特征值之间的中点进行评估。\n\n-   **停止条件**：如果满足以下任一条件，递归分裂过程将在一个节点处终止，使其成为叶节点：\n    1.  节点是纯的（即只包含一个类别的样本）。\n    2.  任何潜在的分裂都会导致子节点样本数少于最少的3个。\n    3.  节点的数据分区中剩余的唯一特征值少于两个，使得分裂不可能进行。\n\n-   **叶节点预测**：叶节点预测其所含样本中的多数类。如果类别计数出现平局，则默认预测为类别0。\n\n**2. 通过最弱连接剪枝进行代价复杂度剪枝**\n\n一棵大而复杂的树可能会过拟合训练数据。代价复杂度剪枝旨在通过平衡经验误差和模型复杂度，找到一个泛化能力更好的小子树。子树 $T$ 的代价复杂度定义为：\n$$C_\\alpha(T) = \\text{Err}(T) + \\alpha |T|$$\n其中 $\\text{Err}(T)$ 是 $T$ 的叶节点在训练数据上的总错分计数， $|T|$ 是 $T$ 中的叶节点数量，$\\alpha \\ge 0$ 是一个复杂度惩罚参数。\n\n剪枝过程遵循最弱连接算法：\n1.  对于完全生长的树 $T_0$ 中的每个内部（非叶）节点 $t$，我们计算一个关键参数值 $\\alpha_t$。该值代表以 $t$ 为根的完整子树 $T_t$ 的代价等于将 $t$ 坍缩成单个叶节点的代价的点。该值由以下公式给出：\n    $$\\alpha_t = \\frac{\\text{err}(t) - \\text{Err}(T_t)}{|T_t| - 1}$$\n    其中 $\\text{err}(t)$ 是如果节点 $t$ 是一个叶节点时的错分计数，$\\text{Err}(T_t)$ 是子树 $T_t$ 中叶节点的错分计数之和，而 $|T_t|$ 是 $T_t$ 中的叶节点数量。\n2.  具有最小正 $\\alpha_t$ 值的节点被认为是“最弱连接”。\n3.  通过迭代地找到最弱连接并剪掉其对应的分支，我们生成一个从完整树 $T_0$ 到仅有根节点的树的有限嵌套子树序列。该序列对应于一个递增的关键 $\\alpha$ 值序列。对于任何给定的 $\\alpha$，最优子树是通过剪掉所有满足 $\\alpha_t \\le \\alpha$ 的节点 $t$ 得到的。\n\n**3. 用于误差估计的 $K$ 折交叉验证**\n\n为了选择最佳的复杂度参数 $\\alpha$，我们使用 $K=5$ 的 $K$ 折交叉验证。\n1.  将训练数据划分为 $K=5$ 个分层折，保持类别比例。\n2.  对于每个折 $k \\in \\{1, \\dots, 5\\}$，在其他 $K-1$ 个折上生长一棵完整的决策树。从这棵树中，提取一个关键 $\\alpha$ 值序列。\n3.  通过取所有 $K$ 个折的所有关键 $\\alpha$ 值的并集，并补充 $\\alpha=0$ 和一个足够大的 $\\alpha$ 以保证可以剪枝到根节点，来创建一个全局 $\\alpha$ 值网格。\n4.  对于每个折 $k$ 和全局网格上的每个 $\\alpha$，将在其他 $K-1$ 个折上训练的树剪枝到对应于该 $\\alpha$ 的最优子树。然后在留出的折 $k$ 上测量该子树的验证误差。\n5.  在遍历所有折之后，对于网格上的每个 $\\alpha$，我们计算平均验证误差 $\\bar{e}(\\alpha)$ 及其标准误差 $s(\\alpha)$。\n\n**4. 1-SE 规则与过度剪枝检测启发式方法**\n\n-   **1-标准误（1-SE）规则**：此规则选择一个更简约的模型，其性能在统计上与最佳模型相当。首先，我们找到最小化平均交叉验证误差 $\\bar{e}(\\alpha)$ 的参数 $\\alpha_{\\min}$。然后，我们在网格中选择最大的 $\\alpha$，记为 $\\alpha_{\\text{1SE}}$，使其满足：\n    $$\\bar{e}(\\alpha_{\\text{1SE}}) \\le \\bar{e}(\\alpha_{\\min}) + s(\\alpha_{\\min})$$\n\n-   **过度剪枝检测**：实验的核心是应用一套标准来标记由 1-SE 规则引起的潜在过度剪枝。当且仅当以下所有三个条件都满足时，布尔标志“over-pruning detected”被设置为真：\n    1.  所选惩罚严格大于给出最小误差的惩罚：$\\alpha_{\\text{1SE}} > \\alpha_{\\min}$。\n    2.  相对于最佳模型，平均验证误差的增加是显著的：$\\bar{e}(\\alpha_{\\text{1SE}}) - \\bar{e}(\\alpha_{\\min}) > 0.05$。\n    3.  在所选点处，误差曲线的局部曲率很高。为了评估这一点，我们将误差 $\\bar{e}$ 分析为 $x = \\log(\\alpha + \\delta)$ 的函数，其中 $\\delta$ 是一个小的正常数。在 $\\alpha_{\\text{1SE}}$ 周围的三个网格点上拟合一个局部二次函数，以估计二阶导数（曲率）$\\frac{d^2\\bar{e}}{dx^2}\\big|_{\\alpha_{\\text{1SE}}}$。该曲率必须超过一个相对于稳健基线定义的阈值：\n        $$\\frac{d^2 \\bar{e}}{dx^2}\\bigg|_{\\alpha_{\\text{1SE}}} > \\max\\left\\{0.3, 3 \\times \\text{median}_{i} \\left|\\frac{d^2 \\bar{e}}{dx^2}\\right|_i\\right\\}$$\n        基线是所有 $\\alpha$ 网格内部点上计算的绝对曲率中位数的三倍。\n\n这些规则的组合将这样一种直觉形式化：当 1-SE 规则选择了一个显著更简单、准确性显著降低、并且位于误差曲线上升陡峭部分上的模型时，就发生了过度剪枝。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass Node:\n    \"\"\"A node in the decision tree.\"\"\"\n    def __init__(self, is_leaf=False, prediction=None, split_threshold=None):\n        self.is_leaf = is_leaf\n        self.prediction = prediction\n        self.split_threshold = split_threshold\n        self.left = None\n        self.right = None\n        # Properties for pruning calculation\n        self.n_samples = 0\n        self.misclassification_error = 0\n        self.n_leaves = 0\n        self.alpha = float('inf')\n\nclass DecisionTreeClassifier:\n    \"\"\"A univariate binary decision tree classifier.\"\"\"\n    def __init__(self, min_leaf_size=3):\n        self.min_leaf_size = min_leaf_size\n        self.root = None\n        self.node_info = {} # Stores class counts for each node\n\n    def _gini_impurity(self, y):\n        \"\"\"Calculates Gini impurity using the 2p(1-p) formula.\"\"\"\n        if y.size == 0:\n            return 0.0\n        p = np.sum(y == 1) / y.size\n        return 2 * p * (1 - p)\n\n    def _find_best_split(self, X, y):\n        \"\"\"Finds the best split for a node.\"\"\"\n        best_split = {'gini': self._gini_impurity(y), 'threshold': None}\n        n_samples = y.size\n        \n        unique_X = np.unique(X)\n        if unique_X.size  2:\n            return best_split\n\n        thresholds = (unique_X[:-1] + unique_X[1:]) / 2.0\n\n        for threshold in thresholds:\n            left_indices = np.where(X  threshold)[0]\n            right_indices = np.where(X >= threshold)[0]\n\n            if len(left_indices)  self.min_leaf_size or len(right_indices)  self.min_leaf_size:\n                continue\n\n            y_left, y_right = y[left_indices], y[right_indices]\n            gini = (len(y_left) / n_samples) * self._gini_impurity(y_left) + \\\n                   (len(y_right) / n_samples) * self._gini_impurity(y_right)\n\n            if gini  best_split['gini']:\n                best_split = {'gini': gini, 'threshold': threshold}\n        \n        return best_split\n\n    def _grow_tree(self, X, y):\n        \"\"\"Recursively grows the decision tree.\"\"\"\n        class_counts = np.bincount(y, minlength=2)\n        pred = 0 if class_counts[0] >= class_counts[1] else 1\n\n        # Stopping conditions\n        is_pure = self._gini_impurity(y) == 0.0\n        no_further_split = len(np.unique(X))  2\n        \n        if is_pure or no_further_split:\n            node = Node(is_leaf=True, prediction=pred)\n            self._finalize_node_properties(node, y, class_counts)\n            return node\n\n        best_split = self._find_best_split(X, y)\n\n        if best_split['threshold'] is None:\n            # No valid split found\n            node = Node(is_leaf=True, prediction=pred)\n            self._finalize_node_properties(node, y, class_counts)\n            return node\n\n        # Create internal node and recurse\n        threshold = best_split['threshold']\n        node = Node(is_leaf=False, split_threshold=threshold)\n        self.node_info[id(node)] = {'class_counts': class_counts}\n        \n        left_indices = np.where(X  threshold)[0]\n        right_indices = np.where(X >= threshold)[0]\n        \n        node.left = self._grow_tree(X[left_indices], y[left_indices])\n        node.right = self._grow_tree(X[right_indices], y[right_indices])\n\n        # Calculate pruning properties\n        node.n_samples = y.size\n        misclass_if_leaf = y.size - np.max(class_counts)\n        subtree_misclass = node.left.misclassification_error + node.right.misclassification_error\n        \n        node.n_leaves = node.left.n_leaves + node.right.n_leaves\n        node.misclassification_error = subtree_misclass\n\n        if node.n_leaves > 1:\n            node.alpha = (misclass_if_leaf - subtree_misclass) / (node.n_leaves - 1)\n        else:\n             node.alpha = float('inf')\n\n        return node\n\n    def _finalize_node_properties(self, node, y, class_counts):\n        \"\"\"Helper to set properties for a new leaf node.\"\"\"\n        node.n_samples = y.size\n        node.misclassification_error = y.size - np.max(class_counts)\n        node.n_leaves = 1\n        self.node_info[id(node)] = {'class_counts': class_counts}\n\n    def fit(self, X, y):\n        \"\"\"Fits the decision tree to the data.\"\"\"\n        self.node_info.clear()\n        self.root = self._grow_tree(X, y.astype(int))\n    \n    def _collect_internal_nodes(self, node, nodes):\n        if not node.is_leaf:\n            nodes.append(node)\n            self._collect_internal_nodes(node.left, nodes)\n            self._collect_internal_nodes(node.right, nodes)\n\n    def get_pruning_sequence(self):\n        \"\"\"Returns the sorted unique critical alpha values for the tree.\"\"\"\n        internal_nodes = []\n        if self.root:\n            self._collect_internal_nodes(self.root, internal_nodes)\n        \n        alphas = [node.alpha for node in internal_nodes if node.alpha != float('inf') and node.alpha > 0]\n        return np.unique(alphas)\n\n    def get_pruned_tree(self, alpha):\n        \"\"\"Returns a new tree pruned at a given alpha.\"\"\"\n        if not self.root:\n            return None\n        \n        def clone_and_prune(node, alpha_thresh):\n            if node.is_leaf:\n                return node\n            \n            # Prune if this node's alpha is smaller or equal to the threshold\n            if node.alpha = alpha_thresh:\n                class_counts = self.node_info[id(node)]['class_counts']\n                pred = 0 if class_counts[0] >= class_counts[1] else 1\n                return Node(is_leaf=True, prediction=pred)\n            \n            new_node = Node(is_leaf=False, split_threshold=node.split_threshold)\n            new_node.left = clone_and_prune(node.left, alpha_thresh)\n            new_node.right = clone_and_prune(node.right, alpha_thresh)\n            return new_node\n\n        pruned_root = clone_and_prune(self.root, alpha)\n        \n        pruned_tree = DecisionTreeClassifier(self.min_leaf_size)\n        pruned_tree.root = pruned_root\n        return pruned_tree\n\n    def _predict_single(self, x, node):\n        if node.is_leaf:\n            return node.prediction\n        if x  node.split_threshold:\n            return self._predict_single(x, node.left)\n        return self._predict_single(x, node.right)\n\n    def predict(self, X):\n        if not self.root:\n            raise RuntimeError(\"Tree is not fitted yet.\")\n        return np.array([self._predict_single(x, self.root) for x in X])\n\ndef generate_case_A(N, seed):\n    rng = np.random.default_rng(seed)\n    n_class = N // 2\n    X0 = rng.normal(loc=-1.0, scale=0.3, size=n_class)\n    X1 = rng.normal(loc=1.0, scale=0.3, size=n_class)\n    X = np.concatenate([X0, X1])\n    y = np.concatenate([np.zeros(n_class, dtype=int), np.ones(n_class, dtype=int)])\n    return X, y\n\ndef generate_case_B(N, seed):\n    rng = np.random.default_rng(seed)\n    X = rng.uniform(-3, 3, size=N)\n    y = np.zeros(N, dtype=int)\n    y[np.where((X >= -3)  (X = -2))] = 1\n    y[np.where((X >= -1)  (X = 0))] = 1\n    y[np.where((X >= 1)  (X = 2))] = 1\n    flip_mask = rng.random(size=N)  0.05\n    y[flip_mask] = 1 - y[flip_mask]\n    return X, y\n\ndef generate_case_C(N, seed):\n    rng = np.random.default_rng(seed)\n    X = rng.normal(0, 1, size=N)\n    y = (X > 0.2).astype(int)\n    return X, y\n\ndef create_stratified_folds(y, K, seed):\n    rng = np.random.default_rng(seed)\n    indices = np.arange(len(y))\n    folds = [[] for _ in range(K)]\n    \n    for class_label in np.unique(y):\n        class_indices = indices[y == class_label]\n        rng.shuffle(class_indices)\n        \n        n_class = len(class_indices)\n        base_size, remainder = divmod(n_class, K)\n        \n        ptr = 0\n        for i in range(K):\n            fold_size = base_size + (1 if i  remainder else 0)\n            folds[i].extend(class_indices[ptr : ptr + fold_size])\n            ptr += fold_size\n            \n    return [np.array(f) for f in folds]\n\ndef calculate_curvature(x_points, y_points):\n    x1, x2, x3 = x_points\n    y1, y2, y3 = y_points\n    \n    # Avoid division by zero if points are not unique\n    if (x1 == x2) or (x1 == x3) or (x2 == x3):\n      return 0.0\n\n    term1 = y1 / ((x1 - x2) * (x1 - x3))\n    term2 = y2 / ((x2 - x1) * (x2 - x3))\n    term3 = y3 / ((x3 - x1) * (x3 - x2))    \n    return 2 * (term1 + term2 + term3)\n\ndef run_case(data_generator, N, seed, K=5):\n    X, y = data_generator(N, seed)\n    folds = create_stratified_folds(y, K, seed)\n    all_indices = np.arange(N)\n    \n    all_alphas = {0.0}\n    fold_trees = []\n\n    for k in range(K):\n        val_indices = folds[k]\n        train_indices = np.setdiff1d(all_indices, val_indices, assume_unique=True)\n        X_train, y_train = X[train_indices], y[train_indices]\n\n        tree = DecisionTreeClassifier()\n        tree.fit(X_train, y_train)\n        fold_trees.append(tree)\n\n        alphas_k = tree.get_pruning_sequence()\n        all_alphas.update(alphas_k)\n    \n    alpha_grid = np.array(sorted(list(all_alphas)))\n    if alpha_grid.size == 0 or alpha_grid[-1] == 0:\n        alpha_grid = np.append(alpha_grid, 1.0)\n    else:\n        alpha_grid = np.append(alpha_grid, alpha_grid[-1] + 1.0)\n\n    validation_errors = np.zeros((len(alpha_grid), K))\n    for k in range(K):\n        tree = fold_trees[k]\n        val_indices = folds[k]\n        X_val, y_val = X[val_indices], y[val_indices]\n\n        for i, alpha in enumerate(alpha_grid):\n            pruned_tree = tree.get_pruned_tree(alpha)\n            if pruned_tree.root:\n                y_pred = pruned_tree.predict(X_val)\n                error = np.mean(y_pred != y_val)\n            else:\n                error = np.mean(tree.root.prediction != y_val) if tree.root else 1.0\n            validation_errors[i, k] = error\n\n    mean_errors = np.mean(validation_errors, axis=1)\n    std_errors = np.std(validation_errors, axis=1, ddof=1) / np.sqrt(K)\n\n    idx_min = np.argmin(mean_errors)\n    alpha_min = alpha_grid[idx_min]\n    target_error = mean_errors[idx_min] + std_errors[idx_min]\n    \n    eligible_indices = np.where(mean_errors = target_error + 1e-9)[0] # Add tolerance for float comparison\n    idx_1se = eligible_indices[-1] if eligible_indices.size > 0 else idx_min\n    alpha_1se = alpha_grid[idx_1se]\n\n    cond1 = alpha_1se > alpha_min\n    cond2 = (mean_errors[idx_1se] - mean_errors[idx_min]) > 0.05\n    \n    if not (cond1 and cond2):\n        return False\n\n    min_pos_alpha = min((a for a in alpha_grid if a > 0), default=None)\n    delta = 0.1 * min_pos_alpha if min_pos_alpha is not None else 1e-6\n    x_grid = np.log(alpha_grid + delta)\n    \n    curvatures = []\n    for i in range(1, len(x_grid) - 1):\n        curv = calculate_curvature(x_grid[i-1:i+2], mean_errors[i-1:i+2])\n        curvatures.append(curv)\n    \n    if not curvatures:\n        return False\n    \n    baseline_curvature = np.median(np.abs(np.array(curvatures)))\n\n    if 1 = idx_1se  len(x_grid) - 1:\n        curvature_1se = calculate_curvature(x_grid[idx_1se-1:idx_1se+2], mean_errors[idx_1se-1:idx_1se+2])\n    elif idx_1se == len(x_grid) - 1 and len(x_grid) >= 3:\n        curvature_1se = calculate_curvature(x_grid[-3:], mean_errors[-3:])\n    else:\n        curvature_1se = 0.0\n    \n    cond3 = curvature_1se > max(0.3, 3 * baseline_curvature)\n    \n    return cond1 and cond2 and cond3\n\ndef solve():\n    \"\"\"Main function to run the experiment for all cases.\"\"\"\n    test_cases = [\n        {'name': 'A', 'generator': generate_case_A, 'N': 120, 'seed': 13},\n        {'name': 'B', 'generator': generate_case_B, 'N': 180, 'seed': 7},\n        {'name': 'C', 'generator': generate_case_C, 'N': 40, 'seed': 29},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(case['generator'], case['N'], case['seed'])\n        results.append(result)\n\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}