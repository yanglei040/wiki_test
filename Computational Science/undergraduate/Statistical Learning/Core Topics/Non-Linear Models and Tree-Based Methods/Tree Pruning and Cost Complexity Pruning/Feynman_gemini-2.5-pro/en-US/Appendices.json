{
    "hands_on_practices": [
        {
            "introduction": "To build robust predictive models, we must balance performance on training data with model simplicity to ensure good generalization. This first exercise  presents a clear, hypothetical scenario where two different subtrees yield the exact same training error, forcing us to look beyond simple error minimization. By applying the cost-complexity criterion, $R_{\\alpha}(T) = R(T) + \\alpha |T|$, you will formalize the choice between them and develop a fundamental intuition for how the penalty parameter $\\alpha$ enforces the principle of parsimony.",
            "id": "3189470",
            "problem": "Consider a binary classification task and a large tree built by Classification and Regression Trees (CART). Let a fixed internal node $t$ of this large tree be the root of a subtree, and suppose two alternative prunings of the subtree at $t$ are candidates for selection: a deeper subtree $T_{A}$ and a shallower subtree $T_{B}$. For each subtree $T$, define the empirical training error under the $0$–$1$ loss as\n$$\nR(T) \\equiv \\sum_{l \\in \\text{Leaves}(T)} \\sum_{i \\in l} \\mathbf{1}\\!\\left(y_{i} \\neq \\hat{y}_{l}\\right),\n$$\nwhere $y_{i}$ is the class label of training point $i$, and $\\hat{y}_{l}$ is the leaf prediction (the majority class in leaf $l$). Let $|\\!T\\!|$ denote the number of terminal nodes (leaves) of subtree $T$.\n\nDesign the following scenario. The leaf-level misclassification counts for the deeper subtree $T_{A}$ are given by the multiset $\\{2,1,0,3,1,0,2,3\\}$ so that $|\\!T_{A}\\!| = 8$ and $R(T_{A}) = 12$. The leaf-level misclassification counts for the shallower subtree $T_{B}$ are given by the multiset $\\{4,0,2,3,3\\}$ so that $|\\!T_{B}\\!| = 5$ and $R(T_{B}) = 12$. Starting from the principle of empirical risk minimization and the structural risk minimization idea that model complexity should be penalized, consider the cost-complexity criterion\n$$\nR_{\\alpha}(T) \\equiv R(T) + \\alpha \\, |\\!T\\!|,\n$$\nwhere $\\alpha \\geq 0$ is a regularization parameter.\n\nUsing only these foundational definitions, explain how cost complexity pruning resolves the tie in $R(T)$ between $T_{A}$ and $T_{B}$ by varying $\\alpha$, and derive the exact threshold value $\\alpha^{\\star}$ at which one subtree strictly dominates the other under $R_{\\alpha}(\\cdot)$ (that is, the smallest $\\alpha$ such that the preferred subtree has strictly smaller $R_{\\alpha}$ than the alternative). Provide your final answer for $\\alpha^{\\star}$ as a single real number. No rounding is required.",
            "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\n- A binary classification task using a CART-built tree.\n- Two alternative subtree prunings, $T_{A}$ and $T_{B}$, rooted at the same internal node $t$.\n- The empirical training error is defined as $R(T) \\equiv \\sum_{l \\in \\text{Leaves}(T)} \\sum_{i \\in l} \\mathbf{1}\\!\\left(y_{i} \\neq \\hat{y}_{l}\\right)$.\n- The complexity of a tree $T$ is its number of leaves, denoted by $|\\!T\\!|$.\n- For the deeper subtree $T_{A}$:\n  - Number of leaves: $|\\!T_{A}\\!| = 8$.\n  - Total empirical error: $R(T_{A}) = 12$.\n- For the shallower subtree $T_{B}$:\n  - Number of leaves: $|\\!T_{B}\\!| = 5$.\n  - Total empirical error: $R(T_{B}) = 12$.\n- The cost-complexity criterion is defined as $R_{\\alpha}(T) \\equiv R(T) + \\alpha \\, |\\!T\\!|$, with the regularization parameter $\\alpha \\geq 0$.\n- The objective is to explain how cost-complexity pruning resolves the tie in $R(T)$ and to find the threshold value $\\alpha^{\\star}$, defined as the smallest $\\alpha$ for which one subtree is strictly dominant.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the standard and well-established theory of Classification and Regression Trees (CART) and cost-complexity pruning, a core concept in statistical learning. The definitions and setup are correct.\n- **Well-Posed**: The problem is clearly stated with all necessary quantitative information. It asks for a specific value, $\\alpha^{\\star}$, which can be uniquely determined from the given data and definitions.\n- **Objective**: The problem is expressed in precise mathematical language, free from subjectivity or ambiguity.\n- The problem is internally consistent, as the stated total errors $R(T_A)$ and $R(T_B)$ match the sums of the provided leaf-level misclassification counts. There are no contradictions or missing pieces of information.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution can be constructed.\n\n### Solution Derivation\n\nThe problem requires us to compare two candidate subtrees, $T_{A}$ and $T_{B}$, which have the same empirical training error but different complexities. This scenario highlights the core principle of structural risk minimization, often associated with Occam's razor: when two models perform equally well on training data, the simpler one should be preferred. Cost-complexity pruning provides a formal mechanism to implement this principle.\n\nThe cost-complexity criterion is given by $R_{\\alpha}(T) = R(T) + \\alpha |\\!T\\!|$. Here, $R(T)$ is the empirical risk (a measure of training data fit), and $\\alpha |\\!T\\!|$ is a penalty term that punishes model complexity, with $|\\!T\\!|$ being the measure of complexity and $\\alpha \\geq 0$ being the parameter that controls the trade-off.\n\nWe are given the following values for the two subtrees:\nFor subtree $T_{A}$:\n- Empirical error: $R(T_{A}) = 12$\n- Complexity (number of leaves): $|\\!T_{A}\\!| = 8$\n\nFor subtree $T_{B}$:\n- Empirical error: $R(T_{B}) = 12$\n- Complexity (number of leaves): $|\\!T_{B}\\!| = 5$\n\nLet's write down the cost-complexity for each subtree as a function of $\\alpha$:\n$$\nR_{\\alpha}(T_{A}) = R(T_{A}) + \\alpha |\\!T_{A}\\!| = 12 + 8\\alpha\n$$\n$$\nR_{\\alpha}(T_{B}) = R(T_{B}) + \\alpha |\\!T_{B}\\!| = 12 + 5\\alpha\n$$\n\nThe goal of pruning is to select the subtree with the minimum cost-complexity $R_{\\alpha}(T)$ for a given $\\alpha$.\n\nFirst, we analyze the case when $\\alpha = 0$. This corresponds to pure empirical risk minimization, where there is no penalty for complexity.\n$$\nR_{0}(T_{A}) = 12 + 8(0) = 12\n$$\n$$\nR_{0}(T_{B}) = 12 + 5(0) = 12\n$$\nIn this case, $R_{0}(T_{A}) = R_{0}(T_{B})$. Based on empirical risk alone, there is no reason to prefer one subtree over the other; they are tied.\n\nNext, we consider the case where $\\alpha > 0$. This introduces a penalty for complexity. The principle of structural risk minimization suggests that the simpler tree, $T_{B}$, should be preferred. Let's verify this by comparing $R_{\\alpha}(T_{A})$ and $R_{\\alpha}(T_{B})$. We seek the condition under which one is strictly smaller than the other.\nLet's determine for which values of $\\alpha$ the subtree $T_{B}$ is preferred, i.e., $R_{\\alpha}(T_{B}) < R_{\\alpha}(T_{A})$.\n$$\n12 + 5\\alpha < 12 + 8\\alpha\n$$\nSubtracting $12$ from both sides gives:\n$$\n5\\alpha < 8\\alpha\n$$\nSubtracting $5\\alpha$ from both sides:\n$$\n0 < 3\\alpha\n$$\nSince $\\alpha$ is a non-negative parameter, this inequality holds for all $\\alpha > 0$.\n\nThis result explains how cost-complexity pruning resolves the tie. As soon as any positive penalty is applied to complexity (i.e., for any $\\alpha > 0$), the tie is broken. Because $T_{B}$ is simpler ($|\\!T_{B}\\!| < |\\!T_{A}\\!|$), its penalty term $\\alpha |\\!T_{B}\\!|$ is smaller than that of $T_{A}$, making its total cost-complexity $R_{\\alpha}(T_{B})$ strictly smaller. Thus, for any $\\alpha > 0$, $T_{B}$ is the strictly dominant subtree.\n\nThe problem asks for the \"threshold value $\\alpha^{\\star}$ at which one subtree strictly dominates the other.\" This threshold is the value of $\\alpha$ that marks the boundary between the regime of indifference and the regime of strict preference. We can find this boundary point by setting the cost-complexities equal to each other and solving for $\\alpha$:\n$$\nR_{\\alpha}(T_{A}) = R_{\\alpha}(T_{B})\n$$\n$$\n12 + 8\\alpha = 12 + 5\\alpha\n$$\n$$\n3\\alpha = 0\n$$\n$$\n\\alpha = 0\n$$\nThis value, $\\alpha = 0$, is the point of transition. At $\\alpha = 0$, the costs are equal. For any value $\\alpha > 0$, a strict preference for $T_{B}$ emerges. The problem defines $\\alpha^{\\star}$ as the smallest $\\alpha$ such that a strict preference exists. The set of $\\alpha$ values for which a strict preference exists is $(0, \\infty)$. The smallest value for which this condition holds is any $\\epsilon > 0$, however small. The question is asking for the threshold, which is the infimum of this set. The infimum of the set $(0, \\infty)$ is $0$.\n\nTherefore, the threshold value $\\alpha^{\\star}$ at which a strict preference appears is $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "While the cost-complexity criterion defines what an optimal subtree is for a given $\\alpha$, it does not specify how to find it efficiently. This next practice  explores the \"weakest-link\" pruning algorithm, the standard procedure for generating the entire sequence of optimal subtrees. You will calculate the critical penalty breakpoints, $\\alpha_{j}$, for a given tree, which are the precise values of $\\alpha$ at which the optimal subtree structure changes, providing a concrete understanding of how the pruning path is constructed.",
            "id": "3189468",
            "problem": "A regression tree is grown to a known maximal tree $T_{\\max}$ on a training set, and pruning is to be carried out using the cost-complexity criterion. Let $R(T)$ denote the empirical residual sum of squares of a tree $T$ on the training set, and let $|T|$ denote the number of terminal nodes (leaves) of $T$. For any nonnegative penalty parameter $\\alpha \\ge 0$, the cost-complexity of $T$ is defined as $R_{\\alpha}(T) = R(T) + \\alpha |T|$. Weakest-link pruning constructs a nested sequence of subtrees by repeatedly collapsing the internal node(s) whose removal causes the smallest increase in $R(T)$ per leaf removed.\n\nYou are given the following structure for $T_{\\max}$:\n- The root node $r$ has two internal children $L$ and $R$.\n- Each of $L$ and $R$ splits once into two terminal nodes; there are no deeper splits. Thus, $|T_{\\max}| = 4$ and the internal nodes are $\\{r,L,R\\}$.\n- The empirical residual sum of squares if a node is a leaf is given by $R(r)=400$, $R(L)=120$, and $R(R)=200$.\n- The empirical residual sum of squares of each two-leaf subtree is $R(T_{L})=80$ and $R(T_{R})=150$. Consequently, $R(T_{\\max}) = R(T_{L}) + R(T_{R}) = 230$.\n\nTask A. Starting from $T_{\\max}$, carry out weakest-link pruning and compute the nondecreasing sequence of penalty breakpoints $\\{\\alpha_{j}\\}$ at which the pruned subtree changes, from the first prune down to the last prune to a single leaf. Report the breakpoints in order as $\\alpha_{0}, \\alpha_{1}, \\alpha_{2}$, each exactly.\n\nTask B. Consider a parametric family of trees with the same topology and with quantities\n$$\\Delta_{L} := R(L) - R(T_{L}), \\quad \\Delta_{R} := R(R) - R(T_{R}), \\quad \\Delta_{r} := R(r) - \\big(R(L) + R(R)\\big),$$\nassumed to satisfy the inequalities $\\Delta_{r} + \\Delta_{L} \\ge 2 \\Delta_{R} \\ge 2 \\Delta_{L} > 0$. Under these constraints, derive symbolically the sequence of pruning breakpoints in terms of $\\Delta_{L}, \\Delta_{R}, \\Delta_{r}$, and then give a single closed-form expression for the largest gap between consecutive breakpoints in that sequence. Explain concisely the geometric reason, in the $(\\alpha, R_{\\alpha}(T))$ plane, why these constraints produce unusually large gaps.\n\nAnswer formatting: Provide your final answer as a single row matrix with four entries, in the order\n$$(\\alpha_{0}, \\alpha_{1}, \\alpha_{2}, \\text{largest-gap-expression}).$$\nNo rounding is required.",
            "solution": "The user-provided problem has been analyzed and validated as a well-posed and scientifically grounded question in the field of statistical learning.\n\nThe problem requires the application of cost-complexity pruning (also known as weakest-link pruning) to a given regression tree. The core of this method is to find a sequence of subtrees of the maximal tree $T_{\\max}$ that are optimal for a continuously increasing penalty parameter $\\alpha$. The cost-complexity of a tree $T$ is defined as $R_{\\alpha}(T) = R(T) + \\alpha |T|$, where $R(T)$ is the residual sum of squares and $|T|$ is the number of terminal nodes.\n\nThe pruning process generates a nested sequence of subtrees $T_0 \\supset T_1 \\supset \\dots \\supset T_k$, where $T_0 = T_{\\max}$. At each stage, the algorithm prunes the internal node $t$ that corresponds to the \"weakest link\", defined as the node that minimizes the function $g(t, T) = \\frac{R(t) - R(T_t)}{|T_t| - 1}$. Here, $T_t$ is the subtree rooted at $t$, and $R(t)$ denotes the residual sum of squares if node $t$ were a terminal node. The value $\\alpha_{k+1} = \\min_{t} g(t, T_k)$ becomes the next breakpoint in the sequence, which is the value of the penalty parameter at which the optimal subtree transitions from $T_k$ to $T_{k+1}$.\n\nLet's define the trees involved in the pruning sequence:\n- $T_0 = T_{\\max}$: The maximal tree with $|T_0| = 4$ leaves. Its RSS is given as $R(T_0) = R(T_{\\max}) = 230$.\n- $T_1$: The tree obtained by pruning the node $L$. It has $|T_1| = 3$ leaves ($L$ and the two children of $R$). Its RSS is $R(T_1) = R(L) + R(T_R) = 120 + 150 = 270$.\n- $T_2$: The tree obtained by pruning both $L$ and $R$. It has $|T_2| = 2$ leaves ($L$ and $R$). Its RSS is $R(T_2) = R(L) + R(R) = 120 + 200 = 320$.\n- $T_3$: The root tree, consisting of a single leaf $\\{r\\}$. It has $|T_3| = 1$ leaf. Its RSS is $R(T_3) = R(r) = 400$.\n\nAnother possible 3-leaf tree exists by pruning node $R$ first, let's call it $T'_1$. Its RSS would be $R(T'_1) = R(T_L) + R(R) = 80 + 200 = 280$. Since $R(T_1)=270 < R(T'_1)=280$, for any $\\alpha>0$, $R_\\alpha(T_1) < R_\\alpha(T'_1)$. Thus, $T'_1$ will never be an optimal subtree and is not on the lower convex hull of the $(|T|,R(T))$ points. The sequence $T_0, T_1, T_2, T_3$ is the correct sequence of optimal subtrees as $\\alpha$ increases.\n\n### Task A: Numerical Calculation of Breakpoints\n\nWe start with the tree $T_0 = T_{\\max}$ and determine the first pruning step. The internal nodes are $L$ and $R$.\nThe weakest-link parameters are:\n- For node $L$: $g(L, T_0) = \\frac{R(L) - R(T_L)}{|T_L| - 1} = \\frac{120 - 80}{2 - 1} = \\frac{40}{1} = 40$.\n- For node $R$: $g(R, T_0) = \\frac{R(R) - R(T_R)}{|T_R| - 1} = \\frac{200 - 150}{2 - 1} = \\frac{50}{1} = 50$.\n\nThe minimum value is $40$, so the first breakpoint is $\\alpha_0 = 40$. At this point, node $L$ is pruned, and the optimal tree becomes $T_1$.\n\nNext, we start with tree $T_1$. Its internal nodes are $R$ and the root $r$.\n- For node $R$: The branch to be pruned is $T_R$. $g(R, T_1) = \\frac{R(R) - R(T_R)}{|T_R| - 1} = \\frac{50}{1} = 50$.\n- For the root node $r$: Pruning at $r$ means collapsing the entire tree $T_1$ into the single node $\\{r\\}$. The increase in RSS is $R(r) - R(T_1) = 400 - 270 = 130$. The number of leaves is reduced by $|T_1| - 1 = 3 - 1 = 2$.\n$g(r, T_1) = \\frac{R(r) - R(T_1)}{|T_1| - 1} = \\frac{130}{2} = 65$.\n\nThe minimum value is $50$, so the second breakpoint is $\\alpha_1 = 50$. At this point, node $R$ is pruned, and the optimal tree becomes $T_2$.\n\nFinally, we start with tree $T_2$. Its only internal node is the root $r$.\n- For the root node $r$: Pruning means collapsing $T_2$ into $\\{r\\}$. The increase in RSS is $R(r) - R(T_2) = 400 - 320 = 80$. The number of leaves is reduced by $|T_2| - 1 = 2 - 1 = 1$.\n$g(r, T_2) = \\frac{R(r) - R(T_2)}{|T_2| - 1} = \\frac{80}{1} = 80$.\n\nThis must be the next breakpoint, so $\\alpha_2 = 80$. For $\\alpha \\ge 80$, the optimal tree is the root tree $T_3$.\n\nThe sequence of breakpoints is $\\{\\alpha_0, \\alpha_1, \\alpha_2\\} = \\{40, 50, 80\\}$.\n\n### Task B: Symbolic Derivation and Analysis\n\nWe are given the definitions:\n$\\Delta_{L} = R(L) - R(T_{L})$\n$\\Delta_{R} = R(R) - R(T_{R})$\n$\\Delta_{r} = R(r) - (R(L) + R(R))$\nAnd the constraints $\\Delta_{r} + \\Delta_{L} \\ge 2 \\Delta_{R} \\ge 2 \\Delta_{L} > 0$.\n\nThe pruning breakpoints can be found by finding the values of $\\alpha$ at which the optimal tree changes. This occurs when $R_\\alpha(T_k) = R_\\alpha(T_{k+1})$.\n1.  **Breakpoint $\\alpha_0$ (transition from $T_0$ to $T_1$)**:\n    $R_{\\alpha}(T_0) = R_{\\alpha}(T_1) \\implies R(T_0) + \\alpha |T_0| = R(T_1) + \\alpha |T_1|$\n    $R(T_{\\max}) + 4\\alpha = (R(L) + R(T_R)) + 3\\alpha$\n    $\\alpha = R(L) + R(T_R) - R(T_{\\max}) = R(L) + R(T_R) - (R(T_L) + R(T_R)) = R(L) - R(T_L) = \\Delta_L$.\n    So, $\\alpha_0 = \\Delta_L$. This is consistent with the weakest link calculation, as from $2\\Delta_R \\ge 2\\Delta_L$, we get $\\Delta_R \\ge \\Delta_L$, making $L$ the weakest link.\n\n2.  **Breakpoint $\\alpha_1$ (transition from $T_1$ to $T_2$)**:\n    $R_{\\alpha}(T_1) = R_{\\alpha}(T_2) \\implies R(T_1) + 3\\alpha = R(T_2) + 2\\alpha$\n    $\\alpha = R(T_2) - R(T_1) = (R(L) + R(R)) - (R(L) + R(T_R)) = R(R) - R(T_R) = \\Delta_R$.\n    So, $\\alpha_1 = \\Delta_R$.\n\n3.  **Breakpoint $\\alpha_2$ (transition from $T_2$ to $T_3$)**:\n    $R_{\\alpha}(T_2) = R_{\\alpha}(T_3) \\implies R(T_2) + 2\\alpha = R(T_3) + \\alpha$\n    $\\alpha = R(T_3) - R(T_2) = R(r) - (R(L) + R(R)) = \\Delta_r$.\n    So, $\\alpha_2 = \\Delta_r$.\n\nThe sequence of breakpoints is $\\{\\Delta_L, \\Delta_R, \\Delta_r\\}$. The given inequality $2\\Delta_R \\ge 2\\Delta_L$ implies $\\Delta_R \\ge \\Delta_L$. The inequality $\\Delta_r + \\Delta_L \\ge 2\\Delta_R$ can be rewritten as $\\Delta_r \\ge 2\\Delta_R - \\Delta_L$. Since $\\Delta_R \\ge \\Delta_L$, we have $-\\Delta_L \\ge -\\Delta_R$, which implies $\\Delta_r \\ge 2\\Delta_R - \\Delta_L \\ge 2\\Delta_R - \\Delta_R = \\Delta_R$. Thus, we have the ordering $\\Delta_r \\ge \\Delta_R \\ge \\Delta_L$, which confirms that $\\{\\alpha_0, \\alpha_1, \\alpha_2\\}$ is a non-decreasing sequence.\n\nThe gaps between consecutive breakpoints are:\n- Gap 1: $\\alpha_1 - \\alpha_0 = \\Delta_R - \\Delta_L$.\n- Gap 2: $\\alpha_2 - \\alpha_1 = \\Delta_r - \\Delta_R$.\n\nTo find the largest gap, we compare these two expressions. The constraint $\\Delta_r + \\Delta_L \\ge 2\\Delta_R$ can be rearranged as $\\Delta_r - \\Delta_R \\ge \\Delta_R - \\Delta_L$.\nThis inequality demonstrates that the second gap is greater than or equal to the first gap. Therefore, the largest gap between consecutive breakpoints is $\\Delta_r - \\Delta_R$.\n\n**Geometric Reason for Large Gaps:**\nIn the $(\\alpha, R_{\\alpha}(T))$ plane, the cost-complexity of each candidate subtree $T_k$ is a line $R_{\\alpha}(T_k) = R(T_k) + \\alpha |T_k|$ with slope $|T_k|$. The optimal cost $R_{\\alpha}^* = \\min_k R_{\\alpha}(T_k)$ is the lower envelope of these lines, which is a piecewise-linear, concave function of $\\alpha$. The breakpoints $\\{\\alpha_j\\}$ are the $\\alpha$-coordinates of the vertices of this lower envelope.\nThe breakpoints were found to be $\\alpha_0 = \\Delta_L, \\alpha_1 = \\Delta_R, \\alpha_2 = \\Delta_r$. The quantities $\\Delta_L, \\Delta_R, \\Delta_r$ represent the increase in RSS for each of the three successive pruning steps. The constraint $\\Delta_r - \\Delta_R \\ge \\Delta_R - \\Delta_L$ implies that the sequence of RSS increases $(\\Delta_L, \\Delta_R, \\Delta_r)$ is convex. This \"acceleration\" in the RSS increments as pruning progresses causes the breakpoints—which are equal to these increments—to be spaced farther apart as $\\alpha$ increases. An \"unusually large gap\" arises when $\\Delta_r+\\Delta_L$ is significantly larger than $2\\Delta_R$, making the sequence of RSS increments sharply convex and thus stretching the interval $(\\alpha_1, \\alpha_2)$ over which the tree $T_2$ is optimal.\n\nFinal Answer Assembly:\n$\\alpha_0 = 40$\n$\\alpha_1 = 50$\n$\\alpha_2 = 80$\nLargest gap expression: $\\Delta_r - \\Delta_R$\n\nThese four values will be placed in a row matrix as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n40 & 50 & 80 & \\Delta_{r} - \\Delta_{R}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The effectiveness of the weakest-link algorithm can be understood more deeply by connecting it to classic optimization theory. This final hands-on practice  guides you to reframe the pruning decision as a 0–1 knapsack problem, where each potential branch offers a \"benefit\" (error reduction) at a fixed \"cost\" (an additional leaf). Verifying this equivalence reveals why a greedy approach—iteratively pruning the branch with the least benefit—is not merely a heuristic but is guaranteed to trace the path of optimal subtrees, providing a powerful theoretical justification for the algorithm.",
            "id": "3189469",
            "problem": "You are asked to formalize, analyze, and test the equivalence of three perspectives on pruning binary decision trees in statistical learning: cost-complexity pruning, a unit-cost knapsack formulation, and the weakest-link pruning path. Start from the following base, widely accepted in the field of decision trees and pruning.\n\nFundamental base and definitions: Consider a classification tree with a finite set of leaves denoted by $T$. The empirical resubstitution error (training misclassification count) of the tree is denoted by $R(T)$. The cost-complexity criterion for a nonnegative regularization parameter $\\alpha$ is\n$$\nC_{\\alpha}(T) \\equiv R(T) + \\alpha \\, |T|,\n$$\nwhere $|T|$ is the number of leaves. A subtree formed by pruning a branch replaces that branch (an internal node and its descendants) by a single leaf at that node. For any prunable branch that increases the number of leaves by $\\Delta |T|$ when kept (relative to pruning) and reduces the resubstitution error by $\\Delta R$ when kept (relative to pruning), the net change in the cost-complexity objective when keeping that branch is\n$$\n\\Delta C_{\\alpha} = -\\Delta R + \\alpha \\, \\Delta |T|.\n$$\nThe weakest-link pruning algorithm removes, at each step, the branch with the smallest ratio $\\Delta R / \\Delta |T|$ and produces a nested sequence of subtrees indexed by $\\alpha$.\n\nDataset design to enforce unit costs: Construct a synthetic dataset that yields $m$ independent candidate branches, each of which, if kept, reduces error by $\\Delta R_j$ and increases $|T|$ by exactly $1$ (hence $\\Delta |T|_j = 1$ for all $j \\in \\{1,\\dots,m\\}$). One way to realize this is to assume a fixed, already-chosen partition that creates $m$ disjoint regions (groups), under which each region has one optional binary split that creates two leaves from one leaf. Let the $j$-th region be composed of two subgroups (call them $s=0$ and $s=1$) with counts $(p_{j0}, n_{j0})$ and $(p_{j1}, n_{j1})$ for positives and negatives, respectively. Then, if the secondary split within the region is kept, the post-split misclassification count in that region is $\\min(p_{j0}, n_{j0}) + \\min(p_{j1}, n_{j1})$, whereas if the split is pruned, the single leaf predicts the regional majority class and the misclassification count is $\\min(p_{j0}+p_{j1},\\, n_{j0}+n_{j1})$. Therefore, the benefit $\\Delta R_j$ of keeping the branch is\n$$\n\\Delta R_j = \\min(p_{j0}+p_{j1},\\, n_{j0}+n_{j1}) - \\big(\\min(p_{j0}, n_{j0}) + \\min(p_{j1}, n_{j1})\\big),\n$$\nand the cost per kept branch is $1$ leaf.\n\nTarget problem: You must write a program that, given several small test suites of $\\Delta R$ and/or explicit subgroup counts, checks the following three equivalences and outputs quantifiable results.\n\n1) For fixed $\\alpha$, cost-complexity optimality versus greedy threshold rule:\n- Using the cost-complexity objective, the globally optimal subset of branches to keep maximizes $\\sum_{j \\in S} \\Delta R_j - \\alpha |S|$ over all subsets $S$. With unit costs $\\Delta |T|_j = 1$, the necessary and sufficient condition for optimality is to keep exactly those branches with $\\Delta R_j \\ge \\alpha$.\n- Verify this equivalence by comparing the exact optimal subset found by exhaustive search to the greedy threshold rule “keep $j$ if and only if $\\Delta R_j \\ge \\alpha$.”\n\n2) Boundary case with ties:\n- For a case with ties at the boundary (i.e., some $\\Delta R_j$ equal to $\\alpha$), adopt the deterministic convention to keep a branch when $\\Delta R_j \\ge \\alpha$ and prune when $\\Delta R_j < \\alpha$. Verify that the exhaustive search agrees with this convention.\n\n3) Budget-constrained knapsack versus Lagrangian form:\n- With a budget $B$ on the number of branches to keep (each with cost $1$), the optimization $\\max_{S: |S|\\le B} \\sum_{j \\in S} \\Delta R_j$ is the $0$–$1$ knapsack problem with unit costs and is solved by choosing the $B$ largest $\\Delta R_j$. Show that there exists $\\alpha$ such that the Lagrangian solution “keep all $\\Delta R_j \\ge \\alpha$” yields exactly $B$ branches and matches the budget solution.\n\n4) Weakest-link pruning path equivalence:\n- For unit costs, the weakest-link step removes the branch with the smallest $\\Delta R_j$, and the entire sequence of pruned trees corresponds to monotonically increasing $\\alpha$. Verify that the sets produced by thresholding at multiple $\\alpha$ values match the sets produced by successively removing the smallest $\\Delta R_j$.\n\n5) Explicit dataset from counts:\n- Use the following explicit counts for $m = 4$ groups:\n  - Group $0$: $(p_{00}, n_{00}) = (7, 1)$ and $(p_{01}, n_{01}) = (1, 5)$.\n  - Group $1$: $(p_{10}, n_{10}) = (3, 2)$ and $(p_{11}, n_{11}) = (2, 3)$.\n  - Group $2$: $(p_{20}, n_{20}) = (6, 0)$ and $(p_{21}, n_{21}) = (0, 4)$.\n  - Group $3$: $(p_{30}, n_{30}) = (2, 1)$ and $(p_{31}, n_{31}) = (1, 2)$.\n  Compute $\\Delta R_j$ for each group from the definition above, and verify the equivalences in items $1$–$4$ for the choice $\\alpha = 1.5$ in raw misclassification counts (so $\\alpha$ is in “number of misclassifications”).\n\nTest suite specification: Your program must implement and evaluate the following parameter sets.\n- Test $1$ (happy path): $\\Delta R = \\{0.31, 0.12, 0.27, 0.08, 0.19, 0.05, 0.14, 0.22\\}$ with $\\alpha = 0.15$.\n- Test $2$ (boundary ties): $\\Delta R = \\{0.20, 0.15, 0.15, 0.10\\}$ with $\\alpha = 0.15$.\n- Test $3$ (budget): $\\Delta R = \\{0.31, 0.12, 0.27, 0.08, 0.19, 0.05, 0.14, 0.22\\}$ with budget $B = 3$ and a Lagrange parameter chosen between the third and fourth largest values.\n- Test $4$ (weakest-link path verification): Reuse $\\Delta R = \\{0.31, 0.12, 0.27, 0.08, 0.19, 0.05, 0.14, 0.22\\}$ and check equivalence across a grid of $\\alpha$ values spanning below the minimum and above the maximum.\n- Test $5$ (explicit counts): Use the $m = 4$ groups specified above and $\\alpha = 1.5$ (counts, not rates), compute $\\Delta R_j$ from counts, and verify equivalence between exhaustive cost-complexity optimization, threshold rule, and weakest-link removal.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the following entries in order:\n- Entry $1$: a boolean for Test $1$ equivalence (true if exhaustive search equals threshold rule).\n- Entry $2$: a boolean for Test $2$ equivalence.\n- Entry $3$: a boolean that is true if the budget-constrained optimal set equals both the top-$B$ greedy set and the Lagrangian set with the chosen $\\alpha$.\n- Entry $4$: a boolean that is true if the weakest-link path equals the thresholded sets across the grid of $\\alpha$ values.\n- Entry $5$: a boolean for Test $5$ equivalence using the explicit counts and $\\alpha = 1.5$.\n- Entry $6$: the sorted list of the unique $\\Delta R$ values for Test $1$.\n\nYour program must be fully deterministic and must not require any user input. All answers are dimensionless except Test $5$, where $\\alpha$ is in “number of misclassifications.” Express any angles in radians if you need them (you do not). The output must be a single line, for example, of the form $[\\text{true},\\text{true},\\text{true},\\text{true},\\text{true},[\\dots]]$.",
            "solution": "We start from the cost-complexity objective for a classification tree, defined as $C_{\\alpha}(T) = R(T) + \\alpha |T|$, where $R(T)$ is the empirical misclassification count and $|T|$ is the number of leaves. Consider a fixed overall tree structure with $m$ independent, optional branches, each of which turns one leaf into two leaves when kept. For branch $j$, keeping it reduces the resubstitution error by $\\Delta R_j$ relative to pruning it and increases the number of leaves by $\\Delta |T|_j = 1$. The change in objective when keeping branch $j$ is\n$$\n\\Delta C_{\\alpha,j} = -\\Delta R_j + \\alpha \\cdot 1,\n$$\nso keeping branch $j$ improves the objective if and only if $\\Delta R_j \\ge \\alpha$. If we let $S$ be the set of kept branches, then relative to the fully pruned tree (where all $m$ optional branches are pruned), the cost-complexity objective is\n$$\nC_{\\alpha}(S) = C_{\\alpha}(\\text{base}) - \\sum_{j \\in S} \\Delta R_j + \\alpha |S|.\n$$\nMinimizing $C_{\\alpha}(S)$ over $S$ is equivalent to maximizing $\\sum_{j \\in S} \\Delta R_j - \\alpha |S|$. For the unit-cost case $\\Delta |T|_j = 1$ for all $j$, this set function is additively separable across $j$; it is optimal to include exactly those $j$ with $\\Delta R_j \\ge \\alpha$. This establishes the equivalence between cost-complexity pruning at fixed $\\alpha$ and a greedy threshold rule. The boundary convention $\\Delta R_j \\ge \\alpha$ ensures deterministic outcomes when $\\Delta R_j = \\alpha$ produces a tie, because including such a branch does not change the objective.\n\nWe now relate the formulation to the $0$–$1$ knapsack problem. Consider the budget-constrained optimization\n$$\n\\max_{S \\subseteq \\{1,\\dots,m\\}} \\sum_{j \\in S} \\Delta R_j\n\\quad \\text{subject to} \\quad |S| \\le B.\n$$\nThis is a knapsack problem with unit costs and values $\\Delta R_j$. With unit costs, the optimal solution is to choose the $B$ branches with the largest $\\Delta R_j$ values. There exists a Lagrange multiplier $\\alpha$ such that the set $\\{j: \\Delta R_j \\ge \\alpha\\}$ has cardinality $B$, provided $\\alpha$ is chosen strictly between the $B$-th and $(B+1)$-th largest distinct $\\Delta R_j$. Thus, the Lagrangian form and the budgeted knapsack solution coincide in this unit-cost setting.\n\nFor weakest-link pruning, the effective complexity parameter at which branch $j$ becomes indifferent is the ratio $\\Delta R_j / \\Delta |T|_j$. Since $\\Delta |T|_j = 1$, this equals $\\Delta R_j$. The weakest-link algorithm removes branches in order of increasing $\\Delta R_j$. As $\\alpha$ increases past a given $\\Delta R_j$, that branch is pruned. Therefore, the entire path of optimal subtrees indexed by $\\alpha$ is in bijection with the threshold rule “keep $j$ if and only if $\\Delta R_j \\ge \\alpha$.” The critical $\\alpha$ values at which the subtree changes are precisely the distinct $\\Delta R_j$ values.\n\nWe now describe the algorithmic checks.\n\n1) Exact cost-complexity optimal subset versus threshold rule:\n- Exact search: enumerate all subsets $S \\subseteq \\{1,\\dots,m\\}$ and select the set maximizing $\\sum_{j \\in S} \\Delta R_j - \\alpha |S|$.\n- Greedy threshold: keep $j$ if and only if $\\Delta R_j \\ge \\alpha$.\n- Compare the selected index sets for equality.\n\n2) Boundary ties:\n- Use the same comparison when some $\\Delta R_j = \\alpha$, with the rule to keep when $\\Delta R_j \\ge \\alpha$.\n- Exhaustive search maximizes the same objective and should agree because adding a branch with $\\Delta R_j = \\alpha$ does not change the objective and we choose the convention that includes tied items.\n\n3) Budget-constrained knapsack versus Lagrangian:\n- Solve the unit-cost knapsack by selecting the $B$ largest $\\Delta R_j$.\n- Choose an $\\alpha$ strictly between the $B$-th and $(B+1)$-th largest $\\Delta R_j$ and compute the Lagrangian threshold set.\n- Verify that the two sets are equal and also maximize $\\sum \\Delta R_j$ subject to $|S| \\le B$.\n\n4) Weakest-link path equivalence:\n- Sort $\\Delta R_j$ and consider a grid of $\\alpha$ values spanning below the minimum $\\Delta R_j$ and above the maximum $\\Delta R_j$.\n- For each $\\alpha$, the threshold set is $\\{j: \\Delta R_j \\ge \\alpha\\}$.\n- The weakest-link set after removing all branches with $\\Delta R_j < \\alpha$ is the same set; verify equality for each $\\alpha$ in the grid.\n\n5) Explicit counts and computed $\\Delta R_j$:\n- For each group $j$, compute\n$$\n\\Delta R_j = \\min(p_{j0}+p_{j1},\\, n_{j0}+n_{j1}) - \\big(\\min(p_{j0}, n_{j0}) + \\min(p_{j1}, n_{j1})\\big).\n$$\n- Use $\\alpha = 1.5$ in misclassification counts and repeat the comparison between exhaustive optimization and the threshold rule, as well as consistency with weakest-link.\n\nTest suite details:\n- Test $1$: $\\Delta R = \\{0.31, 0.12, 0.27, 0.08, 0.19, 0.05, 0.14, 0.22\\}$, $\\alpha = 0.15$.\n- Test $2$: $\\Delta R = \\{0.20, 0.15, 0.15, 0.10\\}$, $\\alpha = 0.15$.\n- Test $3$: same $\\Delta R$ as Test $1$, budget $B = 3$, and choose $\\alpha = 0.205$ (the midpoint between the third-largest $0.22$ and the fourth-largest $0.19$).\n- Test $4$: same $\\Delta R$ as Test $1$, a grid of $\\alpha \\in \\{0.00, 0.06, 0.10, 0.135, 0.17, 0.205, 0.245, 0.29, 0.33\\}$.\n- Test $5$: counts as specified in the problem statement, compute $\\Delta R$ and use $\\alpha = 1.5$.\n\nThe program implements:\n- Exhaustive subset enumeration for exact cost-complexity optimization with fixed $\\alpha$.\n- The threshold rule by direct comparison to $\\alpha$.\n- Unit-cost knapsack by selecting the top $B$ values, and also a dynamic programming confirmation if desired.\n- Weakest-link path by sorting $\\Delta R$ and comparing with thresholded sets across the grid.\n- Computation of $\\Delta R$ from the explicit subgroup counts.\n\nFinally, it prints a single line with a list containing five booleans for Tests $1$–$5$ and the sorted list of unique $\\Delta R$ values for Test $1$. This output confirms the equivalences and provides the critical $\\alpha$ breakpoints implied by the weakest-link path.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef exact_alpha_optimal_indices(delta_r, alpha):\n    \"\"\"\n    Exhaustive search over all subsets to maximize sum(delta_r[j]) - alpha * |S|.\n    Returns the set of indices achieving the maximum (ties broken by choosing the\n    lexicographically smallest bitmask for determinism).\n    \"\"\"\n    m = len(delta_r)\n    best_val = -1e100\n    best_mask = 0\n    # Enumerate all subsets\n    for mask in range(1 << m):\n        k = 0\n        val = 0.0\n        for j in range(m):\n            if (mask >> j) & 1:\n                k += 1\n                val += delta_r[j]\n        val -= alpha * k\n        if val > best_val or (abs(val - best_val) < 1e-12 and mask < best_mask):\n            best_val = val\n            best_mask = mask\n    # Extract indices\n    idx = [j for j in range(m) if (best_mask >> j) & 1]\n    return set(idx)\n\ndef threshold_indices(delta_r, alpha):\n    \"\"\"\n    Threshold rule: keep j iff delta_r[j] >= alpha.\n    \"\"\"\n    return {j for j, dr in enumerate(delta_r) if dr >= alpha}\n\ndef dp_knapsack_unit_cost(delta_r, B):\n    \"\"\"\n    Unit-cost knapsack dynamic programming to choose up to B items\n    maximizing total value sum(delta_r[j]).\n    Returns the set of chosen indices.\n    \"\"\"\n    m = len(delta_r)\n    # dp[i][b] = best value using first i items with capacity b\n    dp = np.full((m + 1, B + 1), -np.inf)\n    dp[0, :] = 0.0\n    # keep track of choices\n    take = np.zeros((m + 1, B + 1), dtype=bool)\n    for i in range(1, m + 1):\n        val = delta_r[i - 1]\n        for b in range(B + 1):\n            # not take\n            best = dp[i - 1, b]\n            take_flag = False\n            # take if possible\n            if b >= 1 and dp[i - 1, b - 1] + val > best:\n                best = dp[i - 1, b - 1] + val\n                take_flag = True\n            dp[i, b] = best\n            take[i, b] = take_flag\n    # Backtrack to recover items\n    res = set()\n    b = B\n    for i in range(m, 0, -1):\n        if take[i, b]:\n            res.add(i - 1)\n            b -= 1\n    return res\n\ndef top_B_indices(delta_r, B):\n    \"\"\"\n    Greedy selection: choose indices of the top B values in delta_r.\n    \"\"\"\n    arr = np.array(delta_r)\n    # argsort descending, take first B\n    idx = np.argsort(-arr)[:B]\n    return set(idx.tolist())\n\ndef weakest_link_sets_over_alpha(delta_r, alpha_grid):\n    \"\"\"\n    For each alpha in alpha_grid, compute set of kept indices by weakest-link notion,\n    which is equivalent to thresholding for unit costs.\n    Returns a list of sets corresponding to alpha_grid.\n    \"\"\"\n    sets = []\n    for a in alpha_grid:\n        keep = {j for j, dr in enumerate(delta_r) if dr >= a}\n        sets.append(keep)\n    return sets\n\ndef compute_delta_r_from_counts(groups):\n    \"\"\"\n    groups: list of tuples ((p0, n0), (p1, n1))\n    Returns a list of Delta R values (counts) per group.\n    \"\"\"\n    delta = []\n    for (p0, n0), (p1, n1) in groups:\n        base_err = min(p0 + p1, n0 + n1)\n        after_err = min(p0, n0) + min(p1, n1)\n        delta.append(base_err - after_err)\n    return delta\n\ndef solve():\n    # Test 1: happy path\n    delta1 = [0.31, 0.12, 0.27, 0.08, 0.19, 0.05, 0.14, 0.22]\n    alpha1 = 0.15\n    exact1 = exact_alpha_optimal_indices(delta1, alpha1)\n    thresh1 = threshold_indices(delta1, alpha1)\n    res1 = (exact1 == thresh1)\n\n    # Test 2: boundary ties\n    delta2 = [0.20, 0.15, 0.15, 0.10]\n    alpha2 = 0.15\n    exact2 = exact_alpha_optimal_indices(delta2, alpha2)\n    thresh2 = threshold_indices(delta2, alpha2)  # keep >= alpha\n    res2 = (exact2 == thresh2)\n\n    # Test 3: budget vs Lagrangian\n    B3 = 3\n    # Greedy top-B\n    topB = top_B_indices(delta1, B3)\n    # DP unit-cost knapsack confirmation\n    dpB = dp_knapsack_unit_cost(delta1, B3)\n    # Choose alpha between 3rd and 4th largest distinct values\n    sorted_vals = sorted(delta1, reverse=True)\n    # Distinct sorted values\n    distinct_sorted = []\n    for v in sorted_vals:\n        if len(distinct_sorted) == 0 or abs(distinct_sorted[-1] - v) > 1e-12:\n            distinct_sorted.append(v)\n    # Third and fourth largest distinct\n    third = distinct_sorted[2]\n    fourth = distinct_sorted[3]\n    alpha3 = (third + fourth) / 2.0\n    lagr3 = threshold_indices(delta1, alpha3)\n    res3 = (topB == dpB == lagr3)\n\n    # Test 4: weakest-link path equivalence across alpha grid\n    alpha_grid4 = [0.00, 0.06, 0.10, 0.135, 0.17, 0.205, 0.245, 0.29, 0.33]\n    wl_sets = weakest_link_sets_over_alpha(delta1, alpha_grid4)\n    thr_sets = [threshold_indices(delta1, a) for a in alpha_grid4]\n    res4 = all(wl_sets[i] == thr_sets[i] for i in range(len(alpha_grid4)))\n\n    # Test 5: explicit counts and alpha in counts\n    groups5 = [\n        ((7, 1), (1, 5)),  # Group 0\n        ((3, 2), (2, 3)),  # Group 1\n        ((6, 0), (0, 4)),  # Group 2\n        ((2, 1), (1, 2)),  # Group 3\n    ]\n    delta5 = compute_delta_r_from_counts(groups5)  # counts\n    alpha5 = 1.5\n    exact5 = exact_alpha_optimal_indices(delta5, alpha5)\n    thresh5 = threshold_indices(delta5, alpha5)\n    # For weakest-link, with unit costs it matches threshold as well\n    wl5 = threshold_indices(delta5, alpha5)\n    res5 = (exact5 == thresh5 == wl5)\n\n    # Entry 6: sorted unique Delta R values for Test 1\n    unique_sorted_delta1 = sorted({float(x) for x in delta1})\n\n    results = [res1, res2, res3, res4, res5, unique_sorted_delta1]\n    print(\"[\" + \",\".join(repr(x) for x in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}