{
    "hands_on_practices": [
        {
            "introduction": "Information gain is a powerful metric used in decision trees to identify the most informative features for splitting data. A feature that effectively separates classes will yield a high information gain. However, this metric can sometimes be misleading, especially in datasets with many features, such as text classification. This exercise demonstrates a classic pitfall where a very rare feature achieves a high information gain by creating a small, pure node, a scenario that often leads to overfitting. By calculating and comparing the information gain for a rare versus a common feature, you will gain a practical understanding of why applying constraints like minimum support is crucial for building robust models. ",
            "id": "3131370",
            "problem": "A binary text classification task has two classes, labeled $y \\in \\{+1,-1\\}$. The training set contains $N=20$ documents, with $N_{+}=10$ positive and $N_{-}=10$ negative. Consider two candidate binary features corresponding to the presence of two words in a document: the rare word “zephyr” and the more common word “team.” The observed document-level co-occurrence counts in the training set are as follows:\n- For “zephyr”: among the $n_{z}=4$ documents containing the word, $4$ are positive and $0$ are negative; among the $N-n_{z}=16$ documents not containing the word, $6$ are positive and $10$ are negative.\n- For “team”: among the $n_{t}=10$ documents containing the word, $6$ are positive and $4$ are negative; among the $N-n_{t}=10$ documents not containing the word, $4$ are positive and $6$ are negative.\n\nUsing only standard definitions from information theory and decision tree splitting in statistical learning, first compute the information gain (IG) for splitting on the presence of “zephyr,” then compute the information gain for splitting on the presence of “team,” both measured in bits (that is, base $2$ logarithms). Argue, based on these calculations, how the rare word can appear to be a strong splitter on the training data and why this may reflect overfitting in a text setting.\n\nTo mitigate overfitting, impose a minimum support constraint requiring any chosen split word to appear in at least $s=5$ training documents. Under this constraint, determine the best available split among the two candidates and compute its information gain in bits. Report as your final answer the information gain of the best available split under the minimum support constraint $s=5$, rounded to $4$ significant figures.",
            "solution": "The user wants me to solve a statistical learning problem related to decision trees and information gain.\n\nHere's a plan:\n1.  **Validate the problem statement.**\n    *   **Step 1: Extract Givens.** I will list all the numbers, variables, and conditions provided in the problem.\n    *   **Step 2: Validate.** I will check the problem for scientific soundness, well-posedness, objectivity, completeness, etc.\n        *   **Scientific Grounding:** The problem uses standard definitions from information theory (entropy, information gain) and statistical learning (decision trees, splitting criteria, overfitting). These are well-established concepts.\n        *   **Well-Posed:** The problem asks for specific calculations (information gain for two features) and then to apply a constraint and re-evaluate. The steps are clearly defined. A unique solution exists.\n        *   **Objective:** The language is formal and quantitative. No subjective claims.\n        *   **Completeness:** All necessary numbers are provided to calculate the required quantities.\n            *   Total documents $N=20$.\n            *   Positive documents $N_+=10$.\n            *   Negative documents $N_-=10$.\n            *   **Feature 'zephyr'**:\n                *   Documents containing 'zephyr': $n_z = 4$.\n                *   Of these 4, 4 are positive, 0 are negative.\n                *   Documents not containing 'zephyr': $N-n_z = 16$.\n                *   Of these 16, 6 are positive, 10 are negative.\n                *   Check for consistency: Total positive = $4+6 = 10$. Total negative = $0+10 = 10$. Total docs = $4+16 = 20$. The numbers are consistent.\n            *   **Feature 'team'**:\n                *   Documents containing 'team': $n_t = 10$.\n                *   Of these 10, 6 are positive, 4 are negative.\n                *   Documents not containing 'team': $N-n_t = 10$.\n                *   Of these 10, 4 are positive, 6 are negative.\n                *   Check for consistency: Total positive = $6+4=10$. Total negative = $4+6=10$. Total docs = $10+10 = 20$. The numbers are consistent.\n            *   **Constraint:** Minimum support $s=5$.\n            *   **Task:** Compute IG for 'zephyr', IG for 'team'. Explain why the rare word might be a good splitter but could be overfitting. Then apply the constraint, find the best split, and report its IG rounded to 4 significant figures.\n        *   The problem seems valid in all respects.\n\n2.  **Proceed with the solution.**\n    *   **Define Information Gain (IG):**\n        *   $IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$\n        *   Where $S$ is the set of examples, $A$ is the attribute to split on.\n        *   $H(S)$ is the entropy of the set $S$.\n        *   The entropy for a binary classification problem is $H(S) = -p_+ \\log_2(p_+) - p_- \\log_2(p_-)$, where $p_+$ and $p_-$ are the proportions of positive and negative examples in $S$.\n\n    *   **Step 1: Calculate the entropy of the parent node (the entire dataset).**\n        *   The dataset has $N=20$ documents.\n        *   $N_+ = 10$ positive, $N_- = 10$ negative.\n        *   $p_+ = \\frac{10}{20} = \\frac{1}{2}$.\n        *   $p_- = \\frac{10}{20} = \\frac{1}{2}$.\n        *   $H(\\text{Parent}) = -(\\frac{1}{2} \\log_2(\\frac{1}{2}) + \\frac{1}{2} \\log_2(\\frac{1}{2})) = -(\\log_2(\\frac{1}{2})) = -(-\\log_2(2)) = 1$ bit. This is the maximum possible entropy for a binary class.\n\n    *   **Step 2: Calculate Information Gain for splitting on \"zephyr\".**\n        *   The attribute is the presence/absence of \"zephyr\".\n        *   **Child node 1 (has \"zephyr\"):**\n            *   Number of documents: $n_z = 4$.\n            *   Class distribution: $4$ positive, $0$ negative.\n            *   $p_+ = \\frac{4}{4} = 1$. $p_- = \\frac{0}{4} = 0$.\n            *   Entropy $H(\\text{has 'zephyr'}) = - (1 \\log_2(1) + 0 \\log_2(0)) = 0$. (Note: $0 \\log_2(0)$ is defined as $0$). The node is pure.\n        *   **Child node 2 (no \"zephyr\"):**\n            *   Number of documents: $16$.\n            *   Class distribution: $6$ positive, $10$ negative.\n            *   $p_+ = \\frac{6}{16} = \\frac{3}{8}$.\n            *   $p_- = \\frac{10}{16} = \\frac{5}{8}$.\n            *   Entropy $H(\\text{no 'zephyr'}) = - (\\frac{3}{8} \\log_2(\\frac{3}{8}) + \\frac{5}{8} \\log_2(\\frac{5}{8}))$.\n        *   **Calculate weighted average entropy of children:**\n            *   $H_{\\text{avg}}(\\text{'zephyr'}) = \\frac{4}{20} H(\\text{has 'zephyr'}) + \\frac{16}{20} H(\\text{no 'zephyr'})$\n            *   $H_{\\text{avg}}(\\text{'zephyr'}) = \\frac{1}{5} (0) + \\frac{4}{5} \\left[ - (\\frac{3}{8} \\log_2(\\frac{3}{8}) + \\frac{5}{8} \\log_2(\\frac{5}{8})) \\right]$\n            *   $H_{\\text{avg}}(\\text{'zephyr'}) = - \\frac{4}{5} (\\frac{3}{8} (\\log_2(3) - \\log_2(8)) + \\frac{5}{8} (\\log_2(5) - \\log_2(8)))$\n            *   $H_{\\text{avg}}(\\text{'zephyr'}) = - \\frac{1}{10} (3 (\\log_2(3) - 3) + 5 (\\log_2(5) - 3))$\n            *   $H_{\\text{avg}}(\\text{'zephyr'}) = - \\frac{1}{10} (3 \\log_2(3) - 9 + 5 \\log_2(5) - 15) = - \\frac{1}{10} (3 \\log_2(3) + 5 \\log_2(5) - 24)$\n            *   $H_{\\text{avg}}(\\text{'zephyr'}) = \\frac{24 - 3 \\log_2(3) - 5 \\log_2(5)}{10}$.\n            *   Let's calculate the numerical value of $H(\\text{no 'zephyr'})$ first.\n            *   $H(\\text{no 'zephyr'}) = - (\\frac{3}{8} \\log_2(\\frac{3}{8}) + \\frac{5}{8} \\log_2(\\frac{5}{8})) \\approx - (0.375 \\times (-1.415) + 0.625 \\times (-0.678)) \\approx -(-0.5306 - 0.4238) \\approx 0.9544$ bits.\n            *   $H_{\\text{avg}}(\\text{'zephyr'}) = \\frac{16}{20} \\times H(\\text{no 'zephyr'}) = \\frac{4}{5} \\times 0.9544 \\approx 0.7635$ bits.\n        *   **Calculate Information Gain for \"zephyr\":**\n            *   $IG(\\text{'zephyr'}) = H(\\text{Parent}) - H_{\\text{avg}}(\\text{'zephyr'})$\n            *   $IG(\\text{'zephyr'}) = 1 - 0.7635 = 0.2365$ bits. Let's do it more precisely.\n            *   $IG(\\text{'zephyr'}) = 1 - \\frac{16}{20} \\left[ - \\frac{6}{16} \\log_2(\\frac{6}{16}) - \\frac{10}{16} \\log_2(\\frac{10}{16}) \\right]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{16}{20} \\left[ \\frac{6}{16} \\log_2(\\frac{6}{16}) + \\frac{10}{16} \\log_2(\\frac{10}{16}) \\right]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{1}{20} \\left[ 6 \\log_2(\\frac{6}{16}) + 10 \\log_2(\\frac{10}{16}) \\right]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{1}{20} \\left[ 6 \\log_2(\\frac{3}{8}) + 10 \\log_2(\\frac{5}{8}) \\right]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{1}{20} [ 6(\\log_2(3) - 3) + 10(\\log_2(5) - 3) ]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{1}{20} [ 6\\log_2(3) - 18 + 10\\log_2(5) - 30 ]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{6\\log_2(3) + 10\\log_2(5) - 48}{20} = \\frac{20 + 6\\log_2(3) + 10\\log_2(5) - 48}{20} = \\frac{6\\log_2(3) + 10\\log_2(5) - 28}{20}$\n            *   Using calculator: $\\log_2(3) \\approx 1.58496$, $\\log_2(5) \\approx 2.32193$.\n            *   $IG(\\text{'zephyr'}) \\approx \\frac{6(1.58496) + 10(2.32193) - 28}{20} = \\frac{9.50976 + 23.2193 - 28}{20} = \\frac{4.72906}{20} \\approx 0.236453$.\n\n    *   **Step 3: Calculate Information Gain for splitting on \"team\".**\n        *   **Child node 1 (has \"team\"):**\n            *   Number of documents: $n_t = 10$.\n            *   Class distribution: $6$ positive, $4$ negative.\n            *   $p_+ = \\frac{6}{10} = \\frac{3}{5}$. $p_- = \\frac{4}{10} = \\frac{2}{5}$.\n            *   Entropy $H(\\text{has 'team'}) = - (\\frac{3}{5} \\log_2(\\frac{3}{5}) + \\frac{2}{5} \\log_2(\\frac{2}{5}))$.\n        *   **Child node 2 (no \"team\"):**\n            *   Number of documents: $10$.\n            *   Class distribution: $4$ positive, $6$ negative.\n            *   $p_+ = \\frac{4}{10} = \\frac{2}{5}$. $p_- = \\frac{6}{10} = \\frac{3}{5}$.\n            *   Entropy $H(\\text{no 'team'}) = - (\\frac{2}{5} \\log_2(\\frac{2}{5}) + \\frac{3}{5} \\log_2(\\frac{3}{5}))$.\n            *   Notice that $H(\\text{has 'team'}) = H(\\text{no 'team'})$.\n        *   **Calculate weighted average entropy of children:**\n            *   $H_{\\text{avg}}(\\text{'team'}) = \\frac{10}{20} H(\\text{has 'team'}) + \\frac{10}{20} H(\\text{no 'team'})$\n            *   $H_{\\text{avg}}(\\text{'team'}) = \\frac{1}{2} H(\\text{has 'team'}) + \\frac{1}{2} H(\\text{has 'team'}) = H(\\text{has 'team'})$\n            *   $H_{\\text{avg}}(\\text{'team'}) = - (\\frac{3}{5} \\log_2(\\frac{3}{5}) + \\frac{2}{5} \\log_2(\\frac{2}{5}))$.\n            *   Let's calculate the numerical value.\n            *   $H_{\\text{avg}}(\\text{'team'}) = - (0.6 \\times \\log_2(0.6) + 0.4 \\times \\log_2(0.4)) \\approx - (0.6 \\times (-0.737) + 0.4 \\times (-1.322)) \\approx -(-0.4422 - 0.5288) \\approx 0.9710$ bits.\n        *   **Calculate Information Gain for \"team\":**\n            *   $IG(\\text{'team'}) = H(\\text{Parent}) - H_{\\text{avg}}(\\text{'team'})$\n            *   $IG(\\text{'team'}) = 1 - 0.9710 = 0.0290$ bits.\n            *   Let's do it precisely.\n            *   $IG(\\text{'team'}) = 1 - \\left[ - \\frac{3}{5} \\log_2(\\frac{3}{5}) - \\frac{2}{5} \\log_2(\\frac{2}{5}) \\right]$\n            *   $IG(\\text{'team'}) = 1 + \\frac{3}{5} (\\log_2(3) - \\log_2(5)) + \\frac{2}{5} (\\log_2(2) - \\log_2(5))$\n            *   $IG(\\text{'team'}) = 1 + \\frac{3}{5} \\log_2(3) - \\frac{3}{5} \\log_2(5) + \\frac{2}{5} - \\frac{2}{5} \\log_2(5)$\n            *   $IG(\\text{'team'}) = \\frac{7}{5} + \\frac{3}{5} \\log_2(3) - \\log_2(5)$\n            *   Using calculator: $IG(\\text{'team'}) \\approx 1.4 + \\frac{3}{5}(1.58496) - 2.32193 = 1.4 + 0.950976 - 2.32193 = 2.350976 - 2.32193 = 0.029046$.\n\n    *   **Comparison and Overfitting Argument:**\n        *   $IG(\\text{'zephyr'}) \\approx 0.2365$ bits.\n        *   $IG(\\text{'team'}) \\approx 0.0290$ bits.\n        *   $IG(\\text{'zephyr'}) > IG(\\text{'team'})$. So, \"zephyr\" seems like a much better splitter on the training data.\n        *   The reason it appears so strong is that it creates a perfectly pure node. All $4$ documents containing \"zephyr\" are positive. This is a very strong signal.\n        *   However, this word is rare ($n_z=4$). Finding such a perfect correlation on a small number of instances is a classic sign of overfitting. The rule \"if 'zephyr' is present, predict positive\" is likely to be a spurious correlation that exists only in this specific small training set. It is unlikely to generalize to new, unseen data. Rare words can, by chance, appear only in documents of one class in a small sample, making them look like powerful predictors. This is a form of learning noise from the data.\n\n    *   **Step 4: Apply the minimum support constraint.**\n        *   The constraint requires the split word to appear in at least $s=5$ training documents.\n        *   Check \"zephyr\": Number of documents containing it is $n_z = 4$. Since $4 < 5$, this split is **not allowed**.\n        *   Check \"team\": Number of documents containing it is $n_t = 10$. Since $10 \\geq 5$, this split is **allowed**.\n        *   Under the constraint, \"team\" is the only available candidate. Therefore, it is the best available split.\n\n    *   **Step 5: Report the Information Gain of the best split under the constraint.**\n        *   The best available split is \"team\".\n        *   Its information gain is $IG(\\text{'team'}) \\approx 0.029046$ bits.\n        *   The problem asks to round the final answer to $4$ significant figures.\n        *   $0.029046 \\to 0.02905$.\n\n    *   **Final check of the calculations.**\n        *   Parent entropy: $H(10, 10) = -(\\frac{10}{20}\\log_2\\frac{10}{20} + \\frac{10}{20}\\log_2\\frac{10}{20}) = 1$. Correct.\n        *   'zephyr' split:\n            *   Branch 1 (word present): $n_1=4$, counts $(4, 0)$. $H(4,0) = 0$.\n            *   Branch 2 (word absent): $n_2=16$, counts $(6, 10)$. $H(6,10) = -(\\frac{6}{16}\\log_2\\frac{6}{16} + \\frac{10}{16}\\log_2\\frac{10}{16}) = -(\\frac{3}{8}\\log_2\\frac{3}{8} + \\frac{5}{8}\\log_2\\frac{5}{8})$.\n            *   $H_{after} = \\frac{4}{20} H(4,0) + \\frac{16}{20} H(6,10) = \\frac{4}{5} H(6,10)$.\n            *   $IG(\\text{'zephyr'}) = H(10,10) - \\frac{4}{5} H(6,10) = 1 - \\frac{4}{5} [ -(\\frac{3}{8}\\log_2\\frac{3}{8} + \\frac{5}{8}\\log_2\\frac{5}{8}) ]$.\n            *   Using $\\log_2(x) = \\ln(x)/\\ln(2)$:\n            *   $H(6,10) = -(\\frac{3}{8} \\frac{\\ln(3/8)}{\\ln(2)} + \\frac{5}{8} \\frac{\\ln(5/8)}{\\ln(2)}) \\approx -(-0.530628 - 0.423792) \\approx 0.95442$.\n            *   $IG(\\text{'zephyr'}) \\approx 1 - \\frac{4}{5} (0.95442) = 1 - 0.763536 = 0.236464$. The previous calculation was correct.\n        *   'team' split:\n            *   Branch 1 (word present): $n_1=10$, counts $(6, 4)$. $H(6,4) = -(\\frac{6}{10}\\log_2\\frac{6}{10} + \\frac{4}{10}\\log_2\\frac{4}{10})$.\n            *   Branch 2 (word absent): $n_2=10$, counts $(4, 6)$. $H(4,6) = -(\\frac{4}{10}\\log_2\\frac{4}{10} + \\frac{6}{10}\\log_2\\frac{6}{10}) = H(6,4)$.\n            *   $H_{after} = \\frac{10}{20} H(6,4) + \\frac{10}{20} H(4,6) = H(6,4)$.\n            *   $IG(\\text{'team'}) = H(10,10) - H(6,4) = 1 - [ -(\\frac{3}{5}\\log_2\\frac{3}{5} + \\frac{2}{5}\\log_2\\frac{2}{5}) ]$.\n            *   Using $\\log_2(x) = \\ln(x)/\\ln(2)$:\n            *   $H(6,4) = -(\\frac{3}{5}\\frac{\\ln(3/5)}{\\ln(2)} + \\frac{2}{5}\\frac{\\ln(2/5)}{\\ln(2)}) \\approx -(-0.442179 - 0.528771) \\approx 0.97095$.\n            *   $IG(\\text{'team'}) \\approx 1 - 0.97095 = 0.02905$. The previous calculation was correct.\n\n    *   **Assemble the final solution.**\n        *   Start with validating the problem statement. State it is valid.\n        *   Define the entropy $H$ and information gain $IG$.\n        *   Calculate the parent entropy $H(\\text{Parent})$.\n        *   Calculate $IG(\\text{zephyr})$.\n        *   Calculate $IG(\\text{team})$.\n        *   Compare the two IG values and present the overfitting argument.\n        *   Apply the minimum support constraint $s=5$.\n        *   Identify \"zephyr\" as invalid and \"team\" as the best available split.\n        *   State the final answer, which is $IG(\\text{team})$ rounded to 4 significant figures.\n        *   $0.02905$. This has 4 sig figs. My previous rounding to $0.02905$ from $0.029046$ was correct.\n        *   The entire output should be in LaTeX.\n        *   I will double check all formulas and calculations to ensure correctness.\n\n        *   Calculation of $IG(\\text{'team'})$:\n            $H_{after} = H(6/10, 4/10) = - \\frac{6}{10}\\log_2\\frac{6}{10} - \\frac{4}{10}\\log_2\\frac{4}{10}$\n            $= -0.6 \\log_2(0.6) - 0.4 \\log_2(0.4)$\n            $= -0.6 \\frac{\\ln(0.6)}{\\ln(2)} - 0.4 \\frac{\\ln(0.4)}{\\ln(2)}$\n            $= \\frac{-0.6(-0.5108256) - 0.4(-0.9162907)}{0.693147}$\n            $= \\frac{0.30649536 + 0.36651628}{0.693147} = \\frac{0.67301164}{0.693147} \\approx 0.9709506$\n            $IG = 1 - 0.9709506 = 0.0290494$\n            Rounding to 4 significant figures: $0.02905$.\n\n        *   Calculation of $IG(\\text{'zephyr'})$:\n            $H_{after} = \\frac{16}{20} H(6/16, 10/16) + \\frac{4}{20}(0) = 0.8 \\times H(3/8, 5/8)$\n            $H(3/8, 5/8) = - \\frac{3}{8}\\log_2\\frac{3}{8} - \\frac{5}{8}\\log_2\\frac{5}{8}$\n            $= -0.375 \\log_2(0.375) - 0.625 \\log_2(0.625)$\n            $= -0.375 \\frac{\\ln(0.375)}{\\ln(2)} - 0.625 \\frac{\\ln(0.625)}{\\ln(2)}$\n            $= \\frac{-0.375(-0.980829) - 0.625(-0.470004)}{0.693147}$\n            $= \\frac{0.36781 + 0.29375}{0.693147} = \\frac{0.66156}{0.693147} \\approx 0.954434$\n            $H_{after} = 0.8 \\times 0.954434 = 0.7635472$\n            $IG = 1 - 0.7635472 = 0.2364528$\n            \n        All calculations are confirmed. The reasoning is sound. I am ready to write the solution. I will follow the structure strictly, including the mandatory LaTeX for all math entities.",
            "answer": "$$\n\\boxed{0.02905}\n$$"
        },
        {
            "introduction": "Decision tree algorithms are typically \"greedy,\" meaning they make the best possible split at each step without looking ahead to future splits. While efficient, this greedy approach is not guaranteed to find the globally optimal tree. This thought experiment uses a specially constructed dataset to illustrate this limitation. You will analyze a scenario where one feature offers a modest initial information gain, while another offers none, yet the first feature unlocks a path to a perfect classification, revealing a critical feature interaction that a greedy one-step-ahead evaluation would miss. ",
            "id": "3131390",
            "problem": "A binary classification task is constructed to highlight the limits of greedy feature selection in decision trees by exhibiting an interaction that only becomes exploitable after a modest early split. Consider a dataset of $16$ examples with binary features $A,B,C \\in \\{0,1\\}$ and binary label $Y \\in \\{0,1\\}$. The data are defined as follows.\n\n1. Across the entire dataset, the joint values $(A,C)$ are uniformly distributed: each of $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$ occurs exactly $4$ times. The label is deterministically given by the exclusive OR (XOR), $Y = A \\oplus C$.\n\n2. The feature $B$ is correlated with $Y$ such that exactly $12$ examples satisfy $B=Y$ and exactly $4$ examples satisfy $B \\neq Y$. Moreover, this correlation is arranged symmetrically across $(A,C)$ as follows:\n   - Within the subset $B=1$ (which has $8$ examples), there are $6$ with $Y=1$ and $2$ with $Y=0$, with $(A,C)$ composition: $(0,1)$ occurs $3$ times, $(1,0)$ occurs $3$ times, $(0,0)$ occurs $1$ time, and $(1,1)$ occurs $1$ time.\n   - Within the subset $B=0$ (which has $8$ examples), there are $6$ with $Y=0$ and $2$ with $Y=1$, with $(A,C)$ composition: $(0,0)$ occurs $3$ times, $(1,1)$ occurs $3$ times, $(0,1)$ occurs $1$ time, and $(1,0)$ occurs $1$ time.\n\nUse the standard definition of entropy with logarithm base $2$, and define information gain (IG) as the reduction in entropy. Consider decision trees limited to depth $2$ (a root split and one additional split per branch). Let $G_A$ denote the maximum total information gain achievable by any depth-$2$ tree whose root split is constrained to be on feature $A$, and let $G_B$ denote the maximum total information gain achievable by any depth-$2$ tree whose root split is constrained to be on feature $B$.\n\nCompute the quantity $\\Delta = G_A - G_B$ in bits. Round your answer to four significant figures.",
            "solution": "We begin from the core definitions. For a binary label $Y$ with empirical class probabilities $p$ and $1-p$, the entropy is\n$$\nH(Y) \\;=\\; -\\,p\\,\\log_{2}(p)\\;-\\;(1-p)\\,\\log_{2}(1-p).\n$$\nFor a feature $X$, the information gain of splitting on $X$ is the reduction in entropy,\n$$\n\\mathrm{IG}(Y;X) \\;=\\; H(Y) \\;-\\; H(Y\\mid X),\n$$\nwhere\n$$\nH(Y\\mid X)\\;=\\;\\sum_{x} P(X=x)\\,H(Y\\mid X=x).\n$$\nA depth-$2$ tree with root $X$ and one further split per child has total information gain equal to the root entropy $H(Y)$ minus the final weighted conditional entropy after the second splits.\n\nStep $1$: Compute the root entropy $H(Y)$. The construction has $(A,C)$ uniformly distributed and $Y = A \\oplus C$, so exactly half the examples have $Y=1$ and half have $Y=0$. Thus $P(Y=1)=P(Y=0)=\\frac{1}{2}$, and\n$$\nH(Y) \\;=\\; -\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\;-\\;\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\;=\\;1.\n$$\n\nStep $2$: Evaluate $G_A$, the best depth-$2$ total information gain with root split on $A$.\n\n- First, compute $H(Y\\mid A)$. Because $(A,C)$ is uniform and $Y=A\\oplus C$, conditioning on $A=0$ yields $Y=C$ with $P(Y=1\\mid A=0)=\\frac{1}{2}$, and conditioning on $A=1$ yields $Y=1-C$ with $P(Y=1\\mid A=1)=\\frac{1}{2}$. Hence $H(Y\\mid A=0)=1$ and $H(Y\\mid A=1)=1$, so $H(Y\\mid A)=1$ and the root information gain on $A$ is $0$.\n- However, after splitting on $A$, an additional split on $C$ in each branch yields perfect purity because $Y$ is a deterministic function of $(A,C)$. Specifically, for $A=0$, $Y=C$, and for $A=1$, $Y=1-C$. Thus $H(Y\\mid A,C)=0$ in both branches. Therefore, the final weighted conditional entropy after the second split is $0$.\n- Consequently, the total information gain with this depth-$2$ tree is\n$$\nG_A \\;=\\; H(Y) - 0 \\;=\\; 1.\n$$\n\nStep $3$: Evaluate $G_B$, the best depth-$2$ total information gain with root split on $B$.\n\nFrom the construction, the branch $B=1$ contains $8$ examples with $6$ of class $Y=1$ and $2$ of class $Y=0$. Therefore\n$$\nH(Y\\mid B=1) \\;=\\; -\\tfrac{6}{8}\\log_{2}\\tfrac{6}{8}\\;-\\;\\tfrac{2}{8}\\log_{2}\\tfrac{2}{8}\n\\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}.\n$$\nSimilarly, the branch $B=0$ contains $8$ examples with $6$ of class $Y=0$ and $2$ of class $Y=1$, which yields the same entropy,\n$$\nH(Y\\mid B=0) \\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}.\n$$\nThus\n$$\nH(Y\\mid B) \\;=\\; \\tfrac{1}{2}\\,H(Y\\mid B=1) + \\tfrac{1}{2}\\,H(Y\\mid B=0)\n\\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}.\n$$\nThe root information gain on $B$ is\n$$\n\\mathrm{IG}(Y;B) \\;=\\; 1 \\;-\\; \\Big(-\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\Big).\n$$\n\nNext, assess the best possible additional split within each $B$ branch. By the specified $(A,C)$ compositions, within $B=1$ the counts by $(A,C)$ are $(0,1)$: $3$, $(1,0)$: $3$ (all with $Y=1$), and $(0,0)$: $1$, $(1,1)$: $1$ (both with $Y=0$). Consider splitting on $A$ in the $B=1$ branch:\n- For $A=0$ within $B=1$, there are $3$ with $Y=1$ and $1$ with $Y=0$, so $H(Y\\mid B=1,A=0)= -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}-\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}$.\n- For $A=1$ within $B=1$, there are $3$ with $Y=1$ and $1$ with $Y=0$, yielding the same entropy.\n\nHence\n$$\nH(Y\\mid B=1, A) \\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\n\\;=\\; H(Y\\mid B=1).\n$$\nSo splitting on $A$ within $B=1$ yields no further reduction. By symmetry of the counts, splitting on $C$ within $B=1$ also yields no reduction. An analogous computation holds in the $B=0$ branch because its $(A,C)$ composition is the complement with the same $3:1$ ratios by $A$ and by $C$. Therefore, no single additional split on $A$ or $C$ within either $B$ branch reduces entropy:\n$$\nH(Y\\mid B, \\text{one more split on } A \\text{ or } C) \\;=\\; H(Y\\mid B).\n$$\nTherefore the best achievable total information gain under the depth-$2$ constraint with root $B$ is just the root gain:\n$$\nG_B \\;=\\; 1 \\;-\\; \\Big(-\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\Big)\n\\;=\\; 1 - h\\!\\left(\\tfrac{1}{4}\\right),\n$$\nwhere $h(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$ is the binary entropy function.\n\nStep $4$: Compute the requested difference\n$$\n\\Delta \\;=\\; G_A - G_B \\;=\\; 1 - \\Big(1 - h\\!\\left(\\tfrac{1}{4}\\right)\\Big) \\;=\\; h\\!\\left(\\tfrac{1}{4}\\right)\n\\;=\\; -\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\;-\\;\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}.\n$$\nNumerically,\n$$\n\\log_{2}\\tfrac{1}{4}=-2,\\quad \\log_{2}\\tfrac{3}{4}=\\log_{2}3 - 2 \\approx 1.5849625 - 2 = -0.4150375,\n$$\nso\n$$\n\\Delta \\;=\\; -\\tfrac{1}{4}(-2)\\;-\\;\\tfrac{3}{4}(-0.4150375)\n\\;=\\; 0.5 + 0.3112781 \\;=\\; 0.8112781\\ldots\n$$\nRounding to four significant figures gives $0.8113$.",
            "answer": "$$\\boxed{0.8113}$$"
        }
    ]
}