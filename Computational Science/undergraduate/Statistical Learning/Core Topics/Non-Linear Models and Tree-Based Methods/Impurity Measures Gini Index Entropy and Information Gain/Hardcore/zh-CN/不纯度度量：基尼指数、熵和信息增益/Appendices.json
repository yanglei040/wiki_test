{
    "hands_on_practices": [
        {
            "introduction": "理论是基础，但真正的理解来自于实践。在文本分类任务中，信息增益是衡量一个词（特征）区分能力的重要指标。这项练习  将指导你通过一个具体的例子，计算并比较一个罕见词和一个常见词的信息增益，并让你体会到为何需要引入最小支持度等策略来防止模型过拟合训练数据中的噪音。",
            "id": "3131370",
            "problem": "一个二元文本分类任务有两个类别，标记为 $y \\in \\{+1,-1\\}$。训练集包含 $N=20$ 个文档，其中有 $N_{+}=10$ 个正例文档和 $N_{-}=10$ 个负例文档。考虑两个候选二元特征，分别对应文档中两个词的出现：罕见词“zephyr”和更常见的词“team”。在训练集中观察到的文档级共现计数如下：\n- 对于“zephyr”：在包含该词的 $n_{z}=4$ 个文档中，$4$ 个是正例，$0$ 个是负例；在不包含该词的 $N-n_{z}=16$ 个文档中，$6$ 个是正例，$10$ 个是负例。\n- 对于“team”：在包含该词的 $n_{t}=10$ 个文档中，$6$ 个是正例，$4$ 个是负例；在不包含该词的 $N-n_{t}=10$ 个文档中，$4$ 个是正例，$6$ 个是负例。\n\n请仅使用信息论和统计学习中决策树分裂的标准定义，首先计算以“zephyr”的出现为分裂依据的信息增益（IG），然后计算以“team”的出现为分裂依据的信息增益，两者均以比特（即以 2 为底的对数）为单位度量。基于这些计算结果，论证罕见词为何在训练数据上看起来是一个强分裂器，并说明为什么这可能反映了文本场景下的过拟合。\n\n为缓解过拟合，施加一个最小支持度约束，要求任何选定的分裂词至少出现在 $s=5$ 个训练文档中。在此约束下，确定两个候选者中的最佳可用分裂，并计算其信息增益（以比特为单位）。将最小支持度约束 $s=5$ 下最佳可用分裂的信息增益作为你的最终答案，结果四舍五入到 $4$ 位有效数字。",
            "solution": "用户要求我解决一个与决策树和信息增益相关的统计学习问题。\n\n计划如下：\n1.  **验证问题陈述。**\n    *   **第1步：提取已知信息。** 我将列出问题中提供的所有数字、变量和条件。\n    *   **第2步：验证。** 我将检查问题的科学合理性、适定性、客观性、完整性等。\n        *   **科学依据：** 问题使用了信息论（熵、信息增益）和统计学习（决策树、分裂标准、过拟合）中的标准定义。这些都是公认的概念。\n        *   **适定性：** 问题要求进行具体计算（两个特征的信息增益），然后应用一个约束条件并重新评估。步骤定义清晰。存在唯一解。\n        *   **客观性：** 语言是形式化和定量的。没有主观论断。\n        *   **完整性：** 提供了计算所需量的所有必要数字。\n            *   总文档数 $N=20$。\n            *   正例文档数 $N_+=10$。\n            *   负例文档数 $N_-=10$。\n            *   **特征‘zephyr’**：\n                *   包含‘zephyr’的文档数：$n_z = 4$。\n                *   这 4 个文档中，4 个为正例，0 个为负例。\n                *   不含‘zephyr’的文档数：$N-n_z = 16$。\n                *   这 16 个文档中，6 个为正例，10 个为负例。\n                *   检查一致性：总正例数 = $4+6 = 10$。总负例数 = $0+10 = 10$。总文档数 = $4+16 = 20$。数据一致。\n            *   **特征‘team’**：\n                *   包含‘team’的文档数：$n_t = 10$。\n                *   这 10 个文档中，6 个为正例，4 个为负例。\n                *   不含‘team’的文档数：$N-n_t = 10$。\n                *   这 10 个文档中，4 个为正例，6 个为负例。\n                *   检查一致性：总正例数 = $6+4=10$。总负例数 = $4+6=10$。总文档数 = $10+10 = 20$。数据一致。\n            *   **约束条件：** 最小支持度 $s=5$。\n            *   **任务：** 计算‘zephyr’和‘team’的信息增益。解释为什么罕见词可能是好的分裂器，但可能导致过拟合。然后应用约束条件，找到最佳分裂，并报告其信息增益（四舍五入到4位有效数字）。\n        *   该问题在各方面均有效。\n\n2.  **进行求解。**\n    *   **定义信息增益 (IG)：**\n        *   $IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$\n        *   其中 $S$ 是样本集，$A$ 是用于分裂的属性。\n        *   $H(S)$ 是集合 $S$ 的熵。\n        *   对于二元分类问题，熵的公式为 $H(S) = -p_+ \\log_2(p_+) - p_- \\log_2(p_-)$，其中 $p_+$ 和 $p_-$ 是 $S$ 中正负样本的比例。\n\n    *   **第1步：计算父节点（整个数据集）的熵。**\n        *   数据集有 $N=20$ 个文档。\n        *   $N_+ = 10$ 个正例，$N_- = 10$ 个负例。\n        *   $p_+ = \\frac{10}{20} = \\frac{1}{2}$。\n        *   $p_- = \\frac{10}{20} = \\frac{1}{2}$。\n        *   $H(\\text{父节点}) = -(\\frac{1}{2} \\log_2(\\frac{1}{2}) + \\frac{1}{2} \\log_2(\\frac{1}{2})) = -(\\log_2(\\frac{1}{2})) = -(-\\log_2(2)) = 1$ 比特。这是二元分类可能的最大熵值。\n\n    *   **第2步：计算以“zephyr”为分裂依据的信息增益。**\n        *   属性是“zephyr”的出现与否。\n        *   **子节点1（包含“zephyr”）：**\n            *   文档数：$n_z = 4$。\n            *   类别分布：$4$ 个正例，$0$ 个负例。\n            *   $p_+ = \\frac{4}{4} = 1$。$p_- = \\frac{0}{4} = 0$。\n            *   熵 $H(\\text{含 'zephyr'}) = - (1 \\log_2(1) + 0 \\log_2(0)) = 0$。（注意：$0 \\log_2(0)$ 定义为 $0$）。该节点是纯的。\n        *   **子节点2（不含“zephyr”）：**\n            *   文档数：$16$。\n            *   类别分布：$6$ 个正例，$10$ 个负例。\n            *   $p_+ = \\frac{6}{16} = \\frac{3}{8}$。\n            *   $p_- = \\frac{10}{16} = \\frac{5}{8}$。\n            *   熵 $H(\\text{不含 'zephyr'}) = - (\\frac{3}{8} \\log_2(\\frac{3}{8}) + \\frac{5}{8} \\log_2(\\frac{5}{8}))$。\n        *   **计算子节点的加权平均熵：**\n            *   $H_{\\text{平均}}(\\text{'zephyr'}) = \\frac{4}{20} H(\\text{含 'zephyr'}) + \\frac{16}{20} H(\\text{不含 'zephyr'})$\n            *   $H_{\\text{平均}}(\\text{'zephyr'}) = \\frac{1}{5} (0) + \\frac{4}{5} \\left[ - (\\frac{3}{8} \\log_2(\\frac{3}{8}) + \\frac{5}{8} \\log_2(\\frac{5}{8})) \\right]$\n            *   $H_{\\text{平均}}(\\text{'zephyr'}) = - \\frac{4}{5} (\\frac{3}{8} (\\log_2(3) - \\log_2(8)) + \\frac{5}{8} (\\log_2(5) - \\log_2(8)))$\n            *   $H_{\\text{平均}}(\\text{'zephyr'}) = - \\frac{1}{10} (3 (\\log_2(3) - 3) + 5 (\\log_2(5) - 3))$\n            *   $H_{\\text{平均}}(\\text{'zephyr'}) = - \\frac{1}{10} (3 \\log_2(3) - 9 + 5 \\log_2(5) - 15) = - \\frac{1}{10} (3 \\log_2(3) + 5 \\log_2(5) - 24)$\n            *   $H_{\\text{平均}}(\\text{'zephyr'}) = \\frac{24 - 3 \\log_2(3) - 5 \\log_2(5)}{10}$。\n            *   我们先计算 $H(\\text{不含 'zephyr'})$ 的数值。\n            *   $H(\\text{不含 'zephyr'}) = - (\\frac{3}{8} \\log_2(\\frac{3}{8}) + \\frac{5}{8} \\log_2(\\frac{5}{8})) \\approx - (0.375 \\times (-1.415) + 0.625 \\times (-0.678)) \\approx -(-0.5306 - 0.4238) \\approx 0.9544$ 比特。\n            *   $H_{\\text{平均}}(\\text{'zephyr'}) = \\frac{16}{20} \\times H(\\text{不含 'zephyr'}) = \\frac{4}{5} \\times 0.9544 \\approx 0.7635$ 比特。\n        *   **计算“zephyr”的信息增益：**\n            *   $IG(\\text{'zephyr'}) = H(\\text{父节点}) - H_{\\text{平均}}(\\text{'zephyr'})$\n            *   $IG(\\text{'zephyr'}) = 1 - 0.7635 = 0.2365$ 比特。我们来更精确地计算。\n            *   $IG(\\text{'zephyr'}) = 1 - \\frac{16}{20} \\left[ - \\frac{6}{16} \\log_2(\\frac{6}{16}) - \\frac{10}{16} \\log_2(\\frac{10}{16}) \\right]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{16}{20} \\left[ \\frac{6}{16} \\log_2(\\frac{6}{16}) + \\frac{10}{16} \\log_2(\\frac{10}{16}) \\right]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{1}{20} \\left[ 6 \\log_2(\\frac{6}{16}) + 10 \\log_2(\\frac{10}{16}) \\right]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{1}{20} \\left[ 6 \\log_2(\\frac{3}{8}) + 10 \\log_2(\\frac{5}{8}) \\right]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{1}{20} [ 6(\\log_2(3) - 3) + 10(\\log_2(5) - 3) ]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{1}{20} [ 6\\log_2(3) - 18 + 10\\log_2(5) - 30 ]$\n            *   $IG(\\text{'zephyr'}) = 1 + \\frac{6\\log_2(3) + 10\\log_2(5) - 48}{20} = \\frac{20 + 6\\log_2(3) + 10\\log_2(5) - 48}{20} = \\frac{6\\log_2(3) + 10\\log_2(5) - 28}{20}$\n            *   使用计算器：$\\log_2(3) \\approx 1.58496$, $\\log_2(5) \\approx 2.32193$。\n            *   $IG(\\text{'zephyr'}) \\approx \\frac{6(1.58496) + 10(2.32193) - 28}{20} = \\frac{9.50976 + 23.2193 - 28}{20} = \\frac{4.72906}{20} \\approx 0.236453$。\n\n    *   **第3步：计算以“team”为分裂依据的信息增益。**\n        *   **子节点1（包含“team”）：**\n            *   文档数：$n_t = 10$。\n            *   类别分布：$6$ 个正例，$4$ 个负例。\n            *   $p_+ = \\frac{6}{10} = \\frac{3}{5}$。$p_- = \\frac{4}{10} = \\frac{2}{5}$。\n            *   熵 $H(\\text{含 'team'}) = - (\\frac{3}{5} \\log_2(\\frac{3}{5}) + \\frac{2}{5} \\log_2(\\frac{2}{5}))$。\n        *   **子节点2（不含“team”）：**\n            *   文档数：$10$。\n            *   类别分布：$4$ 个正例，$6$ 个负例。\n            *   $p_+ = \\frac{4}{10} = \\frac{2}{5}$。$p_- = \\frac{6}{10} = \\frac{3}{5}$。\n            *   熵 $H(\\text{不含 'team'}) = - (\\frac{2}{5} \\log_2(\\frac{2}{5}) + \\frac{3}{5} \\log_2(\\frac{3}{5}))$。\n            *   注意 $H(\\text{含 'team'}) = H(\\text{不含 'team'})$。\n        *   **计算子节点的加权平均熵：**\n            *   $H_{\\text{平均}}(\\text{'team'}) = \\frac{10}{20} H(\\text{含 'team'}) + \\frac{10}{20} H(\\text{不含 'team'})$\n            *   $H_{\\text{平均}}(\\text{'team'}) = \\frac{1}{2} H(\\text{含 'team'}) + \\frac{1}{2} H(\\text{含 'team'}) = H(\\text{含 'team'})$\n            *   $H_{\\text{平均}}(\\text{'team'}) = - (\\frac{3}{5} \\log_2(\\frac{3}{5}) + \\frac{2}{5} \\log_2(\\frac{2}{5}))$。\n            *   我们来计算数值。\n            *   $H_{\\text{平均}}(\\text{'team'}) = - (0.6 \\times \\log_2(0.6) + 0.4 \\times \\log_2(0.4)) \\approx - (0.6 \\times (-0.737) + 0.4 \\times (-1.322)) \\approx -(-0.4422 - 0.5288) \\approx 0.9710$ 比特。\n        *   **计算“team”的信息增益：**\n            *   $IG(\\text{'team'}) = H(\\text{父节点}) - H_{\\text{平均}}(\\text{'team'})$\n            *   $IG(\\text{'team'}) = 1 - 0.9710 = 0.0290$ 比特。\n            *   我们来更精确地计算。\n            *   $IG(\\text{'team'}) = 1 - \\left[ - \\frac{3}{5} \\log_2(\\frac{3}{5}) - \\frac{2}{5} \\log_2(\\frac{2}{5}) \\right]$\n            *   $IG(\\text{'team'}) = 1 + \\frac{3}{5} (\\log_2(3) - \\log_2(5)) + \\frac{2}{5} (\\log_2(2) - \\log_2(5))$\n            *   $IG(\\text{'team'}) = 1 + \\frac{3}{5} \\log_2(3) - \\frac{3}{5} \\log_2(5) + \\frac{2}{5} - \\frac{2}{5} \\log_2(5)$\n            *   $IG(\\text{'team'}) = \\frac{7}{5} + \\frac{3}{5} \\log_2(3) - \\log_2(5)$\n            *   使用计算器：$IG(\\text{'team'}) \\approx 1.4 + \\frac{3}{5}(1.58496) - 2.32193 = 1.4 + 0.950976 - 2.32193 = 2.350976 - 2.32193 = 0.029046$。\n\n    *   **比较与过拟合论证：**\n        *   $IG(\\text{'zephyr'}) \\approx 0.2365$ 比特。\n        *   $IG(\\text{'team'}) \\approx 0.0290$ 比特。\n        *   $IG(\\text{'zephyr'}) > IG(\\text{'team'})$。因此，在训练数据上，“zephyr”似乎是一个好得多的分裂器。\n        *   它看起来如此强大的原因在于它创建了一个完全纯的节点。所有 $4$ 个包含“zephyr”的文档都是正例。这是一个非常强的信号。\n        *   然而，这个词很罕见（$n_z=4$）。在少量实例上发现如此完美的关联是过拟合的典型标志。“如果存在‘zephyr’，则预测为正例”这条规则很可能是一种仅存在于这个特定的小训练集中的伪相关。它不太可能泛化到新的、未见过的数据上。在小样本中，罕见词可能由于偶然只出现在某一类别的文档中，这使它们看起来像是强大的预测因子。这是从数据中学习噪声的一种形式。\n\n    *   **第4步：应用最小支持度约束。**\n        *   该约束要求分裂词至少出现在 $s=5$ 个训练文档中。\n        *   检查“zephyr”：包含它的文档数为 $n_z = 4$。由于 $4  5$，这个分裂是**不允许的**。\n        *   检查“team”：包含它的文档数为 $n_t = 10$。由于 $10 \\geq 5$，这个分裂是**允许的**。\n        *   在该约束下，“team”是唯一可用的候选者。因此，它是最佳可用分裂。\n\n    *   **第5步：报告约束下的最佳分裂的信息增益。**\n        *   最佳可用分裂是“team”。\n        *   其信息增益为 $IG(\\text{'team'}) \\approx 0.029046$ 比特。\n        *   问题要求将最终答案四舍五入到 $4$ 位有效数字。\n        *   $0.029046 \\to 0.02905$。\n\n    *   **最后检查计算。**\n        *   父节点熵：$H(10, 10) = -(\\frac{10}{20}\\log_2\\frac{10}{20} + \\frac{10}{20}\\log_2\\frac{10}{20}) = 1$。正确。\n        *   ‘zephyr’ 分裂：\n            *   分支1（词存在）：$n_1=4$，计数 $(4, 0)$。$H(4,0) = 0$。\n            *   分支2（词不存在）：$n_2=16$，计数 $(6, 10)$。$H(6,10) = -(\\frac{6}{16}\\log_2\\frac{6}{16} + \\frac{10}{16}\\log_2\\frac{10}{16}) = -(\\frac{3}{8}\\log_2\\frac{3}{8} + \\frac{5}{8}\\log_2\\frac{5}{8})$。\n            *   $H_{分裂后} = \\frac{4}{20} H(4,0) + \\frac{16}{20} H(6,10) = \\frac{4}{5} H(6,10)$。\n            *   $IG(\\text{'zephyr'}) = H(10,10) - \\frac{4}{5} H(6,10) = 1 - \\frac{4}{5} [ -(\\frac{3}{8}\\log_2\\frac{3}{8} + \\frac{5}{8}\\log_2\\frac{5}{8}) ]$。\n            *   使用 $\\log_2(x) = \\ln(x)/\\ln(2)$：\n            *   $H(6,10) = -(\\frac{3}{8} \\frac{\\ln(3/8)}{\\ln(2)} + \\frac{5}{8} \\frac{\\ln(5/8)}{\\ln(2)}) \\approx -(-0.530628 - 0.423792) \\approx 0.95442$。\n            *   $IG(\\text{'zephyr'}) \\approx 1 - \\frac{4}{5} (0.95442) = 1 - 0.763536 = 0.236464$。之前的计算是正确的。\n        *   ‘team’ 分裂：\n            *   分支1（词存在）：$n_1=10$，计数 $(6, 4)$。$H(6,4) = -(\\frac{6}{10}\\log_2\\frac{6}{10} + \\frac{4}{10}\\log_2\\frac{4}{10})$。\n            *   分支2（词不存在）：$n_2=10$，计数 $(4, 6)$。$H(4,6) = -(\\frac{4}{10}\\log_2\\frac{4}{10} + \\frac{6}{10}\\log_2\\frac{6}{10}) = H(6,4)$。\n            *   $H_{分裂后} = \\frac{10}{20} H(6,4) + \\frac{10}{20} H(4,6) = H(6,4)$。\n            *   $IG(\\text{'team'}) = H(10,10) - H(6,4) = 1 - [ -(\\frac{3}{5}\\log_2\\frac{3}{5} + \\frac{2}{5}\\log_2\\frac{2}{5}) ]$。\n            *   使用 $\\log_2(x) = \\ln(x)/\\ln(2)$：\n            *   $H(6,4) = -(\\frac{3}{5}\\frac{\\ln(3/5)}{\\ln(2)} + \\frac{2}{5}\\frac{\\ln(2/5)}{\\ln(2)}) \\approx -(-0.442179 - 0.528771) \\approx 0.97095$。\n            *   $IG(\\text{'team'}) \\approx 1 - 0.97095 = 0.02905$。之前的计算是正确的。\n\n    *   **整理最终解。**\n        *   从验证问题陈述开始。说明其有效。\n        *   定义熵 $H$ 和信息增益 $IG$。\n        *   计算父节点熵 $H(\\text{父节点})$。\n        *   计算 $IG(\\text{zephyr})$。\n        *   计算 $IG(\\text{team})$。\n        *   比较两个 IG 值并提出过拟合的论点。\n        *   应用最小支持度约束 $s=5$。\n        *   将“zephyr”识别为无效分裂，而“team”为最佳可用分裂。\n        *   陈述最终答案，即 $IG(\\text{team})$ 四舍五入到4位有效数字。\n        *   $0.02905$。这有4位有效数字。我之前从 $0.029046$ 四舍五入到 $0.02905$ 是正确的。\n        *   所有计算都已确认。推理合理。我准备好编写解题过程了。我将严格遵循结构，包括对所有数学实体强制使用 LaTeX。\n        *   $IG(\\text{'team'})$ 的计算：\n            $H_{分裂后} = H(6/10, 4/10) = - \\frac{6}{10}\\log_2\\frac{6}{10} - \\frac{4}{10}\\log_2\\frac{4}{10}$\n            $= -0.6 \\log_2(0.6) - 0.4 \\log_2(0.4)$\n            $= -0.6 \\frac{\\ln(0.6)}{\\ln(2)} - 0.4 \\frac{\\ln(0.4)}{\\ln(2)}$\n            $= \\frac{-0.6(-0.5108256) - 0.4(-0.9162907)}{0.693147}$\n            $= \\frac{0.30649536 + 0.36651628}{0.693147} = \\frac{0.67301164}{0.693147} \\approx 0.9709506$\n            $IG = 1 - 0.9709506 = 0.0290494$\n            四舍五入到4位有效数字：$0.02905$。\n\n        *   $IG(\\text{'zephyr'})$ 的计算：\n            $H_{分裂后} = \\frac{16}{20} H(6/16, 10/16) + \\frac{4}{20}(0) = 0.8 \\times H(3/8, 5/8)$\n            $H(3/8, 5/8) = - \\frac{3}{8}\\log_2\\frac{3}{8} - \\frac{5}{8}\\log_2\\frac{5}{8}$\n            $= -0.375 \\log_2(0.375) - 0.625 \\log_2(0.625)$\n            $= -0.375 \\frac{\\ln(0.375)}{\\ln(2)} - 0.625 \\frac{\\ln(0.625)}{\\ln(2)}$\n            $= \\frac{-0.375(-0.980829) - 0.625(-0.470004)}{0.693147}$\n            $= \\frac{0.36781 + 0.29375}{0.693147} = \\frac{0.66156}{0.693147} \\approx 0.954434$\n            $H_{分裂后} = 0.8 \\times 0.954434 = 0.7635472$\n            $IG = 1 - 0.7635472 = 0.2364528$\n            \n        所有计算都已确认。推理合理。我准备好编写解题过程了。我将严格遵循结构，包括对所有数学实体强制使用 LaTeX。",
            "answer": "$$\n\\boxed{0.02905}\n$$"
        },
        {
            "introduction": "在掌握了不纯度度量的基本计算后，下一步是将其应用于构建决策树的实际算法中。决策树的生长过程就是在每个节点寻找最优的分裂特征和分裂点。这项编程练习  让你亲手实现这一核心过程，并直接比较两种最常用的不纯度度量——基尼不纯度和熵——在选择分裂点时的行为差异，特别是在处理含有许多稀有类别的数据时，这能加深你对它们各自特点的理解。",
            "id": "3112936",
            "problem": "本题要求您为决策树分类器中的一个分类特征实现并比较两种二元分裂方法，使用的分裂标准为：香农熵（信息增益）和基尼不纯度（基尼减少量）。此任务的重点在于，在存在许多稀有类别的情况下，这些标准的表现行为，以及在捕获信息和对小样本计数过拟合之间的权衡。您的程序必须为每个提供的测试用例，在每种标准下确定类别的最优二元划分，计算训练集的分裂质量指标，并在已知真实类别概率的情况下，评估期望泛化准确率。\n\n定义与基本原理：\n- 设有一个包含 $K$ 个类别的单一分类特征。对于类别 $k \\in \\{1,\\dots,K\\}$，给定其训练样本数 $n_k$、观察到的正类（$y=1$）计数 $c_k$ 以及 $y=1$ 的真实潜在概率 $p_k$。\n- 类别 $k$ 的经验正类率是 $\\hat{p}_k = c_k / n_k$。\n- 父节点总共包含 $N = \\sum_{k=1}^{K} n_k$ 个样本，其经验父节点率为 $\\hat{p}_{\\mathrm{parent}} = \\left(\\sum_{k=1}^{K} c_k\\right)/N$。\n- 对于概率为 $p$ 的二元分布，其香农熵为 $H(p) = -p\\log(p) - (1-p)\\log(1-p)$，基尼不纯度为 $G(p) = 2p(1-p)$。熵的计算使用自然对数；请注意，对数的底数只会对 $H(p)$ 进行常数倍缩放，这不会改变最优分裂点。\n- 对于将 $K$ 个类别二元分裂为左子集 $\\mathcal{L}$ 和右子集 $\\mathcal{R}$ 的任意分裂，其样本数分别为 $N_{\\mathcal{L}}$ 和 $N_{\\mathcal{R}}$，经验率分别为 $\\hat{p}_{\\mathcal{L}}$ 和 $\\hat{p}_{\\mathcal{R}}$，则条件熵为\n$$\nH_{\\mathrm{cond}} = \\frac{N_{\\mathcal{L}}}{N} H(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} H(\\hat{p}_{\\mathcal{R}}),\n$$\n信息增益为\n$$\n\\mathrm{IG} = H(\\hat{p}_{\\mathrm{parent}}) - H_{\\mathrm{cond}}.\n$$\n类似地，条件基尼不纯度为\n$$\nG_{\\mathrm{cond}} = \\frac{N_{\\mathcal{L}}}{N} G(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} G(\\hat{p}_{\\mathcal{R}}),\n$$\n基尼减少量为\n$$\n\\mathrm{GD} = G(\\hat{p}_{\\mathrm{parent}}) - G_{\\mathrm{cond}}.\n$$\n\n二元分裂搜索协议：\n- 按经验率 $\\hat{p}_k$ 的升序对类别进行排序，得到一个类别序列。将搜索范围限定于索引 $t \\in \\{1,\\dots,K-1\\}$ 处的前缀分裂：左子节点包含排序后序列中的前 $t$ 个类别，右子节点包含剩余的 $K-t$ 个类别。当不纯度度量是一个可分离的 Schur 凹函数时，该协议对于二元分类是允许的，并且它避免了对集合划分进行指数级搜索，同时对于包括香农熵和基尼不纯度在内的广义不纯度度量，仍能保留最优分裂。\n- 对于每个 $t$，计算信息增益 $\\mathrm{IG}(t)$ 和基尼减少量 $\\mathrm{GD}(t)$。设 $t^{\\star}_{H}$ 是使 $\\mathrm{IG}(t)$ 最大化的索引，$t^{\\star}_{G}$ 是使 $\\mathrm{GD}(t)$ 最大化的索引。如果出现平局，则选择最小的 $t$。\n- 使用训练数据定义单次分裂分类器：对于每个子节点，根据 $\\hat{p}_{\\mathcal{L}}$ 和 $\\hat{p}_{\\mathcal{R}}$ 预测多数类（当率至少为 $0.5$ 时预测为 $1$，否则预测为 $0$；出现平局时预测为 $1$）。在已知的真实概率 $p_k$ 下，期望泛化准确率计算为\n$$\n\\mathrm{Acc} = \\frac{1}{N}\\left(\\sum_{k \\in \\mathcal{L}} n_k \\cdot \\bigl(\\mathbf{1}[\\hat{p}_{\\mathcal{L}} \\ge 0.5] \\cdot p_k + \\mathbf{1}[\\hat{p}_{\\mathcal{L}}  0.5] \\cdot (1-p_k)\\bigr) + \\sum_{k \\in \\mathcal{R}} n_k \\cdot \\bigl(\\mathbf{1}[\\hat{p}_{\\mathcal{R}} \\ge 0.5] \\cdot p_k + \\mathbf{1}[\\hat{p}_{\\mathcal{R}}  0.5] \\cdot (1-p_k)\\bigr)\\right),\n$$\n其中 $\\mathbf{1}[\\cdot]$ 表示指示函数。该公式评估了每个类别在其真实概率下的期望正确性，并按 $n_k$ 进行加权。\n\n您的程序必须实现上述协议，并为每个测试用例生成以下输出：\n- 由香农熵选择的索引 $t^{\\star}_{H}$。\n- 由基尼不纯度选择的索引 $t^{\\star}_{G}$。\n- 最大信息增益 $\\mathrm{IG}(t^{\\star}_{H})$，以浮点数形式表示。\n- 最大基尼减少量 $\\mathrm{GD}(t^{\\star}_{G})$，以浮点数形式表示。\n- 相应单次分裂分类器的期望泛化准确率 $\\mathrm{Acc}_{H}$ 和 $\\mathrm{Acc}_{G}$，以 $[0,1]$ 范围内的浮点数表示。\n\n测试套件：\n对于每个测试用例，数组按排序前的类别顺序列出。您的程序必须在评估分裂前，按经验率 $\\hat{p}_k$ 进行排序。\n\n- 测试用例 1（存在许多稀有类别，训练数据表明纯度很高，但真实概率适中）：\n    - $n = [200,200,200,200,5,5,5,5,5,5,5,5]$\n    - $c = [130,70,120,80,5,5,5,5,5,5,5,5]$\n    - $p_{\\mathrm{true}} = [0.6,0.4,0.6,0.4,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5]$\n\n- 测试用例 2（稀有类别在小样本量下确实具有信息量；比较不同标准的敏感度）：\n    - $n = [100,100,100,100,10,10,10,10,10,10,10,10,10,10]$\n    - $c = [55,45,60,40,9,9,9,9,9,9,9,9,9,9]$\n    - $p_{\\mathrm{true}} = [0.55,0.45,0.60,0.40,0.95,0.95,0.95,0.95,0.95,0.95,0.95,0.95,0.95,0.95]$\n\n- 测试用例 3（边界情况，所有类别的经验率和真实率均为 0.5）：\n    - $n = [50,50,50,50,50,50]$\n    - $c = [25,25,25,25,25,25]$\n    - $p_{\\mathrm{true}} = [0.5,0.5,0.5,0.5,0.5,0.5]$\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个由方括号括起来的逗号分隔列表，列表中的每个元素对应一个测试用例，其本身也是一个列表，格式为\n$[t^{\\star}_{H}, t^{\\star}_{G}, \\mathrm{IG}(t^{\\star}_{H}), \\mathrm{GD}(t^{\\star}_{G}), \\mathrm{Acc}_{H}, \\mathrm{Acc}_{G}]$。\n例如，打印的输出应类似于\n[[tH1,tG1,IG1,GD1,AccH1,AccG1],[tH2,tG2,IG2,GD2,AccH2,AccG2],[tH3,tG3,IG3,GD3,AccH3,AccG3]]。\n所有输出必须是小数或整数；不要使用百分比。",
            "solution": "该问题要求针对单个分类特征实现决策树的分裂算法，并比较两种常见的分裂标准：信息增益（源于香农熵）和基尼减少量（源于基尼不纯度）。这个过程包括找到能使这些标准最大化的特征类别的最优二元划分，然后评估由此产生的单次分裂分类器的泛化性能。\n\n解决方案的结构如下：\n首先，对于每个测试用例，我们必须准备数据。类别由其样本数（$n_k$）、正类计数（$c_k$）和真实正类概率（$p_k$）定义。用于分裂的主要变量是经验正类率 $\\hat{p}_k = c_k / n_k$。指定分裂协议的核心是按经验率 $\\hat{p}_k$ 的升序对类别进行排序。这将最优二元划分的搜索空间从指数级的集合划分数量减少到线性的（$K-1$）个前缀分裂数量，其中 $K$ 是类别数量。并保证在二元响应的背景下，对于像香农熵和基尼不纯度这样的不纯度函数，能够保留最优分裂。\n\n其次，我们计算在任何分裂之前父节点（包含所有数据）的不纯度。设 $N = \\sum_k n_k$ 为总样本量，$C = \\sum_k c_k$ 为总正类计数。父节点的经验率为 $\\hat{p}_{\\mathrm{parent}} = C/N$。父节点的不纯度使用提供的香农熵 $H(p) = -p\\log(p) - (1-p)\\log(1-p)$ 和基尼不纯度 $G(p) = 2p(1-p)$ 公式计算。当 $p=0$ 或 $p=1$ 时，熵为 0。\n\n第三，我们遍历基于排序后类别的所有可能的前缀分裂。在索引 $t \\in \\{1, \\dots, K-1\\}$ 处的分裂将类别划分为一个包含前 $t$ 个类别的左子节点 $\\mathcal{L}$ 和一个包含剩余 $K-t$ 个类别的右子节点 $\\mathcal{R}$。对于每个潜在的分裂，我们计算子节点的属性：\n- 左子节点 $\\mathcal{L}$：总计数 $N_{\\mathcal{L}} = \\sum_{k=1}^{t} n_{s,k}$ 和正类计数 $C_{\\mathcal{L}} = \\sum_{k=1}^{t} c_{s,k}$，其中下标 $s$ 表示排序后的顺序。经验率为 $\\hat{p}_{\\mathcal{L}} = C_{\\mathcal{L}} / N_{\\mathcal{L}}$。\n- 右子节点 $\\mathcal{R}$：总计数 $N_{\\mathcal{R}} = N - N_{\\mathcal{L}}$ 和正类计数 $C_{\\mathcal{R}} = C - C_{\\mathcal{L}}$。经验率为 $\\hat{p}_{\\mathcal{R}} = C_{\\mathcal{R}} / N_{\\mathcal{R}}$。\n\n使用这些值，我们计算分裂的条件不纯度，它是子节点不纯度的加权平均值：\n$$\nH_{\\mathrm{cond}}(t) = \\frac{N_{\\mathcal{L}}}{N} H(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} H(\\hat{p}_{\\mathcal{R}})\n$$\n$$\nG_{\\mathrm{cond}}(t) = \\frac{N_{\\mathcal{L}}}{N} G(\\hat{p}_{\\mathcal{L}}) + \\frac{N_{\\mathcal{R}}}{N} G(\\hat{p}_{\\mathcal{R}})\n$$\n分裂的质量通过从父节点到子节点的不纯度减少量来量化：\n- 信息增益：$\\mathrm{IG}(t) = H(\\hat{p}_{\\mathrm{parent}}) - H_{\\mathrm{cond}}(t)$\n- 基尼减少量：$\\mathrm{GD}(t) = G(\\hat{p}_{\\mathrm{parent}}) - G_{\\mathrm{cond}}(t)$\n\n我们找到使 $\\mathrm{IG}(t)$ 最大化的分裂索引 $t^{\\star}_{H}$ 和使 $\\mathrm{GD}(t)$ 最大化的 $t^{\\star}_{G}$。对于最大值出现平局的情况，题目指定选择最小的索引 $t$。这通过从 $t=1$ 到 $K-1$ 迭代，并且仅在发现严格更大的不纯度减少量时才更新最优分裂来处理。\n\n第四，一旦通过两种标准确定了最优分裂，我们评估相应的单次分裂分类器的期望泛化准确率。每个子节点（$\\mathcal{L}$ 或 $\\mathcal{R}$）的预测由其经验率决定：如果率大于等于 0.5，则预测为类别 1，否则预测为类别 0。然后使用已知的真实概率 $p_k$ 计算准确率。准确率的公式是所有类别上正确预测概率的加权平均值：\n$$\n\\mathrm{Acc} = \\frac{1}{N}\\left(\\sum_{k \\in \\mathcal{L}} n_k \\cdot P(\\text{correct}|k) + \\sum_{k \\in \\mathcal{R}} n_k \\cdot P(\\text{correct}|k)\\right)\n$$\n其中，如果包含类别 $k$ 的节点的预测为 1，则 $P(\\text{correct}|k)$ 为 $p_k$；如果预测为 0，则为 $1-p_k$。此过程分别针对信息增益找到的分裂（$\\mathrm{Acc}_H$）和基尼减少量找到的分裂（$\\mathrm{Acc}_G$）执行。\n\n整个过程被封装在一个函数中，该函数处理每个测试用例并返回六个指定的结果：$t^{\\star}_{H}$、$t^{\\star}_{G}$、$\\mathrm{IG}(t^{\\star}_{H})$、$\\mathrm{GD}(t^{\\star}_{G})$、$\\mathrm{Acc}_{H}$ 和 $\\mathrm{Acc}_{G}$。实现中使用了 `numpy` 以进行高效的数组操作，特别是通过计算累积和来加速对分裂的迭代过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def entropy(p):\n        \"\"\"Calculates Shannon entropy for a binary distribution.\"\"\"\n        if p == 0 or p == 1:\n            return 0.0\n        return -p * np.log(p) - (1 - p) * np.log(1 - p)\n\n    def gini(p):\n        \"\"\"Calculates Gini impurity for a binary distribution.\"\"\"\n        return 2 * p * (1 - p)\n\n    def solve_case(n_orig, c_orig, p_true_orig):\n        \"\"\"\n        Solves the problem for a single test case.\n        \"\"\"\n        n_orig = np.array(n_orig, dtype=float)\n        c_orig = np.array(c_orig, dtype=float)\n        p_true_orig = np.array(p_true_orig, dtype=float)\n        \n        # 1. Calculate p_hat and combine data\n        p_hat_orig = np.divide(c_orig, n_orig, out=np.zeros_like(c_orig, dtype=float), where=n_orig != 0)\n        \n        K = len(n_orig)\n\n        # Combine into a list of tuples to sort: (p_hat, n, c, p_true)\n        # Using a secondary sort key (original index) for stability, though not required by a problem\n        # but good practice.\n        indexed_data = list(zip(p_hat_orig, n_orig, c_orig, p_true_orig, range(K)))\n        sorted_indexed_data = sorted(indexed_data)\n        \n        if not sorted_indexed_data:\n            # Handle empty input case, though not in test suite\n            return [1, 1, 0.0, 0.0, 0.5, 0.5]\n\n        # Unzip into sorted arrays\n        p_hat_s, n_s, c_s, p_true_s, _ = map(np.array, zip(*sorted_indexed_data))\n\n        # 2. Parent node stats\n        N_total = n_s.sum()\n        C_total = c_s.sum()\n        \n        if N_total == 0:\n            return [1, 1, 0.0, 0.0, 0.5, 0.5]\n\n        p_hat_parent = C_total / N_total\n        H_parent = entropy(p_hat_parent)\n        G_parent = gini(p_hat_parent)\n\n        # Handle case with only one category where no split is possible\n        if K == 1:\n            pred = 1 if p_hat_parent = 0.5 else 0\n            correct_mass = 0\n            if pred == 1:\n                correct_mass = (n_s * p_true_s).sum()\n            else:\n                correct_mass = (n_s * (1 - p_true_s)).sum()\n            acc = correct_mass / N_total if N_total  0 else 0.5\n            return [1, 1, 0.0, 0.0, acc, acc]\n            \n        # 3. Iterate through splits\n        n_cumsum = np.cumsum(n_s)\n        c_cumsum = np.cumsum(c_s)\n\n        max_ig = -np.inf\n        t_star_h = 1\n        max_gd = -np.inf\n        t_star_g = 1\n\n        for i in range(K - 1):\n            t = i + 1\n\n            N_L = n_cumsum[i]\n            C_L = c_cumsum[i]\n            p_hat_L = C_L / N_L if N_L  0 else 0.0\n\n            N_R = N_total - N_L\n            C_R = C_total - C_L\n            p_hat_R = C_R / N_R if N_R  0 else 0.0\n            \n            H_cond = (N_L / N_total) * entropy(p_hat_L) + (N_R / N_total) * entropy(p_hat_R)\n            G_cond = (N_L / N_total) * gini(p_hat_L) + (N_R / N_total) * gini(p_hat_R)\n            \n            ig = H_parent - H_cond\n            gd = G_parent - G_cond\n\n            # First value found becomes the max\n            if i == 0:\n                max_ig = ig\n                max_gd = gd\n            \n            if ig  max_ig:\n                max_ig = ig\n                t_star_h = t\n            \n            if gd  max_gd:\n                max_gd = gd\n                t_star_g = t\n\n        # 4. Calculate accuracies\n        def calculate_accuracy(t_star):\n            # Left child\n            N_L = n_cumsum[t_star - 1]\n            C_L = c_cumsum[t_star - 1]\n            p_hat_L = C_L / N_L if N_L  0 else 0.0\n            pred_L = 1 if p_hat_L = 0.5 else 0\n\n            # Right child\n            N_R = N_total - N_L\n            C_R = C_total - C_L\n            p_hat_R = C_R / N_R if N_R  0 else 0.0\n            pred_R = 1 if p_hat_R = 0.5 else 0\n\n            total_correct_mass = 0.0\n            \n            # Left node contribution\n            n_L_cats = n_s[:t_star]\n            p_true_L_cats = p_true_s[:t_star]\n            if pred_L == 1:\n                total_correct_mass += np.sum(n_L_cats * p_true_L_cats)\n            else:\n                total_correct_mass += np.sum(n_L_cats * (1 - p_true_L_cats))\n            \n            # Right node contribution\n            n_R_cats = n_s[t_star:]\n            p_true_R_cats = p_true_s[t_star:]\n            if pred_R == 1:\n                total_correct_mass += np.sum(n_R_cats * p_true_R_cats)\n            else:\n                total_correct_mass += np.sum(n_R_cats * (1 - p_true_R_cats))\n            \n            return total_correct_mass / N_total\n\n        Acc_H = calculate_accuracy(t_star_h)\n        Acc_G = calculate_accuracy(t_star_g)\n\n        return [t_star_h, t_star_g, max_ig, max_gd, Acc_H, Acc_G]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": [200, 200, 200, 200, 5, 5, 5, 5, 5, 5, 5, 5],\n            \"c\": [130, 70, 120, 80, 5, 5, 5, 5, 5, 5, 5, 5],\n            \"p_true\": [0.6, 0.4, 0.6, 0.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n        },\n        {\n            \"n\": [100, 100, 100, 100, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n            \"c\": [55, 45, 60, 40, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n            \"p_true\": [0.55, 0.45, 0.60, 0.40, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95]\n        },\n        {\n            \"n\": [50, 50, 50, 50, 50, 50],\n            \"c\": [25, 25, 25, 25, 25, 25],\n            \"p_true\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case[\"n\"], case[\"c\"], case[\"p_true\"])\n        results.append(f\"[{','.join(f'{x:.7f}' for x in result)}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "决策树算法通过在每一步选择能带来最大信息增益的分裂，来“贪婪地”构建树。虽然这种策略在大多数情况下行之有效，但它是否总能导向全局最优的树结构呢？这个练习  通过一个精巧构建的数据集（一个经典的异或问题），揭示了贪婪选择的局限性。你将发现，一个初始信息增益较低的分裂，反而可能为后续的完美分类创造条件，这深刻揭示了特征交互的重要性以及贪婪算法的短视特性。",
            "id": "3131390",
            "problem": "我们构建一个二元分类任务，旨在通过展示一种只有在经过一次适度的早期分裂后才能被利用的相互作用，来凸显决策树中贪心特征选择的局限性。考虑一个包含$16$个样本的数据集，其具有二元特征 $A,B,C \\in \\{0,1\\}$ 和二元标签 $Y \\in \\{0,1\\}$。数据定义如下。\n\n1. 在整个数据集中，联合值 $(A,C)$ 是均匀分布的：$(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$ 中的每一种都恰好出现 $4$ 次。标签由异或（XOR）运算确定性地给出，$Y = A \\oplus C$。\n\n2. 特征 $B$ 与 $Y$ 相关，使得恰好有 $12$ 个样本满足 $B=Y$，恰好有 $4$ 个样本满足 $B \\neq Y$。此外，这种相关性在 $(A,C)$ 上对称分布，如下所示：\n   - 在子集 $B=1$（有 $8$ 个样本）中，有 $6$ 个样本的 $Y=1$，有 $2$ 个样本的 $Y=0$，其 $(A,C)$ 组成为：$(0,1)$ 出现 $3$ 次，$(1,0)$ 出现 $3$ 次，$(0,0)$ 出现 $1$ 次，$(1,1)$ 出现 $1$ 次。\n   - 在子集 $B=0$（有 $8$ 个样本）中，有 $6$ 个样本的 $Y=0$，有 $2$ 个样本的 $Y=1$，其 $(A,C)$ 组成为：$(0,0)$ 出现 $3$ 次，$(1,1)$ 出现 $3$ 次，$(0,1)$ 出现 $1$ 次，$(1,0)$ 出现 $1$ 次。\n\n使用以2为底的对数的标准熵定义，并将信息增益（IG）定义为熵的减少量。考虑深度限制为 $2$ 的决策树（一个根节点分裂，每个分支再进行一次分裂）。令 $G_A$ 表示根节点分裂被限制在特征 $A$ 上的任何深度为 $2$ 的决策树可实现的最大总信息增益，令 $G_B$ 表示根节点分裂被限制在特征 $B$ 上的任何深度为 $2$ 的决策树可实现的最大总信息增益。\n\n计算量 $\\Delta = G_A - G_B$（单位为比特）。将你的答案四舍五入到四位有效数字。",
            "solution": "我们从核心定义开始。对于一个经验类别概率为 $p$ 和 $1-p$ 的二元标签 $Y$，其熵为\n$$\nH(Y) \\;=\\; -\\,p\\,\\log_{2}(p)\\;-\\;(1-p)\\,\\log_{2}(1-p).\n$$\n对于一个特征 $X$，在其上进行分裂的信息增益是熵的减少量，\n$$\n\\mathrm{IG}(Y;X) \\;=\\; H(Y) \\;-\\; H(Y\\mid X),\n$$\n其中\n$$\nH(Y\\mid X)\\;=\\;\\sum_{x} P(X=x)\\,H(Y\\mid X=x).\n$$\n一个根节点为 $X$ 且每个子节点再进行一次分裂的深度为 $2$ 的决策树，其总信息增益等于根熵 $H(Y)$ 减去第二次分裂后的最终加权条件熵。\n\n步骤1：计算根熵 $H(Y)$。根据构造，$(A,C)$ 是均匀分布的，且 $Y = A \\oplus C$，因此恰好一半的样本具有 $Y=1$，一半的样本具有 $Y=0$。因此 $P(Y=1)=P(Y=0)=\\frac{1}{2}$，且\n$$\nH(Y) \\;=\\; -\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\;-\\;\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\;=\\;1.\n$$\n\n步骤2：评估 $G_A$，即根节点分裂在特征 $A$ 上的最佳深度为 $2$ 的总信息增益。\n\n- 首先，计算 $H(Y\\mid A)$。因为 $(A,C)$ 是均匀分布的且 $Y=A\\oplus C$，所以在 $A=0$ 的条件下得到 $Y=C$，此时 $P(Y=1\\mid A=0)=\\frac{1}{2}$；在 $A=1$ 的条件下得到 $Y=1-C$，此时 $P(Y=1\\mid A=1)=\\frac{1}{2}$。因此 $H(Y\\mid A=0)=1$ 和 $H(Y\\mid A=1)=1$，所以 $H(Y\\mid A)=1$，在特征 $A$ 上的根信息增益为 $0$。\n- 然而，在对 $A$ 进行分裂后，在每个分支上对 $C$ 进行额外分裂会产生完美的纯度，因为 $Y$ 是 $(A,C)$ 的确定性函数。具体来说，当 $A=0$ 时，$Y=C$；当 $A=1$ 时，$Y=1-C$。因此，在两个分支中 $H(Y\\mid A,C)=0$。所以，第二次分裂后的最终加权条件熵为 $0$。\n- 因此，这个深度为 $2$ 的决策树的总信息增益为\n$$\nG_A \\;=\\; H(Y) - 0 \\;=\\; 1.\n$$\n\n步骤3：评估 $G_B$，即根节点分裂在特征 $B$ 上的最佳深度为 $2$ 的总信息增益。\n\n根据构造，分支 $B=1$ 包含 $8$ 个样本，其中 $6$ 个属于类别 $Y=1$，$2$ 个属于类别 $Y=0$。因此\n$$\nH(Y\\mid B=1) \\;=\\; -\\tfrac{6}{8}\\log_{2}\\tfrac{6}{8}\\;-\\;\\tfrac{2}{8}\\log_{2}\\tfrac{2}{8}\n\\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}.\n$$\n类似地，分支 $B=0$ 包含 $8$ 个样本，其中 $6$ 个属于类别 $Y=0$，$2$ 个属于类别 $Y=1$，这产生了相同的熵，\n$$\nH(Y\\mid B=0) \\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}.\n$$\n因此\n$$\nH(Y\\mid B) \\;=\\; \\tfrac{1}{2}\\,H(Y\\mid B=1) + \\tfrac{1}{2}\\,H(Y\\mid B=0)\n\\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}.\n$$\n在特征 $B$ 上的根信息增益为\n$$\n\\mathrm{IG}(Y;B) \\;=\\; 1 \\;-\\; \\Big(-\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\Big).\n$$\n\n接下来，评估在每个 $B$ 分支内可能的最佳额外分裂。根据指定的 $(A,C)$ 组成，在 $B=1$ 内，按 $(A,C)$ 计数的样本为 $(0,1)$：$3$ 个，$(1,0)$：$3$ 个（所有这些样本的 $Y=1$），以及 $(0,0)$：$1$ 个，$(1,1)$：$1$ 个（这两个样本的 $Y=0$）。考虑在 $B=1$ 分支上对 $A$ 进行分裂：\n- 对于 $B=1, A=0$ 的情况，有 $3$ 个样本 $Y=1$ 和 $1$ 个样本 $Y=0$，所以 $H(Y\\mid B=1,A=0)= -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}-\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}$。\n- 对于 $B=1, A=1$ 的情况，有 $3$ 个样本 $Y=1$ 和 $1$ 个样本 $Y=0$，产生相同的熵。\n\n因此\n$$\nH(Y\\mid B=1, A) \\;=\\; -\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\n\\;=\\; H(Y\\mid B=1).\n$$\n所以在 $B=1$ 内对 $A$ 进行分裂不会带来进一步的熵减少。根据计数的对称性，在 $B=1$ 内对 $C$ 进行分裂也不会带来熵减少。在 $B=0$ 分支中也成立类似的计算，因为它的 $(A,C)$ 组成是互补的，并且按 $A$ 和按 $C$ 分别具有相同的 $3:1$ 比例。因此，在任一 $B$ 分支内对 $A$ 或 $C$ 进行任何单次额外分裂都不能减少熵：\n$$\nH(Y\\mid B, \\text{one more split on } A \\text{ or } C) \\;=\\; H(Y\\mid B).\n$$\n因此，在根节点为 $B$ 的深度为 $2$ 的约束下，可实现的最大总信息增益就是根节点的增益：\n$$\nG_B \\;=\\; 1 \\;-\\; \\Big(-\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}\\;-\\;\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\Big)\n\\;=\\; 1 - h\\!\\left(\\tfrac{1}{4}\\right),\n$$\n其中 $h(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$ 是二元熵函数。\n\n步骤4：计算所要求的差值\n$$\n\\Delta \\;=\\; G_A - G_B \\;=\\; 1 - \\Big(1 - h\\!\\left(\\tfrac{1}{4}\\right)\\Big) \\;=\\; h\\!\\left(\\tfrac{1}{4}\\right)\n\\;=\\; -\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\;-\\;\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}.\n$$\n数值上，\n$$\n\\log_{2}\\tfrac{1}{4}=-2,\\quad \\log_{2}\\tfrac{3}{4}=\\log_{2}3 - 2 \\approx 1.5849625 - 2 = -0.4150375,\n$$\n所以\n$$\n\\Delta \\;=\\; -\\tfrac{1}{4}(-2)\\;-\\;\\tfrac{3}{4}(-0.4150375)\n\\;=\\; 0.5 + 0.3112781 \\;=\\; 0.8112781\\ldots\n$$\n四舍五入到四位有效数字得到 $0.8113$。",
            "answer": "$$\\boxed{0.8113}$$"
        }
    ]
}