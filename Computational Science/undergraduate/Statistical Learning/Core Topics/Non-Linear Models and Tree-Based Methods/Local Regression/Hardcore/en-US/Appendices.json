{
    "hands_on_practices": [
        {
            "introduction": "Local regression's power extends beyond fitting a single curve to a scatterplot. A crucial application involves exploring how the relationship between two variables changes across different groups defined by a categorical predictor. This practice  guides you through modeling such an interaction by fitting separate LOESS curves within distinct data subsets, investigating how the optimal smoothing parameter, or span, can vary between groups and highlighting the importance of adaptive modeling.",
            "id": "3141278",
            "problem": "You will implement and analyze Local Regression (Locally Estimated Scatterplot Smoothing (LOESS)) to study categorical interactions by fitting separate smoothers within groups and comparing a pooled span parameter versus group-specific spans under varying sample sizes. Your program must be a complete, runnable implementation that generates synthetic data, fits LOESS models, evaluates out-of-sample error against known ground-truth functions, and reports comparative results in the specified format.\n\nFundamental base to use:\n- Weighted least squares and local linear regression: For a target location $x_0$, a local linear fit is defined by choosing coefficients $\\beta_0$ and $\\beta_1$ that minimize a weighted sum of squares $\\sum_{i=1}^{n} w_i(x_0)\\,\\big(y_i - \\beta_0 - \\beta_1 (x_i - x_0)\\big)^2$, where $w_i(x_0) \\ge 0$ are proximity-based weights.\n- Neighborhood selection via span: For a span proportion $s \\in (0,1]$ and sample size $n$, form the neighborhood using the $k = \\lceil s\\,n \\rceil$ nearest data points to $x_0$ in the predictor $x$.\n- Kernel weighting: Use a compactly supported kernel that downweights distant points within the neighborhood, with weights decreasing smoothly to $0$ at the boundary.\n\nTask overview:\n1. Data generation per group. There are two groups $g \\in \\{A,B\\}$. For each group $g$, generate training data $\\{(x_i^{(g)}, y_i^{(g)})\\}_{i=1}^{n_g}$ with $x_i^{(g)} \\sim \\mathrm{Uniform}[0,1]$ and\n   - Group $A$: $f_A(x) = \\sin(2\\pi x) + 0.5\\,x$, noise standard deviation $\\sigma_A = 0.20$.\n   - Group $B$: $f_B(x) = \\cos(\\pi x) - x$, noise standard deviation $\\sigma_B = 0.05$.\n   - Observations: $y_i^{(g)} = f_g(x_i^{(g)}) + \\varepsilon_i^{(g)}$ with $\\varepsilon_i^{(g)} \\sim \\mathcal{N}(0,\\sigma_g^2)$, independent across $i$ and $g$.\n2. Modeling. Fit LOESS separately within each group using a local linear fit as described in the fundamental base. Implement:\n   - Neighborhood size $k = \\lceil s\\,n_g \\rceil$ for span $s$ and group size $n_g$, with $k \\ge 2$ enforced.\n   - Tricube kernel weights within the neighborhood: for distances $d_i = |x_i^{(g)} - x_0|$ and $d_{\\max} = \\max$ distance among the $k$ nearest neighbors of $x_0$, define weights that smoothly decay to $0$ at $d_{\\max}$. Use local linear weighted least squares to obtain the fitted value at $x_0$.\n3. Evaluation. Construct an evaluation grid $x_{\\mathrm{eval}}$ of $M = 200$ equally spaced points on $[0,1]$. For each group $g$, compute the mean squared error against the noiseless truth:\n   $$\\mathrm{MSE}_g = \\frac{1}{M}\\sum_{j=1}^{M} \\big(\\widehat{f}_g(x_{\\mathrm{eval},j}) - f_g(x_{\\mathrm{eval},j})\\big)^2.$$\n   Report the combined error as the simple average\n   $$\\mathrm{MSE}_{\\mathrm{combined}} = \\frac{\\mathrm{MSE}_A + \\mathrm{MSE}_B}{2}.$$\n4. Comparison. For each test case, compute two combined errors:\n   - Pooled-span setting: use the same span $s_{\\mathrm{global}}$ for both groups (fitting still done separately within each groupâ€™s data).\n   - Group-specific spans: use $s_A$ for group $A$ and $s_B$ for group $B$.\n   Define the performance difference\n   $$\\Delta = \\mathrm{MSE}_{\\mathrm{combined}}^{\\text{group-specific}} - \\mathrm{MSE}_{\\mathrm{combined}}^{\\text{pooled-span}}.$$\n\nImplementation and numerical details:\n- Use local linear fits only (polynomial degree $1$).\n- Use the tricube kernel within the $k$-nearest-neighbor window determined by the span. If the weighted least squares system is ill-conditioned at some $x_0$, you may fall back to a locally constant weighted average.\n- Use independent pseudorandom number generator seeds per test case as provided below to ensure reproducibility.\n- No units or angles are involved. All numeric answers must be reported as real numbers.\n\nTest suite:\nEach test case is a tuple ($n_A, n_B, s_{\\text{global}}, s_A, s_B$, seed):\n- Case $1$: $(80, 80, 0.4, 0.3, 0.5, 20201)$\n- Case $2$: $(20, 120, 0.4, 0.25, 0.5, 20202)$\n- Case $3$: $(8, 8, 0.6, 0.8, 0.4, 20203)$\n- Case $4$: $(200, 15, 0.3, 0.2, 0.6, 20204)$\n\nRequired final output format:\n- Your program should produce a single line of output containing the values of $\\Delta$ for the four test cases, rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets; for example, $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$.",
            "solution": "We begin from the principle of local modeling via weighted least squares. For a given target location $x_0$ and a training sample $\\{(x_i,y_i)\\}_{i=1}^{n}$, a local linear approximation near $x_0$ is obtained by minimizing the weighted sum of squares\n$$\nS(\\beta_0,\\beta_1;x_0) = \\sum_{i=1}^{n} w_i(x_0)\\,\\big(y_i - \\beta_0 - \\beta_1 (x_i - x_0)\\big)^2,\n$$\nwhere $w_i(x_0)\\ge 0$ are proximity-based weights. The minimizer yields\n$$\n\\widehat{f}(x_0) = \\widehat{\\beta}_0(x_0),\n$$\nthe locally linear estimate at $x_0$.\n\nNeighborhood selection is governed by a span proportion $s \\in (0,1]$. For group size $n_g$, we set $k=\\lceil s\\,n_g\\rceil$, take the $k$ nearest neighbors of $x_0$ in predictor space, and compute a local fit using only those neighbors. This implements the locality essential to Local Regression (LOESS).\n\nWe use the tricube kernel within the $k$-nearest-neighbor window. Let $d_i=|x_i-x_0|$ for the $k$ selected neighbors and $d_{\\max}=\\max_i d_i$. The tricube weight for neighbor $i$ is a smooth function that satisfies $w_i(x_0)=1$ at $d_i=0$ and $w_i(x_0)=0$ at $d_i=d_{\\max}$, with continuous derivatives that reduce the influence of more distant points. The weights are then applied in the weighted least squares criterion above. If $d_{\\max}=0$ (all $k$ neighbors at $x_0$) or the weighted design matrix is ill-conditioned, a stable fallback is to use a locally constant weighted average, which corresponds to setting $\\widehat{\\beta}_1(x_0)=0$ and computing $\\widehat{\\beta}_0(x_0)$ from the weighted mean of $y_i$.\n\nTo implement the local linear estimate numerically, form the design matrix centered at $x_0$,\n$$\nX(x_0) = \\begin{bmatrix} 1 & x_{(1)}-x_0 \\\\ \\vdots & \\vdots \\\\ 1 & x_{(k)}-x_0 \\end{bmatrix},\\quad\n\\mathbf{y} = \\begin{bmatrix} y_{(1)} \\\\ \\vdots \\\\ y_{(k)} \\end{bmatrix},\\quad\nW(x_0) = \\mathrm{diag}\\big(w_{(1)}(x_0),\\ldots,w_{(k)}(x_0)\\big),\n$$\nwhere $(\\cdot)$ indicates ordering restricted to the $k$-nearest neighbors. The weighted least squares solution is\n$$\n\\widehat{\\boldsymbol{\\beta}}(x_0) = \\arg\\min_{\\boldsymbol{\\beta}} \\| W(x_0)^{1/2} (\\mathbf{y} - X(x_0)\\boldsymbol{\\beta})\\|_2^2,\n$$\nwhich we compute by solving the least squares problem for $W(x_0)^{1/2}X(x_0)$ and $W(x_0)^{1/2}\\mathbf{y}$. The predicted value is $\\widehat{f}(x_0)=\\widehat{\\beta}_0(x_0)$.\n\nWe apply this procedure separately to each group $g\\in\\{A,B\\}$:\n\n- Data generation. For each group, draw $x_i^{(g)} \\sim \\mathrm{Uniform}[0,1]$, then set $y_i^{(g)} = f_g(x_i^{(g)}) + \\varepsilon_i^{(g)}$ with $\\varepsilon_i^{(g)} \\sim \\mathcal{N}(0,\\sigma_g^2)$, using the specified ground truths\n  $$\n  f_A(x) = \\sin(2\\pi x) + 0.5\\,x,\\quad \\sigma_A = 0.20,\\qquad\n  f_B(x) = \\cos(\\pi x) - x,\\quad \\sigma_B = 0.05.\n  $$\n  Independent pseudorandom seeds ensure reproducibility across test cases.\n\n- Evaluation. Construct an evaluation grid $x_{\\mathrm{eval}}$ of $M=200$ equally spaced points in $[0,1]$. For each group $g$, compute\n  $$\n  \\mathrm{MSE}_g = \\frac{1}{M}\\sum_{j=1}^{M} \\big(\\widehat{f}_g(x_{\\mathrm{eval},j}) - f_g(x_{\\mathrm{eval},j})\\big)^2.\n  $$\n  Combine the group errors via the simple average\n  $$\n  \\mathrm{MSE}_{\\mathrm{combined}} = \\frac{\\mathrm{MSE}_A + \\mathrm{MSE}_B}{2}.\n  $$\n\n- Comparison across span strategies. For each test case, we fit two configurations:\n  1. Pooled-span: use the same span $s_{\\mathrm{global}}$ for group $A$ and group $B$.\n  2. Group-specific: use span $s_A$ for group $A$ and span $s_B$ for group $B$.\n  Define the difference\n  $$\n  \\Delta = \\mathrm{MSE}_{\\mathrm{combined}}^{\\text{group-specific}} - \\mathrm{MSE}_{\\mathrm{combined}}^{\\text{pooled-span}}.\n  $$\n  Negative $\\Delta$ indicates an advantage for group-specific spans, while positive $\\Delta$ favors the pooled-span.\n\nAlgorithmic steps per test case ($n_A, n_B, s_{\\text{global}}, s_A, s_B$, seed):\n1. Set the pseudorandom generator with the provided seed.\n2. Generate $n_A$ and $n_B$ samples for groups $A$ and $B$ respectively.\n3. Build $x_{\\mathrm{eval}}$ with $M=200$ points in $[0,1]$.\n4. Fit LOESS for each group twice:\n   - With span $s_{\\mathrm{global}}$ to obtain $\\widehat{f}_A^{\\mathrm{pool}}$ and $\\widehat{f}_B^{\\mathrm{pool}}$.\n   - With spans $s_A$ and $s_B$ to obtain $\\widehat{f}_A^{\\mathrm{grp}}$ and $\\widehat{f}_B^{\\mathrm{grp}}$.\n5. Compute $\\mathrm{MSE}_A^{\\mathrm{pool}}$, $\\mathrm{MSE}_B^{\\mathrm{pool}}$, $\\mathrm{MSE}_A^{\\mathrm{grp}}$, $\\mathrm{MSE}_B^{\\mathrm{grp}}$, then the combined errors and $\\Delta$.\n6. Round $\\Delta$ to $6$ decimal places for reporting.\n\nEdge cases and numerical stability:\n- Ensure $k=\\lceil s\\,n_g\\rceil \\ge 2$ to fit at least a line with two points; when $k<2$ due to extreme rounding, enforce $k=2$.\n- If $d_{\\max}=0$ at any $x_0$, use equal weights for the neighborhood and compute a locally constant fit.\n- Use a weighted least squares solver via scaling by $\\sqrt{w_i}$, which is numerically stable and avoids explicit inversion.\n\nThe program outputs the four $\\Delta$ values for the specified test suite as a single line: $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$, with each value rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\n\n# No external inputs; deterministic seeds are provided in the test cases.\n\ndef tricube_weights(distances):\n    \"\"\"\n    Compute tricube weights for a vector of distances scaled by the maximum.\n    distances: 1D array of nonnegative distances for the selected neighbors.\n    Returns: weights array of same shape.\n    \"\"\"\n    dmax = distances.max()\n    if dmax == 0.0:\n        # All neighbors coincide with x0; use equal weights\n        return np.ones_like(distances)\n    u = distances / dmax\n    # Tricube kernel: (1 - u^3)^3 on [0,1], 0 outside.\n    w = (1 - u**3)**3\n    # Numerical safety: clip negatives (can occur at the boundary due to FP)\n    w[w < 0] = 0.0\n    return w\n\ndef loess_local_linear_predict(x_train, y_train, x_eval, span):\n    \"\"\"\n    Local linear LOESS predictions at x_eval given training data and span.\n    Uses k = ceil(span * n) nearest neighbors with tricube weights.\n    Falls back to locally constant weighted average if needed.\n    \"\"\"\n    x = np.asarray(x_train, dtype=float)\n    y = np.asarray(y_train, dtype=float)\n    xe = np.asarray(x_eval, dtype=float)\n    n = x.shape[0]\n    # Ensure at least 2 points in neighborhood for local linear fit\n    k = int(np.ceil(span * n))\n    if k < 2:\n        k = 2\n    if k > n:\n        k = n\n\n    y_pred = np.empty_like(xe, dtype=float)\n\n    for idx, x0 in enumerate(xe):\n        # Compute absolute distances to x0\n        dists = np.abs(x - x0)\n        # Select k nearest neighbors (indices)\n        if k < n:\n            # Use argpartition for efficiency\n            nn_idx = np.argpartition(dists, k - 1)[:k]\n        else:\n            nn_idx = np.arange(n)\n        # Extract local data\n        x_local = x[nn_idx]\n        y_local = y[nn_idx]\n        d_local = dists[nn_idx]\n        # Compute tricube weights on the local neighborhood\n        w = tricube_weights(d_local)\n\n        # Build weighted design for local linear fit centered at x0\n        # Design matrix columns: [1, x - x0]\n        X = np.column_stack((np.ones_like(x_local), x_local - x0))\n        # Apply sqrt weights for weighted least squares\n        sqrt_w = np.sqrt(w)\n        Xw = X * sqrt_w[:, None]\n        yw = y_local * sqrt_w\n\n        # Solve weighted least squares: minimize ||Xw * beta - yw||_2\n        # Use lstsq for numerical stability\n        try:\n            beta, *_ = np.linalg.lstsq(Xw, yw, rcond=None)\n            y_pred[idx] = beta[0]  # fitted value at x0 is intercept in centered design\n        except np.linalg.LinAlgError:\n            # Fallback: locally constant weighted average\n            if w.sum() > 0:\n                y_pred[idx] = np.sum(w * y_local) / np.sum(w)\n            else:\n                # Degenerate case: no weights (shouldn't happen); use simple mean\n                y_pred[idx] = y_local.mean()\n\n    return y_pred\n\ndef f_A(x):\n    return np.sin(2 * np.pi * x) + 0.5 * x\n\ndef f_B(x):\n    return np.cos(np.pi * x) - x\n\ndef generate_group_data(n, group, rng):\n    if group == 'A':\n        x = rng.uniform(0.0, 1.0, size=n)\n        y = f_A(x) + rng.normal(0.0, 0.20, size=n)\n    elif group == 'B':\n        x = rng.uniform(0.0, 1.0, size=n)\n        y = f_B(x) + rng.normal(0.0, 0.05, size=n)\n    else:\n        raise ValueError(\"Unknown group\")\n    return x, y\n\ndef mse_against_truth(y_hat, y_true):\n    diff = y_hat - y_true\n    return float(np.mean(diff * diff))\n\ndef run_case(nA, nB, s_global, sA, sB, seed):\n    rng = np.random.default_rng(seed)\n    # Generate training data\n    xA, yA = generate_group_data(nA, 'A', rng)\n    xB, yB = generate_group_data(nB, 'B', rng)\n    # Evaluation grid\n    M = 200\n    x_eval = np.linspace(0.0, 1.0, M)\n    # Truth on grid\n    truth_A = f_A(x_eval)\n    truth_B = f_B(x_eval)\n\n    # Pooled-span predictions (fit separately within groups, same span)\n    yhatA_pool = loess_local_linear_predict(xA, yA, x_eval, s_global)\n    yhatB_pool = loess_local_linear_predict(xB, yB, x_eval, s_global)\n    mseA_pool = mse_against_truth(yhatA_pool, truth_A)\n    mseB_pool = mse_against_truth(yhatB_pool, truth_B)\n    mse_pool_combined = 0.5 * (mseA_pool + mseB_pool)\n\n    # Group-specific span predictions\n    yhatA_grp = loess_local_linear_predict(xA, yA, x_eval, sA)\n    yhatB_grp = loess_local_linear_predict(xB, yB, x_eval, sB)\n    mseA_grp = mse_against_truth(yhatA_grp, truth_A)\n    mseB_grp = mse_against_truth(yhatB_grp, truth_B)\n    mse_grp_combined = 0.5 * (mseA_grp + mseB_grp)\n\n    # Difference\n    delta = mse_grp_combined - mse_pool_combined\n    return delta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (nA, nB, s_global, sA, sB, seed)\n    test_cases = [\n        (80, 80, 0.4, 0.3, 0.5, 20201),\n        (20, 120, 0.4, 0.25, 0.5, 20202),\n        (8, 8, 0.6, 0.8, 0.4, 20203),\n        (200, 15, 0.3, 0.2, 0.6, 20204),\n    ]\n\n    results = []\n    for case in test_cases:\n        nA, nB, s_global, sA, sB, seed = case\n        delta = run_case(nA, nB, s_global, sA, sB, seed)\n        # Round to 6 decimals as required\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While powerful, standard LOESS is based on least squares, making it sensitive to outliers that can distort the fitted curve. This practice  addresses this critical weakness by introducing you to robust smoothing through Iteratively Reweighted Least Squares (IRLS). You will implement a version of LOESS that automatically identifies and down-weights anomalous data points, leading to a much more reliable fit in the presence of contamination.",
            "id": "3141335",
            "problem": "You are to implement a complete program that constructs an iteratively reweighted local regression estimator, known as LOcally Estimated Scatterplot Smoothing (LOESS), and analyzes its robustness under contamination by high-leverage outliers in the predictor variable. The program must be self-contained and must not require any external input. The algorithm should be framed from well-established principles: weighted least squares and robust scale estimation, without providing shortcut formulas in the problem statement.\n\nThe core task is to perform local polynomial regression of degree $d$ at evaluation points using a neighborhood determined by a span parameter $\\alpha$ in $(0,1]$. Neighborhood weights must depend smoothly on the distances in the predictor variable, and robust iterations must multiplicatively reweight residuals to reduce the influence of extreme deviations in the response variable. The robust reweighting must use a redescending weight function tied to a scale derived from the residual distribution. The algorithm must be carried out for a specified number of iterations, starting from uniform residual weights.\n\nYou will construct a dataset following these rules:\n- Let $n$ be the total number of observations, where the uncontaminated predictor values $x$ are independently sampled from a uniform distribution on $[0,1]$ and correspond to responses $y$ generated from the true function $f(x)$ with additive noise.\n- The true function is $f(x) = \\sin(2\\pi x)$ for $x$ in $[0,1]$.\n- The noise is independent and identically distributed, with each observation perturbed by a zero-mean Gaussian with standard deviation $\\sigma$.\n- A contamination fraction $p$ in $[0,1]$ denotes the proportion of observations replaced by high-leverage outliers: these outliers have predictor values placed at the extremes of the domain (balanced at both ends) and response values shifted by a large fixed amplitude $A$ in opposite directions to be adversarial.\n- The contamination replaces clean points to keep the total sample size $n$ fixed.\n\nThe robustness analysis proceeds by comparing the predictive quality of the LOESS estimator on a dense evaluation grid in $[0,1]$ against the true function $f(x)$. For each contamination fraction $p$, compute the root mean squared error (RMSE) of the LOESS predictions on the grid relative to $f(x)$. Define breakdown to occur at $p$ if the ratio of this RMSE to the RMSE under no contamination exceeds a threshold $\\tau$.\n\nYour program must implement the algorithm and produce results for the following fixed parameter values (these are the test suite):\n- Total sample size $n = 200$.\n- Polynomial degree $d = 1$ (local linear).\n- Span parameter $\\alpha = 0.8$.\n- Noise standard deviation $\\sigma = 0.1$.\n- Outlier amplitude $A = 8.0$.\n- Robust iterations count equal to $2$ after the initial fit.\n- Threshold $\\tau = 3.0$.\n- Evaluation grid: $200$ equally spaced points in $[0,1]$.\n- Contamination fractions $p \\in \\{0.0, 0.1, 0.3, 0.5\\}$.\n\nThe program must use a fixed random seed to ensure deterministic behavior. You must compute, for each value of $p$, a boolean indicating whether breakdown occurs at that $p$. Additionally, compute the smallest contamination fraction among the given set that causes breakdown, or $1.0$ if none cause breakdown.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The list should contain a sequence of booleans (one per contamination fraction, in the same order), followed by a single float giving the smallest contamination fraction at which breakdown occurs, rounded to $3$ decimals.\n- For example, if breakdown occurs at the last two contamination fractions, and the smallest such fraction is $0.3$, the output would be a line of the form $\"[False,True,True,True,0.300]\"$.",
            "solution": "The problem asks for the implementation and analysis of a robust local regression algorithm, specifically LOESS (Locally Estimated Scatterplot Smoothing), also known as locally weighted regression. The analysis involves assessing its breakdown properties when the data is contaminated with high-leverage outliers.\n\nThe problem is scientifically well-grounded, algorithmically specific, and quantitatively defined. It is based on established principles of non-parametric statistics and robust estimation. All parameters and procedures are specified, making the problem well-posed and permitting a unique, verifiable solution for a given source of randomness. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe solution is structured as follows:\n1.  A detailed description of the data generation process, including the introduction of outliers.\n2.  A step-by-step exposition of the iteratively reweighted LOESS algorithm.\n3.  The methodology for performance evaluation and breakdown analysis.\n\n**1. Data Generation**\n\nThe dataset consists of $n = 200$ observations. A fraction $p$ of these are outliers, while the remaining $n(1-p)$ are \"clean\" data points.\n\nThe clean data are generated from the model:\n$$ y_i = f(x_i) + \\epsilon_i $$\nwhere the true underlying function is $f(x) = \\sin(2\\pi x)$ for $x \\in [0, 1]$. The predictor variables $x_i$ are drawn from a uniform distribution, $x_i \\sim U[0, 1]$. The noise terms $\\epsilon_i$ are independent and identically distributed draws from a zero-mean normal distribution with standard deviation $\\sigma = 0.1$, i.e., $\\epsilon_i \\sim N(0, \\sigma^2)$.\n\nA number of points, $n_{out} = \\text{round}(n \\times p)$, are designated as outliers. These are constructed to be high-leverage and adversarial. The outlier predictor values are placed at the extremes of the domain, $x=0$ and $x=1$, to maximize their influence (leverage) on the fit. The placements are balanced, with $\\lfloor n_{out}/2 \\rfloor$ outliers at $x=0$ and $\\lceil n_{out}/2 \\rceil$ at $x=1$. Their corresponding response values are shifted by a large amplitude $A=8.0$ from the true function value. Since $f(0)=f(1)=0$, the outlier responses are set to $y=+A$ for those at $x=0$ and $y=-A$ for those at $x=1$. These outliers replace an equal number of randomly selected clean points to maintain the total sample size $n$.\n\n**2. The Iteratively Reweighted LOESS Algorithm**\n\nLOESS is a non-parametric method that fits a smooth curve to data by performing a series of local weighted regressions. The key idea is that for any evaluation point $x_0$, the fit is determined by a small neighborhood of data points around $x_0$. Points closer to $x_0$ receive more weight. The robustness is achieved by iteratively down-weighting points that have large residuals from the previous fit.\n\nThe algorithm proceeds as follows for a total of $1+k_{robust}$ iterations, where $k_{robust}=2$ is the number of robust reweighting steps.\n\n**Step 2.1: Initialization**\nInitially, all points are considered equally reliable. The robustness weights $\\delta_i$ for all data points $(x_i, y_i)$ are initialized to $1$:\n$$ \\delta_i^{(0)} = 1 \\quad \\text{for } i = 1, \\dots, n $$\n\n**Step 2.2: Local Regression Fit (for each iteration)**\nFor each point $x_0$ in a set of evaluation points, a local polynomial is fitted to the data. In this problem, the polynomial degree is $d=1$ (local linear fit).\n\n**2.2.1. Neighborhood Selection:**\nA neighborhood $\\mathcal{N}(x_0)$ is defined, consisting of the $q$ training points $(x_i, y_i)$ whose $x_i$ values are closest to $x_0$. The size of the neighborhood, $q$, is determined by the span parameter $\\alpha = 0.8$:\n$$ q = \\lceil \\alpha n \\rceil = \\lceil 0.8 \\times 200 \\rceil = 160 $$\nLet $\\Delta(x_0)$ be the distance from $x_0$ to its $q$-th nearest neighbor among the $x_i$. This distance defines the half-width of the neighborhood.\n\n**2.2.2. Neighborhood Weighting:**\nEach point $x_i \\in \\mathcal{N}(x_0)$ is assigned a neighborhood weight $w_i(x_0)$ based on its distance to $x_0$. The standard tricube weight function is used:\n$$ w_i(x_0) = W\\left(\\frac{|x_i - x_0|}{\\Delta(x_0)}\\right) $$\nwhere $W(u) = (1 - |u|^3)^3$ for $|u| < 1$ and $W(u) = 0$ otherwise. This function gives the highest weight to points at $x_0$ and smoothly reduces the weight to zero at the edge of the neighborhood.\n\n**2.2.3. Weighted Least Squares (WLS):**\nAt each evaluation point $x_0$, we fit a local linear model, $g(x) = \\beta_0 + \\beta_1 (x - x_0)$, to the data in $\\mathcal{N}(x_0)$. The coefficients $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)^T$ are found by minimizing the sum of weighted squared residuals:\n$$ \\min_{\\beta_0, \\beta_1} \\sum_{i \\in \\mathcal{N}(x_0)} v_i(x_0) [y_i - (\\beta_0 + \\beta_1(x_i - x_0))]^2 $$\nThe total weight $v_i(x_0)$ for point $i$ in the current iteration is the product of its neighborhood weight $w_i(x_0)$ and its current robustness weight $\\delta_i$:\n$$ v_i(x_0) = w_i(x_0) \\delta_i $$\nThis WLS problem can be solved using the normal equations: $(\\mathbf{X}^T \\mathbf{V} \\mathbf{X})\\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{V} \\mathbf{y}$, where $\\mathbf{X}$ is the design matrix with rows $(1, x_i - x_0)$, $\\mathbf{y}$ is the vector of responses, and $\\mathbf{V}$ is a diagonal matrix of the total weights $v_i(x_0)$.\nThe LOESS fitted value at $x_0$ is then $\\hat{y}(x_0) = \\hat{\\beta}_0$.\n\n**Step 2.3: Robustness Re-weighting**\nAfter an initial fit (iteration $0$), and for each subsequent robust iteration, the robustness weights $\\delta_i$ are updated. This step is what makes the algorithm robust to outliers in the response variable $y$.\n\n**2.3.1. Residual Calculation:**\nFirst, we compute the residuals for all training points, $r_i = y_i - \\hat{y}(x_i)$, where $\\hat{y}(x_i)$ is the LOESS prediction at the training point $x_i$ itself, using the fit from the current iteration.\n\n**2.3.2. Robust Scale Estimation:**\nThe spread of the residuals is estimated robustly using the Median Absolute Deviation (MAD):\n$$ s = \\frac{1}{C} \\cdot \\text{median}_{i} |r_i| $$\nThe constant $C = \\Phi^{-1}(0.75) \\approx 0.6745$, where $\\Phi^{-1}$ is the quantile function of the standard normal distribution, makes $s$ an approximately unbiased estimator of the standard deviation $\\sigma$ if the residuals are normally distributed.\n\n**2.3.3. Robustness Weight Update:**\nThe new robustness weights $\\delta_i$ for the next iteration are calculated using the bisquare (or biweight) function, a redescending M-estimator:\n$$ \\delta_i \\leftarrow B\\left(\\frac{r_i}{6s}\\right) $$\nwhere $B(u) = (1 - u^2)^2$ for $|u| \\le 1$ and $B(u)=0$ otherwise. The factor of $6$ is a tuning constant that balances efficiency and robustness. Points with residuals larger than $6s$ are given a weight of zero, effectively removing them from the next fitting iteration.\n\nThis process (Step 2.2 and 2.3) is repeated for the specified number of robust iterations ($k_{robust}=2$). The final LOESS fit is obtained in the last iteration.\n\n**3. Performance Evaluation and Breakdown Analysis**\n\nThe robustness of the LOESS estimator is evaluated by computing its Root Mean Squared Error (RMSE) against the true function $f(x)$ on a fine evaluation grid of $N_{grid}=200$ equally spaced points in $[0, 1]$. The RMSE for a given contamination fraction $p$ is:\n$$ \\text{RMSE}_p = \\sqrt{\\frac{1}{N_{grid}} \\sum_{j=1}^{N_{grid}} (\\hat{y}_{p}(x_{eval, j}) - f(x_{eval, j}))^2} $$\nwhere $\\hat{y}_{p}$ is the final LOESS estimate under contamination $p$.\n\nBreakdown is defined to occur at a contamination level $p$ if the RMSE at that level is significantly higher than the baseline RMSE under no contamination ($p=0.0$). The condition is:\n$$ \\frac{\\text{RMSE}_p}{\\text{RMSE}_{0.0}} > \\tau $$\nwith the threshold $\\tau=3.0$.\n\nThe program calculates this for each specified contamination fraction $p \\in \\{0.0, 0.1, 0.3, 0.5\\}$. The final output consists of a boolean for each $p$ indicating if breakdown occurred, and the smallest value of $p$ from this set for which breakdown is observed. If no breakdown occurs for any of the tested fractions, this value is reported as $1.0$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements and analyzes the robustness of an iteratively reweighted LOESS estimator.\n    \"\"\"\n    \n    # --- Fixed Parameters from Problem Statement ---\n    N = 200\n    DEGREE = 1\n    ALPHA = 0.8\n    SIGMA = 0.1\n    OUTLIER_AMPLITUDE = 8.0\n    ROBUST_ITERS = 2\n    BREAKDOWN_THRESHOLD = 3.0\n    N_GRID = 200\n    P_VALUES = [0.0, 0.1, 0.3, 0.5]\n    SEED = 42\n\n    # --- Helper Functions for LOESS ---\n\n    def tricube_weight(u):\n        \"\"\"Tricube weight function for neighborhood weights.\"\"\"\n        u = np.abs(u)\n        return np.where(u < 1, (1 - u**3)**3, 0)\n\n    def bisquare_weight(u):\n        \"\"\"Bisquare weight function for robustness weights.\"\"\"\n        u = np.abs(u)\n        return np.where(u < 1, (1 - u**2)**2, 0)\n\n    def loess_fit(eval_points, x_data, y_data, robust_weights, alpha, degree):\n        \"\"\"\n        Performs the LOESS fit for a given set of evaluation points.\n        \"\"\"\n        n_data = len(x_data)\n        q = int(np.ceil(alpha * n_data))\n        \n        y_pred = np.zeros(len(eval_points))\n\n        for i, x0 in enumerate(eval_points):\n            dists = np.abs(x_data - x0)\n            \n            # Find neighborhood\n            sorted_indices = np.argsort(dists)\n            neighborhood_indices = sorted_indices[:q]\n            \n            # Neighborhood radius\n            h = dists[sorted_indices[q-1]]\n            \n            if h == 0.0:\n                # All points in neighborhood are at x0. Take weighted average.\n                mask = dists == 0\n                r_w = robust_weights[mask]\n                y_at_x0 = y_data[mask]\n                if np.sum(r_w) > 0:\n                    y_pred[i] = np.sum(r_w * y_at_x0) / np.sum(r_w)\n                else:\n                    y_pred[i] = np.mean(y_at_x0) # Fallback if all weights are zero\n                continue\n\n            # Data for local regression\n            x_hood = x_data[neighborhood_indices]\n            y_hood = y_data[neighborhood_indices]\n            \n            # Neighborhood weights (tricube)\n            scaled_dists = dists[neighborhood_indices] / h\n            neighborhood_w = tricube_weight(scaled_dists)\n            \n            # Total weights (neighborhood * robustness)\n            robust_w_hood = robust_weights[neighborhood_indices]\n            total_w = neighborhood_w * robust_w_hood\n            \n            # Weighted Least Squares\n            X = np.ones((q, degree + 1))\n            for d in range(1, degree + 1):\n                X[:, d] = (x_hood - x0)**d\n            \n            W = np.diag(total_w)\n            \n            A_matrix = X.T @ W @ X\n            b_vector = X.T @ W @ y_hood\n            \n            try:\n                beta = np.linalg.solve(A_matrix, b_vector)\n                y_pred[i] = beta[0]\n            except np.linalg.LinAlgError:\n                # Fallback to weighted average if matrix is singular\n                if np.sum(total_w) > 0:\n                    y_pred[i] = np.sum(total_w * y_hood) / np.sum(total_w)\n                else: \n                    # If all weights are 0, use unweighted neighborhood mean\n                    y_pred[i] = np.mean(y_hood)\n                    \n        return y_pred\n\n    def robust_loess_pipeline(x_data, y_data, eval_points, alpha, degree, n_robust_iters):\n        \"\"\"\n        Runs the full iteratively reweighted LOESS pipeline.\n        \"\"\"\n        n_data = len(x_data)\n        robust_weights = np.ones(n_data)\n        \n        # Mad constant for normal distribution\n        mad_c = norm.ppf(0.75) # Approximately 0.6745\n\n        # Total iterations = 1 (initial) + n_robust_iters\n        for k in range(n_robust_iters + 1):\n            if k > 0: # Update robust weights for iterations 1, 2, ...\n                # Predict at training points to get residuals\n                y_hat_train = loess_fit(x_data, x_data, y_data, robust_weights, alpha, degree)\n                residuals = y_data - y_hat_train\n                \n                # Robust scale estimation (MAD)\n                s = np.median(np.abs(residuals)) / mad_c\n                if s == 0:\n                    # If MAD is 0, no re-weighting unless there are non-zero residuals\n                    if np.any(residuals != 0):\n                        # Use a small non-zero scale to avoid division by zero\n                        s = np.mean(np.abs(residuals)) / mad_c\n                    else: # All residuals are zero, no need to re-weight\n                        break\n\n                # Bisquare robustness weights\n                scaled_residuals = residuals / (6 * s)\n                robust_weights = bisquare_weight(scaled_residuals)\n        \n        # Final fit on the evaluation grid using final weights\n        y_hat_eval = loess_fit(eval_points, x_data, y_data, robust_weights, alpha, degree)\n        return y_hat_eval\n\n    # --- Main Analysis Logic ---\n    rng = np.random.default_rng(SEED)\n\n    def f_true(x):\n        return np.sin(2 * np.pi * x)\n\n    def generate_data(p):\n        n_clean = int(N * (1-p))\n        n_outliers = N - n_clean\n\n        # Generate base clean data\n        x_clean_full = rng.uniform(0, 1, N)\n        y_clean_full = f_true(x_clean_full) + rng.normal(0, SIGMA, N)\n        \n        # Select a subset of clean points\n        clean_indices = rng.choice(N, size=n_clean, replace=False)\n        x_final = x_clean_full[clean_indices]\n        y_final = y_clean_full[clean_indices]\n\n        if n_outliers > 0:\n            # Generate outliers\n            n_out_x0 = n_outliers // 2\n            n_out_x1 = n_outliers - n_out_x0\n            \n            x_outliers = np.concatenate([np.zeros(n_out_x0), np.ones(n_out_x1)])\n            y_outliers = np.concatenate([\n                np.full(n_out_x0, OUTLIER_AMPLITUDE),\n                np.full(n_out_x1, -OUTLIER_AMPLITUDE)\n            ])\n            \n            # Combine clean and outlier data\n            x_final = np.concatenate([x_final, x_outliers])\n            y_final = np.concatenate([y_final, y_outliers])\n            \n        return x_final, y_final\n\n    eval_grid = np.linspace(0, 1, N_GRID)\n    y_true_grid = f_true(eval_grid)\n    \n    rmses = []\n    for p in P_VALUES:\n        x, y = generate_data(p)\n        y_hat = robust_loess_pipeline(x, y, eval_grid, ALPHA, DEGREE, ROBUST_ITERS)\n        \n        rmse = np.sqrt(np.mean((y_hat - y_true_grid)**2))\n        rmses.append(rmse)\n\n    # --- Breakdown Analysis ---\n    rmse0 = rmses[0]\n    breakdowns = [r / rmse0 > BREAKDOWN_THRESHOLD for r in rmses]\n    \n    breakdown_p = 1.0\n    for i, p in enumerate(P_VALUES):\n        if breakdowns[i]:\n            breakdown_p = p\n            break\n            \n    # Format and print the final result\n    result_list = [str(b).lower() for b in breakdowns]\n    result_list.append(f\"{breakdown_p:.3f}\")\n    \n    print(f\"[{','.join(result_list)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "Extending local regression to multiple predictors introduces new challenges, most notably the problem of collinearity. This issue can become particularly acute within the small, localized neighborhoods used by LOESS, leading to unstable estimates with high variance. This exercise  provides a deep dive into the mechanics of multivariate LOESS, allowing you to analytically diagnose the variance inflation caused by local collinearity and implement Tikhonov regularization as a powerful method to stabilize the fit.",
            "id": "3141257",
            "problem": "You are to implement a fully self-contained program that compares Locally Estimated Scatterplot Smoothing (LOESS) under multivariate predictor collinearity, analyzes how ill-conditioned local design matrices affect the variance of the fitted value, and evaluates the stabilizing effect of Tikhonov regularization. The foundation must be constructed from the principle of weighted least squares and the standard linear model variance propagation, starting with the formulation of local linear regression and the role of a compactly supported kernel to focus attention on nearby data.\n\nConsider a dataset of $n$ bivariate predictors $\\{(x_{1i}, x_{2i})\\}_{i=1}^n$ and responses $\\{y_i\\}_{i=1}^n$ generated from a smooth underlying function with additive homoscedastic noise of variance $\\sigma^2$, and the local linear LOESS estimator evaluated at a target point $(t_1, t_2)$. In local linear LOESS, the local model uses an intercept and predictors centered at the target point, and the estimation proceeds by solving the local weighted least squares normal equations. When the predictors are highly collinear in the neighborhood of the target, the local design matrix becomes ill-conditioned, amplifying variance in the fitted value. Tikhonov regularization adds a multiple of the identity matrix to the local normal matrix to mitigate ill-conditioning and control variance inflation.\n\nYour program must:\n- Construct the local linear LOESS estimator in matrix form using a compactly supported, monotonically decreasing kernel weight function that assigns zero weight to points outside a neighborhood radius and greater weight to points nearer to $(t_1, t_2)$; use the standard tri-cube kernel.\n- Center the predictors at $(t_1, t_2)$ so that the fitted value at $(t_1, t_2)$ equals the estimated intercept.\n- Compute the analytic variance of the fitted value at $(t_1, t_2)$ under homoscedastic noise with variance $\\sigma^2$, using the covariance of the weighted least squares estimator obtained from the linear model and the specified kernel weights.\n- Incorporate Tikhonov regularization by adding a penalty parameter $\\lambda \\ge 0$ to the local normal matrix and recompute the variance of the fitted value under this modification.\n\nDataset generation must be deterministic for the test cases via fixed random seeds. Use $n$ equal to $200$. For collinear scenarios, let $x_{1i}$ be independent draws from the uniform distribution on $[0,1]$ and set $x_{2i} = x_{1i} + \\epsilon_i$ with $\\epsilon_i$ drawn from a normal distribution with mean $0$ and a specified standard deviation; for the independent scenario, draw $x_{2i}$ independently from the uniform distribution on $[0,1]$. The underlying signal $g(x_1, x_2)$ may be any smooth function; the analytic variance of the fitted value depends on the design and weights, not on the responses, under the stated assumptions. Use the tri-cube kernel to compute local weights based on Euclidean distance to $(t_1, t_2)$ and a span fraction $f \\in (0,1)$ indicating the fraction of nearest neighbors used to define the neighborhood radius. All computations must be in pure mathematical terms with no physical units.\n\nCompute the variance of the fitted value at $(t_1, t_2)$ for the following test suite, each specified by a tuple of parameters (scenario, $\\epsilon$ std, $f, \\lambda, n$, seed, $(t_1,t_2), \\sigma^2)$:\n- Case $1$ (general collinearity, unregularized): $(\\text{collinear}, 0.02, 0.25, 0, 200, 0, (0.5, 0.5), 0.0025)$.\n- Case $2$ (general collinearity, light regularization): $(\\text{collinear}, 0.02, 0.25, 10^{-3}, 200, 0, (0.5, 0.5), 0.0025)$.\n- Case $3$ (near-perfect collinearity, unregularized, smaller span): $(\\text{collinear}, 10^{-4}, 0.15, 0, 200, 1, (0.5, 0.5), 0.0025)$.\n- Case $4$ (near-perfect collinearity, stronger regularization): $(\\text{collinear}, 10^{-4}, 0.15, 10^{-2}, 200, 1, (0.5, 0.5), 0.0025)$.\n- Case $5$ (independent predictors, baseline): $(\\text{independent}, 0, 0.25, 0, 200, 2, (0.5, 0.5), 0.0025)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases above, where each entry is the computed variance (a float) of the fitted value at $(t_1, t_2)$ for that case, for example, $\"[v_1,v_2,v_3,v_4,v_5]\"$.",
            "solution": "The problem requires the implementation and analysis of a Locally Estimated Scatterplot Smoothing (LOESS) estimator, with a focus on how the variance of the fitted value is affected by predictor collinearity and Tikhonov regularization. The analysis will be conducted by deriving the analytical variance of the LOESS estimator under the standard linear model assumptions.\n\n### **Theoretical Foundation: Local Weighted Least Squares**\n\nLOESS is a non-parametric regression method that fits a simple model to a localized subset of data. For a given target point $t = (t_1, t_2)$, we approximate the underlying function $g(x_1, x_2)$ with a local linear model (a tangent plane). We use predictors centered at $t$ to ensure the estimated intercept directly corresponds to the fitted value at $t$. The local linear model for a data point $(x_{1i}, x_{2i})$ is:\n$$\ng(x_{1i}, x_{2i}) \\approx \\beta_0 + \\beta_1 (x_{1i} - t_1) + \\beta_2 (x_{2i} - t_2)\n$$\nThe LOESS estimate at the target point $t$ is defined as $\\hat{g}(t) = \\hat{\\beta}_0$. The coefficients $\\beta = [\\beta_0, \\beta_1, \\beta_2]^T$ are estimated by solving a weighted least squares (WLS) problem. The objective is to minimize the weighted sum of squared residuals for all $n$ data points:\n$$\n\\min_{\\beta} \\sum_{i=1}^{n} w_i(t) \\left( y_i - \\left( \\beta_0 + \\beta_1 (x_{1i} - t_1) + \\beta_2 (x_{2i} - t_2) \\right) \\right)^2\n$$\nThe weights $w_i(t)$ are determined by a kernel function, giving greater weight to points closer to $t$ and zero weight to points outside a defined neighborhood.\n\n### **Matrix Formulation and Estimator Variance**\n\nThe WLS problem can be expressed in matrix form. Let $Y$ be the $n \\times 1$ vector of observed responses $\\{y_i\\}$, $X_t$ be the $n \\times 3$ local design matrix, and $W_t$ be the $n \\times n$ diagonal matrix of weights. The $i$-th row of $X_t$ is $[1, x_{1i} - t_1, x_{2i} - t_2]$, and the $i$-th diagonal element of $W_t$ is $w_i(t)$. The minimization problem is equivalent to solving the normal equations:\n$$\n(X_t^T W_t X_t) \\hat{\\beta} = X_t^T W_t Y\n$$\nThe WLS estimator for $\\beta$ is:\n$$\n\\hat{\\beta} = (X_t^T W_t X_t)^{-1} X_t^T W_t Y\n$$\nUnder the assumption of homoscedastic noise, where the responses $Y$ have a true mean $g(X)$ and covariance $\\text{Cov}(Y) = \\sigma^2 I_n$, the covariance matrix of the estimator $\\hat{\\beta}$ is given by:\n$$\n\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (X_t^T W_t X_t)^{-1} (X_t^T W_t^2 X_t) (X_t^T W_t X_t)^{-1}\n$$\nwhere $W_t^2 = W_t W_t$ is a diagonal matrix with elements $w_i(t)^2$. The fitted value at $t$ is $\\hat{g}(t) = \\hat{\\beta}_0 = e_1^T \\hat{\\beta}$, where $e_1 = [1, 0, 0]^T$. The variance of the fitted value is therefore:\n$$\n\\text{Var}(\\hat{g}(t)) = e_1^T \\text{Cov}(\\hat{\\beta}) e_1\n$$\nThis corresponds to the top-left element $(1,1)$ of the $\\text{Cov}(\\hat{\\beta})$ matrix. High collinearity between the predictors $(x_{1i} - t_1)$ and $(x_{2i} - t_2)$ within the neighborhood of $t$ makes the \"local normal matrix\" $X_t^T W_t X_t$ nearly singular, which inflates the elements of its inverse and thus catastrophically increases the variance of the estimator.\n\n### **Tikhonov Regularization**\n\nTo stabilize the solution in the presence of collinearity, we introduce Tikhonov regularization. The objective function is modified to include a penalty term on the magnitude of the coefficients:\n$$\n\\min_{\\beta} \\left\\{ (Y - X_t \\beta)^T W_t (Y - X_t \\beta) + \\lambda \\beta^T \\beta \\right\\}\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter. The regularized normal equations are:\n$$\n(X_t^T W_t X_t + \\lambda I_3) \\hat{\\beta}_{\\lambda} = X_t^T W_t Y\n$$\nwhere $I_3$ is the $3 \\times 3$ identity matrix. The regularized estimator $\\hat{\\beta}_{\\lambda}$ is:\n$$\n\\hat{\\beta}_{\\lambda} = (X_t^T W_t X_t + \\lambda I_3)^{-1} X_t^T W_t Y\n$$\nThe addition of the term $\\lambda I_3$ ensures that the matrix to be inverted is well-conditioned, even if $X_t^T W_t X_t$ is not. Following the same derivation as before, the covariance matrix of the regularized estimator is:\n$$\n\\text{Cov}(\\hat{\\beta}_{\\lambda}) = \\sigma^2 (X_t^T W_t X_t + \\lambda I_3)^{-1} (X_t^T W_t^2 X_t) (X_t^T W_t X_t + \\lambda I_3)^{-1}\n$$\nThe variance of the regularized fitted value $\\hat{g}_{\\lambda}(t)$ is the $(1,1)$ element of this matrix. Note that the unregularized case is a special instance where $\\lambda=0$.\n\n### **Kernel Weights and Computational Procedure**\n\nThe weights $w_i(t)$ are computed using the tri-cube kernel. First, we define a neighborhood around $t$ containing a fraction $f$ of the total data points.\n1.  For all points $i=1, \\dots, n$, calculate the Euclidean distance $d_i = \\|(x_{1i}, x_{2i}) - t\\|_2$.\n2.  The neighborhood size is $q = \\lceil f \\cdot n \\rceil$. The neighborhood radius, $h$, is the distance to the $q$-th nearest neighbor of $t$.\n3.  Scale the distances for all points: $u_i = d_i / h$.\n4.  The tri-cube weight is assigned as $w_i(t) = (1 - u_i^3)^3$ for points within the neighborhood ($u_i < 1$) and $w_i(t) = 0$ for points outside it ($u_i \\ge 1$).\n\nThe computational procedure for each test case involves generating the predictor data $(x_{1i}, x_{2i})$, computing the weight vector using the specified span $f$ and target point $t$, constructing the matrices $X_t^T W_t X_t$ and $X_t^T W_t^2 X_t$, and finally calculating the variance using the appropriate formula for the given regularization parameter $\\lambda$ and noise variance $\\sigma^2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_loess_variance(scenario, eps_std, f, lamb, n, seed, t, sigma2):\n    \"\"\"\n    Computes the analytic variance of the LOESS fitted value at a target point t.\n\n    Args:\n        scenario (str): 'collinear' or 'independent' for data generation.\n        eps_std (float): Standard deviation for collinearity noise.\n        f (float): Span fraction for the LOESS kernel.\n        lamb (float): Tikhonov regularization parameter.\n        n (int): Number of data points.\n        seed (int): Seed for the random number generator.\n        t (tuple): The target point (t1, t2) for the LOESS fit.\n        sigma2 (float): Variance of the homoscedastic noise in the response.\n\n    Returns:\n        float: The computed variance of the fitted value at t.\n    \"\"\"\n    # 1. Generate predictor data deterministically.\n    rng = np.random.default_rng(seed)\n    x1 = rng.uniform(0, 1, n)\n    if scenario == 'collinear':\n        epsilon = rng.normal(0, eps_std, n)\n        x2 = x1 + epsilon\n    elif scenario == 'independent':\n        x2 = rng.uniform(0, 1, n)\n    else:\n        raise ValueError(f\"Unknown scenario: {scenario}\")\n    \n    X_data = np.stack([x1, x2], axis=1)\n    \n    # 2. Calculate tri-cube kernel weights.\n    t_vec = np.array(t)\n    distances = np.linalg.norm(X_data - t_vec, axis=1)\n    \n    # Determine the neighborhood radius h based on the span f.\n    # The neighborhood includes q = ceil(f*n) points.\n    q = int(np.ceil(f * n))\n    if q <= 0 or q > n:\n        # Invalid span, this case should not happen with problem constraints f in (0,1).\n        return np.nan\n\n    # h is the distance to the q-th nearest neighbor (using 0-based index q-1).\n    h = np.sort(distances)[q - 1]\n\n    # Handle the case where the radius is zero to avoid division by zero.\n    if h == 0.0:\n        u = np.full_like(distances, 2.0)  # Weights will be 0\n        u[distances == 0.0] = 0.0        # Points at t get full weight\n    else:\n        u = distances / h\n    \n    # Apply the tri-cube kernel function.\n    weights = np.zeros(n)\n    mask = u < 1.0\n    weights[mask] = (1 - u[mask]**3)**3\n\n    # 3. Construct local design matrix and other required matrices.\n    X_centered = X_data - t_vec\n    Xt = np.hstack([np.ones((n, 1)), X_centered])\n    p = Xt.shape[1] # Number of parameters (intercept, slope1, slope2) is 3\n\n    # Efficiently compute Xt.T @ W @ Xt and Xt.T @ W^2 @ Xt\n    # where W is a diagonal matrix of weights.\n    M_normal = (Xt.T * weights) @ Xt\n    M_squared_weights = (Xt.T * (weights**2)) @ Xt\n\n    # 4. Apply Tikhonov regularization.\n    M_lambda = M_normal + lamb * np.eye(p)\n\n    # 5. Compute the variance of the fitted value.\n    try:\n        inv_M_lambda = np.linalg.inv(M_lambda)\n    except np.linalg.LinAlgError:\n        # If the matrix is singular even after regularization (e.g., lambda=0)\n        return np.inf\n\n    # Variance matrix for the coefficient estimator beta_hat\n    cov_beta_hat_unscaled = inv_M_lambda @ M_squared_weights @ inv_M_lambda\n    \n    # The variance of the fitted value (the intercept) is the top-left element.\n    var_beta0_unscaled = cov_beta_hat_unscaled[0, 0]\n    \n    # Scale by the noise variance sigma^2.\n    final_variance = sigma2 * var_beta0_unscaled\n    \n    return final_variance\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (scenario, eps_std, f, lambda, n, seed, t, sigma2)\n        ('collinear', 0.02, 0.25, 0.0, 200, 0, (0.5, 0.5), 0.0025),\n        ('collinear', 0.02, 0.25, 1e-3, 200, 0, (0.5, 0.5), 0.0025),\n        ('collinear', 1e-4, 0.15, 0.0, 200, 1, (0.5, 0.5), 0.0025),\n        ('collinear', 1e-4, 0.15, 1e-2, 200, 1, (0.5, 0.5), 0.0025),\n        ('independent', 0.0, 0.25, 0.0, 200, 2, (0.5, 0.5), 0.0025),\n    ]\n\n    results = []\n    for case in test_cases:\n        scenario, eps_std, f, lamb, n, seed, t, sigma2 = case\n        variance = calculate_loess_variance(scenario, eps_std, f, lamb, n, seed, t, sigma2)\n        results.append(variance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{v:.8f}' for v in results)}]\")\n\nsolve()\n```"
        }
    ]
}