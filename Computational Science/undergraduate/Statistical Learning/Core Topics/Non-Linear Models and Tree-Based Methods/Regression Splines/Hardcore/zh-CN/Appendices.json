{
    "hands_on_practices": [
        {
            "introduction": "回归样条的威力源于其灵活的函数空间，但要在实践中运用，我们需要一套“基函数”来表示这个空间。此练习  引导你探索两种构建三次样条的常用基：B样条基和截断幂基。通过亲手实现并计算平滑矩阵的“有效自由度”，你将验证一个核心理论：无论选择哪种基，只要它们定义的函数空间相同，最终的拟合从理论上讲是等价的，同时你也会体会到它们在数值稳定性上的重要差异。",
            "id": "3168939",
            "problem": "给定一个在紧凑区间上、具有固定内部节点的三次回归样条的惩罚最小二乘回归问题。其基础出发点是线性惩罚最小二乘估计量的定义：给定一个由基函数构成的设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 和一个非负惩罚参数 $\\lambda \\in \\mathbb{R}_{\\ge 0}$，估计量 $\\hat{\\boldsymbol{\\beta}}$ 最小化目标函数 $\\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\,\\mathcal{J}(\\boldsymbol{\\beta})$，其中 $\\mathcal{J}(\\boldsymbol{\\beta})$ 是一个二次型，用于衡量由 $\\boldsymbol{\\beta}$ 在所选样条基中表示的函数的粗糙度。相应的拟合值为 $\\hat{\\mathbf{y}} = \\mathbf{S}_\\lambda \\,\\mathbf{y}$，其中 $\\mathbf{S}_\\lambda$ 是与该惩罚估计量相关的线性平滑矩阵。有效自由度定义为 $\\mathrm{df}(\\lambda) = \\mathrm{tr}(\\mathbf{S}_\\lambda)$，其中 $\\mathrm{tr}(\\cdot)$ 表示矩阵的迹。\n\n您的任务是实现并比较两组基，它们在 $[0,1]$ 上张成相同的三次回归样条空间，使用相同的粗糙度惩罚（表示为二阶导数平方的积分），并在一系列测试惩罚参数下计算和比较有效自由度：\n\n- 基 A：次数为 $3$ 的三次B样条基，其开放节点向量由内部节点构建。\n- 基 B：三次回归样条的截断幂基，由列向量 $\\{1, x, x^2, x^3, (x-\\kappa_j)_+^3\\}_{j=1}^r$ 组成，其中 $(\\cdot)_+$ 表示取正部，$\\{\\kappa_j\\}$ 是内部节点。\n\n请使用以下规范：\n\n- 在区间 $[0,1]$ 上使用 $n = 80$ 个等距输入位置 $x_i = i/(n-1)$，其中 $i \\in \\{0,1,\\dots,n-1\\}$。\n- 使用内部节点 $\\{\\kappa_1,\\kappa_2,\\kappa_3\\} = \\{0.2, 0.5, 0.8\\}$。\n- 对于代表给定基中函数 $f(x)$ 的系数向量 $\\boldsymbol{\\beta}$，将粗糙度惩罚定义为 $\\mathcal{J}(\\boldsymbol{\\beta}) = \\int_0^1 \\left(f''(x)\\right)^2 \\, dx$。通过一个对称半正定矩阵 $\\mathbf{\\Omega} \\in \\mathbb{R}^{p \\times p}$ 在系数空间中实现此惩罚。该矩阵通过在 $[0,1]$ 上的 $G = 2001$ 个均匀网格点上使用梯形法则近似积分来数值构建。对于每个基函数 $b_j(x)$，在网格上计算其二阶导数 $b_j''(x)$，并使用梯形法则设置 $\\Omega_{ij} \\approx \\int_0^1 b_i''(x)\\,b_j''(x)\\,dx$。\n- 为每个基和每个 $\\lambda$ 构建与惩罚最小二乘解相关的线性平滑矩阵 $\\mathbf{S}_\\lambda$。为每个基计算 $\\mathrm{df}(\\lambda) = \\mathrm{tr}(\\mathbf{S}_\\lambda)$。\n- 使用惩罚参数 $\\lambda \\in \\{0, 0.1, 10, 10^6\\}$。\n\n您的程序必须：\n\n1. 在 $n$ 个输入位置上为两种基构建设计矩阵 $\\mathbf{X}_{\\text{B}}$ 和 $\\mathbf{X}_{\\text{TP}}$。\n2. 通过在网格上进行数值积分来构建两个惩罚矩阵 $\\mathbf{\\Omega}_{\\text{B}}$ 和 $\\mathbf{\\Omega}_{\\text{TP}}$，以近似 $\\int_0^1 b_i''(x)\\,b_j''(x)\\,dx$。\n3. 对于测试集中的每个 $\\lambda$，计算相应的平滑矩阵 $\\mathbf{S}_{\\lambda,\\text{B}}$ 和 $\\mathbf{S}_{\\lambda,\\text{TP}}$ 及其迹 $\\mathrm{df}_{\\text{B}}(\\lambda)$ 和 $\\mathrm{df}_{\\text{TP}}(\\lambda)$。\n4. 报告测试集中前三个 $\\lambda$ 值 $\\{0, 0.1, 10\\}$ 的绝对差 $|\\mathrm{df}_{\\text{B}}(\\lambda) - \\mathrm{df}_{\\text{TP}}(\\lambda)|$，四舍五入到 $6$ 位小数。然后，对于最大值 $\\lambda = 10^6$，报告 $\\mathrm{df}_{\\text{B}}(10^6)$，四舍五入到 $6$ 位小数。为确保数值稳定性，对任何所需的矩阵求逆使用 Moore–Penrose 伪逆。\n\n角度单位不适用。没有物理单位。\n\n测试集及预期覆盖范围：\n\n- $\\lambda = 0$：无惩罚的边界情况；两个基张成相同的空间，因此相应的普通最小二乘投影应该是基不变的。\n- $\\lambda = 0.1$：中度平滑。\n- $\\lambda = 10$：强度平滑。\n- $\\lambda = 10^6$：极强度平滑；对于二阶导数平方惩罚，其零空间对应于线性函数，因此有效自由度应接近 $2$。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表，内含 $4$ 个浮点数，顺序如下：$\\lambda = 0$、$\\lambda = 0.1$ 和 $\\lambda = 10$ 时的绝对自由度差异（按此顺序），然后是 $\\mathrm{df}_{\\text{B}}(10^6)$，所有数值均四舍五入到 $6$ 位小数。例如，输出行可能看起来像 $[0.000000,0.000001,0.000003,2.000000]$。",
            "solution": "该问题要求在一个惩罚最小二乘框架内，比较用于三次回归样条的两种不同基——B样条基和截断幂基。任务的核心是计算和比较在由拟合函数二阶导数平方的积分定义的粗糙度惩罚下，样条平滑器的有效自由度。\n\n惩罚最小二乘目标函数的一般形式为：\n$$ \\text{PLS}(\\boldsymbol{\\beta}) = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}\\|_2^2 + \\lambda \\boldsymbol{\\beta}^\\top \\mathbf{\\Omega} \\boldsymbol{\\beta} $$\n其中 $\\mathbf{X}$ 是在 $n$ 个数据点上评估的 $n \\times p$ 基函数设计矩阵，$\\boldsymbol{\\beta}$ 是 $p \\times 1$ 的基系数向量，$\\lambda \\ge 0$ 是平滑参数，$\\mathbf{\\Omega}$ 是一个 $p \\times p$ 的对称半正定矩阵，它根据系数定义了惩罚。\n\n最小化此目标函数的解 $\\hat{\\boldsymbol{\\beta}}$ 由正规方程给出：\n$$ (\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^\\top\\mathbf{y} $$\n为确保数值稳定性，特别是在 $(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})$ 可能奇异或病态时，我们使用 Moore-Penrose 伪逆，表示为 $(\\cdot)^+$，得到：\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})^{+} \\mathbf{X}^\\top \\mathbf{y} $$\n拟合值向量 $\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$ 是观测值 $\\mathbf{y}$ 的一个线性函数：\n$$ \\hat{\\mathbf{y}} = \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})^{+} \\mathbf{X}^\\top \\mathbf{y} = \\mathbf{S}_\\lambda \\mathbf{y} $$\n矩阵 $\\mathbf{S}_\\lambda = \\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})^{+} \\mathbf{X}^\\top$ 被称为平滑矩阵。惩罚样条拟合的有效自由度定义为该矩阵的迹：\n$$ \\mathrm{df}(\\lambda) = \\mathrm{tr}(\\mathbf{S}_\\lambda) $$\n利用迹的循环性质 $\\mathrm{tr}(AB) = \\mathrm{tr}(BA)$，可以更高效地计算：\n$$ \\mathrm{df}(\\lambda) = \\mathrm{tr}(\\mathbf{X}^\\top\\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})^{+}) $$\n这在计算上更有优势，因为它涉及一个 $p \\times p$ 矩阵的迹，而不是一个 $n \\times n$ 矩阵的迹，且通常 $p \\ll n$。\n\n问题指定了在 $[0,1]$ 上的三次样条函数空间，具有一组 $r=3$ 个内部节点 $\\boldsymbol{\\kappa} = \\{0.2, 0.5, 0.8\\}$。输入数据包含 $n=80$ 个等距点 $x_i = i/(n-1)$，其中 $i \\in \\{0, \\dots, n-1\\}$。\n\n粗糙度惩罚在函数空间上内在地定义为 $\\mathcal{J}(f) = \\int_0^1 (f''(x))^2 dx$。对于一个用基表示的函数 $f(x) = \\sum_{j=1}^p \\beta_j b_j(x)$，该惩罚变为一个二次型 $\\boldsymbol{\\beta}^\\top\\mathbf{\\Omega}\\boldsymbol{\\beta}$，其中惩罚矩阵的元素由 $\\Omega_{ij} = \\int_0^1 b_i''(x)b_j''(x)dx$ 给出。此积分使用梯形法则在一个包含 $G=2001$ 个点的精细网格上进行数值近似。\n\n我们现在为指定的两个基构建设计矩阵和惩罚矩阵。\n\n**基 A：三次B样条基**\n对于在 $[0,1]$ 上具有 $r=3$ 个内部节点的三次样条（$k=3$），我们使用一个开放节点向量来确保函数在边界处表现良好。节点向量 $\\mathbf{t}_{\\text{B}}$ 是通过在每个边界处增加 $k+1 = 4$ 个节点来构建的：\n$$ \\mathbf{t}_{\\text{B}} = \\{ \\underbrace{0, 0, 0, 0}_{4}, 0.2, 0.5, 0.8, \\underbrace{1, 1, 1, 1}_{4} \\} $$\nB样条基函数的数量是 $p_{\\text{B}} = (\\text{节点数}) - k - 1 = 11 - 3 - 1 = 7$。\n设计矩阵 $\\mathbf{X}_{\\text{B}}$ 是一个 $n \\times p_{\\text{B}}$ 的矩阵，其中 $(\\mathbf{X}_{\\text{B}})_{ij} = B_{j,3}(x_i)$，$B_{j,3}(x)$ 是由节点向量 $\\mathbf{t}_{\\text{B}}$ 定义的第 $j$ 个三次B样条基函数。\n惩罚矩阵 $\\mathbf{\\Omega}_{\\text{B}}$ 是一个 $p_{\\text{B}} \\times p_{\\text{B}}$ 的矩阵，其中 $(\\mathbf{\\Omega}_{\\text{B}})_{ij} \\approx \\int_0^1 B_{i,3}''(x) B_{j,3}''(x) dx$，通过数值计算得到。B样条基因其基函数的局部支撑性而具有出色的数值稳定性。\n\n**基 B：截断幂基**\n用于同一函数空间的该基由一个三次多项式基和每个内部节点对应的一个截断幂函数组成：\n$$ \\{1, x, x^2, x^3, (x-0.2)_+^3, (x-0.5)_+^3, (x-0.8)_+^3 \\} $$\n其中 $(u)_+ = \\max(0, u)$。基函数的数量为 $p_{\\text{TP}} = 4 + r = 4 + 3 = 7$。\n设计矩阵 $\\mathbf{X}_{\\text{TP}}$ 是一个 $n \\times p_{\\text{TP}}$ 的矩阵，其列是在数据点 $x_i$ 上评估的这 $7$ 个函数。\n这些基函数的二阶导数是：\n$$ \\{0, 0, 2, 6x, 6(x-0.2)_+, 6(x-0.5)_+, 6(x-0.8)_+ \\} $$\n惩罚矩阵 $\\mathbf{\\Omega}_{\\text{TP}}$ 是一个 $p_{\\text{TP}} \\times p_{\\text{TP}}$ 的矩阵，其中 $(\\mathbf{\\Omega}_{\\text{TP}})_{ij} \\approx \\int_0^1 b_i''(x)b_j''(x)dx$。请注意，$\\mathbf{\\Omega}_{\\text{TP}}$ 的前两行和前两列为零，这反映了惩罚不适用于二阶导数为零的函数（即线性函数），这些函数构成了惩罚的零空间。虽然概念上简单，但已知该基的数值不稳定，因为其基函数高度共线。\n\n**理论等价性与数值实现**\n由于两个基张成相同的函数空间，并且惩罚泛函 $\\mathcal{J}(f)$ 在该空间上是恒定的，因此平滑矩阵 $\\mathbf{S}_\\lambda$ 及其迹 $\\mathrm{df}(\\lambda)$ 理论上必须相同，无论选择哪个基。计算出的 $\\mathrm{df}(\\lambda)$ 值的任何差异都源于惩罚矩阵构建中的数值近似误差，以及更重要的，两个基的不同数值条件。问题要求比较有效自由度，旨在数值验证这种理论上的不变性，并突出基选择的实际后果。\n\n计算过程如下：\n1.  对于每个基（B样条和截断幂），构建设计矩阵（$\\mathbf{X}_{\\text{B}}$, $\\mathbf{X}_{\\text{TP}}$）和惩罚矩阵（$\\mathbf{\\Omega}_{\\text{B}}$, $\\mathbf{\\Omega}_{\\text{TP}}$）。\n2.  对于每个指定的 $\\lambda$ 值 $\\in \\{0, 0.1, 10, 10^6\\}$，使用公式 $\\mathrm{tr}(\\mathbf{X}^\\top\\mathbf{X}(\\mathbf{X}^\\top\\mathbf{X} + \\lambda\\mathbf{\\Omega})^{+})$ 计算有效自由度 $\\mathrm{df}_{\\text{B}}(\\lambda)$ 和 $\\mathrm{df}_{\\text{TP}}(\\lambda)$。\n3.  对于 $\\lambda \\in \\{0, 0.1, 10\\}$，计算绝对差 $|\\mathrm{df}_{\\text{B}}(\\lambda) - \\mathrm{df}_{\\text{TP}}(\\lambda)|$。\n4.  报告 $\\mathrm{df}_{\\text{B}}(10^6)$ 的值，以展示在极端惩罚下的行为。当 $\\lambda \\to \\infty$ 时，拟合被强制进入惩罚的零空间，对于二阶导数惩罚，这是线性多项式空间（维度为 $2$）。因此，我们期望 $\\mathrm{df}(\\lambda) \\to 2$。\n\n实现将利用 `numpy` 进行线性代数运算，并利用 `scipy.interpolate` 进行稳健的B样条计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import BSpline\nfrom scipy.linalg import pinv\n\ndef solve():\n    \"\"\"\n    Computes and compares effective degrees of freedom for cubic regression splines\n    using two different bases: B-splines and truncated power functions.\n    \"\"\"\n\n    # --- Problem Specifications ---\n    n = 80\n    interior_knots = np.array([0.2, 0.5, 0.8])\n    x_data = np.linspace(0, 1, n)\n    lambdas = [0, 0.1, 10, 10**6]\n\n    # Grid for numerical integration\n    G = 2001\n    z_grid = np.linspace(0, 1, G)\n    \n    # --- Helper Functions ---\n\n    def build_bspline_objects(knots, degree):\n        \"\"\"Builds B-spline basis functions and their second derivatives.\"\"\"\n        # Create open knot vector\n        knot_vector = np.concatenate(\n            ([0] * (degree + 1), knots, [1] * (degree + 1))\n        )\n        num_basis_funcs = len(knot_vector) - degree - 1\n        \n        basis_funcs = []\n        deriv2_funcs = []\n        for i in range(num_basis_funcs):\n            c = np.zeros(num_basis_funcs)\n            c[i] = 1.0\n            spl = BSpline(knot_vector, c, degree, extrapolate=False)\n            basis_funcs.append(spl)\n            deriv2_funcs.append(spl.derivative(nu=2))\n        return basis_funcs, deriv2_funcs\n\n    def build_truncated_power_basis_objects(knots, degree):\n        \"\"\"Creates callables for truncated power basis functions and their derivatives.\"\"\"\n        p = degree + 1\n        basis_funcs = []\n        deriv2_funcs = []\n\n        # Polynomial part\n        for d in range(p):\n            basis_funcs.append(lambda x, deg=d: x**deg)\n            if d  2:\n                deriv2_funcs.append(lambda x: np.zeros_like(x))\n            elif d == 2:\n                deriv2_funcs.append(lambda x: np.full_like(x, 2.0))\n            elif d == 3:\n                deriv2_funcs.append(lambda x: 6.0 * x)\n        \n        # Truncated power part\n        for k in knots:\n            basis_funcs.append(lambda x, knot=k, deg=degree: np.maximum(0, x - knot)**deg)\n            deriv2_funcs.append(lambda x, knot=k, deg=degree: deg * (deg - 1) * np.maximum(0, x - knot)**(deg-2))\n\n        return basis_funcs, deriv2_funcs\n\n    def build_design_matrix(basis_objects, x_points):\n        \"\"\"Builds the design matrix from basis functions.\"\"\"\n        p = len(basis_objects)\n        n_pts = len(x_points)\n        X = np.zeros((n_pts, p))\n        for j, func in enumerate(basis_objects):\n            X[:, j] = func(x_points)\n        return X\n\n    def build_penalty_matrix(deriv2_objects, grid):\n        \"\"\"Builds the penalty matrix Omega using numerical integration.\"\"\"\n        p = len(deriv2_objects)\n        Omega = np.zeros((p, p))\n        \n        # Pre-compute second derivatives on the grid\n        deriv2_vals = np.zeros((len(grid), p))\n        for j, func in enumerate(deriv2_objects):\n            deriv2_vals[:, j] = func(grid)\n\n        for i in range(p):\n            for j in range(i, p):\n                integrand = deriv2_vals[:, i] * deriv2_vals[:, j]\n                integral = np.trapz(integrand, grid)\n                Omega[i, j] = integral\n                if i != j:\n                    Omega[j, i] = integral\n        return Omega\n\n    def calculate_df(X, Omega, lam):\n        \"\"\"Calculates the effective degrees of freedom.\"\"\"\n        X_T_X = X.T @ X\n        M = X_T_X + lam * Omega\n        M_inv = pinv(M)\n        df_matrix = X_T_X @ M_inv\n        return np.trace(df_matrix)\n\n    # --- Main Computations ---\n    \n    # Basis A: B-splines\n    bspline_basis, bspline_deriv2 = build_bspline_objects(interior_knots, degree=3)\n    X_B = build_design_matrix(bspline_basis, x_data)\n    Omega_B = build_penalty_matrix(bspline_deriv2, z_grid)\n\n    # Basis B: Truncated Power Basis\n    tpb_basis, tpb_deriv2 = build_truncated_power_basis_objects(interior_knots, degree=3)\n    X_TP = build_design_matrix(tpb_basis, x_data)\n    Omega_TP = build_penalty_matrix(tpb_deriv2, z_grid)\n    \n    dfs_B = {lam: calculate_df(X_B, Omega_B, lam) for lam in lambdas}\n    dfs_TP = {lam: calculate_df(X_TP, Omega_TP, lam) for lam in lambdas}\n\n    # --- Format Results ---\n    \n    # Absolute differences for lambda = 0, 0.1, 10\n    diff_0 = abs(dfs_B[0] - dfs_TP[0])\n    diff_0_1 = abs(dfs_B[0.1] - dfs_TP[0.1])\n    diff_10 = abs(dfs_B[10] - dfs_TP[10])\n    \n    # df for B-spline at lambda = 10^6\n    df_B_1e6 = dfs_B[10**6]\n\n    results = [diff_0, diff_0_1, diff_10, df_B_1e6]\n    formatted_results = [f\"{v:.6f}\" for v in results]\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "选择了基函数后，样条建模中最具挑战性的艺术性决策便是“节点”的安放位置。节点的疏密决定了模型在不同区域的灵活性。此练习  让你通过一个经典的挑战——逼近函数 $f(x)=1/x$ 来掌握节点布局的精髓。你将直观地看到，在函数曲率变化剧烈的区域（如 $x$ 接近零时）增加节点密度，能够如何显著地提升拟合精度，从而将抽象的理论转化为可操作的建模直觉。",
            "id": "3168964",
            "problem": "你需要实现并评估三次回归样条，用于在远离奇点 $x=0$ 的截断定义域上逼近反函数。目标函数为 $f(x)=1/x$，但评估范围被限制在截断定义域 $x\\in[\\tau,1]$ 内，其中 $\\tau\\in(0,1)$ 是一个给定的截断参数。该逼近必须使用通过截断幂基（truncated power basis）定义的三次回归样条，并使用一组指定的内部节点（internal knots）。目标是：从最小二乘逼近的第一性原理出发构建样条；研究在 $x=\\tau$ 附近节点布局对处理 $f(x)$ 陡峭行为的影响；并在一个密集的评估网格上量化逼近误差。\n\n使用的基本原理：\n- 普通最小二乘法（OLS）：给定一组基函数 $\\{\\phi_{j}(x)\\}_{j=1}^{p}$、数据 $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ 和一个线性模型 $g(x)=\\sum_{j=1}^{p}\\beta_{j}\\phi_{j}(x)$，OLS 估计量通过最小化 $\\sum_{i=1}^{n}(y_{i}-g(x_{i}))^{2}$ 来求得。\n- 回归样条基：一个具有内部节点 $\\{t_{1},\\dots,t_{K}\\}$ 的三次回归样条可以使用截断幂基 $\\{1,x,x^{2},x^{3},(x-t_{1})_{+}^{3},\\dots,(x-t_{K})_{+}^{3}\\}$ 来表示，其中 $(u)_{+}=\\max\\{u,0\\}$。\n\n实现要求：\n- 使用截断幂基函数 $\\phi_{0}(x)=1, \\phi_{1}(x)=x, \\phi_{2}(x)=x^{2}, \\phi_{3}(x)=x^{3}$，以及对每个内部节点 $t_{j}$，使用 $\\phi_{3+j}(x)=(x-t_{j})_{+}^{3}$。\n- 对于给定的 $\\tau$ 和一组严格位于 $[\\tau,1]$ 内部的内部节点 $\\{t_{j}\\}$，在 $[\\tau,1]$ 上均匀采样的 $n$ 个训练点 $\\{x_{i}\\}$ 上构建一个设计矩阵 $X\\in\\mathbb{R}^{n\\times p}$。令 $y_{i}=f(x_{i})=1/x_{i}$。计算最小化 $\\|X\\beta-y\\|_{2}^{2}$ 的 OLS 解 $\\hat{\\beta}$。\n- 使用拟合后的模型 $\\hat{g}(x)=\\sum_{j}\\hat{\\beta}_{j}\\phi_{j}(x)$，在 $[\\tau,1]$ 上均匀采样的密集评估网格 $\\{x^{\\ast}_{\\ell}\\}$ 上计算误差。\n- 报告评估网格上的两个误差指标：均方根误差 $\\mathrm{RMSE}=\\sqrt{\\frac{1}{m}\\sum_{\\ell=1}^{m}\\big(\\hat{g}(x^{\\ast}_{\\ell})-f(x^{\\ast}_{\\ell})\\big)^{2}}$ 和最大绝对误差 $\\mathrm{MAX}=\\max_{\\ell}|\\hat{g}(x^{\\ast}_{\\ell})-f(x^{\\ast}_{\\ell})|$。\n\n需要比较的节点布局方案：\n- 均匀内部节点：对于 $K$ 个节点，使用 $t_{j}=\\tau+(1-\\tau)\\cdot\\frac{j}{K+1}$，其中 $j=1,\\dots,K$。\n- 在 $\\tau$ 附近几何集中：对于 $K$ 个节点和指数 $q1$，使用 $t_{j}=\\tau+(1-\\tau)\\cdot\\left(\\frac{j}{K+1}\\right)^{q}$，其中 $j=1,\\dots,K$。这会增加 $x=\\tau$ 附近节点的密度。\n\n需强制执行的数值细节：\n- 所有情况均使用 $n=N_{\\text{train}}=2000$ 个在 $[\\tau,1]$ 上均匀分布的训练点。\n- 所有情况均使用 $m=N_{\\text{test}}=10000$ 个在 $[\\tau,1]$ 上均匀分布的评估点。\n- 所有计算均无单位；不涉及物理单位。\n- 最终报告的数字应四舍五入到 $6$ 位小数。\n\n测试套件：\n- 情况 1 (理想情况): $\\tau=0.1$, $K=6$, 均匀节点。\n- 情况 2 (相同的 $\\tau$ 但节点集中): $\\tau=0.1$, $K=6$, 几何节点, $q=3$。\n- 情况 3 (更强的截断压力): $\\tau=0.01$, $K=12$, 均匀节点。\n- 情况 4 (更强截断且节点集中): $\\tau=0.01$, $K=12$, 几何节点, $q=3$。\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例贡献一个包含两个元素 $[\\mathrm{RMSE},\\mathrm{MAX}]$ 的列表，并四舍五入到 6 位小数。例如，输出应类似于 $[[a_{1},b_{1}],[a_{2},b_{2}],[a_{3},b_{3}],[a_{4},b_{4}]]$，其中每个 $a_{i}$ 和 $b_{i}$ 是浮点数，保留 6 位小数，分别对应上述顺序中第 $i$ 个用例的 $\\mathrm{RMSE}$ 和 $\\mathrm{MAX}$。\n\n你的任务是编写一个完整、可运行的程序，实现此规范并以所要求的确切格式打印单行输出。不允许读取任何输入；所有参数均根据上述测试套件进行硬编码。",
            "solution": "该问题要求实现并评估一个三次回归样条，用以在截断域 $[\\tau, 1]$（其中 $\\tau \\in (0,1)$）上逼近函数 $f(x) = 1/x$。该逼近是使用普通最小二乘法（OLS）和由截断幂函数定义的基来构建的。\n\n三次回归样条模型，记为 $\\hat{g}(x)$，是一个分段三次多项式，它在一组 $K$ 个内部节点 $\\{t_j\\}_{j=1}^K$ 处连续，并且具有连续的一阶和二阶导数。这样的函数可以表示为基函数的线性组合。对于此问题，我们采用截断幂基。模型形式如下：\n$$\n\\hat{g}(x) = \\beta_0 \\phi_0(x) + \\beta_1 \\phi_1(x) + \\beta_2 \\phi_2(x) + \\beta_3 \\phi_3(x) + \\sum_{j=1}^{K} \\beta_{3+j} \\phi_{3+j}(x)\n$$\n其中基函数为 $\\phi_0(x) = 1$, $\\phi_1(x) = x$, $\\phi_2(x) = x^2$, $\\phi_3(x) = x^3$，以及对每个内部节点 $t_j$，有 $\\phi_{3+j}(x) = (x-t_j)_{+}^{3}$。函数 $(u)_{+} = \\max(u, 0)$ 是正部函数。该公式定义了一个共有 $p = 4+K$ 个参数的线性模型，这些参数是系数 $\\beta = (\\beta_0, \\beta_1, \\dots, \\beta_{3+K})^T$。\n\n为了确定系数向量 $\\hat{\\beta}$，我们利用一组从定义域 $[\\tau, 1]$ 上均匀采样的 $n=N_{\\text{train}}$ 个训练点 $\\{x_i\\}_{i=1}^n$。相应的响应值取自目标函数，$y_i = f(x_i) = 1/x_i$。OLS 方法找到最小化残差平方和（SSR）的系数向量 $\\hat{\\beta}$：\n$$\n\\text{SSR}(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{g}(x_i))^2 = \\sum_{i=1}^{n} \\left(y_i - \\sum_{j=0}^{p-1} \\beta_j \\phi_j(x_i)\\right)^2\n$$\n这个最小化问题可以用矩阵表示法简洁地表达。设 $y \\in \\mathbb{R}^n$ 是观测响应向量，其中 $y_i = 1/x_i$。设 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，其元素为 $X_{i,j} = \\phi_j(x_i)$。OLS 问题是找到向量 $\\beta$ 以最小化残差向量的欧几里得范数平方：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\|y - X\\beta\\|_2^2\n$$\n这个标准线性最小二乘问题的解由正规方程给出，$X^T X \\hat{\\beta} = X^T y$。假设矩阵 $X^T X$ 可逆（如果 $n \\ge p$ 且基函数在训练点集上线性无关，则该条件成立），则唯一解为：\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\n为了数值稳定性，特别是当 $X$ 是病态（ill-conditioned）的时候，最好使用 QR 分解或奇异值分解（SVD）等方法来计算此解，这些方法已在标准数值库中实现。\n\n一旦找到最优系数向量 $\\hat{\\beta}$，拟合的样条模型 $\\hat{g}(x) = \\sum_j \\hat{\\beta}_j \\phi_j(x)$ 就完全确定了。其准确性将在一个独立的、密集的包含 $m=N_{\\text{test}}$ 个评估点 $\\{x^*_\\ell\\}_{\\ell=1}^m$ 的网格上进行评估，该网格上的点也在 $[\\tau, 1]$ 上均匀分布。模型在此网格上的预测值为 $\\hat{y}^*_\\ell = \\hat{g}(x^*_\\ell)$。性能通过两个误差指标来量化：\n\n1.  均方根误差（RMSE），它衡量逼近误差的平均大小：\n    $$\n    \\text{RMSE} = \\sqrt{\\frac{1}{m} \\sum_{\\ell=1}^{m} (\\hat{y}^*_\\ell - f(x^*_\\ell))^2}\n    $$\n2.  最大绝对误差（MAX），它衡量评估网格上的最坏情况误差：\n    $$\n    \\text{MAX} = \\max_{1 \\le \\ell \\le m} |\\hat{y}^*_\\ell - f(x^*_\\ell)|\n    $$\n\n节点的放置对于样条的灵活性和逼近能力至关重要。问题指定了两种在区间 $(\\tau, 1)$ 内放置 $K$ 个内部节点的策略：\n\n-   **均匀节点**：节点在定义域内均匀分布，提供统一的灵活性。\n    $$\n    t_j = \\tau + (1-\\tau) \\cdot \\frac{j}{K+1}, \\quad j=1, \\dots, K\n    $$\n-   **几何集中节点**：通过应用指数 $q1$，将节点集中在左边界 $x=\\tau$ 附近。该策略将更多的模型灵活性分配给函数 $f(x)=1/x$ 最陡峭、变化最快的区域。\n    $$\n    t_j = \\tau + (1-\\tau) \\cdot \\left(\\frac{j}{K+1}\\right)^q, \\quad j=1, \\dots, K\n    $$\n\n实现过程将构建一个函数，用于为任何给定的点集 $x$ 和节点集 $\\{t_j\\}$ 生成设计矩阵。该函数将用于创建训练矩阵 $X_{\\text{train}}$ 和评估矩阵 $X_{\\text{test}}$。OLS 解 $\\hat{\\beta}$ 是使用 $X_{\\text{train}}$ 和相应的 $y_{\\text{train}}$ 计算得出的。随后，将 $\\hat{\\beta}$ 应用于 $X_{\\text{test}}$ 以生成预测，并为问题陈述中指定的每个测试用例计算 RMSE 和 MAX 误差指标。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates cubic regression splines for approximating f(x)=1/x\n    on a truncated domain, comparing different knot placement strategies.\n    \"\"\"\n    # Define fixed numerical details from the problem statement.\n    N_train = 2000\n    N_test = 10000\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'tau': 0.1, 'K': 6, 'knot_type': 'uniform', 'q': None},\n        {'tau': 0.1, 'K': 6, 'knot_type': 'geometric', 'q': 3},\n        {'tau': 0.01, 'K': 12, 'knot_type': 'uniform', 'q': None},\n        {'tau': 0.01, 'K': 12, 'knot_type': 'geometric', 'q': 3},\n    ]\n\n    results = []\n\n    def target_function(x):\n        \"\"\"The function to be approximated, f(x) = 1/x.\"\"\"\n        return 1.0 / x\n\n    def generate_knots(tau, K, knot_type, q):\n        \"\"\"Generates internal knots based on the specified scheme.\"\"\"\n        if K == 0:\n            return np.array([])\n        \n        # Proportions for knot placement in the (0, 1) interval\n        proportions = np.arange(1, K + 1) / (K + 1.0)\n        \n        if knot_type == 'geometric':\n            # Concentrate knots near the start of the interval\n            proportions = proportions**q\n            \n        # Scale and shift proportions to the [tau, 1] domain\n        return tau + (1.0 - tau) * proportions\n\n    def construct_design_matrix(x, knots):\n        \"\"\"Constructs the design matrix using the truncated power basis.\"\"\"\n        # Ensure x is a column vector for broadcasting purposes\n        x_col = x.reshape(-1, 1)\n        \n        # Polynomial basis functions: 1, x, x^2, x^3\n        num_poly_basis = 4\n        poly_basis = np.hstack([x_col**i for i in range(num_poly_basis)])\n        \n        if knots.size == 0:\n            return poly_basis\n            \n        # Truncated power basis functions: (x - t_j)_+^3\n        # Broadcasting x_col (n, 1) with knots (1, K) gives a (n, K) matrix\n        knots_row = knots.reshape(1, -1)\n        diff = x_col - knots_row\n        trunc_basis = np.maximum(0, diff)**3\n        \n        # Combine polynomial and truncated power basis functions\n        return np.hstack([poly_basis, trunc_basis])\n\n    for case in test_cases:\n        tau = case['tau']\n        K = case['K']\n        knot_type = case['knot_type']\n        q = case['q']\n\n        # 1. Generate training data points and response values\n        x_train = np.linspace(tau, 1, N_train)\n        y_train = target_function(x_train)\n        \n        # 2. Generate test grid and true function values\n        x_test = np.linspace(tau, 1, N_test)\n        y_test = target_function(x_test)\n\n        # 3. Generate knots for the current case\n        knots = generate_knots(tau, K, knot_type, q)\n\n        # 4. Construct the training design matrix and solve for coefficients\n        X_train = construct_design_matrix(x_train, knots)\n        # Use numpy's least squares solver for numerical stability\n        beta_hat, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n\n        # 5. Evaluate the fitted spline model on the test grid\n        X_test = construct_design_matrix(x_test, knots)\n        y_pred = X_test @ beta_hat\n        \n        # 6. Calculate error metrics\n        errors = y_pred - y_test\n        rmse = np.sqrt(np.mean(errors**2))\n        max_abs_error = np.max(np.abs(errors))\n        \n        results.append([rmse, max_abs_error])\n\n    # 7. Format and print the final output in the exact required format.\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "手动选择节点位置虽然能培养直觉，但在复杂问题中可能变得乏味且主观。有没有一种更自动化的科学方法呢？此练习  将引导你进入一个前沿领域：利用稀疏学习进行节点选择。通过对一个包含大量候选节点的样条模型施加 $L_1$ 惩罚（类似于 LASSO），你可以迫使模型自动“修剪”掉不重要的节点，只保留对数据拟合贡献最大的那些。这项实践不仅解决了节点选择的难题，也为你揭示了样条与现代稀疏建模方法之间的深刻联系。",
            "id": "3168944",
            "problem": "考虑通过截断幂基构建的三次回归样条。对于一个单变量预测变量 $x \\in [0,1]$，通过拼接多项式部分和截断部分来定义设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$，具体如下：前四列是多项式基 $1$、$x$、$x^2$、$x^3$，对于一组候选节点 $\\{\\xi_k\\}_{k=1}^K$，其余的 $K$ 列是 $(x - \\xi_k)_+^3$，其中 $(t)_+ = \\max(t,0)$。给定一个响应向量 $\\mathbf{y} \\in \\mathbb{R}^n$ 和系数 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$，考虑一个仅惩罚截断基系数的优化问题：\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^p} \\ \\left\\| \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right\\|_2^2 \\ + \\ \\lambda \\left\\| \\boldsymbol{\\beta}_{\\text{trunc}} \\right\\|_1,\n$$\n其中 $\\boldsymbol{\\beta}_{\\text{trunc}}$ 表示与截断基列相关联的系数向量，$\\lambda \\ge 0$ 是一个调节参数。多项式部分的系数不被惩罚。\n\n你的任务是实现一个程序，该程序：\n- 使用具有已知活跃节点的立方样条构建一个合成数据集，添加高斯噪声，并建立截断幂基设计矩阵。\n- 基于次梯度微积分和最小二乘法的正规方程，使用坐标级优化方法，从第一性原理出发解决上述优化问题，不依赖于预封装的求解器。\n- 对于每个指定的惩罚参数 $\\lambda$ 值，根据一个数值阈值 $\\tau$，确定哪些候选节点在优化后仍然保持活跃，即相应的截断基系数为非零。\n\n数据生成和基设定：\n- 使用固定的伪随机种子 $42$，从 $[0,1]$ 上独立且均匀地生成 $n = 200$ 个点 $x_i$。\n- 定义真实的回归函数\n$$\nf(x) \\ = \\ 0.5 \\ + \\ 2x \\ - \\ x^2 \\ + \\ 1.5 \\, (x - 0.3)_+^3 \\ - \\ 1.0 \\, (x - 0.7)_+^3.\n$$\n- 生成响应 $y_i = f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立的高斯噪声，$\\sigma = 0.05$。\n- 使用候选节点集 $\\{\\xi_k\\}_{k=1}^K$，其中 $K = 6$ 且 $\\xi_k \\in \\{0.2, 0.3, 0.4, 0.5, 0.6, 0.7\\}$。\n\n稀疏性和剪枝准则：\n- 求解出 $\\boldsymbol{\\beta}$ 后，如果一个截断基系数 $\\beta_j$ 满足 $|\\beta_j| \\ge \\tau$ 且 $\\tau = 10^{-6}$，则声明其为活跃。\n- 报告每个测试用例中活跃截断基函数在候选节点列表中的索引（从零开始）。\n\n测试套件：\n- 针对以下惩罚值求解优化问题 $\\lambda \\in \\{\\ 0, \\ 0.1, \\ 1.0, \\ 100.0 \\ \\}$, 其中包括一个边界情况（$\\lambda = 0$）、一个小惩罚、一个中等惩罚和一个强烈鼓励稀疏性的大惩罚。\n\n输出规范：\n- 你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个条目对应于测试套件中的一个 $\\lambda$ 值，顺序相同，其本身是一个整数列表，表示活跃节点在候选集中的从零开始的索引。例如，一个有四个测试用例的输出应如下所示：$[\\,[i_{1,1},i_{1,2},\\dots],\\,[i_{2,1},\\dots],\\,[\\dots],\\,[\\dots]\\,]$。",
            "solution": "我们从标准的线性回归设置开始，给定一个固定的设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $\\mathbf{y} \\in \\mathbb{R}^n$。目标是最小化关于 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ 的一个带惩罚的目标函数：\n$$\nJ(\\boldsymbol{\\beta}) \\ = \\ \\left\\| \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right\\|_2^2 \\ + \\ \\lambda \\left\\| \\boldsymbol{\\beta}_{\\text{trunc}} \\right\\|_1,\n$$\n其中只有截断基的系数被惩罚。多项式部分（对应于 $1$、$x$、$x^2$、$x^3$ 的前四列）保持不被惩罚。三次回归样条的截断幂基使用函数 $(x - \\xi_k)_+^3$ 作为候选节点 $\\xi_k$；众所周知，这个基能够张成在指定位置有节点的三次样条空间。\n\n为了从第一性原理推导出一个算法，我们使用两个基本工具：\n- 针对固定系数子集的最小二乘回归的正规方程：当固定其他系数时，对单一坐标 $\\beta_j$ 最小化 $\\left\\| \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right\\|_2^2$ 的问题简化为一个一维二次最小化问题。\n- 绝对值惩罚的次梯度微积分。对于一个标量参数 $\\beta_j$，$|\\beta_j|$ 的次梯度是 $\\partial |\\beta_j| = \\{\\ \\text{sgn}(\\beta_j)\\ \\}$ 如果 $\\beta_j \\ne 0$，在 $\\beta_j = 0$ 时是区间 $[-1,1]$。\n\n考虑坐标级优化。固定除 $\\beta_j$ 之外的所有系数，并定义偏残差\n$$\n\\mathbf{r}_j \\ = \\ \\mathbf{y} \\ - \\sum_{k \\ne j} \\mathbf{X}_{\\cdot k} \\beta_k \\ = \\ \\mathbf{y} \\ - \\mathbf{X}\\boldsymbol{\\beta} \\ + \\ \\mathbf{X}_{\\cdot j} \\beta_j,\n$$\n其中 $\\mathbf{X}_{\\cdot j}$ 是 $\\mathbf{X}$ 的第 $j$ 列。限制在 $\\beta_j$ 上的目标函数变为\n$$\n\\phi_j(\\beta_j) \\ = \\ \\left\\| \\mathbf{r}_j \\ - \\ \\mathbf{X}_{\\cdot j} \\beta_j \\right\\|_2^2 \\ + \\ \\lambda \\, p_j \\, |\\beta_j|,\n$$\n其中 $p_j \\in \\{0,1\\}$ 指示坐标 $j$ 是否被惩罚（对于截断部分 $p_j=1$，对于多项式部分 $p_j=0$）。展开平方项，\n$$\n\\left\\| \\mathbf{r}_j - \\mathbf{X}_{\\cdot j} \\beta_j \\right\\|_2^2 \\ = \\ \\mathbf{r}_j^\\top \\mathbf{r}_j \\ - \\ 2 \\beta_j \\, \\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j \\ + \\ \\beta_j^2 \\, \\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j}.\n$$\n对于 $\\beta_j \\ne 0$ 的次梯度最优性条件是\n$$\n\\frac{\\partial}{\\partial \\beta_j} \\left\\| \\mathbf{r}_j - \\mathbf{X}_{\\cdot j} \\beta_j \\right\\|_2^2 \\ + \\ \\lambda \\, p_j \\, \\text{sgn}(\\beta_j) \\ = \\ 0,\n$$\n这得到\n$$\n- 2 \\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j \\ + \\ 2 \\left( \\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j} \\right) \\beta_j \\ + \\ \\lambda \\, p_j \\, \\text{sgn}(\\beta_j) \\ = \\ 0.\n$$\n整理后，\n$$\n2 \\left( \\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j} \\right) \\beta_j \\ = \\ 2 \\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j \\ - \\ \\lambda \\, p_j \\, \\text{sgn}(\\beta_j).\n$$\n这导出了软阈值解\n$$\n\\beta_j^\\star \\ = \\ \\frac{S\\left( \\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j, \\ \\frac{\\lambda \\, p_j}{2} \\right)}{\\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j}},\n$$\n其中 $S(a,t)$ 是软阈值算子 $S(a,t) = \\text{sgn}(a)\\,\\max\\left(|a| - t, \\ 0\\right)$。当 $p_j = 0$（未惩罚的坐标）时，阈值 $t$ 为 $0$，我们恢复了最小二乘更新\n$$\n\\beta_j^\\star \\ = \\ \\frac{\\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j}{\\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j}}.\n$$\n当 $\\beta_j^\\star = 0$ 时，只要 $|\\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j| \\le \\frac{\\lambda p_j}{2}$，在零点的次梯度条件就得到满足，这解释了增加 $\\lambda$ 如何促进被惩罚坐标的稀疏性。\n\n算法设计：\n- 将 $\\boldsymbol{\\beta}$ 初始化为零向量。\n- 遍历坐标 $j = 1, \\dots, p$。对于每个坐标，计算 $\\mathbf{r}_j = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{X}_{\\cdot j}\\beta_j$，评估 $\\mathbf{X}_{\\cdot j}^\\top \\mathbf{r}_j$ 和 $\\mathbf{X}_{\\cdot j}^\\top \\mathbf{X}_{\\cdot j}$，并使用上面的软阈值表达式更新 $\\beta_j$。\n- 重复遍历直到收敛，例如，当 $\\boldsymbol{\\beta}$ 的最大绝对变化量低于一个容忍度 $\\varepsilon$ 时。\n\n这种坐标下降法直接从正规方程和次梯度微积分推导而来，体现了仅应用于截断部分的最小绝对收缩和选择算子（LASSO）原理。收敛后，我们通过检查截断系数的量级与数值阈值 $\\tau$ 的大小来识别活跃节点。\n\n合成数据构建：\n- 抽取 $x_i \\sim \\text{Uniform}([0,1])$，其中 $n = 200$ 且种子为 $42$。\n- 定义真实函数 $f(x) = 0.5 + 2x - x^2 + 1.5(x - 0.3)_+^3 - 1.0(x - 0.7)_+^3$。\n- 生成 $y_i = f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma = 0.05$。\n- 从多项式列和对应于节点 $\\xi_k \\in \\{0.2, 0.3, 0.4, 0.5, 0.6, 0.7\\}$ 的截断列构建 $\\mathbf{X}$。\n\n测试套件和预期行为：\n- 对于 $\\lambda = 0$，解退化为普通最小二乘法，通常导致许多非零的截断系数，因为没有诱导稀疏性的惩罚；一些系数可能由于多重共线性而很小，但没有惩罚意味着没有刻意的剪枝。\n- 对于 $\\lambda = 0.1$ 和 $\\lambda = 1.0$，软阈值化逐渐将较小的截断系数置为零，鼓励稀疏性并剪除在数据中支持较弱的节点。\n- 对于 $\\lambda = 100.0$，阈值 $\\lambda/2 = 50.0$ 非常大，以至于截断系数被强烈地推向零，留下一个纯粹的多项式拟合（所有节点都被剪除）。\n\n最后，对于测试套件中的每个 $\\lambda$，我们输出候选节点列表中相应截断系数为活跃（$|\\beta_j| \\ge \\tau$）的从零开始的索引列表。单行输出简洁地总结了不同惩罚水平下的稀疏模式和节点剪枝情况。",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(a, t):\n    if a > t:\n        return a - t\n    elif a  -t:\n        return a + t\n    else:\n        return 0.0\n\ndef coordinate_descent_partial_l1(X, y, penalty_mask, lam, max_iter=1000, tol=1e-8):\n    \"\"\"\n    Solve min ||y - X beta||_2^2 + lam * ||beta_trunc||_1\n    where penalty_mask is 1 for penalized coordinates (truncated basis) and 0 for unpenalized (polynomial part).\n    Uses coordinate descent with soft-thresholding derived from subgradient conditions.\n    \"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n    # Precompute column norms (X^T X diagonal)\n    col_sq_norms = (X ** 2).sum(axis=0)\n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        for j in range(p):\n            # Partial residual r_j = y - X beta + X_j * beta_j\n            r_j = y - X @ beta + X[:, j] * beta[j]\n            z = X[:, j].dot(r_j)\n            Hjj = col_sq_norms[j]\n            t = (lam / 2.0) * penalty_mask[j]\n            # Update coordinate with soft-thresholding\n            beta[j] = soft_threshold(z, t) / (Hjj if Hjj > 0 else 1.0)\n        if np.max(np.abs(beta - beta_old))  tol:\n            break\n    return beta\n\ndef build_design_matrix(x, knots):\n    \"\"\"\n    Build cubic truncated power basis: [1, x, x^2, x^3, (x - xi)_+^3 for xi in knots]\n    \"\"\"\n    n = x.shape[0]\n    poly = np.stack([np.ones(n), x, x**2, x**3], axis=1)\n    trunc = np.stack([np.maximum(x - xi, 0.0)**3 for xi in knots], axis=1) if len(knots) > 0 else np.empty((n, 0))\n    X = np.concatenate([poly, trunc], axis=1)\n    return X\n\ndef generate_data(n=200, seed=42, sigma=0.05):\n    rng = np.random.RandomState(seed)\n    x = rng.uniform(0.0, 1.0, size=n)\n    f = 0.5 + 2.0*x - 1.0*(x**2) + 1.5*np.maximum(x - 0.3, 0.0)**3 - 1.0*np.maximum(x - 0.7, 0.0)**3\n    y = f + rng.normal(0.0, sigma, size=n)\n    return x, y\n\ndef active_knots_indices(beta, num_poly, tau=1e-6):\n    \"\"\"\n    Return zero-based indices of active truncated coefficients relative to the knot list.\n    \"\"\"\n    trunc_beta = beta[num_poly:]\n    return [i for i, b in enumerate(trunc_beta) if abs(b) >= tau]\n\ndef solve():\n    # Parameters\n    n = 200\n    seed = 42\n    sigma = 0.05\n    knots = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n    num_poly = 4\n    lambdas = [0.0, 0.1, 1.0, 100.0]\n    tau = 1e-6\n\n    # Generate data and design matrix\n    x, y = generate_data(n=n, seed=seed, sigma=sigma)\n    X = build_design_matrix(x, knots)\n\n    # Penalty mask: 0 for polynomial part, 1 for truncated part\n    penalty_mask = np.array([0]*num_poly + [1]*len(knots), dtype=float)\n\n    results = []\n    for lam in lambdas:\n        beta = coordinate_descent_partial_l1(X, y, penalty_mask, lam, max_iter=2000, tol=1e-9)\n        active = active_knots_indices(beta, num_poly=num_poly, tau=tau)\n        results.append(active)\n\n    # Print in the required single-line format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}