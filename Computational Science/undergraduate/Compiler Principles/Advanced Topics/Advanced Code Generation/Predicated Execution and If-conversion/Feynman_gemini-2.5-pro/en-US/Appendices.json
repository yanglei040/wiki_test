{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp if-conversion, we must move from theory to practice by analyzing its performance impact at the instruction level. This exercise challenges you to transform a control-dependent code sequence into a data-flow graph using min/max idioms, a common pattern recognized by compilers. By calculating the critical-path latency and counting micro-operations under different architectural assumptions , you will develop a quantitative understanding of how if-conversion can reduce execution time by eliminating costly branches and enabling further instruction-level optimizations.",
            "id": "3663804",
            "problem": "Consider the following scalar kernel that clamps and combines integer values, written with control-dependent updates:\n\nFor each index $i$,\n- Let $t \\leftarrow A[i] - B[i]$.\n- If $t > \\mathrm{hi}$ then $t \\leftarrow \\mathrm{hi}$.\n- If $t < \\mathrm{lo}$ then $t \\leftarrow \\mathrm{lo}$.\n- If $t < C[i]$ then $u \\leftarrow C[i]$ else $u \\leftarrow t$.\n- Let $v \\leftarrow u + D[i]$.\n- If $v < k$ then $R[i] \\leftarrow v$ else $R[i] \\leftarrow k$.\n\nAssume that all inputs $A[i]$, $B[i]$, $C[i]$, $D[i]$, $\\mathrm{lo}$, $\\mathrm{hi}$, and $k$ are available in registers at time $0$ (no memory latency), and that the machine is an ideal out-of-order core with sufficient issue width and resources so that only true data-dependence chains determine the latency to produce $R[i]$. The compiler targets a reduced instruction set computing (RISC)-like instruction set architecture and may perform if-conversion to eliminate control dependences using predicated execution.\n\nYou are given the following fundamental definitions and cost model for predication and idioms:\n- Predicated execution transforms each control-dependent assignment into a data-dependent operation using a predicate produced by a comparison, for example a conditional move (abbreviated as conditional move (cmov)) or a select.\n- The if-converted form can be expressed using the min and max algebraic idioms: $\\min(x, y)$ and $\\max(x, y)$, which are recognized by the compiler when the condition and the choice of operands match the idiom pattern.\n- Latencies (register-to-register) in cycles:\n  - Integer add or subtract: $L_{a} = 1$.\n  - Compare that produces a predicate: $L_{c} = 1$.\n  - Select or conditional move that consumes a predicate: $L_{s} = 2$.\n  - Fused min or max idiom as a single instruction when recognized: $L_{mm} = 1$.\n- Micro-operation (uop) accounting and fusion opportunities:\n  - Integer add or subtract: $1$ uop.\n  - Compare: $1$ uop.\n  - Select or conditional move: $1$ uop.\n  - Recognized min or max idiom: $1$ uop (this replaces the compare+select pair when applicable, reducing two uops to one).\n  - In the original branched code, each compare followed immediately by a conditional branch can macro-fuse into a single fused uop for fetch/issue purposes, but this does not change the true data-dependence chain latency. After if-conversion there are no branches, so branch macro-fusion does not apply.\n\nTasks:\n- Rewrite the kernel using min and max idioms (that is, express it with $\\min$ and $\\max$ rather than control flow), justifying correctness by the definitions of $\\min$ and $\\max$ and the semantics of predicated execution.\n- Under the cost model above, compute the critical-path data-dependence length (in cycles) to produce $R[i]$ for one iteration in each of the following two if-converted realizations:\n  - Realization $\\mathcal{S}$: implement each $\\min$ or $\\max$ using a compare plus select (no idiom recognition). The data-dependence contribution of each $\\min$ or $\\max$ is $L_{c} + L_{s}$.\n  - Realization $\\mathcal{M}$: implement each $\\min$ or $\\max$ as a recognized fused idiom instruction of latency $L_{mm}$ (idiom recognition enabled).\n  In both realizations, use $L_{a}$ for the add or subtract and assume all inputs are ready at time $0$.\n- Quantify the number of uops per iteration in realizations $\\mathcal{S}$ and $\\mathcal{M}$, and briefly explain how uop fusion opportunities differ from the original branched form.\n- Finally, report a single number: the best-case critical-path latency (in cycles) per iteration among $\\mathcal{S}$ and $\\mathcal{M}$ under the given latencies.\n\nExpress your final answer as an integer number of cycles, with no units or additional text inside the answer box.",
            "solution": "The scalar kernel processes each element $i$ through a sequence of operations involving conditional updates. To analyze its performance under if-conversion, we first transform the control-dependent logic into a data-flow representation using $\\min$ and $\\max$ functions. This transformation is based on the semantics of predicated execution, where conditional assignments are replaced by data-dependent selections.\n\nThe sequence of operations for an index $i$ is:\n1.  Let $t \\leftarrow A[i] - B[i]$.\n2.  If $t > \\mathrm{hi}$ then $t \\leftarrow \\mathrm{hi}$.\n3.  If $t < \\mathrm{lo}$ then $t \\leftarrow \\mathrm{lo}$.\n4.  If $t < C[i]$ then $u \\leftarrow C[i]$ else $u \\leftarrow t$.\n5.  Let $v \\leftarrow u + D[i]$.\n6.  If $v < k$ then $R[i] \\leftarrow v$ else $R[i] \\leftarrow k$.\n\nLet's convert each conditional step into its equivalent `min`/`max` form:\n\n- Step $1$: A simple subtraction. Let's call the result $t_0$.\n    $t_0 \\leftarrow A[i] - B[i]$\n\n- Step $2$: \"If $t > \\mathrm{hi}$ then $t \\leftarrow \\mathrm{hi}$\" sets the value of $t$ to $\\mathrm{hi}$ if it exceeds $\\mathrm{hi}$, and leaves it unchanged otherwise. This is equivalent to taking the minimum of the current value and $\\mathrm{hi}$.\n    $t_1 \\leftarrow \\min(t_0, \\mathrm{hi})$\n\n- Step $3$: \"If $t < \\mathrm{lo}$ then $t \\leftarrow \\mathrm{lo}$\" sets the value of $t$ to $\\mathrm{lo}$ if it is less than $\\mathrm{lo}$, and leaves it unchanged otherwise. This is equivalent to taking the maximum of the current value and $\\mathrm{lo}$.\n    $t_2 \\leftarrow \\max(t_1, \\mathrm{lo})$\n    The combination of steps $2$ and $3$ effectively clamps the value $t_0$ to the range $[\\mathrm{lo}, \\mathrm{hi}]$.\n\n- Step $4$: \"If $t < C[i]$ then $u \\leftarrow C[i]$ else $u \\leftarrow t$\" selects the larger of the two values, $t$ and $C[i]$. This is the definition of the maximum function.\n    $u \\leftarrow \\max(t_2, C[i])$\n\n- Step $5$: A simple addition.\n    $v \\leftarrow u + D[i]$\n\n- Step $6$: \"If $v < k$ then $R[i] \\leftarrow v$ else $R[i] \\leftarrow k$\" selects the smaller of the two values, $v$ and $k$. This is the definition of the minimum function.\n    $R[i] \\leftarrow \\min(v, k)$\n\nThe entire computation can be expressed as a data-flow chain:\n$t_0 \\leftarrow A[i] - B[i]$\n$t_1 \\leftarrow \\min(t_0, \\mathrm{hi})$\n$t_2 \\leftarrow \\max(t_1, \\mathrm{lo})$\n$u \\leftarrow \\max(t_2, C[i])$\n$v \\leftarrow u + D[i]$\n$R[i] \\leftarrow \\min(v, k)$\n\nSince all inputs ($A[i]$, $B[i]$, $C[i]$, $D[i]$, $\\mathrm{lo}$, $\\mathrm{hi}$, $k$) are available at time $0$ and we assume an ideal out-of-order core, the critical path latency is determined by the longest true data-dependence chain. In this case, the computation forms a single linear chain of dependencies. The total latency is the sum of the latencies of each operation in this chain.\n\nThe latencies are given as:\n- Integer add or subtract: $L_{a} = 1$ cycle.\n- Compare: $L_{c} = 1$ cycle.\n- Select/conditional move: $L_{s} = 2$ cycles.\n- Fused min/max idiom: $L_{mm} = 1$ cycle.\n\nWe now compute the critical-path latency for the two specified realizations.\n\n**Realization $\\mathcal{S}$ (no idiom recognition)**: Each $\\min$ or $\\max$ operation is implemented as a `compare` followed by a `select`. The latency for such a sequence is $L_{c} + L_{s} = 1 + 2 = 3$ cycles.\n\n1.  $t_0$ is ready at time $T(t_0) = 0 + L_a = 1$.\n2.  $t_1$ depends on $t_0$. $T(t_1) = T(t_0) + (L_c + L_s) = 1 + 3 = 4$.\n3.  $t_2$ depends on $t_1$. $T(t_2) = T(t_1) + (L_c + L_s) = 4 + 3 = 7$.\n4.  $u$ depends on $t_2$. $T(u) = T(t_2) + (L_c + L_s) = 7 + 3 = 10$.\n5.  $v$ depends on $u$. $T(v) = T(u) + L_a = 10 + 1 = 11$.\n6.  $R[i]$ depends on $v$. $T(R[i]) = T(v) + (L_c + L_s) = 11 + 3 = 14$.\n\nThe critical-path latency for realization $\\mathcal{S}$ is $14$ cycles.\n\n**Realization $\\mathcal{M}$ (with idiom recognition)**: Each $\\min$ or $\\max$ operation is implemented as a single fused instruction with latency $L_{mm} = 1$ cycle.\n\n1.  $t_0$ is ready at time $T(t_0) = 0 + L_a = 1$.\n2.  $t_1$ depends on $t_0$. $T(t_1) = T(t_0) + L_{mm} = 1 + 1 = 2$.\n3.  $t_2$ depends on $t_1$. $T(t_2) = T(t_1) + L_{mm} = 2 + 1 = 3$.\n4.  $u$ depends on $t_2$. $T(u) = T(t_2) + L_{mm} = 3 + 1 = 4$.\n5.  $v$ depends on $u$. $T(v) = T(u) + L_{a} = 4 + 1 = 5$.\n6.  $R[i]$ depends on $v$. $T(R[i]) = T(v) + L_{mm} = 5 + 1 = 6$.\n\nThe critical-path latency for realization $\\mathcal{M}$ is $6$ cycles.\n\nNext, we quantify the number of micro-operations (uops) per iteration. The kernel consists of $1$ subtraction, $1$ addition, and $4$ min/max operations.\n\n- In realization $\\mathcal{S}$, each `min`/`max` is a `compare` ($1$ uop) plus a `select` ($1$ uop), totaling $2$ uops.\n  Number of uops for $\\mathcal{S} = 1_{\\text{sub}} + 4 \\times (1_{\\text{cmp}} + 1_{\\text{sel}}) + 1_{\\text{add}} = 1 + 4 \\times 2 + 1 = 10$ uops.\n\n- In realization $\\mathcal{M}$, each `min`/`max` is a single fused instruction ($1$ uop).\n  Number of uops for $\\mathcal{M} = 1_{\\text{sub}} + 4 \\times (1_{\\text{min/max}}) + 1_{\\text{add}} = 1 + 4 + 1 = 6$ uops.\n\nThe fusion opportunities differ significantly between the original branched code and the if-converted forms. In the original code, the primary fusion opportunity is *macro-fusion*, where a `compare` instruction and its subsequent `conditional branch` are fused at the front-end into a single uop for fetch and decode. This leads to a variable number of executed uops depending on the data-dependent execution path. In contrast, in the if-converted realization $\\mathcal{M}$, the opportunity is *micro-op fusion* (or idiom recognition during instruction selection). A `compare`-`select` pattern is replaced by a single, specialized `min` or `max` instruction. This fusion occurs for every such pattern, resulting in a fixed, and in this case, lower, total number of executed uops ($6$ uops for $\\mathcal{M}$ vs. $10$ for $\\mathcal{S}$).\n\nFinally, the best-case critical-path latency is the minimum of the latencies calculated for realizations $\\mathcal{S}$ and $\\mathcal{M}$.\nBest-case latency $= \\min(14, 6) = 6$ cycles.",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "Building on the fundamentals of performance analysis, we now apply if-conversion to a widespread and practical domain: accelerating code with Single Instruction Multiple Data (SIMD) vectorization. This problem simulates the optimization of an image thresholding loop, a task where unpredictable branches can severely hamper the performance of vectorized code. You will model the speedup gained by replacing a branchy scalar loop with a predicated SIMD implementation, exploring the trade-off between branch misprediction costs and the overhead of predication . This hands-on coding task demonstrates why if-conversion is an indispensable tool for high-performance computing.",
            "id": "3663829",
            "problem": "You are given a branchy scalar thresholding loop applied to an image represented as a one-dimensional array of $N$ pixels, where each pixel is a non-negative integer intensity. The scalar loop examines each pixel against a threshold $T$ and writes either $255$ or $0$ to the output depending on whether the threshold is met. The branchy scalar loop has the following form: for each index $i$, if $\\text{in}[i] \\ge T$ then $\\text{out}[i] \\leftarrow 255$ else $\\text{out}[i] \\leftarrow 0$. You are asked to port this computation to predicated execution using Single Instruction Multiple Data (SIMD) with vector length $VL$ (the number of elements processed per vector operation), and to compute the expected speedup of the predicated SIMD version over the branchy scalar version as a function of the threshold hit rate $r$ (the probability that a given pixel satisfies $\\text{in}[i] \\ge T$) and the vector length $VL$.\n\nThe fundamental base for this problem consists of the following well-tested facts and definitions:\n- The expected value of a random variable is the probability-weighted average of its possible values. If an event with probability $r$ incurs cost $P$, and otherwise cost $0$, then the expected additional cost is $r \\cdot P$.\n- In a simple cost model measured in processor cycles, an instruction has an associated cycle cost. Total cycles for a loop are the sum of the cycles of its constituent operations. For the per-element expected cost, divide by $N$ when aggregating over $N$ independent elements.\n- Predicated SIMD replaces branches with mask computations and selection operations that execute in lockstep for $VL$ elements, making the per-element cost the per-vector cost divided by $VL$.\n\nAssume the following cost model in cycles, which is scientifically plausible for a modern out-of-order superscalar processor:\n- For the branchy scalar loop, each element incurs a base cost equal to the sum of the load cost $c_{\\text{ld}}$, compare cost $c_{\\text{cmp}}$, store cost $c_{\\text{st}}$, and branch resolution overhead $c_{\\text{br}}$. In addition, a branch misprediction penalty $P$ is paid on an unexpected taken branch. Under a static \"predict not taken\" predictor model, a pixel that meets the threshold is treated as \"taken\" with probability $r$, and incurs the penalty $P$ in expectation. Therefore, the expected per-element cost of the scalar branchy loop is the base cost plus $r \\cdot P$.\n- For the predicated SIMD loop, for each vector of $VL$ elements, there is one vector load with cost $C_{\\text{vld}}$, one vector compare with cost $C_{\\text{vcmp}}$, one vector blend/select with cost $C_{\\text{vblend}}$, and one vector store with cost $C_{\\text{vst}}$. There is no data-dependent branch cost. The per-element cost of the SIMD version is the sum of these per-vector costs divided by $VL$.\n\nYour task is to:\n1. Derive the expected per-element cost of the branchy scalar loop as a function of $r$ using the above base facts. Do not use shortcut formulas beyond those implied by the base facts.\n2. Derive the per-element cost of the predicated SIMD loop as a function of $VL$ using the above base facts.\n3. Define the speedup $S(r, VL)$ as the ratio of the scalar expected per-element cost to the predicated SIMD per-element cost.\n4. Implement a program in the C language (standard C23) that computes $S(r, VL)$ for each test case in the test suite specified below, using the given constants. The program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each speedup value must be printed as a floating-point number rounded to six decimal places.\n\nConstants to use in your model:\n- Scalar costs per element: $c_{\\text{ld}} = 1$, $c_{\\text{cmp}} = 1$, $c_{\\text{st}} = 1$, $c_{\\text{br}} = 1$, and branch misprediction penalty $P = 15$.\n- Predicated SIMD costs per vector: $C_{\\text{vld}} = 3$, $C_{\\text{vcmp}} = 3$, $C_{\\text{vblend}} = 3$, $C_{\\text{vst}} = 3$.\n\nTest suite (each case is a pair $(r, VL)$):\n- Case $1$: $r = 0$, $VL = 4$.\n- Case $2$: $r = 0.5$, $VL = 4$.\n- Case $3$: $r = 1.0$, $VL = 4$.\n- Case $4$: $r = 0.1$, $VL = 8$.\n- Case $5$: $r = 0.9$, $VL = 8$.\n- Case $6$: $r = 0.5$, $VL = 1$.\n- Case $7$: $r = 0.5$, $VL = 16$.\n- Case $8$: $r = 0.2$, $VL = 16$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[x_1,x_2,\\dots,x_8]$, where each $x_i$ is the computed speedup for case $i$ rounded to six decimal places. No additional text should be printed.",
            "solution": "We first formalize the branchy scalar and predicated Single Instruction Multiple Data (SIMD) versions of the thresholding loop at the level of per-element and per-vector costs, using the stated base principles of expected value and instruction cost aggregation.\n\nFor the branchy scalar loop, consider one element. The operations involved, regardless of the branch outcome, are a load, a compare, a store, and resolving the branch instruction. Let the per-element cycle costs be $c_{\\text{ld}}$, $c_{\\text{cmp}}$, $c_{\\text{st}}$, and $c_{\\text{br}}$ respectively. Under a static \"predict not taken\" branch predictor, the branch is predicted as not taken by default. When the condition is true, the branch becomes taken and is mispredicted. If we denote the probability that an element meets the threshold as $r$ (the threshold hit rate), the event of misprediction occurs with probability $r$. Let the misprediction penalty be $P$ cycles. By the definition of expected value, the expected additional cost due to misprediction is $r \\cdot P$. Therefore, the expected per-element cost for the scalar branchy loop is\n$$\nC_{\\text{branch}}(r) = c_{\\text{ld}} + c_{\\text{cmp}} + c_{\\text{st}} + c_{\\text{br}} + r \\cdot P.\n$$\nUsing the provided constants $c_{\\text{ld}} = 1$, $c_{\\text{cmp}} = 1$, $c_{\\text{st}} = 1$, $c_{\\text{br}} = 1$, and $P = 15$, this simplifies numerically to\n$$\nC_{\\text{branch}}(r) = 1 + 1 + 1 + 1 + 15 r = 4 + 15 r.\n$$\n\nFor the predicated SIMD loop, predication replaces the branch with the computation of a mask and a selection (blend), operating across $VL$ elements at once. The per-vector operations are a vector load, vector compare, vector blend/select, and vector store. Let their per-vector cycle costs be $C_{\\text{vld}}$, $C_{\\text{vcmp}}$, $C_{\\text{vblend}}$, and $C_{\\text{vst}}$. Since these costs are not data-dependent under predication, the per-vector total cost is\n$$\nC_{\\text{SIMD,vector}} = C_{\\text{vld}} + C_{\\text{vcmp}} + C_{\\text{vblend}} + C_{\\text{vst}}.\n$$\nTo obtain the per-element cost, divide the per-vector cost by the number of elements processed per vector, $VL$, yielding\n$$\nC_{\\text{SIMD,elem}}(VL) = \\frac{C_{\\text{vld}} + C_{\\text{vcmp}} + C_{\\text{vblend}} + C_{\\text{vst}}}{VL}.\n$$\nWith the provided constants $C_{\\text{vld}} = 3$, $C_{\\text{vcmp}} = 3$, $C_{\\text{vblend}} = 3$, and $C_{\\text{vst}} = 3$, we obtain\n$$\nC_{\\text{SIMD,elem}}(VL) = \\frac{3 + 3 + 3 + 3}{VL} = \\frac{12}{VL}.\n$$\n\nDefine the speedup $S(r, VL)$ as the ratio of the scalar expected per-element cost to the predicated SIMD per-element cost, that is\n$$\nS(r, VL) = \\frac{C_{\\text{branch}}(r)}{C_{\\text{SIMD,elem}}(VL)}.\n$$\nSubstituting the expressions derived above,\n$$\nS(r, VL) = \\frac{4 + 15 r}{12 / VL} = \\left(4 + 15 r\\right) \\cdot \\frac{VL}{12}.\n$$\n\nAlgorithmically, the program must implement the following steps for each test case $(r, VL)$:\n1. Compute $C_{\\text{branch}}(r)$ using $C_{\\text{branch}}(r) = 4 + 15 r$.\n2. Compute $C_{\\text{SIMD,elem}}(VL)$ using $C_{\\text{SIMD,elem}}(VL) = 12 / VL$.\n3. Compute $S(r, VL)$ as $C_{\\text{branch}}(r) / C_{\\text{SIMD,elem}}(VL)$.\n4. Print the resulting $S(r, VL)$ rounded to six decimal places.\n\nWe now evaluate the specified test suite:\n- Case $1$: $r = 0$, $VL = 4$. $C_{\\text{branch}}(0) = 4 + 15 \\cdot 0 = 4$. $C_{\\text{SIMD,elem}}(4) = 12 / 4 = 3$. $S(0,4) = 4 / 3 \\approx 1.333333$.\n- Case $2$: $r = 0.5$, $VL = 4$. $C_{\\text{branch}}(0.5) = 4 + 15 \\cdot 0.5 = 11.5$. $C_{\\text{SIMD,elem}}(4) = 3$. $S(0.5,4) = 11.5 / 3 \\approx 3.833333$.\n- Case $3$: $r = 1.0$, $VL = 4$. $C_{\\text{branch}}(1.0) = 4 + 15 = 19$. $C_{\\text{SIMD,elem}}(4) = 3$. $S(1.0,4) = 19 / 3 \\approx 6.333333$.\n- Case $4$: $r = 0.1$, $VL = 8$. $C_{\\text{branch}}(0.1) = 4 + 1.5 = 5.5$. $C_{\\text{SIMD,elem}}(8) = 12 / 8 = 1.5$. $S(0.1,8) = 5.5 / 1.5 \\approx 3.666667$.\n- Case $5$: $r = 0.9$, $VL = 8$. $C_{\\text{branch}}(0.9) = 4 + 13.5 = 17.5$. $C_{\\text{SIMD,elem}}(8) = 1.5$. $S(0.9,8) = 17.5 / 1.5 \\approx 11.666667$.\n- Case $6$: $r = 0.5$, $VL = 1$. $C_{\\text{branch}}(0.5) = 11.5$. $C_{\\text{SIMD,elem}}(1) = 12$. $S(0.5,1) = 11.5 / 12 \\approx 0.958333$.\n- Case $7$: $r = 0.5$, $VL = 16$. $C_{\\text{SIMD,elem}}(16) = 12 / 16 = 0.75$. $S(0.5,16) = 11.5 / 0.75 \\approx 15.333333$.\n- Case $8$: $r = 0.2$, $VL = 16$. $C_{\\text{branch}}(0.2) = 4 + 3 = 7$. $C_{\\text{SIMD,elem}}(16) = 0.75$. $S(0.2,16) = 7 / 0.75 \\approx 9.333333$.\n\nThe program will compute these values and print them as a single bracketed, comma-separated list, rounded to six decimal places.",
            "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    double r;   // Threshold hit rate (probability that pixel >= T)\n    int VL;     // Vector length (number of elements per SIMD vector)\n} TestCase;\n\n// Compute expected scalar branchy per-element cost: C_branch(r) = 4 + 15*r\nstatic inline double scalar_cost(double r, double c_ld, double c_cmp, double c_st, double c_br, double mispred_penalty) {\n    return c_ld + c_cmp + c_st + c_br + r * mispred_penalty;\n}\n\n// Compute predicated SIMD per-element cost: C_SIMD_elem(VL) = (C_vld + C_vcmp + C_vblend + C_vst) / VL\nstatic inline double simd_cost_per_elem(int VL, double C_vld, double C_vcmp, double C_vblend, double C_vst) {\n    double per_vector = C_vld + C_vcmp + C_vblend + C_vst;\n    return per_vector / (double)VL;\n}\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        { 0.0, 4 },\n        { 0.5, 4 },\n        { 1.0, 4 },\n        { 0.1, 8 },\n        { 0.9, 8 },\n        { 0.5, 1 },\n        { 0.5, 16 },\n        { 0.2, 16 }\n    };\n\n    // Scalar cost constants (per element)\n    const double c_ld = 1.0;\n    const double c_cmp = 1.0;\n    const double c_st = 1.0;\n    const double c_br = 1.0;\n    const double mispred_penalty = 15.0;\n\n    // Predicated SIMD cost constants (per vector)\n    const double C_vld = 3.0;\n    const double C_vcmp = 3.0;\n    const double C_vblend = 3.0;\n    const double C_vst = 3.0;\n\n    // Calculate the number of test cases.\n    int num_cases = (int)(sizeof(test_cases) / sizeof(test_cases[0]));\n    double results[num_cases]; // Speedup results for each test case.\n\n    // Calculate the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        double r = test_cases[i].r;\n        int VL = test_cases[i].VL;\n\n        double c_branch = scalar_cost(r, c_ld, c_cmp, c_st, c_br, mispred_penalty);\n        double c_simd_elem = simd_cost_per_elem(VL, C_vld, C_vcmp, C_vblend, C_vst);\n\n        double speedup = c_branch / c_simd_elem;\n        results[i] = speedup;\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement\n    // Single line: comma-separated list enclosed in square brackets, each value rounded to six decimals.\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        if (i > 0) {\n            printf(\",\");\n        }\n        // Round to six decimal places\n        printf(\"%.6f\", results[i]);\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "While if-conversion is a powerful performance optimization, it is not universally applicable, and compilers must apply it with care to ensure program correctness. This practice problem shifts our focus from performance to safety, exploring a critical constraint: the interaction with undefined behavior (UB). By analyzing a code snippet where an untaken path contains a division-by-zero error, you will reason about the compiler's \"as-if\" rule and the architectural requirements, such as fault-suppressing predication, that make if-conversion a legal transformation . Understanding these limitations is essential for any programmer or compiler designer aiming to generate code that is both fast and correct.",
            "id": "3663865",
            "problem": "A compiler performing if-conversion replaces control dependence with data dependence using predicated execution. Consider the following C function, written in a style intended to avoid explicit numeric literals while still being realistic and well-defined on executions that do not evaluate undefined behavior (UB):\n\n\n```c\nint g(int *p, int r) {\n    if (p) {\n        return *p;\n    } else {\n        return r / (r - r);\n    }\n}\n```\n\n\nAssume a typical optimizing compiler for a language with undefined behavior (UB) semantics like C, and a target architecture that may or may not support fault-suppressing predicated instructions. The compiler wishes to if-convert the branch to improve instruction-level parallelism by making the two arms appear as one predicated block or a value-through-select. Your task is to reason from first principles (source language semantics and the as-if rule of program transformation, architectural predication semantics, and the definition of operations that may trap) about the legality and risk of such transformations when UB is present only on the untaken path for certain inputs. In particular, analyze the risk that a transformation which speculatively executes both arms could newly expose the UB path even when the original control flow would not execute it.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. On any target, it is semantics-preserving to rewrite the branch so that both arms are computed eagerly and the final result is chosen with a conditional move or select, because the unused result is discarded before it can affect the program.\n\nB. If the targetâ€™s predicated execution guarantees that a predicated instruction with a false predicate neither evaluates its operands nor raises exceptions, then guarding the division by the negation of the condition makes if-conversion semantics-preserving for inputs where the original program did not execute UB.\n\nC. Even if the untaken path contains UB, as long as the original control flow prevents its execution for the given inputs, the compiler must not introduce a transformation that makes that behavior observable on those inputs; the as-if rule requires preservation of behavior on defined executions.\n\nD. Because UB gives the compiler unrestricted freedom, it may assume that the denominator expression is never equal to $0$ and fold the division away in the original program, which makes if-conversion always safe regardless of whether the division would have been executed.",
            "solution": "The central issue is whether a compiler can transform the control flow of the given C function into a predicated, branchless sequence of instructions. The legality of this transformation hinges on the C language's \"as-if\" rule and the potential for introducing new, observable behavior on well-defined execution paths.\n\nThe original function `g` has two paths:\n1.  If `p` is not `NULL`, the function executes `return *p;`. This path has defined behavior (assuming `p` is a valid pointer).\n2.  If `p` is `NULL`, the function executes `return r / (r - r);`. This is equivalent to division by zero, which is Undefined Behavior (UB) in C.\n\nA naive if-conversion would speculatively execute both branches and then select the result. This means that even when `p` is not `NULL` (a defined execution), the `r / 0` computation would be performed. On most architectures, this causes a hardware trap, leading to program termination. This introduces a new, observable behavior (a crash) on a previously well-defined path, which is a violation of the as-if rule. Therefore, naive if-conversion is illegal here.\n\nWith this in mind, we can evaluate the options:\n\n*   **Statement C is correct** because it perfectly articulates the `as-if` rule in this context. For an input where the original program has defined behavior (i.e., `p` is not `NULL`), the control flow prevents the UB-containing path from executing. Any transformation, such as if-conversion, must not make that UB observable (e.g., by causing a trap).\n\n*   **Statement A is incorrect** because it fails to account for the side effects of speculative execution. It claims the transformation is safe because the *result* is discarded, but ignores that the *computation itself* can cause a program-terminating trap. This trap is an observable behavior that violates the as-if rule.\n\n*   **Statement B is correct** because it posits a specific architectural feature that makes if-conversion safe: fault-suppressing predication. If an instruction (like division) guarded by a false predicate becomes a true no-op (i.e., it doesn't execute and cannot raise a fault), then speculatively executing the `r / 0` on the `p != NULL` path is safe. The false predicate would annul the division, preventing the trap and preserving the program's original, defined behavior. This makes the transformation legal under the as-if rule.\n\n*   **Statement D is incorrect** due to a misunderstanding of how compilers reason about UB. A compiler can assume that code leading to UB is never executed on a well-defined path. Therefore, it can assume that for any defined execution, `p` must not be `NULL`. However, it cannot assume that a mathematical truth, like `r - r == 0`, is false. The reasoning in option D is based on a logical contradiction (`0 != 0`) rather than valid inferences about program state.",
            "answer": "$$\\boxed{BC}$$"
        }
    ]
}