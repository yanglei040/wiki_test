{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration, we will start with the fundamental mechanics of if-conversion. This first exercise challenges you to transform a sequence of conditional statements into an equivalent, branch-free data-flow graph using common compiler idioms like $\\min$ and $\\max$. By analyzing the critical-path latency and micro-operation count under a simplified processor model, you will gain a quantitative understanding of how if-conversion can reduce execution time by eliminating control hazards and enabling instruction-level parallelism .",
            "id": "3663804",
            "problem": "Consider the following scalar kernel that clamps and combines integer values, written with control-dependent updates:\n\nFor each index $i$,\n- Let $t \\leftarrow A[i] - B[i]$.\n- If $t  \\mathrm{hi}$ then $t \\leftarrow \\mathrm{hi}$.\n- If $t  \\mathrm{lo}$ then $t \\leftarrow \\mathrm{lo}$.\n- If $t  C[i]$ then $u \\leftarrow C[i]$ else $u \\leftarrow t$.\n- Let $v \\leftarrow u + D[i]$.\n- If $v  k$ then $R[i] \\leftarrow v$ else $R[i] \\leftarrow k$.\n\nAssume that all inputs $A[i]$, $B[i]$, $C[i]$, $D[i]$, $\\mathrm{lo}$, $\\mathrm{hi}$, and $k$ are available in registers at time $0$ (no memory latency), and that the machine is an ideal out-of-order core with sufficient issue width and resources so that only true data-dependence chains determine the latency to produce $R[i]$. The compiler targets a reduced instruction set computing (RISC)-like instruction set architecture and may perform if-conversion to eliminate control dependences using predicated execution.\n\nYou are given the following fundamental definitions and cost model for predication and idioms:\n- Predicated execution transforms each control-dependent assignment into a data-dependent operation using a predicate produced by a comparison, for example a conditional move (cmov) or a select.\n- The if-converted form can be expressed using the min and max algebraic idioms: $\\min(x, y)$ and $\\max(x, y)$, which are recognized by the compiler when the condition and the choice of operands match the idiom pattern.\n- Latencies (register-to-register) in cycles:\n  - Integer add or subtract: $L_{a} = 1$.\n  - Compare that produces a predicate: $L_{c} = 1$.\n  - Select or conditional move that consumes a predicate: $L_{s} = 2$.\n  - Fused min or max idiom as a single instruction when recognized: $L_{mm} = 1$.\n- Micro-operation (uop) accounting and fusion opportunities:\n  - Integer add or subtract: $1$ uop.\n  - Compare: $1$ uop.\n  - Select or conditional move: $1$ uop.\n  - Recognized min or max idiom: $1$ uop (this replaces the compare+select pair when applicable, reducing two uops to one).\n  - In the original branched code, each compare followed immediately by a conditional branch can macro-fuse front-end into a single fused uop for fetch/issue purposes, but this does not change the true data-dependence chain latency. After if-conversion there are no branches, so branch macro-fusion does not apply.\n\nTasks:\n- Rewrite the kernel using min and max idioms (that is, express it with $\\min$ and $\\max$ rather than control flow), justifying correctness by the definitions of $\\min$ and $\\max$ and the semantics of predicated execution.\n- Under the cost model above, compute the critical-path data-dependence length (in cycles) to produce $R[i]$ for one iteration in each of the following two if-converted realizations:\n  - Realization $\\mathcal{S}$: implement each $\\min$ or $\\max$ using a compare plus select (no idiom recognition). The data-dependence contribution of each $\\min$ or $\\max$ is $L_{c} + L_{s}$.\n  - Realization $\\mathcal{M}$: implement each $\\min$ or $\\max$ as a recognized fused idiom instruction of latency $L_{mm}$ (idiom recognition enabled).\n  In both realizations, use $L_{a}$ for the add or subtract and assume all inputs are ready at time $0$.\n- Quantify the number of uops per iteration in realizations $\\mathcal{S}$ and $\\mathcal{M}$, and briefly explain how uop fusion opportunities differ from the original branched form.\n- Finally, report a single number: the best-case critical-path latency (in cycles) per iteration among $\\mathcal{S}$ and $\\mathcal{M}$ under the given latencies.\n\nExpress your final answer as an integer number of cycles, with no units or additional text inside the answer box.",
            "solution": "The problem has been validated and is found to be scientifically grounded, well-posed, and objective. It provides a clear, self-contained, and consistent scenario for analyzing compiler optimizations and their performance implications on a model architecture. Therefore, a full solution is provided below.\n\nThe scalar kernel processes each element $i$ through a sequence of operations involving conditional updates. To analyze its performance under if-conversion, we first transform the control-dependent logic into a data-flow representation using $\\min$ and $\\max$ functions. This transformation is based on the semantics of predicated execution, where conditional assignments are replaced by data-dependent selections.\n\nThe sequence of operations for an index $i$ is:\n1.  Let $t \\leftarrow A[i] - B[i]$.\n2.  If $t  \\mathrm{hi}$ then $t \\leftarrow \\mathrm{hi}$.\n3.  If $t  \\mathrm{lo}$ then $t \\leftarrow \\mathrm{lo}$.\n4.  If $t  C[i]$ then $u \\leftarrow C[i]$ else $u \\leftarrow t$.\n5.  Let $v \\leftarrow u + D[i]$.\n6.  If $v  k$ then $R[i] \\leftarrow v$ else $R[i] \\leftarrow k$.\n\nLet's convert each conditional step into its equivalent `min`/`max` form:\n\n- Step $1$: A simple subtraction. Let's call the result $t_0$.\n    $t_0 \\leftarrow A[i] - B[i]$\n\n- Step $2$: \"If $t  \\mathrm{hi}$ then $t \\leftarrow \\mathrm{hi}$\" sets the value of $t$ to $\\mathrm{hi}$ if it exceeds $\\mathrm{hi}$, and leaves it unchanged otherwise. This is equivalent to taking the minimum of the current value and $\\mathrm{hi}$.\n    $t_1 \\leftarrow \\min(t_0, \\mathrm{hi})$\n\n- Step $3$: \"If $t  \\mathrm{lo}$ then $t \\leftarrow \\mathrm{lo}$\" sets the value of $t$ to $\\mathrm{lo}$ if it is less than $\\mathrm{lo}$, and leaves it unchanged otherwise. This is equivalent to taking the maximum of the current value and $\\mathrm{lo}$.\n    $t_2 \\leftarrow \\max(t_1, \\mathrm{lo})$\n    The combination of steps $2$ and $3$ effectively clamps the value $t_0$ to the range $[\\mathrm{lo}, \\mathrm{hi}]$.\n\n- Step $4$: \"If $t  C[i]$ then $u \\leftarrow C[i]$ else $u \\leftarrow t$\" selects the larger of the two values, $t$ and $C[i]$. This is the definition of the maximum function.\n    $u \\leftarrow \\max(t_2, C[i])$\n\n- Step $5$: A simple addition.\n    $v \\leftarrow u + D[i]$\n\n- Step $6$: \"If $v  k$ then $R[i] \\leftarrow v$ else $R[i] \\leftarrow k$\" selects the smaller of the two values, $v$ and $k$. This is the definition of the minimum function.\n    $R[i] \\leftarrow \\min(v, k)$\n\nThe entire computation can be expressed as a data-flow chain:\n$t_0 \\leftarrow A[i] - B[i]$\n$t_1 \\leftarrow \\min(t_0, \\mathrm{hi})$\n$t_2 \\leftarrow \\max(t_1, \\mathrm{lo})$\n$u \\leftarrow \\max(t_2, C[i])$\n$v \\leftarrow u + D[i]$\n$R[i] \\leftarrow \\min(v, k)$\n\nSince all inputs ($A[i]$, $B[i]$, $C[i]$, $D[i]$, $\\mathrm{lo}$, $\\mathrm{hi}$, $k$) are available at time $0$ and we assume an ideal out-of-order core, the critical path latency is determined by the longest true data-dependence chain. In this case, the computation forms a single linear chain of dependencies. The total latency is the sum of the latencies of each operation in this chain.\n\nThe latencies are given as:\n- Integer add or subtract: $L_{a} = 1$ cycle.\n- Compare: $L_{c} = 1$ cycle.\n- Select/conditional move: $L_{s} = 2$ cycles.\n- Fused min/max idiom: $L_{mm} = 1$ cycle.\n\nWe now compute the critical-path latency for the two specified realizations.\n\n**Realization $\\mathcal{S}$ (no idiom recognition)**: Each $\\min$ or $\\max$ operation is implemented as a `compare` followed by a `select`. The latency for such a sequence is $L_{c} + L_{s} = 1 + 2 = 3$ cycles.\n\n1.  $t_0$ is ready at time $T(t_0) = 0 + L_a = 1$.\n2.  $t_1$ depends on $t_0$. $T(t_1) = T(t_0) + (L_c + L_s) = 1 + 3 = 4$.\n3.  $t_2$ depends on $t_1$. $T(t_2) = T(t_1) + (L_c + L_s) = 4 + 3 = 7$.\n4.  $u$ depends on $t_2$. $T(u) = T(t_2) + (L_c + L_s) = 7 + 3 = 10$.\n5.  $v$ depends on $u$. $T(v) = T(u) + L_a = 10 + 1 = 11$.\n6.  $R[i]$ depends on $v$. $T(R[i]) = T(v) + (L_c + L_s) = 11 + 3 = 14$.\n\nThe critical-path latency for realization $\\mathcal{S}$ is $14$ cycles.\n\n**Realization $\\mathcal{M}$ (with idiom recognition)**: Each $\\min$ or $\\max$ operation is implemented as a single fused instruction with latency $L_{mm} = 1$ cycle.\n\n1.  $t_0$ is ready at time $T(t_0) = 0 + L_a = 1$.\n2.  $t_1$ depends on $t_0$. $T(t_1) = T(t_0) + L_{mm} = 1 + 1 = 2$.\n3.  $t_2$ depends on $t_1$. $T(t_2) = T(t_1) + L_{mm} = 2 + 1 = 3$.\n4.  $u$ depends on $t_2$. $T(u) = T(t_2) + L_{mm} = 3 + 1 = 4$.\n5.  $v$ depends on $u$. $T(v) = T(u) + L_{a} = 4 + 1 = 5$.\n6.  $R[i]$ depends on $v$. $T(R[i]) = T(v) + L_{mm} = 5 + 1 = 6$.\n\nThe critical-path latency for realization $\\mathcal{M}$ is $6$ cycles.\n\nNext, we quantify the number of micro-operations (uops) per iteration. The kernel consists of $1$ subtraction, $1$ addition, and $4$ min/max operations.\n\n- In realization $\\mathcal{S}$, each `min`/`max` is a `compare` ($1$ uop) plus a `select` ($1$ uop), totaling $2$ uops.\n  Number of uops for $\\mathcal{S} = 1_{\\text{sub}} + 4 \\times (1_{\\text{cmp}} + 1_{\\text{sel}}) + 1_{\\text{add}} = 1 + 4 \\times 2 + 1 = 10$ uops.\n\n- In realization $\\mathcal{M}$, each `min`/`max` is a single fused instruction ($1$ uop).\n  Number of uops for $\\mathcal{M} = 1_{\\text{sub}} + 4 \\times (1_{\\text{min/max}}) + 1_{\\text{add}} = 1 + 4 + 1 = 6$ uops.\n\nThe fusion opportunities differ significantly between the original branched code and the if-converted forms. In the original code, the primary fusion opportunity is *macro-fusion*, where a `compare` instruction and its subsequent `conditional branch` are fused at the front-end into a single uop for fetch and decode. This leads to a variable number of executed uops depending on the data-dependent execution path. In contrast, in the if-converted realization $\\mathcal{M}$, the opportunity is *micro-op fusion* (or idiom recognition during instruction selection). A `compare`-`select` pattern is replaced by a single, specialized `min` or `max` instruction. This fusion occurs for every such pattern, resulting in a fixed, and in this case, lower, total number of executed uops ($6$ uops for $\\mathcal{M}$ vs. $10$ for $\\mathcal{S}$).\n\nFinally, the best-case critical-path latency is the minimum of the latencies calculated for realizations $\\mathcal{S}$ and $\\mathcal{M}$.\nBest-case latency $= \\min(14, 6) = 6$ cycles.",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "Having mastered the basics, we now apply if-conversion to one of its most important use cases: enabling Single Instruction, Multiple Data (SIMD) vectorization. This practice asks you to model the performance of a branchy loop versus its predicated SIMD equivalent, revealing a crucial trade-off between branch misprediction penalties and parallel processing efficiency. By developing a speedup formula that depends on data characteristics and vector length, you will see how the effectiveness of predication is not absolute but is instead a function of the problem context .",
            "id": "3663829",
            "problem": "You are given a branchy scalar thresholding loop applied to an image represented as a one-dimensional array of $N$ pixels, where each pixel is a non-negative integer intensity. The scalar loop examines each pixel against a threshold $T$ and writes either $255$ or $0$ to the output depending on whether the threshold is met. The branchy scalar loop has the following form: for each index $i$, if $\\text{in}[i] \\ge T$ then $\\text{out}[i] \\leftarrow 255$ else $\\text{out}[i] \\leftarrow 0$. You are asked to port this computation to predicated execution using Single Instruction Multiple Data (SIMD) with vector length $VL$ (the number of elements processed per vector operation), and to compute the expected speedup of the predicated SIMD version over the branchy scalar version as a function of the threshold hit rate $r$ (the probability that a given pixel satisfies $\\text{in}[i] \\ge T$) and the vector length $VL$.\n\nThe fundamental base for this problem consists of the following well-tested facts and definitions:\n- The expected value of a random variable is the probability-weighted average of its possible values. If an event with probability $r$ incurs cost $P$, and otherwise cost $0$, then the expected additional cost is $r \\cdot P$.\n- In a simple cost model measured in processor cycles, an instruction has an associated cycle cost. Total cycles for a loop are the sum of the cycles of its constituent operations. For the per-element expected cost, divide by $N$ when aggregating over $N$ independent elements.\n- Predicated SIMD replaces branches with mask computations and selection operations that execute in lockstep for $VL$ elements, making the per-element cost the per-vector cost divided by $VL$.\n\nAssume the following cost model in cycles, which is scientifically plausible for a modern out-of-order superscalar processor:\n- For the branchy scalar loop, each element incurs a base cost equal to the sum of the load cost $c_{\\text{ld}}$, compare cost $c_{\\text{cmp}}$, store cost $c_{\\text{st}}$, and branch resolution overhead $c_{\\text{br}}$. In addition, a branch misprediction penalty $P$ is paid on an unexpected taken branch. Under a static \"predict not taken\" predictor model, a pixel that meets the threshold is treated as \"taken\" with probability $r$, and incurs the penalty $P$ in expectation. Therefore, the expected per-element cost of the scalar branchy loop is the base cost plus $r \\cdot P$.\n- For the predicated SIMD loop, for each vector of $VL$ elements, there is one vector load with cost $C_{\\text{vld}}$, one vector compare with cost $C_{\\text{vcmp}}$, one vector blend/select with cost $C_{\\text{vblend}}$, and one vector store with cost $C_{\\text{vst}}$. There is no data-dependent branch cost. The per-element cost of the SIMD version is the sum of these per-vector costs divided by $VL$.\n\nYour task is to:\n1. Derive the expected per-element cost of the branchy scalar loop as a function of $r$ using the above base facts. Do not use shortcut formulas beyond those implied by the base facts.\n2. Derive the per-element cost of the predicated SIMD loop as a function of $VL$ using the above base facts.\n3. Define the speedup $S(r, VL)$ as the ratio of the scalar expected per-element cost to the predicated SIMD per-element cost.\n4. Implement a program in the C language (standard $C23$) that computes $S(r, VL)$ for each test case in the test suite specified below, using the given constants. The program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each speedup value must be printed as a floating-point number rounded to six decimal places.\n\nConstants to use in your model:\n- Scalar costs per element: $c_{\\text{ld}} = 1$, $c_{\\text{cmp}} = 1$, $c_{\\text{st}} = 1$, $c_{\\text{br}} = 1$, and branch misprediction penalty $P = 15$.\n- Predicated SIMD costs per vector: $C_{\\text{vld}} = 3$, $C_{\\text{vcmp}} = 3$, $C_{\\text{vblend}} = 3$, $C_{\\text{vst}} = 3$.\n\nTest suite (each case is a pair $(r, VL)$):\n- Case $1$: $r = 0$, $VL = 4$.\n- Case $2$: $r = 0.5$, $VL = 4$.\n- Case $3$: $r = 1.0$, $VL = 4$.\n- Case $4$: $r = 0.1$, $VL = 8$.\n- Case $5$: $r = 0.9$, $VL = 8$.\n- Case $6$: $r = 0.5$, $VL = 1$.\n- Case $7$: $r = 0.5$, $VL = 16$.\n- Case $8$: $r = 0.2$, $VL = 16$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[x_1,x_2,\\dots,x_8]$, where each $x_i$ is the computed speedup for case $i$ rounded to six decimal places. No additional text should be printed.",
            "solution": "We first formalize the branchy scalar and predicated Single Instruction Multiple Data (SIMD) versions of the thresholding loop at the level of per-element and per-vector costs, using the stated base principles of expected value and instruction cost aggregation.\n\nFor the branchy scalar loop, consider one element. The operations involved, regardless of the branch outcome, are a load, a compare, a store, and resolving the branch instruction. Let the per-element cycle costs be $c_{\\text{ld}}$, $c_{\\text{cmp}}$, $c_{\\text{st}}$, and $c_{\\text{br}}$ respectively. Under a static \"predict not taken\" branch predictor, the branch is predicted as not taken by default. When the condition is true, the branch becomes taken and is mispredicted. If we denote the probability that an element meets the threshold as $r$ (the threshold hit rate), the event of misprediction occurs with probability $r$. Let the misprediction penalty be $P$ cycles. By the definition of expected value, the expected additional cost due to misprediction is $r \\cdot P$. Therefore, the expected per-element cost for the scalar branchy loop is\n$$\nC_{\\text{branch}}(r) = c_{\\text{ld}} + c_{\\text{cmp}} + c_{\\text{st}} + c_{\\text{br}} + r \\cdot P.\n$$\nUsing the provided constants $c_{\\text{ld}} = 1$, $c_{\\text{cmp}} = 1$, $c_{\\text{st}} = 1$, $c_{\\text{br}} = 1$, and $P = 15$, this simplifies numerically to\n$$\nC_{\\text{branch}}(r) = 1 + 1 + 1 + 1 + 15 r = 4 + 15 r.\n$$\n\nFor the predicated SIMD loop, predication replaces the branch with the computation of a mask and a selection (blend), operating across $VL$ elements at once. The per-vector operations are a vector load, vector compare, vector blend/select, and vector store. Let their per-vector cycle costs be $C_{\\text{vld}}$, $C_{\\text{vcmp}}$, $C_{\\text{vblend}}$, and $C_{\\text{vst}}$. Since these costs are not data-dependent under predication, the per-vector total cost is\n$$\nC_{\\text{SIMD,vector}} = C_{\\text{vld}} + C_{\\text{vcmp}} + C_{\\text{vblend}} + C_{\\text{vst}}.\n$$\nTo obtain the per-element cost, divide the per-vector cost by the number of elements processed per vector, $VL$, yielding\n$$\nC_{\\text{SIMD,elem}}(VL) = \\frac{C_{\\text{vld}} + C_{\\text{vcmp}} + C_{\\text{vblend}} + C_{\\text{vst}}}{VL}.\n$$\nWith the provided constants $C_{\\text{vld}} = 3$, $C_{\\text{vcmp}} = 3$, $C_{\\text{vblend}} = 3$, and $C_{\\text{vst}} = 3$, we obtain\n$$\nC_{\\text{SIMD,elem}}(VL) = \\frac{3 + 3 + 3 + 3}{VL} = \\frac{12}{VL}.\n$$\n\nDefine the speedup $S(r, VL)$ as the ratio of the scalar expected per-element cost to the predicated SIMD per-element cost, that is\n$$\nS(r, VL) = \\frac{C_{\\text{branch}}(r)}{C_{\\text{SIMD,elem}}(VL)}.\n$$\nSubstituting the expressions derived above,\n$$\nS(r, VL) = \\frac{4 + 15 r}{12 / VL} = \\left(4 + 15 r\\right) \\cdot \\frac{VL}{12}.\n$$\n\nAlgorithmically, the program must implement the following steps for each test case $(r, VL)$:\n1. Compute $C_{\\text{branch}}(r)$ using $C_{\\text{branch}}(r) = 4 + 15 r$.\n2. Compute $C_{\\text{SIMD,elem}}(VL)$ using $C_{\\text{SIMD,elem}}(VL) = 12 / VL$.\n3. Compute $S(r, VL)$ as $C_{\\text{branch}}(r) / C_{\\text{SIMD,elem}}(VL)$.\n4. Print the resulting $S(r, VL)$ rounded to six decimal places.\n\nWe now evaluate the specified test suite:\n- Case $1$: $r = 0$, $VL = 4$. $C_{\\text{branch}}(0) = 4 + 15 \\cdot 0 = 4$. $C_{\\text{SIMD,elem}}(4) = 12 / 4 = 3$. $S(0,4) = 4 / 3 = 1.\\overline{3}$, which rounded to six decimals is $1.333333$.\n- Case $2$: $r = 0.5$, $VL = 4$. $C_{\\text{branch}}(0.5) = 4 + 15 \\cdot 0.5 = 11.5$. $C_{\\text{SIMD,elem}}(4) = 3$. $S(0.5,4) = 11.5 / 3 \\approx 3.833333$.\n- Case $3$: $r = 1.0$, $VL = 4$. $C_{\\text{branch}}(1.0) = 4 + 15 = 19$. $C_{\\text{SIMD,elem}}(4) = 3$. $S(1.0,4) = 19 / 3 \\approx 6.333333$.\n- Case $4$: $r = 0.1$, $VL = 8$. $C_{\\text{branch}}(0.1) = 4 + 1.5 = 5.5$. $C_{\\text{SIMD,elem}}(8) = 12 / 8 = 1.5$. $S(0.1,8) = 5.5 / 1.5 \\approx 3.666667$.\n- Case $5$: $r = 0.9$, $VL = 8$. $C_{\\text{branch}}(0.9) = 4 + 13.5 = 17.5$. $C_{\\text{SIMD,elem}}(8) = 1.5$. $S(0.9,8) = 17.5 / 1.5 \\approx 11.666667$.\n- Case $6$: $r = 0.5$, $VL = 1$. $C_{\\text{branch}}(0.5) = 11.5$. $C_{\\text{SIMD,elem}}(1) = 12$. $S(0.5,1) = 11.5 / 12 \\approx 0.958333$.\n- Case $7$: $r = 0.5$, $VL = 16$. $C_{\\text{SIMD,elem}}(16) = 12 / 16 = 0.75$. $S(0.5,16) = 11.5 / 0.75 \\approx 15.333333$.\n- Case $8$: $r = 0.2$, $VL = 16$. $C_{\\text{branch}}(0.2) = 4 + 3 = 7$. $C_{\\text{SIMD,elem}}(16) = 0.75$. $S(0.2,16) = 7 / 0.75 \\approx 9.333333$.\n\nThe program will compute these values and print them as a single bracketed, comma-separated list, rounded to six decimal places.",
            "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include stdio.h\n#include stdlib.h\n#include string.h\n#include math.h\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    double r;   // Threshold hit rate (probability that pixel = T)\n    int VL;     // Vector length (number of elements per SIMD vector)\n} TestCase;\n\n// Compute expected scalar branchy per-element cost: C_branch(r) = 4 + 15*r\nstatic inline double scalar_cost(double r, double c_ld, double c_cmp, double c_st, double c_br, double mispred_penalty) {\n    return c_ld + c_cmp + c_st + c_br + r * mispred_penalty;\n}\n\n// Compute predicated SIMD per-element cost: C_SIMD_elem(VL) = (C_vld + C_vcmp + C_vblend + C_vst) / VL\nstatic inline double simd_cost_per_elem(int VL, double C_vld, double C_vcmp, double C_vblend, double C_vst) {\n    double per_vector = C_vld + C_vcmp + C_vblend + C_vst;\n    return per_vector / (double)VL;\n}\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        { 0.0, 4 },\n        { 0.5, 4 },\n        { 1.0, 4 },\n        { 0.1, 8 },\n        { 0.9, 8 },\n        { 0.5, 1 },\n        { 0.5, 16 },\n        { 0.2, 16 }\n    };\n\n    // Scalar cost constants (per element)\n    const double c_ld = 1.0;\n    const double c_cmp = 1.0;\n    const double c_st = 1.0;\n    const double c_br = 1.0;\n    const double mispred_penalty = 15.0;\n\n    // Predicated SIMD cost constants (per vector)\n    const double C_vld = 3.0;\n    const double C_vcmp = 3.0;\n    const double C_vblend = 3.0;\n    const double C_vst = 3.0;\n\n    // Calculate the number of test cases.\n    int num_cases = (int)(sizeof(test_cases) / sizeof(test_cases[0]));\n    double results[num_cases]; // Speedup results for each test case.\n\n    // Calculate the result for each test case.\n    for (int i = 0; i  num_cases; ++i) {\n        double r = test_cases[i].r;\n        int VL = test_cases[i].VL;\n\n        double c_branch = scalar_cost(r, c_ld, c_cmp, c_st, c_br, mispred_penalty);\n        double c_simd_elem = simd_cost_per_elem(VL, C_vld, C_vcmp, C_vblend, C_vst);\n\n        double speedup = c_branch / c_simd_elem;\n        results[i] = speedup;\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement\n    // Single line: comma-separated list enclosed in square brackets, each value rounded to six decimals.\n    printf(\"[\");\n    for (int i = 0; i  num_cases; ++i) {\n        if (i  0) {\n            printf(\",\");\n        }\n        // Round to six decimal places\n        printf(\"%.6f\", results[i]);\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```"
        },
        {
            "introduction": "A powerful optimization is only useful if it is correct. This final practice moves from performance analysis to the critical issue of safety, exploring the interaction between if-conversion and undefined behavior (UB) in languages like C. You will analyze a scenario where speculative execution of a seemingly untaken path can introduce a fatal error, forcing you to reason about the compiler's as-if rule and the architectural guarantees required to apply such transformations safely . This exercise underscores that a deep understanding of both hardware capabilities and language semantics is essential for a compiler engineer.",
            "id": "3663865",
            "problem": "A compiler performing if-conversion replaces control dependence with data dependence using predicated execution. Consider the following C function, written in a style intended to avoid explicit numeric literals while still being realistic and well-defined on executions that do not evaluate undefined behavior (UB):\n\n\nint g(int *p, int r) {\n    if (p) {\n        return *p;\n    } else {\n        return r / (r - r);\n    }\n}\n\n\nAssume a typical optimizing compiler for a language with undefined behavior (UB) semantics like C, and a target architecture that may or may not support fault-suppressing predicated instructions. The compiler wishes to if-convert the branch to improve instruction-level parallelism by making the two arms appear as one predicated block or a value-through-select. Your task is to reason from first principles (source language semantics and the as-if rule of program transformation, architectural predication semantics, and the definition of operations that may trap) about the legality and risk of such transformations when UB is present only on the untaken path for certain inputs. In particular, analyze the risk that a transformation which speculatively executes both arms could newly expose the UB path even when the original control flow would not execute it.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. On any target, it is semantics-preserving to rewrite the branch so that both arms are computed eagerly and the final result is chosen with a conditional move or select, because the unused result is discarded before it can affect the program.\n\nB. If the target’s predicated execution guarantees that a predicated instruction with a false predicate neither evaluates its operands nor raises exceptions, then guarding the division by the negation of the condition makes if-conversion semantics-preserving for inputs where the original program did not execute UB.\n\nC. Even if the untaken path contains UB, as long as the original control flow prevents its execution for the given inputs, the compiler must not introduce a transformation that makes that behavior observable on those inputs; the as-if rule requires preservation of behavior on defined executions.\n\nD. Because UB gives the compiler unrestricted freedom, it may assume that the denominator expression is never equal to $0$ and fold the division away in the original program, which makes if-conversion always safe regardless of whether the division would have been executed.",
            "solution": "The problem requires an analysis of the legality of applying if-conversion to a C function containing a branch where one path exhibits undefined behavior (UB). The core principles to apply are the C language's `as-if` rule, the semantics of UB, and the behavior of speculative execution on various hardware architectures.\n\n### Step 1: Problem Validation\n\nThe problem statement is valid. It presents a well-defined C function:\n```c\nint g(int *p, int r) {\n    if (p) {\n        return *p;\n    } else {\n        return r / (r - r);\n    }\n}\n```\nThe problem correctly identifies the key elements: a conditional branch, a well-defined path (`if (p)` is true), and a path with undefined behavior (`else`, which contains `r / (r - r)`). The division `r / (r - r)` simplifies to `r / 0`. Integer division by zero is explicitly defined as UB in the C standard. The question concerns the legality of if-conversion, a standard compiler optimization, in this context. This is a classic, non-trivial problem in compiler design that requires understanding the interaction between language semantics and hardware realities. The problem is scientifically grounded, well-posed, and objective.\n\n### Step 2: Derivation from First Principles\n\nThe governing principle for compiler transformations is the **as-if rule**. This rule states that a compiler can perform any transformation, so long as the observable behavior of the transformed program is identical to the observable behavior of the original program for all executions that have defined behavior. Observable behavior includes program termination, I/O, and access to `volatile` variables. A crash or hardware exception constitutes an observable behavior.\n\nLet's analyze the behavior of the original function `g(p, r)`:\n1.  **Case: `p` is not `NULL`**. The condition `if (p)` evaluates to true. The function executes `return *p;`. Assuming `p` points to a valid `int`, this execution path is well-defined. The `else` block, containing the division by zero, is never executed. The observable behavior is the function returning the value pointed to by `p`.\n2.  **Case: `p` is `NULL`**. The condition `if (p)` evaluates to false. The function attempts to execute `return r / (r - r);`. The expression `r - r` is guaranteed to be $0$. The C standard specifies that signed integer division by zero results in undefined behavior. In this case, the standard imposes no requirements on the program's behavior.\n\n**If-Conversion Transformation:**\nIf-conversion aims to remove the control dependency (the `if` statement) and replace it with a data dependency. A general form of this transformation involves speculatively executing both branches and then selecting the correct result. For the function `g`, this would be conceptually equivalent to:\n\n```\n// Conceptual representation of if-conversion\nbool condition = (p != NULL);\nint value_if_true = *p;\nint value_if_false = r / (r - r); // Speculative execution of the UB path\nint result = select(condition, value_if_true, value_if_false);\nreturn result;\n```\n\nThe critical issue arises when `p` is not `NULL`. In the original program, this is a well-defined execution. In the transformed program, the expression `value_if_false = r / (r - r);` is always evaluated. On most common processor architectures (e.g., x86, ARM in A32/T32 mode), an integer division by zero instruction triggers a hardware exception (a trap). This trap typically leads to the operating system terminating the program (e.g., with a `SIGFPE` signal on Unix-like systems).\n\nTherefore, for a well-defined input (`p != NULL`), the original program returns a value, while the naively transformed program crashes. This is a change in observable behavior, making this transformation **illegal** under the `as-if` rule.\n\n### Step 3: Option-by-Option Analysis\n\n**A. On any target, it is semantics-preserving to rewrite the branch so that both arms are computed eagerly and the final result is chosen with a conditional move or select, because the unused result is discarded before it can affect the program.**\n\nThis statement is **Incorrect**. The reasoning is flawed. It focuses on the *result* of the computation being discarded, but ignores the *side effects* of the computation itself. The act of computing `r / 0` can have the observable side effect of a program-terminating trap. This trap occurs during the speculative execution, long before the result could be \"discarded\". As shown in the analysis above, on a typical target architecture that traps on division by zero, this transformation changes the program's behavior for well-defined inputs (where `p != NULL`), violating the as-if rule. Therefore, it is not semantics-preserving on \"any target\".\n\n**B. If the target’s predicated execution guarantees that a predicated instruction with a false predicate neither evaluates its operands nor raises exceptions, then guarding the division by the negation of the condition makes if-conversion semantics-preserving for inputs where the original program did not execute UB.**\n\nThis statement is **Correct**. It describes a specific architectural feature known as **fault-suppressing predication** (prominently featured in Intel's Itanium architecture, for example). The transformation would look conceptually like this:\n\n1.  Let predicate `P` be true if `p != NULL` and false otherwise.\n2.  The `then` branch value is computed: `value_if_true = *p;`. This is safe because it only needs to be correct when `P` is true.\n3.  The `else` branch computation is guarded by the negated predicate, `!P`: `if (!P) value_if_false = r / 0;`\n4.  A final `select` or conditional move instruction picks the result.\n\nThe key is that the hardware instruction for `value_if_false = r / 0` would be a predicated instruction. When `p != NULL`, the predicate `!P` is false. The architectural guarantee specified in the option means that the predicated division-by-zero instruction becomes a no-op; it does not access its operands and, crucially, does not raise a fault. Consequently, for the well-defined case (`p != NULL`), the transformed code does not trap and correctly produces the result from the `then` branch, preserving the original program's behavior. For the UB case (`p == NULL`), the transformed code may or may not trap, which is consistent with the original program's UB. Thus, the transformation is semantics-preserving.\n\n**C. Even if the untaken path contains UB, as long as the original control flow prevents its execution for the given inputs, the compiler must not introduce a transformation that makes that behavior observable on those inputs; the as-if rule requires preservation of behavior on defined executions.**\n\nThis statement is **Correct**. It is a precise and accurate statement of the `as-if` rule as it applies to this scenario.\n-   \"the untaken path contains UB\": This is the `else` branch when `p != NULL`.\n-   \"original control flow prevents its execution\": The `if (p)` check ensures this.\n-   \"the compiler must not introduce a transformation that makes that behavior observable\": Introducing a trap (an observable behavior) on a previously well-defined execution path is illegal.\n-   \"the as-if rule requires preservation of behavior on defined executions\": This is the formal justification.\n\nThis statement correctly identifies the fundamental constraint that makes naive if-conversion illegal in this case.\n\n**D. Because UB gives the compiler unrestricted freedom, it may assume that the denominator expression is never equal to $0$ and fold the division away in the original program, which makes if-conversion always safe regardless of whether the division would have been executed.**\n\nThis statement is **Incorrect**. The reasoning about UB is subtly but fundamentally flawed. When a compiler encounters code that would produce UB, it is permitted to assume that such code is **never executed** on any well-defined program path. In this case, the `r / (r - r)` operation is only executed if `p` is `NULL`. Therefore, the compiler is permitted to assume that `p` is never `NULL` for any well-defined execution. This may allow the compiler to eliminate the `else` branch entirely and simplify the function to `return *p;`.\n\nHowever, the option claims the compiler may \"assume that the denominator expression is never equal to $0$\". The denominator is `r - r`. It is a mathematical and logical certainty that `r - r` is `0`. A compiler's reasoning, while powerful, cannot be based on assuming logical contradictions like $0 \\ne 0$. The compiler reasons about program state (`p != NULL`), not about violating the axioms of arithmetic. Because the premise of the reasoning in this option is unsound, the conclusion is invalid.",
            "answer": "$$\\boxed{BC}$$"
        }
    ]
}