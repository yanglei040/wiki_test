{
    "hands_on_practices": [
        {
            "introduction": "To master list scheduling, we must begin with the fundamentals: calculating instruction priorities and manually simulating the scheduling process. This first exercise  provides a controlled scenario with a simple instruction dependency graph and a single-issue processor. By applying the critical-path priority heuristic and comparing two different tie-breaking strategies, you will gain a concrete understanding of how the scheduler makes decisions cycle-by-cycle and see firsthand how these choices impact the overall execution time.",
            "id": "3650813",
            "problem": "A compiler backend targets a Single Instruction Issue (SII) machine that can issue at most one instruction per clock cycle. Instructions have positive integer latencies. If an instruction $v$ is issued at cycle $t$, its result becomes available at cycle $t + l(v)$, where $l(v)$ is its latency. A successor $u$ of $v$ in the Directed Acyclic Graph (DAG) may be issued only when all of its predecessors have produced their results. A list scheduler maintains a ready list of instructions whose data dependencies are satisfied at a given cycle and chooses one instruction per cycle if available; if the list is empty, the cycle is counted as a stall. The total makespan is the completion time of the last instruction (the maximum over all issued instructions of issue time plus latency).\n\nConsider a basic block whose dependence graph is a Directed Acyclic Graph (DAG) $G=(V,E)$ with node set\n$$V=\\{C_{1},C_{2},C_{3},C_{4},C_{5}\\}\\cup \\{P_{1},P_{2},P_{3},P_{4},P_{5},P_{6},P_{7},P_{8}\\}$$\nand edge set\n$$E=\\{(C_{1},C_{2}),(C_{2},C_{3}),(C_{3},C_{4}),(C_{4},C_{5})\\}.$$\nAll $P_{j}$ nodes have no successors and no predecessors. Latencies are\n$$l(C_{i})=4 \\text{ for } i\\in\\{1,2,3,4,5\\},\\quad l(P_{j})=1 \\text{ for } j\\in\\{1,2,3,4,5,6,7,8\\}.$$\n\nUsing the definition of critical-path priority as the weighted longest-path-to-exit metric,\n$$p(v)=l(v)+\\max_{(v\\to u)\\in E}p(u),\\quad \\max \\emptyset = 0,$$\nperform the following:\n\n- Compute $p(v)$ for all $v\\in V$.\n- Construct two list schedules:\n  - Schedule $\\mathcal{S}_{A}$ uses the ready-list tie-breaker: pick the ready instruction with largest $p(v)$; on ties, pick the one with larger $l(v)$; on further ties, pick by lexicographically increasing identifier ($C$ before $P$, numerically smaller index first).\n  - Schedule $\\mathcal{S}_{B}$ uses the ready-list tie-breaker: pick the ready instruction with smallest $l(v)$; on ties, pick the one with smaller $p(v)$; on further ties, pick by lexicographically increasing identifier.\n\nFor each schedule, report the issue time of every instruction, the number of stall cycles, and the total makespan in cycles. Then, as your final numeric answer, give the difference in makespan (in cycles) defined as\n$$\\text{makespan}(\\mathcal{S}_{B})-\\text{makespan}(\\mathcal{S}_{A}).$$\nExpress your final answer as an integer number of cycles. No rounding is required.",
            "solution": "This problem requires performing list scheduling using two different priority heuristics and comparing the resulting makespans.\n\n**1. Priority Calculation**\n\nFirst, we compute the critical-path priority $p(v)$ for each instruction $v$. The priority is the length of the longest latency-weighted path from $v$ to an exit node. We calculate this by traversing the graph backwards from the exit nodes.\n\nFor the independent instructions $P_j$, which have no successors:\n$p(P_j) = l(P_j) = 1$ for $j \\in \\{1, \\dots, 8\\}$.\n\nFor the dependent chain of instructions $C_i$:\n- $p(C_5) = l(C_5) = 4$\n- $p(C_4) = l(C_4) + p(C_5) = 4 + 4 = 8$\n- $p(C_3) = l(C_3) + p(C_4) = 4 + 8 = 12$\n- $p(C_2) = l(C_2) + p(C_3) = 4 + 12 = 16$\n- $p(C_1) = l(C_1) + p(C_2) = 4 + 16 = 20$\n\n**2. Schedule $\\mathcal{S}_A$ Construction**\n\nHeuristic for $\\mathcal{S}_A$: Prioritize by largest $p(v)$, then largest $l(v)$, then by name. An instruction issued at cycle $t$ completes at $t+l(v)$. We start at cycle $t=1$.\n\n- **Cycle 1:** Ready list: $\\{C_1, P_1, \\dots, P_8\\}$. $p(C_1)=20$ is highest. **Issue $C_1$**. (Completes at $1+4=5$)\n- **Cycle 2:** Ready list: $\\{P_1, \\dots, P_8\\}$. All have $p=1, l=1$. Choose by name. **Issue $P_1$**. (Completes at $2+1=3$)\n- **Cycle 3:** Ready list: $\\{P_2, \\dots, P_8\\}$. **Issue $P_2$**. (Completes at $3+1=4$)\n- **Cycle 4:** Ready list: $\\{P_3, \\dots, P_8\\}$. **Issue $P_3$**. (Completes at $4+1=5$)\n- **Cycle 5:** Ready list: $\\{C_2, P_4, \\dots, P_8\\}$ ($C_1$ is complete). $p(C_2)=16$ is highest. **Issue $C_2$**. (Completes at $5+4=9$)\n- **Cycle 6-8:** Issue $P_4, P_5, P_6$ sequentially. $P_6$ completes at $8+1=9$.\n- **Cycle 9:** Ready list: $\\{C_3, P_7, P_8\\}$ ($C_2$ and $P_6$ are complete). $p(C_3)=12$ is highest. **Issue $C_3$**. (Completes at $9+4=13$)\n- **Cycle 10-11:** Issue $P_7, P_8$ sequentially. $P_8$ completes at $11+1=12$.\n- **Cycle 12:** Ready list is empty. $C_3$ is not yet complete. **Stall**.\n- **Cycle 13:** Ready list: $\\{C_4\\}$ ($C_3$ is complete). **Issue $C_4$**. (Completes at $13+4=17$)\n- **Cycles 14-16:** Ready list is empty. **Stall (3 cycles)**.\n- **Cycle 17:** Ready list: $\\{C_5\\}$ ($C_4$ is complete). **Issue $C_5$**. (Completes at $17+4=21$)\n\nAll 13 instructions have been issued. The last instruction, $C_5$, completes at cycle 21. There were 4 stall cycles in total.\n$\\text{makespan}(\\mathcal{S}_A) = 21$ cycles.\n\n**3. Schedule $\\mathcal{S}_B$ Construction**\n\nHeuristic for $\\mathcal{S}_B$: Prioritize by smallest $l(v)$, then smaller $p(v)$, then by name.\n\n- **Cycle 1:** Ready list: $\\{C_1, P_1, \\dots, P_8\\}$. Smallest $l(v)$ is 1 (for all $P_j$). Among these, $p(v)$ is 1 for all. Choose by name. **Issue $P_1$**. (Completes at $1+1=2$)\n- **Cycles 2-8:** Sequentially issue $P_2, \\dots, P_8$. $P_8$ is issued at cycle 8 and completes at $8+1=9$.\n- **Cycle 9:** Ready list: $\\{C_1\\}$. **Issue $C_1$**. (Completes at $9+4=13$)\n- **Cycles 10-12:** Ready list is empty. **Stall (3 cycles)**.\n- **Cycle 13:** Ready list: $\\{C_2\\}$ ($C_1$ is complete). **Issue $C_2$**. (Completes at $13+4=17$)\n- **Cycles 14-16:** Ready list is empty. **Stall (3 cycles)**.\n- **Cycle 17:** Ready list: $\\{C_3\\}$ ($C_2$ is complete). **Issue $C_3$**. (Completes at $17+4=21$)\n- **Cycles 18-20:** Ready list is empty. **Stall (3 cycles)**.\n- **Cycle 21:** Ready list: $\\{C_4\\}$ ($C_3$ is complete). **Issue $C_4$**. (Completes at $21+4=25$)\n- **Cycles 22-24:** Ready list is empty. **Stall (3 cycles)**.\n- **Cycle 25:** Ready list: $\\{C_5\\}$ ($C_4$ is complete). **Issue $C_5$**. (Completes at $25+4=29$)\n\nAll 13 instructions have been issued. The last instruction, $C_5$, completes at cycle 29. There were 12 stall cycles in total.\n$\\text{makespan}(\\mathcal{S}_B) = 29$ cycles.\n\n**4. Final Calculation**\n\nThe difference in makespan is:\n$\\text{makespan}(\\mathcal{S}_B) - \\text{makespan}(\\mathcal{S}_A) = 29 - 21 = 8$ cycles.",
            "answer": "$$\\boxed{8}$$"
        },
        {
            "introduction": "A scheduler's effectiveness is fundamentally constrained by the accuracy of its dependency graph, especially concerning memory operations. This practice  explores the critical impact of memory aliasing, where the compiler is uncertain if two pointers might refer to the same memory location. You will construct and compare two schedules: one based on an 'optimistic' assumption that they do not alias, and one on a 'pessimistic' assumption that they might, revealing how a single point of uncertainty can introduce new dependencies, propagate stalls, and significantly lengthen the final schedule.",
            "id": "3650816",
            "problem": "A compiler backend targets a statically scheduled Very Long Instruction Word (VLIW) architecture but emits a single-issue schedule to simplify analysis. For a single basic block, consider the following instruction sequence and machine model. The goal is to perform list scheduling twice, first under optimistic memory alias assumptions and then under pessimistic memory alias assumptions, and then to quantify how these assumptions change the schedule. Finally, compute a scalar summary of the divergence between these schedules.\n\nMachine model and scheduling policy:\n- The processor can issue exactly one instruction per cycle. Each issued instruction occupies the single issue slot at its start cycle; while its result latency elapses, the processor may issue other ready instructions in subsequent cycles.\n- Functional unit latencies are fixed: load has latency $3$, store has latency $2$, multiply has latency $3$, and add has latency $1$.\n- A consumer may not be issued until all of its producer results are available; i.e., an instruction with operands becomes ready only when each required operand has been produced and the corresponding latency has elapsed.\n- List scheduling is used with priority equal to the longest weighted path to the exit (critical-path length), computed as the node latency plus the maximum of the successor priorities. When multiple ready instructions have equal priority, break ties by earlier program order.\n- There is a single memory unit; however, because the machine is single-issue, resource conflicts are subsumed by the issue width constraint.\n\nProgram and dependencies:\n- Program order (from first to last): $L_{A}$, $L_{B}$, $M_{1}$, $S_{P}$, $L_{C}$, $M_{2}$, $A$.\n- Semantics:\n  - $L_{A}$: load $a \\leftarrow *p$.\n  - $L_{B}$: load $b \\leftarrow *q$.\n  - $M_{1}$: multiply $x \\leftarrow a \\times b$.\n  - $S_{P}$: store $*p \\leftarrow x$.\n  - $L_{C}$: load $c \\leftarrow *r$.\n  - $M_{2}$: multiply $y \\leftarrow c \\times x$.\n  - $A$: add $z \\leftarrow y + b$.\n- Proven and unknown aliasing facts from alias analysis:\n  - $q$ is proven not to alias with $p$ or $r$.\n  - $p$ may alias with $r$ (unknown alias relationship).\n- True data dependences:\n  - $L_{A} \\rightarrow M_{1}$, $L_{B} \\rightarrow M_{1}$, $M_{1} \\rightarrow S_{P}$, $L_{C} \\rightarrow M_{2}$, $M_{1} \\rightarrow M_{2}$, $M_{2} \\rightarrow A$, $L_{B} \\rightarrow A$.\n- Memory ordering dependences under the two alias assumptions:\n  - Optimistic assumption: do not introduce extra dependences beyond proven ones; in particular, do not constrain $S_{P}$ and $L_{C}$ because $p$ may alias $r$ is unknown.\n  - Pessimistic assumption: introduce a store-to-load dependence $S_{P} \\rightarrow L_{C}$ due to the may-alias between $p$ and $r$. Treat this dependence as requiring the store to complete before the load can be issued.\n\nTasks:\n1. Using the given latencies and dependences, compute the list-scheduling priorities (critical-path lengths to exit) for each instruction under both the optimistic and pessimistic alias assumptions. Then construct the single-issue schedule for each case by selecting, at each cycle starting from cycle $0$, the highest-priority ready instruction (breaking ties by earlier program order), and advancing time by $1$ per issued instruction. An instruction produces its result after its latency has elapsed.\n2. For each schedule, record the start cycle for each instruction and the finish cycle for the last instruction. Define a stall cycle as any cycle in which no instruction is issued because no instruction is ready.\n3. Define the schedule divergence $D$ as the sum over all instructions of the absolute difference between their start cycles under the optimistic and pessimistic schedules. Define the stall propagation difference $\\Delta S$ as the difference between the number of stall cycles in the pessimistic schedule and in the optimistic schedule.\n4. Compute the scalar summary $\\Gamma$ defined by $\\Gamma = \\frac{D}{\\Delta S}$. Round your answer to four significant figures.",
            "solution": "The problem requires a detailed analysis of list scheduling under two different memory aliasing assumptions: optimistic and pessimistic. We must first validate the problem, then compute the schedules, and finally calculate the specified metrics $D$, $\\Delta S$, and $\\Gamma$.\n\n**Instruction and Latency Data**\nThe instructions are denoted $L_A, L_B, M_1, S_P, L_C, M_2, A$. Their respective latencies are:\n- Load ($L_A, L_B, L_C$): $3$ cycles\n- Store ($S_P$): $2$ cycles\n- Multiply ($M_1, M_2$): $3$ cycles\n- Add ($A$): $1$ cycle\n\n**Part 1: Optimistic Scheduling**\n\n**Priorities (Optimistic)**\nUnder the optimistic assumption, there is no dependence between $S_P$ and $L_C$. We compute priorities ($P(I)$) by traversing the DDG backwards from the leaves:\n- $P(A) = \\text{latency}(A) = 1$\n- $P(S_P) = \\text{latency}(S_P) = 2$\n- $P(M_2) = \\text{latency}(M_2) + P(A) = 3 + 1 = 4$\n- $P(M_1) = \\text{latency}(M_1) + \\max(P(S_P), P(M_2)) = 3 + \\max(2, 4) = 7$\n- $P(L_C) = \\text{latency}(L_C) + P(M_2) = 3 + 4 = 7$\n- $P(L_B) = \\text{latency}(L_B) + \\max(P(M_1), P(A)) = 3 + \\max(7, 1) = 10$\n- $P(L_A) = \\text{latency}(L_A) + P(M_1) = 3 + 7 = 10$\n\n**List Scheduling (Optimistic)**\nWe schedule instructions cycle-by-cycle starting from cycle $C=0$, using priorities and program order for tie-breaking.\n- **$C=0$**: Ready: {$L_A, L_B, L_C$}. Priorities: $10, 10, 7$. Tie between $L_A$ and $L_B$. Choose $L_A$ (earlier program order). Issue $L_A$. (Finishes at $0+3=3$)\n- **$C=1$**: Ready: {$L_B, L_C$}. Priorities: $10, 7$. Choose $L_B$. Issue $L_B$. (Finishes at $1+3=4$)\n- **$C=2$**: Ready: {$L_C$}. Priority: $7$. Issue $L_C$. (Finishes at $2+3=5$)\n- **$C=3$**: **Stall**. $M_1$ needs $L_A$ (finishes at 3) and $L_B$ (finishes at 4).\n- **$C=4$**: Ready: {$M_1$}. $L_A$ and $L_B$ are done. Issue $M_1$. (Finishes at $4+3=7$)\n- **$C=5,6$**: **Stall**. $M_2$ needs $L_C$ (done at 5) and $M_1$ (finishes at 7).\n- **$C=7$**: Ready: {$S_P, M_2$}. $M_1$ and $L_C$ are done. Priorities: $2, 4$. Choose $M_2$. Issue $M_2$. (Finishes at $7+3=10$)\n- **$C=8$**: Ready: {$S_P$}. Issue $S_P$. (Finishes at $8+2=10$)\n- **$C=9$**: **Stall**. $A$ needs $M_2$ (finishes at 10).\n- **$C=10$**: Ready: {$A$}. $M_2$ is done. Issue $A$. (Finishes at $10+1=11$)\n\nThe schedule finishes at cycle $11$.\nOptimistic start times ($S_{opt}$): $L_A(0), L_B(1), L_C(2), M_1(4), M_2(7), S_P(8), A(10)$.\nNumber of stall cycles: $11$ total cycles - $7$ instructions = $4$.\n\n**Part 2: Pessimistic Scheduling**\n\n**Priorities (Pessimistic)**\nThe pessimistic assumption adds a dependence $S_P \\rightarrow L_C$. This changes the priorities.\n- $P(A) = 1$\n- $P(M_2) = \\text{latency}(M_2) + P(A) = 3 + 1 = 4$\n- $P(L_C) = \\text{latency}(L_C) + P(M_2) = 3 + 4 = 7$\n- $P(S_P) = \\text{latency}(S_P) + P(L_C) = 2 + 7 = 9$\n- $P(M_1) = \\text{latency}(M_1) + \\max(P(S_P), P(M_2)) = 3 + \\max(9, 4) = 12$\n- $P(L_B) = \\text{latency}(L_B) + \\max(P(M_1), P(A)) = 3 + \\max(12, 1) = 15$\n- $P(L_A) = \\text{latency}(L_A) + P(M_1) = 3 + 12 = 15$\n\n**List Scheduling (Pessimistic)**\n- **$C=0$**: Ready: {$L_A, L_B$}. $L_C$ is not ready. Priorities: $15, 15$. Choose $L_A$. Issue $L_A$. (Finishes at $0+3=3$)\n- **$C=1$**: Ready: {$L_B$}. Priority: $15$. Issue $L_B$. (Finishes at $1+3=4$)\n- **$C=2,3$**: **Stall**.\n- **$C=4$**: Ready: {$M_1$}. Issue $M_1$. (Finishes at $4+3=7$)\n- **$C=5,6$**: **Stall**.\n- **$C=7$**: Ready: {$S_P$}. Issue $S_P$. (Finishes at $7+2=9$)\n- **$C=8$**: **Stall**.\n- **$C=9$**: Ready: {$L_C$}. $S_P$ is done. Issue $L_C$. (Finishes at $9+3=12$)\n- **$C=10,11$**: **Stall**.\n- **$C=12$**: Ready: {$M_2$}. $L_C$ is done. Issue $M_2$. (Finishes at $12+3=15$)\n- **$C=13,14$**: **Stall**.\n- **$C=15$**: Ready: {$A$}. $M_2$ is done. Issue $A$. (Finishes at $15+1=16$)\n\nThe schedule finishes at cycle $16$.\nPessimistic start times ($S_{pess}$): $L_A(0), L_B(1), M_1(4), S_P(7), L_C(9), M_2(12), A(15)$.\nNumber of stall cycles: $16$ total cycles - $7$ instructions = $9$.\n\n**Part 3: Schedule Divergence and Stall Difference**\n\nThe schedule divergence, $D$, is the sum of absolute differences in start cycles:\n$D = |S_{opt}(L_A) - S_{pess}(L_A)| + \\dots$\n$D = |0-0| + |1-1| + |4-4| + |8-7| + |2-9| + |7-12| + |10-15|$\n$D = 0 + 0 + 0 + 1 + 7 + 5 + 5 = 18$\n\nThe stall propagation difference, $\\Delta S$, is the difference in stall counts:\n$\\Delta S = \\text{stalls}_{pess} - \\text{stalls}_{opt} = 9 - 4 = 5$.\n\n**Part 4: Scalar Summary $\\Gamma$**\nThe scalar summary $\\Gamma$ is the ratio of $D$ to $\\Delta S$.\n$$\n\\Gamma = \\frac{D}{\\Delta S} = \\frac{18}{5} = 3.6\n$$\nRounding to four significant figures gives $3.600$.",
            "answer": "$$\n\\boxed{3.600}\n$$"
        },
        {
            "introduction": "Real-world processors execute instructions in parallel, but performance is always limited by either data dependencies or resource availability. This exercise  moves to a more complex dual-issue processor to investigate this trade-off. By analyzing the performance impact of adding an extra ALU, you will identify the conditions under which a schedule is resource-bound versus when it is limited by the inherent latency of its critical path, demonstrating the principle of diminishing returns in processor design.",
            "id": "3650798",
            "problem": "Consider a basic block whose data dependences are captured by the following Directed Acyclic Graph (DAG). There is a single memory-derived chain consisting of $6$ iterations; at iteration $i \\in \\{1,\\dots,6\\}$, an Arithmetic Logic Unit (ALU) instruction $A_i$ computes an address from the value produced by the preceding load, and then a load $L_i$ reads from that address. Concretely, $A_1$ depends on an initial value $V_0$ that is available at time $0$, $L_1$ depends on $A_1$, $A_2$ depends on $L_1$, $L_2$ depends on $A_2$, and so on, ending with $L_6$ depending on $A_6$. In addition to this chain, there are $20$ independent ALU-only instructions $X_1,\\dots,X_{20}$ that are all ready at time $0$ and are not on the memory-derived chain; their results are only needed at the end of the basic block.\n\nMachine model and constraints:\n- The machine is a dual-issue processor: in each cycle at most $W=2$ instructions can be issued.\n- There is a single fully pipelined memory unit with capacity $R_M=1$ for loads, and each load has latency $L_M=3$ cycles (a load issued at cycle $t$ produces its result at cycle $t+3$).\n- There are $R_A$ fully pipelined ALU units. Each ALU instruction has latency $L_A=1$ cycle (an ALU issued at cycle $t$ produces its result at cycle $t+1$).\n- Distinct functional units can be used in the same cycle if issue width allows and operands are ready. An instruction can issue only when all its operands are available. Results become available exactly after their stated latencies.\n- All resources are exclusive-use per issue: a resource used by one instruction for issue in a cycle cannot be used by another instruction of the same type in that cycle.\n\nAssume a standard greedy list scheduling heuristic that always respects data dependences and resource capacities. You will analyze two configurations: baseline with $R_A=1$ ALU and upgraded with $R_A=2$ ALUs. Use only the definitions above and the readiness/latency semantics to determine the makespan under each configuration; do not assume any unstated optimizations.\n\nTask:\n- Compute the speedup $S$ defined as the ratio of the baseline makespan with $R_A=1$ to the upgraded makespan with $R_A=2$. Provide $S$ as a single reduced fraction. No rounding is required.\n- In your reasoning, quantify how sensitive the schedule length is to the ALU resource in this example and identify the true bottleneck that limits the realized speedup, but the only final reported result must be the single numerical value for $S$ as specified above.",
            "solution": "To solve this problem, we must determine the makespan (total execution time) for two machine configurations and then compute the speedup. The makespan is determined by the later of two events: the completion of the last instruction on the critical path, or the completion of the last independent instruction. We assume scheduling starts at cycle $t=1$. An instruction issued at cycle $t$ with latency $L$ completes at the end of cycle $t+L-1$.\n\nThe critical path is the dependent chain of instructions: $A_1 \\rightarrow L_1 \\rightarrow A_2 \\rightarrow \\dots \\rightarrow L_6$. The time to traverse one $A_i \\rightarrow L_i$ link is determined by the sum of their latencies, $L_A + L_M = 1 + 3 = 4$ cycles.\n\n**Case 1: Baseline Machine ($R_A=1$, $R_M=1$, $W=2$)**\n\nIn this configuration, the machine is resource-bound by the single ALU. There are 6 ALU instructions ($A_i$) on the critical path and 20 independent ALU instructions ($X_j$), for a total of 26 ALU instructions. Since the machine can only issue one ALU instruction per cycle ($R_A=1$), the schedule requires at least 26 cycles for the ALU operations alone. This provides a lower bound for the makespan.\n\nLet's schedule the instructions:\n- The critical path instructions are prioritized. The issue time for $A_i$ is determined by the completion of $L_{i-1}$, and the issue time for $L_i$ by the completion of $A_i$.\n  - $T_{issue}(A_1) = 1$. Result is ready at cycle $1+L_A=2$.\n  - $T_{issue}(L_1) = 2$. Result is ready at cycle $2+L_M=5$.\n  - $T_{issue}(A_2) = 5$. Result is ready at cycle $5+L_A=6$.\n  - $T_{issue}(L_2) = 6$. Result is ready at cycle $6+L_M=9$.\n- This pattern continues, with $T_{issue}(A_i) = T_{issue}(A_{i-1}) + 4$. The final chain instruction, $L_6$, is issued at $T_{issue}(L_6)=22$ and completes at the end of cycle $22+3-1 = 24$.\n- Now we account for the 20 independent $X_j$ instructions. With one ALU, they must be scheduled in cycles where the ALU is not used by an $A_i$ instruction.\n  - 6 cycles are used to issue $A_1, \\dots, A_6$.\n  - In the 6 cycles where $L_1, \\dots, L_6$ are issued, the ALU is free, so 6 $X_j$ can be co-issued.\n  - In the gaps between the issue of an $L_i$ and the issue of $A_{i+1}$, there are $L_M-L_A = 3-1=2$ cycles where the ALU is free. This occurs 5 times (for $L_1, \\dots, L_5$), allowing $5 \\times 2 = 10$ more $X_j$ to be issued.\n- By cycle 22, we have issued $6+10=16$ of the $X_j$ instructions. The remaining $20-16=4$ are issued after cycle 22.\n- $T_{issue}(X_{17})=23$, $T_{issue}(X_{18})=24$, $T_{issue}(X_{19})=25$, $T_{issue}(X_{20})=26$.\n- The last instruction, $X_{20}$, completes at the end of cycle $26+1-1=26$.\n- The makespan is the maximum of the completion times: $T_1 = \\max(24, 26) = 26$.\n\n**Case 2: Upgraded Machine ($R_A=2$, $R_M=1$, $W=2$)**\n\nWith two ALUs, the machine is no longer bottlenecked by ALU resources. The total number of ALU slots needed is 26, and with 2 ALUs, this would take $\\lceil 26/2 \\rceil = 13$ cycles if there were no other constraints. The performance is now limited by the data dependencies of the critical path.\n\n- The scheduling of the critical path $A_i \\rightarrow L_i$ chain is unchanged because it is dictated by data dependency, not resource availability. As before, $L_6$ is issued at cycle 22 and completes at the end of cycle 24. This sets a lower bound on the makespan.\n- We must schedule the 20 independent $X_j$ instructions. With two ALUs and dual issue, these can be scheduled much more quickly in parallel with the critical path.\n  - At cycle $t=1$, we can issue $\\{A_1, X_1\\}$.\n  - At cycle $t=2$, we can issue $\\{L_1, X_2\\}$.\n  - In gap cycles like $t=3, 4$, we can issue two ALU instructions: $\\{X_3, X_4\\}$ and $\\{X_5, X_6\\}$.\n- Let's track the issue of the $X_j$ instructions:\n  - By the end of cycle 4, we've issued $A_1$, $L_1$, and 6 $X_j$.\n  - This pattern continues. By cycle 14 ($T_{issue}(L_4)$), all 20 $X_j$ instructions will have been issued.\n  - The last independent instruction, $X_{20}$, is issued at cycle 14 and completes at the end of cycle $14+1-1=14$.\n- All independent work is finished by cycle 14, but the critical path execution continues.\n- The makespan is the maximum of the completion times: $T_2 = \\max(\\text{completion of } L_6, \\text{completion of } X_{20}) = \\max(24, 14) = 24$.\n\n**Speedup Calculation**\n\nThe speedup $S$ is the ratio of the baseline makespan to the upgraded makespan.\n$$S = \\frac{T_1}{T_2} = \\frac{26}{24}$$\nReducing the fraction gives:\n$$S = \\frac{13}{12}$$",
            "answer": "$$\\boxed{\\frac{13}{12}}$$"
        }
    ]
}