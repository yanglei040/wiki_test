{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we will investigate a classic synergistic interaction between Dead Code Elimination (DCE) and Instruction Scheduling. This exercise demonstrates how an early \"cleanup\" pass can have a profound impact on the performance of a later resource management pass. By calculating the critical path length of a code block under different phase orderings, you will quantify how removing unnecessary instructions simplifies the program's dependency graph, enabling the scheduler to produce a much more efficient execution plan .",
            "id": "3662593",
            "problem": "Consider phase ordering in a compiler between Dead Code Elimination (DCE) and instruction scheduling. Dead Code Elimination (DCE) removes computations that do not contribute to any externally observable program state under the assumptions of no side effects and non-volatile memory. Instruction scheduling (IS) orders instructions subject to data dependences but does not alter latencies or dependences. Model the dependence structure of a basic block as a Directed Acyclic Graph (DAG) whose nodes are instructions and whose edges encode true data dependences. The critical path length $\\chi(P)$ of a program block $P$ is defined as the maximum, over all source-to-sink paths in the DAG, of the sum of the instruction latencies along that path.\n\nLet $P$ be the following single basic block (Static Single Assignment (SSA) form), consisting of two independent chains. All operations are pure and memory reads are non-volatile, in-bounds, and cannot trap. Only the variable $y$ is live at block exit; all other temporaries are dead (never used after the block):\n\nLive chain:\n- $I_{1}: r_{a} \\leftarrow \\mathrm{ld}(a)$ with latency $5$,\n- $I_{2}: r_{b} \\leftarrow \\mathrm{ld}(b)$ with latency $5$,\n- $I_{3}: s_{1} \\leftarrow r_{a} + r_{b}$ with latency $1$,\n- $I_{4}: m_{1} \\leftarrow s_{1} \\times c$ with latency $3$,\n- $I_{5}: y \\leftarrow m_{1} + d$ with latency $1$ (live).\n\nDead chain:\n- $I_{6}: r_{e} \\leftarrow \\mathrm{ld}(e)$ with latency $5$,\n- $I_{7}: t_{1} \\leftarrow r_{e} \\times f$ with latency $3$,\n- $I_{8}: t_{2} \\leftarrow t_{1} / g$ with latency $10$,\n- $I_{9}: t_{3} \\leftarrow t_{2} + h$ with latency $1$,\n- $I_{10}: t \\leftarrow t_{3} - k$ with latency $1$ (dead; $t$ is unused).\n\nAssume the scheduler constructs its DAG over the currently present instructions and that the machine has sufficient resources such that the schedule length is lower-bounded by $\\chi(P)$ and cannot be shorter than $\\chi(P)$. Consider two phase orders:\n- $O_{\\mathrm{Sched}\\rightarrow \\mathrm{DCE}}$: perform instruction scheduling first on the original block, then run DCE.\n- $O_{\\mathrm{DCE}\\rightarrow \\mathrm{Sched}}$: run DCE first on the original block, then perform instruction scheduling.\n\nCompute the reduction $R$, defined as\n$$\nR \\equiv \\chi_{O_{\\mathrm{Sched}\\rightarrow \\mathrm{DCE}}}(P) - \\chi_{O_{\\mathrm{DCE}\\rightarrow \\mathrm{Sched}}}(P),\n$$\nand express your final answer in cycles as an exact integer (no rounding).",
            "solution": "We start from the core definitions. The instruction-level dependence structure of the block is a Directed Acyclic Graph (DAG) in which each instruction is a node weighted by its latency, and edges represent true data dependences. The critical path length $\\chi(P)$ is the maximum, over all source-to-sink paths in the DAG, of the sum of the node latencies along the path. Instruction scheduling (IS) respects dependences and latencies; with sufficient resources, the scheduled length cannot be shorter than the DAG’s critical path. Dead Code Elimination (DCE) removes nodes whose results do not contribute to any live output and that have no side effects, thereby potentially reducing the DAG and its critical path.\n\nWe analyze the two independent chains.\n\nLive chain:\n- $I_{1}$ ($5$) and $I_{2}$ ($5$) both feed $I_{3}$ ($1$). Since $I_{3}$ depends on both $r_{a}$ and $r_{b}$, the arrival time to $I_{3}$ is the maximum of the arrival times from its predecessors. Both $I_{1}$ and $I_{2}$ are sources, so the earliest time their values are ready is $5$ for each. Thus, the earliest time $s_{1}$ is ready at $I_{3}$ is $5 + 1 = 6$.\n- $I_{4}$ ($3$) depends on $I_{3}$, so its completion time along the longest path is $6 + 3 = 9$.\n- $I_{5}$ ($1$) depends on $I_{4}$, so its completion time along the longest path is $9 + 1 = 10$.\n\nTherefore, the critical path length along the live chain is\n$$\n\\chi_{\\text{live}} = 10.\n$$\n\nDead chain:\nThis chain is linear, so we sum the latencies:\n- $I_{6}$ ($5$) $\\rightarrow$ $I_{7}$ ($3$): $5 + 3 = 8$,\n- $\\rightarrow I_{8}$ ($10$): $8 + 10 = 18$,\n- $\\rightarrow I_{9}$ ($1$): $18 + 1 = 19$,\n- $\\rightarrow I_{10}$ ($1$): $19 + 1 = 20$.\n\nThus,\n$$\n\\chi_{\\text{dead}} = 20.\n$$\n\nSince the two chains are independent, the overall DAG’s critical path $\\chi(P)$ is the maximum of the two chain critical paths present in the graph at the time of scheduling.\n\nFor the phase order $O_{\\mathrm{Sched}\\rightarrow \\mathrm{DCE}}$, scheduling is run first over the original block, which includes both chains. The scheduler’s DAG therefore includes the dead chain. The critical path at scheduling time is\n$$\n\\chi_{O_{\\mathrm{Sched}\\rightarrow \\mathrm{DCE}}}(P) = \\max\\left(\\chi_{\\text{live}}, \\chi_{\\text{dead}}\\right) = \\max(10, 20) = 20.\n$$\n\nFor the phase order $O_{\\mathrm{DCE}\\rightarrow \\mathrm{Sched}}$, Dead Code Elimination runs first. All operations in the dead chain are pure and the chain’s result $t$ is unused, with memory reads non-volatile and non-trapping, so DCE removes $I_{6}$ through $I_{10}$. The scheduler then operates on the reduced block containing only the live chain. The critical path at scheduling time becomes\n$$\n\\chi_{O_{\\mathrm{DCE}\\rightarrow \\mathrm{Sched}}}(P) = \\chi_{\\text{live}} = 10.\n$$\n\nBy definition, the reduction $R$ is\n$$\nR \\equiv \\chi_{O_{\\mathrm{Sched}\\rightarrow \\mathrm{DCE}}}(P) - \\chi_{O_{\\mathrm{DCE}\\rightarrow \\mathrm{Sched}}}(P) = 20 - 10 = 10.\n$$\n\nHence, performing Dead Code Elimination before instruction scheduling reduces the critical path length available to the scheduler by $10$ cycles.",
            "answer": "$$\\boxed{10}$$"
        },
        {
            "introduction": "Building on the concept of enabling interactions, this next practice explores a more subtle relationship between algebraic simplification and pattern matching. Code can often be written in many syntactically different ways that are all semantically equivalent. This exercise shows how a `Canonicalize` pass, which normalizes expressions into a standard form, can expose hidden optimization opportunities for a subsequent `PatternMatch` pass that would otherwise miss them .",
            "id": "3662645",
            "problem": "Consider an optimizing compiler that operates on an Abstract Syntax Tree (AST) over a simple expression language with three kinds of nodes: variables, integer constants, and binary operators $\\mathrm{Add}$ and $\\mathrm{Mul}$. A compiler pass $O_{\\mathrm{Canonicalize}}$ produces a canonical form for expressions by applying equational axioms that respect semantics. A compiler pass $O_{\\mathrm{PatternMatch}}$ applies a single peephole rewrite driven by a pattern $P$.\n\nFoundational base and core definitions:\n- Expressions are trees over the signature $\\{\\mathrm{Add}, \\mathrm{Mul}\\}$ with leaves drawn from variables and integer constants.\n- The pass $O_{\\mathrm{Canonicalize}}$ applies the following confluent, semantics-preserving rules to saturation at every node (post-order), thereby defining a canonicalization function $C$:\n  1. Commutativity normalization: for $\\mathrm{Add}(u,v)$ and $\\mathrm{Mul}(u,v)$, recursively canonicalize $u$ to $u'$ and $v$ to $v'$, then sort the pair $(u',v')$ so that $u' \\preceq v'$ with respect to a fixed total order $\\preceq$ defined below.\n  2. Neutral-element elimination: $\\mathrm{Add}(u,0) \\to u$, $\\mathrm{Add}(0,u) \\to u$, $\\mathrm{Mul}(u,1) \\to u$, $\\mathrm{Mul}(1,u) \\to u$, and $\\mathrm{Mul}(u,0) \\to 0$, $\\mathrm{Mul}(0,u) \\to 0$.\n  3. Constant folding for binary nodes with two constant children: $\\mathrm{Add}(m,n) \\to m+n$ and $\\mathrm{Mul}(m,n) \\to m \\cdot n$ when $m$ and $n$ are integer constants.\n- The total order $\\preceq$ over canonicalized expressions is defined as follows: constants are ordered by numeric value; variables are ordered lexicographically by name; compound nodes are ordered lexicographically by the triple $(\\mathrm{op}, u', v')$, where $\\mathrm{op} \\in \\{\\mathrm{Add}, \\mathrm{Mul}\\}$ is ordered by the name string, and $u',v'$ are the canonicalized children compared recursively. This determines a deterministic operand order under commutativity.\n- The pass $O_{\\mathrm{PatternMatch}}$ attempts to match, at the root of each top-level expression only, the single pattern $P \\colon \\mathrm{Add}(E,E)$, where $E$ denotes any subtree. When the pattern matches, a rewrite would be eligible (for example, to $\\mathrm{Mul}(2,E)$), but for this question only the number of successful matches matters.\n- Define $m(P)$ for a given sequence of program expressions as the number of top-level roots that match $P$ during a single pass of $O_{\\mathrm{PatternMatch}}$.\n\nTwo phase orders are considered:\n- Order $\\mathcal{A}$: first apply $O_{\\mathrm{Canonicalize}}$ to every top-level expression, then apply $O_{\\mathrm{PatternMatch}}$ once at the root of each top-level expression in the resulting set; let the resulting count be $m_{\\mathcal{A}}(P)$.\n- Order $\\mathcal{B}$: first apply $O_{\\mathrm{PatternMatch}}$ once at the root of each original top-level expression, then apply $O_{\\mathrm{Canonicalize}}$; let the resulting count be $m_{\\mathcal{B}}(P)$.\n\nConsider the following list of $12$ top-level Intermediate Representation (IR) expressions (each represented in prefix form as $\\mathrm{Add}(\\cdot,\\cdot)$ or $\\mathrm{Mul}(\\cdot,\\cdot)$), where $a,b,c,d,f,g,h,i,j,k,m,p$ are distinct variables and $0,1,2,3$ are integer constants:\n- $E_{1} = \\mathrm{Add}(\\mathrm{Mul}(a,b),\\mathrm{Mul}(b,a))$.\n- $E_{2} = \\mathrm{Add}(\\mathrm{Add}(a,0),a)$.\n- $E_{3} = \\mathrm{Add}(\\mathrm{Mul}(c,1),c)$.\n- $E_{4} = \\mathrm{Add}(\\mathrm{Add}(a,b),\\mathrm{Add}(b,a))$.\n- $E_{5} = \\mathrm{Add}(\\mathrm{Mul}(2,d),\\mathrm{Add}(d,d))$.\n- $E_{6} = \\mathrm{Add}(\\mathrm{Mul}(0,p),p)$.\n- $E_{7} = \\mathrm{Add}(\\mathrm{Mul}(f,1),\\mathrm{Mul}(1,f))$.\n- $E_{8} = \\mathrm{Add}(\\mathrm{Add}(g,1),\\mathrm{Add}(1,g))$.\n- $E_{9} = \\mathrm{Add}(\\mathrm{Mul}(3,h),\\mathrm{Mul}(h,3))$.\n- $E_{10} = \\mathrm{Add}(\\mathrm{Add}(i,j),\\mathrm{Add}(i,j))$.\n- $E_{11} = \\mathrm{Add}(\\mathrm{Add}(k,0),\\mathrm{Add}(0,k))$.\n- $E_{12} = \\mathrm{Add}(\\mathrm{Mul}(1,\\mathrm{Add}(m,0)),\\mathrm{Add}(0,\\mathrm{Add}(1,m)))$.\n\nCompute the pair $\\big(m_{\\mathcal{A}}(P), m_{\\mathcal{B}}(P)\\big)$ as defined above for this program, and present the final result as a row matrix. No rounding is required. Do not include any units in your answer.",
            "solution": "The problem requires the computation of a pair of values, $(m_{\\mathcal{A}}(P), m_{\\mathcal{B}}(P))$, which represent the number of successful pattern matches for a pattern $P \\colon \\mathrm{Add}(E,E)$ under two different compiler phase orderings, $\\mathcal{A}$ and $\\mathcal{B}$. The pattern $P$ matches a top-level expression if it is of the form $\\mathrm{Add}(E,E)$, meaning it is an addition of two identical subtrees.\n\nThe two phase orders are:\n- Order $\\mathcal{A}$: Apply the canonicalization pass $O_{\\mathrm{Canonicalize}}$ to all expressions, then apply the pattern matching pass $O_{\\mathrm{PatternMatch}}$. The number of matches is $m_{\\mathcal{A}}(P)$.\n- Order $\\mathcal{B}$: Apply the pattern matching pass $O_{\\mathrm{PatternMatch}}$ to all original expressions, then apply the canonicalization pass $O_{\\mathrm{Canonicalize}}$. The number of matches is $m_{\\mathcal{B}}(P)$.\n\nLet $C(e)$ denote the canonical form of an expression $e$ after applying the pass $O_{\\mathrm{Canonicalize}}$. The rules for canonicalization are: commutativity with sorting based on a total order $\\preceq$, neutral-element elimination, and constant folding. The sorting order $\\preceq$ is defined as constants (by value) $\\prec$ variables (lexicographically) $\\prec$ compound nodes. Compound nodes are ordered lexicographically by operator name (i.e., $\\mathrm{Add} \\prec \\mathrm{Mul}$) and then recursively by their children.\n\nFirst, we compute $m_{\\mathcal{B}}(P)$ for Order $\\mathcal{B}$.\nIn this order, the pattern matching pass $O_{\\mathrm{PatternMatch}}$ is applied first to the original list of $12$ expressions. The count $m_{\\mathcal{B}}(P)$ is the number of expressions that syntactically match $\\mathrm{Add}(E,E)$ before any transformations.\n\nWe examine each expression $E_i$ for $i \\in \\{1, \\dots, 12\\}$:\n- $E_1, \\dots, E_9$: The two children of the root $\\mathrm{Add}$ node are not syntactically identical. For example, in $E_1 = \\mathrm{Add}(\\mathrm{Mul}(a,b),\\mathrm{Mul}(b,a))$, the children $\\mathrm{Mul}(a,b)$ and $\\mathrm{Mul}(b,a)$ are different abstract syntax trees. No match for these.\n- $E_{10} = \\mathrm{Add}(\\mathrm{Add}(i,j),\\mathrm{Add}(i,j))$: The left child, $\\mathrm{Add}(i,j)$, is syntactically identical to the right child, $\\mathrm{Add}(i,j)$. This is a match.\n- $E_{11} = \\mathrm{Add}(\\mathrm{Add}(k,0),\\mathrm{Add}(0,k))$: The children $\\mathrm{Add}(k,0)$ and $\\mathrm{Add}(0,k)$ are not syntactically identical. No match.\n- $E_{12} = \\mathrm{Add}(\\mathrm{Mul}(1,\\mathrm{Add}(m,0)),\\mathrm{Add}(0,\\mathrm{Add}(1,m)))$: The children are not syntactically identical. No match.\n\nOnly one expression, $E_{10}$, matches the pattern $P$ in its original form. Therefore, the count for Order $\\mathcal{B}$ is $m_{\\mathcal{B}}(P) = 1$.\n\nSecond, we compute $m_{\\mathcal{A}}(P)$ for Order $\\mathcal{A}$.\nIn this order, we first apply the canonicalization pass $O_{\\mathrm{Canonicalize}}$ to each expression $E_i$ to obtain its canonical form $C(E_i)$. Then, we check if $C(E_i)$ matches the pattern $\\mathrm{Add}(E,E)$.\n\nWe analyze the canonical form of each expression:\n- $E_1 = \\mathrm{Add}(\\mathrm{Mul}(a,b),\\mathrm{Mul}(b,a))$: $C(\\mathrm{Mul}(a,b)) = \\mathrm{Mul}(a,b)$ (assuming $a \\preceq b$). $C(\\mathrm{Mul}(b,a))$ also becomes $\\mathrm{Mul}(a,b)$ after sorting. So, $C(E_1) = \\mathrm{Add}(\\mathrm{Mul}(a,b),\\mathrm{Mul}(a,b))$. This matches $P$.\n- $E_2 = \\mathrm{Add}(\\mathrm{Add}(a,0),a)$: $C(\\mathrm{Add}(a,0)) = a$ by neutral-element elimination. So, $C(E_2) = \\mathrm{Add}(a,a)$. This matches $P$.\n- $E_3 = \\mathrm{Add}(\\mathrm{Mul}(c,1),c)$: $C(\\mathrm{Mul}(c,1)) = c$ by neutral-element elimination. So, $C(E_3) = \\mathrm{Add}(c,c)$. This matches $P$.\n- $E_4 = \\mathrm{Add}(\\mathrm{Add}(a,b),\\mathrm{Add}(b,a))$: $C(\\mathrm{Add}(a,b)) = \\mathrm{Add}(a,b)$ (assuming $a \\preceq b$). $C(\\mathrm{Add}(b,a))$ becomes $\\mathrm{Add}(a,b)$ after sorting. So, $C(E_4) = \\mathrm{Add}(\\mathrm{Add}(a,b),\\mathrm{Add}(a,b))$. This matches $P$.\n- $E_5 = \\mathrm{Add}(\\mathrm{Mul}(2,d),\\mathrm{Add}(d,d))$: The canonical children are $C(\\mathrm{Mul}(2,d)) = \\mathrm{Mul}(2,d)$ and $C(\\mathrm{Add}(d,d)) = \\mathrm{Add}(d,d)$. According to $\\preceq$, $\\mathrm{Add} \\prec \\mathrm{Mul}$, so $\\mathrm{Add}(d,d) \\preceq \\mathrm{Mul}(2,d)$. Sorting the children gives $C(E_5) = \\mathrm{Add}(\\mathrm{Add}(d,d), \\mathrm{Mul}(2,d))$. The children are not identical. No match.\n- $E_6 = \\mathrm{Add}(\\mathrm{Mul}(0,p),p)$: $C(\\mathrm{Mul}(0,p)) = 0$ by absorbing-element rule. The expression becomes $\\mathrm{Add}(0,p)$, which canonicalizes to $p$. The final form is not an $\\mathrm{Add}$ node. No match.\n- $E_7 = \\mathrm{Add}(\\mathrm{Mul}(f,1),\\mathrm{Mul}(1,f))$: $C(\\mathrm{Mul}(f,1)) = f$ and $C(\\mathrm{Mul}(1,f)) = f$. So, $C(E_7) = \\mathrm{Add}(f,f)$. This matches $P$.\n- $E_8 = \\mathrm{Add}(\\mathrm{Add}(g,1),\\mathrm{Add}(1,g))$: $1 \\preceq g$, so $C(\\mathrm{Add}(g,1)) = \\mathrm{Add}(1,g)$. $C(\\mathrm{Add}(1,g))$ is already canonical. So, $C(E_8) = \\mathrm{Add}(\\mathrm{Add}(1,g),\\mathrm{Add}(1,g))$. This matches $P$.\n- $E_9 = \\mathrm{Add}(\\mathrm{Mul(3,h)},\\mathrm{Mul}(h,3))$: $3 \\preceq h$, so $C(\\mathrm{Mul}(h,3)) = \\mathrm{Mul}(3,h)$. $C(\\mathrm{Mul}(3,h))$ is already canonical. So, $C(E_9) = \\mathrm{Add}(\\mathrm{Mul}(3,h),\\mathrm{Mul}(3,h))$. This matches $P$.\n- $E_{10} = \\mathrm{Add}(\\mathrm{Add}(i,j),\\mathrm{Add}(i,j))$: Assuming $i \\preceq j$, $C(\\mathrm{Add}(i,j)) = \\mathrm{Add}(i,j)$. The children are identical and already canonical. The form remains $\\mathrm{Add}(\\mathrm{Add}(i,j),\\mathrm{Add}(i,j))$. This matches $P$.\n- $E_{11} = \\mathrm{Add}(\\mathrm{Add}(k,0),\\mathrm{Add}(0,k))$: $C(\\mathrm{Add}(k,0)) = k$ and $C(\\mathrm{Add}(0,k)) = k$. So, $C(E_{11}) = \\mathrm{Add}(k,k)$. This matches $P$.\n- $E_{12} = \\mathrm{Add}(\\mathrm{Mul}(1,\\mathrm{Add}(m,0)),\\mathrm{Add}(0,\\mathrm{Add}(1,m)))$: \n  - Left child: $C(\\mathrm{Mul}(1, \\mathrm{Add}(m,0))) = C(\\mathrm{Mul}(1, C(\\mathrm{Add}(m,0)))) = C(\\mathrm{Mul}(1,m)) = m$.\n  - Right child: $C(\\mathrm{Add}(0, \\mathrm{Add}(1,m))) = C(\\mathrm{Add}(0, C(\\mathrm{Add}(1,m)))) = C(\\mathrm{Add}(0, \\mathrm{Add}(1,m))) = \\mathrm{Add}(1,m)$.\n  - The expression becomes $\\mathrm{Add}(m, \\mathrm{Add}(1,m))$. We sort the children. Since variables $\\prec$ compound nodes, $m \\preceq \\mathrm{Add}(1,m)$. The children are already sorted. The final form is $C(E_{12}) = \\mathrm{Add}(m, \\mathrm{Add}(1,m))$. The children are not identical. No match.\n\nThe expressions that match the pattern $P$ after canonicalization are $E_1, E_2, E_3, E_4, E_7, E_8, E_9, E_{10}, E_{11}$. There are $9$ such expressions.\nThus, the count for Order $\\mathcal{A}$ is $m_{\\mathcal{A}}(P) = 9$.\n\nThe resulting pair is $(m_{\\mathcal{A}}(P), m_{\\mathcal{B}}(P)) = (9, 1)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n9  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Not all optimization interactions are positive; sometimes, one pass can prevent another from working. This final practice illustrates a critical conflicting interaction between Inline Expansion and Tail-Call Optimization (TCO). You will analyze a scenario where inlining a helper function, a typically beneficial optimization, inadvertently alters the code structure and disables TCO, leading to a catastrophic increase in stack usage for a recursive function . This exercise highlights the crucial need to consider the potential negative consequences when ordering compiler phases.",
            "id": "3662669",
            "problem": "Consider the following simplified intermediate representation of a program in a language with callable functions and returns. The maximum stack depth is defined as the maximum number of simultaneously live activation records during execution, denoted by $d(P)$ for a program $P$. A call is in tail position if the function returns exactly the value of the call expression with no further computation. Tail-Call Optimization (TCO) is an optimization that rewrites tail-recursive functions into loops, thereby bounding the stack depth to a constant. Inline Expansion (Inlining) is an optimization that replaces calls to small functions with the body of those functions at the call sites.\n\nYou will analyze the phase-ordering interaction between Tail-Call Optimization and Inline Expansion. The program $P_{n}$ is parameterized by an integer $n \\geq 1$ and consists of two functions:\n- An identity helper $I(x)$ that returns its argument.\n- A tail-recursive function $F(i,a)$ that iterates $i$ times, incrementing an accumulator $a$:\n$$\nI(x) \\triangleq x\n$$\n$$\nF(i,a) \\triangleq\n\\begin{cases}\na,  \\text{if } i = 0 \\\\\nI\\big(F(i-1,a+1)\\big),  \\text{if } i  0\n\\end{cases}\n$$\nThe entry point is $F(n,0)$.\n\nThe compiler has two optimization passes:\n- $O_{\\mathrm{TailCallOpt}}$ (Tail-Call Optimization, TCO): This pass rewrites a function $G$ if and only if $G$ returns a direct call expression of the form $G(\\ldots)$ in tail position, that is, the syntactic form is exactly $\\,\\texttt{return}\\;G(\\ldots)\\,$ with no intervening temporary assignment and no post-return epilogue. When applicable to a self-call in $F$, the pass converts the recursion to a loop, yielding constant stack depth $1$ for all $n$.\n- $O_{\\mathrm{Inline}}$ (Inline Expansion): This pass inlines $I$ at its call sites. Inlining $I$ into $F$ transforms the tail return of the form $\\,\\texttt{return}\\;I(F(\\ldots))\\,$ into a two-step sequence that assigns the recursive call to a temporary and then returns the temporary, that is, $\\,\\texttt{tmp} \\leftarrow F(\\ldots);\\;\\texttt{return}\\;\\texttt{tmp}\\,$. Under the stated $O_{\\mathrm{TailCallOpt}}$ rule, such a form is not syntactically recognized as a tail call, so no tail-call rewriting is performed afterwards.\n\nAssume a conventional call-stack semantics where, in the absence of Tail-Call Optimization, each recursive call adds one activation record, so evaluating $F(n,0)$ without Tail-Call Optimization yields a stack depth of $n+1$ at the deepest point (counting the initial frame).\n\nLet $d_{\\mathrm{Inline}\\rightarrow\\mathrm{TCO}}(P_{n})$ denote the maximum stack depth when $O_{\\mathrm{Inline}}$ is run first and then $O_{\\mathrm{TailCallOpt}}$ is run, and let $d_{\\mathrm{TCO}\\rightarrow\\mathrm{Inline}}(P_{n})$ denote the maximum stack depth when $O_{\\mathrm{TailCallOpt}}$ is run first and then $O_{\\mathrm{Inline}}$ is run.\n\nFor the specific input $n = 1024$, compute the ratio\n$$\nR \\triangleq \\frac{d_{\\mathrm{Inline}\\rightarrow\\mathrm{TCO}}(P_{n})}{d_{\\mathrm{TCO}\\rightarrow\\mathrm{Inline}}(P_{n})}.\n$$\nProvide the final numerical value of $R$. No rounding is required. Express your answer as a pure number without units.",
            "solution": "To determine the ratio $R$, we must compute the maximum stack depth resulting from each of the two specified phase orderings for the program $P_n$ with $n=1024$.\n\n**1. Phase Order: $O_{\\mathrm{Inline}} \\rightarrow O_{\\mathrm{TailCallOpt}}$**\n\nFirst, the Inline Expansion pass ($O_{\\mathrm{Inline}}$) is applied. The recursive case in function $F$ is `return I(F(i-1, a+1))`. According to the problem description, inlining the identity function `I` transforms this line into a two-step sequence: `tmp - F(i-1, a+1); return tmp`.\n\nNext, the Tail-Call Optimization pass ($O_{\\mathrm{TailCallOpt}}$) is applied. This pass only recognizes direct tail calls of the form `return G(...)`. The modified code, which uses an intermediate temporary variable `tmp`, no longer matches this required syntactic form. Therefore, TCO is disabled by the prior inlining pass.\n\nSince TCO is not performed, the function $F$ is executed as a standard recursive function. The problem states that without TCO, the execution of $F(n,0)$ results in a maximum stack depth of $n+1$.\nFor $n=1024$, the stack depth is:\n$$ d_{\\mathrm{Inline}\\rightarrow\\mathrm{TCO}}(P_{1024}) = 1024 + 1 = 1025. $$\n\n**2. Phase Order: $O_{\\mathrm{TCO}} \\rightarrow O_{\\mathrm{Inline}}$**\n\nFirst, the Tail-Call Optimization pass ($O_{\\mathrm{TailCallOpt}}$) is applied. The recursive call is `return I(F(i-1, a+1))`. Although this involves a call to the identity function `I`, a sufficiently powerful TCO pass would recognize that `I` does not modify the result of `F`, making the call to `F` effectively in the tail position. The problem states that when TCO is applicable, it \"converts the recursion to a loop, yielding constant stack depth 1\". Thus, TCO successfully transforms the recursive function $F$ into an iterative one.\n\nNext, the Inline Expansion pass ($O_{\\mathrm{Inline}}$) is applied. However, since TCO has already eliminated the recursive call structure and therefore the call to `I`, the inliner has no work to do.\n\nThe resulting program is iterative. Its execution involves only the initial function call, leading to a constant stack depth. As per the problem description, this depth is 1.\nTherefore, the stack depth is:\n$$ d_{\\mathrm{TCO}\\rightarrow\\mathrm{Inline}}(P_{1024}) = 1. $$\n\n**3. Final Ratio Calculation**\n\nThe ratio $R$ is the quotient of the two computed stack depths:\n$$ R = \\frac{d_{\\mathrm{Inline}\\rightarrow\\mathrm{TCO}}(P_{1024})}{d_{\\mathrm{TCO}\\rightarrow\\mathrm{Inline}}(P_{1024})} = \\frac{1025}{1} = 1025. $$",
            "answer": "$$\\boxed{1025}$$"
        }
    ]
}