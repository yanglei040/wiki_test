{
    "hands_on_practices": [
        {
            "introduction": "To truly master Lazy Code Motion, we must first look under the hood at its data-flow engine. The algorithm's decisions are driven by two key properties: *anticipatability* ($Ant$), which identifies where a computation is guaranteed to be needed in the future, and *availability* ($Avail$), which tracks where a computed value is already valid. This exercise will guide you through the manual calculation of these foundational data-flow sets, giving you a deep, mechanical understanding of how LCM analyzes code before transforming it.",
            "id": "3649357",
            "problem": "Consider the following control-flow graph (CFG) of basic blocks that form a single-entry, single-exit procedure. Let the arithmetic expression of interest be the binary expression $e \\equiv a + b$. The blocks are:\n\n- $B_1$: a conditional that branches to $B_2$ or $B_3$; $B_1$ does not assign to $a$ or $b$ and does not compute $e$.\n- $B_2$: first computes $x \\leftarrow a + b$, then assigns $a \\leftarrow a + 1$, then branches to $B_4$.\n- $B_3$: a conditional that branches to $B_4$ or $B_5$; $B_3$ does not assign to $a$ or $b$ and does not compute $e$.\n- $B_5$: computes $y \\leftarrow a + b$, then branches to $B_4$.\n- $B_4$: computes $z \\leftarrow a + b$, then returns.\n\nAssume no other assignments to $a$ or $b$ occur. The control-flow edges are $(B_1,B_2)$, $(B_1,B_3)$, $(B_2,B_4)$, $(B_3,B_4)$, $(B_3,B_5)$, $(B_5,B_4)$. The entry block is $B_1$. You may assume straight-line execution within a block.\n\nYour task is to manually compute, for the single expression $e$, the following quantities used by Lazy Code Motion (LCM) for Partial Redundancy Elimination (PRE), starting only from core data-flow semantics:\n\n- $Ant$: the down-safety (also called anticipability) of $e$ at each program point (in particular, the value at block entry, $Ant\\_in[B]$, and at block exit, $Ant\\_out[B]$).\n- $Avail$: the availability of $e$ at each program point (in particular, the value at block entry, $Avail\\_in[B]$, and at block exit, $Avail\\_out[B]$).\n- $Earliest$: the earliest safe insertion condition of $e$ on each control-flow edge.\n- $Latest$: the latest safe insertion condition of $e$ on each control-flow edge after postponement so as not to increase execution frequency.\n\nBase your derivation on the fundamental definitions of must data-flow analysis for anticipability and availability: a must property holds at a program point if and only if it holds along all paths from or to that point, with meet operator defined by logical conjunction, and block-local transfer that respects whether a block evaluates $e$ and whether it is transparent for $e$ (no assignments to operands of $e$ within the block). Do not assume any specific algorithmic shortcuts beyond these semantics.\n\nThen, interpret $Latest$ as the placement set of LCM: insert $e$ exactly on those edges where $Latest$ holds, and delete any now-redundant original computations.\n\nCompute the total number $N$ of edges on which $Latest$ holds for this CFG and this $e$. Provide $N$ as your final answer. No rounding is required. Express your final answer as a single real-valued number.",
            "solution": "The problem asks for the number of edges where Lazy Code Motion (LCM) will insert a computation of the expression $e \\equiv a + b$. This requires a careful application of the core LCM principles to the given Control-Flow Graph (CFG).\n\n**1. Analyze Local Block Properties and Expression Semantics**\nFirst, we analyze each block's relationship with $e = a+b$:\n- $B_1$: Transparent. Does not use or kill (redefine operands of) $e$.\n- $B_2$: Computes $e$ (`x - a+b`), then kills it (`a - a+1`).\n- $B_3$: Transparent.\n- $B_4$: Computes $e$ (`z - a+b`).\n- $B_5$: Computes $e$ (`y - a+b`).\n\nA crucial observation is the assignment `a - a+1` in $B_2$. This means the expression `a+b` computed in `B_4` after passing through `B_2` is *semantically different* from the `a+b` computed in `B_2`, `B_5`, or `B_4` (when reached from `B_3` or `B_5`). A sound Partial Redundancy Elimination (PRE) algorithm must recognize this. The path $B_1 \\to B_2 \\to B_4$ contains two textually identical but semantically distinct computations; there is no redundancy to eliminate along this path. Therefore, our optimization efforts should focus on the subgraph reachable from $B_1$ via $B_3$.\n\n**2. Analyze Redundancy in the Subgraph through $B_3$**\nLet's analyze the paths starting from $B_1 \\to B_3$:\n- **Path $P_1$**: $B_1 \\to B_3 \\to B_4$. The expression $e$ is computed once in $B_4$.\n- **Path $P_2$**: $B_1 \\to B_3 \\to B_5 \\to B_4$. The expression $e$ is computed in $B_5$ and then again in $B_4$.\n\nThe computation in $B_4$ is *partially redundant* with respect to this subgraph: it is redundant if reached via $P_2$ but not if reached via $P_1$. LCM aims to eliminate this partial redundancy and also the full redundancy of the computation in $B_5$ (which can be seen as redundant with the one in $B_4$ along path $P_2$).\n\n**3. Determine `Earliest` and `Latest` Placements**\nThe LCM algorithm identifies the earliest possible safe insertion points and then \"postpones\" them as far as possible to find the `Latest` placements.\n- **Earliest Placement**: For the subgraph starting at $B_3$, the expression $e$ is anticipated (guaranteed to be used) on all paths. It is not available at the entry of $B_3$. Thus, the earliest safe placement is at the entry of $B_3$.\n- **Postponement (The \"Lazy\" Step)**: The core idea of LCM is to move this placement downward.\n    1.  The placement at the entry of $B_3$ can be postponed through its body, since $B_3$ is transparent (it does not use or kill $e$).\n    2.  This pushes the placement to the exit edges of $B_3$: `(B_3, B_4)` and `(B_3, B_5)`.\n    3.  Can we postpone further?\n        - Into block $B_4$? No. Postponement must stop before a use of the expression. Since $B_4$ computes (and thus uses) $e$, the placement cannot be moved past the entry of $B_4$. Thus, the `Latest` placement remains on the edge **`(B_3, B_4)`**.\n        - Into block $B_5$? No. For the same reason, $B_5$ uses $e$, so postponement stops. The `Latest` placement remains on the edge **`(B_3, B_5)`**.\n\n**4. Final Placement and Conclusion**\nThe `Latest` set, which defines where LCM inserts new computations, consists of the two edges identified above: `(B_3, B_4)` and `(B_3, B_5)`.\n- The insertion on `(B_3, B_5)` makes the original computation in `B_5` fully redundant, so it is deleted. The computed value is then available for the path to `B_4`.\n- The insertion on `(B_3, B_4)` provides the value of $e$ for the direct path to `B_4`.\n- With these insertions, the value of $e$ is now available at the entry of `B_4` for all paths originating from $B_3$. The original computation in `B_4` becomes redundant for these paths and is deleted (or rather, the part of it that was partially redundant is now fully redundant).\n\nThe total number $N$ of edges on which `Latest` holds is 2.\n\n$$N=2$$",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "One of the most powerful applications of code motion is optimizing loops by hoisting loop-invariant expressions—computations whose values do not change across iterations. LCM is particularly well-suited for this, as it can identify such expressions and safely move them to a loop preheader, a block executed only once before the loop begins. This practice  challenges you to analyze a loop structure and determine the correct $Latest$ placement, demonstrating how LCM avoids unprofitable code movement that would increase execution counts.",
            "id": "3649373",
            "problem": "Consider the following program represented by a Control-Flow Graph (CFG). Control-Flow Graph (CFG) is a directed graph whose nodes are basic blocks and whose edges represent possible flow of control. The nodes are $B_{0}$ through $B_{6}$, and edges are as described below. The program uses variables $x$, $y$, $i$, and a positive integer $m \\ge 1$. Let $h_{1}(\\cdot)$ and $h_{2}(\\cdot)$ be pure functions that do not modify $x$ or $y$.\n\n- Block $B_{0}$: initializes $x$, $y$, and $i$ as $x := \\text{input}(),\\ y := \\text{input}(),\\ i := 0$; then transfers control to $B_{1}$.\n- Block $B_{1}$ (preheader): an empty block used to connect entry to the loop; transfers control to $B_{2}$.\n- Block $B_{2}$ (loop body entry): contains a conditional branching on a predicate $p(i)$:\n  - If $p(i)$ is true, control goes to $B_{3}$.\n  - If $p(i)$ is false, control goes to $B_{4}$.\n- Block $B_{3}$: contains the use $u := h_{1}(x + y)$; transfers control to $B_{5}$.\n- Block $B_{4}$: contains the use $v := h_{2}(x + y)$; transfers control to $B_{5}$.\n- Block $B_{5}$: updates $i := i + 1$; if $i  m$, transfers control back to $B_{2}$; otherwise to $B_{6}$.\n- Block $B_{6}$: exit.\n\nAssume the loop is a do-while style: at least one iteration of the loop body is executed before reaching the exit test in $B_{5}$, that is, starting from $B_{1}$, there is no path to $B_{6}$ that avoids executing either $B_{3}$ or $B_{4}$ at least once. Also assume $x$ and $y$ are assigned only in $B_{0}$ and are not modified anywhere else; thus, $x + y$ is loop-invariant. On each iteration, exactly one of $B_{3}$ or $B_{4}$ is executed (depending on $p(i)$), and in whichever branch is taken, a single use of $x + y$ occurs in that iteration.\n\nLazy Code Motion (LCM) is a program optimization that places computations at points that are safe and not redundant by exploiting data-flow properties such as dominance, down-safety, and anticipability. A loop preheader is a block that dominates the loop body and allows a computation to be executed before entering the loop.\n\nTask:\n- Using only the core definitions of Control-Flow Graph (CFG), dominance, loop-invariant expression, and the qualitative principles of Lazy Code Motion (LCM) that avoid computing an expression on paths where it is not needed, determine whether the $Latest$ placement of the computation of $x + y$ will place it outside the loop in $B_{1}$ rather than inside the loop in $B_{2}$ or deeper. Encode your decision as $I$, where $I = 1$ if $Latest$ placement keeps $x + y$ outside the loop (in $B_{1}$), and $I = 0$ otherwise.\n- Suppose the computation of $x + y$ is mis-placed by forcing it to execute at the beginning of $B_{2}$, so that it is recomputed once per iteration regardless of branch $p(i)$. Quantify the resulting increase in total executions $$\\Delta \\mathrm{execs} := \\text{(executions with mis-placement)} - \\text{(executions under $Latest$ placement)}$$ as a closed-form expression in $m$.\n\nProvide your final answer as a two-entry row matrix $\\begin{pmatrix} I  \\Delta \\mathrm{execs} \\end{pmatrix}$. No rounding is required, and no physical units apply.",
            "solution": "The problem is first subjected to validation.\n\n### Step 1: Extract Givens\n- **Control-Flow Graph (CFG) Nodes:** $B_{0}, B_{1}, B_{2}, B_{3}, B_{4}, B_{5}, B_{6}$.\n- **Node $B_{0}$:** $x := \\text{input}()$, $y := \\text{input}()$, $i := 0$.\n- **Node $B_{1}$:** Empty preheader.\n- **Node $B_{2}$:** Conditional jump on $p(i)$.\n- **Node $B_{3}$:** Contains use $u := h_{1}(x + y)$.\n- **Node $B_{4}$:** Contains use $v := h_{2}(x + y)$.\n- **Node $B_{5}$:** $i := i + 1$; conditional jump on $i  m$.\n- **Node $B_{6}$:** Exit.\n- **CFG Edges:** $B_{0} \\to B_{1}$, $B_{1} \\to B_{2}$, $B_{2} \\to B_{3}$ (if $p(i)$), $B_{2} \\to B_{4}$ (if not $p(i)$), $B_{3} \\to B_{5}$, $B_{4} \\to B_{5}$, $B_{5} \\to B_{2}$ (if $i  m$), $B_{5} \\to B_{6}$ (if $i \\ge m$).\n- **Constants and Variables:** $m$ is a positive integer, $m \\ge 1$. $x, y, i$ are variables.\n- **Functions:** $h_1(\\cdot)$ and $h_2(\\cdot)$ are pure functions.\n- **Assumptions:**\n    1. The loop is \"do-while style\", guaranteeing at least one execution.\n    2. $x$ and $y$ are assigned only in $B_0$.\n    3. The expression $x+y$ is loop-invariant.\n    4. On each iteration, exactly one of $B_3$ or $B_4$ is executed, and each contains a use of $x+y$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the domain of compiler theory and program optimization. The concepts of Control-Flow Graphs, loop-invariant expressions, and Lazy Code Motion (LCM) are standard computer science topics. The provided CFG is consistent, and the assumptions are clearly stated. The problem is scientifically grounded, objective, and self-contained. It contains no contradictions or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\nThe task is to determine the optimal placement of the computation of $x+y$ according to the principles of Lazy Code Motion (LCM) and to quantify the cost of a non-optimal placement.\n\n**Part 1: `Latest` Placement of $x+y$ and determination of $I$**\n\nThe expression to be optimized is $E = x + y$.\n1.  **Loop Invariance:** The problem states that $x$ and $y$ are assigned only in the entry block $B_{0}$ and are not modified within the loop. Therefore, the value of $E = x+y$ is constant throughout all iterations of the loop, making it a loop-invariant expression. This is the primary condition for loop code motion.\n\n2.  **Anticipability (Down-Safety):** An expression is anticipated at a program point if it is used on every path originating from that point before any of its operands are redefined. Let's analyze anticipability at the entry of the loop preheader, block $B_1$. Any path from $B_1$ must proceed to $B_2$, then to either $B_3$ or $B_4$, and then to $B_5$. The problem states that $x+y$ is used in both $B_3$ and $B_4$. Thus, regardless of the branch taken at $B_2$, the expression $x+y$ is guaranteed to be used in every iteration of the loop. Since there is no path from $B_1$ to the exit $B_6$ that bypasses the loop, the expression $x+y$ is anticipated at the entry of $B_1$.\n\n3.  **Lazy Code Motion (LCM):** LCM hoists computations to the earliest possible point but places them as late as possible within that hoistable region to minimize register pressure, without increasing the total number of computations. A computation is hoisted out of a loop only if it is safe and profitable.\n    - **Safety:** Placing the computation in the preheader $B_1$ is safe because the expression's value is needed on every path that passes through $B_1$. No path exists where the computation would be performed unnecessarily.\n    - **Profitability:** The original code computes $x+y$ inside the loop, once per iteration. By moving the computation to the preheader $B_1$, which is executed only once, we reduce the number of executions from $m$ (the number of iterations) to $1$. This is a profitable transformation.\n    - **Laziness:** The \"lazy\" principle delays the computation to the latest possible point. However, this sinking of a computation is constrained to not cross a boundary that would increase its execution frequency. The earliest, out-of-loop placement is the preheader $B_{1}$. Sinking it from $B_1$ to $B_2$ would move it from a single-execution region into a multiple-execution region (the loop), increasing the execution count from $1$ to $m$. LCM will not perform such a transformation. The latest possible placement that maintains the execution count of $1$ is within $B_1$.\n\nTherefore, the `Latest` placement for the computation $t := x+y$ is in the loop preheader, $B_1$. This is outside the loop. According to the problem statement, if the placement is in $B_1$, $I=1$.\n$$I = 1$$\n\n**Part 2: Calculation of $\\Delta \\mathrm{execs}$**\n\nWe need to calculate the difference in the total number of executions of $x+y$ between a mis-placement and the `Latest` placement.\n$$\\Delta \\mathrm{execs} = (\\text{executions with mis-placement}) - (\\text{executions under } Latest \\text{ placement})$$\n\n1.  **Executions under `Latest` placement:** As determined above, the computation is placed in $B_1$. The block $B_1$ is executed exactly once before the loop starts.\n    $$ \\text{executions}_{\\text{Latest}} = 1 $$\n\n2.  **Executions with mis-placement:** The problem specifies a mis-placement at the beginning of block $B_2$. Block $B_2$ is the entry point of the loop body. To find the number of executions, we must determine the number of loop iterations.\n    - The loop counter $i$ is initialized to $0$ in $B_0$.\n    - The loop body, which includes $B_2$, is executed.\n    - At the end of each iteration, in block $B_5$, $i$ is incremented ($i := i+1$), and the condition $i  m$ is checked.\n    - The sequence of values of $i$ entering block $B_2$ is $0, 1, 2, \\dots$.\n    - The sequence of values of $i$ being tested in the condition $i  m$ is $1, 2, 3, \\dots$.\n    - The loop continues as long as the check is true. The loop will execute for $i_{check} = 1, 2, \\dots, m-1$. The loop will execute one last time, leading to the check $i_{check}=m$.\n    - For $i_{check}=m$, the condition $m  m$ is false, and the loop terminates.\n    - The loop body is entered for the initial values of $i$ from $0$ up to $m-1$.\n    - The total number of iterations is $(m-1) - 0 + 1 = m$.\n    - Since $B_2$ is executed once per iteration, it is executed $m$ times.\n    $$ \\text{executions}_{\\text{mis-placed}} = m $$\n\n3.  **Difference in Executions:**\n    $$ \\Delta \\mathrm{execs} = \\text{executions}_{\\text{mis-placed}} - \\text{executions}_{\\text{Latest}} = m - 1 $$\n    Given that $m \\ge 1$, the number of redundant executions is $\\Delta \\mathrm{execs} \\ge 0$.\n\nThe final answer is composed of $I$ and $\\Delta \\mathrm{execs}$.\n$I=1$ and $\\Delta \\mathrm{execs} = m-1$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 1  m-1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Beyond fully redundant expressions in loops, Lazy Code Motion also excels at eliminating *partial redundancy*, where an expression's value is available on some, but not all, paths leading to a computation. By strategically inserting code on the paths where the value is missing, LCM makes the original computation fully redundant and thus safe to delete. In this final practice , you will identify a partially redundant expression, determine the final $\\mathrm{Insert}$ and $\\mathrm{Delete}$ sets, and quantify the optimization's benefit by calculating the expected reduction in dynamic executions.",
            "id": "3649324",
            "problem": "Consider the following Control Flow Graph (CFG) for a straight-line region inside a compiler’s intermediate representation. The region consists of five basic blocks $B_{0}$, $B_{1}$, $B_{2}$, $B_{3}$, and $B_{4}$ with directed edges:\n- $B_{0} \\rightarrow B_{1}$,\n- $B_{1} \\rightarrow B_{2}$ and $B_{1} \\rightarrow B_{3}$,\n- $B_{2} \\rightarrow B_{4}$ and $B_{3} \\rightarrow B_{4}$,\n- $B_{4} \\rightarrow \\text{Exit}$.\n\nThe contents of each block are:\n- $B_{0}$: no statements (entry).\n- $B_{1}$: $x := a$; $y := b$; a conditional branch to $B_{2}$ with probability $p = \\frac{3}{5}$, and to $B_{3}$ with probability $1 - p = \\frac{2}{5}$.\n- $B_{2}$: $t := x + y$; $s := t + 1$.\n- $B_{3}$: $r := 2 \\cdot x$.\n- $B_{4}$: $u := x + y$; $v := u \\cdot m$.\n\nAssume:\n- No redefinitions of $x$ or $y$ occur in $B_{2}$, $B_{3}$, or between the end of $B_{2}$/$B_{3}$ and the start of $B_{4}$.\n- The only use of $x + y$ after $B_{4}$ is through $u$ in $B_{4}$; there are no other uses elsewhere.\n- The region is executed once per consideration, and the branch from $B_{1}$ to $B_{2}$ or $B_{3}$ is the only source of path variability.\n\nLet Lazy Code Motion (LCM) denote the transformation that removes partial redundancy by inserting computations at the latest safe points and deleting redundant computations that become unnecessary due to those insertions.\n\nYour tasks:\n1. For the expression $e = x + y$, compute the $\\mathrm{Insert}$ and $\\mathrm{Delete}$ sets at block granularity under Lazy Code Motion. For this problem, interpret $\\mathrm{Insert}(B)$ to mean “insert the computation of $e$ at the entry of block $B$” and $\\mathrm{Delete}(B)$ to mean “delete the original computation of $e$ within block $B$.”\n2. Using the given branch probabilities, count the expected number of dynamic evaluations of $e$ per single execution of the region before applying LCM and after applying LCM.\n3. Provide, as your final answer, the expected reduction in the number of dynamic evaluations of $e$ per single execution of the region (that is, the expected count before LCM minus the expected count after LCM). Express your answer as an exact fraction. Do not round.",
            "solution": "The problem is well-posed, scientifically grounded in the principles of compiler optimization, and provides all necessary information to determine a unique solution. The problem is therefore valid. We proceed with the solution.\n\nThe analysis centers on the expression $e = x + y$ within the provided Control Flow Graph (CFG). The goal is to apply Lazy Code Motion (LCM) to reduce the number of dynamic evaluations of this expression. LCM is a partial-redundancy elimination (PRE) algorithm that avoids unnecessarily early computation by placing code as late as possible.\n\nFirst, we must analyze the occurrences of the expression $e = x+y$ in the original program.\n- In basic block $B_2$, the expression is computed: $t := x + y$. The result is subsequently used within the same block by $s := t + 1$.\n- In basic block $B_4$, the expression is computed again: $u := x + y$.\n\nThe CFG has a diamond structure where the path splits after $B_1$ and rejoins before $B_4$. There are two main paths from $B_1$ to $B_4$:\n1. Path $P_1$: $B_1 \\rightarrow B_2 \\rightarrow B_4$\n2. Path $P_2$: $B_1 \\rightarrow B_3 \\rightarrow B_4$\n\nLet's examine the redundancy of the computation $u := x+y$ in block $B_4$.\n- On path $P_1$, the expression $x+y$ is computed in $B_2$. Since the assumption states that $x$ and $y$ are not redefined between their definitions in $B_1$ and their use in $B_4$, the value of $x+y$ computed in $B_2$ is available at the entry of $B_4$. Therefore, the computation $u := x+y$ in $B_4$ is redundant on this path.\n- On path $P_2$, the expression $x+y$ is *not* computed in $B_3$. Therefore, the computation $u := x+y$ in $B_4$ is necessary on this path.\n\nBecause the computation in $B_4$ is redundant on some, but not all, paths leading to it, it is classified as a *partially redundant* computation. The purpose of LCM is to eliminate this partial redundancy. This is achieved by inserting computations on the paths where the expression is not available, thereby making the partially redundant computation fully redundant and deletable.\n\nTo make the computation $u := x+y$ in $B_4$ fully redundant, we must ensure that the value of $x+y$ is available at the entry of $B_4$ regardless of the path taken. This requires us to insert a computation of $x+y$ on path $P_2$ somewhere before $B_4$. The LCM algorithm will place this new computation in the latest possible safe location, which in this case is within block $B_3$. By doing so, the value of $x+y$ becomes available on both paths entering $B_4$, making the computation in $B_4$ fully redundant.\n\nLet's trace the LCM transformation. The algorithm identifies that to make `u := x+y` in `B4` redundant, a computation must be inserted on path `P2`. The latest safe place is at the end of `B3`. At block granularity, this means `Insert(B3)`. After this insertion, the computation in `B4` is now fully redundant and can be deleted (`Delete(B4)`). The original computation in `B2` is not affected, as it is needed for `s := t+1` and there is no earlier computation to make it redundant.\n\nLet's refine the Insert/Delete sets based on the problem's block-entry interpretation. LCM may hoist the computation from `B2` and the new one from `B3` to an earlier point. The earliest safe point is the exit of `B1`. Pushing this down (\"lazily\") leads to insertions at the entry of `B2` and `B3`.\n\n**Task 1: Compute $\\mathrm{Insert}$ and $\\mathrm{Delete}$ sets**\nA common LCM strategy for this pattern is to hoist the computation to all predecessor paths of the merge block (`B4`), making the computation at the merge fully redundant.\n- $\\mathrm{Insert}(B_3)$: Insert `temp := x+y` at the entry of `B_3` to cover path `P2`.\n- $\\mathrm{Delete}(B_4)$: The original computation `u := x+y` in `B_4` now becomes fully redundant and is deleted.\nA more aggressive hoisting might also create `Insert(B2)` and `Delete(B2)`. Let's calculate the cost for both, but the most direct PRE interpretation is the former. Let's analyze the cost assuming the goal is to make all computations of `x+y` occur exactly once per path, as early as needed but as late as possible. The optimal solution is to compute `x+y` in `B2` on path P1, and in `B3` on path P2, and then use the result in `B4`.\n- **Transformation**: Insert `x+y` into `B3`. Delete `x+y` from `B4`.\n- $\\mathrm{Insert} = \\{B_3\\}$, $\\mathrm{Delete} = \\{B_4\\}$.\n\nLet's re-calculate with the solution's proposed transformation: `Insert = {B2, B3}`, `Delete = {B2, B4}`. This is a valid, though slightly different, PRE scheme. The cost is the same.\n- A computation is inserted at the start of `B2`. The original in `B2` is removed. Net: one computation in `B2`.\n- A computation is inserted at the start of `B3`. Net: one computation in `B3`.\n- The computation in `B4` is removed.\n\n**Task 2: Calculate expected number of dynamic evaluations before and after LCM**\n\nThe probability of taking path $P_1$ (through a branch to $B_2$) is given as $p = \\frac{3}{5}$.\nThe probability of taking path $P_2$ (through a branch to $B_3$) is $1-p = \\frac{2}{5}$.\nThe region is executed once.\n\n**Before LCM:**\n- If path $P_1$ is taken (probability $\\frac{3}{5}$), $e$ is evaluated in $B_2$ and again in $B_4$. This amounts to $2$ dynamic evaluations.\n- If path $P_2$ is taken (probability $\\frac{2}{5}$), $e$ is evaluated only in $B_4$. This amounts to $1$ dynamic evaluation.\nThe expected number of evaluations, $E_{\\text{before}}$, is:\n$$E_{\\text{before}} = 2 \\cdot P(P_1) + 1 \\cdot P(P_2) = 2 \\cdot \\frac{3}{5} + 1 \\cdot \\frac{2}{5} = \\frac{6}{5} + \\frac{2}{5} = \\frac{8}{5}$$\n\n**After LCM:**\nThe transformation ensures `x+y` is computed once on every path before `B4`.\n- If path $P_1$ is taken (probability $\\frac{3}{5}$), the code executes one computation of `x+y` (either the original in `B2` or an inserted one). This amounts to $1$ dynamic evaluation on this path.\n- If path $P_2$ is taken (probability $\\frac{2}{5}$), the code executes one computation of `x+y` (the one inserted into `B3`). This amounts to $1$ dynamic evaluation on this path.\n- In either case, exactly one evaluation occurs before the merge point `B4`.\nThe expected number of evaluations, $E_{\\text{after}}$, is:\n$$E_{\\text{after}} = 1 \\cdot P(P_1) + 1 \\cdot P(P_2) = 1 \\cdot \\frac{3}{5} + 1 \\cdot \\frac{2}{5} = \\frac{3}{5} + \\frac{2}{5} = 1$$\n\n**Task 3: Compute the expected reduction**\n\nThe expected reduction in the number of dynamic evaluations is the difference between the expected counts before and after the transformation.\n$$\\text{Reduction} = E_{\\text{before}} - E_{\\text{after}} = \\frac{8}{5} - 1 = \\frac{8}{5} - \\frac{5}{5} = \\frac{3}{5}$$",
            "answer": "$$\\boxed{\\frac{3}{5}}$$"
        }
    ]
}