{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of global optimization is identifying computations inside a loop that produce the same result in every iteration and moving them outside. This practice, known as Loop-Invariant Code Motion (LICM), can yield significant performance gains. This exercise provides a quantitative look at applying LICM not to user code, but to compiler-generated safety checks, specifically array bounds checking. You will use range analysis to prove the safety of hoisting these checks and build a cost model to evaluate the expected performance speedup, a core skill in compiler design .",
            "id": "3644311",
            "problem": "A compiler for a language with mandatory array bounds checking performs global optimization by hoisting loop-invariant bounds checks out of a loop when sound range analysis can guarantee safety, inserting pre-loop guards to preserve program semantics. Consider arrays $A$ and $B$ of logical lengths $L_A$ and $L_B$, respectively, and the following loop over an induction variable $i$:\nThe loop body computes a scalar accumulation $s := s + A[i] \\cdot B[i+1]$ for all iterations satisfying $0 \\le i  n$. In the unoptimized baseline, the compiler inserts dynamic checks on every iteration to ensure that both $A[i]$ and $B[i+1]$ are in bounds.\n\nYou are to use range analysis derived from $0 \\le i  n$ to hoist the bounds checks out of the loop by adding sound pre-loop guards. Then, derive a cost model that accounts for the global transformation and compute the expected speedup of the hoisted version relative to the baseline.\n\nAssume the following:\n- The fundamental memory safety requirement for an access $A[k]$ is $0 \\le k \\le L_A - 1$, and similarly for $B[k]$ it is $0 \\le k \\le L_B - 1$.\n- The loop executes exactly $n$ iterations with $i$ ranging over $0 \\le i  n$.\n- The per-iteration baseline cost excluding bounds checks is $c_{\\ell}$ cycles for the arithmetic and memory operations.\n- Each dynamic bounds check in the baseline costs $c_b$ cycles and is performed twice per iteration (once for $A[i]$ and once for $B[i+1]$).\n- In the optimized version, two pre-loop guards are inserted and each guard evaluation costs $c_g$ cycles. If the guards succeed, the loop executes with no per-iteration bounds checks. If either guard fails, execution falls back to the baseline version with per-iteration checks. Guards are always evaluated once before entering the loop and their cost is incurred regardless of success or failure.\n- The probability that both guards succeed on a given run is $p$, and the probability they fail (causing fallback) is $1-p$.\n\nUsing only the given definitions and facts, first derive the necessary guard conditions using range analysis and the safety requirement, then derive the expected runtime of the optimized version. Finally, compute the expected speedup as the ratio of the baseline runtime to the expected optimized runtime for the concrete parameters $n = 10^{6}$, $c_b = 7$, $c_{\\ell} = 6$, $c_g = 12$, and $p = 0.99$.\n\nRound your final answer to four significant figures. The final answer must be a single real-valued number with no units.",
            "solution": "The problem statement is evaluated as valid. It is scientifically sound, well-posed, objective, and self-contained, representing a standard problem in compiler optimization analysis. All necessary constants, variables, and conditions are provided to derive a unique solution.\n\nThe solution proceeds in three stages: first, deriving the necessary pre-loop guard conditions using range analysis; second, formulating the cost models for the baseline and optimized versions to derive an expression for the expected speedup; and third, substituting the given numerical values to compute the final answer.\n\n**1. Derivation of Pre-Loop Guard Conditions**\n\nThe loop iterates with the induction variable $i$ over the range $0 \\le i  n$. For the optimization to be sound, every array access within the loop must be proven safe for this entire range of $i$. The safety requirement for an access to an array of logical length $L$ at index $k$ is $0 \\le k \\le L - 1$.\n\n- **Analysis for Array A**: The access is $A[i]$. The index variable is $k = i$.\n  Given the loop range $0 \\le i  n$, the minimum value of the index $k$ is $0$ and the maximum value is $n-1$.\n  The safety condition $0 \\le k \\le L_A - 1$ must hold for all values of $k$ in $[0, n-1]$.\n  - The lower bound check, $0 \\le 0$, is always true.\n  - The upper bound check requires that the maximum index, $n-1$, is within the valid range: $n-1 \\le L_A - 1$.\n  This inequality simplifies to $n \\le L_A$. Thus, the first guard condition is $n \\le L_A$. This check is only necessary if $n0$; if $n=0$, the loop does not execute and no access occurs.\n\n- **Analysis for Array B**: The access is $B[i+1]$. The index is $k = i+1$.\n  Given the loop range $0 \\le i  n$, the range for the index $k$ is $0+1 \\le i+1  n+1$, which is $1 \\le k  n+1$. The minimum value of the index $k$ is $1$ and the maximum value is $n$.\n  The safety condition $0 \\le k \\le L_B - 1$ must hold for all values of $k$ in $[1, n]$.\n  - The lower bound check, $0 \\le 1$, is always true.\n  - The upper bound check requires that the maximum index, $n$, is within the valid range: $n \\le L_B - 1$.\n  This inequality can also be written as $n+1 \\le L_B$. Thus, the second guard condition is $n+1 \\le L_B$.\n\nThe two sound pre-loop guards are therefore ($1$) $n \\le L_A$ and ($2$) $n+1 \\le L_B$.\n\n**2. Derivation of Cost Models and Expected Speedup**\n\n- **Baseline Runtime ($T_{base}$)**: In the unoptimized baseline, each of the $n$ iterations incurs a cost for arithmetic ($c_{\\ell}$) and a cost for two bounds checks ($2 \\cdot c_b$).\n$$T_{base} = n \\cdot (c_{\\ell} + 2 c_b)$$\n\n- **Expected Optimized Runtime ($E[T_{opt}]$)**: The cost of the optimized version is probabilistic. The two guard evaluations are always performed, incurring a cost of $2 c_g$.\n  - With probability $p$, the guards succeed. The loop executes $n$ times with only the arithmetic cost $c_{\\ell}$ per iteration. The total cost in this case is $T_{succ} = 2 c_g + n c_{\\ell}$.\n  - With probability $1-p$, at least one guard fails. Execution falls back to the baseline version. The total cost is the guard evaluation cost plus the full baseline cost: $T_{fail} = 2 c_g + T_{base} = 2 c_g + n(c_{\\ell} + 2 c_b)$.\n\nThe expected runtime is the weighted average of these two outcomes:\n$$E[T_{opt}] = p \\cdot T_{succ} + (1-p) \\cdot T_{fail}$$\n$$E[T_{opt}] = p(2 c_g + n c_{\\ell}) + (1-p)(2 c_g + n(c_{\\ell} + 2 c_b))$$\nWe can simplify this expression:\n$$E[T_{opt}] = p \\cdot 2 c_g + (1-p) \\cdot 2 c_g + p \\cdot n c_{\\ell} + (1-p) \\cdot n c_{\\ell} + (1-p) \\cdot n \\cdot 2 c_b$$\n$$E[T_{opt}] = (p + 1-p) \\cdot 2 c_g + (p + 1-p) \\cdot n c_{\\ell} + (1-p) \\cdot 2 n c_b$$\n$$E[T_{opt}] = 2 c_g + n c_{\\ell} + 2 n c_b (1-p)$$\n\n- **Expected Speedup ($S$)**: Speedup is the ratio of the baseline runtime to the expected optimized runtime.\n$$S = \\frac{T_{base}}{E[T_{opt}]} = \\frac{n(c_{\\ell} + 2 c_b)}{2 c_g + n c_{\\ell} + 2 n c_b (1-p)}$$\n\n**3. Numerical Calculation**\n\nWe are given the following parameters:\n- $n = 10^6$\n- $c_b = 7$\n- $c_{\\ell} = 6$\n- $c_g = 12$\n- $p = 0.99$\n\nFirst, we compute the numerator, $T_{base}$:\n$$T_{base} = 10^6 \\cdot (6 + 2 \\cdot 7) = 10^6 \\cdot (6 + 14) = 10^6 \\cdot 20 = 20,000,000$$\n\nNext, we compute the denominator, $E[T_{opt}]$:\n$$E[T_{opt}] = 2 \\cdot 12 + 10^6 \\cdot 6 + 2 \\cdot 10^6 \\cdot 7 \\cdot (1 - 0.99)$$\n$$E[T_{opt}] = 24 + 6,000,000 + 14 \\cdot 10^6 \\cdot 0.01$$\n$$E[T_{opt}] = 24 + 6,000,000 + 140,000$$\n$$E[T_{opt}] = 6,140,024$$\n\nFinally, we compute the expected speedup, $S$:\n$$S = \\frac{20,000,000}{6,140,024} \\approx 3.2573136...$$\n\nRounding the result to four significant figures gives $3.257$.",
            "answer": "$$\\boxed{3.257}$$"
        },
        {
            "introduction": "After seeing a powerful optimization, it is crucial to understand the rules that govern its safe application. A transformation is only valid if it preserves the program's original meaning, including its error behavior. This exercise explores a scenario where two computations are congruent—they perform the same operation—but moving or merging them is unsafe because it could introduce a division-by-zero error that was not possible in the original code. This problem highlights the critical importance of the *dominance* property in ensuring that global optimizations do not introduce new, observable behaviors .",
            "id": "3644367",
            "problem": "Consider a program in Static Single Assignment form (SSA) with the following Control Flow Graph (CFG). Blocks are labeled $B_0, B_1, B_2, B_3$, and edges follow the natural structure of the conditional branching. Assume integer division raises an exception on divisor equal to $0$. The program is:\n\nEntry block $B_0$:\n- if $(p)$ then goto $B_1$ else goto $B_2$.\n\nBlock $B_1$:\n- if $(y \\ne 0)$ then $t_1 := x / y$ else $t_1 := 0$; goto $B_3$.\n\nBlock $B_2$:\n- if $(y \\ne 0)$ then $t_2 := x / y$ else $t_2 := 0$; goto $B_3$.\n\nJoin block $B_3$:\n- $r := \\phi(t_1, t_2)$; use $r$.\n\nHere $x$, $y$, $p$, $t_1$, $t_2$, $r$ are program variables, $/$ is integer division that raises an exception on divisor $0$, and $\\phi$ is the SSA merge operator. Two expressions are called congruent if they compute the same mathematical function of the same operand values under all executions in which they both evaluate.\n\nUsing only foundational definitions from compiler theory:\n- Dominance: A block $D$ dominates a block $N$ if every path from the entry to $N$ goes through $D$.\n- Semantics-preserving transformation: A program rewrite that yields observationally equivalent behavior for all inputs, including the same exceptions or lack thereof.\n- Global Value Numbering (GVN): An optimization that groups congruent computations into equivalence classes to enable elimination or reuse, subject to standard availability and dominance constraints.\n\nAnswer the following multiple-choice question about the safety of merging the congruent computations $x / y$ across $B_1$ and $B_2$ at the join $B_3$. Select all options that are correct.\n\nA. It is semantics-preserving to replace both guarded divisions with a single computation inserted at the top of $B_3$: insert $t := x / y$ in $B_3$ and set $t_1 := t$ in $B_1$ and $t_2 := t$ in $B_2$.\n\nB. The transformation in option A becomes semantics-preserving if a prior analysis proves that at the beginning of $B_3$ the predicate $y \\ne 0$ holds along all paths.\n\nC. It is semantics-preserving to directly reuse $t_1$ in place of $t_2$ (i.e., replace all uses of $t_2$ with $t_1$) because $x / y$ is computed congruently in $B_1$ and $B_2$, regardless of dominance.\n\nD. A Global Value Numbering (GVN) optimization that respects dominance can assign the same value number to both computations $x / y$ but will not eliminate either computation unless it can place a new computation at a point that dominates all uses without introducing new observable behavior.\n\nE. If the divisions in $B_1$ and $B_2$ are replaced by additions $x + y$ under arithmetic with no overflow, no traps, and no side effects, then inserting a single $x + y$ at the top of $B_3$ is always semantics-preserving, even if the original guarded structure in $B_1$ and $B_2$ still assigns $0$ to the temporary when $y = 0$.",
            "solution": "The problem statement has been validated and is sound. It describes a classic scenario in compiler optimization concerning partially redundant computations and speculative code motion with possibly excepting instructions.\n\nFirst, let us analyze the behavior of the original program. The program is given in Static Single Assignment (SSA) form. The control flow is as follows: block $B_0$ branches to either $B_1$ or $B_2$ based on a predicate $p$. Both $B_1$ and $B_2$ then unconditionally transfer control to a join block $B_3$.\n\nThe key operation is the integer division $x / y$, which is specified to raise an exception if the divisor $y$ is $0$.\n\nIn block $B_1$, the code is: `if ($y \\ne 0$) then $t_1 := x / y$ else $t_1 := 0$`. The division $x / y$ is only performed if $y$ is not equal to $0$. If $y$ is equal to $0$, the division is skipped, and $t_1$ is assigned the value $0$. Therefore, no exception can be raised along the path through $B_1$.\n\nSimilarly, in block $B_2$, the code is: `if ($y \\ne 0$) then $t_2 := x / y$ else $t_2 := 0$`. The division $x / y$ is only performed if $y$ is not equal to $0$. If $y$ is equal to $0$, the division is skipped, and $t_2$ is assigned the value $0$. Therefore, no exception can be raised along the path through $B_2$.\n\nSince any execution of the program must pass through either $B_1$ or $B_2$, and neither path can raise a division-by-zero exception, we can conclude that the original program **never raises an exception**, regardless of the input values for $x$, $y$, and $p$. This is a critical property for evaluating the semantic equivalence of any transformation.\n\nAt the join block $B_3$, the $\\phi$-function merges the results: $r := \\phi(t_1, t_2)$.\n- If execution came from $B_1$, $r$ takes the value of $t_1$.\n- If execution came from $B_2$, $r$ takes the value of $t_2$.\nThe value of $r$ will be $x/y$ if $y \\ne 0$, and $0$ if $y=0$.\n\nNow, we evaluate each option.\n\n**A. It is semantics-preserving to replace both guarded divisions with a single computation inserted at the top of $B_3$: insert $t := x / y$ in $B_3$ and set $t_1 := t$ in $B_1$ and $t_2 := t$ in $B_2$.**\n\nLet's analyze the proposed transformation. The core change is to move the computation $x / y$ to block $B_3$, effectively making the code in $B_3$ be $t := x / y; r := t$. Block $B_3$ is reached on every execution path. Consider an input where $y = 0$. In the original program, execution proceeds without an exception. In the transformed program, execution reaches $B_3$, and the statement $t := x / y$ is executed. Since $y=0$, this will raise a division-by-zero exception.\nIntroducing a new exception is a change in observable behavior. According to the provided definition, this transformation is **not semantics-preserving**. The issue is that a conditionally executed, guarded computation has been moved to a location where it is executed unconditionally, a form of unsafe speculative execution.\n\nVerdict: **Incorrect**.\n\n**B. The transformation in option A becomes semantics-preserving if a prior analysis proves that at the beginning of $B_3$ the predicate $y \\ne 0$ holds along all paths.**\n\nThis option adds a crucial precondition: it is known that whenever control reaches $B_3$, $y \\ne 0$.\nLet's re-evaluate the transformation from A under this new assumption.\n1.  **Exception behavior:** The transformed code computes $t := x / y$ in $B_3$. Since it is proven that $y \\ne 0$ at this point, the division will never raise an exception. The original program also never raises an exception. Thus, the exception behavior is preserved.\n2.  **Value computation:**\n    - In the original program, since any path to $B_3$ implies $y \\ne 0$, the guards in $B_1$ and $B_2$ are always true.\n    - Thus, on the path through $B_1$, $t_1$ is assigned $x / y$.\n    - On the path through $B_2$, $t_2$ is assigned $x / y$.\n    - At $B_3$, the $\\phi$-function $r := \\phi(t_1, t_2)$ will assign $x / y$ to $r$, regardless of the path taken.\n    - In the transformed program, $t := x / y$ is computed in $B_3$, and this value is used for $r$.\n    - The final value of $r$ is $x / y$ in both cases.\nSince both the exception behavior and the computed values are identical under this assumption, the transformation becomes semantics-preserving.\n\nVerdict: **Correct**.\n\n**C. It is semantics-preserving to directly reuse $t_1$ in place of $t_2$ (i.e., replace all uses of $t_2$ with $t_1$) because $x / y$ is computed congruently in $B_1$ and $B_2$, regardless of dominance.**\n\nThis statement proposes a transformation that violates fundamental principles of dataflow in SSA form. The variable $t_1$ is defined in block $B_1$. The variable $t_2$ is defined in block $B_2$. Block $B_1$ does not dominate block $B_2$, nor does $B_2$ dominate $B_1$. They are on mutually exclusive control flow paths. Therefore, the value of $t_1$ is not available in block $B_2$, and its use there would be invalid. For instance, in the $\\phi$-function $r := \\phi(t_1, t_2)$, the second argument $t_2$ must be a variable defined on the path leading from $B_2$. One cannot simply substitute $t_1$. The claim that this is permissible \"regardless of dominance\" is a direct contradiction of the rules that make compiler optimizations sound. While the computations are congruent, their results are not available across the control-flow branches.\n\nVerdict: **Incorrect**.\n\n**D. A Global Value Numbering (GVN) optimization that respects dominance can assign the same value number to both computations $x / y$ but will not eliminate either computation unless it can place a new computation at a point that dominates all uses without introducing new observable behavior.**\n\nThis option describes the behavior of a standard, sound GVN optimization.\n1.  **\"assign the same value number to both computations $x / y$\"**: The expressions $x/y$ in $B_1$ and $B_2$ are textually identical, and their operands, $x$ and $y$, are available at the entry to both blocks with the same value numbers (since no redefinitions occur on the paths from $B_0$). GVN would therefore correctly identify them as congruent and assign them the same value number. This part is correct.\n2.  **\"but will not eliminate either computation unless...\"**: The elimination of a redundant computation requires moving it (or a newly created copy) to a different location.\n3.  **\"place a new computation at a point that dominates all uses\"**: The 'uses' of the computation are the assignments to $t_1$ in $B_1$ and $t_2$ in $B_2$. The only block that dominates both $B_1$ and $B_2$ is the entry block, $B_0$. So, to eliminate the redundancy, GVN would have to hoist $x / y$ to $B_0$.\n4.  **\"without introducing new observable behavior\"**: As analyzed for option A, moving $x / y$ to $B_0$ would cause it to be executed unconditionally. For an input of $y=0$, this would raise an exception not present in the original program. This is a change in observable behavior.\nTherefore, a GVN optimization that is sound (\"respects dominance\" and preserves semantics) will not perform this code motion. The statement accurately describes that GVN will detect the redundancy but will be blocked from performing the optimization due to safety constraints.\n\nVerdict: **Correct**.\n\n**E. If the divisions in $B_1$ and $B_2$ are replaced by additions $x + y$ under arithmetic with no overflow, no traps, and no side effects, then inserting a single $x + y$ at the top of $B_3$ is always semantics-preserving, even if the original guarded structure in $B_1$ and $B_2$ still assigns $0$ to the temporary when $y = 0$.**\n\nLet's analyze this hypothetical scenario. The operation $x+y$ is non-excepting, so the safety issue of code motion is gone. We must now check for value equivalence.\n-   **Original (modified) program's behavior:** The code in $B_1$ is `if ($y \\ne 0$) then $t_1 := x + y$ else $t_1 := 0$`. The same logic applies in $B_2$ for $t_2$. At $B_3$, $r$ becomes either $t_1$ or $t_2$.\n    - If $y \\ne 0$, the result is $r = x+y$.\n    - If $y = 0$, the result is $r = 0$.\n-   **Transformed program's behavior:** A single computation $t := x+y$ is inserted at the top of $B_3$. The value of $r$ will be $x+y$ for all inputs.\n-   **Comparison:** If $y=0$, the original program calculates $r=0$. The transformed program calculates $r = x+0 = x$. If $x \\ne 0$, these values are different. For example, for inputs $x=5, y=0$, the original computes $r=0$ and the transformed computes $r=5$. Since the computed values are not identical for all inputs, the transformation is not semantics-preserving. The code motion is safe from exceptions, but incorrect in its result.\n\nVerdict: **Incorrect**.\n\nIn conclusion, options B and D are correct statements about the given program and compiler optimization principles.",
            "answer": "$$\\boxed{BD}$$"
        },
        {
            "introduction": "An optimization that is semantically valid and appears beneficial in isolation may not be a net win when considering the entire system. Compilers must manage finite resources, and one of the most critical is the set of physical registers. This final practice illustrates a classic trade-off where applying Loop-Invariant Code Motion (LICM) increases the number of variables that must be kept live simultaneously, leading to increased *register pressure*. You will analyze a hypothetical scenario where this pressure becomes so high that it forces the compiler to spill variables to memory, demonstrating that the cost of these extra memory accesses can completely overwhelm the gains from the initial optimization .",
            "id": "3644344",
            "problem": "An optimizing compiler considers applying Loop-Invariant Code Motion (LICM) to an inner loop. The candidate expression is a pure arithmetic computation $E$ whose value does not change across iterations. The compiler targets an in-order Central Processing Unit (CPU) with the following cost model:\n- Each integer arithmetic instruction that is not a multiply costs $1$ cycle, a multiply costs $6$ cycles.\n- Each memory load or store costs $4$ cycles.\n- There is no overlap between memory and computation; costs add linearly.\n\nRegister allocation uses a linear-scan strategy with $R$ allocatable integer registers in the loop body. When the number of simultaneously live temporaries exceeds $R$, exactly the excess number of temporaries are spilled. A spilled temporary that has $u$ uses and $1$ definition per iteration incurs $u$ loads and $1$ store per iteration. A variable is live at a program point if and only if, by the standard data-flow definition of liveness, its current value may be used along some path before being overwritten.\n\nFor a particular loop on this target:\n- A liveness analysis reveals that, before LICM, the loop’s peak simultaneous liveness of temporaries (excluding the loop index and any fixed base pointers already accounted for) is $L_0 = 8$, achieved at two distinct program points $P$ and $Q$ within the loop body. At $Q$, the candidate $E$ is computed and its result is live; at $P$, the candidate $E$ is not yet computed and is not live.\n- The allocator has $R = 8$ registers available for these temporaries in the loop body, so the unoptimized loop allocates with no spills.\n- The expression $E$ is a multiply whose per-evaluation cost is $c_E = 6$ cycles when computed inside the loop.\n- If LICM is applied, $E$ is computed once in the loop preheader, and its result is kept live across the entire loop body. After LICM, at point $Q$ the long-lived result of $E$ replaces the iteration-local temporary, so the live count at $Q$ remains $L_0$. At point $P$, the result of $E$ becomes live in addition to the baseline live set, so the live count at $P$ increases to $L_0 + 1 = 9$, exceeding $R$ by $1$ and forcing exactly one temporary to be spilled throughout the loop.\n- The spilled temporary in question has $u = 2$ uses and $1$ definition per iteration, and thus incurs $u + 1 = 3$ memory operations per iteration under this allocator. Each memory operation costs $c_{\\text{mem}} = 4$ cycles.\n\nLet the loop trip count be $N \\ge 1$, and assume all other parts of the loop are identical across versions.\n\nWhich statement best characterizes the end-to-end runtime impact of hoisting $E$ out of the loop under these conditions?\n\nA. Hoisting $E$ reduces total runtime for all $N \\ge 1$, because saving $N$ multiplications always dominates any spill costs.\n\nB. Hoisting $E$ increases total runtime for all $N \\ge 1$, because the per-iteration spill costs triggered by increased register pressure exceed the saved per-iteration cost of computing $E$.\n\nC. Hoisting $E$ has no effect on total runtime for any $N \\ge 1$, because the live count at $Q$ is unchanged and spills do not occur.\n\nD. Hoisting $E$ is profitable only beyond a threshold $N^\\star$, where $N^\\star$ is some positive integer, because a one-time preheader cost must be amortized across iterations even when spills occur.",
            "solution": "The user wants to determine the runtime impact of applying Loop-Invariant Code Motion (LICM) under a specific cost model where the optimization increases register pressure and causes spilling. To do this, we must formally calculate the total execution cost of the loop with and without the optimization as a function of the loop trip count, $N$.\n\nLet $T_{unopt}(N)$ be the total cycle cost of the unoptimized loop for $N$ iterations, and $T_{opt}(N)$ be the cost of the optimized loop. The total cost is the sum of the cost of the loop preheader and the product of the number of iterations and the cost of the loop body. Let $C_{other}$ be the execution cost in cycles of the parts of the loop body that are unaffected by the optimization. Let $C_{pre,base}$ be the baseline cost of the preheader.\n\nFrom the problem statement, we have the following parameters:\n- Cost of computing expression $E$ (a multiply): $c_E = 6$ cycles.\n- Cost of a memory operation (load or store): $c_{\\text{mem}} = 4$ cycles.\n- Number of available registers: $R = 8$.\n- Loop trip count: $N \\ge 1$.\n\n**Analysis of the Unoptimized Loop**\n\nIn the unoptimized version, the expression $E$ is computed in every iteration.\n- **Preheader Cost:** The preheader has a baseline cost of $C_{pre,base}$.\n- **Loop Body Cost per Iteration:**\n  - The cost of computing $E$ is $c_E = 6$.\n  - The problem states the peak liveness of temporaries is $L_0 = 8$. Since the number of available registers is $R = 8$, and $L_0 \\le R$, there is no spilling. The spill cost is $0$.\n  - The total cost per iteration is the sum of the cost of computing $E$ and the other costs: $C_{body,unopt} = c_E + C_{other} = 6 + C_{other}$.\n- **Total Cost:**\n  $$T_{unopt}(N) = C_{pre,base} + N \\times C_{body,unopt} = C_{pre,base} + N(6 + C_{other})$$\n\n**Analysis of the Optimized Loop (with LICM)**\n\nAfter applying LICM, the expression $E$ is computed once in the preheader, and its result is held in a register throughout the loop's execution.\n- **Preheader Cost:** The computation of $E$ is moved to the preheader, adding its cost.\n  $C_{pre,opt} = C_{pre,base} + c_E = C_{pre,base} + 6$.\n- **Loop Body Cost per Iteration:**\n  - The computation of $E$ is no longer performed inside the loop.\n  - Due to the result of $E$ being live throughout the loop, the peak liveness increases to $9$ at program point $P$.\n  - Since the new peak liveness ($9$) exceeds the available registers ($R=8$) by $1$, one temporary variable must be spilled to memory.\n  - The spilled temporary has $u = 2$ uses and $1$ definition per iteration. This results in $u$ loads and $1$ store, for a total of $u+1 = 2+1 = 3$ memory operations per iteration.\n  - The cost of these memory operations is the spill cost per iteration: $C_{spill} = (u+1) \\times c_{\\text{mem}} = 3 \\times 4 = 12$ cycles.\n  - The total cost per iteration is the sum of the spill cost and the other costs: $C_{body,opt} = C_{spill} + C_{other} = 12 + C_{other}$.\n- **Total Cost:**\n  $$T_{opt}(N) = C_{pre,opt} + N \\times C_{body,opt} = (C_{pre,base} + 6) + N(12 + C_{other})$$\n\n**Comparison of Runtimes**\n\nTo determine the impact of the optimization, we calculate the difference in total runtime, $\\Delta T(N) = T_{opt}(N) - T_{unopt}(N)$. A positive value indicates that the \"optimization\" increased the runtime.\n\n$$ \\Delta T(N) = \\left[ (C_{pre,base} + 6) + N(12 + C_{other}) \\right] - \\left[ C_{pre,base} + N(6 + C_{other}) \\right] $$\n$$ \\Delta T(N) = (C_{pre,base} + 6 - C_{pre,base}) + N(12 + C_{other} - 6 - C_{other}) $$\n$$ \\Delta T(N) = 6 + N(12 - 6) $$\n$$ \\Delta T(N) = 6 + 6N $$\n\nSince the loop trip count $N \\ge 1$, the value of $\\Delta T(N)$ is always positive. For $N=1$, $\\Delta T(1) = 6+6 = 12$. For $N=2$, $\\Delta T(2) = 6+12=18$. The difference grows linearly with $N$.\nThis result demonstrates that hoisting the expression $E$ is detrimental; it increases the total runtime for all possible loop trip counts $N \\ge 1$. The performance degradation stems from the fact that the per-iteration cost saving is less than the per-iteration cost penalty from spilling.\n- Per-iteration cost saving from not computing $E$: $c_E = 6$ cycles.\n- Per-iteration cost penalty from spilling: $C_{spill} = 12$ cycles.\nThe net effect on the loop body is an increase in cost of $12 - 6 = 6$ cycles per iteration.\n\nNow we evaluate each option based on this derivation.\n\n**A. Hoisting E reduces total runtime for all $N \\ge 1$, because saving $N$ multiplications always dominates any spill costs.**\nOur analysis shows that $\\Delta T(N) = 6 + 6N  0$, meaning the total runtime *increases*, not reduces. The reasoning provided is also false; in this case, the per-iteration spill cost of $12$ cycles is greater than the saved multiplication cost of $6$ cycles, so the spill costs dominate.\nVerdict: **Incorrect**.\n\n**B. Hoisting E increases total runtime for all $N \\ge 1$, because the per-iteration spill costs triggered by increased register pressure exceed the saved per-iteration cost of computing E.**\nOur analysis concluded that $T_{opt}(N)  T_{unopt}(N)$ for all $N \\ge 1$, which means hoisting increases the total runtime. The reason provided is that the per-iteration spill cost ($C_{spill} = 12$ cycles) exceeds the saved per-iteration computation cost ($c_E = 6$ cycles). This is precisely the outcome of our analysis ($12  6$).\nVerdict: **Correct**.\n\n**C. Hoisting E has no effect on total runtime for any $N \\ge 1$, because the live count at Q is unchanged and spills do not occur.**\nThis statement is factually incorrect according to the problem description. The problem explicitly states that hoisting $E$ increases the live count at point $P$ to $9$, which \"forc[es] exactly one temporary to be spilled\". Therefore, spills do occur, and they have a significant cost.\nVerdict: **Incorrect**.\n\n**D. Hoisting E is profitable only beyond a threshold $N^\\star$, where $N^\\star$ is some positive integer, because a one-time preheader cost must be amortized across iterations even when spills occur.**\nThis describes a common trade-off where a one-time setup cost is amortized by per-iteration savings. An optimization is profitable if $T_{opt}(N)  T_{unopt}(N)$, which implies $\\Delta T(N)  0$. In our case, this would require $6 + 6N  0$. Since $N \\ge 1$, $6 + 6N$ is always a positive integer, so the condition is never met. This type of threshold exists only when the optimization makes the loop body faster (i.e., $C_{body,opt}  C_{body,unopt}$). Here, the loop body becomes slower ($C_{body,opt}  C_{body,unopt}$).\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}