{
    "hands_on_practices": [
        {
            "introduction": "Loop unrolling is a fundamental optimization that trades increased code size for faster execution by reducing loop overhead and exposing more instruction-level parallelism. This exercise  challenges you to quantify this trade-off using a simple but powerful performance model. By deriving the optimal unroll factor, you will gain hands-on practice in making principled optimization decisions based on hardware constraints and performance goals.",
            "id": "3628478",
            "problem": "Consider a simple-counted loop over an induction variable $i$ with stride $s$ that executes exactly $N$ iterations. The loop body performs computation and accesses memory with address stride $s$ per iteration. Assume the following cost model on a single-core Central Processing Unit (CPU), expressed in cycles:\n- The data-processing part of the body (excluding loop control and stall) costs $b$ cycles per original iteration.\n- The loop control (increment of $i$ by $s$, comparison to a bound, and branch) costs $h$ cycles per original iteration if executed without unrolling.\n- When iterations are executed strictly serially, memory latency and pipeline hazards contribute an additional stall cost of $d(s)$ cycles per original iteration. Assume iterations are independent so that when $u$ copies of the body are unrolled and interleaved, these stalls are fully overlappable across the $u$ copies (that is, inter-iteration overlap can amortize latency).\n- Unrolling by a factor $u$ replicates the loop body $u$ times, reduces the number of loop-control executions proportionally, and leaves the data-processing work $b$ per original iteration unchanged.\n\nLet the code size of a single copy of the loop body be $S$ bytes, and suppose there is a hard budget $M$ bytes such that the generated loop must satisfy $u\\cdot S \\leq M$. Assume $M \\geq S$ so that at least the non-unrolled body fits, and that $u$ must be a positive integer not exceeding $N$.\n\nUsing only the fundamental definitions that total execution time in cycles is the product of dynamic work and per-unit cost, that loop unrolling reduces dynamic loop-control executions by a factor of $u$, and that fully overlappable stalls can be amortized uniformly across independent work, derive the execution time $T(u)$ in cycles as a function of $u$, and then determine the integer unroll factor $u^{\\star}$ that minimizes $T(u)$ subject to $u \\cdot S \\leq M$ and $1 \\leq u \\leq N$. Express your final answer as a single closed-form symbolic expression for $u^{\\star}$ in terms of $M$, $S$, and $N$.",
            "solution": "The problem statement is evaluated as valid. It presents a well-defined optimization problem grounded in the principles of compiler theory and computer architecture. The provided cost model is a standard, albeit simplified, representation of the performance effects of loop unrolling, suitable for analytical treatment. The objective and constraints are stated with sufficient precision to allow for a formal derivation of a unique solution.\n\nThe phrase \"reduces the number of loop-control executions proportionally\" and the requirement for a closed-form solution in terms of only $M$, $S$, and $N$ strongly indicate the intention to use a simplified continuous model for the number of loop executions, which is $\\frac{N}{u}$. This model is common in high-level performance analysis and assumes that the total number of iterations $N$ is large enough that the effects of a potential remainder loop (when $N$ is not an integer multiple of $u$) are negligible. The following derivation will proceed under this standard and contextually justified assumption.\n\nThe total execution time, $T(u)$, is the sum of the costs of its constituent parts: data processing, loop control, and stalls. We derive the total cost for each part over the $N$ original iterations as a function of the unroll factor $u$.\n\n$1$. Data-Processing Cost ($C_{proc}$): The problem states that the data-processing work costs $b$ cycles per original iteration and is unchanged by unrolling. Therefore, the total data-processing cost is constant.\n$$C_{proc} = N \\cdot b$$\n\n$2$. Loop Control Cost ($C_{ctrl}$): For an un-unrolled loop, the control overhead is incurred $N$ times. When the loop is unrolled by a factor $u$, the loop body is replicated $u$ times, and the new loop runs for $\\frac{N}{u}$ iterations. The loop control logic (increment, compare, branch), which costs $h$ cycles, is executed once per iteration of this new loop. Thus, the total loop control cost is:\n$$C_{ctrl}(u) = \\frac{N}{u} \\cdot h$$\n\n$3$. Stall Cost ($C_{stall}$): In the serial case, each iteration incurs a stall of $d(s)$ cycles. When $u$ iterations are unrolled, the stalls are \"fully overlappable\". This means that for each block of $u$ original iterations, the total stall cost is $d(s)$, not $u \\cdot d(s)$. Since there are $\\frac{N}{u}$ such blocks in total, the total stall cost is:\n$$C_{stall}(u) = \\frac{N}{u} \\cdot d(s)$$\n\nThe total execution time $T(u)$ is the sum of these components.\n$$T(u) = C_{proc} + C_{ctrl}(u) + C_{stall}(u) = N \\cdot b + \\frac{N}{u} \\cdot h + \\frac{N}{u} \\cdot d(s)$$\nWe can factor this expression to isolate the terms dependent on $u$:\n$$T(u) = N \\left( b + \\frac{h + d(s)}{u} \\right)$$\n\nNext, we must find the integer unroll factor $u^{\\star}$ that minimizes $T(u)$ subject to the given constraints.\nThe constraints on $u$ are:\n- $u$ must be a positive integer: $u \\in \\{1, 2, 3, \\dots\\}$.\n- The code size of the unrolled loop body must not exceed the budget $M$: $u \\cdot S \\leq M$. Since $u$ must be an integer, this is equivalent to $u \\leq \\left\\lfloor \\frac{M}{S} \\right\\rfloor$.\n- The unroll factor cannot be greater than the number of iterations: $u \\leq N$.\n\nCombining these constraints, the set of feasible integer values for $u$ is given by $1 \\leq u \\leq \\min\\left(N, \\left\\lfloor \\frac{M}{S} \\right\\rfloor\\right)$. The problem states $M \\geq S$, which ensures that $\\left\\lfloor \\frac{M}{S} \\right\\rfloor \\geq 1$, so this range is never empty.\n\nTo minimize $T(u) = N \\left( b + \\frac{h + d(s)}{u} \\right)$, we analyze its dependence on $u$. The parameters $N$, $b$, $h$, and $d(s)$ are non-negative constants for a given loop. We assume that $h + d(s)  0$, as otherwise $T(u)$ would be independent of $u$ and the optimization would be trivial. With $h + d(s)  0$, the term $\\frac{h+d(s)}{u}$ is a strictly decreasing function of $u$ for $u  0$. Consequently, the entire expression for $T(u)$ is a strictly decreasing function of $u$.\n\nTo minimize a strictly decreasing function over a closed interval of integers, one must choose the largest integer value in that interval. The largest permissible integer value for $u$ is the upper bound of the feasible set.\n$$u_{max} = \\min\\left(N, \\left\\lfloor \\frac{M}{S} \\right\\rfloor\\right)$$\nTherefore, the optimal unroll factor $u^{\\star}$ is this maximum allowed value.\n$$u^{\\star} = \\min\\left(N, \\left\\lfloor \\frac{M}{S} \\right\\rfloor\\right)$$\nThis expression gives the integer unroll factor that minimizes the execution time under the given model and constraints.",
            "answer": "$$\\boxed{\\min\\left(N, \\left\\lfloor \\frac{M}{S} \\right\\rfloor\\right)}$$"
        },
        {
            "introduction": "Effective optimization often involves juggling competing resource demands. This practice  explores such a scenario with rematerialization, a technique that can reduce register pressure at the cost of re-computing values. You will apply a hierarchical optimization goal—prioritizing register usage only when it exceeds a critical threshold—to see how compilers make sophisticated decisions when objectives like register allocation and execution speed conflict.",
            "id": "3628550",
            "problem": "Consider a single basic block with twelve instructions and a simple machine cost model where each instruction has an integer cycle cost. The primary optimization goal is defined lexicographically as follows: when the register pressure $R$ exceeds a threshold $\\rho$, minimize $R$ first; otherwise, minimize execution time $T$. Assume rematerialization is permitted only for values computed from constants and that rematerialization recomputes such a value at each use site instead of keeping it live across the block.\n\nUse the following foundational definitions as the base for reasoning. Register pressure $R$ is the maximum over all program points of the number of variables that are simultaneously live. A variable is live at a program point if its value will be used at some later instruction in the block. Execution time $T$ is modeled as the sum of the cycle costs of all instructions in the block. Rematerialization is the transformation that removes a definition of a recomputable value and inserts an equivalent definition immediately before each of its uses; this transformation shortens the live range of that value but increases $T$ by adding instructions. The focus is strictly on shaping optimization goals and scope: the decision to rematerialize is taken only to satisfy the primary goal when $R  \\rho$.\n\nThe block is:\n$$(I_1):\\ x \\leftarrow \\mathrm{load}(M_1)\\ (\\text{cost }4)$$\n$$(I_2):\\ y \\leftarrow \\mathrm{load}(M_2)\\ (\\text{cost }4)$$\n$$(I_3):\\ v \\leftarrow d + e\\ (\\text{cost }1)$$\n$$(I_4):\\ t_1 \\leftarrow x + y\\ (\\text{cost }1)$$\n$$(I_5):\\ t_2 \\leftarrow t_1 \\times c\\ (\\text{cost }2)$$\n$$(I_6):\\ t_3 \\leftarrow x - y\\ (\\text{cost }1)$$\n$$(I_7):\\ t_4 \\leftarrow t_3 + t_2\\ (\\text{cost }1)$$\n$$(I_8):\\ r_2 \\leftarrow t_3 + 1\\ (\\text{cost }1)$$\n$$(I_9):\\ r_3 \\leftarrow t_2 + 1\\ (\\text{cost }1)$$\n$$(I_{10}):\\ r_1 \\leftarrow t_1 + v\\ (\\text{cost }1)$$\n$$(I_{11}):\\ r_4 \\leftarrow t_4 + 1\\ (\\text{cost }1)$$\n$$(I_{12}):\\ r_5 \\leftarrow v + 1\\ (\\text{cost }1)$$\n\nAssume $d$, $e$, and $c$ are constants, so $v$ is rematerializable. The uses are:\n- $x$ at $(I_4)$ and $(I_6)$,\n- $y$ at $(I_4)$ and $(I_6)$,\n- $v$ at $(I_{10})$ and $(I_{12})$,\n- $t_1$ at $(I_5)$ and $(I_{10})$,\n- $t_2$ at $(I_7)$ and $(I_9)$,\n- $t_3$ at $(I_7)$ and $(I_8)$,\n- $t_4$ at $(I_{11})$.\n\nLet the threshold be $\\rho = 4$. Under the stated goal and cost model, decide whether rematerialization of $v$ should be applied, and compute the minimal additional execution time, in cycles, required to satisfy the goal. Express your final numeric answer as a single number. No rounding is required.",
            "solution": "The problem requires an evaluation of whether to apply the rematerialization optimization to a variable $v$ within a given basic block. The decision is governed by a lexicographical optimization goal: first, minimize register pressure $R$ if it exceeds a threshold $\\rho$; second, minimize execution time $T$.\n\nFirst, we analyze the initial state of the basic block without any transformations.\n\nThe total execution time, $T_{orig}$, is the sum of the costs of the $12$ instructions:\n$$T_{orig} = 4 + 4 + 1 + 1 + 2 + 1 + 1 + 1 + 1 + 1 + 1 + 1 = 19 \\text{ cycles}$$\n\nNext, we must determine the register pressure, $R_{orig}$. Register pressure is the maximum number of simultaneously live variables at any program point. A variable is live at a point if its value is used by a subsequent instruction. We determine the set of live variables at the program point immediately following each instruction.\n\nThe live ranges of the temporary variables are as follows:\n- $x$: Defined at $I_1$, last use at $I_6$.\n- $y$: Defined at $I_2$, last use at $I_6$.\n- $v$: Defined at $I_3$, last use at $I_{12}$.\n- $t_1$: Defined at $I_4$, last use at $I_{10}$.\n- $t_2$: Defined at $I_5$, last use at $I_9$.\n- $t_3$: Defined at $I_6$, last use at $I_8$.\n- $t_4$: Defined at $I_7$, last use at $I_{11}$.\n\nLet us determine the size of the live variable set at critical points:\n- After $I_1$: $\\{x\\}$, size $1$.\n- After $I_2$: $\\{x, y\\}$, size $2$.\n- After $I_3$: $\\{x, y, v\\}$, size $3$.\n- After $I_4$: $\\{x, y, v, t_1\\}$, size $4$.\n- After $I_5$: The variables $x$ (used at $I_6$), $y$ (used at $I_6$), $v$ (used at $I_{10}, I_{12}$), $t_1$ (used at $I_{10}$), and $t_2$ (used at $I_7, I_9$) are all live. The live set is $\\{x, y, v, t_1, t_2\\}$, and its size is $5$.\n- After $I_6$: $x$ and $y$ are no longer live. $t_3$ becomes live. The live set is $\\{v, t_1, t_2, t_3\\}$, size $4$.\n- After $I_7$: $t_2$ (used at $I_9$) and $t_3$ (used at $I_8$) are still live. $t_4$ becomes live. The live set is $\\{v, t_1, t_2, t_3, t_4\\}$, size $5$.\n\nThe maximum number of simultaneously live variables is $5$. Therefore, the initial register pressure is $R_{orig} = 5$.\nThe problem states a register pressure threshold of $\\rho = 4$. Since $R_{orig} = 5 > \\rho = 4$, the primary optimization goal is to minimize register pressure. We must evaluate if rematerialization can achieve this.\n\nSecond, we analyze the state of the basic block with rematerialization of $v$.\nThe variable $v$ is computed from constants ($d$ and $e$) and is therefore rematerializable. The transformation involves removing the original definition of $v$ at instruction $I_3$ and inserting a new definition immediately before each of its uses at $I_{10}$ and $I_{12}$.\n\nThe instruction $I_3$ (cost $1$) is removed. Two new instructions, each a copy of $I_3$ and each with cost $1$, are inserted into the block.\nThe total number of instructions becomes $12 - 1 + 2 = 13$.\nThe new execution time, $T_{remat}$, is:\n$$T_{remat} = T_{orig} - \\text{cost}(I_3) + 2 \\times \\text{cost}(\\text{remat_op}) = 19 - 1 + 2 \\times 1 = 20 \\text{ cycles}$$\n\nNow, we compute the new register pressure, $R_{remat}$. The key effect of rematerializing $v$ is the elimination of its long live range, which spanned from after $I_3$ to after $I_{12}$. The new rematerialized instances of $v$ have extremely short live ranges, existing only between their new definition and their single corresponding use.\nWe re-evaluate the size of the live sets at the same critical program points, noting that the variable $v$ is no longer present:\n- After $I_5$: The live set is now $\\{x, y, t_1, t_2\\}$. Its size is $4$.\n- After $I_7$: The live set is now $\\{t_1, t_2, t_3, t_4\\}$. Its size is $4$.\n\nThe maximum number of live variables at any point in the main body of the block is now $4$. The insertion of the new definitions does not create a new maximum. For example, just after the rematerialization of $v$ before $I_{10}$, the live set is $\\{t_1, t_4, v\\}$, which has size $3$. Thus, the new register pressure is $R_{remat} = 4$.\n\nFinally, we apply the decision logic.\nThe initial state has $R_{orig} = 5$, which violates the condition $R \\le \\rho$. The primary goal is to minimize $R$.\nThe rematerialization transformation reduces the register pressure from $R_{orig} = 5$ to $R_{remat} = 4$. Since $R_{remat}  R_{orig}$, the transformation is beneficial and, according to the stated goal, mandatory. By applying rematerialization, the new register pressure $R_{remat} = 4$ satisfies the condition $R \\le \\rho$.\n\nThe question asks for the minimal additional execution time required to satisfy the goal. The transformation was necessary to satisfy the primary goal, and this transformation increased the execution time.\nThe additional execution time is the difference between the new and original execution times:\n$$\\Delta T = T_{remat} - T_{orig} = 20 - 19 = 1$$\n\nTherefore, rematerialization of $v$ should be applied, and the minimal additional execution time required to satisfy the optimization goal is $1$ cycle.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "An optimization that appears perfectly safe in a single-threaded context can introduce subtle and catastrophic bugs in a concurrent program. This final exercise  confronts this modern challenge by examining a seemingly straightforward optimization—hoisting a check out of a loop. You will discover how the presence of another thread radically changes the analysis, demonstrating why the *scope* of optimization must encompass the entire program's concurrent behavior to ensure correctness.",
            "id": "3628541",
            "problem": "Consider a shared-memory program in a simple imperative language with two threads, a shared array $A$ of capacity $N$, and a shared integer $n$ that represents the current logical length of $A$ satisfying $0 \\leq n \\leq N$. The memory model is Sequential Consistency (SC), meaning that all operations from all threads can be viewed as interleaved in a single total order that respects each thread’s program order. The compiler must preserve observable behavior under this SC model, where observable behavior includes memory safety (no out-of-bounds access), values read and written to shared locations, and control-flow decisions that depend on shared data.\n\nThread $T_1$ performs a loop that aggregates elements of $A$ subject to a per-iteration bounds check using the current value of $n$:\n- For each integer $i$ starting at $0$ and increasing by $1$ up to and including $N-1$, the loop body reads $n$ and executes: if $i  n$, then it reads $A[i]$ and adds it to a local accumulator $s$. The accumulation is a local computation and does not write to shared memory. Formally, on iteration $i$, $T_1$ evaluates the predicate $i  n$ using the value of $n$ read at that iteration and accesses $A[i]$ if and only if that predicate is true.\n\nThread $T_2$ may concurrently decrease $n$ during the execution of $T_1$ to reflect a shrink of the logical array. Furthermore, when $T_2$ decreases $n$ to some value $n'$, it may immediately reuse or invalidate positions $A[j]$ for all integers $j$ with $n' \\leq j \\leq N-1$ (for example, by transferring those positions to another pool or marking them as inaccessible). Assume that all reads and writes of $n$ and $A[\\cdot]$ are atomic in the SC sense, but there is no synchronization that establishes a happens-before relation between $T_1$ and $T_2$.\n\nA compiler considers the following optimization of $T_1$: instead of reading $n$ and checking $i  n$ on every iteration, it takes a single snapshot $n_0$ of $n$ before the loop and replaces the guarded loop by an unguarded loop that runs $i$ from $0$ to $n_0 - 1$, accessing $A[i]$ on every iteration. In other words, it transforms iteration-wise checks $i  n$ into a fixed bound $n_0$, assuming that $n$ is invariant during the loop.\n\nBased on the above scenario and the SC model, which of the following statements about the correctness of this optimization are true?\n\nA. Under Sequential Consistency, if $T_2$ can decrease $n$ during $T_1$’s loop and immediately invalidate $A[j]$ for $j \\geq n$ at the moment of decrease, then replacing per-iteration checks $i  n$ with a single pre-loop snapshot $n_0$ is not semantics-preserving, because the optimized loop may access $A[i]$ for indices $i$ such that $i \\geq n$ at the time of access, violating memory safety.\n\nB. Declaring $n$ as an atomic variable alone suffices to make the optimization sound, because atomicity ensures visibility of concurrent writes and thus the single snapshot $n_0$ is an acceptable replacement for repeated checks.\n\nC. The optimization is sound if the optimization scope is constrained to cases where there is a proof (for example, via whole-program analysis or language-level guarantees) that no thread other than $T_1$ writes to $n$ in the dynamic extent of the loop, so that $n$ is effectively invariant while the loop runs.\n\nD. The optimization is sound regardless of concurrency, because $i$ increases monotonically and $i  n$ at loop entry implies that $i  n$ for all subsequent iterations.\n\nE. The optimization is sound if the compiler’s scope is defined as intra-thread semantics that ignores concurrent writes from other threads to shared variables, since correctness can be judged by the behavior of $T_1$ alone.\n\nSelect all that apply. Justify your choice using the SC model and the stated notion of observable behavior. Your justification should make explicit use of the definitions of Sequential Consistency, observable behavior, and the role of per-iteration checks $i  n$ versus a pre-loop snapshot $n_0$ in determining which $A[i]$ are accessed.",
            "solution": "The problem statement is analyzed for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- **Threads and Memory:** A shared-memory program with $2$ threads, $T_1$ and $T_2$.\n- **Shared Variables:**\n    - An array $A$ of capacity $N$.\n    - An integer $n$ representing the logical length of $A$, with $0 \\leq n \\leq N$.\n- **Memory Model:** Sequential Consistency (SC). All operations are part of a single total order that respects the program order of each thread.\n- **Observable Behavior:** The compiler must preserve memory safety (no out-of-bounds access), values read/written to shared locations, and control-flow decisions dependent on shared data.\n- **Thread $T_1$ (Original Behavior):**\n    - Executes a loop for integer $i$ from $0$ to $N-1$.\n    - In each iteration $i$, it reads $n$ and checks if $i  n$.\n    - If and only if $i  n$ is true, it reads $A[i]$ and adds it to a local accumulator $s$.\n- **Thread $T_2$ (Concurrent Behavior):**\n    - May decrease the value of $n$ to a new value $n'$.\n    - Upon decreasing $n$ to $n'$, it may immediately invalidate memory locations $A[j]$ for all $j$ such that $n' \\leq j \\leq N-1$.\n- **Atomicity and Synchronization:**\n    - All reads and writes to $n$ and elements of $A$ are atomic.\n    - There is no synchronization that establishes a happens-before relation between $T_1$ and $T_2$.\n- **The Compiler Optimization:**\n    - The loop in $T_1$ is transformed.\n    - A single snapshot $n_0$ of $n$ is taken before the loop begins.\n    - The loop is changed to iterate from $i=0$ to $n_0-1$.\n    - Inside this new loop, $A[i]$ is accessed unconditionally.\n    - This optimization effectively assumes $n$ is loop-invariant.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is grounded in the well-established domain of compiler construction and concurrent programming. The concepts of Sequential Consistency, data races, atomic operations, and loop-invariant code motion are fundamental topics in computer science. The scenario described is a classic example of a Time-of-check-to-time-of-use (TOCTOU) race condition introduced by a seemingly correct optimization.\n- **Well-Posed:** The problem provides a clear and formal description of the system, the memory model, the behavior of the threads, and the specific optimization being considered. The question asks for an evaluation of the optimization's correctness based on these precise definitions. A definite answer can be derived.\n- **Objective:** The language used is formal and unambiguous. Terms like \"Sequential Consistency\", \"atomic\", and \"observable behavior\" have precise technical meanings.\n\nThe problem statement has no scientific or factual unsoundness, is formalizable, is complete, describes a realistic scenario in systems programming, and is well-posed.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation of Solution\nThe correctness of the optimization hinges on whether all observable behaviors of the optimized program are also legal behaviors of the original program under the Sequential Consistency (SC) model. Observable behavior explicitly includes memory safety.\n\nLet's analyze the behavior of the original program.\nIn the original program, Thread $T_1$ executes a loop from $i=0$ to $N-1$. In each iteration $i$, it performs two distinct serial operations:\n1.  Read the current value of the shared variable $n$.\n2.  Based on the value read, decide whether to access $A[i]$.\n\nConsider a possible interleaving under SC. Let the initial state be $n=k$ for some $k  0$.\n- $T_1$ starts its loop. For $i=0, 1, \\dots, m-1$ (where $m  k$), it reads $n=k$, finds $ik$ to be true, and accesses $A[i]$.\n- Now, an operation from $T_2$ is interleaved. $T_2$ writes a new value to $n$, say $n_{new} = m$. According to the problem, $T_2$ may then invalidate memory locations $A[m], A[m+1], \\dots$.\n- $T_1$ continues its loop at iteration $i=m$. It reads the current value of $n$, which is now $n_{new}=m$.\n- $T_1$ evaluates the predicate $i  n$, which is $m  m$. This is false.\n- Consequently, $T_1$ does **not** access $A[m]$. The check successfully prevents a memory access to an index that is no longer part of the logical array, thus ensuring memory safety.\n\nNow, let's analyze the behavior of the optimized program.\nThe optimized program first reads $n$ into a private local variable $n_0$. Let's assume it reads $n_0=k$. The loop then becomes `for i from 0 to k-1, access A[i]`. This loop no longer contains any reads of the shared variable $n$.\n\nConsider the same interleaving under SC:\n- $T_1$ is about to start its loop. It executes the pre-loop code: read $n$ into $n_0$. At this time, $n=k$, so $n_0=k$.\n- $T_1$ begins its loop, which will run for $i$ from $0$ to $k-1$.\n- For $i=0, 1, \\dots, m-1$, $T_1$ accesses $A[i]$.\n- Now, an operation from $T_2$ is interleaved. $T_2$ writes $n_{new}=m$ to $n$ and invalidates $A[m], A[m+1], \\dots$.\n- $T_1$ continues its loop at iteration $i=m$. The loop's upper bound is fixed at $n_0-1 = k-1$. Since $m  k$, the loop continues.\n- $T_1$ proceeds to access $A[m]$.\n- At this point in the total order of operations, the shared variable $n$ has the value $m$, and the location $A[m]$ has been invalidated by $T_2$. The access to $A[m]$ by $T_1$ is an access to an invalid memory location. This constitutes a memory safety violation.\n\nThis new behavior—the memory safety violation—was not possible in the original program. Therefore, the optimization is not semantics-preserving. It introduces a bug (a data race leading to a safety violation).\n\n### Option-by-Option Analysis\n\n**A. Under Sequential Consistency, if $T_2$ can decrease $n$ during $T_1$’s loop and immediately invalidate $A[j]$ for $j \\geq n$ at the moment of decrease, then replacing per-iteration checks $i  n$ with a single pre-loop snapshot $n_0$ is not semantics-preserving, because the optimized loop may access $A[i]$ for indices $i$ such that $i \\geq n$ at the time of access, violating memory safety.**\n- **Justification:** This statement accurately describes the flaw derived above. The optimization creates a time-of-check-to-time-of-use (TOCTOU) vulnerability. The \"check\" is the single read of $n$ into $n_0$ before the loop, and the \"use\" is the access to $A[i]$ within the loop. If $n$ is decreased by $T_2$ between the check and the use, the optimized loop in $T_1$ may access an index $i$ that is valid according to the stale snapshot $n_0$ but invalid according to the current value of $n$. This violates memory safety, which is part of the observable behavior the compiler must preserve.\n- **Verdict:** **Correct**.\n\n**B. Declaring $n$ as an atomic variable alone suffices to make the optimization sound, because atomicity ensures visibility of concurrent writes and thus the single snapshot $n_0$ is an acceptable replacement for repeated checks.**\n- **Justification:** The problem statement already assumes that reads and writes of $n$ are atomic. Atomicity of an operation prevents torn reads or writes; it ensures that a read sees either the entire old value or the entire new value, not a mix. However, it does not prevent the TOCTOU race. The problem is not the atomicity of the single read of $n_0$, but the fact that the value read can become stale later during the loop's execution. A single atomic read does not provide any guarantee about the variable's value at a later point in time. The reasoning in the option is flawed.\n- **Verdict:** **Incorrect**.\n\n**C. The optimization is sound if the optimization scope is constrained to cases where there is a proof (for example, via whole-program analysis or language-level guarantees) that no thread other than $T_1$ writes to $n$ in the dynamic extent of the loop, so that $n$ is effectively invariant while the loop runs.**\n- **Justification:** The entire problem arises from the concurrent modification of $n$ by $T_2$. If it can be proven that no other thread modifies $n$ while $T_1$'s loop is executing, then $n$ is functionally a loop-invariant variable. In that case, reading $n$ once before the loop is semantically identical to reading it in every iteration, as the value would be the same each time. The transformation would be a valid instance of Loop-Invariant Code Motion (LICM). Therefore, under this very strong condition, the optimization is sound.\n- **Verdict:** **Correct**.\n\n**D. The optimization is sound regardless of concurrency, because $i$ increases monotonically and $i  n$ at loop entry implies that $i  n$ for all subsequent iterations.**\n- **Justification:** This statement makes a false assumption. It implicitly assumes that $n$ is constant or non-decreasing. The problem explicitly states that $T_2$ may *decrease* $n$. If $i$ increases while $n$ decreases, the condition $i  n$ can easily change from true to false. For example, if at one point $i=4$ and $n=5$ ($in$ is true), in the next moment $i$ could be $5$ and $n$ could be $3$, making $in$ false. The reasoning presented is logically invalid in the context of the problem.\n- **Verdict:** **Incorrect**.\n\n**E. The optimization is sound if the compiler’s scope is defined as intra-thread semantics that ignores concurrent writes from other threads to shared variables, since correctness can be judged by the behavior of $T_1$ alone.**\n- **Justification:** This proposes a fundamentally incorrect model for compiling concurrent programs. A compiler for a language with shared-memory concurrency cannot ignore the effects of other threads. The semantics of such languages (like the SC model specified here) are defined precisely to handle the interactions between threads. Judging correctness based on \"intra-thread semantics\" alone is a recipe for introducing data races and other concurrency bugs. An optimization is only sound if it preserves the semantics of the full program, including all inter-thread behaviors allowed by the memory model. Claiming an optimization is \"sound\" within a flawed model that ignores the core problem is a meaningless and dangerous assertion.\n- **Verdict:** **Incorrect**.",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}