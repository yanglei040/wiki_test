{
    "hands_on_practices": [
        {
            "introduction": "To generate efficient code, a compiler must not only choose the right instructions but also arrange them optimally in memory. This exercise  delves into the problem of basic block layout, where the goal is to minimize branch misprediction penalties. You will develop a simple cost model for a processor with static fall-through branch prediction and use probabilistic analysis to determine the code sequence that yields the lowest expected execution time, learning how to navigate trade-offs when global constraints prevent achieving every local optimum.",
            "id": "3674228",
            "problem": "You are modeling a simple target machine to guide basic block ordering. The machine model has the following properties:\n- The processor executes a conditional branch with a fixed base cost of $b$ cycles, independent of the outcome. A conditional branch is statically predicted to follow the fall-through path, which is the basic block that is laid out immediately after the branch. If the actual next block at runtime is not the fall-through, an additional misprediction penalty of $M$ cycles is incurred for that dynamic branch.\n- There is no cost for an unconditional fall-through; there are no other penalties in this model beyond those stated.\n- The expected execution time is the expected value of the sum of the cycles of all executed blocks and branch overheads.\n\nConsider a function with the following control-flow graph (CFG). Let $E$ denote the entry block. Block $E$ ends with a conditional branch that goes to block $A$ with probability $p$ and to block $B$ with probability $1-p$. Block $A$ ends with a conditional branch that goes to a join block $J$ with probability $q_A$ and to a return block $R_A$ with probability $1-q_A$. Similarly, block $B$ ends with a conditional branch that goes to $J$ with probability $q_B$ and to a return block $R_B$ with probability $1-q_B$. Blocks $J$, $R_A$, and $R_B$ return and have no outgoing edges.\n\nThe compile-time layout is a single linear order of basic blocks. For a conditional branch, whichever successor block is placed immediately next in the order is its fall-through and hence is predicted by the hardware. The other successor is reached by taking the branch (still costing $b$ to execute the branch itself), and a misprediction penalty $M$ is paid exactly when the actual successor is not the fall-through. You may invert branch conditions freely at no extra cost. The join block $J$ can be placed immediately after at most one of $A$ or $B$ (because the layout is linear), but it need not be immediately after either.\n\nMachine and program parameters are:\n- Per-block compute costs (excluding any branch overhead): $c_E = 5$, $c_A = 8$, $c_B = 6$, $c_J = 4$, $c_{R_A} = 1$, $c_{R_B} = 1$ (all in cycles).\n- Base conditional-branch cost: $b = 1$ (cycles).\n- Probabilities: $p = 0.6$, $q_A = 0.8$, $q_B = 0.7$.\n\nTask: Using first principles (linearity of expectation and the definition of the misprediction event under a static fall-through predictor), determine the basic block order (i.e., choose the fall-through successors for the branches in $E$, $A$, and $B$ subject to the linear layout constraints) that minimizes the expected cycles per function execution. Then, express the minimal expected cycles per function execution as a closed-form function of $M$. Do not introduce any additional costs beyond those stated. Provide your final answer as a single analytic expression in terms of $M$. No units are required. Do not round; give the exact expression.",
            "solution": "The problem asks for the minimal expected execution time of a function, expressed as a function of the misprediction penalty $M$. The optimization is to be achieved by choosing an optimal linear layout of the basic blocks, which in turn determines the statically predicted fall-through path for each conditional branch.\n\nFirst, we establish the objective function, which is the total expected execution time, $E[T]$. By the linearity of expectation, we can express this as the sum of the expected costs of executing the basic blocks and the expected overhead costs from the branches.\n$$E[T] = E[C_{\\text{blocks}}] + E[C_{\\text{branch}}]$$\n\nLet us calculate the expected cost from basic block computations, $E[C_{\\text{blocks}}]$. This cost is independent of the block layout.\nA block $X$ with computation cost $c_X$ is executed with some probability $P(X)$. The expected cost is $\\sum_X P(X) c_X$.\nThe probabilities of executing each block are:\n- $P(E) = 1$\n- $P(A) = p$\n- $P(B) = 1-p$\n- $P(J) = P(E \\to A \\to J) + P(E \\to B \\to J) = p q_A + (1-p) q_B$\n- $P(R_A) = P(E \\to A \\to R_A) = p(1-q_A)$\n- $P(R_B) = P(E \\to B \\to R_B) = (1-p)(1-q_B)$\n\nSubstituting the given values $p=0.6$, $q_A=0.8$, $q_B=0.7$, and the block costs $c_E=5$, $c_A=8$, $c_B=6$, $c_J=4$, $c_{R_A}=1$, $c_{R_B}=1$:\n- $P(A) = 0.6$\n- $P(B) = 0.4$\n- $P(J) = (0.6)(0.8) + (0.4)(0.7) = 0.48 + 0.28 = 0.76$\n- $P(R_A) = (0.6)(1-0.8) = 0.12$\n- $P(R_B) = (0.4)(1-0.7) = 0.12$\n\nThe expected block computation cost is:\n$$E[C_{\\text{blocks}}] = P(E)c_E + P(A)c_A + P(B)c_B + P(J)c_J + P(R_A)c_{R_A} + P(R_B)c_{R_B}$$\n$$E[C_{\\text{blocks}}] = 1(5) + (0.6)(8) + (0.4)(6) + (0.76)(4) + (0.12)(1) + (0.12)(1)$$\n$$E[C_{\\text{blocks}}] = 5 + 4.8 + 2.4 + 3.04 + 0.12 + 0.12 = 15.48$$\n\nNext, we analyze the expected branch overhead, $E[C_{\\text{branch}}]$. The model states that each executed conditional branch has a base cost of $b$ cycles and an additional misprediction penalty of $M$ cycles if the taken path is not the statically predicted fall-through path. The problem states we can invert branch conditions freely, which means for any branch, we can choose which of its two successors becomes the fall-through path by arranging the layout, and the goal is to choose the fall-through to minimize the expected misprediction penalty.\n\nThe total expected branch cost is the sum of costs from the branches at $E$, $A$, and $B$, weighted by their execution probabilities.\n$$E[C_{\\text{branch}}] = E[C_{\\text{branch},E}] + E[C_{\\text{branch},A}] + E[C_{\\text{branch},B}]$$\nThe branch at $E$ is always executed. The branch at $A$ is executed with probability $p$. The branch at $B$ is executed with probability $1-p$.\nThe overhead for a single branch consists of the base cost $b$ and a potential misprediction penalty $M$.\nThe total expected base cost is $1 \\cdot b + p \\cdot b + (1-p) \\cdot b = 2b = 2(1) = 2$.\nThe total expected misprediction penalty, $E[C_{\\text{penalty}}]$, is what we need to minimize through layout choices.\n$$E[C_{\\text{penalty}}] = M \\left( P(\\text{mispredict at E}) + p \\cdot P(\\text{mispredict at A}) + (1-p) \\cdot P(\\text{mispredict at B}) \\right)$$\nTo minimize the penalty for a single branch, we must choose the more probable successor as the fall-through.\n- **Branch from E:** Successors are $A$ (prob $p=0.6$) and $B$ (prob $1-p=0.4$). To minimize penalty, $A$ should be the fall-through. This is always possible. The misprediction probability is $1-p=0.4$.\n- **Branch from A:** Successors are $J$ (prob $q_A=0.8$) and $R_A$ (prob $1-q_A=0.2$). To minimize penalty, $J$ should be the fall-through. This requires the layout `...; A; J; ...`.\n- **Branch from B:** Successors are $J$ (prob $q_B=0.7$) and $R_B$ (prob $1-q_B=0.3$). To minimize penalty, $J$ should be the fall-through. This requires the layout `...; B; J; ...`.\n\nThe core constraint is that the layout is linear. A block can have only one immediate successor. Therefore, block $J$ cannot be the fall-through successor for both block $A$ and block $B$. This means we cannot simultaneously satisfy the optimal fall-through choices for the branches at $A$ and $B$. We must make a suboptimal choice for one of them.\n\nWe must compare two main strategies, assuming we always make the optimal choice for the branch at $E$ (fall-through to $A$), which is unconstrained.\n**Strategy 1:** Prioritize the branch at $A$. Set the fall-through for $A$ to be $J$. This forces the fall-through for $B$ to be $R_B$ (the less likely path).\nThe layout choices are:\n- Branch E: Fall-through is $A$ (optimal). Misprediction prob: $1-p=0.4$.\n- Branch A: Fall-through is $J$ (optimal). Misprediction prob: $1-q_A=0.2$.\n- Branch B: Fall-through is $R_B$ (suboptimal). Misprediction prob: $q_B=0.7$.\nThe total expected number of mispredictions, $N_1$, is:\n$$N_1 = (1-p) + p(1-q_A) + (1-p)q_B$$\n$$N_1 = 0.4 + (0.6)(0.2) + (0.4)(0.7) = 0.4 + 0.12 + 0.28 = 0.80$$\n\n**Strategy 2:** Prioritize the branch at $B$. Set the fall-through for $B$ to be $J$. This forces the fall-through for $A$ to be $R_A$ (the less likely path).\nThe layout choices are:\n- Branch E: Fall-through is $A$ (optimal). Misprediction prob: $1-p=0.4$.\n- Branch A: Fall-through is $R_A$ (suboptimal). Misprediction prob: $q_A=0.8$.\n- Branch B: Fall-through is $J$ (optimal). Misprediction prob: $1-q_B=0.3$.\nThe total expected number of mispredictions, $N_2$, is:\n$$N_2 = (1-p) + p(q_A) + (1-p)(1-q_B)$$\n$$N_2 = 0.4 + (0.6)(0.8) + (0.4)(0.3) = 0.4 + 0.48 + 0.12 = 1.00$$\n\nComparing the two strategies, $N_1 = 0.80$ is less than $N_2 = 1.00$. Therefore, Strategy 1 yields the minimal expected misprediction penalty. This penalty is $E[C_{\\text{penalty}}]_{\\text{min}} = N_1 M = 0.8M$.\n\nThe minimal total expected execution time is the sum of the constant costs and this minimal penalty.\n$$E[T]_{\\text{min}} = E[C_{\\text{blocks}}] + E[C_{\\text{base branch}}] + E[C_{\\text{penalty}}]_{\\text{min}}$$\n$$E[T]_{\\text{min}} = 15.48 + 2b + 0.8M$$\nWith $b=1$:\n$$E[T]_{\\text{min}} = 15.48 + 2 + 0.8M$$\n$$E[T]_{\\text{min}} = 17.48 + 0.8M$$\nTo express this with exact fractions:\n$17.48 = \\frac{1748}{100} = \\frac{437}{25}$.\n$0.8 = \\frac{8}{10} = \\frac{4}{5}$.\nSo, the minimal expected cycles per execution is $\\frac{437}{25} + \\frac{4}{5}M$. The decimal form is equally exact.",
            "answer": "$$\\boxed{17.48 + 0.8M}$$"
        },
        {
            "introduction": "An effective target machine model must often look beyond the abstract Instruction Set Architecture (ISA) to capture subtle performance effects of the underlying microarchitecture. This practice  explores the real-world issue of partial register stalls, a performance hazard on many modern processors where writing to a small part of a register (like `AL`) creates a hidden dependency on the register's previous full value. By analyzing this phenomenon, you will see why a sophisticated compiler model penalizes such partial writes and prefers instructions that perform full-width updates, thereby guiding the instruction selector to break these \"false\" dependencies and unlock greater performance.",
            "id": "3674249",
            "problem": "An optimizing compiler is building a target machine model for code selection and scheduling on an x86-64 core. The model must capture the performance impact of partial register writes. Consider an out-of-order (OoO) superscalar core with register renaming that eliminates write-after-read (WAR) and write-after-write (WAW) hazards but preserves true read-after-write (RAW) dependencies. On this core, the Instruction Set Architecture (ISA) semantics for partial register writes to the low-byte subregister (for example, writing to AL) have the following microarchitectural effect: the first subsequent consumer that reads the full-width register (for example, reading EAX) must internally merge the high bits from the prior full-width value. The core implements this as an extra merge operation that adds a stall of $1$ cycle to the first such consumer and creates an artificial dependency (a false dependency) on the prior full-width value. A full-width write to EAX breaks the dependency; for example, a zero-extending move such as MOVZX into EAX produces a full-width result. Writes to a $32$-bit register also zero-extend to the corresponding $64$-bit register on this ISA, breaking dependencies on the upper $32$ bits.\n\nAs foundational modeling facts, assume:\n- The core is out-of-order with sufficient rename resources so that only true or ISA-induced dependencies constrain the critical path.\n- The steady-state initiation interval of a tight loop equals the maximum of (i) the resource-constrained lower bound and (ii) the longest loop-carried dependency latency, a standard result in modulo scheduling and pipeline theory.\n- A load that hits in Level-$1$ data cache has a fixed latency of $4$ cycles.\n- An integer add has a latency of $1$ cycle.\n- The “partial-register merge” described above adds $1$ cycle to the first consumer that reads EAX after a write to AL.\n- The instruction MOVZX EAX, r/m8 or MOVZX EAX, r/m16 is a full-width write to EAX with the same latency as the underlying load when its source is memory, and it removes any dependency on prior EAX contents.\n- Front-end bandwidth, execution ports, and retirement bandwidth are provisioned such that resource throughput is not the bottleneck for the code sequences below; the longest loop-carried dependency dominates the steady-state cost.\n\nConsider two loop kernels that run for a large trip count so that steady state dominates:\n\nKernel P:\n  L:  mov   al, byte ptr [rsi]\n      add   eax, edx\n      dec   ecx\n      jne   L\n\nKernel Q:\n  L:  movzx eax, byte ptr [rsi]\n      add   eax, edx\n      dec   ecx\n      jne   L\n\nHere, the memory operand [rsi] is invariant and always hits in Level-$1$ cache; the branch is perfectly predicted; and no other memory or control effects interfere. The destination of ADD is EAX in both kernels. The compiler must decide whether to model a stall for Kernel P and whether to prefer MOVZX or other full-width writes to avoid false dependencies.\n\nWhich of the following statements best captures the correct target machine modeling and code selection outcome under the assumptions above?\n\nA. Kernel P has a steady-state cost of $6$ cycles per iteration and Kernel Q has $5$ cycles per iteration, because the partial-register merge in Kernel P creates a loop-carried false dependency that adds $1$ cycle to the consumer of EAX, while MOVZX in Kernel Q produces a full-width write to EAX that eliminates this dependency. Therefore, the model should penalize partial-register writes and prefer MOVZX or any full-width write to break the dependency.\n\nB. Both kernels have $5$ cycles per iteration because register renaming removes all effects of partial-register writes; MOVZX has no performance advantage and should not be preferred.\n\nC. Kernel P has $5$ cycles per iteration and Kernel Q has $6$ cycles per iteration because MOVZX adds a zero-extension latency on top of the load; therefore, selecting MOVZX harms performance.\n\nD. Kernel P is slower than Kernel Q, but the difference must exceed $1$ cycle per iteration because the merge penalty applies to every consumer of EAX in the iteration and accumulates; thus Kernel P has at least $7$ cycles per iteration while Kernel Q has $5$ cycles per iteration.",
            "solution": "The user wants a meticulous and exacting professor in the STEM fields to solve a problem regarding target machine modeling for an optimizing compiler. The professor's foremost responsibility is to critically validate the problem statement, excluding the options, before attempting a solution.\n\n### Step 1: Extract Givens\n\nThe provided information is as follows:\n- **Core Architecture**: Out-of-order (OoO) superscalar core with register renaming.\n- **Hazard Handling**: Register renaming eliminates write-after-read (WAR) and write-after-write (WAW) hazards. True read-after-write (RAW) dependencies are preserved.\n- **ISA Semantics (Partial Register Write)**: A write to a low-byte subregister (e.g., `$al$`) followed by a read of the full-width register (e.g., `$eax$`) has a specific microarchitectural effect.\n- **Microarchitectural Effect**: The first consumer reading the full-width register must merge the high bits from the prior full-width value. This incurs a stall of $1$ cycle and creates an artificial (false) dependency on the prior full-width value.\n- **Dependency Breaking**: A full-width write to `$eax$` (e.g., `MOVZX` into `$eax$`) breaks this dependency. Writes to a $32$-bit register (e.g., `$eax$`) zero-extend to the corresponding $64$-bit register (e.g., `$rax$`), which also acts as a full-width write breaking dependencies on the upper $32$ bits.\n- **Latency Data**:\n  - Level-$1$ data cache hit load: $4$ cycles.\n  - Integer add: $1$ cycle.\n  - Partial-register merge stall: $1$ cycle, added to the first consumer.\n  - `MOVZX EAX, r/m8` or `MOVZX EAX, r/m16` from memory has the same latency as the underlying load ($4$ cycles).\n- **Performance Model**:\n  - The core has sufficient resources (rename registers, front-end bandwidth, execution ports, retirement bandwidth) that resource throughput is not the bottleneck.\n  - The steady-state initiation interval ($II$) of a tight loop is determined by the longest loop-carried dependency latency.\n- **Loop Kernels**:\n  - **Kernel P**:\n    ```assembly\n    L:  mov   al, byte ptr [rsi]\n        add   eax, edx\n        dec   ecx\n        jne   L\n    ```\n  - **Kernel Q**:\n    ```assembly\n    L:  movzx eax, byte ptr [rsi]\n        add   eax, edx\n        dec   ecx\n        jne   L\n    ```\n- **Loop Invariants/Assumptions**:\n  - Memory operand `[rsi]` is invariant and always hits in Level-$1$ cache.\n  - The branch `jne L` is perfectly predicted.\n  - No other memory or control effects interfere.\n  - The destination of the `add` instruction is `$eax$` in both kernels.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement describes a performance modeling scenario based on the interaction between Instruction Set Architecture (ISA) features and microarchitectural implementation in a modern processor.\n- **Scientifically Grounded**: The concepts are fundamental to computer architecture and compiler design. Partial register stalls (or penalties) are a real performance issue on various x86-64 microarchitectures (e.g., Intel's P6, Sandy Bridge, and their successors). Register renaming, RAW/WAR/WAW hazards, out-of-order execution, and modulo scheduling are all standard, well-established principles. The latencies provided are realistic for such a core. The problem is scientifically and factually sound.\n- **Well-Posed**: The problem is well-posed. It asks for the steady-state cost, defined as the initiation interval ($II$), which is in turn defined by the longest loop-carried dependency. It provides two concrete code snippets and all the necessary latency and dependency information to calculate this value for each kernel. A unique solution can be derived from the given data and rules.\n- **Objective**: The problem is stated in precise, objective, and technical language common to the field. Terms like \"latency,\" \"dependency,\" and \"initiation interval\" have formal meanings. There is no subjective or ambiguous language.\n\nAll other criteria for validity (formalizable, complete, feasible, non-trivial, etc.) are met.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. I will proceed with the solution.\n\n### Derivation of Solution\n\nThe problem asks for the steady-state cost of each loop kernel, which is defined as the initiation interval ($II$). The problem states that $II$ is determined by the longest loop-carried dependency latency, as resource contention is not a bottleneck. We must analyze the dependency chains that cross loop iteration boundaries for each kernel.\n\n**Analysis of Kernel P**\n\nThe code for Kernel P is:\n```assembly\nL:  mov   al, byte ptr [rsi]  ; P1\n    add   eax, edx          ; P2\n```\nThe central data flow involves the `$eax$` register. The `add eax, edx` instruction (P2) reads `$eax$` and writes to `$eax$`. The value of `$eax$` written by P2 in one iteration, say iteration $i-1$, becomes the input value for the next iteration, $i$.\n\nLet's trace the loop-carried dependency on `$eax$`:\n1.  In iteration $i$, the instruction `mov al, byte ptr [rsi]` (P1) is a partial write to `$eax$`. It loads a byte from memory and places it in `$al$`.\n2.  The next instruction, `add eax, edx` (P2), reads the full `$eax$` register. According to the problem statement, this triggers the partial-register merge mechanism.\n3.  The merge requires the high bits from the \"prior full-width value\" of `$eax$`. This prior value is the result of the `add eax, edx` instruction from the previous iteration, $i-1$. This creates a loop-carried dependency from P2 in iteration $i-1$ to P2 in iteration $i$.\n4.  P2 in iteration $i$ has two data dependencies to compute its `$eax$` input:\n    a. The low byte from P1 (`mov al, ...`), which is the result of a load.\n    b. The high bytes from P2 in the previous iteration (`add eax, ...`).\n5.  The problem states that this merge operation \"adds a stall of $1$ cycle to the first such consumer\".\n\nLet's calculate the latency of this recurrence. Let $T_{i-1}$ be the time when the result of P2 from iteration $i-1$ is available.\n-   The instruction P1 in iteration $i$ (`mov al, ...`) is a load with a latency of $4$ cycles. On an OoO core, it can be scheduled as early as possible. Let's assume it starts at time $T_{i-1}$. Its result (the byte value) is available at $T_{i-1} + 4$.\n-   The instruction P2 in iteration $i$ (`add eax, ...`) can start when its operands are ready. Its `$eax$` input depends on both the load (P1) and the previous add (P2 from $i-1$). The hardware needs the load to be complete and the prior full-width value to be available.\n-   The start time for P2 is determined by the latest-arriving input. The high bits are ready at $T_{i-1}$. The low byte is ready at $T_{i-1} + 4$. So, the combined value can begin to be formed at time $T_{i-1} + 4$.\n-   The merge operation adds a $1$-cycle stall. This means the start of P2 is delayed by an additional cycle. So, P2 in iteration $i$ can start at $(T_{i-1} + 4) + 1 = T_{i-1} + 5$.\n-   P2 itself (`add`) has a latency of $1$ cycle.\n-   Therefore, the result of P2 in iteration $i$ is available at time $T_i = (T_{i-1} + 5) + 1 = T_{i-1} + 6$.\n\nThe recurrence relation is $T_i = T_{i-1} + 6$. The latency of the loop-carried dependency is $6$ cycles.\nThus, the steady-state cost for Kernel P is $II_P = 6$ cycles per iteration.\n\n**Analysis of Kernel Q**\n\nThe code for Kernel Q is:\n```assembly\nL:  movzx eax, byte ptr [rsi] ; Q1\n    add   eax, edx          ; Q2\n```\n1.  In iteration $i$, the instruction `movzx eax, byte ptr [rsi]` (Q1) performs a full-width write to `$eax$`. It loads a byte and zero-extends it to fill the entire $32$-bit register.\n2.  The problem explicitly states that such a full-width write \"removes any dependency on prior EAX contents.\"\n3.  This means that the dependency on the result of `add eax, edx` from iteration $i-1$ is broken. There is no loop-carried dependency involving `$eax$`.\n4.  Since there is no loop-carried data dependency on `$eax$` or `$edx$` (which is assumed loop-invariant), the initiation interval ($II_Q$) is not determined by a data recurrence of the main computation. The only other recurrence is on `$ecx` (`dec ecx`), which has a latency of $1$ cycle.\n5.  However, the problem states that the steady-state cost equals the *longest* loop-carried dependency latency, and that resources are not a bottleneck, implying the recurrence is the dominant factor. The options present costs of $5$ or $6$ cycles, which suggests that a $1$-cycle latency from the loop counter is not the intended answer.\n6.  A plausible interpretation, consistent with performance modeling practices and the options provided, is that in the absence of a true recurrence, the \"cost\" is defined by the critical path latency of the work done inside a single loop iteration. For Kernel Q, this chain is Q1 - Q2.\n7.  The latency of Q1 (`movzx` from memory) is $4$ cycles.\n8.  The latency of Q2 (`add`) is $1$ cycle.\n9.  Q2 depends on the result of Q1. So, the total latency of this chain is `latency(Q1) + latency(Q2) = 4 + 1 = 5` cycles.\n\nUnder this interpretation, the cost for Kernel Q is $II_Q = 5$ cycles per iteration. This represents the time from starting the load to getting the final sum for that iteration, and in a simplified model where iterations cannot be perfectly pipelined for other reasons, this can represent the effective throughput limit.\n\n**Summary of Costs**:\n-   Kernel P cost: $6$ cycles/iteration.\n-   Kernel Q cost: $5$ cycles/iteration.\nThe performance model should therefore favor Kernel Q. This involves using full-width writes like `movzx` to avoid the partial register stall and break the false dependency chain.\n\n### Option-by-Option Analysis\n\n**A. Kernel P has a steady-state cost of $6$ cycles per iteration and Kernel Q has $5$ cycles per iteration, because the partial-register merge in Kernel P creates a loop-carried false dependency that adds $1$ cycle to the consumer of EAX, while MOVZX in Kernel Q produces a full-width write to EAX that eliminates this dependency. Therefore, the model should penalize partial-register writes and prefer MOVZX or any full-width write to break the dependency.**\n- My derivation shows the cost of Kernel P is $6$ cycles and Kernel Q is $5$ cycles.\n- The reasoning provided in this option perfectly matches the analysis. The partial write in P creates a loop-carried dependency with a total latency of $6$ cycles. The full-width write in Q breaks this dependency, and its performance is limited by the $5$-cycle intra-iteration dependency chain.\n- The conclusion about compiler modeling is also correct based on this analysis.\n- **Verdict: Correct.**\n\n**B. Both kernels have $5$ cycles per iteration because register renaming removes all effects of partial-register writes; MOVZX has no performance advantage and should not be preferred.**\n- This statement is factually incorrect according to the problem's premises. The problem explicitly states that this particular partial register effect is a microarchitectural artifact that register renaming does *not* solve, creating a false dependency. Thus, Kernel P must be slower than it would be without this effect. My analysis shows Kernel P costs $6$ cycles, not $5$.\n- **Verdict: Incorrect.**\n\n**C. Kernel P has $5$ cycles per iteration and Kernel Q has $6$ cycles per iteration because MOVZX adds a zero-extension latency on top of the load; therefore, selecting MOVZX harms performance.**\n- This option reverses the performance results. My analysis showed P is slower than Q ($6$ vs $5$ cycles).\n- The reasoning is also flawed. The problem states that `MOVZX` has the *same* latency as the underlying load, not an additional one.\n- **Verdict: Incorrect.**\n\n**D. Kernel P is slower than Kernel Q, but the difference must exceed $1$ cycle per iteration because the merge penalty applies to every consumer of EAX in the iteration and accumulates; thus Kernel P has at least $7$ cycles per iteration while Kernel Q has $5$ cycles per iteration.**\n- This option correctly identifies that Kernel P is slower than Kernel Q. However, it incorrectly calculates the penalty.\n- The problem states the stall is \"$1$ cycle\" for the \"first subsequent consumer\". In Kernel P, there is only one consumer (`add eax, edx`) of the partially-written `$eax$` register. Thus, the penalty is exactly $1$ cycle added to the dependency chain, not an accumulating penalty.\n- My calculation showed Kernel P's cost is $6$ cycles, not at least $7$. The performance difference is $6 - 5 = 1$ cycle.\n- **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "One of the most powerful optimizations a compiler can perform is strength reduction: replacing an expensive operation with an equivalent, cheaper sequence of instructions. This problem  challenges you to implement one of the most classic examples—eliminating division by a constant. Working from first principles of fixed-point arithmetic, you will derive a \"magic number\" and a shift amount that use a faster multiplication to produce the same result as a slow division, gaining insight into how a deep model of the target's arithmetic capabilities allows the compiler to synthesize highly efficient code.",
            "id": "3674285",
            "problem": "A compiler back end is being designed for an unsigned fixed-width target machine to eliminate divisions by invariant constants. The target machine model is as follows: the machine has word size $w = 32$; unsigned integer arithmetic is modulo $2^{w}$; an unsigned multiplication of two $w$-bit operands produces the full $2w$-bit product; the instruction $\\mathrm{mulhi}(x,y)$ returns the upper $w$ bits of the $2w$-bit product of $x$ and $y$; and a logical right shift by $k$ bits computes $\\left\\lfloor x / 2^{k} \\right\\rfloor$ for any nonnegative integer $k$.\n\nYou are to replace the unsigned division by a positive invariant constant $d$ with a sequence that uses a single multiplication by a precomputed integer “magic number” followed by a right shift. Specifically, for all unsigned $n \\in \\{0,1,\\dots,2^{w}-1\\}$, compute\n$$\nq(n) \\;=\\; \\left\\lfloor \\frac{n \\cdot m}{2^{w+s}} \\right\\rfloor \\,,\n$$\nusing the machine’s $\\mathrm{mulhi}$ and shift so that $q(n) = \\left\\lfloor \\frac{n}{d} \\right\\rfloor$ holds for every $n$.\n\nWork from first principles of integer division and fixed-point approximation to derive a sufficient condition under which such integers $m$ and $s$ exist and yield exact quotients for all $n$. Then, for the specific divisor $d = 10$ and $w = 32$, determine the smallest nonnegative shift $s$ for which such an $m$ exists, compute the corresponding integer $m$, and justify that your choice satisfies the derived condition for all $n \\in [0,2^{w}-1]$.\n\nYour final answer must be the ordered pair $\\big(m,s\\big)$. No approximation is required, and no units are involved. Express the pair as a single row in the order $\\big(m,s\\big)$.",
            "solution": "The problem requires us to replace unsigned integer division by a constant divisor $d$ with a multiplication by a \"magic number\" $m$ and a right shift. Specifically, we want to find integers $m$ and $s$ such that for a given word size $w$ and for all unsigned integers $n$ in the range $0 \\le n  2^w$, the following equality holds:\n$$ \\left\\lfloor \\frac{n}{d} \\right\\rfloor = \\left\\lfloor \\frac{n \\cdot m}{2^{w+s}} \\right\\rfloor $$\nThe expression on the right is to be computed on a target machine where `mulhi(n, m)` calculates $\\lfloor (n \\cdot m) / 2^w \\rfloor$ and a right shift ` s` calculates $\\lfloor x / 2^s \\rfloor$. The expression $\\lfloor \\frac{n \\cdot m}{2^{w+s}} \\rfloor$ can be computed as `(mulhi(n, m))  s`, which is $\\lfloor \\lfloor (n \\cdot m) / 2^w \\rfloor / 2^s \\rfloor$, and is equivalent to the desired expression due to the property of integer floors $\\lfloor \\lfloor x/a \\rfloor / b \\rfloor = \\lfloor x/(ab) \\rfloor$.\n\nFirst, we derive a sufficient condition for $m$ and $s$ to exist.\nLet the quotient be $q = \\lfloor n/d \\rfloor$ and the remainder be $r_n = n \\pmod d$, such that $n = qd + r_n$ where $0 \\le r_n  d$.\nThe equality we want to hold is $q = \\lfloor nm/2^{w+s} \\rfloor$. This equality is equivalent to the pair of inequalities:\n$$ q \\le \\frac{n \\cdot m}{2^{w+s}}  q+1 $$\nLet's choose our approximation $m/2^{w+s}$ to be slightly larger than $1/d$. This ensures the left-hand inequality is satisfied:\n$$ \\frac{m}{2^{w+s}}  \\frac{1}{d} \\implies \\frac{nm}{2^{w+s}}  \\frac{n}{d} $$\nSince $n/d \\ge \\lfloor n/d \\rfloor = q$, we have $\\frac{nm}{2^{w+s}}  q$. As the floor function is monotonic, $\\lfloor \\frac{nm}{2^{w+s}} \\rfloor \\ge q$. So, if we can guarantee that the quotient is never $q+1$ or greater, we are done.\n\nThe critical part is the right-hand inequality:\n$$ \\frac{n \\cdot m}{2^{w+s}}  q+1 $$\nLet us define the error of our approximation $\\epsilon = \\frac{m}{2^{w+s}} - \\frac{1}{d}$, where we have chosen $\\epsilon  0$.\nSubstituting $m/2^{w+s}$ in the inequality gives:\n$$ n \\left( \\frac{1}{d} + \\epsilon \\right)  q+1 $$\n$$ \\frac{n}{d} + n\\epsilon  q+1 $$\nSubstituting $n=qd+r_n$:\n$$ \\frac{qd+r_n}{d} + n\\epsilon  q+1 $$\n$$ q + \\frac{r_n}{d} + n\\epsilon  q+1 $$\n$$ \\frac{r_n}{d} + n\\epsilon  1 $$\nThis inequality must hold for all $n \\in \\{0, 1, \\dots, 2^w - 1\\}$.\nThe term $n\\epsilon$ is maximized for the largest value of $n$, which is $2^w-1$. The term $r_n/d = (n \\pmod d)/d$ can be as large as $(d-1)/d$. The worst case for the inequality is when $n\\epsilon$ is large and $r_n/d$ is large.\nA strict requirement that covers all cases is:\n$$ n\\epsilon  1 - \\frac{n \\pmod d}{d} = \\frac{d - (n \\pmod d)}{d} $$\nThe left side is maximized when $n$ is maximal ($n=2^w-1$). The right side is minimized when $n \\pmod d$ is maximal ($n \\pmod d = d-1$). So, a sufficient condition to satisfy the inequality for all $n$ is to satisfy it for a combination of worst-case parameters:\n$$ (2^w-1)\\epsilon  \\frac{d-(d-1)}{d} = \\frac{1}{d} $$\nSubstituting back $\\epsilon = \\frac{m}{2^{w+s}} - \\frac{1}{d}$:\n$$ (2^w-1) \\left( \\frac{m}{2^{w+s}} - \\frac{1}{d} \\right)  \\frac{1}{d} $$\n$$ (2^w-1) \\left( \\frac{md - 2^{w+s}}{d \\cdot 2^{w+s}} \\right)  \\frac{1}{d} $$\n$$ (2^w-1)(md - 2^{w+s})  2^{w+s} $$\nWe now must choose $m$. A good choice for $m$ that satisfies $m/2^{w+s}  1/d$ is $m = \\lfloor 2^{w+s}/d \\rfloor + 1$.\nLet $r_s = 2^{w+s} \\pmod d$. Then $2^{w+s} = d \\cdot \\lfloor 2^{w+s}/d \\rfloor + r_s$.\nOur choice of $m$ becomes $m = \\frac{2^{w+s}-r_s}{d} + 1$.\nFrom this, we can express $md-2^{w+s}$:\n$$ md - 2^{w+s} = (2^{w+s} - r_s + d) - 2^{w+s} = d - r_s $$\nSubstituting this into our sufficient condition:\n$$ (2^w-1)(d - r_s)  2^{w+s} $$\nThis is a sufficient condition on $s$. We seek the smallest non-negative integer $s$ that satisfies it.\nA slightly stronger, but simpler, condition can be derived. Since $d-r_s  0$ (for $d$ not a power of $2$), we can write:\n$2^w(d-r_s) - (d-r_s)  2^{w+s}$.\nIf we satisfy $2^w(d-r_s) \\le 2^{w+s}$, the previous inequality will also hold, since $d-r_s \\ge 1$ (as $r_s \\le d-1$).\nThe simpler condition is $2^w(d-r_s) \\le 2^{w+s}$, which simplifies to:\n$$ d - (2^{w+s} \\pmod d) \\le 2^s $$\nThis is a sufficient condition to find a suitable shift $s$. If such an $s$ exists, $m$ is determined by $m = \\lfloor 2^{w+s}/d \\rfloor + 1$.\n\nNow, we apply this to the specific problem with $d=10$ and $w=32$. We need to find the smallest non-negative integer $s$ that satisfies:\n$$ 10 - (2^{32+s} \\pmod{10}) \\le 2^s $$\nLet's analyze the term $r_s = 2^{32+s} \\pmod{10}$. The powers of $2$ modulo $10$ cycle with a period of $4$: $(2, 4, 8, 6)$.\nFor an exponent $E \\ge 1$, $2^E \\pmod{10}$ depends on $E \\pmod 4$.\nThe exponent is $32+s$. Since $32$ is a multiple of $4$, $2^{32} \\pmod{10} = 6$.\nSo for $s=0, 1, 2, 3, \\dots$, the sequence $r_s = 2^{32+s} \\pmod{10}$ is $6, 2, 4, 8, 6, \\dots$.\nWe test values of $s$ beginning with $s=0$:\n\\begin{itemize}\n    \\item For $s=0$: $r_0 = 6$. The condition is $10 - 6 \\le 2^0 \\implies 4 \\le 1$, which is false.\n    \\item For $s=1$: $r_1 = 2$. The condition is $10 - 2 \\le 2^1 \\implies 8 \\le 2$, which is false.\n    \\item For $s=2$: $r_2 = 4$. The condition is $10 - 4 \\le 2^2 \\implies 6 \\le 4$, which is false.\n    \\item For $s=3$: $r_3 = 8$. The condition is $10 - 8 \\le 2^3 \\implies 2 \\le 8$, which is true.\n\\end{itemize}\nThe smallest non-negative integer shift is $s=3$.\n\nNow we compute the corresponding magic number $m$ for $s=3$:\n$$ m = \\left\\lfloor \\frac{2^{32+3}}{10} \\right\\rfloor + 1 = \\left\\lfloor \\frac{2^{35}}{10} \\right\\rfloor + 1 $$\nWe calculate $2^{35}$:\n$2^{10} = 1024$.\n$2^{32} = 2^2 \\cdot (2^{10})^3 = 4 \\cdot (1024)^3 = 4 \\cdot 1073741824 = 4294967296$.\n$2^{35} = 2^3 \\cdot 2^{32} = 8 \\cdot 4294967296 = 34359738368$.\nNow, we compute $m$:\n$$ m = \\left\\lfloor \\frac{34359738368}{10} \\right\\rfloor + 1 = \\lfloor 3435973836.8 \\rfloor + 1 = 3435973836 + 1 = 3435973837 $$\nThe value of $m$ must fit into a $w$-bit unsigned integer, i.e., $m  2^w = 2^{32}$.\n$m = 3435973837$.\n$2^{32} - 1 = 4294967295$.\nSince $3435973837  4294967295$, the magic number $m$ fits within the machine's word size.\n\nOur derived sufficient condition is satisfied for $s=3$. By construction, the pair $(m, s) = (3435973837, 3)$ guarantees that the computation $\\lfloor (n \\cdot m) / 2^{w+s} \\rfloor$ correctly yields $\\lfloor n/d \\rfloor$ for all unsigned $32$-bit integers $n$.\n\nThe final answer is the ordered pair $(m, s)$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 3435973837  3 \\end{pmatrix} } $$"
        }
    ]
}