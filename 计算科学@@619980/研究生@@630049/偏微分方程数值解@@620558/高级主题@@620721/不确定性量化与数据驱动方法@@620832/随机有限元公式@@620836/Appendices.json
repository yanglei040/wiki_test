{"hands_on_practices": [{"introduction": "广义多项式混沌 (gPC) 展开是随机有限元方法 (SFEM) 的基石，其有效性取决于选择一个合适的、有限的多项式基。本练习将通过计算各向同性（标准）和各向异性（为提高效率而定制）截断策略下的基函数数量，来训练您构建 gPC 近似时所需的基本技能。通过这个练习 [@problem_id:3448334]，您将掌握量化和比较不同截断策略计算成本的方法，这对于在实际应用中平衡精度和效率至关重要。", "problem": "考虑一个线性椭圆型偏微分方程，其随机系数通过由独立标准高斯随机变量驱动的截断 Karhunen–Loève 展开 (KLE) 进行建模。具体来说，假设系数场表示为 $a(x,\\omega) = \\bar{a}(x) + \\sum_{i=1}^{M} \\sqrt{\\lambda_{i}}\\, \\phi_{i}(x)\\, Y_{i}(\\omega)$，其中 $x$ 位于 $\\mathbb{R}^{d}$ 的一个有界域中，$(\\lambda_{i}, \\phi_{i})$ 是协方差算子的确定性特征对，而 $Y_{i}$ 是独立的标准正态随机变量。使用由多重指标 $\\alpha = (\\alpha_{1},\\dots,\\alpha_{M}) \\in \\mathbb{N}_{0}^{M}$ 索引的张量化概率论 Hermite 多项式，在 $M$ 维空间上构建一个 Hermite 广义多项式混沌 (gPC) 展开。\n\n将各向同性总次数截断集定义为\n$$\n\\mathcal{A}_{\\mathrm{iso}}(M,p) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\alpha_{i} \\leq p \\right\\},\n$$\n其中 $p \\in \\mathbb{N}_{0}$ 是最大总多项式次数。从 $\\mathcal{A}_{\\mathrm{iso}}(M,p)$ 的定义和基本计数原理出发，推导出 $\\mathcal{A}_{\\mathrm{iso}}(M,p)$ 基数作为 $M$ 和 $p$ 的函数的闭式表达式。\n\n接下来，考虑一个各向异性加权总次数截断，它通过权重 $\\gamma = (\\gamma_{1},\\dots,\\gamma_{M})$（其中 $\\gamma_{i} > 0$）来编码维度重要性。定义\n$$\n\\mathcal{A}_{\\mathrm{ani}}(\\gamma,p_{\\mathrm{w}}) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\gamma_{i}\\,\\alpha_{i} \\leq p_{\\mathrm{w}} \\right\\},\n$$\n其中 $p_{\\mathrm{w}} \\in \\mathbb{R}_{+}$。解释在 Hermite gPC 的背景下，权重 $\\gamma_{i}$ 对截断和收敛性的影响，并针对 $M=3$，$\\gamma = (1,2,1)$ 和 $p_{\\mathrm{w}} = 4$ 的具体情况，计算精确基数 $\\left|\\mathcal{A}_{\\mathrm{ani}}(\\gamma,p_{\\mathrm{w}})\\right|$。将各向同性情况的闭式表达式和计算出的各向异性情况的整数作为最终结果。无需四舍五入，也不涉及物理单位。", "solution": "该问题分为两部分。第一部分要求推导各向同性总次数多项式指标集基数的闭式表达式。第二部分要求解释各向异性截断并计算一个特定各向异性指标集的基数。\n\n首先，我们处理各向同性总次数截断集基数的推导，其定义为\n$$\n\\mathcal{A}_{\\mathrm{iso}}(M,p) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\alpha_{i} \\leq p \\right\\}。\n$$\n这里，$\\alpha = (\\alpha_1, \\dots, \\alpha_M)$ 是一个多重指标，其中每个分量 $\\alpha_i$ 都是非负整数，$M$ 是维度数，$p$ 是最大总多项式次数。\n\n为了求出该集合中的元素数量，记为 $|\\mathcal{A}_{\\mathrm{iso}}(M,p)|$，我们必须计算不等式 $\\alpha_1 + \\alpha_2 + \\dots + \\alpha_M \\leq p$ 的非负整数解的数量。通过引入一个松弛变量 $\\alpha_{M+1} \\in \\mathbb{N}_{0}$，可以将此问题转化为一个等式问题。该不等式等价于方程\n$$\n\\alpha_1 + \\alpha_2 + \\dots + \\alpha_M + \\alpha_{M+1} = p。\n$$\n我们现在需要求出这个关于 $M+1$ 个变量 $(\\alpha_1, \\dots, \\alpha_{M+1})$ 的方程的非负整数解的数量。这是一个经典的组合问题，可以用“隔板法”解决。我们正在将 $p$ 个相同的物品（星星）分配到 $M+1$ 个不同的箱子（变量 $\\alpha_i$）中。这需要 $M$ 个隔板来分隔这 $M+1$ 个箱子。$p$ 个星星和 $M$ 个隔板的总排列数，是从总共 $p+M$ 个位置中选择 $M$ 个隔板位置的方法数。这由二项式系数给出：\n$$\n|\\mathcal{A}_{\\mathrm{iso}}(M,p)| = \\binom{p+M}{M} = \\frac{(p+M)!}{p!M!}。\n$$\n\n接下来，我们讨论各向异性加权总次数截断。该集合定义为\n$$\n\\mathcal{A}_{\\mathrm{ani}}(\\gamma,p_{\\mathrm{w}}) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\gamma_{i}\\,\\alpha_{i} \\leq p_{\\mathrm{w}} \\right\\},\n$$\n其中权重 $\\gamma_i > 0$。引入权重 $\\gamma_i$ 是为了创建一种更高效的截断策略。在 Karhunen–Loève 展开的背景下，随机变量 $Y_i$ 对随机场 $a(x, \\omega)$ 方差的贡献并不相等；$Y_i$ 的贡献由 $\\sqrt{\\lambda_i}$ 进行缩放。协方差算子的特征值 $\\lambda_i$ 通常会衰减，这意味着某些随机维度比其他维度更重要。各向异性截断利用了这一点，通过为不同维度的多项式次数分配不同的“成本”（即权重 $\\gamma_i$）。通常，较大的权重 $\\gamma_i$ 会分配给较不重要的维度（即对应于较小特征值 $\\lambda_i$ 的维度）。对于给定的预算 $p_{\\mathrm{w}}$，这会惩罚较不重要方向上的高阶多项式项 $\\alpha_i$，迫使其变小或为零，同时允许在较重要方向上（即权重较小的方向）使用高阶项。这种定制的计算量分配通常能在给定数量的基函数（即指标集的基数）下获得更精确的近似，从而改善广义多项式混沌展开的收敛性，尤其是在高维问题中。\n\n现在，我们计算 $M=3$，权重 $\\gamma = (1,2,1)$ 和最大加权次数 $p_{\\mathrm{w}} = 4$ 时的具体基数。我们需要求出不等式\n$$\n1 \\cdot \\alpha_1 + 2 \\cdot \\alpha_2 + 1 \\cdot \\alpha_3 \\leq 4\n$$\n的非负整数解 $(\\alpha_1, \\alpha_2, \\alpha_3)$ 的数量。由于没有通用的闭式公式来计算这个数量，我们通过遍历 $\\alpha_2$ 的可能值来进行直接枚举，因为它具有最大的权重。各项的非负性意味着 $2\\alpha_2 \\leq 4$，因此 $\\alpha_2$ 可以在 $\\{0, 1, 2\\}$ 中取值。\n\n情况 1：$\\alpha_2 = 0$。\n不等式简化为 $\\alpha_1 + \\alpha_3 \\leq 4$。这是一个最大次数为 $4$ 的二维各向同性问题。解的数量由之前推导的公式给出：$\\binom{4+2}{2} = \\binom{6}{2} = \\frac{6 \\times 5}{2} = 15$。\n\n情况 2：$\\alpha_2 = 1$。\n不等式变为 $\\alpha_1 + 2(1) + \\alpha_3 \\leq 4$，简化为 $\\alpha_1 + \\alpha_3 \\leq 2$。解的数量为 $\\binom{2+2}{2} = \\binom{4}{2} = \\frac{4 \\times 3}{2} = 6$。\n\n情况 3：$\\alpha_2 = 2$。\n不等式变为 $\\alpha_1 + 2(2) + \\alpha_3 \\leq 4$，简化为 $\\alpha_1 + \\alpha_3 \\leq 0$。由于 $\\alpha_1, \\alpha_3$ 必须为非负数，唯一的解是 $(\\alpha_1, \\alpha_3) = (0,0)$。这给出了 1 个解。该公式也得出 $\\binom{0+2}{2} = \\binom{2}{2} = 1$。\n\n总基数 $|\\mathcal{A}_{\\mathrm{ani}}((1,2,1),4)|$ 是所有情况计数之和：\n$$\n|\\mathcal{A}_{\\mathrm{ani}}| = 15 + 6 + 1 = 22。\n$$\n\n最终答案包含两部分：各向同性情况的闭式表达式和特定各向异性情况的数值。", "answer": "$$\n\\boxed{\\binom{p+M}{M}, 22}\n$$", "id": "3448334"}, {"introduction": "在建立了随机问题的谱表示之后，一个核心任务是计算感兴趣量 (QoI) 的统计矩，例如均值。这通常需要在随机空间中进行数值积分（求积）。本练习 [@problem_id:3448302] 突出了选择与不确定输入的概率分布相匹配的求积法则的极端重要性，并清晰地展示了不匹配的求积法则（例如，对非高斯输入使用高斯-埃尔米特求积）如何导致灾难性的计算失败。", "problem": "考虑单位区间上具有随机材料系数的一维稳态扩散问题，\n$$\n-\\frac{d}{dx}\\!\\left(A(\\xi)\\,\\frac{du}{dx}(x,\\xi)\\right)=1,\\quad x\\in(0,1),\\qquad u(0,\\xi)=0,\\;u(1,\\xi)=0,\n$$\n其中 $A(\\xi)$ 是一个表示不确定扩散率的正随机变量。假设对于每次实现，$A(\\xi)$ 在空间上是恒定的，并且我们感兴趣的量是解的平均值，\n$$\nQ(u(\\cdot,\\xi))=\\int_{0}^{1}u(x,\\xi)\\,dx.\n$$\n在随机有限元方法 (SFEM) 的背景下，目标是计算关于 $A(\\xi)$ 分布的期望 $\\mathbb{E}[Q(u)]$。设随机系数相对于高斯分布是重尾的，即其尾部衰减比高斯分布慢，具体由形状参数 $k1$ 和单位尺度的伽马分布建模：\n$$\nA \\sim \\mathrm{Gamma}(k,1),\\quad p_A(a)=\\frac{a^{k-1}\\,e^{-a}}{\\Gamma(k)},\\quad a\\in(0,\\infty).\n$$\n\n从确定性偏微分方程的强形式和弱形式以及期望的标准定义出发，推导 $Q(u)$ 与随机系数 $A$ 之间的关系。在此基础上，构造三种对 $\\mathbb{E}[Q(u)]$ 的数值求积近似：\n- 在正半轴上误用高斯-埃尔米特求积（“朴素高斯-埃尔米特法”）：将 $n$ 点高斯-埃尔米特节点 $z_i$ 和权重 $w_i^{H}$ 限制在 $z_i0$ 的部分，将受限的权重重新归一化使其和为 $1$，并通过 $\\sum w_i^{H,\\text{renorm}}\\,Q(u(\\cdot;A=z_i))$ 来近似 $\\mathbb{E}[Q(u)]$。此方法故意忽略了高斯权重与 $(0,\\infty)$ 上伽马定律之间的不匹配，以便诊断其失效原因。\n- 广义高斯-拉盖尔求积：使用参数为 $\\alpha=k-1$ 的 $n$ 点广义高斯-拉盖尔节点 $x_i$ 和权重 $w_i^{L}$ 来近似\n$$\n\\mathbb{E}[Q(u)]\\approx \\frac{1}{\\Gamma(k)}\\sum_{i=1}^{n} w_i^{L}\\,Q(u(\\cdot;A=x_i)).\n$$\n- 基于高斯-埃尔米特的变换采样求积：通过标准正态累积分布函数和逆伽马累积分布函数的复合函数 $T(z)=F_A^{-1}(\\Phi(z))$，将标准正态高斯-埃尔米特节点 $z_i$ 和权重 $w_i^{H}$ 进行前推，权重通过 $w_i^{H}/\\sqrt{\\pi}$ 针对标准正态期望进行缩放，并近似为\n$$\n\\mathbb{E}[Q(u)]\\approx \\sum_{i=1}^{n} \\frac{w_i^{H}}{\\sqrt{\\pi}}\\,Q(u(\\cdot;A=T(z_i))).\n$$\n\n实现一个单一的程序，该程序：\n1. 从第一性原理推导 $Q(u)$ 作为 $A$ 的函数。\n2. 对一组测试用例 $(k,n)$，使用上述三种求积策略来近似 $\\mathbb{E}[Q(u)]$。\n3. 通过基于伽马分布的期望解析评估，计算 $\\mathbb{E}[Q(u)]$ 的基准真实值，并比较每种求积的相对误差。\n\n为保证数值稳定性和科学一致性，假设所有量都是无单位的。程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个条目对应一个测试用例，并且本身是一个包含三个浮点数的列表 $[e_{\\text{GH-naive}},e_{\\text{GL}},e_{\\text{Transform}}]$，分别给出朴素高斯-埃尔米特法、高斯-拉盖尔法和变换采样求积法的相对误差。\n\n使用以下测试套件：\n- $(k,n)=(1.20,8)$: 在零附近有重质量，在 $(0,\\infty)$ 上有相对较重的尾部；中等求积阶数。\n- $(k,n)=(2.50,8)$: 更集中的分布；相同的求积阶数。\n- $(k,n)=(5.00,8)$: 良好集中的分布；相同的求积阶数。\n- $(k,n)=(1.05,12)$: 接近可积性边界 $k\\to 1^{+}$；更高的求积阶数。\n- $(k,n)=(3.00,16)$: 中等形状；更高的求积阶数。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如，“[[r11,r12,r13],[r21,r22,r23],...]”），在此单一括号结构之外不得有任何额外的文本或空格。所有数值答案必须是浮点数。此计算不涉及任何物理单位。", "solution": "用户提供了一个偏微分方程不确定性量化领域的问题。任务是使用三种不同的数值求积技术，计算一个随机椭圆边值问题解的泛函的期望，并将其准确性与已知的解析解进行比较。\n\n### 问题验证\n\n**第1步：提取已知条件**\n\n- **控制方程：** $-\\frac{d}{dx}\\!\\left(A(\\xi)\\,\\frac{du}{dx}(x,\\xi)\\right)=1$ 对于 $x\\in(0,1)$。\n- **边界条件：** $u(0,\\xi)=0$, $u(1,\\xi)=0$。\n- **随机系数：** $A(\\xi)$ 是一个空间上恒定的随机变量。\n- **A的分布：** $A \\sim \\mathrm{Gamma}(k,1)$，概率密度函数为 $p_A(a)=\\frac{a^{k-1}\\,e^{-a}}{\\Gamma(k)}$，对于 $a\\in(0,\\infty)$ 且形状参数 $k1$。\n- **感兴趣的量 (QoI)：** $Q(u(\\cdot,\\xi))=\\int_{0}^{1}u(x,\\xi)\\,dx$。\n- **目标：** 计算 $\\mathbb{E}[Q(u)]$。\n- **数值方法：**\n    1.  **朴素高斯-埃尔米特法：** 使用 $n$ 点高斯-埃尔米特节点 $z_i$ 和权重 $w_i^H$。限制在 $z_i0$。将权重重新归一化使其和为 $1$。近似 $\\mathbb{E}[Q(u)] \\approx \\sum w_i^{H,\\text{renorm}}\\,Q(u(\\cdot;A=z_i))$。\n    2.  **广义高斯-拉盖尔法：** 使用参数为 $\\alpha=k-1$ 的 $n$ 点广义高斯-拉盖尔求积。近似由 $\\mathbb{E}[Q(u)]\\approx \\frac{1}{\\Gamma(k)}\\sum_{i=1}^{n} w_i^{L}\\,Q(u(\\cdot;A=x_i))$ 给出。\n    3.  **变换采样（高斯-埃尔米特法）：** 使用 $n$ 点标准高斯-埃尔米特节点 $z_i$ 和权重 $w_i^H$。定义一个变换 $T(z)=F_A^{-1}(\\Phi(z))$，其中 $F_A$ 是伽马累积分布函数 (CDF)，$\\Phi$ 是标准正态累积分布函数 (CDF)。近似为 $\\mathbb{E}[Q(u)]\\approx \\sum_{i=1}^{n} \\frac{w_i^{H}}{\\sqrt{\\pi}}\\,Q(u(\\cdot;A=T(z_i)))$。\n- **测试用例：** $(k,n) \\in \\{(1.20, 8), (2.50, 8), (5.00, 8), (1.05, 12), (3.00, 16)\\}$。\n- **输出：** 每个测试用例的相对误差列表 $[e_{\\text{GH-naive}},e_{\\text{GL}},e_{\\text{Transform}}]$。\n\n**第2步：使用提取的已知条件进行验证**\n\n- **科学依据：** 该问题是一个典型的一维随机边值问题，是不确定性量化领域的标准基准测试。所选的分布（伽马分布）适用于正值随机变量。所描述的数值方法（高斯求积）是数值积分中的基本技术。整个设置在科学上和数学上都是合理的。\n- **适定性：** 对于随机变量 $A(\\xi)=a0$ 的任意给定实现，该微分方程是一个具有狄利克雷边界条件的标准二阶线性常微分方程。存在唯一解。感兴趣的量 $Q(u)$ 及其期望 $\\mathbb{E}[Q(u)]$ 是良定义的数学对象。条件 $k1$ 确保了期望是有限的，如下文所示。该问题是适定的。\n- **客观性：** 该问题以精确的数学语言表述，没有模糊性或主观性陈述。实现三种求积方案的说明是具体的。\n\n**第3步：结论与行动**\n\n该问题是**有效的**。它自成体系、有科学依据且适定。我们可以继续进行求解。\n\n### 推导与求解\n\n**1. $u(x,A)$ 和 $Q(u)$ 的解析解**\n\n对于随机系数的一个固定实现 $A(\\xi)=A$，问题是一个确定性边值问题 (BVP)：\n$$\n-A\\,\\frac{d^2u}{dx^2}=1, \\quad x\\in(0,1)\n$$\n边界条件为 $u(0)=0$ 和 $u(1)=0$。对该方程关于 $x$ 积分两次，得到：\n$$\n\\frac{d^2u}{dx^2} = -\\frac{1}{A}\n$$\n$$\n\\frac{du}{dx} = -\\frac{x}{A} + C_1\n$$\n$$\nu(x) = -\\frac{x^2}{2A} + C_1 x + C_2\n$$\n边界条件 $u(0)=0$ 意味着 $C_2=0$。条件 $u(1)=0$ 意味着：\n$$\n-\\frac{1}{2A} + C_1 = 0 \\implies C_1 = \\frac{1}{2A}\n$$\n因此，对于给定的 $A$，该边值问题的解是：\n$$\nu(x;A) = \\frac{1}{2A}(x-x^2)\n$$\n感兴趣的量 $Q(u)$ 是该解在定义域上的积分：\n$$\nQ(u) = \\int_0^1 u(x;A)\\,dx = \\int_0^1 \\frac{1}{2A}(x-x^2)\\,dx = \\frac{1}{2A}\\left[\\frac{x^2}{2} - \\frac{x^3}{3}\\right]_0^1\n$$\n$$\nQ(u) = \\frac{1}{2A}\\left(\\frac{1}{2} - \\frac{1}{3}\\right) = \\frac{1}{2A}\\left(\\frac{1}{6}\\right) = \\frac{1}{12A}\n$$\n所以，感兴趣的量是随机变量 $A$ 的一个简单函数，$Q(u) = (12A)^{-1}$。\n\n**2. 解析期望（基准真值）**\n\n我们需要计算 $Q(u)$ 关于 $A$ 的分布的期望。\n$$\n\\mathbb{E}[Q(u)] = \\mathbb{E}\\left[\\frac{1}{12A}\\right] = \\frac{1}{12}\\mathbb{E}[A^{-1}]\n$$\n随机变量 $A$ 服从伽马分布，$A \\sim \\mathrm{Gamma}(k, \\theta=1)$，其概率密度函数为 $p_A(a) = \\frac{a^{k-1}e^{-a}}{\\Gamma(k)}$。$A^m$ 的期望由下式给出：\n$$\n\\mathbb{E}[A^m] = \\int_0^\\infty a^m p_A(a)\\,da = \\int_0^\\infty a^m \\frac{a^{k-1}e^{-a}}{\\Gamma(k)}\\,da = \\frac{1}{\\Gamma(k)}\\int_0^\\infty a^{k+m-1}e^{-a}\\,da\n$$\n根据伽马函数的定义，$\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t}\\,dt$，该积分等于 $\\Gamma(k+m)$。因此：\n$$\n\\mathbb{E}[A^m] = \\frac{\\Gamma(k+m)}{\\Gamma(k)}\\theta^m\n$$\n对于我们的情况，我们需要 $\\mathbb{E}[A^{-1}]$，所以我们设 $m=-1$ 且尺度 $\\theta=1$：\n$$\n\\mathbb{E}[A^{-1}] = \\frac{\\Gamma(k-1)}{\\Gamma(k)}\n$$\n使用恒等式 $\\Gamma(k) = (k-1)\\Gamma(k-1)$（对于 $k1$ 成立），我们得到：\n$$\n\\mathbb{E}[A^{-1}] = \\frac{\\Gamma(k-1)}{(k-1)\\Gamma(k-1)} = \\frac{1}{k-1}\n$$\n此结果对于 $k-10$ 或 $k1$ 有效，这在问题陈述中已给出。解析期望，即我们的基准真值，是：\n$$\n\\mathbb{E}[Q(u)] = \\frac{1}{12(k-1)}\n$$\n\n**3. 数值求积方案**\n\n我们的任务是实现三种求积方案来近似 $\\mathbb{E}[Q(u)] = \\int_0^\\infty Q(u;a) p_A(a) da$。\n\n**a. 朴素高斯-埃尔米特求积：** 这个方法是故意误用的。它使用标准高斯-埃尔米特求积（权重函数为 $e^{-z^2}$）的节点 $z_i$ 和权重 $w_i^H$。它将节点截断到正半轴（$z_i0$），将相应的权重 $w_i^{H, \\text{pos}}$ 重新归一化使其和为1（$w_i^{H, \\text{renorm}} = w_i^{H, \\text{pos}} / \\sum_j w_j^{H, \\text{pos}}$），并形成一个离散近似：\n$$\n\\mathbb{E}[Q(u)] \\approx \\sum_{i, z_i0} w_i^{H, \\text{renorm}} Q(u; z_i) = \\sum_{i, z_i0} w_i^{H, \\text{renorm}} \\frac{1}{12 z_i}\n$$\n该方法完全忽略了伽马分布的密度函数，仅将节点 $z_i$ 用作采样点。预计其精度会非常低。\n\n**b. 广义高斯-拉盖尔求积：** 这种求积方法是为形如 $\\int_0^\\infty f(x) x^\\alpha e^{-x} dx$ 的积分设计的。我们的期望积分可以写成：\n$$\n\\mathbb{E}[Q(u)] = \\int_0^\\infty Q(u;a) \\frac{a^{k-1}e^{-a}}{\\Gamma(k)} da = \\frac{1}{\\Gamma(k)} \\int_0^\\infty Q(u;a) a^{k-1}e^{-a} da\n$$\n问题指定使用参数 $\\alpha = k-1$ 的广义高斯-拉盖尔求积，这对应于权重函数 $w(x) = x^{k-1}e^{-x}$。一个具有节点 $x_i$ 和权重 $w_i^L$ 的 $n$ 点求积规则将该积分近似为 $\\sum_{i=1}^n w_i^L f(x_i)$。这里，被积函数是 $f(a) = Q(u;a) = \\frac{1}{12a}$。因此，完整的近似为：\n$$\n\\mathbb{E}[Q(u)] \\approx \\frac{1}{\\Gamma(k)} \\sum_{i=1}^n w_i^L \\frac{1}{12x_i}\n$$\n该方法非常适合本问题，但其精度取决于多项式在测度支撑集上逼近函数 $1/x$ 的效果，当 $k$ 接近 $1$ 时，这可能具有挑战性。\n\n**c. 变换采样求积：** 此方法使用变量替换，将关于伽马分布的积分转换为关于标准正态变量的积分，然后通过高斯-埃尔米特求积进行近似。令 $Z \\sim N(0,1)$，$A \\sim \\mathrm{Gamma}(k,1)$。变换为 $A = F_A^{-1}(\\Phi(Z))$，其中 $F_A$ 和 $\\Phi$ 分别是伽马分布和标准正态分布的累积分布函数 (CDF)。\n期望为 $\\mathbb{E}[Q(u)] = \\mathbb{E}[Q(u; F_A^{-1}(\\Phi(Z)))]$。积分为：\n$$\n\\mathbb{E}[Q(u)] = \\int_{-\\infty}^{\\infty} Q(u; F_A^{-1}(\\Phi(z))) \\frac{e^{-z^2/2}}{\\sqrt{2\\pi}} dz\n$$\n为了使用标准高斯-埃尔米特求积（权重为 $e^{-y^2}$），我们进行变量替换 $z = \\sqrt{2}y$：\n$$\n\\mathbb{E}[Q(u)] = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} Q(u; F_A^{-1}(\\Phi(\\sqrt{2}y))) e^{-y^2} dy\n$$\n这将被近似为 $\\frac{1}{\\sqrt{\\pi}} \\sum_i w_i^H Q(u; F_A^{-1}(\\Phi(\\sqrt{2}z_i)))$，其中 $(z_i, w_i^H)$ 是标准高斯-埃尔米特节点/权重。然而，问题指定了一个略有不同、存在细微缺陷的公式：\n$$\n\\mathbb{E}[Q(u)] \\approx \\sum_{i=1}^{n} \\frac{w_i^{H}}{\\sqrt{\\pi}}\\,Q(u(\\cdot;A=T(z_i))) \\quad \\text{with } T(z) = F_A^{-1}(\\Phi(z))\n$$\n这对应于近似 $Q(u; F_A^{-1}(\\Phi(Y)))$ 的期望，其中 $Y \\sim N(0, 1/2)$，而非 $Y \\sim N(0,1)$。这种方差上的不匹配使得该方法成为一种近似，我们按规定实现它。\n实现部分将计算这三种近似值，并计算它们相对于每个测试用例的解析基准真值的相对误差。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import roots_hermite, roots_genlaguerre, gamma, gammaincinv, erf\n\ndef solve():\n    \"\"\"\n    Solves the stochastic diffusion problem by comparing three quadrature methods.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.20, 8),\n        (2.50, 8),\n        (5.00, 8),\n        (1.05, 12),\n        (3.00, 16),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        k, n = case\n        \n        # --- Ground Truth ---\n        # The analytical expectation is E[Q(u)] = 1 / (12 * (k - 1)).\n        # This is valid for k > 1.\n        ground_truth = 1.0 / (12.0 * (k - 1.0))\n        \n        # Define the Quantity of Interest (QoI) as a function of the parameter A.\n        # Q(u; A) = 1 / (12 * A)\n        def qoi_func(a):\n            # Avoid division by zero for nodes that could be at a=0,\n            # though the quadrature nodes used here will be positive.\n            if np.any(a = 0):\n                # Replace non-positive values with a small epsilon or handle differently\n                # For this problem, all nodes will be > 0.\n                pass\n            return 1.0 / (12.0 * a)\n\n        # --- Method 1: Naive Gauss-Hermite ---\n        z_h, w_h = roots_hermite(n)\n        \n        # Restrict to positive nodes and their weights\n        pos_indices = z_h > 0\n        z_h_pos = z_h[pos_indices]\n        w_h_pos = w_h[pos_indices]\n        \n        # Renormalize weights to sum to 1\n        w_h_renorm = w_h_pos / np.sum(w_h_pos)\n        \n        approx_gh_naive = np.sum(w_h_renorm * qoi_func(z_h_pos))\n        error_gh_naive = np.abs(approx_gh_naive - ground_truth) / np.abs(ground_truth)\n        \n        # --- Method 2: Generalized Gauss-Laguerre ---\n        # The integral is (1/Gamma(k)) * integral(Q(u;a) * a^(k-1) * exp(-a) da).\n        # We use GGL quadrature with alpha = k-1. The function to integrate is Q(u;a).\n        alpha = k - 1.0\n        x_l, w_l = roots_genlaguerre(n, alpha)\n        \n        gamma_k = gamma(k)\n        \n        approx_gl = (1.0 / gamma_k) * np.sum(w_l * qoi_func(x_l))\n        error_gl = np.abs(approx_gl - ground_truth) / np.abs(ground_truth)\n        \n        # --- Method 3: Transformed Sampling (Gauss-Hermite) ---\n        # Get standard Gauss-Hermite nodes and weights (for weight exp(-z^2))\n        z_t, w_t = roots_hermite(n)\n        \n        # Standard normal CDF: Phi(z) = 0.5 * (1 + erf(z/sqrt(2)))\n        def std_normal_cdf(z):\n            return 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n            \n        # Inverse Gamma CDF with shape k and unit scale.\n        def inv_gamma_cdf(p, shape):\n            return gammaincinv(shape, p)\n\n        # Transformation T(z) = F_A^-1(Phi(z))\n        def transform(z, shape):\n            return inv_gamma_cdf(std_normal_cdf(z), shape)\n            \n        # Apply the transformation to the nodes\n        a_samples = transform(z_t, k)\n        \n        # Scale weights for expectation w.r.t N(0, 1/2)\n        # Note: Sum of raw weights w_t is sqrt(pi)\n        weights_t = w_t / np.sqrt(np.pi)\n        \n        approx_transform = np.sum(weights_t * qoi_func(a_samples))\n        error_transform = np.abs(approx_transform - ground_truth) / np.abs(ground_truth)\n        \n        # Store the relative errors for the current test case\n        results.append([error_gh_naive, error_gl, error_transform])\n\n    # Format the output string\n    # e.g., \"[[err11,err12,err13],[err21,err22,err23]]\"\n    output_str = \"[\" + \",\".join([f\"[{e1},{e2},{e3}]\" for e1, e2, e3 in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3448302"}, {"introduction": "虽然像多项式混沌展开这样的代理模型非常强大，但在复杂问题中，高保真度模拟对于验证和获得精确结果仍然是不可或缺的。本练习 [@problem_id:3448294] 探讨了控制变量法，这是一种强大的方差缩减技术，它巧妙地将快速的低保真度代理模型（如低阶 PCE 模型）与少量昂贵的高保真度模拟相结合。通过推导最优混合系数和方差缩减因子，您将深入理解如何显著加速蒙特卡洛方法的收敛速度，这在实际工程应用中是一项至关重要的技能。", "problem": "考虑一个具有随机输入的线性椭圆偏微分方程（PDE），该方程通过使用侵入式广义多项式混沌（gPC）近似的随机有限元方法（SFEM）进行离散化。令目标量为解的一个实值泛函 $Q$，因此高保真模型的输出为 $Y = Q(u)$，并令 $X = Q(u_{p})$ 表示同一数量的低阶侵入式多项式混沌展开（PCE）代理模型，其中 $u_{p}$ 是总阶为 $p$ 的 gPC 解。假设侵入式 gPC 系数已通过 Galerkin 投影计算得出，因此 $X$ 的均值，记为 $\\mu_{X} = \\mathbb{E}[X]$，可以精确地作为零次基函数的系数获得。您需要通过控制变量构造，将低阶侵入式 PCE 与 $Y$ 的高保真蒙特卡洛（MC）采样相结合。\n\n从期望、方差、协方差和相关的基本定义出发，并仅使用多项式基的正交性和期望的线性性质，按以下步骤进行：\n\n1. 构造一个用于 $\\mu_{Y} = \\mathbb{E}[Y]$ 的无偏线性估计量，该估计量通过形成一个控制变量估计量，将 $n$ 个来自相同输入分布的对 $(Y,X)$ 的独立同分布样本 $\\{(Y_{i}, X_{i})\\}_{i=1}^{n}$ 与已知的均值 $\\mu_{X} = \\mathbb{E}[X]$ 相结合，线性地混合高保真样本 $Y$ 和来自 $X$ 的低阶 PCE 信息。\n\n2. 以闭合形式推导在无偏性约束下最小化估计量方差的最优混合系数。用协方差 $\\operatorname{Cov}(Y,X)$ 和方差 $\\sigma_{Y}^{2} = \\operatorname{Var}(Y)$ 及 $\\sigma_{X}^{2} = \\operatorname{Var}(X)$ 表示最优系数。\n\n3. 将最优估计量的最小化方差表示为仅使用 $Y$ 样本的普通蒙特卡洛估计量方差的一个乘法因子。然后将此方差缩减因子表示为仅含相关系数 $\\rho = \\operatorname{Corr}(Y,X)$ 的函数。\n\n4. 在将侵入式 PCE 用作高保真量 $Y = Q(u)$ 的控制变量的背景下解释结果，评论 $\\sigma_{Y}$、$\\sigma_{X}$ 和 $\\rho$ 的作用。\n\n您的程序必须实现最终的闭合形式表达式，以便为给定的参数值测试套件生成数值结果。程序不应执行任何采样；它必须计算上述推导出的闭合形式表达式。\n\n输入数据和输出是无量纲的；不涉及物理单位或角度单位。输出必须是四舍五入到 $6$ 位小数的浮点数。\n\n测试套件：\n- 情况 A（正常路径）：$\\rho = 0.0$, $\\sigma_{Y} = 1.0$, $\\sigma_{X} = 1.0$。\n- 情况 B（中等正相关）：$\\rho = 0.5$, $\\sigma_{Y} = 2.0$, $\\sigma_{X} = 1.0$。\n- 情况 C（中等负相关）：$\\rho = -0.7$, $\\sigma_{Y} = 1.5$, $\\sigma_{X} = 3.0$。\n- 情况 D（近乎完美的正相关边界）：$\\rho = 0.99$, $\\sigma_{Y} = 1.0$, $\\sigma_{X} = 0.8$。\n- 情况 E（近乎完美的负相关边界）：$\\rho = -0.99$, $\\sigma_{Y} = 0.5$, $\\sigma_{X} = 4.0$。\n\n对于每种情况，您的程序必须计算：\n- 出现在结合 $Y$、$X$ 和 $\\mu_{X}$ 的无偏线性估计量中的最优控制变量混合系数 $\\beta^{\\star}$。\n- 方差缩减因子 $\\phi$，定义为使用相同样本量 $n$ 的控制变量估计量的最小化方差与仅基于 $Y$ 样本的普通蒙特卡洛估计量方差之比。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。该列表必须按 A、B、C、D、E 的顺序包含各种情况的配对，平铺成一个列表，格式为 $[\\beta^{\\star}_{A}, \\phi_{A}, \\beta^{\\star}_{B}, \\phi_{B}, \\beta^{\\star}_{C}, \\phi_{C}, \\beta^{\\star}_{D}, \\phi_{D}, \\beta^{\\star}_{E}, \\phi_{E}]$，其中每个浮点数都四舍五入到 $6$ 位小数。", "solution": "问题陈述是一个定义明确的数理统计理论练习，具体涉及蒙特卡洛估计中用于方差缩减的控制变量方法。所有提供的信息都是自洽的，并且在概率论和数值分析方面具有科学依据。随机有限元方法（SFEM）和广义多项式混沌（gPC）展开的背景是该技术的标准且合适的应用领域。因此，该问题被认为是有效的，可以构建一个形式化的解。\n\n推导过程按要求分为四个部分，从基本定义开始。\n\n### 第 1 部分：构造无偏控制变量估计量\n\n令 $\\mu_Y = \\mathbb{E}[Y]$ 是我们感兴趣的高保真量的未知均值 $Y$。我们有 $n$ 个独立同分布（i.i.d.）的样本 $\\{Y_i\\}_{i=1}^n$。$\\mu_Y$ 的标准蒙特卡洛估计量是样本均值：\n$$\n\\hat{\\mu}_{Y,MC} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n$$\n这个估计量是无偏的，因为 $\\mathbb{E}[\\hat{\\mu}_{Y,MC}] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[Y_i] = \\frac{1}{n} (n \\mu_Y) = \\mu_Y$。\n\n我们还有一个相关的随机变量 $X$，即低阶代理模型，其均值 $\\mu_X = \\mathbb{E}[X]$ 是精确已知的。这使我们能够构造一个新的随机变量 $Z = X - \\mu_X$，其均值已知为零：$\\mathbb{E}[Z] = \\mathbb{E}[X - \\mu_X] = \\mathbb{E}[X] - \\mu_X = \\mu_X - \\mu_X = 0$。\n\n$\\mu_Y$ 的控制变量估计量是通过定义一个新的随机变量 $Y(\\beta)$ 来形成的，该变量将 $Y$ 与一个缩放版本的 $Z$ 结合起来：\n$$\nY(\\beta) = Y - \\beta (X - \\mu_X)\n$$\n其中 $\\beta$ 是一个待定的混合系数。新的估计量 $\\hat{\\mu}_{Y,CV}$ 是 $Y_i(\\beta)$ 在 $n$ 个配对样本 $\\{(Y_i, X_i)\\}_{i=1}^n$ 上的样本均值：\n$$\n\\hat{\\mu}_{Y,CV}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} Y_i(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} [Y_i - \\beta(X_i - \\mu_X)]\n$$\n为了验证这个估计量对于任何 $\\beta$ 的选择都是无偏的，我们取其期望。根据期望的线性性质：\n$$\n\\mathbb{E}[\\hat{\\mu}_{Y,CV}(\\beta)] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} [Y_i - \\beta(X_i - \\mu_X)]\\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[Y_i - \\beta(X_i - \\mu_X)]\n$$\n由于每对 $(Y_i, X_i)$ 的分布都与 $(Y, X)$ 相同，\n$$\n\\mathbb{E}[\\hat{\\mu}_{Y,CV}(\\beta)] = \\mathbb{E}[Y - \\beta(X - \\mu_X)] = \\mathbb{E}[Y] - \\beta \\mathbb{E}[X - \\mu_X] = \\mu_Y - \\beta(0) = \\mu_Y\n$$\n因此，对于任何常数 $\\beta$，$\\hat{\\mu}_{Y,CV}(\\beta)$ 都是 $\\mu_Y$ 的一个无偏线性估计量。\n\n### 第 2 部分：最优混合系数的推导\n\n最优系数 $\\beta^{\\star}$ 是使控制变量估计量的方差 $\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta))$ 最小化的系数。该估计量的方差由下式给出：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} Y_i(\\beta)\\right)\n$$\n由于样本是独立同分布的，和的方差是单项方差的 $n$ 倍，而因子 $\\frac{1}{n}$ 变为 $\\frac{1}{n^2}$：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\frac{1}{n^2} \\cdot n \\cdot \\operatorname{Var}(Y(\\beta)) = \\frac{1}{n} \\operatorname{Var}(Y - \\beta(X - \\mu_X))\n$$\n由于加上一个常数（$\\beta\\mu_X$）不改变方差，这可以简化为：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\frac{1}{n} \\operatorname{Var}(Y - \\beta X)\n$$\n使用随机变量线性组合方差的一般性质 $\\operatorname{Var}(A - cB) = \\operatorname{Var}(A) + c^2\\operatorname{Var}(B) - 2c\\operatorname{Cov}(A,B)$，我们得到：\n$$\n\\operatorname{Var}(Y - \\beta X) = \\operatorname{Var}(Y) + \\beta^2 \\operatorname{Var}(X) - 2\\beta \\operatorname{Cov}(Y, X)\n$$\n我们用 $\\sigma_Y^2 = \\operatorname{Var}(Y)$、$\\sigma_X^2 = \\operatorname{Var}(X)$ 和 $\\operatorname{Cov}(Y,X)$ 来表示。估计量的方差是 $\\beta$ 的函数：\n$$\nV(\\beta) = \\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\frac{1}{n} (\\sigma_Y^2 - 2\\beta \\operatorname{Cov}(Y, X) + \\beta^2 \\sigma_X^2)\n$$\n为了找到最小值，我们对 $V(\\beta)$ 关于 $\\beta$ 求导，并令导数为零：\n$$\n\\frac{d V}{d \\beta} = \\frac{1}{n} (-2 \\operatorname{Cov}(Y, X) + 2\\beta \\sigma_X^2) = 0\n$$\n假设 $\\sigma_X^2 \\neq 0$，我们解出 $\\beta$：\n$$\n2\\beta \\sigma_X^2 = 2 \\operatorname{Cov}(Y, X) \\implies \\beta = \\frac{\\operatorname{Cov}(Y, X)}{\\sigma_X^2}\n$$\n这就是最优混合系数 $\\beta^{\\star}$。二阶导数 $\\frac{d^2 V}{d \\beta^2} = \\frac{2\\sigma_X^2}{n}$ 是正的，这证实了该 $\\beta$ 值使方差最小化。\n因此，最优混合系数为：\n$$\n\\beta^{\\star} = \\frac{\\operatorname{Cov}(Y, X)}{\\operatorname{Var}(X)}\n$$\n\n### 第 3 部分：最小化方差和缩减因子\n\n为了找到最小化方差，我们将 $\\beta^{\\star}$ 代入 $V(\\beta)$ 的表达式中：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star})) = \\frac{1}{n} \\left(\\sigma_Y^2 - 2 \\left(\\frac{\\operatorname{Cov}(Y, X)}{\\sigma_X^2}\\right) \\operatorname{Cov}(Y, X) + \\left(\\frac{\\operatorname{Cov}(Y, X)}{\\sigma_X^2}\\right)^2 \\sigma_X^2\\right)\n$$\n$$\n= \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{2(\\operatorname{Cov}(Y, X))^2}{\\sigma_X^2} + \\frac{(\\operatorname{Cov}(Y, X))^2}{\\sigma_X^2}\\right)\n$$\n$$\n= \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{(\\operatorname{Cov}(Y, X))^2}{\\sigma_X^2}\\right)\n$$\n现在，我们引入相关系数 $\\rho = \\operatorname{Corr}(Y,X) = \\frac{\\operatorname{Cov}(Y, X)}{\\sigma_Y \\sigma_X}$。这得到 $\\operatorname{Cov}(Y, X) = \\rho \\sigma_Y \\sigma_X$。将此代入最小化方差表达式：\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star})) = \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{(\\rho \\sigma_Y \\sigma_X)^2}{\\sigma_X^2}\\right) = \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{\\rho^2 \\sigma_Y^2 \\sigma_X^2}{\\sigma_X^2}\\right)\n$$\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star})) = \\frac{1}{n} (\\sigma_Y^2 - \\rho^2 \\sigma_Y^2) = \\frac{\\sigma_Y^2}{n} (1 - \\rho^2)\n$$\n普通蒙特卡洛估计量的方差是 $\\operatorname{Var}(\\hat{\\mu}_{Y,MC}) = \\operatorname{Var}(\\frac{1}{n}\\sum Y_i) = \\frac{1}{n}\\operatorname{Var}(Y) = \\frac{\\sigma_Y^2}{n}$。\n\n方差缩减因子 $\\phi$ 是最小化控制变量方差与普通蒙特卡洛方差之比：\n$$\n\\phi = \\frac{\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star}))}{\\operatorname{Var}(\\hat{\\mu}_{Y,MC})} = \\frac{\\frac{\\sigma_Y^2}{n} (1 - \\rho^2)}{\\frac{\\sigma_Y^2}{n}}\n$$\n$$\n\\phi = 1 - \\rho^2\n$$\n\n### 第 4 部分：解释\n\n分析揭示了两个用于实现的关键结果。首先，最优系数 $\\beta^{\\star}$ 的公式可以用相关性 $\\rho$ 表示：\n$$\n\\beta^{\\star} = \\frac{\\rho \\sigma_Y \\sigma_X}{\\sigma_X^2} = \\rho \\frac{\\sigma_Y}{\\sigma_X}\n$$\n其次，方差缩减因子是 $\\phi = 1 - \\rho^2$。\n\n这些结果为在此背景下的控制变量方法提供了关键的见解：\n1.  **相关性 $\\rho$ 的作用**：因子 $\\phi = 1 - \\rho^2$ 表明，控制变量技术的有效性完全由高保真量 $Y$ 与其低阶代理模型 $X$ 之间相关系数的平方决定。更强的线性关系（即 $|\\rho|$ 更接近 $1$）导致更大的方差缩减。如果 $X$ 和 $Y$ 完全相关（$|\\rho| = 1$），方差变为零（$\\phi=0$），这意味着可以找到确切的均值。如果它们不相关（$\\rho=0$），则没有方差缩减（$\\phi=1$），控制变量是无用的。\n\n2.  **方差 $\\sigma_Y^2$ 和 $\\sigma_X^2$ 的作用**：这些方差不影响方差缩减的百分比，后者仅取决于 $\\rho$。然而，它们决定了控制变量校正项的最优缩放。系数 $\\beta^{\\star} = \\rho \\frac{\\sigma_Y}{\\sigma_X}$ 是 $Y$ 对 $X$ 的最佳线性拟合（回归）的斜率。它缩放代理模型与其均值的偏差 $(X - \\mu_X)$，以最好地抵消 $Y$ 中相应的偏差。\n\n在侵入式 PCE 的背景下，一个高质量（高阶）的 PCE 代理模型预计与完整模型具有非常高的相关性，使其成为一个优秀的控制变量。结果 $\\phi = 1 - \\rho^2$ 精确地量化了构建这样一个代理模型的好处：计算侵入式 PCE 模型（以获得 $\\mu_X$ 和 $X_i$ 的样本）的成本，换来了为达到 $\\mu_Y$ 的目标精度所需昂贵高保真样本 $Y_i$ 数量的急剧减少。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal control variate coefficient and variance reduction factor\n    for a series of test cases based on derived closed-form expressions.\n    \"\"\"\n\n    # Test suite:\n    # Each tuple represents a case: (rho, sigma_Y, sigma_X)\n    test_cases = [\n        # Case A: rho = 0.0, sigma_Y = 1.0, sigma_X = 1.0\n        (0.0, 1.0, 1.0),\n        # Case B: rho = 0.5, sigma_Y = 2.0, sigma_X = 1.0\n        (0.5, 2.0, 1.0),\n        # Case C: rho = -0.7, sigma_Y = 1.5, sigma_X = 3.0\n        (-0.7, 1.5, 3.0),\n        # Case D: rho = 0.99, sigma_Y = 1.0, sigma_X = 0.8\n        (0.99, 1.0, 0.8),\n        # Case E: rho = -0.99, sigma_Y = 0.5, sigma_X = 4.0\n        (-0.99, 0.5, 4.0),\n    ]\n\n    results = []\n    for rho, sigma_y, sigma_x in test_cases:\n        # The derived closed-form expressions are:\n        # 1. Optimal mixing coefficient: beta_star = rho * (sigma_Y / sigma_X)\n        # 2. Variance reduction factor: phi = 1 - rho^2\n\n        # Check for the case where sigma_x is zero to avoid division by zero,\n        # although this is not present in the test cases.\n        if sigma_x == 0:\n            # If sigma_x is 0, X is a constant. If X is a constant, Cov(Y, X) = 0\n            # and rho = 0. beta_star is ill-defined, but since the covariance\n            # is zero, the optimal beta is 0.\n            beta_star = 0.0\n        else:\n            beta_star = rho * (sigma_y / sigma_x)\n\n        # Calculate the variance reduction factor.\n        phi = 1 - rho**2\n\n        # Append the calculated values, rounded to 6 decimal places, to the results list.\n        results.append(round(beta_star, 6))\n        results.append(round(phi, 6))\n\n    # Format the final output string as a comma-separated list within square brackets.\n    # The map(str, ...) converts each float in the results list to its string representation.\n    # ','.join(...) concatenates these strings with a comma in between.\n    # The f-string places this joined string inside the required brackets.\n    output_string = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_string)\n\nsolve()\n```", "id": "3448294"}]}