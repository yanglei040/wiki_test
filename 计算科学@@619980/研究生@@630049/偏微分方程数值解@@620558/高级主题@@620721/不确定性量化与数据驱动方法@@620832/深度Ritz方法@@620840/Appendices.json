{"hands_on_practices": [{"introduction": "深度 Ritz 方法的核心思想是最小化一个能量泛函。为了清晰地揭示这一过程，本练习将复杂的神经网络替换为一个简单的线性函数。通过这种简化，我们可以使用精确积分（而非蒙特卡洛方法）来分析计算能量，并找到其精确的最小值 [@problem_id:3376700]。这个“纸上谈兵”的练习旨在帮助你建立对变分原理和 Ritz 方法基本工作流程的直观理解。", "problem": "考虑单位区间上的具有齐次狄利克雷边界条件的一维泊松偏微分方程 (PDE)：求 $u:(0,1)\\to\\mathbb{R}$ 使得\n$$\n- u''(x) = 1 \\quad \\text{for } x\\in (0,1), \\qquad u(0)=0, \\quad u(1)=0.\n$$\n从分部积分和最小势能原理推导出的弱形式出发，构建能量泛函，使其在索博列夫空间 $H_0^1(0,1)$ 中的最小化子与该偏微分方程的弱解一致。然后，通过一个乘法拟设来强制施加边界条件，从而实例化 Deep Ritz 方法\n$$\nu_{\\theta}(x) = \\phi(x)\\,N_{\\theta}(x),\n$$\n其中 $\\phi(x)=x(1-x)$ 且 $N_{\\theta}(x)=a x + b$，带有可训练参数 $\\theta=(a,b)\\in\\mathbb{R}^2$。使用在 $(0,1)$ 上的精确积分（而非采样积分），将该变分公式所蕴含的能量写成关于 $(a,b)$ 的函数，并计算其在所有 $(a,b)\\in\\mathbb{R}^2$ 上的最小值。将最终的最小能量表示为一个无舍入的精确值。", "solution": "用户希望我在尝试解决问题之前，严格验证问题陈述。\n\n### 第1步：提取已知条件\n- **偏微分方程 (PDE):** $- u''(x) = 1$，对于 $x \\in (0,1)$。\n- **定义域:** $\\Omega = (0,1)$。\n- **边界条件:** $u(0)=0$, $u(1)=0$ (齐次狄利克雷)。\n- **函数空间:** 解 $u$ 在索博列夫空间 $H_0^1(0,1)$ 中寻找。\n- **方法:** 该问题将使用基于最小势能原理的变分方法求解，具体为 Deep Ritz 方法。\n- **拟设 (Ansatz):** 试探函数的形式为 $u_{\\theta}(x) = \\phi(x)\\,N_{\\theta}(x)$，其中 $\\theta = (a,b)$ 是可训练参数。\n- **边界强制函数:** $\\phi(x) = x(1-x)$。\n- **“神经网络”近似:** $N_{\\theta}(x) = a x + b$。\n- **任务:**\n    1. 构建与该 PDE 对应的能量泛函 $J(u)$。\n    2. 将拟设 $u_{\\theta}(x)$ 代入 $J(u)$，得到能量函数 $E(a,b) = J(u_{\\theta})$。\n    3. 使用精确积分将 $E(a,b)$ 表示为 $a$ 和 $b$ 的函数。\n    4. 找到 $E(a,b)$ 在所有 $(a,b) \\in \\mathbb{R}^2$ 上的最小值。\n    5. 最终答案必须是一个精确值。\n\n### 第2步：使用提取的已知条件进行验证\n- **科学依据:** 该问题基于泊松方程、最小势能原理和 Deep Ritz 方法，这些都是应用数学和物理学中标准且成熟的概念。\n- **适定性:** 最小化一个二元二次函数的问题是适定的。其底层的带狄利克雷边界条件的 PDE 也是一个经典的适定问题。\n- **客观性:** 问题使用精确的数学语言陈述，没有主观或含糊不清的术语。\n- **额外检查:** 问题是自洽的，提供了所有必要信息。没有矛盾之处。简化的线性“网络”使得问题如要求的那样可以进行解析处理，并且预期存在唯一的最小值。\n\n### 第3步：结论与行动\n问题是有效的。现在开始求解过程。\n\n泊松方程由 $-u''(x) = f(x)$ 给出，其中 $f(x)=1$。最小势能原理指出，该 PDE 的解是在适当空间（本例中为 $H_0^1(0,1)$）中使能量泛函 $J(u)$ 最小化的函数 $u$。该泛函由下式给出\n$$\nJ(u) = \\frac{1}{2} \\int_{0}^{1} (u'(x))^2 \\, dx - \\int_{0}^{1} f(x)u(x) \\, dx\n$$\n对于 $f(x)=1$，这变为\n$$\nJ(u) = \\frac{1}{2} \\int_{0}^{1} (u'(x))^2 \\, dx - \\int_{0}^{1} u(x) \\, dx\n$$\nDeep Ritz 方法旨在在一族由 $\\theta$ 参数化的试探函数上最小化此泛函。给定的试探函数是 $u_{\\theta}(x)$，其参数为 $\\theta=(a,b)$：\n$$\nu_{a,b}(x) = \\phi(x) N_{\\theta}(x) = (x(1-x))(ax+b)\n$$\n对于任意选择的 $a$ 和 $b$，这种形式都强制满足边界条件 $u_{a,b}(0)=0$ 和 $u_{a,b}(1)=0$。我们展开 $u_{a,b}(x)$ 以便更容易地进行微分和积分：\n$$\nu_{a,b}(x) = (x-x^2)(ax+b) = ax^2 + bx - ax^3 - bx^2 = -ax^3 + (a-b)x^2 + bx\n$$\n接下来，我们计算导数 $u'_{a,b}(x)$：\n$$\nu'_{a,b}(x) = \\frac{d}{dx} \\left( -ax^3 + (a-b)x^2 + bx \\right) = -3ax^2 + 2(a-b)x + b\n$$\n现在我们必须计算能量泛函 $E(a,b) = J(u_{a,b})$ 中的两个积分。\n\n第一个积分（线性项）：\n$$\n\\int_{0}^{1} u_{a,b}(x) \\, dx = \\int_{0}^{1} (-ax^3 + (a-b)x^2 + bx) \\, dx\n$$\n$$\n= \\left[ -a\\frac{x^4}{4} + (a-b)\\frac{x^3}{3} + b\\frac{x^2}{2} \\right]_{0}^{1} = -\\frac{a}{4} + \\frac{a-b}{3} + \\frac{b}{2}\n$$\n$$\n= \\frac{-3a + 4(a-b) + 6b}{12} = \\frac{-3a + 4a - 4b + 6b}{12} = \\frac{a+2b}{12}\n$$\n\n第二个积分（二次项）：\n$$\n\\int_{0}^{1} (u'_{a,b}(x))^2 \\, dx = \\int_{0}^{1} (-3ax^2 + 2(a-b)x + b)^2 \\, dx\n$$\n我们展开被积函数：\n$$\n(-3ax^2 + 2(a-b)x + b)^2 = 9a^2x^4 + 4(a-b)^2x^2 + b^2 - 12a(a-b)x^3 - 6abx^2 + 4b(a-b)x\n$$\n$$\n= 9a^2x^4 - 12a(a-b)x^3 + [4(a-b)^2 - 6ab]x^2 + 4b(a-b)x + b^2\n$$\n从 $x=0$ 到 $x=1$逐项积分：\n$$\n\\int_{0}^{1} (u'_{a,b}(x))^2 \\, dx = \\left[ \\frac{9a^2x^5}{5} - \\frac{12a(a-b)x^4}{4} + \\frac{4(a-b)^2 - 6ab}{3}x^3 + \\frac{4b(a-b)x^2}{2} + b^2x \\right]_{0}^{1}\n$$\n$$\n= \\frac{9a^2}{5} - 3a(a-b) + \\frac{4(a^2-2ab+b^2) - 6ab}{3} + 2b(a-b) + b^2\n$$\n$$\n= \\frac{9a^2}{5} - 3a^2 + 3ab + \\frac{4a^2-8ab-6ab+4b^2}{3} + 2ab - 2b^2 + b^2\n$$\n$$\n= \\frac{9a^2}{5} - 3a^2 + 5ab - b^2 + \\frac{4a^2 - 14ab + 4b^2}{3}\n$$\n将所有项通分为分母 15：\n$$\n= \\frac{3(9a^2) - 15(3a^2) + 15(5ab) - 15b^2 + 5(4a^2 - 14ab + 4b^2)}{15}\n$$\n$$\n= \\frac{27a^2 - 45a^2 + 75ab - 15b^2 + 20a^2 - 70ab + 20b^2}{15}\n$$\n$$\n= \\frac{(27-45+20)a^2 + (75-70)ab + (-15+20)b^2}{15} = \\frac{2a^2 + 5ab + 5b^2}{15}\n$$\n现在，我们组装能量泛函 $E(a,b)$：\n$$\nE(a,b) = \\frac{1}{2} \\left( \\frac{2a^2 + 5ab + 5b^2}{15} \\right) - \\left( \\frac{a+2b}{12} \\right) = \\frac{2a^2 + 5ab + 5b^2}{30} - \\frac{a+2b}{12}\n$$\n为了找到最小值，我们计算关于 $a$ 和 $b$ 的偏导数并令其为零。\n$$\n\\frac{\\partial E}{\\partial a} = \\frac{4a+5b}{30} - \\frac{1}{12} = 0\n$$\n$$\n\\frac{\\partial E}{\\partial b} = \\frac{5a+10b}{30} - \\frac{2}{12} = 0\n$$\n这给出了一个线性方程组：\n1. $12(4a+5b) = 30 \\implies 48a + 60b = 30 \\implies 8a + 10b = 5$\n2. $\\frac{5a+10b}{30} = \\frac{1}{6} \\implies 6(5a+10b) = 30 \\implies 30a + 60b = 30 \\implies a + 2b = 1$\n\n从第二个方程，我们得到 $a=1-2b$。将此代入第一个方程：\n$$\n8(1-2b) + 10b = 5\n$$\n$$\n8 - 16b + 10b = 5 \\implies 8 - 6b = 5 \\implies 3 = 6b \\implies b = \\frac{1}{2}\n$$\n将 $b=\\frac{1}{2}$ 代回 $a=1-2b$：\n$$\na = 1 - 2\\left(\\frac{1}{2}\\right) = 1-1=0\n$$\n能量在 $(a,b) = (0, \\frac{1}{2})$ 处最小化。最后，我们将这些参数代入 $E(a,b)$ 来计算最小能量值：\n$$\nE\\left(0, \\frac{1}{2}\\right) = \\frac{2(0)^2 + 5(0)(\\frac{1}{2}) + 5(\\frac{1}{2})^2}{30} - \\frac{0+2(\\frac{1}{2})}{12}\n$$\n$$\n= \\frac{5(\\frac{1}{4})}{30} - \\frac{1}{12} = \\frac{\\frac{5}{4}}{30} - \\frac{1}{12} = \\frac{5}{120} - \\frac{1}{12}\n$$\n$$\n= \\frac{1}{24} - \\frac{2}{24} = -\\frac{1}{24}\n$$\n最小能量为 $-\\frac{1}{24}$。", "answer": "$$ \\boxed{-\\frac{1}{24}} $$", "id": "3376700"}, {"introduction": "在掌握了核心的最小化思想后，我们来探讨一个关键的实践问题：如何施加边界条件。本练习聚焦于“软约束”的罚函数法，这是一种在深度学习求解器中广泛应用的技术。我们将运用变分法，从理论上推导出在罚函数作用下，解所满足的有效边界条件，从而揭示罚函数参数 $\\alpha$ 如何控制解在边界上的行为 [@problem_id:3376716]。", "problem": "考虑使用深度里兹方法对一维域 $\\Omega=(0,1)$ 上具有齐次狄利克雷边界条件的泊松偏微分方程（PDE）进行数值求解，其中边界条件通过边界惩罚项进行软性施加。深度里兹方法通过一个人工神经网络（ANN）来参数化试探函数 $u$，并在容许空间上最小化一个里兹泛函。在无限模型容量的连续极限下，带惩罚能量的精确最小化子 $u_{\\alpha}$ 可以通过变分法直接研究。\n\n设带惩罚的里兹泛函为\n$$\n\\mathcal{J}_{\\alpha}(u) \\;=\\; \\int_{0}^{1} \\left( \\frac{1}{2}\\,|u'(x)|^{2} \\;-\\; f(x)\\,u(x) \\right)\\,\\mathrm{d}x \\;+\\; \\alpha\\left( |u(0)|^{2} \\,+\\, |u(1)|^{2} \\right),\n$$\n其中 $f(x)=1$ 且惩罚参数 $\\alpha>0$。无约束最小化在 $H^{1}(0,1)$ 上进行，不将 $u(0)=0$ 或 $u(1)=0$ 作为硬约束施加。\n\n从 $\\mathcal{J}_{\\alpha}$ 的驻点满足带有惩罚项导出的相应边界项的欧拉-拉格朗日条件这一基本原理出发，运用变分法的基本原理推导出 $u_{\\alpha}$ 的控制常微分方程和边界条件。然后显式求解所得的边值问题，并确定 $u_{\\alpha}(0)$ 的精确值，表示为关于 $\\alpha$ 的闭式表达式。\n\n请以单个闭式解析表达式的形式提供您的最终答案。无需四舍五入，也不涉及任何单位。", "solution": "用户提供了一个定义明确的变分法问题，该问题源于一维泊松方程的深度里兹方法的连续极限。该问题具有科学依据、是适定的、客观的，并包含获得唯一解所需的所有信息。因此，该问题是有效的。\n\n任务是找到带惩罚的里兹泛函\n$$\n\\mathcal{J}_{\\alpha}(u) = \\int_{0}^{1} \\left( \\frac{1}{2}\\,|u'(x)|^{2} - f(x)\\,u(x) \\right)\\,\\mathrm{d}x + \\alpha\\left( |u(0)|^{2} + |u(1)|^{2} \\right)\n$$\n的最小化子 $u_{\\alpha}(x)$，其中 $u$ 的函数空间为 $H^{1}(0,1)$，$f(x)=1$，$\\alpha>0$ 是一个惩罚参数。我们必须首先推导出控制微分方程和边界条件，然后求解 $u_{\\alpha}(x)$ 以找到 $u_{\\alpha}(0)$ 的值。\n\n泛函 $\\mathcal{J}_{\\alpha}$ 的最小化子 $u_{\\alpha}$ 是一个驻点。为了找到这个驻点，我们使用变分法。我们考虑 $u_{\\alpha}$ 的一个形式为 $u(x) = u_{\\alpha}(x) + \\epsilon \\eta(x)$ 的变分，其中 $\\eta(x) \\in H^{1}(0,1)$ 是一个任意检验函数，$\\epsilon$ 是一个小的实参数。$u_{\\alpha}$ 成为驻点的条件是泛函的一阶变分为零，即\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\mathcal{J}_{\\alpha}(u_{\\alpha} + \\epsilon \\eta) \\bigg|_{\\epsilon=0} = 0\n$$\n对所有容许的 $\\eta$ 成立。\n\n让我们计算导数。由于函数 $u$ 是实值的，我们可以写作 $|u|^{2} = u^{2}$。泛函变为：\n$$\n\\mathcal{J}_{\\alpha}(u_{\\alpha} + \\epsilon \\eta) = \\int_{0}^{1} \\left( \\frac{1}{2}\\,(u'_{\\alpha} + \\epsilon \\eta')^{2} - f(x)\\,(u_{\\alpha} + \\epsilon \\eta) \\right)\\,\\mathrm{d}x + \\alpha\\left( (u_{\\alpha}(0) + \\epsilon \\eta(0))^{2} + (u_{\\alpha}(1) + \\epsilon \\eta(1))^{2} \\right)\n$$\n对 $\\epsilon$ 求导：\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\mathcal{J}_{\\alpha}(u_{\\alpha} + \\epsilon \\eta) = \\int_{0}^{1} \\left( (u'_{\\alpha} + \\epsilon \\eta') \\eta' - f(x)\\eta \\right)\\,\\mathrm{d}x + \\alpha\\left( 2(u_{\\alpha}(0) + \\epsilon \\eta(0))\\eta(0) + 2(u_{\\alpha}(1) + \\epsilon \\eta(1))\\eta(1) \\right)\n$$\n令 $\\epsilon=0$，我们得到一阶变分条件：\n$$\n\\int_{0}^{1} \\left( u'_{\\alpha}(x) \\eta'(x) - f(x) \\eta(x) \\right)\\,\\mathrm{d}x + 2\\alpha u_{\\alpha}(0) \\eta(0) + 2\\alpha u_{\\alpha}(1) \\eta(1) = 0\n$$\n为了找到微分方程的强形式，我们对积分中的第一项应用分部积分：\n$$\n\\int_{0}^{1} u'_{\\alpha}(x) \\eta'(x) \\,\\mathrm{d}x = \\left[ u'_{\\alpha}(x) \\eta(x) \\right]_{0}^{1} - \\int_{0}^{1} u''_{\\alpha}(x) \\eta(x) \\,\\mathrm{d}x = u'_{\\alpha}(1)\\eta(1) - u'_{\\alpha}(0)\\eta(0) - \\int_{0}^{1} u''_{\\alpha}(x)\\eta(x) \\,\\mathrm{d}x\n$$\n将其代回变分方程得到：\n$$\nu'_{\\alpha}(1)\\eta(1) - u'_{\\alpha}(0)\\eta(0) - \\int_{0}^{1} u''_{\\alpha}(x)\\eta(x) \\,\\mathrm{d}x - \\int_{0}^{1} f(x)\\eta(x) \\,\\mathrm{d}x + 2\\alpha u_{\\alpha}(0) \\eta(0) + 2\\alpha u_{\\alpha}(1) \\eta(1) = 0\n$$\n我们可以通过组合积分项和边界求值项来重排各项：\n$$\n\\int_{0}^{1} \\left( -u''_{\\alpha}(x) - f(x) \\right) \\eta(x) \\,\\mathrm{d}x + \\left( u'_{\\alpha}(1) + 2\\alpha u_{\\alpha}(1) \\right) \\eta(1) + \\left( -u'_{\\alpha}(0) + 2\\alpha u_{\\alpha}(0) \\right) \\eta(0) = 0\n$$\n这个方程必须对检验函数 $\\eta(x) \\in H^{1}(0,1)$ 的任何选择都成立。根据变分法基本引理，如果我们首先考虑在边界上为零的检验函数（$\\eta(0)=0$ 和 $\\eta(1)=0$），则积分项必须为零。这给出了域内部的欧拉-拉格朗日方程：\n$$\n-u''_{\\alpha}(x) - f(x) = 0 \\quad \\text{for } x \\in (0,1)\n$$\n在满足此常微分方程（ODE）的情况下，对所有 $\\eta$，积分项都为零，变分方程简化为边界项：\n$$\n\\left( u'_{\\alpha}(1) + 2\\alpha u_{\\alpha}(1) \\right) \\eta(1) + \\left( -u'_{\\alpha}(0) + 2\\alpha u_{\\alpha}(0) \\right) \\eta(0) = 0\n$$\n由于这对 $\\eta(0)$ 和 $\\eta(1)$ 的任意值都必须成立，$\\eta(0)$ 和 $\\eta(1)$ 的系数必须独立为零。这得出了自然边界条件：\n$$\nu'_{\\alpha}(0) = 2\\alpha u_{\\alpha}(0)\n$$\n$$\nu'_{\\alpha}(1) = -2\\alpha u_{\\alpha}(1)\n$$\n我们现在得到了一个关于 $u_{\\alpha}(x)$ 的完整边值问题（BVP）。给定 $f(x)=1$，该边值问题为：\n$$\n\\begin{cases}\n-u''_{\\alpha}(x) = 1  \\text{for } x \\in (0,1) \\\\\nu'_{\\alpha}(0) = 2\\alpha u_{\\alpha}(0) \\\\\nu'_{\\alpha}(1) = -2\\alpha u_{\\alpha}(1)\n\\end{cases}\n$$\n我们着手求解此边值问题。将常微分方程 $u''_{\\alpha}(x) = -1$ 对 $x$ 积分两次，得到通解：\n$$\nu'_{\\alpha}(x) = -x + C_1\n$$\n$$\nu_{\\alpha}(x) = -\\frac{1}{2}x^2 + C_1 x + C_2\n$$\n其中 $C_1$ 和 $C_2$ 是积分常数。我们使用边界条件来确定这些常数。\n从通解中，我们有：\n$$\nu_{\\alpha}(0) = C_2 \\quad \\text{and} \\quad u'_{\\alpha}(0) = C_1\n$$\n$$\nu_{\\alpha}(1) = -\\frac{1}{2} + C_1 + C_2 \\quad \\text{and} \\quad u'_{\\alpha}(1) = -1 + C_1\n$$\n应用第一个边界条件 $u'_{\\alpha}(0) = 2\\alpha u_{\\alpha}(0)$：\n$$\nC_1 = 2\\alpha C_2\n$$\n应用第二个边界条件 $u'_{\\alpha}(1) = -2\\alpha u_{\\alpha}(1)$：\n$$\n-1 + C_1 = -2\\alpha \\left(-\\frac{1}{2} + C_1 + C_2\\right)\n$$\n$$\n-1 + C_1 = \\alpha - 2\\alpha C_1 - 2\\alpha C_2\n$$\n$$\n(1 + 2\\alpha)C_1 + 2\\alpha C_2 = 1 + \\alpha\n$$\n我们现在得到了一个关于 $C_1$ 和 $C_2$ 的二元线性方程组：\n1. $C_1 - 2\\alpha C_2 = 0$\n2. $(1 + 2\\alpha)C_1 + 2\\alpha C_2 = 1 + \\alpha$\n\n将第一个方程的 $C_1 = 2\\alpha C_2$ 代入第二个方程：\n$$\n(1 + 2\\alpha)(2\\alpha C_2) + 2\\alpha C_2 = 1 + \\alpha\n$$\n$$\n(2\\alpha + 4\\alpha^2) C_2 + 2\\alpha C_2 = 1 + \\alpha\n$$\n$$\n(4\\alpha + 4\\alpha^2) C_2 = 1 + \\alpha\n$$\n$$\n4\\alpha(1 + \\alpha) C_2 = 1 + \\alpha\n$$\n由于 $\\alpha > 0$，我们知道 $1+\\alpha \\neq 0$，所以我们可以用它来除：\n$$\nC_2 = \\frac{1+\\alpha}{4\\alpha(1 + \\alpha)} = \\frac{1}{4\\alpha}\n$$\n问题要求 $u_{\\alpha}(0)$ 的值。从我们的通解中，我们知道 $u_{\\alpha}(0) = C_2$。\n因此，精确值为：\n$$\nu_{\\alpha}(0) = \\frac{1}{4\\alpha}\n$$\n为了完整起见，我们也可以求出 $C_1 = 2\\alpha C_2 = 2\\alpha \\left(\\frac{1}{4\\alpha}\\right) = \\frac{1}{2}$。完整的解是 $u_{\\alpha}(x) = -\\frac{1}{2}x^2 + \\frac{1}{2}x + \\frac{1}{4\\alpha}$。", "answer": "$$\n\\boxed{\\frac{1}{4\\alpha}}\n$$", "id": "3376716"}, {"introduction": "现在，让我们从解析练习转向一个完整的计算实现。本练习将引导你为一个参数化的偏微分方程族构建一个深度 Ritz 求解器，这是一个更贴近科研前沿的复杂场景。你将接触到随机特征映射和元学习等现代概念，目标是为一族相关问题找到一个“最优”的初始解，从而在解决新问题时能够更快地收敛 [@problem_id:3376733]。这个实践综合了变分法、数值积分和机器学习，充分展示了深度 Ritz 框架的灵活性与强大功能。", "problem": "要求您为一个参数化椭圆偏微分方程族实现一个深度 Ritz 求解器，该求解器使用一个具有固定隐藏层（随机特征）和线性输出层的神经网络试验空间，并结合一个元学习过程来选择最小化平均能量的初始化。具体来说，考虑在单位正方形域 $\\Omega = (0,1)^2$ 上具有齐次狄利克雷边界条件的问题族：\n$$\n-\\nabla\\cdot\\big(a_{\\mu}(x,y)\\,\\nabla u_{\\mu}(x,y)\\big) = f_{\\mu}(x,y), \\quad (x,y)\\in\\Omega,\\quad u_{\\mu}|_{\\partial\\Omega}=0,\n$$\n其中参数 $\\mu$ 的取值范围是区间 $[0,1]$ 的一个子集。深度 Ritz 方法旨在在一类试验函数 $u$ 上最小化能量泛函\n$$\n\\mathcal{J}_{\\mu}(u) = \\int_{\\Omega} \\left( \\frac{1}{2}\\, a_{\\mu}(x,y)\\, \\lvert \\nabla u(x,y) \\rvert^2 \\;-\\; f_{\\mu}(x,y)\\,u(x,y) \\right)\\, \\mathrm{d}x\\,\\mathrm{d}y.\n$$\n使用一个通过边界因子精确施加狄利克雷边界条件的神经网络试验空间。令 $b(x,y) = x(1-x)\\,y(1-y)$，并使用带有双曲正切激活函数的单个隐藏层定义一个固定的（不可训练的）特征映射 $\\varphi:\\Omega\\to\\mathbb{R}^{D}$：\n$$\n\\varphi(x,y) = \\tanh\\big(W\\,[x,y]^{\\top} + c\\big),\n$$\n其中 $W\\in\\mathbb{R}^{D\\times 2}$ 和 $c\\in\\mathbb{R}^{D}$ 使用固定的随机种子从标准正态分布中一次性抽取。可训练函数被限制在线性张成空间中\n$$\nu_{w}(x,y) \\;=\\; b(x,y)\\,\\varphi(x,y)^{\\top} w, \\quad w\\in \\mathbb{R}^{D}.\n$$\n对于每个参数 $\\mu$，Ritz 能量变为关于 $w$ 的严格凸二次函数，形式如下\n$$\n\\mathcal{J}_{\\mu}(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} H_{\\mu}\\, w \\;-\\; h_{\\mu}^{\\top} w \\;+\\; C_{\\mu},\n$$\n其中 $H_{\\mu}\\in\\mathbb{R}^{D\\times D}$ 和 $h_{\\mu}\\in\\mathbb{R}^{D}$ 由依赖于 $a_{\\mu}$、$f_{\\mu}$、$b$ 和 $\\varphi$ 的内部积分给出。使用在 $\\Omega$ 中均匀分布的 $N$ 个独立同分布样本，通过蒙特卡洛积分来近似这些积分：\n- 对于每个样本 $(x_s,y_s)$，令 $J_s\\in\\mathbb{R}^{D\\times 2}$ 表示雅可比矩阵，其行向量为 $\\nabla\\!\\big(b\\,\\varphi_i\\big)(x_s,y_s)$，因此 $\\lvert \\nabla u_w \\rvert^2 = \\sum_{i,j} w_i w_j \\, J_s(i,:) \\cdot J_s(j,:) = w^{\\top} \\big(J_s J_s^{\\top}\\big) w$。\n- 于是\n$$\nH_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} a_{\\mu}(x_s,y_s)\\; J_s J_s^{\\top},\\quad\nh_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} f_{\\mu}(x_s,y_s)\\; b(x_s,y_s)\\,\\varphi(x_s,y_s).\n$$\n\n真实值参数化。为了进行定量评估，我们从一个光滑的、可精确求解的配置中定义 $a_{\\mu}$ 和 $f_{\\mu}$。对于任意 $\\mu\\in[0,1]$，令\n$$\na_{\\mu}(x,y) \\;=\\; 1 \\;+\\; \\tfrac{1}{2}\\,\\mu\\, \\sin(2\\pi x)\\,\\sin(2\\pi y),\n$$\n并定义精确解\n$$\nu^{\\star}_{\\mu}(x,y) \\;=\\; \\sin(\\pi x)\\,\\sin(\\pi y) \\;+\\; \\mu\\, \\sin(2\\pi x)\\,\\sin(\\pi y).\n$$\n设置\n$$\nf_{\\mu}(x,y) \\;=\\; -\\nabla\\cdot\\big(a_{\\mu}(x,y)\\,\\nabla u^{\\star}_{\\mu}(x,y)\\big),\n$$\n使得 $u^{\\star}_{\\mu}$ 精确满足带有齐次狄利克雷数据的偏微分方程。您必须使用乘法法则解析地计算 $f_{\\mu}$：\n$$\nf_{\\mu} \\;=\\; -\\big(\\nabla a_{\\mu}\\cdot \\nabla u^{\\star}_{\\mu} \\;+\\; a_{\\mu}\\,\\Delta u^{\\star}_{\\mu}\\big).\n$$\n\n元学习设置。考虑一个参数训练集 $\\mathcal{M}_{\\mathrm{train}} = \\{\\, 0.0,\\; 0.4,\\; 0.8 \\,\\}$ 并定义平均能量\n$$\n\\overline{\\mathcal{J}}(w) \\;=\\; \\frac{1}{\\lvert \\mathcal{M}_{\\mathrm{train}}\\rvert}\\sum_{\\mu\\in \\mathcal{M}_{\\mathrm{train}}} \\mathcal{J}_{\\mu}(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} \\overline{H}\\, w \\;-\\; \\overline{h}^{\\top} w \\;+\\; \\overline{C},\n$$\n其中 $\\overline{H}$ 和 $\\overline{h}$ 是 $H_{\\mu}$ 和 $h_{\\mu}$ 在所有 $\\mu$ 上使用相同的内部样本计算出的相应平均值，以减少方差。将元初始化定义为唯一的最小化子\n$$\nw_{\\mathrm{meta}} \\;=\\; \\arg\\min_{w\\in\\mathbb{R}^{D}} \\overline{\\mathcal{J}}(w).\n$$\n逐任务自适应使用梯度下降法对 $\\mathcal{J}_{\\mu}(w)$ 进行优化，从一个初始化 $w_0$ 开始，并使用一个共同的步长 $\\eta>0$：\n$$\nw_{t+1} \\;=\\; w_t - \\eta\\, \\nabla \\mathcal{J}_{\\mu}(w_t) \\;=\\; w_t - \\eta\\,(H_{\\mu} w_t - h_{\\mu}),\n$$\n对于 $t=0,1,\\dots,T-1$。对于严格凸的二次目标函数，经过 $T$ 步后的摊销误差（次优性差距）在 $I-\\eta H_{\\mu}$ 的谱半径方面有一个最坏情况界。令 $\\rho_{\\mu}(\\eta) = \\max_{\\lambda\\in\\mathrm{spec}(H_{\\mu})} \\lvert 1 - \\eta \\lambda\\rvert$。那么对于任意 $w_0$，\n$$\n\\mathcal{J}_{\\mu}(w_T) - \\min_{w}\\mathcal{J}_{\\mu}(w) \\;\\le\\; \\rho_{\\mu}(\\eta)^{2T}\\, \\big(\\mathcal{J}_{\\mu}(w_0) - \\min_{w}\\mathcal{J}_{\\mu}(w)\\big).\n$$\n\n实现要求。\n- 使用 $D = 30$ 个特征，$N = 5000$ 个蒙特卡洛内部样本，以及一个固定的随机种子 $0$ 来生成特征和样本。\n- 对于所有线性求解和特征值计算，使用对称正定的岭正则化矩阵 $H_{\\mu}^{\\epsilon} = H_{\\mu} + \\epsilon I$，其中 $\\epsilon = 10^{-8}$。\n- 通过设置以下方式选择一个单一的步长 $\\eta$，以保证对于评估集中的每一个 $\\mu$ 都满足 $\\eta \\le 1/L_{\\mu}$\n$$\n\\eta \\;=\\; \\frac{9}{10}\\,\\frac{1}{\\max_{\\mu\\in\\mathcal{M}_{\\mathrm{all}}} L_{\\mu}},\\quad L_{\\mu} = \\lambda_{\\max}(H_{\\mu}^{\\epsilon}),\n$$\n其中 $\\mathcal{M}_{\\mathrm{all}} = \\{\\, 0.0,\\; 0.3,\\; 0.4,\\; 0.6,\\; 0.7,\\; 0.8,\\; 0.9 \\,\\}$ 涵盖了下面使用的所有训练和测试参数。\n\n测试套件。您的程序必须按此顺序计算以下四个布尔量：\n- 情况 A（摊销界，一步）：对于 $\\mu = 0.0$ 和 $T = 1$，使用 $w_0 = w_{\\mathrm{meta}}$，验证实际差距是否小于或等于界限：\n$$\n\\mathcal{J}_{\\mu}(w_T) - \\min_w \\mathcal{J}_{\\mu}(w) \\;\\le\\; \\rho_{\\mu}(\\eta)^{2T}\\,\\big(\\mathcal{J}_{\\mu}(w_0) - \\min_w \\mathcal{J}_{\\mu}(w)\\big).\n$$\n- 情况 B（摊销界，多步）：对于 $\\mu = 0.7$ 和 $T = 5$，使用 $w_0 = w_{\\mathrm{meta}}$，验证与情况 A 中相同的界限。\n- 情况 C（元初始化改善平均能量）：检查元初始化相对于零初始化是否改善了平均训练能量，\n$$\n\\overline{\\mathcal{J}}(w_{\\mathrm{meta}})  \\overline{\\mathcal{J}}(0).\n$$\n- 情况 D（平均一步自适应后的收益）：在评估集 $\\{\\, 0.0,\\; 0.3,\\; 0.6,\\; 0.9 \\,\\}$上，比较从 $w_{\\mathrm{meta}}$ 出发和从零向量出发进行一步梯度下降后的平均能量：\n$$\n\\frac{1}{4}\\sum_{\\mu\\in\\{0.0,0.3,0.6,0.9\\}} \\mathcal{J}_{\\mu}\\big(\\,w_{\\mathrm{meta}} - \\eta\\,(H_{\\mu} w_{\\mathrm{meta}} - h_{\\mu})\\,\\big)  \\frac{1}{4}\\sum_{\\mu\\in\\{0.0,0.3,0.6,0.9\\}} \\mathcal{J}_{\\mu}\\big(\\,- \\eta\\,(- h_{\\mu})\\,\\big).\n$$\n\n数值细节。\n- 您必须通过对 $a_{\\mu}$ 和 $u^{\\star}_{\\mu}$ 求导来解析地计算 $f_{\\mu}$。使用恒等式 $\\tanh'(z) = 1 - \\tanh^2(z)$ 和乘法法则来计算 $J_s$ 的条目。\n- 在验证不等式时，允许一个非负容差 $\\tau = 10^{-10}$，并将“$\\le$”视为“$\\le +\\tau$”。\n\n最终输出格式。您的程序应产生单行输出，其中包含一个 Python 风格的列表，按顺序包含情况 A–D 的四个布尔结果，例如，“[True,False,True,True]”。不应打印任何其他文本。", "solution": "用户定义的问题是一个适定的数值练习，旨在使用深度 Ritz 方法的一个特定变体来求解一个参数化偏微分方程（PDE）。该方法结合了变分法、数值线性代数和元学习的概念。该问题具有科学依据、内部一致，并为获得唯一解提供了所有必要的数据和定义。\n\n### 第 1 步：解析推导\n\n首先，我们必须推导出实现所需的解析表达式。\n\n**1.1. 源项 $f_{\\mu}(x,y)$**\n\n源项 $f_{\\mu}$ 由关系 $f_{\\mu} = -\\nabla\\cdot(a_{\\mu}\\nabla u^{\\star}_{\\mu})$ 定义，其中 $u^{\\star}_{\\mu}$ 是精确解。使用向量微积分恒等式 $\\nabla \\cdot (\\phi \\mathbf{F}) = (\\nabla \\phi) \\cdot \\mathbf{F} + \\phi (\\nabla \\cdot \\mathbf{F})$，我们得到 $f_{\\mu} = -(\\nabla a_{\\mu} \\cdot \\nabla u^{\\star}_{\\mu} + a_{\\mu} \\Delta u^{\\star}_{\\mu})$。我们计算每个分量：\n\n- 系数 $a_{\\mu}(x,y) = 1 + \\frac{1}{2}\\mu\\sin(2\\pi x)\\sin(2\\pi y)$。其梯度为：\n$$\n\\nabla a_{\\mu} = \\pi\\mu \\begin{pmatrix} \\cos(2\\pi x)\\sin(2\\pi y) \\\\ \\sin(2\\pi x)\\cos(2\\pi y) \\end{pmatrix}.\n$$\n\n- 精确解为 $u^{\\star}_{\\mu}(x,y) = \\sin(\\pi x)\\sin(\\pi y) + \\mu\\sin(2\\pi x)\\sin(\\pi y)$。其梯度为：\n$$\n\\nabla u^{\\star}_{\\mu} = \\begin{pmatrix} \\pi\\cos(\\pi x)\\sin(\\pi y) + 2\\pi\\mu\\cos(2\\pi x)\\sin(\\pi y) \\\\ \\pi\\sin(\\pi x)\\cos(\\pi y) + \\pi\\mu\\sin(2\\pi x)\\cos(\\pi y) \\end{pmatrix}.\n$$\n\n- 精确解的拉普拉斯算子为 $\\Delta u^{\\star}_{\\mu} = \\frac{\\partial^2 u^{\\star}_{\\mu}}{\\partial x^2} + \\frac{\\partial^2 u^{\\star}_{\\mu}}{\\partial y^2}$：\n$$\n\\frac{\\partial^2 u^{\\star}_{\\mu}}{\\partial x^2} = -\\pi^2\\sin(\\pi x)\\sin(\\pi y) - 4\\pi^2\\mu\\sin(2\\pi x)\\sin(\\pi y) \\\\\n\\frac{\\partial^2 u^{\\star}_{\\mu}}{\\partial y^2} = -\\pi^2\\sin(\\pi x)\\sin(\\pi y) - \\pi^2\\mu\\sin(2\\pi x)\\sin(\\pi y) \\\\\n\\Delta u^{\\star}_{\\mu} = -2\\pi^2\\sin(\\pi x)\\sin(\\pi y) - 5\\pi^2\\mu\\sin(2\\pi x)\\sin(\\pi y).\n$$\n\n结合这些项可得到 $f_{\\mu}(x,y)$ 的完整表达式，该表达式将直接实现。\n\n**1.2. 试验基函数的雅可比矩阵**\n\n试验函数为 $u_w(x,y) = \\sum_{i=1}^{D} w_i \\psi_i(x,y)$，其中基函数为 $\\psi_i(x,y) = b(x,y)\\varphi_i(x,y)$。在样本点 $(x_s, y_s)$ 处的雅可比矩阵 $J_s$ 的行由梯度 $\\nabla \\psi_i (x_s, y_s)$ 给出。使用乘法法则，$\\nabla \\psi_i = (\\nabla b)\\varphi_i + b(\\nabla \\varphi_i)$。\n\n- 边界因子：$b(x,y) = x(1-x)y(1-y)$。其梯度为：\n$$\n\\nabla b(x,y) = \\begin{pmatrix} (1-2x)y(1-y) \\\\ x(1-x)(1-2y) \\end{pmatrix}.\n$$\n\n- 特征映射：$\\varphi_i(x,y) = \\tanh(z_i(x,y))$，其中 $z_i(x,y) = W_{i,1}x + W_{i,2}y + c_i$。使用 $\\tanh'(z) = 1-\\tanh^2(z)$，其梯度为：\n$$\n\\nabla\\varphi_i(x,y) = (1-\\varphi_i(x,y)^2)\\nabla z_i = (1-\\varphi_i(x,y)^2)\\begin{pmatrix} W_{i,1} \\\\ W_{i,2} \\end{pmatrix}.\n$$\n\n因此，雅可比矩阵 $J_s \\in \\mathbb{R}^{D \\times 2}$ 的第 $i$ 行为：\n$$\n\\nabla\\psi_i(x_s, y_s) = \\varphi_i(x_s,y_s) \\nabla b(x_s, y_s) + b(x_s,y_s) (1-\\varphi_i(x_s,y_s)^2) \\begin{pmatrix} W_{i,1} \\\\ W_{i,2} \\end{pmatrix}.\n$$\n\n### 第 2 步：数值方案\n\n问题的核心是构造和操作二次能量泛函 $\\mathcal{J}_{\\mu}(w) = \\frac{1}{2}w^{\\top}H_{\\mu}w - h_{\\mu}^{\\top}w$。\n\n**2.1. $H_{\\mu}$ 和 $h_{\\mu}$ 的组装**\n\n矩阵 $H_{\\mu}$ 和向量 $h_{\\mu}$ 通过对从 $\\Omega=(0,1)^2$ 中均匀抽取的 $N=5000$ 个样本 $(x_s, y_s)$ 进行蒙特卡洛积分来组装：\n$$\nH_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} a_{\\mu}(x_s,y_s)\\; J_s J_s^{\\top},\\quad\nh_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} f_{\\mu}(x_s,y_s)\\; b(x_s,y_s)\\,\\varphi(x_s,y_s).\n$$\n使用固定的随机种子 $0$ 来生成神经网络参数 $(W, c)$ 和样本点。为确保稳定性，我们对线性求解和特征值计算使用正则化矩阵 $H_{\\mu}^{\\epsilon} = H_{\\mu} + \\epsilon I$，其中 $\\epsilon = 10^{-8}$。\n\n**2.2. 元初始化与自适应**\n\n- **元学习**：元初始化 $w_{\\mathrm{meta}}$ 是通过最小化训练集 $\\mathcal{M}_{\\mathrm{train}} = \\{0.0, 0.4, 0.8\\}$ 上的平均能量找到的。这个平均能量也是一个二次函数 $\\overline{\\mathcal{J}}(w) = \\frac{1}{2}w^{\\top}\\overline{H}w - \\overline{h}^{\\top}w$，其中 $\\overline{H}$ 和 $\\overline{h}$ 是 $H_{\\mu}$ 和 $h_{\\mu}$ 在 $\\mathcal{M}_{\\mathrm{train}}$ 上的平均值。最小化子是线性系统 $\\overline{H}^{\\epsilon}w = \\overline{h}$ 的解。\n\n- **自适应**：对于给定的任务 $\\mu$，解从一个初始猜测 $w_0$ 开始，使用梯度下降法进行精化。更新规则是 $w_{t+1} = w_t - \\eta \\nabla\\mathcal{J}_{\\mu}(w_t) = w_t - \\eta(H_{\\mu}w_t - h_{\\mu})$。步长 $\\eta$ 被保守地选择为 $\\eta = \\frac{9}{10} / L_{\\max}$，其中 $L_{\\max} = \\max_{\\mu \\in \\mathcal{M}_{\\mathrm{all}}} \\lambda_{\\max}(H_{\\mu}^{\\epsilon})$。\n\n### 第 3 步：评估测试用例\n\n四个布尔测试用例如下评估：\n\n- **情况 A 和 B（摊销界）**：我们验证不等式 $\\mathcal{J}_{\\mu}(w_T) - \\min \\mathcal{J}_{\\mu} \\le \\rho_{\\mu}(\\eta)^{2T}(\\mathcal{J}_{\\mu}(w_0) - \\min \\mathcal{J}_{\\mu}) + \\tau$。\n    - $w_0 = w_{\\mathrm{meta}}$。\n    - $w_T$ 是通过运行 $T$ 步梯度下降得到的。\n    - $\\min \\mathcal{J}_{\\mu}$ 由 $\\mathcal{J}_{\\mu}(w^*_{\\mu})$ 近似，其中 $w^*_{\\mu} = (H_{\\mu}^{\\epsilon})^{-1}h_{\\mu}$。\n    - 谱半径 $\\rho_{\\mu}(\\eta) = \\max_{\\lambda \\in \\mathrm{spec}(H_{\\mu}^{\\epsilon})} |1-\\eta\\lambda|$。\n\n- **情况 C（元初始化性能）**：我们检查元初始化处的平均能量是否低于原点处的能量：$\\overline{\\mathcal{J}}(w_{\\mathrm{meta}})  \\overline{\\mathcal{J}}(0)$。由于 $\\overline{\\mathcal{J}}(0)=0$ 且 $w_{\\mathrm{meta}}$ 是最小化子，这应该成立。\n\n- **情况 D（平均自适应收益）**：我们在评估集 $\\{0.0, 0.3, 0.6, 0.9\\}$ 上，比较从 $w_{\\mathrm{meta}}$ 开始与从 $w_0=0$ 开始进行一步梯度下降后的平均能量。这测试了元初始化是否平均而言为自适应提供了一个更好的起点。\n\n以下代码实现了这整个过程。", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Implements a Deep Ritz solver with meta-initialization for a parameterized elliptic PDE.\n    \"\"\"\n    # 1. Problem\n    # Constants\n    D = 30\n    N = 5000\n    epsilon = 1e-8\n    seed = 0\n    tau = 1e-10\n\n    # Parameter sets\n    M_train = [0.0, 0.4, 0.8]\n    M_all = [0.0, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n    M_eval_D = [0.0, 0.3, 0.6, 0.9]\n    \n    # 2. Setup\n    rng = np.random.default_rng(seed)\n    W = rng.standard_normal(size=(D, 2))\n    c = rng.standard_normal(size=D)\n    samples = rng.uniform(0, 1, size=(N, 2))\n\n    # 3. Analytical Functions\n    pi = np.pi\n\n    def a_mu_func(x, y, mu):\n        return 1.0 + 0.5 * mu * np.sin(2 * pi * x) * np.sin(2 * pi * y)\n\n    def f_mu_func(x, y, mu):\n        # Derivatives of u_star\n        grad_u_star_x = pi*np.sin(pi*y)*(np.cos(pi*x) + 2*mu*np.cos(2*pi*x))\n        grad_u_star_y = pi*np.cos(pi*y)*(np.sin(pi*x) + mu*np.sin(2*pi*x))\n        grad_u_star = np.array([grad_u_star_x, grad_u_star_y])\n\n        delta_u_star = -pi**2*np.sin(pi*y)*(2*np.sin(pi*x) + 5*mu*np.sin(2*pi*x))\n        \n        # Derivatives of a_mu\n        a_mu_val = a_mu_func(x, y, mu)\n        grad_a_mu_x = pi*mu*np.cos(2*pi*x)*np.sin(2*pi*y)\n        grad_a_mu_y = pi*mu*np.sin(2*pi*x)*np.cos(2*pi*y)\n        grad_a_mu = np.array([grad_a_mu_x, grad_a_mu_y])\n\n        return -(np.dot(grad_a_mu, grad_u_star) + a_mu_val * delta_u_star)\n\n    # 4. Assembly of H and h\n    def assemble_H_h(mu, samples, W, c):\n        H = np.zeros((D, D))\n        h_vec = np.zeros(D)\n        \n        for s in range(N):\n            x, y = samples[s, 0], samples[s, 1]\n            \n            a_val = a_mu_func(x, y, mu)\n            f_val = f_mu_func(x, y, mu)\n            b_val = x * (1 - x) * y * (1 - y)\n            \n            z = W @ np.array([x, y]) + c\n            phi = np.tanh(z)\n            \n            grad_b = np.array([(1-2*x)*y*(1-y), x*(1-x)*(1-2*y)])\n            grad_phi_factor = 1 - phi**2\n            \n            J_s = np.outer(phi, grad_b) + b_val * (grad_phi_factor[:, np.newaxis] * W)\n            \n            H += a_val * (J_s @ J_s.T)\n            h_vec += f_val * b_val * phi\n        \n        return H / N, h_vec / N\n\n    # 5. Pre-computation for all mu\n    task_data = {}\n    for mu in M_all:\n        H, h = assemble_H_h(mu, samples, W, c)\n        H_eps = H + epsilon * np.eye(D)\n        task_data[mu] = {'H': H, 'h': h, 'H_eps': H_eps}\n\n    # 6. Compute stepsize eta\n    L_mus = [linalg.eigh(task_data[mu]['H_eps'], eigvals_only=True)[-1] for mu in M_all]\n    eta = 0.9 / np.max(L_mus)\n\n    # 7. Compute Meta-Initialization w_meta\n    H_bar = np.mean([task_data[mu]['H'] for mu in M_train], axis=0)\n    h_bar = np.mean([task_data[mu]['h'] for mu in M_train], axis=0)\n    H_bar_eps = H_bar + epsilon * np.eye(D)\n    w_meta = linalg.solve(H_bar_eps, h_bar, assume_a='pos')\n\n    # 8. Helper functions for evaluation\n    def compute_energy(w, H, h):\n        return 0.5 * w.T @ H @ w - h.T @ w\n\n    # 9. Evaluate Test Cases\n    results = []\n\n    # Case A: Amortization bound, T=1\n    mu_A = 0.0\n    T_A = 1\n    w0_A = w_meta\n    H_A, h_A, H_eps_A = task_data[mu_A]['H'], task_data[mu_A]['h'], task_data[mu_A]['H_eps']\n    w_T_A = w0_A - eta * (H_A @ w0_A - h_A)\n    w_opt_A = linalg.solve(H_eps_A, h_A, assume_a='pos')\n    E_min_A = compute_energy(w_opt_A, H_A, h_A)\n    E_0_A = compute_energy(w0_A, H_A, h_A)\n    E_T_A = compute_energy(w_T_A, H_A, h_A)\n    actual_gap_A = E_T_A - E_min_A\n    initial_gap_A = E_0_A - E_min_A\n    eigvals_A = linalg.eigh(H_eps_A, eigvals_only=True)\n    rho_mu_A = np.max(np.abs(1 - eta * eigvals_A))\n    bound_A = (rho_mu_A**(2 * T_A)) * initial_gap_A\n    results.append(actual_gap_A = bound_A + tau)\n\n    # Case B: Amortization bound, T=5\n    mu_B = 0.7\n    T_B = 5\n    w_t_B = w_meta\n    H_B, h_B, H_eps_B = task_data[mu_B]['H'], task_data[mu_B]['h'], task_data[mu_B]['H_eps']\n    for _ in range(T_B):\n        w_t_B = w_t_B - eta * (H_B @ w_t_B - h_B)\n    w_T_B = w_t_B\n    w_opt_B = linalg.solve(H_eps_B, h_B, assume_a='pos')\n    E_min_B = compute_energy(w_opt_B, H_B, h_B)\n    E_0_B = compute_energy(w_meta, H_B, h_B)\n    E_T_B = compute_energy(w_T_B, H_B, h_B)\n    actual_gap_B = E_T_B - E_min_B\n    initial_gap_B = E_0_B - E_min_B\n    eigvals_B = linalg.eigh(H_eps_B, eigvals_only=True)\n    rho_mu_B = np.max(np.abs(1 - eta * eigvals_B))\n    bound_B = (rho_mu_B**(2 * T_B)) * initial_gap_B\n    results.append(actual_gap_B = bound_B + tau)\n\n    # Case C: Meta-initialization improves average energy\n    J_bar_w_meta = compute_energy(w_meta, H_bar, h_bar)\n    J_bar_0 = compute_energy(np.zeros(D), H_bar, h_bar)\n    results.append(J_bar_w_meta  J_bar_0)\n\n    # Case D: Benefit after one adaptation step on average\n    E_meta_avg = 0\n    E_zero_avg = 0\n    for mu_D in M_eval_D:\n        H_D, h_D = task_data[mu_D]['H'], task_data[mu_D]['h']\n        w1_meta = w_meta - eta * (H_D @ w_meta - h_D)\n        E_meta_avg += compute_energy(w1_meta, H_D, h_D)\n        w1_zero = eta * h_D\n        E_zero_avg += compute_energy(w1_zero, H_D, h_D)\n    results.append((E_meta_avg / len(M_eval_D))  (E_zero_avg / len(M_eval_D)))\n\n    # Final print statement\n    print(f\"[{','.join(map(str, [bool(r) for r in results]))}]\")\n\nsolve()\n```", "id": "3376733"}]}