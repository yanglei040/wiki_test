{"hands_on_practices": [{"introduction": "近似贝叶斯计算（Approximate Bayesian Computation, ABC）是一种基础的免似然推断方法，它完全依赖于模拟。其核心思想是通过从模型中生成模拟数据，并接受那些能够产生与观测数据“足够接近”的模拟结果的参数。这项练习 [@problem_id:3536607] 的价值在于，它引导我们为一个简单但具有代表性的泊松（Poisson）过程分析推导出后验均值，从而揭示ABC方法的内部机制。通过这个过程，你将从第一性原理出发，深入理解容忍度参数 $\\epsilon$、先验和模型本身是如何共同决定最终的推断结果的。", "problem": "考虑一个玩具高能物理（HEP）截面测量场景，其中总结统计量 $s(x)$ 是事件样本中选定喷注的数量。数据生成过程建模如下：总结统计量 $s(x)$ 是一个非负整数计数 $k$，从速率参数为 $\\theta$ 的泊松分布中抽取，即 $k \\sim \\mathrm{Poisson}(\\theta)$，其中 $\\theta \\ge 0$ 是未知的信号速率。假设 $\\theta \\in [0, \\infty)$ 的非正常均匀先验为 $p(\\theta) \\propto 1$。\n\n使用近似贝叶斯计算（ABC），将模拟数据 $k_{\\mathrm{sim}}$ 相对于观测计数 $k_{\\mathrm{obs}}$ 的接受准则定义为 $|k_{\\mathrm{sim}} - k_{\\mathrm{obs}}| \\le \\epsilon$，其中 $\\epsilon \\ge 0$ 是一个容差。ABC后验是 $\\theta$ 的分布，该分布是以模拟的总结统计量落在观测总结统计量的容差范围内的事件为条件得到的。\n\n从泊松模型和贝叶斯定理的基本定义出发，且不使用任何预先推导的目标公式，推导ABC后验均值 $\\mathbb{E}[\\theta \\mid |K - k_{\\mathrm{obs}}| \\le \\epsilon]$ 的闭合形式，其为观测计数 $k_{\\mathrm{obs}}$ 和容差 $\\epsilon$ 的函数。推导必须基于第一性原理以及泊松模型和伽马函数性质的有效积分变换。\n\n您的程序必须实现此推导出的表达式，以计算下面每个测试用例的ABC后验均值。程序中不允许进行任何模拟；结果必须使用您推导的闭合形式表达式计算。\n\n测试套件：\n- $(k_{\\mathrm{obs}}, \\epsilon) = (12, 0)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (0, 0.5)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (7, 1.0)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (5, 0.7)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (2, 3.0)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (1, 10.0)$\n\n所有量均为无量纲的计数或速率，因此不需要物理单位。您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3]$），其中每个 $r_i$ 是相应测试用例的ABC后验均值，表示为浮点数。", "solution": "问题要求对泊松过程的速率参数 $\\theta$ 的近似贝叶斯计算（ABC）后验均值进行闭合形式推导。推导必须从第一性原理出发。\n\n设 $k_{\\mathrm{obs}}$ 为观测到的非负整数计数，$\\epsilon \\ge 0$ 为容差。数据生成过程是泊松分布，$K \\sim \\mathrm{Poisson}(\\theta)$，其概率质量函数为 $P(K=k \\mid \\theta) = \\frac{e^{-\\theta}\\theta^k}{k!}$，其中 $k \\in \\{0, 1, 2, \\dots\\}$。未知参数 $\\theta$ 的先验是一个非正常均匀先验，$p(\\theta) \\propto 1$，其中 $\\theta \\in [0, \\infty)$。\n\nABC接受准则定义为 $|k_{\\mathrm{sim}} - k_{\\mathrm{obs}}| \\le \\epsilon$，其中 $k_{\\mathrm{sim}}$ 是从模型 $k_{\\mathrm{sim}} \\sim \\mathrm{Poisson}(\\theta)$ 中抽取的模拟数据点。设 $A$ 表示接受事件，$A = \\{ k_{\\mathrm{sim}} : |k_{\\mathrm{sim}} - k_{\\mathrm{obs}}| \\le \\epsilon \\}$。ABC后验分布 $p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}})$ 是在事件 $A$ 发生的条件下 $\\theta$ 的精确后验分布。\n\n根据贝叶斯定理，后验与似然乘以先验成正比：\n$$p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\propto P(A \\mid \\theta) p(\\theta)$$\n\n先验给定为 $p(\\theta) \\propto 1$。$P(A \\mid \\theta)$ 项是“ABC似然”，即在给定 $\\theta$ 值的情况下，抽取的模拟数据点 $k_{\\mathrm{sim}}$ 满足接受准则的概率。这是通过对所有落在接受区域内的整数计数 $k$ 的泊松概率求和来计算的：\n$$P(A \\mid \\theta) = \\sum_{k \\in \\mathbb{Z}_{\\ge 0} \\text{ s.t. } |k - k_{\\mathrm{obs}}| \\le \\epsilon} P(K=k \\mid \\theta)$$\n\n条件 $|k - k_{\\mathrm{obs}}| \\le \\epsilon$ 等价于 $k_{\\mathrm{obs}} - \\epsilon \\le k \\le k_{\\mathrm{obs}} + \\epsilon$。由于 $k$ 必须是非负整数，因此接受的计数集合由以下整数边界定义：\n$$k_{\\min} = \\lceil \\max(0, k_{\\mathrm{obs}} - \\epsilon) \\rceil$$\n$$k_{\\max} = \\lfloor k_{\\mathrm{obs}} + \\epsilon \\rfloor$$\nABC似然现在可以写成在此范围内的和：\n$$P(A \\mid \\theta) = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{e^{-\\theta}\\theta^k}{k!}$$\n如果 $k_{\\min} > k_{\\max}$，则接受集为空，和为零。我们为推导假设 $k_{\\min} \\le k_{\\max}$。\n\n因此，未归一化的ABC后验为：\n$$p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\propto \\left( \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{e^{-\\theta}\\theta^k}{k!} \\right) \\cdot 1 = e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^k}{k!}$$\n\nABC后验均值 $\\mathbb{E}_{\\mathrm{ABC}}[\\theta]$ 是 $\\theta$ 关于此后验分布的期望：\n$$\\mathbb{E}_{\\mathrm{ABC}}[\\theta] = \\mathbb{E}[\\theta \\mid A] = \\frac{\\int_0^\\infty \\theta \\cdot p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\,d\\theta}{\\int_0^\\infty p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\,d\\theta}$$\n\n我们首先计算分母（归一化常数 $Z$）：\n$$Z = \\int_0^\\infty e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^k}{k!} \\,d\\theta$$\n由于和是有限的，我们可以交换求和与积分的顺序：\n$$Z = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{1}{k!} \\int_0^\\infty \\theta^k e^{-\\theta} \\,d\\theta$$\n该积分是伽马函数的定义，$\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t}\\,dt$。具体来说，$\\int_0^\\infty \\theta^k e^{-\\theta} \\,d\\theta = \\Gamma(k+1)$，对于整数 $k$，$\\Gamma(k+1) = k!$。\n将此结果代回 $Z$ 的表达式中：\n$$Z = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{1}{k!} (k!) = \\sum_{k=k_{\\min}}^{k_{\\max}} 1 = k_{\\max} - k_{\\min} + 1$$\n\n接下来，我们计算分子 $N$：\n$$N = \\int_0^\\infty \\theta \\left( e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^k}{k!} \\right) \\,d\\theta = \\int_0^\\infty e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^{k+1}}{k!} \\,d\\theta$$\n再次，我们交换求和与积分的顺序：\n$$N = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{1}{k!} \\int_0^\\infty \\theta^{k+1} e^{-\\theta} \\,d\\theta$$\n该积分为 $\\Gamma(k+2) = (k+1)!$。\n$$N = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{(k+1)!}{k!} = \\sum_{k=k_{\\min}}^{k_{\\max}} (k+1)$$\n这个和是一个等差数列。它的值可以通过对 $k=k_{\\min}$ 到 $k_{\\max}$ 的两项求和得到：\n$$N = \\left(\\sum_{k=k_{\\min}}^{k_{\\max}} k\\right) + \\left(\\sum_{k=k_{\\min}}^{k_{\\max}} 1\\right)$$\n第一项是等差数列的和，即项数乘以首项和末项的平均值：$\\frac{(k_{\\max} - k_{\\min} + 1)(k_{\\min} + k_{\\max})}{2}$。第二项就是 $k_{\\max} - k_{\\min} + 1$。\n$$N = \\frac{(k_{\\max} - k_{\\min} + 1)(k_{\\min} + k_{\\max})}{2} + (k_{\\max} - k_{\\min} + 1)$$\n\n最后，我们通过将 $N$ 除以 $Z$ 来计算ABC后验均值：\n$$\\mathbb{E}_{\\mathrm{ABC}}[\\theta] = \\frac{N}{Z} = \\frac{\\frac{(k_{\\max} - k_{\\min} + 1)(k_{\\min} + k_{\\max})}{2} + (k_{\\max} - k_{\\min} + 1)}{k_{\\max} - k_{\\min} + 1}$$\n$$\\mathbb{E}_{\\mathrm{ABC}}[\\theta] = \\frac{k_{\\min} + k_{\\max}}{2} + 1$$\n\n这就是ABC后验均值的最终闭合形式表达式。它是最小和最大接受整数计数的算术平均值加一。这个表达式将用于解决该问题。\n所需的量为：\n$$k_{\\min} = \\lceil \\max(0, k_{\\mathrm{obs}} - \\epsilon) \\rceil$$\n$$k_{\\max} = \\lfloor k_{\\mathrm{obs}} + \\epsilon \\rfloor$$\n$$\\mathbb{E}[\\theta \\mid |K - k_{\\mathrm{obs}}| \\le \\epsilon] = \\frac{k_{\\min} + k_{\\max}}{2} + 1$$\n此推导完全基于所要求的第一性原理。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma # Not used in final formula but part of environment.\n\ndef solve():\n    \"\"\"\n    Computes the ABC posterior mean for a series of test cases based on the\n    analytically derived closed-form expression.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (12, 0),\n        (0, 0.5),\n        (7, 1.0),\n        (5, 0.7),\n        (2, 3.0),\n        (1, 10.0),\n    ]\n\n    results = []\n    for k_obs, epsilon in test_cases:\n        # The derivation provides a closed-form solution for the ABC posterior mean.\n        # The formula is E[theta] = (k_min + k_max) / 2 + 1.\n        \n        # First, determine the bounds of the accepted integer counts k.\n        # The acceptance condition is |k - k_obs| = epsilon, which is equivalent to\n        # k_obs - epsilon = k = k_obs + epsilon.\n        # Since k must be a non-negative integer, we have:\n        \n        # k_min is the smallest integer k >= 0 such that k >= k_obs - epsilon.\n        # This is found by taking the maximum of 0 and k_obs - epsilon, and then\n        # taking the ceiling to get the next integer.\n        k_min = int(np.ceil(np.maximum(0, k_obs - epsilon)))\n        \n        # k_max is the largest integer k such that k = k_obs + epsilon.\n        # This is found by taking the floor.\n        k_max = int(np.floor(k_obs + epsilon))\n        \n        # The derived formula for the ABC posterior mean.\n        # If k_min > k_max, the acceptance set is empty, and the posterior would be undefined.\n        posterior_mean = (k_min + k_max) / 2.0 + 1.0\n        \n        results.append(posterior_mean)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3536607"}, {"introduction": "在掌握了ABC的基础之上，我们转向现代的基于模拟的推断（Simulation-Based Inference, SBI）范式，其中机器学习分类器扮演了核心角色。一个最优分类器的输出与似然比（likelihood ratio）之间存在着深刻的联系，而似然比是进行统计推断的关键量。这项练习 [@problem_id:3536670] 通过在一个简单的正态（Gaussian）模型中解析地推导这一关系，清晰地揭示了这一联系。完成这项练习将为理解诸如似然比估计（Likelihood Ratio Estimation, LRE）等现代SBI方法为何有效提供坚实的直觉基础。", "problem": "考虑在计算高能物理学中作为基于模拟的推断之基础的二元参数推断设定，其中单个观测值 $x \\in \\mathbb{R}$ 是从一个单位方差和未知均值参数 $\\theta \\in \\{\\theta_{0}, \\theta_{1}\\}$ 的正态分布中生成的。具体来说，假设生成模型为 $x \\sim \\mathcal{N}(\\theta, 1)$，其类先验概率为 $\\pi_{0} = \\mathbb{P}(\\theta = \\theta_{0})$ 和 $\\pi_{1} = \\mathbb{P}(\\theta = \\theta_{1})$，满足 $\\pi_{0} + \\pi_{1} = 1$ 且 $\\pi_{0}, \\pi_{1} \\in (0,1)$。在无似然范式中，为区分 $\\theta_{0}$ 和 $\\theta_{1}$ 而在交叉熵损失下训练的贝叶斯最优逻辑分类器收敛到真实后验概率 $s(x) = \\mathbb{P}(\\theta = \\theta_{1} \\mid x)$，其 logit 定义为 $\\ell(x) = \\ln\\!\\big( s(x) / \\big(1 - s(x)\\big) \\big)$。从第一性原理出发，即贝叶斯定理和正态分布的概率密度函数定义，推导最优逻辑分类器的 logit $\\ell(x)$ 的解析形式，使其成为一个关于 $x$ 的显式线性函数，并利用此推导来建立它与竞争假设 $\\theta_{1}$ 和 $\\theta_{0}$ 之间的精确似然比的映射关系。将您的最终答案表示为关于 $x$、$\\theta_{0}$、$\\theta_{1}$、$\\pi_{0}$ 和 $\\pi_{1}$ 的 $\\ell(x)$ 的单个闭式解析表达式。不需要进行数值近似；请勿引入任何单位。最终答案必须仅为单个表达式。 [@problem_id:44]", "solution": "该场景为二元参数推断，其中有一个单标量观测值，以及关于单位方差正态分布均值的两个竞争假设。其基本原理是贝叶斯定理和正态分布的概率密度函数。我们首先根据似然和先验建立贝叶斯最优分类器及其 logit，然后计算正态模型的显式形式。\n\n根据贝叶斯定理，给定 $x$ 时参数等于 $\\theta_{1}$ 的后验概率为\n$$\n\\mathbb{P}(\\theta = \\theta_{1} \\mid x)\n= \\frac{\\pi_{1} \\, p(x \\mid \\theta_{1})}{\\pi_{1} \\, p(x \\mid \\theta_{1}) + \\pi_{0} \\, p(x \\mid \\theta_{0})} \\, ,\n$$\n其中 $p(x \\mid \\theta)$ 表示正态模型下的似然。定义逻辑分类器的输出 $s(x) = \\mathbb{P}(\\theta = \\theta_{1} \\mid x)$ 及其 logit\n$$\n\\ell(x) = \\ln\\!\\left(\\frac{s(x)}{1 - s(x)}\\right) = \\ln\\!\\left(\\frac{\\mathbb{P}(\\theta = \\theta_{1} \\mid x)}{\\mathbb{P}(\\theta = \\theta_{0} \\mid x)}\\right) \\, .\n$$\n使用贝叶斯定理计算几率（odds），\n$$\n\\frac{\\mathbb{P}(\\theta = \\theta_{1} \\mid x)}{\\mathbb{P}(\\theta = \\theta_{0} \\mid x)}\n= \\frac{\\pi_{1} \\, p(x \\mid \\theta_{1})}{\\pi_{0} \\, p(x \\mid \\theta_{0})} \\, ,\n$$\n所以 logit 为\n$$\n\\ell(x) = \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) + \\ln\\!\\left(\\frac{p(x \\mid \\theta_{1})}{p(x \\mid \\theta_{0})}\\right) \\, .\n$$\n这通过以下方式展示了与精确似然比 $\\Lambda(x) = \\frac{p(x \\mid \\theta_{1})}{p(x \\mid \\theta_{0})}$ 的映射关系：\n$$\n\\ell(x) = \\ln \\Lambda(x) + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) \\, .\n$$\n\n现在我们计算正态模型 $x \\sim \\mathcal{N}(\\theta, 1)$ 下的 $\\ln \\Lambda(x)$。其密度为\n$$\np(x \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left( -\\frac{(x - \\theta)^{2}}{2} \\right) \\, .\n$$\n因此，\n$$\n\\ln \\Lambda(x) = \\ln p(x \\mid \\theta_{1}) - \\ln p(x \\mid \\theta_{0})\n= -\\frac{(x - \\theta_{1})^{2}}{2} + \\frac{(x - \\theta_{0})^{2}}{2} \\, ,\n$$\n因为 $\\ln\\!\\left( \\frac{1}{\\sqrt{2\\pi}} \\right)$ 项相互抵消。展开并化简：\n$$\n-\\frac{(x - \\theta_{1})^{2}}{2} + \\frac{(x - \\theta_{0})^{2}}{2}\n= -\\frac{x^{2} - 2 x \\theta_{1} + \\theta_{1}^{2}}{2} + \\frac{x^{2} - 2 x \\theta_{0} + \\theta_{0}^{2}}{2} \\, ,\n$$\n$$\n= \\left( -\\frac{x^{2}}{2} + x \\theta_{1} - \\frac{\\theta_{1}^{2}}{2} \\right)\n+ \\left( \\frac{x^{2}}{2} - x \\theta_{0} + \\frac{\\theta_{0}^{2}}{2} \\right) \\, ,\n$$\n$$\n= x (\\theta_{1} - \\theta_{0}) - \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} \\, .\n$$\n因此，最优 logit 是\n$$\n\\ell(x) = \\left[ x (\\theta_{1} - \\theta_{0}) - \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} \\right] + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) \\, .\n$$\n\n该表达式是关于 $x$ 的线性函数，由一个斜率 $(\\theta_{1} - \\theta_{0})$ 和一个截距 $- \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right)$ 组成。与似然比的映射关系是明确的：\n$$\n\\ell(x) = \\ln \\Lambda(x) + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) \\quad \\text{其中} \\quad \\ln \\Lambda(x) = x (\\theta_{1} - \\theta_{0}) - \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} \\, .\n$$\n因此，贝叶斯最优逻辑分类器 $s(x)$ 是 $s(x) = \\sigma(\\ell(x))$，其中 $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$，而所要求的 logit 的最终解析形式如上所示。", "answer": "$$\\boxed{x\\left(\\theta_{1}-\\theta_{0}\\right)-\\frac{\\theta_{1}^{2}-\\theta_{0}^{2}}{2}+\\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right)}$$", "id": "3536670"}, {"introduction": "在物理学分析中，一个常见的挑战是处理由测量过程中的模糊性（ambiguities）引起的复杂、多峰的后验分布。归一化流（Normalizing Flows）作为一种强大的神经密度估计器，能够灵活地捕捉这类复杂的分布结构。这项动手实践的编程练习 [@problem_id:3536610] 要求你构建一个混合流模型（mixture-of-flows model）来拟合一个双峰后验。通过这个过程，你将获得实现变量变换定理（change-of-variables formula）和处理数值稳定性问题的实践经验，这些都是在研究中应用先进SBI方法的关键技能。", "problem": "您将处理一个受计算高能物理中双重探测器模糊性启发的玩具设定，其中一个观测到的汇总统计量会引起一个潜在运动学参数的双峰后验分布。为了以无似然的方式近似此后验分布，您必须构建一个一维的流混合模型，并在一组合成测试点上评估其对数似然。\n\n从以下基本原理开始：\n\n- 贝叶斯推断使用贝叶斯定理，即 $p(\\theta \\mid s) \\propto p(s \\mid \\theta)\\,p(\\theta)$，其中 $s$ 是观测到的汇总统计量，$\\theta$ 是一个潜在参数。在模糊的探测器映射中（例如双重符号模糊性），$p(\\theta \\mid s)$ 通常是多峰的。\n- 对于可逆变换，变量替换定理指出，如果对于一个可逆、可微的函数 $f$，有 $x = f(z)$，且 $z$ 的密度为 $p_Z(z)$，那么在 $x$ 上的诱导密度为 $p_X(x) = p_Z(f^{-1}(x))\\,\\left\\lvert \\dfrac{d f^{-1}(x)}{d x} \\right\\rvert$。\n\n将双峰后验建模为两个一维归一化流的混合，每个流将一个基础高斯分布 $z \\sim \\mathcal{N}(0,\\sigma^2)$ 变换为一个分量密度。第 $k$ 个流定义为 $x = f_k(z)$，其中\n$$\nf_k(z) = a_k\\,z + b_k + c_k\\,\\tanh(d_k\\,z),\n$$\n其中所有参数均为实数，且其选择应确保 $f_k$ 是严格递增的（因此是可逆的）。流混合后验近似为\n$$\np(x) = \\sum_{k=1}^{2} w_k\\,p_k(x),\n$$\n其中混合权重 $w_k  0$ 满足 $\\sum_k w_k = 1$，$p_k(x)$ 是通过变量替换定理从基础高斯分布经由 $f_k$ 诱导出的密度。在点 $x$ 处的对数似然为 $\\log p(x)$。\n\n为保证计算稳定性，请使用一种数值稳定的求和策略来计算 $\\log p(x)$，该策略适用于 $\\log\\left(\\sum_k w_k \\exp(\\ell_k)\\right)$ 形式的项，其中 $\\ell_k = \\log p_k(x)$。\n\n使用以下科学上一致的参数化方法，通过设置两个对称的模态来编码双重模糊性：\n- 基础高斯方差 $\\sigma^2$ 且 $\\sigma = 1.0$。\n- 分量 $k=1$：$a_1 = 1.25$，$b_1 = +2.0$，$c_1 = 0.8$，$d_1 = 0.7$，权重 $w_1 = 0.5$。\n- 分量 $k=2$：$a_2 = 1.25$，$b_2 = -2.0$，$c_2 = 0.8$，$d_2 = 0.7$，权重 $w_2 = 0.5$。\n\n可逆性所需的严格单调性得到了保证，因为导数\n$$\n\\frac{d f_k}{d z}(z) = a_k + c_k\\,d_k\\,\\operatorname{sech}^2(d_k\\,z)\n$$\n在给定上述参数值的情况下，对于所有的 $z$ 都满足 $\\frac{d f_k}{d z}(z) \\ge a_k  0$。\n\n为 $f_k$ 实现一个数值求逆程序，以在任意 $x$ 处获得 $z = f_k^{-1}(x)$，需使用一种适用于严格单调函数的稳健的区间套用和二分法。\n\n测试套件：\n在以下合成测试点上评估对数似然 $\\log p(x)$，这些测试点用于探究不同的定性区域：\n- 模态附近的理想路径：$x = -2.0$, $x = +2.0$。\n- 模态之间的谷底：$x = 0.0$。\n- 适度的尾部：$x = -4.0$, $x = +4.0$。\n- 远尾部（边缘情况）：$x = +9.0$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[result_1,result_2,\\dots]$）。每个结果必须是一个浮点数，代表一个测试点的 $\\log p(x)$，并按上面列出的顺序排列。本问题不涉及物理单位；所有量均为无量纲实数。", "solution": "该问题是有效的。这是一个定义良好、有科学依据的计算统计学问题，与高能物理学中基于模拟的推断直接相关。所有必要的参数和函数形式都已提供，任务是实现一个数值稳健的算法来计算一个特定的量。\n\n目标是为一系列测试点 $x$ 计算对数似然 $\\log p(x)$，其中 $p(x)$ 是一个建模为两个一维归一化流混合的双峰概率密度函数。\n\n总体方法包括三个主要阶段：\n1.  推导单个流分量对数似然的解析表达式，即 $\\ell_k(x) = \\log p_k(x)$。\n2.  开发一种数值方法来计算流变换的逆 $z = f_k^{-1}(x)$，这是计算 $\\ell_k(x)$ 的必要输入。\n3.  使用数值稳定的方法，将两个分量的对数似然组合成混合模型的最终对数似然 $\\log p(x)$。\n\n**1. 分量对数似然**\n\n第 $k$ 个分量的密度 $p_k(x)$ 是通过对变换 $x = f_k(z)$ 应用变量替换定理获得的，其中基础变量 $z$ 服从高斯分布 $p_Z(z) = \\mathcal{N}(z \\mid 0, \\sigma^2)$。公式为：\n$$\np_k(x) = p_Z(f_k^{-1}(x)) \\left| \\frac{d f_k^{-1}(x)}{d x} \\right|\n$$\n使用反函数定理，雅可比行列式可以用正向变换 $f_k(z)$ 的导数来表示：\n$$\n\\left| \\frac{d f_k^{-1}(x)}{d x} \\right| = \\left| \\left( \\left. \\frac{d f_k(z)}{dz} \\right|_{z=f_k^{-1}(x)} \\right)^{-1} \\right| = \\left( \\frac{d f_k}{dz}(z) \\right)^{-1}\n$$\n绝对值可以去掉，因为问题指明 $f_k(z)$ 是严格递增的，这保证了其导数为正。\n\n令 $z = f_k^{-1}(x)$，则密度为：\n$$\np_k(x) = p_Z(z) \\left( \\frac{d f_k}{dz}(z) \\right)^{-1}\n$$\n为保证数值稳定性，我们使用对数似然 $\\ell_k(x) = \\log p_k(x)$：\n$$\n\\ell_k(x) = \\log p_Z(z) - \\log\\left(\\frac{d f_k}{dz}(z)\\right)\n$$\n基础分布是高斯分布，$p_Z(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right)$，所以其对数为：\n$$\n\\log p_Z(z) = -\\frac{z^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)\n$$\n变换 $f_k(z) = a_k z + b_k + c_k \\tanh(d_k z)$ 的导数由下式给出：\n$$\n\\frac{d f_k}{dz}(z) = a_k + c_k d_k \\operatorname{sech}^2(d_k z)\n$$\n代入这些表达式，得到分量对数似然的完整公式：\n$$\n\\ell_k(x) = -\\frac{(f_k^{-1}(x))^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2) - \\log\\left( a_k + c_k d_k \\operatorname{sech}^2(d_k f_k^{-1}(x)) \\right)\n$$\n该表达式依赖于 $z = f_k^{-1}(x)$ 的值，该值必须通过数值方法求得。\n\n**2. 流变换的数值求逆**\n\n为了对给定的 $x$ 计算 $\\ell_k(x)$，我们必须首先通过求解非线性方程 $x = f_k(z)$ 来计算 $z = f_k^{-1}(x)$。这等价于找到函数 $g_k(z; x) = f_k(z) - x = 0$ 的根。\n\n由于 $f_k(z)$ 是严格单调且连续的，其定义域和值域均为 $(-\\infty, \\infty)$，因此对于任何实数 $x$，都存在唯一的根 $z$。问题指定使用二分法，这是一种适用于单调函数的稳健求根算法。该算法需要一个包围根的初始区间 $[z_{low}, z_{high}]$，即在该区间两端，$g_k(z_{low}; x)$ 和 $g_k(z_{high}; x)$ 的符号相反。由于当 $|z|$ 很大时，线性项 $a_k z$ 占主导地位，因此一个足够大的固定区间（例如 $[-100, 100]$）可以可靠地包围所考虑的 $x$ 值范围内的根。二分法算法通过迭代地将区间减半，同时保持根被包围，从而以可预测的误差减小速度收敛到根。\n\n**3. 混合模型对数似然**\n\n总概率密度 $p(x)$ 是分量密度的加权和：\n$$\np(x) = \\sum_{k=1}^{2} w_k p_k(x)\n$$\n总对数似然为 $\\log p(x) = \\log\\left(\\sum_{k=1}^{2} w_k p_k(x)\\right)$。如果 $p_k(x)$ 的值非常大或非常小，对此和的朴素计算可能在数值上不稳定。为了避免浮点数上溢或下溢，我们使用 log-sum-exp 技巧。我们首先用分量对数似然 $\\ell_k(x)$ 来重写这个和：\n$$\n\\log p(x) = \\log\\left(\\sum_{k=1}^{2} w_k e^{\\ell_k(x)}\\right) = \\log\\left(\\sum_{k=1}^{2} e^{\\log w_k + \\ell_k(x)}\\right)\n$$\n令 $y_k = \\log w_k + \\ell_k(x)$。表达式变为 $\\log(\\sum_k e^{y_k})$。log-sum-exp 技巧是提出最大项：\n$$\n\\log\\left(\\sum_{k=1}^{2} e^{y_k}\\right) = y_{\\max} + \\log\\left(\\sum_{k=1}^{2} e^{y_k - y_{\\max}}\\right) = y_{\\max} + \\log\\left(e^{y_1 - y_{\\max}} + e^{y_2 - y_{\\max}}\\right)\n$$\n其中 $y_{\\max} = \\max(y_1, y_2)$。这种形式确保了指数函数的参数小于或等于0，从而防止了上溢和精度损失。\n\n**算法摘要**\n\n对于每个测试点 $x$：\n1.  初始化一个数组以保存项 $y_k = \\log w_k + \\ell_k(x)$。\n2.  对于每个分量 $k=1, 2$：\n    a. 使用二分法数值求解 $f_k(z) - x = 0$ 以找到 $z_k = f_k^{-1}(x)$。\n    b. 使用推导出的公式和 $z_k$ 计算分量对数似然 $\\ell_k(x)$。\n    c. 计算 $y_k = \\log(w_k) + \\ell_k(x)$。\n3.  对 $y_k$ 值的数组应用 log-sum-exp 函数，以计算最终的对数似然 $\\log p(x)$。\n4.  存储结果。\n\n此过程将对问题陈述中提供的所有测试点重复进行。", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the problem of evaluating the log-likelihood of a mixture-of-flows model\n    at a set of test points, as specified in the problem statement.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    # Constant for log(2*pi)\n    LOG_2PI = np.log(2 * np.pi)\n\n    # Base Gaussian standard deviation\n    SIGMA = 1.0\n\n    # Parameters for the two flow components\n    PARAMS = {\n        1: {'a': 1.25, 'b': 2.0, 'c': 0.8, 'd': 0.7, 'w': 0.5},\n        2: {'a': 1.25, 'b': -2.0, 'c': 0.8, 'd': 0.7, 'w': 0.5},\n    }\n\n    # Test points to evaluate\n    test_points = [-2.0, 2.0, 0.0, -4.0, 4.0, 9.0]\n\n    # --- Core Functions ---\n\n    def f_k(z, k):\n        \"\"\"The forward transformation f_k(z).\"\"\"\n        p = PARAMS[k]\n        return p['a'] * z + p['b'] + p['c'] * np.tanh(p['d'] * z)\n\n    def dfk_dz(z, k):\n        \"\"\"The derivative df_k/dz.\"\"\"\n        p = PARAMS[k]\n        # sech^2(x) = 1 / cosh^2(x)\n        sech_sq = 1.0 / np.cosh(p['d'] * z)**2\n        return p['a'] + p['c'] * p['d'] * sech_sq\n\n    def invert_fk(x, k, tol=1e-15, max_iter=100, z_bracket=(-100.0, 100.0)):\n        \"\"\"\n        Finds z = f_k^{-1}(x) by solving f_k(z) - x = 0 using bisection.\n        A wide, fixed bracket is sufficient for this monotonic function.\n        \"\"\"\n        z_low, z_high = z_bracket\n        g_low = f_k(z_low, k) - x\n        \n        # Assumption: The root is bracketed. For this problem's function and\n        # a wide bracket, this is a safe assumption.\n        \n        for _ in range(max_iter):\n            if (z_high - z_low)  tol:\n                break\n            \n            z_mid = (z_low + z_high) / 2.0\n            g_mid = f_k(z_mid, k) - x\n            \n            if np.sign(g_mid) == np.sign(g_low):\n                z_low = z_mid\n                g_low = g_mid\n            else:\n                z_high = z_mid\n                \n        return (z_low + z_high) / 2.0\n\n    def log_p_k(x, k):\n        \"\"\"Computes the log-likelihood log p_k(x) for a single component.\"\"\"\n        # Step 1: Numerically find z = f_k^{-1}(x)\n        z = invert_fk(x, k)\n        \n        # Step 2: Compute log of base density p_Z(z)\n        log_p_base = -0.5 * (z**2 / SIGMA**2 + LOG_2PI)\n        \n        # Step 3: Compute log of the Jacobian determinant of the inverse transform\n        # This is -log(derivative of the forward transform)\n        log_det_J_inv = -np.log(dfk_dz(z, k))\n        \n        # The component log-likelihood is the sum of these two terms\n        return log_p_base + log_det_J_inv\n\n    def log_p(x):\n        \"\"\"\n        Computes the total log-likelihood log p(x) for the mixture model\n        using the log-sum-exp trick for numerical stability.\n        \"\"\"\n        keys = sorted(PARAMS.keys())\n        \n        # Compute log(w_k) + log(p_k(x)) for each component\n        y_k = np.array([\n            np.log(PARAMS[k]['w']) + log_p_k(x, k) for k in keys\n        ])\n        \n        # Use scipy's logsumexp to compute log(sum(exp(y_k)))\n        return logsumexp(y_k)\n\n    # --- Main Execution ---\n    results = [log_p(x) for x in test_points]\n    \n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3536610"}]}