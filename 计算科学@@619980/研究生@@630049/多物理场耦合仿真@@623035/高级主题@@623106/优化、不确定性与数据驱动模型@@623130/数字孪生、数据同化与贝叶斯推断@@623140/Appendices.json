{"hands_on_practices": [{"introduction": "在开发可靠的数字孪生过程中，验证其数据同化组件是至关重要的一步。本实践将介绍秩统计直方图，这是一个强大的诊断工具，用于评估集合卡尔曼滤波器（Ensemble Kalman Filter, EnKF）的统计一致性。通过从第一性原理出发实施一个模拟（[@problem_id:3502567]），您将获得验证预测集合是否正确捕捉真实不确定性的实践经验，这是可信状态估计的基石。", "problem": "要求您构建一个独立的程序，为集合卡尔曼滤波器（EnKF）构建基于新息的秩直方图，并在关于模型和噪声规范的不同假设下评估其均匀性。其背景是一个多物理场数字孪生中的标量、线性观测模型，其中集合预报和贝叶斯推断被用于数据同化。推导和算法必须仅从基本概率原理、线性高斯模型定义以及独立同分布变量的可交换性定义出发。\n\n考虑以下设定。真实状态 $x$ 是一个标量随机变量，服从高斯先验 $x \\sim \\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}})$。观测算子是单位算子，因此观测到的量是 $y = x + \\varepsilon$，其中观测噪声 $\\varepsilon \\sim \\mathcal{N}(0, R_{\\text{true}})$，且与 $x$ 独立。集合卡尔曼滤波器（EnKF）产生一个预报集合 $\\{x_f^{(i)}\\}_{i=1}^m$，其中 $m$ 是集合大小。EnKF 使用一个假设的高斯预报分布 $x_f^{(i)} \\sim \\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}})$ 和一个用于扰动观测的假设观测噪声 $e^{(i)} \\sim \\mathcal{N}(0, R_{\\text{assumed}})$，所有这些在 $i$ 上都是独立的，并且与 $x$ 和 $\\varepsilon$ 独立。集合的预测观测值为 $y_f^{(i)} = x_f^{(i)} + e^{(i)}$。\n\n按如下方式定义基于新息的预测秩。对于每个同化周期，抽取一个 $y$ 的实现，抽取一个集合 $\\{y_f^{(i)}\\}_{i=1}^m$，并计算 $y$ 在 $m$ 个独立同分布的值 $\\{y_f^{(i)}\\}_{i=1}^m$ 中的秩 $r \\in \\{0,1,\\dots,m\\}$，即 $r$ 是严格小于 $y$ 的集合预测观测值的数量。在完美的模型和噪声调整下，变量 $\\{y, y_f^{(1)},\\dots,y_f^{(m)}\\}$ 是独立同分布的，并且秩 $r$ 期望在 $\\{0,\\dots,m\\}$ 上均匀分布。\n\n您的任务是：\n- 通过重复上述实验 $N$ 个独立的同化周期，并统计每个秩 $r \\in \\{0,\\dots,m\\}$ 的频率，来构建秩直方图。\n- 使用显著性水平为 $\\alpha$ 的卡方拟合优度检验来定量测试直方图的均匀性，其中原假设是直方图是均匀的，每个区间的概率等于 $1/(m+1)$。使用卡方统计量，并根据具有 $m$ 个自由度的参考分布计算 $p$ 值。\n- 对于测试套件中的每个测试用例，输出一个布尔值，指示在水平 $\\alpha$ 下是否接受均匀性的原假设。\n\n您必须从第一性原理出发解决此问题。推导和算法逻辑应从基本概率定律、高斯模型以及次序统计量和可交换性的性质开始，而不是从超出这些基础的任何预先指定的秩直方图公式开始。\n\n此问题中没有物理单位。不使用角度。所有概率必须表示为 $[0,1]$ 内的实数。\n\n要实现的测试套件：\n- 情况 A（完美调整，大集合和大样本量）：\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 1.0$, $R_{\\text{assumed}} = 1.0$\n- 情况 B（欠分散的预测集合和噪声）：\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 0.25$, $R_{\\text{assumed}} = 0.25$\n- 情况 C（过分散的预测集合和噪声）：\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 4.0$, $R_{\\text{assumed}} = 4.0$\n- 情况 D（有偏的预报均值）：\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.5$, $P_{\\text{assumed}} = 1.0$, $R_{\\text{assumed}} = 1.0$\n- 情况 E（小集合的边界情况）：\n  - $m = 3$, $N = 2000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 1.0$, $R_{\\text{assumed}} = 1.0$\n\n实现要求：\n- 对于每种情况，生成 $N$ 个独立的 $y$ 的抽样，并且每次抽样生成一个包含 $m$ 个成员的集合 $\\{y_f^{(i)}\\}_{i=1}^m$，计算秩 $r \\in \\{0,\\dots,m\\}$，并累积直方图计数。\n- 在 $m+1$ 个区间中，使用期望频率 $N/(m+1)$ 进行卡方拟合优度检验。使用得到的 $p$ 值来决定在水平 $\\alpha$ 下是接受还是拒绝均匀性。\n- 使用固定的随机种子，以便结果是可复现的。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，按情况 A、B、C、D、E 的顺序列出结果，例如 $[b_A,b_B,b_C,b_D,b_E]$，其中每个 $b_{\\cdot}$ 是字符串 $\\texttt{True}$ 或 $\\texttt{False}$，表示在该情况下是否接受水平 $\\alpha$ 的均匀性。\n\n您的程序必须是完整的，并且可以直接运行，无需任何用户输入或外部文件。", "solution": "该问题要求在各种模型和噪声规范情景下，为集合卡尔曼滤波器（EnKF）构建基于新息的秩直方图并进行统计验证。推导和实现将从概率论的第一性原理出发。\n\n首先，我们定义控制该问题的概率分布。“真实”状态 $x$ 是从高斯分布 $x \\sim \\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}})$ 中抽取的标量随机变量。通过一个单位观测算子和与 $x$ 无关的加性高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, R_{\\text{true}})$ 获得观测值 $y$。因此，$y = x + \\varepsilon$。作为两个独立高斯随机变量的和，$y$ 也是一个高斯随机变量。它的分布，我们可以称之为观测的“真实预测分布”，推导如下：\n均值为 $\\mathbb{E}[y] = \\mathbb{E}[x + \\varepsilon] = \\mathbb{E}[x] + \\mathbb{E}[\\varepsilon] = \\mu_{\\text{true}} + 0 = \\mu_{\\text{true}}$。\n方差为 $\\text{Var}(y) = \\text{Var}(x + \\varepsilon) = \\text{Var}(x) + \\text{Var}(\\varepsilon) = P_{\\text{true}} + R_{\\text{true}}$，这是由于 $x$ 和 $\\varepsilon$ 的独立性。\n因此，任何单个真实观测值 $y$ 都是从分布 $y \\sim \\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}} + R_{\\text{true}})$ 中抽取的样本。\n\n接下来，我们定义基于集合的预测观测值的分布。EnKF 使用一个假设的预报分布，从中抽取一个状态集合 $\\{x_f^{(i)}\\}_{i=1}^m$，其中每个成员 $x_f^{(i)} \\sim \\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}})$。观测值被假定受到来自 $\\mathcal{N}(0, R_{\\text{assumed}})$ 分布的噪声的污染。为了构建预测观测集合，每个预报状态 $x_f^{(i)}$ 都被来自这个假设噪声分布的随机抽样 $e^{(i)}$ 所扰动，$e^{(i)} \\sim \\mathcal{N}(0, R_{\\text{assumed}})$。得到的集合预测观测值为 $y_f^{(i)} = x_f^{(i)} + e^{(i)}$。\n与真实观测值类似，每个 $y_f^{(i)}$ 都是两个独立高斯变量的和。其分布推导如下：\n均值为 $\\mathbb{E}[y_f^{(i)}] = \\mathbb{E}[x_f^{(i)} + e^{(i)}] = \\mathbb{E}[x_f^{(i)}] + \\mathbb{E}[e^{(i)}] = \\mu_{\\text{assumed}} + 0 = \\mu_{\\text{assumed}}$。\n方差为 $\\text{Var}(y_f^{(i)}) = \\text{Var}(x_f^{(i)} + e^{(i)}) = \\text{Var}(x_f^{(i)}) + \\text{Var}(e^{(i)}) = P_{\\text{assumed}} + R_{\\text{assumed}}$。\n因此，对于 $i \\in \\{1, \\dots, m\\}$，每个集合预测观测值 $y_f^{(i)}$ 都是从分布 $y_f^{(i)} \\sim \\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}} + R_{\\text{assumed}})$ 中抽取的独立同分布（i.i.d.）样本。\n\n秩直方图背后的基本原理基于可交换性的概念。如果一组随机变量的联合概率分布在变量的任何排列下都保持不变，那么这组变量就是可交换的。一组独立同分布的随机变量总是可交换的。\n考虑 $m+1$ 个随机变量的集合 $\\{y, y_f^{(1)}, \\dots, y_f^{(m)}\\}$。如果滤波器被完美调整，观测的“真实预测分布”必须与“集合预测分布”相同。此条件要求：\n1. 均值相等：$\\mu_{\\text{true}} = \\mu_{\\text{assumed}}$。\n2. 方差相等：$P_{\\text{true}} + R_{\\text{true}} = P_{\\text{assumed}} + R_{\\text{assumed}}$。\n\n当这些条件满足时，所有 $m+1$ 个变量都是独立同分布的，因此是可交换的。对于任何这样一组独立同分布的连续随机变量，如果我们将它们排序，任何特定的变量（在本例中为 $y$）出现在排序后列表中的 $m+1$ 个可能位置中的任何一个位置的概率都是相等的。\n秩 $r$ 定义为严格小于观测值 $y$ 的集合成员 $y_f^{(i)}$ 的数量。这等同于说 $y$ 在组合集合 $\\{y, y_f^{(1)}, \\dots, y_f^{(m)}\\}$ 的排序列表中占据第 $(r+1)$ 个位置。因此，在完美模型的假设下，秩 $r$ 必须在整数 $\\{0, 1, \\dots, m\\}$ 上均匀分布。任何特定秩 $k$ 的概率为 $P(r=k) = \\frac{1}{m+1}$，其中 $k \\in \\{0, 1, \\dots, m\\}$。\n\n构建和检验秩直方图的算法如下：\n1. 初始化一个大小为 $m+1$ 的整数计数数组 `rank_counts` 为零。\n2. 对于 $N$ 个独立的同化周期中的每一个周期：\n    a. 从 $\\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}} + R_{\\text{true}})$ 中抽取一个单一的真实观测值 $y$。\n    b. 从 $\\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}} + R_{\\text{assumed}})$ 中抽取一个包含 $m$ 个预测观测值的集合 $\\{y_f^{(i)}\\}_{i=1}^m$。\n    c. 计算秩 $r = \\sum_{i=1}^{m} \\mathbb{I}(y_f^{(i)}  y)$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数，如果条件为真则为 $1$，否则为 $0$。秩 $r$ 将是一个介于 $0$ 和 $m$ 之间的整数。\n    d. 增加相应的计数器：`rank_counts[r] = rank_counts[r] + 1`。\n3. 经过 $N$ 个周期后，数组 `rank_counts` 包含了每个秩的观测频率 $\\{O_k\\}_{k=0}^m$。\n\n为了检验均匀性，我们采用卡方 ($\\chi^2$) 拟合优度检验。\n原假设 $H_0$ 是秩是均匀分布的。\n在 $H_0$ 下，每个秩区间 $k$ 的期望频率是 $E_k = N \\times P(r=k) = \\frac{N}{m+1}$。\n$\\chi^2$ 检验统计量计算如下：\n$$ \\chi^2 = \\sum_{k=0}^{m} \\frac{(O_k - E_k)^2}{E_k} $$\n将此统计量与卡方分布进行比较。自由度（$df$）是区间数减一，因为分布已由原假设完全指定。因此，$df = (m+1) - 1 = m$。\n$p$ 值是在假设 $H_0$ 为真的情况下，获得至少与观测到的检验统计量一样极端的检验统计量的概率。它计算为 $p = P(\\chi^2_{df=m} \\geq \\chi^2_{\\text{observed}}) = 1 - F_{\\chi^2_m}(\\chi^2_{\\text{observed}})$，其中 $F_{\\chi^2_m}$ 是具有 $m$ 个自由度的卡方分布的累积分布函数（CDF）。\n决策规则是：如果计算出的 $p$ 值小于指定的显著性水平 $\\alpha$，我们拒绝原假设 $H_0$。否则，我们不拒绝（即接受）$H_0$。如果 $p \\ge \\alpha$，检验结果为 `True`，如果 $p  \\alpha$，则为 `False`。\n\n将此完整过程应用于问题陈述中指定的每个测试用例。固定的随机种子确保了蒙特卡洛模拟的可复现性。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Constructs and validates innovation-based rank histograms for an EnKF.\n    \"\"\"\n    # A fixed random seed is used for reproducibility.\n    rng = np.random.default_rng(42)\n\n    # Test suite parameters for Cases A through E.\n    test_cases = [\n        {\n            \"name\": \"Case A\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 1.0, \"R_assumed\": 1.0\n        },\n        {\n            \"name\": \"Case B\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 0.25, \"R_assumed\": 0.25\n        },\n        {\n            \"name\": \"Case C\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 4.0, \"R_assumed\": 4.0\n        },\n        {\n            \"name\": \"Case D\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.5, \"P_assumed\": 1.0, \"R_assumed\": 1.0\n        },\n        {\n            \"name\": \"Case E\", \"m\": 3, \"N\": 2000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 1.0, \"R_assumed\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        m = case[\"m\"]\n        N = case[\"N\"]\n        alpha = case[\"alpha\"]\n\n        # Parameters for the \"true\" predictive observation distribution\n        mu_y_true = case[\"mu_true\"]\n        var_y_true = case[\"P_true\"] + case[\"R_true\"]\n        std_y_true = np.sqrt(var_y_true)\n\n        # Parameters for the \"ensemble\" predictive observation distribution\n        mu_y_assumed = case[\"mu_assumed\"]\n        var_y_assumed = case[\"P_assumed\"] + case[\"R_assumed\"]\n        std_y_assumed = np.sqrt(var_y_assumed)\n\n        # Initialize the histogram for ranks {0, 1, ..., m}\n        # rank_counts has m+1 bins\n        rank_counts = np.zeros(m + 1, dtype=int)\n\n        # Run N independent assimilation cycles\n        for _ in range(N):\n            # 1. Generate one \"true\" observation y\n            y_true = rng.normal(loc=mu_y_true, scale=std_y_true)\n\n            # 2. Generate an m-member ensemble of predictive observations {y_f}\n            y_f_ensemble = rng.normal(loc=mu_y_assumed, scale=std_y_assumed, size=m)\n\n            # 3. Compute the rank of y_true among the ensemble\n            # The rank is the number of ensemble members strictly less than y_true.\n            rank = np.sum(y_f_ensemble  y_true)\n            \n            # 4. Increment the count for the computed rank\n            rank_counts[rank] += 1\n        \n        # Perform the Chi-squared goodness-of-fit test for uniformity\n        \n        # The null hypothesis H0 is that the ranks are uniformly distributed.\n        # The expected count in each of the m+1 bins is N / (m+1).\n        expected_count = N / (m + 1)\n        \n        # Calculate the chi-squared statistic\n        # chi2_stat = sum_{k=0 to m} ( (observed_k - expected_k)^2 / expected_k )\n        chi2_stat = np.sum((rank_counts - expected_count)**2 / expected_count)\n\n        # The degrees of freedom is the number of bins minus 1.\n        # df = (m + 1) - 1 = m\n        df = m\n        \n        # Calculate the p-value.\n        # p_value = P(X^2_df >= chi2_stat), where X^2_df is a chi-squared random variable.\n        # This is 1 - CDF(chi2_stat).\n        p_value = 1.0 - chi2.cdf(chi2_stat, df)\n        \n        # Decision: Accept the null hypothesis (uniformity) if p_value >= alpha.\n        is_uniform = p_value >= alpha\n        results.append(is_uniform)\n\n    # Format the final output as a comma-separated list of booleans\n    # The output format must be exactly \"[True,False,False,False,True]\" (example)\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3502567"}, {"introduction": "现实世界中的传感器数据常常被异常值污染，这违背了许多滤波器中常见的正态噪声假设。本实践将挑战您使用学生$t$分布（Student-$t$）似然函数实现一个鲁棒的推断方案，该方案对极端数据点不那么敏感。通过在存在异常值的情况下，将其性能与标准正态模型进行比较（[@problem_id:3502607]），您将学会一种增强数字孪生鲁棒性和准确性的实用技术。", "problem": "考虑一个用于热-裂纹扩展的简化数字孪生，其中传感器记录的声发射振幅被建模为由多物理场耦合模拟器产生的潜在裂纹活动的线性图像。设传感器读数的正向模型为 $y_i = A s_i + \\varepsilon_i$，其中 $s_i$ 是由数字孪生预测的已知确定性信号，$A$ 是一个待推断的未知标量振幅参数。假设高斯先验 $A \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$。考虑两种观测模型：(i) 尺度参数为 $\\sigma$ 的正态（高斯）似然，以及 (ii) 自由度为 $\\nu$、尺度参数同为 $\\sigma$ 的学生t分布似然。学生t分布似然用于测试对重尾传感器噪声的鲁棒性。仅使用贝叶斯定理以及正态和学生t分布的定义作为唯一起点。除了从这些定义中可以推导出的形式外，不要假设任何闭合形式的后验。使用从学生t分布的尺度混合表示中推导出的迭代重加权最小二乘原理或等效的第一性原理论证来实现鲁棒推断。\n\n任务是计算两种似然下的最大后验（MAP）估计量 $A$，并通过将 MAP 误差与已知的基准真相进行比较来测试对异常值的鲁棒性。对于学生t模型，还要计算拉普拉斯近似曲率（MAP处负对数后验的海森值），并检查其正定性，这是有效局部高斯近似的必要条件。\n\n所有量都是无量纲的，所有输出都必须报告为无量纲值。\n\n数据和参数的定义如下。对于所有测试用例，先验参数和正向模型信号是固定的：\n- 先验均值 $\\mu_0 = 0.0$ 和先验方差 $\\tau_0^2 = 10.0$。\n- 噪声尺度 $\\sigma = 0.1$。\n- 基准真相振幅 $A_{\\text{true}} = 2.0$。\n- 时间索引 $i \\in \\{1,2,\\dots,25\\}$ 和正向信号 $s_i = \\exp(-0.05 i)\\left(1 + 0.5 \\sin(0.3 i)\\right)$。\n- 基线确定性伪噪声项 $n_i = 0.05 \\sin(0.7 i)$。\n\n每个测试用例的观测数据 $y_i$ 定义为 $y_i = A_{\\text{true}} s_i + n_i + o_i$，其中 $o_i$ 是每个测试中不同的异常值项。学生t分布的自由度参数 $\\nu$ 也因测试而异。\n\n测试套件：\n- 测试用例1（顺利路径）：无异常值，因此对所有 $i$ 都有 $o_i = 0$。学生t分布似然使用 $\\nu = 5$。\n- 测试用例2（重尾污染）：在索引 $i \\in \\{5,12,20\\}$ 处有异常值，分别为 $o_5 = 1.5$，$o_{12} = -2.0$，$o_{20} = 2.5$，其他情况下 $o_i = 0$。使用 $\\nu = 3$。\n- 测试用例3（极端异常值）：在索引 $i = 8$ 处有一个极端异常值 $o_8 = 6.0$，其他情况下 $o_i = 0$。使用 $\\nu = 1$。\n\n对于每个测试用例，执行以下操作：\n1. 仅使用第一性原理和尺度混合或等效推导来获得算法，计算学生t分布似然下的 MAP 估计量 $\\hat{A}_{t}$。计算在 $\\hat{A}_t$ 处的负对数后验的曲率 $H_t$（在这个一维参数问题中标量海森值）。报告一个布尔值，指示是否 $H_t > 0$。\n2. 仅使用第一性原理计算正态似然下的 MAP 估计量 $\\hat{A}_{g}$。\n3. 计算绝对误差 $e_t = |\\hat{A}_t - A_{\\text{true}}|$ 和 $e_g = |\\hat{A}_g - A_{\\text{true}}|$。\n4. 报告一个布尔值，指示学生t分布 MAP 是否严格比正态 MAP 更接近基准真相，即是否 $e_t  e_g$。\n\n您的程序必须以完全确定性的方式实现计算，不含随机性。对于三个测试用例中的每一个，程序应返回一个形式为 $[\\hat{A}_t,\\hat{A}_g,e_t,e_g,\\text{is\\_robust},\\text{hessian\\_positive}]$ 的列表，其中 $\\text{is\\_robust}$ 是 $e_t  e_g$ 的布尔值，$\\text{hessian\\_positive}$ 是 $H_t > 0$ 的布尔值。\n\n最终输出格式：您的程序应生成包含三个用例列表的单行输出，列表之间没有空格，格式如 $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$。所有数字都是无量纲的。三角函数内的角度以弧度为单位。输出中不需要也不允许使用物理单位。", "solution": "该问题要求在线性模型中，基于两种不同的似然假设（正态（高斯）和学生t分布），计算和比较标量参数$A$的最大后验（MAP）估计量。将评估这些估计量对异常值的鲁棒性。所有推导都将从第一性原理出发。\n\n问题的核心在于贝叶斯推断。根据贝叶斯定理，给定数据$\\mathbf{y} = \\{y_i\\}_{i=1}^{N}$，参数$A$的后验概率分布与似然和先验的乘积成正比：\n$$\np(A | \\mathbf{y}, \\mathbf{s}) \\propto p(\\mathbf{y} | A, \\mathbf{s}) p(A)\n$$\n其中$\\mathbf{s} = \\{s_i\\}_{i=1}^{N}$是已知信号。MAP估计$\\hat{A}$是使该后验概率最大化的$A$值。最大化后验等价于最小化其负对数。我们定义负对数后验为$L(A) = - \\ln p(\\mathbf{y} | A, \\mathbf{s}) - \\ln p(A)$，忽略不依赖于$A$的常数项。\n\n模型组件定义如下：\n- **正向模型**：$y_i = A s_i + \\varepsilon_i$，其中$\\varepsilon_i$是噪声项。\n- **先验分布**：关于$A$的先验信念被建模为高斯分布，$A \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$。其概率密度函数（PDF）为$p(A) \\propto \\exp\\left(-\\frac{(A - \\mu_0)^2}{2\\tau_0^2}\\right)$。负对数先验为$\\ln p(A) = - \\frac{(A - \\mu_0)^2}{2\\tau_0^2} + \\text{const}$。\n- **似然函数**：假设观测值$y_i$在给定$A$的条件下是独立的，总似然是单个似然的乘积，$p(\\mathbf{y} | A, \\mathbf{s}) = \\prod_{i=1}^{N} p(y_i | A, s_i)$。我们考虑$p(y_i | A, s_i)$的两种形式：\n    1.  **正态似然**：$p(y_i | A, s_i) = \\mathcal{N}(y_i | A s_i, \\sigma^2) \\propto \\exp\\left(-\\frac{(y_i - A s_i)^2}{2\\sigma^2}\\right)$。这对应于假设噪声$\\varepsilon_i$是高斯分布的。\n    2.  **学生t分布似然**：$p(y_i | A, s_i) = \\text{St}(y_i | A s_i, \\sigma^2, \\nu) \\propto \\left(1 + \\frac{(y_i - A s_i)^2}{\\nu \\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}$。这对应于假设噪声$\\varepsilon_i$服从学生t分布，该分布比高斯分布具有更重的尾部，因此对异常值更具鲁棒性。\n\n### 1. 使用正态似然的MAP估计\n\n对于正态似然，负对数后验$L_g(A)$为：\n$$\nL_g(A) = - \\sum_{i=1}^{N} \\ln \\mathcal{N}(y_i | A s_i, \\sigma^2) - \\ln \\mathcal{N}(A | \\mu_0, \\tau_0^2) + C_g\n$$\n$$\nL_g(A) = \\sum_{i=1}^{N} \\frac{(y_i - A s_i)^2}{2\\sigma^2} + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} + C'_g\n$$\n为了找到MAP估计$\\hat{A}_g$，我们将$L_g(A)$对$A$求导并令结果为零：\n$$\n\\frac{d L_g}{d A} = \\sum_{i=1}^{N} \\frac{-s_i(y_i - A s_i)}{\\sigma^2} + \\frac{A - \\mu_0}{\\tau_0^2} = 0\n$$\n重新整理各项以求解$A$：\n$$\n\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} (A s_i^2 - s_i y_i) + \\frac{A - \\mu_0}{\\tau_0^2} = 0\n$$\n$$\nA \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i^2 + \\frac{1}{\\tau_0^2} \\right) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i y_i + \\frac{\\mu_0}{\\tau_0^2}\n$$\n这为高斯MAP估计$\\hat{A}_g$提供了一个闭合形式解：\n$$\n\\hat{A}_g = \\frac{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i y_i + \\frac{\\mu_0}{\\tau_0^2}}{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i^2 + \\frac{1}{\\tau_0^2}}\n$$\n该表达式表示最大似然估计和先验均值的精度加权平均。\n\n### 2. 使用学生t分布似然的MAP估计（IRLS）\n\n对于学生t分布似然，负对数后验$L_t(A)$为：\n$$\nL_t(A) = - \\sum_{i=1}^{N} \\ln \\text{St}(y_i | A s_i, \\sigma^2, \\nu) - \\ln \\mathcal{N}(A | \\mu_0, \\tau_0^2) + C_t\n$$\n$$\nL_t(A) = \\sum_{i=1}^{N} \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{(y_i - A s_i)^2}{\\nu \\sigma^2}\\right) + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} + C'_t\n$$\n对$A$求导得到一个非线性方程：\n$$\n\\frac{d L_t}{d A} = \\sum_{i=1}^{N} \\frac{(\\nu+1) s_i (A s_i - y_i)}{\\nu \\sigma^2 + (y_i - A s_i)^2} + \\frac{A - \\mu_0}{\\tau_0^2} = 0\n$$\n$A$不存在闭合形式解。我们根据题目要求，基于学生t分布的尺度混合表示推导一个迭代算法。一个学生t分布变量可以表示为一个方差在伽玛分布上积分的高斯变量。具体来说，$y_i \\sim \\text{St}(A s_i, \\sigma^2, \\nu)$等价于分层模型：\n$$\ny_i | \\lambda_i \\sim \\mathcal{N}(A s_i, \\sigma^2/\\lambda_i) \\quad \\text{with} \\quad \\lambda_i \\sim \\text{Gamma}(\\nu/2, \\nu/2)\n$$\n这里，$\\lambda_i$是潜在精度变量。$A$和$\\{\\lambda_i\\}$的完整数据负对数后验是：\n$$\nL_t(A, \\{\\lambda_i\\}) = \\sum_{i=1}^{N} \\frac{\\lambda_i(y_i - A s_i)^2}{2\\sigma^2} + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} - \\sum_{i=1}^{N} \\ln p(\\lambda_i) + \\text{const}\n$$\n迭代重加权最小二乘（IRLS）算法是一种期望最大化（EM）变体，它迭代地更新$A$。\n- **E步**：计算给定当前$A$的估计值$A^{(k)}$下，潜在变量$\\lambda_i$的期望。这个期望作为每个数据点的权重。$\\lambda_i$的后验是$\\text{Gamma}(\\lambda_i | \\frac{\\nu+1}{2}, \\frac{\\nu\\sigma^2 + (y_i-A^{(k)}s_i)^2}{2\\sigma^2})$。期望值为：\n$$\nw_i^{(k+1)} = E[\\lambda_i | y_i, A^{(k)}] = \\frac{(\\nu+1)/2}{(\\nu\\sigma^2 + (y_i-A^{(k)}s_i)^2)/(2\\sigma^2)} = \\frac{\\nu+1}{\\nu + \\frac{(y_i - A^{(k)}s_i)^2}{\\sigma^2}}\n$$\n- **M步**：通过最小化期望的完整数据负对数后验来更新$A$的估计，这是一个加权最小二乘问题：\n$$\nA^{(k+1)} = \\arg\\min_A \\left( \\sum_{i=1}^{N} \\frac{w_i^{(k+1)}(y_i - A s_i)^2}{2\\sigma^2} + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} \\right)\n$$\n这个最小化问题的解在形式上与高斯情况相同，但似然和中的每一项都应用了权重$w_i$：\n$$\nA^{(k+1)} = \\frac{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} w_i^{(k+1)} s_i y_i + \\frac{\\mu_0}{\\tau_0^2}}{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} w_i^{(k+1)} s_i^2 + \\frac{1}{\\tau_0^2}}\n$$\n从一个初始猜测（例如，$A^{(0)} = \\hat{A}_g$）开始，迭代这个过程直到$A^{(k)}$收敛到学生t分布的MAP估计$\\hat{A}_t$。\n\n### 3. 负对数后验的曲率\n\n在MAP估计处的负对数后验曲率是二阶导数，$H_t = \\frac{d^2 L_t}{dA^2}\\Big|_{A=\\hat{A}_t}$。该值是拉普拉斯近似（一个以$\\hat{A}_t$为中心的高斯分布）方差的倒数。正定性（$H_t > 0$）是$\\hat{A}_t$成为局部最小值以及拉普拉斯近似成为有效高斯分布（即具有正方差）的必要条件。\n\n将$\\frac{d L_t}{d A}$对$A$求导得到：\n$$\nH_t(A) = \\frac{d^2 L_t}{d A^2} = \\sum_{i=1}^{N} (\\nu+1)s_i^2 \\frac{\\nu\\sigma^2 - (y_i - A s_i)^2}{(\\nu\\sigma^2 + (y_i - A s_i)^2)^2} + \\frac{1}{\\tau_0^2}\n$$\n我们在$A = \\hat{A}_t$处评估此表达式，以找到所需的曲率$H_t$。\n\n### 4. 计算步骤\n对于每个测试用例：\n1.  使用给定的参数$A_{\\text{true}}$、$s_i$、$n_i$和$o_i$构建数据向量$\\mathbf{y}$。\n2.  使用其闭合形式解析解直接计算$\\hat{A}_g$。\n3.  通过执行IRLS算法固定次数的迭代来计算$\\hat{A}_t$，以确保确定性收敛。\n4.  使用在$\\hat{A}_t$处评估的解析公式计算曲率$H_t$。检查条件$H_t > 0$。\n5.  计算绝对误差$e_g = |\\hat{A}_g - A_{\\text{true}}|$和$e_t = |\\hat{A}_t - A_{\\text{true}}|$。\n6.  评估鲁棒性条件$e_t  e_g$。\n然后收集结果并按规定格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes MAP estimators for a linear model with Gaussian and Student-t likelihoods\n    and assesses their robustness to outliers.\n    \"\"\"\n    # Fixed parameters\n    mu0 = 0.0\n    tau0_sq = 10.0\n    sigma = 0.1\n    sigma_sq = sigma**2\n    A_true = 2.0\n    N = 25\n    IRLS_ITERATIONS = 100\n\n    # Generate forward model signal and baseline noise\n    i_vals = np.arange(1, N + 1)\n    s = np.exp(-0.05 * i_vals) * (1.0 + 0.5 * np.sin(0.3 * i_vals))\n    n = 0.05 * np.sin(0.7 * i_vals)\n\n    # Define test cases\n    test_cases = [\n        # Case 1: No outliers, nu=5\n        {'nu': 5.0, 'outliers': {}},\n        # Case 2: Heavy-tailed contamination, nu=3\n        {'nu': 3.0, 'outliers': {5: 1.5, 12: -2.0, 20: 2.5}},\n        # Case 3: Extreme outlier, nu=1\n        {'nu': 1.0, 'outliers': {8: 6.0}},\n    ]\n\n    results = []\n\n    def compute_gaussian_map(y, s_vec, mu0_val, tau0_sq_val, sigma_sq_val):\n        \"\"\"Computes the MAP estimate for A under a Gaussian likelihood.\"\"\"\n        s_sq_sum = np.sum(s_vec**2)\n        sy_sum = np.sum(s_vec * y)\n        \n        inv_sigma_sq = 1.0 / sigma_sq_val\n        inv_tau0_sq = 1.0 / tau0_sq_val\n\n        numerator = inv_sigma_sq * sy_sum + inv_tau0_sq * mu0_val\n        denominator = inv_sigma_sq * s_sq_sum + inv_tau0_sq\n        \n        return numerator / denominator\n\n    def compute_student_t_map(y, s_vec, mu0_val, tau0_sq_val, sigma_sq_val, nu_val, A_init):\n        \"\"\"Computes the MAP estimate for A under a Student-t likelihood using IRLS.\"\"\"\n        A_k = A_init\n        inv_sigma_sq = 1.0 / sigma_sq_val\n        inv_tau0_sq = 1.0 / tau0_sq_val\n\n        for _ in range(IRLS_ITERATIONS):\n            residuals_sq = (y - A_k * s_vec)**2\n            weights = (nu_val + 1.0) / (nu_val + residuals_sq * inv_sigma_sq)\n            \n            w_s_sq_sum = np.sum(weights * s_vec**2)\n            w_sy_sum = np.sum(weights * s_vec * y)\n            \n            numerator = inv_sigma_sq * w_sy_sum + inv_tau0_sq * mu0_val\n            denominator = inv_sigma_sq * w_s_sq_sum + inv_tau0_sq\n            \n            A_k = numerator / denominator\n\n        A_hat_t = A_k\n\n        # Compute curvature (Hessian of negative log-posterior) at A_hat_t\n        residuals = y - A_hat_t * s_vec\n        \n        term1_num = nu_val * sigma_sq_val - residuals**2\n        term1_den = (nu_val * sigma_sq_val + residuals**2)**2\n        \n        hessian_sum = np.sum((nu_val + 1.0) * s_vec**2 * term1_num / term1_den)\n        H_t = hessian_sum + inv_tau0_sq\n        \n        return A_hat_t, H_t\n\n    for case in test_cases:\n        # Construct outlier vector and observed data y\n        o = np.zeros(N)\n        for idx, val in case['outliers'].items():\n            o[idx - 1] = val  # Convert 1-based index to 0-based\n        \n        y = A_true * s + n + o\n        nu = case['nu']\n\n        # 1. Compute MAP for Normal likelihood\n        A_hat_g = compute_gaussian_map(y, s, mu0, tau0_sq, sigma_sq)\n\n        # 2. Compute MAP and Hessian for Student-t likelihood\n        # Initialize IRLS with the Gaussian MAP estimate\n        A_hat_t, H_t = compute_student_t_map(y, s, mu0, tau0_sq, sigma_sq, nu, A_hat_g)\n\n        # 3. Compute errors\n        e_t = abs(A_hat_t - A_true)\n        e_g = abs(A_hat_g - A_true)\n        \n        # 4. Check robustness and Hessian positivity\n        is_robust = e_t  e_g\n        hessian_positive = H_t > 0.0\n        \n        # Collect results for this case\n        case_results = [A_hat_t, A_hat_g, e_t, e_g, is_robust, hessian_positive]\n        results.append(case_results)\n\n    # Format final output string as a list of lists with no spaces\n    # Convert bools to lowercase 'true'/'false' as per python str() default\n    result_str = str(results).replace(\" \", \"\")\n    print(result_str)\n\nsolve()\n```", "id": "3502607"}, {"introduction": "先进的数字孪生通常必须同时学习模型参数和估计系统状态，这个任务一般通过迭代算法来处理。本实践将探讨这种耦合训练-同化方案的设计与分析。通过将交替优化过程构建为一个仿射不动点迭代，并分析其收敛特性（[@problem_id:3502593]），您将对复杂自适应数字孪生系统的稳定性与性能获得关键的理解。", "problem": "一位开发人员的任务是为多物理场耦合模拟中的数字孪生设计一个训练-同化方案，该方案在参数学习和状态估计之间交替进行，并使用贝叶斯推断和数据同化方法，在收缩假设下分析其向不动点的收敛性。考虑如下定义的线性高斯模型。状态为 $x \\in \\mathbb{R}^n$，标量参数为 $\\theta \\in \\mathbb{R}$，观测值为 $y \\in \\mathbb{R}^n$。观测模型为 $y = C x + \\varepsilon$，其中 $C \\in \\mathbb{R}^{n \\times n}$，高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, R)$，这里 $R \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。给定参数下状态的先验分布为 $x \\mid \\theta \\sim \\mathcal{N}(B \\theta, P)$，其中 $B \\in \\mathbb{R}^{n \\times 1}$，$P \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。参数的先验分布为 $\\theta \\sim \\mathcal{N}(\\theta_{\\text{prior}}, S)$，其中 $S \\in \\mathbb{R}_{0}$。\n\n将最大后验（MAP）状态估计量定义为二次目标函数\n$$\nJ(x \\mid \\theta) = (y - C x)^\\top R^{-1} (y - C x) + (x - B \\theta)^\\top P^{-1} (x - B \\theta),\n$$\n的最小化子 $x_{\\text{MAP}}(\\theta)$，并将MAP参数估计量 $\\theta_{\\text{MAP}}(x)$ 定义为\n$$\nL(\\theta \\mid x) = (x - B \\theta)^\\top P^{-1} (x - B \\theta) + (\\theta - \\theta_{\\text{prior}})^\\top S^{-1} (\\theta - \\theta_{\\text{prior}}).\n$$\n的最小化子。考虑一个交替、欠松弛的训练-同化方案\n$$\nx_{k+1} = x_k + \\alpha_x \\left( x_{\\text{MAP}}(\\theta_k) - x_k \\right),\n\\quad\n\\theta_{k+1} = \\theta_k + \\alpha_\\theta \\left( \\theta_{\\text{MAP}}(x_{k+1}) - \\theta_k \\right),\n$$\n其中松弛系数为 $ \\alpha_x \\in \\mathbb{R}$ 和 $ \\alpha_\\theta \\in \\mathbb{R}$。证明该方案可以写为关于增广变量 $z_k \\in \\mathbb{R}^{n+1}$（定义为 $z_k = [x_k; \\theta_k]$）的单一仿射迭代：\n$$\nz_{k+1} = A z_k + b,\n$$\n其中矩阵 $A \\in \\mathbb{R}^{(n+1)\\times(n+1)}$ 和向量 $b \\in \\mathbb{R}^{n+1}$ 依赖于 $C, R, P, B, S, y, \\theta_{\\text{prior}}, \\alpha_x, \\alpha_\\theta$。从高斯后验的核心定义和线性最小二乘法的正规方程出发，推导$A$和$b$的显式表达式。然后，使用有限维空间中的巴拿赫不动点原理和诺伊曼级数特征，根据$A$的谱半径分析收敛条件。具体来说，证明如果谱半径$\\rho(A)  1$，则存在唯一的不动点$z^{\\star}$满足\n$$\nz^{\\star} = A z^{\\star} + b,\n$$\n且对于任何初始值$z_0$，迭代都收敛到$z^{\\star}$。当$\\rho(A)  1$时，推导不动点的闭式解$z^{\\star} = (I - A)^{-1} b$。\n\n您的程序必须：\n- 根据给定的模型参数构造$A$和$b$。\n- 计算谱半径$\\rho(A)$。\n- 从指定的初始条件$z_0$开始，运行迭代$z_{k+1} = A z_k + b$，直至达到固定的最大迭代次数，如果迭代差异满足 $\\|z_{k+1} - z_k\\|_2  \\varepsilon$（其中$\\varepsilon$为容差），则提前停止。\n- 如果$\\rho(A)  1$，计算不动点$z^{\\star} = (I - A)^{-1} b$并报告最终误差$e = \\|z_N - z^{\\star}\\|_2$，其中$z_N$是最后一次迭代的结果。\n- 如果$\\rho(A) \\ge 1$，将误差$e$设置为$+\\infty$。\n- 报告一个收敛指示符$c$，为一个十进制数。如果$ \\rho(A)  1$且最终误差$e$小于容差$\\varepsilon$，则该值为$1.0$，否则为$0.0$。\n\n在所有情况下，使用以下测试套件，其中$n = 2$，并使用指定的数值。下面提供的所有数字都必须严格按照给定值实现：\n- 测试用例1（正常路径）：\n  - $C = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$，\n  - $R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$，\n  - $P = \\begin{bmatrix} 10  0 \\\\ 0  10 \\end{bmatrix}$，\n  - $B = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$，\n  - $S = 0.2$，\n  - $y = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$，\n  - $\\theta_{\\text{prior}} = 0.0$，\n  - $\\alpha_x = 0.6$，\n  - $\\alpha_\\theta = 0.4$，\n  - $z_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n- 测试用例2（近边界收缩）：\n  - $C = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$，\n  - $R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$，\n  - $P = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix}$，\n  - $B = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$，\n  - $S = 1000000.0$，\n  - $y = \\begin{bmatrix} 2 \\\\ -2 \\end{bmatrix}$，\n  - $\\theta_{\\text{prior}} = 0.0$，\n  - $\\alpha_x = 1.0$，\n  - $\\alpha_\\theta = 1.0$，\n  - $z_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n- 测试用例3（通过过松弛导致的非收缩）：\n  - 与测试用例2相同，但$\\alpha_x = 1.2$且$\\alpha_\\theta = 1.0$。\n\n角度单位不适用。不涉及物理单位，因此以数学单位作答即可。最大迭代次数为$N_{\\max} = 200$，容差为$\\varepsilon = 10^{-8}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按以下顺序和类型排列结果：\n$$\n[\\rho_1, c_1, e_1, \\rho_2, c_2, e_2, \\rho_3, c_3, e_3],\n$$\n其中$\\rho_i$是测试用例$i$的谱半径（浮点数），$c_i$是测试用例$i$的收敛指示符（浮点数，等于$1.0$或$0.0$），$e_i$是测试用例$i$的最终误差（浮点数；如果$\\rho_i \\ge 1$，则使用$+\\infty$）。输出必须是此确切格式的单行。", "solution": "目标是为耦合的训练-同化方案推导出一个仿射迭代$z_{k+1} = A z_k + b$，然后对其收敛性进行数值分析。增广状态为$z_k = [x_k^\\top, \\theta_k]^\\top \\in \\mathbb{R}^{n+1}$。\n\n首先，我们推导最大后验（MAP）估计量$x_{\\text{MAP}}(\\theta)$和$\\theta_{\\text{MAP}}(x)$的显式形式。通过最小化它们各自的二次目标函数来找到这些估计量，这可以通过将其梯度设置为零来实现。\n\n状态$x$的目标函数是：\n$$\nJ(x \\mid \\theta) = (y - C x)^\\top R^{-1} (y - C x) + (x - B \\theta)^\\top P^{-1} (x - B \\theta)\n$$\n关于$x$的梯度是：\n$$\n\\nabla_x J(x \\mid \\theta) = -2 C^\\top R^{-1} (y - C x) + 2 P^{-1} (x - B \\theta) = 0\n$$\n$$\nC^\\top R^{-1} C x - C^\\top R^{-1} y + P^{-1}x - P^{-1}B\\theta = 0\n$$\n$$\n(C^\\top R^{-1} C + P^{-1}) x = C^\\top R^{-1} y + P^{-1} B \\theta\n$$\n由于$R$和$P$是对称正定（SPD）的，因此$R^{-1}$和$P^{-1}$也是。矩阵$H_x = C^\\top R^{-1} C + P^{-1}$是一个半正定矩阵与一个正定矩阵之和，因此是正定且可逆的。求解$x$可得MAP估计量：\n$$\nx_{\\text{MAP}}(\\theta) = (C^\\top R^{-1} C + P^{-1})^{-1} (C^\\top R^{-1} y + P^{-1} B \\theta)\n$$\n我们定义$M_x = (C^\\top R^{-1} C + P^{-1})^{-1}$。$x_{\\text{MAP}}(\\theta)$的表达式是$\\theta$的仿射函数：\n$$\nx_{\\text{MAP}}(\\theta) = (M_x P^{-1} B) \\theta + (M_x C^\\top R^{-1} y)\n$$\n\n接下来，我们求解标量参数$\\theta$的MAP估计量。目标函数是：\n$$\nL(\\theta \\mid x) = (x - B \\theta)^\\top P^{-1} (x - B \\theta) + (\\theta - \\theta_{\\text{prior}})^\\top S^{-1} (\\theta - \\theta_{\\text{prior}})\n$$\n由于$\\theta$是标量，因此有$L(\\theta \\mid x) = (x - B \\theta)^\\top P^{-1} (x - B \\theta) + S^{-1} (\\theta - \\theta_{\\text{prior}})^2$。\n关于$\\theta$的导数是：\n$$\n\\frac{dL}{d\\theta} = -2 B^\\top P^{-1} (x - B \\theta) + 2 S^{-1} (\\theta - \\theta_{\\text{prior}}) = 0\n$$\n$$\n-B^\\top P^{-1} x + (B^\\top P^{-1} B) \\theta + S^{-1} \\theta - S^{-1} \\theta_{\\text{prior}} = 0\n$$\n$$\n(B^\\top P^{-1} B + S^{-1}) \\theta = B^\\top P^{-1} x + S^{-1} \\theta_{\\text{prior}}\n$$\n标量项$H_\\theta = B^\\top P^{-1} B + S^{-1}$是严格为正的，因为$P^{-1}$是对称正定的且$S  0$。因此，它是可逆的。求解$\\theta$可得MAP估计量：\n$$\n\\theta_{\\text{MAP}}(x) = (B^\\top P^{-1} B + S^{-1})^{-1} (B^\\top P^{-1} x + S^{-1} \\theta_{\\text{prior}})\n$$\n我们定义标量$M_\\theta = (B^\\top P^{-1} B + S^{-1})^{-1}$。$\\theta_{\\text{MAP}}(x)$的表达式是$x$的仿射函数：\n$$\n\\theta_{\\text{MAP}}(x) = (M_\\theta B^\\top P^{-1}) x + (M_\\theta S^{-1} \\theta_{\\text{prior}})\n$$\n\n现在，我们将这些估计量代入欠松弛迭代方案中：\n1. $x_{k+1} = x_k + \\alpha_x ( x_{\\text{MAP}}(\\theta_k) - x_k ) = (1-\\alpha_x)x_k + \\alpha_x x_{\\text{MAP}}(\\theta_k)$\n2. $\\theta_{k+1} = \\theta_k + \\alpha_\\theta ( \\theta_{\\text{MAP}}(x_{k+1}) - \\theta_k ) = (1-\\alpha_\\theta)\\theta_k + \\alpha_\\theta \\theta_{\\text{MAP}}(x_{k+1})$\n\n将推导出的$x_{\\text{MAP}}(\\theta_k)$表达式代入第一个方程：\n$$\nx_{k+1} = (1-\\alpha_x)x_k + \\alpha_x \\left[ (M_x P^{-1} B) \\theta_k + (M_x C^\\top R^{-1} y) \\right]\n$$\n$$\nx_{k+1} = (1-\\alpha_x)I x_k + (\\alpha_x M_x P^{-1} B) \\theta_k + (\\alpha_x M_x C^\\top R^{-1} y)\n$$\n这提供了我们的仿射系统$z_{k+1}=Az_k+b$的顶部块。\n\n接下来，我们将$x_{k+1}$的表达式代入$\\theta_{k+1}$的方程中：\n$$\n\\theta_{k+1} = (1-\\alpha_\\theta)\\theta_k + \\alpha_\\theta \\left[ (M_\\theta B^\\top P^{-1}) x_{k+1} + (M_\\theta S^{-1} \\theta_{\\text{prior}}) \\right]\n$$\n$$\n\\theta_{k+1} = (1-\\alpha_\\theta)\\theta_k + \\alpha_\\theta M_\\theta B^\\top P^{-1} \\left[ (1-\\alpha_x)I x_k + (\\alpha_x M_x P^{-1} B) \\theta_k + (\\alpha_x M_x C^\\top R^{-1} y) \\right] + \\alpha_\\theta M_\\theta S^{-1} \\theta_{\\text{prior}}\n$$\n我们按$x_k$、$\\theta_k$和常数项对各项进行分组：\n- 含$x_k$的项：$(\\alpha_\\theta (1-\\alpha_x) M_\\theta B^\\top P^{-1}) x_k$\n- 含$\\theta_k$的项：$\\left( (1-\\alpha_\\theta) + \\alpha_\\theta \\alpha_x M_\\theta B^\\top P^{-1} M_x P^{-1} B \\right) \\theta_k$\n- 常数项：$\\alpha_\\theta M_\\theta \\alpha_x B^\\top P^{-1} M_x C^\\top R^{-1} y + \\alpha_\\theta M_\\theta S^{-1} \\theta_{\\text{prior}}$\n\n根据这些表达式，我们可以构造分块形式的矩阵$A \\in \\mathbb{R}^{(n+1)\\times(n+1)}$和向量$b \\in \\mathbb{R}^{n+1}$，其中$z_k = [x_k^\\top, \\theta_k]^\\top$：\n$$\nA = \\begin{bmatrix} A_{xx}  A_{x\\theta} \\\\ A_{\\theta x}  A_{\\theta\\theta} \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_x \\\\ b_\\theta \\end{bmatrix}\n$$\n这些块是：\n$A_{xx} = (1 - \\alpha_x) I_{n \\times n}$\n$A_{x\\theta} = \\alpha_x M_x P^{-1} B$\n$A_{\\theta x} = \\alpha_\\theta (1 - \\alpha_x) M_\\theta B^\\top P^{-1}$\n$A_{\\theta\\theta} = (1 - \\alpha_\\theta) + \\alpha_\\theta \\alpha_x M_\\theta (B^\\top P^{-1} M_x P^{-1} B)$\n\n$b_x = \\alpha_x M_x C^\\top R^{-1} y$\n$b_\\theta = \\alpha_\\theta M_\\theta ( \\alpha_x B^\\top P^{-1} M_x C^\\top R^{-1} y + S^{-1} \\theta_{\\text{prior}})$\n\n迭代$z_{k+1} = A z_k + b$是一个仿射变换。根据巴拿赫不动点定理，当且仅当$A$是收缩映射时，对于任何起始点$z_0$，此迭代收敛到唯一的不动点$z^{\\star}$。对于有限维空间中的线性算子，此条件等价于其谱半径小于1：$\\rho(A)  1$。谱半径定义为$\\rho(A) = \\max_i |\\lambda_i|$，其中$\\lambda_i$是$A$的特征值。\n\n如果$\\rho(A)  1$，不动点$z^{\\star}$满足$z^{\\star} = A z^{\\star} + b$，可以重排为$(I - A) z^{\\star} = b$。由于$\\rho(A)  1$，$1$不是$A$的特征值，因此$(I-A)$是可逆的。唯一的不动点由$z^{\\star} = (I - A)^{-1} b$给出。逆矩阵$(I-A)^{-1}$可以由收敛的诺伊曼级数$\\sum_{k=0}^{\\infty} A^k$表示。\n\n数值实现将构造$A$和$b$，计算$\\rho(A)$，执行迭代，并计算相对于解析确定的不动点$z^{\\star}$的最终误差。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates the calculation for each case and prints the final result.\n    \"\"\"\n\n    # Constants\n    N_max = 200\n    epsilon = 1e-8\n    n = 2\n\n    # Test Case 1: Happy path\n    case1 = {\n        \"C\": np.eye(2),\n        \"R\": np.eye(2),\n        \"P\": np.array([[10.0, 0.0], [0.0, 10.0]]),\n        \"B\": np.array([[3.0], [3.0]]),\n        \"S\": 0.2,\n        \"y\": np.array([[1.0], [-1.0]]),\n        \"theta_prior\": 0.0,\n        \"alpha_x\": 0.6,\n        \"alpha_theta\": 0.4,\n        \"z0\": np.zeros((n + 1, 1))\n    }\n\n    # Test Case 2: Near-boundary contraction\n    case2 = {\n        \"C\": np.eye(2),\n        \"R\": np.eye(2),\n        \"P\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n        \"B\": np.array([[10.0], [10.0]]),\n        \"S\": 1000000.0,\n        \"y\": np.array([[2.0], [-2.0]]),\n        \"theta_prior\": 0.0,\n        \"alpha_x\": 1.0,\n        \"alpha_theta\": 1.0,\n        \"z0\": np.zeros((n + 1, 1))\n    }\n\n    # Test Case 3: Non-contraction via over-relaxation\n    case3 = {\n        \"C\": np.eye(2),\n        \"R\": np.eye(2),\n        \"P\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n        \"B\": np.array([[10.0], [10.0]]),\n        \"S\": 1000000.0,\n        \"y\": np.array([[2.0], [-2.0]]),\n        \"theta_prior\": 0.0,\n        \"alpha_x\": 1.2,\n        \"alpha_theta\": 1.0,\n        \"z0\": np.zeros((n + 1, 1))\n    }\n\n    test_cases = [case1, case2, case3]\n    results = []\n\n    for case_params in test_cases:\n        rho, c, e = process_case(case_params, n, N_max, epsilon)\n        results.extend([rho, c, e])\n\n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef process_case(params, n, N_max, epsilon):\n    \"\"\"\n    Processes a single test case: constructs A and b, computes convergence\n    properties, runs the iteration, and calculates the final error.\n    \"\"\"\n    # Unpack parameters\n    C, R, P, B, S, y = params[\"C\"], params[\"R\"], params[\"P\"], params[\"B\"], params[\"S\"], params[\"y\"]\n    theta_prior, alpha_x, alpha_theta = params[\"theta_prior\"], params[\"alpha_x\"], params[\"alpha_theta\"]\n    z0 = params[\"z0\"]\n\n    # Compute matrix inverses\n    try:\n        R_inv = np.linalg.inv(R)\n        P_inv = np.linalg.inv(P)\n    except np.linalg.LinAlgError:\n        # Handle singular matrices, though not expected from problem spec\n        return np.inf, 0.0, np.inf\n\n    # Compute intermediate quantities M_x and M_theta\n    Mx_inv = C.T @ R_inv @ C + P_inv\n    Mx = np.linalg.inv(Mx_inv)\n    \n    M_theta_inv_val = (B.T @ P_inv @ B)[0, 0] + 1.0/S\n    M_theta = 1.0 / M_theta_inv_val\n\n    # Construct the iteration matrix A\n    A_xx = (1.0 - alpha_x) * np.eye(n)\n    A_xtheta = alpha_x * Mx @ P_inv @ B\n    A_thetax = alpha_theta * (1.0 - alpha_x) * M_theta * (B.T @ P_inv)\n    A_thetatheta_val = (1.0 - alpha_theta) + alpha_theta * alpha_x * M_theta * (B.T @ P_inv @ Mx @ P_inv @ B)[0, 0]\n    A_thetatheta = np.array([[A_thetatheta_val]])\n    A = np.block([[A_xx, A_xtheta], [A_thetax, A_thetatheta]])\n\n    # Construct the constant vector b\n    b_x = alpha_x * Mx @ C.T @ R_inv @ y\n    b_theta_val = alpha_theta * M_theta * (alpha_x * (B.T @ P_inv @ Mx @ C.T @ R_inv @ y)[0, 0] + (1.0/S) * theta_prior)\n    b_theta = np.array([[b_theta_val]])\n    b = np.vstack((b_x, b_theta))\n\n    # Compute spectral radius of A\n    eigenvalues = np.linalg.eigvals(A)\n    rho = np.max(np.abs(eigenvalues))\n\n    # Perform the iteration\n    z = z0.copy()\n    z_N = z0\n    for _ in range(N_max):\n        z_next = A @ z + b\n        diff = np.linalg.norm(z_next - z)\n        z = z_next\n        if diff  epsilon:\n            break\n    z_N = z\n\n    # Compute fixed point and final error\n    if rho >= 1:\n        e = np.inf\n        c = 0.0\n    else:\n        # Compute the true fixed point z_star\n        I_mat = np.eye(n + 1)\n        z_star = np.linalg.inv(I_mat - A) @ b\n        \n        # Compute the final error\n        e = np.linalg.norm(z_N - z_star)\n        \n        # Determine convergence indicator\n        c = 1.0 if e  epsilon else 0.0\n\n    return rho, c, e\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3502593"}]}