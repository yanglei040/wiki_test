{"hands_on_practices": [{"introduction": "卷积是在信号处理和成像领域中的一个基本操作，理解其伴随算子对于构建和解决如去模糊等逆问题至关重要。本练习将引导你通过严格的推导，证明一个经典结果：卷积的伴随算子是相关运算。此练习不仅能巩固无限维空间中伴随算子的定义，还能提升你处理无穷级数求和的技巧。", "problem": "考虑线性算子 $C_{h} : \\ell^{2}(\\mathbb{Z}) \\to \\ell^{2}(\\mathbb{Z})$，它由与序列 $h \\in \\ell^{1}(\\mathbb{Z})$ 的离散卷积定义，即\n$$\n(C_{h} x)[n] \\;=\\; \\sum_{k \\in \\mathbb{Z}} h[k]\\, x[n - k], \\quad n \\in \\mathbb{Z}.\n$$\n假设 $x, y : \\mathbb{Z} \\to \\mathbb{C}$ 并使用标准 $\\ell^{2}$ 内积\n$$\n\\langle u, v \\rangle \\;=\\; \\sum_{n \\in \\mathbb{Z}} u[n]\\, \\overline{v[n]}.\n$$\n仅从 $\\ell^{2}$ 内积、线性伴随和在 $\\mathbb{Z}$ 上的卷积的核心定义出发，推导伴随算子 $C_{h}^{\\ast}$，并将其表示为一个由序列 $\\tilde{h} \\in \\ell^{1}(\\mathbb{Z})$ 构成的卷积。通过使用基于第一性原理的不等式建立绝对收敛性，为任何无穷级数求和顺序的交换提供完整的证明。你的最终答案必须是 $\\tilde{h}$ 的各项关于 $h$ 的单一闭式解析表达式；不要援引任何变换域表征或超出上述核心定义的外部定理。如果你引入任何辅助索引变换，请明确陈述并证明其合理性。", "solution": "该问题陈述是泛函分析中一个有效、适定的数学练习。其科学基础是希尔伯特空间上的线性算子理论，特别是平方可和序列空间 $\\ell^{2}(\\mathbb{Z})$。所有术语都有形式化且无歧义的定义。所提供的条件，即 $h \\in \\ell^{1}(\\mathbb{Z})$ 和 $x, y \\in \\ell^{2}(\\mathbb{Z})$，正是确保该算子是良定义、有界并因此拥有唯一伴随算子所需要的。该问题是自洽且无矛盾的。\n\n线性算子 $C_{h} : \\ell^{2}(\\mathbb{Z}) \\to \\ell^{2}(\\mathbb{Z})$ 的伴随算子 $C_{h}^{\\ast}$ 由以下关系式唯一确定\n$$\n\\langle C_{h} x, y \\rangle = \\langle x, C_{h}^{\\ast} y \\rangle\n$$\n对所有 $x, y \\in \\ell^{2}(\\mathbb{Z})$ 成立。我们首先使用算子 $C_h$ 和内积 $\\langle \\cdot, \\cdot \\rangle$ 的定义来展开左侧。\n\n设 $x, y \\in \\ell^{2}(\\mathbb{Z})$。内积 $\\langle C_{h} x, y \\rangle$ 由下式给出：\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{n \\in \\mathbb{Z}} (C_{h} x)[n]\\, \\overline{y[n]}\n$$\n代入卷积算子的定义 $(C_{h} x)[n] = \\sum_{k \\in \\mathbb{Z}} h[k]\\, x[n - k]$，我们得到一个双重求和：\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{n \\in \\mathbb{Z}} \\left( \\sum_{k \\in \\mathbb{Z}} h[k]\\, x[n - k] \\right) \\overline{y[n]} = \\sum_{n \\in \\mathbb{Z}} \\sum_{k \\in \\mathbb{Z}} h[k]\\, x[n - k]\\, \\overline{y[n]}\n$$\n为了继续推导，我们必须能够交换这些无穷求和的顺序。如果级数绝对收敛，则这是允许的。我们通过考虑各项绝对值之和来验证此条件：\n$$\n\\sum_{n \\in \\mathbb{Z}} \\sum_{k \\in \\mathbb{Z}} |h[k]\\, x[n - k]\\, \\overline{y[n]}| = \\sum_{n \\in \\mathbb{Z}} \\sum_{k \\in \\mathbb{Z}} |h[k]|\\, |x[n - k]|\\, |y[n]|\n$$\n我们可以将此和重排为：\n$$\n\\sum_{k \\in \\mathbb{Z}} |h[k]| \\left( \\sum_{n \\in \\mathbb{Z}} |x[n - k]|\\, |y[n]| \\right)\n$$\n内部的和 $\\sum_{n \\in \\mathbb{Z}} |x[n - k]|\\, |y[n]|$ 可以使用序列的 Cauchy-Schwarz 不等式来界定其上界。这个和是序列 $(|x[n-k]|)_{n \\in \\mathbb{Z}}$ 和 $(|y[n]|)_{n \\in \\mathbb{Z}}$ 的内积。因此，\n$$\n\\sum_{n \\in \\mathbb{Z}} |x[n - k]|\\, |y[n]| \\le \\left( \\sum_{n \\in \\mathbb{Z}} |x[n - k]|^2 \\right)^{1/2} \\left( \\sum_{n \\in \\mathbb{Z}} |y[n]|^2 \\right)^{1/2}\n$$\n$\\ell^{2}$-范数在索引平移下是不变的，所以 $\\left( \\sum_{n \\in \\mathbb{Z}} |x[n - k]|^2 \\right)^{1/2} = \\|x\\|_{\\ell^2}$。第二项是 $\\|y\\|_{\\ell^2}$。因此，内部和的上界为 $\\|x\\|_{\\ell^2} \\|y\\|_{\\ell^2}$。\n将此界带回总和的表达式中，可得：\n$$\n\\sum_{n \\in \\mathbb{Z}} \\sum_{k \\in \\mathbb{Z}} |h[k]\\, x[n - k]\\, \\overline{y[n]}| \\le \\sum_{k \\in \\mathbb{Z}} |h[k]| (\\|x\\|_{\\ell^2} \\|y\\|_{\\ell^2}) = \\|y\\|_{\\ell^2} \\|x\\|_{\\ell^2} \\sum_{k \\in \\mathbb{Z}} |h[k]| = \\|h\\|_{\\ell^1} \\|x\\|_{\\ell^2} \\|y\\|_{\\ell^2}\n$$\n由于 $h \\in \\ell^{1}(\\mathbb{Z})$ 且 $x, y \\in \\ell^{2}(\\mathbb{Z})$，范数 $\\|h\\|_{\\ell^1}$、$\\|x\\|_{\\ell^2}$ 和 $\\|y\\|_{\\ell^2}$ 都是有限的。因此，总和是有限的，这就建立了绝对收敛性。根据用于求和的 Fubini-Tonelli 定理，我们有理由交换求和顺序。\n\n继续对 $\\langle C_h x, y \\rangle$ 进行处理：\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{k \\in \\mathbb{Z}} \\sum_{n \\in \\mathbb{Z}} h[k]\\, x[n - k]\\, \\overline{y[n]}\n$$\n为了分别对含 $x$ 和 $y$ 的项进行分组，我们引入一个索引变换。令 $m = n - k$。这意味着 $n = m + k$。对于任何固定的整数 $k$，映射 $n \\mapsto m$ 是从 $\\mathbb{Z}$ 到 $\\mathbb{Z}$ 的一个双射。代入 $n = m+k$ 并对 $m$ 而不是对 $n$ 求和：\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{k \\in \\mathbb{Z}} \\sum_{m \\in \\mathbb{Z}} h[k]\\, x[m]\\, \\overline{y[m + k]}\n$$\n再次利用已建立的绝对收敛性，我们可以交换求和顺序：\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{m \\in \\mathbb{Z}} \\sum_{k \\in \\mathbb{Z}} x[m]\\, h[k]\\, \\overline{y[m + k]}\n$$\n我们可以从内部和中提出因子 $x[m]$，并使用复共轭的性质 $\\overline{a}b = \\overline{a\\overline{b}}$：\n$$\n\\langle C_{h} x, y \\rangle = \\sum_{m \\in \\mathbb{Z}} x[m] \\left( \\sum_{k \\in \\mathbb{Z}} h[k]\\, \\overline{y[m + k]} \\right) = \\sum_{m \\in \\mathbb{Z}} x[m] \\overline{\\left( \\sum_{k \\in \\mathbb{Z}} \\overline{h[k]}\\, y[m + k] \\right)}\n$$\n该表达式现在具有 $\\langle x, C_{h}^{\\ast} y \\rangle = \\sum_{m \\in \\mathbb{Z}} x[m] \\overline{(C_{h}^{\\ast} y)[m]}$ 的形式。通过比较这两种形式，我们确定了伴随算子在 $y$ 上的作用：\n$$\n(C_{h}^{\\ast} y)[m] = \\sum_{k \\in \\mathbb{Z}} \\overline{h[k]}\\, y[m + k]\n$$\n问题要求将 $C_{h}^{\\ast}$ 表示为卷积算子 $C_{\\tilde{h}}$，其形式为 $(C_{\\tilde{h}} y)[m] = \\sum_{j \\in \\mathbb{Z}} \\tilde{h}[j]\\, y[m - j]$。为了匹配这种形式，我们在 $(C_h^* y)[m]$ 的表达式中引入另一个索引变换。令 $j = -k$。这意味着 $k = -j$。这个映射也是从 $\\mathbb{Z}$ 到 $\\mathbb{Z}$ 的一个双射。代入 $k = -j$：\n$$\n(C_{h}^{\\ast} y)[m] = \\sum_{j \\in \\mathbb{Z}} \\overline{h[-j]}\\, y[m - j]\n$$\n将此与 $C_{\\tilde{h}}$ 的定义进行比较，我们发现算子 $C_{h}^{\\ast}$ 确实是一个卷积算子 $C_{\\tilde{h}}$，其中序列 $\\tilde{h}$ 由其各项定义：\n$$\n\\tilde{h}[j] = \\overline{h[-j]} \\quad \\text{for any } j \\in \\mathbb{Z}.\n$$\n这就是所谓的序列 $h$ 的时间反转共轭。问题暗示 $\\tilde{h}$ 也应该在 $\\ell^1(\\mathbb{Z})$ 中。我们可以验证这一点：\n$$\n\\|\\tilde{h}\\|_{\\ell^1} = \\sum_{j \\in \\mathbb{Z}} |\\tilde{h}[j]| = \\sum_{j \\in \\mathbb{Z}} |\\overline{h[-j]}| = \\sum_{j \\in \\mathbb{Z}} |h[-j]|\n$$\n令 $k = -j$，这个和变为 $\\sum_{k \\in \\mathbb{Z}} |h[k]| = \\|h\\|_{\\ell^1}$。由于 $h \\in \\ell^1(\\mathbb{Z})$，可得出 $\\|\\tilde{h}\\|_{\\ell^1} < \\infty$，所以 $\\tilde{h} \\in \\ell^1(\\mathbb{Z})$。\n\n因此，使用 $n$ 作为索引变量，序列 $\\tilde{h}$ 的各项的解析表达式为 $\\overline{h[-n]}$。", "answer": "$$\\boxed{\\overline{h[-n]}}$$", "id": "3457685"}, {"introduction": "在许多大规模优化问题中，我们需要求解涉及法方程 $A^*Ax=A^*b$ 的系统，一种直接的方法是先计算矩阵 $A^*A$。本思想实验揭示了一个关键的计算考量：对于稀疏问题，显式地构建格拉姆矩阵 $A^*A$ 可能会带来灾难性后果，因为它通常会破坏稀疏性（这种效应被称为“填充”）。通过分析一个概率模型 [@problem_id:3457662]，你将量化这种效应，并理解为何在稀疏优化中，那些分别应用 $A$ 和 $A^*$ 的迭代方法是必不可少的。", "problem": "考虑一个线性算子 $A : \\mathbb{C}^{n} \\to \\mathbb{C}^{m}$，它在压缩感知测量模型中由一个 $m \\times n$ 的稀疏矩阵表示。其伴随算子 $A^{*} : \\mathbb{C}^{m} \\to \\mathbb{C}^{n}$ 由关系 $\\langle A x, y \\rangle = \\langle x, A^{*} y \\rangle$ 定义，对所有 $x \\in \\mathbb{C}^{n}$ 和 $y \\in \\mathbb{C}^{m}$ 成立，其中 $\\langle \\cdot, \\cdot \\rangle$ 是标准的厄米内积。在许多稀疏优化算法中，迭代方法要么分别应用 $A$ 和 $A^{*}$（如在 Least Squares QR (LSQR) 中），要么直接应用正规算子 $A^{*} A$（如在 Conjugate Gradient on the Normal Equations (CGNE) 中）。\n\n构建以下示例以检验稀疏性和填充（fill-in）。设 $A$ 的每一列都恰好有 $s$ 个非零项，其支撑集是从 $m$ 行中无放回地均匀随机选择的，且各列之间独立。假设 $1 \\leq s \\leq m$，并且除了列支撑集的均匀选择外，没有其他结构性约束。设与矩阵 $M$ 进行稀疏矩阵-向量乘法的成本由非零乘法的次数衡量，这等于 $M$ 的非零元素数量。\n\n从上述定义和基本的组合学事实出发，推导将格拉姆算子 $A^{*}A$ 应用于向量的成本与将 $A$ 和 $A^{*}$ 顺序应用于向量的成本之比的期望值的精确闭式表达式。将您的答案表示为 $n$、$m$ 和 $s$ 的单个解析函数 $R(n,m,s)$，并且不要使用近似值。您的最终答案必须是单个符号表达式。不需要四舍五入。", "solution": "所述问题具有科学依据，是适定的、客观的且内部一致的。它在稀疏问题数值算法的分析中，呈现了一个标准的、尽管简化的场景。概率模型定义清晰，问题要求基于此模型推导一个期望值。所有术语在线性代数和数值分析中都是标准的。因此，该问题是有效的，我们可以着手求解。\n\n目标是推导将格拉姆算子 $A^{*}A$ 应用于向量的计算成本与顺序应用算子 $A$ 和 $A^{*}$ 的成本之比。成本定义为算子矩阵表示中的非零元素数量，记作 $\\text{nnz}(\\cdot)$。\n\n令 $C_{seq}$ 表示顺序应用的成本，$C_{gram}$ 表示格拉姆算子应用的成本。所求的比率为 $R = \\frac{E[C_{gram}]}{C_{seq}}$。成本 $C_{seq}$ 是确定性的，所以 $E[C_{seq}] = C_{seq}$。\n\n首先，我们确定顺序应用的成本 $C_{seq}$。这是应用 $A$ 和 $A^{*}$ 的成本之和：\n$$C_{seq} = \\text{nnz}(A) + \\text{nnz}(A^{*})$$\n矩阵 $A$ 的大小为 $m \\times n$。问题陈述其 $n$ 列中的每一列都恰好包含 $s$ 个非零元素。因此，$A$ 中的非零元素总数为：\n$$\\text{nnz}(A) = n \\cdot s$$\n算子 $A^{*}$ 是 $A$ 的伴随算子。在 $\\mathbb{C}^{k}$ 中，伴随算子对应于矩阵表示的共轭转置。非零元素的数量在转置和复共轭下是不变的。因此：\n$$\\text{nnz}(A^{*}) = \\text{nnz}(A) = ns$$\n因此，顺序应用的成本是一个确定性的量：\n$$C_{seq} = ns + ns = 2ns$$\n\n接下来，我们确定应用格拉姆算子的期望成本 $E[C_{gram}]$。格拉姆算子是 $G = A^{*}A$，一个 $n \\times n$ 矩阵。其成本为：\n$$C_{gram} = \\text{nnz}(A^{*}A)$$\n由于 $A$ 的稀疏模式是随机的，$C_{gram}$ 是一个随机变量。我们必须计算其期望 $E[\\text{nnz}(A^{*}A)]$。利用期望的线性性质，期望非零元素数量是每个元素非零的概率之和：\n$$E[\\text{nnz}(A^{*}A)] = E\\left[\\sum_{i=1}^{n}\\sum_{j=1}^{n} \\mathbb{I}((A^{*}A)_{ij} \\neq 0)\\right] = \\sum_{i=1}^{n}\\sum_{j=1}^{n} P((A^{*}A)_{ij} \\neq 0)$$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n\n令 $a_k \\in \\mathbb{C}^{m}$ 表示矩阵 $A$ 的第 $k$ 列。$G=A^{*}A$ 的第 $(i,j)$ 个元素是 $A$ 的第 $i$ 列和第 $j$ 列的内积：\n$$(A^{*}A)_{ij} = \\langle a_j, a_i \\rangle = \\sum_{k=1}^{m} \\overline{A_{ki}} A_{kj}$$\n问题没有指定非零元素的值。我们做一个标准的假设，即没有“意外抵消”发生。这意味着如果两列的支撑集有非空交集，它们的内积就非零。令 $S_k \\subset \\{1, 2, \\dots, m\\}$ 为 $A$ 的第 $k$ 列具有非零元素的行索引集合。根据定义， $|S_k| = s$。假设 $(A^{*}A)_{ij} \\neq 0$ 当且仅当 $S_i \\cap S_j \\neq \\emptyset$。\n\n我们现在通过考虑索引 $i$ 和 $j$ 的两种情况来计算 $P((A^{*}A)_{ij} \\neq 0) = P(S_i \\cap S_j \\neq \\emptyset)$。\n\n情况 1：$i = j$（对角线元素）。\n该元素是 $(A^{*}A)_{ii} = \\langle a_i, a_i \\rangle = \\|a_i\\|_2^2$。由于列 $a_i$ 有 $s \\ge 1$ 个非零元素，其欧几里得范数的平方 $\\|a_i\\|_2^2$ 严格为正。因此，$A^{*}A$ 的对角线元素总是非零的。\n$$P((A^{*}A)_{ii} \\neq 0) = 1$$\n有 $n$ 个这样的对角线元素。\n\n情况 2：$i \\neq j$（非对角线元素）。\n元素 $(A^{*}A)_{ij}$ 非零的概率是 $P(S_i \\cap S_j \\neq \\emptyset)$。计算其补事件的概率 $P(S_i \\cap S_j = \\emptyset)$ 更简单。支撑集 $S_i$ 和 $S_j$ 是从 $\\{1, \\dots, m\\}$ 的所有大小为 $s$ 的可能子集中独立且均匀随机选择的。\n\n选择支撑集 $S_j$ 的总方式数为 $\\binom{m}{s}$。为了使 $S_i$ 和 $S_j$ 不相交，$S_j$ 的 $s$ 个元素必须从不在 $S_i$ 中的 $m-s$ 个索引中选择。这样做的方式数为 $\\binom{m-s}{s}$。这仅在 $m-s \\ge s$ 时有效；否则，$\\binom{m-s}{s} = 0$，这正确地反映了根据鸽巢原理，重叠是必然的。\n\n支撑集不相交的概率是：\n$$P(S_i \\cap S_j = \\emptyset) = \\frac{\\text{Number of ways to choose } S_j \\text{ disjoint from } S_i}{\\text{Total number of ways to choose } S_j} = \\frac{\\binom{m-s}{s}}{\\binom{m}{s}}$$\n因此，非对角线元素非零的概率是：\n$$P((A^{*}A)_{ij} \\neq 0) = 1 - P(S_i \\cap S_j = \\emptyset) = 1 - \\frac{\\binom{m-s}{s}}{\\binom{m}{s}}$$\n有 $n(n-1)$ 个这样的非对角线元素。\n\n我们现在可以整合 $A^{*}A$ 中期望的非零元素数量：\n$$E[\\text{nnz}(A^{*}A)] = \\sum_{i=1}^{n} P((A^{*}A)_{ii} \\neq 0) + \\sum_{i \\neq j} P((A^{*}A)_{ij} \\neq 0)$$\n$$E[\\text{nnz}(A^{*}A)] = n \\cdot 1 + n(n-1) \\left(1 - \\frac{\\binom{m-s}{s}}{\\binom{m}{s}}\\right)$$\n\n最后，我们计算所求的比率 $R(n, m, s)$：\n$$R(n, m, s) = \\frac{E[C_{gram}]}{C_{seq}} = \\frac{E[\\text{nnz}(A^{*}A)]}{2ns}$$\n$$R(n, m, s) = \\frac{n + n(n-1) \\left(1 - \\frac{\\binom{m-s}{s}}{\\binom{m}{s}}\\right)}{2ns}$$\n我们可以通过消去一个因子 $n$ 来简化这个表达式：\n$$R(n, m, s) = \\frac{1 + (n-1) \\left(1 - \\frac{\\binom{m-s}{s}}{\\binom{m}{s}}\\right)}{2s}$$\n$$R(n, m, s) = \\frac{1 + (n-1) - (n-1)\\frac{\\binom{m-s}{s}}{\\binom{m}{s}}}{2s}$$\n$$R(n, m, s) = \\frac{n - (n-1)\\frac{\\binom{m-s}{s}}{\\binom{m}{s}}}{2s}$$\n这就是所求期望比率的最终、精确的闭式表达式。", "answer": "$$\n\\boxed{\\frac{n - (n-1)\\frac{\\binom{m-s}{s}}{\\binom{m}{s}}}{2s}}\n$$", "id": "3457662"}, {"introduction": "前一个练习说明了避免形成 $A^*A$ 的重要性。这就提出了一个实际问题：我们如何在不构建 $A^*A$ 的情况下，计算与其相关的量，例如它的最大特征值（它定义了 $A$ 的算子范数）？本练习要求你设计并论证幂迭代算法 [@problem_id:3457681]，这正是一种用于此目的的经典方法。通过实现一个仅依赖于算子 $A$ 及其伴随算子 $A^*$ 作用的估计器，你将把算子的理论属性与高效的实际计算联系起来。", "problem": "设 $A : \\mathbb{R}^n \\to \\mathbb{R}^m$ 是一个在赋有标准内积和诱导欧几里得范数的有限维欧几里得空间之间的线性算子。其伴随算子 $A^* : \\mathbb{R}^m \\to \\mathbb{R}^n$ 由关系 $\\langle A x, y \\rangle = \\langle x, A^* y \\rangle$ 对所有 $x \\in \\mathbb{R}^n$ 和 $y \\in \\mathbb{R}^m$ 定义。$A$ 的谱范数为 $\\|A\\|_2 = \\sup_{x \\neq 0} \\frac{\\|A x\\|_2}{\\|x\\|_2}$。\n\n您的任务是设计并实现一个幂迭代估计器，仅通过重复应用 $A$ 和 $A^*$ 来近似 $\\|A\\|_2$，而不显式地构造 $A^* A$，也不使用 $A$ 的任何分解。设计必须从第一性原理出发进行论证：从伴随算子和谱范数的定义开始，并且只使用公认的事实，如奇异值分解（SVD）的存在性及其性质，以及对称半正定算子的谱定理。您的算法必须：\n- 对 $A$ 和 $A^*$ 的函数句柄进行操作，这些句柄实现了算子及其伴随算子对向量的作用。\n- 使用随机化的非零初始化以避免与零空间的对抗性对齐，并使用固定种子以保证可复现性。\n- 包含一个基于相对变化低于容差 $\\varepsilon$ 的停止准则，以及一个最大迭代次数上限 $K$。\n- 如果对单位范数向量首次应用 $A$ 得到零向量，则通过返回 $0$ 来稳健地检测零算子。\n\n在压缩感知和稀疏优化的背景下，许多传感算子仅通过其自身的作用及其伴随算子的作用来使用（例如，子采样的正交变换和随机高斯矩阵）。您将使用以下测试套件来验证您的估计器。在每种情况下，将 $A$ 和 $A^*$ 实现为函数，并使用数学上合理的方法计算一个独立的基准谱范数。在所有情况下，为估计器使用以下固定参数：容差 $\\varepsilon = 10^{-12}$，最大迭代次数 $K = 1000$，以及按情况指定的初始化种子。所有输出都应为不带物理单位的实数。\n\n测试套件：\n- 情况 1（稠密高斯矩阵，矩形）：设 $m = 8$，$n = 5$，并使用伪随机种子 $12345$ 构造一个矩阵 $G \\in \\mathbb{R}^{m \\times n}$，其元素为独立的标准正态分布。定义 $A(x) = G x$ 和 $A^*(y) = G^\\top y$。基准谱范数计算为矩阵 $G$ 的 2-范数。\n- 情况 2（零算子）：设 $m = 3$，$n = 7$，并定义 $A(x) = 0 \\in \\mathbb{R}^m$ 和 $A^*(y) = 0 \\in \\mathbb{R}^n$。基准谱范数为 $0$。\n- 情况 3（子采样正交变换）：设 $n = 64$，并考虑正交归一化的 2 型离散余弦变换（DCT）$F : \\mathbb{R}^n \\to \\mathbb{R}^n$。设 $S \\subset \\{0,1,\\dots,n-1\\}$ 为偶数索引 $S = \\{0,2,4,\\dots,62\\}$，并定义 $A(x) = (F x)_S$（选择 $S$ 中的坐标），$A^*(y)$ 则是对应的伴随作用，即在索引 $S$ 处补零回到长度 $n$ 后再进行逆变换。对于此子采样正交算子，基准谱范数为 $1$。\n- 情况 4（对角算子）：设 $n = m = 4$，并定义 $A$ 为对角元素为 $\\{0.1, 2.5, -3.7, 1.2\\}$ 的对角算子。则 $A^* = A$，基准谱范数是对角元素绝对值的最大值，即 $3.7$。\n- 情况 5（缩放的稠密高斯矩阵）：设 $m = 10$，$n = 4$。使用伪随机种子 $2023$ 构造一个矩阵 $H \\in \\mathbb{R}^{m \\times n}$，其元素为独立的标准正态分布，并设 $c = 7.3$。定义 $A(x) = (c H) x$ 和 $A^*(y) = (c H)^\\top y$。基准谱范数是矩阵 $c H$ 的 2-范数。\n\n对于每种情况，使用以下随机起始向量的初始化种子运行您的估计器：情况 1 使用种子 $0$，情况 2 使用种子 $1$，情况 3 使用种子 $2$，情况 4 使用种子 $3$，情况 5 使用种子 $4$。使用绝对容差 $\\delta = 10^{-8}$ 将您的估计值 $\\widehat{\\|A\\|_2}$ 与基准值 $\\|A\\|_2$ 进行比较，并生成一个布尔结果，当且仅当 $|\\widehat{\\|A\\|_2} - \\|A\\|_2| \\le \\delta$ 时为真。\n\n最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,result3,result4,result5]\"），其中每个结果是按情况 1 到 5 的顺序对应的布尔结果。", "solution": "任务是为线性算子 $A : \\mathbb{R}^n \\to \\mathbb{R}^m$ 的谱范数 $\\|A\\|_2$ 设计并实现一个基于幂迭代的估计器，该估计器使用 $A$ 及其伴随算子 $A^*$ 的函数句柄，而不访问它们的矩阵表示。设计必须从第一性原理出发进行论证。\n\n**1. 理论基础**\n\n有限维欧几里得空间之间的线性算子 $A$ 的谱范数定义为：\n$$\n\\|A\\|_2 = \\sup_{x \\in \\mathbb{R}^n, x \\neq 0} \\frac{\\|Ax\\|_2}{\\|x\\|_2}\n$$\n根据定义，谱范数等于 $A$ 的最大奇异值，记为 $\\sigma_{\\max}(A)$。算子 $A$ 的奇异值 $\\sigma_i(A)$ 是算子 $A^*A$ 的特征值的非负平方根，其中 $A^*: \\mathbb{R}^m \\to \\mathbb{R}^n$ 是 $A$ 的伴随算子，由关系 $\\langle Ax, y \\rangle = \\langle x, A^*y \\rangle$ 对所有 $x \\in \\mathbb{R}^n$ 和 $y \\in \\mathbb{R}^m$ 定义。\n\n算子 $B = A^*A : \\mathbb{R}^n \\to \\mathbb{R}^n$ 是自伴随且半正定的：\n- **自伴随：** 对于任意 $x, z \\in \\mathbb{R}^n$，我们有 $\\langle Bx, z \\rangle = \\langle (A^*A)x, z \\rangle = \\langle Ax, Az \\rangle = \\langle x, A^*(Az) \\rangle = \\langle x, (A^*A)z \\rangle = \\langle x, Bz \\rangle$。\n- **半正定：** 对于任意 $x \\in \\mathbb{R}^n$，我们有 $\\langle Bx, x \\rangle = \\langle (A^*A)x, x \\rangle = \\langle Ax, Ax \\rangle = \\|Ax\\|_2^2 \\ge 0$。\n\n根据谱定理，有限维空间上的自伴随算子拥有一组由特征向量组成的完备正交基，且其对应的特征值为实数。对于像 $A^*A$ 这样的半正定算子，这些特征值是非负的。设 $A^*A$ 的特征值为 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$。$A$ 的奇异值与 $A^*A$ 的特征值之间的关系是 $\\sigma_i(A)^2 = \\lambda_i(A^*A)$。\n\n因此，$A$ 的谱范数的平方是 $A^*A$ 的最大特征值：\n$$\n\\|A\\|_2^2 = (\\sigma_{\\max}(A))^2 = \\lambda_{\\max}(A^*A)\n$$\n这便将寻找 $\\|A\\|_2$ 的问题简化为寻找算子 $A^*A$ 的最大特征值的平方根。\n\n**2. 算法设计：幂迭代法**\n\n幂迭代法是寻找一个算子模最大特征值的标准算法。当应用于像 $B = A^*A$ 这样的自伴随、半正定算子时，它会收敛到最大特征值 $\\lambda_{\\max}(B)$。\n\n算法过程如下：\n1. 从一个非零的初始向量 $v_0 \\in \\mathbb{R}^n$ 开始。为避免与对应较小特征值的特征空间发生对抗性对齐，$v_0$ 应随机选择。然后将其归一化为单位范数，即 $\\|v_0\\|_2 = 1$。\n2. 使用以下关系迭代生成向量序列：\n   $$\n   v_{k+1} = \\frac{B v_k}{\\|B v_k\\|_2} = \\frac{(A^*A) v_k}{\\| (A^*A) v_k \\|_2}\n   $$\n当 $k \\to \\infty$ 时，只要 $v_0$ 在对应 $\\lambda_{\\max}(B)$ 的特征向量方向上有非零分量（对于随机的 $v_0$，这以概率 1 成立），向量 $v_k$ 就会收敛到 $B$ 对应于 $\\lambda_{\\max}(B)$ 的单位范数特征向量。\n\n关键的洞见是，算子 $B = A^*A$ 对向量 $v$ 的应用可以分两个阶段执行，仅使用算子 $A$ 和 $A^*$：\n- 首先，计算 $u = A v$。\n- 其次，计算 $w = A^* u$。\n结果是 $w = A^*(Av) = (A^*A)v = Bv$。这避免了显式构造算子 $B$ 的需要，因为在大规模场景中，这通常计算成本高昂或不可行。\n\n相应的最大特征值 $\\lambda_{\\max}(B)$ 可以从迭代中估算。使用瑞利商，我们有：\n$$\n\\lambda_k = \\langle (A^*A) v_k, v_k \\rangle = \\langle A v_k, A v_k \\rangle = \\|A v_k\\|_2^2\n$$\n当 $v_k$ 收敛到主特征向量时，$\\lambda_k$ 收敛到 $\\lambda_{\\max}(A^*A)$。因此，标量序列 $s_k = \\|A v_k\\|_2$ 收敛到 $\\sqrt{\\lambda_{\\max}(A^*A)} = \\|A\\|_2$。这为谱范数提供了一个直接的估计。\n\n**3. 最终算法详述**\n\n基于以上原理，估计器设计如下：\n\n1.  **初始化**：\n    - 使用带种子的伪随机数生成器选择一个随机向量 $v \\in \\mathbb{R}^n$ 以保证可复现性。\n    - 归一化向量：$v \\leftarrow v / \\|v\\|_2$。\n    - 初始化前一次的范数估计值 $\\widehat{\\sigma}_{\\text{old}} = 0.0$。\n2.  **迭代**：对于 $k = 1, \\dots, K$（其中 $K$ 是最大迭代次数）：\n    a. 应用算子 $A$：$u = A(v)$。\n    b. 计算新的范数估计值：$\\widehat{\\sigma}_{\\text{new}} = \\|u\\|_2$。\n    c. **零算子检测**：若在 $k=1$ 时 $\\widehat{\\sigma}_{\\text{new}} = 0$，则算子 $A$ 为零算子。终止并返回 $0.0$。\n    d. **收敛性检查**：若 $\\widehat{\\sigma}_{\\text{new}} > 0$ 且相对变化低于容差 $\\varepsilon$，即 $|\\widehat{\\sigma}_{\\text{new}} - \\widehat{\\sigma}_{\\text{old}}| < \\varepsilon \\cdot \\widehat{\\sigma}_{\\text{new}}$，则估计已收敛。终止并返回 $\\widehat{\\sigma}_{\\text{new}}$。\n    e. 更新前一次的估计值：$\\widehat{\\sigma}_{\\text{old}} = \\widehat{\\sigma}_{\\text{new}}$。\n    f. 应用伴随算子 $A^*$：$w = A^*(u)$。\n    g. **稳定性检查**：若 $\\|w\\|_2 = 0$，迭代已收敛到 $A^*A$ 的零空间。终止并返回当前估计值 $\\widehat{\\sigma}_{\\text{new}}$。\n    h. 归一化得到下一次迭代的向量：$v \\leftarrow w / \\|w\\|_2$。\n3.  **终止**：如果循环在 $K$ 次迭代后仍未收敛，返回最终的估计值 $\\widehat{\\sigma}_{\\text{new}}$。\n\n此设计满足所有问题要求，提供了一种仅使用算子及其伴随算子应用来估计谱范数的稳健方法。", "answer": "```python\nimport numpy as np\nfrom scipy.fft import dct, idct\n\ndef power_iteration_estimator(A, A_star, n, K, epsilon, seed):\n    \"\"\"\n    Estimates the spectral norm of a linear operator A using power iteration.\n\n    Args:\n        A (callable): A function that applies the operator A to a vector.\n        A_star (callable): A function that applies the adjoint operator A* to a vector.\n        n (int): The dimension of the domain of A.\n        K (int): The maximum number of iterations.\n        epsilon (float): The tolerance for the stopping criterion.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        float: The estimated spectral norm of A.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    v = rng.standard_normal(n)\n    v_norm = np.linalg.norm(v)\n    if v_norm == 0.0:\n        # This is extremely unlikely but good practice to handle.\n        v = np.ones(n)\n        v_norm = np.linalg.norm(v)\n    v = v / v_norm\n\n    norm_est = 0.0\n    for k in range(K):\n        # Apply the operator A\n        u = A(v)\n        \n        # Calculate the new estimate for the spectral norm\n        norm_est_new = np.linalg.norm(u)\n\n        # Robustly detect the zero operator on the first iteration\n        if k == 0 and norm_est_new == 0.0:\n            return 0.0\n\n        # Check for convergence based on relative change\n        if norm_est_new > 0.0 and abs(norm_est_new - norm_est) < epsilon * norm_est_new:\n            return norm_est_new\n        \n        norm_est = norm_est_new\n\n        # Apply the adjoint operator A*\n        w = A_star(u)\n        norm_w = np.linalg.norm(w)\n\n        # If w is the zero vector, the algorithm has converged or hit a null space\n        if norm_w == 0.0:\n            return norm_est\n            \n        # Normalize to get the next iterate\n        v = w / norm_w\n    \n    return norm_est\n\ndef solve():\n    \"\"\"\n    Runs the test suite for the power iteration estimator.\n    \"\"\"\n    # Fixed parameters for the estimator\n    EPSILON = 1e-12\n    K = 1000\n    # Absolute tolerance for comparison with ground truth\n    DELTA = 1e-8\n\n    results = []\n\n    # --- Case 1: Dense Gaussian, rectangular ---\n    m1, n1, seed_G, seed_est_1 = 8, 5, 12345, 0\n    rng1 = np.random.default_rng(seed_G)\n    G = rng1.standard_normal((m1, n1))\n    A1 = lambda x: G @ x\n    A1_star = lambda y: G.T @ y\n    gt_1 = np.linalg.norm(G, 2)\n    est_1 = power_iteration_estimator(A1, A1_star, n1, K, EPSILON, seed_est_1)\n    results.append(abs(est_1 - gt_1) <= DELTA)\n\n    # --- Case 2: Zero operator ---\n    m2, n2, seed_est_2 = 3, 7, 1\n    A2 = lambda x: np.zeros(m2)\n    A2_star = lambda y: np.zeros(n2)\n    gt_2 = 0.0\n    est_2 = power_iteration_estimator(A2, A2_star, n2, K, EPSILON, seed_est_2)\n    results.append(abs(est_2 - gt_2) <= DELTA)\n\n    # --- Case 3: Subsampled orthonormal transform (DCT) ---\n    n3, seed_est_3 = 64, 2\n    S = np.arange(0, n3, 2, dtype=int)\n    A3 = lambda x: dct(x, type=2, norm='ortho')[S]\n    def A3_star(y):\n        z = np.zeros(n3)\n        z[S] = y\n        return idct(z, type=2, norm='ortho')\n    gt_3 = 1.0\n    est_3 = power_iteration_estimator(A3, A3_star, n3, K, EPSILON, seed_est_3)\n    results.append(abs(est_3 - gt_3) <= DELTA)\n\n    # --- Case 4: Diagonal operator ---\n    n4, seed_est_4 = 4, 3\n    d = np.array([0.1, 2.5, -3.7, 1.2])\n    A4 = lambda x: d * x\n    A4_star = A4  # Real diagonal operator is self-adjoint\n    gt_4 = np.max(np.abs(d))\n    est_4 = power_iteration_estimator(A4, A4_star, n4, K, EPSILON, seed_est_4)\n    results.append(abs(est_4 - gt_4) <= DELTA)\n\n    # --- Case 5: Scaled dense Gaussian ---\n    m5, n5, seed_H, seed_est_5 = 10, 4, 2023, 4\n    c = 7.3\n    rng5 = np.random.default_rng(seed_H)\n    H = rng5.standard_normal((m5, n5))\n    cH = c * H\n    A5 = lambda x: cH @ x\n    A5_star = lambda y: cH.T @ y\n    gt_5 = np.linalg.norm(cH, 2)\n    est_5 = power_iteration_estimator(A5, A5_star, n5, K, EPSILON, seed_est_5)\n    results.append(abs(est_5 - gt_5) <= DELTA)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3457681"}]}