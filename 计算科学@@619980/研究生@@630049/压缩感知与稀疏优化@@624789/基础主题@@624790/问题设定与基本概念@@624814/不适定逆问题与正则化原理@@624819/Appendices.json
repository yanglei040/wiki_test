{"hands_on_practices": [{"introduction": "理论学习最好通过实践来巩固。本节将通过一系列动手练习，带您深入探索正则化方法的核心思想。我们将从两种经典的正则化技术——吉洪诺夫（Tikhonov）正则化与截断奇异值分解（TSVD）——的直接比较开始。这两种方法都可以被看作是对算子奇异值谱的“滤波”操作，旨在抑制小奇异值对噪声的放大效应。通过一个精心设计的数值实验 [@problem_id:3599470]，您将具体分析吉洪诺夫正则化的平滑滤波与TSVD的硬截断在不同频谱结构下的表现差异，从而深刻理解它们各自的优势与局限性。", "problem": "考虑在数值线性代数的意义下求解一个线性逆问题：给定一个矩阵 $A \\in \\mathbb{R}^{n \\times n}$、一个未知向量 $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$、确定性数据噪声 $\\eta \\in \\mathbb{R}^{n}$ 以及观测数据 $b = A x_{\\mathrm{true}} + \\eta$，比较两种正则化策略：Tikhonov 正则化和截断奇异值分解。目标是计算这两种策略在一组测试用例中的相对解误差，并突出一个由于谱隙的存在，截断奇异值分解性能优于 Tikhonov 正则化的设计。\n\n基本定义和要求：\n- 对于正则化参数 $\\lambda > 0$ 的 Tikhonov 正则化，其解是严格凸目标函数 $\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$ 的最小化子，也就是正规方程 $(A^{\\top} A + \\lambda I) x_{\\mathrm{tik}} = A^{\\top} b$ 的唯一解。\n- 对于截断奇异值分解 (TSVD)，定义奇异值分解 $A = U \\Sigma V^{\\top}$，其中奇异值为 $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{n} \\ge 0$。对于一个截断指数 $k \\in \\{0,1,\\dots,n\\}$，TSVD 解为 $x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i}$。在此问题中，截断指数 $k$ 由规则 $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$ 决定，并约定如果该集合为空，则 $k = 0$。\n- 候选解 $\\widehat{x}$ 相对于 $x_{\\mathrm{true}}$ 的相对误差为 $\\|\\widehat{x} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$。\n\n为可测试性设定的实现约束和特例：\n- 在所有测试用例中，取 $n = 10$ 并选择 $A$ 为对角矩阵，其对角线元素为降序排列的正数，因此 $U = I$，$V = I$，奇异值即为 $A$ 的对角线元素。这使得 $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$ 且 $A^{\\top} A = \\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{n}^{2})$。\n- 对每个测试用例，确定性地定义噪声向量为 $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$，其中 $i = 1,2,\\dots,n$。\n- 对于 Tikhonov 正则化，从此对角设定下的正规方程导出的显式分量表达式为 $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i}$，其中 $i = 1,2,\\dots,n$。\n- 对于 TSVD，给定由上述选择规则确定的 $k$，在此对角设定下的显式分量表达式为 $x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i},  i \\le k, \\\\ 0,  i > k. \\end{cases}$\n\n测试套件：\n- 测试用例 #1 (谱隙；旨在使截断奇异值分解表现更优): \n  - $n = 10$.\n  - 奇异值 $\\sigma = [1.0, 0.6, 0.36, 0.216, 0.1296, 10^{-3}, 5 \\cdot 10^{-4}, 2 \\cdot 10^{-4}, 10^{-4}, 5 \\cdot 10^{-5}]$。\n  - 真实解 $x_{\\mathrm{true}} = [1, -\\tfrac{1}{2}, \\tfrac{1}{4}, -\\tfrac{1}{8}, \\tfrac{1}{16}, 0, 0, 0, 0, 0]$。\n  - 正则化参数 $\\lambda = 10^{-5}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n- 测试用例 #2 (平滑谱；无明显谱隙):\n  - $n = 10$.\n  - 奇异值 $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$，其中 $i = 1,2,\\dots,10$。\n  - 真实解 $x_{\\mathrm{true},i} = 0.8^{i-1}$，其中 $i = 1,2,\\dots,10$。\n  - 正则化参数 $\\lambda = 10^{-4}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n- 测试用例 #3 (极小正则化下的边界条件):\n  - $n = 10$.\n  - 奇异值 $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$，其中 $i = 1,2,\\dots,10$。\n  - 真实解 $x_{\\mathrm{true},i} = 0.8^{i-1}$，其中 $i = 1,2,\\dots,10$。\n  - 正则化参数 $\\lambda = 10^{-12}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n- 测试用例 #4 (极大正则化下的边界条件):\n  - $n = 10$.\n  - 奇异值 $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$，其中 $i = 1,2,\\dots,10$。\n  - 真实解 $x_{\\mathrm{true},i} = 0.8^{i-1}$，其中 $i = 1,2,\\dots,10$。\n  - 正则化参数 $\\lambda = 10^{1}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n\n每个测试用例需要实现的任务：\n- 构造 $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$、$x_{\\mathrm{true}}$、$\\eta$（其中 $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$）以及 $b = A x_{\\mathrm{true}} + \\eta$。\n- 使用 $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i}$ 计算 Tikhonov 解 $x_{\\mathrm{tik}}$。\n- 计算截断指数 $k = \\max\\{ i : \\sigma_{i}^{2} \\ge \\lambda \\}$，并约定若集合为空则 $k = 0$。\n- 使用 $x_{\\mathrm{tsvd},i}^{(k)} = b_{i}/\\sigma_{i}$ (当 $i \\le k$) 和 $x_{\\mathrm{tsvd},i}^{(k)} = 0$ (当 $i > k$) 计算截断奇异值分解解 $x_{\\mathrm{tsvd}}^{(k)}$。\n- 计算相对 $2$-范数误差 $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ 和 $e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$。\n- 同时计算布尔值 $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}}  e_{\\mathrm{tik}})$，以指示在该测试中截断奇异值分解是否严格优于 Tikhonov 正则化。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。每个测试用例贡献一个形如 $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$ 的列表。因此，最终输出应为一个包含四个内部列表的列表，按测试用例 #1 到 #4 的顺序排列，例如 $[[k_{1}, e_{\\mathrm{tsvd},1}, e_{\\mathrm{tik},1}, b_{\\mathrm{adv},1}], [k_{2}, e_{\\mathrm{tsvd},2}, e_{\\mathrm{tik},2}, b_{\\mathrm{adv},2}], [k_{3}, e_{\\mathrm{tsvd},3}, e_{\\mathrm{tik},3}, b_{\\mathrm{adv},3}], [k_{4}, e_{\\mathrm{tsvd},4}, e_{\\mathrm{tik},4}, b_{\\mathrm{adv},4}]]$。\n\n注意：\n- 没有物理单位；所有计算均在无量纲的浮点运算中进行。\n- 不使用角度。\n- 将所有布尔值表示为语言原生的布尔值，所有误差表示为浮点数。", "solution": "所提出的问题要求比较两种用于求解不适定线性逆问题的标准正则化技术：Tikhonov 正则化和截断奇异值分解 (TSVD)。验证证实了该问题在科学上是合理的、适定的，并为数值实验提供了一套清晰、独立的指令和数据。所有定义和公式均与数值线性代数领域的既有文献一致。因此，我们可以着手求解。\n\n问题的核心在于求解线性系统 $A x = b$，其中矩阵 $A$ 是病态的，数据向量 $b$ 受到噪声 $\\eta$ 的污染。模型由 $b = A x_{\\mathrm{true}} + \\eta$ 给出，其中 $x_{\\mathrm{true}}$ 是我们试图逼近的真实解。通过 $x = A^{-1} b$ 求解 $x$ 的简单尝试将得到 $x = x_{\\mathrm{true}} + A^{-1} \\eta$。由于 $A$ 是病态的，其逆矩阵 $A^{-1}$ 具有非常大的范数，导致噪声项 $\\eta$ 被极度放大。正则化方法旨在通过向解中引入受控偏差，以换取由噪声引起的方差的显著减小，来抵消这种影响。\n\n该问题通过考虑一个对角矩阵 $A = \\mathrm{diag}(\\sigma_{1}, \\dots, \\sigma_{n})$ 来简化分析，其中对角线元素 $\\sigma_i  0$ 是 $A$ 的奇异值。在一般情况下，任何矩阵 $A$ 都有一个奇异值分解 (SVD) $A = U \\Sigma V^{\\top}$，其中 $U$ 和 $V$ 是正交矩阵，$\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$。选择一个对角矩阵 $A$ 等价于在一个 SVD 是平凡的（$U=V=I$）基中工作，这使我们能专注于每种正则化方法如何处理奇异值本身。\n\n**Tikhonov 正则化**\n\nTikhonov 正则化将问题重塑为一个优化问题，寻求一个解 $x$ 来最小化数据保真项和解范数惩罚项的组合：\n$$ x_{\\mathrm{tik}} = \\arg\\min_{x} \\left( \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2} \\right) $$\n参数 $\\lambda  0$ 控制着这种权衡。唯一的最小化子 $x_{\\mathrm{tik}}$ 通过求解相关的正规方程得到：$(A^{\\top} A + \\lambda I) x_{\\mathrm{tik}} = A^{\\top} b$。对于我们的对角矩阵 $A=\\mathrm{diag}(\\sigma_i)$，该系统解耦为 $n$ 个标量方程：\n$$ (\\sigma_{i}^{2} + \\lambda) x_{\\mathrm{tik},i} = \\sigma_{i} b_{i} \\quad \\text{for } i \\in \\{1, \\dots, n\\} $$\n这给出了 Tikhonov 解的显式分量式：\n$$ x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i} $$\n项 $f_{i}^{\\mathrm{tik}} = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda}$ 可被视为一个“滤波因子”。它平滑地衰减与小奇异值相关的分量。如果 $\\sigma_i^2 \\gg \\lambda$，则 $f_{i}^{\\mathrm{tik}} \\approx 1$，分量基本不变。如果 $\\sigma_i^2 \\ll \\lambda$，则 $f_{i}^{\\mathrm{tik}} \\approx 0$，分量被抑制。\n\n**截断奇异值分解 (TSVD)**\n\nTSVD 采用更直接的方法，仅使用“重要的”奇异分量来构造解。一般的 TSVD 解由一个截断和给出：\n$$ x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i} $$\n其中 $k$ 是截断指数，决定了包含多少个分量。在我们的对角情况下 ($U=I, V=I$)，这简化为：\n$$ x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i}  \\text{if } i \\le k \\\\ 0  \\text{if } i  k \\end{cases} $$\n这对应于形成一个阶跃函数的滤波因子 $f_{i}^{\\mathrm{tsvd}}$：当 $i \\le k$ 时 $f_{i}^{\\mathrm{tsvd}} = 1$，当 $i  k$ 时 $f_{i}^{\\mathrm{tsvd}} = 0$。该问题通过规则 $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$ 将 $k$ 的选择与 Tikhonov 参数 $\\lambda$ 联系起来，并约定若集合为空则 $k=0$。该规则基本上保留了所有 Tikhonov 滤波因子至少为 $1/2$ 的分量。\n\n**比较与谱隙的作用**\n\n根本区别在于它们的滤波函数：Tikhonov 的是平滑的，而 TSVD 的是锐截止的。测试用例 #1 旨在突出 TSVD 的锐截止具有优势的场景。它具有一个“谱隙”，其中奇异值在 $\\sigma_5$ 和 $\\sigma_6$ 之间出现大幅下降。正则化参数 $\\lambda=10^{-5}$ 被选择落在该谱隙内（即 $\\sigma_5^2 \\gg \\lambda \\gg \\sigma_6^2$）。此外，真实解 $x_{\\mathrm{true}}$ 的信息内容仅限于前 5 个分量。\n\n在这些条件下，TSVD 的截断规则得出 $k=5$。因此，TSVD 保留了前 5 个分量（信号所在之处），并完全丢弃了剩余的分量（由于当 $i5$ 时 $x_{\\mathrm{true},i}=0$，这些分量只包含噪声）。对于这种特定的问题结构，这起到了完美滤波器的作用。相比之下，Tikhonov 正则化将其平滑滤波器应用于所有分量。虽然它严重抑制了 $i  5$ 的分量，但它仍然允许少量经过滤波的噪声通过。更重要的是，它也轻微地衰减了 $i \\le 5$ 的分量，引入了 TSVD 在这些分量上没有的正则化误差。这导致 TSVD 的性能优于 Tikhonov。\n\n对于其他谱衰减更平滑的测试用例，TSVD 的锐截止可能是有害的。如果 $x_{\\mathrm{true}}$ 在被 TSVD 截断的分量中包含显著能量（因为它们的 $\\sigma_i$ 很小），它将产生大的正则化误差。Tikhonov 对这些分量的温和衰减可以得到更好的整体近似。\n\n**计算步骤**\n\n对于四个测试用例中的每一个，执行以下步骤：\n1.  初始化参数：$n=10$、奇异值 $\\sigma$、真实解 $x_{\\mathrm{true}}$、正则化参数 $\\lambda$ 和噪声水平。\n2.  构造噪声向量 $\\eta$，其中对于基于 0 的索引 $j \\in \\{0, \\dots, 9\\}$，$\\eta_{j} = \\mathrm{noise\\_level} \\cdot (-1)^{j+1}$。\n3.  计算数据向量 $b = \\sigma \\odot x_{\\mathrm{true}} + \\eta$，其中 $\\odot$ 表示逐元素乘法。\n4.  使用其分量式公式计算 Tikhonov 解向量 $x_{\\mathrm{tik}}$。\n5.  根据提供的规则确定 TSVD 截断指数 $k$。\n6.  计算 TSVD 解向量 $x_{\\mathrm{tsvd}}^{(k)}$。\n7.  计算两种解的相对 $2$-范数误差：$e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ 和 $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$。\n8.  评估布尔条件 $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}}  e_{\\mathrm{tik}})$。\n然后报告每个案例收集到的结果 $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_test_case(sigma_vals, xtrue_vals, lam, noise_level):\n    \"\"\"\n    Runs a single test case for comparing Tikhonov and TSVD regularization.\n    \"\"\"\n    n = 10\n    sigma = np.array(sigma_vals, dtype=float)\n    xtrue = np.array(xtrue_vals, dtype=float)\n\n    # Construct the noise vector eta and observed data b\n    # The problem uses 1-based indexing i=1,...,n. Python uses 0-based j=0,...,n-1.\n    # eta_i = noise_level * (-1)^i translates to eta[j] = noise_level * (-1)**(j+1)\n    indices_1_based = np.arange(1, n + 1)\n    eta = noise_level * ((-1) ** indices_1_based)\n    b = sigma * xtrue + eta\n\n    # Compute the Tikhonov solution\n    xtik = (sigma / (sigma**2 + lam)) * b\n\n    # Determine the TSVD truncation index k\n    # k = max{ i in {1..n} : sigma_i^2 = lam }\n    # np.where returns 0-based indices. k needs to be a 1-based count.\n    valid_indices = np.where(sigma**2 = lam)[0]\n    if len(valid_indices) == 0:\n        k = 0\n    else:\n        k = int(np.max(valid_indices) + 1)\n\n    # Compute the TSVD solution\n    xtsvd = np.zeros(n)\n    if k  0:\n        # Slicing with :k works correctly for 0-based index up to k-1.\n        xtsvd[:k] = b[:k] / sigma[:k]\n\n    # Compute the relative 2-norm errors\n    norm_xtrue = np.linalg.norm(xtrue)\n    \n    # This problem guarantees norm_xtrue  0, so no division-by-zero check is needed.\n    e_tsvd = np.linalg.norm(xtsvd - xtrue) / norm_xtrue\n    e_tik = np.linalg.norm(xtik - xtrue) / norm_xtrue\n\n    # Determine if TSVD has a strictly smaller error\n    b_adv = bool(e_tsvd  e_tik)\n    \n    return [k, e_tsvd, e_tik, b_adv]\n\ndef solve():\n    \"\"\"\n    Defines and runs the four test cases, then prints the results.\n    \"\"\"\n    # Test case #1: Spectral gap\n    case1 = {\n        \"sigma_vals\": [1.0, 0.6, 0.36, 0.216, 0.1296, 1e-3, 5e-4, 2e-4, 1e-4, 5e-5],\n        \"xtrue_vals\": [1.0, -0.5, 0.25, -0.125, 0.0625, 0, 0, 0, 0, 0],\n        \"lam\": 1e-5,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #2: Smooth spectrum\n    n = 10\n    j_indices = np.arange(n)\n    sigma_smooth = 10**(-j_indices / 3.0)\n    xtrue_smooth = 0.8**j_indices\n    case2 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e-4,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #3: Small regularization parameter\n    case3 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e-12,\n        \"noise_level\": 1e-6\n    }\n\n    # Test case #4: Large regularization parameter\n    case4 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e1,\n        \"noise_level\": 1e-6\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        result = run_test_case(\n            case[\"sigma_vals\"],\n            case[\"xtrue_vals\"],\n            case[\"lam\"],\n            case[\"noise_level\"]\n        )\n        results.append(result)\n\n    # Print in the specified format: [[k1, e_tsvd1, e_tik1, b_adv1], [k2, ...]]\n    # Python's default string representation for a list of lists matches the required format.\n    print(results)\n\nsolve()\n```", "id": "3599470"}, {"introduction": "在现代信号处理和统计学中，稀疏性已成为解决不适定逆问题的核心原则。$L_1$ 正则化是推广稀疏性的关键工具，其在去噪问题中的解对应于软阈值算子。然而，一个关键的实践挑战是如何选择正则化参数 $\\lambda$。此练习 [@problem_id:3452179] 引入了一个强大的统计工具——斯坦无偏风险估计（SURE），它允许我们在不知道真实信号的情况下，对均方误差进行无偏估计。通过推导并最小化SURE，您将学习如何以一种纯粹由数据驱动的方式，为软阈值去噪自动选择最优的 $\\lambda$。", "problem": "考虑一个在正交稀疏变换域中去噪的典型不适定反问题，其中正向算子是单位算子，观测模型是加性高斯白噪声。设观测向量为 $y \\in \\mathbb{R}^{n}$，且 $y = x^{\\star} + \\varepsilon$，其中噪声满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$。我们通过对分量逐个应用阈值为 $\\lambda \\ge 0$ 的软阈值算子来估计 $x^{\\star}$：对于 $t \\in \\mathbb{R}$，\n$$\n\\eta_{\\lambda}(t) = \\operatorname{sign}(t) \\,\\max\\{|t| - \\lambda, 0\\}.\n$$\n这等价于 $\\ell_1$ 正则化子的近端映射，并对应于为稀疏性量身定制的类 Tikhonov 正则化。\n\n从 Stein 引理和 Stein 无偏风险估计（SURE）的定义出发，通过显式计算 $\\eta_{\\lambda}(\\cdot)$ 的散度项，推导出软阈值估计器 $\\eta_{\\lambda}(y)$ 的估计均方误差 $\\operatorname{SURE}(\\lambda)$ 作为 $\\lambda$ 的函数的显式表达式。然后，使用此表达式，通过最小化 $\\operatorname{SURE}(\\lambda)$ 来为以下数据选择 $\\lambda$：\n$$\nn = 6,\\quad \\sigma = 0.5,\\quad y = \\begin{pmatrix} 3.0 \\\\ -2.6 \\\\ 1.1 \\\\ -0.9 \\\\ 0.7 \\\\ 0.2 \\end{pmatrix}.\n$$\n给出最小化估计均方误差的 SURE 最优阈值 $\\lambda^{\\star}$ 的值。将您的答案四舍五入至四位有效数字。将最终答案表示为一个不带单位的实数。", "solution": "本题要求推导软阈值估计器的 Stein 无偏风险估计 (SURE) 的显式表达式，并应用它为给定的数据集找到一个最优阈值 $\\lambda^{\\star}$。\n\n首先，我们验证问题陈述。\n\n### 第1步：提取已知条件\n- **观测模型**：$y = x^{\\star} + \\varepsilon \\in \\mathbb{R}^{n}$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$。\n- **估计器**：软阈值算子 $\\eta_{\\lambda}(y)$，逐分量应用。对于标量 $t \\in \\mathbb{R}$ 和阈值 $\\lambda \\ge 0$，该算子定义为 $\\eta_{\\lambda}(t) = \\operatorname{sign}(t) \\,\\max\\{|t| - \\lambda, 0\\}$。\n- **目标**：使用 SURE 选择最优阈值 $\\lambda^{\\star}$。\n- **任务1**：推导 $\\operatorname{SURE}(\\lambda)$ 的显式表达式。\n- **任务2**：最小化 $\\operatorname{SURE}(\\lambda)$ 以找到给定数据的 $\\lambda^{\\star}$。\n- **数据**：$n = 6$，$\\sigma = 0.5$，观测向量 $y = \\begin{pmatrix} 3.0 \\\\ -2.6 \\\\ 1.1 \\\\ -0.9 \\\\ 0.7 \\\\ 0.2 \\end{pmatrix}$。\n- **输出要求**：将最终答案 $\\lambda^{\\star}$ 四舍五入到四位有效数字。\n\n### 第2步：使用提取的已知条件进行验证\n该问题具有科学依据，是适定且客观的。它是统计信号处理和稀疏恢复中的一个标准问题，基于公认的 Stein 引理和使用 SURE 进行模型选择。所提供的数据和参数是完整且一致的。未发现任何缺陷。\n\n### 第3步：结论与行动\n问题有效。我们继续进行求解。\n\n### 软阈值的 SURE 公式推导\n\nStein 无偏风险估计为在加性高斯噪声中观测到的向量 $x^{\\star}$ 的估计器 $\\hat{x}(y)$ 提供了均方误差 (MSE) $\\mathbb{E}[\\|\\hat{x}(y) - x^{\\star}\\|_2^2]$ 的一个估计。对于观测模型 $y = x^{\\star} + \\varepsilon$ (其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$) 和一个弱可微的估计器 $\\hat{x}(y)$，SURE 由下式给出：\n$$\n\\operatorname{SURE}(y; \\hat{x}) = \\|y - \\hat{x}(y)\\|_2^2 - n\\sigma^2 + 2\\sigma^2 \\nabla \\cdot \\hat{x}(y)\n$$\n其中 $\\nabla \\cdot \\hat{x}(y)$ 是函数 $\\hat{x}(\\cdot)$ 在 $y$ 点的散度。\n\n在我们的情况下，估计器是软阈值算子 $\\hat{x}(y) = \\eta_{\\lambda}(y)$，它是逐分量应用的：$\\hat{x}_i(y) = \\eta_{\\lambda}(y_i)$。SURE 公式是阈值 $\\lambda$ 的函数。\n我们需要计算涉及 $\\eta_{\\lambda}(y)$ 的两项：残差的平方范数 $\\|y - \\eta_{\\lambda}(y)\\|_2^2$ 和散度 $\\nabla \\cdot \\eta_{\\lambda}(y)$。\n\n1.  **散度项**：\n    散度是估计器各分量偏导数的和：\n    $$\n    \\nabla \\cdot \\eta_{\\lambda}(y) = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial y_i} \\eta_{\\lambda}(y_i)\n    $$\n    软阈值函数 $\\eta_{\\lambda}(t)$ 可以分段写出：\n    $$\n    \\eta_{\\lambda}(t) = \\begin{cases} t - \\lambda  \\text{若 } t  \\lambda \\\\ 0  \\text{若 } -\\lambda \\le t \\le \\lambda \\\\ t + \\lambda  \\text{若 } t  -\\lambda \\end{cases}\n    $$\n    这个函数是弱可微的。它关于 $t$ 的弱导数是一个指示函数：\n    $$\n    \\frac{d}{dt}\\eta_{\\lambda}(t) = \\mathbb{I}(|t|  \\lambda)\n    $$\n    其中 $\\mathbb{I}(\\cdot)$ 是指示函数，如果条件为真，其值为1，否则为0。\n    因此，散度是 $y$ 的分量中绝对值超过阈值 $\\lambda$ 的数量：\n    $$\n    \\nabla \\cdot \\eta_{\\lambda}(y) = \\sum_{i=1}^{n} \\mathbb{I}(|y_i|  \\lambda)\n    $$\n\n2.  **残差范数项**：\n    第 $i$ 个分量的残差是 $y_i - \\eta_{\\lambda}(y_i)$。\n    $$\n    y_i - \\eta_{\\lambda}(y_i) = \\begin{cases} y_i - (y_i - \\lambda) = \\lambda  \\text{若 } y_i  \\lambda \\\\ y_i - 0 = y_i  \\text{若 } |y_i| \\le \\lambda \\\\ y_i - (y_i + \\lambda) = -\\lambda  \\text{若 } y_i  -\\lambda \\end{cases}\n    $$\n    这可以紧凑地写为，若 $|y_i|  \\lambda$，则 $y_i - \\eta_{\\lambda}(y_i) = \\operatorname{sign}(y_i)\\lambda$；若 $|y_i| \\le \\lambda$，则为 $y_i$。\n    对第 $i$ 个分量的残差求平方得到：\n    $$\n    (y_i - \\eta_{\\lambda}(y_i))^2 = \\begin{cases} \\lambda^2  \\text{若 } |y_i|  \\lambda \\\\ y_i^2  \\text{若 } |y_i| \\le \\lambda \\end{cases}\n    $$\n    这等价于 $\\min(y_i^2, \\lambda^2)$。\n    残差向量的 $\\ell_2$ 范数的平方是所有分量的和：\n    $$\n    \\|y - \\eta_{\\lambda}(y)\\|_2^2 = \\sum_{i=1}^{n} \\min(y_i^2, \\lambda^2)\n    $$\n\n3.  **完整的 SURE 表达式**：\n    将散度项和残差项代入通用的 SURE 公式，我们得到估计的 MSE 作为 $\\lambda$ 的函数的显式表达式：\n    $$\n    \\operatorname{SURE}(\\lambda) = \\sum_{i=1}^{n} \\min(y_i^2, \\lambda^2) - n\\sigma^2 + 2\\sigma^2 \\sum_{i=1}^{n} \\mathbb{I}(|y_i|  \\lambda)\n    $$\n\n### 对给定数据最小化 SURE\n\n给定 $n=6$，$\\sigma=0.5$ (因此 $\\sigma^2=0.25$)，以及 $y = (3.0, -2.6, 1.1, -0.9, 0.7, 0.2)^T$。\n函数 $\\operatorname{SURE}(\\lambda)$ 在 $\\lambda$ 上是连续的。它关于 $\\lambda$ 的导数是分段定义的。设 $y$ 的各分量绝对值排序后为 $z_1 \\le z_2 \\le \\dots \\le z_n$。对于任何区间 $\\lambda \\in (z_k, z_{k+1})$，项 $\\sum_{i=1}^{n} \\mathbb{I}(|y_i|  \\lambda)$ 是常数，且 $\\sum_{i=1}^{n} \\min(y_i^2, \\lambda^2) = C + (\\text{常数}) \\cdot \\lambda^2$。函数在这些区间上是递增的。因此，对于 $\\lambda \\ge 0$，$\\operatorname{SURE}(\\lambda)$ 的最小值必然出现在集合 $\\{0\\} \\cup \\{|y_i|\\}_{i=1}^n$ 中的某个值处。\n\n$y$ 的分量的绝对值为 $|y_i| \\in \\{3.0, 2.6, 1.1, 0.9, 0.7, 0.2\\}$。\n$\\lambda^{\\star}$ 的候选值是 $\\{0, 0.2, 0.7, 0.9, 1.1, 2.6, 3.0\\}$。\n我们的 SURE 表达式中的常数项是 $-n\\sigma^2 = -6(0.25) = -1.5$。散度计数的系数是 $2\\sigma^2 = 2(0.25) = 0.5$。\n分量的平方值为 $y_i^2 \\in \\{9.0, 6.76, 1.21, 0.81, 0.49, 0.04\\}$。\n\n我们现在为每个候选值计算 $\\operatorname{SURE}(\\lambda)$：\n-   **对于 $\\lambda = 0$**：\n    $\\sum \\min(y_i^2, 0^2) = 0$。\n    $\\#\\{i: |y_i|  0\\} = 6$。\n    $\\operatorname{SURE}(0) = 0 - 1.5 + 0.5 \\times 6 = 1.5$。\n-   **对于 $\\lambda = 0.2$**：\n    $\\sum \\min(y_i^2, 0.2^2) = \\sum_{|y_i|\\le 0.2} y_i^2 + \\sum_{|y_i| 0.2} 0.2^2 = 0.2^2 + 5 \\times (0.2^2) = 0.24$。\n    $\\#\\{i: |y_i|  0.2\\} = 5$。\n    $\\operatorname{SURE}(0.2) = 0.24 - 1.5 + 0.5 \\times 5 = 0.24 + 1.0 = 1.24$。\n-   **对于 $\\lambda = 0.7$**：\n    $\\sum \\min(y_i^2, 0.7^2) = (0.04 + 0.49) + 4 \\times (0.49) = 0.53 + 1.96 = 2.49$。\n    $\\#\\{i: |y_i|  0.7\\} = 4$。\n    $\\operatorname{SURE}(0.7) = 2.49 - 1.5 + 0.5 \\times 4 = 2.49 + 0.5 = 2.99$。\n-   **对于 $\\lambda = 0.9$**：\n    $\\sum \\min(y_i^2, 0.9^2) = (0.04 + 0.49 + 0.81) + 3 \\times (0.81) = 1.34 + 2.43 = 3.77$。\n    $\\#\\{i: |y_i|  0.9\\} = 3$。\n    $\\operatorname{SURE}(0.9) = 3.77 - 1.5 + 0.5 \\times 3 = 3.77$。\n-   **对于 $\\lambda = 1.1$**：\n    $\\sum \\min(y_i^2, 1.1^2) = (0.04 + 0.49 + 0.81 + 1.21) + 2 \\times (1.21) = 2.55 + 2.42 = 4.97$。\n    $\\#\\{i: |y_i|  1.1\\} = 2$。\n    $\\operatorname{SURE}(1.1) = 4.97 - 1.5 + 0.5 \\times 2 = 4.47$。\n-   **对于 $\\lambda = 2.6$**：\n    $\\sum \\min(y_i^2, 2.6^2) = (0.04 + 0.49 + 0.81 + 1.21 + 6.76) + 1 \\times (6.76) = 9.31 + 6.76 = 16.07$。\n    $\\#\\{i: |y_i|  2.6\\} = 1$。\n    $\\operatorname{SURE}(2.6) = 16.07 - 1.5 + 0.5 \\times 1 = 15.07$。\n-   **对于 $\\lambda = 3.0$**：\n    $\\sum \\min(y_i^2, 3.0^2) = (0.04 + 0.49 + 0.81 + 1.21 + 6.76 + 9.0) = 18.31$。\n    $\\#\\{i: |y_i|  3.0\\} = 0$。\n    $\\operatorname{SURE}(3.0) = 18.31 - 1.5 + 0.5 \\times 0 = 16.81$。\n\n比较计算出的 SURE 值：\n$1.5, 1.24, 2.99, 3.77, 4.47, 15.07, 16.81$。\n最小值为 $1.24$，出现在 $\\lambda^{\\star} = 0.2$ 处。\n\nSURE 最优阈值是 $\\lambda^{\\star} = 0.2$。题目要求四舍五入到四位有效数字。", "answer": "$$\n\\boxed{0.2000}\n$$", "id": "3452179"}, {"introduction": "尽管 $L_1$ 正则化因其凸性而广受欢迎，但它会对所有系数（无论大小）施加恒定的收缩，这会为大信号的估计引入系统性偏差。为了克服这一限制，研究者们提出了诸如SCAD和MCP等非凸惩罚函数。这些高级正则化方法旨在对小系数进行强力收缩以实现稀疏性，同时对大系数减少甚至消除惩罚以减小偏差。在此练习 [@problem_id:3452161] 中，您将亲自推导这些非凸惩罚函数对应的阈值算子，并将其与 $L_1$ 范数的软阈值函数进行比较，从而直观地理解非凸正则化在偏差-方差权衡中的优势和潜在挑战。", "problem": "考虑在求解压缩感知中的不适定反问题的迭代方法中出现的一维惩罚最小二乘子问题：\n$$\n\\min_{x \\in \\mathbb{R}} \\; \\frac{1}{2}(x - y)^{2} + \\lambda \\, \\phi(|x|),\n$$\n其中 $y \\in \\mathbb{R}$ 是一个标量充分统计量（例如，在 $A^{\\top}A \\approx I$ 时，近端梯度迭代中数据的当前反投影），$\\lambda  0$ 是一个正则化参数，$\\phi$ 是一个逐元素应用的稀疏性促进惩罚项。使用上述目标函数最小化子 $x^{\\star}$ 的一阶最优性条件作为你的基本出发点，即对于可分离的一维惩罚的次梯度方程：\n$$\n0 \\in x^{\\star} - y + \\lambda \\, \\partial\\big(\\phi(|x^{\\star}|)\\big),\n$$\n以及以下事实：下面的惩罚项是 $x$ 的偶函数，在 $x \\neq 0$ 时可微，并且在 $|x|$ 上具有分段线性的导数。\n\n推导与以下惩罚项对应的显式阈值函数（闭式近端映射）$T_{\\ell_1}(y)$、$T_{\\mathrm{SCAD}}(y)$ 和 $T_{\\mathrm{MCP}}(y)$：\n1. $\\ell_1$ 惩罚 $\\phi_{\\ell_1}(|x|) = |x|$。\n2. 平滑削波绝对偏差（Smoothly Clipped Absolute Deviation, SCAD）惩罚 $\\phi_{\\mathrm{SCAD}}(|x|)$，其参数 $a  2$，其关于 $|x|$ 的导数定义为：\n$$\n\\phi_{\\mathrm{SCAD}}'(|x|) = \n\\begin{cases}\n\\lambda,  0 \\leq |x| \\leq \\lambda, \\\\\n\\dfrac{a\\lambda - |x|}{a - 1},  \\lambda  |x| \\leq a\\lambda, \\\\\n0,  |x|  a\\lambda,\n\\end{cases}\n$$\n并通过 $x$ 的奇对称性扩展到 $x  0$。\n3. 极小化凹惩罚（Minimax Concave Penalty, MCP）$\\phi_{\\mathrm{MCP}}(|x|)$，其参数 $\\gamma  1$，其关于 $|x|$ 的导数定义为：\n$$\n\\phi_{\\mathrm{MCP}}'(|x|) =\n\\begin{cases}\n\\lambda \\left(1 - \\dfrac{|x|}{\\gamma \\lambda}\\right),  0 \\leq |x| \\leq \\gamma \\lambda, \\\\\n0,  |x|  \\gamma \\lambda,\n\\end{cases}\n$$\n并通过 $x$ 的奇对称性扩展到 $x  0$。\n\n你的推导必须从所述的最优性原理开始，并根据分段线性导数对 $|x|$ 进行分情况讨论。清晰地用 $y$、$\\lambda$、$a$ 和 $\\gamma$ 表示最终的阈值函数，并强调它们的奇对称性。\n\n然后，在不适定反问题和压缩感知中的正则化原理的背景下，基于你推导出的阈值函数，讨论为什么像 SCAD 和 MCP 这样的非凸惩罚相对于 $\\ell_1$ 惩罚可以减少大信号系数的偏差，并阐明潜在的缺陷，包括算法的非凸性、对参数选择的敏感性以及对解的稳定性和唯一性的影响。\n\n最后，在参数值 $\\lambda = 1$、$a = 3.7$、$\\gamma = 3$ 和观测值 $y_{0} = 2.2$ 的情况下，计算所有三个阈值函数的值。将三个值 $T_{\\ell_1}(y_{0})$、$T_{\\mathrm{MCP}}(y_{0})$ 和 $T_{\\mathrm{SCAD}}(y_{0})$ 表示为精确的有理数（如果可能）。如果任何值在给定参数下无法简化为闭式有理数，则保留其精确的符号形式。不需要四舍五入。", "solution": "问题要求推导三种阈值函数，讨论它们的性质，并进行数值计算。我们通过求解以下一维优化问题来解决这个问题：\n$$\n\\min_{x \\in \\mathbb{R}} \\; f(x) = \\frac{1}{2}(x - y)^{2} + P(x),\n$$\n其中 $P(x)$ 是相应的惩罚项（$\\ell_1$、SCAD 或 MCP）。解 $x^{\\star}$ 是使 $f(x)$ 最小化的值。\n\n问题陈述中存在一个微小的歧义。目标函数给定为 $\\frac{1}{2}(x - y)^{2} + \\lambda \\, \\phi(|x|)$，但 SCAD 和 MCP 惩罚的导数定义 $\\phi'_{\\mathrm{SCAD}}(|x|)$ 和 $\\phi'_{\\mathrm{MCP}}(|x|)$ 已经包含了参数 $\\lambda$。为了解决这个问题，我们将整个惩罚项解释为 $P(x)$，并将给定的表达式视为 $P(x)$ 对其自变量 $|x|$ 的导数。我们记这个导数为 $P'_{|x|}(|x|) = \\frac{d P(u)}{du}|_{u=|x|}$。\n\n最小化子 $x^{\\star}$ 的一阶必要条件由次梯度包含关系给出：\n$$\n0 \\in \\partial f(x^{\\star}) = x^{\\star} - y + \\partial P(x^{\\star}).\n$$\n这可以重写为 $y - x^{\\star} \\in \\partial P(x^{\\star})$。惩罚函数 $P(x)$ 是偶函数，所以对于 $\\mathbb{R}_{\\ge 0}$ 上的某个函数 $\\tilde{P}$，有 $P(x) = \\tilde{P}(|x|)$。对于 $x \\neq 0$，次梯度 $\\partial P(x)$ 是一个包含导数的单点集，即 $\\partial P(x) = \\{ \\tilde{P}'(|x|) \\mathrm{sgn}(x) \\}$。在 $x=0$ 处，次梯度是一个区间 $\\partial P(0) = [-\\tilde{P}'(0^+), \\tilde{P}'(0^+)]$。\n\n解 $x^\\star = T(y)$ 的一个重要性质是它是 $y$ 的奇函数，即 $T(-y) = -T(y)$。这是因为在目标函数中用 $-y$ 替换 $y$ 并用 $-x$ 替换 $x$ 会得到 $\\frac{1}{2}(-x - (-y))^2 + P(-x) = \\frac{1}{2}(y-x)^2 + P(x)$，这与原目标函数相同。因此，如果 $x^\\star$ 是对于 $y$ 的目标函数的最小化子，那么 $-x^\\star$ 必然是对于 $-y$ 的最小化子。这意味着我们可以先推导 $y  0$（这意味着 $x^{\\star} \\geq 0$）时的解，然后通过奇对称性将其推广到所有 $y \\in \\mathbb{R}$。\n\n对于 $y  0$，我们有 $x^{\\star} \\geq 0$。最优性条件简化为：\n- 如果 $x^{\\star}  0$，则 $y - x^{\\star} = P'_{|x|}(x^{\\star})$。\n- 如果 $x^{\\star} = 0$，则 $y \\in [ -P'_{|x|}(0^+), P'_{|x|}(0^+) ]$。由于 $y  0$，这变成 $0  y \\leq P'_{|x|}(0^+)$。\n\n我们现在将此框架应用于每个惩罚项。\n\n### 1. $\\ell_{1}$ 惩罚：$T_{\\ell_{1}}(y)$\n惩罚项为 $P(x) = \\lambda |x|$。对于 $|x|0$，其关于 $|x|$ 的导数为 $P'_{|x|}(|x|) = \\lambda$。在 $0$ 处的右导数为 $P'_{|x|}(0^+) = \\lambda$。\n设 $y  0$。\n- 如果 $x^{\\star}=0$，条件是 $0  y \\leq \\lambda$。\n- 如果 $x^{\\star}  0$，条件是 $y - x^{\\star} = \\lambda$，这意味着 $x^{\\star} = y - \\lambda$。这仅在 $x^{\\star}  0$ 时有效，即 $y - \\lambda  0$，或 $y  \\lambda$。\n对于 $y0$ 结合这些情况：$x^{\\star} = \\max(0, y-\\lambda)$。\n通过奇对称性扩展到任意 $y \\in \\mathbb{R}$：\n$$\nT_{\\ell_1}(y) = \\mathrm{sgn}(y) \\max(0, |y|-\\lambda).\n$$\n这就是众所周知的软阈值函数。\n\n### 2. SCAD 惩罚：$T_{\\mathrm{SCAD}}(y)$\n对于 $a  2$，惩罚项关于 $|x|$ 的导数如下：\n$$\nP'_{|x|}(|x|) = \n\\begin{cases}\n\\lambda,  0 \\leq |x| \\leq \\lambda, \\\\\n\\dfrac{a\\lambda - |x|}{a - 1},  \\lambda  |x| \\leq a\\lambda, \\\\\n0,  |x|  a\\lambda.\n\\end{cases}\n$$\n在 $0$ 处的右导数为 $P'_{|x|}(0^+) = \\lambda$。\n设 $y  0$，则 $x^{\\star} \\geq 0$。\n- 如果 $x^{\\star}=0$，条件是 $0  y \\leq \\lambda$。\n- 如果 $x^{\\star}  0$，我们有 $y - x^{\\star} = P'_{|x|}(x^{\\star})$。我们按 $x^{\\star}$ 的情况进行分析：\n    - 情况 $0  x^{\\star} \\leq \\lambda$：$y - x^{\\star} = \\lambda \\implies x^{\\star} = y - \\lambda$。该解在 $0  y - \\lambda \\leq \\lambda$ 时有效，对应于 $\\lambda  y \\leq 2\\lambda$。\n    - 情况 $\\lambda  x^{\\star} \\leq a\\lambda$：$y - x^{\\star} = \\frac{a\\lambda - x^{\\star}}{a-1}$。解出 $x^{\\star}$：\n      $(a-1)y - (a-1)x^{\\star} = a\\lambda - x^{\\star} \\implies (a-1)y - a\\lambda = (a-2)x^{\\star} \\implies x^{\\star} = \\frac{(a-1)y - a\\lambda}{a-2}$。\n      该解在 $\\lambda  x^{\\star} \\leq a\\lambda$ 时有效。\n      $\\lambda  \\frac{(a-1)y - a\\lambda}{a-2} \\implies (a-2)\\lambda  (a-1)y - a\\lambda \\implies (2a-2)\\lambda  (a-1)y \\implies y  2\\lambda$。\n      $\\frac{(a-1)y - a\\lambda}{a-2} \\leq a\\lambda \\implies (a-1)y - a\\lambda \\leq a\\lambda(a-2) \\implies (a-1)y \\leq a^2\\lambda-a\\lambda \\implies y \\leq a\\lambda$。\n      因此，这种情况在 $2\\lambda  y \\leq a\\lambda$ 时成立。\n    - 情况 $x^{\\star}  a\\lambda$：$y - x^{\\star} = 0 \\implies x^{\\star} = y$。这在 $y  a\\lambda$ 时有效。\n\n对于 $y0$ 结合所有情况并通过奇对称性扩展：\n$$\nT_{\\mathrm{SCAD}}(y) = \n\\begin{cases}\n\\mathrm{sgn}(y)\\max(0, |y|-\\lambda),  |y| \\leq 2\\lambda, \\\\\n\\dfrac{(a-1)y - a\\lambda\\,\\mathrm{sgn}(y)}{a-2},  2\\lambda  |y| \\leq a\\lambda, \\\\\ny,  |y|  a\\lambda.\n\\end{cases}\n$$\n第一部分，对于 $|y| \\leq 2\\lambda$，与使用参数 $\\lambda$ 的软阈值函数 $T_{\\ell_1}(y)$ 相同。\n\n### 3. MCP 惩罚：$T_{\\mathrm{MCP}}(y)$\n对于 $\\gamma  1$，惩罚项关于 $|x|$ 的导数如下：\n$$\nP'_{|x|}(|x|) =\n\\begin{cases}\n\\lambda \\left(1 - \\dfrac{|x|}{\\gamma \\lambda}\\right),  0 \\leq |x| \\leq \\gamma \\lambda, \\\\\n0,  |x|  \\gamma \\lambda.\n\\end{cases}\n$$\n在 $0$ 处的右导数为 $P'_{|x|}(0^+) = \\lambda$。\n设 $y  0$，则 $x^{\\star} \\geq 0$。\n- 如果 $x^{\\star}=0$，条件是 $0  y \\leq \\lambda$。\n- 如果 $x^{\\star}  0$，我们有 $y - x^{\\star} = P'_{|x|}(x^{\\star})$。\n    - 情况 $0  x^{\\star} \\leq \\gamma\\lambda$：$y - x^{\\star} = \\lambda - \\frac{x^{\\star}}{\\gamma}$。解出 $x^{\\star}$：\n      $y - \\lambda = x^{\\star}(1 - 1/\\gamma) \\implies x^{\\star} = \\frac{y-\\lambda}{1-1/\\gamma} = \\frac{\\gamma(y-\\lambda)}{\\gamma-1}$。\n      该解在 $0  x^{\\star} \\leq \\gamma\\lambda$ 时有效。\n      $x^{\\star}  0$ 要求 $y  \\lambda$。\n      $x^{\\star} \\leq \\gamma\\lambda$ 要求 $\\frac{\\gamma(y-\\lambda)}{\\gamma-1} \\leq \\gamma\\lambda \\implies y-\\lambda \\leq \\lambda(\\gamma-1) \\implies y \\leq \\gamma\\lambda$。\n      因此，这种情况在 $\\lambda  y \\leq \\gamma\\lambda$ 时成立。\n    - 情况 $x^{\\star}  \\gamma\\lambda$：$y - x^{\\star} = 0 \\implies x^{\\star} = y$。这在 $y  \\gamma\\lambda$ 时有效。\n\n对于 $y0$ 结合所有情况并通过奇对称性扩展：\n$$\nT_{\\mathrm{MCP}}(y) = \n\\begin{cases}\n0,  |y| \\leq \\lambda, \\\\\n\\dfrac{\\gamma(y-\\lambda\\,\\mathrm{sgn}(y))}{\\gamma-1},  \\lambda  |y| \\leq \\gamma\\lambda, \\\\\ny,  |y|  \\gamma\\lambda.\n\\end{cases}\n$$\n\n### 讨论\n推导出的阈值函数揭示了惩罚项在不适定反问题背景下的关键性质。\n- **非凸惩罚的偏差减小：**\n$\\ell_1$ 阈值函数 $T_{\\ell_1}(y)=\\mathrm{sgn}(y)\\max(0, |y|-\\lambda)$ 总是将大系数以一个固定的量 $\\lambda$ 向零收缩。如果 $y$ 是一个真实大系数 $x_{true}$ 的观测值，估计值 $x^\\star$ 的幅度将大约是 $|x_{true}|-\\lambda$，这引入了系统性偏差。\n相比之下，对于较大的 $|y|$ 值（具体来说，SCAD 为 $|y|  a\\lambda$，MCP 为 $|y|  \\gamma\\lambda$），SCAD 和 MCP 的阈值函数都变成了恒等函数 $T(y)=y$。这意味着它们不对大系数施加惩罚，从而为大信号提供了渐近无偏的估计。这一性质，通常被称为“无偏性”，是非凸惩罚相对于 $\\ell_1$ 惩罚的一个显著优势。\n\n- **非凸惩罚的潜在缺陷：**\n    - **算法非凸性：** 主要的缺点是总目标函数，例如 $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 + \\sum_i P(x_i)$，会变成非凸的。像近端梯度下降这样的迭代算法，它会求解一系列此处分析的一维子问题，只能保证收敛到一个驻点，而这个驻点可能是局部最小值而非全局最小值。最终的解可能对算法的初始化敏感。\n    - **对参数选择的敏感性：** 除了正则化参数 $\\lambda$ 外，SCAD 和 MCP 的性能高度依赖于其形状参数（分别为 $a$ 和 $\\gamma$）的选择。这些额外参数控制着凹度的程度和无偏性的阈值。为这些参数找到最优值是一个困难的模型选择问题，通常比凸的 $\\ell_1$ 惩罚更复杂且计算量更大。\n    - **解的稳定性和唯一性：** 对于像 LASSO 这样的凸问题，解通常是唯一的，并且在数据扰动下是稳定的。对于非凸惩罚，多个局部最小值的存在可能导致解的不稳定性。数据的微小变化可能导致算法收敛到不同的局部最小值，从而产生显著不同的解。虽然这些估计器具有很强的渐近保证（例如，神谕性质），但它们的有限样本稳定性可能不如其凸对应物那样稳健。\n\n### 数值计算\n我们对给定的参数 $\\lambda = 1$，$a = 3.7$，$\\gamma = 3$ 和观测值 $y_0 = 2.2$ 计算这三个阈值函数。\n\n- **$T_{\\ell_1}(y_0)$：**\n当 $\\lambda=1$ 且 $y_0=2.2$ 时，我们有 $|y_0|  \\lambda$。\n$T_{\\ell_1}(2.2) = \\mathrm{sgn}(2.2)(|2.2| - 1) = 1(2.2 - 1) = 1.2 = \\frac{12}{10} = \\frac{6}{5}$。\n\n- **$T_{\\mathrm{MCP}}(y_0)$：**\n当 $\\lambda=1$，$\\gamma=3$，且 $y_0=2.2$ 时。区间由 $\\lambda=1$ 和 $\\gamma\\lambda = 3$ 定义。因为 $1  |2.2| \\leq 3$，我们使用 $T_{\\mathrm{MCP}}(y)$ 的第二种情况。\n$T_{\\mathrm{MCP}}(2.2) = \\frac{3(2.2 - 1 \\cdot \\mathrm{sgn}(2.2))}{3-1} = \\frac{3(1.2)}{2} = \\frac{3.6}{2} = 1.8 = \\frac{18}{10} = \\frac{9}{5}$。\n\n- **$T_{\\mathrm{SCAD}}(y_0)$：**\n当 $\\lambda=1$，$a=3.7$，且 $y_0=2.2$ 时。区间由 $2\\lambda=2$ 和 $a\\lambda=3.7$ 定义。因为 $2  |2.2| \\leq 3.7$，我们使用 $T_{\\mathrm{SCAD}}(y)$ 的第二种情况。\n$T_{\\mathrm{SCAD}}(2.2) = \\frac{(3.7 - 1) \\cdot 2.2 - 3.7 \\cdot 1 \\cdot \\mathrm{sgn}(2.2)}{3.7 - 2} = \\frac{2.7 \\cdot 2.2 - 3.7}{1.7} = \\frac{5.94 - 3.7}{1.7} = \\frac{2.24}{1.7}$。\n作为一个精确的有理数：\n$\\frac{2.24}{1.7} = \\frac{224/100}{17/10} = \\frac{224}{100} \\cdot \\frac{10}{17} = \\frac{224}{170} = \\frac{112}{85}$。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{6}{5}  \\frac{9}{5}  \\frac{112}{85} \\end{pmatrix}}\n$$", "id": "3452161"}]}