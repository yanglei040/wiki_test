{"hands_on_practices": [{"introduction": "高斯向量的二次型在信号处理、统计学和机器学习中无处不在，例如用于衡量能量或构建卡方检验统计量。这个练习是一个基础但至关重要的推导，它要求你计算高斯二次型的均值和方差，这将巩固你对多维高斯分布、迹运算性质以及高阶矩计算（如伊塞利斯定理 Isserlis' theorem）的理解。通过完成此练习，你将掌握分析高斯模型中非线性变换的关键技巧 [@problem_id:3447473]。", "problem": "考虑一个压缩感知场景，其中使用二次惩罚来度量基于加权度量的能量。令 $n \\in \\mathbb{N}$，令 $A \\in \\mathbb{R}^{n \\times n}$ 为一个确定性对称矩阵，并令 $g \\in \\mathbb{R}^{n}$ 为一个零均值高斯随机向量，其协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{n \\times n}$，即 $g \\sim \\mathcal{N}(0,\\Sigma)$，其中 $\\Sigma$ 是对称半正定矩阵。定义二次型 $Q := g^{\\top} A g$。仅使用期望、协方差和迹的基本定义，结合多元高斯矩的标准性质和基本线性代数恒等式，推导均值 $\\mathbb{E}[Q]$ 和方差 $\\operatorname{Var}(Q)$ 关于 $A$ 和 $\\Sigma$ 的闭式表达式。将您的最终答案表示为有序对 $\\left(\\mathbb{E}[Q], \\operatorname{Var}(Q)\\right)$，形式为一个单一的闭式解析表达式。无需四舍五入，也不涉及单位。", "solution": "所述问题是有效的。这是一个来自概率论及其在工程中应用的领域内的一个适定的、有科学依据的问题，提供了所有必要信息且没有矛盾。我们可以开始推导。\n\n我们感兴趣的量是二次型 $Q = g^{\\top} A g$，其中 $g \\in \\mathbb{R}^{n}$ 是一个服从均值为 $0$、协方差矩阵为 $\\Sigma$ 的多元正态分布的随机向量，记为 $g \\sim \\mathcal{N}(0,\\Sigma)$。矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是一个确定性对称矩阵，即 $A = A^{\\top}$。我们的任务是求出均值 $\\mathbb{E}[Q]$ 和方差 $\\operatorname{Var}(Q)$。\n\n首先，我们计算 $Q$ 的均值。由于 $Q$ 是一个标量，它等于其自身的迹。\n$$Q = g^{\\top} A g = \\operatorname{tr}(g^{\\top} A g)$$\n利用迹算子的循环性质 $\\operatorname{tr}(XYZ) = \\operatorname{tr}(ZXY)$，我们可以重新排列迹内部的项：\n$$Q = \\operatorname{tr}(A g g^{\\top})$$\n现在我们计算期望。根据期望算子的线性性质，我们可以将其移入迹的内部：\n$$\\mathbb{E}[Q] = \\mathbb{E}[\\operatorname{tr}(A g g^{\\top})] = \\operatorname{tr}(\\mathbb{E}[A g g^{\\top}])$$\n由于 $A$ 是一个确定性矩阵，可以将其从期望中提出来：\n$$\\mathbb{E}[Q] = \\operatorname{tr}(A \\, \\mathbb{E}[g g^{\\top}])$$\n项 $\\mathbb{E}[g g^{\\top}]$ 是零均值随机向量的协方差矩阵的定义。我们已知 $g \\sim \\mathcal{N}(0,\\Sigma)$，所以其协方差矩阵是 $\\Sigma$。\n$$\\mathbb{E}[g g^{\\top}] = \\operatorname{Cov}(g) = \\Sigma$$\n将此结果代回，我们得到 $Q$ 的均值的表达式：\n$$\\mathbb{E}[Q] = \\operatorname{tr}(A \\Sigma)$$\n\n接下来，我们计算 $Q$ 的方差。方差定义为：\n$$\\operatorname{Var}(Q) = \\mathbb{E}[Q^2] - (\\mathbb{E}[Q])^2$$\n我们已经求出了 $\\mathbb{E}[Q]$，因此我们需要计算 $\\mathbb{E}[Q^2]$。让我们用索引表示法来表示 $Q$。设 $g_i$ 是 $g$ 的第 $i$ 个分量，$A_{ij}$ 是 $A$ 的第 $i$ 行第 $j$ 列的元素。那么，\n$$Q = \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij} g_i g_j$$\n对该表达式求平方可得：\n$$Q^2 = \\left( \\sum_{i,j} A_{ij} g_i g_j \\right) \\left( \\sum_{k,l} A_{kl} g_k g_l \\right) = \\sum_{i,j,k,l} A_{ij} A_{kl} g_i g_j g_k g_l$$\n取期望，我们得到：\n$$\\mathbb{E}[Q^2] = \\sum_{i,j,k,l} A_{ij} A_{kl} \\mathbb{E}[g_i g_j g_k g_l]$$\n为了计算零均值高斯向量 $g$ 的四阶矩 $\\mathbb{E}[g_i g_j g_k g_l]$，我们使用 Isserlis 定理（或适用于高斯变量的 Wick 定理）。对于零均值多元高斯分布，偶数个变量乘积的期望是所有可能配对的期望之积的和。对于四个变量，即：\n$$\\mathbb{E}[g_i g_j g_k g_l] = \\mathbb{E}[g_i g_j]\\mathbb{E}[g_k g_l] + \\mathbb{E}[g_i g_k]\\mathbb{E}[g_j g_l] + \\mathbb{E}[g_i g_l]\\mathbb{E}[g_j g_k]$$\n设 $\\Sigma_{ab} = \\mathbb{E}[g_a g_b]$ 为协方差矩阵 $\\Sigma$ 的元素。该公式变为：\n$$\\mathbb{E}[g_i g_j g_k g_l] = \\Sigma_{ij}\\Sigma_{kl} + \\Sigma_{ik}\\Sigma_{jl} + \\Sigma_{il}\\Sigma_{jk}$$\n将此代入 $\\mathbb{E}[Q^2]$ 的表达式中，可将求和分为三个部分：\n$$\\mathbb{E}[Q^2] = \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{ij}\\Sigma_{kl} + \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{ik}\\Sigma_{jl} + \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{il}\\Sigma_{jk}$$\n让我们分别分析这三项。\n\n第一项是：\n$$S_1 = \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{ij}\\Sigma_{kl} = \\left(\\sum_{i,j} A_{ij} \\Sigma_{ij}\\right) \\left(\\sum_{k,l} A_{kl} \\Sigma_{kl}\\right)$$\n和 $\\sum_{i,j} A_{ij} \\Sigma_{ij}$ 可以用迹来表示。因为 $\\Sigma$ 是对称的（$\\Sigma_{ij} = \\Sigma_{ji}$），这个和是 $\\sum_{i,j} A_{ij} \\Sigma_{ji} = \\sum_i (A\\Sigma)_{ii} = \\operatorname{tr}(A\\Sigma)$。因此：\n$$S_1 = (\\operatorname{tr}(A\\Sigma))^2 = (\\mathbb{E}[Q])^2$$\n\n第二项与第三项：\n$$S_2 = \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{ik}\\Sigma_{jl}$$\n$$S_3 = \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{il}\\Sigma_{jk}$$\n由于求和的索引 $i,j,k,l$ 是哑变量，我们可以在 $S_3$ 中交换 $k$ 和 $l$：$S_3 = \\sum_{i,j,l,k} A_{ij} A_{lk} \\Sigma_{ik}\\Sigma_{jl}$。又因为矩阵 $A$ 是对称的（$A_{lk} = A_{kl}$），我们得到 $S_3 = \\sum_{i,j,k,l} A_{ij} A_{kl} \\Sigma_{ik}\\Sigma_{jl} = S_2$。\n因此我们只需计算其中一项。我们来计算 $S_3$，因为它的结构更接近于矩阵乘积的迹。首先重新排列乘法顺序：\n$$S_3 = \\sum_{i,j,k,l} A_{ij} \\Sigma_{jk} A_{kl} \\Sigma_{li}$$\n现在我们按照矩阵乘积的定义来分组：\n$$S_3 = \\sum_i \\left( \\sum_j A_{ij} \\left( \\sum_k \\Sigma_{jk} \\left( \\sum_l A_{kl} \\Sigma_{li} \\right) \\right) \\right)$$\n最内层的和是 $\\sum_l A_{kl} \\Sigma_{li} = (A\\Sigma)_{ki}$。\n代入后得到：$\\sum_k \\Sigma_{jk} (A\\Sigma)_{ki} = (\\Sigma (A\\Sigma))_{ji}$。\n再代入得到：$\\sum_j A_{ij} (\\Sigma A\\Sigma)_{ji} = (A (\\Sigma A\\Sigma))_{ii}$。\n最后对 $i$ 求和：\n$$S_3 = \\sum_i (A \\Sigma A \\Sigma)_{ii} = \\operatorname{tr}(A\\Sigma A\\Sigma) = \\operatorname{tr}((A\\Sigma)^2)$$\n因此，$S_2 = S_3 = \\operatorname{tr}((A\\Sigma)^2)$。\n\n将这三项合并，我们得到 $Q$ 的二阶矩：\n$$\\mathbb{E}[Q^2] = S_1 + S_2 + S_3 = (\\operatorname{tr}(A\\Sigma))^2 + 2\\operatorname{tr}((A\\Sigma)^2)$$\n最后，我们计算方差：\n$$\\operatorname{Var}(Q) = \\mathbb{E}[Q^2] - (\\mathbb{E}[Q])^2 = \\left( (\\operatorname{tr}(A\\Sigma))^2 + 2\\operatorname{tr}((A\\Sigma)^2) \\right) - (\\operatorname{tr}(A\\Sigma))^2$$\n$$\\operatorname{Var}(Q) = 2\\operatorname{tr}((A\\Sigma)^2)$$\n\n推导出的均值和方差的表达式为 $\\mathbb{E}[Q] = \\operatorname{tr}(A\\Sigma)$ 和 $\\operatorname{Var}(Q) = 2\\operatorname{tr}((A\\Sigma)^2)$。题目要求以有序对的形式给出答案。", "answer": "$$\n\\boxed{\n(\\operatorname{tr}(A\\Sigma), 2\\operatorname{tr}((A\\Sigma)^2))\n}\n$$", "id": "3447473"}, {"introduction": "在许多实际模型中，我们遇到的随机变量（如噪声）通常被假定在一个有界区间内。这个练习将一个直观的物理属性——有界性——与亚高斯随机变量的抽象定义联系起来。通过应用经典的霍夫丁引理 (Hoeffding's lemma)，你将学会如何将变量的有界性转化为对其亚高斯范数的一个精确上界，并证明这个界是紧的，这对理解理论保证与实际约束之间的关系至关重要 [@problem_id:3447505]。", "problem": "设 $X$ 是一个用于在压缩感知采集管道中为有界噪声建模的实值随机变量。假设 $X$ 几乎必然地支撑在区间 $[a,b]$ 上（其中 $a  b$），且其均值为 $\\mathbb{E}[X]=0$。$X$ 的次高斯范数 $\\|X\\|_{\\psi_{2}}$ 与区间长度 $b-a$ 之间存在一个普适的不等式 $\\|X\\|_{\\psi_{2}} \\le C(b-a)$，其中 $C$ 是一个不依赖于 $X$、 $a$ 或 $b$ 的普适常数。使用霍夫丁引理（Hoeffding's Lemma），该引理指出对于这样一个变量 $X$，其矩生成函数满足 $\\mathbb{E}[\\exp(\\lambda X)] \\le \\exp\\left(\\frac{\\lambda^2(b-a)^2}{8}\\right)$ 对所有 $\\lambda \\in \\mathbb{R}$ 成立，确定最小的可能普适常数 $C$。最终答案应为一个有理数。", "solution": "目标是找到最小的普适常数 $C  0$，使得对于任何均值为 $\\mathbb{E}[X]=0$ 且几乎必然支撑在区间 $[a,b]$ 上的实值随机变量 $X$，不等式 $\\|X\\|_{\\psi_{2}} \\le C(b-a)$ 均成立。\n\n亚高斯 $\\psi_{2}$ 范数定义为：\n$$\n\\|X\\|_{\\psi_{2}} := \\inf\\left\\{ s0 : \\mathbb{E}[\\exp(\\lambda X)] \\le \\exp\\left(\\tfrac{1}{2}s^{2}\\lambda^{2}\\right)\\ \\text{for all}\\ \\lambda \\in \\mathbb{R} \\right\\}\n$$\n题目提供了霍夫丁引理作为出发点。对于任何均值为 $\\mathbb{E}[X]=0$ 且几乎必然满足 $X \\in [a,b]$ 的随机变量 $X$，其矩生成函数 (MGF) $M_{X}(\\lambda) = \\mathbb{E}[\\exp(\\lambda X)]$ 有如下上界：\n$$\nM_{X}(\\lambda) \\le \\exp\\left(\\frac{\\lambda^{2}(b-a)^{2}}{8}\\right)\n$$\n这个不等式为 $X$ 的 MGF 提供了一个“高斯占优”界。\n\n我们可以利用此引理来确定 $\\|X\\|_{\\psi_{2}}$ 的一个上界。根据 $\\psi_{2}$ 范数的定义，$\\|X\\|_{\\psi_{2}}$ 是满足以下不等式的所有 $s0$ 的下确界：\n$$\n\\mathbb{E}[\\exp(\\lambda X)] \\le \\exp\\left(\\frac{1}{2}s^{2}\\lambda^{2}\\right) \\quad \\text{for all } \\lambda \\in \\mathbb{R}\n$$\n根据霍夫丁引理，我们知道上式左侧受 $\\exp\\left(\\frac{\\lambda^{2}(b-a)^{2}}{8}\\right)$ 的限制。因此，如果我们选择的 $s$ 满足以下条件，则 $\\psi_{2}$ 范数的条件将被满足：\n$$\n\\exp\\left(\\frac{\\lambda^{2}(b-a)^{2}}{8}\\right) \\le \\exp\\left(\\frac{1}{2}s^{2}\\lambda^{2}\\right)\n$$\n对两边取自然对数，该不等式等价于：\n$$\n\\frac{\\lambda^{2}(b-a)^{2}}{8} \\le \\frac{1}{2}s^{2}\\lambda^{2}\n$$\n对于任何 $\\lambda \\ne 0$，我们可以两边同除以 $\\lambda^{2}$ 得到：\n$$\n\\frac{(b-a)^{2}}{8} \\le \\frac{s^{2}}{2}\n$$\n重新整理这个不等式以求解 $s^2$，我们得到：\n$$\ns^{2} \\ge \\frac{2(b-a)^{2}}{8} = \\frac{(b-a)^{2}}{4}\n$$\n由于 $s$ 必须为正，我们对两边取平方根：\n$$\ns \\ge \\frac{b-a}{2}\n$$\n这表明，对于任何在 $[a,b]$ 上的零均值随机变量 $X$，任何 $s \\ge \\frac{b-a}{2}$ 的值都满足 $\\psi_{2}$ 范数的定义不等式。范数 $\\|X\\|_{\\psi_{2}}$ 是所有这些有效 $s$ 的下确界。因此，我们必然有：\n$$\n\\|X\\|_{\\psi_{2}} \\le \\frac{b-a}{2}\n$$\n这证明了当 $C = \\frac{1}{2}$ 时，不等式 $\\|X\\|_{\\psi_{2}} \\le C(b-a)$ 成立。这个值是最小普适常数的一个上界。\n\n为了证明 $C = \\frac{1}{2}$ 确实是*最小*的此类常数，我们必须证明这个界是紧的。也就是说，我们必须在所考虑的类别中找到至少一个随机变量 $X_{0}$，使得 $\\|X_{0}\\|_{\\psi_{2}} = \\frac{1}{2}(b-a)$。如果存在这样的变量，那么任何小于 $\\frac{1}{2}$ 的常数都不能普遍地满足该不等式。\n\n考虑一个对称的两点随机变量 $Y$，其支撑区间的长度为 $b-a$。令该随机变量以相等的概率支撑在集合 $\\{-\\frac{b-a}{2}, \\frac{b-a}{2}\\}$ 上：\n$$\nP\\left(Y = \\frac{b-a}{2}\\right) = \\frac{1}{2} \\quad \\text{and} \\quad P\\left(Y = -\\frac{b-a}{2}\\right) = \\frac{1}{2}\n$$\n$Y$ 的均值为 $\\mathbb{E}[Y] = \\frac{1}{2} \\cdot \\frac{b-a}{2} + \\frac{1}{2} \\cdot \\left(-\\frac{b-a}{2}\\right) = 0$。其支撑区间为 $[a', b']$，其中 $a' = -\\frac{b-a}{2}$ 且 $b' = \\frac{b-a}{2}$，因此 $b'-a' = b-a$。所以，$Y$ 属于我们正在考虑的随机变量类别（在不影响区间长度 $b-a$ 的情况下，可能存在区间平移）。\n\n让我们计算 $Y$ 的 MGF：\n$$\nM_{Y}(\\lambda) = \\mathbb{E}[\\exp(\\lambda Y)] = \\frac{1}{2}\\exp\\left(\\lambda \\frac{b-a}{2}\\right) + \\frac{1}{2}\\exp\\left(-\\lambda \\frac{b-a}{2}\\right) = \\cosh\\left(\\lambda \\frac{b-a}{2}\\right)\n$$\n为了找到 $\\|Y\\|_{\\psi_2}$，我们寻求满足以下不等式（对所有 $\\lambda \\in \\mathbb{R}$ 成立）的 $s0$ 的下确界：\n$$\n\\cosh\\left(\\lambda \\frac{b-a}{2}\\right) \\le \\exp\\left(\\frac{1}{2}s^{2}\\lambda^{2}\\right)\n$$\n我们可以通过比较两边在 $\\lambda=0$ 附近的泰勒级数展开，来分析此不等式在 $\\lambda$ 值较小时的情况。\n左侧的展开式为：\n$$\n\\cosh(u) = 1 + \\frac{u^{2}}{2!} + \\frac{u^{4}}{4!} + \\dots \\implies \\cosh\\left(\\lambda \\frac{b-a}{2}\\right) = 1 + \\frac{1}{2}\\lambda^{2}\\left(\\frac{b-a}{2}\\right)^2 + O(\\lambda^4)\n$$\n右侧的展开式为：\n$$\n\\exp(v) = 1 + v + \\frac{v^{2}}{2!} + \\dots \\implies \\exp\\left(\\frac{1}{2}s^{2}\\lambda^{2}\\right) = 1 + \\frac{1}{2}s^{2}\\lambda^{2} + O(\\lambda^4)\n$$\n为使不等式在 $0$ 的一个邻域内对所有 $\\lambda$ 成立，左侧 $\\lambda^2$ 项的系数不能超过右侧的系数：\n$$\n\\frac{1}{2}\\left(\\frac{b-a}{2}\\right)^2 \\le \\frac{1}{2}s^{2}\n$$\n这可以简化为 $s^{2} \\ge \\left(\\frac{b-a}{2}\\right)^2$。由于 $s0$，我们必须有 $s \\ge \\frac{b-a}{2}$。\n这意味着对于随机变量 $Y$，$s$ 的可能取值的下确界受 $\\frac{b-a}{2}$ 的限制。因此，\n$$\n\\|Y\\|_{\\psi_{2}} \\ge \\frac{b-a}{2}\n$$\n将此结果与从霍夫丁引理推导出的通用上界 $\\|Y\\|_{\\psi_{2}} \\le \\frac{b-a}{2}$ 相结合，我们得出结论，对于这个特定的随机变量 $Y$：\n$$\n\\|Y\\|_{\\psi_{2}} = \\frac{b-a}{2}\n$$\n对于这个随机变量，比值为 $\\frac{\\|Y\\|_{\\psi_{2}}}{b-a} = \\frac{(b-a)/2}{b-a} = \\frac{1}{2}$。\n由于我们找到了一个随机变量，其比值等于 $\\frac{1}{2}$，因此普适常数 $C$ 必须至少为 $\\frac{1}{2}$。\n鉴于我们之前已确定 $C \\le \\frac{1}{2}$，C 的最小可能值恰好是 $\\frac{1}{2}$。", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3447505"}, {"introduction": "亚高斯理论最引人注目的推论之一是高维统计中的“普遍性” (universality) 原理，即许多随机系统的宏观行为不依赖于其微观随机成分的具体分布，而只依赖于其尾部行为。本练习让你通过一个压缩感知的计算实验来亲身体验这一原理。你将通过编程模拟稀疏信号恢复过程，并凭经验验证，由不同分布（高斯分布与Rademacher分布）构成的测量矩阵，只要它们都是亚高斯的，就会展现出几乎相同的恢复性能相变曲线 [@problem_id:3447515]。", "problem": "考虑压缩感知中的无噪声稀疏恢复问题。设 $x_{0} \\in \\mathbb{R}^{n}$ 是一个 $s$-稀疏向量，且 $A \\in \\mathbb{R}^{m \\times n}$ 是一个具有独立同分布（i.i.d.）条目的随机测量矩阵。您观测到 $y = A x_{0}$。恢复方法是 $\\ell_{1}$ 最小化（也称为 Basis Pursuit (BP)）：在约束条件 $A x = y$ 下最小化 $\\|x\\|_{1}$，当计算出的最小化解 $\\hat{x}$ 的非零项索引集与 $x_{0}$ 的支撑集完全匹配时，即宣告精确支撑集恢复。\n\n需要比较两种测量系综：\n- Gaussian：$A$ 的条目是独立同分布的 $\\mathcal{N}(0, 1/m)$。\n- 有界次高斯（Rademacher）：$A$ 的条目是独立同分布的，以相等的概率取值 $\\pm 1/\\sqrt{m}$。\n\n使用以下基础和事实作为您的推导和算法设计的出发点：\n- 具有 Orlicz $\\psi_{2}$ 范数的次高斯随机变量 $X$ 的定义，即存在一个常数 $K$，使得对于所有 $t \\ge 0$，都有 $\\mathbb{P}(|X| \\ge t) \\le 2 \\exp\\left(- t^{2}/K^{2}\\right)$。\n- 对于独立同分布的次高斯测量矩阵，测度集中性意味着，只要 $m$ 的量级达到 $s \\log(en/s)$，那么 Restricted Isometry Property (RIP) 就以高概率成立，这其中可能相差一些取决于次高斯参数的绝对常数。\n- 在具有适当常数的 RIP 条件下，$\\ell_{1}$ 最小化在无噪声设置中可实现 $s$-稀疏向量的精确恢复。\n\n您的任务是设计一个程序，通过经验比较 Gaussian 和 Rademacher 系综在精确支撑集恢复方面的相变行为，并量化报告两种阈值估计之间的偏差。该程序必须实现以下普遍适用的数学过程：\n\n- 固定维度 $n = 128$。\n- 对于提供的测试套件中的每个稀疏度 $s$，考虑一个由 $m = \\lceil c \\, s \\, \\log(e n / s) \\rceil$ 生成的小型测量数 $m$ 网格，其中 $c$ 在指定的正正常数有限集合中取值。强制要求 $m \\ge s + 1$ 且 $m \\le n - 1$。\n- 对于每对 $(m, s)$ 和每种系综，通过重复固定次数的独立试验来估计精确支撑集恢复的经验成功概率。在每次试验中：\n  - 从指定的系综中抽样一个随机矩阵 $A$，其条目按上述方式缩放。\n  - 通过均匀随机选择一个支撑集，并将非零项设置为独立的 Rademacher 符号 $\\pm 1$，来抽样一个 $s$-稀疏信号 $x_{0}$。\n  - 构成 $y = A x_{0}$。\n  - 求解 $\\ell_{1}$ 最小化问题以获得 $\\hat{x}$，如果 $\\hat{x}$ 的支撑集（绝对值超过固定容差的索引）与 $x_{0}$ 的支撑集完全匹配，则宣告成功。\n- 对于每种系综和每个 $s$，将经验阈值 $\\hat{m}^{\\star}$ 定义为网格中其估计成功概率至少达到固定目标水平的最小 $m$。如果没有 $m$ 达到目标，则将 $\\hat{m}^{\\star}$ 定义为网格中最大的 $m$。\n- 通过为测试套件中的每个 $s$ 报告三元组 $\\left[\\hat{m}^{\\star}_{\\mathrm{Gauss}}, \\hat{m}^{\\star}_{\\mathrm{Rad}}, \\hat{m}^{\\star}_{\\mathrm{Rad}} - \\hat{m}^{\\star}_{\\mathrm{Gauss}}\\right]$ 来量化系综之间的偏差。\n\n要使用的测试套件和参数：\n- 维度：$n = 128$。\n- 稀疏度：$s \\in \\{1, 8, 16\\}$。\n- 网格常数：$c \\in \\{0.8, 1.2, 1.6, 2.4\\}$。\n- 每个 $(m, s)$ 和系综的试验次数：$T = 4$ 次独立试验。\n- 目标成功水平：至少 $0.5$（即 $4$ 次试验中至少有 $2$ 次成功）。\n- 支撑集检测容差：如果 $|\\hat{x}_{i}| \\ge 10^{-3}$，则声明索引 $i$ 在 $\\hat{x}$ 中是活动的。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个列表的列表，每个 $s$ 对应一个列表（按指定顺序），每个内部列表为 $\\left[\\hat{m}^{\\star}_{\\mathrm{Gauss}}, \\hat{m}^{\\star}_{\\mathrm{Rad}}, \\hat{m}^{\\star}_{\\mathrm{Rad}} - \\hat{m}^{\\star}_{\\mathrm{Gauss}}\\right]$，所有条目均为整数。例如，一个具有三个稀疏度的输出应类似于 $\\left[[a,b,b-a],[c,d,d-c],[e,f,f-e]\\right]$，其中 $a,b,c,d,e,f$ 是整数。", "solution": "该问题要求对两种次高斯测量系综——Gaussian 和 Rademacher——在使用 $\\ell_1$ 最小化进行稀疏恢复的任务中进行经验性比较。该比较基于相变行为，具体而言，是通过估计在 $\\mathbb{R}^n$ 中成功恢复一个 $s$-稀疏向量所需的最小测量数 $m$。\n\n### 1. 理论框架\n\n该问题属于压缩感知领域。其基本目标是从少量线性测量值 $y = A x_0$ 中恢复一个稀疏信号 $x_0 \\in \\mathbb{R}^n$，该信号的特点是只有 $s \\ll n$ 个非零项，其中 $y \\in \\mathbb{R}^m$，$A \\in \\mathbb{R}^{m \\times n}$ 是测量矩阵。挑战在于该系统是欠定的，即 $m  n$。\n\n通过利用 $x_0$ 的稀疏性，恢复成为可能。最直接的方法，即在测量约束下最小化非零项的数量（$\\ell_0$ 范数），在计算上是不可行的（NP-难）。Basis Pursuit (BP)，或称 $\\ell_1$ 最小化，为该问题提供了一个凸松弛：\n$$\n\\hat{x} = \\arg\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = y\n$$\n其中 $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$。\n\n### 2. 转换为线性规划 (LP)\n\n$\\ell_1$ 最小化问题可以转换为一个标准线性规划 (LP) 问题，该问题可在多项式时间内求解。这是通过将变量 $x$ 分解为其正部和负部来实现的：$x = x^+ - x^-$，其中 $x^+ \\ge 0$ 且 $x^- \\ge 0$。这些向量的分量定义为 $x_i^+ = \\max(x_i, 0)$ 和 $x_i^- = \\max(-x_i, 0)$。通过这种代换，$\\ell_1$ 范数变为一个线性函数：$\\|x\\|_1 = \\sum_i |x_i| = \\sum_i (x_i^+ + x_i^-)$。\n\n优化问题随后被重构为：\n$$\n\\min_{x^+, x^- \\in \\mathbb{R}^n} \\sum_{i=1}^n (x_i^+ + x_i^-) \\quad \\text{subject to} \\quad A(x^+ - x^-) = y, \\quad x^+ \\ge 0, \\quad x^- \\ge 0.\n$$\n这是一个关于 $2n$ 维变量 $z = [x^+; x^-]^T$ 的 LP 问题。以标准形式，我们要解决的问题是 $\\min_{z} c^T z$，约束条件为 $A_{eq}z = b_{eq}$ 和 $z \\ge 0$，其中：\n- $c = \\mathbf{1}_{2n}$（一个包含 $2n$ 个 1 的向量）。\n- $A_{eq} = [A, -A] \\in \\mathbb{R}^{m \\times 2n}$。\n- $b_{eq} = y \\in \\mathbb{R}^m$。\n\n这种形式允许使用标准的 LP 求解器，例如 `scipy.optimize.linprog`。\n\n### 3. 测量矩阵系综和 RIP\n\n$\\ell_1$ 最小化的成功取决于矩阵 $A$ 的性质。一个核心概念是 Restricted Isometry Property (RIP)，它指出 $A$ 必须在所有稀疏向量上近似于等距映射。具体来说，如果对于所有 $s$-稀疏向量 $v$，下式成立\n$$\n(1 - \\delta_s) \\|v\\|_2^2 \\le \\|Av\\|_2^2 \\le (1 + \\delta_s) \\|v\\|_2^2\n$$\n则矩阵 $A$ 满足阶数为 $s$、常数为 $\\delta_s \\in (0, 1)$ 的 RIP。如果 $A$ 满足阶数为 $2s$ 且具有足够小的常数（例如 $\\delta_{2s}  \\sqrt{2}-1$）的 RIP，那么 $\\ell_1$ 最小化保证能从 $y = Ax_0$ 中精确恢复任何 $s$-稀疏信号 $x_0$。\n\n随机矩阵理论提供了关键的联系：具有独立同分布次高斯条目的矩阵以高概率满足 RIP，只要测量数 $m$ 的量级达到 $m \\ge C \\cdot s \\log(n/s)$（其中 $C$ 是某个常数）。本问题中的两种系综都属于这一类：\n- **Gaussian 系综**：条目 $A_{ij} \\sim \\mathcal{N}(0, 1/m)$ 是次高斯的。\n- **Rademacher 系综**：条目 $A_{ij} = \\pm 1/\\sqrt{m}$ 从有界分布中抽取，因此也是次高斯的。\n\n缩放因子 $1/\\sqrt{m}$ 是典型的，它确保 $A$ 的列的期望欧几里得范数为 1，这与 RIP 条件相关。尽管理论为两种系综提供了相似的渐近保证，但它们的有限样本性能可能不同，本问题将对此进行经验性探索。\n\n### 4. 相变和经验估计\n\n在压缩感知中，对于给定的信号结构和恢复算法，存在一个急剧的“相变”。对于给定的 $n$ 和 $s$，随着 $m$ 的增加，成功恢复的概率会从接近 0 迅速过渡到接近 1。本问题的目标是经验性地定位这两种系综发生相变的点。\n\n规定的算法实现了这一搜索过程：\n1.  **参数网格**：对于每个稀疏度 $s$，使用理论缩放定律 $m \\propto s \\log(en/s)$ 创建一个测量数 $m$ 的网格，并使用不同的常数 $c$ 来探测预期相变点附近的区域。\n2.  **成功概率估计**：对于每对 $(m,s)$ 和每种系综，通过蒙特卡洛模拟来估计精确支撑集恢复的概率。运行 $T=4$ 次试验。\n3.  **单次试验**：每次试验包括：\n    a. 生成一个具有随机支撑集和随机非零值的 $s$-稀疏信号 $x_0$。\n    b. 从指定的系综中生成一个随机测量矩阵 $A$。\n    c. 计算测量值 $y = Ax_0$。\n    d. 求解 Basis Pursuit 的 LP 形式以找到估计值 $\\hat{x}$。\n    e. 如果 $\\hat{x}$ 的支撑集（非零项的索引集合，为保证数值精度而设置阈值）与 $x_0$ 的支撑集完全匹配，则宣告成功。\n4.  **经验阈值 $\\hat{m}^\\star$**：对于每种系综和稀疏度 $s$，阈值 $\\hat{m}^\\star$ 被定义为网格中经验成功概率达到目标（此处为 0.5，即 4 次试验中成功 2 次）的最小 $m$。这个 $\\hat{m}^\\star$ 作为相变边界的点估计。\n5.  **比较**：最终输出定量地比较阈值 $[\\hat{m}^\\star_{\\mathrm{Gauss}}, \\hat{m}^\\star_{\\mathrm{Rad}}]$ 及其差异，从而揭示在这些有限维设置下两种矩阵系综的相对样本效率。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Empirically compares phase transition behavior for sparse recovery \n    between Gaussian and Rademacher measurement ensembles.\n    \"\"\"\n    #\n    # --- Problem Parameters ---\n    #\n    N_DIM = 128\n    S_VALS = [1, 8, 16]\n    C_VALS = [0.8, 1.2, 1.6, 2.4]\n    TRIALS = 4\n    SUPPORT_TOL = 1e-3\n    TARGET_SUCCESS_PROB = 0.5\n    TARGET_SUCCESS_COUNT = int(TARGET_SUCCESS_PROB * TRIALS)\n    \n    # Use a single random number generator for the entire simulation\n    rng = np.random.default_rng()\n\n    #\n    # --- Helper Function for a Single Experiment ---\n    #\n    def run_single_experiment(m, s, ensemble_type, n_dim):\n        \"\"\"\n        Runs T=TRIALS for a given (m, s, ensemble) and returns success count.\n        \"\"\"\n        successes = 0\n        for _ in range(TRIALS):\n            # 1. Generate s-sparse signal x0\n            support = rng.choice(n_dim, s, replace=False)\n            x0 = np.zeros(n_dim)\n            x0[support] = rng.choice([-1.0, 1.0], s)\n\n            # 2. Generate measurement matrix A\n            if ensemble_type == 'Gauss':\n                A = rng.standard_normal(size=(m, n_dim)) / np.sqrt(m)\n            else:  # Rademacher\n                A = rng.choice([-1.0, 1.0], size=(m, n_dim)) / np.sqrt(m)\n\n            # 3. Form observation y\n            y = A @ x0\n\n            # 4. Solve l1 minimization via Linear Programming\n            # min ||x||_1 s.t. Ax = y  => min 1^Tz s.t. [A -A]z = y, z>=0\n            c_lp = np.ones(2 * n_dim)\n            A_eq = np.hstack([A, -A])\n            \n            # The 'highs' method is robust and default in recent SciPy versions\n            res = linprog(c=c_lp, A_eq=A_eq, b_eq=y, bounds=(0, None), method='highs')\n\n            # 5. Check for exact support recovery\n            if res.success:\n                x_hat = res.x[:n_dim] - res.x[n_dim:]\n                support_hat = np.where(np.abs(x_hat) > SUPPORT_TOL)[0]\n                if set(support) == set(support_hat):\n                    successes += 1\n        return successes\n\n    #\n    # --- Main Simulation Loop ---\n    #\n    all_results = []\n    for s in S_VALS:\n        # Generate the grid of measurement counts m for the current s\n        m_grid = []\n        for c in C_VALS:\n            m_val = int(np.ceil(c * s * np.log(np.e * N_DIM / s)))\n            if s + 1 = m_val = N_DIM - 1:\n                m_grid.append(m_val)\n        m_grid = sorted(list(set(m_grid)))\n        \n        # If the grid is empty, something is wrong, but we proceed\n        # as per validation this will not happen with given parameters.\n        if not m_grid:\n            continue\n\n        ensemble_thresholds = {}\n        for ensemble in ['Gauss', 'Rad']:\n            m_star = m_grid[-1]  # Default to largest m if target not met\n            for m in m_grid:\n                success_count = run_single_experiment(m, s, ensemble, N_DIM)\n                if success_count >= TARGET_SUCCESS_COUNT:\n                    m_star = m\n                    break  # Found the smallest m that meets the criteria\n            ensemble_thresholds[ensemble] = m_star\n\n        m_star_gauss = ensemble_thresholds['Gauss']\n        m_star_rad = ensemble_thresholds['Rad']\n        diff = m_star_rad - m_star_gauss\n        all_results.append([m_star_gauss, m_star_rad, diff])\n\n    #\n    # --- Final Output Formatting ---\n    #\n    # The required format is a compact string representation of a list of lists,\n    # without spaces after commas. e.g., [[a,b,c],[d,e,f]]\n    #\n    result_strings = [f'[{r[0]},{r[1]},{r[2]}]' for r in all_results]\n    final_output_str = f\"[{','.join(result_strings)}]\"\n    print(final_output_str)\n\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3447515"}]}