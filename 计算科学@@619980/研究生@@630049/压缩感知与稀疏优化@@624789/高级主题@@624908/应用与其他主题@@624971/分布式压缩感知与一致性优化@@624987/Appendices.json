{"hands_on_practices": [{"introduction": "交替方向乘子法 (ADMM) 是解决分布式共识优化问题的一种强大且广泛应用的算法。本练习将 ADMM 应用于共识 LASSO 问题，这是分布式稀疏恢复中的一个典型模型。通过亲手推导更新步骤，您将深刻理解该算法如何将一个全局问题分解为局部计算和共识更新，这是掌握分布式优化的基础。[@problem_id:3444486]", "problem": "考虑一个分布式传感网络，其中有 $L \\in \\mathbb{N}$ 个代理，每个代理通过带有加性噪声的线性模型 $y_{\\ell} \\approx A_{\\ell} z^{\\star}$ 测量一个共同的 $n$ 维信号，其数据为 $\\{A_{\\ell} \\in \\mathbb{R}^{m_{\\ell} \\times n},\\, y_{\\ell} \\in \\mathbb{R}^{m_{\\ell}}\\}_{\\ell=1}^{L}$。目标是通过求解一致性最小绝对收缩与选择算子 (LASSO) 问题来估计一个稀疏一致性向量 $z \\in \\mathbb{R}^{n}$\n$$\n\\min_{\\{x_{\\ell}\\}_{\\ell=1}^{L},\\, z \\in \\mathbb{R}^{n}} \\sum_{\\ell=1}^{L} \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} + \\lambda \\|z\\|_{1} \\quad \\text{subject to} \\quad x_{\\ell} = z \\quad \\text{for all } \\ell,\n$$\n其中 $\\lambda > 0$ 是 $\\ell_{1}$ 正则化参数。使用交替方向乘子法 (ADMM; Alternating Direction Method of Multipliers) 来强制达成一致性，其中使用缩放对偶变量 $\\{u_{\\ell}\\}_{\\ell=1}^{L}$ 和惩罚参数 $\\rho > 0$。\n\n从约束形式和带有缩放对偶变量的相应增广拉格朗日量出发，仅使用问题数据 $\\{A_{\\ell}, y_{\\ell}\\}$、当前迭代值 $\\{z^{k}, u_{\\ell}^{k}\\}$ 以及参数 $\\lambda$ 和 $\\rho$，推导出在第 $k+1$ 次迭代时局部原变量 $\\{x_{\\ell}\\}$ 和全局一致性变量 $z$ 的显式闭式 ADMM 更新式。假设对于每个 $\\ell$，矩阵 $A_{\\ell}^{\\top} A_{\\ell} + \\rho I$ 是可逆的，其中 $I$ 表示 $n \\times n$ 的单位矩阵。\n\n将最终答案表示为包含局部原变量、全局一致性变量和缩放对偶变量这三个更新映射的单个闭式解析表达式。您的答案必须是符号形式（无数值计算），并且必须用 $A_{\\ell}$、$y_{\\ell}$、$z^{k}$、$u_{\\ell}^{k}$、$\\lambda$、$\\rho$ 和 $L$ 来表示。最终答案中不要提供不等式或方程；只提供更新的解析表达式。", "solution": "该问题要求推导分布式一致性 LASSO 形式的交替方向乘子法 (ADMM) 更新式。\n\n优化问题如下：\n$$ \\min_{\\{x_{\\ell}\\}_{\\ell=1}^{L},\\, z \\in \\mathbb{R}^{n}} \\sum_{\\ell=1}^{L} \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} + \\lambda \\|z\\|_{1} \\quad \\text{subject to} \\quad x_{\\ell} = z \\quad \\text{for all } \\ell \\in \\{1, \\ldots, L\\} $$\n这是一个一致性问题，其中每个代理 $\\ell$ 都有一个局部变量 $x_{\\ell}$ 和一个局部数据拟合项 $f_{\\ell}(x_{\\ell}) = \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2}$。所有代理必须就单个一致性变量 $z$ 达成一致，该变量通过 $\\ell_1$ 范数进行正则化以促进稀疏性。\n\nADMM 算法通过构造增广拉格朗日量来解决此问题。对于这个问题，使用缩放对偶变量 $\\{u_{\\ell}\\}$，增广拉格朗日量 $\\mathcal{L}_{\\rho}$ 为：\n$$ \\mathcal{L}_{\\rho}(\\{x_{\\ell}\\}, z, \\{u_{\\ell}\\}) = \\sum_{\\ell=1}^{L} \\left( \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} \\right) + \\lambda \\|z\\|_{1} + \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|x_{\\ell} - z + u_{\\ell}\\|_{2}^{2} - \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|u_{\\ell}\\|_{2}^{2} $$\n此处，$\\rho > 0$ 是惩罚参数。ADMM 算法在每次迭代 $k+1$ 中通过迭代以下三个更新步骤进行：\n1.  对局部变量 $\\{x_{\\ell}\\}$ 最小化 $\\mathcal{L}_{\\rho}$。\n2.  对全局一致性变量 $z$ 最小化 $\\mathcal{L}_{\\rho}$。\n3.  更新缩放对偶变量 $\\{u_{\\ell}\\}$。\n\n让我们来显式地推导每个更新步骤。\n\n**1. $x_{\\ell}$-更新 (局部变量更新)**\n在第 $k+1$ 次迭代中，我们通过对 $x_{\\ell}$ 最小化增广拉格朗日量来更新每个 $x_{\\ell}$，同时将其他变量固定在它们的最新值（$z^k$，$u_{\\ell}^k$）。由于拉格朗日量的结构，关于 $\\{x_{\\ell}\\}$ 的最小化问题可以解耦为 $L$ 个独立的问题，每个代理 $\\ell$ 对应一个。\n$$ x_{\\ell}^{k+1} = \\arg\\min_{x_{\\ell}} \\left( \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} + \\frac{\\rho}{2} \\|x_{\\ell} - z^{k} + u_{\\ell}^{k}\\|_{2}^{2} \\right) $$\n这是一个关于 $x_{\\ell}$ 的无约束、严格凸二次函数的最小化问题。通过将其关于 $x_{\\ell}$ 的梯度设为零来找到最小值。设目标函数为 $J(x_{\\ell})$。\n$$ \\nabla_{x_{\\ell}} J(x_{\\ell}) = A_{\\ell}^{\\top}(A_{\\ell} x_{\\ell} - y_{\\ell}) + \\rho(x_{\\ell} - z^{k} + u_{\\ell}^{k}) = 0 $$\n重新整理各项以求解 $x_{\\ell}$：\n$$ A_{\\ell}^{\\top}A_{\\ell} x_{\\ell} + \\rho I x_{\\ell} = A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}) $$\n$$ (A_{\\ell}^{\\top}A_{\\ell} + \\rho I) x_{\\ell} = A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}) $$\n根据问题陈述，矩阵 $(A_{\\ell}^{\\top}A_{\\ell} + \\rho I)$ 是可逆的。因此，我们得到 $x_{\\ell}^{k+1}$ 的闭式更新式：\n$$ x_{\\ell}^{k+1} = (A_{\\ell}^{\\top}A_{\\ell} + \\rho I)^{-1} \\left( A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}) \\right) $$\n对于每个代理 $\\ell = 1, \\dots, L$，此更新是并行执行的。\n\n**2. $z$-更新 (全局一致性更新)**\n接下来，我们使用新计算出的值 $\\{x_{\\ell}^{k+1}\\}$，通过对 $z$ 最小化 $\\mathcal{L}_{\\rho}$ 来更新一致性变量 $z$：\n$$ z^{k+1} = \\arg\\min_{z} \\left( \\lambda \\|z\\|_{1} + \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|x_{\\ell}^{k+1} - z + u_{\\ell}^{k}\\|_{2}^{2} \\right) $$\n我们可以将二次项重写为：\n$$ \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|x_{\\ell}^{k+1} - z + u_{\\ell}^{k}\\|_{2}^{2} = \\frac{\\rho}{2} \\sum_{\\ell=1}^{L} \\|z - (x_{\\ell}^{k+1} + u_{\\ell}^{k})\\|_{2}^{2} $$\n通过对 $z$ 进行配方，这个和可以用平均值 $\\bar{v}^{k+1} = \\frac{1}{L}\\sum_{\\ell=1}^{L}(x_{\\ell}^{k+1} + u_{\\ell}^{k})$ 来表示：\n$$ \\frac{\\rho}{2} \\sum_{\\ell=1}^{L} \\|z - (x_{\\ell}^{k+1} + u_{\\ell}^{k})\\|_{2}^{2} = \\frac{L\\rho}{2} \\|z - \\bar{v}^{k+1}\\|_{2}^{2} + \\text{关于 } z \\text{ 的常数项} $$\n因此，$z$-最小化问题变为：\n$$ z^{k+1} = \\arg\\min_{z} \\left( \\lambda \\|z\\|_{1} + \\frac{L\\rho}{2} \\|z - \\bar{v}^{k+1}\\|_{2}^{2} \\right) $$\n这是 $\\ell_1$ 范数的近端算子的标准形式。具体来说，它等价于：\n$$ z^{k+1} = \\arg\\min_{z} \\left( \\frac{\\lambda}{L\\rho} \\|z\\|_{1} + \\frac{1}{2} \\left\\|z - \\frac{1}{L}\\sum_{\\ell=1}^{L}(x_{\\ell}^{k+1} + u_{\\ell}^{k})\\right\\|_{2}^{2} \\right) $$\n解由软阈值算子给出，记为 $S_{\\gamma}(\\cdot)$，其中对于向量 $v$ 和阈值 $\\gamma > 0$，有 $[S_{\\gamma}(v)]_i = \\text{sign}(v_i) \\max(|v_i| - \\gamma, 0)$。\n$z^{k+1}$ 的更新式为：\n$$ z^{k+1} = S_{\\frac{\\lambda}{L\\rho}} \\left( \\frac{1}{L}\\sum_{j=1}^{L}(x_{j}^{k+1} + u_{j}^{k}) \\right) $$\n其中求和使用了索引 $j$ 以避免歧义。\n\n**3. $u_{\\ell}$-更新 (对偶变量更新)**\n最后，使用原残差 $r_{\\ell}^{k+1} = x_{\\ell}^{k+1} - z^{k+1}$ 更新缩放对偶变量：\n$$ u_{\\ell}^{k+1} = u_{\\ell}^{k} + x_{\\ell}^{k+1} - z^{k+1} $$\n对于每个代理 $\\ell = 1, \\dots, L$，此更新是并行执行的。\n\n总而言之，第 $k+1$ 次迭代的三个顺序更新映射为：\n1.  $x_{\\ell}^{k+1} = (A_{\\ell}^{\\top}A_{\\ell} + \\rho I)^{-1}(A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}))$\n2.  $z^{k+1} = S_{\\frac{\\lambda}{L\\rho}}\\left(\\frac{1}{L}\\sum_{j=1}^{L}(x_{j}^{k+1} + u_{j}^{k})\\right)$\n3.  $u_{\\ell}^{k+1} = u_{\\ell}^{k} + x_{\\ell}^{k+1} - z^{k+1}$\n\n问题要求将这三个更新映射表示为单个表达式。", "answer": "$$ \\boxed{ \\begin{pmatrix} x_{\\ell}^{k+1} = (A_{\\ell}^{\\top}A_{\\ell} + \\rho I)^{-1}(A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k})) \\\\ z^{k+1} = S_{\\frac{\\lambda}{L\\rho}}\\left(\\frac{1}{L}\\sum_{j=1}^{L}(x_{j}^{k+1} + u_{j}^{k})\\right) \\\\ u_{\\ell}^{k+1} = u_{\\ell}^{k} + x_{\\ell}^{k+1} - z^{k+1} \\end{pmatrix} } $$", "id": "3444486"}, {"introduction": "真实的分布式系统并非总是理想化的；部分节点可能会出现故障或恶意行为（即“拜占庭”节点）。本项高级练习挑战您设计一种能够抵抗此类对抗性行为的稀疏恢复算法。您将使用经典的稳健统计工具“修剪均值”来聚合信息，并推导其性能退化的理论界限，从而应对在实际中部署这些系统时至关重要的安全性和可靠性问题。[@problem_id:3444450]", "problem": "给定一个包含 $N$ 个工作节点的分布式稀疏恢复设定。一个全局未知的 $s$-稀疏向量 $x^{\\star} \\in \\mathbb{R}^{p}$ 在节点 $i \\in \\{1,\\dots,N\\}$ 上进行局部测量，其结果为 $y_i = A_i x^{\\star} + w_i$，其中 $A_i \\in \\mathbb{R}^{m \\times p}$ 具有独立同分布的高斯条目，方差为 $1/m$，而 $w_i \\in \\mathbb{R}^{m}$ 是零均值高斯噪声，协方差为 $\\sigma^2 I_m$。其中一个包含 $q$ 个节点的子集是敌对的（拜占庭节点），它们可能会发送任意损坏的更新。\n\n目标是设计一种基于坐标级修剪均值聚合的、鲁棒的、单次分布式稀疏支撑集恢复方法，并为在存在 $q$ 个敌对节点的情况下，恢复出的支撑集的错误发现率（FDR）的增量设定一个上界。\n\n定义与设置：\n\n- 令每个节点的代理为 $z_i = A_i^{\\top} y_i \\in \\mathbb{R}^{p}$。一个鲁棒的聚合器使用修剪参数 $b=q$ 计算坐标级修剪均值：对于每个坐标 $j \\in \\{1,\\dots,p\\}$，收集 $\\{z_{i,j}\\}_{i=1}^N$，排序，丢弃 $q$ 个最小值和 $q$ 个最大值，然后对剩下的 $N - 2q$ 个值求平均，得到聚合后的坐标 $\\widehat{z}_j$。鲁棒支撑集估计为 $\\widehat{S}_{\\mathrm{rob}} = \\mathrm{Top}_s\\left(|\\widehat{z}|\\right)$，即 $\\widehat{z}$ 的 $s$ 个最大幅值的索引。\n\n- 作为没有攻击者时的基线，令 $z^{\\mathrm{ben}} = \\frac{1}{N-q}\\sum_{i \\in \\mathcal{B}} z_i$，其中 $\\mathcal{B}$ 是基数为 $N-q$ 的良性节点集合。基线支撑集估计为 $\\widehat{S}_{\\mathrm{base}} = \\mathrm{Top}_s\\left(|z^{\\mathrm{ben}}|\\right)$。\n\n- 对于一个支撑集估计 $\\widehat{S}$，将错误发现率定义为 $\\mathrm{FDR}(\\widehat{S}) = \\frac{|\\widehat{S} \\setminus S^{\\star}|}{|\\widehat{S}|}$，其中 $S^{\\star} = \\mathrm{supp}(x^{\\star})$。由敌对节点导致的错误发现率增量为 $\\Delta_{\\mathrm{FDR}} = \\mathrm{FDR}(\\widehat{S}_{\\mathrm{rob}}) - \\mathrm{FDR}(\\widehat{S}_{\\mathrm{base}})$。\n\n任务：\n\n- 提出一种使用上述坐标级修剪均值聚合（修剪参数 $b=q$）的鲁棒分布式稀疏支撑集恢复方法。\n\n- 从第一性原理以及鲁棒统计和稀疏恢复中广为接受的事实出发，推导出一个关于 $\\Delta_{\\mathrm{FDR}}$ 的可计算上界，该上界是 $N$、$q$ 和良性局部代理 $\\{z_i\\}_{i \\in \\mathcal{B}}$ 的经验分布的函数。你的推导必须从基本定义（例如，顺序统计量、集合基数和修剪均值的性质）开始，并产生一个仅能从可观察的良性代理统计量和 $q$ 评估的界。\n\n- 实现所提出的鲁棒方法和推导出的界。然后，使用下面的特定测试套件，为每个案例计算经验错误发现率增量 $\\Delta_{\\mathrm{FDR}}$ 是否小于或等于你推导出的上界。最终输出必须是一个布尔值列表，每个测试案例对应一个值，表示该界是否成立。\n\n用于仿真的敌对模型：\n\n- 最多有 $q$ 个节点是敌对的。在仿真中，假设敌对节点发送的损坏代理 $\\widetilde{z}_i$ 是独立的，并且可以具有很大的幅值。你必须选择一种与以下参数一致的具体损坏方式，以确保进行科学上现实的最坏情况压力测试，但你的界除了 $q$ 之外，不能依赖于未知的攻击者值。\n\n测试套件（每个测试案例固定所有参数）：\n\n- 案例1：$N = 9$，$q = 0$，$p = 120$，$s = 5$，$m = 50$，$\\sigma = 0.05$，损坏幅值参数 $B_{\\mathrm{adv}} = 0$。\n\n- 案例2：$N = 9$，$q = 2$，$p = 120$，$s = 5$，$m = 50$，$\\sigma = 0.05$，$B_{\\mathrm{adv}} = 20.0$。\n\n- 案例3：$N = 7$，$q = 3$，$p = 120$，$s = 5$，$m = 50$，$\\sigma = 0.05$，$B_{\\mathrm{adv}} = 100.0$。\n\n实现细节：\n\n- 构造 $x^{\\star}$，使其在均匀随机的支撑集位置上恰好有 $s$ 个非零值，这些非零值具有相等的幅值和随机的符号。\n\n- 生成每个 $A_i$，其条目独立同分布于 $\\mathcal{N}(0, 1/m)$，以及每个 $w_i$，其分布为 $\\mathcal{N}(0, \\sigma^2 I_m)$。\n\n- 对于敌对节点，生成具有独立条目的 $\\widetilde{z}_i$，其幅值在 $B_{\\mathrm{adv}}$ 的数量级，以便对聚合器进行有意义的压力测试。\n\n- 对鲁棒聚合使用修剪参数为 $b=q$ 的坐标级修剪均值，对基线使用良性节点的简单均值。\n\n- 如上定义错误发现率的增量 $\\Delta_{\\mathrm{FDR}}$。使用你的推导，仅根据 $N$、$q$ 和良性代理的坐标级范围，计算一个数据驱动的、有理论依据的上界。\n\n要求的最终输出格式：\n\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[True,False,True]”），其中每个布尔值表示计算出的经验 $\\Delta_{\\mathrm{FDR}}$ 是否小于或等于相应测试案例的推导界。\n\n角度单位不适用。不涉及物理单位。所有数值结果必须为标准十进制形式。程序必须是自包含的，不需要用户输入，并且必须在内部使用固定的随机种子以确保可复现性。", "solution": "用户提出的问题是分布式稀疏信号处理和鲁棒统计领域中一个有效且适定（well-posed）的问题。该问题要求推导在使用坐标级修剪均值聚合器处理存在拜占庭敌对节点的情况下，错误发现率增量的上界，并对此界进行数值验证。\n\n问题陈述已经过验证，并被认定为：\n- **科学上成立**：该设置使用了来自压缩感知（$y_i = A_i x^{\\star} + w_i$）和鲁棒统计（修剪均值估计器）的标准模型。所有定义均与现有文献一致。\n- **适定性**：任务是推导并验证一个上界，这是一个数学上精确的目标。测试案例的参数已指定，并确保修剪均值是良定义的（即 $N-2q > 0$）。\n- **客观性**：问题以精确的数学语言陈述，没有主观性或歧义。\n\n因此，我们可以进行完整的解答。\n\n### 基于第一性原理的错误发现率上界推导\n\n我们的目标是推导 $\\Delta_{\\mathrm{FDR}} = \\mathrm{FDR}(\\widehat{S}_{\\mathrm{rob}}) - \\mathrm{FDR}(\\widehat{S}_{\\mathrm{base}})$ 的一个可计算上界。\n\n**1. 预备知识和定义**\n令 $S^{\\star}$ 为 $s$-稀疏向量 $x^{\\star}$ 的真实支撑集，其大小为 $|S^{\\star}|=s$。支撑集估计 $\\widehat{S}_{\\mathrm{rob}}$ 和 $\\widehat{S}_{\\mathrm{base}}$ 的大小均为 $s$。\n错误发现率（FDR）为 $\\mathrm{FDR}(\\widehat{S}) = \\frac{|\\widehat{S} \\setminus S^{\\star}|}{s}$。\nFDR的增量为 $\\Delta_{\\mathrm{FDR}} = \\frac{1}{s} (|\\widehat{S}_{\\mathrm{rob}} \\setminus S^{\\star}| - |\\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star}|)$。\n\n基线聚合为 $z^{\\mathrm{ben}} = \\frac{1}{N-q}\\sum_{i \\in \\mathcal{B}} z_i$，其中 $\\mathcal{B}$ 是由 $N-q$ 个良性节点组成的集合。基线支撑集为 $\\widehat{S}_{\\mathrm{base}} = \\mathrm{Top}_s(|z^{\\mathrm{ben}}|)$。\n鲁棒聚合 $\\widehat{z}$ 是所有 $N$ 个代理（来自 $N-q$ 个良性节点和 $q$ 个敌对节点）的坐标级修剪均值，修剪参数为 $b=q$。对于每个坐标 $j \\in \\{1, \\dots, p\\}$，设排序后的代理值为 $z_{(1),j} \\le z_{(2),j} \\le \\dots \\le z_{(N),j}$。则 $\\widehat{z}_j = \\frac{1}{N-2q}\\sum_{k=q+1}^{N-q} z_{(k),j}$。鲁棒支撑集为 $\\widehat{S}_{\\mathrm{rob}} = \\mathrm{Top}_s(|\\widehat{z}|)$。\n\n**2. 为来自敌对节点的扰动设定界限**\n我们首先为任意坐标 $j$ 上的鲁棒聚合 $\\widehat{z}_j$ 和基线聚合 $z^{\\mathrm{ben}}_j$ 之间的差异建立一个界。\n\n令 $Z_j^{(\\mathcal{B})} = \\{z_{i,j}\\}_{i \\in \\mathcal{B}}$ 为坐标 $j$ 的 $N-q$ 个良性代理值集合。\n令 $m_j^{(\\mathcal{B})} = \\min(Z_j^{(\\mathcal{B})})$ 和 $M_j^{(\\mathcal{B})} = \\max(Z_j^{(\\mathcal{B})})$。\n基线聚合 $z^{\\mathrm{ben}}_j$ 是这些值的均值，因此它自然地被它们的范围所界定：$m_j^{(\\mathcal{B})} \\le z^{\\mathrm{ben}}_j \\le M_j^{(\\mathcal{B})}$。\n\n鲁棒聚合 $\\widehat{z}_j$ 是全部 $N$ 个代理（包括 $q$ 个任意的敌对值）的修剪均值。在存在 $q$ 个敌对节点的情况下，$q$-修剪均值的一个基本性质是其值包含在良性数据的范围内。\n要理解这一点，考虑被平均的 $N-2q$ 个值 $\\{z_{(q+1),j}, \\dots, z_{(N-q),j}\\}$。其中最小的值 $z_{(q+1),j}$ 是 $N$ 个值的组合集合中的第 $(q+1)$ 个顺序统计量。这 $q+1$ 个最小值中最多可包含 $q$ 个敌对值，因此必须至少包含一个良性值。因此，$z_{(q+1),j} \\ge m_j^{(\\mathcal{B})}$。通过对称论证，可得 $z_{(N-q),j} \\le M_j^{(\\mathcal{B})}$。由于 $\\widehat{z}_j$ 是 $z_{(q+1),j}$ 和 $z_{(N-q),j}$ 之间值的平均值，因此可得出 $m_j^{(\\mathcal{B})} \\le \\widehat{z}_j \\le M_j^{(\\mathcal{B})}$。\n\n$\\widehat{z}_j$ 和 $z^{\\mathrm{ben}}_j$ 都位于区间 $[m_j^{(\\mathcal{B})}, M_j^{(\\mathcal{B})}]$ 内。因此，它们之间可能的最大差异是该区间的长度：\n$$|\\widehat{z}_j - z^{\\mathrm{ben}}_j| \\le M_j^{(\\mathcal{B})} - m_j^{(\\mathcal{B})} =: R_j^{(\\mathcal{B})}$$\n其中 $R_j^{(\\mathcal{B})}$ 是坐标 $j$ 上良性代理值的范围。这为由敌对节点引起的坐标级扰动提供了一个数据驱动的界。这进一步暗示了对幅值变化的界：\n$$||\\widehat{z}_j| - |z^{\\mathrm{ben}}_j|| \\le |\\widehat{z}_j - z^{\\mathrm{ben}}_j| \\le R_j^{(\\mathcal{B})}$$\n\n**3. 分析支撑集变化**\n令 $I_{in} = \\widehat{S}_{\\mathrm{rob}} \\setminus \\widehat{S}_{\\mathrm{base}}$ 为进入前 $s$ 名列表的索引集合，令 $I_{out} = \\widehat{S}_{\\mathrm{base}} \\setminus \\widehat{S}_{\\mathrm{rob}}$ 为离开该列表的索引集合。由于两个支撑集的大小都为 $s$，我们必有 $|I_{in}| = |I_{out}| =: K$。\n\n错误发现数量的变化为 $| \\widehat{S}_{\\mathrm{rob}} \\setminus S^{\\star} | - | \\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star} |$。这可以用 $I_{in}$ 和 $I_{out}$ 来表示：\n$$ \\Delta_{\\mathrm{FDR}} = \\frac{1}{s} \\left( |I_{in} \\setminus S^{\\star}| - |I_{out} \\cap (\\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star})| \\right) $$\n为了获得上界，我们考虑最坏情况，即错误发现的增量最大化。这种情况发生在所有进入的索引都是错误发现（$I_{in} \\cap S^{\\star} = \\emptyset$）且没有已有的错误发现被移除（$I_{out} \\cap (\\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star}) = \\emptyset$）时。在这种情况下，表达式变为：\n$$ \\Delta_{\\mathrm{FDR}} \\le \\frac{|I_{in}|}{s} = \\frac{K}{s} $$\n我们的任务简化为找到 $K$ 的一个上界，$K$ 是在前 $s$ 个元素集合中成员资格发生变化的元素数量。\n\n**4. 为交换数量 K 设定界限**\n一个索引 $j \\in I_{in}$ 和一个索引 $k \\in I_{out}$ 意味着一次排名倒置。在基线中，$k$ 优于 $j$，因此 $|z^{\\mathrm{ben}}_k| \\ge |z^{\\mathrm{ben}}_j|$。攻击后，$j$ 优于 $k$，因此 $|\\widehat{z}_j| > |\\widehat{z}_k|$。\n\n我们可以使用扰动界来找到这种交换的必要条件：\n$$ |\\widehat{z}_j| \\le |z^{\\mathrm{ben}}_j| + R_j^{(\\mathcal{B})} $$\n$$ |\\widehat{z}_k| \\ge |z^{\\mathrm{ben}}_k| - R_k^{(\\mathcal{B})} $$\n要使交换 $|\\widehat{z}_j| > |\\widehat{z}_k|$ 发生，必须满足：\n$$ |z^{\\mathrm{ben}}_j| + R_j^{(\\mathcal{B})} > |z^{\\mathrm{ben}}_k| - R_k^{(\\mathcal{B})} $$\n$$ \\implies |z^{\\mathrm{ben}}_k| - |z^{\\mathrm{ben}}_j|  R_j^{(\\mathcal{B})} + R_k^{(\\mathcal{B})} $$\n这个不等式为一对索引 $(j, k)$（其中 $j \\notin \\widehat{S}_{\\mathrm{base}}$ 且 $k \\in \\widehat{S}_{\\mathrm{base}}$）交换它们的相对排名提供了一个必要条件，从而使得 $j$ 有可能进入前 $s$ 个元素的集合，而 $k$ 离开。\n\n**5. 一个可计算的上界**\n我们现在可以根据这个条件定义“脆弱”索引的集合。这些索引的排名很可能因为有界的扰动而改变。\n令 $V_{out}$ 为不在 $\\widehat{S}_{\\mathrm{base}}$ 中但易于被提升进入该集合的索引集合：\n$$ V_{out} = \\{ j \\notin \\widehat{S}_{\\mathrm{base}} \\mid \\exists k \\in \\widehat{S}_{\\mathrm{base}} \\text{ s.t. } |z^{\\mathrm{ben}}_k| - |z^{\\mathrm{ben}}_j|  R_j^{(\\mathcal{B})} + R_k^{(\\mathcal{B})} \\} $$\n令 $V_{in}$ 为在 $\\widehat{S}_{\\mathrm{base}}$ 中但易于被降级移出该集合的索引集合：\n$$ V_{in} = \\{ k \\in \\widehat{S}_{\\mathrm{base}} \\mid \\exists j \\notin \\widehat{S}_{\\mathrm{base}} \\text{ s.t. } |z^{\\mathrm{ben}}_k| - |z^{\\mathrm{ben}}_j|  R_j^{(\\mathcal{B})} + R_k^{(\\mathcal{B})} \\} $$\n任何被提升的索引 $j \\in I_{in}$ 都必须是脆弱的，因此 $I_{in} \\subseteq V_{out}$。\n任何被降级的索引 $k \\in I_{out}$ 都必须是脆弱的，因此 $I_{out} \\subseteq V_{in}$。\n这意味着 $K = |I_{in}| \\le |V_{out}|$ 且 $K = |I_{out}| \\le |V_{in}|$。\n因此，我们可以通过 $K \\le \\min(|V_{out}|, |V_{in}|)$ 来界定 $K$。\n\n将此代入我们关于 $\\Delta_{\\mathrm{FDR}}$ 的不等式中，我们得到最终的可计算上界：\n$$ \\Delta_{\\mathrm{FDR}} \\le \\frac{\\min(|V_{out}|, |V_{in}|)}{s} $$\n如要求所述，该界仅依赖于 $N, q$（通过大小为 $N-q$ 的集合 $\\mathcal{B}$ 隐式依赖）和良性代理的经验统计量（$z^{\\mathrm{ben}}$ 和 $\\{R_j^{(\\mathcal{B})}\\}_{j=1}^p$）。\n\n所提出的算法是首先通过坐标级修剪均值聚合计算鲁棒支撑集估计 $\\widehat{S}_{\\mathrm{rob}}$。然后，使用基于良性代理统计量的推导公式计算理论上界。最后，通过比较确定经验观察到的 $\\Delta_{\\mathrm{FDR}}$ 是否遵守该理论上界。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the distributed sparse support recovery problem for a given set of test cases.\n\n    For each case, it performs the following steps:\n    1.  Generates synthetic data: a sparse signal `x_star`, and local measurements\n        at N nodes, where q nodes are adversarial.\n    2.  Computes local proxies `z_i` at each node. Adversarial nodes produce\n        corrupted proxies.\n    3.  Calculates the baseline support estimate `S_base` by averaging benign proxies.\n    4.  Calculates the robust support estimate `S_rob` using a coordinate-wise\n        trimmed-mean aggregator over all proxies.\n    5.  Computes the empirical increase in false discovery rate `delta_fdr_empirical`.\n    6.  Derives a theoretical upper bound `delta_fdr_bound` on this increase, based on\n        the statistics of the benign proxies.\n    7.  Checks if the empirical value is less than or equal to the theoretical bound.\n    8.  Outputs a list of booleans indicating the result for each test case.\n    \"\"\"\n    np.random.seed(42)  # For reproducibility\n\n    test_cases = [\n        # (N, q, p, s, m, sigma, B_adv)\n        (9, 0, 120, 5, 50, 0.05, 0.0),\n        (9, 2, 120, 5, 50, 0.05, 20.0),\n        (7, 3, 120, 5, 50, 0.05, 100.0),\n    ]\n\n    results = []\n    for N, q, p, s, m, sigma, B_adv in test_cases:\n        # Step 1: Generate Data\n        # Generate true sparse vector x_star\n        x_star = np.zeros(p)\n        support_indices = np.random.choice(p, s, replace=False)\n        S_star = set(support_indices)\n        magnitudes = np.ones(s)\n        signs = np.random.choice([-1, 1], s)\n        x_star[support_indices] = magnitudes * signs\n\n        # Generate proxies for all nodes\n        benign_proxies = []\n        all_proxies = []\n        benign_node_indices = list(range(N - q))\n        \n        for i in range(N):\n            if i in benign_node_indices:  # Benign node\n                A_i = np.random.normal(0, 1 / np.sqrt(m), (m, p))\n                w_i = np.random.normal(0, sigma, m)\n                y_i = A_i @ x_star + w_i\n                z_i = A_i.T @ y_i\n                benign_proxies.append(z_i)\n                all_proxies.append(z_i)\n            else:  # Adversarial node\n                # Adversaries inject large-magnitude noise\n                z_tilde_i = np.random.uniform(-B_adv, B_adv, p)\n                all_proxies.append(z_tilde_i)\n\n        Z_benign = np.array(benign_proxies) if len(benign_proxies) > 0 else np.array([]).reshape(0,p)\n        Z_all = np.array(all_proxies)\n\n        # Step 2: Compute Aggregates and Supports\n        # Baseline aggregation (mean over benign nodes)\n        if N > q:\n            z_ben = np.mean(Z_benign, axis=0)\n        else: # All nodes are adversarial\n            z_ben = np.zeros(p)\n        \n        magnitudes_base = np.abs(z_ben)\n        base_indices_sorted = np.argsort(magnitudes_base)\n        S_base_indices = base_indices_sorted[-s:]\n        S_base = set(S_base_indices)\n\n        # Robust aggregation (trimmed-mean over all nodes)\n        z_rob = np.zeros(p)\n        if N > 2 * q:\n            for j in range(p):\n                # Sort, trim, and average\n                sorted_vals = np.sort(Z_all[:, j])\n                trimmed_vals = sorted_vals[q:-q] if q > 0 else sorted_vals\n                z_rob[j] = np.mean(trimmed_vals)\n        \n        magnitudes_rob = np.abs(z_rob)\n        S_rob_indices = np.argsort(magnitudes_rob)[-s:]\n        S_rob = set(S_rob_indices)\n        \n        # Step 3: Compute Empirical FDR Increase\n        fdr_base = len(S_base - S_star) / s\n        fdr_rob = len(S_rob - S_star) / s\n        delta_fdr_empirical = fdr_rob - fdr_base\n\n        # Step 4: Compute Theoretical Bound\n        if N > q:\n            # Benign proxy ranges R_j\n            R_benign = np.ptp(Z_benign, axis=0)\n            \n            # S_base_indices and not_S_base_indices\n            not_S_base_indices = base_indices_sorted[:-s]\n\n            # Construct vulnerable sets\n            V_in = set()\n            V_out = set()\n            \n            # This is O(s * (p-s)), which is acceptable for given parameters\n            for k in S_base_indices:\n                for j in not_S_base_indices:\n                    mag_k = magnitudes_base[k]\n                    mag_j = magnitudes_base[j]\n                    R_k = R_benign[k]\n                    R_j = R_benign[j]\n\n                    if mag_k - mag_j  R_j + R_k:\n                        V_in.add(k)\n                        V_out.add(j)\n\n            K_bound = min(len(V_in), len(V_out))\n            delta_fdr_bound = K_bound / s\n        else:\n            # If no benign nodes, baseline is zero, robust may not be.\n            # Bound derivation assumes some benign nodes for z_ben and R_j.\n            # In this scenario, the bound is not well-defined.\n            # However, problem constraints ensure N > q and N > 2q.\n            # For completeness, if this case occurred, we'd predict unconstrained change.\n            delta_fdr_bound = 1.0 \n\n        # Step 5: Compare and store result\n        # Add a small epsilon for floating-point comparisons\n        results.append(delta_fdr_empirical = delta_fdr_bound + 1e-9)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3444450"}]}