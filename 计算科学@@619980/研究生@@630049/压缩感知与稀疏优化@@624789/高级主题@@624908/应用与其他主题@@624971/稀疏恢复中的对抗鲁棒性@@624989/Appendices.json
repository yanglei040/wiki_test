{"hands_on_practices": [{"introduction": "我们的第一个实践练习将为我们打下坚实的基础。我们将探索如何为一种最简单的稀疏恢复方法——通过相关性阈值法进行支撑集检测——构建防御。这个练习将演示如何利用传感矩阵的基本性质，例如相互相干性 (mutual coherence)，来推导出一个具体的、可证明稳健的检测阈值，以抵抗有界对抗性攻击，这是一项关于最坏情况分析的基础性练习。[@problem_id:3430326]", "problem": "考虑一个感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其列为单位 $\\ell_2$-范数向量，其互相关性定义为 $\\mu(A) \\triangleq \\max_{i \\neq j} | \\langle a_{i}, a_{j} \\rangle |$，其中 $a_{i}$ 表示 $A$ 的第 $i$ 列。设未知信号 $x^{\\star} \\in \\mathbb{R}^{n}$ 是 $k$-稀疏的，其支撑集为 $S \\subset \\{1,\\dots,n\\}$，最小非零幅值为 $m_{\\star} \\triangleq \\min_{i \\in S} |x^{\\star}_{i}|$，且其 $\\ell_1$-范数有一个已知的先验界 $\\|x^{\\star}\\|_{1} \\leq L$，其中给定 $L > 0$。测量是无噪声的，由 $y = A x^{\\star}$ 给出。\n\n一个简单的支撑集检测器计算相关代理 $g \\triangleq A^{\\mathsf{T}} y = A^{\\mathsf{T}} A x^{\\star}$，然后通过硬阈值法选择索引。一个对手在阈值处理前扰动该代理，产生受攻击的代理 $v \\triangleq g + e$，其中对手是 $\\ell_{\\infty}$-有界的：$\\|e\\|_{\\infty} \\leq \\eta$，给定 $\\eta \\geq 0$。检测器输出支撑集估计\n$$\nT(\\tau) \\triangleq \\{ i \\in \\{1,\\dots,n\\} : |v_{i}| > \\tau \\},\n$$\n其中阈值 $\\tau > 0$ 有待设计。\n\n仅从互相关性和三角不等式的定义出发，并且除了给定的界限外，不假设 $x^{\\star}$ 的非零项有任何特定分布，推导一个确定性阈值 $\\tau$ 作为 $\\mu(A)$、$L$ 和 $\\eta$ 的函数，该阈值需保证只要 $m_{\\star}$ 相对于 $\\mu(A)$、$L$ 和 $\\eta$ 足够大，就有 $T(\\tau) = S$。您的推导过程应明确地界定在最坏情况的符号和对抗性扰动下，支撑集外的最大可能伪相关和支撑集上的最小可能衰减相关，然后选择 $\\tau$ 来分隔这些界限。\n\n请以 $\\mu(A)$、$L$ 和 $\\eta$ 的单一闭式符号表达式的形式给出阈值 $\\tau$ 的最终答案。最终答案中不要包含条件或不等式。无需四舍五入。", "solution": "问题要求推导一个确定性阈值 $\\tau$，作为互相关性 $\\mu(A)$、$\\ell_1$-范数界 $L$ 和对抗性扰动界 $\\eta$ 的函数。这个阈值必须确保支撑集估计 $T(\\tau) \\triangleq \\{ i \\in \\{1,\\dots,n\\} : |v_{i}| > \\tau \\}$ 能够正确识别 $k$-稀疏信号 $x^{\\star}$ 的真实支撑集 $S$。完美支撑集恢复的条件 $T(\\tau) = S$ 要求满足两个标准：首先，对于真实支撑集 $S$ 内的每个索引 $i$，不等式 $|v_i| > \\tau$ 必须成立；其次，对于支撑集外的每个索引 $j$（即 $j \\in S^c$），不等式 $|v_j| \\leq \\tau$ 必须成立。\n\n为了对任何符合给定界限的有效信号 $x^{\\star}$ 和扰动 $e$ 都满足这些条件，我们必须选择一个 $\\tau$，使其能够分隔支撑集上最坏情况（最小）的幅值和支撑集外最坏情况（最大）的幅值。也就是说，我们寻找一个 $\\tau$ 使得：\n$$\n\\max_{j \\notin S, \\|x^\\star\\|_1 \\le L, \\|e\\|_\\infty \\le \\eta} |v_j| \\leq \\tau  \\min_{i \\in S, \\|x^\\star\\|_1 \\le L, \\|e\\|_\\infty \\le \\eta} |v_i|\n$$\n问题陈述中提到，我们可以假设信号的最小非零幅值 $m_{\\star}$ 足够大，以确保存在这样的分隔。我们的任务是找到一个只依赖于 $\\mu(A)$、$L$ 和 $\\eta$ 的 $\\tau$ 的显式表达式。\n\n受攻击的代理向量为 $v = A^{\\mathsf{T}} A x^{\\star} + e$。其第 $i$ 个分量由下式给出：\n$$\nv_i = (A^{\\mathsf{T}} A x^{\\star})_i + e_i = \\sum_{l=1}^{n} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i\n$$\n其中 $a_i$ 是 $A$ 的第 $i$ 列。由于 $A$ 的列具有单位 $\\ell_2$-范数，我们有 $\\langle a_i, a_i \\rangle = \\|a_i\\|_{2}^{2} = 1$。\n\n首先，我们推导支撑集外分量 $|v_j|$（其中 $j \\notin S$）幅值的上界。\n对于索引 $j \\notin S$，信号分量 $x^{\\star}_j$ 为 $0$。因此，求和仅限于支撑集 $S$：\n$$\nv_j = \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l + e_j\n$$\n使用三角不等式，我们可以界定 $v_j$ 的幅值：\n$$\n|v_j| = \\left| \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l + e_j \\right| \\leq \\left| \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l \\right| + |e_j|\n$$\n对求和项应用三角不等式，并使用互相关性的定义 $|\\langle a_j, a_l \\rangle| \\leq \\mu(A)$ (当 $j \\neq l$ 时成立，这对于 $l \\in S$ 和 $j \\notin S$ 的情况总是成立的)，我们得到：\n$$\n\\left| \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l \\right| \\leq \\sum_{l \\in S} |\\langle a_j, a_l \\rangle| |x^{\\star}_l| \\leq \\sum_{l \\in S} \\mu(A) |x^{\\star}_l| = \\mu(A) \\sum_{l \\in S} |x^{\\star}_l|\n$$\n求和 $\\sum_{l \\in S} |x^{\\star}_l|$ 是信号的 $\\ell_1$-范数，即 $\\|x^{\\star}\\|_1$。对抗性扰动被界定为 $|e_j| \\leq \\|e\\|_{\\infty} \\leq \\eta$。\n结合这些结果，我们得到：\n$$\n|v_j| \\leq \\mu(A) \\|x^{\\star}\\|_1 + \\eta\n$$\n为了获得最坏情况下的界，我们使用给定的先验信息 $\\|x^{\\star}\\|_1 \\leq L$。当 $\\|x^{\\star}\\|_1$ 达到其最大允许值 $L$ 时，右侧最大化。因此，对于任何 $j \\notin S$，我们有确定性上界：\n$$\n|v_j| \\leq \\mu(A) L + \\eta\n$$\n我们将支撑集外分量的这个最坏情况上界记为 $B_{\\text{off}} = \\mu(A) L + \\eta$。\n\n接下来，我们推导支撑集上分量 $|v_i|$（其中 $i \\in S$）幅值的下界。对于索引 $i \\in S$，我们分离出对应于 $x^{\\star}_i$ 的项：\n$$\nv_i = \\langle a_i, a_i \\rangle x^{\\star}_i + \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i = x^{\\star}_i + \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i\n$$\n使用反三角不等式 $|a+b| \\geq |a| - |b|$，我们有：\n$$\n|v_i| \\geq |x^{\\star}_i| - \\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i \\right|\n$$\n对被减去的项应用三角不等式：\n$$\n\\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i \\right| \\leq \\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l \\right| + |e_i|\n$$\n使用互相关性可以界定该求和的幅值：\n$$\n\\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l \\right| \\leq \\sum_{l \\in S, l \\neq i} |\\langle a_i, a_l \\rangle| |x^{\\star}_l| \\leq \\mu(A) \\sum_{l \\in S, l \\neq i} |x^{\\star}_l|\n$$\n该求和可以表示为 $\\sum_{l \\in S, l \\neq i} |x^{\\star}_l| = \\|x^{\\star}\\|_1 - |x^{\\star}_i|$。因此，支撑集上元素的最坏情况（最大）干扰被 $\\mu(A) (\\|x^{\\star}\\|_1 - |x^{\\star}_i|) + \\eta$ 所界定。这导出了 $|v_i|$ 的一个下界：\n$$\n|v_i| \\geq |x^{\\star}_i| - \\left( \\mu(A) (\\|x^{\\star}\\|_1 - |x^{\\star}_i|) + \\eta \\right) = |x^{\\star}_i|(1 + \\mu(A)) - \\mu(A)\\|x^{\\star}\\|_1 - \\eta\n$$\n为了找到一个一般下界，我们必须考虑 $x^\\star$ 的最坏情况选择。当 $\\|x^{\\star}\\|_1$ 最大（最大为 $L$）且 $|x^{\\star}_i|$ 最小（最小为 $m_\\star$）时，该表达式最小化。因此，对于任何 $i \\in S$：\n$$\n|v_i| \\geq m_{\\star}(1 + \\mu(A)) - \\mu(A)L - \\eta\n$$\n我们将支撑集上分量的这个最坏情况下的界记为 $B_{\\text{on}} = m_{\\star}(1 + \\mu(A)) - \\mu(A)L - \\eta$。\n\n我们的目标是找到一个阈值 $\\tau$，使得对于所有 $j \\notin S$ 有 $|v_j| \\leq \\tau$，对于所有 $i \\in S$ 有 $|v_i|  \\tau$。如果我们选择的 $\\tau$ 满足 $B_{\\text{off}} \\leq \\tau  B_{\\text{on}}$，这些条件就能得到保证。\n问题要求阈值 $\\tau$ 仅是 $\\mu(A)$、$L$ 和 $\\eta$ 的函数。界 $B_{\\text{off}} = \\mu(A) L + \\eta$ 满足此要求，而 $B_{\\text{on}}$ 则依赖于 $m_{\\star}$。一个合适且直接的阈值选择是将其设置为支撑集外分量的最大可能幅值。\n我们选择阈值 $\\tau = B_{\\text{off}}$：\n$$\n\\tau = \\mu(A) L + \\eta\n$$\n通过这个选择，支撑集外索引的条件 $|v_j| \\leq \\tau$ 保证成立，因为我们已经严格证明了对于所有 $j \\notin S$ 都有 $|v_j| \\leq B_{\\text{off}} = \\tau$。因此，没有支撑集外的索引会满足 $|v_j|  \\tau$ 而被错误地选中。\n\n对于支撑集上的索引，我们必须确保 $|v_i|  \\tau$。由于我们知道 $|v_i| \\geq B_{\\text{on}}$，如果下界 $B_{\\text{on}}$ 严格大于我们选择的阈值 $\\tau$，这个条件就满足了：\n$$\nm_{\\star}(1 + \\mu(A)) - \\mu(A)L - \\eta  \\mu(A) L + \\eta\n$$\n$$\nm_{\\star}(1 + \\mu(A))  2\\mu(A)L + 2\\eta\n$$\n$$\nm_{\\star}  \\frac{2\\mu(A)L + 2\\eta}{1 + \\mu(A)}\n$$\n这个不等式就是“$m_{\\star}$ 足够大”的精确条件。由于问题陈述允许我们假设该条件成立，因此选择 $\\tau = \\mu(A) L + \\eta$ 是有效的，并在此条件下保证 $T(\\tau) = S$。这个 $\\tau$ 的表达式是根据所需参数给出的一个闭式符号表达式。", "answer": "$$\n\\boxed{\\mu(A)L + \\eta}\n$$", "id": "3430326"}, {"introduction": "在单步方法的基础上，这个练习将深入研究迭代算法中对抗性误差的动态演化，特别是迭代硬阈值（Iterative Hard Thresholding, IHT）算法。你将发现一个关键且有些反直觉的原则：在存在对抗性噪声的情况下，将算法运行至完全收敛并非总是最佳策略。这个练习将引导你量化优化精度与对抗性稳健性之间的权衡，揭示提前停止作为一种正则化形式的力量。[@problem_id:3430320]", "problem": "考虑一个标准的压缩感知测量模型，其中包含一个未知的 $k$-稀疏信号 $x^{\\star} \\in \\mathbb{R}^{n}$，一个传感矩阵 $A \\in \\mathbb{R}^{m \\times n}$，以及满足 $\\|\\delta\\|_{2} \\le \\rho$ 的对抗性测量扰动 $\\delta \\in \\mathbb{R}^{m}$，因此测量值为 $y = A x^{\\star} + \\delta$。设恢复过程通过迭代硬阈值（IHT）算法进行，即以下递归式\n$$\nx_{t+1} = H_{k}\\Big(x_{t} + \\mu A^{\\top}(y - A x_{t})\\Big),\n$$\n其中 $H_{k}(\\cdot)$ 是作用于 $k$ 个最大绝对值坐标的硬阈值算子，$\\mu  0$ 是一个固定的步长。假设以下在科学上标准且被广泛使用的条件成立。\n\n- 阶数为 $k$ 的有限等距性质（RIP）以常数 $\\delta_{k} \\in (0,1)$ 成立，因此对于每个 $k$-稀疏向量 $z$，都有 $(1 - \\delta_{k}) \\|z\\|_{2}^{2} \\le \\|A z\\|_{2}^{2} \\le (1 + \\delta_{k}) \\|z\\|_{2}^{2}$。\n\n- 存在一个阶段（可能在某个您可以在分析中忽略的“热启动”之后），在该阶段中，支撑集 $T := \\mathrm{supp}(x^{\\star})$ 被正确识别并保持不变；在该阶段，$H_{k}$ 的作用是到坐标集 $T$ 上的正交投影算子。您应当完全在这个固定支撑集的阶段内分析迭代过程。\n\n- 记 $A_{T} \\in \\mathbb{R}^{m \\times k}$ 为 $A$ 限制在由 $T$ 索引的列上所形成的子矩阵，并假设步长满足 $0  \\mu  \\frac{2}{1 + \\delta_{k}}$。令 $L := 1 + \\delta_{k}$ 和 $m := 1 - \\delta_{k}$，并定义收缩因子\n$$\n\\alpha := \\max\\{\\,|1 - \\mu m|,\\; |1 - \\mu L|\\,\\},\n$$\n在所述的步长界限下，保证满足 $0  \\alpha  1$。\n\n将第 $t$ 次迭代的对抗性误差放大定义为\n$$\nG(t) := \\sup_{\\|\\delta\\|_{2} \\le \\rho} \\frac{\\|x_{t}(\\delta) - x_{t}(0)\\|_{2}}{\\rho},\n$$\n其中 $x_{t}(\\delta)$ 表示当测量值为 $y = A x^{\\star} + \\delta$ 时产生的 IHT 迭代结果，而 $x_{t}(0)$ 表示当 $\\delta = 0$ 时产生的迭代结果。这个 $G(t)$ 量化了第 $t$ 次迭代结果对测量值的对抗性扰动的最坏情况 $\\ell_2$-敏感度，其稳态极限 $G(\\infty)$ 量化了完全收敛时的对抗性放大效应。\n\n您的任务：\n\n- 从固定支撑集的线性递归和有限等距性质出发，推导出一个形如 $$G(t) \\le \\mu \\|A_{T}\\|_{2} \\sum_{i=0}^{t-1} \\alpha^{i}$$ 的关于 $G(t)$ 的严格上界，然后使用 $\\|A_{T}\\|_{2} \\le \\sqrt{1 + \\delta_{k}}$，用 $\\alpha$、$\\mu$ 和 $\\delta_{k}$ 表示 $G(t)$ 的闭式解。推断在何种条件下，对于任意有限的 $t$，$G(t)  G(\\infty)$ 成立。\n\n- 为了量化优化偏差和对抗性放大之间的权衡，固定一个期望比例 $\\kappa \\in (0,1)$，并定义 $t_{\\max}(\\kappa)$ 为满足 $G(t) \\le \\kappa \\, G(\\infty)$ 的最大整数 $t$。计算仅用 $\\alpha$ 和 $\\kappa$ 表示的 $t_{\\max}(\\kappa)$ 的闭式表达式。\n\n答案规格：您的最终答案必须仅包含您得到的 $t_{\\max}(\\kappa)$ 作为 $\\alpha$ 和 $\\kappa$ 函数的精确解析表达式。不需要进行数值取整。", "solution": "用户提供的问题经过严格验证，确认是压缩感知和稀疏优化领域中一个有效、适定且具有科学依据的问题。所有假设和定义在关于迭代硬阈值（IHT）及其鲁棒性分析的文献中都是标准的。该问题是自洽的，并为得到唯一、可验证的解提供了一条清晰的路径。\n\n我们首先在指定的固定支撑集假设下分析 IHT 递归。问题陈述指出，迭代向量的支撑集 $T := \\mathrm{supp}(x^{\\star})$ 被正确识别并保持不变。在此阶段，硬阈值算子 $H_{k}(\\cdot)$ 的作用是向支撑在 $T$ 上的向量子空间做正交投影。设 $x_{t,T}$ 表示向量 $x_t$ 在 $T$ 中坐标上的限制，它是一个 $k$ 维向量。类似地，设 $A_T$ 是矩阵 $A$ 限制在由 $T$ 索引的列上所形成的子矩阵。迭代向量非零分量的递归可以写为：\n$$\nx_{t+1, T} = x_{t, T} + \\mu A_T^{\\top}(y - A_T x_{t, T})\n$$\n我们关心的是迭代结果对对抗性扰动 $\\delta$ 的敏感度。设 $x_t(\\delta)$ 为测量值为 $y = Ax^{\\star} + \\delta$ 时第 $t$ 步的迭代结果，设 $x_t(0)$ 为 $\\delta=0$（即 $y = Ax^{\\star}$）时的迭代结果。假设两个迭代序列都支撑在 $T$ 上。设它们在 $T$ 上的限制分别表示为 $x_{t,T}(\\delta)$ 和 $x_{t,T}(0)$。\n\n这两个序列的递归式为：\n$$\nx_{t+1, T}(\\delta) = x_{t, T}(\\delta) + \\mu A_T^{\\top}(A_T x^{\\star}_T + \\delta - A_T x_{t, T}(\\delta))\n$$\n$$\nx_{t+1, T}(0) = x_{t, T}(0) + \\mu A_T^{\\top}(A_T x^{\\star}_T - A_T x_{t, T}(0))\n$$\n设支撑集上的误差向量为 $e_t := x_{t, T}(\\delta) - x_{t, T}(0)$。将第二个方程从第一个方程中减去，得到该误差的动态过程：\n$$\ne_{t+1} = e_t + \\mu A_T^{\\top}(\\delta - A_T(x_{t, T}(\\delta) - x_{t, T}(0)))\n$$\n$$\ne_{t+1} = e_t - \\mu A_T^{\\top} A_T e_t + \\mu A_T^{\\top} \\delta\n$$\n$$\ne_{t+1} = (I_k - \\mu A_T^{\\top} A_T) e_t + \\mu A_T^{\\top} \\delta\n$$\n此处，$I_k$ 是 $k \\times k$ 的单位矩阵。我们分析此误差的 $\\ell_2$-范数的增长。使用三角不等式：\n$$\n\\|e_{t+1}\\|_2 \\le \\|(I_k - \\mu A_T^{\\top} A_T) e_t\\|_2 + \\|\\mu A_T^{\\top} \\delta\\|_2\n$$\n$$\n\\|e_{t+1}\\|_2 \\le \\|I_k - \\mu A_T^{\\top} A_T\\|_2 \\|e_t\\|_2 + \\mu \\|A_T^{\\top}\\|_2 \\|\\delta\\|_2\n$$\n阶数为 $k$ 的有限等距性质（RIP）指出，对于任意 $k$-稀疏向量 $z$，$(1-\\delta_k)\\|z\\|_2^2 \\le \\|Az\\|_2^2 \\le (1+\\delta_k)\\|z\\|_2^2$。这等价于说格拉姆矩阵 $A_T^{\\top}A_T$ 的所有特征值 $\\lambda_i$ 都被界定为 $1-\\delta_k \\le \\lambda_i \\le 1+\\delta_k$。令 $m=1-\\delta_k$ 和 $L=1+\\delta_k$。\n矩阵 $I_k - \\mu A_T^{\\top} A_T$ 是对称的，因此其 $\\ell_2$-范数是其谱半径（最大绝对值特征值）。其特征值为 $1 - \\mu\\lambda_i$。根据给定的 $\\lambda_i$ 的界，我们有 $1-\\mu L \\le 1-\\mu\\lambda_i \\le 1-\\mu m$。因此，算子范数为 $\\|I_k - \\mu A_T^{\\top} A_T\\|_2 = \\max\\{|1-\\mu m|, |1-\\mu L|\\}$，它被定义为收缩因子 $\\alpha$。问题陈述指出，步长的选择确保了 $\\alpha \\in (0,1)$。\n\n范数递归变为：\n$$\n\\|e_{t+1}\\|_2 \\le \\alpha \\|e_t\\|_2 + \\mu \\|A_T\\|_2 \\|\\delta\\|_2\n$$\n假设初始值相同，$x_0(\\delta)=x_0(0)$，则有 $e_0=0$。展开递归：\n$$\n\\|e_1\\|_2 \\le \\mu \\|A_T\\|_2 \\|\\delta\\|_2\n$$\n$$\n\\|e_2\\|_2 \\le \\alpha\\|e_1\\|_2 + \\mu \\|A_T\\|_2 \\|\\delta\\|_2 \\le (\\alpha+1)\\mu \\|A_T\\|_2 \\|\\delta\\|_2\n$$\n通过归纳法，对于任意 $t \\ge 1$：\n$$\n\\|e_t\\|_2 \\le \\left(\\sum_{i=0}^{t-1} \\alpha^i\\right) \\mu \\|A_T\\|_2 \\|\\delta\\|_2\n$$\n问题将对抗性误差放大定义为 $G(t) = \\sup_{\\|\\delta\\|_{2} \\le \\rho} \\frac{\\|x_{t}(\\delta) - x_{t}(0)\\|_{2}}{\\rho}$。由于迭代向量支撑在 $T$ 上，$\\|x_{t}(\\delta) - x_{t}(0)\\|_{2} = \\|e_t\\|_2$。\n$$\n\\frac{\\|e_t\\|_2}{\\rho} \\le \\left(\\sum_{i=0}^{t-1} \\alpha^i\\right) \\mu \\|A_T\\|_2 \\frac{\\|\\delta\\|_2}{\\rho}\n$$\n对所有 $\\|\\delta\\|_2 \\le \\rho$ 取上确界，得到所期望的 $G(t)$ 的上界：\n$$\nG(t) \\le \\mu \\|A_T\\|_2 \\sum_{i=0}^{t-1} \\alpha^i\n$$\n这与第一个任务中要求的形式相符。为了找到闭式表达式，我们使用等比数列求和公式 $\\sum_{i=0}^{t-1} \\alpha^i = \\frac{1-\\alpha^t}{1-\\alpha}$ 以及基于 RIP 的界 $\\|A_T\\|_2 \\le \\sqrt{\\sigma_{\\max}(A_T^\\top A_T)} \\le \\sqrt{1+\\delta_k}$。这给出了上界：\n$$\nG(t) \\le \\mu \\sqrt{1+\\delta_k} \\frac{1-\\alpha^t}{1-\\alpha}\n$$\n问题的结构意味着，对于后续任务，我们将 $G(t)$ 等同于这个推导出的上界。稳态放大 $G(\\infty)$ 是当 $t \\to \\infty$ 时此上界的极限。由于 $\\alpha \\in (0,1)$，当 $t \\to \\infty$ 时，$\\alpha^t \\to 0$。\n$$\nG(\\infty) = \\lim_{t\\to\\infty} \\left( \\mu \\sqrt{1+\\delta_k} \\frac{1-\\alpha^t}{1-\\alpha} \\right) = \\mu \\frac{\\sqrt{1+\\delta_k}}{1-\\alpha}\n$$\n为了确定何时 $G(t)  G(\\infty)$，我们比较这两个表达式：\n$$\n\\mu \\sqrt{1+\\delta_k} \\frac{1-\\alpha^t}{1-\\alpha}  \\mu \\frac{\\sqrt{1+\\delta_k}}{1-\\alpha}\n$$\n消去公共的正因子得到 $1-\\alpha^t  1$，即 $-\\alpha^t  0$。由于 $\\alpha \\in (0,1)$，对于任何有限的 $t \\ge 1$，$\\alpha^t$ 都是正的。因此，这个不等式对所有有限的 $t \\ge 1$ 都成立。对于 $t=0$，$G(0)=0$（因为 $e_0=0$）且 $G(\\infty)0$，所以不等式也成立。因此，对于任何有限的 $t \\ge 0$，$G(t)  G(\\infty)$。\n\n对于第二个任务，我们需要找到满足 $G(t) \\le \\kappa G(\\infty)$ 的最大整数 $t_{\\max}(\\kappa)$，其中给定 $\\kappa \\in (0,1)$。使用 $G(t)$ 和 $G(\\infty)$ 的表达式：\n$$\n\\mu \\sqrt{1+\\delta_k} \\frac{1-\\alpha^t}{1-\\alpha} \\le \\kappa \\left( \\mu \\frac{\\sqrt{1+\\delta_k}}{1-\\alpha} \\right)\n$$\n正项 $\\mu \\frac{\\sqrt{1+\\delta_k}}{1-\\alpha}$ 从两边消去，留下一个只含 $\\alpha$、$\\kappa$ 和 $t$ 的简单不等式：\n$$\n1 - \\alpha^t \\le \\kappa\n$$\n整理各项，我们得到：\n$$\n1 - \\kappa \\le \\alpha^t\n$$\n为了解出 $t$，我们对两边取自然对数。因为 $\\alpha \\in (0,1)$，其对数 $\\ln(\\alpha)$ 是负数。因此，当我们将不等式两边除以 $\\ln(\\alpha)$ 时，必须反转不等号的方向。\n$$\n\\ln(1-\\kappa) \\le t \\ln(\\alpha)\n$$\n$$\nt \\le \\frac{\\ln(1-\\kappa)}{\\ln(\\alpha)}\n$$\n注意到由于 $\\kappa \\in (0,1)$，$1-\\kappa$ 也在 $(0,1)$ 区间内，使得 $\\ln(1-\\kappa)$ 为负。两个负数之比为正，这与对 $t$ 的预期相符。\n问题要求的是 $t_{\\max}(\\kappa)$，即满足此条件的最大整数 $t$。这由右侧表达式的向下取整函数（floor function）给出。\n$$\nt_{\\max}(\\kappa) = \\left\\lfloor \\frac{\\ln(1 - \\kappa)}{\\ln(\\alpha)} \\right\\rfloor\n$$\n这就得到了仅用 $\\alpha$ 和 $\\kappa$ 表示的 $t_{\\max}(\\kappa)$ 的所要求的闭式表达式。", "answer": "$$\n\\boxed{\\left\\lfloor \\frac{\\ln(1 - \\kappa)}{\\ln(\\alpha)} \\right\\rfloor}\n$$", "id": "3430320"}, {"introduction": "我们最后的动手实践将理论与计算领域紧密结合。你将同时扮演攻击者和防御者的角色，针对LAD-Lasso估计器——一种以其对异常值 (outliers) 的稳健性而闻名的方法。这项任务不仅要求你推导出一个关于最坏情况恢复误差的理论上界，还需要你通过构建和测试特定的对抗性攻击，实现一个求解器来凭经验找到这个最坏情况误差。这个练习将让你对理论界限与实际性能之间的差距有具体的理解。[@problem_id:3430330]", "problem": "考虑在压缩感知中带有对抗性离群值的测量模型。设 $A \\in \\mathbb{R}^{m \\times n}$ 是一个已知的传感矩阵，$x \\in \\mathbb{R}^{n}$ 是一个固定但未知的信号，$e_s \\in \\mathbb{R}^{m}$ 是一个对抗性离群值向量，其稀疏度为 $\\|e_s\\|_0 = s$。观测值为 $y = A x + e_s$。为了进行恢复，使用最小绝对偏差 (LAD)-Lasso 估计器，其定义为以下凸优化问题的解\n$$\n\\widehat{x}(y) \\in \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\|A z - y\\|_1 + \\lambda \\|z\\|_1 \\right\\},\n$$\n其中 $\\lambda  0$ 是一个正则化参数。假设对抗方受限于一个振幅预算 $\\alpha  0$，使得 $e_s$ 的每个非零项满足 $|e_{s,i}| \\le \\alpha$。对抗方选择 $e_s$ 的支撑集和符号，以最大化恢复误差 $\\|\\widehat{x}(y) - x\\|_2$。\n\n你的任务是：\n- 为每个测试用例构建一个最坏情况下的对抗性离群值向量 $e_s$，其稀疏度为 $\\|e_s\\|_0 = s$ 且在其支撑集上满足 $|e_{s,i}| = \\alpha$，该向量在 LAD-Lasso 估计器下最大化 $\\|\\widehat{x}(A x + e_s) - x\\|_2$。在给定约束下，证明为何对于最坏情况设计，取极值振幅 $|e_{s,i}| = \\alpha$ 是充分的。\n- 从第一性原理出发，推导一个关于可达误差增量 $\\|\\widehat{x}(A x + e_s) - x\\|_2$ 的解析上界，该上界用 $A$、$\\lambda$、$s$ 和 $\\alpha$ 表示。仅使用基本不等式、范数关系和奇异值性质。该界必须是一个可以根据 $A$、$\\lambda$、$s$ 和 $\\alpha$ 进行数值计算的函数。\n- 实现一个程序，为每个测试用例计算在指定的 $s$ 和振幅预算 $\\alpha$ 下，所有稀疏对抗方所导致的最坏情况恢复误差，以及你推导出的解析界。你对 LAD-Lasso 的实现必须是精确的，通过将该优化问题重构为一个线性规划。\n\nLAD-Lasso 问题可以使用辅助变量 $r \\in \\mathbb{R}^{m}$、$t \\in \\mathbb{R}^{m}$ 和 $u \\in \\mathbb{R}^{n}$ 来处理绝对值，从而重构为：\n$$\n\\min_{z, r, t, u} \\sum_{i=1}^{m} t_i + \\lambda \\sum_{j=1}^{n} u_j \\quad \\text{subject to} \\quad r = A z - y,\\; -t \\le r \\le t,\\; -u \\le z \\le u,\\; t \\ge 0,\\; u \\ge 0.\n$$\n这是一个线性规划问题。\n\n你必须从以下基本事实和定义开始你的推导：\n- 范数的三角不等式以及 $\\ell_1$ 和 $\\ell_2$ 范数的定义：对于任意 $v \\in \\mathbb{R}^{k}$，有 $\\|v\\|_1 = \\sum_{i=1}^{k} |v_i|$ 和 $\\|v\\|_2 = \\left(\\sum_{i=1}^{k} v_i^2\\right)^{1/2}$，且 $\\|v\\|_1 \\ge \\|v\\|_2$。\n- 奇异值：对于任意 $A \\in \\mathbb{R}^{m \\times n}$ 和任意 $w \\in \\mathbb{R}^{n}$，有 $\\|A w\\|_2 \\ge \\sigma_{\\min}(A) \\|w\\|_2$，其中 $\\sigma_{\\min}(A)$ 是 $A$ 的最小奇异值。\n- 对于任意 $w \\in \\mathbb{R}^{n}$，有 $\\|w\\|_1 \\le \\sqrt{n} \\|w\\|_2$。\n\n除了这些基本事实外，你不得使用任何未经证明的快捷公式或界。\n\n每个测试用例的对抗方设计空间是集合\n$$\n\\mathcal{E}(s, \\alpha) = \\left\\{ e \\in \\mathbb{R}^{m} : \\|e\\|_0 = s,\\; e_i \\in \\{-\\alpha, +\\alpha\\} \\text{ on the support},\\; e_i = 0 \\text{ off the support} \\right\\}.\n$$\n精确地枚举 $\\mathcal{E}(s, \\alpha)$ 来找到最坏情况误差。\n\n不涉及任何物理或角度单位。所有输出必须是实数。将所有程序输出四舍五入到六位小数。\n\n测试套件：\n- 情况 1：$m = 7$, $n = 5$, \n$$\nA = \\begin{bmatrix}\n0.50  -0.10  0.30  0.20  -0.40 \\\\\n0.00  0.40  -0.20  0.50  0.10 \\\\\n0.30  0.60  0.10  -0.30  0.20 \\\\\n-0.20  0.10  0.50  0.10  0.60 \\\\\n0.40  -0.30  -0.10  0.70  -0.20 \\\\\n-0.10  0.50  0.40  -0.20  0.30 \\\\\n0.20  -0.20  0.60  0.40  -0.10\n\\end{bmatrix},\\quad\nx = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.5 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\\quad s = 2,\\quad \\alpha = 0.5,\\quad \\lambda = 0.02.\n$$\n- 情况 2：$m = 7$, $n = 5$,\n$$\nA = \\begin{bmatrix}\n0.60  0.10  -0.30  0.25  0.20 \\\\\n0.20  -0.50  0.40  -0.10  0.30 \\\\\n-0.10  0.40  0.20  0.50  -0.20 \\\\\n0.30  0.30  -0.20  0.60  0.10 \\\\\n0.50  -0.20  0.10  -0.30  0.40 \\\\\n0.00  0.60  -0.10  0.20  0.50 \\\\\n0.40  -0.10  0.60  -0.20  0.00\n\\end{bmatrix},\\quad\nx = \\begin{bmatrix} 0.7 \\\\ -0.3 \\\\ 0.0 \\\\ 0.5 \\\\ 0.0 \\end{bmatrix},\\quad s = 3,\\quad \\alpha = 0.3,\\quad \\lambda = 0.02.\n$$\n- 情况 3：与情况 1 相同的 $A$ 和 $x$，其中 $s = 0$, $\\alpha = 0.0$, $\\lambda = 0.02$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个列表的列表，每个测试用例对应一个内层列表，格式为 $[\\text{worst\\_error}, \\text{bound}]$，两个值都四舍五入到六位小数。例如：$[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$。", "solution": "该问题要求分析在稀疏对抗性离群值下，最小绝对偏差 (LAD)-Lasso 估计器的最坏情况误差，推导此误差的解析界，并进行数值实现以计算这两个量。\n\n分析过程分为三个部分：对抗方模型的合理性证明、误差界的推导以及计算方法的描述。\n\n**1. 最坏情况下的对抗性离群值设计**\n\n对抗方的目标是选择一个离群值向量 $e_s$ 以最大化恢复误差 $\\|\\widehat{x}(A x + e_s) - x\\|_2$。对抗方受限于稀疏度预算 $\\|e_s\\|_0 = s$ 和振幅预算，其中非零项满足 $|e_{s,i}| \\le \\alpha$。问题指定将搜索空间缩小到非零项具有极值大小（即 $|e_{s,i}| = \\alpha$）的对抗方。\n\n这种缩减的合理性源于对抗方旨在破坏的优化问题的结构。LAD-Lasso 估计器求解：\n$$ \\widehat{x}(y) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\|A z - y\\|_1 + \\lambda \\|z\\|_1 \\right\\} $$\n令 $h = \\widehat{x}(y) - x$ 为恢复误差。LAD-Lasso 问题可以表示为求解 $\\|A(x+h) - (Ax+e_s)\\|_1 + \\lambda \\|x+h\\|_1$ 的最小值，这可以简化为最小化 $\\|Ah - e_s\\|_1 + \\lambda \\|x+h\\|_1$。$e_s$ 项在 $\\ell_1$ 范数项内充当一个仿射扰动。\n\n被最小化的函数是凸函数。对抗方的问题是从可行集 $\\mathcal{C} = \\{e \\in \\mathbb{R}^m : \\|e\\|_0 = s, \\|e\\|_\\infty \\le \\alpha\\}$ 中选择 $e_s$，以最大化所得误差的范数 $\\|\\widehat{h}\\|_2$。对于 $e_s$ 的一个固定支撑集，可行值构成一个超立方体 $[-\\alpha, \\alpha]^s$。对于许多类别的参数优化问题，包括可将 LAD-Lasso 公式化为其一的线性规划 (LP)，响应函数在多面体不确定性集合上的最大值在该集合的一个顶点处取得。直观上，为了最大化解与真实信号 $x$ 之间的偏差，对抗方应施加尽可能大的扰动。超立方体的顶点对应于极值，其中 $e_s$ 的每个非零分量为 $-\\alpha$ 或 $+\\alpha$。因此，要找到最坏情况误差，只需考虑有限的对抗方集合 $\\mathcal{E}(s, \\alpha)$ 就足够了。\n\n**2. 解析误差界的推导**\n\n我们的任务是从第一性原理出发，推导恢复误差 $\\|\\widehat{x}(y) - x\\|_2$ 的一个上界。令 $h = \\widehat{x}(y) - x$ 为误差向量，其中 $y = Ax + e_s$。\n\n根据 $\\widehat{x}(y)$ 作为最小化子的定义，目标函数在 $\\widehat{x}$ 处的值必须小于或等于其在任何其他点（包括真实信号 $x$）处的值。\n$$ \\|A\\widehat{x} - y\\|_1 + \\lambda \\|\\widehat{x}\\|_1 \\le \\|Ax - y\\|_1 + \\lambda \\|x\\|_1 $$\n代入 $\\widehat{x} = x + h$ 和 $y = Ax + e_s$：\n$$ \\|A(x+h) - (Ax+e_s)\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|Ax - (Ax+e_s)\\|_1 + \\lambda \\|x\\|_1 $$\n$$ \\|Ah - e_s\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|-e_s\\|_1 + \\lambda \\|x\\|_1 $$\n使用 $\\| -v \\|_1 = \\|v\\|_1$，我们简化为：\n$$ \\|Ah - e_s\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|e_s\\|_1 + \\lambda \\|x\\|_1 $$\n我们对 $\\|Ah - e_s\\|_1$ 项使用反三角不等式 $\\|u-v\\|_1 \\ge \\|u\\|_1 - \\|v\\|_1$：\n$$ \\|Ah\\|_1 - \\|e_s\\|_1 \\le \\|Ah - e_s\\|_1 $$\n将此代入前一个不等式，得到：\n$$ \\|Ah\\|_1 - \\|e_s\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|e_s\\|_1 + \\lambda \\|x\\|_1 $$\n重新整理各项，将 $\\|e_s\\|_1$ 组合到一边，得到基本不等式：\n$$ \\|Ah\\|_1 + \\lambda (\\|x+h\\|_1 - \\|x\\|_1) \\le 2\\|e_s\\|_1 $$\n为了获得一个与 $x$ 无关的界，我们再次使用反三角不等式 $\\|x+h\\|_1 \\ge \\|x\\|_1 - \\|h\\|_1$，这意味着 $\\|x+h\\|_1 - \\|x\\|_1 \\ge -\\|h\\|_1$。代入此式会得到一个更宽松但更易于处理的不等式：\n$$ \\|Ah\\|_1 - \\lambda \\|h\\|_1 \\le 2\\|e_s\\|_1 $$\n现在，我们使用提供的基本事实来将 $\\ell_1$ 范数与 $h$ 的 $\\ell_2$ 范数联系起来。\n1.  $\\ell_1$ 范数不小于 $\\ell_2$ 范数：$\\|Ah\\|_1 \\ge \\|Ah\\|_2$。\n2.  变换后向量 $Ah$ 的 $\\ell_2$ 范数受 $A$ 的最小奇异值 $\\sigma_{\\min}(A)$ 的约束：$\\|Ah\\|_2 \\ge \\sigma_{\\min}(A) \\|h\\|_2$。\n3.  结合以上两点可得：$\\|Ah\\|_1 \\ge \\sigma_{\\min}(A) \\|h\\|_2$。\n\n4.  $h$ 的 $\\ell_1$ 范数通过以下关系与其 $\\ell_2$ 范数相关联：$\\|h\\|_1 \\le \\sqrt{n} \\|h\\|_2$。\n\n将这些关系代入不等式 $\\|Ah\\|_1 - \\lambda \\|h\\|_1 \\le 2\\|e_s\\|_1$ 中：\n$$ \\sigma_{\\min}(A) \\|h\\|_2 - \\lambda \\sqrt{n} \\|h\\|_2 \\le 2\\|e_s\\|_1 $$\n提取公因式 $\\|h\\|_2$：\n$$ (\\sigma_{\\min}(A) - \\lambda \\sqrt{n}) \\|h\\|_2 \\le 2\\|e_s\\|_1 $$\n对抗方向量 $e_s$ 的 $\\ell_1$ 范数为 $\\|e_s\\|_1 = \\sum_{i \\in \\text{supp}(e_s)} |e_{s,i}| = s \\alpha$。\n因此，我们有：\n$$ (\\sigma_{\\min}(A) - \\lambda \\sqrt{n}) \\|h\\|_2 \\le 2 s \\alpha $$\n在 $\\sigma_{\\min}(A) - \\lambda \\sqrt{n} > 0$ 的条件下，我们可以分离出 $\\|h\\|_2$ 来获得恢复误差的最终上界：\n$$ \\|\\widehat{x}(y) - x\\|_2 \\le \\frac{2 s \\alpha}{\\sigma_{\\min}(A) - \\lambda \\sqrt{n}} $$\n正如要求，该界仅依赖于问题参数 $A$、$\\lambda$、$s$ 和 $\\alpha$。\n\n**3. 计算策略**\n\n数值实现包括两个主要部分：通过枚举计算最坏情况误差，以及评估推导出的解析界。\n\n**a. 通过线性规划 (LP) 求解 LAD-Lasso**\n通过将 LAD-Lasso 优化问题重构为一个 LP，可以精确求解。LP 的变量向量是 $X = [z^T, r^T, t^T, u^T]^T \\in \\mathbb{R}^{2n+2m}$。该 LP 为：\n$$ \\min_{X} c^T X \\quad \\text{subject to} \\quad A_{\\text{eq}}X = b_{\\text{eq}}, \\quad A_{\\text{ub}}X \\le b_{\\text{ub}}, \\quad l \\le X \\le u $$\n- **目标向量 $c$**：$c = [0_n^T, 0_m^T, 1_m^T, \\lambda 1_n^T]^T$。\n- **等式约束 $A_{\\text{eq}}, b_{\\text{eq}}$**：来自 $r = Az - y \\iff -Az + r = -y$。\n  $A_{\\text{eq}} = [-A, I_m, 0_{m \\times m}, 0_{m \\times n}]$, $b_{\\text{eq}} = -y$。\n- **不等式约束 $A_{\\text{ub}}, b_{\\text{ub}}$**：来自 $-t \\le r \\le t$ 和 $-u \\le z \\le u$。\n  $r-t \\le 0$, $-r-t \\le 0$, $z-u \\le 0$, $-z-u \\le 0$。这些被堆叠成一个块状矩阵 $A_{\\text{ub}}$ 和一个零向量 $b_{\\text{ub}}$。\n- **边界 $l, u$**：$z$ 和 $r$ 是无界的。$t \\ge 0$ 且 $u \\ge 0$。这定义了 $X$ 每个分量的下界和上界。\n\n**b. 最坏情况误差计算**\n最坏情况误差是通过搜索整个指定的对抗集合 $\\mathcal{E}(s, \\alpha)$ 来找到的。具体步骤如下：\n1.  从 $\\{1, \\dots, m\\}$ 中生成所有 $s$ 个索引的组合，作为 $e_s$ 的支撑集。这样的支撑集数量为 $\\binom{m}{s}$。\n2.  对于每个支撑集，生成非零项所有 $2^s$ 种可能的符号模式。\n3.  对于每个生成的 $e_s$，计算 $y = Ax+e_s$，使用 LP 求解器求解 $\\widehat{x}(y)$，计算误差 $\\|\\widehat{x}(y)-x\\|_2$，并记录找到的最大误差。\n\n需要测试的对抗方总数为 $\\binom{m}{s} 2^s$，对于给定的测试用例，这个计算量是可行的。\n\n**c. 解析界计算**\n推导出的界 $B = \\frac{2 s \\alpha}{\\sigma_{\\min}(A) - \\lambda \\sqrt{n}}$ 的计算方法如下：\n1.  计算 $A$ 的奇异值并找到最小值 $\\sigma_{\\min}(A)$。\n2.  将 $s, \\alpha, \\lambda, n$ 和 $\\sigma_{\\min}(A)$ 的值代入公式。对于 $s=0$ 的情况，界为 $0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\nfrom itertools import combinations, product\n\ndef solve_lad_lasso(A, y, lambda_val):\n    \"\"\"\n    Solves the LAD-Lasso problem by reformulating it as a Linear Program.\n    min ||Az - y||_1 + lambda * ||z||_1\n    \"\"\"\n    m, n = A.shape\n    \n    # LP variable vector: X = [z, r, t, u]\n    # Dimensions: z (n), r (m), t (m), u (n)\n    # Total variables: 2n + 2m\n\n    # Objective function: min sum(t_i) + lambda * sum(u_j)\n    c = np.concatenate([\n        np.zeros(n),           # Cost for z\n        np.zeros(m),           # Cost for r\n        np.ones(m),            # Cost for t\n        lambda_val * np.ones(n) # Cost for u\n    ])\n\n    # Equality constraint: r = Az - y  => -Az + r = -y\n    A_eq = np.hstack([-A, np.eye(m), np.zeros((m, m)), np.zeros((m, n))])\n    b_eq = -y\n\n    # Inequality constraints: A_ub @ X = b_ub\n    # -t = r = t  =>  r - t = 0  and -r - t = 0\n    # -u = z = u  =>  z - u = 0  and -z - u = 0\n    \n    # r - t = 0\n    A_ub1 = np.hstack([np.zeros((m, n)), np.eye(m), -np.eye(m), np.zeros((m, n))])\n    # -r - t = 0\n    A_ub2 = np.hstack([np.zeros((m, n)), -np.eye(m), -np.eye(m), np.zeros((m, n))])\n    # z - u = 0\n    A_ub3 = np.hstack([np.eye(n), np.zeros((n, m)), np.zeros((n, m)), -np.eye(n)])\n    # -z - u = 0\n    A_ub4 = np.hstack([-np.eye(n), np.zeros((n, m)), np.zeros((n, m)), -np.eye(n)])\n    \n    A_ub = np.vstack([A_ub1, A_ub2, A_ub3, A_ub4])\n    b_ub = np.zeros(A_ub.shape[0])\n\n    # Variable bounds: z, r are free; t, u are non-negative.\n    bounds = ([(None, None)] * (n + m)) + ([(0, None)] * (m + n))\n\n    # Solve the LP\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n\n    if not res.success:\n        # The problem formulation should guarantee a solution exists.\n        # This case is included for robustness but not expected to be hit.\n        raise RuntimeError(\"LP solver failed to find a solution.\")\n\n    # Extract the solution for z\n    z = res.x[:n]\n    return z\n\ndef calculate_worst_error(A, x, s, alpha, lambda_val):\n    \"\"\"\n    Finds the worst-case recovery error by enumerating all adversaries.\n    \"\"\"\n    m, n = A.shape\n    \n    # Base case with no adversary\n    if s == 0:\n        e_s = np.zeros(m)\n        y = A @ x + e_s\n        x_hat = solve_lad_lasso(A, y, lambda_val)\n        return np.linalg.norm(x_hat - x)\n\n    max_sq_error = 0.0\n    indices = np.arange(m)\n\n    # Iterate over all possible supports for the outlier vector e_s\n    for support in combinations(indices, s):\n        # Iterate over all sign patterns (+/- alpha) on the support\n        for signs in product([-1, 1], repeat=s):\n            e_s = np.zeros(m)\n            support_list = list(support)\n            e_s[support_list] = np.array(signs) * alpha\n            \n            y = A @ x + e_s\n            x_hat = solve_lad_lasso(A, y, lambda_val)\n            \n            sq_error = np.sum((x_hat - x)**2)\n            if sq_error > max_sq_error:\n                max_sq_error = sq_error\n                \n    return np.sqrt(max_sq_error)\n\ndef calculate_bound(A, n, s, alpha, lambda_val):\n    \"\"\"\n    Calculates the derived analytic upper bound on the error.\n    \"\"\"\n    if s == 0:\n        return 0.0\n        \n    singular_values = np.linalg.svd(A, compute_uv=False)\n    sigma_min_A = np.min(singular_values)\n    \n    denominator = sigma_min_A - lambda_val * np.sqrt(n)\n    \n    if denominator = 1e-9: # Add a small tolerance for stability; if non-positive, bound is not valid/infinite.\n        return float('inf')\n        \n    bound = (2 * s * alpha) / denominator\n    return bound\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"m\": 7, \"n\": 5,\n            \"A\": np.array([\n                [0.50, -0.10, 0.30, 0.20, -0.40],\n                [0.00, 0.40, -0.20, 0.50, 0.10],\n                [0.30, 0.60, 0.10, -0.30, 0.20],\n                [-0.20, 0.10, 0.50, 0.10, 0.60],\n                [0.40, -0.30, -0.10, 0.70, -0.20],\n                [-0.10, 0.50, 0.40, -0.20, 0.30],\n                [0.20, -0.20, 0.60, 0.40, -0.10]\n            ]),\n            \"x\": np.array([1.0, 0.0, 0.5, 0.0, 0.0]),\n            \"s\": 2, \"alpha\": 0.5, \"lambda\": 0.02\n        },\n        {\n            \"m\": 7, \"n\": 5,\n            \"A\": np.array([\n                [0.60, 0.10, -0.30, 0.25, 0.20],\n                [0.20, -0.50, 0.40, -0.10, 0.30],\n                [-0.10, 0.40, 0.20, 0.50, -0.20],\n                [0.30, 0.30, -0.20, 0.60, 0.10],\n                [0.50, -0.20, 0.10, -0.30, 0.40],\n                [0.00, 0.60, -0.10, 0.20, 0.50],\n                [0.40, -0.10, 0.60, -0.20, 0.00]\n            ]),\n            \"x\": np.array([0.7, -0.3, 0.0, 0.5, 0.0]),\n            \"s\": 3, \"alpha\": 0.3, \"lambda\": 0.02\n        },\n        {\n            \"m\": 7, \"n\": 5,\n            \"A\": np.array([\n                [0.50, -0.10, 0.30, 0.20, -0.40],\n                [0.00, 0.40, -0.20, 0.50, 0.10],\n                [0.30, 0.60, 0.10, -0.30, 0.20],\n                [-0.20, 0.10, 0.50, 0.10, 0.60],\n                [0.40, -0.30, -0.10, 0.70, -0.20],\n                [-0.10, 0.50, 0.40, -0.20, 0.30],\n                [0.20, -0.20, 0.60, 0.40, -0.10]\n            ]),\n            \"x\": np.array([1.0, 0.0, 0.5, 0.0, 0.0]),\n            \"s\": 0, \"alpha\": 0.0, \"lambda\": 0.02\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A, x, s, alpha, lambda_val, n = case[\"A\"], case[\"x\"], case[\"s\"], case[\"alpha\"], case[\"lambda\"], case[\"n\"]\n        \n        worst_error = calculate_worst_error(A, x, s, alpha, lambda_val)\n        bound = calculate_bound(A, n, s, alpha, lambda_val)\n        \n        results.append([round(worst_error, 6), round(bound, 6)])\n\n    # Final print statement in the exact required format.\n    print(f\"[[{results[0][0]:.6f},{results[0][1]:.6f}],[{results[1][0]:.6f},{results[1][1]:.6f}],[{results[2][0]:.6f},{results[2][1]:.6f}]]\")\n\nsolve()\n```", "id": "3430330"}]}