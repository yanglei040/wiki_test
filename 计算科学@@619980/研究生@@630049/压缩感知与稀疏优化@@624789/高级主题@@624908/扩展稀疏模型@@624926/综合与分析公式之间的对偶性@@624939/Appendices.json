{"hands_on_practices": [{"introduction": "我们从一个基础的计算练习开始，以巩固对综合模型基本原理的理解。在这个练习中，你将在一个简单且可控的环境下，手动求解一个综合基追踪（Basis Pursuit）问题 ([@problem_id:3445067])。这个练习的重点不在于数值结果本身，而在于运用拉格朗日对偶等一阶原理来验证解的最优性，并揭示综合模型与其对偶问题之间的深刻联系。", "problem": "考虑压缩感知和稀疏优化中的综合与分析观点。在综合模型中，一个信号 $x \\in \\mathbb{R}^{n}$ 被表示为 $x = D z$，其中 $D \\in \\mathbb{R}^{n \\times p}$ 是一个综合字典，$z \\in \\mathbb{R}^{p}$ 是一个期望稀疏的系数向量。给定无噪声测量值 $y \\in \\mathbb{R}^{m}$ 和一个感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$，综合基追踪（BP）问题旨在找到具有最小 $\\ell_{1}$-范数的 $z$，使得一致性约束 $A D z = y$ 成立。在分析模型中，一个分析算子 $\\Omega \\in \\mathbb{R}^{q \\times n}$ 作用于 $x$ 以产生期望稀疏的 $\\Omega x$；分析BP在测量约束下最小化 $\\Omega x$ 的 $\\ell_{1}$-范数。\n\n在具体设定 $n = 2$, $m = 2$, $p = 2$, $A = I \\in \\mathbb{R}^{2 \\times 2}$, $D = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix}$ 及 $y = (1, 1)^{\\top} \\in \\mathbb{R}^{2}$ 下进行计算。从综合BP的定义（即在线性一致性约束下对 $z$ 的 $\\ell_{1}$-范数进行约束最小化）出发，推导综合BP解 $z^{\\star} \\in \\mathbb{R}^{2}$ 及相应的信号 $x^{\\star} = D z^{\\star} \\in \\mathbb{R}^{2}$。你的推导应基于第一性原理，包括通过拉格朗日对偶性进行的可行性和最优性证明，并应通过对偶可行性条件明确阐述与分析观点的关系。将最终答案表示为数对 $(z^{\\star}, x^{\\star})$。无需四舍五入，且不涉及物理单位。", "solution": "该问题要求解一个综合基追踪（BP）问题的特定实例的解 $(z^{\\star}, x^{\\star})$，并要求推导过程基于拉格朗日对偶性的第一性原理，同时明确阐述其与分析观点的联系。\n\n综合BP问题被表述为在线性一致性约束下，最小化系数向量 $z$ 的 $\\ell_1$-范数：\n$$ \\min_{z \\in \\mathbb{R}^{p}} \\|z\\|_1 \\quad \\text{subject to} \\quad ADz = y $$\n\n首先，我们代入指定的矩阵和向量值：\n- $n = 2$, $m = 2$, $p = 2$\n- 感知矩阵: $A = I \\in \\mathbb{R}^{2 \\times 2}$ (单位矩阵)\n- 综合字典: $D = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\in \\mathbb{R}^{2 \\times 2}$\n- 测量向量: $y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\in \\mathbb{R}^{2}$\n\n约束 $ADz = y$ 简化为 $Dz = y$。因此，原优化问题是：\n$$ \\min_{z = (z_1, z_2)^{\\top} \\in \\mathbb{R}^{2}} |z_1| + |z_2| \\quad \\text{subject to} \\quad \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\n该约束代表了一个包含两个变量的两个线性方程组：\n1. $z_1 + z_2 = 1$\n2. $z_2 = 1$\n\n由第二个方程，我们得到 $z_2 = 1$。将其代入第一个方程，得到 $z_1 + 1 = 1$，这意味着 $z_1 = 0$。\n因此，该优化问题的可行集仅包含一个点，$z = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。由于这是唯一的可行点，它必然是该最小化问题的解。我们将此原问题解表示为：\n$$ z^{\\star} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$\n\n通过应用综合字典 $D$ 可以找到对应的信号 $x^{\\star}$：\n$$ x^{\\star} = D z^{\\star} = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\n为了正式验证此解并满足问题要求，我们使用拉格朗日对偶性框架。原问题的拉格朗日函数是：\n$$ \\mathcal{L}(z, \\nu) = \\|z\\|_1 + \\nu^{\\top}(y - Dz) $$\n其中 $\\nu \\in \\mathbb{R}^2$ 是与等式约束相关联的拉格朗日乘子（或对偶变量）。\n\n对于这个凸问题，Karush-Kuhn-Tucker (KKT) 条件为最优性提供了充要条件。一对 $(z^{\\star}, \\nu^{\\star})$ 是最优的，如果它满足：\n1.  **原问题可行性**：$Dz^{\\star} = y$。\n    如上所示，$z^{\\star} = (0, 1)^{\\top}$ 满足此条件：$D z^{\\star} = (1, 1)^{\\top} = y$。\n2.  **平稳性**：拉格朗日函数关于 $z$ 在 $z^{\\star}$ 处的次梯度必须包含零向量。这意味着 $0 \\in \\partial \\|z^\\star\\|_1 - D^{\\top}\\nu^{\\star}$，或等价地，$D^{\\top}\\nu^{\\star} \\in \\partial \\|z^\\star\\|_1$。\n    $\\ell_1$-范数 $\\|z\\|_1 = |z_1| + |z_2|$ 在 $z^{\\star} = (0, 1)^{\\top}$ 处的次梯度是向量 $s=(s_1, s_2)^{\\top}$ 的集合，其中 $s_1 \\in [-1, 1]$ 且 $s_2 = \\text{sign}(z_2^{\\star}) = 1$。\n    因此，我们必须找到一个对偶向量 $\\nu^{\\star}$，使得对于某个满足 $s_1 \\in [-1, 1]$ 和 $s_2 = 1$ 的 $s$，有 $D^{\\top}\\nu^{\\star} = s$。\n    $D$ 的转置是 $D^{\\top} = \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix}$。该条件是：\n    $$ \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} \\nu_1^{\\star} \\\\ \\nu_2^{\\star} \\end{bmatrix} = \\begin{bmatrix} s_1 \\\\ 1 \\end{bmatrix} $$\n    这得到方程组 $\\nu_1^{\\star} = s_1$ 和 $\\nu_1^{\\star} + \\nu_2^{\\star} = 1$。\n3.  **对偶可行性**：对偶变量 $\\nu^{\\star}$ 必须对对偶问题是可行的。对偶问题是 $\\max_{\\nu} y^{\\top}\\nu$，约束条件为 $\\|D^{\\top}\\nu\\|_{\\infty} \\le 1$。因此条件是 $\\|D^{\\top}\\nu^{\\star}\\|_{\\infty} \\le 1$。\n    根据平稳性条件，我们有 $D^{\\top}\\nu^{\\star} = (s_1, 1)^{\\top}$。因此，对偶可行性条件是 $\\|(s_1, 1)^{\\top}\\|_{\\infty} = \\max(|s_1|, |1|) \\le 1$。由于我们要求 $s_1 \\in [-1, 1]$，所以 $|s_1| \\le 1$，因此 $\\max(|s_1|, 1) = 1$。该条件对于 $s_1 \\in [-1, 1]$ 的任何选择都成立。\n\n我们可以为 $s_1$ 选择一个具体值，例如 $s_1=0$。这得到 $\\nu_1^{\\star} = 0$。从 $\\nu_1^{\\star} + \\nu_2^{\\star} = 1$，我们得到 $\\nu_2^{\\star} = 1$。因此，一个有效的对偶凭证是 $\\nu^{\\star} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。通过此选择，所有KKT条件都得到满足，从而正式证明 $z^{\\star} = (0, 1)^{\\top}$ 是唯一的最优解。\n\n最后，我们讨论与分析观点的关系。分析BP问题是 $\\min_x \\|\\Omega x\\|_1$，约束条件为 $Ax=y$。在这个字典 $D$ 是方阵（$n=p=2$）且可逆的特定设定下，综合问题等价于一个分析问题。通过选择分析算子 $\\Omega = D^{-1}$ 可以建立这种等价性。\n$$ \\Omega = D^{-1} = \\left(\\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix}\\right)^{-1} = \\begin{bmatrix} 1  -1 \\\\ 0  1 \\end{bmatrix} $$\n这两种观点之间的联系通过它们的对偶问题得以揭示。综合问题的对偶问题是：\n$$ (\\text{D}_{\\text{synth}}): \\quad \\max_{\\nu} y^{\\top}\\nu \\quad \\text{subject to} \\quad \\|D^{\\top}\\nu\\|_{\\infty} \\le 1 $$\n分析问题（$\\min_x \\|\\Omega x\\|_1$ s.t. $Ax=y$）的对偶问题是：\n$$ (\\text{D}_{\\text{anal}}): \\quad \\max_{\\lambda} y^{\\top}\\lambda \\quad \\text{subject to} \\quad A^\\top\\lambda = \\Omega^{\\top}\\mu \\text{ for some } \\mu \\text{ with } \\|\\mu\\|_{\\infty} \\le 1 $$\n在我们的设定中 $A=I$，分析对偶约束变为 $\\lambda = \\Omega^\\top \\mu$。将其代入分析对偶可行性条件 $\\| \\mu \\|_\\infty \\le 1$，并使用 $\\Omega=D^{-1}$，我们有 $\\lambda = (D^{-1})^\\top \\mu \\implies D^\\top \\lambda = \\mu$。因此，约束 $\\| \\mu \\|_\\infty \\le 1$ 变为 $\\|D^\\top \\lambda \\|_\\infty \\le 1$。\n\n这表明，综合问题的对偶可行性条件 $\\|D^{\\top}\\nu\\|_{\\infty} \\le 1$ 与分析问题的对偶可行性条件 $\\|D^\\top \\lambda \\|_\\infty \\le 1$ 具有完全相同的形式。综合问题中的对偶变量 $\\nu$ 与分析问题中的对偶变量 $\\lambda$ 受相同的约束，通过它们的对偶表述统一了这两种观点。\n\n推导出的综合BP解是 $z^{\\star} = (0, 1)^{\\top}$，对应的信号是 $x^{\\star} = (1, 1)^{\\top}$。", "answer": "$$\\boxed{\\begin{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} , \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\end{pmatrix}}$$", "id": "3445067"}, {"introduction": "从理论推导转向数值实现，这个练习要求你比较综合与分析LASSO（Least Absolute Shrinkage and Selection Operator）模型的数值解 ([@problem_id:3445015])。你将为这两种模型分别实现合适的凸优化算法，并观察它们在相同数据下产生的恢复结果有何不同。这个实践旨在揭示，尽管在某些理想条件下综合与分析模型等价，但在更普遍的LASSO框架下，它们是不同的模型，选择不同模型会对信号恢复产生实际影响。", "problem": "要求您实现一个完整且可运行的程序，在一个小规模场景下对稀疏恢复的合成公式和分析公式进行数值比较。该比较必须在多个测试用例上进行，以突出两种公式在压缩感知和稀疏优化背景下的结构性差异。其基本基础是针对这两种公式的凸优化框架及相关的近端算法。\n\n令 $A \\in \\mathbb{R}^{2 \\times 3}$ 为一个随机高斯感知矩阵， $D = I_3$ 为用于合成稀疏性的单位字典，并令 $\\Omega \\in \\mathbb{R}^{2 \\times 3}$ 为由下式定义的一阶差分算子\n$$\n\\Omega = \\begin{bmatrix}\n-1  1  0 \\\\\n0  -1  1\n\\end{bmatrix}.\n$$\n对于一个给定的真实信号 $x^\\star \\in \\mathbb{R}^{3}$，测量值为 $y = A x^\\star$ (无噪声)。您将从同一测量值 $y$ 计算以下两个估计量 (最小绝对收缩和选择算子 (LASSO))：\n- 使用 $D=I_3$ 的合成 LASSO：\n$$\n\\widehat{x}_{\\text{synth}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1.\n$$\n- 使用一阶差分的分析 LASSO：\n$$\n\\widehat{x}_{\\text{anal}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|\\Omega x\\|_1.\n$$\n\n您必须使用与上述目标一致的正确凸优化算法来计算数值解。对于合成 LASSO，请使用近端梯度法，例如快速迭代收缩阈值算法 (FISTA)。对于分析 LASSO，请使用分裂方案，例如交替方向乘子法 (ADMM)，并设置分裂变量 $v = \\Omega x$。\n\n为保证可复现性，感知矩阵 $A$ 必须使用固定的伪随机种子生成一次，具体如下：\n- 从零均值、单位方差的高斯分布中独立抽取 $A$ 的元素，并按 $1/\\sqrt{2}$进行缩放，即 $A = \\frac{1}{\\sqrt{2}} G$，其中 $G_{ij} \\sim \\mathcal{N}(0,1)$。\n- 此次生成使用伪随机种子 $7$，并在所有测试用例中保持 $A$ 不变。\n\n不涉及物理单位或角度；不使用百分比。\n\n测试套件：\n- 使用以下四个测试用例。在每个用例中，计算 $y = A x^\\star$。为每个用例报告三个浮点数：\n    1. 两个解之间的欧几里得距离，$\\|\\widehat{x}_{\\text{synth}} - \\widehat{x}_{\\text{anal}}\\|_2$。\n    2. 合成解相对于真实值的欧几里得误差，$\\|\\widehat{x}_{\\text{synth}} - x^\\star\\|_2$。\n    3. 分析解相对于真实值的欧几里得误差，$\\|\\widehat{x}_{\\text{anal}} - x^\\star\\|_2$。\n- 用例如下：\n    - 用例 1：$x^\\star = [1.0, 1.0, 0.0]^\\top$，$\\lambda = 0.2$。\n    - 用例 2：$x^\\star = [1.5, 0.0, 0.0]^\\top$，$\\lambda = 0.2$。\n    - 用例 3：$x^\\star = [0.5, 0.5, 0.5]^\\top$，$\\lambda = 0.5$。\n    - 用例 4：$x^\\star = [0.8, -0.2, 0.8]^\\top$，$\\lambda = 5.0$。\n\n算法要求：\n- 对于合成 LASSO，实现一个可证明收敛的近端梯度算法，其步长需满足数据保真项梯度的 Lipschitz 常数。\n- 对于分析 LASSO，实现交替方向乘子法 (ADMM)，采用分裂 $v = \\Omega x$，对 $v$ 进行软阈值处理，并使用所产生的正定系统对 $x$ 更新进行线性求解。\n- ADMM 停止条件使用绝对容差 $10^{-6}$ 和相对容差 $10^{-5}$，为保证鲁棒性，迭代上限不低于 $5000$ 次。对于近端梯度法，使用不低于 $5000$ 次的固定迭代上限，以及基于连续迭代差的固定停止规则，容差为 $10^{-9}$。\n\n最终输出规格：\n- 您的程序应生成单行输出，包含一个由逗号分隔的列表，列表有四个项目（每个测试用例一个），每个项目本身是按上述顺序排列的三元素列表。所有浮点数必须精确到小数点后六位。格式必须严格如下：\n\"[ [d1,e1s,e1a], [d2,e2s,e2a], [d3,e3s,e3a], [d4,e4s,e4a] ]\"\n不得包含任何额外文本，其中 $d_k$ 是用例 $k$ 的两个解之间的欧几里得距离，$e_{ks}$ 是用例 $k$ 的合成误差，$e_{ka}$ 是用例 $k$ 的分析误差。\n\n注意：\n- 此问题纯属数学和算法问题。在给定种子的情况下，所有量都是确定性的。请通过使用适合小型对称正定系统的方法，确保在 ADMM 中出现的线性求解过程中的数值稳定性。", "solution": "用户提供的问题陈述已经过严格验证，并被认定为**有效**。它在科学上基于凸优化和稀疏信号恢复的原理，设定良好，为确定性数值求解提供了所有必要的参数和条件，并且其表述是客观的。任务是使用适当且指定的迭代算法，比较两种标准的稀疏恢复模型：合成 LASSO 和分析 LASSO。\n\n### 1. 问题公式化\n\n我们给定一个线性测量模型 $y = A x^\\star$，其中 $x^\\star \\in \\mathbb{R}^3$ 是一个真实信号，$A \\in \\mathbb{R}^{2 \\times 3}$ 是一个感知矩阵，$y \\in \\mathbb{R}^2$ 是测量向量。目标是利用两个不同的稀疏性促进优化问题，从 $y$ 和 $A$ 中恢复出 $x^\\star$ 的一个估计。\n\n**合成 LASSO：** 该模型假设信号 $x$ 在一个合成字典 $D$ 中是稀疏的，这里给定字典为单位矩阵 $D=I_3$。其优化问题是：\n$$\n\\widehat{x}_{\\text{synth}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1\n$$\n这里，$\\ell_1$ 范数 $\\|x\\|_1$ 直接促进信号系数的稀疏性。\n\n**分析 LASSO：** 该模型假设信号 $x$ 在经过分析算子 $\\Omega$ 变换后具有稀疏表示。在此问题中，$\\Omega \\in \\mathbb{R}^{2 \\times 3}$ 是一阶差分算子。其优化问题是：\n$$\n\\widehat{x}_{\\text{anal}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|\\Omega x\\|_1\n$$\n该公式鼓励信号梯度的稀疏性，对于分段常数信号是有效的。\n\n### 2. 合成 LASSO：快速迭代收缩阈值算法 (FISTA)\n\n合成 LASSO 问题是一个形式为 $\\min_x f(x) + g(x)$ 的凸优化问题，其中：\n- $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ 是一个光滑、凸、可微的函数。其梯度为 $\\nabla f(x) = A^\\top(A x - y)$。该梯度是 Lipschitz 连续的，其常数 $L = \\|A^\\top A\\|_2$，其中 $\\|\\cdot\\|_2$ 表示谱范数。\n- $g(x) = \\lambda \\|x\\|_1$ 是一个凸的、不可微的函数。\n\n这种结构非常适合使用近端梯度法。FISTA 是基本迭代收缩阈值算法 (ISTA) 的一个加速版本。这些方法的核心是 $g(x)$ 的近端算子，即软阈值函数：\n$$\n\\text{prox}_{\\gamma g}(z) = \\text{soft}(z, \\gamma\\lambda)_i = \\text{sgn}(z_i) \\max(|z_i| - \\gamma\\lambda, 0)\n$$\nFISTA 引入了一个动量项来加速收敛。迭代更新如下，从 $x_0 \\in \\mathbb{R}^3$，$y_1=x_0$，$t_1=1$ 开始：\n对于 $k = 1, 2, \\ldots$：\n1. 通过从动量点 $y_k$ 进行梯度步进并应用近端算子，计算下一个迭代点 $x_k$：\n   $$x_k = \\text{prox}_{\\frac{1}{L}g}(y_k - \\frac{1}{L}\\nabla f(y_k)) = \\text{soft}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - y), \\frac{\\lambda}{L}\\right)$$\n2. 更新动量参数 $t_{k+1}$：\n   $$t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$$\n3. 更新动量点 $y_{k+1}$：\n   $$y_{k+1} = x_k + \\frac{t_k-1}{t_{k+1}}(x_k - x_{k-1})$$\n\n算法以 $x_0 = x_{-1} = 0 \\in \\mathbb{R}^3$ 和 $t_1 = 1$ 初始化。当连续迭代点之间的欧几里得距离低于容差，即 $\\|x_k - x_{k-1}\\|_2  10^{-9}$，或达到最大迭代次数 ($5000$) 时，迭代终止。\n\n### 3. 分析 LASSO：交替方向乘子法 (ADMM)\n\n为了应用 ADMM，分析 LASSO 问题被重新表述为一个约束优化问题。我们引入一个分裂变量 $v \\in \\mathbb{R}^2$，使得 $v = \\Omega x$：\n$$\n\\min_{x, v} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|v\\|_1 \\quad \\text{subject to} \\quad \\Omega x - v = 0\n$$\n该问题的增广拉格朗日量（以缩放对偶形式表示）是：\n$$\nL_\\rho(x, v, u) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda\\|v\\|_1 + \\frac{\\rho}{2}\\|\\Omega x - v + u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2\n$$\n其中 $u \\in \\mathbb{R}^2$ 是缩放对偶变量，$\\rho  0$ 是惩罚参数。ADMM 通过交替地对 $L_\\rho$ 关于 $x$ 和 $v$ 进行最小化，然后更新对偶变量 $u$ 来进行。\n\n迭代更新从 $x_0$、$v_0$、$u_0$（通常为零向量）开始，具体如下：\n1.  **$x$ 更新**：最小化 $L_\\rho$ 关于 $x$：\n    $$x_{k+1} = \\arg\\min_x \\left( \\frac{1}{2}\\|A x - y\\|_2^2 + \\frac{\\rho}{2}\\|\\Omega x - v_k + u_k\\|_2^2 \\right)$$\n    这是一个二次问题，其解通过求解将梯度设置为零得到的线性系统来找到：\n    $$(A^\\top A + \\rho \\Omega^\\top \\Omega) x_{k+1} = A^\\top y + \\rho \\Omega^\\top(v_k - u_k)$$\n    矩阵 $P = A^\\top A + \\rho \\Omega^\\top \\Omega$ 是一个小的（$3 \\times 3$）对称正定矩阵，因此可以预先计算其逆以实现高效更新。\n\n2.  **$v$ 更新**：最小化 $L_\\rho$ 关于 $v$：\n    $$v_{k+1} = \\arg\\min_v \\left( \\lambda\\|v\\|_1 + \\frac{\\rho}{2}\\|\\Omega x_{k+1} - v + u_k\\|_2^2 \\right)$$\n    解由软阈值算子给出：\n    $$v_{k+1} = \\text{soft}\\left(\\Omega x_{k+1} + u_k, \\frac{\\lambda}{\\rho}\\right)$$\n\n3.  **$u$ 更新**：更新对偶变量：\n    $$u_{k+1} = u_k + \\Omega x_{k+1} - v_{k+1}$$\n\n算法根据原始残差和对偶残差终止。\n- 原始残差：$r_{k+1} = \\Omega x_{k+1} - v_{k+1}$\n- 对偶残差：$s_{k+1} = \\rho \\Omega^\\top(v_{k+1} - v_k)$\n\n停止条件是 $\\|r_{k+1}\\|_2 \\leq \\epsilon^{\\text{pri}}$ 和 $\\|s_{k+1}\\|_2 \\leq \\epsilon^{\\text{dual}}$，其中容差为：\n- $\\epsilon^{\\text{pri}} = \\sqrt{2}\\epsilon^{\\text{abs}} + \\epsilon^{\\text{rel}} \\max(\\|\\Omega x_{k+1}\\|_2, \\|v_{k+1}\\|_2)$\n- $\\epsilon^{\\text{dual}} = \\sqrt{3}\\epsilon^{\\text{abs}} + \\epsilon^{\\text{rel}} \\|\\rho \\Omega^\\top u_{k+1}\\|_2$\n\n指定的容差为 $\\epsilon^{\\text{abs}} = 10^{-6}$ 和 $\\epsilon^{\\text{rel}} = 10^{-5}$。使用 $\\rho=1.0$ 的值，最大迭代次数为 $5000$。\n\n### 4. 数值实现与评估\n\n对于四个测试用例中的每一个，执行以下步骤：\n1.  使用 `Numpy` 和固定的随机种子 $7$ 生成一次感知矩阵 $A$。\n2.  设置真实信号 $x^\\star$ 和正则化参数 $\\lambda$。\n3.  计算测量向量 $y = A x^\\star$。\n4.  使用 FISTA 实现计算 $\\widehat{x}_{\\text{synth}}$。\n5.  使用 ADMM 实现计算 $\\widehat{x}_{\\text{anal}}$。\n6.  计算三个所需的度量指标：\n    - 解之间的距离：$d = \\|\\widehat{x}_{\\text{synth}} - \\widehat{x}_{\\text{anal}}\\|_2$\n    - 合成误差：$e_{s} = \\|\\widehat{x}_{\\text{synth}} - x^\\star\\|_2$\n    - 分析误差：$e_{a} = \\|\\widehat{x}_{\\text{anal}} - x^\\star\\|_2$\n\n然后将所有四个用例的结果格式化为指定的字符串输出。", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(u, t):\n    \"\"\"\n    Soft-thresholding operator for vectors.\n    \"\"\"\n    return np.sign(u) * np.maximum(np.abs(u) - t, 0)\n\ndef fista_solver(A, y, lambda_val, max_iter=5000, tol=1e-9):\n    \"\"\"\n    Solves the synthesis LASSO problem using FISTA.\n    min_x 0.5 * ||Ax - y||^2 + lambda * ||x||_1\n    \"\"\"\n    p = A.shape[1]\n    \n    # Lipschitz constant of the gradient of the smooth term\n    L = np.linalg.norm(A.T @ A, ord=2)\n    step_size = 1.0 / L\n\n    x_k = np.zeros((p, 1))\n    y_k = np.zeros((p, 1))\n    t_k = 1.0\n\n    for _ in range(max_iter):\n        x_k_old = x_k\n        grad_f_y = A.T @ (A @ y_k - y)\n        x_k = soft_threshold(y_k - step_size * grad_f_y, step_size * lambda_val)\n\n        if np.linalg.norm(x_k - x_k_old)  tol:\n            break\n\n        t_k_plus_1 = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n        y_k = x_k + ((t_k - 1.0) / t_k_plus_1) * (x_k - x_k_old)\n\n        t_k = t_k_plus_1\n        \n    return x_k\n\ndef admm_solver(A, y, Omega, lambda_val, rho=1.0, max_iter=5000, eps_abs=1e-6, eps_rel=1e-5):\n    \"\"\"\n    Solves the analysis LASSO problem using ADMM.\n    min_x 0.5 * ||Ax - y||^2 + lambda * ||Omega * x||_1\n    \"\"\"\n    p = A.shape[1]\n    q = Omega.shape[0]\n\n    # Pre-compute matrices for x-update\n    AtA = A.T @ A\n    Aty = A.T @ y\n    OmegatOmega = Omega.T @ Omega\n    P_inv = np.linalg.inv(AtA + rho * OmegatOmega)\n\n    x_k = np.zeros((p, 1))\n    v_k = np.zeros((q, 1))\n    u_k = np.zeros((q, 1))\n\n    for k in range(max_iter):\n        # x-update\n        rhs_x = Aty + rho * Omega.T @ (v_k - u_k)\n        x_k_plus_1 = P_inv @ rhs_x\n\n        # v-update\n        v_k_old = v_k\n        v_k_plus_1 = soft_threshold(Omega @ x_k_plus_1 + u_k, lambda_val / rho)\n\n        # u-update\n        u_k_plus_1 = u_k + Omega @ x_k_plus_1 - v_k_plus_1\n\n        # Stopping criteria\n        # Primal residual\n        r_k_plus_1 = Omega @ x_k_plus_1 - v_k_plus_1\n        eps_pri = np.sqrt(q) * eps_abs + eps_rel * np.maximum(np.linalg.norm(Omega @ x_k_plus_1), np.linalg.norm(v_k_plus_1))\n        \n        # Dual residual\n        s_k_plus_1 = rho * Omega.T @ (v_k_plus_1 - v_k_old)\n        eps_dual = np.sqrt(p) * eps_abs + eps_rel * np.linalg.norm(rho * Omega.T @ u_k_plus_1)\n\n        if np.linalg.norm(r_k_plus_1)  eps_pri and np.linalg.norm(s_k_plus_1)  eps_dual:\n            x_k = x_k_plus_1\n            break\n            \n        x_k = x_k_plus_1\n        v_k = v_k_plus_1\n        u_k = u_k_plus_1\n    \n    return x_k\n\ndef solve():\n    # Set up the problem parameters\n    np.random.seed(7)\n    G = np.random.randn(2, 3)\n    A = G / np.sqrt(2)\n    \n    Omega = np.array([[-1., 1., 0.], [0., -1., 1.]])\n\n    test_cases = [\n        {'x_star': np.array([1.0, 1.0, 0.0]), 'lambda': 0.2},\n        {'x_star': np.array([1.5, 0.0, 0.0]), 'lambda': 0.2},\n        {'x_star': np.array([0.5, 0.5, 0.5]), 'lambda': 0.5},\n        {'x_star': np.array([0.8, -0.2, 0.8]), 'lambda': 5.0}\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        x_star = case['x_star'].reshape(3, 1)\n        lambda_val = case['lambda']\n        \n        # Generate measurements\n        y = A @ x_star\n        \n        # Solve for synthesis LASSO\n        x_synth = fista_solver(A, y, lambda_val)\n        \n        # Solve for analysis LASSO\n        x_anal = admm_solver(A, y, Omega, lambda_val)\n\n        # Calculate metrics\n        dist_sol = np.linalg.norm(x_synth - x_anal)\n        err_synth = np.linalg.norm(x_synth - x_star)\n        err_anal = np.linalg.norm(x_anal - x_star)\n        \n        all_results.append([dist_sol, err_synth, err_anal])\n\n    # Format the final output string\n    results_str_list = []\n    for res_tuple in all_results:\n        formatted_tuple = [f\"{x:.6f}\" for x in res_tuple]\n        results_str_list.append(f\"[{','.join(formatted_tuple)}]\")\n    final_output = f\"[{','.join(results_str_list)}]\"\n        \n    print(final_output)\n\nsolve()\n```", "id": "3445015"}, {"introduction": "在上一个练习的基础上，我们现在深入探究一个核心问题：在何种情况下分析模型会优于综合模型？本练习将指导你构建一个计算实验，通过精心设计的信号和算子来量化比较两种模型的样本复杂度 ([@problem_id:3445022])。这个实践旨在通过经验证据，验证一个关键的理论观点：当信号具有特定结构（如分段常数）且综合字典设计不佳（如高度相干）时，分析模型可以用更少的测量值实现精确恢复。", "problem": "考虑一个无噪声的压缩感知设置，其中未知信号 $\\mathbf{x} \\in \\mathbb{R}^{n}$ 通过线性算子 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 进行测量，产生 $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$。使用两种凸公式进行重构：\n\n1. 合成公式：给定一个字典 $\\mathbf{D} \\in \\mathbb{R}^{n \\times p}$，通过以下问题重构 $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{p}$\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^{p}} \\|\\boldsymbol{\\alpha}\\|_{1} \\quad \\text{服从约束} \\quad \\mathbf{A}\\mathbf{D}\\boldsymbol{\\alpha} = \\mathbf{y}, \n$$\n并形成估计 $\\hat{\\mathbf{x}}_{\\text{syn}} = \\mathbf{D}\\hat{\\boldsymbol{\\alpha}}$。\n\n2. 分析公式：给定一个分析算子 $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{r \\times n}$，通过以下问题重构 $\\mathbf{x}$\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\|\\boldsymbol{\\Omega}\\mathbf{x}\\|_{1} \\quad \\text{服从约束} \\quad \\mathbf{A}\\mathbf{x} = \\mathbf{y}.\n$$\n\n两个向量集合之间的互相关性定义为从各自集合中抽取的任意两个单位范数向量之间内积的绝对值的最大值。对于一个字典 $\\mathbf{D}$，其列之间的互相关性为\n$$\n\\mu(\\mathbf{D}) = \\max_{i \\neq j} \\frac{|\\mathbf{d}_i^{\\top} \\mathbf{d}_j|}{\\|\\mathbf{d}_i\\|_2 \\|\\mathbf{d}_j\\|_2},\n$$\n其中 $\\mathbf{d}_i$ 是 $\\mathbf{D}$ 的列。对于一个分析算子 $\\boldsymbol{\\Omega}$ 和测量算子 $\\mathbf{A}$，一个代表性的互相关性是在它们单位范数的行之间计算，\n$$\n\\mu(\\mathbf{A}, \\boldsymbol{\\Omega}) = \\max_{i,j} \\frac{|\\mathbf{a}_i^{\\top} \\boldsymbol{\\omega}_j|}{\\|\\mathbf{a}_i\\|_2 \\|\\boldsymbol{\\omega}_j\\|_2},\n$$\n其中 $\\mathbf{a}_i$ 是 $\\mathbf{A}$ 的行，$\\boldsymbol{\\omega}_j$ 是 $\\boldsymbol{\\Omega}$ 的行。\n\n从几何压缩感知的角度来看，在无噪声设置下足以实现精确重构的测量数量，取决于真实值处目标函数的下降（切）锥的高斯宽度，而高斯宽度又依赖于模型的结构和相干性。在合成公式中，高度相干的字典会产生更宽的下降锥；而在分析公式中，与测量算子具有低互相关性的结构化协支撑会产生更窄的下降锥。\n\n利用这一原理，构建一个自包含的计算实验，具体规格如下：\n\n- 使用 $n = 32$，并通过从一个正交离散余弦变换基（$\\mathbb{R}^{32 \\times 32}$的列）开始，并附加近似重复的列来创建一个高度相干的字典 $\\mathbf{D} \\in \\mathbb{R}^{32 \\times 64}$，以使 $\\mu(\\mathbf{D})$ 超过 $0.95$。将所有列归一化为单位范数。\n- 使用一个分析算子 $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{31 \\times 32}$ 作为一阶离散差分：对于 $i=1,\\dots,31$，每行在位置 $i$ 处为 $+1$，在位置 $i+1$ 处为 $-1$。这为分段常数信号产生了结构化的协支撑。\n- 使用测量算子 $\\mathbf{A} \\in \\mathbb{R}^{m \\times 32}$，其行独立地从标准正态分布中采样，然后归一化为单位范数；对于边界情况 $m=n$，使用 $\\mathbf{A} = \\mathbf{I}_{32}$ 以确保存在精确解。这些随机测量会产生低的 $\\mu(\\mathbf{A}, \\boldsymbol{\\Omega})$。\n- 为测试套件定义三个真实信号 $\\mathbf{x}$：\n  1. 一个具有四个等长段的分段常数信号：$\\mathbf{x} = [0,\\dots,0,\\, 1,\\dots,1,\\, -0.5,\\dots,-0.5,\\, 0.5,\\dots,0.5]$。\n  2. 一个具有单个跳变的边界情况：前 $16$ 个条目等于 $0$，其余 $16$ 个条目等于 $1$。\n  3. 一个边缘情况，其中 $\\mathbf{x}$ 具有独立同分布的标准正态条目，然后缩放到单位范数。\n- 对于每个 $\\mathbf{x}$，当 $m$ 遍历集合 $\\{8,12,16,20,24,28,32\\}$ 时，计算 $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$ 并通过将 $\\ell_{1}$ 目标函数简化为带辅助变量的线性规划来解决合成和分析问题：\n  - 合成线性规划：\n    - 变量：$\\boldsymbol{\\alpha} \\in \\mathbb{R}^{p}$ 和 $\\mathbf{s} \\in \\mathbb{R}^{p}$。\n    - 目标：最小化 $\\sum_{i=1}^{p} s_i$。\n    - 约束：$\\mathbf{A}\\mathbf{D}\\boldsymbol{\\alpha} = \\mathbf{y}$，$-\\mathbf{s} \\le \\boldsymbol{\\alpha} \\le \\mathbf{s}$，以及 $\\mathbf{s} \\ge \\mathbf{0}$。\n  - 分析线性规划：\n    - 变量：$\\mathbf{x} \\in \\mathbb{R}^{n}$ 和 $\\mathbf{t} \\in \\mathbb{R}^{r}$。\n    - 目标：最小化 $\\sum_{i=1}^{r} t_i$。\n    - 约束：$\\mathbf{A}\\mathbf{x} = \\mathbf{y}$，$-\\mathbf{t} \\le \\boldsymbol{\\Omega}\\mathbf{x} \\le \\mathbf{t}$，以及 $\\mathbf{t} \\ge \\mathbf{0}$。\n- 如果相对误差满足 $\\|\\hat{\\mathbf{x}} - \\mathbf{x}\\|_{2} / \\|\\mathbf{x}\\|_{2} \\le 10^{-3}$ 并且线性规划报告成功，则声明为精确重构。\n- 对于测试套件中的每个信号，在给定的 $m$ 值集合上，确定分析重构所需的最小测量数 $m_{\\text{an}}$ 和合成重构所需的最小测量数 $m_{\\text{syn}}$。计算并报告每种情况下的整数差 $m_{\\text{syn}} - m_{\\text{an}}$，以经验性地证明当字典高度相干且协支撑结构化并与测量具有低互相关性时，分析模型能用更少的测量进行重构。\n\n您的程序应生成单行输出，其中包含三个整数差值，格式为逗号分隔的列表，并用方括号括起来（例如，\"[d1,d2,d3]\"）。", "solution": "该问题提出了一个计算实验，以比较压缩感知中两种先验信号模型的样本复杂度：合成模型和分析模型。假设是对于具有特定结构（分段常数）的信号，分析公式将比合成公式用更少的测量实现精确重构，特别是当合成字典高度相干时。这个实验是压缩感知理论基础中一个已知的对偶性和关键概念的演示。该问题是适定的，具有科学依据，并为可复现的计算研究提供了所有必要的规格。\n\n问题的核心在于两种形式化信号稀疏性方法的区别。\n\n$1$. **合成模型**：该模型假定信号 $\\mathbf{x} \\in \\mathbb{R}^{n}$ 可以合成为来自字典 $\\mathbf{D} \\in \\mathbb{R}^{n \\times p}$ 的原子的稀疏线性组合。即 $\\mathbf{x} = \\mathbf{D}\\boldsymbol{\\alpha}$，其中系数向量 $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{p}$ 几乎没有非零项（是稀疏的）。给定测量值 $\\mathbf{y} = \\mathbf{A}\\mathbf{x} = \\mathbf{A}\\mathbf{D}\\boldsymbol{\\alpha}$，通过解决基追踪问题来重构稀疏系数 $\\boldsymbol{\\alpha}$，这是对难以处理的 $\\ell_0$ 伪范数最小化问题的凸松弛：\n$$\n\\hat{\\boldsymbol{\\alpha}} = \\arg\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^{p}} \\|\\boldsymbol{\\alpha}\\|_{1} \\quad \\text{服从约束} \\quad \\mathbf{A}\\mathbf{D}\\boldsymbol{\\alpha} = \\mathbf{y}.\n$$\n然后将信号估计形成为 $\\hat{\\mathbf{x}}_{\\text{syn}} = \\mathbf{D}\\hat{\\boldsymbol{\\alpha}}$。这种重构的成功在很大程度上取决于复合传感矩阵 $\\mathbf{A}\\mathbf{D}$ 的性质。一个关键参数是字典的互相关性 $\\mu(\\mathbf{D})$。高的 $\\mu(\\mathbf{D})$ 意味着字典中的原子几乎是线性相关的，这会降低重构性能，并且通常需要更多的测量次数 $m$。\n\n$2$. **分析模型**：该模型提供了一个更通用的视角。它假设信号 $\\mathbf{x}$ 本身不一定是稀疏的，但在被分析算子 $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{r \\times n}$ 变换后变得稀疏。也就是说，$\\boldsymbol{\\Omega}\\mathbf{x}$ 是稀疏的。一个经典的例子是分段常数信号，它在标准基中不稀疏，但其梯度（或有限差分）是稀疏的。给定测量值 $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$，通过求解以下问题来重构信号：\n$$\n\\hat{\\mathbf{x}}_{\\text{an}} = \\arg\\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\|\\boldsymbol{\\Omega}\\mathbf{x}\\|_{1} \\quad \\text{服从约束} \\quad \\mathbf{A}\\mathbf{x} = \\mathbf{y}.\n$$\n这种方法的成功取决于 $\\mathbf{A}$ 和 $\\boldsymbol{\\Omega}$ 的性质，特别是 $\\mathbf{A}$ 的零空间与具有非稀疏变换的信号集 $\\{\\mathbf{z} : \\|\\boldsymbol{\\Omega}\\mathbf{z}\\|_1 \\text{ 很大}\\}$ 之间的几何关系。测量和分析算子之间的低互相关性 $\\mu(\\mathbf{A}, \\boldsymbol{\\Omega})$ 有利于重构。\n\n该实验旨在创造一种情景，其中合成模型处于劣势，而分析模型处于优势。\n- 构建一个具有 $\\mu(\\mathbf{D})  0.95$ 的高度相干的字典。这使得合成问题难以区分相似的原子，因此需要更多信息（即更多的测量 $m$）。\n- 测试信号被选为分段常数信号，对于这类信号，一阶有限差分算子 $\\boldsymbol{\\Omega}$ 是一种有效的稀疏化变换。\n- 测量矩阵 $\\mathbf{A}$ 由随机高斯行构成，这很大概率会与 $\\boldsymbol{\\Omega}$ 的结构化行具有低相干性。\n\n这种设置预测，对于分段常数信号，分析模型将用比合成模型更少的测量次数（$m_{\\text{an}}$）成功重构信号（$m_{\\text{syn}}$），从而导致正的差异 $m_{\\text{syn}} - m_{\\text{an}}$。一个在任一模型中都不稀疏的随机噪声信号被用作对照案例。\n\n实现过程如下：\n\n首先，我们按规定构建算子和信号测试套件。\n- 维度为 $n=32$。\n- 分析算子 $\\boldsymbol{\\Omega} \\in \\mathbb{R}^{31 \\times 32}$ 是一阶有限差分矩阵。\n- 合成字典 $\\mathbf{D} \\in \\mathbb{R}^{32 \\times 64}$ 由一个 $32 \\times 32$ 的离散余弦变换（DCT）矩阵（称之为 $\\mathbf{C}$）构建。$\\mathbf{D}$ 的前 $32$ 列是 $\\mathbf{C}$ 的列。接下来的 $32$ 列被创建为前 $32$ 列的近似副本，以确保高相干性。具体来说，对于 $\\mathbf{C}$ 的每一列 $\\mathbf{c}_i$，通过 $\\mathbf{d}_{32+i} = \\cos(\\theta)\\mathbf{c}_i + \\sin(\\theta)\\mathbf{c}_{i+1}$（索引循环）形成一个新列，其中 $\\theta$ 是一个小角度。选择 $\\theta = 0.3$ 弧度会得到内积 $\\cos(0.3) \\approx 0.955  0.95$，满足条件。由于 $\\mathbf{C}$ 的列是正交的，新列已经是单位范数。\n- 创建三个真实信号 $\\mathbf{x}$：两个分段常数信号和一个单位范数高斯噪声信号。\n\n其次，对于每个信号，我们遍历测量次数 $m \\in \\{8, 12, 16, 20, 24, 28, 32\\}$。\n- 对于每个 $m$，生成一个测量矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times 32}$（或在 $m=32$ 时设为 $\\mathbf{I}_{32}$），并计算测量值 $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$。\n- 将合成和分析的 $\\ell_1$-最小化问题转换为带有辅助变量的线性规划（LPs）。合成LP涉及 $2p = 128$ 个变量，而分析LP涉及 $n+r = 32+31 = 63$ 个变量。\n- 使用标准的数值求解器解决这些LPs。\n- 在LP成功求解后，获得重构信号 $\\hat{\\mathbf{x}}$，并计算相对重构误差 $\\|\\hat{\\mathbf{x}} - \\mathbf{x}\\|_{2} / \\|\\mathbf{x}\\|_{2}$。\n- 如果误差低于 $10^{-3}$ 的阈值，则认为该 $m$ 值实现了精确重构。发生这种情况的最小 $m$ 被记录为 $m_{\\text{syn}}$ 或 $m_{\\text{an}}$。\n\n最后，为三个测试信号中的每一个计算差值 $m_{\\text{syn}} - m_{\\text{an}}$。所提供的Python代码执行了这个完整的计算实验。", "answer": "```python\nimport numpy as np\nfrom scipy.fftpack import dct\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Solves the compressed sensing problem as described.\n    This function sets up the computational experiment to compare synthesis\n    and analysis recovery models, finds the minimal number of measurements\n    for each, and computes their difference.\n    \"\"\"\n    \n    # --------------------------------------------------------------------------\n    # 1. Define constants and operators\n    # --------------------------------------------------------------------------\n    N = 32\n    P = 64\n    R = N - 1\n    M_VALUES = [8, 12, 16, 20, 24, 28, 32]\n    RECOVERY_TOLERANCE = 1e-3\n    \n    # Use a fixed seed for reproducibility of random matrices\n    RNG = np.random.default_rng(seed=42)\n\n    # Analysis operator Omega (first-order finite difference)\n    Omega = np.zeros((R, N))\n    for i in range(R):\n        Omega[i, i] = 1\n        Omega[i, i + 1] = -1\n\n    # Synthesis dictionary D (highly coherent DCT-based dictionary)\n    # Start with an orthonormal DCT-II basis\n    C = dct(np.eye(N), type=2, norm='ortho')\n    \n    # Append near-duplicate columns to increase coherence\n    C_prime = np.zeros_like(C)\n    theta = 0.3  # cos(0.3) is approx 0.955 > 0.95\n    for i in range(N):\n        j = (i + 1) % N\n        C_prime[:, i] = np.cos(theta) * C[:, i] + np.sin(theta) * C[:, j]\n    \n    # All columns are already unit-norm due to orthonormality of C\n    D = np.hstack([C, C_prime])\n\n    # --------------------------------------------------------------------------\n    # 2. Define ground-truth signals\n    # --------------------------------------------------------------------------\n    x1 = np.concatenate([\n        np.zeros(N // 4),\n        np.ones(N // 4),\n        -0.5 * np.ones(N // 4),\n        0.5 * np.ones(N // 4)\n    ])\n    \n    x2 = np.concatenate([\n        np.zeros(N // 2),\n        np.ones(N // 2)\n    ])\n    \n    x3_raw = RNG.standard_normal(N)\n    x3 = x3_raw / np.linalg.norm(x3_raw)\n    \n    test_signals = [x1, x2, x3]\n\n    # --------------------------------------------------------------------------\n    # 3. Main loop for the experiment\n    # --------------------------------------------------------------------------\n    results = []\n    \n    for sig_idx, x_true in enumerate(test_signals):\n        x_norm = np.linalg.norm(x_true)\n        if x_norm == 0: x_norm = 1.0\n\n        m_syn_min = float('inf')\n        m_an_min = float('inf')\n\n        # Find minimal m for synthesis recovery\n        for m in M_VALUES:\n            # Use a consistent seed for A for this (signal, m) pair\n            RNG_m_syn = np.random.default_rng(seed=42 + sig_idx*100 + m)\n            if m  N:\n                A_raw = RNG_m_syn.standard_normal((m, N))\n                A = A_raw / np.linalg.norm(A_raw, axis=1, keepdims=True)\n            else: # m == N\n                A = np.eye(N)\n                \n            y = A @ x_true\n            \n            # Synthesis LP formulation\n            c_syn = np.concatenate([np.zeros(P), np.ones(P)])\n            AD = A @ D\n            A_eq_syn = np.hstack([AD, np.zeros((m, P))])\n            b_eq_syn = y\n            \n            Id_P = np.eye(P)\n            A_ub_syn = np.vstack([\n                np.hstack([Id_P, -Id_P]),\n                np.hstack([-Id_P, -Id_P])\n            ])\n            b_ub_syn = np.zeros(2 * P)\n            \n            bounds_syn = [(None, None)] * P + [(0, None)] * P\n\n            res_syn = linprog(c_syn, A_ub=A_ub_syn, b_ub=b_ub_syn,\n                              A_eq=A_eq_syn, b_eq=b_eq_syn,\n                              bounds=bounds_syn, method='highs')\n            \n            if res_syn.success:\n                alpha_hat = res_syn.x[:P]\n                x_hat_syn = D @ alpha_hat\n                err = np.linalg.norm(x_hat_syn - x_true) / x_norm\n                if err  RECOVERY_TOLERANCE:\n                    m_syn_min = m\n                    break \n\n        # Find minimal m for analysis recovery\n        for m in M_VALUES:\n            # Use a consistent seed for A for this (signal, m) pair\n            RNG_m_an = np.random.default_rng(seed=42 + sig_idx*100 + m)\n            if m  N:\n                A_raw = RNG_m_an.standard_normal((m,N))\n                A = A_raw / np.linalg.norm(A_raw, axis=1, keepdims=True)\n            else: # m == N\n                A = np.eye(N)\n            \n            y = A @ x_true\n            \n            # Analysis LP formulation\n            c_an = np.concatenate([np.zeros(N), np.ones(R)])\n            A_eq_an = np.hstack([A, np.zeros((m, R))])\n            b_eq_an = y\n            \n            Id_R = np.eye(R)\n            A_ub_an = np.vstack([\n                np.hstack([Omega, -Id_R]),\n                np.hstack([-Omega, -Id_R])\n            ])\n            b_ub_an = np.zeros(2 * R)\n\n            bounds_an = [(None, None)] * N + [(0, None)] * R\n\n            res_an = linprog(c_an, A_ub=A_ub_an, b_ub=b_ub_an,\n                             A_eq=A_eq_an, b_eq=b_eq_an,\n                             bounds=bounds_an, method='highs')\n            \n            if res_an.success:\n                x_hat_an = res_an.x[:N]\n                err = np.linalg.norm(x_hat_an - x_true) / x_norm\n                if err  RECOVERY_TOLERANCE:\n                    m_an_min = m\n                    break\n                    \n        results.append(m_syn_min - m_an_min)\n\n    # --------------------------------------------------------------------------\n    # 4. Final output\n    # --------------------------------------------------------------------------\n    # Forcing to int, as inf-inf=nan\n    final_results_int = [int(r) if not np.isnan(r) else 0 for r in results]\n    print(f\"[{','.join(map(str, final_results_int))}]\")\n\nsolve()\n```", "id": "3445022"}]}