{"hands_on_practices": [{"introduction": "理论的美妙之处在于其通用性。标准的交替方向乘子法（ADMM）框架不仅能处理LASSO问题，还能通过引入指示函数（indicator function）的方式，优雅地将硬约束（hard constraints）纳入其中。本练习将指导您处理一个常见的实际约束——非负性，这在图像处理、物理测量等领域至关重要。您将学习如何将非负约束 $x \\ge 0$ 转化为一个邻近算子（proximal operator）步骤，最终简化为对一个向量进行简单的投影操作，从而深刻理解ADMM处理约束问题的核心思想 [@problem_id:3429989]。", "problem": "考虑在压缩感知设置下带非负约束的最小绝对收缩和选择算子 (LASSO) 问题：最小化二次数据保真项与稀疏性诱导正则化项之和，并满足分量非负的约束条件。设 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，且 $\\lambda > 0$。将该问题表述为在一致性约束下最小化两个函数之和：\n$$\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} f(x) + g(z) \\quad \\text{subject to } x = z,$$\n其中 $f(x)$ 是非负象限的指示函数，$g(z)$ 是二次数据保真项与 $\\ell_{1}$-范数正则化项之和。使用带有缩放对偶变量的交替方向乘子法 (ADMM) 分裂，仅根据当前值 $z^{k}$ 和 $u^{k}$ 以及给定的惩罚参数 $\\rho > 0$，推导在第 $k+1$ 次迭代中对 $x$ 的更新式。您的推导必须从与一致性形式相关的增广拉格朗日量以及指示函数和投影算子的基本性质出发，不得假定任何预先推导出的邻近算子公式。\n\n在推导出 $x$-更新的解析表达式后，使用以下特定输入对其进行求值：\n$$z^{k} = \\begin{pmatrix} 1.2 \\\\ -0.7 \\\\ 0.0 \\\\ 3.1 \\\\ -2.4 \\end{pmatrix}, \\quad u^{k} = \\begin{pmatrix} -0.5 \\\\ 0.9 \\\\ -0.3 \\\\ 1.2 \\\\ -1.1 \\end{pmatrix}, \\quad \\rho = 1.$$\n请将 $x^{k+1}$ 的最终数值以单行矩阵的形式给出。无需四舍五入，且不涉及物理单位。", "solution": "该问题要求推导针对非负 LASSO 问题特定形式的交替方向乘子法 (ADMM) 算法中的 $x$-更新步骤，然后进行数值计算。\n\n首先，我们验证问题陈述的有效性。\n该问题是在约束条件下最小化一个函数，并为 ADMM 方法构建了形式。\n设原始问题为非负 LASSO：\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2}\\|Ax-y\\|_2^2 + \\lambda \\|x\\|_1 \\quad \\text{subject to } x \\ge 0. $$\n给定的一致性形式为：\n$$ \\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{n}} f(x) + g(z) \\quad \\text{subject to } x = z, $$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，且 $\\lambda > 0$ 为给定值。函数定义如下：\n$f(x) = I_{\\mathbb{R}_+^n}(x)$，即非负象限的指示函数。集合 $C$ 的指示函数 $I_C(x)$ 定义为：如果 $x \\in C$，则 $I_C(x) = 0$；如果 $x \\notin C$，则 $I_C(x) = \\infty$。因此，$f(x)$ 强制执行非负约束 $x \\ge 0$。\n$g(z)$ 是二次数据保真项与 $\\ell_1$-范数正则化项之和，对应于不含非负约束的原始 LASSO 目标函数：\n$$ g(z) = \\frac{1}{2}\\|Az-y\\|_2^2 + \\lambda \\|z\\|_1. $$\n该问题具有科学依据、是适定的且客观的。它是 ADMM 在计算数学和信号处理中的一个标准应用。推导和计算所需的所有信息均已提供。该问题是有效的。\n\n现在我们进行推导。ADMM 算法基于增广拉格朗日量。对于约束问题 $\\min f(x) + g(z)$，约束条件为 $x-z=0$，带有缩放对偶变量 $u$ 的增广拉格朗日量由下式给出：\n$$ L_\\rho(x, z, u) = f(x) + g(z) + \\frac{\\rho}{2}\\|x-z+u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2. $$\n此形式是通过对标准增广拉格朗日量 $L_\\rho(x, z, \\nu) = f(x) + g(z) + \\nu^T(x-z) + \\frac{\\rho}{2}\\|x-z\\|_2^2$ 设置对偶变量 $\\nu = \\rho u$ 并进行配方得到的。项 $-\\frac{\\rho}{2}\\|u\\|_2^2$ 相对于 $x$ 和 $z$ 是一个常数，在最小化步骤中可以忽略。\n\nADMM 算法在每次迭代 $k$ 中包含三个更新步骤：\n$1$. $x^{k+1} = \\arg\\min_x L_\\rho(x, z^k, u^k)$\n$2$. $z^{k+1} = \\arg\\min_z L_\\rho(x^{k+1}, z, u^k)$\n$3$. $u^{k+1} = u^k + x^{k+1} - z^{k+1}$\n\n我们的任务是推导在第 $k+1$ 次迭代中对 $x$ 的更新。这是 ADMM 序列的第一步：\n$$ x^{k+1} = \\arg\\min_{x} L_\\rho(x, z^k, u^k). $$\n代入增广拉格朗日量的表达式：\n$$ x^{k+1} = \\arg\\min_{x} \\left( f(x) + g(z^k) + \\frac{\\rho}{2}\\|x-z^k+u^k\\|_2^2 \\right). $$\n在这个最小化问题中，变量是 $x$。项 $g(z^k)$ 相对于 $x$ 是常数，可以从目标函数中去掉而不改变最小化子：\n$$ x^{k+1} = \\arg\\min_{x} \\left( f(x) + \\frac{\\rho}{2}\\|x-z^k+u^k\\|_2^2 \\right). $$\n我们定义一个辅助向量 $v^k = z^k - u^k$。表达式简化为：\n$$ x^{k+1} = \\arg\\min_{x} \\left( f(x) + \\frac{\\rho}{2}\\|x-v^k\\|_2^2 \\right). $$\n函数 $f(x)$ 是非负象限的指示函数 $I_{\\mathbb{R}_+^n}(x)$。因此，该最小化问题等价于：\n$$ x^{k+1} = \\arg\\min_{x \\ge 0} \\frac{\\rho}{2}\\|x-v^k\\|_2^2. $$\n正常数因子 $\\frac{\\rho}{2}$ 不影响最小值点的位置，因此我们可以将问题进一步简化为：\n$$ x^{k+1} = \\arg\\min_{x \\ge 0} \\|x-v^k\\|_2^2. $$\n这是寻找向量 $v^k$ 在凸集 $\\mathbb{R}_+^n$ 上的欧几里得投影的问题。欧几里得范数的平方在向量的各个分量上是可分的：\n$$ \\|x-v^k\\|_2^2 = \\sum_{i=1}^{n} (x_i - v_i^k)^2. $$\n因此，可以对每个分量 $x_i$ 独立进行最小化：\n$$ x_i^{k+1} = \\arg\\min_{x_i \\ge 0} (x_i - v_i^k)^2 \\quad \\text{for } i=1, \\dots, n. $$\n对于每个分量 $i$，我们考虑单变量二次函数 $h(x_i) = (x_i - v_i^k)^2$。$h(x_i)$ 的无约束最小化子位于 $x_i = v_i^k$。\n我们必须满足约束 $x_i \\ge 0$。\n情况1：如果 $v_i^k \\ge 0$，无约束最小化子 $x_i=v_i^k$ 位于可行域 $[0, \\infty)$ 内。因此，解为 $x_i^{k+1} = v_i^k$。\n情况2：如果 $v_i^k  0$，无约束最小化子位于可行域之外。函数 $h(x_i)$ 是一个开口向上、顶点在 $v_i^k  0$ 的抛物线。在区间 $[0, \\infty)$ 上，函数 $h(x_i)$ 是严格递增的。因此，在此区间上的最小值在边界点 $x_i = 0$ 处取得。解为 $x_i^{k+1} = 0$。\n综合两种情况，每个分量的解由下式给出：\n$$ x_i^{k+1} = \\max(0, v_i^k). $$\n用向量形式表示为 $x^{k+1} = (v^k)_+$，其中 $(\\cdot)_+$ 算子表示与 $0$ 逐元素取最大值。将 $v^k = z^k - u^k$ 代回，我们得到 $x$-更新的最终解析表达式：\n$$ x^{k+1} = (z^k - u^k)_+. $$\n\n现在，我们使用给定的输入计算此表达式的值：\n$$ z^{k} = \\begin{pmatrix} 1.2 \\\\ -0.7 \\\\ 0.0 \\\\ 3.1 \\\\ -2.4 \\end{pmatrix}, \\quad u^{k} = \\begin{pmatrix} -0.5 \\\\ 0.9 \\\\ -0.3 \\\\ 1.2 \\\\ -1.1 \\end{pmatrix}. $$\n惩罚参数 $\\rho=1$ 在此特定更新步骤中不需要，因为它已在最小化过程中被约去。\n首先，我们计算向量 $v^k = z^k - u^k$：\n$$ v^k = z^k - u^k = \\begin{pmatrix} 1.2 \\\\ -0.7 \\\\ 0.0 \\\\ 3.1 \\\\ -2.4 \\end{pmatrix} - \\begin{pmatrix} -0.5 \\\\ 0.9 \\\\ -0.3 \\\\ 1.2 \\\\ -1.1 \\end{pmatrix} = \\begin{pmatrix} 1.2 - (-0.5) \\\\ -0.7 - 0.9 \\\\ 0.0 - (-0.3) \\\\ 3.1 - 1.2 \\\\ -2.4 - (-1.1) \\end{pmatrix} = \\begin{pmatrix} 1.7 \\\\ -1.6 \\\\ 0.3 \\\\ 1.9 \\\\ -1.3 \\end{pmatrix}. $$\n接下来，我们应用逐元素到非负象限的投影，$x^{k+1} = (v^k)_+$：\n$$ x^{k+1} = \\left( \\begin{pmatrix} 1.7 \\\\ -1.6 \\\\ 0.3 \\\\ 1.9 \\\\ -1.3 \\end{pmatrix} \\right)_+ = \\begin{pmatrix} \\max(0, 1.7) \\\\ \\max(0, -1.6) \\\\ \\max(0, 0.3) \\\\ \\max(0, 1.9) \\\\ \\max(0, -1.3) \\end{pmatrix} = \\begin{pmatrix} 1.7 \\\\ 0.0 \\\\ 0.3 \\\\ 1.9 \\\\ 0.0 \\end{pmatrix}. $$\n问题要求以单行矩阵的形式给出答案。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.7  0.0  0.3  1.9  0.0\n\\end{pmatrix}\n}\n$$", "id": "3429989"}, {"introduction": "一个算法的理论正确性是第一步，而使其在实践中稳健高效地运行则是另一项关键挑战。ADMM的收敛性能对增广拉格朗日惩罚参数 $\\rho$ 的选择可能很敏感，而数据本身的尺度（scaling）会直接影响 $\\rho$ 的“最优”选择范围。本练习将引导您从理论上分析数据缩放如何影响LASSO问题本身以及ADMM迭代的动态过程。通过这个分析，您将揭示数据标准化和参数选择之间的深刻联系，理解为何良好的预处理是保证算法数值稳定性和快速收敛的基石 [@problem_id:3429991]。", "problem": "考虑在压缩感知设置中的最小绝对值收敛和选择算子 (LASSO) 问题：对向量 $x \\in \\mathbb{R}^n$ 最小化目标函数 $J(x) = \\tfrac{1}{2}\\,\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1$，其中数据矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，测量值为 $b \\in \\mathbb{R}^m$，正则化参数为 $\\lambda  0$。假设您使用标准的变量分裂公式，通过辅助变量 $z \\in \\mathbb{R}^n$ 和惩罚参数 $\\rho  0$ 来应用交替方向乘子法 (ADMM, Alternating Direction Method of Multipliers)。$z$ 的更新涉及一个软阈值步骤，其阈值大小取决于一个结合了 $\\lambda$ 和 $\\rho$ 的比率。现在考虑对数据进行均匀缩放 $(A,b) \\mapsto (s A, s b)$，缩放因子为 $s  0$，而不改变变量 $x$。您希望保持未缩放问题的优化器不变，并稳定 ADMM 的 $z$ 更新中所使用的软阈值大小，以抵抗这种缩放的影响。\n\n以下哪个陈述最好地描述了缩放对 $z$ 更新中使用的有效软阈值大小的影响，并推荐一种归一化策略以确保在不同 $s$ 选择下的稳定性？\n\nA. 在 $(A,b) \\mapsto (s A, s b)$ 的缩放变换下，为了保持相同的优化器，必须将 $\\lambda$ 替换为 $\\lambda' = s^2 \\lambda$。为了保持 $z$ 更新中使用的软阈值大小不变，并使整个 ADMM 迭代对此缩放不变，同时设置 $\\rho' = s^2 \\rho$，这会保持关键比率不变。一种实用的归一化方法是：对 $A$ 进行列归一化，使每列的 $\\ell_2$-范数为单位1，并选择与 $\\|A\\|_2^2$ 成比例的 $\\rho$，这能在测量单位发生变化时，保持软阈值大小在数值上稳定。\n\nB. $z$ 更新的阈值不依赖于 $A$ 或 $b$，因此 $(A,b) \\mapsto (s A, s b)$ 对软阈值的大小没有影响；因此不需要调整 $\\lambda$ 或 $\\rho$，并且归一化 $A$ 或 $b$ 是不必要的，还可能会使解产生偏差。\n\nC. 为了在 $(A,b) \\mapsto (s A, s b)$ 变换后保持相同的软阈值大小，应设置 $\\lambda' = s \\lambda$ 并保持 $\\rho$ 不变，并将 $A$ 的行标准化，使其具有单位 $\\ell_1$-范数，从而稳定阈值。\n\nD. 选择与 $\\|A\\|_2^2$ 成反比的 $\\rho$，同时保持 $\\lambda$ 固定，可以保证在 $(A,b) \\mapsto (s A, s b)$ 缩放后，软阈值大小按 $s^2$ 的比例减小，这是可取的，因为它加强了数据保真度并加速了收敛；不需要进一步的归一化。", "solution": "用户需要对应用于 LASSO 问题的交替方向乘子法 (ADMM) 进行分析，特别是关于数据缩放对算法参数和行为的影响。\n\n### 步骤 1：提取已知信息\n\n-   **目标函数 (LASSO)：** $J(x) = \\tfrac{1}{2}\\,\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1$\n-   **变量和数据：**$x \\in \\mathbb{R}^n$ (优化变量)，$A \\in \\mathbb{R}^{m \\times n}$ (数据矩阵)，$b \\in \\mathbb{R}^m$ (测量值)。\n-   **参数：**$\\lambda  0$ (正则化参数)，$\\rho  0$ (ADMM 惩罚参数)。\n-   **算法：** 使用辅助变量 $z \\in \\mathbb{R}^n$ 的标准变量分裂公式的 ADMM。\n-   **变换：** 对数据进行均匀缩放 $(A,b) \\mapsto (s A, s b)$，缩放因子为标量 $s  0$。\n-   **目标：**\n    1.  保持未缩放问题的优化器不变。\n    2.  确保 ADMM 的 $z$ 更新中的软阈值大小对这种缩放具有稳定性。\n-   **问题：** 描述缩放的影响并推荐一种归一化策略。\n\n### 步骤 2：使用提取的已知信息进行验证\n\n问题陈述在科学上和数学上都是合理的。LASSO 目标是统计学、信号处理和机器学习中一个标准且经过深入研究的问题。ADMM 是解决该问题的经典算法，所述的变量分裂公式是最常用的方法。关于算法参数应如何适应数据缩放的问题是数值优化中一个基本且实际的问题。该问题是适定的、客观的，并包含足够的信息以进行严格分析。未检测到任何缺陷。\n\n### 步骤 3：结论与行动\n\n问题陈述是**有效的**。开始进行求解。\n\n### 推导与分析\n\n**1. LASSO 的 ADMM 公式**\n\nLASSO 问题是：\n$$ \\underset{x \\in \\mathbb{R}^n}{\\text{minimize}} \\quad \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 $$\n为了应用 ADMM，我们使用变量分裂。我们引入一个辅助变量 $z \\in \\mathbb{R}^n$，并将问题重构为一个约束优化问题：\n$$ \\underset{x, z \\in \\mathbb{R}^n}{\\text{minimize}} \\quad \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|z\\|_1 \\quad \\text{subject to} \\quad x - z = 0 $$\n该问题的增广拉格朗日量（以其缩放对偶变量形式）是：\n$$ L_\\rho(x, z, y) = \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|z\\|_1 + \\frac{\\rho}{2}\\|x - z + y\\|_2^2 - \\frac{\\rho}{2}\\|y\\|_2^2 $$\n其中 $y$ 是缩放的对偶变量。ADMM 算法包括交替地对 $L_\\rho$ 关于 $x$ 和 $z$ 进行最小化，然后更新 $y$：\n1.  **x-更新：** $x^{k+1} = \\underset{x}{\\arg\\min} \\left( \\frac{1}{2}\\|Ax - b\\|_2^2 + \\frac{\\rho}{2}\\|x - z^k + y^k\\|_2^2 \\right)$\n2.  **z-更新：** $z^{k+1} = \\underset{z}{\\arg\\min} \\left( \\lambda \\|z\\|_1 + \\frac{\\rho}{2}\\|x^{k+1} - z + y^k\\|_2^2 \\right)$\n3.  **y-更新：** $y^{k+1} = y^k + x^{k+1} - z^{k+1}$\n\n$z$-更新是 $\\ell_1$-范数的近端算子。其解由软阈值函数 $S_K(\\cdot)$ 给出：\n$$ z^{k+1} = S_{\\lambda/\\rho}(x^{k+1} + y^k) $$\n**软阈值大小**是关键参数 $K = \\lambda/\\rho$。\n\n**2. 数据缩放对优化器的影响**\n\n考虑数据缩放 $(A, b) \\mapsto (A', b') = (s A, s b)$，其中 $s  0$。新的目标函数，带有一个可能新的正则化参数 $\\lambda'$，是：\n$$ J'(x) = \\frac{1}{2}\\|A'x - b'\\|_2^2 + \\lambda' \\|x\\|_1 = \\frac{1}{2}\\|sAx - sb\\|_2^2 + \\lambda' \\|x\\|_1 = \\frac{s^2}{2}\\|Ax - b\\|_2^2 + \\lambda' \\|x\\|_1 $$\n为了保持原问题的优化器不变，新的目标函数 $J'(x)$ 必须等价于原目标函数 $J(x)$，即它必须是 $J(x)$ 的一个正常数倍。让我们将 $J(x)$ 乘以 $s^2$：\n$$ s^2 J(x) = s^2 \\left( \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 \\right) = \\frac{s^2}{2}\\|Ax - b\\|_2^2 + s^2 \\lambda \\|x\\|_1 $$\n通过比较 $J'(x)$ 和 $s^2 J(x)$，我们看到要使它们成为相同的函数（从而有相同的最小化点），我们必须设置：\n$$ \\lambda' = s^2 \\lambda $$\n这是在数据缩放情况下，为了保持 LASSO 问题解不变而对 $\\lambda$ 所需的缩放。\n\n**3. 缩放对 ADMM 和阈值稳定性的影响**\n\n我们现在希望选择新的惩罚参数 $\\rho'$ 来稳定 ADMM 算法。问题明确要求保持软阈值大小不变。\n原始的阈值大小是 $K = \\lambda/\\rho$。新的阈值大小是 $K' = \\lambda'/\\rho'$。为了保持大小恒定，我们设置 $K' = K$：\n$$ \\frac{\\lambda'}{\\rho'} = \\frac{\\lambda}{\\rho} $$\n代入 $\\lambda' = s^2 \\lambda$，我们得到：\n$$ \\frac{s^2 \\lambda}{\\rho'} = \\frac{\\lambda}{\\rho} \\implies \\rho' = s^2 \\rho $$\n因此，为了保持一个恒定的软阈值大小，惩罚参数 $\\rho$ 也必须按 $s^2$ 缩放。\n\n让我们验证一下这个选择，$(\\lambda, \\rho) \\mapsto (\\lambda', \\rho') = (s^2\\lambda, s^2\\rho)$，是否使整个 ADMM 迭代保持不变。新的 x-更新是以下线性系统的解：\n$$ ((A')^T A' + \\rho'I) x = (A')^T b' + \\rho'(z^k - y^k) $$\n代入 $A'=sA$，$b'=sb$ 和 $\\rho'=s^2\\rho$：\n$$ ((sA)^T(sA) + s^2\\rho I) x = (sA)^T(sb) + s^2\\rho(z^k - y^k) $$\n$$ (s^2 A^T A + s^2\\rho I) x = s^2 A^T b + s^2\\rho(z^k - y^k) $$\n两边同时除以 $s^2$（因为 $s0$），我们得到：\n$$ (A^T A + \\rho I) x = A^T b + \\rho(z^k - y^k) $$\n这与原始的 x-更新方程完全相同。因此，如果迭代值 $(z^k, y^k)$ 相同，新的迭代值 $x^{k+1}$ 也将相同。\n新的 z-更新是 $z^{k+1} = S_{\\lambda'/\\rho'}(x^{k+1}+y^k)$。由于 $\\lambda'/\\rho' = (s^2\\lambda)/(s^2\\rho) = \\lambda/\\rho$ 并且 x-更新是不变的，所以 z-更新也是不变的。\ny-更新仅依赖于原始变量 $x$ 和 $z$，因此它也是不变的。\n结论是，在数据缩放 $(A,b) \\mapsto (sA, sb)$ 下，整个 ADMM 迭代序列保持不变，当且仅当参数按 $(\\lambda, \\rho) \\mapsto (s^2\\lambda, s^2\\rho)$ 进行缩放。\n\n**4. 实用归一化策略**\n\n上述分析揭示了数据缩放与算法参数最优选择之间的强耦合关系。一个鲁棒的实现应该对输入数据的任意缩放不敏感。实现这一点的一个常用方法是首先对数据进行归一化。对于 LASSO，标准做法是对 $A$ 的列进行归一化，使其具有单位 $\\ell_2$-范数。这确保了所有特征都在可比较的尺度上。\n\n此外，选择 $\\rho$ 的一个常见启发式方法是平衡 x-更新中的各项。x-更新涉及对矩阵 $(A^TA + \\rho I)$ 求逆。为了使该矩阵良态并平衡两项的影响，$\\rho$ 应与 $A^TA$ 的特征值处于同一数量级。$A^TA$ 的最大特征值是 $\\|A\\|_2^2$。因此，一个合理的启发式方法是选择与 $\\|A\\|_2^2$ 成比例的 $\\rho$。\n\n让我们检查这个启发式方法是否与我们的缩放规则一致。假设我们采用规则 $\\rho = c \\|A\\|_2^2$（其中 $c$ 为某个常数）。如果我们把 $A$ 缩放为 $A' = sA$，该规则建议新的惩罚参数应为 $\\rho' = c \\|A'\\|_2^2 = c \\|sA\\|_2^2 = c s^2 \\|A\\|_2^2 = s^2 (c \\|A\\|_2^2) = s^2 \\rho$。这与我们为迭代不变性推导出的要求完全一致。因此，这种实用策略自动地执行了正确的缩放。\n\n### 选项评估\n\n*   **A. 在 $(A,b) \\mapsto (s A, s b)$ 的缩放变换下，为了保持相同的优化器，必须将 $\\lambda$ 替换为 $\\lambda' = s^2 \\lambda$。为了保持 $z$ 更新中使用的软阈值大小不变，并使整个 ADMM 迭代对此缩放不变，同时设置 $\\rho' = s^2 \\rho$，这会保持关键比率不变。一种实用的归一化方法是：对 $A$ 进行列归一化，使每列的 $\\ell_2$-范数为单位1，并选择与 $\\|A\\|_2^2$ 成比例的 $\\rho$，这能在测量单位发生变化时，保持软阈值大小在数值上稳定。**\n    该陈述与我们的推导完全一致。缩放规则 $\\lambda' = s^2 \\lambda$ 和 $\\rho' = s^2 \\rho$ 是正确的。保持比率 $\\lambda/\\rho$ 不变是正确的。推荐的实用策略，即列归一化和选择 $\\rho \\propto \\|A\\|_2^2$，是标准的、合理的，并且与理论发现一致。\n    **结论：正确。**\n\n*   **B. $z$ 更新的阈值不依赖于 $A$ 或 $b$，因此 $(A,b) \\mapsto (s A, s b)$ 对软阈值的大小没有影响；因此不需要调整 $\\lambda$ 或 $\\rho$，并且归一化 $A$ 或 $b$ 是不必要的，还可能会使解产生偏差。**\n    这是不正确的。虽然阈值的公式 $\\lambda/\\rho$ 没有明确包含 $A$ 或 $b$，但问题的上下文是由 $A$ 和 $b$ 定义的。为了保持问题的解不变，必须调整 $\\lambda$。如果调整了 $\\lambda$，那么也必须调整 $\\rho$ 以保持阈值恒定。不调整 $\\lambda$ 会改变所求解的问题。声称归一化不必要的说法与数值优化的最佳实践相悖。\n    **结论：不正确。**\n\n*   **C. 为了在 $(A,b) \\mapsto (s A, s b)$ 变换后保持相同的软阈值大小，应设置 $\\lambda' = s \\lambda$ 并保持 $\\rho$ 不变，并将 $A$ 的行标准化，使其具有单位 $\\ell_1$-范数，从而稳定阈值。**\n    缩放 $\\lambda' = s\\lambda$ 是不正确的；为了保持优化器不变，应为 $\\lambda' = s^2\\lambda$。因此，建立在这个错误前提上的陈述的其余部分也是不正确的。在改变 $\\lambda$ 的同时保持 $\\rho$ 不变会改变阈值的大小。\n    **结论：不正确。**\n\n*   **D. 选择与 $\\|A\\|_2^2$ 成反比的 $\\rho$，同时保持 $\\lambda$ 固定，可以保证在 $(A,b) \\mapsto (s A, s b)$ 缩放后，软阈值大小按 $s^2$ 的比例减小，这是可取的，因为它加强了数据保真度并加速了收敛；不需要进一步的归一化。**\n    这个陈述包含多个错误。首先，在缩放数据的同时保持 $\\lambda$ 不变会改变问题的解，这违反了陈述的目标之一。其次，如果 $\\rho \\propto 1/\\|A\\|_2^2$，那么 $\\rho' = \\rho/s^2$。在 $\\lambda'=\\lambda$ 的情况下，新的阈值将是 $K' = \\lambda'/\\rho' = \\lambda/(\\rho/s^2) = s^2(\\lambda/\\rho)$，这是按 $s^2$ 的因子*增加*，而不是减少。第三，更大的阈值意味着更强的正则化（更低的数据保真度），这与该陈述的说法相矛盾。\n    **结论：不正确。**", "answer": "$$\\boxed{A}$$", "id": "3429991"}, {"introduction": "对于许多优化问题，“条条大路通罗马”，选择不同的路径可能会带来截然不同的计算效率。当处理“宽”矩阵（即样本数 $m$ 远小于特征数 $n$）的LASSO问题时，直接在原始空间求解可能会因处理大型矩阵 $A^T A$ 而变得昂贵。本练习将带您探索一条更高效的路径：在对偶空间（dual space）中求解LASSO。您将从凸分析的基本原理（如Fenchel共轭）出发，完整地推导LASSO的对偶问题，设计并实现对偶ADMM算法，并最终学习如何通过KKT条件从对偶解中恢复原始解，从而全面掌握一种在特定场景下计算优势显著的先进求解策略 [@problem_id:3429924]。", "problem": "考虑标准二次型形式的最小绝对收缩和选择算子 (LASSO) 问题：最小化残差平方和与一个加权的L1范数惩罚项之和。设 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\lambda \\ge 0$。原始目标是找到一个 $x \\in \\mathbb{R}^{n}$，以最小化 $f(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1$。\n\n您的任务是：\n\n- 通过引入一个显式残差变量 $r = A x - b$，构建拉格朗日函数，并利用核心凸分析事实来推导LASSO的对偶形式：\n  - Fenchel共轭定义 $f^*(y) = \\sup_{x} \\{ \\langle y, x \\rangle - f(x) \\}$、Fenchel-Young不等式，以及凸集指示函数的共轭的性质。\n  - 由$\\lambda$缩放的L1范数的共轭，即 $(\\lambda \\lVert \\cdot \\rVert_1)^*(z)$ 等于半径为$\\lambda$的$\\ell_\\infty$球的指示函数。\n  - 凸规划的基本Karush-Kuhn-Tucker (KKT) 最优性条件，包括原始可行性、对偶可行性和平稳性。\n  请不要直接陈述对偶问题的快捷公式，而是从这些基本定义出发，推导出关于变量 $y \\in \\mathbb{R}^m$ 的约束二次优化形式的对偶问题，并附带相应的显式对偶可行性约束。\n\n- 通过将对偶目标和约束分裂为两个块，在对偶空间中构建交替方向乘子法 (ADMM)。其中一个块是光滑二次项，另一个通过辅助变量 $z \\in \\mathbb{R}^n$ 和线性约束 $z = A^T y$ 来编码一个$\\ell_\\infty$球投影。请仔细说明在 $m \\ll n$ 的情况下，如何在 $y$ 的更新步骤中利用 $A A^T \\in \\mathbb{R}^{m \\times m}$ 的线性代数结构来降低计算复杂度。\n\n- 将对偶变量映射回原始次梯度条件。具体来说，使用平稳性条件将 $A^T y$ 与次梯度 $s \\in \\partial \\lVert x \\rVert_1$ 关联起来，解释有效集的特征，即坐标满足 $\\lvert (A^T y)_i \\rvert = \\lambda$ 时对应于可能非零的原始坐标，并在此有效集上构建一个满足KKT条件的一致的原始候选解 $x$。请明确论证为什么非有效坐标为零，以及如何从对偶解确定有效集的符号。\n\n- 实现一个算法，该算法：\n  1. 使用恒定的惩罚参数 $\\rho  0$，在对偶空间中通过ADMM求解对偶LASSO问题。\n  2. 使用基于KKT的有效集方法，从对偶解中恢复原始候选解 $x$。\n  3. 对每个测试用例评估三个量化指标：\n     - 对偶可行性违反度：在所有坐标 $i$ 上的 $\\max\\{ \\lvert (A^T y)_i \\rvert - \\lambda, 0 \\}$，报告为最大违反度（一个非负浮点数）。\n     - 恢复的原始候选解的KKT残差：对候选解 $x$ 的LASSO KKT系统所编码的平稳性和互补松弛类条件的最坏情况违反度的无穷范数，报告为一个非负浮点数。\n     - 原始-对偶间隙：$\\left( \\frac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1 \\right) - \\left( -\\frac{1}{2}\\lVert y + b \\rVert_2^2 + \\frac{1}{2}\\lVert b \\rVert_2^2 \\right)$，报告为一个非负浮点数。此间隙必须使用推导出的对偶目标函数计算，并且当解精确时应很小。\n\n- 基于算法步骤，提供一个简要的讨论，解释为什么在 $m \\ll n$ 的情况下，求解涉及 $I_m + \\rho A A^T$ 的关于变量 $y$ 的线性系统会从维度中受益，以及如何在一个迭代过程中重复使用 $I_m + \\rho A A^T$ 的分解。\n\n测试套件规范：\n\n使用以下确定性测试套件。对于每种情况，您必须按规定构造 $A$、$b$ 和 $\\lambda$。必须使用给定的种子通过现代伪随机数生成器生成随机值。所有生成的矩阵和向量都是无单位的。\n\n- 情况1（典型的压缩感知场景，中等惩罚）：\n  - 种子：$0$。\n  - 维度：$m = 40$，$n = 120$。\n  - 构造 $A$，其元素为独立同分布的高斯分布，并按 $1/\\sqrt{m}$ 进行缩放。\n  - 构造一个基准稀疏向量 $x^\\star$，它有 $10$ 个非零元素，位于均匀随机的不重复索引处，每个非零元素以等概率取值为 $+1$ 或 $-1$。\n  - 设置 $b = A x^\\star$（无噪声）。\n  - 设置 $\\lambda = 0.1 \\cdot \\lVert A^T b \\rVert_\\infty$。\n\n- 情况2（导致零解的大惩罚）：\n  - 种子：$1$。\n  - 维度：$m = 40$，$n = 120$。\n  - 按情况1的方式构造 $A$、$x^\\star$ 和 $b$（使用新的种子）。\n  - 设置 $\\lambda = 1.5 \\cdot \\lVert A^T b \\rVert_\\infty$。\n\n- 情况3（边界惩罚 $\\lambda = 0$，最小二乘极限）：\n  - 种子：$2$。\n  - 维度：$m = 50$，$n = 80$。\n  - 构造 $A$，其元素为高斯分布，并按 $1/\\sqrt{m}$ 进行缩放。\n  - 按情况1的方式构造具有 $8$ 个非零元素的 $x^\\star$。\n  - 设置 $b = A x^\\star + \\eta$，其中 $\\eta$ 是高斯噪声，每个条目的标准差为 $0.01$，且独立于 $A$ 和 $x^\\star$。\n  - 设置 $\\lambda = 0$。\n\n- 情况4（秩亏的 $A$，利用 $A A^T$ 结构）：\n  - 种子：$3$。\n  - 维度：$m = 30$，$n = 60$。\n  - 构造 $A$ 为 $A = S T$，其中 $S \\in \\mathbb{R}^{m \\times k}$ 和 $T \\in \\mathbb{R}^{k \\times n}$ 具有独立的、按 $1/\\sqrt{m}$ 缩放的高斯元素，且 $k=20$。这确保了 $\\operatorname{rank}(A) \\le k$。\n  - 按情况1的方式构造具有 $8$ 个非零元素的 $x^\\star$。\n  - 设置 $b = A x^\\star$（无噪声）。\n  - 设置 $\\lambda = 0.1 \\cdot \\lVert A^T b \\rVert_\\infty$。\n\n算法要求：\n\n- 使用对偶空间中的交替方向乘子法 (ADMM)，常数惩罚参数 $\\rho = 1.0$，并将对偶变量和辅助变量初始化为零。使用基于原始残差和缩放的对偶残差均达到容忍度阈值的停止准则。\n- 在 $y$ 的更新步骤中，通过预计算并重复使用 $I_m + \\rho A A^T$ 的分解来利用 $A A^T$ 的结构。\n- 使用KKT导出的有效集和限制在有效坐标上的线性求解（必要时使用少量Tikhonov正则化以处理潜在的奇异性）从对偶解中恢复原始候选解 $x$。\n\n输出格式：\n\n您的程序应生成一行输出，其中包含一个方括号括起来的逗号分隔列表形式的结果。该列表应按顺序连接四个案例的三个浮点数：对偶可行性违反度、KKT残差和原始-对偶间隙。例如，输出必须具有 $[v_1,r_1,g_1,v_2,r_2,g_2,v_3,r_3,g_3,v_4,r_4,g_4]$ 的形式，其中每个 $v_i$、$r_i$ 和 $g_i$ 都是实数。不应打印任何其他文本。", "solution": "该问题要求对求解LASSO问题的对偶交替方向乘子法（ADMM）方法进行全面的推导、实现和分析。这包括推导对偶形式、在对偶空间中构建ADMM算法、解释如何通过Karush-Kuhn-Tucker (KKT) 条件从对偶解中恢复原始解、实现该算法并评估其性能。\n\n### 1. LASSO对偶问题的推导\n\n标准的LASSO原始问题由下式给出：\n$$\n\\min_{x \\in \\mathbb{R}^n} f(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\lambda \\ge 0$。\n\n为了推导对偶问题，我们首先为残差引入一个辅助变量 $r \\in \\mathbb{R}^m$，即 $r = Ax - b$。该问题可以重写为一个等式约束优化问题：\n$$\n\\min_{x \\in \\mathbb{R}^n, r \\in \\mathbb{R}^m} \\left\\{ \\frac{1}{2}\\lVert r \\rVert_2^2 + \\lambda \\lVert x \\rVert_1 \\right\\} \\quad \\text{subject to} \\quad Ax - r - b = 0\n$$\n我们通过为等式约束引入一个对偶变量 $y \\in \\mathbb{R}^m$ 来构建拉格朗日函数：\n$$\nL(x, r, y) = \\frac{1}{2}\\lVert r \\rVert_2^2 + \\lambda \\lVert x \\rVert_1 + y^T(Ax - r - b)\n$$\n拉格朗日对偶函数 $g(y)$ 是拉格朗日函数关于原始变量 $x$ 和 $r$ 的下确界：\n$$\ng(y) = \\inf_{x, r} L(x, r, y) = \\inf_{x, r} \\left( \\frac{1}{2}\\lVert r \\rVert_2^2 - y^T r \\right) + \\left( \\lambda \\lVert x \\rVert_1 + (A^T y)^T x \\right) - y^T b\n$$\n这个下确界可以分别对涉及 $r$ 和 $x$ 的项进行计算。\n\n对于涉及 $r$ 的项，我们最小化凸二次函数 $\\frac{1}{2}\\lVert r \\rVert_2^2 - y^T r$。当关于 $r$ 的梯度为零时达到最小值：$\\nabla_r (\\frac{1}{2}r^T r - y^T r) = r - y = 0$，这意味着 $r = y$。将其代回得到 $\\frac{1}{2}\\lVert y \\rVert_2^2 - y^T y = -\\frac{1}{2}\\lVert y \\rVert_2^2$。\n\n对于涉及 $x$ 的项，我们需要计算 $\\inf_x \\{ \\lambda \\lVert x \\rVert_1 + (A^T y)^T x \\}$。这与Fenchel共轭有关。一个函数 $f(x)$ 的共轭是 $f^*(z) = \\sup_x \\{ z^T x - f(x) \\}$。因此，$\\inf_x \\{ f(x) - z^T x \\} = -\\sup_x \\{ z^T x - f(x) \\} = -f^*(z)$。\n令 $f_1(x) = \\lambda \\lVert x \\rVert_1$ 和对偶点为 $-A^T y$，则下确界为：\n$$\n\\inf_x \\{ \\lambda \\lVert x \\rVert_1 + (A^T y)^T x \\} = \\inf_x \\{ \\lambda \\lVert x \\rVert_1 - (-A^T y)^T x \\} = -(\\lambda \\lVert \\cdot \\rVert_1)^*(-A^T y)\n$$\n问题陈述中指出，缩放的$\\ell_1$范数的共轭 $(\\lambda \\lVert \\cdot \\rVert_1)^*(z)$ 是半径为 $\\lambda$ 的$\\ell_\\infty$球的指示函数，记作 $I_{B_\\infty(\\lambda)}(z)$。该指示函数当 $\\lVert z \\rVert_\\infty \\le \\lambda$ 时为 $0$，否则为 $+\\infty$。\n因此，关于 $x$ 的下确界是：\n$$\n-I_{B_\\infty(\\lambda)}(-A^T y) = \\begin{cases} 0  \\text{if } \\lVert -A^T y \\rVert_\\infty \\le \\lambda \\\\ -\\infty  \\text{otherwise} \\end{cases}\n$$\n条件 $\\lVert -A^T y \\rVert_\\infty \\le \\lambda$ 等价于 $\\lVert A^T y \\rVert_\\infty \\le \\lambda$。对偶函数 $g(y)$ 仅当此条件成立时才为有限值。\n\n综合各部分，对偶函数为：\n$$\ng(y) = -\\frac{1}{2}\\lVert y \\rVert_2^2 - y^T b, \\quad \\text{subject to} \\quad \\lVert A^T y \\rVert_\\infty \\le \\lambda\n$$\n对偶问题是最大化 $g(y)$。最大化 $g(y)$ 等价于最小化 $-g(y)$：\n$$\n\\min_{y \\in \\mathbb{R}^m} \\frac{1}{2}\\lVert y \\rVert_2^2 + y^T b \\quad \\text{subject to} \\quad \\lVert A^T y \\rVert_\\infty \\le \\lambda\n$$\n通过对目标函数配方，$\\frac{1}{2}\\lVert y \\rVert_2^2 + y^T b = \\frac{1}{2}(\\lVert y \\rVert_2^2 + 2y^T b + \\lVert b \\rVert_2^2) - \\frac{1}{2}\\lVert b \\rVert_2^2 = \\frac{1}{2}\\lVert y + b \\rVert_2^2 - \\frac{1}{2}\\lVert b \\rVert_2^2$。由于 $\\frac{1}{2}\\lVert b \\rVert_2^2$ 是常数，对偶问题等价于：\n$$\n\\min_{y \\in \\mathbb{R}^m} \\frac{1}{2}\\lVert y + b \\rVert_2^2 \\quad \\text{subject to} \\quad \\lVert A^T y \\rVert_\\infty \\le \\lambda\n$$\n这是一个关于对偶变量 $y$ 的约束二次优化问题。\n\n### 2. 对偶问题的ADMM形式\n\n为了应用ADMM，我们引入一个辅助变量 $z \\in \\mathbb{R}^n$ 并将问题分裂如下：\n$$\n\\min_{y \\in \\mathbb{R}^m, z \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2}\\lVert y + b \\rVert_2^2 + I_{B_\\infty(\\lambda)}(z) \\right\\} \\quad \\text{subject to} \\quad A^T y = z\n$$\n其中 $I_{B_\\infty(\\lambda)}(z)$ 是集合 $\\{z \\mid \\lVert z \\rVert_\\infty \\le \\lambda\\}$ 的指示函数。该问题是ADMM一致性形式 $\\min f(y) + g(z)$ subject to $P y + Q z = c$，其中 $f(y) = \\frac{1}{2}\\lVert y+b \\rVert_2^2$，$g(z) = I_{B_\\infty(\\lambda)}(z)$，$P=A^T$，$Q=-I$，$c=0$。\n\n带有惩罚参数 $\\rho  0$ 和缩放的对偶变量 $u \\in \\mathbb{R}^n$ 的缩放的增广拉格朗日函数 $L_\\rho$ 为：\n$$\nL_\\rho(y, z, u) = \\frac{1}{2}\\lVert y+b \\rVert_2^2 + I_{B_\\infty(\\lambda)}(z) + \\frac{\\rho}{2}\\lVert A^T y - z + u \\rVert_2^2 - \\frac{\\rho}{2}\\lVert u \\rVert_2^2\n$$\nADMM算法包括迭代以下更新步骤：\n\n1.  **$y$-更新**：关于 $y$ 最小化 $L_\\rho$：\n    $$\n    y^{k+1} = \\arg\\min_y \\left\\{ \\frac{1}{2}\\lVert y+b \\rVert_2^2 + \\frac{\\rho}{2}\\lVert A^T y - z^k + u^k \\rVert_2^2 \\right\\}\n    $$\n    这是一个无约束二次最小化问题。令关于 $y$ 的梯度为零可得：\n    $$\n    (y+b) + \\rho A (A^T y - z^k + u^k) = 0 \\implies (I_m + \\rho A A^T) y = \\rho A(z^k - u^k) - b\n    $$\n    $y$ 的更新需要求解这个 $m \\times m$ 的线性系统。\n\n2.  **$z$-更新**：关于 $z$ 最小化 $L_\\rho$：\n    $$\n    z^{k+1} = \\arg\\min_z \\left\\{ I_{B_\\infty(\\lambda)}(z) + \\frac{\\rho}{2}\\lVert A^T y^{k+1} - z + u^k \\rVert_2^2 \\right\\}\n    $$\n    这等价于在半径为 $\\lambda$ 的 $\\ell_\\infty$ 球中找到最接近 $v^k = A^T y^{k+1} + u^k$ 的点 $z$。这是到 $\\ell_\\infty$ 球上的投影：\n    $$\n    z^{k+1} = \\Pi_{B_\\infty(\\lambda)}(v^k)\n    $$\n    该投影是逐元素进行的：$(z^{k+1})_i = \\text{sign}((v^k)_i) \\min(\\lvert (v^k)_i \\rvert, \\lambda)$。\n\n3.  **$u$-更新**：更新缩放的对偶变量：\n    $$\n    u^{k+1} = u^k + (A^T y^{k+1} - z^{k+1})\n    $$\n\n**对于 $m \\ll n$ 的计算优势**：计算量最大的步骤是 $y$-更新，它涉及求解一个以矩阵 $M = I_m + \\rho A A^T$ 为系数的线性系统。这是一个 $m \\times m$ 矩阵。当 $m \\ll n$ 时，这比基于原始方法中出现的 $n \\times n$ 矩阵（如 $A^T A$）要小得多。由于 $\\rho$ 是常数，矩阵 $M$ 在整个迭代过程中是恒定的。我们可以在循环开始前计算一次其Cholesky分解 $M = L L^T$（成本为 $O(m^3)$）。然后，每次 $y$-更新只需要两次三角求解（前向和后向代入），成本为 $O(m^2)$。这种分解的重用使得每次ADMM迭代非常高效。\n\n### 3. 通过KKT条件从对偶解恢复原始解\n\n原始LASSO问题的KKT最优性条件连接了原始解 $x$ 和对偶解 $y$。从拉格朗日推导中，我们在最优解处有两个关键的平稳性条件：\n1.  $r = y \\implies y = Ax - b$（带残差变量的原始可行性）\n2.  $-A^T y \\in \\lambda \\partial \\lVert x \\rVert_1$（关于 $x$ 的平稳性条件）\n\n$\\ell_1$范数的次梯度 $s \\in \\partial \\lVert x \\rVert_1$ 的特征如下：\n- 如果 $x_i \\ne 0$，则 $s_i = \\text{sign}(x_i)$\n- 如果 $x_i = 0$，则 $s_i \\in [-1, 1]$\n\n综合这些，我们得到关于 $x$ 和 $y$ 分量的条件：\n- 如果 $\\lvert (A^T y)_i \\rvert  \\lambda$，那么必然有 $-(A^T y)_i / \\lambda$ 是某个满足 $\\lvert s_i \\rvert  1$ 的 $s_i$。这只有当 $x_i = 0$ 时才可能。这些索引构成了“非有效集”。\n- 如果 $x_i \\neq 0$，那么必然有 $s_i = \\text{sign}(x_i)$，这意味着 $\\lvert s_i \\rvert = 1$。这要求 $\\lvert (A^T y)_i / \\lambda \\rvert = 1$，即 $\\lvert (A^T y)_i \\rvert = \\lambda$。这些索引构成了“有效集” $\\mathcal{A}$。\n\n这提供了一个从对偶解 $y$ 恢复原始候选解 $x$ 的过程：\n1.  识别有效集 $\\mathcal{A} = \\{i \\mid \\lvert (A^T y)_i \\rvert \\approx \\lambda\\}$。在实践中，由于数值精度问题，我们使用一个小的容差。\n2.  对于所有非有效索引 $i \\notin \\mathcal{A}$，将原始坐标 $x_i$ 设置为 $0$。\n3.  对于有效索引 $i \\in \\mathcal{A}$，我们知道它们的符号必须满足 $\\text{sign}(x_i) = -\\text{sign}((A^T y)_i)$。为了找到它们的大小，我们使用条件 $y = Ax - b$。由于 $x$ 的非有效部分为零，这变成 $y = A_{\\mathcal{A}} x_{\\mathcal{A}} - b$，其中 $A_{\\mathcal{A}}$ 是由 $A$ 中属于 $\\mathcal{A}$ 的列组成的子矩阵，$x_{\\mathcal{A}}$ 是有效系数的向量。\n4.  我们求解线性系统 $A_{\\mathcal{A}} x_{\\mathcal{A}} = y + b$ 以获得 $x_{\\mathcal{A}}$。为了处理 $A_{\\mathcal{A}}$ 可能病态或非满秩的情况，我们可以求解正则化最小二乘问题 $\\min_{x_{\\mathcal{A}}} \\lVert A_{\\mathcal{A}} x_{\\mathcal{A}} - (y+b) \\rVert_2^2 + \\delta \\lVert x_{\\mathcal{A}} \\rVert_2^2$（其中 $\\delta  0$ 是一个小数）。这对应于求解正规方程：\n    $$\n    (A_{\\mathcal{A}}^T A_{\\mathcal{A}} + \\delta I) x_{\\mathcal{A}} = A_{\\mathcal{A}}^T (y + b)\n    $$\n5.  完整的原始候选解 $x$ 是通过将求解出的 $x_{\\mathcal{A}}$ 放置在有效索引处，并在其他位置填充零来构建的。\n\n### 4. 量化指标\n\n解对 $(x, y)$ 的质量通过三个指标进行评估：\n1.  **对偶可行性违反度**：衡量对偶解 $y$ 违反其约束的程度：\n    $V_{DF} = \\max_{i} \\{ \\max(\\lvert (A^T y)_i \\rvert - \\lambda, 0) \\}$。值为 $0$ 表示对偶可行。\n2.  **KKT残差**：衡量原始平稳性条件 $A^T(Ax-b) + \\lambda s = 0$（其中 $s \\in \\partial \\lVert x \\rVert_1$）的违反程度。对于一个候选解 $x$，令 $g = A^T(Ax-b)$。每个分量的残差为：\n    $$\n    \\text{res}_i = \\begin{cases} \\lvert g_i + \\lambda \\text{sign}(x_i) \\rvert  \\text{if } x_i \\neq 0 \\\\ \\max(\\lvert g_i \\rvert - \\lambda, 0)  \\text{if } x_i = 0 \\end{cases}\n    $$\n    总残差是该向量的无穷范数：$R_{KKT} = \\lVert \\text{res} \\rVert_\\infty$。\n3.  **原始-对偶间隙**：原始目标值和对偶目标值之间的差异。根据弱对偶性，这应该是非负的。\n    - 原始目标：$P(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1$。\n    - 对偶目标：$D(y) = -\\frac{1}{2}\\lVert y + b \\rVert_2^2 + \\frac{1}{2}\\lVert b \\rVert_2^2$。\n    - 间隙：$G = P(x) - D(y)$。对于最优解，间隙为零。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef project_ell_inf_ball(v, radius):\n    \"\"\"Projects a vector onto the L-infinity ball of a given radius.\"\"\"\n    return np.sign(v) * np.minimum(np.abs(v), radius)\n\ndef solve_lasso_dual_admm(A, b, lambda_val, rho, max_iters=2000, tol_abs=1e-4, tol_rel=1e-2):\n    \"\"\"\n    Solves the LASSO problem in the dual domain using ADMM.\n    \n    The problem is min 1/2 ||y+b||^2 s.t. ||A'y||_inf = lambda.\n    ADMM formulation: min 1/2 ||y+b||^2 + I(z) s.t. A'y - z = 0.\n    \"\"\"\n    m, n = A.shape\n    y = np.zeros(m)\n    z = np.zeros(n)\n    u = np.zeros(n)\n\n    # Pre-factorize the matrix for the y-update\n    # M = I_m + rho * A A'\n    M = np.eye(m) + rho * (A @ A.T)\n    # Use Cholesky factorization for the positive-definite matrix M\n    try:\n        L = cholesky(M, lower=True)\n        use_cholesky = True\n    except np.linalg.LinAlgError:\n        use_cholesky = False\n\n\n    for k in range(max_iters):\n        # y-update: solve (I + rho*A*A')y = rho*A(z-u) - b\n        rhs_y = rho * (A @ (z - u)) - b\n        if use_cholesky:\n            # Solve L L' y = rhs_y\n            y_sol_fwd = solve_triangular(L, rhs_y, lower=True)\n            y = solve_triangular(L.T, y_sol_fwd, lower=False)\n        else:\n            y = np.linalg.solve(M, rhs_y)\n\n        # z-update: projection onto L-infinity ball\n        v_z = A.T @ y + u\n        if lambda_val  0:\n            z = project_ell_inf_ball(v_z, lambda_val)\n        else:\n            # For lambda=0, the constraint is ||A'y||_inf = 0, so z must be 0.\n            z = np.zeros(n)\n\n        # u-update\n        u = u + (A.T @ y - z)\n        \n    return y, z, u\n\ndef recover_primal_from_dual(A, b, y, lambda_val, reg=1e-8):\n    \"\"\"\n    Recovers the primal solution x from the dual solution y using KKT conditions.\n    \"\"\"\n    m, n = A.shape\n    x_rec = np.zeros(n)\n    \n    if lambda_val  0:\n        # Active set identification based on ||A'y||_inf = lambda\n        A_T_y = A.T @ y\n        # Use a relative tolerance for a more robust check\n        active_set_indices = np.where(np.isclose(np.abs(A_T_y), lambda_val, rtol=1e-3))[0]\n\n        if active_set_indices.size  0:\n            A_active = A[:, active_set_indices]\n            # Primal-dual relation: y = Ax - b => Ax = y + b\n            # We solve for x_active from A_active * x_active = y + b\n            # Use regularized normal equations for stability\n            ATA = A_active.T @ A_active\n            regularizer = reg * np.eye(ATA.shape[0])\n            rhs = A_active.T @ (y + b)\n            \n            try:\n                x_active = np.linalg.solve(ATA + regularizer, rhs)\n                x_rec[active_set_indices] = x_active\n            except np.linalg.LinAlgError:\n                # Fallback to lstsq if solve fails\n                x_active = np.linalg.lstsq(A_active, y + b, rcond=None)[0]\n                x_rec[active_set_indices] = x_active\n                \n    else:  # lambda = 0 (Least Squares)\n        # KKT condition is A'Ax = A'b.\n        # From dual, Ax = y+b. We solve this system for x.\n        x_rec = np.linalg.lstsq(A, y + b, rcond=None)[0]\n        \n    return x_rec\n\ndef calculate_metrics(A, b, lambda_val, x, y):\n    \"\"\"Calculates dual feasibility, KKT residual, and primal-dual gap.\"\"\"\n    m, n = A.shape\n    \n    # 1. Dual feasibility violation\n    # max( |(A'y)_i| - lambda, 0 )\n    dual_feas_viol = np.max(np.maximum(np.abs(A.T @ y) - lambda_val, 0))\n\n    # 2. KKT residual\n    # Stationarity cond: A'(Ax-b) + lambda * s = 0 for s in subgrad(x)\n    g = A.T @ (A @ x - b)\n    kkt_res_vec = np.zeros(n)\n    \n    is_nonzero = np.abs(x)  1e-8\n    \n    # For non-zero components of x\n    kkt_res_vec[is_nonzero] = np.abs(g[is_nonzero] + lambda_val * np.sign(x[is_nonzero]))\n    # For zero components of x\n    kkt_res_vec[~is_nonzero] = np.maximum(np.abs(g[~is_nonzero]) - lambda_val, 0)\n    \n    kkt_residual = np.max(kkt_res_vec)\n\n    # 3. Primal-dual gap\n    # Primal objective\n    primal_obj = 0.5 * np.linalg.norm(A @ x - b)**2 + lambda_val * np.linalg.norm(x, 1)\n    \n    # Dual objective from derivation\n    # max_y -1/2 ||y||^2 - y'b = min_y 1/2||y+b||^2 - 1/2||b||^2\n    # Dual objective value: - (1/2||y+b||^2 - 1/2||b||^2) = 1/2||b||^2 - 1/2||y+b||^2\n    dual_obj = 0.5 * np.linalg.norm(b)**2 - 0.5 * np.linalg.norm(y + b)**2\n    \n    primal_dual_gap = primal_obj - dual_obj\n    \n    return dual_feas_viol, kkt_residual, primal_dual_gap\n\ndef generate_test_case_data(case_num):\n    \"\"\"Generates problem data A, b, lambda for a specific test case.\"\"\"\n    if case_num == 1:\n        seed, m, n, k = 0, 40, 120, 10\n        rng = np.random.default_rng(seed)\n        A = rng.normal(size=(m, n)) / np.sqrt(m)\n        x_star = np.zeros(n)\n        nonzero_indices = rng.choice(n, k, replace=False)\n        x_star[nonzero_indices] = rng.choice([-1.0, 1.0], k)\n        b = A @ x_star\n        lambda_val = 0.1 * np.linalg.norm(A.T @ b, np.inf)\n\n    elif case_num == 2:\n        seed, m, n, k = 1, 40, 120, 10\n        rng = np.random.default_rng(seed)\n        A = rng.normal(size=(m, n)) / np.sqrt(m)\n        x_star = np.zeros(n)\n        nonzero_indices = rng.choice(n, k, replace=False)\n        x_star[nonzero_indices] = rng.choice([-1.0, 1.0], k)\n        b = A @ x_star\n        lambda_val = 1.5 * np.linalg.norm(A.T @ b, np.inf)\n\n    elif case_num == 3:\n        seed, m, n, k = 2, 50, 80, 8\n        rng = np.random.default_rng(seed)\n        A = rng.normal(size=(m, n)) / np.sqrt(m)\n        x_star = np.zeros(n)\n        nonzero_indices = rng.choice(n, k, replace=False)\n        x_star[nonzero_indices] = rng.choice([-1.0, 1.0], k)\n        noise = rng.normal(0, 0.01, size=m)\n        b = A @ x_star + noise\n        lambda_val = 0.0\n\n    elif case_num == 4:\n        seed, m, n, k, rank_k = 3, 30, 60, 8, 20\n        rng = np.random.default_rng(seed)\n        S = rng.normal(size=(m, rank_k)) / np.sqrt(m)\n        T = rng.normal(size=(rank_k, n))\n        A = S @ T\n        x_star = np.zeros(n)\n        nonzero_indices = rng.choice(n, k, replace=False)\n        x_star[nonzero_indices] = rng.choice([-1.0, 1.0], k)\n        b = A @ x_star\n        lambda_val = 0.1 * np.linalg.norm(A.T @ b, np.inf)\n        \n    else:\n        raise ValueError(\"Invalid case number\")\n        \n    return A, b, lambda_val\n\ndef solve():\n    test_cases = [1, 2, 3, 4]\n    results = []\n    rho = 1.0  # ADMM penalty parameter\n\n    for case_num in test_cases:\n        A, b, lambda_val = generate_test_case_data(case_num)\n        \n        # 1. Solve the dual LASSO using ADMM\n        y_sol, _, _ = solve_lasso_dual_admm(A, b, lambda_val, rho)\n        \n        # 2. Recover a primal candidate from the dual solution\n        x_rec = recover_primal_from_dual(A, b, y_sol, lambda_val)\n        \n        # 3. Evaluate metrics\n        v, r, g = calculate_metrics(A, b, lambda_val, x_rec, y_sol)\n        \n        results.extend([v, r, g])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3429924"}]}