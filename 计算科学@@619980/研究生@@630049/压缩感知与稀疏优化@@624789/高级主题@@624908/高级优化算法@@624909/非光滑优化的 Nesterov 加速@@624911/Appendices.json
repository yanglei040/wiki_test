{"hands_on_practices": [{"introduction": "在构建正确的算法之前，通过一个反例来理解为何简单的方法会失败，这会非常有启发性。本练习将通过一个引导性的计算，展示在梯度步上简单地叠加动量项，而忽略非光滑项的近端算子，会导致算法发散 [@problem_id:3461182]。这凸显了 FISTA 中复合更新步骤的核心作用，并为我们接下来构建稳健的加速算法奠定了基础。", "problem": "考虑压缩感知和稀疏优化中的一个核心复合凸优化问题：最小化函数 $F(x) = f(x) + g(x)$，其中 $f$ 具有 $L$-Lipschitz 连续梯度，$g$ 是一个正常、闭、凸但通常非光滑的正则化项。一个典型实例是一维 Lasso 目标函数，其中 $f(x) = \\tfrac{1}{2} L x^{2}$ (因此 $\\nabla f(x) = L x$) 且 $g(x) = \\lambda |x|$。在此示例中，对于任意 $\\lambda > 0$，极小点为 $x^{\\star} = 0$。快速迭代收缩阈值算法 (Fast Iterative Shrinkage-Thresholding Algorithm, FISTA) 使用一个外推步骤，然后是一个考虑了 $g$ 的复合近端步骤。相比之下，一种应用外推但省略近端步骤的朴素加速方法，会将目标函数视为 $g$ 不存在，并仅对 $f$ 执行加速梯度步骤。\n\n分析以下完全忽略 $g$ 的朴素外推方案：\n- 固定 $L = 1$, $\\lambda > 0$，常数外推参数 $\\beta = 2$，以及步长 $t = 0.1$ (满足 $t \\leq 1/L$)。\n- 定义外推点 $y_{k} = x_{k} + \\beta (x_{k} - x_{k-1})$。\n- 通过只针对光滑梯度的规则进行更新 $x_{k+1} = y_{k} - t \\nabla f(y_{k})$。\n- 使用 $x_{-1} = 0$ 和 $x_{0} = 1$ 进行初始化。\n\n从 $L$-Lipschitz 梯度、凸性以及近端算子的基本定义出发，推导该朴素方案所引出的线性递推关系，并计算控制状态演化 $(x_{k}, x_{k-1})$ 的相关 $2 \\times 2$ 伴随矩阵的谱半径。利用此计算证明，即使步长满足 $t \\leq 1/L$，朴素外推也可能发散，从而证明在处理如 $\\ell_{1}$ 范数之类的非光滑正则化项时，加速方法中复合近端结构的必要性。你的最终答案必须是谱半径的精确实数值。请勿四舍五入。", "solution": "该问题要求分析一个应用于复合优化问题的朴素加速梯度方法。该问题陈述的有效性已得到确认，因为它是科学有据、适定且客观的。我们接下来推导解答。\n\n需要最小化的目标函数是 $F(x) = f(x) + g(x)$，其中 $f(x)$ 具有 $L$-Lipschitz 连续梯度，$g(x)$ 是一个非光滑正则化项。我们考虑的具体实例是一维 Lasso 问题，其中 $f(x) = \\frac{1}{2} L x^2$ 且 $g(x) = \\lambda |x|$。问题指定了以下常数：$L=1$，$\\lambda > 0$，常数外推参数 $\\beta = 2$，以及步长 $t = 0.1$。步长条件 $t \\leq \\frac{1}{L}$ 得到满足，因为 $0.1 \\leq \\frac{1}{1}$。\n\n该朴素方案忽略了非光滑项 $g(x)$，包含以下步骤：\n1. 外推：$y_{k} = x_{k} + \\beta (x_{k} - x_{k-1})$\n2. 仅对 $f$ 进行梯度更新：$x_{k+1} = y_{k} - t \\nabla f(y_{k})$\n\n对于给定的函数 $f(x) = \\frac{1}{2} L x^2$ 且 $L=1$，我们有 $f(x) = \\frac{1}{2} x^2$。$f$ 的梯度是 $\\nabla f(x) = x$。因此，在外推点 $y_k$ 处计算的梯度是 $\\nabla f(y_k) = y_k$。\n\n现在我们可以将给定的参数和表达式代入迭代方案中。\n当 $\\beta=2$ 时，外推步骤为：\n$$y_{k} = x_{k} + 2(x_{k} - x_{k-1}) = 3x_k - 2x_{k-1}$$\n\n当 $t=0.1$ 且 $\\nabla f(y_k) = y_k$ 时，更新步骤为：\n$$x_{k+1} = y_{k} - 0.1 y_{k} = (1 - 0.1) y_{k} = 0.9 y_{k}$$\n\n结合这两个方程，我们可以为序列 $\\{x_k\\}$ 建立一个线性递推关系：\n$$x_{k+1} = 0.9 (3x_k - 2x_{k-1})$$\n$$x_{k+1} = 2.7 x_k - 1.8 x_{k-1}$$\n\n这是一个二阶常系数齐次线性递推关系。为了分析其动态特性，我们可以将其表示为状态空间形式。设第 $k$ 次迭代的状态向量为 $v_k = \\begin{pmatrix} x_k \\\\ x_{k-1} \\end{pmatrix}$。状态转移由 $v_{k+1} = M v_k$ 给出，其中 $M$ 是伴随矩阵。\n下一个状态向量是 $v_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ x_k \\end{pmatrix}$。我们可以将系统写成：\n$$\n\\begin{pmatrix} x_{k+1} \\\\ x_k \\end{pmatrix} = \\begin{pmatrix} 2.7  -1.8 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} x_k \\\\ x_{k-1} \\end{pmatrix}\n$$\n因此，控制状态演化的伴随矩阵是：\n$$M = \\begin{pmatrix} 2.7  -1.8 \\\\ 1  0 \\end{pmatrix}$$\n\n系统的稳定性由 $M$ 的谱半径决定，记为 $\\rho(M)$，它是其特征值绝对值的最大值。为求特征值，我们求解特征方程 $\\det(M - \\zeta I) = 0$，其中 $I$ 是单位矩阵，$\\zeta$ 代表一个特征值。\n$$\\det \\begin{pmatrix} 2.7 - \\zeta  -1.8 \\\\ 1  0 - \\zeta \\end{pmatrix} = 0$$\n$$(2.7 - \\zeta)(-\\zeta) - (-1.8)(1) = 0$$\n$$-2.7\\zeta + \\zeta^2 + 1.8 = 0$$\n$$\\zeta^2 - 2.7\\zeta + 1.8 = 0$$\n\n我们使用求根公式 $\\zeta = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ 来解这个关于 $\\zeta$ 的二次方程，其中 $a=1$, $b=-2.7$, $c=1.8$。\n$$\\zeta = \\frac{-(-2.7) \\pm \\sqrt{(-2.7)^2 - 4(1)(1.8)}}{2(1)}$$\n$$\\zeta = \\frac{2.7 \\pm \\sqrt{7.29 - 7.2}}{2}$$\n$$\\zeta = \\frac{2.7 \\pm \\sqrt{0.09}}{2}$$\n$$\\zeta = \\frac{2.7 \\pm 0.3}{2}$$\n\n两个特征值是：\n$$\\zeta_1 = \\frac{2.7 + 0.3}{2} = \\frac{3.0}{2} = 1.5$$\n$$\\zeta_2 = \\frac{2.7 - 0.3}{2} = \\frac{2.4}{2} = 1.2$$\n\n谱半径 $\\rho(M)$ 是特征值模的最大值：\n$$\\rho(M) = \\max(|\\zeta_1|, |\\zeta_2|) = \\max(|1.5|, |1.2|) = 1.5$$\n\n由于谱半径 $\\rho(M) = 1.5$ 大于 $1$，该迭代方案是不稳定的。对于给定的非零初始化 ($x_{-1}=0, x_0=1$)，状态向量的范数 $\\|v_k\\|$ 将呈指数增长，因此迭代序列 $\\{x_k\\}$ 将会发散。这表明，即使步长 $t$ 的选择小到足以保证标准梯度下降法在光滑部分 $f(x)$ 上收敛，若应用外推而不对非光滑项 $g(x)$ 使用相应的近端步骤，也可能导致发散。这证明了像 FISTA 这类算法中固有的复合近端结构的必要性，这些算法旨在处理非光滑正则化项并确保收敛。", "answer": "$$\\boxed{1.5}$$", "id": "3461182"}, {"introduction": "在理解了算法设计的动机之后，本实践将聚焦于“如何实现”这一核心问题。该练习将指导你为 LASSO 问题从零开始实现一个完整且实用的 FISTA 求解器 [@problem_id:3461192]。你需要将 Nesterov 动量、用于自适应步长的回溯线搜索以及用于提升稳定性的重启规则等关键部分集成起来，这些都是构建一个稳健优化器所必需的。", "problem": "本任务要求您推导并实现一种带线搜索和重启的加速近端梯度法，用于求解在压缩感知和稀疏优化中典型的复合凸目标函数。考虑最小化一个复合函数 $F(x) = f(x) + g(x)$，其中 $f(x)$ 是一个具有Lipschitz连续梯度的光滑凸函数，$g(x)$ 是一个其近端算子可被高效计算的真闭凸函数。在本次任务中，您将专注于带有$\\ell_{1}$范数正则化器的最小二乘数据保真项，即\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; F(x) \\equiv \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，以及 $\\lambda \\ge 0$。\n\n从以下基本要点出发：\n- $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ 的梯度为 $\\nabla f(x) = A^{\\top} (A x - b)$，并且是Lipschitz连续的，其Lipschitz常数 $L_{f}$ 等于 $A$ 的谱范数的平方，即 $L_{f} = \\|A\\|_{2}^{2}$。\n- 缩放后的$\\ell_{1}$范数 $g(x)=\\lambda \\|x\\|_{1}$ 在点 $z$ 处，步长为 $\\alpha>0$ 时的近端算子由逐分量软阈值给出，即 $\\operatorname{prox}_{\\alpha g}(z) = \\mathcal{S}_{\\alpha \\lambda}(z)$，其中 $\\left(\\mathcal{S}_{\\tau}(z)\\right)_{i} = \\operatorname{sign}(z_{i}) \\max\\{|z_{i}| - \\tau, 0\\}$。\n- Nesterov加速可通过一个由标量序列 $\\{t_{k}\\}_{k \\ge 0}$ 定义的动量序列和一个由迭代点 $x_{k}$ 构建的外推步骤 $y_{k}$，来对近端梯度法中的迭代点进行外推。\n- 回溯线搜索可在给定当前点 $y$、试验点 $x$ 和候选Lipschitz常数 $L$ 的情况下，强制满足 $f(x)$ 的二次上界：\n$$\nf(x) \\le f(y) + \\langle \\nabla f(y), x - y \\rangle + \\frac{L}{2}\\|x - y\\|_{2}^{2}.\n$$\n- 当目标函数值增加时，可使用自适应重启条件来重置加速过程。\n\n您的任务是：\n- 基于上述要点，推导出一套完整的更新规则，用于一个带有Nesterov动量、对局部Lipschitz常数进行回溯线搜索以及基于目标函数的自适应重启规则的加速近端梯度法。\n- 将推导出的算法实现为一个程序，用于解决所提供测试套件中的稀疏最小二乘问题。您的实现必须能计算梯度、近端算子，强制执行上述回溯不等式，更新Nesterov动量，并在重启规则有助于提高收敛稳定性时应用该规则。\n\n程序输入和随机性：\n- 程序必须是自包含的，且不得读取任何用户输入。当需要随机性时，请使用固定的随机种子以确保结果是确定性的。\n\n停止规则：\n- 使用一个结合了相对迭代变化阈值和适用于复合优化的近端梯度映射条件的停止规则。设 $\\epsilon$ 表示一个小的容差。迭代变化测试可基于 $\\|x_{k+1} - x_{k}\\|_{2}/\\max\\{1,\\|x_{k}\\|_{2}\\} \\le \\epsilon$。在外推点 $y$ 处，步长为 $L$ 的近端梯度映射为 $G_{L}(y) = L\\left(y - \\operatorname{prox}_{g/L}\\left(y - \\nabla f(y)/L\\right)\\right)$。使用无穷范数 $\\|G_{L}(y)\\|_{\\infty} \\le \\epsilon'$，并选择与 $\\epsilon$ 同数量级的 $\\epsilon'$。\n\n测试套件：\n实现您的方法，并在以下四种情况上运行。所有矩阵和向量必须严格按照规定生成。对于每种情况，最终结果必须是单一基本类型（布尔值、整数或浮点数）。请酌情使用欧几里得范数 $\\|\\cdot\\|_{2}$ 和无穷范数 $\\|\\cdot\\|_{\\infty}$。\n\n- 情况A（压缩感知“理想情况”）：\n  - 维度：$m = 40$，$n = 120$。\n  - 稀疏度：基准真值 $x_{\\star}$ 中有 $k = 8$ 个非零项。\n  - 随机生成：设置随机种子为 $12345$。生成具有独立标准正态分布项的矩阵 $A$，并将其各列按 $1/\\sqrt{m}$ 缩放。通过无放回均匀抽样选择 $k$ 个索引，并为这些项赋予独立的标准正态值（其余项为零），从而生成一个 $k$-稀疏的 $x_{\\star}$。生成噪声 $e$，其项为方差 $\\sigma^{2}$ 的独立正态分布，其中 $\\sigma = 10^{-3}$，并设置 $b = A x_{\\star} + e$。\n  - 正则化：$\\lambda = 0.01 \\cdot \\|A^{\\top} b\\|_{\\infty}$。\n  - 此情况的输出：相对解误差 $\\|x_{\\text{alg}} - x_{\\star}\\|_{2} / \\max\\{1, \\|x_{\\star}\\|_{2}\\}$，以浮点数形式表示。\n\n- 情况B（边界条件 $\\lambda = 0$）：\n  - 维度：$m = 50$，$n = 20$。\n  - 随机生成：设置随机种子为 $54321$。生成具有独立标准正态分布项的矩阵 $A$，并将其各列按 $1/\\sqrt{m}$ 缩放。生成具有独立标准正态分布项的 $b$。\n  - 正则化：$\\lambda = 0$。\n  - 基准：计算最小二乘解 $x_{\\text{LS}}$，作为 $\\min_{x} \\|A x - b\\|_{2}^{2}$ 的最小范数解。\n  - 此情况的输出：相对偏差 $\\|x_{\\text{alg}} - x_{\\text{LS}}\\|_{2} / \\max\\{1, \\|x_{\\text{LS}}\\|_{2}\\}$，以浮点数形式表示。\n\n- 情况C（极强正则化的边缘情况）：\n  - 维度：$m = 30$，$n = 50$。\n  - 随机生成：设置随机种子为 $11111$。生成具有独立标准正态分布项的矩阵 $A$，并将其各列按 $1/\\sqrt{m}$ 缩放。生成具有独立标准正态分布项的 $b$。\n  - 正则化：$\\lambda = 100 \\cdot \\|A^{\\top} b\\|_{\\infty}$。\n  - 此情况的输出：估计支撑集的整数基数，即满足 $|x_{\\text{alg}, i}| > 10^{-8}$ 的索引 $i$ 的数量。\n\n- 情况D（秩亏设计矩阵）：\n  - 维度：$m = 30$，$n = 60$，构建方式如下。设置随机种子为 $22222$。首先生成 $A_{0} \\in \\mathbb{R}^{m \\times 30}$，其项为独立的标准正态分布，并将其各列按 $1/\\sqrt{m}$ 缩放。然后通过将 $A_{0}$ 与自身拼接来设置 $A = [A_{0} \\;\\; A_{0}]$。生成具有独立标准正态分布项的 $b$。\n  - 正则化：$\\lambda = 0.05 \\cdot \\|A^{\\top} b\\|_{\\infty}$。\n  - 此情况的输出：在 $x_{\\text{alg}}$ 处的Karush-Kuhn-Tucker (KKT) 平稳性残差，定义为最小次梯度残差的无穷范数\n    $$\n    r(x) \\equiv \\left\\|\\nabla f(x) + \\lambda v \\right\\|_{\\infty},\n    $$\n    其中 $v \\in \\partial \\|x\\|_{1}$ 是任意次梯度。通过逐分量公式计算 $r(x)$\n    $$\n    \\left(\\nabla f(x)\\right)_{i} = \\left(A^{\\top}(A x - b)\\right)_{i}, \\quad \\text{和} \\quad \n    r(x) = \\max\\left\\{ \\max_{i: x_{i} \\ne 0} \\left| \\left(\\nabla f(x)\\right)_{i} + \\lambda \\operatorname{sign}(x_{i}) \\right|, \\; \\max_{i: x_{i} = 0} \\max\\{0, |\\left(\\nabla f(x)\\right)_{i}| - \\lambda\\} \\right\\}。\n    $$\n    使用约定，即通过 $|x_{i}| \\le 10^{-12}$ 来数值检验“$x_{i} = 0$”。以浮点数形式返回 $r(x)$。\n\n实现约束：\n- 使用Nesterov加速和一个重启规则，当目标函数值增加时重置动量，即如果 $F(x_{k+1}) > F(x_{k})$，则设置动量标量为 $t_{k+1} = 1$，外推点为 $y_{k+1} = x_{k+1}$。\n- 使用回溯线搜索，从局部Lipschitz常数 $L$ 的当前估计值开始，并将其乘以一个因子 $\\eta > 1$，直到满足 $f$ 的二次上界条件。\n- 使用通过奇异值计算或等效的谱范数评估得到的 $L_{0} = \\|A\\|_{2}^{2}$ 来初始化步长。\n\n数值容差：\n- 最大迭代次数使用 $N_{\\max} = 10000$，迭代变化的容差使用 $\\epsilon = 10^{-8}$。选择与 $\\epsilon$ 同数量级的近端梯度映射容差 $\\epsilon'$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四种情况的结果，以逗号分隔并用方括号括起，顺序为情况A、情况B、情况C、情况D。例如，输出格式必须与以下完全一样\n$[r_{A}, r_{B}, r_{C}, r_{D}]$\n其中 $r_{A}$、$r_{B}$ 和 $r_{D}$ 是浮点数，$r_{C}$ 是整数。不应打印任何其他文本。", "solution": "用户要求推导并实现一种为稀疏最小二乘问题（通常称为LASSO）量身定制的加速近端梯度法。该算法必须包含Nesterov风格的动量、用于步长的回溯线搜索，以及基于目标函数行为的自适应重启机制。\n\n### 1. 问题表述\n\n优化问题是最小化一个复合目标函数 $F(x)$：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; F(x) \\equiv f(x) + g(x)\n$$\n其中\n- $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ 是光滑、凸的数据保真项。其梯度为 $\\nabla f(x) = A^{\\top} (A x - b)$，并且是Lipschitz连续的，常数为 $L_f = \\|A\\|_{2}^{2}$。\n- $g(x) = \\lambda \\|x\\|_{1}$ 是凸的、非光滑的正则化项。其近端算子 $\\operatorname{prox}_{\\alpha g}(z)$ 是软阈值算子 $\\mathcal{S}_{\\alpha\\lambda}(z)$，逐分量定义为 $(\\mathcal{S}_{\\tau}(z))_i = \\operatorname{sign}(z_i) \\max\\{|z_i| - \\tau, 0\\}$。\n\n### 2. 算法推导\n\n该算法是快速迭代收缩阈值算法（FISTA）的一个变体。我们将逐步构建更新规则，结合所需的各个组件。设迭代序列为 $\\{x_k\\}$。\n\n#### 2.1. 核心近端梯度步\n\n最小化 $F(x)$ 的基本迭代步骤是近端梯度更新。在迭代点 $y$ 处，步长为 $\\alpha > 0$，下一个迭代点 $x^{+}$ 通过最小化 $f(x)$ 在 $y$ 点的二次近似加上非光滑项 $g(x)$ 来找到：\n$$\nx^{+} = \\arg\\min_{x} \\left( f(y) + \\langle \\nabla f(y), x - y \\rangle + \\frac{1}{2\\alpha}\\|x - y\\|_{2}^{2} + g(x) \\right)\n$$\n通过配方法，这等价于：\n$$\nx^{+} = \\arg\\min_{x} \\left( \\frac{1}{2\\alpha}\\|x - (y - \\alpha \\nabla f(y))\\|_{2}^{2} + g(x) \\right)\n$$\n这就是由 $\\alpha$ 缩放的 $g$ 的近端算子的定义：\n$$\nx^{+} = \\operatorname{prox}_{\\alpha g}(y - \\alpha \\nabla f(y))\n$$\n在我们的问题中，步长为 $\\alpha = 1/L$，其中 $L$ 是局部Lipschitz估计值，更新公式为：\n$$\nx^{+} = \\mathcal{S}_{\\lambda/L}(y - \\frac{1}{L}A^{\\top}(Ay-b))\n$$\n\n#### 2.2. Nesterov加速\n\nNesterov加速通过在外推点 $y_k$（而不是当前迭代点 $x_k$）计算近端梯度步来引入“动量”项。迭代点 $\\{x_k\\}$、外推点 $\\{y_k\\}$ 和动量标量 $\\{t_k\\}$ 的更新序列如下：\n1.  **计算下一个迭代点**：$x_{k+1} = \\operatorname{prox}_{g/L_k}(y_k - \\frac{1}{L_k}\\nabla f(y_k))$\n2.  **更新动量标量**：$t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$\n3.  **为下一步进行外推**：$y_{k+1} = x_{k+1} + \\frac{t_k-1}{t_{k+1}}(x_{k+1} - x_k)$\n\n初始条件为 $x_0 = 0$，$y_0 = x_0$ 和 $t_0 = 1$。注意存在一些变体；我们选择一种与重启规则能很好配合的表述形式。\n\n#### 2.3. 回溯线搜索\n\n全局Lipschitz常数 $L_f = \\|A\\|_2^2$ 可能是对局部曲率的悲观高估，导致收敛缓慢。回溯线搜索在每次迭代中自适应地寻找一个合适的局部常数 $L_k$。从一个对 $L$ 的初始猜测（例如，上一步的值 $L_{k-1}$）开始，我们检查对于从 $y_k$ 计算出的候选点 $x_{k+1}$，以下关于 $f$ 的二次上界是否满足：\n$$\nf(x_{k+1}) \\le f(y_k) + \\langle \\nabla f(y_k), x_{k+1} - y_k \\rangle + \\frac{L_k}{2}\\|x_{k+1} - y_k\\|_{2}^{2}\n$$\n如果条件不满足，我们将 $L_k$ 乘以一个因子 $\\eta > 1$（例如 $\\eta=2$），然后重新计算 $x_{k+1}$，直到条件满足。这确保了目标函数的光滑部分有足够的下降。\n\n#### 2.4. 自适应重启\n\nNesterov加速不是一种下降方法；目标函数 $F(x_k)$ 不保证单调递减。当目标函数值增加时（即 $F(x_{k+1}) > F(x_k)$），这表明动量过于激进，越过了最小值。自适应重启规则通过重置动量来处理这种情况。我们按规定实现该规则：如果 $F(x_{k+1}) > F(x_k)$：\n- 为下一步重置动量标量：$t_{k+1} = 1$。\n- 为下一步重置外推点：$y_{k+1} = x_{k+1}$。\n\n### 3. 完整算法\n\n结合这些组件，我们得到以下算法。\n\n**初始化**：\n- 设置 $k=0$, $x_0 = 0 \\in \\mathbb{R}^n$, $y_0 = x_0$, $t_0=1$。\n- 设置最大迭代次数 $N_{\\max}$，容差 $\\epsilon, \\epsilon'$。\n- 设置回溯因子 $\\eta > 1$。\n- 计算初始Lipschitz估计值 $L_0 = \\|A\\|_2^2$。\n- 设置 $F_{-1} = \\infty$。\n\n**主循环 (对于 $k = 0, 1, \\dots, N_{\\max}-1$)**：\n1.  令 $x_{\\text{prev}} = x_k$，$t_{\\text{prev}} = t_k$。\n2.  **回溯线搜索**：\n    a. 初始化试验值 $L = L_k / \\eta$。\n    b. 重复更新 $L \\leftarrow \\eta L$，直到条件\n       $f(x_{k+1}) \\le f(y_k) + \\langle \\nabla f(y_k), x_{k+1} - y_k \\rangle + \\frac{L}{2}\\|x_{k+1} - y_k\\|_{2}^{2}$\n       满足，其中 $x_{k+1} = \\mathcal{S}_{\\lambda/L}(y_k - \\frac{1}{L}\\nabla f(y_k))$。\n    c. 设置 $L_{k+1} = L$。\n3.  **检查停止准则**：\n    a. 相对变化：$\\delta_x = \\|x_{k+1} - x_k\\|_{2} / \\max\\{1, \\|x_k\\|_{2}\\}$。\n    b. 近端梯度平稳性：$\\|G_L(y_k)\\|_\\infty = \\|L(y_k - x_{k+1})\\|_\\infty$。\n    c. 如果 $\\delta_x \\le \\epsilon$ 并且 $\\|G_L(y_k)\\|_\\infty \\le \\epsilon'$，则终止并返回 $x_{k+1}$。\n4.  **为下一次迭代更新（带重启）**：\n    a. 计算目标函数值 $F_k = F(x_k)$ 和 $F_{k+1} = F(x_{k+1})$。\n    b. **如果** $F_{k+1} > F_k$：\n        i.  $t_{k+1} = 1$。\n        ii. $y_{k+1} = x_{k+1}$。\n    c. **否则**（不重启）：\n        i.  $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$。\n        ii. $y_{k+1} = x_{k+1} + \\frac{t_k-1}{t_{k+1}}(x_{k+1} - x_k)$。\n5.  递增 $k \\leftarrow k+1$。\n\n这就为所要求的算法定义了完整的更新规则集合。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to derive, implement, and test an accelerated proximal gradient method\n    with line search and restart for sparse least-squares problems.\n    \"\"\"\n\n    def fista_with_restart(A, b, lambda_val, max_iter, tol, tol_prox_grad):\n        \"\"\"\n        Implements the FISTA algorithm with backtracking line search and adaptive restart\n        for the LASSO problem: min 0.5*||Ax-b||^2 + lambda*||x||_1.\n\n        Args:\n            A (np.ndarray): The design matrix.\n            b (np.ndarray): The measurement vector.\n            lambda_val (float): The regularization parameter.\n            max_iter (int): Maximum number of iterations.\n            tol (float): Tolerance for relative iterate change.\n            tol_prox_grad (float): Tolerance for the proximal gradient norm.\n\n        Returns:\n            np.ndarray: The optimized solution vector x.\n        \"\"\"\n        m, n = A.shape\n        eta = 2.0\n\n        def f(x_vec, A_mat, b_vec):\n            return 0.5 * np.linalg.norm(A_mat @ x_vec - b_vec)**2\n\n        def g(x_vec, lam):\n            return lam * np.linalg.norm(x_vec, 1)\n\n        def soft_threshold(z, tau):\n            return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n        # Initialization\n        x_curr = np.zeros(n)\n        y_curr = np.zeros(n)\n        t_curr = 1.0\n        \n        # Initial Lipschitz constant estimate from the spectral norm of A.\n        L = np.linalg.norm(A, 2)**2\n\n        # Storing the objective value of the previous iterate for the restart condition.\n        F_x_prev = f(x_curr, A, b) + g(x_curr, lambda_val)\n\n        for k in range(max_iter):\n            x_prev = x_curr\n            t_prev = t_curr\n\n            # Backtracking line search\n            L_trial = L / eta  # Start with a smaller L for efficiency\n            while True:\n                L_trial *= eta\n                grad_y = A.T @ (A @ y_curr - b)\n                z = y_curr - grad_y / L_trial\n                x_curr = soft_threshold(z, lambda_val / L_trial)\n                \n                f_x = f(x_curr, A, b)\n                f_y = f(y_curr, A, b)\n                \n                quadratic_approx = f_y + np.dot(grad_y, x_curr - y_curr) + (L_trial / 2.0) * np.linalg.norm(x_curr - y_curr)**2\n                \n                if f_x = quadratic_approx:\n                    L = L_trial\n                    break\n            \n            # Stopping conditions\n            rel_change = np.linalg.norm(x_curr - x_prev) / max(1.0, np.linalg.norm(x_prev))\n            prox_grad_norm = L * np.linalg.norm(y_curr - x_curr, np.inf)\n\n            if k > 0 and rel_change = tol and prox_grad_norm = tol_prox_grad:\n                break\n            \n            F_x_curr = f_x + g(x_curr, lambda_val)\n            \n            # Restart check and momentum update for the next iteration\n            if k > 0 and F_x_curr > F_x_prev:\n                t_curr = 1.0\n                y_next = x_curr\n            else:\n                t_curr = (1.0 + np.sqrt(1.0 + 4.0 * t_prev**2)) / 2.0\n                y_next = x_curr + ((t_prev - 1.0) / t_curr) * (x_curr - x_prev)\n\n            # Update state for next iteration\n            y_curr = y_next\n            F_x_prev = F_x_curr\n            \n        return x_curr\n\n    # General parameters\n    MAX_ITER = 10000\n    TOL = 1e-8\n    results = []\n    \n    # --- Case A: Compressed sensing \"happy path\" ---\n    m_A, n_A, k_sparse_A = 40, 120, 8\n    rng_A = np.random.default_rng(12345)\n    A_A = rng_A.standard_normal((m_A, n_A)) / np.sqrt(m_A)\n    x_star_A = np.zeros(n_A)\n    support_A = rng_A.choice(n_A, k_sparse_A, replace=False)\n    x_star_A[support_A] = rng_A.standard_normal(k_sparse_A)\n    e_A = rng_A.normal(0, 1e-3, m_A)\n    b_A = A_A @ x_star_A + e_A\n    lambda_A = 0.01 * np.linalg.norm(A_A.T @ b_A, np.inf)\n    \n    x_alg_A = fista_with_restart(A_A, b_A, lambda_A, MAX_ITER, TOL, TOL)\n    rel_err_A = np.linalg.norm(x_alg_A - x_star_A) / max(1.0, np.linalg.norm(x_star_A))\n    results.append(rel_err_A)\n\n    # --- Case B: Boundary condition lambda = 0 ---\n    m_B, n_B = 50, 20\n    rng_B = np.random.default_rng(54321)\n    A_B = rng_B.standard_normal((m_B, n_B)) / np.sqrt(m_B)\n    b_B = rng_B.standard_normal(m_B)\n    lambda_B = 0.0\n    \n    x_alg_B = fista_with_restart(A_B, b_B, lambda_B, MAX_ITER, TOL, TOL)\n    x_ls_B, _, _, _ = np.linalg.lstsq(A_B, b_B, rcond=None)\n    rel_discrepancy_B = np.linalg.norm(x_alg_B - x_ls_B) / max(1.0, np.linalg.norm(x_ls_B))\n    results.append(rel_discrepancy_B)\n\n    # --- Case C: Very strong regularization ---\n    m_C, n_C = 30, 50\n    rng_C = np.random.default_rng(11111)\n    A_C = rng_C.standard_normal((m_C, n_C)) / np.sqrt(m_C)\n    b_C = rng_C.standard_normal(m_C)\n    lambda_C = 100 * np.linalg.norm(A_C.T @ b_C, np.inf)\n    \n    x_alg_C = fista_with_restart(A_C, b_C, lambda_C, MAX_ITER, TOL, TOL)\n    cardinality_C = np.sum(np.abs(x_alg_C) > 1e-8)\n    results.append(cardinality_C)\n\n    # --- Case D: Rank-deficient design matrix ---\n    m_D, n_half_D = 30, 30\n    rng_D = np.random.default_rng(22222)\n    A0_D = rng_D.standard_normal((m_D, n_half_D)) / np.sqrt(m_D)\n    A_D = np.hstack([A0_D, A0_D])\n    b_D = rng_D.standard_normal(m_D)\n    lambda_D = 0.05 * np.linalg.norm(A_D.T @ b_D, np.inf)\n\n    x_alg_D = fista_with_restart(A_D, b_D, lambda_D, MAX_ITER, TOL, TOL)\n    grad_f_D = A_D.T @ (A_D @ x_alg_D - b_D)\n    \n    tol_kkt_zero = 1e-12\n    is_zero = np.abs(x_alg_D) = tol_kkt_zero\n    is_nonzero = ~is_zero\n    \n    res_nonzero = np.abs(grad_f_D[is_nonzero] + lambda_D * np.sign(x_alg_D[is_nonzero]))\n    res_zero = np.maximum(0, np.abs(grad_f_D[is_zero]) - lambda_D)\n    \n    max_res_nonzero = np.max(res_nonzero) if res_nonzero.size > 0 else 0.0\n    max_res_zero = np.max(res_zero) if res_zero.size > 0 else 0.0\n    \n    kkt_residual_D = np.max([max_res_nonzero, max_res_zero])\n    results.append(kkt_residual_D)\n\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]}]\")\n\nsolve()\n```", "id": "3461192"}, {"introduction": "拥有一个可工作的 FISTA 实现后，我们现在可以探索提升其性能的方法。这个高级实践专注于设计和测试比简单的目标函数值增加更复杂的重启准则 [@problem_id:3461157]。通过实现基于原始-对偶间隙和梯度对齐的重启策略，你将研究这些启发式方法如何影响收敛行为，尤其是在稀疏问题中至关重要的支撑集恢复任务上。", "problem": "考虑复合凸优化问题，即最小绝对值收缩和选择算子 (LASSO)：最小化函数 $F(x) \\triangleq f(x) + \\lambda \\lVert x \\rVert_{1}$，其中 $f(x) \\triangleq \\tfrac{1}{2} \\lVert A x - b \\rVert_{2}^{2}$，$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\lambda \\in \\mathbb{R}_{+}$。函数 $f$ 的梯度为 $\\nabla f(x) = A^{\\top} (A x - b)$，它是 Lipschitz 连续的，Lipschitz 常数为 $L \\geq \\lVert A \\rVert_{2}^{2}$。快速迭代收缩阈值算法 (FISTA) 是一种加速的近端梯度法，它使用 Nesterov 加速来对这类复合目标进行非光滑优化。当加速变得有害时，常采用重启机制来恢复快速收敛状态。\n\n您将为 FISTA 设计并测试一种基于 LASSO 原始-对偶间隙的自适应重启策略，并评估在受限等距性质 (RIP) 下，由梯度内积条件触发的额外重启是否与更快的支撑集恢复相关。请纯粹使用数学和算法构造；不涉及任何物理单位。\n\n将使用的定义：\n- 最小绝对值收缩和选择算子 (LASSO)：最小化 $F(x) = \\tfrac{1}{2} \\lVert A x - b \\rVert_{2}^{2} + \\lambda \\lVert x \\rVert_{1}$。\n- 快速迭代收缩阈值算法 (FISTA)：一种用于复合凸最小化问题的加速近端梯度法，使用步长为 $1/L$ 的 $\\ell_{1}$-范数近端算子（软阈值）。\n- 受限等距性质 (RIP)：传感矩阵 $A$ 的一种结构性质，对于稀疏向量 $x$，$A$ 的作用近似保持其 $\\ell_{2}$-范数。您不需要计算 RIP 常数；而是将生成具有通常与近似 RIP 相关属性的矩阵（例如，经列归一化的高斯矩阵），以及一个具有增加的互相干性以近似偏离 RIP 的案例。\n- LASSO 的原始-对偶间隙：将 $f(x) = g(Ax)$，其中 $g(z) = \\tfrac{1}{2} \\lVert z - b \\rVert_{2}^{2}$，以及 $h(x) = \\lambda \\lVert x \\rVert_{1}$。其凸共轭满足 $g^{\\ast}(y) = \\tfrac{1}{2} \\lVert y \\rVert_{2}^{2} + b^{\\top} y$ 和 $h^{\\ast}(s) = \\iota_{\\lVert s \\rVert_{\\infty} \\leq \\lambda}(s)$，其中 $\\iota_{C}$ 是凸集 $C$ 的指示函数。一个对偶可行点 $y$ 必须满足 $\\lVert A^{\\top} y \\rVert_{\\infty} \\leq \\lambda$。给定一个原始迭代点 $x$，可以通过缩放残差 $r(x) \\triangleq A x - b$ 来构造一个对偶可行点 $y(x)$，即 $y(x) \\triangleq \\theta(x) \\, r(x)$，其中 $\\theta(x) \\triangleq \\min\\!\\left\\{ 1, \\, \\dfrac{\\lambda}{\\lVert A^{\\top} r(x) \\rVert_{\\infty}} \\right\\}$。原始-对偶间隙则为 $G(x) \\triangleq \\left(\\tfrac{1}{2} \\lVert A x - b \\rVert_{2}^{2} + \\lambda \\lVert x \\rVert_{1}\\right) - \\left(-\\tfrac{1}{2} \\lVert y(x) \\rVert_{2}^{2} - b^{\\top} y(x)\\right)$，其值为非负。\n\n您的任务：\n- 实现带两种重启触发器的 FISTA：\n  - 基于间隙的自适应重启：如果原始-对偶间隙 $G(x_{k})$ 相对于 $G(x_{k-1})$ 严格增加（允许一个可忽略的数值容差），则通过将加速参数设置为其初始值并将外推点设置为当前迭代点来重置动量。\n  - 梯度内积重启：如果 Nesterov 加速导致错位，表现为条件 $\\langle x_{k} - x_{k-1}, \\nabla f(x_{k}) - \\nabla f(x_{k-1}) \\rangle  0$ 成立，则如上所述重置动量。\n- 为求解每个实例实现两种配置：\n  - 配置 A：仅激活基于间隙的自适应重启。\n  - 配置 B：同时激活基于间隙的重启和基于梯度内积的重启。\n- 将支撑集恢复定义为恢复真实稀疏向量 $x^{\\star}$ 的精确支撑集，即索引集相等 $\\operatorname{supp}(x_{k}) = \\operatorname{supp}(x^{\\star})$，其中 $\\operatorname{supp}(x) \\triangleq \\{ i : |x_{i}|  \\tau \\}$，$\\tau$ 是一个很小的阈值。测量达到首次精确支撑集恢复所需的迭代次数。如果在最大迭代次数 $N_{\\max}$ 内未实现精确恢复，则报告 $N_{\\max} + 1$。\n\n可作为起点的算法基础：\n- 从 $y_{k}$ 计算 $x_{k+1}$ 的近端梯度步，步长为 $1/L$：$x_{k+1} = \\operatorname{soft}(y_{k} - \\tfrac{1}{L} \\nabla f(y_{k}), \\tfrac{\\lambda}{L})$，其中 $\\operatorname{soft}(v, \\tau)$ 逐元素应用软阈值操作 $\\operatorname{soft}(v_{i}, \\tau) \\triangleq \\operatorname{sign}(v_{i}) \\max\\{ |v_{i}| - \\tau, 0 \\}$。\n- 采用标准 FISTA 参数化的 Nesterov 外推更新：$t_{k+1} = \\tfrac{1 + \\sqrt{1 + 4 t_{k}^{2}}}{2}$ 和 $y_{k+1} = x_{k+1} + \\tfrac{t_{k} - 1}{t_{k+1}} (x_{k+1} - x_{k})$，除非触发重启，此时设置 $t_{k+1} = 1$ 和 $y_{k+1} = x_{k+1}$。\n- Lipschitz 常数选择：使用 $L = \\lVert A \\rVert_{2}^{2}$，其中 $\\lVert A \\rVert_{2}$ 是谱范数。\n\n测试套件：\n实现以下三个确定性测试实例。对每个实例，生成传感矩阵 $A$、稀疏真实解 $x^{\\star}$ 和带噪观测值 $b = A x^{\\star} + e$，其信噪比（以分贝计）通过 $20 \\log_{10} \\left( \\dfrac{\\lVert A x^{\\star} \\rVert_{2}}{\\lVert e \\rVert_{2}} \\right)$ 计算。在所有情况下，将 $A$ 的列归一化为单位 $\\ell_{2}$-范数。用指定的 $\\alpha \\in (0,1)$ 定义 $\\lambda = \\alpha \\, \\lVert A^{\\top} b \\rVert_{\\infty}$。\n\n- 案例 1 (近似 RIP，中等难度)：\n  - 维度: $m = 80$, $n = 200$。\n  - 稀疏度: $s = 10$。\n  - 噪声: 信噪比 $40$ dB。\n  - 正则化: $\\alpha = 0.10$。\n  - 随机种子: $0$。\n  - 矩阵模型: 方差为 $1/m$ 的独立高斯条目，然后进行列归一化。\n\n- 案例 2 (增加的互相干性，更难，近似偏离 RIP)：\n  - 维度: $m = 60$, $n = 200$。\n  - 稀疏度: $s = 20$。\n  - 噪声: 信噪比 $30$ dB。\n  - 正则化: $\\alpha = 0.05$。\n  - 随机种子: $1$。\n  - 矩阵模型: 从方差为 $1/m$ 的独立高斯条目开始，然后对于列 $j \\geq 1$，设置 $A_{\\cdot j} \\leftarrow \\operatorname{normalize}(A_{\\cdot j} + 0.5 \\, c)$，其中 $c$ 是相关化前的第一列，然后重新归一化所有列。\n\n- 案例 3 (更高的系统，更容易，强近似 RIP)：\n  - 维度: $m = 120$, $n = 300$。\n  - 稀疏度: $s = 20$。\n  - 噪声: 信噪比 $50$ dB。\n  - 正则化: $\\alpha = 0.10$。\n  - 随机种子: $2$。\n  - 矩阵模型: 方差为 $1/m$ 的独立高斯条目，然后进行列归一化。\n\n执行细节：\n- 使用支撑集阈值 $\\tau = 10^{-6}$ 和最大迭代次数 $N_{\\max} = 2000$。\n- 将 $x_{0}$ 初始化为零向量，且 $t_{0} = 1$。\n- 在比较连续的原始-对偶间隙以判断是否增加时，使用数值容差 $\\varepsilon = 10^{-12}$。\n- 对于每个测试案例，运行配置 A 和配置 B，并记录达到精确支撑集恢复时的迭代次数，分别记为 $K_{A}$ 和 $K_{B}$。\n- 对于每个测试案例，如果 $K_{B} \\leq K_{A}$，则输出布尔值 $\\mathrm{True}$，否则输出 $\\mathrm{False}$。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$，分别对应于案例 1、2 和 3 的结果。", "solution": "用户要求实现并评估一种应用于 LASSO 问题的快速迭代收缩阈值算法 (FISTA) 的自适应重启策略。解决方案将涉及生成测试案例，实现具有两种不同重启配置的 FISTA，并在支撑集恢复速度方面比较它们的性能。\n\n### 1. 问题表述与预备知识\n\n优化问题是 LASSO：\n$$ \\min_{x \\in \\mathbb{R}^n} F(x) \\triangleq \\underbrace{\\frac{1}{2} \\lVert Ax - b \\rVert_2^2}_{f(x)} + \\underbrace{\\lambda \\lVert x \\rVert_1}_{h(x)} $$\n其中 $f(x)$ 是一个光滑凸函数，$h(x)$ 是一个非光滑凸函数。$f(x)$ 的梯度是 $\\nabla f(x) = A^\\top(Ax-b)$，它是 Lipschitz 连续的，常数为 $L$。我们将使用 $L = \\lVert A \\rVert_2^2$，即 $A$ 的谱范数的平方。\n\nFISTA 算法通过从 $x_0 = \\mathbf{0}$、$y_0 = x_0$ 和 $t_0 = 1$ 开始生成一个迭代序列 $(x_k, y_k, t_k)$ 来解决此问题。对于迭代 $k \\geq 0$，更新规则如下：\n1.  **近端梯度步**：从外推点 $y_k$ 计算下一个迭代点 $x_{k+1}$。\n    $$ x_{k+1} = \\operatorname{prox}_{h/L}(y_k - \\frac{1}{L}\\nabla f(y_k)) $$\n    对于 $h(x) = \\lambda \\lVert x \\rVert_1$，近端算子是软阈值函数：\n    $$ x_{k+1} = \\operatorname{soft}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - b), \\frac{\\lambda}{L}\\right) $$\n    其中 $\\operatorname{soft}(v, \\tau)_i = \\operatorname{sign}(v_i)\\max(|v_i|-\\tau, 0)$。\n\n2.  **重启检查**：计算出 $x_{k+1}$ 后，我们检查是否有必要重启。如果任一激活条件满足，则触发重启。使用当前迭代点 $x_{k+1}$ 和前一个迭代点 $x_k$ 来检查这些条件。问题描述中泛指地使用索引 $(k, k-1)$ 来表示连续的迭代点，我们将其在标准自适应 FISTA 循环的上下文中进行解释。\n    *   **基于间隙的条件**：原始-对偶间隙增加。$G(x_{k+1})  G(x_k) + \\varepsilon$。\n    *   **基于梯度的条件**：迭代方向的变化与梯度方向的变化一致。$\\langle x_{k+1} - x_k, \\nabla f(x_{k+1}) - \\nabla f(x_k) \\rangle  0$。\n\n3.  **外推步**：根据重启检查的结果，我们计算下一个动量参数 $t_{k+1}$ 和下一个外推点 $y_{k+1}$。\n    *   **如果重启**：重置动量。\n        $$ t_{k+1} = 1 $$\n        $$ y_{k+1} = x_{k+1} $$\n    *   **如果不重启**：应用 Nesterov 加速。\n        $$ t_{k+1} = \\frac{1 + \\sqrt{1 + 4 t_k^2}}{2} $$\n        $$ y_{k+1} = x_{k+1} + \\frac{t_k - 1}{t_{k+1}}(x_{k+1} - x_k) $$\n\n### 2. 原始-对偶间隙计算\n\n原始-对偶间隙提供了次优性的一个证明。对于一个原始迭代点 $x$，我们构造一个对偶可行变量 $y(x)$：\n$$ y(x) = \\theta(x) (Ax - b) \\quad \\text{with} \\quad \\theta(x) = \\min\\left\\{1, \\frac{\\lambda}{\\lVert A^\\top(Ax-b) \\rVert_\\infty}\\right\\} $$\n这种构造确保了与 $y(x)$ 相关联的对偶变量 $s(x) = -A^\\top y(x)$ 满足对偶可行性条件 $\\lVert s(x) \\rVert_\\infty \\le \\lambda$。\n原始目标是 $P(x) = \\frac{1}{2}\\lVert Ax-b \\rVert_2^2 + \\lambda\\lVert x \\rVert_1$。\n在 $y(x)$ 处求值的对偶目标是 $D(y(x)) = -\\frac{1}{2}\\lVert y(x) \\rVert_2^2 - b^\\top y(x)$。\n间隙是非负量 $G(x) = P(x) - D(y(x))$。\n\n### 3. 测试数据生成\n\n对于每个测试案例，我们根据规格构造矩阵 $A$、稀疏真实解向量 $x^\\star$ 和观测向量 $b$。\n1.  为保证可复现性，对随机数生成器设置种子。\n2.  矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 根据指定的模型（高斯或相关高斯）生成。对于相关情况（案例 2），我们首先生成一个基础高斯矩阵 $A_{\\text{base}}$。$A_{\\text{base}}$ 的第一列成为相关向量 $c$。对于其他每一列 $j$，我们将 $0.5c$ 加到 $A_{\\text{base}}$ 的第 $j$ 列。最后，将所得矩阵的所有列归一化为单位 $\\ell_2$-范数。对于标准高斯情况，我们生成一个独立同分布的高斯矩阵，然后对其列进行归一化。\n3.  真实稀疏向量 $x^\\star \\in \\mathbb{R}^n$ 在随机选择的位置上创建 $s$ 个非零项。非零值从标准正态分布中抽取。\n4.  观测向量 $b$ 计算为 $b = Ax^\\star + e$。噪声向量 $e$ 是一个高斯向量，其范数 $\\lVert e \\rVert_2$ 被缩放以达到期望的信噪比 (SNR) (以 dB 为单位)，定义为 $\\text{SNR} = 20 \\log_{10}(\\lVert Ax^\\star \\rVert_2 / \\lVert e \\rVert_2)$。因此，$\\lVert e \\rVert_2 = \\lVert Ax^\\star \\rVert_2 \\cdot 10^{-\\text{SNR}/20}$。\n5.  正则化参数 $\\lambda$ 通过 $\\lambda = \\alpha \\lVert A^\\top b \\rVert_\\infty$ 相对于数据进行设置。这是一种常见的启发式方法，以确保 $\\lambda$ 处于一个有意义的范围内。\n\n### 4. 算法实现与评估\n\n解决方案的核心是一个实现了带有可配置重启功能的 FISTA 算法的函数。该函数接收问题数据 $(A, b, \\lambda, L)$ 和配置参数（使用哪些重启条件）。它会一直迭代，直到当前迭代点 $x_k$ 的支撑集与真实解 $x^\\star$ 的支撑集匹配，或达到最大迭代次数 $N_{\\max}$。一个迭代点的支撑集定义为 $\\operatorname{supp}(x_k) = \\{ i : |x_{k,i}|  \\tau \\}$，其中 $\\tau = 10^{-6}$。\n\n主脚本将为三个测试案例中的每一个执行以下操作：\n1.  生成数据 $(A, b, x^\\star, \\lambda, L)$。\n2.  使用配置 A（仅基于间隙的重启）运行 FISTA，并记录支撑集恢复的迭代次数 $K_A$。\n3.  使用配置 B（同时使用基于间隙和基于梯度的重启）运行 FISTA，并记录迭代次数 $K_B$。\n4.  比较结果并确定是否 $K_B \\leq K_A$。\n\n最终输出将是一个布尔值列表，对应于每个测试案例的比较结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the complete test suite for FISTA with adaptive restarts.\n    \"\"\"\n    test_cases = [\n        # Case 1: Approximate RIP, moderate difficulty\n        {\n            \"m\": 80, \"n\": 200, \"s\": 10, \"snr_db\": 40, \"alpha\": 0.10, \"seed\": 0,\n            \"matrix_model\": \"gaussian\",\n        },\n        # Case 2: Increased mutual coherence, harder\n        {\n            \"m\": 60, \"n\": 200, \"s\": 20, \"snr_db\": 30, \"alpha\": 0.05, \"seed\": 1,\n            \"matrix_model\": \"correlated_gaussian\",\n        },\n        # Case 3: Taller system, easier\n        {\n            \"m\": 120, \"n\": 300, \"s\": 20, \"snr_db\": 50, \"alpha\": 0.10, \"seed\": 2,\n            \"matrix_model\": \"gaussian\"\n        },\n    ]\n\n    results = []\n    for case_params in test_cases:\n        data = generate_case_data(**case_params)\n\n        # Config A: Gap restart only\n        k_a = run_fista(data, use_gap_restart=True, use_grad_restart=False)\n\n        # Config B: Both restarts active\n        k_b = run_fista(data, use_gap_restart=True, use_grad_restart=True)\n\n        results.append(k_b = k_a)\n\n    # The boolean values need to be capitalized for the desired output format\n    py_bool_to_string = {True: 'True', False: 'False'}\n    formatted_results = [py_bool_to_string[res] for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef generate_case_data(m, n, s, snr_db, alpha, seed, matrix_model):\n    \"\"\"\n    Generates a deterministic test case for the LASSO problem.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate sensing matrix A\n    if matrix_model == \"gaussian\":\n        A = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n    elif matrix_model == \"correlated_gaussian\":\n        A_base = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n        c = A_base[:, 0].copy()\n        A = A_base.copy()\n        for j in range(1, n):\n            A[:, j] += 0.5 * c\n    else:\n        raise ValueError(\"Unknown matrix model\")\n    \n    A /= np.linalg.norm(A, axis=0)\n\n    # Generate sparse ground-truth vector x_star\n    x_star = np.zeros(n)\n    support_indices = rng.choice(n, s, replace=False)\n    x_star[support_indices] = rng.standard_normal(s)\n    x_star_support = set(support_indices)\n\n    # Generate observation vector b\n    Ax_star = A @ x_star\n    signal_norm = np.linalg.norm(Ax_star)\n    noise_norm = signal_norm / (10**(snr_db / 20.0))\n\n    e = rng.standard_normal(m)\n    e *= noise_norm / np.linalg.norm(e)\n    b = Ax_star + e\n\n    # Calculate regularization parameter lambda\n    lambda_ = alpha * np.linalg.norm(A.T @ b, ord=np.inf)\n\n    # Calculate Lipschitz constant L\n    L = np.linalg.norm(A, 2)**2\n\n    return {\n        \"A\": A, \"b\": b, \"lambda_\": lambda_, \"L\": L,\n        \"x_star_support\": x_star_support\n    }\n\ndef soft_threshold(v, tau):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(v) * np.maximum(np.abs(v) - tau, 0)\n\ndef calculate_primal_dual_gap(A, b, lambda_, x):\n    \"\"\"Calculates the primal-dual gap for a LASSO iterate x.\"\"\"\n    residual = A @ x - b\n    primal_obj = 0.5 * np.dot(residual, residual) + lambda_ * np.linalg.norm(x, 1)\n\n    At_residual = A.T @ residual\n    At_residual_inf_norm = np.linalg.norm(At_residual, ord=np.inf)\n    \n    theta = 1.0\n    if At_residual_inf_norm > lambda_:\n        theta = lambda_ / At_residual_inf_norm\n    \n    dual_var = theta * residual\n    dual_obj = -0.5 * np.dot(dual_var, dual_var) - np.dot(b, dual_var)\n    \n    return primal_obj - dual_obj\n\n\ndef run_fista(data, use_gap_restart, use_grad_restart):\n    \"\"\"\n    Runs FISTA with specified restart configuration.\n    \"\"\"\n    A, b, lambda_, L = data[\"A\"], data[\"b\"], data[\"lambda_\"], data[\"L\"]\n    x_star_support = data[\"x_star_support\"]\n    n = A.shape[1]\n    \n    N_MAX, SUPPORT_THRESH, GAP_TOL = 2000, 1e-6, 1e-12\n\n    x_k = np.zeros(n)\n    y_k = np.zeros(n)\n    t_k = 1.0\n    grad_fx_k = A.T @ (A @ x_k - b)\n    gap_k = calculate_primal_dual_gap(A, b, lambda_, x_k)\n\n    for iter_num in range(N_MAX):\n        # Proximal gradient step from extrapolated point y_k\n        grad_fy_k = A.T @ (A @ y_k - b)\n        x_kp1 = soft_threshold(y_k - grad_fy_k / L, lambda_ / L)\n\n        # Check for exact support recovery\n        current_support = {i for i, val in enumerate(x_kp1) if abs(val) > SUPPORT_THRESH}\n        if current_support == x_star_support:\n            return iter_num + 1\n\n        # Check restart conditions based on x_kp1 vs x_k\n        restart_triggered = False\n        grad_fx_kp1 = A.T @ (A @ x_kp1 - b)\n\n        if use_gap_restart:\n            gap_kp1 = calculate_primal_dual_gap(A, b, lambda_, x_kp1)\n            if gap_kp1 > gap_k + GAP_TOL:\n                restart_triggered = True\n        \n        if use_grad_restart and not restart_triggered:\n            if np.dot(x_kp1 - x_k, grad_fx_kp1 - grad_fx_k) > 0:\n                restart_triggered = True\n\n        # Update momentum based on restart check\n        if restart_triggered:\n            t_kp1 = 1.0\n        else:\n            t_kp1 = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n        \n        # Extrapolate for the next iteration\n        y_kp1 = x_kp1 + ((t_k - 1.0) / t_kp1) * (x_kp1 - x_k)\n\n        # Update state for next iteration\n        x_k = x_kp1\n        y_k = y_kp1\n        t_k = t_kp1\n        grad_fx_k = grad_fx_kp1\n        if use_gap_restart:\n            gap_k = gap_kp1\n\n    return N_MAX + 1\n\n# Execute the main function when the script is run\nsolve()\n```", "id": "3461157"}]}