{"hands_on_practices": [{"introduction": "SCAD 和 MCP 惩罚项优化算法的核心在于它们各自对应的阈值函数（或称近端算子）。这个基础练习将引导你对一个具体的数值案例进行直接计算，帮助你建立对这些分段函数结构的直观理解，并观察它们在收缩信号时的具体差异。[@problem_id:3462713]", "problem": "考虑稀疏估计中惩罚最小二乘法的单坐标近端更新，其中估计量 $T(z)$ 最小化关于 $b \\in \\mathbb{R}$ 的单变量目标函数 $F(b;z) = \\frac{1}{2}(b - z)^{2} + P(|b|)$。惩罚项 $P$ 是平滑削波绝对偏差 (SCAD) 惩罚或极小极大凹惩罚 (MCP)，两者均由正则化强度 $\\lambda  0$ 和一个形状参数进行参数化。在标准的坐标下降框架下进行计算，其中可微分支的最优性条件为 $b - z + \\partial P(b) = 0$，将 $\\partial P(b)$ 解释为 $P$ 在 $b \\ge 0$ 上关于 $b$ 的导数，并利用 $z$ 符号的对称性。\n\n在非负半轴上使用以下惩罚项导数的基本定义：\n- 对于参数为 $(\\lambda,a)$ 且 $a  2$ 的 SCAD，对 $t \\ge 0$ 定义 $P_{\\text{SCAD}}(t)$ 的导数 $p'_{\\text{SCAD}}(t)$ 为\n$$\np'_{\\text{SCAD}}(t) = \n\\begin{cases}\n\\lambda,  0 \\le t \\le \\lambda, \\\\\n\\dfrac{a\\lambda - t}{a - 1},  \\lambda  t \\le a\\lambda, \\\\\n0,  t > a\\lambda,\n\\end{cases}\n$$\n并通过 $b$ 中次梯度的奇对称性将其扩展到 $b  0$。\n- 对于参数为 $(\\lambda,\\gamma)$ 且 $\\gamma  1$ 的 MCP，对 $t \\ge 0$ 定义 $P_{\\text{MCP}}(t)$ 的导数 $p'_{\\text{MCP}}(t)$ 为\n$$\np'_{\\text{MCP}}(t) = \n\\begin{cases}\n\\lambda\\left(1 - \\dfrac{t}{\\gamma\\lambda}\\right),  0 \\le t \\le \\gamma\\lambda, \\\\\n0,  t > \\gamma\\lambda,\n\\end{cases}\n$$\n并通过 $b$ 中次梯度的奇对称性将其扩展到 $b  0$。\n\n取数值实例 $z = 3.2$，$\\lambda = 1.0$，$a = 3.7$，以及 $\\gamma = 2.5$。显式计算 SCAD 阈值 $T_{\\text{SCAD}}(z)$ 和 MCP 阈值 $T_{\\text{MCP}}(z)$，它们被理解为在相应惩罚项下 $\\frac{1}{2}(b - z)^{2} + P(|b|)$ 的最小化子。然后，假设真实的潜在系数为零，通过计算 $\\left(T_{\\text{SCAD}}(z) - 0\\right)^{2}$ 和 $\\left(T_{\\text{MCP}}(z) - 0\\right)^{2}$ 在平方损失下比较这两个估计量。给出差值\n$$\n\\Delta = \\left(T_{\\text{MCP}}(z)\\right)^{2} - \\left(T_{\\text{SCAD}}(z)\\right)^{2}\n$$\n的值。\n\n你的最终答案必须是包含 $T_{\\text{SCAD}}(z)$、$T_{\\text{MCP}}(z)$ 和 $\\Delta$ 这三个量的行矩阵，按此顺序排列，并写为精确值，不要四舍五入。", "solution": "该问题要求计算平滑削波绝对偏差 (SCAD) 和极小极大凹惩罚 (MCP) 惩罚项的阈值函数，并随后在一个特定情况下评估这些函数及其在平方损失下的差异。\n\n估计量 $T(z)$ 被定义为单变量目标函数的最小化子：\n$$F(b;z) = \\frac{1}{2}(b - z)^{2} + P(|b|)$$\n最小化子 $b$ 是惩罚项 $P(|\\cdot|)$ 在 $z$ 处的近端算子。由于惩罚项 $P(|b|)$ 和二次项 $(b-z)^2$ 的对称性，解 $b = T(z)$ 将与 $z$ 有相同的符号。给定 $z = 3.2 > 0$，我们可以将对最小化子的搜索限制在 $b \\ge 0$ 的范围内。对于 $b \\ge 0$，目标函数变为 $F(b;z) = \\frac{1}{2}(b - z)^{2} + P(b)$（因为 $|b|=b$）。\n\n在 $b > 0$ 处取最小值的的一阶必要条件是 $F(b;z)$ 关于 $b$ 的导数为零。此条件给出为 $b-z+p'(b) = 0$，可以重排为 $z = b + p'(b)$，其中 $p'(b)$ 是惩罚函数 $P(t)$ 在 $t=b$ 处的导数。为了使 $b=0$ 成为解，我们必须满足次梯度最优性条件 $0 \\in \\partial F(0;z)$，即 $z \\in \\partial P(|b|)|_{b=0}$。对于 SCAD 和 MCP，在原点的次梯度都是区间 $[-\\lambda, \\lambda]$。因此，对于 $z \\in [0, \\lambda]$，解为 $b=0$。\n\n我们给定了数值 $z = 3.2$，$\\lambda = 1.0$，SCAD 的 $a = 3.7$，以及 MCP 的 $\\gamma = 2.5$。\n\n首先，我们计算 SCAD 阈值 $T_{\\text{SCAD}}(z)$。\nSCAD 惩罚项在 $t \\ge 0$ 上的导数如下：\n$$\np'_{\\text{SCAD}}(t) = \n\\begin{cases}\n\\lambda,  0 \\le t \\le \\lambda, \\\\\n\\dfrac{a\\lambda - t}{a - 1},  \\lambda  t \\le a\\lambda, \\\\\n0,  t > a\\lambda.\n\\end{cases}\n$$\n我们基于方程 $z = b + p'_{\\text{SCAD}}(b)$ 对 $z > \\lambda$ 的解 $b$ 进行分析。\n1.  如果解 $b$ 位于 $(0, \\lambda]$，那么 $p'_{\\text{SCAD}}(b) = \\lambda$，因此 $z = b + \\lambda$，解为 $b = z-\\lambda$。此解成立的条件是 $0  z-\\lambda \\le \\lambda$，即 $\\lambda  z \\le 2\\lambda$。\n2.  如果解 $b$ 位于 $(\\lambda, a\\lambda]$，我们使用 $p'_{\\text{SCAD}}(b)$ 的第二种情况：\n    $z = b + \\dfrac{a\\lambda - b}{a - 1} = \\dfrac{b(a-1) + a\\lambda - b}{a - 1} = \\dfrac{b(a-2) + a\\lambda}{a-1}$。\n    解出 $b$：$z(a-1) = b(a-2) + a\\lambda$，得到 $b = \\dfrac{z(a-1) - a\\lambda}{a-2}$。此解成立的条件是 $\\lambda  b \\le a\\lambda$，即 $2\\lambda  z \\le a\\lambda$。\n3.  如果解 $b$ 大于 $a\\lambda$，我们有 $p'_{\\text{SCAD}}(b) = 0$，所以 $z=b$。这对 $z > a\\lambda$ 有效。\n\n使用给定的值，$\\lambda=1.0$ 和 $a=3.7$，对于 $z$ 的临界点是 $2\\lambda = 2.0$ 和 $a\\lambda = 3.7$。给定的值 $z=3.2$ 满足 $2\\lambda  z \\le a\\lambda$（即 $2.0  3.2 \\le 3.7$）。因此，我们使用情况 2 的公式：\n$$T_{\\text{SCAD}}(z) = b = \\frac{z(a-1) - a\\lambda}{a-2}$$\n代入数值：\n$$T_{\\text{SCAD}}(3.2) = \\frac{3.2(3.7-1) - 3.7(1.0)}{3.7-2} = \\frac{3.2(2.7) - 3.7}{1.7} = \\frac{8.64 - 3.7}{1.7} = \\frac{4.94}{1.7}$$\n为了将其表示为精确分数：\n$$T_{\\text{SCAD}}(3.2) = \\frac{4.94}{1.7} = \\frac{494}{170} = \\frac{247}{85}$$\n\n接下来，我们计算 MCP 阈值 $T_{\\text{MCP}}(z)$。\nMCP 惩罚项在 $t \\ge 0$ 上的导数如下：\n$$\np'_{\\text{MCP}}(t) = \n\\begin{cases}\n\\lambda\\left(1 - \\dfrac{t}{\\gamma\\lambda}\\right),  0 \\le t \\le \\gamma\\lambda, \\\\\n0,  t > \\gamma\\lambda.\n\\end{cases}\n$$\n我们从 $z = b + p'_{\\text{MCP}}(b)$ 对 $z > \\lambda$ 的解 $b$ 进行分析。\n1.  如果解 $b$ 位于 $(0, \\gamma\\lambda]$，我们使用 $p'_{\\text{MCP}}(b)$ 的第一种情况：\n    $z = b + \\lambda\\left(1 - \\dfrac{b}{\\gamma\\lambda}\\right) = b + \\lambda - \\dfrac{b}{\\gamma} = b\\left(1 - \\dfrac{1}{\\gamma}\\right) + \\lambda$。\n    解出 $b$：$b\\left(\\dfrac{\\gamma-1}{\\gamma}\\right) = z-\\lambda$，得到 $b = \\dfrac{\\gamma}{\\gamma-1}(z-\\lambda)$。此解成立的条件是 $0  b \\le \\gamma\\lambda$，即 $\\lambda  z \\le \\gamma\\lambda$。\n2.  如果解 $b$ 大于 $\\gamma\\lambda$，我们有 $p'_{\\text{MCP}}(b) = 0$，所以 $z=b$。这对 $z > \\gamma\\lambda$ 有效。\n\n使用给定的值，$\\lambda=1.0$ 和 $\\gamma=2.5$，对于 $z$ 的临界点是 $\\gamma\\lambda = 2.5$。给定的值 $z=3.2$ 满足 $z > \\gamma\\lambda$（即 $3.2 > 2.5$）。因此，我们使用情况 2 的公式：\n$$T_{\\text{MCP}}(z) = z$$\n代入数值 $z=3.2$：\n$$T_{\\text{MCP}}(3.2) = 3.2 = \\frac{32}{10} = \\frac{16}{5}$$\n\n最后，我们计算估计量的平方差 $\\Delta$：\n$$\\Delta = \\left(T_{\\text{MCP}}(z)\\right)^{2} - \\left(T_{\\text{SCAD}}(z)\\right)^{2}$$\n代入计算出的值：\n$$\\Delta = \\left(\\frac{16}{5}\\right)^{2} - \\left(\\frac{247}{85}\\right)^{2}$$\n为了简化计算，我们找一个公分母。因为 $85 = 5 \\times 17$，我们可以写成 $\\frac{16}{5} = \\frac{16 \\times 17}{5 \\times 17} = \\frac{272}{85}$。\n$$\\Delta = \\left(\\frac{272}{85}\\right)^{2} - \\left(\\frac{247}{85}\\right)^{2} = \\frac{272^{2} - 247^{2}}{85^{2}}$$\n使用平方差公式 $x^2 - y^2 = (x-y)(x+y)$：\n$$\\Delta = \\frac{(272 - 247)(272 + 247)}{85^{2}} = \\frac{(25)(519)}{85^{2}}$$\n因为 $85^2 = (5 \\times 17)^2 = 25 \\times 17^2 = 25 \\times 289$：\n$$\\Delta = \\frac{25 \\times 519}{25 \\times 289} = \\frac{519}{289}$$\n分数 $\\frac{519}{289}$ 是不可约的，因为 $289 = 17^2$ 且 $519 = 17 \\times 30 + 9$，所以 $519$ 不能被 $17$ 整除。\n\n所求的三个量是 $T_{\\text{SCAD}}(z) = \\frac{247}{85}$，$T_{\\text{MCP}}(z) = \\frac{16}{5}$ 和 $\\Delta = \\frac{519}{289}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{247}{85}  \\frac{16}{5}  \\frac{519}{289}\n\\end{pmatrix}\n}\n$$", "id": "3462713"}, {"introduction": "使用非凸惩罚项的一个主要动机是为了减少像 LASSO 这样的凸方法所带来的估计偏差。这个练习将重点从计算转向理论，要求你推导出 MCP 惩罚项在何种条件下能够对大信号实现无偏估计，同时又能有效地消除由噪声引起的小系数。通过这个练习，你将揭示 MCP 作为一种强大的稀疏恢复工具其背后的理论基础。[@problem_id:3462708]", "problem": "考虑以下稀疏估计模型。令 $x^{\\star} \\in \\mathbb{R}^{n}$ 为一个 $s$-稀疏向量，其支撑集为 $S \\subset \\{1,\\dots,n\\}$，最小非零幅值为 $x_{\\min} := \\min_{i \\in S} |x^{\\star}_{i}|  0$。观测值为 $y = x^{\\star} + w$，其中 $\\|w\\|_{\\infty} \\le \\sigma$，噪声界 $\\sigma \\ge 0$ 为已知。考虑估计量\n$$\n\\hat{x} \\in \\arg\\min_{b \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|y - b\\|_{2}^{2} + \\sum_{i=1}^{n} p_{\\lambda,\\gamma}(|b_{i}|) \\right\\},\n$$\n其中 $p_{\\lambda,\\gamma}$ 是极小化凹惩罚 (Minimax Concave Penalty, MCP)，对于 $\\lambda  0$ 和 $\\gamma  1$ 定义为\n$$\np_{\\lambda,\\gamma}(t) \\;:=\\; \\lambda \\int_{0}^{t} \\left(1 - \\frac{s}{\\gamma \\lambda}\\right)_{+} \\, ds, \\quad t \\ge 0,\n$$\n且 $(u)_{+} := \\max\\{u,0\\}$。在全文中，将坐标的无偏性解释为没有正则化引起的收缩，即按坐标的解等于相应的数据值 $y_{i}$ 的条件（在无噪声极限下，这与 $x^{\\star}_{i}$ 一致）。\n\n从给定的定义和上述可分离问题的一阶最优性条件出发，推导 $(\\lambda,\\gamma)$ 平面上的一个安全区域，该区域能同时保证以下两点：\n- 对于每个满足 $|x^{\\star}_{i}| = x_{\\min}$ 的 $i \\in S$，在最坏情况下的可容许噪声下，唯一的按坐标的极小化子在上述意义下是无偏的；\n- 对于每个 $j \\notin S$，在最坏情况下的可容许噪声下，唯一的按坐标的极小化子恰好为零。\n\n然后，通过考虑一个幅值为 $x_{\\mathrm{nt}} := \\gamma \\lambda + \\varepsilon$（其中 $\\varepsilon  0$）的假设项，来分析一个近阈值的非零系数，并在相同的最坏情况噪声模型下，确定该项保持无偏时 $\\varepsilon$ 所需满足的充要条件。\n\n最后，以闭式形式表示使所推导的 $(\\lambda,\\gamma)$ 安全区域为非空的 $\\gamma$ 的上确界值（作为 $x_{\\min}$ 和 $\\sigma$ 的函数）。将此上确界作为你的最终答案，以单一解析表达式的形式给出。", "solution": "该问题要求对一个基于极小化凹惩罚 (MCP) 的稀疏估计量进行分析。该过程涉及推导可靠估计的条件，分析近阈值情景，并找出一个惩罚参数的上确界值。\n\n首先，我们必须刻画该估计量。优化问题为\n$$\n\\hat{x} \\in \\arg\\min_{b \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|y - b\\|_{2}^{2} + \\sum_{i=1}^{n} p_{\\lambda,\\gamma}(|b_{i}|) \\right\\}\n$$\n该问题是可分离的，意味着我们可以通过极小化下式来独立求解每个坐标 $\\hat{x}_i$\n$$\nL_i(b_i) = \\frac{1}{2} (y_i - b_i)^2 + p_{\\lambda,\\gamma}(|b_i|)\n$$\n对于 $t \\ge 0$，MCP 惩罚由 $p_{\\lambda,\\gamma}(t) = \\lambda \\int_{0}^{t} \\left(1 - \\frac{s}{\\gamma \\lambda}\\right)_{+} ds$ 给出。其导数 $p'_{\\lambda,\\gamma}(t) = \\lambda \\left(1 - \\frac{t}{\\gamma \\lambda}\\right)_{+}$，对于 $t \\ge 0$ 成立，这是求解一阶最优性条件所必需的。具体来说，\n$$\np'_{\\lambda,\\gamma}(t) = \\begin{cases} \\lambda - \\frac{t}{\\gamma}  \\text{若 } 0 \\le t \\le \\gamma\\lambda \\\\ 0  \\text{若 } t > \\gamma\\lambda \\end{cases}\n$$\n$L_i(b_i)$ 的极小化子 $\\hat{b}_i$ 的一阶条件由次梯度包含关系 $0 \\in \\hat{b}_i - y_i + \\partial_t [p_{\\lambda,\\gamma}(|t|)]|_{t=\\hat{b}_i}$ 给出。求解这些条件可得到 MCP 阈值函数 $\\hat{b}_i = T_{\\lambda,\\gamma}(y_i)$，其形式为\n$$\nT_{\\lambda,\\gamma}(y_i) = \\begin{cases} 0  \\text{若 } |y_i| \\le \\lambda \\\\ \\mathrm{sgn}(y_i) \\frac{\\gamma(|y_i|-\\lambda)}{\\gamma-1}  \\text{若 } \\lambda  |y_i| \\le \\gamma\\lambda \\\\ y_i  \\text{若 } |y_i| > \\gamma\\lambda \\end{cases}\n$$\n对于 $\\gamma > 1$，该函数刻画了唯一的按坐标的极小化子。\n\n现在，我们来推导 $(\\lambda, \\gamma)$ 平面上的安全区域。该区域必须在最坏情况噪声模型 $\\|w\\|_{\\infty} \\le \\sigma$ 下同时满足两个条件。\n\n1.  **支撑集上的无偏性**：对于任意满足 $|x^{\\star}_i| = x_{\\min}$ 的 $i \\in S$，估计量必须是无偏的，即 $\\hat{x}_i = y_i$。根据阈值函数，这当且仅当 $|y_i| > \\gamma\\lambda$ 时发生。“最坏情况下的可容许噪声”这一术语意味着，即使在使该条件最难满足的噪声实现下，该条件也必须成立。这里的挑战是保持 $|y_i|$ 较大。$|y_i| = |x^{\\star}_i + w_i|$ 的值在噪声 $w_i$ 与信号 $x^{\\star}_i$ 方向相反时最小化。最小值为 $\\max(0, |x^{\\star}_i|-\\sigma)$。在我们的情况下，即为 $\\max(0, x_{\\min}-\\sigma)$。因此，我们需要 $\\max(0, x_{\\min}-\\sigma) > \\gamma\\lambda$。这个不等式蕴含了两个条件：首先，$x_{\\min} > \\sigma$，其次，$x_{\\min} - \\sigma > \\gamma\\lambda$。所以安全区域的第一个条件是：\n    $$\n    \\gamma\\lambda  x_{\\min} - \\sigma\n    $$\n\n2.  **支撑集外的稀疏性**：对于任意 $j \\notin S$，估计量必须为零，即 $\\hat{x}_j=0$。根据阈值函数，这当且仅当 $|y_j| \\le \\lambda$ 时发生。对于这样的索引 $j$，$x^{\\star}_j=0$，因此 $y_j = w_j$。这里的“最坏情况下的可容许噪声”是使 $|y_j|$ 尽可能大，从而对条件 $|y_j| \\le \\lambda$ 构成挑战的噪声。这种最坏情况是任意满足 $|w_j|=\\sigma$ 的 $w_j$。因此，我们必须有 $\\max_{|w_j|\\le\\sigma} |w_j| \\le \\lambda$，这可以简化为：\n    $$\n    \\lambda \\ge \\sigma\n    $$\n\n将这两个条件与参数约束 $\\lambda > 0$ 和 $\\gamma > 1$ 相结合，$(\\lambda, \\gamma)$ 平面上的安全区域是满足以下条件的点集\n$$\n\\sigma \\le \\lambda  \\frac{x_{\\min}-\\sigma}{\\gamma} \\quad \\text{且} \\quad \\gamma > 1\n$$\n注意，为使该区域存在，我们必须有 $x_{\\min}-\\sigma > 0$，即 $x_{\\min} > \\sigma$。\n\n接下来，我们分析近阈值系数。考虑一个幅值为 $|x^{\\star}_{\\mathrm{nt}}| = \\gamma\\lambda + \\varepsilon$（其中 $\\varepsilon > 0$）的假设项。为使该项无偏（$\\hat{x}_{\\mathrm{nt}} = y_{\\mathrm{nt}}$），我们需要 $|y_{\\mathrm{nt}}| > \\gamma\\lambda$。这必须在最坏情况噪声下成立，因此我们需要 $\\min_{|w|\\le\\sigma} |x^{\\star}_{\\mathrm{nt}} + w| > \\gamma\\lambda$。这个最小值为 $||x^{\\star}_{\\mathrm{nt}}| - \\sigma|$。代入 $|x^{\\star}_{\\mathrm{nt}}|$ 的值，条件变为\n$$\n|(\\gamma\\lambda + \\varepsilon) - \\sigma| > \\gamma\\lambda\n$$\n假设参数 $(\\lambda, \\gamma)$ 从安全区域中选取，我们有 $\\lambda \\ge \\sigma$ 和 $\\gamma > 1$。让我们检查绝对值内项的符号：$\\gamma\\lambda + \\varepsilon - \\sigma$。由于 $\\lambda \\ge \\sigma$，我们有 $\\gamma\\lambda \\ge \\gamma\\sigma$。因此，$\\gamma\\lambda + \\varepsilon - \\sigma \\ge \\gamma\\sigma + \\varepsilon - \\sigma = (\\gamma-1)\\sigma + \\varepsilon$。因为 $\\gamma > 1$，$\\sigma \\ge 0$ 且 $\\varepsilon > 0$，所以该项严格为正。因此我们可以去掉绝对值：\n$$\n\\gamma\\lambda + \\varepsilon - \\sigma > \\gamma\\lambda\n$$\n这简化为 $\\varepsilon > \\sigma$。这是该项保持无偏时 $\\varepsilon$ 所需满足的充要条件。\n\n最后，我们找出使安全区域为非空的 $\\gamma$ 的上确界值。一个非空的安全区域要求存在一个 $\\lambda$ 使得 $\\sigma \\le \\lambda  \\frac{x_{\\min}-\\sigma}{\\gamma}$。这仅在下界严格小于上界时才可能：\n$$\n\\sigma  \\frac{x_{\\min}-\\sigma}{\\gamma}\n$$\n假设 $\\sigma > 0$，我们可以重排这个不等式来解出 $\\gamma$。由于 $\\gamma > 1$ 为正，我们两边乘以 $\\gamma$：\n$$\n\\gamma\\sigma  x_{\\min} - \\sigma\n$$\n$$\n\\gamma\\sigma + \\sigma  x_{\\min}\n$$\n$$\n(\\gamma+1)\\sigma  x_{\\min}\n$$\n$$\n\\gamma + 1  \\frac{x_{\\min}}{\\sigma}\n$$\n$$\n\\gamma  \\frac{x_{\\min}}{\\sigma} - 1\n$$\n因此，使安全区域为非空的所有 $\\gamma$ 值的集合由条件 $\\gamma > 1$ 和 $\\gamma  \\frac{x_{\\min}}{\\sigma} - 1$ 的交集给出。这定义了区间 $\\left(1, \\frac{x_{\\min}}{\\sigma} - 1\\right)$。为使该区间非空，我们需要 $1  \\frac{x_{\\min}}{\\sigma} - 1$，这意味着 $2  \\frac{x_{\\min}}{\\sigma}$，即 $x_{\\min} > 2\\sigma$，这是稀疏恢复理论中的一个标准条件。\n\n问题要求的是所有可能的 $\\gamma$ 值的集合的上确界。区间 $\\left(1, \\frac{x_{\\min}}{\\sigma} - 1\\right)$ 的上确界是其上端点。\n如果 $\\sigma = 0$，安全区域的条件变为 $\\lambda \\ge 0$ 和 $\\gamma\\lambda  x_{\\min}$。对于任何 $\\gamma > 1$，我们可以选择 $\\lambda \\in (0, x_{\\min}/\\gamma)$，因此该区域非空。在这种情况下，有效的 $\\gamma$ 的集合是 $(1, \\infty)$，其上确界是 $\\infty$。推导出的表达式 $\\frac{x_{\\min}}{\\sigma} - 1$ 在 $\\sigma \\to 0^+$ 时正确地发散到 $\\infty$。因此，它可以作为通用的闭式答案。", "answer": "$$\n\\boxed{\\frac{x_{\\min}}{\\sigma} - 1}\n$$", "id": "3462708"}, {"introduction": "理论模型常常依赖于理想条件，例如设计矩阵的列向量具有完美的单位范数，但这在实践中可能并不成立。这个动手练习旨在挑战你解决这个现实问题，首先从第一性原理出发，推导出一个能适应不同列范数的广义“重归一化”阈值规则。然后，你将实现并测试这个更稳健的算法，从而学习如何将理论概念应用于实际的、非理想的情境中。[@problem_id:3462665]", "problem": "给定一个压缩感知中的线性逆模型，包含一个设计矩阵 $A \\in \\mathbb{R}^{m \\times n}$、一个稀疏真实向量 $x_0 \\in \\mathbb{R}^n$ 以及无噪声测量值 $y = A x_0$。在实践中，$A$ 的列通常是标准化的，但制造或预处理错误可能会引入每列的缩放因子，使得实际使用的设计矩阵是 $A D$，其中 $D = \\mathrm{diag}(s_1,\\dots,s_n)$ 且 $s_j  0$。这意味着列 $A_j$ 被缩放为 $A_j \\gets s_j A_j$，并且每列的平方范数 $c_j = \\| A_j \\|_2^2$ 会偏离其目标值。您将研究此类列归一化错误对非凸惩罚项阈值化行为的影响，并推导出一个重归一化的阈值规则，该规则在使用平滑削减绝对偏差 (SCAD) 和最小最大凹惩罚 (MCP) 进行坐标下降更新时，能正确地考虑 $c_j$。\n\n从目标函数出发：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\| y - A x \\|_2^2 + \\sum_{j=1}^n p(x_j;\\lambda,\\theta),\n$$\n其中 $p(\\cdot;\\lambda,\\theta)$ 是平滑削减绝对偏差 (SCAD) 惩罚或最小最大凹惩罚 (MCP)，两者都由一个正则化水平 $\\lambda  0$ 和一个形状参数 $\\theta$ 参数化（对于 SCAD 使用 $a  2$，对于 MCP 使用 $\\gamma  1$）。请从第一性原理出发，推导第 $j$ 个坐标的坐标级一维子问题，并得到一个重归一化的阈值规则，该规则明确依赖于每列的平方范数 $c_j = \\| A_j \\|_2^2$ 和标量 $z_j = A_j^\\top r + c_j x_j$，其中 $r = y - A x$ 是当前残差，$x_j$ 是第 $j$ 个系数的当前迭代值。您的推导必须从最小二乘项的坐标下降视角和非凸惩罚的次梯度平稳性条件开始，并且不得直接调用任何快捷公式。\n\n平滑削减绝对偏差 (SCAD) 惩罚使用参数 $a  2$。最小最大凹惩罚 (MCP) 使用参数 $\\gamma  1$。对于每种惩罚，推导出一维子问题的分段阈值函数，该函数将 $(z_j, c_j, \\lambda, \\theta)$ 映射到一个更新后的系数 $t_j^\\star$。当 $c_j = 1$ 时，重归一化规则必须退化为已知的阈值化行为，并且当 $c_j \\neq 1$ 时，必须正确调整阈值和收缩。\n\n实现一个近端坐标下降求解器，该求解器：\n- 将 $x$ 初始化为零向量。\n- 维护残差 $r = y - A x$。\n- 按 $j = 1,\\dots,n$ 循环迭代坐标，计算 $z_j = A_j^\\top r + c_j x_j$，然后通过您为所选惩罚推导的重归一化阈值规则更新 $x_j \\gets t_j^\\star$，并相应地更新残差。\n- 当所有坐标上的最大绝对变化量低于某个容差或达到最大迭代次数时停止。\n\n按如下方式模拟随机逐列缩放噪声下的恢复：\n- 生成 $A_{\\text{base}}$，其元素为从标准正态分布中抽取的独立同分布样本，然后将每列归一化为单位欧几里得范数。\n- 对于给定的 $\\delta \\in [0,1)$，从 $[1 - \\delta, 1 + \\delta]$ 中独立均匀地抽取 $s_j$，并设置 $A = A_{\\text{base}} \\cdot \\mathrm{diag}(s)$（逐列缩放）。\n- 均匀随机地选择一个大小为 $k$ 的支撑集，并在该支撑上用从 $[0.5, 1.0]$ 中均匀抽取的随机符号和大小填充 $x_0$。设置 $y = A x_0$。\n- 使用 SCAD 或 MCP 惩罚运行您的求解器，对于 SCAD 使用给定的超参数 $(\\lambda, a)$，对于 MCP 使用 $(\\lambda, \\gamma)$。\n\n将精确支撑恢复定义为恢复出的 $x$ 的 $k$ 个最大绝对值条目的索引集与 $x_0$ 的真实支撑集相等的事件。\n\n您的程序必须评估以下测试套件，每个测试用例指定为 $(\\text{penalty}, \\delta, \\lambda, \\theta, m, n, k, \\text{seed})$：\n- 案例 1：MCP, $\\delta = 0.0$, $\\lambda = 0.08$, $\\gamma = 3.0$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 1$。\n- 案例 2：MCP, $\\delta = 0.1$, $\\lambda = 0.08$, $\\gamma = 3.0$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 2$。\n- 案例 3：MCP, $\\delta = 0.3$, $\\lambda = 0.08$, $\\gamma = 1.2$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 3$。\n- 案例 4：SCAD, $\\delta = 0.0$, $\\lambda = 0.08$, $a = 3.7$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 4$。\n- 案例 5：SCAD, $\\delta = 0.3$, $\\lambda = 0.08$, $a = 2.1$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 5$。\n- 案例 6：SCAD, $\\delta = 0.2$, $\\lambda = 0.25$, $a = 3.7$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 6$。\n\n对于每个案例，您的程序必须生成一个布尔值，指示是否实现了精确支撑恢复。您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5,r_6]$）。", "solution": "目标是在设计矩阵的列不一定是单位范数的情况下，推导并实现一个针对 SCAD 和 MCP 惩罚的重归一化坐标级更新规则。\n\n### 1. 坐标下降子问题\n\n我们从目标函数开始：\n$$\nF(x) = \\frac{1}{2} \\| y - A x \\|_2^2 + \\sum_{j=1}^n p(x_j;\\lambda,\\theta)\n$$\n在坐标下降算法中，我们一次只对单个坐标 $x_j$ 优化 $F(x)$，同时保持所有其他坐标 $x_k$ ($k \\neq j$) 固定。设当前迭代值为 $x$，第 $j$ 个坐标的新值为 $t$。我们可以将被更新的向量表示为 $x - x_j e_j + t e_j$，其中 $e_j$ 是第 $j$ 个标准基向量。\n\n最小二乘项可以写为：\n$$\n\\frac{1}{2} \\| y - A(x - x_j e_j + t e_j) \\|_2^2 = \\frac{1}{2} \\| (y - A x) + (x_j-t)A_j \\|_2^2\n$$\n设 $r = y - Ax$ 为当前残差，$A_j$ 为 $A$ 的第 $j$ 列。展开平方范数，我们得到：\n$$\n\\frac{1}{2} ( \\| r \\|_2^2 + 2(x_j-t) A_j^\\top r + (x_j-t)^2 \\| A_j \\|_2^2 )\n$$\n将 $x_j$ 更新为 $t$ 的一维子问题是最小化：\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}\\| A_j \\|_2^2 (t - x_j)^2 - (t - x_j)A_j^\\top r + p(t;\\lambda,\\theta) \\right\\}\n$$\n令 $c_j = \\| A_j \\|_2^2$。我们收集涉及 $t$ 的项：\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} (t^2 - 2tx_j) - t A_j^\\top r + p(t;\\lambda,\\theta) + \\text{const} \\right\\}\n$$\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} t^2 - t(c_j x_j + A_j^\\top r) + p(t;\\lambda,\\theta) \\right\\}\n$$\n根据问题定义，令 $z_j = c_j x_j + A_j^\\top r$。子问题变为：\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} t^2 - z_j t + p(t;\\lambda,\\theta) \\right\\}\n$$\n通过配方法，这等价于最小化：\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} \\left( t - \\frac{z_j}{c_j} \\right)^2 + p(t;\\lambda,\\theta) \\right\\}\n$$\n这是函数 $\\frac{1}{c_j}p(\\cdot)$ 在 $\\frac{z_j}{c_j}$ 处的近端算子。极小值 $t^\\star_j$ 的一阶必要条件由次梯度平稳性条件给出：\n$$\n0 \\in \\frac{\\partial}{\\partial t} \\left[ \\frac{c_j}{2} t^2 - z_j t \\right] + \\partial p(t;\\lambda,\\theta) \\bigg|_{t=t^\\star_j}\n$$\n$$\n0 \\in c_j t^\\star_j - z_j + \\partial p(t^\\star_j;\\lambda,\\theta) \\quad \\implies \\quad z_j - c_j t^\\star_j \\in \\partial p(t^\\star_j;\\lambda,\\theta)\n$$\n我们现在针对 SCAD 和 MCP 惩罚求解此问题。这些惩罚是对称的，因此我们为 $z_j > 0$（这意味着 $t^\\star_j \\ge 0$）推导规则，然后通过对称性进行扩展。\n\n### 2. 重归一化 SCAD 阈值化\n\n对于 $t>0$ 和参数 $a>2$，SCAD 惩罚及其导数为：\n$$\np'(t) = \\begin{cases} \\lambda  \\text{if } 0  t \\le \\lambda \\\\ \\frac{a\\lambda - t}{a-1}  \\text{if } \\lambda  t \\le a\\lambda \\\\ 0  \\text{if } t > a\\lambda \\end{cases}\n$$\n在 $t=0$ 处的次梯度为 $\\partial p(0) = [-\\lambda, \\lambda]$。\n\n我们求解 $z_j - c_j t = p'(t)$，其中 $t>0$ 且 $z_j>0$。\n\n1.  若 $t^\\star_j = 0$，我们需要 $z_j \\in [-\\lambda, \\lambda]$。对于 $z_j > 0$，这意味着 $0  z_j \\le \\lambda$。\n2.  若 $0  t^\\star_j \\le \\lambda$，$p'(t) = \\lambda$。则 $z_j - c_j t = \\lambda \\implies t^\\star_j = (z_j - \\lambda)/c_j$。这在 $0  (z_j-\\lambda)/c_j \\le \\lambda$ 时有效，即要求 $\\lambda  z_j \\le (c_j+1)\\lambda$。\n3.  若 $t^\\star_j > a\\lambda$，$p'(t) = 0$。则 $z_j - c_j t = 0 \\implies t^\\star_j = z_j/c_j$。这在 $z_j/c_j > a\\lambda$ 时有效，即要求 $z_j > ac_j\\lambda$。\n4.  若 $\\lambda  t^\\star_j \\le a\\lambda$，$p'(t) = (a\\lambda - t)/(a-1)$。则 $z_j - c_j t = (a\\lambda-t)/(a-1) \\implies t^\\star_j(c_j(a-1)-1) = z_j(a-1) - a\\lambda$。\n    若 $c_j(a-1) - 1 \\neq 0$，则 $t^\\star_j = \\frac{(a-1)z_j - a\\lambda}{c_j(a-1)-1}$。这适用于先前区域之间的 $z_j$ 值，即 $(c_j+1)\\lambda  z_j \\le ac_j\\lambda$。\n    如果 $c_j(a-1)-1 = 0$ 会出现奇异点。在这种特殊情况下，一维目标函数是局部线性的，极小值点在区间 $(\\lambda, a\\lambda]$ 的边界上。\n\n综合这些针对 $|z_j|$ 的情况，重归一化 SCAD 阈值算子 $S_{\\text{SCAD}}(z_j, c_j, \\lambda, a)$ 是：\n$$\nt^\\star_j = \\mathrm{sgn}(z_j) \\times \\begin{cases}\n0  \\text{if } |z_j| \\le \\lambda \\\\\n\\frac{|z_j| - \\lambda}{c_j}  \\text{if } \\lambda  |z_j| \\le (c_j+1)\\lambda \\\\\n\\frac{(a-1)|z_j| - a\\lambda}{c_j(a-1)-1}  \\text{if } (c_j+1)\\lambda  |z_j| \\le ac_j\\lambda \\text{ and } c_j(a-1) \\ne 1 \\\\\n\\dots  \\text{(奇异点的特殊处理)} \\\\\n\\frac{|z_j|}{c_j}  \\text{if } |z_j| > ac_j\\lambda\n\\end{cases}\n$$\n\n### 3. 重归一化 MCP 阈值化\n\n对于 $t>0$ 和参数 $\\gamma>1$，MCP 惩罚及其导数为：\n$$\np'(t) = \\begin{cases} \\lambda - t/\\gamma  \\text{if } 0  t \\le \\gamma\\lambda \\\\ 0  \\text{if } t > \\gamma\\lambda \\end{cases}\n$$\n在 $t=0$ 处的次梯度为 $\\partial p(0) = [-\\lambda, \\lambda]$。我们求解 $z_j - c_j t = p'(t)$，其中 $t>0, z_j>0$。\n\n子问题的凸性取决于 $c_j - 1/\\gamma$ 的符号。\n\n**情况 1：$\\gamma c_j > 1$ (凸子问题)**\n子目标是凸的，存在唯一的极小值点。\n1.  若 $t^\\star_j = 0$：$z_j \\in [-\\lambda, \\lambda]$。对于 $z_j>0$，即 $0  z_j \\le \\lambda$。\n2.  若 $t^\\star_j > \\gamma\\lambda$：$p'(t)=0 \\implies t^\\star_j=z_j/c_j$。在 $z_j/c_j > \\gamma\\lambda \\implies z_j > \\gamma c_j \\lambda$ 时有效。\n3.  若 $0  t^\\star_j \\le \\gamma\\lambda$：$z_j - c_j t = \\lambda - t/\\gamma \\implies t(c_j - 1/\\gamma) = z_j-\\lambda \\implies t^\\star_j = \\frac{z_j-\\lambda}{c_j-1/\\gamma} = \\frac{\\gamma(z_j-\\lambda)}{\\gamma c_j-1}$。这在 $0  t^\\star_j \\le \\gamma\\lambda$ 时有效，即要求 $\\lambda  z_j \\le \\gamma c_j \\lambda$。\n\n**情况 2：$\\gamma c_j \\le 1$ (非凸子问题)**\n子目标是非凸的。平稳点分析表明在 $(0, \\gamma\\lambda)$ 内没有解。全局极小值点必须是局部极小值点之一，即在 $t=0$ 和区域 $|t|>\\gamma\\lambda$ 中。在该区域中，极小值点为 $t = z_j/c_j$。我们通过比较这两点的目标函数值来找到全局最小值。\n- $J(0) = \\frac{c_j}{2}(0 - z_j/c_j)^2 + p(0) = \\frac{z_j^2}{2c_j}$。\n- $J(z_j/c_j) = \\frac{c_j}{2}(z_j/c_j - z_j/c_j)^2 + p(z_j/c_j) = p(z_j/c_j)$。如果 $|z_j/c_j| > \\gamma\\lambda$，该值为 $\\frac{\\gamma\\lambda^2}{2}$。\n我们选择 $t^\\star_j=z_j/c_j$ 如果 $J(z_j/c_j)  J(0) \\implies \\frac{\\gamma\\lambda^2}{2}  \\frac{z_j^2}{2c_j} \\implies |z_j| > \\lambda\\sqrt{\\gamma c_j}$。这是一个硬阈值规则。\n\n综合这些针对 $|z_j|$ 的情况，算子 $S_{\\text{MCP}}(z_j, c_j, \\lambda, \\gamma)$ 是：\n如果 $\\gamma c_j > 1$：\n$$\nt^\\star_j = \\mathrm{sgn}(z_j) \\times \\begin{cases}\n0  \\text{if } |z_j| \\le \\lambda \\\\\n\\frac{\\gamma(|z_j| - \\lambda)}{\\gamma c_j - 1}  \\text{if } \\lambda  |z_j| \\le \\gamma c_j \\lambda \\\\\n\\frac{|z_j|}{c_j}  \\text{if } |z_j| > \\gamma c_j \\lambda\n\\end{cases}\n$$\n如果 $\\gamma c_j \\le 1$：\n$$\nt^\\star_j = \\mathrm{sgn}(z_j) \\times \\begin{cases}\n0  \\text{if } |z_j| \\le \\lambda\\sqrt{\\gamma c_j} \\\\\n\\frac{|z_j|}{c_j}  \\text{if } |z_j| > \\lambda\\sqrt{\\gamma c_j}\n\\end{cases} = \\frac{z_j}{c_j} \\mathbb{I}(|z_j| > \\lambda\\sqrt{\\gamma c_j})\n$$\n\n### 4. 近端坐标下降算法\n\n求解器实现近端坐标下降如下：\n1.  初始化 $x=0$，残差 $r=y$。\n2.  预计算列平方范数 $c_j = \\| A_j \\|_2^2$。\n3.  迭代直至收敛：\n    a. 对于每个坐标 $j=1, \\dots, n$：\n        i.   存储旧系数 $x_j^{\\text{old}} = x_j$。\n        ii.  计算 $z_j = A_j^\\top r + c_j x_j^{\\text{old}}$。\n        iii. 使用适当的推导规则 (SCAD 或 MCP) 计算新系数 $x_j^{\\text{new}} = S(z_j, c_j, \\lambda, \\theta)$。\n        iv.  更新残差：$r \\gets r - (x_j^{\\text{new}} - x_j^{\\text{old}}) A_j$。\n        v.   更新系数：$x_j \\gets x_j^{\\text{new}}$。\n    b. 检查停止准则：最大坐标变化量低于某个容差。\n4.  返回估计的稀疏向量 $x$。", "answer": "```python\nimport numpy as np\n\ndef scad_thresh(z, c, lam, a):\n    \"\"\"Renormalized SCAD thresholding operator.\"\"\"\n    abs_z = np.abs(z)\n    sign_z = np.sign(z) if z != 0 else 1\n\n    if abs_z = lam:\n        return 0.0\n\n    denom = c * (a - 1.0) - 1.0\n    \n    # Non-singular cases for a convex subproblem\n    if denom > 0: \n        if abs_z = (c + 1.0) * lam:\n            return (abs_z - lam) * sign_z / c\n        elif abs_z = a * c * lam:\n            return ((a - 1.0) * abs_z - a * lam) * sign_z / denom\n        else: # abs_z > a * c * lam\n            return z / c\n    # Non-convex or singular cases\n    else:\n        # For non-convex subproblems, the global minimum is either 0 or z/c.\n        # This occurs if the middle region stationary point is a maximizer.\n        # Compare objective values at t=0 and t=z/c\n        val_at_zero = z**2 / (2 * c)\n        \n        t_unbiased = z/c\n        abs_t_unbiased = abs_z/c\n        \n        # Calculate penalty at t_unbiased\n        if abs_t_unbiased = lam:\n            penalty_val = lam * abs_t_unbiased\n        elif abs_t_unbiased = a * lam:\n            penalty_val = (-abs_t_unbiased**2 + 2*a*lam*abs_t_unbiased - lam**2) / (2*(a-1))\n        else: # abs_t_unbiased > a*lam\n            penalty_val = (a+1)*lam**2 / 2\n        \n        val_at_unbiased = penalty_val\n\n        if val_at_zero = val_at_unbiased:\n            return 0.0\n        else:\n            return t_unbiased\n\ndef mcp_thresh(z, c, lam, gamma):\n    \"\"\"Renormalized MCP thresholding operator.\"\"\"\n    abs_z = np.abs(z)\n    sign_z = np.sign(z) if z != 0 else 1\n\n    if gamma * c > 1.0:  # Convex subproblem\n        if abs_z = lam:\n            return 0.0\n        elif abs_z = gamma * c * lam:\n            return (gamma * (abs_z - lam)) * sign_z / (gamma * c - 1.0)\n        else: # abs_z > gamma * c * lam\n            return z / c\n    else:  # Non-convex subproblem\n        threshold = lam * np.sqrt(gamma * c)\n        if abs_z = threshold:\n            return 0.0\n        else:\n            return z / c\n\n\ndef pcd_solver(A, y, penalty, lam, theta, tol=1e-6, max_iter=1000):\n    \"\"\"Proximal Coordinate Descent solver.\"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    r = y.copy()\n    \n    c = np.sum(A * A, axis=0)\n    \n    if penalty.lower() == 'scad':\n        a = theta\n        thresh_op = scad_thresh\n        params = (lam, a)\n    elif penalty.lower() == 'mcp':\n        gamma = theta\n        thresh_op = mcp_thresh\n        params = (lam, gamma)\n    else:\n        raise ValueError(\"Unknown penalty type\")\n\n    for _ in range(max_iter):\n        max_change = 0.0\n        for j in range(n):\n            if c[j]  1e-9: continue\n            \n            x_old_j = x[j]\n            z_j = A[:, j].T @ r + c[j] * x_old_j\n            \n            x_new_j = thresh_op(z_j, c[j], *params)\n            \n            delta_x_j = x_new_j - x_old_j\n            \n            if delta_x_j != 0.0:\n                r -= delta_x_j * A[:, j]\n                x[j] = x_new_j\n            \n            max_change = max(max_change, np.abs(delta_x_j))\n            \n        if max_change  tol:\n            break\n            \n    return x\n\ndef solve():\n    test_cases = [\n        ('MCP', 0.0, 0.08, 3.0, 120, 200, 10, 1),\n        ('MCP', 0.1, 0.08, 3.0, 120, 200, 10, 2),\n        ('MCP', 0.3, 0.08, 1.2, 120, 200, 10, 3),\n        ('SCAD', 0.0, 0.08, 3.7, 120, 200, 10, 4),\n        ('SCAD', 0.3, 0.08, 2.1, 120, 200, 10, 5),\n        ('SCAD', 0.2, 0.25, 3.7, 120, 200, 10, 6)\n    ]\n    \n    results = []\n\n    for penalty, delta, lam, theta, m, n, k, seed in test_cases:\n        np.random.seed(seed)\n        \n        # 1. Generate data\n        A_base = np.random.randn(m, n)\n        A_base /= np.linalg.norm(A_base, axis=0) \n        \n        s = np.random.uniform(1 - delta, 1 + delta, n)\n        A = A_base * s \n\n        support = np.random.choice(n, k, replace=False)\n        x0 = np.zeros(n)\n        magnitudes = np.random.uniform(0.5, 1.0, k)\n        signs = np.random.choice([-1, 1], k)\n        x0[support] = magnitudes * signs\n        \n        y = A @ x0\n        \n        # 2. Run solver\n        x_hat = pcd_solver(A, y, penalty=penalty, lam=lam, theta=theta)\n        \n        # 3. Evaluate support recovery\n        true_support = set(np.where(x0 != 0)[0])\n        est_support = set(np.argsort(np.abs(x_hat))[-k:])\n        \n        recovery = (true_support == est_support)\n        results.append(recovery)\n        \n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n\n```", "id": "3462665"}]}