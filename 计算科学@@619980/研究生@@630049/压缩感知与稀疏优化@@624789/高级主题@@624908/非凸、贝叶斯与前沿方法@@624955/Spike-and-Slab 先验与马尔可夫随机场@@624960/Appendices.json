{"hands_on_practices": [{"introduction": "推断结构化先验模型的第一步是掌握最简单的情形：链状图。这个问题通过一个具体的计算任务，引导我们使用动态规划来寻找一维马尔可夫随机场（MRF）上的最大后验（MAP）构型。通过亲手计算，您将直观地理解局部证据（由一元代价 $\\theta_i$ 体现）与鼓励连续性的结构化先验（由成对势能体现）之间是如何权衡的，从而得到一个全局最优的稀疏模式 [@problem_id:3480135]。", "problem": "考虑一个在系数上具有尖峰-厚板先验的压缩感知模型。设 $y \\in \\mathbb{R}^{n}$ 是服从 $y = A x + \\epsilon$ 的测量值，其中 $A \\in \\mathbb{R}^{n \\times p}$ 是一个已知的感知矩阵，$x \\in \\mathbb{R}^{p}$ 是未知系数，$\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$ 是高斯噪声。引入二元隐变量 $z_{i} \\in \\{0,1\\}$，通过尖峰-厚板先验 $p(x_{i} \\mid z_{i}) = (1 - z_{i}) \\delta(x_{i}) + z_{i} \\,\\mathcal{N}(x_{i} \\mid 0, \\tau^{2})$ 来编码稀疏性。假设 $z_{1},\\dots,z_{p}$ 服从一维链式马尔可夫随机场 (MRF) 先验，其成对势函数鼓励连续性，\n$$\n\\phi(z_{i}, z_{i+1}) \\triangleq w \\, |z_{i} - z_{i+1}|,\\quad w > 0,\n$$\n以及一元势函数 $\\psi_{i}(z_{i})$，其源于在似然和先验下 $y$ 的局部证据。在不考虑一个与 $z$ 无关的加法常数的情况下，后验能量具有以下形式\n$$\nE(z_{1},\\dots,z_{p}) \\triangleq \\sum_{i=1}^{p} \\theta_{i} z_{i} \\;+\\; w \\sum_{i=1}^{p-1} |z_{i} - z_{i+1}|,\n$$\n其中 $\\theta_{i} \\in \\mathbb{R}$ 是固定的单元代价，概括了局部证据（例如，支持 $z_{i}=1$ 而非 $z_{i}=0$ 的负对数贝叶斯因子）。\n\n对于一个长度为 $p=5$、具有同质成对权重 $w=1$ 和一元代价\n$$\n\\theta_{1} = -0.6,\\quad \\theta_{2} = -0.4,\\quad \\theta_{3} = 0.2,\\quad \\theta_{4} = -0.5,\\quad \\theta_{5} = -0.3,\n$$\n的维链，执行以下操作：\n\n1. 从链式马尔可夫随机场 (MRF) 上的动态规划 (DP) 的第一性原理出发，推导出一个递推关系，用于计算当 $z_i$ 固定为 $0$ 或 $1$ 时，到索引 $i$ 为止的最小能量。使用该递推关系计算最小化 $E(z)$ 的最大后验 (MAP) 配置 $z^{\\star} \\in \\{0,1\\}^{5}$，并展示恢复 $z^{\\star}$ 的回溯步骤。\n\n2. 定义独立阈值法，该方法通过逐点最小化 $\\sum_{i=1}^{5} \\theta_{i} z_{i}$ 来忽略成对相互作用。计算阈值化配置 $z^{\\mathrm{thr}}$ 并将其能量 $E(z^{\\mathrm{thr}})$ 与 $E(z^{\\star})$ 进行比较，用连续性惩罚来解释任何差异。\n\n使用 LaTeX 将您的最终 MAP 配置表示为一个包含五个条目的二元行矩阵。无需四舍五入。", "solution": "该问题是有效的。它提出了一个在链式马尔可夫随机场上的适定 MAP 推断任务，这是统计信号处理和机器学习中的一个标准问题。所有参数都已提供，目标也已明确定义。我们将按要求分两部分进行解答。\n\n需要最小化的能量函数由下式给出：\n$$\nE(z_{1},\\dots,z_{p}) = \\sum_{i=1}^{p} \\theta_{i} z_{i} + w \\sum_{i=1}^{p-1} |z_{i} - z_{i+1}|\n$$\n对于给定的参数 `p=5`、`$w=1$` 和 `$\\theta = (-0.6, -0.4, 0.2, -0.5, -0.3)$`，一个配置 `$z = (z_1, z_2, z_3, z_4, z_5)$` 的能量是：\n$$\nE(z) = \\sum_{i=1}^{5} \\theta_{i} z_{i} + \\sum_{i=1}^{4} |z_{i} - z_{i+1}|\n$$\n成对代价 $|z_i - z_{i+1}|$ 在 $z_i \\neq z_{i+1}$ 时（状态发生改变）为 `$1$`，在 $z_i = z_{i+1}$ 时（状态保持不变）为 `$0$`。\n\n### 第 1 部分：用于 MAP 推断的动态规划\n\n我们寻求最大后验 (MAP) 配置 $z^{\\star} = \\arg\\min_{z \\in \\{0,1\\}^5} E(z)$。由于底层图模型是一个链，这个问题可以使用动态规划（具体来说，是最小和算法或维特比算法）高效地解决。\n\n设 $M_i(k)$ 是以 $z_i = k$ 结尾的部分配置 $(z_1, \\dots, z_i)$ 的最小能量，其中 $k \\in \\{0, 1\\}$。总能量是可加的，所以我们可以建立一个递推关系。到位置 $i$ 且 $z_i=k$ 时的能量，等于位置 $i$ 的一元代价 ($\\theta_i k$) 加上以 $i-1$ 结尾的路径的最小可能能量，再加上从 $z_{i-1}$ 到 $z_i$ 的转移代价。\n\n递推关系是：\n$$\nM_i(k) = \\theta_i k + \\min_{j \\in \\{0,1\\}} \\left( M_{i-1}(j) + w|k-j| \\right)\n$$\n由于 $w=1$，我们可以显式地写出 $k=0$ 和 $k=1$ 的递推式：\n$$\nM_i(0) = \\theta_i \\cdot 0 + \\min \\left( M_{i-1}(0) + |0-0|, M_{i-1}(1) + |0-1| \\right) = \\min(M_{i-1}(0), M_{i-1}(1) + 1)\n$$\n$$\nM_i(1) = \\theta_i \\cdot 1 + \\min \\left( M_{i-1}(0) + |1-0|, M_{i-1}(1) + |1-1| \\right) = \\theta_i + \\min(M_{i-1}(0) + 1, M_{i-1}(1))\n$$\n为了恢复最优路径，我们还存储回溯指针 $\\psi_i(k)$，它记录了在步骤 $i-1$ 时为 $M_i(k)$ 产生最小代价的状态 $j \\in \\{0,1\\}$：\n$$\n\\psi_i(k) = \\arg\\min_{j \\in \\{0,1\\}} \\left( M_{i-1}(j) + w|k-j| \\right)\n$$\n\n**前向过程：**\n\n**初始化 ($i=1$):**\n长度为 $1$ 的路径的能量就是其一元代价。\n- $M_1(0) = \\theta_1 \\cdot 0 = 0$\n- $M_1(1) = \\theta_1 \\cdot 1 = -0.6$\n\n**步骤 $i=2$ ($ \\theta_2 = -0.4$):**\n- $M_2(0) = \\min(M_1(0), M_1(1) + 1) = \\min(0, -0.6 + 1) = \\min(0, 0.4) = 0$。最小值来自 $M_1(0)$，因此 $\\psi_2(0) = 0$。\n- $M_2(1) = \\theta_2 + \\min(M_1(0) + 1, M_1(1)) = -0.4 + \\min(0+1, -0.6) = -0.4 - 0.6 = -1.0$。最小值来自 $M_1(1)$，因此 $\\psi_2(1) = 1$。\n\n**步骤 $i=3$ ($ \\theta_3 = 0.2$):**\n- $M_3(0) = \\min(M_2(0), M_2(1) + 1) = \\min(0, -1.0 + 1) = \\min(0, 0) = 0$。两条路径都达到了最小值。按照惯例，我们可以选择 $\\psi_3(0) = 0$。\n- $M_3(1) = \\theta_3 + \\min(M_2(0) + 1, M_2(1)) = 0.2 + \\min(0+1, -1.0) = 0.2 - 1.0 = -0.8$。最小值来自 $M_2(1)$，因此 $\\psi_3(1) = 1$。\n\n**步骤 $i=4$ ($ \\theta_4 = -0.5$):**\n- $M_4(0) = \\min(M_3(0), M_3(1) + 1) = \\min(0, -0.8 + 1) = \\min(0, 0.2) = 0$。最小值来自 $M_3(0)$，因此 $\\psi_4(0) = 0$。\n- $M_4(1) = \\theta_4 + \\min(M_3(0) + 1, M_3(1)) = -0.5 + \\min(0+1, -0.8) = -0.5 - 0.8 = -1.3$。最小值来自 $M_3(1)$，因此 $\\psi_4(1) = 1$。\n\n**步骤 $i=5$ ($ \\theta_5 = -0.3$):**\n- $M_5(0) = \\min(M_4(0), M_4(1) + 1) = \\min(0, -1.3 + 1) = \\min(0, -0.3) = -0.3$。最小值来自 $M_4(1)$，因此 $\\psi_5(0) = 1$。\n- $M_5(1) = \\theta_5 + \\min(M_4(0) + 1, M_4(1)) = -0.3 + \\min(0+1, -1.3) = -0.3 - 1.3 = -1.6$。最小值来自 $M_4(1)$，因此 $\\psi_5(1) = 1$。\n\n**回溯：**\n\n整个链的最小总能量是 $E(z^{\\star}) = \\min(M_5(0), M_5(1)) = \\min(-0.3, -1.6) = -1.6$。\n最终位置的最优状态是 $z^{\\star}_5 = \\arg\\min_{k \\in \\{0,1\\}} M_5(k) = 1$。\n\n现在我们使用存储的指针 $\\psi$ 进行回溯，以找到最优路径：\n- $z^{\\star}_5 = 1$\n- $z^{\\star}_4 = \\psi_5(z^{\\star}_5) = \\psi_5(1) = 1$\n- $z^{\\star}_3 = \\psi_4(z^{\\star}_4) = \\psi_4(1) = 1$\n- $z^{\\star}_2 = \\psi_3(z^{\\star}_3) = \\psi_3(1) = 1$\n- $z^{\\star}_1 = \\psi_2(z^{\\star}_2) = \\psi_2(1) = 1$\n\n因此，MAP 配置是 $z^{\\star} = (1, 1, 1, 1, 1)$。\n\n### 第 2 部分：独立阈值法和比较\n\n独立阈值法忽略成对相互作用项 $w \\sum |z_i - z_{i+1}|$，只最小化一元项之和 $E_{\\mathrm{thr}}(z) = \\sum_{i=1}^5 \\theta_i z_i$。这个求和是一个非耦合优化问题，因此我们可以独立地最小化每个项 $\\theta_i z_i$。对于每个位置 $i$，最优选择 $z^{\\mathrm{thr}}_i$ 是：\n$$\nz^{\\mathrm{thr}}_i =\n\\begin{cases}\n1  & \\text{if } \\theta_i < 0 \\\\\n0  & \\text{if } \\theta_i \\ge 0\n\\end{cases}\n$$\n将此规则应用于给定的一元代价：\n- $\\theta_1 = -0.6 < 0 \\implies z^{\\mathrm{thr}}_1 = 1$\n- $\\theta_2 = -0.4 < 0 \\implies z^{\\mathrm{thr}}_2 = 1$\n- $\\theta_3 = 0.2 > 0 \\implies z^{\\mathrm{thr}}_3 = 0$\n- $\\theta_4 = -0.5 < 0 \\implies z^{\\mathrm{thr}}_4 = 1$\n- $\\theta_5 = -0.3 < 0 \\implies z^{\\mathrm{thr}}_5 = 1$\n\n阈值化配置是 $z^{\\mathrm{thr}} = (1, 1, 0, 1, 1)$。\n\n现在，我们使用*完整*的能量函数 $E(z)$ 来计算此配置的能量：\n$$\nE(z^{\\mathrm{thr}}) = \\sum_{i=1}^{5} \\theta_{i} z^{\\mathrm{thr}}_{i} + \\sum_{i=1}^{4} |z^{\\mathrm{thr}}_{i} - z^{\\mathrm{thr}}_{i+1}|\n$$\n一元项的贡献是：\n$$\n\\sum \\theta_i z^{\\mathrm{thr}}_i = (-0.6)\\cdot 1 + (-0.4)\\cdot 1 + (0.2)\\cdot 0 + (-0.5)\\cdot 1 + (-0.3)\\cdot 1 = -0.6 - 0.4 - 0.5 - 0.3 = -1.8\n$$\n成对项的贡献（连续性惩罚）是：\n$$\n\\sum |z^{\\mathrm{thr}}_i - z^{\\mathrm{thr}}_{i+1}| = |1-1| + |1-0| + |0-1| + |1-1| = 0 + 1 + 1 + 0 = 2\n$$\n总能量是 $E(z^{\\mathrm{thr}}) = -1.8 + 2 = 0.2$。\n\n**比较和解释：**\n- MAP 能量：$E(z^{\\star}) = E(1,1,1,1,1) = (-0.6 - 0.4 + 0.2 - 0.5 - 0.3) + 0 = -1.6$。\n- 阈值法能量：$E(z^{\\mathrm{thr}}) = E(1,1,0,1,1) = 0.2$。\n\n正如预期的，$E(z^{\\star})  E(z^{\\mathrm{thr}})$，因为 $z^{\\star}$ 是全局最小化子。与 MAP 解的一元能量（$-1.6$）相比，独立阈值解 $z^{\\mathrm{thr}}$ 获得了一元能量更低的值（$-1.8$）。然而，$z^{\\mathrm{thr}}$ 因在索引 $i=3$ 处打破了连续性而从成对项中付出了显著的惩罚（$2$）。MAP 解 $z^{\\star}$ 在一元项上付出了微小的代价（尽管 $\\theta_3=0.2 > 0$，但仍选择 $z_3=1$），以实现零成对惩罚。在 $i=3$ 处的权衡是明确的：设置 $z_3=0$（其邻居为 $1$）会给能量增加 $(\\theta_3\\cdot 0) + |z_2-z_3| + |z_3-z_4| = 0 + |1-0|+|0-1| = 2$，而设置 $z_3=1$ 会增加 $(\\theta_3\\cdot 1) + |z_2-z_3|+|z_3-z_4| = 0.2 + |1-1|+|1-1| = 0.2$。由于 $0.2  2$，MAP 解正确地选择了 $z_3=1$，这表明了成对势函数如何强制执行结构偏好（连续性）而非局部贪心决策。", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  1  1  1  1 \\end{pmatrix}}\n$$", "id": "3480135"}, {"introduction": "当图模型不再是简单的链条，而是包含环路的更复杂结构（如网格）时，精确推断通常变得不可行。这个问题将我们带入近似推断的世界，要求我们推导并实现两种核心算法：平均场变分推断和环路置信传播（Bethe近似）。通过编程实践并比较这两种方法在同一问题上的表现，您将深入理解它们在计算效率和推断精度之间的根本差异和权衡 [@problem_id:3480139]。", "problem": "考虑一个在二维网格图上的二元潜变量场 $\\{z_i\\}_{i=1}^n$，该图具有四邻域连接性，其中每个 $z_i \\in \\{0,1\\}$。在 $\\boldsymbol{z} = (z_1,\\dots,z_n)$ 上的先验是一个伊辛型马尔可夫随机场（MRF）（马尔可夫随机场（MRF）定义为一个概率图模型，其局部条件独立性遵循一个无向图），具有成对交互和局部偏置。该先验的形式为\n$$\np(\\boldsymbol{z}) \\propto \\exp\\left( \\sum_{i=1}^n b_i z_i + \\sum_{(i,j)\\in E} w\\, z_i z_j \\right),\n$$\n其中 $E$ 表示网格上对应于最近邻对的无向边集，$w \\in \\mathbb{R}$ 是一个统一的交互强度，$b_i \\in \\mathbb{R}$ 是一个从全局稀疏性参数 $\\rho \\in (0,1)$ 通过 $b_i = \\log\\left(\\frac{\\rho}{1-\\rho}\\right)$ 获得的局部偏置。\n\n尖峰-厚板模型通过以下组成部分将 $\\boldsymbol{z}$、$\\boldsymbol{x}$ 和观测值 $\\boldsymbol{y}$ 联系起来：\n- 系数上的尖峰-厚板先验：对于每个索引 $i$，$x_i$ 给定 $z_i$ 的条件分布为\n$$\nx_i \\mid z_i \\sim \n\\begin{cases}\n\\delta_0  \\text{如果 } z_i = 0, \\\\\n\\mathcal{N}(0,\\tau^2)  \\text{如果 } z_i = 1,\n\\end{cases}\n$$\n其中 $\\delta_0$ 表示在 $0$ 处的点质量，且 $\\tau^2  0$。\n- 高斯观测模型：对于每个索引 $i$，\n$$\ny_i \\mid x_i \\sim \\mathcal{N}(x_i,\\sigma^2),\n$$\n其中 $\\sigma^2  0$。\n\n给定 $\\boldsymbol{x}$，观测值在不同索引间是条件独立的。二维网格按行主序指定，因此索引 $i$ 对应于位置 $(r,c)$，其中 $i = r \\cdot C + c + 1$，$C$ 是列数，$r,c$ 是从零开始的行和列索引；邻居是最多四个保持在边界内的基本方向。\n\n任务：\n1. 从变分平均场族 $q(\\boldsymbol{z}) = \\prod_{i=1}^n \\text{Bernoulli}(\\pi_i)$ 的定义出发，推导证据下界（ELBO）的驻点必须为每个 $\\pi_i$ 满足的不动点方程，明确考虑伊辛先验以及从尖峰-厚板和高斯观测模型边缘化得到的局部似然。你的推导必须基于变分推断的基本原理和指数族分布的性质。\n2. 通过在对 $\\boldsymbol{x}$ 进行解析边缘化后得到的二元成对模型上进行含环和-积信念传播来定义 Bethe 近似。推导有向消息 $m_{i\\to j}(z_j)$ 和节点信念 $b_i(z_i)$ 的消息更新方程。\n3. 实现一个程序，该程序：\n   - 为每个测试用例构建网格图的邻居。\n   - 通过带阻尼地迭代不动点方程，直至收敛到指定的容差，来计算 $\\boldsymbol{\\pi}$ 的平均场不动点。\n   - 通过带阻尼的含环和-积信念传播，计算 Bethe 节点信念 $b_i(1)$，直至收敛到指定的容差。\n   - 对于每个测试用例，输出平均场概率 $\\pi_i$ 和 Bethe 信念 $b_i(1)$ 之间的平均绝对差，该差值在所有节点上聚合为每个测试用例的单个标量。\n\n您必须使用以下具有明确参数值的测试套件：\n- 测试用例 1（正常路径）：\n  - 网格大小：$2 \\times 2$。\n  - 交互强度：$w = 0.6$。\n  - 稀疏性参数：$\\rho = 0.3$（因此 $b_i = \\log(\\rho/(1-\\rho))$）。\n  - 厚板方差：$\\tau^2 = 1.0$。\n  - 观测噪声方差：$\\sigma^2 = 0.25$。\n  - 观测值：$\\boldsymbol{y} = (1.2, -0.3, 0.7, 2.0)$，按行主序。\n- 测试用例 2（边界条件：无耦合）：\n  - 网格大小：$3 \\times 3$。\n  - 交互强度：$w = 0.0$。\n  - 稀疏性参数：$\\rho = 0.2$（因此 $b_i = \\log(\\rho/(1-\\rho))$）。\n  - 厚板方差：$\\tau^2 = 1.5$。\n  - 观测噪声方差：$\\sigma^2 = 0.16$。\n  - 观测值：$\\boldsymbol{y} = (-0.1, 0.2, 0.0, 0.5, -1.0, 0.3, 0.7, -0.4, 0.1)$，按行主序。\n- 测试用例 3（边缘情况：强耦合和高噪声）：\n  - 网格大小：$3 \\times 3$。\n  - 交互强度：$w = 1.2$。\n  - 稀疏性参数：$\\rho = 0.5$（因此 $b_i = 0$）。\n  - 厚板方差：$\\tau^2 = 0.5$。\n  - 观测噪声方差：$\\sigma^2 = 1.0$。\n  - 观测值：$\\boldsymbol{y} = (0.0, 0.5, -0.5, 1.5, -1.2, 0.2, -0.8, 0.9, -0.1)$，按行主序。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例 $1$、$2$ 和 $3$ 的顺序排列结果，例如 $[r_1,r_2,r_3]$，其中每个 $r_k$ 是一个实数（浮点数），等于第 $k$ 个测试用例的两组节点边缘概率之间的平均绝对差。", "solution": "该问题要求为一个分层贝叶斯模型推导变分推断的更新方程，并实现它们以进行数值比较。该模型将一个尖峰-厚板先验与一个作用于指示稀疏性的二元潜变量上的伊辛型马尔可夫随机场（MRF）先验相结合。我们将使用两种常见的近似推断方法来推断这些潜变量：平均场变分推断和通过含环信念传播实现的 Bethe 近似。\n\n### 1. 模型设定与边缘化\n\n完整的概率模型定义在观测值 $\\boldsymbol{y}$、稀疏系数 $\\boldsymbol{x}$ 和二元潜变量 $\\boldsymbol{z}$ 上。联合概率为 $p(\\boldsymbol{y}, \\boldsymbol{x}, \\boldsymbol{z}) = p(\\boldsymbol{y} \\mid \\boldsymbol{x}) p(\\boldsymbol{x} \\mid \\boldsymbol{z}) p(\\boldsymbol{z})$。我们的目标是推断潜变量的后验分布 $p(\\boldsymbol{z} \\mid \\boldsymbol{y})$。为此，我们首先边缘化掉连续变量 $\\boldsymbol{x}$。\n\n给定条件独立性假设，观测值给定潜变量的边缘似然 $p(\\boldsymbol{y} \\mid \\boldsymbol{z})$ 可在索引 $i$ 上分解：\n$$\np(\\boldsymbol{y} \\mid \\boldsymbol{z}) = \\prod_{i=1}^n p(y_i \\mid z_i)\n$$\n其中每一项 $p(y_i \\mid z_i)$ 通过对 $x_i$ 积分得到：\n$$\np(y_i \\mid z_i) = \\int p(y_i \\mid x_i) p(x_i \\mid z_i) \\,dx_i.\n$$\n我们对 $z_i$ 的两种情况分别计算此积分：\n- 如果 $z_i=0$，则 $x_i$ 的先验是一个狄拉克δ函数 $p(x_i \\mid z_i=0) = \\delta_0(x_i)$。积分变为在 $x_i=0$ 处求值：\n$$\np(y_i \\mid z_i=0) = p(y_i \\mid x_i=0) = \\mathcal{N}(y_i; 0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{y_i^2}{2\\sigma^2}\\right).\n$$\n- 如果 $z_i=1$，则 $x_i$ 的先验是高斯分布，$p(x_i \\mid z_i=1) = \\mathcal{N}(x_i; 0, \\tau^2)$。观测模型是 $p(y_i \\mid x_i) = \\mathcal{N}(y_i; x_i, \\sigma^2)$。边缘分布 $p(y_i \\mid z_i=1)$ 是两个高斯分布卷积的结果，它本身也是一个高斯分布，其均值为均值之和（$0+0=0$），方差为方差之和（$\\tau^2+\\sigma^2$）：\n$$\np(y_i \\mid z_i=1) = \\mathcal{N}(y_i; 0, \\tau^2+\\sigma^2) = \\frac{1}{\\sqrt{2\\pi(\\tau^2+\\sigma^2)}} \\exp\\left(-\\frac{y_i^2}{2(\\tau^2+\\sigma^2)}\\right).\n$$\n$\\boldsymbol{z}$ 的完整后验则与边缘似然和 $\\boldsymbol{z}$ 的先验的乘积成正比：\n$$\np(\\boldsymbol{z} \\mid \\boldsymbol{y}) \\propto p(\\boldsymbol{y} \\mid \\boldsymbol{z}) p(\\boldsymbol{z}) = \\left(\\prod_{i=1}^n p(y_i \\mid z_i)\\right) \\exp\\left( \\sum_{i=1}^n b_i z_i + \\sum_{(i,j)\\in E} w\\, z_i z_j \\right).\n$$\n\n### 2. 平均场变分推断\n\n在平均场变分推断中，我们用一个完全分解的分布 $q(\\boldsymbol{z}) = \\prod_{i=1}^n q_i(z_i)$ 来近似真实的后验分布 $p(\\boldsymbol{z} \\mid \\boldsymbol{y})$。对于一个二元变量 $z_i$，我们将其 $q_i(z_i)$ 参数化为一个伯努利分布：$q_i(z_i) = \\text{Bernoulli}(z_i; \\pi_i) = \\pi_i^{z_i} (1-\\pi_i)^{1-z_i}$，其中 $\\pi_i = \\mathbb{E}_q[z_i]$。\n\n最大化证据下界（ELBO）的单个变量 $z_k$ 的最优分布 $q_k^*(z_k)$ 由坐标上升更新规则给出：\n$$\n\\log q_k^*(z_k) = \\mathbb{E}_{q_{\\setminus k}}[\\log p(\\boldsymbol{z}, \\boldsymbol{y})] + \\text{常数},\n$$\n其中 $q_{\\setminus k} = \\prod_{j \\neq k} q_j(z_j)$ 且 $\\log p(\\boldsymbol{z}, \\boldsymbol{y}) = \\log p(\\boldsymbol{z} \\mid \\boldsymbol{y}) + \\log p(\\boldsymbol{y})$。\n我们有：\n$$\n\\log p(\\boldsymbol{z}, \\boldsymbol{y}) = \\sum_{i=1}^n \\log p(y_i \\mid z_i) + \\sum_{i=1}^n b_i z_i + \\sum_{(i,j)\\in E} w\\, z_i z_j + \\text{常数}.\n$$\n对 $q_{\\setminus k}$ 求期望并分离出包含 $z_k$ 的项：\n$$\n\\log q_k^*(z_k) \\propto \\log p(y_k \\mid z_k) + b_k z_k + \\mathbb{E}_{q_{\\setminus k}}\\left[\\sum_{j \\in \\mathcal{N}(k)} w z_k z_j\\right],\n$$\n其中 $\\mathcal{N}(k)$ 是节点 $k$ 的邻居集合。\n$$\n\\log q_k^*(z_k) \\propto \\log p(y_k \\mid z_k) + z_k \\left( b_k + w \\sum_{j \\in \\mathcal{N}(k)} \\mathbb{E}_{q_j}[z_j] \\right).\n$$\n因为 $\\mathbb{E}_{q_j}[z_j] = \\pi_j$，我们得到：\n$$\n\\log q_k^*(z_k) \\propto \\log p(y_k \\mid z_k) + z_k \\left( b_k + w \\sum_{j \\in \\mathcal{N}(k)} \\pi_j \\right).\n$$\n分布 $q_k^*$ 是伯努利分布。$\\pi_k = q_k^*(1)$ 的对数几率是：\n$$\n\\text{logit}(\\pi_k) = \\log\\left(\\frac{\\pi_k}{1-\\pi_k}\\right) = \\log\\left(\\frac{q_k^*(1)}{q_k^*(0)}\\right).\n$$\n这可以通过将 $\\log q_k^*(z_k)$ 的表达式在 $z_k=1$ 和 $z_k=0$ 处求值后的差得到：\n$$\n\\text{logit}(\\pi_k) = \\left( \\log p(y_k \\mid z_k=1) + b_k + w \\sum_{j \\in \\mathcal{N}(k)} \\pi_j \\right) - \\left( \\log p(y_k \\mid z_k=0) \\right).\n$$\n我们定义对数似然比项 $\\Delta L_k$：\n$$\n\\Delta L_k = \\log p(y_k \\mid z_k=1) - \\log p(y_k \\mid z_k=0).\n$$\n代入高斯密度函数：\n$$\n\\Delta L_k = \\left(-\\frac{1}{2} \\log(2\\pi(\\tau^2+\\sigma^2)) - \\frac{y_k^2}{2(\\tau^2+\\sigma^2)}\\right) - \\left(-\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{y_k^2}{2\\sigma^2}\\right)\n$$\n$$\n\\Delta L_k = -\\frac{1}{2} \\log\\left(\\frac{\\tau^2+\\sigma^2}{\\sigma^2}\\right) + \\frac{y_k^2}{2}\\left(\\frac{1}{\\sigma^2} - \\frac{1}{\\tau^2+\\sigma^2}\\right) = -\\frac{1}{2}\\log\\left(1+\\frac{\\tau^2}{\\sigma^2}\\right) + \\frac{y_k^2 \\tau^2}{2\\sigma^2(\\tau^2+\\sigma^2)}.\n$$\n$\\pi_k$ 的不动点方程为：\n$$\n\\pi_k = \\text{sigmoid}\\left( b_k + w \\sum_{j \\in \\mathcal{N}(k)} \\pi_j + \\Delta L_k \\right),\n$$\n其中 $b_k = \\log(\\rho/(1-\\rho))$。对所有 $k$ 迭代这些方程直至收敛。\n\n### 3. Bethe 近似 (含环信念传播)\n\nBethe 近似通过考虑成对相关性，提供了比平均场更准确的边缘概率估计。它可以使用和-积算法（信念传播）实现，即使在有环的图上（含环 BP）也可以。该算法通过在相邻节点之间传递消息来工作。\n\n后验 $p(\\boldsymbol{z} \\mid \\boldsymbol{y})$ 可以表示为节点势和边势的乘积：\n$$\np(\\boldsymbol{z} \\mid \\boldsymbol{y}) \\propto \\prod_{i=1}^n \\phi_i(z_i) \\prod_{(i,j)\\in E} \\psi_{ij}(z_i, z_j),\n$$\n其中节点势 $\\phi_i(z_i)$ 包含了局部证据和偏置：\n$$\n\\phi_i(z_i) = p(y_i \\mid z_i) \\exp(b_i z_i),\n$$\n而边势 $\\psi_{ij}(z_i, z_j)$ 捕捉了交互作用：\n$$\n\\psi_{ij}(z_i, z_j) = \\exp(w z_i z_j).\n$$\n一条消息 $m_{i \\to j}(z_j)$ 从节点 $i$ 发送到其邻居 $j$。它代表了 $i$ 对 $j$ 状态的信念。更新规则是：\n$$\nm_{i \\to j}(z_j) \\propto \\sum_{z_i \\in \\{0,1\\}} \\phi_i(z_i) \\psi_{ij}(z_i, z_j) \\prod_{k \\in \\mathcal{N}(i) \\setminus \\{j\\}} m_{k \\to i}(z_i).\n$$\n对于二元变量，使用对数比率消息 $\\mu_{i \\to j} = \\log\\frac{m_{i \\to j}(1)}{m_{i \\to j}(0)}$ 在数值上更稳定和高效。\n$\\mu_{i \\to j}$ 的更新推导如下：\n$$\n\\mu_{i \\to j} = \\log\\left( \\frac{\\sum_{z_i} \\phi_i(z_i) \\psi_{ij}(z_i, 1) \\prod_{k \\in \\mathcal{N}(i) \\setminus j} m_{k \\to i}(z_i)}{\\sum_{z_i} \\phi_i(z_i) \\psi_{ij}(z_i, 0) \\prod_{k \\in \\mathcal{N}(i) \\setminus j} m_{k \\to i}(z_i)} \\right).\n$$\n我们将节点势项的对数比率定义为 $\\lambda_i = \\log(\\phi_i(1)/\\phi_i(0)) = \\Delta L_i + b_i$。\n进入节点 $i$ 的、来自其除 $j$ 之外的邻居的对数比率消息之和是 $\\sum_{k \\in \\mathcal{N}(i) \\setminus j} \\mu_{k \\to i}$。\n令 $S_{i \\setminus j} = \\lambda_i + \\sum_{k \\in \\mathcal{N}(i) \\setminus j} \\mu_{k \\to i}$。这表示基于局部证据和来自除 $j$ 以外邻居的传入消息，$z_i=1$ 的对数几率。\n$\\mu_{i \\to j}$ 更新中分子的和变为：\n$$\n\\sum_{z_i \\in \\{0,1\\}} \\exp(w z_i) \\left( \\phi_i(z_i) \\prod_{k \\in \\mathcal{N}(i) \\setminus j} m_{k \\to i}(z_i) \\right) \\propto 1 \\cdot \\exp(S_{i \\setminus j} \\cdot 0) + \\exp(w) \\exp(S_{i \\setminus j} \\cdot 1) = 1 + \\exp(w+S_{i \\setminus j}).\n$$\n分母中的和，其中 $\\psi_{ij}(z_i, 0)=1$，变为：\n$$\n\\sum_{z_i \\in \\{0,1\\}} 1 \\cdot \\left( \\phi_i(z_i) \\prod_{k \\in \\mathcal{N}(i) \\setminus j} m_{k \\to i}(z_i) \\right) \\propto 1 + \\exp(S_{i \\setminus j}).\n$$\n因此，对数比率形式的消息更新方程为：\n$$\n\\mu_{i \\to j} = \\log\\left(\\frac{1 + \\exp(w+S_{i \\setminus j})}{1 + \\exp(S_{i \\setminus j})}\\right) = \\text{logaddexp}(0, w+S_{i \\setminus j}) - \\text{logaddexp}(0, S_{i \\setminus j}).\n$$\n对所有有向边迭代这些消息更新，直至收敛。\n\n一旦消息收敛，每个节点 $i$ 的（未归一化）边缘信念 $b_i(z_i)$ 通过将节点势与所有传入消息相乘来计算：\n$$\nb_i(z_i) \\propto \\phi_i(z_i) \\prod_{k \\in \\mathcal{N}(i)} m_{k \\to i}(z_i).\n$$\n信念的对数几率是势和消息的对数比率之和：\n$$\n\\text{logit}(b_i(1)) = \\log\\left(\\frac{b_i(1)}{b_i(0)}\\right) = \\lambda_i + \\sum_{k \\in \\mathcal{N}(i)} \\mu_{k \\to i}.\n$$\n最终的信念概率是：\n$$\nb_i(1) = \\text{sigmoid}\\left( \\lambda_i + \\sum_{k \\in \\mathcal{N}(i)} \\mu_{k \\to i} \\right).\n$$\n\n### 4. 算法总结\n\n两种方法都作为迭代算法实现。\n- **平均场**：为所有 $i$ 初始化 $\\pi_i$（例如，为 $0.5$）。重复使用推导出的不动点方程更新所有 $\\pi_i$，应用阻尼以辅助收敛，直到任何 $\\pi_i$ 的最大变化小于一个容差。\n- **含环 BP**：将所有消息 $\\mu_{i \\to j}$ 初始化为 $0$。重复使用推导出的不动点方程更新所有消息，应用阻尼，直到任何消息的最大变化小于一个容差。最后，使用收敛的消息计算信念 $b_i(1)$。\n\n为了数值稳定性，`sigmoid` 函数应能处理大的参数，而 `logaddexp` 应用于 LBP 更新。对于两种算法，都使用一个阻尼因子 $\\alpha \\in (0,1)$：$x_{\\text{new}} = (1-\\alpha) x_{\\text{old}} + \\alpha \\cdot \\text{update}(x_{\\text{old}})$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"grid_size\": (2, 2),\n            \"w\": 0.6,\n            \"rho\": 0.3,\n            \"tau_sq\": 1.0,\n            \"sigma_sq\": 0.25,\n            \"y\": np.array([1.2, -0.3, 0.7, 2.0]),\n        },\n        {\n            \"grid_size\": (3, 3),\n            \"w\": 0.0,\n            \"rho\": 0.2,\n            \"tau_sq\": 1.5,\n            \"sigma_sq\": 0.16,\n            \"y\": np.array([-0.1, 0.2, 0.0, 0.5, -1.0, 0.3, 0.7, -0.4, 0.1]),\n        },\n        {\n            \"grid_size\": (3, 3),\n            \"w\": 1.2,\n            \"rho\": 0.5,\n            \"tau_sq\": 0.5,\n            \"sigma_sq\": 1.0,\n            \"y\": np.array([0.0, 0.5, -0.5, 1.5, -1.2, 0.2, -0.8, 0.9, -0.1]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        R, C = params[\"grid_size\"]\n        n = R * C\n        adj =_build_graph(R, C)\n        \n        # Precompute constants\n        b = np.log(params[\"rho\"] / (1 - params[\"rho\"])) if params[\"rho\"] not in [0, 1] else (-np.inf if params[\"rho\"]==0 else np.inf)\n        if params[\"rho\"] == 0.5: b = 0.0 # Handle log(1) case exactly\n        \n        tau_sq = params[\"tau_sq\"]\n        sigma_sq = params[\"sigma_sq\"]\n        \n        term1 = -0.5 * np.log(1 + tau_sq / sigma_sq)\n        term2_coeff = (tau_sq) / (2 * sigma_sq * (tau_sq + sigma_sq))\n        delta_L = term1 + term2_coeff * params[\"y\"]**2\n        \n        # Run Mean-Field\n        pi = _run_mean_field(n, adj, b, params[\"w\"], delta_L)\n        \n        # Run Loopy BP\n        beliefs = _run_loopy_bp(n, adj, b, params[\"w\"], delta_L)\n        \n        # Compute Mean Absolute Difference\n        mad = np.mean(np.abs(pi - beliefs))\n        results.append(mad)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _build_graph(R, C):\n    \"\"\"Constructs the adjacency list for a grid graph.\"\"\"\n    n = R * C\n    adj = [[] for _ in range(n)]\n    for r in range(R):\n        for c in range(C):\n            idx = r * C + c\n            if r  0:\n                adj[idx].append((r - 1) * C + c)\n            if r  R - 1:\n                adj[idx].append((r + 1) * C + c)\n            if c  0:\n                adj[idx].append(r * C + (c - 1))\n            if c  C - 1:\n                adj[idx].append(r * C + (c + 1))\n    return adj\n\ndef _run_mean_field(n, adj, b, w, delta_L, tol=1e-8, max_iter=1000, damping=0.5):\n    \"\"\"Computes mean-field fixed point probabilities.\"\"\"\n    pi = np.full(n, 0.5)\n    \n    for _ in range(max_iter):\n        pi_old = pi.copy()\n        pi_new = np.zeros(n)\n        \n        for i in range(n):\n            neighbor_sum = np.sum(pi_old[adj[i]])\n            logit_arg = b + w * neighbor_sum + delta_L[i]\n            pi_new[i] = expit(logit_arg)\n        \n        pi = (1 - damping) * pi_old + damping * pi_new\n        \n        if np.max(np.abs(pi - pi_old))  tol:\n            break\n            \n    return pi\n\ndef _run_loopy_bp(n, adj, b, w, delta_L, tol=1e-8, max_iter=1000, damping=0.5):\n    \"\"\"Computes Bethe beliefs via loopy sum-product.\"\"\"\n    lambda_nodes = b + delta_L\n    \n    # Messages mu_{i-j} stored in a dictionary\n    messages = {}\n    for i in range(n):\n        for j in adj[i]:\n            messages[(i, j)] = 0.0\n\n    for _ in range(max_iter):\n        messages_old = messages.copy()\n        \n        max_diff = 0.0\n        \n        # Iterate over all directed edges\n        for i in range(n):\n            for j in adj[i]:\n                # Sum of incoming messages to i, excluding from j\n                incoming_sum = sum(messages_old[(k, i)] for k in adj[i] if k != j)\n                \n                S_ij = lambda_nodes[i] + incoming_sum\n                \n                # Update message mu_{i-j} using numerically stable logaddexp\n                # np.logaddexp(0, x) computes log(1 + exp(x))\n                raw_update = np.logaddexp(0, w + S_ij) - np.logaddexp(0, S_ij)\n                \n                new_msg = (1 - damping) * messages_old[(i, j)] + damping * raw_update\n                messages[(i, j)] = new_msg\n                \n                max_diff = max(max_diff, np.abs(new_msg - messages_old[(i, j)]))\n\n        if max_diff  tol:\n            break\n\n    # Compute final beliefs\n    beliefs = np.zeros(n)\n    for i in range(n):\n        total_incoming = sum(messages[(k, i)] for k in adj[i])\n        logit_arg = lambda_nodes[i] + total_incoming\n        beliefs[i] = expit(logit_arg)\n        \n    return beliefs\n\nsolve()\n```", "id": "3480139"}, {"introduction": "寻找最大后验（MAP）估计本身就是一个困难的非凸优化问题，尤其是在具有环路的图上。这个问题聚焦于一种实用的局部搜索算法——迭代条件模式（ICM），并引入了一种强大的启发式策略——同伦或连续化方法，来引导局部搜索避免陷入劣质的局部最优解。通过将此策略的结果与蛮力搜索得到的全局最优解进行比较，您可以定量评估这种高级优化技巧在复杂能量景观中寻找最优稀疏解的有效性 [@problem_id:3480144]。", "problem": "考虑压缩感知中的线性观测模型，其稀疏信号先验为 spike-and-slab 分布，支撑集指示变量服从马尔可夫随机场 (MRF)。设 $y \\in \\mathbb{R}^{m}$ 由 $y = A x + \\varepsilon$ 生成，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是已知的设计矩阵，$x \\in \\mathbb{R}^{n}$ 是未知的稀疏信号，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ 是方差参数为 $\\sigma^2  0$ 的高斯噪声。spike-and-slab 先验引入了二元支撑指示变量 $z \\in \\{0,1\\}^{n}$ 和连续的 slab 权重 $w \\in \\mathbb{R}^{n}$，使得 $x = z \\odot w$，其中 $\\odot$ 表示逐元素乘法。给定 $z$，slab 权重服从高斯先验 $w \\sim \\mathcal{N}(0, \\tau^2 I_n)$，其中 slab 方差为 $\\tau^2  0$。支撑向量 $z$ 服从链式图上的伊辛型马尔可夫随机场 (MRF) 先验，其具有成对交互作用：\n$$\np(z) \\propto \\exp\\left( \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i \\right),\n$$\n其中 $\\beta \\in \\mathbb{R}$ 是耦合强度，$h \\in \\mathbb{R}$ 是偏向稀疏性的一元场。\n\n对于固定的参数元组 $(\\sigma^2, \\tau^2, \\beta, h)$，最大后验 (MAP) 估计求解以下问题：\n$$\n\\max_{z \\in \\{0,1\\}^{n},\\, w \\in \\mathbb{R}^{n}} \\left\\{ -\\frac{1}{2\\sigma^2}\\|y - A(z \\odot w)\\|_2^2 - \\frac{1}{2\\tau^2}\\|w\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i \\right\\}.\n$$\n对于固定的 $z$，最优的 $w$ 是一个严格凸二次函数的唯一最小化子，因此可以通过求解限制在活动支撑集上的岭回归得到其闭式解。\n\n您必须提出并实现一种连续（同伦）策略，该策略从弱 MRF 耦合和较大的 slab 方差开始，并退火至目标值。具体来说，定义一个包含 $K$ 步的退火计划，该计划线性插值：\n$$\n\\beta_k = \\beta_{\\mathrm{init}} + \\frac{k}{K}(\\beta_{\\mathrm{tgt}} - \\beta_{\\mathrm{init}}), \\quad \\tau_k^2 = \\tau_{\\mathrm{init}}^2 - \\frac{k}{K}(\\tau_{\\mathrm{init}}^2 - \\tau_{\\mathrm{tgt}}^2),\n$$\n对于 $k \\in \\{0,1,\\dots,K\\}$，初始参数为 $\\beta_{\\mathrm{init}} = 0$ 和 $\\tau_{\\mathrm{init}}^2 = 10$。在每一步，计算：\n- 通过对所有 $z \\in \\{0,1\\}^{n}$ 进行精确枚举来计算全局 MAP 支撑集，并对每个 $z$ 优化 $w$ 的闭式解。\n- 使用迭代条件模式 (ICM) 计算局部追踪的 MAP 支撑集，ICM 从上一步的局部追踪支撑集初始化。ICM 交替进行单个 $z_i$ 的离散更新和针对更新后支撑集的 $w$ 的精确重新优化。\n\n同伦路径稳定性定义为：在所有步骤 $k \\in \\{1,\\dots,K\\}$ 中，局部追踪的 MAP 支撑集都等于该步骤的精确全局 MAP 支撑集。目标参数下的吸引盆被量化为：在 ICM 算法下，收敛到目标参数处精确全局 MAP 支撑集的随机初始支撑集的比例。\n\n从第一性原理出发，您的程序必须：\n1. 使用固定的伪随机种子 $0$ 抽取独立的标准正态分布项来构建 $A \\in \\mathbb{R}^{m \\times n}$，然后将每列归一化为单位欧几里得范数。\n2. 固定 $n = 8$ 和 $m = 5$。\n3. 定义一个确定性的基准稀疏信号 $x^{\\star} \\in \\mathbb{R}^{n}$，其活动索引位于位置 $2$、$4$ 和 $7$（基于 1 的索引），即 $x^{\\star}_{2} = 1.5$，$x^{\\star}_{4} = -1.0$，$x^{\\star}_{7} = 0.8$，所有其他项均为 $0$。\n4. 使用固定的伪随机种子 $1$ 和 $\\sigma_{\\mathrm{gen}}^2 = 0.01$ 生成观测值 $y = A x^{\\star} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{gen}}^2 I_m)$。\n5. 使用链式图 MRF 先验 $p(z)$，其边为 $(i, i+1)$，其中 $i \\in \\{1,\\dots,n-1\\}$。\n\n将迭代条件模式 (ICM) 局部优化定义如下：给定 $(\\sigma^2, \\tau^2, \\beta, h)$ 和一个初始化 $z^{(0)}$，重复对 $i \\in \\{1,\\dots,n\\}$ 进行坐标上升扫描；在每个坐标处，评估 $z_i = 0$ 和 $z_i = 1$ 时的后验目标函数值，同时保持其他 $z_j$ 不变，每次都为所得支撑集重新优化 $w$ 的闭式解；将 $z_i$ 更新为能产生更高后验目标值的选择。当一次完整的扫描未做任何更改或达到最多 $50$ 次扫描后停止。\n\n您的程序必须实现以上内容，并为每个测试用例计算两个输出：\n- 一个稳定性标志，如果局部追踪的路径在所有步骤中都与精确的全局路径匹配，则为 $1$，否则为 $0$。\n- 目标参数下的吸引盆分数，定义为 $R$ 个随机初始支撑集（每个都使用固定的伪随机种子 $2$ 从参数为 $0.5$ 的伯努利分布中独立同分布采样）在 ICM 下收敛到目标参数处的精确全局 MAP 支撑集的比例（四舍五入到三位小数）。\n\n测试套件。使用三个具有以下参数的测试用例：\n- 用例 1：$\\beta_{\\mathrm{tgt}} = 0.4$，$\\tau_{\\mathrm{tgt}}^2 = 1.0$，$h = -0.3$，$\\sigma^2 = 0.01$，$K = 6$，$R = 25$。\n- 用例 2：$\\beta_{\\mathrm{tgt}} = 0.0$，$\\tau_{\\mathrm{tgt}}^2 = 1.0$，$h = -0.3$，$\\sigma^2 = 0.01$，$K = 6$，$R = 25$。\n- 用例 3：$\\beta_{\\mathrm{tgt}} = 1.2$，$\\tau_{\\mathrm{tgt}}^2 = 0.3$，$h = -0.15$，$\\sigma^2 = 0.015$，$K = 6$，$R = 25$。\n\n算法要求：\n- 必须在每一步通过枚举所有 $z \\in \\{0,1\\}^{n}$ 来获得精确的全局 MAP 支撑集，并求解限制在活动支撑集上的 $w$ 的岭回归问题：\n$$\nw_S = \\left( \\frac{1}{\\sigma^2} A_S^{\\top} A_S + \\frac{1}{\\tau^2} I_{|S|} \\right)^{-1} \\left( \\frac{1}{\\sigma^2} A_S^{\\top} y \\right),\n$$\n其中 $S = \\{ i \\mid z_i = 1 \\}$ 且 $A_S$ 是由 $A$ 中索引在 $S$ 内的列组成的子矩阵。\n- 对于给定的 $z$，需要最大化的后验目标函数为\n$$\n\\mathcal{L}(z) = -\\frac{1}{2\\sigma^2}\\left\\| y - A(z \\odot w_S) \\right\\|_2^2 - \\frac{1}{2\\tau^2}\\left\\| w_S \\right\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i,\n$$\n其中 $w_S$ 如上所述，且对于 $i \\notin S$，有 $w_{i} = 0$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例的结果，格式为一个逗号分隔的 Python 风格列表，内含三个子列表。每个子列表包含两个条目：稳定性标志（整数 $0$ 或 $1$）和吸引盆分数（浮点数，四舍五入到三位小数）。例如，输出应类似于 $[[1,0.920],[0,0.480],[1,0.840]]$，但使用您的程序计算的实际值。\n\n此问题不涉及物理单位或角度。所有数值必须视为无量纲。通过在指定位置使用规定的伪随机种子来确保确定性行为，并且不要读取任何外部输入。", "solution": "该问题要求在贝叶斯框架下对稀疏信号进行最大后验 (MAP) 估计的分析。这涉及到信号上的 spike-and-slab 先验与其支撑集上的马尔可夫随机场 (MRF) 先验相结合。分析将在同伦或连续方案中比较一种全局优化方法（精确枚举）和一种局部优化方法（迭代条件模式，ICM）。\n\n### 基于原理的设计\n\n#### 1. 贝叶斯模型构建\n该问题始于压缩感知中的标准线性观测模型：\n$$\ny = A x + \\varepsilon\n$$\n其中 $y \\in \\mathbb{R}^{m}$ 是测量向量，$A \\in \\mathbb{R}^{m \\times n}$ 是设计矩阵，$x \\in \\mathbb{R}^{n}$ 是待恢复的稀疏信号，而 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ 是方差为 $\\sigma^2$ 的加性高斯噪声。\n\n$x$ 的稀疏性通过 spike-and-slab 先验来建模。该先验引入一个二元支撑向量 $z \\in \\{0, 1\\}^n$，其中 $z_i=1$ 表示信号的第 $i$ 个分量 $x_i$ 非零，而 $z_i=0$ 表示其为零。信号随后表示为 $x = z \\odot w$，其中 $\\odot$ 是逐元素乘积，$w \\in \\mathbb{R}^n$ 是“slab”系数。先验分布为：\n-   对于 slab 权重 $w$，给定支撑集 $z$，对活动系数假设一个高斯先验：$p(w_i|z_i=1) \\sim \\mathcal{N}(0, \\tau^2)$。这等价于对整个向量 $w$ 的先验 $w \\sim \\mathcal{N}(0, \\tau^2 I_n)$，因为当 $z_i=0$ 时 $x_i=0$，所以 $w_i$ 的后验将无关紧要。\n-   对于支撑向量 $z$，在链式图上定义了一个伊辛型马尔可夫随机场 (MRF) 先验。该先验鼓励结构化稀疏性，即信号中的相邻系数更可能同时处于活动或非活动状态。其概率质量函数为：\n    $$\n    p(z|\\beta, h) \\propto \\exp\\left( \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i \\right)\n    $$\n    参数 $\\beta$ 控制相邻支撑变量之间的耦合强度，而 $h$ 是一个控制整体稀疏水平的全局参数。\n\n#### 2. MAP 估计与剖面目标函数\n目标是通过最大化后验分布 $p(z, w | y)$ 来找到支撑集 $z$ 和权重 $w$ 的 MAP 估计。这等价于最大化对数后验，根据贝叶斯定理，它与对数似然和对数先验之和成正比：\n$$\n\\log p(z, w | y) \\propto \\log p(y | w, z) + \\log p(w | z) + \\log p(z)\n$$\n代入高斯似然和指定的先验，需要最大化的目标函数为：\n$$\n\\mathcal{J}(z, w) = -\\frac{1}{2\\sigma^2}\\|y - A(z \\odot w)\\|_2^2 - \\frac{1}{2\\tau^2}\\|w\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i\n$$\n一个关键的见解是，对于固定的支撑向量 $z$，$w$ 的优化问题是二次且严格凸的。设 $S = \\{i | z_i=1\\}$ 为活动索引集，其基数为 $|S|$。设 $w_S$ 是对应这些索引的权重向量，$A_S$ 是由 $A$ 中索引为 $S$ 的列组成的子矩阵。$w_S$ 的目标函数为：\n$$\n\\mathcal{J}(w_S | z) = -\\frac{1}{2\\sigma^2}\\|y - A_S w_S\\|_2^2 - \\frac{1}{2\\tau^2}\\|w_S\\|_2^2\n$$\n通过将梯度设为零，可以找到唯一的最大化子 $w_S^*$，这得出了一个岭回归问题的解：\n$$\nw_S^*(z) = \\left( A_S^{\\top} A_S + \\frac{\\sigma^2}{\\tau^2} I_{|S|} \\right)^{-1} A_S^{\\top} y\n$$\n通过将这个最优的 $w_S^*(z)$ 代回完整的目标函数，我们得到了一个仅依赖于离散支撑向量 $z$ 的剖面目标函数 $\\mathcal{L}(z)$：\n$$\n\\mathcal{L}(z) = \\mathcal{J}(z, w^*(z)) = -\\frac{1}{2\\sigma^2}\\|y - A_S w_S^*(z)\\|_2^2 - \\frac{1}{2\\tau^2}\\|w_S^*(z)\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i\n$$\nMAP 估计问题现在简化为在 $z \\in \\{0, 1\\}^n$ 上的组合优化问题：\n$$\nz_{\\text{MAP}} = \\arg\\max_{z \\in \\{0,1\\}^n} \\mathcal{L}(z)\n$$\n\n#### 3. 优化与分析策略\n该问题要求在连续框架内比较两种寻找 $z_{\\text{MAP}}$ 的方法。\n\n-   **全局优化**：由于信号维度 $n=8$ 很小，$z$ 的搜索空间大小为 $2^8 = 256$，是可控的。我们可以通过计算每个可能的 $z$ 的 $\\mathcal{L}(z)$ 并选择产生最大值的那个，来找到真正的全局 MAP 支撑集 $z_{\\text{global}}$。这为比较提供了一个黄金标准。\n\n-   **局部优化 (ICM)**：对于更大的 $n$，枚举是不可行的。迭代条件模式 (ICM) 是一种贪婪的局部搜索算法。它一次一个坐标地迭代优化目标函数。对于每个 $z_i$，在保持所有其他 $z_j$（$j \\neq i$）固定的情况下，ICM 计算 $z_i=0$ 和 $z_i=1$ 两种情况下的 $\\mathcal{L}(z)$，并将 $z_i$ 更新为产生更高目标值的那个值。一次完整的扫描包括遍历所有 $i \\in \\{1, \\dots, n\\}$。当一次扫描不产生任何变化时，算法终止，保证收敛到 $\\mathcal{L}(z)$ 的一个局部最大值。\n\n-   **连续（同伦）方法**：像 ICM 这样的局部搜索方法容易陷入较差的局部最优解。连续策略可以缓解这个问题。其思想是从一个“更简单”版本的问题开始，然后逐渐将参数退火至其目标值。在这里，我们从弱 MRF 耦合（$\\beta_{\\mathrm{init}}=0$）和大的 slab 方差（$\\tau_{\\mathrm{init}}^2=10$，这会降低 $w$ 的先验权重）开始。参数在 $K$ 步内线性插值到其目标值 $(\\beta_{\\mathrm{tgt}}, \\tau_{\\mathrm{tgt}}^2)$。第 $k-1$ 步的局部最优解被用作第 $k$ 步 ICM 搜索的起点。这种路径跟踪过程有助于局部搜索保持在全局最优解的吸引盆附近。\n\n#### 4. 评估指标\n\n-   **同伦路径稳定性**：该指标评估连续策略的有效性。如果在退火计划的每一步，局部追踪的 ICM 解都与全局最优解匹配，则宣布为稳定。稳定性标志为 $1$ 表示成功，为 $0$ 表示失败。\n\n-   **吸引盆**：该指标量化了目标参数下最终优化景观的“优良性”。它通过运行 ICM 时，收敛到真实全局 MAP 支撑集的随机初始支撑集的比例来估计。较大的吸引盆分数表明从随机起点更容易找到全局最优解。\n\n实现将首先生成固定的合成数据（$A, y$）。然后，对于每个测试用例，它将执行同伦分析以确定路径稳定性，并随后计算最终参数下的吸引盆分数。所有随机过程都由固定的种子控制，以确保可复现性。", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the three test cases.\n    It orchestrates data generation, and for each case, runs the homotopy\n    analysis and basin of attraction calculation.\n    \"\"\"\n    \n    # Problem constants\n    n, m = 8, 5\n    sigma_gen_sq = 0.01\n\n    # Generate synthetic data A and y (fixed for all test cases)\n    rng_A = np.random.default_rng(0)\n    A = rng_A.standard_normal((m, n))\n    A /= np.linalg.norm(A, axis=0)\n\n    x_star = np.zeros(n)\n    # One-based indexing in problem: 2, 4, 7\n    # Zero-based indexing in python: 1, 3, 6\n    x_star[1] = 1.5\n    x_star[3] = -1.0\n    x_star[6] = 0.8\n\n    rng_eps = np.random.default_rng(1)\n    epsilon = rng_eps.normal(0, np.sqrt(sigma_gen_sq), m)\n    y = A @ x_star + epsilon\n\n    # Test cases\n    test_cases = [\n        # (beta_tgt, tau2_tgt, h, sigma2, K, R)\n        (0.4, 1.0, -0.3, 0.01, 6, 25),\n        (0.0, 1.0, -0.3, 0.01, 6, 25),\n        (1.2, 0.3, -0.15, 0.015, 6, 25),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = process_case(params, A, y, n)\n        all_results.append(result)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef compute_log_posterior(z, sigma2, tau2, beta, h, y, A):\n    \"\"\"\n    Computes the profiled log-posterior objective L(z) for a given support z.\n    \"\"\"\n    z = np.array(z)\n    active_set = np.where(z == 1)[0]\n    k = len(active_set)\n    n = len(z)\n    \n    mrf_term = beta * np.sum(z[:-1] * z[1:]) + h * np.sum(z)\n\n    if k == 0:\n        log_posterior = -0.5 / sigma2 * np.dot(y, y)\n        return log_posterior + mrf_term\n\n    A_S = A[:, active_set]\n    \n    lambda_reg = sigma2 / tau2\n    \n    try:\n        # Ridge regression solution for w_S\n        mat_inv = np.linalg.inv(A_S.T @ A_S + lambda_reg * np.eye(k))\n        w_S = mat_inv @ (A_S.T @ y)\n\n        residual = y - A_S @ w_S\n        log_lik = -0.5 / sigma2 * np.dot(residual, residual)\n        w_prior = -0.5 / tau2 * np.dot(w_S, w_S)\n        \n        log_posterior = log_lik + w_prior\n        return log_posterior + mrf_term\n    except np.linalg.LinAlgError:\n        return -np.inf\n\ndef find_global_map_support(params, y, A, n):\n    \"\"\"\n    Finds the global MAP support by enumerating all 2^n possibilities.\n    \"\"\"\n    sigma2, tau2, beta, h = params\n    best_z = None\n    max_log_post = -np.inf\n\n    for z_tuple in itertools.product([0, 1], repeat=n):\n        z = np.array(z_tuple)\n        log_post = compute_log_posterior(z, sigma2, tau2, beta, h, y, A)\n        if log_post  max_log_post:\n            max_log_post = log_post\n            best_z = z\n            \n    return best_z\n\ndef run_icm(z_init, params, y, A, n, max_sweeps=50):\n    \"\"\"\n    Performs Iterated Conditional Modes (ICM) local search.\n    \"\"\"\n    sigma2, tau2, beta, h = params\n    z_current = np.array(z_init, dtype=int)\n\n    for _ in range(max_sweeps):\n        changed = False\n        for i in range(n):\n            z_off = z_current.copy()\n            z_off[i] = 0\n            log_post_off = compute_log_posterior(z_off, sigma2, tau2, beta, h, y, A)\n\n            z_on = z_current.copy()\n            z_on[i] = 1\n            log_post_on = compute_log_posterior(z_on, sigma2, tau2, beta, h, y, A)\n\n            current_val = z_current[i]\n            if log_post_on  log_post_off:\n                if current_val == 0:\n                    z_current[i] = 1\n                    changed = True\n            else:\n                if current_val == 1:\n                    z_current[i] = 0\n                    changed = True\n        \n        if not changed:\n            break\n            \n    return z_current\n\ndef process_case(case_params, A, y, n):\n    \"\"\"\n    Executes the full analysis for a single test case.\n    \"\"\"\n    beta_tgt, tau2_tgt, h, sigma2, K, R = case_params\n    \n    # Homotopy parameters\n    beta_init = 0.0\n    tau2_init = 10.0\n    \n    betas = np.linspace(beta_init, beta_tgt, K + 1)\n    tau2s = np.linspace(tau2_init, tau2_tgt, K + 1)\n\n    # --- Homotopy Path Stability Analysis ---\n    stability_flag = 1\n    \n    # Step k=0\n    params_k0 = (sigma2, tau2s[0], betas[0], h)\n    z_global_k0 = find_global_map_support(params_k0, y, A, n)\n    z_local_prev = z_global_k0\n    z_global_target = None # Will store the final global MAP\n\n    for k in range(1, K + 1):\n        params_k = (sigma2, tau2s[k], betas[k], h)\n        z_global_k = find_global_map_support(params_k, y, A, n)\n        \n        # Track local MAP from previous step's solution\n        z_local_k = run_icm(z_local_prev, params_k, y, A, n)\n        \n        if not np.array_equal(z_local_k, z_global_k):\n            stability_flag = 0\n            \n        z_local_prev = z_local_k\n        if k == K:\n            z_global_target = z_global_k\n            \n    if K == 0: # Handle edge case where no annealing steps\n        z_global_target = find_global_map_support((sigma2, tau2_tgt, beta_tgt, h), y, A, n)\n\n    # --- Basin of Attraction Analysis ---\n    target_params = (sigma2, tau2s[K], betas[K], h)\n    convergence_count = 0\n    rng_basin = np.random.default_rng(2)\n\n    for _ in range(R):\n        z_init = rng_basin.integers(0, 2, size=n, dtype=int)\n        z_converged = run_icm(z_init, target_params, y, A, n)\n        if np.array_equal(z_converged, z_global_target):\n            convergence_count += 1\n            \n    basin_fraction = round(convergence_count / R, 3)\n\n    return [stability_flag, basin_fraction]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3480144"}]}