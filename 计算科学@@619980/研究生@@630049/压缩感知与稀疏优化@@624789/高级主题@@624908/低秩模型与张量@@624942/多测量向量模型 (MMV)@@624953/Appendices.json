{"hands_on_practices": [{"introduction": "许多解决多测量向量（MMV）问题的高级算法，特别是基于凸优化的算法，都依赖于一个名为“邻近算子”（proximal operator）的关键构件。该算子用于处理目标函数中的非光滑部分，在MMV模型中，这通常是促进联合稀疏性的 $\\ell_{2,1}$ 范数。本练习将指导你推导这一至关重要的算子，并将其应用于一个具体示例，为理解和实现现代恢复算法打下坚实的基础。", "problem": "考虑用于联合稀疏恢复的多测量向量 (MMV) 模型，其中未知系数矩阵 $X \\in \\mathbb{R}^{n \\times L}$ 具有行稀疏结构，数据保真度通过传感矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和测量值 $Y \\in \\mathbb{R}^{m \\times L}$ 来建模。一个广泛用于估计 $X$ 的凸目标函数是二次数据项和混合 $\\ell_{2,1}$ 范数之和，其定义为 $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_{2}$。在近端分裂方法中，需要求解函数 $g(X) = \\tau \\|X\\|_{2,1}$（其中 $\\tau  0$）的近端算子。从近端算子和混合 $\\ell_{2,1}$ 范数的定义出发，通过逐行分析优化问题并建立最优性条件，推导给定矩阵 $V \\in \\mathbb{R}^{n \\times L}$ 的 $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V)$ 的闭式表达式。然后将你的推导应用于具体矩阵\n$$\nV = \\begin{pmatrix}\n3  4  0  0 \\\\\n1  2  2  1 \\\\\n1  1  0  0\n\\end{pmatrix}\n$$\n其中参数 $\\tau = 2$，并计算近端输出 $X = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V)$ 的弗罗贝尼乌斯范数的平方。以精确的闭式表达式给出你的最终答案。不需要四舍五入。", "solution": "该问题要求推导混合 $\\ell_{2,1}$ 范数的近端算子，并将其应用于一个具体的矩阵 $V$ 和参数 $\\tau$。\n\n首先，我们建立近端算子的正式定义。对于一个函数 $g: \\mathbb{R}^{n \\times L} \\to \\mathbb{R}$，其在点 $V \\in \\mathbb{R}^{n \\times L}$ 处的近端算子被定义为以下优化问题的唯一解：\n$$ \\operatorname{prox}_{g}(V) = \\arg\\min_{X \\in \\mathbb{R}^{n \\times L}} \\left( g(X) + \\frac{1}{2} \\|X-V\\|_F^2 \\right) $$\n其中 $\\|\\cdot\\|_F$ 表示弗罗贝尼乌斯范数。\n\n在本问题中，函数为 $g(X) = \\tau \\|X\\|_{2,1}$，其中 $\\tau  0$ 是一个标量参数，$\\|X\\|_{2,1}$ 是混合 $\\ell_{2,1}$ 范数，定义为矩阵 $X$ 各行欧几里得范数之和：\n$$ \\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_2 $$\n这里，$X_{i,:}$ 表示矩阵 $X$ 的第 $i$ 行向量。\n\n因此，函数 $g(X)$ 的近端算子的优化问题是：\n$$ X^* = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V) = \\arg\\min_{X} \\left( \\tau \\sum_{i=1}^{n} \\|X_{i,:}\\|_2 + \\frac{1}{2} \\|X-V\\|_F^2 \\right) $$\n弗罗贝尼乌斯范数的平方是其矩阵参数各行欧几里得范数平方之和：\n$$ \\|X-V\\|_F^2 = \\sum_{i=1}^{n} \\|X_{i,:} - V_{i,:}\\|_2^2 $$\n将这个分解代入目标函数，我们得到：\n$$ X^* = \\arg\\min_{X} \\sum_{i=1}^{n} \\left( \\tau \\|X_{i,:}\\|_2 + \\frac{1}{2} \\|X_{i,:} - V_{i,:}\\|_2^2 \\right) $$\n目标函数是若干项之和，其中第 $i$ 项仅依赖于 $X$ 的第 $i$ 行 $X_{i,:}$。这种可分离性允许我们通过独立地对每一行进行最小化来最小化整个函数。对于每一行 $i \\in \\{1, \\dots, n\\}$，我们求解：\n$$ X_{i,:}^* = \\arg\\min_{x_i \\in \\mathbb{R}^{1 \\times L}} \\left( \\tau \\|x_i\\|_2 + \\frac{1}{2} \\|x_i - v_i\\|_2^2 \\right) $$\n为简化符号，我们令 $x_i = X_{i,:}$ 和 $v_i = V_{i,:}$。这个子问题是缩放后的欧几里得范数 $\\tau \\|\\cdot\\|_2$ 的近端算子。\n\n为了解决这个子问题，我们使用次微分计算。令 $J(x_i) = \\tau \\|x_i\\|_2 + \\frac{1}{2} \\|x_i - v_i\\|_2^2$。最优性的一阶充要条件是 $0 \\in \\partial J(x_i^*)$。$J$ 的次微分是 $\\partial J(x_i) = \\tau \\partial \\|x_i\\|_2 + \\nabla \\left(\\frac{1}{2} \\|x_i - v_i\\|_2^2\\right) = \\tau \\partial \\|x_i\\|_2 + (x_i - v_i)$。\n欧几里得范数的次微分是：\n$$ \\partial \\|x_i\\|_2 = \\begin{cases} \\{ u \\in \\mathbb{R}^{1 \\times L} \\mid \\|u\\|_2 \\le 1 \\}  \\text{若 } x_i = 0 \\\\ \\{ \\frac{x_i}{\\|x_i\\|_2} \\}  \\text{若 } x_i \\ne 0 \\end{cases} $$\n我们考虑解 $x_i^*$ 的两种情况：\n\n情况 1：$x_i^* \\ne 0$。\n最优性条件是 $0 = \\tau \\frac{x_i^*}{\\|x_i^*\\|_2} + x_i^* - v_i$。重新整理这个方程得到 $v_i = x_i^* \\left(1 + \\frac{\\tau}{\\|x_i^*\\|_2}\\right)$。这表明 $v_i$ 和 $x_i^*$ 是共线的。对两边取欧几里得范数，得到 $\\|v_i\\|_2 = \\|x_i^*\\|_2 \\left(1 + \\frac{\\tau}{\\|x_i^*\\|_2}\\right) = \\|x_i^*\\|_2 + \\tau$。因此，$\\|x_i^*\\|_2 = \\|v_i\\|_2 - \\tau$。要使这是一个非零解，我们必须有 $\\|v_i\\|_2 - \\tau  0$，即 $\\|v_i\\|_2  \\tau$。如果这个条件成立，我们可以从共线性中找到 $x_i^*$：$x_i^* = \\frac{\\|x_i^*\\|_2}{\\|v_i\\|_2} v_i = \\frac{\\|v_i\\|_2 - \\tau}{\\|v_i\\|_2} v_i = \\left(1 - \\frac{\\tau}{\\|v_i\\|_2}\\right) v_i$。\n\n情况 2：$x_i^* = 0$。\n最优性条件变为 $0 \\in \\tau \\partial \\|0\\|_2 + (0 - v_i)$，这意味着 $v_i \\in \\tau \\partial \\|0\\|_2$。这表示 $v_i$ 必须属于集合 $\\{ u \\mid \\|u\\|_2 \\le \\tau \\}$，所以 $\\|v_i\\|_2 \\le \\tau$。\n\n结合这两种情况，每一行 $X_{i,:}^*$ 的闭式解是一个块软阈值操作：\n$$ X_{i,:}^* = \\begin{cases} \\left(1 - \\frac{\\tau}{\\|V_{i,:}\\|_2}\\right) V_{i,:}  \\text{若 } \\|V_{i,:}\\|_2  \\tau \\\\ 0  \\text{若 } \\|V_{i,:}\\|_2 \\le \\tau \\end{cases} $$\n这可以紧凑地写成 $X_{i,:}^* = \\max\\left(0, 1 - \\frac{\\tau}{\\|V_{i,:}\\|_2}\\right)V_{i,:}$。\n\n现在我们将这个公式应用于给定的矩阵 $V$ 和参数 $\\tau = 2$：\n$$ V = \\begin{pmatrix} 3  4  0  0 \\\\ 1  2  2  1 \\\\ 1  1  0  0 \\end{pmatrix} $$\n令 $X = \\operatorname{prox}_{2 \\|\\cdot\\|_{2,1}}(V)$。我们逐行计算 $X$。\n\n对于第一行，$V_{1,:} = (3, 4, 0, 0)$：\n其 $\\ell_2$-范数为 $\\|V_{1,:}\\|_2 = \\sqrt{3^2 + 4^2 + 0^2 + 0^2} = \\sqrt{25} = 5$。\n由于 $\\|V_{1,:}\\|_2 = 5  \\tau = 2$，我们应用收缩公式：\n$X_{1,:} = \\left(1 - \\frac{2}{5}\\right) V_{1,:} = \\frac{3}{5} (3, 4, 0, 0)$。\n\n对于第二行，$V_{2,:} = (1, 2, 2, 1)$：\n其 $\\ell_2$-范数为 $\\|V_{2,:}\\|_2 = \\sqrt{1^2 + 2^2 + 2^2 + 1^2} = \\sqrt{10}$。\n由于 $\\|V_{2,:}\\|_2 = \\sqrt{10} \\approx 3.162  \\tau = 2$，我们再次应用收缩公式：\n$X_{2,:} = \\left(1 - \\frac{2}{\\sqrt{10}}\\right) V_{2,:}$。\n\n对于第三行，$V_{3,:} = (1, 1, 0, 0)$：\n其 $\\ell_2$-范数为 $\\|V_{3,:}\\|_2 = \\sqrt{1^2 + 1^2 + 0^2 + 0^2} = \\sqrt{2}$。\n由于 $\\|V_{3,:}\\|_2 = \\sqrt{2} \\approx 1.414 \\le \\tau = 2$，该行被置为零：\n$X_{3,:} = (0, 0, 0, 0)$。\n\n问题要求计算所得矩阵 $X$ 的弗罗贝尼乌斯范数的平方，即 $\\|X\\|_F^2 = \\sum_{i=1}^{3} \\|X_{i,:}\\|_2^2$。\n我们计算 $X$ 各行的范数平方。如果 $\\|V_{i,:}\\|_2  \\tau$，则 $\\|X_{i,:}\\|_2 = \\left(1 - \\frac{\\tau}{\\|V_{i,:}\\|_2}\\right) \\|V_{i,:}\\|_2 = \\|V_{i,:}\\|_2 - \\tau$。因此，$\\|X_{i,:}\\|_2^2 = (\\|V_{i,:}\\|_2 - \\tau)^2$。如果 $\\|V_{i,:}\\|_2 \\le \\tau$，则 $\\|X_{i,:}\\|_2^2 = 0$。\n\n对于第一行：\n$\\|X_{1,:}\\|_2^2 = (\\|V_{1,:}\\|_2 - \\tau)^2 = (5 - 2)^2 = 3^2 = 9$。\n\n对于第二行：\n$\\|X_{2,:}\\|_2^2 = (\\|V_{2,:}\\|_2 - \\tau)^2 = (\\sqrt{10} - 2)^2 = (\\sqrt{10})^2 - 2(2)\\sqrt{10} + 2^2 = 10 - 4\\sqrt{10} + 4 = 14 - 4\\sqrt{10}$。\n\n对于第三行：\n由于 $\\|V_{3,:}\\|_2 \\le \\tau$，我们有 $\\|X_{3,:}\\|_2^2 = 0$。\n\n$X$ 的弗罗贝尼乌斯范数的平方是这些值的总和：\n$\\|X\\|_F^2 = \\|X_{1,:}\\|_2^2 + \\|X_{2,:}\\|_2^2 + \\|X_{3,:}\\|_2^2 = 9 + (14 - 4\\sqrt{10}) + 0 = 23 - 4\\sqrt{10}$。", "answer": "$$\\boxed{23 - 4\\sqrt{10}}$$", "id": "3460758"}, {"introduction": "在上一练习的基础上，我们现在将看到邻近算子如何在一个完整的算法中发挥作用。本练习要求对邻近梯度法（也称为迭代软阈值算法，ISTA）执行一次完整的迭代。这将具体展示算法如何依次处理MMV目标函数中的光滑部分（数据保真项）和非光滑部分（稀疏正则项），让你清晰地了解迭代收缩-阈值算法的内部工作机制。", "problem": "考虑多测量向量（MMV）模型 $Y = A X_{\\star} + E$，其中 $Y \\in \\mathbb{R}^{m \\times L}$，$A \\in \\mathbb{R}^{m \\times n}$，且 $X_{\\star} \\in \\mathbb{R}^{n \\times L}$ 是行稀疏的。假设估计值 $X \\in \\mathbb{R}^{n \\times L}$ 是通过最小化正则化最小二乘目标函数得到的\n$$\nf(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2} + \\lambda \\|X\\|_{2,1},\n$$\n其中 $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_{2}$ 是促进 $L$ 个测量向量联合行稀疏性的混合 $\\ell_{2,1}$ 范数。从初始化 $X^{(0)} = 0_{n \\times L}$ 开始，对 $f$ 执行一次完整的、具有恒定步长 $t$ 的近端梯度迭代，该迭代包括对平滑数据拟合项的梯度步，然后是 $\\ell_{2,1}$ 正则化项的近端算子。使用以下具体数据：\n$$\nA = \\begin{pmatrix}\n2  0  1 \\\\\n0  1  -1\n\\end{pmatrix}, \\quad\nY = \\begin{pmatrix}\n1  3 \\\\\n2  -1\n\\end{pmatrix}, \\quad\nX^{(0)} = \\begin{pmatrix}\n0  0 \\\\\n0  0 \\\\\n0  0\n\\end{pmatrix}, \\quad\nt = 0.5, \\quad \\lambda = 0.8.\n$$\n通过执行一步完整的带有 $\\ell_{2,1}$ 收缩的近端梯度步骤，数值计算更新后的迭代值 $X^{(1)}$，并提供 $X^{(1)}$ 的条目，四舍五入到四位有效数字。将最终条目表示为无量纲数。", "solution": "该问题是计算近端梯度法的一次迭代，以最小化目标函数 $f(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2} + \\lambda \\|X\\|_{2,1}$。该方法也称为迭代收缩阈值算法（ISTA）。目标函数是一个平滑可微项 $g(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2}$ 和一个非平滑凸正则化项 $h(X) = \\lambda \\|X\\|_{2,1}$ 的和。\n\n近端梯度迭代的更新规则如下：\n$$\nX^{(k+1)} = \\text{prox}_{t h}(X^{(k)} - t \\nabla g(X^{(k)}))\n$$\n其中 $t$ 是步长，$\\text{prox}_{t h}$ 是函数 $t h(X)$ 的近端算子。对于 $h(X) = \\lambda \\|X\\|_{2,1}$，近端算子是 $\\text{prox}_{t\\lambda\\|\\cdot\\|_{2,1}}$。我们需要从 $X^{(0)} = 0_{n\\times L}$ 开始计算 $X^{(1)}$。\n\n迭代过程分两步：\n1. 对平滑项 $g(X)$ 进行梯度下降步。\n2. 应用非平滑项 $h(X)$ 的近端算子。\n\n让我们逐步进行。\n\n首先，我们计算平滑项 $g(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2}$ 的梯度。弗罗贝尼乌斯范数的平方是各项平方之和，$\\|M\\|_F^2 = \\text{Tr}(M^T M)$。$g(X)$ 关于矩阵 $X$ 的梯度为：\n$$\n\\nabla g(X) = A^T (A X - Y)\n$$\n我们在初始点 $X^{(0)} = 0_{n \\times L}$ 处计算该梯度：\n$$\n\\nabla g(X^{(0)}) = A^T (A X^{(0)} - Y) = A^T (A \\cdot 0_{n \\times L} - Y) = -A^T Y\n$$\n给定的数据是：\n$$\nA = \\begin{pmatrix} 2  0  1 \\\\ 0  1  -1 \\end{pmatrix}, \\quad Y = \\begin{pmatrix} 1  3 \\\\ 2  -1 \\end{pmatrix}\n$$\n$A$ 的转置是：\n$$\nA^T = \\begin{pmatrix} 2  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix}\n$$\n现在，我们计算乘积 $-A^T Y$：\n$$\n-A^T Y = - \\begin{pmatrix} 2  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1  3 \\\\ 2  -1 \\end{pmatrix} = - \\begin{pmatrix} 2 \\cdot 1 + 0 \\cdot 2  2 \\cdot 3 + 0 \\cdot (-1) \\\\ 0 \\cdot 1 + 1 \\cdot 2  0 \\cdot 3 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + (-1) \\cdot 2  1 \\cdot 3 + (-1) \\cdot (-1) \\end{pmatrix} = - \\begin{pmatrix} 2  6 \\\\ 2  -1 \\\\ -1  4 \\end{pmatrix} = \\begin{pmatrix} -2  -6 \\\\ -2  1 \\\\ 1  -4 \\end{pmatrix}\n$$\n\n接下来，我们执行梯度下降步。设 $Z$ 是此步骤后的矩阵：\n$$\nZ = X^{(0)} - t \\nabla g(X^{(0)}) = 0_{n \\times L} - t (-A^T Y) = t A^T Y\n$$\n使用给定的步长 $t = 0.5$，我们有：\n$$\nZ = 0.5 \\begin{pmatrix} 2  6 \\\\ 2  -1 \\\\ -1  4 \\end{pmatrix} = \\begin{pmatrix} 1  3 \\\\ 1  -0.5 \\\\ -0.5  2 \\end{pmatrix}\n$$\n\n第二步是应用近端算子。我们需要计算 $X^{(1)} = \\text{prox}_{t\\lambda\\|\\cdot\\|_{2,1}}(Z)$。正则化参数为 $\\lambda = 0.8$，因此近端算子的参数是 $\\mu = t\\lambda = 0.5 \\times 0.8 = 0.4$。\n$\\ell_{2,1}$ 范数的近端算子 $\\text{prox}_{\\mu\\|\\cdot\\|_{2,1}}(Z)$ 逐行作用。对于 $Z$ 的每一行 $Z_{i,:}$，输出矩阵的相应行由块软阈值给出：\n$$\n(X^{(1)})_{i,:} = \\left(1 - \\frac{\\mu}{\\|Z_{i,:}\\|_2}\\right)_+ Z_{i,:}\n$$\n其中 $(c)_+ = \\max(c, 0)$。\n\n我们将此公式应用于 $Z$ 的每一行：\n\n第1行：$Z_{1,:} = \\begin{pmatrix} 1  3 \\end{pmatrix}$\n其 $\\ell_2$ 范数为 $\\|Z_{1,:}\\|_2 = \\sqrt{1^2 + 3^2} = \\sqrt{10}$。\n缩放因子为 $s_1 = 1 - \\frac{0.4}{\\sqrt{10}}$。由于 $\\sqrt{10} \\approx 3.162$ 且 $0.4 / \\sqrt{10}  1$，该因子为正。\n$s_1 \\approx 1 - \\frac{0.4}{3.162277...} \\approx 1 - 0.126491... = 0.873509...$\n$(X^{(1)})_{1,:} = s_1 Z_{1,:} \\approx 0.873509 \\times \\begin{pmatrix} 1  3 \\end{pmatrix} = \\begin{pmatrix} 0.873509  2.620527 \\end{pmatrix}$。\n\n第2行：$Z_{2,:} = \\begin{pmatrix} 1  -0.5 \\end{pmatrix}$\n其 $\\ell_2$ 范数为 $\\|Z_{2,:}\\|_2 = \\sqrt{1^2 + (-0.5)^2} = \\sqrt{1 + 0.25} = \\sqrt{1.25} = \\frac{\\sqrt{5}}{2}$。\n缩放因子为 $s_2 = 1 - \\frac{0.4}{\\sqrt{1.25}} = 1 - \\frac{0.4}{\\sqrt{5}/2} = 1 - \\frac{0.8}{\\sqrt{5}}$。由于 $\\sqrt{5} \\approx 2.236$ 且 $0.8 / \\sqrt{5}  1$，该因子为正。\n$s_2 \\approx 1 - \\frac{0.8}{2.236068...} \\approx 1 - 0.357771... = 0.642229...$\n$(X^{(1)})_{2,:} = s_2 Z_{2,:} \\approx 0.642229 \\times \\begin{pmatrix} 1  -0.5 \\end{pmatrix} = \\begin{pmatrix} 0.642229  -0.321115 \\end{pmatrix}$。\n\n第3行：$Z_{3,:} = \\begin{pmatrix} -0.5  2 \\end{pmatrix}$\n其 $\\ell_2$ 范数为 $\\|Z_{3,:}\\|_2 = \\sqrt{(-0.5)^2 + 2^2} = \\sqrt{0.25 + 4} = \\sqrt{4.25} = \\frac{\\sqrt{17}}{2}$。\n缩放因子为 $s_3 = 1 - \\frac{0.4}{\\sqrt{4.25}} = 1 - \\frac{0.4}{\\sqrt{17}/2} = 1 - \\frac{0.8}{\\sqrt{17}}$。由于 $\\sqrt{17} \\approx 4.123$ 且 $0.8 / \\sqrt{17}  1$，该因子为正。\n$s_3 \\approx 1 - \\frac{0.8}{4.123106...} \\approx 1 - 0.194030... = 0.805970...$\n$(X^{(1)})_{3,:} = s_3 Z_{3,:} \\approx 0.805970 \\times \\begin{pmatrix} -0.5  2 \\end{pmatrix} = \\begin{pmatrix} -0.402985  1.611940 \\end{pmatrix}$。\n\n组合成矩阵 $X^{(1)}$ 并将每个条目四舍五入到四位有效数字：\n$X^{(1)}_{11} \\approx 0.8735$\n$X^{(1)}_{12} \\approx 2.621$\n$X^{(1)}_{21} \\approx 0.6422$\n$X^{(1)}_{22} \\approx -0.3211$\n$X^{(1)}_{31} \\approx -0.4030$\n$X^{(1)}_{32} \\approx 1.612$\n\n因此，更新后的迭代值 $X^{(1)}$ 是：\n$$\nX^{(1)} \\approx \\begin{pmatrix}\n0.8735  2.621 \\\\\n0.6422  -0.3211 \\\\\n-0.4030  1.612\n\\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.8735  2.621 \\\\ 0.6422  -0.3211 \\\\ -0.4030  1.612 \\end{pmatrix}}\n$$", "id": "3460767"}, {"introduction": "现在，我们从凸优化方法转向贪婪算法。本练习的重点是实现多测量向量问题中的一个经典且直观的方法——同步正交匹配追踪（SOMP）。与ISTA的整体优化不同，SOMP通过迭代和贪婪的方式来识别稀疏信号的支撑集。这个编程练习将巩固你对该算法循序渐进逻辑的理解，从基于最大相关性的原子选择，到通过正交投影更新残差的整个过程。", "problem": "考虑压缩感知中的多重测量向量（Multiple Measurement Vector, MMV）模型。MMV模型假设$L$个测量向量在未知系数矩阵的行上共享一个共同的稀疏支撑集。形式上，令 $A \\in \\mathbb{R}^{m \\times n}$ 为一个测量矩阵（也称为字典），$X \\in \\mathbb{R}^{n \\times L}$ 为一个行稀疏系数矩阵，其至多有 $k$ 个非零行（联合稀疏性），$Y \\in \\mathbb{R}^{m \\times L}$ 为观测到的测量值，满足\n$$\nY = A X + E,\n$$\n其中 $E \\in \\mathbb{R}^{m \\times L}$ 是加性噪声。同步正交匹配追踪（Simultaneous Orthogonal Matching Pursuit, SOMP）算法贪婪地选择$A$的列（称为原子），这些原子能够最好地解释所有$L$个测量向量上的残差，通过向已选原子的张成空间上进行正交投影来更新残差，并在达到预定迭代次数后停止。\n\n从基本原理出发：\n- 第 $t$ 次迭代的残差定义为 $R^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}$，其中 $S^{(t)}$ 是第 $t$ 次迭代时已选索引的集合，$A_{S^{(t)}}$ 是由 $A$ 中索引为 $S^{(t)}$ 的列构成的子矩阵。\n- 最小二乘更新强制 $X_{S^{(t)}}$ 最小化残差的弗罗贝尼乌斯范数（Frobenius norm），即 $X_{S^{(t)}} = \\operatorname*{arg\\,min}_{Z \\in \\mathbb{R}^{|S^{(t)}| \\times L}} \\| Y - A_{S^{(t)}} Z \\|_F$。当 $A_{S^{(t)}}$ 是满列秩时，到 $A_{S^{(t)}}$ 的张成空间上的正交投影算子表示为 $P_{A_{S^{(t)}}} = A_{S^{(t)}} \\left( A_{S^{(t)}}^\\top A_{S^{(t)}} \\right)^{-1} A_{S^{(t)}}^\\top$，从而得到残差 $R^{(t)} = (I - P_{A_{S^{(t)}}}) Y$。\n- 在第 $t$ 次迭代中，贪婪地选择一个索引 $j \\notin S^{(t-1)}$ 的方法是，将当前残差 $R^{(t-1)}$ 投影到原子 $A_j$ 上，并最大化捕获的能量。对于单位范数列，捕获的能量为 $\\| A_j^\\top R^{(t-1)} \\|_2^2$，因此 SOMP 的选择规则为\n$$\nj^{\\star} \\in \\operatorname*{arg\\,max}_{j \\notin S^{(t-1)}} \\| A_j^\\top R^{(t-1)} \\|_2.\n$$\n\n您的任务是实现 SOMP 算法，并对以下小规模、完全指定的实例运行恰好两次迭代，跟踪残差和已选索引的演变过程。本问题中的所有索引均使用从零开始的编号方式。\n\n实现以下测试套件，参数为 $(m,n,L) = (5,10,3)$，联合稀疏度为 $k = 2$：\n\n- 测试用例1（无噪声，通用字典）：\n  - 使用固定的种子 $s_A = 0$ 从标准正态分布中独立抽取元素来构造 $A$，并将每一列归一化为单位 $\\ell_2$ 范数。\n  - 选择一个已知的支撑集 $S^\\mathrm{true} = [2, 7]$，并将 $X$ 在索引 2 和 7 处的非零行设置为 $X_{2,:} = [1.0, -0.5, 0.8]$ 和 $X_{7,:} = [0.9, 0.3, -1.2]$。$X$ 的所有其他行均为零。\n  - 设置 $E = 0$ 且 $Y = A X$。\n\n- 测试用例2（小噪声，相同字典和支撑集）：\n  - 使用与测试用例1中相同的 $A$ 和 $X$。\n  - 添加高斯噪声 $E$，其条目使用种子 $s_E = 1$ 从标准正态分布中独立抽取，并按 $\\sigma = 0.01$ 进行缩放，即 $E = \\sigma \\cdot W$，其中 $W$ 具有独立的标准正态分布条目。\n  - 设置 $Y = A X + E$。\n\n- 测试用例3（首次选择时出现平局，正交规范原子）：\n  - 构造 $A$ 为 $A = [I_5 \\,|\\, B]$，其中 $I_5 \\in \\mathbb{R}^{5 \\times 5}$ 是单位矩阵（其列已是单位范数），$B \\in \\mathbb{R}^{5 \\times 5}$ 是通过使用固定种子 $s_B = 2$ 从标准正态分布中抽取元素并将其每一列归一化为单位 $\\ell_2$ 范数而形成的。\n  - 选择一个已知的支撑集 $S^\\mathrm{true} = [1, 2]$，并将 $X$ 在索引 1 和 2 处的非零行设置为 $X_{1,:} = [0.6, 0.8, 0.0]$ 和 $X_{2,:} = [0.0, 0.6, 0.8]$。$X$ 的所有其他行均为零。\n  - 设置 $E = 0$ 且 $Y = A X$。\n  - 因为 $\\|X_{1,:}\\|_2 = \\|X_{2,:}\\|_2$，并且前五个原子是正交规范的，所以第一次 SOMP 迭代会在索引 1 和 2 之间产生平局。通过选择最小的索引来解决平局。\n\n算法要求：\n- 在运行 SOMP 之前，将 $A$ 的每一列归一化为单位 $\\ell_2$ 范数。\n- 在每次迭代 $t = 1,2$ 中，计算相关矩阵 $C^{(t)} = A^\\top R^{(t-1)} \\in \\mathbb{R}^{n \\times L}$，然后对所有 $j \\notin S^{(t-1)}$ 计算选择分数 $g_j^{(t)} = \\| C^{(t)}_{j,:} \\|_2$，选择获得最高分数的索引 $j^{\\star}$（通过选择最小索引来打破平局），更新 $S^{(t)} = S^{(t-1)} \\cup \\{ j^{\\star} \\}$，并通过最小二乘法更新残差 $R^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}$，其中 $X_{S^{(t)}}$ 是最小化 $\\| Y - A_{S^{(t)}} Z \\|_F$ 的最小二乘解。\n- 记录所选索引的序列 $[j^{\\star}_1, j^{\\star}_2]$ 和残差的弗罗贝尼乌斯范数 $\\| R^{(1)} \\|_F$ 及 $\\| R^{(2)} \\|_F$。\n\n最终输出格式：\n- 对每个测试用例，输出一个包含两个元素的列表：两次迭代中选定的索引列表，以及两个残差的弗罗贝尼乌斯范数列表（四舍五入到六位小数）。\n- 将所有测试用例的结果按顺序聚合到一个列表中，并打印一行包含此聚合列表的内容，格式为用方括号括起来的逗号分隔列表，不含其他文本。例如，输出应如下所示：\n$$\n[[[j_1,j_2],[r_1,r_2]],[[\\ldots],[\\ldots]],[[\\ldots],[\\ldots]]]\n$$\n其中 $j_1, j_2$是整数，$r_1, r_2$是四舍五入到六位小数的浮点数。\n\n您的程序必须是一个完整的、可运行的实现，能够为上述三个测试用例生成指定的单行输出。", "solution": "该问题是有效的。这是一个适定 (well-posed)、有科学依据且客观的任务，植根于压缩感知和稀疏信号恢复的既定原则。所有参数、条件和步骤都得到了足够清晰和精确的说明，从而能够得到唯一且可验证的解。\n\n任务是实现同步正交匹配追踪（Simultaneous Orthogonal Matching Pursuit, SOMP）算法，以从一组线性测量值 $Y = AX + E$ 中恢复行稀疏矩阵 $X$。SOMP 是一种为多重测量向量（MMV）问题设计的迭代贪婪算法，在 MMV 问题中，多个信号向量共享一个共同的稀疏支撑集。该算法通过从字典 $A$ 中选择能够最好地解释观测测量值 $Y$ 的列（原子），来迭代地识别 $X$ 的支撑集。\n\n实现过程严格遵循 SOMP 算法定义的步骤，并精确执行两次迭代。\n\n**算法原理与实现**\n\nSOMP 算法的核心在于其迭代式的两阶段过程：一个选择阶段和一个更新阶段。\n\n**1. 初始化：**\n算法从一个空的支撑集 $S^{(0)} = \\emptyset$ 开始，初始残差等于测量矩阵 $R^{(0)} = Y$。\n\n**2. 迭代过程（对于迭代 $t=1, 2, \\dots$）：**\n\n**a. 原子选择：**\n选择步骤的基本原理是，从字典中识别出与当前残差 $R^{(t-1)}$ 具有最高相关性的原子 $A_j$。这个相关性是在所有 $L$ 个测量通道上度量的。对于每个原子 $A_j$（其中 $j$ 尚未在支撑集 $S^{(t-1)}$ 中），我们计算多通道残差在其上的投影。该投影的能量由 $\\| A_j^\\top R^{(t-1)} \\|_2^2$ 给出。SOMP 选择使该能量最大化的原子，这等同于最大化相关向量的 $\\ell_2$-范数：\n$$\nj^{\\star} = \\operatorname*{arg\\,max}_{j \\notin S^{(t-1)}} \\| A_j^\\top R^{(t-1)} \\|_2\n$$\n在实现中，这通过首先计算相关矩阵 $C^{(t)} = A^\\top R^{(t-1)}$ 来实现。然后，每个原子 $j$ 的选择分数是 $C^{(t)}$ 第 $j$ 行的 $\\ell_2$-范数。为此，使用了 `numpy.linalg.norm` 函数并设置 `axis=1`。已经选择的索引被屏蔽掉，然后 `numpy.argmax` 找到得分最高的原子的索引。问题规定，平局通过选择最小的索引来打破，这是 `numpy.argmax` 的固有行为。然后更新支撑集：$S^{(t)} = S^{(t-1)} \\cup \\{ j^{\\star} \\}$。\n\n**b. 最小二乘更新与残差计算：**\n一旦支撑集 $S^{(t)}$ 更新，就使用已选择的原子重新估计系数矩阵 $X$，以最好地拟合测量值 $Y$。这是一个经典的最小二乘问题：\n$$\nX_{S^{(t)}} = \\operatorname*{arg\\,min}_{Z \\in \\mathbb{R}^{|S^{(t)}| \\times L}} \\| Y - A_{S^{(t)}} Z \\|_F\n$$\n其中 $A_{S^{(t)}}$ 是包含由 $S^{(t)}$ 索引的列的 $A$ 的子矩阵，$X_{S^{(t)}}$ 是 $X$ 的估计中对应的非零行。此最小化问题的解是通过将 $Y$ 投影到由 $A_{S^{(t)}}$ 的列张成的子空间上给出的。当 $A_{S^{(t)}}$ 的列线性无关时，解是唯一的，并由 $X_{S^{(t)}} = (A_{S^{(t)}}^\\top A_{S^{(t)}})^{-1} A_{S^{(t)}}^\\top Y$ 给出。\n实现中使用了 `numpy.linalg.lstsq(A_S, Y)`，它为此最小二乘问题提供了一个数值上稳定且准确的解，实际上是计算了 $A_{S^{(t)}}$ 的摩尔-彭若斯伪逆（Moore-Penrose pseudoinverse）与 $Y$ 的乘积。\n\n新的残差 $R^{(t)}$ 是 $Y$ 中未被此投影解释的部分：\n$$\nR^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}\n$$\n在每次迭代中计算并记录此残差的弗罗贝尼乌斯范数 $\\|R^{(t)}\\|_F$。此范数量化了剩余误差，其减小表明算法在解释数据方面的进展。\n\n对于指定的三个测试用例中的每一个，此过程都重复进行所要求的两次迭代。每个测试用例的设置包括根据给定参数构造矩阵 $A$、$X$ 和 $Y$，包括用于可复现性的随机种子和特定的噪声条件。最终输出汇总了每个用例的已选索引列表和残差弗罗贝尼乌斯范数列表。", "answer": "```python\nimport numpy as np\n\ndef run_somp(A, Y, num_iterations):\n    \"\"\"\n    Implements the Simultaneous Orthogonal Matching Pursuit (SOMP) algorithm.\n\n    Args:\n        A (np.ndarray): The measurement matrix (m x n).\n        Y (np.ndarray): The measurement vectors (m x L).\n        num_iterations (int): The number of iterations to run.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: The list of selected indices.\n            - list: The list of residual Frobenius norms at each iteration.\n    \"\"\"\n    m, n = A.shape\n    \n    # Initial residual is the measurement matrix itself\n    R = Y.copy()\n    \n    # Set of selected indices\n    S = []\n    \n    # Mask for selected indices to handle argmax efficiently\n    selected_mask = np.zeros(n, dtype=bool)\n    \n    # List to store residual norms\n    residual_norms = []\n\n    for _ in range(num_iterations):\n        # 1. Selection Step: Find the atom most correlated with the residual\n        # Compute correlations: A^T * R\n        C = A.T @ R\n        \n        # Compute scores as the l2-norm of each row of the correlation matrix\n        scores = np.linalg.norm(C, axis=1)\n        \n        # Mask out already selected columns by setting their score to a negative value\n        scores[selected_mask] = -1.0\n        \n        # Select the index with the maximum score. numpy.argmax breaks ties\n        # by choosing the smallest index, as required.\n        j_star = np.argmax(scores)\n        \n        # 2. Update Step\n        # Add the new index to the support set\n        S.append(j_star)\n        selected_mask[j_star] = True\n        \n        # Form the submatrix of A with currently selected columns\n        A_S = A[:, S]\n        \n        # Solve the least-squares problem: min ||Y - A_S * Z||_F\n        # np.linalg.lstsq is a numerically stable way to do this.\n        X_S, _, _, _ = np.linalg.lstsq(A_S, Y, rcond=None)\n        \n        # Update the residual\n        R = Y - A_S @ X_S\n        \n        # Calculate and store the Frobenius norm of the new residual\n        norm_R = np.linalg.norm(R, 'fro')\n        residual_norms.append(norm_R)\n        \n    # Round norms to six decimal places as per the requirement\n    rounded_norms = [round(norm, 6) for norm in residual_norms]\n    \n    return S, rounded_norms\n\ndef solve():\n    \"\"\"\n    Sets up and runs the three test cases for the SOMP algorithm,\n    then prints the formatted results.\n    \"\"\"\n    results = []\n    m, n, L = 5, 10, 3\n    num_iterations = 2\n\n    # --- Test Case 1: Noise-free, general dictionary ---\n    rng_A = np.random.default_rng(seed=0)\n    A1 = rng_A.standard_normal((m, n))\n    A1 /= np.linalg.norm(A1, axis=0)  # Normalize each column to unit l2-norm\n    \n    X1 = np.zeros((n, L))\n    X1[2, :] = [1.0, -0.5, 0.8]\n    X1[7, :] = [0.9, 0.3, -1.2]\n    \n    Y1 = A1 @ X1\n    \n    indices1, norms1 = run_somp(A1, Y1, num_iterations)\n    results.append([indices1, norms1])\n\n    # --- Test Case 2: Small noise, same dictionary and support ---\n    # Use the same A1 and X1\n    rng_E = np.random.default_rng(seed=1)\n    sigma = 0.01\n    E = sigma * rng_E.standard_normal((m, L))\n    \n    Y2 = A1 @ X1 + E\n    \n    indices2, norms2 = run_somp(A1, Y2, num_iterations)\n    results.append([indices2, norms2])\n\n    # --- Test Case 3: Tie in first selection, orthonormal atoms ---\n    rng_B = np.random.default_rng(seed=2)\n    I5 = np.identity(5)\n    B = rng_B.standard_normal((5, 5))\n    B /= np.linalg.norm(B, axis=0)\n    A3 = np.hstack((I5, B))\n    \n    X3 = np.zeros((n, L))\n    X3[1, :] = [0.6, 0.8, 0.0]\n    X3[2, :] = [0.0, 0.6, 0.8]\n    \n    Y3 = A3 @ X3\n    \n    indices3, norms3 = run_somp(A3, Y3, num_iterations)\n    results.append([indices3, norms3])\n\n    # Format the final output string to remove spaces for a compact representation\n    final_output_str = str(results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```", "id": "3460799"}]}