{"hands_on_practices": [{"introduction": "本章的第一个练习是一个基础但至关重要的计算，旨在帮助你熟练掌握高斯宽度的定义。我们将直接计算一个在压缩感知理论中最基本的集合——具有固定支撑集和有界范数的稀疏向量集合的高斯宽度[@problem_id:3448590]。完成这个练习将为你理解更复杂的几何概念奠定坚实的数学基础。", "problem": "考虑一个压缩感知中的测量模型，其中随机线性映射通过戈登(Gordon)的穿透网格定理进行分析。设 $n \\in \\mathbb{N}$，并固定一个索引集 $S \\subset \\{1,2,\\dots,n\\}$，其基数为 $|S| = s$。定义子集 $T \\subset \\mathbb{R}^{n}$ 为支撑集包含于 $S$ 且欧几里得范数有界的 $s$-稀疏向量的集合：\n$$\nT \\triangleq \\left\\{ x \\in \\mathbb{R}^{n} : \\operatorname{supp}(x) \\subseteq S,\\ \\|x\\|_{2} \\leq 1 \\right\\}.\n$$\n设 $g \\in \\mathbb{R}^{n}$ 是一个标准高斯向量，其分量独立同分布于 $\\mathcal{N}(0,1)$，并回顾集合 $T$ 的高斯宽度的定义：\n$$\nw(T) \\triangleq \\mathbb{E}\\left[\\,\\sup_{x \\in T} \\langle g, x \\rangle\\,\\right],\n$$\n其中 $\\langle \\cdot, \\cdot \\rangle$ 表示标准欧几里得内积。从高斯向量和径向分布的基本定义与公认事实出发，推导 $w(T)$ 的一个仅用 $s$ 表示的精确解析表达式。将最终答案表示为单一的闭式符号表达式。无需进行数值舍入，也不涉及任何物理单位。", "solution": "该问题是有效的，因为它有科学依据、是适定的且客观的。这是高维概率论和压缩感知理论中的一个标准计算。我们现在开始推导高斯宽度 $w(T)$ 的解析表达式。\n\n集合 $T$ 的高斯宽度定义为：\n$$\nw(T) \\triangleq \\mathbb{E}\\left[\\,\\sup_{x \\in T} \\langle g, x \\rangle\\,\\right]\n$$\n其中 $T \\triangleq \\left\\{ x \\in \\mathbb{R}^{n} : \\operatorname{supp}(x) \\subseteq S,\\ \\|x\\|_{2} \\leq 1 \\right\\}$，集合 $S$ 的基数为 $|S| = s$，而 $g \\in \\mathbb{R}^{n}$ 是一个标准高斯向量，其分量 $g_i \\sim \\mathcal{N}(0, 1)$ 是独立同分布的。\n\n我们的第一步是对于一个固定的 $g$ 的实现，简化 $\\sup_{x \\in T} \\langle g, x \\rangle$ 这一项。条件 $\\operatorname{supp}(x) \\subseteq S$ 意味着对于任何向量 $x \\in T$ 和任何索引 $i \\notin S$，其分量 $x_i$ 均为零。因此，内积 $\\langle g, x \\rangle$ 可以写为在索引集 $S$ 上的求和：\n$$\n\\langle g, x \\rangle = \\sum_{i=1}^{n} g_i x_i = \\sum_{i \\in S} g_i x_i.\n$$\n我们定义 $g_S \\in \\mathbb{R}^{s}$ 为由 $g$ 中索引在 $S$ 内的分量组成的向量，类似地，定义 $x_S \\in \\mathbb{R}^{s}$ 为 $x$ 的相应分量组成的向量。于是内积为 $\\langle g, x \\rangle = \\langle g_S, x_S \\rangle_{\\mathbb{R}^s}$。约束条件 $\\|x\\|_{2} \\leq 1$ 变为 $\\|x_S\\|_{2} \\leq 1$，因为 $x$ 的所有其他分量都为零。\n\n因此，期望内的优化问题是：\n$$\n\\sup_{x \\in T} \\langle g, x \\rangle = \\sup_{x_S \\in \\mathbb{R}^s, \\|x_S\\|_2 \\leq 1} \\langle g_S, x_S \\rangle.\n$$\n这个表达式是 $g_S$ 的对偶范数的定义。对于 $\\ell_2$-范数，其对偶范数就是 $\\ell_2$-范数本身。根据柯西-施瓦茨不等式，我们有 $\\langle g_S, x_S \\rangle \\leq \\|g_S\\|_2 \\|x_S\\|_2$。考虑到 $\\|x_S\\|_2 \\leq 1$，我们有 $\\langle g_S, x_S \\rangle \\leq \\|g_S\\|_2$。当 $x_S$ 与 $g_S$ 同向且具有最大可能范数时，即 $x_S = g_S / \\|g_S\\|_2$（对于 $g_S \\neq 0$），可以达到这个最大值。因此，我们有：\n$$\n\\sup_{x \\in T} \\langle g, x \\rangle = \\|g_S\\|_2.\n$$\n现在，我们可以将高斯宽度重写为该范数的期望：\n$$\nw(T) = \\mathbb{E}\\left[ \\|g_S\\|_2 \\right].\n$$\n向量 $g_S$ 是 $g$ 的一个子向量，包含 $s$ 个分量，每个分量都是一个独立的标准正态随机变量。因此，$g_S$ 是 $\\mathbb{R}^s$ 中的一个标准高斯随机向量。我们感兴趣的随机变量是 $g_S$ 的欧几里得范数。\n\n令随机变量 $Z$ 为 $g_S$ 的范数平方：\n$$\nZ = \\|g_S\\|_2^2 = \\sum_{i \\in S} g_i^2.\n$$\n由于 $Z$ 是 $s$ 个独立标准正态随机变量的平方和，因此它服从自由度为 $s$ 的卡方分布，记作 $Z \\sim \\chi^2(s)$。\n\n对于 $z > 0$，$Z$ 的概率密度函数 (PDF) 由下式给出：\n$$\nf_Z(z) = \\frac{1}{2^{s/2} \\Gamma(s/2)} z^{s/2 - 1} \\exp\\left(-\\frac{z}{2}\\right),\n$$\n其中 $\\Gamma(\\cdot)$ 是伽马函数。\n\n我们需要计算 $\\|g_S\\|_2 = \\sqrt{Z}$ 的期望：\n$$\nw(T) = \\mathbb{E}[\\sqrt{Z}] = \\int_{0}^{\\infty} \\sqrt{z} f_Z(z) \\, dz.\n$$\n将 $\\chi^2(s)$ 分布的 PDF 代入积分中：\n$$\nw(T) = \\int_{0}^{\\infty} z^{1/2} \\left( \\frac{1}{2^{s/2} \\Gamma(s/2)} z^{s/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\right) \\, dz.\n$$\n我们可以提出常数项并合并 $z$ 的幂次：\n$$\nw(T) = \\frac{1}{2^{s/2} \\Gamma(s/2)} \\int_{0}^{\\infty} z^{1/2 + s/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\, dz = \\frac{1}{2^{s/2} \\Gamma(s/2)} \\int_{0}^{\\infty} z^{(s+1)/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\, dz.\n$$\n该积分是与伽马函数相关的标准形式。对于形状参数 $k > 0$ 和尺度参数 $\\theta > 0$，以下恒等式成立：\n$$\n\\int_{0}^{\\infty} t^{k-1} \\exp\\left(-\\frac{t}{\\theta}\\right) \\, dt = \\theta^k \\Gamma(k).\n$$\n在我们的例子中，积分变量是 $z$，形状参数是 $k = \\frac{s+1}{2}$，尺度参数是 $\\theta=2$。应用此恒等式，该积分的计算结果为：\n$$\n\\int_{0}^{\\infty} z^{(s+1)/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\, dz = 2^{(s+1)/2} \\Gamma\\left(\\frac{s+1}{2}\\right).\n$$\n将此结果代回我们关于 $w(T)$ 的表达式中：\n$$\nw(T) = \\frac{1}{2^{s/2} \\Gamma(s/2)} \\left( 2^{(s+1)/2} \\Gamma\\left(\\frac{s+1}{2}\\right) \\right).\n$$\n最后，我们通过合并 $2$ 的幂次来简化表达式：\n$$\nw(T) = \\frac{2^{(s+1)/2}}{2^{s/2}} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)} = 2^{((s+1)/2) - (s/2)} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)} = 2^{1/2} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)}.\n$$\n这就给出了集合 $T$ 的高斯宽度的精确解析表达式：\n$$\nw(T) = \\sqrt{2} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)}.\n$$\n结果仅取决于稀疏度 $s$，符合题目要求。这是自由度为 $s$ 的$\\chi$分布的均值。", "answer": "$$\\boxed{\\sqrt{2} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)}}$$", "id": "3448590"}, {"introduction": "这个更高级的练习将理论与实际算法联系起来，是理解戈登定理实际影响的关键一步。我们将从简单的稀疏集转向在 $\\ell_{1}$ 范数最小化中起核心作用的下降锥[@problem_id:3448604]。通过这个练习，你将推导出一个深刻的联系：锥的几何大小（统计维度）如何等价于一个可解的优化问题，而该问题的解又直接关联到近似消息传递（AMP）等前沿算法中的最优参数选择。", "problem": "设 $x^{\\star} \\in \\mathbb{R}^{n}$ 是一个固定的 $s$-稀疏向量，其支撑集为 $S \\subset \\{1,\\dots,n\\}$，大小为 $|S| = s$。并假设其非零项的符号已知，对于 $i \\in S$ 有 $v_{i} = \\operatorname{sign}(x^{\\star}_{i})$。考虑在 $x^{\\star}$ 处的 $\\ell_{1}$ 范数的下降锥 $D$，定义为 $D := \\{d \\in \\mathbb{R}^{n} : \\exists t > 0 \\text{ 使得 } \\|x^{\\star} + t d\\|_{1} \\le \\|x^{\\star}\\|_{1}\\}$。设 $g \\sim \\mathcal{N}(0, I_{n})$ 是 $\\mathbb{R}^{n}$ 中的一个标准高斯向量，回顾一下，锥 $D$ 与单位球面 $S^{n-1}$ 的交集的高斯宽度 $w(D \\cap S^{n-1})$ 是 Gordon 的“穿网而逃”定理中的一个核心量，该定理为压缩感知中的精确恢复提供了概率性保证。\n\n从以下几点出发：\n- 下降锥与次微分的锥包之间的极性关系，\n- 闭凸锥的统计维度根据标准高斯向量到其极锥的期望平方距离的定义，\n- 在 $x^{\\star}$ 处 $\\ell_{1}$ 范数的次微分的特征，\n\n推导在 $x^{\\star}$ 处下降锥 $D$ 的统计维度的一个关于标量 $ \\tau > 0$ 的显式优化公式，该公式表示为一个平方欧几里得距离的期望形式 $\\mathbb{E}\\big[\\operatorname{dist}^{2}(g, \\tau \\, \\partial\\|x^{\\star}\\|_{1})\\big]$。然后，通过解析微分，得到确定最小化子 $\\tau^{\\star}$ 的一阶最优性条件。最后，对于具体数值实例 $n = 1000$ 和 $s = 50$，求解此最优性条件得到 $\\tau^{\\star}$ 并给出其值。\n\n在你的推导中，明确地执行计算期望所需的积分运算，这些期望涉及标准高斯密度及其尾函数。简要解释优化子 $\\tau^{\\star}$ 如何与近似消息传递（Approximate Message Passing, AMP）中通过软阈值进行稀疏恢复时出现的数据驱动阈值选择相关联。\n\n将最终的 $\\tau^{\\star}$ 数值四舍五入到四位有效数字。不涉及物理单位。", "solution": "该问题要求推导在稀疏恢复背景下下降锥统计维度的优化公式，求解优化子，并解释其与近似消息传递（AMP）算法的联系。\n\n### 第 1 步：优化问题的构建\n\n问题要求为在 $s$-稀疏向量 $x^\\star$ 处 $\\ell_1$ 范数的下降锥 $D$ 的统计维度 $\\delta(D)$ 建立一个优化公式。下降锥由 $D = \\{d \\in \\mathbb{R}^n : \\sup_{u \\in \\partial\\|x^\\star\\|_1} \\langle u, d \\rangle \\le 0\\}$ 给出，这意味着 $D$ 是次微分集 $\\partial\\|x^\\star\\|_1$ 的极锥。根据锥的双极定理，$D$ 的极锥是次微分的闭锥包：$D^\\circ = \\overline{\\text{cone}}(\\partial\\|x^\\star\\|_1)$。\n\n$D$ 的统计维度定义为 $\\delta(D) = \\mathbb{E}_g[\\text{dist}^2(g, D^\\circ)]$，其中 $g \\sim \\mathcal{N}(0, I_n)$。\n压缩感知分析中的一个关键结果，源于 Gordon 定理及其在凸优化中的应用，建立了一个恒等式，将这个几何量与一个涉及次微分本身的优化问题联系起来。该恒等式表明，统计维度是特定目标函数 $L(\\tau)$ 的最小值：\n$$\n\\delta(D) = \\min_{\\tau > 0} L(\\tau) = \\min_{\\tau > 0} \\mathbb{E}\\big[\\operatorname{dist}^{2}(g, \\tau \\, \\partial\\|x^{\\star}\\|_{1})\\big]\n$$\n我们接下来推导目标函数 $L(\\tau)$ 的显式形式。\n\n在支撑集为 $S$、符号向量为 $v$ 的 $s$-稀疏向量 $x^\\star$ 处的 $\\ell_1$ 范数的次微分是集合：\n$$\n\\partial\\|x^\\star\\|_1 = \\{u \\in \\mathbb{R}^n : u_i = v_i \\text{ for } i \\in S, \\text{ and } |u_i| \\le 1 \\text{ for } i \\in S^c\\}\n$$\n其中 $S^c = \\{1, \\dots, n\\} \\setminus S$。我们将此集合记为 $C$。集合 $\\tau C$ 是 $\\{\\tau u : u \\in C\\}$。\n从 $g$ 到 $\\tau C$ 的平方距离是：\n$$\n\\operatorname{dist}^{2}(g, \\tau C) = \\min_{u \\in C} \\|g - \\tau u\\|_2^2\n$$\n这个最小化问题在支撑集 $S$ 及其补集 $S^c$ 上解耦：\n$$\n\\operatorname{dist}^{2}(g, \\tau C) = \\min_{u \\in C} \\left( \\sum_{i \\in S} (g_i - \\tau u_i)^2 + \\sum_{i \\in S^c} (g_i - \\tau u_i)^2 \\right)\n$$\n对于 $i \\in S$，$u_i$ 固定为 $v_i$，所以第一项就是 $\\sum_{i \\in S} (g_i - \\tau v_i)^2 = \\|g_S - \\tau v\\|_2^2$。\n对于 $i \\in S^c$，我们必须在 $|u_i| \\le 1$ 上最小化 $(g_i - \\tau u_i)^2$。这等价于求 $g_i$ 到区间 $[-\\tau, \\tau]$ 的平方距离。$g_i$ 在 $[-\\tau, \\tau]$ 上的投影是 $\\text{sgn}(g_i)\\min(|g_i|, \\tau)$。平方距离为：\n$$\n\\min_{|u_i| \\le 1} (g_i - \\tau u_i)^2 = \\operatorname{dist}^2(g_i, [-\\tau, \\tau]) = (|g_i| - \\tau)^2 \\mathbb{I}(|g_i| > \\tau)\n$$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数。这个表达式也是 $\\max(0, |g_i|-\\tau)^2$。\n\n结合这些部分，总平方距离为：\n$$\n\\operatorname{dist}^{2}(g, \\tau C) = \\sum_{i \\in S} (g_i - \\tau v_i)^2 + \\sum_{i \\in S^c} (|g_i| - \\tau)^2 \\mathbb{I}(|g_i| > \\tau)\n$$\n目标函数 $L(\\tau)$ 是该量的期望。由于 $g$ 的分量 $g_i$ 是独立同分布的 $\\mathcal{N}(0,1)$，我们可以计算每一项的期望。\n\n对于 $i \\in S$，由于 $v_i^2=1$：\n$$\n\\mathbb{E}[(g_i - \\tau v_i)^2] = \\mathbb{E}[g_i^2] - 2\\tau v_i \\mathbb{E}[g_i] + \\tau^2 v_i^2 = 1 - 0 + \\tau^2 = 1+\\tau^2\n$$\n对 $S$ 的 $s$ 个元素求和，这部分对 $L(\\tau)$ 的贡献是 $s(1+\\tau^2)$。\n\n对于 $i \\in S^c$，设 $Z \\sim \\mathcal{N}(0,1)$。我们需要计算 $\\mathbb{E}[(|Z|-\\tau)^2 \\mathbb{I}(|Z|>\\tau)]$。设 $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^2/2)$ 为标准正态概率密度函数（PDF），$Q(\\tau) = \\int_\\tau^\\infty \\phi(z)dz$ 为其尾概率。\n\\begin{align*}\n\\mathbb{E}[(|Z|-\\tau)^2 \\mathbb{I}(|Z|>\\tau)] = \\int_{-\\infty}^\\infty (|z|-\\tau)^2 \\mathbb{I}(|z|>\\tau) \\phi(z)dz \\\\\n= 2 \\int_\\tau^\\infty (z-\\tau)^2 \\phi(z)dz \\\\\n= 2 \\int_\\tau^\\infty (z^2 - 2\\tau z + \\tau^2) \\phi(z)dz \\\\\n= 2 \\int_\\tau^\\infty z^2 \\phi(z)dz - 4\\tau \\int_\\tau^\\infty z \\phi(z)dz + 2\\tau^2 \\int_\\tau^\\infty \\phi(z)dz\n\\end{align*}\n使用标准高斯积分 $\\int_\\tau^\\infty \\phi(z)dz = Q(\\tau)$，$\\int_\\tau^\\infty z \\phi(z)dz = \\phi(\\tau)$，以及 $\\int_\\tau^\\infty z^2 \\phi(z)dz = \\tau\\phi(\\tau) + Q(\\tau)$（通过分部积分法得到），我们得到：\n$$\n2(\\tau\\phi(\\tau) + Q(\\tau)) - 4\\tau\\phi(\\tau) + 2\\tau^2 Q(\\tau) = (2+2\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) = 2(1+\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau)\n$$\n共有 $n-s$ 个这样的项。综合所有部分，目标函数为：\n$$\nL(\\tau) = s(1+\\tau^2) + (n-s) \\left[ 2(1+\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) \\right]\n$$\n\n### 第 2 步：一阶最优性条件\n\n为了找到最小化子 $\\tau^\\star > 0$，我们将 $L(\\tau)$ 对 $\\tau$ 求导，并令导数为零。我们使用导数 $Q'(\\tau) = -\\phi(\\tau)$ 和 $\\phi'(\\tau) = -\\tau\\phi(\\tau)$。\n$$\n\\frac{dL}{d\\tau} = 2s\\tau + (n-s) \\frac{d}{d\\tau}\\left[ (2+2\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) \\right]\n$$\n括号中项的导数是：\n\\begin{align*}\n\\frac{d}{d\\tau} \\left[ (2+2\\tau^2)Q(\\tau) - 2\\tau\\phi(\\tau) \\right] = \\left(4\\tau Q(\\tau) + (2+2\\tau^2)(-\\phi(\\tau))\\right) - \\left(2\\phi(\\tau) + 2\\tau(-\\tau\\phi(\\tau))\\right) \\\\\n= 4\\tau Q(\\tau) - 2\\phi(\\tau) - 2\\tau^2\\phi(\\tau) - 2\\phi(\\tau) + 2\\tau^2\\phi(\\tau) \\\\\n= 4\\tau Q(\\tau) - 4\\phi(\\tau)\n\\end{align*}\n令 $L'(\\tau) = 0$：\n$$\n2s\\tau + (n-s)[4\\tau Q(\\tau) - 4\\phi(\\tau)] = 0\n$$\n假设 $\\tau > 0$，我们可以除以 $2$：\n$$\ns\\tau + 2(n-s)\\tau Q(\\tau) - 2(n-s)\\phi(\\tau) = 0\n$$\n这简化为 $\\tau^\\star$ 的一阶最优性条件：\n$$\n\\tau (s + 2(n-s)Q(\\tau)) = 2(n-s)\\phi(\\tau)\n$$\n\n### 第 3 步：数值解\n\n对于具体实例 $n=1000$ 和 $s=50$，我们有 $n-s=950$。最优性条件变为：\n$$\n\\tau (50 + 2(950)Q(\\tau)) = 2(950)\\phi(\\tau) \\implies \\tau(50 + 1900 Q(\\tau)) = 1900 \\phi(\\tau)\n$$\n两边除以 $1900$：\n$$\n\\tau\\left(\\frac{50}{1900} + Q(\\tau)\\right) = \\phi(\\tau) \\implies \\tau\\left(\\frac{1}{38} + Q(\\tau)\\right) = \\phi(\\tau)\n$$\n这是一个关于 $\\tau$ 的超越方程，必须进行数值求解。让我们通过将原始一阶条件（FOC）除以 $2(n-s)$ 来重写它：\n$$\n\\frac{s}{n-s}\\frac{\\tau}{2} + \\tau Q(\\tau) - \\phi(\\tau) = 0\n$$\n令 $\\rho = s/(n-s) = 50/950 = 1/19$，方程为：\n$$\n\\frac{1}{19}\\frac{\\tau}{2} + \\tau Q(\\tau) - \\phi(\\tau) = 0 \\iff \\frac{\\tau}{38} + \\tau Q(\\tau) = \\phi(\\tau)\n$$\n数值求解此方程（例如，使用牛顿法或求根求解器）可得到解。数值评估表明，函数 $f(\\tau) = \\frac{\\tau}{38} + \\tau Q(\\tau) - \\phi(\\tau)$ 在 $\\tau=1.398$ 和 $\\tau=1.399$ 之间改变符号。更精确的求根将解定位在 $\\tau^\\star \\approx 1.398$。\n四舍五入到四位有效数字，我们得到 $\\tau^\\star = 1.398$。\n\n### 第 4 步：与近似消息传递（AMP）的联系\n\n推导出的最优性条件及其解 $\\tau^\\star$ 与稀疏恢复的 AMP 算法分析有着深刻的联系。\nAMP 是一种用于求解诸如 $y=Ax+w$ 等系统的迭代算法。在估计稀疏向量 $x^\\star$ 的背景下，AMP 的一个关键组成部分是在每次迭代中应用的非线性函数，对于 $\\ell_1$ 恢复，该函数是软阈值算子 $\\eta(z; \\lambda) = \\text{sgn}(z)\\max(|z|-\\lambda, 0)$。AMP 在高维极限下的性能可以通过一个称为状态演化（state evolution）的标量递归来精确刻画。\n\n状态演化追踪第 $t$ 次迭代时估计中的有效噪声方差 $\\sigma_t^2$。该方差的更新由一个关于阈值函数对信号分量影响的期望给出。对于从观测值 $x^\\star + \\sigma_t Z$（其中 $Z \\sim \\mathcal{N}(0,1)$）估计 $x^\\star$ 的问题，下一状态的方差为 $\\sigma_{t+1}^2 = \\mathbb{E}[(\\eta(x^\\star+\\sigma_t Z; \\lambda_t) - x^\\star)^2]$。\n\n阈值 $\\lambda_t$ 的最优选择是最小化此期望误差的选择。我们计算的项 $\\mathbb{E}[(|Z|-\\tau)^2 \\mathbb{I}(|Z|>\\tau)]$，当在单位方差高斯噪声中估计一个零系数（$x^\\star_i=0$）时，恰好对应于期望平方误差 $\\mathbb{E}[\\eta(Z; \\tau)^2]$。\n\n我们的目标函数 $L(\\tau)$ 可以被看作是一个总风险，它平衡了非零分量上的估计误差（$s(1+\\tau^2)$）和零分量上的估计误差（$(n-s)\\mathbb{E}[\\eta(Z;\\tau)^2]$）。最小化子 $\\tau^\\star$ 是在给定稀疏度 $s$ 和维度 $n$ 的情况下，最优地平衡这种权衡的阈值。\n\n至关重要的是，我们推导出的最优性条件 $\\tau (s + 2(n-s)Q(\\tau)) = 2(n-s)\\phi(\\tau)$，与定义大系统极限下 LASSO 问题状态演化不动点的方程完全相同。解 $\\tau^\\star$ 代表了给定问题配置（$s, n$）下的渐近最优阈值参数。这展示了问题的一个几何性质（下降锥的统计维度）与一个高效算法的参数（AMP 中的阈值）之间的基本联系。优化子 $\\tau^\\star$ 正是 AMP 将收敛到的数据驱动阈值。", "answer": "$$\\boxed{1.398}$$", "id": "3448604"}, {"introduction": "作为本章的总结性实践，这个练习要求你通过编写代码来“眼见为实”。你将不再仅仅是推导理论公式，而是要亲手实现它，通过蒙特卡洛模拟来验证戈登定理的预测能力[@problem_id:3481865]。你将计算出信号恢复的理论“相变”阈值，并观察到在模拟实验中，恢复成功率如何在这个阈值附近发生急剧变化，从而深刻体会理论的现实指导意义。", "problem": "设 $n \\in \\mathbb{N}$，且设 $x^\\star \\in \\mathbb{R}^n$ 是一个 $k$-稀疏信号，这意味着 $x^\\star$ 中恰好有 $k$ 个坐标是非零的。考虑线性测量 $y = A x^\\star$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 的元素是独立同分布的标准正态分布。我们感兴趣的锥是凸函数 $\\|\\cdot\\|_1$在点 $x^\\star$ 处的下降锥 $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$，其定义为\n$$\n\\mathcal{D}(\\|\\cdot\\|_1, x^\\star) = \\operatorname{cone}\\left\\{h \\in \\mathbb{R}^n : \\|x^\\star + h\\|_1 \\le \\|x^\\star\\|_1 \\right\\}.\n$$\n一个锥 $K$ 的高斯宽度 $w(K)$ 定义为\n$$\nw(K) = \\mathbb{E}\\left[ \\sup_{u \\in K \\cap \\mathbb{S}^{n-1}} \\langle g, u \\rangle \\right],\n$$\n其中 $\\mathbb{S}^{n-1}$ 是 $\\mathbb{R}^n$ 中的单位球面，$g \\sim \\mathcal{N}(0, I_n)$ 是一个标准高斯向量。Gordon 的“逃离网格”定理断言，当余维数 $m$ 大于 $K$ 的高斯宽度的某个函数时，一个随机子空间很可能与 $K$ 不相交；反之，对于较小的 $m$，相交是很有可能的。\n\n你的任务是编写一个完整的、独立的程序，通过以下方式为下降锥 $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$ 演示这一现象：\n1. 使用锥几何的第一性原理，通过下降锥的统计维度来计算高斯宽度平方 $w(K)^2$ 的近似值。统计维度定义为\n$$\n\\delta(K) = \\mathbb{E}\\left[ \\|\\Pi_K(g)\\|_2^2 \\right],\n$$\n其中 $\\Pi_K(g)$ 表示 $g$ 到 $K$ 上的欧几里得投影，并且它与 $K \\cap \\mathbb{S}^{n-1}$ 的高斯宽度平方密切相关。\n2. 经验性地估计随机零空间 $\\operatorname{Null}(A)$ 是否与 $K$ 非平凡相交，这等价于表明 $\\ell_1$-最小化恢复失败。具体来说，对于每次试验，求解以下凸优化问题\n$$\n\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad A x = y,\n$$\n如果最小化器在数值容差范围内等于 $x^\\star$，则声明恢复成功，否则声明失败。存在一个非零的 $h \\in \\operatorname{Null}(A) \\cap K$ 表明恢复失败。\n3. 将经验成功率与基于 $m$ 是否大于或小于计算出的 $\\delta(K)$ 的理论预测进行比较。\n\n使用以下经过充分检验的定义和事实作为你的基本依据：\n- 一个真、凸、下半连续函数在某一点的下降锥收集了所有不增加该函数值的方向。\n- 高斯宽度和统计维度是控制高维凸恢复中相变的几何度量。\n- 对于 $\\ell_1$ 范数，在 $k$-稀疏点处的下降锥的统计维度可以通过一个涉及次微分和标量参数的期望来刻画，并且可以使用标准正态随机变量绝对值的分布进行数值计算。\n\n程序要求：\n- 实现一个数值过程，通过对一个非负标量参数进行最小化，来近似给定 $(n,k)$ 的 $\\delta\\!\\left(\\mathcal{D}(\\|\\cdot\\|_1, x^\\star)\\right)$。该最小化涉及一个从 $|g|$（其中 $g \\sim \\mathcal{N}(0,1)$）的半正态分布导出的适当期望。\n- 对于每个测试用例，生成一个具有独立同分布标准正态元素的矩阵 $A$ 和一个随机的 $k$-稀疏向量 $x^\\star$（其非零项从连续分布中抽取），然后使用线性规划公式求解 $\\ell_1$-最小化问题，并记录恢复是否成功。\n- 对于每个测试用例，输出两个数字：预测指标（如果 $m > \\delta(K)$ 则为 $1$，否则为 $0$）和在指定试验次数中的经验恢复成功率（四舍五入到三位小数）。\n\n测试套件：\n- 案例1（顺利路径，高于阈值）：$(n,k,m,T) = (200,10,85,8)$。\n- 案例2（低于阈值）：$(n,k,m,T) = (200,10,55,8)$。\n- 案例3（接近阈值）：$(n,k,m,T) = (200,10,70,8)$。\n- 案例4（边界，零稀疏）：$(n,k,m,T) = (100,0,1,8)$。\n- 案例5（密集相对稀疏度）：$(n,k,m,T) = (200,60,150,6)$。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素对应一个测试用例，并且本身是一个形式为 $[\\text{indicator}, \\text{fraction}]$ 的双元素列表。例如，输出应具有以下形式\n$$\n[[i_1,f_1],[i_2,f_2],\\dots,[i_5,f_5]],\n$$\n其中每个 $i_j$ 是整数 $0$ 或 $1$，每个 $f_j$ 是四舍五入到三位小数的小数。", "solution": "问题陈述在压缩感知和高维概率领域提出了一个有效且定义明确的任务。我们首先验证问题，然后提供一个完整的解决方案。\n\n### 问题验证\n\n**第1步：提取已知信息**\n\n- **信号与稀疏性**：$x^\\star \\in \\mathbb{R}^n$ 是一个 $k$-稀疏信号，其中 $n \\in \\mathbb{N}$，$k$ 是非零坐标的数量。\n- **测量模型**：$y = A x^\\star$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 具有独立同分布（i.i.d.）的标准正态元素。\n- **下降锥**：$K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star) = \\operatorname{cone}\\left\\{h \\in \\mathbb{R}^n : \\|x^\\star + h\\|_1 \\le \\|x^\\star\\|_1 \\right\\}$。\n- **高斯宽度**：$w(K) = \\mathbb{E}\\left[ \\sup_{u \\in K \\cap \\mathbb{S}^{n-1}} \\langle g, u \\rangle \\right]$，其中 $g \\sim \\mathcal{N}(0, I_n)$。\n- **统计维度**：$\\delta(K) = \\mathbb{E}\\left[ \\|\\Pi_K(g)\\|_2^2 \\right]$，其中 $\\Pi_K(g)$ 是 $g$ 到 $K$ 上的欧几里得投影。\n- **恢复问题**：$\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1$ subject to $A x = y$。\n- **恢复成功条件**：恢复问题的最小化器等于 $x^\\star$。这种情况发生当且仅当 $A$ 的零空间 $\\operatorname{Null}(A)$ 与下降锥 $K$ 没有非平凡交集。\n- **任务**：对于给定的参数 $(n,k,m,T)$，计算理论成功指标（如果 $m > \\delta(K)$ 则为 $1$，否则为 $0$）和在 $T$ 次试验中的经验成功率。\n- **测试用例**：\n    1. $(n,k,m,T) = (200,10,85,8)$\n    2. $(n,k,m,T) = (200,10,55,8)$\n    3. $(n,k,m,T) = (200,10,70,8)$\n    4. $(n,k,m,T) = (100,0,1,8)$\n    5. $(n,k,m,T) = (200,60,150,6)$\n\n**第2步：使用提取的已知信息进行验证**\n\n- **科学依据**：该问题牢固地植根于现代压缩感知理论。下降锥、统计维度、高斯宽度等概念，以及它们与凸优化中相变现象的联系，是该领域的核心和既定成果（参见 Amelunxen, Lotz, McCoy, Tropp, 2014, \"Living on the Edge: Phase Transitions in Convex Programs with Random Data\"）。\n- **适定性**：该问题是适定的。它要求通过数值计算和模拟来验证一个已知的理论结果。任务规定明确，提供的参数允许得出唯一的输出集。\n- **客观性**：问题以精确的数学语言陈述，没有主观性。\n- **完整性**：问题提供了所有必要的定义和参数。它隐含地依赖于 $\\ell_1$ 下降锥统计维度的已知公式，这在此背景下是标准的。\n- **无其他缺陷**：问题没有矛盾、不切实际（在数学框架内）或结构不良。\n\n**第3步：结论与行动**\n\n问题是**有效的**。我们继续提供完整解决方案。\n\n### 解决方案\n\n该问题探讨了压缩感知中的相变现象，即通过 $\\ell_1$-最小化从线性测量 $y=Ax^\\star$ 中恢复稀疏信号 $x^\\star$ 的成功率，会随着测量数量 $m$ 的变化而发生急剧改变。这一相变由下降锥 $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$ 的统计维度 $\\delta(K)$ 精确刻画。\n\n基本原理是，对于一个随机高斯矩阵 $A$，如果 $m > \\delta(K)$，则恢复 $x^\\star$ 的概率很高；如果 $m < \\delta(K)$，则概率很低。点 $m \\approx \\delta(K)$ 标志着相变边界。我们的解决方案包括两部分：首先，计算理论阈值 $\\delta(K)$；其次，通过模拟恢复过程来经验性地验证预测。\n\n**1. 通过统计维度进行理论预测**\n\n对于一个 $k$-稀疏信号 $x^\\star \\in \\mathbb{R}^n$，其 $\\ell_1$-范数下降锥（表示为 $K=\\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$）的统计维度与 $x^\\star$ 的非零项的位置和符号无关。它由以下公式给出：\n$$\n\\delta(K) = k + \\min_{\\lambda \\ge 0} \\left\\{ (n-k) \\mathbb{E}\\left[(\\lvert Z \\rvert - \\lambda)^2_+\\right] + k\\lambda^2 \\right\\}\n$$\n其中 $Z \\sim \\mathcal{N}(0,1)$ 是一个标准正态随机变量，$(t)_+ = \\max(t, 0)$ 是取正部函数。期望项可以用一个涉及标准正态变量的概率密度函数 (PDF) $\\phi$ 和累积分布函数 (CDF) $\\Phi$ 的闭合形式计算：\n$$\n\\mathbb{E}\\left[(\\lvert Z \\rvert - \\lambda)^2_+\\right] = 2 \\int_{\\lambda}^{\\infty} (z-\\lambda)^2 \\phi(z) dz = 2 \\left[ (1+\\lambda^2)(1-\\Phi(\\lambda)) - \\lambda\\phi(\\lambda) \\right]\n$$\n对于 $\\lambda \\ge 0$。\n因此，寻找 $\\delta(K)$ 的问题简化为在非负标量 $\\lambda$ 上的一个一维凸优化问题。我们通过数值方法求解此问题，为测试套件中的每一对 $(n,k)$ 找到 $\\delta(K)$ 的值。然后，恢复成功的理论预测由一个指示函数给出：如果 $m > \\delta(K)$ 则为 $1$，否则为 $0$。\n\n一个特殊情况是 $k=0$，此时 $x^\\star = 0$。下降锥为 $K = \\{h : \\|h\\|_1 \\le 0\\} = \\{0\\}$。任何向量在 $\\{0\\}$ 上的投影都是零向量，所以 $\\delta(\\{0\\}) = \\mathbb{E}[\\|\\Pi_{\\{0\\}}(g)\\|_2^2] = 0$。\n\n**2. 通过模拟进行经验验证**\n\n我们执行蒙特卡洛模拟，来为每个测试用例 $(n,k,m,T)$ 估计成功恢复的经验概率。对于 $T$ 次试验中的每一次：\na. 生成一个随机的 $k$-稀疏信号 $x^\\star \\in \\mathbb{R}^n$。随机均匀地选择一个大小为 $k$ 的支撑集，非零项从标准正态分布中抽取。\nb. 生成一个 $m \\times n$ 的测量矩阵 $A$，其元素为独立同分布的 $\\mathcal{N}(0,1)$。\nc. 计算测量向量 $y = Ax^\\star$。\nd. 求解 $\\ell_1$-最小化问题（也称为基追踪），以找到估计值 $\\hat{x}$：\n$$\n\\hat{x} = \\arg\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = y\n$$\n这个凸优化问题可以通过将 $x$ 表示为两个非负向量的差（即 $x = u - v$，其中 $u_i, v_i \\ge 0$）来重构为一个线性规划 (LP) 问题。该 LP 问题是：\n$$\n\\min_{u, v \\in \\mathbb{R}^n} \\sum_{i=1}^n (u_i + v_i) \\quad \\text{subject to} \\quad A(u-v) = y, \\quad u \\ge 0, \\quad v \\ge 0.\n$$\n该 LP 问题使用标准的数值求解器求解。\ne. 如果解 $\\hat{x}$ 在数值上接近原始信号 $x^\\star$，则声明恢复“成功”。我们使用混合的绝对-相对误差容限：对于 $k > 0$，成功定义为 $\\|\\hat{x} - x^\\star\\|_2 / \\|x^\\star\\|_2 < 10^{-6}$；对于 $k=0$，成功定义为 $\\|\\hat{x}\\|_2 < 10^{-6}$。\n经验成功率是成功试验次数除以总试验次数 $T$。将此分数与理论指标进行比较。\n\n为指定的测试用例实现此过程，将展示由统计维度预测的相变的急剧性。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog, minimize_scalar\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes theoretical and empirical results for l1 recovery phase transitions.\n    \"\"\"\n    TOLERANCE = 1e-6\n\n    def calculate_statistical_dimension(n, k):\n        \"\"\"\n        Calculates the statistical dimension delta(K) for the l1 descent cone.\n        \n        The formula is:\n        delta(K) = k + min_{lambda >= 0} { (n-k) * E[(|Z|-lambda)^2_+] + k*lambda^2 }\n        where Z ~ N(0,1).\n        \"\"\"\n        if k == 0:\n            return 0.0\n        if k == n:\n            return float(n)\n\n        def expectation_term(lam):\n            \"\"\"Computes E[(|Z|-lambda)^2_+].\"\"\"\n            if lam  0:\n                # The minimization is over lambda = 0\n                return np.inf\n            # Using the closed-form expression\n            # 2 * [ (1+lam^2)*(1-Phi(lam)) - lam*phi(lam) ]\n            return 2 * ( (1 + lam**2) * (1 - norm.cdf(lam)) - lam * norm.pdf(lam) )\n\n        def objective(lam, n_val, k_val):\n            \"\"\"The function to be minimized over lambda.\"\"\"\n            return (n_val - k_val) * expectation_term(lam) + k_val * lam**2\n\n        # Numerically minimize the objective function to find the minimum value.\n        # The minimization is over lambda = 0.\n        # 'bounded' method is suitable for box constraints.\n        res = minimize_scalar(\n            objective, \n            args=(n, k), \n            bounds=(0, 10), # lambda is unlikely to be large, 10 is a safe upper bound.\n            method='bounded'\n        )\n        \n        min_val = res.fun\n        return k + min_val\n\n    def run_single_trial(n, k, m, rng):\n        \"\"\"\n        Runs a single trial of sparse recovery.\n        \"\"\"\n        # 1. Generate a random k-sparse signal x_star\n        x_star = np.zeros(n)\n        if k > 0:\n            support = rng.choice(n, k, replace=False)\n            x_star[support] = rng.standard_normal(k)\n\n        # 2. Generate measurement matrix A and measurements y\n        A = rng.standard_normal((m, n))\n        y = A @ x_star\n        \n        # 3. Solve the l1 minimization problem (Basis Pursuit) via Linear Programming\n        # Problem: min ||x||_1 s.t. Ax = y\n        # LP form:   min c.T @ z s.t. A_eq @ z = b_eq, z = 0\n        # where z = [u, v], x = u - v, ||x||_1 = 1.T @ u + 1.T @ v\n        c_lp = np.ones(2 * n)\n        A_lp = np.hstack([A, -A])\n        b_lp = y\n        \n        # Using 'highs' solver, which is robust and efficient.\n        res = linprog(c=c_lp, A_eq=A_lp, b_eq=b_lp, bounds=(0, None), method='highs')\n\n        if not res.success:\n            return False\n\n        # 4. Reconstruct the solution and check for success\n        x_sol = res.x[:n] - res.x[n:]\n        \n        if k == 0:\n            # Absolute error for the zero vector\n            norm_x_star = 0\n            if np.linalg.norm(x_sol)  TOLERANCE:\n                return True\n        else:\n            # Relative error for non-zero vectors\n            norm_x_star = np.linalg.norm(x_star)\n            if np.linalg.norm(x_sol - x_star) / norm_x_star  TOLERANCE:\n                return True\n        \n        return False\n\n    # Test cases: (n, k, m, T)\n    test_cases = [\n        (200, 10, 85, 8),\n        (200, 10, 55, 8),\n        (200, 10, 70, 8),\n        (100, 0, 1, 8),\n        (200, 60, 150, 6)\n    ]\n    \n    final_results = []\n    rng = np.random.default_rng(seed=42) # Seed for reproducibility\n    \n    # Cache for statistical dimension calculation\n    delta_cache = {}\n\n    for n, k, m, T in test_cases:\n        # Calculate theoretical prediction\n        if (n, k) not in delta_cache:\n            delta_cache[(n, k)] = calculate_statistical_dimension(n, k)\n        \n        delta_K = delta_cache[(n, k)]\n        theoretical_indicator = 1 if m > delta_K else 0\n        \n        # Run empirical simulations\n        success_count = 0\n        if T > 0:\n            for _ in range(T):\n                if run_single_trial(n, k, m, rng):\n                    success_count += 1\n            empirical_fraction = success_count / T\n        else:\n            empirical_fraction = 0.0\n\n        final_results.append(f\"[{theoretical_indicator},{empirical_fraction:.3f}]\")\n\n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n```", "id": "3481865"}]}