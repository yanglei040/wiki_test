{"hands_on_practices": [{"introduction": "在深入研究迭代算法的细节之前，理解其最终目标至关重要。对于像 LASSO 这样的复合凸优化问题，最优解由一阶平稳性条件定义，这涉及到梯度的概念推广——次梯度。本练习 [@problem_id:3392935] 提供了一个通过具体数值来应用这些理论概念的实践机会，您将学习如何构建次梯度并利用它来检验一个候选解的质量，从而将抽象的理论与可计算的度量联系起来。", "problem": "考虑逆问题和数据同化中迭代软阈值算法（ISTA）所基于的复合凸目标函数，\n$$\nF(x) \\equiv \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，$\\lambda > 0$，$x \\in \\mathbb{R}^{n}$。该问题的一阶最优性条件可以用 $\\ell_{1}$ 范数的次微分来表示。仅使用凸分析和线性代数的基本定义，推导 $F$ 的平稳性条件，然后用它来评估一个候选解的质量。\n\n令 $A = I_{3} \\in \\mathbb{R}^{3 \\times 3}$，$b = \\begin{pmatrix}2 \\\\ 0.4 \\\\ 0\\end{pmatrix}$，$\\lambda = \\frac{1}{2}$，以及候选向量 $x = \\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}$。在给定的 $x$ 处，构造一个有效的次梯度 $s \\in \\partial \\|x\\|_{1}$，使得平稳性残差的欧几里得范数最小化，并计算下式的最小可能值：\n$$\n\\|A^{\\top}(A x - b) + \\lambda s\\|_{2}.\n$$\n给出您的最终数值答案，保留四位有效数字。不需要单位。", "solution": "该问题是有效的，因为它在凸优化方面有科学依据，是适定的、客观的且内部一致的。\n\n待分析的目标函数是一个复合函数 $F(x) = f(x) + g(x)$，其中 $f(x)$ 是一个光滑、凸、可微的项，而 $g(x)$ 是一个凸、非光滑的项。具体来说，我们有：\n$$\nf(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}\n$$\n$$\ng(x) = \\lambda \\|x\\|_{1}\n$$\n根据凸分析中的 Fermat 法则，一个点 $x^*$ 是 $F(x)$ 的全局最小化子，当且仅当零向量是 $F(x)$ 在 $x^*$ 处的次微分的一个元素。这就是一阶最优性条件或平稳性条件：\n$$\n0 \\in \\partial F(x^*)\n$$\n由于 $f(x)$ 是可微的，$F(x)$ 的次微分是 $f(x)$ 的梯度和 $g(x)$ 的次微分之和：\n$$\n\\partial F(x) = \\nabla f(x) + \\partial g(x)\n$$\n首先，我们求 $f(x)$ 的梯度。函数为 $f(x) = \\frac{1}{2}(Ax - b)^{\\top}(Ax - b) = \\frac{1}{2}(x^{\\top}A^{\\top}Ax - 2b^{\\top}Ax + b^{\\top}b)$。关于 $x$ 的梯度是：\n$$\n\\nabla f(x) = \\frac{1}{2}(2A^{\\top}Ax - 2A^{\\top}b) = A^{\\top}(Ax - b)\n$$\n接下来，我们刻画 $g(x) = \\lambda \\|x\\|_{1}$ 的次微分。即 $\\partial g(x) = \\lambda \\partial \\|x\\|_{1}$。$\\ell_1$-范数为 $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$。其次微分是其各分量次微分的笛卡尔积。如果向量 $s \\in \\mathbb{R}^n$ 的分量 $s_i$ 满足以下条件，则称 $s$ 是 $\\ell_1$-范数在 $x$ 处的次梯度，记作 $s \\in \\partial \\|x\\|_1$：\n$$\ns_i =\n\\begin{cases}\n\\text{sgn}(x_i)   \\text{若 } x_i \\neq 0 \\\\\nc_i \\in [-1, 1]  \\text{若 } x_i = 0\n\\end{cases}\n$$\n因此，点 $x$ 的平稳性条件是存在一个次梯度 $s \\in \\partial\\|x\\|_1$ 使得：\n$$\nA^{\\top}(Ax - b) + \\lambda s = 0\n$$\n向量 $R(s) = A^{\\top}(A x - b) + \\lambda s$ 是平稳性残差。问题要求我们为给定的候选解 $x$ 找到一个次梯度 $s$，以最小化该残差的欧几里得范数 $\\|R(s)\\|_2$。\n\n我们被赋予了具体的值：\n$A = I_{3}$ ($3 \\times 3$ 单位矩阵)\n$b = \\begin{pmatrix} 2 \\\\ 0.4 \\\\ 0 \\end{pmatrix}$\n$\\lambda = \\frac{1}{2}$\n$x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$\n\n首先，我们计算梯度项 $\\nabla f(x) = A^{\\top}(Ax-b)$。由于 $A = I_3$，$A^{\\top}=I_3$，表达式简化为 $x-b$：\n$$\n\\nabla f(x) = x - b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 0.4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -0.4 \\\\ 1 \\end{pmatrix}\n$$\n接下来，我们刻画给定 $x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$ 的有效次梯度集合 $s \\in \\partial \\|x\\|_1$：\n对于第一个分量，$x_1 = 1 \\neq 0$，所以 $s_1 = \\text{sgn}(1) = 1$。\n对于第二个分量，$x_2 = 0$，所以 $s_2$ 可以是区间 $[-1, 1]$ 内的任意值。\n对于第三个分量，$x_3 = 1 \\neq 0$，所以 $s_3 = \\text{sgn}(1) = 1$。\n因此，任何有效的次梯度 $s$ 都必须是 $s = \\begin{pmatrix} 1 \\\\ s_2 \\\\ 1 \\end{pmatrix}$ 的形式，其中 $s_2 \\in [-1, 1]$。\n\n平稳性残差为 $R(s) = (x-b) + \\lambda s$。代入已知量：\n$$\nR(s) = \\begin{pmatrix} -1 \\\\ -0.4 \\\\ 1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ s_2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 + 0.5 \\\\ -0.4 + 0.5 s_2 \\\\ 1 + 0.5 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ -0.4 + 0.5 s_2 \\\\ 1.5 \\end{pmatrix}\n$$\n我们需要找到 $s_2 \\in [-1, 1]$ 的值，以最小化该残差的欧几里得范数 $\\|R(s)\\|_2$。最小化 $\\|R(s)\\|_2$ 等价于最小化其平方 $\\|R(s)\\|_2^2$：\n$$\n\\|R(s)\\|_2^2 = (-0.5)^2 + (-0.4 + 0.5 s_2)^2 + (1.5)^2\n$$\n$$\n\\|R(s)\\|_2^2 = 0.25 + (-0.4 + 0.5 s_2)^2 + 2.25 = 2.5 + (-0.4 + 0.5 s_2)^2\n$$\n为了最小化这个表达式，我们必须最小化项 $(-0.4 + 0.5 s_2)^2$。一个实数平方的最小值是 $0$，当平方内的项为零时取到。我们求解 $s_2$：\n$$\n-0.4 + 0.5 s_2 = 0 \\implies 0.5 s_2 = 0.4 \\implies s_2 = \\frac{0.4}{0.5} = 0.8\n$$\n值 $s_2 = 0.8$ 在允许的区间 $[-1, 1]$ 内。因此，使残差范数最小化的次梯度是 $s = \\begin{pmatrix} 1 \\\\ 0.8 \\\\ 1 \\end{pmatrix}$。\n\n通过这个 $s_2$ 的最优选择，残差范数平方的最小值为：\n$$\n\\min \\|R(s)\\|_2^2 = 2.5 + (-0.4 + 0.5 \\times 0.8)^2 = 2.5 + (-0.4 + 0.4)^2 = 2.5 + 0^2 = 2.5\n$$\n范数的最小可能值是该值的平方根：\n$$\n\\min \\|R(s)\\|_2 = \\sqrt{2.5}\n$$\n问题要求一个数值答案，保留四位有效数字。\n$$\n\\sqrt{2.5} \\approx 1.5811388...\n$$\n保留四位有效数字，我们得到 $1.581$。", "answer": "$$\\boxed{1.581}$$", "id": "3392935"}, {"introduction": "理论上的最优性条件为我们提供了目标，但在实践中，迭代算法需要一个明确的规则来判断何时停止。简单地设置最大迭代次数是不够的，我们需要一个能反映解的质量的准则。本练习 [@problem_id:3392931] 将指导您开发一个基于原始-对偶间隙（primal-dual gap）的稳健停止准则，这是一个理论上合理且可计算的度量，用于衡量当前迭代点与真实最优解的差距，是实现高效、可靠算法的关键一步。", "problem": "考虑一个在稀疏逆问题和数据同化中很标准的凸复合优化问题：最小化一个光滑的数据拟合项加上一个非光滑的稀疏性促进惩罚项。具体来说，给定一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$、一个数据向量 $b \\in \\mathbb{R}^{m}$ 和一个正则化参数 $\\lambda  0$，考虑最小绝对收缩和选择算子 (LASSO) 的目标函数，其原始目标为\n$$\nP(x) \\equiv \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1, \\quad x \\in \\mathbb{R}^n.\n$$\n您将为迭代软阈值算法 (ISTA) 设计一个实用的停止准则，该准则基于一个原始-对偶间隙，该间隙是原始次优性的一个上界。这是通过从给定的原始迭代点构造一个对偶可行点，并评估一个可计算的次优性上界来实现的。\n\n问题要求：\n\n1) 从凸共轭、Fenchel 对偶以及范数的次梯度的基本定义出发，从第一性原理推导出与上述原始目标 $P(x)$ 相关联的对偶问题。然后，利用弱对偶性，推导出一个可计算的原始-对偶间隙表达式 $G(x,\\nu)$，该表达式对于任何原始点 $x$ 和任何对偶可行点 $\\nu$ 都是非负的。\n\n2) 从任意原始迭代点 $x$，仅使用 ISTA 迭代过程中可用的量（例如，残差 $Ax - b$ 以及与 $A$ 和 $A^\\top$ 的矩阵-向量乘积）来构造一个对偶可行点 $\\nu \\in \\mathbb{R}^m$。您的构造必须保证对偶约束的可行性，而无需知道确切解。\n\n3) 利用 ISTA 对光滑项 $\\frac{1}{2}\\|Ax-b\\|_2^2$（其梯度是 Lipschitz 连续的，具有由 $A$ 决定的适当常数）和非光滑项 $\\lambda \\|x\\|_1$ 的标准近端梯度结构，指定一个终止规则，该规则使用原始-对偶间隙来判断是否在要求的容差 $\\varepsilon  0$ 内收敛。证明在终止时得到的间隙是实际原始次优性 $P(x) - P(x^\\star)$ 的一个上界，其中 $x^\\star$ 表示一个最小化子。\n\n4) 实现一个程序，该程序：\n   - 计算 $A$ 的谱范数以确定 ISTA 的有效固定步长。\n   - 从零向量开始运行 ISTA，在每次迭代中从当前的原始迭代点构造一个对偶可行点，评估原始-对偶间隙，并在间隙至多为 $\\varepsilon$ 或达到最大迭代次数时终止。\n   - 对于每个测试用例，返回最终的原始-对偶间隙，形式为一个浮点数。\n\n使用以下测试套件。所有随机生成必须使用指定的种子以保证可复现性。\n\n- 测试用例 1（常规路径，过参数化，带噪声的稀疏信号）：\n  - 维度：$m = 40$，$n = 60$。\n  - 随机种子：$123$。\n  - 生成 $A$，其元素为均值为 $0$、方差为 $1/m$ 的独立同分布正态分布。\n  - 生成一个真实的稀疏 $x_{\\mathrm{true}} \\in \\mathbb{R}^n$，在均匀随机位置上恰好有 $k=6$ 个非零值，非零值为独立同分布的标准正态分布。\n  - 生成噪声 $\\eta \\in \\mathbb{R}^m$，其元素为均值为 $0$、标准差为 $\\sigma = 10^{-2}$ 的独立同分布正态分布。\n  - 设置 $b = A x_{\\mathrm{true}} + \\eta$。\n  - 设置 $\\lambda = 0.1 \\cdot \\|A^\\top b\\|_\\infty$。\n  - 容差 $\\varepsilon = 10^{-6}$，最大迭代次数 $10{,}000$。\n\n- 测试用例 2（边界情况，单位算子）：\n  - 维度：$m = 30$，$n = 30$。\n  - 矩阵 $A = I$（$30 \\times 30$ 单位矩阵）。\n  - 随机种子：$2024$。\n  - 生成 $b$，其元素为独立同分布的标准正态分布。\n  - 设置 $\\lambda = \\|A^\\top b\\|_\\infty$。\n  - 容差 $\\varepsilon = 10^{-12}$，最大迭代次数 $1{,}000$。\n\n- 测试用例 3（欠定系统）：\n  - 维度：$m = 30$，$n = 80$。\n  - 随机种子：$321$。\n  - 生成 $A$，其元素为均值为 $0$、方差为 $1/m$ 的独立同分布正态分布。\n  - 生成 $b$，其元素为独立同分布的标准正态分布。\n  - 设置 $\\lambda = 0.01 \\cdot \\|A^\\top b\\|_\\infty$。\n  - 容差 $\\varepsilon = 10^{-5}$，最大迭代次数 $20{,}000$。\n\n- 测试用例 4（病态算子）：\n  - 维度：$m = 50$，$n = 50$。\n  - 随机种子：$999$。\n  - 生成一个基础矩阵，其元素为均值为 $0$、方差为 $1/m$ 的独立同分布正态分布，然后将第 $j$ 列乘以一个缩放因子 $s_j = 10^{4 \\cdot (j-1)/(n-1)}$（对于 $j \\in \\{1,\\dots,n\\}$），以引入一个大约为 $10^4$ 的条件数。\n  - 生成 $b$，其元素为独立同分布的标准正态分布。\n  - 设置 $\\lambda = 0.05 \\cdot \\|A^\\top b\\|_\\infty$。\n  - 容差 $\\varepsilon = 10^{-5}$，最大迭代次数 $20{,}000$。\n\n您的程序必须生成单行输出，其中包含结果，形式为四个测试用例的最终原始-对偶间隙的逗号分隔列表，并用方括号括起来，顺序与上述测试用例的顺序一致（例如，$[g_1,g_2,g_3,g_4]$）。输出必须是浮点数。本问题不涉及物理单位或角度单位。", "solution": "该问题要求为解决 LASSO 优化问题的迭代软阈值算法 (ISTA) 设计并实现一个基于原始-对偶间隙的停止准则。解决方案将按要求分为三部分呈现：对偶问题和间隙的推导、对偶可行点的构造、终止规则的规范，最后是实现细节。\n\n### 第 1 部分：Fenchel 对偶和原始-对偶间隙的推导\n\n原始 LASSO 问题由下式给出：\n$$\nP(x) \\equiv \\min_{x \\in \\mathbb{R}^n} \\left( \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 \\right)\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，且 $\\lambda > 0$。\n\n为了推导对偶问题，我们通过引入一个辅助变量 $r \\in \\mathbb{R}^m$ 并设置约束 $r = Ax - b$ 来重新表述原始问题：\n$$\n\\min_{x \\in \\mathbb{R}^n, r \\in \\mathbb{R}^m} \\frac{1}{2}\\|r\\|_2^2 + \\lambda \\|x\\|_1 \\quad \\text{subject to} \\quad r - Ax + b = 0.\n$$\n我们通过为等式约束引入一个对偶变量（拉格朗日乘子）$\\nu \\in \\mathbb{R}^m$ 来构造拉格朗日函数：\n$$\nL(x, r, \\nu) = \\frac{1}{2}\\|r\\|_2^2 + \\lambda \\|x\\|_1 + \\nu^\\top (r - Ax + b)\n$$\n拉格朗日对偶函数 $D(\\nu)$ 是拉格朗日函数关于原始变量 $x$ 和 $r$ 的下确界：\n$$\nD(\\nu) = \\inf_{x, r} L(x, r, \\nu) = \\inf_{r} \\left( \\frac{1}{2}\\|r\\|_2^2 + \\nu^\\top r \\right) + \\inf_{x} \\left( \\lambda \\|x\\|_1 - \\nu^\\top Ax \\right) + \\nu^\\top b\n$$\n这两个下确界可以分开计算。\n第一项是关于 $r$ 的无约束二次最小化。当梯度为零时达到最小值：$\\nabla_r (\\frac{1}{2}r^\\top r + \\nu^\\top r) = r + \\nu = 0$，这意味着 $r = -\\nu$。将其代回得到最小值为：$\\frac{1}{2}(-\\nu)^\\top(-\\nu) + \\nu^\\top(-\\nu) = \\frac{1}{2}\\|\\nu\\|_2^2 - \\|\\nu\\|_2^2 = -\\frac{1}{2}\\|\\nu\\|_2^2$。\n\n第二项涉及 $\\ell_1$-范数的 Fenchel 共轭。$\\inf_{x} (\\lambda \\|x\\|_1 - (A^\\top\\nu)^\\top x) = -\\sup_{x} ((A^\\top\\nu)^\\top x - \\lambda\\|x\\|_1)$。此上确界是 $g(x)=\\lambda\\|x\\|_1$ 的共轭函数 $g^*(w) = \\sup_x (w^\\top x - g(x))$ 在 $w=A^\\top\\nu$ 处求值的结果。该共轭函数是一个指示函数：\n$$\ng^*(w) = \\begin{cases} 0   \\text{if } \\|w\\|_\\infty \\le \\lambda \\\\ +\\infty   \\text{otherwise} \\end{cases}\n$$\n因此，如果 $\\|A^\\top\\nu\\|_\\infty \\le \\lambda$，该项为 $0$；否则为 $-\\infty$。\n\n将各项结合起来，对偶函数为：\n$$\nD(\\nu) = -\\frac{1}{2}\\|\\nu\\|_2^2 + \\nu^\\top b, \\quad \\text{provided } \\|A^\\top\\nu\\|_\\infty \\le \\lambda.\n$$\n对偶问题是在 $\\nu$ 上最大化此函数：\n$$\n\\max_{\\nu \\in \\mathbb{R}^m} D(\\nu) \\equiv \\max_{\\nu \\in \\mathbb{R}^m} \\left( \\nu^\\top b - \\frac{1}{2}\\|\\nu\\|_2^2 \\right) \\quad \\text{subject to} \\quad \\|A^\\top\\nu\\|_\\infty \\le \\lambda.\n$$\n根据弱对偶性，对于任何原始可行点 $x$ 和任何对偶可行点 $\\nu$（即满足约束 $\\|A^\\top\\nu\\|_\\infty \\le \\lambda$ 的 $\\nu$），我们有 $P(x) \\ge D(\\nu)$。原始-对偶间隙 $G(x, \\nu)$ 定义为它们的差，它总是非负的：\n$$\nG(x, \\nu) = P(x) - D(\\nu) = \\left(\\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\\right) - \\left(\\nu^\\top b - \\frac{1}{2}\\|\\nu\\|_2^2\\right)\n$$\n$$\nG(x, \\nu) = \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 - \\nu^\\top b + \\frac{1}{2}\\|\\nu\\|_2^2 \\ge 0.\n$$\n这个表达式为原始次优性提供了一个可计算的上界，将在第 3 部分中展示。\n\n### 第 2 部分：对偶可行点的构造\n\n在 ISTA 的执行过程中，我们有一系列原始迭代点 $\\{x_k\\}$。对于每个迭代点 $x_k$，我们需要构造一个相应的对偶可行点 $\\nu_k$，用于评估间隙。一个对偶可行点必须满足 $\\|A^\\top\\nu_k\\|_\\infty \\le \\lambda$。\n\n最优性的 Karush-Kuhn-Tucker (KKT) 条件为最优对偶变量的形式提供了启示。对于我们选择的拉格朗日函数，在最优原始-对偶对 $(x^\\star, r^\\star, \\nu^\\star)$ 处，我们有 $r^\\star+\\nu^\\star=0$ 和 $r^\\star=Ax^\\star-b$，这意味着 $\\nu^\\star = -(Ax^\\star-b) = b-Ax^\\star$。这启发我们通过缩放向量 $b-Ax_k$ 来为非最优迭代点 $x_k$ 构造一个对偶可行点 $\\nu_k$。\n\n让我们测试一个形如 $\\nu_k = c_k(b-Ax_k)$ 的候选点，其中 $c_k > 0$ 是一个标量。为了使该点是对偶可行的，我们必须有：\n$$\n\\|A^\\top(c_k(b-Ax_k))\\|_\\infty \\le \\lambda \\implies c_k \\|A^\\top(b-Ax_k)\\|_\\infty \\le \\lambda\n$$\n如果 $A^\\top(b-Ax_k) = 0$，任何 $c_k$ 都有效。否则，我们必须有 $c_k \\le \\frac{\\lambda}{\\|A^\\top(b-Ax_k)\\|_\\infty}$。为了使对偶目标 $D(\\nu_k)$ 尽可能大（从而使间隙尽可能小），我们希望 $\\nu_k$ “大”一些，因此我们选择尽可能大的缩放因子 $c_k$。\n\n一个能处理所有情况的稳健选择是：\n$$\n\\nu_k = \\frac{\\lambda}{\\max(\\lambda, \\|A^\\top(b-Ax_k)\\|_\\infty)} \\cdot (b-Ax_k)\n$$\n让我们验证这种构造的可行性。令 $C_k = \\max(\\lambda, \\|A^\\top(b-Ax_k)\\|_\\infty)$。则 $\\nu_k = \\frac{\\lambda}{C_k}(b-Ax_k)$。\n$$\n\\|A^\\top\\nu_k\\|_\\infty = \\left\\| A^\\top \\left( \\frac{\\lambda}{C_k}(b-Ax_k) \\right) \\right\\|_\\infty = \\frac{\\lambda}{C_k} \\|A^\\top(b-Ax_k)\\|_\\infty.\n$$\n根据 $C_k$ 的定义，我们有 $\\|A^\\top(b-Ax_k)\\|_\\infty \\le C_k$。因此，\n$$\n\\|A^\\top\\nu_k\\|_\\infty \\le \\frac{\\lambda}{C_k} \\cdot C_k = \\lambda.\n$$\n这种构造保证了对于任何原始迭代点 $x_k$，$\\nu_k$ 都是一个对偶可行点。所有需要的量，即残差 $Ax_k-b$ 和与 $A^\\top$ 的矩阵-向量乘积，在 ISTA 迭代中都是现成可用的。\n\n### 第 3 部分：ISTA 终止规则和次优性上界证明\n\nISTA 算法是一种近端梯度方法。对于 LASSO 目标函数 $P(x) = f(x) + h(x)$，其中 $f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2$ 是光滑部分，$h(x) = \\lambda\\|x\\|_1$ 是非光滑部分，其更新规则为：\n$$\nx_{k+1} = \\text{prox}_{\\alpha h}(x_k - \\alpha \\nabla f(x_k))\n$$\n这里，$\\nabla f(x) = A^\\top(Ax-b)$，而 $h(x)$ 的近端算子是软阈值算子，$\\mathcal{S}_{\\tau}(z)_i = \\text{sign}(z_i)\\max(|z_i|-\\tau, 0)$。因此，ISTA 的更新是：\n$$\nx_{k+1} = \\mathcal{S}_{\\alpha\\lambda}(x_k - \\alpha A^\\top(Ax_k-b))\n$$\n为保证收敛，步长 $\\alpha$ 必须满足 $0  \\alpha \\le 1/L$，其中 $L$ 是 $\\nabla f(x)$ 的 Lipschitz 常数。Lipschitz 常数是 $L = \\|A^\\top A\\|_2 = \\|A\\|_2^2$，其中 $\\|A\\|_2$ 是 $A$ 的谱范数。一个有效的固定步长是 $\\alpha = 1/L$。\n\n建议的终止规则如下：\n在 ISTA 的每次迭代 $k$ 中，从初始点 $x_0$ 开始：\n1.  计算当前原始迭代点 $x_k$。\n2.  使用第 2 部分的方法构造对偶可行点 $\\nu_k$。\n3.  评估原始-对偶间隙 $G(x_k, \\nu_k) = P(x_k) - D(\\nu_k)$。\n4.  如果对于给定的容差 $\\varepsilon > 0$，有 $G(x_k, \\nu_k) \\le \\varepsilon$，则终止算法。否则，计算 $x_{k+1}$ 并继续。\n\n**次优性上界证明：**\n令 $x^\\star$ 是原始问题 $P(x)$ 的一个最小化子，因此 $P(x^\\star) = \\min_x P(x)$。一个迭代点 $x_k$ 的原始次优性是非负量 $P(x_k) - P(x^\\star)$。\n根据弱对偶性，对于任何原始点 $x$ 和任何对偶可行点 $\\nu$，我们有 $P(x) \\ge D(\\nu)$。\n由于强对偶性对于 LASSO 问题成立，我们有 $P(x^\\star) = \\max_\\nu D(\\nu)$。\n令 $\\nu_k$ 是从原始迭代点 $x_k$ 构造的对偶可行点。因为 $\\nu_k$ 是对偶可行的，必须有 $D(\\nu_k) \\le \\max_\\nu D(\\nu) = P(x^\\star)$。\n因此，我们有以下不等式：\n$$\nD(\\nu_k) \\le P(x^\\star) \\le P(x_k)\n$$\n原始次优性是 $P(x_k) - P(x^\\star)$。利用不等式 $D(\\nu_k) \\le P(x^\\star)$，我们可以写出：\n$$\nP(x_k) - P(x^\\star) \\le P(x_k) - D(\\nu_k)\n$$\n右边的项正是我们可计算的原始-对偶间隙 $G(x_k, \\nu_k)$。因此，我们已经证明：\n$$\nP(x_k) - P(x^\\star) \\le G(x_k, \\nu_k)\n$$\n这证明了原始-对偶间隙 $G(x_k, \\nu_k)$ 是真实原始次优性的一个上界。因此，如果算法在 $G(x_k, \\nu_k) \\le \\varepsilon$ 时终止，我们保证了原始次优性 $P(x_k) - P(x^\\star)$ 也至多为 $\\varepsilon$。这验证了所提出的停止准则。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the ISTA algorithm with a primal-dual gap stopping criterion\n    on a suite of test cases for the LASSO problem.\n    \"\"\"\n\n    def soft_threshold(z, tau):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    def run_ista(A, b, lam, tol, max_iter):\n        \"\"\"\n        Solves the LASSO problem using ISTA with a primal-dual gap stopping criterion.\n\n        Args:\n            A (np.ndarray): The measurement matrix.\n            b (np.ndarray): The data vector.\n            lam (float): The regularization parameter lambda.\n            tol (float): The tolerance for the primal-dual gap.\n            max_iter (int): The maximum number of iterations.\n\n        Returns:\n            float: The final primal-dual gap.\n        \"\"\"\n        m, n = A.shape\n        \n        # Determine a valid stepsize alpha\n        L = np.linalg.norm(A, 2)**2\n        # A lipshitz constant of 0 can occur if A is a zero matrix.\n        # In this case, x=0 is the solution, and the gradient is constant. Any positive step size will work.\n        # We set alpha=1.0 for this edge case.\n        if L == 0:\n            alpha = 1.0\n        else:\n            alpha = 1.0 / L\n\n        # Initialization\n        x = np.zeros(n)\n        \n        final_gap = np.inf\n\n        for k in range(max_iter):\n            # Primal objective P(x) = 0.5 * ||Ax - b||^2 + lambda * ||x||_1\n            residual = A @ x - b\n            primal_obj = 0.5 * np.linalg.norm(residual)**2 + lam * np.linalg.norm(x, 1)\n\n            # Construct dual feasible point nu\n            # Dual D(nu) = b.T @ nu - 0.5 * ||nu||^2, s.t. ||A.T @ nu||_inf = lambda\n            # Construction: nu = c * (b - Ax), where c is a scaling factor.\n            # We use c = lambda / max(lambda, ||A.T @ (b-Ax)||_inf)\n            b_minus_Ax = -residual\n            At_b_minus_Ax = A.T @ b_minus_Ax\n            norm_At_b_minus_Ax_inf = np.linalg.norm(At_b_minus_Ax, np.inf)\n\n            # Epsilon to avoid division by zero in the theoretical case where max is zero.\n            # max(lambda, norm) should be non-zero since lambda > 0.\n            c = lam / np.maximum(lam, norm_At_b_minus_Ax_inf)\n            nu = c * b_minus_Ax\n\n            # Dual objective D(nu)\n            dual_obj = b.T @ nu - 0.5 * np.linalg.norm(nu)**2\n\n            # Primal-dual gap G(x, nu) = P(x) - D(nu)\n            gap = primal_obj - dual_obj\n            final_gap = gap\n\n            if gap = tol:\n                break\n\n            # ISTA update\n            grad_f = A.T @ residual\n            x = soft_threshold(x - alpha * grad_f, alpha * lam)\n        \n        return final_gap\n\n    test_cases = [\n        {'m': 40, 'n': 60, 'seed': 123, 'k': 6, 'sigma': 1e-2, 'lam_factor': 0.1, 'tol': 1e-6, 'max_iter': 10000, 'name': 'overparameterized_sparse'},\n        {'m': 30, 'n': 30, 'seed': 2024, 'lam_factor': 1.0, 'tol': 1e-12, 'max_iter': 1000, 'name': 'identity'},\n        {'m': 30, 'n': 80, 'seed': 321, 'lam_factor': 0.01, 'tol': 1e-5, 'max_iter': 20000, 'name': 'underdetermined'},\n        {'m': 50, 'n': 50, 'seed': 999, 'lam_factor': 0.05, 'tol': 1e-5, 'max_iter': 20000, 'name': 'ill_conditioned'}\n    ]\n\n    results = []\n    for case in test_cases:\n        m, n, seed = case['m'], case['n'], case['seed']\n        rng = np.random.default_rng(seed)\n\n        if case['name'] == 'identity':\n            A = np.eye(m)\n            b = rng.standard_normal(size=m)\n        elif case['name'] == 'ill_conditioned':\n            A_base = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n            scaling_factors = 10**(4.0 * np.arange(n) / (n - 1))\n            A = A_base * scaling_factors # Broadcasting column-wise\n            b = rng.standard_normal(size=m)\n        else: # Default case for 'overparameterized_sparse' and 'underdetermined'\n            A = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n            if case['name'] == 'overparameterized_sparse':\n                k, sigma = case['k'], case['sigma']\n                x_true = np.zeros(n)\n                indices = rng.choice(n, k, replace=False)\n                x_true[indices] = rng.standard_normal(size=k)\n                noise = rng.normal(0, sigma, size=m)\n                b = A @ x_true + noise\n            else: # 'underdetermined'\n                b = rng.standard_normal(size=m)\n\n        lam = case['lam_factor'] * np.linalg.norm(A.T @ b, np.inf)\n        # Ensure lambda is strictly positive\n        if lam == 0:\n            lam = 1e-10\n\n        final_gap = run_ista(A, b, lam, case['tol'], case['max_iter'])\n        results.append(final_gap)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3392931"}, {"introduction": "虽然迭代软阈值算法（ISTA）保证收敛，但其收敛速度可能较慢，尤其是在处理病态问题时。为了解决这个问题，研究人员提出了快速迭代软阈值算法（FISTA），它利用动量项来加速收敛。然而，这种加速并非没有代价，它可能导致目标函数值的非单调下降。本练习 [@problem_id:3392933] 旨在深入探讨这一现象，并挑战您设计和实现一个“单调”版本的 FISTA，它既能保持 FISTA 的快速收敛优势，又能确保算法的稳定性。", "problem": "考虑在稀疏线性逆问题和稀疏数据同化中出现的复合凸优化问题：最小化一个光滑的数据失配项和一个促进稀疏性的惩罚项之和，\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + g(x),\n$$\n其中\n$$\nf(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2, \\quad g(x) \\equiv \\lambda \\|x\\|_1,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个给定矩阵，$b \\in \\mathbb{R}^m$ 是观测向量，$\\lambda \\ge 0$ 是正则化参数。假设 $f$ 的梯度是 Lipschitz 连续的，其 Lipschitz 常数为 $L0$，即对于所有 $u,v \\in \\mathbb{R}^n$，\n$$\n\\|\\nabla f(u) - \\nabla f(v)\\|_2 \\le L \\|u - v\\|_2,\n$$\n这等价于下降引理成立：\n$$\nf(u) \\le f(v) + \\nabla f(v)^\\top(u-v) + \\tfrac{L}{2}\\|u-v\\|_2^2.\n$$\n迭代软阈值算法 (Iterative Soft-Thresholding Algorithm, ISTA)，也称为近端梯度法，使用 $g$ 的近端算子和对 $f$ 的梯度步来生成序列 $\\{x^k\\}$，步长为 $\\tau \\in (0,1/L]$。快速迭代软阈值算法 (Fast Iterative Soft-Thresholding Algorithm, FISTA) 通过一个外推（动量）步来增强 ISTA，该步骤使用一个由 $\\{x^k\\}$ 构建的辅助序列 $\\{y^k\\}$。\n\n任务 1. 从 $f$ 的下降引理和 $g$ 的近端算子定义出发，提供一个清晰、严谨的论证，解释为什么 FISTA 可能会产生一个非单调的目标函数序列 $\\{F(x^k)\\}$；也就是说，对于某些 $k$，可能会出现 $F(x^{k+1})  F(x^k)$ 的情况。您的解释应基于以下基本原理：\n- 由 $\\nabla f$ 的 Lipschitz 连续性所隐含的、$f$ 在一个基点处的上界化，\n- 近端更新的定义，即其作为一个二次上界与 $g$ 之和的最小化子，\n- 使用当前点 $x^k$ (ISTA) 与外推点 $y^k$ (FISTA) 作为近端梯度步的基点之间的差异。\n\n不要引用任何关于 FISTA 行为的既有定理；直接从上述基本原理推导出其原因。\n\n任务 2. 通过使用局部检查来保障更新 $x^{k+1}$，设计一个 FISTA 的单调变体。该保障措施必须是纯粹局部的（仅依赖于在第 k 次迭代时可计算的量），并且必须保证所产生的目标函数序列是单调非增的。精确定义该保障决策以及它所选择的两种可能的更新方式。您的设计必须包括：\n- 一个将候选的加速更新与基线参考进行比较的条件，\n- 当该条件被违反时，回退到一个非加速的近端梯度步，\n- 一条在回退后用于重置或更新动量/外推参数以保持单调性的规则。\n\n从下降引理和近端最优性条件出发，证明您所设计的受保障方案为何能产生一个单调非增的序列 $\\{F(x^k)\\}$。\n\n任务 3. 实现一个完整的程序，构建几个测试实例，并在单调性和最终目标函数值方面对 ISTA、FISTA 和您的 FISTA 单调变体 (MFISTA) 进行数值比较。请遵循以下所有规范，以确保可复现性和覆盖范围。\n\n定义和实现要求：\n- 使用 $g(x)=\\lambda\\|x\\|_1$ 的近端算子，\n$$\n\\operatorname{prox}_{\\tau g}(v) \\equiv \\arg\\min_{u \\in \\mathbb{R}^n}\\left\\{\\lambda\\|u\\|_1 + \\tfrac{1}{2\\tau}\\|u - v\\|_2^2\\right\\}。\n$$\n- 使用步长 $\\tau = 1/L$，其中 $L$ 是 $\\nabla f$ 的 Lipschitz 常数。对于 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2$，将 $L$ 取为 $A^\\top A$ 的最大特征值（即 $A$ 的最大奇异值的平方）。\n- 所有方法均从零向量 $x^0 = 0$ 开始初始化，对于加速方法，在标准外推参数序列中使用 $t^0 = 1$。\n- 对每个测试实例上的每种方法，运行固定的迭代次数 $K = 200$。\n\n测试套件（四种情况）：\n- 情况 1（病态前向模型，中等正则化）：设置 $m=50, n=100$，随机种子 $s=7$。通过生成一个具有独立标准正态分布条目的高斯矩阵，计算其紧奇异值分解，将其奇异值替换为一个从 $1$ 线性递减到 $10^{-3}$ 的向量，然后重新组装 $A$ 来构造具有指定奇异值的 $A$。通过均匀随机选择 $10$ 个唯一索引并将其条目设置为独立的标准正态值（所有其他条目为零）来构造一个 $10$-稀疏的真实解 $x^\\star$。生成 $b = A x^\\star + \\eta$，其中 $\\eta$ 是方差为 $\\sigma^2 = 10^{-4}$（标准差 $\\sigma = 10^{-2}$）的独立高斯噪声。使用 $\\lambda = 10^{-2}$。\n- 情况 2（良态前向模型，中等正则化）：设置 $m=80, n=100$，种子 $s=11$。生成一个具有独立标准正态分布条目的 $A$，然后将每列缩放至单位欧几里得范数。如情况 1 那样生成具有 $10$ 个非零项的 $x^\\star$ 和噪声 $\\sigma = 10^{-2}$。使用 $\\lambda = 10^{-2}$。\n- 情况 3（良态，强正则化）：与情况 2 相同，但种子为 $s=13$，$\\lambda = 1$。\n- 情况 4（良态，无正则化）：与情况 2 相同，但种子为 $s=17$，$\\lambda = 0$。\n\n对于每种情况：\n- 计算 ISTA、FISTA 和您的 MFISTA 在迭代过程中的目标函数值 $\\{F(x^k)\\}$。\n- 确定每种方法的目标函数序列是否是单调非增的，即在数值容差范围内，$F(x^{k+1}) \\le F(x^k)$ 是否对所有 $k \\in \\{0,\\dots,K-1\\}$ 都成立。\n- 记录每种方法在 $K$ 次迭代后的最终目标函数值。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个长度为四的列表（每个测试用例一个元素）。每个元素本身必须是一个列表，按以下顺序包含五个条目：\n  1. 一个布尔值，指示 FISTA 目标函数序列是否是单调的，\n  2. 一个布尔值，指示 MFISTA 目标函数序列是否是单调的，\n  3. K 次迭代后 ISTA 的最终目标函数值（一个浮点数），\n  4. K 次迭代后 FISTA 的最终目标函数值（一个浮点数），\n  5. K 次迭代后 MFISTA 的最终目标函数值（一个浮点数）。\n- 顶层列表必须打印为单行，格式为用方括号括起来的逗号分隔列表（例如，“[[...],[...],[...],[...]]”）。不得打印任何其他文本。", "solution": "我们考虑复合凸目标函数 $F(x) \\equiv f(x) + g(x)$，其中 $f(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2$ 且 $g(x) \\equiv \\lambda\\|x\\|_1$。梯度 $\\nabla f(x) = A^\\top(Ax - b)$ 是 Lipschitz 连续的，其 Lipschitz 常数为 $L = \\|A^\\top A\\|_2$，即 $A^\\top A$ 的谱范数（最大特征值），等价于 $A$ 的最大奇异值的平方。\n\n原理 1：下降引理和二次上界。对于任意 $u,v \\in \\mathbb{R}^n$，$\\nabla f$ 的 Lipschitz 连续性意味着下降引理成立：\n$$\nf(u) \\le f(v) + \\nabla f(v)^\\top(u-v) + \\tfrac{L}{2}\\|u-v\\|_2^2 \\equiv Q_L(u;v).\n$$\n因此，$Q_L(u;v)$ 是一个围绕基点 $v$ 构建的、$f$ 的全局上界（上界函数）。\n\n原理 2：近端梯度步最小化上界函数与 $g$ 的和。对于步长 $\\tau \\in (0,1/L]$，从基点 $v$ 开始的近端梯度更新计算\n$$\nx^+ \\equiv \\operatorname{prox}_{\\tau g}\\!\\left(v - \\tau \\nabla f(v)\\right) \\in \\arg\\min_{u} \\left\\{g(u) + \\tfrac{1}{2\\tau}\\|u - (v - \\tau \\nabla f(v))\\|_2^2\\right\\}.\n$$\n等价地，$x^+$ 最小化 $g(u) + Q_{1/\\tau}(u;v) - f(v)$，因此\n$$\ng(x^+) + Q_{1/\\tau}(x^+;v) \\le g(v) + Q_{1/\\tau}(v;v) = g(v) + f(v).\n$$\n对于 $\\tau \\le 1/L$，根据下降引理我们有 $Q_{1/\\tau}(u;v) \\ge f(u)$，所以\n$$\nF(x^+) = f(x^+) + g(x^+) \\le g(x^+) + Q_{1/\\tau}(x^+;v) \\le g(v) + f(v) = F(v).\n$$\n\n对 ISTA 的解释。在 ISTA 中，我们取 $v = x^k$，并定义\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\left(x^k - \\tau \\nabla f(x^k)\\right).\n$$\n将 $v = x^k$ 应用于上述不等式，可以显示其单调递减性：\n$$\nF(x^{k+1}) \\le F(x^k) \\quad \\text{对所有 } k \\text{ 当 } \\tau \\le 1/L.\n$$\n\nFISTA 为何可能是非单调的。FISTA 根据 $x^k$ 和 $x^{k-1}$ 构造一个外推点 $y^k$，然后以 $y^k$ 为基点执行近端梯度步：\n$$\nx^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\left(y^k - \\tau \\nabla f(y^k)\\right), \\quad \\text{其中 } y^k \\ne x^k \\text{ 是由动量定义的}。\n$$\n根据原理 2，当 $v = y^k$ 时，我们有\n$$\nF(x^{k+1}) \\le F(y^k).\n$$\n然而，外推点 $y^k$ 并不被约束以满足 $F(y^k) \\le F(x^k)$；选择动量步是为了提高收敛速率，而不是为了保证下降，并且可能会发生过冲。因此，保证 ISTA 单调性的不等式链，即\n$$\nF(x^{k+1}) \\le F(y^k) \\le F(x^k),\n$$\n可能在第二个不等式处不成立。当 $F(y^k) > F(x^k)$ 时，完全有可能 $F(x^{k+1}) \\in [F(x^k), F(y^k)]$，这意味着 $F(x^{k+1}) > F(x^k)$。因此，FISTA 会产生非单调的目标函数序列，因为它的近端步锚定于 $y^k$，而下降引理只提供了相对于基点 $y^k$ 的上界，而不是相对于当前迭代点 $x^k$ 的上界。\n\n通过局部保障措施设计单调变体。为确保单调性，我们将加速候选点与一个局部接受测试相结合。在第 k 次迭代，给定当前点 $x^k$ 和外推点 $y^k$，构造加速候选点\n$$\n\\hat{x}^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\left(y^k - \\tau \\nabla f(y^k)\\right).\n$$\n执行局部检查：如果 $F(\\hat{x}^{k+1}) \\le F(x^k)$，则接受该加速候选点，并像 FISTA 一样更新动量参数；否则，拒绝该候选点，从 $x^k$ 计算回退的 ISTA 步，\n$$\n\\tilde{x}^{k+1} = \\operatorname{prox}_{\\tau g}\\!\\left(x^k - \\tau \\nabla f(x^k)\\right),\n$$\n并重置动量（例如，将外推参数设置为其初始值，以便下一个 $y^{k+1}$ 等于 $x^{k+1}$）。被接受的 $x^{k+1}$ 是\n$$\nx^{k+1} = \\begin{cases}\n\\hat{x}^{k+1},   \\text{如果 } F(\\hat{x}^{k+1}) \\le F(x^k),\\\\\n\\tilde{x}^{k+1},   \\text{否则,}\n\\end{cases}\n$$\n并且，当发生回退时（动量重置），被接受的外推状态 $y^{k+1}$ 取为 $x^{k+1}$，否则取为常规的 FISTA 外推。\n\n单调性证明。如果加速候选点被接受，根据接受规则，我们有 $F(x^{k+1}) \\le F(x^k)$。如果它被拒绝，那么通过将原理 2 应用于 $v = x^k$ 和 $\\tau \\le 1/L$，回退的 ISTA 步满足\n$$\nF(\\tilde{x}^{k+1}) \\le F(x^k).\n$$\n因此，在任一分支中，都有 $F(x^{k+1}) \\le F(x^k)$。根据归纳法，整个序列 $\\{F(x^k)\\}$ 是单调非增的。关键在于，该保障措施是局部的：它只使用在第 k 次迭代时可计算的 $F(x^k)$ 和 $F(\\hat{x}^{k+1})$，并且它在不需要任何全局知识的情况下保证了下降。\n\n$\\ell_1$ 范数的近端算子。为了实现更新，我们需要步长为 $\\tau$ 时 $g(x) = \\lambda\\|x\\|_1$ 的近端算子。根据可分性，对于 $v \\in \\mathbb{R}^n$，近端点 $u = \\operatorname{prox}_{\\tau g}(v)$ 是逐分量最优的：\n$$\nu_i \\in \\arg\\min_{t \\in \\mathbb{R}} \\left\\{\\lambda|t| + \\tfrac{1}{2\\tau}(t - v_i)^2\\right\\}.\n$$\n对于 $t \\ne 0$ 的次梯度最优性条件是\n$$\n0 \\in \\lambda \\,\\partial |t| + \\tfrac{1}{\\tau}(t - v_i) =\n\\lambda \\,\\mathrm{sign}(t) + \\tfrac{1}{\\tau}(t - v_i),\n$$\n这在 $|v_i| > \\tau \\lambda$ 的条件下得出 $t = v_i - \\tau \\lambda \\,\\mathrm{sign}(t)$。如果 $|v_i| \\le \\tau \\lambda$，通过检查次梯度包含关系，最小值在 $t=0$ 处取得。这导出了软阈值算子\n$$\n\\operatorname{prox}_{\\tau g}(v)_i = \\mathrm{sign}(v_i)\\,\\max\\{|v_i| - \\tau \\lambda, 0\\},\n$$\n对 $i=1,\\dots,n$ 逐分量应用。\n\nLipschitz 常数的计算。对于 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2$，我们有 $\\nabla f(x) = A^\\top (A x - b)$，其 Lipschitz 常数是 Hessian 代理 $A^\\top A$ 的算子范数：\n$$\nL = \\|A^\\top A\\|_2 = \\sigma_{\\max}(A)^2,\n$$\n其中 $\\sigma_{\\max}(A)$ 表示 $A$ 的最大奇异值。因此，我们选择步长 $\\tau = 1/L$。\n\n数值实验设计和预期行为。我们在四个测试用例中比较 ISTA、FISTA 和受保障的单调 FISTA (MFISTA)，这些用例探究了病态、良态、强正则化（趋向于稀疏或零）以及 $\\lambda=0$ 的光滑情况。根据下降引理的论证，ISTA 必须产生一个单调的目标函数序列。由于外推过冲，FISTA 可能表现出瞬时的非单调性，尤其是在病态问题或早期迭代中。MFISTA 通过局部接受测试和在需要时回退到 ISTA 来强制实现单调性，因此其目标函数序列保证是单调的。在实践中，MFISTA 通常将 FISTA 的更快收敛速度与 ISTA 的鲁棒性相结合，其最终目标函数值不比 ISTA 差，并且通常与 FISTA 一样好或更好。\n\n实现细节。程序根据指定的种子和过程构造矩阵 $A$ 和向量 $b$，通过 $A$ 的最大奇异值计算 $L$，以 $\\tau=1/L$ 对每种算法运行 $K=200$ 次迭代，记录迭代过程中的目标函数值，通过验证 $F(x^{k+1}) \\le F(x^k)$ （在小的数值容差内）来检查单调性，并以要求的单行格式打印结果，汇总每个测试用例的布尔值和最终目标函数值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef soft_threshold(v, alpha):\n    # Proximal operator of alpha * ||.||_1\n    return np.sign(v) * np.maximum(np.abs(v) - alpha, 0.0)\n\ndef objective(A, b, x, lam):\n    r = A @ x - b\n    return 0.5 * float(r.T @ r) + lam * float(np.linalg.norm(x, 1))\n\ndef lipschitz_constant(A):\n    # L = ||A^T A||_2 = sigma_max(A)^2\n    # Use SVD to get largest singular value\n    s = np.linalg.svd(A, compute_uv=False)\n    sigma_max = np.max(s) if s.size > 0 else 0\n    return sigma_max * sigma_max\n\ndef ista(A, b, lam, K):\n    L = lipschitz_constant(A)\n    tau = 1.0 / L if L > 0 else 1.0\n    x = np.zeros(A.shape[1])\n    objs = []\n    for _ in range(K):\n        grad = A.T @ (A @ x - b)\n        x = soft_threshold(x - tau * grad, tau * lam)\n        objs.append(objective(A, b, x, lam))\n    return x, np.array(objs)\n\ndef fista(A, b, lam, K):\n    L = lipschitz_constant(A)\n    tau = 1.0 / L if L > 0 else 1.0\n    x = np.zeros(A.shape[1])\n    y = x.copy()\n    t = 1.0\n    objs = []\n    for _ in range(K):\n        grad = A.T @ (A @ y - b)\n        x_new = soft_threshold(y - tau * grad, tau * lam)\n        t_new = 0.5 * (1.0 + np.sqrt(1.0 + 4.0 * t * t))\n        y = x_new + ((t - 1.0) / t_new) * (x_new - x)\n        x = x_new\n        t = t_new\n        objs.append(objective(A, b, x, lam))\n    return x, np.array(objs)\n\ndef mfista(A, b, lam, K):\n    L = lipschitz_constant(A)\n    tau = 1.0 / L if L > 0 else 1.0\n    x = np.zeros(A.shape[1])\n    y = x.copy()\n    t = 1.0\n    objs = []\n    f_prev = objective(A, b, x, lam)\n    objs.append(f_prev)\n    # record initial objective at x^0 for comparison in checks\n    for _ in range(K-1):\n        # Accelerated candidate from y\n        grad_y = A.T @ (A @ y - b)\n        x_cand = soft_threshold(y - tau * grad_y, tau * lam)\n        f_cand = objective(A, b, x_cand, lam)\n        if f_cand = f_prev + 1e-12:\n            # accept accelerated step\n            x_new = x_cand\n            f_new = f_cand\n            t_new = 0.5 * (1.0 + np.sqrt(1.0 + 4.0 * t * t))\n            y_new = x_new + ((t - 1.0) / t_new) * (x_new - x)\n            t = t_new\n            y = y_new\n            x = x_new\n            f_prev = f_new\n        else:\n            # fallback to ISTA step from x, reset momentum\n            grad_x = A.T @ (A @ x - b)\n            x_new = soft_threshold(x - tau * grad_x, tau * lam)\n            f_new = objective(A, b, x_new, lam)\n            # Guarantee f_new = f_prev by proximal gradient descent\n            x = x_new\n            y = x_new.copy()\n            t = 1.0\n            f_prev = f_new\n        objs.append(f_prev)\n    return x, np.array(objs)\n\ndef is_monotone_nonincreasing(arr, tol=1e-12):\n    return bool(np.all(np.diff(arr) = tol))\n\ndef make_case_ill_conditioned(seed, m, n, k_sparsity, sigma_noise, lam):\n    rng = np.random.default_rng(seed)\n    G = rng.standard_normal((m, n))\n    # compact SVD\n    U, s_vals_orig, Vt = np.linalg.svd(G, full_matrices=False)\n    r = min(m, n)\n    svals = np.linspace(1.0, 1e-3, r)  # linearly spaced singular values\n    # The Vt from numpy is already V.T\n    A = (U[:, :r] * svals) @ Vt[:r, :]\n    # sparse ground truth\n    x_true = np.zeros(n)\n    idx = rng.choice(n, size=k_sparsity, replace=False)\n    x_true[idx] = rng.standard_normal(k_sparsity)\n    noise = sigma_noise * rng.standard_normal(m)\n    b = A @ x_true + noise\n    return A, b, lam\n\ndef make_case_well_conditioned(seed, m, n, k_sparsity, sigma_noise, lam):\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((m, n))\n    # scale columns to unit norm\n    col_norms = np.linalg.norm(A, axis=0)\n    col_norms[col_norms == 0.0] = 1.0\n    A = A / col_norms\n    x_true = np.zeros(n)\n    idx = rng.choice(n, size=k_sparsity, replace=False)\n    x_true[idx] = rng.standard_normal(k_sparsity)\n    noise = sigma_noise * rng.standard_normal(m) if sigma_noise > 0 else 0\n    b = A @ x_true + noise\n    return A, b, lam\n\ndef solve():\n    K = 200\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"ill\", dict(seed=7, m=50, n=100, k_sparsity=10, sigma_noise=1e-2, lam=1e-2)),\n        (\"well\", dict(seed=11, m=80, n=100, k_sparsity=10, sigma_noise=1e-2, lam=1e-2)),\n        (\"well\", dict(seed=13, m=80, n=100, k_sparsity=10, sigma_noise=1e-2, lam=1.0)),\n        (\"well\", dict(seed=17, m=80, n=100, k_sparsity=10, sigma_noise=1e-2, lam=0.0)),\n    ]\n\n    results = []\n    for case_type, params in test_cases:\n        if case_type == \"ill\":\n            A, b, lam = make_case_ill_conditioned(**params)\n        else:\n            A, b, lam = make_case_well_conditioned(**params)\n\n        # Run ISTA\n        x_i, objs_i = ista(A, b, lam, K)\n        # Run FISTA\n        x_f, objs_f = fista(A, b, lam, K)\n        # Run MFISTA\n        x_m, objs_m = mfista(A, b, lam, K)\n\n        mon_fista = is_monotone_nonincreasing(objs_f)\n        mon_mfista = is_monotone_nonincreasing(objs_m)\n        final_ista = float(objs_i[-1])\n        final_fista = float(objs_f[-1])\n        final_mfista = float(objs_m[-1])\n\n        results.append([mon_fista, mon_mfista, final_ista, final_fista, final_mfista])\n\n    # Final print statement in the exact required format.\n    print(f\"{results}\")\n\nsolve()\n```", "id": "3392933"}]}