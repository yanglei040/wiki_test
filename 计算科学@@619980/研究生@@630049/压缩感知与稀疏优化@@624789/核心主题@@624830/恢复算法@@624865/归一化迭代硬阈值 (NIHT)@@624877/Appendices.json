{"hands_on_practices": [{"introduction": "要理解归一化迭代硬阈值（NIHT）算法，我们必须首先掌握其核心机制：自适应步长的计算。本练习将通过一个具体的小规模问题，引导你分步完成单次 NIHT 迭代。通过亲手计算梯度、确定新支撑集并推导最优步长，你将对算法如何适应目标函数局部几何形状获得切实的理解。[@problem_id:3438872]", "problem": "考虑压缩感知中的最小二乘目标函数 $f(x) = \\tfrac{1}{2}\\|y - A x\\|_{2}^{2}$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$x \\in \\mathbb{R}^{n}$。迭代硬阈值（Iterative Hard Thresholding, IHT）方法通过交替执行梯度步和硬阈值算子 $H_{k}$ 来寻求一个 $k$-稀疏估计。硬阈值算子 $H_{k}$ 保留（绝对值）最大的 $k$ 个元素，并将其余元素置零。归一化迭代硬阈值（Normalized Iterative Hard Thresholding, NIHT）变体在每次迭代中，通过沿着一个由硬阈值化候选支撑集决定的、支撑集受限的梯度方向最小化目标函数，来选择一个数据自适应步长。从最小二乘梯度和硬阈值化的基本定义，以及沿搜索方向进行线搜索最小化的原理出发，按如下步骤确定一次 NIHT 迭代的自适应步长。\n\n- 设 $A \\in \\mathbb{R}^{3 \\times 5}$，$k = 2$，\n$$\nA = \\begin{pmatrix}\n1  0  0  1  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  1\n\\end{pmatrix}, \\quad\nx_{t} = \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n$$\n- 定义残差 $r_{t} = y - A x_{t}$ 和梯度 $g_{t} = \\nabla f(x_{t}) = -A^{\\top} r_{t}$。使用最速下降方向 $-g_{t}$ 及其支撑集受限版本，构造方法如下：首先构造代理向量 $x_{t} - g_{t}$，通过 $H_{k}$ 确定其 $k$-稀疏支撑集，然后将下降方向限制在该支撑集上（将支撑集外的元素置零）。\n- 沿着上面得到的支撑集受限下降方向 $d_{t}$，通过在 $\\mu \\in \\mathbb{R}$ 上最小化 $f(x_{t} + \\mu d_{t})$ 来选择步长 $\\mu_{t}$，然后执行一次 NIHT 更新 $x_{t+1} = H_{k}(x_{t} + \\mu_{t}(-g_{t}))$。\n\n计算此过程所决定的自适应步长 $\\mu_{t}$ 的值。然后执行相应的单次 NIHT 更新。最后，简要评论与使用完整（不受限）梯度方向相比，在步长估计中将方向限制在硬阈值支撑集上如何影响 $\\mu_{t}$ 的大小。\n\n最终答案只报告 $\\mu_{t}$ 的值。将报告的 $\\mu_{t}$ 值四舍五入至四位有效数字。", "solution": "该问题是适定的，在稀疏优化领域有科学依据，并为获得唯一解提供了所有必要信息。因此，该问题是有效的。我们按照指定的步骤进行求解。\n\n目标函数是最小二乘损失：\n$$\nf(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2}\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$x \\in \\mathbb{R}^{n}$。该函数关于 $x$ 的梯度是：\n$$\n\\nabla f(x) = A^{\\top}(A x - y) = -A^{\\top}(y - A x)\n$$\n\n问题提供了以下数据：\n$$\nA = \\begin{pmatrix}\n1  0  0  1  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  1\n\\end{pmatrix}, \\quad\nx_{t} = \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad k=2.\n$$\n\n首先，我们计算当前迭代点 $x_t$ 处的残差 $r_t$。\n$$\nA x_t = \\begin{pmatrix}\n1  0  0  1  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  1\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n= \\begin{pmatrix} 0 \\cdot 0 + 0 \\cdot 0.5 + \\dots \\\\ 0 \\cdot 0 + 1 \\cdot 0.5 + \\dots \\\\ 0 \\cdot 0 + 0 \\cdot 0.5 + \\dots \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\end{pmatrix}\n$$\n$$\nr_t = y - A x_t = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 1 \\end{pmatrix}\n$$\n\n接下来，我们计算梯度 $g_t = \\nabla f(x_t) = -A^{\\top} r_t$。\n$$\nA^{\\top} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  1  1 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n$$\ng_t = - \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  1  1 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 1.5 \\\\ 1 \\end{pmatrix}\n= - \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 1.5+1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix} -1 \\\\ -1.5 \\\\ -2.5 \\\\ -1 \\\\ -1 \\end{pmatrix}\n$$\n最速下降方向为 $-g_t = \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix}$。\n\n根据流程，我们通过 $z_t = x_t - g_t = x_t + (-g_t)$ 构造一个代理向量。\n$$\nz_t = \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n我们应用硬阈值算子 $H_k$（其中 $k=2$）来找到支撑集 $\\Omega_t$。该算子识别 $z_t$ 中 $k=2$ 个绝对值最大元素的索引。$z_t$ 的元素为 $1, 2, 2.5, 1, 1$。两个最大的元素是 $2.5$（在索引 3 处）和 $2$（在索引 2 处）。因此，支撑集为 $\\Omega_t = \\{2, 3\\}$。\n\n搜索方向 $d_t$ 是最速下降方向 $-g_t$ 限制在支撑集 $\\Omega_t$ 上的结果。\n$$\nd_t = \\begin{pmatrix} 0 \\\\ (-g_t)_2 \\\\ (-g_t)_3 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1.5 \\\\ 2.5 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\n自适应步长 $\\mu_t$ 是通过最小化关于 $\\mu$ 的函数 $f(x_t + \\mu d_t)$ 来选择的。令 $h(\\mu) = f(x_t + \\mu d_t)$。\n$$\nh(\\mu) = \\frac{1}{2} \\|y - A(x_t + \\mu d_t)\\|_2^2 = \\frac{1}{2} \\|(y - A x_t) - \\mu A d_t\\|_2^2 = \\frac{1}{2} \\|r_t - \\mu A d_t\\|_2^2\n$$\n这是一个关于 $\\mu$ 的二次函数。为求最小值，我们将其导数设为零：\n$$\n\\frac{dh}{d\\mu} = \\frac{d}{d\\mu} \\left( \\frac{1}{2} (r_t - \\mu A d_t)^\\top (r_t - \\mu A d_t) \\right) = -r_t^\\top A d_t + \\mu (A d_t)^\\top (A d_t) = 0\n$$\n解出 $\\mu_t$：\n$$\n\\mu_t = \\frac{r_t^\\top A d_t}{\\|A d_t\\|_2^2}\n$$\n我们可以简化分子：因为 $g_t = -A^\\top r_t$，所以有 $-g_t = A^\\top r_t$，因此 $r_t^\\top A = (-g_t)^\\top$。所以，$r_t^\\top A d_t = (-g_t)^\\top d_t$。由于 $d_t$ 是 $-g_t$ 在支撑集 $\\Omega_t$ 上的投影，这个内积就变成了 $d_t$ 的二次范数的平方：$(-g_t)^\\top d_t = \\|d_t\\|_2^2$。所以，公式为 $\\mu_t = \\frac{\\|d_t\\|_2^2}{\\|A d_t\\|_2^2}$。\n\n现在我们计算分子和分母。\n分子：\n$$\n\\|d_t\\|_2^2 = 0^2 + (1.5)^2 + (2.5)^2 + 0^2 + 0^2 = 2.25 + 6.25 = 8.5\n$$\n分母：\n$$\nA d_t = \\begin{pmatrix} 1  0  0  1  0 \\\\ 0  1  1  0  0 \\\\ 0  0  1  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1.5 \\\\ 2.5 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1.5+2.5 \\\\ 2.5 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 4 \\\\ 2.5 \\end{pmatrix}\n$$\n$$\n\\|A d_t\\|_2^2 = 0^2 + 4^2 + (2.5)^2 = 16 + 6.25 = 22.25\n$$\n步长是：\n$$\n\\mu_t = \\frac{8.5}{22.25} = \\frac{850}{2225} = \\frac{34}{89} \\approx 0.38202247...\n$$\n四舍五入到四位有效数字，我们得到 $\\mu_t = 0.3820$。\n\n更新后的估计为 $x_{t+1} = H_k(x_t + \\mu_t(-g_t))$。\n$$\nx_t + \\mu_t(-g_t) \\approx \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + 0.3820 \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.3820 \\\\ 0.5 + 0.573 \\\\ 0.955 \\\\ 0.3820 \\\\ 0.3820 \\end{pmatrix} = \\begin{pmatrix} 0.3820 \\\\ 1.073 \\\\ 0.955 \\\\ 0.3820 \\\\ 0.3820 \\end{pmatrix}\n$$\n应用 $H_2$ 得到 $x_{t+1} \\approx \\begin{pmatrix} 0 \\\\ 1.073 \\\\ 0.955 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n最后，我们评论一下 $\\mu_t$ 的大小，与使用完整（不受限）梯度方向 $-g_t$ 所得到的步长 $\\mu'_t$ 进行比较。\n完整方向的步长为 $\\mu'_t = \\frac{\\|-g_t\\|_2^2}{\\|A(-g_t)\\|_2^2}$。\n$$\n\\|-g_t\\|_2^2 = 1^2 + (1.5)^2 + (2.5)^2 + 1^2 + 1^2 = 1 + 2.25 + 6.25 + 1 + 1 = 11.5\n$$\n$$\nA(-g_t) = \\begin{pmatrix} 1  0  0  1  0 \\\\ 0  1  1  0  0 \\\\ 0  0  1  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+1 \\\\ 1.5+2.5 \\\\ 2.5+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 3.5 \\end{pmatrix}\n$$\n$$\n\\|A(-g_t)\\|_2^2 = 2^2 + 4^2 + (3.5)^2 = 4 + 16 + 12.25 = 32.25\n$$\n$$\n\\mu'_t = \\frac{11.5}{32.25} \\approx 0.3566\n$$\n在本例中，NIHT 步长 $\\mu_t \\approx 0.3820$ 大于最速下降步长 $\\mu'_t \\approx 0.3566$。将搜索方向从 $-g_t$ 限制到 $d_t$ 涉及将一些分量置零，这会减小步长公式中分子（$\\|d_t\\|_2^2  \\|-g_t\\|_2^2$）和分母（$\\|A d_t\\|_2^2  \\|A(-g_t)\\|_2^2$）的大小。步长的最终值取决于这两项的相对减少量。通过将问题限制在由支撑集 $\\Omega_t$ 定义的子空间上，由瑞利商 $\\|A d\\|_2^2 / \\|d\\|_2^2$ 衡量的目标函数沿搜索方向的表观曲率会发生变化。在这个具体例子中，分母 $\\|A d_t\\|_2^2$ 的下降比例大于分子 $\\|d_t\\|_2^2$ 的下降比例，导致它们的比值增加，即 $\\mu_t > \\mu'_t$。", "answer": "$$\n\\boxed{0.3820}\n$$", "id": "3438872"}, {"introduction": "像 NIHT 这样的迭代算法的收敛性并非总是能够保证，它关键性地取决于传感矩阵 $A$ 的性质。本练习将探究一个违反了约束等距性质（Restricted Isometry Property, RIP）并导致算法不收敛的情形。通过分析一个简化为线性映射的特例，你将计算迭代矩阵的谱半径，并清晰地看到不良的矩阵条件如何阻止算法收敛，从而在抽象的理论条件与具体的算法行为之间建立明确的联系。[@problem_id:3463046]", "problem": "考虑将归一化迭代硬阈值（NIHT）方法应用于线性逆问题，其中测量值为 $y \\in \\mathbb{R}^{m}$，感知矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，且对于一个 $k$-稀疏向量 $x^{\\star} \\in \\mathbb{R}^{n}$，有 $y = A x^{\\star}$。NIHT 迭代定义为\n$$\nx_{t+1} \\;=\\; H_{k}\\!\\left(x_{t} \\;+\\; \\mu_{t}\\, A^{\\top}(y - A x_{t})\\right),\n$$\n其中 $H_{k}(\\cdot)$ 是硬阈值算子，它保留 $k$ 个最大幅值的元素并将其余元素置零，而 $\\mu_{t}$ 被选为限制在活动支撑集上的子矩阵的谱范数平方的倒数，即\n$$\n\\mu_{t} \\;=\\; \\frac{1}{\\|A_{S_{t}}\\|_{2}^{2}},\n$$\n其中 $S_{t}$ 表示在第 $t$ 次迭代中由 $H_{k}(\\cdot)$ 保留的支撑集。阶数为 $s$、常数为 $\\delta_{s} \\in [0,1)$ 的限制等距性质（RIP）要求对于所有 $s$-稀疏向量 $x$，都满足\n$$\n(1 - \\delta_{s})\\,\\|x\\|_{2}^{2} \\;\\le\\; \\|A x\\|_{2}^{2} \\;\\le\\; (1 + \\delta_{s})\\,\\|x\\|_{2}^{2}.\n$$\n\n请构造一个显式例子，该例子违反 RIP，并通过 $A_{S_{t}}^{\\top} A_{S_{t}}$ 的病态条件导致 NIHT 不收敛，具体如下。设 $m = 1$, $n = 2$, $k = 2$, 且\n$$\nA \\;=\\; \\begin{pmatrix} 1  1 \\end{pmatrix}, \\qquad y \\;=\\; 0.\n$$\n因为 $k = 2$，硬阈值算子 $H_{2}(\\cdot)$ 是 $\\mathbb{R}^{2}$ 上的恒等算子，所以对于所有 $t$，$S_{t} = \\{1,2\\}$，且 $\\mu_{t}$ 是一个固定常数，等于 $1/\\|A\\|_{2}^{2}$。在此设置下，NIHT 迭代简化为一个线性映射\n$$\nx_{t+1} \\;=\\; \\Big(I - \\mu\\, A^{\\top} A\\Big)\\, x_{t},\n$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵，且 $\\mu = 1/\\|A\\|_{2}^{2}$。$A_{S_{t}}^{\\top} A_{S_{t}}$ 的病态条件预计会产生非收缩动态，可能表现为循环或发散。\n\n对于上述的 $A$、$y$ 和 $k$，以及 $\\mu = 1/\\|A\\|_{2}^{2}$，计算迭代矩阵\n$$\nM \\;=\\; I - \\mu\\, A^{\\top} A\n$$\n的谱半径。你的最终答案必须是一个实数。无需四舍五入。", "solution": "问题要求计算在归一化迭代硬阈值（NIHT）算法的一个特定实例中，迭代矩阵 $M = I - \\mu A^{\\top} A$ 的谱半径。该问题经确认是自洽的、数学上合理的且适定的。对于给定的参数，关于违反限制等距性质（RIP）的先验断言是正确的，但这一验证只是上下文细节，并非主要计算所必需。我们开始进行计算。\n\n给定的参数如下：\n- 感知矩阵：$A = \\begin{pmatrix} 1  1 \\end{pmatrix} \\in \\mathbb{R}^{1 \\times 2}$。\n- 稀疏度：$k=2$。\n- 向量维度：$n=2$。\n- 测量数量：$m=1$。\n- 测量值：$y=0$。\n\n由于稀疏度 $k=2$ 等于信号维度 $n=2$，硬阈值算子 $H_{k}(\\cdot)$ 成为 $\\mathbb{R}^{2}$ 上的恒等算子，即对于任意 $v \\in \\mathbb{R}^{2}$ 都有 $H_{2}(v) = v$。因此，支撑集 $S_{t}$ 始终是全集 $\\{1, 2\\}$，并且对于所有迭代 $t$，受限矩阵 $A_{S_{t}}$ 就是 $A$。\n\n步长 $\\mu_{t}$ 由下式给出\n$$\n\\mu_{t} = \\mu = \\frac{1}{\\|A_{S_{t}}\\|_{2}^{2}} = \\frac{1}{\\|A\\|_{2}^{2}}.\n$$\n谱范数 $\\|A\\|_{2}$ 是 $A$ 的最大奇异值。$A$ 的奇异值是矩阵 $A^{\\top} A$ 的特征值的平方根。\n\n首先，我们计算 $A^{\\top} A$：\n$$\nA^{\\top} A = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}.\n$$\n\n接下来，我们求 $A^{\\top} A$ 的特征值。特征方程为 $\\det(A^{\\top} A - \\lambda I) = 0$。\n$$\n\\det\\left(\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\right) = \\det\\begin{pmatrix} 1 - \\lambda  1 \\\\ 1  1 - \\lambda \\end{pmatrix} = 0.\n$$\n这得到 $(1 - \\lambda)^{2} - 1 = 0$，简化为 $\\lambda^{2} - 2\\lambda + 1 - 1 = 0$，或 $\\lambda(\\lambda - 2) = 0$。$A^{\\top} A$ 的特征值为 $\\lambda_{1} = 2$ 和 $\\lambda_{2} = 0$。\n\n$A$ 的奇异值为 $\\sigma_{1} = \\sqrt{\\lambda_{1}} = \\sqrt{2}$ 和 $\\sigma_{2} = \\sqrt{\\lambda_{2}} = 0$。$A$ 的谱范数是最大奇异值，所以 $\\|A\\|_{2} = \\sqrt{2}$。\n\n现在我们可以计算恒定步长 $\\mu$：\n$$\n\\mu = \\frac{1}{\\|A\\|_{2}^{2}} = \\frac{1}{(\\sqrt{2})^{2}} = \\frac{1}{2}.\n$$\n\nNIHT 迭代简化为线性映射 $x_{t+1} = M x_{t}$，其中迭代矩阵 $M$ 由下式给出：\n$$\nM = I - \\mu A^{\\top} A = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}.\n$$\n进行矩阵运算：\n$$\nM = \\begin{pmatrix} 1 - \\frac{1}{2}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  1 - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}.\n$$\n\n为了求 $M$ 的谱半径，记作 $\\rho(M)$，我们必须求出它的特征值。设 $M$ 的特征值为 $\\lambda_{M}$。特征方程为 $\\det(M - \\lambda_{M} I) = 0$:\n$$\n\\det\\begin{pmatrix} \\frac{1}{2} - \\lambda_{M}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{1}{2} - \\lambda_{M} \\end{pmatrix} = 0.\n$$\n这得到 $(\\frac{1}{2} - \\lambda_{M})^{2} - (-\\frac{1}{2})^{2} = 0$，简化为 $\\frac{1}{4} - \\lambda_{M} + \\lambda_{M}^{2} - \\frac{1}{4} = 0$，或 $\\lambda_{M}^{2} - \\lambda_{M} = 0$。因式分解后，我们得到 $\\lambda_{M}(\\lambda_{M} - 1) = 0$。$M$ 的特征值为 $\\lambda_{M,1} = 1$ 和 $\\lambda_{M,2} = 0$。\n\n谱半径是特征值绝对值的最大值：\n$$\n\\rho(M) = \\max(|\\lambda_{M,1}|, |\\lambda_{M,2}|) = \\max(|1|, |0|) = 1.\n$$\n谱半径为 $1$ 表明该线性迭代不是一个收缩映射，这与问题关于不收敛的设定是一致的。", "answer": "$$\\boxed{1}$$", "id": "3463046"}, {"introduction": "从理论走向实践，硬阈值算法面临的一个主要挑战是容易陷入对应于错误稀疏模式的局部最小值。这个编程挑战将让你亲手实现并比较迭代硬阈值（IHT）、NIHT 以及带有回溯搜索的稳健变体。你将构建一个包含“混淆”列的困难恢复问题，并亲眼观察简单的 IHT 如何失败，NIHT 的大步长如何助其跳出陷阱，以及回溯搜索如何在最困难的情况下增加必要的稳定性以取得成功。这项练习将巩固这些算法之间的概念差异，并突显自适应步长和线搜索的实用价值。[@problem_id:3463072]", "problem": "你的任务是构建并分析一个压缩感知中的稀疏恢复实验，该实验具体展示了不良的初始化如何导致算法陷入错误的支撑集吸引子，以及两种设计选择——梯度步长的归一化和回溯法——如何帮助逃离该吸引子。恢复任务被建模为在稀疏性约束下最小化二次损失：\n- 给定一个感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$、一个目标稀疏信号 $x^\\star \\in \\mathbb{R}^n$（满足 $\\lVert x^\\star \\rVert_0 \\leq k$），以及无噪声测量值 $y = A x^\\star$ 或带噪声测量值 $y = A x^\\star + w$，考虑目标函数 $f(x) = \\tfrac{1}{2} \\lVert y - A x \\rVert_2^2$，其中 $x \\in \\mathbb{R}^n$ 受 $\\lVert x \\rVert_0 \\leq k$ 约束。\n\n从以下核心基础开始：\n- 二次损失的梯度是 $\\nabla f(x) = - A^\\top (y - A x)$。\n- $\\nabla f$ 的 Lipschitz 常数是 $L = \\lVert A \\rVert_2^2$，即 $A$ 的谱范数的平方。\n- 硬阈值算子 $H_k(\\cdot)$ 将 $\\mathbb{R}^n$ 中的向量投影到集合 $\\{x : \\lVert x \\rVert_0 \\leq k\\}$ 上，方法是保留幅值最大的 $k$ 个元素，并将其余元素置零。\n\n你必须实现并比较三种算法：\n- 固定步长迭代硬阈值（IHT）：$x_{t+1} = H_k\\!\\big(x_t + \\mu A^\\top (y - A x_t)\\big)$，其中步长 $\\mu$ 为常数。\n- 归一化迭代硬阈值（NIHT）：$x_{t+1} = H_k\\!\\big(x_t + \\mu_t A^\\top (y - A x_t)\\big)$，其中归一化步长 $\\mu_t$ 在每次迭代时选择为沿梯度方向的最速下降线搜索最小值点，即在 $\\mu \\in \\mathbb{R}$ 上最小化 $f(x_t + \\mu A^\\top (y - A x_t))$ 的唯一 $\\mu_t$。\n- 带回溯的 NIHT（NIHT-BT）：将 NIHT 步长与回溯线搜索相结合，该搜索会乘性地减小步长，直到在 $H_k$ 投影后观察到 $f$ 的充分下降。\n\n设计一个实验，通过不良初始化来诱导一个错误的支撑集吸引子，具体如下：\n- 构建一个感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其中 $m = 32$，$n = 64$，通过抽取独立的标准正态分布元素并对列进行单位 $\\ell_2$ 范数归一化。然后，通过创建高度相关的列对来强制产生强烈的列相关性，以制造“支撑集混淆项”：将某些列替换为其他列的近似缩放副本（遵循测试套件中指定的精确方法）。这使得多个 $k$-稀疏支撑集都能对 $y$ 产生同样好的拟合。\n- 选择一个真实（ground-truth）的 $k$-稀疏信号 $x^\\star$，$k=2$，其支撑集位于 $A$ 中其他地方有近似副本的一对列上。\n- 通过在一个使用了副本列的“错误”$k$-稀疏支撑集上求解最小二乘拟合来初始化 $x_0$，即计算 $x_0$ 使得 $\\operatorname{supp}(x_0) = S_{\\text{wrong}}$ 且 $x_0[S_{\\text{wrong}}] = \\arg\\min_{z \\in \\mathbb{R}^k} \\lVert y - A_{S_{\\text{wrong}}} z \\rVert_2$，而其他元素为零。对于步长 $\\mu$ 足够小的固定步长 IHT 算法，这种初始化会产生一个错误的支撑集吸引子，因为 $S_{\\text{wrong}}$ 内部的梯度会消失，而 $S_{\\text{wrong}}$ 外部的缩放后梯度元素仍然太小，无法通过阈值化操作。\n\n程序的必需行为：\n- 按照上述描述实现三种方法（IHT、NIHT、NIHT-BT），使用 $f(x) = \\tfrac{1}{2} \\lVert y - A x \\rVert_2^2$ 作为目标函数，使用 $H_k$ 作为硬阈值算子。\n- 在迭代 $t$ 中实现 NIHT 归一化，作为沿梯度方向 $g_t = A^\\top (y - A x_t)$ 的精确最速下降步长，即选择在 $\\mu \\in \\mathbb{R}$ 上最小化 $\\phi(\\mu) = f(x_t + \\mu g_t)$ 的 $\\mu_t$。\n- 实现带回溯的线搜索，通过一个因子 $\\beta \\in (0,1)$ 进行乘性缩减，直到对暂定更新应用 $H_k$ 后 $f$ 获得充分下降；使用一个标准的 Armijo 型条件和一个小常数 $c \\in (0,1)$。\n\n测试套件：\n- 使用以下测试用例。在所有情况下，为确保可复现性，随机数生成器必须在开始时用种子 $123$ 初始化，并且 $A$ 的所有列都必须缩放到单位 $\\ell_2$ 范数。\n    1. 用例 $1$（固定步长 IHT 的错误支撑集吸引子）：\n        - 维度：$m = 32$，$n = 64$，稀疏度 $k = 2$。\n        - 矩阵 $A$：从独立的标准正态分布元素开始，将列归一化为单位范数。然后，通过将第 $10$ 列设置为第 $0$ 列被一个幅度为 $0.02$ 的小正交分量扰动后的重新归一化版本，以及将第 $11$ 列设置为第 $1$ 列被一个幅度为 $0.02$ 的小正交分量扰动后的重新归一化版本，来强制产生两对中等程度的“混淆项”对。\n        - 真实信号：支撑集 $S^\\star = \\{0, 1\\}$，其中 $x^\\star[0] = 1.0$，$x^\\star[1] = 0.8$，所有其他元素为零。测量值 $y = A x^\\star$。\n        - 初始化：$x_0$ 是在 $S_{\\text{wrong}} = \\{10, 11\\}$ 上的最小二乘解。\n        - 算法：固定步长 IHT，步长 $\\mu = 0.2 / L$，其中 $L = \\lVert A \\rVert_2^2$，运行 $500$ 次迭代。\n        - 此用例的输出：一个布尔值，表示恢复的支撑集是否与 $S^\\star$ 完全相等。\n    2. 用例 $2$（归一化有助于逃离）：\n        - 与用例 1 中相同的 $A$、$x^\\star$、$y$ 和 $x_0$。\n        - 算法：NIHT，每次迭代进行最速下降归一化，运行 $200$ 次迭代，无回溯。\n        - 输出：精确支撑集恢复的布尔值。\n    3. 用例 $3$（更难的混淆项，无回溯）：\n        - 与上面相同的 $m$、$n$、$k$ 和 $A$ 的基础归一化。除了用例 1 中的修改外，通过将第 $20$ 列设置为第 $0$ 列被一个幅度为 $0.001$ 的扰动后的重新归一化版本，以及将第 $21$ 列设置为第 $1$ 列被一个幅度为 $0.001$ 的扰动后的重新归一化版本，来强制产生第二对更强的混淆项。\n        - 真实信号：与之前相同的 $S^\\star$ 和 $x^\\star$。\n        - 测量值：$y = A x^\\star + w$，其中 $w$ 是高斯噪声，其各元素独立且标准差为 $\\sigma = 0.002$。\n        - 初始化：$x_0$ 是在 $S_{\\text{wrong}} = \\{20, 21\\}$ 上的最小二乘解。\n        - 算法：NIHT，无回溯，运行 $50$ 次迭代。\n        - 输出：精确支撑集恢复的布尔值。\n    4. 用例 $4$（在更难的情况下回溯法有帮助）：\n        - 与用例 3 设置相同。\n        - 算法：带回溯的 NIHT，使用 Armijo 参数 $c = 10^{-4}$ 和缩减因子 $\\beta = 0.5$，运行 $500$ 次迭代。\n        - 输出：精确支撑集恢复的布尔值。\n\n实现细节：\n- 硬阈值算子 $H_k$ 必须精确保留绝对值最大的 $k$ 个元素，并将其余元素置零。在出现相等情况时，通过索引的自然顺序来确定性地打破平局。\n- 对所有 $\\ell_2$ 范数使用欧几里得范数。\n- 每个测试用例的终止标准是指定的固定迭代预算；你不需要实现任何自适应停止准则。\n- 为评估精确的支撑集恢复，将返回的估计信号的非零元素的索引集合与 $S^\\star$ 进行比较；如果它们作为集合相等，则报告 true，否则报告 false。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含四个布尔结果，格式为逗号分隔的列表并用方括号括起（例如，$[\\text{True},\\text{False},\\text{True},\\text{True}]$），按顺序对应于用例 $1$ 到 $4$。不得打印任何其他文本。", "solution": "该问题要求实现并比较三种稀疏恢复算法——迭代硬阈值（IHT）、归一化 IHT（NIHT）和带回溯的 NIHT（NIHT-BT）——以展示一种特定的失败模式及其补救措施。核心任务是找到一个 $k$-稀疏信号 $x \\in \\mathbb{R}^n$，以最小化二次损失 $f(x) = \\tfrac{1}{2} \\lVert y - A x \\rVert_2^2$，其中 $y \\in \\mathbb{R}^m$ 是来自感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的测量值。\n\n这三种算法都基于投影梯度下降法。通用的更新步骤是 $x_{t+1} = P_{\\mathcal{C}_k}(x_t - \\mu_t \\nabla f(x_t))$，其中 $\\mathcal{C}_k = \\{x : \\lVert x \\rVert_0 \\leq k\\}$ 是 $k$-稀疏向量的非凸集，投影 $P_{\\mathcal{C}_k}$ 是硬阈值算子 $H_k(\\cdot)$。目标函数的梯度是 $\\nabla f(x) = A^\\top(Ax-y)$。代入后得到更新规则：\n$$x_{t+1} = H_k\\!\\big(x_t + \\mu_t A^\\top (y - A x_t)\\big)$$\n这些算法主要区别在于步长 $\\mu_t$ 的选择。\n\n**实验设计：错误的支撑集吸引子**\n该实验旨在创建一个场景，其中贪心算法很容易陷入与不正确支撑集对应的局部最小值。这是通过构建一个具有高度相关列的感知矩阵 $A$ 来实现的。\n具体来说，如果我们有两个列 $a_i$ 和 $a_j$，使得对于某个标量 $\\alpha$，有 $a_i \\approx \\alpha a_j$，那么它们就变得容易混淆。如果真实信号 $x^\\star$ 的支撑集在索引 $i$ 上，那么模型 $y = A x^\\star \\approx x^\\star_i a_i$ 可以被一个使用列 $j$ 的模型很好地近似，即对于某个 $z_j$ 有 $y \\approx z_j a_j$。\n通过在一个“错误”但高度相关的支撑集 $S_{\\text{wrong}}$ 上用最小二乘解来初始化算法，我们从一个点 $x_0$ 开始，在该点残差 $r_0 = y - Ax_0$ 与 $A_{S_{\\text{wrong}}}$ 中的列几乎正交。因此，梯度分量 $g_0[S_{\\text{wrong}}] = A_{S_{\\text{wrong}}}^\\top r_0$ 非常小。如果步长 $\\mu$ 很小，更新后的 $x_0 + \\mu g_0$ 的最大幅值分量将位于相同的支撑集 $S_{\\text{wrong}}$ 上，导致 $H_k$ 选择相同的错误支撑集。算法就陷入了困境。这就是“错误的支撑集吸引子”。\n\n**算法1：固定步长迭代硬阈值（IHT）**\nIHT 使用固定的步长 $\\mu$。一个保证梯度幅值收敛到零的标准选择要求 $\\mu  2/L$，其中 $L = \\lVert A \\rVert_2^2$ 是 $\\nabla f$ 的 Lipschitz 常数。问题指定了一个保守的步长 $\\mu = 0.2/L$。在用例 1 中，这个小步长不足以克服对错误支撑集 $S_{\\text{wrong}} = \\{10, 11\\}$ 的吸引，预计算法将无法恢复真实支撑集 $S^\\star = \\{0, 1\\}$。\n\n**算法2：归一化迭代硬阈值（NIHT）**\nNIHT 采用在每次迭代中计算的自适应步长 $\\mu_t$。它被选为在投影步骤*之前*，沿梯度方向最小化二次目标函数的最优步长。设 $g_t = A^\\top(y - Ax_t) = -\\nabla f(x_t)$ 为搜索方向。我们寻求最小化 $\\phi(\\mu) = f(x_t + \\mu g_t) = \\tfrac{1}{2} \\lVert (y - Ax_t) - \\mu A g_t \\rVert_2^2$。对 $\\mu$ 求导并令其为零，得到最优步长：\n$$\\mu_t = \\frac{g_t^\\top A^\\top(y - Ax_t)}{\\lVert A g_t \\rVert_2^2} = \\frac{g_t^\\top g_t}{\\lVert A g_t \\rVert_2^2} = \\frac{\\lVert g_t \\rVert_2^2}{\\lVert A g_t \\rVert_2^2}$$\n这个步长通常比 IHT 中的固定步长大得多，也更具侵略性。如用例 2 所示，这个更大的步长可以为迭代提供足够的“推动力”，将其从起点 $x_t$ 移得足够远，使得 $x_t + \\mu_t g_t$ 中的真实支撑集元素变得足够大，以被阈值算子 $H_k$ 选中，从而逃离吸引子。然而，如用例 3 所示，当列相关性极高且存在噪声时，这种侵略性的步长可能导致不稳定或过冲，从而导致失败。\n\n**算法3：带回溯的 NIHT（NIHT-BT）**\n为了增加鲁棒性，NIHT 可以通过回溯线搜索来增强。这确保了每一步都能使目标函数充分下降，防止了普通 NIHT 可能出现的过冲问题。在计算出侵略性的 NIHT 步长 $\\mu_t$ 后，我们测试一个候选更新。只有当更新满足充分下降条件时，才会被接受。问题指定了一个 Armijo 型条件，对于投影梯度法，这通常表示为：\n$$f(x_{t+1}) \\leq f(x_t) + c \\cdot \\nabla f(x_t)^\\top (x_{t+1} - x_t)$$\n其中 $x_{t+1} = H_k(x_t - \\mu \\nabla f(x_t))$ 且 $c \\in (0,1)$ 是一个控制参数（这里给定为 $c=10^{-4}$）。如果不满足该条件，步长 $\\mu$ 会被乘性减小（乘以一个因子 $\\beta=0.5$），然后重复测试。此过程从大的 NIHT 步长开始，仅在必要时才将其缩小，结合了 NIHT 的速度和保证下降方法的稳定性。在用例 4 中，这种稳定化使得算法能够成功地在具有非常高列相关性和噪声的困难恢复环境中导航，最终找到正确的支撑集。\n\n硬阈值算子 $H_k(v)$ 的实现是保留 $v$ 中绝对值最大的 $k$ 个元素。为确保确定性行为，幅值上的平局通过选择较小索引的元素来打破。\n\n这一系列实验旨在表明：\n1.  **用例 1：** 当在错误的支撑集吸引子中初始化时，使用保守步长的简单 IHT 会失败。\n2.  **用例 2：** NIHT 的侵略性归一化步长成功地逃离了该吸引子。\n3.  **用例 3：** 在一个具有更高相关性和噪声的更具挑战性的场景中，侵略性的 NIHT 步长变得不稳定并失败。\n4.  **用例 4：** 为 NIHT 添加回溯机制提供了必要的稳定性，以解决这个具有挑战性的问题。\n\n这个过程具体地展示了迭代稀疏恢复算法中步长选择策略之间的权衡。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hard_threshold(v, k):\n    \"\"\"\n    Performs hard thresholding on a vector v, keeping k elements.\n    Ties in magnitude are broken by the natural ordering of indices.\n    \"\"\"\n    if k == 0:\n        return np.zeros_like(v)\n    if k >= len(v):\n        return v.copy()\n    \n    # Use lexsort for deterministic tie-breaking by index.\n    # Sort keys are (-abs(v), index), so it sorts by descending magnitude,\n    # then ascending index for ties.\n    indices_to_keep = np.lexsort((np.arange(len(v)), -np.abs(v)))[:k]\n    \n    result = np.zeros_like(v)\n    result[indices_to_keep] = v[indices_to_keep]\n    return result\n\ndef create_A_with_confusers(A_base, confusers):\n    \"\"\"\n    Modifies a base matrix A to include confuser columns.\n    \"\"\"\n    A = A_base.copy()\n    n = A.shape[1]\n    for new_idx, ref_idx, pert_mag in confusers:\n        # Use a deterministic source for the orthogonal vector to ensure reproducibility.\n        orth_src_idx = (ref_idx + 5) % n \n        if orth_src_idx == new_idx: # Ensure it's not the column we are creating\n            orth_src_idx = (orth_src_idx + 1) % n\n\n        u = A[:, ref_idx]\n        v = A[:, orth_src_idx]\n        \n        # Gram-Schmidt to get a vector orthogonal to u\n        v_orth = v - (v.T @ u) * u\n        v_orth_unit = v_orth / np.linalg.norm(v_orth)\n        \n        # Create the new column by perturbing u and re-normalize to unit norm\n        new_col = u + pert_mag * v_orth_unit\n        A[:, new_idx] = new_col / np.linalg.norm(new_col)\n    return A\n\ndef compute_x0(A, y, S_wrong):\n    \"\"\"\n    Computes the initial guess x0 as the least-squares solution on the wrong support.\n    \"\"\"\n    S_wrong_list = sorted(list(S_wrong)) # Ensure consistent index order\n    A_wrong = A[:, S_wrong_list]\n    \n    # Solve least squares: z = argmin ||y - A_wrong * z||^2\n    z = np.linalg.lstsq(A_wrong, y, rcond=None)[0]\n    \n    x0 = np.zeros(A.shape[1])\n    x0[S_wrong_list] = z\n    return x0\n\ndef run_iht(A, y, x0, k, S_star, iterations, mu_factor):\n    \"\"\"\n    Case 1: Fixed-step Iterative Hard Thresholding.\n    \"\"\"\n    x = x0.copy()\n    L = np.linalg.norm(A, 2)**2\n    mu = mu_factor / L\n    \n    for _ in range(iterations):\n        grad = A.T @ (A @ x - y)\n        x = hard_threshold(x - mu * grad, k)\n        \n    recovered_support = set(np.where(x != 0)[0])\n    return recovered_support == S_star\n\ndef run_niht(A, y, x0, k, S_star, iterations):\n    \"\"\"\n    Case 2  3: Normalized Iterative Hard Thresholding.\n    \"\"\"\n    x = x0.copy()\n    \n    for _ in range(iterations):\n        r = y - A @ x\n        g = A.T @ r # This is -grad f(x), so we will add mu * g\n        \n        if np.linalg.norm(g)  1e-12:\n            break\n\n        Ag = A @ g\n        denom = Ag.T @ Ag\n        if denom  1e-20:\n            break\n        \n        mu = (g.T @ g) / denom\n        x = hard_threshold(x + mu * g, k)\n        \n    recovered_support = set(np.where(x != 0)[0])\n    return recovered_support == S_star\n\ndef run_niht_bt(A, y, x0, k, S_star, iterations, c, beta):\n    \"\"\"\n    Case 4: NIHT with Backtracking.\n    \"\"\"\n    x = x0.copy()\n    \n    for _ in range(iterations):\n        r = y - A @ x\n        fx = 0.5 * (r.T @ r)\n        g = A.T @ r # Corresponds to -grad f(x)\n        \n        if np.linalg.norm(g)  1e-12:\n            break\n            \n        Ag = A @ g\n        denom = Ag.T @ Ag\n        if denom  1e-20:\n            break\n            \n        mu = (g.T @ g) / denom\n        grad = -g # The actual gradient\n        \n        while True:\n            x_cand = hard_threshold(x + mu * g, k)\n            \n            r_cand = y - A @ x_cand\n            fx_cand = 0.5 * (r_cand.T @ r_cand)\n            \n            # Armijo condition for proximal gradient methods\n            # f(x_cand) = f(x) + c * grad.T @ (x_cand - x)\n            armijo_rhs = fx + c * np.dot(grad, x_cand - x)\n            if fx_cand = armijo_rhs:\n                x = x_cand\n                break\n            \n            mu *= beta\n            if mu  1e-20: # Safety break\n                x = x_cand\n                break\n                \n    recovered_support = set(np.where(x != 0)[0])\n    return recovered_support == S_star\n\ndef main():\n    m, n, k = 32, 64, 2\n    S_star = {0, 1}\n    results = []\n\n    # === Case 1: IHT fails ===\n    np.random.seed(123)\n    A_base = np.random.randn(m, n)\n    A_base /= np.linalg.norm(A_base, axis=0)\n    A1 = create_A_with_confusers(A_base, [(10, 0, 0.02), (11, 1, 0.02)])\n    x_star = np.zeros(n)\n    x_star[0], x_star[1] = 1.0, 0.8\n    y1 = A1 @ x_star\n    x0_1 = compute_x0(A1, y1, {10, 11})\n    results.append(run_iht(A1, y1, x0_1, k, S_star, 500, 0.2))\n\n    # === Case 2: NIHT escapes ===\n    results.append(run_niht(A1, y1, x0_1, k, S_star, 200))\n    \n    # === Case 3: Harder case, NIHT fails ===\n    np.random.seed(123)\n    A_base = np.random.randn(m, n)\n    A_base /= np.linalg.norm(A_base, axis=0)\n    confusers = [(10, 0, 0.02), (11, 1, 0.02), (20, 0, 0.001), (21, 1, 0.001)]\n    A3 = create_A_with_confusers(A_base, confusers)\n    y3 = A3 @ x_star + np.random.normal(0, 0.002, m)\n    x0_3 = compute_x0(A3, y3, {20, 21})\n    results.append(run_niht(A3, y3, x0_3, k, S_star, 50))\n\n    # === Case 4: Harder case, NIHT-BT succeeds ===\n    np.random.seed(123) # Re-seed noise for consistency\n    y4 = A3 @ x_star + np.random.normal(0, 0.002, m)\n    x0_4 = compute_x0(A3, y4, {20, 21})\n    results.append(run_niht_bt(A3, y4, x0_4, k, S_star, 500, c=1e-4, beta=0.5))\n\n    print(results)\n\nif __name__ == \"__main__\":\n    main()\n```", "id": "3463072"}]}