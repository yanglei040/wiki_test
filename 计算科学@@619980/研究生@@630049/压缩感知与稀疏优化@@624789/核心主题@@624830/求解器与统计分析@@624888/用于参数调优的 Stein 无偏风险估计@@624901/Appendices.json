{"hands_on_practices": [{"introduction": "LASSO是稀疏恢复中的一个基石模型，但其估计量是作为一个优化问题的解被隐式定义的。本练习将指导您推导LASSO的广义斯坦无偏风险估计(GSURE)，其关键在于运用隐函数微分处理优化问题的最优性条件，从而得到估计量关于观测数据的散度。掌握这一推导过程将为您处理更广泛的、由优化问题定义的估计量提供坚实的理论基础，这在现代信号处理和机器学习中至关重要 [@problem_id:3482264]。", "problem": "考虑压缩感知中的欠定线性逆问题，其中观测值 $y \\in \\mathbb{R}^{m}$ 服从 $y = A x_{0} + w$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$m  n$，且 $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。设估计量 $x_{\\lambda}(y)$ 被定义为凸规划 $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\lVert A x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_1$ 的唯一最小化子，对于某个固定的 $\\lambda  0$。假设 $x_{\\lambda}(y)$ 是局部唯一的，并且关于 $y$ 几乎处处可微。定义均方预测风险为 $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\lVert A x_{\\lambda}(y) - A x_{0} \\rVert_2^2\\right]$。\n\n从 Stein 高斯向量恒等式作为唯一的基本出发点，并且不引用任何已有的风险估计公式，完成以下任务：\n\n1. 推导一个 $R_{\\mathrm{pred}}(\\lambda)$ 的无偏估计量，该估计量仅依赖于数据 $y$、噪声方差 $\\sigma^{2}$ 以及映射 $y \\mapsto A x_{\\lambda}(y)$ 的散度。你的推导必须基于以下性质：如果 $y = \\mu + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$，并且 $h:\\mathbb{R}^{m} \\to \\mathbb{R}^{m}$ 是弱可微且具有可积散度，则 $\\mathbb{E}\\left[\\langle \\varepsilon, h(y) \\rangle\\right] = \\sigma^{2} \\mathbb{E}\\left[\\operatorname{div} h(y)\\right]$。\n\n2. 证明 $x_{\\lambda}(y)$ 满足不动点关系 $x_{\\lambda}(y) = \\operatorname{prox}_{\\tau \\lambda \\lVert\\cdot\\rVert_{1}}\\!\\left(x_{\\lambda}(y) - \\tau A^{\\top}(A x_{\\lambda}(y) - y)\\right)$，对于任意固定的步长 $\\tau  0$，其中 $\\operatorname{prox}_{\\gamma \\lVert\\cdot\\rVert_{1}}(u) = \\arg\\min_{x} \\frac{1}{2}\\lVert x - u \\rVert_2^2 + \\gamma \\lVert x \\rVert_1$。\n\n3. 使用第 2 部分中不动点方程的隐式微分，并利用软阈值算子的雅可比矩阵是一个对角选择器 $D = \\operatorname{diag}(d_{1},\\ldots,d_{n})$（其中 $d_{i} \\in \\{0,1\\}$ 几乎处处成立，且当且仅当第 $i$ 个坐标在软阈值作用下被激活时 $d_{i} = 1$）这一事实，推导出 $\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right)$ 的一个显式表达式，用 $A$、$\\tau$ 和在不动点处求值的 $D$ 来表示。你可以假设你使用的所有逆矩阵对于几乎所有的 $y$ 都存在。\n\n4. 结合你的结果，将广义 Stein 预测无偏风险估计 $\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau)$ 表示为单个闭式解析表达式，该表达式依赖于 $A$、$y$、$\\lambda$、$\\tau$、$\\sigma^{2}$ 以及在 $x_{\\lambda}(y)$ 处求值的 $D$。将你的最终答案表示为单个解析表达式。不需要进行数值计算，也不需要四舍五入。\n\n你的最终答案必须是单个闭式解析表达式。", "solution": "用户提供的问题是统计信号处理和优化理论中一个良定且有科学依据的练习。它要求推导 LASSO 估计量预测风险的广义 Stein 无偏风险估计 (GSURE)。所有必要的数据、模型和假设都已提供，没有矛盾或含糊之处。该问题是有效的，下面给出了完整的解答。\n\n解答分为四个部分，对应于问题陈述中指定的四个任务。\n\n**第 1 部分：预测风险无偏估计量的推导**\n\n均方预测风险定义为 $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\lVert A x_{\\lambda}(y) - A x_{0} \\rVert_2^2\\right]$，其中期望是关于观测向量 $y$ 的分布。观测模型为 $y = A x_{0} + w$，其中 $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。\n让我们将真实均值信号表示为 $\\mu \\equiv A x_{0}$。那么观测模型为 $y = \\mu + w$，风险为 $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\lVert A x_{\\lambda}(y) - \\mu \\rVert_2^2\\right]$。\n\n我们可以将期望内的平方范数展开如下：\n$$\n\\lVert A x_{\\lambda}(y) - \\mu \\rVert_2^2 = \\lVert(A x_{\\lambda}(y) - y) + (y - \\mu)\\rVert_2^2\n$$\n项 $y - \\mu$ 是噪声向量 $w$。令 $\\hat{\\mu}(y) \\equiv A x_{\\lambda}(y)$ 为估计信号。表达式变为：\n$$\n\\lVert\\hat{\\mu}(y) - \\mu\\rVert_2^2 = \\lVert\\hat{\\mu}(y) - y + w\\rVert_2^2 = \\lVert\\hat{\\mu}(y) - y\\rVert_2^2 + \\lVert w\\rVert_2^2 + 2 \\langle \\hat{\\mu}(y) - y, w \\rangle\n$$\n对每一项取期望：\n1.  项 $\\mathbb{E}\\left[\\lVert\\hat{\\mu}(y) - y\\rVert_2^2\\right]$ 保持不变，因为它涉及对 $y$ 的一个函数的期望。\n2.  项 $\\mathbb{E}\\left[\\lVert w\\rVert_2^2\\right]$ 是一个具有独立同分布分量 $w_i \\sim \\mathcal{N}(0, \\sigma^2)$ 的高斯向量的期望平方范数。\n    $$\n    \\mathbb{E}\\left[\\lVert w\\rVert_2^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{m} w_i^2\\right] = \\sum_{i=1}^{m} \\mathbb{E}[w_i^2] = \\sum_{i=1}^{m} \\sigma^2 = m \\sigma^2\n    $$\n3.  对于交叉项 $\\mathbb{E}\\left[2 \\langle \\hat{\\mu}(y) - y, w \\rangle\\right]$，我们应用给定的 Stein 恒等式。令 $\\varepsilon = w = y - \\mu$。该恒等式指出，对于一个弱可微函数 $h(y)$，$\\mathbb{E}[\\langle \\varepsilon, h(y) \\rangle] = \\sigma^2 \\mathbb{E}[\\operatorname{div} h(y)]$。\n    令 $h(y) = \\hat{\\mu}(y) - y = A x_{\\lambda}(y) - y$。$h(y)$ 关于 $y$ 的散度是：\n    $$\n    \\operatorname{div}_y h(y) = \\operatorname{div}_y (A x_{\\lambda}(y) - y) = \\operatorname{div}_y(A x_{\\lambda}(y)) - \\operatorname{div}_y(y)\n    $$\n    恒等映射 $y \\mapsto y$ 的散度是 $\\operatorname{div}_y(y) = \\sum_{i=1}^{m} \\frac{\\partial y_i}{\\partial y_i} = m$。\n    因此，$\\operatorname{div}_y h(y) = \\operatorname{div}_y(A x_{\\lambda}(y)) - m$。\n    将 Stein 恒等式应用于交叉项：\n    $$\n    \\mathbb{E}[\\langle \\hat{\\mu}(y) - y, w \\rangle] = \\sigma^2 \\mathbb{E}[\\operatorname{div}_y(A x_{\\lambda}(y)) - m]\n    $$\n结合所有项，风险为：\n$$\nR_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\lVert\\hat{\\mu}(y) - y\\rVert_2^2\\right] + m \\sigma^2 + 2\\sigma^2 \\mathbb{E}\\left[\\operatorname{div}_y(A x_{\\lambda}(y)) - m\\right]\n$$\n$$\nR_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[ \\lVert A x_{\\lambda}(y) - y \\rVert_2^2 + m \\sigma^2 + 2\\sigma^2 \\operatorname{div}_y(A x_{\\lambda}(y)) - 2m \\sigma^2 \\right]\n$$\n$$\nR_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[ \\lVert A x_{\\lambda}(y) - y \\rVert_2^2 - m \\sigma^2 + 2\\sigma^2 \\operatorname{div}_y(A x_{\\lambda}(y)) \\right]\n$$\n由于括号内量的期望等于风险，因此这个量是风险的一个无偏估计量。我们将其表示为 $\\mathrm{SURE}_{\\mathrm{pred}}(y)$：\n$$\n\\mathrm{SURE}_{\\mathrm{pred}}(y) = \\lVert A x_{\\lambda}(y) - y \\rVert_2^2 - m \\sigma^2 + 2\\sigma^2 \\operatorname{div}_y(A x_{\\lambda}(y))\n$$\n该估计量依赖于数据 $y$、噪声方差 $\\sigma^2$ 以及映射 $y \\mapsto A x_{\\lambda}(y)$ 的散度，符合要求。\n\n**第 2 部分：估计量的不动点关系**\n\n估计量 $x_{\\lambda}(y)$ 是凸优化问题的解：\n$$\nx_{\\lambda}(y) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ F(x) \\equiv \\frac{1}{2} \\lVert A x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_1 \\right\\}\n$$\n令 $f(x) = \\frac{1}{2} \\lVert A x - y \\rVert_2^2$ 和 $g(x) = \\lambda \\lVert x \\rVert_1$。函数 $f(x)$ 是可微的，其梯度为 $\\nabla f(x) = A^{\\top}(Ax-y)$。函数 $g(x)$ 是凸的但不可微。最小化子 $x^* = x_{\\lambda}(y)$ 的一阶最优性条件表明，零向量必须属于目标函数在 $x^*$ 处的次微分：\n$$\n0 \\in \\partial F(x^*) = \\nabla f(x^*) + \\partial g(x^*)\n$$\n代入梯度和次微分的表达式：\n$$\n0 \\in A^{\\top}(Ax^* - y) + \\lambda \\partial \\lVert x^* \\rVert_1\n$$\n这可以重写为：\n$$\n-A^{\\top}(Ax^* - y) \\in \\lambda \\partial \\lVert x^* \\rVert_1\n$$\n现在，考虑问题中给出的不动点关系：\n$$\nx^* = \\operatorname{prox}_{\\tau \\lambda \\lVert\\cdot\\rVert_{1}}\\!\\left(x^* - \\tau A^{\\top}(A x^* - y)\\right)\n$$\n对于某个步长 $\\tau  0$。根据定义，$z = \\operatorname{prox}_{\\gamma h}(v)$ 是 $\\frac{1}{2}\\lVert z-v \\rVert_2^2 + \\gamma h(z)$ 的唯一最小化子。此近端问题的最优性条件是 $0 \\in (z-v) + \\gamma \\partial h(z)$，这等价于 $v-z \\in \\gamma \\partial h(z)$。\n将此定义应用于我们的不动点关系，我们设：\n- $z = x^*$\n- $v = x^* - \\tau A^{\\top}(A x^* - y)$\n- $\\gamma h(\\cdot) = \\tau \\lambda \\lVert\\cdot\\rVert_1$\n近端映射的最优性条件变为：\n$$\n\\left(x^* - \\tau A^{\\top}(A x^* - y)\\right) - x^* \\in \\tau \\lambda \\partial \\lVert x^* \\rVert_1\n$$\n$$\n-\\tau A^{\\top}(A x^* - y) \\in \\tau \\lambda \\partial \\lVert x^* \\rVert_1\n$$\n由于 $\\tau  0$，我们可以除以 $\\tau$ 得到：\n$$\n-A^{\\top}(A x^* - y) \\in \\lambda \\partial \\lVert x^* \\rVert_1\n$$\n这正是原始 LASSO 问题的最优性条件。因此，对于任何 $\\tau  0$，估计量 $x_{\\lambda}(y)$ 是给定迭代的不动点。\n\n**第 3 部分：散度项的推导**\n\n我们需要计算 $\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right)$。这由 $A x_{\\lambda}(y)$ 关于 $y$ 的雅可比矩阵的迹给出。令 $J_x \\equiv \\frac{\\partial x_{\\lambda}(y)}{\\partial y^{\\top}}$ 为估计量 $x_{\\lambda}(y)$ 关于 $y$ 的 $n \\times m$ 雅可比矩阵。那么 $A x_{\\lambda}(y)$ 的雅可比矩阵是 $A J_x$，这是一个 $m \\times m$ 矩阵。散度是：\n$$\n\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right) = \\operatorname{Tr}(A J_x)\n$$\n为了找到 $J_x$，我们对第 2 部分的不动点方程进行隐式微分。令 $x = x_{\\lambda}(y)$ 并将软阈值算子表示为 $S(u) \\equiv \\operatorname{prox}_{\\tau \\lambda \\lVert\\cdot\\rVert_{1}}(u)$。不动点方程为：\n$$\nx = S\\left(x - \\tau A^{\\top}(A x - y)\\right)\n$$\n令 $S$ 的参数为 $u(x,y) = x - \\tau A^{\\top} A x + \\tau A^{\\top} y$。我们使用链式法则对 $x = S(u(x,y))$ 关于 $y^{\\top}$ 进行微分：\n$$\n\\frac{\\partial x}{\\partial y^{\\top}} = \\frac{\\partial S(u)}{\\partial u^{\\top}} \\left( \\frac{\\partial u}{\\partial x^{\\top}} \\frac{\\partial x}{\\partial y^{\\top}} + \\frac{\\partial u}{\\partial y^{\\top}} \\right)\n$$\n问题陈述中指出，软阈值算子的雅可比矩阵 $\\frac{\\partial S(u)}{\\partial u^{\\top}}$ 是一个对角矩阵 $D \\in \\mathbb{R}^{n \\times n}$，其中 $d_{ii} \\in \\{0, 1\\}$。\n其他雅可比矩阵是：\n- $\\frac{\\partial x}{\\partial y^{\\top}} = J_x$\n- $\\frac{\\partial u}{\\partial x^{\\top}} = I_n - \\tau A^{\\top} A$\n- $\\frac{\\partial u}{\\partial y^{\\top}} = \\tau A^{\\top}$\n将这些代入链式法则方程：\n$$\nJ_x = D \\left( (I_n - \\tau A^{\\top} A) J_x + \\tau A^{\\top} \\right)\n$$\n$$\nJ_x = D (I_n - \\tau A^{\\top} A) J_x + D \\tau A^{\\top}\n$$\n我们现在解出 $J_x$：\n$$\nJ_x - D (I_n - \\tau A^{\\top} A) J_x = \\tau D A^{\\top}\n$$\n$$\n(I_n - D + \\tau D A^{\\top} A) J_x = \\tau D A^{\\top}\n$$\n假设左侧矩阵可逆（根据问题陈述的允许），我们得到 $J_x$：\n$$\nJ_x = \\tau (I_n - D + \\tau D A^{\\top} A)^{-1} D A^{\\top}\n$$\n散度是 $A J_x$ 的迹。使用迹的循环性质 $\\operatorname{Tr}(XY) = \\operatorname{Tr}(YX)$，我们有 $\\operatorname{Tr}(A J_x) = \\operatorname{Tr}(J_x A)$：\n$$\n\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right) = \\operatorname{Tr}(J_x A) = \\operatorname{Tr}\\left( \\tau (I_n - D + \\tau D A^{\\top} A)^{-1} D A^{\\top} A \\right)\n$$\n这为散度提供了所需的关于 $A$、$\\tau$ 和 $D$ 的显式表达式。\n\n**第 4 部分：GSURE 公式**\n\n最后，我们结合第 1 部分和第 3 部分的结果，得到广义 Stein 预测无偏风险估计 $\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau)$ 的闭式表达式。我们将散度的表达式代入 SURE 公式：\n$$\n\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau) = \\lVert A x_{\\lambda}(y) - y \\rVert_2^2 - m \\sigma^{2} + 2 \\sigma^{2} \\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right)\n$$\n$$\n\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau) = \\lVert A x_{\\lambda}(y) - y \\rVert_2^2 - m \\sigma^{2} + 2 \\sigma^{2} \\operatorname{Tr}\\left(\\tau \\left(I_n - D + \\tau D A^{\\top} A\\right)^{-1} D A^{\\top} A \\right)\n$$\n该表达式依赖于指定的量：矩阵 $A$、数据 $y$（它决定了 $x_{\\lambda}(y)$ 和 $D$）、正则化参数 $\\lambda$（隐含在 $x_{\\lambda}(y)$ 和 $D$ 中）、步长参数 $\\tau$、噪声方差 $\\sigma^2$ 和选择器矩阵 $D$。这就是所寻求的最终解析表达式。", "answer": "$$\\boxed{\\lVert A x_{\\lambda}(y) - y \\rVert_2^2 - m \\sigma^{2} + 2 \\sigma^{2} \\operatorname{Tr}\\left(\\tau \\left(I_n - D + \\tau D A^{\\top} A\\right)^{-1} D A^{\\top} A \\right)}$$", "id": "3482264"}, {"introduction": "即便我们拥有如GSURE公式中对散度的精确表达式，其计算（例如矩阵的迹）在计算上可能非常昂贵，这催生了蒙特卡洛(MC)近似方法。本练习深入探讨了用于估计散度的有限差分蒙特卡洛方法，要求您分析其核心的权衡：由有限差分步长 $\\epsilon$ 引入的近似偏差，与由蒙特卡洛采样和数值精度限制引起的估计方差。掌握这种分析对于在实践中稳健地实施基于SURE的方法至关重要，它教会我们如何以有原则的方式选择数值参数以平衡准确性和稳定性 [@problem_id:3482321]。", "problem": "考虑压缩感知和稀疏优化中的标准高斯去噪模型：一个未知的确定性信号向量 $x_{0} \\in \\mathbb{R}^{n}$ 通过含噪测量 $y = x_{0} + w$ 被观测到，其中 $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。令 $f: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ 是一个预测器（例如，一个近端或迭代重构映射），它在一个固定输入 $y$ 的邻域内是三阶连续可微的，其雅可比矩阵 $J_{f}(y)$ 和三阶导数张量 $D^{3}f(y)$ 在该邻域内的算子范数由一个常数 $M_{3}$ 界定。对于 $f$ 在 $y$ 处的均方误差风险，其 Stein 无偏风险估计 (SURE) 为\n$$\n\\mathrm{SURE}(f;y) \\triangleq -n \\sigma^{2} + \\lVert f(y) - y \\rVert_2^2 + 2 \\sigma^{2} \\,\\mathrm{div}\\, f(y),\n$$\n其中 $\\mathrm{div}\\, f(y) = \\mathrm{tr}\\, J_{f}(y)$ 表示 $f$ 在 $y$ 处的散度。\n\n为了在没有显式雅可比矩阵访问权限的情况下近似 $\\mathrm{div}\\, f(y)$，考虑使用有限差分蒙特卡洛 (MC) 估计器：抽取 $K$ 个独立的探测向量 $z_{1}, \\dots, z_{K} \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathcal{N}(0, I_{n})$，并对于一个步长 $\\epsilon  0$ 定义，\n$$\nd_{k}(\\epsilon) \\triangleq \\frac{1}{\\epsilon} \\, z_{k}^{\\top} \\big( f(y + \\epsilon z_{k}) - f(y) \\big), \\quad \\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y) \\triangleq \\frac{1}{K} \\sum_{k=1}^{K} d_{k}(\\epsilon),\n$$\n以及相应的 MC-SURE 估计器\n$$\n\\mathrm{SURE}_{\\epsilon,K}(f;y) \\triangleq -n \\sigma^{2} + \\lVert f(y) - y \\rVert_2^2 + 2 \\sigma^{2} \\, \\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y).\n$$\n\n假设一个标准的浮点计算模型：对于其参数 $y$，$f$ 的每次计算返回 $f(y) + \\eta$，其中 $\\eta$ 是一个零均值扰动，在不同的调用和探测中是独立的，其协方差的算子范数由 $u^{2} s_{f}(y)^{2} I_{n}$ 界定，其中 $s_{f}(y)  0$ 是某个已知的局部尺度，而 $u \\in (0,1)$ 是机器单位舍入。在所述的光滑性条件下，您可以使用泰勒展开和高斯矩恒等式。在 $\\epsilon$ 足够小，以至于高阶项被您保留的项所主导的情况下进行分析。\n\n1. 证明 $\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y)$ 相对于 $\\mathrm{div}\\, f(y)$ 的偏差是 $\\epsilon^{2}$ 阶的，并提供一个形式为\n$$\n\\big| \\mathbb{E}[\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y)] - \\mathrm{div}\\, f(y) \\big| \\leq c_{b} \\, \\epsilon^{2},\n$$\n的界，其中常数 $c_{b}$ 应明确地用 $M_{3}$ 和高斯矩表示。\n\n2. 证明 $\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y)$ 的方差可以分解为一个随 $1/K$ 缩放的探测方差项和一个随 $1/(K \\epsilon^{2})$ 缩放的浮点诱导项，即\n$$\n\\mathrm{Var}\\big( \\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y) \\big) \\leq \\frac{c_{v}}{K} + \\frac{c_{r}}{K \\epsilon^{2}},\n$$\n对于某些常数 $c_{v}$（与 $\\epsilon$ 无关）和 $c_{r}$（取决于 $u$ 和 $s_{f}(y)$），您应当定义这些常数。\n\n3. 结合以上结果，得到 $\\mathrm{SURE}_{\\epsilon,K}(f;y)$ 关于 $\\mathrm{SURE}(f;y)$ 的主阶均方误差 (MSE) 近似。忽略此 MSE 中与 $\\epsilon$ 无关的项，推导出最小化与 $\\epsilon$ 相关的主阶 MSE 的步长 $\\epsilon^{\\star}$，将其表示为 $K$、$c_{b}$ 和 $c_{r}$ 的函数。请将您的最终 $\\epsilon^{\\star}$ 以单个闭式解析表达式的形式给出。\n\n您的最终答案必须是这个关于 $\\epsilon^{\\star}$ 的单一表达式。不需要进行数值计算。", "solution": "该问题要求解出蒙特卡洛 Stein 无偏风险估计 (SURE) 过程中的最优步长 $\\epsilon^{\\star}$。这需要分析散度估计器的均方误差 (MSE)，该误差可分解为其偏差的平方和方差。我们将按顺序解决问题的三个部分。\n\n第一部分：散度估计器的偏差\n\n散度的蒙特卡洛估计器为 $\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y) = \\frac{1}{K} \\sum_{k=1}^{K} d_{k}(\\epsilon)$，其中 $d_{k}(\\epsilon) = \\frac{1}{\\epsilon} z_{k}^{\\top} ( f(y + \\epsilon z_{k}) - f(y) )$。由于探测向量 $z_k$ 的独立同分布性质，估计器的期望为 $\\mathbb{E}[\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y)] = \\mathbb{E}[d_1(\\epsilon)]$。我们来分析对于一个通用探测向量 $z \\sim \\mathcal{N}(0, I_n)$ 的单项 $d(\\epsilon)$ 的期望。\n\n浮点模型指出，在点 $y_{arg}$ 处对 $f$ 的一次计算返回 $\\tilde{f}(y_{arg}) = f(y_{arg}) + \\eta$，其中 $\\mathbb{E}[\\eta]=0$ 且 $\\eta$ 与其他扰动和探测向量 $z$ 独立。\n计算项为 $d(\\epsilon) = \\frac{1}{\\epsilon} z^{\\top} (\\tilde{f}(y+\\epsilon z) - \\tilde{f}(y)) = \\frac{1}{\\epsilon} z^{\\top} (f(y+\\epsilon z) + \\eta_1 - f(y) - \\eta_2)$。\n取期望，我们有：\n$$\n\\mathbb{E}[d(\\epsilon)] = \\mathbb{E}\\left[\\frac{1}{\\epsilon} z^{\\top} (f(y+\\epsilon z) - f(y))\\right] + \\frac{1}{\\epsilon} \\mathbb{E}[z^{\\top}(\\eta_1 - \\eta_2)]\n$$\n由于 $\\eta_1, \\eta_2$ 是零均值且独立于 $z$ 的，第二项为零。因此，浮点误差不会引入偏差。\n现在我们使用 $f$ 在 $y$ 点的泰勒展开来分析第一项。由于 $f$ 是 $C^3$ 的，我们可以写出：\n$$\nf(y + \\epsilon z) = f(y) + \\epsilon J_f(y) z + \\frac{\\epsilon^2}{2} D^2f(y)(z,z) + \\mathcal{R}_3(y, \\epsilon z)\n$$\n其中 $J_f(y)$ 是 $f$ 在 $y$ 处的雅可比矩阵，$D^2f(y)(z,z)$ 是一个向量，其第 $i$ 个分量是 $z^\\top H_i(y) z$（其中 $H_i$ 是 $f_i$ 的海森矩阵），而 $\\mathcal{R}_3$ 是余项。根据带拉格朗日余项的泰勒定理，我们可以用三阶导数来表示余项：\n$f(y + \\epsilon z) - f(y) = \\epsilon J_f(y)z + \\frac{\\epsilon^2}{2} D^2f(y)(z,z) + \\frac{\\epsilon^3}{6} D^3f(y+\\xi \\epsilon z)(z,z,z)$ 其中某个 $\\xi \\in (0,1)$。\n将此代入 $d(\\epsilon)$ 的表达式并取期望：\n$$\n\\mathbb{E}[d(\\epsilon)] = \\mathbb{E}\\left[ z^{\\top}J_f(y)z + \\frac{\\epsilon}{2} z^{\\top}D^2f(y)(z,z) + \\frac{\\epsilon^2}{6} z^{\\top}D^3f(y+\\xi \\epsilon z)(z,z,z) \\right]\n$$\n我们计算关于 $z \\sim \\mathcal{N}(0, I_n)$ 的每一项的期望：\n1.  $\\mathbb{E}[z^{\\top}J_f(y)z] = \\mathbb{E}[\\mathrm{tr}(z^{\\top}J_f(y)z)] = \\mathbb{E}[\\mathrm{tr}(J_f(y)zz^{\\top})] = \\mathrm{tr}(J_f(y)\\mathbb{E}[zz^{\\top}])$。由于 $\\mathbb{E}[zz^{\\top}] = I_n$，该项等于 $\\mathrm{tr}(J_f(y)) = \\mathrm{div}f(y)$。\n2.  第二项涉及 $z^{\\top}D^2f(y)(z,z) = \\sum_{i,j,k} z_i (H_i)_{jk} z_j z_k$，这是关于 $z$ 各分量的三次多项式。由于 $z_i$ 是独立的标准正态变量，其分布是对称的。任何奇数次单项式的期望均为零，所以对于所有 $i,j,k$，$\\mathbb{E}[z_i z_j z_k]=0$。因此，$\\mathbb{E}[z^{\\top}D^2f(y)(z,z)] = 0$。\n因此，估计器的偏差为：\n$$\n\\mathrm{Bias}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) = \\mathbb{E}[d(\\epsilon)] - \\mathrm{div}f(y) = \\frac{\\epsilon^2}{6} \\mathbb{E}\\left[ z^{\\top}D^3f(y+\\xi \\epsilon z)(z,z,z) \\right]\n$$\n为了找到界 $c_b$，我们取绝对值并使用给定的界。对于小的 $\\epsilon$，$D^3f(y+\\xi \\epsilon z) \\approx D^3f(y)$。问题陈述三阶导数张量的算子范数由 $M_3$ 界定。\n$$\n|\\mathrm{Bias}| \\leq \\frac{\\epsilon^2}{6} \\mathbb{E}\\left[ |z^{\\top}D^3f(y+\\xi \\epsilon z)(z,z,z)| \\right] \\leq \\frac{\\epsilon^2}{6} \\mathbb{E}\\left[ \\lVert z\\rVert \\lVert D^3f(y+\\xi \\epsilon z)(z,z,z)\\rVert \\right]\n$$\n使用算子范数定义，$\\lVert D^3f(\\cdot)(z,z,z)\\rVert \\leq \\lVert D^3f(\\cdot)\\rVert_{\\mathrm{op}} \\lVert z\\rVert^3 \\leq M_3 \\lVert z\\rVert^3$。\n$$\n|\\mathrm{Bias}| \\leq \\frac{\\epsilon^2 M_3}{6} \\mathbb{E}[\\lVert z\\rVert^4]\n$$\n我们计算标准高斯向量范数的四阶矩：\n$\\mathbb{E}[\\lVert z\\rVert^4] = \\mathbb{E}[(\\sum_{i=1}^n z_i^2)^2] = \\sum_{i=1}^n \\mathbb{E}[z_i^4] + \\sum_{i \\neq j} \\mathbb{E}[z_i^2 z_j^2]$。\n对于标准正态变量 $z_i$，$\\mathbb{E}[z_i^2]=1$ 且 $\\mathbb{E}[z_i^4]=3$。对于 $i \\neq j$，$z_i$ 和 $z_j$ 是独立的，所以 $\\mathbb{E}[z_i^2 z_j^2]=\\mathbb{E}[z_i^2]\\mathbb{E}[z_j^2]=1$。\n有 $n$ 个对角项和 $n(n-1)$ 个非对角项。\n$\\mathbb{E}[\\lVert z\\rVert^4] = n \\cdot 3 + n(n-1) \\cdot 1 = 3n + n^2 - n = n^2 + 2n$。\n偏差的界为 $|\\mathbb{E}[\\widehat{\\mathrm{div}}_{\\epsilon,K}(f;y)] - \\mathrm{div}\\, f(y)| \\leq \\frac{M_3(n^2+2n)}{6} \\epsilon^2$。\n因此，我们确定常数 $c_b$ 为：\n$$\nc_b = \\frac{M_3(n^2+2n)}{6}\n$$\n\n第二部分：散度估计器的方差\n\n估计器的方差为 $\\mathrm{Var}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) = \\frac{1}{K^2} \\sum_{k=1}^K \\mathrm{Var}(d_k(\\epsilon)) = \\frac{1}{K}\\mathrm{Var}(d(\\epsilon))$，这是由于探测向量是独立同分布的。\n我们将 $d(\\epsilon)$ 分解为来自精确函数的部分和来自舍入误差的部分：\n$d(\\epsilon) = d_{\\mathrm{exact}}(\\epsilon) + d_{\\mathrm{round}}(\\epsilon)$，其中 $d_{\\mathrm{exact}}(\\epsilon) = \\frac{1}{\\epsilon} z^{\\top} (f(y+\\epsilon z) - f(y))$ 和 $d_{\\mathrm{round}}(\\epsilon) = \\frac{1}{\\epsilon} z^{\\top} (\\eta_1 - \\eta_2)$。\n探测向量 $z$ 与浮点误差 $\\eta_1, \\eta_2$ 独立。因此，$d_{\\mathrm{exact}}$ 和 $d_{\\mathrm{round}}$ 是不相关的，且 $\\mathrm{Var}(d(\\epsilon)) = \\mathrm{Var}(d_{\\mathrm{exact}}(\\epsilon)) + \\mathrm{Var}(d_{\\mathrm{round}}(\\epsilon))$。\n\n我们首先分析舍入误差的方差。由于 $\\mathbb{E}[d_{\\mathrm{round}}(\\epsilon)]=0$：\n$$\n\\mathrm{Var}(d_{\\mathrm{round}}(\\epsilon)) = \\mathbb{E}[d_{\\mathrm{round}}(\\epsilon)^2] = \\frac{1}{\\epsilon^2} \\mathbb{E}[(z^{\\top}(\\eta_1-\\eta_2))^2] = \\frac{1}{\\epsilon^2} \\mathbb{E}[z^{\\top}(\\eta_1-\\eta_2)(\\eta_1-\\eta_2)^{\\top}z]\n$$\n使用迹恒等式和期望的线性性：\n$$\n\\mathrm{Var}(d_{\\mathrm{round}}(\\epsilon)) = \\frac{1}{\\epsilon^2} \\mathrm{tr}\\left( \\mathbb{E}[(\\eta_1-\\eta_2)(\\eta_1-\\eta_2)^{\\top}] \\mathbb{E}[zz^{\\top}] \\right)\n$$\n由于 $\\mathbb{E}[zz^{\\top}]=I_n$ 且 $\\eta_1, \\eta_2$ 是独立且零均值的，$\\mathbb{E}[(\\eta_1-\\eta_2)(\\eta_1-\\eta_2)^{\\top}] = \\mathrm{Cov}(\\eta_1) + \\mathrm{Cov}(\\eta_2)$。令 $C_1=\\mathrm{Cov}(\\eta_1)$ 且 $C_2=\\mathrm{Cov}(\\eta_2)$。\n问题陈述协方差有界，满足 $C_i \\preceq u^2 s_f(y_i)^2 I_n$（在 Loewner 序下）。对于小的 $\\epsilon$，我们假设 $s_f(y+\\epsilon z) \\approx s_f(y)$。这意味着 $\\mathrm{tr}(C_i) \\leq \\mathrm{tr}(u^2 s_f(y)^2 I_n) = n u^2 s_f(y)^2$。\n$\\mathrm{Var}(d_{\\mathrm{round}}(\\epsilon)) \\leq \\frac{1}{\\epsilon^2}(\\mathrm{tr}(C_1) + \\mathrm{tr}(C_2)) \\leq \\frac{2n u^2 s_f(y)^2}{\\epsilon^2}$。\n总方差中的项 $\\frac{c_r}{K\\epsilon^2}$ 意味着 $c_r = 2n u^2 s_f(y)^2$。\n\n接下来，我们分析探测方差项 $\\mathrm{Var}(d_{\\mathrm{exact}}(\\epsilon))$。问题要求一个与 $\\epsilon$ 无关的常数 $c_v$，这表明我们应该考虑 $\\epsilon \\to 0$ 的极限。\n$d_{\\mathrm{exact}}(\\epsilon) = z^{\\top}J_f(y)z + O(\\epsilon)$。\n主阶方差是 $\\mathrm{Var}(z^{\\top}J_f(y)z)$。令 $J = J_f(y)$。我们知道 $\\mathbb{E}[z^{\\top}Jz] = \\mathrm{tr}(J)$。\n方差为 $\\mathrm{Var}(z^{\\top}Jz) = \\mathbb{E}[(z^{\\top}Jz)^2] - (\\mathrm{tr}(J))^2$。\n使用 Isserlis 定理，$\\mathbb{E}[(\\sum_{i,j} J_{ij} z_i z_j)^2] = \\sum_{i,j,k,l} J_{ij} J_{kl} \\mathbb{E}[z_i z_j z_k z_l] = \\sum_{i,j,k,l} J_{ij} J_{kl} (\\delta_{ij}\\delta_{kl} + \\delta_{ik}\\delta_{jl} + \\delta_{il}\\delta_{jk})$。\n此式计算结果为 $(\\mathrm{tr}(J))^2 + \\lVert J\\rVert_F^2 + \\mathrm{tr}(J^2)$，其中 $\\lVert J\\rVert_F$ 是 Frobenius 范数。\n所以，$\\mathrm{Var}(z^{\\top}Jz) = \\lVert J\\rVert_F^2 + \\mathrm{tr}(J^2)$。这就是常数 $c_v$。\n$c_v = \\lVert J_f(y)\\rVert_F^2 + \\mathrm{tr}((J_f(y))^2)$。\n结合这些项，散度估计器的方差有界为：\n$$\n\\mathrm{Var}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) \\leq \\frac{c_v}{K} + \\frac{c_r}{K\\epsilon^2}\n$$\n其中 $c_v = \\lVert J_f(y)\\rVert_F^2 + \\mathrm{tr}((J_f(y))^2)$ 且 $c_r = 2n u^2 s_f(y)^2$。\n\n第三部分：最优步长 $\\epsilon^{\\star}$\n\nSURE 估计器的 MSE 为 $\\mathbb{E}[(\\mathrm{SURE}_{\\epsilon,K} - \\mathrm{SURE})^2]$。\n$\\mathrm{SURE}_{\\epsilon,K} - \\mathrm{SURE} = 2\\sigma^2(\\widehat{\\mathrm{div}}_{\\epsilon,K} - \\mathrm{div}f)$。\nMSE 为 $(2\\sigma^2)^2 \\mathbb{E}[(\\widehat{\\mathrm{div}}_{\\epsilon,K} - \\mathrm{div}f)^2] = (2\\sigma^2)^2 \\mathrm{MSE}(\\widehat{\\mathrm{div}}_{\\epsilon,K})$。\n散度估计器的 MSE 是其方差和偏差平方的和：\n$$\n\\mathrm{MSE}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) = \\mathrm{Var}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) + (\\mathrm{Bias}(\\widehat{\\mathrm{div}}_{\\epsilon,K}))^2\n$$\n使用第一部分和第二部分的主阶近似和界：\n$$\n\\mathrm{MSE}(\\widehat{\\mathrm{div}}_{\\epsilon,K}) \\approx \\left(\\frac{c_v}{K} + \\frac{c_r}{K\\epsilon^2}\\right) + (c_b\\epsilon^2)^2 = \\frac{c_v}{K} + \\frac{c_r}{K\\epsilon^2} + c_b^2 \\epsilon^4\n$$\n问题要求最小化与 $\\epsilon$ 相关的主阶 MSE。缩放因子 $(2\\sigma^2)^2$ 和与 $\\epsilon$ 无关的项 $\\frac{c_v}{K}$ 不影响最优的 $\\epsilon$。我们最小化函数：\n$$\ng(\\epsilon) = c_b^2 \\epsilon^4 + \\frac{c_r}{K\\epsilon^2}\n$$\n为了找到最小值，我们将其关于 $\\epsilon$ 的导数设为零：\n$$\n\\frac{dg}{d\\epsilon} = 4c_b^2\\epsilon^3 - \\frac{2c_r}{K\\epsilon^3} = 0\n$$\n$$\n4c_b^2(\\epsilon^{\\star})^3 = \\frac{2c_r}{K(\\epsilon^{\\star})^3} \\implies (\\epsilon^{\\star})^6 = \\frac{2c_r}{4Kc_b^2} = \\frac{c_r}{2Kc_b^2}\n$$\n二阶导数 $g''(\\epsilon) = 12c_b^2\\epsilon^2 + \\frac{6c_r}{K\\epsilon^4}$ 对于 $\\epsilon  0$ 总是正的，所以这是一个最小值点。\n最优步长为：\n$$\n\\epsilon^{\\star} = \\left(\\frac{c_r}{2Kc_b^2}\\right)^{1/6}\n$$\n代入 $c_b$ 和 $c_r$ 的表达式：\n$$\n\\epsilon^{\\star} = \\left(\\frac{2n u^2 s_f(y)^2}{2K \\left(\\frac{M_3(n^2+2n)}{6}\\right)^2}\\right)^{1/6} = \\left(\\frac{n u^2 s_f(y)^2}{K \\frac{M_3^2 n^2(n+2)^2}{36}}\\right)^{1/6} = \\left(\\frac{36 u^2 s_f(y)^2}{K M_3^2 n (n+2)^2}\\right)^{1/6}\n$$\n这就是最优步长 $\\epsilon^{\\star}$ 的最终闭式表达式。", "answer": "$$ \\boxed{ \\left( \\frac{c_r}{2 K c_b^2} \\right)^{1/6} } $$", "id": "3482321"}, {"introduction": "现在我们转向一个更前沿的主题：为非凸罚函数（如最小最大凹罚函数MCP）进行超参数调优，这种方法通常比LASSO能取得更好的稀疏恢复效果。本练习将引导您实现斯坦无偏梯度风险估计(SUGAR)，它能提供对风险*梯度*的无偏估计。通过沿此梯度下降，您可以追踪正则化参数 $\\lambda$ 的最优路径，这代表了斯坦恒等式在前沿研究中的一个尖端应用，它将理论推导、数值近似和优化算法融合成一个高效的超参数调优引擎 [@problem_id:3482335]。", "problem": "考虑一个稀疏去噪模型，其中信号 $x_0 \\in \\mathbb{R}^n$ 通过加性高斯噪声 $w \\sim \\mathcal{N}(0,\\sigma^2 I_n)$ 进行观测，得到 $y = x_0 + w$。定义一个估计量 $\\mu(y; \\lambda, \\gamma)$ 作为目标函数 $J(x; y, \\lambda, \\gamma) = \\tfrac{1}{2}\\lVert x - y \\rVert_2^2 + \\sum_{i=1}^n p_{\\lambda,\\gamma}(|x_i|)$ 的任意一个最小化子，其中 $p_{\\lambda,\\gamma}(\\cdot)$ 是极小极大凹惩罚项 (Minimax Concave Penalty, MCP)，其正则化尺度为 $\\lambda  0$，非凸性参数为 $\\gamma  1$。MCP 的定义为 $p_{\\lambda,\\gamma}(t) = \\lambda \\int_0^t \\max\\{0, 1 - \\tfrac{s}{\\gamma \\lambda}\\} \\, ds$，它是在稀疏优化中经过充分检验的非凸惩罚项。\n\n从高斯噪声模型的基本事实和高斯散度的 Stein 引理出发，推导一个无偏风险估计，该估计仅依赖于 $y$、$\\sigma$ 以及 $\\mu(y; \\lambda, \\gamma)$ 相对于 $y$ 的散度。然后，使用 Stein 无偏梯度风险估计器 (Stein's Unbiased GRAdient risk estimator, SUGAR) 推导出此风险估计相对于 $\\lambda$ 的梯度的可计算表达式，并利用高斯探测进行散度的蒙特卡洛迹估计。将 $\\mu(y; \\lambda, \\gamma)$ 视为一个逐元素作用的可分离估计量，并包括估计量相对于数据 $y$ 和参数 $\\lambda$ 的所有必需的偏导数。您不得依赖任何最终风险或梯度的简化公式；相反，您的推导必须基于所述的定义和性质。\n\n使用这些推导，实现一个连续路径 $\\lambda(t)$，$t \\in [0,1]$，该路径由离散步长 $t_k = \\tfrac{k}{T}$ 参数化，其中 $T$ 是一个固定的正整数。将 $\\lambda(0)$ 初始化为一个依赖于数据的较大值，并使用 SUGAR 梯度，通过对无偏风险估计关于 $\\lambda$ 进行梯度下降来演化 $\\lambda(t)$。在每一步，将 $\\lambda(t)$ 裁剪到一个正区间内，以确保估计量是良定义的。量化以下路径分析量：\n- 一个正则性统计量，定义为路径上的最大绝对步长 $\\max_k |\\lambda(t_{k+1}) - \\lambda(t_k)|$。\n- 路径上的分岔事件数，定义为 $\\mu(y; \\lambda(t_k), \\gamma)$ 的支撑集大小相对于 $\\mu(y; \\lambda(t_{k-1}), \\gamma)$ 发生变化的索引 $k$ 的数量。\n- 一个布尔指标，指示随着 $t$ 的增加，支撑集大小在路径上是否单调非减（将 $\\lambda(t)$ 解释为稀疏恢复中的递减同伦参数）。\n\n您的程序必须为每个测试用例执行以下任务：\n1. 生成一个 $k$-稀疏的真实信号 $x_0 \\in \\mathbb{R}^n$，其非零项独立地从标准正态分布中抽取，并放置在均匀随机的索引上。\n2. 使用测试用例中为可复现性提供的固定伪随机种子生成 $y = x_0 + w$，其中 $w \\sim \\mathcal{N}(0,\\sigma^2 I_n)$。\n3. 通过所述优化目标产生的逐元素 MCP 近端映射来实现估计量 $\\mu(y; \\lambda, \\gamma)$。\n4. 推导并实现以 $y$、$\\sigma$ 和 $\\mu$ 相对于 $y$ 的散度表示的 Stein 无偏风险估计。\n5. 使用高斯探测和一个小的有限差分参数，通过蒙特卡洛迹估计来推导并实现 SUGAR 梯度 $\\tfrac{\\partial}{\\partial \\lambda} \\mathrm{SURE}(y; \\lambda, \\gamma)$。\n6. 使用 SUGAR 梯度，通过梯度下降追踪连续路径 $\\lambda(t)$ 共 $T$ 步，并使用指定的步长和裁剪边界。\n7. 对于每个测试用例，计算并返回一个元组，其中包含最终值 $\\lambda(1)$、分岔事件的数量、正则性统计量以及转换为整数的单调性指标（真为 $1$，假为 $0$）。\n\n如果引入任何角度，必须使用弧度，尽管此处预计不会有。不涉及物理单位。所有数值输出必须是无量纲数。\n\n测试套件：\n对于以下每个测试用例，您的程序必须使用指定的参数和随机种子。\n\n- 测试用例 A（正常路径，中等非凸性）：\n  - $n = 80$, $k = 8$, $\\sigma = 0.15$, $\\gamma = 3.0$, $T = 120$, 步长 $\\alpha = 0.005$, 种子 $s = 1234$。\n- 测试用例 B（边缘情况，强非凸性）：\n  - $n = 80$, $k = 8$, $\\sigma = 0.15$, $\\gamma = 1.2$, $T = 120$, 步长 $\\alpha = 0.001$, 种子 $s = 5678$。\n- 测试用例 C（边界情况，近似于最小绝对收缩和选择算子 (LASSO) 的近凸惩罚项）：\n  - $n = 80$, $k = 8$, $\\sigma = 0.05$, $\\gamma = 100.0$, $T = 120$, 步长 $\\alpha = 0.010$, 种子 $s = 9012$。\n\n初始化和边界：\n- 将 $\\lambda(0)$ 初始化为 $\\lambda_{\\mathrm{max}} = \\max_{i} |y_i|$。\n- 将 $\\lambda(t)$ 裁剪到区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 内，其中 $\\lambda_{\\min} = 10^{-6}$ 且 $\\lambda_{\\max}$ 如上所述。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含三个测试用例的结果，格式为逗号分隔的 Python 风格列表的列表，每个内部列表按上述顺序包含四个值。例如： \"[[lambda_A,bifurcations_A,regularity_A,monotone_A],[lambda_B,bifurcations_B,regularity_B,monotone_B],[lambda_C,bifurcations_C,regularity_C,monotone_C]]\"。第一个和第三个分量应提供为浮点数，第二个和第四个分量应提供为整数。", "solution": "该问题要求在一个稀疏信号去噪的背景下，为极小极大凹惩罚项 (MCP) 推导并实现一个参数调整方案。任务的核心是使用 Stein 无偏风险估计 (SURE) 及其梯度（通过 Stein 无偏梯度风险估计器 (SUGAR) 和蒙特卡洛技术估计），为正则化参数 $\\lambda$ 定义一个连续路径。\n\n首先，我们形式化估计量 $\\mu(y; \\lambda, \\gamma)$。要最小化的目标函数是 $J(x; y, \\lambda, \\gamma) = \\tfrac{1}{2}\\lVert x - y \\rVert_2^2 + \\sum_{i=1}^n p_{\\lambda,\\gamma}(|x_i|)$。这个目标函数是可分离的，意味着我们可以通过独立地最小化每个分量 $x_i$ 来找到最优的 $x = (x_1, \\dots, x_n)$：\n$$\n\\hat{x}_i = \\arg\\min_{x_i \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}(x_i - y_i)^2 + p_{\\lambda,\\gamma}(|x_i|) \\right\\}\n$$\n根据定义，这是函数 $p_{\\lambda,\\gamma}(|\\cdot|)$ 应用于 $y_i$ 的近端算子。令 $\\mu_i(y_i; \\lambda, \\gamma)$ 表示此解。对于 $t  0$，MCP 惩罚项 $p_{\\lambda,\\gamma}(t)$ 的导数是 $p'_{\\lambda,\\gamma}(t) = \\lambda \\max\\{0, 1 - \\tfrac{t}{\\gamma \\lambda}\\}$。一阶最优性条件是 $0 \\in x_i - y_i + \\partial p_{\\lambda,\\gamma}(|x_i|)$，其中 $\\partial$ 表示次梯度。对 $x_i$ 求解，得到逐元素的估计量，它是一个稳定阈值函数 (firm-thresholding function)：\n$$\n\\mu_i(y_i; \\lambda, \\gamma) = \\begin{cases} 0  \\text{if } |y_i| \\le \\lambda \\\\ \\mathrm{sgn}(y_i) \\frac{\\gamma(|y_i| - \\lambda)}{\\gamma - 1}  \\text{if } \\lambda  |y_i| \\le \\gamma \\lambda \\\\ y_i  \\text{if } |y_i|  \\gamma \\lambda \\end{cases}\n$$\n这个函数是可分离的，即 $\\mu_i$ 只依赖于 $y_i$。\n\n接下来，我们推导无偏风险估计。风险，或称期望均方误差 (MSE)，是 $R(\\lambda) = \\mathbb{E}[\\lVert \\mu(y; \\lambda) - x_0 \\rVert_2^2]$。我们可以展开它，并利用 $y = x_0 + w$ 且 $w \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 这一事实：\n$$\n\\begin{aligned}\nR(\\lambda) = \\mathbb{E}[\\lVert (\\mu(y) - y) + (y - x_0) \\rVert_2^2] \\\\\n= \\mathbb{E}[\\lVert \\mu(y) - y \\rVert_2^2 + \\lVert y - x_0 \\rVert_2^2 + 2(y-x_0)^T(\\mu(y) - y)] \\\\\n= \\mathbb{E}[\\lVert \\mu(y) - y \\rVert_2^2] + \\mathbb{E}[\\lVert w \\rVert_2^2] + 2\\mathbb{E}[w^T(\\mu(y) - y)]\n\\end{aligned}\n$$\n第二项是 $\\mathbb{E}[\\lVert w \\rVert_2^2] = n\\sigma^2$。对于第三项，我们应用 Stein 引理，该引理指出，对于一个随机向量 $w \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 和一个弱可微函数 $h: \\mathbb{R}^n \\to \\mathbb{R}^n$，有 $\\mathbb{E}[w^T h(y)] = \\sigma^2 \\mathbb{E}[\\nabla_y \\cdot h(y)]$。令 $h(y) = \\mu(y) - y$。其散度为 $\\nabla_y \\cdot h(y) = \\nabla_y \\cdot \\mu(y) - \\nabla_y \\cdot y = (\\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial y_i}) - n$。\n代回，我们得到：\n$$\nR(\\lambda) = \\mathbb E\\left[\\lVert\\mu(y) - y\\rVert_2^2 + n\\sigma^2 + 2\\sigma^2 (\\nabla_y \\cdot \\mu(y) - n) \\right] = \\mathbb E\\left[\\lVert\\mu(y) - y\\rVert_2^2 - n\\sigma^2 + 2\\sigma^2 \\nabla_y \\cdot \\mu(y) \\right]\n$$\n通过去掉期望，我们得到了 $R(\\lambda)$ 的一个无偏估计量，即 Stein 无偏风险估计 (SURE)：\n$$\n\\mathrm{SURE}(y; \\lambda) = \\lVert\\mu(y) - y\\rVert_2^2 - n\\sigma^2 + 2\\sigma^2 \\nabla_y \\cdot \\mu(y)\n$$\n其中散度项是 $\\nabla_y \\cdot \\mu(y) = \\sum_{i=1}^n \\frac{\\partial \\mu_i(y_i)}{\\partial y_i}$。导数 $\\frac{\\partial \\mu_i}{\\partial y_i}$ 是分段常数：\n$$\n\\frac{\\partial \\mu_i(y_i)}{\\partial y_i} = \\begin{cases} 0  \\text{if } |y_i|  \\lambda \\\\ \\frac{\\gamma}{\\gamma-1}  \\text{if } \\lambda  |y_i|  \\gamma\\lambda \\\\ 1  \\text{if } |y_i|  \\gamma\\lambda \\end{cases}\n$$\n不连续点测度为零，不影响 Stein 引理中的积分。\n\n下一步是找到风险相对于 $\\lambda$ 的梯度 $\\frac{\\partial R}{\\partial \\lambda}$，以执行梯度下降。对 SURE 公式的朴素求导是有问题的，因为它涉及到不连续函数 $\\frac{\\partial \\mu_i}{\\partial y_i}$ 的导数。SUGAR 框架为真实风险梯度提供了一个无偏估计量。风险的梯度是：\n$$\n\\frac{\\partial R}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\mathbb{E}[\\lVert \\mu(y; \\lambda) - x_0 \\rVert_2^2] = \\mathbb{E}\\left[2(\\mu - x_0)^T \\frac{\\partial \\mu}{\\partial \\lambda}\\right]\n$$\n使用 $x_0 = y - w$，我们有：\n$$\n\\frac{\\partial R}{\\partial \\lambda} = \\mathbb{E}\\left[2(\\mu - y)^T \\frac{\\partial \\mu}{\\partial \\lambda} + 2w^T \\frac{\\partial \\mu}{\\partial \\lambda}\\right]\n$$\n将 Stein 引理应用于第二项，其中 $h(y) = \\frac{\\partial \\mu}{\\partial \\lambda}(y; \\lambda)$，得到 $\\mathbb{E}[w^T \\frac{\\partial \\mu}{\\partial \\lambda}] = \\sigma^2 \\mathbb{E}[\\nabla_y \\cdot \\frac{\\partial \\mu}{\\partial \\lambda}]$。因此：\n$$\n\\frac{\\partial R}{\\partial \\lambda} = \\mathbb{E}\\left[2(\\mu - y)^T \\frac{\\partial \\mu}{\\partial \\lambda} + 2\\sigma^2 \\nabla_y \\cdot \\frac{\\partial \\mu}{\\partial \\lambda}\\right]\n$$\nSUGAR 梯度估计量 $\\hat{g}(\\lambda)$ 是期望内的项：\n$$\n\\hat{g}(\\lambda) = 2(\\mu(y; \\lambda) - y)^T \\frac{\\partial \\mu}{\\partial \\lambda}(y; \\lambda) + 2\\sigma^2 \\left(\\nabla_y \\cdot \\frac{\\partial \\mu}{\\partial \\lambda}\\right)(y; \\lambda)\n$$\n为了创建一个可计算的表达式，我们近似偏导数和散度。相对于 $\\lambda$ 的偏导数使用中心有限差分和一个小步长 $\\epsilon_\\lambda$ 来近似：\n$$\n\\frac{\\partial \\mu(y; \\lambda)}{\\partial \\lambda} \\approx \\frac{\\mu(y; \\lambda+\\epsilon_\\lambda) - \\mu(y; \\lambda-\\epsilon_\\lambda)}{2\\epsilon_\\lambda}\n$$\n散度项 $\\nabla_y \\cdot f(y) = \\mathrm{Tr}(J_f(y))$，其中 $f(y) = \\frac{\\partial \\mu}{\\partial \\lambda}$，使用蒙特卡洛方法，通过一个高斯探测向量 $\\delta \\sim \\mathcal{N}(0, I_n)$ 和一个小步长 $\\epsilon_y$ 来估计。散度估计为：\n$$\n\\nabla_y \\cdot f(y) \\approx \\frac{1}{\\epsilon_y} \\delta^T (f(y+\\epsilon_y\\delta) - f(y))\n$$\n结合这些近似，SUGAR 梯度估计器实现如下：\n令 $f(z, \\lambda_{val}) = \\frac{\\mu(z; \\lambda_{val}+\\epsilon_\\lambda) - \\mu(z; \\lambda_{val}-\\epsilon_\\lambda)}{2\\epsilon_\\lambda}$。\n$\\hat{g}(\\lambda)$ 的第一项是 $T_1 = 2(\\mu(y; \\lambda) - y)^T f(y, \\lambda)$。\n散度估计为 $\\nabla_y \\cdot f \\approx \\frac{1}{\\epsilon_y} \\delta^T (f(y+\\epsilon_y\\delta, \\lambda) - f(y, \\lambda))$。\n$\\hat{g}(\\lambda)$ 的第二项是 $T_2 = 2\\sigma^2 \\frac{1}{\\epsilon_y} \\delta^T (f(y+\\epsilon_y\\delta, \\lambda) - f(y, \\lambda))$。\n最终的可计算梯度是 $\\hat{g}(\\lambda) = T_1 + T_2$。每次梯度计算需要对估计量 $\\mu$ 求值四次。\n\n整体算法流程如下：\n1. 对于给定的测试用例，生成稀疏信号 $x_0$ 和带噪观测值 $y$。\n2. 初始化正则化参数 $\\lambda_0 = \\max_i |y_i|$。这个值确保初始估计 $\\mu(y, \\lambda_0)$ 是零向量，这是同伦方法中一个常见的起点。\n3. 预先生成一个探测向量 $\\delta \\sim \\mathcal{N}(0, I_n)$，用于所有 SUGAR 梯度计算。\n4. 对 $k=0, \\dots, T-1$ 进行迭代：\n   a. 在当前参数值 $\\lambda_k = \\lambda(t_k)$ 处计算 SUGAR 梯度 $\\hat{g}(\\lambda_k)$。\n   b. 通过梯度下降更新参数：$\\lambda' = \\lambda_k - \\alpha \\hat{g}(\\lambda_k)$。\n   c. 将新值裁剪到区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 以获得 $\\lambda_{k+1}$。\n5. 经过 $T$ 步后，分析生成的路径 $\\{\\lambda_k\\}_{k=0}^T$。\n   a. 为每个 $\\lambda_k$ 计算支撑集大小 $S_k = \\lVert\\mu(y; \\lambda_k)\\rVert_0$。\n   b. 分岔数是 $k \\in \\{1, \\dots, T\\}$ 中 $S_k \\neq S_{k-1}$ 的计数。\n   c. 正则性统计量是 $\\max_{k \\in \\{0, \\dots, T-1\\}} |\\lambda_{k+1} - \\lambda_k|$。\n   d. 如果对于所有 $k \\in \\{1, \\dots, T\\}$ 都有 $S_k \\ge S_{k-1}$，则单调性指标为 $1$，否则为 $0$。\n6. 一个测试用例的最终结果是元组 $(\\lambda_T, \\text{分岔数}, \\text{正则性}, \\text{单调性})$。", "answer": "```python\nimport numpy as np\n\ndef mcp_prox(y, lam, gam):\n    \"\"\"\n    Computes the element-wise proximal operator of the MCP.\n    This corresponds to the estimator mu(y; lambda, gamma).\n    \"\"\"\n    if lam = 0:\n        return np.full_like(y, np.nan)\n        \n    y_abs = np.abs(y)\n    y_sign = np.sign(y)\n    \n    # Handle the gamma -> 1 case, which approaches hard thresholding\n    if np.isclose(gam, 1.0):\n        res = np.where(y_abs > lam, y, 0.0)\n        return res\n        \n    # Standard MCP for gamma > 1\n    term1 = np.zeros_like(y)\n    term2 = y_sign * gam * (y_abs - lam) / (gam - 1.0)\n    term3 = y\n    \n    # Piecewise application\n    res = np.where(y_abs = lam, term1, term3)\n    res = np.where((y_abs > lam)  (y_abs = gam * lam), term2, res)\n    \n    return res\n\ndef compute_sugar_gradient(y, lam, gam, sigma, delta, eps_y, eps_lam, prox_func):\n    \"\"\"\n    Computes the SUGAR gradient of the risk with respect to lambda.\n    \"\"\"\n    # Finite difference approximation of d(mu)/d(lambda)\n    def grad_lam_mu_approx(vec_y, val_lam):\n        # Clip lambda for finite difference to avoid negative values\n        lam_plus = max(0.0, val_lam + eps_lam)\n        lam_minus = max(0.0, val_lam - eps_lam)\n        mu_p = prox_func(vec_y, lam_plus, gam)\n        mu_m = prox_func(vec_y, lam_minus, gam)\n        return (mu_p - mu_m) / (2.0 * eps_lam)\n\n    # Compute d(mu)/d(lambda) at the current point y\n    grad_lam_mu_at_y = grad_lam_mu_approx(y, lam)\n    \n    # Compute mu at the current point\n    mu_at_y = prox_func(y, lam, gam)\n    \n    # First term of the SUGAR gradient\n    term1 = 2.0 * np.dot(mu_at_y - y, grad_lam_mu_at_y)\n    \n    # Second term (divergence) of the SUGAR gradient\n    # Compute d(mu)/d(lambda) at the perturbed point y + eps_y * delta\n    y_pert = y + eps_y * delta\n    grad_lam_mu_at_y_pert = grad_lam_mu_approx(y_pert, lam)\n    \n    # Estimate divergence using the Gaussian probe\n    div_est = (1.0 / eps_y) * np.dot(delta, grad_lam_mu_at_y_pert - grad_lam_mu_at_y)\n    term2 = 2.0 * sigma**2 * div_est\n    \n    return term1 + term2\n\ndef run_case(n, k, sigma, gam, T, alpha, seed):\n    \"\"\"\n    Runs a single test case for the MCP parameter path analysis.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    \n    # 1. Generate data\n    x0 = np.zeros(n)\n    nonzero_indices = rng.choice(n, k, replace=False)\n    x0[nonzero_indices] = rng.randn(k)\n    \n    noise = rng.randn(n) * sigma\n    y = x0 + noise\n    \n    # 2. Initialization and bounds\n    lambda_max = np.max(np.abs(y))\n    lambda_min = 1e-6\n    \n    lambda_path = [lambda_max]\n    current_lambda = lambda_max\n    \n    # 3. Generate probe for SUGAR gradient\n    delta = rng.randn(n)\n    eps_y = 1e-6\n    eps_lam = 1e-6\n    \n    # 4. Track continuation path\n    for _ in range(T):\n        grad = compute_sugar_gradient(y, current_lambda, gam, sigma, delta, eps_y, eps_lam, mcp_prox)\n        \n        # Gradient descent step\n        current_lambda = current_lambda - alpha * grad\n        \n        # Clipping\n        current_lambda = np.clip(current_lambda, lambda_min, lambda_max)\n        \n        lambda_path.append(current_lambda)\n        \n    lambda_path = np.array(lambda_path)\n    \n    # 5. Path Analysis\n    # a. Final lambda\n    final_lambda = lambda_path[-1]\n    \n    # b. Regularity statistic\n    regularity_stat = np.max(np.abs(np.diff(lambda_path)))\n    \n    # c. Bifurcation events and Monotonicity\n    support_sizes = []\n    for lam in lambda_path:\n        mu = mcp_prox(y, lam, gam)\n        support_sizes.append(np.count_nonzero(mu))\n        \n    support_sizes = np.array(support_sizes)\n    support_diffs = np.diff(support_sizes)\n    \n    bifurcation_events = np.count_nonzero(support_diffs)\n    \n    # Monotonicity check: support must be non-decreasing as t increases (lambda is decreasing)\n    is_monotone = 1 if np.all(support_diffs >= 0) else 0\n\n    return final_lambda, bifurcation_events, regularity_stat, is_monotone\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, k, sigma, gamma, T, step_size_alpha, seed)\n        (80, 8, 0.15, 3.0, 120, 0.005, 1234),  # Test Case A\n        (80, 8, 0.15, 1.2, 120, 0.001, 5678),  # Test Case B\n        (80, 8, 0.05, 100.0, 120, 0.010, 9012) # Test Case C\n    ]\n\n    results = []\n    for case_params in test_cases:\n        final_lambda, bifurcations, regularity, monotone = run_case(*case_params)\n        results.append([final_lambda, bifurcations, regularity, int(monotone)])\n\n    # Format the final output string\n    inner_strings = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3482335"}]}