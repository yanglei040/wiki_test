{"hands_on_practices": [{"introduction": "在深入探讨正式的“神谕不等式”（oracle inequalities）之前，掌握 LASSO 变量选择的基本机理至关重要。本练习利用 KKT 最优性条件，探讨当正则化参数减小时，LASSO 如何选择其第一个活动变量。通过比较有无预测变量标准化这两种情景 [@problem_id:3191299]，您将亲身体会到数据缩放如何直接影响模型优先选择哪些特征。", "problem": "您的任务是研究当两个预测变量与初始残差具有相同相关性但尺度不同时，最小绝对收缩和选择算子 (LASSO) 的变量选择行为。请仅使用 LASSO 的基本定义和最优性条件，来推断当正则化参数从一个非常大的值减小时，在有和没有预测变量标准化的两种情况下，哪个变量会首先被选中。假设不使用截距项，并且所有向量都是实值的。\n\n设 LASSO 估计量定义为以下目标函数的最小化器：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2n}\\left\\|y - X \\beta \\right\\|_2^2 + \\lambda \\left\\|\\beta\\right\\|_1,\n$$\n其中 $n$ 是观测值的数量，$p$ 是预测变量的数量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，其列为 $x_j$ ($j \\in \\{1,\\dots,p\\}$)，$y \\in \\mathbb{R}^n$ 是响应向量，$\\lambda \\ge 0$ 是正则化参数。\n\n我们将重点关注 LASSO 解路径的第一步，即当 $\\lambda$ 从一个非常大的值开始减小，从 $\\beta = 0$ 开始，此时残差为 $r_0 = y$。您的任务是根据基本原理，确定在以下两种情况下，两个候选预测变量 $x_1$ 和 $x_2$ 中哪一个会首先被选中：\n- 场景 A (不进行标准化)：直接使用给定的 $X$ 列。\n- 场景 B (进行标准化)：在应用 LASSO 之前，将每列 $x_j$ 替换为 $x_j / \\|x_j\\|_2$，其中 $\\|x_j\\|_2$ 表示 $x_j$ 的欧几里得范数。\n\n使用 LASSO 的基本最优性条件（在问题陈述中不允许使用快捷公式）来推导在每种情况下哪个预测变量首先进入模型。如果在首次进入时的选择标准出现平局，则采用确定性规则选择较小的索引，即选择索引 0 (代表 $x_1$) 而不是索引 1 (代表 $x_2$)。\n\n您的程序必须为下面提供的每个测试用例计算一对整数 $[i_A, i_B]$，其中 $i_A \\in \\{0,1\\}$ 是在场景 A (不进行标准化) 中首先选中的预测变量的索引，$i_B \\in \\{0,1\\}$ 是在场景 B (进行标准化) 中首先选中的预测变量的索引。您将根据在初始残差 $r_0 = y$ 处 LASSO 的基本最优性条件的推导，实现所隐含的选择规则。\n\n测试套件 (每个案例将 $x_1$、$x_2$ 和 $y$ 列为坐标数组)：\n- 案例 1 (不同尺度，初始相关性相同)：\n  - $x_1 = [2, 0, 0]$\n  - $x_2 = [0, 1, 0]$\n  - $y = [1, 2, 0]$\n- 案例 2 (相同尺度，初始相关性相同)：\n  - $x_1 = [1, 0, 0]$\n  - $x_2 = [0, 1, 0]$\n  - $y = [2, 2, 0]$\n- 案例 3 (与案例 1 的尺度顺序相反，初始相关性相同)：\n  - $x_1 = [1, 0, 0]$\n  - $x_2 = [0, 2, 0]$\n  - $y = [2, 1, 0]$\n\n所有坐标都是纯数字 (无单位)。不涉及角度和百分比。\n\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。输出必须是与上述三个案例按顺序对应的整数对列表，每个整数对的格式为 $[i_A,i_B]$。例如，一个有效的输出格式是：\n$[[i_{A,1},i_{B,1}],[i_{A,2},i_{B,2}],[i_{A,3},i_{B,3}]]$.\n\n您的输出值必须仅为整数。", "solution": "该问题要求我们确定，当正则化参数 $\\lambda$ 从一个非常大的值减小时，LASSO 算法首先选择两个预测变量 $x_1$ 和 $x_2$ 中的哪一个。这一确定必须基于 LASSO 目标函数的基本 Karush-Kuhn-Tucker (KKT) 最优性条件，并且必须在两种不同的场景下进行：有和没有预测变量标准化。\n\n需要最小化的 LASSO 目标函数由下式给出：\n$$\nL(\\beta) = \\frac{1}{2n}\\left\\|y - X \\beta \\right\\|_2^2 + \\lambda \\left\\|\\beta\\right\\|_1\n$$\n此处，$\\beta$ 是系数向量，$y$ 是响应向量，$X$ 是设计矩阵，其列为 $x_j$，$n$ 是观测值的数量，$\\lambda \\ge 0$ 是正则化参数。函数 $L(\\beta)$ 是凸函数，它是一个凸可微函数 (最小二乘项) 和一个凸不可微函数 ($\\ell_1$ 范数) 的和。$L(\\beta)$ 的最小值由次梯度最优性条件来刻画。\n\n最小二乘项 $f(\\beta) = \\frac{1}{2n}\\left\\|y - X \\beta \\right\\|_2^2$ 关于 $\\beta$ 的梯度是：\n$$\n\\nabla f(\\beta) = \\frac{1}{n} X^T (X\\beta - y) = -\\frac{1}{n} X^T (y - X\\beta)\n$$\n$\\ell_1$ 范数项 $h(\\beta) = \\lambda \\|\\beta\\|_1$ 的次梯度是一个向量集合 $\\partial h(\\beta)$。如果一个向量 $v \\in \\mathbb{R}^p$ 的第 $j$ 个分量 $v_j$ 满足以下条件，则该向量是 $h(\\beta)$ 的次梯度：\n$$\nv_j = \\lambda \\, \\text{sign}(\\beta_j) \\quad \\text{if } \\beta_j \\neq 0\n$$\n$$\nv_j \\in [-\\lambda, \\lambda] \\quad \\text{if } \\beta_j = 0\n$$\n最优性条件表明，$\\hat{\\beta}$ 是 $L(\\beta)$ 的一个最小化器，当且仅当零向量在 $L$ 于 $\\hat{\\beta}$ 处的次梯度中，这意味着 $0 \\in \\nabla f(\\hat{\\beta}) + \\partial h(\\hat{\\beta})$。这可以写成：\n$$\n-\\nabla f(\\hat{\\beta}) \\in \\partial h(\\hat{\\beta})\n$$\n代入梯度，我们对每个分量 $j \\in \\{1, \\dots, p\\}$ 得到：\n$$\n\\frac{1}{n} x_j^T (y - X\\hat{\\beta}) = \\lambda \\gamma_j\n$$\n其中如果 $\\hat{\\beta}_j \\neq 0$，则 $\\gamma_j = \\text{sign}(\\hat{\\beta}_j)$；如果 $\\hat{\\beta}_j = 0$，则 $\\gamma_j \\in [-1, 1]$。\n\n我们感兴趣的是 LASSO 路径的第一步，我们从一个非常大的 $\\lambda$ 开始。对于足够大的 $\\lambda$，惩罚项占主导地位，迫使最优解为 $\\hat{\\beta} = 0$。此时，残差为 $r_0 = y - X \\cdot 0 = y$。$\\hat{\\beta} = 0$ 的最优性条件简化为：\n$$\n\\frac{1}{n} x_j^T y = \\lambda \\gamma_j\n$$\n由于对所有 $j$ 都有 $\\hat{\\beta}_j = 0$，我们必须有 $\\gamma_j \\in [-1, 1]$。这导出了条件：\n$$\n\\left| \\frac{1}{n} x_j^T y \\right| \\le \\lambda \\quad \\text{for all } j \\in \\{1, \\dots, p\\}\n$$\n只要 $\\lambda$ 大于或等于这些绝对值的最大值，解 $\\hat{\\beta} = 0$ 就是最优的。第一个系数即将变为非零的 $\\lambda$ 临界值是：\n$$\n\\lambda_{\\text{max}} = \\max_{j \\in \\{1, \\dots, p\\}} \\left| \\frac{1}{n} x_j^T y \\right|\n$$\n当我们从一个大于 $\\lambda_{\\text{max}}$ 的值减小 $\\lambda$ 时，第一个进入模型的预测变量 $k$ (即其系数 $\\beta_k$ 变为非零) 将是使条件 $|\\frac{1}{n} x_k^T y| \\le \\lambda$ 首先被违反的那个。这发生在实现最大值的预测变量 $k$ 上：\n$$\nk = \\arg\\max_{j \\in \\{1, \\dots, p\\}} \\left| \\frac{1}{n} x_j^T y \\right|\n$$\n常数因子 $1/n$ 不影响最大值的索引，因此选择标准简化为找到与初始残差 $y$ 具有最大绝对内积的预测变量 $x_k$：\n$$\nk = \\arg\\max_{j \\in \\{1, \\dots, p\\}} |x_j^T y|\n$$\n这就是我们将应用的基本规则。\n\n**场景 A：不进行标准化**\n预测变量按原样使用。第一个进入模型的预测变量通过比较 $|x_1^T y|$ 和 $|x_2^T y|$ 来确定。如果 $|x_1^T y| \\ge |x_2^T y|$，则所选预测变量的索引 $i_A$ 为 0，否则为 1，并应用指定的平局决胜规则 (在平局情况下选择较小的索引)。\n\n**场景 B：进行标准化**\n在应用 LASSO 之前，每个预测变量列 $x_j$ 被替换为 $x'_j = x_j / \\|x_j\\|_2$。选择规则在结构上保持不变，但应用于标准化后的预测变量：\n$$\nk = \\arg\\max_{j \\in \\{1, 2\\}} |(x'_j)^T y|\n$$\n需要比较的项是：\n$$\n|(x'_1)^T y| = \\left| \\left(\\frac{x_1}{\\|x_1\\|_2}\\right)^T y \\right| = \\frac{|x_1^T y|}{\\|x_1\\|_2}\n$$\n和\n$$\n|(x'_2)^T y| = \\left| \\left(\\frac{x_2}{\\|x_2\\|_2}\\right)^T y \\right| = \\frac{|x_2^T y|}{\\|x_2\\|_2}\n$$\n如果 $\\frac{|x_1^T y|}{\\|x_1\\|_2} \\ge \\frac{|x_2^T y|}{\\|x_2\\|_2}$，则所选预测变量的索引 $i_B$ 为 0，否则为 1，同样应用平局决胜规则。项 $\\frac{|x_j^T y|}{\\|x_j\\|_2}$ 与 $x_j$ 和 $y$ 之间的皮尔逊相关系数的绝对值成正比 (如果向量已经中心化)。\n\n现在我们将这些规则应用于提供的测试用例。\n\n**案例 1：** $x_1 = [2, 0, 0]$, $x_2 = [0, 1, 0]$, $y = [1, 2, 0]$\n- 内积：\n  - $x_1^T y = (2)(1) + (0)(2) + (0)(0) = 2$\n  - $x_2^T y = (0)(1) + (1)(2) + (0)(0) = 2$\n- 范数：\n  - $\\|x_1\\|_2 = \\sqrt{2^2 + 0^2 + 0^2} = 2$\n  - $\\|x_2\\|_2 = \\sqrt{0^2 + 1^2 + 0^2} = 1$\n\n- **场景 A：** 比较 $|x_1^T y| = 2$ 和 $|x_2^T y| = 2$。由于 $2=2$，出现平局。平局决胜规则选择较小的索引。因此，$i_A = 0$。\n- **场景 B：** 比较 $\\frac{|x_1^T y|}{\\|x_1\\|_2} = \\frac{2}{2} = 1$ 和 $\\frac{|x_2^T y|}{\\|x_2\\|_2} = \\frac{2}{1} = 2$。由于 $1  2$，最大值对应于预测变量 $x_2$。因此，$i_B = 1$。\n- 案例 1 的结果：$[0, 1]$\n\n**案例 2：** $x_1 = [1, 0, 0]$, $x_2 = [0, 1, 0]$, $y = [2, 2, 0]$\n- 内积：\n  - $x_1^T y = (1)(2) + (0)(2) + (0)(0) = 2$\n  - $x_2^T y = (0)(2) + (1)(2) + (0)(0) = 2$\n- 范数：\n  - $\\|x_1\\|_2 = \\sqrt{1^2 + 0^2 + 0^2} = 1$\n  - $\\|x_2\\|_2 = \\sqrt{0^2 + 1^2 + 0^2} = 1$\n\n- **场景 A：** 比较 $|x_1^T y| = 2$ 和 $|x_2^T y| = 2$。由于 $2=2$，出现平局，所以 $i_A = 0$。\n- **场景 B：** 比较 $\\frac{|x_1^T y|}{\\|x_1\\|_2} = \\frac{2}{1} = 2$ 和 $\\frac{|x_2^T y|}{\\|x_2\\|_2} = \\frac{2}{1} = 2$。由于 $2=2$，出现平局，所以 $i_B = 0$。\n- 案例 2 的结果：$[0, 0]$\n\n**案例 3：** $x_1 = [1, 0, 0]$, $x_2 = [0, 2, 0]$, $y = [2, 1, 0]$\n- 内积：\n  - $x_1^T y = (1)(2) + (0)(1) + (0)(0) = 2$\n  - $x_2^T y = (0)(2) + (2)(1) + (0)(0) = 2$\n- 范数：\n  - $\\|x_1\\|_2 = \\sqrt{1^2 + 0^2 + 0^2} = 1$\n  - $\\|x_2\\|_2 = \\sqrt{0^2 + 2^2 + 0^2} = 2$\n\n- **场景 A：** 比较 $|x_1^T y| = 2$ 和 $|x_2^T y| = 2$。由于 $2=2$，出现平局，所以 $i_A = 0$。\n- **场景 B：** 比较 $\\frac{|x_1^T y|}{\\|x_1\\|_2} = \\frac{2}{1} = 2$ 和 $\\frac{|x_2^T y|}{\\|x_2\\|_2} = \\frac{2}{2} = 1$。由于 $2  1$，最大值对应于预测变量 $x_1$。因此，$i_B = 0$。\n- 案例 3 的结果：$[0, 0]$\n\n总之，对预测变量进行标准化会改变选择顺序。在不进行标准化的情况下，LASSO 偏爱与残差具有较大内积的预测变量，而内积受预测变量的尺度 (范数) 影响。通过标准化，所有预测变量在尺度 (单位 $\\ell_2$ 范数) 上被置于同等地位，选择则基于哪个标准化后的预测变量与残差最相关。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes which of two predictors is selected first by LASSO under two scenarios:\n    A) No predictor standardization.\n    B) With predictor standardization.\n\n    The selection is based on the foundational optimality conditions of LASSO,\n    which state that the first variable to enter the model (as lambda decreases)\n    is the one that maximizes the absolute inner product with the current residual.\n    At the start (beta=0), the residual is the response vector y.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: different scales with identical initial inner products\n        {'x1': np.array([2.0, 0.0, 0.0]), 'x2': np.array([0.0, 1.0, 0.0]), 'y': np.array([1.0, 2.0, 0.0])},\n        # Case 2: equal scales with identical initial inner products\n        {'x1': np.array([1.0, 0.0, 0.0]), 'x2': np.array([0.0, 1.0, 0.0]), 'y': np.array([2.0, 2.0, 0.0])},\n        # Case 3: reverse of Case 1 scale ordering, identical initial inner products\n        {'x1': np.array([1.0, 0.0, 0.0]), 'x2': np.array([0.0, 2.0, 0.0]), 'y': np.array([2.0, 1.0, 0.0])},\n    ]\n\n    results = []\n    for case in test_cases:\n        x1, x2, y = case['x1'], case['x2'], case['y']\n\n        # --- Scenario A: No Standardization ---\n        # The selection criterion is argmax_j |x_j^T y|.\n        # We compare |x1^T y| and |x2^T y|.\n        dot_prod_1 = np.abs(np.dot(x1, y))\n        dot_prod_2 = np.abs(np.dot(x2, y))\n\n        # Apply the tie-breaking rule: pick smaller index (0) in case of a tie.\n        i_A = 0 if dot_prod_1 = dot_prod_2 else 1\n\n        # --- Scenario B: Standardization ---\n        # Predictors are scaled to have unit l2-norm: x'_j = x_j / ||x_j||_2.\n        # The selection criterion is argmax_j |(x'_j)^T y|.\n        # This is equivalent to comparing |x1^T y| / ||x1||_2 and |x2^T y| / ||x2||_2.\n        norm_1 = np.linalg.norm(x1)\n        norm_2 = np.linalg.norm(x2)\n        \n        # Check for zero norm to avoid division by zero, although not expected in these test cases.\n        # A predictor with zero norm would have a correlation of 0 (or NaN) and would not be selected\n        # unless all other predictors also have zero correlation.\n        corr_like_1 = dot_prod_1 / norm_1 if norm_1  0 else 0.0\n        corr_like_2 = dot_prod_2 / norm_2 if norm_2  0 else 0.0\n        \n        # Apply the tie-breaking rule.\n        i_B = 0 if corr_like_1 = corr_like_2 else 1\n\n        results.append([i_A, i_B])\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, e.g., [[0,1],[0,0],[0,0]]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3191299"}, {"introduction": "理解了基本的选择规则后，我们现在研究 LASSO 在何种条件下能够完美识别真实的活动预测变量集合——这是一项关键的“神谕”性质。本练习将引导您在一个理想化的正交设计背景下，完成一个完整的理论推导 [@problem_id:3464166]。您将建立起所需的最小信号强度、噪声水平和问题维度之间的精确关系，从而揭示高概率支撑集恢复的数学基础。", "problem": "考虑固定设计线性模型 $y = X \\beta^{\\star} + w$，其中 $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，$X \\in \\mathbb{R}^{n \\times p}$ 的列经过归一化，具有正交性，即 $X^{\\top} X = n I_{p}$。令 $S = \\operatorname{supp}(\\beta^{\\star})$ 且 $|S| = s$，并定义 $\\beta_{\\min} = \\min_{j \\in S} |\\beta^{\\star}_{j}|$。最小绝对收缩和选择算子 (LASSO) 估计量 $\\hat{\\beta}$ 定义为以下凸规划的任意一个最小化子：\n\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n\n其中调节参数 $\\lambda  0$。令 $\\delta \\in (0, 1)$ 为一个给定的误差概率，并假设除了所述模型和设计之外没有其他结构。\n\n你的任务是从估计量的定义和高斯随机变量的基本概率尾部界出发，然后执行以下步骤：\n\n(1) 定义 $g = \\frac{1}{n} X^{\\top} w$，并使用标准高斯尾部界和并集界来确定最小的值 $\\tau = \\tau(n, p, \\delta, \\sigma)$，使得事件 $\\mathcal{E} = \\{\\|g\\|_{\\infty} \\le \\tau\\}$ 以至少 $1 - \\delta$ 的概率成立，并将结果表示为关于 $n$、$p$、$\\delta$ 和 $\\sigma$ 的闭式形式。\n\n(2) 通过分析在正交性条件 $X^{\\top} X = n I_{p}$ 下 LASSO 的 Karush–Kuhn–Tucker (KKT) 条件，证明在事件 $\\mathcal{E}$ 上，对于任何选择的 $\\lambda \\ge \\tau$，LASSO 估计量可以按坐标简化为阈值为 $\\lambda$ 的软阈值化操作。仅使用此简化结果，推导：\n- 一个形式为 $\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2} \\le C \\sqrt{s} \\lambda$ 的 $\\ell_{2}$-估计不等式，并找出您可以使用基本不等式和 Cauchy–Schwarz 不等式证明的最小数值常数 $C$。\n- 一个用于精确带符号支撑集恢复的充分信号强度条件，表示为 $\\beta_{\\min}$ 关于 $\\lambda$ 和 $\\tau$ 的下界，该条件确保在事件 $\\mathcal{E}$ 上 $\\operatorname{supp}(\\hat{\\beta}) = S$ 且 $\\operatorname{sign}(\\hat{\\beta}_{S}) = \\operatorname{sign}(\\beta^{\\star}_{S})$。\n\n(3) 在 $\\lambda \\ge \\tau$ 的选择范围内优化第 (2) 部分的充分条件，以获得保证以至少 $1 - \\delta$ 的概率实现精确带符号支撑集恢复的 $\\beta_{\\min}$ 的最小显式下界。将这个最小 $\\beta_{\\min}$ 的最终答案表示为关于 $n$、$p$、$\\delta$ 和 $\\sigma$ 的单一闭式解析表达式。\n\n仅报告最小 $\\beta_{\\min}$ 的表达式作为最终答案。不需要数值近似或四舍五入，也不涉及单位。", "solution": "用户提供了来自高维统计领域的一个有效问题陈述。该问题具有科学依据，提法明确且客观。我们可以进行完整的推导。\n\n本解答根据问题陈述中指定的三个任务进行组织。\n\n**(1) 噪声水平阈值 $\\tau$ 的推导**\n\n问题定义了向量 $g = \\frac{1}{n} X^{\\top} w$，其中 $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。由于 $g$ 是高斯随机向量的线性变换，因此它也是高斯的。我们首先确定其均值和协方差。\n均值为 $E[g] = \\frac{1}{n} X^{\\top} E[w] = \\frac{1}{n} X^{\\top} 0 = 0$。\n协方差矩阵为：\n$$\n\\operatorname{Cov}(g) = E[g g^{\\top}] = E\\left[ \\left(\\frac{1}{n} X^{\\top} w\\right) \\left(\\frac{1}{n} X^{\\top} w\\right)^{\\top} \\right] = \\frac{1}{n^2} X^{\\top} E[w w^{\\top}] X\n$$\n由于 $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，其协方差为 $E[w w^{\\top}] = \\sigma^2 I_n$。代入此式以及给定的设计条件 $X^{\\top} X = n I_{p}$：\n$$\n\\operatorname{Cov}(g) = \\frac{1}{n^2} X^{\\top} (\\sigma^2 I_n) X = \\frac{\\sigma^2}{n^2} (X^{\\top} X) = \\frac{\\sigma^2}{n^2} (n I_p) = \\frac{\\sigma^2}{n} I_p.\n$$\n此结果表明，对于 $j=1, \\dots, p$，分量 $g_j$ 是独立同分布的，且 $g_j \\sim \\mathcal{N}(0, \\sigma^2/n)$。\n\n我们关心事件 $\\mathcal{E} = \\{\\|g\\|_{\\infty} \\le \\tau\\}$，其中 $\\|g\\|_{\\infty} = \\max_{j=1,\\dots,p} |g_j|$。我们希望找到最小的 $\\tau$ 使得 $P(\\mathcal{E}) \\ge 1 - \\delta$。这等价于确保其补事件 $\\mathcal{E}^c = \\{\\|g\\|_{\\infty}  \\tau\\}$ 的概率至多为 $\\delta$。\n补事件可以写成一个并集：$\\mathcal{E}^c = \\bigcup_{j=1}^{p} \\{|g_j|  \\tau\\}$。\n使用并集界，我们有：\n$$\nP(\\mathcal{E}^c) = P\\left(\\bigcup_{j=1}^{p} \\{|g_j|  \\tau\\}\\right) \\le \\sum_{j=1}^{p} P(|g_j|  \\tau) = p \\cdot P(|g_1|  \\tau),\n$$\n其中最后一个等式成立是因为 $g_j$ 是独立同分布的 (i.i.d.)。\n令 $Z$ 为一个标准正态随机变量，$Z \\sim \\mathcal{N}(0, 1)$。那么我们可以写出 $g_1 = \\frac{\\sigma}{\\sqrt{n}} Z$。概率变为：\n$$\nP(|g_1|  \\tau) = P\\left(\\left|\\frac{\\sigma}{\\sqrt{n}} Z\\right|  \\tau\\right) = P\\left(|Z|  \\frac{\\sqrt{n}\\tau}{\\sigma}\\right).\n$$\n对于 $t  0$，使用标准高斯尾部界 $P(|Z|  t) \\le 2 \\exp(-t^2/2)$，我们得到：\n$$\nP(\\mathcal{E}^c) \\le p \\cdot 2 \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\sqrt{n}\\tau}{\\sigma}\\right)^2\\right) = 2p \\exp\\left(-\\frac{n\\tau^2}{2\\sigma^2}\\right).\n$$\n为确保 $P(\\mathcal{E}^c) \\le \\delta$，我们可以将该界设为等于 $\\delta$ 并求解 $\\tau$：\n$$\n2p \\exp\\left(-\\frac{n\\tau^2}{2\\sigma^2}\\right) = \\delta\n$$\n$$\n\\exp\\left(-\\frac{n\\tau^2}{2\\sigma^2}\\right) = \\frac{\\delta}{2p}\n$$\n$$\n-\\frac{n\\tau^2}{2\\sigma^2} = \\ln\\left(\\frac{\\delta}{2p}\\right) = -\\ln\\left(\\frac{2p}{\\delta}\\right)\n$$\n$$\n\\tau^2 = \\frac{2\\sigma^2}{n} \\ln\\left(\\frac{2p}{\\delta}\\right)\n$$\n满足该不等式的最小 $\\tau$ 值为：\n$$\n\\tau = \\sigma \\sqrt{\\frac{2 \\ln(2p/\\delta)}{n}}.\n$$\n\n**(2) KKT 条件、软阈值化简化及其推论**\n\nLASSO 目标函数为 $L(\\beta) = \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$。Karush–Kuhn–Tucker (KKT) 最优性条件表明，向量 $\\hat{\\beta}$ 是一个最小化子当且仅当零向量位于 $L$ 在 $\\hat{\\beta}$ 处的次梯度中：\n$$\n0 \\in \\nabla_{\\beta} \\left(\\frac{1}{2n} \\|y - X \\hat{\\beta}\\|_{2}^{2}\\right) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}.\n$$\n二次项的梯度为 $\\frac{1}{n} X^{\\top}(X\\hat{\\beta} - y)$。代入 $y = X\\beta^{\\star} + w$：\n$$\n0 \\in \\frac{1}{n} X^{\\top}(X\\hat{\\beta} - X\\beta^{\\star} - w) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}\n$$\n$$\n\\frac{1}{n} X^{\\top}w \\in \\frac{1}{n} X^{\\top}X(\\hat{\\beta} - \\beta^{\\star}) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}.\n$$\n使用定义 $g = \\frac{1}{n} X^{\\top}w$ 和正交性条件 $X^{\\top}X = n I_p$，上式简化为：\n$$\ng \\in (\\hat{\\beta} - \\beta^{\\star}) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1} \\quad \\implies \\quad \\beta^{\\star} + g \\in \\hat{\\beta} + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}.\n$$\n这是问题 $\\min_{\\beta} \\frac{1}{2} \\|\\beta - (\\beta^{\\star} + g)\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$ 的 KKT 条件，其唯一解由分量形式的软阈值化算子 $S_{\\lambda}(\\cdot)$ 给出。因此，对于每个坐标 $j=1, \\dots, p$：\n$$\n\\hat{\\beta}_j = S_{\\lambda}(\\beta^{\\star}_j + g_j) \\equiv \\operatorname{sign}(\\beta^{\\star}_j + g_j) \\max(0, |\\beta^{\\star}_j + g_j| - \\lambda).\n$$\n这表明，在正交设计下，LASSO 估计量简化为对普通最小二乘估计量 $\\beta^{\\star}+g$ 的简单软阈值化（因为这里 $(X^\\top X)^{-1}X^\\top y = \\frac{1}{n}X^\\top(X\\beta^\\star+w) = \\beta^\\star+g$）。这种简化对任何 $\\lambda  0$ 都成立。\n\n**$\\ell_2$-估计不等式：**\n我们在事件 $\\mathcal{E}$ 上且对于 $\\lambda \\ge \\tau$ 分析估计误差 $\\Delta = \\hat{\\beta} - \\beta^{\\star}$。\n每个坐标的误差为 $\\Delta_j = \\hat{\\beta}_j - \\beta^{\\star}_j = S_{\\lambda}(\\beta^{\\star}_j + g_j) - \\beta^{\\star}_j$。\n对于 $j \\in S^c$（非支撑集），$\\beta^{\\star}_j = 0$。在事件 $\\mathcal{E}$ 上，我们有 $|g_j| \\le \\|g\\|_{\\infty} \\le \\tau$。由于我们选择了 $\\lambda \\ge \\tau$，我们有 $|g_j| \\le \\lambda$。软阈值化算子给出 $\\hat{\\beta}_j = S_{\\lambda}(g_j) = 0$。因此，对于所有 $j \\in S^c$，$\\Delta_j = 0 - 0 = 0$。\n因此，总平方误差为 $\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2}^2 = \\sum_{j \\in S} (\\hat{\\beta}_j - \\beta^{\\star}_j)^2$。\n对于任何 $z$，有一个已知性质是 $|S_{\\lambda}(z) - z| \\le \\lambda$。令 $u_j = \\beta^{\\star}_j + g_j$。那么 $\\hat{\\beta}_j = S_{\\lambda}(u_j)$，我们可以将误差写为 $\\Delta_j = S_{\\lambda}(u_j) - (u_j - g_j) = (S_{\\lambda}(u_j) - u_j) + g_j$。\n根据三角不等式，有 $|\\Delta_j| \\le |S_{\\lambda}(u_j) - u_j| + |g_j|$。\n在事件 $\\mathcal{E}$ 上，使用 $|S_{\\lambda}(u_j) - u_j| \\le \\lambda$ 和 $|g_j| \\le \\tau$，我们有：\n$|\\Delta_j| \\le \\lambda + \\tau$。\n在条件 $\\lambda \\ge \\tau$ 下，这意味着 $|\\Delta_j| \\le \\lambda + \\lambda = 2\\lambda$。\n对支撑集 $S$ 上的平方求和：\n$$\n\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2}^2 = \\sum_{j \\in S} \\Delta_j^2 \\le \\sum_{j \\in S} (2\\lambda)^2 = s(4\\lambda^2).\n$$\n取平方根得到 $\\ell_2$-估计不等式：\n$$\n\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2} \\le 2\\sqrt{s}\\lambda.\n$$\n从此推导中，我们可以证明的最小常数是 $C=2$。\n\n**精确带符号支撑集恢复的充分条件：**\n我们需要满足两个条件：\n(a) 对于所有 $j \\in S^c$，$\\hat{\\beta}_j = 0$。\n(b) 对于所有 $j \\in S$，$\\operatorname{sign}(\\hat{\\beta}_j) = \\operatorname{sign}(\\beta^{\\star}_j)$。\n\n条件 (a)：对于 $j \\in S^c$，$\\beta^{\\star}_j = 0$，所以 $\\hat{\\beta}_j = S_{\\lambda}(g_j)$。我们需要 $S_{\\lambda}(g_j) = 0$，这当且仅当 $|g_j| \\le \\lambda$ 时成立。在事件 $\\mathcal{E}$ 上，我们有 $|g_j| \\le \\tau$。因此，选择任何 $\\lambda \\ge \\tau$ 是条件 (a) 在 $\\mathcal{E}$ 上成立的充分条件。\n\n条件 (b)：对于 $j \\in S$，$\\beta^{\\star}_j \\ne 0$。我们需要 $\\operatorname{sign}(S_{\\lambda}(\\beta^{\\star}_j + g_j)) = \\operatorname{sign}(\\beta^{\\star}_j)$。如果 $\\operatorname{sign}(\\beta^{\\star}_j + g_j) = \\operatorname{sign}(\\beta^{\\star}_j)$ 且 $|\\beta^{\\star}_j + g_j|  \\lambda$，则此条件成立。\n一个对两者都成立的充分条件是 $|\\beta^{\\star}_j| - |g_j|  \\lambda$。这是因为如果这个不等式成立，则 $|g_j|  |\\beta^{\\star}_j|$，这确保了 $\\beta^{\\star}_j + g_j$ 与 $\\beta^{\\star}_j$ 同号。此外，根据反三角不等式，有 $|\\beta^{\\star}_j + g_j| \\ge |\\beta^{\\star}_j| - |g_j|  \\lambda$。\n为确保此条件在事件 $\\mathcal{E}$ 上对所有 $j \\in S$ 都成立，我们必须防范最坏情况的噪声配置，即 $|g_j|$ 达到最大值，即 $|g_j| = \\tau$。该条件变为 $|\\beta^{\\star}_j| - \\tau  \\lambda$，或 $|\\beta^{\\star}_j|  \\lambda + \\tau$。\n这个条件必须对所有 $j \\in S$ 成立。最弱的此类条件适用于支撑集上最小的系数：$\\min_{j \\in S} |\\beta^{\\star}_j|  \\lambda + \\tau$。\n这恰好是 $\\beta_{\\min}  \\lambda + \\tau$。\n\n总而言之，在事件 $\\mathcal{E}$ 上，条件 $\\lambda \\ge \\tau$ 和 $\\beta_{\\min}  \\lambda + \\tau$ 是精确带符号支撑集恢复的充分条件。\n\n**(3) 支撑集恢复的最小信号强度**\n\n我们寻求 $\\beta_{\\min}$ 的最小下界，以保证以至少 $1-\\delta$ 的概率实现精确带符号支撑集恢复。这发生在事件 $\\mathcal{E}$ 上。推导出的充分条件是 $\\beta_{\\min}  \\lambda + \\tau$，并且它要求我们选择一个调节参数 $\\lambda$ 使得 $\\lambda \\ge \\tau$。\n我们可以自由选择 $\\lambda$。为了得到对 $\\beta_{\\min}$ 最不严格的条件（即最小的下界），我们应该在约束条件 $\\lambda \\ge \\tau$ 下，关于 $\\lambda$ 最小化量 $\\lambda+\\tau$。\n函数 $f(\\lambda) = \\lambda + \\tau$ 在 $\\lambda$ 上是单调递增的。因此，它在定义域 $[\\tau, \\infty)$ 上的最小值在下边界 $\\lambda = \\tau$ 处取得。\n将 $\\lambda = \\tau$ 代入关于 $\\beta_{\\min}$ 的条件，我们得到优化后的充分条件：\n$$\n\\beta_{\\min}  \\tau + \\tau = 2\\tau.\n$$\n因此，$\\beta_{\\min}$ 的最小显式下界是 $2\\tau$。代入第 (1) 部分中 $\\tau$ 的表达式，得到最终答案：\n$$\n\\beta_{\\min}  2 \\sigma \\sqrt{\\frac{2 \\ln(2p/\\delta)}{n}}.\n$$\n该下界的最小值即为右侧的表达式。", "answer": "$$\n\\boxed{2\\sigma \\sqrt{\\frac{2 \\ln\\left(\\frac{2p}{\\delta}\\right)}{n}}}\n$$", "id": "3464166"}, {"introduction": "我们之前对支撑集恢复的分析依赖于简化的正交设计矩阵假设。为了将神谕不等式推广到更普遍的相关设计矩阵，我们必须量化矩阵 $X$ 的几何特性。本练习介绍“相容性常数”（compatibility constant），这是分析 LASSO 在现实情境下性能的一个关键概念，并要求您在一个具体例子中计算它，以揭开这个抽象但强大的理论工具的神秘面纱 [@problem_id:3464165]。", "problem": "考虑线性模型 $y = X \\beta^{\\star} + \\varepsilon$，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中 $X$ 的列经过标准化，使得 $\\frac{1}{n} X^{\\top} X = \\Sigma$。在最小绝对收缩和选择算子 (LASSO) 的研究中，神谕不等式涉及由一个支撑集 $S \\subseteq \\{1, \\dots, p\\}$ 和一个锥约束所编码的几何结构。设 $p = 3$，并假设经验格拉姆矩阵满足\n$$\n\\Sigma = \\begin{pmatrix}\n1  \\rho  0 \\\\\n\\rho  1  0 \\\\\n0  0  1\n\\end{pmatrix},\n$$\n其中相关参数 $0 \\leq \\rho  1$，并考虑支撑集 $S = \\{1, 2\\}$。通过不等式 $\\|v_{S^{c}}\\|_{1} \\leq L \\|v_{S}\\|_{1}$ 定义锥 $C(S)$，其中 $L = 1$，$v \\in \\mathbb{R}^{p}$ 且 $S^{c}$ 表示 $S$ 的补集。与 $S$ 和锥 $C(S)$ 相关联的相容性常数是\n$$\n\\phi_{\\mathrm{comp}}(S) := \\inf_{v \\in C(S) \\setminus \\{0\\}} \\frac{\\sqrt{|S|} \\, \\sqrt{v^{\\top} \\Sigma v}}{\\|v_{S}\\|_{1}}。\n$$\n请确定 $\\phi_{\\mathrm{comp}}(S)$ 作为一个用 $\\rho$ 表示的精确闭式表达式。将您的最终答案表示为单个简化的解析表达式。", "solution": "问题要求计算给定支撑集 $S$、格拉姆矩阵 $\\Sigma$ 和锥约束下的相容性常数 $\\phi_{\\mathrm{comp}}(S)$。\n\n相容性常数的定义为：\n$$\n\\phi_{\\mathrm{comp}}(S) := \\inf_{v \\in C(S) \\setminus \\{0\\}} \\frac{\\sqrt{|S|} \\, \\sqrt{v^{\\top} \\Sigma v}}{\\|v_{S}\\|_{1}}\n$$\n已知条件如下：\n- 维度 $p=3$。\n- 支撑集 $S = \\{1, 2\\}$，所以其基数为 $|S|=2$。其补集为 $S^c = \\{3\\}$。\n- 对于一个向量 $v = (v_1, v_2, v_3)^{\\top} \\in \\mathbb{R}^3$，在支撑集上的子向量是 $v_S = (v_1, v_2)^{\\top}$，在其补集上是 $v_{S^c} = (v_3)$。\n- 相关的范数是 $\\ell_1$ 范数：$\\|v_S\\|_1 = |v_1| + |v_2|$ 和 $\\|v_{S^c}\\|_1 = |v_3|$。\n- 锥 $C(S)$ 由不等式 $\\|v_{S^c}\\|_1 \\leq L \\|v_S\\|_1$ 定义，其中 $L=1$，该不等式简化为 $|v_3| \\leq |v_1| + |v_2|$。\n- 经验格拉姆矩阵为 $\\Sigma = \\begin{pmatrix} 1  \\rho  0 \\\\ \\rho  1  0 \\\\ 0  0  1 \\end{pmatrix}$，其中 $0 \\leq \\rho  1$。\n\n让我们展开二次型 $v^{\\top} \\Sigma v$：\n$$\nv^{\\top} \\Sigma v = \\begin{pmatrix} v_1  v_2  v_3 \\end{pmatrix} \\begin{pmatrix} 1  \\rho  0 \\\\ \\rho  1  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2\n$$\n将已知值代入相容性常数的定义中，我们得到：\n$$\n\\phi_{\\mathrm{comp}}(S) = \\inf_{v \\in C(S) \\setminus \\{0\\}} \\frac{\\sqrt{2} \\sqrt{v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2}}{|v_1| + |v_2|}\n$$\n待最小化的表达式关于 $v$ 是零次齐次的。也就是说，对于任何标量 $c \\neq 0$，如果我们将 $v$ 替换为 $cv$，表达式的值保持不变。这使我们可以通过对 $v$ 施加一个归一化约束来简化问题。一个方便的选择是将搜索约束在满足 $\\|v_S\\|_1 = |v_1| + |v_2| = 1$ 的向量 $v$ 上。在 $v \\in C(S) \\setminus \\{0\\}$ 上的下确界与在 $\\|v_S\\|_1 = 1$ 的子集上的下确界相同。\n\n有了这个约束，问题就变成了求 $\\phi_{\\mathrm{comp}}(S) = \\inf \\sqrt{2(v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2)}$，需满足以下条件：\n1. $|v_1| + |v_2| = 1$\n2. $|v_3| \\leq |v_1| + |v_2|$，即 $|v_3| \\leq 1$。\n\n求该函数的下确界等价于求其平方的下确界。让我们求 $\\phi_{\\mathrm{comp}}(S)^2$ 的最小值：\n$$\n\\phi_{\\mathrm{comp}}(S)^2 = \\inf \\left\\{ 2(v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2) \\right\\}\n$$\n需满足约束条件 $|v_1|+|v_2|=1$ 和 $|v_3| \\leq 1$。\n\n为了最小化该表达式，我们应该选择 $v_3$ 使 $v_3^2$ 项尽可能小。由于 $v_3^2 \\ge 0$，其最小值为 0，在 $v_3=0$ 时达到。这个选择满足约束 $|v_3| \\leq 1$。\n因此，问题简化为一个低维优化问题：\n$$\n\\phi_{\\mathrm{comp}}(S)^2 = \\inf_{|v_1|+|v_2|=1} \\left\\{ 2(v_1^2 + v_2^2 + 2\\rho v_1 v_2) \\right\\}\n$$\n我们来分析二次型 $f(v_1, v_2) = v_1^2 + v_2^2 + 2\\rho v_1 v_2$。这可以写成矩阵形式 $v_S^{\\top} \\Sigma_S v_S$，其中 $\\Sigma_S$ 是 $\\Sigma$ 中对应于 $S$ 中索引的主子矩阵：\n$$\n\\Sigma_S = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\n约束 $|v_1| + |v_2| = 1$ 在 $(v_1,v_2)$ 平面上定义了一个以 $(1,0), (0,1), (-1,0), (0,-1)$ 为顶点的正方形。\n\n为了求出 $v_S^{\\top} \\Sigma_S v_S$ 在这个正方形上的最小值，我们可以对 $\\Sigma_S$ 进行对角化。$\\Sigma_S$ 的特征值 $\\lambda$ 由特征方程 $(1-\\lambda)^2 - \\rho^2 = 0$ 给出，解得 $1-\\lambda = \\pm\\rho$，所以 $\\lambda_1 = 1+\\rho$ 和 $\\lambda_2 = 1-\\rho$。相应的归一化特征向量是：\n- 对于 $\\lambda_1 = 1+\\rho$：$e_1 = \\frac{1}{\\sqrt{2}}(1, 1)^{\\top}$\n- 对于 $\\lambda_2 = 1-\\rho$：$e_2 = \\frac{1}{\\sqrt{2}}(1, -1)^{\\top}$\n\n任何向量 $v_S = (v_1, v_2)^{\\top}$ 都可以用这个特征基表示为 $v_S = c_1 e_1 + c_2 e_2$。二次型变为：\n$$\nv_S^{\\top} \\Sigma_S v_S = (c_1 e_1 + c_2 e_2)^{\\top} \\Sigma_S (c_1 e_1 + c_2 e_2) = c_1^2 \\lambda_1 + c_2^2 \\lambda_2 = c_1^2 (1+\\rho) + c_2^2 (1-\\rho)\n$$\n我们需要用 $c_1$ 和 $c_2$ 来表示约束 $\\|v_S\\|_1 = 1$。\n由 $v_S = c_1 e_1 + c_2 e_2$ 可得：\n$v_1 = \\frac{c_1}{\\sqrt{2}} + \\frac{c_2}{\\sqrt{2}}$ 且 $v_2 = \\frac{c_1}{\\sqrt{2}} - \\frac{c_2}{\\sqrt{2}}$。\n约束变为：\n$$\n|\\frac{c_1}{\\sqrt{2}} + \\frac{c_2}{\\sqrt{2}}| + |\\frac{c_1}{\\sqrt{2}} - \\frac{c_2}{\\sqrt{2}}| = 1\n$$\n使用恒等式 $|a+b| + |a-b| = 2\\max(|a|,|b|)$，其中 $a=c_1/\\sqrt{2}$ 和 $b=c_2/\\sqrt{2}$，我们得到：\n$$\n\\frac{1}{\\sqrt{2}} ( |c_1+c_2| + |c_1-c_2| ) = \\frac{2}{\\sqrt{2}} \\max(|c_1|, |c_2|) = \\sqrt{2} \\max(|c_1|, |c_2|) = 1\n$$\n所以，对 $c_1, c_2$ 的约束是 $\\max(|c_1|, |c_2|) = \\frac{1}{\\sqrt{2}}$。这在 $(c_1, c_2)$ 平面上描述了一个正方形。\n\n我们的问题现在是最小化 $g(c_1, c_2) = c_1^2(1+\\rho) + c_2^2(1-\\rho)$，约束条件为 $\\max(|c_1|, |c_2|) = \\frac{1}{\\sqrt{2}}$。\n由于已知 $0 \\leq \\rho  1$，系数 $(1+\\rho)$ 和 $(1-\\rho)$ 均为正。此外，$(1+\\rho) \\geq (1-\\rho)$。\n为了最小化 $g(c_1, c_2)$，我们应该使有较大系数的项 $c_1^2(1+\\rho)$ 尽可能小，而有较小系数的项 $c_2^2(1-\\rho)$ 可能较大。\n在约束集上， $|c_1|$ 的最小值为 $0$。这迫使 $|c_2|$ 必须为 $\\frac{1}{\\sqrt{2}}$ 以满足 $\\max(|c_1|, |c_2|) = \\frac{1}{\\sqrt{2}}$。\n因此，在 $(c_1, c_2) = (0, \\pm \\frac{1}{\\sqrt{2}})$ 时达到最小值。\n\n二次型 $v_S^{\\top} \\Sigma_S v_S$ 的最小值为：\n$$\n\\min (v_S^{\\top} \\Sigma_S v_S) = 0^2(1+\\rho) + \\left(\\pm \\frac{1}{\\sqrt{2}}\\right)^2 (1-\\rho) = \\frac{1}{2}(1-\\rho)\n$$\n现在，我们将其代回 $\\phi_{\\mathrm{comp}}(S)^2$ 的表达式中：\n$$\n\\phi_{\\mathrm{comp}}(S)^2 = 2 \\times \\min_{|v_1|+|v_2|=1} (v_1^2 + v_2^2 + 2\\rho v_1 v_2) = 2 \\times \\frac{1-\\rho}{2} = 1-\\rho\n$$\n取平方根得到相容性常数：\n$$\n\\phi_{\\mathrm{comp}}(S) = \\sqrt{1-\\rho}\n$$\n对于给定的 $\\rho$ 范围，$1-\\rho > 0$，所以此表达式有明确定义。", "answer": "$$\n\\boxed{\\sqrt{1-\\rho}}\n$$", "id": "3464165"}]}