{"hands_on_practices": [{"introduction": "在深入研究循环坐标下降的动态过程之前，理解其根本目标至关重要：即找到满足 Karush-Kuhn-Tucker (KKT) 最优性条件的解。本练习 [@problem_id:3442175] 通过一个具体的数值实例，让您亲手检验一个候选解是否满足 KKT 条件。通过计算梯度和次梯度，您可以确定哪些坐标尚未达到最优，从而直观地理解坐标下降法为何以及如何对特定坐标进行更新，这构成了从理论到算法实践的关键一步。", "problem": "考虑最小绝对值收敛和选择算子 (LASSO) 问题，该问题旨在最小化目标函数\n$$\nF(\\beta) \\;=\\; \\frac{1}{2}\\,\\|y - X\\beta\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个设计矩阵，$y \\in \\mathbb{R}^{n}$ 是一个响应向量，$\\beta \\in \\mathbb{R}^{p}$ 是参数向量，$\\lambda > 0$ 是一个正则化参数。$\\ell_{1}$ 范数的次梯度按分量定义为 $s_{j} \\in \\partial |\\beta_{j}|$，其中如果 $\\beta_{j} \\neq 0$，则 $s_{j} = \\operatorname{sign}(\\beta_{j})$；如果 $\\beta_{j} = 0$，则 $s_{j} \\in [-1,1]$。凸复合优化的 Karush–Kuhn–Tucker (KKT) 条件要求光滑项的梯度与非光滑项的次梯度之和的平稳性，以及次梯度的可行性。\n\n根据这些核心定义，为一个小型的人工合成实例数值上验证 KKT 平稳性条件。设\n$$\nX \\;=\\; \\begin{pmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n1  1  0\n\\end{pmatrix}, \n\\qquad\ny \\;=\\; \\begin{pmatrix}\n3 \\\\ 0 \\\\ 1\n\\end{pmatrix},\n\\qquad\n\\lambda \\;=\\; 1,\n\\qquad\n\\hat{\\beta} \\;=\\; \\begin{pmatrix}\n1 \\\\ 0 \\\\ \\tfrac{1}{2}\n\\end{pmatrix}.\n$$\n仅使用上述基本定义，完成以下任务。第一，推导针对此 LASSO 问题的 KKT 平稳性条件：用在 $\\hat{\\beta}$ 处求值的二次损失的梯度和 $\\ell_{1}$ 范数的次梯度来表示它。第二，计算残差向量 $r = X\\hat{\\beta} - y$、梯度 $g = X^{\\top} r$，以及一个次梯度向量 $s \\in \\partial \\|\\hat{\\beta}\\|_{1}$，该向量在满足次梯度可行性约束的条件下，逐分量地最小化 KKT 平稳性残差。第三，数值上验证 KKT 平稳性条件在 $\\hat{\\beta}$ 处是否成立，并根据在循环坐标下降法中将会执行的坐标最小化步骤来解释任何违背之处。特别地，将任何违背条件的坐标与其从第一性原理推导出的单步坐标更新联系起来。\n\n最后，令 $t = g + \\lambda s$ 表示在 $\\hat{\\beta}$ 处，对于你最优选择的 $s$ 的 KKT 平稳性残差向量。报告无穷范数 $\\|t\\|_{\\infty}$ 的值作为你的最终答案。请以精确分数形式提供最终答案（不要四舍五入）。", "solution": "LASSO 目标函数由 $F(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda\\|\\beta\\|_{1}$ 给出。Karush–Kuhn–Tucker (KKT) 条件为点 $\\hat{\\beta}$ 成为最小化点提供了充分必要条件。平稳性条件要求零向量是 $F(\\beta)$ 在 $\\hat{\\beta}$ 处的次微分的元素。目标函数是一个可微项 $L(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$ 和一个不可微凸项 $R(\\beta) = \\lambda\\|\\beta\\|_{1}$ 的和。和的次微分是可微部分的梯度与不可微部分的次微分之和。\n\n首先，我们推导一般的 KKT 平稳性条件。最小二乘损失项 $L(\\beta)$ 的梯度是 $\\nabla L(\\beta) = X^{\\top}(X\\beta - y)$。$\\ell_{1}$ 范数项 $R(\\beta)$ 的次微分是 $\\partial R(\\beta) = \\lambda \\, \\partial \\|\\beta\\|_{1}$。平稳性条件是 $0 \\in \\nabla L(\\hat{\\beta}) + \\partial R(\\hat{\\beta})$，可以写作：\n$$\n-\\nabla L(\\hat{\\beta}) \\in \\partial R(\\hat{\\beta})\n$$\n或者等价地，必须存在一个次梯度向量 $s \\in \\partial \\|\\hat{\\beta}\\|_{1}$ 使得\n$$\n\\nabla L(\\hat{\\beta}) + \\lambda s = 0.\n$$\n按分量来看，对于每个 $j \\in \\{1, \\dots, p\\}$，我们必须有 $(\\nabla L(\\hat{\\beta}))_j + \\lambda s_j = 0$，其中如果 $\\hat{\\beta}_j \\neq 0$，则 $s_j = \\operatorname{sign}(\\hat{\\beta}_j)$；如果 $\\hat{\\beta}_j = 0$，则 $s_j \\in [-1, 1]$。\n\n其次，我们为给定的数值实例计算必要的量。\n数据如下：\n$$\nX = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  0 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 1, \\quad \\hat{\\beta} = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\tfrac{1}{2} \\end{pmatrix}.\n$$\n我们首先计算预测响应 $X\\hat{\\beta}$：\n$$\nX\\hat{\\beta} = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 0 + 2 \\cdot \\frac{1}{2} \\\\ 0 \\cdot 1 + 1 \\cdot 0 + (-1) \\cdot \\frac{1}{2} \\\\ 1 \\cdot 1 + 1 \\cdot 0 + 0 \\cdot \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix}.\n$$\n接下来，我们计算残差向量 $r = X\\hat{\\beta} - y$：\n$$\nr = \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix}.\n$$\n现在，我们计算损失项的梯度，$g = \\nabla L(\\hat{\\beta}) = X^{\\top}r$：\n$$\ng = X^{\\top}r = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 2  -1  0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(-1) + 0(-\\frac{1}{2}) + 1(0) \\\\ 0(-1) + 1(-\\frac{1}{2}) + 1(0) \\\\ 2(-1) + (-1)(-\\frac{1}{2}) + 0(0) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix}.\n$$\n第三，我们必须找到一个次梯度向量 $s \\in \\partial \\|\\hat{\\beta}\\|_{1}$，以最小化 KKT 平稳性残差 $t = g + \\lambda s$。$\\hat{\\beta}$ 的分量是 $\\hat{\\beta}_1 = 1$，$\\hat{\\beta}_2 = 0$ 和 $\\hat{\\beta}_3 = \\frac{1}{2}$。\n对于 $\\hat{\\beta}_1 = 1 \\neq 0$，次梯度分量 $s_1$ 唯一确定为 $s_1 = \\operatorname{sign}(1) = 1$。\n对于 $\\hat{\\beta}_3 = \\frac{1}{2} \\neq 0$，次梯度分量 $s_3$ 唯一确定为 $s_3 = \\operatorname{sign}(\\frac{1}{2}) = 1$。\n对于 $\\hat{\\beta}_2 = 0$，次梯度分量 $s_2$ 可以是区间 $[-1, 1]$ 中的任意值。为了最小化 KKT 残差，我们选择 $s_2$ 使 $t = g + \\lambda s$ 的第二个分量尽可能接近零。我们想要最小化 $|t_2| = |g_2 + \\lambda s_2| = |-\\frac{1}{2} + 1 \\cdot s_2|$。当 $s_2 = \\frac{1}{2}$ 时，该值最小。由于 $\\frac{1}{2} \\in [-1, 1]$，这是一个有效的选择。\n因此，最优选择的次梯度向量是 $s = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix}$。\n\n使用这个 $s$，KKT 平稳性残差向量 $t$ 是：\n$$\nt = g + \\lambda s = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1+1 \\\\ -\\frac{1}{2}+\\frac{1}{2} \\\\ -\\frac{3}{2}+1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -\\frac{1}{2} \\end{pmatrix}.\n$$\nKKT 平稳性条件 $t=0$ 没有被满足，因为 $t_3 = -\\frac{1}{2} \\neq 0$。因此，$\\hat{\\beta}$ 不是该 LASSO 问题的最优解。\n\n违背发生在第三个坐标上。这意味着如果我们执行循环坐标下降，$\\beta_3$ 的值将从其当前值 $\\frac{1}{2}$ 更新。为了验证这一点，我们找到在保持 $\\beta_1=1$ 和 $\\beta_2=0$ 固定的情况下，最小化目标函数的 $\\beta_3$ 的值。将目标函数看作关于 $\\beta_3$ 的函数，它是：\n$$\nF_3(\\beta_3) = \\frac{1}{2}\\left\\| y - X_{\\cdot 1}\\hat{\\beta}_1 - X_{\\cdot 2}\\hat{\\beta}_2 - X_{\\cdot 3}\\beta_3 \\right\\|_2^2 + \\lambda|\\hat{\\beta}_1| + \\lambda|\\hat{\\beta}_2| + \\lambda|\\beta_3|\n$$\n最小化它等价于解决一维 LASSO 问题：\n$$\n\\min_{\\beta_3} \\frac{1}{2}\\left\\| (y - X_{\\cdot 1}\\hat{\\beta}_1 - X_{\\cdot 2}\\hat{\\beta}_2) - X_{\\cdot 3}\\beta_3 \\right\\|_2^2 + \\lambda|\\beta_3|\n$$\n残差化响应是 $y' = y - X_{\\cdot 1}(1) - X_{\\cdot 2}(0) = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n预测变量是 $X_{\\cdot 3} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$。其范数的平方是 $\\|X_{\\cdot 3}\\|_2^2 = 2^2 + (-1)^2 + 0^2 = 5$。\n坐标下降的更新由软阈值算子给出：\n$$\n\\beta_3^{\\text{new}} = S_{\\lambda/\\|X_{\\cdot 3}\\|_2^2}\\left(\\frac{X_{\\cdot 3}^{\\top}y'}{\\|X_{\\cdot 3}\\|_2^2}\\right)\n$$\n我们计算 $X_{\\cdot 3}^{\\top}y' = \\begin{pmatrix} 2  -1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = 4$。\n当 $\\lambda=1$ 时，更新为：\n$$\n\\beta_3^{\\text{new}} = S_{1/5}\\left(\\frac{4}{5}\\right) = \\operatorname{sign}\\left(\\frac{4}{5}\\right) \\max\\left(\\left|\\frac{4}{5}\\right| - \\frac{1}{5}, 0\\right) = 1 \\cdot \\left(\\frac{4}{5} - \\frac{1}{5}\\right) = \\frac{3}{5}.\n$$\n由于更新后的值 $\\beta_3^{\\text{new}} = \\frac{3}{5}$ 与当前值 $\\hat{\\beta}_3 = \\frac{1}{2}$ 不同，这证实了第三个坐标不是最优的。非零的 KKT 残差 $t_3 = -\\frac{1}{2}$ 正确地指出了这种次优性。\n\n最后，我们报告 KKT 平稳性残差向量 $t$ 的无穷范数：\n$$\n\\|t\\|_{\\infty} = \\max(|t_1|, |t_2|, |t_3|) = \\max\\left(|0|, |0|, \\left|-\\frac{1}{2}\\right|\\right) = \\frac{1}{2}.\n$$", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "3442175"}, {"introduction": "循环坐标下降算法中的“循环”一词并非无足轻重，更新坐标的顺序会对算法的收敛行为产生实际影响。本练习 [@problem_id:3442192] 旨在通过一个精心设计的二维例子，揭示这一特性。您将通过手动计算，比较两种不同的循环更新顺序，并观察它们达到相同收敛精度所需的迭代次数差异，从而深刻体会到坐标更新顺序对收敛速度的影响。", "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题，其目标函数为\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1,\n$$\n并假设我们使用循环坐标下降法，该方法定义为在保持其他坐标固定的情况下，按照指定的循环顺序，每次对一个坐标进行精确最小化。Karush–Kuhn–Tucker (KKT) 容差由坐标级别的最大 KKT 残差衡量\n$$\nV(\\beta) \\equiv \\max_{1 \\leq j \\leq p} v_j(\\beta),\n$$\n其中，记 $r \\equiv y - X\\beta$，\n$$\nv_j(\\beta) \\equiv \n\\begin{cases}\n\\left|x_j^{\\top} r - \\lambda \\,\\mathrm{sign}(\\beta_j)\\right|,  \\text{若 } \\beta_j \\neq 0,\\\\[4pt]\n\\max\\!\\big(0, \\, |x_j^{\\top} r| - \\lambda\\big),  \\text{若 } \\beta_j = 0,\n\\end{cases}\n$$\n且 $x_j$ 表示 $X$ 的第 $j$ 列。一次迭代定义为按给定的循环顺序对所有坐标进行一次完整遍历。从 $\\beta^{(0)} = (0, 0)^{\\top}$ 开始。\n\n使用以下特定数据\n$$\nX \\;=\\; \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}, \\qquad y \\;=\\; \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\qquad \\lambda \\;=\\; 1,\n$$\n因此 $n = 2$ 且 $p = 2$。考虑两种循环顺序：\n- 顺序 $\\mathcal{A}$：在每次遍历中先更新坐标 $1$，然后更新坐标 $2$。\n- 顺序 $\\mathcal{B}$：在每次遍历中先更新坐标 $2$，然后更新坐标 $1$。\n\n仅使用 LASSO 目标函数的定义和必要的最优性条件，分析每种顺序下的循环坐标下降。在每次内部更新中，给定其他坐标的当前值，对所选坐标精确最小化目标函数。定义停止准则为 KKT 容差达到或低于固定阈值\n$$\n\\tau \\;=\\; 0.2.\n$$\n\n确定在每种顺序下，达到 $V(\\beta) \\leq \\tau$ 所需的完整迭代次数（完整遍历次数），并报告其差值\n$$\nN_{\\mathcal{B}} - N_{\\mathcal{A}}.\n$$\n将你的最终答案以单个实数值的形式给出。无需四舍五入。", "solution": "问题要求计算在两种不同更新顺序下，循环坐标下降法解决 LASSO 问题收敛所需的迭代次数之差。停止准则是 Karush-Kuhn-Tucker (KKT) 容差 $V(\\beta)$ 小于或等于阈值 $\\tau = 0.2$。\n\nLASSO 目标函数由下式给出：\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\n$$\n其中 $\\beta \\in \\mathbb{R}^p$。给定 $p=2$ 和数据：\n$$\nX = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad \\lambda = 1\n$$\n$X$ 的列向量为 $x_1 = (1, 0)^{\\top}$ 和 $x_2 = (1, 1)^{\\top}$。初始估计为 $\\beta^{(0)} = (0, 0)^{\\top}$。\n\n首先，我们推导通用的坐标更新规则。为了更新坐标 $\\beta_j$，我们固定所有其他坐标 $\\beta_k$（$k \\ne j$）。目标函数作为 $\\beta_j$ 的函数是：\n$$\nL(\\beta_j) = \\frac{1}{2} \\|y - \\sum_{k \\ne j} x_k \\beta_k - x_j \\beta_j\\|_2^2 + \\lambda |\\beta_j| + C\n$$\n其中 $C$ 是一个关于 $\\beta_j$ 的常数。令 $r^{(j)} = y - \\sum_{k \\ne j} x_k \\beta_k$ 为部分残差。目标函数简化为最小化 $\\frac{1}{2} \\|r^{(j)} - x_j \\beta_j\\|_2^2 + \\lambda |\\beta_j|$。展开二次项并对 $\\beta_j$ 求次梯度，即可得到最优性条件。解通过软阈值法得到：\n$$\n\\beta_j \\leftarrow \\frac{1}{x_j^{\\top}x_j} S_{\\lambda}\\left(x_j^{\\top} r^{(j)}\\right)\n$$\n其中 $S_{\\alpha}(z) = \\mathrm{sign}(z) \\max(|z| - \\alpha, 0)$。\n\n让我们预先计算一些值：\n$x_1^{\\top}x_1 = 1^2 + 0^2 = 1$。\n$x_2^{\\top}x_2 = 1^2 + 1^2 = 2$。\n$x_1^{\\top}y = 1 \\cdot 2 + 0 \\cdot 0 = 2$。\n$x_2^{\\top}y = 1 \\cdot 2 + 1 \\cdot 0 = 2$。\n\nKKT 容差为 $V(\\beta) = \\max_j v_j(\\beta)$，其中 $r = y - X\\beta$ 且\n$$\nv_j(\\beta) = \n\\begin{cases}\n\\left|x_j^{\\top} r - \\lambda \\,\\mathrm{sign}(\\beta_j)\\right|,  \\text{若 } \\beta_j \\neq 0 \\\\\n\\max\\!\\big(0, \\, |x_j^{\\top} r| - \\lambda\\big),  \\text{若 } \\beta_j = 0\n\\end{cases}\n$$\n\n### 顺序 $\\mathcal{A}$ 的分析：先更新 $\\beta_1$ 后更新 $\\beta_2$\n\n我们从 $\\beta^{(0)} = (0, 0)^{\\top}$ 开始。一次迭代即一次完整遍历。\n\n**迭代 1 (顺序 $\\mathcal{A}$):**\n1.  **更新 $\\beta_1$**：我们固定 $\\beta_2 = 0$。部分残差为 $r^{(1)} = y - x_2 \\beta_2 = y = (2, 0)^{\\top}$。\n    $$\n    \\beta_1 \\leftarrow \\frac{1}{x_1^{\\top}x_1} S_{\\lambda}(x_1^{\\top} y) = \\frac{1}{1} S_1(2) = \\mathrm{sign}(2) \\max(|2|-1, 0) = 1\n    $$\n    状态变为 $(\\beta_1, \\beta_2) = (1, 0)^{\\top}$。\n\n2.  **更新 $\\beta_2$**：我们固定 $\\beta_1 = 1$。部分残差为 $r^{(2)} = y - x_1 \\beta_1 = (2, 0)^{\\top} - (1, 0)^{\\top} \\cdot 1 = (1, 0)^{\\top}$。\n    $$\n    x_2^{\\top} r^{(2)} = (1, 1)^{\\top} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1\n    $$\n    $$\n    \\beta_2 \\leftarrow \\frac{1}{x_2^{\\top}x_2} S_{\\lambda}(x_2^{\\top} r^{(2)}) = \\frac{1}{2} S_1(1) = \\frac{1}{2} \\mathrm{sign}(1) \\max(|1|-1, 0) = 0\n    $$\n    状态保持为 $(1, 0)^{\\top}$。\n\n迭代 1 结束：$\\beta^{(1)} = (1, 0)^{\\top}$。\n\n**检查 $\\beta^{(1)}$ 的停止准则：**\n完整残差为 $r = y - X\\beta^{(1)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n-   对于 $j=1$：$\\beta_1 = 1 \\neq 0$。\n    $x_1^{\\top}r = (1, 0)^{\\top} (1, 0)^{\\top} = 1$。\n    $v_1(\\beta^{(1)}) = |x_1^{\\top}r - \\lambda \\mathrm{sign}(\\beta_1)| = |1 - 1 \\cdot 1| = 0$。\n-   对于 $j=2$：$\\beta_2 = 0$。\n    $x_2^{\\top}r = (1, 1)^{\\top} (1, 0)^{\\top} = 1$。\n    $v_2(\\beta^{(1)}) = \\max(0, |x_2^{\\top}r| - \\lambda) = \\max(0, |1| - 1) = 0$。\n\nKKT 容差为 $V(\\beta^{(1)}) = \\max(0, 0) = 0$。由于 $0 \\le \\tau = 0.2$，算法停止。\n顺序 $\\mathcal{A}$ 所需的完整迭代次数为 $N_{\\mathcal{A}} = 1$。\n\n### 顺序 $\\mathcal{B}$ 的分析：先更新 $\\beta_2$ 后更新 $\\beta_1$\n\n我们从 $\\beta^{(0)} = (0, 0)^{\\top}$ 开始。\n\n**迭代 1 (顺序 $\\mathcal{B}$):**\n1.  **更新 $\\beta_2$**：我们固定 $\\beta_1 = 0$。部分残差为 $r^{(2)} = y - x_1 \\beta_1 = y = (2, 0)^{\\top}$。\n    $$\n    \\beta_2 \\leftarrow \\frac{1}{x_2^{\\top}x_2} S_{\\lambda}(x_2^{\\top} y) = \\frac{1}{2} S_1(2) = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}\n    $$\n    状态变为 $(0, 1/2)^{\\top}$。\n2.  **更新 $\\beta_1$**：我们固定 $\\beta_2 = 1/2$。部分残差为 $r^{(1)} = y - x_2 \\beta_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\frac{1}{2} = \\begin{pmatrix} 3/2 \\\\ -1/2 \\end{pmatrix}$。\n    $$\n    x_1^{\\top} r^{(1)} = (1, 0)^{\\top} (3/2, -1/2)^{\\top} = 3/2\n    $$\n    $$\n    \\beta_1 \\leftarrow \\frac{1}{x_1^{\\top}x_1} S_{\\lambda}(x_1^{\\top} r^{(1)}) = \\frac{1}{1} S_1(3/2) = \\frac{1}{2}\n    $$\n迭代 1 结束：$\\beta^{(1)} = (1/2, 1/2)^{\\top}$。\n检查 KKT：$r = y - X\\beta^{(1)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1/2 \\end{pmatrix}$。\n$v_1(\\beta^{(1)}) = |x_1^{\\top}r - \\lambda \\mathrm{sign}(\\beta_1)| = |1 - 1\\cdot 1| = 0$。\n$v_2(\\beta^{(1)}) = |x_2^{\\top}r - \\lambda \\mathrm{sign}(\\beta_2)| = |(1,1)^{\\top}(1, -1/2)^{\\top} - 1| = |1/2-1| = 1/2 = 0.5$。\n$V(\\beta^{(1)}) = 0.5 > 0.2$。我们继续。\n\n**迭代 2 (顺序 $\\mathcal{B}$):**\n从 $\\beta = (1/2, 1/2)^{\\top}$ 开始。\n1.  **更新 $\\beta_2$**：固定 $\\beta_1=1/2$。$r^{(2)} = y-x_1(1/2) = (3/2, 0)^\\top$。\n    $x_2^\\top r^{(2)} = 3/2$。 $\\beta_2 \\leftarrow \\frac{1}{2}S_1(3/2) = \\frac{1}{4}$。 状态：$(1/2, 1/4)^{\\top}$。\n2.  **更新 $\\beta_1$**：固定 $\\beta_2=1/4$。$r^{(1)} = y-x_2(1/4) = (7/4, -1/4)^\\top$。\n    $x_1^\\top r^{(1)} = 7/4$。 $\\beta_1 \\leftarrow S_1(7/4) = 3/4$。\n迭代 2 结束：$\\beta^{(2)} = (3/4, 1/4)^{\\top}$。\n检查 KKT：$r = y-X\\beta^{(2)} = \\begin{pmatrix} 1 \\\\ -1/4 \\end{pmatrix}$。\n$v_1(\\beta^{(2)}) = |1-1| = 0$。\n$v_2(\\beta^{(2)}) = |x_2^\\top r - 1| = |3/4-1| = 1/4 = 0.25$。\n$V(\\beta^{(2)}) = 0.25 > 0.2$。我们继续。\n\n**迭代 3 (顺序 $\\mathcal{B}$):**\n从 $\\beta = (3/4, 1/4)^{\\top}$ 开始。\n1.  **更新 $\\beta_2$**：固定 $\\beta_1=3/4$。$r^{(2)} = y-x_1(3/4) = (5/4, 0)^\\top$。\n    $x_2^\\top r^{(2)} = 5/4$。 $\\beta_2 \\leftarrow \\frac{1}{2}S_1(5/4) = \\frac{1}{8}$。 状态：$(3/4, 1/8)^{\\top}$。\n2.  **更新 $\\beta_1$**：固定 $\\beta_2=1/8$。$r^{(1)} = y-x_2(1/8) = (15/8, -1/8)^\\top$。\n    $x_1^\\top r^{(1)} = 15/8$。 $\\beta_1 \\leftarrow S_1(15/8) = 7/8$。\n迭代 3 结束：$\\beta^{(3)} = (7/8, 1/8)^{\\top}$。\n检查 KKT：$r = y-X\\beta^{(3)} = \\begin{pmatrix} 1 \\\\ -1/8 \\end{pmatrix}$。\n$v_1(\\beta^{(3)}) = |1-1| = 0$。\n$v_2(\\beta^{(3)}) = |x_2^\\top r - 1| = |7/8-1| = 1/8 = 0.125$。\n$V(\\beta^{(3)}) = 0.125 \\le 0.2$。算法停止。\n顺序 $\\mathcal{B}$ 所需的完整迭代次数为 $N_{\\mathcal{B}} = 3$。\n\n### 最终计算\n\n顺序 $\\mathcal{A}$ 的迭代次数为 $N_{\\mathcal{A}} = 1$。\n顺序 $\\mathcal{B}$ 的迭代次数为 $N_{\\mathcal{B}} = 3$。\n所求的差值为：\n$$\nN_{\\mathcal{B}} - N_{\\mathcal{A}} = 3 - 1 = 2\n$$", "answer": "$$\\boxed{2}$$", "id": "3442192"}, {"introduction": "在探讨了更新顺序对收敛速度的影响之后，我们将通过一个编程实践，来研究一个更深层次的现象：路径依赖性。当特征之间存在共线性（在真实数据中很常见）时，LASSO 问题可能存在多个最优解。本练习 [@problem_id:3111866] 将引导您观察，对于完全相同的两个特征，循环坐标下降算法会根据更新顺序的不同而得到不同的系数分配，尽管它们都能达到相同的最优目标值。这种现象揭示了算法在非严格凸优化问题上的微妙行为，对于理解其在实践中的应用至关重要。", "problem": "您的任务是为最小绝对收缩和选择算子 (LASSO) 实现一个循环坐标下降算法，该算法最小化如下凸目标函数\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^p} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1,\n$$\n其中 $n$ 是样本数量，$p$ 是特征数量，$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\boldsymbol{y} \\in \\mathbb{R}^{n}$ 是响应向量，$\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda \\in \\mathbb{R}_{\\ge 0}$ 是正则化参数，$\\left\\| \\cdot \\right\\|_2$ 表示欧几里得范数，$\\left\\| \\cdot \\right\\|_1$ 表示 $L_1$ 范数。从基本的凸优化原理和次梯度最优性条件出发，推导并实现一个坐标更新法则，该法则在固定其他坐标的同时，沿单个坐标最小化目标函数。您的实现必须：\n- 在遍历所有坐标的过程中，按照指定的坐标顺序执行循环更新。\n- 在每次坐标更新后，高效地维护和更新残差 $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$。\n- 当单次完整遍历中坐标的最大绝对变化量小于一个容差（使用 $10^{-12}$）或达到最大遍历次数（使用 $100$）时终止。\n\n需要研究的核心现象是当 $\\boldsymbol{X}$ 的两列完全相同时解的非唯一性。在这种情况下，目标函数仅取决于重复系数的和，对重复坐标的不同分配可以达到相同的目标函数值。您必须通过实验证明坐标下降的路径依赖性和对称性破缺：改变坐标更新的顺序会导致重复特征的系数分配不同，尽管预测结果相同。\n\n实现一个程序，对每个测试用例运行两次坐标下降算法，使用两种不同的坐标顺序：首先是顺序 $[0,1]$，然后是顺序 $[1,0]$。计算并报告捕捉两次运行之间差异的量化指标。\n\n对所有测试用例使用以下固定数据：\n- 设 $n = 10$ 并定义向量 $\\boldsymbol{x} = [1,2,3,4,5,6,7,8,9,10]^\\top$。\n- 构建 $\\boldsymbol{X} \\in \\mathbb{R}^{10 \\times 2}$，其两列相同，即 $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$。\n- 定义三个测试用例，它们的响应向量 $\\boldsymbol{y}$ 和正则化参数 $\\lambda$ 不同：\n  1. 测试用例 A（理想情况）：$\\boldsymbol{y} = 3 \\boldsymbol{x}$ 和 $\\lambda = 5$。\n  2. 测试用例 B（符号边界情况）：$\\boldsymbol{y} = -3 \\boldsymbol{x}$ 和 $\\lambda = 5$。\n  3. 测试用例 C（边界情况）：$\\boldsymbol{y} = 3 \\boldsymbol{x}$ 和 $\\lambda = 200$。\n\n初始化和停止条件：\n- 对所有运行，将 $\\boldsymbol{\\beta}$ 初始化为零向量。\n- 按规定使用容差 $10^{-12}$ 和最大遍历次数 $100$。\n\n对于每个测试用例，运行算法两次（一次使用坐标更新顺序 $[0,1]$，一次使用 $[1,0]$），并计算以下四个指标：\n1. 两个系数向量之差的欧几里得范数，即 $ \\left\\| \\boldsymbol{\\beta}^{(01)} - \\boldsymbol{\\beta}^{(10)} \\right\\|_2 $，以浮点数形式表示。\n2. 重复系数之和的绝对差值，即 $ \\left| \\left( \\beta^{(01)}_0 + \\beta^{(01)}_1 \\right) - \\left( \\beta^{(10)}_0 + \\beta^{(10)}_1 \\right) \\right| $，以浮点数形式表示。\n3. 两个预测向量之差的欧几里得范数，即 $ \\left\\| \\boldsymbol{X}\\boldsymbol{\\beta}^{(01)} - \\boldsymbol{X}\\boldsymbol{\\beta}^{(10)} \\right\\|_2 $，以浮点数形式表示。\n4. 一个布尔值，指示是否发生对称性破缺，定义为在至少一次运行中重复系数是否不同，即 $ \\left| \\beta^{(01)}_0 - \\beta^{(01)}_1 \\right| > 10^{-9} $ 或 $ \\left| \\beta^{(10)}_0 - \\beta^{(10)}_1 \\right| > 10^{-9} $ 是否成立。\n\n您的程序应生成单行输出，其中包含三个测试用例的结果，形式为方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是按上述顺序排列的四个指标的列表。例如，输出必须具有以下形式\n$$\n[\\,[m_{A,1},m_{A,2},m_{A,3},b_A],\\,[m_{B,1},m_{B,2},m_{B,3},b_B],\\,[m_{C,1},m_{C,2},m_{C,3},b_C]\\,],\n$$\n其中 $m_{\\cdot,\\cdot}$ 是浮点数，$b_{\\cdot}$ 是布尔值。不涉及物理单位或角度；所有量都是无量纲的实数。通过遵循目标函数的凸性和基于次梯度的坐标最小化定义，而不是依赖于临时启发式方法，来确保科学真实性。", "solution": "用户提供的问题是有效的。这是一个适定且有科学依据的计算统计学任务，特别关注用于 LASSO 回归的坐标下降算法。所有必要的参数和条件都已提供，其目标是探索在存在共线性特征时路径依赖解这一已知现象，这是优化中的一个标准课题。\n\n### 1. LASSO 目标函数\n\n该问题要求最小化 LASSO 目标函数，该函数由一个最小二乘数据保真项和一个 $L_1$ 范数正则化项组成。对于系数向量 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$，目标函数 $L(\\boldsymbol{\\beta})$ 由下式给出：\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 + \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1\n$$\n在这里，$\\boldsymbol{y} \\in \\mathbb{R}^n$ 是响应向量，$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$n$ 是样本数量，$p$ 是特征数量，$\\lambda \\ge 0$ 是正则化参数。$L_1$ 范数定义为 $\\left\\| \\boldsymbol{\\beta} \\right\\|_1 = \\sum_{j=1}^p |\\beta_j|$。该目标函数是凸函数，这保证了全局最小值的存在。\n\n### 2. 坐标更新法则的推导\n\n坐标下降一次只针对一个坐标优化目标函数，同时保持所有其他坐标固定。为了推导第 $k$ 个系数 $\\beta_k$ 的更新规则，我们视所有其他系数 $\\beta_j$（对于 $j \\neq k$）为常数。\n\n目标函数可以写成隔离包含 $\\beta_k$ 的项的形式：\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda \\sum_{j \\neq k} |\\beta_j| + \\lambda |\\beta_k|\n$$\n我们定义偏残差 $\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j$。如果从模型中移除特征 $k$，这就是残差。不涉及 $\\beta_k$ 的项在对 $\\beta_k$ 进行最小化时是常数。因此，为 $\\beta_k$ 最小化的目标是：\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left\\| \\boldsymbol{r}_k - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda |\\beta_k|\n$$\n展开二次项可得：\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left( \\boldsymbol{r}_k^\\top \\boldsymbol{r}_k - 2\\beta_k \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k^2 \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k} \\right) + \\lambda |\\beta_k|\n$$\n为了找到这个凸函数的最小值，我们使用次梯度最优性条件，该条件表明在最小值点，$0$ 必须在 $L_k(\\beta_k)$ 的次梯度中。绝对值函数 $|\\cdot|$ 的次梯度是符号函数 `sgn`，它在 $0$ 处是一个集值函数。\n$$\n\\frac{\\partial L_k(\\beta_k)}{\\partial \\beta_k} = \\frac{1}{n} \\left( -\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) \\right) + \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n令次梯度包含 $0$：\n$$\n0 \\in \\frac{1}{n} \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k - \\beta_k \\frac{1}{n} \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n我们定义 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k$ 和 $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}$。条件变为：\n$$\n\\beta_k z_k \\in \\rho_k - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n这导出了软阈值函数 $S(a, \\delta) = \\text{sgn}(a)\\max(|a|-\\delta, 0)$。$\\beta_k$ 的解是：\n$$\n\\beta_k = \\frac{S(\\rho_k, \\lambda)}{z_k}\n$$\n此更新规则沿第 $k$ 个坐标最小化目标函数。\n\n### 3. 坐标下降算法与高效更新\n\n循环坐标下降算法重复迭代所有坐标 $k=0, 1, \\dots, p-1$ 直至收敛。为提高计算效率，我们避免在每一步重新计算偏残差 $\\boldsymbol{r}_k$。取而代之的是，我们维护完整的残差 $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$。偏残差可以用完整残差和更新前 $\\beta_k$ 的当前值来表示：\n$$\n\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j = \\left(\\boldsymbol{y} - \\sum_{j=1}^p \\boldsymbol{X}_{:,j}\\beta_j \\right) + \\boldsymbol{X}_{:,k}\\beta_k = \\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k\n$$\n因此，项 $\\rho_k$ 可以计算为 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k)$，其中 $\\boldsymbol{r}$ 和 $\\beta_k$ 是坐标 $k$ 更新前的值。\n\n将 $\\beta_k$ 从 $\\beta_k^{\\text{old}}$ 更新到 $\\beta_k^{\\text{new}}$ 后，完整残差 $\\boldsymbol{r}$ 也必须更新。预测值的变化是 $\\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$。新的残差是：\n$$\n\\boldsymbol{r}^{\\text{new}} = \\boldsymbol{r}^{\\text{old}} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})\n$$\n这是一个高效的 $O(n)$ 更新。算法如下：\n\n1.  初始化 $\\boldsymbol{\\beta} = \\boldsymbol{0}$ 和 $\\boldsymbol{r} = \\boldsymbol{y}$。对所有 $k$ 预计算 $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top\\boldsymbol{X}_{:,k}$。\n2.  对于每次遍历，直到达到最大遍历次数：\n    a. 初始化 `max_abs_change` 为 $0$。\n    b. 按照指定的顺序对每个坐标 $k$：\n        i.   存储 $\\beta_k^{\\text{old}} = \\beta_k$。\n        ii.  计算 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k^{\\text{old}})$。\n        iii. 计算 $\\beta_k^{\\text{new}} = S(\\rho_k, \\lambda) / z_k$。\n        iv.  更新残差：$\\boldsymbol{r} \\leftarrow \\boldsymbol{r} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$。\n        v.   更新系数：$\\beta_k \\leftarrow \\beta_k^{\\text{new}}$。\n        vi.  用 $|\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}}|$ 更新 `max_abs_change`。\n    c. 如果 `max_abs_change` 小于容差（例如，$10^{-12}$），则终止。\n3.  返回最终的系数向量 $\\boldsymbol{\\beta}$。\n\n### 4. 列相同时的非唯一性\n\n问题设置了一个场景，其中 $\\boldsymbol{X}$ 的两列相同，即 $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$。在这种情况下，模型预测为：\n$$\n\\boldsymbol{X}\\boldsymbol{\\beta} = \\boldsymbol{X}_{:,0}\\beta_0 + \\boldsymbol{X}_{:,1}\\beta_1 = \\boldsymbol{x}\\beta_0 + \\boldsymbol{x}\\beta_1 = (\\beta_0 + \\beta_1)\\boldsymbol{x}\n$$\n目标函数的数据保真项仅取决于系数之和 $\\beta_{\\text{sum}} = \\beta_0 + \\beta_1$。$L_1$ 惩罚项是 $\\lambda(|\\beta_0| + |\\beta_1|)$。优化问题变为：\n$$\n\\min_{\\beta_0, \\beta_1} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - (\\beta_0 + \\beta_1)\\boldsymbol{x} \\right\\|_2^2 + \\lambda(|\\beta_0| + |\\beta_1|)\n$$\n虽然和 $\\beta_{\\text{sum}}$ 的最优值是唯一的，但可以有无穷多对 $(\\beta_0, \\beta_1)$ 产生这个和。对于任何给定的最优 $\\beta_{\\text{sum}}$，当 $\\beta_{\\text{sum}}$ 的全部大小被分配给单个系数时（例如，$\\beta_0 = \\beta_{\\text{sum}}, \\beta_1 = 0$ 或反之亦然），$L_1$ 惩罚项 $\\lambda(|\\beta_0| + |\\beta_1|)$ 被最小化，前提是 $\\beta_{\\text{sum}} \\neq 0$。\n\n循环坐标下降算法根据更新顺序打破了这种对称性。当首先更新坐标 0 时，它将尽可能多地吸收信号，可能为坐标 1 留下一个很小的残差。如果残差信号低于由 $\\lambda$ 设定的阈值，则 $\\beta_1$ 的更新将为零。将顺序颠倒为 $[1, 0]$ 将导致 $\\beta_1$ 首先吸收信号，从而得到一个不同的最终系数向量 $\\boldsymbol{\\beta}$，即使和 $\\beta_0+\\beta_1$ 以及预测 $\\boldsymbol{X}\\boldsymbol{\\beta}$ 是相同的（在数值精度范围内）。这种路径依赖性是坐标下降在非严格凸目标上运行的一个关键特征。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the coordinate descent for LASSO to demonstrate path dependence.\n    \"\"\"\n\n    def coordinate_descent(X, y, lambda_val, order, tol, max_passes):\n        \"\"\"\n        Performs cyclic coordinate descent for LASSO.\n        \"\"\"\n        n, p = X.shape\n        beta = np.zeros(p)\n        residual = y.copy()\n\n        # Precompute column normalization factors\n        z = np.array([(1/n) * X[:, k].T @ X[:, k] for k in range(p)])\n\n        def soft_threshold(a, delta):\n            return np.sign(a) * np.maximum(np.abs(a) - delta, 0)\n\n        for _ in range(max_passes):\n            max_abs_change = 0.0\n            \n            for k in order:\n                beta_old_k = beta[k]\n                \n                # Calculate rho_k using the efficient residual update formula\n                # rho_k = (1/n) * X_k^T * (y - sum_{j!=k} X_j * beta_j)\n                # which is equivalent to (1/n) * X_k^T * (residual + X_k * beta_old_k)\n                rho_k = (1/n) * X[:, k].T @ (residual + X[:, k] * beta_old_k)\n\n                # Update beta_k using soft-thresholding\n                beta[k] = soft_threshold(rho_k, lambda_val) / z[k]\n                \n                delta_beta_k = beta[k] - beta_old_k\n                \n                if delta_beta_k != 0:\n                    # Update residual efficiently\n                    residual -= X[:, k] * delta_beta_k\n\n                max_abs_change = max(max_abs_change, np.abs(delta_beta_k))\n\n            if max_abs_change  tol:\n                break\n                \n        return beta\n\n    # --- Fixed Data ---\n    n = 10\n    x_vec = np.arange(1, 11, dtype=float)\n    X = np.stack([x_vec, x_vec], axis=1)\n    \n    # --- Algorithm Parameters ---\n    tol = 1e-12\n    max_passes = 100\n    sym_tol = 1e-9\n\n    # --- Test Cases ---\n    test_cases_params = [\n        # Case A: happy path\n        {'y': 3 * x_vec, 'lambda': 5.0},\n        # Case B: sign edge case\n        {'y': -3 * x_vec, 'lambda': 5.0},\n        # Case C: boundary case (high regularization)\n        {'y': 3 * x_vec, 'lambda': 200.0},\n    ]\n\n    results = []\n    \n    for params in test_cases_params:\n        y = params['y']\n        lambda_val = params['lambda']\n\n        # Run with order [0, 1]\n        beta_01 = coordinate_descent(X, y, lambda_val, order=[0, 1], tol=tol, max_passes=max_passes)\n        \n        # Run with order [1, 0]\n        beta_10 = coordinate_descent(X, y, lambda_val, order=[1, 0], tol=tol, max_passes=max_passes)\n\n        # --- Compute Metrics ---\n        # 1. Euclidean norm of the difference between the two coefficient vectors\n        metric1 = np.linalg.norm(beta_01 - beta_10)\n\n        # 2. Absolute difference in the sum of the duplicate coefficients\n        metric2 = np.abs(np.sum(beta_01) - np.sum(beta_10))\n\n        # 3. Euclidean norm of the difference between the two prediction vectors\n        pred_01 = X @ beta_01\n        pred_10 = X @ beta_10\n        metric3 = np.linalg.norm(pred_01 - pred_10)\n\n        # 4. Boolean indicating symmetry-breaking\n        m4_cond1 = np.abs(beta_01[0] - beta_01[1]) > sym_tol\n        m4_cond2 = np.abs(beta_10[0] - beta_10[1]) > sym_tol\n        metric4 = m4_cond1 or m4_cond2\n\n        results.append([metric1, metric2, metric3, metric4])\n\n    # Format output as specified: [[m_A1,m_A2,m_A3,b_A],[m_B1,m_B2,m_B3,b_B],...]\n    inner_results_str = []\n    for metrics in results:\n        m1, m2, m3, b = metrics\n        # Use str(b) to get 'True' or 'False'\n        inner_results_str.append(f\"[{m1},{m2},{m3},{str(b).lower()}]\")\n    \n    # Correction: The original prompt expects python boolean literals, not strings.\n    final_output = \"[\" + \",\".join(\n        f\"[{m[0]},{m[1]},{m[2]},{str(m[3])}]\" for m in results\n    ) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "3111866"}]}