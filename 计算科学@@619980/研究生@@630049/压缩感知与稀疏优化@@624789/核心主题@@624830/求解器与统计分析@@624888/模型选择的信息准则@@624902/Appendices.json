{"hands_on_practices": [{"introduction": "本练习旨在提供一个基础性的实践机会，让您能够直接应用赤池信息准则（AIC）和贝叶斯信息准则（BIC）的定义。通过在一个简化的正交设计下进行计算，我们可以剥离最小二乘法拟合的复杂性，从而将注意力集中在AIC和BIC的核心区别上：罚项对样本量$n$的不同依赖关系。这个练习将清晰地揭示，为何在实践中BIC通常倾向于选择比AIC更简洁的模型。[@problem_id:3452903]", "problem": "考虑一个压缩感知中的线性观测模型，其带有高斯噪声，由 $y \\in \\mathbb{R}^{n}$ 和 $X \\in \\mathbb{R}^{n \\times p}$ 给出，其中 $X$ 的列是标准正交的，因此 $X^{\\top}X = I_{p}$。假设 $y = X \\beta^{\\star} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。定义最佳k项近似为在 $X$ 的所有 $k$ 列子集上最小化经验平方误差的k项线性模型。令 $z = X^{\\top} y \\in \\mathbb{R}^{p}$，并令 $|z|_{(1)} \\geq |z|_{(2)} \\geq \\cdots \\geq |z|_{(p)}$ 表示 $z$ 各分量绝对值的顺序统计量，其对应的带符号值为 $z_{(j)}$。\n\n从高斯对数似然和 $X$ 的标准正交性出发，推导残差平方和作为 $k$ 的函数在最佳k项路径上的显式表达式。然后，利用赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 作为从最大化高斯似然导出的信息准则（其惩罚项与自由参数数量成正比）的定义，求出它们在此路径上作为 $k$ 的函数的表达式。你可以忽略不依赖于 $k$ 的加法常数。\n\n现在，将问题具体到以下数据：\n- $n = 120$, $p = 10$,\n- $\\|y\\|^{2} = 300$,\n- $z = X^{\\top} y = [\\,5.2,\\,-3.1,\\,2.7,\\,-2.2,\\,1.9,\\,-1.5,\\,1.2,\\,0.9,\\,-0.6,\\,0.4\\,]$.\n\n计算在从给定 $z$ 构建的最佳k项路径上，分别最小化 AIC 和 BIC 的 $k$ 值。如果出现值相同的情况，选择较小的 $k$。以数对 $(k_{\\mathrm{AIC}}, k_{\\mathrm{BIC}})$ 的形式报告你的最终答案。最终答案必须作为一个整体给出，且不需要四舍五入。", "solution": "用户想要解决一个在压缩感知背景下的模型选择问题。\n第一步是验证问题陈述。\n\n### 步骤1：提取已知条件\n- **模型**：线性观测模型 $y = X \\beta^{\\star} + \\varepsilon \\in \\mathbb{R}^{n}$。\n- **传感矩阵**：$X \\in \\mathbb{R}^{n \\times p}$，其列是标准正交的，即 $X^{\\top}X = I_{p}$。\n- **噪声**：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。\n- **数据变换**：$z = X^{\\top} y \\in \\mathbb{R}^{p}$。\n- **顺序统计量**：$|z|_{(1)} \\geq |z|_{(2)} \\geq \\cdots \\geq |z|_{(p)}$ 是 $z$ 各分量绝对值的有序值。相应的带符号值表示为 $z_{(j)}$。\n- **最佳k项近似**：在 $X$ 的所有 $k$ 列子集上最小化经验平方误差的k项线性模型。\n- **信息准则**：从最大化高斯对数似然导出的赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC)。\n- **数据**：$n = 120$, $p = 10$, $\\|y\\|^{2} = 300$。\n- **变换后的数据**：$z = [\\,5.2,\\,-3.1,\\,2.7,\\,-2.2,\\,1.9,\\,-1.5,\\,1.2,\\,0.9,\\,-0.6,\\,0.4\\,]$。\n- **任务**：找到分别最小化 AIC 和 BIC 的整数 $(k_{\\mathrm{AIC}}, k_{\\mathrm{BIC}})$。如果出现平局，选择较小的 k。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题有科学依据、适定、客观且自洽。\n- **科学合理性**：问题设置是高维统计学和压缩感知中的一个标准场景，依赖于带有高斯噪声的线性模型和标准正交设计矩阵。AIC 和 BIC 的定义在统计模型选择中是标准的。\n- **适定性**：目标明确：推导 AIC 和 BIC 的表达式，然后找到最小化每个准则的 $k$ 值。所提供的数据可以得到唯一解。$X$ 的标准正交性大大简化了问题，这是一个常见的理论假设。\n- **客观性**：语言精确，没有主观论断。\n- **完整性**：提供了解决该问题所需的所有必要数据和定义。\n该问题是有效的。\n\n### 步骤3：进行求解\n该问题是有效的。我现在将提供一个完整且有理有据的解法。\n\n模型 $y = X\\beta + \\varepsilon$ 的高斯对数似然由下式给出：\n$$ \\ell(\\beta, \\sigma^2; y) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|y - X\\beta\\|^2 $$\n对于一个固定的模型（即 $\\beta$ 的一个固定支撑集），在 $\\beta$ 上最大化对数似然等价于最小化残差平方和 (RSS)，其定义为 $\\mathrm{RSS}(\\beta) = \\|y - X\\beta\\|^2$。\n\n让我们考虑一个具有 $k$ 个非零系数的模型，其索引由集合 $S \\subset \\{1, \\dots, p\\}$ 给出，且 $|S|=k$。该模型为 $y = X_S \\beta_S + \\varepsilon$，其中 $X_S$ 包含由 $S$ 索引的 $X$ 的列。$\\beta_S$ 的最小二乘估计是 $\\hat{\\beta}_S = (X_S^{\\top}X_S)^{-1} X_S^{\\top}y$。\n鉴于 $X$ 的列是标准正交的，我们有 $X^{\\top}X = I_p$。这意味着对于任何列子集 $S$，有 $X_S^{\\top}X_S = I_k$。\n因此，估计量简化为 $\\hat{\\beta}_S = X_S^{\\top}y$。$\\hat{\\beta}_S$ 的分量是 $z=X^{\\top}y$ 的相应分量。也就是说，对于 $j \\in S$，$\\hat{\\beta}_j = z_j$。对于 $j \\notin S$，我们有 $\\hat{\\beta}_j = 0$。\n\n该模型的 RSS 为：\n$$ \\mathrm{RSS}(S) = \\|y - X_S \\hat{\\beta}_S\\|^2 = \\|y - X_S (X_S^{\\top}y)\\|^2 $$\n向量 $X_S(X_S^{\\top}y)$ 是 $y$ 在由 $X_S$ 中各列张成的子空间上的正交投影。根据勾股定理，$\\|y\\|^2 = \\|X_S(X_S^{\\top}y)\\|^2 + \\mathrm{RSS}(S)$。\n投影的平方范数为：\n$$ \\|X_S(X_S^{\\top}y)\\|^2 = (X_S^{\\top}y)^{\\top} (X_S^{\\top}X_S) (X_S^{\\top}y) = (X_S^{\\top}y)^{\\top} I_k (X_S^{\\top}y) = \\|X_S^{\\top}y\\|^2 = \\sum_{j \\in S} z_j^2 $$\n因此，RSS 为：\n$$ \\mathrm{RSS}(S) = \\|y\\|^2 - \\sum_{j \\in S} z_j^2 $$\n最佳k项近似是最小化 RSS 的那个。为了在 $k$ 固定的情况下最小化 $\\mathrm{RSS}(S)$，我们必须选择使 $\\sum_{j \\in S} z_j^2$ 最大化的集合 $S$。这通过选择与 $|z_j|$（或 $z_j^2$）的 $k$ 个最大值相对应的 $k$ 个索引来实现。\n令 $S_{(k)}$ 为这 $k$ 个索引的最优集合。最佳k项模型的 RSS，记为 $\\mathrm{RSS}_k$，是：\n$$ \\mathrm{RSS}_k = \\|y\\|^2 - \\sum_{j=1}^{k} |z|_{(j)}^2 = \\|y\\|^2 - \\sum_{j=1}^{k} z_{(j)}^2 $$\n其中 $z_{(j)}^2$ 是第 $j$ 大的平方绝对值。\n如果 $k=0$（零模型），我们有 $\\hat{\\beta}=0$，因此 $\\mathrm{RSS}_0 = \\|y\\|^2$。\n\n为了找到 AIC 和 BIC 的表达式，我们首先需要最大化的对数似然。对于一个大小为 $k$ 的模型，方差的最大似然估计是 $\\hat{\\sigma}_k^2 = \\frac{\\mathrm{RSS}_k}{n}$。将 $\\hat{\\beta}_{S_{(k)}}$ 和 $\\hat{\\sigma}_k^2$ 代入对数似然表达式，得到：\n$$ \\hat{\\ell}_k = -\\frac{n}{2} \\ln(2\\pi) - \\frac{n}{2}\\ln(\\hat{\\sigma}_k^2) - \\frac{1}{2\\hat{\\sigma}_k^2}\\mathrm{RSS}_k = -\\frac{n}{2}\\ln(2\\pi\\frac{\\mathrm{RSS}_k}{n}) - \\frac{n}{2} = -\\frac{n}{2}\\ln(\\mathrm{RSS}_k) + C $$\n其中 $C$ 是一个不依赖于 $k$ 的常数。\n\nAIC 定义为 $-2\\hat{\\ell} + 2 \\times (\\text{参数数量})$。一个大小为 $k$ 的模型有 $k$ 个非零系数和一个估计的方差 $\\sigma^2$，所以有 $k+1$ 个自由参数。\n$$ \\mathrm{AIC}(k) = -2\\hat{\\ell}_k + 2(k+1) = n\\ln(\\mathrm{RSS}_k) - 2C + 2(k+1) $$\n忽略在 $k$ 上为常数的项，要最小化的表达式是：\n$$ \\mathrm{AIC}(k) \\propto n\\ln(\\mathrm{RSS}_k) + 2k $$\n\nBIC 定义为 $-2\\hat{\\ell} + \\ln(n) \\times (\\text{参数数量})$。\n$$ \\mathrm{BIC}(k) = -2\\hat{\\ell}_k + (k+1)\\ln(n) = n\\ln(\\mathrm{RSS}_k) - 2C + (k+1)\\ln(n) $$\n忽略在 $k$ 上为常数的项，要最小化的表达式是：\n$$ \\mathrm{BIC}(k) \\propto n\\ln(\\mathrm{RSS}_k) + k\\ln(n) $$\n\n现在我们使用所提供的数据来找到 $k_{\\mathrm{AIC}}$ 和 $k_{\\mathrm{BIC}}$。\n$n = 120$, $p = 10$, $\\|y\\|^2 = 300$。\n$z = [\\,5.2,\\,-3.1,\\,2.7,\\,-2.2,\\,1.9,\\,-1.5,\\,1.2,\\,0.9,\\,-0.6,\\,0.4\\,]$。\n$z$ 的分量已经按绝对值降序排列。我们计算平方值：\n$z_{(1)}^2 = 5.2^2 = 27.04$\n$z_{(2)}^2 = (-3.1)^2 = 9.61$\n$z_{(3)}^2 = 2.7^2 = 7.29$\n$z_{(4)}^2 = (-2.2)^2 = 4.84$\n$z_{(5)}^2 = 1.9^2 = 3.61$\n$z_{(6)}^2 = (-1.5)^2 = 2.25$\n$z_{(7)}^2 = 1.2^2 = 1.44$\n$z_{(8)}^2 = 0.9^2 = 0.81$\n$z_{(9)}^2 = (-0.6)^2 = 0.36$\n$z_{(10)}^2 = 0.4^2 = 0.16$\n\n令 $S_k = \\sum_{j=1}^{k} z_{(j)}^2$。我们计算累加和：\n$S_0 = 0$\n$S_1 = 27.04$\n$S_2 = 36.65$\n$S_3 = 43.94$\n$S_4 = 48.78$\n$S_5 = 52.39$\n$S_6 = 54.64$\n$S_7 = 56.08$\n$S_8 = 56.89$\n$S_9 = 57.25$\n$S_{10} = 57.41$\n\n现在，我们计算 $\\mathrm{RSS}_k = \\|y\\|^2 - S_k = 300 - S_k$：\n$\\mathrm{RSS}_0 = 300$\n$\\mathrm{RSS}_1 = 272.96$\n$\\mathrm{RSS}_2 = 263.35$\n$\\mathrm{RSS}_3 = 256.06$\n$\\mathrm{RSS}_4 = 251.22$\n$\\mathrm{RSS}_5 = 247.61$\n$\\mathrm{RSS}_6 = 245.36$\n$\\mathrm{RSS}_7 = 243.92$\n$\\mathrm{RSS}_8 = 243.11$\n$\\mathrm{RSS}_9 = 242.75$\n$\\mathrm{RSS}_{10} = 242.59$\n\n为了找到 $k_{\\mathrm{AIC}}$，我们最小化 $\\mathrm{AIC}(k) \\propto 120\\ln(\\mathrm{RSS}_k) + 2k$。\n$\\mathrm{AIC}(0) \\propto 120\\ln(300) + 0 \\approx 684.46$\n$\\mathrm{AIC}(1) \\propto 120\\ln(272.96) + 2 \\approx 673.12 + 2 = 675.12$\n$\\mathrm{AIC}(2) \\propto 120\\ln(263.35) + 4 \\approx 668.82 + 4 = 672.82$\n$\\mathrm{AIC}(3) \\propto 120\\ln(256.06) + 6 \\approx 665.45 + 6 = 671.45$\n$\\mathrm{AIC}(4) \\propto 120\\ln(251.22) + 8 \\approx 663.16 + 8 = 671.16$\n$\\mathrm{AIC}(5) \\propto 120\\ln(247.61) + 10 \\approx 661.42 + 10 = 671.42$\n$\\mathrm{AIC}(6) \\propto 120\\ln(245.36) + 12 \\approx 660.32 + 12 = 672.32$\n最小值出现在 $k=4$ 处。在 $k=5$ 处的值略大。因此，$k_{\\mathrm{AIC}} = 4$。\n\n为了找到 $k_{\\mathrm{BIC}}$，我们最小化 $\\mathrm{BIC}(k) \\propto 120\\ln(\\mathrm{RSS}_k) + k\\ln(120)$。\n我们有 $\\ln(120) \\approx 4.7875$。\n$\\mathrm{BIC}(0) \\propto 120\\ln(300) + 0 \\approx 684.46$\n$\\mathrm{BIC}(1) \\propto 120\\ln(272.96) + \\ln(120) \\approx 673.12 + 4.79 = 677.91$\n$\\mathrm{BIC}(2) \\propto 120\\ln(263.35) + 2\\ln(120) \\approx 668.82 + 9.58 = 678.40$\n$\\mathrm{BIC}(3) \\propto 120\\ln(256.06) + 3\\ln(120) \\approx 665.45 + 14.36 = 679.81$\n$\\mathrm{BIC}(k)$ 的值在 $k=1$ 之后增加，因为惩罚项 $k\\ln(120)$ 的增长速度快于拟合优度项 $120\\ln(\\mathrm{RSS}_k)$ 的下降速度。最小值在 $k=1$ 处。因此，$k_{\\mathrm{BIC}} = 1$。\n\n最优模型大小的数对是 $(k_{\\mathrm{AIC}}, k_{\\mathrm{BIC}})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4  1\n\\end{pmatrix}\n}\n$$", "id": "3452903"}, {"introduction": "在复杂的正则化估计中，模型的“复杂度”不能简单地用非零参数的个数来衡量。本练习将引导您超越简单的参数计数，探索一个更广义的概念——“自由度”。我们将运用高斯积分中的一个强大工具——斯坦因恒等式（Stein's identity），来精确推导软阈值估计器的自由度，并证明它等于模型中非零系数的期望数量。进而，我们将此结果与斯坦因无偏风险估计（SURE）联系起来，展示如何利用SURE为正则化参数（如阈值$\\lambda$）的选择提供一种不依赖于交叉验证的替代方案。[@problem_id:3452918]", "problem": "考虑压缩感知中的正交设计设置，其中观测响应向量在 $\\mathbb{R}^{n}$ 中建模为 $Y = \\theta + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，且均值向量 $\\theta \\in \\mathbb{R}^{n}$ 未知。按坐标定义在阈值 $\\lambda \\ge 0$ 处的软阈值估计量为\n$$\n\\big(\\eta_{\\lambda}(y)\\big)_{i} = \\operatorname{sign}(y_{i}) \\cdot \\big(|y_{i}| - \\lambda\\big)_{+}, \\quad i = 1, \\dots, n,\n$$\n其中 $(t)_{+} = \\max\\{t, 0\\}$。\n\n仅使用基于高斯微积分（特别是针对高斯期望的 Stein 恒等式）的基本原理和自由度的定义\n$$\n\\mathrm{df}(g) = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} \\mathrm{Cov}\\big(g_{i}(Y), Y_{i}\\big),\n$$\n执行以下任务：\n\n1. 推导软阈值估计量 $\\eta_{\\lambda}$ 的精确自由度，并证明它等于在 $Y$ 的抽样分布下非零坐标的期望数量。用 $\\theta$、$\\sigma$ 和 $\\lambda$ 显式地表达这个期望。\n\n2. 基于 Stein 无偏风险估计 (SURE)，根据观测数据向量 $y$、阈值 $\\lambda$ 和噪声水平 $\\sigma$，推导预测风险 $\\mathbb{E}\\big[\\|\\eta_{\\lambda}(Y) - \\theta\\|_{2}^{2}\\big]$ 的一个显式路径无偏估计量。\n\n3. 利用你获得的结构来论证 SURE 作为 $\\lambda$ 的函数是分段光滑的，其变化仅发生在 $y$ 坐标的绝对值处。结论是，SURE 在 $\\lambda \\ge 0$ 上的任何全局最小值点都必须位于有限集 $\\{0, |y|_{(1)}, |y|_{(2)}, \\dots, |y|_{(n)}\\}$ 中，其中 $|y|_{(1)} \\le |y|_{(2)} \\le \\dots \\le |y|_{(n)}$ 是 $\\{|y_{i}|\\}_{i=1}^{n}$ 的顺序统计量。\n\n你的最终答案必须是 SURE 最小化阈值 $\\hat{\\lambda}$ 作为 $y$ 和 $\\sigma$ 的函数的单个闭式解析表达式。无需进行数值计算。", "solution": "该问题是有效的，因为它在科学上基于统计估计理论，是适定的、客观的且内部一致的。\n\n解法按要求分三部分呈现。观测数据向量为 $Y \\in \\mathbb{R}^n$，且 $Y \\sim \\mathcal{N}(\\theta, \\sigma^2 I_n)$，这意味着坐标 $Y_i$ 是独立的，其中 $Y_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$。软阈值估计量由 $\\eta_{\\lambda}(y)$ 给出，其第 $i$ 个坐标为 $\\eta_{\\lambda, i}(y_i) = \\operatorname{sign}(y_i) (|y_i|-\\lambda)_+$。\n\n### 第一部分：软阈值估计量的自由度\n\n估计量 $g(Y)$ 的自由度定义为 $\\mathrm{df}(g) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\mathrm{Cov}(g_i(Y), Y_i)$。对于软阈值估计量 $\\eta_{\\lambda}$，各坐标是独立处理的，因此 $g_i(Y) = \\eta_{\\lambda, i}(Y_i)$。该和式可以逐项分析。\n\n我们对高斯随机变量 $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ 和一个弱可微函数 $h$ 使用 Stein 恒等式。该恒等式表明 $\\mathrm{Cov}(h(Z), Z) = \\sigma^2 \\mathbb{E}[h'(Z)]$。将此应用于每个坐标 $Y_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$ 和函数 $h(y_i) = \\eta_{\\lambda, i}(y_i)$，我们得到：\n$$\n\\mathrm{Cov}(\\eta_{\\lambda, i}(Y_i), Y_i) = \\sigma^2 \\mathbb{E}\\left[\\frac{d}{dY_i}\\eta_{\\lambda, i}(Y_i)\\right]\n$$\n函数 $\\eta_{\\lambda, i}(y_i)$ 可以分段写出：\n$$\n\\eta_{\\lambda, i}(y_i) =\n\\begin{cases}\ny_i - \\lambda  &\\text{若 } y_i > \\lambda \\\\\n0  &\\text{若 } -\\lambda \\le y_i \\le \\lambda \\\\\ny_i + \\lambda  &\\text{若 } y_i  -\\lambda\n\\end{cases}\n$$\n此函数是连续的。它关于 $y_i$ 的弱导数由下式给出：\n$$\n\\frac{d}{dy_i}\\eta_{\\lambda, i}(y_i) =\n\\begin{cases}\n1  \\text{若 } y_i  \\lambda \\\\\n0  \\text{若 } -\\lambda  y_i  \\lambda \\\\\n1  \\text{若 } y_i  -\\lambda\n\\end{cases}\n$$\n该导数可以使用指示函数紧凑地写为 $\\mathbf{1}_{\\{|y_i|  \\lambda\\}}$。\n将此代入期望中，我们有：\n$$\n\\mathrm{Cov}(\\eta_{\\lambda, i}(Y_i), Y_i) = \\sigma^2 \\mathbb{E}[\\mathbf{1}_{\\{|Y_i|  \\lambda\\}}] = \\sigma^2 P(|Y_i|  \\lambda)\n$$\n对所有坐标 $i=1, \\dots, n$ 求和，总自由度为：\n$$\n\\mathrm{df}(\\eta_{\\lambda}) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\sigma^2 P(|Y_i|  \\lambda) = \\sum_{i=1}^n P(|Y_i|  \\lambda)\n$$\n现在，我们证明这等于 $\\eta_{\\lambda}(Y)$ 的非零坐标的期望数量。一个坐标 $\\eta_{\\lambda, i}(Y_i)$ 非零当且仅当 $|Y_i|  \\lambda$。令 $Z_i = \\mathbf{1}_{\\{\\eta_{\\lambda, i}(Y_i) \\neq 0\\}} = \\mathbf{1}_{\\{|Y_i|  \\lambda\\}}$ 为第 $i$ 个坐标非零的指示变量。非零坐标的总数为 $\\sum_{i=1}^n Z_i$。其期望为：\n$$\n\\mathbb{E}\\left[\\sum_{i=1}^n Z_i\\right] = \\sum_{i=1}^n \\mathbb{E}[Z_i] = \\sum_{i=1}^n P(\\eta_{\\lambda, i}(Y_i) \\neq 0) = \\sum_{i=1}^n P(|Y_i|  \\lambda)\n$$\n这与 $\\mathrm{df}(\\eta_{\\lambda})$ 的表达式相同。\n为了显式地表达这一点，我们利用 $Y_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$ 的事实。令 $\\Phi(\\cdot)$ 为标准正态分布 $\\mathcal{N}(0, 1)$ 的累积分布函数。\n$$\nP(|Y_i|  \\lambda) = P(Y_i  \\lambda) + P(Y_i  -\\lambda) = P\\left(\\frac{Y_i - \\theta_i}{\\sigma}  \\frac{\\lambda - \\theta_i}{\\sigma}\\right) + P\\left(\\frac{Y_i - \\theta_i}{\\sigma}  \\frac{-\\lambda - \\theta_i}{\\sigma}\\right)\n$$\n$$\nP(|Y_i|  \\lambda) = \\left(1 - \\Phi\\left(\\frac{\\lambda - \\theta_i}{\\sigma}\\right)\\right) + \\Phi\\left(\\frac{-\\lambda - \\theta_i}{\\sigma}\\right) = \\Phi\\left(\\frac{\\theta_i - \\lambda}{\\sigma}\\right) + \\Phi\\left(\\frac{-\\theta_i - \\lambda}{\\sigma}\\right)\n$$\n所以，自由度为 $\\mathrm{df}(\\eta_{\\lambda}) = \\sum_{i=1}^n \\left[ \\Phi\\left(\\frac{\\theta_i - \\lambda}{\\sigma}\\right) + \\Phi\\left(\\frac{-\\theta_i - \\lambda}{\\sigma}\\right) \\right]$。\n\n### 第二部分：Stein 无偏风险估计 (SURE)\n\n预测风险为 $R(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - \\theta\\|_2^2]$。我们可以将风险分解为：\n$$\nR(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y + Y - \\theta\\|_2^2] = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y\\|_2^2] + \\mathbb{E}[\\|Y - \\theta\\|_2^2] + 2\\mathbb{E}[\\langle \\eta_{\\lambda}(Y) - Y, Y - \\theta \\rangle]\n$$\n中间项为 $\\mathbb{E}[\\|\\epsilon\\|_2^2] = n\\sigma^2$。交叉项可以使用 Stein 恒等式进行分析。\n$$\n\\mathbb{E}[\\langle \\eta_{\\lambda}(Y) - Y, Y - \\theta \\rangle] = \\sum_{i=1}^n \\mathbb{E}[(\\eta_{\\lambda, i}(Y_i) - Y_i)(Y_i - \\theta_i)]\n$$\n令 $h_i(y_i) = \\eta_{\\lambda, i}(y_i) - y_i$。使用 Stein 恒等式 $\\mathbb{E}[h_i(Y_i)(Y_i - \\theta_i)] = \\sigma^2 \\mathbb{E}[h_i'(Y_i)]$：\n$$\n\\mathbb{E}[h_i(Y_i)(Y_i - \\theta_i)] = \\sigma^2 \\mathbb{E}\\left[\\frac{d}{dY_i}(\\eta_{\\lambda, i}(Y_i) - Y_i)\\right] = \\sigma^2 \\mathbb{E}[\\mathbf{1}_{\\{|Y_i|  \\lambda\\}} - 1]\n$$\n将此代回风险展开式中：\n$$\nR(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y\\|_2^2] + n\\sigma^2 + 2\\sum_{i=1}^n \\sigma^2 \\mathbb{E}[\\mathbf{1}_{\\{|Y_i|  \\lambda\\}} - 1]\n$$\n$$\nR(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y\\|_2^2] + n\\sigma^2 + 2\\sigma^2\\mathbb{E}\\left[\\sum_{i=1}^n \\mathbf{1}_{\\{|Y_i|  \\lambda\\}}\\right] - 2n\\sigma^2\n$$\n$$\nR(\\lambda) = \\mathbb{E}\\left[ \\|\\eta_{\\lambda}(Y) - Y\\|_2^2 - n\\sigma^2 + 2\\sigma^2\\sum_{i=1}^n \\mathbf{1}_{\\{|Y_i|  \\lambda\\}} \\right]\n$$\n根据全期望定律，期望内的表达式是风险的无偏估计量。这就是 SURE。对于一个观测到的数据向量 $y$，估计值为：\n$$\n\\mathrm{SURE}(y, \\lambda) = \\|\\eta_{\\lambda}(y) - y\\|_2^2 - n\\sigma^2 + 2\\sigma^2\\sum_{i=1}^n \\mathbf{1}_{\\{|y_i|  \\lambda\\}}\n$$\n我们可以更明确地写出第一项。对于每个坐标 $i$：\n$$\n(\\eta_{\\lambda, i}(y_i) - y_i)^2 =\n\\begin{cases}\n(y_i - \\lambda - y_i)^2 = \\lambda^2  \\text{若 } y_i  \\lambda \\\\\n(0 - y_i)^2 = y_i^2  \\text{若 } |y_i| \\le \\lambda \\\\\n(y_i + \\lambda - y_i)^2 = \\lambda^2  \\text{若 } y_i  -\\lambda\n\\end{cases}\n$$\n这等价于 $\\min(y_i^2, \\lambda^2)$。因此，最终的路径无偏风险估计量是：\n$$\n\\mathrm{SURE}(y, \\lambda) = \\sum_{i=1}^n \\min(y_i^2, \\lambda^2) - n\\sigma^2 + 2\\sigma^2\\sum_{i=1}^n \\mathbf{1}_{\\{|y_i|  \\lambda\\}}\n$$\n\n### 第三部分：SURE 的最小化\n\n我们寻求找到 $\\hat{\\lambda} = \\operatorname{argmin}_{\\lambda \\ge 0} \\mathrm{SURE}(y, \\lambda)$。这等价于最小化函数 $g(\\lambda) = \\mathrm{SURE}(y, \\lambda) + n\\sigma^2$，其表达式为：\n$$\ng(\\lambda) = \\sum_{i=1}^n \\left( \\min(y_i^2, \\lambda^2) + 2\\sigma^2 \\mathbf{1}_{\\{|y_i|  \\lambda\\}} \\right)\n$$\n函数 $g(\\lambda)$ 是函数 $g_i(\\lambda) = \\min(y_i^2, \\lambda^2) + 2\\sigma^2 \\mathbf{1}_{\\{|y_i|  \\lambda\\}}$ 的和。对于固定的 $i$，$g_i(\\lambda)$ 在 $\\lambda \\ge 0$ 上是连续的，除了在 $\\lambda = |y_i|$ 处。$\\min(y_i^2, \\lambda^2)$ 关于 $\\lambda$ 的导数在 $\\lambda  |y_i|$ 时为 $2\\lambda$，在 $\\lambda > |y_i|$ 时为 $0$。指示函数 $\\mathbf{1}_{\\{|y_i|  \\lambda\\}}$ 是分段常数。\n令 $|y|_{(0)} \\equiv 0$ 和 $|y|_{(1)} \\le |y|_{(2)} \\le \\dots \\le |y|_{(n)}$ 为 $y$ 坐标绝对值的顺序统计量。这些值将定义域 $\\lambda \\ge 0$ 划分为区间 $[|y|_{(k)}, |y|_{(k+1)})$ (对于 $k=0, \\dots, n-1$) 和 $[|y|_{(n)}, \\infty)$。\n\n对于任何在开区间 $(|y|_{(k)}, |y|_{(k+1)})$ 内的 $\\lambda$：\n- 有 $k$ 个坐标满足 $|y_i| \\le |y|_{(k)}  \\lambda$。对于这些坐标，$\\min(y_i^2, \\lambda^2) = y_i^2$ 且 $\\mathbf{1}_{\\{|y_i|\\lambda\\}} = 0$。\n- 有 $n-k$ 个坐标满足 $|y_i| \\ge |y|_{(k+1)}  \\lambda$。对于这些坐标，$\\min(y_i^2, \\lambda^2) = \\lambda^2$ 且 $\\mathbf{1}_{\\{|y_i|\\lambda\\}} = 1$。\n所以，对于 $\\lambda \\in (|y|_{(k)}, |y|_{(k+1)})$，该函数为：\n$$\ng(\\lambda) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)(\\lambda^2 + 2\\sigma^2)\n$$\n在此区间上关于 $\\lambda$ 的导数为 $\\frac{dg}{d\\lambda} = 2(n-k)\\lambda$。由于 $\\lambda  0$ 且 $n-k \\ge 1$ (对于 $k  n$)，该导数是严格为正的。这意味着 $g(\\lambda)$ 在每个开区间 $(|y|_{(k)}, |y|_{(k+1)})$ 上是严格递增的。\n对于 $\\lambda  |y|_{(n)}$，所有 $|y_i| \\le \\lambda$，所以 $g(\\lambda) = \\sum_{i=1}^n y_i^2$，这是一个常数。\n由于该函数在点集 $\\{0, |y|_{(1)}, \\dots, |y|_{(n)}\\}$ 之间的区间上是递增的，并且在最后一个点之后是常数，所以 $g(\\lambda)$ 在 $\\lambda \\ge 0$ 上的全局最小值必须在这些点之一处取得。\n\n因此，我们需要找到使 $g(|y|_{(k)})$ 最小化的 $k \\in \\{0, 1, \\dots, n\\}$ 的值。我们定义准则 $S(k) = g(|y|_{(k)})$。\n在 $\\lambda = |y|_{(k)}$ 处：\n- $\\sum_{i=1}^n \\min(y_i^2, |y|_{(k)}^2) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)|y|_{(k)}^2$。\n- 项 $\\sum_{i=1}^n \\mathbf{1}_{\\{|y_i|  |y|_{(k)}\\}}$ 是绝对值严格大于 $|y|_{(k)}$ 的坐标数量。这取决于 $|y|_{(j)}$ 的值是否唯一。然而，$g(\\lambda)$ 在区间 $[|y|_{(k)}, |y|_{(k+1)})$ 上的形式为 $g(\\lambda) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)(\\lambda^2+2\\sigma^2)$，它是连续且递增的。它在此区间上的最小值在 $\\lambda = |y|_{(k)}$ 处取得。其值为 $g(|y|_{(k)}) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)(|y|_{(k)}^2+2\\sigma^2)$。这个表达式正确地处理了相等的情况，并可作为在 $k$ 上进行最小化的准则。\n所以，我们需要找到 $\\hat{k} = \\operatorname{argmin}_{k \\in \\{0, 1, \\dots, n\\}} S(k)$，其中\n$$\nS(k) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)\\left(|y|_{(k)}^2 + 2\\sigma^2\\right)\n$$\n并约定 $|y|_{(0)} = 0$ 和 $\\sum_{j=1}^0 (\\cdot) = 0$。最小化阈值则为 $\\hat{\\lambda} = |y|_{(\\hat{k})}$。这构成了一个闭式解析表达式，因为它指定了一个直接的计算过程，涉及排序和有限次的比较，而非迭代优化。\n\n因此，最终表达式为 $\\hat{\\lambda}=|y|_{(\\hat{k})}$，其中 $\\hat{k}$ 是使显式函数 $S(k)$ 最小化的索引。", "answer": "$$\n\\boxed{\\hat{\\lambda} = |y|_{(\\hat{k})}, \\quad \\text{其中 } \\hat{k} = \\underset{k \\in \\{0, 1, \\dots, n\\}}{\\operatorname{argmin}} \\left\\{ \\sum_{j=1}^{k} |y|_{(j)}^{2} + (n-k)\\left(|y|_{(k)}^{2} + 2\\sigma^{2}\\right) \\right\\}}\n$$", "id": "3452918"}, {"introduction": "模型选择准则并非“一刀切”的解决方案，它们的设计往往服务于不同的目标。本练习从计算转向高层次的辨析与解读，通过一个典型的高维场景，探讨了交叉验证（CV）与扩展贝叶斯信息准则（EBIC）之间的本质差异。它旨在阐明优化预测准确性（CV的目标）与保证模型选择一致性（EBIC的目标）之间的根本权衡。理解这种权衡对于在现代统计学和机器学习应用中，根据具体问题目标来选择合适的模型选择策略至关重要。[@problem_id:3452881]", "problem": "考虑高维线性模型 $y = X \\beta^{\\star} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，$X \\in \\mathbb{R}^{n \\times p}$ 的列已标准化，$n = 120$，$p = 1000$，且 $\\sigma = 1$。真实系数向量 $\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是 $s$-稀疏的，有 $s = 6$ 个非零项，位于一个未知的支撑集 $S^{\\star}$ 上，其大小分别为 $\\{3.0, -3.0, 2.5, -2.0, 2.0, -1.5\\}$。设计矩阵 $X$ 具有以下结构：各列近似标准化且相互独立，但有三个空特征（即索引在 $S^{\\star}$ 之外）与第一个真实特征之间存在 $\\rho = 0.6$ 的两两相关性，而在其他方面不相关。一位从业者沿着惩罚参数 $\\lambda$ 的网格拟合了最小绝对收缩和选择算子 (Lasso)，并通过 10 折交叉验证 (CV) 或扩展贝叶斯信息准则 (EBIC)（超参数 $\\gamma = 1$）来选择 $\\lambda$。\n\n在此数据生成过程的一次代表性试验中，10 折交叉验证选择了一个包含 $k_{\\mathrm{CV}} = 11$ 个非零系数的模型，该模型包含了所有 6 个真实特征以及 5 个相关的空特征，并实现了 $1.06 \\sigma^{2}$ 的估计测试均方预测误差。扩展贝叶斯信息准则选择了一个包含 $k_{\\mathrm{EBIC}} = 6$ 个非零系数的模型，该模型与真实支撑集 $S^{\\star}$ 匹配，并实现了 $1.12 \\sigma^{2}$ 的估计测试均方预测误差。假设限制性特征值和相容性条件对真实支撑集及其小的超集成立，并且非零系数超过了在适当调整的惩罚下用于支撑集恢复的常规 $\\ell_{\\infty}$-信号强度阈值。\n\n从第一性原理出发，运用预测风险的偏差-方差分解和惩罚似然准则的解释，推断为何这两种选择程序在这种情况下会偏好不同大小的模型。以下哪种陈述最准确地解释了为什么在这里 EBIC 选择的模型比交叉验证更小，并从预测与识别的偏差-方差权衡角度解释了这种差异？\n\nA. 交叉验证旨在最小化样本外预测风险，因此它会容忍额外的相关空特征，如果这些特征能轻微减少偏差从而降低估计的预测误差，即使这是以增加方差为代价。EBIC 施加了随模型大小和组合模型空间而扩展的强复杂度惩罚，优先考虑支撑集识别和模型选择一致性；因此，它偏好更小的真实模型，可能在有限样本中以微小的预测风险为代价。\n\nB. 在 $p \\gg n$ 的高维设置中，交叉验证对于精确支撑集恢复是一致的，因此它必然能识别出最小的正确模型；相比之下，EBIC 优化预测风险，因此会扩大所选模型以对冲偏差。\n\nC. EBIC 产生的预测误差总是严格高于交叉验证，无论是在有限样本中还是在渐近情况下，因为其惩罚在渐近时会主导似然函数并导致欠拟合。\n\nD. 在存在相关特征的情况下，EBIC 会扩大模型大小以吸收共线性效应，而交叉验证会缩小模型大小以稳定方差；因此，在这里 EBIC 本应选择更大的模型，观察到的结果与标准理论相矛盾。\n\n选择唯一的最佳选项。", "solution": "用户需要对问题陈述进行批判性验证，然后进行第一性原理推导并评估所提供的选项。\n\n### 问题验证\n\n**第 1 步：提取给定信息**\n- **模型：** 高维线性模型 $y = X \\beta^{\\star} + \\varepsilon$。\n- **噪声：** $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。\n- **噪声方差：** $\\sigma = 1$。\n- **维度：** 样本量 $n = 120$，特征数量 $p = 1000$。\n- **设计矩阵：** $X \\in \\mathbb{R}^{n \\times p}$ 的列已标准化。\n- **真实系数向量 $\\beta^{\\star}$：** $\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是 $s$-稀疏的，稀疏度为 $s = 6$。\n- **真实支撑集：** $\\beta^{\\star}$ 的支撑集，表示为 $S^{\\star}$，是未知的。\n- **真实非零系数：** 非零项的大小为 $\\{3.0, -3.0, 2.5, -2.0, 2.0, -1.5\\}$。\n- **$X$ 的相关性结构：** 各列近似相互独立，例外情况是三个空特征（索引 $j \\notin S^{\\star}$）与第一个真实特征表现出 $\\rho = 0.6$ 的两两相关性。这三个空特征在其他方面不相关。\n- **方法论：** 使用 Lasso 沿惩罚参数 $\\lambda$ 的网格拟合模型。\n- **模型选择准则：** 10 折交叉验证 (CV) 和扩展贝叶斯信息准则 (EBIC)，超参数 $\\gamma = 1$。\n- **一次代表性试验中的观察结果：**\n    - **CV 结果：** 选择一个有 $k_{\\mathrm{CV}} = 11$ 个非零系数的模型，其中包括所有 $s=6$ 个真实特征外加 5 个被称为“相关的”空特征。估计的测试均方预测误差 (MSPE) 为 $1.06 \\sigma^{2}$。\n    - **EBIC 结果：** 选择一个有 $k_{\\mathrm{EBIC}} = 6$ 个非零系数的模型，正确识别了真实支撑集 $S^{\\star}$。估计的测试 MSPE 为 $1.12 \\sigma^{2}$。\n- **技术假设：** 限制性特征值 (RE) 和相容性条件对 $S^{\\star}$ 及其小的超集成立。非零系数的信号强度足以进行支撑集恢复。\n\n**第 2 步：使用提取的给定信息进行验证**\n- **科学依据性：** 问题牢固地植根于高维统计和正则化回归这一成熟领域。线性模型、高斯噪声、Lasso 估计器、CV 和 EBIC 都是该领域理论和实践的标准组成部分。所描述的具有相关特征的场景是用于研究模型选择方法性能的经典设置。所有方面在科学上都是合理的。\n- **良定性：** 问题是良定的。它要求通过诉诸两种选择方法的已知理论性质，对一个具体的、描述清晰的结果进行概念性解释。所提供的数据足以进行有原则的推理过程。\n- **客观性：** 问题以客观、技术性的语言陈述。场景和结果作为一次“代表性试验”的经验事实呈现，为科学解释提供了坚实的基础，没有引入主观性。\n\n**关于缺陷的结论：**\n1.  **科学或事实上的不健全：** 无。该设置是统计学习理论中标准且现实的模拟场景。\n2.  **不可形式化或不相关：** 无。问题直接切题，并且可以在统计模型选择的框架内形式化。\n3.  **不完整或矛盾的设置：** 无。问题提供了所有必要信息，以推断 CV 和 EBIC 的不同行为。关于“5 个相关的空特征”的描述，而明确定义为相关的只有 3 个，这是“代表性试验”叙述中的一个微小不精确之处，但这不会造成矛盾或妨碍逻辑分析；核心点是 CV 选择了与真实信号相关的假阳性。\n4.  **不现实或不可行：** 无。参数（$n, p, s, \\sigma, \\rho$）和报告的结果对于这类统计实验是完全现实的。\n5.  **病态或结构不良：** 无。问题清晰，并根据既定理论导向一个唯一的最佳解释。\n6.  **伪深刻、琐碎或同义反复：** 无。问题探讨了预测准确性和模型选择一致性之间的一个根本性且非琐碎的权衡，这是现代统计学中的一个关键概念。\n7.  **超出科学可验证性范围：** 无。CV 和 EBIC 的性质是数学上定义和广泛研究的。\n\n**第 3 步：结论和行动**\n问题陈述是 **有效的**。将推导完整的解决方案。\n\n### 推导与选项分析\n\n问题的核心在于理解交叉验证 (CV) 和扩展贝叶斯信息准则 (EBIC) 试图优化的不同目标函数。\n\n**第一性原理：偏差-方差权衡与模型选择准则**\n\n对于新观测 $x_0$ 的估计器 $\\hat{y}(x_0) = x_0^T \\hat{\\beta}$ 的均方预测误差 (MSPE) 可以分解。对训练数据和新观测噪声的分布取平均，期望 MSPE 为：\n$$ E[\\{y_0 - \\hat{y}(x_0)\\}^2] = \\sigma^2 + (x_0^T \\beta^{\\star} - E[x_0^T \\hat{\\beta}])^2 + E[(x_0^T \\hat{\\beta} - E[x_o^T \\hat{\\beta}])^2] $$\n$$ \\text{期望 MSPE} = \\text{不可约误差} + (\\text{偏差})^2 + \\text{方差} $$\n\n1.  **交叉验证 (CV)：** $K$ 折 CV 旨在找到一个能最小化样本外预测误差估计值的模型。它被设计为对预测最优。在高维设置 ($p \\gg n$) 中，CV 通常对于模型选择是*不一致*的（即，它不能可靠地恢复真实的支撑集 $S^{\\star}$）。它经常选择比真实模型更大的模型。发生这种情况是因为包含一些空变量，特别是那些与真实预测变量相关的变量，可以减少整体预测 $\\hat{y}$ 的估计偏差。Lasso 会将真实预测变量的系数向零收缩，这引入了向下的偏差。通过包含一个相关的空变量，模型可以利用这个额外的特征来“吸收”一些被收缩的真实系数无法完全捕捉的信号。这可能导致预测的整体 $(\\text{Bias})^2$ 项减少。如果这种偏差的减少大于因估计一个额外参数而导致的相应方差增加，那么总的 MSPE 将会降低。CV 作为估计 MSPE 的优化器，会偏爱这个更大、稍欠简约的模型。在问题中，CV 选择了一个大小为 $k_{\\mathrm{CV}} = 11$ 的模型，并获得了比真实模型（$1.12 \\sigma^2$）更低的估计 MSPE（$1.06 \\sigma^2$），这是这种行为的典型表现。\n\n2.  **扩展贝叶斯信息准则 (EBIC)：** EBIC 是一种信息准则，专为高维设置下的模型选择一致性而设计。对于一个大小为 $k=|\\mathcal{M}|$ 的候选模型 $\\mathcal{M}$，其公式为：\n    $$ \\text{EBIC}_{\\gamma}(\\mathcal{M}) = \\text{RSS}(\\mathcal{M}) + k \\log(n) + 2 \\gamma k \\log(p) $$\n    （这里我们使用残差平方和 (RSS) 的形式，对于高斯模型，这在常数和缩放因子上等价于似然形式）。关键部分是惩罚项，它有两个部分：标准的 BIC 惩罚 $k \\log(n)$ 和一个额外的惩罚 $2 \\gamma k \\log(p)$，该惩罚考虑了当 $p$ 很大时可能存在的巨大模型数量。当 $\\gamma  0$ 时（这里 $\\gamma=1$），对模型复杂度的这种惩罚非常严厉。它不仅随模型大小 $k$ 增加，还随总特征数 $p$ 的对数增加。这种强惩罚的目的是控制在大小为 $k$ 的 $\\binom{p}{k}$ 个可能模型中错误包含特征的全族错误率。通过如此严厉地惩罚复杂度，EBIC 的主要目标变成识别能够充分解释数据的最简约模型，在适当条件下，这就是真实的稀疏模型。这样做，它优先考虑选择一致性，而不是在有限样本中实现最小可能的预测误差。问题陈述 EBIC 选择了大小为 $k_{\\mathrm{EBIC}} = 6$ 的真实模型。为这种模型识别的正确性付出的代价是略高的有限样本预测误差（$1.12 \\sigma^2$），这展示了识别与预测之间的权衡。\n\n**与问题情景的联系**\n-   CV 发现，在大小为 6 的真实模型基础上增加 5 个空特征（很可能是 3 个高度相关的特征和另外 2 个特征）能将估计的预测误差从 $1.12\\sigma^2$ 减少到 $1.06\\sigma^2$。它正确地实现了其最小化预测风险的目标。\n-   EBIC 在面对大小为 $k=6$ 的模型和大小为 $k=11$ 的模型时，评估权衡的方式不同。增加 5 个空特征带来的 RSS 减少不足以克服惩罚项的大幅增加，对于 $\\gamma=1$，惩罚项的增加为 $(11-6)\\log(120) + 2(11-6)\\log(1000) = 5\\log(120) + 10\\log(1000) \\approx 5(4.79) + 10(6.91) \\approx 23.95 + 69.1 = 93.05$。EBIC 正确地偏好更简约——并且在这种情况下是真实的——大小为 6 的模型。\n\n### 逐项分析\n\n**A. 交叉验证旨在最小化样本外预测风险，因此它会容忍额外的相关空特征，如果这些特征能轻微减少偏差从而降低估计的预测误差，即使这是以增加方差为代价。EBIC 施加了随模型大小和组合模型空间而扩展的强复杂度惩罚，优先考虑支撑集识别和模型选择一致性；因此，它偏好更小的真实模型，可能在有限样本中以微小的预测风险为代价。**\n-   这个陈述是对理论推理的精确和准确总结。它正确地指出了 CV 的目标是最小化预测风险，并解释了为什么这可以通过偏差-方差权衡导致包含相关的空特征。它正确地将 EBIC 的强惩罚识别为实现模型选择一致性的工具，这解释了它为何偏好更小的真实模型，并以在这个有限样本案例中略高的预测误差为代价。\n-   **结论：正确。**\n\n**B. 在 $p \\gg n$ 的高维设置中，交叉验证对于精确支撑集恢复是一致的，因此它必然能识别出最小的正确模型；相比之下，EBIC 优化预测风险，因此会扩大所选模型以对冲偏差。**\n-   这个陈述颠倒了两种方法的作用。众所周知，交叉验证在许多高维设置中对于支撑集恢复是不一致的，经常选择过大的模型。问题本身就提供了一个反例（$k_{\\mathrm{CV}}=11 \\neq 6$）。相反，EBIC（当 $\\gamma  0$ 时）是专门为选择一致性而设计的，并使用强惩罚来*避免*模型膨胀，而不是不惜一切代价优化预测风险。\n-   **结论：不正确。**\n\n**C. EBIC 产生的预测误差总是严格高于交叉验证，无论是在有限样本中还是在渐近情况下，因为其惩罚在渐近时会主导似然函数并导致欠拟合。**\n-   声称 EBIC 在有限样本中“总是”产生更高的预测误差过于绝对，并非普遍成立，尽管这种情况很常见。更重要的是，渐近的说法是错误的。在适当条件下，EBIC 在渐近上是选择一致的，意味着它能找到真实模型。而 CV 找到的模型可能不是真实模型。认为 EBIC“导致欠拟合”是一种误解；它强制简约以找到正确的稀疏模型，当真实模型确实是稀疏的时，这与欠拟合恰恰相反。\n-   **结论：不正确。**\n\n**D. 在存在相关特征的情况下，EBIC 会扩大模型大小以吸收共线性效应，而交叉验证会缩小模型大小以稳定方差；因此，在这里 EBIC 本应选择更大的模型，观察到的结果与标准理论相矛盾。**\n-   这个陈述在所有方面都是事实错误的。EBIC 的强惩罚积极地*阻止*模型膨胀。交叉验证并不会系统性地“缩小”模型大小；它选择能最小化预测误差的大小，正如所示，这个大小可能比真实大小更大。观察到的结果，即 EBIC 比 CV 更简约，完全符合——实际上是教科书级的例证——标准理论。\n-   **结论：不正确。**", "answer": "$$\\boxed{A}$$", "id": "3452881"}]}