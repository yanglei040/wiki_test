{"hands_on_practices": [{"introduction": "稀疏恢复的理论保证，例如基于限制等距性质 (Restricted Isometry Property, RIP) 的保证，通常显得较为抽象。本练习提供了一种连接理论与实践的具体方法。通过经验性地估计 RIP 常数的代理指标，并观察其如何与恢复算法的实际成功率直接相关，学生们可以将这一核心理论概念变得触手可及。", "problem": "实现一个完整、可运行的程序，通过经验性研究来探究受限等距性质（RIP）常数的一个代理（proxy）与压缩感知中稀疏恢复算法的观测性能之间的相关性。仅在数学模型中进行操作，其中测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 作用于一个 $k$-稀疏向量 $x_{0} \\in \\mathbb{R}^{n}$，产生无噪声的测量值 $y = A x_{0}$。目标是估计一个经验性受限等距常数代理，并将其与经验恢复概率和重构误差联系起来。\n\n仅使用以下基本依据和定义：\n- 如果一个向量 $x \\in \\mathbb{R}^{n}$ 最多有 $k$ 个非零项，则称其为 $k$-稀疏的。\n- 阶数为 $k$ 的受限等距性质（RIP）常数，记为 $\\delta_{k}$，是满足对所有 $k$-稀疏向量 $x$ 均有\n$$\n(1 - \\delta_{k}) \\lVert x \\rVert_{2}^{2} \\le \\lVert A x \\rVert_{2}^{2} \\le (1 + \\delta_{k}) \\lVert x \\rVert_{2}^{2}\n$$\n的最小非负数。\n- 等价地，对于任意支撑集 $S \\subset \\{1,\\dots,n\\}$ 且 $\\lvert S \\rvert = k$，如果 $A_{S}$ 表示由 $A$ 中索引为 $S$ 的列构成的子矩阵，那么当 $A$ 被适当缩放时，格拉姆矩阵 (Gram matrix) $G_{S} = A_{S}^{\\top} A_{S}$ 满足其所有特征值都位于区间 $[1 - \\delta_{k}, 1 + \\delta_{k}]$ 内。因此，可以通过对随机支撑集 $S$ 进行采样，并计算 $G_{S}$ 的特征值与 1 之间的极值偏差，来获得 $\\delta_{k}$ 的一个经验代理。\n\n为每个测试用例设计程序以执行以下操作：\n1. 构造一个指定类型的测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$，并将其列归一化，使 $\\ell_{2}$ 范数为单位1。\n2. 通过蒙特卡洛（Monte Carlo）方法估计经验 RIP 代理 $\\widehat{\\delta}_{k}$：\n   - 对从 $\\{1,\\dots,n\\}$ 中无放回均匀采样的 $N_{\\text{RIP}}$ 个大小为 $k$ 的随机支撑集 $S$ 重复以下操作：\n     - 构建 $G_{S} = A_{S}^{\\top} A_{S}$，并计算其最小和最大特征值 $\\lambda_{\\min}(G_{S})$ 和 $\\lambda_{\\max}(G_{S})$。\n     - 对于该支撑集，定义偏差 $d(S) = \\max\\{ \\lvert \\lambda_{\\min}(G_{S}) - 1 \\rvert, \\lvert \\lambda_{\\max}(G_{S}) - 1 \\rvert \\}$。\n   - 在采样的支撑集上，设 $\\widehat{\\delta}_{k} = \\max_{S} d(S)$。\n3. 在 $T$ 次独立试验中，使用正交匹配追踪（OMP）评估经验恢复性能：\n   - 对于每次试验，通过均匀随机选择一个大小为 $k$ 的支撑集，在该支撑集上赋予独立的标准正态系数，并缩放至 $\\lVert x_{0} \\rVert_{2} = 1$，来抽取一个随机的 $k$-稀疏向量 $x_{0} \\in \\mathbb{R}^{n}$。\n   - 生成 $y = A x_{0}$（无噪声）。\n   - 使用 $k$ 次 OMP 迭代从 $(A,y)$ 中恢复 $\\widehat{x}$。\n   - 计算相对重构误差 $e = \\lVert \\widehat{x} - x_{0} \\rVert_{2} / \\lVert x_{0} \\rVert_{2}$。\n   - 如果 $e \\le \\tau$，则声明成功，其中 $\\tau$ 是一个固定的阈值。\n   - 汇总经验成功概率 $\\widehat{p} = (\\text{成功次数}) / T$ 和平均相对误差 $\\overline{e} = \\frac{1}{T} \\sum e$。\n4. 通过计算 $\\widehat{\\delta}_{k}$ 值向量与 $\\widehat{p}$ 值向量之间的皮尔逊相关系数，以及 $\\widehat{\\delta}_{k}$ 值向量与 $\\overline{e}$ 值向量之间的皮尔逊相关系数，来关联整个测试套件中经验 RIP 代理与性能。在相关性计算中，将每个测试用例视为一个样本。\n\n使用的矩阵构造方法（构造后所有列都必须归一化为单位 $\\ell_{2}$ 范数）：\n- 类型 “gaussian”：元素独立地从标准正态分布中抽取，然后对列进行归一化。\n- 类型 “bernoulli”：元素独立地从等概率取值 $\\{-1,+1\\}$ 的拉德马赫（Rademacher）分布中抽取，然后对列进行归一化。\n- 类型 “correlated”：从一个高斯矩阵开始，然后通过以下方式引入列相关性\n$$\nA = (1 - \\rho) A_{0} + \\rho \\, v \\mathbf{1}^{\\top},\n$$\n其中 $A_{0}$ 具有独立的标准正态元素，$v \\in \\mathbb{R}^{m}$ 是一个随机标准正态向量，$\\mathbf{1} \\in \\mathbb{R}^{n}$ 是全一向量；然后对列进行归一化。\n\n算法约束：\n- 使用正交匹配追踪（OMP）进行恢复。实现的 OMP 在每次迭代中，选择与当前残差具有最大绝对相关性的列，更新活动集，在活动集上解决最小二乘子问题，并更新残差。在 $k$ 次迭代后或当残差范数低于数值容差时停止。\n- 使用适用于对称矩阵的特征值计算方法来评估 $G_{S}$ 的 $\\lambda_{\\min}$ 和 $\\lambda_{\\max}$。\n\n所有实验中使用的数值参数：\n- 成功容差：$\\tau = 10^{-3}$。\n- OMP 的残差停止容差：$10^{-8}$。\n- 每个测试用例中用于 RIP 代理的蒙特卡洛支撑集数量：$N_{\\text{RIP}} = 80$。\n- 每个测试用例的恢复试验次数：$T = 40$。\n- 完全按照测试套件中的规定使用提供的种子以确保可复现性。\n\n要实现的测试套件：\n- 用例 A：类型 “gaussian”，$m = 32$，$n = 64$，$k = 4$，$\\rho$ 被忽略，种子 $= 1$。\n- 用例 B：类型 “gaussian”，$m = 32$，$n = 64$，$k = 10$，$\\rho$ 被忽略，种子 $= 2$。\n- 用例 C：类型 “bernoulli”，$m = 32$，$n = 64$，$k = 8$，$\\rho$ 被忽略，种子 $= 3$。\n- 用例 D：类型 “correlated”，$m = 32$，$n = 64$，$k = 8$，$\\rho = 0.6$，种子 $= 4$。\n\n最终输出规格：\n- 对于每个测试用例，生成一个包含三个浮点数的列表 $[\\widehat{\\delta}_{k}, \\widehat{p}, \\overline{e}]$，每个浮点数四舍五入到三位小数。\n- 在所有测试用例之后，计算另外两个浮点数：$\\widehat{\\delta}_{k}$ 值向量与 $\\widehat{p}$ 值向量之间的皮尔逊相关系数，以及 $\\widehat{\\delta}_{k}$ 值向量与 $\\overline{e}$ 值向量之间的皮尔逊相关系数，每个浮点数四舍五入到三位小数。\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表被方括号括起来，第一项是按 A、B、C、D 顺序排列的每个测试用例的三元组列表，后跟两个相关系数。例如，结构必须是\n$$\n\\big[ \\big[ [\\widehat{\\delta}_{k}^{A}, \\widehat{p}^{A}, \\overline{e}^{A}], [\\widehat{\\delta}_{k}^{B}, \\widehat{p}^{B}, \\overline{e}^{B}], [\\widehat{\\delta}_{k}^{C}, \\widehat{p}^{C}, \\overline{e}^{C}], [\\widehat{\\delta}_{k}^{D}, \\widehat{p}^{D}, \\overline{e}^{D}] \\big], r_{\\delta,p}, r_{\\delta,e} \\big],\n$$\n打印在一行中，所有浮点数四舍五入到三位小数。不涉及单位。不涉及角度。不要用百分比表示任何量；仅使用小数。", "solution": "我们从受限等距性质（RIP）的核心定义开始。对于一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其 $k$ 阶 RIP 常数 $\\delta_{k}$ 是满足对所有 $k$-稀疏向量 $x \\in \\mathbb{R}^{n}$ 均有\n$$\n(1 - \\delta) \\lVert x \\rVert_{2}^{2} \\le \\lVert A x \\rVert_{2}^{2} \\le (1 + \\delta) \\lVert x \\rVert_{2}^{2}\n$$\n的最小 $\\delta$。\n等价地，对于任意支撑集 $S \\subset \\{1,\\dots,n\\}$ 且 $\\lvert S \\rvert = k$，如果 $A_{S}$ 是由 $A$ 中索引为 $S$ 的列构成的子矩阵，且 $G_{S} = A_{S}^{\\top} A_{S}$ 是格拉姆矩阵，那么 $\\delta_{k}$ 是使得 $G_{S}$ 的每个特征值都属于 $[1 - \\delta, 1 + \\delta]$ 的最小 $\\delta$。这可以从恒等式 $\\lVert A x \\rVert_{2}^{2} = x^{\\top} A^{\\top} A x$ 和瑞利商（Rayleigh quotient）界得出，因为对于支撑在 $S$ 上的 $k$-稀疏向量 $x$，有 $\\lVert A x \\rVert_{2}^{2} = x_{S}^{\\top} G_{S} x_{S}$，其在所有非零 $x_{S}$ 上的极值恰好是 $G_{S}$ 的极值特征值。因此，可以通过对支撑集 $S$ 进行采样，并测量 $G_{S}$ 的谱与 1 的偏离程度，来估计 $\\delta_{k}$ 的一个代理。\n\n这引出了以下经验估计器。固定 $N_{\\text{RIP}}$，无放回地均匀随机采样大小为 $k$ 的支撑集 $S_{1},\\dots,S_{N_{\\text{RIP}}}$，对每个 $S_{i}$ 计算格拉姆矩阵 $G_{S_{i}} = A_{S_{i}}^{\\top} A_{S_{i}}$，找到 $\\lambda_{\\min}(G_{S_{i}})$ 和 $\\lambda_{\\max}(G_{S_{i}})$，并定义\n$$\nd(S_{i}) = \\max \\{ \\lvert \\lambda_{\\min}(G_{S_{i}}) - 1 \\rvert, \\lvert \\lambda_{\\max}(G_{S_{i}}) - 1 \\rvert \\}.\n$$\n经验 RIP 代理则为\n$$\n\\widehat{\\delta}_{k} = \\max_{i \\in \\{1,\\dots,N_{\\text{RIP}}\\}} d(S_{i}).\n$$\n对 $A$ 的列进行单位 $\\ell_{2}$ 范数归一化，以保持尺度一致，确保当列近似正交时，格拉姆矩阵 $G_{S}$ 趋近于单位矩阵，从而使 1 成为其特征值的自然参考值。\n\n对于恢复性能，我们采用正交匹配追踪（OMP）。正交匹配追踪（OMP）是一种贪婪算法，其理论依据是：对于稀疏的 $x_{0}$，由 $x_{0}$ 的支撑集索引的 $A$ 的列往往与测量值 $y = A x_{0}$ 有很大的相关性。具体来说，OMP 迭代执行以下步骤：\n- 计算与残差 $r$ 的相关性 $A^{\\top} r$，\n- 选择具有最大绝对相关性幅值的列索引，\n- 扩充活动集，\n- 在活动集上解决一个最小二乘问题以更新当前估计，\n- 并更新残差。\n在有利条件下，经过 $k$ 次迭代，OMP 可以完美地恢复支撑集和系数。虽然严格的保证是以 RIP 或互相关性界的形式陈述的，但在此我们研究的是经验性能及其与经验 RIP 代理的相关性。\n\n该程序为每个测试用例实现以下步骤：\n1. 生成指定类型的矩阵 $A$：\n   - “gaussian”：元素 $A_{ij} \\sim \\mathcal{N}(0,1)$。\n   - “bernoulli”：元素 $A_{ij} \\in \\{-1,+1\\}$，等概率。\n   - “correlated”：抽取 $A_{0}$（其元素为 $\\mathcal{N}(0,1)$）和 $v \\sim \\mathcal{N}(0, I_{m})$，然后设 $A = (1 - \\rho) A_{0} + \\rho \\, v \\mathbf{1}^{\\top}$，其中 $\\mathbf{1}$ 是 $\\mathbb{R}^{n}$ 中的全一向量。\n   构造后，将 $A$ 的每一列归一化为单位 $\\ell_{2}$ 范数以稳定尺度。\n2. 使用 $N_{\\text{RIP}}$ 个随机支撑集，通过构建 $G_{S}$ 并使用适用于实对称矩阵的对称特征值程序提取其极值特征值，来计算 $\\widehat{\\delta}_{k}$。\n3. 对于 $T$ 次独立试验，抽取一个随机的 $k$-稀疏向量 $x_{0}$（在大小为 $k$ 的随机支撑集上具有标准正态系数），将 $x_{0}$ 归一化为单位 $\\ell_{2}$ 范数，设 $y = A x_{0}$，并运行 $k$ 步 OMP（如果残差范数低于 $10^{-8}$ 则提前停止）。计算相对误差 $e = \\lVert \\widehat{x} - x_{0} \\rVert_{2} / \\lVert x_{0} \\rVert_{2}$ 和成功指示符 $\\mathbb{I}\\{ e \\le \\tau \\}$，其中 $\\tau = 10^{-3}$。汇总经验成功概率 $\\widehat{p}$ 和平均相对误差 $\\overline{e}$。\n4. 处理完所有测试用例后，计算列表 $[\\widehat{\\delta}_{k}^{A}, \\widehat{\\delta}_{k}^{B}, \\widehat{\\delta}_{k}^{C}, \\widehat{\\delta}_{k}^{D}]$ 与 $[\\widehat{p}^{A}, \\widehat{p}^{B}, \\widehat{p}^{C}, \\widehat{p}^{D}]$ 之间的皮尔逊相关系数 $r_{\\delta,p}$，以及 $\\widehat{\\delta}_{k}$ 值列表与 $[\\overline{e}^{A}, \\overline{e}^{B}, \\overline{e}^{C}, \\overline{e}^{D}]$ 之间的皮尔逊相关系数 $r_{\\delta,e}$。如果由于某个列表中的方差为零导致相关系数分母在数值上为零，则将相关性定义为 $0$ 以避免未定义值。\n\n测试套件规定：\n- 用例 A：类型 “gaussian”，$m = 32$，$n = 64$，$k = 4$，种子 $= 1$。\n- 用例 B：类型 “gaussian”，$m = 32$，$n = 64$，$k = 10$，种子 $= 2$。\n- 用例 C：类型 “bernoulli”，$m = 32$，$n = 64$，$k = 8$，种子 $= 3$。\n- 用例 D：类型 “correlated”，$m = 32$，$n = 64$，$k = 8$，$\\rho = 0.6$，种子 $= 4$。\n\n对于每个用例，我们使用 $N_{\\text{RIP}} = 80$ 个支撑集，$T = 40$ 次试验，$\\tau = 10^{-3}$，以及 OMP 残差容差 $10^{-8}$。\n\n最终程序输出单行，其结构为\n$$\n\\big[ \\big[ [\\widehat{\\delta}_{k}^{A}, \\widehat{p}^{A}, \\overline{e}^{A}], [\\widehat{\\delta}_{k}^{B}, \\widehat{p}^{B}, \\overline{e}^{B}], [\\widehat{\\delta}_{k}^{C}, \\widehat{p}^{C}, \\overline{e}^{C}], [\\widehat{\\delta}_{k}^{D}, \\widehat{p}^{D}, \\overline{e}^{D}] \\big], r_{\\delta,p}, r_{\\delta,e} \\big],\n$$\n所有浮点数四舍五入到三位小数。此设计将经验 RIP 代理与测量的恢复性能直接联系起来，并量化了不同结构矩阵系综之间的相关性，使研究基于核心定义和蒙特卡洛估计，而不依赖未经证实的启发式方法或无关的假设。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef normalize_columns(A: np.ndarray) - np.ndarray:\n    \"\"\"Normalize columns of A to unit l2 norm.\"\"\"\n    norms = np.linalg.norm(A, axis=0)\n    norms[norms == 0.0] = 1.0\n    return A / norms\n\ndef make_matrix(m: int, n: int, kind: str, rho: float, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Construct matrix A of specified kind and normalize columns.\"\"\"\n    if kind == \"gaussian\":\n        A = rng.standard_normal((m, n))\n    elif kind == \"bernoulli\":\n        A = rng.choice([-1.0, 1.0], size=(m, n))\n    elif kind == \"correlated\":\n        A0 = rng.standard_normal((m, n))\n        v = rng.standard_normal((m, 1))\n        A = (1.0 - rho) * A0 + rho * v @ np.ones((1, n))\n    else:\n        raise ValueError(f\"Unknown matrix kind: {kind}\")\n    A = normalize_columns(A)\n    return A\n\ndef empirical_delta_k(A: np.ndarray, k: int, num_samples: int, rng: np.random.Generator) - float:\n    \"\"\"Monte Carlo estimate of empirical RIP constant proxy via Gram eigenvalues.\"\"\"\n    m, n = A.shape\n    if k == 0 or k  n:\n        return float('nan')\n    delta_max = 0.0\n    # Precompute A^T A could be heavy; we sample supports and compute Gram via A_S.T @ A_S\n    for _ in range(num_samples):\n        S = rng.choice(n, size=k, replace=False)\n        AS = A[:, S]\n        G = AS.T @ AS  # symmetric k x k\n        # Use eigenvalues for symmetric matrices\n        eigvals = np.linalg.eigvalsh(G)\n        dev = max(abs(eigvals[0] - 1.0), abs(eigvals[-1] - 1.0))\n        if dev  delta_max:\n            delta_max = float(dev)\n    return float(delta_max)\n\ndef omp(A: np.ndarray, y: np.ndarray, k: int, tol: float = 1e-8) - np.ndarray:\n    \"\"\"Orthogonal Matching Pursuit up to k iterations or until residual below tol.\"\"\"\n    m, n = A.shape\n    residual = y.copy()\n    support = []\n    x_s = np.array([])\n    for _ in range(k):\n        correlations = A.T @ residual\n        # Zero out already selected to avoid reselection\n        if support:\n            correlations[np.array(support, dtype=int)] = 0.0\n        idx = int(np.argmax(np.abs(correlations)))\n        # Stop if idx already selected (degenerate), though we zeroed it; safeguard anyway\n        if idx in support:\n            break\n        support.append(idx)\n        AS = A[:, support]\n        # Least squares on active set\n        x_s, _, _, _ = np.linalg.lstsq(AS, y, rcond=None)\n        residual = y - AS @ x_s\n        if np.linalg.norm(residual) = tol:\n            break\n    x_hat = np.zeros(n)\n    if support:\n        x_hat[np.array(support, dtype=int)] = x_s\n    return x_hat\n\ndef generate_k_sparse(n: int, k: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Generate a random k-sparse unit-norm vector with standard normal nonzeros.\"\"\"\n    x = np.zeros(n)\n    support = rng.choice(n, size=k, replace=False)\n    coeffs = rng.standard_normal(k)\n    x[support] = coeffs\n    norm = np.linalg.norm(x)\n    if norm == 0:\n        # Extremely unlikely; regenerate\n        return generate_k_sparse(n, k, rng)\n    return x / norm\n\ndef pearson_corr(x: np.ndarray, y: np.ndarray) - float:\n    \"\"\"Compute Pearson correlation; return 0 if variance is zero.\"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    sx = np.std(x)\n    sy = np.std(y)\n    if sx  1e-15 or sy  1e-15:\n        return 0.0\n    cov = float(np.mean((x - np.mean(x)) * (y - np.mean(y))))\n    return cov / (sx * sy)\n\ndef run_case(kind: str, m: int, n: int, k: int, rho: float, seed: int,\n             num_rip_samples: int, num_trials: int, tau: float) - tuple[float, float, float]:\n    \"\"\"Run one test case: build A, estimate delta_k, run OMP trials to get success and mean errors.\"\"\"\n    rng = np.random.default_rng(seed)\n    A = make_matrix(m, n, kind, rho, rng)\n    delta_emp = empirical_delta_k(A, k, num_rip_samples, rng)\n    successes = 0\n    errors = []\n    for _ in range(num_trials):\n        x0 = generate_k_sparse(n, k, rng)\n        y = A @ x0\n        xhat = omp(A, y, k, tol=1e-8)\n        rel_err = float(np.linalg.norm(xhat - x0) / (np.linalg.norm(x0) + 1e-16))\n        errors.append(rel_err)\n        if rel_err = tau:\n            successes += 1\n    success_rate = successes / num_trials\n    mean_rel_error = float(np.mean(errors) if errors else float('nan'))\n    return float(delta_emp), float(success_rate), float(mean_rel_error)\n\ndef format_triplet(triplet: tuple[float, float, float]) - str:\n    \"\"\"Format a (delta, success_rate, mean_error) triplet to three decimals.\"\"\"\n    return \"[\" + \",\".join(f\"{v:.3f}\" for v in triplet) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (kind, m, n, k, rho, seed)\n    test_cases = [\n        (\"gaussian\", 32, 64, 4, 0.0, 1),\n        (\"gaussian\", 32, 64, 10, 0.0, 2),\n        (\"bernoulli\", 32, 64, 8, 0.0, 3),\n        (\"correlated\", 32, 64, 8, 0.6, 4),\n    ]\n    N_RIP = 80\n    T = 40\n    tau = 1e-3\n\n    results = []\n    for kind, m, n, k, rho, seed in test_cases:\n        delta_emp, success_rate, mean_rel_error = run_case(\n            kind, m, n, k, rho, seed, N_RIP, T, tau\n        )\n        results.append((delta_emp, success_rate, mean_rel_error))\n\n    # Compute correlations across test cases\n    deltas = np.array([r[0] for r in results], dtype=float)\n    succs = np.array([r[1] for r in results], dtype=float)\n    errs = np.array([r[2] for r in results], dtype=float)\n    corr_delta_success = pearson_corr(deltas, succs)\n    corr_delta_error = pearson_corr(deltas, errs)\n\n    # Build final output string\n    per_case_str = \"[\" + \",\".join(format_triplet(r) for r in results) + \"]\"\n    final_str = f\"[{per_case_str},{corr_delta_success:.3f},{corr_delta_error:.3f}]\"\n    print(final_str)\n\nsolve()\n```", "id": "3446301"}, {"introduction": "在不进行大量蒙特卡洛模拟的情况下，我们仍然可以分析算法的性能。本练习介绍了一种利用简化分析模型来预测关键性能指标（如真阳性率 True Positive Rate 和错误发现率 False Discovery Rate）的方法。通过对 LASSO 算法在不同信噪比下的表现进行理论计算，这个练习展示了如何定量地评估和预测算法的行为。", "problem": "考虑标准线性模型 $y = X \\beta^{\\star} + \\varepsilon$，其中 $X$ 是一个 $n \\times p$ 的设计矩阵，其列是标准正交的，$\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是一个系数向量，它有 $k$ 个大小为 $a$ 的非零项（符号任意），$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 是加性噪声。在标准正交列的条件下，最小绝对收缩和选择算子 (LASSO) 通过对统计量 $z_{i} = x_{i}^{\\top} y$ 进行软阈值处理来选择坐标 $i$；如果 $|z_{i}|$ 超过阈值 $\\lambda$，则一个特征被宣告为已发现。你将使用 $z_{i}$ 的零分布来确定一个能控制每个特征的错误包含概率的 $\\lambda$。\n\n给定 $p = 1000$，$k = 40$，$\\sigma = 1$ 以及由 $a / \\sigma \\in \\{0.5, 1.5, 3.0\\}$ 定义的三个信噪比 (SNR)，通过要求每个特征的零分布尾部概率等于 $\\tau = 0.01$ 来选择一个单一的阈值 $\\lambda$，也就是说，将一个零特征宣告为已发现的概率是 $\\mathbb{P}(|Z_{0}|  \\lambda) = \\tau$，其中 $Z_{0} \\sim \\mathcal{N}(0, \\sigma^{2})$。对于每个信噪比值，将信号坐标视为 $Z_{s} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$（其中 $\\mu = a$）并计算：\n- 真阳性率 $\\mathrm{TPR} = \\mathbb{P}(|Z_{s}|  \\lambda)$，\n- 近似期望错误发现率 $\\mathrm{FDR} \\approx \\dfrac{(p - k)\\,\\mathbb{P}(|Z_{0}|  \\lambda)}{(p - k)\\,\\mathbb{P}(|Z_{0}|  \\lambda) + k\\,\\mathbb{P}(|Z_{s}|  \\lambda)}$，\n\n假设坐标之间相互独立。定义每个信噪比下的 $F1$ 分数为 $F1 = \\dfrac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$，其中 $\\mathrm{precision} = 1 - \\mathrm{FDR}$ 且 $\\mathrm{recall} = \\mathrm{TPR}$。最后，报告通过对三个每个信噪比下的 $F1$ 分数求平均得到的宏平均 $F1$ 分数。将最终答案四舍五入到四位有效数字。", "solution": "用户提供了一个在统计信号处理和稀疏恢复领域中定义明确的问题。该问题具有科学依据、内容完整，并且所有参数都已指定。我将进行完整的解答。\n\n该问题要求在使用软阈值规则进行特征选择的情况下，计算宏平均 $F1$ 分数。软阈值规则是 LASSO 估计量的一个特征，这里是在一个简化的标准正交设计设置下。分析过程分为四个主要步骤：\n1.  根据指定的错误包含控制，确定选择阈值 $\\lambda$。\n2.  对于每个信噪比 (SNR)，计算真阳性率 (TPR)，也称为召回率或灵敏度。\n3.  对于每个信噪比，计算近似期望错误发现率 (FDR)、精确率以及相应的 $F1$ 分数。\n4.  计算 $F1$ 分数的宏平均值。\n\n问题提供了以下参数：总共 $p=1000$ 个特征，其中 $k=40$ 个是“信号”特征（非零系数），$p-k=960$ 个是“零”特征（零系数）。噪声标准差为 $\\sigma=1$。信号幅度由 $a/\\sigma \\in \\{0.5, 1.5, 3.0\\}$ 给出，因为 $\\sigma=1$，所以简化为 $a \\in \\{0.5, 1.5, 3.0\\}$。\n\n首先，我们确定阈值 $\\lambda$。它被设定为用于控制每个特征的错误包含概率。对于一个零特征，其对应的统计量 $Z_0$ 服从正态分布 $Z_0 \\sim \\mathcal{N}(0, \\sigma^2)$。如果 $|Z_0|  \\lambda$，则发生一次错误包含。该概率被设定为 $\\tau = 0.01$。\n$$ \\mathbb{P}(|Z_0|  \\lambda) = \\tau $$\n因为 $Z_0/\\sigma \\sim \\mathcal{N}(0, 1)$，我们可以对随机变量进行标准化。设 $\\Phi(\\cdot)$ 为标准正态分布的累积分布函数 (CDF)。\n$$ \\mathbb{P}\\left(\\left|\\frac{Z_0}{\\sigma}\\right|  \\frac{\\lambda}{\\sigma}\\right) = \\tau $$\n根据正态分布的对称性，这等价于：\n$$ 2 \\cdot \\mathbb{P}\\left(\\frac{Z_0}{\\sigma}  \\frac{\\lambda}{\\sigma}\\right) = \\tau $$\n$$ 1 - \\Phi\\left(\\frac{\\lambda}{\\sigma}\\right) = \\frac{\\tau}{2} $$\n$$ \\Phi\\left(\\frac{\\lambda}{\\sigma}\\right) = 1 - \\frac{\\tau}{2} $$\n对 $\\lambda$ 求解，我们得到：\n$$ \\lambda = \\sigma \\Phi^{-1}\\left(1 - \\frac{\\tau}{2}\\right) $$\n代入给定值 $\\sigma=1$ 和 $\\tau=0.01$：\n$$ \\lambda = 1 \\cdot \\Phi^{-1}\\left(1 - \\frac{0.01}{2}\\right) = \\Phi^{-1}(0.995) \\approx 2.575829 $$\n\n接下来，我们为三种信号强度 $a \\in \\{0.5, 1.5, 3.0\\}$ 中的每一种评估性能。为此，我们需要真阳性率 (TPR)，即正确识别一个信号特征的概率。信号特征的统计量 $Z_s$ 被建模为 $Z_s \\sim \\mathcal{N}(a, \\sigma^2)$。\n$$ \\mathrm{TPR} = \\mathbb{P}(|Z_s|  \\lambda) = \\mathbb{P}(Z_s  \\lambda) + \\mathbb{P}(Z_s  -\\lambda) $$\n对变量 $Z_s$ 进行标准化：\n$$ \\mathrm{TPR} = \\mathbb{P}\\left(\\frac{Z_s - a}{\\sigma}  \\frac{\\lambda - a}{\\sigma}\\right) + \\mathbb{P}\\left(\\frac{Z_s - a}{\\sigma}  \\frac{-\\lambda - a}{\\sigma}\\right) $$\n因为 $(Z_s - a)/\\sigma \\sim \\mathcal{N}(0, 1)$，我们有：\n$$ \\mathrm{TPR}(a) = \\left(1 - \\Phi\\left(\\frac{\\lambda - a}{\\sigma}\\right)\\right) + \\Phi\\left(\\frac{-\\lambda - a}{\\sigma}\\right) $$\n当 $\\sigma=1$ 时，这简化为：\n$$ \\mathrm{TPR}(a) = 1 - \\Phi(\\lambda - a) + \\Phi(-\\lambda - a) $$\n召回率定义为 $\\mathrm{recall} = \\mathrm{TPR}$。\n\n近似期望错误发现率 (FDR) 由下式给出：\n$$ \\mathrm{FDR} \\approx \\frac{(p - k)\\,\\mathbb{P}(|Z_{0}|  \\lambda)}{(p - k)\\,\\mathbb{P}(|Z_{0}|  \\lambda) + k\\,\\mathbb{P}(|Z_{s}|  \\lambda)} = \\frac{(p - k)\\tau}{(p-k)\\tau + k \\cdot \\mathrm{TPR}} $$\n精确率为 $\\mathrm{precision} = 1 - \\mathrm{FDR}$。$F1$ 分数是精确率和召回率的调和平均数：\n$$ F1 = \\frac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}} $$\n我们可以写出 $F1$ 分数的直接公式。首先，注意到 $\\mathrm{precision} = \\frac{k \\cdot \\mathrm{TPR}}{(p-k)\\tau + k \\cdot \\mathrm{TPR}}$。将这个以及 $\\mathrm{recall} = \\mathrm{TPR}$ 代入 $F1$ 公式并化简得到：\n$$ F1 = \\frac{2 k \\cdot \\mathrm{TPR}}{k + (p-k)\\tau + k \\cdot \\mathrm{TPR}} $$\n现在我们使用 $p=1000$，$k=40$，$\\tau=0.01$ 和 $\\lambda \\approx 2.575829$ 来计算每种情况下的值。期望的假阳性数量为 $(p-k)\\tau = (1000-40) \\times 0.01 = 960 \\times 0.01 = 9.6$。\n\n情况 1: $a=0.5$\n$$ \\mathrm{TPR}_1 = 1 - \\Phi(2.575829 - 0.5) + \\Phi(-2.575829 - 0.5) $$\n$$ \\mathrm{TPR}_1 = 1 - \\Phi(2.075829) + \\Phi(-3.075829) \\approx 1 - 0.981042 + 0.001051 \\approx 0.020009 $$\n使用这个 TPR，我们计算 $F1$ 分数：\n$$ F1_1 = \\frac{2 \\cdot 40 \\cdot 0.020009}{40 + 9.6 + 40 \\cdot 0.020009} = \\frac{1.60072}{49.6 + 0.80036} = \\frac{1.60072}{50.40036} \\approx 0.031760 $$\n\n情况 2: $a=1.5$\n$$ \\mathrm{TPR}_2 = 1 - \\Phi(2.575829 - 1.5) + \\Phi(-2.575829 - 1.5) $$\n$$ \\mathrm{TPR}_2 = 1 - \\Phi(1.075829) + \\Phi(-4.075829) \\approx 1 - 0.859000 + 2.34 \\times 10^{-5} \\approx 0.141023 $$\n相应的 $F1$ 分数为：\n$$ F1_2 = \\frac{2 \\cdot 40 \\cdot 0.141023}{40 + 9.6 + 40 \\cdot 0.141023} = \\frac{11.28184}{49.6 + 5.64092} = \\frac{11.28184}{55.24092} \\approx 0.204232 $$\n\n情况 3: $a=3.0$\n$$ \\mathrm{TPR}_3 = 1 - \\Phi(2.575829 - 3.0) + \\Phi(-2.575829 - 3.0) $$\n$$ \\mathrm{TPR}_3 = 1 - \\Phi(-0.424171) + \\Phi(-5.575829) \\approx 1 - 0.335680 + 1.22 \\times 10^{-8} \\approx 0.664320 $$\n相应的 $F1$ 分数为：\n$$ F1_3 = \\frac{2 \\cdot 40 \\cdot 0.664320}{40 + 9.6 + 40 \\cdot 0.664320} = \\frac{53.1456}{49.6 + 26.5728} = \\frac{53.1456}{76.1728} \\approx 0.697746 $$\n\n最后，我们通过对三个单独的分数求平均来计算宏平均 $F1$ 分数：\n$$ \\text{宏平均 } F1 = \\frac{F1_1 + F1_2 + F1_3}{3} $$\n$$ \\text{宏平均 } F1 \\approx \\frac{0.031760 + 0.204232 + 0.697746}{3} = \\frac{0.933738}{3} \\approx 0.311246 $$\n四舍五入到四位有效数字，最终结果是 $0.3112$。", "answer": "$$\\boxed{0.3112}$$", "id": "3446230"}, {"introduction": "在选择了恢复算法之后，其实际性能在很大程度上取决于实现细节，特别是迭代过程的终止时机。本练习探讨了迭代软阈值算法 (ISTA) 中不同停止准则之间的权衡。学生们将评估这些准则在噪声环境下对解的精度、计算成本以及支撑集恢复（即过拟合与欠拟合）的影响。", "problem": "考虑在一个带噪线性模型中使用最小绝对收缩与选择算子 (LASSO) 来恢复稀疏信号。底层数据生成过程基于以下基本要素：一个测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$，一个 $k$-稀疏的真实向量 $x_0 \\in \\mathbb{R}^n$（即 $x_0$ 最多有 $k$ 个非零分量），以及一个测量噪声向量 $w \\in \\mathbb{R}^m$，其分量是均值为 $0$、方差为 $\\sigma^2$ 的独立同分布高斯随机变量。观测到的测量值为 $y = A x_0 + w$。恢复问题被构建为如下的 LASSO 优化问题\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\|A x - y\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $\\lambda  0$ 用于平衡数据保真度项和促进稀疏性的正则化项。\n\n您必须对求解上述问题的近端梯度法（迭代软阈值算法）的三种停止准则进行实证比较：\n- 固定迭代次数停止：总是精确运行 $T_{\\text{fixed}}$ 次迭代。\n- 对偶间隙停止：一旦原始-对偶间隙低于容差 $\\varepsilon_{\\text{dual}}$ 就停止。\n- 基于残差的停止（偏差原则）：一旦残差范数与噪声水平相匹配，即当 $\\|A x - y\\|_2 \\leq \\tau \\sigma \\sqrt{m}$ 时停止。\n\n迭代软阈值算法 (ISTA) 使用软阈值算子 $S_{\\theta}(\\cdot)$，其按分量定义为 $S_{\\theta}(z)_i = \\operatorname{sign}(z_i) \\max\\{|z_i| - \\theta, 0\\}$，并执行迭代\n$$\nx^{t+1} \\;=\\; S_{\\lambda \\eta}\\Big(x^{t} - \\eta A^\\top (A x^{t} - y)\\Big)\n$$\n其中 $\\eta  0$ 是步长，选择为 $\\eta = 1/L$，$L$ 是数据保真度项梯度的 Lipschitz 常数，具体为 $L = \\|A\\|_2^2$（$A$ 的谱范数的平方）。\n\n使用 LASSO 问题的 Fenchel 对偶来计算一个有效的对偶下界以及在迭代点 $x$ 处的原始-对偶间隙。令 $r = A x - y$。其对偶问题是\n$$\n\\max_{u \\in \\mathbb{R}^m} \\; -\\frac{1}{2} \\|u\\|_2^2 - y^\\top u \\quad \\text{约束条件为} \\quad \\|A^\\top u\\|_\\infty \\leq \\lambda.\n$$\n给定一个残差为 $r$ 的原始迭代点 $x$，通过将 $r$ 径向投影到对偶可行集上，构造一个可行的对偶变量：设置 $\\alpha = \\min\\big\\{1, \\lambda / \\|A^\\top r\\|_\\infty \\big\\}$，并定义 $u = \\alpha r$。原始目标值为 $p(x) = \\frac{1}{2}\\|r\\|_2^2 + \\lambda \\|x\\|_1$，在 $u$ 处的对偶目标值为 $d(u) = -\\frac{1}{2}\\|u\\|_2^2 - y^\\top u$。于是，原始-对偶间隙为 $g(x) = p(x) - d(u)$。\n\n对于每种停止准则，通过以下指标量化其经验性能：\n- 准确度：归一化均方误差 $\\mathrm{NMSE}(x) = \\|x - x_0\\|_2^2/\\|x_0\\|_2^2$。\n- 过拟合指数：假发现率 $\\mathrm{FDP}(x)$，定义为满足 $x_i \\neq 0$ 且 $(x_0)_i = 0$ 的索引 $i$ 的数量除以 $\\max\\{1, |\\mathrm{supp}(x)|\\}$，其中 $\\mathrm{supp}(x) = \\{i : |x_i|  \\theta_{\\text{supp}}\\}$。\n- 欠拟合指数：漏检率 $\\mathrm{MISS}(x)$，定义为满足 $(x_0)_i \\neq 0$ 且 $x_i = 0$ 的索引 $i$ 的数量除以 $|\\mathrm{supp}(x_0)|$，使用相同的支撑集检测规则 $|x_i|  \\theta_{\\text{supp}}$。\n- 相对于固定迭代次数基线的计算节省量：对于一个在 $T$ 次迭代后停止的准则，定义 $\\mathrm{SAVE} = (T_{\\text{fixed}} - T)/T_{\\text{fixed}}$。\n\n您必须实现带有这三种停止准则的 ISTA 算法，按所述生成合成数据，并报告相对于固定迭代次数基线的上述指标。支撑集检测阈值必须为 $\\theta_{\\text{supp}} = 10^{-3}$。\n\n对所有测试用例使用以下通用设置：维度 $n = 256$，测量次数 $m = 120$，稀疏度 $k = 20$，固定迭代预算 $T_{\\text{fixed}} = 200$，以及通过经验法则 $\\lambda = \\sigma \\sqrt{2 \\log n}$ 选择的软阈值参数 $\\lambda$。构造 $A$ 使其分量 $A_{ij} \\sim \\mathcal{N}(0, 1/m)$ 独立，选择一个大小为 $k$ 的随机支撑集，并将 $x_0$ 的非零分量设置为在 $[1, 2]$ 上均匀分布且带有随机符号的独立样本。\n\n测试套件：\n- 用例 1：随机种子 $0$，噪声水平 $\\sigma = 0.02$，对偶间隙容差 $\\varepsilon_{\\text{dual}} = 10^{-6}$，偏差参数 $\\tau = 1.05$。\n- 用例 2：随机种子 $1$，噪声水平 $\\sigma = 0.1$，对偶间隙容差 $\\varepsilon_{\\text{dual}} = 10^{-4}$，偏差参数 $\\tau = 1.05$。\n- 用例 3：随机种子 $2$，噪声水平 $\\sigma = 0.3$，对偶间隙容差 $\\varepsilon_{\\text{dual}} = 10^{-3}$，偏差参数 $\\tau = 1.05$。\n\n对于每个用例，运行 ISTA 三次：使用固定迭代次数停止（$T_{\\text{fixed}}$ 次迭代），使用对偶间隙停止（一旦 $g(x) \\leq \\varepsilon_{\\text{dual}}$ 或在 $T_{\\text{fixed}}$ 次迭代后仍不满足时停止），以及使用基于残差的偏差原则停止（一旦 $\\|A x - y\\|_2 \\leq \\tau \\sigma \\sqrt{m}$ 或在 $T_{\\text{fixed}}$ 次迭代后仍不满足时停止）。按所列的确切顺序，为每个用例计算以下 8 个浮点数输出：\n- $\\mathrm{SAVE}_{\\text{dual}}$,\n- $\\mathrm{SAVE}_{\\text{res}}$,\n- $\\mathrm{NMSE}_{\\text{dual}} - \\mathrm{NMSE}_{\\text{fixed}}$,\n- $\\mathrm{NMSE}_{\\text{res}} - \\mathrm{NMSE}_{\\text{fixed}}$,\n- $\\mathrm{FDP}_{\\text{dual}}$,\n- $\\mathrm{MISS}_{\\text{dual}}$,\n- $\\mathrm{FDP}_{\\text{res}}$,\n- $\\mathrm{MISS}_{\\text{res}}$.\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如 $[\\text{result1},\\text{result2},\\text{result3},\\ldots]$）。总共将有 3 个用例，因此列表必须包含 24 个浮点数。不涉及任何物理单位。所有角度（如果存在）必须以弧度为单位，但此处未使用角度。所有百分比必须表示为小数，而不是使用百分号。程序必须是自包含的，并且不得读取任何外部输入。", "solution": "用户提供的问题是稀疏信号恢复和计算优化领域一个定义明确的数值实验。它要求对应用于 LASSO 问题的迭代软阈值算法 (ISTA) 的三种不同停止准则进行实证比较。该问题具有科学依据、是自包含的，并且所有参数和程序都已明确无误地指定。因此，它被认为是有效的，下面提供了完整的解决方案。\n\n问题的核心是从带噪线性测量值 $y \\in \\mathbb{R}^m$ 中求解稀疏信号 $x \\in \\mathbb{R}^n$ 的 LASSO 优化问题。数据生成模型由\n$$\ny = A x_0 + w,\n$$\n给出，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个测量矩阵，$x_0 \\in \\mathbb{R}^n$ 是真实的 $k$-稀疏信号，$w \\in \\mathbb{R}^m$ 是一个独立同分布高斯噪声向量，其分量 $w_i \\sim \\mathcal{N}(0, \\sigma^2)$。LASSO 公式通过求解以下问题来寻找 $x_0$ 的一个估计：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) := \\underbrace{\\frac{1}{2} \\|A x - y\\|_2^2}_{\\text{数据保真度}} + \\underbrace{\\lambda \\|x\\|_1}_{\\text{稀疏性正则化器}}。\n$$\n此处，$\\lambda  0$ 是一个正则化参数，它平衡了拟合测量值 $y$ 和在解向量 $x$ 中强制稀疏性之间的权衡。\n\n选定的求解器是迭代软阈值算法 (ISTA)，这是一种近端梯度法。数据保真度项的梯度为 $\\nabla(\\frac{1}{2} \\|A x - y\\|_2^2) = A^\\top(Ax-y)$。该项的梯度是 Lipschitz 连续的，常数为 $L = \\|A^\\top A\\|_2 = \\|A\\|_2^2$，其中 $\\|A\\|_2$ 是 $A$ 的谱范数。ISTA 的过程是在目标函数的光滑部分上执行一个梯度下降步，然后应用非光滑 $\\ell_1$ 范数的近端算子，即软阈值算子 $S_{\\theta}(\\cdot)$。当步长为 $\\eta = 1/L$ 时，第 $t+1$ 次迭代的更新规则是：\n$$\nx^{t+1} = S_{\\lambda \\eta}\\left(x^t - \\eta A^\\top(Ax^t - y)\\right),\n$$\n其中软阈值算子是逐元素应用的：$S_{\\theta}(z)_i = \\operatorname{sign}(z_i) \\max(|z_i| - \\theta, 0)$。所有运行都以 $x^0 = \\mathbf{0}$ 进行初始化。\n\n我们将比较此迭代过程的三种停止准则：\n1.  **固定迭代次数停止**：算法在固定的迭代次数 $T = T_{\\text{fixed}}$ 后终止。这作为计算成本和准确度的基线。\n2.  **对偶间隙停止**：此准则利用了对偶理论。在迭代点 $x^t$ 处的原始目标值为 $p(x^t) = \\frac{1}{2}\\|Ax^t - y\\|_2^2 + \\lambda\\|x^t\\|_1$。Fenchel 对偶问题是 $\\max_{u \\in \\mathbb{R}^m} \\; -\\frac{1}{2} \\|u\\|_2^2 - y^\\top u$，其约束条件为 $\\|A^\\top u\\|_\\infty \\leq \\lambda$。根据一个原始迭代点 $x^t$，我们构造一个对偶可行变量 $u^t$。令残差为 $r^t = Ax^t - y$。通过设置 $u^t = \\alpha r^t$ 来构造一个对偶可行变量 $u^t$，其中 $\\alpha = \\min\\left(1, \\frac{\\lambda}{\\|A^\\top r^t\\|_\\infty}\\right)$。这个选择确保了 $\\|A^\\top u^t\\|_\\infty \\le \\lambda$。对偶目标值为 $d(u^t) = -\\frac{1}{2}\\|u^t\\|_2^2 - y^\\top u^t$。原始-对偶间隙为 $g(x^t) = p(x^t) - d(u^t) \\geq 0$。当 $g(x^t) \\leq \\varepsilon_{\\text{dual}}$ 时，算法停止。\n3.  **基于残差的停止（偏差原则）**：此准则受噪声的统计特性启发。由于噪声能量期望为 $\\|w\\|_2^2 \\approx m \\sigma^2$，我们预期真实解的残差范数 $\\|Ax_0 - y\\|_2 = \\|-w\\|_2$ 大约在 $\\sigma\\sqrt{m}$ 左右。因此，当当前迭代点 $\\|Ax^t-y\\|_2$ 的残差范数降至与此噪声水平相关的阈值以下时，算法即停止：$\\|Ax^t - y\\|_2 \\leq \\tau \\sigma \\sqrt{m}$，其中 $\\tau \\ge 1$ 是一个容差因子。\n\n对于所有准则，作为一项安全措施，最多执行 $T_{\\text{fixed}}$ 次迭代。\n\n每个恢复解 $x$ 的经验性能通过几个指标进行评估：\n-   **归一化均方误差**：$\\mathrm{NMSE}(x) = \\frac{\\|x - x_0\\|_2^2}{\\|x_0\\|_2^2}$，用于衡量重建准确度。\n-   **支撑集恢复指标**：为了评估 $x_0$ 的稀疏模式被恢复得如何，我们首先定义 $x$ 的估计支撑集为 $\\mathrm{supp}(x) = \\{i : |x_i|  \\theta_{\\text{supp}}\\}$，对于一个小的阈值 $\\theta_{\\text{supp}}=10^{-3}$。\n    -   **假发现率**：$\\mathrm{FDP}(x) = \\frac{|\\{i : x_i \\neq 0 \\text{ and } (x_0)_i = 0\\}|}{\\max(1, |\\mathrm{supp}(x)|)}$，衡量过拟合（虚假的非零项）。\n    -   **漏检率**：$\\mathrm{MISS}(x) = \\frac{|\\{i : (x_0)_i \\neq 0 \\text{ and } x_i = 0\\}|}{|\\mathrm{supp}(x_0)|}$，衡量欠拟合（遗漏的真实非零项）。注意，$x_i=0$ 被解释为 $|x_i| \\le \\theta_{\\text{supp}}$。\n-   **计算节省量**：对于一个在 $T$ 次迭代后停止的准则，$\\mathrm{SAVE} = \\frac{T_{\\text{fixed}} - T}{T_{\\text{fixed}}}$。\n\n该实现根据提供的规格（$n=256$, $m=120$, $k=20$, $T_{\\text{fixed}}=200$）为每个测试用例生成合成数据。矩阵 $A$ 的分量独立同分布于 $\\mathcal{N}(0, 1/m)$。真实值 $x_0$ 有一个随机选择的大小为 $k$ 的支撑集，其非零值从 $[1, 2]$ 上的均匀分布中抽取并带有随机符号。正则化参数通过经验法则 $\\lambda = \\sigma \\sqrt{2 \\log n}$ 设置。对于所提供的三个测试用例中的每一个，我们都使用每种停止准则运行 ISTA，计算指定的指标，并计算相对于固定迭代次数基线所需的 8 个输出值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nTHETA_SUPP = 1e-3\n\ndef soft_threshold(z, theta):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - theta, 0)\n\ndef calculate_nmse(x_est, x0):\n    \"\"\"Calculates Normalized Mean-Squared Error.\"\"\"\n    norm_x0_sq = np.linalg.norm(x0)**2\n    if norm_x0_sq == 0:\n        return np.linalg.norm(x_est)**2\n    return np.linalg.norm(x_est - x0)**2 / norm_x0_sq\n\ndef calculate_fdp_miss(x_est, x0):\n    \"\"\"Calculates False Discovery Proportion and Miss Rate.\"\"\"\n    supp_x0 = np.where(x0 != 0)[0]\n    supp_x_est = np.where(np.abs(x_est)  THETA_SUPP)[0]\n\n    num_supp_x0 = len(supp_x0)\n    num_supp_x_est = len(supp_x_est)\n\n    false_discoveries = len(np.setdiff1d(supp_x_est, supp_x0))\n    misses = len(np.setdiff1d(supp_x0, supp_x_est))\n\n    fdp = false_discoveries / max(1, num_supp_x_est)\n    miss_rate = misses / num_supp_x0 if num_supp_x0  0 else 0.0\n\n    return fdp, miss_rate\n\ndef run_ista(A, y, x0, lambda_val, eta, T_fixed, stopping_rule, **kwargs):\n    \"\"\"Runs the ISTA algorithm with a specified stopping rule.\"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    \n    # Unpack stopping rule parameters from kwargs\n    eps_dual = kwargs.get('eps_dual')\n    tau = kwargs.get('tau')\n    sigma = kwargs.get('sigma')\n\n    for t in range(T_fixed):\n        # ISTA update step\n        residual = A @ x - y\n        grad = A.T @ residual\n        z = x - eta * grad\n        x = soft_threshold(z, lambda_val * eta)\n\n        # Check stopping rule\n        if stopping_rule == 'dual_gap':\n            # Calculate primal-dual gap\n            r = A @ x - y\n            At_r = A.T @ r\n            At_r_inf_norm = np.linalg.norm(At_r, np.inf)\n            \n            # Avoid division by zero\n            alpha = 1.0 if At_r_inf_norm  1e-12 else min(1.0, lambda_val / At_r_inf_norm)\n            \n            u = alpha * r\n            primal_obj = 0.5 * np.dot(r, r) + lambda_val * np.linalg.norm(x, 1)\n            dual_obj = -0.5 * np.dot(u, u) - np.dot(y, u)\n            gap = primal_obj - dual_obj\n            \n            if gap = eps_dual:\n                return x, t + 1\n                \n        elif stopping_rule == 'residual':\n            # Check discrepancy principle\n            res_norm = np.linalg.norm(A @ x - y, 2)\n            res_thresh = tau * sigma * np.sqrt(m)\n            if res_norm = res_thresh:\n                return x, t + 1\n    \n    # If loop completes, return result after T_fixed iterations\n    return x, T_fixed\n\ndef run_case(seed, sigma, eps_dual, tau):\n    \"\"\"Runs a single test case with all three stopping rules.\"\"\"\n    # Common settings from problem statement\n    n = 256\n    m = 120\n    k = 20\n    T_fixed = 200\n\n    # Generate synthetic data\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((m, n)) / np.sqrt(m)\n    \n    x0 = np.zeros(n)\n    support = rng.choice(n, k, replace=False)\n    values = rng.uniform(1, 2, size=k)\n    signs = rng.choice([-1, 1], size=k)\n    x0[support] = values * signs\n    \n    w = rng.standard_normal(m) * sigma\n    y = A @ x0 + w\n    \n    # Algorithm parameters\n    lambda_val = sigma * np.sqrt(2 * np.log(n))\n    L = np.linalg.norm(A, 2)**2\n    eta = 1.0 / L\n\n    # --- Run with Fixed-Iteration Stopping ---\n    x_fixed, T_actual_fixed = run_ista(A, y, x0, lambda_val, eta, T_fixed, 'fixed')\n\n    # --- Run with Dual-Gap Stopping ---\n    x_dual, T_dual = run_ista(A, y, x0, lambda_val, eta, T_fixed, 'dual_gap', eps_dual=eps_dual)\n\n    # --- Run with Residual-Based Stopping ---\n    x_res, T_res = run_ista(A, y, x0, lambda_val, eta, T_fixed, 'residual', tau=tau, sigma=sigma)\n\n    # Calculate metrics for the baseline\n    nmse_fixed = calculate_nmse(x_fixed, x0)\n\n    # Calculate metrics for dual-gap rule\n    nmse_dual = calculate_nmse(x_dual, x0)\n    fdp_dual, miss_dual = calculate_fdp_miss(x_dual, x0)\n    save_dual = (T_fixed - T_dual) / T_fixed\n    nmse_diff_dual = nmse_dual - nmse_fixed\n\n    # Calculate metrics for residual-based rule\n    nmse_res = calculate_nmse(x_res, x0)\n    fdp_res, miss_res = calculate_fdp_miss(x_res, x0)\n    save_res = (T_fixed - T_res) / T_fixed\n    nmse_diff_res = nmse_res - nmse_fixed\n    \n    return [\n        save_dual,\n        save_res,\n        nmse_diff_dual,\n        nmse_diff_res,\n        fdp_dual,\n        miss_dual,\n        fdp_res,\n        miss_res\n    ]\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        # (seed, sigma, eps_dual, tau)\n        (0, 0.02, 1e-6, 1.05),\n        (1, 0.1,  1e-4, 1.05),\n        (2, 0.3,  1e-3, 1.05),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_results = run_case(*case)\n        all_results.extend(case_results)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3446302"}]}