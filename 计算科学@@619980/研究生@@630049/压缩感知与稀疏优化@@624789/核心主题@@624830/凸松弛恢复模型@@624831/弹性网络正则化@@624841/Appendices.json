{"hands_on_practices": [{"introduction": "我们首先在一个简化的“理想”场景中研究弹性网络，即所有特征都不相关且具有相等的方差（正交设计）。这个思想实验剥离了特征相互作用带来的复杂性，使我们能够为每个系数推导出一个简洁的闭式解。通过将此解与 LASSO 和岭回归的解进行比较，我们可以清楚地看到弹性网络如何独特地结合了 LASSO 的稀疏性诱导软阈值和岭回归的连续收缩效应。[@problem_id:3487889]", "problem": "考虑一个线性回归模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，满足正交设计条件 $X^{\\top}X = I$，其中 $I$ 是单位矩阵。令 $y \\in \\mathbb{R}^{n}$ 为响应向量，并定义向量 $z = X^{\\top}y \\in \\mathbb{R}^{p}$。对于一个系数向量 $\\beta \\in \\mathbb{R}^{p}$，考虑以下三种惩罚最小二乘估计量：\n1. 最小绝对收缩和选择算子 (LASSO)，定义为以下表达式的最小化子：\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1},\n$$\n其中 $\\lambda_{1} > 0$ 且 $\\|\\beta\\|_{1} = \\sum_{j=1}^{p} |\\beta_{j}|$。\n2. 岭回归，定义为以下表达式的最小化子：\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\lambda_{2}}{2}\\|\\beta\\|_{2}^{2},\n$$\n其中 $\\lambda_{2} > 0$ 且 $\\|\\beta\\|_{2}^{2} = \\sum_{j=1}^{p} \\beta_{j}^{2}$。\n3. 弹性网络，定义为以下表达式的最小化子：\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2}\\|\\beta\\|_{2}^{2},\n$$\n其中 $\\lambda_{1} > 0$ 且 $\\lambda_{2} > 0$。\n\n从凸优化的第一性原理和正交条件 $X^{\\top}X = I$ 出发，推导弹性网络估计量关于 $z_{j}$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 的逐坐标闭式解。然后类似地，求出 LASSO 和岭回归的逐坐标解。最后，对于单个坐标 $j$，$z_{j} = 2.7$，$\\lambda_{1} = 1.5$ 和 $\\lambda_{2} = 0.5$，分别计算弹性网络、LASSO 和岭回归的三个逐坐标估计值。将最终数值答案以单行矩阵的形式呈现，顺序为：弹性网络、LASSO、岭回归。无需四舍五入。", "solution": "该问题是有效的，因为它科学地基于成熟的正则化线性模型理论，是适定的（其唯一解由凸目标函数保证），并且陈述客观，提供了所有必要信息。\n\n问题的核心是找到三个不同目标函数的最小化子。这三个目标函数共享一个共同的最小二乘项 $\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$。我们可以使用给定的正交设计条件 $X^{\\top}X = I$ 和定义 $z = X^{\\top}y$ 来简化此项。\n\n让我们展开平方$\\ell_2$范数：\n$$\n\\|y - X\\beta\\|_{2}^{2} = (y - X\\beta)^{\\top}(y - X\\beta) = y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta\n$$\n使用 $X^{\\top}X = I$ 和 $z = X^{\\top}y$（这意味着 $z^{\\top} = y^{\\top}X$），该表达式变为：\n$$\ny^{\\top}y - 2\\beta^{\\top}z + \\|\\beta\\|_{2}^{2}\n$$\n我们可以通过对 $\\beta$ 配方来重写此式：\n$$\ny^{\\top}y - 2\\beta^{\\top}z + \\|\\beta\\|_{2}^{2} = \\|\\beta\\|_{2}^{2} - 2\\beta^{\\top}z + \\|z\\|_{2}^{2} - \\|z\\|_{2}^{2} + y^{\\top}y = \\|z - \\beta\\|_{2}^{2} + y^{\\top}y - \\|z\\|_{2}^{2}\n$$\n由于 $y^{\\top}y - \\|z\\|_{2}^{2}$ 是一个关于 $\\beta$ 的常数，最小化形如 $\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + P(\\beta)$ 的目标函数等价于最小化 $\\frac{1}{2}\\|z - \\beta\\|_{2}^{2} + P(\\beta)$，其中 $P(\\beta)$ 是惩罚项。\n\n简化后的目标函数关于 $\\beta$ 的坐标是可分的，因为平方误差项 $\\frac{1}{2}\\|z - \\beta\\|_{2}^{2} = \\frac{1}{2}\\sum_{j=1}^{p}(z_j - \\beta_j)^2$ 和惩罚项（$\\|\\beta\\|_1 = \\sum_j |\\beta_j|$, $\\|\\beta\\|_2^2 = \\sum_j \\beta_j^2$）都是对单个坐标的求和。因此，我们可以通过最小化其对应的逐坐标目标函数来独立求解每个系数 $\\hat{\\beta}_j$。\n\n首先，我们推导弹性网络估计量的解。对于单个坐标 $\\beta_j$ 要最小化的目标函数是：\n$$\nJ_{\\text{enet}}(\\beta_j) = \\frac{1}{2}(z_j - \\beta_j)^2 + \\lambda_1 |\\beta_j| + \\frac{\\lambda_2}{2}\\beta_j^2\n$$\n这是一个凸函数。最小值在其次梯度关于 $\\beta_j$ 包含 $0$ 的点取到。次梯度为：\n$$\n\\partial_{\\beta_j} J_{\\text{enet}}(\\beta_j) = -(z_j - \\beta_j) + \\lambda_1 \\partial(|\\beta_j|) + \\lambda_2 \\beta_j\n$$\n其中 $\\partial(|\\beta_j|)$ 是绝对值函数的次梯度：当 $\\beta_j \\neq 0$ 时，它为 $\\text{sgn}(\\beta_j)$；当 $\\beta_j = 0$ 时，它为区间 $[-1, 1]$。\n\n我们分析最优值 $\\hat{\\beta}_j$ 的三种情况：\n1.  如果 $\\hat{\\beta}_j > 0$，则 $\\partial(|\\hat{\\beta}_j|) = 1$。令梯度为 $0$：\n    $$\n    -\\left(z_j - \\hat{\\beta}_j\\right) + \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j - z_j + \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j(1 + \\lambda_2) = z_j - \\lambda_1\n    $$\n    $$\n    \\hat{\\beta}_j = \\frac{z_j - \\lambda_1}{1 + \\lambda_2}\n    $$\n    此解仅在 $\\hat{\\beta}_j > 0$ 时有效，这意味着 $z_j - \\lambda_1 > 0$，即 $z_j > \\lambda_1$。\n\n2.  如果 $\\hat{\\beta}_j < 0$，则 $\\partial(|\\hat{\\beta}_j|) = -1$。令梯度为 $0$：\n    $$\n    -\\left(z_j - \\hat{\\beta}_j\\right) - \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j - z_j - \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j(1 + \\lambda_2) = z_j + \\lambda_1\n    $$\n    $$\n    \\hat{\\beta}_j = \\frac{z_j + \\lambda_1}{1 + \\lambda_2}\n    $$\n    此解仅在 $\\hat{\\beta}_j < 0$ 时有效，这意味着 $z_j + \\lambda_1 < 0$，即 $z_j < -\\lambda_1$。\n\n3.  如果 $\\hat{\\beta}_j = 0$，次梯度条件为 $0 \\in -(z_j - 0) + \\lambda_1[-1, 1] + 0$。这可以简化为 $0 \\in -z_j + [-\\lambda_1, \\lambda_1]$，即 $z_j \\in [-\\lambda_1, \\lambda_1]$，或 $|z_j| \\le \\lambda_1$。\n\n结合这三种情况，我们得到弹性网络估计量的逐坐标解：\n$$\n\\hat{\\beta}_{j}^{\\text{enet}} = \\begin{cases} \\frac{z_j - \\lambda_1}{1 + \\lambda_2} & \\text{若 } z_j > \\lambda_1 \\\\ \\frac{z_j + \\lambda_1}{1 + \\lambda_2} & \\text{若 } z_j < -\\lambda_1 \\\\ 0 & \\text{若 } |z_j| \\le \\lambda_1 \\end{cases}\n$$\n这可以使用软阈值算子 $S_{\\lambda}(x) = \\text{sgn}(x)\\max(0, |x|-\\lambda)$ 紧凑地写成：\n$$\n\\hat{\\beta}_{j}^{\\text{enet}} = \\frac{S_{\\lambda_1}(z_j)}{1 + \\lambda_2}\n$$\n\n接下来，我们求 LASSO 估计量的解。LASSO 是弹性网络在 $\\lambda_2 = 0$ 时的特例。将 $\\lambda_2 = 0$ 代入弹性网络的解，得到 LASSO 的逐坐标解：\n$$\n\\hat{\\beta}_{j}^{\\text{lasso}} = \\frac{S_{\\lambda_1}(z_j)}{1 + 0} = S_{\\lambda_1}(z_j) = \\text{sgn}(z_j)\\max(0, |z_j|-\\lambda_1)\n$$\n\n最后，我们求岭回归估计量的解。岭回归对应于弹性网络在 $\\lambda_1 = 0$ 时的情况。将 $\\lambda_1 = 0$ 代入弹性网络的解：\n条件 $z_j > \\lambda_1$ 变为 $z_j > 0$。条件 $z_j < -\\lambda_1$ 变为 $z_j < 0$。条件 $|z_j| \\le \\lambda_1$ 变为 $z_j=0$。\n因此，如果 $z_j > 0$，$\\hat{\\beta}_j = \\frac{z_j - 0}{1 + \\lambda_2} = \\frac{z_j}{1 + \\lambda_2}$。\n如果 $z_j < 0$，$\\hat{\\beta}_j = \\frac{z_j + 0}{1 + \\lambda_2} = \\frac{z_j}{1 + \\lambda_2}$。\n如果 $z_j = 0$，$\\hat{\\beta}_j = 0$，这也由公式 $\\frac{z_j}{1 + \\lambda_2}$ 给出。\n因此，岭回归的逐坐标解为：\n$$\n\\hat{\\beta}_{j}^{\\text{ridge}} = \\frac{z_j}{1 + \\lambda_2}\n$$\n这也可以通过直接最小化可微目标函数 $J_{\\text{ridge}}(\\beta_j) = \\frac{1}{2}(z_j - \\beta_j)^2 + \\frac{\\lambda_2}{2}\\beta_j^2$ 来推导。将其导数设为 $0$ 可得 $\\beta_j(1 + \\lambda_2) - z_j = 0$，给出相同的结果。\n\n现在我们使用给定的值 $z_j = 2.7$，$\\lambda_1 = 1.5$ 和 $\\lambda_2 = 0.5$ 来计算单个坐标 $j$ 的数值估计。\n\n对于弹性网络的估计，我们首先检查阈值条件：$|z_j| = 2.7 > 1.5 = \\lambda_1$。由于 $z_j > \\lambda_1$，我们使用第一种情况：\n$$\n\\hat{\\beta}_{j}^{\\text{enet}} = \\frac{z_j - \\lambda_1}{1 + \\lambda_2} = \\frac{2.7 - 1.5}{1 + 0.5} = \\frac{1.2}{1.5} = \\frac{12}{15} = \\frac{4}{5} = 0.8\n$$\n\n对于 LASSO 的估计，我们使用软阈值函数：\n$$\n\\hat{\\beta}_{j}^{\\text{lasso}} = S_{\\lambda_1}(z_j) = S_{1.5}(2.7) = \\text{sgn}(2.7)\\max(0, |2.7|-1.5) = 1 \\cdot (2.7 - 1.5) = 1.2\n$$\n\n对于岭回归的估计，我们使用其特定公式：\n$$\n\\hat{\\beta}_{j}^{\\text{ridge}} = \\frac{z_j}{1 + \\lambda_2} = \\frac{2.7}{1 + 0.5} = \\frac{2.7}{1.5} = \\frac{27}{15} = \\frac{9}{5} = 1.8\n$$\n\n坐标 $\\beta_j$ 的三个估计值分别为：弹性网络为 $0.8$，LASSO 为 $1.2$，岭回归为 $1.8$。", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.8 & 1.2 & 1.8 \\end{pmatrix}}\n$$", "id": "3487889"}, {"introduction": "超越理想化的正交设计案例，我们现在来解决一个实际问题：如何为一般数据求解弹性网络问题。本练习将指导您推导出坐标下降更新，这是解决此类问题最广泛使用的算法。您将看到如何通过解决一系列简单的一维问题来优化复杂的多变量目标函数，并学习高效的残差缓存对性能的重要性。[@problem_id:3487939]", "problem": "考虑在压缩感知（CS）和稀疏优化的背景下的弹性网络正则化问题。设 $X \\in \\mathbb{R}^{n \\times p}$ 是一个特征矩阵，其列为 $x_{j} \\in \\mathbb{R}^{n}$（$j \\in \\{1,\\dots,p\\}$），并设 $y \\in \\mathbb{R}^{n}$ 是一个响应向量。弹性网络的目标函数为\n$$\nF(\\beta) \\;=\\; \\frac{1}{2n}\\,\\|y - X\\beta\\|_{2}^{2} \\;+\\; \\lambda_{1}\\,\\|\\beta\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|\\beta\\|_{2}^{2},\n$$\n其中 $\\beta \\in \\mathbb{R}^{p}$，$\\lambda_{1} \\ge 0$ 且 $\\lambda_{2} \\ge 0$。考虑一个循环坐标下降算法，该算法维护缓存的残差 $r := y - X\\beta$ 和对所有 $j \\in \\{1,\\dots,p\\}$ 预先计算的 Gram 矩阵对角线元素 $d_{j} := \\frac{1}{n}\\,\\|x_{j}\\|_{2}^{2}$。对于给定的坐标 $j$，定义偏残差 $r^{(j)} := r + x_{j}\\beta_{j}$，并令 $s_{j} := \\mathrm{nnz}(x_{j})$ 表示列 $x_{j}$ 中非零元素的数量。\n\n从凸函数的基本最优性条件和 $\\ell_{1}$ 范数的次梯度微积分出发，推导出一个 $\\beta_{j}$ 的精确闭式坐标更新表达式，该表达式完全用 $x_{j}$、$r$、$d_{j}$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 表示，并同时给出在 $\\beta_{j}$ 改变后保持 $r = y - X\\beta$ 的残差更新。然后，在 $p \\gg n$ 的极端情况下，对于稀疏矩阵 $X$，分析每次迭代的计算复杂度，假设重复使用 $r$ 和 $d_{j}$ 并在每次坐标改变后执行残差更新。使用渐近符号，将遍历所有 $p$ 个坐标的一整轮的计算开销表示为 $\\{s_{j}\\}_{j=1}^{p}$ 的函数。\n\n你的最终答案必须是一个包含两个元素的行矩阵，其第一个元素是 $\\beta_{j}$ 的精确坐标更新，第二个元素是遍历所有 $p$ 个坐标的一整轮的渐近复杂度，两者都以闭式表达式给出。不需要进行数值计算。", "solution": "我们从弹性网络的目标函数开始\n$$\nF(\\beta) \\;=\\; \\frac{1}{2n}\\,\\|y - X\\beta\\|_{2}^{2} \\;+\\; \\lambda_{1}\\,\\|\\beta\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|\\beta\\|_{2}^{2}.\n$$\n固定除 $j$ 以外的所有坐标，并考虑替代 $\\beta_{j}$ 的变量 $t \\in \\mathbb{R}$ 的一维子问题。设当前迭代值为 $\\beta$，残差为 $r := y - X\\beta$，偏残差为 $r^{(j)} := r + x_{j}\\beta_{j} = y - \\sum_{k \\ne j} x_{k}\\beta_{k}$。限制于 $t$ 的目标函数为\n$$\n\\phi_{j}(t)\n\\;=\\;\n\\frac{1}{2n}\\,\\|r^{(j)} - x_{j} t\\|_{2}^{2}\n\\;+\\;\n\\lambda_{1}\\,|t|\n\\;+\\;\n\\frac{\\lambda_{2}}{2}\\,t^{2}\n\\;+\\; \\text{const},\n$$\n其中“const”不依赖于 $t$。展开二次项并合并系数可得\n$$\n\\phi_{j}(t)\n\\;=\\;\n\\frac{1}{2}\\,(d_{j} + \\lambda_{2})\\,t^{2}\n\\;-\\;\n\\left(\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)}\\right) t\n\\;+\\;\n\\lambda_{1}\\,|t|\n\\;+\\; \\text{const},\n$$\n其中 $d_{j} := \\frac{1}{n}\\,\\|x_{j}\\|_{2}^{2}$。这是一个严格凸的一维问题，包含一个二次项和一个 $\\ell_{1}$ 惩罚项。最小化子 $t^{\\star}$ 的次梯度最优性条件是\n$$\n0 \\;\\in\\; (d_{j} + \\lambda_{2})\\,t^{\\star} \\;-\\; \\left(\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)}\\right) \\;+\\; \\lambda_{1}\\,\\partial|t^{\\star}|,\n$$\n其中 $\\partial|t|$ 表示绝对值函数在 $t$ 处的次微分。解由软阈值化来表征：\n$$\nt^{\\star} \\;=\\; \\frac{1}{d_{j} + \\lambda_{2}} \\; S\\!\\left(\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)},\\, \\lambda_{1}\\right),\n$$\n其中软阈值算子 $S(a,\\tau)$ 定义为 $S(a,\\tau) := \\mathrm{sign}(a)\\,\\max\\{|a| - \\tau,\\, 0\\}$。\n\n为了用缓存的残差 $r$ 和 $d_{j}$ 来表示该更新，注意到\n$$\nx_{j}^{\\top} r^{(j)} \\;=\\; x_{j}^{\\top}(r + x_{j}\\beta_{j}) \\;=\\; x_{j}^{\\top} r \\;+\\; \\|x_{j}\\|_{2}^{2}\\,\\beta_{j},\n$$\n因此\n$$\n\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)} \\;=\\; \\frac{1}{n}\\,x_{j}^{\\top} r \\;+\\; d_{j}\\,\\beta_{j}.\n$$\n将此式代入软阈值表达式，得到精确的坐标更新\n$$\n\\beta_{j}^{\\mathrm{new}}\n\\;=\\;\n\\frac{1}{d_{j} + \\lambda_{2}}\n\\; S\\!\\left(\\frac{1}{n}\\,x_{j}^{\\top} r \\;+\\; d_{j}\\,\\beta_{j},\\, \\lambda_{1}\\right).\n$$\n计算出 $\\beta_{j}^{\\mathrm{new}}$ 后，通过以下方式更新缓存的残差以保持 $r = y - X\\beta$：\n$$\nr \\;\\leftarrow\\; r \\;-\\; x_{j}\\,\\big(\\beta_{j}^{\\mathrm{new}} - \\beta_{j}^{\\mathrm{old}}\\big).\n$$\n\n现在我们分析在 $p \\gg n$ 且 $X$ 为稀疏矩阵的情况下的计算复杂度。设 $s_{j} := \\mathrm{nnz}(x_{j})$ 为第 $j$ 列中非零元的数量。每次坐标更新的操作包括：\n- 计算 $\\frac{1}{n}\\,x_{j}^{\\top} r$：这需要 $\\mathcal{O}(s_{j})$ 时间，因为只有 $x_{j}$ 的非零项有贡献。\n- 计算 $\\frac{1}{n}\\,x_{j}^{\\top} r + d_{j}\\,\\beta_{j}$：这需要 $\\mathcal{O}(1)$ 时间，因为 $d_{j}$ 是预先计算好的。\n- 应用软阈值化并通过乘以 $(d_{j} + \\lambda_{2})^{-1}$ 进行缩放：这是 $\\mathcal{O}(1)$。\n- 更新残差 $r \\leftarrow r - x_{j}\\,(\\beta_{j}^{\\mathrm{new}} - \\beta_{j}^{\\mathrm{old}})$：这需要 $\\mathcal{O}(s_{j})$ 时间，因为只需要改变 $x_{j}$ 非零的位置。\n\n因此，每个坐标的总开销是 $\\mathcal{O}(s_{j})$，而遍历所有 $p$ 个坐标的一整轮的开销为\n$$\n\\mathcal{O}\\!\\left(\\sum_{j=1}^{p} s_{j}\\right),\n$$\n即 $\\mathcal{O}(\\mathrm{nnz}(X))$，其中 $\\mathrm{nnz}(X)$ 表示 $X$ 中非零元的总数。在 $p \\gg n$ 且列稀疏的极端情况下，$s_{j} \\le n$，而 $\\sum_{j=1}^{p} s_{j}$ 捕获了总体的稀疏性；重复使用缓存的残差和预计算的 $d_{j}$ 确保了每轮遍历的复杂度与稀疏度（而不是 $np$）成比例。", "answer": "$$\\boxed{\\begin{pmatrix}\n\\dfrac{1}{d_{j} + \\lambda_{2}}\\, S\\!\\left(\\dfrac{1}{n}\\,x_{j}^{\\top} r + d_{j}\\,\\beta_{j},\\, \\lambda_{1}\\right) & \\mathcal{O}\\!\\left(\\displaystyle\\sum_{j=1}^{p} s_{j}\\right)\n\\end{pmatrix}}$$", "id": "3487939"}, {"introduction": "一旦像坐标下降这样的迭代算法开始运行，我们如何知道何时停止？这个练习深入探讨了优美的 Fenchel 对偶理论来回答这个问题。通过构建弹性网络问题的对偶问题，您将推导出对偶间隙，这是一个强大的量，它为您当前解与真实最优解之间的差距提供了一个可靠的上限，从而形成了一个稳健且理论上合理的停止准则。[@problem_id:3487899]", "problem": "考虑压缩感知和稀疏优化中的弹性网络正则化最小二乘问题：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，$\\lambda_{1} > 0$ 且 $\\lambda_{2} > 0$。从Fenchel共轭和Fenchel对偶的基本定义出发，推导其Lagrange-Fenchel对偶问题，并针对给定的原始-对偶对 $(x,u)$，计算相应的凸对偶间隙。其中对偶变量 $u \\in \\mathbb{R}^{m}$ 与数据拟合项相关联。仅使用凸分析的核心定义和范数的基本性质。\n\n然后，对于具体数据\n$$\nA = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\lambda_{1} = 1, \\quad \\lambda_{2} = 2,\n$$\n以及给定的原始-对偶对\n$$\nx = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad u = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix},\n$$\n计算对偶间隙，并基于此间隙陈述一个能够保证最优目标值绝对精度的停止准则。你的停止准则必须清楚地说明如何证明当前原始目标值在最优值的指定容差范围内。\n\n最终答案仅报告所提供数据的对偶间隙值。无需四舍五入，答案不涉及单位。", "solution": "问题要求我们推导弹性网络正则化最小二乘问题的Fenchel对偶，为特定的原始-对偶对计算对偶间隙，并基于此间隙陈述一个停止准则。\n\n原始问题由下式给出：\n$$\nP_{\\text{opt}} = \\min_{x \\in \\mathbb{R}^{n}} \\; \\underbrace{\\frac{1}{2}\\|A x - b\\|_{2}^{2}}_{f(Ax)} + \\underbrace{\\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}}_{g(x)},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\lambda_{1}, \\lambda_{2} > 0$。该问题是标准形式 $\\min_{x} f(Ax) + g(x)$，其中 $f: \\mathbb{R}^{m} \\to \\mathbb{R}$ 和 $g: \\mathbb{R}^{n} \\to \\mathbb{R}$ 是凸函数。\n函数为：\n$f(y) = \\frac{1}{2}\\|y - b\\|_{2}^{2}$\n$g(x) = \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$\n\nLagrange-Fenchel对偶问题由 $D_{\\text{opt}} = \\max_{u \\in \\mathbb{R}^{m}} -f^*(u) - g^*(-A^T u)$ 给出，其中 $f^*$ 和 $g^*$ 分别是 $f$ 和 $g$ 的Fenchel共轭。该问题满足强对偶性，即 $P_{\\text{opt}} = D_{\\text{opt}}$。\n\n首先，我们计算 $f(y)$ 的Fenchel共轭：\n$$\nf^*(u) = \\sup_{y \\in \\mathbb{R}^{m}} \\{u^T y - f(y)\\} = \\sup_{y \\in \\mathbb{R}^{m}} \\left\\{ u^T y - \\frac{1}{2}\\|y - b\\|_{2}^{2} \\right\\}.\n$$\n上确界内的表达式是关于 $y$ 的凹二次函数。其最大值可以通过将关于 $y$ 的梯度置零来找到：\n$$\n\\nabla_y \\left( u^T y - \\frac{1}{2}(y - b)^T(y - b) \\right) = u - (y - b) = 0 \\implies y = u + b.\n$$\n将此代入表达式中，得到：\n$$\nf^*(u) = u^T(u + b) - \\frac{1}{2}\\|(u + b) - b\\|_{2}^{2} = u^T u + u^T b - \\frac{1}{2}\\|u\\|_{2}^{2} = \\frac{1}{2}\\|u\\|_{2}^{2} + b^T u.\n$$\n\n接下来，我们计算 $g(x)$ 的Fenchel共轭：\n$$\ng^*(v) = \\sup_{x \\in \\mathbb{R}^{n}} \\{v^T x - g(x)\\} = \\sup_{x \\in \\mathbb{R}^{n}} \\left\\{ v^T x - \\lambda_{1}\\|x\\|_{1} - \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} \\right\\}.\n$$\n该问题关于 $x$ 的分量 $x_i$ 是可分的：\n$$\ng^*(v) = \\sum_{i=1}^{n} \\sup_{x_i \\in \\mathbb{R}} \\left\\{ v_i x_i - \\lambda_{1}|x_i| - \\frac{\\lambda_{2}}{2}x_i^2 \\right\\}.\n$$\n我们来分析和式中的一维项。令 $h(x_i) = v_i x_i - \\lambda_{1}|x_i| - \\frac{\\lambda_{2}}{2}x_i^2$。\n如果 $x_i > 0$，$h'(x_i) = v_i - \\lambda_{1} - \\lambda_{2}x_i = 0 \\implies x_i = (v_i - \\lambda_{1})/\\lambda_{2}$。当 $v_i > \\lambda_{1}$ 时，这是一个有效的最大化点。最大值为 $\\frac{1}{2\\lambda_{2}}(v_i-\\lambda_{1})^2$。\n如果 $x_i < 0$，$h'(x_i) = v_i + \\lambda_{1} - \\lambda_{2}x_i = 0 \\implies x_i = (v_i + \\lambda_{1})/\\lambda_{2}$。当 $v_i < -\\lambda_{1}$ 时，这是一个有效的最大化点。最大值为 $\\frac{1}{2\\lambda_{2}}(v_i+\\lambda_{1})^2$。\n如果 $|v_i| \\le \\lambda_{1}$，对于 $x_i \\neq 0$ 导数不为零，函数在 $x_i=0$ 处取得最大值，值为 $0$。\n综合这些情况，每个分量的上确界是 $\\frac{1}{2\\lambda_{2}} (\\max(0, |v_i| - \\lambda_{1}))^2$。对所有分量求和：\n$$\ng^*(v) = \\sum_{i=1}^{n} \\frac{1}{2\\lambda_{2}} (\\max(0, |v_i| - \\lambda_{1}))^2 = \\frac{1}{2\\lambda_{2}} \\|S_{\\lambda_1}(v)\\|_2^2,\n$$\n其中 $S_{\\lambda_1}(v)$ 是向量软阈值算子，其分量为 $(S_{\\lambda_1}(v))_i = \\text{sign}(v_i) \\max(|v_i|-\\lambda_1, 0)$。\n\n组合成对偶问题，我们将 $v = -A^T u$ 代入 $g^*(v)$：\n$$\nD_{\\text{opt}} = \\max_{u \\in \\mathbb{R}^{m}} \\left\\{ -\\left(\\frac{1}{2}\\|u\\|_{2}^{2} + b^T u\\right) - \\frac{1}{2\\lambda_{2}}\\|S_{\\lambda_1}(-A^T u)\\|_2^2 \\right\\}.\n$$\n等价地，这可以写成一个最小化问题：\n$$\n\\min_{u \\in \\mathbb{R}^{m}} \\left\\{ \\frac{1}{2}\\|u\\|_{2}^{2} + b^T u + \\frac{1}{2\\lambda_{2}}\\|S_{\\lambda_1}(-A^T u)\\|_2^2 \\right\\}.\n$$\n这个最小化的值是 $-P_{\\text{opt}}$。\n\n对于给定的原始-对偶对 $(x,u)$，对偶间隙是原始目标值 $P(x)$ 和对偶目标值 $D(u)$ 之间的差：\n$$\n\\text{gap}(x,u) = P(x) - D(u) = \\left( \\frac{1}{2}\\|Ax-b\\|_{2}^{2} + \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} \\right) - \\left( -\\left(\\frac{1}{2}\\|u\\|_{2}^{2} + b^T u\\right) - g^*(-A^T u) \\right).\n$$\n这可以简化为：\n$$\n\\text{gap}(x,u) = P(x) + \\frac{1}{2}\\|u\\|_{2}^{2} + b^T u + g^*(-A^T u).\n$$\n\n现在，我们为给定的数据计算对偶间隙：\n$A = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\lambda_{1} = 1, \\quad \\lambda_{2} = 2$。\n$x = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad u = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}$。\n\n首先，我们计算原始目标 $P(x)$：\n$Ax = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1-2 \\\\ 0-1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$。\n$Ax-b = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}$。\n数据拟合项是 $\\frac{1}{2}\\|Ax-b\\|_{2}^{2} = \\frac{1}{2}((-2)^2 + (-1)^2) = \\frac{1}{2}(4+1) = \\frac{5}{2}$。\n正则化项是：\n$\\lambda_{1}\\|x\\|_{1} = 1 \\cdot (|1| + |-1|) = 2$。\n$\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} = \\frac{2}{2}(1^2 + (-1)^2) = 1(1+1) = 2$。\n因此，原始目标值为 $P(x) = \\frac{5}{2} + 2 + 2 = \\frac{13}{2}$。\n\n接下来，我们计算对偶目标 $D(u)$。我们首先需要计算 $g^*(-A^T u)$。\n$A^T = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix}$。\n$A^T u = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -4-1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -5 \\end{pmatrix}$。\n令 $v = -A^T u = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}$。\n现在我们计算 $g^*(v)$：\n$g^*(v) = \\frac{1}{2\\lambda_2} \\sum_{i=1}^{2} (\\max(0, |v_i|-\\lambda_1))^2 = \\frac{1}{2(2)} \\left[ (\\max(0, |2|-1))^2 + (\\max(0, |5|-1))^2 \\right]$。\n$g^*(v) = \\frac{1}{4} \\left[ (\\max(0, 1))^2 + (\\max(0, 4))^2 \\right] = \\frac{1}{4} (1^2 + 4^2) = \\frac{1}{4}(1+16) = \\frac{17}{4}$。\n所以，$g^*(-A^T u) = \\frac{17}{4}$。\n对偶目标值为 $D(u) = -(\\frac{1}{2}\\|u\\|_{2}^{2} + b^T u) - g^*(-A^T u)$。\n$\\|u\\|_{2}^{2} = (-2)^2 + (-1)^2 = 5$。\n$b^T u = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = -2$。\n$D(u) = -(\\frac{1}{2}(5) - 2) - \\frac{17}{4} = -(\\frac{5}{2} - \\frac{4}{2}) - \\frac{17}{4} = -\\frac{1}{2} - \\frac{17}{4} = -\\frac{2}{4} - \\frac{17}{4} = -\\frac{19}{4}$。\n\n对偶间隙是：\n$\\text{gap}(x,u) = P(x) - D(u) = \\frac{13}{2} - \\left(-\\frac{19}{4}\\right) = \\frac{26}{4} + \\frac{19}{4} = \\frac{45}{4}$。\n\n最后，我们陈述停止准则。根据弱对偶性，对于任意原始可行的 $x$ 和对偶可行的 $u$，我们有 $D(u) \\le P_{\\text{opt}} \\le P(x)$。\n这意味着当前原始解 $x$ 的次优性由对偶间隙界定：\n$$\n0 \\le P(x) - P_{\\text{opt}} \\le P(x) - D(u) = \\text{gap}(x,u).\n$$\n一个保证最优目标值绝对精度为 $\\epsilon > 0$ 的停止准则是，当找到一个原始-对偶对 $(x, u)$ 满足以下条件时，终止迭代算法：\n$$\n\\text{gap}(x, u) \\le \\epsilon.\n$$\n当满足此条件时，我们就有一个证书，证明当前的原始目标值 $P(x)$ 最多比真实最优值 $P_{\\text{opt}}$ 大 $\\epsilon$。\n\n问题要求提供所给数据的对偶间隙值。\n该值为 $\\frac{45}{4}$。", "answer": "$$\\boxed{\\frac{45}{4}}$$", "id": "3487899"}]}