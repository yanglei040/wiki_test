{"hands_on_practices": [{"introduction": "虽然有限维空间中的所有范数都是等价的，但其等价常数可能很大且依赖于维度。本练习将引导你构造一个“最坏情况”的向量，该向量使得 $L_1$ 范数与 $L_\\infty$ 范数之比达到最大值，从而揭示两者之间的最大可能差异。理解这种差异对于机器学习等领域至关重要，例如，在选择 $L_1$ (Lasso) 和 $L_2$ (Ridge) 正则化时，这种差异起着决定性作用。[@problem_id:3544607]", "problem": "设 $n \\in \\mathbb{N}$ 且 $n \\ge 2$，并设 $x \\in \\mathbb{R}^{n}$。对于 $x = (x_{1},\\dots,x_{n})$，回顾向量范数的标准定义：$\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$，$\\|x\\|_{\\infty} = \\max_{1 \\le i \\le n} |x_{i}|$，以及 $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$。\n\n(a) 构造一个向量 $x^{\\star} \\in \\mathbb{R}^{n}$，使得 $\\|x^{\\star}\\|_{1} = n \\,\\|x^{\\star}\\|_{\\infty}$，并从第一性原理（仅使用范数的定义）证明，对于非零向量 $x \\in \\mathbb{R}^{n}$，这实现了比值 $\\|x\\|_{1}/\\|x\\|_{\\infty}$ 的最大可能值。\n\n(b) 考虑 $\\mathbb{R}^{n}$ 中的两个罚项最小二乘目标，它们具有共同的二次损失函数 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 和 $b \\in \\mathbb{R}^{m}$ 是固定的但其他方面是任意的。第一个正则化器是最小绝对值收缩和选择算子（lasso）惩罚项 $\\lambda \\|x\\|_{1}$，第二个是岭（ridge）惩罚项 $\\mu \\|x\\|_{2}$。为了分离范数几何形状的影响，在归一化条件 $\\|x^{\\star}\\|_{\\infty} = 1$ 下，比较在 (a) 部分得到的单点 $x^{\\star}$ 处的惩罚项大小。选择 $\\lambda > 0$ 和 $\\mu > 0$，使得两个惩罚项在 $x^{\\star}$ 处相等，即 $\\lambda \\|x^{\\star}\\|_{1} = \\mu \\|x^{\\star}\\|_{2}$。比值 $\\mu/\\lambda$ 作为 $n$ 的函数的精确值是多少？请用关于 $n$ 的封闭形式表达式报告你的答案。\n\n(c) 仅使用范数的定义和基本不等式，论证在有限维空间中范数的等价性如何限制了 lasso 和 ridge 惩罚项在 $n$ 较小时的几何可区分性，并简要解释在约束 $\\|x\\|_{\\infty}=1$ 下，(a) 部分的构造如何代表了稀疏性判别的最坏情况。你的论证应是定性的，并依赖于从第一性原理推导出的紧界；此部分不需要数值答案。\n\n你提交的最终答案必须是 (b) 部分中比值 $\\mu/\\lambda$ 的单一封闭形式表达式。", "solution": "问题分为三个部分。我们将按顺序解答。最终答案是 (b) 部分的结果。\n\n(a) 构造并证明比值 $\\|x\\|_{1}/\\|x\\|_{\\infty}$ 的最大性。\n\n设 $x = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$ 为任意非零向量。$L_1$范数和$L_\\infty$范数的定义如下：\n$$ \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}| $$\n$$ \\|x\\|_{\\infty} = \\max_{1 \\le i \\le n} |x_{i}| $$\n根据最大值的定义，对于向量 $x$ 的任意分量 $x_i$，我们有不等式 $|x_{i}| \\le \\|x\\|_{\\infty}$。将此不等式对所有分量从 $i=1$ 到 $n$ 求和：\n$$ \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}| \\le \\sum_{i=1}^{n} \\|x\\|_{\\infty} $$\n由于 $\\|x\\|_{\\infty}$ 相对于求和指数 $i$ 是一个常数，右侧的和变为 $n \\|x\\|_{\\infty}$。这就建立了一般不等式：\n$$ \\|x\\|_{1} \\le n \\|x\\|_{\\infty} $$\n对于任何非零向量 $x$，我们可以除以 $\\|x\\|_{\\infty}$（其必须为正），得到该比值的上界：\n$$ \\frac{\\|x\\|_{1}}{\\|x\\|_{\\infty}} \\le n $$\n为了证明 $n$ 是该比值的最大可能值，我们必须证明这个界是紧的。这需要构造一个特定的向量 $x^{\\star}$，使得等式 $\\|x^{\\star}\\|_{1} = n \\|x^{\\star}\\|_{\\infty}$ 成立。\n推导过程中的等式 $\\sum_{i=1}^{n} |x_{i}| \\le \\sum_{i=1}^{n} \\|x\\|_{\\infty}$ 成立的充要条件是，对于所有 $i \\in \\{1, \\dots, n\\}$，都有 $|x_{i}| = \\|x\\|_{\\infty}$。\n我们可以构造这样一个向量。设 $c$ 为任意非零实数，定义向量 $x^{\\star} \\in \\mathbb{R}^{n}$ 为 $x^{\\star} = (c, c, \\dots, c)$。\n为简单起见，我们选择 $c=1$。那么 $x^{\\star} = (1, 1, \\dots, 1)$。\n我们计算这个向量的范数：\n$$ \\|x^{\\star}\\|_{\\infty} = \\max_{1 \\le i \\le n} |1| = 1 $$\n$$ \\|x^{\\star}\\|_{1} = \\sum_{i=1}^{n} |1| = n $$\n现在我们验证是否满足题目中的条件：\n$$ \\|x^{\\star}\\|_{1} = n \\quad \\text{且} \\quad n \\|x^{\\star}\\|_{\\infty} = n \\cdot 1 = n $$\n因此，$\\|x^{\\star}\\|_{1} = n \\|x^{\\star}\\|_{\\infty}$ 成立。既然我们找到了一个达到上界 $n$ 的向量，我们就证明了对于任何非零向量 $x \\in \\mathbb{R}^{n}$，比值 $\\|x\\|_{1} / \\|x\\|_{\\infty}$ 的最大值是 $n$。\n\n(b) 计算比值 $\\mu/\\lambda$。\n\n题目要求我们比较在 (a) 部分得到的特定向量 $x^{\\star}$ 处的 lasso 惩罚项 $\\lambda \\|x\\|_{1}$ 和 ridge 惩罚项 $\\mu \\|x\\|_{2}$。我们使用向量 $x^{\\star} = (1, 1, \\dots, 1)$，它满足给定的归一化条件 $\\|x^{\\star}\\|_{\\infty} = 1$。\n我们需要计算 $x^{\\star}$ 的 $L_1$ 和 $L_2$ 范数。\n从 (a) 部分可知，我们有 $\\|x^{\\star}\\|_{1} = n$。\n$L_2$范数定义为 $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$。对于我们的向量 $x^{\\star}$：\n$$ \\|x^{\\star}\\|_{2} = \\left(\\sum_{i=1}^{n} 1^{2}\\right)^{1/2} = \\left(\\sum_{i=1}^{n} 1\\right)^{1/2} = n^{1/2} = \\sqrt{n} $$\n参数 $\\lambda > 0$ 和 $\\mu > 0$ 的选择使得两个惩罚项在 $x^{\\star}$ 处相等：\n$$ \\lambda \\|x^{\\star}\\|_{1} = \\mu \\|x^{\\star}\\|_{2} $$\n将计算出的范数值代入此方程：\n$$ \\lambda \\cdot n = \\mu \\cdot \\sqrt{n} $$\n为了求出比值 $\\mu/\\lambda$ 的精确值，我们整理该方程：\n$$ \\frac{\\mu}{\\lambda} = \\frac{n}{\\sqrt{n}} $$\n化简该表达式得到最终结果：\n$$ \\frac{\\mu}{\\lambda} = \\sqrt{n} $$\n\n(c) 定性论证。\n\n在有限维向量空间（如 $\\mathbb{R}^{n}$）上的所有范数都是等价的。这意味着对于任意两个范数，比如 $\\|\\cdot\\|_a$ 和 $\\|\\cdot\\|_b$，都存在正常数 $c_{1}$ 和 $c_{2}$（可能依赖于维度 $n$），使得对于所有 $x \\in \\mathbb{R}^n$：$c_{1} \\|x\\|_{a} \\le \\|x\\|_{b} \\le c_{2} \\|x\\|_{a}$。\n对于 $L_1$ 和 $L_2$ 范数，其紧等价不等式为：\n$$ \\|x\\|_{2} \\le \\|x\\|_{1} \\le \\sqrt{n} \\|x\\|_{2} $$\n下界 $\\|x\\|_{2} \\le \\|x\\|_{1}$ 可由 $\\|x\\|_{1}^2 = (\\sum |x_{i}|)^2 = \\sum x_{i}^2 + \\sum_{i \\neq j} |x_i||x_j| \\ge \\sum x_{i}^2 = \\|x\\|_{2}^2$ 推得。上界 $\\|x\\|_{1} \\le \\sqrt{n} \\|x\\|_{2}$ 可由柯西-施瓦茨不等式推得：$\\|x\\|_{1} = \\sum |x_i| \\cdot 1 \\le (\\sum |x_i|^2)^{1/2} (\\sum 1^2)^{1/2} = \\|x\\|_2 \\sqrt{n}$。\n常数之比 $c_2/c_1$ 为 $\\sqrt{n}/1 = \\sqrt{n}$。对于较小的 $n$，这个比值接近 1，这意味着两种范数在几何上是相似的。例如，当 $n=2$ 时，$L_1$ 单位球是一个旋转了 $45^\\circ$ 的正方形，而 $L_2$ 单位球是一个圆形；它们没有本质上的不同。这种相对的相似性意味着 lasso 惩罚项的水平集（$\\lambda \\|x\\|_1 = \\text{常数}$）和 ridge 惩罚项的水平集（$\\mu \\|x\\|_2 = \\text{常数}$）在几何上是相似的，这使得它们的正则化效果在 $n$ 较小时更难区分。\n\n在约束 $\\|x^{\\star}\\|_\\infty = 1$ 下，向量 $x^{\\star} = (1, 1, \\dots, 1)$ 是最大程度非稀疏或“稠密”的，因为它的所有分量都是非零的，并且具有可能的最大幅值。lasso 惩罚项的关键特征是它能够促进稀疏性（即解中包含许多零分量），这一性质源于其多面体单位球位于坐标轴上的尖锐顶点。向量 $x^{\\star}$ 与此截然相反；它指向 $L_1$ 球的一个面的中心，这个区域距离稀疏顶点最远。\n(a) 部分的构造将 $x^{\\star}$ 确定为实现不等式 $\\|x\\|_{1} \\le \\sqrt{n} \\|x\\|_{2}$ 中等号成立的向量。也就是说，$x^{\\star}$ 是一个使得比值 $\\|x\\|_{1}/\\|x\\|_{2}$ 达到其最大可能值 $\\sqrt{n}$ 的实例。相反，对于一个稀疏向量，如 $x_s = (1, 0, \\dots, 0)$，我们有 $\\|x_s\\|_{1}=1$ 和 $\\|x_s\\|_{2}=1$，比值为 1。因此，向量 $x^{\\star}$ 代表了 $L_1$ 和 $L_2$ 范数之间几何差异最大的点。在这个差异最大的点上校准惩罚项，代表了稀疏性判别的“最坏情况”场景。它突显了两种惩罚项在缩放比例上的最极端差异，这种情况发生在稠密向量上，而对于这些向量，$L_1$ 范数独特的稀疏诱导特性恰恰是最不相关的。", "answer": "$$\\boxed{\\sqrt{n}}$$", "id": "3544607"}, {"introduction": "范数等价及其维度依赖性的概念可以从向量范数推广到算子范数（矩阵范数）。本练习将展示一个矩阵的特定结构如何导致其在不同范数（$L_1$、$L_2$ 或 $L_\\infty$）下的度量值产生巨大差异。对于分析数值算法的稳定性与误差传播而言，理解这一点至关重要。[@problem_id:3544612]", "problem": "设 $n \\geq 2$ 是一个整数。考虑与$\\mathbb{R}^{n}$上的向量$p$-范数（定义为 $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$，$\\|x\\|_{2} = \\left( \\sum_{i=1}^{n} |x_{i}|^{2} \\right)^{1/2}$，以及 $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$）相关联的$\\mathbb{R}^{n \\times n}$上的诱导算子范数，以及诱导算子范数的定义 $\\|A\\|_{p \\to p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$。从这些定义和$\\mathbb{R}^{n}$上向量范数的基本等价性出发，推导出一个关于 $\\|A\\|_{1 \\to 1}$ 和 $\\|A\\|_{\\infty \\to \\infty}$ 的、对所有 $A \\in \\mathbb{R}^{n \\times n}$ 均成立的 $\\|A\\|_{2 \\to 2}$ 的维度相关上界。然后，通过取\n$$\nA = u e_{1}^{\\top}, \\quad \\text{其中 } u = (1,1,\\dots,1)^{\\top} \\in \\mathbb{R}^{n} \\text{ 且 } e_{1} = (1,0,\\dots,0)^{\\top} \\in \\mathbb{R}^{n}\n$$\n来设计一个特定的矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其列/行结构使得某个诱导范数很小而其他范数很大。仅使用上述定义和第一性原理，精确计算 $\\|A\\|_{1 \\to 1}$，$\\|A\\|_{\\infty \\to \\infty}$ 和 $\\|A\\|_{2 \\to 2}$ 这三个量，然后构成比率向量\n$$\nr(n) = \\left( \\frac{\\|A\\|_{1 \\to 1}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\|A\\|_{\\infty \\to \\infty}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\sqrt{\\|A\\|_{1 \\to 1} \\, \\|A\\|_{\\infty \\to \\infty}}}{\\|A\\|_{2 \\to 2}} \\right).\n$$\n解释这种构造如何凸显出，当不同方法针对具有高度非均匀列/行结构的矩阵，分别用 $\\| \\cdot \\|_{1 \\to 1}$、$\\| \\cdot \\|_{\\infty \\to \\infty}$ 或 $\\| \\cdot \\|_{2 \\to 2}$ 来控制误差或稳定性时，所产生的依赖于算法的敏感性。将 $r(n)$ 的闭式表达式作为你的最终答案；无需四舍五入，也无需单位。", "solution": "所述问题在数学上是合理的、适定的且内部一致的。这是一个关于矩阵范数性质的数值线性代数标准练习。因此，我们可以进行形式化的求解。\n\n该问题分为两个主要部分。首先，我们推导矩阵 $2$-范数的一个通用上界，用 $1$-范数和 $\\infty$-范数表示。其次，我们分析一个特定矩阵，以说明矩阵范数的范数等价常数的维度依赖性。\n\n第一部分：上界的推导\n\n我们被要求从诱导范数的定义和向量范数的等价性出发，为矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 推导 $\\|A\\|_{2 \\to 2}$ 的一个上界，用 $\\|A\\|_{1 \\to 1}$ 和 $\\|A\\|_{\\infty \\to \\infty}$ 表示。\n\n诱导算子范数定义为 $\\|A\\|_{p \\to p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$。我们将使用的$\\mathbb{R}^n$中的基本向量范数等价关系是：\n1. 对任意 $v \\in \\mathbb{R}^n$，有 $\\|v\\|_{\\infty} \\leq \\|v\\|_{2}$。\n2. 对任意 $v \\in \\mathbb{R}^n$，有 $\\|v\\|_{2} \\leq \\sqrt{n} \\|v\\|_{\\infty}$。\n3. 对任意 $v \\in \\mathbb{R}^n$，有 $\\|v\\|_{1} \\leq \\sqrt{n} \\|v\\|_{2}$。\n\n让我们开始对任意 $A \\in \\mathbb{R}^{n \\times n}$ 进行推导。\n根据定义，$\\|A\\|_{2 \\to 2} = \\sup_{\\|x\\|_{2}=1} \\|Ax\\|_{2}$。\n我们可以利用向量范数等价性和诱导范数的定义来构造不等式链。\n使用等价关系2，我们有 $\\|Ax\\|_{2} \\leq \\sqrt{n} \\|Ax\\|_{\\infty}$。\n根据诱导 $\\infty$-范数的定义，$\\|Ax\\|_{\\infty} \\leq \\|A\\|_{\\infty \\to \\infty} \\|x\\|_{\\infty}$。\n将它们结合起来，我们得到 $\\|Ax\\|_{2} \\leq \\sqrt{n} \\|A\\|_{\\infty \\to \\infty} \\|x\\|_{\\infty}$。\n现在，使用等价关系1，我们有 $\\|x\\|_{\\infty} \\leq \\|x\\|_{2}$。对于单位球面上的向量 $x$，$\\|x\\|_{2}=1$，所以 $\\|x\\|_{\\infty} \\leq 1$。\n将此代入我们的不等式，得到 $\\|Ax\\|_{2} \\leq \\sqrt{n} \\|A\\|_{\\infty \\to \\infty} \\cdot 1$。\n由于这对任何 $\\|x\\|_{2}=1$ 的 $x$ 都成立，它也必须对使 $\\|Ax\\|_{2}$ 最大化的那个 $x$ 成立。因此，我们得到了第一个界：\n$$\n\\|A\\|_{2 \\to 2} = \\sup_{\\|x\\|_{2}=1} \\|Ax\\|_{2} \\leq \\sqrt{n} \\|A\\|_{\\infty \\to \\infty}.\n$$\n为了引入 $1$-范数，我们可以使用类似的论证或利用转置的性质。一个标准结果是 $\\|A^T\\|_{2 \\to 2} = \\|A\\|_{2 \\to 2}$，$\\|A^T\\|_{1 \\to 1} = \\|A\\|_{\\infty \\to \\infty}$，以及 $\\|A^T\\|_{\\infty \\to \\infty} = \\|A\\|_{1 \\to 1}$。将我们刚刚推导出的不等式应用于矩阵 $A^T$：\n$$\n\\|A^T\\|_{2 \\to 2} \\leq \\sqrt{n} \\|A^T\\|_{\\infty \\to \\infty}.\n$$\n代入这些恒等式，我们得到第二个界：\n$$\n\\|A\\|_{2 \\to 2} \\leq \\sqrt{n} \\|A\\|_{1 \\to 1}.\n$$\n我们现在有了关于 $\\|A\\|_{2 \\to 2}$ 的两个上界。问题要求一个用 $\\|A\\|_{1 \\to 1}$ 和 $\\|A\\|_{\\infty \\to \\infty}$ 表示的单一上界。我们可以结合我们的两个结果。例如，我们可以说 $\\|A\\|_{2 \\to 2}$ 小于或等于这两个界的几何平均值：\n$$\n\\|A\\|_{2 \\to 2} \\leq \\sqrt{(\\sqrt{n} \\|A\\|_{1 \\to 1}) (\\sqrt{n} \\|A\\|_{\\infty \\to \\infty})} = \\sqrt{n \\cdot \\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}.\n$$\n这是一个按要求从第一性原理推导出的有效的维度相关上界。请注意，一个更紧的界 $\\|A\\|_{2 \\to 2} \\leq \\sqrt{\\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}$ 也存在，但其推导比问题框架所建议的要复杂得多。\n\n第二部分：特定矩阵的分析\n\n我们给定的矩阵是 $A = u e_{1}^{\\top}$，其中 $u = (1,1,\\dots,1)^{\\top} \\in \\mathbb{R}^{n}$ 且 $e_{1} = (1,0,\\dots,0)^{\\top} \\in \\mathbb{R}^{n}$。\n矩阵 $A$ 是这两个向量的外积：\n$$\nA = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1  0  \\dots  0 \\end{pmatrix} = \\begin{pmatrix}\n1  0  \\dots  0 \\\\\n1  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n1  0  \\dots  0\n\\end{pmatrix}.\n$$\n这是一个秩为 $1$ 的矩阵，其第一列全为1，所有其他列都是零向量。\n\n我们现在计算这个矩阵 $A$ 的三个指定的诱导范数。\n\n1.  $\\|A\\|_{1 \\to 1}$：此范数定义为最大绝对列和。设 $A_j$ 是 $A$ 的第 $j$ 列。\n    对于第一列（$j=1$）：$\\|A_1\\|_1 = \\sum_{i=1}^{n} |a_{i1}| = \\sum_{i=1}^{n} |1| = n$。\n    对于任何其他列（$j > 1$）：$\\|A_j\\|_1 = \\sum_{i=1}^{n} |a_{ij}| = \\sum_{i=1}^{n} |0| = 0$。\n    这些值的最大值是 $n$。因此，\n    $$\n    \\|A\\|_{1 \\to 1} = n.\n    $$\n\n2.  $\\|A\\|_{\\infty \\to \\infty}$：此范数定义为最大绝对行和。设 $A_{i,:}$ 是 $A$ 的第 $i$ 行。\n    对于任意行 $i$：$\\|A_{i,:}\\|_1 = \\sum_{j=1}^{n} |a_{ij}| = |1| + |0| + \\dots + |0| = 1$。\n    由于这对所有行都成立，最大值是 $1$。因此，\n    $$\n    \\|A\\|_{\\infty \\to \\infty} = 1.\n    $$\n\n3.  $\\|A\\|_{2 \\to 2}$：此范数是 $A$ 的最大奇异值，即 $\\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^T A)}$。让我们计算 $A^T A$。\n    $A^T = (u e_1^T)^T = e_1 u^T$。\n    $A^T A = (e_1 u^T) (u e_1^T) = e_1 (u^T u) e_1^T$。\n    内积 $u^T u$ 是 $\\sum_{i=1}^n 1^2 = n$。\n    所以，$A^T A = n (e_1 e_1^T)$。矩阵 $e_1 e_1^T$ 是一个 $n \\times n$ 矩阵，在 $(1,1)$ 位置有一个 $1$，其他位置都是零。\n    $$\n    A^T A = n \\begin{pmatrix}\n    1  0  \\dots  0 \\\\\n    0  0  \\dots  0 \\\\\n    \\vdots  \\vdots  \\ddots  \\vdots \\\\\n    0  0  \\dots  0\n    \\end{pmatrix} = \\begin{pmatrix}\n    n  0  \\dots  0 \\\\\n    0  0  \\dots  0 \\\\\n    \\vdots  \\vdots  \\ddots  \\vdots \\\\\n    0  0  \\dots  0\n    \\end{pmatrix}.\n    $$\n    这个对角矩阵的特征值是其对角线元素：$n$ 和 $0$（重数为 $n-1$）。最大特征值是 $\\lambda_{\\max}(A^T A) = n$。\n    $2$-范数是这个值的平方根：\n    $$\n    \\|A\\|_{2 \\to 2} = \\sqrt{n}.\n    $$\n\n第三部分：比率向量与解释\n\n我们被要求计算比率向量 $r(n) = \\left( \\frac{\\|A\\|_{1 \\to 1}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\|A\\|_{\\infty \\to \\infty}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\sqrt{\\|A\\|_{1 \\to 1} \\, \\|A\\|_{\\infty \\to \\infty}}}{\\|A\\|_{2 \\to 2}} \\right)$。\n代入我们计算出的值：\n-   第一个分量：$\\frac{\\|A\\|_{1 \\to 1}}{\\|A\\|_{2 \\to 2}} = \\frac{n}{\\sqrt{n}} = \\sqrt{n}$。\n-   第二个分量：$\\frac{\\|A\\|_{\\infty \\to \\infty}}{\\|A\\|_{2 \\to 2}} = \\frac{1}{\\sqrt{n}}$。\n-   第三个分量：$\\frac{\\sqrt{\\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}}{\\|A\\|_{2 \\to 2}} = \\frac{\\sqrt{n \\cdot 1}}{\\sqrt{n}} = \\frac{\\sqrt{n}}{\\sqrt{n}} = 1$。\n\n所以，比率向量是：\n$$\nr(n) = \\left( \\sqrt{n}, \\; \\frac{1}{\\sqrt{n}}, \\; 1 \\right).\n$$\n\n当 $n$ 增长时，这些比率的行为凸显了对于这个特定矩阵，不同矩阵范数之间的差异。对于大的 $n$，$\\|A\\|_{1 \\to 1} = n$ 远大于 $\\|A\\|_{2 \\to 2} = \\sqrt{n}$，而后者又远大于 $\\|A\\|_{\\infty \\to \\infty} = 1$。发生这种情况是因为矩阵 $A$ 的所有“权重”都集中在单一一列中。$1$-范数对列和敏感，因此记录了一个很大的值。$\\infty$-范数对行和敏感，因此记录了一个很小的值。$2$-范数提供了一个中间的度量。\n\n这对数值算法的分析具有重要意义。许多误差界和稳定性分析都依赖于特定的矩阵范数选择。\n- 使用 $1$-范数的分析会将矩阵 $A$ 视为一个“大”对象，其范数随维度 $n$ 线性增长。这可能导致对应用于具有这种结构的矩阵的算法的误差传播或收敛速率的界估计过于悲观或宽松。\n- 相反，使用 $\\infty$-范数的分析会将 $A$ 视为一个“小”的、行为良好的对象，其范数为 $1$，与维度无关。这可能导致过于乐观的理论保证。\n- $2$-范数提供了一种几何平均行为，随 $\\sqrt{n}$ 缩放。$r(n)$ 的第三个分量为 $1$ 表明，对于这个特定的矩阵 $A$，锐不等式 $\\|A\\|_{2 \\to 2} \\le \\sqrt{\\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}$ 等号成立。\n\n这个例子鲜明地说明了，虽然有限维空间上的所有范数都是等价的，但矩阵范数的等价常数依赖于维度 $n$。对于具有高度非均匀结构的矩阵，范数的选择不是一个微不足道的细节；它可以从根本上改变从算法性能的理论分析中得出的结论。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{n} & \\frac{1}{\\sqrt{n}} & 1 \\end{pmatrix}}\n$$", "id": "3544612"}, {"introduction": "本练习将你置于一个“黑箱”挑战中，模拟一个与内部机制未知的系统交互的真实场景。你需要运用平行四边形法则等基本原理，首先判断未知范数的*类型*，然后设计策略来估计其等价常数。这个练习将理论知识与实践性的实验思维相结合，旨在解决一个非标准的侦测问题。[@problem_id:3544597]", "problem": "您将获得对一个黑箱残差报告器的查询权限。对于一个有限维实向量空间中的任何输入向量，该报告器会返回其在某个未知范数下的残差大小。形式上，对于给定的维度 $n \\in \\mathbb{N}$，您可以查询一个预言机，它对任意 $x \\in \\mathbb{R}^n$ 返回一个标量 $\\|x\\|_{?} \\in \\mathbb{R}_{\\ge 0}$。未知范数 $\\|\\cdot\\|_{?}$ 保证与 $\\mathbb{R}^n$ 上的欧几里得范数 $\\|\\cdot\\|_{2}$ 等价，即存在常数 $c_1, c_2 \\in \\mathbb{R}$ 满足 $0 < c_1 \\le c_2 < \\infty$，使得对所有 $x \\in \\mathbb{R}^n$，不等式 $c_1 \\|x\\|_2 \\le \\|x\\|_{?} \\le c_2 \\|x\\|_2$ 均成立。任务是设计一个检测实验，并将其实现为一个程序。该程序仅通过查询 $\\|\\cdot\\|_{?}$ 来估计 $c_1$ 和 $c_2$，并利用这些估计值重新校准停止阈值，以满足预设的欧几里得残差目标。\n\n您的设计和实现必须基于以下基本原理：\n- 范数的定义、欧几里得范数 $\\|\\cdot\\|_2$ 以及有限维空间上的范数等价性。\n- 通过平行四边形法则对内积诱导范数的特征描述：一个范数 $\\|\\cdot\\|$ 是由某个内积诱导的，当且仅当对所有 $x,y \\in \\mathbb{R}^n$，恒等式 $\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2$ 成立。\n- 实内积空间中的极化恒等式：对于一个内积诱导的范数 $\\|\\cdot\\|$，其关联的内积 $\\langle x,y\\rangle$ 对所有 $x,y \\in \\mathbb{R}^n$ 满足 $\\langle x,y\\rangle = \\tfrac{1}{4}(\\|x+y\\|^2 - \\|x-y\\|^2)$。\n- 当未知范数由对称正定 (SPD) 矩阵 $M \\in \\mathbb{R}^{n \\times n}$ 通过 $\\|x\\|_{M} = \\sqrt{x^\\top M x}$ 诱导时，等价常数的谱特征描述：若 $\\lambda_{\\min}(M)$ 和 $\\lambda_{\\max}(M)$ 表示 $M$ 的最小和最大特征值，则对所有 $x \\in \\mathbb{R}^n$，不等式 $\\sqrt{\\lambda_{\\min}(M)} \\|x\\|_2 \\le \\|x\\|_M \\le \\sqrt{\\lambda_{\\max}(M)} \\|x\\|_2$ 成立。\n- 关联 $\\mathbb{R}^n$ 上 $\\ell_p$ 范数和欧几里得范数的基本不等式：对于 $\\ell_1$ 范数，对所有 $x \\in \\mathbb{R}^n$ 有 $\\|x\\|_2 \\le \\|x\\|_1 \\le \\sqrt{n} \\|x\\|_2$；对于 $\\ell_\\infty$ 范数，对所有 $x \\in \\mathbb{R}^n$ 有 $n^{-1/2}\\|x\\|_2 \\le \\|x\\|_\\infty \\le \\|x\\|_2$。\n\n您的程序必须实现以下检测实验：\n- 对于给定的固定维度 $n$ 和对 $\\| \\cdot \\|_{?}$ 的查询权限，首先通过在一组随机采样的向量对 $(x,y)$ 上检验平行四边形法则，来测试 $\\|\\cdot\\|_{?}$ 是否由内积诱导。如果测试表明（在您必须证明其合理性的数值容差范围内）这是一个内积诱导的范数，则使用仅限于标准基向量的极化恒等式来重构相关的格拉姆矩阵 $G \\in \\mathbb{R}^{n \\times n}$。然后，将 $c_1$ 和 $c_2$ 分别估计为 $\\sqrt{\\lambda_{\\min}(G)}$ 和 $\\sqrt{\\lambda_{\\max}(G)}$，其中 $\\lambda_{\\min}(G)$ 和 $\\lambda_{\\max}(G)$ 是 $G$ 的最小和最大特征值。\n- 如果平行四边形法则测试失败（表明这是一个不一定由内积诱导的一般范数），则通过在球面 $\\{x \\in \\mathbb{R}^n : \\|x\\|_2 = 1\\}$ 上采样单位欧几里得向量，并计算该样本上比率 $\\|x\\|_{?}/\\|x\\|_2$ 的最小值和最大值来估计 $c_1$ 和 $c_2$。您的采样集必须包括一个确定性的候选方向覆盖，该覆盖能捕获经典范数的极值点，包括所有坐标轴及其负方向，以及均匀方向 $u = \\tfrac{1}{\\sqrt{n}}(1,\\dots,1)^\\top$，同时还要包括适量的随机方向。请说明为什么这个集合能够检测到 $\\ell_1$ 和 $\\ell_\\infty$ 范数的精确常数，并在一般情况下提供上界和下界。\n\n重新校准规则：\n- 假设一个黑箱求解器在残差向量 $r \\in \\mathbb{R}^n$ 满足 $\\|r\\|_{?} \\le \\tau$ 时停止。根据范数等价性，确保 $\\|r\\|_2 \\le \\varepsilon_2$ 的一个安全充分条件是 $\\tau \\le c_1 \\varepsilon_2$。因此，给定一个目标欧几里得残差容差 $\\varepsilon_2 > 0$，您的程序必须使用您的估计值 $\\widehat{c}_1$ 来计算 $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$。\n\n测试套件：\n实现您的程序，使其能在以下测试用例上运行。每个用例定义了维度 $n$、通过预言机定义的未知范数 $\\|\\cdot\\|_{?}$ 和一个目标欧几里得容差 $\\varepsilon_2$。对于每个用例，您的程序必须估计 $\\widehat{c}_1$ 和 $\\widehat{c}_2$，计算 $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$，并报告真实的常数 $c_1$ 和 $c_2$ 以供验证。\n\n- 用例 A：$n = 3$。未知范数 $\\|x\\|_{?} = \\sqrt{x^\\top M x}$，其中 $M = \\mathrm{diag}(1, 4, 9)$。目标 $\\varepsilon_2 = 10^{-4}$。\n- 用例 B：$n = 4$。未知范数 $\\|x\\|_{?} = \\|x\\|_1$。目标 $\\varepsilon_2 = 10^{-3}$。\n- 用例 C：$n = 5$。未知范数 $\\|x\\|_{?} = \\|x\\|_\\infty$。目标 $\\varepsilon_2 = 2 \\times 10^{-6}$。\n- 用例 D：$n = 3$。未知范数 $\\|x\\|_{?} = \\sqrt{x^\\top M x}$，其中 $M = \\mathrm{diag}(10^{-4}, 1, 100)$。目标 $\\varepsilon_2 = 10^{-5}$。\n- 用例 E：$n = 1$。未知范数 $\\|x\\|_{?} = \\alpha \\|x\\|_2$，其中 $\\alpha = 7$。目标 $\\varepsilon_2 = 3 \\times 10^{-5}$。\n\n答案规格：\n- 对于每个用例，您的程序必须按此顺序输出一个包含五个浮点数的列表 $[\\widehat{c}_1, \\widehat{c}_2, \\widehat{\\tau}, c_1, c_2]$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，列表中的每个元素本身是对应一个用例的列表，其顺序与上面用例出现的顺序相同。例如，输出应类似于 $[[a_1,a_2,a_3,a_4,a_5],[b_1,b_2,b_3,b_4,b_5],\\dots]$。\n- 所有计算都是无量纲的，不涉及任何物理单位。\n- 您的实现必须是自包含的，且不得使用任何外部输入。检测容差、采样大小以及任何随机化组件都必须在您的解题说明中证明其合理性，并进行选择以确保对于指定的测试套件，估计值在所有用例中都与真实常数完全匹配。", "solution": "该问题要求设计并实现一个实验，用以估计未知范数 $\\|\\cdot\\|_{?}$ 与 $\\mathbb{R}^n$ 上的标准欧几里得范数 $\\|\\cdot\\|_2$ 之间的等价常数 $c_1$ 和 $c_2$。这些常数由不等式 $c_1 \\|x\\|_2 \\le \\|x\\|_{?} \\le c_2 \\|x\\|_2$（对所有 $x \\in \\mathbb{R}^n$ 成立）定义。估计过程仅能通过查询 $\\|\\cdot\\|_{?}$ 的预言机来执行。然后，这些估计值将用于重新校准求解器的停止阈值 $\\tau$，以满足预设的欧几里得容差 $\\varepsilon_2$。\n\n该设计根植于向量空间范数的一个基本性质：一个范数由内积诱导，当且仅当它满足平行四边形法则。这为区分两类范数提供了明确的标准，每一类范数都有其不同且高效的估计策略。\n\n整体算法分三个阶段进行：\n1.  通过在一组随机向量样本上验证平行四边形法则，测试未知范数 $\\|\\cdot\\|_{?}$ 是否由内积诱导。\n2.  根据测试结果，应用专门的程序来估计 $c_1$ 和 $c_2$。\n    a. 如果是内积诱导的范数，则重构其关联的格拉姆矩阵，并根据其特征值计算常数。\n    b. 如果是一般范数，则通过在一组精心选择的方向上对单位球面进行采样来估计常数。\n3.  使用估计值 $\\widehat{c}_1$ 计算重新校准的停止阈值 $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$。\n\n我们现在详细说明此设计的每个阶段。\n\n**阶段 1：平行四边形法则测试**\n\n一个实向量空间上的范数 $\\|\\cdot\\|$ 是由内积诱导的，当且仅当它对所有向量 $x, y$ 满足平行四边形法则：\n$$\n\\|x+y\\|^2 + \\|x-y\\|^2 = 2\\|x\\|^2 + 2\\|y\\|^2\n$$\n由于浮点运算的存在，精确的相等性检查是不可行的。因此，我们改为对一组随机生成的向量对 $(x,y)$ 进行概率性测试，检查其相对误差是否在小的数值容差 $\\delta_{PL}$ 之内。对每一对向量，我们计算误差：\n$$\nE(x,y) = \\frac{| (\\|x+y\\|_{?}^2 + \\|x-y\\|_{?}^2) - (2\\|x\\|_{?}^2 + 2\\|y\\|_{?}^2) |}{2\\|x\\|_{?}^2 + 2\\|y\\|_{?}^2}\n$$\n如果在足够数量的样本（例如 $100$ 个）上的最大误差小于 $\\delta_{PL}$，我们就将该范数归类为由内积诱导。容差 $\\delta_{PL} = 10^{-9}$ 是合适的，因为它足够小，可以容纳标准的双精度浮点不精确性，但又足够大，能够检测出非内积诱导范数的结构性违反。例如，对于像标准基向量这样的简单向量，$\\ell_1$ 和 $\\ell_\\infty$ 范数对此法则表现出大的、数量级为一的违反，这很容易通过随机采样检测出来。\n\n**阶段 2a：内积诱导范数的估计**\n\n如果平行四边形法则测试通过，我们就在 $\\|\\cdot\\|_{?}$ 是由内积 $\\langle \\cdot, \\cdot \\rangle_?$ 诱导的假设下继续，即 $\\|x\\|_{?}^2 = \\langle x, x \\rangle_?$。这个内积可以由一个关于标准基 $\\{e_i\\}_{i=1}^n$ 的对称正定 (SPD) 矩阵 $G \\in \\mathbb{R}^{n \\times n}$ 表示，该矩阵称为格拉姆矩阵。我们有 $\\|x\\|_{?}^2 = x^\\top G x$。该矩阵的元素由 $G_{ij} = \\langle e_i, e_j \\rangle_?$ 给出。\n\n为了重构 $G$，我们利用极化恒等式，它用范数来表示内积：\n$$\n\\langle u, v \\rangle_? = \\frac{1}{4} \\left( \\|u+v\\|_{?}^2 - \\|u-v\\|_{?}^2 \\right)\n$$\n通过对所有 $i,j \\in \\{1,\\dots,n\\}$ 代入 $u=e_i$ 和 $v=e_j$，我们可以利用对预言机的查询来计算 $G$ 的每一个元素。\n$$\nG_{ij} = \\frac{1}{4} \\left( \\|e_i+e_j\\|_{?}^2 - \\|e_i-e_j\\|_{?}^2 \\right)\n$$\n一旦矩阵 $\\widehat{G}$ 在数值上被构建出来，等价常数就由其最小和最大特征值的平方根给出。这是 Rayleigh-Ritz 定理的一个直接推论，该定理指出，对于任何非零向量 $x \\in \\mathbb{R}^n$：\n$$\n\\lambda_{\\min}(G) \\le \\frac{x^\\top G x}{x^\\top x} \\le \\lambda_{\\max}(G)\n$$\n代入 $\\|x\\|_{?}^2 = x^\\top G x$ 和 $\\|x\\|_2^2 = x^\\top x$，我们得到 $\\lambda_{\\min}(G) \\|x\\|_2^2 \\le \\|x\\|_{?}^2 \\le \\lambda_{\\max}(G) \\|x\\|_2^2$。取平方根即可得到等价不等式，其中 $c_1 = \\sqrt{\\lambda_{\\min}(G)}$ 和 $c_2 = \\sqrt{\\lambda_{\\max}(G)}$。因此，我们的估计值为 $\\widehat{c}_1 = \\sqrt{\\lambda_{\\min}(\\widehat{G})}$ 和 $\\widehat{c}_2 = \\sqrt{\\lambda_{\\max}(\\widehat{G})}$。\n\n**阶段 2b：一般范数的估计**\n\n如果平行四边形法则测试失败，我们必须采用另一种方法。常数 $c_1$ 和 $c_2$ 在形式上被定义为函数 $f(x) = \\|x\\|_{?}$ 在单位球面 $S^{n-1} = \\{x \\in \\mathbb{R}^n : \\|x\\|_2 = 1\\}$ 上的最小值和最大值：\n$$\nc_1 = \\min_{x \\in S^{n-1}} \\|x\\|_{?} \\quad , \\quad c_2 = \\max_{x \\in S^{n-1}} \\|x\\|_{?}\n$$\n由于搜索整个连续球面是不可能的，我们通过在一个有限的、精心选择的样本向量子集上寻找最小值和最大值来近似求解。根据问题的要求，我们的采样集将包括：\n1.  一组确定性的方向，已知它们是常见范数的极值点方向：$\\{ \\pm e_i \\}_{i=1}^n$（坐标轴）和 $\\pm \\frac{1}{\\sqrt{n}}(1, \\dots, 1)^\\top$（均匀方向）。\n2.  一组随机生成的方向，归一化为单位欧几里得长度，以改进对确定性集合未覆盖的任意范数的估计。\n\n这种确定性集合的选择是至关重要的。对于 $\\ell_1$ 范数，一个标准结果是 $\\|x\\|_2 \\le \\|x\\|_1 \\le \\sqrt{n}\\|x\\|_2$。下界在 $x=e_i$ 时达到，此时比率为 $\\|e_i\\|_1/\\|e_i\\|_2 = 1/1=1$。上界在 $x = \\frac{1}{\\sqrt{n}}(1,\\dots,1)^\\top$ 时达到，此时比率为 $\\|x\\|_1/\\|x\\|_2 = \\sqrt{n}/1 = \\sqrt{n}$。因此，我们的确定性集合保证能找到 $\\ell_1$ 范数的精确常数 $c_1=1$ 和 $c_2=\\sqrt{n}$。\n\n对于 $\\ell_\\infty$ 范数，标准结果是 $n^{-1/2}\\|x\\|_2 \\le \\|x\\|_\\infty \\le \\|x\\|_2$。下界在 $x = \\frac{1}{\\sqrt{n}}(1,\\dots,1)^\\top$ 时达到，比率为 $n^{-1/2}$。上界在 $x=e_i$ 时达到，比率为 $1$。同样，我们的确定性集合能找到精确常数 $c_1=n^{-1/2}$ 和 $c_2=1$。\n\n对于所提供的测试用例，此采样策略足以找到精确的常数。估计值为 $\\widehat{c}_1 = \\min_{x \\in S} \\|x\\|_{?}$ 和 $\\widehat{c}_2 = \\max_{x \\in S} \\|x\\|_{?}$，其中 $S$ 是我们合并后的单位向量样本集。\n\n**阶段 3：阈值重新校准**\n\n最后一步是为一个黑箱求解器计算新的停止阈值 $\\widehat{\\tau}$。我们的目标是确保最终的残差向量 $r$ 满足 $\\|r\\|_2 \\le \\varepsilon_2$。范数等价不等式 $\\|r\\|_{?} \\ge c_1 \\|r\\|_2$ 蕴含了 $\\|r\\|_2 \\le \\frac{1}{c_1} \\|r\\|_{?}$。为了满足 $\\|r\\|_2 \\le \\varepsilon_2$，只需强制 $\\frac{1}{c_1} \\|r\\|_{?} \\le \\varepsilon_2$ 即可，这等价于 $\\|r\\|_{?} \\le c_1 \\varepsilon_2$。求解器在 $\\|r\\|_{?} \\le \\tau$ 时停止，因此选择 $\\tau \\le c_1 \\varepsilon_2$ 提供了一个安全的停止条件。\n\n问题指定将重新校准的阈值计算为 $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$。这是一个有效且安全的程序，前提是我们的估计值 $\\widehat{c}_1$ 等于或小于真实常数 $c_1$。对于内积范数的情况，我们的重构技术在浮点精度范围内是精确的，因此 $\\widehat{c}_1 \\approx c_1$。对于测试套件中的一般范数情况，我们的确定性采样策略确保了 $\\widehat{c}_1 = c_1$。在更一般的情况下，如果采样可能只产生一个上界 $\\widehat{c}_1 \\ge c_1$，那么规则 $\\widehat{\\tau} = \\widehat{c}_1 \\varepsilon_2$ 将不能保证是安全的。然而，就本问题的范围和要求而言，该程序是可靠的。", "answer": "```python\nimport numpy as np\n\ndef test_parallelogram_law(oracle, n, num_samples=100, tolerance=1e-9):\n    \"\"\"\n    Tests if a norm satisfies the parallelogram law within a given tolerance.\n\n    A norm ||.|| is induced by an inner product if and only if\n    ||x+y||^2 + ||x-y||^2 = 2(||x||^2 + ||y||^2) for all x, y.\n\n    Args:\n        oracle (callable): The black-box norm function.\n        n (int): The dimension of the vector space.\n        num_samples (int): The number of random vector pairs to test.\n        tolerance (float): The relative error tolerance for the check.\n\n    Returns:\n        bool: True if the law holds for all samples, False otherwise.\n    \"\"\"\n    # Fix the random seed for reproducibility\n    rng = np.random.default_rng(seed=42)\n    max_relative_error = 0.0\n\n    for _ in range(num_samples):\n        x = rng.standard_normal(n)\n        y = rng.standard_normal(n)\n\n        # Handle the case where x and y are zero vectors\n        norm_x_sq = oracle(x)**2\n        norm_y_sq = oracle(y)**2\n        if norm_x_sq == 0 and norm_y_sq == 0:\n            continue\n\n        lhs = oracle(x + y)**2 + oracle(x - y)**2\n        rhs = 2 * (norm_x_sq + norm_y_sq)\n\n        if rhs == 0: # This case should not be hit if x or y is non-zero\n            relative_error = 0.0 if lhs == 0 else np.inf\n        else:\n            relative_error = np.abs(lhs - rhs) / rhs\n        \n        if relative_error > max_relative_error:\n            max_relative_error = relative_error\n\n    return max_relative_error < tolerance\n\ndef estimate_from_gram_matrix(oracle, n):\n    \"\"\"\n    Estimates c1 and c2 for an inner-product-induced norm.\n\n    This is done by reconstructing the Gram matrix G using the polarization\n    identity and finding the square roots of its min and max eigenvalues.\n\n    Args:\n        oracle (callable): The black-box norm function.\n        n (int): The dimension of the vector space.\n\n    Returns:\n        tuple[float, float]: The estimated constants (c1_hat, c2_hat).\n    \"\"\"\n    G = np.zeros((n, n))\n    I = np.eye(n)\n    \n    for i in range(n):\n        for j in range(i, n):\n            ei, ej = I[:, i], I[:, j]\n            val = 0.25 * (oracle(ei + ej)**2 - oracle(ei - ej)**2)\n            G[i, j] = val\n            if i != j:\n                G[j, i] = val\n\n    # Eigenvalues of a real symmetric matrix are real.\n    # eigvalsh is efficient for symmetric matrices.\n    eigenvalues = np.linalg.eigvalsh(G)\n    \n    lambda_min = np.min(eigenvalues)\n    lambda_max = np.max(eigenvalues)\n\n    # The eigenvalues of an SPD matrix must be positive.\n    # Add a small clip for numerical stability near zero.\n    c1_hat = np.sqrt(np.maximum(0, lambda_min))\n    c2_hat = np.sqrt(np.maximum(0, lambda_max))\n    \n    return c1_hat, c2_hat\n\ndef estimate_from_sampling(oracle, n, num_random_samples=1000):\n    \"\"\"\n    Estimates c1 and c2 for a general norm by sampling the unit sphere.\n\n    The sample set includes deterministic directions that are extremizers for\n    l1 and l_inf norms, plus random directions.\n\n    Args:\n        oracle (callable): The black-box norm function.\n        n (int): The dimension of the vector space.\n        num_random_samples (int): The number of random directions to sample.\n\n    Returns:\n        tuple[float, float]: The estimated constants (c1_hat, c2_hat).\n    \"\"\"\n    # 1. Deterministic sample set\n    samples = []\n    \n    # Standard basis vectors and their negatives\n    I = np.eye(n)\n    for i in range(n):\n        samples.append(I[:, i])\n        samples.append(-I[:, i])\n\n    # Uniform direction vector and its negative\n    if n > 0:\n        uniform_vec = np.ones(n) / np.sqrt(n)\n        samples.append(uniform_vec)\n        samples.append(-uniform_vec)\n\n    # 2. Random sample set\n    rng = np.random.default_rng(seed=42)\n    random_vectors = rng.standard_normal((num_random_samples, n))\n    norms = np.linalg.norm(random_vectors, axis=1, keepdims=True)\n    # Avoid division by zero if a zero vector is somehow generated\n    non_zero_norms = np.where(norms == 0, 1, norms)\n    normalized_random_vectors = random_vectors / non_zero_norms\n    \n    for vec in normalized_random_vectors:\n        samples.append(vec)\n        \n    # All vectors in samples have ||x||_2 = 1\n    # The value of ||x||_? is the ratio ||x||_? / ||x||_2\n    norm_values = [oracle(x) for x in samples]\n    \n    c1_hat = np.min(norm_values)\n    c2_hat = np.max(norm_values)\n    \n    return c1_hat, c2_hat\n\n\ndef solve():\n    \"\"\"\n    Main solver function to run the detection experiment on all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            \"n\": 3,\n            \"oracle\": lambda x: np.sqrt(x.T @ np.diag([1, 4, 9]) @ x),\n            \"eps2\": 1e-4,\n            \"c1_true\": 1.0,\n            \"c2_true\": 3.0,\n        },\n        # Case B\n        {\n            \"n\": 4,\n            \"oracle\": lambda x: np.linalg.norm(x, ord=1),\n            \"eps2\": 1e-3,\n            \"c1_true\": 1.0,\n            \"c2_true\": 2.0,\n        },\n        # Case C\n        {\n            \"n\": 5,\n            \"oracle\": lambda x: np.linalg.norm(x, ord=np.inf),\n            \"eps2\": 2e-6,\n            \"c1_true\": 1 / np.sqrt(5),\n            \"c2_true\": 1.0,\n        },\n        # Case D\n        {\n            \"n\": 3,\n            \"oracle\": lambda x: np.sqrt(x.T @ np.diag([1e-4, 1, 100]) @ x),\n            \"eps2\": 1e-5,\n            \"c1_true\": 0.01,\n            \"c2_true\": 10.0,\n        },\n        # Case E\n        {\n            \"n\": 1,\n            \"oracle\": lambda x: 7 * np.linalg.norm(x, ord=2),\n            \"eps2\": 3e-5,\n            \"c1_true\": 7.0,\n            \"c2_true\": 7.0,\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n = case[\"n\"]\n        oracle = case[\"oracle\"]\n        eps2 = case[\"eps2\"]\n        c1_true = case[\"c1_true\"]\n        c2_true = case[\"c2_true\"]\n        \n        is_inner_product = test_parallelogram_law(oracle, n)\n        \n        if is_inner_product:\n            c1_hat, c2_hat = estimate_from_gram_matrix(oracle, n)\n        else:\n            # For general norms, the number of random samples is chosen\n            # to be moderate as per the problem. \n            # 100*n is more than sufficient.\n            c1_hat, c2_hat = estimate_from_sampling(oracle, n, num_random_samples=100*n)\n            \n        tau_hat = c1_hat * eps2\n        \n        all_results.append([c1_hat, c2_hat, tau_hat, c1_true, c2_true])\n        \n    # Format the output as a string representing a list of lists.\n    # Python's str() on a list gives a square-bracketed representation,\n    # so we just join these strings with commas.\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3544597"}]}