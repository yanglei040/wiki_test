{"hands_on_practices": [{"introduction": "在许多反问题中，不适定性源于模型本身的结构，而非数据噪声。这个练习通过一个简单的模型，揭示了当一个空间场和一个全局增益同时未知时产生的尺度模糊性。通过分析推导这种模糊性并设计约束来消除它，你将学会如何在建模阶段就诊断并解决参数不可辨识性问题 [@problem_id:3412214]。", "problem": "考虑数据同化中的一个线性测量算子，该算子由一个未知的空间场和一个未知的全局增益定义：一个状态向量 $x \\in \\mathbb{R}^{m}$ 和一个标量 $k \\in \\mathbb{R}$ 通过正演模型 $y = k x$ 共同生成无噪声观测值 $y \\in \\mathbb{R}^{m}$。$x$ 和 $k$ 均为未知，必须从 $y$ 推断得出。目标是量化由参数不可辨识性引起的实际不适定性来源，并设计恢复适定性的约束条件。\n\n从以下定义出发：如果参数的任何非平凡变换都不会改变数据，则参数是可辨识的；局部可辨识性由数据映射相对于参数的雅可比矩阵的秩确定。请完成以下任务：\n\n1. 证明对于任何非零标量 $c \\in \\mathbb{R} \\setminus \\{0\\}$，变换 $(x, k) \\mapsto (c x, k / c)$ 保持 $y$ 不变。利用这种不变性来刻画对于通用情况 $y \\neq 0$ 时产生相同 $y$ 的参数等价类。\n2. 通过分析映射 $\\theta \\mapsto y(\\theta)$ 在一个通用点处的雅可比矩阵的零空间，计算连续模糊性的维度（即在保持 $y$ 固定的情况下 $(x, k)$ 可以变化的连续自由度数），其中 $\\theta = (x, k) \\in \\mathbb{R}^{m} \\times \\mathbb{R}$ 且 $y(\\theta) = k x$。\n3. 提出在科学上合理且在实践中可强制执行的约束条件，以打破第2部分中确定的模糊性并恢复局部可辨识性。然后，通过证明在一个通用点处，约束流形与模糊性方向的交集是平凡的，来论证这些约束消除了所有连续的不可辨识性。你可以假设约束条件为 $g_{1}(x, k) = \\|x\\|_{2} - 1 = 0$ 和 $g_{2}(x, k): k > 0$。\n4. 报告两个量：施加任何约束之前的连续模糊性维度，以及施加 $g_{1}$ 和 $g_{2}$ 两个约束之后的连续模糊性维度。以行向量的形式提供你的最终答案，不要包含单位。不需要四舍五入。\n\n你的推导必须是自洽的，并从可辨识性和局域微分分析的核心定义开始。避免使用快捷公式；相反，应从数据映射及其雅可比矩阵推导出结果。确保你提出的约束在反问题和数据同化领域中被证明是实践上合理的。", "solution": "该问题要求在一个简单的线性反问题中分析不适定性，特别是参数的不可辨识性。将状态向量 $x \\in \\mathbb{R}^{m}$ 和标量增益 $k \\in \\mathbb{R}$ 映射到一组无噪声观测值 $y \\in \\mathbb{R}^{m}$ 的正演模型由以下公式给出：\n$$ y = kx $$\n总参数向量为 $\\theta = (x, k)$，它位于参数空间 $\\mathbb{R}^{m} \\times \\mathbb{R} \\cong \\mathbb{R}^{m+1}$ 中。我们被要求刻画不可辨识性，提出解决该问题的约束条件，并报告施加约束前后模糊性的维度。\n\n1.  **变换下的不变性与等价类刻画**\n\n我们被要求证明对于任何非零标量 $c \\in \\mathbb{R} \\setminus \\{0\\}$，变换 $(x, k) \\mapsto (c x, k / c)$ 保持观测值 $y$ 不变。设原始参数为 $(x, k)$，变换后的参数为 $(x', k')$，其中 $x' = cx$ 且 $k' = k/c$。\n由变换后的参数生成的观测值 $y'$ 为：\n$$ y' = k' x' = \\left(\\frac{k}{c}\\right) (c x) $$\n根据标量乘法的结合律，我们有：\n$$ y' = \\left(\\frac{k}{c} \\cdot c\\right) x = (1) kx = kx $$\n由于 $y = kx$，我们得到 $y' = y$。这证明了对于任何 $c \\in \\mathbb{R} \\setminus \\{0\\}$，观测值 $y$ 在此变换下是不变的。\n\n这种不变性在参数空间上定义了一个等价关系。对于一个通用的、非零的观测值 $y \\neq 0$，所有产生该观测值的参数对 $(\\tilde{x}, \\tilde{k})$ 都属于同一个等价类。设 $(x, k)$ 是这样一个参数对，因此 $y = kx$。那么，同一类中的任何其他参数对 $(\\tilde{x}, \\tilde{k})$ 都必须满足 $\\tilde{k}\\tilde{x} = y = kx$。\n由于 $y \\neq 0$，所以 $k$ 和 $x$ 都不能为零（或零向量）。因此，$\\tilde{x}$ 必须是 $x$ 的标量倍数，即对于某个标量 $c \\in \\mathbb{R}$，有 $\\tilde{x} = cx$。由于 $\\tilde{x}$ 必须非零（因为 $\\tilde{k}\\tilde{x} = y \\neq 0$），我们必须有 $c \\neq 0$。\n将 $\\tilde{x} = cx$ 代入方程，得到：\n$$ \\tilde{k}(cx) = kx $$\n$$ (c\\tilde{k} - k)x = 0 $$\n由于 $x \\neq 0$，标量因子必须为零：$c\\tilde{k} - k = 0$，这意味着 $\\tilde{k} = k/c$。\n因此，产生给定非零观测值 $y$ 的参数等价类是所有形如 $(cx, k/c)$ 的参数对的集合，其中 $c \\in \\mathbb{R} \\setminus \\{0\\}$，由一个参考对 $(x, k)$ 定义。该集合在参数空间中形成一个一维双曲线。\n\n2.  **通过雅可比分析计算连续模糊性的维度**\n\n局部可辨识性通过正演映射 $F(\\theta) = y(\\theta)$（其中 $\\theta = (x, k)$）的雅可比矩阵来评估。在一点 $\\theta$ 处的连续模糊性维度是在该点 $\\theta$ 处计算的雅可比矩阵 $J$ 的零空间维度。\n正演映射为 $F:(x_1, \\dots, x_m, k) \\mapsto (kx_1, \\dots, kx_m)$。雅可比矩阵 $J$ 是一个 $m \\times (m+1)$ 矩阵，其中 $J_{ij} = \\frac{\\partial y_i}{\\partial \\theta_j}$。参数向量为 $\\theta = (x_1, \\dots, x_m, k)^T$。\n\n偏导数如下：\n对于 $j \\in \\{1, \\dots, m\\}$，$\\frac{\\partial y_i}{\\partial x_j} = \\frac{\\partial(kx_i)}{\\partial x_j} = k \\delta_{ij}$，其中 $\\delta_{ij}$ 是克罗内克 delta。\n对于第 $(m+1)$ 个参数 $k$，$\\frac{\\partial y_i}{\\partial k} = \\frac{\\partial(kx_i)}{\\partial k} = x_i$。\n\n雅可比矩阵是：\n$$ J = \\begin{pmatrix} \\frac{\\partial y_1}{\\partial x_1}  \\dots  \\frac{\\partial y_1}{\\partial x_m}  \\frac{\\partial y_1}{\\partial k} \\\\ \\vdots  \\ddots  \\vdots  \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_1}  \\dots  \\frac{\\partial y_m}{\\partial x_m}  \\frac{\\partial y_m}{\\partial k} \\end{pmatrix} = \\begin{pmatrix} k  0  \\dots  0  x_1 \\\\ 0  k  \\dots  0  x_2 \\\\ \\vdots  \\vdots  \\ddots  \\vdots  \\vdots \\\\ 0  0  \\dots  k  x_m \\end{pmatrix} $$\n这可以写成块形式 $J = [k I_m \\mid x]$，其中 $I_m$ 是 $m \\times m$ 的单位矩阵。\n\n$J$ 的零空间由向量 $v = (\\delta x, \\delta k)^T \\in \\mathbb{R}^{m+1}$ 组成，满足 $Jv = 0$。\n$$ [k I_m \\mid x] \\begin{pmatrix} \\delta x \\\\ \\delta k \\end{pmatrix} = 0 $$\n$$ k \\delta x + x \\delta k = 0 $$\n在一个通用点，我们假设 $k \\neq 0$。然后我们可以求解 $\\delta x$：\n$$ \\delta x = -\\frac{\\delta k}{k} x $$\n令 $\\delta k = \\alpha$，其中 $\\alpha$ 是任意实数。那么零空间中的任何向量 $v$ 都可以写成：\n$$ v = \\begin{pmatrix} -\\frac{\\alpha}{k} x \\\\ \\alpha \\end{pmatrix} = \\alpha \\begin{pmatrix} -x/k \\\\ 1 \\end{pmatrix} $$\n零空间由单个向量 $v_0 = (-x/k, 1)^T$ 张成。向量空间的维度是其基中向量的数量。由于零空间由单个非零向量张成，其维度为 $1$。\n这个单一自由度对应于第1部分中确定的连续模糊性。模糊性曲线 $\\gamma(c) = (cx, k/c)$ 在 $c=1$ 处的切向量是 $\\gamma'(1) = (x, -k)$，它是 $v_0$ 的一个非零标量倍数（具体来说，是写成 $(x, -k)$ 形式的 $v_0$ 的 $-k$ 倍），因此位于同一零空间中。\n连续模糊性的维度是 $\\dim(\\ker(J)) = 1$。\n\n3.  **用约束恢复可辨识性**\n\n我们引入两个约束：$g_1(x, k) = \\|x\\|_2 - 1 = 0$ 和 $g_2(x, k): k > 0$。\n**科学论证：**模糊性 $y = (k/c)(cx)$ 表明尺度是任意的。约束 $\\|x\\|_2=1$ 是一种归一化，它固定了空间场 $x$ 的尺度，迫使整体振幅的任何变化都被增益 $k$ 吸收。当 $x$ 代表一个模态形状或模式，其绝对大小不如其形式重要时，这种做法是实用的。约束 $k>0$ 对于本质上为正的物理参数是常见的，例如浓度、强度或绝对温度。\n\n**有效性证明：**为了消除连续模糊性，模糊性方向不能是约束流形上的可行变化方向。一个无穷小变化 $(\\delta x, \\delta k)$ 在流形 $g_1=0$ 上是可行的，当且仅当它与流形相切，即与 $g_1$ 的梯度正交。\n$g_1(x,k) = (\\sum_{i=1}^m x_i^2)^{1/2} - 1$ 相对于 $\\theta=(x, k)$ 的梯度是：\n$$ \\nabla g_1 = \\left( \\frac{\\partial g_1}{\\partial x_1}, \\dots, \\frac{\\partial g_1}{\\partial x_m}, \\frac{\\partial g_1}{\\partial k} \\right) $$\n导数为 $\\frac{\\partial g_1}{\\partial x_j} = \\frac{x_j}{\\|x\\|_2}$ 和 $\\frac{\\partial g_1}{\\partial k} = 0$。所以，$\\nabla g_1 = (\\frac{x}{\\|x\\|_2}, 0)$。\n在约束流形本身上，$\\|x\\|_2=1$，所以梯度简化为 $\\nabla g_1 = (x, 0)$。\n切线条件是 $\\nabla g_1 \\cdot (\\delta x, \\delta k)^T = 0$。\n$$ (x, 0) \\cdot \\begin{pmatrix} \\delta x \\\\ \\delta k \\end{pmatrix} = x^T \\delta x = 0 $$\n表示模糊性方向的向量形式为 $v = (\\alpha x, -\\alpha k)$，对于某个标量 $\\alpha$（这是切向量 $\\alpha \\gamma'(1)$）。为了使这种变化在约束下成为可能，它必须满足切线条件：\n$$ x^T (\\alpha x) = \\alpha (x^T x) = \\alpha \\|x\\|_2^2 = 0 $$\n由于我们在约束流形上，$\\|x\\|_2 = 1$，所以条件变为 $\\alpha(1)^2 = \\alpha = 0$。\n$\\alpha=0$ 意味着沿着模糊性方向唯一允许的变化是零向量 $(\\delta x, \\delta k) = (0, 0)$。这意味着约束 $g_1$ 消除了所有连续的不可辨识性。$J$ 的零空间与约束流形的切空间的交集是 $\\{0\\}$，其维度为 $0$。\n\n仅靠约束 $g_1$ 不足以保证唯一解。它将等价类从一条连续曲线减少为一组离散的点。如果 $(x,k)$ 是一个满足 $\\|x\\|_2 = 1$ 的解，那么 $(\\tilde{x}, \\tilde{k}) = (cx, k/c)$ 是一个等价解。为了使 $\\|\\tilde{x}\\|_2 = 1$，我们需要 $\\|cx\\|_2 = |c|\\|x\\|_2 = |c| = 1$，这意味着 $c=1$ 或 $c=-1$。\n这就留下了两个可能的解：$(x, k)$ 和 $(-x, -k)$。\n第二个约束 $g_2: k>0$ 解决了这个剩余的离散模糊性。如果我们假设我们的解 $(x,k)$ 满足 $k>0$，那么另一个解 $(-x, -k)$ 的增益为 $-k  0$，这违反了 $g_2$。因此，只剩下一个解。\n\n$g_1$ 和 $g_2$ 的共同作用是从每个等价类中选择一个单点，从而恢复局部（在本例中也是全局）可辨识性。任何剩余的连续模糊性的维度都是 $0$。\n\n4.  **最终量**\n\n- 施加任何约束之前的连续模糊性维度是雅可比矩阵零空间的维度，即 $1$。\n- 施加约束 $g_1(x,k) = \\|x\\|_2 - 1 = 0$ 和 $g_2(x,k): k > 0$ 之后的连续模糊性维度是雅可比矩阵零空间与活动约束的切空间的交集的维度。我们发现这个维度是 $0$。\n这两个量是 $1$ 和 $0$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  0\n\\end{pmatrix}\n}\n$$", "id": "3412214"}, {"introduction": "理论上可解的问题在离散化后，其数值解可能对微小扰动极其敏感，从而导致实际上的不适定性。希尔伯特矩阵是极端病态的经典范例，这个问题要求你通过编程来构建它，并观察其条件数 $ \\kappa_2 $ 随矩阵维度增加而爆炸性增长的现象。通过这个实践，你将直观地理解离散化和有限精度计算如何成为不适定性的实际来源 [@problem_id:3412219]。", "problem": "您的任务是构造一系列希尔伯特矩阵，并通过其2-范数条件数的增长行为来展示病态性的一个实际来源。阶数为 $n$ 的希尔伯特矩阵，记作 $H_n$，其定义规则为 $$(H_n)_{ij} = \\frac{1}{i + j - 1}$$，适用于所有索引 $i,j \\in \\{1,2,\\dots,n\\}$。一个非奇异矩阵 $A$ 的2-范数条件数，记作 $\\kappa_2(A)$，定义为 $$\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2 = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)},$$ 其中 $\\sigma_{\\max}(A)$ 和 $\\sigma_{\\min}(A)$ 分别是 $A$ 的最大和最小奇异值。奇异值被定义为 $A^\\top A$ 的特征值的平方根。针对对称正定矩阵的柯西交错定理揭示了嵌套主子矩阵奇异值的单调性，这将有助于推断 $\\kappa_2(H_n)$ 如何随 $n$ 演变。\n\n您的程序必须实现以下功能：\n\n- 对于集合 $S = \\{2,3,4,8,12\\}$ 中的每个 $n$，使用上述定义和电气和电子工程师协会 (IEEE) 754 标准指定的标准双精度格式的浮点运算来构造 $H_n$。\n- 对于每个 $n \\in S$，使用定义 $\\kappa_2(H_n) = \\sigma_{\\max}(H_n) / \\sigma_{\\min}(H_n)$ 计算 $\\kappa_2(H_n)$，其中奇异值通过对 $H_n$ 进行奇异值分解获得。\n- 对于有序集合 $S$ 中的每对连续元素 $(n_i,n_{i+1})$，通过评估布尔谓词 $$\\frac{\\kappa_2(H_{n_{i+1}})}{\\kappa_2(H_{n_i})} > \\frac{n_{i+1}}{n_i}$$ 来展示超线性增长。返回一个布尔值列表，其顺序与配对 $[(2,3),(3,4),(4,8),(8,12)]$ 的顺序相同。\n- 通过检查最小奇异值 $\\sigma_{\\min}(H_{12})$ 是否低于双精度机器ε $\\epsilon_{\\mathrm{mach}}$（即，测试 $\\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}}$）来说明有限精度病态性。使用运行时环境为64位浮点数提供的标准机器ε值。\n\n在您的推理和实现中需要明确使用的基本概念包括：希尔伯特矩阵 $H_n$ 的定义、2-范数条件数 $\\kappa_2(\\cdot)$、奇异值分解，以及IEEE 754双精度算术中机器ε的概念。\n\n测试套件：\n- 计算并报告 $n \\in \\{2,3,4,8,12\\}$ 的 $\\kappa_2(H_n)$。\n- 对于 $(n_i,n_{i+1}) \\in \\{(2,3),(3,4),(4,8),(8,12)\\}$，计算并报告 $$\\frac{\\kappa_2(H_{n_{i+1}})}{\\kappa_2(H_{n_i})} > \\frac{n_{i+1}}{n_i}$$ 是否成立。\n- 计算并报告 $\\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}}$ 是否成立。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。第一个元素必须是浮点值列表 $[\\kappa_2(H_2),\\kappa_2(H_3),\\kappa_2(H_4),\\kappa_2(H_8),\\kappa_2(H_{12})]$。第二个元素必须是与上述指定的连续配对的超线性谓词相对应的布尔值列表。第三个元素必须是与谓词 $\\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}}$ 相对应的单个布尔值。确切地说，输出必须如下所示：$$[\\,[\\kappa_2(H_2),\\kappa_2(H_3),\\kappa_2(H_4),\\kappa_2(H_8),\\kappa_2(H_{12})],\\,[b_1,b_2,b_3,b_4],\\,b_5\\,],$$ 其中每个 $b_i$ 是按描述计算的布尔值。此问题不涉及任何单位或角度，也不应打印任何百分比。", "solution": "该问题要求研究希尔伯特矩阵的数值病态性，希尔伯特矩阵是数值线性代数中用于展示求解线性系统挑战的典型例子。研究过程包括：为一系列递增的矩阵阶数 $n$ 计算2-范数条件数 $\\kappa_2(H_n)$，并分析其增长率和在有限精度算术中的行为。\n\n问题的核心组成部分是：\n1.  阶数为 $n$ 的希尔伯特矩阵 $H_n$，其元素为 $(H_n)_{ij} = \\frac{1}{i + j - 1}$，其中 $i,j \\in \\{1, 2, \\dots, n\\}$。\n2.  2-范数条件数 $\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}$，其中 $\\sigma_{\\max}$ 和 $\\sigma_{\\min}$ 是矩阵 $A$ 的最大和最小奇异值。\n3.  机器ε的概念，$\\epsilon_{\\mathrm{mach}}$，对于IEEE 754双精度，它是满足 $1.0 + \\epsilon_{\\mathrm{mach}} \\neq 1.0$ 的最小数。\n\n该方法涉及多个计算和分析步骤，针对集合 $S = \\{2, 3, 4, 8, 12\\}$ 中的矩阵阶数 $n$ 执行。\n\n**步骤 1：希尔伯特矩阵的构造**\n对于每个阶数 $n \\in S$，构造一个 $n \\times n$ 的希尔伯特矩阵 $H_n$。位于第 $i$ 行和第 $j$ 列的元素（如定义中使用基于1的索引）由公式 $(H_n)_{ij} = \\frac{1}{i+j-1}$ 给出。对于使用基于0的索引的实现，这变为 $(H_n)_{ij} = \\frac{1}{(i+1)+(j+1)-1} = \\frac{1}{i+j+1}$。所有计算均使用标准双精度浮点算术执行。\n\n**步骤 2：通过奇异值分解（SVD）计算条件数**\n每个矩阵 $H_n$ 的奇异值都通过奇异值分解（SVD）计算。由于希尔伯特矩阵是对称正定的，其奇异值与其特征值相同。SVD提供了一个奇异值向量，从中可以提取最大值 $\\sigma_{\\max}(H_n)$ 和最小值 $\\sigma_{\\min}(H_n)$。然后，2-范数条件数计算为它们的比值：\n$$ \\kappa_2(H_n) = \\frac{\\sigma_{\\max}(H_n)}{\\sigma_{\\min}(H_n)} $$\n对每个 $n \\in S$ 重复此过程。预计 $\\kappa_2(H_n)$ 会快速增长。问题提到了柯西交错定理，它为此增长提供了理论支持。由于 $H_n$ 是 $H_{n+1}$ 的一个前导主子矩阵，该定理意味着 $H_n$ 和 $H_{n+1}$ 的奇异值是交错的。具体来说，$\\sigma_{\\max}(H_{n+1}) \\ge \\sigma_{\\max}(H_n)$ 且 $\\sigma_{\\min}(H_{n+1}) \\le \\sigma_{\\min}(H_n)$。增大的（或非减小的）最大奇异值和减小的（或非增大的）最小奇异值的组合导致了条件数的单调递增。\n\n**步骤 3：超线性增长分析**\n问题要求证明其增长率快于矩阵维数的增长率。这通过使用以下谓词进行测试：\n$$ \\frac{\\kappa_2(H_{n_{i+1}})}{\\kappa_2(H_{n_i})} > \\frac{n_{i+1}}{n_i} $$\n对于排序集合 $S$ 中每对连续的阶数 $(n_i, n_{i+1})$，都会对此不等式进行评估。这些配对是 $(2,3)$、$(3,4)$、$(4,8)$ 和 $(8,12)$。每次检查都会确定一个布尔结果（`True` 或 `False`）。\n\n**步骤 4：有限精度效应和数值奇异性**\n最后一步展示了极端病态的一个实际后果。如果涉及一个矩阵的计算被浮点误差所主导，那么该矩阵被认为是数值奇异的。这通常发生在它的条件数与 $1/\\epsilon_{\\mathrm{mach}}$ 的数量级相当时。诊断这种情况的一个直接方法是检查其最小奇异值的大小。问题要求评估以下谓词：\n$$ \\sigma_{\\min}(H_{12})  \\epsilon_{\\mathrm{mach}} $$\n这里，$\\sigma_{\\min}(H_{12})$是12阶希尔伯特矩阵的最小奇异值，而 $\\epsilon_{\\mathrm{mach}}$ 是双精度算术的机器ε，其值为 $2^{-52}$，约等于 $2.2204 \\times 10^{-16}$。如果这个谓词为真，意味着最小奇异值已被舍入到一个非常小的值，低于该数字格式的相对精度阈值。从计算的角度来看，这实际上使矩阵变为奇异，因为对其求逆会将噪声放大到一个非常大的因子（$ \\kappa_2 > 10^{16} $），使得结果毫无意义。\n\n实现将把计算出的条件数列表、超线性测试的布尔结果列表以及机器ε测试的单个布尔结果整合到一个最终的嵌套列表结构中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs Hilbert matrices, computes their condition numbers, and analyzes\n    their growth and finite-precision properties as specified in the problem.\n    \"\"\"\n    \n    # Define the set of matrix orders to be evaluated.\n    orders = [2, 3, 4, 8, 12]\n    \n    # Get the machine epsilon for double-precision floating-point numbers.\n    machine_epsilon = np.finfo(float).eps\n\n    cond_numbers = {}\n    singular_values_map = {}\n\n    # Step 1  2: Construct Hilbert matrix and compute condition number for each order.\n    for n in orders:\n        # Construct the Hilbert matrix H_n.\n        # Using 0-based indexing for implementation: (H_n)_ij = 1 / (i+j+1)\n        # where i,j are in {0, 1, ..., n-1}.\n        i = np.arange(1, n + 1).reshape(n, 1)\n        j = np.arange(1, n + 1).reshape(1, n)\n        H = 1.0 / (i + j - 1)\n\n        # Compute singular values using SVD. compute_uv=False is more efficient.\n        sv = np.linalg.svd(H, compute_uv=False)\n        singular_values_map[n] = sv\n        \n        # Calculate the 2-norm condition number.\n        sigma_max = sv[0]\n        sigma_min = sv[-1]\n        cond_numbers[n] = sigma_max / sigma_min\n\n    # Store the list of condition numbers in the required order.\n    k_values = [cond_numbers[n] for n in orders]\n\n    # Step 3: Analyze superlinear growth.\n    superlinearity_checks = []\n    for i in range(len(orders) - 1):\n        n_i = orders[i]\n        n_i_plus_1 = orders[i+1]\n        \n        # Ratio of condition numbers\n        cond_ratio = cond_numbers[n_i_plus_1] / cond_numbers[n_i]\n        \n        # Ratio of dimensions\n        dim_ratio = n_i_plus_1 / n_i\n        \n        # Evaluate the predicate and store the boolean result.\n        superlinearity_checks.append(cond_ratio > dim_ratio)\n\n    # Step 4: Check for finite-precision ill-posedness.\n    # Retrieve the smallest singular value for H_12.\n    sigma_min_h12 = singular_values_map[12][-1]\n    \n    # Evaluate the predicate against machine epsilon.\n    epsilon_check = sigma_min_h12  machine_epsilon\n    \n    # Assemble the final result in the specified format.\n    # To match the no-space-after-comma format, we build the string manually.\n    s1 = '[' + ','.join(map(str, k_values)) + ']'\n    s2 = '[' + ','.join(map(str, superlinearity_checks)) + ']'\n    s3 = str(epsilon_check)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{s1},{s2},{s3}]\")\n\nsolve()\n```", "id": "3412219"}, {"introduction": "诊断不适定性是第一步，而解决它才是最终目标。正则化是应对不适定性的核心技术，但不同的正则化方法在引入先验知识以稳定解的同时，也会带来各自特有的副作用。本练习将带你从诊断走向求解，通过一个常见的一维信号去模糊问题，比较Tikhonov正则化和全变分(Total Variation)正则化的效果，让你亲手实践并量化分析它们各自产生的过度平滑与阶梯效应等典型伪影 [@problem_id:3412170]。", "problem": "考虑一个一维离散逆问题，其中一个未知的的分段常数信号 $x_{\\mathrm{true}} \\in \\mathbb{R}^N$ 通过循环卷积模糊和加性噪声进行观测。正向模型为 $y = K x_{\\mathrm{true}} + \\varepsilon$，其中 $K$ 是与一个非负、和为1的离散高斯核 $h \\in \\mathbb{R}^N$ 进行循环卷积的线性算子，而 $\\varepsilon$ 是零均值、已知方差的独立同分布高斯噪声。当 $N$ 很大时，模糊算子 $K$ 在 $\\mathbb{R}^N$ 上是紧的，并且其傅里叶幅值随着频率的升高而衰减。这种衰减导致了不适定反演，因为 $x_{\\mathrm{true}}$ 的高频分量被强烈衰减，而在尝试反卷积时噪声被放大。你将比较两种正则化策略，并量化称为阶梯效应和过度平滑的伪影。\n\n未知信号 $x_{\\mathrm{true}}$ 是具有三个变化点的分段常数。固定 $N = 256$ 并通过以下公式定义 $x_{\\mathrm{true}}[i]$（对于 $i = 0,1,\\dots,N-1$）：\n$$\nx_{\\mathrm{true}}[i] =\n\\begin{cases}\n0.0,   0 \\le i \\le 63, \\\\\n1.0,   64 \\le i \\le 127, \\\\\n0.5,   128 \\le i \\le 191, \\\\\n1.5,   192 \\le i \\le 255,\n\\end{cases}\n$$\n真实变化点索引集为 $C = \\{63,127,191\\}$。正向模糊 $K$ 由与一个离散高斯核 $h$ 的循环卷积定义，其形式为 $h[j] \\propto \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{j'}{\\sigma}\\right)^2\\right)$（对于 $j = 0,1,\\dots,N-1$），其中 $j'$ 是中心化索引 $j' = \\min(j, N-j)$，$\\sigma$ 是以样本为单位的标准差参数，并进行归一化以使 $\\sum_{j=0}^{N-1} h[j] = 1$。\n\n你将通过求解以下两个优化问题来从 $y$ 重建 $x$：\n\n1. 带单位罚项的 Tikhonov 正则化：\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\frac{\\lambda_{\\mathrm{T}}}{2}\\|x\\|_2^2,\n$$\n其中 $\\lambda_{\\mathrm{T}} > 0$。\n\n2. 带离散梯度的全变分正则化：\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\lambda_{\\mathrm{TV}} \\|D x\\|_1,\n$$\n其中 $\\lambda_{\\mathrm{TV}} > 0$，且 $D: \\mathbb{R}^N \\to \\mathbb{R}^N$ 是循环前向差分算子 $(D x)[i] = x[(i+1) \\bmod N] - x[i]$。你可以使用任何收敛且正确的方法来求解全变分问题；例如，使用诸如交替方向乘子法 (ADMM) 的分裂法，并进行变量分裂 $z = D x$。\n\n为任意重建信号 $\\hat{x} \\in \\mathbb{R}^N$ 定义以下定量伪影度量：\n\n- 均方误差：\n$$\n\\mathrm{MSE}(\\hat{x}) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\hat{x}[i] - x_{\\mathrm{true}}[i] \\right)^2.\n$$\n\n- 相对于真实变化点索引集 $C$ 的假跳变计数，阈值为 $\\tau$：\n$$\nJ_{\\mathrm{false}}(\\hat{x}; \\tau) = \\#\\left\\{ i \\in \\{0,\\dots,N-2\\} \\setminus C \\,:\\, \\left| \\hat{x}[i+1] - \\hat{x}[i] \\right| \\ge \\tau \\right\\}.\n$$\n\n- 平均边缘衰减亏损（过度平滑伪影），在真实变化点索引处定义：\n$$\n\\mathrm{AD}(\\hat{x}) = \\frac{1}{|C|} \\sum_{t \\in C} \\max\\!\\left( 0, 1 - \\frac{ \\left| \\hat{x}[t+1] - \\hat{x}[t] \\right| }{ \\left| x_{\\mathrm{true}}[t+1] - x_{\\mathrm{true}}[t] \\right| } \\right) \\right).\n$$\n该量的值在 $[0,1]$ 范围内，当边缘被衰减得更严重时，其值更大。\n\n基于真实跳变设置跳变检测阈值 $\\tau$ 为 $\\tau = \\alpha \\cdot A_{\\mathrm{mean}}$，其中\n$$\nA_{\\mathrm{mean}} = \\frac{1}{|C|} \\sum_{t \\in C} \\left| x_{\\mathrm{true}}[t+1] - x_{\\mathrm{true}}[t] \\right|,\n$$\n并固定 $\\alpha = 0.25$。\n\n实现一个程序，对于下面套件中的每个测试用例，构建 $h$，模拟数据 $y$，计算重建结果 $\\hat{x}_{\\mathrm{T}}$ 和 $\\hat{x}_{\\mathrm{TV}}$，并为每种方法评估指标 $\\mathrm{MSE}$、$J_{\\mathrm{false}}$ 和 $\\mathrm{AD}$。\n\n测试套件参数集是元组 $(\\sigma, \\sigma_{\\varepsilon}, \\lambda_{\\mathrm{T}}, \\lambda_{\\mathrm{TV}}, \\rho, I, \\mathrm{seed})$，其中 $\\sigma$ 是以样本为单位的模糊标准差，$\\sigma_{\\varepsilon}$ 是噪声标准差，$\\lambda_{\\mathrm{T}}$ 和 $\\lambda_{\\mathrm{TV}}$ 是正则化强度，$\\rho$ 是交替方向乘子法 (ADMM) 的增广拉格朗日惩罚参数，$I$ 是 ADMM 迭代次数，$\\mathrm{seed}$ 是用于可复现性的伪随机数生成器种子。使用以下测试套件：\n\n- 用例 A (中等模糊和噪声): $(2.0, 0.01, 0.01, 0.10, 1.0, 200, 0)$。\n- 用例 B (更高噪声): $(2.0, 0.05, 0.05, 0.20, 1.0, 300, 1)$。\n- 用例 C (强模糊): $(5.0, 0.01, 0.02, 0.15, 1.5, 300, 2)$。\n- 用例 D (低噪声但强正则化): $(2.0, 0.005, 0.50, 0.80, 1.0, 300, 3)$。\n\n你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个测试用例贡献一个包含六个数字的列表，顺序为 $[\\mathrm{MSE}(\\hat{x}_{\\mathrm{T}}), \\mathrm{MSE}(\\hat{x}_{\\mathrm{TV}}), J_{\\mathrm{false}}(\\hat{x}_{\\mathrm{T}};\\tau), J_{\\mathrm{false}}(\\hat{x}_{\\mathrm{TV}};\\tau), \\mathrm{AD}(\\hat{x}_{\\mathrm{T}}), \\mathrm{AD}(\\hat{x}_{\\mathrm{TV}})]$。例如，最终输出格式必须为\n$$\n\\left[ [r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}, r_{1,5}, r_{1,6}], [r_{2,1}, \\dots, r_{2,6}], [r_{3,1}, \\dots, r_{3,6}], [r_{4,1}, \\dots, r_{4,6}] \\right],\n$$\n其中每个 $r_{k,\\ell}$ 是一个实数或整数。不应打印任何其他文本。", "solution": "所述问题是逆问题和计算成像领域中一个良定的数值练习。它在科学上是合理的、自洽的且客观的。所有用于模拟数据、执行重建和评估结果的必要参数、模型和定义均已提供。该问题要求在一个反卷积问题上比较两种标准的正则化技术——Tikhonov 正则化和全变分正则化，这是该领域的典型任务。指定的伪影度量是标准的，并且与这些正则化方法的已知行为直接相关。因此，该问题被认为是有效的。将遵循指定的方法论构建一个解决方案。\n\n该解决方案的实现首先是建立一维反卷积问题。这包括定义真实的分段常数信号 $x_{\\mathrm{true}}$、高斯模糊核 $h$、循环卷积算子 $K$ 以及模拟带噪测量数据 $y$。随后，应用两种不同的正则化反演算法从 $y$ 中重建信号。最后，计算定量指标以评估每次重建的质量，并突出显示与每种正则化方法相关的特征伪影。\n\n首先，生成真实信号 $x_{\\mathrm{true}} \\in \\mathbb{R}^N$（其中 $N=256$），它是一个具有四个不同水平和三个变化点（位于索引 $C = \\{63, 127, 191\\}$）的分段常数向量。模糊算子 $K$ 对应于循环卷积。卷积核 $h$ 是一个离散高斯核，其构造是对称的，并被归一化以使其和为一。卷积运算 $Kx$ 使用快速傅里叶变换 (FFT) 在傅里叶域中高效计算，在傅里叶域中，该运算变为逐元素乘积：$(Kx)_{\\mathrm{FFT}} = \\mathrm{FFT}(h) \\odot \\mathrm{FFT}(x)$。然后通过计算 $y = Kx_{\\mathrm{true}} + \\varepsilon$ 来模拟观测数据 $y$，其中 $\\varepsilon$ 是均值为0、标准差为指定值 $\\sigma_{\\varepsilon}$ 的独立同分布高斯噪声样本向量。为保证可复现性，随机种子是固定的。\n\n第一种重建方法是 Tikhonov 正则化。目标是找到求解以下问题的 $\\hat{x}_{\\mathrm{T}}$：\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\frac{\\lambda_{\\mathrm{T}}}{2}\\|x\\|_2^2\n$$\n这是一个二次优化问题，其闭式解可以从正规方程 $(K^T K + \\lambda_{\\mathrm{T}} I) x = K^T y$ 导出。由于 $K$ 是表示循环卷积的循环矩阵，它可以被 FFT 对角化。算子 $K^T K + \\lambda_{\\mathrm{T}} I$ 也是循环的，因此在傅里叶域中很容易求逆。令 $H = \\mathrm{FFT}(h)$，$Y = \\mathrm{FFT}(y)$，以及 $\\hat{X}_{\\mathrm{T}} = \\mathrm{FFT}(\\hat{x}_{\\mathrm{T}})$。傅里叶域中的解由下式给出：\n$$\n\\hat{X}_{\\mathrm{T}} = \\frac{\\overline{H} \\odot Y}{|H|^2 + \\lambda_{\\mathrm{T}}}\n$$\n其中 $\\overline{H}$ 是 $H$ 的复共轭， $|H|^2$ 是逐元素的平方幅值，除法是逐元素的。然后通过应用逆 FFT 获得重建信号 $\\hat{x}_{\\mathrm{T}}$。该方法以产生平滑的重建结果而闻名，这可能导致锐利边缘的过度平滑。\n\n第二种重建方法是全变分 (TV) 正则化。目标是找到求解以下问题的 $\\hat{x}_{\\mathrm{TV}}$：\n$$\n\\min_{x \\in \\mathbb{R}^N} \\frac{1}{2}\\|K x - y\\|_2^2 + \\lambda_{\\mathrm{TV}} \\|D x\\|_1\n$$\n其中 $D$ 是循环前向差分算子，$(Dx)[i] = x[(i+1) \\bmod N] - x[i]$，$\\|\\cdot\\|_1$ 是 $\\ell_1$-范数。不可微的 $\\ell_1$-范数项促进 $x$ 梯度的稀疏性，这有利于分段常数解并能保持锐利边缘。这个问题使用交替方向乘子法 (ADMM) 并通过变量分裂 $z=Dx$ 来求解。增广拉格朗日量为：\n$$\nL_{\\rho}(x, z, u) = \\frac{1}{2}\\|K x - y\\|_2^2 + \\lambda_{\\mathrm{TV}} \\|z\\|_1 + \\frac{\\rho}{2}\\|Dx - z + u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2\n$$\n其中 $u$ 是缩放的对偶变量，$\\rho$ 是惩罚参数。ADMM 算法迭代更新 $x$、$z$ 和 $u$：\n1.  **$x$-更新**: $\\min_x \\left( \\frac{1}{2}\\|K x - y\\|_2^2 + \\frac{\\rho}{2}\\|Dx - z^k + u^k\\|_2^2 \\right)$。这是一个二次问题，其解可以类似于 Tikhonov 情况，在傅里叶域中高效地找到。算子 $K$ 和 $D$ 都是循环的，因此 $\\hat{X}^{k+1} = \\mathrm{FFT}(x^{k+1})$ 的解是：\n    $$\n    \\hat{X}^{k+1} = \\frac{\\overline{H} \\odot Y + \\rho \\overline{\\Delta} \\odot (Z^k - U^k)}{|H|^2 + \\rho |\\Delta|^2}\n    $$\n    其中 $\\Delta = \\mathrm{FFT}(d)$，$d$ 是算子 $D$ 的核。\n2.  **$z$-更新**: $\\min_z \\left( \\lambda_{\\mathrm{TV}}\\|z\\|_1 + \\frac{\\rho}{2}\\|Dx^{k+1} - z + u^k\\|_2^2 \\right)$。这有一个由逐元素的软阈值算子给出的闭式解：\n    $$\n    z^{k+1} = \\mathcal{S}_{\\lambda_{\\mathrm{TV}}/\\rho}(Dx^{k+1} + u^k)\n    $$\n    其中 $\\mathcal{S}_{\\tau}(v)_i = \\mathrm{sign}(v_i) \\max(|v_i| - \\tau, 0)$。\n3.  **$u$-更新**: $u^{k+1} = u^k + Dx^{k+1} - z^{k+1}$。\n这些步骤重复固定的迭代次数 $I$。TV 正则化以保持边缘而闻名，但可能在平滑变化的区域引入称为阶梯效应的分段常数伪影。\n\n最后，对两种重建结果 $\\hat{x}_{\\mathrm{T}}$ 和 $\\hat{x}_{\\mathrm{TV}}$ 评估三个指标：\n1.  **均方误差 (MSE)**: $\\mathrm{MSE}(\\hat{x}) = \\frac{1}{N}\\|\\hat{x} - x_{\\mathrm{true}}\\|_2^2$，一个衡量重建保真度的全局度量。\n2.  **假跳变计数 ($J_{\\mathrm{false}}$)**: 该指标通过计算重建信号中在真实变化点以外位置的显著跳变数量来量化阶梯效应。如果索引 $i$ 处的跳变幅度 $|\\hat{x}[i+1] - \\hat{x}[i]|$ 超过阈值 $\\tau=0.25 \\cdot A_{\\mathrm{mean}}$，则认为该跳变是显著的，其中 $A_{\\mathrm{mean}}$ 是 $x_{\\mathrm{true}}$ 中真实跳变的平均幅度。\n3.  **平均边缘衰减亏损 (AD)**: 该指标通过测量真实边缘在重建中被衰减的程度来量化过度平滑。它被计算为在真实变化点处的平均亏损 $1 - |\\Delta \\hat{x}|/|\\Delta x_{\\mathrm{true}}|$，并在 0 处截断。更高的 AD 值（接近 1）表示真实边缘的过度平滑更严重。\n\n对提供的测试套件中的每个参数集执行整个过程，并收集每个用例的六个结果指标值，将其格式化为所要求的输出字符串。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the deconvolution comparison experiment.\n    \"\"\"\n    N = 256\n    C = [63, 127, 191]\n\n    # Test suite parameter sets:\n    # (sigma, sigma_eps, lambda_T, lambda_TV, rho, I, seed)\n    test_cases = [\n        (2.0, 0.01, 0.01, 0.10, 1.0, 200, 0),    # Case A\n        (2.0, 0.05, 0.05, 0.20, 1.0, 300, 1),    # Case B\n        (5.0, 0.01, 0.02, 0.15, 1.5, 300, 2),    # Case C\n        (2.0, 0.005, 0.50, 0.80, 1.0, 300, 3)     # Case D\n    ]\n\n    # --- Helper Functions ---\n\n    def create_true_signal(n_samples):\n        x_true = np.zeros(n_samples)\n        x_true[0:64] = 0.0\n        x_true[64:128] = 1.0\n        x_true[128:192] = 0.5\n        x_true[192:256] = 1.5\n        return x_true\n\n    def create_gaussian_kernel(sigma, n_samples):\n        j = np.arange(n_samples)\n        j_centered = np.minimum(j, n_samples - j)\n        h = np.exp(-0.5 * (j_centered / sigma)**2)\n        return h / np.sum(h)\n\n    def solve_tikhonov(y, h, lambda_t):\n        H = np.fft.fft(h)\n        Y = np.fft.fft(y)\n        X_hat_fft = (np.conj(H) * Y) / (np.abs(H)**2 + lambda_t)\n        return np.real(np.fft.ifft(X_hat_fft))\n\n    def soft_threshold(v, kappa):\n        return np.sign(v) * np.maximum(np.abs(v) - kappa, 0)\n\n    def solve_tv_admm(y, h, lambda_tv, rho, iterations):\n        n = len(y)\n        # Initialize variables\n        x = np.zeros(n)\n        z = np.zeros(n)\n        u = np.zeros(n)\n\n        # Pre-compute FFTs\n        H = np.fft.fft(h)\n        Y = np.fft.fft(y)\n        \n        # FFT of forward difference operator D\n        d_kernel = np.zeros(n)\n        d_kernel[0] = -1\n        d_kernel[1] = 1\n        Delta = np.fft.fft(d_kernel)\n\n        H_conj = np.conj(H)\n        Delta_conj = np.conj(Delta)\n        H_sq_abs = np.abs(H)**2\n        Delta_sq_abs = np.abs(Delta)**2\n\n        # Pre-compute denominator for x-update\n        x_update_denom = H_sq_abs + rho * Delta_sq_abs\n        \n        # ADMM loop\n        for _ in range(iterations):\n            # x-update\n            Z = np.fft.fft(z)\n            U = np.fft.fft(u)\n            x_update_num = H_conj * Y + rho * Delta_conj * (Z - U)\n            X = x_update_num / x_update_denom\n            x = np.real(np.fft.ifft(X))\n            \n            # z-update\n            Dx = np.roll(x, -1) - x\n            v = Dx + u\n            z = soft_threshold(v, lambda_tv / rho)\n            \n            # u-update\n            u = u + Dx - z\n\n        return x\n\n    def calculate_metrics(x_hat, x_true, change_indices, tau):\n        n_samples = len(x_true)\n        # 1. MSE\n        mse = np.mean((x_hat - x_true)**2)\n\n        # 2. False Jump Count\n        diff_x_hat = np.abs(x_hat[1:] - x_hat[:-1])\n        all_indices = set(range(n_samples - 1))\n        true_jump_indices = set(change_indices)\n        false_jump_check_indices = sorted(list(all_indices - true_jump_indices))\n        \n        j_false = 0\n        for i in false_jump_check_indices:\n            if diff_x_hat[i] >= tau:\n                j_false += 1\n\n        # 3. Attenuation Deficit\n        ad_sum = 0\n        for t in change_indices:\n            true_jump_mag = np.abs(x_true[t + 1] - x_true[t])\n            if true_jump_mag > 1e-9: # Avoid division by zero\n                recon_jump_mag = np.abs(x_hat[t + 1] - x_hat[t])\n                ratio = recon_jump_mag / true_jump_mag\n                ad_sum += np.maximum(0, 1 - ratio)\n        ad = ad_sum / len(change_indices)\n        \n        return mse, j_false, ad\n\n    # --- Main Loop ---\n    \n    x_true = create_true_signal(N)\n\n    # Calculate jump detection threshold tau\n    true_jump_mags = np.abs([x_true[t+1] - x_true[t] for t in C])\n    A_mean = np.mean(true_jump_mags)\n    alpha = 0.25\n    tau = alpha * A_mean\n\n    all_results = []\n    \n    for params in test_cases:\n        sigma, sigma_eps, lambda_T, lambda_TV, rho, I, seed = params\n        \n        np.random.seed(seed)\n        \n        # 1. Setup problem\n        h = create_gaussian_kernel(sigma, N)\n        Kx_true = np.real(np.fft.ifft(np.fft.fft(h) * np.fft.fft(x_true)))\n        epsilon = np.random.normal(0, sigma_eps, N)\n        y = Kx_true + epsilon\n        \n        # 2. Solve for reconstructions\n        x_hat_T = solve_tikhonov(y, h, lambda_T)\n        x_hat_TV = solve_tv_admm(y, h, lambda_TV, rho, I)\n        \n        # 3. Calculate metrics\n        mse_T, j_false_T, ad_T = calculate_metrics(x_hat_T, x_true, C, tau)\n        mse_TV, j_false_TV, ad_TV = calculate_metrics(x_hat_TV, x_true, C, tau)\n        \n        case_results = [mse_T, mse_TV, j_false_T, j_false_TV, ad_T, ad_TV]\n        all_results.append(case_results)\n        \n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3412170"}]}