{"hands_on_practices": [{"introduction": "为了有效管理粒子滤波器，我们首先需要一种量化权重退化问题的方法。有效粒子数（ESS）是用于此诊断的标准度量。本练习将引导你进行一次解析推导，让你清晰地看到当权重集中于单个粒子时，ESS是如何衰减的，从而对这一关键概念建立起清晰的数学理解。[@problem_id:3417298]", "problem": "考虑一个用于反问题数据同化场景的序贯蒙特卡罗（SMC）粒子滤波器，其中包含 $N$ 个粒子。在同化单个观测值后，假设（非负）重要性权重为 $\\{w_{i}\\}_{i=1}^{N}$，并且权重向量呈现单个主导权重，其中一个分量 $w_{1}$ 大于其他 $N-1$ 个分量。假设 $N-1$ 个非主导权重相等。定义归一化权重为 $\\tilde{w}_{i} = w_{i} / \\sum_{j=1}^{N} w_{j}$，定义主导水平为 $\\rho = \\tilde{w}_{1}$，并考虑将有效样本量（ESS）$N_{\\mathrm{eff}}$ 作为序贯蒙特卡罗中标准的退化诊断指标。\n\n从粒子滤波中基于归一化权重的退化标准定义出发，在 $N-1$ 个非主导归一化权重相等的假设下，推导 $N_{\\mathrm{eff}}$ 作为粒子数 $N$ 和主导水平 $\\rho$ 的函数的闭式解析表达式。然后，通过刻画当 $\\rho \\in [0,1]$ 区间内变化时 $N_{\\mathrm{eff}}$ 的单调性和极值，来分析 $N_{\\mathrm{eff}}$ 相对于 $\\rho$ 的敏感性，并解释其对权重退化和维度灾难的影响。\n\n将 $N_{\\mathrm{eff}}$ 的最终答案表示为关于 $N$ 和 $\\rho$ 的单个闭式解析表达式。无需进行数值计算或四舍五入。", "solution": "首先验证问题陈述，以确保其科学上合理、良定且客观。\n\n### 步骤 1：提取已知条件\n- 考虑一个序贯蒙特卡罗（SMC）粒子滤波器。\n- 粒子数量为 $N$。\n- 非负重要性权重集为 $\\{w_{i}\\}_{i=1}^{N}$。\n- 权重向量有一个单一的主导权重 $w_{1}$，其值大于其他 $N-1$ 个分量。\n- $N-1$ 个非主导权重彼此相等。\n- 归一化权重定义为 $\\tilde{w}_{i} = w_{i} / \\sum_{j=1}^{N} w_{j}$。\n- 主导水平定义为 $\\rho = \\tilde{w}_{1}$。\n- 有效样本量 $N_{\\mathrm{eff}}$ 被用作退化诊断指标。\n- 任务是使用退化的标准定义，推导 $N_{\\mathrm{eff}}$ 作为 $N$ 和 $\\rho$ 的函数的闭式表达式。\n- 任务包括通过刻画当 $\\rho \\in [0,1]$ 时 $N_{\\mathrm{eff}}$ 的单调性和极值，来分析其对 $\\rho$ 的敏感性。\n- 任务要求在权重退化和维度灾难的背景下解释结果。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学或事实合理性**：该问题在科学上是合理的。它基于数据同化和统计信号处理领域的标准、基本概念，特别是序贯蒙特卡罗方法（粒子滤波器）。归一化权重、有效样本量、权重退化和维度灾难的定义在该领域都是标准的。$N_{\\mathrm{eff}}$ 的标准定义是 $N_{\\mathrm{eff}} = (\\sum_{i=1}^{N} \\tilde{w}_{i}^{2})^{-1}$。\n2.  **不可形式化或不相关**：该问题高度相关且可直接形式化。它在指定主题内提出了一个精确的数学问题。\n3.  **不完整或矛盾的设定**：设定是完整的且内部一致。所提供的定义和假设足以推导出所要求的表达式。$w_1$ 是最大权重的假设意味着对于 $j \\neq 1$ 有 $\\tilde{w}_1 > \\tilde{w}_j$。这意味着 $\\rho > (1-\\rho)/(N-1)$，简化后为 $\\rho > 1/N$。要求在 $\\rho \\in [0,1]$ 上分析该函数是一个标准的数学练习，旨在理解函数的行为，即使子区间 $\\rho \\in [0, 1/N]$ 与“主导”假设相矛盾。这并不会使问题无效，但需要在分析中注意这一点。核心推导过程不受影响。\n4.  **不切实际或不可行**：所描述的场景非常现实。权重坍缩到一个或少数几个粒子上是粒子滤波器实际应用中的一个常见且关键的问题，尤其是在高维系统中。\n5.  **病态或结构不良**：该问题是良定的。所给假设能够导出一个唯一的解析解。各项术语定义明确。\n6.  **伪深刻、琐碎或同义反复**：该问题并非琐碎。它需要基于该方法的第一性原理进行形式推导和解析推理。\n7.  **超出科学可验证范围**：该问题完全是数学性的，其解可通过逻辑推演和计算进行验证。\n\n### 步骤 3：结论与行动\n问题被判定为**有效**。我们可以继续进行求解。\n\n### $N_{\\mathrm{eff}}$ 表达式的推导\n\n基于归一化重要性权重 $\\{\\tilde{w}_{i}\\}_{i=1}^{N}$ 的有效样本量 $N_{\\mathrm{eff}}$ 的标准定义由下式给出：\n$$N_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\tilde{w}_{i}^{2}}$$\n归一化权重的总和必须等于 $1$：\n$$\\sum_{i=1}^{N} \\tilde{w}_{i} = 1$$\n根据问题陈述，主导水平 $\\rho$ 是第一个粒子的归一化权重：\n$$\\tilde{w}_{1} = \\rho$$\n问题还指出，其余 $N-1$ 个非主导权重相等。我们用 $\\tilde{w}_{k}$ 表示这些权重的值，其中 $k \\in \\{2, 3, \\ldots, N\\}$。我们可以用 $\\rho$ 和 $N$ 来表示 $\\tilde{w}_{k}$。\n使用归一化条件：\n$$\\tilde{w}_{1} + \\sum_{k=2}^{N} \\tilde{w}_{k} = 1$$\n代入 $\\tilde{w}_{1} = \\rho$ 并注意到其他 $N-1$ 个权重相等：\n$$\\rho + (N-1)\\tilde{w}_{k} = 1$$\n求解 $\\tilde{w}_{k}$（假设 $N > 1$，这在问题的表述中是隐含的）：\n$$\\tilde{w}_{k} = \\frac{1-\\rho}{N-1}$$\n现在，我们可以计算归一化权重的平方和 $\\sum_{i=1}^{N} \\tilde{w}_{i}^{2}$：\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\tilde{w}_{1}^{2} + \\sum_{k=2}^{N} \\tilde{w}_{k}^{2}$$\n代入 $\\tilde{w}_{1}$ 和 $\\tilde{w}_{k}$ 的表达式：\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\rho^{2} + (N-1) \\left( \\frac{1-\\rho}{N-1} \\right)^{2}$$\n简化表达式：\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\rho^{2} + (N-1) \\frac{(1-\\rho)^{2}}{(N-1)^{2}} = \\rho^{2} + \\frac{(1-\\rho)^{2}}{N-1}$$\n为了合并各项，我们找到一个公分母：\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\frac{(N-1)\\rho^{2} + (1-\\rho)^{2}}{N-1} = \\frac{(N-1)\\rho^{2} + (1 - 2\\rho + \\rho^{2})}{N-1}$$\n$$= \\frac{N\\rho^{2} - \\rho^{2} + 1 - 2\\rho + \\rho^{2}}{N-1} = \\frac{N\\rho^{2} - 2\\rho + 1}{N-1}$$\n最后，我们将此结果代回 $N_{\\mathrm{eff}}$ 的定义中：\n$$N_{\\mathrm{eff}} = \\frac{1}{\\frac{N\\rho^{2} - 2\\rho + 1}{N-1}} = \\frac{N-1}{N\\rho^{2} - 2\\rho + 1}$$\n这就是 $N_{\\mathrm{eff}}$ 作为 $N$ 和 $\\rho$ 的函数的闭式解析表达式。\n\n### 敏感性分析与解释\n\n为了分析 $N_{\\mathrm{eff}}$ 对 $\\rho$ 的敏感性，我们计算其一阶导数 $\\frac{d N_{\\mathrm{eff}}}{d \\rho}$。设分母为 $f(\\rho) = N\\rho^{2} - 2\\rho + 1$。\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = \\frac{d}{d\\rho} \\left( (N-1) (N\\rho^{2} - 2\\rho + 1)^{-1} \\right)$$\n使用链式法则：\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = (N-1) (-1) (N\\rho^{2} - 2\\rho + 1)^{-2} (2N\\rho - 2)$$\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = -2(N-1) \\frac{N\\rho - 1}{(N\\rho^{2} - 2\\rho + 1)^{2}}$$\n分母 $(N\\rho^{2} - 2\\rho + 1)^{2}$ 总是非负的。当 $N>1$ 时，其根是复数，因此它严格为正。导数的符号因此由分子的非常数项 $-(N\\rho - 1)$ 的符号决定。\n1.  如果 $N\\rho - 1 > 0 \\implies \\rho > 1/N$，则 $\\frac{d N_{\\mathrm{eff}}}{d \\rho}  0$。$N_{\\mathrm{eff}}$ 是 $\\rho$ 的单调递减函数。\n2.  如果 $N\\rho - 1  0 \\implies \\rho  1/N$，则 $\\frac{d N_{\\mathrm{eff}}}{d \\rho} > 0$。$N_{\\mathrm{eff}}$ 是 $\\rho$ 的单调递增函数。\n3.  如果 $N\\rho - 1 = 0 \\implies \\rho = 1/N$，则 $\\frac{d N_{\\mathrm{eff}}}{d \\rho} = 0$。这表示一个临界点。\n\n分析揭示了在 $\\rho = 1/N$ 处存在一个极值。由于导数在该点由正变负，因此这是一个局部最大值。让我们在区间 $[0,1]$ 的边界和这个临界点上计算 $N_{\\mathrm{eff}}$ 的值。\n\n-   **最大值**：在 $\\rho = 1/N$ 时，\n    $$N_{\\mathrm{eff}}\\left(\\rho = \\frac{1}{N}\\right) = \\frac{N-1}{N\\left(\\frac{1}{N}\\right)^{2} - 2\\left(\\frac{1}{N}\\right) + 1} = \\frac{N-1}{\\frac{1}{N} - \\frac{2}{N} + 1} = \\frac{N-1}{\\frac{N-1}{N}} = N$$\n    这种情况对应于所有权重都相等，即对所有 $i$ 都有 $\\tilde{w}_{i} = 1/N$，表示没有发生退化。有效样本量等于实际的粒子数量。\n\n-   **最小值**：在 $\\rho = 1$ 时，\n    $$N_{\\mathrm{eff}}(\\rho = 1) = \\frac{N-1}{N(1)^{2} - 2(1) + 1} = \\frac{N-1}{N-1} = 1$$\n    这种情况对应于完全退化，即一个粒子拥有全部权重（$\\tilde{w}_{1} = 1$），而所有其他粒子的权重为零。有效样本量坍缩为 1。\n\n-   在 $\\rho = 0$ 时：\n    $$N_{\\mathrm{eff}}(\\rho = 0) = \\frac{N-1}{N(0)^{2} - 2(0) + 1} = N-1$$\n    这对应于主导粒子的权重为零，而权重在其他 $N-1$ 个粒子中均匀分布。\n\n考虑到 $\\tilde{w}_1$ 是最大权重，$\\rho$ 的物理上有意义的定义域是 $[\\frac{1}{N}, 1]$。在此定义域上，$N_{\\mathrm{eff}}$ 是 $\\rho$ 的一个严格单调递减函数。\n\n**解释**：\n-   **权重退化**：推导出的表达式及其分析在数学上形式化了权重退化的概念。随着权重分布变得更加倾斜（即，当 $\\rho$ 从其最均匀的值 $1/N$ 向 $1$ 增加时），对后验分布近似做出贡献的有效粒子数量从 $N$ 急剧减少到 $1$。一个低的 $N_{\\mathrm{eff}}$ 是一个信号，表明粒子表示效果不佳，需要一个重采样步骤来通过消除低权重粒子并复制高权重粒子来减轻退化。\n-   **维度灾难**：在高维状态空间中，众所周知，似然函数 $p(\\text{observation}|\\text{state})$ 倾向于高度集中在空间的一个小体积内。在更新粒子权重时，粒子落入这个高似然区域变得极其罕见。因此，更新后，大多数粒子的权重接近于零，而恰好位于正确位置的一个或极少数粒子几乎捕获了全部权重。这种情况直接导致 $\\rho$ 值接近 $1$。我们的分析表明，当 $\\rho \\to 1$ 时，$N_{\\mathrm{eff}} \\to 1$，而这与粒子总数 $N$ 无关。这说明了粒子滤波器维度灾难的一个关键方面：仅仅增加 $N$ 并不是一个有效的解决方案，因为要充分采样高似然区域（从而避免权重坍缩）所需的粒子数量通常随状态空间的维度呈指数级增长。", "answer": "$$\n\\boxed{\\frac{N-1}{N\\rho^2 - 2\\rho + 1}}\n$$", "id": "3417298"}, {"introduction": "一旦检测到权重退化，标准的解决方案是执行重采样步骤。本练习将让你超越重采样的抽象概念，通过一个具体实例，一步步执行一种流行且高效的算法——系统重采样。通过为一组给定的权重计算选择索引，你将对其工作机制及其保留局部粒子结构的倾向获得具体的理解。[@problem_id:3417312]", "problem": "考虑一个应用于数据同化中贝叶斯反问题的序贯蒙特卡洛（SMC）粒子方法，该方法有 $N$ 个粒子和归一化的重要性权重 $\\{w_i\\}_{i=1}^N$，其总和为 $1$。粒子按标量状态坐标的升序索引，因此相邻的索引对应于相邻的状态。SMC算法采用系统重采样方案，其定义如下：从 $[0, 1/N)$ 上的均匀分布中抽取单个 $U$，并为 $k = 1, \\dots, N$ 形成阈值序列 $t_k = U + (k-1)/N$。令 $c_j = \\sum_{i=1}^j w_i$ 表示权重的累积和。位置 $k$ 处的重采样祖先索引定义为 $a_k = \\min\\{j \\in \\{1,\\dots,N\\} : c_j \\ge t_k\\}$。\n\n给定 $N = 10$，归一化权重\n$$\n(w_1, \\dots, w_{10}) = (0.24, 0.01, 0.01, 0.18, 0.18, 0.20, 0.05, 0.03, 0.05, 0.05),\n$$\n以及一个固定的 $U = 0.07$。请根据上述系统重采样定义，计算完整的选择索引向量 $(a_1, \\dots, a_{10})$。\n\n为了量化索引空间中相邻选择的相干性（这反映了系统重采样众所周知的相关结构），定义邻居相干性指数\n$$\n\\kappa = \\frac{1}{N-1} \\sum_{k=1}^{N-1} \\mathbf{1}\\left(|a_{k+1} - a_k| \\le 1\\right),\n$$\n其中 $\\mathbf{1}(\\cdot)$ 是指示函数。以 $\\kappa$ 的值作为最终答案。最终答案表示为最简分数。在您的推导中，将邻居相干性的概念与权重退化情况下的粒子多样性联系起来，并讨论其在高维反问题中对维度灾难的影响。最终答案必须是无单位的单个数字，并且必须表示为最简分数。", "solution": "首先根据所需标准对问题进行验证。问题陈述是自洽的，科学上基于序贯蒙特卡洛方法的理论，并且在算法上是适定的。所有必要的数据都已提供，包括粒子数 $N=10$、归一化权重 $\\{w_i\\}$ 以及随机抽取的特定值 $U=0.07$。所提供权重的总和确实是 $0.24 + 0.01 + 0.01 + 0.18 + 0.18 + 0.20 + 0.05 + 0.03 + 0.05 + 0.05 = 1.00$，证实了它们的归一化。值 $U=0.07$ 与要求的分布 $\\mathcal{U}[0, 1/N) = \\mathcal{U}[0, 0.1)$ 一致。系统重采样和邻居相干性指数 $\\kappa$ 的定义是精确的，并允许唯一的解。因此，该问题被认为是有效的，有必要提供完整的解答。\n\n解答过程分为三步：首先，我们计算祖先索引向量 $(a_1, \\dots, a_{10})$；其次，我们用该向量计算邻居相干性指数 $\\kappa$；最后，我们讨论这一结果的更广泛含义。\n\n步骤1：计算祖先索引向量 $(a_1, \\dots, a_{10})$。\n\n根据系统重采样的定义，我们必须首先计算权重的累积和 $c_j = \\sum_{i=1}^j w_i$，以及阈值序列 $t_k = U + (k-1)/N$。\n\n给定的权重是：\n$$ (w_1, \\dots, w_{10}) = (0.24, 0.01, 0.01, 0.18, 0.18, 0.20, 0.05, 0.03, 0.05, 0.05) $$\n累积和 $c_j$ 是：\n\\begin{align*}\nc_1 = 0.24 \\\\\nc_2 = 0.24 + 0.01 = 0.25 \\\\\nc_3 = 0.25 + 0.01 = 0.26 \\\\\nc_4 = 0.26 + 0.18 = 0.44 \\\\\nc_5 = 0.44 + 0.18 = 0.62 \\\\\nc_6 = 0.62 + 0.20 = 0.82 \\\\\nc_7 = 0.82 + 0.05 = 0.87 \\\\\nc_8 = 0.87 + 0.03 = 0.90 \\\\\nc_9 = 0.90 + 0.05 = 0.95 \\\\\nc_{10} = 0.95 + 0.05 = 1.00\n\\end{align*}\n\n粒子数为 $N=10$，随机抽取值为 $U=0.07$。对于 $k=1, \\dots, 10$，阈值序列 $t_k = 0.07 + (k-1)/10$ 是：\n\\begin{align*}\nt_1 = 0.07 + 0.0 = 0.07 \\\\\nt_2 = 0.07 + 0.1 = 0.17 \\\\\nt_3 = 0.07 + 0.2 = 0.27 \\\\\nt_4 = 0.07 + 0.3 = 0.37 \\\\\nt_5 = 0.07 + 0.4 = 0.47 \\\\\nt_6 = 0.07 + 0.5 = 0.57 \\\\\nt_7 = 0.07 + 0.6 = 0.67 \\\\\nt_8 = 0.07 + 0.7 = 0.77 \\\\\nt_9 = 0.07 + 0.8 = 0.87 \\\\\nt_{10} = 0.07 + 0.9 = 0.97\n\\end{align*}\n\n祖先索引 $a_k$ 是满足 $c_j \\ge t_k$ 的最小索引 $j$。我们通过将 $t_k$ 与累积权重序列 $c_j$ 进行比较来找到每个 $a_k$：\n\\begin{itemize}\n    \\item 对于 $t_1 = 0.07$：第一个累积权重 $c_1=0.24$ 大于 $0.07$。因此，$a_1=1$。\n    \\item 对于 $t_2 = 0.17$：第一个累积权重 $c_1=0.24$ 大于 $0.17$。因此，$a_2=1$。\n    \\item 对于 $t_3 = 0.27$：$c_3=0.26  0.27$，但 $c_4=0.44 \\ge 0.27$。因此，$a_3=4$。\n    \\item 对于 $t_4 = 0.37$：$c_3=0.26  0.37$，但 $c_4=0.44 \\ge 0.37$。因此，$a_4=4$。\n    \\item 对于 $t_5 = 0.47$：$c_4=0.44  0.47$，但 $c_5=0.62 \\ge 0.47$。因此，$a_5=5$。\n    \\item 对于 $t_6 = 0.57$：$c_4=0.44  0.57$，但 $c_5=0.62 \\ge 0.57$。因此，$a_6=5$。\n    \\item 对于 $t_7 = 0.67$：$c_5=0.62  0.67$，但 $c_6=0.82 \\ge 0.67$。因此，$a_7=6$。\n    \\item 对于 $t_8 = 0.77$：$c_5=0.62  0.77$，但 $c_6=0.82 \\ge 0.77$。因此，$a_8=6$。\n    \\item 对于 $t_9 = 0.87$：$c_6=0.82  0.87$，但 $c_7=0.87 \\ge 0.87$。因此，$a_9=7$。\n    \\item 对于 $t_{10} = 0.97$：$c_9=0.95  0.97$，但 $c_{10}=1.00 \\ge 0.97$。因此，$a_{10}=10$。\n\\end{itemize}\n完整的选择索引向量是 $(a_1, \\dots, a_{10}) = (1, 1, 4, 4, 5, 5, 6, 6, 7, 10)$。\n\n步骤2：计算邻居相干性指数 $\\kappa$。\n\n该指数定义为 $\\kappa = \\frac{1}{N-1} \\sum_{k=1}^{N-1} \\mathbf{1}\\left(|a_{k+1} - a_k| \\le 1\\right)$。当 $N=10$ 时，我们有 $N-1=9$。我们对向量 $a$ 中的每一对相邻项评估指示函数 $\\mathbf{1}(\\cdot)$：\n\\begin{itemize}\n    \\item $k=1$: $|a_2 - a_1| = |1 - 1| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$。\n    \\item $k=2$: $|a_3 - a_2| = |4 - 1| = 3 > 1 \\implies \\mathbf{1}(\\cdot) = 0$。\n    \\item $k=3$: $|a_4 - a_3| = |4 - 4| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$。\n    \\item $k=4$: $|a_5 - a_4| = |5 - 4| = 1 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$。\n    \\item $k=5$: $|a_6 - a_5| = |5 - 5| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$。\n    \\item $k=6$: $|a_7 - a_6| = |6 - 5| = 1 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$。\n    \\item $k=7$: $|a_8 - a_7| = |6 - 6| = 0 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$。\n    \\item $k=8$: $|a_9 - a_8| = |7 - 6| = 1 \\le 1 \\implies \\mathbf{1}(\\cdot) = 1$。\n    \\item $k=9$: $|a_{10} - a_9| = |10 - 7| = 3 > 1 \\implies \\mathbf{1}(\\cdot) = 0$。\n\\end{itemize}\n指示函数值的总和是 $1 + 0 + 1 + 1 + 1 + 1 + 1 + 1 + 0 = 7$。\n因此，邻居相干性指数是：\n$$ \\kappa = \\frac{7}{9} $$\n\n步骤3：讨论。\n\n计算出的 $\\kappa = 7/9$ 值很高，表明重采样序列中的大多数相邻索引对应的祖先在原始有序粒子集中是相同或紧邻的。这种高相干性是系统重采样的一个标志。与多项式重采样（其中每个新粒子都是独立抽取的）不同，系统重采样通过单个随机抽取值 $U$ 固定了整个选择模式。阈值 $t_k$ 是等距的，因此它们倾向于选择连续的祖先块，从而保留了局部结构。\n\n这直接关联到 **权重退化** 的问题。在SMC中，一个常见的问题是重要性权重集中在少数几个粒子上，而其余粒子的权重可以忽略不计。这就是 **权重退化**。重采样是标准的补救措施，旨在丢弃低权重粒子并复制高权重粒子，以便将计算精力集中在状态空间中有希望的区域。所提供的权重，$w_1=0.24$、$w_4=0.18$、$w_5=0.18$ 和 $w_6=0.20$，显示出中等程度的退化。\n\n重采样虽然是必要的，但它引入了其自身的问题：**粒子多样性** 的损失，因为重采样集合中独特祖先的数量小于 $N$。这种样本的贫化是粒子方法的一个关键限制。通过 $\\kappa$ 测量出的高相干性体现了系统重采样如何管理这种权衡。通过保留邻域结构，它避免了独立重采样可能导致的粒子位置完全随机化，如果高权重粒子在状态空间中形成一个“簇”，这可能是有益的。\n\n整个动态被 **维度灾难** 严重加剧，这是高维反问题中的一个关键问题。随着状态空间维度的增长，空间的体积呈指数级增长。SMC采样器所针对的后验分布通常集中在该体积中一个极小的部分。因此，固定数量的随机粒子 $N$ 落入这个高概率区域的可能性越来越小。这导致极端的权重退化，通常在更新步骤之后只有一个粒子具有非零权重。在这种灾难性的崩溃中，任何重采样方案（包括系统重采样）都只会选择那一个粒子 $N$ 次。得到的重采样集将是 $(a_1, \\dots, a_N) = (j, j, \\dots, j)$，对于某个索引 $j$。在这种情况下，$\\kappa$ 将为 $1$，但这将意味着多样性的完全丧失，而不是结构的健康保存。因此，系统重采样的相干性特性并非维度灾难的万灵药；它仅仅反映了其所获得的权重的结构。根本问题在于重要性采样无法有效探索高维空间，这是一个需要超越简单重采样的更高级技术的挑战。", "answer": "$$ \\boxed{\\frac{7}{9}} $$", "id": "3417312"}, {"introduction": "维度灾难常常导致重要性权重变得极小，以至于在标准浮点数运算中下溢为零，从而引发灾难性的算法失败。这个实践性的编程练习将直接应对这一关键的实现挑战。你将比较一个简单的权重计算方案和一个使用“log-sum-exp”技巧的数值稳定方法，从而证明为何这类技术对于将粒子方法应用于高维问题是不可或缺的。[@problem_id:3417322]", "problem": "考虑一个在用于逆问题和数据同化的粒子方法中的序贯重要性采样步骤，其中观测算子为单位算子，观测噪声为独立同分布的高斯噪声，其协方差为 $\\sigma^2 I_d$。现有 $N$ 个粒子 $\\{x_i\\}_{i=1}^N \\subset \\mathbb{R}^d$ 和一个观测值 $y \\in \\mathbb{R}^d$。在标准重要性加权中，粒子 $i$ 的未归一化权重与似然 $p(y \\mid x_i)$ 成正比，而归一化权重则通过除以它们的总和得到。在高维 $d$ 中，直接计算似然中的 $\\exp(\\cdot)$ 可能会遭遇浮点数下溢，导致归一化错误，从而使有效样本量（ESS）的估计和重采样决策产生偏差。\n\n从独立分量的加性高斯似然的基本原理出发，对于单位观测算子，其似然为 $p(y \\mid x) \\propto \\exp\\left(-\\tfrac{1}{2} \\lVert y - x \\rVert^2 / \\sigma^2\\right)$，请推导一种使用对数权重来稳定权重计算的方法。解释为什么对所有对数权重加上一个常数偏移量不会改变归一化后的权重，并展示如何选择最大对数权重作为偏移量来减轻下溢问题。定义并计算有效样本量（ESS）、基于阈值规则的重采样决策，并量化归一化误差。\n\n使用的定义：\n- 有效样本量 (ESS)：$\\mathrm{ESS}(w) = \\left(\\sum_{i=1}^N w_i^2\\right)^{-1}$，其中归一化权重 $w_i \\ge 0$ 且 $\\sum_{i=1}^N w_i = 1$。\n- 重采样决策规则：如果 $\\mathrm{ESS}(w)/N  \\tau$，则进行重采样，其中 $\\tau \\in (0,1]$ 是一个阈值。\n- 两个离散分布 $w$ 和 $v$ 之间的全变差 (TV) 距离：$\\mathrm{TV}(w,v) = \\tfrac{1}{2} \\sum_{i=1}^N |w_i - v_i|$。\n\n实现以下两种加权方案：\n- 朴素方案：通过 $w_i^{\\text{raw}} = \\exp\\left(-\\tfrac{1}{2} \\lVert y - x_i \\rVert^2 / \\sigma^2\\right)$ 计算原始权重，然后通过 $w_i = w_i^{\\text{raw}} / \\sum_{j=1}^N w_j^{\\text{raw}}$ 进行归一化。如果因下溢导致 $\\sum_{j=1}^N w_j^{\\text{raw}} = 0$，则将所有归一化权重设为 $0$，且 $\\mathrm{ESS} = 0$。\n- 对数权重稳定方案：计算对数权重 $\\ell_i = -\\tfrac{1}{2} \\lVert y - x_i \\rVert^2 / \\sigma^2$，然后通过 $m = \\max_i \\ell_i$ 进行平移，并对差值进行指数化以获得 $u_i = \\exp(\\ell_i - m)$，最后通过 $w_i = u_i / \\sum_{j=1}^N u_j$ 进行归一化。\n\n对于每个测试用例，报告：\n- $E_{\\text{naive}}$：朴素方案下的 ESS，\n- $E_{\\text{log}}$：对数权重稳定方案下的 ESS，\n- $R_{\\text{naive}}$：朴素方案下的布尔型重采样决策，\n- $R_{\\text{log}}$：对数权重稳定方案下的布尔型重采样决策，\n- $T$：全变差距离 $\\mathrm{TV}(w^{\\text{naive}}, w^{\\text{log}})$。如果朴素方案归一化失败（原始权重之和为零），则定义 $T = 1$。\n\n使用以下确定性参数值测试套件，粒子和观测值按规定生成。对于每个用例，除非另有说明，否则从标准正态分布中抽取粒子，并设置 $y = s \\cdot \\mathbf{1}_d$，其中 $\\mathbf{1}_d$ 是 $d$ 维全一向量，$s$ 是偏移量。按给定值为每个用例使用固定的随机种子。在尖峰情况下，将第一个粒子精确设置为等于 $y$，并将所有其他粒子设置为 $y + 5 \\cdot \\mathbf{1}_d$。\n- 用例 1（理想情况）：$N=500$，$d=5$，$\\sigma=1.0$，$\\tau=0.5$，种子 $=0$，偏移量 $s=1.0$。\n- 用例 2（高维下溢）：$N=1000$，$d=500$，$\\sigma=0.2$，$\\tau=0.5$，种子 $=1$，偏移量 $s=2.0$。\n- 用例 3（极端下溢边界）：$N=800$，$d=2000$，$\\sigma=1.0$，$\\tau=0.5$，种子 $=2$，偏移量 $s=10.0$。\n- 用例 4（近乎相等的权重）：$N=300$，$d=50$，$\\sigma=10^6$，$\\tau=0.5$，种子 $=3$，偏移量 $s=5.0$。\n- 用例 5（尖峰权重）：$N=1000$，$d=100$，$\\sigma=0.1$，$\\tau=0.5$，种子 $=4$，偏移量 $s=0.0$；第一个粒子等于 $y$，其他粒子等于 $y + 5 \\cdot \\mathbf{1}_d$。\n\n你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个测试用例的结果本身是一个列表 $[E_{\\text{naive}}, E_{\\text{log}}, R_{\\text{naive}}, R_{\\text{log}}, T]$。例如，输出必须是以下形式：\n$[[E_{\\text{naive}}^{(1)},E_{\\text{log}}^{(1)},R_{\\text{naive}}^{(1)},R_{\\text{log}}^{(1)},T^{(1)}],\\dots,[E_{\\text{naive}}^{(5)},E_{\\text{log}}^{(5)},R_{\\text{naive}}^{(5)},R_{\\text{log}}^{(5)},T^{(5)}]]$。", "solution": "该问题陈述被评估为有效。它在科学上基于贝叶斯推断和粒子滤波器数值方法的原理，问题设定良好，具有明确的目标和确定性的测试用例，并以客观、明确的语言进行阐述。问题的核心在于解决计算统计学中的一个真实且关键的问题——高维空间中的数值稳定性。\n\n任务是分析并实现两种计算粒子滤波器中重要性权重的方法，展示对数空间稳定技术相对于朴素方法的优越性，尤其是在浮点数下溢成为一个重要问题的高维设置中。\n\n加权步骤的基础是给定一个状态（粒子）$x \\in \\mathbb{R}^d$ 时观测到 $y \\in \\mathbb{R}^d$ 的似然。在单位观测算子和独立同分布（i.i.d.）高斯噪声（方差为 $\\sigma^2$）的条件下，似然函数由下式给出：\n$$\np(y \\mid x) = \\prod_{j=1}^d \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_j - x_j)^2}{2\\sigma^2}\\right)\n$$\n在重要性采样中，我们只需要权重在相差一个归一化常数的范围内是准确的。因此，粒子 $x_i$ 的未归一化权重 $w^{\\text{raw}}$ 与似然成正比：\n$$\nw_i^{\\text{raw}} \\propto p(y \\mid x_i) \\propto \\exp\\left(-\\frac{\\lVert y - x_i \\rVert^2}{2\\sigma^2}\\right)\n$$\n然后，归一化权重 $w_i$ 计算为 $w_i = w_i^{\\text{raw}} / \\sum_{k=1}^N w_k^{\\text{raw}}$。\n\n**下溢问题（维度灾难）**\n\n在高维空间（$d$ 很大）中，随机粒子 $x_i$ 与观测值 $y$ 之间的平方欧几里得距离 $\\lVert y - x_i \\rVert^2$ 往往很大。这一现象是维度灾难的一种表现。因此，指数函数的参数 $-\\frac{\\lVert y - x_i \\rVert^2}{2\\sigma^2}$ 会变成一个绝对值很大的负数。对于标准的双精度浮点数，当 $z \\lesssim -709$ 时，$\\exp(z)$ 会下溢为 $0.0$。当所有粒子都发生这种情况时，所有的原始权重 $w_i^{\\text{raw}}$ 都将变为零。权重之和变为零，使得归一化无法进行，并导致算法的灾难性失败。所有关于粒子相对质量的信息都丢失了。\n\n**对数权重稳定化**\n\n为规避这种数值不稳定性，我们可以在对数空间中进行计算。未归一化的对数权重 $\\ell_i$ 定义为未归一化权重的自然对数：\n$$\n\\ell_i = \\ln(w_i^{\\text{raw}}) = -\\frac{\\lVert y - x_i \\rVert^2}{2\\sigma^2}\n$$\n归一化权重 $w_i$ 可以用对数权重表示：\n$$\nw_i = \\frac{\\exp(\\ell_i)}{\\sum_{k=1}^N \\exp(\\ell_k)}\n$$\n这个表达式的分子和分母仍然容易发生下溢。然而，我们可以利用该表达式的一个性质来提高其稳定性。让我们给每个对数权重加上一个任意常数 $C$。新的权重 $w'_i$ 为：\n$$\nw'_i = \\frac{\\exp(\\ell_i + C)}{\\sum_{k=1}^N \\exp(\\ell_k + C)} = \\frac{\\exp(\\ell_i) \\exp(C)}{\\sum_{k=1}^N \\exp(\\ell_k)\\exp(C)} = \\frac{\\exp(C) \\exp(\\ell_i)}{\\exp(C) \\sum_{k=1}^N \\exp(\\ell_k)} = \\frac{\\exp(\\ell_i)}{\\sum_{k=1}^N \\exp(\\ell_k)} = w_i\n$$\n这表明将所有对数权重平移一个常数不会改变最终的归一化权重。为最大化数值稳定性，我们选择偏移量为 $C = -m$，其中 $m = \\max_k \\ell_k$。平移后的对数权重为 $\\ell_i' = \\ell_i - m$。此时，最大的平移后对数权重为 $\\max_k \\ell_k' = \\max_k (\\ell_k - m) = m - m = 0$。所有其他的平移后对数权重均为非正数。\n然后，稳定化的未归一化权重 $u_i$ 计算如下：\n$$\nu_i = \\exp(\\ell_i - m)\n$$\n$u_i$ 的最大值为 $\\exp(0) = 1$，这可以防止上溢。由于至少有一个 $u_i$ 等于 $1$，它们的和 $\\sum_k u_k$ 保证至少为 $1$，从而防止了分母和为零以及因下溢导致的失败。这个技术通常被称为 log-sum-exp 技巧。\n\n**评估指标**\n\n该问题要求计算几个量来比较朴素方案和稳定方案。\n\n1.  **有效样本量 (ESS)**：衡量粒子权重的退化程度。一个接近 $N$ 的值表示所有粒子具有相似的权重，而一个接近 $1$ 的值表示一个粒子的权重接近 $1$，而所有其他粒子的权重接近 $0$。其定义为：\n    $$\n    \\mathrm{ESS}(w) = \\frac{1}{\\sum_{i=1}^N w_i^2}\n    $$\n    对于朴素方案，当所有权重下溢为 $0$ 时，分母中的和为 $0$，导致除零错误。在这种失败情况下，问题将 $\\mathrm{ESS}$ 定义为 $0$。\n\n2.  **重采样决策**：在粒子滤波中，一个常见的做法是当 ESS 低于粒子总数 $N$ 的某个比例时对粒子进行重采样。规则是：\n    $$\n    \\text{如果 } \\frac{\\mathrm{ESS}(w)}{N}  \\tau \\text{ 则重采样}\n    $$\n    其中 $\\tau \\in (0,1]$ 是一个预定义的阈值。如果朴素方案失败且 $\\mathrm{ESS}=0$，此条件变为 $0  \\tau$，对于给定的 $\\tau=0.5$ 这是成立的。因此，朴素方案的失败会正确地触发重采样决策。\n\n3.  **全变差 (TV) 距离**：衡量两个得到的权重概率分布 $w^{\\text{naive}}$ 和 $w^{\\text{log}}$ 之间的差异。\n    $$\n    \\mathrm{TV}(w^{\\text{naive}}, w^{\\text{log}}) = \\frac{1}{2} \\sum_{i=1}^N |w_i^{\\text{naive}} - w_i^{\\text{log}}|\n    $$\n    TV 距离为 $0$ 意味着分布相同，而距离为 $1$ 意味着它们最大程度不同（即具有不相交的支撑集）。在朴素方案失败的情况下，问题规定将 $T$ 设为 $1$。这是一个合理的约定，因为全零的“分布”与任何有效的概率分布都具有最大距离。\n\n实现将首先为每个测试用例根据指定参数生成粒子和观测值。然后，对于每个用例，它将使用朴素方法和对数稳定方法计算权重，并导出所需的指标（$E_{\\text{naive}}, E_{\\text{log}}, R_{\\text{naive}}, R_{\\text{log}}, T$）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the particle weighting problem for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path)\n        {'N': 500, 'd': 5, 'sigma': 1.0, 'tau': 0.5, 'seed': 0, 's': 1.0, 'peaked': False},\n        # Case 2 (high dimension underflow)\n        {'N': 1000, 'd': 500, 'sigma': 0.2, 'tau': 0.5, 'seed': 1, 's': 2.0, 'peaked': False},\n        # Case 3 (extreme underflow boundary)\n        {'N': 800, 'd': 2000, 'sigma': 1.0, 'tau': 0.5, 'seed': 2, 's': 10.0, 'peaked': False},\n        # Case 4 (near-equal weights)\n        {'N': 300, 'd': 50, 'sigma': 1e6, 'tau': 0.5, 'seed': 3, 's': 5.0, 'peaked': False},\n        # Case 5 (peaked weight)\n        {'N': 1000, 'd': 100, 'sigma': 0.1, 'tau': 0.5, 'seed': 4, 's': 0.0, 'peaked': True},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        d = case['d']\n        sigma = case['sigma']\n        tau = case['tau']\n        seed = case['seed']\n        s = case['s']\n        is_peaked = case['peaked']\n\n        # --- Generate particles and observation ---\n        y = s * np.ones(d, dtype=np.float64)\n        \n        if is_peaked:\n            particles = np.full((N, d), y + 5.0 * np.ones(d), dtype=np.float64)\n            particles[0, :] = y\n        else:\n            rng = np.random.default_rng(seed)\n            particles = rng.standard_normal(size=(N, d), dtype=np.float64)\n\n        # --- Base calculation ---\n        # Calculate squared L2 distance for all particles\n        sq_dists = np.sum((particles - y)**2, axis=1)\n        \n        # Calculate log-weights (common for both schemes)\n        log_weights = -0.5 * sq_dists / (sigma**2)\n\n        # --- Naive Scheme ---\n        w_naive = np.zeros(N, dtype=np.float64)\n        e_naive = 0.0\n        \n        raw_weights = np.exp(log_weights)\n        sum_raw_weights = np.sum(raw_weights)\n\n        naive_failed = (sum_raw_weights == 0.0)\n\n        if not naive_failed:\n            w_naive = raw_weights / sum_raw_weights\n            sum_sq_w_naive = np.sum(w_naive**2)\n            if sum_sq_w_naive > 0:\n                e_naive = 1.0 / sum_sq_w_naive\n        \n        # Resampling decision: if ESS is 0, ESS/N = 0, which is  tau\n        r_naive = (e_naive / N)  tau\n\n        # --- Log-Weight Stabilized Scheme ---\n        max_log_weight = np.max(log_weights)\n        shifted_log_weights = log_weights - max_log_weight\n        u_weights = np.exp(shifted_log_weights)\n        sum_u_weights = np.sum(u_weights)\n        \n        w_log = u_weights / sum_u_weights\n        \n        sum_sq_w_log = np.sum(w_log**2)\n        e_log = 0.0\n        if sum_sq_w_log > 0:\n            e_log = 1.0 / sum_sq_w_log\n            \n        r_log = (e_log / N)  tau\n\n        # --- Total Variation Distance ---\n        if naive_failed:\n            t_dist = 1.0\n        else:\n            t_dist = 0.5 * np.sum(np.abs(w_naive - w_log))\n            \n        results.append([e_naive, e_log, r_naive, r_log, t_dist])\n\n    # --- Format output string ---\n    # The output format must match exactly: [[v1,v2,...],[v1,v2,...]] with no spaces.\n    # Standard str(list) adds spaces, so we build the string manually.\n    sub_results_str = []\n    for r in results:\n        # Convert boolean to required 'True'/'False' string literal\n        r_str = [f\"{val}\" for val in r]\n        sub_results_str.append(f\"[{','.join(r_str)}]\")\n    \n    final_output_str = f\"[{','.join(sub_results_str)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "3417322"}]}