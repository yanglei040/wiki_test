{"hands_on_practices": [{"introduction": "我们从一个理想化的线性高斯系统开始，以建立坚实的理论基础。这项练习要求您通过数学推导来证明，对于参数估计，联合卡尔曼滤波更新与一种双重策略（即先更新状态，然后通过回归更新参数）会得到完全相同的结果 ([@problem_id:3421586])。这项实践对于理解联合与双重策略之间选择的意义至关重要，它揭示了只有当系统偏离这些理想条件时，策略间的差异才会显现出来。", "problem": "考虑一个增广空间中的线性高斯状态-参数估计问题，其增广状态为 $z = (x,\\theta)^{\\top} \\in \\mathbb{R}^{2}$。预报（背景）分布是高斯分布，其均值为 $m^{f} = (m_{x}^{f}, m_{\\theta}^{f})^{\\top}$，协方差为\n$$\nP^{f} \\;=\\; \\begin{pmatrix}\np_{x}  c \\\\\nc  p_{\\theta}\n\\end{pmatrix},\n$$\n其中 $p_{x} > 0$，$p_{\\theta} > 0$，且 $|c|  \\sqrt{p_{x} p_{\\theta}}$。观测模型为 $y = H z + v$，其中 $H = (1,\\,0)$，因此测量值仅依赖于状态分量 $x$，并且 $v \\sim \\mathcal{N}(0,R)$，其中 $R = r > 0$。假设在大集合、线性高斯极限下，集合变换卡尔曼滤波器 (ETKF) 的均值更新与卡尔曼滤波器 (KF) 的均值更新一致。\n\n定义增广空间中状态和参数子空间上的正交投影为\n$$\nP_{x} \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  0\n\\end{pmatrix}, \n\\qquad\nP_{\\theta} \\;=\\; \\begin{pmatrix}\n0  0 \\\\\n0  1\n\\end{pmatrix}.\n$$\n\n比较以下两种计算参数 $\\theta$ 的分析均值的策略：\n\n1. 对增广状态 $z$ 进行联合 ETKF，即在增广空间中使用 $H$ 进行单次分析更新。\n\n2. 两步 ETKF（对偶策略）：首先仅使用 $y$ 更新状态 $x$；然后通过使用背景交叉协方差将 $\\theta$ 回归到 $x$ 上来更新 $\\theta$，即使用由正交投影耦合 $G = P_{\\theta} P^{f} P_{x} \\left(P_{x} P^{f} P_{x}\\right)^{-1}$ 导出的线性映射，使得参数均值增量为 $m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + G\\left(m_{x}^{a} - m_{x}^{f}\\right)$。\n\n从高斯-贝叶斯/卡尔曼更新原理和上述投影定义出发，推导参数分析均值之差的闭式表达式\n$$\n\\Delta m_{\\theta} \\;=\\; m_{\\theta}^{a,\\text{dual}} - m_{\\theta}^{a,\\text{joint}}\n$$\n用 $m_{x}^{f}$，$m_{\\theta}^{f}$，$y$，$p_{x}$，$c$ 和 $r$ 表示。将最终答案表示为单一的简化解析表达式。无需数值舍入。此处不适用角度或物理单位。", "solution": "该问题要求在线性高斯框架内比较两种状态-参数估计策略：联合更新和对偶（两步）更新。我们需要求出这两种方法产生的参数分析均值之差。分析是在集合变换卡尔曼滤波器 (ETKF) 的均值更新等同于标准卡尔曼滤波器 (KF) 更新的假设下进行的。\n\n状态向量 $z$ 的通用卡尔曼滤波器分析均值更新由下式给出：\n$$m^{a} = m^{f} + K(y - H m^{f})$$\n其中 $m^{f}$ 是预报（背景）均值，$y$ 是观测值，$H$ 是观测算子，$K$ 是卡尔曼增益，定义为：\n$$K = P^{f} H^{\\top} (H P^{f} H^{\\top} + R)^{-1}$$\n此处，$P^{f}$ 是预报误差协方差，$R$ 是观测误差协方差。\n\n给定以下信息：\n增广状态为 $z = (x, \\theta)^{\\top}$。\n预报均值为 $m^{f} = (m_{x}^{f}, m_{\\theta}^{f})^{\\top}$。\n预报协方差为 $P^{f} = \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix}$。\n观测模型为 $y = H z + v$，其中 $H = (1, 0)$ 且 $v \\sim \\mathcal{N}(0, r)$，因此 $R=r$。\n\n首先，让我们使用联合更新策略推导参数 $\\theta$ 的分析均值，记为 $m_{\\theta}^{a,\\text{joint}}$。\n\n**1. 联合 ETKF/KF 更新**\n\n在此策略中，更新应用于完整的增广状态 $z$。我们首先计算卡尔曼增益 $K$ 的分量。\n观测算子为 $H = (1, 0)$，所以其转置为 $H^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n项 $H P^{f} H^{\\top}$ 为：\n$$H P^{f} H^{\\top} = (1, 0) \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (p_{x}, c) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = p_{x}$$\n因此，项 $(H P^{f} H^{\\top} + R)^{-1}$ 为 $(p_{x} + r)^{-1}$。\n\n项 $P^{f} H^{\\top}$ 为：\n$$P^{f} H^{\\top} = \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix}$$\n现在，我们可以计算卡尔曼增益 $K$：\n$$K = \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix} (p_{x} + r)^{-1} = \\frac{1}{p_{x} + r} \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix}$$\n\n新息项为 $y - H m^{f}$：\n$$y - H m^{f} = y - (1, 0) \\begin{pmatrix} m_{x}^{f} \\\\ m_{\\theta}^{f} \\end{pmatrix} = y - m_{x}^{f}$$\n增广均值向量 $m^{a,\\text{joint}}$ 的分析更新为：\n$$m^{a,\\text{joint}} = \\begin{pmatrix} m_{x}^{a,\\text{joint}} \\\\ m_{\\theta}^{a,\\text{joint}} \\end{pmatrix} = \\begin{pmatrix} m_{x}^{f} \\\\ m_{\\theta}^{f} \\end{pmatrix} + \\frac{1}{p_{x} + r} \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix} (y - m_{x}^{f})$$\n从此表达式中，我们提取参数分量 $\\theta$ 的分析均值：\n$$m_{\\theta}^{a,\\text{joint}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n\n接下来，让我们使用对偶策略推导参数 $\\theta$ 的分析均值 $m_{\\theta}^{a,\\text{dual}}$。\n\n**2. 对偶（两步）ETKF/KF 更新**\n\n此策略包括两个步骤。\n\n**步骤 2a：更新状态分量 $x$。**\n这是一个标量卡尔曼滤波器问题。状态为 $x$，预报均值为 $m_{x}^{f}$，预报方差为 $p_{x}$。观测为 $y = x + v$，观测算子为 $H_{x}=1$，观测误差方差为 $r$。\n标量卡尔曼增益 $K_{x}$ 为：\n$$K_{x} = p_{x} (H_{x} p_{x} H_{x}^{\\top} + r)^{-1} = p_{x} (1 \\cdot p_{x} \\cdot 1 + r)^{-1} = \\frac{p_{x}}{p_{x} + r}$$\n$x$ 的分析均值，记为 $m_{x}^{a}$，为：\n$$m_{x}^{a} = m_{x}^{f} + K_{x} (y - H_{x} m_{x}^{f}) = m_{x}^{f} + \\frac{p_{x}}{p_{x} + r} (y - m_{x}^{f})$$\n\n**步骤 2b：通过回归更新参数分量 $\\theta$。**\n问题指定了参数均值的更新规则：\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + G(m_{x}^{a} - m_{x}^{f})$$\n其中 $G$ 是回归系数，表示 $x$ 的更新对 $\\theta$ 的影响。该系数由背景协方差结构导出。对于多元高斯分布，给定 $x$ 时 $\\theta$ 的条件期望为 $\\mathbb{E}[\\theta|x] = m_{\\theta}^{f} + \\text{Cov}(\\theta, x) \\text{Var}(x)^{-1} (x - m_{x}^{f})$。因此，回归系数为 $G = \\text{Cov}(\\theta, x) \\text{Var}(x)^{-1}$。从背景协方差矩阵 $P^{f}$ 中，我们有 $\\text{Cov}(\\theta, x) = c$ 和 $\\text{Var}(x) = p_{x}$。\n因此，回归系数为 $G = \\frac{c}{p_{x}}$。\n\n问题使用投影算子给出了 $G$ 的一个形式化定义，$G = P_{\\theta} P^{f} P_{x} (P_{x} P^{f} P_{x})^{-1}$。我们来验证这是否能得出相同的标量系数。该算子将 $x$ 的子空间映射到 $\\theta$ 的子空间。\n$P_{x} P^{f} P_{x} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} p_{x}  0 \\\\ 0  0 \\end{pmatrix}$。\n在相关子空间上的逆（或伪逆）是 $(P_{x} P^{f} P_{x})^{\\dagger} = \\begin{pmatrix} p_{x}^{-1}  0 \\\\ 0  0 \\end{pmatrix}$。\n$P_{\\theta} P^{f} P_{x} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ c  0 \\end{pmatrix}$。\n完整的算子是 $G_{op} = \\begin{pmatrix} 0  0 \\\\ c  0 \\end{pmatrix} \\begin{pmatrix} p_{x}^{-1}  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ c p_{x}^{-1}  0 \\end{pmatrix}$。该算子将增量向量 $(m_x^a - m_x^f, 0)^{\\top}$ 映射到 $(0, \\frac{c}{p_x}(m_x^a - m_x^f))^{\\top}$。标量回归系数 $G$ 确实是 $\\frac{c}{p_{x}}$。\n\n现在，我们将步骤 2a 中的状态增量 $(m_{x}^{a} - m_{x}^{f})$ 代入回归公式：\n$$m_{x}^{a} - m_{x}^{f} = \\frac{p_{x}}{p_{x} + r} (y - m_{x}^{f})$$\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + \\left(\\frac{c}{p_{x}}\\right) \\left( \\frac{p_{x}}{p_{x} + r} (y - m_{x}^{f}) \\right)$$\n分子和分母中的项 $p_{x}$ 相互抵消：\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n\n**3. 比较与最终差异**\n\n现在我们有了两种策略下参数分析均值的表达式：\n$$m_{\\theta}^{a,\\text{joint}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n这两个表达式是相同的。这表明，对于观测仅依赖于状态分量的线性高斯系统，参数的联合更新等同于先更新状态，然后使用背景统计量通过回归更新参数的两步过程。\n\n因此，参数分析均值之差为：\n$$\\Delta m_{\\theta} = m_{\\theta}^{a,\\text{dual}} - m_{\\theta}^{a,\\text{joint}}$$\n$$\\Delta m_{\\theta} = \\left( m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f}) \\right) - \\left( m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f}) \\right)$$\n$$\\Delta m_{\\theta} = 0$$\n\n该差异的闭式表达式就是数字零。", "answer": "$$\\boxed{0}$$", "id": "3421586"}, {"introduction": "在理想情况的基础上，本实践引入了所有反演问题中的一个根本性挑战：参数可辨识性。您将分析一个观测数据由于其结构对称性而无法提供足够信息以区分两个参数的情景 ([@problem_id:3421599])。这个思想实验强调了，如果没有一个适定的问题，无论是联合策略还是双重策略都无法成功，从而突出了实验设计的关键作用。", "problem": "考虑一个线性高斯状态-参数估计的设定。未知参数向量为 $\\theta \\in \\mathbb{R}^{3}$，其分量为 $\\theta = (\\theta_{1}, \\theta_{2}, \\theta_{3})^{\\top}$。您进行一个单步实验，使用一个确定性状态模型（即过程噪声恒为零），该模型根据以下公式将参数映射到状态 $x \\in \\mathbb{R}^{2}$\n$$\nx = M(\\theta) \\equiv \\begin{pmatrix} \\theta_{1} + \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix},\n$$\n并且您观测到\n$$\ny = H x + v, \\quad H \\equiv \\begin{pmatrix} 1  0 \\end{pmatrix},\n$$\n其中测量噪声为 $v \\sim \\mathcal{N}(0, \\sigma^{2})$，且噪声方差 $\\sigma^{2} > 0$ 已知。假设 $\\theta$ 的高斯先验具有零均值和对角协方差 $\\Sigma_{0} = \\mathrm{diag}(\\tau^{2}, \\tau^{2}, \\tau_{3}^{2})$，其中 $\\tau^{2} > 0$ 和 $\\tau_{3}^{2} > 0$ 是有限的，并考虑极限 $\\tau^{2} \\to \\infty$ 来为 $(\\theta_{1}, \\theta_{2})$ 建立一个无信息先验模型。\n\n1. 仅使用高斯密度和线性映射的贝叶斯法则，解释为什么在第一次实验中，当 $\\tau^{2} \\to \\infty$ 时，$(\\theta_{1}, \\theta_{2})$ 是不可辨识的，即 $(\\theta_{1}, \\theta_{2})$ 上的后验分布沿至少一个方向变得退化。通过刻画似然对 $(\\theta_{1}, \\theta_{2})$ 后验精度贡献的秩来证明您的结论。\n\n2. 为解决此模糊性，您通过增加第二次独立的、使用修改后映射 $M^{\\prime}(\\theta)$ 和观测算子 $H^{\\prime}$ 的实验来重新设计实验：\n$$\nx^{\\prime} = M^{\\prime}(\\theta) \\equiv \\begin{pmatrix} \\theta_{1} - \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix}, \\qquad y^{\\prime} = H^{\\prime} x^{\\prime} + v^{\\prime}, \\quad H^{\\prime} \\equiv \\begin{pmatrix} 1  0 \\end{pmatrix},\n$$\n其中 $v^{\\prime} \\sim \\mathcal{N}(0, \\sigma^{2})$ 独立于 $v$，且 $\\theta$ 的先验与上述相同。简要解释为什么这一改变消除了导致不可辨识性的对称性。\n\n3. 考虑两种估计策略：\n- 联合策略：在单次贝叶斯更新中，从 $(y, y^{\\prime})$ 联合估计 $(x, x^{\\prime}, \\theta)$。\n- 对偶策略：首先利用状态 $(x, x^{\\prime})$ 关于 $\\theta$ 的确定性定义，解析地消去它们，然后直接从 $(y, y^{\\prime})$ 估计 $\\theta$。\n\n在不使用任何特定算法公式的情况下，论证在此线性高斯设定中，两种策略必须得出相同的 $\\theta$ 后验分布。\n\n在包含第二次实验后，计算在极限 $\\tau^{2} \\to \\infty$（其中 $\\sigma^{2} > 0$ 和 $\\tau_{3}^{2} > 0$ 固定）下 $\\theta_{1}$ 的边际后验方差的闭式解。将您的最终答案表示为 $\\sigma^{2}$ 的符号函数。不要对结果进行四舍五入。最终答案必须是无单位的单个解析表达式。", "solution": "该问题要求分析线性高斯系统中的参数可辨识性，并计算后验方差。我们将依次解决这四个部分。\n\n首先，我们建立简化观测模型，该模型将参数 $\\theta$ 与观测值 $y$ 和 $y^{\\prime}$ 直接关联。这是问题中提到的对偶策略的一个实例，它简化了分析。\n\n对于第一次实验，状态为 $x = M(\\theta) = \\begin{pmatrix} \\theta_{1} + \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix}$。观测值为 $y = Hx + v = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\theta_{1} + \\theta_{2} \\\\ \\theta_{3} \\end{pmatrix} + v = \\theta_{1} + \\theta_{2} + v$。这可以写成 $y = K\\theta + v$ 的形式，其中前向算子为 $K = \\begin{pmatrix} 1  1  0 \\end{pmatrix}$。噪声 $v$ 是高斯的，$v \\sim \\mathcal{N}(0, \\sigma^2)$，协方差为 $R = \\sigma^2$。\n\n$\\theta$ 的先验是 $p(\\theta) \\sim \\mathcal{N}(0, \\Sigma_0)$，其中 $\\Sigma_0 = \\mathrm{diag}(\\tau^{2}, \\tau^{2}, \\tau_{3}^{2})$。\n给定 $y$ 时 $\\theta$ 的后验分布也是高斯的，$p(\\theta|y) \\sim \\mathcal{N}(\\mu_{post}, \\Sigma_{post})$，其逆协方差（精度）矩阵由线性高斯模型的标准贝叶斯更新公式给出：\n$$\n\\Sigma_{post}^{-1} = \\Sigma_0^{-1} + K^{\\top}R^{-1}K\n$$\n先验精度矩阵为 $\\Sigma_0^{-1} = \\mathrm{diag}(\\frac{1}{\\tau^2}, \\frac{1}{\\tau^2}, \\frac{1}{\\tau_3^2})$。\n项 $K^{\\top}R^{-1}K$ 代表从测量中获得的信息，即似然对后验精度的贡献。\n\n1. 第一次实验中的不可辨识性。\n来自似然的贡献是：\n$$\nK^{\\top}R^{-1}K = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} (\\sigma^2)^{-1} \\begin{pmatrix} 1  1  0 \\end{pmatrix} = \\frac{1}{\\sigma^2} \\begin{pmatrix} 1  1  0 \\\\ 1  1  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\n问题关注 $(\\theta_1, \\theta_2)$ 的可辨识性。我们检查该精度贡献的左上角 $2 \\times 2$ 分块，它对应于这些参数。该分块为 $\\frac{1}{\\sigma^2}\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$。此矩阵的秩为 $1$，因为其行列式为 $0$ 且它不是零矩阵。对于一个 $2 \\times 2$ 矩阵，秩为 $1$ 意味着存在一个一维的零空间。\n$\\theta$ 的后验精度为：\n$$\n\\Sigma_{post}^{-1} = \\begin{pmatrix} \\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}  \\frac{1}{\\sigma^2}  0 \\\\ \\frac{1}{\\sigma^2}  \\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}  0 \\\\ 0  0  \\frac{1}{\\tau_3^2} \\end{pmatrix}\n$$\n在极限 $\\tau^2 \\to \\infty$ 下，$(\\theta_1, \\theta_2)$ 的先验变为无信息的，先验精度项 $\\frac{1}{\\tau^2}$ 趋于 $0$。$\\theta$ 的后验精度矩阵收敛于：\n$$\n\\lim_{\\tau^2 \\to \\infty} \\Sigma_{post}^{-1} = \\begin{pmatrix} \\frac{1}{\\sigma^2}  \\frac{1}{\\sigma^2}  0 \\\\ \\frac{1}{\\sigma^2}  \\frac{1}{\\sigma^2}  0 \\\\ 0  0  \\frac{1}{\\tau_3^2} \\end{pmatrix}\n$$\n$(\\theta_1, \\theta_2)$ 的 $2 \\times 2$ 子矩阵是 $\\frac{1}{\\sigma^2}\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$。这个矩阵是奇异的。奇异的精度矩阵对应于其零空间方向上的无限方差（因此是一个退化的或非正常的后验分布）。该矩阵的零空间由向量 $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ 张成。这意味着观测值 $y$ 没有提供关于组合 $\\theta_1 - \\theta_2$ 的任何信息。数据仅约束了和 $\\theta_1 + \\theta_2$。因此，$(\\theta_1, \\theta_2)$ 是不可辨识的；对于任意常数 $c$，任何配对 $(\\theta_1+c, \\theta_2-c)$ 都给出相同的和，因此在给定数据的情况下是等可能性的。\n\n2. 通过第二次实验解决模糊性。\n第二次实验的状态为 $x' = M'(\\theta) = \\begin{pmatrix} \\theta_1 - \\theta_2 \\\\ \\theta_3 \\end{pmatrix}$，观测值为 $y' = H'x' + v' = \\theta_1 - \\theta_2 + v'$。这对应于一个简化模型 $y' = K'\\theta + v'$，其中 $K' = \\begin{pmatrix} 1  -1  0 \\end{pmatrix}$。\n由于测量值 $(y, y')$ 是独立的，总似然是各个似然的乘积。在线性高斯框架中，这意味着精度贡献是相加的。设 $\\mathbf{y} = \\begin{pmatrix} y \\\\ y' \\end{pmatrix}$，$\\mathbf{v} = \\begin{pmatrix} v \\\\ v' \\end{pmatrix}$，且 $\\mathbf{K} = \\begin{pmatrix} K \\\\ K' \\end{pmatrix} = \\begin{pmatrix} 1  1  0 \\\\ 1  -1  0 \\end{pmatrix}$。噪声协方差为 $\\mathbf{R} = \\mathrm{diag}(\\sigma^2, \\sigma^2)$。\n总似然对后验精度的贡献是 $\\mathbf{K}^{\\top}\\mathbf{R}^{-1}\\mathbf{K}$：\n$$\n\\mathbf{K}^{\\top}\\mathbf{R}^{-1}\\mathbf{K} = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sigma^2}  0 \\\\ 0  \\frac{1}{\\sigma^2} \\end{pmatrix} \\begin{pmatrix} 1  1  0 \\\\ 1  -1  0 \\end{pmatrix} = \\frac{1}{\\sigma^2} \\begin{pmatrix} 1  1 \\\\ 1  -1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 1  1  0 \\\\ 1  -1  0 \\end{pmatrix} = \\frac{1}{\\sigma^2} \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\n对应于 $(\\theta_1, \\theta_2)$ 的 $2 \\times 2$ 子矩阵现在是 $\\frac{2}{\\sigma^2} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。这是一个对角的、满秩的矩阵（秩为 $2$）。即使在极限 $\\tau^2 \\to \\infty$ 下，$(\\theta_1, \\theta_2)$ 的后验精度也是非奇异的。模糊性得到解决，因为第一次实验提供了关于 $\\theta_1 + \\theta_2$ 的信息，而第二次实验提供了关于 $\\theta_1 - \\theta_2$ 的信息。它们共同对 $\\theta_1$ 和 $\\theta_2$ 进行了单独的约束。\n\n3. 联合策略与对偶策略的等价性。\n这种等价性是贝叶斯推断的一个基本性质，源于概率论的规则。\n联合策略考虑一个增广状态向量，包括参数和状态，例如 $\\mathbf{z} = (x^{\\top}, x'^{\\top}, \\theta^{\\top})^{\\top}$。目标是找到参数的边际后验 $p(\\theta|y, y')$。这可以通过首先找到联合后验 $p(x, x', \\theta|y, y')$，然后对状态 $x$ 和 $x'$ 进行积分消去来获得：\n$$\np(\\theta|y, y') = \\iint p(x, x', \\theta|y, y') \\, dx \\, dx'\n$$\n使用贝叶斯法则，联合后验为 $p(x, x', \\theta|y, y') \\propto p(y, y'|x, x', \\theta) p(x, x', \\theta)$。完整的联合分布是 $p(y, y', x, x', \\theta) = p(y, y'|x, x', \\theta) p(x, x'|\\theta) p(\\theta)$。由于模型结构，$y$ 仅依赖于 $x$，$y'$ 仅依赖于 $x'$，所以 $p(y, y'|x, x', \\theta) = p(y|x)p(y'|x')$。状态是 $\\theta$ 的确定性函数，因此它们的条件概率是狄拉克δ函数：$p(x|\\theta) = \\delta(x - M(\\theta))$ 和 $p(x'|\\theta) = \\delta(x' - M'(\\theta))$。因此，\n$$\np(\\theta|y, y') \\propto p(\\theta) \\iint p(y|x) p(y'|x') \\delta(x - M(\\theta)) \\delta(x' - M'(\\theta)) \\, dx \\, dx'\n$$\n对δ函数的积分只是将确定性关系代入似然函数中：\n$$\np(\\theta|y, y') \\propto p(\\theta) p(y|M(\\theta)) p(y'|M'(\\theta))\n$$\n对偶策略从将状态定义代入观测模型开始，创建一个直接连接参数与观测值的简化模型：$y = H M(\\theta) + v$ 和 $y' = H' M'(\\theta) + v'$。然后似然函数直接写为 $\\theta$ 的函数，$p(y, y'|\\theta) = p(y|\\theta) p(y'|\\theta)$。根据贝叶斯法则，后验是 $p(\\theta|y, y') \\propto p(y, y'|\\theta) p(\\theta) = p(y|\\theta) p(y'|\\theta) p(\\theta)$。由于 $p(y|\\theta)$ 等价于 $p(y|M(\\theta))$，因此得到的后验表达式与相同的 $\\theta$ 函数成正比。因此，这两种策略必须得出相同的后验分布。在线性高斯情况下，这意味着它们会得到相同的后验均值和协方差。\n\n4. 计算 $\\theta_{1}$ 的边际后验方差。\n两次实验后 $\\theta$ 的后验精度为 $\\Sigma_{post}^{-1} = \\Sigma_0^{-1} + \\mathbf{K}^{\\top}\\mathbf{R}^{-1}\\mathbf{K}$。\n$$\n\\Sigma_{post}^{-1} = \\begin{pmatrix} \\frac{1}{\\tau^2}  0  0 \\\\ 0  \\frac{1}{\\tau^2}  0 \\\\ 0  0  \\frac{1}{\\tau_3^2} \\end{pmatrix} + \\frac{1}{\\sigma^2} \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\tau^2} + \\frac{2}{\\sigma^2}  0  0 \\\\ 0  \\frac{1}{\\tau^2} + \\frac{2}{\\sigma^2}  0 \\\\ 0  0  \\frac{1}{\\tau_3^2} \\end{pmatrix}\n$$\n我们取极限 $\\tau^2 \\to \\infty$：\n$$\n\\Sigma_{post, \\infty}^{-1} = \\lim_{\\tau^2 \\to \\infty} \\Sigma_{post}^{-1} = \\begin{pmatrix} \\frac{2}{\\sigma^2}  0  0 \\\\ 0  \\frac{2}{\\sigma^2}  0 \\\\ 0  0  \\frac{1}{\\tau_3^2} \\end{pmatrix}\n$$\n后验协方差矩阵 $\\Sigma_{post, \\infty}$ 是后验精度矩阵的逆。由于精度矩阵是对角的，其逆矩阵是一个对角矩阵，其元素是原始对角元素的倒数：\n$$\n\\Sigma_{post, \\infty} = (\\Sigma_{post, \\infty}^{-1})^{-1} = \\begin{pmatrix} \\frac{\\sigma^2}{2}  0  0 \\\\ 0  \\frac{\\sigma^2}{2}  0 \\\\ 0  0  \\tau_3^2 \\end{pmatrix}\n$$\n参数的边际后验方差是后验协方差矩阵对应的对角元素。因此，$\\theta_1$ 的边际后验方差是 $\\Sigma_{post, \\infty}$ 的 $(1,1)$ 元素。\n$$\n\\mathrm{Var}(\\theta_1 | y, y') = (\\Sigma_{post, \\infty})_{11} = \\frac{\\sigma^2}{2}\n$$\n此结果是 $\\sigma^2$ 的函数，符合要求。", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{2}}$$", "id": "3421599"}, {"introduction": "现在，我们从理论转向一个真实且复杂的应用。这项练习要求您在一个经典的混沌系统——Lorenz-96模型上，实现并比较联合与双重集合卡尔曼滤波器 (EnKF) ([@problem_id:3421568])。这个动手编程问题将让您探索在非线性设置下，第一个练习中的理论等价性是如何被打破的，并研究在不同条件下两种策略在滤波器稳定性等方面的实际性能差异。", "problem": "要求您实现、比较和分析两种集合卡尔曼滤波器 (EnKF) 策略——联合 (joint) 和对偶 (dual)——用于在离散时间 Lorenz-96 系统上同时进行状态-参数估计。Lorenz-96 动力学由以下常微分方程给出：\n$$\n\\frac{dx_i}{dt} = \\left( x_{i+1} - x_{i-2} \\right) x_{i-1} - x_i + F,\n$$\n其中索引是循环的，通过一个固定步长的四阶龙格-库塔格式进行离散化，以定义映射关系\n$$\nx_{t+1} = M(x_t, \\theta_t) + \\eta_t,\n$$\n其中 $x_t \\in \\mathbb{R}^N$ 是状态向量，$\\theta_t \\in \\mathbb{R}$ 是一个标量、时变参数，扮演强迫项 $F$ 的角色，$M(\\cdot)$ 是单步数值积分器，而 $\\eta_t$ 是加性模型噪声。该参数按随机游走演化\n$$\n\\theta_{t+1} = \\theta_t + \\xi_t,\n$$\n其中 $\\xi_t$ 是方差为 $Q_\\theta$ 的高斯噪声。观测是线性和局部的：\n$$\ny_t = H x_t + \\varepsilon_t,\n$$\n其中 $H \\in \\mathbb{R}^{m \\times N}$ 是一个选择算子，$\\varepsilon_t$ 是协方差为 $R$ 的高斯观测噪声。\n\n解决方案的基本基础包括：(i) 状态空间模型的定义；(ii) 针对线性观测算子的高斯贝叶斯更新，\n$$\nx^a = x^f + K(y - H x^f), \\quad K = P_{xy}\\left(P_{yy} + R\\right)^{-1},\n$$\n其中 $x^f$ 表示预报，$x^a$ 表示分析，$P_{xy}$ 是状态（或增广状态）与预报观测之间的互协方差，$P_{yy}$ 是预报观测的协方差；以及 (iii) 集合卡尔曼滤波器原理，该原理通过集合的样本协方差来近似协方差。\n\n您必须实现两种策略：\n- 联合 EnKF：将参数增广到状态中，形成 $z_t = [x_t; \\theta_t]$，用各自的动力学模型对其进行预报，并仅使用作用于状态分量的线性观测算子对增广集合进行单次卡尔曼分析。\n- 对偶 EnKF：每个时间步执行两次序贯分析。首先，仅使用状态-观测互协方差更新状态。其次，仅使用参数-观测互协方差更新参数。每次更新都使用与 $R$ 一致的独立扰动观测。\n\n使用以下固定设置：\n- 状态维度 $N=10$。\n- 时间步长 $dt=0.05$。\n- 同化步数 $T=120$。\n- 观测算子 $H$ 从索引 $0$ 开始，每隔一个状态分量选择一次，因此 $m=5$。\n- 真实初始参数 $\\theta_0 = 8.0$。\n- 真实初始状态 $x_0$ 按 $x_{0,i} = \\theta_0 + \\delta_i$ 抽取，其中 $\\delta_i \\sim \\mathcal{N}(0,1)$ 对 $i=1,\\dots,N$ 独立。\n- 状态模型噪声方差 $Q_x = 0.01$（对角协方差）。\n- 观测噪声协方差 $R = \\sigma_{\\text{obs}}^2 I_m$，其中 $\\sigma_{\\text{obs}} = 1.0$。\n- 集合大小 $N_e = 25$。\n- 初始集合平均猜测值：$x_0^{\\text{guess}} = x_0 + \\epsilon$，其中 $\\epsilon_i \\sim \\mathcal{N}(0,1)$，以及 $\\theta_0^{\\text{guess}} = \\theta_0 - 1.0$，状态的集合离散度为 $1.0$，参数的集合离散度为 $0.5$。\n\n将参数过程噪声方差 $Q_\\theta$ 视为研究变量。对于下面给出的测试套件中的每个 $Q_\\theta$ 值，使用该 $Q_\\theta$ 在真实参数演化中生成单个“真实”轨迹和相应的观测序列。然后，使用在其参数预报模型中相同的 $Q_\\theta$ 运行两种滤波器。对于每种策略和每个 $Q_\\theta$，计算在同化窗口内状态的均方根误差 (RMSE) 的时间平均值\n$$\n\\mathrm{RMSE}_x = \\frac{1}{T} \\sum_{t=1}^{T} \\left\\| x_t - \\bar{x}_t \\right\\|_2 \\big/ \\sqrt{N},\n$$\n以及参数的均方根误差\n$$\n\\mathrm{RMSE}_\\theta = \\frac{1}{T} \\sum_{t=1}^{T} \\left| \\theta_t - \\bar{\\theta}_t \\right|,\n$$\n其中 $\\bar{x}_t$ 和 $\\bar{\\theta}_t$ 是分析后在时间 $t$ 的集合平均值。\n\n按如下方式为给定的 $Q_\\theta$ 定义滤波器的稳定性：如果滤波器在所有时间点都产生有限值，并且满足 $\\mathrm{RMSE}_x \\le \\tau_x$ 和 $\\mathrm{RMSE}_\\theta \\le \\tau_\\theta$，则滤波器是稳定的，阈值为 $\\tau_x = 3.0$ 和 $\\tau_\\theta = 2.0$。策略在测试套件上的稳定性阈值定义为套件中滤波器保持稳定的最大 $Q_\\theta$ 值。\n\n参数过程噪声方差 $Q_\\theta$ 的测试套件：\n- 案例 1：$Q_\\theta = 0.0$。\n- 案例 2：$Q_\\theta = 0.001$。\n- 案例 3：$Q_\\theta = 0.01$。\n- 案例 4：$Q_\\theta = 0.05$。\n- 案例 5：$Q_\\theta = 0.1$。\n- 案例 6：$Q_\\theta = 0.5$。\n\n您的程序必须：\n- 实现具有循环索引和四阶龙格-库塔方法的 Lorenz-96 模型积分 $M(\\cdot)$，以应对给定的 $dt$。\n- 按照描述实现联合和对偶 EnKF 策略，使用带扰动观测的随机 EnKF。\n- 对于测试套件中的每个 $Q_\\theta$，计算联合和对偶滤波器的稳定性布尔值。\n- 计算两种滤波器的稳定性阈值，即每个滤波器保持稳定的测试套件中的最大 $Q_\\theta$ 值；如果没有一个是稳定的，则报告 $0.0$。\n\n最终输出规范：\n- 您的程序应产生一行输出，其中包含一个用方括号括起来的逗号分隔列表，格式如下\n$$\n[\\text{JT}, \\text{DT}, [s^{(J)}_1,\\dots,s^{(J)}_6], [s^{(D)}_1,\\dots,s^{(D)}_6]],\n$$\n其中 $\\text{JT}$ 是联合滤波器的稳定性阈值（浮点数），$\\text{DT}$ 是对偶滤波器的稳定性阈值（浮点数），$s^{(J)}_k$ 是联合滤波器在测试案例 $k$ 的布尔稳定性，而 $s^{(D)}_k$ 是对偶滤波器对应的布尔值。不应打印其他任何文本。", "solution": "Lorenz-96 系统是用于数据同化研究的典型混沌模型。我们首先指定动力学和观测模型。离散时间映射 $M(\\cdot)$ 是通过使用四阶龙格-库塔格式将常微分方程\n$$\n\\frac{dx_i}{dt} = \\left( x_{i+1} - x_{i-2} \\right) x_{i-1} - x_i + F\n$$\n在一个时间步 $dt$ 上积分得到的。索引是循环的，意味着 $x_{-1} = x_{N-1}$ 和 $x_{N} = x_0$ 等，以确保周期性边界条件。\n\n离散时间下的随机状态空间模型为\n$$\nx_{t+1} = M(x_t, \\theta_t) + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}\\left(0, Q_x I_N\\right),\n$$\n参数演化为\n$$\n\\theta_{t+1} = \\theta_t + \\xi_t, \\quad \\xi_t \\sim \\mathcal{N}(0, Q_\\theta).\n$$\n观测由以下公式给出\n$$\ny_t = H x_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}\\left(0, R\\right),\n$$\n其中 $H \\in \\mathbb{R}^{m \\times N}$ 选择分量的一个子集。我们通过选择索引 $0,2,4,\\dots$ 来取 $m = N/2$。\n\n集合卡尔曼滤波器 (EnKF) 基于用于线性观测算子的高斯贝叶斯更新，其协方差从一个集合中估计。在经典的线性高斯情况下，卡尔曼分析满足\n$$\nx^a = x^f + K(y - H x^f), \\quad K = P_{xy}\\left(P_{yy} + R\\right)^{-1},\n$$\n其中 $P_{xy}$ 是预报状态和预报观测之间的互协方差，$P_{yy}$ 是预报观测的协方差。在 EnKF 中，给定一个集合 $\\{x^{f,(k)}\\}_{k=1}^{N_e}$，这些协方差通过样本协方差来近似\n$$\n\\bar{x}^f = \\frac{1}{N_e} \\sum_{k=1}^{N_e} x^{f,(k)}, \\quad X' = \\left[x^{f,(1)} - \\bar{x}^f, \\dots, x^{f,(N_e)} - \\bar{x}^f\\right],\n$$\n$$\ny^{f,(k)} = H x^{f,(k)}, \\quad \\bar{y}^f = \\frac{1}{N_e} \\sum_{k=1}^{N_e} y^{f,(k)}, \\quad Y' = \\left[y^{f,(1)} - \\bar{y}^f, \\dots, y^{f,(N_e)} - \\bar{y}^f\\right],\n$$\n$$\nP_{xy} \\approx \\frac{1}{N_e-1} X' (Y')^\\top, \\quad P_{yy} \\approx \\frac{1}{N_e-1} Y' (Y')^\\top.\n$$\n在随机 EnKF 中，分析是针对每个成员使用扰动观测逐个执行的\n$$\ny^{(k)} = y + \\varepsilon^{(k)}, \\quad \\varepsilon^{(k)} \\sim \\mathcal{N}(0,R),\n$$\n并且每个成员通过以下方式更新\n$$\nx^{a,(k)} = x^{f,(k)} + K\\left(y^{(k)} - y^{f,(k)}\\right).\n$$\n\n对于联合状态-参数估计，我们考虑增广状态\n$$\nz_t = \\begin{bmatrix} x_t \\\\ \\theta_t \\end{bmatrix} \\in \\mathbb{R}^{N+1},\n$$\n其预报集合成员通过应用各自的动力学模型获得：\n$$\nx_{t+1}^{f,(k)} = M\\left(x_t^{a,(k)}, \\theta_t^{a,(k)}\\right) + \\eta_t^{(k)}, \\quad \\theta_{t+1}^{f,(k)} = \\theta_t^{a,(k)} + \\xi_t^{(k)}.\n$$\n我们像之前一样形成增广异常和观测异常，然后计算\n$$\nP_{zy} \\approx \\frac{1}{N_e-1} Z' (Y')^\\top, \\quad P_{yy} \\approx \\frac{1}{N_e-1} Y' (Y')^\\top,\n$$\n和增广卡尔曼增益\n$$\nK_z = P_{zy}\\left(P_{yy} + R\\right)^{-1}.\n$$\n每个集合成员的联合分析更新为\n$$\nz^{a,(k)} = z^{f,(k)} + K_z\\left(y^{(k)} - y^{f,(k)}\\right),\n$$\n其中 $y^{f,(k)} = H x^{f,(k)}$ 并且观测扰动 $\\varepsilon^{(k)}$ 是独立抽取的。\n\n对于对偶 EnKF，我们执行两次序贯分析。首先，使用状态-观测互协方差进行仅状态分析\n$$\nP_{xy} \\approx \\frac{1}{N_e-1} X' (Y')^\\top, \\quad K_x = P_{xy}\\left(P_{yy} + R\\right)^{-1},\n$$\n成员更新为\n$$\nx^{a,(k)} = x^{f,(k)} + K_x\\left(y^{(k)} - y^{f,(k)}\\right),\n$$\n在此步骤中保持 $\\theta^{f,(k)}$ 不变。其次，由参数-观测互协方差驱动的仅参数分析\n$$\nP_{\\theta y} \\approx \\frac{1}{N_e-1} \\Theta' (Y')^\\top,\n$$\n其中 $\\Theta' = [\\theta^{f,(1)} - \\bar{\\theta}^f, \\dots, \\theta^{f,(N_e)} - \\bar{\\theta}^f]$ 是参数值的 $1 \\times N_e$ 异常向量。参数增益为\n$$\nK_\\theta = P_{\\theta y}\\left(P_{yy} + R\\right)^{-1},\n$$\n成员参数更新为\n$$\n\\theta^{a,(k)} = \\theta^{f,(k)} + K_\\theta\\left(y^{(k)} - y^{f,(k)}\\right).\n$$\n在这种对偶方法中，预报观测 $y^{f,(k)}$ 及其异常是根据预报状态计算的；将它们同时用于状态和参数更新，确保了一致地使用相同的创新 (innovations)。\n\n为了评估稳定性，我们计算时间平均的均方根误差：\n$$\n\\mathrm{RMSE}_x = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\left\\| x_t - \\bar{x}_t \\right\\|_2}{\\sqrt{N}}, \\quad \\mathrm{RMSE}_\\theta = \\frac{1}{T} \\sum_{t=1}^{T} \\left| \\theta_t - \\bar{\\theta}_t \\right|.\n$$\n对于给定的 $Q_\\theta$，如果所有集合分析都产生有限数（没有非数值或无穷大），并且满足阈值 $\\tau_x = 3.0$ 和 $\\tau_\\theta = 2.0$：\n$$\n\\mathrm{RMSE}_x \\le \\tau_x, \\quad \\mathrm{RMSE}_\\theta \\le \\tau_\\theta,\n$$\n我们就宣布滤波器是稳定的。在测试套件上的稳定性阈值定义为在测试值中保持稳定性的最大 $Q_\\theta$。\n\n算法设计细节：\n- Lorenz-96 右端项使用循环索引进行评估以保持动力学特性。\n- 状态的预报噪声使用 $Q_x I_N$，在数值积分后添加；参数预报使用 $Q_\\theta$ 作为随机游走的方差。\n- 使用随机 EnKF，其中观测扰动 $\\varepsilon^{(k)} \\sim \\mathcal{N}(0,R)$ 对每个成员和每次更新独立抽取。\n- 样本协方差通过异常计算，观测协方差 $P_{yy}$ 通过正定矩阵 $R$ 隐式正则化；如有必要，可向对角线添加小的数值抖动以保证数值稳定性。\n- 观测算子 $H$ 是一个线性选择器，选择一半的状态分量，确保了一个非平凡的部分观测场景。\n- 程序循环遍历指定的 $Q_\\theta$ 值，为每个案例生成一致的真实值和观测序列，运行两种滤波器，并汇总稳定性结果。\n\n最后，程序打印一行，包含联合阈值、对偶阈值以及两个布尔值列表，标记每个测试案例的稳定性，格式完全符合要求：\n$$\n[\\text{JT}, \\text{DT}, [s^{(J)}_1,\\dots,s^{(J)}_6], [s^{(D)}_1,\\dots,s^{(D)}_6]].\n$$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lorenz96_rhs(x, F):\n    \"\"\"\n    Compute the Lorenz-96 right-hand side for state x and forcing F.\n    x: array of shape (N,)\n    F: scalar forcing\n    Returns dx/dt: array of shape (N,)\n    \"\"\"\n    N = x.shape[0]\n    dx = np.empty_like(x)\n    # cyclic indices: x_{i+1}, x_{i-2}, x_{i-1}\n    for i in range(N):\n        xp1 = x[(i + 1) % N]\n        xm2 = x[(i - 2) % N]\n        xm1 = x[(i - 1) % N]\n        dx[i] = (xp1 - xm2) * xm1 - x[i] + F\n    return dx\n\ndef rk4_step(x, F, dt):\n    \"\"\"\n    One RK4 step for Lorenz-96.\n    \"\"\"\n    k1 = lorenz96_rhs(x, F)\n    k2 = lorenz96_rhs(x + 0.5 * dt * k1, F)\n    k3 = lorenz96_rhs(x + 0.5 * dt * k2, F)\n    k4 = lorenz96_rhs(x + dt * k3, F)\n    return x + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n\ndef build_observation_matrix(N):\n    \"\"\"\n    Build H selecting every other component starting at index 0.\n    Returns H of shape (m, N), where m = N//2 for even N.\n    \"\"\"\n    indices = np.arange(0, N, 2)\n    m = len(indices)\n    H = np.zeros((m, N))\n    for row, idx in enumerate(indices):\n        H[row, idx] = 1.0\n    return H\n\ndef generate_truth(N, T, dt, F0, Q_theta, Q_x, H, R, rng):\n    \"\"\"\n    Generate true trajectory (X_true, Theta_true) and observations Y.\n    \"\"\"\n    X_true = np.zeros((T+1, N))\n    Theta_true = np.zeros(T+1)\n    # Initial true state around F0\n    X_true[0] = F0 + rng.normal(0.0, 1.0, size=N)\n    Theta_true[0] = F0\n\n    for t in range(T):\n        F_t = Theta_true[t]\n        x_next = rk4_step(X_true[t], F_t, dt)\n        # Add model noise\n        x_next = x_next + rng.normal(0.0, np.sqrt(Q_x), size=N)\n        X_true[t+1] = x_next\n        # Parameter random walk\n        Theta_true[t+1] = Theta_true[t] + rng.normal(0.0, np.sqrt(Q_theta))\n\n    # Observations for t=1..T corresponding to X_true[t]\n    m = H.shape[0]\n    Y = np.zeros((T, m))\n    for t in range(1, T+1):\n        y = H @ X_true[t]\n        y = y + rng.normal(0.0, np.sqrt(np.diag(R)), size=m)\n        Y[t-1] = y\n    return X_true, Theta_true, Y\n\ndef enkf_joint(N, T, dt, Q_theta, Q_x, H, R, Y, X_true, Theta_true, x0_guess, theta0_guess, Ne, rng):\n    \"\"\"\n    Joint EnKF on augmented state [x; theta].\n    Returns (rmse_x, rmse_theta, stable_boolean).\n    \"\"\"\n    m = H.shape[0]\n    # Initialize ensemble\n    x_ens = x0_guess + rng.normal(0.0, 1.0, size=(Ne, N))\n    theta_ens = theta0_guess + rng.normal(0.0, 0.5, size=Ne)\n\n    # Storage for ensemble means\n    x_mean = np.zeros((T+1, N))\n    theta_mean = np.zeros(T+1)\n    x_mean[0] = np.mean(x_ens, axis=0)\n    theta_mean[0] = np.mean(theta_ens)\n\n    # Precompute sqrt covariances for obs perturbations\n    obs_std = np.sqrt(np.diag(R))\n\n    stable = True\n    eps_jitter = 1e-9\n\n    for t in range(T):\n        # Forecast step\n        x_f = np.zeros_like(x_ens)\n        theta_f = np.zeros_like(theta_ens)\n        for k in range(Ne):\n            x_f[k] = rk4_step(x_ens[k], theta_ens[k], dt)\n            x_f[k] += rng.normal(0.0, np.sqrt(Q_x), size=N)\n            theta_f[k] = theta_ens[k] + rng.normal(0.0, np.sqrt(Q_theta))\n        # Predicted observations\n        y_pred = (H @ x_f.T).T  # shape (Ne, m)\n\n        # Compute anomalies for augmented state and obs\n        x_f_mean = np.mean(x_f, axis=0)\n        theta_f_mean = np.mean(theta_f)\n        y_mean = np.mean(y_pred, axis=0)\n\n        X_anom = x_f - x_f_mean  # (Ne, N)\n        theta_anom = theta_f - theta_f_mean  # (Ne,)\n        # Build augmented anomalies Z' of shape (Ne, N+1)\n        Z_anom = np.hstack([X_anom, theta_anom[:, None]])\n        Y_anom = y_pred - y_mean  # (Ne, m)\n\n        # Covariances\n        P_zy = (Z_anom.T @ Y_anom) / (Ne - 1)  # (N+1, m)\n        P_yy = (Y_anom.T @ Y_anom) / (Ne - 1) + R + eps_jitter * np.eye(m)  # (m, m)\n\n        # Solve for gain K = P_zy @ inv(P_yy)\n        try:\n            K = np.linalg.solve(P_yy.T, P_zy.T).T  # more stable than inv; solve P_yy^T * X^T = P_zy^T\n        except np.linalg.LinAlgError:\n            stable = False\n            break\n\n        # Analysis with perturbed observations\n        x_a = np.zeros_like(x_f)\n        theta_a = np.zeros_like(theta_f)\n        y_obs = Y[t]  # observation at time t+1\n        for k in range(Ne):\n            eps = rng.normal(0.0, obs_std, size=m)\n            innov = (y_obs + eps) - y_pred[k]\n            update = K @ innov  # (N+1,)\n            x_a[k] = x_f[k] + update[:N]\n            theta_a[k] = theta_f[k] + update[N]\n\n        # Check for numerical issues\n        if not (np.isfinite(x_a).all() and np.isfinite(theta_a).all()):\n            stable = False\n            break\n\n        # Prepare for next step\n        x_ens = x_a\n        theta_ens = theta_a\n\n        # Save means\n        x_mean[t+1] = np.mean(x_ens, axis=0)\n        theta_mean[t+1] = np.mean(theta_ens)\n\n    # Compute RMSEs\n    if not stable:\n        return np.inf, np.inf, False\n\n    # Time-averaged RMSE over t=1..T\n    rmse_x = 0.0\n    rmse_theta = 0.0\n    for t in range(1, T+1):\n        rmse_x += np.linalg.norm(X_true[t] - x_mean[t]) / np.sqrt(N)\n        rmse_theta += abs(Theta_true[t] - theta_mean[t])\n    rmse_x /= T\n    rmse_theta /= T\n\n    # Stability thresholds\n    tau_x = 3.0\n    tau_theta = 2.0\n    stable = np.isfinite(rmse_x) and np.isfinite(rmse_theta) and (rmse_x = tau_x) and (rmse_theta = tau_theta)\n    return rmse_x, rmse_theta, stable\n\ndef enkf_dual(N, T, dt, Q_theta, Q_x, H, R, Y, X_true, Theta_true, x0_guess, theta0_guess, Ne, rng):\n    \"\"\"\n    Dual EnKF: sequential state update then parameter update per time step.\n    Returns (rmse_x, rmse_theta, stable_boolean).\n    \"\"\"\n    m = H.shape[0]\n    # Initialize ensemble\n    x_ens = x0_guess + rng.normal(0.0, 1.0, size=(Ne, N))\n    theta_ens = theta0_guess + rng.normal(0.0, 0.5, size=Ne)\n\n    x_mean = np.zeros((T+1, N))\n    theta_mean = np.zeros(T+1)\n    x_mean[0] = np.mean(x_ens, axis=0)\n    theta_mean[0] = np.mean(theta_ens)\n\n    obs_std = np.sqrt(np.diag(R))\n    stable = True\n    eps_jitter = 1e-9\n\n    for t in range(T):\n        # Forecast step\n        x_f = np.zeros_like(x_ens)\n        theta_f = np.zeros_like(theta_ens)\n        for k in range(Ne):\n            x_f[k] = rk4_step(x_ens[k], theta_ens[k], dt)\n            x_f[k] += rng.normal(0.0, np.sqrt(Q_x), size=N)\n            theta_f[k] = theta_ens[k] + rng.normal(0.0, np.sqrt(Q_theta))\n\n        y_pred = (H @ x_f.T).T  # (Ne, m)\n        y_mean = np.mean(y_pred, axis=0)\n        Y_anom = y_pred - y_mean  # (Ne, m)\n\n        # State update\n        x_f_mean = np.mean(x_f, axis=0)\n        X_anom = x_f - x_f_mean  # (Ne, N)\n        P_xy = (X_anom.T @ Y_anom) / (Ne - 1)  # (N, m)\n        P_yy = (Y_anom.T @ Y_anom) / (Ne - 1) + R + eps_jitter * np.eye(m)\n        try:\n            K_x = np.linalg.solve(P_yy.T, P_xy.T).T  # (N, m)\n        except np.linalg.LinAlgError:\n            stable = False\n            break\n\n        y_obs = Y[t]\n        x_a = np.zeros_like(x_f)\n        for k in range(Ne):\n            eps = rng.normal(0.0, obs_std, size=m)\n            innov = (y_obs + eps) - y_pred[k]\n            x_a[k] = x_f[k] + K_x @ innov\n\n        # Parameter update using forecast obs anomalies (dual strategy)\n        theta_f_mean = np.mean(theta_f)\n        theta_anom = theta_f - theta_f_mean  # (Ne,)\n        # P_{theta y}: (1 x m)\n        P_thetay = (theta_anom[:, None].T @ Y_anom) / (Ne - 1)  # (1, m)\n        try:\n            K_theta = np.linalg.solve(P_yy.T, P_thetay.T).T  # (1, m)\n        except np.linalg.LinAlgError:\n            stable = False\n            break\n\n        theta_a = np.zeros_like(theta_f)\n        for k in range(Ne):\n            eps = rng.normal(0.0, obs_std, size=m)\n            innov = (y_obs + eps) - y_pred[k]\n            theta_a[k] = theta_f[k] + float(K_theta @ innov)\n\n        # Check for numerical issues\n        if not (np.isfinite(x_a).all() and np.isfinite(theta_a).all()):\n            stable = False\n            break\n\n        # Prepare next step\n        x_ens = x_a\n        theta_ens = theta_a\n\n        x_mean[t+1] = np.mean(x_ens, axis=0)\n        theta_mean[t+1] = np.mean(theta_ens)\n\n    if not stable:\n        return np.inf, np.inf, False\n\n    rmse_x = 0.0\n    rmse_theta = 0.0\n    for t in range(1, T+1):\n        rmse_x += np.linalg.norm(X_true[t] - x_mean[t]) / np.sqrt(N)\n        rmse_theta += abs(Theta_true[t] - theta_mean[t])\n    rmse_x /= T\n    rmse_theta /= T\n\n    tau_x = 3.0\n    tau_theta = 2.0\n    stable = np.isfinite(rmse_x) and np.isfinite(rmse_theta) and (rmse_x = tau_x) and (rmse_theta = tau_theta)\n    return rmse_x, rmse_theta, stable\n\ndef solve():\n    rng = np.random.default_rng(42)\n\n    # Fixed settings\n    N = 10\n    dt = 0.05\n    T = 120\n    Ne = 25\n    Q_x = 0.01\n    sigma_obs = 1.0\n    R = (sigma_obs ** 2) * np.eye(N // 2)\n    H = build_observation_matrix(N)\n    F0 = 8.0\n\n    # Test suite of Q_theta values\n    test_cases = [0.0, 0.001, 0.01, 0.05, 0.1, 0.5]\n\n    stable_joint = []\n    stable_dual = []\n\n    # For threshold computation\n    joint_stable_Qs = []\n    dual_stable_Qs = []\n\n    # Run through test cases\n    for Q_theta in test_cases:\n        # Re-seed for reproducibility per case\n        case_rng = np.random.default_rng(1000 + int(Q_theta * 1e6))\n\n        # Generate truth and observations\n        X_true, Theta_true, Y = generate_truth(\n            N=N, T=T, dt=dt, F0=F0, Q_theta=Q_theta, Q_x=Q_x, H=H, R=R, rng=case_rng\n        )\n\n        # Initial guesses\n        x0_guess = X_true[0] + rng.normal(0.0, 1.0, size=N)\n        theta0_guess = F0 - 1.0\n\n        # Joint EnKF\n        rmse_x_j, rmse_th_j, st_j = enkf_joint(\n            N=N, T=T, dt=dt, Q_theta=Q_theta, Q_x=Q_x, H=H, R=R, Y=Y,\n            X_true=X_true, Theta_true=Theta_true,\n            x0_guess=x0_guess, theta0_guess=theta0_guess, Ne=Ne, rng=np.random.default_rng(2000 + int(Q_theta * 1e6))\n        )\n        stable_joint.append(st_j)\n        if st_j:\n            joint_stable_Qs.append(Q_theta)\n\n        # Dual EnKF\n        rmse_x_d, rmse_th_d, st_d = enkf_dual(\n            N=N, T=T, dt=dt, Q_theta=Q_theta, Q_x=Q_x, H=H, R=R, Y=Y,\n            X_true=X_true, Theta_true=Theta_true,\n            x0_guess=x0_guess, theta0_guess=theta0_guess, Ne=Ne, rng=np.random.default_rng(3000 + int(Q_theta * 1e6))\n        )\n        stable_dual.append(st_d)\n        if st_d:\n            dual_stable_Qs.append(Q_theta)\n\n    # Stability thresholds (maximum Q_theta in test suite with stability)\n    JT = max(joint_stable_Qs) if joint_stable_Qs else 0.0\n    DT = max(dual_stable_Qs) if dual_stable_Qs else 0.0\n\n    # Final print statement in the exact required format.\n    # Convert booleans to 'True'/'False' strings automatically by str().\n    print(f\"[{JT},{DT},[{','.join(map(str, stable_joint))}],[{','.join(map(str, stable_dual))}]]\")\n\nsolve()\n```", "id": "3421568"}]}