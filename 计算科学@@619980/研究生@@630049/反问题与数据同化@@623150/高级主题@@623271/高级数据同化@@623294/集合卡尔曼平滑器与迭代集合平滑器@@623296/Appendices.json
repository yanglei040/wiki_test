{"hands_on_practices": [{"introduction": "要掌握集合卡尔曼平滑器，最好的起点是亲手实现其核心算法。本练习将指导您为一个简单的线性系统构建一个双时集合卡尔曼平滑器（EnKS）。通过这个实践，您将学习如何利用未来观测来更新过去的状态估计，其关键在于使用集合交叉协方差将未来分析步的增量“回归”到过去的时间点上 [@problem_id:3379469]。这个练习旨在从基本原理出发，巩固您对集合平滑机制的理解。", "problem": "为一个线性标量状态空间系统实现一个双时间集合卡尔曼平滑器 (Ensemble Kalman Smoother, EnKS)，并在同化下一个时刻的单个观测后，计算初始时刻的平滑集合均值。使用以下基础和定义来推导您的算法。\n\n该系统是线性、标量和高斯的：\n- 状态演化：$x_1 = a \\, x_0 + w_0$，其中 $w_0 \\sim \\mathcal{N}(0, q)$，$x_0$ 表示时间 $t = 0$ 时的状态，$x_1$ 表示时间 $t = 1$ 时的状态。\n- 观测：$y_1 = H \\, x_1 + v_1$，其中 $H = 1$，$v_1 \\sim \\mathcal{N}(0, r)$，$y_1$ 是时间 $t = 1$ 时的观测值。\n\n假设使用一个大小为 $N$ 的集合来表示时间 $t = 0$ 时的先验，其先验分布为 $x_0 \\sim \\mathcal{N}(m_0, P_0)$。目标是使用双时间 EnKS 在同化时间 $t = 1$ 的观测 $y_1$ 后，计算时间 $t = 0$ 的平滑集合均值。\n\n使用这些基础且经过充分检验的定义：\n- 向量 $\\{z^{(n)}\\}_{n=1}^N$ 的样本均值为 $\\bar{z} = \\frac{1}{N} \\sum_{n=1}^N z^{(n)}$。\n- 集合向量 $\\{x^{(n)}\\}_{n=1}^N$ 和 $\\{y^{(n)}\\}_{n=1}^N$ 之间的无偏样本协方差为 $\\operatorname{Cov}(x, y) = \\frac{1}{N - 1} \\sum_{n=1}^N \\left(x^{(n)} - \\bar{x}\\right)\\left(y^{(n)} - \\bar{y}\\right)$，无偏样本方差为 $\\operatorname{Var}(x) = \\operatorname{Cov}(x, x)$。\n- 随机集合卡尔曼滤波器 (Ensemble Kalman Filter, EnKF) 对每个集合成员独立使用扰动观测 $\\tilde{y}^{(n)} = y_1 + \\epsilon^{(n)}$（其中 $\\epsilon^{(n)} \\sim \\mathcal{N}(0, r)$），以确保在线性高斯情况下后验方差的正确性。\n\n根据这些基本原则实现以下算法大纲：\n- 在时间 $t = 0$ 时，从 $\\mathcal{N}(m_0, P_0)$ 中抽取一个初始集合 $\\{x_0^{(n)}\\}_{n=1}^N$。\n- 将每个集合成员向前传播，以获得时间 $t = 1$ 时的预报集合：$x_1^{f,(n)} = a \\, x_0^{(n)} + w_0^{(n)}$，其中 $w_0^{(n)} \\sim \\mathcal{N}(0, q)$ 且相互独立。\n- 在时间 $t = 1$ 时，使用扰动观测执行随机 EnKF 分析，通过基于集合协方差的最佳线性更新，从 $\\{x_1^{f,(n)}\\}_{n=1}^N$ 中获得分析集合 $\\{x_1^{a,(n)}\\}_{n=1}^N$。\n- 应用双时间 EnKS 更新，使用基于 $\\{x_0^{(n)}\\}_{n=1}^N$ 和 $\\{x_1^{f,(n)}\\}_{n=1}^N$ 之间集合交叉协方差的最佳线性预测器，将时间 $t = 1$ 时的分析增量映射回时间 $t = 0$。如果时间 $t = 1$ 时的样本方差在数值上为零，则将平滑增益设置为 $0$。\n- 输出平滑集合均值 $\\bar{x}_0^{s} = \\frac{1}{N} \\sum_{n=1}^N x_0^{s,(n)}$。\n\n使用固定参数 $a = 0.9$、$q = 0.1$、$r = 0.4$、$N = 10$ 和 $H = 1$。为保证可复现性，对于索引为 $i$（从 $i = 0$ 开始）的每个测试用例，将随机种子设置为 $1000 + i$。当从 $\\mathcal{N}(m_0, P_0)$ 采样时，如果 $P_0 = 0$，则将所有集合成员精确地设为 $m_0$。\n\n测试套件。对于下面的每一组参数，计算同化 $y_1$ 后在时间 $t = 0$ 的平滑集合均值：\n- 情况 1：$(m_0, P_0, y_1) = (1.0, 1.0, 0.5)$，种子 $= 1000$。\n- 情况 2：$(m_0, P_0, y_1) = (0.0, 0.0, 0.0)$，种子 $= 1001$。\n- 情况 3：$(m_0, P_0, y_1) = (-2.0, 4.0, 1.0)$，种子 $= 1002$。\n- 情况 4：$(m_0, P_0, y_1) = (0.5, 0.5, 0.45)$，种子 $= 1003$。\n- 情况 5：$(m_0, P_0, y_1) = (0.0, 1.0, 5.0)$，种子 $= 1004$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个包含 5 个平滑均值的列表，四舍五入到 6 位小数，以逗号分隔并用方括号括起来（例如 $[x_1, x_2, x_3, x_4, x_5]$）。", "solution": "该问题要求为一个线性、标量、高斯状态空间系统实现一个双时间集合卡尔曼平滑器 (EnKS)。目标是在同化时间 $t=1$ 的单个观测后，计算初始时间 $t=0$ 的状态平滑集合均值。该解决方案源于基于集合的数据同化的基本原理。\n\n该系统由以下部分定义：\n1.  状态演化模型：$x_1 = a \\, x_0 + w_0$，其中 $x_k$ 是时间 $t=k$ 时的状态，$a$ 是一个常数系数，$w_0$ 是过程噪声，从正态分布 $w_0 \\sim \\mathcal{N}(0, q)$ 中抽取。\n2.  观测模型：$y_1 = H \\, x_1 + v_1$，其中 $y_1$ 是 $t=1$ 时的观测，$H$ 是观测算子（给定为 $H=1$），$v_1$ 是观测噪声，$v_1 \\sim \\mathcal{N}(0, r)$。\n\n该算法分几个步骤进行，从代表 $t=0$ 时状态先验知识的初始集合开始，并根据 $t=1$ 时的观测顺序更新该知识。\n\n**步骤 1：集合初始化**\n关于 $t=0$ 时状态的先验知识由正态分布 $x_0 \\sim \\mathcal{N}(m_0, P_0)$ 给出。该分布由一个包含 $N$ 个成员的集合 $\\{x_0^{(n)}\\}_{n=1}^N$ 表示。每个成员都是从该分布中抽取的样本。对于非零先验方差 $P_0  0$，样本生成为 $x_0^{(n)} \\sim \\mathcal{N}(m_0, P_0)$。在先验方差为零的特殊情况下，$P_0=0$，状态是完全已知的，所有集合成员都设置为其均值，即对所有 $n \\in \\{1, \\dots, N\\}$，$x_0^{(n)} = m_0$。\n\n**步骤 2：预报至时间 $t=1$**\n初始集合的每个成员都使用状态演化模型向前传播。一个独立的过程噪声实现 $w_0^{(n)} \\sim \\mathcal{N}(0, q)$ 被添加到每个成员上。这产生了 $t=1$ 时的预报集合，记为 $\\{x_1^{f,(n)}\\}_{n=1}^N$：\n$$x_1^{f,(n)} = a \\, x_0^{(n)} + w_0^{(n)}$$\n\n**步骤 3：在时间 $t=1$ 进行分析**\n分析步骤使用观测 $y_1$ 来更新预报集合。采用随机集合卡尔曼滤波器 (EnKF)。其核心思想是从样本统计数据中计算出类似卡尔曼的增益，并更新每个集合成员。为确保后验集合具有统计上正确的方差，使用了扰动观测。\n\n每个集合成员的分析更新为：\n$$x_1^{a,(n)} = x_1^{f,(n)} + K_1 \\left( \\tilde{y}^{(n)} - H x_1^{f,(n)} \\right)$$\n其中 $\\{x_1^{a,(n)}\\}_{n=1}^N$ 是 $t=1$ 时的分析集合。\n\n此更新的组成部分是：\n-   **扰动观测**：$\\tilde{y}^{(n)} = y_1 + \\epsilon^{(n)}$，其中每个 $\\epsilon^{(n)}$ 是从观测噪声分布中独立抽取的样本，$\\epsilon^{(n)} \\sim \\mathcal{N}(0, r)$。\n-   **预报观测**：由于 $H=1$，每个成员的预报观测为 $y_1^{f,(n)} = H x_1^{f,(n)} = x_1^{f,(n)}$。\n-   **卡尔曼增益 $K_1$**：该增益使用样本协方差计算。它是状态与观测的样本协方差与包含观测误差的预报观测样本方差之比。\n    $$K_1 = \\frac{\\operatorname{Cov}(x_1^f, y_1^f)}{\\operatorname{Var}(y_1^f) + r}$$\n    使用无偏样本方差算子 $\\operatorname{Var}(\\cdot)$ 且 $H=1$，这变为：\n    $$K_1 = \\frac{\\operatorname{Var}(x_1^f)}{\\operatorname{Var}(x_1^f) + r}$$\n    其中 $\\operatorname{Var}(x_1^f) = \\frac{1}{N-1} \\sum_{n=1}^N \\left(x_1^{f,(n)} - \\bar{x}_1^f\\right)^2$，而 $\\bar{x}_1^f$ 是预报集合的均值。\n\n结合这些，分析更新为：\n$$x_1^{a,(n)} = x_1^{f,(n)} + \\frac{\\operatorname{Var}(x_1^f)}{\\operatorname{Var}(x_1^f) + r} \\left( y_1 + \\epsilon^{(n)} - x_1^{f,(n)} \\right)$$\n\n**步骤 4：平滑至时间 $t=0$**\n集合卡尔曼平滑器 (EnKS) 更新 $t=0$ 时的初始集合，使其与 $t=1$ 时的分析结果保持一致。这是通过将 $t=1$ 时的分析增量 $(x_1^{a,(n)} - x_1^{f,(n)})$ 回归到 $t=0$ 时的状态来实现的。该回归基于 $t=0$ 和 $t=1$ 时状态之间的交叉协方差。\n\n$t=0$ 时的平滑集合 $\\{x_0^{s,(n)}\\}_{n=1}^N$ 由下式给出：\n$$x_0^{s,(n)} = x_0^{(n)} + K_0 \\left( x_1^{a,(n)} - x_1^{f,(n)} \\right)$$\n\n-   **平滑增益 $K_0$**：此增益是从集合中计算出的最佳线性预测器，它将 $t=1$ 时的异常映射到 $t=0$ 时的异常。\n    $$K_0 = \\frac{\\operatorname{Cov}(x_0, x_1^f)}{\\operatorname{Var}(x_1^f)}$$\n    无偏样本交叉协方差为 $\\operatorname{Cov}(x_0, x_1^f) = \\frac{1}{N-1} \\sum_{n=1}^N \\left(x_0^{(n)} - \\bar{x}_0\\right)\\left(x_1^{f,(n)} - \\bar{x}_1^f\\right)$。\n    \n处理一种特殊情况：如果 $\\operatorname{Var}(x_1^f)$ 在数值上为零，则意味着预报集合没有离散度，无法执行更新。在这种情况下，平滑增益 $K_0$ 设置为 $0$。如果初始集合没有离散度（$P_0=0$），也会发生这种情况，这将导致 $\\operatorname{Cov}(x_0, x_1^f) = 0$，因此 $K_0 = 0$。\n\n**步骤 5：最终平滑均值**\n最终结果是 $t=0$ 时平滑集合的均值：\n$$\\bar{x}_0^{s} = \\frac{1}{N} \\sum_{n=1}^N x_0^{s,(n)}$$\n\n该算法将针对每个测试用例实现，使用指定的参数：$a=0.9$、$q=0.1$、$r=0.4$、$N=10$、$H=1$。为保证可复现性，将为随机数生成器设置种子。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a two-time Ensemble Kalman Smoother (EnKS) for a linear scalar system\n    and computes the smoothed ensemble mean at the initial time for several test cases.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    a = 0.9\n    q = 0.1\n    r = 0.4\n    N = 10\n    H = 1.0  # Observation operator\n\n    # Test cases from the problem statement\n    test_cases = [\n        (1.0, 1.0, 0.5),   # Case 1: (m0, P0, y1)\n        (0.0, 0.0, 0.0),   # Case 2\n        (-2.0, 4.0, 1.0),  # Case 3\n        (0.5, 0.5, 0.45),  # Case 4\n        (0.0, 1.0, 5.0),   # Case 5\n    ]\n\n    results = []\n    for i, (m0, P0, y1) in enumerate(test_cases):\n        # Set the random seed for reproducibility\n        seed = 1000 + i\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Ensemble Initialization at t=0\n        if P0 == 0.0:\n            x0_ensemble = np.full(N, m0)\n        else:\n            x0_ensemble = rng.normal(loc=m0, scale=np.sqrt(P0), size=N)\n\n        # Step 2: Forecast to t=1\n        w0_noise = rng.normal(loc=0.0, scale=np.sqrt(q), size=N)\n        x1_forecast_ensemble = a * x0_ensemble + w0_noise\n\n        # Step 3: Analysis at t=1 (Stochastic EnKF)\n        # Calculate sample variance of the forecast ensemble\n        var_x1_forecast = np.var(x1_forecast_ensemble, ddof=1)\n\n        # Calculate Kalman gain K1\n        # Denominator is Var(H*x_f) + r = Var(x_f) + r, since H=1\n        K1 = var_x1_forecast / (var_x1_forecast + r)\n        \n        # Perturb observations\n        obs_perturbations = rng.normal(loc=0.0, scale=np.sqrt(r), size=N)\n        perturbed_obs = y1 + obs_perturbations\n        \n        # Apply analysis update to each ensemble member\n        # Forecast observations are H * x1_forecast_ensemble = x1_forecast_ensemble\n        innovation = perturbed_obs - x1_forecast_ensemble\n        x1_analysis_ensemble = x1_forecast_ensemble + K1 * innovation\n        \n        # Step 4: Smoothing to t=0 (EnKS)\n        # Calculate smoother gain K0\n        # K0 = Cov(x0, x1_f) / Var(x1_f)\n        if np.isclose(var_x1_forecast, 0.0):\n            K0 = 0.0\n        else:\n            # np.cov returns a 2x2 matrix for 2 1D inputs\n            # [[Var(x0), Cov(x0, x1f)], [Cov(x0, x1f), Var(x1f)]]\n            cov_matrix = np.cov(x0_ensemble, x1_forecast_ensemble, ddof=1)\n            cov_x0_x1f = cov_matrix[0, 1]\n            K0 = cov_x0_x1f / var_x1_forecast\n\n        # Apply smoothing update\n        analysis_increment = x1_analysis_ensemble - x1_forecast_ensemble\n        x0_smoothed_ensemble = x0_ensemble + K0 * analysis_increment\n        \n        # Step 5: Compute Smoothed Mean\n        smoothed_mean_x0 = np.mean(x0_smoothed_ensemble)\n        \n        # Round result to 6 decimal places and store\n        results.append(np.round(smoothed_mean_x0, 6))\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3379469"}, {"introduction": "当系统具有强非线性时，标准的集合卡尔曼平滑器（EnKS）可能会表现不佳，这正是迭代式平滑器（IES）的用武之地。本练习聚焦于迭代式平滑器的核心——高斯-牛顿（Gauss-Newton）迭代。您将为一个非线性观测模型实现单步高斯-牛顿更新，学习如何通过在当前估计周围进行线性化来求解更新步长，从而逐步逼近后验分布的模 [@problem_id:3379443]。这个实践揭示了迭代法处理非线性问题的基本思想。", "problem": "在迭代集合平滑器 (IES) 框架内为非线性标量观测模型实现单次高斯-牛顿迭代。基本出发点必须是具有高斯先验和高斯数据误差模型的贝叶斯逆问题公式。具体来说，考虑一个参数 $x \\in \\mathbb{R}$，其先验分布为 $x \\sim \\mathcal{N}(0,1)$，观测算子为 $h(x) = x^2$。观测数据为 $y \\in \\mathbb{R}$，数据误差协方差为标量 $R  0$。根据高斯先验和高斯似然定义负对数后验目标函数 $J(x)$。通过在当前集合均值处对 $h(x)$ 进行线性化，并使用得到的近似梯度和海森矩阵计算一个单一更新量，该更新量统一应用于所有集合成员，从而实现一次高斯-牛顿迭代。你必须从先验分布 $x \\sim \\mathcal{N}(0,1)$ 中独立抽取一个大小为 $N$ 的集合，计算当前集合均值 $m$，基于在 $m$ 处的线性化执行一个高斯-牛顿步骤，通过将计算出的步长平移每个成员来更新集合，并报告更新后的集合均值 $m_{\\text{updated}}$。\n\n计算过程必须是自包含且可复现的。为保证可复现性，为每个测试用例使用指定的伪随机数生成器种子。程序必须通过以下步骤实现迭代集合平滑器 (IES) 的步骤：\n- 使用给定的种子从先验分布 $\\mathcal{N}(0,1)$ 中抽取 $N$ 个独立样本。\n- 计算当前集合均值 $m$。\n- 在 $m$ 处对 $h(x)$ 进行线性化，并从贝叶斯目标函数构建高斯-牛顿搜索方向，然后通过向每个成员添加相同的标量增量来更新所有集合成员。\n- 在这次单次迭代后报告更新后的集合均值 $m_{\\text{updated}}$。\n\n本问题不涉及物理单位。不使用角度。不使用百分比。目标输出为实数。\n\n测试套件：\n为以下参数集 $(y, R, N, \\text{seed})$ 提供结果：\n1. $(1.0, 0.1, 20, 0)$\n2. $(1.0, 1.0, 20, 1)$\n3. $(0.0, 0.1, 20, 2)$\n4. $(1.0, 0.001, 20, 3)$\n5. $(4.0, 0.1, 3, 4)$\n6. $(1.0, 0.1, 1, 5)$\n\n最终输出格式：\n你的程序应生成单行输出，其中包含这六个测试用例的更新后集合均值，格式为用方括号括起来的逗号分隔列表。每个数字必须四舍五入到六位小数。例如：“[0.123456,0.234567,0.345678,0.456789,0.567890,0.678901]”。", "solution": "本问题要求为一个非线性标量逆问题实现单步迭代集合平滑器（IES），其核心是高斯-牛顿（Gauss-Newton）迭代。我们将从贝叶斯定理出发，构建优化目标，并推导其更新步骤。\n\n**1. 贝叶斯逆问题框架**\n\n我们旨在根据观测 $y$ 来估计参数 $x$。根据贝叶斯定理，后验分布 $p(x|y)$ 正比于先验分布 $p(x)$ 和似然函数 $p(y|x)$ 的乘积：\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\n问题给定：\n- 先验分布: $x \\sim \\mathcal{N}(0, 1)$，即 $p(x) \\propto \\exp\\left(-\\frac{1}{2}x^2\\right)$。\n- 观测模型: $y = h(x) + v = x^2 + v$，其中观测误差 $v \\sim \\mathcal{N}(0, R)$。\n- 似然函数: $p(y|x) \\propto \\exp\\left(-\\frac{1}{2R}(y - h(x))^2\\right) = \\exp\\left(-\\frac{1}{2R}(y - x^2)^2\\right)$。\n\n**2. 负对数后验目标函数 $J(x)$**\n\n寻找后验分布的众数（mode）等价于最小化负对数后验概率。这个函数通常被称为代价函数或目标函数 $J(x)$：\n$$\nJ(x) = -\\log(p(x|y)) = \\text{const} - \\log(p(y|x)) - \\log(p(x))\n$$\n忽略常数项，我们得到：\n$$\nJ(x) = \\frac{1}{2R}(y - x^2)^2 + \\frac{1}{2}x^2\n$$\n\n**3. 高斯-牛顿方法**\n\n高斯-牛顿法是一种用于求解非线性最小二乘问题的迭代算法。对于目标函数 $J(x)$，更新步骤为：\n$$\nx_{k+1} = x_k - [H_{GN}(x_k)]^{-1} g(x_k)\n$$\n其中 $g(x_k)$ 是 $J(x)$ 在 $x_k$ 处的梯度，而 $H_{GN}(x_k)$ 是 $J(x)$ 在 $x_k$ 处的海森矩阵的高斯-牛顿近似。\n\n- **梯度 $g(x) = \\nabla J(x)$**:\n  $$\n  g(x) = \\frac{dJ}{dx} = \\frac{1}{2R} \\cdot 2(y - x^2)(-2x) + x = x - \\frac{2x}{R}(y - x^2)\n  $$\n\n- **海森矩阵 $H(x) = \\nabla^2 J(x)$**:\n  $$\n  H(x) = \\frac{d^2J}{dx^2} = 1 - \\frac{2}{R}\\left[(y - x^2) + x(-2x)\\right] = 1 - \\frac{2(y - x^2)}{R} + \\frac{4x^2}{R}\n  $$\n\n- **高斯-牛顿海森矩阵近似 $H_{GN}(x)$**:\n  高斯-牛顿法通过忽略海森矩阵中涉及残差二阶导数的项来简化计算。在我们的例子中，残差是 $(y-x^2)$，所以我们忽略包含 $(y-x^2)$ 的项。因此，近似海森矩阵为：\n  $$\n  H_{GN}(x) = 1 + \\frac{4x^2}{R}\n  $$\n  这个近似等价于假设 $h(x)$ 在局部是线性的，它保证了海森矩阵是半正定的。\n\n**4. IES 迭代步骤**\n\n迭代集合平滑器（IES）将此优化思想应用于集合。对于单步更新：\n1.  从先验 $\\mathcal{N}(0,1)$ 中抽取一个集合 $\\{x_i\\}_{i=1}^N$。\n2.  计算当前集合的均值 $m = \\frac{1}{N}\\sum x_i$。\n3.  在集合均值 $m$ 处对问题进行线性化，即计算梯度 $g(m)$ 和高斯-牛顿海森矩阵 $H_{GN}(m)$。\n4.  计算更新步长 $\\delta m$：\n    $$\n    \\delta m = -[H_{GN}(m)]^{-1} g(m) = -\\frac{g(m)}{H_{GN}(m)} = -\\frac{m - \\frac{2m}{R}(y - m^2)}{1 + \\frac{4m^2}{R}}\n    $$\n5.  将此更新步长统一应用于所有集合成员：$x_i^{\\text{updated}} = x_i + \\delta m$。\n6.  更新后的集合均值为 $m_{\\text{updated}} = \\text{mean}(x_i^{\\text{updated}}) = \\text{mean}(x_i) + \\delta m = m + \\delta m$。\n\n该算法将为每个测试用例执行一次，使用给定的参数 $(y, R, N, \\text{seed})$ 计算 $m_{\\text{updated}}$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a single Gauss-Newton iteration for an Iterative Ensemble Smoother (IES)\n    to solve a nonlinear scalar inverse problem.\n    \"\"\"\n    \n    test_cases = [\n        (1.0, 0.1, 20, 0),\n        (1.0, 1.0, 20, 1),\n        (0.0, 0.1, 20, 2),\n        (1.0, 0.001, 20, 3),\n        (4.0, 0.1, 3, 4),\n        (1.0, 0.1, 1, 5)\n    ]\n\n    results = []\n    \n    for y, R, N, seed in test_cases:\n        # Step 1: Draw an ensemble from the prior distribution.\n        # The prior is x ~ N(0, 1).\n        rng = np.random.default_rng(seed)\n        ensemble_x = rng.standard_normal(N)\n\n        # Step 2: Compute the current ensemble mean.\n        m = np.mean(ensemble_x)\n\n        # Step 3: Compute the Gauss-Newton update step based on the ensemble mean.\n        # The update step delta_m = -[H_GN(m)]^-1 * g(m),\n        # where g is the gradient and H_GN is the Gauss-Newton approximation of the Hessian\n        # of the negative log-posterior objective function J(x).\n        \n        # Gradient of J(x) at m: g(m) = m - 2*m/R * (y - m^2)\n        gradient_at_m = m - (2.0 * m / R) * (y - m**2)\n\n        # Gauss-Newton approximation of the Hessian of J(x) at m: H_GN(m) = 1 + 4*m^2/R\n        hessian_approx_at_m = 1.0 + (4.0 * m**2) / R\n        \n        # Check for division by zero, although it is mathematically impossible here\n        # since R > 0 implies hessian_approx_at_m >= 1.\n        if hessian_approx_at_m == 0:\n            # This case should not be reached. If it is, no update is performed.\n            delta_m = 0.0\n        else:\n            delta_m = -gradient_at_m / hessian_approx_at_m\n\n        # Step 4: Compute the updated ensemble mean.\n        # A uniform update is applied to all ensemble members: x_i_new = x_i + delta_m.\n        # The new mean is therefore m_updated = mean(x_i + delta_m) = mean(x_i) + delta_m = m + delta_m.\n        m_updated = m + delta_m\n        \n        results.append(m_updated)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3379443"}, {"introduction": "高效的迭代平滑器不仅需要正确的算法结构，还需要精心的方案设计。本练习将带您探索一种流行的迭代平滑器——多重数据同化集合平滑器（ES-MDA）中的一个高级概念：退火（tempering）。您将推导退火与观测误差协方差膨胀之间的等价关系，并在此基础上，为一个理想化模型设计一个最优的退火序列，以平衡迭代过程中的线性化误差和采样误差 [@problem_id:3379440]。这不仅是一次计算，更是对算法设计背后权衡思想的深刻体验。", "problem": "考虑一个贝叶斯逆问题，其状态向量为 $x \\in \\mathbb{R}^{n}$，先验密度为 $p(x) \\propto \\exp\\!\\big(-\\tfrac{1}{2}(x-m^{b})^{\\top}(C^{b})^{-1}(x-m^{b})\\big)$，均值为 $m^{b} \\in \\mathbb{R}^{n}$，协方差为 $C^{b} \\in \\mathbb{R}^{n \\times n}$。观测模型为 $y = h(x) + \\xi$，其中 $h:\\mathbb{R}^{n} \\to \\mathbb{R}^{p}$ 可能为非线性函数，观测误差 $\\xi \\sim \\mathcal{N}(0,R)$，其中 $R \\in \\mathbb{R}^{p \\times p}$ 为正定矩阵，已实现的观测数据为 $y^{\\mathrm{obs}} \\in \\mathbb{R}^{p}$。在迭代集合平滑器中，似然退火将似然函数提升至一个幂次 $\\beta \\in (0,1]$。请基于以下基本依据进行推导：高斯似然 $p(y^{\\mathrm{obs}} \\mid x) \\propto \\exp\\!\\big(-\\tfrac{1}{2}(y^{\\mathrm{obs}} - h(x))^{\\top}R^{-1}(y^{\\mathrm{obs}} - h(x))\\big)$，Bayes 定理，以及对于线性 $h$ 的卡尔曼分析会最小化一个由先验项和数据失配项组成的二次代价函数这一事实。\n\n任务 A：从 Bayes 定理和似然函数的高斯形式出发，推导为何通过因子 $\\beta \\in (0,1]$ 进行退火等价于在似然函数中将观测误差协方差 $R$ 替换为 $R/\\beta$。解释在多重数据同化集合平滑器 (ES-MDA) 中，这种等价性如何意味着在同化步骤 $m \\in \\{1,\\dots,M\\}$ 使用膨胀的协方差 $\\alpha_{m} R$ 实现了一个退火指数 $\\beta_{m} = 1/\\alpha_{m}$，并证明需要满足条件 $\\sum_{m=1}^{M} \\beta_{m} = 1$ 才能在 $M$ 步后恢复未经退火的后验。\n\n任务 B：假设一个包含 $M$ 个退火 ES-MDA 更新的序列，其退火指数为 $\\{\\beta_{m}\\}_{m=1}^{M}$，对应的协方差为 $\\{R/\\beta_{m}\\}_{m=1}^{M}$。将每一步 $m$ 的竞争误差建模如下：\n- 一个局部线性化（建模）误差贡献，其上界为 $c_{L}\\,\\beta_{m}^{2}$，其中 $c_{L}  0$ 是一个不依赖于 $m$ 的常数。\n- 一个随机集合更新中的采样误差贡献，其上界为 $c_{S}/(N\\,\\beta_{m})$，其中 $c_{S}  0$ 是一个不依赖于 $m$ 的常数，而 $N \\in \\mathbb{N}$ 是集合大小。\n\n在这些建模假设下，确定一个方案 $\\{\\beta_{m}\\}_{m=1}^{M}$，该方案能在约束条件 $\\beta_{m}  0$ (对所有 $m$) 和 $\\sum_{m=1}^{M} \\beta_{m} = 1$ 下，最小化总上界 $\\sum_{m=1}^{M}\\big(c_{L}\\,\\beta_{m}^{2} + c_{S}/(N\\,\\beta_{m})\\big)$。将你的方案表示为仅含 $M$ 的单个闭式解析表达式。说明你的答案在何种条件下有效，并简要论证为何它在这种理想化模型中平衡了线性化误差和采样误差。\n\n你的最终答案必须是仅含 $M$ 的单个闭式解析表达式。无需数值取整。不涉及物理单位。除了所要求的表达式，最终答案中不要包含任何额外的评论。", "solution": "该问题被验证为具有科学依据、良态且客观。它设定在贝叶斯数据同化的标准数学框架内，并提出了一个可解的约束优化问题。所有定义和条件都是自洽且一致的。\n\n### 任务 A：退火、协方差膨胀和 ES-MDA\n\n我们首先分析退火似然的结构及其与多重数据同化集合平滑器 (ES-MDA) 框架的联系。\n\n后验概率密度函数 $p(x \\mid y^{\\mathrm{obs}})$ 由 Bayes 定理给出：\n$$\np(x \\mid y^{\\mathrm{obs}}) \\propto p(y^{\\mathrm{obs}} \\mid x) \\, p(x)\n$$\n其中 $p(y^{\\mathrm{obs}} \\mid x)$ 是似然函数，$p(x)$ 是先验。问题指定了高斯先验和高斯似然。似然函数由下式给出：\n$$\np(y^{\\mathrm{obs}} \\mid x) \\propto \\exp\\left(-\\frac{1}{2}(y^{\\mathrm{obs}} - h(x))^{\\top}R^{-1}(y^{\\mathrm{obs}} - h(x))\\right)\n$$\n似然退火涉及将似然函数提升至一个幂次 $\\beta \\in (0,1]$。因此，一个退火后验正比于 $[p(y^{\\mathrm{obs}} \\mid x)]^{\\beta} p(x)$。我们来研究一下退火似然项：\n$$\n[p(y^{\\mathrm{obs}} \\mid x)]^{\\beta} \\propto \\left[\\exp\\left(-\\frac{1}{2}(y^{\\mathrm{obs}} - h(x))^{\\top}R^{-1}(y^{\\mathrm{obs}} - h(x))\\right)\\right]^{\\beta}\n$$\n利用性质 $(\\exp(a))^b = \\exp(ab)$，这变为：\n$$\n[p(y^{\\mathrm{obs}} \\mid x)]^{\\beta} \\propto \\exp\\left(-\\frac{\\beta}{2}(y^{\\mathrm{obs}} - h(x))^{\\top}R^{-1}(y^{\\mathrm{obs}} - h(x))\\right)\n$$\n为证明这等价于将观测误差协方差 $R$ 替换为一个新的协方差 $R_{\\text{new}}$，我们需要将指数写成 $-\\frac{1}{2}(\\dots)^{\\top}R_{\\text{new}}^{-1}(\\dots)$ 的形式。我们可以将缩放后的逆协方差重写为：\n$$\n\\beta R^{-1} = (R/\\beta)^{-1}\n$$\n这里我们使用了对于标量 $c$ 的矩阵恒等式 $(cA)^{-1} = c^{-1}A^{-1}$。将其代回退火似然的表达式中，得到：\n$$\n[p(y^{\\mathrm{obs}} \\mid x)]^{\\beta} \\propto \\exp\\left(-\\frac{1}{2}(y^{\\mathrm{obs}} - h(x))^{\\top}(R/\\beta)^{-1}(y^{\\mathrm{obs}} - h(x))\\right)\n$$\n该表达式具有高斯似然的精确函数形式，其中观测误差协方差被替换为 $R/\\beta$。这就完成了任务 A 的第一部分。\n\n对于第二部分，ES-MDA 执行 M 次卡尔曼类型的更新序列。在每个步骤 $m \\in \\{1, \\dots, M\\}$，更新是使用一个膨胀的观测误差协方差矩阵 $\\alpha_m R$ 来执行的，其中 $\\alpha_m \\geq 1$。基于刚才推导出的等价关系，使用协方差 $\\alpha_m R$ 等价于使用一个退火指数 $\\beta_m$，使得：\n$$\n\\alpha_m R = \\frac{R}{\\beta_m}\n$$\n由于 $R$ 是可逆的，我们可以在右侧乘以 $R^{-1}$ 以得到 $\\alpha_m I = \\beta_m^{-1} I$，这意味着 $\\beta_m = 1/\\alpha_m$。\n\n最后，我们说明为什么条件 $\\sum_{m=1}^{M} \\beta_{m} = 1$ 是必需的。ES-MDA 过程通过使用修改后的似然多次同化相同的数据 $y^{\\mathrm{obs}}$ 来序贯地更新状态。令步骤 $m=1$ 处的先验为 $p_0(x) = p(x)$。第一步之后的后验 $p_1(x)$ 是使用退火指数 $\\beta_1$ 得到的：\n$$\np_1(x) \\propto [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_1} p_0(x) = [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_1} p(x)\n$$\n这个后验 $p_1(x)$ 接着作为第二步的先验。第二步之后的后验 $p_2(x)$ 是：\n$$\np_2(x) \\propto [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_2} p_1(x) \\propto [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_2} \\left( [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_1} p(x) \\right) = [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_1+\\beta_2} p(x)\n$$\n通过归纳法，经过 M 个这样的步骤后，最终的后验分布 $p_M(x)$ 由下式给出：\n$$\np_M(x) \\propto [p(y^{\\mathrm{obs}} \\mid x)]^{\\sum_{m=1}^{M} \\beta_m} p(x)\n$$\n为了恢复真实的、未经退火的贝叶斯后验 $p(x \\mid y^{\\mathrm{obs}})$，似然上的总指数必须等于 $1$。因此，退火指数序列必须满足条件：\n$$\n\\sum_{m=1}^{M} \\beta_{m} = 1\n$$\n\n### 任务 B：最优退火方案\n\n我们的任务是找到能最小化误差总上界的退火指数序列 $\\{\\beta_m\\}_{m=1}^M$，同时满足约束条件。需要最小化的目标函数是：\n$$\nJ(\\beta_1, \\dots, \\beta_M) = \\sum_{m=1}^{M}\\left(c_{L}\\,\\beta_{m}^{2} + \\frac{c_{S}}{N\\,\\beta_{m}}\\right)\n$$\n最小化受制于约束条件 $\\beta_m  0$ (对所有 $m \\in \\{1, \\dots, M\\}$) 以及在任务 A 中推导的一致性条件：\n$$\n\\sum_{m=1}^{M} \\beta_{m} = 1\n$$\n这是一个约束优化问题。我们可以使用拉格朗日乘子法来解决它。拉格朗日函数 $\\mathcal{L}$ 定义为：\n$$\n\\mathcal{L}(\\beta_1, \\dots, \\beta_M, \\lambda) = \\sum_{m=1}^{M}\\left(c_{L}\\,\\beta_{m}^{2} + \\frac{c_{S}}{N\\,\\beta_{m}}\\right) - \\lambda \\left(\\sum_{m=1}^{M} \\beta_{m} - 1\\right)\n$$\n为求最小值，我们将 $\\mathcal{L}$ 对每个 $\\beta_k$ (对于 $k = 1, \\dots, M$) 的偏导数设为零：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\left(c_{L}\\,\\beta_{k}^{2} + \\frac{c_{S}}{N\\,\\beta_{k}}\\right) - \\lambda = 0\n$$\n$$\n2c_{L}\\,\\beta_{k} - \\frac{c_{S}}{N\\,\\beta_{k}^{2}} - \\lambda = 0\n$$\n这个方程必须对每一个 $k \\in \\{1, \\dots, M\\}$ 都成立。该方程的形式对所有 $\\beta_k$ 都是相同的。这意味着在最优点，所有的 $\\beta_k$ 都必须相等。我们用 $\\beta$ 表示这个共同的值。\n$$\n\\beta_1 = \\beta_2 = \\dots = \\beta_M = \\beta\n$$\n一个更严谨的论证来自于以下事实：目标函数是关于每个 $\\beta_m$ 的相同、严格凸函数（因为对于 $\\beta_m  0$，在给定 $c_L0, c_S0, N0$ 的情况下，二阶导数 $2c_L + 2c_S/(N\\beta_m^3)$ 严格为正）之和，并且约束条件相对于 $\\beta_m$ 的排列是对称的。对于这类对称问题，唯一的最小值必然出现在所有变量都相等的对称点上。\n\n将 $\\beta_m = \\beta$ 代入约束方程：\n$$\n\\sum_{m=1}^{M} \\beta = M\\beta = 1\n$$\n求解 $\\beta$，我们得到每个指数的最优值：\n$$\n\\beta = \\frac{1}{M}\n$$\n因此，最优方案是 $\\beta_m = 1/M$，对于所有 $m = 1, \\dots, M$。这个解与常数 $c_L$、$c_S$ 和 $N$ 无关，正如题目要求的那样。\n\n此答案在题目给出的条件下有效：$c_L  0$，$c_S  0$，$N \\in \\mathbb{N}$ 是一个正整数，并且 $M \\in \\mathbb{N}$ 也是一个正整数。因为 $M \\geq 1$，所以约束 $\\beta_m  0$ 得以满足。\n\n这个恒定方案 $\\beta_m = 1/M$ 在以下意义上平衡了两种竞争的误差源。最优性条件要求总误差相对于任何 $\\beta_k$ 的边际变化，即 $2c_{L}\\,\\beta_{k} - c_{S}/(N\\,\\beta_{k}^{2})$，对所有 $k$ 都必须相同。通过将所有 $\\beta_k$ 设置为相同的值 $1/M$，此条件得到满足。这意味着在最优点，不可能通过稍微增加一个 $\\beta_k$ 同时减少另一个 $\\beta_j$ 以维持总和约束的方式来降低总误差。该解使得分配退火“预算”到任何特定步骤的边际成本（或收益）均等，从而实现了对总误差上界的最优平衡。", "answer": "$$\\boxed{\\frac{1}{M}}$$", "id": "3379440"}]}