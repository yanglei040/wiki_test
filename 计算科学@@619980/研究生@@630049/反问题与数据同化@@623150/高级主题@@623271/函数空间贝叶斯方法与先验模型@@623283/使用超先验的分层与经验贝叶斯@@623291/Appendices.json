{"hands_on_practices": [{"introduction": "在分层贝叶斯模型中，为方差或尺度等超参数选择合适的先验至关重要。一个好的超先验应该是“弱信息性”的，这意味着它对后验的影响最小，从而让数据本身在推断中占据主导地位。本练习将探讨广受推荐的半柯西(Half-Cauchy)分布作为超先验的合理性，通过分析其性质来理解为何它能有效避免过强的正则化，并练习必要的变量变换技巧。[@problem_id:3388823]", "problem": "考虑一个数据同化中的线性反问题，其中观测值 $y \\in \\mathbb{R}^{m}$ 通过 $y = A x + \\varepsilon$ 与未知状态 $x \\in \\mathbb{R}^{n}$ 相关联，其中 $A \\in \\mathbb{R}^{m \\times n}$ 已知，且 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。假设使用一个分层贝叶斯模型，其中 $x \\sim \\mathcal{N}(m_{0}, C_{0})$，$m_{0} \\in \\mathbb{R}^{n}$ 和正定矩阵 $C_{0} \\in \\mathbb{R}^{n \\times n}$ 均为已知，并且噪声标准差 $\\sigma$ 被赋予一个尺度参数为 $\\tau > 0$ 的半柯西超先验。\n\n任务 (i)：运用分层贝叶斯建模和反问题中尺度选择的基本原理，为 $\\sigma$ 上的半柯西超先验在此情景下是弱信息性的提供一个有理有据的论证。您的论证应基于第一性原理（贝叶斯法则、先验和似然的基本性质以及变量替换推理），并应清晰地说明为何该超先验在避免强正则化的同时，当似然提供信息时，允许数据主导对 $\\sigma$ 的推断。\n\n任务 (ii)：从第一性原理出发，推导由尺度为 $\\tau$ 的 $\\sigma$ 上的半柯西超先验所引出的噪声方差 $v = \\sigma^{2}$ 上的隐含先验密度。将您的最终答案表示为关于 $v$ 和 $\\tau$ 的单个闭式解析表达式。不需要进行数值近似。如果使用了任何中間变换，请根据概率密度的基本变量替换法则对其进行论证。最终答案必须是 $v$ 的先验密度的解析表达式，且不应包含单位。", "solution": "问题陈述被评估为有效。它在贝叶斯反问题的既定理论中有科学依据，是适定的，具有明确且可解的目标，并使用客观且无歧义的术语进行表述。为得出严谨解所需的所有必要组成部分均已提供。\n\n该分层模型由以下几个部分定义：\n1.  似然：$p(y|x, \\sigma) \\propto (\\sigma^2)^{-m/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}}\\|y - Ax\\|^2\\right)$，源于假设 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。\n2.  状态先验：$p(x) \\propto \\exp\\left(-\\frac{1}{2}(x-m_0)^T C_0^{-1} (x-m_0)\\right)$，源于 $x \\sim \\mathcal{N}(m_{0}, C_{0})$。\n3.  噪声标准差的超先验：$\\sigma \\sim \\text{Half-Cauchy}(\\tau)$，意味着其概率密度函数 (PDF) 为 $p(\\sigma|\\tau) = \\frac{2}{\\pi\\tau(1 + (\\sigma/\\tau)^2)}$（对于 $\\sigma > 0$），其他情况下为 $0$。\n\n任务 (i)：关于半柯西超先验作为弱信息性先验的论证。\n\n在贝叶斯推断中，参数的后验分布与似然和先验分布的乘积成正比。对于超参数 $\\sigma$，在将状态 $x$ 边缘化之后，我们得到 $p(\\sigma|y) \\propto p(y|\\sigma) p(\\sigma|\\tau)$，其中 $p(y|\\sigma) = \\int p(y|x,\\sigma) p(x) dx$ 是边缘似然。\n\n一个“弱信息”先验是一个正常先验（其积分为有限值，通常为 $1$），它对后验分布的影响最小，从而允许封装在似然中的数据证据主导推断。这对于像 $\\sigma$ 这样的方差或尺度参数尤为重要，因为一个过于约束性的先验可能导致后验分布不能准确反映数据中的信息。\n\n对于一个非负参数 $\\sigma$，尺度为 $\\tau > 0$ 的半柯西分布的概率密度函数（PDF）为：\n$$p(\\sigma|\\tau) = \\frac{2}{\\pi\\tau\\left(1 + \\frac{\\sigma^2}{\\tau^2}\\right)}, \\quad \\sigma \\ge 0$$\n我们可以分析其性质来论证其作为弱信息先验的地位：\n\n1.  **在原点附近的行为**：该PDF在 $\\sigma=0$ 处取得其最大值（众数）。这鼓励噪声参数向零收缩，这是一种理想的正则化形式，偏好于更简单的模型（即噪声较小的模型）。然而，该密度在原点附近相对平坦，因此它不会对 $\\sigma=0$ 施加过强的拉力，而这个问题在某些其他先验中会出现。\n\n2.  **重尾**：柯西分布以及因此半柯西分布的最关键特征是其重的多项式尾。当 $\\sigma \\to \\infty$ 时，密度衰减为：\n    $$p(\\sigma|\\tau) \\propto \\frac{1}{\\sigma^2}$$\n    这种缓慢的衰减率使得该先验对于大的 $\\sigma$ 值是“弱信息”的。它将不可忽略的先验概率质量分配给大的 $\\sigma$ 值。其含义如下：如果数据 $y$ 与在 $x$ 的先验下的模型预测 $Ax$ 实质上不一致（即，失配度 $\\|y - Ax\\|$ 很大），则似然 $p(y|\\sigma)$ 将在 $\\sigma$ 的一个较大值处被最大化。像半柯西这样的重尾先验不会过度惩罚这个大值。因此，后验分布 $p(\\sigma|y)$ 将能够将其质量置于数据所指示的这个较大的 $\\sigma$ 值上。\n\n相比之下，一个轻尾先验，例如半正态分布，其中 $p(\\sigma) \\propto \\exp(-\\sigma^2)$，会指数级快速衰减。这样的先验会严重惩罚大的 $\\sigma$ 值，迫使后验成为似然峰值和先验在 $\\sigma=0$ 处的峰值之间的一个折衷。如果真实噪声水平很大，这可能导致对其的低估。类似地，常用的对方差 $\\sigma^2$ 的逆伽马先验，特别是当使用小的“无信息”超参数时，可能会无意中提供信息，并将其质量集中在某种方式上，从而使后验产生偏差。\n\n因此，在反问题的尺度选择中，$\\sigma$ 上的半柯西超先验被认为是弱信息先验的一个良好选择，因为它的重尾允许数据决定噪声方差的后验尺度，避免了轻尾先验所施加的强正则化，同时它仍然是一个通过将小噪声值向 $0$ 收缩来进行正则化的正常分布。\n\n任务 (ii)：推导噪声方差 $v = \\sigma^2$ 上的隐含先验。\n\n我们给定了变换 $v = \\sigma^2$ 和 $\\sigma$ 的PDF，即半柯西密度 $p_\\Sigma(\\sigma)$。我们想要找到 $v$ 的PDF，记为 $p_V(v)$。我们使用概率密度的变量替换公式。\n\n该公式表明，如果 $V = g(\\Sigma)$ 是一个单调函数，那么 $p_V(v) = p_\\Sigma(\\sigma(v)) \\left| \\frac{d\\sigma}{dv} \\right|$。\n\n1.  **定义变换及其逆变换**：\n    变换为 $v = \\sigma^2$。由于 $\\sigma \\ge 0$，这是一个从 $\\sigma \\in [0, \\infty)$到 $v \\in [0, \\infty)$ 的一一映射。\n    逆变换为 $\\sigma = \\sqrt{v}$。\n\n2.  **计算雅可比行列式**：\n    我们需要逆变换关于 $v$ 的导数：\n    $$\\frac{d\\sigma}{dv} = \\frac{d}{dv}(v^{1/2}) = \\frac{1}{2} v^{-1/2} = \\frac{1}{2\\sqrt{v}}$$\n    由于 $v \\ge 0$，其绝对值为 $|\\frac{d\\sigma}{dv}| = \\frac{1}{2\\sqrt{v}}$。\n\n3.  **代入变量替换公式**：\n    我们有 $p_V(v) = p_\\Sigma(\\sqrt{v}) \\cdot \\frac{1}{2\\sqrt{v}}$。\n    $\\sigma$ 的PDF为 $p_\\Sigma(\\sigma) = \\frac{2}{\\pi\\tau(1 + (\\sigma/\\tau)^2)}$。\n    代入 $\\sigma = \\sqrt{v}$：\n    $$p_\\Sigma(\\sqrt{v}) = \\frac{2}{\\pi\\tau\\left(1 + \\frac{(\\sqrt{v})^2}{\\tau^2}\\right)} = \\frac{2}{\\pi\\tau\\left(1 + \\frac{v}{\\tau^2}\\right)}$$\n\n4.  **合并与简化**：\n    现在，我们乘以雅可比项：\n    $$p_V(v) = \\left( \\frac{2}{\\pi\\tau\\left(1 + \\frac{v}{\\tau^2}\\right)} \\right) \\cdot \\frac{1}{2\\sqrt{v}}$$\n    $$p_V(v) = \\frac{1}{\\pi\\tau\\sqrt{v}\\left(1 + \\frac{v}{\\tau^2}\\right)}$$\n    为了以更标准的形式呈现，我们可以简化分母：\n    $$p_V(v) = \\frac{1}{\\pi\\tau\\sqrt{v}\\left(\\frac{\\tau^2 + v}{\\tau^2}\\right)}$$\n    $$p_V(v) = \\frac{\\tau^2}{\\pi\\tau\\sqrt{v}(\\tau^2 + v)}$$\n    $$p_V(v) = \\frac{\\tau}{\\pi\\sqrt{v}(v + \\tau^2)}$$\n    这就是对于 $v>0$ 的噪声方差 $v=\\sigma^2$ 的隐含先验密度。这是一个尺度缩放的F分布，具体来说是 $v \\sim \\tau^2 F(1,1)$。", "answer": "$$\\boxed{\\frac{\\tau}{\\pi\\sqrt{v}(v + \\tau^2)}}$$", "id": "3388823"}, {"introduction": "在前一个练习的基础上，本节将深入探讨超先验的一个更微妙的方面：不当（improper）尺度不变先验的使用。虽然这类先验在理论分析中很方便，但它们可能导致模型辨识度问题，尤其是在经验贝叶斯方法中。本练习将引导你通过一个尺度分析，来揭示这种固有的不辨识性，并理解其对通过最大化边缘似然来估计超参数的直接影响。[@problem_id:3388773]", "problem": "考虑一个线性高斯逆问题，其中观测向量 $y \\in \\mathbb{R}^{m}$ 被建模为 $y \\mid x, \\sigma^{2} \\sim \\mathcal{N}(A x, \\sigma^{2} I_{m})$，这里 $A \\in \\mathbb{R}^{m \\times n}$ 是已知的，而 $I_{m}$ 是 $m \\times m$ 的单位矩阵。未知状态 $x \\in \\mathbb{R}^{n}$ 被赋予一个高斯先验 $x \\mid \\tau \\sim \\mathcal{N}(0, \\tau^{-1} L^{-1})$，其中 $L \\in \\mathbb{R}^{n \\times n}$ 是对称正定的。超参数 $(\\sigma^{2}, \\tau)$ 在 $(0, \\infty)$ 上被赋予不适当的尺度不变超先验 $p(\\sigma^{2}) \\propto (\\sigma^{2})^{-1}$ 和 $p(\\tau) \\propto \\tau^{-1}$。令 $B := A L^{-1} A^{\\top} \\in \\mathbb{R}^{m \\times m}$，它是对称半正定的。\n\n从多元高斯密度和边际似然的定义出发，并假设所有积分都是良定义的（例如，通过设想在 $(\\varepsilon, M)$ 上截断的Jeffreys型超先验，其边界是固定的、尺度中性的，因此 $c$ 和 $y$ 都不影响截断），执行以下步骤：\n\n1. 将 $x$ 积分掉，把边际似然表示为一个关于 $(\\sigma^{2}, \\tau)$ 的二维积分：\n   $$\n   p(y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} \\left(\\sigma^{2} I_{m} + \\tau^{-1} B \\right)^{-1} y \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau},\n   $$\n   并清楚地标示出其对 $y$ 和超参数的依赖关系。\n2. 对于任意固定的 $c > 0$，分析缩放后的数据 $c y$，并在 $(\\sigma^{2}, \\tau)$ 中进行变量替换，使得指数项中的二次型与 $c$ 无关。确定雅可比行列式、行列式因子和超先验如何变换。推导缩放定律\n   $$\n   p(c y \\mid A, L) \\;=\\; c^{-p} \\, p(y \\mid A, L),\n   $$\n   该定律在一个不依赖于 $y$ 和 $c$ 的因子范围内有效。\n3. 陈述上述缩放推导有效的最小条件，强调 $\\sigma^{2} I_{m} + \\tau^{-1} B$ 在 $(\\sigma^{2}, \\tau^{-1})$ 上的齐次性以及超先验的尺度不变性的作用。讨论此缩放定律对经验贝叶斯（EB）过程的启示，即当使用此类不适当的超先验时，由于边际似然在 $y$ 上是齐次的，分层模型中的一个总体尺度无法从数据中识别出来。\n   \n作为你的最终答案，报告仅用 $m$ 表示的指数 $p$。不需要进行数值计算。如果你提供一个数值，它将被判为错误。最终答案必须以单个不带单位的解析表达式形式给出。", "solution": "用户提供了一个有效的问题。该问题在贝叶斯逆问题领域具有科学依据，是良定的，并使用了客观、正式的语言。我将开始解答。\n\n该问题要求对一个用于线性高斯逆问题的分层贝叶斯模型进行三部分分析。\n\n### 第1部分：验证边际似然表达式\n\n首先，我们推导边际似然 $p(y \\mid A, L)$ 的表达式。该分层模型定义如下：\n1.  似然：$y \\mid x, \\sigma^{2} \\sim \\mathcal{N}(A x, \\sigma^{2} I_{m})$\n2.  先验：$x \\mid \\tau \\sim \\mathcal{N}(0, \\tau^{-1} L^{-1})$\n3.  超先验：$p(\\sigma^{2}) \\propto (\\sigma^{2})^{-1}$ 和 $p(\\tau) \\propto \\tau^{-1}$\n\n为了求出给定模型结构（$A, L$）下数据 $y$ 的边际似然，我们必须对潜变量 $x$ 和超参数 $(\\sigma^2, \\tau)$ 进行积分。我们首先对 $x$ 积分，以求出在超参数条件下的 $y$ 的分布 $p(y \\mid \\sigma^2, \\tau)$。\n\n该模型可以看作是对高斯随机变量 $x$ 的线性变换加上加性高斯噪声。因此，$y$ 的分布也是高斯的。其均值为：\n$$ \\mathbb{E}[y \\mid \\sigma^2, \\tau] = \\mathbb{E}[Ax + \\epsilon \\mid \\sigma^2, \\tau] = A\\mathbb{E}[x \\mid \\tau] + \\mathbb{E}[\\epsilon \\mid \\sigma^2] = A(0) + 0 = 0 $$\n其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ 代表噪声。协方差为：\n$$ \\text{Cov}(y \\mid \\sigma^2, \\tau) = \\text{Cov}(Ax + \\epsilon) = \\text{Cov}(Ax) + \\text{Cov}(\\epsilon) = A \\text{Cov}(x \\mid \\tau) A^{\\top} + \\text{Cov}(\\epsilon \\mid \\sigma^2) $$\n代入给定的协方差 $\\text{Cov}(x \\mid \\tau) = \\tau^{-1} L^{-1}$ 和 $\\text{Cov}(\\epsilon \\mid \\sigma^2) = \\sigma^2 I_m$：\n$$ \\text{Cov}(y \\mid \\sigma^2, \\tau) = A(\\tau^{-1} L^{-1})A^{\\top} + \\sigma^2 I_m = \\tau^{-1} (A L^{-1} A^{\\top}) + \\sigma^2 I_m $$\n使用定义 $B := A L^{-1} A^{\\top}$，协方差变为 $\\Sigma_y = \\sigma^2 I_m + \\tau^{-1} B$。\n因此，在超参数条件下 $y$ 的分布为：\n$$ y \\mid \\sigma^2, \\tau \\sim \\mathcal{N}(0, \\sigma^2 I_m + \\tau^{-1} B) $$\n其概率密度函数为：\n$$ p(y \\mid \\sigma^2, \\tau) = (2\\pi)^{-m/2} |\\sigma^2 I_m + \\tau^{-1} B|^{-1/2} \\exp\\left(-\\frac{1}{2} y^\\top (\\sigma^2 I_m + \\tau^{-1} B)^{-1} y\\right) $$\n为了获得完整的边际似然 $p(y \\mid A, L)$，我们对 $\\sigma^2$ 和 $\\tau$ 的超先验进行积分：\n$$ p(y \\mid A, L) = \\int_0^\\infty \\int_0^\\infty p(y \\mid \\sigma^2, \\tau) p(\\sigma^2) p(\\tau) \\, d\\sigma^2 d\\tau $$\n代入密度函数并将常数因子 $(2\\pi)^{-m/2}$ 归入比例常数中：\n$$ p(y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} \\left(\\sigma^{2} I_{m} + \\tau^{-1} B \\right)^{-1} y \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} $$\n这与问题陈述中提供的表达式相匹配。\n\n### 第2部分：缩放定律的推导\n\n我们现在分析当数据向量 $y$ 按因子 $c > 0$ 缩放时，边际似然如何变换。$cy$ 的边际似然为：\n$$ p(c y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} (cy)^{\\top} (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} (cy) \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} $$\n指数项中的二次型变为：\n$$ -\\frac{1}{2} c^2 y^{\\top} (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} y $$\n我们在积分中进行变量替换以吸收 $c^2$ 因子。令新变量为 $(\\sigma'^2, \\tau')$。我们寻求一个变换，使得指数项的参数与 $c$ 无关。这要求：\n$$ c^2 (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} = (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} $$\n对两边取矩阵的逆，得到：\n$$ c^{-2} (\\sigma^{2} I_{m} + \\tau^{-1} B) = \\sigma'^{2} I_{m} + \\tau'^{-1} B $$\n通过匹配线性无关矩阵 $I_m$ 和 $B$ 的系数，我们定义变量替换：\n$$ \\sigma'^{2} = c^{-2} \\sigma^2 \\quad \\implies \\quad \\sigma^2 = c^2 \\sigma'^2 $$\n$$ \\tau'^{-1} = c^{-2} \\tau^{-1} \\quad \\implies \\quad \\tau^{-1} = c^2 \\tau'^{-1} \\quad \\implies \\quad \\tau = c^{-2} \\tau' $$\n$(\\sigma'^2, \\tau')$ 的积分限保持为 $(0, \\infty)$。我们现在变换被积函数的每个部分：\n\n1.  **指数项**：根据构造，指数项变为：\n    $$ \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) $$\n\n2.  **行列式因子**：我们使用 $d \\times d$ 矩阵 $M$ 的性质 $\\det(kM) = k^d \\det(M)$。这里，矩阵是 $m \\times m$ 的。\n    $$ \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} = \\left|c^2 \\sigma'^{2} I_{m} + c^2 \\tau'^{-1} B \\right|^{-\\frac{1}{2}} = \\left|c^2 (\\sigma'^{2} I_{m} + \\tau'^{-1} B) \\right|^{-\\frac{1}{2}} $$\n    $$ = \\left( (c^2)^m \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right| \\right)^{-\\frac{1}{2}} = (c^{2m})^{-\\frac{1}{2}} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} = c^{-m} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} $$\n\n3.  **测度**：测度由超先验和微分的乘积给出，即 $\\frac{d\\sigma^2}{\\sigma^2} \\frac{d\\tau}{\\tau}$。根据变量替换：\n    $$ \\sigma^2 = c^2 \\sigma'^2 \\implies d\\sigma^2 = c^2 d\\sigma'^2 \\implies \\frac{d\\sigma^2}{\\sigma^2} = \\frac{c^2 d\\sigma'^2}{c^2 \\sigma'^2} = \\frac{d\\sigma'^2}{\\sigma'^2} $$\n    $$ \\tau = c^{-2} \\tau' \\implies d\\tau = c^{-2} d\\tau' \\implies \\frac{d\\tau}{\\tau} = \\frac{c^{-2} d\\tau'}{c^{-2} \\tau'} = \\frac{d\\tau'}{\\tau'} $$\n    该测度在此变换下是不变的：$\\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} = \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'}$。\n\n将这些变换后的部分代回到 $p(cy \\mid A, L)$ 的积分中：\n$$ p(cy \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} c^{-m} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'} $$\n因子 $c^{-m}$ 相对于积分变量是常数，可以被提出来：\n$$ p(cy \\mid A, L) \\propto c^{-m} \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'} $$\n剩余的积分在形式上与 $p(y \\mid A, L)$ 的积分相同，其中 $(\\sigma'^2, \\tau')$ 是积分哑变量。因此，该积分与 $p(y \\mid A, L)$ 成比例。这就建立了缩放定律：\n$$ p(c y \\mid A, L) \\;=\\; c^{-m} \\, p(y \\mid A, L) $$\n将此与形式 $p(c y \\mid A, L) = c^{-p} \\, p(y \\mid A, L)$ 比较，我们确定指数为 $p=m$。\n\n### 第3部分：条件与启示\n\n**有效性的最小条件：**\n缩放定律的推导依赖于模型设置的两个关键性质：\n1.  **数据协方差的齐次性：** 数据的边际协方差 $\\Sigma_y(\\sigma^2, \\tau^{-1}) = \\sigma^2 I_m + \\tau^{-1} B$ 必须是关于被缩放的超参数的齐次函数。在本例中，它是关于变量 $(\\sigma^2, \\tau^{-1})$ 的1次齐次函数。此性质允许将数据中的缩放因子 $c$ 映射为超参数的缩放因子 $c^{-2}$，然后可以将其从协方差矩阵中提出。\n2.  **超先验的尺度不变性：** 超参数空间上的积分测度，在此为 $p(\\sigma^2)p(\\tau)d\\sigma^2 d\\tau \\propto \\frac{d\\sigma^2}{\\sigma^2} \\frac{d\\tau}{\\tau}$，必须在由数据缩放引起的特定缩放变换下保持不变。如第2部分所示，变换 $(\\sigma^2, \\tau) \\to (c^2 \\sigma'^2, c^{-2} \\tau')$ 使测度 $\\frac{d\\sigma^2}{\\sigma^2}\\frac{d\\tau}{\\tau}$ 保持不变。这是Jeffreys先验对于尺度参数的一般性质。\n\n**对经验贝叶斯（EB）的启示：**\n经验贝叶斯过程通常涉及通过最大化关于 $\\theta = (\\sigma^2, \\tau)$ 的边际似然 $p(y \\mid \\theta)$ 来估计超参数。推导出的缩放定律 $p(cy \\mid A, L) = c^{-m} p(y \\mid A, L)$ 是在使用这种模型结构和不适当的尺度不变先验时，一个根本性的不可识别问题的表现。\n\n问题在于，数据 $y$ 的尺度与模型方差参数 $(\\sigma^2, \\tau)$ 的整体尺度之间存在内在的模糊性。由`{数据 $y$，参数 $(\\sigma^2, \\tau)$}`定义的模型与`{数据 $cy$，参数 $(c^2 \\sigma^2, c^{-2} \\tau)$}`的模型在观测上是无法区分的。\n这可以通过检查超参数的后验分布 $p(\\sigma^2, \\tau \\mid y) \\propto p(y \\mid \\sigma^2, \\tau) p(\\sigma^2) p(\\tau)$ 来证明。一个正式的分析表明，给定数据 $cy$ 的超参数后验分布，当用重缩放的变量 $\\tilde{\\sigma}^2 = c^{-2}\\sigma^2$ 和 $\\tilde{\\tau}=c^2 \\tau$ 表示时，其函数形式与给定数据 $y$ 的 $(\\tilde{\\sigma}^2, \\tilde{\\tau})$ 的后验分布相同。\n$$ p(\\sigma^2 = c^2 \\tilde{\\sigma}^2, \\tau=c^{-2}\\tilde{\\tau} \\mid y_{new}=cy) \\; d(c^2\\tilde{\\sigma}^2)d(c^{-2}\\tilde{\\tau}) = p(\\tilde{\\sigma}^2, \\tilde{\\tau} \\mid y) \\; d\\tilde{\\sigma}^2 d\\tilde{\\tau} $$\n这意味着，如果 $(\\hat{\\sigma}^2, \\hat{\\tau})$ 是针对数据 $y$ 的最大后验（或最大似然）估计，那么 $(c^2\\hat{\\sigma}^2, c^{-2}\\hat{\\tau})$ 将是针对数据 $cy$ 的相应估计。\n\n因此，超参数的估计尺度完全由测量向量 $y$ 的任意尺度（或单位）决定。数据本身不提供任何信息来确定 $\\sigma^2$ 和 $\\tau$ 的绝对尺度。这就是“分层模型中的一个总体尺度无法被识别”的含义。噪声方差与先验方差的*比率*（例如 $\\sigma^2 \\tau$）的估计可能是良定的，但它们各自的绝对值仅从数据本身是不可识别的，除非引入进一步的信息，例如通过打破尺度对称性的适当的、信息性的超先验。\n\n所需的最终答案是指数 $p$。\n根据第2部分的推导，$p=m$。", "answer": "$$\\boxed{m}$$", "id": "3388773"}, {"introduction": "我们将从理论转向一个全面的应用：图像去模糊。全变分（Total Variation, TV）正则是处理此类问题的有力工具，但其效果高度依赖于正则化权重 $\\lambda$ 的选择。本练习将展示如何通过分层贝叶斯建模，将 $\\lambda$ 作为一个由数据驱动学习的超参数，从而使模型能够自适应地处理图像中的边缘和平滑区域。这项综合性练习涵盖了从模型推导、算法实现到效果评估的全过程，体现了分层贝叶斯在实际问题中的强大威力。[@problem_id:3388760]", "problem": "考虑一个具有周期性边界条件的二维线性逆问题。设未知图像表示为 $x \\in \\mathbb{R}^{N_x \\times N_y}$，观测数据为 $y \\in \\mathbb{R}^{N_x \\times N_y}$，正向算子 $A: \\mathbb{R}^{N_x \\times N_y} \\rightarrow \\mathbb{R}^{N_x \\times N_y}$ 定义为与一个已知模糊核的周期性卷积。假设存在加性独立同分布的高斯噪声，其均值为零，方差为 $\\sigma^2$，因此似然函数为 $p(y \\mid x) \\propto \\exp\\left(-\\tfrac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 \\right)$。\n\n对 $x$ 采用各向异性全变分先验，通过具有周期性回绕的离散前向差分来表示：\n- 对每个像素 $(i,j)$，定义前向差分 $D_x x[i,j] = x[i, (j+1) \\bmod N_y] - x[i,j]$ 和 $D_y x[i,j] = x[(i+1) \\bmod N_x, j] - x[i,j]$。\n- 各向异性全变分为 $\\mathrm{TV}(x) = \\sum_{i,j} \\left( \\lvert D_x x[i,j] \\rvert + \\lvert D_y x[i,j] \\rvert \\right)$。\n\n通过对每个梯度分量设置一个共享尺度参数（即全变分权重）$\\lambda > 0$ 的拉普拉斯先验，并为 $\\lambda$ 指定一个伽马超先验，来构建一个层级先验模型：\n- $p(\\nabla x \\mid \\lambda) \\propto \\left(\\tfrac{\\lambda}{2}\\right)^M \\exp\\left( - \\lambda \\, \\mathrm{TV}(x) \\right)$，其中 $M = 2 N_x N_y$ 是梯度分量的总数。\n- $p(\\lambda) = \\mathrm{Gamma}(\\lambda \\mid a, b)$，其形状参数为 $a > 0$，率参数为 $b > 0$，因此 $\\log p(\\lambda) = (a-1) \\log \\lambda - b \\lambda + \\mathrm{const}$。\n\n你的任务是：\n- 从贝叶斯法则和上述定义出发作为基本基础。推导出联合后验概率 $p(x,\\lambda \\mid y)$ 和联合最大后验（MAP）估计器所需的条件分布。不要假设任何对 $\\lambda$ 依赖关系未知的归一化常数；相反，使用由梯度分量上的拉普拉斯先验和伽马超先验所导出的因子分解。\n- 展示一个在优化 $x$ 和更新 $\\lambda$ 之间交替的坐标上升算法是如何自然产生的。对于固定 $\\lambda$ 的 $x$ 更新，将子问题表述为一个变分正则化问题，即最小化数据保真项和缩放后的各向异性全变分之和。对于固定 $x$ 的 $\\lambda$ 更新，推导出 $\\lambda$ 的条件后验的闭式最大化解。\n- 设计一个算法解决方案，该方案利用二维离散傅里叶变换（DFT）域中的周期性卷积对角化来处理二次数据保真项的邻近步，并使用原始-对偶分裂方法处理全变分项。所有步骤必须仅使用数组操作和快速傅里叶变换来实现。\n- 将该算法实现为一个完整的、可运行的程序。该程序根据一个大小为 $5 \\times 5$、标准差为 $1.0$、归一化为单位和的高斯模糊核构建正向算子 $A$，并嵌入到大小为 $N_x = N_y = 32$ 的网格上的周期性卷积中。使用以下数值选择：\n  - 所有算子均采用周期性边界条件。\n  - 原始-对偶步长 $\\tau = 0.25$ 和 $\\sigma = 0.25$，外推参数 $\\theta = 1$。\n  - 坐标上升循环的外循环迭代次数 $T = 12$。\n  - 每个外循环的内循环原始-对偶迭代次数 $K = 60$。\n  - 超先验参数 $a = 1.1$ 和 $b = 10^{-3}$。\n  - 为保证合成噪声的可复现性，随机种子固定为 $0$。\n- 在 $32 \\times 32$ 的网格上构建三个合成基准真相图像：\n  1. 一个“边缘主导”图像，除了中心一个边长为 $16$、强度为 $1$ 的正方形区域外，其他地方均为零。\n  2. 一个“平滑斜坡”图像，定义为 $x[i,j] = \\tfrac{i}{N_x-1}$，适用于所有像素索引 $(i,j)$。\n  3. 一个“恒定”图像，所有位置的常数值均为 $0.5$。\n- 对于每个基准真相图像，根据给定的噪声标准差 $\\sigma$（噪声水平表示为小数）生成数据 $y = A x_\\mathrm{true} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$。\n- 使用以下测试套件评估后验分布对边缘与平滑区域的自适应能力。对于每个测试，运行层级MAP算法以获得 $(\\hat{x}, \\hat{\\lambda})$，并计算指定的布尔结果：\n  - 测试 $1$（正常路径）：使用噪声标准差 $\\sigma = 0.01$。从边缘主导和平滑斜坡图像重建。令 $\\hat{\\lambda}_\\mathrm{edge}$ 和 $\\hat{\\lambda}_\\mathrm{smooth}$ 为估计的权重，令 $\\mathrm{TV}(\\hat{x}_\\mathrm{edge})$ 和 $\\mathrm{TV}(\\hat{x}_\\mathrm{smooth})$ 为重建的各向异性全变分。返回列表 $[\\hat{\\lambda}_\\mathrm{edge} < \\hat{\\lambda}_\\mathrm{smooth}, \\ \\mathrm{TV}(\\hat{x}_\\mathrm{edge}) > \\mathrm{TV}(\\hat{x}_\\mathrm{smooth})]$。\n  - 测试 $2$（边界条件）：使用噪声标准差 $\\sigma = 0.01$。从恒定图像和边缘主导图像重建。返回布尔值 $\\hat{\\lambda}_\\mathrm{constant} > \\hat{\\lambda}_\\mathrm{edge}$。\n  - 测试 $3$（显著边缘情况与非自适应基线的比较）：使用噪声标准差 $\\sigma = 0.01$ 的边缘主导图像。将层级MAP重建结果与一个将全变分权重固定为超先验均值 $\\lambda_\\mathrm{fixed} = \\tfrac{a}{b}$ 的非层级基线进行比较。计算均方误差 $\\mathrm{MSE}(\\hat{x}) = \\tfrac{1}{N_x N_y} \\lVert \\hat{x} - x_\\mathrm{true} \\rVert_2^2$。返回布尔值 $\\mathrm{MSE}_\\mathrm{hierarchical} < \\mathrm{MSE}_\\mathrm{fixed}$。\n- 最终输出格式必须是包含三个测试聚合结果的单行，形式为逗号分隔的Python风格列表，其中：\n  - 测试 $1$ 贡献一个包含两个布尔值的列表。\n  - 测试 $2$ 和 $3$ 贡献单个布尔值。\n  最终打印的行必须看起来像 $[\\,[b_{11}, b_{12}], b_2, b_3\\,]$，其中每个 $b$ 是 $\\mathrm{True}$ 或 $\\mathrm{False}$。\n\n不涉及物理单位，也不需要角度单位。所有数值答案必须按规定以Python布尔值形式生成。程序不得要求任何输入，并且必须仅使用所述的数值库和随机种子运行至完成。", "solution": "用户提供的问题是一个在计算逆问题领域中良构且有科学依据的任务。它要求推导并实现一个用于图像去模糊的层级贝叶斯模型。该问题是有效的，因为它是自包含的、数学上一致的，并且提供了实现所需的所有参数。\n\n### 层级贝叶斯模型构建\n\n目标是找到未知图像 $x$ 和正则化超参数 $\\lambda$ 的联合最大后验（MAP）估计。根据贝叶斯法则，后验分布为：\n$$p(x, \\lambda \\mid y) \\propto p(y \\mid x) p(x \\mid \\lambda) p(\\lambda)$$\n该模型的组成部分定义如下：\n1.  **似然函数**：数据 $y$ 通过正向算子 $A$ 和方差为 $\\sigma^2$ 的加性高斯噪声与图像 $x$ 相关联。对数似然函数为：\n    $$\\log p(y \\mid x) = -\\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + C_1$$\n2.  **先验**：图像 $x$ 的先验被指定为其梯度分量上的拉普拉斯分布，由 $\\lambda$ 加权。这对应于一个各向异性全变分（TV）先验。对于 $M = 2N_xN_y$ 个梯度分量，对数先验为：\n    $$\\log p(x \\mid \\lambda) = M \\log\\lambda - \\lambda \\, \\mathrm{TV}(x) + C_2$$\n    其中 $\\mathrm{TV}(x) = \\sum_{i,j} (\\lvert (D_x x)_{i,j} \\rvert + \\lvert (D_y x)_{i,j} \\rvert)$。\n3.  **超先验**：超参数 $\\lambda$ 本身服从伽马分布，$\\lambda \\sim \\mathrm{Gamma}(a,b)$，其形状参数为 $a$，率参数为 $b$。对数超先验为：\n    $$\\log p(\\lambda) = (a-1)\\log\\lambda - b\\lambda + C_3$$\n\n结合这些项，我们旨在最小化的联合后验负对数 $J(x, \\lambda) = -\\log p(x, \\lambda \\mid y)$ 为：\n$$J(x, \\lambda) = \\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + \\lambda \\, \\mathrm{TV}(x) + b\\lambda - (M+a-1)\\log\\lambda + C_4$$\n\n### 用于联合 MAP 估计的坐标上升法\n\n最小化 $J(x, \\lambda)$ 的一个自然方法是采用坐标上升算法，该算法在固定 $\\lambda$ 的情况下优化 $x$，和固定 $x$ 的情况下优化 $\\lambda$ 之间交替进行。\n\n**1. $x$-子问题（固定 $\\lambda$）：**\n对于一个固定的超参数值 $\\lambda_k$，关于 $x$ 的优化问题变为：\n$$x_{k+1} = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + \\lambda_k \\mathrm{TV}(x) \\right\\}$$\n这是一个标准的用于TV正则化图像重建的凸问题。它由一个二次数据保真项和一个非光滑但凸的TV正则项组成。\n\n**2. $\\lambda$-子问题（固定 $x$）：**\n对于一个固定的图像估计 $x_{k+1}$，关于 $\\lambda$ 的优化问题只涉及 $J(x, \\lambda)$ 中依赖于 $\\lambda$ 的项：\n$$\\lambda_{k+1} = \\arg\\min_{\\lambda > 0} \\left\\{ \\lambda \\, \\mathrm{TV}(x_{k+1}) + b\\lambda - (M+a-1)\\log\\lambda \\right\\}$$\n这是一个关于 $\\lambda$ 的凸函数。我们可以通过将其对 $\\lambda$ 的导数设为零来找到最小化解：\n$$\\frac{\\partial}{\\partial\\lambda} \\left[ (\\mathrm{TV}(x_{k+1}) + b)\\lambda - (M+a-1)\\log\\lambda \\right] = \\mathrm{TV}(x_{k+1}) + b - \\frac{M+a-1}{\\lambda} = 0$$\n解出 $\\lambda$ 得到闭式更新规则：\n$$\\lambda_{k+1} = \\frac{M+a-1}{\\mathrm{TV}(x_{k+1}) + b}$$\n此更新规则能自动调整正则化强度：当前估计中较高的TV值（意味着强边缘或噪声）会导致较小的 $\\lambda$，从而在下一次迭代中放宽TV惩罚。反之，平滑的估计（低TV）会导致较大的 $\\lambda$，从而施加更强的平滑。\n\n### $x$-子问题的算法设计\n\n$x$-子问题的形式为 $\\min_x G(x) + F(Kx)$，适合使用诸如Chambolle-Pock算法之类的原始-对偶分裂方法求解。我们定义：\n- $G(x) = \\frac{1}{2\\sigma^2} \\lVert Ax-y \\rVert_2^2$ (光滑，二次)\n- $Kx = \\nabla x = (D_y x, D_x x)$ (离散梯度算子)\n- $F(u) = \\lambda_k \\lVert u \\rVert_1$ (非光滑，可分)\n\n原始变量 $x$ 和对偶变量 $p$ 的迭代更新如下：\n1.  **对偶更新**：$p^{j+1} = \\mathrm{prox}_{\\sigma_{\\text{pd}} F^*} (p^j + \\sigma_{\\text{pd}} K \\bar{x}^j)$\n2.  **原始更新**：$x^{j+1} = \\mathrm{prox}_{\\tau G} (x^j - \\tau K^* p^{j+1})$\n3.  **外推**：$\\bar{x}^{j+1} = x^{j+1} + \\theta(x^{j+1} - x^j)$\n\n邻近算子是关键：\n- $\\mathrm{prox}_{\\sigma_{\\text{pd}} F^*}$：$F(u)=\\lambda_k \\lVert u \\rVert_1$ 的凸共轭是半径为 $\\lambda_k$ 的 $\\ell_\\infty$ 球的指示函数。因此，其邻近算子是到这个球上的投影，这简化为对对偶变量 $p$ 的逐元素裁剪操作：\n  $$p \\mapsto \\mathrm{clip}(p, -\\lambda_k, \\lambda_k)$$\n- $\\mathrm{prox}_{\\tau G}$：二次项 $G(x)$ 的邻近算子由下式给出：\n  $$\\mathrm{prox}_{\\tau G}(z) = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2}\\lVert Ax-y \\rVert_2^2 + \\frac{1}{2\\tau}\\lVert x-z \\rVert_2^2 \\right\\}$$\n  由于周期性边界条件，卷积算子 $A$ 可被二维离散傅里叶变换（DFT）$\\mathcal{F}$ 对角化。设 $\\hat{h} = \\mathcal{F}(h)$ 是模糊核 $h$ 的DFT。解可以在傅里叶域中高效计算：\n  $$x = \\mathcal{F}^{-1} \\left( \\frac{\\frac{1}{\\sigma^2}\\overline{\\hat{h}} \\odot \\mathcal{F}(y) + \\frac{1}{\\tau}\\mathcal{F}(z)}{\\frac{1}{\\sigma^2}|\\hat{h}|^2 + \\frac{1}{\\tau}} \\right)$$\n  其中 $\\odot$ 和分数线分别表示逐元素乘法和除法，$\\overline{\\hat{h}}$ 是 $\\hat{h}$ 的复共轭。这使得原始更新可以通过几次快速傅里叶变换来完成。\n\n最终算法在 $T$ 次外循环中交替进行：通过 $K$ 次内循环原始-对偶迭代更新 $x$，然后使用闭式解更新 $\\lambda$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a hierarchical Bayesian method for 2D deblurring.\n    \"\"\"\n    # ---- 1. Simulation Setup ----\n    N = 32\n    T = 12  # Outer iterations for hierarchical model\n    K = 60  # Inner primal-dual iterations per outer loop\n    a = 1.1\n    b = 1e-3\n    tau = 0.25 # Primal-dual step size\n    sigma_pd = 0.25 # Primal-dual step size\n    theta = 1.0 # Primal-dual extrapolation parameter\n    noise_sigma = 0.01\n\n    rng = np.random.default_rng(0)\n\n    # ---- 2. Helper Functions and Operators ----\n    def create_gaussian_kernel(size, std_dev):\n        \"\"\"Creates a 2D Gaussian kernel.\"\"\"\n        ax = np.arange(-size // 2 + 1., size // 2 + 1.)\n        xx, yy = np.meshgrid(ax, ax)\n        kernel = np.exp(-(xx**2 + yy**2) / (2. * std_dev**2))\n        return kernel / np.sum(kernel)\n\n    def create_x_true(name, n):\n        \"\"\"Generates the ground-truth images.\"\"\"\n        x_true = np.zeros((n, n), dtype=float)\n        if name == \"edge\":\n            s, h = n // 2, n // 4\n            x_true[s-h:s+h, s-h:s+h] = 1.0\n        elif name == \"smooth\":\n            x_true = np.fromfunction(lambda i, j: i / (n - 1), (n, n), dtype=float)\n        elif name == \"constant\":\n            x_true = np.full((n, n), 0.5)\n        return x_true\n\n    def grad(u):\n        \"\"\"Computes the discrete gradient with periodic boundary conditions.\"\"\"\n        dx = np.roll(u, -1, axis=1) - u\n        dy = np.roll(u, -1, axis=0) - u\n        return np.array([dy, dx])\n\n    def div(p):\n        \"\"\"Computes the negative divergence, adjoint of the gradient.\"\"\"\n        px, py = p[1], p[0]\n        div_x = px - np.roll(px, 1, axis=1)\n        div_y = py - np.roll(py, 1, axis=0)\n        return -(div_x + div_y)\n\n    def tv(u):\n        \"\"\"Computes the anisotropic total variation.\"\"\"\n        g = grad(u)\n        return np.sum(np.abs(g[0])) + np.sum(np.abs(g[1]))\n\n    def solve_reconstruction(y_obs, h_fft, n_val, noise_sig, outer_iters, inner_iters, a_hyper, b_hyper, \n                             tau_pd, sigma_pd_val, theta_val, fixed_lambda=None):\n        \"\"\"Solves the reconstruction problem using coordinate ascent and primal-dual.\"\"\"\n        M = 2 * n_val * n_val\n        h_fft_sq = np.abs(h_fft)**2\n        y_fft = np.fft.fft2(y_obs)\n        \n        prox_g_denom = (1 / noise_sig**2) * h_fft_sq + (1 / tau_pd)\n        prox_g_num_y_part = (1 / noise_sig**2) * np.conj(h_fft) * y_fft\n\n        x = np.copy(y_obs)\n        p = np.zeros((2, n_val, n_val))\n\n        if fixed_lambda is not None:\n            lambda_reg = fixed_lambda\n            run_outer_iters = 1\n        else:\n            lambda_reg = a_hyper / b_hyper\n            run_outer_iters = outer_iters\n\n        for _ in range(run_outer_iters):\n            x_bar = np.copy(x)\n            for _ in range(inner_iters): # Inner loop for x-update\n                grad_x_bar = grad(x_bar)\n                p_new = p + sigma_pd_val * grad_x_bar\n                p = np.clip(p_new, -lambda_reg, lambda_reg)\n\n                x_old = np.copy(x)\n                z = x - tau_pd * div(p)\n                z_fft = np.fft.fft2(z)\n                \n                prox_g_num = prox_g_num_y_part + (1/tau_pd) * z_fft\n                x_fft = prox_g_num / prox_g_denom\n                x = np.real(np.fft.ifft2(x_fft))\n                \n                x_bar = x + theta_val * (x - x_old)\n            \n            if fixed_lambda is None: # Lambda-update\n                current_tv = tv(x)\n                lambda_reg = (M + a_hyper - 1) / (b_hyper + current_tv)\n\n        # Final lambda value for the hierarchical case\n        if fixed_lambda is None:\n            current_tv = tv(x)\n            lambda_reg = (M + a_hyper - 1) / (b_hyper + current_tv)\n            \n        return x, lambda_reg\n\n    # ---- 3. Setup Forward Operator ----\n    kernel_size = 5\n    kernel_std = 1.0\n    kernel = create_gaussian_kernel(kernel_size, kernel_std)\n    \n    padded_kernel = np.zeros((N, N))\n    pad_y, pad_x = (N - kernel_size) // 2, (N - kernel_size) // 2\n    padded_kernel[pad_y:pad_y + kernel_size, pad_x:pad_x + kernel_size] = kernel\n    padded_kernel = np.fft.ifftshift(padded_kernel)\n    h_fft = np.fft.fft2(padded_kernel)\n\n    def A(x_in):\n        return np.real(np.fft.ifft2(h_fft * np.fft.fft2(x_in)))\n\n    # ---- 4. Run Test Cases ----\n    \n    # Test 1: Edge-dominated vs. Smooth-ramp\n    x_true_edge = create_x_true(\"edge\", N)\n    y_edge = A(x_true_edge) + rng.normal(0, noise_sigma, (N, N))\n    x_hat_edge, lambda_hat_edge = solve_reconstruction(\n        y_edge, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    tv_hat_edge = tv(x_hat_edge)\n\n    x_true_smooth = create_x_true(\"smooth\", N)\n    y_smooth = A(x_true_smooth) + rng.normal(0, noise_sigma, (N, N))\n    x_hat_smooth, lambda_hat_smooth = solve_reconstruction(\n        y_smooth, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    tv_hat_smooth = tv(x_hat_smooth)\n    \n    test1_results = [lambda_hat_edge < lambda_hat_smooth, tv_hat_edge > tv_hat_smooth]\n    \n    # Test 2: Constant vs. Edge-dominated\n    x_true_const = create_x_true(\"constant\", N)\n    y_const = A(x_true_const) + rng.normal(0, noise_sigma, (N, N))\n    _, lambda_hat_const = solve_reconstruction(\n        y_const, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    \n    test2_result = lambda_hat_const > lambda_hat_edge\n    \n    # Test 3: Hierarchical vs. Fixed Lambda\n    x_hat_hierarchical = x_hat_edge # Re-use from Test 1\n    mse_hierarchical = np.mean((x_hat_hierarchical - x_true_edge)**2)\n\n    lambda_fixed = a / b\n    x_hat_fixed, _ = solve_reconstruction(\n        y_edge, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta, fixed_lambda=lambda_fixed)\n    mse_fixed = np.mean((x_hat_fixed - x_true_edge)**2)\n\n    test3_result = mse_hierarchical < mse_fixed\n    \n    # ---- 5. Final Output ----\n    b11, b12 = test1_results\n    b2 = test2_result\n    b3 = test3_result\n    print(f\"[[{b11}, {b12}], {b2}, {b3}]\")\n\nsolve()\n```", "id": "3388760"}]}