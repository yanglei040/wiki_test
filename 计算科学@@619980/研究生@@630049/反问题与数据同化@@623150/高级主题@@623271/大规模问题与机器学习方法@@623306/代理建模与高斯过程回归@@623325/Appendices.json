{"hands_on_practices": [{"introduction": "高斯过程回归（GPR）的核心是多元高斯分布的边缘似然函数。这个练习将带你亲手实践计算该似然函数所需的两个基本线性代数构建模块：协方差矩阵的对数行列式和 Cholesky 分解 [@problem_id:3423975]。掌握这些计算是理解 GPR 模型底层机制以及后续进行超参数优化的关键第一步。", "problem": "考虑一个数据同化设置中的线性观测模型，其中潜在状态被赋予一个高斯过程（GP）先验。在高斯过程回归（GPR）中，两个输入位置上两个带噪声观测的函数值的协方差是先验协方差与一个独立的观测噪声方差之和。具体来说，设先验协方差矩阵为 $K=\\begin{pmatrix}1  0.9\\\\ 0.9  1\\end{pmatrix}$，观测噪声方差为 $\\sigma^{2}=0.1$，因此观测数据的总协方差为 $A=K+\\sigma^{2}I$。为了逆问题中的代理建模和超参数推断，需要两个计算基元：行列式的自然对数 $\\ln\\det(A)$ 和具有正对角线元素且满足 $A=L\\,L^{\\top}$ 的下三角 Cholesky 因子 $L$。\n\n从行列式和对称正定矩阵的 Cholesky 分解的定义和基本性质出发，对给定的 $K$ 和 $\\sigma^{2}$，显式地计算 $\\ln\\det(K+\\sigma^{2}I)$ 和 Cholesky 因子 $L$。使用自然对数并提供精确的代数表达式（不要四舍五入）。你的最终答案必须包含这两个量，其中 Cholesky 因子由其非零项指定。", "solution": "用户提供了一个来自数据同化和机器学习领域的有效且适定的问题陈述。\n\n该问题要求计算高斯过程回归的两个基本量：协方差矩阵的行列式的自然对数 $\\ln\\det(A)$，以及该矩阵的 Cholesky 因子 $L$。\n\n给定的量是：\n先验协方差矩阵 $K = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix}$。\n观测噪声方差 $\\sigma^{2}=0.1$。\n观测数据的总协方差由模型 $A=K+\\sigma^{2}I$ 给出，其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n\n首先，我们构造矩阵 $A$。\n$$\nI = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\nA = K + \\sigma^{2}I = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix} + 0.1 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\nA = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix} + \\begin{pmatrix} 0.1  0 \\\\ 0  0.1 \\end{pmatrix} = \\begin{pmatrix} 1.1  0.9 \\\\ 0.9  1.1 \\end{pmatrix}\n$$\n为了保持精确性，我们可以将这些小数值表示为分数：\n$$\nA = \\begin{pmatrix} \\frac{11}{10}  \\frac{9}{10} \\\\ \\frac{9}{10}  \\frac{11}{10} \\end{pmatrix}\n$$\n\n接下来，我们计算第一个所需量，$\\ln\\det(A)$。\n一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ 的行列式由公式 $ad - bc$ 给出。\n$$\n\\det(A) = \\left(\\frac{11}{10}\\right) \\left(\\frac{11}{10}\\right) - \\left(\\frac{9}{10}\\right) \\left(\\frac{9}{10}\\right) = \\frac{121}{100} - \\frac{81}{100} = \\frac{121 - 81}{100} = \\frac{40}{100} = \\frac{2}{5}\n$$\n因此，行列式的自然对数是：\n$$\n\\ln\\det(A) = \\ln\\left(\\frac{2}{5}\\right)\n$$\n\n其次，我们计算 Cholesky 因子 $L$。$A$ 是一个对称正定矩阵，因此存在一个唯一的具有正对角线元素的下三角矩阵 $L$，使得 $A = L L^{\\top}$。设 $L$ 为：\n$$\nL = \\begin{pmatrix} L_{11}  0 \\\\ L_{21}  L_{22} \\end{pmatrix}\n$$\n条件 $A=LL^{\\top}$ 给出：\n$$\n\\begin{pmatrix} A_{11}  A_{12} \\\\ A_{21}  A_{22} \\end{pmatrix} = \\begin{pmatrix} L_{11}  0 \\\\ L_{21}  L_{22} \\end{pmatrix} \\begin{pmatrix} L_{11}  L_{21} \\\\ 0  L_{22} \\end{pmatrix} = \\begin{pmatrix} L_{11}^2  L_{11}L_{21} \\\\ L_{21}L_{11}  L_{21}^2 + L_{22}^2 \\end{pmatrix}\n$$\n我们通过令矩阵的元素相等来求解 $L$ 的各项：\n第 $(1,1)$ 个元素给出 $A_{11} = L_{11}^2$：\n$$\nL_{11}^2 = \\frac{11}{10} \\implies L_{11} = \\sqrt{\\frac{11}{10}}\n$$\n根据 Cholesky 分解的要求，我们取正根。\n\n第 $(2,1)$ 个元素给出 $A_{21} = L_{21}L_{11}$：\n$$\nL_{21} = \\frac{A_{21}}{L_{11}} = \\frac{9/10}{\\sqrt{11/10}} = \\frac{9}{10} \\frac{\\sqrt{10}}{\\sqrt{11}} = \\frac{9}{\\sqrt{10}\\sqrt{11}} = \\frac{9}{\\sqrt{110}}\n$$\n\n第 $(2,2)$ 个元素给出 $A_{22} = L_{21}^2 + L_{22}^2$：\n$$\nL_{22}^2 = A_{22} - L_{21}^2 = \\frac{11}{10} - \\left(\\frac{9}{\\sqrt{110}}\\right)^2 = \\frac{11}{10} - \\frac{81}{110}\n$$\n为了进行减法，我们找到一个公分母：\n$$\nL_{22}^2 = \\frac{11 \\times 11}{110} - \\frac{81}{110} = \\frac{121 - 81}{110} = \\frac{40}{110} = \\frac{4}{11}\n$$\n取正平方根：\n$$\nL_{22} = \\sqrt{\\frac{4}{11}} = \\frac{2}{\\sqrt{11}}\n$$\n因此，Cholesky 因子是：\n$$\nL = \\begin{pmatrix} \\sqrt{\\frac{11}{10}}  0 \\\\ \\frac{9}{\\sqrt{110}}  \\frac{2}{\\sqrt{11}} \\end{pmatrix}\n$$\n问题要求计算两个量：$\\ln\\det(A)$ 和 Cholesky 因子 $L$ 的非零项，即 $L_{11}$、$L_{21}$ 和 $L_{22}$。\n\n最终值为：\n$\\ln\\det(A) = \\ln(\\frac{2}{5})$\n$L_{11} = \\sqrt{\\frac{11}{10}}$\n$L_{21} = \\frac{9}{\\sqrt{110}}$\n$L_{22} = \\frac{2}{\\sqrt{11}}$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\ln\\left(\\frac{2}{5}\\right)  \\sqrt{\\frac{11}{10}}  \\frac{9}{\\sqrt{110}}  \\frac{2}{\\sqrt{11}} \\end{pmatrix}}\n$$", "id": "3423975"}, {"introduction": "高斯过程模型的灵活性和表现力在很大程度上取决于其超参数（例如，核函数参数和噪声方差）的选择。这个练习将指导你从高斯过程的边缘似然函数出发，通过矩阵微积分推导出噪声方差的最大似然估计所满足的一阶条件 [@problem_id:3423961]。通过数值求解这个条件，你将学习到如何从数据中自动学习模型参数，这是将理论应用于实际问题的核心技能。", "problem": "考虑一个用于逆问题和数据同化背景下昂贵正演模型的一维高斯过程 (GP) 回归代理模型。给定一个大小为 $n=5$ 的小数据集，包含输入位置和相应的标量输出。输出之间的协方差由具有固定信号方差和长度尺度超参数的平方指数 (SE) 核函数 $k_{\\mathrm{SE}}$ 建模。假设存在方差为 $\\sigma^2$ (未知) 的加性独立同分布高斯观测噪声。你的目标是通过对精确边缘似然进行微分并数值求解一阶条件 (FOC)，来获得 $\\sigma^2$ 的最大似然估计 (MLE)。\n\n从高斯过程回归和多元正态分布的精确边缘似然的基本定义出发，仅使用矩阵微积分中公认的性质，例如对数行列式的导数和矩阵逆相对于标量参数的导数，来推导 $\\sigma^2$ 的一阶最优性条件。不要假设任何跳过此类推导的快捷公式。然后，实现一个数值求解器来找到满足所推导 FOC 的 MLE $\\hat{\\sigma}^2$。如果 FOC 在开区间 $\\sigma^20$ 内无解，则通过正确分析边界行为来确定最大化器。\n\n使用以下固定的核函数和数据规范。对于每个测试用例，协方差矩阵定义为\n$$\nK_{ij} = k_{\\mathrm{SE}}(x_i, x_j) = \\alpha^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{|x_i - x_j|}{\\ell}\\right)^2\\right),\n$$\n其中 $\\alpha^2$ 是信号方差，$\\ell$ 是长度尺度。观测模型为 $y \\sim \\mathcal{N}(0, K + \\sigma^2 I)$，其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n实现一个程序，对于每个测试用例，通过以下步骤计算 MLE $\\hat{\\sigma}^2$：\n- 使用给定的 $(\\alpha^2, \\ell)$ 和 $k_{\\mathrm{SE}}$ 构建协方差矩阵 $K$，\n- 在 $K + \\sigma^2 I$ 下，构建 $y$ 的精确对数边缘似然，\n- 对 $\\sigma^2$ 求导以获得 FOC，\n- 数值求解 $\\sigma^2  0$ 的 FOC；如果不存在内部解，则选择正确的边界最大化器。\n\n将每个 $\\hat{\\sigma}^2$ 报告为四舍五入到六位小数的浮点数。\n\n测试套件：\n- 案例 1 (一般情况)：$x = [0.0, 0.25, 0.5, 0.75, 1.0]$, $y = [0.05, 0.6, 0.9, 0.6, 0.0]$, $\\alpha^2 = 1.2$, $\\ell = 0.3$。\n- 案例 2 (信号接近零的边界情况)：$x = [0.0, 0.1, 0.4, 0.8, 1.2]$, $y = [0.0, 0.0, 0.0, 0.0, 0.0]$, $\\alpha^2 = 0.8$, $\\ell = 0.5$。\n- 案例 3 (不同尺度和曲率)：$x = [0.0, 0.2, 0.5, 0.9, 1.4]$, $y = [1.0, 0.9, 0.4, -0.2, -0.7]$, $\\alpha^2 = 1.5$, $\\ell = 0.4$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[\\text{result1},\\text{result2},\\text{result3}]$），每个 $\\hat{\\sigma}^2$ 四舍五入到六位小数。本问题不涉及物理单位，并且所有角度（如有）都默认为无量纲实数。最终输出必须是浮点数。", "solution": "用户希望在一个高斯过程 (GP) 回归模型中，找到噪声方差 ($\\sigma^2$) 的最大似然估计 (MLE)。该过程包括两个主要阶段：首先，从基本原理形式化推导 $\\sigma^2$ 的 MLE 的一阶条件 (FOC)；其次，实现一个数值求解器，以找到满足该条件的 $\\sigma^2$ 值，用于给定的数据集。\n\n### 第 1 部分：一阶条件的推导\n\n问题指定了一个用于在输入位置 $X$ 处的一组观测值 $y$ 的 GP 模型。假设观测值遵循一个均值为零的多元正态分布：\n$$\ny \\sim \\mathcal{N}(0, K_y)\n$$\n其中 $K_y$ 是带噪观测值的协方差矩阵。它被定义为信号协方差矩阵 $K$ 和噪声协方差矩阵之和。信号协方差由平方指数 (SE) 核函数给出，噪声被假定为方差为 $\\sigma^2$ 的独立同分布高斯噪声。因此，总协方差矩阵为：\n$$\nK_y = K + \\sigma^2 I\n$$\n其中 $K_{ij} = k_{\\mathrm{SE}}(x_i, x_j) = \\alpha^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{|x_i - x_j|}{\\ell}\\right)^2\\right)$，$I$ 是 $n \\times n$ 的单位矩阵，其中 $n$ 是数据点的数量。\n\n这个多元正态分布的概率密度函数 (PDF) 是：\n$$\np(y | X, \\alpha^2, \\ell, \\sigma^2) = \\frac{1}{(2\\pi)^{n/2} (\\det(K_y))^{1/2}} \\exp\\left(-\\frac{1}{2} y^T K_y^{-1} y\\right)\n$$\n对数边缘似然，记为 $\\mathcal{L}(\\sigma^2)$，是此 PDF 的自然对数。我们将 $\\sigma^2$ 视为待优化的变量，而其他参数是固定的。\n$$\n\\mathcal{L}(\\sigma^2) = \\log p(y | \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\det(K_y)) - \\frac{1}{2} y^T K_y^{-1} y\n$$\n为了找到 $\\sigma^2$ 的 MLE，我们必须最大化 $\\mathcal{L}(\\sigma^2)$。这等价于最大化去掉常数项 $-\\frac{n}{2}\\log(2\\pi)$ 后的表达式。为简化符号，我们定义 $\\theta = \\sigma^2$。目标函数变为：\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{2}\\log(\\det(K + \\theta I)) - \\frac{1}{2} y^T (K + \\theta I)^{-1} y\n$$\n我们通过计算 $\\mathcal{L}(\\theta)$ 相对于 $\\theta$ 的导数并将其设为零来找到最大值。这是一阶条件 (FOC)。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = -\\frac{1}{2} \\frac{\\partial}{\\partial \\theta} \\left[ \\log(\\det(K + \\theta I)) \\right] - \\frac{1}{2} \\frac{\\partial}{\\partial \\theta} \\left[ y^T (K + \\theta I)^{-1} y \\right] = 0\n$$\n为了计算这个导数，我们使用矩阵微积分中的两个标准恒等式。对于一个作为标量 $t$ 的函数的对称矩阵 $A(t)$：\n1. 对数行列式的导数：$\\frac{\\partial}{\\partial t} \\log(\\det(A(t))) = \\text{tr}\\left(A(t)^{-1} \\frac{\\partial A(t)}{\\partial t}\\right)$\n2. 矩阵逆的导数：$\\frac{\\partial}{\\partial t} A(t)^{-1} = -A(t)^{-1} \\frac{\\partial A(t)}{\\partial t} A(t)^{-1}$\n\n在我们的问题中，矩阵是 $A(\\theta) = K + \\theta I$。它相对于 $\\theta$ 的导数是：\n$$\n\\frac{\\partial A(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta}(K + \\theta I) = I\n$$\n因为 $K$ 相对于 $\\theta$ 是常数。\n\n现在，我们将这些恒等式应用于 $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$ 的每一项：\n\n对于第一项（对数行列式）：\n$$\n\\frac{\\partial}{\\partial \\theta} \\log(\\det(K + \\theta I)) = \\text{tr}\\left((K + \\theta I)^{-1} \\frac{\\partial(K + \\theta I)}{\\partial \\theta}\\right) = \\text{tr}\\left((K + \\theta I)^{-1} I\\right) = \\text{tr}((K + \\theta I)^{-1})\n$$\n\n对于第二项（二次型）：\n$$\n\\frac{\\partial}{\\partial \\theta} \\left[ y^T (K + \\theta I)^{-1} y \\right] = y^T \\left( \\frac{\\partial}{\\partial \\theta} (K + \\theta I)^{-1} \\right) y\n$$\n使用矩阵逆的导数恒等式：\n$$\n\\frac{\\partial}{\\partial \\theta} (K + \\theta I)^{-1} = -(K + \\theta I)^{-1} \\left(\\frac{\\partial(K + \\theta I)}{\\partial \\theta}\\right) (K + \\theta I)^{-1} = -(K + \\theta I)^{-1} I (K + \\theta I)^{-1} = -(K + \\theta I)^{-2}\n$$\n将其代回，二次项的导数是：\n$$\ny^T \\left( -(K + \\theta I)^{-2} \\right) y = -y^T (K + \\theta I)^{-2} y\n$$\n\n结合两项的导数，我们得到对数边缘似然的导数：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = -\\frac{1}{2} \\left[ \\text{tr}((K + \\theta I)^{-1}) \\right] - \\frac{1}{2} \\left[ -y^T (K + \\theta I)^{-2} y \\right]\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{1}{2} \\left[ y^T (K + \\theta I)^{-2} y - \\text{tr}((K + \\theta I)^{-1}) \\right]\n$$\n将此导数设为零，得到 MLE $\\hat{\\theta} = \\hat{\\sigma}^2$ 的 FOC：\n$$\ny^T (K + \\hat{\\sigma}^2 I)^{-2} y = \\text{tr}((K + \\hat{\\sigma}^2 I)^{-1})\n$$\n这是一个关于 $\\hat{\\sigma}^2$ 的非线性标量方程，必须数值求解。\n\n### 第 2 部分：数值求解策略\n\nFOC 要求找到函数的一个根 $\\hat{\\sigma}^2  0$：\n$$\nf(\\sigma^2) = y^T (K + \\sigma^2 I)^{-2} y - \\text{tr}((K + \\sigma^2 I)^{-1})\n$$\n对解的搜索必须小心处理，因为最大化器可能位于有效域的边界上，即 $\\hat{\\sigma}^2 = 0$。对数似然函数 $\\mathcal{L}(\\sigma^2)$ 的行为由其导数的符号决定，即 $\\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} = \\frac{1}{2}f(\\sigma^2)$。\n\n我们分析在边界 $\\sigma^2 \\to 0^+$ 处导数的符号：\n$$\n\\lim_{\\sigma^2 \\to 0^+} \\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} = \\frac{1}{2} \\left[ y^T K^{-2} y - \\text{tr}(K^{-1}) \\right]\n$$\n（这假设 $K$ 是可逆的，对于具有不同输入的 SE 核函数来说，这是成立的）。\n\n1. 如果在 $\\sigma^2 = 0^+$ 时 $\\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} \\le 0$，则对数似然函数初始是非递增的。由于对于大的 $\\sigma^2$，$\\mathcal{L}(\\sigma^2) \\to -\\infty$，函数在边界处达到最大值。因此，MLE 是 $\\hat{\\sigma}^2 = 0$。\n\n2. 如果在 $\\sigma^2 = 0^+$ 时 $\\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2}  0$，则对数似然函数初始是递增的。由于它最终必须递减到 $-\\infty$，所以在域的内部必须存在一个最大值，即在某个 $\\hat{\\sigma}^2  0$ 处。这个内部最大值对应于 FOC 的一个根，$f(\\hat{\\sigma}^2)=0$。\n\n数值算法如下：\n1. 对于给定的测试用例，构建核矩阵 $K$。\n2. 对于一个小的正值，例如 $\\sigma^2 = 10^{-9}$，数值计算 $f(\\sigma^2)$ 的符号，以近似边界处的行为。\n3. 如果 $f(10^{-9}) \\le 0$，则 MLE 是 $\\hat{\\sigma}^2 = 0$。\n4. 如果 $f(10^{-9})  0$，则存在一个内部根。我们可以使用像 `scipy.optimize.brentq` 这样的数值求解器来找到这个根。这需要将根括在一个区间内。我们知道 $f(\\sigma^2)$ 在零附近是正的。对于大的 $\\sigma^2$，项 $-\\text{tr}((K + \\sigma^2 I)^{-1}) \\approx -n/\\sigma^2$ 会主导项 $y^T (K + \\sigma^2 I)^{-2} y \\approx (y^T y)/\\sigma^4$，所以 $f(\\sigma^2)$ 会变为负。这保证了可以找到一个括号区间 $[a, b]$ 使得 $f(a)  0$ 和 $f(b)  0$，从而允许求根器收敛到唯一的内部最大值。\n\n以下 Python 代码实现了这一逻辑。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Computes the Maximum Likelihood Estimate (MLE) for the noise variance sigma^2\n    in a Gaussian Process regression model for several test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"x\": np.array([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"y\": np.array([0.05, 0.6, 0.9, 0.6, 0.0]),\n            \"alpha_sq\": 1.2,\n            \"ell\": 0.3\n        },\n        {\n            \"x\": np.array([0.0, 0.1, 0.4, 0.8, 1.2]),\n            \"y\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"alpha_sq\": 0.8,\n            \"ell\": 0.5\n        },\n        {\n            \"x\": np.array([0.0, 0.2, 0.5, 0.9, 1.4]),\n            \"y\": np.array([1.0, 0.9, 0.4, -0.2, -0.7]),\n            \"alpha_sq\": 1.5,\n            \"ell\": 0.4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mle_sigma_sq = compute_mle_for_case(\n            case[\"x\"], case[\"y\"], case[\"alpha_sq\"], case[\"ell\"]\n        )\n        results.append(f\"{mle_sigma_sq:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef build_se_kernel(x, alpha_sq, ell):\n    \"\"\"Constructs the Squared Exponential kernel matrix K.\"\"\"\n    n = len(x)\n    K = np.zeros((n, n))\n    l_sq = ell**2\n    for i in range(n):\n        for j in range(n):\n            dist_sq = (x[i] - x[j])**2\n            K[i, j] = alpha_sq * np.exp(-0.5 * dist_sq / l_sq)\n    return K\n\ndef foc_function(sigma_sq, K, y_col):\n    \"\"\"\n    Computes the value of the First-Order Condition (FOC) function for a given sigma^2.\n    The function is f(sigma^2) = y^T(K+sigma^2*I)^(-2)y - tr((K+sigma^2*I)^(-1)).\n    The MLE is a root of this function.\n    \n    Args:\n        sigma_sq (float): The noise variance parameter sigma^2.\n        K (np.ndarray): The signal covariance matrix.\n        y_col (np.ndarray): The observation vector (as a column vector).\n\n    Returns:\n        float: The value of the FOC function.\n    \"\"\"\n    if sigma_sq  0:\n        return np.inf  # Enforce non-negativity constraint.\n    \n    n = K.shape[0]\n    I = np.eye(n)\n    K_y = K + sigma_sq * I\n    \n    try:\n        inv_K_y = np.linalg.inv(K_y)\n    except np.linalg.LinAlgError:\n        # If matrix is singular, return a large value to repel the solver.\n        return np.inf\n        \n    # Efficient computation of the quadratic form term\n    # alpha = inv(K_y) @ y\n    # y.T @ inv(K_y)^2 @ y = y.T @ inv(K_y).T @ inv(K_y) @ y = (inv(K_y) @ y).T @ (inv(K_y) @ y)\n    alpha = inv_K_y @ y_col\n    quadratic_term = alpha.T @ alpha\n    \n    # Trace term\n    trace_term = np.trace(inv_K_y)\n    \n    return quadratic_term - trace_term\n\ndef compute_mle_for_case(x, y, alpha_sq, ell):\n    \"\"\"\n    Determines the MLE of sigma^2 for a single case.\n    \n    It first checks the derivative of the log-likelihood at the boundary (sigma^2=0).\n    If it's non-positive, the maximum is at the boundary (sigma^2=0).\n    If it's positive, an interior maximum exists and is found using a numerical root-finder.\n    \"\"\"\n    K = build_se_kernel(x, alpha_sq, ell)\n    y_col = y.reshape(-1, 1)\n\n    # The sign of the FOC function f(sigma^2) is the sign of the derivative of the log-likelihood.\n    # Check the sign at a small positive value to determine behavior at the boundary.\n    epsilon = 1e-9\n    \n    try:\n        foc_at_zero = foc_function(epsilon, K, y_col)\n    except np.linalg.LinAlgError:\n        # This handles the rare case where K itself is singular.\n        # This implies likelihood at sigma^2=0 is -inf, so derivative must be positive.\n        foc_at_zero = np.inf\n\n    # If the derivative is non-positive at the boundary, max is at sigma^2 = 0.\n    if foc_at_zero = 0:\n        return 0.0\n    else:\n        # Interior solution exists. Find a bracketing interval for the root-finder.\n        # The FOC function is positive near zero and becomes negative for large sigma^2.\n        lower_bound = epsilon\n        upper_bound = 1.0\n        \n        # Search for an upper bound where the function is negative.\n        for _ in range(10): # Limit iterations to prevent infinite loop\n            if foc_function(upper_bound, K, y_col)  0:\n                break\n            upper_bound *= 10\n        else:\n            # This should not be reached in a well-behaved problem.\n            # It indicates the derivative doesn't become negative within a reasonable range.\n            # Returning a sentinel value would be appropriate in a general library,\n            # but for this specific problem, the root is expected to be found.\n            # Assume it will be found for these test cases.\n            pass\n\n        # Use Brent's method to find the root in the established bracket.\n        mle_sigma_sq = brentq(foc_function, lower_bound, upper_bound, args=(K, y_col))\n        return mle_sigma_sq\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3423961"}, {"introduction": "高斯过程代理模型的应用远不止于对昂贵模拟进行预测，它还能主动指导数据采集过程，以最高效的方式获取信息。这个高级练习探讨了高斯过程在贝叶斯实验设计中的应用，任务是量化在何处进行下一次模拟能够最大程度地减少模型参数后验分布的不确定性 [@problem_id:3423926]。通过计算预期熵减，你将体验到如何利用代理模型做出智能决策，这是在资源受限的科学研究中一个极其强大的范式。", "problem": "要求您形式化并计算一个逆问题中未知参数向量后验分布的微分熵的期望减少量。在该问题中，代理前向模型由高斯过程模拟，并且在局部线性化后，似然是线性高斯的。\n\n参数向量 $\\theta \\in \\mathbb{R}^d$ 服从高斯先验 $\\mathcal{N}(m_0, P_0)$。标量观测值 $y \\in \\mathbb{R}$ 来自带有加性高斯噪声的前向映射 $g(\\theta)$。在固定的线性化点 $\\bar{\\theta}$ 周围，似然是局部线性高斯的，即 $g(\\theta)$ 被局部近似为 $g(\\bar{\\theta}) + J (\\theta - \\bar{\\theta})$，其中 $J \\in \\mathbb{R}^{1 \\times d}$ 是在 $\\bar{\\theta}$ 处的雅可比矩阵，测量噪声是高斯的，其方差为 $R \\in \\mathbb{R}_{0}$。前向映射 $g$ 由高斯过程（GP）模拟，该模拟器在线性化位置引入了额外的预测方差，在似然中被视为一个加性方差项。该高斯过程在 $\\bar{\\theta}$ 附近的一维局域标量化坐标 $t \\in \\mathbb{R}$ 中的输入处的带噪声模拟器输出上进行训练；其核函数是具有已知超参数的平方指数核，并且模拟器输出上存在已知的独立同分布高斯噪声。模拟器在线性化位置 $t_0$ 的预测协方差不依赖于训练输出的值，仅依赖于训练输入位置、候选新输入位置以及核函数超参数。\n\n给定一组候选模拟器输入位置 $\\{x_i\\}$，可以在其中任一位置获取一个额外的模拟器评估。对于每个候选点 $x_i$，将期望熵减定义为当前 $\\theta$ 的近似后验分布的熵与在将高斯过程训练集增加一个位于 $x_i$ 的额外观测值后的近似后验分布的期望熵之差，其中期望是基于当前高斯过程后验预测分布，对 $x_i$ 处的（未知）模拟器输出求得。您的任务是从基本原理出发，推导为何该期望熵减仅依赖于协方差而不依赖于任何特定的已实现的模拟器输出，并使用以下参数值为每个候选点进行数值计算。\n\n仅使用以下基本依据：\n- 多元高斯分布的定义和基本性质，包括在线性高斯似然下的高斯先验的后验分布，以及多元高斯分布的微分熵。\n- 带有平方指数核和独立高斯观测噪声的高斯过程的定义，包括高斯过程后验协方差仅依赖于输入位置和核函数超参数，而不依赖于观测到的输出值这一事实。\n\n对于数值评估，请使用以下具体值：\n- 维度 $d = 2$，先验均值 $m_0 \\in \\mathbb{R}^2$ 为任意值（计算熵不需要），先验协方差为\n  $$P_0 = \\begin{bmatrix} 0.6  0.1 \\\\ 0.1  0.4 \\end{bmatrix}.$$\n- 在 $\\bar{\\theta}$ 处的局部雅可比矩阵（行向量），\n  $$J = \\begin{bmatrix} 0.8  -0.3 \\end{bmatrix}.$$\n- 测量噪声方差 $R = 0.05$。\n- 局域标量化坐标 $t \\in \\mathbb{R}$，线性化位置为 $t_0 = 0.2$。\n- 高斯过程核函数为平方指数核，\n  $$k(t, t') = \\sigma_f^2 \\exp\\!\\left(-\\frac{1}{2}\\frac{(t - t')^2}{\\ell^2}\\right),$$\n  其中方差 $\\sigma_f^2 = 1.2$，长度尺度 $\\ell = 0.5$。\n- 当前高斯过程训练输入位置（一维）为\n  $$T = [-0.8,\\, 0.0,\\, 0.9,\\, 1.5],$$\n  模拟器输出上存在独立同分布的高斯观测噪声，其方差为 $\\sigma_n^2 = 10^{-4}$。\n- 候选新输入位置为\n  $$\\{x_i\\} = [0.2,\\, 0.0,\\, 3.0,\\, -3.0,\\, 1.5].$$\n\n假设已从物理系统中收集了单个标量观测值 $y$；其实现值对于熵的计算并非必需。在局部线性高斯近似下，对模拟器的唯一依赖性体现在模拟器在 $t_0$ 处的预测方差上，当在候选点 $x_i$ 处添加一个额外的模拟器输入时，该方差会发生变化。\n\n您的程序必须对每个候选点 $x_i$：\n- 根据 $(T, \\sigma_f^2, \\ell, \\sigma_n^2)$ 计算当前高斯过程在 $t_0$ 处的预测方差。\n- 在假设以相同的模拟器噪声方差 $\\sigma_n^2$ 在 $x_i$ 处增加一个新的训练输入后，计算高斯过程在 $t_0$ 处的更新后预测方差（期望后验协方差不依赖于未知的模拟器输出值）。\n- 使用这些方差构成当前和更新后的有效似然方差，然后根据线性高斯模型得出当前和更新后的 $\\theta$ 的后验协方差。\n- 计算每个候选点的期望熵减（单位：奈特），即当前后验熵与更新后后验熵之差。\n\n测试套件和输出规范：\n- 使用上面列出的五个候选点 $\\{x_i\\}$ 作为测试套件。这些情况涵盖了候选点等于 $t_0$ 的理想情况、复制现有训练位置的情况，以及远离训练集且影响可忽略不计的点的情况。\n- 所需输出是表示熵减的实数（浮点数），单位为奈特。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，`[r1,r2,r3,r4,r5]`），其顺序与候选点 `[0.2, 0.0, 3.0, -3.0, 1.5]` 相对应。\n\n不涉及物理单位或角度单位。所有结果均以浮点数（单位：奈特）表示。", "solution": "用户提供的问题被评估为有效，因为它在科学上基于贝叶斯推断和高斯过程回归，问题陈述良好，具有完整且一致的参数集，并以客观、正式的方式呈现。任务是计算当高斯过程代理模型用新数据点更新时，参数后验分布的微分熵的期望减少量。\n\n推导和计算应从基本原理出发。\n\n**1. 逆问题的贝叶斯框架**\n\n该问题涉及对参数向量 $\\theta \\in \\mathbb{R}^d$ 的推断。\n关于 $\\theta$ 的先验知识被编码在一个多元高斯分布中：\n$$p(\\theta) = \\mathcal{N}(\\theta | m_0, P_0)$$\n其中 $m_0 \\in \\mathbb{R}^d$ 是先验均值， $P_0 \\in \\mathbb{R}^{d \\times d}$ 是先验协方差矩阵。\n\n单个标量观测值 $y \\in \\mathbb{R}$ 通过一个带有加性测量噪声 $\\epsilon \\sim \\mathcal{N}(0, R)$ 的前向模型 $g(\\theta)$ 与 $\\theta$ 相关联：\n$$y = g(\\theta) + \\epsilon$$\n前向模型 $g(\\theta)$ 很复杂，由一个代理模型，特别是高斯过程（GP），来近似。此外，为了推断的目的，该模型在一个点 $\\bar{\\theta}$ 周围进行线性化：\n$$g(\\theta) \\approx g(\\bar{\\theta}) + J(\\theta - \\bar{\\theta})$$\n其中 $J \\in \\mathbb{R}^{1 \\times d}$ 是在 $\\bar{\\theta}$ 处评估的 $g$ 的雅可比矩阵。点 $\\bar{\\theta}$ 对应于标量坐标空间 $t$ 中的一个位置 $t_0$。\n\n$g$ 的代理模型引入了其自身的不确定性。GP 模拟器为 $g(t_0)$ 提供了一个预测分布，该分布是高斯的：$g(t_0) \\sim \\mathcal{N}(m_{GP}(t_0), V_{GP}(t_0))$。预测方差 $V_{GP}(t_0)$ 被视为一个额外的不确定性来源，与测量噪声方差 $R$ 相加。\n因此，有效的观测模型变为：\n$$y \\sim \\mathcal{N}(y | m_{GP}(t_0) + J(\\theta - \\bar{\\theta}), R + V_{GP}(t_0))$$\n这为 $\\theta$ 定义了一个线性高斯似然。项 $m_{GP}(t_0) - J\\bar{\\theta}$ 可以被吸收到一个有效的观测值中，而对于计算后验协方差和熵，只有方差项是相关的。有效似然方差是 $R_{eff} = R + V_{GP}(t_0)$。\n\n**2. 后验分布与微分熵**\n\n给定高斯先验 $p(\\theta) = \\mathcal{N}(\\theta|m_0, P_0)$ 和线性高斯似然 $p(y|\\theta) \\propto \\mathcal{N}(\\text{const} + J\\theta, R_{eff})$，后验分布 $p(\\theta|y)$ 也是高斯的，即 $p(\\theta|y) = \\mathcal{N}(\\theta|m_{post}, P_{post})$。其逆协方差矩阵由高斯分布的贝叶斯定理给出：\n$$P_{post}^{-1} = P_0^{-1} + J^T R_{eff}^{-1} J$$\n一个协方差矩阵为 $P$ 的 $d$ 维高斯分布的微分熵（单位：奈特）是：\n$$H(P) = \\frac{1}{2}\\ln(\\det(P)) + \\frac{d}{2}(1 + \\ln(2\\pi))$$\n\n**3. 期望熵减**\n\n目标是计算当 GP 训练集 $T$ 增加一个新的输入位置 $x_i$ 时，熵的期望减少量 $\\Delta H_i$。熵减定义为当前后验熵与更新后期望后验熵之差。\n\n设 $H_{current}$ 是当前后验分布的熵，使用基于当前训练集 $T$ 的 GP 预测方差 $V_{GP}^{current}(t_0)$。\n$$H_{current} = \\frac{1}{2}\\ln(\\det(P_{post}^{current})) + \\frac{d}{2}(1 + \\ln(2\\pi))$$\n其中 $(P_{post}^{current})^{-1} = P_0^{-1} + J^T (R_{eff}^{current})^{-1} J$ 且 $R_{eff}^{current} = R + V_{GP}^{current}(t_0)$。\n\n当我们考虑添加一个新的训练点 $(x_i, z)$，其中 $z$ 是在 $x_i$ 处的未知模拟器输出时，在 $t_0$ 处的新 GP 预测方差变为 $V_{GP}^{new}(t_0)$。高斯过程的一个基本性质是，后验预测协方差（以及方差）仅依赖于训练*输入*位置、核函数超参数和测试输入位置。它*不*依赖于观测到的训练*输出*值（如 $z$）。因此，对于给定的 $x_i$，$V_{GP}^{new}(t_0)$ 是一个确定性量，与 $z$ 的值无关。\n\n因此，新的后验协方差 $P_{post}^{new}$ 及其熵 $H_{new}$ 也与 $z$ 无关。对 $z$ 的期望变得微不足道：\n$$E_z[H_{new}] = H_{new}$$\n于是，期望熵减为：\n$$\\Delta H_i = H_{current} - E_z[H_{new}] = H_{current} - H_{new}$$\n常数项 $\\frac{d}{2}(1 + \\ln(2\\pi))$ 会抵消，剩下：\n$$\\Delta H_i = \\frac{1}{2}\\ln(\\det(P_{post}^{current})) - \\frac{1}{2}\\ln(\\det(P_{post}^{new})) = \\frac{1}{2}\\ln\\left(\\frac{\\det(P_{post}^{current})}{\\det(P_{post}^{new})}\\right)$$\n这证实了计算仅依赖于协方差的变化，而不依赖于任何已实现的输出值。\n\n**4. 计算简化**\n\n为了计算 $\\det(P_{post})$，我们使用矩阵行列式引理：$\\det(A+uv^T) = \\det(A)(1+v^T A^{-1} u)$。对于我们的秩-1 更新 $P_{post}^{-1} = P_0^{-1} + \\frac{1}{R_{eff}}J^T J$，我们有 $A=P_0^{-1}$，$u = J^T/\\sqrt{R_{eff}}$，以及 $v = J^T/\\sqrt{R_{eff}}$。\n$$\\det(P_{post}^{-1}) = \\det(P_0^{-1})\\left(1 + \\frac{1}{R_{eff}} J P_0 J^T\\right)$$\n由于 $\\det(P_{post}^{-1}) = 1/\\det(P_{post})$ 且 $\\det(P_0^{-1}) = 1/\\det(P_0)$，我们得到：\n$$\\det(P_{post}) = \\frac{\\det(P_0)}{1 + (J P_0 J^T)/R_{eff}}$$\n项 $S = J P_0 J^T$ 是一个标量常数。熵减公式简化为：\n$$\\Delta H_i = \\frac{1}{2}\\ln\\left(\\frac{\\det(P_0)/(1 + S/R_{eff}^{current})}{\\det(P_0)/(1 + S/R_{eff}^{new})}\\right) = \\frac{1}{2}\\ln\\left(\\frac{1 + S/R_{eff}^{new}}{1 + S/R_{eff}^{current}}\\right)$$\n这个最终表达式在数值上稳定且计算高效。信息增加对应于 $V_{GP}^{new}(t_0)  V_{GP}^{current}(t_0)$，这使得 $R_{eff}^{new}  R_{eff}^{current}$。这导致对数内的比率大于1，从而产生正的熵减，正如预期的那样。\n\n**5. 高斯过程预测方差**\n\n给定训练输入 $T = \\{t_1, \\dots, t_N\\}$、核函数 $k(t,t')$ 和模拟器噪声方差 $\\sigma_n^2$，在测试点 $t_*$ 处的 GP 预测方差为：\n$$V_{GP}(t_* | T) = k(t_*, t_*) - \\mathbf{k}_*^T (K_{T,T} + \\sigma_n^2 I)^{-1} \\mathbf{k}_*$$\n其中：\n- $k(t, t') = \\sigma_f^2 \\exp(-\\frac{(t - t')^2}{2\\ell^2})$ 是平方指数核。\n- $K_{T,T}$ 是所有训练输入对之间的核函数求值构成的 $N \\times N$ 矩阵，$(K_{T,T})_{ij}=k(t_i, t_j)$。\n- $\\mathbf{k}_*$ 是训练输入和测试点之间的核函数求值构成的 $N \\times 1$ 向量，$(\\mathbf{k}_*)_i = k(t_i, t_*)$。\n- $I$ 是 $N \\times N$ 单位矩阵。\n\n计算过程首先使用初始训练集 $T$ 计算 $V_{GP}^{current}(t_0)$，然后对于每个候选点 $x_i$，使用扩充后的训练集 $T' = T \\cup \\{x_i\\}$ 计算 $V_{GP}^{new}(t_0)$。然后将这些方差代入简化的熵减公式中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import inv\n\ndef solve():\n    \"\"\"\n    Computes the expected reduction in differential entropy for a set of candidate\n    simulator locations, based on a Gaussian process surrogate model within a\n    Bayesian inverse problem framework.\n    \"\"\"\n\n    # Define the problem parameters as specified.\n    # Parameter space dimension d=2\n    # Prior covariance\n    P0 = np.array([[0.6, 0.1], \n                   [0.1, 0.4]])\n    # Jacobian of the forward model\n    J = np.array([[0.8, -0.3]])\n    # Measurement noise variance\n    R = 0.05\n    # Linearization point in scalar coordinate\n    t0 = 0.2\n    # GP kernel hyperparameters\n    sigma_f_sq = 1.2\n    ell = 0.5\n    # GP simulator noise variance\n    sigma_n_sq = 1e-4\n\n    # Initial set of GP training input locations\n    T = np.array([-0.8, 0.0, 0.9, 1.5])\n    \n    # Candidate new input locations for the GP\n    candidates = np.array([0.2, 0.0, 3.0, -3.0, 1.5])\n\n    # Pre-compute constant values for efficiency\n    ell_sq = ell**2\n    # This scalar term S = J * P0 * J^T appears in the entropy calculation\n    S = (J @ P0 @ J.T)[0, 0]\n\n    def kernel(t1, t2, sigma_f_sq_val, ell_sq_val):\n        \"\"\"\n        Squared-exponential kernel function.\n        Handles scalar and broadcastable numpy array inputs.\n        \"\"\"\n        sq_dist = np.subtract.outer(t1, t2)**2\n        return sigma_f_sq_val * np.exp(-0.5 * sq_dist / ell_sq_val)\n\n    def get_gp_pred_var(train_inputs, test_point, sigma_f_sq_val, ell_sq_val, sigma_n_sq_val):\n        \"\"\"\n        Computes the GP posterior predictive variance at a test point.\n        \"\"\"\n        # If there are no training points, predictive variance is the prior variance.\n        if train_inputs.size == 0:\n            return kernel(np.array([test_point]), np.array([test_point]), sigma_f_sq_val, ell_sq_val)[0,0]\n\n        # Build kernel matrices required for the predictive variance formula.\n        K_TT = kernel(train_inputs, train_inputs, sigma_f_sq_val, ell_sq_val)\n        K_TT_noisy = K_TT + sigma_n_sq_val * np.eye(len(train_inputs))\n        \n        k_T_star = kernel(train_inputs, np.array([test_point]), sigma_f_sq_val, ell_sq_val)\n        k_star_star = kernel(np.array([test_point]), np.array([test_point]), sigma_f_sq_val, ell_sq_val)[0,0]\n        \n        # Predictive variance formula: V(t*) = k(t*,t*) - k(t*,T) @ inv(K(T,T)+sigma_n^2*I) @ k(T,t*)\n        inv_K = inv(K_TT_noisy)\n        pred_var = k_star_star - k_T_star.T @ inv_K @ k_T_star\n        return pred_var.item()\n\n    # 1. Compute the state before adding any new point.\n    V_gp_current = get_gp_pred_var(T, t0, sigma_f_sq, ell_sq, sigma_n_sq)\n    R_eff_current = R + V_gp_current\n\n    results = []\n    # 2. Iterate through each candidate point.\n    for x_i in candidates:\n        # Create an augmented training set for the hypothetical new observation.\n        T_new = np.append(T, x_i)\n        \n        # Compute the new GP predictive variance at t0 with the augmented training set.\n        V_gp_new = get_gp_pred_var(T_new, t0, sigma_f_sq, ell_sq, sigma_n_sq)\n        \n        # Compute the new effective likelihood variance.\n        R_eff_new = R + V_gp_new\n        \n        # 3. Calculate the expected entropy reduction using the derived simplified formula.\n        # Delta_H = 0.5 * log( (1 + S/R_eff_new) / (1 + S/R_eff_current) )\n        term_current = 1.0 + S / R_eff_current\n        term_new = 1.0 + S / R_eff_new\n        \n        entropy_reduction = 0.5 * np.log(term_new / term_current)\n        results.append(entropy_reduction)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3423926"}]}