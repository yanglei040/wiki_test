{"hands_on_practices": [{"introduction": "为了建立对双层优化框架的核心直觉，我们从一个简化的标量问题开始。通过在理想化的设定下进行推导，我们可以获得正则化参数的解析解，从而清晰地揭示训练数据和验证数据如何共同决定最优超参数。这个练习旨在通过第一性原理加深您对超梯度概念的理解。[@problem_id:3368781]", "problem": "考虑一个标量线性逆问题，其中正向算子由一个正标量 $a > 0$ 表示，正则化算子由一个正标量 $l > 0$ 表示。给定一个训练数据 $y_{t} > 0$ 和一个验证数据 $y_{v} > 0$，并假设 $\\frac{y_{t}}{y_{v}} > 1$，我们的目标是通过一个双层优化框架来学习正则化参数 $\\alpha \\geq 0$。\n\n底层问题是Tikhonov正则化最小二乘估计器，它对于任何固定的 $\\alpha \\geq 0$ 求解\n$$\n\\min_{x \\in \\mathbb{R}} \\; \\frac{1}{2}\\left(a x - y_{t}\\right)^{2} + \\frac{1}{2} \\alpha \\left(l x\\right)^{2}.\n$$\n上层问题选择参数 $\\alpha$ 以最小化验证数据失配，\n$$\n\\min_{\\alpha \\geq 0} \\; \\frac{1}{2}\\left(a x_{\\alpha} - y_{v}\\right)^{2},\n$$\n其中 $x_{\\alpha}$ 表示对于给定的 $\\alpha$，底层问题的唯一最小化子。\n\n从最小二乘估计和Tikhonov正则化的定义出发，利用底层问题的一阶最优性条件以及对底层问题解映射的微分，推导出双层最优正则化参数 $\\alpha^{\\star}$ 关于 $a$、$l$、$y_{t}$ 和 $y_{v}$ 的闭式解析表达式。将最终答案表示为单个解析表达式。无需四舍五入，且不涉及物理单位。", "solution": "首先根据所需标准对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- 正向算子：一个正标量 $a > 0$。\n- 正则化算子：一个正标量 $l > 0$。\n- 训练数据：$y_{t} > 0$。\n- 验证数据：$y_{v} > 0$。\n- 先验条件：$\\frac{y_{t}}{y_{v}} > 1$。\n- 底层问题：$\\min_{x \\in \\mathbb{R}} \\; \\frac{1}{2}\\left(a x - y_{t}\\right)^{2} + \\frac{1}{2} \\alpha \\left(l x\\right)^{2}$，对于一个固定的 $\\alpha \\geq 0$。\n- 上层问题：$\\min_{\\alpha \\geq 0} \\; \\frac{1}{2}\\left(a x_{\\alpha} - y_{v}\\right)^{2}$，其中 $x_{\\alpha}$ 是底层问题的解。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据：** 该问题描述了一个标准的双层优化框架，用于在线性逆问题的Tikhonov正则化背景下学习超参数（正则化参数 $\\alpha$）。这是数值优化、逆问题和机器学习领域一个成熟且基础的课题。\n- **适定性：** 对于任何 $\\alpha \\geq 0$（因为 $a > 0, l > 0$），底层问题是最小化一个严格凸二次函数，这保证了唯一最小化子 $x_{\\alpha}$ 的存在。因此，上层问题是良定义的。条件 $\\frac{y_{t}}{y_{v}} > 1$ 至关重要，如下文所示，它确保了最优的 $\\alpha$ 为正，从而使问题结构良好。\n- **客观性：** 问题以精确的数学语言表达，没有主观或模棱两可的术语。\n- **完整性和一致性：** 提供了推导闭式解所需的所有变量、常数和条件。不存在内部矛盾。\n\n### 步骤 3：结论与行动\n该问题是有效的。将提供一个完整、有理有据的解法。\n\n### 解法推导\n\n推导过程分为三个主要步骤：\n1.  求解底层问题，以找到作为正则化参数 $\\alpha$ 函数的最优状态 $x_{\\alpha}$。\n2.  将此解代入上层问题，以获得一个仅依赖于 $\\alpha$ 的目标函数。\n3.  通过最小化此目标函数来求解关于 $\\alpha$ 的上层问题。\n\n**步骤 1：求解底层问题**\n\n底层目标函数由下式给出\n$$\nJ_{\\alpha}(x) = \\frac{1}{2}(ax - y_t)^2 + \\frac{1}{2}\\alpha (lx)^2.\n$$\n该函数是可微且凸的。通过将关于 $x$ 的一阶导数设为零（一阶最优性条件），可以找到唯一的最小化子 $x_{\\alpha}$：\n$$\n\\frac{dJ_{\\alpha}}{dx} = 0.\n$$\n计算导数，我们得到\n$$\n\\frac{dJ_{\\alpha}}{dx} = a(ax - y_t) + \\alpha l^2 x = a^2 x - ay_t + \\alpha l^2 x.\n$$\n将导数设为零可得\n$$\n(a^2 + \\alpha l^2)x - ay_t = 0.\n$$\n由于给定 $a > 0$ 和 $l > 0$，对于任何 $\\alpha \\geq 0$，项 $(a^2 + \\alpha l^2)$ 严格为正。因此，我们可以解出 $x$ 以获得解映射 $x_{\\alpha}$：\n$$\nx_{\\alpha} = \\frac{ay_t}{a^2 + \\alpha l^2}.\n$$\n\n**步骤 2：构建上层问题**\n\n上层问题是找到最小化验证误差的 $\\alpha$ 值。上层目标函数是\n$$\nE(\\alpha) = \\frac{1}{2}(ax_{\\alpha} - y_v)^2.\n$$\n将步骤1中 $x_{\\alpha}$ 的表达式代入 $E(\\alpha)$，我们得到一个仅依赖于 $\\alpha$ 的目标函数：\n$$\nE(\\alpha) = \\frac{1}{2} \\left( a \\left( \\frac{ay_t}{a^2 + \\alpha l^2} \\right) - y_v \\right)^2 = \\frac{1}{2} \\left( \\frac{a^2 y_t}{a^2 + \\alpha l^2} - y_v \\right)^2.\n$$\n\n**步骤 3：求解上层问题**\n\n我们寻求找到 $\\alpha^{\\star}$ 使得\n$$\n\\alpha^{\\star} = \\arg\\min_{\\alpha \\geq 0} E(\\alpha).\n$$\n由于 $E(\\alpha)$ 是非负的，其最小值为 $0$。如果我们能找到一个 $\\alpha \\geq 0$ 使得方括号内的项为零，则可以达到该最小值。让我们通过解以下方程来找到这样的 $\\alpha$：\n$$\n\\frac{a^2 y_t}{a^2 + \\alpha l^2} - y_v = 0.\n$$\n整理各项，我们得到\n$$\n\\frac{a^2 y_t}{a^2 + \\alpha l^2} = y_v.\n$$\n假设 $y_v > 0$，我们可以交叉相乘：\n$$\na^2 y_t = y_v (a^2 + \\alpha l^2).\n$$\n$$\na^2 y_t = a^2 y_v + \\alpha l^2 y_v.\n$$\n现在，我们分离出包含 $\\alpha$ 的项：\n$$\n\\alpha l^2 y_v = a^2 y_t - a^2 y_v = a^2(y_t - y_v).\n$$\n由于 $l > 0$ 且 $y_v > 0$，我们可以除以 $l^2 y_v$ 来解出 $\\alpha$：\n$$\n\\alpha^{\\star} = \\frac{a^2(y_t - y_v)}{l^2 y_v}.\n$$\n该表达式可以改写为\n$$\n\\alpha^{\\star} = \\frac{a^2}{l^2} \\left(\\frac{y_t - y_v}{y_v}\\right) = \\frac{a^2}{l^2} \\left(\\frac{y_t}{y_v} - 1\\right).\n$$\n我们必须验证此解满足约束条件 $\\alpha \\geq 0$。我们已知 $a > 0$ 和 $l > 0$，所以项 $\\frac{a^2}{l^2}$ 是正的。问题陈述中包含明确条件 $\\frac{y_t}{y_v} > 1$，这意味着项 $\\left(\\frac{y_t}{y_v} - 1\\right)$ 严格为正。因此，$\\alpha^{\\star} > 0$。\n\n这个值 $\\alpha^{\\star}$ 得出 $E(\\alpha^{\\star}) = 0$。由于对于所有 $\\alpha$ 都有 $E(\\alpha) \\geq 0$，这是该函数的全局最小值。因此，这就是双层最优正则化参数。\n\n最优正则化参数 $\\alpha^{\\star}$ 的最终解析表达式是：\n$$\n\\alpha^{\\star} = \\frac{a^2}{l^2} \\left(\\frac{y_t}{y_v} - 1\\right).\n$$", "answer": "$$\n\\boxed{\\frac{a^{2}}{l^{2}} \\left( \\frac{y_{t}}{y_{v}} - 1 \\right)}\n$$", "id": "3368781"}, {"introduction": "在掌握了理论基础之后，我们将转向更实际的计算任务，因为大多数现实世界中的问题无法求得解析解。这个练习将指导您为一个经典的Tikhonov正则化问题实现完整的双层优化流程，包括构建正向算子和正则化算子，并使用数值方法求解。通过解决一系列精心设计的测试案例，您将深入理解正则化参数在不同噪声和模型条件下的影响。[@problem_id:3368812]", "problem": "考虑一个具有确定性观测模型和二次正则化的线性逆问题。令 $A \\in \\mathbb{R}^{m \\times n}$ 表示一个已知的正向算子，$x \\in \\mathbb{R}^{n}$ 为未知状态，$y \\in \\mathbb{R}^{m}$ 为给定观测。对于给定的正则化参数 $\\lambda \\in \\mathbb{R}_{>0}$，其底层重构定义为 Tikhonov 泛函的唯一最小化子 $x_{\\lambda}$：\n$$\n\\Phi_{\\lambda}(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{1}{2}\\lambda \\|L x\\|_{2}^{2},\n$$\n其中 $L \\in \\mathbb{R}^{p \\times n}$ 是一个已知的线性算子，用于编码先验结构（例如，离散导数算子）。双层优化问题旨在通过最小化一个验证目标函数来学习正则化参数 $\\lambda$，该过程可利用已知的参考状态 $x^{\\star} \\in \\mathbb{R}^{n}$：\n$$\nJ(\\lambda) = \\frac{1}{2}\\|x_{\\lambda} - x^{\\star}\\|_{2}^{2}.\n$$\n您必须设计并实现一个完整的、可运行的程序，对于每个指定的测试用例，在标量变量 $\\lambda$ 上数值求解该双层优化问题，以在规定界限内产生一个最优值 $\\lambda^{\\star}$。该程序应对每个候选 $\\lambda$ 精确求解底层问题，然后使用科学上合理的数值方法，在给定区间上最小化上层目标。\n\n使用的基本假设和核心定义：\n- 观测物理过程建模为 $y = A x^{\\star} + \\varepsilon$，其中 $\\varepsilon \\in \\mathbb{R}^{m}$ 是确定性的。\n- 除非另有说明，正则化算子 $L$ 通过离散一阶差分来编码平滑性。\n- 当 $\\lambda > 0$ 且 $A^{\\top}A + \\lambda L^{\\top}L$ 是正定矩阵时，$\\Phi_{\\lambda}$ 的严格凸性保证了底层最小化子 $x_{\\lambda}$ 的唯一性。\n- 所有计算都在具有标准 2-范数的实值欧几里得空间中进行。\n\n您的程序必须从第一性原理出发，实现以下内容：\n1. 精确地按规定构建每个矩阵 $A$ 和算子 $L$。\n2. 对于任意给定的 $\\lambda$，通过求解从 $\\Phi_{\\lambda}$ 的一阶最优性条件导出的线性系统来计算 $x_{\\lambda}$。\n3. 按规定定义上层目标 $J(\\lambda)$，对任意 $\\lambda$ 求值，并在指定区间上执行稳健的一维最小化，以获得每个测试用例的 $\\lambda^{\\star}$。\n\n测试套件和数据规范：\n- 对于所有测试用例，正则化参数的搜索区间为 $\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]$，其中 $\\lambda_{\\min} = 10^{-6}$，$\\lambda_{\\max} = 10^{2}$。\n\n- 测试用例 1（一般适定模糊与轻度噪声；理想路径）：\n  - 维度：$n = 12$, $m = 18$。\n  - 正向算子构建：定义一个高斯模糊矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其元素为\n    $$\n    A_{ij} = \\exp\\left(-\\frac{\\left(\\frac{i-1}{m-1} - \\frac{j-1}{n-1}\\right)^{2}}{2\\sigma^{2}}\\right), \\quad \\sigma = 0.8,\n    $$\n    然后进行行归一化：对于每个 $i \\in \\{1,\\dots,m\\}$，将第 $i$ 行替换为 $A_{i\\cdot}/\\left(\\sum_{j=1}^{n} A_{ij}\\right)$。\n  - 正则化算子：$L \\in \\mathbb{R}^{(n-1) \\times n}$ 是一阶差分矩阵，其元素为 $L_{k,k} = -1$, $L_{k,k+1} = 1$（对于 $k \\in \\{1,\\dots,n-1\\}$），所有其他元素为 0。\n  - 参考状态：$x^{\\star} \\in \\mathbb{R}^{n}$，其分量为\n    $$\n    x^{\\star}_{j} = \\sin\\left(\\frac{2\\pi (j-1)}{n}\\right) + 0.2 \\sin\\left(\\frac{4\\pi (j-1)}{n}\\right), \\quad j \\in \\{1,\\dots,n\\}.\n    $$\n  - 观测：$y = A x^{\\star} + \\varepsilon$，其中确定性噪声 $\\varepsilon \\in \\mathbb{R}^{m}$ 由 $\\varepsilon_{i} = 0.01 \\cos\\left(\\frac{2\\pi (i-1)}{m}\\right)$ 给出。\n\n- 测试用例 2（更强的模糊和更高的噪声；测试灵敏度）：\n  - 维度：$n = 12$, $m = 18$。\n  - 正向算子构建：$A \\in \\mathbb{R}^{m \\times n}$ 的定义如测试用例 1，但 $\\sigma = 1.5$，然后应用相同的行归一化。\n  - 正则化算子：与测试用例 1 中相同的一阶差分矩阵 $L \\in \\mathbb{R}^{(n-1) \\times n}$。\n  - 参考状态：与测试用例 1 中相同的 $x^{\\star} \\in \\mathbb{R}^{n}$。\n  - 观测：$y = A x^{\\star} + \\varepsilon$，其中噪声为 $\\varepsilon_{i} = 0.05 \\cos\\left(\\frac{2\\pi (i-1)}{m}\\right)$。\n\n- 测试用例 3（单位正向模型和单位正则化；边界行为检查）：\n  - 维度：$n = 6$, $m = 6$。\n  - 正向算子：$A = I_{n}$，$n \\times n$ 单位矩阵。\n  - 正则化算子：$L = I_{n}$。\n  - 参考状态：$x^{\\star} = [1, 0.5, -0.3, 0.2, -0.1, 0]^{\\top}$。\n  - 观测：$y = [1.2, 0.4, -0.25, 0.1, -0.15, 0.05]^{\\top}$。\n\n- 测试用例 4（秩亏正向算子；测试正则化带来的稳定性）：\n  - 维度：$n = 10$, $m = 10$。\n  - 正向算子：$A = \\operatorname{diag}(d) \\in \\mathbb{R}^{n \\times n}$，其中\n    $$\n    d = [1, 1, 1, 1, 1, 0, 0, 0.5, 0.5, 0.5]^{\\top}.\n    $$\n  - 正则化算子：与测试用例 1 中相同的一阶差分矩阵 $L \\in \\mathbb{R}^{(n-1) \\times n}$。\n  - 参考状态：$x^{\\star}_{j} = \\sin\\left(\\frac{\\pi (j-1)}{n-1}\\right)$，对于 $j \\in \\{1,\\dots,n\\}$。\n  - 观测：$y = A x^{\\star} + \\varepsilon$，其中 $\\varepsilon_{i} = 0.02 (-1)^{i-1}$。\n\n计算要求：\n- 对于每个测试用例，在 $\\lambda \\in [10^{-6}, 10^{2}]$ 上最小化 $J(\\lambda)$。\n- 对于任何试验的 $\\lambda$，通过求解由 $\\Phi_{\\lambda}$ 的一阶条件所蕴含的线性系统来精确计算 $x_{\\lambda}$，使用数值稳定的线性代数方法。\n- 产生最终学到的参数值 $\\lambda^{\\star}$，为四舍五入到六位小数的浮点数。\n\n最终输出格式：\n- 您的程序应输出单行，包含用方括号括起来的逗号分隔列表形式的结果，例如 $[r_{1},r_{2},r_{3},r_{4}]$，其中每个 $r_{k}$ 是第 $k$ 个测试用例学到的 $\\lambda^{\\star}$，四舍五入到六位小数。\n- 输出中不允许有额外的文本或换行。", "solution": "我们从线性逆问题和二次正则化的核心定义出发，推导 Tikhonov 泛函的底层解，以及用于学习正则化参数的上层双层公式。\n\n底层目标由下式给出\n$$\n\\Phi_{\\lambda}(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{1}{2}\\lambda \\|L x\\|_{2}^{2},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$L \\in \\mathbb{R}^{p \\times n}$，$x \\in \\mathbb{R}^{n}$，$y \\in \\mathbb{R}^{m}$，以及 $\\lambda \\in \\mathbb{R}_{>0}$。在 $A^{\\top}A + \\lambda L^{\\top}L$ 是正定矩阵的条件下，泛函 $\\Phi_{\\lambda}$ 关于 $x$ 是严格凸的。当 $\\lambda > 0$ 且 $L$ 对 $A$ 的零空间分量施加足够的惩罚以正则化该问题时，此条件得到保证。\n\n为了计算最小化子 $x_{\\lambda}$，我们援引凸优化的第一性原理：在最小化点，关于 $x$ 的梯度为零。$\\Phi_{\\lambda}$ 的梯度为\n$$\n\\nabla_{x} \\Phi_{\\lambda}(x) = A^{\\top}(A x - y) + \\lambda L^{\\top}(L x).\n$$\n令 $\\nabla_{x} \\Phi_{\\lambda}(x) = 0$ 可得正规方程\n$$\n\\left(A^{\\top}A + \\lambda L^{\\top}L\\right) x = A^{\\top} y.\n$$\n因此，唯一的最小化子 $x_{\\lambda}$ 可通过求解以下线性系统得到\n$$\nH(\\lambda) x_{\\lambda} = A^{\\top} y, \\quad \\text{其中 } H(\\lambda) = A^{\\top}A + \\lambda L^{\\top}L.\n$$\n由于给定问题结构，$H(\\lambda)$ 在 $\\lambda > 0$ 时是对称正定的，因此可以使用 Cholesky 分解或稳定的直接求解器等数值线性代数方法来计算 $x_{\\lambda}$。\n\n双层优化定义了一个上层目标，该目标衡量重构状态与已知参考状态 $x^{\\star}$ 之间的差异：\n$$\nJ(\\lambda) = \\frac{1}{2}\\|x_{\\lambda} - x^{\\star}\\|_{2}^{2}.\n$$\n我们的目标是计算\n$$\n\\lambda^{\\star} = \\operatorname{argmin}_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} J(\\lambda),\n$$\n其中，对每个测试用例，区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 指定为 $[10^{-6}, 10^{2}]$。\n\n针对此双层问题的一个基于原理的算法采用以下步骤：\n- 对于任何候选 $\\lambda$，构建矩阵 $H(\\lambda) = A^{\\top} A + \\lambda L^{\\top} L$ 和右侧项 $b = A^{\\top} y$。\n- 使用数值稳定的线性求解器精确求解 $H(\\lambda) x_{\\lambda} = b$。\n- 评估上层目标 $J(\\lambda) = \\frac{1}{2}\\|x_{\\lambda} - x^{\\star}\\|_{2}^{2}$。\n- 通过稳健的一维数值优化（例如，区间限定和黄金分割搜索或有界标量最小化），在 $\\lambda \\in [10^{-6}, 10^{2}]$ 上最小化 $J(\\lambda)$，利用由 $x_{\\lambda}$ 对 $\\lambda$ 的隐式依赖性所诱导的 $J$ 在 $\\lambda$ 上的连续性和可微性。\n\n为求完整，我们概述了可以导出 $J(\\lambda)$ 梯度的隐式微分方法，尽管在此问题中，无导数的有界最小化方法已经足够且在数值上是稳健的。对正规方程关于 $\\lambda$ 求导：\n$$\n\\frac{d}{d\\lambda}\\left(H(\\lambda) x_{\\lambda}\\right) = \\frac{d}{d\\lambda}\\left(A^{\\top}y\\right) = 0,\n$$\n这给出\n$$\n\\left(\\frac{dH}{d\\lambda}\\right) x_{\\lambda} + H(\\lambda) \\frac{d x_{\\lambda}}{d\\lambda} = 0.\n$$\n由于 $\\frac{dH}{d\\lambda} = L^{\\top}L$，我们得到\n$$\nH(\\lambda) \\frac{d x_{\\lambda}}{d\\lambda} = - L^{\\top}L\\, x_{\\lambda},\n$$\n因此\n$$\n\\frac{d x_{\\lambda}}{d\\lambda} = - H(\\lambda)^{-1} L^{\\top}L\\, x_{\\lambda}.\n$$\n令 $r(\\lambda) = x_{\\lambda} - x^{\\star}$。上层目标的梯度为\n$$\n\\frac{dJ}{d\\lambda} = r(\\lambda)^{\\top} \\frac{d x_{\\lambda}}{d\\lambda} = - r(\\lambda)^{\\top} H(\\lambda)^{-1} L^{\\top}L\\, x_{\\lambda}.\n$$\n该表达式由链式法则和二次型的线性性质得出。在实践中，可以通过求解一个与 $H(\\lambda)$ 矩阵相同、右侧项为 $L^{\\top}L\\, x_{\\lambda}$ 的线性系统来计算 $H(\\lambda)^{-1} L^{\\top}L\\, x_{\\lambda}$，从而为提高效率而重用矩阵分解。\n\n然而，鉴于 $\\lambda$ 的标量性质和 $J(\\lambda)$ 的平滑性，一个有界的、无导数的标量最小化方法就足够了，并且可以避免梯度精度可能带来的数值上的细微问题。因此，我们采用一种有界最小化策略，它在整个区间上评估 $J(\\lambda)$ 并收敛到 $\\lambda^{\\star}$。\n\n每个测试用例的实现细节遵循所要求的科学构建方式：\n- 对于测试用例 1 和测试用例 2 中的高斯模糊算子，我们使用规定的公式构建 $A$：\n  $$\n  A_{ij} = \\exp\\left(-\\frac{\\left(\\frac{i-1}{m-1} - \\frac{j-1}{n-1}\\right)^{2}}{2\\sigma^{2}}\\right),\n  $$\n  然后通过除以 $\\sum_{j=1}^{n} A_{ij}$ 来归一化每一行 $i$，以保持跨测量的一致缩放。这模拟了一个在连续网格映射到离散矩阵之间的平滑类卷积正向模型，与适定的数值离散化一致。\n- 一阶差分算子 $L$ 构建为 $L_{k,k} = -1$ 和 $L_{k,k+1} = 1$（对于 $k \\in \\{1,\\dots,n-1\\}$），产生一个强制平滑性的离散梯度惩罚。\n- 参考状态和确定性噪声由指定的三角序列定义，确保了一个可复现的、科学上有意义的测试套件。\n\n各测试用例的数值算法总结：\n- 精确定义 $A$、$L$、$x^{\\star}$ 和 $y$。\n- 对于 $\\lambda \\in [10^{-6}, 10^{2}]$，通过求解 $H(\\lambda) x_{\\lambda} = A^{\\top} y$ 来计算 $x_{\\lambda}$。\n- 评估 $J(\\lambda) = \\frac{1}{2}\\|x_{\\lambda} - x^{\\star}\\|_{2}^{2}$。\n- 使用有界标量优化方法最小化 $J(\\lambda)$ 以获得 $\\lambda^{\\star}$。\n- 将 $\\lambda^{\\star}$ 四舍五入到六位小数。\n\n此方法整合了逆问题和双层优化的基本原理：底层问题的严格凸性、通过正规方程求解线性系统，以及通过最小化平滑的验证目标进行标量上层参数学习。最终的程序将遵守指定的执行环境，并生成包含所有测试用例学到的参数的单行输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef first_difference_matrix(n: int) -> np.ndarray:\n    \"\"\"Construct the (n-1) x n first-difference matrix L.\"\"\"\n    L = np.zeros((n - 1, n))\n    for k in range(n - 1):\n        L[k, k] = -1.0\n        L[k, k + 1] = 1.0\n    return L\n\ndef gaussian_blur_matrix(m: int, n: int, sigma: float) -> np.ndarray:\n    \"\"\"Construct an m x n Gaussian blur matrix with row-wise normalization.\"\"\"\n    A = np.zeros((m, n))\n    # Grid points scaled to [0,1]\n    for i in range(m):\n        xi = i / (m - 1) if m > 1 else 0.0\n        for j in range(n):\n            xj = j / (n - 1) if n > 1 else 0.0\n            diff = xi - xj\n            A[i, j] = np.exp(- (diff * diff) / (2.0 * sigma * sigma))\n    # Row-wise normalization to keep consistent measurement scale\n    row_sums = A.sum(axis=1, keepdims=True)\n    # Avoid division by zero; if a row sum is zero, keep the row as zeros\n    row_sums[row_sums == 0.0] = 1.0\n    A = A / row_sums\n    return A\n\ndef solve_lower_level(A: np.ndarray, L: np.ndarray, y: np.ndarray, lam: float) -> np.ndarray:\n    \"\"\"Solve (A^T A + lam L^T L) x = A^T y for x.\"\"\"\n    AtA = A.T @ A\n    LtL = L.T @ L\n    H = AtA + lam * LtL\n    b = A.T @ y\n    # Use a stable solver; for small sizes, numpy.linalg.solve is sufficient\n    x = np.linalg.solve(H, b)\n    return x\n\ndef upper_objective(lam: float, A: np.ndarray, L: np.ndarray, y: np.ndarray, x_star: np.ndarray) -> float:\n    \"\"\"Compute J(lam) = 0.5 * ||x_lam - x_star||^2.\"\"\"\n    x_lam = solve_lower_level(A, L, y, lam)\n    r = x_lam - x_star\n    return 0.5 * float(r.T @ r)\n\ndef learn_lambda(A: np.ndarray, L: np.ndarray, y: np.ndarray, x_star: np.ndarray,\n                 lam_min: float = 1e-6, lam_max: float = 1e2) -> float:\n    \"\"\"Minimize the upper-level objective over [lam_min, lam_max] to learn lambda.\"\"\"\n    # Use bounded scalar minimization for robust 1D search\n    res = minimize_scalar(\n        lambda lam: upper_objective(lam, A, L, y, x_star),\n        bounds=(lam_min, lam_max),\n        method='bounded',\n        options={'xatol': 1e-12, 'maxiter': 500}\n    )\n    return float(res.x)\n\ndef test_case_1():\n    # n = 12, m = 18, sigma = 0.8\n    n, m, sigma = 12, 18, 0.8\n    A = gaussian_blur_matrix(m, n, sigma)\n    L = first_difference_matrix(n)\n    # x_star[j] = sin(2*pi*(j-1)/n) + 0.2 * sin(4*pi*(j-1)/n)\n    j_idx = np.arange(n)\n    x_star = np.sin(2.0 * np.pi * (j_idx) / n) + 0.2 * np.sin(4.0 * np.pi * (j_idx) / n)\n    # y = A x_star + eps, eps[i] = 0.01 * cos(2*pi*(i-1)/m)\n    i_idx = np.arange(m)\n    eps = 0.01 * np.cos(2.0 * np.pi * (i_idx) / m)\n    y = A @ x_star + eps\n    lam = learn_lambda(A, L, y, x_star)\n    return lam\n\ndef test_case_2():\n    # n = 12, m = 18, sigma = 1.5\n    n, m, sigma = 12, 18, 1.5\n    A = gaussian_blur_matrix(m, n, sigma)\n    L = first_difference_matrix(n)\n    # x_star as in test case 1\n    j_idx = np.arange(n)\n    x_star = np.sin(2.0 * np.pi * (j_idx) / n) + 0.2 * np.sin(4.0 * np.pi * (j_idx) / n)\n    # y = A x_star + eps, eps[i] = 0.05 * cos(2*pi*(i-1)/m)\n    i_idx = np.arange(m)\n    eps = 0.05 * np.cos(2.0 * np.pi * (i_idx) / m)\n    y = A @ x_star + eps\n    lam = learn_lambda(A, L, y, x_star)\n    return lam\n\ndef test_case_3():\n    # n = 6, m = 6, A = I, L = I\n    n = 6\n    A = np.eye(n)\n    L = np.eye(n)\n    x_star = np.array([1.0, 0.5, -0.3, 0.2, -0.1, 0.0], dtype=float)\n    y = np.array([1.2, 0.4, -0.25, 0.1, -0.15, 0.05], dtype=float)\n    lam = learn_lambda(A, L, y, x_star)\n    return lam\n\ndef test_case_4():\n    # n = 10, m = 10, A = diag(d) with rank deficiency\n    n = 10\n    d = np.array([1, 1, 1, 1, 1, 0, 0, 0.5, 0.5, 0.5], dtype=float)\n    A = np.diag(d)\n    L = first_difference_matrix(n)\n    j_idx = np.arange(n)\n    x_star = np.sin(np.pi * (j_idx) / (n - 1))\n    i_idx = np.arange(n)\n    eps = 0.02 * ((-1.0) ** i_idx)\n    y = A @ x_star + eps\n    lam = learn_lambda(A, L, y, x_star)\n    return lam\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        test_case_1,\n        test_case_2,\n        test_case_3,\n        test_case_4,\n    ]\n\n    results = []\n    for case_fn in test_cases:\n        lam_opt = case_fn()\n        results.append(lam_opt)\n\n    # Format results rounded to six decimals\n    formatted = [f\"{r:.6f}\" for r in results]\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```", "id": "3368812"}, {"introduction": "现实中的许多问题，特别是需要稀疏解的问题，会使用像 $\\ell_1$ 范数这样的非光滑正则化项。在这种情况下，简单的梯度计算方法不再适用，我们需要更强大的工具。本练习将引导您探索基于KKT条件和定常点假设的隐式微分方法，来计算非光滑问题中的超梯度，这是处理现代机器学习和信号处理中复杂优化问题的关键技能。[@problem_id:3368774]", "problem": "给定一个双层优化设置，用于在线性逆问题中学习一个带有非光滑惩罚项的正则化参数。内层问题旨在求解强凸复合目标函数的唯一最小化子 $x^\\star(\\lambda)$\n$$\n\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2}\\|A x - y_{\\text{train}}\\|_2^2 + \\frac{\\gamma}{2}\\|x\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个已知的前向算子，$y_{\\text{train}} \\in \\mathbb{R}^m$ 是给定的训练数据，$\\gamma \\in \\mathbb{R}_{>0}$ 是确保强凸性的一个正常数，$\\lambda \\in \\mathbb{R}_{\\ge 0}$ 是待学习的正则化权重。外层目标通过以下方式在验证数据上评估学习到的估计器\n$$\nJ(\\lambda) \\;=\\; \\frac{1}{2}\\|B x^\\star(\\lambda) - y_{\\text{val}}\\|_2^2,\n$$\n其中给定 $B \\in \\mathbb{R}^{p \\times n}$ 和 $y_{\\text{val}} \\in \\mathbb{R}^p$。\n\n您的任务是实现一个程序，在非光滑 $\\ell_1$ 项的固定激活集假设下，使用基于 Karush–Kuhn–Tucker (KKT) 条件的微分方法计算导数 $\\frac{dJ}{d\\lambda}$。该过程必须在科学上基于以下原则：\n- 复合内层目标是强凸的，因此对于每个 $\\lambda$，$x^\\star(\\lambda)$ 是唯一的。\n- $\\ell_1$ 范数的次微分为 $\\partial \\|x\\|_1 = \\{ z \\in \\mathbb{R}^n \\,|\\, z_i = \\operatorname{sign}(x_i) \\text{ if } x_i \\neq 0; \\, z_i \\in [-1,1] \\text{ if } x_i = 0 \\}$。\n- 在 $x^\\star(\\lambda)$ 的支撑集和符号保持恒定的 $\\lambda$ 区域上，KKT 条件产生一个可微的隐式关系，该关系可通过求解限制在激活集上的线性系统来计算 $\\frac{d x^\\star}{d\\lambda}$。\n- 如果支撑集为空，则 $x^\\star(\\lambda) = 0$ 且导数 $\\frac{d x^\\star}{d\\lambda} = 0$。\n\n算法要求：\n1. 对于给定的 $\\lambda$，使用适用于非光滑惩罚项的一阶方法（例如迭代收缩阈值算法，ISTA）求解内层问题，以获得 $x^\\star(\\lambda)$。\n2. 识别激活集 $S = \\{ i \\in \\{1,\\dots,n\\} \\,|\\, x^\\star_i(\\lambda) \\neq 0\\}$ 和相应的符号向量 $s_S = \\operatorname{sign}(x^\\star_S(\\lambda))$。\n3. 假设激活集和符号在 $\\lambda$ 的局部是固定的，通过求解由限制在 $S$ 上的 KKT 条件所蕴含的相应线性系统来计算在 $S$ 上的 $\\frac{d x^\\star}{d\\lambda}$。对于 $i \\notin S$，设置 $\\frac{d x^\\star_i}{d\\lambda} = 0$。使用此敏感度和链式法则来获得 $\\frac{dJ}{d\\lambda}$。\n4. 在特殊情况 $S = \\emptyset$ 下，设置 $\\frac{dJ}{d\\lambda} = 0$。\n\n对所有测试用例使用以下固定数据：\n- 维度：$m=5$，$n=4$，$p=4$。\n- 矩阵 $A$：\n$$\nA = \\begin{bmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1 \\\\\n1  1  1  1\n\\end{bmatrix}.\n$$\n- 训练数据 $y_{\\text{train}}$：\n$$\ny_{\\text{train}} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\\\ 0.5 \\\\ 0 \\end{bmatrix}.\n$$\n- 强凸性参数 $\\gamma$：\n$$\n\\gamma = 0.2.\n$$\n- 矩阵 $B$（$\\mathbb{R}^{4 \\times 4}$ 中的单位矩阵）：\n$$\nB = I_4.\n$$\n- 验证数据 $y_{\\text{val}}$：\n$$\ny_{\\text{val}} = \\begin{bmatrix} 0.9 \\\\ -1.2 \\\\ 1.8 \\\\ 0.4 \\end{bmatrix}.\n$$\n\n$\\lambda$ 的测试套件：\n- 情况 1 (正常路径，混合支撑集)：$\\lambda = 0.3$。\n- 情况 2 (边界情况，预期支撑集为空)：$\\lambda = 100.0$。\n- 情况 3 (低正则化，可能为全支撑集)：$\\lambda = 0.01$。\n\n您的程序必须：\n- 实现上述算法，包括内层求解器和基于 KKT 的微分。\n- 生成单行输出，包含三个测试用例的导数值，格式为方括号内以逗号分隔的列表（例如，“[result1,result2,result3]”）。\n- 每个结果必须是一个浮点数（无单位）。\n\n不需要用户输入。您的实现必须是确定性的和自包含的。", "solution": "该问题是有效的，因为它在科学上基于逆问题的双层优化这一成熟领域，在数学上是适定的，并且所有提供的数据和条件都是完整、一致和客观的。\n\n任务是计算外层目标函数 $J(\\lambda)$ 相对于正则化参数 $\\lambda$ 的导数。参数 $\\lambda$ 是一个内层优化问题的一部分，该问题定义了一个中间变量 $x^\\star(\\lambda)$。\n\n内层优化问题是找到复合目标函数 $F(x, \\lambda)$ 的唯一最小化子 $x^\\star(\\lambda)$：\n$$\nx^\\star(\\lambda) = \\arg\\min_{x \\in \\mathbb{R}^n} F(x, \\lambda)\n$$\n其中\n$$\nF(x, \\lambda) = \\underbrace{\\frac{1}{2}\\|A x - y_{\\text{train}}\\|_2^2 + \\frac{\\gamma}{2}\\|x\\|_2^2}_{f(x)} + \\underbrace{\\lambda \\|x\\|_1}_{g(x, \\lambda)}.\n$$\n函数 $f(x)$ 是光滑且强凸的（因为 $\\gamma > 0$），而 $g(x, \\lambda)$ 是凸但非光滑的。$F(x, \\lambda)$ 的强凸性保证了对于任何给定的 $\\lambda \\ge 0$，最小化子 $x^\\star(\\lambda)$ 是唯一的。\n\n外层目标函数 $J(\\lambda)$ 定义为：\n$$\nJ(\\lambda) = \\frac{1}{2}\\|B x^\\star(\\lambda) - y_{\\text{val}}\\|_2^2.\n$$\n我们的目标是计算导数 $\\frac{dJ}{d\\lambda}$。\n\n整个过程包括三个主要步骤：\n1.  求解内层优化问题以找到给定 $\\lambda$ 的 $x^\\star(\\lambda)$。\n2.  通过对内层问题的最优性条件进行微分，计算解的敏感度 $\\frac{dx^\\star}{d\\lambda}$。\n3.  使用链式法则计算最终导数 $\\frac{dJ}{d\\lambda}$。\n\n**步骤 1：求解内层问题**\n\n内层目标是一个光滑可微函数 $f(x)$ 和一个非光滑但可计算邻近算子的函数 $g(x, \\lambda)$ 的和。这种结构非常适合使用邻近梯度法，例如迭代收缩阈值算法 (Iterative Shrinkage-Thresholding Algorithm, ISTA)。\n\n光滑部分 $f(x)$ 的梯度是：\n$$\n\\nabla f(x) = A^T(Ax - y_{\\text{train}}) + \\gamma x.\n$$\n非光滑部分 $g(x, \\lambda) = \\lambda \\|x\\|_1$ 的邻近算子是软阈值算子 $S_{\\alpha\\lambda}(\\cdot)$，定义为：\n$$\n\\text{prox}_{\\alpha g(\\cdot, \\lambda)}(v) = S_{\\alpha\\lambda}(v) = \\text{sign}(v) \\odot \\max(|v| - \\alpha\\lambda, 0),\n$$\n其中运算是逐元素的。参数 $\\alpha$ 是步长。\n\nISTA 更新规则由下式给出：\n$$\nx_{k+1} = S_{\\alpha\\lambda}(x_k - \\alpha \\nabla f(x_k)).\n$$\n为了保证收敛，步长 $\\alpha$ 必须满足 $0  \\alpha  2/L$，其中 $L$ 是 $\\nabla f(x)$ 的 Lipschitz 常数。一个常见且安全的选择是 $\\alpha = 1/L$。Lipschitz 常数 $L$ 是 $f(x)$ 的 Hessian 矩阵的最大特征值，该矩阵为：\n$$\nH = \\nabla^2 f(x) = A^T A + \\gamma I.\n$$\n由于 $H$ 是一个常数正定矩阵，$L = \\lambda_{\\max}(H)$，可以预先计算。我们从一个初始猜测（例如，$x_0 = 0$）开始迭代 ISTA 更新规则，直到收敛以获得 $x^\\star(\\lambda)$。\n\n**步骤 2：对解映射 $x^\\star(\\lambda)$ 进行微分**\n\n内层问题的 Karush-Kuhn-Tucker (KKT) 最优性条件表明，在解 $x^\\star = x^\\star(\\lambda)$ 处，必须存在一个向量 $z \\in \\partial \\|x^\\star\\|_1$，使得：\n$$\n\\nabla f(x^\\star) + \\lambda z = 0.\n$$\n$\\ell_1$ 范数的次微分为 $\\partial \\|x\\|_1 = \\{ z \\in \\mathbb{R}^n \\,|\\, z_i = \\operatorname{sign}(x_i) \\text{ if } x_i \\neq 0; \\, z_i \\in [-1,1] \\text{ if } x_i = 0 \\}$。\n\n令 $S = \\{i \\mid x^\\star_i \\neq 0\\}$ 为激活集，$N = \\{i \\mid x^\\star_i = 0\\}$ 为非激活集。对于索引 $i \\in S$，KKT 条件变为一个等式：$(\\nabla f(x^\\star))_i + \\lambda \\operatorname{sign}(x_i^\\star) = 0$。\n\n我们假设对于 $\\lambda$ 的一个小的扰动，激活集 $S$ 及其分量的符号 $s_S = \\operatorname{sign}(x^\\star_S)$ 保持不变。这意味着对于 $i \\in N$，$x_i^\\star$ 保持为零，因此 $\\frac{dx^\\star_i}{d\\lambda} = 0$。我们可以对 $i \\in S$ 的 KKT 等式条件关于 $\\lambda$ 进行微分：\n$$\n\\frac{d}{d\\lambda} \\left[ (\\nabla f(x^\\star(\\lambda)))_i + \\lambda s_i \\right] = 0, \\quad \\forall i \\in S.\n$$\n使用链式法则，这变为：\n$$\n\\sum_{j=1}^n \\frac{\\partial (\\nabla f(x^\\star))_i}{\\partial x_j} \\frac{dx^\\star_j}{d\\lambda} + s_i = 0.\n$$\n项 $\\frac{\\partial (\\nabla f(x^\\star))_i}{\\partial x_j}$ 就是 Hessian 矩阵 $H = A^TA + \\gamma I$ 的第 $(i, j)$ 个元素。因为我们假设对于 $j \\notin S$ 有 $\\frac{dx^\\star_j}{d\\lambda} = 0$，所以求和可以简化为只对 $S$ 中的索引进行：\n$$\n\\sum_{j \\in S} H_{ij} \\frac{dx^\\star_j}{d\\lambda} + s_i = 0, \\quad \\forall i \\in S.\n$$\n这就形成了一个关于激活集上导数分量的线性方程组，我们用向量 $\\frac{dx^\\star_S}{d\\lambda}$ 表示：\n$$\nH_{SS} \\frac{dx^\\star_S}{d\\lambda} = -s_S,\n$$\n其中 $H_{SS}$ 是 $H$ 的子矩阵，其行和列对应于激活集 $S$。由于 $H$ 是正定的，$H_{SS}$ 也是正定的，因此是可逆的。我们可以求解这个系统来找到 $\\frac{dx^\\star_S}{d\\lambda}$。然后，通过将这些解分量放置在激活索引处，并在非激活索引处放置零，来构成完整的导数向量 $\\frac{dx^\\star}{d\\lambda}$。\n\n如果激活集 $S$ 为空（即 $x^\\star(\\lambda)=0$），则 $\\frac{dx^\\star}{d\\lambda} = 0$。\n\n**步骤 3：计算外层目标梯度**\n\n最后，我们对外层目标 $J(\\lambda)$ 应用链式法则：\n$$\n\\frac{dJ}{d\\lambda} = \\nabla_x J(x^\\star(\\lambda))^T \\frac{dx^\\star}{d\\lambda}.\n$$\n$J$ 相对于 $x$ 的梯度是：\n$$\n\\nabla_x J(x) = \\nabla_x \\left( \\frac{1}{2}\\|Bx - y_{\\text{val}}\\|_2^2 \\right) = B^T(Bx - y_{\\text{val}}).\n$$\n代入 $x = x^\\star(\\lambda)$ 和计算出的导数 $\\frac{dx^\\star}{d\\lambda}$，我们得到最终结果：\n$$\n\\frac{dJ}{d\\lambda} = (B^T(Bx^\\star(\\lambda) - y_{\\text{val}}))^T \\frac{dx^\\star}{d\\lambda}.\n$$\n如果激活集为空，$\\frac{dx^\\star}{d\\lambda}$ 是零向量，这正确地意味着 $\\frac{dJ}{d\\lambda}=0$。这便完成了算法的推导。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the derivative of the outer objective J with respect to lambda\n    for a bilevel optimization problem using KKT-based differentiation.\n    \"\"\"\n    \n    # Define the fixed data from the problem statement\n    m, n, p = 5, 4, 4\n    A = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1],\n        [1, 1, 1, 1]\n    ], dtype=np.float64)\n    \n    y_train = np.array([1, -1, 2, 0.5, 0], dtype=np.float64)\n    gamma = 0.2\n    \n    # B is the 4x4 identity matrix\n    B = np.eye(p, dtype=np.float64)\n    y_val = np.array([0.9, -1.2, 1.8, 0.4], dtype=np.float64)\n\n    # Test cases for lambda\n    test_lambdas = [0.3, 100.0, 0.01]\n\n    # --- Pre-computation ---\n    # Hessian of the smooth part of the inner objective\n    H = A.T @ A + gamma * np.eye(n, dtype=np.float64)\n    # Lipschitz constant for ISTA step size\n    L = np.max(np.linalg.eigvalsh(H))\n    # ISTA step size\n    alpha = 1.0 / L\n\n    # Helper function for the soft-thresholding operator\n    def soft_threshold(v, t):\n        return np.sign(v) * np.maximum(np.abs(v) - t, 0.0)\n\n    # Helper function for the ISTA solver\n    def run_ista(lambda_val, max_iter=10000):\n        x = np.zeros(n, dtype=np.float64)\n        for _ in range(max_iter):\n            grad_f = A.T @ (A @ x - y_train) + gamma * x\n            x_update = x - alpha * grad_f\n            x = soft_threshold(x_update, alpha * lambda_val)\n        return x\n\n    results = []\n    for lambda_val in test_lambdas:\n        # Step 1: Solve the inner problem to find x_star(lambda)\n        x_star = run_ista(lambda_val)\n\n        # Step 2: Identify the active set\n        # Use a small tolerance to account for floating-point inaccuracies\n        tol = 1e-9\n        active_set_indices = np.where(np.abs(x_star) > tol)[0]\n\n        # Step 3  4: Compute the derivative dJ/dlambda\n        if active_set_indices.size == 0:\n            # If the active set is empty, x_star is 0, so dx_star/dlambda is 0.\n            # This makes dJ/dlambda = 0.\n            dJ_dlambda = 0.0\n        else:\n            # Get signs of the active components\n            s_S = np.sign(x_star[active_set_indices])\n            \n            # Extract the submatrix H_SS from the Hessian H\n            H_SS = H[np.ix_(active_set_indices, active_set_indices)]\n            \n            # Solve the linear system: H_SS * dx_S/dlambda = -s_S\n            dx_star_S_dlambda = np.linalg.solve(H_SS, -s_S)\n            \n            # Construct the full derivative vector dx_star/dlambda\n            dx_star_dlambda = np.zeros(n, dtype=np.float64)\n            dx_star_dlambda[active_set_indices] = dx_star_S_dlambda\n            \n            # Compute the gradient of the outer objective J with respect to x\n            grad_J_x = B.T @ (B @ x_star - y_val)\n            \n            # Compute dJ/dlambda using the chain rule\n            dJ_dlambda = grad_J_x.T @ dx_star_dlambda\n        \n        results.append(dJ_dlambda)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3368774"}]}