{"hands_on_practices": [{"introduction": "理论的价值在于实践。本练习旨在将广义交叉验证（GCV）的理论转化为高效的数值算法。我们将从第一性原理出发，推导基于奇异值分解（SVD）的吉洪诺夫（Tikhonov）正则化解以及相应的GCV函数形式，然后通过编写代码来解决一个实际的线性逆问题。这个过程不仅能加深对GCV背后数学原理的理解，还能锻炼将复杂公式转化为可执行代码的核心工程能力。[@problem_id:3419911]", "problem": "考虑一个线性离散逆问题，即从带有噪声的观测值 $b \\in \\mathbb{R}^m$ 中估计一个未知状态向量 $x \\in \\mathbb{R}^n$。它们通过一个已知的前向算子 $A \\in \\mathbb{R}^{m \\times n}$ 和模型 $b = A x + \\varepsilon$ 相关联，其中 $\\varepsilon$ 代表加性测量噪声。我们专注于零阶吉洪诺夫（Tikhonov）正则化，其中 $x$ 通过最小化吉洪诺夫泛函 $\\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2$ 来估计，正则化参数为 $\\alpha > 0$。估计必须使用奇异值分解（Singular Value Decomposition, SVD）来表示，并且其解释必须用滤波因子（filter factors）来给出。拟合质量应使用广义交叉验证（Generalized Cross-Validation, GCV）进行评估（广义交叉验证（GCV）会对线性预测算子的影响进行惩罚）。\n\n从以下基本出发点开始：\n- 离散前向模型 $b = A x + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}^m$。\n- 零阶吉洪诺夫正则化解 $\\hat{x}_\\alpha$ 是 $\\min_{x} \\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2$ 的解，其中 $\\alpha > 0$。\n- $A$ 的奇异值分解（SVD）为 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{m \\times r}$，$V \\in \\mathbb{R}^{n \\times r}$，$r = \\mathrm{rank}(A) \\le \\min(m,n)$，$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是对角矩阵，其对角线上的正奇异值为 $\\sigma_i$。\n- 影响矩阵（也称为帽子矩阵）是 $H_\\alpha \\in \\mathbb{R}^{m \\times m}$，使得预测数据为 $\\hat{b}_\\alpha = A \\hat{x}_\\alpha = H_\\alpha b$。\n\n你的任务是：\n1. 从第一性原理出发，推导基于 SVD 的吉洪诺夫解 $\\hat{x}_\\alpha$ 的形式，并根据奇异值 $\\sigma_i$ 确定相应的滤波因子。\n2. 推导影响矩阵 $H_\\alpha$，并用奇异值 $\\sigma_i$ 和正则化参数 $\\alpha$ 表示其迹。\n3. 推导零阶吉洪诺夫估计器的广义交叉验证（GCV）泛函 $G(\\alpha)$，使其仅依赖于从矩阵 $A$ 的 SVD 和数据向量 $b$ 计算出的量，而无需构建除 SVD 所必需的任何大型密集矩阵。GCV 泛函必须推导为依赖于残差范数 $\\|A \\hat{x}_\\alpha - b\\|_2$ 和 $\\mathrm{trace}(H_\\alpha)$ 的函数。\n4. 实现一个数值算法，该算法：\n   - 计算 $A$ 的 SVD。\n   - 在 $\\alpha \\in [10^{-8}, 10^{2}]$ 的对数间隔网格上使用 $200$ 个点评估 $G(\\alpha)$，然后通过在最小化 $\\alpha$ 周围的一个乘法因子为 $10$ 的范围内（同时遵守原始边界）的新对数网格（包含 200 个点）上搜索来进行优化。\n   - 对于每个测试用例，返回最小化正则化参数 $\\alpha^\\star$、残差范数的平方 $\\|A \\hat{x}_{\\alpha^\\star} - b\\|_2^2$ 和 $\\mathrm{trace}(H_{\\alpha^\\star})$；所有值都必须以双精度计算。\n5. 对所有三角函数使用弧度制。\n\n测试套件：\n- 案例 1（理想情况，轻度病态，超定）：设 $m = 50$, $n = 30$。定义矩阵 $A \\in \\mathbb{R}^{50 \\times 30}$ 如下\n  $$A_{ij} = \\frac{1}{1 + |i - j|} + 10^{-3} \\cdot \\sin\\left(\\frac{i + j}{10}\\right), \\quad 1 \\le i \\le 50, \\; 1 \\le j \\le 30,$$\n  其中正弦函数的参数以弧度为单位。定义真实状态 $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$ 如下\n  $$x_{\\mathrm{true},j} = \\sin\\left(\\frac{j}{4}\\right), \\quad 1 \\le j \\le 30,$$\n  其中正弦函数以弧度为单位。定义数据 $b \\in \\mathbb{R}^{50}$ 如下\n  $$b_i = \\sum_{j=1}^{30} A_{ij} x_{\\mathrm{true},j} + 10^{-3} \\cdot (-1)^i, \\quad 1 \\le i \\le 50.$$\n- 案例 2（病态模糊，方阵系统，较强噪声）：设 $m = n = 40$。定义矩阵 $A \\in \\mathbb{R}^{40 \\times 40}$ 如下\n  $$A_{ij} = \\exp\\left( - \\frac{(i - j)^2}{2 \\cdot 25} \\right), \\quad 1 \\le i,j \\le 40.$$\n  定义真实状态 $x_{\\mathrm{true}} \\in \\mathbb{R}^{40}$ 如下\n  $$x_{\\mathrm{true},j} = \\cos\\left(\\frac{j}{8}\\right), \\quad 1 \\le j \\le 40,$$\n  其中余弦函数以弧度为单位。定义数据 $b \\in \\mathbb{R}^{40}$ 如下\n  $$b_i = \\sum_{j=1}^{40} A_{ij} x_{\\mathrm{true},j} + 10^{-2} \\cdot \\sin\\left(\\frac{i}{3}\\right), \\quad 1 \\le i \\le 40,$$\n  其中正弦函数以弧度为单位。\n- 案例 3（边界情况，$A$ 中无信息）：设 $m = 20$, $n = 10$。定义 $A$ 如下\n  $$A_{ij} = 0, \\quad 1 \\le i \\le 20, \\; 1 \\le j \\le 10,$$\n  并定义 $b \\in \\mathbb{R}^{20}$ 如下\n  $$b_i = (-1)^i, \\quad 1 \\le i \\le 20.$$\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含所有三个测试用例的结果，格式为方括号括起来的逗号分隔列表。对于每个测试用例，输出三元组 $[\\alpha^\\star, \\|A \\hat{x}_{\\alpha^\\star} - b\\|_2^2, \\mathrm{trace}(H_{\\alpha^\\star})]$。每个浮点数必须格式化为小数点后恰好 $6$ 位。\n- 例如，输出结构必须是以下形式\n  $$\\left[ [\\alpha^\\star_1, r^2_1, t_1], [\\alpha^\\star_2, r^2_2, t_2], [\\alpha^\\star_3, r^2_3, t_3] \\right],$$\n  打印为单行： \n  `[[\\alpha^\\star_1,r^2_1,t_1],[\\alpha^\\star_2,r^2_2,t_2],[\\alpha^\\star_3,r^2_3,t_3]]`", "solution": "该问题已经过严格验证并被认为是有效的。这是一个在逆问题领域内提法良好、具有科学依据的问题，提供了所有必要的信息，并且没有明显的矛盾。\n\n我们的任务是找到线性逆问题 $b = Ax + \\varepsilon$ 的零阶吉洪诺夫正则化解 $\\hat{x}_\\alpha$。解 $\\hat{x}_\\alpha$ 是吉洪诺夫泛函的最小化子：\n$$\nJ(x) = \\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^n$，$b \\in \\mathbb{R}^m$，且 $\\alpha > 0$ 是正则化参数。解必须使用矩阵 $A$ 的奇异值分解（SVD）来表示，最优的 $\\alpha$ 将通过最小化广义交叉验证（GCV）泛函来找到。\n\n**1. 基于 SVD 的吉洪诺夫解和滤波因子**\n\n$J(x)$ 的最小化子 $\\hat{x}_\\alpha$ 满足梯度 $\\nabla_x J(x)$ 为零的条件。计算梯度可得：\n$$\n\\nabla_x J(x) = \\nabla_x ( (Ax-b)^\\top(Ax-b) + \\alpha^2 x^\\top x ) = 2 A^\\top (Ax - b) + 2 \\alpha^2 x\n$$\n将梯度设为零，得到吉洪诺夫问题的正规方程：\n$$\n(A^\\top A + \\alpha^2 I_n) \\hat{x}_\\alpha = A^\\top b\n$$\n其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\n设 $A$ 的秩为 $r \\le \\min(m, n)$。$A$ 的 SVD 分解为 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{m \\times r}$ 具有标准正交列 ($u_1, \\ldots, u_r$)，$V \\in \\mathbb{R}^{n \\times r}$ 具有标准正交列 ($v_1, \\ldots, v_r$)，而 $\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是一个对角矩阵，其对角线上的正奇异值为 $\\sigma_1 \\ge \\sigma_2 \\ge \\ldots \\ge \\sigma_r > 0$。\n\n使用 SVD，我们可以表示正规方程中的各项：\n$$\nA^\\top A = (U \\Sigma V^\\top)^\\top (U \\Sigma V^\\top) = V \\Sigma^\\top U^\\top U \\Sigma V^\\top = V \\Sigma^2 V^\\top\n$$\n$$\nA^\\top b = (U \\Sigma V^\\top)^\\top b = V \\Sigma U^\\top b\n$$\n将这些代入正规方程得到：\n$$\n(V \\Sigma^2 V^\\top + \\alpha^2 I_n) \\hat{x}_\\alpha = V \\Sigma U^\\top b\n$$\n解 $\\hat{x}_\\alpha$ 必须位于 $V$ 的列张成的空间内，该空间是 $A$ 的零空间的正交补。$x$ 在 $A$ 的零空间中的任何分量都会增加惩罚项 $\\alpha^2 \\|x\\|_2^2$ 而不会减小残差项 $\\|A x - b\\|_2^2$。因此，我们可以将解写成基向量 $v_i$ 的线性组合：$\\hat{x}_\\alpha = \\sum_{i=1}^r c_i v_i = Vc$，其中 $c \\in \\mathbb{R}^r$ 是某个系数向量。将此代入方程并使用 $V^\\top V = I_r$：\n$$\n(V \\Sigma^2 V^\\top + \\alpha^2 I_n) V c = V \\Sigma U^\\top b\n$$\n左乘 $V^\\top$：\n$$\nV^\\top(V \\Sigma^2 V^\\top + \\alpha^2 I_n) V c = V^\\top(V \\Sigma U^\\top b)\n$$\n$$\n(\\Sigma^2 + \\alpha^2 I_r) c = \\Sigma U^\\top b\n$$\n由于 $\\sigma_i > 0$ 且 $\\alpha > 0$，矩阵 $(\\Sigma^2 + \\alpha^2 I_r)$ 是对角且可逆的。我们可以解出 $c$：\n$$\nc = (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b\n$$\n那么，吉洪诺夫解为 $\\hat{x}_\\alpha = Vc$：\n$$\n\\hat{x}_\\alpha = V (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b\n$$\n以求和形式表示，$c$ 的第 $i$ 个分量是 $c_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b)$。解变为：\n$$\n\\hat{x}_\\alpha = \\sum_{i=1}^r c_i v_i = \\sum_{i=1}^r \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) v_i\n$$\n标准的（非正则化）伪逆解是 $x^\\dagger = A^\\dagger b = \\sum_{i=1}^r \\frac{1}{\\sigma_i} (u_i^\\top b) v_i$。我们可以用 $x^\\dagger$ 的分量来表示 $\\hat{x}_\\alpha$：\n$$\n\\hat{x}_\\alpha = \\sum_{i=1}^r \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2}\\right) \\frac{u_i^\\top b}{\\sigma_i} v_i\n$$\n项 $f_i(\\alpha) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2}$ 是**滤波因子**。它们对解的谱分量进行滤波，衰减与小奇异值相关的分量，从而稳定解以抵抗噪声。\n\n**2. 影响矩阵及其迹**\n\n预测数据为 $\\hat{b}_\\alpha = A \\hat{x}_\\alpha$。影响矩阵（或帽子矩阵）$H_\\alpha$ 通过 $\\hat{b}_\\alpha = H_\\alpha b$ 将原始数据 $b$ 与预测数据 $\\hat{b}_\\alpha$ 联系起来。代入 $A$ 和 $\\hat{x}_\\alpha$ 的 SVD 表达式：\n$$\n\\hat{b}_\\alpha = (U \\Sigma V^\\top) \\left( V (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b \\right)\n$$\n使用 $V^\\top V = I_r$，可简化为：\n$$\n\\hat{b}_\\alpha = U \\Sigma (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b = U \\Sigma^2 (\\Sigma^2 + \\alpha^2 I_r)^{-1} U^\\top b\n$$\n由此，我们确定影响矩阵：\n$$\nH_\\alpha = U \\Sigma^2 (\\Sigma^2 + \\alpha^2 I_r)^{-1} U^\\top = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} u_i u_i^\\top\n$$\n影响矩阵的迹可利用迹算子的线性和性质 $\\mathrm{trace}(uv^\\top) = v^\\top u$ 求得：\n$$\n\\mathrm{trace}(H_\\alpha) = \\mathrm{trace} \\left(\\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} u_i u_i^\\top \\right) = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} \\mathrm{trace}(u_i u_i^\\top)\n$$\n因为 $u_i^\\top u_i = 1$，我们有 $\\mathrm{trace}(u_i u_i^\\top) = 1$。因此：\n$$\n\\mathrm{trace}(H_\\alpha) = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} = \\sum_{i=1}^r f_i(\\alpha)\n$$\n这个量通常被解释为正则化模型的有效自由度。\n\n**3. 广义交叉验证（GCV）泛函**\n\nGCV 泛函提供了一种通过最小化以下表达式来估计最优 $\\alpha$ 的方法：\n$$\nG(\\alpha) = \\frac{m \\|A \\hat{x}_\\alpha - b\\|_2^2}{(m - \\mathrm{trace}(H_\\alpha))^2}\n$$\n我们需要用 SVD 分量来表示分子，即残差 $r_\\alpha = A\\hat{x}_\\alpha - b$ 的平方范数。残差可以写为 $r_\\alpha = \\hat{b}_\\alpha - b = (H_\\alpha - I_m)b$。\n我们将数据向量 $b$ 分解为其在 $A$ 的列空间（即 $U$ 的列所张成的空间）上的投影及其正交补：\n$$\nb = UU^\\top b + (I_m - UU^\\top)b = \\sum_{i=1}^r (u_i^\\top b) u_i + b_{ortho}\n$$\n将 $(H_\\alpha - I_m)$ 应用于 $b$：\n$$\nr_\\alpha = \\left( \\sum_{i=1}^r f_i(\\alpha) u_i u_i^\\top \\right) b - b = \\sum_{i=1}^r f_i(\\alpha) (u_i^\\top b) u_i - \\left(\\sum_{i=1}^r (u_i^\\top b) u_i + b_{ortho}\\right)\n$$\n$$\nr_\\alpha = \\sum_{i=1}^r (f_i(\\alpha) - 1) (u_i^\\top b) u_i - b_{ortho}\n$$\n代入 $f_i(\\alpha) - 1 = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} - 1 = \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2}$：\n$$\nr_\\alpha = \\sum_{i=1}^r \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) u_i - b_{ortho}\n$$\n求和项中的各项与 $b_{ortho}$ 正交。根据勾股定理，平方范数为：\n$$\n\\|r_\\alpha\\|_2^2 = \\left\\| \\sum_{i=1}^r \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) u_i \\right\\|_2^2 + \\|b_{ortho}\\|_2^2\n$$\n由于向量 $u_i$ 是标准正交的，这变为：\n$$\n\\|A \\hat{x}_\\alpha - b\\|_2^2 = \\sum_{i=1}^r \\left( \\frac{\\alpha^2}{\\sigma_i^2 + \\alpha^2} \\right)^2 (u_i^\\top b)^2 + \\|(I_m - UU^\\top) b\\|_2^2\n$$\n第二项 $\\|b_{ortho}\\|_2^2$ 是 $b$ 正交于 $A$ 的列空间的分量的平方范数。它可以高效地计算为 $\\|b\\|_2^2 - \\|UU^\\top b\\|_2^2 = \\|b\\|_2^2 - \\sum_{i=1}^r(u_i^\\top b)^2$。\n\nGCV 泛函 $G(\\alpha)$ 的所有分量现在都只需使用奇异值 $\\sigma_i$、投影数据分量 $u_i^\\top b$、总大小 $m$ 和 $b$ 的范数即可计算。这避免了显式构造像 $H_\\alpha$ 或 $A^\\top A$ 这样的大型矩阵。数值实现将遵循这种基于 SVD 的公式。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Tikhonov regularization problem for all test cases.\n    \"\"\"\n\n    def generate_test_cases():\n        \"\"\"\n        Generates the A matrices and b vectors for the three test cases.\n        \"\"\"\n        # Case 1\n        m1, n1 = 50, 30\n        A1 = np.zeros((m1, n1), dtype=np.float64)\n        i_idx, j_idx = np.ogrid[1:m1+1, 1:n1+1]\n        A1 = 1 / (1 + np.abs(i_idx - j_idx)) + 1e-3 * np.sin((i_idx + j_idx) / 10)\n        \n        j_vec1 = np.arange(1, n1 + 1)\n        xtrue1 = np.sin(j_vec1 / 4)\n        \n        i_vec1 = np.arange(1, m1 + 1)\n        noise1 = 1e-3 * ((-1)**i_vec1)\n        b1 = A1 @ xtrue1 + noise1\n\n        # Case 2\n        m2, n2 = 40, 40\n        i_idx2, j_idx2 = np.ogrid[1:m2+1, 1:n2+1]\n        A2 = np.exp(-((i_idx2 - j_idx2)**2) / (2 * 25))\n        \n        j_vec2 = np.arange(1, n2 + 1)\n        xtrue2 = np.cos(j_vec2 / 8)\n        \n        i_vec2 = np.arange(1, m2 + 1)\n        noise2 = 1e-2 * np.sin(i_vec2 / 3)\n        b2 = A2 @ xtrue2 + noise2\n        \n        # Case 3\n        m3, n3 = 20, 10\n        A3 = np.zeros((m3, n3), dtype=np.float64)\n        \n        i_vec3 = np.arange(1, m3 + 1)\n        b3 = (-1)**i_vec3\n        \n        return [(A1, b1), (A2, b2), (A3, b3)]\n\n    def compute_gcv_outputs(A, b):\n        \"\"\"\n        Computes the optimal alpha and corresponding outputs using GCV.\n        \"\"\"\n        m, n = A.shape\n        \n        # Step 1: Compute economy SVD\n        U, s, Vh = np.linalg.svd(A, full_matrices=False)\n        \n        # Step 2: Pre-compute SVD-based quantities\n        btilde = U.T @ b  # Components u_i^T * b\n        b_norm_sq = np.linalg.norm(b)**2\n        b_ortho_norm_sq = b_norm_sq - np.sum(btilde**2)\n        s_sq = s**2\n        k = len(s)\n\n        def gcv_func_vectorized(alphas):\n            \"\"\"\n            Calculates GCV values for a vector of alphas.\n            \"\"\"\n            alphas_sq = alphas[:, np.newaxis]**2  # Shape (num_alphas, 1)\n            f = s_sq / (s_sq + alphas_sq)         # Shape (num_alphas, k) using broadcasting\n            \n            trace_H = np.sum(f, axis=1) # Shape (num_alphas,)\n            \n            # Residual norm calculation\n            # (1-f_i)^2 = (alpha^2 / (sigma_i^2 + alpha^2))^2\n            term1 = np.sum(((1 - f)**2) * (btilde**2), axis=1) # Shape (num_alphas,)\n            residual_norm_sq = term1 + b_ortho_norm_sq\n            \n            denom = m - trace_H\n            # Handle potential division by zero\n            gcv_vals = np.full_like(denom, np.inf)\n            safe_indices = ~np.isclose(denom, 0)\n            gcv_vals[safe_indices] = m * residual_norm_sq[safe_indices] / (denom[safe_indices]**2)\n            \n            return gcv_vals\n\n        # Step 3: Search for optimal alpha\n        # Coarse Search\n        alphas1 = np.logspace(-8, 2, 200)\n        gcv_values1 = gcv_func_vectorized(alphas1)\n        min_idx1 = np.argmin(gcv_values1)\n        alpha_min1 = alphas1[min_idx1]\n        \n        # Refined Search\n        lower_bound = max(1e-8, alpha_min1 / 10)\n        upper_bound = min(1e2, alpha_min1 * 10)\n        alphas2 = np.logspace(np.log10(lower_bound), np.log10(upper_bound), 200)\n        gcv_values2 = gcv_func_vectorized(alphas2)\n        min_idx2 = np.argmin(gcv_values2)\n        alpha_star = alphas2[min_idx2]\n        \n        # Step 4: Calculate final results for alpha_star\n        alpha_star_sq = alpha_star**2\n        f_star = s_sq / (s_sq + alpha_star_sq)\n        \n        trace_H_star = np.sum(f_star)\n        \n        term1_star = np.sum(((1 - f_star)**2) * (btilde**2))\n        residual_norm_sq_star = term1_star + b_ortho_norm_sq\n        \n        return alpha_star, residual_norm_sq_star, trace_H_star\n\n    test_cases = generate_test_cases()\n    results = []\n\n    for A, b in test_cases:\n        alpha_star, res_norm_sq, trace_H = compute_gcv_outputs(A, b)\n        results.append(f\"[{alpha_star:.6f},{res_norm_sq:.6f},{trace_H:.6f}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3419911"}, {"introduction": "广义交叉验证（GCV）提供了一种无需真实解即可估计最优正则化参数的方法，但我们如何衡量其有效性？本练习设计了一个理想化的场景：我们已知问题的“真实解”$x_{true}$。通过将GCV选择的正则化参数 $\\alpha_{gcv}$ 与最小化真实误差 $\\|x_{\\alpha} - x_{true}\\|_2$ 的“最优”参数 $\\alpha_{opt}$ 进行直接比较，我们可以量化GCV作为预测误差代理的性能。[@problem_id:3283866]", "problem": "考虑一个线性反问题，其中未知向量 $x \\in \\mathbb{R}^{n}$ 是从数据 $b \\in \\mathbb{R}^{n}$ 中估计的，数据与已知矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 通过模型 $b = A x + \\varepsilon$ 相关联，其中 $\\varepsilon$ 表示加性噪声。假设为了评估目的，未知的真实解 $x_{true}$ 是已知的。估计值 $x_{\\alpha}$ 是通过最小化二次泛函 $J_{\\alpha}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\alpha^{2} \\lVert x \\rVert_{2}^{2}$ 得到的，其中 $\\alpha > 0$ 是一个正则化参数。您的任务是选择 $\\alpha$ 以最小化差异 $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$，并将此选择与广义交叉验证 (GCV) 选择的 $\\alpha$ 进行比较。广义交叉验证 (GCV) 是一种通过最小化函数 $G(\\alpha) = \\lVert (I - S(\\alpha)) b \\rVert_{2}^{2} / \\left(\\operatorname{trace}(I - S(\\alpha))\\right)^{2}$ 来选择 $\\alpha$ 的方法，其中 $S(\\alpha)$ 是将 $b$ 映射到拟合数据 $A x_{\\alpha}$ 的线性平滑算子。\n\n使用的基本原理：\n- 严格凸二次泛函的最小化子通过将其梯度置零来表征。\n- 线性估计器的平滑矩阵是线性算子 $S(\\alpha)$，它将数据 $b$ 映射到拟合值 $A x_{\\alpha}$。\n\n实现细节：\n- 对于所有测试用例，使用由对角线上的对角奇异值定义的方阵 $A$。具体来说，对于索引 $i$（$1 \\leq i \\leq n$），设置 $A_{ii} = s_{i}$ 且当 $i \\neq j$ 时 $A_{ij} = 0$，其中序列 $\\{s_{i}\\}$ 按下文每个测试用例的描述给出。\n- 将真实解定义为 $x_{true} \\in \\mathbb{R}^{n}$，其分量为 $x_{true,i} = \\sin\\left( \\frac{2\\pi i}{n} \\right) + \\frac{1}{2} \\cos\\left( \\frac{\\pi i}{n} \\right)$，其中 $i = 1, 2, \\dots, n$。所有角度必须以弧度为单位。\n- 将噪声向量定义为 $\\varepsilon \\in \\mathbb{R}^{n}$，其分量为 $\\varepsilon_{j} = \\sigma \\sin(j)$，其中 $j = 1, 2, \\dots, n$，$\\sigma$ 是每个测试用例指定的噪声水平。所有角度必须以弧度为单位。\n- 为了评估 $\\alpha$，使用一个在区间 $[10^{-8}, 10^{0}]$ 内包含 $N_{\\alpha}=121$ 个对数间隔值的网格，不包括 $\\alpha = 0$。具体来说，设置 $N_{\\alpha} = 121$，并在对数尺度上从 $10^{-8}$ 到 $10^{0}$ 均匀采样 $\\alpha$ 值。\n\n对于网格中的每个 $\\alpha$：\n- 通过最小化 $J_{\\alpha}(x)$ 来计算 $x_{\\alpha}$。\n- 计算误差范数 $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$。\n- 计算与估计器 $A x_{\\alpha}$ 关联的平滑矩阵 $S(\\alpha)$，然后计算 GCV 分数 $G(\\alpha)$。\n\n对于每个测试用例：\n- 选择最优的 $\\alpha$（记为 $\\alpha_{opt}$），该值在网格上最小化 $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$。\n- 选择由广义交叉验证选定的 $\\alpha$（记为 $\\alpha_{gcv}$），该值在网格上最小化 $G(\\alpha)$。\n- 计算比率 $R = \\lVert x_{\\alpha_{gcv}} - x_{true} \\rVert_{2} / \\lVert x_{\\alpha_{opt}} - x_{true} \\rVert_{2}$。\n\n测试套件：\n- 测试用例 1：$n = 20$， $s_{i} = i^{-2}$， $\\sigma = 10^{-2}$。\n- 测试用例 2：$n = 20$， $s_{i} = 10^{-i/4}$， $\\sigma = 0$。\n- 测试用例 3：$n = 30$， $s_{i} = i^{-3}$， $\\sigma = 5 \\cdot 10^{-2}$。\n\n您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果本身必须是一个包含三个浮点数 $[\\alpha_{opt}, \\alpha_{gcv}, R]$ 的列表，每个浮点数四舍五入到 $6$ 位小数。因此，最终输出必须具有形式 $[[\\alpha_{opt}^{(1)}, \\alpha_{gcv}^{(1)}, R^{(1)}],[\\alpha_{opt}^{(2)}, \\alpha_{gcv}^{(2)}, R^{(2)}],[\\alpha_{opt}^{(3)}, \\alpha_{gcv}^{(3)}, R^{(3)}]]$。", "solution": "该问题要求确定一个线性反问题的吉洪诺夫 (Tikhonov) 正则化参数 $\\alpha$。我们必须找到最小化真实误差范数 $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$ 的参数 $\\alpha_{opt}$，并将其与通过广义交叉验证 (GCV) 方法选择的参数 $\\alpha_{gcv}$ 进行比较。\n\n首先，我们推导正则化解 $x_{\\alpha}$ 的表达式。该解被定义为吉洪诺夫泛函的最小化子：\n$$J_{\\alpha}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\alpha^{2} \\lVert x \\rVert_{2}^{2}$$\n其中 $\\alpha > 0$ 是正则化参数。这是一个关于 $x$ 的二次泛函。我们可以将其展开为：\n$$J_{\\alpha}(x) = (A x - b)^T (A x - b) + \\alpha^2 x^T x = x^T A^T A x - 2 b^T A x + b^T b + \\alpha^2 x^T x$$\n当 $\\alpha > 0$ 时，泛函 $J_{\\alpha}(x)$ 是严格凸的，因此其唯一最小值可以通过将其关于 $x$ 的梯度置零来找到。\n$$\\nabla_x J_{\\alpha}(x) = 2 A^T A x - 2 A^T b + 2 \\alpha^2 I x = 0$$\n其中 $I$ 是单位矩阵。重新整理各项，我们得到：\n$$(A^T A + \\alpha^2 I) x = A^T b$$\n因此，正则化解 $x_{\\alpha}$ 由下式给出：\n$$x_{\\alpha} = (A^T A + \\alpha^2 I)^{-1} A^T b$$\n这是吉洪诺夫正则化解的标准形式。问题指定矩阵 $A$ 是一个方形对角矩阵，其元素为 $A_{ii} = s_i$ 且当 $i \\neq j$ 时 $A_{ij} = 0$。因此，$A = \\mathrm{diag}(s_1, s_2, \\dots, s_n)$。由于 $A$ 是实对角矩阵，因此它是对称的，所以 $A^T = A$。$x_{\\alpha}$ 的表达式得以简化。\n所涉及的矩阵都是对角矩阵：\n- $A^T A = A^2 = \\mathrm{diag}(s_1^2, s_2^2, \\dots, s_n^2)$\n- $A^T A + \\alpha^2 I = \\mathrm{diag}(s_1^2 + \\alpha^2, s_2^2 + \\alpha^2, \\dots, s_n^2 + \\alpha^2)$\n- $(A^T A + \\alpha^2 I)^{-1} = \\mathrm{diag}\\left(\\frac{1}{s_1^2 + \\alpha^2}, \\dots, \\frac{1}{s_n^2 + \\alpha^2}\\right)$\n解向量 $x_{\\alpha}$ 可以按分量计算。$x_{\\alpha}$ 的第 $i$ 个分量是：\n$$(x_{\\alpha})_i = \\left(\\frac{1}{s_i^2 + \\alpha^2}\\right) (s_i) (b_i) = \\frac{s_i}{s_i^2 + \\alpha^2} b_i$$\n这些项 $\\frac{s_i}{s_i^2 + \\alpha^2}$ 被称为滤波因子。\n\n接下来，我们处理广义交叉验证 (GCV) 方法。GCV 函数定义为：\n$$G(\\alpha) = \\frac{\\lVert (I - S(\\alpha)) b \\rVert_{2}^{2}}{\\left(\\operatorname{trace}(I - S(\\alpha))\\right)^{2}}$$\n其中 $S(\\alpha)$ 是将数据向量 $b$ 映射到拟合数据 $A x_{\\alpha}$ 的平滑矩阵，即 $A x_{\\alpha} = S(\\alpha) b$。从 $x_{\\alpha}$ 的表达式中，我们可以推导出 $S(\\alpha)$：\n$$S(\\alpha) = A (A^T A + \\alpha^2 I)^{-1} A^T$$\n鉴于 $A$ 是对角矩阵，$S(\\alpha)$ 也是对角矩阵：\n$$S(\\alpha) = \\mathrm{diag}(s_i) \\cdot \\mathrm{diag}\\left(\\frac{1}{s_i^2 + \\alpha^2}\\right) \\cdot \\mathrm{diag}(s_i) = \\mathrm{diag}\\left(\\frac{s_i^2}{s_i^2 + \\alpha^2}\\right)$$\n现在我们可以评估 GCV 函数的两个部分。项 $I - S(\\alpha)$ 是一个对角矩阵：\n$$I - S(\\alpha) = \\mathrm{diag}\\left(1 - \\frac{s_i^2}{s_i^2 + \\alpha^2}\\right) = \\mathrm{diag}\\left(\\frac{\\alpha^2}{s_i^2 + \\alpha^2}\\right)$$\n该矩阵的迹是其对角线元素之和：\n$$\\operatorname{trace}(I - S(\\alpha)) = \\sum_{i=1}^{n} \\frac{\\alpha^2}{s_i^2 + \\alpha^2}$$\n$G(\\alpha)$ 的分子是残差向量 $(I - S(\\alpha)) b$ 的平方范数：\n$$\\lVert (I - S(\\alpha)) b \\rVert_{2}^{2} = \\sum_{i=1}^{n} \\left( \\left(\\frac{\\alpha^2}{s_i^2 + \\alpha^2}\\right) b_i \\right)^2$$\n因此，需要最小化的 GCV 函数是：\n$$G(\\alpha) = \\frac{\\sum_{i=1}^{n} \\left(\\frac{\\alpha^2 b_i}{s_i^2 + \\alpha^2}\\right)^2}{\\left(\\sum_{i=1}^{n} \\frac{\\alpha^2}{s_i^2 + \\alpha^2}\\right)^2}$$\n\n对于每个测试用例，整体算法按以下步骤进行：\n1.  定义参数 $n$、$\\sigma$ 以及奇异值 $s_i$ 的函数。\n2.  根据问题规范，为 $i=1, \\dots, n$ 构造向量 $s_i$、$x_{true,i}$ 和 $\\varepsilon_i$。\n3.  使用模型 $b_i = s_i x_{true,i} + \\varepsilon_i$ 计算数据向量 $b$。\n4.  为 $\\alpha$ 创建一个从 $10^{-8}$ 到 $10^{0}$ 的包含 $N_{\\alpha}=121$ 个值的对数间隔网格。\n5.  对于网格中的每个 $\\alpha$：\n    a. 使用 $(x_{\\alpha})_i = \\frac{s_i}{s_i^2 + \\alpha^2} b_i$ 计算正则化解向量 $x_{\\alpha}$。\n    b. 计算并存储误差范数 $\\lVert x_{\\alpha} - x_{true} \\rVert_2$。\n    c. 使用推导出的公式计算并存储 GCV 分数 $G(\\alpha)$。\n6.  在网格中找到最小化误差范数的 $\\alpha_{opt}$ 值。这对应于存储的误差列表中的最小值。\n7.  在网格中找到最小化 GCV 分数的 $\\alpha_{gcv}$ 值。这对应于存储的 GCV 分数列表中的最小值。\n8.  计算比率 $R = \\frac{\\lVert x_{\\alpha_{gcv}} - x_{true} \\rVert_{2}}{\\lVert x_{\\alpha_{opt}} - x_{true} \\rVert_{2}}$，其中 $\\lVert x_{\\alpha_{opt}} - x_{true} \\rVert_{2}$ 是在步骤 6 中找到的最小误差，而 $\\lVert x_{\\alpha_{gcv}} - x_{true} \\rVert_{2}$ 是对应于 $\\alpha_{gcv}$ 的误差。\n9.  为该测试用例存储结果 $[\\alpha_{opt}, \\alpha_{gcv}, R]$。\n\n对所有三个测试用例重复此过程，并按规定汇总和格式化结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n\n    def process_case(n, s_func, sigma):\n        \"\"\"\n        Solves a single test case for Tikhonov regularization and GCV.\n\n        Args:\n            n (int): The dimension of the problem.\n            s_func (function): A function that takes an array of indices i and returns singular values s_i.\n            sigma (float): The noise level.\n\n        Returns:\n            list: A list containing [alpha_opt, alpha_gcv, R] rounded to 6 decimal places.\n        \"\"\"\n        # 1. Generate problem data: s, x_true, noise, b\n        i_vals = np.arange(1.0, n + 1.0)\n        s = s_func(i_vals)\n        \n        x_true = np.sin(2 * np.pi * i_vals / n) + 0.5 * np.cos(np.pi * i_vals / n)\n        \n        if sigma == 0.0:\n            noise = np.zeros(n)\n        else:\n            noise = sigma * np.sin(i_vals)\n            \n        b = s * x_true + noise\n\n        # 2. Setup alpha grid for regularization parameter search\n        alphas = np.logspace(-8, 0, 121)\n        \n        errors = []\n        gcv_scores = []\n        \n        s2 = s**2\n\n        # 3. Loop over all alpha values to compute errors and GCV scores\n        for alpha in alphas:\n            alpha2 = alpha**2\n            \n            # Compute the Tikhonov-regularized solution x_alpha\n            # (x_alpha)_i = (s_i / (s_i^2 + alpha^2)) * b_i\n            filter_factors = s / (s2 + alpha2)\n            x_alpha = filter_factors * b\n            \n            # Compute the error norm ||x_alpha - x_true||_2\n            error = np.linalg.norm(x_alpha - x_true)\n            errors.append(error)\n            \n            # Compute the GCV score G(alpha)\n            # Numerator: ||(I - S(alpha))b||^2 = sum_i ((alpha^2 * b_i) / (s_i^2 + alpha^2))^2\n            # Denominator: (tr(I - S(alpha)))^2 = (sum_i alpha^2 / (s_i^2 + alpha^2))^2\n            common_term_gcv = alpha2 / (s2 + alpha2)\n            \n            gcv_num = np.sum((common_term_gcv * b)**2)\n            gcv_den = np.sum(common_term_gcv)**2\n            \n            if gcv_den == 0.0:\n                gcv_score = np.inf\n            else:\n                gcv_score = gcv_num / gcv_den\n            gcv_scores.append(gcv_score)\n\n        # 4. Find optimal alpha and GCV-chosen alpha from the grid\n        errors = np.array(errors)\n        gcv_scores = np.array(gcv_scores)\n        \n        # Find alpha_opt, which minimizes the true error\n        idx_opt = np.argmin(errors)\n        alpha_opt = alphas[idx_opt]\n        min_error = errors[idx_opt]\n        \n        # Find alpha_gcv, which minimizes the GCV score\n        idx_gcv = np.argmin(gcv_scores)\n        alpha_gcv = alphas[idx_gcv]\n        \n        # 5. Compute the performance ratio R\n        error_at_gcv = errors[idx_gcv]\n        \n        if min_error == 0.0:\n            R = 1.0 if error_at_gcv == 0.0 else np.inf\n        else:\n            R = error_at_gcv / min_error\n            \n        return [alpha_opt, alpha_gcv, R]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, s_func, sigma)\n        {'n': 20, 's_func': lambda i: i**(-2), 'sigma': 1e-2},\n        {'n': 20, 's_func': lambda i: 10**(-i/4.0), 'sigma': 0.0},\n        {'n': 30, 's_func': lambda i: i**(-3), 'sigma': 5e-2},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case['n'], case['s_func'], case['sigma'])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    inner_parts = []\n    for r in all_results:\n        inner_parts.append(f\"[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]\")\n    output_str = f\"[{','.join(inner_parts)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3283866"}, {"introduction": "任何方法都有其适用边界，理解这些边界是成为专家的关键。GCV的核心假设是残差主要由統計噪声构成，但在非线性问题中，这一假设常常失效。本练习将引导你构建一个反例，揭示在高斯-牛顿（Gauss–Newton）迭代中天真地应用GCV可能导致的“过度平滑”陷阱，此时残差主要是结构化的线性化误差，而非噪声。通过这个实践，你将深刻体会到在应用任何算法之前，批判性地审视其基本假设的重要性。[@problem_id:3385851]", "problem": "考虑一个非线性反问题：根据正向映射 $F: \\mathbb{R}^n \\to \\mathbb{R}^m$ 从数据 $y \\in \\mathbb{R}^m$ 估计未知参数向量 $x \\in \\mathbb{R}^n$。假设观测模型是无噪声的，即对于一个固定的真实参数 $x^\\star$，有 $y = F(x^\\star)$。目标是分析一个 Gauss–Newton 方案的行为。该方案在第 $k$ 次迭代时，围绕当前迭代点 $x_k$ 对正向映射进行线性化，计算雅可比矩阵 $J_k = \\nabla F(x_k)$，并通过求解一个 Tikhonov 正则化的线性最小二乘子问题来确定步长 $d_k$，其形式如下：\n$$\n\\min_{d \\in \\mathbb{R}^n} \\ \\|J_k d - r_k\\|_2^2 + \\alpha_k^2 \\|d\\|_2^2,\n$$\n其中 $r_k = y - F(x_k)$ 是当前残差，$\\alpha_k \\ge 0$ 是一个正则化参数。在应用于非线性问题的朴素广义交叉验证 (GCV) 方法中，$\\alpha_k$ 在每次迭代时通过最小化与基于 $J_k$ 和 $r_k$ 的线性化子问题相关联的广义交叉验证泛函来选择。\n\n构建一个反例，在该反例中，于非线性设定下朴素地使用广义交叉验证会选择过大的 $\\alpha_k$，从而沿信息方向过度平滑，并导致 Gauss–Newton 迭代停滞。你的构造必须是显式的和算法化的，并且必须能够通过计算得到可验证的证明。\n\n使用以下显式正向映射，其中 $n = 2$ 且 $m = 3$：\n$$\nF(x) = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\ns \\tanh\\!\\big(\\gamma (x_1 + x_2)\\big)\n\\end{bmatrix},\n$$\n其中 $x = (x_1,x_2)^\\top$，$s > 0$ 是一个尺度参数，$\\gamma > 0$ 控制非线性的强度。其雅可比矩阵为：\n$$\nJ(x) = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\ns \\gamma \\operatorname{sech}^2\\!\\big(\\gamma (x_1 + x_2)\\big) & s \\gamma \\operatorname{sech}^2\\!\\big(\\gamma (x_1 + x_2)\\big)\n\\end{bmatrix}.\n$$\n\n目标解固定为 $x^\\star = (1,1)^\\top$，观测值为 $y = F(x^\\star)$。从一个指定的 $x_0$ 开始初始化 Gauss–Newton 方法，并通过 $x_{k+1} = x_k + d_k$ 进行更新。在每次迭代中，通过应用于线性化子问题（即将 $J_k$ 和 $r_k$ 视为来自带有恒等惩罚的 Tikhonov 正则化的线性模型）的朴素广义交叉验证来选择 $\\alpha_k$。运行迭代，直到达到最大迭代次数或满足停滞准则。\n\n你的程序必须实现两种方案：\n- 一种朴素广义交叉验证方案，在每次迭代时仅从 $x_k$ 处的线性化子问题中选择 $\\alpha_k$。\n- 一种固定正则化的基准 Gauss–Newton 方案，在所有迭代中使用一个小的常数 $\\alpha > 0$。\n\n朴素广义交叉验证方案的停滞定义如下：最终残差范数比 $\\|r_{\\text{final}}\\|_2 / \\|r_{\\text{initial}}\\|_2 \\ge 0.95$，而基准固定正则化方案实现的残差范数比小于或等于 $0.70$。该准则捕捉了广义交叉验证沿信息方向（此处为前两个分量）过度平滑并阻碍进展，而适度的固定正则化却能取得进展的现象。\n\n使用以下测试套件，该套件改变非线性强度 $\\gamma$ 和初始点 $x_0$，并固定 $s = 1$：\n- 测试用例 1（强非线性，远初始点）：$\\gamma = 10$, $x_0 = (-1,-1)^\\top$。\n- 测试用例 2（中等非线性，远初始点）：$\\gamma = 0.5$, $x_0 = (-1,-1)^\\top$。\n- 测试用例 3（强非线性，近初始点）：$\\gamma = 10$, $x_0 = (0.9,0.9)^\\top$。\n\n对于每个测试用例，使用相同的最大迭代次数和更新规则运行这两种方案。最终输出必须是单行，包含一个布尔值列表，按顺序指出每个测试用例的朴素广义交叉验证方案是否根据上述定义停滞。\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，`` `[True, False, True]` ``）。布尔值必须格式化为 Python 布尔字面量，即 \"True\" 或 \"False\"。", "solution": "从数据 $y \\in \\mathbb{R}^m$ 通过非线性正向映射 $F(x)$ 估计未知参数向量 $x \\in \\mathbb{R}^n$ 的问题是科学计算的基石。Gauss–Newton 方法是解决此类非线性最小二乘问题的标准迭代技术。在每次迭代 $k$ 中，该方法围绕当前估计 $x_k$ 对问题进行线性化，并求解一个线性最小二乘子问题以找到更新步长 $d_k$。为了在存在病态条件时稳定解，这个子问题通常被正则化，例如，使用 Tikhonov 方法：\n$$\n\\min_{d \\in \\mathbb{R}^n} \\ \\|J_k d - r_k\\|_2^2 + \\alpha_k^2 \\|d\\|_2^2,\n$$\n其中 $J_k = \\nabla F(x_k)$ 是雅可比矩阵，$r_k = y - F(x_k)$ 是残差。该方法的一个关键方面是正则化参数 $\\alpha_k$ 的选择。\n\n广义交叉验证 (Generalized Cross-Validation, GCV) 是在线性反问题 $y = Kx + \\epsilon$（其中 $\\epsilon$ 是白噪声）中选择 $\\alpha_k$ 的一种强大方法。GCV 泛函 $V(\\alpha)$ 旨在估计预测误差，通过最小化该泛函来找到最优的 $\\alpha$。其推导假设线性系统的残差向量主要由统计噪声构成。\n\n在非线性 Gauss–Newton 上下文中“朴素”应用 GCV，涉及将每一步的线性化子问题视为一个独立的线性反问题。也就是说，对于系统 $J_k d_k \\approx r_k$，使用 GCV 通过最小化以下泛函来找到 $\\alpha_k$：\n$$\nV(\\alpha_k) = \\frac{\\|J_k d_k(\\alpha_k) - r_k\\|_2^2}{\\left( \\operatorname{Tr}\\left(I - A_k(\\alpha_k)\\right) \\right)^2},\n$$\n其中 $A_k(\\alpha_k) = J_k(J_k^\\top J_k + \\alpha_k^2 I)^{-1}J_k^\\top$ 是影响矩阵。\n\n这个反例旨在揭示的这种朴素方法的基本缺陷在于，非线性问题中的残差 $r_k = F(x^\\star) - F(x_k)$ 与统计噪声并不可比。它主要由**线性化误差**构成，特别是当迭代点 $x_k$ 远离真实解 $x^\\star$ 且函数 $F$ 高度非线性时。也就是说，$r_k = J_k(x^\\star - x_k) + \\mathcal{O}(\\|x^\\star - x_k\\|^2)$。高阶项是确定性的，而非随机的。GCV 无法区分这种确定性结构与噪声，将大的线性化误差误解为大的噪声。因此，它会选择一个过大的正则化参数 $\\alpha_k$ 来“平滑”这种感知到的噪声，这会过度抑制更新步长 $d_k$，并导致迭代停滞或陷入僵局。\n\n我们使用指定的正向映射 $F(x) = [x_1, x_2, s \\tanh(\\gamma(x_1+x_2))]^\\top$ 来构造这个反例。参数 $\\gamma$ 控制非线性。当 $\\gamma$ 很大时，$\\tanh$ 函数会迅速饱和，意味着其导数 $\\operatorname{sech}^2$ 对于幅值大的参数会变得接近于零。这一结构特性是关键。\n\n让我们分析测试用例 1：$\\gamma=10$，$x_0 = (-1,-1)^\\top$，$s=1$，以及 $x^\\star = (1,1)^\\top$。\n初始迭代点 $x_0$ 距离解 $x^\\star$ “很远”。在 $x_0$ 处 $\\tanh$ 函数的参数是 $\\gamma(x_{0,1}+x_{0,2}) = 10(-2) = -20$。这处于 $\\tanh$ 函数的深度饱和区。\n在 $x_0$ 处的雅可比矩阵是：\n$$\nJ(x_0) = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 10 \\operatorname{sech}^2(-20) & 10 \\operatorname{sech}^2(-20) \\end{bmatrix}.\n$$\n由于 $\\operatorname{sech}^2(-20) \\approx 4.2 \\times 10^{-18}$，雅可比矩阵的第三行实际上是零。因此，$J_0 \\approx \\begin{bsmallmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bsmallmatrix}$。$J_0$ 的值域基本上是 $\\mathbb{R}^3$ 中的 $xy$-平面。\n\n真实数据是 $y = F(x^\\star) = [1, 1, \\tanh(20)]^\\top \\approx [1, 1, 1]^\\top$。\n在 $x_0$ 处的模型输出是 $F(x_0) = [-1, -1, \\tanh(-20)]^\\top \\approx [-1, -1, -1]^\\top$。\n初始残差是 $r_0 = y - F(x_0) \\approx [2, 2, 2]^\\top$。\n\n关键的观察是，$r_0$ 有一个很大的分量 $[0,0,2]^\\top$，该分量与 $J_0$ 的值域正交。线性模型 $J_0 d$ 无法解释残差的这个分量。GCV 泛函将这个无法解释的大残差分量视为存在巨大噪声的证据，其最小化过程会指定一个非常大的 $\\alpha_0$ 值来抑制它。对于一个非常大（$\\alpha_0 \\to \\infty$）的 GCV 泛函的最小化器，得到的正则化步长为 $d_0 \\approx (J_0^\\top J_0 + \\alpha_0^2 I)^{-1} J_0^\\top r_0 \\approx \\frac{1}{\\alpha_0^2} J_0^\\top r_0$，该值变得微乎其微。因此，Gauss–Newton 迭代几乎没有进展，即 $x_1 \\approx x_0$，过程陷入停滞。\n\n相比之下，采用小的固定 $\\alpha$ 的基准方案基本上是在执行一个标准的 Gauss–Newton 步：$d_0 \\approx J_0^\\dagger r_0 = [2,2]^\\top$。这会产生更新 $x_1 = x_0 + d_0 = (-1,-1)^\\top + (2,2)^\\top = (1,1)^\\top = x^\\star$，在一次迭代中就收敛。这鲜明地展示了朴素 GCV 方法的失败。\n\n对于测试用例 2（$\\gamma=0.5$），非线性很弱。雅可比矩阵在各处都良态，线性化误差很小。GCV 表现良好，选择了合理的 $\\alpha_k$，并且没有停滞。\n对于测试用例 3（$\\gamma=10$，$x_0=(0.9,0.9)^\\top$），尽管非线性很强，但初始猜测接近解。残差 $r_0 \\approx [0.1, 0.1, 0]^\\top$ 很小，并且几乎完全位于 $J_0$ 的值域内。GCV 正确地识别出几乎不需要平滑，选择了 $\\alpha_0 \\approx 0$，方法迅速收敛。\n\n所提供的程序以计算方式实现了这两种方案——朴素 GCV 和固定 alpha 基准——并将它们应用于三个测试用例。它计算了每种方案的最终与初始残差范数比，并评估了规定的停滞准则，从而验证了第一种情况下的预期停滞以及其他情况下的成功收敛。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Implements and tests two Gauss-Newton schemes to demonstrate the\n    failure of naive Generalized Cross-Validation (GCV) for a specific\n    nonlinear inverse problem.\n    \"\"\"\n    # --- Constants  Parameters from problem statement and implementation choices ---\n    S = 1.0\n    X_STAR = np.array([1.0, 1.0])\n    MAX_ITER = 50\n    FIXED_ALPHA = 1e-6\n    STAGNATION_THRESHOLD_GCV = 0.95\n    CONVERGENCE_THRESHOLD_FIXED = 0.70\n\n    # --- Problem-specific functions: Forward Map and Jacobian ---\n    def forward_map(x, s, gamma):\n        x1, x2 = x\n        val = s * np.tanh(gamma * (x1 + x2))\n        return np.array([x1, x2, val])\n\n    def jacobian(x, s, gamma):\n        x1, x2 = x\n        cosh_arg = gamma * (x1 + x2)\n        # Avoid overflow for large arguments in cosh\n        if np.abs(cosh_arg) > 30:\n            sech_val = 0.0\n        else:\n            sech_val = 1.0 / np.cosh(cosh_arg)\n        \n        d_tanh = s * gamma * sech_val**2\n        return np.array([\n            [1.0, 0.0],\n            [0.0, 1.0],\n            [d_tanh, d_tanh]\n        ])\n\n    # --- GCV Functional Implementation ---\n    def gcv_function(alpha, J, r):\n        m, n = J.shape\n        alpha2 = alpha**2\n        \n        try:\n            # SVD is the standard, numerically stable way to analyze the GCV functional.\n            U, s_vals, _ = np.linalg.svd(J, full_matrices=True)\n        except np.linalg.LinAlgError:\n            return np.inf\n\n        s2 = s_vals**2\n        r_hat = U.T @ r\n\n        # Numerator: ||(I - A)r||^2 where A is the influence matrix\n        f_res = alpha2 / (s2 + alpha2)\n        num_term1 = np.sum((f_res * r_hat[:n])**2) # Component in range(J)\n        num_term2 = np.sum(r_hat[n:]**2)      # Component in null(J^T)\n        numerator = num_term1 + num_term2\n\n        # Denominator: (Tr(I - A))^2\n        trace_I_minus_A = (m - n) + np.sum(alpha2 / (s2 + alpha2))\n        \n        if trace_I_minus_A < 1e-15:\n            return np.inf\n            \n        denominator = trace_I_minus_A**2\n        \n        return numerator / denominator\n\n    # --- Gauss-Newton Solver ---\n    def solve_gauss_newton(x0, gamma, s, y_true, method, fixed_alpha, max_iter):\n        x_k = np.copy(x0.astype(np.float64))\n        \n        initial_residual = y_true - forward_map(x_k, s, gamma)\n        initial_residual_norm = np.linalg.norm(initial_residual)\n\n        if initial_residual_norm < 1e-12: # Already converged\n            return 0.0\n\n        for _ in range(max_iter):\n            r_k = y_true - forward_map(x_k, s, gamma)\n            J_k = jacobian(x_k, s, gamma)\n\n            if method == 'gcv':\n                # Find alpha_k by minimizing the GCV functional on a log-spaced grid or via optimizer\n                res = minimize_scalar(\n                    lambda alpha: gcv_function(alpha, J_k, r_k),\n                    bounds=(1e-10, 1e4), # A wide search range for alpha\n                    method='bounded'\n                )\n                alpha_k = res.x\n            elif method == 'fixed':\n                alpha_k = fixed_alpha\n            else:\n                raise ValueError(\"Invalid method specified.\")\n\n            # Solve the Tikhonov-regularized normal equations for the step d_k\n            # (J_k^T J_k + alpha_k^2 I) d_k = J_k^T r_k\n            A = J_k.T @ J_k + (alpha_k**2) * np.identity(x_k.shape[0])\n            b = J_k.T @ r_k\n            \n            try:\n                d_k = np.linalg.solve(A, b)\n            except np.linalg.LinAlgError:\n                # If matrix is singular, iteration cannot proceed.\n                break\n\n            x_k += d_k\n        \n        final_residual_norm = np.linalg.norm(y_true - forward_map(x_k, s, gamma))\n        \n        return final_residual_norm / initial_residual_norm\n\n    # --- Main Driver Script ---\n    test_cases = [\n        # (gamma, x0)\n        (10.0, np.array([-1.0, -1.0])),\n        (0.5, np.array([-1.0, -1.0])),\n        (10.0, np.array([0.9, 0.9]))\n    ]\n    \n    results = []\n\n    for gamma, x0 in test_cases:\n        # The true data y depends on the forward map's parameters\n        y_true = forward_map(X_STAR, S, gamma)\n\n        # Run with Naive GCV\n        ratio_gcv = solve_gauss_newton(\n            x0=x0, gamma=gamma, s=S, y_true=y_true,\n            method='gcv', fixed_alpha=None, max_iter=MAX_ITER\n        )\n\n        # Run with Fixed Alpha Baseline\n        ratio_fixed = solve_gauss_newton(\n            x0=x0, gamma=gamma, s=S, y_true=y_true,\n            method='fixed', fixed_alpha=FIXED_ALPHA, max_iter=MAX_ITER\n        )\n\n        # Evaluate the stagnation criterion from the problem description\n        stagnated = (\n            ratio_gcv >= STAGNATION_THRESHOLD_GCV and\n            ratio_fixed <= CONVERGENCE_THRESHOLD_FIXED\n        )\n        results.append(stagnated)\n\n    # Print the final result in the exact specified format\n    print(f\"[{','.join(str(r).capitalize() for r in results)}]\")\n\nsolve()\n```", "id": "3385851"}]}