{"hands_on_practices": [{"introduction": "在实际的历史匹配问题中，观测数据常常被异常值（outliers）污染，这些异常值会严重扭曲基于标准最小二乘法的结果。本练习将引导您探索稳健数据同化的概念，通过使用具有更重尾部的学生$t$分布（Student's $t$-distribution）替代传统的正态分布似然函数。通过推导和比较两者的梯度，您将亲身体验不同的统计假设如何影响优化算法的行为，并深刻理解构建稳健参数估计方法的原理。[@problem_id:3389148]", "problem": "您正在使用简化的递减曲线正演模型，对油藏中单井生产率的历史拟合进行建模。设模型参数向量为 $m = [k,a]^{\\top}$，其中 $k$ 是一个正振幅，$a$ 是一个正递减率。设在观测时间 $t_i$ 的正演算子为 $h_i(m) = k \\exp(-a t_i)$，$h(m) \\in \\mathbb{R}^{n}$ 是将这些分量（对于 $i = 1,\\dots,n$）堆叠而成的向量。观测值 $y^{\\mathrm{obs}} \\in \\mathbb{R}^{n}$ 通过 $y^{\\mathrm{obs}} = h(m^{\\star}) + \\epsilon$ 与模型相关联，其中 $m^{\\star}$ 是未知的真实参数，$\\epsilon$ 是测量误差。在用于历史拟合的稳健数据同化中，通常使用自由度为 $\\nu$ 的学生t分布来描述测量误差，以减轻异常值的影响。\n\n从具有自由度 $\\nu$ 和每个观测值已知正尺度 $s$ 的独立残差的学生t概率密度函数（PDF）的定义出发，并仅使用基本微积分法则（链式法则、乘积法则）和似然函数的定义，完成以下任务：\n\n1) 在独立的t分布分量假设下，推导残差向量 $r(m) = y^{\\mathrm{obs}} - h(m)$ 的负对数似然，并根据雅可比矩阵 $J(m) = \\partial h(m)/\\partial m$ 计算其关于 $m$ 的梯度。清晰地指出分量级的“影响函数”，该函数通过链式法则将每个残差分量 $r_i$ 映射到梯度中的相应贡献。\n\n2) 作为对比，对每个分量方差为 $s^2$ 的独立高斯测量误差重复此推导，并指出相应的影响函数。\n\n3) 分析学生t分布和高斯分布的影响函数在处理大残差（异常值）和小残差时的定性差异。解释哪种似然函数会降低异常值的影响，并说明原因。\n\n4) 实现一个程序，对于给定的参数值，计算学生t模型和高斯模型下负对数似然梯度的欧几里得范数，以及异常残差和典型残差的绝对影响大小之比。使用以下具体且完全指定的设置：\n- 观测时间 $t = [0,1,2,3,4]$。\n- 真实参数 $m^{\\star} = [1.5, 0.4]^{\\top}$。\n- 用于线性化和评估的初始猜测值 $m_0 = [1.2, 0.5]^{\\top}$。\n- 基线无噪声观测值 $y^{\\mathrm{base}} = h(m^{\\star})$。\n- 在索引 $i_{\\mathrm{out}} = 2$ 处（即第三个观测值）注入一个异常值，通过向该分量添加一个标量偏移量 $\\Delta_{\\mathrm{out}}$：$y^{\\mathrm{obs}} = y^{\\mathrm{base}}$，其中 $y^{\\mathrm{obs}}[i_{\\mathrm{out}}] \\leftarrow y^{\\mathrm{obs}}[i_{\\mathrm{out}}] + \\Delta_{\\mathrm{out}}$。\n- 典型残差索引 $i_{\\mathrm{typ}} = 0$。\n- 所有观测值具有独立同分布的尺度 $s = 0.1$。\n- 使用正演模型定义所隐含的 $h(m)$ 相对于 $m$ 的精确雅可比矩阵。\n\n将测试套件定义为以下四种情况，每种情况由 $(\\nu, \\Delta_{\\mathrm{out}})$ 指定：\n- 情况 A: $(\\nu = 3.0, \\Delta_{\\mathrm{out}} = 1.5)$。\n- 情况 B: $(\\nu = 30.0, \\Delta_{\\mathrm{out}} = 1.5)$。\n- 情况 C: $(\\nu = 1.0, \\Delta_{\\mathrm{out}} = 1.5)$。\n- 情况 D: $(\\nu = 3.0, \\Delta_{\\mathrm{out}} = 0.0)$。\n\n对于每种情况，计算在 $m_0$ 处评估的以下四个量：\n- $g^{(t)}$-范数：学生t负对数似然梯度的欧几里得范数。\n- $g^{(g)}$-范数：高斯负对数似然梯度的欧几里得范数。\n- 异常值影响比：$\\left|\\psi^{(t)}(r_{i_{\\mathrm{out}}})\\right| \\big/ \\left|\\psi^{(g)}(r_{i_{\\mathrm{out}}})\\right|$，其中 $\\psi^{(t)}$ 和 $\\psi^{(g)}$ 分别是学生t和高斯的影响函数。\n- 典型值影响比：$\\left|\\psi^{(t)}(r_{i_{\\mathrm{typ}}})\\right| \\big/ \\left|\\psi^{(g)}(r_{i_{\\mathrm{typ}}})\\right|$。\n\n所有量都是无量纲的；不需要物理单位。最终程序必须生成单行输出，其中包含四种情况的结果，格式为逗号分隔的列表的列表，每个内部列表按上述顺序包含四个浮点数。例如，输出格式必须严格为\n\"[[g_t_A,g_g_A,ratio_out_A,ratio_typ_A],[g_t_B,g_g_B,ratio_out_B,ratio_typ_B],[g_t_C,g_g_C,ratio_out_C,ratio_typ_C],[g_t_D,g_g_D,ratio_out_D,ratio_typ_D]]\"\n的形式，不含任何额外的空格或文本。所有数值结果必须以标准十进制表示法打印。", "solution": "我们从观测模型 $y^{\\mathrm{obs}} = h(m^{\\star}) + \\epsilon$ 开始，并定义残差 $r(m) = y^{\\mathrm{obs}} - h(m) \\in \\mathbb{R}^{n}$。用于数据同化的负对数似然泛函由 $-\\log p(y^{\\mathrm{obs}} \\mid m)$ 定义，相差一个不依赖于 $m$ 的加性常数。其关于 $m$ 的梯度通过链式法则求得，使用雅可比矩阵 $J(m) = \\partial h(m)/\\partial m \\in \\mathbb{R}^{n \\times p}$，其中 $p$ 是参数的数量。\n\n学生t误差的推导。假设残差分量是独立的，每个分量都服从自由度为 $\\nu$、正尺度为 $s$ 的学生t分布。单个分量的概率密度函数（PDF）为\n$$\np(r_i) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\, s} \\left(1 + \\frac{r_i^2}{\\nu s^2}\\right)^{-\\frac{\\nu+1}{2}},\n$$\n其中 $\\Gamma(\\cdot)$ 是伽马函数。独立分量的负对数似然为\n$$\n\\ell^{(t)}(m) = -\\sum_{i=1}^{n} \\log p(r_i(m)) = C + \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\log\\left(1 + \\frac{r_i(m)^2}{\\nu s^2}\\right),\n$$\n其中 $C$ 是一个不依赖于 $m$ 的常数。对 $\\ell^{(t)}(m)$ 关于 $m$ 求导，并使用链式法则 $\\, \\mathrm{d}r_i/\\mathrm{d}m = -J_i(m)$，其中 $J_i(m)$ 表示 $J(m)$ 的第 $i$ 行，得到\n$$\n\\nabla_m \\ell^{(t)}(m) = \\sum_{i=1}^{n} \\left[ \\frac{\\nu+1}{2} \\cdot \\frac{1}{1 + \\frac{r_i^2}{\\nu s^2}} \\cdot \\frac{2 r_i}{\\nu s^2} \\right] \\left(-J_i(m) \\right)\n= - J(m)^{\\top} \\psi^{(t)}(r(m)),\n$$\n其中影响函数向量 $\\psi^{(t)}(r) \\in \\mathbb{R}^{n}$ 的分量定义为\n$$\n\\psi^{(t)}_i(r_i) = \\frac{(\\nu+1) r_i}{\\nu s^2 + r_i^2}.\n$$\n\n高斯误差的推导。对于方差为 $s^2$ 的独立高斯误差，每个分量的 PDF 为\n$$\np(r_i) = \\frac{1}{\\sqrt{2\\pi} s} \\exp\\left(-\\frac{r_i^2}{2 s^2}\\right).\n$$\n负对数似然是\n$$\n\\ell^{(g)}(m) = C' + \\frac{1}{2} \\sum_{i=1}^{n} \\frac{r_i(m)^2}{s^2},\n$$\n其中 $C'$ 不依赖于 $m$。求导和链式法则给出\n$$\n\\nabla_m \\ell^{(g)}(m) = \\sum_{i=1}^{n} \\left( \\frac{r_i}{s^2} \\right) \\left( -J_i(m) \\right)\n= - J(m)^{\\top} \\psi^{(g)}(r(m)),\n$$\n其中高斯影响函数是\n$$\n\\psi^{(g)}_i(r_i) = \\frac{r_i}{s^2}.\n$$\n\n影响分析。影响函数的大小决定了每个残差分量对梯度的贡献程度。对于小残差 $|r_i| \\ll s \\sqrt{\\nu}$，我们有\n$$\n\\psi^{(t)}_i(r_i) \\approx \\frac{\\nu+1}{\\nu s^2} r_i,\n$$\n这与 $r_i$ 近似成线性关系，类似于高斯情况。对于大残差 $|r_i| \\gg s \\sqrt{\\nu}$，学生t分布的影响函数会饱和并衰减为\n$$\n\\psi^{(t)}_i(r_i) \\approx \\frac{\\nu+1}{r_i},\n$$\n因此其大小的行为类似于 $(\\nu+1)/|r_i|$，并随着异常值的增大而减小。相比之下，高斯影响函数随 $|r_i|$ 线性增长，为 $|r_i|/s^2$。因此，学生t似然函数会强烈地降低异常值的影响，而高斯似然函数会放大它们的影响。\n\n正演算子和雅可比矩阵。对于 $h_i(m) = k \\exp(-a t_i)$ 和 $m = [k,a]^{\\top}$，雅可比矩阵的各行为\n$$\n\\frac{\\partial h_i}{\\partial k} = \\exp(-a t_i), \\quad \\frac{\\partial h_i}{\\partial a} = -k t_i \\exp(-a t_i).\n$$\n将这些行（对于 $i=1,\\dots,n$）堆叠起来，得到 $J(m) \\in \\mathbb{R}^{n \\times 2}$。\n\n为每个测试案例 $(\\nu, \\Delta_{\\mathrm{out}})$ 计算所要求输出的算法步骤：\n1) 构建 $t = [0,1,2,3,4]$ 并用 $m^{\\star} = [1.5, 0.4]^{\\top}$ 计算 $y^{\\mathrm{base}} = h(m^{\\star})$。\n2) 复制 $y^{\\mathrm{base}}$ 来构成 $y^{\\mathrm{obs}}$，并在索引 $i_{\\mathrm{out}} = 2$ 处加上 $\\Delta_{\\mathrm{out}}$。\n3) 在 $m_0 = [1.2, 0.5]^{\\top}$ 处，计算残差 $r = y^{\\mathrm{obs}} - h(m_0)$ 和雅可比矩阵 $J(m_0)$。\n4) 使用 $\\psi^{(t)}_i(r_i) = (\\nu+1) r_i / (\\nu s^2 + r_i^2)$（其中 $s = 0.1$）计算 $\\psi^{(t)}(r)$，并使用 $\\psi^{(g)}_i(r_i) = r_i / s^2$ 计算 $\\psi^{(g)}(r)$。\n5) 计算梯度 $\\nabla_m \\ell^{(t)}(m_0) = - J(m_0)^{\\top} \\psi^{(t)}(r)$ 和 $\\nabla_m \\ell^{(g)}(m_0) = - J(m_0)^{\\top} \\psi^{(g)}(r)$，以及它们的欧几里得范数。\n6) 计算异常值影响比 $\\left|\\psi^{(t)}(r_{i_{\\mathrm{out}}})\\right| / \\left|\\psi^{(g)}(r_{i_{\\mathrm{out}}})\\right|$ 和典型值影响比 $\\left|\\psi^{(t)}(r_{i_{\\mathrm{typ}}})\\right| / \\left|\\psi^{(g)}(r_{i_{\\mathrm{typ}}})\\right|$，其中 $i_{\\mathrm{typ}} = 0$。\n7) 为每个测试案例以指定的单行列表的列表格式返回四个浮点数。\n\n定性预期。对于 $(\\nu = 3.0, \\Delta_{\\mathrm{out}} = 1.5)$ 和 $(\\nu = 1.0, \\Delta_{\\mathrm{out}} = 1.5)$，由于强烈的权重降低效应，异常值影响比应显著低于1，其中 $\\nu=1.0$ 的情况会产生最小的比率。对于 $(\\nu = 30.0, \\Delta_{\\mathrm{out}} = 1.5)$，异常值影响比应接近1，反映了近高斯行为。当 $\\Delta_{\\mathrm{out}} = 0.0$ 时，两个比率都应接近1，因为残差不受异常值主导，且两个模型在原点附近表现相似。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef forward_model(m, t):\n    \"\"\"\n    Forward operator h(m): h_i = k * exp(-a * t_i)\n    m: array-like, shape (2,) with m = [k, a]\n    t: array-like of times\n    Returns y_pred of shape (n_times,)\n    \"\"\"\n    k, a = m\n    t = np.asarray(t, dtype=float)\n    return k * np.exp(-a * t)\n\ndef jacobian(m, t):\n    \"\"\"\n    Jacobian J(m) of h(m) with respect to m = [k, a]\n    J[i, 0] = d h_i / d k = exp(-a * t_i)\n    J[i, 1] = d h_i / d a = -k * t_i * exp(-a * t_i)\n    \"\"\"\n    k, a = m\n    t = np.asarray(t, dtype=float)\n    exp_term = np.exp(-a * t)\n    J = np.zeros((t.size, 2), dtype=float)\n    J[:, 0] = exp_term\n    J[:, 1] = -k * t * exp_term\n    return J\n\ndef influence_student_t(r, s, nu):\n    \"\"\"\n    Student-t influence function psi^(t)_i = ((nu+1) * r_i) / (nu * s^2 + r_i^2)\n    r: residual vector\n    s: scalar scale\n    nu: degrees of freedom (scalar)\n    \"\"\"\n    r = np.asarray(r, dtype=float)\n    denom = nu * (s ** 2) + r**2\n    return (nu + 1.0) * r / denom\n\ndef influence_gaussian(r, s):\n    \"\"\"\n    Gaussian influence function psi^(g)_i = r_i / s^2\n    \"\"\"\n    r = np.asarray(r, dtype=float)\n    return r / (s ** 2)\n\ndef gradient_from_influence(m, t, psi):\n    \"\"\"\n    Compute gradient of negative log-likelihood: grad = - J^T * psi\n    \"\"\"\n    J = jacobian(m, t)\n    return - J.T @ psi\n\ndef run_case(nu, delta_out):\n    # Setup constants\n    t = np.array([0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n    m_true = np.array([1.5, 0.4], dtype=float)\n    m0 = np.array([1.2, 0.5], dtype=float)\n    s = 0.1\n    i_out = 2\n    i_typ = 0\n\n    # Build observations with specified outlier\n    y_base = forward_model(m_true, t)\n    y_obs = y_base.copy()\n    y_obs[i_out] += delta_out\n\n    # Residual at m0\n    r = y_obs - forward_model(m0, t)\n\n    # Influences\n    psi_t = influence_student_t(r, s, nu)\n    psi_g = influence_gaussian(r, s)\n\n    # Gradients\n    grad_t = gradient_from_influence(m0, t, psi_t)\n    grad_g = gradient_from_influence(m0, t, psi_g)\n\n    # Norms\n    g_t_norm = float(np.linalg.norm(grad_t))\n    g_g_norm = float(np.linalg.norm(grad_g))\n\n    # Influence ratios\n    # Guard against division by zero in ratios; if denominator is zero, define ratio as np.nan\n    denom_out = abs(psi_g[i_out])\n    denom_typ = abs(psi_g[i_typ])\n    ratio_out = float(abs(psi_t[i_out]) / denom_out) if denom_out != 0.0 else float(\"nan\")\n    ratio_typ = float(abs(psi_t[i_typ]) / denom_typ) if denom_typ != 0.0 else float(\"nan\")\n\n    return [g_t_norm, g_g_norm, ratio_out, ratio_typ]\n\ndef solve():\n    # Define the test cases from the problem statement: (nu, delta_out)\n    test_cases = [\n        (3.0, 1.5),   # Case A\n        (30.0, 1.5),  # Case B\n        (1.0, 1.5),   # Case C\n        (3.0, 0.0),   # Case D\n    ]\n\n    results = []\n    for nu, delta_out in test_cases:\n        result = run_case(nu, delta_out)\n        # Optionally round to a reasonable number of decimals for stable textual output\n        rounded = [round(x, 10) if isinstance(x, float) and np.isfinite(x) else x for x in result]\n        results.append(rounded)\n\n    # Format as required: single line, list of lists, comma-separated, no extra spaces\n    def fmt_list(lst):\n        return \"[\" + \",\".join(str(x) for x in lst) + \"]\"\n    out = \"[\" + \",\".join(fmt_list(r) for r in results) + \"]\"\n    print(out)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3389148"}, {"introduction": "在启动昂贵而复杂的数据同化工作流之前，评估我们究竟能从可用数据中学到什么是至关重要的。本练习将通过对灵敏度矩阵（雅可比矩阵）进行奇异值分解（SVD），介绍一种强大的参数可辨识性分析方法，用以判断哪些参数组合能被数据有效约束。在此基础上，您将进一步实现一个D-最优设计（D-optimal design）算法，学会如何在预算和操作限制下，智能地选择能最大程度降低参数不确定性的未来观测量，这是规划数据采集方案的一项核心技能。[@problem_id:3389162]", "problem": "给定一个用于油藏建模中历史拟合的线性化数据同化设置，该设置采用导点对数渗透率参数化。从参数到数据的正向映射表示为 $G(m)$，其中 $m \\in \\mathbb{R}^{n_m}$ 代表导点处的对数渗透率。灵敏度（雅可比）矩阵是在背景参数 $m_0$ 处评估的 $J(m_0) = \\partial G(m)/\\partial m \\big|_{m_0}$。假设在单相、微可压缩流态下，通过线性化和叠加，测量影响会随着空间分离而衰减，并随时间根据经过充分测试的指数形式饱和。\n\n基本基底和模型规范：\n- 在井 $w$ 和时间 $t$ 处，对导点 $i$ 处对数渗透率的单位扰动的响应，与空间核 $K(d_{iw})$ 和时间增益 $T(t)$ 成正比，其中 $d_{iw}$ 是导点 $i$ 和井 $w$ 之间的欧几里得距离。使用 $K(d) = \\exp(-d/L)$ 和 $T(t) = 1 - \\exp(-t/\\tau)$，其中 $L > 0$ 是空间影响长度，$\\tau > 0$ 是特征时间常数。在此假设下，对应于测量 $(w,t)$ 的雅可比矩阵行的条目为\n$$\nJ_{(w,t),i} = K\\!\\left(d_{iw}\\right)\\, T(t) = \\exp\\!\\left(-\\frac{d_{iw}}{L}\\right) \\left(1 - \\exp\\!\\left(-\\frac{t}{\\tau}\\right)\\right).\n$$\n- 测量的噪声是独立的，服从高斯分布，标准差为 $\\sigma_{(w,t)}$；测量协方差 $R$ 是对角矩阵，其条目为 $\\sigma_{(w,t)}^2$；逆协方差（权重）为 $W = R^{-1}$。对 $m$ 的高斯先验产生一个先验精度矩阵 $\\Gamma_{\\text{prior}}^{-1} = \\alpha I$，其中 $\\alpha > 0$。\n\n可辨识性分析：\n- 对 $J$ 进行奇异值分解 (SVD)，即 $J = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{n_d \\times n_d}$，$\\Sigma \\in \\mathbb{R}^{n_d \\times n_m}$ 是具有非负对角奇异值 $s_1 \\ge s_2 \\ge \\dots$ 的矩阵，以及 $V \\in \\mathbb{R}^{n_m \\times n_m}$。定义一个阈值 $\\epsilon \\in (0,1)$，并将可辨识方向的数量 $N_{\\text{eff}}$ 计为严格大于 $\\epsilon s_{\\max}$ 的奇异值的数量，即\n$$\nN_{\\text{eff}} = \\left|\\left\\{ i \\, : \\, s_i > \\epsilon \\, s_{\\max} \\right\\}\\right|, \\quad s_{\\max} = \\max_i s_i.\n$$\n\nD-最优采集设计：\n- 将选定的测量机会子集 $\\mathcal{S}$ 的费雪信息矩阵 (FIM) 定义为\n$$\nF(\\mathcal{S}) = \\Gamma_{\\text{prior}}^{-1} + \\sum_{(w,t) \\in \\mathcal{S}} \\frac{1}{\\sigma_{(w,t)}^2} \\, J_{(w,t)}^\\top J_{(w,t)},\n$$\n其中 $J_{(w,t)} \\in \\mathbb{R}^{n_m}$ 表示 $J$ 中对应于测量 $(w,t)$ 的行。D-最优性准则旨在最大化 $\\log\\det(F(\\mathcal{S}))$，同时满足以下操作约束：\n- 全局预算约束：最多选择 $K$ 个测量。\n- 每时间点限制：在时间 $t_k$，最多选择 $q_k$ 个测量。\n- 不可用性：一组测量机会 $\\mathcal{U}$ 不允许被选择。\n\n使用贪心选择算法，该算法从先验精度 $\\Gamma_{\\text{prior}}^{-1}$ 开始，每次迭代添加一个能最大程度增加 $\\log\\det(F)$ 的容许测量，直到预算用尽或没有剩余的容许测量为止。\n\n采集方案的编码：\n- 按固定顺序（时间为主序，然后在每个时间内按井索引）枚举所有测量机会。将选定的子集 $\\mathcal{S}$ 编码为一个整数位掩码 $E = \\sum_{j \\in \\mathcal{I}(\\mathcal{S})} 2^j$，其中 $j$ 是测量机会在固定顺序中的索引，$\\mathcal{I}(\\mathcal{S})$ 给出所选测量的索引集合。\n\n容差：\n- 使用 $\\epsilon = 10^{-2}$ 来定义可辨识性阈值。\n\n您的任务：\n1.  对于每个测试案例，使用指定的 $K(d)$ 和 $T(t)$ 以及根据给定坐标计算的距离 $d_{iw}$ 来构造 $J$。\n2.  使用阈值规则和 $\\epsilon = 10^{-2}$，通过 $J$ 的 SVD 计算 $N_{\\text{eff}}$。\n3.  在约束条件下执行贪心 D-最优采集，以产生优化的 $\\log\\det(F)$ 和编码后的方案整数 $E$。\n\n测试套件与参数：\n- 所有案例的导点均固定，有 $n_m = 5$ 个，坐标为\n$$\np_0 = (0.0, 0.0), \\quad p_1 = (1.0, 0.0), \\quad p_2 = (0.0, 1.0), \\quad p_3 = (1.0, 1.0), \\quad p_4 = (0.5, 0.6).\n$$\n- 所有案例的井均固定，有 $n_w = 4$ 口，坐标为\n$$\nw_0 = (0.1, 0.2), \\quad w_1 = (0.9, 0.1), \\quad w_2 = (0.2, 0.8), \\quad w_3 = (0.8, 0.9).\n$$\n\n案例1（标准情况）：\n- 时间 $t \\in \\{0.5, 1.0, 2.0\\}$，因此 $n_t = 3$ 且 $n_d = n_t \\cdot n_w = 12$。\n- 空间长度 $L = 0.5$，时间常数 $\\tau = 0.8$。\n- 每口井的噪声基准：$\\sigma_{\\text{base}} = [0.04, 0.05, 0.04, 0.05]$；对于时间 $t$ 的每次测量，使用 $\\sigma_{(w,t)} = \\sigma_{\\text{base},w} \\sqrt{1 + t}$。\n- 先验精度标量 $\\alpha = 0.01$，因此 $\\Gamma_{\\text{prior}}^{-1} = \\alpha I$。\n- 全局预算 $K = 5$。\n- 每时间点限制 $(q_0, q_1, q_2) = (2, 2, 1)$。\n- 不可用集合 $\\mathcal{U} = \\{(t = 1.0, w = 3)\\}$，即在时间 $1.0$ 时，索引为 $3$ 的井不能进行测量。\n\n案例2（边界约束，零采集）：\n- 时间 $t \\in \\{0.5\\}$，因此 $n_t = 1$ 且 $n_d = 4$。\n- 空间长度 $L = 0.5$，时间常数 $\\tau = 0.6$。\n- 每口井的噪声基准：$\\sigma_{\\text{base}} = [0.2, 0.2, 0.2, 0.2]$；对于每次测量，$\\sigma_{(w,t)} = \\sigma_{\\text{base},w} \\sqrt{1 + t}$。\n- 先验精度标量 $\\alpha = 0.02$，因此 $\\Gamma_{\\text{prior}}^{-1} = \\alpha I$。\n- 全局预算 $K = 0$。\n- 每时间点限制 $(q_0) = (0)$。\n- 不可用集合 $\\mathcal{U} = \\emptyset$。\n\n案例3（近秩亏和有限的每时间点采集）：\n- 时间 $t \\in \\{0.2, 0.3, 0.4\\}$，因此 $n_t = 3$ 且 $n_d = 12$。\n- 空间长度 $L = 1.5$，时间常数 $\\tau = 1.5$。\n- 每口井的噪声基准：$\\sigma_{\\text{base}} = [0.1, 0.1, 0.12, 0.1]$；对于每次测量，$\\sigma_{(w,t)} = \\sigma_{\\text{base},w} \\sqrt{1 + t}$。\n- 先验精度标量 $\\alpha = 0.0001$，因此 $\\Gamma_{\\text{prior}}^{-1} = \\alpha I$。\n- 全局预算 $K = 2$。\n- 每时间点限制 $(q_0, q_1, q_2) = (1, 1, 0)$。\n- 不可用集合 $\\mathcal{U} = \\emptyset$。\n\n角度单位不适用。无需物理单位转换；所有量均为无量纲。\n\n所需输出规范：\n- 对于每个案例，按顺序计算并返回三个值：可辨识数量 $N_{\\text{eff}}$（一个整数）、优化的 $\\log\\det(F)$（一个浮点数）和编码后的方案整数 $E$（一个整数）。将所有三个案例的结果汇总到一个列表中，按案例排序，在案例内部按指定的值顺序排序。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_9]$）。", "solution": "该问题要求对油藏历史拟合的线性化数据同化设置进行分析。解决方案涉及对三个不同测试案例执行三个主要任务：（1）构建灵敏度（雅可比）矩阵，（2）通过奇异值分解（SVD）进行可辨识性分析，以及（3）使用贪心算法进行 D-最优实验设计。\n\n每个案例的方法论步骤如下：\n\n首先，我们建立几何配置。计算每个 $n_m=5$ 个导点 $p_i$ 与 $n_w=4$ 口井 $w_w$ 之间的欧几里得距离 $d_{iw}$。这些距离在所有测试案例中都是恒定的。坐标提供如下：\n- 导点：$p_0 = (0.0, 0.0), p_1 = (1.0, 0.0), p_2 = (0.0, 1.0), p_3 = (1.0, 1.0), p_4 = (0.5, 0.6)$。\n- 井：$w_0 = (0.1, 0.2), w_1 = (0.9, 0.1), w_2 = (0.2, 0.8), w_3 = (0.8, 0.9)$。\n\n距离 $d_{iw}$ 计算为 $d_{iw} = \\sqrt{(p_{ix} - w_{wx})^2 + (p_{iy} - w_{wy})^2}$。\n\n**1. 雅可比矩阵构建**\n\n雅可比矩阵 $J$ 的维度为 $n_d \\times n_m$，它量化了测量对对数渗透率参数扰动的敏感性。测量次数为 $n_d = n_t \\cdot n_w$，其中 $n_t$ 是测量时间的数量，$n_w$ 是井的数量。问题为 $J$ 的条目指定了一个唯象模型。对应于在井 $w$ 和时间 $t$ 进行的测量，以及第 $i$ 个导点参数的条目由下式给出：\n$$\nJ_{(w,t),i} = K(d_{iw}) T(t) = \\exp\\left(-\\frac{d_{iw}}{L}\\right) \\left(1 - \\exp\\left(-\\frac{t}{\\tau}\\right)\\right)\n$$\n其中 $L$ 是空间影响长度，$\\tau$ 是特征时间常数。对于每个案例，我们通过为所有测量机会（按时间主序，然后是井主序索引）和所有导点计算此值，来构建完整的 $n_d \\times n_m$ 矩阵 $J$。\n\n**2. 可辨识性分析**\n\n有效可辨识的参数组合数量 $N_{\\text{eff}}$ 是从雅可比矩阵 $J$ 的奇异值谱中估计的。我们对 $J$ 进行奇异值分解（SVD）：\n$$\nJ = U \\Sigma V^\\top\n$$\n其中 $\\Sigma$ 是一个矩形对角矩阵，其对角线上有非负奇异值 $s_1 \\ge s_2 \\ge \\dots \\ge s_{\\min(n_d, n_m)} \\ge 0$。可辨识方向的数量定义为严格大于最大奇异值 $s_{\\max} = s_1$ 的阈值分数 $\\epsilon$ 的奇异值的计数。给定 $\\epsilon = 10^{-2}$，我们计算：\n$$\nN_{\\text{eff}} = \\left|\\left\\{ i \\, : \\, s_i > 10^{-2} s_{\\max} \\right\\}\\right|\n$$\n该值提供了关于雅可比矩阵的实际秩以及可以被数据约束的独立参数方向数量的洞察。\n\n**3. D-最优采集设计**\n\n目标是在一组约束下，选择一个关于参数信息量最大的测量子集 $\\mathcal{S}$。我们使用 D-最优性准则，该准则旨在最大化费雪信息矩阵（FIM）的行列式。对于集合 $\\mathcal{S}$ 的 FIM 是：\n$$\nF(\\mathcal{S}) = \\Gamma_{\\text{prior}}^{-1} + \\sum_{(w,t) \\in \\mathcal{S}} \\frac{1}{\\sigma_{(w,t)}^2} J_{(w,t)}^\\top J_{(w,t)}\n$$\n其中 $\\Gamma_{\\text{prior}}^{-1} = \\alpha I$ 是先验精度矩阵，$\\sigma_{(w,t)}^2$ 是测量方差，而 $J_{(w,t)}$ 是对应于测量 $(w,t)$ 的雅可比矩阵的行。最大化 $\\det(F(\\mathcal{S}))$ 等同于最小化参数的后验不确定性椭球的体积。为数值稳定性和方便起见，我们最大化其对数 $\\log\\det(F(\\mathcal{S}))$。\n\n由于找到最优的 $\\mathcal{S}$ 是一个组合问题，我们采用贪心算法：\n1.  用先验信息初始化 FIM：$F_{current} = \\Gamma_{\\text{prior}}^{-1}$。所选测量的集合初始为空，$\\mathcal{S} = \\emptyset$。\n2.  逐个迭代添加测量，最多达到全局预算 $K$。在每次迭代中：\n    a. 识别容许的候选测量集合。一个测量是容许的，如果它不在不可用集合 $\\mathcal{U}$ 中，尚未被选择，并且其选择不违反每时间点的采集限制。\n    b. 对于每个容许的候选者 $j$，计算目标函数的潜在增益：$\\Delta_j = \\log\\det(F_{current} + \\frac{1}{\\sigma_j^2} J_j^\\top J_j) - \\log\\det(F_{current})$。\n    c. 选择产生最大增益的测量 $j^*$，即 $\\arg\\max_j \\Delta_j$。\n    d. 更新 FIM：$F_{current} \\leftarrow F_{current} + \\frac{1}{\\sigma_{j^*}^2} J_{j^*}^\\top J_{j^*}$。将 $j^*$ 添加到 $\\mathcal{S}$ 中，并更新相应时间已进行的测量计数。\n3.  当选择了 $K$ 个测量或没有更多可容许的测量可以添加时，过程终止。\n4.  最终的 $\\log\\det(F_{current})$ 是优化后的值。所选方案被编码为位掩码整数 $E = \\sum_{j \\in \\mathcal{S}} 2^j$，其中 $j$ 是测量在时间主序、井主序枚举中的从零开始的索引。\n\n此过程应用于三个测试案例中的每一个，使用它们特定的参数（$L, \\tau, \\alpha, K$ 等）。\n\n**案例1：** 一个标准场景，预算为 $K=5$，混合的每时间点限制，以及一个不可用的测量。所有三个任务都如上所述执行。\n\n**案例2：** 一个边界情况，其中采集预算 $K=0$，每时间点限制也为 $0$。对雅可比矩阵进行可辨识性分析。对于采集设计，不选择任何测量。最终的 FIM 就是先验精度矩阵 $\\Gamma_{\\text{prior}}^{-1}$，方案编码 $E$ 为 $0$。\n\n**案例3：** 一个设计为病态的案例，其参数 $L$ 和 $\\tau$ 导致雅可比矩阵近乎秩亏。预算很小（$K=2$），并且每时间点限制很严格，有一个时间段完全不允许采集。遵循标准程序。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reservoir modeling problem for three test cases.\n    For each case, it computes:\n    1. Neff: The number of identifiable parameter directions.\n    2. log_det_F: The D-optimal log-determinant of the Fisher Information Matrix.\n    3. E: The integer encoding of the optimal measurement schedule.\n    \"\"\"\n    pilot_points = np.array([\n        [0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [0.5, 0.6]\n    ])\n    wells = np.array([\n        [0.1, 0.2], [0.9, 0.1], [0.2, 0.8], [0.8, 0.9]\n    ])\n\n    n_m = pilot_points.shape[0]\n    n_w = wells.shape[0]\n\n    # Pre-compute distance matrix d_iw\n    dist_matrix = np.zeros((n_m, n_w))\n    for i in range(n_m):\n        for w in range(n_w):\n            dist_matrix[i, w] = np.linalg.norm(pilot_points[i] - wells[w])\n\n    test_cases = [\n        # Case 1\n        {\n            \"times\": [0.5, 1.0, 2.0],\n            \"L\": 0.5, \"tau\": 0.8,\n            \"sigma_base\": [0.04, 0.05, 0.04, 0.05],\n            \"alpha\": 0.01,\n            \"K\": 5,\n            \"q_k\": [2, 2, 1],\n            \"unavailable\": [(1.0, 3)],\n        },\n        # Case 2\n        {\n            \"times\": [0.5],\n            \"L\": 0.5, \"tau\": 0.6,\n            \"sigma_base\": [0.2, 0.2, 0.2, 0.2],\n            \"alpha\": 0.02,\n            \"K\": 0,\n            \"q_k\": [0],\n            \"unavailable\": [],\n        },\n        # Case 3\n        {\n            \"times\": [0.2, 0.3, 0.4],\n            \"L\": 1.5, \"tau\": 1.5,\n            \"sigma_base\": [0.1, 0.1, 0.12, 0.1],\n            \"alpha\": 0.0001,\n            \"K\": 2,\n            \"q_k\": [1, 1, 0],\n            \"unavailable\": [],\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        times = case[\"times\"]\n        L, tau = case[\"L\"], case[\"tau\"]\n        sigma_base = np.array(case[\"sigma_base\"])\n        alpha, K = case[\"alpha\"], case[\"K\"]\n        q_k = case[\"q_k\"]\n        unavailable_spec = case[\"unavailable\"]\n        \n        n_t = len(times)\n        n_d = n_t * n_w\n\n        # --- 1. Construct Jacobian J ---\n        J = np.zeros((n_d, n_m))\n        sigmas = np.zeros(n_d)\n        \n        time_map = {t: i for i, t in enumerate(times)}\n        unavailable_indices = {time_map[t] * n_w + w for t, w in unavailable_spec}\n\n        for t_idx, t_val in enumerate(times):\n            temporal_gain = 1.0 - np.exp(-t_val / tau)\n            for w_idx in range(n_w):\n                meas_idx = t_idx * n_w + w_idx\n                spatial_kernel = np.exp(-dist_matrix[:, w_idx] / L)\n                J[meas_idx, :] = spatial_kernel * temporal_gain\n                sigmas[meas_idx] = sigma_base[w_idx] * np.sqrt(1.0 + t_val)\n\n        # --- 2. Compute N_eff ---\n        if J.shape[0] > 0:\n            singular_values = np.linalg.svd(J, compute_uv=False)\n            s_max = singular_values[0] if len(singular_values) > 0 else 0.0\n            if s_max > 0:\n                N_eff = np.sum(singular_values > 1e-2 * s_max)\n            else:\n                N_eff = 0\n        else:\n            N_eff = 0\n\n        # --- 3. Greedy D-optimal Design ---\n        F = alpha * np.identity(n_m)\n        selected_indices = set()\n        \n        counts_per_time = np.zeros(n_t, dtype=int)\n        \n        available_indices = set(range(n_d)) - unavailable_indices\n\n        for _ in range(K):\n            best_gain = -1.0\n            best_idx = -1\n            \n            candidate_indices = []\n            for idx in available_indices:\n                t_idx = idx // n_w\n                if counts_per_time[t_idx]  q_k[t_idx]:\n                    candidate_indices.append(idx)\n            \n            if not candidate_indices:\n                break\n            \n            _, current_log_det_F = np.linalg.slogdet(F)\n\n            for idx in candidate_indices:\n                J_row = J[idx, :].reshape(1, -1)\n                rank_one_update = (J_row.T @ J_row) / (sigmas[idx]**2)\n                F_candidate = F + rank_one_update\n                \n                _, log_det_candidate = np.linalg.slogdet(F_candidate)\n                gain = log_det_candidate - current_log_det_F\n                \n                if gain > best_gain:\n                    best_gain = gain\n                    best_idx = idx\n\n            if best_idx != -1:\n                J_row = J[best_idx, :].reshape(1, -1)\n                rank_one_update = (J_row.T @ J_row) / (sigmas[best_idx]**2)\n                F += rank_one_update\n                \n                selected_indices.add(best_idx)\n                available_indices.remove(best_idx)\n                \n                t_idx = best_idx // n_w\n                counts_per_time[t_idx] += 1\n            else:\n                # No admissible measurement can be added\n                break\n\n        _, final_log_det_F = np.linalg.slogdet(F)\n        \n        E = 0\n        for idx in selected_indices:\n            E += 2**idx\n\n        all_results.extend([int(N_eff), final_log_det_F, int(E)])\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3389162"}, {"introduction": "虽然基于梯度的方法可以找到一个最优模型，但要全面量化不确定性，则需要完整的贝叶斯分析。然而，对历史匹配中典型的复杂后验分布进行直接采样往往非常困难。这个高级练习将指导您实现一种用于随机最大似然法（RML）的自适应回火方案，这是一种能够从宽泛的先验分布平稳地过渡到集中的后验分布的稳健方法，同时确保算法的数值稳定性和统计合理性。[@problem_id:3389130]", "problem": "您的任务是在油藏建模历史拟合（作为一种带数据同化的反演问题）的背景下，为随机最大似然 (RML) 设计并实现一种自适应退火方案。其目标是自动选择退火增量 $\\{\\Delta \\beta_k\\}$，进而确定累积退火因子 $\\{\\beta_k\\}$，从而在每个阶段使集合保持可接受的有效样本量 (ESS)，并使数据失配遵循差异原则。所有量均为无量纲。\n\n请从以下基本基础开始：\n\n- 带有调和似然的贝叶斯推断：在退火因子 $\\beta \\in [0,1]$ 处，后验密度与 $\\exp\\big(-\\beta \\, \\Phi(x)\\big) \\, \\pi(x)$ 成正比，其中 $\\pi(x)$ 是先验密度，$\\Phi(x)$ 是数据失配函数。\n- 高斯观测噪声：定义白化失配函数为 $\\Phi(x) = \\frac{1}{2} \\big\\| \\Gamma^{-1/2} (G(x) - y) \\big\\|^2$，其中 $G(x)$ 是正演算子，$y$ 是数据向量，$\\Gamma$ 是观测噪声协方差。当 $\\Gamma = \\sigma^2 I_m$ 且数据通过 $\\Gamma^{-1/2}$ 白化时，差异原则规定后验的期望失配应为 $\\frac{m}{2}$，其中 $m$ 是数据维度。\n- 退火下的序列重要性采样权重：给定一个集合 $\\{x_i\\}_{i=1}^J$ 和一个建议的增量 $\\Delta \\beta_k$，未归一化的重要性权重为 $v_i(\\Delta \\beta_k) = \\exp\\big(-\\Delta \\beta_k \\, \\Phi_i^k\\big)$，其中 $\\Phi_i^k := \\Phi(x_i^k)$ 是当前阶段的失配，归一化权重为 $w_i(\\Delta \\beta_k) = \\frac{v_i(\\Delta \\beta_k)}{\\sum_{j=1}^J v_j(\\Delta \\beta_k)}$。\n- 有效样本量 (ESS)：${\\rm ESS}(\\Delta \\beta_k) = \\frac{1}{\\sum_{i=1}^J \\big(w_i(\\Delta \\beta_k)\\big)^2}$。\n\n您将实现一个在每个阶段自适应选择 $\\Delta \\beta_k$ 的方案，并满足以下要求：\n\n1. 给定当前集合的失配 $\\{\\Phi_i^k\\}_{i=1}^J$，选择 $\\Delta \\beta_k \\in [0, 1 - \\beta_k]$，使得加权平均失配遵循差异原则：\n   $$\\sum_{i=1}^J w_i(\\Delta \\beta_k) \\, \\Phi_i^k = \\frac{m}{2}。$$\n2. 通过施加 ESS 约束来控制退化：\n   $${\\rm ESS}(\\Delta \\beta_k) \\geq \\eta J, $$\n   其中 $\\eta \\in (0,1)$ 是一个预设的 ESS 分数。\n3. 如果容许区间内没有 $\\Delta \\beta_k$ 能满足差异原则，则选择仍能满足 ${\\rm ESS}(\\Delta \\beta_k) \\geq \\eta J$ 的最大 $\\Delta \\beta_k$。\n4. 更新累积退火因子为 $\\beta_{k+1} = \\beta_k + \\Delta \\beta_k$。\n5. 使用与线性高斯 RML 一致的确定性插值，将集合状态向最大后验 (MAP) 点传播：在累积退火 $\\beta$ 处，定义集合状态为\n   $$x_i(\\beta) = (1 - \\beta) \\, \\xi_i + \\beta \\, x_{\\rm MAP},$$\n   其中 $\\xi_i$ 是独立的先验样本，$x_{\\rm MAP}$ 是二次目标\n   $$J(x) = \\frac{1}{2} \\big\\| \\Gamma^{-1/2} (H x - y) \\big\\|^2 + \\frac{1}{2} \\big\\| C^{-1/2} (x - \\mu) \\big\\|^2$$\n   的最小化子，其中 $G(x) = H x$ 是一个线性正演算子，$C$ 是先验协方差，$\\mu$ 是先验均值。对于线性高斯情况，$x_{\\rm MAP}$ 是以下方程的解：\n   $$(C^{-1} + H^\\top \\Gamma^{-1} H) \\, x_{\\rm MAP} = C^{-1} \\mu + H^\\top \\Gamma^{-1} y。$$\n\n您的任务是实现一个完整的、可运行的程序，该程序：\n\n- 构建一个合成线性油藏模型，其中 $G(x) = H x$，$H \\in \\mathbb{R}^{m \\times n}$ 是一个其元素从标准正态分布中抽取的随机矩阵，先验为高斯分布 $x \\sim \\mathcal{N}(\\mu, C)$，其中 $\\mu = 0$ 和 $C = I_n$，观测噪声为高斯噪声，其协方差为 $\\Gamma = \\sigma^2 I_m$。\n- 从先验中生成一个真实状态 $x_{\\rm true}$ 和合成观测数据 $y = H x_{\\rm true} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$。\n- 从先验中初始化一个集合 $\\{\\xi_i\\}_{i=1}^J$。在阶段 $k$ 且累积 $\\beta_k$ 时，通过 $x_i^k = (1 - \\beta_k) \\xi_i + \\beta_k x_{\\rm MAP}$ 计算当前集合 $\\{x_i^k\\}$ 以及相应的失配 $\\Phi_i^k = \\frac{1}{2} \\|\\Gamma^{-1/2}(H x_i^k - y)\\|^2$。\n- 使用二分法对差异函数 $\\sum_i w_i(\\Delta \\beta_k) \\Phi_i^k - \\frac{m}{2}$ 进行求解，自适应地计算 $\\Delta \\beta_k$，同时通过将 $\\Delta \\beta_k$ 裁剪到满足 ${\\rm ESS}(\\Delta \\beta_k) \\geq \\eta J$ 的最大值来强制执行 ESS 约束。使用数值稳定的计算方法来计算权重。\n- 重复各个阶段，直到 $\\beta$ 达到 $1$ 或达到指定的最大阶段数。返回每个测试用例的累积退火因子序列 $\\{\\beta_k\\}$。\n\n重要实现说明：\n\n- 所有量均为无量纲；不使用任何物理单位。\n- 不涉及角度，因此无需特别说明。\n- 百分比必须表示为小数（例如，ESS 阈值分数 $\\eta$ 是一个小数）。\n\n测试套件：\n\n提供并解决以下测试用例。每个用例指定 $(n,m,J,\\sigma,\\eta,{\\rm max\\_stages},{\\rm seed})$。\n\n- 用例 1：$n=10$, $m=20$, $J=50$, $\\sigma=0.1$, $\\eta=0.6$, ${\\rm max\\_stages}=6$, ${\\rm seed}=1$。\n- 用例 2：$n=10$, $m=20$, $J=50$, $\\sigma=0.1$, $\\eta=0.9$, ${\\rm max\\_stages}=6$, ${\\rm seed}=1$。\n- 用例 3：$n=10$, $m=20$, $J=50$, $\\sigma=2.0$, $\\eta=0.6$, ${\\rm max\\_stages}=6$, ${\\rm seed}=2$。\n\n最终输出格式：\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个元素对应一个测试用例，并且其本身是每个阶段累积退火因子的列表。例如，输出应如下所示：\n$$[ [\\beta_1^{(1)}, \\beta_2^{(1)}, \\ldots], [\\beta_1^{(2)}, \\beta_2^{(2)}, \\ldots], [\\beta_1^{(3)}, \\beta_2^{(3)}, \\ldots] ]。$$", "solution": "该任务是为应用于线性高斯反演问题的随机最大似然 (RML) 设计并实现一种自适应退火算法，这是油藏建模历史拟合中的一种常见情景。该算法的核心是自动选择退火增量 $\\{\\Delta\\beta_k\\}$，这些增量决定了退火后验分布的序列。\n\n该问题被置于贝叶斯框架内。给定观测数据 $\\mathbf{y}$，模型参数 $\\mathbf{x}$ 的后验概率密度由贝叶斯定理给出：$p(\\mathbf{x}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\mathbf{x})p(\\mathbf{x})$。这里，$p(\\mathbf{x})$ 是先验密度，表示为 $\\pi(\\mathbf{x})$，$p(\\mathbf{y}|\\mathbf{x})$ 是似然。退火引入一个参数 $\\beta \\in [0, 1]$ 来逐渐桥接先验分布 ($\\beta=0$) 和后验分布 ($\\beta=1$)。$\\beta$-退火后验定义为：\n$$p_\\beta(\\mathbf{x}|\\mathbf{y}) \\propto [p(\\mathbf{y}|\\mathbf{x})]^\\beta \\pi(\\mathbf{x})$$\n假设噪声为高斯分布，似然与 $\\exp(-\\Phi(\\mathbf{x}))$ 成正比，其中 $\\Phi(\\mathbf{x})$ 是数据失配函数。因此，退火后验可以写为：\n$$p_\\beta(\\mathbf{x}|\\mathbf{y}) \\propto \\exp(-\\beta \\Phi(\\mathbf{x})) \\pi(\\mathbf{x})$$\n问题指定了线性正演模型 $\\mathbf{G}(\\mathbf{x}) = \\mathbf{H}\\mathbf{x}$，高斯先验 $\\mathbf{x} \\sim \\mathcal{N}(\\mu, \\mathbf{C})$，以及高斯观测噪声 $\\varepsilon \\sim \\mathcal{N}(\\mathbf{0}, \\Gamma)$。白化的数据失配为：\n$$\\Phi(\\mathbf{x}) = \\frac{1}{2} \\|\\Gamma^{-1/2} (\\mathbf{H}\\mathbf{x} - \\mathbf{y})\\|^2$$\n\n该自适应算法在每个阶段 $k$ 基于两个准则选择增量 $\\Delta\\beta_k$，使用一个粒子集合 $\\{ \\mathbf{x}_i^k \\}_{i=1}^J$ 来近似当前的分布 $p_{\\beta_k}(\\mathbf{x}|\\mathbf{y})$。当从 $\\beta_k$ 移动到 $\\beta_{k+1} = \\beta_k + \\Delta\\beta_k$ 时，粒子通过重要性权重 $w_i(\\Delta\\beta_k) \\propto \\exp(-\\Delta\\beta_k \\Phi(\\mathbf{x}_i^k))$ 被重新加权。\n\n选择 $\\Delta\\beta_k$ 的两个准则是：\n1.  **差异原则**：此原则为数据失配提供了一个目标。对于一个白化失配函数和维度为 $m$ 的数据，在真实后验下失配的期望值为 $\\mathbb{E}[\\Phi(\\mathbf{x})] = m/2$。该算法旨在选择 $\\Delta\\beta_k$，使得集合的加权平均失配与此目标相匹配：\n    $$\\sum_{i=1}^J w_i(\\Delta\\beta_k) \\Phi(\\mathbf{x}_i^k) = \\frac{m}{2}$$\n    这构成了关于 $\\Delta\\beta_k$ 的一个求根问题。\n\n2.  **有效样本量 (ESS)**：重要性采样可能导致权重退化，即少数几个粒子获得了几乎所有的权重，导致对分布的不良表示。有效样本量 ${\\rm ESS}(\\Delta\\beta_k) = \\left(\\sum_{i=1}^J [w_i(\\Delta\\beta_k)]^2\\right)^{-1}$ 对此进行量化。为防止退化，我们施加约束：\n    $${\\rm ESS}(\\Delta\\beta_k) \\ge \\eta J$$\n    其中 $\\eta \\in (0, 1)$ 是一个用户定义的阈值。由于 ESS 是 $\\Delta\\beta_k$ 的单调递减函数，此约束为允许的步长设定了上限。\n\n$\\Delta\\beta_k$ 的选择算法综合了这两个准则。首先，我们确定最大允许步长 $\\Delta\\beta_{\\rm max}$，即在 $[0, 1-\\beta_k]$ 中满足 ESS 约束的最大值。这可以通过使用二分法求解 ${\\rm ESS}(\\Delta\\beta) - \\eta J = 0$ 来找到。然后，我们在区间 $[0, \\Delta\\beta_{\\rm max}]$ 内搜索一个满足差异原则方程的 $\\Delta\\beta_k$。由于加权平均失配也是 $\\Delta\\beta_k$ 的一个单调函数，因此再次采用二分法。如果在此区间内不存在差异方程的解，回退规则规定选择可能的最大步长，即 $\\Delta\\beta_k = \\Delta\\beta_{\\rm max}$。\n\n集合成员根据与线性高斯 RML 一致的规则进行传播。在给定的退火水平 $\\beta$ 下，粒子 $i$ 的状态是其初始状态 $\\xi_i$（从先验中抽取）与全局最大后验 (MAP) 估计 $\\mathbf{x}_{\\rm MAP}$ 之间的线性插值：\n$$\\mathbf{x}_i(\\beta) = (1 - \\beta)\\xi_i + \\beta \\mathbf{x}_{\\rm MAP}$$\nMAP 估计 $\\mathbf{x}_{\\rm MAP}$ 是使后验密度最大化的点，等价于最小化目标函数 $J(\\mathbf{x}) = \\Phi(\\mathbf{x}) + \\frac{1}{2}\\|\\mathbf{C}^{-1/2}(\\mathbf{x}-\\mu)\\|^2_2$。对于给定的线性高斯设置，$\\mathbf{x}_{\\rm MAP}$ 具有一个从正规方程导出的闭式解：\n$$(\\mathbf{C}^{-1} + \\mathbf{H}^\\top \\Gamma^{-1} \\mathbf{H})\\mathbf{x}_{\\rm MAP} = \\mathbf{C}^{-1}\\mu + \\mathbf{H}^\\top \\Gamma^{-1}\\mathbf{y}$$\n这个线性系统在过程开始时求解一次。\n\n总体流程如下：\n1.  初始化参数并设置合成问题：生成随机正演矩阵 $\\mathbf{H}$，真实状态 $\\mathbf{x}_{\\rm true} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)$，以及合成数据 $\\mathbf{y} = \\mathbf{H}\\mathbf{x}_{\\rm true} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I}_m)$。\n2.  通过求解由正规方程定义的线性系统来计算 $\\mathbf{x}_{\\rm MAP}$。\n3.  通过从先验分布 $\\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)$ 中抽样来生成初始集合 $\\{\\xi_i\\}_{i=1}^J$。\n4.  初始化 $\\beta_0 = 0$ 并开始迭代退火过程。\n5.  在每个阶段 $k$（直至达到最大阶段数）：\n    a. 确定当前集合状态 $\\mathbf{x}_i^k = (1-\\beta_k)\\xi_i + \\beta_k\\mathbf{x}_{\\rm MAP}$。\n    b. 为每个粒子计算失配 $\\Phi_i^k = \\frac{1}{2\\sigma^2}\\|\\mathbf{H}\\mathbf{x}_i^k - \\mathbf{y}\\|^2_2$。\n    c. 应用自适应算法找到最优且稳定的增量 $\\Delta\\beta_k$。这涉及使用二分法来遵守 ESS 约束，然后求解以达到差异原则目标。\n    d. 更新累积退火因子：$\\beta_{k+1} = \\beta_k + \\Delta\\beta_k$。\n    e. 如果 $\\beta_{k+1} \\geq 1$，则将其设置为 $1$ 并终止。存储 $\\{\\beta_k\\}$ 值的序列。\n该算法为在反演问题中同化数据提供了一种稳健的方法，确保了数值稳定性和对统计原则的遵守。", "answer": "```python\nimport numpy as np\nimport scipy.optimize\n\ndef solve():\n    \"\"\"\n    Main function to run the adaptive tempering simulation for all test cases.\n    \"\"\"\n    test_cases = [\n        # (n, m, J, sigma, eta, max_stages, seed)\n        (10, 20, 50, 0.1, 0.6, 6, 1),\n        (10, 20, 50, 0.1, 0.9, 6, 1),\n        (10, 20, 50, 2.0, 0.6, 6, 2),\n    ]\n\n    results = []\n    for case in test_cases:\n        beta_sequence = solve_case(*case)\n        results.append(beta_sequence)\n\n    case_results_str = [str(res).replace(\" \", \"\") for res in results]\n    print(f\"[{','.join(case_results_str)}]\")\n\ndef solve_case(n, m, J, sigma, eta, max_stages, seed):\n    \"\"\"\n    Solves a single test case of the adaptive tempering problem.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Setup synthetic model and data\n    H = rng.standard_normal(size=(m, n))\n    x_true = rng.standard_normal(size=n)\n    noise = rng.normal(scale=sigma, size=m)\n    y = H @ x_true + noise\n\n    # Prior is N(0, I), so mu=0, C=I, C_inv=I\n    mu = np.zeros(n)\n    C_inv = np.eye(n)\n    \n    # Noise covariance Gamma = sigma^2 * I, Gamma_inv = (1/sigma^2) * I\n    gamma_inv_val = 1.0 / (sigma**2)\n\n    # 2. Compute x_MAP\n    # (C_inv + H^T Gamma_inv H) x_MAP = C_inv mu + H^T Gamma_inv y\n    # (I + (1/sigma^2) H^T H) x_MAP = (1/sigma^2) H^T y\n    A = C_inv + H.T @ H * gamma_inv_val\n    b = H.T @ y * gamma_inv_val\n    x_map = np.linalg.solve(A, b)\n\n    # 3. Initialize prior ensemble\n    xi_ensemble = rng.standard_normal(size=(J, n))\n\n    # 4. Iterative tempering\n    beta_k = 0.0\n    beta_sequence = []\n    \n    for _ in range(max_stages):\n        if beta_k >= 1.0:\n            break\n\n        # a. Compute current ensemble and misfits\n        x_k_ensemble = (1 - beta_k) * xi_ensemble + beta_k * x_map\n        residuals = x_k_ensemble @ H.T - y\n        misfits = 0.5 * gamma_inv_val * np.sum(residuals**2, axis=1)\n\n        # b. Find delta_beta\n        delta_beta = find_delta_beta(misfits, m, J, eta, beta_k)\n        \n        # c. Update beta\n        beta_k += delta_beta\n        if beta_k > 1.0:\n            beta_k = 1.0\n        \n        beta_sequence.append(beta_k)\n\n    return beta_sequence\n\ndef get_weights(delta_beta, misfits):\n    \"\"\"Numerically stable computation of importance weights.\"\"\"\n    if delta_beta == 0:\n        return np.full_like(misfits, 1.0 / len(misfits))\n    \n    log_weights_unnormalized = -delta_beta * misfits\n    # Shift for stability to avoid overflow/underflow in exp\n    log_weights_unnormalized -= np.max(log_weights_unnormalized)\n    weights_unnormalized = np.exp(log_weights_unnormalized)\n    sum_weights = np.sum(weights_unnormalized)\n    return weights_unnormalized / sum_weights\n\ndef calculate_ess_and_weighted_misfit(delta_beta, misfits):\n    \"\"\"Computes ESS and weighted misfit for a given delta_beta.\"\"\"\n    weights = get_weights(delta_beta, misfits)\n    ess = 1.0 / np.sum(weights**2)\n    weighted_misfit = np.sum(weights * misfits)\n    return ess, weighted_misfit\n\ndef find_delta_beta(misfits, m, J, eta, beta_k):\n    \"\"\"\n    Finds the adaptive tempering increment delta_beta.\n    \"\"\"\n    upper_bound_beta = 1.0 - beta_k\n    if upper_bound_beta = 1e-9:\n        return 0.0\n    \n    # Define objective functions for bisection\n    def ess_func(d_beta, misfits_arr, J_val, eta_val):\n        ess, _ = calculate_ess_and_weighted_misfit(d_beta, misfits_arr)\n        return ess - eta_val * J_val\n\n    def discrepancy_func(d_beta, misfits_arr, m_val):\n        _, w_misfit = calculate_ess_and_weighted_misfit(d_beta, misfits_arr)\n        return w_misfit - m_val / 2.0\n\n    # 1. Find the maximum delta_beta allowed by the ESS constraint\n    delta_beta_max_ess = upper_bound_beta\n    # ESS is decreasing in delta_beta. ess_func(0) is (1-eta)J > 0.\n    if ess_func(upper_bound_beta, misfits, J, eta)  0:\n        try:\n            delta_beta_max_ess = scipy.optimize.bisect(\n                ess_func, 0, upper_bound_beta, args=(misfits, J, eta)\n            )\n        except ValueError:\n            # Should not happen as we bracketed the root\n            pass\n    \n    # 2. Find delta_beta that satisfies the discrepancy principle within the ESS-allowed range\n    val_at_0 = discrepancy_func(0, misfits, m)\n    val_at_max_ess = discrepancy_func(delta_beta_max_ess, misfits, m)\n\n    # Discrepancy function is monotonically decreasing.\n    # If signs are different, a root exists in the interval.\n    if val_at_0 * val_at_max_ess  0:\n        try:\n            delta_beta = scipy.optimize.bisect(\n                discrepancy_func, 0, delta_beta_max_ess, args=(misfits, m)\n            )\n            return delta_beta\n        except ValueError:\n            # Should not happen if signs are different\n            pass\n\n    # If no root is bracketed, apply fallback rule:\n    # use the largest delta_beta that satisfies the ESS constraint.\n    return delta_beta_max_ess\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3389130"}]}