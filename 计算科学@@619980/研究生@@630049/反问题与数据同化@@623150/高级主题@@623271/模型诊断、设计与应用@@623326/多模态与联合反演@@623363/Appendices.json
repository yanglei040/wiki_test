{"hands_on_practices": [{"introduction": "在我们开始组合不同数据类型之前，必须首先理解其背后的概念性目标。联合反演本质上是一个多目标优化问题，本练习将探讨帕累托前沿（Pareto front）的概念，它代表了在相互冲突的目标之间的所有最优权衡。理解这一点有助于我们解释通过常用的加权和方法所得到解的意义。[@problem_id:3404773]", "problem": "考虑一个联合反演设定，其中模型向量 $m \\in \\mathcal{M}$ 必须解释两种模态的数据，产生两个凸的失配目标函数 $\\phi_1(m)$ 和 $\\phi_2(m)$，其中 $\\mathcal{M}$ 是一个非空凸可行集。假设每种模态的正演算子是线性的，数据保真项是二次的，因此每个 $\\phi_i(m)$ 在 $\\mathcal{M}$ 上是凸的。定义该双目标问题的帕累托集，并分析此帕累托集与以下形式的标量化优化问题之间的关系\n$$\n\\min_{m \\in \\mathcal{M}} \\ \\alpha \\, \\phi_1(m) + (1-\\alpha) \\, \\phi_2(m), \\quad \\text{with } \\alpha \\in (0,1).\n$$\n你的回答应基于凸分析和多目标最优化的基本原理。\n\n在给定的凸性假设下，选择最准确的陈述：\n\nA. 帕累托集是同时最小化 $\\phi_1(m)$ 和 $\\phi_2(m)$ 的 $m \\in \\mathcal{M}$ 的集合，并且 $\\alpha \\in (0,1)$ 的任何选择都能恢复所有帕累托点（每个帕累托点都是某个 $\\alpha$ 的标量化问题的解），即使可达目标集 $\\{(\\phi_1(m),\\phi_2(m)) : m \\in \\mathcal{M}\\}$ 是非凸的。\n\nB. 帕累托集是非支配的 $m \\in \\mathcal{M}$ 的集合，使得不存在 $m' \\in \\mathcal{M}$ 满足对 $i=1,2$ 都有 $\\phi_i(m') \\le \\phi_i(m)$ 且至少在一个目标上是严格不等式；对于任何 $\\alpha \\in (0,1)$，$\\alpha \\phi_1 + (1-\\alpha)\\phi_2$ 的每个最小化子都是帕累托最优的，并且改变 $\\alpha$ 可以恢复所有受支持的（弱）帕累托点。如果可达目标集 $\\{(\\phi_1(m),\\phi_2(m)) : m \\in \\mathcal{M}\\}$ 是凸的，那么每个帕累托最优点都是受支持的，并且可以通过某个 $\\alpha \\in (0,1)$ 恢复。\n\nC. 帕累托集由所有使 $\\phi_1(m) + \\phi_2(m)$ 最小化的 $m \\in \\mathcal{M}$ 组成，并且即使当 $\\phi_1$ 和 $\\phi_2$ 在凸集 $\\mathcal{M}$ 上是凸的，使用 $\\alpha \\in (0,1)$ 的标量化也可能返回非帕累托点。\n\nD. 帕累托集等于交集 $\\bigcap_{\\alpha \\in (0,1)} \\arg\\min_{m \\in \\mathcal{M}} \\alpha \\phi_1(m) + (1-\\alpha) \\phi_2(m)$，因此标量化问题等价于多目标最优化，没有损失，无论可达目标集的几何形状如何。", "solution": "在尝试解答之前，将对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n\n-   模型向量：$m \\in \\mathcal{M}$\n-   可行集：$\\mathcal{M}$ 是一个非空凸集。\n-   失配目标函数：$\\phi_1(m)$ 和 $\\phi_2(m)$，代表两种数据模态。\n-   函数性质：$\\phi_1(m)$ 和 $\\phi_2(m)$ 是在 $\\mathcal{M}$ 上的凸函数。这被陈述为是线性正演算子和二次数据保真项的结果。\n-   标量化优化问题：$\\min_{m \\in \\mathcal{M}} \\ \\alpha \\, \\phi_1(m) + (1-\\alpha) \\, \\phi_2(m)$，其中 $\\alpha \\in (0,1)$。\n-   任务：分析双目标问题的帕累托集与标量化问题解之间的关系，并在选项中选择最准确的陈述。\n\n### 步骤2：使用提取的已知条件进行验证\n\n-   **科学基础（关键）**：该问题牢固地植根于多目标优化和凸分析的数学理论，这些是反演问题和数据同化领域的标准工具。帕累托最优性、凸函数和标量化（加权和方法）等概念定义明确且基础。该设定在科学上是合理的。\n-   **良态的**：该问题要求在特定的数学条件下，对不同类型解集之间的关系进行理论分析。所提供的信息（函数的凸性和可行集）足以基于已建立的定理进行此分析。该问题是良态的。\n-   **客观的（关键）**：问题使用精确、形式化的数学语言陈述。诸如“凸集”、“凸函数”和“帕累托集”之类的术语具有明确的定义。问题是客观的。\n-   **不完整或矛盾的设置**：该设定是自洽的，并为推理该问题提供了必要的假设（凸性）。没有矛盾之处。\n-   **不切实际或不可行**：所描述的设定是多模态联合反演中的一个标准且广泛使用的模型，其中人们寻求用单一模型来解释来自不同物理测量（例如，地球物理学中的地震和重力数据）的数据。在此理论背景下，这些假设是现实的。\n-   **未检测到其他缺陷**：该问题不是微不足道的、病态的或隐喻性的。它解决了多目标优化中的一个核心概念挑战。\n\n### 步骤3：结论和行动\n\n问题陈述是**有效的**。将基于基本原理推导解决方案。\n\n### 推导\n\n问题的核心在于多目标优化问题的解与相关的单目标、标量化问题的解之间的关系。\n\n**1. 定义**\n\n-   **多目标问题**：问题是在概念上同时“最小化” $m \\in \\mathcal{M}$ 的 $\\phi_1(m)$ 和 $\\phi_2(m)$。我们可以将其写为：\n    $$ \\min_{m \\in \\mathcal{M}} \\ (\\phi_1(m), \\phi_2(m)) $$\n-   **帕累托支配**：如果对于 $i=1, 2$ 有 $\\phi_i(m') \\leq \\phi_i(m)$，并且存在至少一个索引 $j \\in \\{1, 2\\}$ 使得 $\\phi_j(m')  \\phi_j(m)$，则模型 $m' \\in \\mathcal{M}$ *支配*模型 $m \\in \\mathcal{M}$。\n-   **帕累托最优性**：如果没有其他模型 $m' \\in \\mathcal{M}$ 支配它，则模型 $m^* \\in \\mathcal{M}$ 是**帕累托最优的**（或非支配的、或有效的）。所有这些点的集合就是**帕累托集**。\n-   **标量化问题（加权和方法）**：对于给定的权重 $\\alpha \\in (0,1)$，标量化问题是：\n    $$ P(\\alpha): \\min_{m \\in \\mathcal{M}} \\Phi_\\alpha(m) \\text{ where } \\Phi_\\alpha(m) = \\alpha \\, \\phi_1(m) + (1-\\alpha) \\, \\phi_2(m) $$\n    目标函数 $\\Phi_\\alpha(m)$ 是两个凸函数（$\\phi_1, \\phi_2$）的正加权和，因此 $\\Phi_\\alpha(m)$ 也是凸的。最小化是在一个凸集 $\\mathcal{M}$ 上进行的。这是一个标准的凸优化问题。\n\n**2. 标量化与帕累托最优性之间的关系**\n\n-   **第1部分：对于 $\\alpha \\in (0,1)$，$P(\\alpha)$ 的任何解都是帕累托最优的。**\n    设 $m^*$ 是某个 $\\alpha \\in (0,1)$ 的 $\\Phi_\\alpha(m)$ 的一个最小化子。为寻求矛盾，假设 $m^*$ 不是帕累托最优的。那么存在一个 $m' \\in \\mathcal{M}$ 支配 $m^*$。根据支配的定义，这意味着：\n    1.  $\\phi_1(m') \\leq \\phi_1(m^*)$\n    2.  $\\phi_2(m') \\leq \\phi_2(m^*)$\n    3.  这些不等式中至少有一个是严格的。\n    \n    由于 $\\alpha > 0$ 且 $1-\\alpha > 0$，我们可以用权重乘以这些不等式：\n    1.  $\\alpha \\phi_1(m') \\leq \\alpha \\phi_1(m^*)$\n    2.  $(1-\\alpha) \\phi_2(m') \\leq (1-\\alpha) \\phi_2(m^*)$\n\n    将它们相加得到：\n    $$ \\alpha \\phi_1(m') + (1-\\alpha) \\phi_2(m') \\leq \\alpha \\phi_1(m^*) + (1-\\alpha) \\phi_2(m^*) $$\n    $$ \\implies \\Phi_\\alpha(m') \\leq \\Phi_\\alpha(m^*) $$\n    此外，由于初始不等式中有一个是严格的（例如，$\\phi_1(m')  \\phi_1(m^*)$），并且其对应的权重（$\\alpha$）是严格为正的，因此最终的不等式也必须是严格的：\n    $$ \\Phi_\\alpha(m')  \\Phi_\\alpha(m^*) $$\n    这与 $m^*$ 是 $\\Phi_\\alpha(m)$ 的最小化子的假设相矛盾。因此，初始假设必定是错误的， $m^*$ 必须是帕累托最优的。这对于权重在 $(0,1)$ 内的标量化问题的任何最小化子都成立。\n\n-   **第2部分：每个帕累托最优点都是某个 $\\alpha$ 的 $P(\\alpha)$ 的解吗？**\n    一般情况下这不能保证。答案取决于**可达目标集**的几何形状，其定义为 $Y = \\{(\\phi_1(m), \\phi_2(m)) \\mid m \\in \\mathcal{M}\\}$。对于所有 $\\alpha \\in [0,1]$，标量化问题的解对应于 $Y$ 的凸包边界上的点。这些点被称为**受支持的帕累托最优点**。\n    即使模型空间 $\\mathcal{M}$ 是凸的，目标函数 $\\phi_i$ 也是凸的，映射 $m \\mapsto (\\phi_1(m), \\phi_2(m))$ 是一个向量值函数，一个凸集在此映射下的像**通常不是**凸的。\n    如果 $Y$ 不是凸的，就可能存在不受支持的帕累托最优点。这些“不受支持的”点位于帕累托前沿的非凸“凹陷”处，无法通过加权和方法找到。\n    然而，有一个关键定理：**如果可达目标集 $Y$ 是凸的，那么每个帕累托最优点都是一个受支持的点。**因此，对于任何帕累托最优的 $m^*$，都存在一个 $\\alpha \\in [0,1]$ 使得 $m^*$ 是 $P(\\alpha)$ 的一个解。\n\n### 逐项分析\n\n-   **A. 帕累托集是同时最小化 $\\phi_1(m)$ 和 $\\phi_2(m)$ 的 $m \\in \\mathcal{M}$ 的集合，并且 $\\alpha \\in (0,1)$ 的任何选择都能恢复所有帕累托点（每个帕累托点都是某个 $\\alpha$ 的标量化问题的解），即使可达目标集 $\\{(\\phi_1(m),\\phi_2(m)) : m \\in \\mathcal{M}\\}$ 是非凸的。**\n    该陈述的第一部分给出了一个不正确的帕累托集定义。同时最小化两个目标的点集在目标空间中被称为“乌托邦点”，而实现这一点的模型由于权衡的存在在实践中通常是不存在的。帕累托集是所有非支配的权衡解的整个集合。第二部分也是不正确的。如果可达目标集是非凸的，加权和方法（标量化）不保证能恢复所有帕累托点；它只能恢复受支持的点。\n    **结论：不正确。**\n\n-   **B. 帕累托集是非支配的 $m \\in \\mathcal{M}$ 的集合，使得不存在 $m' \\in \\mathcal{M}$ 满足对 $i=1,2$ 都有 $\\phi_i(m') \\le \\phi_i(m)$ 且至少在一个目标上是严格不等式；对于任何 $\\alpha \\in (0,1)$，$\\alpha \\phi_1 + (1-\\alpha)\\phi_2$ 的每个最小化子都是帕累托最优的，并且改变 $\\alpha$ 可以恢复所有受支持的（弱）帕累托点。如果可达目标集 $\\{(\\phi_1(m),\\phi_2(m)) : m \\in \\mathcal{M}\\}$ 是凸的，那么每个帕累托最优点都是受支持的，并且可以通过某个 $\\alpha \\in (0,1)$ 恢复。**\n    这个陈述由几个正确的主张组成。\n    1. 它通过非支配给出了帕累托集的正确定义。\n    2. 它正确地指出，对于 $\\alpha \\in (0,1)$，标量化问题的任何最小化子都是帕累托最优的（如上所证）。\n    3. 它正确地将通过改变 $\\alpha$ 找到的点集描述为“受支持的”帕累托点。\n    4. 它正确地陈述了基本结果，即如果可达目标集是凸的，受支持点和不受支持点之间的区别就消失了，并且（几乎所有）帕累托点都可以通过标量化找到。使用 $\\alpha \\in (0,1)$ 在技术上排除了可能唯一地最小化某个目标并需要 $\\alpha=0$ 或 $\\alpha=1$ 的极端端点，但这是一个次要细节，并且这个陈述是迄今为止最准确和最全面的。\n    **结论：正确。**\n\n-   **C. 帕累托集由所有使 $\\phi_1(m) + \\phi_2(m)$ 最小化的 $m \\in \\mathcal{M}$ 组成，并且即使当 $\\phi_1$ 和 $\\phi_2$ 在凸集 $\\mathcal{M}$ 上是凸的，使用 $\\alpha \\in (0,1)$ 的标量化也可能返回非帕累托点。**\n    第一部分是不正确的。最小化和 $\\phi_1(m) + \\phi_2(m)$ 对应于帕累托前沿上的单个点（或点集）（具体来说，对应于 $\\alpha=1/2$）。这并不是整个帕累托集。第二部分也是不正确的。如推导所示，在指定条件下（$\\phi_i$ 是凸的，`\\mathcal{M}` 是凸的）且 $\\alpha \\in (0,1)$，一个最小化子总是帕累托最优的。\n    **结论：不正确。**\n\n-   **D. 帕累托集等于交集 $\\bigcap_{\\alpha \\in (0,1)} \\arg\\min_{m \\in \\mathcal{M}} \\alpha \\phi_1(m) + (1-\\alpha) \\phi_2(m)$，因此标量化问题等价于多目标最优化，没有损失，无论可达目标集的几何形状如何。**\n    这个陈述有根本性的缺陷。帕累托集与不同 $\\alpha$ 的解集的*并集*有关，而不是*交集*。交集将只包含对所有权重同时都是最优的模型，这是一个极其严格的条件，通常会得到一个空集，或者如果存在的话，一个单一的乌托邦点。此外，“无论可达目标集的几何形状如何”等价性都成立的说法是错误的。几何形状，特别是这个集合的凸性，是决定标量化是否能恢复整个帕累托集的关键因素。\n    **结论：不正确。**", "answer": "$$\\boxed{B}$$", "id": "3404773"}, {"introduction": "在确定了平衡多个目标的需求之后，一个关键问题是如何选择权重。本练习将演示最常用且具有统计学依据的方法：通过数据误差的逆协方差来加权每个数据失配项。通过这个动手计算，你将理解不确定性更高（即方差更大）的数据在联合目标函数中是如何被自然地降低权重的。[@problem_id:3404765]", "problem": "考虑一个联合反演问题，其具有两种由 $i \\in \\{1,2\\}$ 索引的数据模态。正演算子为 $F_1(m)$ 和 $F_2(m)$，与线性观测算子 $H_1$ 和 $H_2$ 复合，得到残差 $r_i(m) = H_i F_i(m) - d_i$。假设每种模态都存在加性的、零均值的高斯数据误差，其协方差矩阵分别为 $C_1$ 和 $C_2$，且已知为正定矩阵。在此假设下，联合目标函数的数据失配部分是使用高斯误差模型所隐含的适当的协方差加权来构建的。\n\n在一个特定的候选模型 $m^\\star$ 处，假设协方差为 $C_1 = 9\\,I$ 和 $C_2 = I$，其中 $I$ 表示维度兼容的单位矩阵，且欧几里得残差范数为 $\\|r_1(m^\\star)\\|_2 = 3$ 和 $\\|r_2(m^\\star)\\|_2 = 1$。使用基于高斯噪声的数据失配目标函数构建方法，并在所有模态间使用一个共同的标量前置因子，计算在 $m^\\star$ 处每种模态对总数据失配的相对贡献，即分数 $J_1(m^\\star)/J_{\\mathrm{data}}(m^\\star)$ 和 $J_2(m^\\star)/J_{\\mathrm{data}}(m^\\star)$，其中 $J_{\\mathrm{data}}(m^\\star) = J_1(m^\\star) + J_2(m^\\star)$ 表示总数据失配。\n\n将你的最终答案以一个包含两个相对贡献（按顺序）的双元素行向量的形式报告。不需要单位。不需要四舍五入。", "solution": "该问题要求计算在特定候选模型 $m^\\star$ 处，两种数据模态对总数据失配目标函数的相对贡献。目标函数的构建基于存在加性的、零均值的高斯误差且协方差矩阵已知的假设。\n\n对于模态 $i$，给定模型 $m$ 时的数据 $d_i$ 的概率密度函数，是基于误差项的高斯分布，该误差项被等同于残差 $r_i(m) = H_i F_i(m) - d_i$。模态 $i$ 的似然函数由下式给出：\n$$L_i(m) \\propto \\exp\\left(-\\frac{1}{2} r_i(m)^T C_i^{-1} r_i(m)\\right)$$\n其中 $C_i$ 是数据误差协方差矩阵。\n\n对于一个具有两个统计独立的数据模态的联合反演问题，总似然是各个似然的乘积：\n$$L_{\\mathrm{total}}(m) = L_1(m) \\cdot L_2(m) \\propto \\exp\\left(-\\frac{1}{2} r_1(m)^T C_1^{-1} r_1(m) - \\frac{1}{2} r_2(m)^T C_2^{-1} r_2(m)\\right)$$\n\n数据失配目标函数 $J_{\\mathrm{data}}(m)$ 通常通过取负对数似然得到，并忽略任何不依赖于模型 $m$ 的常数项。这产生了一个二次型之和：\n$$J_{\\mathrm{data}}(m) = \\frac{1}{2} r_1(m)^T C_1^{-1} r_1(m) + \\frac{1}{2} r_2(m)^T C_2^{-1} r_2(m)$$\n问题陈述中提到“一个共同的标量前置因子”，这与两项中都出现的因子 $\\frac{1}{2}$ 相一致。我们可以将单个的失配贡献定义为：\n$$J_1(m) = \\frac{1}{2} r_1(m)^T C_1^{-1} r_1(m)$$\n$$J_2(m) = \\frac{1}{2} r_2(m)^T C_2^{-1} r_2(m)$$\n使得 $J_{\\mathrm{data}}(m) = J_1(m) + J_2(m)$。\n\n我们被要求在特定模型 $m^\\star$ 处评估这些贡献。用于此评估的已知条件是：\n- 模态 1 的协方差：$C_1 = 9\\,I$\n- 模态 2 的协方差：$C_2 = I$\n- 模态 1 的残差范数：$\\|r_1(m^\\star)\\|_2 = 3$\n- 模态 2 的残差范数：$\\|r_2(m^\\star)\\|_2 = 1$\n\n首先，我们求逆协方差矩阵：\n$$C_1^{-1} = (9\\,I)^{-1} = \\frac{1}{9}I^{-1} = \\frac{1}{9}I$$\n$$C_2^{-1} = I^{-1} = I$$\n\n现在，我们计算 $J_1(m^\\star)$ 的值。我们将 $C_1^{-1}$ 代入 $J_1(m)$ 的表达式中：\n$$J_1(m^\\star) = \\frac{1}{2} r_1(m^\\star)^T \\left(\\frac{1}{9}I\\right) r_1(m^\\star)$$\n项 $r_1(m^\\star)^T I r_1(m^\\star)$ 等价于点积 $r_1(m^\\star) \\cdot r_1(m^\\star)$，也就是欧几里得范数的平方，$\\|r_1(m^\\star)\\|_2^2$。\n$$J_1(m^\\star) = \\frac{1}{2} \\cdot \\frac{1}{9} \\|r_1(m^\\star)\\|_2^2 = \\frac{1}{18} \\|r_1(m^\\star)\\|_2^2$$\n代入给定值 $\\|r_1(m^\\star)\\|_2 = 3$：\n$$J_1(m^\\star) = \\frac{1}{18} (3^2) = \\frac{9}{18} = \\frac{1}{2}$$\n\n接下来，我们计算 $J_2(m^\\star)$ 的值。我们将 $C_2^{-1}$ 代入 $J_2(m)$ 的表达式中：\n$$J_2(m^\\star) = \\frac{1}{2} r_2(m^\\star)^T (I) r_2(m^\\star)$$\n这可以简化为：\n$$J_2(m^\\star) = \\frac{1}{2} \\|r_2(m^\\star)\\|_2^2$$\n代入给定值 $\\|r_2(m^\\star)\\|_2 = 1$：\n$$J_2(m^\\star) = \\frac{1}{2} (1^2) = \\frac{1}{2}$$\n\n在 $m^\\star$ 处的总数据失配是各个贡献之和：\n$$J_{\\mathrm{data}}(m^\\star) = J_1(m^\\star) + J_2(m^\\star) = \\frac{1}{2} + \\frac{1}{2} = 1$$\n\n最后，我们计算每种模态对总数据失配的相对贡献。\n模态 1 的相对贡献是：\n$$\\frac{J_1(m^\\star)}{J_{\\mathrm{data}}(m^\\star)} = \\frac{1/2}{1} = \\frac{1}{2}$$\n模态 2 的相对贡献是：\n$$\\frac{J_2(m^\\star)}{J_{\\mathrm{data}}(m^\\star)} = \\frac{1/2}{1} = \\frac{1}{2}$$\n\n问题要求答案是一个包含这两个分数（按顺序）的双元素行向量。此结果展示了逆协方差加权原理：模态1的原始残差更大，但由于其相关的数据误差具有更大的方差（9相较于1），其对目标函数的贡献被相应地降低权重，从而导致在 $m^\\star$ 点两种模态的贡献相等。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}}$$", "id": "3404765"}, {"introduction": "本练习在一个数据同化中常见的线性高斯框架下，将所有概念融合成一个实际的更新步骤。你将推导并应用贝叶斯更新公式，将关于模型的先验信念与来自两种不同数据源的信息相结合。这将演示一个联合的“卡尔曼增益”如何最优地融合所有可用信息，从而生成一个改进的模型状态后验估计。[@problem_id:3404715]", "problem": "在一个联合反演框架中，考虑两种互补的传感模式观测一个共同的潜在地球物理状态，假设一个线性高斯模型。设潜在状态为 $x \\in \\mathbb{R}^{2}$，其高斯先验为 $p(x) = \\mathcal{N}(x_{0}, P_{0})$，其中给定了 $x_{0} \\in \\mathbb{R}^{2}$ 和 $P_{0} \\in \\mathbb{R}^{2 \\times 2}$。两种独立的传感模式产生观测值 $y_{1} \\in \\mathbb{R}$ 和 $y_{2} \\in \\mathbb{R}$，其线性模型为 $y_{1} = H_{1} x + v_{1}$ 和 $y_{2} = H_{2} x + v_{2}$，其中 $v_{1} \\sim \\mathcal{N}(0, R_{1})$，$v_{2} \\sim \\mathcal{N}(0, R_{2})$，并且在给定 $x$ 的条件下 $v_{1}$ 和 $v_{2}$ 是独立的。定义堆叠观测模型 $y = \\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix} = H x + v$，其中 $H = \\begin{pmatrix} H_{1} \\\\ H_{2} \\end{pmatrix}$，$v = \\begin{pmatrix} v_{1} \\\\ v_{2} \\end{pmatrix}$，且噪声协方差为块对角矩阵 $R = \\begin{pmatrix} R_{1}  0 \\\\ 0  R_{2} \\end{pmatrix}$。从 Bayes 法则和多元高斯分布的性质出发，从第一性原理推导后验均值 $\\hat{x}$ 的闭式表达式，以及相应的最优线性更新增益（在线性高斯设置中常称为卡尔曼增益），该增益以堆叠形式融合了两种模式。\n\n然后，对以下给定的数值实例计算您的表达式：\n- 先验均值 $x_{0} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$ 和先验协方差 $P_{0} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$。\n- 观测算子 $H_{1} = \\begin{pmatrix} 1  2 \\end{pmatrix}$ 和 $H_{2} = \\begin{pmatrix} 2  -1 \\end{pmatrix}$，因此 $H = \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix}$。\n- 噪声协方差 $R_{1} = 1$ 和 $R_{2} = 4$，因此 $R = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}$。\n- 观测值 $y_{1} = 5$ 和 $y_{2} = -1$，因此 $y = \\begin{pmatrix} 5 \\\\ -1 \\end{pmatrix}$。\n\n以有理数形式提供精确的联合增益矩阵和更新后的后验均值。不要四舍五入。最终答案必须以包含增益矩阵和更新后均值的单个表达式给出，无单位，并且精确表示（无近似或四舍五入）。", "solution": "该问题要求推导线性高斯系统的后验均值和最优线性更新增益，然后进行数值计算。推导将从 Bayes 法则开始。\n\n### 第一部分：从第一性原理推导\n\n根据 Bayes 法则，后验概率密度函数 (PDF) $p(x|y)$ 正比于似然 $p(y|x)$ 和先验 $p(x)$ 的乘积：\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\n问题陈述了潜在状态 $x \\in \\mathbb{R}^2$ 的高斯先验：\n$$\np(x) = \\mathcal{N}(x_0, P_0) \\propto \\exp\\left(-\\frac{1}{2}(x - x_0)^T P_0^{-1} (x - x_0)\\right)\n$$\n观测模型是带有加性高斯噪声的线性模型，$y = Hx + v$，其中 $v \\sim \\mathcal{N}(0, R)$。这将似然函数 $p(y|x)$ 定义为 $y$ 的一个高斯分布，其均值为 $Hx$，协方差为 $R$：\n$$\np(y|x) = \\mathcal{N}(Hx, R) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)\n$$\n结合这些，后验概率密度函数为：\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2}(x - x_0)^T P_0^{-1} (x - x_0)\\right) \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)\n$$\n$$\np(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (x - x_0)^T P_0^{-1} (x - x_0) + (y - Hx)^T R^{-1} (y - Hx) \\right]\\right)\n$$\n指数中的项，我们将其表示为代价函数 $J(x)$，是 $x$ 的一个二次函数。这意味着后验分布也是高斯分布，记为 $p(x|y) = \\mathcal{N}(\\hat{x}, \\hat{P})$，其概率密度函数正比于 $\\exp\\left(-\\frac{1}{2}(x-\\hat{x})^T \\hat{P}^{-1} (x-\\hat{x})\\right)$。为了找到后验均值 $\\hat{x}$ 和协方差 $\\hat{P}$，我们展开 $J(x)$ 并按 $x$ 的幂次收集各项。\n\n$J(x) = (x^T P_0^{-1} x - 2x^T P_0^{-1} x_0 + x_0^T P_0^{-1} x_0) + ((Hx)^T R^{-1} (Hx) - 2(Hx)^T R^{-1} y + y^T R^{-1} y)$\n$J(x) = x^T P_0^{-1} x - 2x^T P_0^{-1} x_0 + x^T H^T R^{-1} H x - 2x^T H^T R^{-1} y + (\\text{不依赖于 } x \\text{ 的项})$\n分组各项：\n$J(x) = x^T (P_0^{-1} + H^T R^{-1} H) x - 2x^T (P_0^{-1} x_0 + H^T R^{-1} y) + \\text{const.}$\n\n高斯分布 $\\mathcal{N}(\\hat{x}, \\hat{P})$ 指数的标准形式是 $(x - \\hat{x})^T \\hat{P}^{-1} (x - \\hat{x}) = x^T \\hat{P}^{-1} x - 2x^T \\hat{P}^{-1} \\hat{x} + \\hat{x}^T \\hat{P}^{-1} \\hat{x}$。通过将 $J(x)$ 与 $x$ 的二次项和线性项进行比较，我们可以确定后验逆协方差 $\\hat{P}^{-1}$ 和后验均值 $\\hat{x}$：\n\n1.  从二次项 ($x^T(\\dots)x$)：\n    $$\n    \\hat{P}^{-1} = P_0^{-1} + H^T R^{-1} H\n    $$\n    所以后验协方差为 $\\hat{P} = (P_0^{-1} + H^T R^{-1} H)^{-1}$。\n\n2.  从线性项 ($-2x^T(\\dots)$)：\n    $$\n    \\hat{P}^{-1} \\hat{x} = P_0^{-1} x_0 + H^T R^{-1} y\n    $$\n    求解后验均值 $\\hat{x}$：\n    $$\n    \\hat{x} = (\\hat{P}^{-1})^{-1} (P_0^{-1} x_0 + H^T R^{-1} y) = \\hat{P} (P_0^{-1} x_0 + H^T R^{-1} y)\n    $$\n为了推导包含增益矩阵的更新形式，我们使用矩阵求逆引理（特别是 Woodbury 恒等式）将 $\\hat{P}$ 表示为不同的形式：\n$\\hat{P} = P_0 - P_0 H^T (H P_0 H^T + R)^{-1} H P_0$。\n我们将最优线性更新增益矩阵 $K$（也称为卡尔曼增益）定义为：\n$$\nK = P_0 H^T (H P_0 H^T + R)^{-1}\n$$\n根据这个定义，后验协方差变为 $\\hat{P} = P_0 - K H P_0 = (I - KH)P_0$。\n\n现在我们重新整理 $\\hat{x}$ 的表达式，以匹配更新形式 $\\hat{x} = x_0 + K(y - Hx_0)$。\n从 $I = \\hat{P} \\hat{P}^{-1} = \\hat{P} (P_0^{-1} + H^T R^{-1} H) = \\hat{P} P_0^{-1} + \\hat{P} H^T R^{-1} H$ 开始，我们可以写出 $\\hat{P}P_0^{-1} = I - \\hat{P} H^T R^{-1} H$。\n增益 $K$ 的另一个表达式是 $K = \\hat{P} H^T R^{-1}$。\n代入这个表达式得到 $\\hat{P}P_0^{-1} = I - K H$。\n现在，我们重写后验均值表达式：\n$\\hat{x} = \\hat{P} (P_0^{-1} x_0 + H^T R^{-1} y) = (\\hat{P} P_0^{-1}) x_0 + (\\hat{P} H^T R^{-1}) y$\n代入我们刚找到的恒等式：\n$\\hat{x} = (I - K H) x_0 + K y = x_0 - K H x_0 + K y$\n$$\n\\hat{x} = x_0 + K(y - Hx_0)\n$$\n这就是所求的后验均值的闭式表达式。残差项 $(y - Hx_0)$ 称为新息 (innovation)。增益 $K$ 将这个新信息与先验估计 $x_0$ 进行了最优融合。\n\n### 第二部分：数值计算\n\n我们被给予以下数值：\n- 先验均值：$x_{0} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$\n- 先验协方差：$P_{0} = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$\n- 观测算子：$H = \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix}$\n- 噪声协方差：$R = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}$\n- 观测向量：$y = \\begin{pmatrix} 5 \\\\ -1 \\end{pmatrix}$\n\n我们将计算增益 $K = P_0 H^T (H P_0 H^T + R)^{-1}$ 和后验均值 $\\hat{x} = x_0 + K(y - Hx_0)$。\n\n1.  **计算新息协方差 $S = H P_0 H^T + R$：**\n    首先，计算 $P_0 H^T$：\n    $$\n    P_0 H^T = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix}^T = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 1 + 1 \\cdot 2  2 \\cdot 2 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + 3 \\cdot 2  1 \\cdot 2 + 3 \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 4  3 \\\\ 7  -1 \\end{pmatrix}\n    $$\n    接下来，计算 $H P_0 H^T$：\n    $$\n    H (P_0 H^T) = \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix} \\begin{pmatrix} 4  3 \\\\ 7  -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 4 + 2 \\cdot 7  1 \\cdot 3 + 2 \\cdot (-1) \\\\ 2 \\cdot 4 + (-1) \\cdot 7  2 \\cdot 3 + (-1) \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 18  1 \\\\ 1  7 \\end{pmatrix}\n    $$\n    现在，加上 $R$：\n    $$\n    S = H P_0 H^T + R = \\begin{pmatrix} 18  1 \\\\ 1  7 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix} = \\begin{pmatrix} 19  1 \\\\ 1  11 \\end{pmatrix}\n    $$\n\n2.  **计算 $S^{-1}$：**\n    $S$ 的行列式是 $\\det(S) = (19)(11) - (1)(1) = 209 - 1 = 208$。\n    其逆矩阵为：\n    $$\n    S^{-1} = \\frac{1}{208} \\begin{pmatrix} 11  -1 \\\\ -1  19 \\end{pmatrix}\n    $$\n\n3.  **计算增益矩阵 $K = P_0 H^T S^{-1}$：**\n    $$\n    K = \\begin{pmatrix} 4  3 \\\\ 7  -1 \\end{pmatrix} \\frac{1}{208} \\begin{pmatrix} 11  -1 \\\\ -1  19 \\end{pmatrix} = \\frac{1}{208} \\begin{pmatrix} 4 \\cdot 11 + 3 \\cdot (-1)  4 \\cdot (-1) + 3 \\cdot 19 \\\\ 7 \\cdot 11 + (-1) \\cdot (-1)  7 \\cdot (-1) + (-1) \\cdot 19 \\end{pmatrix}\n    $$\n    $$\n    K = \\frac{1}{208} \\begin{pmatrix} 44 - 3  -4 + 57 \\\\ 77 + 1  -7 - 19 \\end{pmatrix} = \\frac{1}{208} \\begin{pmatrix} 41  53 \\\\ 78  -26 \\end{pmatrix}\n    $$\n    简化分数：\n    $$\n    K = \\begin{pmatrix} \\frac{41}{208}  \\frac{53}{208} \\\\ \\frac{78}{208}  \\frac{-26}{208} \\end{pmatrix} = \\begin{pmatrix} \\frac{41}{208}  \\frac{53}{208} \\\\ \\frac{3}{8}  -\\frac{1}{8} \\end{pmatrix}\n    $$\n\n4.  **计算后验均值 $\\hat{x} = x_0 + K(y - Hx_0)$：**\n    首先，计算新息 $(y - Hx_0)$：\n    $$\n    H x_0 = \\begin{pmatrix} 1  2 \\\\ 2  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 2 \\cdot (-2) \\\\ 2 \\cdot 1 + (-1) \\cdot (-2) \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ 4 \\end{pmatrix}\n    $$\n    $$\n    y - Hx_0 = \\begin{pmatrix} 5 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} -3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix}\n    $$\n    接下来，计算更新项 $K(y - Hx_0)$：\n    $$\n    K(y - Hx_0) = \\begin{pmatrix} \\frac{41}{208}  \\frac{53}{208} \\\\ \\frac{3}{8}  -\\frac{1}{8} \\end{pmatrix} \\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix} = \\begin{pmatrix} \\frac{41}{208} \\cdot 8 + \\frac{53}{208} \\cdot (-5) \\\\ \\frac{3}{8} \\cdot 8 + (-\\frac{1}{8}) \\cdot (-5) \\end{pmatrix} = \\begin{pmatrix} \\frac{328 - 265}{208} \\\\ 3 + \\frac{5}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{63}{208} \\\\ \\frac{29}{8} \\end{pmatrix}\n    $$\n    最后，计算后验均值 $\\hat{x}$：\n    $$\n    \\hat{x} = x_0 + K(y - Hx_0) = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} + \\begin{pmatrix} \\frac{63}{208} \\\\ \\frac{29}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{208}{208} + \\frac{63}{208} \\\\ -\\frac{16}{8} + \\frac{29}{8} \\end{pmatrix} = \\begin{pmatrix} \\frac{271}{208} \\\\ \\frac{13}{8} \\end{pmatrix}\n    $$\n\n联合增益矩阵是 $K = \\begin{pmatrix} \\frac{41}{208}  \\frac{53}{208} \\\\ \\frac{3}{8}  -\\frac{1}{8} \\end{pmatrix}$，更新后的后验均值是 $\\hat{x} = \\begin{pmatrix} \\frac{271}{208} \\\\ \\frac{13}{8} \\end{pmatrix}$。\n为了将答案表示为单个表达式，我们构建一个分块矩阵，其中前两列代表增益矩阵，第三列代表后验均值向量。\n$$\n\\begin{pmatrix}\nK  \\hat{x}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{41}{208}  \\frac{53}{208}  \\frac{271}{208} \\\\\n\\frac{3}{8}  -\\frac{1}{8}  \\frac{13}{8}\n\\end{pmatrix}\n$$\n值 $\\frac{13}{8}$ 也可以写成 $\\frac{338}{208}$ 以使第二行有共同的分母，但这不是必需的。$\\frac{3}{8} = \\frac{78}{208}$ 和 $-\\frac{1}{8} = -\\frac{26}{208}$ 已经被简化。保留 $\\frac{13}{8}$ 更简单。\n最后检查：$\\frac{13}{8} = \\frac{13 \\times 26}{8 \\times 26} = \\frac{338}{208}$。所以该矩阵可以写成 $\\frac{1}{208}\\begin{pmatrix} 41  53  271 \\\\ 78  -26  338 \\end{pmatrix}$。这也是一个有效的表示。所要求的格式是精确的有理数，混合分母的形式满足了这一要求。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{41}{208}  \\frac{53}{208}  \\frac{271}{208} \\\\\n\\frac{3}{8}  -\\frac{1}{8}  \\frac{13}{8}\n\\end{pmatrix}\n}\n$$", "id": "3404715"}]}