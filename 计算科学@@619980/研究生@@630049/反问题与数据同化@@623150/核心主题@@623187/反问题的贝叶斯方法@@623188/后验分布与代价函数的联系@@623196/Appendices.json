{"hands_on_practices": [{"introduction": "变分数据同化的第一步，是将贝叶斯推断问题转化为一个优化问题。本练习将指导你从广义线性模型 (GLM) 的一般统计描述和高斯先验出发，亲手构建相应的负对数后验概率，也就是我们所说的代价函数 [@problem_id:3411489]。这项实践旨在强化一个核心原则：代价函数并非随意设定，而是由后验概率密度函数严格推导而来。", "problem": "考虑一个广义线性模型（GLM），该模型具有典范连结，用于处理以预测变量 $\\{\\mathbf{x}_i\\}_{i=1}^{n}$ 为条件的独立观测值 $\\{y_i\\}_{i=1}^{n}$，其中每个 $y_i$ 服从单参数正则指数族，其密度为\n$$\np(y_i \\mid \\theta_i) = \\exp\\!\\big(y_i \\theta_i - b(\\theta_i) + c(y_i)\\big),\n$$\n其中 $b(\\theta)$ 是累积量（对数配分）函数，$c(y)$ 是基测度。在典范连结 GLM 中，自然参数等于线性预测器，即 $\\theta_i = \\eta_i = \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}$，其中 $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ 是一个系数向量。假设 $\\boldsymbol{\\beta}$ 服从高斯先验，即 $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$，其中 $\\boldsymbol{\\mu} \\in \\mathbb{R}^{p}$ 且 $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$ 是对称正定矩阵。\n\n从指数族和典范连结的定义性质出发，推导出联合似然 $p(\\mathbf{y} \\mid \\boldsymbol{\\beta})$，然后推导出作为 $\\boldsymbol{\\beta}$ 函数的负对数似然，并从基本原理出发仔细证明其凸性。使用贝叶斯法则和高斯先验，写出等于负对数后验（不含依赖于 $\\boldsymbol{\\beta}$ 的加性常数）的组合成本函数。将您的最终答案表示为包含 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n}$、$b(\\cdot)$、$\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\Sigma}$ 的单一闭式解析表达式。\n\n您的最终答案必须是一个单一的解析表达式。不需要数值近似。", "solution": "问题陈述内部一致，科学上基于统计模型理论，且提法恰当。所有定义和条件在广义线性模型和贝叶斯推断领域都是标准的。因此，该问题被认为是有效的，可以构建一个完整的解。\n\n我们首先根据所提供的信息建立数学框架。数据由 $n$ 个独立观测值 $\\{y_i\\}_{i=1}^{n}$ 组成。给定自然参数 $\\theta_i$ 时，每个观测值 $y_i$ 的条件概率密度函数属于单参数正则指数族：\n$$\np(y_i \\mid \\theta_i) = \\exp\\big(y_i \\theta_i - b(\\theta_i) + c(y_i)\\big)\n$$\n在具有典范连结函数的广义线性模型（GLM）中，自然参数 $\\theta_i$ 被设定为等于线性预测器 $\\eta_i$。线性预测器定义为 $\\eta_i = \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^{p}$ 是观测值 $i$ 的预测变量向量，$\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ 是模型系数向量。因此，我们有以下关系：\n$$\n\\theta_i = \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\n$$\n将此关系代入 $y_i$ 的密度函数，我们得到给定系数向量 $\\boldsymbol{\\beta}$ 时 $y_i$ 的条件密度：\n$$\np(y_i \\mid \\boldsymbol{\\beta}) = \\exp\\big(y_i (\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) + c(y_i)\\big)\n$$\n\n问题陈述中指出观测值是独立的。因此，给定 $\\boldsymbol{\\beta}$ 时，整个数据集 $\\mathbf{y} = (y_1, \\dots, y_n)^{\\top}$ 的联合似然是各个似然的乘积：\n$$\np(\\mathbf{y} \\mid \\boldsymbol{\\beta}) = \\prod_{i=1}^{n} p(y_i \\mid \\boldsymbol{\\beta}) = \\prod_{i=1}^{n} \\exp\\big(y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} - b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) + c(y_i)\\big)\n$$\n利用指数函数的性质，即指数的乘积等于其参数之和的指数，我们可以将联合似然写为：\n$$\np(\\mathbf{y} \\mid \\boldsymbol{\\beta}) = \\exp\\left( \\sum_{i=1}^{n} \\left( y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} - b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) + c(y_i) \\right) \\right)\n$$\n\n对数似然函数，记为 $\\ell(\\boldsymbol{\\beta})$，是联合似然的自然对数：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\ln p(\\mathbf{y} \\mid \\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} - b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) + c(y_i) \\right)\n$$\n问题要求的是负对数似然，我们将其记为 $L(\\boldsymbol{\\beta})$：\n$$\nL(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} - c(y_i) \\right)\n$$\n\n接下来，我们必须从基本原理出发证明 $L(\\boldsymbol{\\beta})$ 关于 $\\boldsymbol{\\beta}$ 的凸性。如果一个函数的海森矩阵（二阶偏导数矩阵）是半正定的，则该函数是凸的。我们来计算 $L(\\boldsymbol{\\beta})$ 的海森矩阵。项 $\\sum y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}$ 关于 $\\boldsymbol{\\beta}$ 是线性的，而项 $\\sum c(y_i)$ 关于 $\\boldsymbol{\\beta}$ 是常数。这些项的二阶导数为零。因此，$L(\\boldsymbol{\\beta})$ 的凸性完全由项 $\\sum_{i=1}^{n} b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta})$ 决定。\n令 $\\theta_i(\\boldsymbol{\\beta}) = \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}$。$L(\\boldsymbol{\\beta})$ 关于 $\\boldsymbol{\\beta}$ 的梯度是：\n$$\n\\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( \\nabla_{\\boldsymbol{\\beta}} b(\\theta_i(\\boldsymbol{\\beta})) - \\nabla_{\\boldsymbol{\\beta}} (y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) \\right)\n$$\n使用链式法则，$\\nabla_{\\boldsymbol{\\beta}} b(\\theta_i) = b'(\\theta_i) \\nabla_{\\boldsymbol{\\beta}} \\theta_i = b'(\\theta_i) \\mathbf{x}_i$。线性项的梯度是 $\\nabla_{\\boldsymbol{\\beta}} (y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) = y_i \\mathbf{x}_i$。所以，\n$$\n\\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left(b'(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - y_i\\right) \\mathbf{x}_i\n$$\n海森矩阵 $\\mathbf{H}_{\\boldsymbol{\\beta}}[L]$ 通过对梯度求导得到：\n$$\n\\mathbf{H}_{\\boldsymbol{\\beta}}[L] = \\nabla_{\\boldsymbol{\\beta}} \\left( \\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) \\right)^{\\top} = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\beta}} \\left( \\left(b'(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - y_i\\right) \\mathbf{x}_i^{\\top} \\right)\n$$\n再次应用链式法则：\n$$\n\\mathbf{H}_{\\boldsymbol{\\beta}}[L] = \\sum_{i=1}^{n} \\mathbf{x}_i \\left( \\nabla_{\\boldsymbol{\\beta}} b'(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) \\right)^{\\top} = \\sum_{i=1}^{n} \\mathbf{x}_i \\left( b''(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) (\\nabla_{\\boldsymbol{\\beta}} (\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}))^{\\top} \\right) = \\sum_{i=1}^{n} b''(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) \\mathbf{x}_i \\mathbf{x}_i^{\\top}\n$$\n任何正则指数族的一个基本性质是累积量函数 $b(\\theta)$ 是严格凸的，这意味着其二阶导数是非负的：$b''(\\theta) \\ge 0$。事实上，对于一个正则族，$b''(\\theta) = \\text{Var}(Y \\mid \\theta)$，这必须是非负的。矩阵 $\\mathbf{x}_i \\mathbf{x}_i^{\\top}$ 是一个外积，它总是半正定的，因为对于任何向量 $\\mathbf{v} \\in \\mathbb{R}^p$，我们有 $\\mathbf{v}^{\\top}(\\mathbf{x}_i \\mathbf{x}_i^{\\top})\\mathbf{v} = (\\mathbf{v}^{\\top}\\mathbf{x}_i)(\\mathbf{x}_i^{\\top}\\mathbf{v}) = (\\mathbf{x}_i^{\\top}\\mathbf{v})^2 \\ge 0$。\n一个非负标量 $b''(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta})$ 与一个半正定矩阵 $\\mathbf{x}_i \\mathbf{x}_i^{\\top}$ 的乘积也是半正定的。$L(\\boldsymbol{\\beta})$ 的海森矩阵是这些半正定矩阵的和。半正定矩阵的和本身也是半正定的。因此，$\\mathbf{H}_{\\boldsymbol{\\beta}}[L]$ 是半正定的，这证明了负对数似然 $L(\\boldsymbol{\\beta})$ 是 $\\boldsymbol{\\beta}$ 的一个凸函数。\n\n现在我们引入 $\\boldsymbol{\\beta}$ 的先验分布。先验被指定为高斯分布：$\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$。其概率密度函数是：\n$$\np(\\boldsymbol{\\beta}) = \\frac{1}{(2\\pi)^{p/2} \\det(\\boldsymbol{\\Sigma})^{1/2}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\beta} - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\mu})\\right)\n$$\n负对数先验，忽略与 $\\boldsymbol{\\beta}$ 无关的常数项，是：\n$$\n-\\ln p(\\boldsymbol{\\beta}) \\propto \\frac{1}{2}(\\boldsymbol{\\beta} - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\mu})\n$$\n根据贝叶斯法则，$\\boldsymbol{\\beta}$ 的后验分布与似然和先验的乘积成正比：\n$$\np(\\boldsymbol{\\beta} \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\boldsymbol{\\beta}) p(\\boldsymbol{\\beta})\n$$\n因此，对数后验在相差一个加性常数的情况下为：\n$$\n\\ln p(\\boldsymbol{\\beta} \\mid \\mathbf{y}) \\approx \\ln p(\\mathbf{y} \\mid \\boldsymbol{\\beta}) + \\ln p(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) + \\ln p(\\boldsymbol{\\beta})\n$$\n成本函数 $J(\\boldsymbol{\\beta})$ 定义为负对数后验，不含不依赖于 $\\boldsymbol{\\beta}$ 的加性常数。\n$$\nJ(\\boldsymbol{\\beta}) = - \\ln p(\\boldsymbol{\\beta} \\mid \\mathbf{y}) \\approx -\\ln p(\\mathbf{y} \\mid \\boldsymbol{\\beta}) - \\ln p(\\boldsymbol{\\beta})\n$$\n代入负对数似然和负对数先验的表达式：\n$$\nJ(\\boldsymbol{\\beta}) \\approx \\left(\\sum_{i=1}^{n} \\left( b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} \\right)\\right) + \\frac{1}{2}(\\boldsymbol{\\beta} - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\mu})\n$$\n这里我们从负对数似然中省略了项 $-\\sum c(y_i)$，因为它不依赖于 $\\boldsymbol{\\beta}$。这个最终表达式代表了常用于最大后验（MAP）估计的成本函数。它由一个数据保真项（负对数似然）和一个正则化项（负对数先验）组成。负对数似然的凸性和二次先验项的严格凸性（因为 $\\boldsymbol{\\Sigma}^{-1}$ 是正定的）确保了总成本函数 $J(\\boldsymbol{\\beta})$ 是严格凸的，从而保证了唯一最小值的存在。\n\n所要求的最终答案是该成本函数的闭式解析表达式。", "answer": "$$\n\\boxed{\\sum_{i=1}^{n} \\left(b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\right) + \\frac{1}{2}(\\boldsymbol{\\beta} - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\mu})}\n$$", "id": "3411489"}, {"introduction": "在确定了通过最小化代价函数来寻找后验众数后，我们必须面对一个关键而又微妙的问题。本练习将通过一个简单的参数重整化案例，揭示最大后验 (MAP) 估计量在坐标变换下不具备不变性的性质 [@problem_id:3411380]。这个结果强调了MAP估计不仅仅是一个优化问题，它与参数化的选择紧密相关，凸显了在处理后验分布时正确运用变量替换法则的重要性。", "problem": "考虑一个数据同化中的一维反问题，其中未知状态 $u \\in \\mathbb{R}$ 是从单个观测值 $y \\in \\mathbb{R}$ 推断出来的。$u$ 的先验分布是高斯分布，其密度为 $p_U(u) \\propto \\exp\\!\\big(-\\frac{1}{2}u^{2}\\big)$，观测模型也是高斯分布，其密度为 $p(y \\mid u) \\propto \\exp\\!\\big(-\\frac{1}{2}(y - u)^{2}\\big)$。您的观测值为 $y = 1$。后验分布 $p(u \\mid y)$ 由贝叶斯法则定义，在 $u$ 参数化下，相关的变分目标（负对数后验，不计加性常数）是 $J_u(u) = \\frac{1}{2}(y - u)^{2} + \\frac{1}{2}u^{2}$。\n\n现在考虑重参数化 $v = \\phi(u)$，其中 $\\phi(u) = \\exp(u)$，它定义了一个新参数 $v \\in (0, \\infty)$，通过 $u = \\ln(v)$ 与 $u$ 相关联。变换后的后验密度 $p(v \\mid y)$ 是通过概率密度的变量替换公式定义的。同时，在 $v$ 参数化下定义一个错误指定的变分目标，\n$$\nJ_v^{\\mathrm{naive}}(v) = \\frac{1}{2}(v - y)^{2} + \\frac{1}{2}v^{2},\n$$\n该目标直接将 $v$ 视为具有高斯先验，并忽略了变换的雅可比项。这个 $J_v^{\\mathrm{naive}}(v)$ 不等于正确变换后的 $J_u(\\ln v)$ 加上由后验密度的变量替换所隐含的雅可比项。\n\n从高斯似然和先验、贝叶斯法则以及概率密度的变量替换公式的基本定义出发，进行以下计算：\n\n1. 计算在 $u$ 参数化下的最大后验（MAP）位置，其定义为 $J_u(u)$ 的最小值点。\n2. 计算在 $v$ 参数化下的 MAP，该 MAP 是通过使用变量替换公式将后验 $p(u \\mid y)$ 一致地变换为 $p(v \\mid y)$，然后在 $v \\in (0, \\infty)$ 上最大化 $p(v \\mid y)$ 得到的。\n3. 计算错误指定的目标 $J_v^{\\mathrm{naive}}(v)$ 的最小值点。\n\n将这三个值组成的元组，按 $(u_{\\mathrm{MAP}}, v_{\\mathrm{MAP}}^{\\mathrm{transformed}}, v_{\\mathrm{MAP}}^{\\mathrm{naive}})$ 的顺序，使用 LaTeX 的 $\\mathrm{pmatrix}$ 环境表示为单个行矩阵。无需四舍五入。不涉及单位。仅以所要求的行矩阵形式清晰地陈述最终答案。", "solution": "该问题要求计算三个值：参数 $u$ 的最大后验（MAP）估计，通过一致的变量替换得到的变换后参数 $v = \\exp(u)$ 的 MAP 估计，以及通过最小化一个朴素构建的目标函数得到的第三个 $v$ 的估计。给定观测值 $y = 1$。我们将按顺序处理每个计算。\n\n### 1. 计算 $u$ 的 MAP 估计（$u_{\\mathrm{MAP}}$）\n\n未知状态 $u \\in \\mathbb{R}$ 的后验分布由贝叶斯法则给出：\n$$ p(u \\mid y) \\propto p(y \\mid u) p_U(u) $$\n给定先验密度 $p_U(u) \\propto \\exp\\big(-\\frac{1}{2}u^{2}\\big)$ 和似然 $p(y \\mid u) \\propto \\exp\\big(-\\frac{1}{2}(y - u)^{2}\\big)$，后验密度为：\n$$ p(u \\mid y) \\propto \\exp\\left(-\\frac{1}{2}(y - u)^{2}\\right) \\exp\\left(-\\frac{1}{2}u^{2}\\right) = \\exp\\left(-\\left[\\frac{1}{2}(y - u)^{2} + \\frac{1}{2}u^{2}\\right]\\right) $$\nMAP 估计 $u_{\\mathrm{MAP}}$ 是使后验密度 $p(u \\mid y)$ 最大化的 $u$ 的值。这等价于最小化后验密度的负对数。问题将相关的变分目标（或代价函数）定义为：\n$$ J_u(u) = \\frac{1}{2}(y - u)^{2} + \\frac{1}{2}u^{2} $$\n为了找到 $J_u(u)$ 的最小值点，我们计算它关于 $u$ 的导数并将其设为零。\n$$ \\frac{dJ_u}{du} = \\frac{d}{du}\\left(\\frac{1}{2}(y^{2} - 2yu + u^{2}) + \\frac{1}{2}u^{2}\\right) = \\frac{d}{du}\\left(u^{2} - yu + \\frac{1}{2}y^{2}\\right) $$\n$$ \\frac{dJ_u}{du} = 2u - y $$\n将导数设为零，得到极值点的位置：\n$$ 2u - y = 0 \\implies u = \\frac{y}{2} $$\n二阶导数为 $\\frac{d^2J_u}{du^2} = 2  0$，这证实了该点是一个最小值点。\n因此，$u$ 的 MAP 估计为 $u_{\\mathrm{MAP}} = \\frac{y}{2}$。\n代入给定的观测值 $y = 1$：\n$$ u_{\\mathrm{MAP}} = \\frac{1}{2} $$\n\n### 2. 通过变换计算 $v$ 的 MAP 估计（$v_{\\mathrm{MAP}}^{\\mathrm{transformed}}$）\n\n给定重参数化 $v = \\phi(u) = \\exp(u)$，其中 $v \\in (0, \\infty)$。逆变换是 $u = \\ln(v)$。为了求出 $v$ 的后验密度 $p(v \\mid y)$，我们使用概率密度的变量替换公式：\n$$ p(v \\mid y) = p(u(v) \\mid y) \\left| \\frac{du}{dv} \\right| $$\n变换的雅可比行列式是 $\\left| \\frac{du}{dv} \\right| = \\left| \\frac{1}{v} \\right| = \\frac{1}{v}$，因为 $v  0$。\n将 $u = \\ln(v)$ 和雅可比行列式代入后验表达式中，我们得到：\n$$ p(v \\mid y) \\propto \\exp\\left(-J_u(\\ln(v))\\right) \\cdot \\frac{1}{v} = \\frac{1}{v} \\exp\\left(-\\left[\\frac{1}{2}(y - \\ln(v))^{2} + \\frac{1}{2}(\\ln(v))^{2}\\right]\\right) $$\n$v$ 的 MAP 估计，记为 $v_{\\mathrm{MAP}}^{\\mathrm{transformed}}$，是使 $p(v \\mid y)$ 最大化的值。这等价于最小化该密度的负对数。让我们将 $v$ 的正确目标函数定义为 $J_v^{\\mathrm{correct}}(v) = -\\ln(p(v \\mid y))$（不计加性常数）：\n$$ J_v^{\\mathrm{correct}}(v) = \\left[\\frac{1}{2}(y - \\ln(v))^{2} + \\frac{1}{2}(\\ln(v))^{2}\\right] - \\ln\\left(\\frac{1}{v}\\right) $$\n$$ J_v^{\\mathrm{correct}}(v) = \\frac{1}{2}(y - \\ln(v))^{2} + \\frac{1}{2}(\\ln(v))^{2} + \\ln(v) $$\n为了找到最小值点，我们将 $J_v^{\\mathrm{correct}}(v)$ 对 $v$ 求导，并令导数为零：\n$$ \\frac{dJ_v^{\\mathrm{correct}}}{dv} = \\frac{1}{2} \\cdot 2(y - \\ln(v)) \\cdot \\left(-\\frac{1}{v}\\right) + \\frac{1}{2} \\cdot 2\\ln(v) \\cdot \\frac{1}{v} + \\frac{1}{v} $$\n$$ \\frac{dJ_v^{\\mathrm{correct}}}{dv} = \\frac{1}{v} \\left( -(y - \\ln(v)) + \\ln(v) + 1 \\right) $$\n$$ \\frac{dJ_v^{\\mathrm{correct}}}{dv} = \\frac{1}{v} \\left( -y + \\ln(v) + \\ln(v) + 1 \\right) = \\frac{1}{v} \\left( 2\\ln(v) - y + 1 \\right) $$\n由于 $v  0$，我们将括号中的项设为零：\n$$ 2\\ln(v) - y + 1 = 0 \\implies \\ln(v) = \\frac{y - 1}{2} $$\n$$ v = \\exp\\left(\\frac{y - 1}{2}\\right) $$\n二阶导数检验证实了这是一个最小值点。因此，$v_{\\mathrm{MAP}}^{\\mathrm{transformed}} = \\exp\\left(\\frac{y-1}{2}\\right)$。\n代入给定的观测值 $y = 1$：\n$$ v_{\\mathrm{MAP}}^{\\mathrm{transformed}} = \\exp\\left(\\frac{1 - 1}{2}\\right) = \\exp(0) = 1 $$\n\n### 3. 计算朴素目标函数的最小值点（$v_{\\mathrm{MAP}}^{\\mathrm{naive}}$）\n\n问题为 $v$ 定义了一个错误指定的或“朴素”的目标函数：\n$$ J_v^{\\mathrm{naive}}(v) = \\frac{1}{2}(v - y)^{2} + \\frac{1}{2}v^{2} $$\n这种函数形式错误地假设 $v$ 具有高斯先验，并且观测模型在 $v$ 中是线性的。我们被要求找到这个函数的最小值点，记为 $v_{\\mathrm{MAP}}^{\\mathrm{naive}}$。我们通过求导来解决：\n$$ \\frac{dJ_v^{\\mathrm{naive}}}{dv} = \\frac{d}{dv}\\left(\\frac{1}{2}(v^2 - 2vy + y^2) + \\frac{1}{2}v^2\\right) = \\frac{d}{dv}\\left(v^2 - vy + \\frac{1}{2}y^2\\right) $$\n$$ \\frac{dJ_v^{\\mathrm{naive}}}{dv} = 2v - y $$\n将导数设为零可得：\n$$ 2v - y = 0 \\implies v = \\frac{y}{2} $$\n二阶导数为 $\\frac{d^2J_v^{\\mathrm{naive}}}{dv^2} = 2  0$，证实了这是一个最小值点。\n因此，最小值点为 $v_{\\mathrm{MAP}}^{\\mathrm{naive}} = \\frac{y}{2}$。\n代入给定的观测值 $y = 1$：\n$$ v_{\\mathrm{MAP}}^{\\mathrm{naive}} = \\frac{1}{2} $$\n\n### 结果总结\n\n计算出的三个值是：\n1.  $u_{\\mathrm{MAP}} = \\frac{1}{2}$\n2.  $v_{\\mathrm{MAP}}^{\\mathrm{transformed}} = 1$\n3.  $v_{\\mathrm{MAP}}^{\\mathrm{naive}} = \\frac{1}{2}$\n\n最终答案是行矩阵 $(u_{\\mathrm{MAP}}, v_{\\mathrm{MAP}}^{\\mathrm{transformed}}, v_{\\mathrm{MAP}}^{\\mathrm{naive}})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}  1  \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3411380"}, {"introduction": "接下来我们进入更具挑战性的实际应用，即非线性模型的优化与不确定性量化。在本练习中，你将通过编程实现一个算法，用于比较代价函数的真实Hessian矩阵与其常见的高斯-牛顿近似 [@problem_id:3411467]。这项计算任务将让你直观地理解模型非线性是如何导致真实曲率与近似曲率之间产生差异的，而这种差异直接影响了优化算法的收敛效率和不确定性评估的准确性。", "problem": "考虑一个贝叶斯逆问题，其参数向量为 $u \\in \\mathbb{R}^d$，正演映射为 $G:\\mathbb{R}^d \\to \\mathbb{R}^m$，观测数据为 $y \\in \\mathbb{R}^m$，加性高斯观测噪声的协方差为 $\\Gamma \\in \\mathbb{R}^{m \\times m}$，以及一个高斯先验 $u \\sim \\mathcal{N}(u_0, C)$，其均值为 $u_0 \\in \\mathbb{R}^d$，协方差为 $C \\in \\mathbb{R}^{d \\times d}$。负对数后验（在数据同化中通常称为代价函数）为\n$$\n\\Phi(u) = \\frac{1}{2}\\,(y - G(u))^\\top \\Gamma^{-1} (y - G(u)) + \\frac{1}{2}\\,(u - u_0)^\\top C^{-1} (u - u_0).\n$$\n最大后验（MAP）估计量 $u^\\star$ 是 $\\Phi$ 的任意极小化子，在 $u^\\star$ 处的后验曲率是其 Hessian 矩阵 $\\nabla^2 \\Phi(u^\\star)$。基于伴随的 Gauss-Newton Hessian 矩阵仅使用 $G$ 的一阶导数，并通过以下近似给出\n$$\nH_{\\mathrm{GN}}(u) = J_G(u)^\\top \\Gamma^{-1} J_G(u) + C^{-1},\n$$\n其中 $J_G(u)$ 表示 $G$ 在 $u$ 处的 Jacobian 矩阵。在非线性问题中，$H_{\\mathrm{GN}}(u^\\star)$ 通常与真实后验曲率 $\\nabla^2 \\Phi(u^\\star)$ 不同，而这种差异通过改变后验协方差的高斯近似来影响不确定性量化。\n\n从贝叶斯定理、似然和先验的定义，以及复合函数的梯度和 Hessian 矩阵的计算法则出发，推导一个关于真实后验曲率 $\\nabla^2 \\Phi(u^\\star)$ 与 Gauss-Newton 近似 $H_{\\mathrm{GN}}(u^\\star)$ 之间差异的可计算表达式，该表达式应以 Jacobian 矩阵 $J_G(u^\\star)$、残差 $r(u^\\star) = y - G(u^\\star)$ 以及正演模型各分量的二阶导数矩阵集合来表示。然后，设计一个算法，该算法：\n- 通过使用精确梯度和 Hessian 矩阵的二阶方法来最小化 $\\Phi(u)$，从而计算 $u^\\star$，\n- 在 $u^\\star$ 处评估真实后验曲率和 Gauss-Newton 近似，\n- 量化曲率间隙及其对不确定性量化的下游影响。\n\n仅使用以下基本基础和定义：\n- 连接后验、先验和似然的贝叶斯定理。\n- 高斯噪声的负对数似然和高斯先验的负对数先验。\n- 复合映射的梯度和 Hessian 矩阵的链式法则。\n- 矩阵范数和迹的标准线性代数恒等式。\n\n您必须为每个测试用例实现并报告两个标量诊断指标：\n- 曲率间隙比率 $g = \\lVert \\nabla^2 \\Phi(u^\\star) - H_{\\mathrm{GN}}(u^\\star) \\rVert_F \\,\\big/\\, \\lVert \\nabla^2 \\Phi(u^\\star) \\rVert_F$，其中 $\\lVert \\cdot \\rVert_F$ 表示 Frobenius 范数，\n- 两个高斯后验近似 $\\mathcal{N}(u^\\star, \\nabla^2 \\Phi(u^\\star)^{-1})$ 和 $\\mathcal{N}(u^\\star, H_{\\mathrm{GN}}(u^\\star)^{-1})$ 之间的对称化 Kullback–Leibler 散度 $D$，定义为这两个高斯分布之间的两个有向 Kullback–Leibler 散度之和。\n\n您的程序必须实现以下测试套件，其中所有矩阵和向量都已明确指定。在每个测试中，参数维度为 $d=2$，观测维度为 $m=2$。每个测试都指定了正演映射 $G$、先验 $(u_0, C)$、观测配置 $(y, \\Gamma)$，以及一个等于 $u_0$ 的规定优化初始猜测值。\n\n- 测试 1（线性，预期无曲率间隙）：\n  - $G(u) = \\begin{bmatrix} u_1 + 0.5\\,u_2 \\\\ u_2 \\end{bmatrix}$，\n  - $u_0 = \\begin{bmatrix} 0.2 \\\\ -0.3 \\end{bmatrix}$，\n  - $C = \\mathrm{diag}\\big(0.49, 0.25\\big)$，\n  - $\\Gamma = \\begin{bmatrix} 0.09  0.03 \\\\ 0.03  0.16 \\end{bmatrix}$，\n  - $y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$。\n\n- 测试 2（非线性，在先验均值处后验精确平稳，MAP 处残差为零）：\n  - $G(u) = \\begin{bmatrix} u_1 \\\\ \\exp(u_2) \\end{bmatrix}$，\n  - $u_0 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$，\n  - $C = \\mathrm{diag}\\big(1.0, 1.0\\big)$，\n  - $\\Gamma = \\mathrm{diag}\\big(0.01, 0.01\\big)$，\n  - $y = \\begin{bmatrix} 0.2 \\\\ \\exp(-0.1) \\end{bmatrix}$。\n\n- 测试 3（非线性，数据约束良好，预期曲率间隙较小）：\n  - $G(u) = \\begin{bmatrix} u_1 \\\\ \\exp(u_2) \\end{bmatrix}$，\n  - $u_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$，\n  - $C = \\mathrm{diag}\\big(10.0, 10.0\\big)$，\n  - $\\Gamma = \\mathrm{diag}\\big(0.04, 0.04\\big)$，\n  - $y = \\begin{bmatrix} 0.8 \\\\ \\exp(0.3) \\end{bmatrix}$。\n\n- 测试 4（非线性，弱数据和信息丰富的先验，预期曲率间隙较大）：\n  - $G(u) = \\begin{bmatrix} u_1 \\\\ \\exp(u_2) \\end{bmatrix}$，\n  - $u_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$，\n  - $C = \\mathrm{diag}\\big(0.25, 0.25\\big)$，\n  - $\\Gamma = \\mathrm{diag}\\big(4.0, 4.0\\big)$，\n  - $y = \\begin{bmatrix} 0.8 \\\\ \\exp(0.3) \\end{bmatrix}$。\n\n- 测试 5（具有耦合和相关噪声的非线性问题）：\n  - $G(u) = \\begin{bmatrix} \\exp(u_1 + 0.5\\,u_2) \\\\ u_1^2 - u_2 \\end{bmatrix}$，\n  - $u_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$，\n  - $C = \\mathrm{diag}\\big(1.0, 1.0\\big)$，\n  - $\\Gamma = \\begin{bmatrix} 0.09  0.04 \\\\ 0.04  0.16 \\end{bmatrix}$，\n  - $y = \\begin{bmatrix} 1.0 \\\\ 0.21 \\end{bmatrix}$。\n\n算法要求：\n- 对每个测试，通过使用精确梯度和 Hessian 矩阵的二阶方法最小化 $\\Phi(u)$ 来计算 $u^\\star$。\n- 在 $u^\\star$ 处，构建残差 $r(u^\\star) = y - G(u^\\star)$、Jacobian 矩阵 $J_G(u^\\star)$ 和对于 $i \\in \\{1,2\\}$ 的逐分量 Hessian 矩阵 $\\nabla^2 G_i(u^\\star)$。\n- 组装真实后验曲率 $\\nabla^2 \\Phi(u^\\star)$ 和 Gauss-Newton 曲率 $H_{\\mathrm{GN}}(u^\\star)$。\n- 按规定计算曲率间隙比率 $g$。\n- 计算 $\\mathcal{N}(u^\\star, \\nabla^2 \\Phi(u^\\star)^{-1})$ 和 $\\mathcal{N}(u^\\star, H_{\\mathrm{GN}}(u^\\star)^{-1})$ 之间的对称化 Kullback–Leibler 散度 $D$。\n- 将每个测试的两个诊断指标聚合为一个长度为 2 的列表 $[g, D]$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试对应一个条目，并按上述顺序排列。每个条目本身必须是按 $[g,D]$ 顺序排列的两个浮点数列表。例如，输出格式必须类似于 $[[g_1,D_1],[g_2,D_2],\\dots]$。", "solution": "用户希望得到真实后验 Hessian 矩阵与其 Gauss-Newton 近似之间差异的详细推导，并随后通过数值实现来对几个测试用例量化此差距。\n\n### 问题验证\n\n根据指定标准对问题陈述进行解析和验证。\n\n**步骤 1：提取已知条件**\n\n- **参数向量**：$u \\in \\mathbb{R}^d$\n- **观测向量**：$y \\in \\mathbb{R}^m$\n- **正演映射**：$G:\\mathbb{R}^d \\to \\mathbb{R}^m$\n- **先验分布**：$u \\sim \\mathcal{N}(u_0, C)$，均值为 $u_0 \\in \\mathbb{R}^d$，协方差为 $C \\in \\mathbb{R}^{d \\times d}$。\n- **观测噪声**：加性高斯噪声，协方差为 $\\Gamma \\in \\mathbb{R}^{m \\times m}$。\n- **负对数后验（代价函数）**：$\\Phi(u) = \\frac{1}{2}\\,(y - G(u))^\\top \\Gamma^{-1} (y - G(u)) + \\frac{1}{2}\\,(u - u_0)^\\top C^{-1} (u - u_0)$。\n- **最大后验（MAP）估计量**：$u^\\star$ 是 $\\Phi(u)$ 的一个极小化子。\n- **Gauss-Newton Hessian 近似**：$H_{\\mathrm{GN}}(u) = J_G(u)^\\top \\Gamma^{-1} J_G(u) + C^{-1}$，其中 $J_G(u)$ 是 $G$ 的 Jacobian 矩阵。\n- **诊断指标**：\n    - 曲率间隙比率：$g = \\lVert \\nabla^2 \\Phi(u^\\star) - H_{\\mathrm{GN}}(u^\\star) \\rVert_F \\,\\big/\\, \\lVert \\nabla^2 \\Phi(u^\\star) \\rVert_F$\n    - $\\mathcal{N}(u^\\star, (\\nabla^2 \\Phi(u^\\star))^{-1})$ 与 $\\mathcal{N}(u^\\star, H_{\\mathrm{GN}}(u^\\star)^{-1})$ 之间的对称化 Kullback–Leibler 散度 $D$。\n- **测试用例**：五个完全指定的测试用例，其中 $d=2, m=2$，提供了 $G(u)$、$u_0$、$C$、$\\Gamma$ 和 $y$。优化的初始猜测值为 $u_0$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n- **科学基础**：该问题根植于贝叶斯推断用于逆问题的标准数学框架。后验分布、MAP 估计和 Gauss-Newton 近似等概念是该领域的核心。\n- **适定性**：该问题是适定的。它要求推导一个标准结果，并在明确定义的测试用例上进行数值验证。所有必要的函数、参数和矩阵都已提供，使得问题明确且可解。\n- **客观性**：问题使用精确、客观的数学语言陈述，不含任何主观因素。\n\n问题表述无任何缺陷。它在科学上是合理的、自洽的，并且可以形式化。\n\n**步骤 3：结论与行动**\n\n此问题是**有效的**。将提供完整解决方案。\n\n### Hessian 矩阵差异的推导\n\n目标是推导 $\\nabla^2 \\Phi(u) - H_{\\mathrm{GN}}(u)$ 的表达式。我们首先将负对数后验 $\\Phi(u)$ 分解为其似然和先验分量：\n$$\n\\Phi(u) = \\Phi_{\\text{like}}(u) + \\Phi_{\\text{prior}}(u)\n$$\n其中负对数似然为 $\\Phi_{\\text{like}}(u) = \\frac{1}{2}\\,(y - G(u))^\\top \\Gamma^{-1} (y - G(u))$，负对数先验为 $\\Phi_{\\text{prior}}(u) = \\frac{1}{2}\\,(u - u_0)^\\top C^{-1} (u - u_0)$。\n\nHessian 矩阵是可加的，所以 $\\nabla^2 \\Phi(u) = \\nabla^2 \\Phi_{\\text{like}}(u) + \\nabla^2 \\Phi_{\\text{prior}}(u)$。\n\n**1. 先验项的 Hessian 矩阵**\n\n先验项 $\\Phi_{\\text{prior}}(u)$ 是 $u$ 的二次函数。\n$$\n\\Phi_{\\text{prior}}(u) = \\frac{1}{2} (u^\\top C^{-1} u - u^\\top C^{-1} u_0 - u_0^\\top C^{-1} u + u_0^\\top C^{-1} u_0)\n$$\n使用向量微积分的标准法则，其梯度为：\n$$\n\\nabla \\Phi_{\\text{prior}}(u) = C^{-1}(u - u_0)\n$$\n再次对 $u$ 求导，得到 Hessian 矩阵：\n$$\n\\nabla^2 \\Phi_{\\text{prior}}(u) = C^{-1}\n$$\n这是一个常数矩阵，与 $u$ 无关。\n\n**2. 似然项的 Hessian 矩阵**\n\n似然项 $\\Phi_{\\text{like}}(u)$ 涉及正演模型 $G(u)$ 的复合。设残差为 $r(u) = y - G(u)$。则 $\\Phi_{\\text{like}}(u) = \\frac{1}{2} r(u)^\\top \\Gamma^{-1} r(u)$。\n我们使用链式法则来求梯度 $\\nabla \\Phi_{\\text{like}}(u)$。其第 $k$ 个分量为：\n$$\n\\frac{\\partial \\Phi_{\\text{like}}}{\\partial u_k} = \\frac{\\partial r}{\\partial u_k}^\\top \\Gamma^{-1} r(u) = - \\frac{\\partial G}{\\partial u_k}^\\top \\Gamma^{-1} r(u)\n$$\n以向量形式表示，梯度是由上述分量组成的向量，可以使用 Jacobian 矩阵 $J_G(u) = \\nabla G(u)$ 紧凑地写出：\n$$\n\\nabla \\Phi_{\\text{like}}(u) = -J_G(u)^\\top \\Gamma^{-1} r(u) = -J_G(u)^\\top \\Gamma^{-1} (y - G(u))\n$$\n为了求 Hessian 矩阵 $\\nabla^2 \\Phi_{\\text{like}}(u)$，我们将梯度对 $u$ 求导。以分量形式表示，Hessian 矩阵的第 $(k,l)$ 个元素是：\n$$\n\\frac{\\partial^2 \\Phi_{\\text{like}}}{\\partial u_l \\partial u_k} = \\frac{\\partial}{\\partial u_l} \\left( -\\sum_{i,j} \\frac{\\partial G_i}{\\partial u_k} (\\Gamma^{-1})_{ij} r_j(u) \\right)\n$$\n使用乘积法则：\n$$\n\\frac{\\partial^2 \\Phi_{\\text{like}}}{\\partial u_l \\partial u_k} = - \\sum_{i,j} \\left( \\frac{\\partial^2 G_i}{\\partial u_l \\partial u_k} (\\Gamma^{-1})_{ij} r_j(u) + \\frac{\\partial G_i}{\\partial u_k} (\\Gamma^{-1})_{ij} \\frac{\\partial r_j}{\\partial u_l} \\right)\n$$\n由于 $\\frac{\\partial r_j}{\\partial u_l} = -\\frac{\\partial G_j}{\\partial u_l}$，上式变为：\n$$\n\\frac{\\partial^2 \\Phi_{\\text{like}}}{\\partial u_l \\partial u_k} = \\sum_{i,j} \\frac{\\partial G_i}{\\partial u_k} (\\Gamma^{-1})_{ij} \\frac{\\partial G_j}{\\partial u_l} - \\sum_{i,j} \\frac{\\partial^2 G_i}{\\partial u_l \\partial u_k} (\\Gamma^{-1})_{ij} r_j(u)\n$$\n将其重组为矩阵形式：\n第一项是 $J_G(u)^\\top \\Gamma^{-1} J_G(u)$ 的第 $(k,l)$ 个元素。\n第二项可以写成关于残差分量的和。设 $H_i(u) = \\nabla^2 G_i(u)$ 为正演映射第 $i$ 个分量的 Hessian 矩阵。则第二项为：\n$$\n- \\sum_{i,j} (\\Gamma^{-1})_{ij} r_j(u) (H_i(u))_{lk} = - \\sum_i \\left( \\sum_j (\\Gamma^{-1})_{ij} r_j(u) \\right) (H_i(u))_{lk}\n$$\n括号中的项是向量 $\\Gamma^{-1} r(u)$ 的第 $i$ 个分量。设此向量为 $w(u) = \\Gamma^{-1}(y - G(u))$，通常称为加权残差向量。\n因此，第二项为 $-\\sum_{i=1}^m w_i(u) H_i(u)$。\n\n综合这些，似然项的 Hessian 矩阵是：\n$$\n\\nabla^2 \\Phi_{\\text{like}}(u) = J_G(u)^\\top \\Gamma^{-1} J_G(u) - \\sum_{i=1}^m w_i(u) \\nabla^2 G_i(u)\n$$\n\n**3. 总 Hessian 矩阵与差异**\n\n负对数后验的总 Hessian 矩阵是：\n$$\n\\nabla^2 \\Phi(u) = \\nabla^2 \\Phi_{\\text{like}}(u) + \\nabla^2 \\Phi_{\\text{prior}}(u) = \\left( J_G(u)^\\top \\Gamma^{-1} J_G(u) - \\sum_{i=1}^m w_i(u) \\nabla^2 G_i(u) \\right) + C^{-1}\n$$\n重新整理各项以组合 Gauss-Newton 部分：\n$$\n\\nabla^2 \\Phi(u) = \\underbrace{J_G(u)^\\top \\Gamma^{-1} J_G(u) + C^{-1}}_{H_{\\mathrm{GN}}(u)} - \\sum_{i=1}^m \\left[\\Gamma^{-1}(y - G(u))\\right]_i \\nabla^2 G_i(u)\n$$\n因此，真实 Hessian 矩阵与 Gauss-Newton 近似之间的差异为：\n$$\n\\nabla^2 \\Phi(u) - H_{\\mathrm{GN}}(u) = - \\sum_{i=1}^m \\left[\\Gamma^{-1}(y - G(u))\\right]_i \\nabla^2 G_i(u)\n$$\n此表达式在 MAP 估计 $u^\\star$ 处求值，以量化曲率间隙。该间隙线性依赖于加权残差和正演模型的二阶导数（非线性）。如果正演模型是线性的（对所有 $i$ 都有 $\\nabla^2 G_i(u) = 0$），或者如果模型在 $u^\\star$ 处能完美拟合数据（$G(u^\\star) = y$，因此残差为零），则该间隙为零。\n\n### 算法设计与实现\n\n对每个测试用例，算法按以下步骤进行：\n\n1.  **优化**：从规定的初始猜测值 $u_0$ 开始，通过最小化 $\\Phi(u)$ 来找到 MAP 估计 $u^\\star$。我们使用信赖域牛顿共轭梯度法（`scipy.optimize.minimize` 中的 `trust-ncg`），这是一种需要精确梯度 $\\nabla\\Phi(u)$ 和 Hessian 矩阵 $\\nabla^2\\Phi(u)$ 的二阶方法。这些都根据上述推导实现为 Python 函数。\n\n2.  **曲率评估**：一旦确定了 $u^\\star$，我们就评估两个曲率矩阵：\n    - 真实后验曲率 $\\nabla^2\\Phi(u^\\star)$，直接使用为优化器开发的 Hessian 函数计算。\n    - Gauss-Newton 近似 $H_{\\mathrm{GN}}(u^\\star)$，使用其定义计算：$H_{\\mathrm{GN}}(u^\\star) = J_G(u^\\star)^\\top \\Gamma^{-1} J_G(u^\\star) + C^{-1}$。\n\n3.  **诊断指标计算**：\n    - **曲率间隙比率 ($g$)**：两个 Hessian 矩阵的相对差异用 Frobenius 范数衡量：\n      $$ g = \\frac{\\lVert \\nabla^2 \\Phi(u^\\star) - H_{\\mathrm{GN}}(u^\\star) \\rVert_F}{\\lVert \\nabla^2 \\Phi(u^\\star) \\rVert_F} $$\n    - **对称化 KL 散度 ($D$)**：两个具有相同均值的多元高斯分布 $\\mathcal{P}_1 = \\mathcal{N}(\\mu, \\Sigma_1)$ 和 $\\mathcal{P}_2 = \\mathcal{N}(\\mu, \\Sigma_2)$ 之间的 Kullback-Leibler 散度为 $D_{\\mathrm{KL}}(\\mathcal{P}_1 \\| \\mathcal{P}_2) = \\frac{1}{2} \\left( \\mathrm{tr}(\\Sigma_2^{-1} \\Sigma_1) - d + \\log(\\det(\\Sigma_2)/\\det(\\Sigma_1)) \\right)$。我们定义两个高斯近似为 $\\mathcal{P}_{\\text{true}} = \\mathcal{N}(u^\\star, (\\nabla^2 \\Phi(u^\\star))^{-1})$ 和 $\\mathcal{P}_{\\text{GN}} = \\mathcal{N}(u^\\star, H_{\\mathrm{GN}}(u^\\star)^{-1})$。对称化散度是 $D = D_{\\mathrm{KL}}(\\mathcal{P}_{\\text{true}} \\| \\mathcal{P}_{\\text{GN}}) + D_{\\mathrm{KL}}(\\mathcal{P}_{\\text{GN}} \\| \\mathcal{P}_{\\text{true}})$。对数项相互抵消，得到简化表达式：\n      $$ D = \\frac{1}{2} \\left( \\mathrm{tr}\\left(H_{\\mathrm{GN}}(u^\\star) (\\nabla^2 \\Phi(u^\\star))^{-1}\\right) + \\mathrm{tr}\\left(\\nabla^2 \\Phi(u^\\star) (H_{\\mathrm{GN}}(u^\\star))^{-1}\\right) - 2d \\right) $$\n      其中 $d=2$ 是参数维度。\n\n4.  **聚合**：为每个测试用例计算诊断指标对 $[g, D]$，并收集到一个最终的列表的列表中以供输出。\n\n此过程在下面的 Python 代码中实现，为每个测试用例的正演模型及其导数提供了特定的函数。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the results.\n    \"\"\"\n    \n    test_cases = [\n        # Test 1 (linear, no curvature gap expected)\n        {\n            \"G\": lambda u: np.array([u[0] + 0.5 * u[1], u[1]]),\n            \"J_G\": lambda u: np.array([[1.0, 0.5], [0.0, 1.0]]),\n            \"hess_G_components\": lambda u: [np.zeros((2, 2)), np.zeros((2, 2))],\n            \"u0\": np.array([0.2, -0.3]),\n            \"C\": np.diag([0.49, 0.25]),\n            \"Gamma\": np.array([[0.09, 0.03], [0.03, 0.16]]),\n            \"y\": np.array([1.0, -0.5])\n        },\n        # Test 2 (nonlinear, zero residual at MAP)\n        {\n            \"G\": lambda u: np.array([u[0], np.exp(u[1])]),\n            \"J_G\": lambda u: np.array([[1.0, 0.0], [0.0, np.exp(u[1])]]),\n            \"hess_G_components\": lambda u: [np.zeros((2, 2)), np.array([[0.0, 0.0], [0.0, np.exp(u[1])]])],\n            \"u0\": np.array([0.2, -0.1]),\n            \"C\": np.diag([1.0, 1.0]),\n            \"Gamma\": np.diag([0.01, 0.01]),\n            \"y\": np.array([0.2, np.exp(-0.1)])\n        },\n        # Test 3 (nonlinear, well-constrained, small gap)\n        {\n            \"G\": lambda u: np.array([u[0], np.exp(u[1])]),\n            \"J_G\": lambda u: np.array([[1.0, 0.0], [0.0, np.exp(u[1])]]),\n            \"hess_G_components\": lambda u: [np.zeros((2, 2)), np.array([[0.0, 0.0], [0.0, np.exp(u[1])]])],\n            \"u0\": np.array([0.0, 0.0]),\n            \"C\": np.diag([10.0, 10.0]),\n            \"Gamma\": np.diag([0.04, 0.04]),\n            \"y\": np.array([0.8, np.exp(0.3)])\n        },\n        # Test 4 (nonlinear, weak data, larger gap)\n        {\n            \"G\": lambda u: np.array([u[0], np.exp(u[1])]),\n            \"J_G\": lambda u: np.array([[1.0, 0.0], [0.0, np.exp(u[1])]]),\n            \"hess_G_components\": lambda u: [np.zeros((2, 2)), np.array([[0.0, 0.0], [0.0, np.exp(u[1])]])],\n            \"u0\": np.array([0.0, 0.0]),\n            \"C\": np.diag([0.25, 0.25]),\n            \"Gamma\": np.diag([4.0, 4.0]),\n            \"y\": np.array([0.8, np.exp(0.3)])\n        },\n        # Test 5 (nonlinear with coupling and correlated noise)\n        {\n            \"G\": lambda u: np.array([np.exp(u[0] + 0.5 * u[1]), u[0]**2 - u[1]]),\n            \"J_G\": lambda u: np.array([\n                [np.exp(u[0] + 0.5 * u[1]), 0.5 * np.exp(u[0] + 0.5 * u[1])],\n                [2 * u[0], -1.0]\n            ]),\n            \"hess_G_components\": lambda u: [\n                np.exp(u[0] + 0.5 * u[1]) * np.array([[1.0, 0.5], [0.5, 0.25]]),\n                np.array([[2.0, 0.0], [0.0, 0.0]])\n            ],\n            \"u0\": np.array([0.0, 0.0]),\n            \"C\": np.diag([1.0, 1.0]),\n            \"Gamma\": np.array([[0.09, 0.04], [0.04, 0.16]]),\n            \"y\": np.array([1.0, 0.21])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        d = 2 \n        G = case[\"G\"]\n        J_G = case[\"J_G\"]\n        hess_G_components = case[\"hess_G_components\"]\n        u0 = case[\"u0\"]\n        C = case[\"C\"]\n        Gamma = case[\"Gamma\"]\n        y = case[\"y\"]\n\n        C_inv = np.linalg.inv(C)\n        Gamma_inv = np.linalg.inv(Gamma)\n\n        def phi(u):\n            res_obs = y - G(u)\n            res_prior = u - u0\n            return 0.5 * res_obs.T @ Gamma_inv @ res_obs + 0.5 * res_prior.T @ C_inv @ res_prior\n\n        def grad_phi(u):\n            JG_u = J_G(u)\n            res_obs = y - G(u)\n            res_prior = u - u0\n            return -JG_u.T @ Gamma_inv @ res_obs + C_inv @ res_prior\n\n        def hess_phi(u):\n            JG_u = J_G(u)\n            res_obs = y - G(u)\n            hess_G = hess_G_components(u)\n            \n            H_GN = JG_u.T @ Gamma_inv @ JG_u + C_inv\n            \n            w = Gamma_inv @ res_obs\n            discrepancy_term = np.zeros((d, d))\n            for i in range(len(w)):\n                discrepancy_term -= w[i] * hess_G[i]\n                \n            return H_GN + discrepancy_term\n\n        # Find the MAP estimate u_star using a second-order method\n        opt_result = minimize(phi, u0, method='trust-ncg', jac=grad_phi, hess=hess_phi, tol=1e-9)\n        u_star = opt_result.x\n        \n        # Evaluate true and Gauss-Newton Hessians at u_star\n        H_true = hess_phi(u_star)\n        J_G_star = J_G(u_star)\n        H_GN = J_G_star.T @ Gamma_inv @ J_G_star + C_inv\n\n        # Compute diagnostic 1: curvature gap ratio g\n        norm_diff = np.linalg.norm(H_true - H_GN, 'fro')\n        norm_true = np.linalg.norm(H_true, 'fro')\n        g = norm_diff / norm_true if norm_true  0 else 0.0\n\n        # Compute diagnostic 2: symmetrized KL divergence D\n        try:\n            H_true_inv = np.linalg.inv(H_true)\n            H_GN_inv = np.linalg.inv(H_GN)\n            \n            term1 = np.trace(H_GN @ H_true_inv)\n            term2 = np.trace(H_true @ H_GN_inv)\n\n            D_val = 0.5 * (term1 + term2 - 2 * d)\n        except np.linalg.LinAlgError:\n            D_val = np.nan\n\n        results.append([g, D_val])\n\n    # Format output as specified\n    formatted_results = [f\"[{g:.8f},{D:.8f}]\" for g, D in results]\n    print(f\"[[{','.join(formatted_results)}]]\")\n\nsolve()\n```", "id": "3411467"}]}